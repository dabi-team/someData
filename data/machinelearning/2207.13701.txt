2
2
0
2

l
u
J

6
2

]

G
L
.
s
c
[

1
v
1
0
7
3
1
.
7
0
2
2
:
v
i
X
r
a

Branch Ranking for Eï¬ƒcient Mixed-Integer
Programming via Oï¬„ine Ranking-based Policy
Learning

Zeren Huang1, Wenhao Chen1, Weinan Zhang1 ((cid:0)), Chuhan Shi1, Furui Liu2,
Hui-Ling Zhen2, Mingxuan Yuan2, Jianye Hao2, Yong Yu1, and Jun Wang2,3

1 Shanghai Jiao Tong University
2 Huawei Noahâ€™s Ark lab
3 University College London
{sjtu_hzr, wnzhang}@sjtu.edu.cn,{liufurui2, zhenhuiling2}@huawei.com

Abstract. Deriving a good variable selection strategy in branch-and-
bound is essential for the eï¬ƒciency of modern mixed-integer program-
ming (MIP) solvers. With MIP branching data collected during the pre-
vious solution process, learning to branch methods have recently become
superior over heuristics. As branch-and-bound is naturally a sequential
decision making task, one should learn to optimize the utility of the whole
MIP solving process instead of being myopic on each step. In this work,
we formulate learning to branch as an oï¬„ine reinforcement learning (RL)
problem, and propose a long-sighted hybrid search scheme to construct
the oï¬„ine MIP dataset, which values the long-term utilities of branching
decisions. During the policy training phase, we deploy a ranking-based
reward assignment scheme to distinguish the promising samples from
the long-term or short-term view, and train the branching model named
Branch Ranking via oï¬„ine policy learning. Experiments on synthetic
MIP benchmarks and real-world tasks demonstrate that Branch Rank-
ing is more eï¬ƒcient and robust, and can better generalize to large scales
of MIP instances compared to the widely used heuristics and state-of-
the-art learning-based branching models.

Keywords: Combinatorial Optimization, Reinforcement Learning, Deep
Learning

1

Introduction

Mixed-integer programming (MIP) has a wide range of real-world applications
such as scheduling, manufacturing and routing [18,13]. A universally applicable
method for solving MIPs is branch-and-bound (B&B) [14], which performs de-
composition of the solution set iteratively, and thus building a search tree with
nodes corresponding to MIP problems.

Node selection and variable selection are two important sequential decisions
to be made at each iteration of the B&B algorithm. The node selection strategy
decides which node to process next, while the variable selection strategy decides

 
 
 
 
 
 
2

Z. Huang et al.

which fractional variable to branch on at the current node. In this work, we focus
on variable selection, which can dramatically inï¬‚uence the performance of the
B&B algorithm, as discussed in [14].

The variable selection strategies in modern MIP solvers are generally based
on manually designed heuristics, which are strongly dependent on the problem
property. Therefore, when the structure or the scale of the problem changes,
an adjustment of the heuristics is often required, which is time-consuming and
labor-intensive [6]. To tackle the above issues, machine learning naturally be-
comes a candidate for constructing a more eï¬ƒcient and generalizable variable
selection policy [3]. The similarity among many previous learning-based meth-
ods lies in the way to train the branching policy [12,1,7,10,15]. Speciï¬cally, the
policy models are usually trained via imitation learning to mimic an eï¬€ective but
slow heuristic, i.e., strong branching (SB), which performs a one-step branching
simulation for each candidate variable and greedily selects one with the highest
one-step utility, the SB score, which is related to the dual bound improvements.
As B&B is naturally a sequential decision making task, such a heuristic is con-
sidered to be myopic, that is, it only focuses on the short-term utility while
neglecting the long-term impact on the bottom levels of the B&B tree. Under
some circumstances, the optimal short-term branching decision can lead to poor
long-term utility.

To address the above issues, in this paper, we formulate variable selection in
B&B as an oï¬„ine reinforcement learning (RL) problem, and propose a top-down
hybrid search scheme to construct an oï¬„ine dataset which involves branching in-
formation of both long-term and short-term decisions. Using the oï¬„ine dataset,
we deploy a ranking-based reward assignment scheme to distinguish the promis-
ing samples from other samples. Finally, the variable selection policy, which is
named Branch Ranking (BR), is trained via ranking-based policy learning
method, which is derived equivalent as maximizing the log-likelihood of samples
with the corresponding rewards as the weights.

We conduct extensive experiments on four classes of NP-hard MIP bench-
marks and deploy our proposed policy in the real-world supply and demand
simulation tasks, and the results demonstrate that Branch Ranking is supe-
rior over widely used heuristics as well as oï¬„ine state-of-the-art learning-based
branching models, regarding the solution time and the number of nodes pro-
cessed in the B&B tree. Furthermore, the evaluation results on diï¬€erent scales
of MIPs show that Branch Ranking also has better generalization ability over
larger scales of problems compared to other learning-based policies.

Branch Ranking has been deployed in real-world demand and supply simu-
tion tasks of Huawei, a global commercial technology enterprise, where the exper-
iments demonstrate that Branch Ranking is applicable in decision problems
encountered in practice.

In summary, the main technical contributions of this work are threefold:

â€“ We formulate the variable selection in B&B as an oï¬„ine RL task, and design
a novel search scheme to construct the oï¬„ine dataset which involves both
long-term and short-term branching information.

Branch Ranking for Eï¬ƒcient Mixed-Integer Programming

3

â€“ Based on the constructed oï¬„ine dataset, we propose a ranking-based re-
ward assignment scheme to distinguish the promising samples, and train the
variable selection policy named Branch Ranking.

â€“ Extensive experiments demonstrate that Branch Ranking can make better
improvements to the optimization algorithm compared to other state-of-the-
art heuristics and learning-based branching polices. Branch Ranking also
shows better generalization ability to MIP problems with diï¬€erent scales.

2 Related Work

The branching decision is one of the most important decisions to be made in the
MIP solvers, which can signiï¬cantly inï¬‚uence the eï¬ƒciency of the optimization
algorithm. The core of branching for mixed-integer programming (MIP) is the
evaluation of the branching scores and cost via tuning hyper-parameters. It is not
surprising that the most of previous branching methods are devoted to imitate
strong branching (SB), because with one step forward, SB can often eï¬€ectively
reduce the number of search tree nodes.

Khalil et al. [12] extract 72 branching features artiï¬cially including static
and dynamic ones, and mimicked the strong branching strategy via a pair-wise
variable ranking formulation. Similarly, our work also adopts a ranking-based
scheme, however, we apply it in reward assignment which assigns higher implicit
rewards to the top-ranking promising branching samples from the long-term or
short-term view, and the assigned reward can be regarded the sample weight in
the learning objective. More recently, Gasse et al. [7] train their policy model
via imitation learning from the strong branching expert rules. The inspiring in-
novation is a novel graph convolutional network (GCN) integrated into Markov
decision process (MDP) for variable selection which is beneï¬cial to generalization
and branching cost. Based on a similar policy architecture and learning method,
Nair et al. [15] propose neural branching, which enables the expert policy to
scale to large MIP instances through hardware acceleration. To attain a com-
putationally inexpensive model compared to GCN [7], Gupta et al. [10] propose
a hybrid architecture which combines the expressive GCN and computationally
inexpensive multi-layer perceptrons (MLP) for eï¬ƒcient branching with limited
computing power. With the aim at learning a branching policy that generalizes
across heterogeneous MIPs, Zarpellon et al. [17] incorporate an explicit param-
eterization of the state of the search tree to modulate the branching decision.
From another perspective, Balcan et al. [2] propose to learn a weighted version
of several existing variable scoring rules to improve the branching decision. Dif-
ferent from the previous works, Sun et al. [16] employ reinforcement learning
with a novelty based evolutionary strategy for learning a branching policy. An-
other reinforcement learning approach to tackle the branching decision problem
comes from [5], which uses approximate Q-learning and the subtree size as value
function. Note that the main algorithms of the above two reinforcement learning
methods are on-policy, which evaluates and improves the same policy for mak-
ing decisions, thus cannot leverage the abundant pre-collected branching data
to train the model.

4

Z. Huang et al.

3 Background and Preliminaries

In this section, we ï¬rst introduce the background of mixed-integer programming
and the branch-and-bound algorithm. Then, we introduce several typical and
eï¬€ective variable selection heuristics for branching.

3.1 Mixed-Integer Programming Problem

The general combinatorial optimization task is usually formulated as a mixed-
integer programming (MIP) problem, which can be written as the following
form:

(cid:8)z(cid:62)x | Ax â‰¤ b, x âˆˆ Zp Ã— Rnâˆ’p(cid:9) ,

(1)

arg min
x

where x is the vector of n decision variables, x âˆˆ Zp Ã— Rnâˆ’p means p out of
n variables have integer constraints, z âˆˆ Rn is the objective coeï¬ƒcient vector,
b âˆˆ Rm is the right-hand side vector, A âˆˆ RmÃ—n is the constraint matrix.

3.2 Branch-and-Bound

B&B is a classic approach for solving MIPs, which adopts a search tree consisting
of nodes and branches to partition the total set of feasible solutions into smaller
subsets.

The procedure of B&B can be described as follows: denote the optimal solu-
tion to the LP relaxation of Eq. (1) as xâˆ—, if it happens to satisfy the integrality
requirements, then it is also the solution to Eq. (1); else some component of xâˆ—
is not integer (while restricted to be integer), then one can select a fractional
variable xi and decompose the LP relaxation into two sub-problems by adding
rounding bounds xi â‰¥ (cid:100)xâˆ—
i (cid:99), respectively. By recursively perform-
ing the above binary decomposition, B&B naturally builds a search tree, within
which each node corresponds to a MIP.

i (cid:101) and xi â‰¤ (cid:98)xâˆ—

During the search process, the best feasible solution of the MIP provides an
upper bound (or primal bound) for the optimal objective value; and solving the
LP relaxation of the MIP provides the lower bound (or dual bound). The B&B
algorithm terminates when the gap between the upper bound and the lower
bound reduces to some tolerance threshold.

3.3 Variable Selection Heuristics for Branching

In the B&B framework for solving MIPs, for a given node, variable selection
refers to the decision to select an integer variable to branch on. Designing a
good variable selection strategy is essential for the eï¬ƒciency of solving MIPs,
which can lead to a much smaller search tree, that is, the number of processed
nodes is signiï¬cantly reduced and thus the whole solution process speeds up.

In modern MIP solvers, the variable selection module for branching is re-
garded as a core component of the optimization algorithm, and is generally based

Branch Ranking for Eï¬ƒcient Mixed-Integer Programming

5

on manually designed heuristics. One of the classic heuristics is strong branching,
which involves computing the dual bound improvements for each candidate vari-
able by solving two resulting LP relaxations after temporarily adding bounds.
Strong branching can often yield the smallest search trees while bringing much
more computational costs, therefore, it is impractical to apply strong branching
at each node.

Another more eï¬ƒcient heuristic, pseudo cost branching, is designed to imitate
strong branching, which estimates the dual bound improvements of candidate
variables based on historical information gathered in the tree. The combination
of the above two heuristics is called hybrid branching, which employs strong
branching at the beginning of the algorithm, and performs pseudo cost branching
when more branching history information is available.

The current state-of-the-art variable selection heuristic is reliability branch-
ing, which is a reï¬nement of pseudo cost branching, and is deployed as the default
branching strategy by many modern MIP solvers (e.g., CPLEX [4], SCIP [9],
etc). During the solution process, reliability branching applies strong branching
to those variables whose pseudo costs are uninitialized or unreliable. The pseudo
costs are considered to be unreliable if there is not enough historical branching
data aggregated.

4 Methodology

In this section, we ï¬rst formulate learning variable selection strategy for branch-
ing as an oï¬„ine RL problem, and introduce our method to derive a more eï¬ƒcient
and robust variable selection policy named Branch Ranking.

4.1 Oï¬„ine RL Formulation

The variable selection problem in branch-and-bound can be formulated as a
sequential decision-making process with tuple (S, A, Î , g), where S is the state
space, A is the action space, Î  is the policy space and the function g(s, a) is
the dynamics function of the environment. At time step t â‰¥ 0, a policy Ï€ âˆˆ Î 
maps the environment state st âˆˆ S to an action at âˆˆ A: Ï€(st) = at, then the
next state is st+1 = g(st, at). In the following, we clarify the state space S, the
action space A, the transition function g(s, a) and the roll-out trajectory Ï„ in
the context of branch-and-bound.

State Space S. The representation of S consists of the whole search tree with
previous branching decisions, the LP solution of each node, the processing leaf
node and also other statistics stored during the solution process. In practice, we
encode the state using GCN as [7].

Action Space A. At the tth iteration, the action space A contains all the candi-
date variables X t
d} to branch on at the currently processing
node.

cand = {xt

2, . . . , xt

1, xt

6

Z. Huang et al.

Transition Function g(s, a). After a policy Ï€ selects an action at = x âˆˆ X t
cand in
state st, the search tree is expanded and the LP relaxations of the two sub-nodes
are solved. Then, the tree is pruned if possible, and eventually the next leaf node
to process is selected. The environment then proceeds to the next state st+1.

Trajectory Ï„ A roll-out trajectory Ï„ comprises a sequence of states and actions:
{s0, a0, s1, a1, . . . , sT , aT }. We deï¬ne the return RÏ„ for the trajectory Ï„ as the
negative value of the number of visited (processed) nodes within it. Intuitively,
each trajectory corresponds to a B&B tree. Note that there is no explicit reward
deï¬ned for each state-action pair.

Based on the above MDP formulation, online RL algorithms usually suï¬€er
from low sample eï¬ƒciency, which makes the training prohibitively long and leaves
the pre-collected branching data of no use. To derive a more eï¬ƒcient scheme, we
model the learning problem in the oï¬„ine RL settings, where a batch of m roll-out
trajectories D = {(Ï„i, RÏ„i ), i = 1, 2, . . . , m} is collected using some policy Ï€D.
Using this dataset (without interaction with the environment), our objective is
to construct a policy Ï€(a|s) which incurs the highest trajectory return when it
is actually applied in the MDP.

4.2 Collecting Oï¬„ine Data through Top-Down Hybrid Search

To construct the oï¬„ine dataset, a commonly-used roll-out policy is strong branch-
ing, which is time-consuming while often resulting in a small search tree. Under
this case, the roll-out policy can be regarded as the expert policy, which di-
rectly provides the supervised data. As shown in the top dotted box in Figure 1,
the blue node represents the action selected by strong branching, which has the
largest dual bound improvement after a one-step branching simulation. Strong
branching can be viewed as a greedy method for variable selection, therefore, we
consider such a strategy to be myopic, that is, it only focuses on the one-step
return, which can lead to poor long-term outcome in some cases. As discussed in
[8], a deeper lookahead can often be useful for making better branching decisions
at the top levels of the search tree.

To derive a more farsighted and robust variable selection strategy for branch-
ing, we introduce a top-down hybrid search scheme to collect oï¬„ine branching
data which contains more information about the long-term impact of the branch-
ing decision. Figure 1 presents an overview of our proposed hybrid search scheme,
which is a combination of short-term and long-term search. The basic procedure
is as follows:

â€“ At the tth iteration, for the currently processing node, the candidate vari-
d}. First, we randomly sample the vari-
exp =

cand for k times, which returns a subset of k variables X t

cand = {xt

2, . . . , xt

1, xt

able set is X t
ables in X t
, xt
{xt
e2
e1

, . . . , xt
ek
â€“ For each variable x âˆˆ X t

}.

exp, we perform a one-step branching simulation,
and obtain k simulation trees with diï¬€erent branches at the current node.
Denote the set of k simulation trees as T t

}.

exp = {T t
e1

, T t
e2

, . . . , T t
ek

Branch Ranking for Eï¬ƒcient Mixed-Integer Programming

7

Fig. 1: Hybrid oï¬„ine data collection

â€“ As it shown in the lower dotted box in Figure 1, for each simulation tree T t
ei

âˆˆ
T t
exp, we continue the branching simulation, that is, to roll out the trajectory
using some myopic policy (e.g. strong branching) until the termination node
(black nodes), which returns a trajectory return Rt
. Then, we obtain a
ei
set consisting of pairs of the top-level exploring action and the long-term
trajectory return: Bt = {(xt
ei

â€“ Back up at the node processing at the tth iteration, we select the action
exp with the highest long-term return in Bt (the green node in Figure
x âˆˆ X t
1), and execute the corresponding branching in the real environment. The
search tree then picks the next node to process using the node selection
policy incorporated in the solver, and then the environment transits to the
next state. The above process continues until the problem instance is solved.

), i = 1, 2, . . . , k}.

, Rt
ei

Note that though collection branching data leads to multiple rounds of branch-
ing simulation, the whole process is conducted in an oï¬„ine way, and thus the in-
curred cost is acceptable for practical use. Denote the collected oï¬„ine dataset us-
ing the above hybrid search scheme as Dhyb = (cid:8){(sL
)}N
i , RL
i , RL
in which DL = {(sL
i=1 is collected by the top-down long-term search
as mentioned above, and DSB = {(sSB
i=1 is collected by strong branch-
ing during the simulation process.

i=1âˆª{(sSB

i )}M

i )}M

i , aL

i , aL

, aSB
i

, aSB
i

)}N

i

i

i=1

(cid:9),

4.3 Ranking-based Policy Learning

Given the oï¬„ine dataset Dhyb = {DL âˆª DSB} collected by our proposed hybrid
search scheme, our goal is to derive a eï¬ƒcient and robust variable selection policy

â€¦â€¦ğ’™ğŸâ‰¤ğ’™ğŸâˆ—(ğ’™ğŸ,ğ’™ğŸâˆ—,(ğ’™ğŸ,ğ’™ğŸâˆ—),ğ’™ğŸ‘,ğ’™ğŸ‘âˆ—,â‹¯}ğ’™ğŸâ‰¥ğ’™ğŸâˆ—ğ’™ğŸâ‰¤ğ’™ğŸâˆ—ğ’™ğŸâ‰¥ğ’™ğŸâˆ—ğ’™ğŸ‘â‰¥ğ’™ğŸ‘âˆ—ğ’™ğŸ‘â‰¤ğ’™ğŸ‘âˆ—current nodebest action withone-step lookaheadStrong Branchingroll out until termination ğ’“ğ’†ğ’•ğ’–ğ’“ğ’=ğ‘¹ğŸ‘best actionwith the highestlong-term returncandidate variablesğ’“ğ’†ğ’•ğ’–ğ’“ğ’=ğ‘¹ğŸğ’“ğ’†ğ’•ğ’–ğ’“ğ’=ğ‘¹ğŸ8

Z. Huang et al.

Ï€Î¸ (Î¸ is the policy parameter) which can yield a smaller search tree, and thus
leading to faster performance.

Assume that the optimal policy for branching is Ï€âˆ—, and if Ï€âˆ— is available, the
policy Ï€ can be trained via minimizing the KL divergence of Ï€âˆ— and Ï€, which is
also equivalent to maximizing the log-likelihood:

min
Î¸

KL [Ï€âˆ—(cid:107)Ï€Î¸] = max

Î¸

E(s,a)âˆ¼Dexp [Ï€âˆ—(a|s) log Ï€Î¸(a|s)] ,

(2)

where Dexp is the roll-out dataset collected by Ï€âˆ—. We regard Ï€âˆ—(a|s) as the
implicit reward Ë†r(s, a) of the state-action pair (s, a), which measures the implicit
value of taking action a in state s in the branch-and-bound tree. Assume that
the state-action pairs are sampled from the oï¬„ine dataset, the learning objective
can be approximated in the following form:

max
Î¸

E(s,a)âˆ¼Dhyb [Ë†r(s, a) log Ï€Î¸(a|s)] .

(3)

Using the oï¬„ine dataset Dhyb = {DLâˆªDSB}, we propose a simple while eï¬€ec-
tive ranking-based implicit reward assignment scheme. To derive a policy which
can balance the short-term and long-term beneï¬ts, we assign higher implicit re-
wards to the promising state-action pairs which lead to the top-ranked long-term
or short-term return. We ï¬rst give a ranking-based deï¬nition of long-term and
short-term promising state-action pairs in Deï¬nition 1 and 2, respectively.

Deï¬nition 1 (Long-Term Promising). For a state-action pair (s, a), its cor-
responding trajectory return RL(s, a) is the return of the trajectory starting from
state s and selecting the action a. A state-action pair (s, a) is deï¬ned to be long-
term promising if RL(s, a) ranks in the top p% trajectories starting from state s
and with the highest trajectory returns. Note that p is a tunable hyper-parameter.

Deï¬nition 2 (Short-Term Promising). A state-action pair (s, a) is short-
term promising if the one-step look-ahead return of the action a ranks the highest
in state s in terms of strong branching score, which is related to the dual bound
improvements after a one-step simulation.

Based on the above ranking-based deï¬nitions, for a state-action pair (s, a), its
implicit reward Ë†r(s, a) is deï¬ned as

Ë†r(s, a) =

(cid:26) 1,
0,

(s, a) is long-term or short-term promising
otherwise.

(4)

After the reward assignment, the ï¬nal constructed training dataset is a combina-
tion of long-term and short-term promising samples. We denote the proportion
of short-term promising samples as h, which is a tunable hyper-parameter.

We then train our policy Ï€Î¸ via Eq. (3), and evaluate Ï€Î¸ on the new problem

instances. The ï¬‚owchart of our proposed method is shown in Figure 2.

Branch Ranking for Eï¬ƒcient Mixed-Integer Programming

9

Fig. 2: The ï¬‚owchart of Branch Ranking method.

5 Experiments

We conduct extensive experiments on the MIP benchmarks, and evaluate our
policy compared to other state-of-the-art variable selection heuristics as well as
learning-based branching models. By comparative analysis on the experimental
results, we try to answer the following research questions:

RQ1: Is our policy more eï¬ƒcient and robust compared to typical heuristics, and

learning-based models which imitate the strong branching strategy?

RQ2: Can our policy generalize to diï¬€erent scales of problem instances of the

same MIP classes?

RQ3: How does diï¬€erent proportions of long-term and short-term promising

samples inï¬‚uence the ï¬nal performance of our trained polices?

RQ4: Can our policy be deployed in challenging real-world tasks and improve

the solution quality given a solving time limit?

5.1 Experimental Setup

Benchmarks used for training and evaluating The synthetic MIP bench-
marks consist of four classes of NP-hard MIP problems as Gasse et al. [7]: set
covering, combinatorial auction, capacitated facility location and maximum in-
dependent set, which cover a wide range of real-world decision problems. For
each class of MIPs, we randomly generate three scales of problem instances for
training and evaluating. Since larger MIPs are usually more diï¬ƒcult to solve, the
generated instances are referred to as easy, medium and hard instances accord-
ing to the problem scales. For a more detailed description of the MIP problems,
one can refer to [7]. For each MIP class, we train the learning-based policy only

Roll-out Dataset   MIP InstancesOptimizationRanking-based  Reward Assignment Data withassignedrewardHybrid SearchLearning a policy viaoffline training  new MIP Instanceswith  potentiallydifferent scalesOptimizationApply in BranchingSolutiongeneralizable range of MIPs 10

Z. Huang et al.

on easy instances, and evaluate on other new easy instances as well as medium
and hard instances. For each diï¬ƒculty, the number of instances generated for
evaluation is 20.

Note that we are also aware of other MIP benchmarks such as MIPLIB or
the problem benchmarks of ML4CO, which we do not include in the experiments
since our benchmarks are representative of the types of the MIP problems en-
countered in practice. Moreover, the problem benchmarks of ML4CO or MIPLIB
are also based on typical binary MIP problems which shares the similar formu-
lation as our benchmarks.

We use the open-source MIP solver SCIP 6.0.1 [9] for solving the MIPs and
implementing our variable selection policy. The main algorithmic framework for
optimizing the MIPs is branch-and-bound, combined with the cutting planes
enabled at the root node and some other MIP heuristics. The maximum time
limit of solving a problem instance is set to 3600 seconds.

Metrics We consider both the number of processed B&B nodes and the MIP
solution time as the metrics to demonstrate the comprehensive performance
of diï¬€erent variable selection policies. For each evaluated policy, we show the
average value and the standard deviation of the evaluation results over a number
of test instances. Note that the number of processed B&B nodes are reported
only on solved instances (integrality gap is zero) within the time limit.

Baselines We compare Branch Ranking with six baselines.

â€“ Branching heuristics: we compare against four commonly-used and ef-
fective branching heuristics in MIP solvers: inference (Inference), mostinf
(Mostinf), pseudocost branching (Pscost) and reliability pseudocost branch-
ing (Relpcost). One can ï¬nd a detailed description of the heuristics in the
documentation of [9].

â€“ Khalil [12]: the state representation of MIP is based on available MIP statis-
tics. A SVMrank [11] model is used to learn an approximation to strong
branching.

â€“ GCN [7]: the state representation of MIP is the bipartite graph, and the
policy architecture is based on GCN which takes the MIP state as the inputs.
The policy is trained to mimic the strong branching strategy via imitation
learning.

Note that the method of [16] or [5] is not included as a learning-based baseline
since the proposed algorithmic framework is on-policy, while in this work, we
formulate learning to branch in the oï¬„ine settings, and propose to derive a
branching policy via oï¬„ine learning, which we consider to be more suitable for
the nature of the variable selection problem in B&B.

Hyper-parameters The architecture of our policy network is based on GCN as
[7]. For other hyper-parameters, we set the exploring times k to 30, the ranking

Branch Ranking for Eï¬ƒcient Mixed-Integer Programming

11

parameter p to 10. We ï¬ne-tune the data proportion parameter h for each MIP
class, and set h to 0.7, 0.9, 0.95, 0.9, 0.9 for set covering, combinatorial auction,
capacitated facility location and maximum independent set, respectively. For each
learning-based branching model, the number of training and validation samples
are 50000 and 5000, respectively.

5.2 Performance Comparison (RQ1 & RQ2)

As shown in Table 1, for medium and hard instances of each MIP class, our pro-
posed Branch Ranking clearly outperforms all the baselines in terms of the
solution time, and also leads to the smallest number of processed B&B nodes
on three classes of the MIP problems, set covering, combinatorial auction and
maximum independent set. For capacitated facility location, Branch Ranking
produces a B&B tree with fewer nodes compared to other learning-based mod-
els and heuristics except Relpcost. However, Branch Ranking reduces the
solution time notably.

As for the easy instances, consider-
ing the solution time, Branch Rank-
ing is superior over other baselines on
set covering, capacitated facility location
and maximum independent set. For the
combinatorial auction problems, Branch
Ranking achieves the second minimum
solution time. On account of the num-
ber of processed B&B nodes, Branch
Ranking has also shown comparable per-
formance on each MIP class. Note that
though Relpcost leads to smaller B&B
trees, it is not competitive regarding the
solution time.

Fig. 3: Number of instances with dif-
ferent solution time on the hard set
covering problems.

The comparative results demonstrate
that our proposed policy Branch Rank-
ing is more
compared to
eï¬ƒcient
the state-of-the-art oï¬„ine learning-based
branching models, and also branching heuristics adopted in modern MIP solvers.
Moreover, Branch Ranking is also more robust compared to the learning-based
models which imitate the strong branching strategy. Speciï¬cally, Khalil and
GCN suï¬€er from performance ï¬‚uctuations on the medium and hard maximum
independent set instances, while Branch Ranking still achieves less solution
time and fewer B&B nodes compared to other well-performed baselines.

Since the learning models are trained only on the easy instances, we are also
able to evaluate the generalization ability of diï¬€erent polices. The evaluation re-
sults on medium and hard instances show that our proposed Branch Ranking
performs remarkably well on larger instances with the same MIP class, and even
gains higher performance improvements compared to other baselines when the
problem size becomes larger. The results on diï¬€erent scales of instances indicate

0500100015002000250030003500 6 R O X W L R Q  W L P H   V 0246810 1 X P E H U  R I  L Q V W D Q F H V 5 H O S V F R V W * & 1 % U D Q F K  5 D Q N L Q J12

Z. Huang et al.

Table 1: Evaluation results of diï¬€erent heuristics and learning-based policies
in terms of the solution time and the number of processed B&B nodes. For
each MIP class, the machine learning models are trained on easy instances only.
The best evaluation result is highlighted. Among the compared methods, the
learning-based ones are marked with âˆ—.

Method

Easy

Set Covering

Medium

Hard

Time

Nodes

Time

Nodes

Time

Nodes

Inference 16.15Â±17.17 1366Â±1636 689.85Â±893.48 62956Â±91525
Mostinf
31.22Â±33.59 2397Â±2565 1216.57Â±1042.13 73955Â±91098
Pscost
139.28Â±178.43 13101Â±17965
Relpscost
Khalil *
GCN *

6.96Â±2.38
9.84Â±3.07 105Â±125 106.21Â±112.43
146.58Â±185.44
8.18Â±3.76
86.76Â±98.56
6.99Â±1.80
80.76Â±87.97

118Â±88
168Â±138
BR (Ours) * 6.93Â±1.67 163Â±129

N/A
3600.00Â±0.00
N/A
3600.00Â±0.00
2867.07Â±882.83
128027Â±53366
2530.76Â±1121.30 100952Â±65534
7223Â±9853
73780Â±33070
2866.54Â±973.77
6896Â±9330
5067Â±6512
71576Â±47806
2356.36Â±1189.37
4624Â±5912 2199.49Â±1170.82 67784Â±46808

209Â±134

Combinatorial Auction

Method

Easy

Medium

Hard

Time

Nodes

Time

Nodes

Time

Nodes

25.09Â±13.84

Inference
Mostinf
Pscost
Relpscost
Khalil *
GCN *

2.22Â±1.40 638Â±922
91328Â±99142
3.20Â±3.36 938Â±1569 332.19Â±269.72 76349Â±62942 2786.27Â±73.02 282830Â±42980
22.91Â±17.12
39135Â±37257
1.97Â±0.98 374Â±432
3843Â±3593
19.53Â±7.15
14788Â±19115
28Â±54
3.18Â±1.58
987Â±843
19291Â±25459
24.43Â±13.66
2.17Â±1.16 123Â±129
1154Â±782
970Â±793
14529Â±20819
12.74Â±6.92
1.97Â±0.60 103Â±105
BR (Ours) * 1.98Â±0.62 106Â±102 12.12Â±5.72
902Â±627 198.02Â±248.70 14380Â±20057

386.61Â±351.55
213.66Â±229.91
612.89Â±784.94
201.78Â±260.23

655.30Â±695.36

6043Â±4173

Capacitated Facility Location

Method

Easy

Medium

Hard

Time

Nodes

Time

Nodes

Time

Nodes

Inference
Mostinf
Pscost
Relpscost
Khalil *
GCN *

75.54Â±75.50 254Â±455 356.02Â±400.02 1040Â±1214 756.98Â±408.78 547Â±384
662.62Â±375.54 393Â±273
43.88Â±33.26 128Â±194 280.38Â±281.19
40.80Â±31.99 107Â±161 271.28Â±241.35
644.65Â±400.78 433Â±360
42.29Â±29.15 72Â±131 320.14Â±286.33 383Â±513 703.75Â±425.64 233Â±239
664.58Â±420.50 369Â±275
41.88Â±30.64 106Â±155 282.97Â±342.77
638.07Â±410.58 479Â±444
34.03Â±33.05 160Â±214 255.35Â±259.19
BR (Ours) * 33.92Â±33.90 169Â±239 249.02Â±222.21 570Â±563 633.59Â±406.13 459Â±399

681Â±913
584Â±647

673Â±740
673Â±637

Maximum Independent Set

Method

Easy

Medium

Hard

Time

Nodes

Time

Nodes

Time

Nodes

Inference 32.99Â±103.48 16105Â±58614 1807.13Â±750.10 116957Â±84167
N/A
Mostinf
30.34Â±91.58 10084Â±35418 1547.43Â±901.47 60128Â±52244
N/A
Pscost
1565Â±3935 1042.47Â±798.46 43256Â±38143
10.11Â±13.88
66258Â±23654
Relpscost
4911Â±5774
136.78Â±113.04
209Â±603
8.83Â±5.11
53636Â±38140
Khalil *
503.94Â±712.89
353Â±917
10.50Â±16.62
7983Â±10915
60800Â±23837
GCN *
53804Â±37253
256.23Â±509.38 10948Â±23020
265Â±805
7.55Â±10.11
BR (Ours) * 7.54Â±9.53
102.91Â±118.77 3908Â±5613 2320.20Â±1457.44 50460Â±37859
260Â±742

3600.00Â±0.00
3600.00Â±0.00
3181.14Â±455.73
2462.67Â±1391.50
3023.02Â±798.52
2485.27Â±1446.23

Branch Ranking for Eï¬ƒcient Mixed-Integer Programming

13

that Branch Ranking is capable of generalizing over problems with larger
scales, and has better generalization ability in comparison to other learning-
based branching policies.

Furthermore, for a better visualization of the generalization results of the
well-performed baselines and our proposed Branch Ranking, Figure 3 shows
the number of instances with diï¬€erent solution time for Relpcost, GCN and
Branch Ranking on the hard set covering problems. The visualization re-
sults show that more instances are solved with less solution time using Branch
Ranking. Such results are consistent with our previous ï¬ndings, which also
proves that Branch Ranking can better improve the solution process for larger
MIP problems.

5.3 Hyper-parameter Study (RQ3)

To better understand the inï¬‚uence of the proportion of long-term or short-term
promising samples on the ï¬nal performance of Branch Ranking, we conduct a
hyper-parameter study to evaluate Branch Ranking with diï¬€erent proportions
of short-term promising samples on the set covering problems. The results are
shown in Figure 4, in which the dotted line represents the state-of-the-art oï¬„ine
learning-based branching model, GCN, which can be regarded as a policy trained
using only the short-term promising samples.

Fig. 4: Hyper-parameter study results of Branch Ranking with diï¬€erent pro-
portions of short-term promising samples on the set covering problems.

As demonstrated in Figure 4, for the easy instances, our proposed Branch
Ranking slightly outperforms GCN in terms of solution time and number of

                                                 % U D Q F K  5 D Q N L Q J * & 1Time (s)NodesNodesNodesEasyMediumHard14

Z. Huang et al.

processed B&B nodes under diï¬€erent proportions. Moreover, the policy with an
intermediate proportion achieves the best performance considering both the two
metrics.

For medium and hard instances, Branch Ranking gains remarkable perfor-
mance improvements by combining long-term and short-term promising samples.
Moreover, the empirical results show that too large or too small short-term sam-
ple proportion will worsen the performance of Branch Ranking. Precisely, for
the Set Covering problems, we ï¬nd that setting the short-term sample proportion
between 70% and 80% often yields a better branching policy.

The results of our hyper-parameter study can be explained from the follow-
ing aspects. First, as shown by Figure 4, the long-term promising samples is
beneï¬cial for deriving a more eï¬ƒcient and robust branching policy which can
make better decisions from a long-term view. Moreover, using excessive long-
term promising samples will probably not produce the best policy, since long-
term samples are also more diï¬ƒcult to learn for the policy models. To ease this
situation, as introduced in this work, we propose to combine the long-term and
short-term promising samples for learning a better variable selection policy for
branching.

5.4 Deployment in Real-World Tasks (RQ4)

To further verify the eï¬€ectiveness of our proposed branching policy, we deploy
Branch Ranking in the real-world supply and demand simulation tasks of
Huawei, a global commercial technology enterprise. The objective of this task
is to ï¬nd the optimal production planning to meet the current order demand,
according to the current raw materials, semi-ï¬nished products and the upper
limit of production capacity. The basic constraints include the production limit
for each production line in each factory, the lot size on product ratio, the order
rate, transportation limit, etc.

The collected real-world tasks consist of eight raw cases obtained from the
enterprise, each of which can be modeled as a MIP problem instance, and are
challenging for the solvers. We regard the raw MIP instances as the test instances,
and generate the training instances by adding Gaussian noise to the raw cases.
In our experiments, we compare Branch Ranking with the default branch-
ing strategy incorporated in SCIP (i.e., Relpcost in most cases). We report the
MIP solution quality and the MIP solution time on the raw instances. Specif-
ically, the MIP solution quality is measured by the primal-dual gap (lower is
better). As for the hyper-parameters, we ï¬ne-tune the data proportion parame-
ter h and set it to 0.9, and keep other hyper-parameters the same. The maximum
time limit of solving a problem instance is set to 1800 seconds. Note that fol-
lowing the industry procedure, when the problem instance is feasible and not
solved to optimal within the time limit, the solver will return an approximate
MIP solution.

As shown in Table 2, since the real-world supply and demand simulation
problems are challenging for the solvers, most of the cases are not solved to op-
timal within the time limit. For case 3 and 4, the MIPs are found infeasible. For

Branch Ranking for Eï¬ƒcient Mixed-Integer Programming

15

Table 2: Evaluation results of Branch Ranking and the default branching
strategy in SCIP in terms of the solution time and the solution quality (primal-
dual gap, the lower is better). Case 3 and 4 are found infeasible.

Method

Case 1

Case 2

Case 3

Case 4

Case 5

Case 6

Case 7

Case 8

Time Gap Time Gap Time Gap Time Gap Time Gap Time Gap Time Gap Time Gap
Default 1800.0 98.59 1800.0 1.36 N/A inf. N/A inf. 460.98 0 1800.0 1.14 1800.0 0.07 1800.0 0.21
BR (Ours) 1800.0 62.26 1800.0 0.87 N/A inf. N/A inf. 451.30 0 1800.0 1.14 1800.0 0.06 1800.0 0.19

case 5, the optimal MIP solution is found for both the baseline and Branch
Ranking, while Branch Ranking leads to less solution time. For other cases,
though the optimal solution is not found within the time limit, Branch Rank-
ing improves the solution quality notably: the average primal-dual gap reduction
ratio has reached 22.74% compared to the branching strategy incorporated in
SCIP. The results further indicate the signiï¬cance of making good branching
decisions. Our proposed Branch Ranking has also shown to be more eï¬ƒcient
on diï¬ƒcult real-world tasks.

6 Conclusion

In this paper, we present an oï¬„ine RL formulation for variable selection in the
branch-and-bound algorithm. To derive a more long-sighted branching policy
under such a setting, we propose a top-down hybrid search scheme to collect
the oï¬„ine samples which involves more long-term information. During the pol-
icy learning phase, we deploy a ranking-based reward assignment scheme which
assigns higher implicit rewards to the long-term or short-term promising sam-
ples, and learn a branching policy named Branch Ranking by maximizing the
log-likelihood weighted by the assigned rewards. The experimental results on syn-
thetic benchmarks and real-world tasks show that our derived policy Branch
Ranking is more eï¬ƒcient and robust compared to the state-of-the-art heuristics
and learning-based policies for branching. Furthermore, Branch Ranking can
also better generalize to the same class of MIP problems with larger scales.

Acknowledgements

The SJTU team is supported by Shanghai Municipal Science and Technology
Major Project (2021SHZDZX0102) and National Natural Science Foundation of
China (62076161). The work is also sponsored by Huawei Innovation Research
Program.

References

1. Alvarez, A.M., Louveaux, Q., Wehenkel, L.: A machine learning-based approxima-
tion of strong branching. INFORMS Journal on Computing 29(1), 185â€“195 (2017)

16

Z. Huang et al.

2. Balcan, M.F., Dick, T., Sandholm, T., Vitercik, E.: Learning to branch. In: Inter-

national Conference on Machine Learning. pp. 344â€“353. PMLR (2018)

3. Bengio, Y., Lodi, A., Prouvost, A.: Machine learning for combinatorial optimiza-
tion: a methodological tour dâ€™horizon. European Journal of Operational Research
290(2), 405â€“421 (2021)

4. Cplex, I.I.: V12. 1: Userâ€™s manual for cplex. International Business Machines Cor-

poration 46(53), 157 (2009)

5. Etheve, M., AlÃ¨s, Z., Bissuel, C., Juan, O., Kedad-Sidhoum, S.: Reinforcement
learning for variable selection in a branch and bound algorithm. In: International
Conference on Integration of Constraint Programming, Artiï¬cial Intelligence, and
Operations Research. pp. 176â€“185. Springer (2020)

6. Fischetti, M., Lodi, A.: Heuristics in mixed integer programming. Wiley Encyclo-

pedia of Operations Research and Management Science (2010)

7. Gasse, M., ChÃ©telat, D., Ferroni, N., Charlin, L., Lodi, A.: Exact combinatorial
optimization with graph convolutional neural networks. In: Proceedings of the
33rd International Conference on Neural Information Processing Systems-Volume
2. pp. 15580â€“15592 (2019)

8. Glankwamdee, W., Linderoth, J.: Lookahead branching for mixed integer program-

ming. Tech. rep., Citeseer (2006)

9. Gleixner, A., Bastubbe, M., Eiï¬‚er, L., Gally, T., Gamrath, G., Gottwald, R.L.,
Hendel, G., Hojny, C., Koch, T., LÃ¼bbecke, M.E., Maher, S.J., Miltenberger, M.,
MÃ¼ller, B., Pfetsch, M.E., Puchert, C., Rehfeldt, D., SchlÃ¶sser, F., Schubert, C.,
Serrano, F., Shinano, Y., Viernickel, J.M., Walter, M., Wegscheider, F., Witt, J.T.,
Witzig, J.: The SCIP Optimization Suite 6.0. ZIB-Report 18-26, Zuse Institute
Berlin (July 2018), http://nbn-resolving.de/urn:nbn:de:0297-zib-69361
10. Gupta, P., Gasse, M., Khalil, E., Mudigonda, P., Lodi, A., Bengio, Y.: Hybrid
models for learning to branch. Advances in neural information processing systems
33, 18087â€“18097 (2020)

11. Joachims, T.: Optimizing search engines using clickthrough data. In: Proceedings
of the eighth ACM SIGKDD international conference on Knowledge discovery and
data mining. pp. 133â€“142 (2002)

12. Khalil, E.B., Bodic, P.L., Song, L., Nemhauser, G., Dilkina, B.: Learning to branch
in mixed integer programming. In: Proceedings of the Thirtieth AAAI Conference
on Artiï¬cial Intelligence. pp. 724â€“731 (2016)

13. MaraÅ¡, V., LaziÄ‡, J., DavidoviÄ‡, T., MladenoviÄ‡, N.: Routing of barge container
ships by mixed-integer programming heuristics. Applied Soft Computing 13(8),
3515â€“3528 (2013)

14. Morrison, D.R., Jacobson, S.H., Sauppe, J.J., Sewell, E.C.: Branch-and-bound al-
gorithms: A survey of recent advances in searching, branching, and pruning. Dis-
crete Optimization 19, 79â€“102 (2016)

15. Nair, V., Bartunov, S., Gimeno, F., von Glehn, I., Lichocki, P., Lobov, I.,
Oâ€™Donoghue, B., Sonnerat, N., Tjandraatmadja, C., Wang, P., et al.: Solving mixed
integer programs using neural networks. arXiv preprint arXiv:2012.13349 (2020)

16. Sun, H., Chen, W., Li, H., Song, L.: Improving learning to branch via reinforcement
learning. In: Learning Meets Combinatorial Algorithms at NeurIPS2020 (2020)
17. Zarpellon, G., Jo, J., Lodi, A., Bengio, Y.: Parameterizing branch-and-bound
search trees to learn branching policies. In: Proceedings of the AAAI Conference
on Artiï¬cial Intelligence. vol. 35, pp. 3931â€“3939 (2021)

18. Zhu, Z., Heady, R.B.: Minimizing the sum of earliness/tardiness in multi-machine
scheduling: a mixed integer programming approach. Computers & Industrial En-
gineering 38(2), 297â€“305 (2000)

