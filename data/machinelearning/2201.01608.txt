2
2
0
2

g
u
A
1
2

]
I
S
.
s
c
[

2
v
8
0
6
1
0
.
1
0
2
2
:
v
i
X
r
a

Botometer 101:
Social bot practicum for computational social scientists

Kai-Cheng Yang1, Emilio Ferrara2, and Filippo Menczer1

1Observatory on Social Media, Indiana University Bloomington, USA
2Information Sciences Institute, University of Southern California, USA

August 23, 2022

Abstract

Social bots have become an important component of online social media. Decep-
tive bots, in particular, can manipulate online discussions of important issues ranging
from elections to public health, threatening the constructive exchange of information.
Their ubiquity makes them an interesting research subject and requires researchers to
properly handle them when conducting studies using social media data. Therefore, it
is important for researchers to gain access to bot detection tools that are reliable and
easy to use. This paper aims to provide an introductory tutorial of Botometer, a public
tool for bot detection on Twitter, for readers who are new to this topic and may not be
familiar with programming and machine learning. We introduce how Botometer works,
the diﬀerent ways users can access it, and present a case study as a demonstration.
Readers can use the case study code as a template for their own research. We also
discuss recommended practice for using Botometer.

1

Introduction

Social bots are social media accounts controlled in part by software that can post con-
tent and interact with other accounts programmatically and possibly automatically [1].
While many social bots are benign, malicious bots can deceptively impersonate humans
to manipulate and pollute the information ecosystem. Such malicious bots are involved
with all types of online discussions, especially controversial ones. Studies have iden-
tiﬁed interference of social bots in U.S. elections [2, 3, 4, 5], French elections [6], the
Brexit referendum [7, 8, 3, 9], German elections [10], and the 2017 Catalan referen-
dum [11]. Bots also actively participate in public health debates [12] including those
about vaccines [13, 14], the COVID-19 pandemic [15, 16, 17, 18], and cannabis [19].
Research has also reported on the presence of social bots in discussions about climate
change [20, 21, 22], cryptocurrency [23], and the stock market [24, 25].

Malicious social bots demonstrate various behavioral patterns in their actions. They
may simply generate a large volume of posts to amplify certain narratives [21, 26]
or to manipulate the price of stocks [24, 25] and cryptocurrencies [23]. They can
also disseminate low-credibility information strategically by getting involved in the
early stage of the spreading process and targeting popular users through mentions
and replies [2]. Some bots act as fake followers to inﬂate the popularity of other

1

 
 
 
 
 
 
accounts [27, 28, 29]. In terms of content, malicious bots are found to engage other
accounts with negative and inﬂammatory language [11] or hate speech [30, 17]. In some
cases, bots form dense social networks to boost engagement and popularity metrics and
to amplify each other’s messages [31, 32, 33].

Most existing reports and studies on social bots focus on Twitter, largely because
its data can be easily accessed. Although Twitter strengthened their eﬀorts to contain
malicious actors in recent years,1 deceptive bots remain prevalent and display evolving
tactics to evade detection [34]. This has two implications for researchers. First, charac-
terizing the behavior of and assessing the impact of social bots remains an interesting
research topic [35]. Second, researchers need to properly handle bots in their data since
their presence may distort analyses [12, 36]. It is therefore crucial for researchers to
have access to a reliable tool for detecting social bots.

This practicum aims to provide a tutorial for Botometer, a machine learning tool for
bot detection on Twitter. Although other bot detection tools such as tweetbotornot2
and Bot Sentinel3 exist, we focus on Botometer for several reasons. First, it is well
maintained and has been serving the community for the past seven years without major
outages. It has also been routinely upgraded to stay accurate and relevant. Second,
Botometer is easily accessible through both a web interface and an application pro-
gramming interface (API). Anyone with a Twitter account can use the web version for
free; researchers with Twitter developer accounts can use the API endpoints to ana-
lyze large-scale datasets. The API has a nominal fee for heavy use, which discourages
abuse and partially oﬀsets infrastructure and maintenance costs. Third, Botometer is
quite popular. It handles around a quarter million daily queries—over half a billion in
total since its inception. Finally, Botometer has been extensively validated in the ﬁeld.
Many researchers have applied Botometer in their studies to directly investigate social
bots and their impact [13, 10, 19, 25], or to distinguish human accounts and bot-like
accounts in order to better address their questions of interest [37, 38, 39].

This tutorial is designed for data scientists and computational social scientists who
might not be familiar with Botometer, the machine learning methods behind it, its
programmatic interface, or how to interpret its results. We start with an introduction
to how Botometer works and how users can access it. We then present a case study to
demonstrate Botometer usage. The source code for this case study is shared through
a public repository for readers to replicate this analysis and use it as a template for
their own research. We ﬁnally discuss recommended practice.

2 How Botometer works

Figure 1 presents the timeline and key characteristics of successive Botometer versions
over the years. Since the behaviors of bot and human accounts evolve over time,
version upgrades are necessary for Botometer to stay accurate and relevant. Upgrades
typically included adding new training data and updating model features. The most
recent version also involved major architectural changes. Users of Botometer should be
aware that results from diﬀerent versions are usually not comparable and the format
of input and output might change as well.

For details of early versions such as V2 [40] and V3 [34], readers can refer to the
corresponding papers. This tutorial focuses on V4 [41]. In addition to new training
data and new features, this version introduced a new architecture. We will also brieﬂy

1blog.twitter.com/common-thread/en/topics/stories/2021/the-secret-world-of-good-bots
2An R package for classifying Twitter accounts as bot or not available at github.com/mkearney/

Tweetbotornot

3A platform that classiﬁes and tracks inauthentic accounts and toxic trolls available at botsentinel.com

2

Figure 1: The timeline of Botometer versions.

Table 1: Annotated datasets of human and bot accounts used to train Botometer.

Dataset
varol-icwsm
cresci-17
pronbots
celebrity
vendor-purchased
botometer-feedback
political-bots
gilani-17
cresci-rtbust
cresci-stock
botwiki
midterm-2018
astroturf
kaiser

0

Bots Humans Annotation method
1,495 Human annotation
733
2,764 Various methods
7,049
Spam bots
17,882
5,918 Celebrity accounts
0
1,087
0 Fake followers
139
62
1,090
353
7,102
698
0
505
875

Signs of coordination
Self-declared
7,459 Human annotation
0 Human annotation
499 Politicians + bots

380 Human annotation
0 Human annotation
1,413 Human annotation
340 Human annotation

6,174
0

Ref.
[40]
[45]
[34]
[34]
[34]
[34]
[34]
[46]
[47]
[48]
[42]
[42]
[41]
[49]

cover a recently added model for fast bot detection [42].

2.1 Supervised machine learning for bot detection

Under the hood, Botometer is a supervised machine learning classiﬁer that distin-
guishes bot-like and human-like accounts based on their features (i.e., characteristics).
Unsupervised methods have also been proposed in the literature [43, 44], but they only
allow for the detection of speciﬁc, predeﬁned behaviors. Therefore they are not suitable
to build a general detection tool.

Botometer considers over 1,000 features that can be categorized into six classes:
user proﬁle, friends, network, temporal, content and language, and sentiment [40]. For
example, the user proﬁle category includes features such as the length of the screen
name, whether the account uses the default proﬁle picture and background, the age of
the account, etc. The content and language category consists of features such as the
number of verbs, nouns, and adjectives in the tweets. For a given account, these features
are extracted and encoded as numbers. This way the account can be represented
by a vector of feature numbers, enabling machine learning classiﬁers to process the
information.

3

Year20142015201620172018201920202021V1Original BotometerV2Additional training dataV3Additional training data
New features
Calibrated scores
CAP scoresV4 (ESC)ESC architecture
Additional training data
New featuresLiteData selection technique
Reduced featuresSupervised machine learning algorithms such as Botometer depend on the avail-
ability of training data—accounts labeled as either human or bot. These labels usually
come from human annotation [40], automated methods (e.g., honey pots [50]), or bot-
nets that display suspicious behaviors [51, 44]. A critical issue with existing datasets
is the lack of ground truth. There is no objective, agreed-upon, operational deﬁnition
of social bot. A further complicating factor is the prevalence of accounts that lie in
the gray area between human and bot behavior, where even experienced researchers
cannot easily discriminate. Nevertheless, datasets do include many typical bots; using
the training labels as proxies for ground truth makes it possible to build practically
viable tools.

Botometer-V4 is trained on a variety of datasets shown in Table 1, which are pub-
licly available in a Bot Repository.4 With all training accounts being represented as
feature vectors, a classiﬁer can learn the characteristics of bot and human accounts.
Botometer uses a classiﬁcation model called Random Forest, which consists of many
rules learned from the training data.

To evaluate a Twitter account, Botometer ﬁrst fetches its 200 most recent tweets
and tweets mentioning it from Twitter, extracts its features from the collected data,
and represents this information as a feature vector. Each model rule uses some of the
features and provides a vote on whether an account is more similar to bot or human
accounts in the training data. Based on how many rules vote for the bot or human class,
the model provides a “bot score” between zero and one: a score close to one means the
account is highly automated, while a score near zero means a human is likely handling
the account. Some accounts may demonstrate the characteristics of both humans and
bots. For instance, a bot creator might generate content like a regular user but uses a
script to control many accounts. These cases can be confusing for the classiﬁer, which
would then produce scores around 0.5.

While human accounts tend to behave similarly, diﬀerent types of bots usually have
unique behavioral patterns. Based on this observation, Botometer-V4 uses several
specialized Random Forest classiﬁers: one for each type of bots in the training data
and one for humans. The results of this Ensemble of Specialized Classiﬁers (ESC) are
aggregated to produce a ﬁnal result. More details about the ESC architecture can be
found in the original paper [41]. At the end of the day, the ESC architecture is still a
machine learning classiﬁer, which yields scores between 0 and 1. Diﬀerent from a single
Random Forest, the scores generated by ESC tend to have a bimodal distribution.

It is worth mentioning that the content and language features and sentiment features
are based on English. When a non-English account is passed to Botometer, these
features become meaningless and might aﬀect the classiﬁcation. As a workaround,
Botometer also returns a language-independent score, which is generated without any
language-related features. Users need to be aware of the account language and choose
the most appropriate Botometer score.

2.2 Model accuracy

The accuracy of the model is evaluated through 5-fold cross-validation on the annotated
datasets shown in Table 1. Simply speaking, the classiﬁer is trained on part of the
annotated datasets and tested on the rest to provide a sense of its accuracy. In the
experimental environment, Botometer works really well. V4 has an AUC (area under
the receiver operating characteristic curve) of 0.99, suggesting that the model can
distinguish bot and human accounts in Table 1—as well as accounts in the wild that
resemble those in the training datasets—with very high accuracy.

4botometer.osome.iu.edu/bot-repository

4

However, Botometer is not perfect and may misclassify accounts due to several
factors. For example, the training datasets might have conﬂicts because they were
created by diﬀerent people with diﬀerent standards. In some cases Botometer fails to
capture the features that can help distinguishing diﬀerent accounts. Botometer some-
times struggles with inactive accounts since not enough data is available for evaluation.
The accuracy of the model may further decay when dealing with new accounts diﬀer-
ent from those in the training datasets. These accounts might come from a diﬀerent
context, use diﬀerent languages other than English [52, 53], or show novel behavioral
patterns [45, 34, 54]. These limitations are inevitable for all supervised machine learn-
ing algorithms, and are the reasons why Botometer has to be upgraded routinely.

Some critics exploit these limitations to undermine the entire ﬁeld of study devoted
to social bots. For example, one might select small sets of accounts with large false-
positive error rates to argue that no bot detection tool is valid or that social bots do
not exist at all. These arguments use fallacies such as cherry-picking and strawman in
disingenuous ways. Validation through manual annotations is extremely valuable, espe-
cially when highlighting cases where existing machine learning models perform poorly,
but should be used in constructive ways. New manually-annotated datasets should be
made available, ideally via the public Bot Repository, to support the development of
improved models.

2.3 Results interpretation

Early versions of Botometer returned to users raw scores in the unit interval, produced
by the Random Forest classiﬁers. Although users often treated them as probabilities,
such interpretation is inaccurate. Consider Twitter accounts a and b and their respec-
tive scores 0.7 and 0.3 produced by a Random Forest classiﬁer. We can say that a is
more bot-like than b, but it is inaccurate to say that there is a 70% chance that a is a
bot or that a is 70% bot. Since Botometer-V3, the scores displayed in the web interface
are rescaled to the range 0–5 to discourage inaccurate probabilistic interpretations.

For users who need a probabilistic interpretation of a bot score, the Complete
Automation Probability (CAP) represents the probability that an account with a given
score or greater is automated. CAP scores have also been available since Botometer-
V3. The CAP scores are Bayesian posteriors that reﬂect both the results from the
classiﬁer and prior knowledge of the prevalence of bots on Twitter, so as to balance
false positives with false negatives. For example, suppose an account has a raw bot
score of 0.96/1 (equivalent to 4.8/5 display score on the website) and a CAP score of
90%. This means that 90% of accounts with a raw bot score above 0.96 are labeled
as bots, or, as indicated on the website, 10% of accounts with a bot score above 4.8/5
In other words, if you use a threshold of 0.96 on the raw
are labeled as humans.
bot score (or 4.8 on the display score) to classify accounts as human/bot, you would
wrongly classify 10% of accounts as bots—a false positive rate of 10%. This helps
researchers determine an appropriate threshold based on acceptable false positive and
false negative error rates for a given analysis.

2.4 Fast bot classiﬁcation

When Botometer-V4 was released, a new model called BotometerLite was added to
the Botometer family [42]. BotometerLite was created to enable fast bot detection for
large scale datasets. The speed of bot detection methods is bounded by the platform’s
rate limits. For example, the Twitter API endpoint used by Botometer-V4 to fetch an
account’s most recent 200 tweets and recent mentions from other users has a limit of
43,200 accounts per app key, per day. Many studies using Twitter data have millions

5

of accounts to analyze; with Botometer-V4, this may take weeks or even months.

To achieve scalability, BotometerLite relies only on features extracted from user
metadata, contained in the so-called user object from the Twitter API. The rate limit
for fetching user objects is over 200 times the rate limit that bounds Botometer-V4.
Moreover, each tweet collected from Twitter has an embedded user object. This brings
two extra advantages. First, once tweets are collected, no extra queries to Twitter are
needed for bot detection. Second, the user object embedded in each tweet reﬂects the
user proﬁle at the moment when the tweet is collected. This makes bot detection on
archived historical data possible.

In addition to the improved scalability, BotometerLite employs a novel data se-
lection mechanism to ensure its accuracy and generalizability. Instead of throwing all
training data into the classiﬁer, a subset is selected by optimizing three evaluation met-
rics: cross-validation accuracy on the training data, generalization to holdout datasets,
and consistency with Botometer. This mechanism was inspired by the observation that
some datasets are contradictory to each other. After evaluating the classiﬁers trained
on all possible combinations of candidate training sets, the winning classiﬁer only uses
ﬁve out of eight datasets but performs well in terms of all evaluation metrics.

BotometerLite allows researchers to analyze large-volume streams of accounts in
real time, while the limited training data may involve a compromise in accuracy on
certain bot classes compared to Botometer-V4. In terms of how to choose between the
two endpoints, we still recommend using Botometer-V4 when feasible since it analyzes
more data and produces more detailed results.

3 Botometer interface

Although the machine learning model might seem complicated, the interface of Botome-
ter is designed to be easy to use. Botometer has a website and API endpoints with
similar functionality. The website5 is handy for users who need to quickly check several
accounts. With a Twitter account, users can access the Botometer website from any
web browsers, even on their mobile devices. The website is straightforward to use:
after authorizing Botometer to fetch Twitter data, users just need to type a Twitter
handle of interest and click the “Check user” button.

The Botometer Pro API6 can be more useful for research since it allows to program-
matically check accounts in bulk. The API is hosted by RapidAPI, a platform that
helps developers manage API rate limits and user subscriptions. Using the Botome-
ter API requires keys associated with a Twitter app, which can be obtained through
Twitter’s developer portal.7 One also needs a RapidAPI account and a subscription
to one of the API usage plans.

When querying the API, users are responsible to send the required data (i.e., 200
most recent tweets by the account being checked and tweets mentioning this account)
in a speciﬁed format through HTTPS requests. The Botometer API will process the
data and return the results. While queries can be sent through any programming
language, we recommend using Python and the oﬃcial botometer-python package
that we maintain.8 The package can fetch data from Twitter, format the data, and
query the API on behalf of the user with a few lines of code:

import botometer

5botometer.org
6rapidapi.com/OSoMe/api/botometer-pro
7developer.twitter.com
8github.com/IUNetSci/botometer-python

6

Table 2: Comparison of Botometer-V4 and BotometerLite APIs.

Model
Endpoint
Query payload

Response

Botometer-V4
Check account
User object,
200
most recent tweets,
mentions
scores,
Raw bot
CAP
sub-scores,
scores,
ac-
count information,
etc.
43,200

basic

BotometerLite
Check account in bulk
List of user objects and times-
tamps

BotometerLite scores

∼ 8.6 million

check account

Daily number of ac-
counts allowed*
Corresponding
botometer-python
method(s)
* The values represent the upper bounds based on Twitter’s API rate limit
when using a single app key. The actually numbers depend on other factors
such as internet speed as well.

check accounts from tweets,
check accounts from user ids,
check accounts from screen names

bom = botometer . Botometer (
rapidapi_key =" XYZ " ,
consumer_key =" XYZ " ,
consumer_secret =" XYZ " ,
access_token =" XYZ " ,
a c c e s s_ t o k e n _ s e cr e t =" XYZ "
)

result = bom . check_account (" @yang3kc ")

print ( f " Bot score ={ result [ ’ display_scores ’][ ’ english ’][ ’ overall ’]}/5")
print ( f " CAP score ={ result [ ’ cap ’][ ’ english ’]:.2 f }")

BotometerLite is also available as an endpoint through the Botometer Pro APIs. We
list the the input, output, and limitations of the API endpoints for Botometer-V4 and
BotometerLite side by side in Table 2. We also summarize the common resources for
using Botometer in Table 3 to help the readers navigate these resources.

Note that both Botometer and Twitter APIs have rate limits, meaning that users
can only make a certain number of queries in a given time period. Please check the
respective websites for detailed documentation. Getting familiar with the rate limits
can help researchers better estimate the time needed for their analysis.

4 Case study

Since some readers may not be familiar with programming, querying the API could be
challenging. Moreover, analyzing the results returned by Botometer API is not trivial.
In this section, we provide a simple case study as a demonstration. Diﬀerent ways of
analyzing the data are shown with recommended practice. We share the code for this
case study in a public repository9 so that readers can use it as a template for their own

9github.com/osome-iu/Botometer101

7

Table 3: Common resources for using Botometer.

Resource name
Botometer website

Resource
botometer.org

Botometer Pro API

Botometer-python
package

Botometer
study

case

Bot repository

rapidapi.com/
OSoMe/api/
botometer-pro
github.com/
IUNetSci/
botometer-python
github.com/
osome-iu/
Botometer101
botometer.
osome.iu.edu/
bot-repository

Note
Web interface of Botome-
ter: useful for checking a
small amount of accounts
API of Botometer: use-
ful for checking accounts
in bulk programmatically
Python package to access
Botometer Pro API

Case study using Botome-
ter with source code

Annotated
datasets for Botometer

training

Table 4: Numbers of tweets and unique accounts mentioning diﬀerent cashtags in raw data
and analytical sample.

Raw data

Analytical sample

Cashtag Tweets Unique accounts Tweets Unique accounts
$SHIB
1,111
$FLOKI
860
$AAPL
1,006

1,819
1,893
1,864

1,241
937
1,107

2,000
2,000
2,000

research. Next we outline the data collection and analysis steps implemented in this
software repository.

4.1 Data collection
Let us consider two cryptocurrency cashtags, $FLOKI and $SHIB, and the cashtag of
Apple Inc., $AAPL, and attempt to quantify which is more ampliﬁed by bot-like ac-
counts. A cashtag works like a hashtag but consists of a dollar sign “$” and a stock
or cryptocurrency symbol to help users track related discussions. We use Tweepy,10 a
Python package that helps access the Twitter API, to search tweets containing these
cashtags. For each cashtag, we only collect 2,000 tweets, which are suﬃcient for the
demonstration.

First, let us count the number of unique accounts in each dataset, as shown in
Table 4. The number of unique accounts is much smaller than the number of tweets
in all three datasets, suggesting that some accounts tweeted the same cashtag multiple
times.

The next step is to query the Botometer API for bot analysis. Instead of going
through each tweet and check every user encountered, researchers can keep a record
of accounts already queried to avoid repetition and increase eﬃciency. The Botometer

10tweepy.org

8

Figure 2: Percentage of accounts using each language in the three datasets combined.

API returns rich information about each account. We recommend storing the full
results from Botometer for ﬂexibility.

As mentioned above, Botometer generates an overall score and a language-independent

score. Since the two scores come from diﬀerent classiﬁers, they are not comparable and
should not be mixed together. To decide which one to use, let us calculate the pro-
portion of accounts using each language. We can see in Figure 2 that the majority of
accounts in our raw data tweet in English. Therefore we only include English-speaking
accounts and their tweets in our analytical sample (see Table 4 for summary statistics)
and use the overall bot score.

4.2 Analysis

We plot the bot score distribution for tweets mentioning each cashtag in Figure 3(a).
Here we base our analysis on the raw scores in the unit interval. Since we are interested
in the bot activity level of each cashtag, we use tweets (as opposed to accounts) as the
units of analysis. This means that accounts tweeting the same cashtag multiple times
have a larger contribution.

In all three cases, the distribution has a bimodal pattern, a result of the ESC
architecture of Botometer-V4. We can observe some spikes in all cases, which are
caused by accounts tweeting the same cashtag repeatedly. For example, the spike near
0.89 for $SHIB and $FLOKI comes from a bot-like account that replied the same message
promoting cryptocurrency tokens to a large number of tweets containing the keyword
“NFT”; see the screenshot of the message in Figure 4.

To address our research question, we need to quantify the bot activity level for
each cashtag and compare them. The ﬁrst approach is to compare their bot score
distributions with two-sided Mann–Whitney U tests (see results in Figure 3(c)). The
bot score distributions of $SHIB and $FLOKI are not signiﬁcantly diﬀerent from each
other (p = 0.56), but both of them have a higher bot activity level than $AAPL ($SHIB
vs. $AAPL: p < 0.001; $FLOKI vs. $AAPL: p < 0.001).

The second approach dichotomizes the bot scores and considers the accounts with
scores higher than a threshold as likely bots. Then the proportion of tweets from
likely bots can be calculated and compared. In this approach, a threshold has to be
chosen. In the literature, 0.5 is the most common choice [2, 37, 4]; higher values, such
as 0.7 [38] and 0.8 [13], are also used. One may also consider running the same analysis
with diﬀerent threshold values to test the robustness of the ﬁndings [2].

Here we use both 0.5 and 0.7 as thresholds and show the results in Figure 3(b)
and (d), respectively. We apply two-proportions z-tests to estimate the signiﬁcance

9

English (90.4%)Japanese (2.0%)Unknown (1.8%)Spanish (1.3%)Turkish (1.1%)Arabic (1.0%)Others (2.2%)Figure 3:
(a) Bot score distributions for tweets mentioning diﬀerent cashtags. (b) Per-
centage of tweets posted by likely bots using 0.5 as a threshold. (c) Box plots of the bot
scores for tweets mentioning diﬀerent cashtags. The white lines indicate the median values;
the white dots indicate the mean values. (d) Similar to (b) but using a bot score threshold
of 0.7. Statistical tests are performed for pairs of results in (b–d). Signiﬁcance level is
represented by the stars: ***p ≤ 0.001, **p ≤ 0.01, *p ≤ 0.05, NS= p > 0.05.

Figure 4:
Screenshot of a bot-like account replying to a tweet containing the keyword
“NFT” with a message promoting cryptocurrencies. The same message was replied by this
account to a large number of tweets.

10

(a)(b)(c)(d)NS******NS**************Figure 5: Time series of bot scores of an account from September 2020 to November 2021.
The queries were not made regularly, so the time intervals between consecutive data points
vary.

level of the diﬀerences. When using 0.5 as the threshold, the percentage of tweets
from likely bots that mentioned $SHIB is signiﬁcantly higher than those in the $FLOKI
(p = 0.009) and $AAPL datasets (p < 0.001). The percentage of tweets from likely
bots that mentioned $FLOKI is also signiﬁcantly higher than that in the $AAPL dataset
(p < 0.001). However, when using 0.7 as the threshold, the results change: percentages
of tweets from likely bots in $SHIB and $FLOKI datasets are no longer signiﬁcantly
diﬀerent from each other (p = 0.38); both of them are lower than that in the $AAPL
dataset ($SHIB vs. $AAPL: p < 0.001; $FLOKI vs. $AAPL: p < 0.001).

In other studies, diﬀerent approaches or threshold choices may yield consistent
results. However, they lead to seemingly diﬀerent conclusions in this case. This is
because diﬀerent measures represent diﬀerent properties of the bot score distribution.
If we revisit Figure 3(a), we can see that although the distributions of $SHIB and $FLOKI
scores have more mass in the (0.5, 1] region than that of $AAPL scores, the mass tends
to concentrate around 0.6, while the distribution of $AAPL scores has more mass near 1.
This nuanced diﬀerence causes the contradictory results when using diﬀerent threshold
values.

By reconciling the results from diﬀerent approaches, we can answer our research
question now. It appears that discussions about the cryptocurrencies $SHIB and $FLOKI
show more automated activities than that about $AAPL, but among the accounts tweet-
ing $AAPL, we ﬁnd more highly automated bot-like accounts. Note that the analysis
here is mainly for demonstrating the use of Botometer; the samples of tweets analyzed
are small and not representative of the entire discussion, so the conclusions only reﬂect
the status of the collected data and should not be generalized.

5 Recommended practice

The sections above cover some recommended practice such as being careful when in-
terpreting raw bot scores, being mindful about user language, and being aware of
diﬀerent versions of Botometer. Here we make a few more recommendations to help
avoid common pitfalls.

11

2020 Sep. 1Oct. 1Nov. 1Dec. 12021 Jan. 1Feb. 1Mar. 1Apr. 1May 1Jun. 1Jul. 1Aug. 1Sep. 1Oct. 1Nov. 1Dec. 10.000.250.500.751.00Bot score5.1 Transient nature of Botometer scores

Recall that Botometer uses the 200 most recent tweets by an account and other tweets
mentioning the account for analysis. This means that the results of Botometer change
over time, especially for very active accounts. To demonstrate this, we plot the time
series of the overall bot score of an account in Figure 5. This account posts roughly
16 tweets each week and gets mentioned by others frequently. We can see that the
bot score ﬂuctuates over time. In some other cases, an account might be suspended or
removed after a while, making it impossible to analyze.

Due to the transient nature of Botometer scores, a single bot score only reﬂects the
status of the account at the moment when it is evaluated. Users should be careful when
drawing conclusions based on the bot scores of individual accounts. For researchers, a
common practice is to collect tweets ﬁrst, then perform bot detection later. To reduce
the eﬀect of unavailable accounts and to keep the bot scores relevant, bot analysis
should be conducted right after data collection.

5.2 Evaluating bot score distributions

Whenever possible, we recommend collecting large datasets and use statistical analyses
to evaluate bot activity based on comparisons of score distributions across diﬀerent
groups of accounts. As demonstrated in the case study, bot score distributions can
reveal rich information about the data. Using distributions for analysis also reduces
the uncertainty level of Botometer due to its imperfection and transient nature. Most
importantly, comparing distributions of scores—e.g., for accounts tweeting about a
given topic versus a suitable baseline—allows for statistical tests that are impossible
at the level of individual accounts.

5.3 Validating thresholds

In some analyses, dichotomizing the bot scores based on a threshold is necessary. In
these cases, we recommend validating the choice of threshold. For researchers with the
ability and resources, the ideal approach is to manually annotate a batch of bot and
human accounts in their datasets. Such a preliminary analysis could be used, ﬁrst, to
determine whether Botometer is a helpful tool to evaluate a given scenario. Assuming it
is, one can then vary the threshold and select the value that optimizes some appropriate
metric on the annotated accounts. Depending on the desire to maximize accuracy,
minimize false positive errors, minimize false negative errors, or some combination, one
can use metrics such as accuracy, precision, recall, or F1. When annotating additional
accounts is not feasible, we suggest running multiple analyses using diﬀerent threshold
choices to conﬁrm the robustness of the ﬁndings.

5.4 Using Botometer in a civil way

We have noticed that Botometer has been used to attack others. For example, some
users may call others with whom they disagree “bots” and use the results of Botometer
as justiﬁcation. This is a misuse of Botometer. Users should keep in mind that any
classiﬁer such as Botometer can mislabel individual accounts. Furthermore, even if an
account is automated, it does not mean it is deceptive or malicious. Most importantly,
such name calling is not helpful for creating healthy and informative conversations.

12

References

[1] Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessandro
Flammini. The rise of social bots. Communications of the ACM, 59(7):96–104,
2016.

[2] Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang,
Alessandro Flammini, and Filippo Menczer. The spread of low-credibility con-
tent by social bots. Nature Communications, 9(1):4787, 2018.

[3] Yuriy Gorodnichenko, Tho Pham, and Oleksandr Talavera. Social media, senti-
ment and public opinions: Evidence from #Brexit and #USElection. European
Economic Review, 136:103772, 2021.

[4] Alessandro Bessi and Emilio Ferrara. Social bots distort the 2016 U.S. Presidential

election online discussion. First Monday, 2016.

[5] Emilio Ferrara, Herbert Chang, Emily Chen, Goran Muric, and Jaimin Patel.
Characterizing social media manipulation in the 2020 U.S. presidential election.
First Monday, 2020.

[6] Emilio Ferrara. Disinformation and social bot operations in the run up to the

2017 French presidential election. First Monday, 2017.

[7] Marco Bastos and Dan Mercea. The public accountability of social platforms:
lessons from a study on bots and trolls in the Brexit campaign. Philosophical
Transactions of the Royal Society A: Mathematical, Physical and Engineering Sci-
ences, 376(2128):20180003, 2018.

[8] Marco T. Bastos and Dan Mercea. The Brexit Botnet and User-Generated Hy-

perpartisan News. Social Science Computer Review, 37(1):38–54, 2019.

[9] Andrej Duh, Marjan Slak Rupnik, and Dean Koroˇsak. Collective Behavior of
Social Bots Is Encoded in Their Temporal Twitter Activity. Big Data, 6(2):113–
123, 2018.

[10] Tobias R. Keller and Ulrike Klinger. Social Bots in Election Campaigns: The-
oretical, Empirical, and Methodological Implications. Political Communication,
36(1):171–189, 2019.

[11] Massimo Stella, Emilio Ferrara, and Manlio De Domenico. Bots increase exposure
to negative and inﬂammatory content in online social systems. Proceedings of the
National Academy of Sciences, 115(49):12435–12440, 2018.

[12] Amelia M. Jamison, David A. Broniatowski, and Sandra Crouse Quinn. Malicious
Actors on Twitter: A Guide for Public Health Researchers. American Journal of
Public Health, 109(5):688–692, 2019.

[13] David A. Broniatowski, Amelia M. Jamison, SiHua Qi, Lulwah AlKulaib, Tao
Chen, Adrian Benton, Sandra C. Quinn, and Mark Dredze. Weaponized Health
Communication: Twitter Bots and Russian Trolls Amplify the Vaccine Debate.
American Journal of Public Health, 108(10):1378–1384, 2018.

[14] Xiaoyi Yuan, Ross J. Schuchard, and Andrew T. Crooks. Examining Emergent
Communities and Social Bots Within the Polarized Online Vaccination Debate in
Twitter. Social Media + Society, 5(3):2056305119865465, 2019.

[15] Emilio Ferrara. What types of COVID-19 conspiracies are populated by Twitter

bots? First Monday, 2020.

[16] Wen Shi, Diyi Liu, Jing Yang, Jing Zhang, Sanmei Wen, and Jing Su. Social
Bots’ Sentiment Engagement in Health Emergencies: A Topic-Based Analysis
of the COVID-19 Pandemic Discussions on Twitter.
International Journal of
Environmental Research and Public Health, 17(22):8701, 2020.

13

[17] Joshua Uyheng and Kathleen M. Carley. Bots and online hate during the COVID-
19 pandemic: case studies in the United States and the Philippines. Journal of
Computational Social Science, 3(2):445–468, 2020.

[18] Kai-Cheng Yang, Christopher Torres-Lugo, and Filippo Menczer. Prevalence of
low-credibility information on twitter during the COVID-19 outbreak. In Proceed-
ings of the ICWSM International Workshop on Cyber Social Threats, 2020.

[19] Jon-Patrick Allem, Patricia Escobedo, and Likhit Dharmapuri. Cannabis Surveil-
lance With Twitter Data: Emerging Topics and Social Bots. American Journal
of Public Health, 110(3):357–362, 2020.

[20] Thomas Marlow, Sean Miller, and J. Timmons Roberts. Twitter Discourses on
Climate Change: Exploring Topics and the Presence of Bots. SocArXiv, 2020.

[21] Thomas Marlow, Sean Miller, and J. Timmons Roberts. Bots and online climate
discourses: Twitter discourse on President Trump’s announcement of U.S. with-
drawal from the Paris Agreement. Climate Policy, 21(6):765–777, 2021.

[22] Chang-Feng Chen, Wen Shi, Jing Yang, and Hao-Huan Fu. Social bots’ role in
climate change discussion on Twitter: Measuring standpoints, topics, and inter-
action strategies. Advances in Climate Change Research, 2021.

[23] Leonardo Nizzoli, Serena Tardelli, Marco Avvenuti, Stefano Cresci, Maurizio
Tesconi, and Emilio Ferrara. Charting the Landscape of Online Cryptocurrency
Manipulation. IEEE Access, 8:113230–113245, 2020.

[24] Stefano Cresci, Fabrizio Lillo, Daniele Regoli, Serena Tardelli, and Maurizio
Tesconi. Cashtag Piggybacking: Uncovering Spam and Bot Activity in Stock
Microblogs on Twitter. ACM Transactions on the Web, 13(2):11:1–11:27, April
2019.

[25] Rui Fan, Oleksandr Talavera, and Vu Tran. Social media bots and stock markets.

European Financial Management, 26(3):753–777, 2020.

[26] Franziska B Keller, David Schoch, Sebastian Stier, and JungHwan Yang. Political
astroturﬁng on twitter: How to coordinate a disinformation campaign. Political
Communication, 37(2):256–280, 2020.

[27] Nick Bilton. Social Media Bots Oﬀer Phony Friends and Real Proﬁt. The New

York Times, 2014.

[28] Nicholas Confessore, Gabriel J. X. Dance, Rich Harris, and Mark Hansen. The

Follower Factory. The New York Times, 2018.

[29] Onur Varol and Ismail Uluturk. Journalists on twitter: self-branding, audiences,
and involvement of bots. Journal of Computational Social Science, 3(1):83–101,
2020.

[30] Nuha Albadi, Maram Kurdi, and Shivakant Mishra. Hateful People or Hateful
Bots? Detection and Characterization of Bots Spreading Religious Hatred in
Arabic Social Media. Proceedings of the ACM on Human-Computer Interaction,
3(CSCW):61:1–61:25, 2019.

[31] Guido Caldarelli, Rocco De Nicola, Fabio Del Vigna, Marinella Petrocchi, and
Fabio Saracco. The role of bot squads in the political propaganda on Twitter.
Communications Physics, 3(1):1–15, 2020.

[32] Christopher Torres-Lugo, Kai-Cheng Yang, and Filippo Menczer. The manufac-
ture of political echo chambers by follow train abuse on twitter. In Proceedings of
the International AAAI Conference on Web and Social Media, 2022.

[33] Wen Chen, Diogo Pacheco, Kai-Cheng Yang, and Filippo Menczer. Neutral bots
probe political bias on social media. Nature Communications, 12:5580, 2021.

14

[34] Kai-Cheng Yang, Onur Varol, Clayton A Davis, Emilio Ferrara, Alessandro Flam-
mini, and Filippo Menczer. Arming the public with artiﬁcial intelligence to counter
social bots. Human Behavior and Emerging Technologies, 1(1):48–61, 2019.

[35] Iyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh Bongard, Jean-Fran¸cois
Bonnefon, Cynthia Breazeal, Jacob W. Crandall, Nicholas A. Christakis, Iain D.
Couzin, Matthew O. Jackson, Nicholas R. Jennings, Ece Kamar, Isabel M.
Kloumann, Hugo Larochelle, David Lazer, Richard McElreath, Alan Mislove,
David C. Parkes, Alex ’Sandy’ Pentland, Margaret E. Roberts, Azim Shariﬀ,
Joshua B. Tenenbaum, and Michael Wellman. Machine behaviour. Nature,
568(7753), 2019.

[36] Heidi Ledford. Social scientists battle bots to glean insights from online chatter.

Nature, 578(7793):17–17, 2020.

[37] Soroush Vosoughi, Deb Roy, and Sinan Aral. The spread of true and false news

online. Science, 359(6380):1146–1151, 2018.

[38] Nir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and
David Lazer. Fake news on Twitter during the 2016 U.S. presidential election.
Science, 363(6425):374–378, 2019.

[39] Alexandre Bovet and Hern´an A. Makse. Inﬂuence of fake news in Twitter during

the 2016 US presidential election. Nature Communications, 10(1):7, 2019.

[40] Onur Varol, Emilio Ferrara, Clayton A Davis, Filippo Menczer, and Alessandro
Flammini. Online human-bot interactions: Detection, estimation, and character-
ization. In Proceedings of the International AAAI Conference on Web and Social
Media, 2017.

[41] Mohsen Sayyadiharikandeh, Onur Varol, Kai-Cheng Yang, Alessandro Flammini,
and Filippo Menczer. Detection of novel social bots by ensembles of specialized
classiﬁers. In Proceedings of the 29th ACM International Conference on Informa-
tion & Knowledge Management, pages 2725–2732, 2020.

[42] Kai-Cheng Yang, Onur Varol, Pik-Mai Hui, and Filippo Menczer. Scalable and
Generalizable Social Bot Detection through Data Selection. Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, 34(01):1096–1103, 2020.

[43] Nikan Chavoshi, Hossein Hamooni, and Abdullah Mueen. Debot: Twitter bot
detection via warped correlation. In Proceedings of the 2016 IEEE International
Conference on Data Mining, pages 817–822, 2016.

[44] Juan Echeverria and Shi Zhou. Discovery, retrieval, and analysis of the ‘star
In Proceedings of the 2017 IEEE/ACM International
wars’ botnet in twitter.
Conference on Advances in Social Networks Analysis and Mining (ASONAM),
pages 1–8, 2017.

[45] Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, and
Maurizio Tesconi. The paradigm-shift of social spambots: Evidence, theories, and
tools for the arms race. In Proceedings of the 26th International Conference on
World Wide Web Companion, pages 963–972, 2017.

[46] Zafar Gilani, Reza Farahbakhsh, Gareth Tyson, Liang Wang, and Jon Crowcroft.
Of bots and humans (on Twitter). In Proceedings of the International Conference
on Advances in Social Networks Analysis and Mining, pages 349–354. ACM, 2017.

[47] Michele Mazza, Stefano Cresci, Marco Avvenuti, Walter Quattrociocchi, and Mau-
rizio Tesconi. RTbust: Exploiting Temporal Patterns for Botnet Detection on
Twitter. In Proceedings of the 10th ACM Conference on Web Science, pages 183–
192, 2019.

15

[48] Stefano Cresci, Fabrizio Lillo, Daniele Regoli, Serena Tardelli, and Maurizio
Tesconi. $FAKE: Evidence of Spam and Bot Activity in Stock Microblogs on
Twitter. Proceedings of the International AAAI Conference on Web and Social
Media, 12(1), 2018.

[49] Adrian Rauchﬂeisch and Jonas Kaiser. Dataset for paper: The false positive
problem of automatic bot detection in social science research. Harvard Dataverse,
2020. https://doi.org/10.7910/DVN/XVCKRS/P2ZKRU.

[50] Kyumin Lee, Brian David Eoﬀ, and James Caverlee. Seven Months with the
Devils: A Long-Term Study of Content Polluters on Twitter. In Proceedings of
the International AAAI Conference on Web and Social Media, 2011.

[51] Juan Echeverria and Shi Zhou. Discovery of the twitter bursty botnet. arXiv

preprint arXiv:1709.06740, 2017.

[52] Adrian Rauchﬂeisch and Jonas Kaiser. The False positive problem of automatic
bot detection in social science research. PLOS ONE, 15(10):e0241045, 2020.

[53] Franziska Martini, Paul Samula, Tobias R Keller, and Ulrike Klinger. Bot, or not?
Comparing three methods for detecting social bots in ﬁve political discourses. Big
Data & Society, 8(2):20539517211033566, 2021.

[54] Ilias Dimitriadis, Konstantinos Georgiou, and Athena Vakali. Social botomics: A
systematic ensemble ml approach for explainable and multi-class bot detection.
Applied Sciences, 11(21):9857, 2021.

16

