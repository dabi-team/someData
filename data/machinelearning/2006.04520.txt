Maximizing Cumulative User Engagement in Sequential
Recommendation: An Online Optimization Perspective

Yifei Zhao
MIND, Alibaba Group
andy.zyf@alibaba-inc.com

Yu-Hang Zhou
MIND, Alibaba Group
zyh174606@alibaba-inc.com

Mingdong Ou
MIND, Alibaba Group
mingdong.omd@alibaba-inc.com

Huan Xu
MIND, Alibaba Group
huan.xu@alibaba-inc.com

Nan Li
STCA, Microsoft
alex.nan@microsoft.com

0
2
0
2

n
u
J

2

]

R

I
.
s
c
[

1
v
0
2
5
4
0
.
6
0
0
2
:
v
i
X
r
a

ABSTRACT
To maximize cumulative user engagement (e.g. cumulative clicks)
in sequential recommendation, it is often needed to tradeoff two
potentially conflicting objectives, that is, pursuing higher immedi-
ate user engagement (e.g., click-through rate) and encouraging user
browsing (i.e., more items exposured). Existing works often study
these two tasks separately, thus tend to result in sub-optimal results.
In this paper, we study this problem from an online optimization
perspective, and propose a flexible and practical framework to ex-
plicitly tradeoff longer user browsing length and high immediate
user engagement. Specifically, by considering items as actions,
user’s requests as states and user leaving as an absorbing state, we
formulate each user’s behavior as a personalized Markov decision
process (MDP), and the problem of maximizing cumulative user
engagement is reduced to a stochastic shortest path (SSP) problem.
Meanwhile, with immediate user engagement and quit probability
estimation, it is shown that the SSP problem can be efficiently solved
via dynamic programming. Experiments on real-world datasets
demonstrate the effectiveness of the proposed approach. Moreover,
this approach is deployed at a large E-commerce platform, achieved
over 7% improvement of cumulative clicks.

CCS CONCEPTS
•Information systems → Social recommendation; Personal-
ization; Recommender systems; •Computing methodologies
→ Sequential decision making; Markov decision processes; •Mathematics
of computing → Markov processes; •Theory of computation →
Shortest paths; Markov decision processes;

KEYWORDS
Sequential Recommendation, Cumulative User Engagement, Sto-
chastic Shortest Path, Markov Decision Process

ACM Reference format:
Yifei Zhao, Yu-Hang Zhou, Mingdong Ou, Huan Xu, and Nan Li. 2020.
Maximizing Cumulative User Engagement in Sequential Recommendation:

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’20, Virtual Event, CA, USA
© 2020 ACM. 978-1-4503-7998-4/20/08. . . $15.00
DOI: 10.1145/3394486.3403329

Figure 1: Sequential Recommendation Procedure

An Online Optimization Perspective. In Proceedings of Proceedings of the
26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
Virtual Event, CA, USA, August 23–27, 2020 (KDD ’20), 9 pages.
DOI: 10.1145/3394486.3403329

1 INTRODUCTION
In recent years, sequential recommendation has drawn attention
due to its wide application in various domains [8, 11, 12, 16, 22],
such as E-commerce, social media, digital entertainment and so on.
1, Tik
It even props up popular stand-alone products like Toutiao
Tok

2 and so on.
Different from traditional recommender systems where it is often
assumed that the number of recommended items is fixed, the most
important feature of sequential recommendation is that it iteratively
recommend items until the user quits (as depicted in Figure 1),
which means that users can browse endless items if they want. Its
goal is to maximize cumulative user engagement in each session,
such as cumulative clicks, cumulative dwell time, etc. To this end,
the recommender systems need to simultaneously achieve two
objectives:

a) Attracting users to have a longer session such that more

items can be browsed;

b) Capturing user interests such that higher immediate user

engagement can be achieved.

In traditional recommender systems, since the number of rec-
ommended items is fixed, most efforts are spent on improving
immediate user engagement, which is often measured by click-
through rate, etc. However, when such a strategy is adopted for
sequential recommendation, it tends to result in sub-optimal cumu-
lative user engagement, due to the limited number of browsed items.
Moreover, due to their inherent conflicts, it is not a trivial task to
achieve a longer session and higher immediate user engagement

1https://www.toutiao.com/
2https://www.tiktok.com/

BrowseUserYesNoQuitRecommendationAgentActionShownGenerateItemsItem1Item2Item3… 
 
 
 
 
 
KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Zhao and Zhou, et al.

simultaneously (which can be demonstrated in the experiments).
For example, to achieve a longer session, it is generally needed to
explore diverse recommendation results; almost for sure, this will
sacrifice immediate user engagement. Therefore, how to tradeoff
a longer session and higher immediate user engagement becomes
critical to achieve higher cumulative user engagement, and this is
essentially the key problem of sequential recommendation.

Generally speaking, existing works on sequential recommenda-
tion fall into two groups. The first group of works try to leverage
sequential information (e.g., users’ interaction behavior) to estimate
the probability of user engagement (e.g., click-through rate) more
accurately [8, 11, 12, 16, 22], for example, by using recurrent neural
network or its variants [8, 11, 25]. By exploiting the sequential
behavior pattern, these methods focus on capturing user interests
more accurately, but do not consider to extend session length thus
may lead to sub-optimal results. Based on the observation that di-
verse results tend to attract users to browse more items, the second
group of methods explicitly considers the diversity of recommenda-
tion results [7, 9, 23]. However, the relationship between diversity
and user browsing length is mostly empirical; thus it is not so
well-founded to optimize diversity directly, especially when it is
still a fact that there are no well-accepted diversity measures so
far. Therefore, it is still a challenge to optimize cumulative user
engagement in the sequential recommendation.

In this paper, we consider the problem of maximizing cumu-
lative user engagement in sequential recommendation from an
online optimization perspective, and propose a flexible and prac-
tical framework to solve it. Specifically, by considering different
items as different actions, user’s different requests as states and user
leaving as an absorbing state, we consider user browsing process
in the Markov decision process (MDP) framework, and as a conse-
quence the problem of maximizing cumulative user engagement
can be reduced to a stochastic shortest path (SSP) problem. To make
this framework practical, at each state (except absorbing state), we
need to know two probabilities for each possible action, i.e., the
probability of achieving user engagement (e.g., click) and the prob-
ability of transitioning to the absorbing state which means that the
user quits the browsing process. Obviously, the problem of estimat-
ing the probability of user engagement has been well studied, and
many existing machine learning methods can be employed. Mean-
while, we propose a multi-instance learning method to estimate
the probability of transitioning to the absorbing state (i.e., user
quit model). With this framework and corresponding probabilities
effectively estimated, the SSP problem can be solved efficiently via
dynamic programming. Experiments on real-world datasets and an
online E-commerce platform demonstrate the effectiveness of the
proposed approach.

In summary, our main contributions are listed as below:

• We solve the problem of maximizing cumulative user en-
gagement within an online optimization framework. Within
this framework, we explicitly tradeoff longer user browsing
session and high immediate user engagement to maximize
cumulative user engagement in sequential recommenda-
tion.

• Within the online optimization framework, we propose a
practical approach which is efficient and easy to implement

in real-world applications. In this approach, existing works
on user engagement estimation can be exploited, a new
multi-instance learning method is used for user quit model
estimation, and the corresponding optimization problem
can be efficiently solved via dynamic programming.

• Experiments on real-world dataset demonstrate the effec-
tiveness of the proposed approach, and detailed analysis
shows the correlation between user browsing and imme-
diate user engagement. Moreover, the proposed approach
has been deployed on a large E-commerce platform, and
achieve over 7% improvement on cumulative clicks.

The rest of the paper is organized as follows. In Section 2, we
discuss some related works. Problem statement is given in Section 3.
Section 4 provides the framework named MDP-SSP and the related
algorithms. Experiments are carried out in Section 5, and finally
we give a conclusion in Section 6.

2 RELATED WORK
2.1 Sequential Recommendation
In recent years, conventional recommendation methods, e.g., RNN
models [11, 15, 16], memory networks with attention [8, 12, 16],
etc., are applied in sequential recommendation scenarios widely.
In order to find the next item that should be recommended, RNN
models capture the user’s sequence pattens by utilizing historic se-
quential information. One could also train a memory network and
introduce the attention mechanism to weighting some sequential
elements. [11, 22] show that these methods significantly outper-
form the classic ones which ignored the sequential information.
Essentially, they are still estimating the immediate user engage-
ment (i.e. click-through rate) on the next item, without considering
quit probability. Therefore further improvements are necessary to
maximize cumulative user engagement.

2.2 MDP and SSP
Stochastic Shortest Path (SSP) is a stochastic version of the clas-
sical shortest path problem: for each node of a graph, we must
choose a probability distribution over the set of successor nodes
so as to reach a certain destination node with minimum expected
cost [4, 21]. SSP problem is essentially a Markov Decision Process
(MDP) problem, with an assumption that there is an absorbing state
and a proper strategy. Some variants of Dynamic Programming can
be adopted to solve the problem [3, 5, 17, 24]. Real Time Dynamic
Program (RTDP) is an algorithm for solving non-deterministic plan-
ning problems with full observability, which can be understood
either as an heuristic search or as a dynamic programming (DP)
procedure [3]. Labeled RTDP [5] is a variant of RTDP, and the key
idea is to label a state as solved if the state and its successors have
converged, and the solved states will not be updated further.

2.3 Multi-Instance Learning
In Multi-instance learning (MIL) tasks, each example is represented
by a bag of instances [10]. A bag is positive if it contains at least
one positive instance, and negative otherwise. The approaches for
MIL can fall into three paradigms according to [1]: the instance-
space paradigm, the bag-space paradigm and the embedded-space

Maximizing Cumulative User Engagement

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Figure 2: MDP Finite-State Machine

paradigm. For our sequential recommendation setting, the need
of modeling the transiting probability is in accordance with the
instance-space paradigm. Several SVM based methods are proposed
in instance-level MIL tasks [2, 6, 19, 26]. MI-SVM is a variant of
SVM-like MIL approaches, the main idea is that it forces the instance
farthest to the decision hyperplane (with the largest margin) to be
positive in each iteration.

3 PROBLEM STATEMENT
We model each browsing process as a personalized Markov Deci-
sion Process (MDP) including an absorbing state, and consider the
problem of maximizing cumulative user engagement as a stochastic
shortest path (SSP) problem.

3.1 Personalized MDP Model
The MDP consists of a tuple with four elements (S, A, R, P):

• State space S : S = {s1, s2, s3, ..., st , ..., sT , sA}. Here
we take each step in the recommendation sequence as an
individual state and define st = t, where t is the step index.
Since only one item is shown to the user in each step, t is
also the sequence number of the browsed items. T is the
upper limit of the browsing session length, which is large
enough for the recommendation senarioes. sA is defined
as the absorbing state meaning that the user has left.
• Action space A : A = {1, 2, 3, ..., K }. Action space A
contains all candidates that could be recommended in the
present session.

• Reward R : R ∈ R(T +1)×K . Denote s as a state in S, and a
as an action in A, and then Rs,a is the reward after taking
action a in state s. Specifically, Rst ,at
is the immediate user
engagement (e.g., click-through rate) in the t-th step.
• Transition probability P : P ∈ R(T +1)×K ×(T +1), and
Ps,a,s (cid:48) ∈ [0, 1] is the probability of transiting from state s
to state s (cid:48) after taking action a.

Since the states in S are sequential, we introduce a regulation
on P that from all states except sT and sA, users can only transit
to the next state (go on browsing) or jump into the absorbing state
(quit). Moreover, from the last browsing step, users could only be
admitted to jump into absorbing state. Formally, we have
(cid:26) Psi,ai,si +1 + Psi,ai,sA
PsT ,at ,sA

= 1, i < T
= 1, i = T

(1)

The finite-state machine of the procedure is shown as Figure 2.
Furthermore, it should be emphasized that the proposed MDP model
is personalized and we will infer a new MDP model for each online
session. An efficient algorithm for generating MDP models will be
presented later.

(4)

(5)

3.2 SSP Problem
Based on the MDP model, the optimization of cumulative rewards
in a sequential recommendation can be formally formulated as a
SSP problem: Given a MDP, the objective is to find a policy π ∗:
S → A, which can help us to plan a path with maximal cumulative
rewards, i.e.

π ∗ = arg max

π

E(

Rst ,at ) ,

(2)

τ
(cid:213)

t =1

where τ is the actual browse length. The distribution of τ can be
derived as

P(τ ≥ t) = (cid:214)
i <t
Thus the expected cumulative rewards in Equation (2) can be repre-
sented as

Psi,ai,si +1 .

(3)

E(

τ
(cid:213)

t =1

Rst ,at ) = (cid:213)

Rst ,at

P(τ ≥ t) ,

t ≤T

Finally, by introducing Equation (1) into Equation (4), we have

E(

τ
(cid:213)

t =1

Rst ,at ) =

T
(cid:213)

t =1

Rst ,at ×

(cid:214)

(1 − Psi,ai,sA ) .

i <t

3.2.1 Remark 1. Maximize Equation (5) is simultaneously op-
timizing two points mentioned in Introduction: 1) user browse
length, i.e. τ , and 2) immediate user engagement, i.e. Rst ,at

According to the formulation, we should first estimate Rst ,at
and Psi,ai,sA
in Equation (5), which is essentially generating a per-
sonalized MDP model. Then we optimize a policy by maximizing
Equation (5), which could be used to plan a recommendation se-
quence [a1, · · · , aT ] (or called Path in SSP) to the corresponding
user.

.

4 THE PROPOSED APPROACH
In this section, we first propose an online optimization framework
named MDP-SSP considering browsing session length and imme-
diate user engagement simultaneously and maximizing the cumu-
lative user engagement directly. Then the related algorithms are
presented detailedly.

4.1 MDP-SSP Framework
In order to maximize the expected cumulative rewards, as men-
tioned previously, we should learn a MDP generator from which
the personalized MDP model can be generated online, and then
plan the recommendation sequence with the personalized MDP
model. Therefore, the proposed MDP-SSP framework consists of
two parts: an offline MDP Generator and an online SSP Planner,
which is shown in Figure 3.

4.1.1 MDP Generator. is designed to generate personalized MDPs
for each online sessions. There are two submodules in this part:
Model Worker and Calibration Worker. Model Worker is used to
learn a model from offline historic data, aiming to provide neces-
sary elements of the personalized MDP. Specifically, the reward
function Rst ,at
in Equation (5) are
could be an immediate user engagement, e.g.
needed. Here Rst ,at
immediate click, thus Model Worker contains the corresponding
is
estimation model, e.g. click model. In the meanwhile, Psi,ai,sA

and the quit probability Psi,ai,sA

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Zhao and Zhou, et al.

Algorithm 1 User Quit Model
Require:

Historic browse session set: H ={(B

1: H is converted to H0 by NSK [13], and a initial SV M0 is ob-

+
1 , B

+
2 , …,B

+
i

,…,B−

l eave

)}

with maximum value according to SV Mt −1
+
i, j

+
it,max

, · · · , B−

)

l eave

tained
2: t = 1
3: for all B t ∈ H do
4:

for all Bi ∈ B t do

5:

6:

7:

8:

+
i, j
= B

Select B
+
B
it,max
end for
B t = (· · · , B

9: end for
10: Ht = {B t }
11: Train SV Mt based on Ht
12: t = t + 1
13: repeat
line 3-12
14:
15: until Ht = Ht −1
16: return SV Mt

in instance level. In order to cope with this problem, we introduce
MI-SVM [2] to help us train an instance level model with the bag
level data, which is a novel application of MIL to recommendation
to the best of our knowledge. The process for quit model training
is shown in Algorithm 1.

4.2.3 Model Calibration. In the industrial recommendation sys-
tem, ranking scores provided by the click model and quit model are
and the transition probability
not equivalent to the reward Rst ,at
of MDPs. Thus it is necessary to calibrate the model out-
Psi,ai,sA
puts to real probabilities. Readers interested in this topic may go
to [14, 18, 20] for details. In this paper, denoting the predicted score
as f (Bi, j ), the real probability value can be represented as follow:

P(y = 1|Bi, j ) =

1
1 + exp(A ∗ f (Bi, j ) + B)

,

(6)

where A and B are two scalar parameters can be learned from
historic data.

4.3 Online SSP Planner Algorithm
Based on the MDP Generator discussed in the last subsection, we
formally introduce SSP Planner, which consists of MDP Producer
and SSP Solver.

4.3.1 MDP Producer. When a new session comes, the MDP Pro-
ducer receives online information of user and items from server, and
then feeds them into the generators derived from MDP Generator.
After that, the reward and transition probability can be obtained
and a personalized MDP is produced in real time. It’s worth noting
that, the information about how many items the user has browsed,
how many times the item’s category have been shown to the user
and clicked by the user, should be considered. These interactive
features play an important role in causing the user go on browse
or quit intuitively.

Figure 3: MDP-SSP Framework

related to a quit model which determines the browse session length
and is an important component of Model Worker.

Moreover, since the efficiency of SSP planning depends on the
accuracy of the generated MDP model, we introduce an additional
Calibration Worker to calibrate the ranking scores obtained from
the learned model to the real value [14, 18, 20].

4.1.2

SSP Planner. plans a shortest path (with maximal cumula-
tive rewards) consisting of sequential recommended items. It also
contains two submodules: MDP Producer and SSP Solver. Based
on the generator learned by the offline MDP Generator algorithm,
MDP Producer generates a personalized MDP for the user of present
session. Then SSP Solver will plan an optimal path based on the
personalized MDP to the user.

and the quit probability Psi,ai,sA

4.2 Offline MDP Generator Algorithm
In this section, we present an offline algorithm to learn the re-
ward function Rst ,at
, which are
required to generate online personalized MDPs. We will see that
is more critical and difficult. In
the problem of modeling Psi,ai,sA
practice, the historic data we obtain is often an item set containing
the items seen by the user until the end of a session, which would
make the users quit or go on browsing. However, it is hard to know
which item in the item set is exactly the chief cause. In order to
estimate the quit probability for each item, we alternatively adopt
the Multi-Instance Learning (MIL) framework by taking item set as
baд and item as instance. Detailedly, if the item set causes a quit,
the user dislikes all the items in this set; if the item set causes a
continues browse, at least one item in the item set is accepted by
the user, which is consistent with the MIL setting.

4.2.1 Remark 2. The standard MIL assumption states that all
negative bags contain only negative instances, and that positive
bags contain at least one positive instance.

By utilizing some classical MIL techniques, we can obtain the

following user quit model.

4.2.2 User Quit Model. Based on the users’ browse history, we
can get sequences consisted with bags Bi , and one may verify that
only the last bag in a browse session cannot keep the users going on
browsing. We assume that the bag which can keep the user is a pos-
, and the last one is the negative bag written
itive bag, written as B
+
as B−
). Our
1 , ..., B
job is to construct a model to predict the quit probability for each
new instance B∗, j . However, there exists a gap that the training
labels we have are in bag level, while the predictions we need are

, so a browse session is B = (B

+
i , ..., B−

l eave

l eave

+
i

Maximizing Cumulative User Engagement

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

4.3.2

SSP Solver. From MDP Producer we can get a personal-
ized MDP for the present session, and the next job is to find a path
[a1, · · · , aT ] with maximal cumulative rewards. Except absorb-
ing state, the corresponding MDP has T states, and then optimal
state value function can be addressed with dynamic programing
in T -steps interaction. Furthermore, it is easy to verify that the
transition matrix of our specifically designed MDP preserves an
upper triangular structure, shown as Equation (7).

Algorithm 2 SSP Solver
Require:

User Request, MDP Generator

1: Generating a personalized MDP for the current user
2: Initialize a vector Path with length T
3: Obtain an optimal policy, i.e., π ∗(sT ) = aT according to Equa-

tion (9)

4: Obtain an optimal state value V ∗(sT ) according to Equation (10)

(7)

5: Update Path[T ] = aT
6: for t=T-1, …, 2, 1 do
7:

0
0
· · ·
· · ·
· · ·

Ps1,s2
0
· · ·
· · ·
· · ·

0
Ps2,s3
· · ·
· · ·
· · ·

0
0
Psi,si +1
· · ·
· · ·

· · ·
· · ·
· · ·
· · ·
· · ·

Ps1,sA
Ps2,sA
Psi,sA
PsT ,sA
· · ·























Based on the special structured transition matrix, it is easy to find
that the latter state value function will not change when we update
the current state value function. Therefore the backwards induction
could be adopted. One may start from the absorbing state, and
iteratively obtain the optimal policy as well as the related optimal
state value function. We formally summarize this procedures as
follow:

V ∗(sA) = 0 .

Further more, when i = T , we have
π ∗(sT ) = arg max
{RsT ,aT
aT
= arg max

V ∗(sT ) = max
aT

when i < T , we have

{RsT ,aT } ,

aT
{RsT ,aT } ,

+ PsT ,aT ,sAV ∗(sA)}

π ∗(st ) = arg max
at

{Rst ,at

+ Pst ,at ,st +1V ∗(st +1)

+ Pst ,at ,sAV ∗(sA)}

= arg max

{Rst ,at

+ Pst ,at ,st +1V ∗(st +1)} ,

V ∗(st ) = max
at

at
{Rst ,at

+ Pst ,at ,st +1V ∗(st +1)} .

(8)

(9)

(10)

(11)

(12)

Based on the Equations (8)-(12), we can plan an optimal path
[a1, · · · , aT ]. The optimization procedure is shown in Algorithm 2.
We can see that the whole planning procedure is quite simple and
clear, which benefits the online application of the proposed method.
Specifically, assuming there are K candidates, the complexity of
SSP is O(T K) .

5 EXPERIMENTS
The experiments are conducted on a large E-commerce platform.
We first analyze the characteristics of data which demonstrates the
necessity of applying SSP, and then evaluate SSP offline and online.

5.1 Data Set
Dataset 1: This dataset is for MDP Generator. It consists of 15
days historic data of user item interactions, based on which we
may learn models for predicting the click-through rate and quit
probability of any user item pair.
Dataset 2: This dataset is for SSP offline evaluation. We collect the
active users and their corresponding browse sessions, and discard
those that are inactive or excessive active. The sampling is according
to the criterion that whether the browse session length is between
50 items and 100 items. Finally, we get 1000 users and corresponding

8:

Obtain an optimal policy, i.e., π ∗(st ) = at according to Equa-
tion (11)
Obtain an optimal state value V ∗(st ) according to Equa-
tion (12)
Update Path[t] = at

9:
10: end for
11: return Path = [a1, · · · , aT ]

browse sessions. The average length of the browse sessions is 57.
Dataset 3: This dataset is for SSP online evaluation. It is actually
online environment, and has about ten millions of users and a
hundred millions of items each day.

Various strategies (including SSP) will be deployed to rerank
the personalized candidate items for each user in Dataset 2 and
Dataset 3, to validate their effect on maximizing cumulative user
engagement. Before that we should first verify the datasets are in
accordance with the following characteristics:

• Discrimination: Different items should provide different
quit probabilities, and they should have a significant dis-
crimination. Otherwise quit probability is not necessary to
be considered when make recommendations.

• Weakly related: The quit probability of an item for a user
should be weakly related with click-through rate. Other-
wise SSP and Greedy will be the same.

5.2 Evaluation Measures
In the experiment, we consider the cumulative clicks as cumulative
user engagement. Moreover, we name cumulative clicks as IPV
for short, which means Item Page View and is commonly used in
industry. Browse length(BL, for short) is also a measurement since
IPV can be maximized through making users browse more items.
In offline evaluation, assuming that the recommended sequence

length is T , with Equation (1)-(5) we have

T
(cid:213)

t =1
T
(cid:213)

IPV =

BL =

Rst ,at ×

(cid:214)

(1 − Psi,ai,sA ) ,

i <t

(cid:214)

(1 − Psi,ai,A) .

t =1

i <t

(13)

(14)

Furthermore, define the CT R of the recommended sequence as
CT R = IPV
BL

.

(15)

In online evaluation, IPV can be counted according to the actual

online situation, follows that

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Zhao and Zhou, et al.

Table 1: CTR Model and Browse Model

Table 2: Quit Model Comparison

Model

AUC

CTR Model
Quit Model

0.7194
0.8376

IPV =

τ
(cid:213)

t =1

ct ,

(16)

where ct ∈ {0, 1} indicates the click behavior in step t, and τ is the
browse length, i.e. BL = τ .

5.3 Compared Strategies

• Greedy: The key difference between our methods and tradi-
tional methods is that: we take user’s quit probability into
consideration and plan a path by directly maximizing IPV ,
while most other methods try hard to estimate each step
as exact as possible. However when plan-
reward Rst ,at
ning they just rank the items greedily according to Rst ,at
,
ignoring that Psi,ai,sA
is also crucial to IPV . Greedy is the
first compared strategy in which quit probability Psi,ai,sA
cannot be involved. Assuming that there are K candidates
and the length of planning path is T , the complexity is
O(T K).

• Beam Search: It is a search algorithm that balances per-
formance and consumption. Its purpose is to decode rel-
atively optimal paths in sequences.
It is chosen as the
compared strategy because the quit probability Psi,ai,sA
can be involved. We calculate beam path score according
to Equation (13), so that Beam Search applied here directly
optimize IPV . Assuming that there are K candidates and
the length of planning path is T , the complexity is O(ST K),
where S is beam size.

5.4 MDP Generator Learning
We first describe MDP Generator learning in section 5.4.1 and
section5.4.2, with which we verify the characteristics of datasets in
section 5.4.3 and section5.4.4.

5.4.1 Model Learning. In model learning, we take full use of
user attributes and item attributes. Further more, we add interactive
features, for example how many times the item’s category have
been shown to the user and clicked by the user, which intuitively
play a important role in making the user go on browse. Area Under
the Curve (AUC)3, which is a frequently-used metric in industry
and research, is adopted to measure the learned model, and the
result is shown in Table 1.

Here we state briefly about Quit Model testing method. As we
indeed do not know which item makes the user go on browsing
in practice, thus AUC cannot be directly calculated on instance
level. It is more rational to calculate AUC in bag level with instance
prediction, as we can assume that the bag is positive if it contains at
least one positive instance, the bag is negative if all the instances are
negative.

Model

QuitModelno M I L
Quit Model

AUC

0.8259
0.8376

Table 3: Model Calibration

RMSE Before After

CTR
Quit

0.0957
0.6046

0.0179
0.0077

Figure 4: Calibration of Click Model.

Furthermore, we take a comparison experiment on Quit Model
to show the necessary of adopting MIL. As bag labels are known,
the most intuitive idea is that using the bag’s label to represent the
instance’s label. Based on this idea, we obtain a QuitModelno M I L,
and AUC is calculated also in bag level. The results are shown in
Table 2, from which we can see adopting MIL gives a improvement
for Quit Model learning.

5.4.2 Model Calibration. Calibration tries to map the ranking
scores obtained from models to real value. It is very important here
for the error will accumulate, see Equation (11)˜(12). We apply platt
scaling method, and Root Mean Square Error (RMSE)4 is adopted
as measurement. The results is shown in Table 3.

From Table 3, it can be seen significant improvement has been
achieved after calibration, and the curve of real value and calibrated
value is shown in Figure 4 and Figure 5. Abscissa axis is items
sorted by the predicted scores, and vertical axis is calibrated score
and the real score. The real score is obtained from items bin.

5.4.3 Discrimination. In Dataset 2, for each user we get the quit
probability of corresponding candidates from the MDP Genera-
tor, i.e. the items in the user’s browse session. Then a user’s quit
probability list lu = (q1, ..., qi , ..., qn ) is obtained, where qi is the
quit probability when recommending item i to user u. Standard
Error (STD) and MEAN are calculated for each list, and the sta-
tistics of the dataset is shown in Table 4. From the table, it can

3https://en.wikipedia.org/wiki/Receiver operating characteristic#Area under the curve

4https://en.wikipedia.org/wiki/Root-mean-square deviation

00.10.20.30.40.50.60.70.80.910102030405060708090100110120130140150160170180190200210220230240250260CTR CalibrationCalibrated ValueReal ValueMaximizing Cumulative User Engagement

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

• The longer the number of steps, the greater the advantages
on IPV and BL. See T = 20 and T = 50, when T times 2.5,
from 20 to 50, the improvement of both IPV and BL are
more than 2.5 times(1347.57 vs 392.97, 4045.47 vs 1066.08
respectively). This result is in line with our expectation
as planning more steps could lead to a bigger effect of the
quit probability on users.

Table 6: IPV of Offline Evaluation

Method

T=20
BL

IPV

CTR

IPV

T=50
BL

CTR

GREEDY

168.10
Beam Search 321.91

455.36
977.78

0.37
0.32
392.97 1347.57 0.29 1066.08 4045.47 0.26

0.37
0.33

765.79
2039.88

280.63
660.90

SSP

5.5.2

SSP Plan with Duplicate Removal. In some practical sce-
narios, items are forbidden to display repeatedly. We need to make
a compromise on the three strategies.

• Greedy: The items selected in the previous t steps should

be removed from the candidate set for step t + 1.

• Beam Search: The items selected in the previous t steps
should be removed from the candidate set for step t + 1.
• SSP: When planing, we plan from step T to step 1 accord-
ing to the upper bound of V ∗(st ) of each step, and keep
the optimal T items as the step’s candidates at each step.
When selecting, we do the selection from step 1 to step N .
Specifically, we choose the optimal one item and remove it
from the remaining steps’ candidates simultaneously.

From the detailed results in Table 7, we can find that although the
compromises hurts the ideal effects, SSP still outperforms Greedy
and Beam Search.

Table 7: IPV of Offline Evaluation with Deduplication

Figure 5: Calibration of Quit Model

be demonstrated that, for each user, different candidates make dif-
ferent contributions to keep the user browsing, and they have a
significant discrimination.

Table 4: Discrimination of Quit Probabilites

STD MEAN STD/MEAN

0.1963

0.7348

0.3135

5.4.4 Weakly Related. We further study the correlation between
quit probability and immediate user engagement (i.e. each step
reward). For each user, we get two item lists lu1 and lu2 with
length L = 20. lu1 and lu2 are formed greedily according to Rst ,at
and (1 − Psi,ai,sA ) respectively. If (1 − Psi,ai,sA ) and Rst ,at
are
completely positive correlation, lu1 and lu2 will be the same, which
leads to the equality of SSP and Greedy. We use Jaccard Index5 and
NDCG6 to measure the similarity of lu1 and lu2, and the average
result of the dataset is shown in Table 5. From the table, we find
that in the dataset quit probability and immediate user engagement
are weakly related.

Table 5: The Correlation between Quit Probability and CTR

Method

T=20
BL

IPV

CTR

IPV

T=50
BL

CTR

Mean Length

List Length

Jaccard Index NDCG

57

20

0.33

0.52

SSP

GREEDY

68.06
Beam Search 105.52

216.38 0.31
0.25
427.80

0.31
0.24
189.11 999.51 0.19 242.77 1632.56 0.15

217.93
439.59

68.23
107.06

5.5 SSP Planner: Offline Evaluation
Now we deploy the strategies in Dataset 2.

5.5.1

SSP Plan. We plan a sequence list with T steps: LT =
(a1, a2, · · · , aT ), according to each strategy mentioned above. The
revenue of LT can be calculated according to Equation (13)-(15).

The detailed results are shown in Table 6, and we can find that:
• Greedy achieves the best CT R, while SSP achieves the best
IPV and BL. This demonstrates our idea that IPV can be
improved through making users browse more. SSP does
not aim to optimize each step effectiveness, and its purpose
is to improve the total amount of cumulative clicks.

5https://en.wikipedia.org/wiki/Jaccard index
6https://en.wikipedia.org/wiki/Discounted cumulative gain

5.5.3

SSP Plan with Noise. Since there may exist a gap between
offline environment and online environment, which makes the
predicted click-through rate and quit probability offline are not
absolutely equivalent to the real value online, we introduce a set of
noise experiments before deploying MDP-SSP online.

The experiments are conducted in the following way: we add
random noises in the click-through rate and quit probability given
by the offline environment. Assuming the noise e ∼ U (a, b) where
U (a, b) is a uniform distribution, we define a = −0.02m, b = 0.02m,
where m is an integer ranges from 0 to 10. We plan according to the
value with noise, and calculate the final revenue with the real value.
The results are shown in Figure 6. The horizontal axis represents
the noise, i.e. b in U (a, b), and the vertical axis is the revenue, i.e.
cumulative clicks.

00.10.20.30.40.50.60.70.80.910102030405060708090100110120130140150160170180190200210220230240250260Quit Probability CalibrationCalibrated ValueReal ValueKDD ’20, August 23–27, 2020, Virtual Event, CA, USA

Zhao and Zhou, et al.

(a) T=20

(b) T=50

Figure 6: Noise Experiments

From Figure 6 we can find that although SSP is more sensitive to
noise, it performs better than Greedy and Beam Search. It demon-
strates that considering quit probability plays a very important role
in IPV issue.

5.6 SSP Planner: Online Evaluation
For online evaluation, we deployed SSP and Greedy strategies on
a real E-commerce APP. For further comparison, we conduct a
experiments with quit model which does not introduce MIL, and
the strategy is named SSPno M I L. Three strategies run online with
the same traffic for one week, and the results shown in Table 87
demonstrate that:

• For cumulative clicks, quit probability cannot be ignored
in sequential recommendations, see SSP and Greedy.
• The accuracy of quit probability directly influence the re-

sults, see SSP and SSPno M I L.

Table 8: IPV of online Evaluation

Method

IPV

BL

Greedy
SSPno M I L
SSP

0.9296
0.9638
1

0.9440
0.9789
1

6 CONCLUSIONS
In this paper, we study the problem of maximizing cumulative
user engagement in sequential recommendation where the browse
length is not fixed. Furthermore, we propose an online optimization
framework, in which the problem can be reduced to a SSP problem.
Then a practical approach that is easy to implement in real-world ap-
plications is proposed, and the corresponding optimization problem
can be efficiently solved via dynamic programming. The superior
advantage of our method is also verified with both offline and online
experiments by generating optimal personalized recommendations.
In the future, we will study the MDP-SSP with deduplication.
While the current MDP-SSP could yield good sequential recom-
mendation, it fails to consider the item duplication issue, which is
commonly not allowed in practice. Although we propose a compro-
mise strategy and it outperforms Beam Search and Greedy strategy
(which are commonly used in practice), it is not the optimal solution
when considering items deduplication. It will bring more insights
if the deduplication constraint can be modeled into the strategy.

7The data has been processed for business reason.

0501001502000.000.020.040.060.080.100.120.140.160.180.20IPVT=20N=20Beam SearchGreedySSP0501001502002503000.000.020.040.060.080.100.120.140.160.180.20IPV T=50 N=50Beam SearchGreedySSPMaximizing Cumulative User Engagement

KDD ’20, August 23–27, 2020, Virtual Event, CA, USA

[24] Felipe W Trevizan, Sylvie Thi´ebaux, Pedro Henrique Santana, and Brian Charles
Williams. 2016. Heuristic search in dual space for constrained stochastic short-
est path problems.. In Proceedings of the Thirteenth International Conference on
Automated Planning and Scheduling. London, UK, 326–334.

[25] Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing. 2017.
Recurrent recommender networks. In Proceedings of The Tenth ACM International
Conference on Web Search and Data Mining. ACM, Cambridge, UK, 495–503.
[26] Qi Zhang and Sally A Goldman. 2002. EM-DD: An improved multiple-instance
learning technique. In Advances in Neural Information Processing Systems. Van-
couver, British Columbia, Canada, 1073–1080.

REFERENCES
[1]

Jaume Amores. 2013. Multiple instance classification: review, taxonomy and
comparative study. Artificial intelligence 201 (2013), 81–105.

[2] Stuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann. 2003. Support
vector machines for multiple-instance learning. In Advances in Neural Information
Processing Systems. Vancouver, Canada, 577–584.

[3] Andrew G Barto, Steven J Bradtke, and Satinder P Singh. 1995. Learning to
act using real-time dynamic programming. Artificial Intelligence 72, 1-2 (1995),
81–138.

[7]

[4] Dimitri P Bertsekas and John N Tsitsiklis. 1991. An analysis of stochastic shortest
path problems. Mathematics of Operations Research 16, 3 (1991), 580–595.
[5] Blai Bonet and Hector Geffner. 2003. Labeled RTDP: improving the convergence
of real-time dynamic programming.. In Proceedings of the Thirteenth International
Conference on Automated Planning and Scheduling, Vol. 3. Trento, Italy, 12–21.
[6] Razvan C Bunescu and Raymond J Mooney. 2007. Multiple instance learning for
sparse positive bags. In Proceedings of the Twenty-Fourth International Conference
on Machine Learning. ACM, Corvallis, Oregon, USA, 105–112.
Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based
reranking for reordering documents and producing summaries. In Proceedings of
the Twentiy-First Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, Melbourne, Australia, 335–336.
[8] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and
Hongyuan Zha. 2018. Sequential recommendation with user memory networks.
In Proceedings of the Eleventh ACM International Conference on Web Search and
Data Mining. ACM, Los Angeles, California, USA, 108–116.

[9] Robin Devooght and Hugues Bersini. 2017. Long and short-term recommenda-
tions with recurrent neural networks. In Proceedings of The Twenty-Fifth Confer-
ence on User Modeling, Adaptation and Personalization. ACM, Bratislava, Slovakia,
13–21.

[10] Thomas G Dietterich, Richard H Lathrop, and Tom´as Lozano-P´erez. 1997. Solving
the multiple instance problem with axis-parallel rectangles. Artificial Intelligence
89, 1-2 (1997), 31–71.

[11] Tim Donkers, Benedikt Loepp, and J¨urgen Ziegler. 2017. Sequential user-based
recurrent neural network recommendations. In Proceedings of the Eleventh ACM
Conference on Recommender Systems. ACM, Como, Italy, 152–160.

[12] Travis Ebesu, Bin Shen, and Yi Fang. 2018. Collaborative memory network for

recommendation systems. arXiv preprint arXiv:1804.10862 (2018).

[13] Thomas G¨artner, Peter A Flach, Adam Kowalczyk, and Alexander J Smola. 2002.
Multi-instance kernels. In Proceedings of the Nineteenth International Conference
on Machine Learning, Vol. 2. Sydney, Australia, 179–186.

[14] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi,
Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from
predicting clicks on ads at facebook. In Proceedings of the Eighth International
Workshop on Data Mining for Online Advertising. ACM, New York, NY, USA, 1–9.
[15] Bal´azs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based recommendations with recurrent neural networks. arXiv
preprint arXiv:1511.06939 (2015).
Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang.
2018. Improving sequential recommendation with knowledge-enhanced memory
networks. In Proceedings of the Forty-First International ACM SIGIR Conference
on Research & Development in Information Retrieval. ACM, Ann Arbor Michigan,
USA, 505–514.

[16]

[17] Andrey Kolobov, Mausam Mausam, Daniel S Weld, and Hector Geffner. 2011.
Heuristic search for generalized stochastic shortest path MDPs. In Proceedings of
the Twenty-First International Conference on Automated Planning and Scheduling.
Freiburg, Germany.

[18] Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C Weng. 2007. A note on Plattfis
probabilistic outputs for support vector machines. Machine learning 68, 3 (2007),
267–276.

[19] Oded Maron and Tom´as Lozano-P´erez. 1998. A framework for multiple-instance
learning. In Advances in Neural Information Processing Systems. Massachusetts,
USA, 570–576.

[20] Alexandru Niculescu-Mizil and Rich Caruana. 2005. Predicting good probabili-
ties with supervised learning. In Proceedings of the Twenty-Second International
Conference on Machine Learning. ACM, Bonn, Germany, 625–632.

[21] George H Polychronopoulos and John N Tsitsiklis. 1996. Stochastic shortest
path problems with recourse. Networks: An International Journal 27, 2 (1996),
133–143.
Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation
via convolutional sequence embedding. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining. ACM, Los Angeles,
California, USA, 565–573.

[22]

[23] Choon Hui Teo, Houssam Nassif, Daniel Hill, Sriram Srinivasan, Mitchell Good-
man, Vijai Mohan, and SVN Vishwanathan. 2016. Adaptive, personalized di-
versity for visual discovery. In Proceedings of The Tenth ACM Conference on
Recommender Systems. ACM, Boston, MA, USA, 35–38.

