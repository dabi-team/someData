1
2
0
2

t
c
O
0
3

]

G
L
.
s
c
[

1
v
4
5
2
0
0
.
1
1
1
2
:
v
i
X
r
a

Equinox: neural networks in JAX via callable
PyTrees and ﬁltered transformations

Patrick Kidger
University of Oxford
The Alan Turing Insitute
kidger@maths.ox.ac.uk

Cristian Garcia
Quansight
cgarcia@quansight.com

Abstract

JAX and PyTorch are two popular Python autodifferentiation frameworks. JAX
is based around pure functions and functional programming. PyTorch has popu-
larised the use of an object-oriented (OO) class-based syntax for deﬁning param-
eterised functions, such as neural networks. That this seems like a fundamental
difference means current libraries for building parameterised functions in JAX
have either rejected the OO approach entirely (Stax) or have introduced OO-to-
functional transformations, multiple new abstractions, and been limited in the ex-
tent to which they integrate with JAX (Flax, Haiku, Objax). Either way this OO/-
functional difference has been a source of tension. Here, we introduce ‘Equinox’,
a small neural network library showing how a PyTorch-like class-based approach
may be admitted without sacriﬁcing JAX-like functional programming. We pro-
vide two main ideas. One: parameterised functions are themselves represented as
‘PyTrees’, which means that the parameterisation of a function is transparent to
the JAX framework. Two: we ﬁlter a PyTree to isolate just those components that
should be treated when transforming (‘jit’, ‘grad’ or ‘vmap’-ing) a higher-order
function of a parameterised function – such as a loss function applied to a model.
Overall Equinox resolves the above tension without introducing any new program-
matic abstractions: only PyTrees and transformations, just as with regular JAX.
Equinox is available at https://github.com/patrick-kidger/equinox.

1 Introduction

JAX is a popular Python framework for autodifferentiation [2, 4]. It has introduced several popular
new abstractions, such as the use of ‘PyTrees’ to represent data, and several ‘transformations’, which
are higher-order functions – the most notable of which are ‘jit’, ‘grad’ and ‘vmap’ – to manipulate
pure functions.

These two concepts are central to programming in JAX.

We will assume familiarity with the terminology and syntax of the Python programming language.

1.1 PyTrees and transformations

PyTree A PyTree is any arbitrarily nested composition of ‘node’ types (dictionaries, lists, and
tuples) containing ‘leaf’ types (all other Python types). For example,

1 v a r i a b l e = [ 4 , { " key1 " : 3 . 0 ,

" key2 " : True ,

" key3 " : o b j e c t ( ) } ]

is a PyTree with structure [*, {"key1": *, "key2": *, "key3": *}] and leaves 4, 3.0,
True, object().

35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

 
 
 
 
 
 
JAX assumes referential transparency in its treatment of PyTrees – that is, each PyTree describes
a tree, and not a directed acyclic graph (DAG). It is generally a mistake to use the same object in
multiple leaves of a tree.

JAX allows custom types to be registered as node types. This will be important for Equinox later.

Transformations

JAX provides several operations which act on pure functions.

‘jit’ traces a pure function and records its computation graph. This computation graph is just-in-time
(JIT) compiled to hardware-speciﬁc code via the XLA intermediate representation. The compiled
(pure) function is returned. This is primarily used to improve execution speed.

‘grad’ traces a pure function, and then applies reverse-mode autodifferentiation to return the gradi-
ents of the outputs with respect to its inputs.

‘vmap’ wraps a pure function, so that each of its constituent operations are repeated across a batch
of inputs. This is primarily used to write simpler code: the action of a function may be described
on just a single element, and then the framework takes care of the batching. There may be implicit
efﬁciency gains: a matrix-vector multiply, batched over the vector, produces an asymptotically more
efﬁcient matrix-matrix multiply.

‘jit-grad-vmap’ is a common pattern when training neural networks. Having described the forward
pass of a neural network model, it is vmap’d over a batch of data, this forward pass is differentiated
with respect to the parameters of a model, and then this whole operation is JIT-compiled.

1 @jax . j i t
2 @jax . g r a d
3 d e f
4
5

l o s s ( p a r a m e t e r s ,
f e a t u r e s ,
p r e d _ l a b e l s = j a x . vmap ( f o r w a r d ) ( p a r a m e t e r s ,
r e t u r n j a x . numpy . mean ( ( l a b e l s − p r e d _ l a b e l s ) ** 2 )

l a b e l s ) :

f e a t u r e s )

In particular this highlights the composability of the different transformations provided by JAX. The
above code snippet assumes that a suitable pure function ‘forward’ has been deﬁned.

JAX also provides several other transformations (‘vjp’, ‘jvp’, ‘jacrev’, ‘jacfwd’, ‘linearize’ and so
on) that we will not discuss here.

1.2 Object-oriented model building

A parameterised function may be naturally represented as a class, which encapsulates state whilst
deﬁning methods parameterised by that state. This encourages an object-oriented (OO) approach to
model building.

See for example the following snippet of PyTorch [15] code, in which the weights and biases of
the network parameterise the forward pass of the model. torchtyping is used to provide type
annotations for tensor shapes [11].

1
2
3
4
5
6
7
8
9
10
11
12
13
14

from t o r c h import nn

c l a s s Model ( nn . Module ) :

d e f _ _ i n i t _ _ ( s e l f ,

i n _ s i z e :

i n t , h i d d e n _ s i z e :

i n t , o u t _ s i z e :

i n t ) :

s u p e r ( ) . _ _ i n i t _ _ ( )
s e l f . w e i g h t 1 = nn . P a r a m e t e r ( t o r c h . r a n d n ( i n _ s i z e , h i d d e n _ s i z e ) )
s e l f . w e i g h t 2 = nn . P a r a m e t e r ( t o r c h . r a n d n ( h i d d e n _ s i z e , o u t _ s i z e ) )

d e f

f o r w a r d ( s e l f , x :

t o r c h t y p i n g . T e n s o r T y p e [ " b a t c h " ,

" i n _ s i z e " ]

) −> t o r c h t y p i n g . T e n s o r T y p e [ " b a t c h " ,

" o u t _ s i z e " ] :

x = x @ s e l f . w e i g h t 1
x = t o r c h . r e l u ( x )
x = x @ s e l f . w e i g h t 2
r e t u r n x

The primary attraction here is the elegant syntax for model-building. (Not the ability to mutate
instance state – in principle out-of-place updates sufﬁce for most purposes.)

2

1.3 Contributions

We introduce Equinox. Half tech demo, half neural network library, Equinox demonstrates that a
PyTorch-like class-based syntax may be used without sacriﬁcing JAX-like functional programming.

To the best of our knowledge Equinox is the ﬁrst library of its kind, in that it does not exhibit any of
the limitations introduced by previous libraries. Indeed Equinox may be most meaningfully deﬁned
by what it does not have.

1. It does not introduce any new abstractions – everything is either a PyTree or a transforma-

tion.

2. It does not introduce an OO-to-functional transform, for translating from one paradigm to

the other.

3. It does not place limitations or require special care on integrating with arbitrary JAX code.
4. It does not introduce any specially-wrapped library-speciﬁc versions of ‘jit’/‘grad’/‘vmap’

needed to work with its parameterised functions.1

That Equinox does not require these complexities makes it easy to learn, makes it absent of ‘sharp
bits’ around which a user must be careful, and requires no special effort to interoperate with other
JAX libraries.

Equinox has already seen success. In the short time since Equinox was announced, at least three
copycat2 libraries have already appeared, quoting Equinox as a primary source of inspiration and
offering variations on its ideas [5, 12, 13].

The two main ideas introduced with Equinox are the representation of parameterised functions as
data (Section 3) and ﬁltering PyTrees (Section 4), the latter being particularly useful around trans-
formations such as ‘jit’ and ‘grad’.

Equinox is available at https://github.com/patrick-kidger/equinox and may be installed
via pip install equinox. At time of writing it is at version 0.1.1.
It is available under the
Apache-2.0 license.

2 Related work

Note that here and throughout we distinguish the task of building neural networks (requiring deﬁni-
tions of convolutions, dropout and so on) from the lower-level task of representing and manipulating
parameterised functions (which is our main focus).

Init/apply Stax [17] forgoes trying to represent parameterised functions entirely. Instead, a param-
eterised function fθ is decomposed into two pieces: θ and (θ, x) 7→ fθ(x). The former is a PyTree
of data; the latter is a pure function. This is sometimes known as the ‘init/apply’ approach.

feature is an OO-to-functional operation.

Init/apply wrappers Haiku’s [8] central
A
model is built using OO syntax, and then haiku.transform is used to translate between
paradigms. The return value is a pair of init/apply functions. However multiple variations
are needed to handle all cases (haiku.transform_with_state, haiku.multi_transform,
haiku.multi_transform_with_state). Additionally, native JAX operations do not always work
within the OO paradigm (only after the OO-to-functional transform), requiring wrapped haiku.jit
(and so on) operations to function correctly.

Flax [7] provides a class-based syntax for representing parameterised functions. Around JAX API
boundaries it again operates in init/apply style, and again substantial complexity is introduced.
This includes new abstractions like custom parameter groups (flax.core.variables.Variable),
wrapped versions of JAX transformations (flax.linen.vmap, flax.linen.jit), and even
wrapped versions of other JAX operations (flax.linen.scan).

1Equinox does introduce some wrapped ‘jit’/‘grad’ operations, namely ‘ﬁlter_jit’ and ‘ﬁlter_grad’ (Section
4) but these are a convenience only, and operate on arbitrary PyTrees. That is, the key point is that there is no
coupling between custom transformations and the representation of parameterised functions.

2A term we use without negative connotation.

3

Complete wrappers Objax [14] offers an OO-based syntax that forgoes almost all interaction
with native JAX, in favour of providing a wrapped API for all relevant operations. This involves
introducing several new abstractions (variables, modules). Such a ‘framework-oriented’ approach
also limits compatibility with third-party JAX libraries.

Objax emphasises (although Flax, Haiku behave similarly) the use of a ‘Module system’. That
is, parameterised functions are represented via a ‘Module’, which is a privileged type with special
behaviour. This is contrast to Equinox, later, in which ‘Module’s will be PyTrees like any other.

Classes as PyTrees Representing classes as data is a precursor to a class-based syntax for rep-
resenting parameterised functions as data. In particular, to use a class-based syntax just to group
together related information. Flax provides this through flax.struct and Chex [3] provides this
through chex.dataclass.

Parameterised functions as PyTrees jax.tree_util.Partial represents a parameterised
function as data, and may be found within the main JAX library itself. This is philosophically
very similar to Equinox – after instantiation it is a callable PyTree. What it lacks is a convenient
class-based syntax for readability and composability.

In other frameworks/languages Some notable points of reference exist beyond JAX.

Flux.jl [9, 10] in the Julia language [1] is philosophically similar to Equinox. It offers a class-based
PyTree-like system to deﬁne parameterised functions, which it refers to as ‘functors’. (In reference
to the functional programming / category-theoretic notion known in JAX as jax.tree_map.) Swift
for TensorFlow [16] also does something similar.

There is interest in introducing JAX-like transformations (‘grad’, ‘vmap’) to the PyTorch frame-
work, via the functorch project [6]. Just like Haiku this includes an OO-to-functional transform
functorch.make_functional for transforming the PyTorch Module system into pure functions.

3 Parameterised functions as data

Equinox represents parameterised functions as class instances. These instances are immutable, for
consistency with JAX’s functional programming principles. (And parameters updates occur out-of-
place.)

A parameterised function fθ is represented as an instance of a class, with the class corresponding to
the function family {fθ | θ ∈ Θ}. A separate class is deﬁned for each function family.

Every such class is registered as a custom PyTree node type. That is, JAX is told how to (de)serialise
class instances into a standardised format.

Class instance state is used to represent the parameterisation θ. Methods may then be used to deﬁne
the forward operation. Unbound methods deﬁne the pure unparameterised function (θ, x) 7→ fθ(x).
Bound methods (with self already passed) deﬁne the pure parameterised function fθ.

That class instances are also PyTrees crucially means that this state is transparent to JAX. That
is, given some higher-order pure function g for which g(fθ) is deﬁned, then we may for example
dg
differentiate g with respect to fθ to compute
df (fθ).
Indeed this is a typical use-case, such as
differentiating a loss function g(fθ, x, y) = (y − fθ(x))2 with respect to fθ.
The essential ideas may be summarised in the following code.

1
2
3
4
5
6
7
8
9
10

import
import

j a x
j a x . numpy a s

j n p

c l a s s Adder :

d e f _ _ i n i t _ _ ( s e l f , p a r a m e t e r :

f l o a t ) :

s e l f . p a r a m e t e r = p a r a m e t e r

@jax . j i t
d e f _ _ c a l l _ _ ( s e l f , x :

j n p . n d a r r a y ) −> j n p . n d a r r a y :

r e t u r n x + s e l f . p a r a m e t e r

4

d e f

@ c l a s s m e t h o d
d e f

t r e e _ f l a t t e n ( s e l f ) :
r e t u r n ( s e l f . p a r a m e t e r , ) , None

t r e e _ u n f l a t t e n ( c l s , _ , p a r a m e t e r ) :
r e t u r n Adder ( p a r a m e t e r [ 0 ] )

11
12
13
14
15
16
17
18
19
20 @jax . g r a d
21 d e f h i g h e r _ o r d e r _ f u n c ( a d d e r : Adder , d a t a :
22

r e t u r n a d d e r ( d a t a )

j a x . t r e e _ u t i l . r e g i s t e r _ p y t r e e _ n o d e _ c l a s s ( Adder )

j n p . n d a r r a y ) :

The gradient returned by higher_order_func will itself be an instance of Adder, whose
parameter attribute will be the gradient of the parameter attribute used in the forward pass.

Note how we may JIT-compile Adder.__call__, which is a pure function when including the
self argument. This JIT compilation is included by way of demonstration. This is unlike previous
libraries: arbitrary JAX transformations (‘jit’) may safely be used anywhere in the forward pass of
the model.

In practice the (de)serialisation and registering as a PyTree node may be neatly handled through a
traditional subclassing syntax:

1
2
3
4
5
6

c l a s s Adder ( e q u i n o x . Module ) :

p a r a m e t e r :

f l o a t

@jax . j i t
d e f _ _ c a l l _ _ ( s e l f , x :

j n p . n d a r r a y ) −> j n p . n d a r r a y :

r e t u r n x + s e l f . p a r a m e t e r

The simplicity of this approach belies the complexities that have been overcome. Indeed we recall
our list of things Equinox has not required, relative to previous libraries (Section 1.3, Section 2). We
sometimes refer to this approach as a ‘callable PyTree’.

It is actually of some surprise to us that this idea appears to be new to JAX. (Or at least the idea that
it is sufﬁcient to build fully-featured models around.) As noted in Section 2, the idea already exists
in Julia (Flux.jl functors) and similar ideas already exist in JAX (jax.tree_util.Partial).

A larger self-contained example is available in Appendix A.

4 Filtering

There is (only) one foible. The example of Section 3 had one key simplicity; the parameters of
Adder – in practice it had only a single one – were of a type understood by JAX. More speciﬁcally,
all of the leaves of the PyTree were of types for which JAX had differentiation rules.

This cannot be true in general. We may wish to parameterise a function by arbitrary Python types,
unknown to JAX. More generally we may wish to JIT or differentiate a model with respect to only
some of its parameters. Typical examples of this include:

1. The activation function in an feedforward network may be an arbitrary Python callable.

2. It is typical to want to JIT a forward pass with respect to all JAX arrays, but only to differ-

entiate with respect to all ﬂoating-point JAX arrays.

3. It may be desirable to leave some parameters of a model ‘frozen’, and differentiate only

with respect to the others.

This foible is handled through ‘ﬁltering’, as in the following example.

1 @jax . g r a d
2 d e f
3
4

l o s s ( p a r a m e t e r s ,
model = e q u i n o x . combine ( p a r a m e t e r s ,
r e t u r n model ( d a t a )

s t a t i c , d a t a ) :

s t a t i c )

5

5
6 model = e q u i n o x . nn . MLP ( . . . )
7 d a t a = . . .
8 p a r a m e t e r s ,
9

l o s s ( p a r a m e t e r s ,

s t a t i c , d a t a )

s t a t i c = e q u i n o x . p a r t i t i o n ( model ,

e q u i n o x . i s _ a r r a y )

A PyTree is partitioned into two pieces on one side of an API boundary – corresponding to those
pieces that should/shouldn’t be differentiated – and then reconstituted again on the other side. Each
piece (parameters, static) is another instance of the model-as-PyTree, containing just the leaves
that should be treated in the appropriate way and with dummy values on the others.

In practice such partitioning occurs almost exclusively across API boundaries described by JAX
transformations. For simplicity (only), Equinox provides wrappers for common use cases.

l o s s ( model , d a t a ) :
r e t u r n model ( d a t a )

1 @equinox . f i l t e r _ g r a d
2 d e f
3
4
5 model = e q u i n o x . nn . MLP ( . . . )
6 d a t a = . . .
7

l o s s ( model , d a t a )

We emphasise that such ‘ﬁltered transformations’ are entirely unlike the specially-wrapped trans-
formations often introduced in previous libraries, needed to handle the library-speciﬁc notion of a
parameterised function. There is no coupling between filter_grad and the model: the former
operates on arbitrary PyTrees, whilst the latter is a PyTree like any other.

5 Further topics

Design goals Equinox has two key design goals. One: a very strong regularisation towards sim-
plicity, in particular in the implementation of Equinox itself. Equinox introduces no new abstractions
on top of regular JAX: everything is just PyTrees and transformations on PyTrees. Two: that using
Equinox should feel like (and be fully compatible with) the main JAX library itself. Our litmus test
for whether to add a feature is ‘could this plausibly be in the main JAX library?’.

Location of ﬁltering Filtering takes place at the call site of a (‘jit’/‘grad’/‘vmap’) transformation,
and is not embedded into the PyTree structure. For example this is unlike PyTorch (and other JAX
libraries) for which whether a tensor participates in autodifferentiation is metadata speciﬁed by a
requires_grad ﬂag attached to the tensor itself. This is in-line with native JAX, and furthermore
ensures compatibility with any other transformations introduced into JAX at a later date.

Lack of module system Equinox Modules are PyTrees like any other, and are never special-cased.
This is unlike PyTorch, and other JAX libraries, which for example often feature a ‘module map’
analogous to jax.tree_map.

Bound methods are PyTrees too Section 3 discusses letting class instances be PyTrees, so that
__call__ is a pure parameterised function transparent to JAX. We go further and treat all bound
methods as PyTrees, via Python’s descriptor protocol. That is, the bound method is a PyTree whose
single child subtree is its implicit self parameter.

Managing state
‘State’ is used to refer to parameters updated other than through gradient descent,
such as batch normalisation statistics. This may be handled by mutating the model in-place as
desired, and then returning the updated model out of any JAX API boundaries [5].

Neural network library The focus of Equinox is the representation and manipulation of parame-
terised functions as data. Neural networks are of course a major use case for parameterised functions,
so for convenience (and as proof of its efﬁcacy) Equinox also includes a small equinox.nn library.

6

6 Conclusion

We have introduced Equinox, which demonstrates how we may use a PyTorch-like class-based API
for building models (parameterised functions) without sacriﬁcing JAX-style functional program-
ming.

Acknowledgments and Disclosure of Funding

PK was supported by the EPSRC grant EP/L015811/1 and by the Alan Turing Institute under the
EPSRC grant EP/N510129/1.

References

[1]

[2]

et

al.

SIAM Review 59.1

Jeff Bezanson
In:
https://epubs.siam.org/doi/10.1137/141000671.
James Bradbury et al. JAX: composable transformations of Python+NumPy programs. Ver-
sion 0.2.5. 2018. URL: http://github.com/google/jax.

computing”.
65–98. DOI: 10.1137/141000671. URL:

“Julia: A fresh
(2017), pp.

numerical

approach

to

[3] David Budden et al. Chex: Testing made fun,

in JAX! Version 0.0.1. 2020. URL:

http://github.com/deepmind/chex.

[4] Roy Frostig et al. “Decomposing reverse-mode automatic differentiation”. In: LAFI workshop,

POPL (2021).

[5] Cristian Garcia. Treex. Accessed 2021. URL: https://github.com/cgarciae/treex.
[6] Horace He and Richard Zou. functorch: JAX-like composable function transforms for PyTorch.

[7]

2021. URL: https://github.com/facebookresearch/functorch.
Jonathan Heek et al. Flax: A neural network library and ecosystem for JAX. Version 0.3.4.
2020. URL: http://github.com/google/flax.
for

JAX. Version

2020. URL:

0.0.3.

[8] Tom Hennigan

Sonnet
http://github.com/deepmind/dm-haiku.

al. Haiku:

et

[9] Michael Innes et al. “Fashionable Modelling with Flux”. In: arXiv:1811.01457 (2018).

[10] Mike Innes. “Flux: Elegant Machine Learning with Julia”. In: Journal of Open Source Soft-

ware (2018).

[11] P. Kidger. torchtyping. Accessed 2021. URL: https://github.com/patrick-kidger/torchtyping.
[12] NTT123. Opax. Accessed 2021. URL: https://github.com/NTT123/opax.
[13] NTT123. Pax. Accessed 2021. URL: https://github.com/NTT123/pax.
[14] Objax

Version

Objax.

2020.

1.2.0.

URL:

Developers.
https://github.com/google/objax.

[15] A. Paszke et al. “PyTorch: An Imperative Style, High-Performance Deep Learning Library”.

In: Advances in Neural Information Processing Systems 32. 2019, pp. 8024–8035.

[16] Brennan Saeta et al. “Swift for TensorFlow: A portable, ﬂexible platform for deep learning”.

In: arXiv:2102.13243 (2021).

[17] Stax. Accessed 2021. URL: https://jax.readthedocs.io/en/latest/jax.experimental.stax.html.

7

A Example

The following is example code for deﬁning a custom parameterised function in Equinox.

It demonstrates how to compose one parameterised function with another (the linear layers) and how
to deﬁne new parameters directly (the bias).

It additionally demonstrates the use of ﬁltered transformations, to handle the fact that some of the
leaves of the model-as-PyTree are nondifferentiable/non-JIT-able.

j n p

j n n

l i s t

them ;

j r a n d o m

c a l l a b l e

s u b m o d u l e s

a t t r i b u t e s ;

j n p . n d a r r a y

c l a s s MyModule ( eqx . Module ) :

# And how t o i n i t i a l i s e
d e f _ _ i n i t _ _ ( s e l f , key ) :

j a x
j a x . nn a s
j a x . numpy a s
j a x . random a s

# n e s t e d l i s t o f
# a r b i t r a r y P y t h o n o b j e c t
# p a r a m e t e r

# S p e c i f y t h e module ’ s
l a y e r s :
a c t i v a t i o n :
b i a s :

import e q u i n o x a s eqx
import
import
import
import

key1 , key2 = j r a n d o m . s p l i t ( key )
s e l f . l a y e r s = [ eqx . nn . L i n e a r ( 2 , 8 , key = key1 ) ,
eqx . nn . L i n e a r ( 8 , 2 , key = key2 ) ]

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27 @eqx . f i l t e r _ j i t
28 @eqx . f i l t e r _ g r a d
29 d e f
30
31
32
33 x_key , y_key , model_key = j r a n d o m . s p l i t ( j r a n d o m . PRNGKey ( 0 ) , 3 )
34 x , y = j r a n d o m . n o r m a l ( x_key ,
35 model = MyModule ( model_key )
36 g r a d s = l o s s ( model , x , y )

# And t h e f o r w a r d p a s s o f
d e f _ _ c a l l _ _ ( s e l f , x ) :
l a y e r
x = s e l f . a c t i v a t i o n ( l a y e r ( x ) )
r e t u r n s e l f . l a y e r s [ − 1 ] ( x ) + s e l f . b i a s

l o s s ( model , x , y ) :
p r e d _ y = j a x . vmap ( model ) ( x )
r e t u r n j n p . mean ( ( y − p r e d _ y ) ** 2 )

s e l f . a c t i v a t i o n = j n n . r e l u
s e l f . b i a s = j n p . o n e s ( 2 )

j r a n d o m . n o r m a l ( y_key ,

i n s e l f . l a y e r s [ : − 1 ] :

( 1 0 0 , 2 ) ) ,

t h e model .

f o r

( 1 0 0 , 2 ) )

8

