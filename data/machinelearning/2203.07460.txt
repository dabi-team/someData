Machine Learning and LHC Event Generation

Anja Butter1,2, Tilman Plehn1, Steffen Schumann3 (Editors),
Simon Badger4, Sascha Caron5, 6 Kyle Cranmer7,8, Francesco Armando Di Bello9,
Etienne Dreyer10, Stefano Forte11, Sanmay Ganguly12, Dorival Gonçalves13, Eilam Gross10,
Theo Heimel1, Gudrun Heinrich14, Lukas Heinrich15, Alexander Held16, Stefan Höche17,
Jessica N. Howard18, Philip Ilten19, Joshua Isaacson17, Timo Janßen3, Stephen Jones20,
Marumi Kado9,21, Michael Kagan22, Gregor Kasieczka23, Felix Kling24, Sabine Kraml25,
Claudius Krause26, Frank Krauss20, Kevin Kröninger27, Rahool Kumar Barman13,
Michel Luchmann1, Vitaly Magerya14, Daniel Maitre20, Bogdan Malaescu2,
Fabio Maltoni28,29, Till Martini30, Olivier Mattelaer28, Benjamin Nachman31,32,
Sebastian Pitz1, Juan Rojo33,34, Matthew Schwartz35, David Shih25, Frank Siegert36,
Roy Stegeman11, Bob Stienen5, Jesse Thaler37, Rob Verheyen38, Daniel Whiteson18,
Ramon Winterhalder28, and Jure Zupan19

Abstract

First-principle simulations are at the heart of the high-energy physics research program.
They link the vast data output of multi-purpose detectors with fundamental theory pre-
dictions and interpretation. This review illustrates a wide range of applications of mod-
ern machine learning to event generation and simulation-based inference, including con-
ceptional developments driven by the speciﬁc requirements of particle physics. New
ideas and tools developed at the interface of particle physics and machine learning will
improve the speed and precision of forward simulations, handle the complexity of colli-
sion data, and enhance inference as an inverse simulation problem.

Submitted to the Proceedings of the US Community Study
on the Future of Particle Physics (Snowmass)

2
2
0
2

r
a

M
4
1

]
h
p
-
p
e
h
[

1
v
0
6
4
7
0
.
3
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Contents

1 Introduction

2 Machine Learning in event generators

2.1 Phase space sampling
2.2 Scattering Amplitudes
2.3 Loop integrals
2.4 Parton shower
2.5 Parton distribution functions
2.6 Fragmentation functions

3 End-to-end ML-generators

3.1 Fast generative networks
3.2 Control and precision

4 Inverse simulations and inference

4.1 Particle reconstruction
4.2 Detector unfolding
4.3 Unfolding to parton level
4.4 MadMiner
4.5 Matrix element method

5 Synergies, transparency and reproducibility

6 Outlook

References

4

5
6
7
9
10
11
12

13
13
15

16
17
17
19
20
22

23

24

25

2

1 Institut für Theoretische Physik, Universität Heidelberg, Germany
2 LPNHE, Sorbonne Université, Université Paris Cité, CNRS/IN2P3, Paris, France
3 Institut für Theoretische Physik, Georg-August-Universität Göttingen, Germany
4 Physics Department, Torino University and INFN Torino, Italy
5 IMAPP, Radboud Universiteit Nijmegen, The Netherlands
6 Nikhef Theory Group, Nikhef, Amsterdam, The Netherlands
7 Center for Cosmology and Particle Physics, New York University, New York, NY USA
8 Center for Data Science, New York University, New York, NY USA
9 Universita di Roma Sapienza, Roma, INFN, Italy
10 Weizmann Institute of Science, Rehovot, Israel
11 Dipartimento di Fisica, Università di Milano and INFN Sezione di Milano, Italy
12 ICEPP, University of Tokyo, Japan
13 Department of Physics, Oklahoma State University, Stillwater, OK, USA
14 Institut für Theoretische Physik, Karlsruher Institut für Technologie, Germany
15 Physik-Department, Technische Universität München, Germany
16 Department of Physics, New York University, New York, NY USA
17 Theoretical Physics Division, Fermi National Accelerator Laboratory, Batavia, IL, USA
18 Department of Physics & Astronomy, UC Irvine, Irvine, CA, USA
19 Department of Physics, University of Cincinnati, Cincinnati, OH, USA
20 IPPP, Physics Department, Durham University, Durham, UK
21 Université Paris-Saclay, CNRS/IN2P3, IJCLab, Orsay, France
22 Fundamental Physics Department, SLAC National Accelerator Laboratory, USA
23 Institut für Experimentalphysik, Universität Hamburg, Germany
24 Deutsches Elektronen-Synchrotron DESY, Germany
25 Univ. Grenoble Alpes, CNRS, Grenoble INP, LPSC-IN2P3, France
26 NHETC, Dept. of Physics and Astronomy, Rutgers University, Piscataway, NJ, USA
27 Department of Physics, TU Dortmund University, Germany
28 CP3, Université Catholique de Louvain, Louvain-la-Neuve, Belgium
29 Dipartimento di Fisica e Astronomia, Università di Bologna, Italy
30 Institut für Physik, Humboldt-Universität zu Berlin, Germany
31 Physics Division, Lawrence Berkeley National Laboratory, Berkeley, CA, USA
32 Berkeley Institute for Data Science, University of California, Berkeley, CA, USA
33 Department of Physics and Astronomy, Vrije Universiteit Amsterdam, The Netherlands
34 Nikhef Theory Group, Nikhef, Amsterdam, The Netherlands
35 Department of Physics, Harvard University, Cambridge MA, USA
36 Institute of Nuclear and Particle Physics, Technische Universität Dresden, Germany
37 Center for Theoretical Physics, MIT, Cambridge, MA, USA
38 Department of Physics & Astronomy, University College London, UK

3

1 Introduction

The deﬁning goal of particle physics is to understand the fundamental nature of elementary
particles and their interactions. The outcome of a particle physics measurement is expressed
in terms of a quantum ﬁeld theory Lagrangian and its parameters. The great experimental
strength of collider-based particle physics is the availability of a huge amount of data and
measurements in combination with a well-controlled environment. The theoretical and ex-
perimental poles are linked through precision simulations, starting from the Standard Model
or a hypothetical Lagrangian, generating particle-level events, and eventually simulating the
detector. The simulation chain realized by the standard LHC event generators [1–5] and il-
lustrated in Fig. 1, should be based on ﬁrst-principles physics rather than empiric modeling.
For these simulations precision and speed are essentially two sides of the same medal. Adding
modern machine learning to the numerics toolbox will help provide the simulations needed
for the LHC Run 3 and HL-LHC [6], as well as future energy frontier machines.

From a fundamental physics perspective there exist three distinctly different kinds of mea-
surements at the LHC. First, basic and purely experimental measurements should be as inde-
pendent of theory considerations and ﬁrst-principle simulations as possible, to avoid expiration
dates. Their problem is that they provide no information about fundamental physics. These
basic measurements beneﬁt from modern machine learning for instance in understanding the
data and calibrating the detectors. A second class of measurements is supplemented with a
fundamental theory interpretation framework. Examples are well-deﬁned inclusive production
rates, like ﬁducial or total cross sections. They can be compared to predictions from pertur-
bative quantum ﬁeld theory. When we expect to ﬁnd agreement with the Standard Model,
modern machine learning can help us in using these measurements to extract parton densities
or improve our Monte Carlo simulations. A third kind of measurement reﬂects our goal to
further our understanding of fundamental physics by comparing data to predictions from per-
turbative or non-perturbative quantum ﬁeld theory. We assume that interesting physics signals
hide in speciﬁc kinematic regions. Here, we can search for deviations between the Standard
Model predictions and experimental results, measure Standard Model parameters or higher-
dimensional Wilson coefﬁcients, and aim for anomalies and eventually a proper discovery.
Such measurements of all possible features in the vast phase space of LHC collisions require
precision simulations, speciﬁcally theory-based event generators. We will show how all of
these aspects beneﬁt signiﬁcantly from the application of modern machine learning methods.
The challenges for event generators are, ﬁrst of all, deﬁned by the increase of the LHC
luminosity and the expected advances in experimental precision and reach. Going from the
−1 suggests that experimen-
Run 2 dataset of 139 fb
tal uncertainties at and below the percent level will become standard and need to be matched
by theory predictions, to allow for any kind of precision measurement. The same increase in
rate will allow us to probe more and more exotic kinematic regions, with the hope of ﬁnding
hints for new particles and interactions. The higher rates and an improved experimental un-

−1 to the projected HL-LHC dataset of 4 ab

Figure 1: Illustration of the LHC simulation chain. The forward direction is discussed
in Secs. 2 and 3, while the inverse simulation is the topic of Sec. 4.

4

detectorsEventsQCDscatteringdecayfragmentationshowerTheoryforwardinversederstanding will also allow us to study more and more complex signatures with an increasing
multiplicity of backgrounds and potential signals. Each of these aspects poses a challenge to
the established event generators, and we will discuss ways modern machine learning can help
us meet them in Sec. 2. Next, we will introduce end-to-end (soup-to-nuts) ML-generators,
similar in structure to ML-detector simulations in Sec. 3. Finally, we discuss conceptual bene-
ﬁts from modern machine learning, for instance related to an invertible simulation chain and
simulation based inference, in Sec. 4. Because the main purpose of this report is to show new,
ML-driven developments in event generation, we refer to the main event generator Snowmass
white paper for a list of references and a detailed discussion of the physics background and
the classical approaches.

Executive summary

Modern machine learning is driving recent progress in event generation, simulation, and infer-
ence for high-energy colliders, as showcased in this contribution to the Snowmass community
report. Like all ﬁelds of fundamental science with a strong numerical foundation, collider
physics is beneﬁting in a transformative way from new ideas, concepts, and tools developed
under the broad umbrella of artiﬁcial intelligence/machine learning (AI/ML) research. Con-
crete improvements in LHC event generation and simulation, as well as new ideas for LHC
analysis and inference, are rapidly leading towards particle-physics-speciﬁc contributions to
applied machine learning. This in turn is inspiring a new generation of high-energy physics re-
searchers bridging theory, experiment, and statistics. To exploit the vast dataset of the coming
LHC runs and optimize the design of a possible future collider, we must sustain the extremely
fruitful exchange between particle physics and AI/ML research. One way to cultivate and capi-
talize on these ideas is to create a world-wide research network for AI/ML-related fundamental
physics, through targeted funding calls and long-term research structures at laboratories and
universities. Such a network would open attractive career paths for young researchers in the
high-energy physics community, establish crucial ties between particle physics and AI/ML re-
search, and foster collaborations between academia and industry.

2 Machine Learning in event generators

Current multi-purpose event generators feature a modular structure, that reﬂects the factor-
ization property of physics aspects at very different relevant energy scales [1–5]. While the
highest energy transfers, i.e. the hard process and QCD parton showers, can be treated by
perturbative methods, phenomenological models are used to account for the hadronization
transition, as well as non-trivial secondary interactions. The increase in perturbative precision
needed to address the physics challenges posed by current and future collider experiments,
adds a sizeable number of more specialized numerical codes to the simulation toolbox. This
includes, for example, dedicated codes to construct and evaluate higher-order tree-level or
loop amplitudes. Modern machine learning techniques can improve all aspects of event gen-
eration, ultimately making it more resource efﬁcient and opening paths to yet more versatile
and accurate predictions. This includes important ingredients to precision predictions such
as parton densities and fragmentation functions, where neural network (NN) techniques are
routinely used already. First steps towards modeling the hadronization process with ML tech-
niques have been presented in [7]. For the tuning of non-perturbative simulation parameters,
including an underlying event model, NN-based approaches have recently shown promise [8].

5

2.1 Phase space sampling

The core of any scattering event simulation is the assumed hard process conﬁguration or par-
tonic scattering event. These are described by QFT transition amplitudes, where the physics
demands of the LHC experiments require us to consider high-multiplicity ﬁnal states and one-
or even two-loop QCD and/or EW corrections. The complexity of the resulting matrix elements
and the dimensionality of their phase space severely challenge the integration of cross sections
and the generation of partonic momentum conﬁgurations. Modern NN-techniques are ideally
suited to assist in these tasks. The standard technique used so far is based on importance
sampling, employing mappings (cid:126)y : V → U ⊆ (cid:82)d for phase space integrals

I =

(cid:90)

V

dd x f (x) =

(cid:90)

U

dd y

f (x)
g(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)x≡x( y)

with

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂ y(x)
∂ x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= g(x) .

(1)

It can be chosen such that f /g ≈ const, to reduce the variance of the Monte Carlo integral
estimate. However, for complex matrix elements and high-dimensional phase spaces it is often
not possible to ﬁnd a single function g that approximates the target function f sufﬁciently well.
Therefore, event generators use a multi-channel approach with independent mappings (cid:126)yi for
each channel i. Deﬁning a total density g(x) = (cid:80)
≤ 1,
where β

i are the channel weights, the phase space integral can be parametrized as

= 1 and 0 ≤ β

(x), with (cid:80)

i gi

β

β

i

i

i

i

I =

(cid:90)

V

dd x f (x) = (cid:88)

(cid:90)

dd x β

i gi

i

V

(x) f (x)
g(x)

= (cid:88)
i

(cid:90)

U

dd yi

β

i

f (x)
g(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)x≡x( yi

)

.

(2)

Two ML-based approaches to phase space integration and event generation can be distin-
guished. The ﬁrst directly hooks into existing phase space integrators and uses trainable maps
given for example by bijective normalizing ﬂows to redistribute input random variables to the
mapping functions (cid:126)yi and better adapt to the integrand [9–15]. After an initial adaptation
phase these integrators can efﬁciently be used for generating weighted or unweighted events.
However, the very expressive NN-transformations can also deal with non-factorizable phase
space structures and correlations. Promising results in terms of efﬁciency improvements and
speed gains have been reported, see for example Tabs. 1 and 2. However, in particular for
high-multiplicity processes with non-trivial topologies the effective gains when comparing to
the established methods can fall below unity, cf. Tab 2. Therefore, next steps will be to better
combine NN-based approaches with multi-channel integrators [9, 13]. For example, one can
allow the channel weights to be phase space dependent, β
(x), and solely start from the
condition (cid:80)

(x) = 1 and 0 ≤ α

(x) ≤ 1,

→ α

α

i

i

i

i

i

I =

(cid:90)

V

dd x f (x) = (cid:88)

(cid:90)

dd x α

i

(x) f (x) = (cid:88)

(cid:90)

dd yi

α

i

i

V

i

U

(x) f (x)
(x)
gi

(cid:12)
(cid:12)
(cid:12)
(cid:12)x≡x( yi

)

.

(3)

Sample ε

uw

top decays
[GeV]

EN

top-pair production

g g → 3g

g g → 4g

ε

uw

EN

[fb]

ε

uw

EN

[fb]

ε

uw

EN

[fb]

Uniform 59 % 0.1679(2)
VEGAS
NN

35 % 1.5254(8)
50 % 0.16782(4) 40 % 1.5251(1)
84 % 0.167865(5) 78 % 1.52531(2)

3.0 % 24806(55) 2.7 % 9869(20)
27.7 % 24813(23) 31.8 % 9868(10)
64.3 % 24847(21) 48.9 % 9859(10)

Table 1: Results for sampling the top decay width, the total cross section of top-pair
production and decay in e+e− collisions, and g g → 3g and 4g production. Shown
are the integral estimate, EN , and the unweighting efﬁciency, ε
uw, for a standard
importance sampler (Uniform), VEGAS, and NN-based optimization [9].

6

In fact, Eqs. (2) and (3) are mathematically equivalent, connected by α
(x)/g(x).
In NN-optimized event generation, Eq.(3) splits the optimization task into learning appropriate
phase space mappings for each channel and training another network to ﬁnd optimal weights
(x) to connect all channels. This separation has two advantages: (i) possible missing correla-
α
tions between the different channels can be described and recovered by the phase-space depen-
dent channel weights, and (ii) the second network allows for a more ﬂexible parametrization
as it does not need to be bijective.

(x) = β

i gi

i

i

A second approach to ML-assisted phase space sampling is based on directly learning the
phase space distribution of events from input training samples, either weighted or unweighted.
Solutions employ autoregressive ﬂows [16], generative adversarial networks (GANs) [17–20],
or variational autoencoders (VAEs) [19]. This motivates R&D to improve training through
differentiable programming; by merging matrix element codes with automatic differentia-
tion [21], i.e. the automatic generation of derivatives of programs that is the backbone of
neural networks software frameworks. The gradients of matrix elements can be evaluated
and used as additional information for training generative models. Initial studies using dif-
ferentiable matrix elements from MADJAX have explored extending normalizing ﬂow training
with schemes uniquely enabled by the ability to automatically compute matrix element gradi-
ents [22], and show promise in terms of improving modeling and reducing the needed scale
of simulated datasets for training.

Closely related activities attempt to facilitate faster event unweighting and reweighting
methods using NN generative models [23, 24] or fast to evaluate NN surrogates for the tran-
sition amplitudes [25].

2.2 Scattering Amplitudes

Perturbative precision calculations use interpolation methods to reduce the evaluation time for
expensive loop amplitudes, deﬁning a task where appropriately designed neural networks can
be expected to outperform standard methods [25–29]. The challenge in NN-based surrogate
models for integrands and amplitudes is to ensure that all relevant features are indeed encoded
in the network at sufﬁcient precision and to establish a reliable uncertainty treatment of the
network training.

A relevant test case are loop-induced amplitudes such as those for

g g → Z Z

and

g g → γγ + jets .

(4)

The application of simple, gradient boosted machines to g g → Z Z highlights that fast interpo-

unweighting eff. ε
process/sampling

uw

n =0

n =1

LO QCD
n =2

n =3

n =4

NLO QCD (RS)
n =1
n =0

W + + n jets

SHERPA 2.8 · 10−1 3.8 · 10−2 7.5 · 10−3 1.5 · 10−3 8.3 · 10−4 9.5 · 10−2 4.5 · 10−3
6.1 · 10−1 1.2 · 10−1 1.0 · 10−3 1.8 · 10−3 8.9 · 10−4 1.6 · 10−1 4.1 · 10−3

NN

Gain

2.2

3.3

1.4

1.2

1.1

1.6

0.91

Z/γ∗ + n jets SHERPA 3.1 · 10−1 3.6 · 10−2 1.5 · 10−2 4.7 · 10−3
3.8 · 10−1 1.0 · 10−1 1.4 · 10−2 2.4 · 10−3

NN

1.2 · 10−1 5.3 · 10−3
1.8 · 10−3 5.7 · 10−3

Gain

1.2

2.9

0.91

0.51

1.5

1.1

Table 2: Unweighting efﬁciencies for V +jets production at the LHC. ‘SHERPA’ relies
on multi-channel importance sampling using VEGAS; ‘NN’ uses a normalizing ﬂow;
‘Gain’ shows the improvement of NN over SHERPA. Results from Ref. [13].

7

Figure 2: Left: comparison of the evaluation times for loop-induced amplitudes
for g g → γγ+ jets. NN-interpolation times include the averaging over en-
sembles for the uncertainty estimate. Figure from Ref. [28]. Right: precision
∆train = |
− 1 for the process g g → γγ j using a Bayesian network
with boosted training, ordered by the size of the amplitude. Figure from Ref. [30].

|2
train

|2
NN

M

M

/|

lation times can lead to signiﬁcant improvements in overall simulation times, if reliable models
can be trained on fewer points than the original Monte Carlos. To control the features of the
amplitude relevant for differential cross sections, separating soft and collinear regions enables
an ensemble of networks to reliably model full one-loop amplitudes for e+e− →≤ 4 jets [27].
In Fig. 2 this scaling is shown for high-multiplicity scattering described by the NJET generator
for g g → γγ+ jets. Simulations for hadron colliders show overall improvements of around a
[28] can be achieved. The right panel of Fig. 2 shows the achievable
factor Ninference
precision on the γγ j loop amplitude from a single Bayesian network with boosted training to
improve the precision [31–34].

/Ntraining

The reliability of the trained network is particularly at risk in divergent regions. How-
ever, these are precisely the phase space regions where the soft and collinear behavior of the
amplitudes is universal and well known. Building the infrared factorization properties into
the NN-based model can lead to substantial improvements for the tree-level e+e− → jets am-
plitude. Figure 3 shows that adopting a factorization-aware parametrization the achievable
precision is brought down to the per-mille level for 5-jet production [29]. While the shown
precision for this process does not translate into a clear improvement of higher-order LHC pre-
dictions, it illustrates how physics-informed network architectures can signiﬁcantly improve

Figure 3: Accuracy for the full model from Ref. [29] for tree-level e+e− → 5 jets
amplitudes. Figure adapted from Ref. [29].

8

456Multiplicity10−210−1100101102103104Evaluationtime(ms)NumericalAnalyticalAveragedmodels0.040.020.000.020.04(train)0102030405060normalizedlargest 1% ANNlargest 10% ANNlargest 100% ANNboosted BNNtrainingFigure 4: Relative integration error for sector one of a 2-loop triangle integral (left)
and a 2-loop box integral known to contain elliptic functions (right) using the stan-
dard pySECDEC algorithm (green), the ML-assisted Λ-glob algorithm (blue) and in-
cluding an additional normalizing ﬂow (red). The lower panel shows the ratios to
the standard method. The ﬁgures are taken from Ref. [35].

the network precision as the key criterion for an application in the LHC simulation chain. Per-
haps of even greater interest would be the use of a single trained model to integrate over a
wide range of kinematic cuts, jet algorithms, PDF sets, scale choices, which could enable a
further order of magnitude in overall performance.

2.3 Loop integrals

Amplitudes beyond the leading order contain loop integrals, and machine learning can im-
prove the calculation of (multi-)loop integrals by optimizing the integrands in Feynman pa-
rameter space [35]. When an analytic solution to these integrals is not feasible, they must be
evaluated numerically. Before attempting a numerical evaluation, the poles of the integrand
In dimensional regularization, ultraviolet and infrared poles can be
need to be controlled.
factorized efﬁciently with sector decomposition. After factorizing these poles, integrable sin-
gularities related, for example, to thresholds, remain. Such poles, located on the real axis in
Feynman parameter space, can be avoided by a deformation of the integration contour into the
complex plane. An automated procedure to do this has already been implemented in standard
tools like SECDEC, FIESTA, and pySECDEC. The deformation of the integration contour is not
unique and can be performed in many ways. In fact, the numerical precision of the integration
can vary by orders of magnitude depending on the chosen contour.

For standard integrals, the contour deformation procedure implemented in pySECDEC works
fast and usually produces satisfactory contours in practice. However, for more complicated
integrals and in speciﬁc phase-space regions, the chosen contour is sub-optimal and can be
optimized signiﬁcantly, see Fig. 4. In this case, ML-assisted, or more speciﬁcally, NN-assisted
algorithms, offer great potential to amplify the precision. Like in the neural importance sam-
pling methods [9, 12, 13, 16] for phase-space integrals, normalizing ﬂows can be used to ﬁnd
a better parametrization of the integration domain. As these contour integrals need to sat-
isfy certain boundary conditions, originating, for instance, from the Landau equations and
Cauchy’s theorem, the NN setup needs to be extended to obey these constraints. Furthermore,
the usage of complex-valued ﬂoats can entail the necessity to construct own implementations

9

10°310°410°510°610°710°8Relativeintegrationerrortriangle2L(Sector1)pySecDec§-glob§-glob+Flow10°210°1100101102103104s/m210°1100101102Ratiomean:5.2mean:1.610°310°410°510°610°710°8Relativeintegrationerrorelliptic2L(Sector1)pySecDec§-glob§-glob+Flow100101102103104s/m210°1100101102Ratiomean:6.7mean:2.1Figure 5: A promising approach to learning parton showers is to use a structure
inspired by the semi-classical approximation as a backbone for a general probability
estimator. In the JUNIPR approach, a recurrent neural network is used to emulated
the Markov-process aspect of a parton shower. Figure taken from Ref. [36].

for objects like gradients of complex determinants occurring during training and optimization.

2.4 Parton shower

It describes the
The parton shower is an essential element of particle physics simulations.
evolution of particles between the hard scale of the collision ∼ 100 GeV to the hadronization
scale Λ
QCD. This evolution is typically modeled as a Markov process where partons evolve semi-
classically, radiating gluons as they move with probabilities determined only by properties of
the parton splitting and perhaps one or two spectator partons in the event. Although the
semi-classical approximation can be justiﬁed in the limit where the daughter particles are
emitted at small angles with respect to the mother, parton showers are used well outside of this
regime. The use of parton showers is thus justiﬁed not by physics but by necessity: computing
the full distribution from ﬁrst principles is computationally intractable. This limitation is an
opportunity for machine learning; perhaps an improved parton shower could be learned rather
than built.

The simulation of parton showers offers an interesting structure compared to other gen-
erative tasks. When simulating entire collision events, as discussed in Sec. 3, commonly a
representation encoding a small and often ﬁxed number of 4-vectors is chosen. Simulating
showers all the way down to calorimeter sensors, or with calorimeter sensors themselves,
yields a much larger number of particles in the ﬁnal state. However, the output nodes of a
generative model can still be identiﬁed with different cells of the physical detector and there-
fore allow architectures that for example use convolutional layers.

Within the semi-classical approximation and even though the probability function at each
branching in the shower is relatively simple, the overall distribution of particles produced is
quite complex. It would be seriously challenging to learn this ﬁnal distribution without some
domain knowledge of its structure [37]. One approach is to scaffold a learnable model over
a semi-classical framework [36, 38], as sketched in Fig. 5. Additionally, network architectures
based on sets or graphs explicitly encoding permutation symmetry of the ﬁnal state particles
have been investigated [39–44].

An alternative way of improving parton shower with ML-methods might be to stick to the
fundamental splitting structure and measure the QCD splitting kernels in low-level observ-
ables. As before, the challenge of generating many particles covering several orders of mag-
nitude in energy is taken care of by the usual Monte Carlo method. A modiﬁed and shower-

10

= h(t 1)h(t)h(t+1)······(cid:98)(cid:63)(cid:81)(cid:96)(cid:105)(cid:63)(cid:28)(cid:77)(cid:47)h(t 1)(cid:95)(cid:76)(cid:76)(cid:44)h(t)(cid:47)(cid:28)(cid:109)(cid:59)(cid:63)(cid:105)(cid:50)(cid:96)(cid:98)k(t)d1k(t)d2(cid:47)(cid:28)(cid:109)(cid:59)(cid:63)(cid:105)(cid:50)(cid:96)(cid:98)k(t)d1k(t)d2(cid:47)(cid:28)(cid:109)(cid:59)(cid:63)(cid:105)(cid:50)(cid:96)(cid:98)k(t 1)d1k(t 1)d2(cid:47)(cid:28)(cid:109)(cid:59)(cid:63)(cid:105)(cid:50)(cid:96)(cid:98)k(t+1)d1k(t+1)d2Figure 6: Comparison between two results for the strange-quark PDF: one where
overﬁtting is clearly present, and one where this is not the case. Figure taken from
Ref. [46].

speciﬁc form of the splitting kernels can be extracted from a combination of QCD predictions
and data using ML-based inference [45]. While this approach has practical advantages, it is
limited by the applicability of the simple splittings picture.

2.5 Parton distribution functions

Parton distribution functions (PDFs), encoding the structure of colliding protons, are vital
for the calculation of hard scattering cross sections at the LHC and appear in several stages
of the event simulation chain, in particular they guide the initial state parton showers and
affect the underlying event activity. The determination of PDFs is a classic pattern recognition
problem: it is known that an underlying law exists (the true analytic form of the PDFs, as
determined by QCD in the non-perturbative regime) but its explicit form is not known, and
it must be inferred from discrete data (the cross-sections of PDF-dependent hard processes),
that moreover are correlated to it indirectly and in a convoluted way. In comparison to more
standard pattern recognition problems, it has two peculiarities. First, the pattern – the set
of PDFs — is a probability, rather than a deterministic outcome. Second, due to the noisy
nature of the input, which is affected by both experimental and theoretical uncertainties, with
a complex correlation pattern, the ﬁnal deliverable is a probability distribution of possible
results. Hence, one is delivering a probability distribution of probability distributions.

The way to approach this problem as a machine learning challenge was ﬁrst suggested
long ago [47]: the basic idea is to deliver a Monte Carlo ensemble of machine learning mod-
els, such as neural networks, that provide the desired representation of a probability of prob-
abilities. The successful implementation of this idea has led to the NNPDF family of proton
PDF determinations [46, 48–50] as well as to variants in the context of polarised PDF [51]
and nuclear PDF [52, 53] global analyses. The current implementation frontier, which has led
to the recent NNPDF4.0 determination, involves a suite of contemporary machine learning
methods and tools, speciﬁcally cross-validation to avoid overtraining, hyperoptimization [54]
combined with K-folding for the automatic selection of the methodology, feature scaling of the
input for the optimization of the neural networks used as basic underlying model [55], and
GAN-enhanced compression for ﬁnal efﬁcient delivery [56, 57].

The current main challenge remains the maximal optimization of the extraction of available
information while avoiding overﬁtting, and the generalization to cases in which information
is scarce or altogether absent, such as extrapolation to kinematic regions where there are
no data. This is the physically most interesting case, as these are the regions where new

11

0.20.40.60.8x0.000.020.040.060.08xs(x)s at 1.65 GeVNot OverfittedOverfittedFigure 7: Fragmentation functions of up (left) and charm quarks (right) into charged
pions as a function of the time-like momentum fraction z, comparing the results of
two approaches (MAP and NNFF) based on machine learning techniques. Figure
taken from Ref. [58].

physics is being searched for, and also a challenge at the frontier of machine learning. While
several machine learning tools have been implemented with the aim of preventing overﬁtting,
conﬁrming whether the PDF resulting of a ﬁt is indeed free of overﬁtting still relies – at least
in part – on the ﬁtter’s accumulated knowledge of PDFs. To illustrate this point, Fig. 6 shows
a comparison of the strange-quark PDF xs(x, Q) at Q = 1.65 GeV, both for a good ﬁt and a
clearly overﬁtted alternative. The development of reliable quantitative measures of the degree
of overﬁtting is a challenge, both within the context of PDF determination and more in general
in machine learning, and it is a topic of ongoing research.

2.6 Fragmentation functions

Fragmentation functions (FFs) are the time-like equivalent of PDFs and encode the probabili-
ties associated to the transition between partons produced in the hard-scattering and speciﬁc
types of hadrons. Being based on the perturbative QCD factorization framework, FFs represent
an alternative strategy to model partonic hadronization as compared to the phenomenological
models available in most MC event generators. FFs can be determined from a global analysis of
hard-scattering data from electron-positron collisions, semi-inclusive deep inelastic scattering,
and proton-proton collisions (RHIC and LHC) with identiﬁed ﬁnal-state hadrons.

(z, Q0

A phenomenological analysis of FFs requires introducing a parametrization for their initial-
(h)
), where i is a partonic index
scale (Q0) dependence with the momentum fraction z, zD
i
and (h) a hadronic label. To remove theory bias and model-dependence in the determination
of FFs, machine learning techniques can be adopted [58–61]. Feed-forward neural networks
(h)
), whose weight and threshold
are deployed as universal unbiased interpolants for zD
i
parameters are obtained from a log-likelihood maximization by comparison with experimen-
tal data. This approach can be combined with the Monte Carlo replica method, originally
deployed for PDFs [62], to estimate and propagate the uncertainties from the input data to
the output FFs. The basic strategy is to generate Nrep replicas which sample the probabil-
ity density associated to the data, and then train a separate neural network to each of these
replicas. The spread of the resulting networks (i.e. 68% CL intervals) provides then a robust
estimate of the uncertainties associated to the FFs.

(z, Q0

Fig. 7 displays a comparison between FFs determined in two approaches (MAP and NNFF)
based on machine learning techniques. We show the FFs associated with the transition of up
and charm quarks into charged pions (π+ + π−) as a function of the time-like momentum
fraction z. The bands represent the corresponding 68% CL ranges. It is worth emphasizing

12

that the resulting shapes, given the outcome of the NNs, are completely driven by the data,
with no speciﬁc models (more or less inspired by QCD) assumed. The combination of the FFs
(h)
) obtained in this manner with higher-order perturbative QCD calculations provides
zD
i
precise and accurate predictions for hard-scattering processes including identiﬁed hadrons in
the ﬁnal state, which are important for many key phenomenological applications.

(z, Q0

3 End-to-end ML-generators

In addition to applying a wide range of machine learning tools to improve the modules of clas-
sic event generators, we can train generative neural networks to directly generate events, at
parton level and with or without detector effects [6]. End-to-end or, better, soup-to-nuts ML-
generators have to be developed together with the established generators and serve as studies
for phase space generators, enable inverted simulations, provide datasets for phenomenologi-
cal analyses, and allow us to efﬁciently ship event samples. Their advantages include training
on data combined with simulations, manipulation of event samples [63], or post-processing of
MC data for example to unweight events [16, 23, 25]. Finally, they deﬁne useful benchmarks
for conceptual work on uncertainty estimates for generative neural networks.

The work horses behind ML-generators are GANs, VAEs, optimal-transport-based proba-
bilistic autoencoders, normalizing ﬂows, and their invertible network (INN) variant. Given
the interpolation properties of neural networks and the beneﬁts of their implicit bias in the
applications described in Sec. 2, we can quantify the ampliﬁcation of statistics-limited training
data through generative networks [64, 65].

3.1 Fast generative networks

Theory-driven ML-generators at the parton level [17, 68] can be combined with experiment-
driven fast detector simulations [69–80] into single generative networks [67,81–85], provided
we have sufﬁcient control over the network and its uncertainties. Single, soup-to-nuts sim-
ulation networks are inspired by the fundamental goal of the detection process, namely to
reconstruct parton-level information as accurately as possible.

Figure 8: Distributions of the invariant mass (left) and transverse momentum (right)
of the t¯t system in pp → t¯t generated using MC@NLO. The true distribution (red)
is compared with the normalizing ﬂow distributions excluding (blue) or including
(green) negative event weights. Figure from Ref. [66].

13

1234dσdmtt[pb/GeV]×10−2TrueNoWeightsFlow4005006007008009001000mtt[GeV]0.81.01.20.020.040.060.080.100.12dσdpT,tt[pb/GeV]TrueNoWeightsFlow050100150200250300pT,tt[GeV]0.81.01.2Figure 9: Visualization of the transformation from parton-level theory-space to re-
. Left: learned transformation of OTUS’s decoder,
constructed data-space,
(x | z). Right: true transformation from simulated samples, for comparison. Col-
pD
ors in the
for a given sample. Figure from
Ref. [67].

projection indicate the source bin in

→

Z

Z

X

X

X

to the phase space

Comparing different generative network architectures, we start with highly expressive
VAEs. They can be trained to generate events at the parton level, without or with fast de-
tector simulation, by maximizing a lower bound of the data likelihood through variational
inference (ELBO). The model consists of a decoder p(x|z) which maps from a latent space
, and an encoder q(z|x) which is a variational approximation to the
Z
inverse of p(x|z). In practice, it is difﬁcult to simultaneously optimize the separate compo-
nents of the ELBO and the VAE performance can be improved by weighting the KL-divergence
in the loss function term by a factor β. The B-VAE [19] is characterized by the limit β (cid:28) 1
and a strong preference for the reconstruction loss. After optimization, the Gaussian latent
distribution is replaced by a buffer which consists of the latent distribution derived from train-
ing events. This model simultaneously achieves a highly-optimized reconstruction loss, but
with a closely-matched and non-Gaussian latent distribution. While VAEs are very expressive
probabilistic models, the approximate nature of the ELBO and the need to balance the two
components of the loss function can become limiting factors.

Similarly, GANs can extract and reproduce the phase space density of LHC events. While
technically the difference between training on events without or with detector effects is negli-
gible, parton-level events are more challenging when it comes to sharp kinematic features like
Breit-Wigner mass peaks. GANs generically do not achieve the necessary precision for such
features, so they have to be enhanced, for example with a targeted MMD loss [17]. The main
challenge of GANs is the precision they can achieve in the underlying phase space density
while ﬁnding a Nash equilibrium.

Finally, normalizing ﬂows avoid some of the limitations of the above architectures for LHC
event generation [66, 68, 86]. At the cost of some ﬂexibility, they offer a direct evaluation of
the likelihood without having to resort to variational inference. They start from a latent space
.
and apply a series of bijective transforms, with tractable Jacobian, to the phase space
X
Z
While the expressivity of the model may in some cases be limited, the advantage of a tractable
likelihood is signiﬁcant. Flows can be trained on weighted events, including negative weights,
through a simple modiﬁcation of the loss [66]. Figure 8 illustrates their performance applied to

14

pp → t¯t events at the parton level, including shower evolution, generated with MC@NLO. In
addition, normalizing ﬂows come with signiﬁcant advantages in controlling their performance
and quantifying uncertainties, as discussed in the next section. Their invertible structure is
useful for many LHC-applications, including anomaly detection or related density estimation
tasks [87–90].

X

An attractive application of soup-to-nuts networks can be targeted using Optimal Transport-
based probabilistic autoencoders [67]. Their structural advantage is that they learn the map-
ping from parton-level information in theory space,
, to detected and reconstructed objects
, without requiring paired event samples, {z, x}. The probabilistic autoen-
in data space,
coder’s latent space is identiﬁed with a physically meaningful representation of parton-level
theory-space information, so the encoder and decoder networks deﬁne a simulator mapping,
. Properties of the OT-based method encourage
Z
the encoder and decoder to be conditional mappings, effectively sampling from the proba-
(x|z), respectively. Over many samples, these distributions
bility distributions pE
will marginalize to the appropriate theory-space and data-space priors, p(z) and p(x), respec-
tively. Alternative methods to encode an unfolding mapping in neural networks are discussed
in Sec. 4.

, and an unfolding mapping,

(z|x) and pD

→

→

Z

Z

X

X

Despite having no training pair information, OTUS’s learned mappings exhibit physical-
behavior, even picking up on invariant masses which were withheld during training. This sug-
gests that further development in this direction should produce physically meaningful map-
pings, even if relations are missed or unknown, and therefore not included in the training
process. On the other hand, providing known relations as inductive biases on the data inputs,
network architectures, or loss functions will likely improve performance. Figure 9 depicts the
joint distribution and marginals of OTUS’ trained simulator as well as the true simulator for
one test-case. Despite OTUS only having information about marginal-matching during train-
ing, the decoder network learns a mapping which is qualitatively similar to the true simulator.

3.2 Control and precision

If we use neural networks to encode theory predictions for the LHC, we need to ensure that
all relevant phase space features are described with the required precision [91]. For neural
networks, this problem can be split into two distinct tasks: ﬁrst, we need control over the
relevant phase space features, so the network does not interpolate over relevant, but narrow
phase space regions. Second, we have to estimate the precision with which the network has
learned the underlying phase space density.

Neural networks work much like a ﬁt and not like an interpolation in the sense that they
do not reproduce the training data faithfully and instead learn a smooth approximation [64,
65]. This is where we can gain some intuition for a NN-uncertainty treatment. For a ﬁt,
uncertainties on the training data are crucial information in the loss function. We then monitor
the ﬁt quality and ensure that the ﬁt is reliable over the entire phase space.

To guarantee that all relevant features are encoded in a generative network, we can follow
the GAN inspiration and train a simple discriminator network to identify and quantify devia-
tions between training and generated data. As a post-processing step such a discriminator can
be used to reweight the events from the generative network [24, 68, 92]. In the GAN spirit we
can incorporate the discriminator into the generator training, either through adversarial train-
ing searching for a Nash equilibrium, or through alternative approaches for a normalizing ﬂow
generator. Such a joint training will improve the generator, provide an uncertainty estimate,
and prepare any remaining information in the discriminator for reweighting, as illustrated for
Z+jets production at the parton level in Fig. 10.

Once we know that the neural network describes all features, we determine how well
it does. This can be done with Bayesian networks, where the learned network weights are

15

Figure 10: Illustration of a complete control and uncertainty treatment for generative
networks applied to LHC event generation and simulation. Figure from Ref. [68].

replaced by learned network weight distributions [31,32]. Bayesian network approaches have
been shown to describe uncertainties in regression [34] and classiﬁcation [33] tasks, and the
concept can be expanded to generative networks [86]. For generative networks we can assign
a training-related uncertainty in the underlying phase space density to the (unit) weight of
each event. In Fig. 10 we see, for instance, the increasing uncertainty in the kinematic tail,
driven by a lack of training data.

We often know systematic or theoretical limitations of describing certain kinematic regimes.
In that case we augment the training data, representing this uncertainty through an additional
parameter in event weights. We train the generative network conditionally on this parameter,
either in a deterministic or a Bayesian setup, and generate events either for a given parameter
or sampling over it. Again, this approach is illustrated in Fig. 10, where a directly affects the
pT -distribution of the leading jet and enters many other observables through kinematic corre-
lations. We see that its effect is larger than the uncertainty from the Bayesian network for the
individual a-values. This ﬁrst attempt of a comprehensive uncertainty treatment for genera-
tive networks will allow us to build conﬁdence in the applications of generative networks to
LHC simulation and inference.

4 Inverse simulations and inference

Monte Carlo simulations based on ﬁrst principles have allowed us to properly understand
essentially all aspects of LHC data. The price to pay for an extremely fast and reliable forward
Monte Carlo simulation chain is that the corresponding inverse simulation is not feasible in
practice. ML-based simulations can be built symmetrically, for instance INNs encode a bijective
mapping between two physics spaces linking different levels of the simulation chain illustrated
in Fig. 1 [93, 94]. Similarly, we can relate different levels of the simulation chain through
a reweighting procedure working on the full respective phase spaces and accounting for all
correlations [95]. Moreover, as ML-based simulations are often differentiable, we can use

16

10−510−3normalizedZ+1jetexclusiveReweightedTrain0.11.010.0δ[%]0.91.01.1wD1.01.2BINNTruth050100150pT,µ1[GeV]12ConditionedTrutha∈[0,6,12]their gradients to probe and learn about distributions on phase space [96]. Finally, we can
construct generative inverse simulations with conditional versions of the respective forward
generative networks [94, 97, 98]. This last approach is based on progress with soup-to-nuts
ML-generators and their essentially identical network architectures.

4.1 Particle reconstruction

The ﬁrst stage of the inverse problem uses the set of energy deposits in the detector to recon-
struct the set of particles present at the ﬁrst interaction with the detector, that is, following
hadronization. In its fullest sense, reconstruction also involves the prediction of the particles’
properties, in particular, their class and momenta. The difﬁculty of this task stems from the
busy LHC environment caused by pileup interactions and the inherently collimated signatures
associated with jets. Traditional particle ﬂow (PF) algorithms rely on parameterized schemes
for merging and splitting to disentangle overlapping calorimeter cell clusters as well as track-
based subtraction to infer the contribution from neutral particles.

A series of publications [99–101] have established the potential for ML-based reconstruc-
tion to go beyond traditional PF algorithms. In Ref. [99], particle reconstruction was recast
as a computer vision problem using state-of-the-art ML architectures including U-net, graph
neural network (GNN) and DeepSets. A simpliﬁed dataset was used comprising overlapping
pairs of charged and neutral pions in a 6-layer calorimeter block.
In comparison to a tra-
ditional PF algorithm, the ML models regress the component of neutral energy better by a
factor of two to four in terms of resolution. The study also ﬁnds signiﬁcant improvements via
a super-resolution approach (see also Ref. [102]), where the network is trained to predict a
corresponding calorimeter signature with higher granularity.

This proof of concept has been extended to particle reconstruction in more realistic environ-
ments resembling multiple pileup interactions in a full-coverage simulated detector [100,101].
In both cases, GNN architectures are employed for their ability to handle the complexity of
detector data: variable numbers of input and target entities, lack of ordering, irregularity of
detector components, and sparsity of “pixels”. Moreover, GNNs are able to leverage the spatial
relationships between calorimeter cells alongside their input features to optimize the predic-
tion tasks.

Based on these developments, it can already be anticipated that ML methods will take a key
role in particle reconstruction at future runs of the LHC, especially to handle HL-LHC condi-
tions. GNN-based models in particular show potential to outperform current PF algorithms for
particle identiﬁcation and regression while opening new possibilities such as super-resolution
and resolving neutral particles inside of jets. Finally, the learned deep latent representation of
detector information, which underlies the prediction tasks, should serve as a more expressive
input format for both event classiﬁcation and downstream tasks in the inverse problem.

4.2 Detector unfolding

While the physical processes behind an LHC collision are described by fundamental physics
and are therefore universal, the observed data depend in an intimate way on the technical de-
tails of the detector. Detector effects like phase space coverage, detection thresholds, particle
reconstruction, efﬁciencies, or calibration induce not only resolution smearing in the measure-
ments, but can lead to systematic deviations between the properties of particles reaching the
detector and the objects reconstructed from actually measured data. For individual experi-
ments, these detector effects differ greatly and can only be estimated by the collaboration. It
is therefore essential for future interpretations of a measurement to unfold detector effects
so that we can compare measurements by different experiments to each other and to theory
predictions.

17

Figure 11: An illustration of classiﬁer-based (OmniFold) and density-based (cINN)
unfolding for the two-dimensional space of N -subjettiness variables τ
2. The
right plot shows that since the unfolding is done simultaneously and unbinned, we
can produce a measurement of the widely-used N -subjettiness ratio for free. Figure
from Ref. [103].

1 and τ

Traditional approaches to unfolding are based on matrices connecting binned particle-level
distributions at truth level with histograms of corresponding detector-level observables. While
the folding or convolution of detector effects with kinematic distributions at particle level is
possible with Monte Carlo simulations, the inverse direction often suffers from instabilities and
scales poorly for high-dimensional phase spaces. The limitation to low-dimensional represen-
tations requires an unwanted pre-selection of interesting observables. Finally, the matrix-based
approach requires ﬁxed bin sizes, which limits the re-optimization options for future analyses.
ML-approaches establish high-dimensional and binning-independent unfolding. We can
distinguish two fundamentally different concepts [103]: a classiﬁcation-based approach to
reweight a Monte Carlo simulation with the learned likelihood ratio of data and simula-
tion [95]; and a complementary approach that learns directly the probability density at particle-
level [94]. Figure 11 illustrates that both approaches can perform unbinned unfolding in mul-
tiple dimensions.

Classiﬁcation-based approaches start by learning the likelihood ratio between data and
simulation at reconstruction level [95, 104, 105]. Using matched event pairs at the truth and
reconstruction levels, the resulting weights are pulled to the particle level. Next, a classiﬁer
learns the likelihood ratio of the weighted and unweighted distributions at particle level to
replace an event-based weight with a generalized weighting function. After several iterations
of weight updates, the algorithm converges to an unfolded distribution which is compatible
with the observed measurement.

Density-based approaches build on generative networks that predict probability conﬁgu-
rations of truth-level events given a detector-level measurement. They are trained on pairs
of reconstruction- and particle-level events from Monte Carlo simulations, to learn a direct
mapping between both levels. Unfolding built on generative adversarial networks has been
shown to work on kinematic distributions [93, 98]. Event-wise unfolding requires a meaning-
ful probabilistic treatment, which can be achieved with conditional normalizing ﬂows [94,96],
the kind of generative networks which also allows for the uncertainty treatment discussed in
Sec. 3.2. This unfolding method yields calibrated probability distributions for each measured
event. It admits multiple approaches; one approach frames unfolding in terms of learning a

18

0.000.050.100.150.200.250.300246810Number of Events / 104Truth: Pythia 8.243Truth (alt.): Herwig 8.1.5Detector: Delphes 3.4.2 (CMS)Z+jet: pZT>200 GeV, R=0.4TruthOmniFoldcINN0.000.050.100.150.200.250.3020.751.001.251.50Ratio to Truth0.000.250.500.751.001.251.501.752.0001234567Number of Events / 104Truth: Pythia 8.243Truth (alt.): Herwig 8.1.5Detector: Delphes 3.4.2 (CMS)Z+jet: pZT>200 GeV, R=0.4TruthOmniFoldcINN0.00.51.01.52.021=2/10.751.001.251.50Ratio to TruthFigure 12: Unfolded parton-level distributions for the process pp → Z(cid:96)(cid:96)Wj j
+jets
using a cINN. The unfolding covers detector effects as well as additional jet radiation.
Figure from Ref. [94].

conditional density of particle-level quantities conditioned on reconstructed inputs [94], while
another approach frames unfolding as an empirical Bayes / maximum marginal likelihood /
data-informed prior learning problem [96].

Because classiﬁcation-based and density-based unfolding techniques have distinct strengths
and weaknesses, the natural next step will be to combine the two methods to beneﬁt from
both. While there is an extensive R&D program required to integrate both methodologies
and to achieve precision, these tools are starting to be applied to data analysis in collider
physics [106]. Looking ahead, it is clear that future versions of these tools will play an im-
portant role in the data analysis of future colliders. Unfolded differential cross sections are
one of the main data products from collider experiments. By performing the unfolding with as
much information as possible, we ensure that the measurements achieve the maximal preci-
sion, making the best use of the data. Furthermore, high-dimensional and unbinned unfolding
ensures that these data products are ‘future proof’ in the sense that binning and even observ-
ables can be chosen post-hoc [103]. This enables downstream data analysis long after the data
were published, including when new theoretical insights are available.

4.3 Unfolding to parton level

Once we control ML-unfolding of detector effects, we can target other parts of the simulation
chain shown in Fig. 1 and invert them for a given LHC analysis. To probe the kinematics of
a hard scattering process we can use neural networks to unfold QCD jet radiation and heavy
particle decays to study the production kinematics of top quarks, electroweak gauge bosons, or
the Higgs without binning and with full correlations. Such measurements are standard in top
physics and provide the ideal input to global SMEFT analyses. Once we know the parton-level
conﬁgurations for a given observed event, we can use NN-techniques to evaluate observables
like CP-sensitive angular correlations in their original reference frames.

The inversion of QCD radiation or decays relies on the same classiﬁcation or generative
networks as detector unfolding. For instance, we can train a normalizing ﬂow to map random
numbers to the parton-level phase space, under the condition of a given detector-level event.
The underlying model is encoded in the forward simulation chain used to train the network.
Part of it is the assumed hard process, including the number of jets which are part of the hard
scattering and do not get unfolded. When analyzing an event and sampling into parton-level
phase space, we extract a probability distribution of parton-level conﬁgurations [94], which
we can use to deﬁne observables suitable for standard analyses.

19

0.00.20.40.60.81.01.21σdσdpT,W[GeV−1]×10−22jetincl.PartonTruthPartoncINNDetectorTruth050100150200250300350400pT,W[GeV]0.81.01.2cINNTruth6065707580859095100MW,reco[GeV]10−410−310−210−11σdσdMW,reco[GeV−1]cINNvsTruth2jet3jet4jetOne challenge for such analyses are combinatorics. For the hard scattering q¯q → Z(cid:96)(cid:96)Wj j
and up to two additional QCD jets we ask how well cINN-unfolding extracts the W -kinematics.
In the left panel of Fig. 12 we illustrate how the network reproduces the momentum of the
decaying W -boson. The relation between the up to four jets and the two partonic quarks from
the W -decay is learned by the network.
In the right panel of Fig. 12 we show the recon-
structed W -mass stacked for different numbers of jets. The network resolves the underlying
combinatorics such that the W -widths for the different jet multiplicities are identical, all by by
accessing correlations combined with the truth information from the forward simulation. This
corresponds to results from a systematic study which shows that deep networks outperform
classical approaches to solving the combinatorics in the reconstruction of top-quark ﬁnal states
signiﬁcantly [107].

High-level observables encoded into neural networks will ﬁnd their way into standard
experimental analyses. They are motivated by existing top-sector measurements, and using
NN-techniques will simplify their use considerably. Moreover, the comprehensive uncertainty
treatment discussed Sec. 3.2 and the merged classiﬁcation-based [95] and density-based tech-
niques from Sec. 4.2 can be applied to any part of an inverted or unfolded simulation chain.

4.4 MadMiner

The relation between data x and physics parameters θ is, fundamentally, described by the
likelihood function or normalized fully differential cross section, which we can predict in a
factorized form,

p(x|θ ) = 1
σ(θ )

dσ(x|θ )
dx

.

(5)

While we can predict this likelihood at the detector level using the standard, forward simula-
tion tools, we can only compute it in a closed form at the parton level. This challenge in the
relation of simulations and inference is where neural networks might lead to transformative
progress.

Inspired by the standard simulation chain we can assume that the likelihood of Eq.(5)

approximately factorizes into the form [108, 109]

p(x|θ ) =

(cid:90)

(cid:90)

(cid:90)

dzd

dzs

dzp p(x|zd
(cid:124)

) p(zd

) p(zs
|zs
(cid:123)(cid:122)
p(x,z|θ )

|zp

) p(zp

.

|θ )
(cid:125)

(6)

Here we integrate over latent variables z, where zd characterize the detector effects, zs the par-
ton shower and hadronization, and zp the partonic phase space including helicities, charges,
and ﬂavors, etc. Given the typically large number of latent variables, it is unrealistic to inte-
grate over them or evaluate the joint-likelihood p(x, z|θ ). However, it is possible to calculate
the joint likelihood ratio relative to a reference point in terms of the ratio of squared matrix
elements from parton-level generators [108–112],

r(x, z|θ ) = p(x, z|θ )
p(x, z|θ

ref

)

=

|θ )

p(zp
|θ
p(zp

ref

∼

|

)

|
M

|2(zp
|θ
|2(zp

|θ )

ref

M

)

σ(θ
ref
σ(θ )

.

)

(7)

The starting point to new ML-methods is to construct functionals in terms of the joint likelihood
ratio r(x, z|θ ), which are minimized by the true likelihood or likelihood ratio function [113,
114]. The result of this training are neural networks that approximate the true likelihood
ratio r(x|θ ). Given such a neural network, established statistical techniques can be used to
construct conﬁdence limits in parameter space.

20

Figure 13: Expected sensitivity of a MadMiner based analysis for t¯t H production,
probing SMEFT coefﬁcients (left) and the CP-structure of the top Yukawa coupling
(right). Figures taken from Ref. [112] and Ref. [115], respectively.

Note that here simulation-based inference provides the primary statistical model, i.e. the
probability model p(x|θ , ν) that describes the dependence on the data x, the parameters of
interest θ , and the nuisance parameters ν, even when the data is high-dimensional and tra-
ditional modeling approaches are inadequate. The publication of the trained network in a
re-usable form, as discussed below, can thus be of great beneﬁt for an optimal use of experi-
mental results [116]. This approach is separate from tools like DNNLikelikood [117], which
aims at approximating likelihoods derived from traditional approaches to model building, us-
ing libraries like RooFit.

Instead of the full likelihood function, one can also use the score t(x|θ ) = ∇θ log p(x|θ )
to deﬁne statistically optimal observables at the detector level. This approach is motivated by
an expansion of the log likelihood ratio around θ

ref,

log r(x|θ ) = log r(x|θ

ref

) + t(x|θ

ref

) (θ − θ

ref

) + · · ·

(8)

ref

For parameter points close enough to θ
ref the score components are the sufﬁcient statistics,
so for measuring θ knowing t(x|θ
) is as powerful as the full likelihood. Since the score
is deﬁned through the likelihood function, it is also intractable. However, similarly to the
approach discussed above, we can train a neural network on a suitable loss function such that
it will converge to the score. The trained network will now represent the optimal observable. In
a next step, the likelihood can be determined for instance with simple histograms of the score
components [112, 118]. This approach requires only minor changes to established analysis
pipelines. Alternatively, the scores can be used to evaluate the Fisher Information and set
limits based on the Cramer-Rao bound [112]. One challenge with training using the score is
that the relevant gradient information of the matrix elements must be accessible for training
the neural network, but this information is typically only accessible for a subset of parameters
with analytic dependence that facilitates easy gradient estimation. One approach to enable
score based training for any parameter is through differentiable programming; when matrix
elements are merged with automatic differentiation frameworks, the required gradients can
be computed automatically with relatively small additional computational overhead. Case
studies using differentiable matrix elements from MADJAX for score based training successfully
trained networks for inference on parameters that were inaccessible without differentiable
matrix elements [22].

21

0.40.20.00.20.4cu0.60.30.00.30.6100cGRatepT,SALLYALICES60300+30+600.60.81.01.21.4t Searches in pptth @ HL-LHC (68% CL)Di-leptonic tthSemi-leptonic tthHadronic tthCombinationThe previously outlined inference strategy has been fully automated in the MadMiner
tool [112, 119]. The increase in physics sensitivity relative to a total rate or single kinematic
distribution is illustrated in Fig. 13. In the left panel we consider t¯t H production to constrain
the two SMEFT Wilson coefﬁcients cu and cG. In the right panel we consider the same process
to constrain CP-violation in the top-Higgs coupling, as parameterized by the magnitude κ
t and
CP-phase α of the top Yukawa coupling [115, 120].

4.5 Matrix element method

Inverting the entire simulation chain in Fig. 1 allows us to extract the transition amplitude for
an observed event and relating it to a theory prediction. This so-called matrix element method
(MEM) can be used to estimate free parameters of a fundamental physics model and has, for
instance, been applied for measure the top mass. Being deﬁned on the level of individual
events, it is in particular suitable for low-statistics signals, where an optimal exploitation of all
kinematic features is critical.

The MEM relies on our ability to extract the likelihood for detector-level events as a func-
tion of a model parameter θ , as an ML-application through density-based unfolding or an
inverted simulation. Extending the discussion in Sec. 4.4, the transition amplitude as a func-
tion of detector-level phase space is unknown, but it can be calculated at the parton level. The
two phase spaces can be related by transfer functions T ((cid:126)x, (cid:126)z), probabilities to observe parton-
level conﬁgurations (cid:126)z as detector-level signatures (cid:126)x, as part of the forward simulation. In the
N -event likelihood they appear as

(θ ) =

L

N
(cid:89)

i=1

p((cid:126)x

(i)|θ ) =

N
(cid:89)

i=1

1
(θ )

σ

ﬁd

dl σ(θ )
dx1... dxl

(cid:12)
(cid:12)
(cid:12)
(cid:12)(cid:126)x (i)

=

N
(cid:89)

i=1

1
(θ )

σ

ﬁd

(cid:90)

dmz

dmσ(θ )
dz1... dzm

(i)

T ((cid:126)x

, (cid:126)z) .

(9)

Note, the dimensions of the parton-level and detector-level phase spaces are different. For
instance, longitudinal neutrino momenta are unobservable, while additional jets have to be
included with higher-order QCD corrections. Existing approaches model the transfer functions
heuristically, and for non-trivial cases the numerical convolution is impossible. The general
form of Eq.(9) indicates ways of enhancing the accuracy of the matrix element method: ﬁrst,
higher-order corrections can be included at parton level, for instance using the MEM@NLO
program. Second, general and highly non-Gaussian transfer functions can account for parton
shower, hadronization, detector resolution, acceptance, and efﬁciency, as well as a possible
mismatch between theoretically described and actually measured quantities, event by event.
For an ML-based inverse simulation, (cid:126)x → (cid:126)z, we can assume that every detector-level event
comes from some parton-level conﬁguration, including an efﬁciency, so it is easiest to approxi-
mate the transfer function by a factorized form T ((cid:126)x, (cid:126)z) = p((cid:126)z|(cid:126)x) ε((cid:126)x). The normalization ε((cid:126)x)
could in principle also depend on θ , just like the ﬁducial cross section. With the factorized
transfer function, Eq.(9) becomes

(θ ) =

L

≡

N
(cid:89)

i=1
N
(cid:89)

i=1

ε((cid:126)x (i))
(θ )
σ

ﬁd
ε((cid:126)x (i))
(θ )
σ

ﬁd

(cid:90)

dmr

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂ (z1, . . . , zm
∂ (r1, . . . , rm

)
)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

p((cid:126)z|(cid:126)x

(i)) dmσ(θ )
dz1... dzm

(cid:173) dmσ(θ )
dz1... dzm

(cid:183)

(cid:126)z∼p((cid:126)z|(cid:126)x (i))

.

(10)

The convolution integral is now reduced to an expectation value of the differential cross sec-
tion with respect to the probability density p((cid:126)z|(cid:126)x).
In analogy to Sec. 4.2 or Sec. 4.3, the
partonic conﬁgurations (cid:126)z for a given detector event (cid:126)x (i) can be sampled according to p((cid:126)z|(cid:126)x) by

22

a conditional normalizing ﬂow or INN. This way, density-based unfolding will allow us to make
optimal use of the statistical power of the MEM, exploiting the full and correlated event kine-
matics event by event for critical LHC observables like the top mass, the Higgs self-coupling,
or CP-violating phases.

5 Synergies, transparency and reproducibility

A key paradigm in the development of simulation tools for high-energy collider experiments is
publicly accessible open source software. The versioning of code releases and the reproducibil-
ity of predictions is vital for a reliable analysis and interpretation of collider data. As we have
seen in the previous sections, ML-methods are entering all aspects of the simulations chain at
high pace. They range from initial proof-of-concept applications to well established use cases
with largely consolidated techniques, for example in the determination of parton densities.

Machine learning models efﬁciently encode arbitrary decision functions of a given set of
inputs, and thus offer a chance to easily exchange complex relations. This might correspond
to the value of a scattering matrix element given a set of momenta, or probability models
from simulation-based inference, like MEM or MadMiner. The sharing of neural networks
used for various generative or discriminative tasks will be of central importance and should
be further extended. This will allow researchers to critically examine and build upon previous
results more easily, enable synergies between different use cases, and facilitate reproducibility
of results.

Successfully sharing a machine learning model entails two challenges: (i) sharing the
model itself, including architectures, software versions, and weights; and (ii) sharing data
it can be used on. Exchanging models is technically relatively straightforward and several
corresponding tools exist, for example Open Neural Network Exchange (ONNX). It allows the
exchange of neural networks and BDTs between training frameworks.

Suitable input data poses the more difﬁcult problem. On the side of results by large col-
laborations, this adds additional weight to the ongoing move towards publishing open data
along with measurements. Containerization, as enabled by software tools like Docker can be
useful in bundling the correct versions of different software packages used for data processing
and machine learning in a coherent fashion.

An opportunity exists in the realm of phenomenological studies based on the DELPHES
detector simulation [121]. Here a common speciﬁcation on how quantities are translated into
the inputs to machine learning algorithms might — together with publishing the ML models —
boost sharing and meaningful exchange. Another interesting angle are generative models. As
these do not need data to evaluate, sharing the architecture and weights is already sufﬁcient.
Generative networks themselves can even be used as an efﬁcient alternative way of sharing
simulated data.

Publication of ML models for their reuse is not yet standard in the particle physics com-
munity. Examples where trained networks have been published in ONNX format for future
reuse are the DNNLikelihood [122], a package for cross-disciplinary training of discriminator
networks [123], and the ATLAS search for R-parity-violating supersymmetry [124, 125], the
latter also being available in the ATLAS SimpleAnalysis framework. However, detailed doc-
umentation for instance of the input variables is missing. Further development is strongly
encouraged for, e.g., the purpose of analysis preservation [126, 127], and in general for the
implementation of the Findable, Accessible, Interoperable, and Reusable (FAIR) principles for
scientiﬁc data management [128] of ML models. An example for a dataset with special em-
phasis on these aspects can be found in Ref. [129]. Making the newly developed simulation
and analysis tools, along with the required data, accessible to other scientists and future users

23

forms an essential element of open and thriving science.

6 Outlook

As a ﬁeld combining vast datasets with excellent, ﬁrst-principle simulations, particle physics
is beneﬁting tremendously from developments in data science and machine learning. While
new AI-inspired methods will not magically solve all challenges in LHC simulations and anal-
ysis, they are providing a crucial and transformative contribution to our numerical toolbox.
Moreover, given the quality of the LHC datasets, simulations, and simulation-based analysis
methods, we expect particle physics to eventually contribute to broader machine learning re-
search.

Event generation, or the simulation of signals for the LHC detectors from QFT Lagrangians,
is the main link between experimental and theoretical particle physics. It has stringent require-
ments when it comes to ﬁrst principles vs modeling, control, precision, speed, and ﬂexibility. In
this review we have shown that even within the physics-motivated modular structure of stan-
dard event generators, there is no aspect that cannot be improved through modern machine
learning. This includes phase space sampling, scattering amplitudes, loop integrals, parton
showers, parton densities, and fragmentation. Some of these ML-applications have a long
history and are accepted as standard approaches, other ML-based improvements of physics
modules are currently under rapid development and are ﬁnding their way into standard gen-
erators. All of them will be key to address the needs for example of the HL-LHC.

In addition to ML-enhanced event generators, an interesting application of generative neu-
ral networks are ML-generators at parton level and fast ML-detector simulations. They provide
an excellent testing ground for phase space generators, precision networks, and inverted sim-
ulations. This includes conceptual developments in the ﬁeld of generative networks, driven by
LHC-speciﬁc requirements of controlling precision-generative networks as numerical tools and
providing a full range of uncertainties. They allow us to deﬁne, produce, and encode datasets
for phenomenological studies and serve as a compression for data entering experimental anal-
yses.

The main conceptual advantage of ML-event generation is that simulations with generative
networks are symmetric: given a fundamental physics model we can predict the probability
distributions of LHC events over phase space, or we can predict the probability distributions of
model parameters given observed LHC events. Different ML-approaches to simulation-based
inference include classiﬁcation-based methods, conditional generative networks as a direct
inversion, or indirect ways of learning likelihood ratios. In combination, they will allow us to
systematically use unfolding or inverted simulations at the HL-LHC, from particle identiﬁcation
and detector unfolding all the way to an event-wise matrix element method analysis.

Finally, there are many simulation-related questions in fundamental physics, where AI-
methods allow us to make signiﬁcant progress. Examples going beyond immediate appli-
cations to event generation include symbolic regression [130], sample and data compres-
sion [57, 131], detection of symmetries [132–135], and many other fascinating new ideas
and concepts.

Acknowledgments

Anja Butter, Gudrun Heinrich, and Tilman Plehn are supported by the Deutsche Forschungs-
gemeinschaft under grant 396021762 – TRR 257 Particle Physics Phenomenology after the
Higgs Discovery. Lukas Heinrich is supported by the Excellence Cluster ORIGINS, which is
funded by the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy - EXC-
2094-390783311. Alexander Held is supported by the U.S. National Science Foundation (NSF)

24

Cooperative Agreement OAC-1836650. Stefan Höche and Joshua Isaacson are supported by
the Fermi National Accelerator Laboratory (Fermilab), a U.S. Department of Energy, Ofﬁce of
Science, HEP User Facility. Fermilab is managed by Fermi Research Alliance, LLC (FRA), acting
under Contract No. DE–AC02–07CH11359. Jessica N. Howard is supported by the U.S. Na-
tional Science Foundation under the grant DGE-1839285. Michael Kagan is supported by the
US Department of Energy (DOE) under grant DE-AC02-76SF00515. Gregor Kasieczka and Fe-
lix Kling are supported by the Deutsche Forschungsgemeinschaft under Germany’s Excellence
Strategy – EXC 2121 Quantum Universe – 390833306. Sabine Kraml acknowledges support
by the IN2P3 master project Théorie – BSMGA and the joint ANR-FWF project PRCI SLDNP
grant no. ANR-21-CE31-0023. Claudius Krause and David Shih are supported by DOE grant
DOE-SC0010008. Rahool Kumar Barman and Dorival Gonçalves thank the U.S. Department
of Energy for the ﬁnancial support under grant number DE-SC 0016013. Benjamin Nachman
is supported by the U.S. Department of Energy, Ofﬁce of Science under contract DE-AC02-
05CH11231. Tilman Plehn is supported by the Deutsche Forschungsgemeinschaft under Ger-
many’s Excellence Strategy EXC 2181/1 - 390900948 (the Heidelberg STRUCTURES Excel-
lence Cluster). Steffen Schumann acknowledges support from the German Federal Ministry
of Education and Research (BMBF, grant 05H21MGCAB) and the German Research Founda-
tion (DFG, project number 456104544). Rob Verheyen is supported by the European Research
Council (ERC) under the European Union’s Horizon 2020 research and innovation programme
(grant agreement No. 788223, PanScales). Ramon Winterhalder is supported by FRS-FNRS
(Belgian National Scientiﬁc Research Fund) IISN projects 4.4503.16.

References

[1] T. Sjöstrand, S. Ask, J. R. Christiansen, R. Corke, N. Desai, P. Ilten, S. Mrenna, S. Prestel,
C. O. Rasmussen and P. Z. Skands, An Introduction to PYTHIA 8.2, Comput. Phys.
Commun. 191, 159 (2015), doi:10.1016/j.cpc.2015.01.024, arXiv:1410.3012.

[2] E. Bothmann et al., Event Generation with Sherpa 2.2, SciPost Phys. 7(3), 034 (2019),

doi:10.21468/SciPostPhys.7.3.034, arXiv:1905.09127.

[3] J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H. S. Shao,
T. Stelzer, P. Torrielli and M. Zaro, The automated computation of tree-level and next-to-
leading order differential cross sections, and their matching to parton shower simulations,
JHEP 07, 079 (2014), doi:10.1007/JHEP07(2014)079, arXiv:1405.0301.

[4] J. Bellm et al., Herwig 7.0/Herwig++ 3.0 release note, Eur. Phys. J. C 76(4), 196 (2016),

doi:10.1140/epjc/s10052-016-4018-8, arXiv:1512.01178.

[5] W. Kilian, T. Ohl and J. Reuter, WHIZARD: Simulating Multi-Particle Processes at LHC
and ILC, Eur. Phys. J. C 71, 1742 (2011), doi:10.1140/epjc/s10052-011-1742-y,
arXiv:0708.4233.

[6] A. Butter and T. Plehn, Generative Networks for LHC events (2020), arXiv:2008.08558.

[7] P. Ilten, T. Menzo, A. Youssef and J. Zupan, Modeling hadronization using machine learn-

ing (2022), arXiv:2203.04983.

[8] M. Lazzarin, S. Alioli and S. Carrazza, MCNNTUNES: Tuning Shower Monte Carlo
generators with machine learning, Comput. Phys. Commun. 263, 107908 (2021),
doi:10.1016/j.cpc.2021.107908, arXiv:2010.02213.

25

[9] E. Bothmann, T. Janßen, M. Knobbe, T. Schmale and S. Schumann,

Exploring
SciPost Phys. 8(4), 069 (2020),

phase space with Neural Importance Sampling,
doi:10.21468/SciPostPhys.8.4.069, arXiv:2001.05478.

[10] J. Bendavid, Efﬁcient Monte Carlo Integration Using Boosted Decision Trees and Generative

Deep Neural Networks (2017), arXiv:1707.00028.

[11] M. D. Klimek and M. Perelstein, Neural Network-Based Approach to Phase Space
doi:10.21468/SciPostPhys.9.4.053,

SciPost Phys. 9, 053 (2020),

Integration,
arXiv:1810.11509.

[12] C. Gao, J. Isaacson and C. Krause,

i-ﬂow: High-dimensional Integration and Sam-
pling with Normalizing Flows, Mach. Learn. Sci. Tech. 1(4), 045023 (2020),
doi:10.1088/2632-2153/abab62, arXiv:2001.05486.

[13] C. Gao, S. Höche, J. Isaacson, C. Krause and H. Schulz, Event Generation with Normal-
izing Flows, Phys. Rev. D 101(7), 076002 (2020), doi:10.1103/PhysRevD.101.076002,
arXiv:2001.10028.

[14] I.-K. Chen, M. D. Klimek and M. Perelstein,

Simulation,
arXiv:2009.07819.

SciPost Phys. 10, 023 (2021),

Improved Neural Network Monte Carlo
doi:10.21468/SciPostPhys.10.1.023,

[15] S. Pina-Otey, V. Gaitan, F. Sánchez and T. Lux, Exhaustive neural importance sam-
pling applied to Monte Carlo event generation, Phys. Rev. D 102(1), 013003 (2020),
doi:10.1103/PhysRevD.102.013003, arXiv:2005.12719.

[16] B. Stienen and R. Verheyen,

Weighted Events with Autoregressive Flows,
doi:10.21468/SciPostPhys.10.2.038, arXiv:2011.13445.

Phase Space Sampling and Inference

from
SciPost Phys. 10, 038 (2021),

[17] A. Butter, T. Plehn and R. Winterhalder, How to GAN LHC Events, SciPost Phys. 7(6),

075 (2019), doi:10.21468/SciPostPhys.7.6.075, arXiv:1907.03764.

[18] R. Di Sipio, M. Faucci Giannelli, S. Ketabchi Haghighat and S. Palazzo, DijetGAN: A
Generative-Adversarial Network Approach for the Simulation of QCD Dijet Events at the
LHC, JHEP 08, 110 (2019), doi:10.1007/JHEP08(2019)110, arXiv:1903.02433.

[19] S. Otten, S. Caron, W. de Swart, M. van Beekveld, L. Hendriks, C. van Leeuwen, D. Po-
dareanu, R. Ruiz de Austri and R. Verheyen, Event Generation and Statistical Sampling
for Physics with Deep Generative Models and a Density Information Buffer, Nature Com-
mun. 12(1), 2985 (2021), doi:10.1038/s41467-021-22616-z, arXiv:1901.00875.

[20] S. Choi and J. H. Lim, A Data-driven Event Generator for Hadron Colliders using
Wasserstein Generative Adversarial Network, J. Korean Phys. Soc. 78(6), 482 (2021),
doi:10.1007/s40042-021-00095-1, arXiv:2102.11524.

[21] A. G. Baydin, B. A. Pearlmutter, A. A. Radul and J. M. Siskind, Automatic differentiation
in machine learning: a survey, Journal of Machine Learning Research 18(153), 1 (2018),
http://jmlr.org/papers/v18/17-468.html.

[22] L. Heinrich and M. Kagan,

Differentiable Matrix Elements with MadJax (2022),

arXiv:2203.00057.

[23] M. Backes, A. Butter, T. Plehn and R. Winterhalder, How to GAN Event Unweighting, Sci-
Post Phys. 10(4), 089 (2021), doi:10.21468/SciPostPhys.10.4.089, arXiv:2012.07873.

26

[24] R. Winterhalder, M. Bellagente and B. Nachman, Latent Space Reﬁnement for Deep

Generative Models (2021), arXiv:2106.00792.

[25] K. Danziger, T. Janßen, S. Schumann and F. Siegert, Accelerating Monte Carlo event
generation – rejection sampling using neural network event-weight estimates (2021),
arXiv:2109.11964.

[26] F. Bishara and M. Montull, (Machine) Learning Amplitudes for Faster Event Generation

(2019), arXiv:1912.11055.

[27] S. Badger and J. Bullock, Using neural networks for efﬁcient evaluation of high multi-
plicity scattering amplitudes, JHEP 06, 114 (2020), doi:10.1007/JHEP06(2020)114,
arXiv:2002.07516.

[28] J. Aylett-Bullock, S. Badger and R. Moodie, Optimising simulations for diphoton pro-
duction at hadron colliders using amplitude neural networks, JHEP 08, 066 (2021),
doi:10.1007/JHEP08(2021)066, arXiv:2106.09474.

[29] D. Maître and H. Truong, A factorisation-aware Matrix element emulator, JHEP 11, 066

(2021), doi:10.1007/JHEP11(2021)066, arXiv:2107.06625.

[30] S. Badger, A. Butter, M. Luchmann, S. Pitz and T. Plehn, Loop Amplitudes from a Boosted

Precision Network (2022), arXiv:to appear.

[31] Y. Gal, Uncertainty in Deep Learning, Ph.D. thesis, Cambridge (2016).

[32] A. Kendall and Y. Gal, What Uncertainties Do We Need in Bayesian Deep Learning for

Computer Vision?, Proc. NIPS (2017), arXiv:1703.04977.

[33] S. Bollweg, M. Haußmann, G. Kasieczka, M. Luchmann, T. Plehn and J. Thomp-
son, Deep-Learning Jets with Uncertainties and More, SciPost Phys. 8(1), 006 (2020),
doi:10.21468/SciPostPhys.8.1.006, arXiv:1904.10004.

[34] G. Kasieczka, M. Luchmann, F. Otterpohl and T. Plehn, Per-Object Systematics using Deep-
Learned Calibration, SciPost Phys. 9, 089 (2020), doi:10.21468/SciPostPhys.9.6.089,
arXiv:2003.11099.

[35] R. Winterhalder, V. Magerya, E. Villa, S. P. Jones, M. Kerner, A. Butter, G. Hein-
Targeting Multi-Loop Integrals with Neural Networks (2021),

rich and T. Plehn,
arXiv:2112.09145.

[36] A. Andreassen, I. Feige, C. Frye and M. D. Schwartz,

Binary JUNIPR: an inter-
pretable probabilistic model for discrimination, Phys. Rev. Lett. 123(18), 182001 (2019),
doi:10.1103/PhysRevLett.123.182001, arXiv:1906.10137.

[37] Y. S. Lai, D. Neill, M. Płosko´n and F. Ringer, Explainable machine learning of the under-

lying physics of high-energy particle collisions (2020), arXiv:2012.06582.

[38] A. Andreassen, I. Feige, C. Frye and M. D. Schwartz, JUNIPR: a Framework for Un-
supervised Machine Learning in Particle Physics, Eur. Phys. J. C79(2), 102 (2019),
doi:10.1140/epjc/s10052-019-6607-9, arXiv:1804.09720.

[39] E. Bothmann and L. Debbio,
work: the ﬁnal-state case,
arXiv:1808.07802.

Reweighting a parton shower using a neural net-
JHEP 01, 033 (2019), doi:10.1007/JHEP01(2019)033,

27

[40] L. de Oliveira, M. Paganini and B. Nachman, Learning Particle Physics by Example:
Location-Aware Generative Adversarial Networks for Physics Synthesis, Comput. Softw.
Big Sci. 1(1), 4 (2017), doi:10.1007/s41781-017-0004-6, arXiv:1701.05927.

[41] J. W. Monk,

Deep Learning as a Parton Shower,

JHEP 12, 021 (2018),

doi:10.1007/JHEP12(2018)021, arXiv:1807.03685.

[42] K. Dohi, Variational Autoencoders for Jet Simulation (2020), arXiv:2009.04842.

[43] B. Orzari, T. Tomei, M. Pierini, M. Touranakou, J. Duarte, R. Kansal, J.-R. Vlimant and
D. Gunopulos, Sparse Data Generation for Particle-Based Simulation of Hadronic Jets
in the LHC, In 38th International Conference on Machine Learning Conference (2021),
arXiv:2109.15197.

[44] S. Tsan, R. Kansal, A. Aportela, D. Diaz, J. Duarte, S. Krishna, F. Mokhtar, J.-R. Vli-
mant and M. Pierini, Particle Graph Autoencoders and Differentiable, Learned Energy
Mover’s Distance, In 35th Conference on Neural Information Processing Systems (2021),
arXiv:2111.12849.

[45] S. Bieringer, A. Butter, T. Heimel, S. Höche, U. Köthe, T. Plehn and S. T. Radev,
Measuring QCD Splittings with Invertible Networks, SciPost Phys. 10(6), 126 (2021),
doi:10.21468/SciPostPhys.10.6.126, arXiv:2012.09873.

[46] R. D. Ball et al.,
arXiv:2109.02653.

The Path to Proton Structure at One-Percent Accuracy (2021),

[47] S. Forte, L. Garrido, J. I. Latorre and A. Piccione, Neural network parametriza-
tion of deep inelastic structure functions, JHEP 05, 062 (2002), doi:10.1088/1126-
6708/2002/05/062, arXiv:hep-ph/0204232.

[48] R. D. Ball, L. Del Debbio, S. Forte, A. Guffanti, J. I. Latorre, J. Rojo and M. Ubiali, A ﬁrst
unbiased global NLO determination of parton distributions and their uncertainties, Nucl.
Phys. B 838, 136 (2010), doi:10.1016/j.nuclphysb.2010.05.008, arXiv:1002.4407.

[49] R. D. Ball et al., Parton distributions for the LHC Run II,

JHEP 04, 040 (2015),

doi:10.1007/JHEP04(2015)040, arXiv:1410.8849.

[50] R. D. Ball et al., An open-source machine learning framework for global analyses of par-
ton distributions, Eur. Phys. J. C 81(10), 958 (2021), doi:10.1140/epjc/s10052-021-
09747-9, arXiv:2109.02671.

[51] E. R. Nocera, R. D. Ball, S. Forte, G. Ridolﬁ and J. Rojo, A ﬁrst unbiased global de-
termination of polarized PDFs and their uncertainties, Nucl. Phys. B 887, 276 (2014),
doi:10.1016/j.nuclphysb.2014.08.008, arXiv:1406.5539.

[52] R. A. Khalek, R. Gauld, T. Giani, E. R. Nocera, T. R. Rabemananjara and J. Rojo,
Evidence for a modiﬁed partonic structure in heavy nuclei (2022),

nNNPDF3.0:
arXiv:2201.12363.

[53] R. Abdul Khalek, J. J. Ethier, J. Rojo and G. van Weelden, nNNPDF2.0: quark ﬂavor sepa-
ration in nuclei from LHC data, JHEP 09, 183 (2020), doi:10.1007/JHEP09(2020)183,
arXiv:2006.14629.

[54] S. Carrazza and J. Cruz-Martinez, Towards a new generation of parton densities with
deep learning models, Eur. Phys. J. C 79(8), 676 (2019), doi:10.1140/epjc/s10052-
019-7197-2, arXiv:1907.05075.

28

[55] S. Carrazza, J. M. Cruz-Martinez and R. Stegeman, A data-based parametrization of par-
ton distribution functions, Eur. Phys. J. C 82(2), 163 (2022), doi:10.1140/epjc/s10052-
022-10136-z, arXiv:2111.02954.

[56] S. Carrazza, J. I. Latorre, J. Rojo and G. Watt, A compression algorithm for the combina-
tion of PDF sets, Eur. Phys. J. C 75, 474 (2015), doi:10.1140/epjc/s10052-015-3703-3,
arXiv:1504.06469.

[57] S. Carrazza, J. M. Cruz-Martinez and T. R. Rabemananjara,

sets using generative adversarial networks,
doi:10.1140/epjc/s10052-021-09338-8, arXiv:2104.04535.

Compressing PDF
Eur. Phys. J. C 81(6), 530 (2021),

[58] R. A. Khalek, V. Bertone and E. R. Nocera, Determination of unpolarized pion fragmen-
tation functions using semi-inclusive deep-inelastic-scattering data, Phys. Rev. D 104(3),
034007 (2021), doi:10.1103/PhysRevD.104.034007, arXiv:2105.08725.

[59] V. Bertone, S. Carrazza, N. P. Hartland, E. R. Nocera and J. Rojo, A determination of the
fragmentation functions of pions, kaons, and protons with faithful uncertainties, Eur. Phys.
J. C 77(8), 516 (2017), doi:10.1140/epjc/s10052-017-5088-y, arXiv:1706.07049.

[60] V. Bertone, N. P. Hartland, E. R. Nocera, J. Rojo and L. Rottoli, Charged hadron
Eur. Phys. J. C 78(8), 651 (2018),

fragmentation functions from collider data,
doi:10.1140/epjc/s10052-018-6130-4, arXiv:1807.03310.

[61] M. Soleymaninia, H. Hashamipour and H. Khanpour, SHK22.h: Neural Network QCD
analysis of charged hadron Fragmentation Functions in the presence of SIDIS data (2022),
arXiv:2202.10779.

[62] L. Del Debbio, S. Forte, J. I. Latorre, A. Piccione and J. Rojo, Unbiased determination of
the proton structure function F(2)**p with faithful uncertainty estimation, JHEP 03, 080
(2005), doi:10.1088/1126-6708/2005/03/080, arXiv:hep-ph/0501067.

[63] A. Butter, T. Plehn and R. Winterhalder, How to GAN Event Subtraction, SciPost Phys.
Core 3, 009 (2020), doi:10.21468/SciPostPhysCore.3.2.009, arXiv:1912.08824.

[64] A. Butter, S. Diefenbacher, G. Kasieczka, B. Nachman and T. Plehn, GANplifying
event samples, SciPost Phys. 10(6), 139 (2021), doi:10.21468/SciPostPhys.10.6.139,
arXiv:2008.06545.

[65] S. Bieringer, A. Butter, S. Diefenbacher, E. Eren, F. Gaede, D. Hundhausen, G. Kasieczka,
B. Nachman, T. Plehn and M. Trabs, Calompliﬁcation – The Power of Generative Calorime-
ter Models (2022), arXiv:2202.07352.

[66] B. Stienen and R. Verheyen,

Phase
weighted events with autoregressive ﬂows,
doi:10.21468/SciPostPhys.10.2.038, arXiv:2011.13445.

space

from
sampling and inference
SciPost Phys. 10(2), 038 (2021),

[67] J. N. Howard, S. Mandt, D. Whiteson and Y. Yang, Foundations of a Fast, Data-Driven,

Machine-Learned Simulator (2021), arXiv:2101.08944.

[68] A. Butter, T. Heimel, S. Hummerich, T. Krebs, T. Plehn, A. Rousselot and S. Vent, Gener-

ative Networks for Precision Enthusiasts (2021), arXiv:2110.13632.

[69] M. Paganini, L. de Oliveira and B. Nachman, Accelerating Science with Generative
Adversarial Networks: An Application to 3D Particle Showers in Multilayer Calorime-
ters, Phys. Rev. Lett. 120(4), 042003 (2018), doi:10.1103/PhysRevLett.120.042003,
arXiv:1705.02355.

29

[70] M. Paganini, L. de Oliveira and B. Nachman, CaloGAN : Simulating 3D high energy
particle showers in multilayer electromagnetic calorimeters with generative adversarial
networks, Phys. Rev. D97(1), 014021 (2018), doi:10.1103/PhysRevD.97.014021,
arXiv:1712.10321.

[71] P. Musella and F. Pandolﬁ, Fast and Accurate Simulation of Particle Detectors Using Genera-
tive Adversarial Networks, Comput. Softw. Big Sci. 2(1), 8 (2018), doi:10.1007/s41781-
018-0015-y, arXiv:1805.00850.

[72] M. Erdmann, L. Geiger, J. Glombitza and D. Schmidt, Generating and reﬁning particle
detector simulations using the Wasserstein distance in adversarial networks, Comput.
Softw. Big Sci. 2(1), 4 (2018), doi:10.1007/s41781-018-0008-x, arXiv:1802.03325.

[73] M. Erdmann, J. Glombitza and T. Quast, Precise simulation of electromagnetic calorimeter
showers using a Wasserstein Generative Adversarial Network, Comput. Softw. Big Sci. 3,
4 (2019), doi:10.1007/s41781-018-0019-7, arXiv:1807.01954.

[74] ATLAS

Collaboration,

for
Fast
ATLAS-SIM-2019-004,
Https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PLOTS/SIM-2019-004/ (2019).

Simulation

resolution

Shower

ATLAS,

Energy

GAN

with

in

a

[75] D. Belayneh et al., Calorimetry with Deep Learning: Particle Simulation and Reconstruc-
tion for Collider Physics, Eur. Phys. J. C 80(7), 688 (2020), doi:10.1140/epjc/s10052-
020-8251-9, arXiv:1912.06794.

[76] E. Buhmann, S. Diefenbacher, E. Eren, F. Gaede, G. Kasieczka, A. Korol and K. Krüger,
Getting High: High Fidelity Simulation of High Granularity Calorimeters with High
Speed, Comput. Softw. Big Sci. 5(1), 13 (2021), doi:10.1007/s41781-021-00056-0,
arXiv:2005.05334.

[77] E. Buhmann, S. Diefenbacher, E. Eren, F. Gaede, G. Kasieczka, A. Korol and K. Krüger,
Decoding Photons: Physics in the Latent Space of a BIB-AE Generative Network, EPJ Web
Conf. 251, 03003 (2021), doi:10.1051/epjconf/202125103003, arXiv:2102.12491.

[78] C. Chen, O. Cerri, T. Q. Nguyen, J. R. Vlimant and M. Pierini, Analysis-Speciﬁc Fast
Simulation at the LHC with Deep Learning, Comput. Softw. Big Sci. 5(1), 15 (2021),
doi:10.1007/s41781-021-00060-4.

[79] C. Krause and D. Shih, CaloFlow: Fast and Accurate Generation of Calorimeter Showers

with Normalizing Flows (2021), arXiv:2106.05285.

[80] C. Krause and D. Shih, CaloFlow II: Even Faster and Still Accurate Generation of Calorime-

ter Showers with Normalizing Flows (2021), arXiv:2110.11377.

[81] S. Otten, S. Caron, W. de Swart, M. van Beekveld, L. Hendriks, C. van Leeuwen, D. Po-
dareanu, R. Ruiz de Austri and R. Verheyen, Event Generation and Statistical Sampling
for Physics with Deep Generative Models and a Density Information Buffer, Nature Com-
mun. 12(1), 2985 (2021), doi:10.1038/s41467-021-22616-z, arXiv:1901.00875.

[82] B. Hashemi, N. Amin, K. Datta, D. Olivito and M. Pierini, LHC analysis-speciﬁc datasets

with Generative Adversarial Networks (2019), arXiv:1901.05282.

[83] R. Di Sipio, M. Faucci Giannelli, S. Ketabchi Haghighat and S. Palazzo, DijetGAN: A
Generative-Adversarial Network Approach for the Simulation of QCD Dijet Events at the
LHC, JHEP 08, 110 (2020), doi:10.1007/JHEP08(2019)110, arXiv:1903.02433.

30

[84] J. Arjona Martínez, T. Q. Nguyen, M. Pierini, M. Spiropulu and J.-R. Vlimant, Particle
Generative Adversarial Networks for full-event simulation at the LHC and their application
to pileup description, J. Phys. Conf. Ser. 1525(1), 012081 (2020), doi:10.1088/1742-
6596/1525/1/012081, arXiv:1912.02748.

[85] Y. Alanazi, N. Sato, T. Liu, W. Melnitchouk, M. P. Kuchera, E. Pritchard, M. Robertson,
R. Strauss, L. Velasco and Y. Li, Simulation of electron-proton scattering events by a
Feature-Augmented and Transformed Generative Adversarial Network (FAT-GAN) (2020),
arXiv:2001.11103.

[86] M. Bellagente, M. Haußmann, M. Luchmann and T. Plehn, Understanding Event-

Generation Networks via Uncertainties (2021), arXiv:2104.04543.

[87] S. Caron, L. Hendriks and R. Verheyen, Rare and Different: Anomaly Scores from a
combination of likelihood and out-of-distribution models to detect new physics at the LHC,
SciPost Phys. 12, 077 (2022), doi:10.21468/SciPostPhys.12.2.077, arXiv:2106.10164.

[88] A. Hallin, J. Isaacson, G. Kasieczka, C. Krause, B. Nachman, T. Quadfasel, M. Schlaffer,
D. Shih and M. Sommerhalder, Classifying Anomalies THrough Outer Density Estimation
(CATHODE) (2021), arXiv:2109.00546.

[89] B. Nachman and D. Shih, Anomaly Detection with Density Estimation, Phys. Rev. D 101,

075042 (2020), doi:10.1103/PhysRevD.101.075042, arXiv:2001.04990.

[90] T. Buss, B. M. Dillon, T. Finke, M. Krämer, A. Morandini, A. Mück, I. Oleksiyuk and

T. Plehn, What’s Anomalous in LHC Jets? (2022), arXiv:2202.00686.

[91] B. Nachman,

A guide for deploying Deep Learning in LHC searches: How to
SciPost Phys. 8, 090 (2020),

achieve optimality and account for uncertainty,
doi:10.21468/SciPostPhys.8.6.090, arXiv:1909.03081.

[92] S. Diefenbacher, E. Eren, G. Kasieczka, A. Korol, B. Nachman and D. Shih, DCTRGAN:
Improving the Precision of Generative Models with Reweighting, JINST 15(11), P11004
(2020), doi:10.1088/1748-0221/15/11/P11004, arXiv:2009.03796.

[93] K. Datta, D. Kar and D. Roy, Unfolding with Generative Adversarial Networks (2018),

arXiv:1806.00433.

[94] M. Bellagente, A. Butter, G. Kasieczka, T. Plehn, A. Rousselot, R. Winterhalder, L. Ardiz-
zone and U. Köthe, Invertible Networks or Partons to Detector and Back Again, SciPost
Phys. 9, 074 (2020), doi:10.21468/SciPostPhys.9.5.074, arXiv:2006.06685.

[95] A. Andreassen, P. T. Komiske, E. M. Metodiev, B. Nachman and J. Thaler, OmniFold:
A Method to Simultaneously Unfold All Observables, Phys. Rev. Lett. 124(18), 182001
(2020), doi:10.1103/PhysRevLett.124.182001, arXiv:1911.09107.

[96] M. Vandegar, M. Kagan, A. Wehenkel and G. Louppe, Neural empirical bayes: Source
distribution estimation and its applications to simulation-based inference,
In A. Baner-
jee and K. Fukumizu, eds., Proceedings of The 24th International Conference on Artiﬁ-
cial Intelligence and Statistics, vol. 130 of Proceedings of Machine Learning Research, pp.
2107–2115. PMLR (2021), arXiv:2011.05836.

[97] L. Ardizzone, C. Lüth, J. Kruse, C. Rother and U. Köthe, Guided image generation with

conditional invertible neural networks (2019), arXiv:1907.02392.

31

[98] M. Bellagente, A. Butter, G. Kasieczka, T. Plehn and R. Winterhalder, How to GAN away
Detector Effects, SciPost Phys. 8(4), 070 (2020), doi:10.21468/SciPostPhys.8.4.070,
arXiv:1912.00477.

[99] F. A. Di Bello, S. Ganguly, E. Gross, M. Kado, M. Pitt, L. Santi and J. Shlomi,
Eur. Phys. J. C 81(2), 107 (2021),

Towards a Computer Vision Particle Flow,
doi:10.1140/epjc/s10052-021-08897-0, arXiv:2003.08863.

[100] J. Pata, J. Duarte, J.-R. Vlimant, M. Pierini and M. Spiropulu, MLPF: Efﬁcient machine-
learned particle-ﬂow reconstruction using graph neural networks, Eur. Phys. J. C 81(5),
381 (2021), doi:10.1140/epjc/s10052-021-09158-w, arXiv:2101.08578.

[101] S. R. Qasim, K. Long, J. Kieseler, M. Pierini and R. Nawaz, Multi-particle reconstruc-
tion in the High Granularity Calorimeter using object condensation and graph neural
networks, EPJ Web Conf. 251, 03072 (2021), doi:10.1051/epjconf/202125103072,
arXiv:2106.01832.

[102] P. Baldi, L. Blecher, A. Butter, J. Collado, J. N. Howard, F. Keilbach, T. Plehn, G. Kasieczka
and D. Whiteson, How to GAN Higher Jet Resolution (2020), arXiv:2012.11944.

[103] M. Arratia et al., Publishing unbinned differential cross section results, JINST 17(01),
P01024 (2022), doi:10.1088/1748-0221/17/01/P01024, arXiv:2109.13243.

[104] A. Andreassen, P. T. Komiske, E. M. Metodiev, B. Nachman, A. Suresh and J. Thaler,
Scaffolding Simulations with Deep Learning for High-dimensional Deconvolution, In 9th
International Conference on Learning Representations (2021), arXiv:2105.04448.

[105] P. Komiske, W. P. McCormack and B. Nachman,

multaneously unfolding all observables,
doi:10.1103/PhysRevD.104.076027, arXiv:2105.09923.

Preserving new physics while si-
Phys. Rev. D 104(7), 076027 (2021),

[106] V. Andreev et al., Measurement of lepton-jet correlation in deep-inelastic scattering with
the H1 detector using machine learning for unfolding (2021), arXiv:2108.12376.

[107] J. Erdmann, T. Kallage, K. Kröninger and O. Nackenhorst, From the bottom to the
JINST 14(11), P11015 (2019),

top—reconstruction of t¯t events with deep learning,
doi:10.1088/1748-0221/14/11/P11015, arXiv:1907.11181.

[108] J. Brehmer, K. Cranmer, G. Louppe and J. Pavez,

Theories with Machine Learning,
doi:10.1103/PhysRevLett.121.111801, arXiv:1805.00013.

Constraining Effective Field
Phys. Rev. Lett. 121(11), 111801 (2018),

[109] J. Brehmer, K. Cranmer, G. Louppe and J. Pavez, A Guide to Constraining Effec-
Phys. Rev. D 98(5), 052004 (2018),

tive Field Theories with Machine Learning,
doi:10.1103/PhysRevD.98.052004, arXiv:1805.00020.

[110] J. Brehmer, G. Louppe, J. Pavez and K. Cranmer, Mining gold from implicit mod-
els to improve likelihood-free inference, Proc. Nat. Acad. Sci. 117(10), 5242 (2020),
doi:10.1073/pnas.1915980117, arXiv:1805.12244.

[111] M. Stoye, J. Brehmer, G. Louppe, J. Pavez and K. Cranmer, Likelihood-free inference with

an improved cross-entropy estimator (2018), arXiv:1808.00973.

[112] J. Brehmer, F. Kling, I. Espejo and K. Cranmer, MadMiner: Machine learning-based infer-
ence for particle physics, Comput. Softw. Big Sci. 4(1), 3 (2020), doi:10.1007/s41781-
020-0035-2, arXiv:1907.10621.

32

[113] K. Cranmer, J. Pavez and G. Louppe, Approximating Likelihood Ratios with Calibrated

Discriminative Classiﬁers (2015), arXiv:1506.02169.

[114] P. Baldi, K. Cranmer, T. Faucett, P. Sadowski and D. Whiteson,

Parameterized
Eur. Phys. J. C 76(5), 235 (2016),

neural networks for high-energy physics,
doi:10.1140/epjc/s10052-016-4099-4, arXiv:1601.07913.

[115] R. K. Barman, D. Gonçalves and F. Kling, Machine learning the Higgs boson-top quark
CP phase, Phys. Rev. D 105(3), 035023 (2022), doi:10.1103/PhysRevD.105.035023,
arXiv:2110.07635.

[116] K. Cranmer et al., Publishing statistical models: Getting the most out of particle physics
doi:10.21468/SciPostPhys.12.1.037,

SciPost Phys. 12, 037 (2022),

experiments,
arXiv:2109.04981.

[117] A. Coccaro, M. Pierini, L. Silvestrini and R. Torre,

The DNNLikelihood: enhanc-
ing likelihood distribution with Deep Learning, Eur. Phys. J. C 80(7), 664 (2020),
doi:10.1140/epjc/s10052-020-8230-1, arXiv:1911.03305.

[118] J. Brehmer, K. Cranmer and F. Kling,

ments and machine learning,
doi:10.1142/S0217751X20410080.

Improving inference with matrix ele-
Int. J. Mod. Phys. A 35(15n16), 2041008 (2020),

[119] J. Brehmer, F. Kling,

I. Espejo, S. Perez and K. Cranmer,

MadMiner,

doi:10.5281/zenodo.1489147.

[120] H. Bahl and S. Brass,

Constraining

teraction using machine-learning-based inference,
doi:10.1007/JHEP03(2022)017, arXiv:2110.10177.

CP

-violation in the Higgs-top-quark in-
017 (2022),

JHEP 03,

[121] J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V. Lemaître, A. Mertens and M. Sel-
vaggi, DELPHES 3, A modular framework for fast simulation of a generic collider experi-
ment, JHEP 02, 057 (2014), doi:10.1007/JHEP02(2014)057, arXiv:1307.6346.

[122] A. Coccaro, M. Pierini, L. Silvestrini and R. Torre, The DNNLikelihood: enhancing likeli-

hood distribution with Deep Learning, doi:10.5281/zenodo.3567822 (2019).

[123] L. Benato et al., Shared Data and Algorithms for Deep Learning in Fundamental Physics

(2021), arXiv:2107.00656.

[124] G. Aad et al., Search for R-parity-violating supersymmetry in a ﬁnal state containing
s = 13TeV proton–proton colli-
leptons and many jets with the ATLAS experiment using
sion data, Eur. Phys. J. C 81(11), 1023 (2021), doi:10.1140/epjc/s10052-021-09761-x,
arXiv:2106.09609.

(cid:112)

[125] ATLAS Collaboration, “SUSY-2019-04-ONNX.tgz” of “Search for R-parity-violating su-
persymmetry in a ﬁnal state containing leptons and many jets with the ATLAS experi-
s = 13TeV proton–proton collision data” (Version 1), HEPData (other),
ment using
https://doi.org/10.17182/hepdata.104860.v1/r3 (2021).

(cid:112)

[126] W. Abdallah et al., Reinterpretation of LHC Results for New Physics: Status and Recommen-
dations after Run 2, SciPost Phys. 9(2), 022 (2020), doi:10.21468/SciPostPhys.9.2.022,
arXiv:2003.07868.

[127] S. Bailey et al., Data and Analysis Preservation, Recasting, and Reinterpretation, In Pro-
ceedings of the US Community Study on the Future of Particle Physics (Snowmass 2021).

33

[128] M. D. Wilkinson et al., The FAIR Guiding Principles for scientiﬁc data management and

stewardship, Scientiﬁc Data 3 (2016), doi:10.1038/sdata.2016.18.

[129] Y. Chen et al.,

A FAIR and AI-ready Higgs boson decay dataset (2021),

doi:10.1038/s41597-021-01109-0, arXiv:2108.02214.

[130] A. Butter, T. Plehn, N. Soybelman and J. Brehmer, Back to the Formula – LHC Edition

(2021), arXiv:2109.10414.

[131] A. Butter, S. Diefenbacher, G. Kasieczka, B. Nachman, T. Plehn, D. Shih and R. Win-
terhalder, Ephemeral Learning - Augmenting Triggers with Online-Trained Normalizing
Flows (2022), arXiv:2202.09375.

[132] P. Betzler and S. Krippendorf, Connecting Dualities and Machine Learning, Fortsch. Phys.

68(5), 2000022 (2020), doi:10.1002/prop.202000022, arXiv:2002.05169.

[133] S. Krippendorf and M. Syvaeri, Detecting Symmetries with Neural Networks (2020),

arXiv:2003.13679.

[134] G. Barenboim, J. Hirn and V. Sanz, Symmetry meets AI, SciPost Phys. 11, 014 (2021),

doi:10.21468/SciPostPhys.11.1.014, arXiv:2103.06115.

[135] K. Desai, B. Nachman and J. Thaler, SymmetryGAN: Symmetry Discovery with Deep

Learning (2021), arXiv:2112.05722.

34

