0
2
0
2

v
o
N
2

]

Y
S
.
s
s
e
e
[

1
v
8
2
1
1
0
.
1
1
0
2
:
v
i
X
r
a

Reinforcement Learning of Structured Control for
Linear Systems with Unknown State Matrix

Sayak Mukherjee, Thanh Long Vu ∗

Abstract

This paper delves into designing stabilizing feedback control gains for continuous linear
systems with unknown state matrix, in which the control is subject to a general structural
constraint. We bring forth the ideas from reinforcement learning (RL) in conjunction with
suﬃcient stability and performance guarantees in order to design these structured gains using
the trajectory measurements of states and controls. We ﬁrst formulate a model-based framework
using dynamic programming (DP) to embed the structural constraint to the Linear Quadratic
Regulator (LQR) gain computation in the continuous-time setting. Subsequently, we transform
this LQR formulation into a policy iteration RL algorithm that can alleviate the requirement of
known state matrix in conjunction with maintaining the feedback gain structure. Theoretical
guarantees are provided for stability and convergence of the structured RL (SRL) algorithm.
The introduced RL framework is general and can be applied to any control structure. A special
control structure enabled by this RL framework is distributed learning control which is necessary
for many large-scale cyber-physical systems. As such, we validate our theoretical results with
numerical simulations on a multi-agent networked linear time invariant (LTI) dynamic system.

Keywords: Structured learning, distributed control, reinforcement learning, stability guarantee,
linear quadratic regulator.

1

Introduction

Researchers over the last decades have been intrigued to ﬁnd distributed control solutions for
interconnected cyber-physical systems, in which the controller of each subsystem will receive mea-
surement signals from some neighboring subsystems, instead of the full system state, in order to
derive at a decision. On the other hand, in most practical systems, only a subset of the system
state is available for the feedback control. There are typical examples of structured feedback con-
trol in which a predeﬁned structure is imposed on the feedback control mechanism for practical
implementation. Many research works have been geared toward structured control solutions for
diﬀerent structures and diﬀerent system models. Optimal control laws that can capture decentral-
ized [1] or a more generic distributed [2] structure has been investigated under notions of quadratic
invariance (QI) [3,4], structured linear matrix inequalities (LMIs) [5,6], sparsity promoting optimal
control [7] etc. [8] discusses a structural feedback gain computation for discrete-time systems with
known dynamics.

It is worth noting that, for practical dynamic systems, the dynamic model and its parameters
may not always be known accurately (e.g., the US eastern interconnection transmission grid model).
Several non-idealities such as unmodeled dynamics from the coupled processes, parameter drift

∗S. Mukherjee and T. L. Vu are with the Optimization and Control Group, Paciﬁc Northwest National Laboratory

(PNNL), Richland, WA, USA. Emails: (sayak.mukherjee, thanhlong.vu)@pnnl.gov.

1

 
 
 
 
 
 
issues over time can make model based control computations insuﬃcient. Unfortunately, most of
the aforementioned techniques for the strctured control system design assume that the designer
has explicit knowledge of the dynamic system. In recent times much attention has been given to
model-free decision making of dynamical systems by marrying the ideas from machine learning
with the classical control theory, resulting into ﬂourish in the area of reinforcement learning [9]. In
the RL framework, the control agent tries to optimize its actions based on the interactions with
the environment or the dynamic systems quantiﬁed in terms of rewards due to such interactions.
These techniques were traditionally introduced in the context of sequential decision making using
Markov decision processes (MDPs) [10–12], and has since been driving force in developing lots of
advanced algorithms in RL with applications to games, robotics etc.

Although many sophisticated machine learning-based control algorithms are developed to achieve
certain tasks, these algorithms many times suﬀer from the lack of stability and optimally guaran-
tees due to multiple heuristics involved in their designs. Recent works such as [13,14] have brought
together the good from two worlds to the control of dynamical systems: the capability to learn
without model from machine learning and the capability to make decision with rigorous stability
guarantee from automatic control theory. Basically, these works leveraged the conceptual equiva-
lence of RL/ADP algorithms with the classical optimal [15] and adaptive control [16] of the dynamic
systems. References [13,14,17–21] cover many of such work for systems with partially or completely
model free designs.

However, the area of structure based RL/ADP design for dynamic systems is still unexplored to
some extent. In [21,22], a projection based reduced-dimensional RL variant have been proposed for
singularly perturbed systems along with a block-decentralized design for two time-scale networks,
[23] presents a decentralized design for a class of interconnected systems, [24] presents a structured
design for discrete-time time-varying systems in the recent times. Along this line of research, this
paper will present a structured optimal feedback control methodology using reinforcement learning
without knowing the state dynamic model of system.

We ﬁrst formulate a model-based constrained optimal control criterion using the methodologies
and guarantees from dynamic programming. Subsequently, we formulate a model-free RL gain
computation algorithm that can encode the general structural constraint on the optimal control.
This structured learning algorithm - SRL encapsulates the stability and convergence guarantees
along with the sub-optimality for the controllers with the speciﬁed structure. We substantiate our
design on a 6-agent network system. The paper is organized as follows. We discuss the problem
formulation and the required assumptions in the section II. In section III, we discuss the structured
RL algorithm development incorporating the structural constraint. Numerical simulation example
is given in Section IV, and concluding remarks are given in Section V.

2 Model and Problem Formulation

We consider a linear time-invariant (LTI) continuous-time dynamic system:

˙x = Ax + Bu, x(0) = x0,

(1)

where x ∈ Rn, u ∈ Rm are the states and control inputs. We, hereby, make the following assumption.
Assumption 1: The dynamic state matrix A is unknown.

With this unknown state matrix, we would like to learn an optimal feedback gain u = −Kx.
However, instead of unrestricted control gain K ∈ Rm×n, we impose some structure on the gain.
We would like to have K ∈ K, which we call structural constraint, where K is the set of all

2

Figure 1: An example of structured feedback for agent 1

structured controllers such that:

K := {K ∈ Rm×n | F (K) = 0}.

(2)

Here F (.) is the matricial function that can capture any structure in the control gain matrix. This
can encode which elements in the K matrix will be non-zero, for example, with the multi-agent
example as given in Fig. 1, the control for the agent 1 can be constrained in the form:

K1 ∈ (cid:2)0 0 ∗ ∗ ∗ 0(cid:3) .

(3)

K1 denotes the ﬁrst row of K. Similarly, the feedback communication requirement on all other
agents can be encoded. Here all such feedback gains with the speciﬁed structure will be captured
in the set K. Therefore, we make the following assumption on the control gain structure.
Assumption 2: The communication structure required to implement the feedback control is known
to the designer, and it is sparse in nature.

This assumption means that the structure K is known. This captures the limitations in the
feedback communication infrastructure, and can also some time represent a much less dense infras-
tructure to minimize the cost of deployment of the network. For many network physical systems,
the communication infrastructure can be already existing, for example, in some peer-to-peer archi-
tecture, agents can only receive feedback from their neighbors. Another very commonly designed
control structure is of block-decentralized nature where local states are used to perform feedback
control. Therefore, our general constraint set will encompass all such scenarios. We also make the
standard stabilizability assumption.
Assumption 3: The pair (A, B) is stabilizable and (A, Q1/2) is observable.
We can now state the model-free learning control problem as follows.

P. For the system (1) satisfying Assumptions 1, 2, 3, learn structured control policies u = −Kx,
where K ∈ K described in (2), such that the closed-loop system is stable and the following objective
is minimized

J(x(0), uK) =

(cid:90) ∞

0

(xT Qx + uT Ru)dτ.

(4)

3 Structured Reinforcement Learning (SRL)

To develop the learning control design we will take the following route. We will ﬁrst formulate
a model-based solution of the optimal control problem via a modiﬁed Algebraic Riccati Equation
(ARE) in continuous-time that can solve the structured optimal control when all the state matrices

3

123456Physical couplingFeedbackcommunicationare known. Thereafter, we will formulate a learning algorithm that does not require the knowledge
about the state matrix A by embedding the model-based framework with guarantees into RL, which
will then automatically ensure the convergence and stability of the learning solution. First of all,
we present the following central result of this paper.
Theorem 1: For any matrix L ∈ Rm×n, let P (cid:31) 0 be the solution of the following modiﬁed Riccati
equation

AT P + P A − P BR−1BT P + Q + LT RL = 0.

Then, the control gain

K = K(L) = R−1BT P − L,

(5)

(6)

will ensure closed-loop stability, i.e., A − BK ∈ RH∞.
(cid:3)
Proof: We look into the optimal control solution of the dynamic system (1) with the objective (2)
using dynamic programming (DP) to ensure theoretical guarantees. We assume at time t, the state
is at x = x1. We deﬁne the ﬁnite time optimal value function with the unconstrained control as:

Vt(x1) = minu

(cid:90) T

t

(xT Qx + uT Ru)dτ,

(7)

with x(t) = x1, ˙x = Ax + Bu. Staring from state x1, the optimal Vt(x1) gives the minimum
LQR cost-to-go. Now as the value function is quadratic, we can write it in a quadratic form as,
Vt(x1) = xT
1 Ptx1, Pt (cid:31) 0. For a small time interval [t, t + h], where h > 0 is small, we assume that
the control is constant at u = u1 and is optimal. Then cost incurred over the interval [t, t + h] is

U1 =

(cid:90) t+h

t

(xT Qx + uT Ru)dτ ≈ h(xT

1 Qx1 + uT

1 Ru1).

Also, the control input evolves the states at time t + h to,

x(t + h) = x1 + h(Ax1 + Bu1).

Then, the minimum cost-to-go from x(t + h) is:

Vt+h(x(t + h)) = (x1 + h(Ax1 + Bu1))T Pt+h(x1

+ h(Ax1 + Bu1)).

Expanding Pt+h as (Pt + h ˙Pt) we have

Vt+h(x(t + h)) =
(x1 + h(Ax1 + Bu1))T (Pt + h ˙Pt)(x1 + h(Ax1 + Bu1)),
≈ xT
1 Pt(Ax1 + Bu1)
+ xT
1

1 Ptx1 + h((Ax1 + Bu1)T Ptx1 + xT
˙Ptx1).

Here, we neglect higher-order terms. Therefore, the total cost-

Vt(x1) = U = U1 + Vt+h(x(t + h)),
1 Qx1 + uT
1 Ptx1 + h(xT
1 Pt(Ax1 + Bu1) + xT
1

= xT
+ xT

1 Ru1 + (Ax1 + Bu1)T Ptx1
˙Ptx1).

4

(8)

(9)

(10)

(11)

(12)

(13)

(14)

If the control u = u1 is optimal then the total cost must be minimized. Minimizing Vt(x1) over u1
we have,

uT
1 R + xT
1 PtB = 0,
u1 = −R−1BT Ptx1.

(15)

(16)

Now, this gives us an optimal gain K = R−1BT Pt which solves the unconstrained LQR. However,
we are not interested in the unconstrained optimal gain, as that cannot impose any structure as
such. In order to impose structure in the feedback gains, the feedback control will have to deviate
from the optimal solution of R−1BT Pt, and following [8], we introduce another matrix L ∈ Rm×n
such that,

K + L = R−1BT Pt,
K = R−1BT Pt − L.

(17)

(18)

The matrix L will help us to impose the structure, i.e., K ∈ K, which we will discuss later.
Therefore, the structured implemented control is given by,

u1 = −Kx1 = −R−1BT Ptx1 + Lx1.

(19)

We have u1 ∈ uK, where uK is the set of all control inputs when following K ∈ K. Now, with slight
abuse of notation, we denote the matrix Pt to be the solution corresponding to the structured
optimal control. The Hamilton-Jacobi equation with the structured control is given by,

V K∈K
(x1) ≈ minu1∈uKU,
t
1 Ptx1 ≈ minu1∈uK(xT
xT

1 Ptx1 + h(xT
+ (Ax1 + Bu1)T Ptx1 + xT
xT
1

˙Ptx1)).

1 Qx1 + uT
1 Pt(Ax1 + Bu1)+

1 Ru1

Putting (19), neglecting higher order terms, and after simplifying we get,
− ˙Pt = AT Pt + PtA − PtBR−1BT Pt + Q + LT RL.

For steady-state solution, we will have,

AT P + P A − P BR−1BT P + Q + LT RL = 0.

(20)

(21)

(22)

(23)

This proves the modiﬁed Riccati equation of the theorem. Now let us look into the stability of the
closed-loop system with the gain K = R−1BT P − L. We can consider the Lyapunov function:

Therefore, the time derivative along the closed-loop trajectory is given as,

W = xT P x, P (cid:31) 0.

˙W = xT P ˙x + ˙xT P x,

= xT P (Ax + B(−R−1BT P x + Lx)) + .
(Ax + B(−R−1BT P x + Lx))T P x,
= xT [P A + AT P − P BR−1BT P − P BR−1BT P

+ P BL + LT BT P ]x,

= xT [−Q − P BR−1BT P + 2P BL − LT RL]x,
= xT [−Q − (P BR−1 − LT )R(P BR−1 − LT )T ]x.

5

(24)

(25)

(26)

(27)

(28)

(29)

Now as R is positive deﬁnite, the terms of form X T RX are at-least positive semi-deﬁnite. Therefore,
we have,

˙W ≤ −xT Qx.

(30)

This ensures, closed-loop system stability. For the linear system, with the assumption that (A, Q1/2)
is observable, the globally asymptotic stability can be proved by using the LaSalle’s invariance
(cid:3)
principle. This completes the proof.
At this point, we investigate closely the matricial structure constraint. Let IK denotes the
indicator matrix for the structured matrix set K where this matrix contains element 1 whenever
the corresponding element in K is non-zero. For the example (3), we will have for the ﬁrst row as,

IK1 = (cid:2)0 0 1 1 1 0(cid:3) .

Therefore structural constraint is simply written as:

F (K) = K ◦ I c

K = 0.

(31)

(32)

Here, ◦ denotes the element-wise/ Hadamard product, and I c
K is the complement of IK. We, here-
after, state the following theorem on the choice of L to impose structure on K. This follows the
similar form of discrete-time condition of [8].

Theorem 2: Let P be the solution of the modiﬁed ARE (5) where L = F (φ(P )) and φ(P ) =
R−1BT P. Then, the control gain K = R−1BT P − L designed as in Theorem 1 will satisfy the
structural constraint F (K) = 0.
Proof : We have,

K = φ(P ) − L = φ(P ) − F (φ(P ))

= φ(P ) − φ(P ) ◦ I c
K
= φ(P ) ◦ (1m×n − I c
K)
= φ(P ) ◦ IK ∈ K.

(33)

(34)

(35)

(36)

(cid:3)

This concludes the proof.

We note that the implicit assumption here is the existence of the solution of the modiﬁed ARE
(5) where L = F (φ(P )) and φ(P ) = R−1BT P. It is still an open question on the necessary and
suﬃcient condition on the structure F (K) for the existence of this solution. However, once the
solution exists, we can iteratively compute it and the associated control gain K using the following
model-based algorithm.
Theorem 3: Let K0 be such that A − BK0 is Hurwitz. Then, for k = 0, 1, . . .
1. Solve for Pk(Policy Evaluation) :

AT

k Pk + PkAk + KT

k RKk + Q = 0, Ak = A − BKk.

2. Update the control gain (Policy update):

Kk+1 = R−1BT Pk − F (φ(Pk)), φ(Pk) = R−1BT Pk.

(37)

(38)

Then A − BK is Hurwitz and Kk ∈ K and Pk converge to structured K ∈ K, and P as k → ∞. (cid:3)

6

Proof: The theorem is formulated by taking motivation from the Kleinman’s algorithm [25] that
has been used for unstructured feedback gain computations. Comparing with the Kleinman’s
algorithm, the control gain update step is now modiﬁed to impose the structure. With some math-
ematical manipulations it can be shown that using Kk = R−1BT Pk −Fk, where Fk = R−1BT Pk ◦I c
K,
we have,

(A − BKk)T Pk + Pk(A − BKk) + KT
AT Pk + PkA − PkBR−1BT Pk + F T

k RKk =
k RFk = −Q.

(39)

(40)

Therefore, (37) is equivalent to (5) for the kth iteration. As we shown the stability and convergence
via dynamic programming and Lyapunov analysis in Theorem 1, considering the equivalence of
(cid:3)
Theorem 1 with this iterative version, the theorem can be proved.
Although, we have formulated an iterative solution to compute structured feedback gains, the
algorithm is still model-dependent. We, hereby, start to move into a state matrix agnostic design
using reinforcement learning. We write (1) incorporating u = −Kkx, Kk ∈ K as

˙x = Ax + Bu = (A − BKk)x + B(Kkx + u).

(41)

We explore the system by injecting a probing signal u = u0 such that the system states do not
become unbounded [17]. For example, following [17] one can design u0 as a sum of sinusoids.
Thereafter, we consider a quadratic Lyapunov function xT P x, P (cid:31) 0, and we can take the time-
derivative along the state trajectories, and use Theorem 3 to alleviate the dependency on the state
matrix A.

d
dt

(xT Pkx) = xT (AT

k Pk + PkAk)x+
2(Kkx + u0)T BT Pkx,

= −xT ( ¯Qk + 2(Kkx + u0)T R(Ki(k+1) + Fk))x,

where, ¯Qk = Q + KT

k RKk. Therefore, starting with an arbitrary control policy u0 we have,

(t+T )Pkx(t+T ) − xT
xT

t Pkxt − 2

(cid:90) t+T

t

= −

(cid:90) t+T

t

(xT ¯Qkx)dτ.

((Kkx + u0)T R(Kk+1 + Fk)x)dτ

(42)

(43)

We, thereafter, solve (43) by formulating an iterative algorithm using the measurements of the
state and control trajectories. The design will require to gather data matrices D = {Sxx, Txx, Txu0}
for suﬃcient number of time samples (discussed shortly) where,

,

· · ·

, x ⊗ x|tl+T

(cid:105)T

,

Sxx =

Txx =

t1

(cid:104)
x ⊗ x|t1+T
(cid:104)(cid:82) t1+T
t1
(cid:104)(cid:82) t1+T
t1

(x ⊗ x)dτ,

(cid:105)T

(x ⊗ x)dτ

tl
, (cid:82) tl+T
tl
, (cid:82) tl+T
tl

,

(cid:105)T

.

(44)

(45)

(46)

Txu0 =

(x ⊗ u0)dτ,

(x ⊗ u0)dτ

· · ·

· · ·

7

Algorithm 1 Structured Reinforcement Learning (SRL) Control
1. Gather suﬃcient data: Store data (x, and u0) for interval (t1, t2, · · · , tl), ti − ti−1 = T . Then construct the
following data matrices D = {Sxx, Txx, Txu0 } such that rank(Txx Txu0 ) = n(n + 1)/2 + |K|.

2. Controller update iteration : Starting with a stabilizing K0, Compute K iteratively (k = 0, 1, · · · ) using the
following iterative equation
for k = 0, 1, 2, ..
A. Solve for Pk, and Kk+1 + Fk:

k R) − 2Txu0 (In ⊗ R)(cid:3)
(cid:2)Sxx −2Txx(In ⊗ K T
(cid:124)
(cid:123)(cid:122)
(cid:125)
Θk

(cid:20)

vec(Pk)
vec(Kk+1 + Fk)

(cid:21)

= −Txxvec( ¯Qk)
(cid:123)(cid:122)
(cid:125)
Φk

(cid:124)

.

(47)

B. Compute Fk = R−1BT Pk ◦ I c
C. Update the gain Kk+1.
D. Terminate the loop when ||Pk − Pk−1|| < ς, ς > 0 is a small threshold.
endfor

K using the feedback structure matrix.

3. Applying K on the system : Finally, apply u = −Kx, K ∈ K, and remove u0.

Algorithm 1 presents the steps to compute the structured feedback gain K ∈ K without knowing

the state matrix A.
Remark 1: If A is Hurwitz, then the controller update iteration in (47) can be started without
any stabilizing initial control. Otherwise, stabilizing K0 is required, as commonly encountered in
the RL literature [17]. This is mainly due to its equivalence with modiﬁed Kleinman’s algorithm
in Theorem 3.
Remark 2: The rank condition dictates the amount of data sample needs to be gathered. For this
algorithm we need rank(Txx Txu0) = n(n+1)/2+|K|, where |K| is the number of non-zero elements
in the structured feedback control matrices. This is based on the number of unknown variables
in the least squares. The number of data samples can be considered to be twice this number to
guarantee convergence.

The next theorem describes the sub-optimality of the structured solution with respect to the

unconstrained objective.
Theorem 4: The diﬀerence between the optimal structured control objective value J and the
optimal unstructured objective ¯J is bounded as:

(cid:107)J − ¯J(cid:107) ≤

l
2g

(cid:107)(xT

0 ⊗ xT

0 )(cid:107),

(48)

for any control structure. Here g = (cid:107)BR−1BT (cid:107)2, l = (cid:107)V−1(cid:107)−1, where V : Rn → Rn is a operator
deﬁned as:

VW = (A − BR−1BT )T W + W (A − BR−1BT ).

(49)

(cid:3)
Proof: Let the unstructured solution of the ARE be denoted as ¯P , then the unstructured objective
value is ¯J = xT
¯P x0, whereas, the learned structured control will result into the objective J =
0
xT
0 P x0, therefore we have,

(cid:107)J − ¯J(cid:107) = (cid:107)(xT
≤ (cid:107)(xT

0 ⊗ xT
0 ⊗ xT

0 )vec(P − ¯P )(cid:107)
0 )(cid:107)(cid:107)(P − ¯P )(cid:107)F

(50)

8

Using g, and l as deﬁned in the Theorem 4 statement, following [26, Theorem 3] with (cid:15) = (cid:107)LT RL(cid:107)
we have,

l

,

(cid:107)(P − ¯P )(cid:107)F ≤

=

2l(cid:15)(l − (cid:112)l2 − 4lg(cid:15))
4lg(cid:15)

=

2l(cid:15)
l + (cid:112)l2 − 4lg(cid:15)
(l − (cid:112)l2 − 4lg(cid:15))
2g

<

l
2g

.

As such, the diﬀerence between the optimal values J and ¯J is bounded by

(cid:107)J − ¯J(cid:107) ≤

l
2g

(cid:107)(xT

0 ⊗ xT

0 )(cid:107)

(51)

for any structure of the control. We note that g and l are not dependent on the control structure.
Therefore, the inequality (51) indicates that the diﬀerence between the optimal control value J with
K ∈ K, and optimal unstructured control value ¯J is linearly bounded by the Kronecker combination
(cid:3)
of the initial value (cid:107)(xT

0 )(cid:107) for any control structure.

0 ⊗ xT

4 Numerical Example

We consider a multi-agent network with 6 agents following the interaction structure shown in Fig.
1. We consider each agent to follow a consensus dynamics with its neighbors such that:

(cid:88)

˙xi =

j∈Ni,i(cid:54)=j

αij(xj − xi) + ui, xi(0) = xi0,

(52)

where αij > 0 are the coupling coeﬃcients. We consider the state and input matrix to be:

A =









−5
2
2 −6
3
0
0
0

3
0
0 −5
0
1
3

0
0
2
2 −2
0
0

0
1
0
0
0 −4
0

0
3
0
0
3
3 −6









, B = I6.

(53)

The dynamics given as above is generally referred to as a Laplacian dynamics with A.1n = 0 result-
ing into a zero eigenvalue. We would like the controller to improve the damping of the eigenvalues
closer to instability. The eigenvalues of the system are −10.00, −8.27, −6.00, −3.00, −0.72, −0.00.
We choose initial conditions as [0.3, 0.5, 0.4, 0.8, 0.9, 0.6]T . We consider two scenarios with struc-
tured gains: A. IK(1, 2) = 0, IK(1, 6) = 0, IK(1, 1) = 0, IK(2, 4) = 0, IK(2, 6) = 0, IK(3, 4) =
0, IK(3, 5) = 0, and B. Along with the sparsity pattern in scenario A, we also have IK(4, 1) =
0, IK(4, 2) = 0, IK(5, 4) = 0, IK(5, 3) = 0, IK(6, 4) = 0, IK(6, 1) = 0. Please note that we consider an
arbitrary sparsity pattern. We assume that the states of all the agents can be measured.

We consider the design parameters as Q = 30I6, R = I6. First we describe the scenario A.
Here we have n = 6, m = 6, and number of non-zero elements of K is 29, therefore, we require to
gather data for at-least 2(n(n + 1)/2 + |K|) samples, which is 100 data samples. We consider the
time step to be 0.01s, and gather data for 1.4 s. The iteration for K and P took around 0.02 s on
an average with a Macbook laptop of Catalina OS, 2.8 GHz Quad-Core Intel Core i7 with 16 GB
RAM. During exploration, we have used sum of sinusoids based perturbation signal to persistently
excite the network. Please note that the majority of the learning is spent on the exploration of the

9

Figure 2: Scenario A: State trajectories during exploration and control implementation

Figure 3: Scenario A: P convergence

Figure 4: Scenario A: K convergence

system because of the requirement of persistent excitation and the least square iteration is a order
of magnitude smaller in comparison to the exploration time. With faster processing units, the least
square iteration can be made much faster. Fig. 2 shows the state trajectories of the agents during
exploration, and also with control implementation phase. The structured control gain learned in
this scenario is given as:

K A

K =











0.0000
1.0455
1.2527
0.2901
0.1468
0.3306

0.0000
2.7516
0.2686
0.0364
0.7485
1.1411

1.2527
0.2686
2.9976
1.0471
0.0288
0.0670

0.2901
0.0000
0.0000
4.1729
0.0025
0.0054

0.1468
0.7485
0.0000
0.0025
3.2813
1.2978











0.0000
0.0000
0.0670
0.0054
1.2978
2.8851

(54)

The total cost comes out to be 12.4705 units. Fig. 3-4 show that the P and K iteration converges
after around 6 iterations. The structured solution also matches with the model-based solution from
Theorem 3 with high accuracy. Whenever, the learning has been performed for unstructured LQR
control gain, the solution comes out to be:

Kunstruc =











2.9234
0.7255
1.1487
0.3057
0.1397
0.2342

0.7255
2.6395
0.2282
0.0418
0.7435
1.0987

1.1487
0.2282
2.9751
1.0436
0.0269
0.0547

0.3057
0.0418
1.0436
4.0820
0.0001
0.0041

0.1397
0.7435
0.0269
0.0001
3.2790
1.2881











0.2342
1.0987
0.0547
0.0041
1.2881
2.7975

,

(55)

with the objective of 12.0428 units.

We then perform similar experimentation with the scenario B, where we have removed more
set of communication links. Here, the number of non-zero elements of K is 23. We need to gather

10

012345Time (sec)00.511.5State trajectoriesx1x2x3x4x5x6246810iteration02468101214||Pk - Pk-1||246810iteration024681012||Kk - Kk-1||Figure 5: Scenario B: State trajectories during exploration and control implementation

Figure 6: Scenario B: P convergence

Figure 7: Scenario B: K convergence

at-least 88 data samples, therefore, we perform around 1 s of exploration with 0.01 s time step.
The structured control learned for this scenario is given by,

K B

K =











0.0000
1.0617
1.2544
0.0000
0.1561
0.0000

0.0000
2.7750
0.2702
0.0000
0.7683
1.2338

1.2544
0.2702
2.9979
1.0470
0.0000
0.0725

0.2898
0.0000
0.0000
4.1729
0.0000
0.0000

0.1561
0.7683
0.0000
0.0022
3.3002
1.3786











0.0000
0.0000
0.0725
0.0046
1.3786
0.0000

,

(56)

with the objective of 12.9764 units. The state trajectories for scenario B during the exploration
and the control implementation is given in Fig. 5. The convergence of the least squre iterates
for P and K are shown in Figs. 6-7, where we can see that convergence is being reached after 8
iterations. Also, the damping of the eigenvalues are improved with the control with the closed-loop
eigenvalues are placed at −10.61, −3.58, −4.22, −5.70, −9.19, −7.91.
5 Conclusions and path forward

This paper discussed model-free learning based computation of stable and close-to-optimal feedback
gains which are constrained to a speciﬁc yet general structure. We ﬁrst formulated a model-based
condition to compute structured feedback control, which resulted in a modiﬁed algebraic Riccati
equation (ARE). Techniques from dynamic programming and Lyapunov based stability criterion
ensured the stability and the convergence. Thereafter, we embed the model-based condition into
the a RL algorithm where trajectories of states and controls are used to compute the gains. The
algorithm uses an interleaved policy evaluation and policy improvement steps. We also analyzed
the sub-optimality of the structured optimal control solution in comparison with the unstructured

11

0.511.522.533.54Time (sec)00.511.5State trajectoriesx1x2x3x4x5x62468iteration02468101214||Pk - Pk-1||246810iteration0246810||Kk - Kk-1||optimal solutions. Simulations on a multi-agent network with constrained communication infras-
tructure validated the theoretical results. Our future work will investigate the possibility to embed
a prescribed degree of stability margin along with the structural constraint, as well as necessary
and suﬃcient conditions for the existence of the optimal structured control.

References

[1] L. Bakule, “Decentralized control: an overview,” Annual reviews in control, vol. 32, pp. 87–98,

2008.

[2] F. Deroo, “Control of interconnected systems with distributed model knowledge,” PhD thesis,

TU Munchen, Germany, 2016.

[3] M. Rotkowitz and S. Lall, “A characterization of convex problems in decentralized controlast,”

IEEE Transactions on Automatic Control, vol. 51, no. 2, pp. 274–286, 2006.

[4] L. Lessard and S. Lall, “Quadratic invariance is necessary and suﬃcient for convexity,” in

Proceedings of the 2011 American Control Conference, 2011, pp. 5360–5362.

[5] C. Langbort, R. S. Chandra, and R. D’Andrea, “Distributed control design for systems inter-
connected over an arbitrary graph,” IEEE Transactions on Automatic Control, vol. 49, no. 9,
pp. 1502–1519, 2004.

[6] P. Massioni and M. Verhaegen, “Distributed control for identical dynamically coupled systems:
A decomposition approach,” IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 124–
135, 2009.

[7] M. Fardad, F. Lin, and M. R. Jovanovi´c, “Sparsity-promoting optimal control for a class
of distributed systems,” in Proceedings of the 2011 American Control Conference, 2011, pp.
2050–2055.

[8] J. C. Geromel, “Structural constrained controllers for linear discrete dynamic systems,” IFAC

Proceedings Volumes, vol. 17, no. 2, pp. 435 – 440, 1984, 9th IFAC World Congress.

[9] R. Sutton and A. Barto, Reinforcement learning - An introduction. MIT press, Cambridge,

1998, 1998.

[10] C. Watkins, “Learning from delayed systems,” PhD thesis, King’s college of Cambridge, 1989.

[11] D. P. Bertsekas, Dynamic Programming and Optimal Control: Approximate Dynamic Pro-

gramming, 4th ed. Athena Scientiﬁc, Belmont, MA, USA., 2012.

[12] W. Powell, Approximate dynamic programming. Wiley, 2007.

[13] D. Vrabie, O. Pastravanu, M. Abu-Khalaf, and F. Lewis, “Adaptive optimal control for
continuous-time linear systems based on policy iteration,” Automatica, vol. 45, pp. 477–484,
2009.

[14] Y. Jiang and Z.-P. Jiang, “Computational adaptive optimal control for continuous-time linear
systems with completely unknown dynamics,” Automatica, vol. 48, pp. 2699–2704, 2012.

[15] F. Lewis, Optimal Control. Wiley, New York, 1995.

12

[16] P. Ioannou and B. Fidan, Adaptive control tutorial. SIAM, 2006, 2006.

[17] Y. Jiang and Z.-P. Jiang, Robust Adaptive Dynamic Programming. Wiley-IEEE press, 2017.

[18] K. Vamvoudakis, “Q-learning for continuous-time linear systems: A model-free inﬁnite horizon

optimal control approach,” Systems and Control Letters, vol. 100, pp. 14–20, 2017.

[19] B. Kiumarsi, K. Vamvoudakis, H. Modares, and F. Lewis, “Optimal and autonomous con-
trol using reinforcement learning: A survey,” IEEE Trans. on Neural Networks and Learning
Systems, 2018.

[20] S. Mukherjee, H. Bai, and A. Chakrabortty, “On model-free reinforcement learning of reduced-
order optimal control for singularly perturbed systems,” in IEEE Conference on Decision and
Conrol 2018, Miami, FL, USA.

[21] ——, “Reduced-dimensional reinforcement learning control using singular perturbation ap-

proximations,” arXiv 2004.14501, 2020.

[22] ——, “Block-decentralized model-free reinforcement learning control of two time-scale net-

works,” in American Control Conference 2019, Philadelphia, USA.

[23] Y. Jiang and Z. Jiang, “Robust adaptive dynamic programming for large-scale systems with
an application to multimachine power systems,” IEEE Transactions on Circuits and Systems
II: Express Briefs, vol. 59, no. 10, pp. 693–697, Oct 2012.

[24] L. Furieri, Y. Zheng, and M. Kamgarpour, “Learning the globally optimal distributed lq

regulator,” arXiv:1912.08774v3, 2020.

[25] D. Kleinman, “On an iterative technique for riccati equation computations,” IEEE Trans. on

Automatic Control, vol. 13, no. 1, pp. 114–115, 1968.

[26] J. guang Sun, “Perturbation theory for algebraic riccati equations,” SIAM Journal on Matrix

Analysis and Applications, vol. 19, pp. 39–65, 1998.

13

