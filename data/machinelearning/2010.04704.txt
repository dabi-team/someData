Recursive Top-Down Production for Sentence Generation
with Latent Trees

Shawn Tan∗
Mila / University of Montreal
tanjings@mila.quebec

Yikang Shen∗
Mila / University of Montreal
yi-kang.shen@umontreal.ca

Timothy J. O’Donnell
Dept. of Linguistics
Mila / McGill University
Canada CIFAR AI Chair

Abstract

Alessandro Sordoni
Microsoft Research

Aaron Courville
Mila / University of Montreal
Canada CIFAR AI Chair

0
2
0
2

t
c
O
9

]
L
C
.
s
c
[

1
v
4
0
7
4
0
.
0
1
0
2
:
v
i
X
r
a

We model
the recursive production prop-
erty of context-free grammars for natural
and synthetic languages. To this end, we
present a dynamic programming algorithm
that marginalises over latent binary tree struc-
tures with N leaves, allowing us to compute
the likelihood of a sequence of N tokens un-
der a latent tree model, which we maximise to
train a recursive neural function. We demon-
strate performance on two synthetic tasks:
SCAN (Lake and Baroni, 2017), where it
outperforms previous models on the LENGTH
split, and English question formation (McCoy
et al., 2020), where it performs comparably
to decoders with the ground-truth tree struc-
ture. We also present experimental results on
German-English translation on the Multi30k
dataset (Elliott et al., 2016), and qualitatively
analyse the induced tree structures our model
learns for the SCAN tasks and the German-
English translation task.

1

Introduction

Given the hierarchical nature of natural language,
tree structures have long been considered a funda-
mental part of natural language understanding. In
recent years, a number of studies have shown that
incorporating these structures into deep learning
systems can be beneﬁcial for various natural lan-
guage tasks (Socher et al., 2013; Bowman et al.,
2015; Eriguchi et al., 2016).

Various work has explored the introduction of
syntactic structures into recursive encoders, either
with explicit syntactic information (Du et al., 2020;
Socher et al., 2010; Dyer et al., 2016) or by means
of unsupervised latent tree learning (Williams
et al., 2018; Shen et al., 2019; Kim et al., 2019b).
Some attempts at formulating structured decoders
are Zhang et al. (2015a) and Alvarez-Melis and

∗Equal contribution

Figure 1: Our generative model is a recursive top-down
neural network that recursively splits a root node em-
bedding with some node-dependent probability. When
the splitting stops, it emits a word with some probabil-
ity. The joint probability of a sentence and its associ-
ated binary tree is the product of the probability of the
tree (1 − l1)(1 − l3)(1 − l5)l2l4l6l7 and the probabili-
ties of the word emitted at its leaves. We devise a novel
marginalisation algorithm over binary trees to compute
the likelihood of a sentence.

Jaakkola (2016) which propose binary top-down
tree LSTM architectures for natural language. Chen
et al. (2018) proposes a tree-structured decoder for
code generation. These methods require ground-
truth trees from an external source, and this extra
input may not be available for all languages or data
sources.

In this work, we propose a tree-based proba-
bilistic decoder model for sequence-to-sequence
tasks. Our model generates sentences from a latent
tree structure that aims to reﬂect natural language
syntax. The method assumes that each token in
a sentence is emitted at the leaves of a full but
latent binary tree (Fig. 1). The tree is obtained
by recursively producing node embeddings from
a root embedding with a recursive neural network.
Word emission probabilities are function of the leaf
embeddings. We describe a novel dynamic pro-
gramming algorithm for exact marginalisation over
the large number of latent binary trees.

1234576Investorssuﬀeredheavylosses1-l11-l31-l5l7l6l2l4 
 
 
 
 
 
Our generative model parametrizes a prior over
binary trees with a stick-breaking process, similar
to the “penetration probabilities” deﬁned in Mochi-
hashi and Sumita (2008). It is related to a long
tradition of unsupervised grammar induction mod-
els that formulate a generative model of sen-
tences (Klein and Manning, 2001; Bod, 2006; Klein
and Manning, 2005).

Unlike more recent bottom-up approaches such
as Kim et al. (2019a) which require the inside-
outside algorithm (Baker, 1979) to marginalise
over tree structures, our approach is top-down
and comes with an efﬁcient algorithm to perform
marginalisation. Top-down models can be useful,
as the decoder is encouraged by design to keep
global context while generating sentences (Du and
Black, 2019; G¯u et al., 2018).

In the next section, we will describe the algo-
rithm that marginalises over latent tree structures
under some independence assumptions. We ﬁrst
introduce these assumptions and show that by in-
troducing the notion of successive leaves, we can
efﬁciently sum over different tree structures. We
then introduce the details of the recursive archi-
tecture used. Finally, we present the experimental
results of the model in Section 5.

2 Method

2.1 Generative Process

We assume that each sequence is generated by
means of an underlying tree structure which takes
the form of a full binary tree, which is a tree for
which each node is either a leaf or has two children.
A sequence of tokens is produced with the follow-
ing generative process: ﬁrst, sample a full binary
tree T from a distribution p(T ). Denote the sets of
leaves of T as L(T ). Then for each leaf v in L(T ),
sample a token x ∈ V, where V is the vocabulary,
from a conditional distribution p(x|v).

Under this model, the probability of a sequence
x1:N can be obtained by marginalising over possi-
ble tree structures with N leaves:

p(x1:N ) =

=

(cid:88)

T
(cid:88)

T

p(x1:N , T )

p(x1:N |T )p(T )

(1)

given the tree structure, the probability of each
word is independent of the other words, i.e.:

p(x1:N |T ) =

N
(cid:89)

n=1

p(xn | Ln(T )),

(2)

where Ln(T ) represents the n-th leaf of T . In what
follows, we describe an algorithm to efﬁciently
marginalise over possible tree structures, such that
the involved distributions can be parametrized by
neural networks and can be trained end-to-end
by maximizing log-likelihood of the observed se-
quences. We ﬁrst describe how we model the prior
p(T ) and then how to compute p(x1:N ) efﬁciently.

2.2 Probability of a full binary tree

We model the prior probability of a full binary tree
p(T ) by using a branching process similar to the
stick-breaking construction, which can be used to
model a series of stochastic binary decisions un-
til success (Sethuraman, 1994). In our model, we
perform a series of binary decisions at each vertex,
starting at the root and branching downwards. Each
decision consists in whether to expand the current
node by creating two children or not. This binary
decision is therefore modeled with a Bernoulli ran-
dom variable.

Let us deﬁne a complete binary tree TC of depth
DC with vertices {v1, . . . , vM }, M = 2DC +1 − 1.
Each vertex above is associated with a Bernoulli
parameter l, θ = {l1, . . . , l2DC +1−1}, li ∈ [0, 1],
modeling its split probability. The probabilities
(1 − li) are similar to the “penetration probabilities”
mentioned in Mochihashi and Sumita (2008). A
full binary tree depth D ≤ DC is contained in TC,
so we will refer to it as an internal tree from here
on1. See Fig. 1 for an example of two internal trees
with three leaves. Its probability can be expressed
using parameters li as follows. The probability
p(T ) = π(root), where π is deﬁned recursively as:

π(vi) =






li

if vi ∈ L(T ),

(1 − li) ·

π(left(vi)) ·

else

π(right(vi))

(3)

We assume that the probability of sequences with
lengths different from the number of leaves in the
tree is 0. Our generative process prescribes that,

where left(vi) and right(vi) are the left child and
right child respectively.

1This is not to be confused with the notion of subtrees.

4

TC =

2

6

1

3

5

7

θ = (

l1

, l2

, l3

, l4

, l5

, l6

, l7

)

T1 =

2

1

3

4

6

T2 = 2

4

6

5

7

L(T1) = {1, 3, 6}

L(T2) = {2, 5, 7}

p(T1; θ) = (1 − l4)(1 − l2)l1l3l6

p(T2; θ) = (1 − l4)(1 − l6)l2l5l7

= m(v1) · m(v3) · m(v6)

= m(v2) · m(v5) · m(v7)

Figure 2: In this ﬁgure, root(TC) = v4. Given N = 3, there are two possible trees, T1 and T2. The probabilities
of the trees can be expressed as the recurrent process described in Equation 3, or as a product of m(·) at the leaf
vertices of the internal tree.

2.2.1 Memoizing the value at each vertex
We can compute Eq. (3) efﬁciently by storing a
partial computation for each vertex and multiplying
the values at the leaves to get the tree probability:

p(T ; θ) =

N
(cid:89)

n=1

m(Ln(T ))

(4)

where Ln(T ) denotes the vertex corresponding to
the n-th leaf of T . We deﬁne this value at the vertex
vi to be m(vi):

m(vi) = li

(cid:89)

(1 − lj)

2

1
|Vi→j |

(5)

vj ∈Vi→root

where Vi→j denotes the set of vertices in the path
from node vi to node vj inclusive. These values
can be efﬁciently computed with this top-down
recurrence relation:

m(vi) = ( ˜m(parent(vi)))

1

2 · li

˜m(vi) = ( ˜m(parent(vi)))

1

2 · (1 − li)

(6)

(7)

where the parent(vi) is the parent of vi, and
˜m(parent(root)) = 1. For example, in Fig. 2,
m(1) = (1 − l4)1/4(1 − l2)1/2l1 and we demon-
strate the case for two internal trees with D = 2
and N = 3 leaves.

We can then use Eq. (2) and Eq. (4) to write the

joint probability of a sequence and a tree:

p(x1:N , T ) =

N
(cid:89)

n=1

p(xn|Ln(T )) · m(Ln(T )) (8)

Note that the joint probability factorises as a prod-
uct over the token probability and the value at the
vertex. As we will see later, our method works by
traversing the leaves of all possible internal trees,
computing the product of the values at the leaves
along the way. Therefore, expressing the probabil-
ity of a full tree as a product of these values ensures
that marginalisation stays tractable.

4

4

4

2

2

2

3

3

3

6

6

6

5

5

5

7

7

7

1

1

1

M (·, n − 1)

M (·, n)

Figure 3: Successive leaf transitions for a tree of DC =
2. The arrows show the possible transitions from each
vertex. To enumerate T3 (trees with 3 leaves) we start
at any of the vertices the left boundary ( 1 , 2 , or 4 ),
and make 2 transitions (left-to-right arrows) over suc-
cessive leaves to any vertex in the right boundary ( 4 ,
6 or 7 ), keeping track of the vertices visited along
the way. There are two ways this can be done, which
are the examples shown in Figure 2.

2.3 Marginalising over trees

Now that we can compute the probability of a given
tree, we need to marginalise over all full binary
trees with exactly N leaves. We will denote this
|L(T )| = N }.
formally by the set TN = {T :
The crux of the problem surrounds marginalising
over TN . We know |TN | ≤ CN −1 , where Cn is
the n-th Catalan number 2, with equality occuring
when N ≤ DC − 1.

Successive leaves
In order to efﬁciently enumer-
ate all possible internal trees, we deﬁne a set of
admissible transitions between the vertices of TC.
First, let us deﬁne the left and right boundaries of
a TC. Starting from the root node, traversing down
the all left children recursively until the leftmost
leaf, all vertices visited in this process belong to
the left boundary Bl. This notion is similarly de-
ﬁned for all right children in the right boundary Br.
Given a vertex v, we deﬁne the successive leaves
of v as any of the next possible leaves in a internal

2https://oeis.org/A000108

We ﬁrst initialise the values at M (v, 1) at the left
boundary:

M (vi, 1) =

(cid:26) p(x1|vi) · m(vi)

0

if vi ∈ Bl
else

which should be the state of the table for all preﬁxes
sequences of length 1. Then for 1 < n ≤ N ,

M (vi, n) = p(xn|vi) · m(vi)

(cid:88)

M (vj, n − 1)

vj : (vj ,vi)∈S

(10)

where we see that Eq. (9) can be recovered by push-
ing the product p(xn|vi) · m(vi) inside the sum in
Eq. (10). The sum describes the situation when
vertices have more than one incoming arrow, as
depicted in Fig. 3. It should be noted that a large
number of these values will be zero, which signify
that there are no incomplete trees that end on that
vertex. In order to compute the marginalisation
over TN , we have to ﬁnally sum over the values at
the right boundary:

p(x1:N ) =

(cid:88)

vi∈Br

M (vi, N )

(11)

since valid full binary trees must also end on the
3. Note that the values of
right boundary of TC
any trajectory that do not form a full binary tree
by N − 1 iterations, i.e. those that do not reach
the right boundary, do not get summed. Another
interesting property is that full binary trees with
fewer leaves than N would have their trajectories
reach the right boundaries much earlier, and those
values do not get propagated forward once they do.

2.4 Decoding from the model

During decoding, we can perform the follow-
ing maximisation based on a modiﬁcation of the
marginalisation algorithm,

arg max
x1:N ,T

p(x1:N , T ).

(12)

This technique borrows heavily from Viterbi
(1967). We perform the same dynamic program-
ming procedure as above, but replacing summa-
tions with maximizations, and maintaining a back-
pointer to the summand that was the highest:

M ∗(vi, n) = p(xn|vi) · m(vi)
· max
(vj ,vi)∈S

(13)

M ∗(vj, n − 1)

3Since for any full binary tree, every node has either 0 or
2 children, this means that any full binary tree needs to have
one leaf in Br.

Figure 4: In a binary tree, the left boundary of any right
subtree are all successive leaves of the right boundary
of its corresponding left subtree.

binary tree in which v is a leaf. As an example, in
Figure 3, vertices 5 and 6 are successive leaves
of both vertices 2 and 3 . Therefore, if we start at
a vertex in the left boundary and travel along these
allowed transitions until we reach the right bound-
ary, the vertices visited along this path describe the
leaves of an internal tree. This notion is indepen-
dent of the length of any sequence, and a traversal
from the left boundary of TC to the right boundary
will induce the leaves of a valid internal T . As an
example, in Figure 3, the admissible transitions 1
→ 3 → 6 form a valid internal tree, as well as
1 → 3 → 5 → 7 .

To list all pairs of allowed transitions vi to vj,
we compute the Cartesian product of the vertices
in the right boundary of the left subtree and the
left boundary of the right subtree, and do this re-
cursively for each vertex. See Figure 4 for an il-
lustration of the concept. The pseudo-code for
generating all such transitions in a tree is shown
in Appendix B: SUCCESSIVELEAVES. The result
of SUCCESSIVELEAVES(root) is the set S, which
contains pairs of vertices (vi, vj) such that vj is
a successive leaf of vi. Taking N − 1 transitions
from the left boundary to the right boundary of TC
results in visiting the N leaves of an internal tree.
Proof is in Appendix A.

Marginalisation We can use our transitions S to
marginalise over internal trees with N leaves as
follows: we ﬁll a table M (v, n) that contains the
marginal probability of preﬁx x1:n, where we sum
over all partial trees for which vertex v has emitted
token xn:

M (vi, n) =
(cid:88)

(cid:89)

T : Ln(T )=vi

n(cid:48)≤n

p(xn(cid:48)|Ln(cid:48)(T )) · m(Ln(cid:48)(T ))

(9)

neural network with a ReLU hidden layer:

(cid:126)h = relu(W1(cid:126)hv + U1(cid:126)c + b1)
[(cid:126)cleft; (cid:126)cright] = tanh(layernorm(W2 · (cid:126)h + (cid:126)b2))
[(cid:126)gleft; (cid:126)gright] = sigmoid(W3 · (cid:126)h + (cid:126)b3)

(cid:126)hleft = (cid:126)gleft (cid:12) (cid:126)cleft + (1 − (cid:126)gleft) (cid:12) (cid:126)hv
(cid:126)hright = (cid:126)gright (cid:12) (cid:126)cright + (1 − (cid:126)gright) (cid:12) (cid:126)hv

where layernorm is layer normalization (Ba et al.,
2016). We ﬁx the hidden size to be two times of
the dimension of the input vertex embedding.

The splitting probability lv and the emission
probabilities p(x|v) are deﬁned as functions of the
vertex embedding:

p(x|v) = gx((cid:126)hv);

lv = gl((cid:126)hv)

(15)

The leaf prediction gl is a linear transform into a
two-dimensional output space followed by a soft-
max. The speciﬁc form of the emission probability
function gx can vary with the task. Unless speciﬁed,
gx is an MLP.

3.2 Procedural Description
Starting with the root representation (cid:126)hρ and its
eventual contextual information (cid:126)cρ, we recursively
apply f . This can be done efﬁciently in parallel
breadth-wise, doubling the hidden representations
at every level. We apply gl at each level, and then
Eq. (6) and Eq. (7) to get m(v), which depend only
on the parents. We then apply f recursively un-
til a pre-deﬁned depth DC. We transform all the
vertex embeddings using the emission function gx
in parallel, and multiply p(x | v) · m(v) for all
vertices and words in the vocabulary. We have now
computed the sufﬁcient statistics in order to apply
the algorithm described in the previous section to
compute the marginal probability of the observed
sentence.

DC is a hyper-parameter that depends on mem-
ory and time constraints: if DC is large, the number
of representations grows exponentially with it, as
does the time for computing the likelihood. If the
depth of the latent trees used to generate the data
has an upper bound, we can also restrict the class
of trees being learned by setting DC as well.

4 Related Work

Non-parametric Bayesian approaches to learning
a hierarchy over the observed data has been pro-
posed in the past (Ghahramani et al., 2010; Grifﬁths

Figure 5: Schema of a single production function ap-
plication. From the representation (cid:126)hi, (1) compute the
context vector c((cid:126)hi) by attending on the encoder, (2)
the distribution over word probabilities and the leaf
probability parameter l, are computed, (3) apply the
Cell(·, ·) function to produce the child representations
hj and hk. Repeat until the maximum depth is reached.

Since we do not know the length of the sequence
being decoded, we need to decide on a stopping
criteria. We know that any subsequent multiplica-
tion to values in M (·, ·) would decrease it, since
p(xn|vi) · m(vi) ≤ 1. Thus, we also know that if
the current best full sequence has probability p∗,
then if all probabilities at the frontier are < p∗, no
sequence with a higher probability can be found.
We can then stop the search, and return the cur-
rent best solution. Algorithm 2 in the Appendix C
contains the pseudo-code for decoding.

3 Architecture

3.1 Connectionist Tree (CTree) Decoder

We parameterize the emission probabilities p(x|vi)
and the splitting probability at each vertex li with
a recursive neural network. The neural network
recursively splits a root embedding into internal
hidden states of the binary tree structures via a
production function f :

((cid:126)hleft(v), (cid:126)hright(v)) = f ((cid:126)hv, (cid:126)cv)

(14)

where (cid:126)hv is the embedding of the vertex v and (cid:126)c
is a generic context embedding that can be option-
ally vertex dependent and carries external informa-
tion, e.g. it can be used to pass information in an
encoder-decoder setting.

We parameterise f ((cid:126)hv, (cid:126)c) as a gated two layer

c(hi)actv.actv.MLP𝞂𝞂hjhkhip(x|vi)let al., 2004). These works generally learn a prior on
tree-structured data, and assumes a common super-
structure that generated the corpus instead of as-
suming that each observed datapoint may have been
produced by a different hierarchical structure. Our
generative assumptions are generally stronger but
they allow us for tractable marginalisation without
costly iterative inference procedures, e.g. MCMC.

Our method shares similarities with the forward
algorithm (Baum and Eagon, 1967; Baum and
Sell, 1968) which computes likelihoods for Hid-
den Markov Models (HMM), and CTC (Graves
et al., 2006). While the forward algorithm fac-
tors in the transition probabilities, both CTC and
our algorithm have placed a conditional indepen-
dence assumption in the factorisation of the like-
lihood of the output sequence. The inside-outside
algorithm (Baker, 1979) is usually employed when
it comes to learning parameters for PCFGs. Kim
et al. (2019a) gives a modern treatment to PCFGs
by introducing Compound PCFGs. In this work,
the CFG production probabilities are conditioned
on a continuous latent variable, and the entire
model is trained using amortized variational infer-
ence (Kingma and Welling, 2013). This allows the
production rules to be conditioned on a sentence-
level random variable, allowing it to model cor-
relations over rules that were not possible with a
standard PCFG. However, all co-dependence be-
tween the rules can only be captured through the
global latent variable. In CTC, Compound PCFGs,
and our work, the fact that the dynamic program-
ming algorithm is differentiable is exploited to train
the model.

While typical language modelling is done with a
left-to-right autoregressive structure, there has been
recent work that change the conditional factorisa-
tion order (Cho et al., 2019; Yang et al., 2019),
and even learn a good factorisation order (Stern
et al., 2019; Gu et al., 2019). For hierarchical text
generation, Chen et al. (2018) and Zhang et al.
(2015b) have attempted to model this hierarchy
using ground-truth parse trees from a parser. How-
ever, the parser was trained based on parses an-
notated using rules designed by linguists, which
presents two challenges: (1) we may not always
have these rules, particularly when it comes to low-
resource languages, and (2) it may be possible that
the structure required for different tasks are slightly
different, enforcing the structure based on a univer-
sal parse structure may not be optimal. Jacob et al.

(2018) attempts to learn a tree structure using dis-
crete split and merge with REINFORCE (Williams,
1992). However, the method is known to have high
variance (Tucker et al., 2017).

There has also been some work that use sequen-
tial models for learning a latent hierarchy. Chung
et al. (2016) again uses discrete binary sampling
units to learn a hierarchy. Shen et al. (2018) en-
forces an ordering to the hidden state of the LSTM
(Hochreiter and Schmidhuber, 1997) that allows
the hidden representations to be interpreted as a
tree structure. In their follow up work, Shen et al.
(2019) encodes sequences to a single vector rep-
resentation, which we use in this work as the en-
coder.

5 Experiments

We evaluate our method on three different
sequence-to-sequence tasks. Unless otherwise
stated, we are using the Ordered Memory
(OM) (Shen et al., 2019) as our encoder. Further
details can be found in Appendix D.1.

5.1 SCAN

The SCAN dataset (Lake and Baroni, 2017) con-
sists of a set of navigation commands as well as
their corresponding action sequences. As an exam-
ple, an input of jump opposite left and walk
thrice shoud yield LTURN LTURN JUMP WALK
WALK WALK. The dataset is designed as a test bed
for examining the systematic generalization of neu-
ral models. We follow the experiment settings in
Bastings et al. (2018), where the different splits
test for different properties of generalisation. We
apply our model to the 4 experimentation settings
and compare our model with the baselines in the
literature (See Table 1).

The SIMPLE split has the same data distribu-
tion for both the training set and test set. The
TURN LEFT split partitions the data so that while
jump left, and turn right would be examples
present in the training set, turn left are not, but
the model must be able to learn from these exam-
ples to produce LTURN when it sees turn left as
input.

Lexical Attention Li et al. (2019) and Russin
et al. (2019) propose a similar parameterization of
the token output distribution based on key-value
attention: the hidden states of the decoder (queries)
attend on the hidden states of the encoder (keys),
but only a-contextual word embeddings are used as

MODEL

SIMPLE

+ TURN LEFT

+ JUMP

LENGTH

BASTINGS ET AL. (2018)
BASTINGS ET AL. (2018) - DEP
RUSSIN ET AL. (2019) (LA)
LI ET AL. (2019) (LA)
OM-SEQ Cell + LA

100 ± 0.0
100 ± 0.0
100 ± 0.0
99.9 ± 0.0
99.8 ± 0.0

59.1 ± 16.8
90.8 ± 3.6
99.9 ± 0.16
99.7 ± 0.4
99.4 ± 1.4

12.5 ± 6.6
0.7 ± 0.4
78.4 ± 27.4
98.8 ± 1.4
3.5 ± 8.1

18.1 ± 1.1
17.8 ± 1.7
15.2 ± 0.7
20.3 ± 1.1
20.9 ± 3.1

BIRNN-CTREE + LA

99.9 ± 0.0

85.5 ± 2.2

56.5 ± 15.8

19.8 ± 0.0

OM-CTREE

99.9 ± 0.1

93.0 ± 7.5

0.1 ± 0.2

40.3 ± 22.5

OM-CTREE + LA

100.0 ± 0.0

100.0 ± 0.0

80.1 ± 17.3

44.7 ± 33.5

Table 1: Results on the different splits on the SCAN dataset. The labels are written in the format ENCODER-
DECODER. CTREE + LA is our decoder with lexical attention. Mean and standard deviation are over 10 runs.

walk opposite left after look left twice

lturn

lturn look lturn look

lturn walk

Figure 6: Example of a tree inferred by our model from
SCAN.

values. This allows the model to make one-to-one
mappings between input token embeddings and
output token embeddings (e.g., jump in the input
always maps to JUMP in the output), resulting in
huge improvements in performance on the JUMP
split. We refer to this method as lexical attention
(LA).

Results We report results in Table 1. Our model
performs well on the SCAN splits. Figure 6 shows
one tree induced from a model trained on SIMPLE.
The resulting parses hint at the model learning to
“reuse” some lower-level concepts when twice ap-
pears in the input, for instance. The two most
challenging tasks are JUMP and LENGTH splits.
In JUMP, the input token jump only appears alone
during training and the model has to learn to use
it in different contexts during testing. Surprisingly,
this model fails to generalise in the JUMP split, sug-
gesting that the capability of our model to perform
well on the JUMP split may be dependent on the
hierarchical decoding as well as the leaf attention.
The LENGTH split partitions the data so that
the distribution of output sequences seen in the
training set is much shorter than those seen in the
test set. Interestingly, our model converges to a
solution that results in a 19.8% accuracy in 5 out
of the 10 random seeds we use. In the other runs,

the model achieves 25% or higher, with 2 runs
achieving > 99% accuracy. The high variance of
the model deserves more study, but we suspect
in the failure cases, the model does not learn a
meaningful concept of thrice. Overall, LENGTH
requires some generalisation at the structural level
during decoding, and has thus far been the most
challenging for current sequential models. Given
the results, we believe our model has made some
improvements on this front.

5.2 English Question Formation

McCoy et al. (2020) proposed linguistic syn-
thetic tasks to test for hierarchical inductive bi-
ases in models. One such task is the formation
of English questions: the zebra does chuckle
→ does the zebra chuckle ?. It gets challeng-
ing when further relative clauses are inserted into
the sentence: your zebras that don’t dance
do chuckle. The heuristic that may work in the
ﬁrst case — moving the ﬁrst verb to the front
of the sentence — would fail, since the right
output would be do your zebras that don’t
dance chuckle ?. The task involves having two
modes of generation, depending on the ﬁnal token
of the input sentence. If it ends with DECL, the de-
coder simply has to copy the input. If it ends with
QUEST, the decoder has to produce the question.
The authors argue, and provide evidence, that the
models that do this task well have syntactic struc-
ture. Like SCAN, a generalisation set is included
to test for out-of-distribution examples and only
the ﬁrst-word accuracy is reported for the generali-
sation set.

Results Training our model on this task, we
achieve comparable results to their models that are

MODEL

FULL (TEST)

FIRST-WORD (GEN.)

Ein ¨alterer Mann spielt ein Videospiel.

Structure information given
0.96
TREE-TREE
0.00
SEQ-TREE
0.96
TREE-SEQ

No structure information
SEQ-SEQ
SEQ-CTREE
OM-CTREE

0.88
1.00 ± 0.00
1.00 ± 0.00

†∗

†∗

0.99
0.90
0.13

0.03
0.83 ± 0.19
0.93 ± 0.07

Table 2: English Question Formation results. Our mod-
els are annotated with †, and we report mean and stan-
dard deviation over 5 runs. Models that use attention
are noted with *.

given the syntactic structure of the sentence, after
considering the results of the sequential models
that they used. The results for this task are reported
in Table 2.

5.3 Multi30k Translation

The Multi30k English-German translation task (El-
liott et al., 2016), is a corpus of short English-
German sentence pairs. The original dataset in-
cludes a picture for each pair, but we have ex-
cluded them to focus on the text translation task.
Our baseline models include an LSTM sequence-
to-sequence with attention, Transformer (Vaswani
et al., 2017), and a non-autoregressive model
LaNMT (Shu et al., 2020). For a fair compar-
ison, we trained all models with negative log-
likelihood loss or knowledge distillation (Kim and
Rush, 2016) if applicable.

Results As shown in Table 3, our model achieved
comparable performance to its autoregressive coun-
terparts, and outperforms the non-autoregressive
model. However, we did not observe signiﬁcant
performance improvements as a result of the gen-
eralisation capabilities shown in the previous ex-
periments. This suggests further study is needed
to overcome remaining issues before deep learning
models can really utilise productivity in language.
On the other hand, examples in Figure 7 shows
our model does acquire some grammatical knowl-
edge. The model tends to generate all noun phrases
(e.g. an older man, a video game) in separate
subtrees. But it also tends to split the sentence be-
fore noun phrases. For example, the model splits
the sub-clause while in the air into two differ-
ent subtrees. Similarly, previous latent tree induc-
tion models (Shen et al., 2017, 2018) also shows a
higher afﬁnity for noun phrases compared to adjec-

.

an

is playing a

older man

video game

Figure 7: Example of a tree inferred by our model from
Multi30K De-En.

EN-DE
PARAM BLEU

DE-EN
PARAM BLEU

TRANSFORMER
LSTM†

†

Non-autoregressive
LANMT‡
+ DISTILL
OM-CTREE
+ DISTILL

69M
34M

96M
96M
20M
20M

33.6
35.2

26.6
28.5
33.4
34.7

65M
30M

96M
96M
20M
20M

37.8
38.0

27.9
32.0
34.4
36.6

Table 3: Multi30K results. † — Implemented by Open-
‡ — Trained and ﬁne-
NMT (Klein et al., 2017).
tuned with the released code https://github.com/
zomux/lanmt.

tive and prepositional phrases.

6 Conclusion

In this paper, we propose a new algorithm for learn-
ing a latent structure for sequences of tokens. Given
the current interest in systematic generalisation and
compositionality, we hope our work will lead to
interesting avenues of research in this direction.

Firstly, the connectionist tree decoding frame-
work allows for different architectural designs for
the recurrent function used. Secondly, while the dy-
namic programming algorithm is an improvement
over a naive enumeration over different trees, there
is room for improvement. For one, exploiting the
sparsity of the M (·, ·) table can perhaps result in
some memory and time gains. Finally, the need
to recursively expand to a complete tree results in
exponential growth with respect to the input length.
These results, while preliminary, suggests that
the method holds some potential. The experimental
results reveal some interesting behaviours that re-
quire further study. Nevertheless, we demonstrate
that it performs comparably to current algorithms,
and surpasses current models in synthetic tasks that
have been known to require structure in the models
to perform well.

References

David Alvarez-Melis and Tommi S Jaakkola. 2016.
Tree-structured decoding with doubly-recurrent neu-
ral networks.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
arXiv preprint

ton. 2016. Layer normalization.
arXiv:1607.06450.

James K Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65(S1):S132–S132.

Joost Bastings, Marco Baroni,

Jason Weston,
Jump
Kyunghyun Cho, and Douwe Kiela. 2018.
to better conclusions: Scan both left and right.
In Proceedings of
the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP, pages 47–55.

Leonard E Baum and John Alonzo Eagon. 1967. An
inequality with applications to statistical estimation
for probabilistic functions of markov processes and
to a model for ecology. Bulletin of the American
Mathematical Society, 73(3):360–363.

Leonard E Baum and George Sell. 1968. Growth trans-
formations for functions on manifolds. Paciﬁc Jour-
nal of Mathematics, 27(2):211–227.

Rens Bod. 2006. An all-subtrees approach to unsuper-
In Proceedings of the 21st Interna-
vised parsing.
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, pages 865–872. Association
for Computational Linguistics.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-
to-tree neural networks for program translation. In
Advances in neural information processing systems,
pages 2547–2557.

Kyunghyun Cho, Hal Daum´e III, Sean Welleck, et al.
2019. Non-monotonic sequential text generation.
In Proceedings of the 2019 Workshop on Widening
NLP, pages 57–59.

Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259.

Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2016. Hierarchical multiscale recurrent neural net-
works. arXiv preprint arXiv:1609.01704.

Wenchao Du and Alan W Black. 2019. Top-down
structurally-constrained neural response generation
with lexicalized probabilistic context-free grammar.
In Proceedings of the 2019 Conference of the North

American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 3762–
3771.

Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J.
O’Donnell, Yoshua Bengio, and Yue Zhang. 2020.
Exploiting syntactic structure for better language
In Pro-
modeling: A syntactic distance approach.
ceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics, Seattle, Wash-
ington.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A Smith. 2016. Recurrent neural network
grammars. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 199–209.

Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-
cia Specia. 2016. Multi30k: Multilingual English-
German image descriptions. In Proceedings of the
5th Workshop on Vision and Language, pages 70–74.
Association for Computational Linguistics.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neural
machine translation. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 823–
833.

Zoubin Ghahramani, Michael I Jordan, and Ryan P
Adams. 2010. Tree-structured stick breaking for hi-
erarchical data. In Advances in neural information
processing systems, pages 19–27.

Alex Graves, Santiago Fern´andez, Faustino Gomez,
and J¨urgen Schmidhuber. 2006.
Connectionist
temporal classiﬁcation:
labelling unsegmented se-
quence data with recurrent neural networks. In Pro-
ceedings of the 23rd international conference on Ma-
chine learning, pages 369–376.

Thomas L Grifﬁths, Michael I Jordan, Joshua B Tenen-
baum, and David M Blei. 2004. Hierarchical topic
models and the nested chinese restaurant process. In
Advances in neural information processing systems,
pages 17–24.

Jetic G¯u, Hassan S. Shavarani, and Anoop Sarkar. 2018.
Top-down tree structured decoding with syntactic
connections for neural machine translation and pars-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 401–413, Brussels, Belgium. Association for
Computational Linguistics.

Jiatao Gu, Qi Liu, and Kyunghyun Cho. 2019.
Insertion-based decoding with automatically in-
ferred generation order. Transactions of the Asso-
ciation for Computational Linguistics, 7:661–676.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Athul Paul Jacob, Zhouhan Lin, Alessandro Sordoni,
Learning hierarchi-
and Yoshua Bengio. 2018.
cal structures on-the-ﬂy with a recurrent-recursive
model for sequences. In Proceedings of The Third
Workshop on Representation Learning for NLP,
pages 154–158.

Yoon Kim, Chris Dyer, and Alexander M Rush. 2019a.
Compound probabilistic context-free grammars for
grammar induction. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 2369–2385.

Yoon Kim and Alexander M Rush. 2016. Sequence-
arXiv preprint

level knowledge distillation.
arXiv:1606.07947.

Yoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kun-
coro, Chris Dyer, and G´abor Melis. 2019b. Unsu-
pervised recurrent neural network grammars. arXiv
preprint arXiv:1904.03746.

Diederik P Kingma and Max Welling. 2013. Auto-
arXiv preprint

encoding variational bayes.
arXiv:1312.6114.

Dan Klein and Christopher D Manning. 2001. Natu-
ral language grammar induction using a constituent-
context model. In Proceedings of the 14th Interna-
tional Conference on Neural Information Processing
Systems: Natural and Synthetic, pages 35–42.

Dan Klein and Christopher D Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context model. Pattern recognition,
38(9):1407–1419.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-
lart, and Alexander M. Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
In Proc. ACL.

Brenden M Lake and Marco Baroni. 2017. General-
ization without systematicity: On the compositional
skills of sequence-to-sequence recurrent networks.
arXiv preprint arXiv:1711.00350.

Yuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hes-
tness. 2019. Compositional generalization for prim-
itive substitutions. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 4284–4293.

R Thomas McCoy, Robert Frank, and Tal Linzen. 2020.
Does syntax need to grow on trees? sources of hier-
archical inductive bias in sequence-to-sequence net-
works. arXiv preprint arXiv:2001.03632.

Daichi Mochihashi and Eiichiro Sumita. 2008. The in-
In Advances in neural infor-

ﬁnite markov model.
mation processing systems, pages 1017–1024.

Jake Russin, Jason Jo, and Randall C O’Reilly. 2019.
Compositional generalization in a deep seq2seq
model by separating syntax and semantics. arXiv
preprint arXiv:1904.09708.

Jayaram Sethuraman. 1994. A constructive deﬁnition
of dirichlet priors. Statistica sinica, pages 639–650.

Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and
Aaron Courville. 2017. Neural language model-
ing by jointly learning syntax and lexicon. arXiv
preprint arXiv:1711.02013.

Yikang Shen, Shawn Tan, Arian Hosseini, Zhouhan
Lin, Alessandro Sordoni, and Aaron C Courville.
2019. Ordered memory. In Advances in Neural In-
formation Processing Systems, pages 5038–5049.

Yikang Shen, Shawn Tan, Alessandro Sordoni, and
Aaron Courville. 2018. Ordered neurons: Integrat-
ing tree structures into recurrent neural networks.
arXiv preprint arXiv:1810.09536.

Raphael Shu, Jason Lee, Hideki Nakayama, and
Latent-variable non-
Kyunghyun Cho. 2020.
autoregressive neural machine translation with deter-
ministic inference using a delta posterior. AAAI.

Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase repre-
sentations and syntactic parsing with recursive neu-
ral networks. In Proceedings of the NIPS-2010 Deep
Learning and Unsupervised Feature Learning Work-
shop, volume 2010, pages 1–9.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing,
pages 1631–1642.

Mitchell Stern, William Chan, Jamie Kiros, and Jakob
Uszkoreit. 2019. Insertion transformer: Flexible se-
In In-
quence generation via insertion operations.
ternational Conference on Machine Learning, pages
5976–5985.

George Tucker, Andriy Mnih, Chris J Maddison, John
Lawson, and Jascha Sohl-Dickstein. 2017. Rebar:
Low-variance, unbiased gradient estimates for dis-
crete latent variable models. In Advances in Neural
Information Processing Systems, pages 2627–2636.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE transactions on Information Theory,
13(2):260–269.

Adina Williams, Andrew Drozdov*, and Samuel R
Bowman. 2018. Do latent tree learning models iden-
Transac-
tify meaningful structure in sentences?
tions of the Association of Computational Linguis-
tics, 6:253–267.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for
language understanding. In Advances in neural in-
formation processing systems, pages 5754–5764.

Xingxing Zhang, Liang Lu, and Mirella Lapata. 2015a.
Top-down tree long short-term memory networks.
arXiv preprint arXiv:1511.00060.

Xingxing Zhang, Liang Lu, and Mirella Lapata. 2015b.
Tree recurrent neural networks with application to
language modeling. CoRR, abs/1511.00060.

A Proofs

In this context, all trees are rooted.

Deﬁnition 1. A full binary tree is a tree where each vertex has either 0 or 2 children.

Deﬁnition 2. A complete binary tree TC is a tree where each vertex that is not a leaf has 2 children.
Deﬁnition 3. An internal tree T of a complete binary tree TC is a full binary tree T such that root(T ) =
root(TC) and whose vertices and edges are a subset of TC.
Deﬁnition 4. The set T (TC) of all internal trees of TC.
Deﬁnition 5. L(T ) is the ordered set of all leaf nodes in T , starting from the left-most leaf to the
right-most leaf. Given a left and right subtree T (cid:48) and T (cid:48)(cid:48) of the tree T ,

L(T ) = [L(T (cid:48)); L(T (cid:48)(cid:48))]

Deﬁnition 6. Left-most leaf is L1(T ) and the right-most leaf is L|L(T )|(T )
Deﬁnition 7. Successive leaf transitions are pairs of vertices (vi, vj),

S(TC) =

(cid:91)

{(Ln(T ), Ln+1(T )) : 1 ≤ n < |L(T )|}

T ∈T (TC )

where Ln(T ) is the n-th leaf of T
Deﬁnition 8. A left boundary Bl(T ) of a tree is the set of vertices induced by recursively visiting the left
vertex from the root.

(cid:110)

Bl(T ) =

v : v = leftk(root), k > 1

∪ {root}

(cid:111)

The notion is similarly deﬁned for the right boundary Br.
Deﬁnition 9. The probability p(T ) = π(root), where π is deﬁned recursively as:

π(vi) =






li

if vi ∈ L(T ),

(1 − li) ·

π(left(vi)) ·

else

π(right(vi))

where left(vi) and right(vi) are the left child and right child respectively.
Proposition 1. If T (cid:48) and T (cid:48)(cid:48) are the left and right subtrees of T respectively, and T (cid:48)
of TC, then

C and T (cid:48)(cid:48)

C are subtrees

Proof.

T ∈ T (TC) → T (cid:48) ∈ T (T (cid:48)

C), T (cid:48)(cid:48) ∈ T (T (cid:48)(cid:48)
C)

root(TC) = root(T )
left(root(T )) = root(T (cid:48))

= left(root(TC)) = root(T (cid:48)

C)

Since the vertices of T (cid:48) and T (cid:48)(cid:48) are subsets of vertices of T (cid:48)
C. Therefore T (cid:48) ∈ T (T (cid:48)
trees of T (cid:48)

C), T (cid:48)(cid:48) ∈ T (T (cid:48)(cid:48)
C)

C and T (cid:48)(cid:48)

C and T (cid:48)(cid:48)

C respectively, they are each internal

Proposition 2. If for all vi ∈ L(TC) → li = 1, then

(cid:88)

p(T ) = 1

T ∈T (TC )

Proof. Base case: TC is is of depth 0, then T (TC) = {T }, where T = TC = root., and since root is a
leaf l = 1.

(cid:80)

Inductive case: Let the left and right subtrees of TC be T (cid:48)
T ∈T (T (cid:48)

C ) p(T ) = 1, and same for T (cid:48)(cid:48)

C

C and T (cid:48)(cid:48)

C respectively, and assume

(cid:88)

p(T )

T ∈T (TC )

= lroot +

(cid:88)

p(T )

T ∈(T (TC )\{root}

Second term has common factor, since root is not a leaf,

= lroot + (1 − lroot)

= lroot + (1 − lroot)

(cid:88)

π(root(T (cid:48))) · π(root(T (cid:48)(cid:48)))
T (cid:48)∈T (T (cid:48)
C )
T (cid:48)(cid:48)∈T (T (cid:48)(cid:48)
C )
(cid:88)
p(T (cid:48)) · p(T (cid:48)(cid:48))
T (cid:48)∈T (T (cid:48)
C )
T (cid:48)(cid:48)∈T (T (cid:48)(cid:48)
C )








= lroot + (1 − lroot)



(cid:88)





p(T (cid:48))
C )

(cid:88)

p(T (cid:48)(cid:48))
T (cid:48)(cid:48)∈T (T (cid:48)(cid:48)
C )



T (cid:48)∈T (T (cid:48)

By the inductive assumption,

= lroot + (1 − lroot) · 1 · 1
= 1

Proposition 3. Let

then,

Proof. We can write,

m(vi) = ( ˜m(parent(vi)))

1

2 · li

˜m(vi) = ( ˜m(parent(vi)))

1

2 · (1 − li)

p(T ) =

N
(cid:89)

n=1

m(Ln(T ))

N
(cid:89)

n=1

m(Ln(T )) =

(cid:89)

v∈V N

( ˜m(parent(v)))

1
2 · π(v)

(16)

where V N = L(T ), and |V N | = N .

If V 1, then V 1 = {root(T )}, then m(root(T )) = lroot(T ).
If |V N | > 1, since T is a full binary tree, then there exists at least two vertices vi, vj ∈ V such that

parent(vi) = parent(vj) = vk. Let V N −1 = (V \ {vi, vj}) ∪ {vk}. Then,

( ˜m(parent(v)))

1
2 · π(v)

(cid:89)

v∈V

= ( ˜m(parent(vk)))

1

2 · (1 − lk)π(vi)π(vj)

(cid:89)

( ˜m(parent(v)))

1
2 · π(v)

v∈(V \{vi,vj })

= ( ˜m(parent(vk)))

1

2 · π(vk)

(cid:89)

( ˜m(parent(v)))

1
2 · π(v)

v∈(V \{vi,vj })

(cid:89)

=
v∈V N −1

( ˜m(parent(v)))

1
2 · π(v)

Then V N −1 forms another full binary tree T (cid:48), where vk is now a leaf, and we can assign lk := π(vk)
Applying this identity, we can repeatedly reduce the number of factors by 1, until we get V 1

Proposition 4. If T is an internal tree of TC,

L1(T ) ∈ Bl(TC), L|L(T )|(T ) ∈ Br(TC)

Proof. If T = root, then the leftmost vertex is root, which is in Bl by deﬁnition.

Otherwise, from Deﬁnitions 3 & 1 we know that if left(v) for a given v is φ, then v is a leaf. We can
then ﬁnd the left-most leaf of T by recursively calling v = left(v), until left(v) = φ. Since all vertices of
T are vertices of TC, and both trees share root, the left-most leaf of T , v ∈ Bl

The argument for the rightmost vertex is symmetric.

Proposition 5. Let T (cid:48)

C and T (cid:48)(cid:48)

C be left and right subtrees of TC. Then,

S(TC) = S(T (cid:48)

C) ∪ S(T (cid:48)(cid:48)

C) ∪ (Bl(T (cid:48)

C) × Br(T (cid:48)(cid:48)

C))

Proof. TC is a complete tree so the left and right subtree T (cid:48)
any T ∈ T (TC), then by Deﬁnition 5, we can ﬁnd T (cid:48) and T (cid:48)(cid:48) which are internal trees of T (cid:48)
respectively, such that L(T ) = [L(T (cid:48)); L(T (cid:48)(cid:48))]. Then,

C are both complete trees. For
C and T (cid:48)(cid:48)
C

C and T (cid:48)(cid:48)

For 1 ≤ n < |L(T (cid:48))|,

(Ln(T ), Ln+1(T ))

= (Ln(T (cid:48)), Ln+1(T (cid:48))) ∈ S(T (cid:48)

C)

For |L(T (cid:48))| + 1 ≤ n < |L(T )|,

(Ln(T ), Ln+1(T ))

= (Ln−|L(T (cid:48))|(T (cid:48)(cid:48)), Ln−|L(T (cid:48))|+1(T (cid:48)(cid:48))) ∈ S(T (cid:48)(cid:48)
C)

by Deﬁnition 7.

For n = |L(T (cid:48))|, we know from Prop. 4,

Therefore,

Ln(T ) = Ln(T (cid:48)) ∈ Br(T (cid:48)
C)
Ln+1(T ) = L1(T (cid:48)(cid:48)) ∈ Bl(T (cid:48)(cid:48)
C)

(Ln(T ), Ln+1(T )) ∈ Bl(T (cid:48)

C) × Br(T (cid:48)(cid:48)
C)

B Successive Leaf Construction Algorithm

Algorithm 1 SUCCESSIVELEAVES

Input: vertex vi
Output: successive leaf transitions S = {(vj, vk), . . . }
Output: left boundary Bl = {i, . . . }
Output: right boundary Br = {i, . . . }
if vi is a leaf then

S ← {}
Bl, Br ← {vi}, {vi}

else

r ← SUCCESSIVELEAVES(left(vi))
r ← SUCCESSIVELEAVES(right(vi))

r × B(cid:48)(cid:48)
l )

l, B(cid:48)
S (cid:48), B(cid:48)
S (cid:48)(cid:48), B(cid:48)(cid:48)
l , B(cid:48)(cid:48)
S ← S (cid:48) ∪ S (cid:48)(cid:48) ∪ (B(cid:48)
Bl ← B(cid:48)
Br ← B(cid:48)(cid:48)

l ∪ {vi}
r ∪ {vi}

end if

C Decoding Algorithm

Algorithm 2 DECODEJOINT

Input: [p(x|v1), . . . p(x|v|V |)]
Output: x∗
for all vi ∈ V do

1:N ∗

arg(vi) ← arg maxx p(x = x|vi)

m∗
m∗(i) ← maxx p(x = x|vi)

end for
n ← 1
for all vi ∈ Bl do

M ∗(vi, 1) ← m∗(vi)

{Initialise}

end for
while maxv∈V M ∗(v, n) ≥ p∗ do

if maxvi∈Br M ∗(vi, n) > p∗ then

{Compute current best}

N ∗ ← t
v∗ ← arg maxvi∈Br M ∗(vi, N ∗)
arg(v∗, N ∗)]
x∗ ← [m∗

end if
t ← t + 1
for all vi ∈ V do

M ∗(vi, n) ← m∗(vi) · max

M ∗(vj, n − 1)

M ∗

arg(vi, n) ← arg max
vj |(vj ,vi)∈S

vj |(vj ,vi)∈S

M ∗(vj, n − 1)

end for
end while
for t ← N ∗ to 2 do
arg(v∗, t)
arg(v∗, t)].x∗

v∗ ← M ∗
x∗ ← [m∗

end for

{Backtrace}

D Experiments

D.1 Encoder

Before the embeddings are fed into the OM, we ﬁrst produce contextualised embeddings, by ﬁrst feeding it
into a one layer bidirectional Gated Recurrent Unit (GRU; Cho et al. 2014). We then expose the following
representations from the encoder to the decoder:

Encodeρ — Final representation computed by OM. Can be thought of as the root representation.
Encodeι — Intermediate states ( ˆM1 . . . ˆMS) concatenated. Can be thought of as the representations of

the internal nodes and the leaves.

Encode(cid:96) — Input representations to the OM. Can be thought of as the representation of the leaves.
Encodece — Contextualized embeddings from the GRU.
Encodee — Embeddings fed to the GRU.
We also use the Cell(·, ·) function as deﬁned in the paper.

D.2 SCAN sample trees

run around left thrice after turn left thrice

lturn

lturn lturn

lturn run

lturn run lturn run

lturn run lturn run

lturn run

lturn run lturn run

Figure 8: Erroneous tree example from the model trained on the LENGTH split.

D.3 Multi30k Translation Sample Trees

Ein ¨alterer Mann spielt ein Videospiel.

.

an

is playing a

older man

video game

Ein M¨adchen an einer K¨uste mit einem Berg im Hintergrund.

a girl

with a mountain

.

on

in

a shore

the background

Ein Junge greift sich ans Bein w¨ahrend er in die Luft springt.

.

the air

a boy

grabs

while in

his legs

Figure 9: Trees found by our model from Multi30K De-En.

