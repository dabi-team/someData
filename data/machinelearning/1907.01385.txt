0
2
0
2

g
u
A
9
2

]

G
L
.
s
c
[

3
v
5
8
3
1
0
.
7
0
9
1
:
v
i
X
r
a

1

Voting-Based Multi-Agent Reinforcement Learning
for Intelligent IoT

Yue Xu1,4, Zengde Deng2, Mengdi Wang3, Wenjun Xu1, Anthony Man-Cho So2, Shuguang Cui4

1Key Lab of Universal Wireless Communications, Ministry of Education
Beijing University of Posts and Telecommunications
2Department of Systems Engineering and Engineering Management,
The Chinese University of Hong Kong, Hong Kong
3Department of Operations Research and Financial Engineering, Princeton University
4Shenzhen Research Institute of Big Data and The Chinese University of Hong Kong, Shenzhen

Abstract—The recent success of single-agent reinforcement
learning (RL) in Internet of things (IoT) systems motivates the
study of multi-agent reinforcement learning (MARL), which is
more challenging but more useful
in large-scale IoT. In this
paper, we consider a voting-based MARL problem, in which
the agents vote to make group decisions and the goal
is to
maximize the globally averaged returns. To this end, we formulate
the MARL problem based on the linear programming form
of the policy optimization problem and propose a primal-dual
algorithm to obtain the optimal solution. We also propose a voting
mechanism through which the distributed learning achieves the
same sublinear convergence rate as centralized learning. In other
words, the distributed decision making does not slow down the
process of achieving global consensus on optimality. Lastly, we
verify the convergence of our proposed algorithm with numerical
simulations and conduct case studies in practical multi-agent IoT
systems.

Index Terms—Multi-agent reinforcement

learning, voting

mechanism, primal-dual algorithm

I. INTRODUCTION

Reinforcement learning (RL) aims at maximizing a cumu-
lative reward by selecting a sequence of optimal actions to
interact with a stochastic unknown environment, where the
dynamics is usually modeled as a Markov decision process
(MDP) [1]. Recently, single-agent RL has been successfully
applied to contribute adaptive and autonomous intelligence in
many Internet of things (IoT) applications, including smart
cellular networks [2–4], smart vehicle networks [5–7], and
smart unmanned aerial vehicles (UAV) networks [8–10]. De-
spite these successes, many recent studies envision that the IoT
entities, e.g., smartphones, sensors, and UAVs, will become
more decentralized, ad-hoc, and autonomous in nature [11],
[12]. This encourages the extension from single-agent RL
to multi-agent RL (MARL) to study the smart collaboration
among local entities in order to deliver a superior collective
intelligence, instead of simply treating them as independent
learners. However, MARL is more challenging since each
agent interacts with not only the environment but also the
other agents.

Although a number of collaborative learning models based
on MARL have been recently proposed [13–21], they usually

impose a discount factor γ ∈ (0, 1) on the future rewards
to render the problem more tractable, e.g., bounding the
cumulative reward [22–24]. However, many optimization tasks
in the IoT systems, e.g., resource allocation and admission
control, are long-run or non-terminating tasks. Existing studies
reveal that the RL methods based on discounted MDP may
yield a poor performance in the continuing tasks and become
computationally challenging when the discount factor is close
to one [1], [25–27]. This necessitates the development of
MARL models based on the undiscounted average-reward
MDP (AMDP) to tackle the continuing optimization tasks in
IoT systems. Moreover, existing MARL models usually exhibit
a performance degradation compared with their centralized
versions [21], [28] and only provide asymptotic convergence
to an optimal point [21], [28] or simply give empirical eval-
uations without theoretical guarantees [16–20]. In contrast, in
this paper, we give a sublinear convergence rate and theo-
retically prove that our proposed MARL model achieves the
same convergence rate as centralized learning, which makes it
a decent learning paradigm for distributed IoT systems.

Meanwhile, it is critical to specify a proper collaboration
protocol in order to promote safe and efﬁcient cooperations in
MARL systems. Many existing MARL models are built upon
the centralized learning with decentralized execution frame-
work where the agents perform iterative parameter consensus
with a centralized server [14], [18], [29], [30]. Moreover, the
centralized server is assumed to have access to the behavioral
policy or value functions of all distributed agents for model
training. However, in many IoT applications (e.g., location
services),
the privacy-sensitive data (e.g., policy or value
functions) should not be logged onto a centralized center due
to privacy and security concerns. On the other hand, recent
works also propose a number of decentralized solutions which
coordinate the agents through iterative parameter consensus
among neighboring agents [21–24]. However, this may give
rise to massive communication overhead in large-scale IoT net-
works. Besides, their convergence depends on the connectivity
properties of the networked agents, which can be topology
prohibitive in a randomly deployed IoT network. The above
issues motivate us to propose a new collaboration protocol for

 
 
 
 
 
 
MARL which can coordinate the local entities in a safe and
communication-efﬁcient way.

In this paper, we consider a collaborative MARL setting
where the agents vote to make group decisions and the aim
is to maximize the globally averaged return of all agents in
the environment. Our primary interest is to develop a sample-
efﬁcient model-free MARL algorithm built upon voting-based
coordinations in the context of inﬁnite-horizon AMDP. Par-
ticularly, the considered AMDP does not assume the future
rewards to be discounted while only needing to satisfy cer-
tain fast mixing property. This signiﬁcantly complicates our
analysis when compared with the discounted cases. The main
contributions are summarized as follows.

• We formulate the MARL problem in the context of
AMDP based on the linear programming form of the
policy optimization problem and propose a primal-dual
algorithm to obtain the optimal solution.

• We provide the ﬁrst sublinear convergence rate for solv-
ing the MARL problem for inﬁnite-horizon AMDP. The
proposed algorithm and theoretical analysis also cover
the single-agent RL as a special case, which makes them
more general.

• We propose a voting-based collaboration protocol for
the proposed MARL algorithm, through which the dis-
tributed learning achieves the same sublinear convergence
as centralized learning. In other words,
the proposed
distributed decision-making process does not slow down
the process of achieving global optimality. Moreover, the
proposed voting-based protocol has superior data privacy
and communication-efﬁciency than existing parameter-
consensus-based protocols.

In addition, we also verify the convergence of our proposed
algorithm through numerical simulations and conduct a case
study in a multi-agent IoT system to justify the learning
effectiveness.

The proposed model is promising for solving the long-run or
non-terminating optimization tasks in multi-agent IoT systems,
where distributed agents vote to determine a joint action,
aiming at maximizing the globally averaged return of all
agents. For example, the model can be employed to learn the
optimal resource (e.g., communication bandwidth and channel)
allocation policy for a group of IoT devices to improve the
overall capacity; learn the optimal on/off policy for a group
of base stations to improve the overall energy efﬁciency; learn
the optimal trajectory planning policy for a group of UAVs to
avoid collisions. Moreover, since the distributed agents only
need to exchange their vote information for collaboration,
without revealing their policy or value functions to each other,
the proposed model would be preferable in privacy-sensitive
applications, e.g., location services.

The remainder of this paper is organized as follows. Sec-
tion II reviews the existing works on MARL. Section III
introduces the problem formulations. Section IV presents the
voting-based multi-agent reinforcement
learning algorithm.
Section V presents the convergence analysis of our proposed
algorithm. Section VI discusses the simulation results. Finally,
Section VII concludes the paper.

2

√

Notation: For a vector x ∈ Rn, we denote its i-th com-
ponent as xi, its transpose as x(cid:62), and its Euclidean norm as
x(cid:62)x. For a positive number x, we write log x for its
(cid:107)x(cid:107) =
natural logarithm. For a vector e = (1, . . . , 1)(cid:62), we denote by
ei the vector with its i-th entry equaling 1 and other entries
equaling 0. For two probability distributions p, q over a ﬁnite
set X, we denote their Kullback-Leibler (KL) divergence as
DKL(p||q) = (cid:80)

x∈X p(x) log p(x)
q(x) .

II. RELATED WORK

Many existing model-free MARL algorithms are based
on the framework of Markov games [31–35] or temporal-
difference RL [16–21]. In the context of Markov games, the
study of MARL usually models the MARL as stochastic
games, such as cooperative games [31], zero-sum stochas-
tic games [32], [36–38], general-sum stochastic games [33],
decentralized Q-Learning [35], and the recent mean-ﬁeld
MARL [34]. Alternatively, the study of MARL in the context
of temporal-difference RL mainly originates from dynamic
programming, which learns by following the Bellman equa-
tion, including the ones based on deep neural networks [16–
20] and the ones based on linear function approximators [21].
However, ﬁrst, the above MARL models can only provide
asymptotic convergence [21] to an optimal point or simply pro-
vide empirical evaluations without theoretical guarantees [16–
20]. Second,
they are all based on the discounted MDP,
instead of the undiscounted AMDP. On the other hand, though
average-reward RL has received much attention in recent
years, most of them focus on the single-agent cases [25–
27], [39], [40]. The research on average-reward MARL still
undergoes exploration.

There are two lines of research in existing literature that fo-
cus on the saddle-point formulation of RL. One line studies the
saddle-point formulation resulted from the ﬁxed-point problem
of policy evaluation [22–24], [41], [42], i.e., learning the value
function of a ﬁxed policy. Among others, the works [23], [24]
provided the sample complexity analysis of policy evaluation
in the context of MARL, where the policies of all agents are
ﬁxed. The other line, which includes this paper, focuses on the
saddle-point formulation resulted from the policy optimization
problem [39], [40], where the policy is continuously updated
towards the optimal one. This makes the analysis substantially
more challenging than that for policy evaluation. In the single-
agent setting, our work is closely related to [39]. However, to
the best of our knowledge, our work is the ﬁrst to consider
solving a saddle-point policy optimization in the context of
MARL, which takes the coordination among multiple agents
into account. Moreover, we also provide numerical simulations
and case studies to corroborate our theoretical results, while
previous works mainly focus on theoretical analysis [39], [40].
Finally, most MARL models are based on the parameter-
consensus-based coordination, where the local agents con-
sensus their parameters with a centralized server [14], [18],
[29], [30] or their neighboring agents [21–24]. Although
many works adopted voting-based coordination in their pro-
posed learning algorithms [43–45], they are not developed for
MARL. A relevant work is [46], which proposed a dedicated

majority voting rule to coordinate the MARL agents under
discounted MDP, which, however, is a heuristic strategy with-
out theoretical guarantees and may not perform well on non-
terminating tasks.

III. PROBLEM FORMULATION

In this paper, we consider the MARL in the presence of a
generative model of the MDP [47–49]. The underlying MDP is
unknown but having access to a sampling oracle, which takes
an arbitrary state-action pair (i, a) as input and generates the
next state j with probability pij(a), along with an immediate
reward for each individual agent. The goal is to ﬁnd the
optimal policy of the unknown AMDP by interacting with
the sampling oracle. Such a simulator-deﬁned MDP has been
studied by existing literatures in the context of single-agent
RL, including the model-based RL [47–49] and model-free
RL [50], [51]. In what follows, we ﬁrst introduce the settings
of the multi-agent AMDP and then formulate the multi-agent
policy optimization problem as a primal-dual saddle point
optimization problem.

A. Multi-Agent AMDP

We focus on the inﬁnite-horizon AMDP, which aims at
optimizing the average-per-time-step reward over an inﬁnite
decision sequence. Existing works on RL usually impose a
discount factor γ ∈ (0, 1) on the future rewards to render the
problem more tractable; e.g., by making the cumulative reward
bounded. However, discounted RL may yield a poor perfor-
mance over long-run (especially non-terminating) tasks and
become computationally challenging when the discount factor
is close to one [1], [25–27]. In this paper, we do not assume
that the future rewards are discounted. Rather, we assume
that the AMDP satisﬁes certain fast mixing property (given
in Sec. V), which signiﬁcantly complicates our analysis when
compared with the discounted cases.

A multi-agent AMDP can be described by the tuple

(cid:16)

S, A, P, {Rm}M

m=1

(cid:17)

,

where S is the state space, A is the action space, P =
{pij(a) | i, j ∈ S, a ∈ A} is the collection of state-to-state
transition probabilities, and {Rm}M
m=1 is the collection of
local reward functions with Rm = (cid:8)rm
ij (a) | i, j ∈ S, a ∈ A(cid:9)
and M being the number of agents. We consider the setting
where the reward functions of the agents may differ from
each other and are private to each corresponding agent. We
assume that the reward rm
ij (a), where i, j ∈ S, a ∈ A, and
m = 1, . . . , M , lie in [0, 1]. This public state with private
reward setting is widely considered in many recent works on
collaborative MARL [21], [23], [24]. Moreover, we assume
that the multi-agent AMDP is ergodic (i.e., aperiodic and
recurrent), so that there is a unique stationary distribution
under any stationary policy. The MARL system selects the
action to take according to the votes from local agents. Each
agent determines its vote individually without communicating
with others. In particular, at each time step t, the MARL
system works as follows: 1) all agents observe the state it ∈ S;
2) each agent votes for the action at to take under it; 3) the

3

system executes at according to the votes; 4) the system shifts
to a new state it+1 ∈ S with probability pitit+1(at) and returns
the rewards {rm

m=1 to the agents.

(at)}M

itit+1

B. Multi-Agent Policy Optimization

We denote the global acting policy, which determines the
joint action to take, as πg ∈ Ξ ⊆ R|S|×|A|, where Ξ consists
of non-negative matrices whose (i, a)-th entry πg
i,a speciﬁes
the probability of taking action a in state i. The multi-agent
policy optimization problem aims at improving the global
acting policy by maximizing the sum of local average-rewards,
i.e.,

(cid:26)
¯vπg

max
πg

= lim
T →∞

Eπg(cid:20) 1
T

T
(cid:88)

M
(cid:88)

t=1

m=1

rm
itit+1

(cid:12)
(cid:12)
(cid:12)i1 = i
(at)

(cid:21)

, i ∈ S

(cid:27)
,

(1)
where Eπg
[·] denotes the expectation over all the state-action
trajectories generated by the MARL system when following
the acting policy πg. According to the theory of dynamic
programming [52], [53], the value ¯v∗ is the optimal average
reward to problem (1) if and only if it satisﬁes the following
Bellman equation:
¯v∗ + v∗(i)






M
(cid:88)

pij(a)v∗(j)+

pij(a)

(cid:88)

rm
ij (a)

, ∀ i ∈ S,

= max
a∈A

(cid:88)



j∈S

j∈S

m=1



(2)

where pij(a) is the transition probability from state i to state
j after taking the action a and v∗ ∈ R|S| is known as the
difference-of-value vector that characterizes the transient effect
of each initial state under the optimal policy [39]. Note that
there exist inﬁnitely many v∗ that satisfy (2); e.g., by adding
constant shifts. However, this does not affect our analysis.
More detailed descriptions of v∗ can be found in [39].

C. Saddle-Point Formulation

The Bellman equation in (2) can be written as the following

linear programming problem:

min
¯v,v

¯v

s.t.

¯v · e + (I − Pa) v −

¯rm
a ≥ 0, ∀ a ∈ A,

(3)

M
(cid:88)

m=1

where Pa ∈ R|S|×|S|
is the MDP transition matrix under
a ∈ R|S|
action a whose (i, j)-th entry is pij(a) and ¯rm
is the expected state-transition reward under action a with
i,a = (cid:80)
¯rm
ij (a), ∀i ∈ S. The dual of (3) can be
written as

j∈S pij(a)rm

max
µ

s.t.

(cid:88)

(cid:88)

M
(cid:88)

µi,a¯rm
i,a

m=1

i∈S
(cid:88)

a∈A
µ(cid:62)

a (I − Pa) = 0,

a∈A
(cid:88)

(cid:88)

i∈S

a∈A

µi,a = 1, µi,a ≥ 0,

(4)

(6)

(cid:41)

where µ is the dual variable. By linear programming strong
duality, if (¯v∗, v∗) and µ∗ are optimal solutions to the primal
and dual problems (3) and (4), respectively, then they satisfy
the zero complementarity gap condition:

0 =

(cid:88)

a∈A

(cid:32)

(µ∗

a)(cid:62)

¯v∗ · e + (I − Pa) v∗ −

M
(cid:88)

m=1

= ¯v∗ +

(cid:88)

a∈A

(cid:32)

(µ∗

a)(cid:62)

(I − Pa) v∗ −

M
(cid:88)

m=1

¯rm
a

.

(cid:33)

¯rm
a

(cid:33)

(5)

Observe that problems (3) and (4) involve rather compli-
cated constraints. Hence, it is common to consider their saddle-
point formulation, whose constraints are simpler:

min
v∈V

max
µ∈U

(cid:88)

a∈A

µ(cid:62)
a

(cid:32)

(Pa − I)v +

(cid:33)

¯rm
a

.

M
(cid:88)

m=1

Here,

V = R|S|, U =

(cid:40)

µ ∈ R|S|×|A| (cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

i∈S

a∈A

µi,a = 1, µ ≥ 0

are the primal and dual constraint sets, respectively. Later,
we shall focus on multi-agent AMDPs that satisfy certain fast
mixing property. This will allow us to use a smaller but still
structured primal constraint set V; see Sec. V.

It is known that there is a correspondence between ran-
domized stationary policies and feasible solutions to the dual
problem (4) [52]. In particular, given an optimal dual solution
µ∗ ∈ R|S|×|A|, the optimal acting policy πg can be obtained
i,a/ (cid:80)
via π∗
i,a. Hence, our goal now is to obtain
an optimal dual solution µ∗.

i,a = µ∗

a∈A µ∗

IV. VOTING-BASED LEARNING ALGORITHM

In this section, we propose a voting mechanism that spec-
iﬁes how local votes determine the global action. Then,
we prove that the voting mechanism yields an equivalence
between the update on the global acting policy and that on
the distributed voting policies. Consequently, problem (6) can
be solved in a distributed manner, and we propose a primal-
dual learning algorithm for it.

A. Voting Mechanism

We denote the pair of primal and dual variables correspond-
ing to the global acting policy πg as vg and µg, respectively.
We also introduce a pair of local primal and dual variables
corresponding to each local voting πm (m = 1, . . . , M ) as
vm and µm, where πm ∈ Ξ ⊆ R|S|×|A| is a randomized
stationary policy. Then, the voting mechanism takes the form

µg,t
i,a ∝

M
(cid:89)

m=1

µm,t
i,a .

4

B. Primal-Dual Learning Algorithm

We now develop a primal-dual learning algorithm to solve
problem (6) in a distributed manner based on a double-
sampling strategy. Recall that we consider the MARL under
a generative MDP, where the agents are interacting with a
black-box sampling oracle to learn the optimal policy. The
sampling oracle works in a similar way as the experience
replay used in deep RL models [4], [14], [18], [20]. In practical
applications, the sampling oracle or experience replay can be
placed in a centralized node which can communicate with the
local agents, as in many existing MARL frameworks [14],
[18], [29], [30]. However, it only needs to collect the vote
information µm
i,a from the agents in order to coordinate the
sampling during the learning process, instead of performing
iterative parameter consensus as existing methods [14], [18],
[29], [30]. The detailed procedure is provided in Algorithm 1.
In what follows, we ﬁrst introduce the local dual and primal
updates in our algorithm. Then, we prove that the local updates
are equivalent to the global updates if the voting mechanism
is speciﬁed properly.

1) Local Dual Update: We update the local dual variables
based on uniform sampling. Speciﬁcally, the ﬁrst state-action
pair (it, at) to update the local dual variables is sampled with
uniform probability pdual
|S|·|A| . The MARL system then
shifts to the next state jt conditioned on (it, at) and returns
the local rewards {rm
m=1 to the agents. The local dual
itjt
variable µm,t of agent m is updated as

i,a = 1

(at)}M

µm,t+1
i,a =

i,a exp (cid:8)∆m,t
(cid:26) µm,t
µm,t
i,a ,

i,a

(cid:9),

if i = it, a = at,
otherwise,

(7)

where

∆m,t

i,a = β

(cid:32) 1

β log xt + vt
M

j − vt

i − C

(cid:33)

+ rm

ij (a)

(8)

with (i, a, j) = (it, at, jt), β > 0 being the step-size, C being
a parameter to be specﬁed, and

xt =

(cid:80)

i∈S,a∈A

1
(cid:81)M

m=1 µm,t

i,a

.

(9)

Here, xt can be viewed as the proportion between the locally
recovered partial derivatives and the global true partial deriva-
tives of the minimax objective in (6). It also deﬁnes the explicit
form of the voting mechanism; see Lemma 1 below. However,
it is important to note that we do not need to compute xt in
our algorithm, as it does not inﬂuence the sampling in the
subsequent primal update step and is used purely for analysis
purposes. In other words, one can remove the term of log xt
from (8) without inﬂuencing the learning performance.

2) Local Primal Update: We update the local primal vari-
ables based on probability sampling, where the probability
is speciﬁed by the dual variables. Speciﬁcally, the second
state-action pair (it, at) to update the local primal variables
is sampled with probability

The voting mechanism indeed reveals the relationship between
the global acting policy and the local voting policies.

pprimal
it,at

=

(cid:81)M

m=1 µm,t
it,at
(cid:81)M
m=1 µm,t

i,a

i∈S,a∈A

(cid:80)

.

(10)

The system then shifts to the next state jt conditioned on
(it, at), and returns the local rewards to the agents. The local
primal variable vt is updated as

We remark that the global primal-dual updates (14) are con-
ditionally unbiased partial derivatives of the minimax objective
given in (6).

where

vt+1 = ΠV

(cid:8)vt + dt(cid:9) ,

dt = α(ei − ej)

(11)

(12)

Proof. Recall that the local dual variable µm,t of agent m is
updated by (7). We now prove a recursive relationship between
µg,t+1
i,a as follows. Given (i, a) = (it, at), starting
i,a
from the voting mechanism deﬁned in (13), we have

and µg,t

5

with (i, j) = (it, jt); α > 0 is the step-size; ΠV {·} denotes
the projector onto the search space V, which will be deﬁned
in Sec. V. Note that the local primal update is identical across
the agents. Hence, we use the same notation vt
i in the primal
update for all the agents in the sequel.

3) Communication: The centralized sampling oracle needs
to collect the vote information µm
i,a to compute the probability
pprimal
according to (10) and returns the reward information
it,at
to the agents. However, note that the vote information µm
i,a
and the reward information rm
i,j(a) of each agent is a scalar,
such that the communication overhead at each learning step of
our method only scales as O(M ). In contrast, most existing
MARL methods are developed based on parameter consensus,
where local agents need to reach consensus on its value or
policy function with a centralized center [14], [18], [29], [30]
or their nearby agents [21–24]. Since the value or policy
function scales as O(|S| · |A|), the communication overhead
at each learning step of their models scales as O(M ·|S|·|A|).
Although this cost can be reduced if they adopt linear or
nonlinear function to approximate the value or policy function,
it is still related to the size of the function approximators,
which can be enormous if they are deep neural networks.
Moreover, the exchanged information in our algorithm is the
vote information, instead of the privacy-sensitive policy or
value information, which can alleviate privacy and security
concerns considerably.

4) Equivalent Global Update: We now prove that with a
properly speciﬁed voting mechanism, the primal-dual updates
on the local voting policies are equivalent to the centralized
primal-dual updates on the global acting policy.

Lemma 1 (Equivalent Global Update): By specifying the

voting mechanism as

µg,t+1
i,a = xt+1

M
(cid:89)

µm,t+1
i,a

m=1
M
(cid:89)

(cid:16)

i,a exp (cid:8)∆m,t
µm,t

i,a

(cid:9)(cid:17)

µm,t
i,a exp

(cid:40) M
(cid:88)

(cid:41)

∆m,t
i,a

= xt+1

= xt+1

m=1
M
(cid:89)

m=1

m=1
(cid:40) M
(cid:88)

m=1

(cid:41)

∆m,t
i,a

= xt+1(xt)−1µg,t

i,a exp

(cid:40)

(cid:32)

= xt+1µg,t

i,a exp

β

j − vt
vt

i − C +

(cid:33)(cid:41)

rm
ij (a)

.

M
(cid:88)

m=1

Hence, using the deﬁnition of ∆g,t
update based on ∆m,t
global dual update based on ∆g,t

i,a in (15), the local dual
i,a can be equivalently expressed as the
i,a, i.e., (14a) holds.

As for the local primal update, since the oracle generates the
given by (10), which is
i,a given in (13),
(cid:4)

second sample with probability pprimal
exactly the same as the global dual variable µg,t
the local and global primal updates are identical.

i,a

Lemma 2 (Unbiasedness): Consider the voting mechanism
in Lemma 1. Let Ft be the ﬁltration at time t, i.e., information
about all the state-action pair sampling and state transition
right before time t. Then, the dual update weight ∆g,t
i,a is, up to
a constant shift, a multiple of the conditional partial derivative
of the minimax objective in (6) with respect to µi,a:

µg,t
i,a = xt

M
(cid:89)

m=1

µm,t
i,a ,

(13)

E(cid:2)∆g,t

i,a | Ft

(cid:3)

where xt is given by (9), the local primal-dual updates (7)
and (11) are equivalent to the following global primal-dual
updates:

i,a = xt+1µg,t
µg,t+1
vt+1 = ΠV

(cid:8)vt + dt(cid:9) .

i,a exp (cid:8)∆g,t

i,a

(cid:9) , ∀ i ∈ S, a ∈ A,

Here,

(cid:32)

∆g,t

i,a = β

j − vt
vt

i − C +

(cid:33)

rm
ij (a)

M
(cid:88)

m=1

(14a)

(14b)

(15)

and dt = α(ei − ej), where (i, a) = (it, at) with probability
µg,t
and j = jt is obtained from the system by conditioning
it,at
(cid:4)
on (it, at).

=

β
|S| · |A|

(cid:32)

(Pa − I)vt +

(cid:33)

¯rm
a − C · e

M
(cid:88)

m=1

,

i

∀ i ∈ S, a ∈ A.

Moreover, the primal update weight dt
i is a multiple of the
conditional partial derivative of the minimax objective in (6)
with respect to vi:

E (cid:2)dt | Ft

(cid:3) = α

(cid:88)

a∈A

(I − Pa)(cid:62)µg,t
a .

(cid:4)

Proof. For arbitrary i ∈ S and a ∈ A, we use (15) to compute

1
β

· E (cid:2)∆g,t

i,a | Ft


(cid:3)



=

1
|S| · |A|

(cid:88)



pij(a)vt

j − vt
i



j∈S


+

1
|S| · |A|

(cid:88)

M
(cid:88)



pij(a)rm

ij (a) − C





j∈S

m=1

=

1
|S| · |A|

(cid:32)

(Pa − I)vt +

M
(cid:88)

m=1

¯rm
a − C · e

(cid:33)

.

i

On the other hand, using (12) and the fact that the state-
action pair for updating the primal variables is generated with
probability µg,t, we compute, for an arbitrary i ∈ S,

(cid:3)

E (cid:2)dt | Ft


Pr (it = i | Ft) ei −

= α

(cid:88)



i∈S







(cid:88)

j∈S

Pr (jt = j | Ft) ej



= α

(cid:88)



(cid:88)

µg,t
i,aei −

(cid:88)

(cid:88)

(cid:88)

pij(a)µg,t

i,aej



a∈A

j∈S

i∈S

a∈A

i∈S
(cid:88)

= α

(I − Pa)(cid:62)µg,t
a .

a∈A

This completes the proof.

(cid:4)

V. THEORETICAL RESULTS

In this section, we present

the convergence analysis of
Algorithm 1. We start by making the following assumption
on the considered multi-agent AMDP. A similar assumption
has also been used in [39], [40] for the case of a single-agent
RL.

Assumption 1: There exists a constant t∗

mix > 0 such that

for any stationary policy πg, we have

t∗
mix ≥ min

t

(cid:26)

(cid:12)
(cid:12) (cid:107)(P πg
(cid:12)

t

)t(i, ·) − νπg

(cid:107)T V ≤

(cid:27)

, ∀ i ∈ S

,

1
4

where (cid:107) · (cid:107)T V is the total variation and P πg
(cid:80)

a∈A πg

i,apij(a).

(i, j) =
(cid:4)

The above assumption requires the multi-agent AMDP to be
sufﬁciently rapidly mixing, with the parameter t∗
mix character-
izing how fast the multi-agent AMDP reaches its stationary
distribution from any state under any acting policy [39]. In
particular, t∗
mix controls the distance between any stationary
policy and the optimal policy under the considered multi-agent
AMDP. It has been shown in [39] that under Assumption 1, an
optimal difference-of-value vector v∗ satisfying (cid:107)v(cid:107)∞ ≤ 2t∗
mix
exists.

Based on the above discussion, we can use the following

smaller constraint set V for the global primal variable v:

V =

v ∈ R|S| (cid:12)
(cid:110)
(cid:12) (cid:107)v(cid:107)∞ ≤ 2t∗
(cid:12)

mix

(cid:111)

.

6

Algorithm 1 Voting-Based MARL
1: Initialization: MARL tuple M = (S, A, P, {Rm}M

m=1),

time horizon T , parameters
(cid:115)

α = (4t∗

mix + M )

|S|
|A|

·

log(|S| · |A|)
2T

,

β =

4t∗
C = 4t∗

1
mix + M
mix + M.

(cid:114)

|S| · |A| · log(|S| · |A|)
2T

,

i,a = 1

|S|·|A| , ∀ i ∈ S, a ∈ A.

2: Set v = 0 ∈ R|S| and µm,0
3: Iteration:
4: for t = 1, 2, . . . , T do
5:

6:

7:
8:

9:
10:

11:

12:
13:

The system samples (it, at) with probability pdual
The system shifts to next state jt conditioned on (it, at)
and generates the local rewards (cid:8)rm
itjt
for m = 1, 2, . . . , M do

(at)(cid:9)M

it,at.

m=1

.

The agent m updates its local dual variable according
to (7).
end for
The system collects the updated µm,t+1
agents and samples (it, at) with probability pprimal
it,at
The system shifts to next state jt conditioned on (it, at)
and generates the local rewards (cid:8)rm
which
itjt
are returned to the agents.
for m = 1, 2, . . . , M do

(at)(cid:9)M

from local

m=1

i,a

.

The agent m updates its local primal variable accord-
ing to (11).

end for

14:
15: end for
16: Set ˆµg
i,a = 1
T
17: Return: ˆπg
i,a =

(cid:80)T

(cid:81)M
ˆµg
a∈A ˆµg

i,a

t=1

(cid:80)

i,a

m=1 µm,t

i,a , ∀ i ∈ S, a ∈ A.

, ∀ i ∈ S, a ∈ A.

Theorem 1 (Finite-Iteration Duality Gap): Let M =
(S, A, P, {Rm}M
m=1) be an arbitrary multi-agent AMDP tu-
ple satisfying Assumption 1. Then, the sequence of iterates
generated by Algorithm 1 satisﬁes

¯v∗ +

T
(cid:88)

E





(cid:88)

t=1

a∈A

1
T

(cid:32)

(I − Pa)v∗ −

(cid:33)(cid:62)



¯rm
a

µg,t
a



M
(cid:88)

m=1

(cid:18)

≤ ˜O

(cid:114)

(4t∗

mix + M )

|S| · |A|
T

(cid:19)
,

(cid:4)

where ˜O(·) hides polylogarithmic factors.
Recall from (5) that the complementarity gap of a pair of
optimal solutions to the primal-dual problems (3) and (4) is
zero. Hence, Theorem 1 suggests that the iterates {µg,t}t≥0
converge to an optimal solution to the dual problem (4) at a
sublinear rate. The result also covers the single-agent RL [39]
as a special case, which makes our model more general. We
defer the proof of Theorem 1 to the appendix.

Now, we are ready to establish the convergence of our pro-
posed Algorithm 1.

It is worth pointing out that in our proof, the scalar M
in Theorem 1, i.e., the number of agents, comes from the

(a) Convergence of duality gap

7

that they sum to one. The optimal policy is generated with
purposeful behavior by letting the agent favor a single action
in each state and assigning it with a higher expected reward
in [0, 1].

In Fig. 1, we show the empirical convergence results of
1) the duality gap, i.e., the one given in Theorem 1; 2) the
distance between the optimal policy and the learned policy,
i.e., (cid:107)π∗ − ˆπ(cid:107)1. The convergence curves are averaged over
100 instances. Generally,
the empirical convergence rates
corroborate the result given in Theorem 1. Besides, we also
present 1) the performance change as the number of local
agents varies from M = 5 to M = 100 and 2) the performance
of centralized learning, which directly uses the global primal-
dual updates to learn the global policy. The result shows that
the empirical convergence rates of the centralized case and
the distributed case are the same for different numbers of
agents M . This indicates that distributed decision making does
not slow down the process of achieving global consensus on
optimality.

(b) Convergence to the global optimal policy

B. Application in Multi-Agent IoT Systems

Fig. 1: Empirical convergence rate of the proposed algorithm.
Each generated MDP instance contains |S| = 50 states and
|A| = 10 actions at each state. The number of local agents
varies from M = 5 to M = 100 with the total reward of all
agents bounded in [0, 1].

bound of the total reward of all agents (cid:80)M
ij (a) ∈ [0, M ],
∀ i, j ∈ S, a ∈ A. As such, if we consider a normalized
reward where (cid:80)M
ij (a) ∈ [0, 1], then the complexity in
Theorem 1 will be independent of M .

m=1 rm

m=1 rm

VI. NUMERICAL RESULTS

In this section, we evaluate the proposed voting-based
MARL algorithm through two case studies. In the ﬁrst case
study, we verify the convergence of our proposed algorithm
with the generated MDP instances. In the second case study,
we exhibit how to apply our proposed algorithm to solve the
placement optimization task in a UAV-assisted IoT network,
where the ground base stations and the UAV-mounted base
station are treated as the IoT devices. The UAV-mounted base
station collects vote information from the ground base stations,
which is then used to determine the placement of the the UAV-
mounted base station to maximize the overall system capacity.
Our results show that the distributed decision making does
not slow down the process of achieving global consensus on
optimality and that voting-based learning is more efﬁcient than
letting agents behave individually and selﬁshly.

A. Empirical Convergence

We generate instances of the multi-agent MDP using a sim-
ilar setup as in [54]. Speciﬁcally, given a state and an action,
the multi-agent MDP shifts to the next state assigned from
the entire set without replacement. The transition probabilities
are generated randomly from [0, 1] and then normalized so

We now apply the proposed voting-based MARL algo-
rithm to a multi-agent IoT system which contains ground
base stations, smartphones, and UAVs. In particular, UAV-
assisted wireless communication has recently attracted much
attention [9], [55–57], due to that UAV mounted with a mobile
base station (UAV-BS) can provide high-speed air-to-ground
data access by using the line-of-sight (LoS) communication
links. However, obtaining the best performance in an UAV-
BS-assisted wireless system highly depends on the placement
of the UAV-BS [9], [55], [56]. Here, we consider optimizing
the placement of UAV-BS continuously through our proposed
voting-based MARL algorithm.

Existing works on the placement optimization of UAV-
BS have two major drawbacks. First, many of them do not
consider user movements [56], [58–61], but the change of
user distribution can largely inﬂuence the system performance.
Second, many of them determine the optimal placement of
the performance gain of each
UAV-BS by assuming that
ground BS is public information [9], [61], which may be
impractical in real-world wireless systems that have mixed
wireless operators, infrastructures, and protocols. To overcome
these drawbacks, we model the UAV-BS placement optimiza-
tion as a voting-based MARL problem, where multiple ground
BS learn to place the UAV-BS optimally with adaptation to
user movements and without the need to share their reward
information. The aim is to maximize the global performance
gain of all ground BS.

We consider the downlink of a wireless cellular network. As
shown in Fig. 2, the 2km×2km area of interest has M = 20
regularly deployed ground BS, one UAV-BS ﬂying at 200m
to provide air-to-ground communications, and 200 mobile
users moving according to the random walk model in [62],
each having a constant-bit-rate communication demand. The
UAV-BS can move to any one of the aerial locations from
a ﬁnite set |A| to provide air-to-ground communication. The
user mobility follows the random walk model in [62], where

0.00.61.21.82.43.0Iteration Steps (Million)0.00.20.40.60.81.0Duality GapGlobal Vote-MARLDistributed M=5Distributed M=10Distributed M=20Distributed M=1000.00.61.21.82.43.0Iteration Ste s (Million)0481216||π*−̂π||1Global Vote-MARLDistributed M=5Distributed M=10Distributed M=20Distributed M=1008

Fig. 2: 3D distribution of the investigated 4km2 area, which
contains M = 20 randomly deployed ground BS, one UAV-
BS ﬂying among |A| = 9 candidate aerial locations, and
200 mobile users moving by following the random walk
model [62]. The action is to move the UAV-BS to any one
of the |A| = 9 candidate aerial locations, which is determined
by the votes from the ground BS.

TABLE I: Parameters

Parameters

CBR (Cu)
Total 2D area
Total bandwidth
Carrier frequency (fc)
PRB bandwith (B)
Max user velocity (cmax)
Ground BS max transmit power (Pm)
UAV-BS max transmit power (PU )
Additional LoS path loss (ηLoS )
Noise power spectral density (N0)

Values

128 kbps
4 km2
20 MHz
2 GHz
180 kHz
10 m/s
46 dBm
20 dBm
1 dB
−174 dBm/Hz

each user moves at an angle uniformly distributed between
[0, 2π] and a random speed between [0, cmax] with cmax being
the maximum moving speed. Table I summarizes the main
parameters. The air-to-ground channel and ground-to-ground
channel are modeled according to [63] (Sec. II). The load of
each base station is deﬁned as the ratio between the required
number of PRBs and the total number of available PRBs
according to [4] (Sec. II-B).

The learning context is deﬁned as follows. 1) States: We
divide the area of interest into 3 × 3 grids and use the load of
each grid to characterize the wireless system status. The load
of each grid is indicated by one of two states: a) overloaded,
if the users’ demand within the grid is higher than the mean
demands of all the grids; b) underloaded, otherwise. Since the
grids cannot be all overloaded or all underloaded, there are
only |S| = 510 states for the wireless system with 9 grids.
2) Actions: The action set A is deﬁned as the available aerial
locations for the placement of the UAV-BS. At each time
t, the UAV-BS chooses an action at ∈ A for placement.
3) Rewards: The reward function is deﬁned with the aim to
maximize user throughput. Speciﬁcally, we assume that users
are always handed over to the BS with the best SINR, so that
an increased load at the UAV-BS usually indicates an increased
user throughput due to better user SINRs. Hence, we deﬁne
the reward to be the increased load at the UAV-BS.

Fig. 3: Rewards for the UAV-BS placement optimization.

We compare the proposed voting-based MARL algorithm
with four baselines: 1) the classic Q-learning algorithm [1],
which uses centralized Q-learning to learn the optimal UAV
placement policy; 2) the multi-agent actor-critic algorithm
based on the centralized learning with decentralized execu-
tion framework [18], where distributed agents optimize the
placement policy jointly by communicating with a central-
ized center; 3) the multi-agent Q-learning algorithm proposed
in [15], where each agent performs independent Q-learning
and treats the other agents as part of the environment; 4) the
optimal scheme, obtained by assuming that the underlying
MDP is known. We refer to them as centralized QL, multi-
agent AC, multi-agent QL, and optimal for short, respectively.
In addition, we adopt the majority voting rule proposed for
multi-agent Q-learning in [46] to determine the joint action
for both the multi-agent AC algorithm and the multi-agent
QL algorithm.

In Fig. 3, we present the averaged rewards over 20 runs.
The result shows that the performance of our proposed voting-
based MARL algorithm outperforms all the comparing algo-
rithms and is close to the optimal scheme. The discount factor
for discounted RL methods is set to be 0.9. The performance
gap between our proposed method and the centralized QL
indicates that undiscounted RL methods are likely to outper-
form discounted RL methods in continuing optimization tasks.
The performance gap between centralized QL and multi-agent
AC/QL indicates that existing MARL algorithms exhibit a
performance degradation compared with their centralized ver-
sions. In contrast, our proposed MARL algorithm achieves an
equivalent performance to its centralized version. In addition,
the performance of the multi-agent QL algorithm is the worst
and has a large variance. This veriﬁes that specifying a proper
collaboration protocol among the distributed agents is critical
in MARL in order to improve the learning performance.

We further compare our proposed voting-based scheme with
two baselines: 1) the random-voting scheme, where the MARL
system randomly chooses one agent to determine the global
action per iteration; 2) the greedy scheme, where the MARL
system aims at maximizing the cumulative reward of a single
agent. Fig. 4 presents the averaged reward of each agent
over 20 runs. The rewards of the greedy-maximizing scheme
indicate the maximum obtainable reward of each agent, while

X-axis (m)0500100015002000Y-axis (m)0500100015002000Altitude (m)0100200300UsersGround-BSUAV-BS (Candidate Positions)0.00.51.01.52.02.53.0Iteration Steps (Million)0.50.60.70.80.91.0RewardProposed MethodCentralized QLMulti-agent ACMulti-agent QLOptimal9

Fig. 4: Local reward of each agent.

the rewards of the random-voting scheme indicate the learning
effectiveness without the proposed voting mechanism. The per-
formance of our proposed voting-based scheme lies between
the two baselines, which indicates that the agents are learning
to compromise in order to maximize the cumulative global
reward.

VII. CONCLUSIONS

In this paper, we considered a collaborative MARL problem,
where the agents vote to make group decisions. Speciﬁcally,
the agents are coordinated to follow the proposed voting
mechanism without revealing their own rewards to each other.
We gave a saddle-point formulation of the concerned MARL
problem and proposed a primal-dual learning algorithm for
solving it. We showed that our proposed algorithm achieves
the same sublinear convergence rate as centralized learning.
Finally, we provided empirical results to demonstrate the
learning effectiveness. More interesting applications in the IoT
system and the voting mechanism in the context of competitive
MARL can be explored in the future.

APPENDIX
PROOF OF THEOREM 1

Our proof shares a similar spirit as that of Theorem 1
in [39]. However, the analysis in [39] does not readily extend
to the case of multi-agent AMDP. As a result, we have to
develop a separate new convergence analysis here.

By virtue of Lemma 1, it sufﬁces to study the progress made
by the sequences of global dual variables {µg,t}t≥0 and global
primal variables {vt}t≥0 in Algorithm 1. We begin with the
following lemma, which gives an estimate of the progress of
the dual variables in terms of KL-divergence.

Proof. By deﬁnition, we have

DKL(µg,∗(cid:107)µg,t+1) − DKL(µg,∗(cid:107)µg,t)

=

=

(cid:88)

(cid:88)

i∈S

a∈A

(cid:88)

(cid:88)

i∈S

a∈A

µg,∗
i,a log

µg,∗
i,a log

µg,∗
i,a
µg,t+1
i,a
µg,t
i,a
µg,t+1
i,a

(cid:88)

(cid:88)

−

µg,∗
i,a log

i∈S

a∈A

µg,∗
i,a
µg,t
i,a

.

According to (9), (13), and (14a), we have

log µg,t+1

i,a = log

(cid:80)

i∈S

where Z = (cid:80)

= log µg,t
(cid:80)
a∈A µg,t
DKL(µg,∗(cid:107)µg,t+1) − DKL(µg,∗(cid:107)µg,t)

i,a + ∆g,t
i,a exp{∆g,t

i∈S

i,a exp{∆g,t
µg,t
i,a}
(cid:80)
a∈A µg,t
i,a exp{∆g,t
i,a}
i,a − log(Z),

i,a}. It follows that

=

=

(cid:88)

(cid:88)

i∈S
(cid:88)

a∈A
(cid:88)

i∈S

a∈A

µg,∗
i,a log

µg,t
i,a
µg,t+1
i,a
(cid:0) log µg,t

µg,∗
i,a

i,a − log µg,t

i,a − ∆g,t

i,a + log(Z)(cid:1)

= log(Z) −

(cid:88)

(cid:88)

µg,∗
i,a ∆g,t
i,a.

i∈S

a∈A
Now, for any vt ∈ V, we have (cid:107)vt(cid:107)∞ ≤ 2t∗
have rm

ij (a) ∈ [0, 1] by assumption. Hence, we have

mix. Moreover, we

j − vt
vt

i +

M
(cid:88)

m=1

ij (a) ≤ 4t∗
rm

mix + M.

This, together with the fact that C = 4t∗
0, ∀ i ∈ S, a ∈ A, t = 0, 1, . . .. On the other hand,

mix+M , implies ∆g,t

i,a ≤

log(Z)

Lemma 3 (Dual Improvement in KL-Divergence): The iter-

= log

ates generated by Algorithm 1 will satisfy

(cid:32)

(cid:88)

(cid:88)

i∈S

a∈A

(cid:33)

i,a exp (cid:8)∆g,t
µg,t

i,a

(cid:9)

E (cid:2)DKL(µg,∗(cid:107)µg,t+1) | Ft
(µg,t
≤

i,a − µg,∗

(cid:88)

(cid:88)

i,a )E (cid:2)∆g,t

(cid:3) − DKL(µg,∗(cid:107)µg,t)
(cid:3)

i,a | Ft

i∈S
1
2

+

a∈A
(cid:88)

(cid:88)

i∈S

a∈A

µg,t
i,a

E(cid:2)(cid:0)∆g,t

i,a

(cid:1)2

| Ft

(cid:3),

for all t ≥ 0.

(16)

(cid:4)

(cid:88)

(cid:88)

≤ log

µg,t
i,a

a∈A

i∈S
(cid:32)

= log

1 +

(cid:18)

1 + ∆g,t

i,a +

(cid:1)2(cid:19)

1
2

(cid:0)∆g,t

i,a

(17a)

(cid:88)

(cid:88)

i∈S

a∈A

i,a∆g,t
µg,t

i,a +

1
2

(cid:88)

(cid:88)

i∈S

a∈A

(cid:33)

µg,t
i,a

(cid:0)∆g,t

i,a

(cid:1)2

(cid:88)

(cid:88)

≤

i,a∆g,t
µg,t

i,a +

i∈S

a∈A

1
2

(cid:88)

(cid:88)

i∈S

a∈A

µg,t
i,a

(cid:0)∆g,t

i,a

(cid:1)2

,

(17b)

0.00.51.01.52.00.050.070.090.110.13Agent-1MaximumRandom VotingProposed Voting00.51.01.52.00.050.070.090.110.13Agent-200.51.01.52.00.050.070.090.110.13Agent-300.51.01.52.00.050.070.090.110.13Agent-4Iteration Steps (million)Rewardwhere (17a) uses the fact that exp {x} ≤ 1 + x + 1
2 x2 for
x ≤ 0 and (17b) uses the fact that log(1 + x) ≤ x for
x > −1. Therefore, by combining the above results and taking
conditional expectation E [· | Ft] on both sides, we obtain (16),
(cid:4)
as desired.

Our strategy now is to bound the two terms on the right-

hand side of (16) separately.

Lemma 4: The iterates generated by Algorithm 1 satisfy

(cid:88)

(cid:88)

i∈S

a∈A

(µg,t

i,a − µg,∗

i,a )E (cid:2)∆g,t

i,a | Ft

(cid:3)

=

β
|S| · |A|

(cid:88)

a∈A

for all t ≥ 0.

(cid:0)µg,t

a − µg,∗
a

(cid:1)(cid:62)

(cid:32)

(Pa − I)vt +

(cid:33)

¯rm
a

M
(cid:88)

m=1

(cid:4)

Proof. For arbitrary i ∈ S and a ∈ A, we have
i,a )E (cid:2)∆g,t

i,a − µg,∗

i,a | Ft

(µg,t

(cid:88)

(cid:88)

(cid:3)

i∈S

a∈A

=

β
|S| · |A|

(cid:88)

(cid:88)

i∈S

a∈A

(cid:32)

(µg,t

i,a − µg,∗
i,a )

(Pa − I)vt +

−

Cβ
|S| · |A|

(cid:88)

(cid:88)

i∈S

a∈A

(µg,t

i,a − µg,∗
i,a )

M
(cid:88)

m=1

(cid:33)

¯rm
a

i

(18)

=

β
|S| · |A|

(cid:88)

a∈A

(µg,t

a − µg,∗

a )(cid:62)

(cid:32)

(Pa − I)vt +

(cid:33)

¯rm
a

,

M
(cid:88)

m=1

(19)

where (18) follows from Lemma 2 and (19) comes from the
fact that

(cid:88)

(cid:88)

(cid:88)

(cid:88)

µg,t
i,a =

µg,∗
i,a = 1.

(cid:4)

(cid:4)

This completes the proof.

Lemma 5: The iterates generated by Algorithm 1 satisfy

(cid:88)

(cid:88)

i∈S

a∈A

µg,t
i,a

E(cid:2)(cid:0)∆g,t

i,a

(cid:1)2

| Ft

(cid:3) ≤

4β2
|S| · |A|

(cid:0)4t∗

mix + M (cid:1)2

for all t ≥ 0.
Proof. Using (15), the assumptions that rm
vt ∈ V, and the deﬁnition of C, we compute

ij (a) ∈ [0, 1] and

E(cid:2)(cid:0)∆g,t

i,a

(cid:1)2

(cid:3)

| Ft

=

≤

=

β2
|S| · |A|

4β2
|S| · |A|

4β2
|S| · |A|

(cid:32)

pij(a)

j − vt
vt

i − C +

(cid:33)2

rm
ij (a)

M
(cid:88)

m=1

pij(a) (4t∗

mix + M )2

(cid:88)

j∈S

(cid:88)

j∈S

(4t∗

mix + M )2 .

Since (cid:80)

i∈S

(cid:80)

a∈A µg,t+1

i,a = 1, the result follows.

(cid:4)

Next, we give an estimate on the distance of the primal

iterate vt to the optimal primal variable v∗.

10

Lemma 6 (Distance to Primal Optimality): The iterates

generated by Algorithm 1 satisfy

E (cid:2)(cid:107)vt+1 − v∗(cid:107)2 | Ft

(cid:3)

≤ (cid:107)vt − v∗(cid:107)2 + 2α(vt − v∗)(cid:62)

for all t ≥ 0.

Proof. We compute

(cid:33)

(I − Pa)(cid:62)µg,t
a

+ 2α2

(cid:32)

(cid:88)

a∈A

(cid:4)

(cid:3)

E (cid:2)(cid:107)vt+1 − v∗(cid:107)2 | Ft
= E (cid:2)(cid:107)ΠV {vt + dt} − v∗(cid:107)2 | Ft
≤ E (cid:2)(cid:107)vt + dt − v∗(cid:107)2 | Ft
= (cid:107)vt − v∗(cid:107)2 + 2(vt − v∗)(cid:62)E (cid:2)dt | Ft

(cid:3)

(cid:3)

(cid:3) + E (cid:2)(cid:107)dt(cid:107)2 | Ft

(cid:3) ,

where the inequality follows from the fact that v∗ ∈ V and
the projector ΠV {·} is non-expansive. By Lemma 2, we have

E (cid:2)dt | Ft

(cid:3) = α

(cid:88)

a∈A

(I − Pa)(cid:62)µg,t
a .

Finally, using the deﬁnition of dt
E (cid:2)(cid:107)dt(cid:107)2 | Ft

(cid:3) = 2α2. This completes the proof.

in (12), we have
(cid:4)

We are now ready to establish the key recursion that will
lead to our desired bound on the convergence rate of our
proposed Algorithm 1.
Lemma 7: Deﬁne

1
2|S|(4t∗

mix + M )2 (cid:107)vt −v∗(cid:107)2,

(cid:32)

W t =

(cid:88)

a∈A

(µg,t

a )(cid:62)

(I − Pa)v∗ −

¯rm
a

+ ¯v∗.

(cid:33)

M
(cid:88)

m=1

The iterates generated by Algorithm 1 satisfy

E (cid:2)V t+1 | Ft

(cid:3) ≤ V t −

β
|S| · |A|

W t + 3β2 ·

(4t∗

mix + M )2
|S| · |A|

for all t ≥ 0.

(cid:4)

Proof. Using the results in Lemmas 3–6 and taking α =
1
|A| (4t∗

mix + M )2β, we compute

E (cid:2)V t+1 | Ft

(cid:3)

≤ V t +

(cid:18)

2 +

(cid:19)

1
|A|

β2 ·

(4t∗

mix + M )2
|S| · |A|
(cid:32)

(cid:0)µg,t

a − µg,∗
a

(cid:1)(cid:62)

(Pa − I)vt +

(cid:33)

¯rm
a

M
(cid:88)

m=1

β
|S| · |A|

(cid:88)

a∈A

+

+

β
|S| · |A|

(vt − v∗)(cid:62)

(cid:33)

(I − Pa)(cid:62)µg,t
a

.

(cid:32)

(cid:88)

a∈A

i∈S

a∈A

i∈S

a∈A

V t = DKL(µg,∗(cid:107)µg,t) +

Now, observe that

(cid:32)

(cid:0)µg,t

a − µg,∗
a

(cid:1)(cid:62)

(Pa − I)vt +

(cid:88)

a∈A

(cid:33)

¯rm
a

M
(cid:88)

m=1
(cid:33)

+ (vt − v∗)(cid:62)

(I − Pa)(cid:62)µg,t
a

(cid:32)

(cid:88)

a∈A

(cid:32)

(cid:88)

a∈A

(cid:32)

(cid:0)µg,t

a − µg,∗
a

(cid:1)(cid:62)

(Pa − I)vt +

M
(cid:88)

m=1

(cid:33)

¯rm
a

(cid:33)

+ (vt − v∗)(cid:62)

(I − Pa)(cid:62)(µg,t

a − µg,∗
a )

(20a)

(cid:32)

(cid:0)µg,t

a − µg,∗
a

(cid:1)(cid:62)

(Pa − I)v∗ +

(cid:32)

(µg,t

a )(cid:62)

(Pa − I)v∗ +

(cid:32)

(µg,t

a )(cid:62)

(Pa − I)v∗ +

M
(cid:88)

m=1

M
(cid:88)

m=1

M
(cid:88)

(cid:33)

¯rm
a

m=1
(cid:33)

− ¯v∗ (cid:88)

a∈A

¯rm
a

(cid:33)

¯rm
a

− ¯v∗,

(µg,∗

a )(cid:62)e

(20b)

(20c)

=

(cid:88)

a∈A

=

=

(cid:88)

a∈A

(cid:88)

a∈A

=

(cid:88)

a∈A

where (20a) and (20c) use the dual feasibility conditions
(cid:80)
i,a = 1 in

a )(cid:62)(I − Pa) = 0 and (cid:80)

a∈A(µg,∗

a∈A µg,∗

(cid:80)

i∈S

(4), respectively; (20b) uses the complementarity condition

(cid:32)

µg,∗
i,a

(Pa − I)v∗ +

M
(cid:88)

m=1

a − ¯v∗ · e
¯rm

(cid:33)

i

= 0, ∀ i ∈ S, a ∈ A

of the linear program (3). Combining the preceding relations,
(cid:4)
we obtain Lemma 7.

Proof of Theorem 1: We claim that

V 1 ≤ log(|S| · |A|) +

2(t∗
mix)2
mix + M )2 .

(4t∗

To see this, we note that µg,1 is the uniform distribution and
v0, v∗ ∈ V. Therefore, we have DKL(µg,∗(cid:107)µg,1) ≤ log(|S| ·
mix)2 for t = 0, 1, . . .. This yields
|A|) and (cid:107)vt−v∗(cid:107)2 ≤ 4|S|(t∗

V 1 ≤ DKL(µg,∗(cid:107)µg,1) +

mix + M )2 (cid:107)v1 − v∗(cid:107)2

≤ log(|S| · |A|) +

1
2|S|(4t∗
mix)2
2(t∗
mix + M )2 .

(4t∗

Now, we rearrange the terms in Lemma 7 and obtain

W t ≤

|S| · |A|
β

(V t − E (cid:2)V t+1 | Ft

(cid:3)) + 3β(4t∗

mix + M )2.

Summing over t = 1, . . . , T and taking the expectation, we

11

have

(cid:34) T

(cid:88)

E

(cid:35)

W t

t=1

|S| · |A|
β

T
(cid:88)

t=1

(E (cid:2)V t(cid:3) − E (cid:2)V t+1(cid:3)) + 3βT (4t∗

mix + M )2

≤

=

≤

|S| · |A|
β
|S| · |A|
β
+ 3βT (4t∗

(cid:0)E (cid:2)V 1(cid:3) − E (cid:2)V T +1(cid:3)(cid:1) + 3βT (4t∗
mix + M )2
(cid:19)
(cid:18)

log(|S| · |A|) +

mix + M )2.

2(t∗
mix)2
mix + M )2

(4t∗

By taking

(cid:114)

β =

1
mix + M

4t∗

|S| · |A| · log(|S| · |A|)
2T

,

we obtain
(cid:34)

E

1
T

(cid:35)

(cid:32)

W t

= ˜O

(4t∗

mix + M )

(cid:114)

(cid:33)

,

|S| · |A|
T

T
(cid:88)

t=1

as desired.

(cid:4)

REFERENCES

[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

Cambridge, MA, USA: MIT press, 2018.

[2] M. Chu, H. Li, X. Liao, and S. Cui, “Reinforcement learning-based
multiaccess control and battery prediction with energy harvesting in IoT
systems,” IEEE Internet Things J., vol. 6, no. 2, pp. 2009–2020, April
2019.

[3] N. Jiang, Y. Deng, A. Nallanathan, and J. A. Chambers, “Reinforcement
learning for real-time optimization in NB-IoT networks,” IEEE J. Sel.
Areas Commun., vol. 37, no. 6, pp. 1424–1440, June 2019.

[4] Y. Xu, W. Xu, Z. Wang, J. Lin, and S. Cui, “Load balancing for ultra-
dense networks: A deep reinforcement learning based approach,” IEEE
Internet Things J., vol. 6, no. 6, pp. 9399–9412, December 2019.

[5] H. Ye, G. Y. Li, and B. F. Juang, “Deep reinforcement

learning
based resource allocation for V2V communications,” IEEE Trans. Veh.
Technol., vol. 68, no. 4, pp. 3163–3173, April 2019.

[6] X. Zhang, M. Peng, S. Yan, and Y. Sun, “Deep reinforcement learning
based mode selection and resource allocation for cellular V2X commu-
nications,” IEEE Internet Things J., December 2019, to appear.

[7] Y. Liu, H. Yu, S. Xie, and Y. Zhang, “Deep reinforcement learning
for ofﬂoading and resource allocation in vehicle edge computing and
networks,” IEEE Trans. Veh. Technol., vol. 68, no. 11, pp. 11 158–11 168,
November 2019.

[8] C. H. Liu, Z. Chen, J. Tang, J. Xu, and C. Piao, “Energy-efﬁcient
UAV control for effective and fair communication coverage: A deep
reinforcement learning approach,” IEEE J. Sel. Areas Commun., vol. 36,
no. 9, pp. 2059–2070, Sep. 2018.

[9] R. Ghanavi, E. Kalantari, M. Sabbaghian, H. Yanikomeroglu, and
A. Yongacoglu, “Efﬁcient 3D aerial base station placement considering
users mobility by reinforcement learning,” in IEEE Wireless Communi-
cations and Networking Conference (WCNC), Barcelona, Spain, April
2018, pp. 1–6.

[10] N. Cheng, F. Lyu, W. Quan, C. Zhou, H. He, W. Shi, and X. Shen,
“Space/aerial-assisted computing ofﬂoading for IoT applications: A
learning-based approach,” IEEE J. Sel. Areas Commun., vol. 37, no. 5,
pp. 1117–1129, May 2019.

[11] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y. Liang, and
D. I. Kim, “Applications of deep reinforcement learning in communica-
tions and networking: A survey,” IEEE Commun. Surveys Tuts., vol. 21,
no. 4, pp. 3133–3174, Fourthquarter 2019.

[12] H. El-Sayed, S. Sankar, M. Prasad, D. Puthal, A. Gupta, M. Mohanty,
and C. Lin, “Edge of things: The big picture on the integration of edge,
IoT and the cloud in a distributed computing environment,” IEEE Access,
vol. 6, pp. 1706–1717, December 2018.

[13] W. Jiang, G. Feng, S. Qin, T. S. P. Yum, and G. Cao, “Multi-agent
reinforcement learning for efﬁcient content caching in mobile D2D
networks,” IEEE Trans. Wireless Commun., vol. 18, no. 3, pp. 1610–
1622, March 2019.

[14] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning for
dynamic power allocation in wireless networks,” IEEE J. Sel. Areas
Commun., vol. 37, no. 10, pp. 2239–2250, October 2019.

[15] J. Cui, Y. Liu, and A. Nallanathan, “Multi-agent reinforcement learning-
based resource allocation for UAV networks,” IEEE Trans. Wireless
Commun., vol. 19, no. 2, pp. 729–743, February 2020.

[16] J. Foerster, I. A. Assael, N. de Freitas, and S. Whiteson, “Learning
to communicate with deep multi-agent reinforcement learning,” in Ad-
vances in Neural Information Processing Systems (NeurIPS), Barcelona,
Spain, December 2016, pp. 2137–2145.

[17] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative multi-agent
control using deep reinforcement learning,” in International Conference
on Autonomous Agents and Multiagent Systems (AAAMS), So Paulo,
Brazil, May 2017, pp. 66–83.

[18] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-
agent actor-critic for mixed cooperative-competitive environments,” in
Advances in Neural Information Processing Systems (NeurIPS), Long
Beach, CA, USA, December 2017, pp. 6379–6390.

[19] S. Omidshaﬁei, J. Pazis, C. Amato, J. P. How, and J. Vian, “Deep
decentralized multi-task multi-agent reinforcement learning under par-
tial observability,” in International Conference on Machine Learning
(ICML), Sydney, NSW, Australia, August 2017, pp. 2681–2690.
[20] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. Torr, P. Kohli, and
S. Whiteson, “Stabilising experience replay for deep multi-agent rein-
forcement learning,” in International Conference on Machine Learning
(ICML), Sydney, Australia, August 2017, pp. 1146–1155.

[21] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar, “Fully decentralized
multi-agent reinforcement learning with networked agents,” in Interna-
tional Conference on Machine Learning (ICML), Stockholm, Sweden,
July 2018, pp. 9340–9371.

[22] S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed, “Distributed policy
evaluation under multiple behavior strategies,” IEEE Trans. Autom.
Control, vol. 60, no. 5, pp. 1260–1274, May 2015.

[23] H.-T. Wai, Z. Yang, Z. Wang, and M. Hong, “Multi-agent reinforcement
learning via double averaging primal-dual optimization,” in Advances
in Neural Information Processing Systems (NeurIPS), Montral, Canada,
December 2018, pp. 9649–9660.

[24] D. Lee, H. Yoon, and N. Hovakimyan, “Primal-dual algorithm for dis-
tributed reinforcement learning: distributed GTD,” in IEEE Conference
on Decision and Control (CDC), Miami Beach, USA, December 2018,
pp. 1967–1972.

[25] J. Yang, Y. Li, H. Chen, and J. Li, “Average reward reinforcement learn-
ing for semi-Markov decision processes,” in International Conference on
Neural Information Processing, Guangzhou, China, November 2017, pp.
768–777.

[26] S. Yang, Y. Gao, B. An, H. Wang, and X. Chen, “Efﬁcient average
reward reinforcement learning using constant shifting values,” in AAAI
Conference on Artiﬁcial Intelligence (AAAI), Phoenix, Arizona, July
2016, p. 22582264.

[27] M. Ghavamzadeh and S. Mahadevan, “Deep-reinforcement

learning
multiple access for heterogeneous wireless networks,” J. Mach. Learn.
Res., vol. 8, no. 1, p. 26292669, December 2007.

[28] A. Mathkar and V. S. Borkar, “Distributed reinforcement learning via
gossip,” IEEE Trans. Autom. Control, vol. 62, no. 3, pp. 1465–1470,
March 2017.

[29] L. Kraemer and B. Banerjee, “Multi-agent reinforcement learning as a
rehearsal for decentralized planning,” Neurocomputing, vol. 190, no. 1,
pp. 82–94, May 2016.

[30] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement
learning for multiagent systems: A review of challenges, solutions, and
applications,” IEEE Trans. Cybern., pp. 1–14, March 2020, to appear.
[31] L. Panait and S. Luke, “Cooperative multi-agent learning: The state of
the art,” Auton. Agent. Multi. Agent. Syst., vol. 11, no. 3, pp. 387–434,
November 2005.

[32] M. L. Littman, “Markov games as a framework for multi-agent rein-
forcement learning,” in International Conference on Machine Learning
(ICML), New Brunswick, NJ, USA, July 1994, pp. 157–163.

[33] ——, “Friend-or-foe Q-learning in general-sum games,” in International
Conference on Machine Learning (ICML), Williamstown, MA, USA,
July 2001, pp. 322–328.

[34] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean
ﬁeld multi-agent reinforcement learning,” in International Conference on

12

Machine Learning (ICML), Stockholm, Sweden, July 2018, pp. 5571–
5580.

[35] G. Arslan and S. Yksel, “Decentralized Q-learning for stochastic teams
and games,” IEEE Trans. Autom. Control, vol. 62, no. 4, pp. 1545–1558,
April 2017.

[36] K. G. Vamvoudakis, H. Modares, B. Kiumarsi, and F. L. Lewis, “Game
theory-based control system algorithms with real-time reinforcement
learning: How to solve multiplayer games online,” IEEE Control Syst.
Mag., vol. 37, no. 1, pp. 33–52, February 2017.

[37] A. Al-Tamimi, F. L. Lewis, and M. Abu-Khalaf, “Model-free Q-learning
designs for linear discrete-time zero-sum games with application to h-
inﬁnity control,” Automatica, vol. 43, no. 3, p. 473481, January 2007.
[38] J.-H. Kim and F. L. Lewis, “Model-free h-inﬁnity control design
for unknown linear discrete-time systems via Q-learning with LMI,”
Automatica, vol. 46, no. 8, p. 13201326, August 2010.

[39] M. Wang, “Primal-dual π learning: Sample complexity and sublinear run
time for ergodic Markov decision problems,” arXiv preprint:1710.06100,
October 2017. [Online]. Available: https://arxiv.org/abs/1710.06100
[40] Y. Chen, L. Li, and M. Wang, “Scalable bilinear π learning using state
and action features,” in International Conference on Machine Learning
(ICML), Stockholm, Sweden, July 2018, pp. 834–843.

[41] B. Dai, N. He, Y. Pan, B. Boots, and L. Song, “Learning from
conditional distributions via dual embeddings,” in Artiﬁcial Intelligence
and Statistics (AISTATS), Florida, USA, April 2017, pp. 1458–1467.

[42] S. S. Du, J. Chen, L. Li, L. Xiao, and D. Zhou, “Stochastic variance
reduction methods for policy evaluation,” in International Conference on
Machine Learning (ICML), Sydney, Australia, August 2017, pp. 1049–
1058.

[43] B. Aygun and A. M. Wyglinski, “A voting-based distributed cooperative
spectrum sensing strategy for connected vehicles,” IEEE Trans. Veh.
Technol., vol. 66, no. 6, pp. 5109–5121, June 2017.

[44] S. M. Nam and T. H. Cho, “Context-aware architecture for probabilistic
voting-based ﬁltering scheme in sensor networks,” IEEE Trans. Mobile
Comput., vol. 16, no. 10, pp. 2751–2763, October 2017.

[45] N. Katenka, E. Levina, and G. Michailidis, “Local vote decision fusion
for target detection in wireless sensor networks,” IEEE Trans. Signal
Process., vol. 56, no. 1, pp. 329–338, January 2008.

[46] I. Partalas, I. Feneris, and I. Vlahavas, “Multi-agent reinforcement
learning using strategies and voting,” in IEEE International Conference
on Tools with Artiﬁcial Intelligence (ICTAI), Patras, Greece, January
2007, pp. 318–324.

[47] T. G. Dietterich, M. A. Taleghan, and M. Crowley, “PAC optimal
planning for invasive species management: Improved exploration for
reinforcement learning from simulator-deﬁned MDPs,” in AAAI Confer-
ence on Artiﬁcial Intelligence (AAAI), Bellevue, Washington, July 2013.
[48] M. A. Taleghan, T. G. Dietterich, M. Crowley, K. Hall, and H. J.
Albers, “PAC optimal MDP planning with application to invasive species
management,” J. Mach. Learn. Res., vol. 16, no. 1, pp. 3877–3903,
January 2015.

[49] M. G. Azar, R. Munos, and H. J. Kappen, “Minimax PAC bounds on the
sample complexity of reinforcement learning with a generative model,”
Mach. Learn., vol. 91, no. 3, pp. 325–349, June 2013.

[50] M. Kearns, Y. Mansour, and A. Y. Ng, “A sparse sampling algorithm
for near-optimal planning in large Markov decision processes,” Mach.
Learn., vol. 49, no. 2-3, pp. 193–208, November 2002.

[51] M. J. Kearns and S. P. Singh, “Finite-sample convergence rates for Q-
learning and indirect algorithms,” in Advances in Neural Information
Processing Systems (NeurIPS), Denver, CO, December 1999, pp. 996–
1002.

[52] M. L. Puterman, Markov decision processes: discrete stochastic dynamic
programming. Hoboken, New Jersey: John Wiley & Sons, 2014.
[53] D. P. Bertsekas, Dynamic programming and optimal control. Belmont,

MA, USA: Athena scientiﬁc, 2005.

[54] A. Adam and M. White, “Investigating practical linear temporal differ-
ence learning,” in International Conference on Autonomous Agents and
Multiagent Systems (AAMAS), Singapore, May 2016, pp. 494–502.
[55] M. Mozaffari, W. Saad, M. Bennis, Y. Nam, and M. Debbah, “A tutorial
on UAVs for wireless networks: Applications, challenges, and open
problems,” IEEE Commun. Surveys Tuts., vol. 21, no. 3, pp. 2334–2360,
thirdquarter 2019.

[56] J. Lyu, Y. Zeng, R. Zhang, and T. J. Lim, “Placement optimization
of UAV-mounted mobile base stations,” IEEE Commun. Lett., vol. 21,
no. 3, pp. 604–607, March 2017.

[57] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong,
“Caching in the sky: Proactive deployment of cache-enabled unmanned
aerial vehicles for optimized quality-of-experience,” IEEE J. Sel. Areas
Commun., vol. 35, no. 5, pp. 1046–1061, May 2017.

13

[58] Y. Sun, T. Wang, and S. Wang, “Location optimization for unmanned
aerial vehicles assisted mobile networks,” in IEEE International Con-
ference on Communications (ICC), Kansas City, MO, USA, May 2018,
pp. 1–6.

[59] M. Alzenad, A. El-Keyi, F. Lagum, and H. Yanikomeroglu, “3-D
placement of an unmanned aerial vehicle base station (UAV-BS) for
energy-efﬁcient maximal coverage,” IEEE Commun. Lett., vol. 6, no. 4,
pp. 434–437, August 2017.

[60] R. I. Bor-Yaliniz, A. El-Keyi, and H. Yanikomeroglu, “Efﬁcient 3-D
placement of an aerial base station in next generation cellular networks,”
in IEEE International Conference on Communications (ICC), Kuala
Lumpur, Malaysia, May 2016, pp. 1–5.

[61] A. Merwaday and I. Guvenc, “UAV assisted heterogeneous networks
for public safety communications,” in IEEE Wireless Communications
and Networking Conference Workshops (WCNCW), New Orleans, LA,
USA, March 2015, pp. 329–334.

[62] J. Lyu, Y. Zeng, R. Zhang, and T. J. Lim, “A survey of mobility models
for ad hoc network research,” Wireless Communications and Mobile
Computing, vol. 2, no. 5, pp. 483–502, August 2002.

[63] U. Challita, W. Saad, and C. Bettstetter, “Interference management
for cellular-connected UAVs: A deep reinforcement learning approach,”
IEEE Trans. Wireless Commun., vol. 18, no. 4, pp. 2125–2140, March
2019.

