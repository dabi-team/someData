Feasibility Layer Aided Machine Learning 
Approach for Day-Ahead Operations 

Arun Venkatesh Ramesh, Student Member, IEEE and Xingpeng Li, Senior Member, IEEE 

1 

   Abstract—  Day-ahead  operation  involves  a  complex  and 
computationally  intensive  optimization  process  to  determine  the 
generator  commitment  schedule  and  dispatch.  The  optimization 
process is a mixed-integer linear program (MILP) also known as 
security-constrained  unit  commitment  (SCUC).  Independent 
system  operators  (ISOs)  run  SCUC  daily  and  require  state-of-
the-art  algorithms  to  speed  up  the  process.  Existing  patterns  in 
historical  information  can  be  leveraged  for  model  reduction  of 
SCUC, which can provide significant time savings. In this paper, 
machine  learning  (ML)  based  classification  approaches,  namely 
logistic  regression,  neural  networks,  random  forest  and  K-
nearest  neighbor,  were  studied  for  model  reduction  of  SCUC. 
The  ML  was  then  aided  with  a  feasibility  layer  (FL)  and  post-
process technique to ensure high quality solutions. The proposed 
approach  is  validated  on  several  test  systems  namely,  IEEE  24-
Bus system, IEEE-73 Bus system, IEEE 118-Bus system, 500-Bus 
system, and Polish 2383-Bus system. Moreover, model reduction 
of  a  stochastic  SCUC  (SSCUC)  was  demonstrated  utilizing  a 
modified  IEEE  24-Bus  system  with  renewable  generation. 
Simulation  results  demonstrate  a  high  training  accuracy  to 
identify commitment schedule while FL and post-process ensure 
ML  predictions  do  not  lead  to  infeasible  solutions  with  minimal 
loss in solution quality.     

Index  Terms—  Constraint  reduction,  Model  reduction, 
Variable  reduction,  Logistic  regression,  Neural  network, 
Random  forest,  K-nearest  neighbor,  Machine  learning,  Mixed-
integer 
unit 
commitment. 

Security-constrained 

programming, 

linear 

Sets: 
𝐺 
𝑔(𝑛) 
𝐾 
𝑁 
𝑇 
S 
W 
𝑤(𝑛) 
𝛿+(𝑛) 
𝛿−(𝑛) 
𝑀 
𝑀𝑡𝑒𝑠𝑡 
𝑀𝑡𝑟𝑎𝑖𝑛 

NOMENCLATURE 

Set of generators. 
Set of generators connecting bus n. 
Set of all transmission elements. 
Set of all buses. 
Set of Time intervals. 
Set of Scenarios. 
Set of renewable units. 
Set of generators connecting bus n. 
Set of lines with bus n as receiving bus. 
Set of lines with bus n as sending bus. 
Total samples. 
Test Samples. 
Training Samples. 

Parameters: 
𝑏𝑘 
𝑐𝑔 
𝑁𝐿 
𝑐𝑔

Susceptance of line k. 
Linear cost for generator g. 
No-load cost for generator g. 

Arun  Venkatesh  Ramesh  and  Xingpeng  Li  are  with  the  Department  of 
Electrical  and  Computer  Engineering,  University  of  Houston,  Houston,  TX, 
77204, USA. (e-mail: aramesh4@uh.edu; xingpeng.li@asu.edu).  

𝑆𝑈 
𝑐𝑔

𝑚  
𝑑𝑔,𝑡

𝑚𝑎𝑥 
𝑚𝑖𝑛 
𝑚𝑎𝑥 
10 
ℎ𝑟 
𝑆𝐷 
𝑆𝑈 

𝑃𝑔
𝑃𝑔
𝑃𝑘
𝑅𝑔
𝑅𝑔
𝑅𝑔
𝑅𝑔

𝑀𝐿  
𝑈𝑚,𝑔,𝑡

𝜋𝑠 
𝑃𝑤,𝑡,𝑠 

Variables: 
𝑃𝑔,𝑡,𝑠 
𝑃𝑘,𝑡,𝑠 
𝑟𝑔,𝑡,𝑠 
𝑚  
𝑢𝑔,𝑡
𝑣𝑔,𝑡 
𝜃𝑖,𝑡,𝑠 
𝜃𝑜,𝑡,𝑠 
𝑀𝐹  
𝑢𝑚,𝑔,𝑡

𝑀𝐹  
𝑣𝑚,𝑔,𝑡

𝑢𝑝 
𝑢𝑔,𝑡

𝑑𝑛 
𝑢𝑔,𝑡

Start-up cost for generator g. 
Predicted  demand  of  bus  n  in  time  period  t  for 
generated sample m. 
Maximum capacity of generator g. 
Minimum capacity of generator g. 
Long-term thermal line limit for line k. 
10-minute outage ramping limit of generator g. 
Regular hourly ramping limit of generator g.  
Shut-down ramping limit of generator g. 
Start-up ramping limit of generator g. 
Machine learning predicted commitment status for 
generator g in period t for sample m. 
Probability of renewable energy scenario s. 
Renewable output for unit w in time period t and 
scenario s. 

Output of generator g in time period t and scenario s. 
Lineflow of line k in time period t and scenario s. 
Reserve of generator g in time period t and scenario s. 
Commitment status of generator g in time period t for 
generated sample m. 
Start-up variable of generator g in time period t. 
Phase angle of bus i in time period t and scenario s. 
Phase angle of bus o in time period t and scenario s. 
Feasibility  layer  processed  commitment  status  for 
generator g in period t for sample m. 
Feasibility 
generator g in period t for sample m. 
turned  ON 
Commitment  status 
feasibility layer for generator g in period t. 
Commitment  status 
feasibility layer for generator g in period t. 

layer  processed 

from  OFF  by 

from  ON  by 

turned  OFF 

start-up 

status 

for 

I.  INTRODUCTION 

T 

he  short-term  power  system  operation  is  a  complex 
process  which  begins  with  day-ahead  markets  where 
generator schedules are identified for a least operational cost. 
Here, unit commitment is an optimization problem utilized to 
meet  the  supply  and  demand  for  tomorrow’s  need.  The  day-
ahead market is responsible to schedule and commit majority 
of  the  demand  requirement  making  it  a  vital  step  in  power 
system  operations.  Since  the  optimization  problem  involves 
the ON/OFF  status of generators,  it involves binary variables 
and  constraints  making  the  problem  a  mixed-integer  linear 
program  (MILP).  However, MILP  makes  the  problem  harder 
to  solve  typically  for  larger  systems.  Moreover,  there  are 
several security constraints and physical constraints to adhere 
with  to  ensure  reliable  and  low-cost  solutions.  Thus,  the 
resulting  MILP  is  a  security-constrained  unit  commitment 
(SCUC)  [1]-[5].  In  the  deregulated  regions  in  the  United 
States,  SCUC  is  solved  by  independent  system  operators 

 
 
 
 
 
(ISOs).  ISOs  have  strict  timelines  to  produce  results  for 
example, California ISO closes the input bids by 10:00 am and 
posts  the  schedules  by  01:00  pm  whereas  New  York  ISO 
collects the bids by 05:00 am and posts solution by 08:00 am. 
This  implies  that  the  day-ahead  market  is  cleared  and  the 
commitment  schedules  are provided  in 3  hours  [6]-[7].  Here, 
state-of-the-art  algorithms  are  required  to  provide  significant 
in  solution  quality. 
loss 
time  saving  benefits  without 
Therefore, 
several  heuristic  or  decomposition  based 
algorithms  were  proposed  to  obtain  the  solution  faster  [8]-
[10].  However,  techniques  involving  machine  learning  (ML) 
to  enhance  SCUC  were  seldom  studied.  In  comparison, 
learning  historical  information  can  be  beneficial  in  reducing 
the  complexity  of  the  SCUC.  Not  only  that,  learning-based 
methods  can  also  be  used  in  tandem  with  other  heuristic  or 
decomposition to obtain further improvements. 

An  important  factor  for  ML  methods  is  the  availability  of 
good  data  and  the  right  models  for  training  to  provide  high 
quality  outputs.  Since  the  SCUC  is  run  daily,  the  historical 
information can be leveraged to learn non-linear relationships 
between inputs and outputs. ML has been successfully utilized 
in  the  prediction  or decision  support  in  complex  problems  in 
various  power  system  fields  [11]-[15].  The  advantage  of ML 
is  that  once  the  model  is  trained  the  outputs  can  be  obtained 
instantaneously  for  similar  inputs.  Since  ML  uses  large 
amount  of  data  to  train,  it  can  be  robust  to  noisy  data. 
traditional 
Therefore,  combining  ML 
algorithms  such  as  SCUC  can 
the  overall 
performance [16]-[34].  

techniques  with 

improve 

The  SCUC  problem  consists  of  parameters  (known  fixed 
values),  variables  (continuous  and  binary)  and  constraints 
(equalities  and  inequalities).  The  SCUC  problem  can  have 
multiple  feasible  solutions  but  the  optimal  commitment  and 
dispatch  schedule  leads  to  the  lowest-cost  solution.  ML 
techniques  and  data-driven  approaches  have  been  utilized 
recently  in  aiding  or  replacing  the  SCUC  process.  However, 
most papers predominantly focus only on  replacing the MILP 
with  ML  [17]-[19]  or  screening  redundant  constraints  [20]-
[28].  In  particular,  replacing  SCUC  with  ML  techniques  can 
definitely  provide  the  most  time-saving  benefits  but  it  can 
never  guarantee  feasibility,  and/or  optimality.  An  infeasible 
solution  is  not  a  practical  solution  since  several  physical 
constraints can be breeched and [17]-[19] did not compare the 
solutions with the respective MILP solutions.  

The papers proposing screening of constraints mostly focus 
only  removing  redundant  transmission  constraints  in  SCUC. 
In  [20],  a  good  starting  solution  was  achieved  for  SCUC  by 
integrating  data-driven  approach  along  with  variable 
categorizing  to  improve  the  computational  performance  of 
SCUC.  In  [21],  historical  data  was  utilized  to  screen 
transmission constraints that  are non-binding in the SCUC to 
speed up the process. Similarly,  [22] uses an offline  ML tool 
to learn about outage schedules and identifies planned outages. 
In  [23],  the  authors  perform  a  feasibility  study  where  they 
mention  that  ML  techniques cannot  guarantee  optimality  and 
hence can only be used for warm-start application. The same 
authors in [24] then use ML techniques to identify line outages 
under  drastic  weather  conditions  for  stochastic  SCUC  to 
eliminate  congested  transmission  constraints.  In  [25],  the 
optimization is benefitted by replacing few active and inactive 

2 

constraints  line-flow  constraints  by  cost-based  inequality 
through ML. In  [26], a two-step offline and online process is 
implemented  where  the  offline  process  screens  security 
constraints  for  SCUC  whereas  this  further  screened  in  real-
time  in  the  security-constrained  economic  dispatch  (SCED). 
Similarly, [27] performs screening only for SCED which does 
not  bring  about  much  time-saving  benefits  whereas  [28] 
creates artificial colorful images to utilize convolutional neural 
networks (CNN) in SCED to study the network constraints. 

redundant  constraints  or 

Though  constraint  screening  relaxes  the  SCUC  algorithm 
when  aided  by  ML,  they  cannot  offer  a  greater  time-saving 
benefits  than  variable  reduction  by  learning  the  commitment 
schedules  as  seen  in  [29]-[34].  This  is  because  in  constraint 
screening the feasibility region of the SCUC solutions remain 
unaltered  and  only 
inactive 
constraints  are  eliminated.  [29]-[31]  tries  to  eliminate  all 
binary variables in SCUC and perform SCED. This may work 
for smaller systems or eliminating temporal constraints (single 
period application) or if the dataset is invariable which is not 
practical.  Hence,  this  does  not  guarantee  feasibility  of  the 
SCUC problem. Only [32]-[34] performs a reduced-SCUC (R-
SCUC) which were also tested on large practical systems and 
can  be  considered  as  the  state-of-the-art  methods.  A  few 
machine  learning  techniques  are  proposed  in  [33]  to  use 
historical information to improve the performance of SCUC to 
solve  identical  instances  in  the  future.  However,  [33]  uses 
support vector machine (SVM) and k-nearest neighbor (KNN) 
classification  algorithms  to  learn  commitment  solutions  for 
SCUC and yet are associated with drawbacks from infeasible 
problems.  [34]  utilizes  an  offline  ML  tool  to  categorize  load 
profile 
into  different  categories  with  a  pre-determined 
commitment  schedule  from  history.  However,  [34]  provides 
only  a  feasible  solution  and does  not  guarantee  optimality  or 
high solution quality. Also the proposed methods in [32]-[34] 
do  not  address  renewable  generation  and  only  works  on 
deterministic  models.  Renewable  energy  source  (RES)  is 
addressed in [35]-[36] albeit the proposed methods only learn 
the  varying  nature  of  renewables  to  identify  a  most  likely 
scenario. 

to 

Hence,  we  focus  on  building  a  supervised  ML  model  to 
predict  the  commitment  status  of  each  generator  𝑔  in  each 
time  interval 𝑡 (24-hours)  for day-ahead  operations  which  are 
further aided by post-processes to determine the confidence of 
the  solution.  The  commitment  status  of  one  implies  the 
generator is ON whereas zero represents the generator is OFF. 
Ideally, a classification model can be utilized when the targets 
two  classes,  also  known  as  binary 
only  belong 
classification.  This  paper  extends  on  the  preliminary  idea  in 
[32]  but  has  several  improvements  and  innovations  to 
supersede  the  prior  work.  Several  ML  classifications  were 
studied in this paper, namely, KNN, random forest (RF), fully 
connected neural networks (NN), logistic regression (LR), and 
a novel multi-target logistic regression (MTLR). Among these, 
the  LR,  MTLR  and  NN  algorithm  provided  the  most 
flexibility to post-process the ML outputs while also providing 
very high quality solutions. The proposed method in this paper 
focuses  on  innovative  partial  use  of  ML  solutions  with  post-
processing  ability  to  reduce  variables  while  ensuring  high 
solution  quality  along  with  benefits  of  significant  solve-time 

 
reductions  for  SCUC.  The  contributions  of  this  paper  are 
presented as follows:  

∑

𝑡+𝐷𝑇𝑔
𝑞=𝑡+1

•  An  NN  model  and  an  innovative  multi-target  logistic 
regression  (MTLR)  model  are  utilized  to  leverage 
historic  demand  profiles 
to  predict  generation 
commitment schedule as an offline step.  

•  Effective  post-processing  methods,  utilizing  the  ML 
in  SCUC  model 
the  variables 
output 
achieving  model-reduction,  are  addressed  while 
maintaining solution quality. 

to  reduce 

•  A feasibility layer (FL) is proposed to ensure feasibility 

of ML solution in online optimization step.   

•  A  bus-correlated  randomized  profile  generation  (BC-
RPG) method is used to obtain data to train ML models. 
•  The  proposed  FL-aided  R-SCUC (R-SCUC-FL) model 
can  work  on  stochastic,  deterministic,  or  decomposed 
SCUC models.   

The  rest  of  this  paper  is  organized  as  follows.  Section  II 
presents the SCUC model considered in this work. Section III 
describes  the  data  collection  procedure  while  Section  IV 
models  the  ML  architecture  and  the  proposed  methods  while 
the  results  are  discussed  in  Section  V.  Finally,  Section  VI 
concludes the paper.  

II.  SCUC FORMULATION  

The  operational  cost  of  generators,  which  includes  the 
production,  start-up  and  no-load  costs,  is  minimized  in  the 
SCUC  objective  (1)  subject  to  generation  and  power  flow 
physical  constraints.  The  generation  constraints  are  modelled 
in  (2)-(11).  Here,  (2)  and  (3)  represent  the  minimum  and 
maximum  generation  limits  respectively;  (4)  enforces  the 
emergency 10-min reserve ramp requirement; (5) ensures that 
reserves are held at the least to handle the failure of the largest 
system  generator;  (6)  and  (7)  models  the  hourly  generation 
ramp  capability.  Generator  start-up  indication  variable  is 
defined in (8)-(10) through minimum-up and minimum-down 
time  constraints.  The  generator  commitment  status  and  start-
up variables are binary  variables as shown in (11). The base-
case  physical  power  flow  constraint  is  represented  through 
(12)-(14).  (12)  depicts  the  power  flow  calculation;  (13) 
represents  the  long-term  thermal  limits  of  transmission 
elements;  and  (14)  enforces  nodal  power  balance.  Slack 
equation, (15), is added to define the reference phase angle in 
the base-case solution. 

Objective: 

𝑀𝑖𝑛  ∑ ∑ (𝑐𝑔

𝑡𝑔

𝑁𝐿𝑢𝑔,𝑡 + 𝑐𝑔

𝑆𝑈𝑣𝑔,𝑡 + ∑ (𝜋𝑠𝑐𝑔𝑃𝑔,𝑡,𝑠)
𝑠

)

(1) 

s.t.: 
Generation constraints: 

𝑃𝑔

𝑚𝑖𝑛𝑢𝑔,𝑡
𝑃𝑔,𝑡 + 𝑟𝑔,𝑡,𝑠 ≤ 𝑃𝑔
0 ≤ 𝑟𝑔,𝑡,𝑠 ≤ 𝑅𝑔

𝑚 ≤ 𝑃𝑔,𝑡,𝑠, ∀𝑔, 𝑡, 𝑠 
𝑚𝑎𝑥𝑢𝑔,𝑡
10𝑢𝑔,𝑡

𝑚 , ∀𝑔, 𝑡, 𝑠 

𝑚 , ∀𝑔, 𝑡, 𝑠  

∑

𝑟𝑞,𝑡,𝑠

≥ 𝑃𝑔,𝑡,𝑠 + 𝑟𝑔,𝑡,𝑠, ∀𝑔, 𝑡, 𝑠  
𝑞∈𝐺
𝑚 + 𝑅𝑔
𝑃𝑔,𝑡,𝑠 − 𝑃𝑔,𝑡−1,𝑠 ≤ 𝑅𝑔
𝑚 + 𝑅𝑔
𝑃𝑔,𝑡−1,𝑠 − 𝑃𝑔,𝑡,𝑠 ≤ 𝑅𝑔
𝑚 ), ∀𝑔, 𝑡, 𝑠  
𝑢𝑔,𝑡−1
≤ 𝑢𝑔,𝑡
𝑣𝑔,𝑞

ℎ𝑟𝑢𝑔,𝑡−1
ℎ𝑟𝑢𝑔,𝑡

𝑚 , ∀𝑔, 𝑡 ≥ 𝑈𝑇𝑔  

𝑡
∑
𝑞=𝑡−𝑈𝑇𝑔+1

𝑆𝑈𝑣𝑔,𝑡, ∀𝑔, 𝑡, 𝑠 
𝑚 +

𝑆𝐷(𝑣𝑔,𝑡 − 𝑢𝑔,𝑡

(2) 
(3) 
(4) 
(5) 
(6) 

(7) 

(8) 

3 

(9) 

(10) 
(11) 

(12) 
(13) 

(14) 

(15) 

≤ 1 − 𝑢𝑔,𝑡

𝑚 , ∀𝑔, 𝑡 ≤ 𝑇 − 𝐷𝑇𝑔  

𝑣𝑔,𝑞
𝑣𝑔,𝑡 ≥ 𝑢𝑔,𝑡
𝑣𝑔,𝑡, 𝑢𝑔,𝑡

𝑚 , ∀𝑔, 𝑡  

𝑚 − 𝑢𝑔,𝑡−1
𝑚 ∈ {0,1}, ∀𝑔, 𝑡  

Power flow constraints: 

𝑃𝑘,𝑡,𝑠 − 𝑏𝑘(𝜃𝑖,𝑡,𝑠 − 𝜃𝑜,𝑡,𝑠) = 0, ∀𝑘, 𝑡, 𝑠 

∑

𝑔∈𝑔(𝑛)

−𝑃𝑘
𝑃𝑔,𝑡,𝑠
𝑑𝑛,𝑡

𝑚𝑎𝑥 ≤  𝑃𝑘,𝑡,𝑠 ≤ 𝑃𝑘
𝑃𝑘,𝑡,𝑠
𝑘∈𝛿+(𝑛)
𝑃𝑤,𝑡,𝑠

+ ∑
𝑚 −   ∑

𝑤∈𝑤(𝑛)

𝑚𝑎𝑥, ∀𝑘, 𝑡, 𝑠  

− ∑
𝑘∈𝛿−(𝑛)
, ∀𝑛, 𝑡, 𝑠  

𝑃𝑘,𝑡,𝑠

=

𝜃𝑟𝑒𝑓,𝑡,𝑠 = 0 ∀𝑡, 𝑠 

A  SCUC  model  with  relevant  base  case  constraints  are 
formulated  to  highlight  the  proposed  ML  based  approach. 
Based  on  the  above  constraints,  the  deterministic  SCUC 
formulation is  represented by (1)-(15) with one scenario, 𝑠  ∈
{1},   𝜋𝑠 = 1.0  and  𝑃𝑤,𝑡,𝑠 = 0 ∀𝑔, 𝑡, .  The  stochastic  SCUC 
(SSCUC)  is  formulated  by  (1)-(15)  with  N  renewable 
scenarios, 𝑠  ∈ {1,2, … , 𝑁} and 𝜋𝑠 =

.   

1

𝑁

III.  DATA GENERATION 

ISO’s  run  the  SCUC  daily,  therefore,  data  related  to  daily 
load-profiles  and  respective  cleared  generator  commitment 
and dispatch schedules are stored. This data is assumed to be 
the  starting  point  for  this  work.  To  train  ML  models,  a  large 
amount of data is required. Hence, the SCUC model specified 
in  Section-II  is  utilized  to  generate  the  data.  By  varying  the 
input  nodal  load-profile,  we  can  generate  multiple  optimal 
commitment  and  dispatch  schedules  for  respective  profiles 
that can be collected as historical information.  

It can be noted that RES can also be modelled in this step if 
the system has wind/solar units. The load profile then becomes 
a net-load profile with multiple scenarios. RES are integrated 
to SCUC with a multi-scenario stochastic approach in SCUC 
as  seen  in  [4]-[5]  and  only  a  single  resultant  commitment 
schedule satisfies all the scenarios.   

For  the  test  systems  considered  in  this  study,  the historical 
information  is  generated  by  modifying  the  nodal  load  profile 
artificially  mimicking  uncertainty.  Since  the  test  systems 
considered  do  not  consist  of  the  same  information,  a  data 
creation step using the proposed BC-RPG method is required. 
To  begin,  a  common  load  profile  for  each  test  system  is 
considered with average seasonal peak information from [37]. 
If  seasonal  information  are  considered  then  average  seasonal 
load-profile  can  be  utilized  and  different  ML  model  can  be 
trained  and  stored  for  each  season  by  curating  the  data  into 
seasonal  buckets,  if  needed.  Once  the  standard  profile  is 
chosen,  multiple  profiles  can  be  generated  using  random 
variables as seen in (16) where the random variables, 𝛼𝑚 and 
𝑚 ,  shift  the  entire  system  load  profile  up/down  or  the 
𝛽𝑛,𝑡
composition  of  the  system  load  profile  can  be  altered, 
respectively.  Since  demand  profiles  only  change  marginally 
day-to-day,  the  value  for    𝛼𝑚  is  considered  to  be  ± 10% . 
Nodal values cannot be altered significantly as this would lose 
𝑚  is 
the  correlation  of  nodal 
considered  to  be  ± 4% .  The  combination  of  both  random 
variables provide varying load-profile curves. From Fig. 1, for 
example,  curve  1  represents  the  initial  load  profile  whereas 
curve 2 and curve 3 are generated only using only 𝛼𝑚, curve 4 

information.  Therefore  𝛽𝑛,𝑡

 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
is  generated  through  only 𝛽𝑛,𝑡
generated using the combination of both random variables.   

𝑚 ,  and  curve  5  and  curve  6  are 

 𝑆𝑦𝑠𝐷𝑡

𝑚 = ∑

𝑛∈𝑁

𝑚
𝑑𝑛,𝑡

= (∑

𝑛∈𝑁

(𝑑𝑛,𝑡 + 𝛽𝑛,𝑡

𝑚 𝑑𝑛,𝑡)) ∗ (1 +
𝛼𝑚) , ∀𝑚 ∈ 𝑀, 𝑡 ∈ 𝑇 

(16) 

𝑚 is the system demand in time period 𝑡 for sample 𝑚. 

Where,  
𝑆𝑦𝑠𝐷𝑡
𝛼𝑚 is a random variable (± 10%) for sample 𝑚. 
𝛽𝑛,𝑡
time period 𝑡. 

𝑚  is a random variable (± 4%) for sample 𝑚 for bus 𝑛 in 

100

90

80

70

60

50

40

)

%

(

e
l
i
f
o
r
P
d
n
a
m
e
D
m
e
t
s
y
S

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24

Curve 1
Curve 4

Time Period (hours)
Curve 2
Curve 5

Curve 3
Curve 6

Fig. 1. Sample demand profile curves. 

Only  feasible  samples  of  the  SCUC  are  considered  and  a 
total  of  1500  samples  denoted  as 𝑀 are  created  for  each  test 
system, which can be considered as an equivalent of 3-4 years 
of  data.  The  created 𝑀 samples,  once  shuffled,  are  split  into 
two datasets: 80% training samples (1200 samples) 𝑀𝑡𝑟𝑎𝑖𝑛 and 
20% testing samples (300 samples) 𝑀𝑡𝑒𝑠𝑡.   

IV.  PROPOSED OFFLINE APPROACH 

A.  ML Approach (Model Reduction) 

The ML step is utilized to reduce the number of variables in 
the SCUC model. The traditional approach is to utilize all the 
information such as constants, continuous and binary variables 
in  an  online  SCUC  model  as  shown  in  Fig.  2.  However,  we 
can  train  an  ML  algorithm  to  identify  variables  that  follow  a 
pattern,  especially  binary  variables  by  leveraging  historical 
information.  It  is  known  that  binary  variables  increase  the 
complexity in an MILP [17]-[18], [29]-[34].  

Fig. 2. SCUC Model Reduction. 

In  SCUC, 

the  generator 
the  binary  variables  are 
commitment  schedule.  The  constants  include  forecasted  load 
profiles,  generator  cost  and  ramping  information  whereas the 
continuous variables are generator dispatch, line flows and bus 
angles. By studying the historical commitment schedules with 
respective load profiles, the ML algorithm  can identify many 
generator states with certainty for any given load profile. The 

4 

generator  states  can  be  classified  as  either  (i)  flexible,  to  be 
determined  by  online  optimization  step,  or  (ii)  fixed,  as 
identified  by  offline  ML  algorithm.  Therefore,  the  fixed 
generators are now constants and the resultant R-SCUC online 
model only determines the states of flexible generators. 

It can be noted here that this approach of model reduction is 
agnostic  to  the  MILP  model.  This  implies  that  the  proposed 
approach of model reduction is unaltered and can be applied to 
deterministic,  stochastic  and/or  decomposition  or  heuristic 
techniques based SCUC models.  

B.  Classification Models from Scikit-learn 

In  this  paper,  we  initially  compare  the  performances  of 
several  classification  model,  namely  KNN,  RF,  NN  and  LR. 
All  the  models  used for  comparison  are  obtained  from  Scikit 
package, [38]. The neighbors-based classification is a type  of 
instance-based  learning  or  non-generalizing  learning.  This 
implies  a general internal model is not constructed  but  rather 
training  data  are  stored  as 
is 
computed from a simple majority vote of the nearest neighbors 
of  each  point.  For  KNN  classification,  the  optimal  choice  of 
the  value  K  is  highly  data-dependent:  in  general,  a  larger  K 
suppresses  the  effects  of  noise,  but  makes  the  classification 
boundaries  less  distinct.  KNN  works  by  identifying  the  most 
similar  examples  in  the  training  dataset  and  conducting  a 
simple majority vote [39]. 

instances.  Classification 

Another  supervised  learning method  used  for  classification 
is  the  class  of  non-parametric  decision  trees  where  a  target 
variable is predicted by the model by learning simple decision 
rules inferred from the data features.  Here, a tree can be seen 
as  a  piecewise  constant  approximation.  The  RF  model  is  a 
classification where an ensemble of many individual decision 
trees is used for prediction. Each individual tree in the random 
forest predicts a class output and the class with the most votes 
becomes the RF prediction [40].  

LR is a well-established classifier method which works in a 
one  vs  rest  classification  meaning  it  identifies  one  output 
based  on  the  set  of  training  inputs  [32].  LR  algorithm  using 
the solver liblinear which uses a coordinate descent algorithm 
and  only  supports  binary  classification.  The  package  is 
capable  of  handling  one  target  or  output  at  a  time  and  the 
regularization  is  applied  by  default  [41].  The  architecture  of 
LR is represented in Fig. 3. 

Fig. 3. LR architecture. 

Multi-layer  perceptron  (MLP)  is  a  supervised  learning 
algorithm that learns a mapping between inputs and outputs by 
training on a dataset. MLP is also known as NN where a non-
linear  function  approximator  for  either  classification  or 
regression  is  used  for  learning.  Mainly,  NN  differs  from  LR, 
in that between the input and the output layer, there can be one 
or more non-linear layers hidden layers. The outputs from the 
hidden  layer  are  processed  by  a  sigmoid  layer  to  provide 
probability  estimates  and  followed  by  a  classification  to 
represent  the  class.  It  uses  a  cross-entropy  loss  function  and 
trains via backpropagation. For classification, it minimizes the 

 
 
 
 
 
 
 
cross-entropy  loss  function,  providing  a  vector  of  probability 
estimates [42]. The architecture of NN is shown in Fig. 4. 

accuracy is calculated for both 𝑚 ∈ 𝑀𝑡𝑟𝑎𝑖𝑛 and 𝑚 ∈ 𝑀𝑡𝑒𝑠𝑡 using 
(23). Here, N represents the number of respective parameters. 

5 

𝐴𝑐𝑐 = 1 −

1
𝑁𝑚∗𝑁𝑔∗𝑁𝑡

𝑁𝑚
∑ (∑
𝑖=1

𝑔∈𝐺

𝑀𝐿 |
∑ |𝑢𝑖,𝑔,𝑡 − 𝑢𝑖,𝑔,𝑡

𝑡∈𝑇

)

(23) 

D.  Feasibility Layer 

Fig. 4. NN architecture. 

C.  Multi-Target Logistic Regression 

In  this  section,  the  training  model/algorithm  considered  is 
an MTLR model as denoted in Fig. 5. This model is similar to 
LR  as  it  is  a  regression  model  which  predicts  the  value  of 
probability  of  an  output  being  1  [43]-[44].  The  difference  is 
that  MTLR  uses  a  single  set  of  weights, 𝑤𝑗 ,  as  opposed  to 
multiple models with different weights, 𝑤𝑗,𝑚,  in LR.  

𝑀𝐿

Once the ML model provides the classification results, a FL 
is added to avoid any erroneous commitment schedules in ML 
,  by  making  minor  but  necessary  corrections 
outputs,  𝑈𝑚,𝑔,𝑡
𝑀𝐿 = 1, 
across time period 𝑡 ∈ 𝑇 as defined in (24). Here, if 𝑈𝑚,𝑔,𝑡
𝑀𝐿 = 0, 
𝐷𝑛 = 1. Similarly, if 𝑈𝑚,𝑔,𝑡
then it can be turned off with 𝑢𝑔,𝑡
𝑈𝑝 = 1 as shown in (25). The FL 
then it can be turned on with 𝑢𝑔,𝑡
ensures  that  minimum  up/down  time  limit  constraints  (8)-(9) 
are  not  violated  by  reforming  them  as  (26)-(27).  (28)  defines 
𝑀𝐹 .  Finally,  (29)  ensures 
the  respective  start-up  variable,  𝑣𝑔,𝑡,𝑚
that the flexible generator can either be turned on or turned off 
whereas  (30)  describes  the  variables  are  bound  by  binary 
integrality.  The FL is represented by (24)-(30) and  is solved 
in  the  online  phase.  Therefore,  it  is  performed  for  each 
generator  g  independently  per  sample  m  ∈ 𝑀𝑡𝑒𝑠𝑡 during  the 
verification  process.  Here,  it  can  be  noted  that  Always 
ON/OFF as determined by ML outputs, 𝑢𝑚,𝑔,𝑡
, in each sample 
m  can  be  excluded  as  they  are  already  feasible  for  minimum 
up/down constraints. The solve time for FL for each generator 
g  is  aggregated  and  added  to  the  respective  R-SCUC  solve 
time for each sample m.  

𝑀𝐿

Fig. 5. MTLR based SCUC model reduction algorithm. 

Objective: 

The hypothesis of LR model, (17), is a linear summation of 

normalized nodal demand and the parameters/weights, 𝑤𝑗. 

s.t.: 

ℎ𝑤(𝑥) = 𝑤0 + ∑

𝑗∈𝑁∗𝑇

𝑤𝑗𝑥𝑗

(17) 

The LR model uses a sigmoid activation layer, (18), which 
restricts  the  output  from  0-1 which  represents  the  probability 
of the output being 1, 𝑃(𝑦 = 1).  
1

1 −𝑒−𝑡  
Finally,  the  output,  𝑦̂ ,  is  obtained  after  the  hypothesis 

𝜎(𝑡) =

(18) 

𝑀𝑖𝑛  ∑ (𝑢𝑔,𝑡
𝑡

𝑈𝑝 + 𝑢𝑔,𝑡
𝐷𝑛)

∑

𝑈𝑝 − 𝑢𝑔,𝑡

𝐷𝑛 ∀𝑡 

𝑀𝐿 + 𝑢𝑔,𝑡
≤ 1 − 𝑢𝑚,𝑔,𝑡
𝑀𝐹
𝑣𝑔,𝑝

𝑀𝐹 = 𝑈𝑚,𝑔,𝑡
𝑢𝑚,𝑔,𝑡
𝑀𝐹
𝑣𝑔,𝑝

𝑡+𝐷𝑇𝑔
𝑝=𝑡+1
𝑡
∑
𝑝=𝑡−𝑈𝑇𝑔+1
𝑀𝐹 ≥ 𝑢𝑚,𝑔,𝑡
𝑣𝑚,𝑔,𝑡
𝑈𝑝 + 𝑢𝑔,𝑡
𝑢𝑔,𝑡
𝐷𝑛, 𝑣𝑚,𝑔,𝑡

𝑀𝐹 , 𝑢𝑚,𝑔,𝑡

𝑈𝑝, 𝑢𝑔,𝑡
𝑢𝑔,𝑡

𝑀𝐹  ∀𝑡 ≤ 𝑛𝑇 − 𝐷𝑇𝑔  
𝑀𝐹  ∀𝑡 ≥ 𝑈𝑇𝑔  
≤ 𝑢𝑚,𝑔,𝑡
𝑀𝐹 − 𝑢𝑚,𝑔,𝑡−1
𝑀𝐹
𝐷𝑛 ≤ 1 ∀𝑡 

 ∀𝑡 

𝑀𝐹 ∈ {0,1}, ∀𝑡 

(24) 

(25) 

(26) 

(27) 

(28) 

(29) 

(30) 

function followed by the sigmoid activation as seen in (19).  

E.  Post-Process Technique 

𝑦̂ = 𝜎(ℎ𝑤(𝑥)) = 𝑃(𝑦 = 1)  

(19) 

To  train  the  LR  model,  we  need  to  obtain  the  best 
parameters, 𝑤𝑗,  that  fit  the  input  and  output  features.  This  is 
implemented using the LR cost/loss function, (20). 

𝐽(ℎ𝑤(𝑥)) = −

1

𝑚
𝑖=1
(1 − 𝑦(𝑖)) 𝑙𝑜𝑔(1 − ℎ𝑤(𝑥(𝑖))) )]  

[∑ (𝑦(𝑖) 𝑙𝑜𝑔 ℎ𝑤(𝑥(𝑖)) +

𝑚

(20) 

To  obtain  the  weights,  we  minimize  the  LR  cost/loss 
function, (21), by using a gradient descent algorithm, (22), for 
several  iterations  until  the  cost/loss  values  saturates  for  all 
samples  in 𝑀𝑡𝑟𝑎𝑖𝑛.  Here, 𝛿 represents  the  learning  rate  of  the 
gradient  descent  algorithm.  The  number  of  iterations  and 
learning rate represents the hyper-parameters of the LR model. 
(21) 

Repeat {𝜔𝑖 ≔ 𝜔𝑖 − 𝛿 ∑ (ℎ𝑤(𝑥(𝑖)) − 𝑦(𝑖) )

(𝑖)} 
𝑥𝑗

(22) 

The  model  accuracy  can  be  verified  using  the  post-
processed  outputs.  Once  the  model  is  trained,  the  output 
probabilities are post-processed as 𝑃 ≥ 0.5 as 1 and 𝑃 < 0.5 as 0 
𝑀𝐿 .  The 
to  obtain  the  predicted  commitment  schedule,  𝑢𝑖,𝑔,𝑡

𝑚𝑖𝑛𝑤 𝐽(ℎ𝑤(𝑥))  
𝑚
𝑖=1

The LR, MTLR and NN models presented in Section IV-B 
and  Section  IV-C  are  extended  with  a  post-processing 
technique which includes the FL described in Section IV-D as 
seen in Fig. 6. Since the ML outputs of the above models are 
probabilities  of  generator  being  ON,  a  decision  boundary  of 
𝑃𝑡ℎ = 0.5 is used to classify ON and OFF status of generators. 
𝑀𝐿 ) ≥ 𝑃𝑡ℎ 
This  implies,  the  generator  status 𝑢𝑚,𝑔,𝑡
𝑀𝐿 ) < 𝑃𝑡ℎ .  Since  this  would  lead  to 
or  𝑢𝑚,𝑔,𝑡
inaccuracies along the decision boundary which in-turn lead to 
infeasible  solutions,  the  outputs  are  further  checked  for 
feasibility  using  the  FL,  discussed  in  Section  IV-D.  The 
following  steps  are  used  to  complete  the  post-process 
technique for each training sample m: 

𝑀𝐿 = 0  if 𝑃(𝑢𝑚,𝑔,𝑡

𝑀𝐿 = 1 if 𝑃(𝑢𝑚,𝑔,𝑡

•  Step  1:  Identify  always  ON/OFF  generators  using 
𝑀𝐿 ) ≥ 0.95) 
𝑀𝐿
. If a generator 𝑔 is always ON (𝑃(𝑢𝑚,𝑔,𝑡
𝑢𝑚,𝑔,𝑡
𝑚 = 1  for  all  periods  for  the 
in  each  𝑡 ∈ 𝑇  then  𝑓𝑖𝑥 𝑢𝑔,𝑡
corresponding generator. Similarly, if the generator 𝑔 is 
𝑚 = 0 
always OFF (𝑃(𝑢𝑚,𝑔,𝑡
for all periods for the corresponding generator. 

𝑀𝐿 ) ≤ 0.05) in 𝑡 ∈ 𝑇 then 𝑓𝑖𝑥 𝑢𝑔,𝑡

 
 
 
 
  
 
 
 
 
 
  
 
  
 
 
 
 
 
 
𝑀𝐿 ) ≥ 0.90  or  𝑃(𝑢𝑚,𝑔,𝑡

•  Step 2: for remaining generators after Step 1, run FL. If 
𝑀𝐹  
 =  𝑢𝑚,𝑔,𝑡
𝑃(𝑢𝑚,𝑔,𝑡
then  generator  g  in  time-period  t  has  a  fixed  state, 
𝑓𝑖𝑥 𝑢𝑔,𝑡

𝑀𝐿 ) ≤ 0.10  and  𝑢𝑚,𝑔,𝑡

𝑀𝐹 . 
𝑚 = 𝑢𝑚,𝑔,𝑡

𝑀𝐿

•  Step 3: If generator g in time-period t is identified as a 
𝑀𝐿 ≠

𝑀𝐿 ) < 0.9 or if  𝑢𝑚,𝑔,𝑡

flexible generator, i.e. 0.1 < (𝑢𝑚,𝑔,𝑡
𝑀𝐹  then warm-start 𝑢𝑔,𝑡
𝑢𝑚,𝑔,𝑡

𝑀𝐹 . 
𝑚  with 𝑢𝑚,𝑔,𝑡

For each sample 𝑚 ∈ 𝑀𝑡𝑒𝑠𝑡, the above steps are implemented 
and  the  respective  R-SCUC  is  solved  to  verify  the  quality  of 
the ML solution. The overall flow of the process is represented 
in Algorithm 1. 

Fig. 6. Post-process technique with FL. 

𝑚 , objective and time 

Algorithm 1 ML assisted R-SCUC-FL process 
1:  Repeat 
2: 
  randomize nodal demand 
3: 
  Solve SCUC 
𝑚 , 𝑢𝑔,𝑡
4: 
  Store 𝑑𝑛,𝑡
5:  until 𝑚 ∈ 𝑀  
6:  Shuffle 𝑀 samples 
7:  Split 𝑀 as 80% for 𝑀𝑡𝑟𝑎𝑖𝑛 and 20% for 𝑀𝑡𝑒𝑠𝑡  
8:  Train ML using 𝑀𝑡𝑟𝑎𝑖𝑛 for different hyper-parameters 
9:  Calculate train and test accuracy 
10:  Tuning: identify hyper-parameters with best accuracy  
11:  Save ML predicted output probabilities 
12:  Repeat 
𝑚  
13:    Perform step 1-step 3 and verify R-SCUC using 𝑢𝑔,𝑡
14:    record objective and time  
15:  until 𝑚 ∈ 𝑀𝑡𝑒𝑠𝑡     

V.  RESULTS AND ANALYSIS 

The mathematical model is  formulated in AMPL. The data 
creation  and  verification  steps  are  implemented  using  AMPL 
and solved using Gurobi solver. For  ML step is implemented 
in Python 3.6. A computer with Intel® Xeon(R) W-2295 CPU 
@  3.00GHz,  256  GB  of  RAM  and  NVIDIA  Quadro  RTX 
8000, 48GB GPU was utilized.  

TABLE I. TEST SYSTEM SUMMARY 

System 

IEEE 24-Bus [37] 
Modified IEEE 24-Bus [5] 
IEEE 73-Bus [37] 
IEEE 118-Bus [45] 
Synthetic grid (SG) [46] 
Polish [45] 

Gen cap 
(MW) 
3,393 
3,793 
10,215 
5,859 
12,189 
30,053 

# bus 

#gen 

# branch 

24 
24 
73 
118 
500 
2,383 

33 
35 
99 
54 
90 
327 

38 
38 
117 
186 
597 
2,895 

The following standard test systems summarized in Table I 
were  utilized  for  results  and  analysis.  It  can  be  noted  that,  a 
modified  IEEE  24-bus  system  was  also  introduced  with  2 

6 

additional  renewable  units  with  a  peak  capacity  of  200MW 
each.  Three  scenarios  with  varying  renewable  outputs  are 
considered in the modified IEEE 24-bus system. 

A.  Comparison with Scikit-learn Packages:  

There  are  several  classification 

techniques  currently 
available.  By  utilizing  scikit-learn  package,  we  were  able  to 
compare  some  common  classification  techniques,  namely, 
KNN, RF, LR, and a fully connected two layer neural network 
NN on the IEEE 24-Bus system data. The models were trained 
using 𝑀𝑡𝑟𝑎𝑖𝑛 and  tested  on 𝑀𝑡𝑒𝑠𝑡 .  The  accuracy  is  calculated 
using (23).  

TABLE II. CLASSIFICATION MODEL COMPARISON 

Classification 
model 
LR 
NN 
KNN 
RF 

Training 
accuracy 
97.95% 
97.48% 
97.48% 
97.31% 

Testing 
accuracy 
97.55% 
97.46% 
97.42% 
97.28% 

Infeasible test 
samples 
43.14% 
44.48% 
41.14% 
47.16% 

From  Table  II,  it  can  be  noticed  that  all  the  classification 
methods  fare  well  for  commitment  outputs.  LR  provides  the 
highest accuracy followed by NN, KNN, and RF, respectively. 
To  verify  which  model  results  in  identifying  more  accurate 
sequences,  we  implement  SCED.  From  Algorithm  1,  SCED 
𝑚 =
can  be  implemented  by  replacing  step1-step3  as  𝑓𝑖𝑥 𝑢𝑔,𝑡
𝑀𝐿
 ∀ 𝑚  ∈   𝑀𝑡𝑒𝑠𝑡, 𝑔  ∈ 𝐺, 𝑡 ∈ 𝑇.  By  performing  SCED  using 
𝑢𝑚,𝑔,𝑡
ML  solutions,  we  understand 
that  ML  models  cannot 
accurately identify the sequences and may either result in sub-
optimal  solutions  or  infeasible  solutions.  Therefore,  ML 
cannot  completely  replace  the  SCUC.  However,  we  realized 
that  the accuracy  alone  is not  the best  metric  since  KNN  has 
lower  accuracy  than  LR  and  NN  but  results  in  fewest 
infeasible samples in comparison. RF has the lowest accuracy 
and this is also seen in the number of infeasible cases.  

TABLE III. CONFUSION MATRIX COMPARISON 

Classification 
Model 
LR 
NN 
KNN 
RF 

True + 

50.47% 
50.29% 
50.77% 
50.23% 

True – 

False + 

False – 

47.01% 
47.16% 
46.65% 
47.04% 

1.25% 
1.17% 
1.67% 
1.28% 

1.20% 
1.38% 
0.91% 
1.44% 

On  studying  the  results  further,  Table  III  summarizes  the 
confusion  matrix  respective  to  the  result  in  Table  II.  A 
confusion  matrix  provides  an  idea  on  the  number  of  true 
predictions and false predictions in 𝑀𝑡𝑒𝑠𝑡. For any sample 𝑚  ∈
 𝑀𝑡𝑒𝑠𝑡,  if  the  predictions  are  accurate  and  entire  sequence  is 
identified implies the optimal solution is predicted. But if the 
number of false negatives increases, this implies that generator 
g in period t is identified as OFF instead of ON. This directly 
affects the number of feasible samples as the flexibility in the 
system  in  the  system  is  lost  and  constraints  are  violated, 
especially  (6)-(10).  Not  only  that,  as  the  number  of  false 
positives increase, this implies that the respective generator  g 
in  period  t  was  identified  as  ON  but  in  reality,  it  should  be 
the  solution  quality  as  sub-optimal 
OFF.  This  affects 
generators  or  generators  with  insufficient  capacity  maybe 
turned ON.  

However, it can also be noted that ML does provide a high 
number  of  accurate  predictions  in  each  sample.  Therefore, 

 
 
 
identifying  a  post-procedure  may  be  beneficial  to  selectively 
utilize  ML  solutions  that  are  known  in  high  confidence.  In 
order to leverage this, probability is a great metric.  However, 
KNN  and  random  forest  models  are  inherently  classification 
only  model.  This  implies  that  the  outcomes  belong  in one  of 
several classes as generator schedules are grouped together in 
unique schedule buckets. Hence, these models cannot provide 
a probability for individual generators being ON/OFF. Models 
such as LR are inherently probabilistic in nature as the outputs 
are  probability  before  the  decision  boundary  is  utilized  to 
classify the outputs. Similarly, NN can also prove probabilistic 
outputs when a sigmoid layer is utilized. This implies that LR 
and NN are more flexible in nature to partially utilize the ML 
solution for variable reduction. This means that the probability 
can be construed as a true trained outcome of the  ML model. 
Among  the  two  models,  LR  results  in  lower  false  negative 
predictions  which 
to  higher  accuracy  and  fewer 
infeasible  cases  and  hence  chosen  as  the  classification  model 
for further analysis.  

leads 

B.  Comparison between LR and MTLR  

Even  though  ML  training  is  an  offline  step,  while  training 
larger models LR required high amount of training time. LR is 
traditionally developed as one vs rest algorithm which implies 
that the existing packages for LR only performs for each target 
(generator  g  in  time  period  t)  separately  [43]-[44].  This  is 
computationally expensive since this requires training multiple 
sets of weights for each generator g in each time period t in the 
test systems. Hence, we proposed the MTLR described in sub-
section  IV.C.  In  comparison,  the  proposed  MTLR  provides 
outputs for all targets (generators) using single set of weights.  
For  validation  of  the  proposed  MTLR,  we  compared 
accuracy  using  LR  from  Scikit-learn  package  [38].  From 
Table  IV,  as  the  model  size  increases,  the  training  time 
significantly  increases  as  noted  in  the  polish  system  which 
requires 6,178 seconds (~1.7 hours) to train. But in MTLR we 
notice  that  a  minor  trade-off  in  accuracy  results  in  2.4x 
speedup  over  LR  on  average  across  all  test  systems.  This 
results  in  a  significant  computational  speed-up  during  offline 
training in larger test systems. Not only that, LR method from 
scikit-learn  only  works  if  the  training  set  contains  both 
ON/OFF  samples  for  each  generator  which  implies  that  LR 
can only be applied for generators showing a flexible trend. In 
practicality,  there  can  be  few  generators  such  as  base  plants 
and  hydro  plants  that  are  ON  irrespective  of  the  load  profile 
over the horizon and/or in all data samples. An assumption is 
required  for  such  generators  per  the  generic  trend.  In 
comparison,  the  proposed  MTLR  method  can  handle  this 
certainty in schedules for fixed generators since true label is a 
vector  of  schedules  of  all  generators  in  each  sample  and  the 
entire  schedule  can  be  unique  in  nature.  Hence,  for  these 
reasons, MTLR is used in subsequent results. 

TABLE IV. VALIDATION OF MTLR ON IEEE 24-BUS SYSTEM 

# Bus 

24 
73 
118 
500 
2383 

LR Test 
Accuracy 
97.55% 
95.39% 
95.98% 
98.87% 
98.18% 

LR Train 
Time (s) 
16.19 
374.2 
344.85 
743 
6,178.33 

MTLR Test 
Accuracy 
97.44% 
95.96% 
95.52% 
98.80% 
98.17% 

MTLR Train 
Time (s) 
8.18 
176.29 
143.21 
339.63 
2,445.28 

7 

C.  MTLR Hyper-parameter Tuning  

Each  test  system  is  trained  using  the  MTLR  model 
separately  by  utilizing  the  respective  generated  data, 𝑀𝑡𝑟𝑎𝑖𝑛. 
During  training,  the  samples  are  considered  as  a  single  full 
batch for 𝑚  ∈   𝑀𝑡𝑟𝑎𝑖𝑛. The best trained hyper-parameters with 
highest accuracy will be utilized for further tests. For each test 
system,  the  hyper-parameter  learning  rate  (𝛿)  is  varied  from 
0.001-0.05.  For  each 𝛿,  systems  were  trained  until  the  cost 
saturates and then the accuracy was then calculated using (23).  
During training, the cost vs iterations or epoch is registered 
to  plot  learning  rate  (𝛿) graph.  The 𝛿  graph  represents  the 
iteration  which  provides 
the 
loss/cost  with  respect 
information about the training when different hyper-parameter 
is utilized. Here, Fig. 7, represents the learning curve for IEEE 
24-Bus  system  when  trained  for  1000  iterations  to  show  the 
saturation  of  the  cost.  For 𝛿 ≥ 0.03,  the  training  cost  never 
saturates  which  implies  that  the  step  is  too  large.  For  𝛿 ≤
0.001,  it  is  slower  to  converge  in  training  which  implies  the 
step  is  too  small.  The  accuracy  is  also  calculated  for  each 𝛿 
using  (23).  Between 0.003 ≤ 𝛿 ≤ 0.01,  the 𝛿 = 0.01 is  chosen 
for  the  IEEE  24-Bus  system  which  provides  the  highest 
training  accuracy  and  a  strictly  decreasing  curve  for  learning 
rate. 

to 

In  comparison,  the  scikit-learn  models  are  trained  using 
standard  parameters  provided  by  the  package  which  includes 
an adaptive learning rate and early stopping functionality.  

Fig. 7. Learning rate (𝛿) curves for IEEE 24-Bus system (0.001 ≤ 𝛿 ≤ 0.05). 

D.  Training Summary (Offline) 

Each  system  is  trained  using  both  MTLR  and  NN  model 
using  the  respective  system  data,  𝑀𝑡𝑟𝑎𝑖𝑛 ,  as  described  in 
section  III.  For  all  test  systems,  as  described  in  Section  III, 
1500 samples were created, shuffled and split 80% for training 
and 20% for testing. The training is an offline step performed 
once for each system.  

TABLE V. MTLR MODEL TRAINING SUMMARY 

# Bus 

24 
73 
118 
500 
2383 

MTLR Accuracy 
Test 
97.44% 
95.96% 
95.52% 
98.80% 
98.17% 

Train 
97.50% 
95.97% 
97.57% 
98.81% 
98.34% 

NN Accuracy 

Train 
97.48% 
95.37% 
97.83% 
99.06% 
98.11% 

Test 
97.46% 
95.30% 
97.62% 
99.04% 
97.98% 

During training, the samples are considered as a single full 
batch.  For  MTLR,  the  hyper-parameter  learning  rate  (𝛿 )  is 
identified and trained as per sub-section V.D. The training and 

 
 
testing  accuracy  was  then  calculated  using  (23).  Table  V 
summarizes 𝛿, accuracy and training time for each test system. 
The MTLR and NN model provides high training and testing 
accuracy  >95%  for  all  the  test  systems  considered  in  this 
𝑀𝐿 and 
work. Once the model is trained then the predictions, 𝑢𝑚,𝑔,𝑡
𝑀𝐿 )  for  each  test  samples  𝑚  ∈   𝑀𝑡𝑒𝑠𝑡  are  obtained  and 
𝑃(𝑢𝑚,𝑔,𝑡
stored for all test systems. 

E.  Verification Results (Online) & Feasibility Layer Benefits 

SCUC-FL result in better cost compared to SCUC. Similarly, 
the  IEEE  73-bus  system,  NN  R-SCUC  and  NN  R-SCUC-FL 
result in lower cost. This is because model reduction on top of 
time-saving  can  also  tighten  the  MIPGAP  to  a  high  degree 
resulting  in  a  better  MIPGAP  solution  in  some  test  systems 
when compared to SCUC. However, on average across all test 
system,  the  solution  quality  is  maintained  to  high  degree  of 
<0.1  %  deviation  for  MTLR  R-SCUC-FL  and  NN  R-SCUC-
FL.   

8 

𝑀𝐿

 and  𝑃(𝑢𝑚,𝑔,𝑡

𝑀𝐿 )  is  verified 

In order to successfully assist SCUC, we  developed the FL 
and  the  post-processing  technique  mentioned  in  sub-section 
IV.D  and  IV.E,  respectively.  The  MTLR  and  NN  based  test 
predictions/outputs,  𝑢𝑚,𝑔,𝑡
for 
𝑀𝐹  and then post-processed. To 
feasibility with FL to obtain 𝑢𝑚,𝑔,𝑡
address model-reduction, benefits verification is performed for 
all test samples. The verification is an optimization step based 
on  the  ML  outputs  and  therefore  is  an  online  step.  Since  the 
FL is also an optimization step,  the solve time is inclusive of 
both post-processing and the MILP solve time. The R-SCUC-
FL  is implemented as per algorithm I in sub-section IV.E. In 
order to compare the benefits of FL, the R-SCUC (i.e without 
FL), is also performed. R-SCUC is implemented by replacing 
step 2 and step 3 in Algorithm I by: 

•  Step II: for remaining generators after Step 1, 𝑓𝑖𝑥 𝑢𝑔,𝑡

𝑚 = 0 if 𝑃(𝑢𝑚,𝑔,𝑡

𝑚 =
𝑀𝐿 ) ≤ 10% and 

1 if 𝑃(𝑢𝑚,𝑔,𝑡
warm-start 𝑢𝑔,𝑡

𝑀𝐿 ) ≥ 90%, 𝑓𝑖𝑥 𝑢𝑔,𝑡
𝑚 = 𝑢𝑚,𝑔,𝑡

𝑀𝐿

 if 10% < 𝑃(𝑢𝑚,𝑔,𝑡

𝑀𝐿 ) < 90%.   

Table  VI represents  the  infeasible  problems  corrected  with 
R-SCUC-FL  by  using  MTLR  and  NN  based  ML  outputs 
respectively. The infeasible problems arise in R-SCUC. Based 
on  our  study,  we  noted  that  R-SCUC  resulted  in  infeasible 
in  many  samples  since  ML  mainly  cannot 
problems 
distinguish  minimum  up/down 
time  physical  constraint 
requirement for generators (8)-(9). It only requires incorrectly 
identifying  one  generator  g  in  time  period  t  to  result  in  an 
infeasible solution for R-SCUC. For example, in IEEE 24-bus 
system,  there  are  33  generators  and  24  time  periods,  which 
implies  a  total  of  792  predictions  per  sample  m  to  identify 
commitment  schedule.  However,  we  notice  that  the  FL 
eliminates all infeasible samples in all test systems.  Here, NN 
R-SCUC is more susceptible to infeasible samples in R-SCUC 
in comparison to MTLR R-SCUC. But, irrespective of the ML 
model,  the  FL  ensures  that  the  ML  outputs  adhere  to  MILP 
requirements  particularly  the  generator  minimum  on/off  time 
limit constraints. 

TABLE VI. FL INFEASIBLE PROBLEMS ELIMINATION  

System 

NN 
MTLR 

IEEE 
24-Bus 
28 
4 

IEEE 
73-Bus 
18 
6 

IEEE 
118-Bus 
4 
0 

SG 500-
Bus 
32 
8 

Polish 
2383-Bus 
6 
4 

Fig.  8  represents  the  solution  quality  whereas  Fig.  9 
represents  the  solve  time  averaged  over  all  test  samples  for 
each  test system utilizing the MTLR and NN based R-SCUC 
with  and  without  FL.  The  objective  cost  and  solve  time  for 
reduced  models  are  represented  as  base-normalized  (BN) 
values  averaged  over  all  test  samples.  The  base  model  is 
normal SCUC that does not use any ML outputs. From Fig. 8, 
it can also be noted that the solution quality is maintained to a 
high degree for R-SCUC-FL without infeasibilities. In the case 
of IEEE 24-bus system, both MTLR  R-SCUC and  MTLR R-

)

%

(

t
s
o
c

d
e
z
i
l
a
m
r
o
n
-
e
s
a
B

100.6

100.4

100.2

100.09

100.08

100.01

100.48

100.46

100.26

100.41

100.15
100.15

100.1

99.84

99.86

100

99.8

99.6

100.098

99.93

99.9

99.69

100.04

100.02

100.02

100.01

IEEE 24-bus

IEEE 73-bus IEEE 118-bus SG 500-bus Polish 2383-

Test Systems

bus

MTLR R-SCUC

NN R-SCUC

MTLR R-SCUC-FL

NN R-SCUC-FL

Fig. 8. R-SCUC and R-SCUC-FL solution quality. 

The  BN  solve  time  shows  that  R-SCUC-FL  requires  a 
minor  additional  time  for  ML  prediction  post-processing  as 
two MILP models are solved when compared with R-SCUC to 
ensure solution quality and eliminating infeasibility. MTLR R-
SCUC-FL results in a speed-up factor of 1.7x, 3.3x, 2.1x, 2.3x 
and  8.5x,  whereas  NN  R-SCUC-FL  results  in  a  speed-up 
factor of 1.6x, 3.7x, 1.9x, 2.8x and 6.9x for the IEEE 24-bus, 
IEEE 73-bus, IEEE 118-bus, SG 500-bus and Polish systems, 
respectively  on  average  over  all  testing  samples, 𝑚  ∈   𝑀𝑡𝑒𝑠𝑡, 
when  compared  with  SCUC.  When  averaged  across  all  test 
systems,  MTLR  R-SCUC-FL  results  in  speed-up  factor  of 
3.6x whereas NN R-SCUC-FL results in a speed-up factor of 
3.4x while also ensuring feasibility of all test samples. 

62.99

61.64

59.08

55.01

80

70

60

50

40

30

20

10

0

)

%

(

e
m

i
t

e
v
l
o
s

d
e
z
i
l
a
m
r
o
n
-
e
s
a
B

51.72

47.65

47.31

30.05

43.98

39.78

47.68

21.25

36.28

25.96

26.86

16.64

14.57

13.49

11.82

11.47

Polish 2383-
bus

IEEE 24-bus

IEEE 73-bus IEEE 118-bus SG 500-bus

Test Systems

MTLR R-SCUC

NN R-SCUC

MTLR R-SCUC-FL

NN R-SCUC-FL

Fig. 9. R-SCUC and R-SCUC-FL solve time comparison. 

F.  Out of Sample Testing 

To understand the robustness of the proposed FL, an out-of-
sample  testing  was  further  performed.  The  out-of-sample  set 

 
 
 
 
 
 
 
 
consists of 100 samples that were not included in the training 
or testing samples of the verification process.  Here, care was 
taken  to  introduce  higher  variability  with  𝛼𝑚 =   ±25%  and 
𝑚 =   ±10% in (16) in order to avoid mimicking the original 
𝛽𝑛,𝑡
dataset and increase number of infeasible samples. 

TABLE VII. INFEASIBLE PROBLEMS IN OUT-OF-SAMPLE DATA 

Test System 

IEEE 24-Bus 
IEEE 73-Bus 
IEEE 118-Bus 
SG 500-Bus 
Polish 2383-Bus 

MTLR 
R-SCUC 
40 
95 
63 
82 
37 

MTLR R-
SCUC-FL  
14 (65% ↓) 
54 (41% ↓) 
27 (57% ↓) 
36 (56% ↓) 
9 ( 75% ↓) 

NN R-
SCUC 
59 
100 
78 
100 
45 

NN R-
SCUC-FL 
28 (53% ↓) 
74 (26% ↓) 
18 (77% ↓) 
67 (33% ↓) 
16 ( 64% ↓) 

These  samples  were  never  utilized  in  the  offline  training 
phase  or  online  verification  phase.  Therefore,  the  trained 
model  might  not  fare  as  well  in  the  out-of-sample  dataset 
(with  much  larger  variations)  when  compared  to  the  original 
dataset.  Despite  this,  from  Table  VII,  we  notice  a  significant 
reduction in infeasible problems when the FL was introduced 
in R-SCUC in all test systems. This  resulted in  reductions of 
infeasible samples by 58.8% and 50.6% when averaged across 
all test systems for MTLR and NN models, respectively.  

G.  Case Study: Multi-Scenario Renewable Source  

As stated in the prior section, the proposed MTLR methods 
are agnostic to the MILP model. Hence, it can be utilized for 
both  stochastic-SCUC  (SSCUC)  and  deterministic  SCUC 
cases.  In  a  deterministic  scenario,  renewable  profile  can  be 
captured  through  net-load  profile.  However,  the  renewable 
energy  is  unpredictable  in  nature,  the  scenarios  of  wind  and 
solar outputs are often considered. But it can be noted that in 
SSCUC,  a  single  commitment  schedule  that  satisfies  all  the 
scenarios  are  obtained  as  outputs.  In  terms  of  ML,  this  only 
increases  the  number  of  inputs  but  the  targets/outputs  remain 
the same. Therefore, the MTLR and NN models are modified 
to increase S scenarios of net nodal load as input. 

TABLE VIII. MODIFIED IEEE 24-BUS RENEWABLE SYSTEM RESULTS  

Metrics 
Training Accuracy 
Testing Accuracy 
Infeasible samples 
BN cost 
BN Time 

MTLR R-SSCUC FL  NN R-SSCUC FL 

96.57% 
94.53% 
0 
100.07% 
43.57% 

97.01% 
96.25% 
0 
100.01% 
36.78% 

The  proposed  MTLR  R-SCUC  FL  and  NN  R-SCUC  FL 
were tested on the modified IEEE 24-Bus renewable test case 
with  two  renewable  units.  Table  VIII  shows  the  online 
verification results. It can  be  noted that, the proposed MTLR 
and NN models can successfully handle stochastic inputs with 
solution  accuracy  of  94.53%  and  96.25%  for  test  samples. 
This is marginally lower than the deterministic case. However, 
utilizing  the  MTLR  and  NN  solutions,  we  notice  that  the 
Reduced-SSCUC-FL  (R-SSCUC)  results 
time 
savings  of  56.43%  and  63.22%  with  respect  to  SSCUC.  In 
comparison, the deterministic MTLR and NN based R-SCUC-
FL  only results in a time saving of  40.92% and 38.36% with 
respect to SCUC. This is because, reducing equivalent number 
of  variables  benefits  R-SSCUC  more  since  this  directly 
relaxes higher number of constraints when compared with  R-
SCUC. 

in  higher 

9 

VI.  CONCLUSIONS 

In  this  paper,  we  studied  the  differences  between  different 
classification techniques as an offline step namely, KNN, RF, 
LR  and  NN  for  predicting  commitment  schedules  given  the 
load profile. It was concluded that ML cannot directly replace 
optimization 
through  SCUC.  However,  by  choosing  a 
confidence level through probabilistic outputs, selective binary 
variables  were  reduced  in  SCUC.  LR  and  NN  were  more 
flexible due  to the ability to result in probability estimates of 
commitment  status  of  generators.  Not  only  that,  by  studying 
the confusion matrix for ML predictions, both LR and NN led 
to  higher  accuracy  and  resulted  in  better  predictions  when 
compared  to  KNN  and  RF.  Furthermore,  LR  was  also 
addressed  for  computation  efficiency  through  a  novel  MTLR 
model. On average, the MTLR model was 2.4x faster than LR 
during offline training.  

The  trained  models  were  then  introduced  for  online 
verification  on  test  samples  through  post-processing  ML 
solutions with FL. A confidence based variable selection and 
FL  in  combination  produced  desired  effects  of  eliminating 
infeasible  outputs  while  also  maintaining  high  degree  of 
solution-quality.  On  average  across  all  test  systems,  model 
reductions with the proposed MTLR R-SCUC FL and NN R-
SCUC-FL resulted in a speed-up 3.6x and 3.4x, respectively, 
when compared with SCUC. 

On top of this, it was established that the proposed approach 
is  agnostic  of  MILP  models.  Therefore,  the  ML  model  was 
also  tested  on  a  modified  IEEE  24-bus  system  with  three 
renewable scenarios. The ML outputs were then similarly used 
for  variable  reduction  in  SSCUC.  Online  verification  of 
MTLR and NN based R-SSCUC-FL resulted in a speed-up of 
2.3x  and  2.7x,  respectively,  when  compared  to  SSCUC.  In 
comparison,  the  deterministic  R-SCUC-FL  resulted  in  a 
speed-up  of  1.7x  and  1.6x,  respectively,  when  compared  to 
SCUC for the IEEE 24-bus system. It is also worth noting that 
the proposed model reduction approaches are compatible with 
any  existing  optimization/decomposition  methods  as  well  as 
ML methods aiming to remove some unnecessary constraints. 

VII.  REFERENCES 

[1]  Mingjian  Tuo  and  Xingpeng  Li,  “Security-Constrained  Unit 
Commitment Considering Locational Frequency Stability in Low-Inertia 
Power Grids”, arXiv:2110.11498, Oct. 2021. 
Jin  Lu  and  Xingpeng  Li,  “The  Benefits  of  Hydrogen  Energy 
Transmission  and  Conversion  Systems  to  the  Renewable  Power  Grids: 
Day-ahead Unit Commitment”, arXiv:2206.14279, Jun. 2022. 

[2] 

[3]  Arun  Venkatesh  Ramesh,  Xingpeng  Li,  "Security  Constrained  Unit 
Commitment  with  Corrective  Transmission  Switching," 2019  North 
American Power Symposium (NAPS), Wichita, KS, USA, October 2019, 
pp. 1-6, doi: 10.1109/NAPS46351.2019.9000308. 

[4]  Arun  Venkatesh  Ramesh,  Xingpeng  Li,  "Network  Reconfiguration 
Impact  on  Renewable  Energy  System  and  Energy  Storage  System  in 
Day-Ahead  Scheduling," 2021  IEEE  Power  &  Energy  Society  General 
doi: 
2021, 
(PESGM), 
Meeting 
10.1109/PESGM46819.2021.9638033. 

01-05, 

pp. 

[5]  Arun Venkatesh Ramesh, Xingpeng Li, "Reducing Congestion-Induced 
Renewable  Curtailment  with  Corrective  Network  Reconfiguration  in 
Day-Ahead  Scheduling," 2020  IEEE  Power  &  Energy  Society  General 
doi: 
2020, 
Meeting 
10.1109/PESGM41954.2020.9281399. 

(PESGM), 

1-5, 

pp. 

[6]  Day-ahead  market 

overview,  CA-ISO, 
http://www.caiso.com/Documents/Presentation-Existing-Day-Ahead-
Market-Overview.pdf 

[Online].  Available: 

[7]  Manual  11,  Day-ahead  scheduling  manual,  NY-ISO, 

[Online]. 

Available: 

 
https://www.nyiso.com/documents/20142/2923301/dayahd_schd_mnl.p 
df/0024bc71-4dd9-fa80-a816-f9f3e26ea53a 

Transactions on Power Systems, vol. 36, no. 3, pp. 2614-2622, May 
2021, doi: 10.1109/TPWRS.2020.3040222. 

10 

[8]  Wu,  L.,  &  Shahidehpour,  M.  (2010).  Accelerating  the  Benders 
decomposition  for  network-constrained  unit  commitment  problems. 
Energy Systems (Berlin. Periodical), 1(3), 339–376. 

[9]  R.  Saavedra,  A.  Street,  and  J.  M.  Arroyo,  “Day-Ahead  Contingency-
Constrained  Unit  Commitment  with  Co-Optimized  Post-Contingency 
Transmission Switching”, IEEE Transactions on Power Systems, 2020. 

[10]  Arun  Venkatesh  Ramesh,  Xingpeng  Li,  and  Kory  W.  Hedman,  "An 
Accelerated-Decomposition  Approach  for  Security-Constrained  Unit 
Commitment  With  Corrective  Network  Reconfiguration,"  in  IEEE 
Transactions  on  Power  Systems,  vol.  37,  no.  2,  pp.  887-900,  March 
2022, doi: 10.1109/TPWRS.2021.3098771. 

[11]  Cunzhi  Zhao  and  Xingpeng  Li,  “Microgrid  Day-Ahead  Scheduling 
Considering  Neural  Network  based  Battery  Degradation  Model”, 
arXiv:2112.08418, Feb. 2022. 

[12]  Vasudharini  Sridharan,  Mingjian  Tuo  and  Xingpeng  Li,  “Wholesale 
Electricity  Price  Forecasting  using  Integrated  Long-term  Recurrent 
Convolutional Network Model”, arXiv:2112.13681, Dec. 2021. 

[13]  Thuan  Pham  and  Xingpeng  Li,  “Neural  Network-based  Power  Flow 
Model”, IEEE  Green  Technologies  Conference,  Houston,  TX,  USA, 
Mar. 2022. 

[14]  Mingjian  Tuo  and  Xingpeng  Li,  “Long-term  Recurrent  Convolutional 
Ambient 
Networks-based 
Measurements”, IEEE  IAS  Annual  Meeting,  Detroit,  MI,  USA,  Oct. 
2022. 

Estimation 

Inertia 

using 

[15]  Thuan  Pham  and  Xingpeng  Li,  “Reduced  Optimal  Power  Flow  Using 

Graph Neural Network”, arXiv:2206.13591, Jun. 2022. 

[16]  V. Nair, S. Bartunov, F. Gimeno, I. von Glehn, P. Lichocki, I. Lobov, B. 
O'Donoghue, N. Sonnerat, C. Tjandraatmadja, P. Wang et al., "Solving 
mixed integer programs using neural networks", arXiv preprint, 2020. 

[17]  X.  Lin,  Z.  J.  Hou,  H.  Ren  and  F.  Pan,  "Approximate  Mixed-Integer 
Programming  Solution  with  Machine  Learning  Technique  and  Linear 
Programming Relaxation," 2019 3rd International Conference on Smart 
Grid  and  Smart  Cities 
(ICSGSC),  2019,  pp.  101-107,  doi: 
10.1109/ICSGSC.2019.00-11. 

[18]  J.  Qin,  N.  Yu  and  Y.  Gao,  "Solving  Unit  Commitment  Problems  with 
Multi-step  Deep  Reinforcement  Learning,"  2021  IEEE  International 
Conference on Communications, Control, and Computing Technologies 
for  Smart  Grids 
(SmartGridComm),  2021,  pp.  140-145,  doi: 
10.1109/SmartGridComm51999.2021.9632339. 

[19]  F. Li, J. Qin and W. X. Zheng, "Distributed Q  -Learning-Based Online 
Optimization  Algorithm  for  Unit  Commitment  and  Dispatch  in  Smart 
Grid,"  in  IEEE  Transactions  on  Cybernetics,  vol.  50,  no.  9,  pp.  4146-
4156, Sept. 2020, doi: 10.1109/TCYB.2019.2921475. 

[20]  Y. Yang, X. Lu, and L Wu. “Integrated data-driven framework for fast 
SCUC  calculation,”  IET  Generation,  Transmission  &  Distribution,  vol. 
14, no. 24, pp. 5728-5738, Dec. 2020. 

[21]  S.  Pineda,  J.  M.  Morales,  and  A.  Jiménez-Cordero,  “Data-driven 
screening  of  network  constraints  for  unit  commitment,”  IEEE 
Transactions  on  Power  Systems,  vol.  35,  no.  5,  pp.  3695-3705,  Sept. 
2020. 

[22]  G.  Dalal,  E.  Gilboa,  S.  Mannor,  and  L.  Wehenkel,  “Unit  commitment 
using nearest neighbor as a short-term proxy,” in Proc. 20th Power Syst. 
Comput. Conf., 2018, doi: 10.23919/PSCC.2018.8442516. 

[23]  F.  Mohammadi,  M.  Sahraei-Ardakani,  D.  N.  Trakas  and  N.  D. 
Hatziargyriou, 
"Machine  Learning  Assisted  Stochastic  Unit 
Commitment  During  Hurricanes  With  Predictable  Line  Outages," 
in IEEE Transactions on Power Systems, vol. 36, no. 6, pp. 5131-5142, 
Nov. 2021. 

[24]  F.  Mohammadi,  M.  Sahraei-Ardakani,  D.  N.  Trakas  and  N.  D. 
Hatziargyriou, 
"Machine  Learning  Assisted  Stochastic  Unit 
Commitment:  A  Feasibility  Study," 2020  52nd  North  American  Power 
Symposium 
doi: 
2021, 
10.1109/NAPS50074.2021.9449789. 

(NAPS), 

1-6, 

pp. 

[25]  A.  Porras,  S.  Pineda,  J.  M.  Morales  and  A.  Jimenez-Cordero,  "Cost-
driven  Screening  of  Network  Constraints  for  the  Unit  Commitment 
Problem," 
doi: 
in IEEE 
10.1109/TPWRS.2022.3160016. 

Transactions 

on  Power 

Systems, 

[26]  S. Zhang, H. Ye, F. Wang, Y. Chen, S. Rose and Y. Ma, "Data-Aided 
Offline and Online Screening for Security Constraint," in IEEE 

[27]  Y. Yang, Z. Yang, J. Yu, K. Xie and L. Jin, "Fast Economic Dispatch in 
Smart Grids Using Deep Learning: An Active Constraint Screening 
Approach," in IEEE Internet of Things Journal, vol. 7, no. 11, pp. 
11030-11040, Nov. 2020, doi: 10.1109/JIOT.2020.2993567. 

[28]  F. Hasan and A. Kargarian, "Topology-aware Learning Assisted Branch 
and  Ramp  Constraints  Screening  for  Dynamic  Economic  Dispatch," 
in IEEE 
doi: 
10.1109/TPWRS.2022.3142957. 

Transactions 

Systems, 

Power 

on 

[29]  S.  Schmitt,  I.  Harjunkoski,  M.  Giuntoli,  J.  Poland  and  X.  Feng,  "Fast 
of  Unit  Commitment  Using  Machine  Learning 
International  Energy  Conference 
doi: 

Solution 
Approaches," 2022 
(ENERGYCON), 
pp. 
10.1109/ENERGYCON53164.2022.9830191. 

IEEE  7th 
2022, 

1-6, 

[30]  T. Iqbal, H. U. Banna, A. Feliachi and M. Choudhry, "Solving Security 
Constrained  Unit  Commitment 
Inductive 
Learning," 2022  IEEE  Kansas  Power  and  Energy  Conference  (KPEC), 
2022, pp. 1-4, doi: 10.1109/KPEC54747.2022.9814780. 

Problem  Using 

[31]  T. Iqbal, H. U. Banna and A. Feliachi, "AI-Driven Security Constrained 
Unit  Commitment  Using  Eigen  Decomposition  And  Linear  Shift 
Factors," 2021  North  American  Power  Symposium  (NAPS),  2021,  pp. 
01-06, doi: 10.1109/NAPS52732.2021.9654579. 

[32]  Arun Venkatesh Ramesh and Xingpeng Li, “Machine Learning Assisted 
Commitment”, 
Security-Constrained 

Unit 

Approach 
for 
 arXiv:2111.09824, Nov. 2021. 

[33]  A.S. Xavier, F. Qiu, S. Ahmed, “Learning to solve large-scale security-
constrained  unit  commitment  problems,”  INFORMS  Journal  on 
Computing, 2020. 

[34]  Y.  Zhou,  Q.  Zhai,  L.  Wu  and  M.  Shahidehpour,  "A  Data-Driven 
Variable  Reduction  Approach  for  Transmission-Constrained  Unit 
Commitment  of  Large-Scale  Systems,"  in Journal  of  Modern  Power 
Systems and Clean Energy, doi: 10.35833/MPCE.2021.000382. 

[35]  T. Wu, Y. -J. Angela Zhang and S. Wang, "Deep Learning to Optimize: 
Security-Constrained  Unit  Commitment  With  Uncertain  Wind  Power 
Generation  and  BESSs,"  in IEEE  Transactions  on  Sustainable  Energy, 
doi: 
vol. 
10.1109/TSTE.2021.3107848. 

231-240, 

2022, 

Jan. 

pp. 

no. 

13, 

1, 

[36]  M. Li, W. Wei, Y. Chen, M. -F. Ge and J. P. S. Catalão, "Learning the 
Optimal Strategy of Power System Operation With Varying Renewable 
Generations," in IEEE Transactions on Sustainable Energy, vol. 12, no. 
4, pp. 2293-2305, Oct. 2021, doi: 10.1109/TSTE.2021.3088951. 

[37]  C.  Grigg et  al.,  "The  IEEE  Reliability  Test  System-1996.  A  report 
prepared by the Reliability Test System Task Force of the Application of 
Probability  Methods  Subcommittee,"  IEEE  Transactions  on  Power 
Systems, vol. 14, no. 3, pp. 1010-1020, Aug. 1999. 

[38]  Pedregosa, Fabian, et al. "Scikit-learn: Machine learning in Python." the 

Journal of machine Learning research 12 (2011): 2825-2830. 

[39]  SCIKIT K Nearest Neighbors model, [Online]. Available: https://scikit-
learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassi
fier.html 

[40]  Scikit  Random  forest  model,  [Online].  Available:  https://scikit-
learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClas
sifier.html 

[41]  Scikit  Logistic  regression  model,  [Online].  Available:  https://scikit-
learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegres
sion.html 

[42]  Scikit  Multi-layer  perceptron  model,  [Online].  Available:  https://scikit-
learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifi
er.html 

[43]  Andrew NG’s machine learning course. Lecture on Supervised Learning 

[online]. Available:  http://cs229.stanford.edu/materials.  

[44]  A.Y. Ng and M.I. Jordan, “On Discriminative vs. Generative Classifiers: 
A  Comparison  of  Logistic  Regression  and  Naive  Bayes,”  Advances  in 
Neural Information Processing Systems, pp. 841-848, 2002. 

[45]  R.  D.  Zimmerman,  C.  E.  Murillo-Sanchez,  and  R.  J.  Thomas, 
“MATPOWER:  Steady-State  Operations,  Planning,  and  Analysis  Tools 
for  Power  Systems  Research  and  Education”,  IEEE  Transactions  on 
Power Systems, vol. 26, no. 1, pp. 12-19, 2011. 

[46]  T.  Xu;  A.  B.  Birchfield;  K.  S.  Shetye;  T.  J.  Overbye,“Creation  of 
synthetic electric grid models for transient stability studies,” accepted by 
2017  IREP  Symposium  Bulk  Power  System  Dynamics  and  Control, 
Espinho, Portugal, 2017. 

 
