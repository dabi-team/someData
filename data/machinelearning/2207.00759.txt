2
2
0
2

l
u
J

2

]
E
S
.
s
c
[

1
v
9
5
7
0
0
.
7
0
2
2
:
v
i
X
r
a

Abstraction and Refinement: Towards Scalable and Exact
Verification of Neural Networks

JIAXIANG LIU, YUNHAN XING, and XIAOMU SHI, Shenzhen University, China
FU SONG, ShanghaiTech University, China
ZHIWU XU and ZHONG MING, Shenzhen University, China

As a new programming paradigm, deep neural networks (DNNs) have been increasingly deployed in practice,
but the lack of robustness hinders their applications in safety-critical domains. While there are techniques
for verifying DNNs with formal guarantees, they are limited in scalability and accuracy. In this paper, we
present a novel abstraction-refinement approach for scalable and exact DNN verification. Specifically, we
propose a novel abstraction to break down the size of DNNs by over-approximation. The result of verifying
the abstract DNN is always conclusive if no spurious counterexample is reported. To eliminate spurious
counterexamples introduced by abstraction, we propose a novel counterexample-guided refinement that
refines the abstract DNN to exclude a given spurious counterexample while still over-approximating the
original one. Our approach is orthogonal to and can be integrated with many existing verification techniques.
For demonstration, we implement our approach using two promising and exact tools Marabou and Planet as
the underlying verification engines, and evaluate on widely-used benchmarks ACAS Xu, MNIST and CIFAR-10.
The results show that our approach can boost their performance by solving more problems and reducing
up to 86.3% and 78.0% verification time, respectively. Compared to the most relevant abstraction-refinement
approach, our approach is 11.6–26.6 times faster.

1 INTRODUCTION

Due to surprising breakthroughs in many challenging tasks such as image recognition [Russakovsky
et al. 2015] and natural language processing [Hinton et al. 2012], deep learning has arguably
become a new programming paradigm that takes over traditional software programs in many
areas. For instance, deep neural networks (DNNs) are increasingly being deployed in safety-critical
applications, e.g., autonomous driving [Urmson and Whittaker 2008] and medical systems [Litjens
et al. 2017]. However, DNNs are fragile to small perturbations due to the lack of robustness [Carlini
and Wagner 2017; Dalvi et al. 2004; Goodfellow et al. 2015; Kurakin et al. 2017; Papernot et al. 2016;
Szegedy et al. 2014]. Therefore, it is important to formally guarantee the robustness of DNNs before
to deploy them in safety-critical applications.

Many efforts have been made to verify DNNs [Bunel et al. 2017; Ehlers 2017; Elboher et al. 2020;
Huang et al. 2017; Katz et al. 2017, 2019; Lin et al. 2019; Pulina and Tacchella 2010; Wong and Kolter
2018]. Early work relies on using constraint solvers, often providing soundness and completeness
guarantees. However, their scalability is limited due to the intrinsic computational complexity, e.g.,
NP-complete even for simple neural networks and properties [Katz et al. 2017]. Another line of
work is based on abstract interpretation to improve the scalability at the cost of precision [Gehr
et al. 2018; Singh et al. 2018, 2019; Tran et al. 2019; Wang et al. 2018; Yang et al. 2021]. Although
few of them incorporate refinement strategies to improve accuracy [Singh et al. 2019; Wang et al.
2018; Yang et al. 2021], it remains a great challenge to efficiently and precisely verify large-scale
DNNs. One of the most promising techniques used in formal verification to improve the efficiency is
counterexample-guided abstraction refinement (CEGAR) framework [Clarke et al. 2000]. The essential
idea of CEGAR is that, when given a target system 𝑆 to verify, an over-approximation, small-sized
system ¯𝑆 is constructed by abstraction and verified by an off-the-shelf tool. The result is always
conclusive if no spurious counterexample is reported. Otherwise, to regain precision, the abstract

Authors’ addresses: Jiaxiang Liu, jiaxiang0924@gmail.com; Yunhan Xing, xingyunhan@email.szu.edu.cn; Xiaomu Shi,
xshi0811@gmail.com, Shenzhen University, Shenzhen, China; Fu Song, ShanghaiTech University, Shanghai, China, songfu@
shanghaitech.edu.cn; Zhiwu Xu, xuzhiwu@szu.edu.cn; Zhong Ming, Shenzhen University, Shenzhen, China.

 
 
 
 
 
 
2

Liu et al.

system ¯𝑆 is refined guided by the spurious counterexample to exclude it. The verification process is
repeated on the refined system until the original system is proved or a genuine counterexample is
found.

To instantiate the CEGAR framework, one needs to address the following two questions: (1) how
to abstract a target system and (2) how to refine an abstract system. When instantiating CEGAR in
DNN verification, there are four technical challenges:

C1: The abstraction should guarantee soundness, i.e., if an abstract DNN ¯𝑁 is proved robust, the

target DNN 𝑁 must be robust.

C2: The abstraction should reduce the network size as much as possible while preserving accuracy
as much as possible, because coarse-grained abstract DNNs may result in plenty of spurious
counterexamples, thus requiring more refinement steps.

C3: The refinement must preserve soundness as well, similar to C1, and also excludes a given

counterexample.

C4: The refinement should regain the accuracy as much as possible, meanwhile enlarging the

network size as little as possible.

In this paper, addressing the above challenges, we present a scalable and exact CEGAR-based

approach for DNN verification by proposing novel procedures for abstraction and refinement.

We define the abstraction procedure as a synergistic integration of two novel abstraction prim-
itives: one is to merge neurons and the other is to remove neurons, both of which are able to
reduce network size. To address C1, these primitives are well-designed according to the weights
and bounds of neurons, thus provide soundness guarantees. To address C2, the iterative application
of abstraction primitives is guided by a strategy that selects a primitive aimed at minimizing the
loss of accuracy during each iteration, thus can abstract out more neurons when sacrificing the
same accuracy.

The refinement procedure is defined as a synergistic integration of two novel refinement primitives:
one splits a single neuron into two neurons and the other recovers a removed neuron. To address
C3, the iterative application of refinement primitives is restricted by a dependency graph, a novel
notion proposed to characterize the dependency between refinement steps. Under the restriction of
the dependency graph, the refinement procedure is proved sound, otherwise may not be sound.
Last but not least, the refinement procedure is also guided by a strategy to address C4 which selects
a refinement primitive to regain the most accuracy, thus can keep the network size smaller when
restoring the same amount of accuracy.

Our approach is orthogonal to and can be integrated with many existing approaches. For
evaluation, we implement our approach as a tool NARv using two promising and exact tools
Marabou [Katz et al. 2019] and Planet [Ehlers 2017] as back-end verification engines. The experi-
mental results show that our approach can improve their scalability and boost their performance
by reducing up to 86.3% and 78.0% verification time, respectively. Moreover, our approach is illus-
trated to significantly outperform the only tool that supports structure-oriented CEGAR-based
verification [Elboher et al. 2020], 11.6–26.6 times faster.

To sum up, the main contributions of this work are as follows:

• We propose a novel abstraction procedure that synergistically integrates two abstraction
primitives using a novel abstraction strategy, allowing to soundly and maximally reduce the
network size when sacrificing the same accuracy.

• We propose a novel refinement procedure consisting of two refinement primitives, a notion
of dependency graphs and a strategy for their synergistic integration, allowing to soundly
refine the network and keep the network size as small as possible when restoring the same
amount of accuracy.

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

3

• We implement our approach as a tool NARv with two promising DNN verification engines
Marabou and Planet and conduct an extensive evaluation, demonstrating the efficacy of
our approach.

Outline. Section 2 defines basic notations. Section 3 presents the overview of our approach. We
propose our abstraction procedure and refinement procedure in Section 4 and Section 5, respectively.
Section 6 reports experimental results. Finally, after discussing related work in Section 7, we
conclude the paper in Section 8.

To foster further research, benchmarks and experimental data are released at https://github.com/

NARv22/data. The source code is available at https://github.com/formes20/narv.

2 PRELIMINARIES

2.1 Deep Neural Networks
A fully connected feedforward deep neural network (DNN ) with ℓ + 1 layers is an acyclic graph
structured in layers, where 0-th and ℓ-th layers are input layer and output layer, respectively and
the other layers are hidden layers. The nodes in each layer are neurons. We use 𝑣𝑖,𝑗 to denote the
value of the 𝑗-th neuron in layer 𝑖, and 𝒗𝑖 = (𝑣𝑖,1, . . . , 𝑣𝑖,𝑛)𝑇 the output vector of layer 𝑖 containing
𝑛 neurons. Sometimes, 𝑣𝑖,𝑗 also denotes the neuron itself. Each neuron 𝑣𝑖,𝑗 in layer 𝑖 (1 ≤ 𝑖 ≤ ℓ)
is associated with a bias 𝑏 (𝑣𝑖,𝑗 ), and is connected by the weighted edges 𝑤 (𝑣𝑖−1,𝑘, 𝑣𝑖,𝑗 ) from the
neurons 𝑣𝑖−1,𝑘 in layer 𝑖 − 1. A DNN computes the output of a given input by propagating it through
the network, where the value of each neuron is calculated by applying an activation function to the
weighted sum of the neuron values from the preceding layer.

Formally, a DNN is a function 𝑁 (𝒙) defined by:

𝑁 (𝒙) = 𝑊ℓ𝒗ℓ−1 + 𝒃ℓ,
where 𝒗0 = 𝒙, 𝒗𝑖 = 𝜎 (𝑊𝑖𝒗𝑖−1 +𝒃𝑖 ) for 1 ≤ 𝑖 < ℓ, 𝑊𝑖 and 𝒃𝑖 are respectively the weight matrix and bias
vector associated with layer 𝑖, and 𝜎 is an activation function applied in an element-wise manner.
In this paper, we focus on the most commonly used ReLU activation function 𝑅𝑒𝐿𝑈 (𝑥) = max(𝑥, 0).
𝑁 (𝑣𝑖,𝑗 )) denotes an upper (resp., lower) bound of 𝑣𝑖,𝑗 in 𝑁 , that is,
The notation ub
lb

𝑁 (𝑣𝑖,𝑗 ), w.r.t. a given input space.

𝑁 (𝑣𝑖,𝑗 ) ≤ 𝑣𝑖,𝑗 ≤ ub

𝑁 (𝑣𝑖,𝑗 ) (resp., lb

2.2 Formal Verification of DNNs
Given a DNN 𝑁 , a property 𝑃 over the inputs 𝒙 and a property 𝑄 over outputs 𝒚 = 𝑁 (𝒙), a
verification problem 𝜑 = ⟨𝑁 , 𝑃, 𝑄⟩ is to check whether any input 𝒙 that fulfils 𝑃 will result in an
output 𝒚 = 𝑁 (𝒙) that satisfies 𝑄, where 𝑃 forms the input space of interests. As usual, we consider
input properties in conjunctions of linear constraints. W.l.o.g., we assume that the output layer
only contains a single neuron 𝑦, and the output property is of the form 𝑦 ≤ 𝑐 for a given constant
𝑐 [Elboher et al. 2020].

Given a verification problem 𝜑 = ⟨𝑁 , 𝑃, 𝑄⟩, a DNN ¯𝑁 is an over-approximation of 𝑁 if 𝑁 (𝒙) ≤
¯𝑁 (𝒙) for every 𝒙 that fulfils 𝑃. Note that 𝑁 (𝒙) ≤ ¯𝑁 (𝒙) implies that ¯𝑁 (𝒙) ≤ 𝑐 ⇒ 𝑁 (𝒙) ≤ 𝑐. An
input 𝒙 is a counterexample of 𝑁 if 𝒙 fulfils 𝑃 but 𝑁 (𝒙) > 𝑐. A counterexample 𝒙 of ¯𝑁 is spurious
on 𝑁 if 𝑁 (𝒙) ≤ 𝑐.

3 OVERVIEW OF OUR APPROACH

Our CEGAR-based approach is described in Algorithm 1, which invokes two vital components: the
abstraction procedure Abstract and the refinement procedure Refine. Given a verification problem
⟨𝑁 , 𝑃, 𝑄⟩, Algorithm 1 returns either YES indicating that the problem holds or a counterexample
cex as the witness of the violation.

4

Liu et al.

Algorithm 1 CEGAR-Based Framework of Our Approach
Input: A verification problem ⟨𝑁 , 𝑃, 𝑄⟩
Output: YES if the problem holds; otherwise a counterexample

1: Build an abstract DNN ¯𝑁 ← Abstract(𝑁 , 𝑃)
2: while Verify(⟨ ¯𝑁 , 𝑃, 𝑄⟩) = NO do
Extract a counterexample cex
3:
if cex is a counterexample of ⟨𝑁 , 𝑃, 𝑄⟩ then
4:

return cex

else ¯𝑁 ← Refine( ¯𝑁 , cex)

5:
6:
7: return YES

To solve a verification problem ⟨𝑁 , 𝑃, 𝑄⟩, we first build an over-approximation ¯𝑁 of 𝑁 by invoking
Abstract (line 1). Abstract first transforms 𝑁 into an equivalent DNN 𝑁 ′ such that increasing
the value of each single hidden neuron in 𝑁 ′ either increases or decreases the network output,
thus establishing monotonicity between the value of each hidden neuron and the network’s output.
Then, we build the over-approximation ¯𝑁 from the DNN 𝑁 ′ by a synergistic integration of two
novel abstraction primitives: Merge and Freeze, where Merge merges two neurons with the same
monotonicity into a single one while Freeze deletes a neuron from the network. Both Merge
and Freeze build an over-approximation of a given DNN, thus provide soundness guarantees (i.e.,
challenge C1). To address C2, abstraction primitives are iteratively applied according to a strategy
until a given accuracy threshold is reached, where the strategy is designed to minimize the loss of
accuracy during each iteration, thus reduces the network size as much as possible when sacrificing
the same accuracy. To achieve this, we measure the loss of accuracy induced by applying abstraction
primitives.

Next, we check if the verification problem ⟨ ¯𝑁 , 𝑃, 𝑄⟩ holds or not by invoking a verification engine
Verify (line 2). If ⟨ ¯𝑁 , 𝑃, 𝑄⟩ holds, we can conclude that the original verification problem ⟨𝑁 , 𝑃, 𝑄⟩
holds as well, because ¯𝑁 over-approximates 𝑁 . Otherwise, a counterexample cex is extracted from
¯𝑁 . If cex is a genuine counterexample of ⟨𝑁 , 𝑃, 𝑄⟩, cex is reported as the witness of the violation to
the property 𝑄 (line 5). If cex is a spurious counterexample of ⟨𝑁 , 𝑃, 𝑄⟩, ¯𝑁 is refined to exclude cex
by invoking Refine (line 6).

Refine is also a synergistic integration of two novel refinement primitives: Split and Recover,
where Split splits an abstract neuron into two while Recover gets back a deleted neuron. However,
applying Split or Recover without any restriction does not necessarily yield an over-approximation
of the original DNN 𝑁 (i.e., challenge C3). To solve this issue, Split and Recover are applied
according to a dependency graph, a novel notion proposed to characterize their dependency. To
address C4, refinement primitives are iteratively applied according to a strategy until the spurious
counterexample cex is excluded, where the strategy is designed to regain the most accuracy during
each iteration, thus keeps the network size as small as possible when restoring the same amount of
accuracy. To achieve this, we introduce a profit function parameterized by the counterexample cex
to measure the accuracy that can be restored via refinement primitives on different neurons.

4 NETWORK ABSTRACTION

In this section, we present our abstraction procedure Abstract.

4.1 Preprocessing
Before to abstract a given DNN, all hidden neurons should be classified into inc or dec, indicating
their monotonic effect on the network’s output. A neuron is inc if increasing its value, while

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

5

2

4

-1

𝑣1,1
-1

𝑣1,2

4

𝑥1

2

𝑥2

2

-1

3

𝑦

𝑣 +
2,1

𝑣 −
2,2

𝑣 +
2,3

𝑥1

4

4

2

2

𝑥2

-1

𝑣 +
1,1
-1

2

4

𝑣 −
1,1
-1

𝑣1,2

2

-1

3

𝑦

𝑣 +
2,1

𝑣 −
2,2

𝑣 +
2,3

(a) Initial Network

(b) After Preprocessing 𝑣1,1

Fig. 1. Example for Preprocessing

keeping all the inputs unchanged, increases the network’s output. Symmetrically, a neuron is dec if
decreasing its value increases the network’s output. However, not all hidden neurons in an arbitrary
DNN can be simply classified. To achieve this, we transform the given DNN 𝑁 into an equivalent
DNN 𝑁 ′, namely, 𝑁 (𝒙) = 𝑁 ′(𝒙) for any input 𝒙, during which the classification is performed on
𝑁 ′.

The preprocessing procedure starts with setting the single output 𝑦 as inc, and proceeds back-
wards layer by layer. Suppose that neurons in layer 𝑖 + 1 have been classified as inc or dec. A
hidden neuron 𝑣𝑖,𝑗 in layer 𝑖 will be split into two new neurons 𝑣 +
𝑖,𝑗 and 𝑣 −
𝑖,𝑗 by copying all incoming
𝑖,𝑗 only keeps positive-weighted ones pointing from 𝑣𝑖,𝑗
edges of 𝑣𝑖,𝑗 . As for the outgoing edges, 𝑣 +
to inc neurons and negative-weighted ones pointing from 𝑣𝑖,𝑗 to dec neurons. Similarly, 𝑣 −
𝑖,𝑗 only
keeps positive-weighted ones from 𝑣𝑖,𝑗 to dec neurons and negative-weighted ones from 𝑣𝑖,𝑗 to inc
neurons. The new neurons 𝑣 +

𝑖,𝑗 are now respectively inc and dec neurons.

𝑖,𝑗 and 𝑣 −

Formally, for every neuron 𝑣𝑖−1,𝑘 in layer 𝑖 − 1, we set:

𝑖,𝑗 ) = 𝑤 (𝑣𝑖−1,𝑘, 𝑣 −
And for every neuron 𝑣𝑖+1,𝑘 in layer 𝑖 + 1 of 𝑁 ′, we set:

𝑤 (𝑣𝑖−1,𝑘, 𝑣 +

𝑖,𝑗 ) = 𝑤 (𝑣𝑖−1,𝑘, 𝑣𝑖,𝑗 ).

𝑤 (𝑣 +

𝑖,𝑗, 𝑣𝑖+1,𝑘 ) =

𝑤 (𝑣 −

𝑖,𝑗, 𝑣𝑖+1,𝑘 ) =

(cid:26)𝑤 (𝑣𝑖,𝑗, 𝑣𝑖+1,𝑘 ), if 𝑤 (𝑣𝑖,𝑗, 𝑣𝑖+1,𝑘 ) · 𝑠 (𝑣𝑖+1,𝑘 ) > 0;
0,
(cid:26)𝑤 (𝑣𝑖,𝑗, 𝑣𝑖+1,𝑘 ), if 𝑤 (𝑣𝑖,𝑗, 𝑣𝑖+1,𝑘 ) · 𝑠 (𝑣𝑖+1,𝑘 ) < 0;
0,

otherwise

otherwise

where 𝑠 (𝑣) = 1 if 𝑣 is an inc neuron and 𝑠 (𝑣) = −1 if 𝑣 is a dec neuron. Furthermore, the biases of
new neurons are copied from 𝑣𝑖,𝑗 , i.e., 𝑏 (𝑣 +

𝑖,𝑗 ) = 𝑏 (𝑣𝑖,𝑗 ).

𝑖,𝑗 ) = 𝑏 (𝑣 −

Intuitively, when the neuron 𝑣 +

𝑖,𝑗 increases, all the inc (resp., dec) neurons in layer 𝑖 + 1 increase
(resp., decrease) since they connect to 𝑣 +
𝑖,𝑗 by positive (resp., negative) weights. Both the increment
of inc neurons and decrement of dec neurons in layer 𝑖 + 1 will increase the network’s output by
definition. Hence the neuron 𝑣 +

𝑖,𝑗 is inc. The case of 𝑣 −

𝑖,𝑗 follows similarly.

Example 4.1. Consider the DNN shown in Figure 1(a), where the last two layers have been
preprocessed. Consider neuron 𝑣1,1 whose bias is −1 and the related weights are shown. The DNN
1,1 and 𝑣 −
after preprocessing 𝑣1,1 is depicted in Figure 1(b), where: 𝑣1,1 is split into two neurons 𝑣 +
1,1
that have the same incoming edges and biases as 𝑣1,1, 𝑣 +
1,1 keeps the outgoing edge of weight 2
1,1 keeps the outgoing edge of weight 4 pointing to the dec neuron
pointing to the inc neuron 𝑣 +
2,1, 𝑣 −
2,2 and the outgoing edge of weight −1 pointing to the inc neuron 𝑣 +
𝑣 −
1,1 and 𝑣 −
1,1
are now respectively inc and dec.

2,3. The neurons 𝑣 +

6

Liu et al.

After the preprocessing of all the hidden neurons, we obtain a new DNN 𝑁 ′ that is equivalent to

𝑁 , in which each hidden neuron is classified into either inc or dec. Thus, we have:

Lemma 4.2. Any DNN 𝑁 can be transformed into an equivalent DNN 𝑁 ′ where each hidden neuron

is classified into either inc or dec, by increasing the network size by a factor of at most 2.

By Lemma 4.2, we hereafter assume that each given DNN has been preprocessed and all its

hidden neurons have been classified into inc/dec.

4.2 Abstraction Primitives

As aforementioned, we propose two novel abstraction primitives: Merge and Freeze, to construct
over-approximations of DNNs.

The Merge primitive is to merge a pair of hidden neurons with same label inc/dec in the same
layer into a single one. We seek to increase the values of inc neurons and decrease the values of
dec neurons, ensuring that the network’s output always increases. Suppose we are constructing
an over-approximation ¯𝑁 of 𝑁 . Let ¯𝑤 and ¯𝑏 denote respectively the weights and biases in the
constructed network ¯𝑁 . The Merge primitive merges two hidden inc neurons 𝑣𝑖,𝑗 and 𝑣𝑖,𝑘 into a
new inc neuron 𝑣𝑖,𝑡 via the following steps:

(1) all edges connecting to 𝑣𝑖,𝑗 or 𝑣𝑖,𝑘 are removed;
(2) neurons 𝑣𝑖,𝑗 and 𝑣𝑖,𝑘 are replaced by a new neuron 𝑣𝑖,𝑡 ;
(3) from each neuron 𝑣𝑖−1,𝑝 in the preceding layer, an incoming edge to 𝑣𝑖,𝑡 is added as

¯𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑡 ) = max{𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑗 ), 𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑘 )};

(4) to each neuron 𝑣𝑖+1,𝑞 in the succeeding layer, an outgoing edge from 𝑣𝑖,𝑡 is added as

¯𝑤 (𝑣𝑖,𝑡, 𝑣𝑖+1,𝑞) = 𝑤 (𝑣𝑖,𝑗, 𝑣𝑖+1,𝑞) + 𝑤 (𝑣𝑖,𝑘, 𝑣𝑖+1,𝑞);

(5) the bias of 𝑣𝑖,𝑡 is ¯𝑏 (𝑣𝑖,𝑡 ) = max{𝑏 (𝑣𝑖,𝑗 ), 𝑏 (𝑣𝑖,𝑘 )}.
Intuitively, the max operation in steps (3) and (5) guarantees that 𝑣𝑖,𝑡 is no less than the original
neurons 𝑣𝑖,𝑗 and 𝑣𝑖,𝑘 . By the definition of outgoing edges, this amounts to increasing or keeping 𝑣𝑖,𝑗
and 𝑣𝑖,𝑘 in 𝑁 . Since 𝑣𝑖,𝑗 and 𝑣𝑖,𝑘 are both inc, it ensures that the output either does not change or is
increased by Merge. Similarly, the Merge primitive for dec neurons is defined except that max is
replaced by min. A neuron produced by Merge is called an abstract neuron, otherwise an atomic
neuron.

Example 4.3. Consider the inc neurons 𝑣1 and 𝑣2 of the DNN 𝑁 shown in Figure 2(a). After
merging 𝑣1 and 𝑣2, we obtain the DNN ¯𝑁1 shown in Figure 2(b), where for the abstract neuron
of (𝑣1, 𝑣2), the weight of its incoming edge from 𝑥1 is 4 = max{1, 4}, its bias is 2 = max{1, 2},
and the weight of its outgoing edge to 𝑦 is 3 = 2 + 1. Given an input 𝒙0 = (1, 1)𝑇 , we have
¯𝑁1(𝒙0) = 15 > 5 = 𝑁 (𝒙0).

The following lemma justifies the soundness of Merge.
Lemma 4.4. Let ¯𝑁 be the DNN constructed from 𝑁 by a single application of Merge. It holds that

¯𝑁 (𝒙) ≥ 𝑁 (𝒙) for each input 𝒙.

One application of Merge reduces the network size by 1, but may decrease the network’s
accuracy. The induced inaccuracy sometimes can be considerable, for instance, when the two
weights in the max operation have a big difference as in Example 4.3. To avoid this issue, we
introduce another abstraction primitive Freeze, to freeze hidden neurons using constants.

Consider a hidden neuron 𝑣𝑖,𝑗 and a constant 𝑎. If 𝑎 is an upper bound of 𝑣𝑖,𝑗 , then freezing
𝑣𝑖,𝑗 by 𝑎 amounts to increasing or keeping 𝑣𝑖,𝑗 . Thus, the network’s output is guaranteed to non-
decrease when 𝑣𝑖,𝑗 is inc. Similarly, if 𝑎 is a lower bound of the dec neuron 𝑣𝑖,𝑗 , the network’s

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

7

𝑥1

2

-1

𝑥2

1
4

-3
-2

1

𝑣1
2

𝑣2
0

𝑣3

2

1

1

(a) Initial Network 𝑁
2

𝑥1

2

𝑥2

4

-3
-2

2

1

1

𝑣1
2

𝑣2
0

𝑣3

0

𝑦

0

𝑦

2

𝑣1

𝑣2

0

𝑣3

𝑥1

𝑥2

4
2

-1

-2

0

𝑦

3

1

(b) ¯𝑁1: Merge 𝑣1 and 𝑣2 in 𝑁

𝑥1

2

𝑥2

4

-3
-2

2

𝑣2
0

𝑣3

4

𝑦

1

1

(c) ¯𝑁2: Freeze 𝑣1 in 𝑁

(d) ¯𝑁 ′

2: Propagate 𝑣1 in ¯𝑁2

Fig. 2. Example for Abstraction Primitives

output is also guaranteed to non-decrease by applying Freeze. Formally, Freeze constructs an
over-approximation ¯𝑁 of 𝑁 as follows: for a hidden neuron 𝑣𝑖,𝑗 ,

(1) all incoming edges to 𝑣𝑖,𝑗 are removed, i.e.,

¯𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑗 ) = 0, for each 𝑣𝑖−1,𝑝 in layer 𝑖 − 1;

(2) the value of the neuron 𝑣𝑖,𝑗 is replaced by a constant as

¯𝑏 (𝑣𝑖,𝑗 ) =

(cid:26) ub
lb

𝑁 (𝑣𝑖,𝑗 ),
𝑁 (𝑣𝑖,𝑗 ),

if 𝑣𝑖,𝑗 is inc;
if 𝑣𝑖,𝑗 is dec.

Intuitively, Freeze produces neurons whose all incoming edges are weighted by 0 and their
values are indeed their biases ¯𝑏 (𝑣𝑖,𝑗 ). Neurons introduced by Freeze are called constant neurons.

Example 4.5. Assume that the range of the inc neuron 𝑣1 in Figure 2(a) is [0, 2]. Then the
network ¯𝑁2 constructed from 𝑁 by Freeze on 𝑣1 is shown in Figure 2(c). All the incoming edges
to 𝑣1 are removed. Its bias is replaced with its upper bound 2. Given the input 𝒙0 = (1, 1)𝑇 ,
¯𝑁2(𝒙0) = 7 < 15 = ¯𝑁1(𝒙0) shows that Freeze can be more accurate than Merge in some situations.

The following lemma provides the soundness of Freeze.

Lemma 4.6. Let ¯𝑁 be the DNN constructed from 𝑁 by a single application of Freeze. It holds that

¯𝑁 (𝒙) ≥ 𝑁 (𝒙) for each input 𝒙.

One may notice that currently Freeze does not reduce the network size. To eliminate constant
neurons introduced by Freeze, we propose a new procedure Propagate. Consider a constant
neuron 𝑣𝑖,𝑗 in the DNN 𝑁 , Propagate works as follows:

(1) to propagate the value of 𝑣𝑖,𝑗 to the succeeding layer 𝑖 + 1, for each neuron 𝑣𝑖+1,𝑞 in layer 𝑖 + 1,

we set:

(2) the constant neuron 𝑣𝑖,𝑗 and related edges are removed.

𝑏 ′(𝑣𝑖+1,𝑞) = 𝑏 (𝑣𝑖+1,𝑞) + 𝑤 (𝑣𝑖,𝑗, 𝑣𝑖+1,𝑞) · 𝑏 (𝑣𝑖,𝑗 );

8

Liu et al.

Example 4.7. Consider the constant neuron 𝑣1 in the DNN ¯𝑁2 shown in Figure 2(c). By applying
Propagate to 𝑣1, 𝑣1 is removed and its value 2 is propagated to the neuron 𝑦, resulting in the DNN
shown in Figure 2(d).

By definition, the obtained DNN 𝑁 ′ after Propagate is equivalent to 𝑁 . Thus, we get:

Lemma 4.8. Let 𝑁 ′ be the DNN constructed from 𝑁 by a single application of Propagate on a

constant neuron. It holds that 𝑁 ′(𝒙) = 𝑁 (𝒙) for each input 𝒙.

Freeze and Propagate can cooperate to delete hidden neurons, hence reducing network size. We
design Propagate as an individual procedure instead of merging with Freeze due to the following
reasons: (i) to improve abstraction efficiency, Propagate is invoked only once before invoking the
verification engine Verify; (ii) to keep Freeze local, i.e., without affecting the succeeding layer, as
locality makes abstraction steps less dependent on each other, thus allows us to extend abstraction
primitives further in Section 4.3.

Following Lemmas 4.4, 4.6 and 4.8, we conclude that our two abstraction primitives Merge and

Freeze (followed by Propagate) do construct over-approximations.

Corollary 4.9. Let ¯𝑁 be the DNN obtained from 𝑁 by iteratively applying abstraction primitives:

Merge and/or Freeze (followed by Propagate). It holds that ¯𝑁 (𝒙) ≥ 𝑁 (𝒙) for each input 𝒙.

Given a DNN 𝑁 to verify, after iteratively applying our abstraction primitives, we get an abstract
DNN ¯𝑁 . Corollary 4.9 ensures that ¯𝑁 is an over-approximation of 𝑁 , namely, if the specified
property holds for ¯𝑁 , it holds as well for 𝑁 . Therefore, we solve the challenge C1.

4.3 Generalizing the Freeze Primitive
Starting from a DNN 𝑁0, iterating the application of abstraction primitives derives a sequence of
DNNs 𝑁1, 𝑁2, · · · , 𝑁𝑘 . Corollary 4.9 guarantees that 𝑁 𝑗 is an over-approximation of 𝑁𝑖 for any
0 ≤ 𝑖 ≤ 𝑗 ≤ 𝑘. Recall that when applying Freeze on some neuron in 𝑁𝑖 , its bounds with respect to
current DNN 𝑁𝑖 is required by definition. Since the structures of DNNs change along the sequence
of DNNs due to abstraction, the bounds in 𝑁𝑖 may be different from those in 𝑁 𝑗 if 𝑖 ≠ 𝑗. To apply
the abstraction primitive Freeze, a naive approach is to calculate the bounds each time before
applying Freeze. The calculation is no doubt a considerable overhead. To mitigate this issue, we
generalize the abstraction primitive Freeze based on the following observation.

Observe that an abstraction primitive applied to layer 𝑖 does not change the values of neurons
in layers 𝑗 < 𝑖, thus their bounds. A more efficient way is to calculate the bounds only once in
the initial network 𝑁0, and all abstraction primitives are applied backwards layer by layer. That
is, an abstraction primitive to layer 𝑖 must be applied after the applications of primitives to layers
𝑗 > 𝑖. This constrained order enables all Freeze primitives to make use of the bounds calculated
in the DNN 𝑁0, thus avoids the considerable calculation overhead, at the cost of flexibility when
performing abstraction. However, it is sometimes too restricted to achieve a good abstraction. To
gain more flexibility while preserving the efficiency, we generalize the primitive Freeze as follows.
+ of Freeze is defined to freeze a neuron w.r.t. a given DNN 𝑀.
+ is parameterized by the given DNN 𝑀. To construct a network ¯𝑁 from
+ works almost the same as Freeze
+ leverages bounds w.r.t. the given DNN 𝑀 instead of 𝑁 . More specifically,

+ on a hidden neuron 𝑣𝑖,𝑗 of 𝑁 , Freeze𝑀

A generalized primitive Freeze𝑀

We emphasize that Freeze𝑀
𝑁 by applying Freeze𝑀
except that Freeze𝑀
Freeze𝑀

+ works as follows:

(1) all incoming edges to 𝑣𝑖,𝑗 are removed, i.e.,

¯𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑗 ) = 0, for each 𝑣𝑖−1,𝑝 in layer 𝑖 − 1;

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

9

(2) the value of the neuron 𝑣𝑖,𝑗 is replaced by a constant as

¯𝑏 (𝑣𝑖,𝑗 ) =

(cid:26) ub
𝑀 (𝑣𝑖,𝑗 ),
𝑀 (𝑣𝑖,𝑗 ),
lb
𝑀 (𝑣𝑖,𝑗 ) and lb

if 𝑣𝑖,𝑗 is inc;
if 𝑣𝑖,𝑗 is dec.
𝑀 (𝑣𝑖,𝑗 ) are used instead of ub

lb

Compared to Freeze, the bounds ub
𝑁 (𝑣𝑖,𝑗 ).
As a result, Lemma 4.6 cannot be generalized to the new primitive Freeze𝑀

+ . That is, the con-
structed ¯𝑁 does not necessarily over-approximate 𝑁 . Nevertheless, we can show that Lemma 4.6
𝑀 (𝑣𝑖,𝑗 )
can be generalized to Freeze𝑀
(resp., lb

𝑀 (𝑣𝑖,𝑗 )) is also an upper (resp., lower) bound of 𝑣𝑖,𝑗 in 𝑁 . Thus, we have:

+ conforms to the definition of Freeze, i.e., ub

+ if Freeze𝑀

𝑁 (𝑣𝑖,𝑗 ) and

Lemma 4.10. Given two DNNs 𝑁 and 𝑀, let ¯𝑁 be the constructed network from 𝑁 by Freeze𝑀

𝑀 (𝑣) (resp., lower bound lb

+ on
𝑀 (𝑣))

neuron 𝑣. ¯𝑁 (𝒙) ≥ 𝑁 (𝒙) holds for each input 𝒙, if the upper bound ub
is also an upper (resp., lower) bound of 𝑣 in 𝑁 .

Given a DNN 𝑀, Freeze𝑀

+ is called a quasi-abstraction primitive w.r.t. 𝑀, where the prefix "quasi"
indicates that it is not yet ready to be used as an abstraction primitive, since it does not have a
strong property like Lemma 4.6.

To leverage Freeze𝑀

+ in the abstraction procedure, we define an abstraction step as the instance
of applying an abstraction primitive on one or two specific neurons. Abstraction steps thus relate
abstraction primitives to their target neurons. We denote abstraction steps by abstract steps.
Similarly, we have Merge, Freeze and Freeze𝑀
+ steps
together as quasi-abstraction steps w.r.t. 𝑀, denoted by q-abstract𝑀 steps. The q-abstract𝑀
steps have the following important property:

+ steps. We refer to Merge and Freeze𝑀

Lemma 4.11. Given two DNNs 𝑁 and 𝑀, let ¯𝑁 be the network constructed from 𝑁 via iteratively
applying the sequence 𝜏 of q-abstract𝑀 steps. A permutation 𝜏 ′ of 𝜏 can be constructed such that

+ step precedes all Merge steps in 𝜏 ′;

(i) each Freeze𝑀
(ii) all Freeze𝑀
+ steps in 𝜏 ′ are applied backwards layer by layer;
(iii) the subsequence of Merge steps in 𝜏 ′ is identical to that in 𝜏;
(iv) the network ¯𝑁 ′ obtained by applying 𝜏 ′ on 𝑁 is identical to the network ¯𝑁 .

We call 𝜏 ′ an implicit order or implicit sequence of 𝜏.

Lemma 4.11 indicates that a series of q-abstract𝑀 steps can be reordered, such that all Freeze𝑀
+
steps are firstly performed backwards layer by layer, and then the Merge steps are performed.
Moreover, the reordering does not change the resulting network. Selecting the initial target DNN
𝑁 as 𝑀 used for the generalized primitive Freeze𝑀
+ step in the implicit sequence
builds an over-approximation by Lemma 4.10. The following theorem justifies that q-abstract𝑁
steps, therefore Freeze𝑁

+ , can be leveraged to perform abstraction:

+ , each Freeze𝑁

Theorem 4.12. Let ¯𝑁 be the DNN constructed from 𝑁 by a series of q-abstract𝑁 steps. It holds

that ¯𝑁 (𝒙) ≥ 𝑁 (𝒙) for each input 𝒙.

To abstract a given DNN 𝑁 , Theorem 4.12 guarantees that we only need to calculate the bounds of
neurons once on the initial 𝑁 . The quasi-abstraction primitive Freeze𝑁
+ can be used for abstraction
instead of Freeze. In the abstraction procedure, primitives Merge and Freeze𝑁
+ can be applied to any
neuron in any order to construct over-approximations. The soundness still holds by Theorem 4.12.
In the rest of paper, since we will always use the initial network 𝑁 for the generalized primitive
+ and the calculation of bounds, for the sake of simplicity, we use the notation Freeze+ for
+ . And when referring to abstraction steps/primitives, we now mean Merge and Freeze+,

Freeze𝑁
Freeze𝑁
thanks to Theorem 4.12.

10

4.4 Abstraction Strategy

Liu et al.

Having only abstraction primitives is not enough to accomplish the abstraction procedure due
to challenge C2, i.e., reducing the network size as much as possible while preserving accuracy as
much as possible. To build an initial abstraction addressing challenge C2, two questions have to be
answered: what should be done for a single abstraction step and how many steps should be performed?
The objective for the first question is to introduce less inaccuracy at each abstraction step, thus
admitting more abstraction steps and reducing the size more, when sacrificing the same accuracy.
For this purpose, we propose to first locate a less important neuron and then abstract it, where a
neuron is less important if it contributes less to the network’s output. We measure the importance
of neurons by their values, where the value 𝑉 (𝑣𝑖,𝑗 ) of a hidden neuron 𝑣𝑖,𝑗 is estimated using the
mean of its upper and lower bounds, i.e.,

𝑉 (𝑣𝑖,𝑗 ) =

1
2

(ub (𝑣𝑖,𝑗 ) + lb (𝑣𝑖,𝑗 )).

After a neuron 𝑣𝑖,𝑗 with the minimum estimated value 𝑉 (𝑣𝑖,𝑗 ) is found, we need decide which
abstraction primitive–Merge or Freeze+–should be applied to minimize the loss of accuracy.
Therefore, we measure the loss of accuracy induced by abstraction primitives, based on which
abstraction primitive is chosen.

The loss 𝐿f (𝑣𝑖,𝑗 ) of accuracy by applying Freeze+ on a neuron 𝑣𝑖,𝑗 is measured by the difference

between the estimated value 𝑉 (𝑣𝑖,𝑗 ) of 𝑣𝑖,𝑗 and the constant ¯𝑏 (𝑣𝑖,𝑗 ) used for freezing 𝑣𝑖,𝑗 :

𝐿f(𝑣𝑖,𝑗 ) = | ¯𝑏 (𝑣𝑖,𝑗 ) − 𝑉 (𝑣𝑖,𝑗 )|.

As applying Merge on two neurons 𝑣𝑖,𝑗 and 𝑣𝑖,𝑘 produces an abstract neuron 𝑣𝑖,𝑡 , the loss
𝐿m (𝑣𝑖,𝑗, 𝑣𝑖,𝑘 ) of accuracy by applying Merge on 𝑣𝑖,𝑗 and 𝑣𝑖,𝑘 should depend on the changes on
weights as well as the values of both neurons. Thus, a linear combination of the estimated values is
used to estimate 𝐿m(𝑣𝑖,𝑗, 𝑣𝑖,𝑘 ):

𝐿m (𝑣𝑖,𝑗, 𝑣𝑖,𝑘 ) = 𝑅(𝑣𝑖,𝑗, 𝑣𝑖,𝑡 ) · 𝑉 (𝑣𝑖,𝑗 ) + 𝑅(𝑣𝑖,𝑘, 𝑣𝑖,𝑡 ) · 𝑉 (𝑣𝑖,𝑘 )

where each coefficient 𝑅(·) is a ratio characterizing changes on the weights of incoming edges to
𝑣𝑖,𝑡 :

𝑅(𝑣𝑖,𝑗, 𝑣𝑖,𝑡 ) =

(cid:205)𝑝 | ¯𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑡 ) − 𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑗 )|
(cid:205)𝑝 |𝑤 (𝑣𝑖−1,𝑝, 𝑣𝑖,𝑗 )|

.

Based on the above loss measurements, we propose a value-guided abstraction strategy, described
in Algorithm 2, to synergistically apply abstraction primitives. It determines which abstraction
step should be chosen at the current iteration. It starts by selecting the neuron with the minimum
estimated value 𝑉 (𝑣𝑖,𝑗 ) as the target neuron (line 1). Then it chooses among all available abstraction
steps the one that loses the least accuracy (lines 3–7). At the end, this optimum abstraction step is
applied to the network 𝑁 resulting in an over-approximation ¯𝑁 (line 8). Furthermore, the mapping
𝑉 (·) is updated accordingly for next use.

For the second question in generating the initial abstraction, we adopt the terminating condition
proposed in [Elboher et al. 2020]. A set 𝑋 of inputs satisfying the input property 𝑃 is sampled
before applying abstraction. The Abstract procedure iteratively applies abstraction steps to build a
sequence of over-approximations 𝑁1, 𝑁2, · · · , 𝑁𝑘 according to the value-guided abstraction strategy
(i.e., Algorithm 2) until 𝑁𝑘 (𝒙) for some input 𝒙 ∈ 𝑋 violates the output property 𝑄.

Remark that the value-guided abstraction strategy together with the terminating condition is a

trade-off for the challenge C2.

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

11

Algorithm 2 Value-Guided Abstraction Strategy
Input: A DNN 𝑁 , a mapping 𝑉 estimating values
Output: An over-approximation ¯𝑁 of 𝑁 , updated 𝑉

minLoss ← 𝐿m (𝑣𝑖,𝑗, 𝑣𝑖,𝑘 ), bestStep ← Merge(𝑣𝑖,𝑗, 𝑣𝑖,𝑘 )

if 𝐿m (𝑣𝑖,𝑗, 𝑣𝑖,𝑘 ) < minLoss then

1: Select 𝑣𝑖,𝑗 with the minimum estimated value 𝑉 (𝑣𝑖,𝑗 )
2: minLoss ← ∞, bestStep ← ⊥
3: for each hidden neuron 𝑣𝑖,𝑘 of same label as 𝑣𝑖,𝑗 do
4:
5:
6: if 𝐿f (𝑣𝑖,𝑗 ) < minLoss then
7:
8: Apply bestStep on 𝑁 to construct ¯𝑁
9: Update 𝑉 according to bestStep
10: return ¯𝑁 , 𝑉

minLoss ← 𝐿f(𝑣𝑖,𝑗 ), bestStep ← Freeze+ (𝑣𝑖,𝑗 )

5 NETWORK REFINEMENT

In this section, we present our refinement procedure Refine.

5.1 Refinement Primitives
Given a verification problem ⟨𝑁 , 𝑃, 𝑄⟩ and a counterexample cex of ¯𝑁 which is spurious on 𝑁 , to
address challenge C3, the goal of refining an over-approximation ¯𝑁 of the DNN 𝑁 is defined to
construct a network ¯𝑁 ′ satisfying the following condition:

¯𝑁 (𝒙) ≥ ¯𝑁 ′(𝒙) ≥ 𝑁 (𝒙) for each 𝒙 and ¯𝑁 ′(cex) satisfies 𝑄.
That means refinement primitives should construct a DNN ¯𝑁 ′ over-approximating 𝑁 , while ¯𝑁 ′ is
itself over-approximated by ¯𝑁 to exclude the spurious counterexample cex of 𝑁 .

We define refinement primitives as the inverses of abstraction primitives. Particularly, refinement
steps, the instances of refinement primitives, are the inverses of abstraction steps. Therefore,
we introduce two refinement primitives: Split and Recover, corresponding to the abstraction
primitives Merge and Freeze+, respectively. The Split primitive splits an abstract neuron back into
two neurons that were merged by Merge during abstraction and the Recover primitive recovers a
constant neuron into the status before Freeze+.

Now the question is, can we freely choose any abstract or constant neuron in ¯𝑁 to apply a refinement
step without damaging soundness (i.e., challenge C3), like what we do when applying an abstraction
step? Unfortunately, the answer is negative.

Example 5.1. Let us consider the DNN 𝑁0 shown in Figure 3, where all neurons are inc. Assume
all biases are 0. By iteratively applying the abstraction steps Merge(𝑣3, 𝑣4) and Merge(𝑣1, 𝑣2) (resp.
Merge(𝑣1, 𝑣2) and Merge(𝑣3, 𝑣4)) on 𝑁0, we obtain the DNN ¯𝑁2 (resp. ¯𝑁4) as depicted. It is easy to
see that ¯𝑁2 and ¯𝑁4 are different.

Example 5.1 shows that different orders of abstraction steps may result in different over-
approximations. Conversely, to reverse the effects of the abstraction steps, the refinement steps
must follow some specific order as well.

Recall that given a sequence 𝜏 of abstraction steps, Lemma 4.11 enables it to be permutated into
an implicit sequence 𝜏 ′. When applied, 𝜏 ′ produces the same over-approximation as 𝜏. Nevertheless,
Example 5.1 indicates that the order of refinement steps should be constrained by the order of
abstraction steps. This motivates us to introduce a dependency relation between abstraction steps,
which restricts the order of refinement steps.

12

Liu et al.

1

1

3
1
1

2

𝑣1

𝑣2

(𝑁0)

2

𝑦

𝑣3

𝑣4

𝑥1

1

1

𝑥1

𝑣3

𝑣4

Merge(𝑣1, 𝑣2)

2

𝑦

1

𝑥1

𝑣3

𝑣4

5

( ¯𝑁2)

Merge(𝑣3, 𝑣4)

3

2

( ¯𝑁1)

𝑥1

1

1

1

𝑥1

𝑣1

𝑣2

𝑣1

𝑣2

1

𝑦

𝑣1

𝑣2

𝑣1

𝑣2

Merge(𝑣1, 𝑣2)

𝑣3

𝑣4

1

1

𝑦

4

3

( ¯𝑁3)

Merge(𝑣3, 𝑣4)

2

𝑦

𝑣3

𝑣4

4

( ¯𝑁4)

Fig. 3. Different Orders of Applying Same Merge Steps

A straightforward dependency relation can be established on the implicit sequence: each step
depends on all steps applied before it. Based on this dependency, refinement steps can be performed
backwards along the implicit sequence of abstraction steps, ensuring soundness by Lemmas 4.4
and 4.10. We define the following dependency relation which provides more flexibility for refine-
ment.

Definition 5.2. Given an implicit sequence of abstraction steps 𝜏 = 𝛽1𝛽2 · · · 𝛽𝑚. The dependency

relation is defined as follows:

(i) each step 𝛽𝑖 applied in layer 𝑘𝑖 depends on the Freeze+ step 𝛽 𝑗 that happens in layer 𝑘 𝑗 , if

𝑗 < 𝑖 and 𝑘 𝑗 > 𝑘𝑖 ;

(ii) the Merge step 𝛽𝑖 that merges 𝑣 and 𝑣 ′ in layer 𝑘, depends on (a) all Merge steps producing

𝑣 and 𝑣 ′ (if any), and (b) all Merge steps 𝛽 𝑗 at both layers 𝑘 − 1 and 𝑘 + 1 if 𝑗 < 𝑖.

Intuitively, Item (i) ensures that all Freeze+ steps at layer 𝑘 𝑗 happen before any abstraction step
at layer 𝑘𝑖 < 𝑘 𝑗 , so that Lemma 4.10 is applicable. Item (ii) guarantees well-definedness of the
Merge step on 𝑣 and 𝑣 ′, because it requires information provided by both Merge steps producing
𝑣 and 𝑣 ′ as well as all Merge steps that happened at adjacent layers.

A dependency graph G of 𝜏 is a directed acyclic graph derived by the dependency relation, where
the vertices are abstraction steps and an edge from 𝛽𝑖 to 𝛽 𝑗 exists if 𝛽𝑖 depends on 𝛽 𝑗 . Thus, we get:

Theorem 5.3. Let ¯𝑁 be the DNN constructed from 𝑁 by applying a sequence 𝜏 of abstraction steps,
G the dependency graph of the implicit order 𝜏 ′ for 𝜏, and ¯𝑁 ′ the DNN refined from ¯𝑁 by a refinement
step 𝛾. It holds that ¯𝑁 (𝒙) ≥ ¯𝑁 ′(𝒙) ≥ 𝑁 (𝒙) for each input 𝒙, if 𝛾 is the inverse of an abstraction step
𝛽 in G that has no incoming edges.

In summary, to refine an over-approximation ¯𝑁 , we first construct the implicit sequence 𝜏 ′ and
the corresponding dependency graph G, respectively. We pick any abstraction step in G with no
incoming edges, i.e. not depended by others, and perform a corresponding refinement step. By
Theorem 5.3, the refined DNN ¯𝑁 ′ in such a way ensures soundness, thus, partially solving challenge
C3.

Example 5.4. Recall Example 5.1 for the left part in Figure 3. ¯𝑁2 is obtained from 𝑁0 by sequence
𝜏 = [Merge(𝑣3, 𝑣4), Merge(𝑣1, 𝑣2)], whose implicit sequence 𝜏 ′ = 𝜏 by Lemma 4.11. The corre-
sponding dependency graph G contains only two vertices Merge(𝑣3, 𝑣4) and Merge(𝑣1, 𝑣2) and an

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

13

Algorithm 3 Counterexample-Guided Refinement Strategy
Input: 𝑁 , abstraction ¯𝑁 , dependency graph G, counterexample cex
Output: Refined network ¯𝑁 ′, updated G

if 𝑃cex(𝑠) > maxGain then

1: Extract the set 𝐶 of applied abstraction steps from G
2: maxGain ← 0, bestStep ← ⊥
3: for each candidate step 𝑠 in 𝐶 do
4:
5:
6: Build ¯𝑁 ′ from ¯𝑁 by applying the refinement step bestStep
7: Update dependency graph G
8: return ¯𝑁 ′, G

maxGain ← 𝑃cex(𝑠), bestStep ← refinement step for 𝑠

edge from Merge(𝑣1, 𝑣2) to Merge(𝑣3, 𝑣4). To soundly refine ¯𝑁2, only Merge(𝑣1, 𝑣2) can be picked
to reverse.

5.2 Counterexample-Guided Refinement

Similar as the abstraction procedure, to address challenge C4, the following two questions should
be considered in the refinement procedure: which available refinement step should be chosen, and
how many steps should be performed?

According to Theorem 5.3, any abstraction step in the dependency graph without incoming
edges can be chosen for refinement. Thus, there may exist several applicable refinement steps
during an iteration of the refinement procedure. Contrary to the abstraction strategy, we expect a
refinement step to restore more accuracy, which corresponds to the candidate abstraction step that
induced more inaccuracy. Therefore, we estimate the gain of accuracy for reversing a candidate
abstraction step 𝑠 via the following profit function 𝑃cex(·) w.r.t. the spurious counterexample cex,

𝑃cex (𝑠) =

(cid:26) | (cid:205)𝑘 𝑣 (𝑘) (cex) − ¯𝑣 (cex)|, if 𝑠 is Merge on 𝑣 (𝑘) ’s producing ¯𝑣;
|𝑣 (cex) − ¯𝑏 ( ¯𝑣)|,

if 𝑠 is Freeze+ on 𝑣 producing ¯𝑣;

where 𝑣 (𝑘) ’s denote the atomic neurons merged to ¯𝑣, 𝑣 (cex) denotes the exact value of a neuron 𝑣
under the input cex.

Based on 𝑃cex(·), we propose a counterexample-guided refinement strategy (cf. Algorithm 3),

which chooses a candidate refinement step, by which we can regain the most accuracy.

However, the strategy in Algorithm 3 does not guarantee that the counterexample cex is ruled
out by a single refinement step, i.e., cex is still a counterexample of the refined DNN ¯𝑁 ′. To solve
the second question for the refinement procedure, the routine in Algorithm 3 is repeated until cex
is ruled out from the refined DNN.

The counterexample-guided refinement strategy, coupled with the terminating condition and

refinement primitives, solves challenges C3 and C4.

6 EVALUATION

We implement our approach in a tool NARv (Network Abstraction-Refinement for verification).
NARv utilizes the promising symbolic interval analysis tool ReluVal [Wang et al. 2018] for
computing the bounds of neurons, while the back-end verification engine can be configured with
any sound tool that can produce a counterexample when the verification problem does not hold.
To evaluate NARv, two promising tools Marabou [Katz et al. 2019] and Planet [Ehlers 2017] are
integrated as the back-end verification engines. Both Marabou and Planet are sound and complete,
thus NARv is sound and complete. Marabou is chosen as the back-end verification engine, because

14

Liu et al.

it is the most efficient tool among 13 tools participating in the 2nd International Verification of
Neural Networks Competition (VNN-COMP’21) [vnn 2021] (cf. Table 5 in the summary and results
of VNN-COMP’21 [Bak et al. 2021]). Planet is comparable to Marabou (even better on some
benchmarks) [Katz et al. 2019].

The experiments are designed to answer the following research questions:

RQ1: Effectiveness. Can NARv boost the sound and complete verification tools Marabou and

Planet?

RQ2: Performance. Does NARv outperform CEGAR-NN [Elboher et al. 2020], the only work that

supports structure-oriented CEGAR?

We evaluate NARv on three widely-used benchmarks and datasets: the DNNs from ACAS Xu [Julian
et al. 2019], the DNNs trained by datasets MNIST [LeCun 1998] and CIFAR-10 [Krizhevsky et al.
2009]. The DNNs trained by MNIST and CIFAR-10 are relatively large. Not all of complete methods
are effective to solve useful verification problems in acceptable time for them [Urban and Miné
2021].

ACAS Xu, is a collision avoidance system built for unmanned aircrafts. The system consists of 45
real-world DNNs, each of which has 310 neurons including 5 inputs, 6 hidden layers and 5 outputs.
The inputs take normalized data from airborne sensors, representing the relative position and
speed of intruders. The outputs provide 5 kinds of turning advisories to prevent the aircraft from
collision.

MNIST, is a standard dataset for handwritten digit recognition. The DNN used in our evaluation is
provided by VNN-COMP’21. It contains 1306 neurons, including 2 hidden layers with 256 neurons
per layer. Due to the 28×28 format of the images, the input layer takes 784 pixels in greyscale and
the output layer has 10 neurons producing the classification scores for the 10 possible digits.

CIFAR-10, is a colored image dataset that consists of 60000, 32×32 RGB images in 10 classes
(e.g., cat or dog). The DNN used in our evaluation is collected from the benchmarks of the ERAN
toolset [Lab 2022]. The size of its hidden layers is 6 × 100. The DNN has 3072 input neurons
representing the pixel values of the 3 color channels and 10 outputs as the classification results.
The total size is 3682.

The properties to be verified are the robustness of DNNs against adversarial examples. We verify
whether the classification result for each input on a target DNN remains the same after adding
small perturbations onto that input, where the perturbations are limited within a given threshold 𝛿
using the 𝐿∞ norm [Carlini and Wagner 2017].

All experiments are run on a Linux server with two Intel Xeon Sliver-4214 CPUs and 64 GB
memory. The timeout is set to 1 hour for each ACAS Xu verification problem as in VNN-COMP’21,
whereas 10 hours for each verification problem on MNIST and CIFAR-10 DNNs.

The experimental results show that our approach is able to significantly boost the performance
and scalability of both Marabou and Planet and significantly outperforms CEGAR-NN, the only
work that supports structure-oriented CEGAR for DNN verification.

6.1 RQ1: Effectiveness Evaluation

Setup. To answer RQ1, we evaluate the effectiveness of NARv on the relatively large networks
trained by MNIST and CIFAR-10. Two NARv configurations NARv[M] and NARv[P] are set up
respectively with Marabou and Planet as their back-end verification engines. NARv[M] and
NARv[P] are then compared with Marabou and Planet, respectively, on both the MNIST and the
CIFAR-10 networks. For each network, we verify robustness against 4 perturbation thresholds
𝛿 ranging: from 0.02 to 0.05 for the MNIST network, and from 0.001 to 0.004 for the CIFAR-10
network. These thresholds are selected since the robustness threshold is close to 0.06 for the MNIST

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

15

Table 1. Effectiveness Evaluation of our Approach on Marabou and Planet

Avg. time (s)

#Verified

Avg. time (s)

#Verified

Dataset

MNIST

CIFAR-10

Dataset

MNIST

CIFAR-10

𝛿
0.02
0.03
0.04
0.05
0.001
0.002
0.003
0.004
𝛿
0.02
0.03
0.04
0.05
0.001
0.002
0.003
0.004

NARv[M]

(△ 65.0%)
99.54
(▼ 86.3%)
971.70
2458.05 (▼ 85.6%)
8056.73 (▼ 71.7%)
1581.66 (▼ 82.1%)
12545.33 (▼ 56.0%)
15241.70 (▼ 57.7%)
15200.91 (▼ 57.8%)

NARv[P]

(▼ 67.5%)
118.91
(▼ 78.0%)
259.54
(▼ 60.3%)
454.14
(▼ 54.1%)
585.59
12682.02 (▼ 63.4%)
19097.04 (▼ 47.0%)
15249.89 (▼ 57.6%)
21187.41 (▼ 41.1%)

25
( - )
25 (▲ 12%)
25 (▲ 40%)
20 (▲ 52%)
12
( - )
8 (▲ 33%)
7 (▲ 58%)
7 (▲ 58%)

( - )
25
( - )
25
( - )
25
25
( - )
8 (▲ 58%)
7 (▲ 58%)
3 (▲ 25%)
3 (▲ 25%)

Marabou
60.34
7084.99
17063.42
28465.46
8824.25
28515.69
36000.00
36000.00

Planet

366.39
1182.32
1143.16
1275.57
34641.92
36000.00
36000.00
36000.00

25
22
15
7
12
4
0
0

25
25
25
25
1
0
0
0

network, and 0.005 for CIFAR-10 network, thus yielding interesting yet hard robustness verification
problems. There are 25 and 12 verification problems for each perturbation threshold 𝛿 respectively
for MNIST and CIFAR-10.
Results on the MNIST network. Table 1 (upper part) reports the results on the MNIST network,
where the average verification time in seconds ("Avg. time") and the numbers of successfully verified
problems ("#Verified") by each tool are depicted.

We can observe that NARv[M] and NARv[P] can solve more problems and in general are much
faster than Marabou and Planet, in particular on hard verification problems. This indicates that
our approach is able to significantly boost the performance of both Marabou and Planet in most
cases on the MNIST network. The best improvements appear when 𝛿 = 0.03, reducing 86.3% and
78.0% verification time respectively for Marabou and Planet.

In detail, Marabou appears to have a good performance when the perturbation threshold 𝛿 is
small, as the solving space of the verification problem is small. It begins to time out (10 hours) when
𝛿 ≥ 0.03, and the numbers of solved problems become less and less with the increase of 𝛿. When
𝛿 = 0.05, it only successfully solves 7 (28%) problems out of all the 25 verification problems. In
contrast, NARv[M] is able to solve most of the problems for all perturbation thresholds except for
𝛿 = 0.05. When 𝛿 = 0.05, it solves 20 problems, namely 52% (= (20 − 7)/25) more than Marabou.
In addition, NARv[M] spends 71.7% (≈ (28465.46 − 8056.73)/28465.46) less verification time on
average when 𝛿 = 0.05.

Compared over Planet which is able to solve all the MNIST verification problems as depicted in
Table 1, NARv[P] also solves all the problems, meanwhile spends less time on average for all the
perturbation thresholds, reducing at least 54.1% verification time.

Finally, we should emphasize that verified robustness with large perturbation threshold 𝛿 is more

interesting in practice.
Results on the CIFAR-10 network. The experimental results on the CIFAR-10 network are de-
picted in Figure 4, where Figure 4(a) (resp. Figure 4(b)) compares the verification time required

16

Liu et al.

(a) NARv[M] vs. Marabou

(b) NARv[P] vs. Planet

Fig. 4. Verification Time on CIFAR-10 Network (logscale), with Marabou, Planet and their Boosted Versions

by NARv[M] and Marabou (resp. NARv[P] and Planet) for each verification problem, and the
x-axis and y-axis refer to the time (in seconds, logscale) spent by NARv[M] and Marabou (resp.
NARv[P] and Planet). The blue dots represent the solved problems and the red crosses on the top
and right borders are the cases where Marabou (resp. Planet) and NARv[M] (resp. NARv[P])
run out of time. Therefore, the dots and crosses distributed above the green line indicate the cases
where NARv[M] (resp. NARv[P]) is faster.

It is easy to observe that most of the dots and crosses are situated strictly above the green line
in both Figures 4(a) and 4(b), indicating that our approach can significantly boost Marabou and
Planet in most cases. We should point out that Planet can hardly solve any verification problems
in the CIFAR-10 network. Nevertheless, boosted by our approach, NARv[P] is still able to solve
many verification problems within the time limit (10 hours).

Table 1 (lower part) gives the detailed results. It shows that Planet fails to solve any problems
before timing out when 𝛿 ≥ 0.002. It successfully solves only 1 (2.1%) problem out of all 48
problems for 4 different perturbation thresholds. Boosted by our approach, NARv[P] is able to
solve 7 (58%) and 3 (25%) problems respectively when 𝛿 = 0.002 and 𝛿 ≥ 0.003, succeeding in 41.7%
(≈ (8 + 7 + 3 + 3 − 1)/48) more in total. On the other hand, NARv[M] is able to solve 37.5% more
problems in total than Marabou.

Answer to RQ1: NARv can significantly boost the performance of two promising tools
Marabou and Planet.

6.2 RQ2: Performance Evaluation

Setup. To answer RQ2, we compare NARv over CEGAR-NN, both of which use the same back-end
verification engine Marabou. CEGAR-NN provides two abstraction strategies, named indicator-
guided abstraction and abstraction to saturation, where the former iteratively merges two neurons
until some pre-sampled input violates the given property and the latter aggressively and iteratively
merges two neurons producing the smallest over-approximation. We refer to CEGAR-NN with those
abstraction strategies as CEGAR-NN[I] and CEGAR-NN[S], respectively. NARv is then compared
with both of them using the same benchmark ACAS Xu as adopted in [Elboher et al. 2020]. The
perturbation threshold 𝛿 is set from 0.01 to 0.04.
Results. Figure 5 depicts the comparison of the verification time between NARv and CEGAR-NN[I]
for each problem. We observe that most of the dots and crosses are located above the green
line for all perturbation thresholds, indicating that NARv is significantly more efficient than
CEGAR-NN[I] on most verification problems. The comparison of the verification time between
NARv and CEGAR-NN[S] is illustrated in Figure 6, where a similar conclusion can be drawn.

100101102103104100101102103104 finished timeout   y=xMarabou (sec)NARv[M] (sec)100101102103104100101102103104 finished timeout   y=xPlanet (sec)NARv[P] (sec)Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

17

(a) 𝛿 = 0.01

(b) 𝛿 = 0.02

(c) 𝛿 = 0.03

(d) 𝛿 = 0.04

Fig. 5. Comparison to CEGAR-NN[I] on ACAS Xu (logscale)

(a) 𝛿 = 0.01

(b) 𝛿 = 0.02

(c) 𝛿 = 0.03

(d) 𝛿 = 0.04

Fig. 6. Comparison to CEGAR-NN[S] on ACAS Xu (logscale)

Table 2 reports the details of all the results, where columns ("Avg. time") and ("#Verified") are the
same as above, and columns ("Avg. size") give the average sizes of hidden neurons in the abstract
networks when the tools successfully verify the problems.

 finished timeout   y=xCEGAR-NN[I] (sec)NARv (sec) finished timeout   y=xCEGAR-NN[I] (sec)NARv (sec) finished timeout   y=xCEGAR-NN[I] (sec)NARv (sec) finished timeout   y=xCEGAR-NN[I] (sec)NARv (sec) finished timeout   y=xCEGAR-NN[S] (sec)NARv (sec) finished timeout   y=xCEGAR-NN[S] (sec)NARv (sec) finished timeout   y=xCEGAR-NN[S] (sec)NARv (sec) finished timeout   y=xCEGAR-NN[S] (sec)NARv (sec)18

Liu et al.

Table 2. Abstraction and Verification Results for ACAS Xu Networks

𝛿
0.01
0.02
0.03
0.04
𝛿
0.01
0.02
0.03
0.04
𝛿
0.01
0.02
0.03
0.04

NARv

49.82
88.10
105.04
169.46

888 (98.7%)
881 (97.9%)
893 (99.2%)
886 (98.4%)

271 (▼ 10%)
210 (▼ 30%)
212 (▼ 29%)
229 (▼ 24%)

CEGAR-NN[I]
Avg. time (s)
(12.6×)
629.16
1184.92 (13.4×)
2331.45 (22.2×)
2742.36 (16.2×)

#Verified

892
815
497
327

860
882
887
884

(99.1%)
(90.6%)
(55.2%)
(36.3%)

Avg. size

(△ 187%)
(△ 194%)
(△ 196%)
(△ 195%)

CEGAR-NN[S]

1373.35 (27.6×)
1980.36 (22.5×)
2835.72 (27.0×)
3034.63 (17.9×)

884
731
368
233

828
824
784
684

(98.2%)
(81.2%)
(40.9%)
(25.9%)

(△ 176%)
(△ 175%)
(△ 161%)
(△ 128%)

Table 2 shows that NARv is much faster than both CEGAR-NN[I] and CEGAR-NN[S]. Partic-
ularly, when 𝛿 = 0.01, NARv is on average 11.6 (≈ (629.16 − 49.82)/49.82) times faster than
CEGAR-NN[I], and 26.6 (≈ (1373.35 − 49.82)/49.82) times faster than CEGAR-NN[S]. For the
network sizes, recall that there are 300 hidden neurons in each given ACAS Xu network. NARv
succeeds in reducing the sizes of hidden neurons for all perturbation thresholds, the best reduction
being 30% (= (300 − 210)/300) on average when 𝛿 = 0.02. However, neither of the two abstraction
strategies of CEGAR-NN has progress in size reduction. One of the main reasons is that CEGAR-NN
preprocesses the target DNN by first quadrupling the size, leading to a heavy load for the abstraction
procedure. As a structure-oriented abstraction approach, the abstraction of CEGAR-NN appears
less practical. A well-designed abstraction procedure leads to a better verification performance.
As a result, NARv is able to solve the most problems in a reasonable time among the three tools
when 𝛿 ≥ 0.02, and is comparable to the other two when 𝛿 = 0.01. In particular, NARv successfully
verifies 72.6% (≈ (886 − 233)/900) more problems than CEGAR-NN[S] when 𝛿 = 0.04, spending
only 5.6% (≈ 169.46/3034.63) of time on average.

Answer to RQ2: NARv significantly outperforms CEGAR-NN, the only work that supports
structure-oriented CEGAR.

6.3 Threats to Validity

Our approach is designed for fully connected feedforward deep neural networks with the ReLU
activation function. It could cope with any monotonic activation functions such as sigmoid and
tanh [Paulsen and Wang 2022], but we have not yet evaluated the effectiveness. There are deep neural
networks such as convolutional and recurrent ones. The former could be verified by equivalently
transforming into fully connected ones [Ma and Lu 2017], but utilizing the particular properties of
convolutional constructs to build abstraction is certainly an interesting future work. The feasibility
has been shown in the concurrent work [Ostrovsky et al. 2022]. However, it remains an open
problem for recurrent neural networks.

The verification engines Marabou and Planet adopted in the experiments are primarily based
on SMT solving and thus complete but computational expensive. Our structure-oriented CEGAR
approach is proposed to boost them. We have not evaluated with the abstract interpretation

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

19

approaches, which can be seen as computation-oriented abstraction, thus are incomplete, e.g., [Gehr
et al. 2018; Singh et al. 2019]. Although refinement techniques also have been proposed, e.g., input
refinement [Wang et al. 2018], such computation-oriented abstraction-refinement frameworks are
orthogonal to our structure-oriented one. Investigating the synergy between them is left for future
work, for instance, using them at a back-end verification engine in our approach. To reduce this
threat, we use the most efficient one in VNN-COMP’21 (i.e., Marabou) and another comparable
tool Planet.

Our approach may fail to boost DNN verification when the verification problems are very easy.
More effective heuristics should be added to improve the efficiency for such verification problems.
For instance, abstract interpretation based approaches could be leveraged before applying our
approach. Nevertheless, our approach can significantly boost the verification of hard problems
which are arguably more interesting and important in practice.

7 RELATED WORK

A large and growing body of work studies heuristic search or other dynamic analysis techniques
to test robustness of neural networks, e.g., [Carlini and Wagner 2017; Ma et al. 2018a,b; Pei et al.
2019; Sun et al. 2018; Tian et al. 2018], cf. [Zhang et al. 2022] for a survey. They are often effective
in finding adversarial examples or violations of properties, as opposed to proving the absence of
violations. Thus, they are orthogonal to formal verification considered in this work.

The simple and earlier neural network verification makes use of constraint solving, where a
verification problem is reduced to the solving of constraints, e.g., SAT/SMT solving [Ehlers 2017;
Huang et al. 2017; Katz et al. 2017, 2019; Pulina and Tacchella 2010], LP/MILP solving [Dutta et al.
2018; Lin et al. 2019; Lomuscio and Maganti 2017; Tjeng and Tedrake 2017; Wong and Kolter 2018].
Most of these works focus on DNNs with the ReLU activation function. Although these techniques
are often sound and complete, they are limited in scalability. Our work targets this scalability
problem and boosts existing DNN verification techniques by reducing network sizes.

To improve the scalability, some other DNN verification techniques utilize the idea of abstraction
through abstract interpretation [Cousot and Cousot 1977]. These techniques include AI2 [Gehr
et al. 2018], DeepZ [Singh et al. 2018], DeepPoly [Singh et al. 2019], ReluVal [Wang et al. 2018],
NNV [Tran et al. 2019], DeepSRGR [Yang et al. 2021] and so on. The key idea is to use well-designed
numerical abstract domains, such as boxes [Cousot and Cousot 1977], zonotopes [Ghorbal et al.
2009] and polyhedra [Cousot and Halbwachs 1978], to over-approximate the computations on sets
of inputs in the target DNNs. Due to the over-approximation, these techniques are incomplete.
To improve accuracy, some of them incorporate refinement strategies, such as input region split-
ting [Wang et al. 2018] and output bound tightening [Singh et al. 2019; Yang et al. 2021]. Different
from them, the CEGAR-based approach adopted in this work is based on the structure of DNNs,
rather than on the computations, thus is orthogonal to them.

There are other structure-oriented techniques. The abstraction technique proposed in [Prabhakar
and Afzal 2019] represents the network weights via intervals and merges randomly chosen neurons
by merging intervals. A k-means clustering algorithm is leveraged to cluster neurons [Ashok et al.
2020]. In contrast to ours, they rely on specific underlying verification engines and do not provide
refinement mechanisms when some spurious counterexamples are reported, thus are incomplete.
Gokulanathan et al. [Gokulanathan et al. 2020] proposed to identify and remove inactive neurons
via DNN verification solving without harming accuracy. But it has to invoke a large number of
DNN verification queries.

The closest work to ours is CEGAR-NN [Elboher et al. 2020] which proposed the first structure-
oriented CEGAR approach for DNN verification. Inspired by CEGAR-NN, although the high-level
CEGAR framework is the same, our work makes three significant technical contributions: (i) our

20

Liu et al.

preprocessing at most doubles the network size, whilst CEGAR-NN at most quadruples the size,
leaving a heavy load to the following abstraction procedure as shown in our experimental results;
(ii) we provide two complementary abstraction (resp. refinement) primitives that can induce less
inaccuracy (resp. regain more accuracy) whereas CEGAR-NN only provides one abstraction (resp.
refinement) primitive; (iii) we propose a novel abstraction (resp. refinement) strategy to syncretize
abstraction (resp. refinement) steps, achieving significantly better performance than CEGAR-NN.
Finally, we remark that any sound tool with the ability to produce counterexamples when

properties are violated could be used as the back-end verification engine in our approach.

8 CONCLUSION

We have presented a novel CEGAR-based approach for scalable and exact verification of neural
networks. Specifically, we have proposed two structure-oriented abstraction primitives and an
abstraction strategy to syncretize abstraction steps, resulting in a novel abstraction procedure.
We also have proposed two corresponding refinement primitives and a refinement strategy to
syncretize refinement steps, resulting in a novel refinement procedure. We have implemented
our approach in a tool and conducted an extensive evaluation on three widely-used benchmarks.
The results demonstrate that our approach can significantly boost the scalability and efficiency
of two promising and exact verification tools Marabou and Planet without loss of accuracy,
in particular, for difficult verification problems. Our approach also significantly outperforms the
unique structure-oriented CEGAR-based approach CEGAR-NN, namely, 11.6–26.6 times faster.

REFERENCES

2021. 2nd International Verification of Neural Networks Competition (VNN-COMP’21). Retrieved 2021 from https://sites.

google.com/view/vnn2021

Pranav Ashok, Vahid Hashemi, Jan Kretínský, and Stefanie Mohr. 2020. DeepAbstract: Neural Network Abstraction for
Accelerating Verification. In Automated Technology for Verification and Analysis - 18th International Symposium, ATVA
2020, Hanoi, Vietnam, October 19-23, 2020, Proceedings (Lecture Notes in Computer Science, Vol. 12302), Dang Van Hung
and Oleg Sokolsky (Eds.). Springer, 92–107. https://doi.org/10.1007/978-3-030-59152-6_5

Stanley Bak, Changliu Liu, and Taylor T. Johnson. 2021. The Second International Verification of Neural Networks
Competition (VNN-COMP 2021): Summary and Results. CoRR abs/2109.00498 (2021). arXiv:2109.00498 https://arxiv.org/
abs/2109.00498

Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and M. Pawan Kumar. 2017. Piecewise Linear Neural Network

verification: A comparative study. CoRR abs/1711.00455 (2017). arXiv:1711.00455 http://arxiv.org/abs/1711.00455

Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In 2017 IEEE
Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017. 39–57. https://doi.org/10.1109/SP.2017.49
Edmund M. Clarke, Orna Grumberg, Somesh Jha, Yuan Lu, and Helmut Veith. 2000. Counterexample-Guided Abstraction
Refinement. In Computer Aided Verification, 12th International Conference, CAV 2000, Chicago, IL, USA, July 15-19, 2000,
Proceedings (Lecture Notes in Computer Science, Vol. 1855), E. Allen Emerson and A. Prasad Sistla (Eds.). Springer, 154–169.
https://doi.org/10.1007/10722167_15

Patrick Cousot and Radhia Cousot. 1977. Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs
by Construction or Approximation of Fixpoints. In Conference Record of the Fourth ACM Symposium on Principles of
Programming Languages, Los Angeles, California, USA, January 1977, Robert M. Graham, Michael A. Harrison, and Ravi
Sethi (Eds.). ACM, 238–252. https://doi.org/10.1145/512950.512973

Patrick Cousot and Nicolas Halbwachs. 1978. Automatic Discovery of Linear Restraints Among Variables of a Program.
In Conference Record of the Fifth Annual ACM Symposium on Principles of Programming Languages, Tucson, Arizona,
USA, January 1978, Alfred V. Aho, Stephen N. Zilles, and Thomas G. Szymanski (Eds.). ACM Press, 84–96. https:
//doi.org/10.1145/512760.512770

Nilesh N. Dalvi, Pedro M. Domingos, Mausam, Sumit K. Sanghai, and Deepak Verma. 2004. Adversarial classification.
In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 99–108.
https://doi.org/10.1145/1014052.1014066

Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. 2018. Output Range Analysis for Deep
Feedforward Neural Networks. In NASA Formal Methods - 10th International Symposium, NFM 2018, Newport News, VA,

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

21

USA, April 17-19, 2018, Proceedings (Lecture Notes in Computer Science, Vol. 10811), Aaron Dutle, César A. Muñoz, and
Anthony Narkawicz (Eds.). Springer, 121–138. https://doi.org/10.1007/978-3-319-77935-5_9

Rüdiger Ehlers. 2017. Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks. In Automated Technology
for Verification and Analysis - 15th International Symposium, ATVA 2017, Pune, India, October 3-6, 2017, Proceedings
(Lecture Notes in Computer Science, Vol. 10482), Deepak D’Souza and K. Narayan Kumar (Eds.). Springer, 269–286.
https://doi.org/10.1007/978-3-319-68167-2_19

Yizhak Yisrael Elboher, Justin Gottschlich, and Guy Katz. 2020. An Abstraction-Based Framework for Neural Network
Verification. In Computer Aided Verification - 32nd International Conference, CAV 2020, Los Angeles, CA, USA, July 21-24,
2020, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 12224), Shuvendu K. Lahiri and Chao Wang (Eds.).
Springer, 43–65. https://doi.org/10.1007/978-3-030-53288-8_3

Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin T. Vechev. 2018.
AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation. In 2018 IEEE Symposium on
Security and Privacy, SP 2018, Proceedings, 21-23 May 2018, San Francisco, California, USA. IEEE Computer Society, 3–18.
https://doi.org/10.1109/SP.2018.00058

Khalil Ghorbal, Eric Goubault, and Sylvie Putot. 2009. The Zonotope Abstract Domain Taylor1+. In Computer Aided
Verification, 21st International Conference, CAV 2009, Grenoble, France, June 26 - July 2, 2009. Proceedings (Lecture Notes in
Computer Science, Vol. 5643), Ahmed Bouajjani and Oded Maler (Eds.). Springer, 627–633. https://doi.org/10.1007/978-3-
642-02658-4_47

Sumathi Gokulanathan, Alexander Feldsher, Adi Malca, Clark W. Barrett, and Guy Katz. 2020. Simplifying Neural Networks
Using Formal Verification. In NASA Formal Methods - 12th International Symposium, NFM 2020, Moffett Field, CA, USA,
May 11-15, 2020, Proceedings (Lecture Notes in Computer Science, Vol. 12229), Ritchie Lee, Susmit Jha, and Anastasia
Mavridou (Eds.). Springer, 85–93. https://doi.org/10.1007/978-3-030-55754-6_5

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In 3rd
International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings. http://arxiv.org/abs/1412.6572

Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent
Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury. 2012. Deep Neural Networks for Acoustic Modeling
IEEE Signal Process. Mag. 29, 6 (2012), 82–97.
in Speech Recognition: The Shared Views of Four Research Groups.
https://doi.org/10.1109/MSP.2012.2205597

Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety Verification of Deep Neural Networks. In
Computer Aided Verification - 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings,
Part I (Lecture Notes in Computer Science, Vol. 10426), Rupak Majumdar and Viktor Kuncak (Eds.). Springer, 3–29.
https://doi.org/10.1007/978-3-319-63387-9_1

Kyle D Julian, Mykel J Kochenderfer, and Michael P Owen. 2019. Deep neural network compression for aircraft collision

avoidance systems. Journal of Guidance, Control, and Dynamics 42, 3 (2019), 598–608.

Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. 2017. Reluplex: An Efficient SMT Solver
for Verifying Deep Neural Networks. In Computer Aided Verification - 29th International Conference, CAV 2017, Heidelberg,
Germany, July 24-28, 2017, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 10426), Rupak Majumdar and Viktor
Kuncak (Eds.). Springer, 97–117. https://doi.org/10.1007/978-3-319-63387-9_5

Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor,
Haoze Wu, Aleksandar Zeljic, David L. Dill, Mykel J. Kochenderfer, and Clark W. Barrett. 2019. The Marabou Framework
for Verification and Analysis of Deep Neural Networks. In Computer Aided Verification - 31st International Conference,
CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 11561), Isil
Dillig and Serdar Tasiran (Eds.). Springer, 443–452. https://doi.org/10.1007/978-3-030-25540-4_26

Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009).
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial examples in the physical world. In 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. https:
//openreview.net/forum?id=HJGU3Rodl

SRI Lab. 2022. ETH Robustness Analyzer for Neural Networks (ERAN). https://github.com/eth-sri/eran
Yann LeCun. 1998. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/ (1998).
Wang Lin, Zhengfeng Yang, Xin Chen, Qingye Zhao, Xiangkun Li, Zhiming Liu, and Jifeng He. 2019. Robustness Verification
of Classification Deep Neural Networks via Linear Programming. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE, 11418–11427.
https://doi.org/10.1109/CVPR.2019.01168

Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian,
Jeroen A. W. M. van der Laak, Bram van Ginneken, and Clara I. Sánchez. 2017. A survey on deep learning in medical
image analysis. Medical Image Anal. 42 (2017), 60–88. https://doi.org/10.1016/j.media.2017.07.005

22

Liu et al.

Alessio Lomuscio and Lalit Maganti. 2017. An approach to reachability analysis for feed-forward ReLU neural networks.

CoRR abs/1706.07351 (2017). arXiv:1706.07351 http://arxiv.org/abs/1706.07351

Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun
Zhao, and Yadong Wang. 2018a. DeepGauge: multi-granularity testing criteria for deep learning systems. In Proceedings
of the 33rd ACM/IEEE International Conference on Automated Software Engineering. 120–131.

Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama. 2018b. MODE: automated neural network
model debugging via state differential analysis and input selection. In Proceedings of the 2018 ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 175–186.

Wei Ma and Jun Lu. 2017. An Equivalence of Fully Connected Layer and Convolutional Layer. CoRR abs/1712.01252 (2017).

arXiv:1712.01252 http://arxiv.org/abs/1712.01252

Matan Ostrovsky, Clark W. Barrett, and Guy Katz. 2022. An Abstraction-Refinement Approach to Verifying Convolutional

Neural Networks. CoRR abs/2201.01978 (2022). arXiv:2201.01978 https://arxiv.org/abs/2201.01978

Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016. The
Limitations of Deep Learning in Adversarial Settings. In IEEE European Symposium on Security and Privacy, EuroS&P
2016, Saarbrücken, Germany, March 21-24, 2016. 372–387. https://doi.org/10.1109/EuroSP.2016.36

Brandon Paulsen and Chao Wang. 2022. LinSyn: Synthesizing Tight Linear Bounds for Arbitrary Neural Network Activation

Functions. CoRR abs/2201.13351 (2022).

Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2019. DeepXplore: automated whitebox testing of deep learning

systems. Commun. ACM 62, 11 (2019), 137–145.

Pavithra Prabhakar and Zahra Rahimi Afzal. 2019. Abstraction based Output Range Analysis for Neural Networks. In
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,
Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15762–15772.

Luca Pulina and Armando Tacchella. 2010. An Abstraction-Refinement Approach to Verification of Artificial Neural
Networks. In Computer Aided Verification, 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010.
Proceedings (Lecture Notes in Computer Science, Vol. 6174), Tayssir Touili, Byron Cook, and Paul B. Jackson (Eds.). Springer,
243–257. https://doi.org/10.1007/978-3-642-14295-6_24

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. 2015. ImageNet Large Scale Visual Recognition
Challenge. Int. J. Comput. Vis. 115, 3 (2015), 211–252. https://doi.org/10.1007/s11263-015-0816-y

Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin T. Vechev. 2018. Fast and Effective Robustness
Certification. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 10825–10836.

Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. 2019. An abstract domain for certifying neural

networks. Proc. ACM Program. Lang. 3, POPL (2019), 41:1–41:30. https://doi.org/10.1145/3290354

Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and Daniel Kroening. 2018. Concolic testing for
deep neural networks. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering.
109–119.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.
Intriguing properties of neural networks. In Proceedings of the 2nd International Conference on Learning Representations.
Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: automated testing of deep-neural-network-driven

autonomous cars. In Proceedings of the 40th International Conference on Software Engineering. 303–314.

Vincent Tjeng and Russ Tedrake. 2017. Verifying Neural Networks with Mixed Integer Programming. CoRR abs/1711.07356

(2017). arXiv:1711.07356 http://arxiv.org/abs/1711.07356

Hoang-Dung Tran, Diego Manzanas Lopez, Patrick Musau, Xiaodong Yang, Luan Viet Nguyen, Weiming Xiang, and Taylor T.
Johnson. 2019. Star-Based Reachability Analysis of Deep Neural Networks. In Formal Methods - The Next 30 Years - Third
World Congress, FM 2019, Porto, Portugal, October 7-11, 2019, Proceedings (Lecture Notes in Computer Science, Vol. 11800),
Maurice H. ter Beek, Annabelle McIver, and José N. Oliveira (Eds.). Springer, 670–686. https://doi.org/10.1007/978-3-030-
30942-8_39

Caterina Urban and Antoine Miné. 2021. A Review of Formal Methods applied to Machine Learning. arXiv preprint

arXiv:2104.02466 (2021).

Chris Urmson and William Whittaker. 2008. Self-Driving Cars and the Urban Challenge. IEEE Intell. Syst. 23, 2 (2008), 66–68.

https://doi.org/10.1109/MIS.2008.34

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Formal Security Analysis of Neural
Networks using Symbolic Intervals. In 27th USENIX Security Symposium, USENIX Security 2018, Baltimore, MD, USA,
August 15-17, 2018, William Enck and Adrienne Porter Felt (Eds.). USENIX Association, 1599–1614. https://www.usenix.

Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks

23

org/conference/usenixsecurity18/presentation/wang-shiqi

Eric Wong and J. Zico Kolter. 2018. Provable Defenses against Adversarial Examples via the Convex Outer Adversarial
Polytope. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,
Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.).
PMLR, 5283–5292. http://proceedings.mlr.press/v80/wong18a.html

Pengfei Yang, Renjue Li, Jianlin Li, Cheng-Chao Huang, Jingyi Wang, Jun Sun, Bai Xue, and Lijun Zhang. 2021. Improving
Neural Network Verification through Spurious Region Guided Refinement. In Tools and Algorithms for the Construction
and Analysis of Systems - 27th International Conference, TACAS 2021, Held as Part of the European Joint Conferences on
Theory and Practice of Software, ETAPS 2021, Luxembourg City, Luxembourg, March 27 - April 1, 2021, Proceedings, Part I
(Lecture Notes in Computer Science, Vol. 12651), Jan Friso Groote and Kim Guldstrand Larsen (Eds.). Springer, 389–408.
https://doi.org/10.1007/978-3-030-72016-2_21

Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2022. Machine Learning Testing: Survey, Landscapes and Horizons.

IEEE Trans. Software Eng. 48, 2 (2022), 1–36.

