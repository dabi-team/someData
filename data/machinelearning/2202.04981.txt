BARWISE COMPRESSION SCHEMES
FOR AUDIO-BASED MUSIC STRUCTURE ANALYSIS

Axel Marmoret

Fr´ed´eric Bimbot

Univ Rennes, Inria, CNRS, IRISA, France.
firstname.name@irisa.fr

J´er´emy E. Cohen
Univ Lyon, INSA-Lyon, UCBL,
UJM-Saint Etienne, CNRS, Inserm,
CREATIS UMR5220, U1206, France
jeremy.cohen@cnrs.fr

2
2
0
2

r
p
A
5
1

]

D
S
.
s
c
[

2
v
1
8
9
4
0
.
2
0
2
2
:
v
i
X
r
a

ABSTRACT

Music Structure Analysis (MSA) consists in segmenting
a music piece in several distinct sections. We approach
MSA within a compression framework, under the hypoth-
esis that the structure is more easily revealed by a sim-
pliﬁed representation of the original content of the song.
More speciﬁcally, under the hypothesis that MSA is corre-
lated with similarities occurring at the bar scale, this arti-
cle introduces the use of linear and non-linear compression
schemes on barwise audio signals. Compressed represen-
tations capture the most salient components of the differ-
ent bars in the song and are then used to infer the song
structure using a dynamic programming algorithm. This
work explores both low-rank approximation models such
as Principal Component Analysis or Nonnegative Matrix
Factorization and “piece-speciﬁc” Auto-Encoding Neural
Networks, with the objective to learn latent representations
speciﬁc to a given song. Such approaches do not rely on
supervision nor annotations, which are well-known to be
tedious to collect and possibly ambiguous in MSA descrip-
tion. In our experiments, several unsupervised compres-
sion schemes achieve a level of performance comparable
to that of state-of-the-art supervised methods (for 3s toler-
ance) on the RWC-Pop dataset, showcasing the importance
of the barwise compression processing for MSA.

1. INTRODUCTION

Music Structure Analysis (MSA) consists in subdividing
a music piece in several distinct parts, which represent a
mid-level description of a song. Structure is an important
part of music, and could be the main difference between
music and random noise [1]. In that sense, MSA has been
extensively studied in Music Information Retrieval (MIR),
see [1, 2] for overviews. Practically, in the era of com-
putational analysis of music with Machine Learning al-
gorithms, structure could be an important mid-level fea-
ture for tasks such as cover detection, genre recognition,
music summarization, for better recommendation systems,
and many other tasks. Segmentation is usually based on

Copyright: © 2022 Axel Marmoret et al. This is an open-access article distributed

under the terms of the Creative Commons Attribution 3.0 Unported License, which

permits unrestricted use, distribution, and reproduction in any medium, provided

the original author and source are credited.

criteria such as homogeneity, novelty, repetition and reg-
ularity [2]. When performed algorithmically, MSA often
relies on similarity criteria within passages of a song sum-
marized in an autosimilarity matrix [3–11], in which each
coefﬁcient represents an estimation of the similarity be-
tween two musical fragments. Related work mainly splits
in two categories: blind (unsupervised) and learning-based
(supervised) techniques. Blind segmentation techniques,
such as this work, do not use training datasets.

Similarity between two frames can be obtained from the
feature representation of the signal, such as the Short-Time
Fourier Transform (STFT) of the song [3]. Boundaries can
then be detected as points of strong dissimilarity between
the near past and future of a given instant, as in [3]. Still,
recent works have aimed at designing new representations
of the original music, able to capture the similarity between
two frames while maintaining a high level of dissimilarity
between dissimilar frames, either in a blind [2, 4–9] or in
a supervised [10, 11] fashion. This generally consists in
projecting the data in a new feature space and computing
the similarity in the feature space.
In particular, McFee
and Ellis made use of spectral clustering as a segmentation
method by interpreting the autosimilarity as a graph and
clustering principally connected vertices as segments [5].
This work performed best among blind segmentation tech-
niques in the last structural segmentation MIREX cam-
paign in 2016 [12]. Recently, we used tensor decomposi-
tion (Nonnegative Tucker Decomposition, NTD) as a way
to describe music as barwise patterns, which then served
as features for the computation of the autosimilarity ma-
trix [9].

In most former works, the similarity is expressed between
beatwise aligned features, as in [5, 6]. Instead, the present
work considers that repetitions are more prone to happen
at the barscale, and hence focuses on barwise aligned fea-
tures, as in [9]. A comparison between both beatwise and
barwise aligned features has been made in [10], without
one singling out. In this work, we ﬁrst explore low-rank
approximation methods to perform barwise dimensional-
ity reduction using Principal Component Analysis (PCA)
and Nonnegative Matrix Factorization (NMF), and then
design “piece-speciﬁc” Auto-Encoding Neural Networks
(AE). While PCA and NMF are standard methods nowa-
days, the work presented in this article is the ﬁrst one, as
far as we know, to consider them for barwise dimensional-
ity reduction.

 
 
 
 
 
 
This work extends the concept of barwise compression
by introducing a song-dependent autoencoder, i.e. an AE
which is speciﬁcally trained to compress a given song.
This technique is called “Single-Song Auto-Encoding” and
the latent representation of each bar in the AE is used as a
sequence of features for segmenting the song. Indeed, re-
cently, Deep Neural Networks (DNN) have lead to some
high level of excitement in MIR research, and notably in
MSA [8, 13]. In general, DNN approaches rely on large
databases which make it possible to learn a large num-
ber of parameters, which in turn yields better performance
than previously established machine learning approaches.
In particular, to the best of the authors’ knowledge, the cur-
rent state-of-the-art approach for the structural segmenta-
tion of Popular music is a supervised CNN developed by
Grill and Schl¨uter [13]. This is the consequence of the abil-
ity of DNNs to learn complex nonlinear mappings through
which musical objects can be expected to be better sepa-
rated [14]. Hence, while DNNs generally learn “deep” fea-
tures stemming from multiple examples in a training phase,
and then evaluate the potential of learned features [15], our
single-song AE approach focuses on the different events
within a single song and tries to learn nonlinear latent rep-
resentations, used to infer the structure.

To study the relevance of the barwise compression ap-
proach, this work evaluates different dimensionality reduc-
tion techniques on the RWC-Pop dataset [16] in their audio
form using various time-frequency features. Segmentation
results on this database show levels of performance which
are outperforming the current blind state-of-the-art [3,5,9],
and our best performing model outperforms the supervised
state-of-the-art [13] with 3-seconds tolerance.

The rest of the article is structured as follows: Section 2
details the motivations and approaches for barwise music
compression. Section 3 presents the various compression
schemes used in this work. Section 4 presents the seg-
mentation process. Section 5 reports on the experimental
results.

2. BARWISE MUSIC COMPRESSION

2.1 Motivations

The underlying idea of this work is that music structure
can be related to compression of information. Indeed, a
common view of music structure is to consider structural
segments as internally coherent passages, and automatic
retrieval techniques generally focus on ﬁnding them by
maximizing homogeneity and repetition and/or by setting
boundaries between dissimilar segments, as points of high
novelty [2]. In the context of compressed representations,
each passage is transformed in a vector of small dimension,
compelling this representation to summarize the original
content. From this angle, similar passages are expected
to be represented by similar representations, as they share
underlying properties (such as coherence and redundancy),
while dissimilar passages are bound to create strong dis-
crepancies at their boundaries. Thus, we expect that com-
pressed representations will enhance the original structure
while reducing incidental signal-wise properties which do

not contribute to the structure. To the best of our knowl-
edge, this work, our former work [9], and recent work by
Wang et al. [10] are the only barwise compression schemes
with application to structural segmentation of music.

Conversely to ﬁxed-size frame analysis, barwise compu-
tation guarantees that the information contained in each
frame does not depend on the tempo, but on the metrical
positions, which is a more abstract musical notion to de-
scribe time. As a consequence, comparing bars is more re-
liable than comparing frames of arbitrarily ﬁxed size as it
allows to cope with small variations of tempo. In addition,
pop music (i.e. our case study) is generally quite regular
at the bar level: repetitions occur at the bar scale and mo-
tivic patterns tend to develop within a limited number of
bars, suggesting that frontiers mainly sit between bars. To
support this claim, Table 1 of Section 5 presents segmen-
tation results when realigning the annotation on the closest
downbeat.

Accordingly, the proposed method relies on a consistent
bar division of music. It also requires a powerful tool to
detect bars, as otherwise errors could propagate and affect
the performance. Results reported in [9] and in Table 1
tend to show that the madmom toolbox [17] is efﬁcient in
that respect on the RWC-Pop dataset. Nonetheless, bar-
wise processing may hinder the retrieval of boundaries de-
limiting changes of tempo, as discussed in [18].
It also
relies on some consistent bar division of music, which is
generally the case for contemporary western music, but is
not a universal rule, and/or may turn out to be ambiguous.

2.2 Barwise Music Processing

Following our former work in [9], we process music as bar-
wise spectrograms, with a ﬁxed number of frames per bar.
Practically, spectrograms are computed using librosa [19]
with a low hop length of 32 frames at a sampling rate of
44.1kHz, and downbeats are estimated with the madmom
toolbox [17]. This allows us to split the original spectro-
gram in b barwise spectrograms (b being the number of
bars in this song) each containing nb frames. As bars can
be of different lengths (because of tempi differences or
small inconsistencies/imperfections in the performance),
different bars can contain different numbers nb of frames.
Thus, we deﬁne an integer s, the subdivision parameter,
which is the desired number of frames in each bar.

(cid:111)

, 0 ≤ k < s, k ∈ N

Starting from the subdivision s, and from indexes fs and
fe, respectively the indexes of the closest frames to the
downbeats starting and ending the bar, we select all frames
(cid:110) fs+k×(fe−fs)
, i.e. equally-spaced
s
frames in the bar to ﬁt the chosen subdivision. This in-
deed results in barwise spectrograms containing exactly s
frames. Other techniques could be applied to reduce the
number of frames (for example averaging the content of
several frames instead of choosing one), but we did not
pursue that lead. We chose s = 96 as in [9].

This technique result in b barwise Time-Frequency spec-
trograms, of size f × s, with f the number of components
in the chosen feature, as presented in section 2.3. Com-
pression is generally performed on matrices (as PCA and
NMF for instance). In that sense, by vectorizing each of

b

f

Original
Spectrogram

f

Barwise
Spectrograms

s × b

s

f × s

Barwise
TF
matrix

b

Figure 1: Barwise processing of an input spectrogram, resulting in a barwise TF matrix.

the previous barwise spectrograms, we introduce the “bar-
wise TF” representation, consisting in a matrix of size
b×(f ×s). The aforementioned process is described in Fig-
ure 1. Note that we chose to vectorize the time-frequency
features, thus discarding the dependencies between these
two dimensions.

2.3 Features

Music is represented with different features throughout our
experiments, focusing on different aspects of music such as
harmony or timbre. These features are presented hereafter.
Chromagram A chromagram represents the time-
frequency aspect of music as sequences of 12-row vectors,
corresponding to the 12 semi-tones of the classical western
music chromatic scale (C, C#, ..., B), which is largely used
in Pop music. Hence, f = 12, and each row represents
the weight of a semi-tone (and its octave counterparts) at a
particular instant.

Mel spectrogram A Mel spectrogram corresponds to the
STFT representation of a song, whose frequency bins are
recast in the Mel scale. These bands account for the expo-
nential spread of frequencies throughout the octaves. Mel
spectrograms are dimensioned following the work of [13]
(80 ﬁlters, from 80Hz to 16kHz), hence f = 80. STFT are
computed as power spectrograms.

Log Mel spectrogram (LMS) A Log Mel spectrogram
(abbreviated “LMS”) corresponds to the logarithmic val-
ues of the precedent spectrogram, hence f is still equal to
80. This representation accounts for the exponential decay
of power with frequency observed in STFT.

Nonnegative Log Mel spectrogram (NNLMS) Nonneg-
ative Log Mel spectrogram (abbreviated “NNLMS”) is
a nonnegative version of the precedent LMS. This rep-
resentation is motivated by the fact that some compres-
sion algorithms are nonnegative (in particular, NMF), and
thus need nonnegative inputs. NNLMS are computed as
log(M el + 1) where M el represent the coefﬁcients of the
Mel spectrogram, which are nonnegative, and log the ele-
mentwise logarithm.

MFCC spectrogram Mel-Frequency Cepstral Coefﬁ-
cients (MFCCs) are timbre-related coefﬁcients, obtained
by a discrete cosine transform of a Log Mel spectrogram 1 .
This spectrogram contains f = 32 coefﬁcients, follow-
ing [20].

1 Note that we use the default librosa settings for MFCC, hence the

Log Mel spectrogram used for MFCC is not the same as for the LMS.

3. COMPRESSION SCHEMES

3.1 Low-Rank Approximations
Given a collection of input vectors {xi}i=1,...,b ∈ Rn
stored in a matrix X, low-rank approximations search for
projections in a low-dimensional subspace which approxi-
mates the input vectors. We study two different low-ranks
approximations methods, namely PCA and NMF.

PCA is maybe the most well-known and used low-rank
approximation technique.
It can be seen as a singular
value decomposition applied on the debiased matrix X −µ,
where µ is the mean of the columns of X. In short, we ob-
tain an approximation X − µ ≈ W H where W ∈ Rn×dc
is orthogonal, and H ∈ Rdc×b is the product of a diagonal
and an orthogonal matrix. The rank of the approximation
dc is chosen by the user, and should be smaller than both
dimensions.

NMF is similar to PCA but does not impose orthogonal-
ity, nor does it unbias the data. Instead, it builds a cone
containing the data, which extreme rays are the columns of
W . To achieve this, nonnegativity is required elementwise
on both W and H. The low-rank approximation is in fact
the solution of an optimization problem, here:

arg min
W ≥0,H≥0

(cid:107)X − W H(cid:107)2
F

(1)

using the Frobenius norm as a loss function. NMF typ-
ically yields more interpretable features than PCA, but it
is much harder to compute, and requires that the data is
mostly nonnegative [21]. In both cases, H is the barwise
compressed representation.

3.2 Autoencoders

Autoencoders are neural networks, which, by design, per-
form unsupervised dimensionality reduction. Throughout
the years, AE have received increasing interest, notably
due to their ability to extract salient latent representations
without the need of large amount of annotations. Recently,
AE also showed great results as a generation tool [22, 23].
Still, as presented in [23], PCA and AE are competitive
as compression schemes, and PCA even leads to lower re-
construction error when AE are too “simple” (in particular
when networks are linear or “shallow”, i.e. with only a few
layers).
Practically, given a generic entry x ∈ Rn, an AE learns a
nonlinear function f with parameters θ (weights and biases
of the network) such that ˆx = f (x, θ) ∈ Rn reconstructs x

as faithfully as possible. This is achieved by minimizing a
given loss function such as the Mean Square Error (MSE)
between x and ˆx w.r.t. parameters θ.

arg min
θ

MSE(x, ˆx) =

1
n

n−1
(cid:88)

(xi − ˆxi)2 .

i=0

(2)

Other metrics can be used, such as the Kullback-Leibler
divergence, but we restrict this work to MSE.

An autoencoder is divided into two parts: an encoder,
which compresses the input x ∈ Rn into a latent represen-
tation z ∈ Rdc of smaller dimension (generally, dc (cid:28) n),
and a decoder, which reconstructs ˆx from z. While Au-
toencoders generally learn a common latent representation
for an entire dataset, our technique optimizes a network for
each song. This framework is called “single-song autoen-
coding”.

In this work, layers are of two types: fully-connected,
or convolutional. Convolutional layers lead to impres-
sive results in image processing due to their ability to dis-
cover local correlations (such as lines or edges), which
turn to higher-order features with the depth of the net-
work [24] [25, Ch. 9]. While local correlations are less ob-
vious in spectrogram processing [26], Convolutional Neu-
ral Networks (CNN) still perform well in MIR tasks, such
as MSA [13].

The tested AE is hence a CNN. We use the nowadays
quite standard Rectiﬁed Linear Unit (ReLU) function as
the activation function, except in the latent layer because
it could lead to null latent variables. The encoder is com-
posed of ﬁve hidden layers: 2 convolutional/max-pooling
blocks, followed by a fully-connected layer, controlling the
size dc of the latent space. Convolutional kernels are of
size 3x3, and the pooling is of size 2x2. Convolutional
layers deﬁne respectively 4 and 16 feature maps.

The decoder is composed of 3 hidden layers: a fully-
connected layer (inverse of the previous one) and 2 “trans-
posed convolutional” layers of size 3x3 and stride 2x2. A
transposed convolution is similar to the convolution opera-
tion taken in the backward pass: an operation which takes
one scalar as input and returns several scalars as output [27,
Ch. 4]. Hence, it is well suited to inverse the convolution
process. This network is represented in Figure 2.

Figure 2: Architecture of the autoencoder

3.3 Dimensions of Compression

Selecting the dimension of the compression dc, i.e.
the
number of components to use for compressing the input
data, is not trivial. In particular, it is well-known that se-
lecting the number of components for NMF is a complex
problem, generally leading to manual tuning or dedicated

heuristics [21, 28]. We also observed a great inﬂuence of
the size of the latent space of the AE on the segmenta-
tion results, without a clear candidate singling out. In that
sense, we will compare values for dc ∈ {8, 16, 24, 32, 40}.
Even if such heuristics are standard for PCA (such as the
elbow method), as no obvious candidate heuristic stood
out relatively to the quality of segmentation, and for fair
comparison with other techniques, PCA will be tested with
the same dc values. A clever dimension selection method
could be studied in future work.

4. STRUCTURAL SEGMENTATION

4.1 General Principle

The ability of these compression methods to separate and
group bars is evaluated on the MSA task, as presented
in [29]. Thereby, for a given song in a given representa-
tion, we obtain compressed representations zi for all bars
1 ≤ i ≤ b of the song. These compressed representa-
tions are at the heart of the segmentation strategy, via their
autosimilarity matrix. Denoting Z ∈ Rdc×b the matrix
resulting from the concatenation of all dc-dimensional zi
barwise compressed representations, its autosimilarity is
deﬁned as Z T Z, i.e. the b × b matrix of the dot products
between every zi. These dot products are then normalized
to 1, resulting in a matrix of cosine similarities. Examples
of autosimilarities are shown in Figure 3.

Segmentation is obtained via the dynamic programming
algorithm presented in [9], and inspired from [30]. The
principle of dynamic programming is to solve a combina-
torial optimization problem by dividing it in several atomic
problems, easier to solve, and which solutions can be
stitched together to form the global solution. The Viterbi
and Dynamic Time Warping (DTW) algorithms are exam-
ples of dynamic programming algorithms with a lot of ap-
plications in the Audio community. In our context, a “seg-
mentation cost” is computed for every segment in the song.
These individual segment costs constitute the atomic prob-
lems to solve. Then, the optimal segmentation consists in
the global maximum of the sum of the segment costs for
all the segments in this segmentation.

The cost of each segment is designed in order to favor
their homogeneity. As the autosimilarity matrix represent
the similarity between pairs of bars in the song, the higher
is this similarity, the most similar are the bars. Hence, the
cost of a segment is deﬁned as an aggregated value of the
similarity between all pairs of bars in this segment. Practi-
cally, this is obtained by convolving a kernel with the au-
tosimilarity restricted to this segment. Hence, to compute
the cost of each segment in the song, the algorithm applies
a sliding convolving kernel on the diagonal of the autosim-
ilarity matrix. This convolving kernel is a square matrix,
the size of which is that of the potential segment.

While sharing the principle of a sliding kernel with the
work of Foote [3], largely described in the literature [2],
the proposed kernel focuses on homogeneity rather than
novelty. When the diagonal of the autosimilarity is struc-
tured in several self-similar blocks, the algorithm frames
and partitions these blocks.

Figure 3: Barwise autosimilarities for the Nonnegative Log Mel spectrogram (left) and of the compressed representation
with dc = 24 (from left to right: PCA, NMF and AE) computed on the song “Pop01” from RWC-Pop. Horizontal and
vertical lines represent the annotation.

2
1
0

Figure 4: Kernel of size 10

4.2 Technical Details

The design of the kernel deﬁnes how to transform bar sim-
ilarities into segment homogeneity in the form of a cost.
A kernel matrix full of ones would imply that the segment
cost is the sum of the similarity values in this segment.
The proposed kernel is equal to 0 on the diagonal, 2 on
the 4 sub and super diagonals, and 1 everywhere else. The
diagonal is equal to zero so as to disregard perfect self-
similarity of each bar with itself (normalized to 1). The
other elements of this kernel are designed so as to empha-
size the short-term similarity in the 4 contiguous bars, and
still catch longer-term similarity for long segments. It is
presented in Figure 4. This kernel performs best in com-
parative experiments 2 of [9], and, notably, better than a
kernel matrix of 1 with a 0 diagonal.

To account for regularity constraints as in [30],

the
present algorithm adds a regularity penalty p(n) to the
plain convolution score, which is a function of the size n
(in bars) of the segment. This function p(n) is equal to 0
if n = 8, 1
2 if n ≡ 0 (mod 2), and ﬁnally
1 otherwise. This penalty function is musically motivated
for pop music, as segments are more likely to be of size 4
or 8 bars (especially in RWC-Pop with MIREX10 annota-
tions [30]), than of odd bar size, as shown in Figure 5.

4 if n = 4, 1

Finally, for all possible segments [b1, b2[ in the song, the

algorithm computes a score:

sb1,b2 =

cb1,b2
cmax
k8

− p(n),

(3)

where cb1,b2 is the convolution cost, and where cmax
is
the highest convolution score on this autosimilarity with a
kernel of size 8, used for normalization purposes.

k8

Figure 5: Distribution of segment’ sizes in number of bars
in MIREX10.

5. EXPERIMENTS

The proposed segmentation pipeline is studied on the
RWC Pop dataset, which consists in 100 Pop songs of
high recording quality [16], along with the MIREX10 an-
notations [31]. We compared the barwise compression
schemes described in the present work with three blind
methods and a supervised method: Foote’s novelty ker-
nel [3], Spectral Clustering by McFee and Ellis [5], our
former NTD method [9], and the supervised CNN of Grill
and Schl¨uter [13], the latter being the current state-of-the-
art. Results for [3] and [5] were computed with the MSAF
toolbox [32], and boundaries were aligned to the closest
downbeat for fairer comparison (as in [9]). Results for [13]
were taken from the 2015 MIREX campaign [12].

We focus on boundary retrieval and ignore segment la-
belling. Boundaries are evaluated using the hit-rate met-
ric, which considers a boundary valid if it is approximately
equal to an annotation, within a ﬁxed tolerance window.
Consistently with MIREX standards [12], tolerances are
equal to 0.5s and 3s. The hit-rate is then expressed in terms
of Precision, Recall, and F-measure, resulting respectively
in P0.5, R0.5, F0.5, P3, R3, F3.

5.1 Practical Considerations

2 https://gitlab.inria.fr/amarmore/musicntd/-

/blob/v0.2.0/Notebooks/5%20-%20Different%20kernels.ipynb

PCA is computed with the scikit-learn [33] toolbox, using
the ARPACK solver. NMF is computed using the nn fac

020406080100020406080100Autosimilarity of barwise FT NNLMS020406080100020406080100Autosimilarity of PCA-compressed NNLMS020406080100020406080100Autosimilarity of NMF-compressed NNLMS020406080100020406080100Autosimilarity of AE-compressed NNLMS0510152025Size of the segment, in number of bars0.00.10.20.30.40.5Proportion among all segment' sizes(a) 0.5 seconds tolerance results (F0.5)

(b) 3 seconds tolerance results (F3)

Figure 6: Segmentation results for the different methods and representation.

Table 1: Results of best-performing methods and state-of-the-art on the RWC-Pop dataset.
contest [12].)

(*Results: 2015 MIREX

Compression methods

State-of-the-art

PCA
NMF
Autoencoder
Foote [3]
Spectral clustering [5]
NTD [9]
Supervised CNN [13]*

Aligning annotation on downbeats

toolbox [34]. The AE is developed with Pytorch 1.8.0 [35],
trained with the Adam optimizer [36], with a learning rate
of 0.001, divided by 10 when the loss function reaches
a plateau (20 iterations without improvement) until 1e-5.
The optimization stops if no progress is made during 100
consecutive epochs, or after a total of 1000 epochs. The
network is initialized with the uniform distribution deﬁned
in [37], also known as “kaiming” initialization. Bars were
estimated with madmom [17], and spectrograms computed
with librosa [19] with default settings, unless speciﬁed. All
segmentation scores were computed with mir eval [38].

The entire code for this work is open-source, and contains
experimental notebooks for reproducibility 3 . Performing
1000 epochs for a song takes between 1.5 minute (for chro-
magrams) and 5 minutes (for Mel/Log Mel spectrograms)
on an Intel® Core(TM) i7 CPU, NMF takes less than 3 sec-
onds and PCA less than a second.

5.2 Results

Figure 6 presents results on the RWC-Pop dataset for all
the methods and the features. The dimension of compres-
sion dc is chosen as the best performing one for the F3
metric. The Log Mel and Nonnegative Log Mel spectro-
gram seem to outperform the other features at both tol-
erances, except for the PCA which obtain similar perfor-
mances with MFCC. PCA and AE achieve the state-of-
the-art level of performance with 3-seconds tolerance, as
presented in Table 1. With 0.5-second tolerance, all tech-
niques obtain lower results than those of the state-of-the-
art, but outperform the blind state-of-the-art. Results are
consistent with those of [23], where PCA obtain similar
or better reconstruction errors than linear or shallow au-
toencoders. Still, we believe that the presented AE can be

3 https://gitlab.inria.fr/amarmore/BarwiseMusicCompression

P3

F3

R3

F0.5

R0.5

P0.5
60.7% 66.9% 63.1% 76.1% 84.0% 79.2%
57.4% 60.4% 58.3% 73.6% 77.6% 74.9%
60.1% 62.6% 60.9% 78.9% 82.5% 80.1%
42.0% 30.0% 34.5% 67.1% 47.7% 55.0%
49.2% 45.0% 45.0% 65.5% 60.6% 60.3%
58.4% 60.7% 59.0% 72.5% 75.3% 73.2%
80.4% 62.7% 69.7% 91.9% 71.1% 79.3%
96.5% 96.2% 96.3% 100% 99.7% 99.9%

greatly improved.

NMF obtain lower results than the other methods, but,
due to the nonnegativity, it is expected that NMF result in
a part-based decomposition of the original barwise spec-
trogram. The part-based property can lead to interpretable
factors, as observed in many applications [21, 39], and is
important for pattern uncovering in NTD [9]. Nonetheless,
in this study, we do not have quantitative results to support
this claim.

Results seem still improvable with 0.5-second tolerance.
A wrong estimation at 0.5s can be due to an incorrect fron-
tier estimation, but also to an incorrect bar estimation. Re-
sults when aligning the annotation on downbeats presented
in Table 1 indicate that bar estimation seems precise on the
RWC Pop dataset.

6. CONCLUSION

This article introduce barwise compression schemes,
which appear as competitive schemes for Music Struc-
ture Analysis. In our experiments on the RWC-Pop mu-
sic dataset, segmentation scores obtained with compressed
representations outperform the results of the blind state-of-
the-art, and our best-performing methods reach the level of
performance of the global state-of-the-art at 3-seconds tol-
erance, while being unsupervised. Still, this work focus
on the RWC-Pop dataset, and would beneﬁt from further
In particu-
investigations on different musical contexts.
lar, it would be interesting to study the barwise compres-
sion schemes in condition where bars are more ambigu-
ous and/or less regular (polyrhythms/polymeters, changing
meters, etc) and on more erratic structures (i.e. segments
less concentrated around the sizes of 4 and 8 bars).

Future work will focus on improving the proposed
paradigm, for instance with nonnlinear compression meth-

ChromagramMelLMSNNLMSMFCC405060708090100F measure (percentage)38.6%46.3%46.0%42.8%40.9%53.6%53.9%63.2%62.4%63.1%56.5%53.0%58.3%51.2%53.2%59.9%60.9%54.0%Barwise FTPCANMFAEChromagramMelLMSNNLMSMFCC405060708090100F measure (percentage)62.0%70.6%71.0%65.0%66.4%69.1%72.3%79.1%76.4%79.2%72.8%73.4%74.9%72.0%74.9%79.8%80.1%76.0%Barwise FTPCANMFAEods such as kernel methods, or by improving the autoen-
coder in using strategies such as transfer learning from a
song-independent AE or exploring various network archi-
tectures or regularizations. Finally, while the convolutional
dynamic programming algorithm is competitive, it is un-
stable to small variations in autosimilarities and should be
made more robust.

Altogether, we believe that these ﬁrst results pave the way
to an interesting paradigm using barwise compression al-
gorithms, and notably single-song AEs, for the description
of structural elements in music.

7. REFERENCES

[1] J. Paulus, I. C. IIS, M. M¨uller, and A. Klapuri, “Audio-

based music structure analysis,” 2010.

[2] O. Nieto, G. J. Mysore, C.-i. Wang, J. B. Smith,
J. Schl¨uter, T. Grill, and B. McFee, “Audio-based mu-
sic structure analysis: Current trends, open challenges,
and applications,” Trans. Int. Soc. for Music Informa-
tion Retrieval, vol. 3, no. 1, 2020.

[3] J. Foote, “Automatic audio segmentation using a mea-
sure of audio novelty,” in 2000 IEEE Int. Conf. Multi-
media and Expo. ICME2000. Proc. Latest Advances in
the Fast Changing World of Multimedia, vol. 1.
IEEE,
2000, pp. 452–455.

[4] J. Serra, M. M¨uller, P. Grosche, and J. L. Arcos, “Un-
supervised music structure annotation by time series
structure features and segment similarity,” IEEE Trans-
actions on Multimedia, vol. 16, no. 5, pp. 1229–1240,
2014.

[5] B. McFee and D. Ellis, “Analyzing song structure with
spectral clustering,” in Int. Soc. Music Information Re-
trieval (ISMIR), 2014, pp. 405–410.

[6] O. Nieto and T. Jehan, “Convex non-negative matrix
factorization for automatic music structure identiﬁca-
tion,” in 2013 IEEE Int. Conf. Acoustics, Speech and
Signal Processing (ICASSP).
IEEE, 2013, pp. 236–
240.

[7] I. Theodorakopoulos, G. Economou, and S. Fotopou-
los, “Unsupervised music segmentation via multi-scale
processing of compressive features’ representation,” in
2013 18th International Conference on Digital Signal
Processing (DSP).

IEEE, 2013, pp. 1–6.

[8] M. McCallum, “Unsupervised learning of deep fea-
IEEE,

tures for music segmentation,” in ICASSP.
2019, pp. 346–350.

[9] A. Marmoret, J. Cohen, N. Bertin, and F. Bimbot,
“Uncovering audio patterns in music with nonnegative
tucker decomposition for structural segmentation,” in
ISMIR, 2020, pp. 788–794.

[10] J.-C. Wang, J. B. Smith, W.-T. Lu, and X. Song, “Su-
pervised metric learning for music structure feature,”
ISMIR, 2021.

[11] J. Salamon, O. Nieto, and N. J. Bryan, “Deep embed-
dings and section fusion improve music segmentation,”
ISMIR, 2021.

[12] “MIREX wiki,” https://www.music-ir.org/mirex/wiki/

MIREX HOME.

[13] T. Grill and J. Schl¨uter, “Music boundary detection us-
ing neural networks on combined features and two-
level annotations,” in ISMIR, 2015, pp. 531–537.

[14] E. Humphrey, J. Bello, and Y. LeCun, “Feature learn-
ing and deep architectures: New directions for mu-
sic informatics,” J. Intelligent Information Systems,
vol. 41, no. 3, pp. 461–481, 2013.

[15] R. Agrawal and S. Dixon, “Learning frame similarity
using siamese networks for audio-to-score alignment,”
in 2020 28th European Signal Processing Conference
(EUSIPCO).
IEEE, 2021, pp. 141–145.

[16] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka,
“RWC Music Database: Popular, Classical and Jazz
Music Databases,” in ISMIR, vol. 2, 2002, pp. 287–
288.

[17] S. B¨ock, F. Korzeniowski, J. Schl¨uter, F. Krebs, and
G. Widmer, “madmom: a new Python Audio and Mu-
sic Signal Processing Library,” in Proc. 24th ACM Int.
Conf. Multimedia, Amsterdam, The Netherlands, 10
2016, pp. 1174–1178.

[18] I. Vatolkin, F. Ostermann, and M. M¨uller, “An evolu-
tionary multi-objective feature selection approach for
detecting music segment boundaries of speciﬁc types,”
in Proc. Genetic and Evolutionary Computation Conf.,
2021, pp. 1061–1069.

[19] B. McFee et al., “librosa/librosa: 0.8.1rc2.” Zenodo,
https://doi.org/10.

May 2021. [Online]. Available:
5281/zenodo.4792298

[20] B. McFee and D. Ellis, “Learning to segment songs
with ordinal linear discriminant analysis,” in ICASSP.
IEEE, 2014, pp. 5197–5201.

[21] N. Gillis, Nonnegative Matrix Factorization. Philadel-
phia, PA: Society for Industrial and Applied Mathemat-
ics, 2020.

[22] J. Engel et al., “Neural audio synthesis of musical
notes with wavenet autoencoders,” in Int. Conf. Ma-
chine Learning. PMLR, 2017, pp. 1068–1077.

[23] F. Roche, T. Hueber, S. Limier, and L. Girin, “Autoen-
coders for music sound modeling: a comparison of lin-
ear, shallow, deep, recurrent and variational models,”
arXiv preprint arXiv:1806.04096, 2018.

[24] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,
“Gradient-based learning applied to document recog-
nition,” Proc. of the IEEE, vol. 86, no. 11, pp. 2278–
2324, 1998.

[25] I. Goodfellow, Y. Bengio,

Deep Learning.
//www.deeplearningbook.org.

and A. Courville,
http:
2016,

MIT Press,

[39] D. Lee and H. Seung, “Learning the parts of objects by
non-negative matrix factorization,” Nature, vol. 401,
no. 6755, pp. 788–791, 1999.

[26] G. Peeters and G. Richard, “Deep Learning for Audio
and Music,” in Multi-faceted Deep Learning: Models
and Data.
[Online]. Available:
https://hal.telecom-paris.fr/hal-03153938

Springer, 2021.

[27] V. Dumoulin and F. Visin, “A guide to convolu-
tion arithmetic for deep learning,” arXiv preprint
arXiv:1603.07285, 2016.

[28] B. T. Nebgen, R. Vangara, M. A. Hombrados-Herrera,
S. Kuksova, and B. S. Alexandrov, “A neural net-
work for determination of latent dimensionality in non-
negative matrix factorization,” Machine Learning: Sci-
ence and Technology, vol. 2, no. 2, p. 025012, 2021.

[29] J. Paulus, M. M¨uller, and A. Klapuri, “Audio-based
music structure analysis.” in ISMIR. Utrecht, 2010,
pp. 625–636.

[30] G. Sargent, F. Bimbot, and E. Vincent, “A regularity-
constrained viterbi algorithm and its application to the
structural segmentation of songs,” in International So-
ciety for Music Information Retrieval Conference (IS-
MIR), 2011.

[31] F. Bimbot, G. Sargent, E. Deruty, C. Guichaoua, and
E. Vincent, “Semiotic description of music structure:
An introduction to the quaero/metiss structural annota-
tions,” in 53rd Int. Conf. Audio Engineering Soc., 2014.

[32] O. Nieto and J. Bello, “Systematic exploration of com-
putational music structure research.” in ISMIR, 2016,
pp. 547–553.

[33] F. Pedregosa et al., “Scikit-learn: Machine learning
in Python,” Journal of Machine Learning Research,
vol. 12, pp. 2825–2830, 2011.

[34] A. Marmoret and J. Cohen, “nn fac: Nonnegative fac-

torization techniques toolbox,” 2020.

[35] A. Paszke et al., “Pytorch: An imperative style,
high-performance deep learning library,” in Advances
Information Processing Systems 32.
in Neural
Inc., 2019, pp. 8024–8035.
Curran Associates,
[Online]. Available:
http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf

[36] D. Kingma and J. Ba, “Adam: A method for stochastic
optimization,” arXiv preprint arXiv:1412.6980, 2014.

[37] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep
into rectiﬁers: Surpassing human-level performance on
imagenet classiﬁcation,” in Proc. IEEE Int. Conf. Com-
puter Vision, 2015, pp. 1026–1034.

[38] C. Raffel et al., “mir eval: A transparent implementa-
tion of common MIR metrics,” in ISMIR, 2014.

