This work was accepted and presented at the Data Science Meets Optimization (DSO) workshop at the 29th International Joint Conference
on Artiﬁcial Intelligence and 17th Paciﬁc Rim Conference on Artiﬁcial Intelligence (IJCAI-PRICAI) 2020 on January 7, 2021.

Optimizing Hyperparameters in CNNs using Bilevel Programming
for Time Series Data

Taniya Seth , Pranab K. Muhuri

Department of Computer Science, South Asian University, New Delhi, India
taniya.seth@students.sau.ac.in, pranabmuhuri@cs.sau.ac.in

1
2
0
2

n
a
J

9
1

]

G
L
.
s
c
[

1
v
2
9
4
7
0
.
1
0
1
2
:
v
i
X
r
a

Abstract

Hyperparameter optimization has remained a cen-
tral topic within the machine learning community
due to its ability to produce state-of-the-art results.
With the recent interest growing in the usage of
CNNs for time series prediction, we propose the
notion of optimizing Hyperparameters in CNNs for
the purpose of time series prediction. In this po-
sition paper, we give away the idea of modeling
the concerned hyperparameter optimization prob-
lem using bilevel programming.

1 Introduction

Training a machine to perform humanely tasks such as image
recognition and data prediction, involves preparing a good
enough model that learns the given data. This model predom-
inantly involves a training algorithm that is responsible for
this learning task. Furthermore, the job of this training algo-
rithm is to develop a function, which in essence minimizes a
loss on some data samples (a subset of the ground truth data)
introduced to it. This trained model is then applied to the test
data (and out-of-sample subset of the ground truth data), on
which the model is evaluated based on another loss.

Who evaluates the performance of this model? Who de-

cides exactly how much is good enough?

Literature on machine learning has blessed us with answers
to such questions while actually producing good models to
make machines perform various tasks.

The answer to the above question is that, the evaluation
of the training algorithm in the model is done using a train-
ing loss, which identiﬁes the difference between the actual
state of the model’s learning and the training data it is pro-
vided with to learn. The function mentioned earlier in this
section minimizes this difference between the learnt data and
the training data. This is, with respect some parameters of the
model, say θ. The training algorithm gradually learns these
parameters during the concerned process, model weights, for
example.

However, a model also includes the hyperparameters, λ in
the scene, which [Bergstra and Bengio, 2012] refer to as the
“bells and whistles” of a training algorithm. In practice, hy-
perparameters are chosen ﬁrst, which is then followed by the

development of the training algorithm. Due to their impor-
tance and inﬂuence on the training, these hyperparameters
require expert intervention to be chosen.

When optimized values of hyperparameters are supplied to
the training algorithm, it learns well from the training data,
while additionally performing well on the out-of-sample test
data. The performance of the model on the test data is evalu-
ated based on a validation loss, which must be minimized for
the model to generalize well.

This discussion deﬁnes the necessity of hyperparameter
optimization (HO) within machine learning models. This
[Bergstra et
problem has been studied for a long time.
al., 2011] utilized various approaches such as the sequen-
tial model based approach, Gaussian process approach, tree-
structure Parzen estimator approach etc for optimizing the
estimated improvement criteria. Random search was sub-
sequently studied for HO in [Bergstra and Bengio, 2012].
[Thornton et al., 2013] introduced Auto-WEKA for
Later,
the combined selection and HO in classiﬁcation algorithms.
[Eggensperger et al., 2013] put forward an empirical study to
deal with Bayesian optimization for hyperparameters. Most
importantly, gradient-based HO was discussed in [Maclau-
rin et al., 2015], wherein exact gradients of hyperparameters
were computed by chaining their derivatives backwards in the
training procedure through reversible learning. Other works
on HO include [Feurer et al., 2015] and [Li et al., 2017].

Having discussed the problem of HO above, one can no-
tice the dual structure that the problem encompasses. In other
words, performance of a machine learning model is optimized
based on the training and validation losses. This optimization
is subject to the values chosen for hyperparameters, λ of the
model. This can be stated as the following: the validation loss
of a model is minimized with respect to minimized training
loss, for the model which is parameterized by the hyperpa-
rameters.

Such a dual structure is noticed in multiple real-life sit-
uations, which can be modeled using the bilevel program-
ming strategy [Bard, 2013]. Solving these problems follow
a leader-follower approach, inspired from the game theory
[Von Stackelberg and Von, 1952]. Within these problems, the
solution space of the objective function (OF) of the leader
is constrained by that of the follower problem. Hence, a
proper solution is sought that satisﬁes both the leader’s and
follower’s solution space while optimizing their individual

 
 
 
 
 
 
This work was accepted and presented at the Data Science Meets Optimization (DSO) workshop at the 29th International Joint Conference
on Artiﬁcial Intelligence and 17th Paciﬁc Rim Conference on Artiﬁcial Intelligence (IJCAI-PRICAI) 2020 on January 7, 2021.

objectives.

Recently, the idea of HO using bilevel programming was
proposed in [Franceschi et al., 2018]. Franceschi and co-
authors developed a bilevel optimization framework for HO.
Upon formulating the bilevel for HO, they observed that it is
difﬁcult to obtain a solution to the bilevel model, especially
when λ is a real-valued vector of hyperparameters. To over-
come this, the exact problem of the bilevel model was ap-
proximated and later proven to guarantee solutions.

In the literature so far, the problem of HO has been dealt
with mostly for the cases of images. In today’s world, time
series is available in abundance. From stock market to daily
average temperatures, human activity data and now most im-
portantly COVID-19 data, everything is available as a time
series. Leveraging such series for either classiﬁcation or pre-
diction is crucial. Convolutional neural networks (CNN) have
been utilized for both classiﬁcation and prediction purposes
on time series data.

[Zheng et al., 2014] time series data utilizing multiple
channels deep CNNs with special attention to exploration of
feature learning techniques. In [Yang et al., 2015], classiﬁ-
cation of the human activity recognition (HAR) is done using
deep CNNs, whereas in [Cui et al., 2016], time series classi-
ﬁcation is done using multiscale CNNs. Other recent works
on time series forecast and classiﬁcation using CNNs in-clude
[Borovykh et al., 2017] and [Yazdanbakhsh and Dick, 2019].
Keeping an eye on the relevance of time series prediction in
today’s world, one can observe that the literature lacks works
where a machine learning model has been optimized for per-
formance on time-series data.

Hence, in this position paper, we propose the idea of utiliz-
ing bilevel programming for HO within CNNs for time se-
ries prediction. We ﬁrst introduce the bilevel framework to
model the overall performance of the machine learning model
in terms of the training and validation loss. This is done
in Section 2. Subsequently, in the same section, we revisit
the approximation strategy for the bilevel framework of HO,
along with the gradient-based approach to solve the problem.
In Section 3, we introduce our proposed framework of uti-
lizing bilevel programming for HO in CNNs for time series
prediction. We conclude the position paper in Section 4.

2 Preliminary Knowledge
2.1 Bilevel programming framework
In this section, we revisit the structure of the bilevel program-
ming framework for a machine learning model. As speciﬁed
in [Franceschi et al., 2018], bilevel programming problems
of the following forms are considered:

min{f (λ) : Λ ∈ λ}

(1)

where,

f (λ) = inf{E(wλ, λ) : wλ ∈ arg min
u∈Rd

Lλ(u)}

(2)

In the above equations, f : Λ → R is deﬁned at λ ∈ Λ.
E : Rd × Λ → R is the leader objective. Also, ∀λ ∈ Λ,
Lλ : Rd → R is the follower objective given that Lλ : λ ∈ Λ
is the class of OFs parameterized by λ.

2.2 Bilevel programming framework for HO
As mentioned earlier, the validation error is sought to be min-
imized for a machine learning model. Let the model be de-
noted as gw : X → Y Let it be parameterized by the vector
w, with respect to one vector of hyperparameters λ. For a
predeﬁned loss function l, the leader and follower objectives
can be given as follows:

E(w, λ) = Σ(x,y)∈Dvalidationl(gw(w), y)

(3)

Lλ(w) = Σ(x,y)∈Dtrainl(gw(w), y) + penalty

(4)

Here, Dvalidation is the validation data presented to gw, for
evaluation after it has been trained on Dtrain. The penalty
term can be implemented as a regularizer for the network
model to improve the performance.

2.3 Gradient based approach to solve bilevel

optimization for HO

[Franceschi et al., 2018], speciﬁed an approximation of the
bilevel problem given in (1) and (2). It is given as follows:

min
λ

fT (λ) = E(wT,λ, λ)

(5)

w0,λ = φ0(λ), wt,λ = φt(wt−1,λ, λ), t ∈ [T ]
In the above equations, [T ] is a predeﬁned positive integer
such that [T ] = {1, . . . , T }, φ0 : Rm → Rd is a smooth
initialization dynamic, and ∀t ∈ [T ], Rd × Rm → Rdis a
smooth mapping the operation of an optimization algorithm
at the tth step. The optimization dynamic φ is implemented
using the gradient descent optimization algorithm.

(6)

In [Franceschi et al., 2018], certain assumptions are cho-
sen to reduce the bilevel framework given in (1)-(2), to prove
the existence of solutions of the reduced problem and also the
existence of the convergence of approximate problems to the
reduced problem. They are omitted from this position paper
for simplicity.

3 HO using bilevel programming within

CNNs

We discuss our proposed idea in this section.

We ﬁrst deﬁne our CNN model for classiﬁcation purposes.
For our time series data, we utilize 1D convolutional layers,
which are ﬁt for situations dealing with time series informa-
tion.

For explanation, we utilize a time series dataset with 128
time steps and 9 features of data. Our deep CNN model
for this time series data begins with an input layer, followed
by two 1D convolutional layers each encompassing 64 ﬁlters
with a ﬁlter size of 3. Both layers have the ReLU activation
applied. These are followed by a dropout layer with a 50%
dropout rate, followed by a max pooling layer. The output
from the max pooling layer is then ﬂattened and forwarded to
a dense layer with 100 connections and the ReLU activation,
followed by a ﬁnal dense layer with 9 output units and the
softplus activation. The model structure is depicted in Fig. 1.

This work was accepted and presented at the Data Science Meets Optimization (DSO) workshop at the 29th International Joint Conference
on Artiﬁcial Intelligence and 17th Paciﬁc Rim Conference on Artiﬁcial Intelligence (IJCAI-PRICAI) 2020 on January 7, 2021.

produce state-of-the-art results in terms of MSE.

We plan to implement this scenario in on a machine with
the following speciﬁcations:
Intel Core 140 i3-6100 CPU
with 12 GB of RAM and Windows 10 OS. The GPU em-
ployed is the NVIDIA GeForce 141 GTX 1660 Super.

4 Conclusions and Future Work

In this position paper, we have introduced the idea of using
bilevel programming for HO within CNNs for time series
data. Since the literature on HO for time series prediction
or classiﬁcation tasks is scarce, we believe that the idea pre-
sented here will mark a good start in the research in this di-
rection.

We utilized a deep CNN architecture to deﬁne the model
for the purpose of time series prediction. Based on this, we
deﬁned a framework for the bilevel programming problem
that must be solved to obtain the better results than most of
the existing models.

Our subsequent plans are to implement the scenario intro-
duced within this position paper. Within this implementa-
tion, we shall perform a sensitivity analysis on different val-
ues of T , to obtain varied results. We also plan to compare
the impact of HO using bilevel programming within the pre-
diction and classiﬁcation tasks on time series data. We plan to
perform our experiments, on the human activity recognition
(HAR) data to observe the results.

References

[Bard, 2013] Jonathan F Bard. Practical bilevel optimiza-
tion: algorithms and applications, volume 30. Springer
Science & Business Media, 2013.

[Bergstra and Bengio, 2012] James Bergstra and Yoshua
Bengio. Random search for hyper-parameter optimization.
The Journal of Machine Learning Research, 13(1):281–
305, 2012.

[Bergstra et al., 2011] James Bergstra, R´emi Bardenet,
Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-
parameter optimization. Advances in neural information
processing systems, 24:2546–2554, 2011.

[Borovykh et al., 2017] Anastasia Borovykh, Sander Bohte,
and Cornelis W Oosterlee. Conditional time series fore-
casting with convolutional neural networks. arXiv preprint
arXiv:1703.04691, 2017.

[Cui et al., 2016] Zhicheng Cui, Wenlin Chen, and Yixin
Chen. Multi-scale convolutional neural networks for time
arXiv preprint arXiv:1603.06995,
series classiﬁcation.
2016.

[Eggensperger et al., 2013] Katharina

Eggensperger,
Matthias Feurer, Frank Hutter, James Bergstra, Jasper
Snoek, Holger Hoos, and Kevin Leyton-Brown. Towards
an empirical foundation for assessing bayesian optimiza-
tion of hyperparameters. In NIPS workshop on Bayesian
Optimization in Theory and Practice, volume 10, page 3,
2013.

Figure 1: Network structure for time series prediction using a deep
CNN architecture

For this model and data, we consider the example weights
(w) and the learning rate (lr) of the neurons as the hyper-
parameters to be optimized. The metric to be minimized is
given by the model is the Mean Squared Error (MSE), while
the optimizer utilized is the Adam optimizer.

With this scenario deﬁned, the bilevel programming frame-
work for our CNN model for the time series data is described
below. For the following, λ = {w, lr} and T = 200.

min
λ

fT (λ) = Adam(wT,λ, λ)

and,

φ0(λ) = {w0 = [0], lr = 0.01},
φt(wt−1,λ, λ) = wt − ηt∇Lλ, t ∈ [T ]

(7)

(8)

The follower level optimizer is deﬁned by the gradient de-
scent optimizer as given in [Franceschi et al., 2018]. This
follower level optimizer is deﬁned for the hyperparameter, lr.
While the w is the minimizer for the problems in ( 7)-( 8).

We believe that solving this bilevel problem to obtain the
optimized value of lr with respect to the minimizer w, shall

This work was accepted and presented at the Data Science Meets Optimization (DSO) workshop at the 29th International Joint Conference
on Artiﬁcial Intelligence and 17th Paciﬁc Rim Conference on Artiﬁcial Intelligence (IJCAI-PRICAI) 2020 on January 7, 2021.

[Feurer et al., 2015] Matthias Feurer, Jost Springenberg, and
Initializing bayesian hyperparameter opti-
In Proceedings of the AAAI

Frank Hutter.
mization via meta-learning.
Conference on Artiﬁcial Intelligence, volume 29, 2015.
[Franceschi et al., 2018] Luca Franceschi, Paolo Frasconi,
Saverio Salzo, Riccardo Grazzi, and Massimilano Pontil.
Bilevel programming for hyperparameter optimization and
meta-learning. arXiv preprint arXiv:1806.04910, 2018.
[Li et al., 2017] Lisha Li, Kevin Jamieson, Giulia DeSalvo,
Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:
A novel bandit-based approach to hyperparameter opti-
mization. The Journal of Machine Learning Research,
18(1):6765–6816, 2017.

[Maclaurin et al., 2015] Dougal Maclaurin, David Duve-
naud, and Ryan Adams. Gradient-based hyperparame-
In Inter-
ter optimization through reversible learning.
national Conference on Machine Learning, pages 2113–
2122, 2015.

[Thornton et al., 2013] Chris Thornton, Frank Hutter, Hol-
ger H Hoos, and Kevin Leyton-Brown. Auto-weka: Com-
bined selection and hyperparameter optimization of clas-
In Proceedings of the 19th ACM
siﬁcation algorithms.
SIGKDD international conference on Knowledge discov-
ery and data mining, pages 847–855, 2013.

[Von Stackelberg and Von, 1952] Heinrich Von Stackelberg
and Stackelberg Heinrich Von. The theory of the market
economy. Oxford University Press, 1952.

[Yang et al., 2015] Jianbo Yang, Minh Nhut Nguyen,
Phyo Phyo San, Xiaoli Li, and Shonali Krishnaswamy.
Deep convolutional neural networks on multichannel time
series for human activity recognition. In Ijcai, volume 15,
pages 3995–4001. Buenos Aires, Argentina, 2015.

[Yazdanbakhsh and Dick, 2019] Omolbanin Yazdanbakhsh
and Scott Dick. Multivariate time series classiﬁcation us-
ing dilated convolutional neural network. arXiv preprint
arXiv:1905.01697, 2019.

[Zheng et al., 2014] Yi Zheng, Qi Liu, Enhong Chen, Yong
Ge, and J Leon Zhao. Time series classiﬁcation using
multi-channels deep convolutional neural networks. In In-
ternational Conference on Web-Age Information Manage-
ment, pages 298–310. Springer, 2014.

