Received: Added at production

Revised: Added at production

Accepted: Added at production

DOI: xxx/xxxx

ARTICLE TYPE

Feature Grouping and Sparse Principal Component Analysis with
Truncated Regularization

Haiyan Jiang1 | Shanshan Qin*2 | Oscar Hernan Madrid Padilla3

1School of Statistics and Data Science,
Nankai University, Tianjin, China.
Email: jianghaiyan.cn@gmail.com
2School of Statistics, Tianjin University
of Finance and Economics, Tianjin,
China. Email: qinsslzu@gmail.com
3Department of Statistics, University of
California, Los Angeles, CA, United
States.
Email: oscar.madrid@stat.ucla.edu

Correspondence

*Shanshan Qin. Email:

qinsslzu@gmail.com

Summary

In this paper, we consider a new variant for principal component analysis (PCA), aiming to

capture the grouping and/or sparse structures of factor loadings simultaneously. To achieve

these goals, we employ a non-convex truncated regularization with naturally adjustable

sparsity and grouping eﬀects, and propose the Feature Grouping and Sparse Principal

Component Analysis (FGSPCA). The proposed FGSPCA method encourages the factor

loadings with similar values to collapse into disjoint homogeneous groups for feature group-

ing or into a special zero-valued group for feature selection, which in turn helps reducing

model complexity and increasing model interpretation. Usually, existing structured PCA

methods require prior knowledge to construct the regularization term. However, the pro-

posed FGSPCA can simultaneously capture the grouping and/or sparse structures of factor

loadings without any prior information. To solve the resulting non-convex optimization

problem, we propose an alternating algorithm that incorporates the diﬀerence-of-convex

programming, augmented Lagrange method and coordinate descent method. Experimen-

tal results demonstrate the promising performance and eﬃciency of the new method on

both synthetic and real-world datasets. An R implementation of FGSPCA can be found

on github https://github.com/higeeks/FGSPCA.

KEYWORDS:
Principal Component Analysis, Non-convex Truncated Regularization, Feature Grouping,

Feature Selection, Sparsity

1

INTRODUCTION

Principal component analysis (PCA) (Jolliﬀe 1986) is an important unsupervised technique for feature extraction and dimension reduction, with

numerous applications in statistics and machine learning, such as gene representation and face recognition. The goal of PCA is to ﬁnd a sequence of

linear combinations of the original variables/predictors by projecting the original data onto an orthogonal linear space, called principal components

(PCs), such that the derived PCs capture the maximum variance along the orthogonal direction. Numerically, PCA can be obtained via the singular
value decomposition (SVD) of the data matrix. Denote Xn×p ∈ Rn×p a data matrix consisting of n observations of a random vector x ∈ Rp with
a population covariance matrix Σ ∈ Rp×p, where n and p are the number of observations and the number of variables/predictors, respectively.
Without loss of generality, assume that all the predictors are centered with 0 means. Let the SVD of X be X = UDVT. The projection of the data
Z = UD(= XV) are the derived PCs, and the columns of V are the corresponding factor loadings (or factor coeﬃcients, or PC vectors).

PCA aims to recover the top k leading eigenvectors u1, · · · , uk of the population covariance matrix Σ, with the corresponding eigenvalues
λ1 ≥ · · · ≥ λk. In high dimensional settings with p (cid:29) n, the ordinary PCA can be inconsistent (Johnstone & Lu 2009; Nadler 2008; Paul 2007), and
additional assumptions are needed to avoid the curse of dimensionality (Z. Wang, Lu, & Liu 2014). Besides, a simple property of the ordinary PCA is

2
2
0
2

p
e
S
3
1

]
E
M

.
t
a
t
s
[

2
v
5
8
6
3
1
.
6
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Jiang et al

that each PC usually involves all the original variables and the loadings (factor coeﬃcients) are typically nonzero, which hinders the interpretability

of the derived PCs. In order to deal with the curse of dimensionality and improve the interpretability of the derived PCs, a sparsity assumption is

often imposed on the loadings to get a sparsely weighted linear combination of the original variables. PCA with sparse loadings and its variants (Cai,

Ma, & Wu 2013; Erichson et al. 2020; Vu, Cho, Lei, & Rohe 2013; Zou, Hastie, & Tibshirani 2006) have been widely studied. In the last decades,

signiﬁcant progress has been made on the methodological development as well as theoretical understanding of sparse PCA. One can turn to Croux,

Filzmoser, and Fritz (2013); Erichson et al. (2020); Grbovic, Dance, and Vucetic (2012); Jenatton, Audibert, and Bach (2011); Jenatton, Obozinski,

and Bach (2010); Jin and Sidford (2019); Khan, Shafait, and Mian (2015); Tian, Nie, Wang, and Li (2020); Z. Wang et al. (2014); Yi, Lai, He,

Cheung, and Liu (2017); R. Zhang and Tong (2019); Zou and Xue (2018), among others, for an overview of the literature. Methods introduced in

these articles intend to seek modiﬁed principal components for various sparsity properties. For example, SCoTLASS (Jolliﬀe, Trendaﬁlov, & Uddin
2003) is proposed by directly imposing an (cid:96)1 penalty on the ordinary PC vectors to get sparse loadings. Sparse PCA (SPCA) (Zou et al. 2006) seeks
sparse loadings by extending the elastic net (Zou & Hastie 2005) procedure and relaxing the orthogonality constraint of the ordinary PC vectors.

In addition to the sparsity property among loadings, structured grouping property can also lead to good interpretability of the resulting PCs.

SPCA (Zou et al. 2006) can achieve better interpretability by producing modiﬁed PCs with sparse loadings. However, it does not take into account

the structured grouping property among loadings, i.e., clusters or groups. Based on the structured variable selection method (Jenatton et al. 2011),

a structured sparse PCA (Jenatton et al. 2010) is proposed to explore the structural information, as an extension of sparse PCA, and it incorporates

prior knowledge into the sparsity-inducing regularization and is able to encode more sophisticated sparsity patterns. In order to capture the

‘blocking’ structures in the factor loadings, Guo, James, Levina, Michailidis, and Zhu (2010) proposed another variant of PCA with sparse fused

loadings, named sparse fused PCA (SFPCA), by introducing a fusion penalty that encourages the loadings associated with high correlation to be close

to get the ‘blocking’ structures. Recently, Tian et al. (2020) proposed the feature-sparsity (row-sparsity) constrained PCA by considering feature-

sparsity structures for feature selection and PCA simultaneously. However, these methods depend heavily on the structured prior knowledge

which is usually challenging to obtain or specify in real applications. In Guo et al. (2010), for example, the ‘blocking’ structure is captured by the

fusion penalty, where the fusion penalty depends on the sample correlation which serves as the prior information. Moreover, even though the PC

vectors derived from the structured sparse PCA possess some sparse structures, they suﬀer from the same issue, that is, the structured sparsity

depends on the given structural prior information.

In the ordinary PCA, each PC is a linear combination of all p variables, and the loadings are typically nonzero and have no grouping eﬀect. As is
discussed above, the loadings can be sparse in sparse PCA (Zou et al. 2006), but dismissing grouping eﬀect or clustering eﬀect among the loadings.

In structured PCA (Guo et al. 2010; Jenatton et al. 2010; Tian et al. 2020), the structures of the loadings can be learned based on the structural

prior knowledge/information which should be given to construct the regularization term in these methods.

In this paper, we propose a new variant of PCA, named feature grouping and sparse principal component analysis (FGSPCA), which can simul-

taneously capture the grouping and sparse structure of factor loadings, leading to modiﬁed PCs with grouping- and sparse-guided loadings. By

adopting the fact that PCA can be formulated as a regression-type optimization problem, the grouping- and sparse-guided loadings are obtained

by imposing the grouping and sparsity constraints on the regression coeﬃcients. We make the following contributions.

• To our knowledge, we initially consider simultaneously the grouping eﬀect as well as the sparsity eﬀect among factor loadings of PCA

in the absence of prior knowledge. The proposed FGSPCA method achieves the goal of feature grouping and feature selection through

regularization, whose construction does not depend on any prior knowledge. The grouping and sparsity structure is learned naturally from

the model rather than from given prior information.

• The proposed FGSPCA method imposes a non-convex regularization term with naturally adjustable sparsity and grouping eﬀect. We solve

the non-convex FGSPCA problem approximated by a sequence of linear convex subproblems via the diﬀerence-of-convex programming

(DC). Each of the convex subproblems is solved iteratively by incorporating augmented Lagrange method (AL), and coordinate descent

method (CD).

• The experiments on both synthetic and real-world data demonstrate the promising performance of the proposed FGSPCA method.

Through out this paper, we use the following notations. Bold-face lower-case letters refer to vectors, e.g. α, β, and bold-face upper-case letters
2 = (cid:80)p
refer to matrices, e.g. A, B. For a vector w ∈ Rp, denote by (cid:107)w(cid:107)2
j=1 |wj|, and (cid:107)w(cid:107)∞ = maxj∈{1,...,p}{|wj|} the
denote the Frobenius norm. Note
squared (cid:96)2 norm, the (cid:96)1 norm, and the maximum norm, respectively. For a matrix W, let (cid:107)W(cid:107)F =
that (cid:107)W(cid:107)2

F = tr(WTW). Denote by Ik×k the identity matrix in Rk×k. Let I{A} = 1 if the condition A holds, otherwise I{A} = 0.

, (cid:107)w(cid:107)1 = (cid:80)p

j=1 w2
j

i,j w2
ij

(cid:113)(cid:80)

The rest of the paper is organized as follows. In Sect. 2, the PCA is revisited. Sect. 3 introduces the proposed FGSPCA and its connections to

other sparse PCA variants. We propose an alternating algorithm to solve the FGSPCA problem in Sect. 4. Experiments to show the performance

of FGSPCA and comparisons with other dimension reduction methods are presented in Sect. 5. A discussion on the extension of FGSPCA to the

settings with non-negative loadings falls into Sect. 6. We conclude the paper in Sect. 7.

Jiang et al

3

2

PRINCIPAL COMPONENT ANALYSIS REVISITED

Let X = (xij)n×p denote a data matrix with n observations and p variables. Assume that the columns of X are all centered. In PCA, each PC is
obtained by constructing linear combinations of the original variables that maximize the variance. Denote the SVD of X by X = UDVT. Let
Zj = UjDjj be the j-th PC, and columns of V = [V1, · · · , Vk] be the PC vectors or PC loadings. Except for the SVD decomposition, another way
to derive the PC vectors is to solve the following constrained least squares problem,

min
A

(cid:107)X − XAAT (cid:107)2

2 , s.t. AT A = Ik×k ,

(2.1)

where A = [α1, · · · , αk] is a p × k matrix with orthogonal columns. The estimated (cid:98)A contains the ﬁrst k PC vectors, and the projection of the
data (cid:98)Z = X (cid:98)A are the ﬁrst k PCs.

By relaxing the orthogonality requirement and imposing an (cid:96)2 penalty, Zou et al. (2006) proposed and reformulated PCA as the following
regularized regression optimization problem which is deﬁned in Lemma 1 (Theorem 3 in Zou et al. (2006), see Appendix A in Supporting information).

Lemma 1. Consider the ﬁrst k principal components. Let xi be the i-th row of data matrix X. Denote Ap×k = [α1, · · · , αk], Bp×k = [β1, · · · , βk].
For any λ > 0, let

( (cid:98)A, (cid:98)B) = arg min

A,B

n
(cid:88)

i=1

(cid:107)xi − ABT xi(cid:107)2

2 + λ

k
(cid:88)

j=1

(cid:107)βj (cid:107)2

2 , s.t. AT A = Ik×k .

(2.2)

Then (cid:98)βj ∝ Vj

for j = 1, · · · , k.

The PCA problem is transformed into a regression-type optimization problem with orthonormal constraints on A, and all the sequences of
2 =
, whose minimizer under the orthonormal constraint on A consists exactly of the ﬁrst k PC vectors of the ordinary PCA.

principal components can be derived through Lemma 1. With the restriction B = A, the objective function becomes (cid:80)n
(cid:80)n

i=1 (cid:107)xi − ABTxi(cid:107)2

i=1 (cid:107)xi − AATxi(cid:107)2
2

Lemma Lemma 1 shows that the exact PCA can still be obtained by relaxing the restriction B = A and adding the ridge penalty term.

Note that

n
(cid:88)

i=1

(cid:13)
(cid:13)xi − ABT xi
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

=

(cid:13)
(cid:13)

(cid:13)X − XBAT (cid:13)

2
(cid:13)
(cid:13)

F

.

let A⊥ be any orthonormal matrix such that

is p × p orthonormal. Then we have
. Suppose that A is given, then the optimal B can be obtained by minimizing

[A; A⊥]

j=1 (cid:107)Xαj − Xβj(cid:107)2

2 + (cid:107)XA⊥(cid:107)2

F

Since A is orthonormal,
(cid:13)X − XBAT(cid:13)
(cid:13)
2
F = (cid:80)k
(cid:13)
(cid:80)k
j=1 (cid:107)Xαj − Xβj(cid:107)2

2 + λ (cid:80)k

j=1 (cid:107)βj(cid:107)2
2

, which is equivalent to k independent ridge regression problems.

3

THE METHODS

3.1

Feature Grouping and Sparse Loadings

In order to investigate the structures among loadings, we extend the optimization problem (2.2) by imposing feature grouping and feature selec-

tion penalties simultaneously, to get feature grouping and sparse loadings simultaneously. The proposed FGSPCA model is based on solving the
following optimization problem,

min
A,B

n
(cid:88)

i=1

(cid:107)xi − ABT xi(cid:107)2

2 + λ

k
(cid:88)

j=1

(cid:107)βj (cid:107)2

2 + λ1

k
(cid:88)

j=1

p1(βj ) + λ2

k
(cid:88)

j=1

p2(βj ) , s.t. AT A = Ik×k,

where p1(β) and p2(β) are regularization functions, taking the following penalty forms,

p1(βj ) =

p
(cid:88)

l=1

min

(cid:26) |βl(j)|
τ

(cid:27)

, 1

,

p2(βj ) =

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:26) |βl(j) − βl(cid:48)(j)|
τ

(cid:27)

, 1

,

(3.1)

(3.2)

where βl(j) denotes the l-th element of the vector βj
. λ1(> 0), λ2(> 0) are the corresponding tuning parameters, and τ > 0 is a thresholding
parameter which determines when a small coeﬃcient or a small diﬀerence between two coeﬃcients will be penalized. The notation E ⊂ {1, · · · , p}2
refers to a set of edges on a fully connected and undirected graph (complete graph), with l ∼ l(cid:48) indicating an edge directly connecting two distinct
nodes l (cid:54)= l(cid:48), where each node represents a variable. Fig. 1 gives a comparison of diﬀerent penalty functions and their thresholding functions. We
refer the reader to Appendix B in Supporting information for more structured sparsity regularization functions.

Remark 1. A). The key point of the FGSPCA with p1(·) and p2(·) penalty functions can be viewed as performing feature selection and feature
grouping simultaneously. B). As shown in Shen, Huang, and Pan (2012), the truncated (cid:96)1-function min( |βl|
τ , 1) can be regarded as a non-convex
and non-smooth surrogate of (cid:96)0-function I(βl
(cid:54)= 0) when τ → 0. Besides, the selection consistency can be achieved by the (cid:96)0-penalty and its
surrogate—the truncated (cid:96)1-penalty (Dai, Dai, Huang, Kang, & Lu 2021; Shen, Pan, Zhu, & Zhou 2013). Therefore, the sparse PCA with (cid:96)1 penalty

4

Jiang et al

(a) Penalty Functions

(b) Thresholding Functions

FIGURE 1 Comparison of diﬀerent penalty functions (left panel): the (cid:96)1-function (solid line), the truncated (cid:96)1-function (dashed line), and the (cid:96)0-
function (dotted line), and their corresponding thresholding functions (right panel). The truncated (cid:96)1-function min{ |x|
τ , 1} approximates the (cid:96)0-
function I(x (cid:54)= 0) as τ → 0, and it is closer to the (cid:96)0 penalty than the (cid:96)1 penalty. The thresholding functions show that, compared to the (cid:96)1 penalty,
the truncated (cid:96)1 penalty penalizes more aggressively with small coeﬃcients preferred, and it has no bias with large coeﬃcients.

cannot achieve selection consistency. The intuition is that compared to the (cid:96)1 penalty, the truncated (cid:96)1 penalty is closer to the (cid:96)0 penalty and
penalizes more aggressively with small coeﬃcients preferred. Meanwhile, the truncated (cid:96)1-function min( |βl|
τ , 1) can be a good approximation of
(cid:96)1-function as τ → ∞. C). One may use the (cid:96)1-function |βl| as a smooth approximation of (cid:96)0-function. However, the shrinkage bias tends to be
larger as parameter size gets larger (Wu, Liu, & Wen 2018; Yun, Zheng, Yang, Lozano, & Aravkin 2019) since the (cid:96)1 penalty is proportional to
the size of parameters. The smooth approximation, (cid:96)1-function, has the drawback of producing biased estimates for large coeﬃcients and lacking
oracle property (Fan & Li 2001; C.-H. Zhang & Huang 2008).

3.2

Connection to Sparse PCA Variants

By relaxing the orthogonality requirement and extending the elastic net procedure, the sparse PCA (SPCA) (Zou et al. 2006) solves the following

regularized optimization problem,

( (cid:98)A, (cid:98)B) = arg min

A,B

n
(cid:88)

i=1

(cid:107)xi − ABTxi(cid:107)2

2 + λ

k
(cid:88)

j=1

(cid:107)βj(cid:107)2

2 + λ1

k
(cid:88)

j=1

(cid:107)βj(cid:107)1

subject to ATA = Ik×k.

(3.3)

Note that the optimization problem in (3.3) is a special case of (3.1) as τ → ∞ and λ2 = 0. By imposing a fusion penalty, the sparse fused PCA
(SFPCA) with sparse fused loadings (Guo et al. 2010) solves the following regularized optimization problem,

min
A,B

n
(cid:88)

(cid:107)xi − ABTxi(cid:107)2

2 + λ

k
(cid:88)

(cid:107)βj(cid:107)2

2 + λ1

i=1
subject to ATA = Ik×k ,

j=1

k
(cid:88)

j=1

(cid:107)βj(cid:107)1 + λ2

k
(cid:88)

(cid:88)

j=1

s(cid:54)=t

|ρs,t||βs(j) − sign(ρs,t)βt(j)|

(3.4)

where ρs,t denotes the sample correlation between variables Xs and Xt, and sign(x) returns the sign of x. For fair comparison, we add an (cid:96)2 penalty
to the objective function of the SFPCA criterion. The SFPCA (Guo et al. 2010) can obtain sparse fused loadings in a more interpretive way, where

−2−10120.00.20.40.60.81.01.2L1−functionTruncated L1L0−function−1.0−0.50.00.51.0−1.0−0.50.00.51.0L1−functionTruncated L1Jiang et al

5

the fusion penalty depends on the sample correlation, serving as prior knowledge. Therefore, the SFPCA encourages the loadings associated with

high correlation to have the same magnitude.

4

THE ALGORITHMS

4.1

Alternating Optimization Algorithm of FGSPCA

In this section, we discuss the algorithms to optimize the proposed objective function in (3.1). An alternating optimization algorithm over A and
B is employed, analogously to the SPCA algorithm (Zou et al. 2006) and SFPCA algorithm (Guo et al. 2010). Specially, the alternating algorithm to
solve the optimization problem (3.1) proceeds as follows.

Algorithm 1. The FGSPCA Algorithm.

Step 1.

Initialize (cid:98)A by setting it to be V[, 1 : k], the ﬁrst k ordinary PC vectors.

Step 2.

(Estimation of B given A). Given a ﬁxed A = [α1, · · · , αk], minimizing the objective function (3.1) over B is equivalent to solving the
following k separate subproblems, for j = 1, · · · , k,

(cid:98)βj = arg min

βj

(cid:107)Zj − Xβj (cid:107)2

2 + λ(cid:107)βj (cid:107)2

2 + λ1

p
(cid:88)

l=1

min

(cid:18) |βl(j)|
τ

(cid:19)

, 1

+ λ2

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |βl(j) − βl(cid:48)(j)|
τ

(cid:19)

, 1

,

(4.1)

where Zj = Xαj. The optimization of (4.1) is discussed in Sect. 4.2. In this step, we update B and obtain the estimate (cid:98)B = [(cid:98)β1, · · · , (cid:98)βk].

Step 3.

(Estimation of A given B). Given a ﬁxed B = [β1, · · · , βk], minimizing the objective function (3.1) over A is equivalent to solving the
following problem,

(cid:13)
(cid:13)

(cid:13)X − XBAT (cid:13)

(cid:13)
(cid:13)

min
A

2

F

s.t. AT A = Ik×k.

(4.2)

The solution to (4.2) can be obtained through a reduced rank Procrustes Rotation (Theorem 4 in Zou et al. (2006), see Appendix C in
Supporting information). We compute the SVD of XTXB as XTXB = UDVT, then the solution of (4.2) is derived by (cid:98)A = UVT. In this
step, we update A and obtain the estimate (cid:98)A = [ (cid:98)α1, · · · , (cid:98)αk].

Step 4. Repeat Steps 2—3 until convergence.

Remark 2. A). The initialization of A, V[, 1 : k], can be loadings of any PCA method. For simplicity, let V[, 1 : k] be the ﬁrst k ordinary PC loadings.
Clearly, V[, 1 : k] can also be initialized as the ﬁrst k PC loadings of SPCA (Zou et al. 2006), or the ﬁrst k PC loadings of SFPCA (Guo et al. 2010). B).
The convergence criterion in Step 4 can be veriﬁed by that the diﬀerence between two adjacent iterations of B is small. We use Frobenius norm
to measure the matrix diﬀerence, that is, (cid:107)B1 − B2(cid:107)2

F ≤ (cid:15), where (cid:15) is a small positive value, say, 1e-5.

4.2

Estimation of B given A

Eﬃciently solving the subproblem (4.1) plays a key role in solving the problem (3.1). The objective function (4.1) is a special case of a regularized

regression problem with feature grouping and sparsity constraints (FGS). Thus this section gives an algorithm for the FGS problem, which is a core

part of the Algorithm 1. The general form of the FGS problem is stated as follows,

min
β

n
(cid:88)

i=1

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l + λ1

p
(cid:88)

l=1

min

(cid:18) |βl|
τ

(cid:19)

, 1

+ λ2

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |βl − βl(cid:48) |
τ

(cid:19)

, 1

.

(4.3)

Since the above problem (4.3) is a non-convex optimization problem, we employ the diﬀerence-of-convex programming (DC) (An & Tao 2005). Our
algorithmic solution for (4.3) is an extension of the algorithms in Qin, Ding, Wu, and Liu (2020); Shen et al. (2012) by adding the (cid:96)2 penalty. Our
main technical contribution is to extend the algorithm in Qin et al. (2020); Shen et al. (2012) to applications of developing more interpretable PCA.

We propose an integrated algorithm for the estimation of B given A (algorithm 1) which integrates the diﬀerence-of-convex algorithm (DC), the
augmented Lagrange method and coordinate descent method (AL-CD), for eﬃcient computation. The procedure to solve the FGS problem consists

of three steps. First, the non-convex objective function is decomposed into a diﬀerence of two convex functions using DC. Then a sequence of
approximations of the trailing convex function is constructed with its aﬃne minorization (through linearizing). Second, a quadratic problem with
equality constraints is converted to an unconstrained version with slack variables, which is subsequently reconstructed by the augmented Lagrange

method. Third, the unconstrained optimization problem is solved via coordinate descent method. The detailed derivation procedures of DC, AL-CD
are given in Appendix D in Supporting information. For simplicity, only the derived results are provided.

Denote βll(cid:48) = βl − βl(cid:48) and deﬁne ξ = (β1, · · · , βp, β12, · · · , β1p, · · · , β(p−1)p). Then update (cid:98)ξ

(m,k) by the following formulas, for k = 1, 2, · · ·

6

Jiang et al

• Given ˆβ(m,k−1)

l

, update ˆβ(m,k)

l

(l = 1, 2, · · · , p) by

ˆβ(m,k)
l

= α−1γ,

where α = 2λ + 2 (cid:80)n
i=1 x2
sign(x)(|x| − δ)+ is the soft threshold function, and

il + ν(k) (cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)

(cid:12). And γ = γ∗ if | ˆβ(m−1)

l

| ≥ τ ; otherwise, γ = ST(γ∗, λ1

τ ) . Here ST(x, δ) =

γ∗ = 2

n
(cid:88)

i=1

xilb(m,k)

i(−l) −

(cid:88)

ll(cid:48) + ν(k) (cid:88)
τ (k)

(cid:16) ˆβ(m,k)
l(cid:48)

+ ˆβ(m,k)
ll(cid:48)

(cid:17)

,

(l,l(cid:48))∈E(m−1)

(l,l(cid:48))∈E(m−1)

where b(m,k)

i(−l) = yi − xT

i(−l) (cid:98)β

(m,k)
(−l)

; xi(−l) is the vector xi without the l-th component, E (m−1) = {(l, l(cid:48)) ∈ E, | ˆβ(m−1)

l

− ˆβ(m−1)
l(cid:48)

| < τ } .

• Given ˆβ(m,k−1)

ll(cid:48)

, update ˆβ(m,k)

ll(cid:48)

(1 ≤ l < l(cid:48) ≤ p) (with ˆβ(m,k)

l

already updated and ﬁxed). Then

ˆβ(m,k)
ll(cid:48)

=






1
ν(k) ST
ˆβ(m−1)
ll(cid:48)

(cid:16)
ll(cid:48) + ν(k)( ˆβ(m,k)
τ (k)

l

− ˆβ(m,k)
l(cid:48)

), λ2
τ

(cid:17)

if (l, l(cid:48)) ∈ E (m−1),

if (l, l(cid:48)) (cid:54)∈ E (m−1).

(4.4)

The process of coordinate descent iterates until convergence, satisﬁes the termination condition (cid:107)(cid:98)β
Hence, (cid:98)β

(m,t∗), where t∗ denotes the iteration at termination. Specially, we take ρ = 1.05, ν = 1, δ∗ = 10−5 in the simulations.

(cid:107)∞ ≤ δ∗ (e.g. δ∗ = 10−5).

= (cid:98)β

− (cid:98)β

(m)

(m,k)

(m,k−1)

4.3

Convergence and Computational Complexity

The convergence of the algorithm essentially follows the standard result. Note that we have a closed-form solution of A when ﬁxing B. Since
the truncated penalties are not convex in B, and thus the objective function is not convex in B when ﬁxing A, and that is when the diﬀerence-
of-convex function kicks in to convert the non-convex function to the diﬀerence of two convex functions. When solving the problem (4.1), the

proposed algorithm could potentially lead to a local optimum as the objective function of estimating B when ﬁxing A in (4.1) is non-convex. But the
objective function with linear constraints in AL-CD procedure obtained from the local linear approximation is diﬀerentiable everywhere, and thus

the convergence of coordinate descent is guaranteed. Therefore, it is only necessary to ensure that each step is guaranteed to converge. In Step

3, the optimized objective function is (4.2), and we can obtain the exact solution in closed form. In Step 2, we solve the optimization problem (4.1)

iteratively.The convergence of the integrated algorithm for the subproblem of estimating B when ﬁxing A is given in Lemma 2. Denote
p
(cid:88)

p
(cid:88)

n
(cid:88)

(cid:88)

(cid:19)

(cid:19)

S(β) =

(yi − xT

i β)2 + λ

β2
l + λ1

min

, 1

+ λ2

min

(cid:18) |βl − βl(cid:48) |
τ

, 1

.

(cid:18) |βl|
τ

i=1

l=1

l=1

l<l(cid:48):(l,l(cid:48))∈E

Lemma 2. The proposed algorithm for estimation of B given A converges. That is

(m)

S((cid:98)β

) → c, as m → ∞,

where c is a constant value, and m is the number of iterations of the integrated algorithm for problem (4.3).

The Lemma 2 above guarantees the convergence of the algorithm for estimation of B given A theoretically, which is analogous to Theorem 1
(0). Since (4.1) is a

in Shen et al. (2012) and Theorem 3 in Qin et al. (2020). Thus we omit the proof. It is crucial to pick a suitable initial value (cid:98)β
regression problem, possible candidate initial values are those estimated by any regression solver, such as glmnet in R and sklearn in python.

As for the computational complexity, the coordinate descent updating involves calculating of (cid:80)n

i=1 xilbi(−l), which requires O(np2k)
operations. The construction of E requires O(kp2) operations. Therefore, each update in updating B is of order O(np2k). The estimation of A by
solving an SVD needs O(pk2) operations. The total computational cost is O(np2k) + O(pk2).

and (cid:80)n

i=1 x2
il

4.4

The Selection of Tuning Parameters

The cross-validation (CV) can always be one way to select the optimal values, but it is computationally expensive. Here the Bayesian information

criterion (BIC) is employed as the approach for tuning parameter selection, which we use in simulations in Sect. 5. In general, solutions from cross-

validation and BIC are comparable. We select the model that has the minimum BIC value when using such criteria. Our proposed method has four
tuning parameters, λ, λ1, λ2, τ . Let φ denote the parameters that need to be tuned or selected in the candidate model. Then φ = (λ, λ1, λ2, τ ) for
our proposed FGSPCA, φ = (λ, λ1, λ2) for SFPCA (Guo et al. 2010), and φ = (λ, λ1) for the SPCA (Zou et al. 2006). Let Aφ = [αφ
k ], and
Bφ = [βφ

k ], be the estimates of A and B in (3.1) based on tuning parameters φ.

1 , · · · , αφ

1 , · · · , βφ

We deﬁne the BIC criterion of PCA variants as follows,

BIC(φ) = n log

(cid:26)(cid:13)
(cid:13)X − XBφ(Aφ)T (cid:13)
(cid:13)

2
(cid:13)
(cid:13)

F

(cid:27)

/n

+ log(n) · df ,

(4.5)

Jiang et al

7

where df represents the degree of freedom, denoted dfSPCA for SPCA (Zou et al. 2006), dfSFPCA dfSPCA for SFPCA (Guo et al. 2010), and
dfFGSPCA for FGSPCA. Specially, dfSPCA is deﬁned as the number of all nonzero elements in Bφ, dfSFPCA and dfFGSPCA are deﬁned as the
number of all non-distinct groups in Bφ. The deﬁnitions are similar to df deﬁned for Lasso and fused Lasso (Tibshirani, Saunders, Rosset, Zhu, &
Knight 2005; Zou, Hastie, & Tibshirani 2007). Intuitively, the involvement of the truncated parameter τ makes more complex the method and the
parameter tuning process. However, empirical studies show that the involvement of the truncated parameter τ establishes a trade-oﬀ between τ
and (λ1, λ2), reducing the sensitivity of the tuning of λ1 and λ2.

5

EXPERIMENTS

Adjusted Variance
Denote (cid:98)Z = [ˆZ1, · · · , ˆZk] the modiﬁed PCs. Due to the grouping and sparsity constraints, ˆZk is no longer orthogonal to ˆZi, i = 1, · · · , k − 1.
Instead, they are correlated with each other. Thus, we remove from ˆZk the correlation eﬀect of ˆZi, i = 1, · · · , k − 1 using regression projection. The
deﬁnition of the adjusted variance is adopted from Zou et al. (2006), which is computed based on the QR decomposition. Suppose (cid:98)Z = QR, where
Q is orthonormal and R is upper triangular. The adjusted variance of the j-th PC is R2
. The explained total variance is the cumulative adjusted
jj
variance, which is deﬁned as (cid:80)k

.

j=1 R2
jj

5.1

Pitprops Data

The pitprops data is a classic dataset widely used for PCA analysis, as it is usually diﬃcult to show the interpretability of principal components. In

the pitprops data, there are 180 observations and 13 measured variables. It is used in ScoTLASS (Jolliﬀe et al. 2003) and SPCA (Zou et al. 2006).

As a demonstration of the performance of the FGSPCA method, especially the grouping eﬀect and sparsity eﬀect, we consider the ﬁrst six PCs of

pitprops data.

(a) PEV (Percentage of Explained Variance)

(b) Cumulative Variance

FIGURE 2 Comparison of PEV, Cumulative Variance of Diﬀerent Dimension Reduction Methods (PCA, SCoTLASS, SPCA, and FGSPCA) on the
pitprops dataset. PCA: ordinary PCA based on SVD (Jolliﬀe 1986), SCoTLASS: modiﬁed PCA (Jolliﬀe et al. 2003), SPCA: sparse PCA (Zou et al.

2006), and FGSPCA: our new method of feature grouping and sparse PCA.

1234565101520253035Number of ComponentsPercentage of Explained Variance (%)PCASCoTLASSSPCAFGSPCA1234562030405060708090Number of ComponentsCumulative Variance (%)PCASCoTLASSSPCAFGSPCA8

Jiang et al

TABLE 1 Pitprops Data: Loadings of the ﬁrst six PCs by SPCA and FGSPCA. The “No. Groups” shows the number of groups of loadings where a
small number indicates strong grouping eﬀect with similar loadings collapsing into groups. The ‘No. Nonzeros’ is the number of non-zero loadings,

which indicates the sparsity, the smaller the more sparse. ‘Adj.V (%)’ is the proportions of adjusted variance, and ‘CV (%)’ is the proportions of

cumulative adjusted variance.

SPCA

FGSPCA

Variable

PC1

PC2

PC3

PC4

PC5

PC6

PC1

PC2

PC3

PC4

PC5

PC6

topdiam

length

moist

testsg

ovensg

ringtop

ringbut

bowmax

bowdist

whorls

clear

knots

diaknot

0.785

0.619

−0.477

−0.476

0.177

−0.250

−0.344 −0.021

−0.416

−0.400

0.013

No. Groups

No. Nonzeroes

7

7

4

4

Variance (%)

28.011

14.368

0.641

0.589

0.492

−0.016
4

4

15

28.035

13.966

13.298

Adj.V (%)

CV (%)

0.707

0.707

0.577

0.577

0.577

−0.408

−0.408

−0.408

−0.408

−0.408

−0.408

−1

1

1

−1

1

1

1

1

1

−1

1

1

−1

1

1

1

1

1

1

6

1

2

1

3

7.692

7.445

7.692

6.802

7.692

6.227

28.797

14.477

15.246

28.797

14.099

11.617

7.692

7.442

7.692

6.769

7.692

6.233

28.035

42

55.299

62.744

69.546

75.773

28.797

42.896

54.513

61.955

68.724

74.957

Table 1 shows the sparse loadings and the corresponding variance obtained by SPCA (Zou et al. 2006) and FGSPCA. As can be seen from Table 1,

both SPCA and FGSPCA show strong sparsity eﬀects with respect to the number of zero loadings. On the other hand, FGSPCA has a strong grouping

eﬀect in terms of the number of loading groups, while SPCA has a weaker grouping eﬀect compared to FGSPCA. Interestingly, through the grouping

eﬀect introduced in FGSPCA, FGSPCA shows a stronger sparsity compared to SPCA with respect to the number of zeroes. In detail, for the ﬁrst

PC obtained by FGSPCA, the loadings belong to two distinct groups with nonzero values and one sparse-group with zero values. Furthermore,

these groups are learned automatically from the FGSPCA model rather than from prior knowledge. The grouping eﬀect among loadings further

improves the interpretability of the PCA.

It can be seen from Fig. 2 that, the ﬁrst six PCs obtained by FGSPCA and SPCA account for almost the same amount of total variance, 74.96%
for FGSPCA and 75.77% for SPCA respectively, which is much larger compared with SCoTLASS (69.3%). The signiﬁcant improvement in the total
variance explained by FGSPCA and SPCA may result from the sparse structure on the loadings, since the derived PCs obtained by SCoTLASS are

not sparse enough as analyzes in Zou et al. (2006).

5.2

Synthetic Data

Simulation 1

We adopt the same synthetic example settings as Zou et al. (2006). The generating mechanism of the synthetic data consists of three hidden factors,
i.e.,

V1 ∼ N(0, 290),

V2 ∼ N(0, 300),

V3 = −0.3V1 + 0.925V2 + (cid:15),

(cid:15) ∼ N(0, 1) ,

(5.1)

Jiang et al

9

where V1, V2, (cid:15) are independent. Next, 10 observable variables are constructed as follows,

Xj =





V1 + εj, 1 ≤ j ≤ 4,

V2 + εj, 5 ≤ j ≤ 8,

V3 + εj,

j = 9, 10,

where (cid:15)j, j = 1, . . . , 10, are independent and identically distributed (i.i.d) with N(0, 1). Note that the variances of the three hidden factors are
290, 300, and 282.7875 respectively. Note that by the data generating mechanism, the variables X1 to X4 form a block/group with a constant weight
(‘block 1’), while variables X5 to X8 and X9, X10 form another two blocks, ‘block 2’ and ‘block 3’, respectively. Since ‘block 2’ and ‘block 3’ are highly
correlated, thus they can be merged into one group, say ‘BLOCK 0’. Ideally, a sparse ﬁrst derived PC1 should recover ‘BLOCK 0’ of the hidden
factor V2 using X5, X6, X7, X8, X9, X10 with equal loadings, while a sparse second derived PC2 should pick up X1, X2, X3, X4 to recover ‘block 1’ of
the hidden factor V1 with the same weights, since the variance of V2 is larger than that of V1.

Zou et al. (2006) computed sparse PCA using the true covariance matrix as the data generating mechanism is known and the true covariance
matrix of the ten observable variables {X1, · · · , X10} can be easily calculated. In our simulation, we adopt the same setting procedure as in Guo
et al. (2010) to generate data Xn×p with n = 50 according to the above data generating mechanism and repeated the simulation 50 times. And
we perform the ordinary PCA, SPCA (Sparse PCA), ST (Simple Thresholding) and FGSPCA on X. PC loadings from ordinary PCA, SPCA, ST and
FGSPCA are reported in Table 2.

TABLE 2 Synthetic example with three hidden factors: Loadings of the ﬁrst three/two principal components by PCA, SPCA (Sparse PCA), ST (Simple
Thresholding) and FGSPCA, as well as the number of groups, the number of nonzeroes and variance. ‘Adj.V (%)’ is the proportions of adjusted

variance, and ‘CV (%)’ is the proportions of cumulative adjusted variance.

PCA

SPCA

ST

FGSPCA

Variable

PC1

PC2

PC3

PC1

PC2

PC1

PC2

PC1

PC2

X1

X2

X3

X4

X5

X6

X7

X8

X9

0.479

0.479

0.479

0.479

0.062

0.059

0.114

0.114

0.5

0.5

0.5

0.5

0.116

0.116

0.116

0.116

−0.395

−0.395

−0.395

−0.395

0.145 −0.269
0.145 −0.269
0.145 −0.269
0.145 −0.269
0.582

−0.5

−0.5

−0.5

−0.5

1

4

1

4

−0.497

−0.497

−0.503

−0.503
2

4

−0.5

−0.5

−0.5

−0.5

1

4

0.5

0.5

0.5

0.5

1

4

−0.408

−0.408

−0.408

−0.408

−0.408

−0.408
1

6

41.02

39.65

38.88

39.65

59.01

39.65

41.02

39.65

38.88

38.73

100

41.02

80.67

38.88

77.61

59.08

39.25

59.08

98.33

−0.401 −0.010

X10

No. Groups

−0.401 −0.010
3

3

No. Nonzeroes

10

10

Variance (%)

69.64

30.36

Adj.V (%)

CV (%)

-

69.64

-

100

0.582

5

10

-

-

Table 2 lists three PCs of the ordinary PCA. It shows that the ﬁrst two PCs account for 100% of the total explained variance, suggesting that other
dimension reduction methods can consider only the ﬁrst two derived PCs. Results on Table 2 show that all the methods (SPCA, ST, FGSPCA) can
perfectly recover ‘block 1’ with hidden factor V1 using the derived PC2. However, as for the ﬁrst derived PC1, SPCA recovers the hidden factor V2
only using X5, X6, X7, X8 without X9, X10, as the weights on X9, X10 are zeroes. The ST method recovers the hidden factor V2 using X7, X8, X9, X10
which is far from being correct by imposing zero weights on X5, X6. FGSPCA perfectly recovers the hidden factor V2 using X5, X6, X7, X8, X9, X10
with the same weights, which is consistent with the ideal results analyzed above.

10

Jiang et al

The results of variance from Table 2 show that the total variance explained by the ﬁrst two PCs is 98.33% for FGSPCA and 80.67% for SPCA,
a great improvement of 17.66% due to the grouping eﬀect of FGSPCA. Moreover, compared with ordinary PCA (100% explained total variance),
FGSPCA is only 1.67% less with respect to the total variance explained. Most importantly, FGSPCA achieves a remarkable improvement in the
interpretability of PCs with the same value, which is the grouping eﬀect.

Simulation 2

In this example, we consider a high dimensional version (p > n) of Simulation 1. We deﬁne

Xj =





V1 + εj, 1 ≤ j ≤ 20,

V2 + εj, 21 ≤ j ≤ 40,

V3 + εj, 41 ≤ j ≤ 50 ,

where (cid:15)j, j = 1, . . . , 50, are i.i.d. N(0, 1). We generate a data matrix Xn×p with n = 20, and we conduct 50 repetitions. The estimated loadings
are illustrated in Fig. 3. Results show that SFPCA and FGSPCA produce similar sparse structures in the loadings. However, compared with the

‘scattered’ loadings from SFPCA, the loadings estimated by FGSPCA are smooth and easier for interpretation.

FIGURE 3 Factor loadings of the ﬁrst (left column) and second (right column) PC vectors estimated by SFPCA (Guo et al. 2010) (ﬁrst row), and our
proposed FGSPCA (second row). The horizontal axis is the variables and the vertical axis is the value of the loadings. Each colored curve represents

the PC vector in one replication.

01020304050−0.20.00.10.20.30.4PC 1 of SFPCA01020304050−0.20.00.10.20.30.4PC 2 of SFPCA01020304050−0.20.00.10.20.30.4PC 1 of FGSPCA01020304050−0.20.00.10.20.30.4PC 2 of FGSPCAJiang et al

6

DISCUSSION AND EXTENSION

11

One limitation of FGSPCA is that it uses non-convex regularizers, neither smooth nor diﬀerentiable. Recent research work (Birgin, Martínez, &

Ramos 2021; Wen, Chu, Liu, & Qiu 2018; Y. Zhang, Li, Guo, & Wu 2020) has shown better denoising advantages of non-convex regularizers

over convex ones. However, when solving the subproblem (4.1) with non-convex penalties, the proposed method could potentially lead to a local

optimum, as the objective function in (4.1) is non-convex. As is pointed out in Wen et al. (2018), the performance of non-convex optimization

problems is usually closely related to the initialization, which are inherent drawbacks of non-convex optimization problems. Hence, it is desirable
(0). Since each subprpblem (4.1) is a classical regression problem, possible candidate initial values are those
to pick a suitable initial value of (cid:98)β
estimated by any regression solver, such as the R package glmnet (Friedman, Hastie, & Tibshirani 2009) and the python sklearn. For simplicity,
we use the result of SVD as the initialization in this paper.

The FGSPCA can be easily extended to the case with non-negative loadings, namely nnFGSPCA. In light of the work in Qin et al. (2020),
l=1(min{βl, 0})2 that characterizes the non-negativity, into the objective function. The

we incorporate another regularization term, p3(β) = (cid:80)p
optimization problem of nnFGSPCA becomes,

min
A,B

n
(cid:88)

(cid:107)xi − ABTxi(cid:107)2

2 + λ

i=1
s.t. ATA = Ik×k,

k
(cid:88)

j=1

(cid:107)βj(cid:107)2

2 + λ1

k
(cid:88)

j=1

p1(βj) + λ2

k
(cid:88)

j=1

p2(βj) + λ3

k
(cid:88)

j=1

p3(βj) ,

(6.1)

The nnFGSPCA can be easily solved using similar techniques (See Appendix E in Supporting information for details).

7

CONCLUSION

In this paper, we propose the FGSPCA method to produce modiﬁed principal components by considering additional grouping structures where the

loadings share similar coeﬃcients (i.e., feature grouping), besides a special group with all coeﬃcients being zero (i.e., feature selection). The pro-

posed FGSPCA method can perform simultaneous feature clustering/grouping and feature selection by imposing the non-convex regularization

with naturally adjustable sparsity and grouping eﬀect. Therefore, the model learns the grouping structure rather than from given prior informa-

tion. Eﬃcient algorithms are designed and experiment results show that the proposed FGSPCA beneﬁts from the grouping eﬀect compared with

methods without grouping eﬀect.

Conﬂict of interest

The authors declare no potential conﬂict of interests.

SUPPORTING INFORMATION

Additional information for this article is available. It contains the proofs of Lemma 1, Structured Sparsity, the Procrustes Problem and the extension

to nnFSGPCA. An R implementation of FGSPCA can be found on github https://github.com/higeeks/FGSPCA.

References

An, L. T. H., & Tao, P. D. (2005). The dc (diﬀerence of convex functions) programming and dca revisited with dc models of real world nonconvex

optimization problems. Annals of operations research, 133(1), 23–46.

Birgin, E. G., Martínez, J. M., & Ramos, A.

(2021). On constrained optimization with nonconvex regularization. Numerical Algorithms, 86(3),

1165–1188.

Cai, T. T., Ma, Z., & Wu, Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. The Annals of Statistics, 41(6), 3074–3110.
Croux, C., Filzmoser, P., & Fritz, H. (2013). Robust sparse principal component analysis. Technometrics, 55(2), 202–214.
Dai, M., Dai, S., Huang, J., Kang, L., & Lu, X.

(2021). Truncated L1 regularized linear regression: Theory and algorithm. Commun. Comput. Phys.,

30(1), 190–209.

Erichson, N. B., Zheng, P., Manohar, K., Brunton, S. L., Kutz, J. N., & Aravkin, A. Y. (2020). Sparse principal component analysis via variable projection.

SIAM Journal on Applied Mathematics, 80(2), 977–1002.

12

Jiang et al

Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association,

96(456), 1348–1360.

Friedman, J., Hastie, T., & Tibshirani, R. (2009). glmnet: Lasso and elastic-net regularized generalized linear models. R package version, 1(4), 1–24.
Friedman, J., Hastie, T., & Tibshirani, R. (2010). A note on the group lasso and a sparse group lasso. arXiv preprint arXiv:1001.0736.
Grbovic, M., Dance, C., & Vucetic, S. (2012). Sparse principal component analysis with constraints. In Proceedings of the aaai conference on artiﬁcial

intelligence (Vol. 26).

Guo, J., James, G., Levina, E., Michailidis, G., & Zhu, J. (2010). Principal component analysis with sparse fused loadings. Journal of Computational

and Graphical Statistics, 19(4), 930–946.

Hoeﬂing, H. (2010). A path algorithm for the fused lasso signal approximator. Journal of Computational and Graphical Statistics, 19(4), 984–1006.
Jenatton, R., Audibert, J.-Y., & Bach, F. (2011). Structured variable selection with sparsity-inducing norms. The Journal of Machine Learning Research,

12, 2777–2824.

Jenatton, R., Obozinski, G., & Bach, F. (2010). Structured sparse principal component analysis. In Proceedings of the thirteenth international conference

on artiﬁcial intelligence and statistics (pp. 366–373).

Jin, Y., & Sidford, A. (2019). Principal component projection and regression in nearly linear time through asymmetric svrg. In Advances in neural
information processing systems (Vol. 32). Curran Associates, Inc. Retrieved from https://proceedings.neurips.cc/paper/2019/file/

3b92d18aa7a6176dd37d372bc2f1eb71-Paper.pdf

Johnstone, I. M., & Lu, A. Y.

(2009). On consistency and sparsity for principal components analysis in high dimensions. Journal of the American

Statistical Association, 104(486), 682–693.

Jolliﬀe, I. T. (1986). Principal components in regression analysis. In Principal component analysis (pp. 129–155). Springer.
Jolliﬀe, I. T., Trendaﬁlov, N. T., & Uddin, M. (2003). A modiﬁed principal component technique based on the lasso. Journal of Computational and

Graphical Statistics, 12(3), 531–547.

Khan, Z., Shafait, F., & Mian, A. (2015). Joint group sparse PCA for compressed hyperspectral imaging. IEEE Transactions on Image Processing, 24(12),

4934–4942.

Meier, L., Van De Geer, S., & Bühlmann, P. (2008). The group lasso for logistic regression. Journal of the Royal Statistical Society: Series B (Statistical

Methodology), 70(1), 53–71.

Nadler, B. (2008). Finite sample approximation results for principal component analysis: A matrix perturbation approach. The Annals of Statistics,

36(6), 2791–2817.

Paul, D. (2007). Asymptotics of sample eigenstructure for a large dimensional spiked covariance model. Statistica Sinica, 1617–1642.
Qin, S., Ding, H., Wu, Y., & Liu, F. (2020). High-dimensional sign-constrained feature selection and grouping. Annals of the Institute of Statistical

Mathematics, 1–33.

Rinaldo, A. (2009). Properties and reﬁnements of the fused lasso. Annals of Statistics, 37(5B), 2922–2952.
Shen, X., Huang, H.-C., & Pan, W. (2012). Simultaneous supervised clustering and feature selection over a graph. Biometrika, 99(4), 899–914.
Shen, X., Pan, W., Zhu, Y., & Zhou, H.

(2013). On constrained and regularized high-dimensional regression. Annals of the Institute of Statistical

Mathematics, 65(5), 807–832.

Tian, L., Nie, F., Wang, R., & Li, X.

(2020). Learning feature sparse principal subspace.

In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, &

H. Lin (Eds.), Advances in neural information processing systems (Vol. 33, pp. 14997–15008). Curran Associates, Inc. Retrieved from https://

proceedings.neurips.cc/paper/2020/file/ab7a710458b8378b523e39143a6764d6-Paper.pdf

Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., & Knight, K.

(2005). Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical

Society: Series B (Statistical Methodology), 67(1), 91–108.

Vu, V. Q., Cho, J., Lei, J., & Rohe, K. (2013). Fantope projection and selection: A near-optimal convex relaxation of sparse pca. In Proceedings of the
26th international conference on neural information processing systems - volume 2 (p. 2670–2678). Red Hook, NY, USA: Curran Associates Inc.
(2019). Two-layer feature reduction for sparse-group lasso via decomposition of convex sets. Journal of Machine

Wang, J., Zhang, Z., & Ye, J.

Learning Research, 20(163), 1–42.

Wang, Z., Lu, H., & Liu, H. (2014). Tighten after relax: Minimax-optimal sparse pca in polynomial time. Advances in Neural Information Processing

Systems, 2014, 3383.

Wen, F., Chu, L., Liu, P., & Qiu, R. C.

(2018). A survey on nonconvex regularization-based sparse and low-rank recovery in signal processing,

statistics, and machine learning. IEEE Access, 6, 69883–69906.

Wu, C., Liu, Z., & Wen, S. (2018). A general truncated regularization framework for contrast-preserving variational signal and image restoration:

Motivation and implementation. Science China Mathematics, 61(9), 1711–1732.

Yang, S., Yuan, L., Lai, Y.-C., Shen, X., Wonka, P., & Ye, J. (2012). Feature grouping and selection over an undirected graph. In Proceedings of the

18th acm sigkdd international conference on knowledge discovery and data mining (pp. 922–930).

Jiang et al

13

Yi, S., Lai, Z., He, Z., Cheung, Y.-m., & Liu, Y. (2017). Joint sparse principal component analysis. Pattern Recognition, 61, 524–536.
Yuan, L., Liu, J., & Ye, J. (2011). Eﬃcient methods for overlapping group lasso. Advances in neural information processing systems, 24, 352–360.
Yuan, M., & Lin, Y.

(2006). Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B

(Statistical Methodology), 68(1), 49–67.

Yun, J., Zheng, P., Yang, E., Lozano, A., & Aravkin, A. (2019). Trimming the (cid:96)_1 regularizer: Statistical analysis, optimization, and applications to deep

learning. In International conference on machine learning (pp. 7242–7251).

Zhang, C.-H., & Huang, J. (2008). The sparsity and bias of the lasso selection in high-dimensional linear regression. The Annals of Statistics, 36(4),

1567–1594.

Zhang, R., & Tong, H.

(2019).

tion processing systems (Vol. 32).

Curran Associates, Inc.

73f104c9fba50050eea11d9d075247cc-Paper.pdf

Robust principal component analysis with adaptive neighbors.

informa-
Retrieved from https://proceedings.neurips.cc/paper/2019/file/

In Advances in neural

Zhang, Y., Li, S., Guo, Z., & Wu, B. (2020). An adaptive total variational despeckling model based on gray level indicator frame. Inverse Problems &

Imaging, 1.

Zhang, Y., Zhang, N., Sun, D., & Toh, K.-C.

(2020). An eﬃcient hessian based algorithm for solving large-scale sparse group lasso problems.

Mathematical Programming, 179(1), 223–263.

Zou, H., & Hastie, T.

(2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: series B (statistical

methodology), 67(2), 301–320.

Zou, H., Hastie, T., & Tibshirani, R. (2006). Sparse principal component analysis. Journal of Computational and Graphical Statistics, 15(2), 265–286.
Zou, H., Hastie, T., & Tibshirani, R. (2007). On the “degrees of freedom” of the lasso. The Annals of Statistics, 35(5), 2173–2192.
Zou, H., & Xue, L. (2018). A selective overview of sparse principal component analysis. Proceedings of the IEEE, 106(8), 1311–1320.

14

Jiang et al

APPENDIX

A PROOF OF LEMMA 1

Lemma 3. Consider the ridge regression criterion,

Denote the solution of ridge regression (cid:98)β = arg minβ Jλ(β) = (XTX + λI)−1XTy. Then

Jλ(β) = (cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)2
2 ,

where

Jλ((cid:98)β) = yT (I − Hλ)y,

Hλ = X(XT X + λI)−1XT .

Proof of Lemma 1.
We use the notation Ap×k = [α1, · · · , αk] and Bp×k = [β1, · · · , βk]. Let

Jλ(A, B) =

n
(cid:88)

i=1

(cid:107)xi − ABT xi(cid:107)2

2 + λ(cid:107)β(cid:107)2
2 .

Deﬁne Ip×p = [A A⊥]. And A ∈ Rp×k, A⊥ ∈ Rp×(p−k), B ∈ Rp×k. With the orthogonal constraint, ATA = Ik×k, BTB = Ik×k, and constraint
ATA⊥ = 0k×(p−k), we have

(cid:13)
(cid:13)

(cid:13)X − XBAT(cid:13)

2
(cid:13)
(cid:13)
F

n
(cid:88)

i=1

(cid:107)xi − ABTxi(cid:107)2

2 =

=

=

(cid:13)
(cid:13)X[A A⊥] − XBAT[A A⊥]
(cid:13)
(cid:13)
(cid:13)[XA XA⊥] − [XBATA XBATA⊥]
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

2

(cid:13)
(cid:13)
(cid:13)
(ATA = Ik×k, ATA⊥ = 0)

F

= (cid:107)[(XA − XB) XA⊥](cid:107)2
F
F + (cid:107)XA⊥(cid:107)2
= (cid:107)XA − XB(cid:107)2

F

=

k
(cid:88)

j=1

(cid:107)Xαj − Xβj(cid:107)2

2 + (cid:107)XA⊥(cid:107)2

F .

Therefore, when A is ﬁxed, solving arg minB Jλ(A, B) is equivalent to solving the series of ridge regressions

Denote the solutions

k
(cid:88)

j=1

arg min
(k)
{βj }
j=1

(cid:8)(cid:107)Xαj − Xβj (cid:107)2

2 + λ(cid:107)βj (cid:107)2
2

(cid:9) .

(cid:98)B = (XT X + λI)−1XT XA.

Using Lemma 3, (A1) and (A2), we have the partially optimized criterion

Jλ(A, (cid:98)B) = (cid:107)XA⊥(cid:107)2

F +

k
(cid:88)

(cid:110)

j=1

(cid:107)Xαj − X(cid:98)βj(cid:107)2

2 + λ(cid:107)(cid:98)βj(cid:107)2
2

(cid:111)

= (cid:107)XA⊥(cid:107)2

F + tr{(XA)T(I − Hλ)(XA)}.

(cid:107)XA⊥(cid:107)2

F + tr{(XA)T(XA)} = tr{(XA⊥)T(XA⊥)} + tr{(XA)T(XA)}

= (cid:107)XA⊥(cid:107)2
= (cid:107)X(cid:107)2

F + (cid:107)XA(cid:107)2
F = tr(XTX).

F = (cid:107)X[A A⊥](cid:107)2

F

Note that

Then we have

(A1)

(A2)

Jλ(A, (cid:98)B) = tr(XT X) + tr(AT XT HλXA) ,

(A3)

Jiang et al

15

which should be minimized with respect to A with constraint that ATA = I. The solution of (A3) should be taken to the top k eigenvectors of
XTHλX. If the SVD of X = UDVT, we can easily get

XT HλX = VD2(D2 + λI)−1D2VT ,

then we have

By plugging in the SVD of X into (A2), each (cid:98)βj

is proportional to Vj with

(cid:98)A = V[, 1 : k].

(cid:98)βj = Vj

D2
jj
jj + λ

D2

∝ Vj .

B STRUCTURED SPARSITY

We consider the structured regularization functions of variables (factors) in regression models, as variable selection and model selection are two

essential issues which have been extensively studied in the framework of regression, especially in the high dimensional settings.

Elastic net (Zou & Hastie 2005)

The naive elastic net criterion (Zou & Hastie 2005) is deﬁned as

(cid:107)Y − Xβ(cid:107)2

2 + λ2(cid:107)β(cid:107)2

2 + λ1(cid:107)β(cid:107)1 .

(B1)

If there is a group of variables among which the pairwise correlations are very high, the lasso tends to select only one variable from the group and

does not care which one is selected. Unlike the lasso, the elastic net encourages a grouping eﬀect, where strongly correlated predictors tend to be

in or out of the model together.

Fused lasso (Tibshirani et al. 2005)

The fused lasso (Tibshirani et al. 2005) is deﬁned as follows,

(cid:98)β = arg min

β

(cid:107)Y − Xβ(cid:107)2
2,

subject to

p
(cid:88)

j=1

|βj | ≤ s1,

p
(cid:88)

j=2

|βj − βj−1| ≤ s2.

(B2)

The ﬁrst constraint induces sparsity in the coeﬃcients; the second results in sparsity in their successive diﬀerences, i.e. local constancy of the
coeﬃcient proﬁles βj as a function of j. The fused lasso gives a way to incorporate information about spatial or temporal structure in the data.
However, it requires the features to be ordered in some meaningful way before the construction of the problem.

Group lasso (M. Yuan & Lin 2006)

Consider the general regression problem with J groups/factors,

Y =

J
(cid:88)

j=1

Xj βj + (cid:15).

is the coeﬃcient vector of size pj, j = 1, · · · , J.
Here Y ∈ Rn×1, (cid:15) ∼ Nn(0, σ2I), Xj is an n × pj matrix corresponding to the j-th factor and βj
For a vector η ∈ Rd, d ≥ 1, and a symmetric d × d positive deﬁnite matrix K, we denote (cid:107)η(cid:107)K = (ηKη)1/2 . Given positive deﬁnite matrices
K1, · · · , KJ, the group lasso (M. Yuan & Lin 2006) estimate is deﬁned as the solution to the following minimization problem,

min
β

=

1
2n

(cid:107)Y −

J
(cid:88)

j=1

Xj βj (cid:107)2

2 + λ

J
(cid:88)

j=1

(cid:107)βj (cid:107)Kj .

(B3)

In the group lasso problem, the non-squared Euclidean (cid:96)2-norm penalty encourages factor/group-level sparsity, where the entire group of predictors
can be retained or discarded in the model. Thus the group lasso can conduct feature selection along the group level and select groups of variables.

However, this kind of group-level sparsity depends on the predeﬁned group partition.

16

Jiang et al

Structured sparsity-inducing norms (Jenatton et al. 2011)

Consider the empirical risk minimization problem for linear supervised learning, with regularization by structured sparsity-inducing norms (Jenatton

et al. 2011),

1
n
i=1
where λ is a regularization parameter, L(w) = 1
i w) the empirical risk of a weight vector w ∈ Rp, (cid:96)(·, ·) is a loss function which is
n
usually assumed convex and continuously diﬀerentiable with respect to the second parameter. The Ω(w) is a general family of sparsity-inducing
norms that allow the penalization of subsets of variables grouped together, which is deﬁned as follows,

i w) + λΩ(w) ,

i=1 (cid:96)(yi, xT

(cid:96)(yi, xT

min
w

(B4)

(cid:80)n

n
(cid:88)

Ω(w) =

(cid:88)





(cid:88)



1
2

(dG

j )2|wj |2



=

(cid:88)

G∈G

j∈G

G∈G
j > 0 if j ∈ G and dG

(cid:13)
(cid:13)dG ◦ w
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

.

(B5)

j = 0 otherwise, and G denotes a subset of the power set of
Here (dG)G∈G is a |G|-tuple of p-dimensional vectors such that dG
{1, · · · , p} such that (cid:80)
G∈G = {1, · · · , p}, that is, a spanning set of subsets of {1, · · · , p}. It is possible for elements of G to overlap. This general
formulation has several important sub-cases such as (cid:96)2-norm penalty, (cid:96)1-norm penalty, group (cid:96)1-norm penalty, and elastic net penalty. However,
the structured sparsity-inducing regularization can only encode prior knowledge about the expected sparsity patterns.

C THE PROCRUSTES PROBLEM

Lemma S1

Reduced Rank Procrustes Rotation. Mn×p and Nn×k denote two matrices. Consider the constrained minimization problem
(cid:13)
(cid:13)

2

s.t. AT A = Ik×k .

(cid:98)A = arg min

(cid:13)M − NAT (cid:13)

(cid:13)
(cid:13)

F

A
Suppose the SVD of MTN is UDVT, then (cid:98)A = UVT.

(C1)

Proof of Lemma S1

In the orthogonal Procrustes problem, we seek an orthornormal matrix such that

First, we expand the matrix norm in the above objective function

A = arg min

A

(cid:13)
(cid:13)

(cid:13)M − NAT (cid:13)

2
(cid:13)
(cid:13)

F

, s.t. AT A = Ik×k.

(cid:13)
(cid:13)

(cid:13)M − NAT (cid:13)

2
(cid:13)
(cid:13)

F

= tr(MT M) + tr(ANT NAT ) − 2 tr(MT NAT ) .

Since ATA = Ik×k and tr(AB) = tr(BA), then the second term becomes

tr(ANT NAT ) = tr(NT NAT A) = tr(NT N) .

The problem is equivalent to ﬁnding an orthornormal matrix A which maximize tr(MTNAT). We proceed by substituting the SVD of MTN =
UDVT and obtain

tr(MT NAT ) = tr(UDVT AT ) = tr{UD(AV)T } = tr{(AV)T UD}.

As V is k × k orthonormal, we have (AV)T(AV) = VTATAV = Ik×k . Note that D is diagonal with non-negative entries, tr{(AV)TUD} is
maximized when the diagonal of (AV)TU is positive and maximized. By Cauchy-Schwartz inequality, this is achieved when (AV)T = UT, and in
this case the diagonal elements are all ones, (AV)TU = UTU = I . Hence, an optimal solution is given by (cid:98)A = UVT .

D THE DETAILED PROCEDURE TO ESTIMATION OF B GIVEN A

An integrated algorithm for the estimation of B given A (algorithm 1) integrates the diﬀerence-of-convex algorithm (DC), the augmented Lagrange
method (AL) and coordinate descent method (CD), for eﬃcient computation. The procedure to solve the FGS problem consists of three steps.

Jiang et al

The Diﬀerence-of-Convex Algorithm (DC).

17

Denote

S(β) =

n
(cid:88)

i=1

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l + λ1

p
(cid:88)

l=1

min

(cid:18) |βl|
τ

(cid:19)

, 1

+ λ2

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:18) |βl − βl(cid:48) |
τ

(cid:19)

, 1

.

Using min(a, b) = a − (a − b)+, we decompose the non-convex objective function S(β) into a diﬀerence of two convex functions,
S(β) = S1(β) − S2(β), where the two convex functions are given respectively by

S1(β) =

n
(cid:88)

i=1

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l + λ1

p
(cid:88)

l=1

|βl|
τ

+ λ2

p
(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

|βl − βl(cid:48) |
τ

,

S2(β) = λ1

p
(cid:88)

l=1

(cid:18) |βl|
τ

(cid:19)

− 1

+ λ2

p
(cid:88)

+

l<l(cid:48):(l,l(cid:48))∈E

(cid:18) |βl − βl(cid:48) |
τ

(cid:19)

− 1

.

+

We then construct a sequence of approximations of S2(β) iteratively. At the m-th iteration, we replace S2(β) with its aﬃne minorization at the
(m − 1)-th iteration. Specially,

S(m)
2

(β) = S2((cid:98)β

(m−1)

) + (cid:104)β − (cid:98)β

(m−1)

, ∂S2(β)|

(m−1) (cid:105)

β= (cid:98)β

= S2((cid:98)β

(m−1)

) +

λ1
τ

p
(cid:88)

l=1

I(cid:110)

(m−1)
| ˆβ
l

|≥τ

(cid:111) · |βl| +

λ2
τ

(cid:88)

I(cid:110)

l<l(cid:48):(l,l(cid:48))∈E

(m−1)
| ˆβ
l

(m−1)
− ˆβ
l(cid:48)

|≥τ

(cid:111) · |βl − βl(cid:48) | .

Finally, a sequence of approximations of S(β) is constructed iteratively. For the m-th approximation, an upper convex approximating function to
S(β) can be obtained by S(m)(β) = S1(β) − S(m)

(β), which formulates the following subproblem:

2

S(m)(β) =

min
β

n
(cid:88)

i=1

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l +

λ1
τ

(cid:88)

l∈F (m−1)

|βl| +

λ2
τ

(cid:88)

|βl − βl(cid:48) | ,

l<l(cid:48):(l,l(cid:48))∈E(m−1)

where

F (m−1) = {l : | ˆβ(m−1)

l

| < τ } ,

E (m−1) = {(l, l(cid:48)) ∈ E, | ˆβ(m−1)

l

− ˆβ(m−1)
l(cid:48)

| < τ } .

(D1)

(D2)

Augmented Lagrange Method and Coordinate Descent Method (AL-CD).
Denote βll(cid:48) = βl − βl(cid:48) and deﬁne ξ = (β1, · · · , βp, β12, · · · , β1p, · · · , β(p−1)p). The m-th subproblem (D1) can be reformulated as an equality-
constrained convex optimization problem,

min
ξ

n
(cid:88)

i=1

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l +

λ1
τ

(cid:88)

l∈F (m−1)

|βl| +

λ2
τ

(cid:88)

|βll(cid:48) | ,

l<l(cid:48):(l,l(cid:48))∈E(m−1)

subject to βll(cid:48) = βl − βl(cid:48) ,

∀l < l(cid:48) : (l, l(cid:48)) ∈ E (m−1) .

(D3)

For the equality-constrained problem (D3), we employ the augmented Lagrange method to solve its equivalent unconstrained version iteratively

with respect to k for the m-th approximation. For the m-th approximation, the augmented Lagrange method for (D3) is

L(m)
ν

(ξ, τ ) =

n
(cid:88)

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l +

λ1
τ

(cid:88)

|βl| +

i=1

+

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E(m−1)

τll(cid:48) (βl − βl(cid:48) − βll(cid:48) ) +

l∈F (m−1)
ν
2

λ2
τ

(cid:88)

(cid:88)

|βll(cid:48) |

l<l(cid:48):(l,l(cid:48))∈E(m−1)

(βl − βl(cid:48) − βll(cid:48) )2 .

l<l(cid:48):(l,l(cid:48))∈E(m−1)

Here τll(cid:48) and ν are the Lagrangian multipliers for the linear constraints and for the computational acceleration, which are updated as follows,

τ (k+1)
ll(cid:48)

= τ (k)

ll(cid:48) + ν(k)( ˆβ(m,k)

l

− ˆβ(m,k)
l(cid:48)

− ˆβ(m,k)
ll(cid:48)

),

ν(k+1) = ρν(k) .

(D4)

(D5)

Here ρ controls the convergence speed of the algorithm, which is chosen to be larger than 1 (e.g. ρ = 1.05) for acceleration of the convergence.

For the ξ minimization step in (D4), we use the coordinate descent methods to compute the update. Denote a solution of (D4) as (cid:98)ξ

each component of ξ, we ﬁx the other components at their current values. Set an initial value (cid:98)ξ
the subproblem (D1) for the (m − 1)-th approximation. Then update (cid:98)ξ

(m,k) by the following formulas, for k = 1, 2, · · ·

= (cid:98)ξ

(m,0)

(m−1), where (cid:98)ξ

(m,k+1). For
(m−1) is the solution of

• Given ˆβ(m,k−1)

l

, update ˆβ(m,k)

l

(l = 1, 2, · · · , p) by

ˆβ(m,k)
l

= α−1γ,

18

Jiang et al

where α = 2λ + 2 (cid:80)n
i=1 x2
sign(x)(|x| − δ)+ is the soft threshold function, and

il + ν(k) (cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)

(cid:12). And γ = γ∗ if | ˆβ(m−1)

l

| ≥ τ ; otherwise, γ = ST(γ∗, λ1

τ ) . Here ST(x, δ) =

γ∗ = 2

n
(cid:88)

i=1

xilb(m,k)

i(−l) −

(cid:88)

ll(cid:48) + ν(k) (cid:88)
τ (k)

(cid:16) ˆβ(m,k)
l(cid:48)

+ ˆβ(m,k)
ll(cid:48)

(cid:17)

,

(l,l(cid:48))∈E(m−1)

(l,l(cid:48))∈E(m−1)

where b(m,k)

i(−l) = yi − xT

i(−l) (cid:98)β

(m,k)
(−l)

; xi(−l) is the vector xi without the l-th component.

• Given ˆβ(m,k−1)

ll(cid:48)

, update ˆβ(m,k)

ll(cid:48)

(1 ≤ l < l(cid:48) ≤ p) (with ˆβ(m,k)

l

already updated and ﬁxed). Then

ˆβ(m,k)
ll(cid:48)

=






1
ν(k) ST
ˆβ(m−1)
ll(cid:48)

(cid:16)
ll(cid:48) + ν(k)( ˆβ(m,k)
τ (k)

l

− ˆβ(m,k)
l(cid:48)

), λ2
τ

(cid:17)

if (l, l(cid:48)) ∈ E (m−1),

if (l, l(cid:48)) (cid:54)∈ E (m−1).

The process of coordinate descent iterates until convergence, satisﬁes the termination condition (cid:107)(cid:98)β
Hence, (cid:98)β

(m,t∗), where t∗ denotes the iteration at termination. Specially, we take ρ = 1.05, ν = 1, δ∗ = 10−5 in the simulations.

(cid:107)∞ ≤ δ∗ (e.g. δ∗ = 10−5).

= (cid:98)β

− (cid:98)β

(m)

(m,k)

(m,k−1)

E EXTENSION TO NNFGSPCA

The nnFGSPCA criterion

For the FGSPCA criterion, by adding another regularization function controlling the non-negativity of the loadings, we can obtain the nnFGSPCA
criterion (E1) easily,

min
A,B

n
(cid:88)

(cid:107)xi − ABTxi(cid:107)2

2 + Ψ(B) ,

i=1
subject to ATA = Ik×k ,

where

Ψ(B) = λ

k
(cid:88)

j=1

(cid:107)βj (cid:107)2

2 + λ1

k
(cid:88)

j=1

p1(βj ) + λ2

k
(cid:88)

j=1

p2(βj ) + λ3

k
(cid:88)

j=1

p3(βj ) .

(E1)

(E2)

Here p1(β) and p2(β) are the same regularization functions as that in the FGSPCA criterion, and p3(β) is a new regularization function controlling
the non-negativity of the loadings, which takes the following penalty form,

p3(βj ) =

p
(cid:88)

l=1

(cid:2)min (cid:0)βl(j), 0(cid:1)(cid:3)2 .

(E3)

In order to be self-contained, we also list here the regularization functions of p1(β) and p2(β)

p1(βj ) =

p
(cid:88)

l=1

min

(cid:26) |βl(j)|
τ

(cid:27)

, 1

,

p2(βj ) =

(cid:88)

l<l(cid:48):(l,l(cid:48))∈E

min

(cid:26) |βl(j) − βl(cid:48)(j)|
τ

(cid:27)

, 1

.

The algorithm to solve the nnFGSPCA problem should be similar to the algorithms in Section 4. The procedure of updating A is the same, only the
updating of ˆβ(m,k)

is slightly diﬀerent.

l

To calculate B.
If A is given, for each j, denote Yj = Xαj. To estimate (cid:98)B = [(cid:98)β1, · · · , (cid:98)βk], the nnFGSPCA criterion is equivalent to k independent non-negative
feature-grouping-and-sparsity constrained regression subproblems (nnFGS) deﬁned in the following

(cid:8)S(β) = (cid:107)Yj − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)2

2 + λ1p1(β) + λ2p2(β) + λ3p3(β)(cid:9) .

min
β

(E4)

Each (cid:98)βj = arg minβ S(β) is a solution of the nnFGS problem, which can be obtained through a slightly diﬀerent updating process of ˆβ(m,k)

l

.

Note that p3(β) should be decomposed by the diﬀerence-of-convex programming just as p1(β) and p2(β) do. In particular, S(β) can be

decomposed as follows,

S(β) = S1(β) − S2(β) ,

Jiang et al

19

where the two convex functions S1(β) and S2(β) are given respectively by
n
(cid:88)

p
(cid:88)

p
(cid:88)

S1(β) =

(yi − xT

i β)2 + λ

β2
l + λ1

i=1

l=1

l=1

|βl|
τ

p
(cid:88)

+ λ2

l<l(cid:48):(l,l(cid:48))∈E

|βl − βl(cid:48) |
τ

+ λ3

p
(cid:88)

l=1

β2
l ,

S2(β) = λ1

p
(cid:88)

l=1

(cid:18) |βl|
τ

(cid:19)

− 1

+ λ2

p
(cid:88)

+

l<l(cid:48):(l,l(cid:48))∈E

(cid:18) |βl − βl(cid:48) |
τ

(cid:19)

− 1

+ λ3

+

p
(cid:88)

l=1

[(βl)+]2.

For the m-th iteration, we replace S2(β) with its aﬃne minorization at the (m − 1)-th iteration.

S(m)
2

(β) ∝S2((cid:98)β

(m−1)

) +

(cid:68)

β − (cid:98)β

(m−1)

, ∂S2(β)|

(cid:69)

(m−1)

β= (cid:98)β

∝S2((cid:98)β

(m−1)

) +

λ1
τ

p
(cid:88)

l=1

I(cid:110)

(m−1)
| ˆβ
l

|≥τ

(cid:111) · |βl| +

λ2
τ

(cid:88)

I(cid:110)

l<l(cid:48):(l,l(cid:48))∈E

(m−1)
| ˆβ
l

(m−1)
− ˆβ
l(cid:48)

|≥τ

(cid:111) · |βl − βl(cid:48) |

+ λ3

p
(cid:88)

l=1

I(cid:110) ˆβ

(m−1)
l

≥0

(cid:111) · β2
l .

For the m-th approximation, an upper convex approximating function to S(β) can be obtained by S(m)(β) = S1(β) − S(m)

2

(β), which formulates

the following subproblem.

min
β

n
(cid:88)

i=1

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l +

λ1
τ

(cid:88)

l∈F (m−1)

|βl| +

λ2
τ

(cid:88)

|βl − βl(cid:48) | + λ3

l<l(cid:48):(l,l(cid:48))∈E(m−1)

(cid:88)

β2
l ,

l∈N (m−1)

where

F (m−1) =

E (m−1) =

N (m−1) =

(cid:110)

(cid:110)

(cid:110)

l : | ˆβ(m−1)
l

| < τ

(cid:111)

,

(l, l(cid:48)) ∈ E, | ˆβ(m−1)

l

− ˆβ(m−1)
l(cid:48)

| < τ

(cid:111)

,

l : ˆβ(m−1)
l

(cid:111)

< 0

.

(E5)

Denote βll(cid:48) = βl − βl(cid:48) and deﬁne ξ = (β1, · · · , βp, β12, · · · , β1p, · · · , β(p−1)p). The m-th subproblem can be reformulated as an equality-

constrained convex optimization problem,

min
ξ

n
(cid:88)

i=1

(yi − xT

i β)2 + λ

p
(cid:88)

l=1

β2
l +

λ1
τ

(cid:88)

l∈F (m−1)

|βl| +

λ2
τ

(cid:88)

|βll(cid:48) | + λ3

(cid:88)

β2
l ,

l<l(cid:48):(l,l(cid:48))∈E(m−1)

l∈N (m−1)

subject to βll(cid:48) = βl − βl(cid:48) ,

∀l < l(cid:48) : (l, l(cid:48)) ∈ E (m−1).

The only diﬀerence for nnFGSPCA is the updating rule of ˆβ(m,k)

l

, since p3(β) does not involve other variables but only βl. In particular, when

updating by

The new α is formulated as follows,

ˆβ(m,k)
l

= α−1γ.

where α is diﬀerent compared to the solution of FGSPCA and γ stays the same.

α = 2λ + 2λ3I(cid:110) ˆβ

(m−1)
l

(cid:111) + 2

<0

n
(cid:88)

i=1

il + ν(k) (cid:12)
x2
(cid:12)

(cid:12)l(cid:48) : (l, l(cid:48)) ∈ E (m−1)(cid:12)
(cid:12)
(cid:12) ,

(E6)

