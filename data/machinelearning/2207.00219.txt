RankingConstraintRelaxationsforMixedIntegerProgramsUsingaMachineLearningApproachJakeWeinera1,AndreasT.Ernstb,XiaodongLia,YuanSunbaSchoolofComputingTechnologies,RMITUniversity,124LaTrobeStreet,Melbourne,AustraliabSchoolofMathematics,ClaytonCampus,MonashUniversity,Melbourne,AustraliaAbstractSolvinglarge-scaleMixedIntegerPrograms(MIP)canbediﬃcultwithoutadvancedalgorithmssuchasde-compositionbasedtechniques.Evenifadecompositiontechniquemightbeappropriate,therearestillmanypossibledecompositionsforanylargeMIPanditmaynotbeobviouswhichwillbethemosteﬀective.ThispaperpresentsacomprehensiveanalysisofthepredictivecapabilitiesofaMachineLearningranking(ML)functionforpredictingthequalityofMixedIntegerProgramming(MIP)decompositionscreatedviaconstraintrelaxation.Inthisanalysis,theroleofinstancesimilarityandMLpredictionqualityisexplored,aswellasthebenchmarkingofaMLrankingfunctionagainstexistingheuristicfunctions.Forthisanalysis,anewdatasetconsistingofover40000uniquedecompositionssampledfromacross24instancesfromtheMIPLIB2017libraryhasbeenestablished.Thesedecompostionshavebeencreatedbybothagreedyrelaxationalgorithmaswellasapopulationbasedmulti-objectivealgorithm,whichhaspreviouslybeenshowntoproducehighqualitydecompositions.Inthispaper,wedemonstratethataMLrankingfunctionisabletoprovidestate-of-the-artpredictionswhenbenchmarkedagainstexistingheuristicrankingfunctions.Additionally,wedemonstratethatbyonlyconsideringasmallsetoffeaturesrelatedtotherelaxedconstraintsineachdecomposition,aMLrankingfunctionisstillabletobecompetitivewithheuristictechniques.Suchaﬁndingispromisingforfutureconstraintrelaxationapproaches,asthesefeaturescanbeusedtoguidedecompositioncreation.Finally,wehighlightwhereaMLrankingfunctionwouldbebeneﬁcialinadecompositioncreationframework.Keywords:MachineLearning,IntegerProgramming,Heuristics,LargeScaleOptimization1.IntroductionAsMixedIntegerProgramming(MIP)problemscontinuetoincreaseinbothsizeandcomplexity,newsolutiontechniquesarerequiredtokeepupwiththeincreasedproblemcomplexities.ThiscontinualincreaseisreﬂectedinthepopularMIPbenchmarkdatasetMIPLIB,wherefrom2003to2017thelargestinstancesinthedatasetincreasedfromjustover200,000variablestoover1,400,000variables(Achterbergetal.,2006;Gleixneretal.,2021).Asmosttraditionalmethodsrequirenon-polynomialtimecomplexitiesforsolvingtheseMIPs,thisincreaseinproblemsizecannotbeaddressedbyimprovementsmadeincomputerprocessingpoweralone.Onepopularapproachusedtosolvetheselargescaleproblemsisviadecomposition,alongstandingandpowerfulideaforwhichthreeprominentmethods-BendersDecomposition(BD)(Geoﬀrion,1972),Dantzig-WolfeReformulation(DWR)(Vanderbeck&Savelsbergh,2006)andLagrangianRelaxation(LR)(Geoﬀrion,1974)aremostcommonlyused.Allmethodsareoftenabletogeneratetighterboundsthanlinearprogrammingbasedrelaxationbounds,aswellasfeasiblesolutionsifarepairheuristicisusedorifthemethodisembeddedwithinaBranchandBoundalgorithm.Asaresult,thesedecompositiontechniquesarestillprominentintheresearchcommunitytoday,despitebeingdecadesold.1CorrespondingAuthor.EmailAddresses:s3730771@student.rmit.edu.au(J.Weiner),andreas.ernst@monash.edu(A.Ernst),xiaodong.li@rmit.edu.au(X.Li),yuan.sun@monash.edu(Y.Sun)PreprintsubmittedtoElsevierJuly4,2022arXiv:2207.00219v1  [math.OC]  1 Jul 2022Whilstproblemdecompositionhasbeenextremelysuccessful,thisapproachhasonlybeenapplicabletoalimitednumberofproblemtypesintheliterature,forwhichtheproblemstructurelendsitselftobenicelydecomposable.Ideally,whenpractitionersareattemptingtosolveanewandpreviouslyunseenMIPwithoutaknowndecomposablestructure,itwouldbebeneﬁcialifanautomaticdecompositionframeworkcouldbeusedtodetermineiftheproblemmightbeneﬁtfromadecompositionbasedapproachandwhichdecompositionismostpromising.Anautomaticdecompositionframeworksuchasthismightexposenumerousproblemstobedecomposablewhenotherwiseadecompositionapproachwouldnotbeattempted.Atthesametime,itwouldallowpractitionerstotryadecompositionapproachwithouthavingsigniﬁcantdomainspeciﬁcknowledgeabouttheproblem.Whilstanautomaticdecompositionframeworkmightbeextremelybeneﬁcial,untilrecently,therehasbeenonlylimitedworkcarriedoutinthisarea.Threestudiesrelatedtothecreationofanautomaticdecompositionframeworkwerecarriedoutin(Bergneretal.,2015),(Khaniyevetal.,2018)andin(Weineretal.,2020),aswellasanautomatedDWRtoolcalledGCG(Gamrath&L¨ubbecke,2010).In(Bergneretal.,2015),theauthorsrepresenttheoriginalconstraintmatrixasahypergraph,afterwhichak-waypartitioningalgorithmisruninanattempttocreatekequalsizedindependentblocks.ThedecompositionsarethensolvedwithinaDWRframework.Theonlyuserinputrequiredforthisprocedureisthenumberofpre-determinedblocksintowhichtheproblemshouldbedecomposedandanumberofdummynodesforpartitionbalancingpurposes.Onesigniﬁcantdrawbackforthisapproachisthea-priorirequirementofknowingthecorrectnumberofblocksintowhichtheproblemshoulddecompose,afeaturewhichcanvarysigniﬁcantlyonaninstancebyinstancebasis.In(Khaniyevetal.,2018)and(Weineretal.,2020),theauthorsaddressthisissueandproposeautomaticdecompositionmethodswithoutrequiringanyuserinput.In(Khaniyevetal.,2018),thisisachievedviaacommunitydetectionalgorithmandauniquelydeﬁneddecompositionqualitymetriclabelledastheGoodnessscore.Thisscoreisderivedfrombothnon-zeropercentagesinthesubproblemsandtheproportionofrelaxedconstraintsintheborder.Theauthorsin(Weineretal.,2020)insteadtreattheproblemofdecompositionasamulti-objectiveone,usingtheNon-DominatedSortingGeneticAlgorithmII(NSGA-II)algorithm(Debetal.,2002)toautomaticallycreatedecompositionswhichminimizethenumberofconstraintsrelaxedandthesizeofthelargestsubproblemsimultaneously.Whatallthreepaperspotentiallylackisthattheyonlyconsiderasmallsetoffeaturestoguidetheautomaticdecompositionprocess,withthenumberofblocks,thenumberofrelaxedconstraintsandthesizeofthesubproblemsprimarilyusedasfeatures.Inaddition,forpopulationbasedheuristicssuchastheNSGA-IIalgorithmdescribedin(Weineretal.,2020),whenmultipledecompositionsareavailable,havingasingleobjectiverankingfunctionbecomesimportant.Morerecently,anewbranchofworkhasemerged,consideringnumerousfeaturesandusingaMachineLearning(ML)methodtoidentifybothwhenadecompositionapproachshouldbeattemptedandwhatdecom-positionismostpromisingamongstseveralcandidates.Theauthorsin(Kruberetal.,2017)trainavarietyofclassiﬁerstoanswerthequestionofwhetherornotadecompositionislikelytoresultinproducingbetterboundsthansimplyusingagenericsolverontheoriginalproblem.Whilstsomesuccesswasshownusingthisapproach,thislineofresearchonlyhelpstoaddresswhetherornotadecompositionapproachmaybeappropriate.Itdoesnot,however,provideinsightastowhatdecompositionshouldbeselectedforsolvingwhenconsideringmultiplecandidates.Theauthorsin(Bassoetal.,2020)aretheﬁrsttoattempttoﬁllinthisgapregardingtherankingofdecompositionqualitya-prioribasedonavarietyofdecompositionfeatures.Whilstsomepromisingresultswerefound,ultimatelythislineofresearchisstillinitsinfancy,withonlylimitedsuccessbeingdemonstrated.Inparticular,webelievefurtheranalysisisrequiredregarding1)Whatroleinstancesimilarityplaysinrankingperformance;2)ifaMLrankingfunctionisabletoaccuratelypredictdecompositionqualityintestinstancesnotsampledinthetrainingset;3)howaMLrankingfunctionperformswhenbenchmarkedagainstotherheuristicmeasuresofdecompositionquality;and4)arethereanyconstraintfeatureswhichcanbeidentiﬁedasbeingimportanttodecompositionquality,asthiscouldbeusedinfuturesamplingprocedures.Inthispaper,weprovideanextensiveanalysisregardingtheeﬀectivenessofaMLapproachtorankde-compositionquality,whereacombinationofsolvetimeandboundqualityareusedasametricforquantifyingdecompositionquality.Todoso,wehavecreatedasigniﬁcantdatasetofover40,000uniquedecompositionssampledfrom24instancesintheMIPLIB2017library.Thesedecompositionswerecreatedusingthemulti-objectiveapproachasdescribedin(Weineretal.,2020)whichisabletoproducearichsetofParetooptimaldecompositionsusingbothborderandsubproblemmetrics,aswellasagreedyrandomselectionalgorithmto2D1D2D3...Dk······Ac······Figure1:BorderedBlock-DiagonalMatrixStructure.DecomposedsubproblemsD1,...,DkcontainindependentvariablesandcanbesolvedindependentlyifthecomplicatingconstraintsAcareremoved.ensurethedatasetmaintainssomediversity.WeshowtheeﬀectofinstancesimilarityonMLperformance,anddemonstratehowaMLrankingfunctioncanproducestate-of-the-artpredictionswhenbenchmarkedagainstfourestablishedheuristicfunctionsfoundinboththeliteratureandinonlinesolvers.Wealsoshowwhichcon-straintfeaturescouldbebeneﬁcialinguidingfutureautomaticdecompositionframeworksinvolvingconstraintrelaxation.Finally,weprovideasummaryandpossiblefutureworkonhowourﬁndingsmaybeusedinaheuristictocreatefullyautomateddecompositions.Therestofthispaperisstructuredasfollows.Section2introducesthebackgroundandrelatedwork.OurmainapproachisdescribedinSection3.Section4detailstheexperimentaldesign,Section5presentstheresultsandSection6thenconcludesthepaper.2.BackgroundInthissectionwebrieﬂydescribetheLagrangianRelaxationframeworkforempiricalevaluationofade-composition’seﬀectivenessandpreviousworkswhichattempttoquantifydecompositionqualitythrougheitherheuristicorMLbasedmethods,thegreedyandNSGA-IIframeworksusedtocreatedecompositionsasdescribedin(Weineretal.,2020).2.1.LagrangianRelaxationLagrangianRelaxation(LR)isapopulardecompositiontechniquethatiscommonlyimplementedintheOperationsResearch(OR)community.LRismostoftenappliedwhenthe‘right’setofcomplicatingconstraintsareidentiﬁedandrelaxed,therebydecomposingtheoriginalproblemintomultipleindependentsubproblemswhicharesigniﬁcantlyeasiertosolve.ProblemssuitableforthistypeofdecompositiondisplayangularmatrixpatternssuchasthoseseeninFigure1.Inthisangularconstraintmatrix,oftenreferredtoasaBorderedBlock-Diagonalmatrixstructure,therearecomplicatingconstraintsAcandindependentblockstructuresD1,...,Dk.Ifthesecomplicatingconstraintswereremoved,thesubproblemsD1,...,Dkarenaturallyabletobedecom-posedandsolvedindependently,withthehopethatsolvingthesesubproblemstooptimalitycanbeachievedsigniﬁcantlyfasterthantheoriginalproblemasawhole,althoughthereisnoguarantee.ThenewLagrangianobjectivefunctionisformedbyshiftingtherelaxedconstraintstotheobjectivefunction,withapenaltyterm(LagrangianMultiplier)attached.AbasicimplementationoftheprimalandLagrangiandualformulationsareshowinEqns(1)-(2)respectively,wherexarethedecisionvariables,caretheassoci-atedcosts,AandbaretheconstraintmatrixandresourceconstraintsandλaretheLagrangianMultipliersintroduced.WhilstthisformulationisrepresentativeofaBinaryLinearProgram,itcaneasilybeextendedtoaMixedIntegerProgram.SolvingthisnewLagrangiandualwiththeoptimalLagrangianMultipliersisoftenabletoprovidehighqualityproblembounds.Inaddition,solutionstothenewLagrangiandualproblemareoftenalmostfeasiblefortheoriginalprimalproblem,andcanoftenbemadefeasiblethroughanappropriaterepairheuristic,orwhenembeddedinaBranchandBoundframework.maxxf(x)=cTxs.t.Ax≤b,Dx≤d,x∈{0,1}n(1)minλ≥0LR(λ)=max(cid:8)cTx+λT(b−Ax):Dx≤d,x∈{0,1}n(cid:9)(2)ThechoiceofdecompositionherecorrespondstothechoiceofhowtheconstraintsarepartitionedbetweentheAandDmatrix.TheeﬀectivenessoftheLRapproachisinﬂuencedbythediﬃcultyinsolvingthenon-smooth3dualproblem(minimisationoverλ)andthetrade-oﬀbetweenthebestlowerboundthatcanbeachieved(tightrelaxation)andtheeaseofevaluatingLR(λ)(small,easilysolvablesubproblems).AsbothboundqualityandsolvetimeareimportantmetricsinevaluatingtheeﬀectivenessofaLRbasedapproach,forthispaperweassignscorestodecompositionsusingbothofthesemetricsasdescribedinSection3.4.2.ToﬁndtheoptimalLagrangianMultipliers,thereexistawidevarietyofpotentialapproaches,includingSub-gradientOptimization(Fisher,2004),BundleMethods(Barahona&Anbil,2000),CoordinateAscent(Wedelin,1995)andHybridtechniques(Ernst&Singh,2012;Weineretal.,2021).ForthepurposesofthispaperwewillavoidthequestionofthesolutionapproachbyevaluatingLR(λ)onlyoncewiththeoptimaldualvaluesλ∗fromthecorrespondingconstraintsintheLPrelaxationoftheprimalproblem(1).WarmstartingtheLRprocedurewiththeoptimaldualLPvaluesalsoprovidesaclearmetricastotheusefulnessofthedecomposition.IftheIntegralityPropertyholds,theoptimalLPsolutionisequivalenttotheLagrangiandualLR(λ∗)andisindicativethatthedecompositionisnotusefulwhensolvedusingLR.ThisisunlessoptimisingtheLagrangiandualisabletobeachievedfasterthansolvingtheLP,whichisveryunlikelytooccurwhensolvinggeneralMIPswithoutaspecialisedalgorithm.Thiswarmstartapproachforﬁndingtheinitial,orpotentiallyﬁnalLagrangianMultipliersisalsoproposedin(Geoﬀrion,1974).AsevenonlysolvingtheLagrangianobjectivefunctioncanbecomputationallyexpensiveforlargeproblems,theaimistoﬁndsomequalitymetricsthataccuratelypredictperformancebasedondirectlyobservableattributesofthedecomposition.2.2.HeuristicDecompositionQualityMetricsWithintheliteraturethereexistsonlyalimitednumberofstudiesinvestigatingthequalityofdecompositionsforgeneralMIPs.TheseapproachescanbesplitintoheuristicbasedmethodsandMachineLearningbasedapproaches.Theheuristicbasedapproachesarepresentedin(Bergneretal.,2015;Khaniyevetal.,2018;Weineretal.,2020)andinvestigatedecompositionqualitiesusingdeﬁnedheuristicmeasures.Twooftheseheuristics(Bergneretal.,2015;Khaniyevetal.,2018)areusedasbenchmarksinSection5andtheﬁnalapproach(Weineretal.,2020)isusedtogeneratethesetofdecompositionsfortrainingandtestingpurposes.RelativeBorderAreaIn2015,theﬁrstpaper(Bergneretal.,2015)demonstratingthepotentialforanautomaticdecompositionapproachwaspresented,inthecontextofsolvingdecompositionsviaaDantzig-WolfeReformulationprocess.Todoso,theauthorsrepresenttheconstraintmatricesofgeneralMixedIntegerProgramsashypergraphs,forwhichtheythensolvetheminimumweightbalancedk-partitionproblem(Karypisetal.,1999)inanattempttocreatekequalsizedsubproblemswhilstreducingthenumberofrelaxedconstraintsrequiredtoﬁndsuchapartition.WithoutpresentingaframeworktodiscoverwhichsingledecompositionshouldbesolvedviaDWR,theauthorsinsteadperformanexploratoryanalysisondiﬀerentparameterselections,inparticularthenumberofksubproblemscreated.Theauthorsalsopresentaheuristicmeasurewhichtheysuggestcanbeusedtoﬁndgooddecompositionsfromamongstseveralcandidates,referringtothismeasureastheRelativeBorderArea(RBA).Formally,theRBAispresentedas:ml×n+m×nl−ml×nlm×nwheremlisthenumberoflinkingconstraints,nlisthenumberoflinkingvariables,misthetotalnumberofconstraintsandnisthetotalnumberofvariables.SmallRBAvaluesareindicativeofhighqualitydecompositions.Theauthorsnotehowever,thatfor26outof39instancestested,decompositionswithoutlinkingvariablesperformedthebest.Whennolinkingvariablesareconsidered,theRBAissimplymlm,thepercentageofconstraintsrelaxed.Whilstthisisafairlysimpleheuristic,theauthorsnote‘DWRunfoldsitspotentialwhenrelaxingafewlinkingconstraintsisenoughtodecomposetheproblemintomoretractablesubproblems’.GoodnessScoreAnotherautomaticdecompositionframeworkispresentedin(Khaniyevetal.,2018)andusesacommunitydetectionalgorithmtomaximiseauniquelydeﬁneddecompositionqualitymetrictheauthorshavedeﬁnedastheGoodnessscore.Asnotedbytheauthors,agooddecompositioncanbecalculatedbyanalysingboththesubproblemandbordercomponentsseparately,whicharelabelledasQandPmeasuresrespectively.ThesemetricsarethencombinedtogiveanoverallGoodnessscore.Thesubproblemandborderscorescanbecalculatedasfollows:4SubproblemComponentsWithregardstosubproblemcomponents,threefactorsareidentiﬁedinbeingimportanttodecompositionquality:1.Granularity-Adecompositionwithalargenumberofsubproblemsisdesirable,asintheory,solvingsmallersubproblemsiscomputationallybeneﬁcial.2.Homogeneity-Thesubproblemsshouldideallybeequallysized.Homogeneityisespeciallyimportantwhensolvingsubproblemsonparallelprocessors,ensuringthatthelargestsubproblemdoesnotdominatethesolvetimetaken.3.Isomorphism-Ideally,subproblemsshouldbeidenticalnotonlyinsize,butinobjectivefunctioncoeﬃcientsandright-handsidevalues.ThisresultsinasinglesubproblemrequiredtobesolvedateveryiterationoftheLagrangianRelaxationalgorithminsteadofallsubproblems.TheoverallproxyusedtomeasurethesesubproblemstatisticsispresentedasQ=(PKi=1nzim[1−nzim])wherenziisthenumberofnonzeroentriesinblocki:∀i∈Kblocksandmisthenumberofnon-zeroesintheblock-diagonalcomponentoftheconstraintmatrix.BorderComponentFollowingonfromtheresultspresentedin(Bergneretal.,2015),thequalityofthebordercomponentcalculatedwithintheGoodnessscoreisthepercentageofconstraintsrelaxed,albeitinasimpleexponentialdecayfunction,astheauthorsnotethatthereisnon-linearityinthecorrelationbetweenbordersizeandoptimalitygap.TheoverallproxyusedtomeasurethequalityoftheborderispresentedasP=(e−λ(bM))wherebisthenumberofconstraintsintheborderandMisthetotalnumberofconstraints.TheﬁnalGoodnessscoreforadecompositioniscalculatedasQ×Pandisboundedbetween[0,1].Decompo-sitionswhichhavelargerGoodnessscoresareconsideredtobehigherquality.Multi-objectiveApproachAmulti-objectiveapproachforautomaticallygeneratinggooddecompositionsispresentedin(Weineretal.,2020)andusesthewellknownNonDominatedSortingGeneticAlgorithmII(NSGAII)tocreateandevolveapopulationofdecompositions.InamannersimilartotheGoodnessscore,twoobjectivesareminimisedwhichhavebeenshowntoresultinhighqualitydecompositions.Themulti-objectivealgorithmaimstominimiseboththenumberofconstraintsrelaxed(smallborderarea)andthesizeofthelargestsubproblemforeachdecomposition.Thisapproachwasusedtogenerateboththetrainingandtestdatasetsusedinthispaper,asthisframeworkisabletogeneratealargenumberofgooddecompositionsinarelativelyshortamountoftime.HeuristicMeasuresoutsideoftheliteratureWithintheGCGsolver(Gamrath&L¨ubbecke,2010)aqualitymetricforgeneraldecompositionwithlinkingconstraintsonlycanbefoundintheopensourcecode.Forbenchmarking,wehavelabelledthisheuristicasGCGOpenSource(GCGOS),andiscalculatedas:(0.6×mM)+0.01+(0.2×(1−min{d1,...,dk}))wheremisthenumberofconstraintsrelaxed,Misthetotalnumberofconstraintsanddkisthenon-zerodensityofthecoeﬃcientmatrixforsubproblemk.AnotherheuristicthatexistsistheMax-White(MW)scorepresentedontheMIPLIB2017(Gleixneretal.,2021)websiteandisusedtodetermineifaninstanceissuitablefordecompositionornot.TheMWscoreiscalculatedas1−(s+tnvars∗ncons)wheret=nvars1×ncons1ands=Pnb+1i=2nvarsi∗nconsiwherenbisthenumberofsubproblems,withtheﬁrstsubsetbeingtheborder.2.3.RandomSamplingandMachineLearningToourknowledge,theauthorsin(Bassoetal.,2020)aretheﬁrsttoaddressthequestionofhowtorankdecompositionqualitya-prioribasedoninstanceanddecompositionfeatures.Todoso,apseudorandomsamplingalgorithmwasusedtogeneratedecompositions,andbothclassiﬁcationandregressionmodelsweretrainedtopredictParetooptimalsolutions(whereboundqualityandsolutiontimeformthetwoobjectives),andthensubsequentlyranktheselecteddecompositionsaccordingtodistancesfromtheclosestParetooptimal5solution.Thisframeworkisreferredtoasthedata-drivenprocess.Forclassiﬁcation,ininitialexperimentswhendecompositionsfromeachbaseinstancewereusedasthetestsetandtheclassiﬁerwastrainedondecompositionsfromallotherinstances,poorresultswereobservedfromallbut5outofthe36instancestested.Furthermore,evenwhendecompositionsfromallinstanceswereincludedinthetrainingset,thetestprecisionscore(P),whereP=TPTP+FP,isonly0.0714,withasigniﬁcantnumberoffalsepositivespredicted.Theauthorsgivetwopotentialreasonsforthepoorresultsthataroseduringclassiﬁcation.Firstly,theauthorsnotethatpositivedecompositionsindiﬀerentinstancesmightnothaveenoughcommoncharacteristics,andsuggestthatperhapsmorepositivedecompositionsarerequiredinthedataset.Becauseofthisﬁnding,wehaveincludeddecompositionsinourdatasetwhichhavebeencreatedviatheNSGA-IIalgorithmasdescribedin(Weineretal.,2020),asthesedecompositionshavebeenshowntobeofgreaterqualitythanthoseproducedbyapseudo-randomsamplingprocess.Asecondreasongivenforpoorclassiﬁcationresultsisthatinstancesinthedatasetarelikelytobestructurallydiﬀerent,andthereforeincludingdecompositionsfromallinstancesfortrainingcouldbedetrimentalifgooddecompositionpatternsvarydependingonthestructureoftheinstance.Theauthorssuggestfurtherresearchisneededtoinvestigatetheeﬀectofinstancesimilarityandtheperformanceofatrainedclassiﬁer.Assuch,inthispaperwefocusonproblemsthatincludeanetworkstructureinthehopeofincreasinginstancesimilarity.WeprovideananalysisontheimportanceofinstancesimilarityinSection5.1.Thenextphaseofthedata-drivenapproachwastotrainaregressorinordertothenrankthepositivedecompositionschosenbytheclassiﬁcationstep.Whilsttherearesomeinterestingpreliminaryresults,asnotedbythesameauthorsin(Basso&Ceselli,2018),theperformanceoftheregressortendedtobequitepoorwhenusedasarankingfunction.In(Basso&Ceselli,2018),theauthorsattempttodeﬁneanewrankingfunctionbasedondominancepercentageinsteadofParetodistances,withsomeimprovementsbeingnoted.Finally,thebenchmarkingcarriedoutin(Bassoetal.,2020)onlycomparesalimitednumberofdecompositionscreatedviaadata-drivenprocessagainstdecompositionscreatedbythestaticdetectorsinGCG.Whatislacking,ishowaMLrankingfunctioncomparestootherheuristicbaseddecompositionrankingfunctions,suchastheMax-Whitescore(Gleixneretal.,2021),thedecompositionscoreforborderedblock-diagonaldecompositionsasfoundinGCG’sOpenSourcecode(GCGOS),theRelativeBorderArea(RBA)metricdiscussedin(Bergneretal.,2015)andtheGoodnessScorepresentedin(Khaniyevetal.,2018).3.ApproachTheapproachcarriedoutinthispaperconsistsoffourmaintasksthataredescribedinmoredetailbelow:1.Decompositiongenerationusingbothgreedy-randomandmulti-objectiveapproaches.2.Decompositionpost-processing,includingtheremovalofbothredundantconstraintsandduplicatede-compositions.3.EstablishingMIPboundsbysolvingtheLagrangianfunctionfordecompositionsusingtheoptimaldualvaluesfromtheLPrelaxationasLagrangianMultipliers.4.Analysisofdecompositionresults.Theanalysiscarriedoutaddressedthreemainareas:•Investigatetherelationshipbetweeninstancesimilarityandpredictionperformance.•BenchmarkingofMLmethodsagainstHeuristictechniquesforbothsimilaranddissimilarinstances.•InvestigatetherelationshipbetweenRelaxedConstraintfeaturesandpredictionquality.3.1.DecompositionGenerationAsnotedbytheauthorsin(Bassoetal.,2020),usingarandomisedsamplingalgorithmfromanarbitraryMIPisunlikelytoyieldpromisingdecompositions.Theauthorsthereforeimplementedapseudorandomsamplingalgorithm,selectingconstraintswithaprobabilityinproportiontotheirsparsity,inamannersimilartothegreedy-randomdecompositionsasdiscussedin(Weineretal.,2020).Forthedatasetusedinthispaper,weuseddecompositionscreatedbothbytheNSGA-IIalgorithmandgreedy-randomapproachesasdescribedin(Weineretal.,2020).WeincludedtheNSGA-IIgenerateddecompositionsasthesewerepreviouslyshowntoproducehighqualitydecompositionswhencomparedtogreedy-randomgenerateddecompositions.Additionally,inordertomakesurethedatasetwasnottoobiasedtowardsdecompositionsthatonlyconsiderthetwometricsusedinthemulti-objectiveapproach,thegreedy-randomapproachwasalsoincluded.Forbothapproaches,6theconstraintmatrixoftheMIPbeingsampledistranslatedintoahypergraph,whererowsarerepresentedbyhyperedgesandcolumnsformthenodeswithinthehypergraph.Oncethehypergraphiscreated,thesetofconstraintstorelaxareselectedandconsequentlythecorrespondinghyperedgesareremovedfromthehypergraph.ABreadthFirstSearch(BFS)algorithmisthenruntoidentifytheindependentpartitionswhichnowexist,representingtheindependentsubproblemscreated.TheBFSalgorithmisabletoruninO(nz)wherenzisthenumberofnon-zeroentriesintheconstraintcoeﬃcientmatrix.Inthispaperwehavealsointroducedtwopostprocessingstepstoremoveunnecessarydecompositioncomponentsinordertobetterﬁlterthedataset.Greedy-randomDecompositionsAgreedy-randomapproachwasusedtogenerateadiverserangeofdecompositions,withabiasinrelaxingconstraintswithalargernumberofnon-zeroes,inthehopethatdoingsowouldleadtomoresubproblemswhicharesmaller.Tocreateadecomposition,constraintsaresortedaccordingtothenumberofvariablestheycontain,thentheyareiteratedoverwithaprobabilityofbeingrelaxedequaltopi=|Vi|SV×Q×|C|,wherepiistheprobabilityofconstraintibeingrelaxed,Viisthesetofvariablescontainedinconstrainti,SVthesumofallvariablecountsacrossthesetofallconstraintsCintheoriginalproblemandQisthedesiredproportionofconstraintstoberelaxed.Thisiterativeloopisrununtilthedesiredproportionofrelaxedconstraintshasbeenselected.NSGA-IIDecompositionsTheNSGA-IIdecompositionswerecreatedasdescribedin(Weineretal.,2020).IntheNSGA-IIimplemen-tation,theﬁtnessfunctionconsistsofminimisingtwoobjectives:1)thenumberofconstraintsrelaxedand2)thesizeofthelargestsubproblem(measuredbythenumberofvariables).Theinitialpopulationisseededwithsomegreedy-randomsolutions,containingavarietyofdiﬀerentpercentagesofrelaxedconstraints,inaneﬀorttoassistwithsearchexploration.Anarbitrarynumberofindividualsareinitialisedwithvaryingnumbersofconstraintsrelaxed,from5%to99%ofthetotalnumberofconstraints.3.2.DecompositionPost-processingTwodecompositionpost-processingstepshavebeencreatedtoeliminatebothredundantrelaxedconstraintsandduplicatedecompositions.Forthepurposesofthispaper,whenreferringtoaconstraintasredundant,itisdonesowithregardstohowlikelyitistoaﬀecttoeitherthesolvetimeorboundqualityofthedecompositionifthisconstraintisrelaxed.Itisnotsuggestingthatremovingsuchaconstraintdoesnotchangethesetoffeasiblesolutions.Postdecompositioncreation,wehaveidentiﬁedtwowaysinwhicharelaxedconstraintmaybeconsideredredundantandnotprovideanymeaningfulcontributiontodecompositionquality.Examiningtherelaxedconstraintsandthesubproblemswhichoccurasaresult,arelaxedconstraintisconsideredredundantifeither:1.Allvariablesassociatedwiththeconstraintarealreadyasubsetofthevariablesinoneofthesubproblems.2.Allvariablesassociatedwiththeconstraintareonlyfoundinsinglevariablesubproblems.Intheﬁrstdeﬁnitionofaredundantconstraint,ifallvariablesofarelaxedconstraintarecontainedwithinasubproblem,movingtheconstraintfromtheborderbacktothesubproblemisunlikelytoaﬀectthesolvetimeofthesubprobleminanysubstantialway,howeveritcanresultintighterbounds.Assuchwehavedeemedthisconstrainttoberedundantforthepurposesofdecomposition.Intheseconddeﬁnitionofaredundantconstraint,ifallvariablesinaconstraintarefoundinonlysinglevariablesubproblems,thesolutionofthevariablesinthesesubproblemsissimplysettothevariablebounds,providingnotighteningoftheLPbound.AnexampleoftwoconstraintswhichareconsideredredundantisshowninFigure2.Showningreenisaconstraintwhichisconsideredredundant,asallnon-zeroescontainedwithinarealreadypartofanexistingsubproblem.Showninredisaconstraintinwhichallnon-zeroesarecontainedonlyinsinglevariablesubproblems.Thesesinglevariablesubproblemscanariseasaresultofconstraintrelaxation,wherevariablesarenolongerpartofanyconstraintsandaresimplyconstrainedbytheirbounds.Finally,afterallconstraintpost-processing,allduplicatedecompositionsareremovedfromthedataset.7Figure2:ConstraintRedundancyProcessing.Showningreenandredaretwoexamplesofrelaxedconstraintswhichcanbeconsideredtoberedundant.Showningreenisanexampleofaredundantconstraintinwhichallnon-zeroesbelongtoasubproblem,inwhichcasethereisnodecompositionofthesubproblemwhenthisconstraintisrelaxed.ShowninRedisaconstraintinwhichallnon-zeroesarefoundonlyinsinglevariablesubproblems,forwhichwhensolvedprovidenoadditionaltighteningoftheLPbounds.3.3.EstablishingMIPBoundsToestablishMIPbounds,theLPrelaxationoftheoriginalproblemwassolvedﬁrst,withtheoptimaldualvaluesλ∗usedastheLagrangianMultipliers.TheLagrangianMultiplierswerethenusedtosolvetheLagrangianfunction.Eachindependentsubproblemwassolvedtowithin1%ofoptimality,asatthisstageonlyboundqualityisconsideredandtherecanbesigniﬁcanttimespentbymodernsolverstryingtoproveoptimality.ThisevaluationoftheLagrangianfunctionwasruninaneﬀorttoclosetheintegralitygap(Fisher,2004),improvingupontheLPrelaxationboundandprovidingaproxyofthedecompositionboundquality.SolvingtheLagrangianfunctionwithLagrangianMultiplierssettotheoptimaldualvaluesfromthecor-respondingconstraintsintheLPrelaxationoftheprimalproblemwaschosenforseveralreasonsandisalsorecommendedin(Geoﬀrion,1974).First,duetothesigniﬁcantnumberofdecompositionscreated(≥40,000),thecomputingtimerequiredtorunthefullLagrangianRelaxationalgorithmwithamultiplierupdateprocedureforseveraliterationswouldbecomputationallyprohibitive.Aseachsubproblemisattemptedtobesolvedtooptimality,thisprocesscanbeextremelycomputationallyexpensive.Secondly,theintroductionofamultiplierupdateprocedureaddsanotherlayerofcomplexityandcouldresultindecompositionqualitiesbeingreliantontheupdateprocedure,forwhichthereexistavarietyofpotentialmethodsavailable,suchasSubgradientOptimization(Fisher,2004),BundleMethods(Barahona&Anbil,2000),CoordinateAscent(Wedelin,1995)andHybridtechniques(Ernst&Singh,2012;Weineretal.,2021).Inaddition,forsomeofthesemethods,astochasticelementexistsforwhichmultiplerunswouldneedtobecarriedout.Duetotherelaxationofconstraints,theremayexistsubproblemswhichcontainonlysinglevariables.Forthesesubproblems,theycanbesolvedtooptimalityusingthevariableboundsinsteadofviasolvingaMIP.Afterrelaxation,subproblemsweresortedinascendingorderbyvariablesizesandgivenaruntimelimitproportionaltothesquarenumberofvariables,asalinearrelationshipbetweensubproblemvariablesizesandMIPsolutiontimesseemsunlikely.Thesolvetimelimitforeachsubproblemwascalculatedasti=(v2i/PKi=1v2i)×CPUwheretiisthesubproblemruntime,viisthenumberofvariablesinsubproblemi,KisthesetofsubproblemsandCPUisthetotalCPUbudgetallocatedtosolvetheLagrangianObjectivefunctionwiththeoptimalLPdualvalues.Fortheﬁnalandlargestsubproblem,thefullremainingrun-timewasallocated.Ifasubproblemwasunabletobesolvedtooptimalitywithinthegivenrun-timelimit,thebestboundfoundduringtheBranchandBoundprocesswasusedasthesubproblemsolution.Iftherootnodewasunabletobe8processedinthistime,anadditional60(s)ofCPUruntimewasgiveninanattempttoestablishavalidboundforthesubproblemandtoavoidpotentialbounderrorsfoundbytheMIPsolverused.Intherareeventthattherootnodewasstillunabletobeprocessed,thesubproblemwassolvedasaLPandtheLPboundwasused,withnoadditionaltimetakentosolvethesubproblemasaLPcountedtowardsthetotalruntime.3.4.DatasetSelectionInaddressingtheﬁrstpartoftheanalysisprocessasdescribedinSection3regardinghowinstancesimilarityaﬀectspredictionperformance,weselectedthreediﬀerentnetworkproblemtypesfromtheMIPLIB2017library(Gleixneretal.,2021),formingwhatwerefertoastheNetworkdataset.Thesethreeproblemtypes-NetworkDesign(ND),FixedCostNetworkFlow(FCNF)andSupplyNetworkPlanning(SNP)eachcontaininstancesgeneratedfromthesameoptimisationmodelbutusingdiﬀerentproblemdata.InordertoaddresshowwellaMLapproachwouldthengeneralisetorandomlyselectedinstances,wealsoselected10instancesfromtheMIPLIB2017librarycontainingpropertieswhichrepresentabroadrangeofpotentialunseeninstances.WerefertothedatasetcomprisedoftheseinstancesastheRandom(Rand)dataset.The10instancesfromtheMIPLIBlibrarywerechosensuchthat1)AsigniﬁcantproportionofdiscretevariablesinordertofacilitatetighteningoftheLPbound,although,thisisnotnecessarilyalwaysarequirement,asevenafewbinaryorintegervariablescanstillsigniﬁcantlytightentheLPbound;2)Areasonablenumberofnon-zeroesinorderforconstraintprocessinganddecompositioncreationwereabletorunwithinareasonableamountoftime;3)Apre-processingsteptoensurethattheLPboundsformostoftheinstanceswasnotoptimal,anecessaryrequirementinordertoeﬀectivelyrankdecompositions;4)Someinstanceswhichareeasilysolvedtooptimalityquickly,perhapsindicatingthatrelaxingnoconstraintsisconsideredthebestdecomposition.ThefullsetofinstancesandinstancestatisticsisshowninTable1.3.4.1.FeaturesConsideredWhilstthereisnoconsensusamongsttheoptimisationcommunityastoexactlywhichinstanceanddecom-positionfeaturesdirectlycorrespondtodecompositionquality,ithasbeenfoundthatminimisingthenumberoflinkingconstraintsandmaximisingthenumberofsimilarlysizedsubproblemscanclosetheintegralitygapandresultinfasterdecompositionruntimesrespectively(Bergneretal.,2015;Khaniyevetal.,2018).Minimisingthenumberofrelaxedconstraintsandthesizeofthelargestsubproblemwerealsoshowntobebeneﬁcialingen-eratinggooddecompositions(Weineretal.,2020).InordertoexploittheMachineLearningmodelapproach,weuseamoreextensivelistoffeatures,relyingonbothsubproblemstatisticsandborderstatistics.ThefulllistoffeaturescanbefoundinTableA1intheAppendixandcontainmanyofthefeaturesusedin(Kruberetal.,2017)and(Bassoetal.,2020),forwhichtheauthorsnotethereexistscombinationoffeatureswhichareimportanttodecompositionquality.ShowninAppendixFiguresA1andA2arethespreadofthenormalisedfeaturedataforboththeNetworkandRandomdatasetsrespectively.Withinthesedatasets,thepercentageofdecompositionsinwhichallfeaturesarebetween[Q1−1.5×IQR,Q3+1.5×IQR],whereQ1isthe25thpercentile,Q3isthe75thpercentileandIQRistheinterquartilerange,are9.68%and4.43%respectively.SuchaspreadhighlightsthepotentialdiﬃcultyofusingaMLapproach,asfeaturedatacanvarysigniﬁcantlybetweeninstances,containingasigniﬁcantnumberof‘outlier’points.3.4.2.TrainingandTestingFinally,forthepurposesofthispaper,thebestdecompositionforeachinstanceistheonethatachievesthelowestscore,whichwehavedeﬁnedasacombinationofbothboundquality(representedastheoptimalitygap(|UB−LBUB|∗100)andsolvetime.UsingtheWeightedSumModel(Triantaphyllou,2000),weattributeequalimportancetobothgapvalues(g)andsolvetimes(t),score=0.5∗g+0.5∗t,althoughinfutureworksthisweightingcanchangedependingonuserrequirements.Alldecompositionsareassignedscoresonaninstancebyinstancebasis,wherealloptimalitygapsandsolvetimesarenormalisedusingMin-Maxnormalisation.AMin-Maxnormalisationprocesswasalsousedtonormaliseallfeaturesnotalreadyina0-1scalerange.Avarietyofregressionmodelswerethentrainedtopredictdecompositionscore,inaneﬀorttorankthequalityofdecompositionsa-prioribasedonthefeaturesconsidered.BecauseaMLmodelisonlyusefulifdecompositionqualitycanbepredictedforunseeninstances,anapproachapproximatingLeave-One-Out-Cross-Validation9Table1:InstanceStatistics:Shownforinstanceinthedatasetarethetotalnumberofvariables(var),numberofbinary(bin),integer(int)andcontinuous(cont)variables,numberofconstraints(constr)andthenumberofnon-zeroes(nz).Eachinstanceisalsoshownwhichproblemtypeitbelongsto.InstanceNameVarBinIntContConstrNZProblemTypecost266-UUE416117103990144612312NDdfn-bwin-DBE3285247508102359855NDgermany50-UUM69710886883208820737NDta1-UUM2288060516834395654NDta2-UUE9241118808053268726533NDg200x740148074007409402960FCNFh50x24504900245002450254912152FCNFh80x6320d12640632006320655831521FCNFk16x240b4802400240256960FCNFsnp-02-004-104228350167167228016126512463941SNPsnp-04-052-05222143845464546212346129662459205SNPsnp-06-004-052328461494494327473183168668716SNPsnp-10-004-0525387778158155371473003481097780SNPsnp-10-052-05254902111059110595269033208361138760SNPsplice1k1325332521065051761020Randneos-4954672-berkel1533630090318488007Randdws008-0111096660804488606456400Randtraininstance2128905278260250101560341531Randneos-4338804-snowy13441260424217016342Randneos-4387871-tavua4004200002004455423496Rand30n20b81838018318620576109706Randair05719571950042652121Randblp-ic981364013550090717191947Randair0310757107570012491028Rand(Sam,2010)methodwasusedfortestingandisfurtherdescribedinSection5.Theregressionmodelstrainedincludesomeofthemostpopularlinearandnonlinearregressionmodelsaswellastwodiﬀerentensemblemethodsincluding:1.LinearRegressionwithRidgeRegularisation(Ridge)2.LinearRegressionwithLassoRegularisation(Lasso)3.SupportVectorRegression(SVR)witharadialbasisfunctionkernel4.KNearestNeighboursRegression(KNN)5.RandomForestRegressor(RF)6.Multi-LayerPerceptron(MLP)7.StackingEnsembleusingRidge,Lasso,SVR,KNNandMLPtrainedregressorsandaLinearRegressionEstimator(Stacking)8.VotingEnsembleusingRidge,Lasso,SVR,KNNandMLPtrainedregressors(Voting)4.ExperimentalSetupHypergraphpartitioningandNSGA-IIalgorithmswererunonanInteli7-7500UCPUandallLRtestswerecarriedoutontheMulti-modalAustralianScienceSImagingandVisualisationEnvironment(MASSIVE)network,whichrunsonanIntelXeonCPUE5-2680v3processor.CPLEX12.8.0usingasinglethreadwasusedtosolveallMIPsubproblemsandLPbenchmarks.AllLRrunsweregivenalimitedruntimeof300CPU10secondsforfeasibilityreasons,asthetotalnumberofdecompositionstestedwasinexcessof40,000asshowninTable??.TheNSGA-IIalgorithmwasrunusingthePagmoframework(Biscani&Izzo,2019)withdefaultparametersettings.Thesesettingsinclude:CrossoverProbability=0.95,DistributionindexforCrossover=10.0,MutationProbability=0.01,DistributionindexforMutation=50.0.TheNSGA-IIalgorithmwasrunusing300generationswithpopulationsizesof32.Whilstgoodconvergencewasdemonstratedforsmallergenerationnumbers,usingthepercentageofParetooptimalsolutionsingenerationnwhichwerealsofoundingenerationn−1asaperformancemeasure,duetothefastrun-timeofourhypergraphpartitioningalgorithm,weincluded300generationsinordertoattainaricherandmorediversedataset.ItshouldbenotedthatforsomeoftheSupplyNetworkPlanninginstances,thefullNSGA-IIalgorithmwasunabletobecompletedwithintheallocatedruntime.ThisisduetotheimplementationoftheNSGA-IIalgorithm,forwhichitwasdiscoveredpostanalysistherewasaslightlogicerrorwithinthecrossoverimplementation.Forfuturework,thiserrorwithinthecrossoverimplementationmaybeaddressedtospeedupthesearchtime,howeverforourpurposesasuﬃcientnumberofdecompositionsweregeneratedforanalysis.Forthegreedy-randomdecompositions,999decompositionsweregeneratedforeachinstance,with111decompositionscreatedforspeciﬁedrelaxedconstraintpercentagesrangingfrom10%to90%oftotalconstraints.Includedamongstthedecompositionstestedwasalsoadecompositioninwhichnoconstraintswererelaxed.Whileover10,000decompositionsforeachinstanceweregeneratedwiththeaboveapproaches,afterremovalofduplicatesthenumberofdecompositionsavailablefortrainingandtestingisintherangeof500–2,500perinstanceasshowninTable??.AllMLmodelsweretrainedwithdefaultparametersettingsaspresentedintheScikit-learnlibrary(Pe-dregosaetal.,2011)withPython3.6.13.TheonlyexceptiontothiswerethealpharegularisationparameterschosenfortheLassoandRidgeLinearRegressionmodels,forwhicharangeofregularisationparameterswastested,asthetrainingtimeforthesemodelswasrelativelyinsigniﬁcant.FortheLassoandRidgemodels,thealphaparametersselectedwere0.001and0.01respectively.5.ResultsMultipleexperimentswerecarriedouttoidentify1)TheeﬀectofinstancesimilarityonthepredictionqualityoftheMLmethods;2)HowdoesaMLrankingapproachcomparetootherheuristicbasedrankingfunctionsfoundintheliteratureandinopensourcesolvers?;3)Whichrelaxedconstraintsfeaturesareimportanttodecompositionquality?and4)HowwelldoMLmethodsperformwhentestedonrandomlyselectedinstancesfromtheMIPLIBlibrarywithseeminglynosimilaritiesinproblemstructure?Whenpresentingthescoresofthebestdecompositionselectedbyeachoftherankingmethods,thisisthebestscorefromamongstthetop8decompositionsidentiﬁedbytherankingmethod.Asimilarapproachisusedin(Bassoetal.,2020),asinpracticeitisfairlytrivialtosolve8decompositionsinparallelonmostmoderncomputerarchitecturesandthereforethebestdecompositionamongstthetop8selectedcanbeeasilybeidentiﬁed.Finally,whenpresentingthescoresfromthebestdecompositionsselectedbythediﬀerentrankingmethods,anadditionalMin-Maxnormalisationwascarriedoutfordecompositionscoresonaninstancebyinstancebasis.Thisadditionalnormalisationmoreeasilyshowsthequalityoftheselecteddecompositionsbytherankingmethods,asascoreof0indicatesthebestdecompositionfromthepopulationwasselectedandascoreof1indicatestheworstdecompositionfromthepopulationwasselected.5.1.InstanceSimilarityandPerformanceAsnotedin(Bassoetal.,2020),instancesimilarityisapotentialreasonfortherelativelypoorresultsfoundviatheclassiﬁcationexperimentstheauthorscarriedoutwhenpredictingParetooptimaldecomposi-tions.Therefore,weinvestigatethesigniﬁcanceofinstancesimilarityonpredictionqualitybycomparingMLmodelstrainedandtestedondecompositionsfromthesameproblemtypeandmodelstrainedandtestedondecompositionsfromotherproblemtypes.Foreachinstanceandmodeltype,wetrainedthemodelonalldecompositionsfromthesameproblemtypeexcludingthetestinstance,anapproachapproximatingLeave-One-Out-Cross-Validation(LOOCV).Thepredictionsofthismodelwasthencomparedagainstthesamemodeltrainedonalldecompositionsfromeachoftheotherproblemtypes.ThedecompositionscoresofthebestdecompositionsselectedbytheMLmodelsarepresentedinTablesA4,A5andA6intheAppendix,withboxplotsoftheresultsshowninFigure3.Theseresultsshowthatmodels11trainedandtestedoninstancesfromthesameproblemtypeareabletobetterpredictdecompositionqualitythanmodelstrainedandtestedondiﬀerentproblem.Todetectifthereisanystatisticalsigniﬁcancebetweenmodelstrainedoninstancesfromthesameproblemtypeasopposedtoinstancestrainedoninstancesfromadiﬀerentproblemtype,wecarriedoutpairwisecomparisonsusingtheFriedmanAlignedRankingsandapost-hocanalysisasdescribedin(Derracetal.,2011).Thisnon-parametricstatisticaltestwaschosenasthereisnounderlyingassumptionthatthepredicteddecompositionscoreswouldfollowanormaldistribution.TheFriedmanAlignedRankingsandsubsequentpost-hocanalysiswereusedasthenumberofcomparisonmethodsisrelativelylow(=3)andassuchthisstatisticalapproachisrecommended(Derracetal.,2011).Thez-scoresforeachpairwisecomparisoncanbecalculatedasz=(˜Ri−˜Rj)/rk(n+1)6,where˜Riand˜RjaretheaverageFriedmanAlignedRankforthecontrolalgorithmandcomparisonalgorithmrespectively,kisthenumberofcomparisonalgorithmsandnisthenumberofsamples.Fromthecalculatedz-scores,anunadjustedp-valuecanbefoundfromthetableofnormaldistributionN(0,1).Ineverypairwisecomparison(exceptforone)betweenmodelstrainedandtestedoninstancesfromthesameproblemtype,andmodelstrainedandtestedoninstancesfromadiﬀerentproblemtype,thereisastatisticalsigniﬁcancedetected(p≤0.05)asshowninTablesA4,A5andA6.BasedontheseresultsweconcludethatinstancesimilaritydoesplayanimportantpartinthepredictioncapabilitiesofaMLbasedrankingfunction.EvenamongstinstanceswhichallcontainaNetworkstructure,aMLrankingfunctionclearlyperformsbetterwhentestinstancesarefromthesameproblemtypeasthetraininginstances,albeitfromdiﬀerentdatasources.Inordertovisualiseifinstancesimilaritycanbecapturedonlybyconsideringsomerelativelysimplefeatures,wecarriedoutaPrincipleComponentAnalysis(PCA)usingalineardimensionalityreductiononallinstancesfromthethreenetworkproblemtypes,usingtheinstancederivedfeaturesasshowninTableA7.AkernelbasedPCAwasalsocarriedout,includingthirddegreepolynomial,radialbasisfunction,sigmoidandcosinekernels,howevertheseshowednoimprovementsuponinstanceseparationandclusteringtendencies.AscanbeseeninFigure4,theSupplyNetworkPlanningproblemtypedisplaysexcellentclusteringtendencies,indicatinghighsimilaritybetweentheinstanceswithinthisproblemtype.Similarly,theNetworkDesigninstancesalsoseemtobemoresimilartooneanotherinthisfeaturespace,exceptforoneoutlierinstance.FortheFixedCostNetworkFlowproblemtype,theinstancesseemtobelesssimilartoeachother.ThesePCAvisualisationsappeartobeinlinewithourﬁndingsinFigure3,forwhichthemodelsbothtrainedandtestedontheNetworkDesignandSupplyNetworkPlanningproblemtypesseemtoperformsigniﬁcantlybetterthanwhenthemodelsaretrainedandtestedondiﬀerentproblemtypes.Anadditionalobservationcanbemaderegardinginstancesimilarityinthisfeaturespacewehaveexplored.Inthisfeaturespace,theNetworkDesignandSupplyNetworkPlanninginstancesseemtobemoresimilartoeachotherthaninstancesfromtheFixedCostNetworkFlowproblemtype.Fromthis,wecouldassumemodelstrainedandtestedontheseinstancesmightperformbetterthanmodelstrainedoneitheroftheseproblemtypesandtestedontheFixedCostNetworkFlowproblemtype,orviceversa.Fromourﬁndingsthisisnotalwaysthecase,indicatingthattherearestillotherfeatureswhichcouldbeimportantindetermininginstancesimilarity.5.2.State-of-the-artBenchmarkingInordertoinvestigatehoweﬀectiveaMLmodelisatselectingthebestqualitydecompositionfromapopulationofdecompositions,webenchmarkedthevarioustrainedMLmodelsagainstfourheuristicapproacheswhichhavebeenusedformeasuringdecompositionquality,allofwhichcanbefoundintheliteratureandinsolveropensourcecode.TheseincludethedecompositionscoremetricthatcanbefoundinGCG’sOpenSourcecodebase(GCGOS)(Gamrath&L¨ubbecke,2010),theGoodnessscoreaspresentedin(Khaniyevetal.,2018)(Goodness),theMaxWhitescoreasdescribedonMIPLIB2017(MW)(Gleixneretal.,2021)andtheRelativeBorderAreametricasdescribedin(Bergneretal.,2015).ShowninTable2arethedecompositionscoresofthebestdecompositionsselectedbyeachrankingmethod.FortheMLmethods,eachmodelwastrainedonalldecompositionsfromtheNetworkdatasetexcludingthetestinstance,anapproachapproximatingLeave-One-Out-Cross-Validation.TheseresultsarevisualisedinFigure5,withthetestRootMeanSquareErrors(RMSE)fortheMLpredictionsshowninFigure6.12NDFCNFSNPProblemTypeModelsTrainedOn0.00.10.20.30.40.50.60.70.8DecompositionScores(a)TestingProblemType:NetworkDesignNDFCNFSNPProblemTypeModelsTrainedOn0.00.20.40.60.8DecompositionScores(b)TestingProblemType:FixedCostNetworkFlowNDFCNFSNPProblemTypeModelsTrainedOn0.00.20.40.60.8DecompositionScores(c)TestingProblemType:SupplyNetworkPlanningFigure3:InstanceSimilarityandPredictionQuality:ShownforwheninstancesfromeachproblemtypeintheNetworkdatasetareusedastestinstances(unseenbythetrainedmodels),arethedecompositionscoresofthebestpredicteddecompositionsselectedbyeachoftheMLmodelswhentrainedonthediﬀerentproblemtypes.Formodelstrainedonthesameproblemtypeasthetestproblemtype,themodelistrainedonallinstancesintheproblemtypeexceptforthetestinstance.Whenthetrainingproblemtypeisdiﬀerenttothetestingproblemtype,allinstancesinthetrainingproblemtypeareusedfortraining.Thedecompositionscoresrangefrom0(thebestdecompositionwasselected)to1(theworstdecompositionwasselected).13−0.6−0.4−0.20.00.20.40.60.81.0PrincipalComponent1-ExplainedVarianceRatio=0.60−0.75−0.50−0.250.000.250.500.75PrincipalComponent2-ExplainedVarianceRatio=0.27FixedCostNetworkFlowNetworkDesignSupplyNetworkPlanningFigure4:PrincipalComponentAnalysisoftheNetworkDesign,FixedCostNetworkFlowandSupplyNetworkinginstancesusingthefeaturesdescribedinTableA7.Showninthisﬁgurearethetwoprinciplecomponents,comprising87%oftheexplainedvariance.Ascanbeseen,instancesfromtheSupplyNetworkPlanningandNetworkDesignproblemtypesdisplaygoodclusteringtendencies,indicatinghighsimilaritybetweentheinstanceswithintheseproblemtypes.TheFixedCostNetworkFlowproblemtypeshowspoorclustering,indicatinginstanceswithinthisproblemtypearelesssimilartoeachotherthantheotherNetworkproblemtypes.14Table2:DecompositionPredictionResults:Shownforeachrankingmethodisthedecompositionscoreforthebestdecompositionselectedbythemethod.Thedecompositionscoresrangefrom0(thebestdecompositioninthetestset)to1(theworstdecompositioninthetestset).Showninboldarethescoresforthebestpredicteddecompositionforeachinstance.Alsoshownistheaveragedecompositionscorepredictedbyeachrankingmethod.RankingMethodInstanceNameRidgeLassoSVRKNNRFMLPStackingVotingRBAMWGCGOSGoodnesscost266-UUE.mps0.1260.4110.6420.2650.0000.2060.4340.3780.3630.6420.3630.642dfn-bwin-DBE.mps0.0590.4830.0350.5720.0570.1540.4050.0200.4830.5100.4830.410germany50-UUM.mps0.7040.3110.0710.2260.1470.0370.2390.0710.0710.7040.0710.704ta1-UUM.mps0.0830.3290.3420.1390.1560.2840.0750.1460.0000.4380.0170.438ta2-UUE.mps0.1320.1320.1320.7360.1320.1320.6310.1320.4160.8080.4160.808g200x740.mps0.1970.0000.1490.1280.0880.1450.1080.1380.0000.4350.0000.250h50x2450.mps0.0000.0000.0000.1980.0000.1820.1730.1820.3720.8240.3720.824h80x6320d.mps0.0000.0000.0000.0000.0000.0000.0000.0000.0000.8290.0000.829k16x240b.mps0.3190.0000.1880.4540.5250.1360.1470.1760.0350.8260.0350.858snp-02-004-104.mps0.5120.1480.1380.0910.1080.1380.1720.0910.1480.1100.1480.091snp-04-052-052.mps0.5920.5300.3480.3100.0360.4250.1790.1790.5300.2200.5300.220snp-06-004-052.mps0.2050.1470.0880.0680.1920.1910.1570.1570.4940.0000.4940.000snp-10-004-052.mps0.2440.1960.0000.0180.0900.0000.0000.0000.4990.1330.4990.133snp-10-052-052.mps0.0720.2420.0000.2100.0320.0000.0000.0000.5490.0320.5490.032Average0.2320.2090.1520.2440.1120.1450.1940.1190.2830.4650.2840.44615RidgeLassoSVRKNNRFMLPStackingVotingRBAMWGCGOSGoodnessRanking Method0.00.20.40.60.8Decomposition ScoresFigure5:PredictionBenchmarking.Shownforeachrankingmethodisaboxplotofthescoresforthebestdecompositionsselectedbyeachrankingmethodacrossthe14testinstancesintheNetworkdataset.Thedecompositionscoresrangefrom0(thebestdecompositioninthetestset)to1(theworstdecompositioninthetestset).RidgeLassoSVRKNNRFMLPStackingVotingRanking Method0.050.100.150.200.250.300.35Test RMSEFigure6:MLRMSEScores.ShownforeachMLrankingmethodisaboxplotofthetestRMSEscoresforthe14testinstancesintheNetworkdataset.5.2.1.TheBestRankingMethodWecomparedallrankingmethodsagainstoneanotherusingtheFriedmanteststatisticwhichcanbecalculatedasFf=12nk(k+1)[PjR2j−k(k+1)24],wherenisthenumberoftestinstances,kisthenumberofcomparisonalgorithms,Rjistheaveragerankofalgorithmj.UsingtheCONTROLTESTpackageasreferencedin(Derracetal.,2011)showedastatisticalsigniﬁcancebetweenthediﬀerentrankingmethods(p=0.0133).16ToreducethechanceofmakingaType-1errorwhencomparingmultiplealgorithms,wecarriedoutpairwisecomparisonsbetweenthebestMLmethod(Voting)andthebestheuristicmethods(RBAandGoodness)usingtheAlignedFriedmanRanks.TheseheuristicmethodswerechosenastheyproducesimilarpredictionresultstotheGCGOSandMWmethodsrespectively,albeitslightlybetter.Thesepairwisecomparisonsgiveassociatedp-valuesof0.033and3.81e−04respectively,showingstatisticalsigniﬁcanceattheα=0.05levelandindicatingthesuperiorityofaMLrankingmethodatpredictingdecompositionqualities.AmongstallrankingfunctionstheVotingmethodwasabletoprovideequalbestpredictionsfor6outofthe14instancesandtheoutrightbestpredictionforanadditionalinstance.TheVotingmethodwasalsoshowntoneverpredicttheworstdecompositionwhencomparedtotheheuristicrankingtechniques.ItshouldbenotedthattheperformancesofthediﬀerentMLmodelswerenotsigniﬁcantlydiﬀerentfromoneanother,indicatingthatfurtherparametertuningisunlikelytoyieldsigniﬁcantlybetterresults.Inaddition,oneofthekeyundertakingsofthispaperwastodetermineifaMLapproachingeneralcanmoreaccuratelypredictdecompositionqualitythantraditionalheuristicmeasures,whichhasbeendemonstratedinourﬁndings.Forthesereasonswehavenotcarriedoutanyextensivehyper-parametertuning,althoughfutureworkmaylookatcarryingouthyper-parametertuningonaselectedMLmodelinanattempttofurtherimproverankingperformance.5.2.2.ImprovementsoveraMulti-ObjectiveApproachWhilstapopulationbasedmetaheuristicsuchastheNSGA-IIalgorithmisabletoproducealargepoolofsolutionsinareasonableamountoftime,thecurrentobjectivesusedtoevolvethepopulationmightnotalwayscorrelatewellwithdecompositionquality.Inaddition,asdemonstratedin(Weineretal.,2020),evenamongsttheﬁnalParetooptimalsolutionstherecanbesigniﬁcantvariabilityinboundqualityandruntime,requiringthepractitionertothentestapotentiallylargenumberofdecompositions.ShowninFigure7andFigure8aretwoexamplesofwherearankingfunctioncouldbeappliedinconjunctionwithanevolutionaryalgorithmtoidentifyhighqualitydecompositions,withouthavingtoevaluateallParetooptimalsolutionsfoundbytheNSGA-IIalgorithm.InFigure7(a)thereappearstobenocorrelationbetweendecompositionqualityandthetwoobjectivesusedintheNSGA-IIalgorithm,highlightingwhenconsideringonlythesetwofeaturesresultsinpoordecompositions.InFigure7(b),whilstthereisacorrelationbetweendecompositionqualityandthetwoobjectivesusedintheNSGA-IIalgorithm,therearemanyhighqualitydecompositionswhichlieoutsideoftheParetofront,andwouldpotentiallybeignored.UsingaMLapproachhowever,showsahighcorrelationbetweenpredicteddecompositionscoresandactualdecompositionscoresasshowninFigure8.TheMLrankingfunctioncanthereforebesuitableinselectingwhichdecompositionstorunthroughafullLRframework,orcanbeusedasasearchguideinsteadofthetwoobjectivesusedintheNSGA-IIalgorithm.0102030RelaxedConstraint(%)020406080100LargestSubproblem(%)0.40.50.60.7DecompScore(a)h50x2450.mps0102030RelaxedConstraint(%)020406080100LargestSubproblem(%)0.20.40.60.8DecompScore(b)snp-10-004-052.mpsFigure7:Multi-objectiveCorrelation:Shownforalldecompositionsinbothh50x2450.mpsandsnp-10-004-052.mpsinstancesarethecorrelationsbetweenRelaxedConstraint(%)andLargestSubproblem(%),whereLargestSubproblem(%)iscalculatedasthepercentageofallMIPvariablescontainedwithinthelargestsubproblem.Asseeninh50x2450.mps,minimisingboththeRelaxedConstraint(%)andtheLargestSubproblem(%)doesnotcorrelatewelldecompositionquality.Incontrast,thesnp-10-004-052.mpsinstanceshowsgoodcorrelationbetweenminimisingboththegivenobjectivesanddecompositionquality,althoughtherearemanygooddecompositionsbothinandoutsideoftheParetooptimalsolutionfront.170.350.400.450.500.550.600.650.700.75Decompscore0.400.450.500.550.600.650.700.750.80Predictedscore(a)h50x2450.mps0.00.20.40.60.8Decompscore0.300.350.400.450.500.550.600.65Predictedscore(b)snp-10-004-052.mpsFigure8:MachineLearningCorrelation:Shownforalldecompositionsinbothh50x2450.mpsandsnp-10-004-052.mpsinstancesarethecorrelationsbetweenthepredicteddecompositionscoreandtheactualdecompositionscoreforthetrainedVotingensemblefunction.Asseeninbothh50x2450.mpsandsnp-10-004-052.mpsinstances,theMLrankingfunctionshowsgoodcorrelationbetweenpredicteddecompositionscoresandactualdecompositionscores.5.3.ConsideringRelaxedConstraintFeaturesOnlyCurrently,inthegreedyrandomselectionprocedureusedtogeneratethedecompositionsinthispaper,constraintsareselectedforrelaxationinaprobabilisticmannerusingonlynon-zeroproportions.Throughmanualfeatureselection,weconsideredthreeadditionalfeaturestocreateabetterconstraintrankingfunction,includingtheaveragebinary+integerproportionsofrelaxedconstraints,theaverageRHSvalueofrelaxedconstraintsandtheaveragesumofobjectivecoeﬃcientsofthevariablesassociatedwithrelaxedconstraints.Thesefeatureswerechosenastheyareeasilycalculableandcaneasilybeincorporatedinagreedyconstraintselectionprocessforfuturework.WetestedthesefeaturesandtheireﬀectonpredictionqualitybytrainingtwoLinearRegressionmodelswithLassoandRidgeregularisationrespectively.ALinearRegressionmodelwaschosenassuchamodelistransparent,containingfeaturecoeﬃcientswhichareeasilyextractabletouseinafutureconstraintrankingfunction.AsshowninFigure9,fortheNetworkdataset,aLinearRegressionmodelusingonlythesefourfeaturesisoftenabletoproducepredictionqualitiesbetterthanorequaltotheheuristictechniques,showingpromisethatsuchamodelhasgoodpredictivecapability.ShowninTable3arethecoeﬃcientvaluesfoundintheLinearRegressionmodelusingRidgeregularisation.Table3:RelaxedConstraintFeatureCoeﬃcients.ShownfortheLinearRegressionmodelwithRidgeRegularisationarethecoeﬃcientsofthefeaturesinthemodelwhentrainedontheNetworkdataset.Thesecoeﬃcientscanbeusedforfutureconstraintrelaxationselectionprocedures,asthismodelhasdemonstratedgoodpredictivecapabilities.FeatureCoeﬃcientAverageRelaxedConstraintStatisticsNonzeroprops-10.238AverageRelaxedConstraintStatisticsSumobj-0.180AverageRelaxedConstraintStatisticsRHSvals0.253AverageRelaxedConstraintStatisticsBinIntprops4.2235.4.TestingonRandomlySelectedInstancesAﬁnalexperimentwascarriedouttoseehowaMLmodeltrainedonallinstancesfromtheNetworkdatasetwouldperformonrandomlyselectedinstancesfromtheMIPLIB2017library.ThepredictionresultsforthediﬀerentrankingmethodsconsideredarepresentedinFigure10.Ascanbeseen,theMLbasedrankingfunctionsarecompetitivewiththeheuristicbasedtechniqueswithoutbeingstate-of-the-art.UsingtheFriedmantest,nostatisticalsigniﬁcancewasdetected(p=0.419)amongstthediﬀerentrankingmethods.Theseﬁndingsappeartobeconsistentwiththosein(Bassoetal.,2020),indicatingthataMLbasedrankingfunctionisonlyuseful18LassoRidgeRBAMWGCGOSGoodnessRanking Method0.00.20.40.60.8Decomposition ScoresFigure9:RelaxedConstraintFeaturesandPredictionQuality.Shownforeachrankingmethodisaboxplotofthescoresforthebestdecompositionsselectedbyeachrankingmethodacrossthe14testinstancesintheNetworkdataset.ThetwoLinearRegressionmodelsusingLassoandRidgeregularisationweretrainedusingonlytheselectedrelaxedconstraintfeatures.ThesebestselecteddecompositionsbytheMLmethodsusingonlytherelaxedconstraintfeaturesarestillcompetitivewiththedecompositionsselectedbytheheuristicrankingmethods.Thedecompositionscoresrangefrom0(thebestdecompositioninthetestset)to1(theworstdecompositioninthetestset).whentestinstancesaresomewhatsimilartothetraininginstances.APCAplotusingtheinstancefeaturesdescribedinTableA7isshowninFigure11.Unlikepreviousproblemtypestested,therandomlyselectedMIPLIBinstancesdonotseemtoformanysigniﬁcantclusters,showingthatatleastinthisfeaturespacetheinstancesarequitedissimilar.5.5.DiscussionInlightofourﬁndings,wepresentabriefdiscussionofhowourworkcouldbeleveragedinfutureautomaticdecompositionframeworks.Asdemonstratedinourﬁndings,thereisaclearlinkbetweeninstancesimilarityandthepredictivecapabilitiesofaMLbasedrankingfunctionforpredictingdecompositionqualities.Assuch,webelievethatfutureworksrelyingondecompositionqualitypredictionsshouldhaveaninitialinstanceclassiﬁcationprocedure,selectingapre-trainedMLmodelthatmightbemostsuitablefortheinstancetobesolved.Whilstoutsidethescopeofthispaper,thisclassiﬁcationsteprequiressigniﬁcantlymoredataandcomputationalworkfornewproblemtypesnotexploredinthispaper,inordertocreateasuitablesetofdecompositionsfortrainingpurposes.Inadditiontoclassifyinginstancesbasedonproblemtype,classiﬁcationonothermetricsmightalsobeappropriate,suchastheproportionsofdiﬀerentconstrainttypese.g.,SetPartitioning,Binpacking,SetCovering,Knapsacketc.Unlikeinpreviousstudies(Bassoetal.,2020;Weineretal.,2020),whereconstraintsarerelaxedinagreedymannerusingonlytheirnon-zeroproportions,weproposethatotherconstraintfeaturesbeconsideredsuchasthoseshowninTable3,aswehavedemonstratedtheimportancethesefeatureshaveondecompositionquality.Generatingavaliddecompositionviaconstraintrelaxationcanbecarriedoutsigniﬁcantlyfastusingahypergraphpartitioningmethodasdiscussedin(Bergneretal.,2015;Weineretal.,2020),requiringO(nz)timecomplexity,wherenzisthenumberofnon-zeroesintheconstraintmatrix.SuchaphenomenonmeansthatalargepopulationofdecompositionscanberankedusingaMLfunctionrelativelyquickly,inordertodeterminewhichdecompositionsshouldbeselectedforsolving,orpotentiallyasacandidateforfurtherimprovementsusingalocalsearchoperator.19RidgeLassoSVRKNNRFMLPStackingVotingRBAMWGCGOSGoodnessRanking Method0.00.20.40.60.8Decomposition ScoresFigure10:RandomMIPLIBPredictionBenchmarking.Shownforeachrankingmethodisaboxplotofthescoresforthebestdecompositionsselectedbyeachrankingmethodacrossthe10randomMIPLIBinstances.TheMLmodelspresentedweretrainedonallinstancesfromtheNetworkdataset.Thedecompositionscoresrangefrom0(thebestdecompositioninthetestset)to1(theworstdecompositioninthetestset).−0.6−0.4−0.20.00.20.40.60.8PrincipalComponent1-ExplainedVarianceRatio=0.51−0.6−0.4−0.20.00.20.40.6PrincipalComponent2-ExplainedVarianceRatio=0.26FixedCostNetworkFlowNetworkDesignRandomMIPLIBSupplyNetworkPlanningFigure11:PCAAnalysisofallproblemtypesincludingNetworkDesign,FixedCostNetworkFlow,SupplyNetworkingandRandomMIPLIBinstances.Showninthisﬁgurearetheﬁrsttwoprinciplecomponents,comprisingover77%oftheexplainedvariance.Ascanbeseen,theRandomMIPLIBinstancesappeartoshownoclusteringpatternsincomparisontotheNetworksubproblemtypessuchasSupplyNetworkPlanningandNetworkDesigntypes.206.ConclusionsThispaperexploredhowaMachineLearning(ML)approachcanbeusedeﬀectivelyasadecompositionrankingfunctionforLagrangianRelaxationofMixedIntegerPrograms.Throughbenchmarkcomparisonswithpreviouslypublishedhand-craftedheuristicmeasures,itwasdemonstratedthataMLapproachwasabletoprovidestate-of-the-artresultsinpredictingdecompositionqualitiesfromamongstalargenumberofcandidatedecompositions.ThispaperproducedarichdatasetofdecompositionsrelatingtobothnetworktypeinstancesandrandomlyselectedinstancesfromtheMIPLIB2017librarywhichisfreelyavailableforotherresearcherstoaccess2.ThispaperalsoexploredhowinstancesimilarityplaysacriticalroleinMLpredictionqualities,suggestingthatfutureworkininstanceclassiﬁcationandtheexplorationofadditionalinstancefeaturescouldprovideapromisingresearchdirection.Finally,anewconstraintrankingfunctionwasalsoprovided,whichhasshownpromisingpredictioncapabilitiesandcanbeusedtorankconstraintsforrelaxationa-prioritoanyheuristicselectionalgorithm.FutureworkmayinvolveembeddingtheMLrankingfunctiondescribedinthispaperwithinaheuristicbasedsearchtechnique,inordertoﬁndhighqualitysolutionswithoutdirectlyevaluatingalargepopulationofcandidatedecompositions.Whilstbetterresultscouldpotentiallybefoundthroughhyperparametertuningofmodels,byprimarilyusingonlydefaultmodelsettingswewerestillabletodemonstratethataMLapproachcansigniﬁcantlyoutperformcurrentbenchmarkheuristicmethods.FutureworkmaylookattestingbothmoreMLmodelsaswellasadditionalhyperparametertuning.AcknowledgementsThisresearchwassupportedbyanARC(AustralianResearchCouncil)DiscoveryGrant(DP180101170)References(2010).Leave-One-OutCross-ValidationBT-EncyclopediaofMachineLearning.(pp.600–601).Boston,MA:SpringerUS.doi:10.1007/978-0-387-30164-8_469.Achterberg,T.,Koch,T.,&Martin,A.(2006).MIPLIB2003.OperationsResearchLetters,34,361–372.doi:https://doi.org/10.1016/j.orl.2005.07.009.Barahona,F.,&Anbil,R.(2000).Thevolumealgorithm:producingprimalsolutionswithasubgradientmethod.MathematicalProgramming,87,385–399.doi:10.1007/s101070050002.Basso,S.,&Ceselli,A.(2018).Computationalevaluationofrankingmodelsinanautomaticdecompositionframework.ElectronicNotesinDiscreteMathematics,69,245–252.doi:https://doi.org/10.1016/j.endm.2018.07.032.Basso,S.,Ceselli,A.,&Tettamanzi,A.(2020).Randomsamplingandmachinelearningtounderstandgooddecompositions.AnnalsofOperationsResearch,284,501–526.doi:10.1007/s10479-018-3067-9.Bergner,M.,Caprara,A.,Ceselli,A.,Furini,F.,L¨ubbecke,M.E.,Malaguti,E.,&Traversi,E.(2015).Auto-maticDantzig–Wolfereformulationofmixedintegerprograms.MathematicalProgramming,149,391–424.Biscani,F.,&Izzo,D.(2019).esa/pagmo2:pagmo2.11.4,.doi:10.5281/ZENODO.3464510.Deb,K.,Pratap,A.,Agarwal,S.,&Meyarivan,T.(2002).Afastandelitistmultiobjectivegeneticalgorithm:NSGA-II.IEEETransactionsonEvolutionaryComputation,6,182–197.doi:10.1109/4235.996017.Derrac,J.,Garc´ıa,S.,Molina,D.,&Herrera,F.(2011).Apracticaltutorialontheuseofnonparametricstatisticaltestsasamethodologyforcomparingevolutionaryandswarmintelligencealgorithms.SwarmandEvolutionaryComputation,1,3–18.doi:https://doi.org/10.1016/j.swevo.2011.02.002.2Thedatasetandcodeusedinthispaperwillbecomefreelyaccessibleupontheacceptanceofthispaper.21Ernst,A.T.,&Singh,G.(2012).Lagrangianparticleswarmoptimizationforaresourceconstrainedmachineschedulingproblem.InEvolutionaryComputation(CEC),2012IEEECongresson(pp.1–8).IEEE.Fisher,M.L.(2004).TheLagrangianrelaxationmethodforsolvingintegerprogrammingproblems.Managementscience,50,1861–1871.Gamrath,G.,&L¨ubbecke,M.E.(2010).ExperimentswithagenericDantzig-Wolfedecompositionforintegerprograms.InInternationalSymposiumonExperimentalAlgorithms(pp.239–252).Springer.Geoﬀrion,A.M.(1972).Generalizedbendersdecomposition.Journalofoptimizationtheoryandapplications,10,237–260.Geoﬀrion,A.M.(1974).Lagrangeanrelaxationforintegerprogramming.InApproachestointegerprogramming(pp.82–114).Springer.Gleixner,A.,Hendel,G.,Gamrath,G.,Achterberg,T.,Bastubbe,M.,Berthold,T.,Christophel,P.,Jarck,K.,Koch,T.,Linderoth,J.,L¨ubbecke,M.,Mittelmann,H.D.,Ozyurt,D.,Ralphs,T.K.,Salvagnin,D.,&Shinano,Y.(2021).MIPLIB2017:data-drivencompilationofthe6thmixed-integerprogramminglibrary.MathematicalProgrammingComputation,.doi:10.1007/s12532-020-00194-3.Karypis,G.,Aggarwal,R.,Kumar,V.,&Shekhar,S.(1999).Multilevelhypergraphpartitioning:applicationsinVLSIdomain.IEEETransactionsonVeryLargeScaleIntegration(VLSI)Systems,7,69–79.doi:10.1109/92.748202.Khaniyev,T.,Elhedhli,S.,&Erenay,F.S.(2018).StructureDetectioninMixed-IntegerPrograms.INFORMSJournalonComputing,30,570–587.doi:10.1287/ijoc.2017.0797.Kruber,M.,L¨ubbecke,M.E.,&Parmentier,A.(2017).LearningWhentoUseaDecompositionBT-Inte-grationofAIandORTechniquesinConstraintProgramming.(pp.202–210).Cham:SpringerInternationalPublishing.Pedregosa,F.,Varoquaux,G.,Gramfort,A.,Michel,V.,Thirion,B.,Grisel,O.,Blondel,M.,Prettenhofer,P.,Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cournapeau,D.,Brucher,M.,Perrot,M.,&Duchesnay,E.(2011).Scikit-learn:MachineLearningin{P}ython.JournalofMachineLearningResearch,12,2825–2830.Triantaphyllou,E.(2000).Multi-criteriaDecisionMakingMethodsAComparativeStudy.AppliedOptimiza-tion,44(1sted.).NewYork,NY:SpringerUS.doi:10.1007/978-1-4757-3157-6.Vanderbeck,F.,&Savelsbergh,M.W.P.(2006).AgenericviewofDantzig–Wolfedecompositioninmixedintegerprogramming.OperationsResearchLetters,34,296–306.doi:https://doi.org/10.1016/j.orl.2005.05.009.Wedelin,D.(1995).Analgorithmforlargescale0–1integerprogrammingwithapplicationtoairlinecrewscheduling.AnnalsofOperationsResearch,57,283–301.doi:10.1007/BF02099703.Weiner,J.,Ernst,A.,Li,X.,&Sun,Y.(2020).AutomaticDecompositionofMixedIntegerProgramsforLagrangianRelaxationUsingaMultiobjectiveApproach.InProceedingsofthe2020GeneticandEvolutionaryComputationConferenceGECCO’20(pp.263–270).NewYork,NY,USA:AssociationforComputingMachinery.doi:10.1145/3377930.3390233.Weiner,J.,Ernst,A.T.,Li,X.,Sun,Y.,&Deb,K.(2021).SolvingthemaximumedgedisjointpathproblemusingamodiﬁedLagrangianparticleswarmoptimisationhybrid.EuropeanJournalofOperationalResearch,293,847–862.doi:https://doi.org/10.1016/j.ejor.2021.01.009.22TableA1:FullFeaturesList:ShownisthefulllistoffeaturesusedtotrainandtesttheMachineLearningmodelsRelaxedConstraintStatisticsMin,Max,Ave,StddevofConstraintRHSValueMin,Max,Ave,StddevofSumofObjCoeﬃcientsofVariablesinConstraintMin,Max,Ave,StddevofPropofInstanceBininConstraintMin,Max,Ave,StddevofPropofInstanceIntinConstraintMin,Max,Ave,StddevofPropofInstanceBin+IntinConstraintMin,Max,Ave,StddevofNonZeroPropsinConstraintPropofConstraintsRelaxedPropofConstraintsRelaxedwhichareEqualitySubproblemStatisiticsMin,Max,Ave,StddevofPropofInstanceBininSubproblemsMin,Max,Ave,StddevofPropofInstanceIntinSubproblemsMin,Max,Ave,StddevofPropofInstanceBin+IntinSubproblemsMin,Max,Ave,StddevofSubproblemDensitiesMin,Max,Ave,StddevofConstraintPropinSubproblemsMin,Max,Ave,StddevEqualityPropinSubproblemsMin,Max,Ave,StddevVariablePropinSubproblemsMin,Max,Ave,StddevNonZeroPropsinSubproblemsAve,StddevofSumofObjectiveCoeﬃcientsforVariablesinSubproblemsAve,StddevofRangeofObjectiveCoeﬃcientsforVariablesinSubproblemsAve,StddevofAverageRHSValueinSubproblemsAve,StddevofRHSRangeinSubproblemsAve,StddevofSubproblemShapes1arXiv:2207.00219v1  [math.OC]  1 Jul 2022TableA2:DecompositionCounts:Shownforinstanceinthedatasetarethetotalnumberdecompositionscreatedforbothtrainingandtestingpurposes.ProblemTypeInstanceNameNumberofDecompositionsNetworkDesigncost266-UUE.mps1743NetworkDesigndfn-bwin-DBE.mps1996NetworkDesigngermany50-UUM.mps1780NetworkDesignta1-UUM.mps1848NetworkDesignta2-UUE.mps1673FixedCostNetworkFlowg200x740.mps2268FixedCostNetworkFlowh50x2450.mps504FixedCostNetworkFlowh80x6320d.mps469FixedCostNetworkFlowk16x240b.mps640SupplyNetworkPlanningsnp-02-004-104.mps2485SupplyNetworkPlanningsnp-04-052-052.mps2462SupplyNetworkPlanningsnp-06-004-052.mps1538SupplyNetworkPlanningsnp-10-004-052.mps575SupplyNetworkPlanningsnp-10-052-052.mps582RandomMIPLIBblp-ic98.mps702RandomMIPLIBdws008-01.mps2314RandomMIPLIB30n20b8.mps1669RandomMIPLIBair03.mps2616RandomMIPLIBtraininstance2.mps2090RandomMIPLIBneos-4387871-tavua.mps2535RandomMIPLIBneos-4338804-snowy.mps1934RandomMIPLIBair05.mps2607RandomMIPLIBneos-4954672-berkel.mps1841RandomMIPLIBsplice1k1.mps13182TableA3:RawBoundResults:Shownforeachinstancearetheboundsoftheminandmaxboundsfoundamongstalldecompositionstested.Asallinstancessolvedareminimizationtypeproblems,smallerLRboundsareofworsequalitythanlargerLRbounds.ShownforeachinstanceisalsotheLPboundaswellasthebestknownprimalsolutionasreportedonMIPLIB2017.Itshouldbenotedthatassubproblemsweresolvedtowithin1%ofoptimality,astheprimalsolutionstothesubproblemswereusedinboundcalculationswhensolutionswerewithinthisoptimalitytolerance,itispossiblefortheMaxLRboundtobeslightlyhigher(<1%)thanthebestknownprimalsolution.BoundComparisonsInstanceMinLRBoundMaxLRBoundLPBoundBestKnownPrimalSolutioncost266-UUE.mps20161500.0024150800.0020161500.0025148940.56dfn-bwin-DBE.mps17890.9050921.2017890.9073623.79germany50-UUM.mps597932.00618552.00597932.00628490.00ta1-UUM.mps3693670.007237730.003693670.007518328.20ta2-UUE.mps36964000.0037886500.0036964000.0037871728.59g200x740.mps34077.5044356.0034077.5044316.00h50x2450.mps11147.7032972.5011147.7032906.88h80x6320d.mps5325.166382.105325.166382.10k16x240b.mps3320.7711331.703320.7711393.00snp-02-004-104.mps548045000.00586972000.00548045000.00586803238.66snp-04-052-052.mps728302000.00857416000.00728196000.00885202237.19snp-06-004-052.mps1787140000.001875140000.001787140000.001869531919.90snp-10-004-052.mps5842870000.005914170000.005842630000.005906642865.78snp-10-052-052.mps5843240000.005944030000.005842630000.006364531568.74blp-ic98.mps4331.174515.024331.174491.45dws008-01.mps584.5025532.30584.5037412.6030n20b8.mps1.57302.001.57302.00air03.mps338864.00342760.00338864.00340160.00traininstance2.mps0.0017760.000.0071820.00neos-4387871-tavua.mps10.1427.8510.1433.38neos-4338804-snowy.mps1447.001473.001447.001471.00air05.mps25877.6026497.0025877.6026374.00neos-4954672-berkel.mps1150230.002345130.001150230.002612710.00splice1k1.mps-1646.78-798.09-1646.78-394.0030.00.20.40.60.81.0Min_Relaxed_Constraint_Statistics_Non_zero_countsMax_Relaxed_Constraint_Statistics_Non_zero_countsAverage_Relaxed_Constraint_Statistics_Non_zero_countsStddev_Relaxed_Constraint_Statistics_Non_zero_countsRelaxed Constraint Prop_Relaxed_Constraint_Statistics_single_statsEquality Prop_Relaxed_Constraint_Statistics_single_statsMin_Relaxed_Constraint_Statistics_Sum_objMax_Relaxed_Constraint_Statistics_Sum_objAverage_Relaxed_Constraint_Statistics_Sum_objStddev_Relaxed_Constraint_Statistics_Sum_objMin_Relaxed_Constraint_Statistics_Int_propsMax_Relaxed_Constraint_Statistics_Int_propsAverage_Relaxed_Constraint_Statistics_Int_propsStddev_Relaxed_Constraint_Statistics_Int_propsMin_Relaxed_Constraint_Statistics_Bin_propsMax_Relaxed_Constraint_Statistics_Bin_propsAverage_Relaxed_Constraint_Statistics_Bin_propsStddev_Relaxed_Constraint_Statistics_Bin_propsMin_Relaxed_Constraint_Statistics_RHS_valsMax_Relaxed_Constraint_Statistics_RHS_valsAverage_Relaxed_Constraint_Statistics_RHS_valsStddev_Relaxed_Constraint_Statistics_RHS_valsMin_Relaxed_Constraint_Statistics_Bin_Int_propsMax_Relaxed_Constraint_Statistics_Bin_Int_propsAverage_Relaxed_Constraint_Statistics_Bin_Int_propsStddev_Relaxed_Constraint_Statistics_Bin_Int_propsAverage_Subproblem_Statistics_average_RHSStddev_Subproblem_Statistics_average_RHSAverage_Subproblem_Statistics_RHS_rangeStddev_Subproblem_Statistics_RHS_rangeMin_Subproblem_Statistics_Equality_propsMax_Subproblem_Statistics_Equality_propsAverage_Subproblem_Statistics_Equality_propsStddev_Subproblem_Statistics_Equality_propsAverage_Subproblem_Statistics_Sum_objStddev_Subproblem_Statistics_Sum_objAverage_Subproblem_Statistics_ShapesStddev_Subproblem_Statistics_ShapesAverage_Subproblem_Statistics_Obj_rangeStddev_Subproblem_Statistics_Obj_rangeMin_Subproblem_Statistics_Var_propsMax_Subproblem_Statistics_Var_propsAverage_Subproblem_Statistics_Var_propsStddev_Subproblem_Statistics_Var_propsMin_Subproblem_Statistics_Int_propsMax_Subproblem_Statistics_Int_propsAverage_Subproblem_Statistics_Int_propsStddev_Subproblem_Statistics_Int_propsMin_Subproblem_Statistics_Bin_propsMax_Subproblem_Statistics_Bin_propsAverage_Subproblem_Statistics_Bin_propsStddev_Subproblem_Statistics_Bin_propsMin_Subproblem_Statistics_Const_propsMax_Subproblem_Statistics_Const_propsAverage_Subproblem_Statistics_Const_propsStddev_Subproblem_Statistics_Const_propsMin_Subproblem_Statistics_non_zeroes_propsMax_Subproblem_Statistics_non_zeroes_propsAverage_Subproblem_Statistics_non_zeroes_propsStddev_Subproblem_Statistics_non_zeroes_propsMin_Subproblem_Statistics_Bin_Int_propsMax_Subproblem_Statistics_Bin_Int_propsAverage_Subproblem_Statistics_Bin_Int_propsStddev_Subproblem_Statistics_Bin_Int_propsMin_Subproblem_Statistics_DensitiesMax_Subproblem_Statistics_DensitiesAverage_Subproblem_Statistics_DensitiesStddev_Subproblem_Statistics_DensitiesFigureA1:NetworkDatasetFeatureSpread.ShownarethespreadoffeaturedataforalldecompositionscontainedintheNetworkdataset,containingthenetworkproblemtypes-NetworkDesign,FixedCostNetworkFlowandSupplyNetworkPlanning.Intotalthereare20563decompositionsintheNetworkdatasetobtainedfrom14instances.40.00.20.40.60.81.0Min_Relaxed_Constraint_Statistics_Non_zero_countsMax_Relaxed_Constraint_Statistics_Non_zero_countsAverage_Relaxed_Constraint_Statistics_Non_zero_countsStddev_Relaxed_Constraint_Statistics_Non_zero_countsRelaxed Constraint Prop_Relaxed_Constraint_Statistics_single_statsEquality Prop_Relaxed_Constraint_Statistics_single_statsMin_Relaxed_Constraint_Statistics_Sum_objMax_Relaxed_Constraint_Statistics_Sum_objAverage_Relaxed_Constraint_Statistics_Sum_objStddev_Relaxed_Constraint_Statistics_Sum_objMin_Relaxed_Constraint_Statistics_Int_propsMax_Relaxed_Constraint_Statistics_Int_propsAverage_Relaxed_Constraint_Statistics_Int_propsStddev_Relaxed_Constraint_Statistics_Int_propsMin_Relaxed_Constraint_Statistics_Bin_propsMax_Relaxed_Constraint_Statistics_Bin_propsAverage_Relaxed_Constraint_Statistics_Bin_propsStddev_Relaxed_Constraint_Statistics_Bin_propsMin_Relaxed_Constraint_Statistics_RHS_valsMax_Relaxed_Constraint_Statistics_RHS_valsAverage_Relaxed_Constraint_Statistics_RHS_valsStddev_Relaxed_Constraint_Statistics_RHS_valsMin_Relaxed_Constraint_Statistics_Bin_Int_propsMax_Relaxed_Constraint_Statistics_Bin_Int_propsAverage_Relaxed_Constraint_Statistics_Bin_Int_propsStddev_Relaxed_Constraint_Statistics_Bin_Int_propsAverage_Subproblem_Statistics_average_RHSStddev_Subproblem_Statistics_average_RHSAverage_Subproblem_Statistics_RHS_rangeStddev_Subproblem_Statistics_RHS_rangeMin_Subproblem_Statistics_Equality_propsMax_Subproblem_Statistics_Equality_propsAverage_Subproblem_Statistics_Equality_propsStddev_Subproblem_Statistics_Equality_propsAverage_Subproblem_Statistics_Sum_objStddev_Subproblem_Statistics_Sum_objAverage_Subproblem_Statistics_ShapesStddev_Subproblem_Statistics_ShapesAverage_Subproblem_Statistics_Obj_rangeStddev_Subproblem_Statistics_Obj_rangeMin_Subproblem_Statistics_Var_propsMax_Subproblem_Statistics_Var_propsAverage_Subproblem_Statistics_Var_propsStddev_Subproblem_Statistics_Var_propsMin_Subproblem_Statistics_Int_propsMax_Subproblem_Statistics_Int_propsAverage_Subproblem_Statistics_Int_propsStddev_Subproblem_Statistics_Int_propsMin_Subproblem_Statistics_Bin_propsMax_Subproblem_Statistics_Bin_propsAverage_Subproblem_Statistics_Bin_propsStddev_Subproblem_Statistics_Bin_propsMin_Subproblem_Statistics_Const_propsMax_Subproblem_Statistics_Const_propsAverage_Subproblem_Statistics_Const_propsStddev_Subproblem_Statistics_Const_propsMin_Subproblem_Statistics_non_zeroes_propsMax_Subproblem_Statistics_non_zeroes_propsAverage_Subproblem_Statistics_non_zeroes_propsStddev_Subproblem_Statistics_non_zeroes_propsMin_Subproblem_Statistics_Bin_Int_propsMax_Subproblem_Statistics_Bin_Int_propsAverage_Subproblem_Statistics_Bin_Int_propsStddev_Subproblem_Statistics_Bin_Int_propsMin_Subproblem_Statistics_DensitiesMax_Subproblem_Statistics_DensitiesAverage_Subproblem_Statistics_DensitiesStddev_Subproblem_Statistics_DensitiesFigureA2:RandomDatasetDecompositionFeatureSpread.ShownarethespreadoffeaturedataforalldecompositionscontainedintheRandomdataset.Intotalthereare19626decompositionsintheRandomdatasetobtainedfrom10instances.5TableA4:NetworkDesignPredictionResults:ShownforeachMLrankingmethodandtestinstance(ModelInstance)isthescoreforthebestdecompositionselectedforeachinstancefromtheNetworkDesignproblemtype,with0indicatingthebestdecompositionfromthetestinstancewasselectedand1indicatingtheworstdecompositionfromthetestinstancewasselected.EachcolumnrepresentstheproblemtypetheMLmodelwastrainedon.Forthenetworkdesignproblemtype,themodelwastrainedonallinstancesexceptforthetestinstance.Fortheotherproblemtypes,themodelwastrainedonallinstances.Tovalidatethesigniﬁcancetheproblemtypethemodelistrainedonhasonperformance,thez-scoresandassociatedp-valuesshownarecalculatedviapairwisecomparisonsbetweenthecontrolmethod(modelstrainedonthesameproblemtype)andcomparisonmethods(modelstrainedondiﬀerentproblemtypes)usingtheAlignedFriedmanranks.Finally,theaveragescoreforthebestdecompositionsselectedbyeachmodelwhentrainedonthediﬀerentproblemtypesisalsopresented.ModelInstancenetworkdesignﬁxedcostnetworkﬂowsupplynetworkplanningLassocost266-UUE.mps0.0000.1630.628Lassodfn-bwin-DBE.mps0.4150.4830.410Lassogermany50-UUM.mps0.0710.2440.239Lassota1-UUM.mps0.0170.2060.054Lassota2-UUE.mps0.5940.1320.800Ridgecost266-UUE.mps0.2310.6280.040Ridgedfn-bwin-DBE.mps0.4150.4830.415Ridgegermany50-UUM.mps0.0710.7040.071Ridgeta1-UUM.mps0.1670.4370.000Ridgeta2-UUE.mps0.1320.8000.391SVRcost266-UUE.mps0.0000.2060.642SVRdfn-bwin-DBE.mps0.1090.2730.000SVRgermany50-UUM.mps0.0770.1920.704SVRta1-UUM.mps0.3060.3580.438SVRta2-UUE.mps0.0870.1320.391KNNcost266-UUE.mps0.2650.0080.364KNNdfn-bwin-DBE.mps0.4400.0960.493KNNgermany50-UUM.mps0.0710.2590.698KNNta1-UUM.mps0.2110.2700.438KNNta2-UUE.mps0.7360.1320.449RFcost266-UUE.mps0.1850.4660.642RFdfn-bwin-DBE.mps0.6190.2850.562RFgermany50-UUM.mps0.1300.1990.244RFta1-UUM.mps0.1540.0840.395RFta2-UUE.mps0.3810.8000.800MLPcost266-UUE.mps0.1000.0400.078MLPdfn-bwin-DBE.mps0.0370.5710.101MLPgermany50-UUM.mps0.0370.0770.071MLPta1-UUM.mps0.1820.1820.017MLPta2-UUE.mps0.6320.4070.393Stackingcost266-UUE.mps0.1580.0400.078Stackingdfn-bwin-DBE.mps0.6410.5710.402Stackinggermany50-UUM.mps0.0710.0770.071Stackingta1-UUM.mps0.1670.1820.017Stackingta2-UUE.mps0.1320.1320.304Votingcost266-UUE.mps0.1160.4750.078Votingdfn-bwin-DBE.mps0.1790.4830.421Votinggermany50-UUM.mps0.0710.3660.071Votingta1-UUM.mps0.1670.4240.000Votingta2-UUE.mps0.6320.4490.391Average0.2300.3130.320z-score-3.531-3.675p-value2.07E-041.19E-046TableA5:FixedCostNetworkFlowPredictionResults:ShownforeachMLrankingmethodandtestinstance(ModelInstance)isthescoreforthebestdecompositionselectedforeachinstancefromtheFixedCostNetworkFlowproblemtype,with0indicatingthebestdecompositionfromthetestinstancewasselectedand1indicatingtheworstdecompositionfromthetestinstancewasselected.EachcolumnrepresentstheproblemtypetheMLmodelwastrainedon.FortheFixedCostNetworkFlow,themodelwastrainedonallinstancesexceptforthetestinstance.Fortheotherproblemtypes,themodelwastrainedonallinstances.Tovalidatethesigniﬁcancetheproblemtypethemodelistrainedonhasonperformance,thez-scoresandassociatedp-valuesshownarecalculatedviapairwisecomparisonsbetweenthecontrolmethod(modelstrainedonthesameproblemtype)andcomparisonmethods(modelstrainedondiﬀerentproblemtypes)usingtheAlignedFriedmanranks.Finally,theaveragescoreforthebestdecompositionsselectedbyeachmodelwhentrainedonthediﬀerentproblemtypesisalsopresented.ModelInstanceﬁxedcostnetworkﬂownetworkdesignsupplynetworkplanningLassog200x740.mps0.0000.0000.000Lassoh50x2450.mps0.0000.0000.372Lassoh80x6320d.mps0.0000.0000.000Lassok16x240b.mps0.1520.0000.000Ridgeg200x740.mps0.1130.0770.000Ridgeh50x2450.mps0.0660.0000.000Ridgeh80x6320d.mps0.0730.0000.000Ridgek16x240b.mps0.1070.7590.035SVRg200x740.mps0.1370.1390.000SVRh50x2450.mps0.0000.0000.173SVRh80x6320d.mps0.0000.0000.000SVRk16x240b.mps0.1070.4970.107KNNg200x740.mps0.1080.1080.176KNNh50x2450.mps0.3720.0000.173KNNh80x6320d.mps0.0000.0730.074KNNk16x240b.mps0.0350.2330.175RFg200x740.mps0.1180.0770.541RFh50x2450.mps0.0000.1350.372RFh80x6320d.mps0.0000.0740.000RFk16x240b.mps0.7890.1870.136MLPg200x740.mps0.1070.1080.000MLPh50x2450.mps0.0000.7510.173MLPh80x6320d.mps0.0740.8480.000MLPk16x240b.mps0.1070.0000.035Stackingg200x740.mps0.1180.0770.000Stackingh50x2450.mps0.0000.1350.173Stackingh80x6320d.mps0.0000.0740.000Stackingk16x240b.mps0.0000.7930.035Votingg200x740.mps0.1180.0770.000Votingh50x2450.mps0.0000.0000.180Votingh80x6320d.mps0.0000.0000.000Votingk16x240b.mps0.1070.4550.035Average0.0880.1770.093z-score-2.8770.177p-value0.0020.5707TableA6:SupplyNetworkPlanningPredictionResults:ShownforeachMLrankingmethodandtestinstance(ModelInstance)isthescoreforthebestdecompositionselectedeachinstancefromtheSupplyNetworkPlanningproblemtype,with0indicatingthebestdecompositionfromthetestinstancewasselectedand1indicatingtheworstdecompositionfromthetestinstancewasselected.EachcolumnrepresentstheproblemtypetheMLmodelwastrainedon.FortheSupplyNetworkPlanningproblemtype,themodelwastrainedonallinstancesexceptforthetestinstance.Fortheotherproblemtypes,themodelwastrainedonallinstances.Tovalidatethesigniﬁcancetheproblemtypethemodelistrainedonhasonperformance,thez-scoresandassociatedp-valuesshownarecalculatedviapairwisecomparisonsbetweenthecontrolmethod(modelstrainedonthesameproblemtype)andcomparisonmethods(modelstrainedondiﬀerentproblemtypes)usingtheAlignedFriedmanranks.Finally,theaveragescoreforthebestdecompositionsselectedbyeachmodelwhentrainedonthediﬀerentproblemtypesisalsopresented.ModelInstancesupplynetworkplanningﬁxedcostnetworkﬂownetworkdesignLassosnp-02-004-104.mps0.4990.0190.148Lassosnp-04-052-052.mps0.5300.5300.530Lassosnp-06-004-052.mps0.1550.0780.494Lassosnp-10-004-052.mps0.0360.0000.499Lassosnp-10-052-052.mps0.0000.4870.549Ridgesnp-02-004-104.mps0.4870.0540.398Ridgesnp-04-052-052.mps0.2780.5930.823Ridgesnp-06-004-052.mps0.1550.0800.491Ridgesnp-10-004-052.mps0.0360.2070.484Ridgesnp-10-052-052.mps0.0000.4370.803SVRsnp-02-004-104.mps0.1380.0120.148SVRsnp-04-052-052.mps0.5920.4670.530SVRsnp-06-004-052.mps0.0880.2460.488SVRsnp-10-004-052.mps0.0000.0000.499SVRsnp-10-052-052.mps0.0000.4870.549KNNsnp-02-004-104.mps0.0910.0120.148KNNsnp-04-052-052.mps0.3100.4670.530KNNsnp-06-004-052.mps0.1820.1770.493KNNsnp-10-004-052.mps0.0040.0000.492KNNsnp-10-052-052.mps0.2100.4870.549RFsnp-02-004-104.mps0.1080.1480.012RFsnp-04-052-052.mps0.1390.5300.530RFsnp-06-004-052.mps0.2260.4940.491RFsnp-10-004-052.mps0.1330.4990.273RFsnp-10-052-052.mps0.0320.5490.549MLPsnp-02-004-104.mps0.1380.0120.148MLPsnp-04-052-052.mps0.5920.4560.530MLPsnp-06-004-052.mps0.0880.1910.490MLPsnp-10-004-052.mps0.0000.0000.499MLPsnp-10-052-052.mps0.0000.5490.549Stackingsnp-02-004-104.mps0.1080.1480.389Stackingsnp-04-052-052.mps0.1790.5300.799Stackingsnp-06-004-052.mps0.1910.4150.491Stackingsnp-10-004-052.mps0.1330.4990.499Stackingsnp-10-052-052.mps0.0320.5490.565Votingsnp-02-004-104.mps0.1080.0260.386Votingsnp-04-052-052.mps0.5920.5300.821Votingsnp-06-004-052.mps0.0880.0780.493Votingsnp-10-004-052.mps0.0360.0000.499Votingsnp-10-052-052.mps0.0000.5490.811Average0.1680.2900.487z-score-4.743-12.965p-value1.05E-069.70E-398TableA7:InstanceFeaturesforPCAAnalysis.Allfeatures,exceptforDensity,ShapeandEqualityfeatureswerenormalisedusingmin-maxnormalisationonaninstancebyinstancebasis.DensityandEqualityfeatureswerenotnormalised,whilsttheShapefeatureswerenormalisedusingmin-maxnormalisationbyconsideringallinstancesinvolvedintheanalysis.FeaturesDescriptionConstrSumAbsObjmeanMeansumofabsolutevaluesofobjectivecoeﬃcientsassociatedwitheachconstraintConstrSumAbsObjstddevStddevsumofabsolutevaluesofobjectiveassociatedwitheachconstraintConstrSumObjmeanMeansumofvaluesofobjectivecoeﬃcientsassociatedwitheachconstraintConstrSumObjstddevStandardDeviationsumofvaluesofobjectivecoeﬃcientsassociatedwitheachconstraintObjtermsmeanMeanofobjectivecoeﬃcientsintheInstanceObjtermsstddevStddevofobjectivecoeﬃcientsintheInstanceNonZeroesmeanMeanNo.NonZeroesinconstraintsNonZeroesstddevStddevofNo.NonZeroesinconstraintsRHSValsmeanMeanconstraintRHSRHSValsstddevStddevofconstraintRHSDensityNo.NonZeroes/(No.Variables*No.Constraints)ShapeNo.Variables/No.ConstraintsEqualityPropNo.EqualityConstraints/TotalNo.Constraints9TableA8:PredictionRMSEResults:ShownforeachMachineLearningbasedrankingmethodarethetestRMSEscoresfound.RankingMethodInstanceNameRidgeLassoSVRKNNRFMLPStackingVotingcost266-UUE.mps0.0590.0730.0890.0620.0590.0690.0830.050dfn-bwin-DBE.mps0.2470.1460.1280.1890.1180.1230.1070.140germany50-UUM.mps0.1440.0730.1130.0830.0880.0990.0840.100ta1-UUM.mps0.2440.0820.1530.1260.0970.1320.0900.137ta2-UUE.mps0.0800.0460.0650.0470.0440.0300.0390.036g200x740.mps0.1130.1190.0840.1100.1230.1010.1810.070h50x2450.mps0.1970.0590.0540.2270.0640.0410.0960.076h80x6320d.mps0.2450.3560.2780.2060.3020.2430.3350.254k16x240b.mps0.1570.1430.1340.1160.0990.0770.0780.090snp-02-004-104.mps0.2390.2030.2410.2170.2070.2220.2160.219snp-04-052-052.mps0.2380.1810.1600.1440.1700.1540.1470.155snp-06-004-052.mps0.1370.1480.1610.1330.1150.1560.1040.119snp-10-004-052.mps0.1350.1610.1380.1290.1000.1350.1130.116snp-10-052-052.mps0.1870.1960.1560.1740.1930.1610.1730.15910