2
2
0
2

p
e
S
5
2

]

G
L
.
s
c
[

3
v
4
4
9
9
0
.
8
0
2
2
:
v
i
X
r
a

MolGraph: a Python package for the implementation of small
molecular graphs and graph neural networks with TensorFlow and
Keras

A PREPRINT

Alexander Kensert1,2,* , Gert Desmet2 and Deirdre Cabooter1

(1) University of Leuven (KU Leuven), Department for Pharmaceutical and Pharmacological
Sciences, Pharmaceutical Analysis, Herestraat 49, 3000 Leuven, Belgium
(2) Vrije Universiteit Brussel (VUB), Department of Chemical Engineering, Pleinlaan 2,
1050 Brussel, Belgium
(*) Corresponding author, email: alexander.kensert@gmail.com

Abstract

Molecular machine learning (ML) has proven important for tackling various molecular problems,
including the prediction of protein-drug interactions and blood brain-barrier permeability. Since
relatively recently, so-called graph neural networks (GNNs) have been implemented for molecular ML,
showing comparable or superior performance to descriptor-based approaches. Although various tools
and packages exist to apply GNNs for molecular ML, a new GNN package, named MolGraph, was
developed in this work with the motivation to create GNNs highly compatible with the TensorFlow and
Keras application programming interface (API). As MolGraph focuses speciﬁcally and exclusively
on molecular ML, a chemistry module was implemented to accommodate the generation of small
molecular graphs — which could then be inputted to the GNNs for molecular ML. To validate the
GNNs, they were benchmarked against the datasets of MoleculeNet, as well as three chromatographic
retention time datasets. The results on these benchmarks show that the GNNs performed as expected.
Additionally, the GNNs proved useful for molecular identiﬁcation and improved interpretability of
chromatographic retention time data. MolGraph is available at https://github.com/akensert/
molgraph.

1

Introduction

Molecular machine learning (ML) has been important in solving various molecular problems; these include prediction of
aqueous solubility[1], binding of drugs to speciﬁc target proteins[2] and blood brain-barrier permeability[3]. Speciﬁcally,
molecular ML tackles these problems by linking structural information (these could be descriptors[4] or molecular
ﬁngerprints[5]) to some label (e.g., aqueous solubility measurements, binding or no binding to a protein, or permeability).
As molecules are naturally represented as graphs G = (V, E), where V is a set of vertices (or nodes) corresponding to
the atoms, and E a set of edges corresponding to the bonds, it is strongly desired to encode molecules as graphs. By then
subjecting these molecular graphs to graph neural networks[6, 7] (GNNs), highly expressive and meaningful numerical
representations of molecules for downstream classiﬁcation or regression tasks can be obtained. As the application of
GNNs for molecular ML has already proven useful for both predictive modeling[6, 7, 8, 9, 10] and interpretability[9, 11],
it was desired to develop a Python package, MolGraph (available at https://github.com/akensert/molgraph),
for building GNNs speciﬁcally for those purposes (see Figure 1). Importantly, as MolGraph is built to be highly
compatible with the TensorFlow[12] (TF) and Keras[13] application programming interface (API), a GNN model can
be built and utilized with ease (see Appendix A).

 
 
 
 
 
 
MolGraph

A PREPRINT

Figure 1: Schematic visualization of the different modules of MolGraph, and how they are utilized for molecular ML.

1.1 Related works and motivation

Many ML libraries exist in Python, these include Scikit-learn (for implementing classical ML models such as random
forest, gradient boosting, multi-layer perceptrons, support vector machines, etc.), TF, Keras and Pytorch[14] (for
implementing deep neural networks). Built on top of these libraries, people have developed Python packages that
specialize in certain applications. For instance, for GNN applications, these include Spektral[15] (based on Keras/TF),
Deep Graph Library[16] (based on PyTorch, TF and Apache MXNet), DeepChem[17] (based on Keras/TF and PyTorch),
PyTorch Geometric[18], and Graph Nets[19] (based on TF and Sonnet). In contrast to the aforementioned Python
packages, MolGraph focuses exclusively on GNN applications for small molecular graphs, and therefore specializes in
the generation of these graphs. Additionally, MolGraph is developed with the aim to be integrated seamlessly with the
TF and Keras API. This includes compatibility with the Keras Sequential model API and TF’s saved_model and
Dataset API.

Both DeepChem (a prominent Python package for molecular ML) and Spektral (a prominent Python package for
GNN applications) have a broader focus than MolGraph and thus a larger set of tools and utilities. While DeepChem
implements its own dataset and model API, MolGraph relies directly on the dataset and model API of TF and Keras.
Although the latter requires knowledge of TF and Keras, it allows for greater ﬂexibility, where MolGraph’s modules
(mainly its layers module) can be used with TF’s and Keras’ existing modules to tailor GNN implementations to the
speciﬁc need of the user. Spektral, like MolGraph, relies (at least in part) on the model API of TF and Keras. However,
while Spektral implements four different Data modes, depending on the problem at hand, MolGraph only implements
one: the GraphTensor (from hereon referred to as graph tensor). The graph tensor is indexable, conveniently updatable,
batchable, and easily convertible between batches of (sub)graphs and a single disjoint graph — both of which can be
passed to e.g. a Keras model, comprising GNNs (see Appendix B). Ultimately, MolGraph is implemented to reduce
cognitive load.

2 API

2.1 Graph tensor

To make MolGraph compatible with TF, it was important to deﬁne an appropriate Tensor class (namely, the graph
tensor) that can hold graph data (speciﬁcally molecular graphs). The graph tensor has two “conﬁgurations”: a “ragged”
and a “non-ragged” conﬁguration (see explanation later). The former makes the graph tensor batchable, while the latter
makes the graph tensor efﬁcient for the GNN. Speciﬁcally, the graph tensor contains nested tensors, either ragged or
non-ragged, which together encode a molecular graph. These nested tensors include:

• [required] a sparse adjacency matrix, represented as two vectors (tensors) indicating the indices of the
destination nodes and source nodes. Row j corresponds to row j of edge features; and entry (index) i
corresponds to row i of node features;

• [required] node features (or node states);

2

MolGraph

A PREPRINT

• [optional] edge features (or edge states);
• [optional] graph indicator (wherein each entry indicates which graph the node belongs to, relevant for the

non-ragged conﬁguration). Row i corresponds to row i of node features;

• [optional] a positional encoding. Row i corresponds to row i of node features.

The graph tensor implements methods (speciﬁcally, merge and separate) to go between the two conﬁgurations;
and an update (and remove) method that allows to update (and remove) the nested tensors within the graph tensor
(see Appendix B). For the ragged conﬁguration, the graph tensor separates each subgraph (molecule), causing the
second dimension of the nested tensors to be ragged (as the molecules have different numbers of nodes/edges); for the
non-ragged conﬁguration, all subgraphs are merged into a single disjoint graph, separable by the graph indicator.

2.2 Molecular graph

The initial state of the graph tensor is computed via the chemistry module of MolGraph, wherein a
MolecularGraphEncoder transforms string representations of molecules (e.g., SMILES or InChI) to a molecu-
lar graph encoded as a graph tensor (see Appendix A). Internally, the node (atom) and edge (bond) features are
computed from atomic encoders based on a set of features (Table 1). As atoms of a molecule may be assigned 3D
coordinates, MolGraph also supplies a molecular encoder to compute, in addition to the required sparse adjacency
matrix and node features, the distance geometry of a molecule. Specialized GNNs, such as DTNN[20], can then exploit
such information to generate a potentially improved representation of the molecule (for downstream regression or
classiﬁcation tasks).

Table 1: List of available atom and bond features of MolGraph

Atom features

Bond features

Description

Chiral center

Chirality (R, S or None)

Crippen Log P contribution

Crippen Molar refractivity contribution

Degree

Formal charge

Gasteiger charge

Hybridization

Aromatic

Hydrogen donor

Hydrogen acceptor

Hetero

Part of ring

Part of ring of size n

Encoding Description

Encoding

Binary

Bond type

One-hot

Conjugated

Float

Float

Rotatable

Stereo

One-hot

Part of ring

One-hot

Binary

Binary

One-hot

Binary

One-hot

Part of ring of size n One-hot

Float

One-hot

Binary

Binary

Binary

Binary

Binary

One-hot

Labute accessible surface area contribution

Float

Number of hydrogens

Number of radical electrons

Number of valence electrons

Symbol (atom type)

One-hot

One-hot

One-hot

One-hot

Topological polar surface area contribution

Float

3

MolGraph

A PREPRINT

2.3 GNN layers

Similar to how a feed-forward neural network (FFNN) is usually a stack of fully-connected layers, a GNN is usually a
stack of GNN layers. Although the GNN layers of MolGraph take many shapes and forms, they are interchangeable,
making it easy to replace one GNN layer with another. These GNN layers simply take as input a graph tensor (containing
[at least] a sparse adjacency matrix and a set of node features), and outputs a new graph tensor with an updated set
of node features. In essence, the GNN layers operate in two steps: (1) transformation of the node features and (2)
aggregation of the node features (based on the sparse adjacency matrix). In the simplest case, the transformation is a
single linear transformation via a learnable weight matrix on the node features, directly followed by an aggregation,
by averaging, for each destination node, all source nodes’ features. For some of the GNN layers however, these two
steps are more complicated, wherein the updated node features might depend on the relevant edge features and/or an
attention mechanism. The GNN layers of MolGraph can be separated into three different categories: convolutional
(e.g., GCN[21, 22] and GIN[22, 23]), attentional (e.g., GAT[22, 24] and GMM[22, 25]) and message passing (e.g.,
MPNN[8]) (see Figure 2).

Convolutional layers perform an arbitrary transformation on the node states, and subsequently aggregate neighboring
node states j ∈ N (i) to node i via normalization coefﬁcients cij, as follows:

h(l+1)
i

= σ







(cid:88)

cijψ(h(l)
j )

 ,

j∈N (i)

(1)

were σ is a non-linear activation function, e.g., ReLU. The attentional layer adds an attention mechanism, as a replacement
for cij:

h(l+1)
i

= σ







(cid:88)

ij ψ(h(l)
α(l)
j )

 ,

j∈N (i)

where α denotes the attention coefﬁcients, computed based on h(l)
i

and h(l)
j :

ij = a(h(l)
α(l)

i

, h(l)

j ),

(2)

(3)

and a is a function which computes the attention coefﬁcients α(l)
ij . The message-passing layer uses a so-called message-
function which computes the “message” to be propagated to the destination (target) nodes. Speciﬁcally, in comparison
with the attentional (and convolutional) layer, the message-passing layer does not pass h(l)
j directly, but indirectly,
through the message-function; which computes the message based on h(l)
i

and h(l)
j :

h(l+1)
i

= σ







(cid:88)

ψ(h(l)
i

, h(l)
j )



j∈N (i)

(4)

As can be seen, the transformation function depends on both h(l)
i
most expressive.

and h(l)

j , and not only h(l)

j , which makes this approach

In addition to the above, a given GNN layer may implement edge states eij, in addition to the node states; where i and j
indicate that it is the edge state of the edge between node i and j. For example, the graph attention network (GAT) of
MolGraph optionally incorporates edge states, wherein the attention coefﬁcients α(l)
ij are computed based on both the
node states and the associated edge state:

ij = a(h(l)
α(l)

i

, h(l)

j , e(l)
ij )

(5)

4

MolGraph

A PREPRINT

Figure 2: Schematic illustration of the three main types of graph neural networks.

2.4 Readout

After L steps of transformation and aggregation, h(L)
can be
used to perform e.g., node predictions, edges predictions or graph predictions. For graph (molecule) predictions, node
representations h(L) ∈ Rmxk (where m is the number of nodes and k the number of features) need to be reduced to a
one-dimensional vector representation z ∈ Rk of the graph (molecule). MolGraph implements several different layers
that perform this reduction (known as readout). The most basic readout layer simply sums over the nodes (dimension
m) as follows:

is obtained for all nodes i in the [molecular] graph. h(L)

i

i

zk =

h(L)
ik

(cid:88)

i∈m

Subsequently, z can be passed to an FFNN (denoted fθ) to get a prediction (denoted ˆy):

2.5 GNN models

ˆy = fθ(z)

(6)

(7)

Classiﬁcation and regression As the GNN layers can be implemented with either Keras’ Functional API,
Sequential API, or via Keras’ subclassed models, GNN models for classiﬁcation and regression can both eas-
ily and ﬂexibly be created. The fit, evaluate and predict methods of these models then accept either a graph tensor
or a TF Dataset constructed from a graph tensor (see Appendix B). Importantly, to allow for batching, the graph tensor
needs to be in its ragged conﬁguration.

Saliency and gradient activation maps There is an increasing interest in better understanding and interpreting
deep learning models (including GNNs). Among many techniques that can be used for this purpose, two prominent
and well researched techniques are saliency and gradient activation mapping[11]. To allow better understanding and
interpretability of the GNNs, both saliency and gradient activation maps are implemented in MolGraph (see Appendix
A). These two techniques can be used to better understand what molecular substructures are important for the prediction;
in other words, what parts of the molecules the GNN is “looking”.

2.6 TF records

To support larger datasets (exceeding 50 or 100 thousand molecules), which may not ﬁt into memory (RAM), a TF
Records module is implemented in MolGraph (see Appendix C). In brief, TF records allow the user to save data to
disk, which can then efﬁciently be read (from disk) for modeling. Hence, the user may write data, namely molecular

5

MolGraph

A PREPRINT

graphs (graph tensors) and associated labels, as TF records, which can then be read in a computationally and memory
efﬁcient way later on. Depending on the size of the molecular graph, as well as the computational resources, it may
take anything from approximately 30 to 300 seconds to write 100 thousand small molecular graphs to TF record ﬁles;
reading the TF records is signiﬁcantly faster.

3 Experiments and results

3.1 Predictive performance

To verify and evaluate the different GNNs, they were implemented with the Keras Sequential model API, and
subsequently ﬁtted to, and evaluated on, a selection of benchmarks (datasets). The benchmarks included 15 datasets
from MoleculeNet[26], one dataset from Domingo-Almenara et al.[27] (in this study, referred to as “SMRT”; an
abbreviation for small molecular retention times), and two datasets from Bonini et al.[28] (in this study, referred to as
“RPLC” and “HILIC”; reﬂecting which chromatographic separation mode was used, namely, reversed-phase liquid
chromatography and hydrophilic interaction liquid chromatography). To get a better understanding of how well these
GNNs performed, prediction errors were compared between the models of this study and the original publications
of the benchmarks (referenced above). Importantly, the exact train, validation and test splits could not be extracted
from the original publications (except for RPLC and HILIC), and thus exact conclusions cannot be drawn about which
models performed better or worse. Instead, the comparisons give an indication of whether the models of this study
(MolGraph) performed as expected. Furthermore, no hyper-parameter search was performed for any of the models of
MolGraph, due to limited computational resources. Hence, the default hyperparameters were used for the GNNs. For
optimization, Adam[29] was used, with a starting learning rate of 0.0001 and ending learning rate of 0.000001, which
decayed, on every plateau (deﬁned as no improvement for 10 epochs), by a factor of 0.1. The training stopped when no
improvement was made for 20 epochs, wherein the weights of the best epoch were restored for evaluation (on the test
sets). The loss functions used were, for classiﬁcation, binary cross entropy (BCE); and for regression, depending on the
evaluation metric, mean absolute error (MAE), root mean squared error (RMSE) or Huber.

As can be seen in Table 2 and Table 3, the GNNs implemented in MolGraph performed as expected as they showed
comparable performance to the models of MoleculeNet (GC, Weave, MPNN and DTNN). Furthermore, comparing
the GNNs of MolGraph, the attentional and message-passing type models seem, overall, to perform marginally better
than the convolutional type models. This suggests that increasing the complexity of the models could improve the
performance — though at the cost of signiﬁcantly increasing the computational cost. Although the models (of MolGraph)
were run only once against each dataset (one replica), consequently disregarding the variation in performance over
replicate runs, it seems that the relative performance of each model heavily depended on the task it tried to solve
(which dataset was used). For instance, the GT (graph transformer) models showed impressive performance on the
quantum-chemistry datasets (qm7, qm8 and qm9), but less so for the physiology datasets (muv, hiv, pcba and bace). An
explanation for this could be that the GT models, which use a relatively complex attention mechanism, work well when
the molecular graphs are smaller in size (in the case of the quantum-chemistry datasets).

Similar to the observations on MoleculeNet, the performance of the GNNs of MolGraph performed as expected on
the chromatographic retention time (RT) datasets (Table 2). In contrast to the MoleculeNet models, the two models
from literature (namely, for SMRT: DLM; and for RPLC and HILIC: Keras), were not GNNs, but FFNNs. Notably,
the FFNNs were based on molecular descriptors — vectors of precomputed features of molecules. It was of interest
to compare these descriptor-based models (which are commonly used in molecular ML) with the GNNs. Although a
direct comparison cannot be made to the DLM (as the exact splitting of the dataset could not be derived), the results of
this study indicate superior performance of the GNNs. However, a study by Jiang et al.[10] showed somewhat different
results, where descriptor-based models performed comparable to the GNNs on the MoleculeNet datasets.

6

MolGraph

A PREPRINT

Table 2: Predictive performance on the three quantum-chemistry datasets, three physical-chemistry datasets and ﬁve
chromatographic retention time datasets. Dashed line visually separate the models (and results) of this study (MolGraph)
and models (and results) of other studies (see footnote of table). “(E)” indicates that edge features were used. Each
value associated with the quantum-chemistry datasets indicates the mean absolute error (MAE); each value associated
with the physical-chemistry datasets indicates the root mean squared error (RMSE); and each value associated with the
chromatographic RT datasets indicate the mean relative error (MRE). Convolutional type models: GCN, RGCN(E),
GIN, and GraphSage; attentional type models: GAT, GAT(E), GatedGCN, GatedGCN(E), GMM, GT, and GT(E); and
message-passing type models: MPNN(E).

quantum-chemistry

physical-chemistry

chromatography

qm7

qm8

qm9

esol

lipoph.

freesolv

SMRT

RPLC4

HILIC4

GCN

18.9011

0.0086

2.4795

0.5682

0.5412

0.8453

RGCN(E)

13.6039

0.0086

2.0556

0.5992

0.5639

1.3143

GIN

19.2505

0.0084

2.0315

0.5865

0.5381

0.9066

GraphSage

20.4176

0.0092

2.4739

0.5919

0.5449

1.6146

GAT

GAT(E)

22.0590

0.0088

2.6678

0.6471

0.5380

1.0530

17.9556

0.0087

2.1902

0.5961

0.5402

1.1959

GatedGCN

12.2210

0.0088

2.3740

0.5914

0.5253

1.1264

GatedGCN(E)

10.2520

0.008

1.9858

0.5755

0.5422

1.4468

GMM

GT

GT(E)

18.5327

0.0091

2.4415

0.5453

0.5417

0.8613

11.1366

0.0101

2.2205

0.6713

0.6195

1.0952

7.5603

0.0095

1.7375

0.6992

0.5703

1.1807

test

ext.

test

ext.

0.09

0.11

0.10

0.10

0.09

0.11

0.10

0.10

0.09

0.13

0.19

0.17

0.15

0.15

0.16

0.15

0.14

0.15

0.16

0.18

0.17

0.17

0.18

0.16

0.16

0.16

0.16

0.35

0.33

0.30

0.32

0.28

0.29

0.31

0.32

0.28

0.038

0.037

0.037

0.039

0.038

0.038

0.037

0.037

0.038

MPNN(E)

15.0094

0.0089

2.5013

0.5505

0.5365

0.9538

0.038

0.09

0.16

0.15

0.32

10.8403

0.0114

1.3970

77.90

0.0148

0.0143

0.0169

8.8

4.7

3.2

2.4

0.97

0.58

0.655

0.719

1.40

1.15

DTNN
GC1
MPNN1
DTNN1
DLM2
Keras3

0.068

0.13

0.18

0.2

0.46

1 Values are directly extracted from the MoleculeNet paper[26]. The exact dataset splits producing these results may differ from the

dataset splits of this study.

2 Values are directly extracted from Domingo-Almenara et al.[27] The exact dataset splits producing these results differ from the
dataset splits of this study. In their study, a training and test set were used, with 75% and 25% of the data respectively. In this
study, a training, validation and test set were used, with 70%, 5% and 25% of the data respectively.

3 Values are extracted from Bonini et al.[28]. "Keras" refers to the name of their model; a deep fully-connected neural network,

implemented in Keras.

4 These datasets include two test sets: a "test set" and an "external set". See Bonini et al.[28] for more information.

7

MolGraph

A PREPRINT

Table 3: Predictive performance on the four biophysics datasets and ﬁve physiology datasets. Dashed line visually
separate the models (and results) of this study (MolGraph) and models (and results) of other studies (see footnote
of table). “(E)” indicates that edge features were used. Each value associated with muv and pcba indicates the area
under the precision-recall curve (PRC-AUC). Each remaining value indicates the area under the receiver operating
characteristic curve (ROC-AUC). Convolutional type models: GCN, RGCN(E), GIN, and GraphSage; attentional
type models: GAT, GAT(E), GatedGCN, GatedGCN(E), GMM, GT, and GT(E); and message-passing type models:
MPNN(E).

biophysics

physiology

muv

hiv

pcba

bace

clintox

sider

toxcast

tox21

bbbp

GCN

0.0373

0.7964

0.1387

0.8111

0.7800

0.6230

0.7472

0.8233

0.6872

RGCN(E)

0.0377

0.7945

0.1478

0.8213

0.7268

0.6583

0.7501

0.8209

0.6948

GIN

0.0033

0.7677

0.1524

0.8231

0.8188

0.6202

0.7447

0.8420

0.6763

GraphSage

0.0331

0.7763

0.1345

0.8129

0.8320

0.6376

0.7357

0.8264

0.6969

GAT

GAT(E)

0.0308

0.8009

0.1450

0.8294

0.7515

0.6091

0.7413

0.8336

0.7196

0.0279

0.7756

0.1500

0.7806

0.7483

0.6204

0.7342

0.8151

0.6938

GatedGCN

0.0693

0.7669

0.1475

0.8063

0.7754

0.5995

0.7397

0.8348

0.6807

GatedGCN(E)

0.0160

0.7689

0.1482

0.7974

0.8746

0.6302

0.7384

0.8287

0.6967

GMM

GT

GT(E)

MPNN(E)
GC1
Weave1

0.0380

0.7643

0.1466

0.7732

0.8128

0.6280

0.7473

0.8283

0.6903

0.0034

0.7493

0.1356

0.7325

0.8322

0.5704

0.7309

0.8057

0.6857

0.0145

0.7543

0.1404

0.7958

0.8430

0.5922

0.7264

0.7982

0.6726

0.0400

0.7603

0.1427

0.8320

0.8329

0.6400

0.7524

0.8345

0.6942

0.046

0.109

0.763

0.703

0.136

0.783

0.806

0.807

0.832

0.638

0.581

0.716

0.742

0.829

0.820

0.690

0.671

1 Values are directly extracted from the MoleculeNet paper[26]. The exact dataset splits producing these results may

differ from the dataset splits of this study.

8

MolGraph

A PREPRINT

3.2 Molecular identiﬁcation

In addition to benchmarking the predictive performance of the GNNs of MolGraph on the MoleculeNet datasets, it was
also of interest to verify the capabilities of a graph convolutional network (GCN) to assist in molecular identiﬁcation.
For that purpose, a plant metabolite database (PLaSMA, available at http://plasma.riken.jp/) consisting of
liquid chromatography-tandem mass spectrometry (LC-MS/MS) data was used. In brief, the identity (structure) of an
unknown molecule (analyte) in a given sample can be predicted based on LC-MS/MS data; where the MS/MS data is
used to predict the structure of the analyte (using a software such as MSFinder[30, 31]), and the LC RTs are used as
an additional conﬁrmation of the proposed structure. Thus, the GCN was trained to correlate the structure of known
molecules in the PLaSMA dataset to their associated RTs, and subsequently used to obtain more conﬁdence about
proposed structures of analytes. Similar to the benchmarking, the PLaSMA dataset was divided into training, validation
and test sets; in which the training set was used to train the model; the validation set was used to supervise the training
and to determine thresholds for a RT ﬁlter; and the test set was used to evaluate how well the GCN model could ﬁlter
out structures (candidates) suggested by the MSFinder software.

Figure 3 aims to visualize the process of eliminating candidates proposed by MSFinder. The upper plots of Figure 3
illustrate how the validation set was used to deﬁne the RT ﬁlter (two thresholds, or bounds) that would decide whether a
candidate would be discarded (or not) as a possible identity of the analyte. Speciﬁcally, if the distance between the
predicted RT of the candidate and the RT of the analyte was within the bounds, it was kept, otherwise, discarded. The
bounds were deﬁned by the mean and standard deviation of the errors, speciﬁcally, µ ± 2.58σ (−1.313 and 1.337). The
bottom subplot of Figure 3 illustrates to what extent the GCN could ﬁlter out candidates with the RT ﬁlter. As can be
seen, a signiﬁcant portion (about 40%) of the candidates were ﬁltered out (although in speciﬁc cases, none, or very
few, got ﬁltered out). In one case, a false negative occurred — i.e., the candidate matching the identity of the analyte
was discarded. Overall, the results show evidence that the utilization of the GCN for RT predictions may facilitate the
process of identifying analytes.

Figure 3: Retention time (RT) ﬁlter. Thresholds (−1.313 and 1.337) were computed from the validation set and used to
ﬁlter out candidates suggested by MSFinder. The red bar indicates a false negative.

9

MolGraph

A PREPRINT

To inspect the ﬁltering procedure in more detail, the MSFinder candidate scores as well as predicted candidate RTs
(of the GCN) corresponding to analyte 10 (of Figure 3) are shown in Table 4. The candidate in bold is the most likely
identity based on the MSFinder score and RT ﬁlter. In this speciﬁc case, the predicted RT proved to be important for
correctly identifying the analyte. Comparing MSFinder’s three highest scoring candidates and fourth highest scoring
candidate (which ended up being the most likely candidate after applying the RT ﬁlter), it can be observed that the three
highest scoring candidates are signiﬁcantly more polar — hence low RTs in an RPLC setting. As the analyte had a
signiﬁcantly higher RT (because it is signiﬁcantly less polar), the RT ﬁlter sufﬁced to discard them. Although this is
just a single example, it aims to illustrate how combining information of predicted structures based on MS/MS data and
predicted RTs can increase the chances of correctly identifying the analyte.

Table 4: In bold: the most likely identity of the analyte, based on both the MSFinder score and the difference between
predicted RT and analyte RT.

Target Identity:

Target RT:

CS(=O)CCCCCCCN=C=S

5.41

Candidates

Score (MSFinder)

Predicted candidate RT Difference

Filtered out

NC(CCOC(=O)CCC(=O)O)C(=O)O

CC(=O)NC1C(=O)OC(CO)C(O)C1O

CC(O)C(=O)NC(CCC(=O)O)C(=O)O

CS(=O)CCCCCCCN=C=S

CC(OC(=O)CCC(N)C(=O)O)C(=O)O

Cc1ccc(C=NNc2cn[nH]c(=O)n2)o1

Cc1nonc1NC(=O)Nc1cccnc1

CSC=CC(=O)NC=Cc1ccccc1

CSC=CNC(=O)C=Cc1ccccc1

CCOC(=O)NP(=O)(N1CC1)N1CC1

3.3 Gradient activation maps

7.32

7.12

7.06

7.06

6.87

6.73

5.95

5.78

5.72

5.34

1.39

1.71

2.03

6.07

1.63

2.60

1.94

4.63

4.95

2.12

4.02

3.70

3.38

-0.66

3.78

2.81

3.47

0.78

0.46

3.29

Yes

Yes

Yes

No

Yes

Yes

Yes

No

No

Yes

One of the potential strengths of the GNN models is their ability to compute saliency and/or gradient activation maps to
better interpret e.g. retention data; i.e., to better understand what substructures contribute to the (RT) prediction. Figure
4 visualizes four different molecules from the RPLC and HILIC datasets, with gradient activation maps superimposed
on 2D structures of these molecules. More contour lines and greener color indicate more importance; i.e., structures that
are more important for RT prediction, relative to substructures with fewer (or no) contour lines. Interestingly, though
expectedly, non-polar substructures are clearly most important in the RPLC setting, while polar substructures are clearly
most important in the HILIC setting. These results are expected as the non-polar stationary phase of RPLC is expected
to interact with the non-polar substructures of the molecules (causing a difference in retention); and similarly, the polar
stationary phase of HILIC is expected to interact with the polar substructures of the molecules. These observations
agree with the observations of Kensert et al.[9], where saliency maps were used (instead of gradient activation maps).
However, the advantage of using gradient activation maps over saliency maps, is that the former makes use of resulting
node states of every GNN layer — consequently generating more robust maps.

4 Conclusions

One of the main motivations behind MolGraph was to develop a GNN package highly compatible with TF and Keras.
Consequently, MolGraph’s GNN layers can be built and utilized with ease — using the TF Dataset API for efﬁcient
data pipelines, and the Keras Sequential (or Functional) API for concise implementation of models. MolGraph has
an intended limited scope, which on the one hand accommodated an improved chemistry module, but on the other hand
limited the capabilities of MolGraph. In prospect, it would be interesting to implement modules for more advanced
GNN models, including generative and self-supervised models[32, 33]; and furthermore, to focus on larger graphs,
such as proteins and oligonucleotides, which are of great interest in pharmaceutical science[34, 35].

10

MolGraph

A PREPRINT

Figure 4: Comparison of gradient activation maps between the RPLC dataset and the HILIC dataset. Gradient activation
maps (in green) indicate what substructures were important for the RT prediction.

Availability

Source code and license can be found at https://github.com/akensert/molgraph. Documentation and tutorials
can be found at https://molgraph.readthedocs.io/en/latest/.

Authors’ contributions

AK wrote the source code and documentation for the MolGraph package. AK performed the experiments and wrote the
original draft of the manuscript; GD and DC reviewed and edited the manuscript. GD and DC supervised, and acquired
funding for, the project. AK, GD and DC conceptualized the project.

Acknowledgement

Alexander Kensert is funded by a joint-initiative of the Research Foundation Flanders (FWO) and the Walloon Fund for
Scientiﬁc Research (FNRS) (EOS – research project “Chimic” (EOS ID: 30897864)).

Conﬂict of interests

The authors declare that there is no conﬂict of interest.

11

MolGraph

A PREPRINT

References

[1] Qiuji Cui, Shuai Lu, Bingwei Ni, Xian Zeng, Ying Tan, Ya Dong Chen, and Hongping Zhao. Improved prediction
of aqueous solubility of novel compounds by going deeper with deep learning. Frontiers in oncology, 10:121,
2020.

[2] Yan-Bin Wang, Zhu-Hong You, Shan Yang, Hai-Cheng Yi, Zhan-Heng Chen, and Kai Zheng. A deep learning-
based method for drug-target interaction prediction based on long short-term memory neural network. BMC
medical informatics and decision making, 20(2):1–9, 2020.

[3] Yaxia Yuan, Fang Zheng, and Chang-Guo Zhan. Improved prediction of blood–brain barrier permeability through
machine learning with combined use of molecular property-based descriptors and ﬁngerprints. The AAPS journal,
20(3):1–10, 2018.

[4] Roberto Todeschini and Viviana Consonni. Handbook of molecular descriptors. John Wiley & Sons, 2008.
[5] David Rogers and Mathew Hahn. Extended-connectivity ﬁngerprints. Journal of chemical information and

modeling, 50(5):742–754, 2010.

[6] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions:

moving beyond ﬁngerprints. Journal of computer-aided molecular design, 30(8):595–608, 2016.

[7] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik,
and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. Advances in neural
information processing systems, 28, 2015.

[8] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing
for quantum chemistry. In International conference on machine learning, pages 1263–1272. PMLR, 2017.

[9] Alexander Kensert, Robbin Bouwmeester, Kyriakos Efthymiadis, Peter Van Broeck, Gert Desmet, and Deirdre
Cabooter. Graph convolutional networks for improved prediction and interpretability of chromatographic retention
data. Analytical Chemistry, 93(47):15633–15641, 2021.

[10] Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao,
Jian Wu, and Tingjun Hou. Could graph neural networks learn better molecular representation for drug discovery?
a comparison study of descriptor-based and graph-based models. Journal of cheminformatics, 13(1):1–23, 2021.

[11] Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability
methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 10772–10781, 2019.

[12] Sanjay Surendranath Girija. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.

Software available from tensorﬂow. org, 39(9), 2016.

[13] Francois Chollet et al. Keras, 2015.

[14] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems, 32, 2019.

[15] Daniele Grattarola and Cesare Alippi. Graph neural networks in tensorﬂow and keras with spektral [application

notes]. IEEE Computational Intelligence Magazine, 16(1):99–106, 2021.

[16] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai,
et al. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprint
arXiv:1909.01315, 2019.

[17] Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, and Zhenqin Wu. Deep Learning

for the Life Sciences. O’Reilly Media, 2019.

[18] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint

arXiv:1903.02428, 2019.

[19] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz
Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases,
deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.

[20] Kristof T Schütt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Müller, and Alexandre Tkatchenko. Quantum-

chemical insights from deep tensor neural networks. Nature communications, 8(1):1–8, 2017.

[21] Max Welling and Thomas N Kipf. Semi-supervised classiﬁcation with graph convolutional networks. In J.

International Conference on Learning Representations (ICLR 2017), 2016.

12

MolGraph

A PREPRINT

[22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking

graph neural networks. arXiv preprint arXiv:2003.00982, 2020.

[23] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv

preprint arXiv:1810.00826, 2018.

[24] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph

attention networks. stat, 1050:20, 2017.

[25] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein.
In Proceedings of the IEEE

Geometric deep learning on graphs and manifolds using mixture model cnns.
conference on computer vision and pattern recognition, pages 5115–5124, 2017.

[26] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl
Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science,
9(2):513–530, 2018.

[27] Xavier Domingo-Almenara, Carlos Guijas, Elizabeth Billings, J Rafael Montenegro-Burke, Winnie Uritboonthai,
Aries E Aisporna, Emily Chen, H Paul Benton, and Gary Siuzdak. The metlin small molecule dataset for machine
learning-based retention time prediction. Nature communications, 10(1):1–9, 2019.

[28] Paolo Bonini, Tobias Kind, Hiroshi Tsugawa, Dinesh Kumar Barupal, and Oliver Fiehn. Retip: retention time
prediction for compound annotation in untargeted metabolomics. Analytical chemistry, 92(11):7515–7522, 2020.
[29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

[30] Hiroshi Tsugawa, Tobias Kind, Ryo Nakabayashi, Daichi Yukihira, Wataru Tanaka, Tomas Cajka, Kazuki Saito,
Oliver Fiehn, and Masanori Arita. Hydrogen rearrangement rules: computational ms/ms fragmentation and
structure elucidation using ms-ﬁnder software. Analytical chemistry, 88(16):7946–7958, 2016.

[31] Zijuan Lai, Hiroshi Tsugawa, Gert Wohlgemuth, Sajjan Mehta, Matthew Mueller, Yuxuan Zheng, Atsushi Ogiwara,
John Meissen, Megan Showalter, Kohei Takeuchi, et al. Identifying metabolites by integrating metabolome
databases with mass spectrometry cheminformatics. Nature methods, 15(1):53–56, 2018.

[32] Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. arXiv

preprint arXiv:1805.11973, 2018.

[33] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies

for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.

[34] Hiroshi Tsugawa, Tobias Kind, Ryo Nakabayashi, Daichi Yukihira, Wataru Tanaka, Tomas Cajka, Kazuki Saito,
Oliver Fiehn, and Masanori Arita. Hydrogen rearrangement rules: computational ms/ms fragmentation and
structure elucidation using ms-ﬁnder software. Analytical chemistry, 88(16):7946–7958, 2016.

[35] Thomas C Roberts, Robert Langer, and Matthew JA Wood. Advances in oligonucleotide drug delivery. Nature

Reviews Drug Discovery, 19(10):673–694, 2020.

13

MolGraph

A PREPRINT

A A complete GNN implementation for molecular ML

from tensorflow import keras
from molgraph import chemistry
from molgraph import layers
from molgraph import models

# Obtain dataset, specifically ESOL
esol = chemistry.datasets.get(’esol’)
# Define atom and bond encoder
atom_encoder = chemistry.Featurizer([
chemistry.features.Symbol(),
chemistry.features.Hybridization(),
# ...

])
bond_encoder = chemistry.Featurizer([
chemistry.features.BondType(),
# ...

])
# Define molecular graph encoder (from atom and bond encoder)
encoder = chemistry.MolecularGraphEncoder(atom_encoder, bond_encoder)

# Obtain inputs (graph tensors) and associated labels
x_train = encoder(esol[’train’][’x’])
y_train = esol[’train’][’y’]
x_test = encoder(esol[’test’][’x’])
y_test = esol[’test’][’y’]

# Build model via Keras Sequential API
gnn_model = keras.Sequential([

keras.layers.Input(type_spec=x_train.unspecific_spec),
layers.GATConv(name=’gat_conv_1’),
layers.GATConv(name=’gat_conv_2’),
layers.Readout(),
keras.layers.Dense(units=1024, activation=’relu’),
keras.layers.Dense(units=y_train.shape[-1])

])

# Compile, fit and evaluate
gnn_model.compile(optimizer=’adam’, loss=’mae’)
gnn_model.fit(x_train, y_train, epochs=50)
scores = gnn_model.evaluate(x_test, y_test)

# Compute gradient activation maps
gam_model = models.GradientActivationMapping(

model=gnn_model, layer_names=[’gat_conv_1’, ’gat_conv_2’])

maps = gam_model.predict(x_train)

14

MolGraph

A PREPRINT

B The graph tensor and its utility

import tensorflow as tf
import molgraph

# Obtain graph tensor (of non-ragged configuration)
graph_tensor = molgraph.GraphTensor(

data={

’edge_dst’: [0, 1, 2, 2, 3, 3, 4, 4],
’edge_src’: [1, 0, 3, 4, 2, 4, 3, 2],
’node_feature’: [

[1.0, 0.0],
[1.0, 0.0],
[1.0, 0.0],
[1.0, 0.0],
[0.0, 1.0]

],
’graph_indicator’: [0, 0, 1, 1, 1],

}

)
# Index as int (obtain subgraph)
subgraph = graph_tensor[0]
# Index as str (obtain data field)
node_feature = graph_tensor[’node_feature’]
# Convert to ragged configuration
graph_tensor = graph_tensor.separate()
# Convert back to non-ragged configuration
graph_tensor = graph_tensor.merge()
# Update node features
random_feature = tf.random.uniform(node_feature.shape)
graph_tensor = graph_tensor.update({

’node_feature’: random_feature

})
# Construct tf.data.Dataset and batch
graph_tensor = graph_tensor.separate()
dummy_labels = tf.constant([4.2, 3.1])
dataset = tf.data.Dataset.from_tensor_slices((graph_tensor, dummy_labels))
dataset = dataset.batch(2)
# Build model via Keras Sequential API
gcn_model = tf.keras.Sequential([

tf.keras.layers.Input(type_spec=graph_tensor.unspecific_spec),
molgraph.layers.GCNConv(),
molgraph.layers.GCNConv(),
molgraph.layers.Readout()

])
# Compile, fit and evaluate
gcn_model.compile(’sgd’, ’mse’)
gcn_model.fit(dataset) # or gcn_model.fit(graph_tensor, dummy_labels)
scores = gcn_model.evaluate(dataset) # or gcn_model.evaluate(graph_tensor, dummy_labels)

15

MolGraph

A PREPRINT

C TF Records for large datasets

import molgraph

7.4,

# Let’s assume this is a large dataset (>100k molecules)
x_dummy = [’CC’, ’CCC’, ’CCCC’]
y_dummy = [ 5.2,
# Define molecular graph encoder
encoder = molgraph.chemistry.MolecularGraphEncoder(
atom_encoder=molgraph.chemistry.Featurizer([
molgraph.chemistry.features.Symbol(),
molgraph.chemistry.features.Hybridization(),

8.1]

])

)
# Write tf records
molgraph.chemistry.tf_records.write(
path=’/tmp/dummy_records/’,
inputs={’x’: x_dummy, ’y’: y_dummy},
encoder=encoder

)
# Obtain dataset from tf records
ds = molgraph.chemistry.tf_records.load(

path=’/tmp/dummy_records/’, # extract_tuple=(’x’, ’y’)

)
ds = ds.shuffle(3)
ds = ds.batch(2)
ds = ds.prefetch(-1)
for batch in ds.take(1):
print(batch[’x’])

16

