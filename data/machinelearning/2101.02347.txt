2
2
0
2

r
a

M
8
1

]
T
S
.
h
t
a
m

[

4
v
7
4
3
2
0
.
1
0
1
2
:
v
i
X
r
a

SDP Achieves Exact Minimax Optimality in Phase
Synchronization

Chao Gao1 and Anderson Y. Zhang2

1 University of Chicago
2 University of Pennsylvania

March 21, 2022

Abstract

We study the phase synchronization problem with noisy measurements Y = z∗z∗H +
σW ∈ Cn×n, where z∗ is an n-dimensional complex unit-modulus vector and W is a
complex-valued Gaussian random matrix. It is assumed that each entry Yjk is observed
with probability p. We prove that an SDP relaxation of the MLE achieves the error bound
(1 + o(1)) σ2
2np under a normalized squared (cid:96)2 loss. This result matches the minimax lower
bound of the problem, and even the leading constant is sharp. The analysis of the SDP
is based on an equivalent non-convex programming whose solution can be characterized
as a ﬁxed point of the generalized power iteration lifted to a higher dimensional space.
This viewpoint uniﬁes the proofs of the statistical optimality of three diﬀerent meth-
ods: MLE, SDP, and generalized power method. The technique is also applied to the
analysis of the SDP for Z2 synchronization, and we achieve the minimax optimal error
exp (cid:0)−(1 − o(1)) np
2σ2

(cid:1) with a sharp constant in the exponent.

1

Introduction

Consider the problem of phase synchronization [26] with observations

Yjk = z∗

j ¯z∗

k + σWjk ∈ C,

(1)

n ∈ C1 = {x ∈ C : |x| = 1}. Since |z∗

k stands for the complex conjugate of z∗

for 1 ≤ j < k ≤ n, where ¯z∗
k. Our goal is to
j = eiθ∗
estimate z∗
1, · · · , z∗
j with
some θ∗
j ∈ (0, 2π] for all j ∈ [n], and thus Yjk is understood to be a noisy observation of
the pairwise diﬀerence between two angles θ∗
j and θ∗
k. Following [2, 5, 17, 30], we consider
an additive noise model and we assume that Wjk is a standard complex Gaussian variable
independently for all 1 ≤ j < k ≤ n.1

j | = 1, we can write z∗

1For Wjk ∼ CN (0, 1), we have Re(Wjk) ∼ N (cid:0)0, 1

2

(cid:1) and Im(Wjk) ∼ N (cid:0)0, 1

2

(cid:1) independently.

1

 
 
 
 
 
 
(3)

(4)

Recently, minimax risk of estimating z∗ ∈ Cn

1 has been studied by [16] under the loss

function

(cid:96)((cid:98)z, z) = min
a∈C1

1
n

n
(cid:88)

j=1

|(cid:98)zj − zja|2.

(2)

We note that the minimization of a ∈ Cn
1 in the deﬁnition of (2) is necessary, since a
global rotation of the angles θ∗
n does not change the distribution of the observations
{Yjk}1≤j<k≤n. It was proved by [16] that the minimax risk of phase synchronization has the
following lower bound

1, · · · , θ∗

inf
(cid:98)z∈Cn
1

sup
z∈Cn
1

Ez(cid:96)((cid:98)z, z) ≥ (1 − o(1))

σ2
2n

,

and the maximum likelihood estimator (MLE), deﬁned as a global maximizer of

zHY z,

max
z∈Cn
1

is proved to achieve the error bound (1 + o(1)) σ2
2n , and is therefore asymptotically minimax
optimal. However, the optimization problem (4) is a constrained quadratic programming that
is generally known to be NP-hard. This motivates researchers to consider a convex relaxation
of (4) in the form of semi-deﬁnite programming (SDP) [5, 6, 23, 26, 30]. Write Z = zzH. For
any z ∈ Cn
1 , Z is a complex positive-semideﬁnite Hermitian matrix whose diagonal entries
are all one. The SDP relaxation of (4) is then deﬁned as

max
Z=ZH∈Cn×n

Tr(Y Z)

subject to

diag(Z) = In and Z (cid:23) 0.

(5)

A global maximizer of (5), denoted as (cid:98)Z, can thus be used as an estimator of the matrix
z∗z∗H. The tightness of the SDP (5) has been thoroughly investigated in the literature of
phase synchronization. When σ2 = O(n1/2), it was proved by [5] that the solution to (5) is
a rank-one matrix (cid:98)Z = (cid:98)z(cid:98)zH, with (cid:98)z being a global maximizer of (4). This result was recently
proved by [30] to hold under a weaker condition σ2 = O
. Given the tightness of
the SDP and the minimax optimality of the MLE in [16], we can immediately claim that
(cid:17)
the SDP (5) is also asymptotically minimax optimal under the condition σ2 = O
.

(cid:16) n
log n

(cid:17)

(cid:16) n
log n

, whether SDP is still statistically optimal remains as

Without the condition σ2 = O
an open question in the literature.

(cid:16) n
log n

(cid:17)

(cid:17)

(cid:16) n
log n

In this paper, we study the statistical properties of the SDP (5) directly without the
need to establish any connection between SDP and MLE. This allows us to go beyond the
condition σ2 = O
and we are able to derive sharp statistical error bounds of the SDP
(5) as long as σ2 = o(n). According to the minimax lower bound (3), the condition σ2 = o(n)
is necessary for any estimator to have an error rate of a nontrivial order. To formally state
our main result, we introduce a more general statistical estimation setting that allows the
possibility of missing data. Instead of observing Yjk for all 1 ≤ j < k ≤ n, we assume each Yjk
is observed with probability p. In other words, consider a random graph Ajk ∼ Bernoulli(p)
independently for all 1 ≤ j < k ≤ n, and we only observe Yjk that follows (1) when Ajk = 1.

2

The SDP can be extended to this more general setting by replacing all Yjk’s with AjkYjk’s
in (5). The formula will be given by (13) in Section 2.

Theorem 1.1. Assume np
log n → ∞. Let (cid:98)Z be a global maximizer of the SDP
(13) and (cid:98)zj = uj/|uj| for j ∈ [n] with u ∈ Cn being the leading eigenvector of (cid:98)Z. There exists
some δ = o(1) such that

σ2 → ∞ and np

1
n2 (cid:107) (cid:98)Z − z∗z∗H(cid:107)2

F ≤ (1 + δ)

,

σ2
np
σ2
2np

,

(cid:96)((cid:98)z, z∗) ≤ (1 + δ)

with probability at least 1 − n−8 − exp

(cid:16)

− (cid:0) np
σ2

(cid:1)1/4(cid:17)

.

Compared with the minimax lower bound (Theorem 2.1 in Section 2), Theorem 1.1 shows
that SDP leads to both minimax optimal estimations of the matrix z∗z∗H and of the vector
z∗. The two error bounds are not just rate-optimal, but the leading constants are sharp as
well. We remark that both conditions σ2 = o(np) and np
log n → ∞ are essential for the results
of the above theorem to hold. Since the minimax risk of the problem is of order σ2
np , the
condition σ2 = o(np), which is equivalent to σ2
np = o(1), guarantees that the minimax risk is
of smaller order than the trivial one. The order O(1) is trivial, as it can simply be achieved
by random guess. The condition np
log n → ∞ guarantees that the random graph A is connected
with high probability. Our technical analysis would still go through when p is of the same
order as log n

n , but in this regime only rate optimality is achieved.

Our analysis of the SDP does not rely on its connection to the MLE, and it is therefore
fundamentally diﬀerent from the approaches considered by [5, 23, 30]. To study the statistical
properties of SDP directly, we consider the following iteration procedure, 2

V (t)
j =

(cid:80)

(cid:80)

(cid:13)
(cid:13)
(cid:13)

k∈[n]\{j}

k∈[n]\{j}

¯YjkV (t−1)
k
¯YjkV (t−1)
k

(cid:13)
(cid:13)
(cid:13)

∈ Cn,

j = 1, · · · , n.

(6)

Deﬁne the matrix V (t) ∈ Cn×n with its jth column being V (t)
. The above iteration can
be shorthanded as V (t) = f (V (t−1)). We use (6) as a non-convex characterization of the
SDP (5), because the solution to (5) can always be written as (cid:98)Z = (cid:98)V H (cid:98)V for some (cid:98)V ∈ Cn×n
satisfying the ﬁxed-point equation (cid:98)V = f ( (cid:98)V ). Note that the iterative procedure (6) resembles
the formula of the generalized power method (GPM) [8, 26, 30], 3

j

z(t)
j =

(cid:80)

(cid:12)
(cid:80)
(cid:12)
(cid:12)

k∈[n]\{j} Yjkz(t−1)
k∈[n]\{j} Yjkz(t−1)

k

k

∈ C,

j = 1, · · · , n.

(7)

(cid:12)
(cid:12)
(cid:12)

2When the denominator (6) is zero, take V (t)
3When the denominator (7) is zero, take z(t)

j = V (t−1)
j = z(t−1)
.

j

j

.

3

We can therefore think of (6) as a lift of the GPM (7) into a higher dimensional space. This
allows us to analyze the statistical error of SDP from an iterative algorithm perspective,
and previous techniques of analyzing general iterative algorithms in [15, 24] can be borrowed
for the current purpose. To understand the exact statistical error of SDP, we establish the
following convergence result for the iterative procedure (6),

(cid:96)(V (t), z∗) ≤ δ(cid:96)(V (t−1), z∗) + optimal statistical error,

for all t ≥ 1,

(8)

for some δ = o(1) with high probability, as long as it is properly initialized. Here, with slight
abuse of notation, the loss of (cid:98)V is deﬁned by

(cid:96)( (cid:98)V , z∗) =

min
a∈Cn:(cid:107)a(cid:107)2=1

1
n

n
(cid:88)

j=1

(cid:107) (cid:98)Vj − ¯z∗

j a(cid:107)2,

(9)

which is natural given that the matrix (cid:98)Z = (cid:98)V H (cid:98)V is used to estimate z∗z∗H. Since the SDP
solution is a ﬁxed point of the iteration (6), the convergence result (8) directly leads to the
sharp statistical error bounds in Theorem 1.1.

Our analysis of SDP through (6) also uniﬁes the understandings of the GPM and the

MLE. Given the relation between (6) and (7), the convergence result (8) directly implies

(cid:96)(z(t), z∗) ≤ δ(cid:96)(z(t−1), z∗) + optimal statistical error,

for all t ≥ 1,

(10)

for some δ = o(1) with high probability, as long as the GPM is properly initialized. This
provides an alternative proof to the minimax optimality of the GPM that has been previously
established by [16]. In addition, just as the SDP can be viewed as a ﬁxed point of the iteration
(6), the MLE can be viewed as a ﬁxed point of the iteration (7). The minimax optimality
of the MLE can also be derived. To summarize, we are able to show the exact minimax
optimality of SDP, GPM, and MLE using a single proof based on the iterative procedure (6).
In addition to phase synchronization, we also establish the optimality of the SDP for Z2
k + σWjk ∈ R
synchronization. In the setting of Z2 synchronization, one observes Yjk = z∗
for 1 ≤ j < k ≤ n, and the goal is to estimate z∗
n ∈ {−1, 1}. Assume Wjk ∼ N (0, 1)
and each Yjk is observed with probability p, we show that the SDP for Z2 synchronization
achieves the error

1, · · · , z∗

j z∗

exp

−(1 − o(1))

.

(11)

(cid:16)

(cid:17)

np
2σ2

We also prove a matching lower bound for this problem. Since Z2 synchronization is a discrete
parameter estimation problem, the minimax risk is an exponential function of the signal-
to-noise ratio, compared with the polynomial function for phase synchronization. Despite
being a continuous optimization method, the SDP is able to adapt to the discreteness of
the problem. The exponential rate (11) has been previously derived for p = 1 by [13]. Our
analysis based on the iterative algorithm perspective generalizes their result to more general
values of p (cid:29) log n
n .

4

Paper Organization. The rest of the paper is organized as follows.
In Section 2, we
establish the statistical optimality of the SDP for phase synchronization. The implications of
the SDP analysis on the statistical error bounds of GPM and MLE are discussed in Section
3. The analysis of the SDP for Z2 synchronization is presented in Section 4. Finally, Section
5 collects all the technical proofs of the paper.

(cid:113)(cid:80)d

Notation. For d ∈ N, we write [d] = {1, . . . , d}. Given a, b ∈ R, we write a ∨ b = max(a, b)
and a ∧ b = min(a, b). For a set S, we use I{S} and |S| to denote its indicator function and
cardinality respectively. For a complex number x ∈ C, we use ¯x for its complex conjugate,
Re(x) for its real part, Im(x) for its imaginary part, and |x| for its modulus. For a complex
j=1 |xj|2 for its norm. For a matrix B = (Bjk) ∈ Cd1×d2,
vector x ∈ Cd, we use (cid:107)x(cid:107) =
we use BH ∈ Cd2×d1 for its conjugate transpose such that BH = ( ¯Bkj). The Frobenius
k=1 |Bjk|2 and (cid:107)B(cid:107)op =
norm and operator norm of B are deﬁned by (cid:107)B(cid:107)F =
supu∈Cd1 ,v∈Cd2 :(cid:107)u(cid:107)=(cid:107)v(cid:107)=1 uHBv. We use Tr(B) for the trace of a squared matrix B. For
U, V ∈ Cd1×d2, U ◦ V ∈ Rd1×d2 is the Hadamard product U ◦ V = (UjkVjk). The notation
P and E are generic probability and expectation operators whose distribution is determined
from the context. For two positive sequences {an} and {bn}, an (cid:46) bn or an = O(bn) means
an ≤ Cbn for some constant C > 0 independent of n. We also write an = o(bn) or bn
→ ∞
an
when lim supn

(cid:113)(cid:80)d1
j=1

= 0.

(cid:80)d2

an
bn

2 Main Results

2.1 Problem Settings

Recall that we observe a random graph Ajk ∼ Bernoulli(p) independently for all 1 ≤ j <
k ≤ n. For each pair (j, k), we observe Yjk = z∗
k + σWjk with Wjk ∼ CN (0, 1) whenever
Ajk = 1. The observations can be organized as an adjacency matrix A and a masked version
of the pairwise interactions A ◦ Y . All the matrices A, W , and Y are Hermitian as we deﬁne
Ajk = Akj, Wjk = ¯Wkj, and Yjk = ¯Ykj for all 1 ≤ k < j ≤ n and Ajj = Wjj = 0 and Yjj = 1
for all j ∈ [n]. Hence we have the matrix representation Y = z∗z∗H + σW .

j ¯z∗

To estimate the vector z∗ ∈ Cn

1 , the MLE is deﬁned as a global maximizer of the following

optimization problem

zH(A ◦ Y )z.

max
z∈Cn
1

(12)

Since (12) is computationally infeasible, we consider the following convex relaxation of (12)
via SDP,

max
Z=ZH∈Cn×n

Tr((A ◦ Y )Z)

subject to

diag(Z) = In and Z (cid:23) 0.

(13)

The goal of our paper is to establish the statistical optimality of the SDP (13). We ﬁrst
provide a minimax lower bound as the benchmark of the problem.

5

Theorem 2.1 (Theorem 4.1 of [16]). Assume σ2 = o(np). Then, we have

inf
(cid:98)Z∈Cn×n

sup
z∈Cn
1

Ez

1
n2 (cid:107) (cid:98)Z − zzH(cid:107)2

F ≥ (1 − δ)

,

σ2
np
σ2
2np

,

inf
(cid:98)z∈Cn
1

sup
z∈Cn
1

Ez(cid:96)((cid:98)z, z) ≥ (1 − δ)

for some δ = o(1).

The above theorem has been established by [16] as the minimax lower bound for phase
synchronization. In fact, Theorem 4.1 of [16] only states the lower bound result for the loss
function (cid:96)((cid:98)z, z). However, the proof of Theorem 4.1 of [16] actually established the lower
bound under the loss 1
n2 (cid:107) (cid:98)Z − zzH(cid:107)2
F, and the lower bound for (cid:96)((cid:98)z, z) is proved as a direct
consequence in view of the inequality

inf
(cid:98)z∈Cn
1

sup
z∈Cn
1

Ez(cid:96)((cid:98)z, z) ≥

1
2

inf
(cid:98)Z∈Cn×n

sup
z∈Cn
1

Ez

1
n2 (cid:107) (cid:98)Z − zzH(cid:107)2
F.

Since the solution of the SDP (13) is a matrix, it is natural to study the statistical error
under 1

F in addition to the loss (cid:96)((cid:98)z, z).

n2 (cid:107) (cid:98)Z − zzH(cid:107)2

2.2 A Convergence Lemma

Our analysis of the SDP (13) relies on an equivalent non-convex characterization. Since Z is
a positive semi-deﬁnite Hermitian matrix, it admits a decomposition

Z = V HV,

for some V ∈ Cn×n. Let Vj be the jth column of V , and we have Zjk = V H
j Vk. In particular,
the constraint diag(Z) = In can be written as Zjj = (cid:107)Vj(cid:107)2 = 1 for all j ∈ [n]. Replacing Z
by V HV , the SDP (13) can be equivalently represented as

max
V ∈Cn×n

Tr((A ◦ Y )V HV )

subject to (cid:107)Vj(cid:107)2 = 1 for all j ∈ [n].

(14)

The formulation (14) is closely related to the Burer-Monteiro problem [9, 20] for the SDP
except that here V is still an n × n matrix without dimension reduction. This non-convex
formulation allows us to derive sharp statistical error bounds of the SDP (13).

We analyze (14) through the following iteration procedure,

V (t)
j =

(cid:80)

k∈[n]\{j} Ajk
k∈[n]\{j} Ajk

¯YjkV (t−1)
k
¯YjkV (t−1)
k

(cid:13)
(cid:13)
(cid:13)






(cid:13)
(cid:80)
(cid:13)
(cid:13)
V (t−1)
j

,

, (cid:80)

k∈[n]\{j} Ajk ¯YjkV (t−1)
k∈[n]\{j} Ajk ¯YjkV (t−1)

k

k

(cid:80)

Let us shorthand the above formula by

V (t) = f (V (t−1)),

6

(cid:54)= 0,

= 0.

(15)

(16)

by introducing a map f : Cn×n
such that the jth column of f (V (t−1)) is given by
(15). We use the notation Cn×n
for the set of n × n complex matrices whose columns all
have unit norms. The update (16) can be seen as a local approach (or more precisely, a
block coordinate ascent approach) [12, 27] to solve (14). To see why this is true, consider the
following local optimization problem

1 → Cn×n
1

1

max
Vj ∈Cn:(cid:107)Vj (cid:107)2=1


V H
j










H



(cid:88)

Ajk ¯YjkV (t−1)

k

 +



(cid:88)

Ajk ¯YjkV (t−1)

k



Vj

 .

(17)

k∈[n]\{j}

k∈[n]\{j}

The objective of (17) collects the terms in the expansion of Tr((A◦Y )V HV ) = (cid:80)
that depend on Vj and replaces Vk by V (t−1)
see the solution of (17) is exactly (15).

j Vk
for all k ∈ [n]\{j}. By simple algebra, we can

jk Ajk ¯YjkV H

k

Let (cid:98)V be a global maximizer of (14). The matrix (cid:98)V must be a ﬁxed point of the map f ,

(cid:98)V = f ( (cid:98)V ).

(18)

To see why (18) holds, we consider the local optimization problem (17) with V (t−1)
replaced
by (cid:98)Vk for all k ∈ [n]\{j}. Thus, as long as (cid:98)V maximizes (14), its jth column (cid:98)Vj must maximize
this local optimization problem, which then implies the ﬁxed-point equation (18).

k

Since the SDP solution (cid:98)Z = (cid:98)V H (cid:98)V is an estimator of the matrix z∗z∗H, we can think of (cid:98)Vj

as an estimator of ¯z∗

j embedded in Cn. Note that

j ¯z∗
z∗

k = z∗

j aHa¯z∗
k,

for any a ∈ Cn such that (cid:107)a(cid:107)2 = 1, and thus we can embed each ¯z∗
vector ¯z∗
following lemma characterizes the evolution of this loss function through the map f .

j in Cn by considering the
j a ∈ Cn. This motivates the deﬁnition of the loss function (cid:96)( (cid:98)V , z∗) given in (9). The

Lemma 2.1. Assume np
Then, for any γ ∈ [0, 1/16), we have

σ2 > c1 and np

log n > c2 for some suﬃciently large constants c1, c2 > 0.

(cid:18)

(cid:96)(f (V ), z∗) ≤ δ1(cid:96)(V, z∗) + (1 + δ2)

P

σ2
2np

for all V ∈ Cn×n

1

such that (cid:96)(V, z∗) ≤ γ

(cid:19)

≥ 1 − (2n)−1 − exp

(cid:18)

−

(cid:16) np
σ2

(cid:17)1/4(cid:19)

.

where δ1 = C1

(cid:113) log n+σ2
np

and δ2 = C2

(cid:16)

γ2 + log n+σ2

np

(cid:17)1/4

for some constants C1, C2 > 0.

The lemma shows that for any V ∈ Cn×n

that has a nontrivial error, the matrix f (V ) will
have an error that is smaller by a multiplicative factor δ1 up to an additive term (1 + δ2) σ2
2np .
Deﬁne V ∗ ∈ Cn×n
j a for some a ∈ Cn that satisﬁes
1
(cid:107)a(cid:107)2 = 1. We immediately have

with the jth column given by V ∗

j = ¯z∗

1

(cid:96)(f (V ∗), z∗) ≤ (1 + δ2)

σ2
2np

.

7

Thus, the additive term (1 + δ2) σ2
the knowledge of z∗.

2np can be understood as the oracle statistical error given

np

σ2 ≥ c1 and np

The two conditions np

log n ≥ c2 are essential for the result to hold. While
σ2 ≥ c1 makes sure that the statistical error σ2
2np is of a nontrivial order, the condition
np
log n ≥ c2 guarantees that the random graph is connected. We can slightly strengthen both
conditions to np

log n → ∞ so that both δ1 and δ2 are varnishing.

σ2 → ∞ and np

2.3 Statistical Optimality of SDP

In this section, we show the result of Lemma 2.1 implies the statistical optimality of the
SDP (13). Since the solution of the SDP can be written as (cid:98)Z = (cid:98)V H (cid:98)V with (cid:98)V satisfying the
ﬁxed-point equation (18), we can apply the result of Lemma 2.1 to (cid:98)V = f ( (cid:98)V ) as long as a
crude bound (cid:96)( (cid:98)V , z∗) ≤ γ can be proved for some γ < 1/16.
Lemma 2.2. Assume np
a global maximizer of the SDP (13). Then, there exits some constant C > 0 such that

log n > c for some suﬃciently large constant c > 0. Let (cid:98)Z = (cid:98)V H (cid:98)V be

(cid:96)( (cid:98)V , z∗) ≤ C

(cid:115)

σ2 + 1
np

,

with probability at least 1 − n−9.

Under the condition that np

σ2 and np

log n are suﬃciently large, we have (cid:96)( (cid:98)V , z∗) ≤ γ for some

γ < 1/16. Thus, Lemma 2.1 and the fact (cid:98)V = f ( (cid:98)V ) imply that
σ2
2np

(cid:96)( (cid:98)V , z∗) ≤ δ1(cid:96)( (cid:98)V , z∗) + (1 + δ2)

.

(19)

After rearrangement, we obtain the bound (cid:96)( (cid:98)V , z∗) ≤ 1+δ2
1−δ1
into the following theorem.

σ2
2np . The result is summarized

Theorem 2.2. Assume np
log n > c2 for some suﬃciently large constants c1, c2 >
0. Let (cid:98)Z = (cid:98)V H (cid:98)V be a global maximizer of the SDP (13). Then, there exists some δ =

σ2 > c1 and np

(cid:16) log n+σ2
np

C

(cid:17)1/4

for some constant C > 0, such that

(cid:96)( (cid:98)V , z∗) ≤ (1 + δ)

with probability at least 1 − 2n−9 − exp

F ≤ (1 + δ)

1
n2 (cid:107) (cid:98)Z − z∗z∗H(cid:107)2
− (cid:0) np
σ2

(cid:16)

(cid:1)1/4(cid:17)

.

,

σ2
2np
σ2
np

,

Theorem 2.2 gives sharp statistical error bounds for both loss functions (cid:96)( (cid:98)V , z∗) and
n2 (cid:107) (cid:98)Z −

F. While the result for (cid:96)( (cid:98)V , z∗) is derived from (19), the result for 1

1

n2 (cid:107) (cid:98)Z − z∗z∗H(cid:107)2
z∗z∗H(cid:107)2

F is a consequence of the inequality

1
n2 (cid:107) (cid:98)V H (cid:98)V − z∗z∗H(cid:107)2

F ≤ 2(cid:96)( (cid:98)V , z∗),

8

which is established by Lemma 5.6 in Section 5.1. Compared with the minimax lower bound
in Theorem 2.1, we can conclude that the SDP (13) is minimax optimal for the estimation of
the matrix z∗z∗H. It not only achieves the optimal rate, but the leading constant is also sharp
when σ2 = o(np) and np
log n → ∞. Figure 1 veriﬁes the correctness of the leading constants of
the two loss functions. Both loss functions are approximately linear at least when σ2 is small.
When σ2 and np are of the same order, SDP does not have the optimal constant anymore,
and its asymptotics is predicted by a very diﬀerent technique [20]. We also remark that the
(cid:96)2 error control does not imply that each individual z∗
j can be accurately recovered. This is
also reﬂected in Figure 1 with the comparison between (cid:96)2 and (cid:96)∞ loss.

Figure 1: Left: average values of (cid:96)( (cid:98)V , z∗) (orange solid), 1
F (blue dashed) and
max1≤j≤n (cid:107) (cid:98)Vj − ¯z∗
j a(cid:107)2 (purple dash-dotted) with n = 100, p = 0.2 and σ2 varying in [0, 6]
across 400 independent experiments. The vector a ∈ Cn used in the squared (cid:96)∞ loss is the
minimizer of the right hand side of (9). Right: probability of the event that the second
eigenvalue of (cid:98)Z is nonzero (i.e., (cid:98)Z is not rank-one) for the same experiments.

n2 (cid:107) (cid:98)Z − z∗z∗H(cid:107)2

We emphasize that our proof of the optimality of the SDP is based on a direct statistical
error analysis, regardless of whether the SDP relaxation is tight or not. It is shown by [30]
that the tightness of SDP (when the solution has rank one) requires σ2 = O
at least
when p = 1. When σ2 = o(np), it is possible that SDP is not tight but still statistically
optimal. This point is also illustrated by Figure 1.

(cid:16) n
log n

(cid:17)

Since the solution of the SDP is a matrix, some post-processing step is required to obtain
a vector estimator for z∗. This can easily be done by extracting the leading eigenvector of
(cid:98)Z. Let u ∈ Cn be the leading eigenvector of (cid:98)Z, and deﬁne (cid:98)z with each entry (cid:98)zj = uj/|uj|.
If uj = 0 we can take (cid:98)zj = 1. The statistical optimality of (cid:98)z is established by the following
result. Recall that for two vectors in Cn

1 , the deﬁnition of the loss (cid:96)((cid:98)z, z∗) is given by (2).
log n > c2 for some suﬃciently large constants c1, c2 > 0.

Theorem 2.3. Assume np
Let (cid:98)Z = (cid:98)V H (cid:98)V be a global maximizer of the SDP (13). Then,
(cid:19)1/4(cid:33)

σ2 > c1 and np

(cid:32)

(cid:96)((cid:98)z, z∗) ≤

1 + C

(cid:18) log n + σ2
np

σ2
2np

,

9

01234560.00.51.01.52.02.5s2Squared l2 normSquared Frobenius normSquared l¥ norm01234560.00.20.40.60.81.0s2with probability at least 1 − 2n−9 − exp

(cid:16)

− (cid:0) np
σ2

(cid:1)1/4(cid:17)

for some constant C > 0.

Compared with the minimax lower bound in Theorem 2.1, the SDP (13) is also minimax
optimal for the estimation of the vector z∗ in phase synchronization. Theorem 2.2 and
Theorem 2.3 together establish Theorem 1.1.

3

Implications on Generalized Power Method and MLE

In this section, we show that the analysis of the SDP through Lemma 2.1 also leads to
statistical optimality of the generalized power method (GPM) and the maximum likelihood
estimator (MLE). We note that it has already been established by [16] that both GPM and
MLE achieve the optimal error bound (1 + o(1)) σ2
2np under the loss (cid:96)((cid:98)z, z∗). By deriving the
same results using the analysis of the SDP, we can unify the three proofs and thus form a
coherent understanding of the three diﬀerent methods.
The iteration of GPM of phase synchronization is

z(t)
j =

k∈[n]\{j} AjkYjkz(t−1)
k∈[n]\{j} AjkYjkz(t−1)

k

k






(cid:80)
(cid:12)
(cid:80)
(cid:12)
(cid:12)
z(t−1)
j

,

(cid:12)
(cid:12)
(cid:12)

, (cid:80)

k∈[n]\{j} AjkYjkz(t−1)
k∈[n]\{j} AjkYjkz(t−1)

k

k

(cid:80)

(cid:54)= 0,

= 0.

(20)

The similarity between (20) and (15) is obvious. To make an explicit connection between the
two iteration procedures, we can embed (20) into the space of (15). Let e1 ∈ Cn be the ﬁrst
canonical vector with the ﬁrst entry 1 and the remaining entries all 0. It is easy to check
that as long as V (t−1)
j = ¯z(t)
j e1 for all j ∈ [n].
j
This is because once the columns V (t)
lie in the same one-dimensional subspace for
n
some t, the iteration (15) remains in this subspace. Thus, the formula (15) exactly describes
the GPM iteration (20). In addition to the connection between (20) and (15), the two loss
functions (cid:96)(V, z∗) and (cid:96)(z, z∗) are also equivalent. Under the condition that Vj = ¯zje1 for all
j ∈ [n], we have

e1 for all j ∈ [n], we also have V (t)

1 , · · · , V (t)

= ¯z(t−1)
j

Therefore, Lemma 2.1 directly implies that

(cid:96)(V, z∗) = (cid:96)(z, z∗).

(cid:96)(g(z), z∗) ≤ δ1(cid:96)(z, z∗) + (1 + δ2)

σ2
2np

,

(21)

uniformly over all z ∈ Cn
Cn

1 is deﬁned so that (20) can be shorthanded by z(t) = g(z(t−1)).

1 such that (cid:96)(z, z∗) < 1/16 with high probability. The map g : Cn

1 →

From (21), we know that as long as (cid:96)(z(t−1), z∗) ≤ γ for some γ < 1/16, the next step of

power iteration (20) satisﬁes

(cid:96)(z(t), z∗) ≤ δ1(cid:96)(z(t−1), z∗) + (1 + δ2)

σ2
2np

.

(22)

The condition (cid:96)(z(t−1), z∗) ≤ γ then implies (cid:96)(z(t), z∗) ≤ δ1γ + (1 + δ2) σ2
2np is suﬃciently small, we can always choose γ < 1/16 that satisﬁes σ2
σ2

2np ≤ γ

2np . Given that
2 . Therefore,

10

(cid:96)(z(t), z∗) ≤ γ. Thus, a simple induction argument implies that (22) holds for all t ≥ 1 as
long as (cid:96)(z(0), z∗) ≤ γ. The one-step iteration bound (22) immediately implies the linear
convergence

(cid:96)(z(t), z∗) ≤ δt

1(cid:96)(z(0), z∗) +

1 + δ2
1 − δ1

σ2
2np

,

(23)

for all t ≥ 1. It has been shown by [16] that the initial error condition (cid:96)(z(0), z∗) ≤ γ < 1/16
is satisﬁed by a simple eigenvector method. That is, z(0)
j = vj/|vj| with v ∈ Cn being the
leading eigenvector of the matrix A ◦ Y . Then, (23) implies (cid:96)(z(t), z∗) ≤ (1 + o(1)) σ2
2np for all
t ≥ log (cid:0) 1
σ2

(cid:1).

The optimality of the MLE can be derived from a similar embedding argument. Let (cid:98)z be

a global maximizer of (12). By the deﬁnition of (cid:98)z, its jth entry must satisfy
k∈[n]\{j} AjkYjk(cid:98)zk
k∈[n]\{j} AjkYjk(cid:98)zk

(cid:98)zj = argmin
zj ∈C1

Ajk|Yjk − zj

¯
(cid:98)zk|2 =

k∈[n]\{j}

(cid:88)

(cid:12)
(cid:80)
(cid:12)
(cid:12)

(cid:80)

(cid:12)
(cid:12)
(cid:12)

,

as long as (cid:80)
k∈[n]\{j} AjkYjk(cid:98)zk (cid:54)= 0. By letting (cid:98)V = e1(cid:98)zH, it can be shown that the ﬁxed-point
equation (cid:98)V = f ( (cid:98)V ) holds. Given the equivalence of the loss (cid:96)( (cid:98)V , z∗) = (cid:96)((cid:98)z, z∗), as long as we
can show a crude bound (cid:96)((cid:98)z, z∗) ≤ γ < 1/16 for the MLE, the inequality (19) holds and it
can be written as

(cid:96)((cid:98)z, z∗) ≤ δ1(cid:96)((cid:98)z, z∗) + (1 + δ2)

σ2
2np
σ2
which implies (cid:96)((cid:98)z, z∗) ≤ 1+δ2
2np after rearrangement. The crude bound (cid:96)((cid:98)z, z∗) ≤ γ < 1/16
can be easily established for the MLE using the argument in [16] or by a similar argument to
the proof of Lemma 2.2, and thus we obtain the optimal error bound (cid:96)((cid:98)z, z∗) ≤ (1 + o(1)) σ2
for the MLE.

1−δ1

2np

,

4 SDP for Z2 Synchronization

In this section, we show our analysis of SDP can also be applied to Z2 synchronization and
leads to a sharp exponential statistical error rate. Suppose we observe a random graph
Ajk ∼ Bernoulli(p) independently for all 1 ≤ j < k ≤ n. For each pair (j, k), we observe
In Z2
Yjk = z∗
synchronization, our goal is to estimate the binary vector z∗ ∈ {−1, 1}n from observations
{Ajk}1≤j<k≤n and {AjkYjk}1≤j<k≤n. We organize the data into two matrices A and A ◦ Y .
Both the matrices A and Y are symmetric as we deﬁne Yjk = Ykj and Ajk = Akj for all
1 ≤ k < j ≤ n and Yjj = Ajj = 0 for all j ∈ [n].

k ∈ {−1, 1} and Wjk ∼ N (0, 1) whenever Ajk = 1.

k + σWjk with z∗

j , z∗

j z∗

With slight abuse of notation, we consider the loss function

(cid:96)((cid:98)z, z) = min

a∈{−1,1}

1
n

n
(cid:88)

j=1

|(cid:98)zj − zja|2,

11

for any (cid:98)z, z ∈ {−1, 1}n. Since |(cid:98)zj − zja|2 = 4I{(cid:98)zj (cid:54)= zja}, the loss (cid:96)((cid:98)z, z) is also called the
misclassiﬁcation proportion in a clustering problem [15, 29]. We ﬁrst present the minimax
lower bound of Z2 synchronization under this loss function.

Theorem 4.1. Assume np
Then, we have

σ2 > c1 and np

log n > c2 for some suﬃciently large constants c1, c2 > 0.

inf
(cid:98)Z∈Rn×n

sup
z∈{−1,1}n

Ez

1
n2 (cid:107) (cid:98)Z − zzT(cid:107)2

F ≥ exp

(cid:16)

−(1 + δ)

(cid:17)

,

(cid:17)

,

np
2σ2
np
2σ2

−(1 + δ)

inf
(cid:98)z∈{−1,1}n

sup
z∈{−1,1}n

Ez(cid:96)((cid:98)z, z) ≥ exp

(cid:16)

where δ = C

(cid:113) log n+σ2
np

for some constant C > 0.

When p = 1, the above result has been proved by [13], but the lower bound result for a
general p is unknown in the literature. Compared with Theorem 2.1, the minimax lower bound
for Z2 synchronization is an exponential function of the signal-to-noise ratio, a consequence
of the discreteness of the problem.

To estimate z∗ ∈ {−1, 1}n, the MLE is deﬁned as the global maximizer of the following

optimization problem

max
z∈{−1,1}n

zT(A ◦ Y )z.

(24)

Similar to (13), a convex relaxation of (24) leads to the following SDP,

max
Z=ZT∈Rn×n

Tr((A ◦ Y )Z)

subject to

diag(Z) = In and Z (cid:23) 0.

(25)

The SDP for Z2 synchronization is almost in the exact form of (13). The only diﬀerence
between (25) and (13) is that the optimization (25) is over real symmetric matrices and the
optimization of (13) is over complex Hermitian matrices.

Our analysis of the SDP (25) for Z2 synchronization relies on a non-convex characteriza-
tion that is similar to (14). For any Z that is a positive semi-deﬁnite real symmetric matrix,
it admits a decomposition Z = V TV for some V ∈ Rn×n. By writing the jth column of V
as Vj, we can replace the constraint diag(Z) = In by (cid:107)Vj(cid:107)2 = 1 for all j ∈ [n]. Then, an
equivalent non-convex form of the SDP (25) is

max
V ∈Rn×n

Tr((A ◦ Y )V TV )

subject to (cid:107)Vj(cid:107)2 = 1 for all j ∈ [n].

(26)

We will study the solution of (26) using the following loss function,

(cid:96)( (cid:98)V , z) =

min
a∈Rn:(cid:107)a(cid:107)2=1

1
n

n
(cid:88)

j=1

(cid:107) (cid:98)Vj − zja(cid:107)2.

By the same argument that leads to (18), we know that if (cid:98)V is a global maximizer of (26),
it will satisfy the equation (cid:98)V = f ( (cid:98)V ), where f : Rn×n
is a map such that the jth

1 → Rn×n

1

12

column of f ( (cid:98)V ) is given by

[f ( (cid:98)V )]j =




(cid:80)

(cid:80)

(cid:13)
(cid:13)
(cid:13)

k∈[n]\{j} AjkYjk (cid:98)V (t−1)
k∈[n]\{j} AjkYjk (cid:98)V (t−1)

k

k

(cid:13)
(cid:13)
(cid:13)



(cid:98)V (t−1)

j

,

, (cid:80)

k∈[n]\{j} AjkYjk (cid:98)V (t−1)
k∈[n]\{j} AjkYjk (cid:98)V (t−1)

k

k

(cid:54)= 0,

= 0.

(cid:80)

Here, we use the notation Rn×n
norms. For each j ∈ [n], deﬁne the random variable

1

for set of n × n real matrices whose columns all have unit

Uj =

σ
(n − 1)p

(cid:88)

z∗
kAjkWjk.

k∈[n]\{j}

The following lemma characterizes the evolution of the loss (cid:96)(V, z∗) through the map f .

Lemma 4.1. Assume np
Then, for any γ ∈ [0, 1/16), we have

σ2 ≥ c2 and np

log n ≥ c2 for some suﬃciently large constants c1, c2 > 0.


(cid:96)(f (V ), z∗) ≤

P

(cid:96)(V, z∗) +

1
2

4
n

n
(cid:88)

j=1

≥ 1 − 2n−9,

I{|Uj| > 1 − δ} for all V ∈ Rn×n

1

such that (cid:96)(V, z∗) ≤ γ





where δ = C

(cid:16)√

γ +

(cid:113) log n+σ2
np

(cid:17)

for some constant C > 0.

Lemma 4.1 immediately implies that for any (cid:98)V that satisﬁes the ﬁxed-point equation

(cid:98)V = f ( (cid:98)V ) and the crude error bound (cid:96)( (cid:98)V , z∗) ≤ γ < 1/16, we have

(cid:96)( (cid:98)V , z∗) ≤

8
n

n
(cid:88)

j=1

I{|Uj| > 1 − δ},

(27)

with high probability. The property of the random variable 8
n
easily analyzed, and we present the following lemma.

(cid:80)n

j=1

I{|Uj| > 1 − δ} can be

Lemma 4.2. Assume np
Then, for any δ ∈ (0, 1), we have

σ2 ≥ c1 and np

log n ≥ c2 for some suﬃciently large constants c1, c2 > 0.

8
n

n
(cid:88)

j=1

I{|Uj| > 1 − δ} ≤ exp

(cid:16)

−(1 − δ(cid:48))

(cid:17)

,

np
2σ2

(cid:16)

δ +

(cid:17)

(cid:113) log n
np

for some constant

with probability at least 1 − exp
C > 0. If we additionally assume (1 − δ(cid:48)) np

−

− n−9, where δ(cid:48) = C
2σ2 > log n, then

(cid:16)

(cid:17)

(cid:113) np
σ2

8
n

n
(cid:88)

j=1

I{|Uj| > 1 − δ} = 0,

with probability at least 1 − exp

(cid:16)

(cid:113) np
σ2

−

(cid:17)

− n−9.

13

We also need a lemma to establish a crude error bound for (cid:96)( (cid:98)V , z∗).

Lemma 4.3. Assume np
of the SDP (25). Then, there exits some constant C > 0 such that

log n ≥ c for some constant c > 0. Let (cid:98)Z = (cid:98)V T (cid:98)V be a global maximizer

(cid:96)( (cid:98)V , z∗) ≤ C

(cid:115)

σ2 + 1
np

,

with probability at least 1 − n−9.

The results of Lemma 4.1, Lemma 4.2 and Lemma 4.3 immediately imply the statistical

optimality of the SDP (25).

σ2 ≥ c1 and np

Theorem 4.2. Assume np
log n ≥ c2 for some suﬃciently large constants c1, c2 > 0.
Let (cid:98)Z = (cid:98)V T (cid:98)V be a global maximizer of the SDP (25) and u ∈ Rn be the leading eigenvector
of (cid:98)Z. Deﬁne (cid:98)z with each entry (cid:98)zj = uj/|uj|. If uj = 0 we can take (cid:98)zj = 1. Then, there exists
some δ = C

4 for some constant C > 0, such that

(cid:17) 1

(cid:16) log n+σ2
np

(cid:96)( (cid:98)V , z∗) ≤ exp

1
n2 (cid:107) (cid:98)Z − z∗z∗T(cid:107)2

F ≤ exp

(cid:96)((cid:98)z, z∗) ≤ exp
(cid:17)
(cid:113) np
σ2

(cid:16)

(cid:16)

(cid:16)

(cid:16)

−(1 − δ)

−(1 − δ)

−(1 − δ)

(cid:17)

(cid:17)

(cid:17)

,

,

,

np
2σ2
np
2σ2
np
2σ2

with probability at least 1 − exp
(1 − (cid:15)) np
matrix that satisﬁes (cid:98)Z = z∗z∗T with probability at least 1 − exp

−

2 log n for some arbitrarily small constant (cid:15) > 0, the SDP solution (cid:98)Z is a rank-one

− 2n−9. Moreover, if we additionally assume σ2 <

(cid:16)

(cid:113) np
σ2

−

(cid:17)

− 2n−9.

While the ﬁrst conclusion of the theorem is a direct consequence of Lemma 4.1, the second

conclusion can be derived from the inequality

1
n2 (cid:107) (cid:98)V T (cid:98)V − z∗z∗T(cid:107)2

F ≤ 2(cid:96)( (cid:98)V , z∗),

which is established by Lemma 5.6 in Section 5.1. The result for the loss (cid:96)((cid:98)z, z∗) is resulted
from a matrix perturbation bound [11].

Theorem 4.2 has established the minimax optimality of the SDP (25) for Z2 synchroniza-
tion in view of the matching lower bound results in Theorem 4.1. The special case p = 1
recovers the results of [13]. Moreover, under the condition σ2 < (1−(cid:15)) np
2 log n , we show that the
SDP solution (cid:98)Z is exactly rank-one and therefore rounding through the leading eigenvector is
not needed. This result generalizes the exact recovery threshold of Z2 synchronization when
p = 1 [1, 4, 5]. The phenomenon that SDP can achieve exact recovery has also been revealed
in community detection under stochastic block models [3, 10, 18, 19, 22, 25].

We shall compare Theorem 4.2 to Theorem 2.2 and Theorem 2.3. Though the two SDPs
(25) and (13) have the same type of constraints, the diﬀerence of the domain implies two

14

(cid:1) and (1 + o(1)) σ2

types of convergence rates exp (cid:0)−(1 − o(1)) np
2np . It is quite surprising that
2σ2
the SDP (25), a continuous optimization problem, is able to achieve an exponential rate,
which is typical for a discrete problem. The adaptation of the SDP (25) to the discrete
structure is a consequence of the fact that both (25) and (26) are optimization problems over
Rn×n. We make this eﬀect explicit by bounding the statistical error by the random variable
8
n

j=1
To close this section, we brieﬂy discuss the implications of Lemma 4.1 on the MLE (24)

I{|Uj| > 1 − δ} in Lemma 4.1.

(cid:80)n

and the generalized power method deﬁned by the iteration procedure

z(t)
j =

k∈[n]\{j} AjkYjkz(t−1)
k∈[n]\{j} AjkYjkz(t−1)

k

k






(cid:80)
(cid:12)
(cid:80)
(cid:12)
(cid:12)
z(t−1)
j

,

(cid:12)
(cid:12)
(cid:12)

, (cid:80)

k∈[n]\{j} AjkYjkz(t−1)
k∈[n]\{j} AjkYjkz(t−1)

k

k

(cid:80)

(cid:54)= 0,

= 0.

(28)

We note that the iteration (28) is real-valued so that we always have z(t)
j ∈ {−1, 1}, which
makes it diﬀerent from (20). The statistical optimality of the generalized power method
(28) has been established by [15] for Z2 synchronization when p = 1. Following the same
argument in Section 3, we can embed both MLE and GPM into Rn×n
, and thus Lemma 4.1
also implies that both MLE and GPM achieve the optimal rate exp (cid:0)−(1 − o(1)) np
(cid:1) for a
2σ2
general p as well. Just as what we have for phase synchronization, the analyses of MLE,
GPM, and SDP for Z2 synchronization are all based on Lemma 4.1, and thus we have uniﬁed
the three diﬀerent methods from an iterative algorithm perspective.

1

5 Proofs

This section presents the proofs of all technical results in the paper. We ﬁrst list some
auxiliary lemmas in Section 5.1. The key lemmas of the SDP analyses, Lemma 2.1 and
Lemma 4.1, are proved in Section 5.2 and Section 5.3, respectively. We then prove the main
results including Theorem 2.2, Theorem 2.3 and Theorem 4.2 in Section 5.4. Theorem 4.1
is proved in Section 5.5. Finally, the proofs of Lemma 2.2, Lemma 4.3 and Lemma 4.2 are
given in Section 5.6.

5.1 Some Auxiliary Lemmas

Lemma 5.1. Assume np

log n → ∞. Then, there exists a constant C > 0, such that





max
j∈[n]


2

(cid:88)

(Ajk − p)



≤ Cnp log n,

k∈[n]\{j}

and

(cid:107)A − EA(cid:107)op ≤ C

√

np,

with probability at least 1 − n−10.

15

Proof. The ﬁrst result is a direct application of union bound and Bernstein’s inequality. The
second result is Theorem 5.2 of [21].

The following result is essentially Corollary 3.11 of [7]. The speciﬁc form that we need is

from Lemma 5.2 in [16].

Lemma 5.2 (Corollary 3.11 of [7]). Assume np
such that

log n → ∞. Then, there exists a constant C > 0,

(cid:107)A ◦ W (cid:107)op ≤ C

√

np,

with probability at least 1 − n−10. The result holds for both complex W in Section 2 and real
W in Section 4.

np
Lemma 5.3 (Lemma 5.3 of [16]). Assume
log n > c for some suﬃciently large constant
c > 0. Consider independent random variables Xjk ∼ N (0, 1) for 1 ≤ j < k ≤ n. Assume
Xkj = Xjk for 1 ≤ j < k ≤ n. Then, we have

n
(cid:88)





(cid:88)

j=1

k∈[n]\{j}



2

AjkXjk



≤ n(n − 1)p + C

(cid:112)

n3p2 log n,

with probability at least 1 − n−10. The same result holds if Xkj = −Xjk is assumed instead
for 1 ≤ j < k ≤ n.

Lemma 5.4 (Lemma 13 of [14]). Consider independent random variables Xj ∼ N (0, 1) and
Ej ∼ Bernoulli(p). Then,

P





(cid:12)
(cid:12)
n
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=1

XjEj/p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



(cid:18)

> t

 ≤ 2 exp

− min

(cid:18) pt2
16n

,

pt
2

(cid:19)(cid:19)

,

for any t > 0.

Lemma 5.5. The following three statements hold:

1. For any x, y ∈ Cn such that (cid:107)y(cid:107) = 1 and Re(yHx) > 0, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x
(cid:107)x(cid:107)

− y

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

≤

(cid:107)(In − yyH)x(cid:107)2 + |Im(yHx)|2
|Re(yHx)|2

.

2. For any x, y ∈ Rn such that (cid:107)y(cid:107) = 1 and yTx > 0, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x
(cid:107)x(cid:107)

− y

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

≤

(cid:107)(In − yyT)x(cid:107)2
|yTx|2

.

3. For any x ∈ C such that Re(x) > 0, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x
|x|

− 1

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

≤

|Im(x)|2
|Re(x)|2 .

16

Proof. It is easy to see that the last two statements are special cases of the ﬁrst one. Thus,
we only need to prove the ﬁrst statement. Note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

x
(cid:107)x(cid:107)

− y

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

=

=

=

− y

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(In − yyH)x + (yHx)y
(cid:107)x(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:107)(In − yyH)x(cid:107)2 + |yHx − (cid:107)x(cid:107)|2
(cid:107)x(cid:107)2
a2 + b2(cid:17)2

b2 +

a −

√

(cid:16)

a2 + b2

,

where a = Re(yHx) > 0 and b = (cid:112)|Im(yHx)|2 + (cid:107)(In − yyH)x(cid:107)2. Since

(cid:16)

a −

√

a2 + b2(cid:17)2

b2 +

a2 + b2

=

2b2
a2 + b2 + a

√

a2 + b2

≤

b2
a2 ,

the proof is complete.

Lemma 5.6. For any V = (V1, · · · , Vn) ∈ Cn×n and any z ∈ Cn
j ∈ [n], we have

1 such that (cid:107)Vj(cid:107) = 1 for all

n−2(cid:107)V HV − zzH(cid:107)2

F ≤ 2(cid:96)(V, z).

For any V = (V1, · · · , Vn) ∈ Rn×n and any z ∈ {−1, 1}n such that (cid:107)Vj(cid:107) = 1 for all j ∈ [n],
we have

n−2(cid:107)V TV − zzT(cid:107)2

F ≤ 2(cid:96)(V, z).

Proof. We only prove the complex version of the inequality. The real version follows the same
argument. By deﬁnition, we have



(cid:96)(V, z) =

2 − max

a∈Cn:(cid:107)a(cid:107)2=1


aH





1
n

n
(cid:88)

j=1





zjVj

 +



1
n

n
(cid:88)

j=1


H





zjVj



a







= 2

1 −

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

j=1

zjVj



(cid:13)
(cid:13)
(cid:13)
 .
(cid:13)
(cid:13)
(cid:13)

Then,

n−2(cid:107)V HV − zzH(cid:107)2

F =

≤

1
n2

1
n2

n
(cid:88)

n
(cid:88)

j=1
n
(cid:88)

l=1
n
(cid:88)

|V H

j Vl − zj ¯zl|2

(cid:0)2 − V H

j Vl ¯zjzl − V H

l Vjzj ¯zl

(cid:1)

j=1



= 2

1 −

l=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

j=1

zjVj

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

 .

Therefore, n−2(cid:107)V HV − zzH(cid:107)2

F ≤ (cid:96)(V, z) (cid:0)2 − 1

2 (cid:96)(V, z)(cid:1) ≤ 2(cid:96)(V, z), and the proof is complete.

17

5.2 Proof of Lemma 2.1

We organize the proof into four steps. We ﬁrst list a few high-probability events in Step 1.
These events are assumed to be true in later steps. Step 2 provides an error decomposition
of (cid:96)(f (V ), z∗), and then each error term in the decomposition will be analyzed and bounded
in Step 3. Finally, we combine the bounds and derive the desired result in Step 4.

Step 1: Some high-probability events. By Lemma 5.1, Lemma 5.2, and Lemma 5.3,
we know that

(cid:88)

Ajk ≥ (n − 1)p − C(cid:112)np log n,

k∈[n]\{j}
(cid:88)

Ajk ≤ (n − 1)p + C(cid:112)np log n,

min
j∈[n]

max
j∈[n]

√

√

np,

np,

k∈[n]\{j}
(cid:107)A − EA(cid:107)op ≤ C
(cid:107)A ◦ W (cid:107)op ≤ C
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

kz∗
j )

kz∗
j )

≤

≤

n2p
2

n2p
2

AjkIm( ¯Wjk ¯z∗

AjkRe( ¯Wjk ¯z∗

n
(cid:88)

j=1

n
(cid:88)

j=1

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}

(cid:88)

(cid:32)

(cid:32)

1 + C

1 + C

(cid:114)

(cid:114)

(cid:33)

(cid:33)

,

,

log n
n

log n
n

(29)

(30)

(31)

(32)

(33)

(34)

√

√

2Im( ¯Wjk ¯z∗

all hold with probability at least 1 − n−9 for some constant C > 0. To establish (33)-
kz∗
kz∗
j ) are all independently standard normally
(34), note that
distributed for 1 ≤ j < k ≤ n. We also have −Im( ¯Wjk ¯z∗
j z∗
k)
and Re( ¯Wjk ¯z∗

j z∗
In addition to (29)-(34), we need another high-probability inequality. For a suﬃciently
is suﬃciently large, we want to upper bound the random variable

j ) = Im(Wjkz∗
k) for any 1 ≤ j < k ≤ n.

j ) = Im( ¯Wkj ¯z∗

j ) = Re( ¯Wkj ¯z∗

j ) = Re(Wjkz∗

2Re( ¯Wjk ¯z∗

kz∗

k ¯z∗

k ¯z∗

kz∗

j ),

k∈[n]\{j} Ajk ¯Wjk ¯z∗

. The existence of such ρ is guaranteed by the con-
σ2 is suﬃciently large, and the speciﬁc choice will be given later. We ﬁrst bound its

k

(cid:12)
(cid:111)
(cid:12)
(cid:12) > ρ

I

(cid:110) 2σ
np

small ρ such that ρ2np
σ2
(cid:12)
(cid:80)n
(cid:12)
(cid:12)
j=1
dition np
expectation by Lemma 5.4,

(cid:80)

n
(cid:88)

P

j=1






2σ
np

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

Ajk ¯Wjk ¯z∗
k

k∈[n]\{j}




> ρ



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

n
(cid:88)

P

j=1






2σ
np

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}

(cid:12)
(cid:12)
(cid:12)
AjkRe( ¯Wjk ¯z∗
k)
(cid:12)
(cid:12)
(cid:12)






ρ
2

>




n
(cid:88)

P

+

j=1


(cid:18)

2σ
np

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}
(cid:19)
ρ2np
256σ2

AjkIm( ¯Wjk ¯z∗
k)

+ 4n exp

(cid:16)

−

ρnp
8σ






ρ
2

>

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:17)

.

≤ 4n exp

−

18

By Markov inequality, we have

n
(cid:88)

I

j=1






2σ
np

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}

Ajk ¯Wjk ¯z∗
k




> ρ



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

4σ2
ρ2p

(cid:32)

exp

−

(cid:114)

(cid:33)

,

ρ2np
σ2

1
16

(35)

with probability at least
(cid:32)

1 −

≥ 1 −

ρ2pn
σ2

2ρ2pn
σ2

(cid:32)

(cid:32)

exp

−

(cid:32)

exp

−

ρ2np
256σ2 +
(cid:114)

1
16

ρ2np
σ2

(cid:114)

(cid:33)

.

ρ2np
σ2

1
32

≥ 1 − exp

−

(cid:114)

ρ2np
σ2

(cid:33)

(cid:32)

+ exp

−

ρnp
8σ

+

1
16

(cid:114)

ρ2np
σ2

(cid:33)(cid:33)

1
16
(cid:33)

Finally, we conclude that the events (29)-(35) hold simultaneously with probability at least

1 − n−9 − exp

(cid:18)

(cid:113) ρ2np
σ2

(cid:19)
.

− 1
32

Step 2: Error decomposition. For any V ∈ Cn×n
(cid:101)V ∈ Cn×n such that

1

such that (cid:96)(V, z∗) ≤ γ, we can deﬁne

(cid:101)Vj =

(cid:80)

k∈[n]\{j} Ajk ¯YjkVk
(cid:80)
k∈[n]\{j} Ajk

,

for each j ∈ [n]. Denote (cid:98)V = f (V ) then (cid:98)Vj = (cid:101)Vj/(cid:107) (cid:101)Vj(cid:107) for each coordinate such that (cid:101)Vj (cid:54)= 0.
The condition (cid:96)(V, z∗) ≤ γ implies there exists some b ∈ Cn such that (cid:107)b(cid:107) = 1 and

n(cid:96)(V, z∗) = (cid:107)V − bz∗H(cid:107)2

F ≤ γn. By direct calculation, we can write
(cid:80)
k(Vk − ¯z∗

k∈[n]\{j} Ajkz∗

σz∗

kb)

+

j b (cid:80)
(cid:80)

k∈[n]\{j} Ajk ¯Wjk ¯z∗
k∈[n]\{j} Ajk

k

z∗
j (cid:101)Vj = b +

(cid:80)

k∈[n]\{j} Ajk

(cid:80)

σz∗
j

+

k∈[n]\{j} Ajk ¯Wjk(Vk − ¯z∗
k∈[n]\{j} Ajk

(cid:80)

kb)

= b +

1
n − 1

k(Vk − ¯z∗
z∗

kb) −

1
n − 1

j (Vj − ¯z∗
z∗

j b)

n
(cid:88)

k=1

(cid:32) (cid:80)

k∈[n]\{j} Ajkz∗

k(Vk − ¯z∗

kb)

+

+

σz∗

j b (cid:80)
(cid:80)

(cid:80)

k∈[n]\{j} Ajk
k∈[n]\{j} Ajk ¯Wjk ¯z∗
k∈[n]\{j} Ajk
(cid:80)n

k

k=1 z∗

−

1
n − 1
(cid:80)

σz∗
j

+

(cid:33)

k(Vk − ¯z∗
z∗

kb)

n
(cid:88)

k=1

k∈[n]\{j} Ajk ¯Wjk(Vk − ¯z∗
k∈[n]\{j} Ajk

(cid:80)

kb)

.

Now we deﬁne a0 = b + 1
n−1

kb) and a = a0/(cid:107)a0(cid:107), and we have

z∗
j aH (cid:101)Vj = (cid:107)a0(cid:107) −

j aH(Vj − ¯z∗
z∗

j b) + aHFj + aHbGj + aHHj,

(cid:107)(In − aaH) (cid:101)Vj(cid:107) ≤

1
n − 1

j b(cid:107) + (cid:107)Fj(cid:107) + (cid:107)(In − aaH)b(cid:107)|Gj| + (cid:107)Hj(cid:107),

(36)

(37)

k(Vk − ¯z∗
1
n − 1
(cid:107)Vj − ¯z∗

19

where

Fj =

Gj =

Hj =

−

1
n − 1

n
(cid:88)

k=1

k(Vk − ¯z∗
z∗

kb),

(cid:80)

k∈[n]\{j} Ajkz∗

k(Vk − ¯z∗

kb)

σz∗
j

σz∗
j

(cid:80)

(cid:80)

(cid:80)

k∈[n]\{j} Ajk
k∈[n]\{j} Ajk ¯Wjk ¯z∗
(cid:80)
k∈[n]\{j} Ajk
k∈[n]\{j} Ajk ¯Wjk(Vk − ¯z∗
k∈[n]\{j} Ajk

(cid:80)

k

,

kb)

.

By Lemma 5.5, we have the bound

(cid:107) (cid:98)Vj − ¯z∗

j a(cid:107)2 ≤

(cid:107)(In − aaH) (cid:101)Vj(cid:107)2 + |Im(z∗
|Re(z∗

j aH (cid:101)Vj)|2

j aH (cid:101)Vj)|2

,

(38)

whenever Re(z∗

j aH (cid:101)Vj) > 0 holds. Since

(cid:107)a0 − b(cid:107) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n − 1

n
(cid:88)

k=1

k(Vk − ¯z∗
z∗

(cid:13)
(cid:13)
(cid:13)
kb)
(cid:13)
(cid:13)

≤

√

1
n − 1

n(cid:107)V − bz∗H(cid:107)F ≤

√

n
n − 1

√

γ ≤ 2

γ,

we have (cid:107)a − b(cid:107) ≤ 2(cid:107)a0 − b(cid:107) ≤ 4

√

γ. Therefore,

(cid:107)a0(cid:107) ≥ (cid:107)b(cid:107) − (cid:107)a0 − b(cid:107) ≥ 1 − 2
|aHb − 1| = |(a − b)Hb| ≤ (cid:107)a − b(cid:107) ≤ 4
√

γ,
√

(cid:107)(In − aaH)b(cid:107) ≤ (cid:107)a − b(cid:107) + |aHb − 1| ≤ 8

γ.

γ,

√

We also have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n − 1

j aH(Vj − ¯z∗
z∗

j b)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
n − 1

(cid:107)Vj − ¯z∗

j b(cid:107) ≤

√

γn
n − 1

.

Therefore, as long as (cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) ≤ ρ, we have

(cid:16)

Re

z∗
j aH (cid:101)Vj

(cid:17)

√

≥ 1 − 2

γ −

√

γn
n − 1

− 3ρ ≥ 1 − 3(

√

γ + ρ) > 0,

(39)

(40)

(41)

(42)

(43)

where we have used (36), (39), and (42), and the last inequality is due to the assumption
that γ < 1/16 and ρ is suﬃciently small. Hence, the event { (cid:101)Vj = 0} is included in the event
{(cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) > ρ}.

20

By (38), we obtain the bound

(cid:107) (cid:98)Vj − ¯z∗

j a(cid:107)2 ≤

≤

≤

(cid:107)(In − aaH) (cid:101)Vj(cid:107)2 + |Im(z∗
|Re(z∗

j aH (cid:101)Vj)|2

j aH (cid:101)Vj)|2

I{(cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) ≤ ρ}

+4I{(cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) > ρ}
(cid:16) 1
√
n−1 (cid:107)Vj − ¯z∗

j b(cid:107) + (cid:107)Fj(cid:107) + 8
(1 − 3(

√

γ + ρ))2

γ|Gj| + (cid:107)Hj(cid:107)

(cid:17)2

(cid:16) 1
n−1 (cid:107)Vj − ¯z∗

j b(cid:107) + (cid:107)Fj(cid:107) + |Im(aHbGj)| + (cid:107)Hj(cid:107)
γ + ρ))2

(1 − 3(

√

+

(cid:17)2

+4I{(cid:107)Fj(cid:107) > ρ} + 4I{|Gj| > ρ} + 4I{(cid:107)Hj(cid:107) > ρ}
(1 + η)|Im(aHbGj)|2 + 256γ|Gj|2
√

(7 + 4η−1)

+

(1 − 3(

γ + ρ))2
(cid:16) 1
(n−1)2 (cid:107)Vj − ¯z∗
(1 − 3(

√

j b(cid:107)2 + (cid:107)Fj(cid:107)2 + (cid:107)Hj(cid:107)2(cid:17)
γ + ρ))2

+4I{(cid:107)Fj(cid:107) > ρ} + 4I{|Gj| > ρ} + 4I{(cid:107)Hj(cid:107) > ρ},

for some η to be speciﬁed later. The last inequality above is due to Jensen’s inequality.

Step 3: Analysis of each error term. Next, we will analyze the error terms Fj, Hj and
Gj separately. By triangle inequality, (29) and (30), we have

(cid:80)

(cid:107)Fj(cid:107) ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k∈[n]\{j}(Ajk − p)z∗
k∈[n]\{j} Ajk

(cid:80)

k(Vk − ¯z∗

kb)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

p

(cid:88)

k∈[n]\{j}

k(Vk − ¯z∗
z∗

(cid:13)
(cid:13)
(cid:13)
kb)
(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
k∈[n]\{j} Ajk

(cid:80)

−

1
(n − 1)p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≤

2
np

2
np

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k∈[n]\{j}

(cid:88)

(cid:88)

k∈[n]\{j}

(Ajk − p)z∗

k(Vk − ¯z∗

(Ajk − p)z∗

k(Vk − ¯z∗

√

+ p

n(cid:107)V − bz∗H(cid:107)F

(cid:12)
(cid:80)
(cid:12)
(cid:12)

2

k∈[n]\{j}(Ajk − p)

(cid:12)
(cid:12)
(cid:12)

n2p2

√

p log n
np

+ C1

(cid:107)V − bz∗H(cid:107)F.

(cid:13)
(cid:13)
(cid:13)
kb)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
kb)
(cid:13)
(cid:13)
(cid:13)

Using (31), we have

n
(cid:88)

j=1

(cid:107)Fj(cid:107)2 ≤

≤

(cid:88)

n
(cid:88)

8
n2p2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
8
n2p2 (cid:107)A − EA(cid:107)2

j=1

k∈[n]\{j}

(Ajk − p)z∗

k(Vk − ¯z∗

op(cid:107)V − bz∗H(cid:107)2

F + 2C2
1

kb)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
log n
np

+ 2C2
1

log n
np

(cid:107)V − bz∗H(cid:107)2
F

(cid:107)V − bz∗H(cid:107)2
F

(44)

≤ C2

log n
np

(cid:107)V − bz∗H(cid:107)2
F.

21

The above bound also implies

n
(cid:88)

j=1

I{(cid:107)Fj(cid:107) > ρ} ≤ ρ−2

n
(cid:88)

j=1

(cid:107)Fj(cid:107)2 ≤

C2
ρ2

log n
np

(cid:107)V − bz∗H(cid:107)2
F.

Similarly, we can also bound the error terms that depend on Hj. By (29) and (32), we have

n
(cid:88)

j=1

(cid:107)Hj(cid:107)2 ≤

2σ2
n2p2

n
(cid:88)

j=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

Ajk ¯Wjk(Vk − ¯z∗

kb)

k∈[n]\{j}

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

≤

2σ2
n2p2 (cid:107)(V − bz∗H)(A ◦ W )H(cid:107)2
2σ2
op(cid:107)V − bz∗H(cid:107)2
n2p2 (cid:107)A ◦ W (cid:107)2
F

F

≤ C3

σ2
np

(cid:107)V − bz∗H(cid:107)2
F,

and thus

n
(cid:88)

j=1

I{(cid:107)Hj(cid:107) > ρ} ≤ ρ−2

n
(cid:88)

j=1

(cid:107)Hj(cid:107)2 ≤

C3
ρ2

σ2
np

(cid:107)V − bz∗H(cid:107)2
F.

For the contribution of Gj, we use (29) and (35), and have

I{|Gj| > ρ} ≤

n
(cid:88)

I

n
(cid:88)

j=1

Ajk ¯Wjk ¯z∗
k




> ρ



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)






2σ
np

(cid:32)

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}
(cid:114)

exp

−

1
16

(cid:33)

.

ρ2np
σ2

j=1

4σ2
ρ2p

≤

Next, we study the main error term |Im(aHbGj)|2. By (29), we have

(45)

(46)

2

n
(cid:88)

j=1

|Im(aHbGj)|2 ≤

1 + C4

(cid:32)

(cid:115)

(cid:33)2

log n
np

σ2
n2p2

n
(cid:88)

j=1

(cid:32)

(cid:115)

≤ (1 + η)

1 + C4

(cid:33)2

log n
np

σ2
n2p2

(cid:32)

(cid:115)

+(1 + η−1)

1 + C4

log n
np

(cid:33)2

2

(cid:12)
(cid:12)
(cid:12)
kaHb)
(cid:12)
(cid:12)
(cid:12)

AjkIm( ¯Wjkz∗

j ¯z∗

(cid:88)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}

σ2
n2p2

n
(cid:88)

(cid:88)

(cid:88)

j=1

j=1

AjkIm( ¯Wjkz∗

(cid:12)
(cid:12)
(cid:12)
j ¯z∗
k)
(cid:12)
(cid:12)
(cid:12)

AjkRe( ¯Wjkz∗

j ¯z∗
k)

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Im(aHb)|2.

By (40), we have

|Im(aHb)| = |Im(aHb − 1)| ≤ |aHb − 1| ≤ 4

√

γ.

Together with (33) and (34), we have

(cid:32)

(cid:32)

|Im(aHbGj)|2 ≤

1 + C5

η + η−1γ +

(cid:33)(cid:33)

(cid:115)

log n
np

σ2
2p

.

n
(cid:88)

j=1

(47)

22

We also have

by (33) and (34).

n
(cid:88)

j=1

|Gj|2 ≤

2σ2
n2p2

n
(cid:88)

j=1

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}

Ajk ¯Wjkz∗

j ¯z∗
k

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ C6

σ2
p

,

(48)

Step 4: Combining the bounds. Plugging all the individual error bounds obtained in
Step 3 into the error decomposition in Step 2, we obtain

n(cid:96)( (cid:98)V , z∗) ≤

n
(cid:88)

j=1
(cid:32)

(cid:107) (cid:98)Vj − ¯z∗

j a(cid:107)2

(cid:32)

≤

1 + C7

ρ + η +

√

γ + η−1γ +

+

16σ2
ρ2p

(cid:32)

exp

−

(cid:33)

(cid:114)

1
16

ρ2np
σ2

+ C7

(cid:115)

(cid:33)(cid:33)

log n
np

σ2
2p
(cid:0)η−1 + ρ−2(cid:1) log n + σ2

np

n(cid:96)(V, z∗).

We set

(cid:115)

η =

γ +

log n + σ2
np

and ρ2 =

√

(cid:115)

32

log n + σ2
np

.

Then, since ρ2np
σ2

is suﬃciently large, we have

16σ2
ρ2p

(cid:32)

exp

−

(cid:33)

(cid:114)

1
16

ρ2np
σ2

≤

(cid:19)2

σ2
ρ2p

(cid:18) σ2
ρ2np

≤

σ2
p

(cid:115)

σ2
np

.

Therefore, we have

(cid:32)

(cid:96)( (cid:98)V , z∗) ≤

1 + C8

(cid:18)

γ2 +

log n + σ2
np

(cid:19)1/4(cid:33)

(cid:115)

σ2
2np

+ C8

log n + σ2
np

(cid:96)(V, z∗).

Since the above inequality is derived from the conditions (29)-(35) and (cid:96)(V, z∗) ≤ γ, it holds
uniformly over all V ∈ Cn×n
such that (cid:96)(V, z∗) ≤ γ with probability at least 1 − n−9 −
exp

. The proof is complete.

(cid:1)1/4(cid:17)

(cid:16)

1

− (cid:0) np
σ2

5.3 Proof of Lemma 4.1

Similar to the proof of Lemma 2.1, we organize the proof of Lemma 4.1 into four steps.

Step 1: Some high-probability events. We already know that (29), (30) and (31) hold
with probability at least 1 − n−9. We also have

(cid:107)A ◦ W (cid:107)op ≤ C

√

np,

(49)

23

with probability at least 1−n−9 by Lemma 5.2. Note that the matrix W in (49) is real-valued,
compared with the complex version of the bound (32). Another high-probability event we
need is for the random variable (cid:80)n
similar analysis that leads to (34), we can conclude that with probability at least 1 − n−9,

. By Lemma 5.3 and with a

k∈[n]\{j} AjkWjkz∗

(cid:12)
(cid:80)
(cid:12)
(cid:12)

j z∗
k

j=1

(cid:12)
(cid:12)
(cid:12)

2

n
(cid:88)

j=1

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}

AjkWjkz∗

j z∗
k

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32)

≤ n2p

1 + C

(cid:114)

(cid:33)

.

log n
n

(50)

In the end, we conclude that the events (29), (30), (31), (49) and (50) hold simultaneously
with probability at least 1 − 2n−9.

Step 2: Error decomposition. For any V ∈ Rn×n
(cid:98)V = f (V ) with each column (cid:98)Vj = (cid:101)Vj/(cid:107) (cid:101)Vj(cid:107), where

1

such that (cid:96)(V, z∗) ≤ γ, we can write

(cid:80)

(cid:101)Vj =

k∈[n]\{j} AjkYjkVk
(cid:80)
k∈[n]\{j} Ajk

.

The condition (cid:96)(V, z∗) ≤ γ implies there exists some b ∈ Rn such that (cid:107)b(cid:107) = 1 and n(cid:96)(V, z∗) =
(cid:107)V − bz∗T(cid:107)2

F ≤ γn. By direct calculation, we can write

z∗
j (cid:101)Vj = b +

1
n − 1

k(Vk − z∗
z∗

kb) −

1
n − 1

j (Vj − z∗
z∗

j b)

n
(cid:88)

k=1

(cid:32) (cid:80)

k∈[n]\{j} Ajkz∗

k(Vk − z∗

kb)

+

+

(cid:80)

k∈[n]\{j} Ajk
k∈[n]\{j} AjkWjkz∗
k
k∈[n]\{j} Ajk

σz∗

j b (cid:80)
(cid:80)

(cid:33)

k(Vk − z∗
z∗

kb)

n
(cid:88)

k=1

−

1
n − 1
(cid:80)

σz∗
j

+

k∈[n]\{j} AjkWjk(Vk − z∗
k∈[n]\{j} Ajk

(cid:80)

kb)

.

Now we deﬁne a0 = b + 1
n−1

(cid:80)n

k=1 z∗

k(Vk − z∗

kb) and a = a0/(cid:107)a0(cid:107), and we have

(cid:107)(In − aaT) (cid:101)Vj(cid:107) ≤

1
n − 1

where

1
n − 1
(cid:107)Vj − z∗

z∗
j aT (cid:101)Vj = (cid:107)a0(cid:107) −

j aT(Vj − z∗
z∗

j b) + aTFj + aTbGj + aTHj,

j b(cid:107) + (cid:107)Fj(cid:107) + (cid:107)(In − aaT)b(cid:107)|Gj| + (cid:107)Hj(cid:107),

Fj =

Gj =

Hj =

−

1
n − 1

n
(cid:88)

k=1

k(Vk − z∗
z∗

kb),

(cid:80)

k∈[n]\{j} Ajkz∗

k(Vk − z∗

kb)

σz∗
j

σz∗
j

(cid:80)

(cid:80)

(cid:80)

k∈[n]\{j} Ajk
k∈[n]\{j} AjkWjkz∗
k
(cid:80)
k∈[n]\{j} Ajk
k∈[n]\{j} AjkWjk(Vk − z∗
k∈[n]\{j} Ajk

(cid:80)

,

kb)

.

24

By Lemma 5.5, we have the bound

(cid:107) (cid:98)Vj − z∗

j a(cid:107)2 ≤

(cid:107)(In − aaT) (cid:101)Vj(cid:107)2
j aT (cid:101)Vj|2

|z∗

,

whenever z∗

j aT (cid:101)Vj > 0 holds. Since

(cid:107)a0 − b(cid:107) =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n − 1

n
(cid:88)

k=1

k(Vk − z∗
z∗

(cid:13)
(cid:13)
(cid:13)
kb)
(cid:13)
(cid:13)

≤

√

1
n − 1

n(cid:107)V − bz∗T(cid:107)F ≤

2
√
n

(cid:107)V − bz∗T(cid:107)F,

we have (cid:107)a − b(cid:107) ≤ 2(cid:107)a0 − b(cid:107) ≤ 4√

n (cid:107)V − bz∗T(cid:107)F. Therefore,

(cid:107)a0(cid:107) ≥ (cid:107)b(cid:107) − (cid:107)a0 − b(cid:107) ≥ 1 −

2
√
n

(cid:107)V − bz∗T(cid:107)F,

|aTb − 1| = |(a − b)Tb| ≤ (cid:107)a − b(cid:107) ≤

4
√
n

(cid:107)V − bz∗T(cid:107)F,

(cid:107)(In − aaT)b(cid:107) ≤ (cid:107)a − b(cid:107) + |aTb − 1| ≤

8
√
n

(cid:107)V − bz∗T(cid:107)F.

We also have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n − 1

j aT(Vj − z∗
z∗

(cid:12)
(cid:12)
j b)
(cid:12)
(cid:12)

≤

1
n − 1

(cid:107)Vj − z∗

j b(cid:107) ≤

1
n − 1

(cid:107)V − bz∗T(cid:107)F.

Therefore, as long as (cid:107)Fj(cid:107) ∨ (cid:107)Hj(cid:107) ≤ ρ and |Gj| ≤ 1 − 4ρ, we have

(51)

(52)

(53)

(54)

(55)

z∗
j aT (cid:101)Vj ≥ 1 −

(cid:107)V − bz∗T(cid:107)F −

2
√
n
√
γ − 2ρ − (1 − 4ρ)

1
n − 1

≥ 1 − 3

(cid:107)V − bz∗T(cid:107)F − (cid:107)Fj(cid:107) − |Gj| − (cid:107)Hj(cid:107)

≥ ρ,

where we have used (52) and (55), and we set ρ to satisfy ρ ≥ 3
γ. The speciﬁc choice of
ρ will be given later. Hence, the event { (cid:101)Vj = 0} is included in the event {(cid:107)Fj(cid:107) ∨ (cid:107)Hj(cid:107) >
ρ or |Gj| > 1 − 4ρ}.

√

25

By (51), we obtain the bound

(cid:107) (cid:98)Vj − z∗

j a(cid:107)2 ≤

(cid:107)(In − aaT) (cid:101)Vj(cid:107)2
j aT (cid:101)Vj|2

|z∗

I{(cid:107)Fj(cid:107) ∨ (cid:107)Hj(cid:107) ≤ ρ, |Gj| ≤ 1 − 4ρ}

≤

≤

≤

j b(cid:107) + (cid:107)Fj(cid:107) + (cid:107)(In − aaT)b(cid:107)|Gj| + (cid:107)Hj(cid:107)

n − 1

(cid:18) 1

(cid:107)Vj − z∗

+4I{(cid:107)Fj(cid:107) ∨ (cid:107)Hj(cid:107) > ρ or |Gj| > 1 − 4ρ}
1
ρ2
+4I{(cid:107)Fj(cid:107) > ρ} + 4I{|Gj| > 1 − 4ρ} + 4I{(cid:107)Hj(cid:107) > ρ}
4(cid:107)Vj − z∗
ρ2(n − 1)2 +
4(cid:107)Fj(cid:107)2

4(cid:107)(In − aaT)b(cid:107)2|Gj|2
ρ2

ρ2 +

4(cid:107)Hj(cid:107)2

4(cid:107)Fj(cid:107)2

j b(cid:107)2

+

4(cid:107)Hj(cid:107)2
ρ2

(cid:19)2

ρ2 + 4I{|Gj| > 1 − 4ρ}
256(cid:107)V − bz∗T(cid:107)2
8(cid:107)Fj(cid:107)2

+

ρ2 +
j b(cid:107)2
4(cid:107)Vj − z∗
ρ2(n − 1)2 +
+4I{|Gj| > 1 − 4ρ}.

ρ2 +

F|Gj|2

+

8(cid:107)Hj(cid:107)2
ρ2

nρ2

We have used (54), Jensen’s inequality and Markov’s inequality in the above derivation.

Step 3: Analysis of each error term. Next, we will analyze the error terms Fj, Hj and
Gj separately. Following the same analysis that leads to (44), (45) and (48), we have

n
(cid:88)

(cid:107)Fj(cid:107)2 ≤ C1

j=1
n
(cid:88)

(cid:107)Hj(cid:107)2 ≤ C2

log n
np

(cid:107)V − bz∗H(cid:107)2
F,

σ2
np

(cid:107)V − bz∗H(cid:107)2
F,

j=1
n
(cid:88)

j=1

|Gj|2 ≤ C3

σ2
p

.

Note that the above three bounds are based on the events (29), (30), (31), (49) and (50).

Step 4: Combining the bounds. Plugging all the individual error bounds obtained in
Step 3 into the error decomposition in Step 2, we obtain

n(cid:96)( (cid:98)V , z∗) ≤

n
(cid:88)

(cid:107) (cid:98)Vj − z∗

j a(cid:107)2

j=1
(cid:18)

4
ρ2(n − 1)2 +

≤

Set

8C1 log n + (8C2 + 256C3)σ2
ρ2np

(cid:19)

n(cid:96)(V, z∗) + 4

n
(cid:88)

j=1

I{|Gj| > 1 − 4ρ}.

ρ2 =

(cid:18)

C4

log n + σ2
np

(cid:19)

∨ (3γ) ,

26

for some suﬃciently large constant C4 such that
we have

4

ρ2(n−1)2 + 8C1 log n+(8C2+256C3)σ2

ρ2np

≤ 1

2 . Then,

(cid:96)( (cid:98)V , z∗) ≤

≤

(cid:96)(V, z∗) +

(cid:96)(V, z∗) +

1
2

1
2

4
n

4
n

n
(cid:88)

j=1

n
(cid:88)

j=1

I{|Gj| > 1 − 4ρ}






I

σ
(n − 1)p

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n]\{j}

z∗
kAjkWjk

(cid:32)

(cid:115)

>

1 − C5

log n + σ2
np

− (cid:112)3γ

(cid:33)




,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the last inequality is by (29) and (30). Note that log n+σ2

is suﬃciently small and

np

γ < 1/16, we have δ < 1 with δ = C5
3γ. Since the above about is derived from
np +
the conditions (29), (30), (31), (49) and (50) and (cid:96)(V, z∗) ≤ γ, it holds uniformly over all
V ∈ Rn×n
such that (cid:96)(V, z∗) ≤ γ with probability at least 1 − 2n−9. The proof is complete.

1

(cid:113) log n+σ2

√

5.4 Proofs of Theorem 2.2, Theorem 2.3, and Theorem 4.2

Proof of Theorem 2.2. We obtain (19) as a consequence of Lemma 2.1 and Lemma 2.2, which
immediately implies the ﬁrst conclusion. The second conclusion is a consequence of Lemma
5.6.

Proof of Theorem 2.3. By Theorem 2.2, we have (cid:107) (cid:98)V − bz∗H(cid:107)2
p with high probability for
some b ∈ Cn such that (cid:107)b(cid:107) = 1. Since (cid:98)V = f ( (cid:98)V ), we can follow the same analysis in the
proof of Lemma 2.1 and obtain the bound

F ≤ σ2

(cid:107) (cid:98)V − az∗H(cid:107)2

F ≤ (1 + δ)

σ2
2p

,

(56)

with high probability, where δ = C
¯z∗
kb). Let (cid:101)z = (cid:98)V H
can write (cid:98)zj = (cid:101)zj/|(cid:101)zj| for all j ∈ [n] with non-zero (cid:101)zj.

(cid:16) log n+σ2
np

(cid:17)1/4

By (56) and Wedin’s sin-theta theorem [28], we have

(cid:98)a where (cid:98)a is the leading left singular vector of (cid:98)V . By the deﬁnition of (cid:98)z, we

and a = a0/(cid:107)a0(cid:107) with a0 = b+ 1
n−1

(cid:80)n

k=1 z∗

k( (cid:98)Vk−

(cid:107)(cid:98)a − ha(cid:107)2 ≤

(cid:107) (cid:98)V − az∗H(cid:107)2
F
n

≤

σ2
np

,

form some h ∈ C1. Deﬁne d0 = aH

(cid:98)a and d = d0/|d0|. With (cid:101)zj = (cid:98)V H

j (cid:98)a, we have

(cid:101)zj ¯z∗

j

¯d = |d0| + h ¯d¯z∗

j ( (cid:98)Vj − ¯z∗

j a)Ha + ¯d( (cid:98)Vj − ¯z∗

j a)H((cid:98)a − ha)¯z∗
j .

By Lemma 5.5, we have the bound

|(cid:98)zj − dz∗

j |2 =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j

(cid:101)zj ¯z∗
|(cid:101)zj ¯z∗

j

j

|Im((cid:101)zj ¯z∗
|Re((cid:101)zj ¯z∗

j

¯d)|2
¯d)|2

,

¯d
¯d|

2

(cid:12)
(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

≤

27

(57)

(58)

(59)

as long as Re((cid:101)zj ¯z∗
ha)¯z∗

j
np (cid:107) (cid:98)Vj − ¯z∗

(cid:113) σ2

j | ≤

j a(cid:107). Moreover,

¯d) > 0. By (57), we have |d0| ≥ Re(h(cid:98)aHa) ≥ 1 − σ2

2np and | ¯d( (cid:98)Vj − ¯z∗

j a)H((cid:98)a −

|h ¯d¯z∗

j ( (cid:98)Vj − ¯z∗

j a)Ha| ≤ |( (cid:98)Vj − ¯z∗

j a)Ha| = | (cid:98)V H

j a¯z∗

j − 1| = |aH (cid:98)Vjz∗

j − 1|.

Therefore,

Re((cid:101)zj ¯z∗

j

¯d) ≥ 1 −

σ2
2np

− |aH (cid:98)Vjz∗

j − 1| −

(cid:115)

σ2
np

(cid:107) (cid:98)Vj − ¯z∗

j a(cid:107).

(60)

In the following, we are going to establish a lower bound for (60) using some similar
analysis as in the proof of Lemma 2.1. Since (cid:98)V = f ( (cid:98)V ), we can write (cid:98)Vj = (cid:101)Vj/(cid:107) (cid:101)Vj(cid:107) for all
non-zero (cid:101)Vj, where

(cid:80)

(cid:101)Vj =

k∈[n]\{j} AjkYjk (cid:98)Vk
(cid:80)
k∈[n]\{j} Ajk

.

Similar to the decomposition (36), we can write

aH (cid:101)Vjz∗

j = (cid:107)a0(cid:107) −

1
n − 1

j aH( (cid:98)Vj − ¯z∗
z∗

j b) + aHFj + aHbGj + aHHj,

where

Fj =

Gj =

Hj =

−

1
n − 1

n
(cid:88)

k=1

k( (cid:98)Vk − ¯z∗
z∗

kb),

(cid:80)

k∈[n]\{j} Ajkz∗

k( (cid:98)Vk − ¯z∗

kb)

σz∗
j

σz∗
j

(cid:80)

(cid:80)

(cid:80)

k∈[n]\{j} Ajk
k∈[n]\{j} Ajk ¯Wjk ¯z∗
(cid:80)
k∈[n]\{j} Ajk
k∈[n]\{j} Ajk ¯Wjk( (cid:98)Vk − ¯z∗
k∈[n]\{j} Ajk

(cid:80)

k

,

kb)

.

By the same argument that leads to (43) with γ = σ2
as (cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) ≤ ρ, we have

np , for any ρ > 0, we know that as long

|Re(aH (cid:101)Vjz∗

j ) − 1| ≤ 3ρ + 3

(cid:115)

σ2
np

.

Moreover,

|Im(aH (cid:101)Vjz∗

j )| ≤

≤

≤

1
n − 1

1
n − 1

1
n − 1

j b(cid:107) + (cid:107)Fj(cid:107) + |Im(aHbGj)|2 + (cid:107)Hj(cid:107)

+ (cid:107)Fj(cid:107) + |Im(aHbGj)|2 + (cid:107)Hj(cid:107)

(cid:107) (cid:98)Vj − ¯z∗
(cid:115)

σ2
p

(cid:115)

σ2
p

+ 3ρ.

28

(61)

(62)

By a similar bound to (37), we also have

(cid:107)(In − aaH) (cid:101)Vj(cid:107) ≤

1
n − 1

(cid:107) (cid:98)Vj − ¯z∗

j b(cid:107) + (cid:107)Fj(cid:107) + |Gj| + (cid:107)Hj(cid:107) ≤

1
n − 1

(cid:115)

σ2
p

+ 3ρ.

With the decomposition (cid:107) (cid:101)Vj(cid:107)2 = (cid:107)(In − aaH) (cid:101)Vj(cid:107)2 + |Im(aH (cid:101)Vjz∗

j )|2 + |Re(aH (cid:101)Vjz∗

j )|2, we have

(cid:12)
(cid:12)
(cid:12) ≤ (cid:107)(In − aaH) (cid:101)Vj(cid:107)2 + |Im(aH (cid:101)Vjz∗
(cid:12)(cid:107) (cid:101)Vj(cid:107)2 − 1
(cid:12)
(cid:12)

j )|2 +

(cid:12)
(cid:12)|Re(aH (cid:101)Vjz∗
(cid:12)

j )|2 − 1

(cid:12)
(cid:12)
(cid:12) ≤ 4ρ + 4

(cid:115)

σ2
np

.

(63)

Let ρ be a suﬃciently small with explicit expression to be given later. Together with the
assumption that σ2
np is also suﬃciently small, both (61) and (63) can be upper bounded by
1/2, which implies Re(aH (cid:101)Vjz∗
j ) > 1/2 and (cid:101)Vj (cid:54)= 0 respectively. Then (cid:98)Vj = (cid:101)Vj/(cid:107) (cid:101)Vj(cid:107) leads to
the bound

|aH (cid:98)Vjz∗

j − 1| ≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Re(aH (cid:101)Vjz∗
(cid:107) (cid:101)Vj(cid:107)
Re(aH (cid:101)Vjz∗
(cid:107) (cid:101)Vj(cid:107)
(cid:115)

(cid:32)

j ) − (cid:107) (cid:101)Vj(cid:107)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

j ) − 1

(cid:33)

σ2
np

≤ C1

ρ +

j )|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

|Im(aH (cid:101)Vjz∗
(cid:107) (cid:101)Vj(cid:107)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 − (cid:107) (cid:101)Vj(cid:107)
(cid:107) (cid:101)Vj(cid:107)

+

|Im(aH (cid:101)Vjz∗
(cid:107) (cid:101)Vj(cid:107)

j )|

where we use (61)-(63). By Lemma 5.5, we have the bound

(cid:107) (cid:98)Vj − ¯z∗

j a(cid:107)2 ≤

(cid:107)(In − aaH) (cid:101)Vj(cid:107)2 + |Im(z∗
|Re(z∗

j aH (cid:101)Vj)|2

j aH (cid:101)Vj)|2

≤ C2

(cid:18)

ρ2 +

(cid:19)

.

σ2
n2p

(64)

Plugging the above two bounds into (60), we have

Re((cid:101)zj ¯z∗

j

¯d) ≥ 1 − C3

ρ +

(cid:32)

(cid:115)

(cid:33)

,

σ2
np

which is positive since ρ and σ2
¯d) > 0 and
the bound (59) holds when (cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) ≤ ρ. Also this implies the event {(cid:101)zj = 0} is
included in the event {(cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) > ρ}. As a consequence, we have

np are suﬃciently small. Therefore, we have Re((cid:101)zj ¯z∗

j

|(cid:98)zj − dz∗

j |2 ≤

(cid:16)

j

|Im((cid:101)zj ¯z∗
(cid:16)
ρ +

1 − C3

¯d)|2
(cid:113) σ2
np

(cid:17)(cid:17)2

I{(cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) ≤ ρ} + 4I{(cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) > ρ}.

Now we need to bound |Im((cid:101)zj ¯z∗
(cid:12)
(cid:12) + |Im(h ¯d)|
(cid:12)

(cid:12)
(cid:12)Im(¯z∗
(cid:12)

j ( (cid:98)Vj − ¯z∗

j a)Ha)

¯d)| ≤

|Im((cid:101)zj ¯z∗

j

j

¯d)| according to the expansion (58). We have

(cid:12)
(cid:12)Re(¯z∗
(cid:12)

j ( (cid:98)Vj − ¯z∗

(cid:12)
(cid:12) + (cid:107) (cid:98)Vj − ¯z∗
(cid:12)
j a)Ha)

j a(cid:107)(cid:107)(cid:98)a − ha(cid:107).
(65)

29

By (57) and (64), the third term in the bound (65) can be further bounded by C2
ρ +
To bound the second term on the right hand side of (65), we have |Im(h ¯d)| ≤ |Im(haH
(cid:112)1 − |Re(haHa)|2 ≤
ρ +
By (63), we can bound the ﬁrst term in the bound (65) by

np by (57). Together with (61), we obtain the bound 3

(cid:113) σ2
np

(cid:113) σ2

(cid:16)

(cid:98)a)| ≤
(cid:113) σ2
(cid:17)
np

.

(cid:16)

(cid:113) σ2
np

(cid:113) σ2
n2p

(cid:17)

.

(cid:12)
(cid:12)Im(¯z∗
(cid:12)

j ( (cid:98)Vj − ¯z∗

j a)Ha)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)Im( (cid:98)V H

(cid:12)
j a¯z∗
(cid:12)
(cid:12) =
j )

(cid:12)
(cid:12)Im(aH (cid:98)Vjz∗
(cid:12)
j )

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)Im(aH (cid:101)Vjz∗
(cid:12)
(cid:12)
j )
(cid:12)
(cid:113) σ2
(cid:16)
ρ +
np

1 − 4

(cid:17) .

Then, we have

|(cid:98)zj − dz∗

j |2 ≤

(cid:32)

(cid:32)

(cid:115)

1 + C4

ρ +

(cid:33)(cid:33)

σ2
np

(cid:12)
(cid:12)Im(aH (cid:101)Vjz∗
(cid:12)
j )

(cid:12)
2
(cid:12)
(cid:12)

+ C5

σ2
np

(cid:18)

ρ2 +

(cid:19)

σ2
np

+4I{(cid:107)Fj(cid:107) ∨ |Gj| ∨ (cid:107)Hj(cid:107) > ρ}
(cid:115)
(cid:32)

(cid:33)(cid:33)

(cid:32)

≤

1 + C6

ρ +

+C5

(cid:18)

ρ2 +

σ2
np

σ2
np

σ2
np
(cid:19)

+ η

|Im(aHbGj)|2 + C7η−1

(cid:18) σ2
n2p

+ (cid:107)Fj(cid:107)2 + (cid:107)Hj(cid:107)2

(cid:19)

+ 4ρ−2 (cid:0)(cid:107)Fj(cid:107)2 + (cid:107)Hj(cid:107)2(cid:1) + 4I{|Gj| > ρ},

for some η to be speciﬁed later, where the last inequality is by (62). Summing over j ∈ [n],
we obtain

n(cid:96)((cid:98)z, z∗) ≤

n
(cid:88)

j=1
(cid:32)

j |2
|(cid:98)zj − dz∗

(cid:32)

(cid:115)

≤

1 + C6

ρ +

σ2
np

+ η

(cid:33)(cid:33) n
(cid:88)

j=1

|Im(aHbGj)|2 + C7η−1 σ2
np

+ C5

σ2
p

(cid:18)

ρ2 +

(cid:19)

σ2
np

+(C7η−1 + 4ρ−2)

n
(cid:88)

j=1

(cid:0)(cid:107)Fj(cid:107)2 + (cid:107)Hj(cid:107)2(cid:1) + 4

n
(cid:88)

j=1

I{|Gj| > ρ}.

By the same argument that leads to the bound (44)-(47) (with γ = σ2

np in (47)), we have

n
(cid:88)

j=1

((cid:107)Fj(cid:107)2 + (cid:107)Hj(cid:107)2) ≤ C(cid:48) log n
np

(cid:107) (cid:98)V − bz∗H(cid:107)2

F ≤ C(cid:48) log n
np

σ2
p

,

Take η = ρ2 =

j=1
(cid:113) log n+σ2
np

n
(cid:88)

j=1

I{|Gj| > ρ} ≤

4σ2
ρ2p

(cid:32)

(cid:32)

exp

−

(cid:114)

(cid:33)

,

ρ2np
σ2

1
16

n
(cid:88)

|Im(aHbGj)|2 ≤

1 + C(cid:48)(cid:48)

(cid:32)
η + η−1 σ2
np

+

(cid:115)

log n
np

(cid:33)(cid:33)

σ2
2p

.

, and we have some constant C(cid:48)(cid:48)(cid:48) > 0 such that

(cid:32)

(cid:96)((cid:98)z, z∗) ≤

1 + C(cid:48)(cid:48)(cid:48)

(cid:18) log n + σ2
np

(cid:19)1/4(cid:33)

σ2
2np

.

30

Note that the above bound is derived from conditions (29)-(34), and thus the result holds
with high probability.

Proof of Theorem 4.2. The ﬁrst conclusion is an immediate consequence of Lemma 4.1, Lemma
4.2 and Lemma 4.3. By Lemma 5.6, we also obtain the second conclusion. For the last con-
j | and |(cid:98)zj + z∗
clusion, we have |(cid:98)zj − z∗
j | by the deﬁnition of (cid:98)zj.
Then,

nuj − z∗

nuj + z∗

j | ≤ 2|

j | ≤ 2|

√

√

√
(cid:96)((cid:98)z, z∗) ≤ 4 (cid:0)(cid:107)u − z∗/

n(cid:107)2 ∧ (cid:107)u + z∗/

√

n(cid:107)2(cid:1) ≤

16
n2 (cid:107) (cid:98)Z − z∗z∗T(cid:107)2
F,

by Davis-Kahan theorem [11]. Thus, we can derive the third conclusion from the second
one. Finally, when σ2 < (1 − (cid:15)) np
2 log n , we know from (27) and Lemma 4.2 that (cid:96)( (cid:98)V , z∗) = 0.
Lemma 5.6 implies that (cid:107) (cid:98)Z − z∗z∗T(cid:107)2
F = 0 and thus (cid:98)Z = z∗z∗T is a rank-one matrix.

5.5 Proof of Theorem 4.1
Since (cid:96)((cid:98)z, z) = 2 (cid:0)1 − 1

n |(cid:98)zTz|(cid:1) and n−2(cid:107)(cid:98)z(cid:98)zT − zzT(cid:107)2

n−2(cid:107)(cid:98)z(cid:98)zT − zzT(cid:107)2

F = (cid:96)((cid:98)z, z)

and thus

n2 |(cid:98)zTz|2(cid:1), we have

F = 2 (cid:0)1 − 1
(cid:19)

(cid:18)

1 +

|(cid:98)zTz|

≤ 2(cid:96)((cid:98)z, z),

1
n

inf
(cid:98)z∈{−1,1}n

sup
z∈{−1,1}n

Ez(cid:96)((cid:98)z, z) ≥

≥

inf
(cid:98)z∈{−1,1}n
1
2

inf
(cid:98)Z∈Rn×n

sup
z∈{−1,1}n

Ez

sup
z∈{−1,1}n

Ez

1
2n2 (cid:107)(cid:98)z(cid:98)zT − zzT(cid:107)2
1
n2 (cid:107) (cid:98)Z − zzT(cid:107)2
F.

F

It suﬃces to prove a lower bound for the loss (cid:107) (cid:98)Z − zzT(cid:107)2
by a Bayes risk

F. We lower bound the minimax risk

inf
(cid:98)Z∈Rn×n

inf
(cid:98)Z∈Rn×n

sup
z∈{−1,1}n
1
(cid:88)
2n

(cid:88)

1
n2

z∈{−1,1}n
1
2n−2

≥

≥

Ez

1
n2 (cid:107) (cid:98)Z − zzT(cid:107)2

F

Ez

F

1
n2 (cid:107) (cid:98)Z − zzT(cid:107)2
1
4

(cid:88)

inf
(cid:98)T

1≤j(cid:54)=k≤n

z−(j,k)∈{−1,1}n−2

(cid:88)

(cid:88)

Ez| (cid:98)T − zjzk|2,

zj ∈{−1,1}

zk∈{−1,1}

where z−(j,k) is a sub-vector of z by excluding the jth and the kth entries. For each z−(j,k),
we have

inf
(cid:98)T

≥ inf
(cid:98)T
(cid:90)

≥ 2

(cid:88)

(cid:88)

Ez| (cid:98)T − zjzk|2

zj ∈{−1,1}
(cid:16)

zk∈{−1,1}

E(z−(j,k),zj =1,zk=−1)| (cid:98)T + 1|2 + E(z−(j,k),zj =1,zk=1)| (cid:98)T − 1|2(cid:17)

dP(z−(j,k),zj =1,zk=−1) ∧ dP(z−(j,k),zj =1,zk=1),

31

where the last inequality is due to the classical Le Cam’s two-point method. The total
variation aﬃnity characterizes the optimal testing error between two simple hypotheses of
zk = −1 versus zk = 1 with the values of all other parameters are known. By Neyman-Pearson
lemma, we have

(cid:90)

dP(z−(j,k),zj =1,zk=−1) ∧ dP(z−(j,k),zj =1,zk=1)

≥ P(z−(j,k),zj =1,zk=−1)

(cid:32) dP(z−(j,k),zj =1,zk=1)
dP(z−(j,k),zj =1,zk=−1)

= P(z−(j,k),zj =1,zk=−1)





(cid:88)

j∈[n]\{k}

zjAjkYjk > 0



(cid:33)

> 1





= P

σ

(cid:88)

zjAjkWjk >

(cid:88)

j∈[n]\{k}

j∈[n]\{k}



Ajk

 .

Let A be the collections of A’s that satisfy the conclusions of Lemma 5.1, and we know that
P(A) ≥ 1 − n−10. Let PA be the shorthand of the conditional probability P(·|A). For each
A ∈ A, a standard Gaussian tail bound implies



PA

σ

(cid:88)

zjAjkWjk >

(cid:88)

Ajk

 ≥ exp

j∈[n]\{k}

j∈[n]\{k}



(cid:16)

−(1 + δ)

(cid:17)

,

np
2σ2

where δ = C

(cid:113) log n+σ2
np

for some constant C > 0. This implies



P

σ

(cid:88)

j∈[n]\{k}


zjAjkWjk >



Ajk



(cid:88)

j∈[n]\{k}

≥ inf
A∈A

PA

σ

(cid:88)

zjAjkWjk >


 P(A)

Ajk

(cid:88)

j∈[n]\{k}

≥

(cid:16)

exp

1
2

Therefore,

j∈[n]\{k}
np
2σ2

−(1 + δ)

inf
(cid:98)z∈{−1,1}n

sup
z∈{−1,1}n

Ez(cid:96)((cid:98)z, z) ≥

≥

(cid:17)

.

1
2

inf
(cid:98)Z∈Rn×n
(cid:16)

exp

1
16

sup
z∈{−1,1}n

Ez

−(1 + δ)

np
2σ2

1
n2 (cid:107) (cid:98)Z − zzT(cid:107)2
(cid:17)

F

.

By absorbing the constant 1/16 into the exponent, the proof is complete.

32

5.6 Proofs of Lemma 2.2, Lemma 4.3, and Lemma 4.2

Proof of Lemma 2.2. By the deﬁnition of (cid:98)Z = (cid:98)V H (cid:98)V , we have Tr((A◦Y ) (cid:98)Z) ≥ Tr((A◦Y )z∗z∗H).
Rearranging this inequality, we obtain

Tr(z∗z∗H(z∗z∗H − (cid:98)Z)) ≤ Tr

(cid:16)

(A ◦ Y /p − z∗z∗H)( (cid:98)Z − z∗z∗H)

(cid:17)

.

(66)

The right hand side of (66) can be bounded by

(cid:16)

(cid:12)
(cid:12)
(cid:12)Tr

(A ◦ Y /p − z∗z∗H) (cid:98)Z

(cid:17)(cid:12)
(cid:12) + |Tr ((A ◦ Y /p − z∗z∗H)z∗z∗H)|
(cid:12)
≤ (cid:107)A ◦ Y /p − z∗z∗H(cid:107)op Tr( (cid:98)Z) + (cid:107)A ◦ Y /p − z∗z∗H(cid:107)op Tr(z∗z∗H)
= 2n(cid:107)A ◦ Y /p − z∗z∗H(cid:107)op

≤ 2n

(cid:18) 1
p

(cid:107)(A − EA) ◦ z∗z∗H(cid:107)op +

(cid:19)

(cid:107)A ◦ W (cid:107)op

.

σ
p

By Lemma 5.1,

(cid:107)(A − EA) ◦ z∗z∗H(cid:107)op = sup
(cid:107)u(cid:107)=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(Ajk − p)z∗

j ¯z∗

kuj ¯uk

1≤j(cid:54)=k≤n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:107)A − EA(cid:107)op
√
≤ C1

np,

with probability at least 1 − n−10. By Lemma 5.2, (cid:107)A ◦ W (cid:107)op ≤ C2
least 1 − n−10. Thus, we have

√

np with probability at

Tr(z∗z∗H(z∗z∗H − (cid:98)Z)) ≤ C3n

(cid:115)

(1 + σ2)n
p

.

33

Deﬁne m = 1
n

(cid:80)n

j=1 (cid:98)Vjz∗

j . By the inequality (cid:107)x/(cid:107)x(cid:107) − y/(cid:107)y(cid:107)(cid:107) ≤ 2(cid:107)x − y(cid:107)/(cid:107)x(cid:107), we have

(cid:96)( (cid:98)V , z∗) =

min
a∈Cn:(cid:107)a(cid:107)2=1

1
n

n
(cid:88)

j=1

(cid:107) (cid:98)Vjz∗

j − a(cid:107)2

=

≤

=

=

=

=

=

4
n

2
n2

2
n2

4
n2

min
a∈Cn\{0}

min
a∈Cn\{0}

1
n

4
n

n
(cid:88)

j=1
n
(cid:88)

j=1

(cid:107) (cid:98)Vjz∗

j − a/(cid:107)a(cid:107)(cid:107)2

(cid:107) (cid:98)Vjz∗

j − a(cid:107)2

n
(cid:88)

(cid:107) (cid:98)Vjz∗

j − m(cid:107)2

j=1
n
(cid:88)

n
(cid:88)

(cid:16)

(cid:107) (cid:98)Vjz∗

j − m(cid:107)2 + (cid:107) (cid:98)Vlz∗

l − m(cid:107)2(cid:17)

j=1
n
(cid:88)

l=1
n
(cid:88)

(cid:107) (cid:98)Vjz∗

j − (cid:98)Vlz∗

l (cid:107)2

j=1
n
(cid:88)

l=1
n
(cid:88)

(1 − ¯z∗

j z∗

l (cid:98)V H

j (cid:98)Vl)

j=1

l=1

4
n2 Tr(z∗z∗H(z∗z∗H − (cid:98)Z)).
(cid:113) (1+σ2)
np

Therefore, we have (cid:96)( (cid:98)V , z∗) ≤ 4C3

, and the proof is complete.

Proof of Lemma 4.3. Following the same argument in the proof of Lemma 2.2, we have

Tr(z∗z∗T(z∗z∗T − (cid:98)Z)) ≤ Cn

(cid:115)

(1 + σ2)n
p

,

with probability at least 1 − n−9 and (cid:96)( (cid:98)V , z∗) ≤ 4
the bound (cid:96)( (cid:98)V , z∗) ≤ 4C

(cid:113) (1+σ2)
np

, and the proof is complete.

n2 Tr(z∗z∗T(z∗z∗T − (cid:98)Z)). Then, we obtain

Proof of Lemma 4.2. Let A be the collections of A’s that satisfy the conclusions of Lemma
5.1, and we know that P(A) ≥ 1−n−10. Let PA be the shorthand of the conditional probability
P(·|A). For each A ∈ A, a standard Gaussian tail bound implies

8
n

n
(cid:88)

j=1

PA (|Uj| > 1 − δ) ≤

16
n

n
(cid:88)

exp

(cid:32)

−

j=1
(cid:16)

≤ exp

−(1 − ¯δ)

(cid:33)

(1 − δ)2(n − 1)2p2
2σ2 (cid:80)
k∈[n]\{j} Ajk
(cid:17)
np
2σ2

,

34

where ¯δ = C

(cid:16)

δ +

(cid:17)

(cid:113) log n
np

for some constant C > 0. Therefore,

I{|Uj| > 1 − δ} > exp

−

1 − ¯δ −

(cid:32)

(cid:32)

(cid:115)

(cid:33)

(cid:33)


np
2σ2

2σ2
np

n
(cid:88)

I{|Uj| > 1 − δ} > exp

−

1 − ¯δ −

(cid:32)

(cid:32)

(cid:115)

(cid:33)

2σ2
np

np
2σ2

(cid:33)

 + P(Ac)





8
n

P

n
(cid:88)

j=1


≤ sup
A∈A

PA



8
n

≤ exp

−

(cid:18)

(cid:114) np
σ2

j=1
(cid:19)

+ n−10,

by Markov’s inequality. This immediately implies the ﬁrst conclusion. For the second con-
I{|Uj| >

2σ2 > log n, we have 8

(cid:113) 2σ2
np

1 − ¯δ −

(cid:17) np

(cid:80)n

j=1

(cid:16)

n

clusion, it is easy to see that when
n , and thus the value of 8
1 − δ} ≤ 1

n

(cid:80)n

j=1

I{|Uj| > 1 − δ} has to be 0.

References

[1] Abbe, E., Fan, J., Wang, K. and Zhong, Y. [2020]. Entrywise eigenvector analysis of

random matrices with low expected rank, Annals of Statistics 48(3): 1452–1474.

[2] Abbe, E., Massoulie, L., Montanari, A., Sly, A. and Srivastava, N. [2017]. Group syn-

chronization on grids, arXiv preprint arXiv:1706.08561 .

[3] Amini, A. A., Levina, E. et al. [2018]. On semideﬁnite relaxations for the block model,

The Annals of Statistics 46(1): 149–179.

[4] Bandeira, A. S. [2018]. Random laplacian matrices and convex relaxations, Foundations

of Computational Mathematics 18(2): 345–379.

[5] Bandeira, A. S., Boumal, N. and Singer, A. [2017]. Tightness of the maximum likelihood
semideﬁnite relaxation for angular synchronization, Mathematical Programming 163(1-
2): 145–167.

[6] Bandeira, A. S., Khoo, Y. and Singer, A. [2014]. Open problem: Tightness of maximum

likelihood semideﬁnite relaxations, arXiv preprint arXiv:1404.2655 .

[7] Bandeira, A. S. and Van Handel, R. [2016]. Sharp nonasymptotic bounds on the norm of
random matrices with independent entries, The Annals of Probability 44(4): 2479–2506.

[8] Boumal, N. [2016]. Nonconvex phase synchronization, SIAM Journal on Optimization

26(4): 2355–2377.

[9] Burer, S. and Monteiro, R. D.

[2003]. A nonlinear programming algorithm for
solving semideﬁnite programs via low-rank factorization, Mathematical Programming
95(2): 329–357.

35

[10] Chen, Y., Li, X. and Xu, J. [2018]. Convexiﬁed modularity maximization for degree-

corrected stochastic block models, The Annals of Statistics 46(4): 1573–1602.

[11] Davis, C. and Kahan, W. M. [1970]. The rotation of eigenvectors by a perturbation. iii,

SIAM Journal on Numerical Analysis 7(1): 1–46.

[12] Erdogdu, M. A., Ozdaglar, A., Parrilo, P. A. and Vanli, N. D. [2018]. Convergence rate
of block-coordinate maximization burer-monteiro method for solving large sdps, arXiv
preprint arXiv:1807.04428 .

[13] Fei, Y. and Chen, Y. [2020]. Achieving the bayes error rate in synchronization and block
models by sdp, robustly, IEEE Transactions on Information Theory 66(6): 3929–3953.

[14] Gao, C., Lu, Y., Ma, Z. and Zhou, H. H. [2016]. Optimal estimation and completion
of matrices with biclustering structures, The Journal of Machine Learning Research
17(1): 5602–5630.

[15] Gao, C. and Zhang, A. Y. [2019]. Iterative algorithm for discrete structure recovery,

arXiv preprint arXiv:1911.01018 .

[16] Gao, C. and Zhang, A. Y. [2020]. Exact minimax estimation for phase synchronization,

arXiv preprint arXiv:2010.04345 .

[17] Gao, T. and Zhao, Z. [2020]. Multi-frequency phase synchronization, Proceedings of

Machine Learning Research 97.

[18] Hajek, B., Wu, Y. and Xu, J. [2016a]. Achieving exact cluster recovery threshold via
semideﬁnite programming, IEEE Transactions on Information Theory 62(5): 2788–2797.

[19] Hajek, B., Wu, Y. and Xu, J. [2016b]. Achieving exact cluster recovery threshold
via semideﬁnite programming: Extensions, IEEE Transactions on Information Theory
62(10): 5918–5937.

[20] Javanmard, A., Montanari, A. and Ricci-Tersenghi, F.

Phase transi-
tions in semideﬁnite relaxations, Proceedings of the National Academy of Sciences
113(16): E2218–E2223.

[2016].

[21] Lei, J. and Rinaldo, A. [2015]. Consistency of spectral clustering in stochastic block

models, The Annals of Statistics 43(1): 215–237.

[22] Li, X., Chen, Y. and Xu, J. [2018]. Convex relaxation methods for community detection,

arXiv preprint arXiv:1810.00315 .

[23] Ling, S. [2020]. Solving orthogonal group synchronization via convex and low-rank
optimization: Tightness and landscape analysis, arXiv preprint arXiv:2006.00902 .

[24] Lu, Y. and Zhou, H. H. [2016]. Statistical and computational guarantees of lloyd’s

algorithm and its variants, arXiv preprint arXiv:1612.02099 .

36

[25] Perry, A. and Wein, A. S. [2017]. A semideﬁnite program for unbalanced multisection
in the stochastic block model, 2017 International Conference on Sampling Theory and
Applications (SampTA), IEEE, pp. 64–67.

[26] Singer, A. [2011]. Angular synchronization by eigenvectors and semideﬁnite program-

ming, Applied and computational harmonic analysis 30(1): 20–36.

[27] Wang, P.-W., Chang, W.-C. and Kolter, J. Z. [2017]. The mixing method: low-rank co-
ordinate descent for semideﬁnite programming with diagonal constraints, arXiv preprint
arXiv:1706.00476 .

[28] Wedin, P.-˚A. [1972]. Perturbation bounds in connection with singular value decomposi-

tion, BIT Numerical Mathematics 12(1): 99–111.

[29] Zhang, A. Y. and Zhou, H. H. [2016]. Minimax rates of community detection in stochastic

block models, The Annals of Statistics 44(5): 2252–2280.

[30] Zhong, Y. and Boumal, N. [2018]. Near-optimal bounds for phase synchronization, SIAM

Journal on Optimization 28(2): 989–1016.

37

