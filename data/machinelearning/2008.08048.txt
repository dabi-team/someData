0
2
0
2

g
u
A
8
1

]
E
M

.
t
a
t
s
[

1
v
8
4
0
8
0
.
8
0
0
2
:
v
i
X
r
a

Manuscript submitted to arXiv , pp. 1–31.

Learning Structure in Nested Logit Models

Youssef M. Aboutaleb†, Moshe E. Ben-Akiva †, Patrick Jaillet †

† Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139, USA.
E-mail: ymedhat@mit.edu, mba@mit.edu, jaillet@mit.edu

Summary
This paper introduces a new data-driven methodology for nested logit structure dis-
covery. Nested logit models allow the modeling of positive correlations between the error terms of the
utility speciﬁcations of the diﬀerent alternatives in a discrete choice scenario through the speciﬁcation
of a nesting structure. Current nested logit model estimation practices require an a priori speciﬁcation
of a nesting structure by the modeler. In this we work we optimize over all possible speciﬁcations
of the nested logit model that are consistent with rational utility maximization. We formulate the
problem of learning an optimal nesting structure from the data as a mixed integer nonlinear program-
ming (MINLP) optimization problem and solve it using a variant of the linear outer approximation
algorithm. We exploit the tree structure of the problem and utilize the latest advances in integer op-
timization to bring practical tractability to the optimization problem we introduce. We demonstrate
the ability of our algorithm to correctly recover the true nesting structure from synthetic data in a
Monte Carlo experiment. In an empirical illustration using a stated preference survey on modes of
transportation in the U.S. state of Massachusetts, we use our algorithm to obtain an optimal nesting
tree representing the correlations between the unobserved eﬀects of the diﬀerent travel mode choices.
We provide our implementation as a customizable and open-source code base written in the Julia
programming language.

Keywords: Nested Logit, Discrete Choice, Algorithmic Model Selection, Machine Learning

1. INTRODUCTION

First introduced by Ben-Akiva (1973) in the context of travel demand modeling and extended by
McFadden (1978) in the context of resdiential location choice, the nested logit model is widely used in
the marketing and transportation science literatures among others for modeling the choice a rational
decision-making agent makes from a set of mutually exclusive alternatives (Ben-Akiva and Lerman,
1985). That choice might be the travel mode of daily commute, a ﬂight to book, or an economics
textbook to buy. The nested logit model belongs to a wider class of behavioral choice models known
as random utility models (RUMs).

RUMs turn on the assumption that the decision maker ranks the available choices (or alternatives) in
order of preference as represented by a latent (i.e., unobserved from the modeler’s perspective) utility
function. The utility that a decision-making agent associates with a particular alternative is assumed
to be comprised of a systematic component and an error term. The systematic component is speciﬁed
as a function (typically linear-in-parameters) of the attributes of the alternative and characteristics of
the decision maker. The error term is modeled as a random variable. There is a utility equation for
each alternative. Logit models are a family of RUMs that assume a joint extreme-value distribution of
the error terms across the alternatives. Further assumptions on the the error terms lead to diﬀerent
types of logit models and give rise to diﬀerent mathematical forms for the choice probabilities.

The multinomial (ﬂat) logit model assumes independence and homoscedasticty of the error terms
across the diﬀerent alternatives. The independence assumption implies that the odds ratios between
any two alternatives are independent of the attributes or availability of the other alternatives. This
property, called the independence from irrelevant alternatives (IIA), is convenient from an estimation
stand point but is an unrealistic restriction on agent behavior in some applications. One implication
of the IIA assumption is “proportional substitution” between the alternatives. The red bus/blue bus
paradox where the introduction of a blue bus travel mode, with the same attributes as an existing red
bus travel mode, draws equally from the market shares of all of the available alternatives (instead of
splitting the red bus market share), is a classic example of a choice setting where the IIA property

 
 
 
 
 
 
2

Aboutaleb et al.

is inappropriate. Demand predictions can be seriously compromised by incorrectly assuming the IIA
property (McFadden and Train, 1978).

The nested logit model is invoked when it is thought that there are common unobserved attributes
of the choice alternatives. The independence assumption is relaxed by partitioning the alternatives
into mutually exclusive and collectively exhaustive subsets called nests. The error terms in the utility
expressions are decomposed into alternative speciﬁc and nest speciﬁc error terms, the latter of which
introduces correlations between the collective error terms of alternatives sharing a common nest. This
breaks the independence assumption of the multinomial logit and allows the modeling of more general
substitution patterns. The homoscedasticty assumption remains, the total variance of the error terms
is assumed to be the same for all the alternatives. Each nest is associated with a scale parameter which
quantiﬁes the variance of the nest speciﬁc error terms and consequently the covariance between the
collective error terms of any two alternatives sharing the nest.

There are references in the literature to two “diﬀerent” nested logit models: a non-normalized nested
logit (NNNL) and a utility maximization nested logit (UMNL) model (Koppelman and Wen, 1998a;
Koppelman and Wen, 1998b ). These two models where shown to be equivalent– the diﬀerence being
in the normalization of the scale parameters; see Hensher and Greene (2002).

To specify the nested logit model, a nesting partition of the alternatives is necessary. In some appli-
cations, the appropriate partition to use is immediately clear as in McFadden (1978) where residential
location choice is made ﬁrst by community and then by dwelling type. In many other applications,
however, the partition choice is ad hoc. That the estimation results (including the policy parameters
in the systematic speciﬁcation) are aﬀected by the partition choice is viewed as a problematic aspect
of the nested logit model; see Greene (2003).

The large set of possible partitions precludes an exhaustive search and presents a signiﬁcant mod-
eling challenge in deciding which partitioning of the alternatives best reﬂects the underlying choice
behaviour of the population. Typically, shortcuts are made on the basis of a priori reasoning in deter-
mining a partition. The current modus operandi is to rely on domain knowledge to substantially reduce
the feasible set to a small set of candidate partitions and employ statistical techniques to determine
goodness of ﬁt. Many studies in literature present several sets of results based on diﬀerent partitions.
This is done at the risk of potentially excluding some ostensibly non-intuitive structures which might
actually provide a better description of the choice behaviour of the population under study; see Kop-
pelman and Bhat (2006) and Daly (1987). This motivates the approach we take in this paper for a
more holistic view of nested logit model estimation, i.e., one that optimizes over the nesting partition
as well as model parameters.

The desire for ﬁnding an optimal nesting tree (or partition) was ﬁrst articulated in the literature by
Daly (1987) who spoke of an “ultimate need” of a method to empirically identify an optimal structure.
Daly (1987) formulated the likelihood as a function of the nesting tree and the model parameters but
remarked that since the function is discrete the only reliable method (at the time) was to evaluate
the maximum of the likelihood at diﬀerent candidate tree speciﬁcations and choose the speciﬁcation
that provides the best ﬁt. A key intuition from Daly (1987)’s formulation is that the optimization has
to be made jointly with nesting structure, scale parameters, and model parameters in the systematic
speciﬁcation. Indeed the optimal structure is not independent of the systematic speciﬁcation. We
indulge on this point later in this section. The desire for a systematic method for learning nested logit
structure has been reiterated in many works including Bierlaire (1998) and Greene (2003).

Testing departures from the IIA property is sometimes used to guide the speciﬁcation of the nesting
partition. Hausmans speciﬁcation test (Hausman and McFadden, 1984) turn on the observation that
if a subset of the alternatives is truly irrelevant then its omission, although might lead to ineﬃciencies,
will not systematically aﬀect the estimated parameters. The test compares the estimated parameters
with and without the subset of alternatives to determine is the IIA is violated. Hausmans test of course
requires the researcher to specify which subsets to omit and therefore can not be the basis of a practical
nested structure learning algorithm since there is exponentially many such subsets of alternatives to
test.

Perhaps the ﬁrst attempt at automating the nested logit structure speciﬁcation appears in Benson

Learning Nesting Structure

3

et al. (2016). The authors notice that an alternative k is an irrelevant alternative of alternatives i and j
if and only if including it in the choice set does not signiﬁcantly aﬀect the pairwise preference of item i
over item j. They describe a battery of statistical tests to compare the probability of choosing i over j
and formulate an algorithm to systematically test the IIA assumption and build a corresponding nesting
tree. Once the structure is discovered in this way, the authors then specify a systematic component
for the choice model and estimate its the parameters.

The method of Benson et al. (2016) fails to account for the interdependence between the optimal
nesting structure and the systematic speciﬁcation of the utility equations. Recall that the error terms
are deﬁned simply as the diﬀerence between the true latent utility and the systematic utility speciﬁed by
the modeler, therefore the independence (or lack thereof) of the error terms depends on the speciﬁcation
of the systematic utility. For a given choice situation, two diﬀerent speciﬁcations of systematic utility
will result in two diﬀerent sets of error terms. One set might be independent while the other might
not. Therefore, the IIA assumption might hold for one speciﬁcation of systematic utility but not for
another, even though both speciﬁcations relate to the same choice situation. This means that, strictly
speaking, the IIA property is or is not valid for a particular speciﬁcation of systematic utility in a
logit model of a particular choice situation, not for the choice situation itself; see McFadden and Train
(1978) for a more complete discussion.

In this work, we take the full likelihood approach ﬁrst suggested by Daly (1987). We introduce
and formulate the nested logit structure learning problem (NLSLP) as a linearly constrained mixed
integer nonlinear programming (MINLP) problem. We introduce a solution algorithm based on the
linear outer approximation algorithm (Fletcher and Leyﬀer, 1994). Over the past 25 years, algorithmic
advances in integer optimization coupled with hardware improvements have resulted in an 800 billion
factor speedup in mixed-integer optimization (Bertsimas and Dunn, 2017). Furthermore, besides the
massive speed increases, the introduction of “lazy constraints” implementation in modern-day solvers
(which provide for an eﬃcient way of dealing with constraints that grow exponentially in number
with the size of the problem) has brought tractability to many problems of practical interest (Pearce,
2019). This has enabled recent successes when applying modern mixed integer optimization methods
to a selection of statistical problems (Bertsimas et al. (2016); Bertsimas et al. (2014); Bertsimas et al.
(2017); Bertsimas and King (2016); Aboutaleb et al. (2020)). We make use of the state-of-the art in
algorithmic development in solving mixed integer optimization problems and lazy constraints to bring
practical tractability to the NLSLP.

The remainder of this paper is organized as follows. Section 2 introduces the notations we use
throughout the paper and reviews the relationship between nesting trees and the variance-covariance
matrix of the error terms in a nested logit model. We also introduce an important result that brings
needed tractability to the NLSLP. In Section 3, we formally introduce the NLSLP and describe in
detail its formulation. In Section 4, we present a complete solution algorithm for the NLSLP. Section
5 presents Monte Carlo simulations to validate our proposed methodology, in addition to an empirical
application. Section 6 concludes the paper.

2. TECHNICAL SETUP AND FOUNDATIONAL RESULTS

In this section we introduce the notation we use throughout the paper and present a result that brings
tractability to the nested logit structure learning problem that we use in our formulation Section
3. In accordance with the notation in Ben-Akiva and Lerman (1985), let Uin = Vin + (cid:15)in be the
utility agent n ∈ I associates with alternative i ∈ Cn ⊆ C. Vin is the systematic utility component (a
deterministic, typically linear, function of model “taste” parameters, attributes of the alternatives, and
characteristics of the agent) and (cid:15)in are extreme-value distributed errors. The error terms represent the
eﬀects of omitted taste variations, choice attributes and socioeconomic variables. Under the nested logit
framework, the universal choice set C is partitioned into mutually exclusive and collectively exhaustive
“nests” formally deﬁned as follows:

Definition 2.1. A nested partition, B, of a set C is a set of nonempty subsets Bm ⊆ C such that

4

Aboutaleb et al.

(1) One of the subsets Bm is the universal set C. (2) The subsets are nested, i.e., whenever two distinct
subsets are overlapping (Bm ∩B(cid:48)
m ⊆ Bm).
The subsets Bm constituting a nesting partition B as referred to as “nests”.

m (cid:54)= ∅), one subset contains the other (either Bm ⊆ B(cid:48)

m or B(cid:48)

A nest Bm ∈ B is called degenerate if |Bm| = 1. A nested partition B is called degenerate if it

contains one or more degenerate nests.

Definition 2.2. Given a nested partition B, let B(j) denote the smallest nest Bm ∈ B containing the
elemental alternative j ∈ C. Similarly, B(C) denotes the smallest subset Bm ∈ B such that C ⊆ Bm.

Having introduced nested partitions in terms of subsets, we now introduce the more familiar graph
based representation of a nested partition i.e., nesting trees. The machinery of graph theory will be used
in the next section to enforce the desired arborescence (tree) properties in the optimization formulation.
The subset-based representation is convenient from a notation standpoint, and is useful when proving
certain properties such as counting the total number of possible partitions.

The following theorem states that if we don’t allow degenerate nests, the maximum number of
possible nests in a nesting partition is two less than the number of elemental alternatives. The proof
of the theorem also shows the process of building a tree representation of a nested partition.

Theorem 2.1. Let C be a ﬁnite set with n elements, where n ≥ 2. Any non-degenerate nested partition
of C can be represented by a unique tree with n leaves and at most n − 2 internal nodes.

Proof. Suppose B is a non-degenerate nested partition of C. To construct a tree representation, T,
of B:

Step 1. Create a root node (◦) and let it represent the universal choice set C.
Step 2. Represent each nest Bm ∈ B containing a strict subset of the alternatives in B by an
internal node ((cid:52)).
Step 3. Represent each elemental alternative j ∈ C by a leaf node (•).
Step 4. Connect each leaf node to the internal node corresponding to the smallest nest containing
it. Formally for each j ∈ C ﬁnd B(j) and draw a directed edge from the node representing B(j)
to the leaf node representing j.
Step 5. Similarly, connect each nest to the smallest set containing it. For each Bm ∈ B ﬁnd
B(Bm) and connect the node representing B(Bm) to that representing Bm.

Property 1 of Deﬁnition 2.1 guarantees that each B(.) is deﬁned, and property 2 guarantees that each
B(.) is unique (since degeneracy is not allowed).

We now show that the number of nest nodes is at most n − 2. Let b denote the number of nest
nodes. The n leaf nodes are terminal nodes and must have degree one. Since the nesting partition is
non-degenerate by assumption, all internal nodes must have at least two children. This implies that (i)
the root has at least degree two, and (ii) the nest nodes have at least degree three (one parent and at
least two children). Therefore deg(T) ≥ n + 3b + 2. Now, by the Handshaking lemma (Vasudev, 2006)
we have that the degree of the graph is twice the number of edges: deg(T) = 2|E|. Furthermore, since
T is a tree, the number of edges is one less than the number of nodes so that |E| = (n + b + 1) − 1,
therefore, 2(n + b) ≥ n + 3b + 2 and hence b ≤ n − 2.

(cid:3)
As an illustration, Figure 1 shows the four possible nesting partitions and their corresponding tree
representations for the universal choice set C = {1, 2, 3}.

Theorem 2.1 brings tractability to the problem of learning nested logit structures. Namely, in an
optimization framework, we can simply include the maximum number of nests allowed and let an

Learning Nesting Structure

5

optimization procedure guide the inclusion or exclusion of these nests. Degenerate nests do not aﬀect
the likelihood and can be omitted without loss of generality.

The nesting structure determines how the alternatives are correlated, and the associated scale param-
eters determine by what amount they are correlated. The exact relation between the nesting structure
and correlation of the error terms is laid out in the following proposition; see Daganzo and Kusnic
(1993) and Galichon (2019) for a theoretical derivation.

Proposition 2.1. Let B be a nested partition of the choice set C. If the nested logit assumptions are
satisﬁed, then the covariance cov((cid:15)i, (cid:15)j) between the error terms in the utility expression of any two
distinct alternatives i, j ∈ C is given by:

π2
6

(cid:0) 1
µ2
r

−

1

µ2

B({i,j})

(cid:1)

where µr is the scale of the root node and µB({i,j}) is the scale of the nest node corresponding to the
smallest partition containing both i and j.

Since the overall scale of the utility equations is not deﬁned, only |C| − 1 scale parameters may be
identiﬁed. We follow the convention of normalizing the scale of the root nest, µr, to 1. The nested
logit model is consistent with rational utility maximization only if the scale parameters increase with
increasing nesting levels; see B¨orsch-Supan (1990). Formally the scale parameters must satisfy:

Bm ⊆ Bm(cid:48) =⇒ µm(cid:48) ≤ µm

(2.1)

Note that this constraint on the scale parameters restricts that covariances to be non-negative (this
should be clear from the expression in Proposition 2.1). Intuitively, the covariance between two error
terms in a nested logit model is the variance of the nest speciﬁc error term of the shared nest which
can not be negative; see Williams and Ort´uzar (1982) for details and Dong et al. (2017) for modeling
implications. Furthermore by the same proposition, two alternatives are uncorrelated if the smallest
partition containing both is the universal set C (or equivalently their youngest common ancestor is the
root node). If a nested partition consists only of the universal set C, the nested logit model reduces to
the multinomial logit model. Figure 2 illustrates such a partition, and the corresponding tree structure
(c.f. Theorem 2.1) and covariance matrix (c.f. Proposition 2.1). Figure 3 shows another example for
the same choice set, but where the nested partition is given by {{a1, a2, a3, a4}, {a2, a3, a4}, {a3, a4}}.
Given a nested partition B over C, consider the probability Pn{j} of agent n ∈ I choosing alternative
j ∈ C. Using the product rule and conditioning over all the nests in B containing j in ascending order
of cardinality, we have:

Pn{j} = Pn{j|B(j)}Pn{B(j)|B2(j)} . . . Pn{Bk−1(j)|Bk(j)}Pn{Bk(j)}
Where Bs(.) is the s-fold application of the function B(.), and k is the number of partitions in B that
contain the alternative j. Note that Bk(j) = C, i.e., the largest set containing the alternative j is the
set C. Pn{j|B(j)} is the conditional probability of choosing j given that some alternative in B(j) is
chosen. This quantity is given by the relative attractiveness of alternative j compared to the maximum
utility obtainable by choosing some alternative in B(j), which we denote by Γn{B(j)}. The Inclusive
value, Γn{Bm}, of a nest Bm is its expected maximum utility to agent n (Ben-Akiva and Lerman,
1985), deﬁned as follows:

(2.2)

Γn{Bm} =

1
µBm

ln (cid:0) (cid:88)

eµBm Vjn +

(cid:88)

eµBm Γn{Bm(cid:48) }(cid:1)

(2.3)

j∈C|B(j)=Bm

Bm(cid:48) ∈B|Bm(cid:48) ⊆Bm

So that

Pn{j|B(j)} = exp(µB(j)(Vj − Γn{B(j)}))
Similarly, the probability of choosing nest Bs(j) conditional on choosing Bs+1(j), the smallest nest

(2.4)

6

Aboutaleb et al.

containing it, is given analogously to (2.3):

Pn{Bs(j)|Bs+1(j)} = exp(µBs+1(j)(Γn{Bs(j)} − Γn{Bs+1(j)}))

(2.5)

Finally, since some alternative in C has to be chosen, we have Pn{Bk(j)} = 1. Putting it all together,
we have

Pn{j} = exp

(cid:16)

µB(j)Vjn +

k−1
(cid:88)

(cid:17)
(µBi+1(j) − µBi(j))Γn{Bi(j)} − µBk(j)Γn{Bk(j)}

(2.6)

i=1

The closed-form (2.6), while mathematically tractable, is a rather complicated function involving nested
logs of sums of exponential terms. We discuss the implications of this on the solution algorithm in
Section 4.

In this section we reviewed the modeling ﬂexibility the nested logit model brings in the speciﬁcation
of the variance-covariance matrix of the error terms while maintaining closed-form expressions for the
choice probabilities. Our motivation is to algorithmically and systematically determine which of the
allowable variance-covariance matrix structures (as determined by the nesting trees) best ﬁts the data.
Working with the variance-covariance matrix directly however is not very attractive in the nested
logit framework because of the many constraints on which matrix structures are permissible in that
framework. We introduced notations and equivalences between three diﬀerent representations of nested
logit models namely (i) a nested partition based representation that was used to formulate the choice
probabilities, (ii) a tree based representation that was used to prove Theorem 2.1, and will be useful
in the optimization framework we introduce in the next section, and (iii) a variance covariance based
representation which helped us understand the ﬂexibility and limitations of the nested logit model.

3. THE NESTED LOGIT STRUCTURE LEARNING PROBLEM

The general framework we take for ﬁnding an optimal nesting tree T (representing a nesting partition)
and its parameters θT (the scale and taste parameters) is a mixed-integer non-linear program. At a
high level, the problem can be written as follows:

maxT,θT Model Likelihood
subject to T is a valid nesting tree

Scale parameters are non-decreasing with nesting level

Number of nests = M

Nesting levels = L

(3.1)

(3.2)

(3.3)

(3.4)

(3.5)

The objective is to optimize over all possible nesting tree speciﬁcations. This is achieved by maximizing
the model likelihood while penalizing model complexity to avoid over-ﬁtting. The constraints restrict
the search to valid nesting trees that satisfy the utility maximization constraints given by equation
(2.1).

Constraint (3.2) guarantees that T is a valid nesting tree which requires the prohibition of cycles
and enforcement of graph connectivity. Constraint (3.3) places restrictions on the scale parameters of
the tree. In order for the estimated parameters to be consistent with utility maximization the scale
parameters can not decrease with nesting level. The last two constraints, (3.4) and (3.5), limit the
complexity of the model. The optimal parameters M and L are determined through cross-validation
(by evaluating the likelihood on a hold out “validation” data-set).

In this section, we obtain a closed form expression for the likelihood (Section 3.1) as a function of
nesting structure and model parameters and mathematically formulate (3.2)-(3.5) as linear constraints
(Section 3.2). We end with a discussion on the regularization framework we adopt in Section 3.3.

Learning Nesting Structure

7

3.1. A Mixed-Integer Nonlinear Program Formulation

Let B be a non-degenerate nested partition of the set of alternatives C and consider the graph repre-
sentation T of B. T is a tree (cf. Theorem 2.1) and in building T from B we have:

1 A root node r representing the choice set C.
2 Internal nest nodes N each representing a nest in B \ C.
3 Leaf nodes representing each alternative in C.

Formally, T = (V, E) is a directed graph where V = {r} ∪ N ∪ C is the set of vertices and E the
set of edges. Let xu,v ∈ {0, 1} equal one if there is a directed edge between nodes u and v. There is a
directed edge between two nodes u and v in T only if node u is the nest or root node representing the
smallest nesting partition containing the nest or alternative represented by v. Formally:

xu,v = 1 ⇐⇒ B(u) = v
In learning T from the data, we do not know a priori the structure of the tree, or if nesting is
present. The goal is to use optimization to reveal this structure. Theorem 2.1, provides the following
guarantee:

(3.6)

|N | ≤ |C| − 2

(3.7)

For convenience let p = |C| − 2. Now, since any nested partition can be represented by at most p nest
nodes, we start with the nest node set N containing that maximum number of nest nodes (i.e., p), and
let an optimization procedure guide the inclusion or exclusion of these nests. To this end, we deﬁne
for every nest node v ∈ N , a binary decision variable yv ∈ {0, 1} equal to one if nest v is included in
nesting tree T. We shall collectively denote {xe, e ∈ E} by x. Similarly, we denote {yv, v ∈ N } by y.

3.1.1. Objective Function
In this section we obtain a closed form expression of the log-likelihood
function. Let cna be a binary variable indicating if agent n ∈ I chose alternative a ∈ C. Recall that
Van is the systematic utility of alternative a to agent n and is speciﬁed a (linear) function of model
parameters β, attributes, and socioeconomic variables. The probability of choosing some alternative
j ∈ C can be found by conditioning on the path from the root r to the leaf node j. Let {r −→ j}T
denote the set of all possible paths from r to j ∈ C on graph T. If T is a tree, the path is unique by
deﬁnition (Korte et al., 2012). Formally, {r −→ j}T is a set of sets of ordered sequence of nodes visited
on the path from r to j. For any given path l ∈ {r −→ j}T denote these nodes by b(1)
l where
l = r and b(s)
b(1)

l = j, where s is the length of the path l. We can now write the log-likelihood as:
(cid:88)

, ..., b(s)

(cid:88)

(cid:88)

(cid:16)

(cid:17)

l

L(x, β, µ) =

cna

xl ln Pn{a|l})

(3.8)

Where,

ln Pn{a|l} = µb(s−1)

l

Van +

s−2
(cid:88)

i=2

(µb(s−i)

l

n∈I

a∈C

l∈{r−→a}

− µb(s−i+1)

l

)Γn{b(s−i+1)
l

} + (µr − µb(2)

l

)Γn{b(2)

l } − µrΓn{r}

and Γn{v} is the inclusive value of internal node v ∈ N ∪ {r} for agent n:

Γn{b} =

1
µb

ln (cid:0) (cid:88)

a∈C

xbaeµbVan + xbb(cid:48)

(cid:88)

eµbΓn{b(cid:48)}(cid:1),

b(cid:48)∈N

(3.9)

(3.10)

The likelihood function (3.8) has an exponential number of terms and even for simple graphs can take
exponential time to evaluate “top-down” i.e., by enumerating all possible paths from the root to each
alternative. However at valid tree solutions, there is a unique path from the root to each alternative.
In such cases to compute the likelihood, for each individual n, one starts at the chosen alternative,

8

Aboutaleb et al.

cna, and follows the alternative’s ancestry adding to the utility of the alternative the inclusive values
along the unique path to the root and scaling appropriately. We discuss matters regarding the eﬃcient
evaluation of the likelihood in Section 4.2.1.

3.2. Constraints

There are two main types of constraints: constraints that guarantee (i) a valid nesting tree and (ii)
consistency with rational utility theory. There are also additional constraints on the structure of the
tree T. We discuss these individually in what follows. Crucially, we show that all the desired properties
can be enforced using linear constraints.

3.2.1. Arborescence A tree with m nodes must have exactly m − 1 edges (otherwise the addition of
an edge results in a cycle and the removal of an edge results in a disconnected graph). The following
constraint guarantees that the total number of edges is one less than the total number of nodes.
= (cid:0) (cid:88)

yu + |C| + 1(cid:1)

(3.11)

(cid:88)

−1

xe

e∈E
(cid:124) (cid:123)(cid:122) (cid:125)
Number of edges

u∈N

(cid:124)

(cid:123)(cid:122)
Number of nodes

(cid:125)

To guarantee that the graph is cycle free, any strict subset of the nodes of the tree must have at most
one less edge than the number of nodes in said subset. The following of constraints takes any potential
cycle and declares it illegal:

(cid:88)

xe ≤ |A| − 1, ∀A ⊂ V

{(u,v)∈E:u∈A,v∈A}

(3.12)

This formulation is based on the sub-tour elimination constraints from the classic travelling salesman
problem (Bertsimas and Tsitsiklis, 1997). Since the number of subsets that can be formed from a given
set is exponential in the size of the set, the number of cycle elimination constraints is exponential in
the number of nodes of the tree. We discuss a very practical and eﬃcient way of addressing this in
Section 4.2.2 using “lazy constraints”.

3.2.2. Scale Constraints
In order for the estimated model to be consistent with utility maximization
we require the scale parameters µ to increase with nesting level. The implication is that whenever
xuv = 1 we must have that µu ≤ µv. This can be enforced through the following constraints:

µu − ¯µ(1 − xuv) ≤ µv ∀ distinct u, v ∈ N

(3.13)

where ¯µ is an upper bound on the scale parameters.

3.2.3. Structure Constraints We describe a number of constraints on the structure of the graph. Let
u = {(u, v) ∈ E} denote the set of edges that originate in node u, and similarly let δin
δout
u = {(v, u) ∈ E}
denote the set of edges that terminate in node u

Each leaf node must belong to one and only one nest. The sum of edges incident to leaf nodes must

sum to unity:

Next, if a nest node is included, it must have exactly one parent:

(cid:88)

e∈δin
a

xe = 1 ∀a ∈ C

(cid:88)

e∈δin

v

xe = yv ∀v ∈ N

(3.14)

(3.15)

Furthermore, if a nest node is included it must have a directed edge to at least two nodes (so that it

is not degenerate), and if it is not included it cannot make connections to other nodes:

Learning Nesting Structure

2 · yu ≤

(cid:88)

e∈δout
u

xe ≤ |C| − 1 · yu ∀u ∈ N

9

(3.16)

Similarly, the root node can not be degenerate and must connect to at least two nodes in the tree

If the tree has nest nodes, at least one nest node should be connected to the root:

2 ≤

(cid:88)

xe

e∈δout
r

1 − (1 − δ) ≤

(cid:88)

u∈N

xru

(cid:88)

u∈N

yu ≤ |C| − 2 · δ,

where δ ∈ {0, 1} is a decision variable equal to one if there is at least one nest node in the tree.
Choice nodes must be leaf nodes and cannot have originating edges:

The root node r, by deﬁnition, can not have incident edges:

(cid:88)

e∈δout
a

xe = 0 ∀a ∈ C

Finally, we disallow self-arcs:

(cid:88)

e∈δout
r

xe = 0

xuu = 0 ∀u ∈ V

(3.17)

(3.18)

(3.19)

(3.20)

(3.21)

(3.22)

3.2.4. Regularization Constraints Finally, we discuss enforcing constraints (3.4) and (3.5). The num-
ber of nests in tree T can be constrained, with great facility, to equal any given integer M ∈ [0, p]
through the following expression:

(cid:88)

u∈N

yu = M

(3.23)

Enforcing (3.5) is a little more involved. First we establish a relationship between nesting level and tree
height. The height of a tree is the depth of its deepest node. The depth of a node is the number of edges
on the longest downward path between the root r and said node. We see that there is a one-to-one
equivalence between nesting level and tree height. As an example, the tree shown in Figure 3 has 3
nesting levels (the root, nest b1, and nest b2). We can also count 3 edges on the path from r to leaf
node a4 which is the deepest node of that tree, so the tree shown has height 3. To enforce a speciﬁc
tree height L, there has to be (i) at least one path between the root r and some leaf node a ∈ C with
L edges and (ii) no other path between the root and any node has strictly more than L edges.

Starting with (i), we notice that since any path with more than one edge from the root to a leaf
node must pass through some intermediate nest nodes, the requirement that “at least one downward
path between the root r and some leaf node a ∈ C has L edges” can be expressed as “at least one
downward path between the root r and some nest node v ∈ N has L − 1 edges”. Since the nests do
not have any intrinsic labels, we can enforce a path of this length, without loss of generality, using any
L − 2 nest nodes. For convenience we choose the lexicographically ﬁrst L − 2 nest nodes. Suppose the
nest node labels are ordered from nest1, ..., nestp, we enforce (i) as follows:

xr,nest1 = 1, xnesti,nesti+1 = 1 i = 1, ..., L − 3

(3.24)

Next, enforcing (ii) in a direct way requires enumerating all possible paths from the root to the
leaf nodes with more than L edges and excluding those paths from the feasible set. Clearly there is

10

Aboutaleb et al.

an exponential number of such paths and the direct method is untenable. Instead we enforce (ii) in
an indirect way using “lazy constraints”. In short the optimization problem is ﬁrst solved without
enforcing (ii). If the solution includes a violating path, such path is excluded and the problem is
resolved. More details can be found in Section 4.2.2.

3.3. Regularization

Regularization is a key concept in machine learning techniques. The idea is to appropriately penalize
complexity in the model to avoid ﬁtting to noise (i.e., over-ﬁtting). There are two questions here: what
to penalize and by what amount. The optimal amount of penalty can be decided by evaluating the
model subjected to various levels of regularization on a hold-out dataset. A model with an optimal
amount of penalty does best on data the training procedure hasn’t seen. This is a technique in machine
learning known as cross-validation; see Bishop (2006) for more details. In regards to what to penalize,
there are two ways of making a nested logit model more complicated (i) increasing the number of nests
(leading to more scale parameters to be estimated) (ii) increasing the nesting level (resulting in more
non-zeros in the variance covariance matrix of the error terms).

Typically the training likelihood is a non-decreasing function of complexity. For example, adding
regressors to a linear regression model cannot worsen the training likelihood. This is because the
training procedure can simply set the coeﬃcients of the added regressors to zero and obtain the same
likelihood as a model without the additional regressors. In the nested logit structure learning problem
however it turns out that we should not expect any trends on the likelihood of the training dataset. This
is somewhat counter-intuitive and is due to the fact that we disallow degenerate nests (see constraint
3.16). For more details, refer to Section A.1 of the appendix.

4. SOLUTION ALGORITHM

Section 3 introduced a formulation of the nested logit structure learning problem (NLSLP) as a mixed
integer non-linear program with linear constraints. In this section we ﬁnd a practical solution algorithm
to the NLSLP as introduced. The NLSLP brings with it several challenges which ultimately shape our
approach to ﬁnding a practical solution method.

First, the likelihood function can only evaluated at tree solutions. This is because the inclusive
values (3.10) are deﬁned recursively, and therefore the presence of any cycles will introduce circular
references. Furthermore, since the likelihood function, as written in (3.8), is deﬁned in terms of all
possible tree paths from the root to each of the leaf nodes, it is not possible to explicitly load the entire
function on a computer for problems of practical size. We consider a very eﬃcient method in Section
4.2.1 for evaluating the likelihood function at a tree solutions without enumerating all possible paths
from the root as the closed form expression for the likelihood might initially suggest. The inclusive
values introduce another complication, namely the coupling together of all the model parameters.
This precludes the possibility of using local search algorithms that rely on evaluating the eﬀect on the
likelihood of the inclusion or exclusion of an edge. In other words, the likelihood can not be decomposed
by “edge eﬀects”. If that were the case, it would suﬃce to use, for example, a maximum spanning tree
algorithm to arrive at the optimal structure as in Janhunen et al. (2017).

Second, the likelihood function we seek to maximize (3.1) is jointly non-concave in the discrete and
continuous decision variables. Once the discrete decision variables are ﬁxed, however, the problem is
reduced to the usual nested logit model estimation, which has been studied extensively in the econo-
metrics literature (Hensher and Greene, 2002) and (Brownstone and Small, 1989), with the addition of
the linear scale constraints for which several optimization techniques already exist including Lagrange
multipliers methods (Bertsekas, 1982) and conjugate gradient methods (Goldfarb and Lapidus, 1968).
Third, the number of constraints is exponential in the number of nodes of the graph. Recall that the
cycle elimination constraints (3.12) are applied to every subset of the nodes of the graph. Furthermore,
our regularization framework requires enforcing a speciﬁc tree height which also involves, in theory,

Learning Nesting Structure

11

an exponential number of constraints. We discuss an eﬃcient workaround using “lazy constraints” in
Section 4.2.2.

Finally, it is not possible to evaluate the likelihood function when the discrete variables are relaxed
(i.e., allowed to take on continuous values). Methods in the literature of obtaining concave relaxations
of functions rely on the ability to evaluate the likelihood at relaxations of the discrete variables; see for
example Scott et al. (2011). In the NLSLP however, the discrete decision variables represent structure
and can not be relaxed– an edge or a nest is either present or not.

In lieu of the above, we ﬁnd that the most appropriate solution methodology is through a variation
of the linear outer approximation algorithm introduced by Duran and Grossmann (1986a) and later
developed by Fletcher and Leyﬀer (1994). We view the discrete decision variables as complicating
variables and instead of solving the optimization problem in “one go”, we iteratively solve two easier
sub-problems. The ﬁrst sub-problem deals with estimating the nested logit model parameters for a ﬁxed
tree. The second sub-problem ﬁnds the most promising tree structure to pivot to at every iteration.
We discuss this procedure at length in the next section.

4.1. General Algorithm Overview

Denote the cardinality of the choice set C by m. Let q denote the number of parameters in the systematic
component of the utility equations. Let f (x, β, µ) = −L(x, β, µ). The NLSLP can be reformulated as
the following optimization problem:

z∗ = min
x,y,β,µ

f (x, β, µ)

s.t. x, y ∈ T

Cx + Bµ ≤ d
µ ∈ Rm−1, β ∈ Rq
x ∈ {0, 1}2m−1×2m−1
y ∈ {0, 1}m−2

(4.1)

(4.2)

(4.3)

(4.4)

(4.5)

(4.6)

Where T is the set of binary vectors (x, y) that satisfy the arborescence (3.11-3.12), structural (3.14-
3.22) and regularization constraints (3.23) and (3.5), and for some suitably deﬁned matrices C and B
and vector d that describe the scale constraints (3.13).

Now, for any feasible tree solution x(k), we deﬁne the nonlinear sub-problem, NLP(k), as

zNLP(x(k)) = min
β,µ
s.t. Cx(k) + Bµ ≤ d

f (x(k), β, µ)

µ ∈ Rm−1, β ∈ Rq

(4.7)

(4.8)

(4.9)

The nonlinear sub-problem NLP(k) ﬁnds optimal parameters β, µ for a given tree x(k). This problem
is simply a nested logit estimation problem with the addition of scale constraints. As the feasible set
of NLP(k) is a subset of the feasible set of the original problem, we have that for all x(k) ∈ T ,

z∗ ≤ zNLP(x(k))
In other words, the solution to any of the non-linear sub-problems NLP(k) provides a rigorous upper
bound on the objective function value of z∗. We refer to this problem as the upper bounding sub-
problem.

(4.10)

Next, we approximate the function f , as the maximum of its linear approximations around a set of
feasible solutions O(k) = {(x(1), β(1), µ(1)), ..., (x(k), β(k), µ(k))}. If f were a convex function then the
following problem, called the “master” mixed-integer linear program (MILP), always provides a lower
bound to the original optimization problem, i.e.,

zMILP

(k) ≤ z∗

(4.11)

12

Aboutaleb et al.

The “lower bounding” MILP master problem is given by:

zMILP

(k) = min

η,x,y,β,µ

η





x − x(i)
β − β(i)
µ − µ(i)

s.t. η ≥ f (x(i), β(i), µ(i)) + ∇f (x(i), β(i), µ(i))T

x ∈ T

Cx + Bµ ≤ d
µ ∈ Rm−1, β ∈ Rq
x ∈ {0, 1}2m−1×2m−1
y ∈ {0, 1}m−2


 ∀(x(i), β(i), µ(i)) ∈ O(k)

(4.12)

(4.13)

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

The representation above is the so-called epigraph formulation, where the objective function f is
moved out of the objective into the feasible set. If f is convex the linearizations around the set of points
O(k) overestimate the feasible region and we obtain a lower bound on the objective function value as
stated in (4.1). f is not convex (since −f = L is not concave), and the tangent hyper-planes (4.13)
are not necessarily global under-estimators of f . Consequently, the MILP problem above may cut oﬀ
regions of the feasible space. An established heuristic to overcome this is to allow the linearizations to
move away from the feasible region. This is done through the use of artiﬁcial non-negative variables
that are penalized in the objective; see Viswanathan and Grossmann (1990).

In summary, the linear outer approximation solves the original optimization problem (4.1)-(4.6)
by iteratively solving a sequence of two easier problems: an MILP master problem (4.12)-(4.18) and
an NLP sub-problem (4.7)-(4.9). Algorithm 1 describes the steps of the linear outer approximation
procedure as it applies to the NLSLP.

Algorithm 1: Linear Outer Approximation for the NLSLP

Step 1. Find an initial tree solution with M nests and L nesting levels, call it x(1).
Step 2. Estimate the taste β(1) and scale µ(1) parameters for the tree x(1) found in step 1 by
solving NLP(1).
Step 3. Form a set of visited solutions O(1) = {(x(1), β(1), µ(1))}. Evaluate f (x(1), β(1), µ(1))
and ∇f (x(1), β(1), µ(1)) and form the linearization constraints (4.13).
Step 4. Solve the Master MILP(1) (4.12)-(4.18) to obtain a new solution x(2) .
Step 5. Solve NLP(2), augment the set of visited solutions with the newly optimal solution
O(2) = O(1) ∪ {(x(2), β(2), µ(2))}, form the linearization constraints (4.13)
Step 6. Solve MILP(2) to pivot to a new solution.
Step 7. Continue iterating between NLP(k) and MILP(k) until the MILP problem is infeasible
or a termination criteria is met.
Step 8. Evaluate the likelihood (3.8) on training and validation data-sets.
Step 9. Repeat steps 1 to 8 for all feasible combinations of M and L.
Step 10. Choose the speciﬁcation with the best validation likelihood.
Step 11. Estimate a nested logit model with optimal nesting structure speciﬁcation as
determined from step 10 on the full dataset.

A few remarks on Algorithm 1 are in order. It is easy to ﬁnd an initial tree solution with the desired
number of nests and nesting levels to start the algorithm in step 1. Algorithmically, this can be done
by modifying the MILP by replacing (4.13) with the constraint η ≥ 0 and removing constraints (4.15)
and (4.16).

Whenever the MILP is solved additional constraints are added to cut-oﬀ previously found trees and

Learning Nesting Structure

13

all trees in their “equivalence class” to guarantee ﬁnite convergence and prevent cycling behavior. In
our implementation the nests are labeled, however the nest labels have no eﬀect on the likelihood. When
cutting a previously visited tree, one must also cut, from the feasible set, all trees in its equivalence
class, i.e., all trees such that when the nest labels are removed, the resulting tree structure is the same.
The exact form of the cuts is discussed in Section 4.2.2.

In practice, the linear outer approximation may take a large number of iterations to converge. Typ-
ically however, the majority of the optimality gap (the diﬀerence in objective function value between
the NLP and MILP) is closed during the ﬁrst few iterations, and a commonly used termination criteria
is iteration limit. Another criteria is the worsening of the objective function value of two successive
NLP sub-problems Duran and Grossmann (1986b). We use an iteration limit in our implementation.
As linearizations are added in step 5, the master MILP becomes an improved approximation of the
original optimization problem. Convergence to an optimum occurs when the value of the master MILP
is worse that the value associated with the NLP subproblem, and the optimum is guaranteed to be a
global optimum if the function f is convex. Since f is not a convex function convergence to a global
optimum cannot be guaranteed.

4.2. Practical Matters

In this section we discuss a few practical implementation details. Crucial to the outer approximation
algorithm is the ability to evaluate the value of the function f and its gradients ∇f at points in the
feasible set. Since the likelihood function (3.8) is deﬁned in terms of all possible paths from the root
to the leaf nodes, direct evaluation of this function is prohibitive for any choice set where the number
of alternatives is not very small. Fortunately, the tree structure of the problem can be exploited as a
workaround as we see in Section 4.2.1.

In Section 4.2.2, we discuss an eﬃcient method for dealing with the exponentially many constraints
in the master MILP. We end with a note on the code implementation of Algorithm 1 in section 4.2.3.

4.2.1. Evaluating the likelihood and its gradients

Eﬃcient evaluation of the likelihood function Evaluating the likelihood function (3.8), “top-
down” would require enumerating all paths l ∈ {r −→ a} for each a ∈ C - which is a prohibitive task
for choice sets of practical size. In fact a short proof (see Section A.3 of the Appendix) reveals that
the total number of terms in (3.8) is |I| · |C| · (cid:98)|C|! · e(cid:99). Instead, consider the following algorithm for

eﬃciently computing the term

in (3.8) at tree solutions for a ﬁxed n ∈ I

(cid:16)

cna

(cid:80)

(cid:17)
l∈{r−→a} xl ln P (a|l)

and a ∈ C. At such solutions, there is a single unique active path l ∈ {r −→ a} for each a ∈ C. Fix a ∈ C,
and denote its path by the set of nodes b(1)
l = a, where s is the length
of the path l which we do not necessarily know a priori. Algorithm 2 describes an eﬃcient method of
calculating the likelihood (3.8) at tree solutions:

l = r and b(s)

l where b(1)

, ..., b(s)

l

Algorithm 2: Eﬃcient evaluation of the likelihood at tree solutions

Step 1. For each n ∈ I and a ∈ C do steps 2 to 5.
Step 2. If cna = 1 continue to step 2, otherwise the contribution to the likelihood is zero.
Step 3. Start at a leaf node a and propagate to the node’s parent B(a). Add to the likelihood,
the quantity µB(a)Van.
Step 4. If the current node is the root node add the quantity −µrΓn{r} to the likelihood and
stop. Otherwise, propagate to the current node’s parent B(B(a)) and add the following
quantity to the likelihood (µB(B(a)) − µB(a))Γn{B(a)}. Set the current node to B(a).
Step 5. Continue adding contributions as in step 4 until the root node is reached.

14

Aboutaleb et al.

Computing gradients Central to the linear outer approximation algorithm is the availability of
gradients of the likelihood function at speciﬁed tree solutions. We make a distinction here between the
continuous variables β and µ, and the discrete variables x.

Derivatives of the likelihood function L with respect to continuous variables can be computed ana-
lytically or through automatic diﬀerentiation (Baydin et al., 2017). Automatic diﬀerentiation however,
can not reliably handle derivatives with respect to discrete decision variables. We resort to analyt-
ical diﬀerentiation and ﬁnd that closed form derivatives exist, and can be eﬃciently evaluated at
tree solutions using variations of Algorithm 2. These derivatives are somewhat involved, the complete
derivations can be found in Section A.2 of the appendix.

4.2.2. Lazy constraints and formulations Lazy constraints make it possible to leave out constraints
from an optimization problem that must be satisﬁed by any valid solution but which may not be
required by the solution algorithm to arrive at the optimal solution – resulting in a smaller problem
that is easier to solve. Only those constraints that are required for ﬁnding the optimal solution are
included, and only when they are needed (Pearce, 2019).

In the NLSLP, the cycle elimination constraints and the anti-cycling constraints are exponentially
sized in the cardinality of the choice set. The tree height constraints are diﬃcult to explicitly describe
but a violating solution can be easily detected and removed from the feasible set. We describe lazy
constraint formulations to address these points.

Cycle elimination constraints The cycle elimination constraints (3.12) are necessary to avoid
passing non-tree solutions to the NLP sub-problem. There is one such constraint for every strict subset
of the nodes of the graph, consequently if a graph has N nodes there is 2N − 1 of these constraints.
Generating and adding these constraints at once to the MILP is not practical. Instead we take a lazy
constraint approach. Since it is easy to detect the presence of cycles in current solutions and produce
constraints to remove such solutions from the feasible set we generate and add only the required cycle
elimination constraints “on the ﬂy” as needed.

The MILP problem is ﬁrst solved without the cycle elimination constraints. A “separation oracle”
is then used to determine which of cycle elimination constraints are violated. Only the violated con-
straints are added and the MILP is re-solved. It is important that the separation oracle is eﬃcient at
detecting constraint violations. A naive exponential time oracle will generate and check each of the
cycle elimination constraints for violations. Instead, it suﬃces to run a depth ﬁrst traversal of the
graph and check if that yields any back edges. This algorithm has a polynomial worst-case run time;
see Cormen et al. (2009).

Anti-cycling constraints

Since nest labels have no eﬀect on the likelihood, there is in fact an
equivalence class of tree solutions. As an example Figure 4 shows two trees representing the same
nesting structure with diﬀerent labeling of the nests. Visiting either of the trees necessitates removing
the other from pivot consideration in step 6 of Algorithm 1. In general suppose a tree solution has N
nests, then there exists N ! trees in its equivalency class (corresponding to all possible permutations of
the nest labels). It is therefore crucial to avoid pivoting to trees that belong to the equivalence classes
of any previously visited solution (otherwise Algorithm 1 may cycle through all N ! trees for a given
class before pivoting to a truly new solution). All trees in the same equivalence class share a common
“signature” -namely the ancestry of the elementary alternatives. The ancestry of a current solution
is used to determine if it belongs to the equivalence class of a tree that has been previously visited.
That solution is then removed from the feasible set (4.14) accordingly through the constraints we now
describe.

Suppose we wish to exclude a particular tree solution x from the feasible set. Deﬁne the index sets
O = {i : xi = 1} and Z = {i : xi = 0}. The following constraint removes the solution x from the
feasible set:

(cid:88)

i∈O

xi −

(cid:88)

i∈Z

xi ≤ |O| − 1

(4.19)

Again, we take a lazy constraint approach. These cuts are not generated at once but are instead added
on the ﬂy as needed.

Learning Nesting Structure

15

Tree height constraints The regularization framework (3.4)-(3.5) of the NLSLP involves limiting
the nesting levels (or equivalently the height of the nesting tree). We noted in section 3.2.4 that to
enforce a speciﬁc tree height L, we require that (i) at least one path between the root r and some leaf
node a ∈ C has L edges and (ii) no other path between the root and any node has strictly more than L
edges. We elaborate on the lazy constraint approach we apply to enforcing (ii). At a given tree solution
we traverse to the root node starting from each of the elemental alternatives counting the number of
edges along the way. If a path of length strictly greater than L is found, the tree solution is eliminated
from the feasible set using constraints (4.19).

4.2.3. Implementation The nested logit structure learning problem has been implemented in the
Julia programming language (Bezanson et al., 2017). The Gurboi solver (Gurobi Optimization, 2015)
is used to solve the MILP master problems. The JuMP interface (Dunning et al., 2017) in Julia allows
for user-deﬁned lazy cuts, which is critical to our implementation as described in section 4.2.2. IPOPT
(W¨achter and Biegler, 2006) is used to solve the NLP sub-problems. IPOPT applies an interior point
algorithm to solve the linearly constrained NLP sub-problems. The authors make the source code
accessible under an MIT licence through github.com/ymedhat95/nested-logit.git.

5. COMPUTATIONAL EXPERIMENTS

In this section we validate Algorithm 1 introduced in Section 4.1. The algorithm is used to learn an
optimal nesting tree speciﬁcation empirically from the data. In Section 5.1, we demonstrate, through a
Monte Carlo experiment that Algorithm 1 can correctly recover the true tree structure as the sample
size increases. In Section 5.2, we apply our algorithm a travel mode choice dataset from Viegas de
Lima et al. (2018).

5.1. Monte Carlo Experiments

The purpose of this section is to demonstrate the statistical consistency of our methodology of learning
a nested logit structure from the data. To this end, we simulate a synthetic choice scenario as follows.
We consider a population of rational agents n ∈ I each making a decision from a set of eight alternatives
C = {a1, ..., a8}. The utility of alternative a ∈ C to agent n ∈ I is given through the following utility
equations:

Una = ASCa + (cid:15)na
(5.1)
Where ASCa is an alternative speciﬁc constant for alternative a. The error terms, (cid:15)na, are independent
across agents but are correlated for the same individual across the diﬀerent alternatives. The correlation
structure of the error terms is described by the tree shown in Figure 5. The nesting tree and associated
scale parameters shown in Figure 5 imply a variance-covariance matrix for the joint distribution of
the error terms, (cid:15)na1, ..., (cid:15)na8 for a given agent. This variance-covariance matrix and values for the
alternative speciﬁc constants are shown in Table 1. For identiﬁcation purposes the ASC of the ﬁrst
alternative is normalized to zero. Two blocks of non-zero covariances (highlighted) can be seen in Table
1. The goal is to use Algorithm 1 to correctly recover the values for the ASCs and the elements of the
variance-covariance matrix.

Synthetic data was generated according to the choice scenario and parameters just described for 35
repetitions for various sample sizes of agents: (a) 25,000 (b) 10,000 and (c) 5000. For every repetition
we used Algorithm 1 to learn the nesting tree, its parameters and estimate the ASCs from the data.
We then used Proposition 2.1 to obtain the equivalent variance-covariance matrix representation of the
estimated tree structure and scale parameters. The estimated variance-covariance matrix and ASCs
where then averaged over the 35 diﬀerent repetitions to obtain the means and standard errors for the
various sample sizes.

16

Aboutaleb et al.

The results are shown in Table 2. For the largest sample size, (a), we can see that Algorithm 1
correctly recovers the underlying tree structure and its parameters in all 35 repetitions. For (b) the
algorithm sometimes misses nests n1 and n2 thus shrinking some of the signiﬁcant covariances. Once
or twice, the algorithm captures some spurious correlations as indicated by the very small non-zeros
shown in the unshaded blocks with large standard errors. Similar but more pronounced eﬀects are
observed for the smallest sample size (c).

5.2. Empirical Application

Data from the 2010 Massachusetts Travel Survey (MTS) and matrices for car and transit travel times
and costs, provided by Bostons Central Transportation Planning Staﬀ (CTPS) (de Lima et al., 2018),
are used to estimate a nested logit choice model for the work travel mode. Individuals were asked to
ﬁll out all activities performed on a designated weekday, and to provide the activity location and the
transport mode used to arrive at this location. The survey also collected individual and household
characteristics for participants. There are six main travel modes reported in the survey: Walk, Bike,
Car, Car Pool (2 people), Car Pool (3+ people), and Transit.

The systematic utilities for each of these travel modes are speciﬁed in Table 3. The speciﬁcation is
linear in parameters and includes a constant (intercept). The travel mode attributes travel time, travel
cost, an indicator of whether the trip is made to the central business district (CBD) of Boston as well as
traveler’s income and gender also enter the utility equations. There is an alternative speciﬁc coeﬃcient
β for each travel mode attribute and each traveler characteristic. For purposes of identiﬁcation, all
alternative speciﬁc coeﬃcients for the walk alternative (except the travel time coeﬃcient) are ﬁxed
to zero. All six alternative speciﬁc travel time coeﬃcients are identiﬁable because the travel time
varies over the six alternatives, this is not true of the other variables (e.g. income). A normalization is
therefore needed.

The goal is to estimate a nested logit model (structure and parameters), consistent with utility
maximization, from the data. Algorithm 1 is used. Table 4 shows the training and validation negative
log-likelihood at converged solutions for all feasible combinations of the regularization parameters M
and L in Algorithm 1. The best performing model on the validation dataset is the tree with 4 nests and
5 nesting levels shown in Figure 6 (left). The model with the best validation log likelihood is chosen.
The nested logit model was then estimated for this tree on the full dataset. The estimated parameters
are shown in Table 5.

The estimated alternative speciﬁc constants C show that, ceteris paribus, the bike, car pooling,
and transit travel modes are less preferred than the walk mode, while the car mode is slightly more
preferred. Travelers are most sensitive to walk and bike travel times and least sensitive to car travel
time. The possession of a transit pass makes a traveler more likely to bike or take transit and less likely
to travel by car or car pool than walk. Trips to the CBD are more likely to be made by transit and
less likely to be made by car, car pooling or bike relative to walk.

The complicated nesting structure in Figure 6 (left) reveals nonzero correlations between the error
terms in the utility equations of all the travel modes except for the carpool2 mode. Looking at the
estimated scale parameters on the full dataset shown in Table 5 we notice that the scale parameter for
n1, 1.215, is not signiﬁcantly diﬀerent from the scale parameter for the root node which is normalized
to 1. Consequently n1 can be collapsed to the root node. Similarly the scale parameter for n3 is not
statistically diﬀerent from the scale parameter of n2 and the former nest can be collapsed into the
latter. The resulting tree shown in Figure 6 (right) is simpler and an attempt at “labeling” the nests
has be made. The ﬁnal nesting tree groups the car, transit, walk, and bike modes in a nest “Traditional”
modes of transportation. Within that nest, the walk and bike modes are grouped into another nest for
“Active” travel modes.

Figure 7 shows two alternative models that would typically be estimated for a travel model choice
on the basis of intuition. The ﬁrst model, Model A, groups the walk and bike travel modes into an
“Active” nest and the car and car pooling modes into an “Auto” nest. The second model, Model B,
groups transit and the car pooling modes into a “Shared” nest and the walk, bike, and car travel

Learning Nesting Structure

17

modes in to an “Unshared” nest. Table 6 compares the ﬁt on the data of these two models to the one
obtained through Algorithm 1. The multinomial logit (no nesting) model is used as a benchmark.

It can be seen that the model obtained through Algorithm 1 provides a much superior ﬁt than
the rest of the models. A total of 45 nesting trees visited during the search for the this tree (this
is the number of NLP sub-problems solved in Algorithm 1), a small fraction of the 2712 possible
non-degenerate nesting trees that could be formed with 6 alternatives.

6. CONCLUDING REMARKS

Since its introduction, the nested logit model has been found to be extremely ﬂexible with a myriad
of applications in economics, marketing, and transportation. A large number of alternative nesting
structures are possible in any choice context in which the number of alternatives is not very small. An
appropriate method for systematically choosing an optimal nesting structure has not yet appeared in
the literature. The dependency of the optimal structure on the systematic speciﬁcation complicates
this task yet further. While a priori reasoning may be of some help, results in the literature have
suggested that intuition is an imperfect guide.

In this work we provide an optimization-based framework for learning an optimal nesting structure
from the data. We formally introduced the nested logit structure learning problem (NLSLP) and
provided a complete solution algorithm in addition to an open-source code implementation available
to practitioners. Our holistic view of nested logit estimation takes into consideration the dependency of
the optimal structure on the systematic speciﬁcation of the utility equations. While the optimization
may appear intractable at ﬁrst glance, we have found ways to exploit the tree structure of the problem
and utilized the state-of-the art in algorithmic development in solving mixed integer optimization
problems and lazy constraints to bring practical tractability to the NLSLP.

We have demonstrated the eﬃcacy of our algorithm by applying it to a synthetic dataset where
the correct nesting tree structure was successfully recovered. Finally, we demonstrated an empirical
application to a travel mode choice dataset.

REFERENCES

Aboutaleb, Y. M., M. Danaf, Y. Xie, and M. Ben-Akiva (2020). Sparse covariance estimation in logit

mixture models. The Econometrics Journal (to appear).

Baydin, A. G., B. A. Pearlmutter, A. A. Radul, and J. M. Siskind (2017). Automatic diﬀerentiation

in machine learning: a survey. The Journal of Machine Learning Research 18 (1), 5595–5637.

Ben-Akiva, M. E. (1973). Structure of passenger travel demand models. Ph. D. thesis, Massachusetts

Institute of Technology.

Ben-Akiva, M. E. and S. R. Lerman (1985). Discrete choice analysis: theory and application to travel

demand, Volume 9. MIT press.

Benson, A. R., R. Kumar, and A. Tomkins (2016). On the relevance of irrelevant alternatives. In

Proceedings of the 25th International Conference on World Wide Web, pp. 963–973.

Bertsekas, D. P. (1982). Constrained optimization and Lagrange multiplier methods. Academic press.
Bertsimas, D. and J. Dunn (2017). Optimal classiﬁcation trees. Machine Learning 106 (7), 1039–1082.
Bertsimas, D. and A. King (2016). Or foruman algorithmic approach to linear regression. Operations

Research 64 (1), 2–16.

Bertsimas, D., A. King, et al. (2017). Logistic regression: From art to science. Statistical Science 32 (3),

367–384.

Bertsimas, D., A. King, R. Mazumder, et al. (2016). Best subset selection via a modern optimization

lens. The annals of statistics 44 (2), 813–852.

Bertsimas, D., R. Mazumder, et al. (2014). Least quantile regression via modern optimization. The

Annals of Statistics 42 (6), 2494–2525.
Bertsimas, D. and J. N. Tsitsiklis (1997).

Scientiﬁc Belmont, MA.

Introduction to linear optimization, Volume 6. Athena

Bezanson, J., A. Edelman, S. Karpinski, and V. B. Shah (2017). Julia: A fresh approach to numerical

computing. SIAM review 59 (1), 65–98.

18

Aboutaleb et al.

Bierlaire, M. (1998). Discrete choice models. In Operations research and decision aid methodologies in

traﬃc and transportation management, pp. 203–227. Springer.

Bishop, C. M. (2006). Pattern recognition and machine learning. springer.
B¨orsch-Supan, A. (1990). On the compatibility of nested logit models with utility maximization.

Journal of Econometrics 43 (3), 373–388.

Brownstone, D. and K. A. Small (1989). Eﬃcient estimation of nested logit models. Journal of Business

& Economic Statistics 7 (1), 67–74.

Cormen, T. H., C. E. Leiserson, R. L. Rivest, and C. Stein (2009). Introduction to algorithms. MIT

press.

Daganzo, C. F. and M. Kusnic (1993). Two properties of the nested logit model. Transportation

Science 27 (4), 395–400.

Daly, A. (1987). Estimating tree logit models. Transportation Research Part B: Methodological 21 (4),

251–267.

de Lima, I. V., M. Danaf, A. Akkinepally, C. Azevedo, and M. Ben-Akiva (2018). Modelling framework
and implementation of activity-and agent-based simulation: Application to the greater boston area.
In Transportation Research Board 97th Ann. Meeting.

Dong, H., E. Ben-Elia, C. Cirillo, T. Toledo, and J. N. Prashker (2017). On negative correlation: a
comparison between multinomial probit and gev-based discrete choice models. Transportmetrica A:
Transport Science 13 (4), 356–379.

Dunning, I., J. Huchette, and M. Lubin (2017). Jump: A modeling language for mathematical opti-

mization. SIAM Review 59 (2), 295–320.

Duran, M. A. and I. E. Grossmann (1986a). An outer-approximation algorithm for a class of mixed-

integer nonlinear programs. Mathematical programming 36 (3), 307–339.

Duran, M. A. and I. E. Grossmann (1986b). An outer-approximation algorithm for a class of mixed-

integer nonlinear programs. Mathematical programming 36 (3), 307–339.

Fletcher, R. and S. Leyﬀer (1994). Solving mixed integer nonlinear programs by outer approximation.

Mathematical programming 66 (1-3), 327–349.

Galichon, A. (2019). On the representation of the nested logit model. arXiv , arXiv–1907.
Goldfarb, D. and L. Lapidus (1968). Conjugate gradient method for nonlinear programming problems

with linear constraints. Industrial & Engineering Chemistry Fundamentals 7 (1), 142–151.

Greene, W. H. (2003). Econometric analysis. Pearson Education India.
Gurobi Optimization, I. (2015). Gurobi optimizer reference manual. URL http://www. gurobi. com.
Hausman, J. and D. McFadden (1984). Speciﬁcation tests for the multinomial logit model. Economet-

rica: Journal of the Econometric Society, 1219–1240.

Hensher, D. A. and W. H. Greene (2002). Speciﬁcation and estimation of the nested logit model:

alternative normalisations. Transportation Research Part B: Methodological 36 (1), 1–17.

Janhunen, T., M. Gebser, J. Rintanen, H. Nyman, J. Pensar, and J. Corander (2017). Learning discrete
decomposable graphical models via constraint optimization. Statistics and Computing 27 (1), 115–
130.

Koppelman, F. S. and C. Bhat (2006). A self instructing course in mode choice modeling: multinomial

and nested logit models.

Koppelman, F. S. and C.-H. Wen (1998a). Alternative nested logit models: structure, properties and

estimation. Transportation Research Part B: Methodological 32 (5), 289–298.

Koppelman, F. S. and C.-H. Wen (1998b). Nested logit models which are you using? Transportation

Research Record 1645 (1), 1–7.

Korte, B., J. Vygen, B. Korte, and J. Vygen (2012). Combinatorial optimization, Volume 2. Springer.
location. Transportation Research
McFadden, D. (1978). Modeling the choice of residential

Record (673).

McFadden, D. and K. Train (1978). An application of diagnostic tests for the independence from

irrelevant alternatives property of the multinomial logit model.

Pearce, R. H. (2019). Towards a general formulation of lazy constraints.
Scott, J. K., M. D. Stuber, and P. I. Barton (2011). Generalized mccormick relaxations. Journal of

Global Optimization 51 (4), 569–606.

Learning Nesting Structure

19

Vasudev, C. (2006). Graph theory with applications. New Age International.
Viegas de Lima, I., M. Danaf, A. Akkinepally, C. L. De Azevedo, and M. Ben-Akiva (2018). Modeling
framework and implementation of activity-and agent-based simulation: an application to the greater
boston area. Transportation Research Record 2672 (49), 146–157.

Viswanathan, J. and I. E. Grossmann (1990). A combined penalty function and outer-approximation

method for minlp optimization. Computers & Chemical Engineering 14 (7), 769–782.

W¨achter, A. and L. T. Biegler (2006). On the implementation of an interior-point ﬁlter line-search

algorithm for large-scale nonlinear programming. Mathematical programming 106 (1), 25–57.

Williams, H. and J. d. D. Ort´uzar (1982). Behavioural theories of dispersion and the mis-speciﬁcation

of travel demand models. Transportation Research Part B: Methodological 16 (3), 167–219.

20
Aboutaleb et al.
Table 1: Parameter values of the alternative speciﬁc constants and the variance-covariances of the error
terms used to generate the synthetic data.

Note: The variance-covariance values are given by π2

6 times the entries shown in the table (c.f. Proposition 2.1).

Table 2: Mean parameter values and standard errors for the alternative speciﬁc constants and the
recovered variance-covariance matrix for diﬀerent sample sizes.

(a) Ntrain = 20000, Nvalidation = 5000

Learning Nesting Structure
21
Table 2 (continued): Mean parameter values and standard errors for the alternative speciﬁc constants
and the recovered variance-covariance matrix for diﬀerent sample sizes.

(b) Ntrain = 7500, Nvalidation = 2500

(c) Ntrain = 3750, Nvalidation = 1250

Note: Means and standard errors where calculated across 35 repetitions for each sample size. Standard errors are shown
in brackets below the corresponding mean. The variance-covariance values are given by π2
6 times the entries shown in
the table (c.f. Proposition 2.1).

22

Aboutaleb et al.
Table 3: Systematic speciﬁcation of the utility equations for the work travel mode choice model.

Attributes & Characteristics

Constant Travel Time Travel Cost Transit Pass CBD Trip
βtc

βcbd

βtp

C

e
v
i
t
a
n
r
e
t
l
A

walk
bike
car
carpool2
carpool3+
transit

0
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

βtt
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
-
(cid:88)
(cid:88)
(cid:88)
(cid:88)

0
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

0
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Income Female
βf emale

βinc

0
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

0
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Note: A check mark indicates that the attribute or characteristic enter the utility function of the corresponding alter-
native linearly. A zero indicates that the corresponding parameter is ﬁxed to zero for identiﬁcation. All parameters are
alternative speciﬁc.

Table 4: Training and validation log-likelihoods at converged solutions for all feasible combinations of
number of nests and nesting levels. The best validation log-likelihood is underlined.

Training Negative Log-likelihood −L

Validation Negative Log-likelihood −L

{{2, 3}, {1, 2, 3}}

{{1, 2}, {1, 2, 3}}

{{1, 3}, {1, 2, 3}}

{{1, 2, 3}}

Figure 1: The four possible non-degenerate nesting partitions for the set {1, 2, 3}.

Learning Nesting Structure
Table 5: Estimated parameters for the optimal nesting structure for work mode choice model.

23

C

βtt

βtc

car

bike

walk

0
-
-.802
(.2478)
.557
(.1258)
-1.478
(.1467)
carpool3+ -1.810
(.2805)
-.783
(.1760)

carpool2

transit

-2.364
(.4466)
-2.587
(.5203)
-.399
(.3016)
-.443
(.4416)
-.454
(.5165)
-.840
(.1415)

-.101
(.0329)
-.265
(.0924)
-.420
(.1703)
-.022
(.0117)

e
v
i
t
a
n
r
e
t
l
A

Estimated Parameters
βinc
βcbd
βtp

0
-
.041
(.0701)
-1.001
(.1533)
-1.074
(.1445)
-1.156
(.1886)
.829
(.1394)

0
-
-.307
(.1301)
-.977
(.1664)
-1.022
(.1742)
-.926
(.2106)
.148
(.1258)

0
-
.006
(.0054)
-.017
(.0065)
.010
(.0078)
.010
(.0085)
-.022
(.0072)

βf emale

0
-
-.284
(.1215)
.128
(.0871)
.423
(.1055)
.416
(.1198)
.113
(.0928)

n1

n2

n3

n4

t
s
e
N

µ

1.215
(.1498)
1.747
(.1954)
1.847
(.2966)
4.350
(1.6591)

Note: The nesting speciﬁcation for the estimated model is shown in Figure 6 (left) and the systematic speciﬁcation is
shown in Table 5. The scale parameter of the root node, µr, is normalized to 1. Standard errors are shown in brackets
below the corresponding estimate.

Table 6: Comparison of ﬁt on the work travel mode choice dataset between diﬀerent nesting speciﬁca-
tions.

Tree

No. of Nests

Negative Log-likelihood −L
Full dataset Training Validation

No. of Visited Trees

Algorithm 1
Figure 6 (left)
Model A
Figure 7 (left)
Model B
Figure 7 (right)
Multinomial

4

2

2

0

Sample Size

7492

7523

7487

7511

8936

5615

5642

5617

5637

6702

1874

1887

1888

1883

2234

45

1

1

1

Total Possible: 2712

Note: The training to validation split ratio is 3:1. The validation log-likelihood is obtained by evaluating the log-likelihood
(3.8) on the validation data using parameters estimated on the training data.



π2

6µ2

r

a1
1
0
0
0

a2
0
1
0
0

a3
0
0
1
0

a4
0
0
0
1






a1
a2
a3
a4

Σ =

Figure 2: The nested partition B = {{a1, ..., a4}} over the set C = {a1, ..., a4} and the equivalent
nesting tree and variance-covariance matrix of the joint distribution of the error terms (cid:15)a1 , ..., (cid:15)a4 for
a multinomial logit model.

24

Aboutaleb et al.

Σ =




π2


6µ2

r


a1
1
0

0

0

a2
0
1
1 − µ2
r
µ2
b1
1 − µ2
r
µ2
b1

a3
0
1 − µ2
r
µ2
b1

1
1 − µ2
r
µ2
b2

a4
0
1 − µ2
r
µ2
b1
1 − µ2
r
µ2
b2









1

a1
a2

a3

a4

Figure 3: The nested partition B = {{a1, a2, a3, a4}, {a1}, {a2, a3, a4}, {a3, a4}} of the set C =
{a1, a2, a3, a4} and the equivalent nesting tree and variance-covariance matrix of the joint distribu-
tion of the error terms (cid:15)a1, ..., (cid:15)a4 for a nested logit model.

Figure 4: Trees representing the same nesting structure with diﬀerent nest labels. These two trees
belong to the same equivalency class.

Learning Nesting Structure

25

Figure 5: The nesting tree encoding the correlation structure of the synthetic data. The scale parameters
are shown below the respective nests. The root scale parameter is normalized to one.

Figure 6: The learned nesting structure for the work travel model dataset (left) and a possible inter-
pretation of the tree after collapsing nests with statistically insigniﬁcant scale parameters on the full
dataset.

26

Aboutaleb et al.

Figure 7: Alternative models for the work travel mode choice dataset. Model A (left) and Model B
(right)

Learning Nesting Structure

A. APPENDIX

A.1. More on regularization

27

We look at three possible cases and provide counter examples that show that the training likelihood
can worsen by increasing complexity as deﬁned by the number of nests and the nesting level:

Case 1. Increasing the number of nests by 1, while holding the nesting level constant. Consider the
optimal tree with 2 nests and height 2 shown in the left of Figure A.1. In this example, we are
able to increase the number of nests by 1 without increasing the nesting level by grouping leaf
nodes 3 and 4 in a new nest c. The likelihood cannot worsen, as the estimated scale of the new
nest c can be made equal to the normalized root scale– eﬀectively setting the covariance between
the error terms of alternatives 3 and 4 to zero (c.f. Proposition 2.1).
However, it is not always possible to increase the number of nests without worsening the training
likelihood. As an example, consider the optimal tree with 2 nests and 2 nesting levels shown in
Figure A.2, and consider the addition a third nest c to this tree. It is not possible to add this
nest without either changing the alternative to nest allocations, increasing the nesting level, or
running into degeneracy. Therefore, there can be no guarantee that the likelihood will not worsen.
Case 2. Increasing the nesting level by 1, while holding the number of nests constant: Since increasing
the nesting level without adding any additional nests would entail changing the nesting structure
of a present nest or nests, there is clearly no guarantee that the likelihood cannot worsen.
Case 3. Increasing the number of nests and the nesting level each by 1 : Consider the optimal tree
with a single nest and 2 nesting levels shown in the left of Figure A.3. Leaf nodes 3, 4, and 5
can be nested together in a new nest b with the same scale parameter as nest a increasing the
number of nests and the nesting level without worsening the likelihood.
However, there is no guarantee in general that the likelihood cannot worsen when both the
number of nests and the nesting level are increased by one. As an example, consider the tree with
2 nests and 3 nesting levels shown in Figure A.4. The only way of increasing the nesting level of
this tree is by nesting leaf nodes 4 and 5 together in one new nest c, making nest b degenerate.

A.2. Derivatives of the likelihood function with respect to the edge indicators

The discrete edge indicator variables x can be broken down into four distinct types. The derivative of
the likelihood function with respect to each of these four types has a diﬀerent closed form which we
derive in what follows.

(1) First order partial derivative of the likelihood function with respect to edges between the root

and alternatives xra:
∂L
∂xra

cna

(cid:88)

=

(cid:16)

n∈I

(cid:0) ln Pn{a|{r, a}} +

(cid:88)

l∈{r−→a}

xl

∂ ln Pn{a|l}
∂xra

(cid:1) +

(cid:88)

(cid:0) (cid:88)

cna(cid:48)

h∈{r−→a(cid:48)}

=

=

(cid:88)

(cid:16)

n∈I
(cid:88)

(cid:16)

n∈I

cna

(cid:0)µrVan − µrΓn{r}(cid:1) +

cna

(cid:0)µrVan − µrΓn{r}(cid:1) −

(cid:88)

a(cid:48)∈C
(cid:88)

a(cid:48)∈C

cna(cid:48)

(cid:0) − µr

cna(cid:48) exp (cid:0)µr

a(cid:48)∈C\{a}
(cid:1)(cid:17)

∂Γn{r}
∂xra
(cid:0)Van − Γn{r}(cid:1)(cid:1)(cid:17)

xh

∂ ln Pn{a(cid:48)|h}
∂xra

(cid:1)(cid:17)

(A.1)

Expression (A.1) is deﬁned in terms of summations over the elementary alternatives C and the set of
individuals I and can be easily evaluated.
(2) First order partial derivative of the likelihood function with respect to edges between the root and
nests xrb:

28

Aboutaleb et al.

By “conditioning” on the nest node b we can rewrite the log-likelihood as

(cid:88)

(cid:88)

(cid:16)

L =

n∈I

a∈C

cna(xra ln Pn{a|{r, a}} +

(cid:88)

(cid:88)

b∈N

l∈{b−→a}

xrbxl ln Pn{a|{r, l}})

(cid:17)

Then,

∂L
∂xrb

=

=

=

(cid:88)

(cid:88)

(cid:16)

(cid:88)

cna(

n∈I

a∈C

(cid:88)

(cid:88)

(cid:16)

n∈I
(cid:88)

a∈C
(cid:88)

(cid:16)

l∈{b−→a}

(cid:88)

cna(

l∈{b−→a}
(cid:88)

cna(

n∈I

a∈C

l∈{b−→a}

(xl ln Pn{a|{r, l}} − xrbxlµr

∂Γn{r}
∂xrb

) −

(cid:88)

(cid:88)

h∈{b(cid:48)−→a}

xrb(cid:48)xhµr

∂Γn{r}
∂xrb

(cid:17)
)

b(cid:48)∈N \{b}
(cid:17)

∂Γn{r}
∂xrb

)

xl ln Pn{a|{r, l}} −

(cid:88)

xhµr

xl ln Pn{a|{r, l}}) −

h∈{r−→a}
(cid:88)

h∈{r−→a}

xh exp (cid:0)µr

(cid:0)Γn{b} − Γn{r}(cid:1)(cid:1)(cid:17)

(A.2)

Now, at tree solutions, there is a unique path h ∈ {r −→ a} such that xh = 1. We can therefore do
away with the fourth summation in (A.2). Finally we obtain:

∂L
∂xrb

=

(cid:88)

(cid:88)

(cid:16)

(cid:88)

cna(

n∈I

a∈C

l∈{b−→a}

xb−→a ln Pn{a|{r, l}}) − exp (cid:0)µr

(cid:0)Γn{b} − Γn{r}(cid:1)(cid:1)(cid:17)

(A.3)

To evaluate the contributions of the third summation eﬃciently at tree solutions, we need not enu-
merate all paths b −→ a. Instead, a simple variant of Algorithm 2 can be adapted by treating node b as
the root node.
(3) First order partial derivative of the likelihood function with respect to edges between nests and
other nests xbb(cid:48):
By conditioning on nests b and b(cid:48), we can rewrite the likelihood in a more convenient form
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:16)

(cid:17)

L =

cna(

xlxbb(cid:48)xh ln Pn{a|{l, h}})

n∈I

a∈C

b,b(cid:48)∈N

l∈{r−→b}

h∈{b(cid:48)−→a}

Then,

∂L
∂xbb(cid:48)

=

(cid:88)

(cid:88)

(cid:16)

cna(

(cid:88)

(cid:88)

n∈I

a∈C

l∈{r−→b}

h∈{b(cid:48)−→a}

xlxh ln Pn{a|{l, h}} +

(cid:88)

k∈{r−→a}

xk

∂ ln Pn{a|k}
∂xbb(cid:48)

(cid:1)(cid:17)

(A.4)

To evaluate (A.4) eﬃciently at tree solutions, note that contribution of the term xlxh ln Pn{a|{l, h}}
is zero unless there is a path from the root to b and from b(cid:48) to a which can be easily checked. If there
is such paths at the tree solution at which the derivative is being evaluated, then ln Pn{a|{l, h}} can
be computed using steps 2 to 5 of Algorithm 2. Evaluating (cid:80)
is somewhat more
involved. For a path k ∈ {r −→ a}, let b(1)
and b(s)

k be the set of nodes visited along k where b(1)

k = a, where s is the length of the path k. Using (3.9) we have

∂ ln Pn{a|k}
∂xbb(cid:48)

k , ..., b(s)

k∈{r−→a} xk

k = r

∂ ln Pn{a|k}
∂xbb(cid:48)

=

s−2
(cid:88)

i=2

(µb(s−i)

k

− µb(s−i+1)

k

)

∂Γn{b(s−i+1)
k
∂xbb(cid:48)

}

+ (µr − µb(2)

k

)

∂Γn{nb(2)
k }
∂xbb(cid:48)

− µr

∂Γn{r}
∂xbb(cid:48)

(A.5)

The derivative of the inclusive value of a nest g with respect to xbb(cid:48) is computed recursively. Using
(3.10) we have:

∂Γn{g}
xbb(cid:48)

= µg

(cid:88)

j∈N

xgj exp(µg(Γn{j} − Γn{g}))

∂Γn{j}
xbb(cid:48)

The base case for this recursion is

∂Γn{b}
xbb(cid:48)

=

1
µb

exp(µb(Γn{b(cid:48)} − Γn{b}))

(A.6)

(A.7)

Learning Nesting Structure

29

Any call of (A.6) will terminate with a call of (A.7) without any further recursive calls. The evaluation
of (A.5) can be computed eﬃciently using a similar idea to Algorithm 2 where the contributions are
calculated according to (A.6).

(4) First order partial derivative of the likelihood function with respect to edges between nests and

alternatives xba:

Then,

L =

(cid:88)

(cid:88)

(cid:16)

cna(

(cid:88)

(cid:88)

n∈I

a∈C

b∈N

l∈{r−→b}

xlxba ln Pn{a|{l, b}})

(cid:17)

∂L
∂xba

=

(cid:88)

(cid:16)

cna(

(cid:88)

n∈I

l∈{r−→b}

xl ln Pn{a|{l, b}}) +

(cid:88)

a(cid:48)∈C

cna(cid:48)(

(cid:88)

h∈{r−→a(cid:48)}

xh

∂ ln Pn{a(cid:48)|h}
∂xba

(cid:1)(cid:17)

(A.8)

The contribution of the term xl ln Pn{a|{l, b}}) in (4.26) is zero unless there is a path from the root to
nest b. At tree solutions this contribution can be evaluated through a modiﬁcation of Algorithm 2 by
starting from the nest b and propagating upwards to the root. Finally the contribution of the rightmost
summation in (A.8) is computed using recursion using similar expressions to (A.6) and (A.7). The base
case for the recursion in this case is diﬀerent, and is given by

∂Γn{b}
xba

=

1
µb

exp(µb(Van − Γn{b}))

(A.9)

A.3. Counting the total number of possible paths from the root to the alternatives

We ﬁrst rewrite the log-likelihood function more explicitly:

L =

(cid:88)

(cid:88)

(cid:104)

cna

xra(µrVan − µrΓn{r}) +

(cid:88)

b∈N

xrbxba(µbVan + (µr − µb)Γn{b} − µrΓn{r}) + . . .

n∈I

a∈C

(cid:88)

+

b1,b2,...,bp∈N

(xrb1

p−1
(cid:89)

i=2

xbibi+1xbpa)(µbp Van +

p−1
(cid:88)

(cid:105)
(µbi − µbi+1)Γn{bi+1} + (µr − µb1)Γn{b1} − µrΓn{r})

i=1

(A.10)

The total number of terms in the expression (A.10) above is |I| · |C| · (cid:98)|C|! · e(cid:99). This is also the total
number of paths from the root node to the alternative nodes a ∈ C.
To see why this is so, recall that p = |N | = |C| − 2 and notice that the number of terms in the
square brackets is equal to the number of permutations of the index b. which is p(1 + p + p(p −
1) + . . . + p!) = p(1 + p! (cid:80)p−1
1
1
k! = p!e. The diﬀerence is given by
k=1
k! = 1 + p! (cid:80)∞
p! (cid:80)∞
1
k! < 2.

k! ). Now p! (cid:80)p−1

k! < p! (cid:80)∞

k=n+1

k=n

k=1

k=1

1

1

30

Aboutaleb et al.

Figure A.1: A case where it is possible to increase the number of nests while holding the nesting level
ﬁxed without worsening the likelihood

Figure A.2: A case where it is not possible to increase the number of nests while holding the nesting
level ﬁxed without worsening the likelihood

Figure A.3: A case where increasing the number of nests and nesting level each by one does not
worsen the likelihood.

Learning Nesting Structure

31

Figure A.4: A case where it is not possible to increase the number of nests and the nesting levels
each by one without worsening the likelihood.

