2
2
0
2

l
u
J

5
2

]

G
L
.
s
c
[

3
v
1
6
9
6
0
.
2
1
0
2
:
v
i
X
r
a

Online Stochastic Optimization with Wasserstein
Based Non-stationarity

Jiashuo Jiang† Xiaocheng Li‡

Jiawei Zhang†

† Department of Technology, Operations & Statistics, Stern School of Business, New York University
‡ Imperial College Business School

Abstract: We consider a general online stochastic optimization problem with multiple budget constraints

over a horizon of ﬁnite time periods. In each time period, a reward function and multiple cost functions are

revealed, and the decision maker needs to specify an action from a convex and compact action set to collect

the reward and consume the budgets. Each cost function corresponds to the consumption of one budget

constraint. The reward function and the cost functions of each time period are drawn from an unknown

distribution, which is non-stationary across time. The objective of the decision maker is to maximize the

cumulative reward subject to the budget constraints. This formulation captures a wide range of applications

including online linear programming and network revenue management, among others. In this paper, we

consider two settings: (i) a data-driven setting where the true distribution is unknown but a prior estimate

(possibly inaccurate) is available; (ii) an uninformative setting where the true distribution is completely

unknown. We propose a uniﬁed Wasserstein-distance based measure to quantify the inaccuracy of the prior

estimate in Setting (i) and the non-stationarity of the environment in Setting (ii). We show that the proposed

measure leads to a necessary and suﬃcient condition for the attainability of a sublinear regret in both settings.

For Setting (i), we propose an informative gradient descent algorithm. The algorithm takes a primal-dual

perspective and it integrates the prior information of the underlying distributions into an online gradient

descent procedure in the dual space. The algorithm also naturally extends to the uninformative setting (ii).

Under both settings, we show the corresponding algorithm achieves a regret of optimal order. We illustrate

the algorithm performance through numerical experiments.

1. Introduction

In this paper, we study a general online stochastic optimization problem with m budget constraints,

each with an initial capacity, over a ﬁnite horizon of discrete time periods. At each time t, a reward
function ft : X → R and a cost function gt : X → Rm are drawn independently from a distribution.

Upon the observation of the functions, the decision maker speciﬁes a decision xt ∈ X , where X

is assumed to be a convex and compact set. Accordingly, a reward f (xt) is collected, and each

budget i ∈ {1, 2, . . . , m} is consumed by an amount of git(xt), where gt(x) = (g1t(x), ..., gmt(x))(cid:62).

The decision maker’s objective is to maximize the totally collected reward subject to the budget

capacity constraints.

Our formulation generalizes several existing problems studied in the literature. When ft and gt

are linear functions for each t, our formulation reduces to the online linear programming (OLP)

1

 
 
 
 
 
 
2

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

problem (Buchbinder and Naor, 2009). Our formulation also covers the network revenue manage-

ment (NRM) problem (Talluri and Van Ryzin, 2006), including the quantity-based model, the

price-based model and the choice-based model (Talluri and Van Ryzin, 2004). Note that for the

OLP literature, the reward function and cost functions are assumed to be drawn from an unknown

distribution which is stationary over time (or from a random permutation), while the NRM liter-

ature always assumes a precise knowledge of the true distribution, though it can be nonidentical

across time. In this paper, our focus is an unknown non-stationary setting where the functions ft

and gt are drawn from a distribution Pt that is nonidentical over time and unknown to the decision

maker. Speciﬁcally, we mainly consider two settings: (i) a data-driven setting where there exists an
available prior estimate ˆPt (possibly inaccurate) for the true distribution Pt of each time period
and (ii) an uninformative setting where the true distribution Pt is completely unknown. When the

prior estimates are identical to the true distributions, the data-driven setting reduces to the known

non-stationary setting considered in the NRM literature. When the distribution is identical over

time, the uninformative setting reduces to the unknown stationary setting considered in the OLP

literature.

For both settings, we assume that the true distribution falls into an uncertainty set, which

controls the inaccuracy of the prior estimate in the data-driven setting and the non-stationarity

of the distributions in the uninformative setting. Our goal is to derive near-optimal policies for

both settings, which perform well over the entire uncertainty set. We compare the performances

of our algorithms/policies to the “oﬄine” optimization problem which maximizes the total reward

with full information/knowledge of all the ft’s and gt’s. We use regret as the performance metric,

which is deﬁned as worst-case additive optimality gap (over the uncertainty set) between the total

reward generated by an algorithm/policy and the oﬄine optimal objective value.

1.1. Main Results and Contributions

For the data-driven setting (in Section 4), the true distribution Pt is unknown, but we assume
the availability of a prior estimate ˆPt. The prior estimate can be based on some history data.
We propose a Wasserstein-based deviation measure, Wasserstein-based deviation budget (WBDB),

to quantify the deviation of the prior estimate from the true distribution. Based on WBDB, we

introduce an uncertainty set driven by the notion of WBDB and a parameter WT (called deviation

budget), and the uncertainty set encapsulates all the distributions Pt’s that have WBDB no greater

than WT . We illustrate the sharpness of WBDB by showing that if the variation budget WT is

linear in T , sublinear regret could not be achieved by any admissible policy. Next, we develop a new

Informative Gradient Descent with prior estimate (IGDP) algorithm, which adaptively combines

the prior distribution knowledge into an update in the dual space. Our algorithm is motivated

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

3

by the traditional online gradient descent (OGD) algorithm (Hazan, 2016). The OGD algorithm

applies a linear update rule according to the gradient information at the current period and has

been shown to work well in the stationary setting, even when the distribution is unknown (Lu

et al., 2020; Sun et al., 2020; Li et al., 2020). The OGD type algorithm has also been developed

under stationary setting with unknown distributions in Agrawal and Devanur (2014b) by assuming

further stronger conditions on the dual optimal solution, and with known distributions for service

level problems (Jiang et al., 2019). However, the update in OGD for each time period only involves

information gathered up to the current time period, but for the non-stationary setting, we also need

to take advantage of the prior estimates of the future time periods. Speciﬁcally, based on a primal-

dual convex relaxation of the underlying oﬄine problem, we obtain a prescribed allocation of the

budgets over the entire horizon based on the prior estimates. Then, the IGDP algorithm uses this

allocation to adjust the gradient descent direction. This idea is new for the related literature in that

the IGDP descent direction at each period does not simply come from the historical observations,

but it is also informed by the distribution knowledge of the entire horizon. We show that the IGDP

algorithm achieves the ﬁrst optimal regret upper bound O(max{

T , WT }).

√

A few recent works also study similar online decision making problems in a non-stationary envi-

ronment where Pt may vary over time. Devanur et al. (2019) study the case of known distribution

(WT = 0) and obtain a 1 − O(1/

c) competitive ratio, where c denotes the minimal capacity of the

√

budget constraints. It remains unclear how to generalize the method in (Devanur et al., 2019) to the

setting of WT > 0 where the distribution knowledge is inaccurate or absent. A line of works (Vera

and Banerjee, 2020; Banerjee and Freund, 2020a,b) study the known distribution setting (WT = 0)

with an additional assumption that the underlying distribution takes a ﬁnite support. These works

develop algorithms that achieve bounded regret that bears no dependency on T . Compared to this

stream of literature, the main results of our paper do not assume the ﬁnite supportedness. In addi-

tion, when the underlying distribution is ﬁnite, we extend the previous algorithm and analysis for

the case of WT > 0. Another recent work (Cheung et al., 2020) studies the non-stationary problem

and proposes dual-based algorithms that utilize the trajectories sampled from the prior estimate

distribution, which can be classiﬁed as a static policy (see Section 6.2). We show a major disad-

vantage of the static policy is that when directly applied to the data-driven setting with estimation

erros (WT > 0), it could incur linear regret even when WT is sublinear; and also the Wasserstein

distance is in general tighter than the total variation distance used therein.

For the uninformative setting, we assume no prior knowledge on the true distribution. This

setting is consistent with the unknown distribution setting in the literature of OLP problem (Moli-

naro and Ravi, 2013; Agrawal et al., 2014; Gupta and Molinaro, 2014) and the setting of blind

4

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

NRM (Besbes and Zeevi, 2012; Jasin, 2015). We modify the WBDB by replacing the prior esti-

mate of each distribution with their uniform mixture distribution to propose a new measure called

Wasserstein-based non-stationarity measure (WBNB). By its deﬁnition, the WBNB captures the

cumulative deviation for all the distribution Pt from their centric distribution, and it thus reﬂects

the intensity of the non-stationarity associated with Pt’s. In this sense, the WBNB concerns the

global change of the distributions, whereas the previous non-stationarity measures (Besbes et al.,

2014, 2015; Cheung et al., 2019) in an unconstrained setting characterize the local and temporal

change of the distributions over time. In Section 5.1, we illustrate by a simple example that such

temporal change measures actually fail in a constrained setting. Thus it addresses the necessity of

such a global measure and reveals the interaction between the constraints and the non-stationary

environment. Note that a simultaneous and independent work (Balseiro et al., 2020) also uses

global change of the distributions to derive a measure of non-stationarity. However, their measure

is based on the total variation metric between distributions. With the same example, we illustrate

the advantage of using Wasserstein distance instead of total variation distance or KL-divergence.

Speciﬁcally, the Wasserstein distance compares both the support and densities between two dis-

tributions, while total variation distance or KL-divergence compares only the densities. Therefore

the Wasserstein-based measure is sharper and more proper for the general online stochastic opti-

mization problem. We formulate the uncertainty set accordingly with the WBNB and propose

Uninformative Gradient Descent Algorithm (UGD) as a natural reduction of the IGD algorithm

in the uninformative setting. We prove that UGD algorithm achieves a regret bound of optimal

order.

As a probability distance metric, the Wasserstein distance has been widely used as a measure

of the deviation between estimate and true distribution in the distributionally robust optimization

literature (e.g. Esfahani and Kuhn (2018)) to represent conﬁdence set and it has demonstrated

good performance both theoretically and empirically. To the best of our knowledge, we are the ﬁrst

to use the Wasserstein distance in an online optimization/learning context. From a modeling per-

spective, the two proposed measures WBDB and WBNB contribute to the study of non-stationary

environment for online optimization/learning problem. Speciﬁcally, the data-driven setting relaxes

the common assumption adopted in the NRM literature that the true distributions are known to

the decision maker by allowing the prior estimates to deviate from the true distributions. This

deviation can be interpreted as an estimation or model misspeciﬁcation error, and WBDB estab-

lishes a connection between the deviation and algorithmic performance. The uninformative setting

generalizes a stream of online learning literature (e.g. Besbes et al. (2015)), which mainly concerned

with the unconstrained settings and includes bandits problem (Garivier and Moulines, 2008; Besbes

et al., 2014) and reinforcement learning problem (Cheung et al., 2019; Lecarpentier and Rachelson,

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

5

2019) as special cases. WBDB adds to the current dictionary of non-stationarity deﬁnitions and it

specializes for a characterization of the constrained setting.

Finally, we conclude our discussion with several extensions. For the majority of this paper, we

analyze algorithm performance under the above two settings without imposing additional structures

on the random functions ft and gt’s. When these functions have a stronger structure such as taking

a ﬁnite support, we show that a better regret bound can be obtained through a re-solving design.

The result complements to the existing works on re-solving algorithms (Jasin and Kumar, 2012;

Bumpensanti and Wang, 2020; Vera and Banerjee, 2020) in that it provides a robustness analysis

of the re-solving algorithm when the underlying distribution is misspeciﬁed. Also, throughout

the paper, we provide a number of lower bound results. These lower bound results illustrate the

following points: (i) our proposed algorithm has an optimal order of regret in the worst-case sense;

(ii) the Wasserstein’s distance is sharp; (iii) a dynamic algorithm is necessary to achieve sublinear

regret in a non-stationary environment.

1.2. Other Related Literature

As mentioned above, our formulation of the online stochastic optimization problem roots in two

major applications: the online linear programming problem and the network revenue management

problem. The online linear programming (OLP) problem (Molinaro and Ravi, 2013; Agrawal et al.,

2014; Gupta and Molinaro, 2014) covers a wide range of applications through diﬀerent ways of

specifying the underlying LP, including secretary problem (Ferguson et al., 1989), online knapsack

problem (Arlotto and Xie, 2020; Jiang and Zhang, 2020), resource allocation problem (Vanderbei

et al., 2015; Asadpour et al., 2020), quantity-based network revenue management problem (Jasin

and Kumar, 2012; Jasin, 2015), network routing problem (Buchbinder and Naor, 2009), matching

problem (Mehta et al., 2005), etc. Notably, the problem has been studied under either (i) the

stochastic input model where the coeﬃcient in the objective function, together with the corre-

sponding column in the constraint matrix is drawn from an unknown distribution P, or (ii) the

random permutation model where they arrive in a random order. As noted in the paper (Li et al.,

2020), the random permutation model exhibits similar concentration behavior as the stochastic

input model. The non-stationary setting of our paper relaxes the i.i.d. structure and it can be

viewed as a third paradigm for analyzing the OLP problem.

The network revenue management (NRM) problem has been extensively studied in the literature

and a main focus is to propose near-optimal policies with strong theoretical guarantees. One popular

way is to construct a deterministic linear program as an upper bound of the optimal revenue and use

its optimal solution to derive heuristic policies. Speciﬁcally, Talluri and Van Ryzin (1998) propose a

static bid-price policy based on the dual variable of the linear programming upper bound and proves

6

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

that the revenue loss is O(

√

k) when each period is repeated k times and the capacities are scaled

by k. Subsequently, Reiman and Wang (2008) show that by re-solving the linear programming

upper bound once, one can obtain an o(

k) upper bound on the revenue loss. Then, Jasin and

√

Kumar (2012) show that under a non-degeneracy condition for the underlying LP, a policy which

re-solves the linear programming upper bound at each time period will lead to an O(1) revenue

loss, which is independent of the scaling factor k. The relationship between the performances of

the control policies and the number of times of re-solving the linear programming upper bound

is further discussed in their later paper (Jasin and Kumar, 2013). Recently, Bumpensanti and

Wang (2020) propose an infrequent re-solving policy and show that their policy achieves an O(1)

upper bound of the revenue loss even without the “non-degeneracy” assumption. With a diﬀerent

approach, Vera and Banerjee (2020) prove the same O(1) upper bound for the NRM problem and

their approach is further generalized in (Vera et al., 2019; Banerjee and Freund, 2020a,b) for other

online decision making problems, including online stochastic knapsack, online probing, bin packing,

and dynamic pricing. Note that all the approaches mentioned above are mainly developed for the

stochastic/stationary setting. When the arrival process of customers is non-stationary over time,

Adelman (2007) develops a strong heuristic based on a novel approximate dynamic programming

(DP) approach. This approach is further investigated under various settings in the literature (for

example (Zhang and Adelman, 2009; Kunnumkal and Talluri, 2016)). Remarkably, although the

approximate DP heuristic is one of the strongest heuristics in practice, it does not feature for a

theoretical bound. Finally, by using non-linear basis functions to approximate the value of the DP,

Ma et al. (2020) develop a novel approximate DP policy and derive a constant competitiveness ratio

dependent on the problem parameters. Compared to this line of works, our contribution is two-fold.

First, the two main settings that we consider generalize the existing framework of network revenue

management in two aspects: (i) we do not assume the knowledge of the underlying distribution;

(ii) we do not require the distribution to take a ﬁnite support. Second, with the ﬁnite-support

condition on the underlying distribution, we characterize the relationship between the algorithm

performance and the misspeciﬁcation error of the underlying distribution.

Besides, our problem is also related to the literature of Online Convex Optimization (OCO) and

model predictive control. We will discuss the model predictive control literature here and leave the

discussion on OCO after presenting our formulation in the next secton. Speciﬁcally, our problem

can also be formulated as a nonlinear model predictive control (MPC) problem (Rawlings, 2000).

In MPC, the decision maker executes the online decisions based on some benchmarks. In our paper,

we adopt the oﬄine optimum as the benchmark and develop online algorithms based on the oﬄine

optimum. Speciﬁcally, for the data-driven setting, we assume that the prior estimates deviate from

the true distribution, which corresponds to the broad literature on robust MPC (Bemporad and

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

7

Morari, 1999), where there are model uncertainties and noises. Compared t this line of works, our

paper is the ﬁrst to propose the use of Wasserstein distance to measure the model uncertainty,

and we derive tight regret bound that relates the best achievable algorithm performance with the

model uncertainty. Our algorithm also extends to the uninformative setting where there are no

prior estimates, and the information about the underlying benchmark has to be learned on-the-ﬂy.

In this way, our algorithm for the uninformative setting establishes a connection between online

learning and MPC for a time-varying (non-stationary) environment. We refer interested readers to

Wagener et al. (2019) for a more detailed discussion over the connections between online learning

and MPC.

2. Problem Formulation

Consider the following convex optimization problem

T
(cid:88)

t=1
T
(cid:88)

max

s.t.

ft(xt)

(CP)

git(xt) ≤ ci,

i = 1, ..., m,

t=1
xt ∈ X ,

t = 1, ..., T,

where the decision variables are xt ∈ X for t = 1, ..., T and X is a compact convex set in Rk. The
function ft’s are functions in the space F = F(X ) of concave continuous functions and git’s are

functions in the space G = G(X ) of convex continuous functions, both of which are supported on X .
Compactly, we deﬁne the vector-value function gt(x) = (g1t(x), ..., gmt(x))(cid:62) : Rk → Rm. Throughout
the paper, we use i to index the constraint and t (or sometimes j) to index the decision variables,

and we use bold symbols to denote vectors/matrices and normal symbols to denote scalars.

In this paper, we study the online stochastic optimization problem where the functions in the

optimization problem (CP) are revealed in an online fashion and one needs to determine the value

of decision variables sequentially. Speciﬁcally, at each time t, the functions (ft, gt) are ﬁrst revealed,

and then the decision maker needs to decide the value of xt. Diﬀerent from the oﬄine setting, at

each time t, we do not have the information of the future part of the optimization problem (from
time t + 1 to T ). Given the history Ht−1 = {fj, gj, xj}t−1

j=1, the decision of xt can be expressed as a

policy function of the history and the observation at the current time period. That is,

xt = πt(ft, gt, Ht−1)

(1)

where the policy function πt can be time-dependent. We denote the policy π = (π1, ..., πT ). The

decision variables xt’s must conform to the constraints in (CP) throughout the procedure, and the

objective is aligned with the maximization objective of the oﬄine problem (CP).

8

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

2.1. Parameterized Form, Probability Space, and Assumptions

Consider a parametric form of the underlying problem (CP) where the functions (ft, gt) are param-
eterized by a (random) vector θt ∈ Θ ⊂ Rl. Speciﬁcally,

ft(xt) := f (xt; θt), git(xt; θt) := gi(xt; θt)

for each i = 1, ..., m and t = 1, ..., T . We denote the vector-valued constraint function by g(x; θ) =
(g1(x; θ), ..., gm(x; θ))(cid:62) : X → Rm. Then the problem (CP) can be rewritten as the following param-

eterized convex program

T
(cid:88)

t=1
T
(cid:88)

max

s.t.

f (xt; θt)

(PCP)

gi(xt; θt) ≤ ci,

i = 1, ..., m,

t=1
xt ∈ X ,

t = 1, ..., T,

where the decision variables are (x1, ...., xT ). We note that this parametric form (PCP) avoids

the complication of dealing with probability measure in function space. It is introduced mainly

for presentation purpose, and it will change the nature of the problem. Moreover, we assume the

knowledge of f and g a priori. Here and hereafter, we will use (PCP) as the underlying form of

the online stochastic optimization problem.

The problem of online stochastic optimization, as its name refers, involves stochasticity on the

functions for the underlying optimization problem. The parametric form (PCP) reduces the ran-

domness from the function to the parameters θt’s, and therefore the probability measure can be

deﬁned in the parameter space of Θ. First, we consider the following distance function between

two parameters θ, θ(cid:48) ∈ Θ,

ρ(θ, θ(cid:48)) := sup
x∈X

(cid:107)(f (x; θ), g(x; θ)) − (f (x; θ(cid:48)), g(x; θ(cid:48)))(cid:107)∞

(2)

where (cid:107) · (cid:107)∞ is the supremum norm in Rm+1. Without loss of generality, let Θ be a set of class
representatives, that is, for any θ (cid:54)= θ(cid:48) ∈ Θ, ρ(θ, θ(cid:48)) > 0. In this way, the parameter space Θ can be

viewed as a metric space equipped with metric ρ(·, ·). We choose supremum norm because in our

model, one single action xt is made at each period t and it can take any arbitrary value in X . As a

result, the comparison between (f (·; θ), g(·; θ)) and (f (·; θ(cid:48)), g(·; θ(cid:48))) should be made at each point

x and thus the supremum norm is a natural choice for deﬁning ρ(θ, θ(cid:48)). In this way, the deﬁnition
of ρ is based on the vector-valued function (f, g) : X → Rm+1. Thus it captures the eﬀect of diﬀerent

parameters on the function value rather than the original Euclidean diﬀerence in the parameter

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

9

space. Let BΘ be the smallest σ-algebra in Θ that contains all open subsets (under metric ρ) of Θ.

We denote the distribution of θt as Pt which can be viewed as a probability measure on (Θ, BΘ).

Throughout the paper, we make the following assumptions. Assumption 1 (a) and (b) impose

boundedness on function f and gi’s. Assumption 1 (c) states the ratio between f and gi is uniformly

bounded by q for all x and θ. Intuitively, it tells that for each unit consumption of resource, the

maximum amount of revenue earned is upper bounded by q. In this paper, this condition will

mainly be used to give an upper bound on the dual optimal solution. In Assumption 1 (d), we

assume Pt’s are independent of each other but we do not assume the exact knowledge of them. Also,

there can be dependence between components in the vector-value functions (f, g). In Assumption 1

(e), we require some convexity structure for the underlying functions. In the rest of the paper, this

assumption will only be used to ensure that the Lagrangian problem maxx∈X {f (x; θ) − p(cid:62)g(x; θ)}

can be eﬃciently solved for any ﬁxed p ≥ 0.

Assumption 1 (Boundedness and Independence) We assume

(a) |f (x; θ)| ≤ 1 for all x ∈ X , θ ∈ Θ.

(b) gi(x; θ) ∈ [0, 1] for all x ∈ X , θ ∈ Θ and i = 1, ..., m. In particular, 0 ∈ X and gi(0; θ) = 0 for

all θ ∈ Θ.

(c) There exists a positive constant q such that for any θ ∈ Θ and each i, we have that f (x; θ) ≤

q · gi(x; θ) holds for any x ∈ X when gi(x; θ) > 0.

(d) θt ∼ Pt and Pt’s are independent with each others.

(e) The function f (x; θ) is concave over x and the function gi(x; θ) is convex over x for any

θ ∈ Θ and i = 1, . . . , m.

In the following, we illustrate the online formulation through two main application contexts:

online linear programming and online network revenue management. We choose the more general

convex formulation (PCP) to uncover the key mathematical structures for this online optimization

problem, but we will occasionally return to these two examples to generate intuitions throughout

the paper.

2.2. Examples

Online linear programming (LP): The online LP problem (Molinaro and Ravi, 2013; Agrawal

et al., 2014; Gupta and Molinaro, 2014) can be viewed as an example of the online stochastic

optimization formulation of (PCP). Speciﬁcally, the decision variable xt ∈ X = [0, 1], the functions

f and gi are linear functions, and the parameter θt = (rt, at) where at = (a1t, ..., amt). That is,

f (xt; θt) = rtxt and gi(xt; θt) = aitxt. At each time t = 1, ..., T , the coeﬃcient in the objective rt

together with the corresponding column at in the constraint matrix is revealed, and then one needs

10

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

to determine the value of xt immediately. As mentioned earlier, the online LP problem covers

a wide range of applications, through diﬀerent speciﬁcations of θt, including secretary problem,

online knapsack problem, resource allocation problem, generalized assignment problem, network

routing problem, and matching problem.

Price-based network revenue management (NRM): In the price-based NRM problem (Gal-

lego and Van Ryzin, 1994), a seller is selling a given stock of products over a ﬁnite time horizon by

posting a price at each time. The demand is price-sensitive and the ﬁrm’s objective is to maximize

the total collected revenue. This problem could be cast in the formulation (PCP) as follows. The

parameter θt refers to the type of the t-th arriving customer. There is usually a ﬁnite number

of customer types, so the parameter set Θ is ﬁnite. The type of each customer arrival can be

based on the side information such as demographic features and purchasing history. Each diﬀerent

customer type speciﬁes a diﬀerent demand function between the posted price and the realized

demand/revenue. Speciﬁcally, the decision variable xt represents to the price posted by the deci-

sion maker at time t. The constraint function g(xt; θt) denotes the resource consumption (demand)

under the price xt and f (xt; θt) denotes the collected revenue. In such context, there is usually an

extra layer of randomness for the functions f (xt; θt) and g(xt; θt) given the parameter θt, diﬀerent

from our main setting where f and g are deterministic and known. We will discuss this extension

in Section 6.4.

Choice-based network revenue management: In the choice-based NRM problem (Talluri and

Van Ryzin, 2004), the seller oﬀers an assortment of the products to the customer arriving in each

time period, and the customer chooses a product from the assortment to purchase according to

a given choice model. Now we discuss how our formulation (PCP) covers the choice-based NRM

problem as a special case. As in the price-based NRM model, the parameter θt represents the

customer type at time t, and the parameter set Θ is ﬁnite. Then we denote by S the set of all

possible assortments the seller can oﬀer to the customers. At each time t, the decision variable
encodes the assortment decision, i.e., xt = (xt,1, . . . , xt,|S|) ∈ [0, 1]|S| and (cid:80)

s∈S xt,s = 1. Here, xt,s

denotes the probability that an assortment s ∈ S is oﬀered at period t. The parameter θt speciﬁes the

choice model of the t-th customer arrival, and, together with the assortment decision xt, determines

the revenue through the function f (xt; θt) and the resource consumption (demand) through the

function g(xt; θt). As in the price-based NRM problem, there is an extra layer of randomness for

the functions f (xt; θt) and g(xt; θt) given the parameter θt and the assortment xt. We will also

elaborate more in Section 6.4.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

11

2.3. Performance Measure

We denote the oﬄine optimal solution of optimization problem (CP) as x∗ = (x∗

1, ..., x∗

T ), and the

oﬄine (online) objective value as R∗

T (RT ). Speciﬁcally,

R∗

T :=

RT (π) :=

T
(cid:88)

t=1
T
(cid:88)

t=1

ft(x∗
t )

ft(xt).

where the online objective value depends on the policy π. Aligned with general online learn-

ing/optimization problems, we focus on minimizing the gap between the online and oﬄine objective

values. Speciﬁcally, the optimality gap is deﬁned as follows:

RegT (H, π) := R∗

T − RT (π)

where the problem proﬁle H encapsulates the realization of the random parameters, i.e., H :=

(θ1, ..., θT ). Note that R∗

t and xt are all dependent on the problem proﬁle H, but we omit
H in these terms for notation simplicity when there is no ambiguity. We deﬁne the performance

T , RT (π), x∗

measure of the online stochastic optimization problem formally as regret

RegT (π) := max
P∈Ξ

EH∼P[RegT (H, π)]

(3)

where P = (P1, ..., PT ) denotes the probability measure of all time periods and the expectation is

taken with respect to the parameter θt ∼ Pt; compactly, we write the problem proﬁle H ∼ P. We

consider the worst-case regret for all the distribution P in a certain set Ξ where the set Ξ will be

speciﬁed in later sections.

The speciﬁcation of the set Ξ imposes more structure on the distributions of (P1, ..., PT ) and

this is one of the main themes of our paper. In the canonical setting of online stochastic learning

problem, all the distributions Pt’s are the same, i.e., Pt = P0 for t = 1, ..., T. Meanwhile, the adver-

sarial setting of online learning problem refers to the case when Pt’s are adversarially chosen. Our

work aims to bridge these two ends of the spectrum with a novel notion of non-stationarity, and to

relate the algorithm performance with certain structural property of P = (P1, ..., PT ). In the same

spirit, the work on non-stationary stochastic optimization (Besbes et al., 2015) proposes an elegant

notion of non-stationarity called variation budget. Subsequent works consider similar notions in

the settings of bandits (Besbes et al., 2014; Russac et al., 2019) and reinforcement learning (Che-

ung et al., 2019). To the best of our knowledge, all the previous works along this line consider

unconstrained setting and thus our work contributes to this line of work in illustrating how the

constraints interact with the non-stationarity.

12

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Discussion on the (constrained) online learning/optimization literature.

Now we discuss the positioning of our work against the vast literature on the (constrained) online

learning/optimization problem. Generally speaking, the formulations on this topic fall into two

categories: (i) ﬁrst-observe-then-decide and (ii) ﬁrst-decide-then-observe. Our formulation belongs

to the ﬁrst category in that at each time t, the decision maker ﬁrst observes the parameter θt (and

hence the functions (f (x; θt), g(x; θt))), and then determines the value of xt. This formulation cov-

ers many applications in operations research and operations management, such as online LP and

NRM. In these application contexts, the observations represent customers/orders arriving sequen-

tially to the system, and the decision variables capture accordingly the acceptance/rejection/pricing

decisions of the customers. Technically, the nature of having the observation before making the

decision enables the stronger dynamic benchmark that allows a diﬀerent xt across diﬀerent time

periods as the deﬁnition of R∗

T in above.

One representative problem for the second category is the online convex optimization (OCO)

problem. The OCO problem can be viewed as a ﬁrst-decide-then-observe problem in that at each

time t, the decision maker ﬁrst chooses the decision variable xt and then observes the function

ft (incurring a loss of ft(xt)). The OCO problem is mainly motivated from machine learning

applications such as online linear regression or online support vector machine (Hazan, 2016). From

an information perspective, the OCO problem can be viewed as a partial information setting,

whereas our online stochastic optimization can be viewed as a full information setting (Lattimore

and Szepesv´ari, 2020). Accordingly, the standard OCO problem generally adopts the (weaker)

static benchmark, i.e., x∗

T . This discrepancy in regret
benchmark prevents direct applications of OCO algorithms and analyses to our context. There

t ’s need to be the same when deﬁning R∗

are results that consider a dynamic or partially dynamic regret benchmark for OCO in a non-

stationary environment (Besbes et al., 2015) or an adversarial environment (Hall and Willett,

2013; Jadbabaie et al., 2015), but all these works consider the unconstrained problem. A line of

works study the problem of online convex optimization with constraints (OCOwC) under a static

or stochastic generation of the constraint functions gt’s. Speciﬁcally, Jenatton et al. (2016); Yuan

and Lamperski (2018); Yi et al. (2021) all consider the OCOwC problem with static constraint

(gt = g for some g), while Neely and Yu (2017) mainly study a setting where gt is i.i.d. generated

from some distribution. To the best of our knowledge, no existing work along this line of literature

allows non-stationarily generated constraint functions.

Another representative ﬁrst-decide-then-observe problem is the bandits with knapsacks (BwK)

problem which can also be viewed as a constrained online learning problem. The existing BwK

results consider either a stochastic setting (Badanidiyuru et al., 2013; Agrawal and Devanur, 2014a)

or an adversarial setting (Rangi et al., 2018; Immorlica et al., 2019). The algorithms along this

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

13

line of literature are mainly based on the underlying primal and dual LPs. Speciﬁcally, Agrawal

and Devanur (2014b) develop a fast dual-based algorithm for the online stochastic optimization

problem (ﬁrst-observe-then-decide) and analyzes the algorithm performance under further stronger

conditions on the dual optimal solution. Agrawal and Devanur (2014a) then extend to the ﬁrst-

decide-then-observe problem of BwK. The idea has been recently further applied to online learning

in revenue management problems in (Miao et al., 2021). But these learning models and algorithms

are developed in the stationary (stochastic) environment, which cannot be applied to the non-

stationary setting.

We remark that both the ﬁrst-decide-then-observe and the ﬁrst-observe-then-decide frameworks

can be useful in modeling some application context. For example, an Adwords problem under pay

for conversions can be modeled by OCOwO or BwK problems, while an Adwords problem under

pay for impressions is usually modeled by our online stochastic optimization framework or OLP

problem (Mehta et al., 2005).

3. Known Distribution and Informative Gradient Descent

We begin our discussion with the case when the distributions Pt’s are all known a priori. We

use this case to motivate and present our prototypical algorithm – informative gradient descent

which incorporates the prior information of Pt’s with the online gradient descent algorithm. In the

following sections, we will discuss the case when the distributions Pt’s are not known precisely and

analyze the algorithm performance accordingly.

3.1. Deterministic Upper Bound and Dual Problem

We ﬁrst introduce the standard deterministic upper bound for the regret benchmark – the “oﬄine”
T ]. We deﬁne the following expectation for a function u(x; θ) : X → R and a proba-
optimum E[R∗

bility measure P in the parameter space Θ,

Pu(x(θ); θ) :=

(cid:90)

θ(cid:48)∈Θ

u(x(θ(cid:48)); θ(cid:48))dP(θ(cid:48))

where x(θ) : Θ → X is a measurable function. Thus Pu(·) can be viewed as a deterministic func-

tional that maps function x(θ) to a real value and it is obtained by taking expectation with respect

to the parameter θ ∼ P.

Then, consider the following optimization problem

RUB

T = max

s.t.

T
(cid:88)

t=1
T
(cid:88)

Ptf (xt(θt); θt)

Ptgi(xt(θt); θt) ≤ ci,

i = 1, ..., m,

t=1
xt(θt) : Θ → X is a measurable function for t = 1, ..., T.

(4)

14

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

where θt follows the distribution Pt. The optimization problem (4) can be viewed as a convex

relaxation of (PCP) where the objective and constraints are all replaced with their expected coun-

terparts. Here x1:T = (x1(θ1), ..., xT (θT )) encapsulates all the primal decision variables. The primal

variables are expressed in a function form of θt in that for each diﬀerent θt, we allow a diﬀerent

choice of the primal variables. This is aligned with the “ﬁrst-observe-then-decide” setting where

the decision maker ﬁrst observes the realization of θt ∼ Pt and then decide the value of xt. As

a standard result in literature (Gallego and Van Ryzin, 1994), Lemma 1 establishes the optimal

objective value RUB

T

as an upper bound for E[R∗

T ].

Lemma 1 It holds that RUB

T ≥ E[R∗

T ].

The deterministic upper bound and the optimization problem (4) are commonly used to design

algorithms in the literature. To proceed, we introduce the Lagrangian of (4),

L(p, x1:T ) := c(cid:62)p +

T
(cid:88)

t=1

(cid:0)f (xt(θt); θt) − p(cid:62)g(xt(θt); θt)(cid:1)

Pt

where θt follows the distribution Pt. The (Lagrange multipliers) vector p = (p1, ..., pm)(cid:62) conveys

a meaning of shadow price for each budget, and pi ≥ 0 is the multiplier/dual variable associated

with the i-th constraint. Furthermore, we deﬁne the following function based on a point-wise

optimization for the primal variables,

h(p; θ) := max
x(θ)∈X

(cid:8)f (x(θ); θ) − p(cid:62)g(x(θ); θ)(cid:9) .

Here the point-wise optimization emphasizes that the primal variables can be dependent on (and

as a measurable function of) the parameter θt. For example, the pricing and assortment decisions

can be made upon the observation of the customer type. Then the dual problem of (4) becomes

L(p) := c(cid:62)p +

min
p≥0

T
(cid:88)

t=1

Pth(p; θt)

where θt follows the distribution Pt as before.

Let p∗ denote an optimal dual solution, i.e.,

p∗ ∈ argminp≥0L(p)

(5)

and for each t,

γt := Ptg(x∗(θ); θ) where x∗(θ) = argmaxx∈X {f (x; θ) − (p∗)(cid:62) · g(x; θ)}.

(6)

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

15

Here, x∗(θ) is the associated primal (optimal) solution under the dual optimal solution p∗, and γt

can be interpreted as the expected budget consumption in the t-th time period under the optimal

primal-dual pair (x∗(θ), p∗). Accordingly, for each t, we deﬁne

The following proposition states the relation between L(·) and Lt(·) and establishes an upper bound

Lt(p) := γ (cid:62)

t p + Pth(p; θ).

for the benchmark using the dual problem.

Proposition 1 For each t = 1, ..., T , it holds that

where p∗ is deﬁned in (5) as one dual optimal solution. Moreover, we have

p∗ ∈ argminp≥0Lt(p)

E[R∗

T ] ≤ min
p≥0

T
(cid:88)

t=1

Lt(p) =

T
(cid:88)

t=1

min
pt≥0

Lt(pt).

The ﬁrst part of the proposition says that all the Lt’s share the same minimizer of p∗ as the

dual problem. In fact, this is the reason why we introduce γt’s to deﬁne Lt’s and the key how the

prior knowledge of non-stationarity can be used for algorithm design. This property of a shared

minimizer can be useful for online optimization in a non-stationary environment. First, we note

that the primal optimal solution can be largely determined by the dual optimal solution p∗. At each

time t, the decision maker only has observations of the (random) realizations of Ls for s = 1, ..., t.

Intuitively, the proposition implies that these past observations can be eﬀectively used to estimate

the dual optimal solution p∗ as all the Lt’s share the same optimal solution p∗. Another advantage

of this dual-based representation is that, through the adjustment of γt’s, the dual optimal solution

(of Lt(·)) for each time period is the same, whereas the primal optimal solution xt to (PCP)

may be diﬀerent from each t. As mentioned earlier, there are two types of regret benchmarks in

the literature of online optimization: static oracle and dynamic oracle. The static oracle refers

to the case where we compare to an oﬄine decision maker adopting a common optimal solution

throughout the entire horizon, and the dynamic oracle allows the oﬄine decision maker to take

an individual optimal solution for each time period. Proposition 1 states that in our setting, the

static oracle and the dynamic oracle connect with each other in the dual space through a careful

construction of Lt(·): the primal optimal solution is dynamic (diﬀerent over time) while the dual

optimal solution is static after the adjustment of γt’s. This connection makes it possible to apply

the gradient descent-based algorithms from OCO literature for the dual space in the non-stationary

setting. Besbes et al. (2015) derive a similar argument to connect the dynamic oracle and static

oracle for the unconstrained setting as a backbone for the algorithm design and regret analysis

therein.

16

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

3.2. Main Algorithm and Regret Analysis

Now we present our main algorithm – Informative Gradient Descent – fully described as Algorithm

1. The algorithm is described as a meta algorithm with an input γ. In the following sections, we

will discuss how to apply the algorithm to diﬀerent settings with diﬀerent speciﬁcations of γ. When

the distributions Pt’s are known, the algorithm is motivated from the dual-based representation in

Proposition 1 and the input γ is accordingly deﬁned by (6). Speciﬁcally, the algorithm maintains

a dual vector/price pt, and at each time t, it performs a stochastic gradient descent update for pt

with respect to the function Lt(·). To see that the expectation of the dual gradient update (7) is

the gradient with respect to the function Lt(·) evaluated at pt,

E [g( ˜xt; θt) − γt] = −γt + Ptg( ˜xt; θ)

= −

∂
∂p

(cid:0)γ (cid:62)

t p + Pth(p; θ)(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)p=pt

where the ﬁrst line comes from taking expectation with respect to θt and the second line comes from

the deﬁnition of ˜xt in the algorithm. At each time t, the primal decision variable xt is determined

based on the dual price pt and the observation θt jointly, in the same manner as the deﬁnition of

the function h(p; θ). Assumption 1 (e) ensures the optimization problem that deﬁnes h(p; θ) can

be solved eﬃciently (See further discussion in Section A3).

We now provide an alternative perspective to interpret Algorithm 1 for the case when the

distributions are known. Note that by deﬁnition (6), γt’s represent the “optimal” way to allocate

the resource budget over time according to the dual optimal solution p∗. Speciﬁcally, a larger (resp.

smaller) value of γi,t, where γi,t denotes the i-th component of γt, indicates that more (resp. less)

budget should be allocated to time period t for constraint i. In Algorithm 1, from the update rule

(7) of the dual variable at time period t, we know that if the budget consumption of constraint

i is larger (resp. smaller) than γi,t, i.e., gi( ˜xt; θt) > γi,t (resp. gi( ˜xt; θt) < γi,t), then we have that

pi,t+1 ≤ pi,t (resp. pi,t+1 > pi,t), where pi,t denotes the i-th component of pt. That is, if more (resp.

less) budget is consumed in the earlier periods, then the dual price will be more likely to increase

(resp. decrease), and consequently, less (resp. more) budget will be consumed in the future periods.

In this way, the dual variable pt dynamically balances the budget consumption: it ensures that for

each t, the cumulative budget consumption of Algorithm 1 during the ﬁrst t time periods always
stay “close” to the optimal scheme of (cid:80)t

j=1 γj. Later in Section 6.2, we will show that a dynamic
policy that incorporates the resource consumption process into the online decisions is necessary

in a non-stationary environment, and any static policy can incur a linear regret even when the

underlying non-stationarity intensity is sublinear.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

17

Algorithm 1 Informative Gradient Descent Algorithm (IGD(γ))

1: Input: parameters γ = (γ1, . . . , γT ).

2: Initialize the initial dual price p1 = 0 and initial constraint capacity c1 = c

3: for t = 1, ..., T do

4:

Observe θt and solve

˜xt = argmaxx∈X {f (x; θt) − p(cid:62)

t · g(x; θt)}

where g(x, θt) = (g1(x, θt), ..., gm(x, θt))(cid:62)

5:

Set

xt =

6:

Update the dual price

(cid:40) ˜xt,

if ct permits a consumption of g( ˜xt; θt)

0,

otherwise

(cid:18)

pt+1 =

pt +

1
√
T

(g( ˜xt; θt) − γt)

(cid:19)

∨ 0

(7)

where the element-wise maximum operator u ∨ v = max{v, u}

7:

Update the remaining capacity

ct+1 = ct − g(xt; θt)

8: end for

9: Output: x = (x1, ..., xT )

Now we analyze the performance of Algorithm 1 for the known distribution setting. The following

lemma says that the dual price vector pt remains bounded throughout the procedure. Its proof

largely relies on Assumption 1 (c), and also, the main usage of Assumption 1 (c) throughout our

analysis is to ensure the boundedness of the dual vector.

Lemma 2 Under Assumption 1, the dual price vector satisﬁes (cid:107)pt(cid:107)∞ ≤ q + 1 for t = 1, 2, . . . , T .

Here pt is computed by (7) of IGD(γ) in Algorithm 1 with γ speciﬁed by (6), and the constant q

is deﬁned in Assumption 1 (c).

The following theorem builds upon Proposition 1 and Lemma 2 and it states that the regret of

Algorithm 1 is upper bounded by O(

T ).

√

Theorem 1 Under Assumption 1, if we consider the set Ξ = {P : P = (P1, ..., PT ), ∀P1, . . . , ∀PT },

then the regret of IGD(γ) in Algorithm 1, with γ deﬁned by (6), has the following upper bound

RegT (πIGD) ≤ O(

√

T )

√

18

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

where πIGD stands for the policy speciﬁed by IGD(γ) in Algorithm 1.

√

The O(

T ) regret upper bound of Algorithm 1 under a known non-stationary environment com-

plements to several existing results in the literature. For the NRM problem, Talluri and Van Ryzin

(1998) derives a O(

k) regret bound when the system size is scaled by k times, i.e., each period in

the original problem is split into k statistically independent and identical periods and the capacities

are scaled up k times. Following works subsequently improves this bound to O(1) (e.g. (Reiman and

Wang, 2008; Jasin and Kumar, 2012; Bumpensanti and Wang, 2020; Vera and Banerjee, 2020)).

However, these methods are developed for a stationary setting under the ﬁnite support assumption

over the distributions and do not account for the achievability of a sublinear regret under a non-

stationary setting where the distribution at each period could be arbitrarily diﬀerent from each

other. Along this line, a recent work (Banerjee and Freund, 2020a) considers a setting that the

length horizon T is random, but the paper still requires the underlying distribution to be ﬁnite-

support and a “pseudo-stationary” environment where a concentration over time holds. The most

similar result to Theorem 1 is from (Devanur et al., 2019) where the authors derives a 1 − 1/

√

cmin

competitive ratio under the non-stationary setting, where cmin = min{c1, c2, . . . , cm} is the mini-

mal budget and they assume that ft(·) and gt(·) are all linear functions for each t. However, the

competitive ratio result cannot be translated into a regret bound in our setting since we do not

assume any relationship between the horizon length T and the initial budget c. Speciﬁcally, the

method therein is based on showing that the arrivals possess a concentration property and applying

Chernoﬀ-type inequality to derive high probability bounds on the event that all the constraints

are not violated. In contrast, our method is based on using the dual variable pt to balance the

budget consumption on every sample path and showing that the dual variable is bounded over time

according to our update rule (7). A recent work (Cheung et al., 2020) also derives an O(

T ) regret

√

bound for the setting of non-stationary environment with known distribution. The analysis therein

builds upon a static dual-based algorithm; we defer more discussions to Section 6.2 and Section 7.

In the following sections, we continue our pursuit and investigate how the IGD algorithm rolls out

in a non-stationary environment when the true distribution is unknown.

Proposition 2 Under Assumption 1, there is no algorithm that can achieve a regret better than
˜Ω(

T ) where ˜Ω hides a logarithmic term.

√

Proposition 2 constructs a problem instance showing that even for a stationary setting where Pt for
each t is identical to each other and known a priori, the lower bound of any online policy is ˜Ω(

T ).

√

For the problem instance, the underlying distribution Pt takes an inﬁnite support. We note that

when the parameter distribution has a ﬁnite support, an O(1) regret bound can be derived following

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

19

the approach in Vera and Banerjee (2020); Banerjee and Freund (2020a,b), which implies that the

gap between O(

T ) and O(1) is caused by whether the support of the parameter distribution is

√

inﬁnite or ﬁnite. In Section 6.1, we will further exploit the ﬁnite support structure and achieve

better regret bound. The proof of Proposition 2 builds upon the analysis of Lemma 1 in Arlotto

and Gurvich (2019). Diﬀerent from the existing lower bound examples (Arlotto and Gurvich, 2019;

Bumpensanti and Wang, 2020; Balseiro et al., 2020), the distribution of our problem instance bears

no dependence on the horizon T ; that is, the same static problem instance establishes the lower

bound for all T . Theorem 1 and Proposition 2 altogether state that Algorithm 1 is optimal in a

worst-case sense when no additional structure is imposed on Pt’s.

4. Non-stationary Environment with Prior Estimate: Wasserstein Based

Ambiguity and Analysis

In this section, we consider a “data-driven” setting where the true distribution is unknown, but

a prior estimate of the true distribution is available. The setting relaxes the assumption on the

exact knowledge of the true distribution in the last section. In practice, the availability of the

prior estimate may characterize the predictable patterns of the non-stationarity in various appli-

cation contexts. For example, the decision maker may not be able to foresee the future demand

(distribution), but (s)he can construct some estimate based on history data or domain expertise

of demand seasonality, the day-of-week eﬀect, and demand surge due to pre-scheduled promotions

or shopping festivals. When such prior estimate is accurate (the same as the true distribution),

the setting of prior estimate in this section reduces to the discussion in the last section. However,

when the prior estimate deviates from the true distribution, as is often the case in reality, then two

natural questions are: (i) how to properly measure the inaccuracy of the prior estimate from the

true distribution, (ii) how to design and analyze algorithm with such prior estimate. We answer

these two questions in this section.

4.1. Wasserstein-Based Measure of Deviation

Consider the decision maker has a prior estimate/prediction ˆPt for the true distribution Pt for each
t, and all the predictions { ˆPt}T
t=1 are made available at the very beginning of the procedure. We use
the Wasserstein distance between ˆPt and Pt to measure the deviation of the prior estimate from
the true distribution. In following, we ﬁrst formalize the deﬁnition and then discuss the suitability

of the proposed Wasserstein-based measure.

The Wasserstein distance, also known as Kantorovich-Rubinstein metric or optimal transport

distance (Villani, 2008; Galichon, 2018), is a distance function deﬁned between probability dis-

tributions on a metric space. Its notion has a long history dating back over decades ago and

20

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

gains increasingly popularity in recent years with a wide range of applications including genera-

tive modeling (Arjovsky et al., 2017), robust optimization (Esfahani and Kuhn, 2018), statistical

estimation (Blanchet et al., 2019), etc. In our context, the Wasserstein distance for two probability

distributions Q1 and Q2 on the metric parameter space (Θ, BΘ) is deﬁned as follows,

W(Q1, Q2) :=

inf
Q1,2∈J (Q1,Q2)

(cid:90)

ρ(θ1, θ2)dQ1,2(θ1, θ2)

(8)

where J (Q1, Q2) denotes all the joint distributions Q1,2 for (θ1, θ2) that have marginals Q1 and

Q2. The distance function ρ(·, ·) is deﬁned earlier in (2).

We deﬁne the following Wasserstein-based deviation budget (WBDB) to measure the cumulative

deviation of the prior estimate,

WT (P, ˆP) :=

T
(cid:88)

t=1

W(Pt, ˆPt)

where P = (P1, ..., PT ) denotes the true distribution and ˆP = ( ˆP1, ..., ˆPT ) denotes the prior estimate.

Based on the notion of WBDB, we deﬁne a set of distributions

ΞP (WT ) := {P : WT (P, ˆP) ≤ WT , P = (P1, ..., PT )}

for a non-negative constant WT , which we call as deviation budget. In this section, we consider

a regret based on the set ΞP as deﬁned in (3). In this way, the regret characterizes a worst-case

performance of a certain algorithm for all the distributions P = (P1, ..., PT ) within the set ΞP

prescribed by some WT . Speciﬁcally, the deviation budget WT deﬁnes the set ΞP by inducing an

upper bound for the deviation of prior estimate. Our next theorem provides an intuitive result that

WT is an inevitable loss (in terms of the algorithm regret) as a result of the inaccuracy of the prior

estimate.

Theorem 2 Under Assumption 1, if we consider the set ΞP (WT ) := {P : WT (P, ˆP) ≤ WT , P =

(P1, ..., PT )}, there is no algorithm that can achieve a regret better than Ω(max{

T , WT }).

√

Theorem 2 states that the lower bound of the regret is Ω(max{

√

T , WT }). The theorem charac-

terizes the best achievable algorithm performance under an inaccurate prior estimate, and precisely,

√

the lower bound is linear in respect with the deviation of the prior estimate from the true distribu-

tion. The Ω(

T ) part inherits the result in Proposition 2 and it captures the intrinsic uncertainty of

the underlying stochastic process over a time horizon T . The Ω(WT ) part captures the uncertainty

arising from the inaccurate prior estimate.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

21

4.2. Informative Gradient Descent Algorithm with Prior Estimate

Now we apply our informative gradient descent algorithm to the setting of prior estimate. A natural
idea is to pretend that the prior estimate ˆPt is indeed the true distribution Pt. To implement the
idea, we deﬁne

ˆL(p) = c(cid:62)p +

T
(cid:88)

t=1

ˆPth(p; θ)

where the true distribution Pt is replaced by its estimate ˆPt for each component in function L(·).
Thus it can be viewed as an approximation for the true dual function L(·) based on prior estimate.
Let ˆp∗ denote an optimal solution to ˆL(·),

ˆp∗ ∈ argminp≥0

ˆL(p)

(9)

and for each t, deﬁne

ˆγt := ˆPtg( ˆx(θ); θ) where ˆx(θ) = argmaxx∈X {f (x; θ) − ( ˆp∗)(cid:62) · g(x; θ)}.

(10)

Here, ˆγt denotes the “optimal” expected budget consumption in the t-th time under the prior esti-

mate. In the setting of prior estimate, we do not have the exact knowledge of the true distributions

Pt’s and therefore γt’s, so we alternatively use ˆγt as a substitute. Thus, the algorithm for the prior

estimate setting, denoted by IGD(ˆγ), implements Algorithm 1 with the input ˆγ deﬁned by (10).

Speciﬁcally, the dual update step will become

(cid:18)

pt+1 =

pt +

1
√
T

(g( ˜xt; θt) − ˆγt)

(cid:19)

∨ 0

(11)

in Algorithm 1.

4.3. Regret Analysis

The analysis of IGD(ˆγ) is slightly more complicated than that of IGD(γ) in theorem 1 because
the algorithm is built upon the function ˆL(·) deﬁned by the prior estimate instead of the true
distribution. So we ﬁrst study how to bound the diﬀerence between the function ˆL(·) and L(·)

using the deviation between prior estimate and true distribution. For a probability measure Q over

the metric parameter space (Θ, BΘ), we denote

LQ(p) := Qh(p; θ) =

(cid:90)

θ(cid:48)∈Θ

h(p; θ(cid:48))dQ(θ(cid:48)).

Then the function ˆL(·) and L(·) can be expressed as

ˆL(p) = p(cid:62)c +

T
(cid:88)

t=1

L ˆPt

(p) and L(p) = p(cid:62)c +

T
(cid:88)

t=1

LPt(p)

22

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Lemma 3 states that the function LQ(p) has certain “Lipschitz continuity” in regard with the

underlying distribution Q. Speciﬁcally, the supremum norm between two functions LQ1(p) and
LQ2(p) is bounded by the Wasserstein distance between two distributions Q1 and Q2 up to a
constant dependent on the dimension and the boundedness of the function’s argument.

Lemma 3 For two probability measures Q1 and Q2 over the metric parameter space (Θ, BΘ), we

have that

sup
p∈Ω ¯p

|LQ1(p) − LQ2(p)| ≤ max{1, ¯p} · (m + 1)W(Q1, Q2)

(12)

where Ω ¯p = [0, ¯p]m and ¯p is an arbitrary positive constant.

Note that the Lipschitz constant in Lemma 3 involves an upper bound of the function argument p.

The following lemma provides such an upper bound for the dual price pt in IGD(ˆγ). The derivation

is essentially the same as Lemma 2.

Lemma 4 For each t = 1, 2, . . . , T , we have that (cid:107)pt(cid:107)∞ ≤ q + 1 with probability 1, where pt is

speciﬁed by (11) in IGD(ˆγ).

The rest of the analysis for IGD(ˆγ) is similar to that of IGD(γ) in Theorem 1. The regret of

IGD(ˆγ) is formally stated in Theorem 3. Notably, the algorithm’s regret matches the lower bound

of Theorem 2 and thus it establishes the optimality of the algorithm.

Theorem 3 Under Assumption 1, suppose a prior estimate ˆP is available and the regret is deﬁned

based on the set ΞP (WT ), then the regret of IGD(ˆγ) has the following upper bound

RegT (πIGDP) ≤ O(max{

√

T , WT })

where πIGDP stands for the policy speciﬁed by IGD(ˆγ).

We remark the algorithm IGD(ˆγ) does not depend on or utilize the knowledge of the quantity

WT . On the upside, this avoids the assumption on the prior knowledge of WT (as the knowledge

of variation budget VT (Besbes et al., 2014, 2015; Cheung et al., 2019)). On the downside, there

is nothing the algorithm can do even when it knows a priori WT is small or large. Technically, it

means for our algorithm IGD(ˆγ), the WBNB contributes nothing in the dimension of algorithm

design, and it will only inﬂuence the algorithm analysis. In particular, if we compare Theorem 3

with Theorem 1, the extra term WT captures how the deviation of the prior estimate from the

true distribution will deteriorate the performance of the gradient-based algorithm. When WT is

small, the O(

T ) will be dominant and we do not need to worry about the deviation because its

√

eﬀect on the regret is secondary. In this light, the regret bound illuminates the eﬀect of model

misspeciﬁcation/estimation error on the algorithm’s performance in a non-stationary environment.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

23

5. Non-stationary Environment Without Prior Estimate

In this section, we consider an uninformative setting where the true distribution is completely

unknown to the decision maker. To one end, the discussion in this section can be viewed as a

reduction of the results in the last section to a setting with an “uninformative” prior estimate.

To the other, the uninformative setting draws an interesting comparison with the literature on

(unconstrained) online learning/optimization in non-stationary environment (Besbes et al., 2014,

2015; Cheung et al., 2019) and its analysis exempliﬁes the interaction between the constraints and

the non-stationarity.

5.1. Wasserstein-based Non-stationarity

We ﬁrst illustrate how the non-stationarity over {Pt}T

t=1 interplays with the constraints through
the following example adapted from (Golrezaei et al., 2014). The original usage of the example

in their paper is to stress the importance of balancing resource consumption in an online setting.

Speciﬁcally, consider the following two linear programs as the underlying problem (PCP) for two

online stochastic optimization problems,

max x1 + ... + xc + (1 + κ)xc+1 + ... + (1 + κ)xT

s.t. x1 + ... + xc + xc+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

max x1 + ... + xc + (1 − κ)xc+1 + ... + (1 − κ)xT

s.t. x1 + ... + xc + xc+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

(13)

(14)

where κ ∈ (0, 1), c = T

2 and the true distributions for both scenarios are point-mass distributions.
Without loss of generality, we assume c is an integer. For the ﬁrst LP (13), the optimal solution is

to wait and accept the later half of the orders, while for the second LP (14), the optimal solution

is to accept the ﬁrst half of the orders and deplete the resource at half time. The contrast between

these two LPs (two scenarios of whether the ﬁrst half or the second half is more proﬁtable) creates

diﬃculty for the online decision making. Without knowledge of the future orders, there is no way

we can obtain a sub-linear regret in both scenarios simultaneously. Because if we exhaust too much

resource in the ﬁrst half of the time, then for the ﬁrst scenario (13), we do not have enough capacity

to accept all the relatively proﬁtable orders in the second half. On the contrary, if we have too much

remaining resource at the half way, then for the second scenario (14), those relatively proﬁtable

orders that we miss in the ﬁrst half are irrevocable.

24

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

An equivalent view of these two examples is to consider the existence of an adversary: the

adversary is aware of the policy of the decision maker at the very beginning and then chooses the

distribution Pt in an adversarial manner. Speciﬁcally, the adversary acts against us and aims to

maximize the optimality gap between the oﬄine optimal objective value and the online objective

value. For example, in (13) and (14), the adversary can make a decision of which scenario for us

to enter for the second half of the time based on our remaining inventory at the half way. The

adversary view augments our previous interpretation of WT as the maximal derivation (of the prior

estimate from the true distribution): the regret deﬁnition based on ΞP in the last section can be

viewed as a partially adversarial setting where the adversary chooses the true distribution against

our will in a sequential manner but the choice of the distributions Pt’s is subject to the set ΞP .

Then the parameter WT that deﬁnes the set ΞP serves as a measure of both the estimation error

and the intensity of adversity.

Proposition 3 The worst-case regret of online constrained stochastic optimization in an adver-

sarial setting is Ω(T ).

Proposition 3 states that a fully adversarial setting where Pt can change arbitrarily over t does

not permit a sub-linear regret. The same observation is also made in the literature (Besbes et al.,

2014, 2015; Cheung et al., 2019) for unconstrained online learning problems where there is no

function g(·) and the decision xt is made before the revealing of f (·). Speciﬁcally, Besbes et al.

(2015) propose a novel measure of non-stationarity as follows (in the language of our paper),

VT :=

T −1
(cid:88)

t=1

T V (Pt, Pt+1)

where T V (·, ·) denotes the total variation distance between two distributions. The quantity VT

represents the cumulative temporal change of the distributions by comparing Pt and Pt+1. Unfor-

tunately, such temporal measure fails in the constrained setting. Note that for both (13) and (14),

there is only one change point throughout the whole procedure thus the non-stationarity; their

temporal change measure is O(1) but a sub-linear regret is still unattainable.

Now, we propose the deﬁnition of the Wasserstein-based non-stationarity budget (WBNB) as

WT (P) :=

T
(cid:88)

t=1

W(Pt, ¯PT )

(cid:80)T

where P = (P1, ..., PT ) and ¯PT is deﬁned to be the uniform mixture distribution of {Pt}T
t=1, i.e.,
¯PT := 1
t=1 Pt. The non-stationarity measure WBNB can be viewed as a degeneration of our
previous deviation measure WBDB in that WBNB replaces all the prior estimates ˆPt’s with the
uniform mixture ¯PT . The caveat is that in the uninformative setting, no distribution knowledge

T

√

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

25

is assumed, so the decision maker does not have access to ¯PT unlike the prior estimate in the
last section. As we will see shortly, the knowledge of ¯PT does not aﬀect anything in terms of the
algorithm design and analysis.

Based on the notion of WBNB, we deﬁne a set of distributions

ΞU (WT ) = {P : WT (P) ≤ WT , P = (P1, ..., PT )}

for a non-negative constant WT , which we call as variation budget. Throughout this section, we

consider a regret based on the set ΞU as deﬁned in (3) in aim to characterize a “worst-case”

performance of certain policy/algorithm for all the distributions in the set ΞU .

The variation budget WT deﬁnes the uncertainty set ΞU by providing an upper bound on the

non-stationarity of the distributions. Our next theorem states that it is impossible to get rid of WT

in the regret bound of any algorithm, which illustrates the sharpness of our deﬁnition of WBNB.

Intuitively, it means that apart from the intrinsic stochasticity term O(

T ), the (unknown) non-

stationarity of the underlying distributions deﬁned by WBNB appears to be a second bottleneck

for algorithm performance. The proof of the theorem follows the same argument as Theorem 2.

Theorem 4 Under Assumption 1, if we consider the set ΞU (WT ) = {P : WT (P) ≤ WT , P =

(P1, ..., PT )}, there is no algorithm that can achieve a regret better than Ω(max{

T , WT }).

√

5.2. Algorithm and Regret Analysis

One pillar of designing IGD(γ) and IGD(ˆγ) is the budget allocation plan γt’s (or ˆγt) prescribed by

either the true distribution or the prior estimate. In the uninformative setting, the most straight-

forward (and probably optimal) plan is to allocate the budget evenly over the entire horizon.

Algorithm 2 – uninformative gradient descent (UGD) – implements the intuition by evenly dis-

tributing the budget without referring to any information. Thus the UGD algorithm can be viewed
(cid:1) . Returning to the point mentioned earlier on the knowledge of the centric distribution
as IGD(cid:0) c
¯PT , it does not matter we know it or not; because as long as all the prior estimate distributions ˆPt
are the same over time, we always have the same budget allocation plan. Furthermore, when all the

T

Pt’s are the same, which means the variation budget WT = 0, Algorithm 2 and its analysis collapse

into several recent studies on the gradient-based online algorithm under a stationary environment

(Lu et al., 2020; Li et al., 2020).

Theorem 5 Under Assumption 1, if we consider the set ΞU (WT ) = {P : WT (P) ≤ WT , P =

(P1, ..., PT )}, then the regret of Algorithm 2 has the following upper bound

RegT (πUGD) ≤ O(max{

√

T , WT })

where πUGD stands for the policy speciﬁed by Algorithm 2.

26

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Algorithm 2 Uninformative Gradient Descent Algorithm (UGD)

1: Initialize the initial dual price p1 = 0 and initial constraint capacity c1 = c.

2: for t = 1, ..., T do

3:

Observe θt and solve

˜xt = argmaxx∈X {f (x; θt) − p(cid:62)

t · g(x; θt)}

where g(x, θt) = (g1(x, θt), ..., gm(x, θt))(cid:62)

4:

Set

xt =

5:

Update the dual price

(cid:40) ˜xt,

if ct permits a consumption of g( ˜xt; θt)

0,

otherwise

(cid:18)

pt+1 =

pt +

(cid:16)

1
√
T

g( ˜xt; θt) −

(cid:17)(cid:19)

c
T

∨ 0

(15)

where the element-wise maximum operator u ∨ v = max{v, u}

6:

Update the remaining capacity

ct+1 = ct − g(xt; θt)

7: end for

8: Output: x = (x1, ..., xT )

Theorem 5 states the upper bound of Algorithm 2, which matches the regret lower bound in

Theorem 4. Remarkably, the factors on T and WT are additive in the regret upper bound of

Algorithm 2. In comparison, the factor on T and the variation budget VT are usually multiplicative

in the regret upper bounds in the line of works that adopts the temporal change variation budget

as nonstationary measure (Besbes et al., 2014, 2015; Cheung et al., 2019). The price of such an

advantage for WBNB is that the WBNB is a more restrictive notion than the variation budget;

recall that in (13) and (14), the temporal change variational budget is O(1), but the WBNB is

O(κT ). Again, as the setting with prior estimate, the knowledge of the quantity WT does not aﬀect

the algorithm design. By putting together Theorem 4 and Theorem 5, we argue that the knowledge

of WT does not help to further improve the algorithm performance.

6. Extensions and Discussions

6.1. Improved Regret Bound under Finite Support Assumption

In previous sections, we impose no additional structure on the underlying distributions Pt’s other

than the nonstationarity variation WT . A natural question is whether a better regret bound is

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

27

achievable when the underlying distribution has more structures. In this subsection, we provide a

positive answer to the question under an online LP formulation. Speciﬁcally, for the online linear

programming problem, θt = (rt, at) and xt ∈ [0, 1]. The functions f (xt, θt) = rtxt and gi(xt, θt) =

aitxt are linear. We make the following assumption on the randomness of θt.

Assumption 2 For each t, the distribution of θt has a ﬁnite support, denoted by {θ(1), . . . , θ(n)}.

Moreover, there exists pmin > 0 such that for each t = 1, . . . , T and each j = 1, . . . , n, it holds that

P (θt = θ(j)) ≥ pmin.

Under this ﬁnite support assumption, the online LP problem is also known as the quantity based

network revenue management problem and has been studied extensively in the literature. Here we

consider a setting where the true distribution Pt that governs θt is unknown but a prior estimate
ˆPt is available. Jasin and Kumar (2012); Vera et al. (2019) develop re-solving algorithms and derive
bounded regret results when the underlying distribution is known, i.e., ˆPt is accurate. Compared
to these existing works, our setting of prior estimate captures the potential misspeciﬁcation or

estimation error of the underlying distribution. Thus our analysis examines the robustness of the

online algorithm against such error.

Next, we introduce an algorithm that extends the Fluid Bayes Selector algorithm in (Vera

et al., 2019). Speciﬁcally, our algorithm uses the prior estimate instead of the true distribution for

prescribing decisions. To describe the algorithm, we denote by ct the remaining capacity at the

beginning of each period t, and denote by H(t) = {θt, . . . , θT } the (future) trajectory from period t

to period T . Let Hj(t) be the number of times that θτ is realized as θ(j) for τ = t, . . . , T . Then, the

trajectory H(t) can be equivalently represented by H(t) = (H1(t), . . . , Hn(t)). The oﬄine hindsight

problem (PCP) with the remaining capacity ct from period t to period T can be written as

R∗

t (ct, H(t)) = max

n
(cid:88)

rjzj,

j=1
n
(cid:88)

s.t.

ajizj ≤ cti, ∀i = 1, . . . , m,

j=1
0 ≤ xj ≤ Hj(t), ∀j = 1, . . . , n

where the decision variable zj can be interpreted as the number of accepted orders of the j-th type,

and each unit acceptance is associated with an reward rj and a resource consumption vector of

(aj1, ..., ajm). By taking expectation of the arrival counts Hj(t), an upper bound on the expected

optimal objective value of the hindsight problem can be obtained as follows

RUB
t

(ct, P) = max

n
(cid:88)

j=1

rjzj,

(16)

28

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

s.t.

n
(cid:88)

j=1

ajizj ≤ cti, ∀i = 1, . . . , m,

0 ≤ zj ≤ EH∼P[Hj(t)], ∀j = 1, . . . , n,

where P = (Pt, ..., PT ) encapsulates the underlying distributions. The Fluid Bayes Selector in (Vera

et al., 2019) solves the LP (16) and uses its optimal solution to guide the online decisions. As the

resource capacity cit changes over time and the LP (16) is solved at each time period, this type of

algorithms is known as a re-solving algorithm.

When the true distribution P is unknown but some prior estimate ˆP is available, a natural idea

is to calculate the right-hand-side of constraints (the expectations) in (16) using the prior estimate

distributions. Speciﬁcally, in Algorithm 3, the decision maker ﬁrst observes the parameter type jt
and refer to the optimal solution of (16) (using ˆP instead of P as its input) to choose the value of

xt.

Algorithm 3 Re-solving with Prior Estimate

1: Initialize constraint capacity c1 = c.

2: for t = 1, ..., T do

3:

4:

5:

6:

7:

Solve RUB

t

(ct, ˆP) in (16) and obtain {ˆzj(ct)}n

j=1 as one optimal solution.

Observe the realization of θt as θ(jt).
2 · E
if ˆzjt(ct) ≥ 1
else set xt = 0.

H∼ ˆP[Hjt(t)], then set xt = 1.

end if

8: end for

9: Output: x = (x1, ..., xT )

Theorem 6 Suppose a prior estimate ˆP is available and the regret is deﬁned based on the set

ΞP (WT ). Then, under Assumption 1 and Assumption 2, the regret of Algorithm 3 has the following

upper bound

RegT (πResolve) ≤ O

(cid:18) 1
pmin

(cid:19)

(1 + WT )

where πResolve stands for the policy speciﬁed by Algorithm 3.

Theorem 6 states the regret bound of Algorithm 3. We provide two ways to interpret the regret

bound. First, when the prior estimate is accurate (WT = 0), the algorithm and its analysis reduce

to the results in (Vera and Banerjee, 2020). Compared to the O(

T ) result in Section 3, the

√

bounded regret here relies on the ﬁnite-supportedness and the existence of pmin in Assumption

√

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

29

2. Speciﬁcally, the assumption essentially requires the distributions Pt to be almost stationary

over time (some concentration property holds) and thus guarantees that the budget consumption

stays close to its expectation. Banerjee and Freund (2020a,b) further relax the assumption into a

concentration condition and obtain more general results for problems such as online bin packing.

When Assumption 2 or the concentration condition does not hold, the analyses in (Vera and

Banerjee, 2020; Banerjee and Freund, 2020a,b) no longer work, but our regret bound of O(

T )

in Section 3 is still valid as it does not impose any condition on Pt’s. Second, when the prior

estimate is inaccurate (WT > 0), the regret bound in Theorem 6 captures the algorithm performance

deterioration caused by the inaccurate prior estimation. Importantly, this term related to WT is

not reducible even with stronger condition of the functions f and gi such as Assumption 2. In

this sense, the result extends the existing ones which assumes the knowledge of true distributions

(Vera and Banerjee, 2020; Banerjee and Freund, 2020a,b) to a setting with model (distribution)

misspeciﬁcation.

6.2. Sub-optimality of Static Policies

Both of our main algorithms – Algorithm 1 and Algorithm 2 are gradient based. Compared to

Algorithm 3 and other existing re-solving algorithms (Jasin and Kumar, 2012; Bumpensanti and

Wang, 2020; Vera and Banerjee, 2020), the gradient-based algorithms feature for simplicity and

computational eﬃciency. In addition, the gradient-based algorithms have an adaptive and dynamic

design that is crucial in stabilizing the resource consumption (i.e., not to exhaust the resource too

early or have too much resource left-over). As discussed in Section 3.2, this is achieved inherently

by using the realized resource consumption at each time period in the gradient update. We argue

that such a dynamic design that relates the online decisions with the realized resource consumption

process is necessary in achieving an optimal order of regret. In contrast, for a static policy, the

online decisions can be dependent on the realized parameters θt’s but will not be aﬀected by the

dynamic of the resource consumption process. We remark that a static policy can utilize the prior

estimate and be time-dependent; by “static”, it means the policy remains the same regardless the

realization of the resource consumption process. Examples of static policies include the bid-price

policy (Talluri and Van Ryzin, 1998) and the oﬄine-to-online policy (Cheung et al., 2020).

Deﬁnition 1 A static policy π is described by a set of functions {hπ

t : Θ → X is
allowed to be a random function. At each period t, given the type θt, the policy π will take the

t=1, where hπ

t }T

action hπ

t (θt) if the budget constraints are not violated.

Next, we illustrate the sub-optimality of any static policy’s for both of the two settings. First,

for the uninformative setting in Section 5, a static policy clearly cannot work since there is no

30

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

prior information that can be used to design the policy. Second, for the data-driven setting in

Section 4, it is not as obvious whether a static policy can achieve the same order of regret optimality

as the gradient-based algorithms. For example, what if we simply use the “oﬄine” optimal dual

solution p∗ or ˆp∗ to form a static decision rule throughout the procedure? This implements bid-

price policy (Talluri and Van Ryzin, 1998) for the network revenue management problem under the

known distribution setting. In Section D2.2, we show that this bid-price policy can incur a linear

regret under an environment with slight non-stationarity (arbitrarily small WT ). The following

proposition provides a more general statement on the sub-optimality of any static policy under a

non-stationarity environment.

Proposition 4 Suppose π is a static policy such that E
T for some
constant C1 > 0, where ˆP = { ˆP1, . . . , ˆPT } denotes the set of prior estimates. There exists a true
distribution P = {P1, . . . , PT } such that WT (P, ˆP) ≤ WT = 4

C1 · T 3/4 and EH∼P[RUB

T − RT (π)] ≤ C1 ·

H∼ ˆP[RUB

T − RT (π)] ≥

√

√

C2 · T for some constant C2 > 0.

In the proposition, π denotes an arbitrary static policy that achieves an O(

T ) regret when
the prior estimate ˆP is accurate. But when there is a diﬀerence between the prior estimate ˆP and

√

P and even if the deviation budget WT is sublinear in T , the static policy π may still incur a

linear regret for some problem instances. The implication is that for a static policy that works

well under a known distribution setting, its performance can drastically deteriorate when there

exists an estimation error or non-stationarity. Thus the dynamic design of gradient update or re-

solving is both eﬀective and necessary in overcoming the estimation error and the environment

non-stationarity.

6.3. Advantage of Wasserstein distance

In the previous sections, we use Wasserstein distance to deﬁne both the deviation budget WBDN

and the non-stationarity budget WBNB. We note that the analyses and regret bounds still hold

if we change the underlying distance to total variation distance or KL-divergence. We choose

the Wasserstein distance because it is a tighter measure than the total variation distance or the

KL-divergence for deﬁning WBNB. If we revisit the examples (13) and (14), a smaller value of

κ ∈ (0, 1) should indicate a smaller variation/non-stationarity between the ﬁrst half and the second

half of observations in both examples. However, the total variation distance fails to characterize

for t ≤ T

this subtlety in that for any non-zero value of κ, the total variation distance between Pt and Pt(cid:48)
2 < t(cid:48) is always 1 (since Pt and Pt(cid:48) have diﬀerent supports). In other words, if we replace
the Wasserstein distance with the total variation distance in our deﬁnition of WBNB, then the

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

31

quantity will always be T

2 for all κ ∈ (0, 1). Formally, for any two distributions P1 and P2, the

following inequality holds (Gibbs and Su, 2002),

W(P1, P2) ≤ TV(P1, P2).

Interestingly, this coincides with the intuitions in the literature of generative adversarial network

(GAN) where Arjovsky et al. (2017) replace the KL-divergence with the Wasserstein distance in

training GANs. Simultaneously and independently, Balseiro et al. (2020) analyze the dual mirror

descent algorithm under a similar setting as our results in this section. The paper only discusses our

uninformative setting, but not the known true distribution setting and the prior estimate setting.

For the uninformative setting, their deﬁnition of non-stationarity is parallel to WBNB; both can

be viewed as a reduction from the more general WBDB. The key diﬀerence is that Balseiro et al.

(2020) from WBNB consider the total variation distance, which inherits the deﬁnition of variation

functional from (Besbes et al., 2015).

A recent paper Cheung et al. (2020) proposes to measure the deviation budget as follows:

Ψ(P, ˆP) =

1
m + 1

·

sup
x(θ):Θ→X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

Eθt∼Pt [(f (x(θt), θt), g(x(θt), θt))] − E

θt∼ ˆPt

[(f (x(θt), θt), g(x(θt), θt))]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

where m is the number of resource constraints. The measure is used to analyze the proposed

oﬄine-to-online policy, and it captures the supremum distance between the true environment and

the estimated environment under the same decision function x(θ). Apart from the static policy’s

sub-optimality discussed earlier, the measure is also looser than the Wasserstein-based measure.

Speciﬁcally, as the previous examples (13) and (14), if we choose the function x(θ) such that

x(θ) = 0 when θ = 1 and θ = 1 − κ (the reward rt is 1 or 1 − κ) and x(θ) = 1 when θ = 1 + κ (the
reward rt is 1 + κ), then Ψ(P, ˆP) is always lower bounded by

2(m+1) regardless of the value of κ. In
contrast, the Wasserstein distance is more sensitive and can capture the intensity of the parameter

T

κ.

An additional practical beneﬁt of Wasserstein distance is that the measure features for natu-
ral data-driven bounds. For example, the prior estimate ˆPt can be constructed by the empirical

distribution over Nt history samples over Pt. Suppose that there exists a constant c1 such that
ρ(θ1, θ2) ≤ c1 · (cid:107)θ1 − θ2(cid:107)2. Then the following bound can be obtained for W(Pt, ˆPt) (Theorem 1
of Fournier and Guillin (2015)), which explicitly relates the deviation (from ˆPt to Pt) with the

number of history samples:

(cid:16)

(cid:17)
W(Pt, ˆPt) ≤ (cid:15)

P

≥ 1 − c2 · exp(−c3 · Nt(cid:15)max{K,2})

32

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

where K is the dimension of the parameter θt, and c2, c3 are two positive constants. With (cid:15) = 1√
T
and Nt = 1
c3

2 ·max{K,2} · log(T /δ), we have the following inequality holds

· T 1

(cid:18)

P

W(Pt, ˆPt) ≤

(cid:19)

1
√
T

≥ 1 −

c2 · δ
T

holds for each t. Therefore, from union bound over t = 1, . . . , T , we know that when Nt = 1
c3
T 1

2 ·max{K,2} · log(T /δ), we have

·

WT =

T
(cid:88)

t=1

W(Pt, ˆPt) ≤

√

T

holds with probability at least 1 − c2 · δ.

6.4. Algorithm with Random Reward and Consumption

In this subsection, we consider another natural extension of our model where the reward and the

budget consumption can be stochastic after the parameter θt is revealed and the action xt is made.

To be speciﬁc, given each x and θ, the reward f (x; θ) and the budget consumption g(x; θ) are

all random variables. Such an extension allows us to cover the price-based NRM problem and the

choice-based NRM problem ﬁrst described in Section 2.2.

We note that the algorithms and results in the previous sections can be directly extended to

this setting. We ﬁrst illustrate the setting of known distribution described in Section 3 where the

distributions Pt’s are known a priori. For each x and θ, we introduce

ˆf (x; θ) := E[f (x; θ)] and ˆg(x; θ) := E[g(x; θ)]

where the expectation is taken with respect the functions (f and g) and conditional on x and θ.
Then, we use ˆf , ˆg to revise the deﬁnition of the function h by

h(p; θ) := max
x∈X

{ ˆf (x; θ) − pT ˆg(x; θ)}.

Accordingly, the function L can be deﬁned by the new function h

L(p) := cT p +

T
(cid:88)

t=1

Pth(p; θ)

(17)

and then the deﬁnition of γ is given by

γt := Pt ˆg(x∗(θ); θ) where x∗(θ) = argmaxx∈X { ˆf (x; θ) − (p∗)(cid:62) · ˆg(x; θ)}

(18)

where p∗ ∈ argminp≥0L(p). Also, the benchmark is set as RUB
in (4) with the functions f, g replaced
by their expectations ˆf , ˆg. While implementing the algorithm IGD(γ) in Algorithm 1, the decision

T

˜xt is then set as

˜xt = argmaxx∈X { ˆf (x; θt) − pT

t · ˆg(x; θt)}.

(19)

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

33

Note that (19) can be solved eﬃciently (see discussions on function h in Section A3). We have the

following regret upper bound regarding the algorithm IGD(γ) for this new setting with random

reward and resource consumption. The result can be viewed as a generalization of Theorem 1.

Theorem 7 Under Assumption 1, if we consider the set Ξ = {P : P = (P1, ..., PT ), ∀P1, . . . , ∀PT },

then the regret of IGD(γ) has the following upper bound

RegT (πIGD) ≤ O(

√

T )

where πIGD stands for the policy speciﬁed by IGD(γ) with γ computed from (18).

For the data-driven setting with prior estimate, the distance between two parameters θ, θ(cid:48) ∈ Θ

is adjusted by

ρ(θ, θ(cid:48)) := sup
x∈X

(cid:107)( ˆf (x; θ), ˆg(x; θ)) − ( ˆf (x; θ(cid:48)), ˆg(x; θ(cid:48)))(cid:107)∞,

(20)

and the deﬁnition of W(Pt, ˆPt) follows. Then, let the parameters ˆγ be computed from (10) with
respect to ˆf , ˆg. As a corollary of Theorem 7, we have the following regret bound under the data-

driven setting.

Corollary 1 Under Assumption 1, suppose a prior estimate ˆP is available and the regret is deﬁned
based on the set ΞP (WT ) = {P : (cid:80)T
t=1 W(Pt, ˆPt) ≤ Wt}, where the distance W(Pt, ˆPt) is deﬁned
following the distance in (20), then the regret of IGD(ˆγ) has the following upper bound

RegT (πIGDP) ≤ O(max{

√

T , WT })

where πIGDP stands for the policy speciﬁed by IGD(ˆγ).

For the uninformative setting where the prior estimates are not available, it is direct to see that the

regret bound in Corollary 1 continues to hold as long as WBNB is deﬁned following the distance

in (20).

6.5. Algorithm with Infrequent Update

In this subsection, we study the performance of our algorithm with an infrequent update scheme.

Speciﬁcally, the infrequent update refers to that the dual update step (7) for Algorithm 1 is not

done at each period, instead, the dual variable vector p is updated only after K periods. In this

way, our algorithm can be viewed as a batched algorithm, where the arrivals from period t to

period t + K, for some t, are viewed as a batch of samples to update the dual variable. Note that

the dual vector determines the online decision rule of the primal variable xt. An infrequent scheme

has the practical beneﬁts of inducing more consistency in the decision rule over time.

34

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Algorithm 4 Batched Informative Gradient Descent Algorithm (B-IGD(γ, K, αT,K))

1: Input: parameters γ = (γ1, . . . , γT ), the batch size K, and the step size αT,K.

2: Initialize the initial dual price p1 = 0 and initial constraint capacity c1 = c

3: for t = 1, ..., T do

4:

5:

Specify the constant l = (cid:98)t/K(cid:99) + 1, where (cid:98)·(cid:99) denotes the ﬂoor function.

Observe θt and solve

˜xt = argmaxx∈X {f (x; θt) − p(cid:62)

l · g(x; θt)}

where g(x, θt) = (g1(x, θt), ..., gm(x, θt))(cid:62)

6:

Set

xt =

(cid:40) ˜xt,

if ct permits a consumption of g( ˜xt; θt)

0,

otherwise

7:

If t = l · K, then we update the dual price



pl+1 =

pl + αT,K ·

lK
(cid:88)

τ =(l−1)K+1



(g( ˜xτ ; θτ ) − γτ )

 ∨ 0

(21)

where the element-wise maximum operator u ∨ v = max{v, u}

8:

Update the remaining capacity

ct+1 = ct − g(xt; θt)

9: end for

10: Output: x = (x1, ..., xT )

The algorithm is formally described in Algorithm 4. It takes the same structure as Algorithm 1

and Algorithm 2 but only updates the dual variables every K time periods. We have the following

regret bound for Algorithm 4 for the known distribution setting.

Theorem 8 Under Assumption 1, if we consider the set Ξ = {P : P = (P1, ..., PT ), ∀P1, . . . , ∀PT },

then the regret of B-IGD(γ, K, αT,K), with γ computed from (6), has the following upper bound

RegT (πB-IGD) ≤ O

T KαT,K +

(cid:18)

(cid:19)

+ K

1
αT,K

where πB-IGD stands for the policy speciﬁed by IGD(γ) in Algorithm 4.

With a choice of αT,K = 1√

T K

, we obtain an regret upper bound of O(

√

T K) for the known dis-

tribution setting. Following the same spirit, for the data-driven setting, we obtain the following

regret bound for Algorithm 4.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

35

Corollary 2 Suppose a prior estimate ˆP is available and the regret is deﬁned based on the set

ΞP (WT ). Then, under Assumption 1, the regret of B-IGD(ˆγ, K, αT,K), with ˆγ computed from (10),

has the following upper bound

RegT (πB-IGD) ≤ O(T KαT,K +

1
αT,K

+ K + WT )

where πB-IGD stands for the policy speciﬁed by B-IGD(ˆγ, K, αT,K).

With a choice of αT,K = 1√

T K

, we obtain the regret bound O(max{

√

T K, WT }) in Corollary 2 under

the data-driven setting. For the uninformative setting, it is clear to see that the regret bound in

Corollary 2 still holds when WT refers to the WBNB. For both regret bounds in Theorem 8 and

√

K is the price paid for the infrequent update.

Corollary 2, the additional factor of

7. Numerical Experiments

7.1. Experiment I: Online Linear Programming

We ﬁrst present some synthetic experiments for the setting of online LP where both the reward and

cost function are linear, i.e., ft(x) = rtx and git(x) = aitx for i = 1, ..., m and t = 1, ..., T . Suppose

that both the true distribution and the prior estimate distribution of ait follow Uniform[0.1, 1.1]

throughout the entire horizon. We consider three diﬀerent settings for the true distribution and

the prior estimate distribution of rt (summarized in Table 1). The parameters α and β are to be

speciﬁed: α reﬂects the intensity of the non-stationarity over time and β represents the error of

the prior estimate. Speciﬁcally, the deviation budget WBDB (in Section 4) grows linearly with β,

while the non-stationarity budget WBNB (in Section 5) grows linearly with α.

The ﬁrst setting considers a uniform distribution for rt: the true distribution of rt follows

Uniform[0, 1] for the ﬁrst half of the time and Uniform[0, α] for the second half, while the prior

estimate distribution follows Uniform[0, 1 + β] for the ﬁrst half of the time and Uniform[0, α + β]

for the second half.

The second setting considers a truncated normal distribution for rt: the true distribution of rt

follows Normal(1, 1) for the ﬁrst half of the time and Normal(α, 1) for the second half, while the

prior estimate distribution follows Normal(1+β, 1) for the ﬁrst half of the time and Normal(α+β, 1)

for the second half. All the normal distributions are made non-negative by truncating at 0.

The third setting considers a mixture of the previous two distributions of uniform and truncated

normal: the true distribution of rt follows a uniform mixture of Uniform[0, 1] and Normal(1, 1)

for the ﬁrst half of the time and a uniform mixture of Uniform[0, α] and Normal(α, 1) for the

second half, while the prior estimate distribution follows a uniform mixture of Uniform[0, 1 + β]

and Normal(1 + β, 1) for the ﬁrst half of the time and a uniform mixture of Uniform[0, α + β] and

36

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

True Distribution
rt

ait

Prior Estimate
ˆrt

ˆait

Periods
t = 1, . . . , (cid:98) T

The uniform setting

The normal setting

The mixed setting

t = 1, . . . , (cid:98) T

2 (cid:99)

2 (cid:99)

Unif[0, 1] Unif[0.1, 1.1] Unif[0, 1 + β] Unif[0.1, 1.1]
2 (cid:99) + 1, . . . , T Unif[0, α] Unif[0.1, 1.1] Unif[0, α + β] Unif[0.1, 1.1]
Norm(1, 1) Unif[0.1, 1.1] Norm(1 + β, 1) Unif[0.1, 1.1]
2 (cid:99) + 1, . . . , T Norm(α, 1) Unif[0.1, 1.1] Norm(α + β, 1) Unif[0.1, 1.1]
Unif[0.1, 1.1]
Unif[0.1, 1.1]

Unif[0.1, 1.1]
Unif[0.1, 1.1] Mix(α + β)

Mix(1)
2 (cid:99) + 1, . . . , T Mix(α)

Mix(β)

2 (cid:99)

t = (cid:98) T

t = (cid:98) T

t = (cid:98) T

t = 1, . . . , (cid:98) T

Table 1

Input of Experiment I: Unif[a, b] denotes the uniform distribution over the interval [a, b]. Norm(a, b)

denotes a normal distribution with mean a and standard deviation b, truncated at 0. Mix(a) denotes a uniform

mixture of Unif[0, a] and Norm(a, 1).

Normal(α + β, 1) for the second half. As the second setting, all the normal distributions are made

non-negative by truncating at 0.

For the numerical experiments, we implement Algorithm 1 – IGD(ˆγ) and Algorithm 2 (UGD)

under diﬀerent choices of α and β to study the algorithm performance under both settings in our

paper. Besides, we implement the classic ﬁxed bid price control heuristics (FBP) proposed in Talluri

and Van Ryzin (1998). The FBP method uses ˆp∗ computed from the prior estimate (9) as the

bid price. It then accepts the order (setting xt = 1) when rt ≥ a(cid:62)

t ˆp∗ and there is enough resource;
otherwise, it will reject the order (setting xt = 0). We also compare with the oﬄine-to-online (O2O)

algorithm proposed in Cheung et al. (2020). The O2O algorithm is a dual-based online algorithm

for a non-stationary setting. It ﬁrst runs an oﬄine procedure to construct a set of dual variables
using prior estimates { ˆPt}T
t=1. Then, at each period t, after observing the realized parameter θt,
the O2O algorithm draws a dual variable p uniformly randomly from the set, and then implements

˜xt = argmaxx∈X {f (x; θt) − p(cid:62) · g(x; θt)}. Note that the O2O algorithm is a static policy discussed
in Section 6.2. In our implementation, we select diﬀerent step sizes to generate multiple sets of

dual variables. Then, we apply each set of dual variables to the online problem and we select the

best reward to report the performance of the O2O algorithm.

Figure 1 and Tables 3-5 report the performances of the four algorithms with the horizon T = 1000,

number of constraints m = 10, and initial resource capacity ci = 200 for each i = 1, ..., m.

In Figure 1, the case of ﬁxed α and varying β examines the robustness of an algorithm to

prior estimation error (diﬀerent β) under a nonstationary environment (α > 0). In comparison,

Algorithm UGD and IGDP have signiﬁcant better performance than FBP and O2O. This shows

the eﬀectiveness of the gradient-based dynamic update and distinguishes our algorithms of UGD

and IGDP from static policies such as FBP and O2O. Speciﬁcally, the performances of FBP and

O2O deteriorate quickly as the estimation error β increases, while both gradient-based algorithms

remain stable. Note that FBP computes a ﬁxed dual price vector and O2O constructs a set of ﬁxed

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

37

(a) β = 0 for the uniform setting

(b) α = 2 for the uniform setting

(c) β = 0 for the normal setting

(d) α = 2 for the normal setting

(e) β = 0 for the mixed setting

(f) α = 2 for the mixed setting

Figure 1

The percentage of the total reward collected by IGDP, UGD, O2O and FBP over the upper bound for

diﬀerent α and diﬀerent β under diﬀerent setting.

dual vectors completely based on the prior estimates. However, as the estimation errors increase,

the gap between the “true” dual vector and the pre-computed dual vector(s) in FBP or O2O will

also increase, and the static nature of the two algorithms prevents a dynamic correction of the

38

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

dual vector. As a result, the performances of FBP and O2O become worse as the estimation error

increases. In contrast, the dynamic dual update of UGD and IGDP naturally combines the demand

realization and the prior estimates (through parameters ˆγ). Therefore, the performance of IGDP

remains stable as the estimation errors increase. These numerical evidence reinforces our previous

theoretical arguments on the sub-optimality of static policies in Section 6.2.

In Figure 1, the case of varying α and β = 0 corresponds to the setting of Section 5 where

the value of α indicates the intensity of the non-stationarity for the underlying distribution Pt’s.

The performance of UGD deteriorates as α increases, and thus it validates the role of WBNB in

characterizing the algorithm performance. The algorithms IGDP, O2O and FBP provide unsur-

prisingly better and more stable performance because all of them utilize the prior estimate (which

is exactly the true distribution when β = 0). This comparison highlights the eﬀectiveness of WBNB

in characterizing the learnability of a non-stationary environment when there is no prior knowledge

available (as for UGD), and also it underscores the usefulness of prior knowledge.

Based on these experiments, we make the following remarks: First, both of FBP and O2O

algorithms can deal with the non-stationarity to some extent (as in Figure 1(a)). However, their

performances highly depends on the accuracy of the prior estimates. If the deviation of the prior

estimates from the true distributions is large, both performances can become very poor. Second,

the performance of UGD is robust to the deviation of prior estimate since it does not utilize any

prior estimate. Meanwhile, the downside is that UGD is more sensitive to the intensity of non-

stationarity compared to the other two algorithms. Third, the performance of IGDP is relatively

more robust to both the non-stationarity and the deviation of the prior estimates. Speciﬁcally,

IGDP obtains parameters γt for each t to handle the non-stationarity and also utilizes the gradient

descent updates in a dynamic way to hedge against the deviation. In this light, IGDP combines

both the advantages of FBP, UGD and O2O.

7.2. Experiment II: Network Revenue Management with Resolving Heuristics

Our second numerical experiment is adapted from the network revenue management experiment

in Jasin (2015). We consider a hub-and-spoke model with 8 cities, 14 connecting ﬂights, and 41

itineraries. The detailed itinerary structure and the normalized capacities in our experiment are

referred to Table 1 and Table 2 in Jasin (2015), respectively. To model the non-stationarity, we
add random noises to the arrival probabilities of the itineraries. Speciﬁcally, we denote P0 ∈ R41 as

the arrival probabilities in Jasin (2015) and we set α ∈ [0, 1] as a parameter, which will be speciﬁed

later. For each t, the arrival probabilities are set as

Pt = (P0 + α · Unif(41, 1))/sum(P0 + α · Unif(41, 1))

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

39

where Unif(41, 1) denotes a 41-dimensional vector of i.i.d. uniformly distributed random variables

over [0, 1] and sum(·) denotes the summation of all the components of a vector. For each time t,

an independent random noise vector will be generated. In our experiment, the probability vector

Pt is only generated once at the very beginning, and it will remain the same in all the simulation

trials. Intuitively, the value of α represents the magnitude of adjustment compared to the original

probability P0, and thus it reﬂects the intensity of non-stationarity of the underlying distributions.
Similarly, we introduce another parameter β to reﬂect the deviation of the prior estimates ˆPt from
the true distribution Pt. Let

ˆPt = (Pt + β · Unif(41, 1))/sum(Pt + β · Unif(41, 1)).

We report the algorithm performance for diﬀerent (α, β) in Table 2.

In the table, we also implement a re-solving version of IGD(ˆγ) which periodically invokes a

re-solving procedure based on the remaining resource and the prior estimate to the dual updates
in IGD(ˆγ). Speciﬁcally, the re-solving heuristic re-solves the upper bound function ˆL(·) based on

the current time period t and the remaining budget ct to obtain an updated pt on a regular basis.

For each time t ∈ T ⊂ {1, ..., T }, we solve the following problem

t = argminp≥0c(cid:62)
ˆp∗

t p +

T
(cid:88)

j=t

ˆPjh(p, θ)

and use its optimal solution as the dual price for the t-th time period. Also, we update ˆγt’s

accordingly. Here the set T contains the time periods that we will re-solve the problem. For each

time t ∈ T , the preceding dual price pt−1 will be discarded and after time period t, we will continue

to implement the gradient-based update as in IGD(ˆγ) until the next re-solving time. The re-solving

method has gain great popularity in both theory and application (Gallego et al., 2019), and in

this experiment, we investigate how the technique can be used to further boost the performance

of IGD(ˆγ).

Table 2 reports the performance of IGD(ˆγ) and its re-solving heuristics with frequency k =

1, 50, 100, 200. First, we note that the original version of IGD(ˆγ) exhibits stably well performance

across diﬀerent combinations of α (the non-stationary intensity) and β (the estimation error). Sec-

ond, the re-solving heuristic further boosts the performance of IGD(ˆγ). We observe an improved

performance even when the frequency k = 200 (which means only re-solving for 5 times through-

out the horizon), but the performance improvement becomes marginal as we further increase the

re-solving frequency. From a computational perspective, the re-solving heuristic for IGD(ˆγ) pro-

vides a good trade-oﬀ between computational eﬃciency and algorithm performance, especially for

large-scale system. It naturally blends the computational advantage of IGD(ˆγ) and the algorith-

mic adaptivity of the re-solving technique. In recent years, the literature has developed a good

40

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

understanding of both the advantages and drawbacks of the re-solving technique in a stationary

environment (See (Cooper, 2002; Reiman and Wang, 2008; Jasin and Kumar, 2012; Jasin, 2015;

Bumpensanti and Wang, 2020) among others). In parallel, the experiment here raises an interest-

ing but challenging future direction of understanding the re-solving technique in a non-stationary

environment.

References

Adelman, Daniel. 2007. Dynamic bid prices in revenue management. Operations Research 55(4) 647–661.

Agrawal, Shipra, Nikhil R Devanur. 2014a. Bandits with concave rewards and convex knapsacks. Proceedings

of the ﬁfteenth ACM conference on Economics and computation. 989–1006.

Agrawal, Shipra, Nikhil R Devanur. 2014b. Fast algorithms for online stochastic convex programming.

Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms. SIAM, 1405–

1424.

Agrawal, Shipra, Zizhuo Wang, Yinyu Ye. 2014. A dynamic near-optimal algorithm for online linear pro-

gramming. Operations Research 62(4) 876–890.

Arjovsky, Martin, Soumith Chintala, L´eon Bottou. 2017. Wasserstein generative adversarial networks. Pro-

ceedings of the 34th International Conference on Machine Learning-Volume 70 . 214–223.

Arlotto, Alessandro, Itai Gurvich. 2019. Uniformly bounded regret in the multisecretary problem. Stochastic

Systems .

Arlotto, Alessandro, Xinchang Xie. 2020. Logarithmic regret in the dynamic and stochastic knapsack problem

with equal rewards. Stochastic Systems .

Asadpour, Arash, Xuan Wang, Jiawei Zhang. 2020. Online resource allocation with limited ﬂexibility. Man-

agement Science 66(2) 642–666.

Badanidiyuru, Ashwinkumar, Robert Kleinberg, Aleksandrs Slivkins. 2013. Bandits with knapsacks. 2013

IEEE 54th Annual Symposium on Foundations of Computer Science. IEEE, 207–216.

Balseiro, Santiago, Haihao Lu, Vahab Mirrokni. 2020. The best of many worlds: Dual mirror descent for

online allocation problems. arXiv preprint arXiv:2011.10124 .

Banerjee, Siddhartha, Daniel Freund. 2020a. Good prophets know when the end is near. Available at SSRN

3479189 .

Banerjee, Siddhartha, Daniel Freund. 2020b. Uniform loss algorithms for online stochastic decision-making

with applications to bin packing. SIGMETRICS .

Bemporad, Alberto, Manfred Morari. 1999. Robust model predictive control: A survey. Robustness in

identiﬁcation and control . Springer, 207–226.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

41

Berbeglia, Gerardo, Gwena¨el Joret. 2020. Assortment optimisation under a general discrete choice model: A

tight analysis of revenue-ordered assortments. Algorithmica 82(4) 681–720.

Besbes, Omar, Yonatan Gur, Assaf Zeevi. 2014. Stochastic multi-armed-bandit problem with non-stationary

rewards. Advances in neural information processing systems. 199–207.

Besbes, Omar, Yonatan Gur, Assaf Zeevi. 2015. Non-stationary stochastic optimization. Operations research

63(5) 1227–1244.

Besbes, Omar, Assaf Zeevi. 2012. Blind network revenue management. Operations research 60(6) 1537–1550.

Blanchet, Jose, Guillermo Gallego, Vineet Goyal. 2016. A markov chain approximation to choice modeling.

Operations Research 64(4) 886–905.

Blanchet, Jose, Yang Kang, Karthyek Murthy. 2019. Robust wasserstein proﬁle inference and applications

to machine learning. Journal of Applied Probability 56(3) 830–857.

Buchbinder, Niv, Joseph Naor. 2009. Online primal-dual algorithms for covering and packing. Mathematics

of Operations Research 34(2) 270–286.

Bumpensanti, Pornpawee, He Wang. 2020. A re-solving heuristic with uniformly bounded loss for network

revenue management. Management Science .

Cheung, Wang Chi, Guodong Lyu, Chung-Piaw Teo, Hai Wang. 2020. Online planning with oﬄine simulation.

Available at SSRN 3709882 .

Cheung, Wang Chi, David Simchi-Levi, Ruihao Zhu. 2019. Non-stationary reinforcement learning: The

blessing of (more) optimism. Available at SSRN 3397818 .

Cooper, William L. 2002. Asymptotic behavior of an allocation policy for revenue management. Operations

Research 50(4) 720–727.

Davis, James, Guillermo Gallego, Huseyin Topaloglu. 2013. Assortment planning under the multinomial

logit model with totally unimodular constraint structures. Work in Progress .

Devanur, Nikhil R, Kamal Jain, Balasubramanian Sivan, Christopher A Wilkens. 2019. Near optimal online

algorithms and fast approximation algorithms for resource allocation problems. Journal of the ACM

(JACM) 66(1) 7.

Esfahani, Peyman Mohajerin, Daniel Kuhn. 2018. Data-driven distributionally robust optimization using the

wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming

171(1-2) 115–166.

Feldman, Jacob B, Huseyin Topaloglu. 2017. Revenue management under the markov chain choice model.

Operations Research 65(5) 1322–1342.

Ferguson, Thomas S, et al. 1989. Who solved the secretary problem? Statistical science 4(3) 282–289.

Fournier, Nicolas, Arnaud Guillin. 2015. On the rate of convergence in wasserstein distance of the empirical

measure. Probability Theory and Related Fields 162(3) 707–738.

42

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Galichon, Alfred. 2018. Optimal transport methods in economics. Princeton University Press.

Gallego, Guillermo, Huseyin Topaloglu. 2014. Constrained assortment optimization for the nested logit

model. Management Science 60(10) 2583–2601.

Gallego, Guillermo, Huseyin Topaloglu, et al. 2019. Revenue management and pricing analytics, vol. 209.

Springer.

Gallego, Guillermo, Garrett Van Ryzin. 1994. Optimal dynamic pricing of inventories with stochastic demand

over ﬁnite horizons. Management science 40(8) 999–1020.

Garivier, Aur´elien, Eric Moulines. 2008. On upper-conﬁdence bound policies for non-stationary bandit

problems. arXiv preprint arXiv:0805.3415 .

Gibbs, Alison L, Francis Edward Su. 2002. On choosing and bounding probability metrics. International

statistical review 70(3) 419–435.

Golrezaei, Negin, Hamid Nazerzadeh, Paat Rusmevichientong. 2014. Real-time optimization of personalized

assortments. Management Science 60(6) 1532–1551.

Gupta, Anupam, Marco Molinaro. 2014. How experts can solve lps online. European Symposium on Algo-

rithms. Springer, 517–529.

Hall, Eric, Rebecca Willett. 2013. Dynamical models and tracking regret in online convex programming.

International Conference on Machine Learning. PMLR, 579–587.

Hazan, Elad. 2016. Introduction to online convex optimization. Foundations and Trends in Optimization

2(3-4) 157–325.

Immorlica, Nicole, Karthik Abinav Sankararaman, Robert Schapire, Aleksandrs Slivkins. 2019. Adversarial

bandits with knapsacks. 2019 IEEE 60th Annual Symposium on Foundations of Computer Science

(FOCS). IEEE, 202–219.

Jadbabaie, Ali, Alexander Rakhlin, Shahin Shahrampour, Karthik Sridharan. 2015. Online optimization:

Competing with dynamic comparators. Artiﬁcial Intelligence and Statistics. PMLR, 398–406.

Jagabathula, Srikanth. 2014. Assortment optimization under general choice. Available at SSRN 2512831 .

Jasin, Stefanus. 2015. Performance of an lp-based control for revenue management with unknown demand

parameters. Operations Research 63(4) 909–915.

Jasin, Stefanus, Sunil Kumar. 2012. A re-solving heuristic with bounded revenue loss for network revenue

management with customer choice. Mathematics of Operations Research 37(2) 313–345.

Jasin, Stefanus, Sunil Kumar. 2013. Analysis of deterministic lp-based booking limit and bid price controls

for revenue management. Operations Research 61(6) 1312–1320.

Jenatton, Rodolphe, Jim Huang, C´edric Archambeau. 2016. Adaptive algorithms for online convex opti-

mization with long-term constraints. International Conference on Machine Learning. PMLR, 402–411.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

43

Jiang, Jiashuo, Shixin Wang, Jiawei Zhang. 2019. Achieving high individual service-levels without safety

stock? optimal rationing policy of pooled resources. Optimal Rationing Policy of Pooled Resources

(May 2, 2019). .

Jiang, Jiashuo, Jiawei Zhang. 2020. Online resource allocation with stochastic resource consumption. arXiv

preprint arXiv:2012.07933 .

Kunnumkal, Sumit, Kalyan Talluri. 2016. On a piecewise-linear approximation for network revenue manage-

ment. Mathematics of Operations Research 41(1) 72–91.

Lattimore, Tor, Csaba Szepesv´ari. 2020. Bandit algorithms. Cambridge University Press.

Lecarpentier, Erwan, Emmanuel Rachelson. 2019. Non-stationary markov decision processes, a worst-case

approach using model-based reinforcement learning. Advances in Neural Information Processing Sys-

tems. 7216–7225.

Li, Guang, Paat Rusmevichientong, Huseyin Topaloglu. 2015. The d-level nested logit model: Assortment

and price optimization problems. Operations Research 63(2) 325–342.

Li, Xiaocheng, Chunlin Sun, Yinyu Ye. 2020. Simple and fast algorithm for binary integer and online linear

programming. arXiv preprint arXiv:2003.02513 .

Lu, Haihao, Santiago Balseiro, Vahab Mirrokni. 2020. Dual mirror descent for online allocation problems.

arXiv preprint arXiv:2002.10421 .

Ma, Yuhang, Paat Rusmevichientong, Mika Sumida, Huseyin Topaloglu. 2020. An approximation algorithm

for network revenue management under nonstationary arrivals. Operations Research 68(3) 834–855.

Mangasarian, Olvi L, T-H Shiau. 1987. Lipschitz continuity of solutions of linear inequalities, programs and

complementarity problems. SIAM Journal on Control and Optimization 25(3) 583–595.

Mehta, Aranyak, Amin Saberi, Umesh Vazirani, Vijay Vazirani. 2005. Adwords and generalized on-line

matching. 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS’05). IEEE,

264–273.

Miao, Sentao, Yining Wang, Jiawei Zhang. 2021. A general framework for resource constrained revenue

management with demand learning and large action space. Available at SSRN 3841273 .

Molinaro, Marco, Ramamoorthi Ravi. 2013. The geometry of online packing linear programs. Mathematics

of Operations Research 39(1) 46–59.

Neely, Michael J, Hao Yu. 2017. Online convex optimization with time-varying constraints. arXiv preprint

arXiv:1702.04783 .

Rangi, Anshuka, Massimo Franceschetti, Long Tran-Thanh. 2018. Unifying the stochastic and the adversarial

bandits with knapsack. arXiv preprint arXiv:1811.12253 .

Rawlings, James B. 2000. Tutorial overview of model predictive control. IEEE control systems magazine

20(3) 38–52.

44

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Reiman, Martin I, Qiong Wang. 2008. An asymptotically optimal policy for a quantity-based network revenue

management problem. Mathematics of Operations Research 33(2) 257–282.

Rusmevichientong, Paat, Zuo-Jun Max Shen, David B Shmoys. 2010. Dynamic assortment optimization

with a multinomial logit choice model and capacity constraint. Operations research 58(6) 1666–1680.

Russac, Yoan, Claire Vernade, Olivier Capp´e. 2019. Weighted linear bandits for non-stationary environments.

Advances in Neural Information Processing Systems. 12040–12049.

Sun, Rui, Xinshang Wang, Zijie Zhou. 2020. Near-optimal primal-dual algorithms for quantity-based network

revenue management. arXiv preprint arXiv:2011.06327 .

Talluri, Kalyan, Garrett Van Ryzin. 1998. An analysis of bid-price controls for network revenue management.

Management science 44(11-part-1) 1577–1593.

Talluri, Kalyan, Garrett Van Ryzin. 2004. Revenue management under a general discrete choice model of

consumer behavior. Management Science 50(1) 15–33.

Talluri, Kalyan T, Garrett J Van Ryzin. 2006. The theory and practice of revenue management, vol. 68.

Springer Science & Business Media.

Vanderbei, Robert J, et al. 2015. Linear programming. Springer.

Vera, Alberto, Siddhartha Banerjee. 2020. The bayesian prophet: A low-regret framework for online decision

making. Management Science .

Vera, Alberto, Siddhartha Banerjee, Itai Gurvich. 2019. Online allocation and pricing: Constant regret via

bellman inequalities. arXiv preprint arXiv:1906.06361 .

Villani, C´edric. 2008. Optimal transport: old and new , vol. 338. Springer Science & Business Media.

Wagener, Nolan, Ching-An Cheng, Jacob Sacks, Byron Boots. 2019. An online learning approach to model

predictive control. arXiv preprint arXiv:1902.08967 .

Yi, Xinlei, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, Karl Johansson. 2021. Regret and cumulative

constraint violation analysis for online convex optimization with long term constraints. International

Conference on Machine Learning. PMLR, 11998–12008.

Yuan, Jianjun, Andrew Lamperski. 2018. Online convex optimization for cumulative constraints. Advances

in Neural Information Processing Systems. 6137–6146.

Zhang, Dan, Daniel Adelman. 2009. An approximate dynamic programming approach to network revenue

management with customer choice. Transportation Science 43(3) 381–394.

Appendix. Proofs of Lemmas, Propositions and Theorems

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

45

A. Proofs of Section 3

A1. Proof of Lemma 1

Proof: Given the realized parameters H = (θ1, θ2, . . . , θT ), we can denote the oﬄine optimum

of (PCP) as a function of H, namely, {x∗

t (H)}T

t=1. Let

˜xt(θ) = E [x∗

t (H)|θt = θ]

where the conditional expectation is taken with respect to Pj for j (cid:54)= t. We show that ˜xt(θ) is a

feasible solution to (4). Speciﬁcally, note that for each i = 1, ..., m,

(cid:34) T

(cid:88)

ci ≥ E

(cid:35)

gi(x∗

t (H); θt)

=

T
(cid:88)

t=1

Eθt∼Pt [E[gi(x∗

t (H); θt)|θt = θ]]

t=1
(cid:90)

T
(cid:88)

≥

t=1

θ∈Θ

gi( ˜xt(θ); θ)dPt(θ) =

T
(cid:88)

t=1

Ptgi(xt; θ)

where the ﬁrst inequality comes from the feasibility of the optimal solution x∗

t (H) and the second
inequality follows from that the function gi(·; θt) is a convex function for each i and θt ∈ Θ. Thus,

{ ˜xt(θ)} is a feasible solution to (4). Similarly, we can analyze the objective function

(cid:34) T

(cid:88)

E[R∗

T ] = E

(cid:35)

f (x∗

t (H); θt)

=

T
(cid:88)

t=1

Eθt∼Pt [E[f (x∗

t (H); θt)|θt = θ]]

t=1
(cid:90)

T
(cid:88)

≤

t=1

θ∈Θ

f ( ˜xt(θ); θ)dPt(θ) ≤ RUB

T

where the ﬁrst inequality follows from that the function f (·; θ) is a concave function for any θ ∈ Θ

and the last inequality comes from the optimality of RUB

T . Thus we complete the proof. (cid:3)

A2. Proof of Proposition 1

Proof: We ﬁrst prove that p∗ is an optimal solution for Lt. Note that for each t, Lt(p) is a

convex function over p and

∇Lt(p∗) = γt + Pt∇h(p∗; θt) = γt − Ptg(x∗(θt); θt)

where x∗(θ) = argmaxx∈X {f (x; θ) − (p∗)T · g(x; θ)}. With the deﬁnition of γt in (6), it follows
immediately that

∇Lt(p∗) = 0

which implies that p∗ is a minimizer of the function L(·) for each t. We then prove that L(p∗) =
(cid:80)T

t=1 Lt(p∗). Deﬁne the set of binding constraints IB = {i : p∗

i > 0, i = 1, ..., m}. From the convexity

of the function L(p) over p, for each i ∈ IB, it holds that

0 = ∇iL(p∗) = ci −

T
(cid:88)

t=1

Ptg(x∗(θt); θt) = ci −

T
(cid:88)

t=1

γt,i

46

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Thus, we have that

c(cid:62) · p∗ =

T
(cid:88)

t=1

γ (cid:62)
t

· p∗

It follows immediately that L(p∗) = (cid:80)T

t=1 Lt(p∗). (cid:3)

A3. Discussions on the function h

The deﬁnition of the function h(p; θ), described as follows for completeness, plays a critical role in

deriving our results,

(cid:40)

h(p; θ) = max
x∈X

f (x; θ) −

(cid:41)

pi · gi(x; θ)

.

n
(cid:88)

i=1

(22)

Throughout the paper, we assume that the optimization problem in (22) can be solved eﬃciently so

as to obtain both its optimal solution and optimal objective value. Here, we justify this assumption

with a discussion of the computational aspect of solving (22). Note that when the function h is

deﬁned in the setting with random reward and consumption given in Section 6.4 where f, g are
random functions, f, g can be simply replaced by their expectations ˆf , ˆg in the deﬁnition of h in
(22). In the following discussion, we use the notations of f, g and ˆf , ˆg exchangeably for simplicity.

When (PCP) refers to the online linear programming problem or price-based network revenue

management (NRM) problem as described in Section 2.2, the optimization problem in (22) is

reduced to a simple convex optimization problem and it can indeed be solved in polynomial time by

existing methods. We now consider (22) when (PCP) represents the choice-based NRM problem.

In the choice-based NRM problem, there are n products and each product i is associated with

a revenue ri. The deﬁnitions of xt and the functions f, g are described in Section 2.2. Given the

assortment s, the customer with the type θt chooses one product i to purchase with a probability

ηi(s; θt), where η is speciﬁed by the choice model of the customer and is assumed to be known to

the decision maker. Here, the function f refers to the expected revenue of the assortment xt and

the function gi refers to the probability that a product i is purchased:

f (xt; θt) =

(cid:88)

n
(cid:88)

s∈S

j=1

xt,s · rj · ηj(s; θt) and gi(xt; θt) =

(cid:88)

s∈S

xt,s · ηi(s; θt) ∀i

Then, the optimization problem in (22) could be rewritten in the following equivalent formulation:

max
s∈S⊂{0,1}n

n
(cid:88)

i=1

(ri − pi) · ηi(s; θt)

(23)

With the following choice models to specify the function ηi’s, the optimization problem in (23)

could all be solved eﬃciently.

Multinomial logit model (NML): Talluri and Van Ryzin (2004) show that without further

constraints, it is optimal to sort the products according to a decreasing order ri − pi and ﬁnd

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

47

an optimal revenue-order assortment among {1}, {1, 2, }, . . . , {1, 2 . . . , n}. Rusmevichientong et al.

(2010) further propose a simple polynomial-time algorithm to compute the optimal assortment

where there is a capacity constraint. Davis et al. (2013) consider the assortment problem with

totally unimodular constraints and show that the constrained problem could be solved as an equiv-

alent linear program.

Nested logit model: Li et al. (2015) consider the assortment optimization problem under a

d-level nested logit model and propose an algorithm to compute the optimal assortment in polyno-

mial time. Gallego and Topaloglu (2014) consider the setting with cardinality constraint and show

that the optimal assortment can be obtained eﬃciently by solving a linear program. Gallego and

Topaloglu (2014) further consider the setting with a capacity constraint and propose an algorithm

with constant competitiveness and good empirical performance.

Markov chain based models: Blanchet et al. (2016) propose the Markov chain based model

and develop polynomial-time solution algorithms. Feldman and Topaloglu (2017) propose a linear

programming approach to obtain the optimal assortment under this model.

General choice model: There are also papers considering the assortment problem without

exploiting the speciﬁc structures of the choice model. Jagabathula (2014) studies a local search

heuristic and shows its great empirical performance. Also, Berbeglia and Joret (2020) analyze the

performance of the revenue-order assortment heuristic under the general discrete choice model,

which can be applied to solve (23) approximately.

A4. Proof of Lemma 2

Proof: Note that the following two properties are satisﬁed by the update rule (7):

(i). If (cid:107)pt(cid:107)∞ ≤ q, then we must have (cid:107)pt+1(cid:107)∞ ≤ q + 1 by noting that for each i, the i-th component

of pt, denoted as pt,i, is nonnegative and gi(·, θt) is normalized within [0, 1].

(ii). If there exists i such that pt,i > q, then we must have pt+1,i < pt,i. Speciﬁcally, when pt,i > q,

we must have that gi( ˜xt; θt) = 0, otherwise we would have that

f ( ˜xt; θt) − p(cid:62)

t · g( ˜xt; θt) ≤ f ( ˜xt; θt) − pt,i · gi( ˜xt; θt) < 0

which contradicts the deﬁnition of ˜xt in Algorithm 1 since we could always select xt = 0 to

obtain a zero objective value as per Assumption 1. Then from (7), it holds that pt+1,i < pt,i.

Starting from p1 = 0 and iteratively applying the above two property to control the increase of pt

from t = 1 to T , we obtain that for the ﬁrst time that one component of pt exceeds the threshold

q, it is upper bounded by q + 1 and this component will continue to decrease until it falls below
the threshold q. Thus, we have (cid:107)pt(cid:107)∞ ≤ q + 1 with probability 1 for each t. (cid:3)

48

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

A5. Proof of Theorem 1

Proof:

In IGD(γ), the true action xt taken by the decision maker diﬀers from the virtual action

˜xt if and only if ct cannot fully satisfy g( ˜xt; θt). Thus, we have that

f ( ˜xt; θt) − f (xt; θt) ≤ f ( ˜xt; θt) · I {∃i : ct,i < gi( ˜xt; θt)}

where ct,i denotes the i-th component of ct and I {·} denotes the indicator function. Moreover, we
know

I {∃i : ct,i < gi( ˜xt; θt)} ≤

gi( ˜xj; θj) > ci

.

m
(cid:88)

I

(cid:40) t

(cid:88)

(cid:41)

Recall that the maximum reward generated by consuming per unit of budget of each constraint is

i=1

j=1

upper bounded by q. We have

f ( ˜xt; θt) · I {∃i : ct,i < gi( ˜xt; θt)} ≤ q

m
(cid:88)

i=1

gi( ˜xt; θ) · I

(cid:40) t

(cid:88)

j=1

gi( ˜xj; θj) > ci

From the fact that g(·; θt) ∈ [0, 1]m,

(cid:41)

(cid:41)

T
(cid:88)

t=1

f ( ˜xt; θt) −

T
(cid:88)

t=1

f (xt; θt) ≤ q ·

m
(cid:88)

T
(cid:88)

gi( ˜xt; θt) · I

(cid:40) t

(cid:88)

j=1

gi( ˜xj; θj) > ci

(cid:35)+

i=1

m
(cid:88)

t=1
(cid:34) T

(cid:88)

i=1

t=1

≤ q ·

gi( ˜xt; θt) − (ci − 1)

which related the total collected reward by the true action {xt}T

t=1 and the virtual action { ˜xt}T

t=1.

Further from Proposition 1, we have that

RegT (π) ≤ min
p≥0

T
(cid:88)

t=1

Lt(p) − E

(cid:35)

f (xt; θt)

≤

(cid:34) T

(cid:88)

t=1

T
(cid:88)

t=1
(cid:124)

Lt(p) − E

min
p≥0

(cid:34) T

(cid:88)

t=1

f ( ˜xt; θt)

(cid:35)

(cid:125)

(cid:123)(cid:122)
I





m
(cid:88)

(cid:34) T

(cid:88)

+ q · E

(cid:124)

gi( ˜xt; θt) − (ci − 1)

i=1

t=1

(cid:123)(cid:122)
II

(cid:125)

(cid:35)+


We then bound the term I and term II separately to derive our regret bound.

Bound I: Note that for each t, the distribution of pt is independent from the distribution of θτ

for any τ ≤ t, then we have that

Lt(p) ≤ Ept [Lt(pt)] = Ept

(cid:2)γ (cid:62)

t pt + Pth(pt; θt)(cid:3)

min
p≥0

where the expectation is taken with respect to the randomness of the dual price pt. Thus, we have

I ≤

T
(cid:88)

t=1

Ept

(cid:2)γ (cid:62)

t pt + Pt {h(pt; θt) − f ( ˜xt; θt)}(cid:3)

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

49

From the deﬁnition of ˜xt, we get that h(pt; θt) − f ( ˜xt; θt) = −p(cid:62)

t · g( ˜xt; θt), which implies that

I ≤

T
(cid:88)

t=1

Ept

(cid:2)p(cid:62)

t · (γt − Ptg( ˜xt; θt))(cid:3)

Note that from the update rule (7), we have that

(cid:107)pt+1(cid:107)2

2 ≤ (cid:107)pt(cid:107)2

2 +

1
T

· (cid:107)g( ˜xt; θt) − γt(cid:107)2

2 −

2
√
T

· p(cid:62)

t · (γt − g( ˜xt; θt))

which implies that

Ept

(cid:2)p(cid:62)

t · (γt − Ptg( ˜xt; θt))(cid:3) ≤

√

T
2

· (cid:0)E[(cid:107)pt(cid:107)2

2] − E[(cid:107)pt+1(cid:107)2

2](cid:1) +

Thus, it holds that

√

T

m

2

I ≤

Bound II: Note that from the update rule (7), we have that

√

T · pt+1 ≥

√

T · pt + g( ˜xt; θt) − γt

which implies that

Thus, it holds that

T
(cid:88)

t=1

g( ˜xt; θt) − c ≤

T
(cid:88)

t=1

g( ˜xt; θt) −

√

γt ≤

T · pT +1

T
(cid:88)

t=1

m
√
T
2

(24)

II = q · E





m
(cid:88)

(cid:34) T

(cid:88)

(cid:35)+

gi( ˜xt; θt) − (ci − 1)

 ≤ mq(q + 1) ·

√

T + qm

(25)

We obtain the O(

√

T ) regret bound immediately by combining (24) and (25). (cid:3)

i=1

t=1

A6. Proof of Proposition 2

Proof: We construct the following distribution P over the parameter θ. Suppose that there is a

single resource. Denote the support set of P as {θ(0), θ(1), θ(2), . . . , θ(j), . . . } and for each j = 1, 2, . . . ,

we have that

P(θ = θ(j)) =

1
2

·

1
2j

and P(θ = θ(0)) =

1
2

Moreover, we have the following condition over the support set {θ(1), θ(2), . . . , θ(j), . . . } of P. Denote

sequences of constants {aj}, {bj}, where

aj = 1 +

2j + 1
2j(j + 1)

, bj = 1 + 1/(j + 1)

for j = 1, 2, .... Clearly, we have 1 ≤ · · · ≤ aj+1 < bj < aj · · · ≤ 2. The reward function satisﬁes

0 ≤ min
0<x≤1

f (x; θ(0))
x

≤ max
0<x≤1

f (x; θ(0))
x

≤ 1 and bj ≤ min
0<x≤1

f (x; θ(j))
x

≤ max
0<x≤1

f (x; θ(j))
x

≤ aj, ∀j = 1, 2, . . .

50

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

There is a single resource and the resource consumption function satisﬁes g(x; θ) = x for each θ.

We are now ready to construct the example and show the lower bound.

Suppose there is a single resource with an initial capacity T

2 , where T is the total time periods.

For a ﬁxed T , there must exists an integer k such that

Now, we assume without loss of generality that T is set such that

1
√
2

T

≤

1
2k+2

≤

1
√
T

1
2k+2

=

1
√
T

We now divide the support set {θ(0), θ(1), θ(2), . . . , θ(j), . . . } into three subsets:

A1 = {θ(j) : 1 ≤ j < k}, A2 = {θ(k)} and A3 = {θ(0), θ(j) : j ≥ k + 1}.

Clearly, at each time period t, we have that

P (θt ∈ A1) =

1
2

+

1
2k+1

, P (θt ∈ A2) =

1
2k+1

and P (θt ∈ A3) =

1
2

−

2
2k+1

.

Denote ε = 1

where q = 1, 2, 3. We now denote Z t

2k+2 . We call that we encounter a type q request whenever the event {θt ∈ Aq} happens,
q as the number type q requests in the ﬁrst t periods. We next

introduce the event:
(cid:26) 1
(cid:15)

H1 =

≤ Z T

2 ≤ min{2Z T /2
(cid:27)

(cid:26)

2

(cid:26)

Z T

1 ≥

(cid:26)

Z T

1 ≤

H2 =

H3 =

T
2
T
2

+

−

2
(cid:15)
4
(cid:15)

(cid:27)

6
(cid:15)

=

1 ≥ E[Z T
Z T

1 ] +

(cid:27)

= (cid:8)Z T

1 ≤ E[Z T

1 ](cid:9)

(cid:27)

,

2
(cid:15)

}

=

(cid:26) 1
2

E[Z T

2 ] ≤ Z T

2 ≤ min{2Z T /2

2

(cid:27)

, E[Z T

2 ]}

It is obvious that on event H2, particularly on the event H2 ∩ H1, all the resources will be consumed

by type 1 requests by the oﬄine optimum. That is, at each period t, the oﬄine optimum will set the

decision variable xt non-zero only when the event {θt ∈ A1} happens. Similarly, on event H1 ∩ H3,

the oﬄine optimum will set the decision variable xt = 1 whenever the event {θt ∈ A1} or {θt ∈ A2}

happens. Following the same argument as Arlloto and Gurvich (2019), we know that there exists

a constant α1 > 0, independent of (cid:15), such that

P (H1 ∩ H2) ≥ α1 and P (H1 ∩ H3) ≥ α1

Next, we consider the diﬀerence between the oﬄine optimum and the dynamic programming policy.
We denote ST /2

as the amount of resource consumed by type 2 request during the ﬁrst T /2 periods,

2

and we consider the event

H4 =

(cid:26)

ST /2

2 ≥

(cid:27)

Z T
2
4

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

51

Now, on the event Hc

request. On the other hand, the optimal online policy consumes at most Z T

4 ∩ H1 ∩ H3, the oﬄine optimum will set xt = 1 for all type 1 and type 2
2 /4 amount of resource
using type 2 requests during the ﬁrst T /2 periods. Note that as induced by the event H1, at most
Z T

2 /2 number of type 2 request can arrive during the last T /2 periods. Thus, for the optimal online
2 amount of resource, while the oﬄine optimum
2 amount of resource with type 2 request. Further note that the reward/size for type
2 request is at least bk and the reward/size for type 3 request is at most ak+1. This will incur a

policy, type 2 request can consume at most 3

consume Z T

4 · Z T

regret at least

E[Regret] ≥

bk − ak+1
4

· E[Z T

2 · 1{Hc

4 ∩ H1 ∩ H3}] ≥

bk − ak+1
4(cid:15)

· P (Hc

4 ∩ H1 ∩ H3)

For each arrival sample path that falls in the set H4 ∩ H1 ∩ H3, we can ﬁnd another sample path in

H4 ∩ H1 ∩ H2 by keeping the ﬁrst T /2 arrivals unchanged, and for the last T /2 periods, replacing

at most 6/(cid:15) type 3 requests with type 1 requests. We denote this resulting set of sample path as

L. Note that for each period, the arrival probability for type 1 request and type 3 request are both

bounded away from 0. We know that there exists a constant α2 > 0, independent of (cid:15), such that

P (L) ≥ α2 · P (H4 ∩ H1 ∩ H3).

Moreover, for each sample path in the set L, since it is on the event H4 ∩ H1 ∩ H2, we know that

the oﬄine optimum will consume all the resource with type 1 request, while the optimal online
policy consumes at least ZT
4 amount of resource with type 2 requests. Note that the reward/size
for type 1 is at least bk−1, while the reward/size for type 2 is at most ak. This will incur a regret

2

at least

E[regret] ≥

bk−1 − ak
4

· E[Z T

2 · 1{L}] ≥

bk−1 − ak
4(cid:15)

· P (L) ≥ α2 ·

bk−1 − ak
4(cid:15)

· P (H4 ∩ H1 ∩ H3).

Since

P (H1 ∩ H3) = P (Hc

4 ∩ H1 ∩ H3) + P (H4 ∩ H1 ∩ H3) ≥ α1,

we know that there exists a constant α3 > 0, independent of (cid:15), such that

Recall that (cid:15) = 1/

√

T and

E[regret] ≥

α3
(cid:15)

· min{bk−1 − ak, bk − ak+1}.

and that

1

2k+2 = 1√

T

min{bk−1 − ak, bk − ak+1} =

1
(k + 1)(k + 2)

, i.e., k + 2 = log T

2 . We have

E[regret] ≥

√

T
(log T )2

α3
4

·

which completes our proof.

52

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

B. Proofs of Section 4

B1. Proof of Theorem 2

Proof:

It follows directly from Proposition 2 that for any policy π, we have RegT (π) ≥ Ω(

√

T ).

Thus, it is enough to consider the Ω(WT ) part in the lower bound. We consider the following

estimated problem, where the true coeﬃcients in (PCP) are replaced by the estimates:

max x1 + ... + xc + xc+1 + ... + xT

(26)

s.t. x1 + ... + xc + xc+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

where c = T

2 and the prior estimate ˆPt is simply a one-point distribution for each t. Now we consider
the following two possible true problems, the distributions of which are all one-point distributions

and belong to the set ΞP with variation budget WT :

max x1 + ... + xc +

1 +

(cid:18)

(cid:19)

WT
T

(cid:18)

xc+1 + ... +

1 +

(cid:19)

xT

WT
T

s.t. x1 + ... + xc + xc+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.
(cid:19)

(cid:18)

WT
T

max x1 + ... + xc +

1 −

xc+1 + ... +

1 −

(cid:18)

(cid:19)

xT

WT
T

(27)

(28)

s.t. x1 + ... + xc + xc+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

where c = T

2 . Denote x1

t (π) as the decision of any policy π at period t for scenario (27) and denote
x2
t (π) as the decision of policy π at period t for scenario (28). Further deﬁne T1(π) (resp. T2(π))
as the expected capacity consumption of policy π on scenario (27) (resp. scenario (28)) during the

ﬁrst T

2 time periods:

T1(π) = E

Then, we have that

T

2(cid:88)






 and T2(π) = E

x1
t (π)

T

2(cid:88)







x2
t (π)



t=1

t=1

R1

T (π) =

T + WT
2

−

WT
T

· T1(π)

and R2

T (π) =

T − WT
2

+

WT
T

· T2(π)

T (π) (resp. R2

where R1
(resp. scenario (28)). Thus, the regret of policy π on scenario (27) and (28) are WT
WT − WT

T (π)) denotes the expected reward collected by policy π on scenario (27)
T · T1(π) and
T · T2(π) respectively. Further note that since the implementation of policy π at each time

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

53

period should be only dependent on the historical information and the coeﬃcients in the estimated

problem (26), we must have T1(π) = T2(π). Thus, we have that

RegT (π) ≥ max

(cid:26) WT
T

· T1(π), WT −

WT
T

(cid:27)

· T1(π)

≥

WT
2

= Ω(WT )

which completes our proof. (cid:3)

B2. Proof of Lemma 3

Proof: Due to symmetry, it is suﬃcient to show that for every p such that p ∈ Ω ¯p,

LQ2(p) − LQ1(p) ≤ max{1, ¯p} · W (Q1, Q2).

Denote Q∗

1,2 as the optimal coupling of the distribution Q1 and Q2, i.e., the optimal solution to

(8), and denote

x∗(θ) = argmaxx∈X

f (x; θ) −

(cid:40)

(cid:41)

pi · gi(x; θ)

m
(cid:88)

i=1

Then for each θ1 ∈ Θ, we deﬁne

(cid:90)

ˆx(θ1) =

x∗(θ2)

dQ∗

1,2(θ1, θ2)
dQ1(θ1)

θ2∈Θ

dQ∗

1,2(θ1,θ2)
dQ1(θ1)

where

is the Radon–Nikodym derivative of Q∗

as the conditional distribution of θ2 given θ1. Note that from the deﬁnition of Q∗
(cid:82)

1,2 with respect Q1 and it can be interpreted
1,2, we have that
1,2(θ1,θ2)
dQ1(θ1) = 1. Thus, ˆx(θ1) is actually a convex combination of {x∗(θ2)}∀θ2∈Θ. Moreover,

θ2∈Θ
from the concavity of f (·; θ1) and the convexity of gi(·; θ1) for each i, we have that

dQ∗

and

Thus, we have that

(cid:90)

(cid:90)

LQ1(p) =

≥

f (ˆx(θ1); θ1) ≥

gi(ˆx(θ1); θ1) ≤

(cid:90)

θ2∈Θ

(cid:90)

θ2∈Θ

f (x∗(θ2); θ1) ·

dQ∗

1,2(θ1, θ2)
dQ1(θ1)

gi(x∗(θ2); θ1) ·

dQ∗

1,2(θ1, θ2)
dQ1(θ1)

(cid:40)

f (x; θ1) −

m
(cid:88)

(cid:41)

pi · gi(x; θ1)

dQ1(θ1)

θ1∈Θ

max
x∈X
(cid:40)

f (ˆx(θ1); θ1) −

(cid:41)

pi · gi(ˆx(θ1); θ1)

dQ1(θ1)

i=1
m
(cid:88)

i=1

θ1∈Θ

(cid:90)

≥

(cid:90)

θ1∈Θ

θ2∈Θ

(cid:40)

f (x∗(θ2); θ1) −

(cid:41)

pi · gi(x∗(θ2); θ1)

dQ∗

1,2(θ1, θ2)

m
(cid:88)

i=1

Also, note that for any θ1, θ2 ∈ Θ, it holds that

f (x∗(θ2); θ1)−

m
(cid:88)

i=1

pi ·gi(x∗(θ2); θ1) ≥ f (x∗(θ2); θ2)−

m
(cid:88)

i=1

pi ·gi(x∗(θ2); θ2)−max{1, ¯p}·(m+1)ρ(θ1, θ2)

54

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

which follows the deﬁnition of ρ(θ1, θ2) in (2). Thus, we get that

(cid:90)

(cid:90)

LQ1(p) ≥

θ1∈Θ

θ2∈Θ

(cid:40)

f (x∗(θ2); θ2) −

− max{1, ¯p} · (m + 1)

(cid:90)

(cid:90)

(cid:41)

pi · gi(x∗(θ2); θ2)

dQ∗

1,2(θ1, θ2)

m
(cid:88)

i=1

ρ(θ1, θ2)dQ∗

1,2(θ1, θ2)

(cid:40)

(cid:90)

=

θ2∈Θ

f (x∗(θ2); θ2) −

i=1

θ1∈Θ
m
(cid:88)

θ2∈Θ

(cid:41)

pi · gi(x∗(θ2); θ2)

dQ2(θ2) − max{1, ¯p} · (m + 1)W(Q1, Q2)

= LQ2(p) − max{1, ¯p} · (m + 1)W(Q1, Q2)

where the ﬁrst equality holds by noting that (cid:82)

θ1∈Θ dQ∗
As a remark, we note that the proof of Lemma 3 will still go through even when the concavity

1,2(θ1, θ2) = dQ2(θ2). (cid:3)

and convexity of f (·; θ) and gi(·; θ) do not hold. To see this, we use F to denote a distribution over

the action set X and accordingly,

(cid:90)

ˆf (F ; θ) =

x∈X

ˆf (x; θ)dF (x) and ˆgi(F ; θ) =

(cid:90)

x∈X

gi(x; θ)dF (x).

Then, we denote

and

ˆh(p; θ) := max

F

(cid:110) ˆf (F ; θ) − p(cid:62) ˆg(F ; θ)

(cid:111)

, ˆLQ(p) = Qˆh(p; θ)

ˆρ(θ, θ(cid:48)) = sup

(cid:107)( ˆf (F ; θ), ˆg(F ; θ))−( ˆf (F ; θ(cid:48)), ˆg(F ; θ(cid:48)))(cid:107)∞, ˆW(Q1, Q2) :=

F

(cid:90)

inf
Q1,2∈J (Q1,Q2)

ˆρ(θ1, θ2)dQ1,2(θ1, θ2).

Now that ˆf (F ; θ) and ˆg(F ; θ) can be regarded as linear functions of (dF (x), ∀x ∈ X ), which fully

characterizes the distribution F , we can apply the same procedure as the proof of Lemma 3 to

show that

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆLQ1(p) − ˆLQ2(p)
(cid:12) ≤ max{1, ¯p} · (m + 1) ˆW(Q1, Q2).
(cid:12)

sup
p∈Ω ¯p

On the other hand, note that

ˆh(p; θ) := max

F

(cid:110) ˆf (F ; θ) − p(cid:62) ˆg(F ; θ)

(cid:111)

= max
x∈X

(cid:8)f (x; θ) − p(cid:62)g(x; θ)(cid:9) = h(p; θ)

and ˆρ(θ, θ(cid:48)) = ρ(θ, θ(cid:48)). We know that

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆLQ1(p) − ˆLQ2(p)
(cid:12) = |LQ1(p) − LQ2(p)| and ˆW(Q1, Q2) = W(Q1, Q2).
(cid:12)

Therefore, (12) can be proved to hold for general f (·; θ) and gi(·; θ) without any convexity or

concavity structure.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

55

B3. Proof of Lemma 4

Proof: Note that the following two property is satisﬁed by the update rule (11):

(i). If (cid:107)pt(cid:107)∞ ≤ q, then we must have (cid:107)pt+1(cid:107)∞ ≤ q + 1 by noting that for each i, the i-th component

of pt, denoted as pt,i, is nonnegative and gi(·, θt) is normalized within [0, 1].

(ii). If there exists i such that pt,i > q, then we must have pt+1,i < pt,i. Speciﬁcally, when pt,i > q,

we must have that gi( ˜xt; θt) = 0, otherwise we would have that

f ( ˜xt; θt) − p(cid:62)

t · g( ˜xt; θt) ≤ f ( ˜xt; θt) − pt,i · gi( ˜xt; θt) < 0

which contradicts the deﬁnition of ˜xt in IGD(ˆγ) since we could always select ˜xt = 0 to obtain

0 in the objective value. Then from the non-negativity of ˆct, it holds that pt+1,i < pt,i in (11).

Starting from p1 = 0 and iteratively applying the above two property to control the increase of pt

from t = 1 to T , we obtain that for the ﬁrst time that one component of pt exceeds the threshold

q, it is upper bounded by q + 1 and this component will continue to decrease until it falls below
the threshold q. Thus, it is obvious that we have (cid:107)pt(cid:107)∞ ≤ q + 1 with probability 1 for each t. (cid:3)

B4. Proof of Theorem 3
Similar to case of known distribution, we deﬁne the following function ˆLt(·), based on the prior
estimate ˆPt.

ˆLt(p) := ˆγ (cid:62)

t p + ˆPth(p; θ).

(29)

Then we have the following relation between ˆL(·) and ˆLt(·). As its analysis is identical to Proposition
1, we omit its proof for simplicity.

Lemma 5 For each t = 1, ..., T , it holds that

ˆp∗ ∈ argminp≥0

ˆLt(p)

where ˆp∗ is deﬁned in (9) as the minimizer of the function ˆL(·). Moreover, it holds that

ˆL( ˆp∗) =

T
(cid:88)

t=1

ˆLt( ˆp∗).

(30)

(31)

Now we proof Theorem 3 and the idea of proof is similar to Theorem 1.

Proof: From the proof of Theorem 1, we have

T
(cid:88)

t=1

f ( ˜xt; θt) −

T
(cid:88)

t=1

f (xt; θt) ≤ q ·

m
(cid:88)

T
(cid:88)

gi( ˜xt; θt) · I

(cid:41)

gi( ˜xj; θj) > ci

(cid:40) t

(cid:88)

j=1

(cid:35)+

i=1

m
(cid:88)

t=1
(cid:34) T

(cid:88)

i=1

t=1

≤ q ·

gi( ˜xt; θt) − (ci − 1)

56

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

which related the total collected reward by the true action {xt}T

t=1 and the virtual action { ˜xt}T

t=1

of the algorithm IGD(ˆγ). Further from Proposition 1, we have that

RegT (π) ≤ min
p≥0

L(p) − E

(cid:34) T

(cid:88)

t=1

(cid:35)

f (xt; θt)

≤ min
p≥0

L(p) − E

(cid:34) T

(cid:88)

t=1

(cid:123)(cid:122)
I

f ( ˜xt; θt)

(cid:35)

(cid:125)

(cid:124)

+ q · E

(cid:124)





m
(cid:88)

(cid:34) T

(cid:88)

gi( ˜xt; θt) − (ci − 1)

i=1

t=1

(cid:123)(cid:122)
II

(cid:125)

(cid:35)+


We then bound the term I and term II separately to derive our regret bound.

Bound I: Note that (cid:107) ˆp∗(cid:107)∞ ≤ q, it follows directly from Lemma 3 and Lemma 5 that

L(p) ≤ L( ˆp∗) ≤ ˆL( ˆp∗) + max{q, 1} · (m + 1) · WT =

min
p≥0

T
(cid:88)

t=1

ˆLt( ˆp∗) + max{q, 1} · (m + 1) · WT

Note that from Lemma 4, we have that for each t, (cid:107)pt(cid:107)∞ ≤ (q + 1) with probability 1. Further note

that for each t, the distribution of pt is independent from the distribution of θt, then from Lemma

3 and Lemma 5, we have that

ˆLt( ˆp∗) = min

p≥0

ˆLt(p) ≤ Ept

(cid:104) ˆLt(pt)

(cid:105)

≤ Ept

(cid:2)ˆγ (cid:62)

t pt + Pth(pt; θt)(cid:3) + (q + 1)(m + 1) · W (Pt, ˆP)

where the expectation is taken with respect to the randomness of the dual price pt. Thus, we have

I ≤

T
(cid:88)

t=1

Ept

(cid:2)ˆγ (cid:62)

t pt + Pt {h(pt; θt) − f ( ˜xt; θt)}(cid:3) + 2(q + 1)(m + 1) · WT .

From the deﬁnition of ˜xt, we get that h(pt; θt) − f ( ˜xt; θt) = −p(cid:62)

t · g( ˜xt; θt), which implies that

I ≤

T
(cid:88)

t=1

Ept

(cid:2)p(cid:62)

t · (ˆγt − Ptg( ˜xt; θt))(cid:3) + 2(q + 1)(m + 1) · WT

Note that from the update rule (11), we have that

(cid:107)pt+1(cid:107)2

2 ≤ (cid:107)pt(cid:107)2

2 +

1
T

· (cid:107)g( ˜xt; θt) − ˆγt(cid:107)2

2 −

2
√
T

· p(cid:62)

t · (ˆγt − g( ˜xt; θt))

which implies that

Ept

(cid:2)p(cid:62)

t · (ˆγt − Ptg( ˜xt; θt))(cid:3) ≤

√

T
2

· (cid:0)E[(cid:107)pt(cid:107)2

2] − E[(cid:107)pt+1(cid:107)2

2](cid:1) +

m
√
T
2

Thus, it holds that

√

T

m

2

I ≤

+ 2(q + 1)(m + 1) · WT

(32)

Bound II: Note that from the update rule (11), we have that

√

T · pt+1 ≥

√

T · pt + g( ˜xt; θt) − ˆγt

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

57

which implies that

Thus, it holds that

T
(cid:88)

t=1

g( ˜xt; θt) − c ≤

T
(cid:88)

t=1

g( ˜xt; θt) −

√

ˆγt ≤

T · pT +1

T
(cid:88)

t=1

II = q · E





m
(cid:88)

(cid:34) T

(cid:88)

(cid:35)+

gi( ˜xt; θt) − (ci − 1)

 ≤ mq(q + 1) ·

√

T + qm

(33)

We obtain the O(max{

√

T , WT }) regret bound immediately by combining (32) and (33). (cid:3)

i=1

t=1

C. Proofs of Section 5

C1. Proof of Proposition 3

Proof: We consider the implementation of any online policy π on the two scenarios (13) and

(14) for κ = 1, which is replicated as follows for completeness:

max x1 + ... + xc + 2xc+1 + ... + 2xT

s.t. x1 + ... + xc + xc+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

max x1 + ... + xc

s.t. x1 + ... + xc + xc+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

(34)

(35)

where c = T

2 . Denote x1

t (π) as the decision of policy π at period t for scenario (34) and denote
x2
t (π) as the decision of policy π at period t for scenario (35). Further deﬁne T1(π) (resp. T2(π))
as the expected capacity consumption of policy π on scenario (34) (resp. scenario (35)) during the

ﬁrst T

2 time periods:

Then, we have that

T1(π) = E

T

2(cid:88)






 and T2(π) = E

x1
t (π)

T

2(cid:88)







x2
t (π)



t=1

t=1

R1

T (π) = T − T1(π)

and R2

T (π) = T2(π)

where R1

T (π) (resp. R2

T (π)) denotes the expected reward collected by policy π on scenario (34)
(resp. scenario (35)). Thus, the regret of policy π on scenario (34) and (35) are T1(π) and T − T2(π)

respectively. Further note that since the implementation of policy π at each time period should be

independent of the future information, we must have T1(π) = T2(π). Thus, we have that

RegT (π) ≥ max{T1(π), T − T1(π)} ≥

T
2

= Ω(T )

which completes our proof. (cid:3)

58

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

C2. Proof of Theorem 4

The proof of the theorem can be directly obtained from Theorem 2.

C3. Proof of Theorem 5

We ﬁrst prove the following lemma, which implies that the dual variable updated in (15) is always

bounded. It derivation is essentially the same as Lemma 2, so we omit its proof for simplicity.

Lemma 6 Under Assumption 1, for each t = 1, 2, . . . , T , the dual price vector satisﬁes (cid:107)pt(cid:107)∞ ≤

q + 1, where pt is speciﬁed by (15) in Algorithm 2 and the constant q is deﬁned in Assumption 1

(c).

Now we proceed to prove Theorem 5.

Proof: From the proof of Theorem 1, we have

T
(cid:88)

t=1

f ( ˜xt; θt) −

T
(cid:88)

t=1

f (xt; θt) ≤ q ·

m
(cid:88)

(cid:34) T

(cid:88)

i=1

t=1

gi( ˜xt; θt) − (ci − 1)

(cid:35)+

which relates the total collected reward by the true action {xt}T
t=1.
Here [·]+ denotes the positive part function. Furthermore, from Proposition 1 and the feasibility,

t=1 and the virtual action { ˜xt}T

we have that

RegT (π) ≤ min
p≥0

L(p) − E

(cid:34) T

(cid:88)

t=1

(cid:35)

f (xt; θt)

≤ min
p≥0

L(p) − E

(cid:34) T

(cid:88)

t=1

(cid:123)(cid:122)
I

f ( ˜xt; θt)

(cid:35)

(cid:125)

(cid:124)

+ q · E

(cid:124)





m
(cid:88)

(cid:34) T

(cid:88)

gi( ˜xt; θt) − (ci − 1)

i=1

t=1

(cid:123)(cid:122)
II

(cid:125)

(cid:35)+


Next, we bound the term I and term II separately to derive our regret bound.
Bound I: We ﬁrst deﬁne the following function ¯L(·):

¯L(p) :=

1
T

p(cid:62)c + ¯PT h(p; θ)

t=1 Pt, it holds that L(p) = T · ¯L(p) for any p. From Lemma 6, we know
Note that ˆPT = 1
that for each t, (cid:107)pt(cid:107)∞ ≤ q + 1 with probability 1. In addition, for each t, the distribution of pt is

T

(cid:80)T

independent from the distribution of θt, then from Lemma 3, we have that

¯L(p) ≤ Ept

(cid:2) ¯L(pt)(cid:3) ≤ Ept

min
p≥0

p(cid:62)

t c + Pth(pt; θt)

(cid:21)

(cid:20) 1
T

+ (q + 1)(m + 1) · W(Pt, ¯PT ),

(36)

where the expectation is taken with respect to pt in a random realization of the algorithm. Thus,

we have the ﬁrst term

I ≤

T
(cid:88)

t=1

Ept

(cid:20) 1
T

c(cid:62)pt + Pt {h(pt; θt) − f ( ˜xt; θt)}

(cid:21)

+ (q + 1)(m + 1) · W(Pt, ¯PT )

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

59

which comes from combining (36) with the relation L(p) = T · ¯L(p). By the deﬁnition of ˜xt,
h(pt; θt) − f ( ˜xt; θt) = −p(cid:62)

t · g( ˜xt; θt), which implies that

I ≤

T
(cid:88)

t=1

Ept

(cid:104)

p(cid:62)
t ·

(cid:16) c
T

− Ptg( ˜xt; θt)

(cid:17)(cid:105)

+ (q + 1)(m + 1) · W(Pt, ¯PT )

(37)

Note that from the update rule (15), we have that

(cid:107)pt+1(cid:107)2

2 ≤ (cid:107)pt(cid:107)2

2 +

1
T

· (cid:107)g( ˜xt; θt) −

c
T

(cid:107)2
2 −

2
√
T

· p(cid:62)
t ·

(cid:16) c
T

− g( ˜xt; θt)

(cid:17)

.

By taking expectation with respect to both sides,

Ept

(cid:104)

p(cid:62)
t ·

(cid:16) c
T

− Ptg( ˜xt; θt)

(cid:17)(cid:105)

≤

√

T
2

· (cid:0)E[(cid:107)pt(cid:107)2

2] − E[(cid:107)pt+1(cid:107)2

2](cid:1) +

Plugging (38) into (37), we obtain an upper bound on Term I,

√

T

m

2

I ≤

+ (q + 1)(m + 1) · WT

Bound II: Note that from the update rule (15), we have

√

T · pt+1 ≥

√

T · pt + g( ˜xt; θt) −

c
T

Taking a summation with respect to both sides,

g( ˜xt; θt) − c ≤

√

T · pT +1

T
(cid:88)

t=1

m
√
T

2

(38)

(39)

Applying Lemma 6 for a bound on pT +1, we obtain the upper bound for Term II,

II = q · E





m
(cid:88)

(cid:34) T

(cid:88)

(cid:35)+

gi( ˜xt; θt) − (ci − 1)

 ≤ mq(q + 1) ·

√

T + qm

(40)

i=1

t=1

We obtain the desired regret bound by combining (39) and (40). (cid:3)

D. Proofs of Section 6

D1. Proof of Theorem 6

Proof: Note that the regret can be denoted as

RegT (H, πResolve) = EH∼P[R∗

1(c1, H(1))] − EH∼P[

T
(cid:88)

rt · xt]

=

T
(cid:88)

t=1

EH∼P[R∗
(cid:124)

t (ct, H(t)) − R∗

t+1(ct+1, H(t + 1)) − rt · xt]
(cid:123)(cid:122)
(cid:125)
It

t=1

(41)

60

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

Now we denote {x∗

t (ct, H(t)). Clearly, we have the following:
if θt is realized as θ(j), i.e., Hj(t) = Hj(t + 1) + 1 and Hj(cid:48)(t) = Hj(cid:48)(t + 1) for other j(cid:48), it holds that

j (ct, H(t))} as one optimal solution to R∗

R∗

t (ct, H(t)) = rj + R∗

t+1(ct − aj, H(t + 1)),

R∗

t (ct, H(t)) = R∗

t+1(ct, H(t + 1)),

if x∗

j (ct, H(t)) ≥ 1

if Hj(t) − x∗

j (ct, H(t)) ≥ 1

(42)

We also denote the event At = {x∗

j (ct, H(t)) ≥ 1} and the event Bt = {Hj(t) − x∗

j (ct, H(t)) ≥ 1}.

In the rest of the proof, we mainly analyze the term It in (41). We ﬁrst focus on the case when

t ≤ T + 1 − 4(αβ2+β2)

· WT − 4
t=1
pmin
pmin
to the true distributions {Pt}T
t=1, pmin is given in Assumption 2, and α, β2 are parameters to be

, where WT is the deviation budget from the estimates { ˆPt}T

determined later.

Case I: ˆxjt(ct) ≥ 1

2 · E

H∼ ˆP[Hjt(t)]. Then we have xt = 1, and from (42), we have

It =EH∼P[R∗

t (ct, H(t)) − R∗

t+1(ct+1, H(t + 1)) − rt · xt]

=E (cid:2)E[rjt − rjt · xt + R∗
+ E (cid:2)E[−rjt · xt + R∗

=E (cid:2)E[−rjt · xt + R∗
≤P (Ac
t)

t+1(ct − ajt, H(t + 1)) − R∗

t+1(ct − ajt · xt, H(t + 1))|At](cid:3)

t+1(ct, H(t + 1)) − R∗

t+1(ct, H(t + 1)) − R∗

t](cid:3)
t+1(ct − ajt · xt, H(t + 1))|Ac
t](cid:3)
t+1(ct − ajt · xt, H(t + 1))|Ac

where Ac

t denotes the complementary event of At. We now bound P (Ac

t).

From Theorem 2.4 in (Mangasarian and Shiau, 1987), there exists a constant α > 0, which

depends only on {(r1, a1), . . . , (rn, an)}, such that

(cid:107)x∗(ct, H(t)) − ˆx(ct)(cid:107)∞ ≤ α · (cid:107)H(t) − E

H∼ ˆP[H(t)](cid:107)∞.

Moreover, from Hoeﬀdings’ inequality, we know that

(cid:18)

P

(cid:107)H(t) − E

H∼ ˆP[H(t)](cid:107)∞ ≤

pmin · (T − t + 1)
4α

(cid:19)

≥ 1 − exp(−β1 · (T − t + 1))

for some constant β1 > 0, and

(cid:107)E

H∼ ˆP[H(t)] − EH∼P[H(t)](cid:107)∞ ≤ β2 · WT .

Thus, we have that

(cid:18)

P

(cid:107)x∗(ct, H(t)) − ˆx(ct)(cid:107)∞ ≤

pmin · (T − t + 1)
4

(cid:19)

+ αβ2 · WT

≥ 1 − exp(−β1 · (T − t + 1))

Since we have

ˆxjt(ct) ≥

1
2

· E

H∼ ˆP[Hjt(t)] ≥

1
2

· pmin · (T − t + 1) − β2 · WT ,

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

61

we have that

(cid:18)

P

x∗
jt

(ct, H(t)) ≥

pmin · (T − t + 1)
4

− (αβ2 + β2) · WT ≥ 1

≥ 1 − exp(−β1 · (T − t + 1)).

(cid:19)

when t ≤ T + 1 − 4(αβ2+β2)

pmin

Thus, when t ≤ T + 1 − 4(α+1)
pmin

.

· WT − 4
pmin
· WT − 4
pmin

, we know that P (Ac

t) ≤ exp(−β1 · (T − t + 1)).

Case II: ˆxjt(ct) < 1

2 · E

H∼ ˆP[Hjt(t)]. Then we have xt = 0, and from (42), we have

It =E[R∗

t (ct, H(t)) − R∗
=E (cid:2)E[−rjt · xt + R∗

t+1(ct+1, H(t + 1)) − rt · xt]

t+1(ct, H(t + 1)) − R∗

t+1(ct − ajt · xt, H(t + 1))|Bt](cid:3)

+ E (cid:2)E[rjt − rjt · xt + R∗

t+1(ct − ajt, H(t + 1)) − R∗

t ](cid:3)
t+1(ct − ajt · xt, H(t + 1))|Bc

t+1(ct − ajt, H(t + 1)) − R∗

t ](cid:3)
t+1(ct, H(t + 1))|Bc

=E (cid:2)E[rjt + R∗
≤P (Bc
t )

Following the same approach, we have that P (Bc

t ) ≤ exp(−β1 ·(T −t+1)) when t ≤ T +1− 4(αβ2+β2)

·

pmin

WT − 4
pmin

.

Thus, on both cases, we conclude that

It ≤ exp(−β1 · (T − t + 1))

when t ≤ T + 1 − 4(αβ2+β2)

pmin

· WT − 4
pmin

. Thus, from (41), we have that

RegT (H, πResolve) ≤

T +1−

4(αβ2+β2)
pmin
(cid:88)

·WT − 4

pmin

t=1

It +

4(αβ2 + β2)
pmin

· WT +

4
pmin

≤

T
(cid:88)

t=1

exp(−β1 · (T − t + 1)) +

4(αβ2 + β2)
pmin

· WT +

4
pmin

= max{O(1), O(WT )}

which completes our proof. (cid:3)

Remark: Note that Vera and Banerjee (2020); Banerjee and Freund (2020a,b) utilize the condi-

tion of pmin in Assumption 2 to control the non-stationarity of the distribution Pt. To be speciﬁc,

suppose the support set Θ = {θ(1), . . . , θ(n)} and for each j = 1, . . . , n, we denote Hj(t) as the num-

ber of times that θτ is realized as θj for τ = t, . . . , T . Denote by H(t) = (H1(t), . . . , Hn(t)). Then

this concentration condition of pmin in Vera and Banerjee (2020); Banerjee and Freund (2020a,b)

essentially requires that

(cid:18)

P

(cid:107)H(t) − E[H(t)](cid:107) ≥

(cid:19)

E[Hj(t)]
2κ1
j

≤

cj
(T − t)2

, ∀t ≤ T − κ2

j , ∀j = 1, . . . , n

(43)

for some constants κ1

j , κ2

j and cj.

62

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

We now show through the following example for which our WBNB is sublinear in T and the

example can not covered by the above concentration condition. The example can be constructed

as follows. For t = 1, . . . , T −

T , we have Pt = P1 and for t = T −

T + 1, . . . , T , we have Pt = P2,

where P1 and P2 are two diﬀerent distributions and it is satisﬁed that P2(θ = θ(cid:48)

j) = 0 for a j(cid:48).
Clearly, (43) is not satisﬁed for j = j(cid:48), thus, the concentration condition does not hold. However,

√

√

√

T ), which is sublinear in T .

it is direct to check that the WBNB can be upper bounded by O(

D2. Analysis of the sub-optimality of static policies

D2.1. Proof of Proposition 4

Proof: We ﬁrst describe a problem instance. Consider a single resource with initial capacity

c = 3T /4. The parameter θt = (rt, 1); the functions f (xt, θt) = rt · xt and g(xt, θt) = xt, where
xt ∈ [0, 1] is the decision variable at time period t. Suppose that the prior estimate ˆP is given by






rt =

1,

w.p.

Unif[0, 1], w.p.

3
4
1
4

for each t. Here Unif[0, 1] denotes a uniform distribution over [0, 1]. Clearly, the deterministic
upper bound under prior estimate ˆP takes a value of 3T

4 . Furthermore, we denote E[hπ

t (r)] as the

expectation of the decision variable under the static policy π when the reward of period t is realized

as r. Then, we make following claim.

Claim: There exists a ˆr ∈ [1 − 2WT

T , 1 − WT

T ] such that (cid:80)3T /4
T , 1 − WT

E[hπ
t (ˆr)] ≤ T /4.
T ], we have (cid:80)3T /4

t=1

Otherwise, suppose that for each r ∈ [1 − 2WT

we compute the expected reward gained by the policy π from r ∈ [1 − 2WT

t (r)] ≥ T /4. Then,
T ] during the
ﬁrst 3T /4 periods, where the budget will never be violated. To be speciﬁc, note that the event

E[hπ
T , 1 − WT

t=1

r ∈ [1 − 2WT

T , 1 − WT
will cause a gap of at least WT
T

T ] happens with probability WT

4T at each period t. Comparing with RU B

for π to collect a reward r ∈ [1 − 2WT

T , 1 − WT

RUB

T and π on ˆP, caused from π obtaining reward from r ∈ [1 − 2WT

T , 1 − WT

T , this
T ]. Thus, the gap of
T ] during the ﬁrst 3T /4

periods is at least

WT
T

·

WT
4T

·

T
4

= C1 · T 1/2

which violates the regret upper bound. Thus, the claim is proved.

Denote by ˆr the value described in the claim. Now we construct a true distribution P =

{P1, . . . , PT } as follows:






rt =

ˆr,

w.p.

Unif[0, 1], w.p.

3
4
1
4

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

63

Clearly, the deviation budget is upper bounded by WT . Then, we compute the total expected

reward that policy π can collect on the true distribution P. The expected reward collected by

policy π during the ﬁrst 3T /4 periods is at most

1
4

·

1
2

·

3T
4

+

3
4

·

T
4

· ˆr ≤

9T
32

where the ﬁrst term in the LHS denotes the upper bound of the expected reward collected from

Unif[0, 1], and the second term in the LHS denotes the upper bound of the expected reward collect

from ˆr, which follows from the claim. Finally, without considering the budget violation and we let

π to collect every reward during the last T /4 periods. The policy π can collect reward at most

T
4

· (

1
2

·

1
4

+

3
4

) =

7T
32

during the last T /4 periods. Thus, the policy π can collect at most T /2 reward on the true
distribution P during the entire horizon. However, the value of EP[RUB
2WT
T ) ≥ 9T
policy π on the true distribution P is at least T
16 .

4 · (1 −
8 which clearly holds since WT grows sublinearly in T . Thus, the regret of

16 , when WT ≤ T

T ] is at least 3T

4 · ˆr ≥ 3T

D2.2. Discussion on Bid Price Policy

The well-known bid price policy (Talluri and Van Ryzin (1998)) computes a dual optimal solution

(bid price) ˆp∗ based on the prior estimates. Speciﬁcally, the policy makes the decision xt at each

period t based on the ﬁxed ˆp∗ throughout the procedure:

xt ∈ argmaxx∈X f (x, θt) − ( ˆp∗)(cid:62) · g(x, θt).

When there is no deviation between the true distributions and prior estimates, i.e., Pt = ˆPt for

each t, Talluri and Van Ryzin (1998) show that bid price policy is asymptotically optimal if each

period is repeated suﬃciently many times and the capacity is scaled up accordingly. The following

example shows that the bid price policy may fail drastically even when the deviations between the

true distributions and prior estimates are small. The example follows the same spirit as the lower

bound examples in our paper.

Consider the following linear program as the underlying problem (PCP) for the online stochastic

optimization problem:

max x1 + ... + x2c +

1
2

x2c+1 + ... +

1
2

xT

s.t. x1 + ... + x2c + x2c+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

(44)

64

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

where c = T

3 and without loss of generality, we assume c is an integer. Suppose that the prior
estimates for the coeﬃcients in the objective function is larger than the true coeﬃcients by 2(cid:15) for

the ﬁrst T

3 time periods and by (cid:15) for the last 2T

3 time periods. Then we obtain the following linear

program based on the prior estimates.

max (1 + 2(cid:15))x1 + ... + (1 + 2(cid:15))xc + (1 + (cid:15))xc+1 + ... + (1 + (cid:15))x2c + (

s.t. x1 + ... + x2c + x2c+1 + ... + xT ≤ c

0 ≤ xt ≤ 1 for t = 1, ..., T.

1
2

+ (cid:15))x2c+1 + ... + (

1
2

+ (cid:15))xT

(45)

Obviously, the optimal dual solution for (45) can take any value in (1 + (cid:15), 1 + 2(cid:15)). When we apply

such a bid price policy to the true problem (44), the policy will set xt = 0 throughout the horizon

as the reward per time period under the true problem (44) is no greater than 1. Given the optimal

objective value is T

3 , the bid price policy will incur a regret of T
3 .

As a remark, we note that the regret bound for our algorithm IGD(ˆγ) is upper bounded by

√

2(cid:15)T +

T which can be much smaller than T

3 for small (cid:15).

D3. Proof of Theorem 7

The proof follows the same argument as that of the previous upper bounds in Theorem 3 and

Theorem 5. We prove the details for completeness.

Proof: From the proof of Theorem 1, we have

T
(cid:88)

t=1

f ( ˜xt; θt) −

T
(cid:88)

t=1

f (xt; θt) ≤ q ·

m
(cid:88)

(cid:34) T

(cid:88)

i=1

t=1

gi( ˜xt; θt) − (ci − 1)

(cid:35)+

which relates the total collected reward by the true action {xt}T

t=1 and the virtual action { ˜xt}T

t=1.

Here [·]+ denotes the positive part function. Furthermore, from Proposition 1, we have that

RegT (π) ≤ min
p≥0

L(p) − E

(cid:34) T

(cid:88)

t=1

(cid:35)

f (xt; θt)

≤ min
p≥0

L(p) − E

(cid:34) T

(cid:88)

t=1

(cid:123)(cid:122)
I

f ( ˜xt; θt)

(cid:35)

(cid:125)

(cid:124)

+ q · E

(cid:124)





m
(cid:88)

(cid:34) T

(cid:88)

gi( ˜xt; θt) − (ci − 1)

i=1

t=1

(cid:123)(cid:122)
II

(cid:125)

(cid:35)+


where the function L(·) is given in (17). Next, we bound the term I and term II separately to derive

our regret bound.
Bound I: We ﬁrst deﬁne the following function ¯L(·):

¯L(p) :=

1
T

p(cid:62)c + ¯PT h(p; θ).

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

65

t=1 Pt, it holds that L(p) = T · ¯L(p) for any p. From Lemma 2, we know
Note that ˆPT = 1
that for each t, (cid:107)pt(cid:107)∞ ≤ q + 1 with probability 1. In addition, for each t, the distribution of pt is

T

(cid:80)T

independent from the distribution of θt, then from Lemma 3, we have that

¯L(p) ≤ Ept

(cid:2) ¯L(pt)(cid:3) ≤ Ept

min
p≥0

p(cid:62)

t c + Pth(pt; θt)

(cid:21)

(cid:20) 1
T

+ (q + 1)(m + 1) · W(Pt, ¯PT ),

(46)

where the expectation is taken with respect to pt in a random realization of the algorithm. Thus,

we have the ﬁrst term

I ≤

T
(cid:88)

t=1

Ept

(cid:20) 1
T

c(cid:62)pt + Pt

(cid:110)
h(pt; θt) − ˆf ( ˜xt; θt)

(cid:111)(cid:21)

+ (q + 1)(m + 1) · W(Pt, ¯PT )

which comes from ﬁrst taking expectation over f ( ˜xt; θt) for given ˜xt, θt, and then combining (46)
with the relation L(p) = T · ¯L(p). By the deﬁnition of ˜xt, h(pt; θt) − ˆf ( ˜xt; θt) = −p(cid:62)
which implies that

· ˆg( ˜xt; θt),

t

I ≤

T
(cid:88)

t=1

Ept

(cid:104)

p(cid:62)
t ·

(cid:16) c
T

− Pt ˆg( ˜xt; θt)

(cid:17)(cid:105)

+ (q + 1)(m + 1) · W(Pt, ¯PT )

(47)

Note that from the update rule (15), we have that

(cid:107)pt+1(cid:107)2

2 ≤ (cid:107)pt(cid:107)2

2 +

1
T

· (cid:107)g( ˜xt; θt) −

c
T

(cid:107)2
2 −

2
√
T

· p(cid:62)
t ·

(cid:16) c
T

− g( ˜xt; θt)

(cid:17)

.

By taking expectation with respect to both sides,

Ept

(cid:104)

p(cid:62)
t ·

(cid:16) c
T

− Pt ˆg( ˜xt; θt)

(cid:17)(cid:105)

≤

√

T
2

· (cid:0)E[(cid:107)pt(cid:107)2

2] − E[(cid:107)pt+1(cid:107)2

2](cid:1) +

Plugging (48) into (47), we obtain an upper bound on Term I,

√

T

m

2

I ≤

+ (q + 1)(m + 1) · WT

Bound II: Note that from the update rule (15), we have

√

T · pt+1 ≥

√

T · pt + g( ˜xt; θt) −

c
T

Taking a summation with respect to both sides,

g( ˜xt; θt) − c ≤

√

T · pT +1

T
(cid:88)

t=1

m
√
T

2

(48)

(49)

Applying Lemma 2 for a bound on pT +1, we obtain the upper bound for Term II,

II = q · E





m
(cid:88)

(cid:34) T

(cid:88)

(cid:35)+

gi( ˜xt; θt) − (ci − 1)

 ≤ mq(q + 1) ·

√

T + qm

(50)

We obtain the desired regret bound by combining (49) and (50). (cid:3)

i=1

t=1

66

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

D4. Proof of Theorem 8

The proof follows the same argument as that of the previous upper bounds in Theorem 3 and

Theorem 5. We prove the details for completeness.

We ﬁrst prove the following lemma, which implies that the dual variable updated in (21) is

always bounded. The analysis is similar to that of Lemma 2.

Lemma 7 Under Assumption 1, for each l = 1, 2, . . . , (cid:98)T /K(cid:99), the dual price vector satisﬁes

(cid:107)pl(cid:107)∞ ≤ q + αT,K · K, where pl is speciﬁed by (21) in Algorithm 4 and the constant q is deﬁned in

Assumption 1 (c).

Proof: Note that the following two properties are satisﬁed by the update rule (21):

(i). If (cid:107)pl(cid:107)∞ ≤ q, then we must have (cid:107)pl+1(cid:107)∞ ≤ q + αT,K · K by noting that for each i, the i-th

component of pl, denoted as pl,i, is nonnegative and gi(·, θt) is normalized within [0, 1] for

each t = (l − 1)K + 1, . . . , lK.

(ii). If there exists i such that pl,i > q, then we must have pl+1,i < pl,i. Speciﬁcally, when pl,i > q,

we must have that gi( ˜xt; θt) = 0 for each t = (l − 1)K + 1, . . . , lK, otherwise, we would have

that

f ( ˜xt; θt) − p(cid:62)

l · g( ˜xt; θt) ≤ f ( ˜xt; θt) − pl,i · gi( ˜xt; θt) < 0

which contradicts the deﬁnition of ˜xt in Algorithm 4 since we could always select xt = 0 to

obtain a zero objective value as per Assumption 1. Then from (21), it holds that pl+1,i < pl,i.

Starting from p1 = 0 and iteratively applying the above two property to control the increase of

pl from l = 1 to (cid:98)T /K(cid:99), we obtain that for the ﬁrst time that one component of pl exceeds the

threshold q, it is upper bounded by q + αT,K · K and this component will continue to decrease until

it falls below the threshold q. Thus, it is obvious that we have (cid:107)pl(cid:107)∞ ≤ q + αT,K · K with probability
1 for each l. (cid:3)

Now we proceed to prove Theorem 8.

Proof: From the proof of Theorem 1, we have

RegT (π) ≤

T
(cid:88)

t=1
(cid:124)

Lt(p) − E

min
p≥0

(cid:123)(cid:122)
I

(cid:34) T

(cid:88)

t=1

f ( ˜xt; θt)

(cid:35)

(cid:125)

+ q · E

(cid:124)





m
(cid:88)

(cid:34) T

(cid:88)

gi( ˜xt; θt) − (ci − 1)

(cid:35)+


i=1

t=1

(cid:123)(cid:122)
II

(cid:125)

We then bound the term I and term II separately to derive our regret bound.

Bound I: Note that for each t, the distribution of pl is independent from the distribution of θt for

any t = (l − 1)K + 1, . . . , lK, then we have that

Lt(p) ≤ Epl [Lt(pl)] = Epl

(cid:2)γ (cid:62)

t pl + Pth(pl; θt)(cid:3)

min
p≥0

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

67

where the expectation is taken with respect to the randomness of the dual price pt. Thus, we have

(cid:98)T /K(cid:99)
(cid:88)

lK∧T
(cid:88)

I ≤

l=1

t=(l−1)K+1

Epl

(cid:2)γ (cid:62)

t pl + Pt {h(pl; θt) − f ( ˜xt; θt)}(cid:3)

From the deﬁnition of ˜xt, we get that h(pt; θt) − f ( ˜xt; θt) = −p(cid:62)
l

· g( ˜xt; θt) for t = (l − 1)K +

1, . . . , lK, which implies that

(cid:98)T /K(cid:99)
(cid:88)

lK∧T
(cid:88)

I ≤

l=1

t=(l−1)K+1

Epl

(cid:2)p(cid:62)

l · (γt − Ptg( ˜xt; θt))(cid:3)

Note that from the update rule (21), we have that

(cid:107)pl+1(cid:107)2

2 ≤ (cid:107)pl(cid:107)2

2 + α2

T,K · (cid:107)

lK
(cid:88)

g( ˜xt; θt) − γt(cid:107)2

2 − 2αT,K ·

lK∧T
(cid:88)

p(cid:62)

t · (γt − g( ˜xt; θt))

t=(l−1)K+1

t=(l−1)K+1

which implies that

lK∧T
(cid:88)

t=(l−1)K+1

Epl

(cid:2)p(cid:62)

l · (γt − Ptg( ˜xt; θt))(cid:3) ≤

1
2αT,K

Thus, it holds that

· (cid:0)E[(cid:107)pl(cid:107)2

2] − E[(cid:107)pl+1(cid:107)2

2](cid:1) +

Bound II: Note that from the update rule (21), we have that

I ≤

mT K · αT,K
2

1
αT,K

· pl+1 ≥

1
αT,K

· pl +

lK
(cid:88)

(g( ˜xt; θt) − γt)

t=(l−1)K+1

which implies that

mK 2 · αT,K
2

(51)

T
(cid:88)

t=1

g( ˜xt; θt) − c ≤

(cid:98)T /K(cid:99)
(cid:88)

lK∧T
(cid:88)

(g( ˜xt; θt) − γt) ≤

l=1

t=(l−1)K+1

1
αT,K

· p(cid:98)T /K(cid:99)+1

Thus, it holds that

II = q · E





m
(cid:88)

(cid:34) T

(cid:88)

(cid:35)+

gi( ˜xt; θt) − (ci − 1)

 ≤ mq(q + αT,K · K) ·

i=1

t=1

1
αT,K

+ qm

(52)

We obtain the O(T KαT,K + 1

αT,K

+ K) regret bound immediately by combining (51) and (52). (cid:3)

E. More Numerical Results for Experiment I

68

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

α = 0

α = 0.01

α = 0.02

α = 0.04

Upper Bound

537000

550158

559358

571775

IGDP

520173(96.9%) 528884(96.1%) 535678(95.8%) 542302(94.9%)

Re-solve (200) 524233(97.6%) 536703(97.6%) 543961(97.3%) 555291(97.1%)

β = 0

Re-solve (100) 523685(97.5%) 536427(97.5%) 544318(97.3%) 556507(97.3%)

Re-solve (50)

523700(97.5%) 537134(97.6%) 544105(97.3%) 555119(97.1%)

Re-solve (1)

527839(98.3%) 540176(98.2%) 547443(97.9%) 558640(97.7%)

IGDP

518718(96.6%) 528611(96.1%) 534221(95.6%) 543138(95.0%)

Re-solve (200) 524267(97.6%) 536168(97.6%) 545057(97.5%) 555260(97.1%)

β = 0.01

Re-solve (100) 524438(97.7%) 537389(97.7%) 544832(97.5%) 557295(97.5%)

Re-solve (50)

525178(97.8%) 537462(97.7%) 545845(97.7%) 556276(97.3%)

Re-solve (1)

529752(98.7%) 541583(98.5%) 549673(98.3%) 560308(98.0%)

IGDP

518287(96.5%) 527937(96.0%) 533538(95.5%) 545654(95.5%)

Re-solve (200) 523689(97.5%) 534737(97.3%) 543426(97.2%) 556498(97.4%)

β = 0.02

Re-solve (100) 525031(97.8%) 537302(97.7%) 545829(97.7%) 555397(97.3%)

Re-solve (50)

525356(97.8%) 537465(97.8%) 544992(97.5%) 555949(97.3%)

Re-solve (1)

529732(98.7%) 541779(98.5%) 550519(98.5%) 560719(98.2%)

IGDP

518405(96.5%) 528083(96.0%) 536737(96.0%) 545405(95.5%)

Re-solve (200) 524127(97.6%) 535318(97.4%) 544818(97.5%) 554169(97.0%)

β = 0.04

Re-solve (100) 523991(97.6%) 536770(97.6%) 544874(97.5%) 556560(97.5%)

Re-solve (50)

525299(97.8%) 536481(97.6%) 545335(97.6%) 556776(97.5%)

Re-solve (1)

530349(98.8%) 541633(98.5%) 550063(98.4%) 560365(98.1%)

Table 2

The numerical experiment based on the network revenue management problem (Jasin (2015)). The

results are obtained for T = 1000 and 100 trails. Re-solve (a) denotes the frequency of re-solving, namely, re-solving

the upper bound problem to update dual variable and ˆγt for every a periods. For each entry b(c) of the table, b

denotes the expected reward collected by the algorithm and c denotes the percentage of the expected reward of the

algorithm over the upper bound.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

69

α = 1

α = 1.5

α = 2

α = 2.5

α = 3

Upper Bound

282.5433

363.7044

459.7807

563.3545

670.5960

IGDP 270.2411 (96%) 349.1769 (96%) 441.6677 (96%) 543.3373 (96%) 645.6582 (96%)

UGD 270.3621(96%) 337.3192 (93%) 403.7044 (88%) 469.7643 (83%) 535.0654 (80%)

FBP 270.1211 (96%) 347.4997 (96%) 439.7016 (96%) 539.9865 (96%) 642.3940 (96%)

O2O 270.6872 (96%) 351.2534 (96%) 440.9020 (96%) 545.2106 (96%) 644.4059 (96%)

IGDP 270.1595 (96%) 347.9148 (96%) 439.6166 (96%) 539.8719 (96%) 643.6777 (96%)

UGD 270.3568 (96%) 338.6916 (93%) 405.7927 (88%) 473.6640 (84%) 540.0894 (81%)

FBP 64.7526 (23%)

174.2642 (48%) 314.7806 (68%) 446.7646 (79%) 582.7744 (87%)

O2O 202.3966 (72%) 289.6858 (80%) 423.5247 (92%) 536.0362 (95%) 621.4144 (93%)

IGDP 269.8058(95%) 347.1246 (95%) 437.6279 (95%) 535.3521 (95%) 638.8322 (95%)

UGD 269.6893 (95%) 339.1676 (93%) 408.3862 (89%) 477.2329 (84%) 544.9401 (81%)

FBP

4.8549 (2%)

53.7881 (15%)

188.3271 (41%)

340.850 (61%)

486.4006 (73%)

O2O 202.6792 (72%) 202.9555 (56%) 330.5638 (72%) 480.2536 (85%) 620.2584 (92%)

IGDP 265.1512 (94%) 343.7802 (95%) 432.2275 (94%) 527.4351 (94%) 627.7440 (94%)

UGD 265.4187 (94%) 337.4751 (93%) 410.3510 (89%) 482.8652 (86%) 554.0038 (83%)

FBP

0.0171 (0%)

1.6210 (0.5%)

22.5201 (5%)

104.5026 (19%) 243.1575 (36%)

β = 0

β = 0.5

β = 1

β = 2

O2O 203.5851 (72%) 206.4316 (57%) 210.6264 (46%) 361.1129 (64%) 514.6730 (77%)

Table 3

Computation results for Experiment I under the uniform setting: The results are reported based on 500

simulation trials. For each entry b(c) of the table, b denotes the expected reward collected by the algorithm and c

denotes the percentage of the expected reward of the algorithm over the upper bound.

70

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

α = 1

α = 1.5

α = 2

α = 2.5

α = 3

Upper Bound

705.1450

803.5559

921.6550

1060.5567

1213.3552

IGDP 679.9687 (96%) 771.9511 (96%) 884.2661 (96%) 1022.9439 (96%) 1171.1208 (97%)

UGD 681.1875 (97%) 764.6037 (95%) 840.5685 (91%)

928.6610 (88%)

1018.0635 (84%)

FBP 674.968 (96%)

768.0327 (96%) 883.1946 (96%) 1013.5706 (96%) 1165.1827 (96%)

O2O 676.3823 (96%)

769.396 (96%)

882.5831 (96%) 1017.4399 (96%) 1159.9922(96%)

IGDP 680.4068 (97%) 773.4652 (96%) 890.2161 (96%) 1023.3161 (97%) 1170.2227 (96%)

UGD 680.931 (97%)

765.1636 (95%) 851.4681 (92%)

934.5877 (88%)

1028.4509 (85%)

FBP

459.16 (65%)

525.5739 (66%)

619.198 (67%)

751.7758 (71%)

907.9537 (75%)

O2O 656.6648 (93%)

712.215 (89%)

784.1766 (85%)

873.1237(82%)

996.943 (82%)

IGDP 680.5808 (97%) 777.7524 (97%) 888.5206 (96%) 1024.4519 (97%) 1175.2438 (97%)

UGD 681.824 (97%)

766.3631 (95%) 851.6300 (92%)

944.0707 (89%)

1035.5280 (85%)

FBP 254.2547 (36%) 296.1898 (37%) 375.3518 (41%)

476.4165 (45%)

610.2982 (50%)

O2O 588.1039 (83%) 744.5618 (93%) 833.9981 (90%)

916.2876 (86%)

1009.1384 (83%)

IGDP 666.6237 (94%) 764.0318 (95%) 873.8978 (95%) 1014.7539 (96%) 1160.7693 (96%)

UGD 667.4160 (95%) 756.7188 (94%) 851.0122 (92%)

944.1322 (89%)

1046.7647 (86%)

FBP

51.9166 (7%)

68.2078(8%)

100.4066 (11%)

148.1866 (14%)

212.5127 (18%)

β = 0

β = 0.5

β = 1

β = 2

O2O 466.4348 (66%) 535.7102 (67%) 728.6945 (79%)

957.5749 (90%)

1072.6659(88%)

Table 4

Computation results for Experiment I under the normal setting: The results are reported based on 500

simulation trials. For each entry b(c) of the table, b denotes the expected reward collected by the algorithm and c

denotes the percentage of the expected reward of the algorithm over the upper bound.

Jiang, Li and Zhang: Online Stochastic Optimization with Wasserstein Based Non-stationarity

71

α = 1

α = 1.5

α = 2

α = 2.5

α = 3

Upper Bound

532.6379

630.1063

746.5027

871.63281

1010.7956

IGDP 513.2625 (96%) 609.6643 (97%) 717.3434 (96%) 840.7550 (96%) 973.4778 (96%)

UGD 513.9690 (97%) 592.9739 (94%) 672.7648 (90%) 758.0174 (87%) 840.9000 (83%)

FBP 511.9639 (96%) 601.4187 (95%) 714.4849 (96%) 831.7872 (95%) 964.0174 (95%)

O2O 512.9872 (96%) 603.5403 (96%) 713.4253 (96%) 834.2972 (96%) 970.8423 (96%)

IGDP 517.2177 (97%) 607.7310 (96%) 714.3025 (96%) 843.8988 (97%) 975.2676 (97%)

UGD 514.1228 (96%) 596.0002 (95%) 678.3624 (91%) 762.7194 (87%) 844.0139 (84%)

FBP 372.5857 (70%) 441.8714 (70%) 542.3811 (73%) 667.7373 (77%) 821.8302 (82%)

O2O 478.3224 (90%) 600.7489 (95%) 681.4778 (91%) 774.0669 (89%) 881.9813 (87%)

IGDP 517.8502 (97%) 609.3739 (97%) 717.3040 (96%) 842.7028 (97%) 977.9344 (97%)

UGD 515.1376 (97%) 598.2675 (95%) 683.0459 (92%) 769.4935 (88%) 853.1061 (85%)

FBP 253.2191 (47%) 295.4627 (47%) 372.9051 (50%) 469.2851 (54%) 613.1751 (61%)

O2O 391.8374 (73%) 509.6126 (81%) 674.1803 (90%) 813.8023 (93%) 912.8408 (91%)

IGDP 507.9654 (95%) 598.2148 (95%) 713.2802 (96%) 835.5463 (96%) 963.8020 (95%)

UGD 505.8237 (95%) 591.2771 (94%) 684.4290 (92%) 770.5260 (88%) 866.7157 (86%)

FBP 78.3954 (15%)

96.3983 (15%)

136.5682 (18%) 197.7166 (23%) 281.8724 (28%)

β = 0

β = 0.5

β = 1

β = 2

O2O 343.4216 (64%) 358.0907 (57%) 494.0880 (66%) 686.6650 (79%) 890.9912 (88%)

Table 5

Computation results for Experiment I under the mixed setting: The results are reported based on 500

simulation trials. For each entry b(c) of the table, b denotes the expected reward collected by the algorithm and c

denotes the percentage of the expected reward of the algorithm over the upper bound.

