0
2
0
2

l
u
J

6

]
L
M

.
t
a
t
s
[

1
v
9
2
8
2
0
.
7
0
0
2
:
v
i
X
r
a

Solving Bayesian Network Structure Learning
Problem with Integer Linear Programming

Ronald Seoh

A Dissertation Submitted to the Department of Management
of the London School of Economics and Political Science
for the Degree of Master of Science

01 Sep 2015

 
 
 
 
 
 
Abstract

This dissertation investigates integer linear programming (ILP) formulation of
Bayesian Network structure learning problem. We review the deﬁnition and key
properties of Bayesian network and explain score metrics used to measure how
well certain Bayesian network structure ﬁts the dataset. We outline the integer
linear programming formulation based on the decomposability of score metrics.
In order to ensure acyclicity of the structure, we add “cluster constraints”
developed speciﬁcally for Bayesian network, in addition to cycle constraints ap-
plicable to directed acyclic graphs in general. Since there would be exponential
number of these constraints if we specify them fully, we explain the methods to
add them as cutting planes without declaring them all in the initial model. Also,
we develop a heuristic algorithm that ﬁnds a feasible solution based on the idea
of sink node on directed acyclic graphs.

We implemented the ILP formulation and cutting planes as a Python pack-
age, and present the results of experiments with diﬀerent settings on reference
datasets.

Contents

1 Introduction

2 Preliminaries

2.2

2.1 Bayesian Network . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.2 Key Characteristics of Bayesian Network . . . . . . . . . .
Integer Linear Programming . . . . . . . . . . . . . . . . . . . . .
2.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Branch-and-Bound . . . . . . . . . . . . . . . . . . . . . .
2.2.3 Cutting Planes
. . . . . . . . . . . . . . . . . . . . . . . .
2.2.4 Branch-and-Cut . . . . . . . . . . . . . . . . . . . . . . . .

3 Learning the Structure

3.2

3.1 Score Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 Bayesian Dirichlet . . . . . . . . . . . . . . . . . . . . . . .
ILP Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1
. . . . . . . . . . . . . . . . .
3.2.2 Objective . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.3 Constraints

Sets, Parameters, Variables

4 Finding Solutions

4.1 Cluster Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Cycle Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Sink Finding Heuristic . . . . . . . . . . . . . . . . . . . . . . . .

5 Implementation and Experiments

5.1

Implementation Details . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
5.1.1 Notes on Branch-and-Cut
5.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.1
Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.2 Experiment 1: All Features Turned On . . . . . . . . . . .
5.2.3 Experiment 2: Without Cycle Cuts . . . . . . . . . . . . .
5.2.4 Experiment 3: Without Gomory Cuts . . . . . . . . . . . .
5.2.5 Performance of Sink Finding Heuristic . . . . . . . . . . .

1

3
3
3
5
8
8
8
10
10

12
12
13
18
18
18
19

21
21
23
23

26
26
27
28
28
29
30
31
35

6 Conclusion

6.1 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.1.1
Statistical Modelling . . . . . . . . . . . . . . . . . . . . .
6.1.2 Optimisation . . . . . . . . . . . . . . . . . . . . . . . . .

Appendices

A Software Instructions

36
36
36
37

39

39

Chapter 1

Introduction

Bayesian network is a probabilistic graphical model using directed acyclic graphs
to express joint probability distributions and conditional dependencies between
diﬀerent random variables. Nodes represent random variables and directed arcs
are drawn from parent nodes to child nodes to show that the child node is condi-
tionally dependent to its parent nodes. Aside from its mathematical properties,
Bayesian network’s visual presentation make them easily perceivable, and many
researchers in diﬀerent ﬁelds have used it to model and study their systems.

Constructing a Bayesian network requires two major components: its graph
topology, and parameters for the joint probability distribution. In some cases,
the structure of the graph gets gets speciﬁed in advance by “experts” and we
ﬁnd the values for the parameters that ﬁt the given data, more speciﬁcally by
using maximum likelihood approach.

The problem gets more complicated when we do not know the graph structure
and have to learn it from the given data. This would be the case when the
problem domain is too large and it is extremely diﬃcult or impractical for humans
to manually deﬁne the structure. Learning the structure of the network from data
have been proven NP-hard, and there have been several diﬀerent approaches over
the years to tackle this problem.

Early methods were approximate searches, where the algorithm searches
through the candidates to look for the most probable one based on their strate-
gies but usually does not provide any guarantee of optimality. Later on, there
have developments on exact searches based on conditional independence testing
or dynamic programming. While these did provide some level of optimality, their
real world applicability was limited as the amount of computation became infea-
sible for larger number of datasets, or the underlying assumptions could not be
easily met in reality.

In this dissertation, we discuss another method of conducting exact search of
Bayesian network structure using integer linear programming (ILP). While this
approach is relatively new in the ﬁeld, it achieves fast search process based on
various integer programming techniques and state-of-the-art solvers, and allows
users to incorporate prior knowledge easily as constraints.

1

2

One of the main challenges in Bayesian network structure learning is on how
to enforce acyclicity of the resulting structure. While acyclicity constraint devel-
oped for general DAGs also applies to Bayesian network, it is not tight enough for
our ILP formulation due to the fact that we select the set of parent nodes for each
variables rather than individual edges in the graph. We study so-called “cluster
constraints” developed by Jaakkola et al.[27] that provides stronger enforcement
of acyclicity on Bayesian network structure. Since there will be exponential num-
ber of such constraints if we specify them fully, we also cover how we can add
them as cutting planes when needed during the solution process.

We also implemented a test computer program based on this ILP formulation
and examined their performance on some of the sample datasets. There are few
other implementation publicily available, but we provide a clean object-oriented
version written in Python programming language.

The structure of this disseration will be as follows: Chapter 2 will review the
concept of integer linear programming and Bayesian network needed to under-
stand the problem. Chapter 3 will examine score metrics used for BN structures,
and present the ILP formulation. Chapter 4 will explain our approach on adding
cluster constraints as cutting planes to the ILP model, and a heuristic algorithm
to obtain good feasible solutions. Chapter 5 will present our implementation
details and benchmark results. Lastly, Chapter 6 will provide pointers to further
development of our methods.

Chapter 2

Preliminaries

This chapter reviews two theoretical and conceptual foundations behind the topic
of this dissertation: Bayesian Network (BN) and Integer Linear Programming
(ILP).

2.1 Bayesian Network

2.1.1 Overview

Consider a dataset D that contains n predictor variables x1, x2, x3, ..., xn, the
class variable y that can take m classes y1, y2, y3, ..., ym and k samples s1, s2, s3, ..., sk.
Let’s suppose we want to ﬁnd probabilities for each possible values of y, given
In other words, if we want to
the observations of all the predictor variables.
calculate the probabilities of y = y1, it can written as

P (y = y1 | x1, x2, ..., xn) =

P (y = y1) × P (x1, x2, ..., xn | y)
P (x1, x2, ..., xn)

(2.1)

where the left hand side shows posterior probabilities, P (y = y1) on the right
hand side prior proabibilities of y = y1, P (x1, x2, ..., xn | y) support data provides
for y = y1, and P (x1, x2, ..., xn) on the denominator normalising constant.

One of the simplest and most popular approaches to the task stated above
is Naive Bayes[35], where we simply assume that all the predictor variables are
indepedent from each other. Then Equation 2.1 becomes

P (y = y1 | x1, x2, ..., xn) =

P (y = y1) × (cid:81)

i=1 P (xi | y)

P (x1, x2, ..., xn)

(2.2)

The question is, what if these predictors are actually not completely inde-
pendent as Naive Bayes assumed? Depending on the subject domain, it might
If we
be the case that some of the predictors have dependency relationships.
assume that all the variables are binary and we need to store all the arbitrary

3

4

dependencies without any additional assumptions, that means we need to store
2n − 1 values in the memory - which would become too much computational
burden even for relatively small number of variables.

This is where Bayesian network comes in - it leverages a graph structure to
provide more intuitive representation of conditional dependencies in the domain
and allow the user to perform inference tasks in reasonable amount of time and
resources.

Deﬁnition 2.1.1. A Bayesian Network G = (N, A) is a probabilistic graphical
model in a directed acyclic graph (DAG) where each node n ∈ N represents a
variable in the dataset and each arc (i, j) ∈ A indicates the variable j being
probabilistically dependent on i.1

We can therefore perceive Bayesian network as consisting of two components:

1. Structure, which refers to the directed acyclic graph itself: nodes and

arcs that specify dependencies between the variables,

2. Parameters, corresponding conditional probabilities of each node (vari-

able) given its parents.

Figure 2.1: ASIA Bayesian Network Structure

One example BN constructed from the ASIA dataset by Lauritzen and Spiegel-
halter is presented in Figure 2.1.[18]

1We are using the neutral expression ‘probabilistically dependent’ as the interpretation of
the arcs might become based on the assumptions on the domain. Some researchers interpret i
to be direct cause of j, which might not be valid in other cases.

5

2.1.2 Key Characteristics of Bayesian Network

Markov Condition

The key assumption behind Bayesian network is that one node (variable) is
conditionally independent on its non-descendants, given its parent nodes. This
assumption signiﬁcantly reduces the space required for inference tasks, since one
would need to have only the values of the parents.

To be precise, let’s say our Bayesian network tell us that x2 is a parent node
of x1, but not others. That means x1 is independent of the rest of the variables
given x2. Then we can change Equation 2.1 into

P (y = y1 | x1, x2, ..., xn) =

P (y = y1) × P (x1|x2, y) × P (x2, ..., xn | y)
P (x1, x2, ..., xn)

(2.3)

If we can ﬁnd other conditional independencies like this using Bayesian net-
work, we can keep changing Equation 2.3 above into hopefully smaller number
of terms than what we would have had by assuming arbitrary dependencies and
independencies.

The assumption explained above is called Markov condition, which the formal

deﬁnition[18] is stated below:

Deﬁnition 2.1.2. Given a graph G = (N, A) and a joint probability distribu-
tion P deﬁned over variables represented by the nodes n ∈ N . If the following
statement is also true

∀n ∈ N, n ⊥⊥p N D(v) | P a(v)

where N D(v) refers to the non-descendants and P a(v) parent nodes of v,
then we can say that G satisﬁes the Markov condition with P, and (G,P) is a
Bayesian network.

d-separation

As stated in Deﬁnition 2.1.1, the structure of Bayesian network is a directed
acyclic graph: main motivation for using it was to make use of conditional in-
dependence to store uncertain information eﬃciently by associating dependence
with connectedness and independence with un-connectedness in graphs. By ex-
ploiting the paths on DAG, Judea Pearl introduced a graphical test called d-
separation in 1988 that can discover all the conditional independencies that are

6

implied from the structure (or equivalently Markov condition stated in Deﬁni-
tion 2.1.2):

Deﬁnition 2.1.3. A trail in a directed acyclic graph G is an undirected path in
G, which is a connected sequence of edges in G(cid:48) where all the directed edges in
G are replaced with undirected edges.

Deﬁnition 2.1.4. A head-to-head node with respect to trail t is a node x in t
where there are consecutive edges (α, x), (x, β) for some nodes α and β in t.

Deﬁnition 2.1.5. (Pearl 1988) If J, K, L are three disjoint subsets of nodes
in a DAG D, then L is said to d-separate J from K, denoted I(J, L, K)D, iﬀ
there is no trail t between a node in J and a node in K along which (1) every
head-to-head node (w.r.t. t) either is or has a descent in L and (2) every node
that delivers an arrow along t is outside L. A trail satisfying the two conditions
above is said to be active, otherwise it is said to be blocked (by L).[20]

Expanding Deﬁnition 2.1.5, we can summarise the types of dependency rela-

tionships extractable from Bayesian network into following[8][31][17]:

Figure 2.2: Diﬀerent Connection Types in DAG

• Indirect cause: Consider the connection type ‘Linear’ in Figure 2.2. We
can say that a is independent of c given b. Although a might explain c up to
some degree, it becomes conditionally independent when we have b. In light
of d-separation deﬁnition, we can say that a c is d-connected and active
when we condition against something other than a, b, c. However, when we
consider b as a condition (member of L in the deﬁnition), b d-separates a
and c.

• Common eﬀect: For the ‘Converging’ connection type, we can say that
a and c becomes dependent given b. a and c are independent without b.
However, once we condition on b, the two become dependent - once we

7

know b has occurred, either one of a and c would explains away the other
since b is probabilistically dependent on a and c.

• Common cause: For the ‘Diverging’ connection type, a is independent of

c given b.

Markov Equivalent Structures

Another charcteristic that arise from the structure of Bayesian network is that
it might be possible to ﬁnd another DAG that encodes same set of conditional
independencies as original. Two directed acyclic graphs are considered Markov
equivalent[33] iﬀ they have

• same skeletons, which is a graph which all the directed edges of the DAG

are replaced with undirected ones, and

• same v-structures, which refers to all the head-to-head meetings of di-

rected edges without unjoined tails in the DAG.

To represent this equivalence class, we use Partially Directed Acyclic Graph
(PDAG) with every directed edge compelled and undirected edge reversible.

8

2.2

Integer Linear Programming

2.2.1 Overview

Integer Linear Programming (ILP) refers to the special group of Linear Program-
ming (LP) problems where the domains of some or all the variables are conﬁned
to integers, instead of real numbers in LP. Such problems can be written in the
form

max c(cid:62)x

subject to Ax ≤ b
x ∈ Z

(2.4)

If the problem have both non-integer and integer variables, we call it a Mixed
Integer Linear Programming (MILP) problem. While ILP obviously can be used
in situations where only integral quantities make sense, for example, number
of cars or people, more eﬀective uses of integer programming often incorporate
binary variables to represent logical conditions like yes or no decisions.[34]

The major issue involved with solving ILPs is that they are usually harder
to solve than non-integer LP problems. Unlike conventional LPs which Simplex
algorithm can directly produce the solution by checking extreme points of feasible
region,2 ILP need additional steps to ﬁnd a speciﬁc solution that are strictly
integers. There are mainly two approaches to solving ILP - 1) Branch-and-Bound
and 2) Cutting Planes. In fact, these two are not mutually exclusive to be precise
but rather can be combined and used together to solve ILP, the approach called
3) Branch-and-Cut. The following subsections will brieﬂy describe the ﬁrst two
and outline the details of the third, which is actually used in the main part of
this dissertation.

2.2.2 Branch-and-Bound

The term “Branch-and-Bound” (BnB) itself refers to an algorithm design paradigm
that was ﬁrst introduced to solve discrete programming problems, developed by
Alisa H. Land and Alison G. Doig of the London School of Economics in 1960.[29]
BnB have been the basis of solving a variety of discrete and combinatorial opti-
misation problems,[11] most notably integer programming.

2It is therefore possible that such solution happens to be integers; however, it is generally

known from past experience that such case rarely appears in practice.

9

Figure 2.3: Branch-and-Bound example

BnB in IP can be best described as a divide-and-conquer approach to sys-
tematically explore feasible regions of the problem. Graphical representation of
this process is shown in Figure 2.3.[12] We start from the solution of LP-relaxed
version of the original problem, and choose one of the variables with non-integer
solutions - let’s call this with variable xi with non-integer solution f . Then we
create two additional sub-problems (branching) by having

• one of them with additional constraint that xi ≤ (cid:98)f (cid:99),

• another with xi ≥ (cid:100)f (cid:101).

We choose and solve one of the two problems.

If the solutions still aren’t
integers, branch on that as above and solve one of the new nodes (problems),
and so on.

• If one of the new problems return integer solutions, we don’t have to branch
any more on that node (pruned by integrality), and this solution is called
incumbent solution - the best one yet.

• If a new problem is infeasible, we don’t have to branch on that as well

(pruned by infeasibility).

• If the problem returns integer solutions but the objective value is smaller
than the incumbent, then we stop branching on that node (pruned by
bound).

When branching terminates on certain node, we go back to the direct parent
node and start exploring the branches that haven’t been explored yet. After we
keep following these rules and there are no more branches to explore, than the
incumbent solution at that moment is the optimal solution.

10

2.2.3 Cutting Planes

Cutting planes is another approach for solving ILP that was developed before
BnB. The basic idea is that we generate constraints (which would be a hyper-
plane) that can cut out the currunt non-integer solution and tighten the feasible
region. If we solve the problem again with the new constraint and get integer
solutions, then the process ends. If not, we continue adding cutting planes and
solve the problem until we get integer solutions.

The question is how we generate the one that can cut out the region as much
as possible. There are a number of diﬀerent strategies available for this task,
but the most representative and the one implemented for this project is called
Gomory Fractional Cut:

n
(cid:88)

j=1

(aj − (cid:98)aj(cid:99))xj >= a0 − (cid:98)ao(cid:99)

(2.5)

The equation above is called Gomory fractional cut,[21][12] where aj, xj comes
from the row of the optimal tableau. Since (cid:80)n
j=1 ajxj = a0, there must be a
k ∈ Z that satisﬁes (cid:80)n
j=1(aj − (cid:98)aj(cid:99))xj = a0 − (cid:98)ao(cid:99) + k. Also, k is non-negative
since (cid:80)n
j=1(aj − (cid:98)aj(cid:99))xj is non-negative. Therefore Equation 2.5 holds. As a
result, Gomory factional cut can cut out the current fractional solution from the
feasible region.

In additional to general purpose cuts like Gomory cuts that can be applied to
all the IP problems, we might be able to generate cutting planes that are speciﬁc
to the problem. This dissertation also examines some domain-speciﬁc cutting
planes for the BN structure learning problem, which is done in chapter 4.

2.2.4 Branch-and-Cut

Branch-and-Cut combines branch-and-bound and cutting planes into one algo-
rithm, and it is the most successful way of solving ILP problems to date. Es-
sentially following the structure of BnB to explore the solution space, we add
cutting planes whenever possible on LP-relaxed problems before branching to
tighten the upper bound, so that we can hopefully branch less than the stan-
dard BnB. The decision of whether to add cutting planes depends on the speciﬁc
problems and success of previously added cuts. Pseudocode of Branch-and-Cut

11

algorithm is presented in algorithm 1. The criteria used for our problem will also
be discussed in chapter 4.

Algorithm 1: Branch-and-Cut Algorithm

Require: initial problem, problem list, objective upper bound, best solution
Ensure: best solution ∈ Z

objective lower bound ⇐ −∞
problem list ⇐ problem list ∪ initial problem
while problem list (cid:54)= ∅ do

current problem ⇐ p ∈ problem list
Solve LP-Relaxed current problem
if infeasible then

Go back to the beginning of the loop

else

z ⇐ current objective, x ⇐ current solution

end if
if z ≤ objective upper bound then

Go back to the beginning of the loop

end if
if x ∈ Z then

best solution ⇐ x
objective upper bound ⇐ z
Go back to the beginning of the loop

end if
if cutting planes applicable then

Find cutting planes violated by x
add the cutting plane and go back to ‘Solve LP-Relaxed
current problem’

else

Branch using non-integer solutions in x
problem list ⇐ problem list ∪ new problems

end if
end while
return best solution

Chapter 3

Learning the Structure

In some cases, it might be possible to create a Bayesian network manually if
there’s a subject expert who already have a knowledge about the relationships
between the variables. Many statistical softwares such as OpenBUGS1 and Net-
ica2 allow the users to specify BN models.

However, it would be impossible to create such models in other cases, either
because we simply do not have such domain knowledge or there are too many
variables to consider. Therefore, there have been constant interests in ways to
automatically learn the Bayesian network structure that best explains the data.
Chickering (1996) have showed that the structure learning problem of Bayesian
network is NP-hard.[9] Even with additional conditions such as having an inde-
pendence oracle or the number of parents limited to 2, the problem still remains
intractable.[10][19] Therefore, the eﬀorts have been focused on making the com-
putation feasible using various assumptions and constraints while attempting to
provide some guarantee of optimality.

Before going into the actual solutions, the ﬁrst thing we need to consider
is how we should deﬁne the best structure. Since Bayesian network is about
revealing conditional independencies in the domain, the best Bayesian network
should be able to identify as many dependencies as possible that are highly likely
in terms of probability.

Researchers formalised this notion as score-and-search, where we search through

the space of structure candidates with scores for each of them, and select the
highest scoring one. Section 3.1 explains how such score metric works. After ex-
plaining the score metric, our integer linear programming formulation of ﬁnding
the best Bayesian network structure is presented in section 3.2.

3.1 Score Metrics

By scoring a candidate BN structure, we are measuring a probability of the can-
didate being the BN structure representing joint probability distribution (JPD)

1http://www.openbugs.net
2https://www.norsys.com/netica.html

12

that our training data is sampled from. Following Bayes’ Theorem, we want to
calculate the posterior probability

13

P (Bh

S | D) = c × P (Bh

S) × P (D | Bh
S)

(3.1)

where Bh

S is a hypothesis that the candidate structure Bs represents the JPD
S) being the prior probability of
S) support D provides for the hypothesis, and c is a

that the dataset D was sampled from, P (Bh
the hypothesis, P (D | Bh
normalising constant.

For the priors of each candidate hypothesis, a uniform prior is usually as-
sumed, even though it might be possible to assign diﬀerent priors based on
expert knowledge.[25] The main problem here is how to measure the support
P (D | Bh
S) - more generally, how we can measure the goodness of ﬁt of the
candidate structure to the dataset. There are a number of score metrics based
on diﬀerent theoretical foundations, but this dissertation uses BDeu metric, the
special type of Bayesian Dirichlet-based metrics which their inner workings are
explained below:

3.1.1 Bayesian Dirichlet

Bayesian Dirichlet (BD)-based score metrics are Bayesian approaches to scoring
using Dirichlet distribution to calculate the posterior probability of the structure.
These score metrics have the following assumptions in common[7]:

Notation

• ΘG = {Θi} for i = 1, ..., n: Set of parameters of all the variables i = 1, ..., n

in Bayesian Network DAG G

• Θi = {Θij} for j = 1, ..., qi: Set of parameters of all the qi parent conﬁgu-

rations j = 1, ..., n for just one variable i

• Θij = {θijk} for k = 1, ..., ri: Set of parameters (physical probabilities) of i

taking each ri number of values, given one parent conﬁguration j

Assumptions

1. Multinomial Samples: Dataset D have multinomial samples with yet

unknown physical probabilities θijk.

14

2. Dirichlet Distribution: Set of physical probabilities in Θij = θij1, θij2, ..., θijri

follows Dirichlet distribution.

3. Parameter Independence: All the Θi are independent with each other
(global parameter independence), and all the Θij are independent with each
other as well (local parameter independence).

4. Parameter Modularity: Given two BN structures G and G(cid:48), if i have
same set of parents in both structures, then G and G have identical Θij.

Before going deeper, let’s brieﬂy go over the rationale behind using a Dirich-
If we were certain about the values of all the θijk, we could
let distribution.
simply just express them as a multinomial distribution. However, since we don’t
have such information, we estimate the behaviour of such distribution by using
Dirichlet distribution, which allows us to reﬂect our beliefs about each θijk using
corresponding parameters αijk, which are constructed before taking our data D
into account.

While it is theoretically possible to use a distribution other than Dirichlet, we
use them as it is algebraically straightforward to calculate posterior probabilities.
First, since Θij follows Dirichlet distribution, we know

p(Θij | Bh

S, α) =

Γ((cid:80)ri
k=1 αijk)
(cid:81)ri

k=1

×

ri(cid:89)

k=1

θαijk−1
x=k

, where α0 = (cid:80)
Also, expected value E(θijk) is αijk
α0
With multinomial sample D, We want to calculate

ijk αijk.

p(Θij | D, Bh

S, α) = c ×

ri(cid:89)

k=1

p(Θij | Bh

S, α) × θNijk

ijk

Since we have Equation 3.2, we can rewrite Equation 3.3 as

p(Θij | D, Bh

S, α) = c ×

ri(cid:89)

k=1

θ(αijk+Nijk)−1
ijk

(3.2)

(3.3)

(3.4)

where Nijk is a number of times ijk appeared in D.
So Equation 3.4 shows that the posterior distribution of Θij given D is also
a Dirichlet distribution. We say Dirichlet distribution is a conjugate prior to

15

multinomial samples, where both prior and posterior distributions are Dirichlet.
Also, we now have

E(θijk | D, Bh

S, α) =

αijk + Nijk
αij + Nij

, where Nij =

(cid:88)

ijk

Nijk

(3.5)

BD metric

With Equation 3.5, we can also calculate the probability of seeing a certain
combination of all the variable values Cm+1, which can be written as

p(Cm+1 | D, Bh

S, α) =

q
(cid:89)

qi(cid:89)

i=1

j=1

αijk + Nijk
αij + Nij

(3.6)

Equation 3.6 makes our original task of calculating the support of dataset
S, α) algebraically convenient. Let’s say our

to candidate hypothesis P (D | Bh
dataset D have m instances, then

P (D | Bh

S, α) =

m
(cid:89)

d=1

p(Cd | C1, ..., cd−1, Bh

S, α)

(3.7)

where each Cd represents an instance in D with certain combination of vari-

able values. Expanding 3.6, Equation 3.7 can be calculated by

P (D | Bh

S, α) =

n
(cid:89)

qi(cid:89)

(cid:26)(cid:20) αij1
αij

×

αij1 + 1
αij + 1

× ... ×

αij1 + (Nij1 − 1)
αij + (Nij1 − 1)

(cid:21)

αij2 + 1
αij + Nij1 + 1

× ... ×

αij2 + (Nij2 − 1)
αij + Nij1 + (Nij2 − 1)

(3.8)

(cid:21)

×

(cid:20)

i=1
(cid:20)

j=1
αij2
αij + Nij1
αijri
αij + (cid:80)ri−1
n
(cid:89)

qi(cid:89)

×

×

=

i=1

j=1

αijri + 1

×

k=1 Nijk
Γ(αij)
Γ(αij + Nij)

αij + (cid:80)ri−1
ri(cid:89)

k=1 Nijk + 1
Γ(αijk + Nijk)
Γ(αijk)

×

k=1

× ... ×

αijri + (Nijri − 1)
αij + (Nij − 1)

(cid:21)(cid:27)

Therefore, the probability of the hypothesis P (D | Bh

S, α) can be calculcated

using 3.8, and if we log-transform it we get

16

BD(B, D) = log(P (Bh

S | α)) +

n
(cid:88)

qi(cid:88)

(cid:18)

log

i=1

j=1

(cid:18) Γ(αij)

Γ(αij + Nij)

(cid:19)

+

ri(cid:88)

k=1

log

(cid:18) Γ(αijk + Nijk)
Γ(αijk)

(cid:19)(cid:19)

(3.9)

Equation 3.9 is called BD scoring function, introduced by Heckerman, Geiger

and Chickering.[26]

K2, BDe and BDeu

While BD metric is logically sound, it is practically unusable as we need to have
all the αijk in hand to calculate the total score. K2 metric in Equation 3.10 by
Cooper and Herskovits (1992)[13], which was actually developed before BD met-
ric, simply assigns αijk = 1 for all ijk, assuming uniform Dirichlet distribution
(not uniform distribution) on the prior.

K2(B, D) = log(P (B)) +

n
(cid:88)

qi(cid:88)

(cid:18)

log

i=1

j=1

(cid:18) (ri − 1)!

(Nij + ri − 1)!

(cid:19)

+

ri(cid:88)

k=1

(cid:19)

log(Nijk! )

(3.10)

BDe metric[26] attempts to reduce the number of αijk that needs to be spec-
iﬁed by introducing likelihood equivalence. Let’s suppose there’s a complete BN
structure G that speciﬁes all the ‘true’ conditional dependencies without any
missing edges, and our candidate structure Bh
S is Markov equivalent to it. Then
their parameters for Dirichlet distribution and hence their likelihood should be
the same as they represent same joint probability distribution. Then we can get
αijk = α(cid:48) × P (Xi = xik, (cid:81)
= wij | G), where α(cid:48) represent the level of belief
on the prior and wij is j-th conﬁguration of parents for i in G. In other words,
we only need αijk for the conﬁguration that are actually represented by edges in
BS.

Xi

= wij | G) = 1

BDeu metric[5] goes one step further from BDe by simply assuming that
P (Xi = xik, (cid:81)
, which assigns uniform probabilities to all
combinations of xi values and its parent conﬁgurations. This allows the user to
calculate the scores with limited prior knowledge, while maintaining the property
of likelihood equivalence which K2 metric cannot.

ri×qi

Xi

17

Decomposability of Score Metrics

As seen from Equation 3.9, these score metrics are log-transformed to provide
decomposability. Since the outermost sum is summed over each variable i, we
can say that BDeu score is a sum of local scores for each variable given their
parent node conﬁgurations. Therefore, we can say that we are trying to ﬁnd
the best structure with the highest score by choosing the parent nodes for each
variable that maximises their local scores. This goal is expressed as the objective
in Equation 3.11.

18

3.2

ILP Formulation

Based on the decomposable score metrics in section 3.1, we present integer linear
programming formulation of ﬁnding the best Bayesian network structure. One
of the early works on the ILP formulation have been done by Cussens[15] for
reconstructing pedigrees (‘family tree’) as a special type of BN.

The work independently done by Jaakkola et al.[27] provides proof that the
well known acylic subgraph polytope Pdag is not tight enough for the BN struc-
ture learning problem and presents tighter constraint of maintaining acyclicity
dubbed cluster constraint, along with the algorithm to approximate the tightened
polytope using duals.

Cussens incorporated the ﬁndings of [27] into branch-and-cut algorithm by
adding cluster constraints as cutting planes, along with general purpose cutting
planes and heuristics algorithm for speeding up the performance.[14][2]. This
dissertation largely follows the work of Barlett and Cussens[2], but with slightly
diﬀerent strategies for ﬁnding cutting planes and object-oriented implementation
of experiment program from scratch using Python language and Gurobi solver,
which is completely independent from Cussens’s computer program GOBNILP
written in C language[16]3.

3.2.1 Sets, Parameters, Variables

• v = node in BN

• W = parent set candidate for v

• c(v, W ) = local scores for v having W as parents

• I(W → v) = binary variable for W being selected for v.

3.2.2 Objective

maximise total score:

(cid:88)

v,W

c(v, W ) × I(W → v)

(3.11)

3https://www.cs.york.ac.uk/aig/sw/gobnilp/

19

3.2.3 Constraints

Only One Parent Set Constraint

∀v ∈ V :

(cid:88)

W

I(W → v) = 1

(3.12)

Acyclicity Constraint

Since Bayesian network takes a form of DAG, the constraint to ensure acyclicity is
the primary challenge of this ILP formulation. Acyclicity constraints on directed
acyclic graphs for use in linear programming have been studied quite extensively
as facets of acylic subgraph polytope Pdag. Typically, such constraints can be
expressed in the form

(cid:88)

(i,j)∈C

xij ≤ |C|−1

(3.13)

where xij is a binary variable for each directed edge (i, j) and C is any subset of
nodes.

While this constraint is obviously valid for Bayesian network structures as
well and actually used as cutting planes in section 4.2, this is not tight enough
for our model for two reasons. First, it is already known that 3.13 still allows
edge selections that are actually not valid except for some special cases of planar
DAG[22].

More importantly however, our binary variable selects a group of parent nodes
but not individual edges. This creates another problem as simplying having just
3.13 would allow a situation where the choice of parent nodes for one variable
gets divided over two candidate sets.

Proposition 3.2.1. There exists some cases of dicycles in Bayesian network G
that cannot be cut oﬀ by the constraint in Equation 3.13.

Proof. We prove by counterexample. Let’s suppose we have variable nodes N =
A, B, C. If the solver assigned 0.5 to the parent set choices (A | B, C) (B | A, C),
(C | A, B) and 0.5 to (A | ∅), (B | ∅), (C | ∅), then these still satisiﬁes 3.12 by
0.5+0.5 = 1 and 3.13 by 0.5+0.5+0.5 = 1.5 < 3 for each node, but this solution
does not represent any valid BN structure.

20

∀ cluster C ⊆ V :

(cid:88)

(cid:88)

v∈C

W ∩C=∅

I(W → v) ≥ 1

(3.14)

Equation 3.14 is a constraint developed by Jaakkola et al.[27] to overcome
this issue. The basic idea is that for every possible cluster (subset of nodes) in the
graph, there should be at least one node that does not have any of its parents
in the same cluster, or have no parents at all. This would be able to enforce
acyclicity as there would be no edges at all that points to such node within the
cluster (but might have nodes that start from this node to the other node in the
cluster). Since this constraint applies to clusters of every size in the graph, we
can see that this constraint is much tighter than 3.13.

Since both Equation 3.13 and Equation 3.14 would require exponential num-
ber of constraints if we add every possible cases of them at once into our model,
we instead add them as cutting planes if needed. We ﬁrst solve LP relaxation
without them, search for constraints that are violated by the current solution,
then add those to the model and solve again. The details of how we search for
these cutting planes are explained in chapter 4.

Chapter 4

Finding Solutions

Based on the formulation outlined in section 3.2, we present the actual process
developed to solve the problem. Due to the number of acyclicity constraints
that grows exponentially with respect to the number of variables, we do not add
them directly to the formulation as that will complicate the shape of our feasible
region and make the solution process diﬃcult, let alone the time of generating all
the constraint statements. Instead, we add them as cutting planes on demand
- we ﬁrst solve the problem without those constraints and search for only the
constraints that are actually violated by the current solution. In addition, we
considered some heuristic algorithm that takes advantage of the relaxed solution
to make our solving process faster.

4.1 Cluster Cuts

To add Equation 3.14 as cutting planes, we need to ﬁnd a cluster or set of nodes
that violates 3.14 under the solution yet without this constraint. Since cutting
planes allow us to tighten our feasible region and speed up the solution process,
we want to look for the cluster that violates the constraint the most, i.e. the
one that has the most members with their parents within the same cluster. To
express this idea formally, let’s ﬁrst see that Equation 3.14 can be rewritten as

∀C ⊆ V :

(cid:88)

(cid:88)

v∈C

W :|W ∩C|≥1

I(W → v) ≤ |C|−1

(4.1)

This is because we have Equation 3.12, where we should have exactly one parent
set choices for each node. There should be exactly |C| parent set choices, but
at least one of them should choose a parent set that is completely outside C.
Therefore, The number of I(W → v) that chooses W with one or more of its
members inside C (|W ∩ C|≥ 1) should be limited to |C|−1.

So we are looking for a cluster C that makes the LHS of 4.1 exceed the RHS
the most. Cussens[16] have suggested one way of achieving this by formulating

21

22

this as another small IP problem based on the current relaxed solution. For
each non-zero I(W → v) in current relaxed solution, we create corresponding
binary variable J(W → v). We also put these non-zero solutions as coeﬃcients
of J(W → v), denoted as x(W → v) below. Lastly, we create binary variables
M (v ∈ C) for all the nodes v, which will indicate that each node is chosen to be
included.

Objective: maximise

(cid:88)

v,W

Constraint:

x(W → v) × J(W → v) −

M (v ∈ C)

(4.2)

(cid:88)

v∈V

M (v ∈ C) = 1 for each J(W → v) = 1

(4.3)

M (w ∈ C) = 1 for at least one w ∈ W for each J(W → v) = 1

(4.4)

We are having x(W → v) as coeﬃcients here because we want to choose the
cluster that is supported by the current relaxed solution. That is, we want
the choice of I(W → v) to be the one that cuts out the feasible region the
most including the current relaxed solution, not just some arbitrary space in
the feasible region. We also have (cid:80)
v∈V M (v ∈ C) that represents the size of
the found cluster C. Since we are trying to get a single cluster that have more
I(W → v) > 0 than the total number of nodes, we want (cid:80) x(W → v) × J(W →
v) to exceed (cid:80)
v∈V M (v ∈ C) as much as possible. In addition, we are ruling out
any solution with the objective value ≤ −1 to avoid any unviolated clusters.

The two constraints are the representation of I(W → v) as node in the clus-
ter. For any cluster with I(W → v), then v must be inside such cluster. For
members of W , at least one of them should be in the same cluster. [16] have im-
plemented these two constraints by using their SCIP solver’s logicor functionality,
which is based on constraint programming. In more generic linear programming
convention, we rather express these by

∀J(W → v) : J(W → v) − M (v ∈ C) = 0

∀J(W → v) : J(W → v) ≤

(cid:88)

w∈W

M (w ∈ C)

(4.5)

(4.6)

where the ﬁrst constraint simply says M (v ∈ C) must be 1 if J(W → v) = 1, and
the second constraint forces at least one M (w ∈ C) to be 1 if J(W → v) = 1.

23

We pass this formulation to the solver to get the best cluster cuts possible.
When the solver returns the solution with the values of each M (v ∈ C), we now
have a cluster C to add to the main model. We add additional constraint as
expressed in Equation 4.1, but only for the particular cluster found by the above
model.

Given that this is a relatively simple ILP problem, we can obtain the cluster
cut in a very short amount of time in conjunction with a fast solver. While there
is an alternative approach that formulates this problem as an all-pairs shortest
path algorithm[27] based on the same ideas explained here, we chose to use ILP
solvers as it was more practical to achieve faster solving process with the solver
than trying to solve this problem with an algorithm written in Python.

4.2 Cycle Cuts

We add exactly one cluster cuts each time we solve the sub-IP problem. However,
it might be also helpful to rule out all the cycles that can be directly detected
in the current solution and tighten the feasible region. That is, we want to add
Equation 3.13 as cutting planes as well. In fact, all the cycles can be directly
converted as cluster cuts, since all the clusters simply refer to any set of nodes
with a limit on the number of edges. However, the converse is not true since
there might be cluster cuts that might not be shown as cycles, as proved in
Proposition 3.2.1.

In order to add cycle cuts, we ﬁrst need to ﬁnd all the existing cycles in the
current solution. There are several diﬀerent approaches to acheive this, but we
used the algorithm developed by Johnson[28], which is currently one of the best
known universally applicable version. After we get all the unique elementary
cycles, we go over each of them to get Equation 4.1 over the members of each
cycle.

4.3 Sink Finding Heuristic

With cluster cuts and cycle cuts, we now can obtain optimal bayesian networks
by solving the ILP model. However, depending on the size of dataset and the
number of variables, it might be too time consuming to wait until the program
reaches the optimal solution. Rather, we might want to get sub-optimal but
feasible solutions that might be reasonably close to the optimal one. This also
allows us to get a better lower bound on the problem and prevent excessive
branchings on the solver.

24

[16] have suggested an idea for a heuristic algorithm to acquire such solution,
which is based on the fact that every DAG has at least one sink node, or the
node that has no outgoing edge. Such sink node in a Bayesian network structure
would indicate that the variable would not directly inﬂuence others, and we can
freely choose a parent set for that variable node without creating any cycles.
Let’s suppose that we remove that node and its incoming edges. In the resulting
DAG, there should be another sink node to maintain acyclicity, which we can
If we keep following this rule and decide on
again choose a parent set for it.
all the variables we have, a feasible DAG is constructed. We illustrate this idea
graphically in Figure 4.1.

So we are essentially adding nodes one by one to construct a DAG. We want
to add them in an order that maximises the total score. During the ﬁrst time
we decide on the node to assign parents, we check the local scores of all the
parents and pick the highest scoring set for each node. We then look at the
current relaxed solution and rank these node/parent combinations based on their
closeness to 1. We assign 1 to the variable representing the combination that
are closest to 1. Since this node is a sink node, we must make sure that there’s
no other nodes where this node becomes a parent. We check all the parent
candidates of other nodes and assign 0 to all the variables representing parents
with this node.

All the following iterations would be identical as the ﬁrst except we rule out
all the parents that are already assigned 0 in previous iterations when picking
the highest scoring parent candidate. These procedures are fully outlined in
algorithm 2.

Figure 4.1: Graphical Illustration of Using Sink Nodes to Construct a DAG.

25

Algorithm 2: Sink Finding Heuristic Algorithm

Require: current solution
Ensure: nodes to decide ⇐ 0
heuristic BN ⇐ empty list
while nodes to decide > 0 do
best parents ⇐ empty list
Get all the local scores for this node sorted in descending order
for each sorted parent candidates W of this node do

if W not in heuristic BN then

best parent for this node ⇐ W
break

end if
end for
best parents ∪ best parent for this node
best distance ⇐ ∞
for chosen W ∈ best parents do

distance of W ⇐ (1.0 − current solution(W))
if distance of W < best distance then

best distance ⇐ distance of W

end if
end for
heuristic BN(best distance) ⇐ 1
for all other nodes do

heuristic BN(parent) ⇐ 0 for all parents with W

end for
for all other parents of this node do
heuristic BN(other parents) ⇐ 0

end for
end while
return heuristic BN

Chapter 5

Implementation and Experiments

Based on the ILP formulation with problem-speciﬁc cutting planes and heuristic
algorithm developed in chapter 3 and 4, we present the details of our computer
program implementation written in Python language with Gurobi solver and ex-
amine its performance on the reference datasets.

5.1

Implementation Details

Figure 5.1: Bayene Program Design

We created a Python package named Bayene (Bayesian network) that discovers
an optimal Bayesian network structure based on our ILP formulation given the
data input. Refer to Figure 5.1 for the overall structural design of Bayene. We
have solution controller that takes charge of controlling the overall solving process
and facilitating communication between the model writer and solver. Two model
writers, one for the main model and another for the cluster cut ﬁnding model,
transforms ILP problems as programming objects that can be transferred to the
solver and always modiﬁable through dedicated functions when needed. Model
writers and interfaces to solvers are written with Pyomo package[24]. In addition,
solution controller includes our own sink-ﬁnding heurisitic algorithm, and a cycle
detector based on the elementary cycle ﬁnding algorithm implementation from
NetworkX package[23].

26

27

Figure 5.2: Comparison of Diﬀerent ILP Solvers from SCIP website

GOBNILP makes an extensive use of APIs provided by the SCIP framework[1]:
SCIP leverages constraint programming (CP) techniques to solve integer pro-
gramming problems, making it one of the fastest non-commercial MIP solvers
in the world. Please see Figure 5.2 for the speed comparison between SCIP and
other existing MIP solvers.1 One thing we were curious about during the imple-
mentation was whether we could achieve even faster speeds by formulating our
problem in a more generic ILP manner and implementing simpler layers between
our program and more conventional but still faster solvers such as CPLEX and
Gurobi. Also, we wanted Bayene to be more ﬂexibly designed to allow easier
adaptation of future developments in our ILP formulation and portability across
diﬀerent solvers.

5.1.1 Notes on Branch-and-Cut

The biggest challenge we faced during the implementation was to modify the
solver’s branch-and-cut process to our needs. Since we do not specify all the
cluster constraints all at once when we transfer our model to the solver, we
add them as cutting planes whenever the current relaxed solution violates the
conditions. The main problem was that due to the limitations in Pyomo package,
we were allowed to add these constraints only when the solver ﬁnishes its Branch-
and-Cut process completely. This was problematic as this meant that the solver
would have to go over branch-and-bound tree to get the integer solution, only to
ﬁnd that it violates the cluster constraint and needs to cut oﬀ. Also, the solver
will have to completely restart the branch-and-cut tree from scratch, which would
add even more time to the solution process. We have tried adjusting several
parameters of the solver regarding Branch-and-Cut such as limiting the time
spent or number of nodes explored, but these tunings were eventually abandoned

1http://scip.zib.de/

28

as they fail to gurantee any optimality and often terminated the process too early
without returning any feasible solution.

Even with the direct interface to the solvers however, there are some issues
with the way the solvers handle user constraints. They distinguish two type of
cuts users can add to the model, one being user cuts and another being lazy
constraints. User cuts refer to the cutting planes that are implied by the model
but cannot be directly inferred by the solver. These constraints tightens the
feasible region of the LP relaxation but does not cut oﬀ any of the IP region. On
the other hand, lazy constraints are the ones that are actually required to get
the correct solution but cannot be added all at once because there are too many
of them or simply impossible to specify them all in the beginning.

Our cluster constraints fall into the second category - lazy constraints. The
problem is that these lazy constraints can only be added to the solver when they
reaches integer feasible solution, which can take signiﬁcantly more time than
checking lazy constraints at non-integer solution node. Reference manuals of
the solvers do not fully specify the reasoning behind this, but our guess is that
adding violated cutting planes at every nodes of BnB tree might complicate the
solution process too much with excessive branching.

We have eventually settled on the Pyomo-based implementation due to time
constraint. We also attempted implementing branch-and-cut outside the solver
and control the solution process by ourselves, but this showed to be extremely
diﬃcult as branchings occurred very often early in the process and number of
nodes on BnB tree grew rapidly beyond our memory control despite diﬀerent
branching strategies we tried.

Understanding various behaviours and techniques of ILP solvers and adjust-
ing them for the best performance on our speciﬁc problem requires more thorough
investigation on their own, and call for more attention in the future research.

5.2 Experiments

We experimented with Bayene to see how they perform in practice using ref-
erence datasets of diﬀerent conditions. We also examined how turning on and
oﬀ Bayene’s diﬀerent features - cycle cuts, sink-ﬁnding heuristic, and gomory
fractional cuts - changes its behaviour or performance.

5.2.1 Setup

• We used pre-calculated BDeu local score ﬁles provided by Cussens, avail-
able on his GOBNILP website. These score ﬁles are based on the reference

29

datasets used for benchmarks in past Bayesian Network literatures, which
the original versions can be obtained from the website Bayesian Network
Repository by Marco Scutari2 with the orignal source information.

• Each dataset was sampled to have 100, 1000, and 10000 instances, and two
scores ﬁles were created for every dataset, with parent set size limit of 2
and 3 respectively.

• Cussens[16] have indicated that some of the parent candidate sets have

been pruned using the methods published by de Campos and Ji.[6]

• Our benchmark have been conducted on Apple Mid 2012 MacBook Air
machine with Intel Core i5 3317U 1.7Ghz, 4GB RAM, and OS X 10.11
operating system.

• While Bayene works on any ILP solver Pyomo supports, we used Gurobi as
it was one of the fastest commercial solvers without the problem size limit
on academic use.

• We turned oﬀ all the general purpose MIP cuts by setting Gurobi parameter
Cuts to 0, except for Gomory fractional cuts by setting GomoryPasses to
20000000 (unlimited).

5.2.2 Experiment 1: All Features Turned On

For the ﬁrst experiment, we applied both cluster cuts and cycle cuts, along with
Gomory fractional cuts by the solver. Please see Table 5.1 and Table 5.2 for the
detailed results. We were able to solve most of the ILP problems within 1-hour
limit, ranging from less than a second for asia dataset (8 attributes, 118 ILP
variables) to over 20 minutes for alarm (37 attributes, 2736 ILP variables).

Please see Figure 5.3a for example BN structure generated by Bayene for the
water dataset. Figure 5.3b shows how objective values have progressed during
the solution process. Each dots are plotted whenever we add cluster cuts and the
solver returns the current solution. ILP objectives are shown as blue dots, and
sink heuristic objectives as red dotted lines. Note that sink heuristic objective
value do not get changed unless we get bigger objective value.

In the beginning without most of cluster and cycle constraints, we begin with
a quite high objective value, but falls rapidly on the next two iterations. The
objective value then changes very little until we reach the optimum solution.

2http://www.bnlearn.com/bnrepository/

30

(a) Resulting BN structure

(b) Progression of Objective Values

Figure 5.3: Results from insurance with 1000 instances, parent size limit = 2.

Interestingly enough, the objective value of sink heuristic reaches near the range
of the actual optimum in the very beginning and do not change until the end.

Although cluster cuts and cycle cuts have been invoked same number of
times for almost all cases, the total number of cycles that have been ruled out
through cycle cuts exceeds the number of cluster cuts by a huge margin. These
large number of cuts allow the solver to reach the range of valid solutions more
quickly and eventually the optimum.

5.2.3 Experiment 2: Without Cycle Cuts

(a) insurance with 1000 instances

(b) water with 100 instances

Figure 5.4: Progression of Objective Values from Diﬀerent Datasets in the Second
Experiment. Both problems did not reach optimal solution within the time limit.

For the second experiment, we applied just the cluster cuts and kept the
Gomory cuts. Please see Table 5.3 and Table 5.4 for the detailed results. We were

31

not able to solve most of the problems within the time limit, as the objective value
progressed really slowly as seen from Figure 5.4a and Figure 5.4b. Moreover, we
observed that the speed of the solver started to slow down over each iteration,
while still having the wide gap between the sink heuristic objective value.
It
seems that adding few cluster cuts already complicates the shape of the feasible
region heavily, while they don’t cut enough to reach the area with valid solutions.
Things get worse since we are restarting the branch-and-bound tree every time
we add cluster cuts. We could see that cycle cuts we add by detecting all the
elementary cycles serve a signiﬁcant role in getting the optimal solutions in a
reasonable amount of time.

5.2.4 Experiment 3: Without Gomory Cuts

(a) Resulting BN structure

(b) Progression of Objective Values

Figure 5.5: Results from water with 10000 instances, parent size limit = 3.

For the third experiment, we applied both cluster cuts and cycle cuts, but
completely disabled Gomory frational cuts from the solver. Please see Table 5.5
and Table 5.6 for the detailed results. While the third experiment was able to
solve most of the problems as the ﬁrst experiment did, disabling Gomory cuts
improved the solution time signiﬁcantly in many cases, especially the hardest
ones in the ﬁrst experiment where it took 22 minutes to solve alarm with 10000
instances but 12 minutes in the third experiment. Instead, a little more cluster
and cycle cuts were added to solve the problems than the ﬁrst experiment. This
implies that general purpose cutting planes like Gomory cuts that are used to
tighten the bound on BnB tree can be countereﬀective in our cases. Patterns of
objective value changes were not diﬀerent from the ﬁrst experiment.

Title
asia
asia
asia
insurance
insurance
insurance
water
water
water
alarm
alarm
alarm
hailﬁnder
hailﬁnder
hailﬁnder
carpo
carpo
carpo

Title
asia
asia
asia
insurance
insurance
insurance
water
water
water
alarm
alarm
alarm
hailﬁnder
hailﬁnder
hailﬁnder
carpo
carpo
carpo

# Attributes # Instances # ILP Variables BDeu Score
−245.644264
41
8
−2317.411506
88
8
−22466.396546
118
8
−1687.683853
266
27
−13892.798172
702
27
−133111.964488
2082
27
−1501.644722
356
32
−13263.115737
507
32
−128810.974528
813
32
−1362.995568
591
37
−11248.39992
1309
37
−105486.499123
2736
37
−6021.269394
214
56
−52473.926982
671
56
−498383.409915
2260
56
−−
2139
60
−−
2208
60
−−
4354
60

100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000

Time Elapsed (in sec) # Cluster Cut Iterations # Cycle Cut Iterations # Cycle Cut Count
0.1605529785
0.493724823
0.5763838291
1.1246800423
31.290997982
531.714365005
3.298566103
5.2235310078
10.9444692135
33.3234539032
258.938903093
1331.78003311
1.3409891129
10.9388239384
942.131913185
−−
−−
−−

3
8
8
9
20
28
17
16
16
23
46
44
10
27
68
−−
−−
−−

3
8
8
9
20
28
17
16
16
23
46
44
10
27
69
−−
−−
−−

11
25
28
59
295
780
147
227
248
446
532
885
63
194
639
−−
−−
−−

Table 5.1: Experiment 1 with parent set size limit = 2. ‘−−’ indicates that the problem was not solved within 1 hour.

# Attributes # Instances # ILP Variables BDeu Score
−245.644264
41
8
−2317.411506
107
8
−22466.396548
161
8
−1686.225878
279
27
−13887.350147
774
27
−−
3652
27
−1500.988471
482
32
−13262.367639
573
32
−128705.656236
961
32
−1349.227422
907
37
−11240.347094
1928
37
−−
2736
37
−6019.469926
244
56
−52473.24561
761
56
−−
3768
56
−−
2139
60
−−
2208
60
−−
4354
60

100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000

Time Elapsed (in sec) # Cluster Cut Iterations # Cycle Cut Iterations # Cycle Cut Count
0.2108130455
0.4743950367
1.2923400402
1.3009831905
30.3841409683
−−
8.4975321293
4.0370209217
29.74208498
132.99851799
947.912580967
−−
1.100315094
21.7998039722
−−
−−
−−
−−

4
7
10
10
17
−−
15
12
16
19
47
−−
7
22
−−
−−
−−
−−

11
27
41
66
269
−−
228
210
399
871
841
−−
78
247
−−
−−
−−
−−

4
7
10
10
17
−−
15
12
16
19
47
−−
7
22
−−
−−
−−
−−

Table 5.2: Experiment 1 with parent set size limit = 3. ‘−−’ indicates that the problem was not solved within 1 hour.

Title
asia
asia
asia
insurance
insurance
insurance
water
water
water
alarm
alarm
alarm
hailﬁnder
hailﬁnder
hailﬁnder
carpo
carpo
carpo

Title
asia
asia
asia
insurance
insurance
insurance
water
water
water
alarm
alarm
alarm
hailﬁnder
hailﬁnder
hailﬁnder
carpo
carpo
carpo

# Attributes # Instances # ILP Variables BDeu Score
−245.644264
41
8
−2317.411506
88
8
−22466.396546
118
8
−−
266
27
−−
702
27
−−
2082
27
−−
356
32
−−
507
32
−−
813
32
−−
591
37
−−
1309
37
−−
2736
37
−−
214
56
−−
671
56
−−
2260
56
−−
2139
60
−−
2208
60
−−
4354
60

100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000

Time Elapsed (in sec) # Cluster Cut Iterations # Cycle Cut Iterations # Cycle Cut Count
1.2105300427
3.3687419891
4.5231909752
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−

N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A

N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A

30
60
64
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−

Table 5.3: Experiment 2 with parent set size limit = 2. ‘−−’ indicates that the problem was not solved within 1 hour.

# Attributes # Instances # ILP Variables BDeu Score
−245.644264
41
8
−2317.411506
107
8
−22466.396547
161
8
−−
279
27
−−
774
27
−−
3652
27
−−
482
32
−−
573
32
−−
961
32
−−
907
37
−−
1928
37
−−
2736
37
−−
244
56
−−
761
56
−−
3768
56
−−
5068
60
−−
3827
60
−−
16391
60

100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000

Time Elapsed (in sec) # Cluster Cut Iterations # Cycle Cut Iterations # Cycle Cut Count
1.2677178383
2.9705760479
5.4340219498
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−

N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A

N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A
N A

29
50
59
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−

Table 5.4: Experiment 2 with parent set size limit = 3. ‘−−’ indicates that the problem was not solved within 1 hour.

Title
asia
asia
asia
insurance
insurance
insurance
water
water
water
alarm
alarm
alarm
hailﬁnder
hailﬁnder
hailﬁnder
carpo
carpo
carpo

Title
asia
asia
asia
insurance
insurance
insurance
water
water
water
alarm
alarm
alarm
hailﬁnder
hailﬁnder
hailﬁnder
carpo
carpo
carpo

# Attributes # Instances # ILP Variables BDeu Score
−245.644264
41
8
−2317.411506
88
8
−22466.396546
118
8
−1687.683853
266
27
−13892.798172
702
27
−133111.964488
2082
27
−1501.644722
356
32
−13263.115737
507
32
−128810.974528
813
32
−1362.995568
591
37
−11248.39992
1309
37
−105486.499123
2736
37
−6021.269394
214
56
−52473.926982
671
56
−−
2260
56
−−
2139
60
−−
2208
60
−−
4354
60

100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000

Time Elapsed (in sec) # Cluster Cut Iterations # Cycle Cut Iterations # Cycle Cut Count
0.229927063
0.5296328068
0.6273970604
1.0839219093
26.9869270325
319.534489155
6.8530170918
4.921047926
48.8902380466
28.4560148716
173.517156839
721.979949951
1.3437559605
9.6990509033
−−
−−
−−
−−

4
8
8
9
21
28
20
16
19
23
47
45
10
24
−−
−−
−−
−−

4
8
8
9
21
28
20
16
19
23
47
45
10
24
−−
−−
−−
−−

11
25
28
59
318
776
183
186
981
434
548
869
63
191
−−
−−
−−
−−

Table 5.5: Experiment 3 with parent set size limit = 2. ‘−−’ indicates that the problem was not solved within 1 hour.

# Attributes # Instances # ILP Variables BDeu Score
−245.644264
41
8
−2317.411506
107
8
−22466.396546
161
8
−1686.225878
279
27
−13887.350147
774
27
−−
3652
27
−1500.968391
482
32
−13262.465272
573
32
−128705.731312
961
32
−1349.227422
907
37
−11240.347094
1928
37
−−
6473
37
−6019.469926
244
56
−52473.24561
761
56
−−
3768
56
−−
5068
60
−−
3827
60
−−
16391
60

100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000
100
1000
10000

Time Elapsed (in sec) # Cluster Cut Iterations # Cycle Cut Iterations # Cycle Cut Count
0.2282519341
0.5901920795
1.231169939
1.3578009605
20.3226950169
−−
8.6199500561
7.5233428478
28.2011299133
121.306695938
754.878758907
−−
1.1171729565
14.2832479477
−−
−−
−−
−−

4
7
10
10
17
−−
13
13
13
19
44
−−
7
22
−−
−−
−−
−−

11
27
41
66
269
−−
206
227
456
871
854
−−
78
258
−−
−−
−−
−−

4
7
10
10
17
−−
13
13
13
19
44
−−
7
22
−−
−−
−−
−−

Table 5.6: Experiment 3 with parent set size limit = 3. ‘−−’ indicates that the problem was not solved within 1 hour.

5.2.5 Performance of Sink Finding Heuristic

35

Figure 5.6: Percentage diﬀerence between best heuristic objective at each itera-
tion and the ﬁnal optimal objective value, on insurance dataset of 1000 instances
and parent size limit = 3.

One interesting aspect of our sink ﬁnding heuristic is the proximity of its
output to the ﬁnal optimal solution. For the case of insurance dataset as seen
on Figure 5.6, the heuristic algorithm was able to produce a solution with an
objective value that diﬀered around 6% from the ﬁnal optimal objective value,
even for the ﬁrst iteration. It soon reached around 1% in next few iterations.
While we only have limited knowledge about the shape of polytopes for BN
structures, we can see that our sink ﬁnding heuristic can reach the vicinity of
the optimal solution quite well. Although Bayene sends the solutions generated
by the heuristic to the solver for warm starts, it seems that the solver does not
actually make much use of them since the solutions for the relaxed problem yet
without all the necessary cuts are bigger than the sink ﬁnding heuristic solution.
Further study of BN structure polytopes and solutions generated by the sink
ﬁnding heuristic would help us to get optimal solutions faster.

Chapter 6

Conclusion

This dissertation reviewed the conceptual foundation behind Bayesian network
and studied formulating the problem of learning the BN structure from data
as an integer linear programming problem. We went over the inner workings
of score metrics used to measure the statistical ﬁt of the BN structure to the
dataset, and presented the ILP formulation based on the decomposability of the
score metric. In order to deal with the exponential number of constraints, we
investigated diﬀerent ways to add constraints to the model on the ﬂy as cutting
planes rather than fully specifying them initially.

We implemented the ILP formulation and cutting planes as a computer soft-
ware and conducted a benchmark on various reference datasets. We saw that
ruling out all the cycles found in the solution at each iteration is critical to
reaching the optimality in a reasonable amount of time. We also found out that
general purpose cutting planes such as Gomory fractional cuts that are used to
tighten the bound on BnB tree can backﬁre in some cases such as ours. Lastly,
we discovered that our sink ﬁnding heuristic algorithm returns solutions that are
quite close to the ﬁnal optimal solution very early on in the process.

6.1 Future Directions

This section will present some of the ideas for the further development of Bayene
its statistical modelling capabilities and mathematical opti-
from two aspects:
misation techniques employed.

6.1.1 Statistical Modelling

Alternate Scoring Functions

For this dissertation, we focused on using Bayesian Dirichlet-based score met-
ric, which is based on the assumption of multinomial data and Dirichlet prior.
We used BDeu score metric, which adds additional assumptions of likelihood
equivalence and uniform prior probabilities. In addition to Dirichlet-based score

36

37

metric, there are a number of diﬀerent information theoretic score functions
such as MDL and BIC used in BN literatures. Also, there have been some recent
development on score metrics such as SparsityBoost by Brenner and Sontag[4]
that reduces computational burden and attempts to incorporate aspects of con-
ditional independence testing. Understanding diﬀerences between these score
metrics would be important in making the eﬀectiveness of our Bayesian network
structure learning as a statistical model.

Other Types of Statistical Distribution

Adding on to alternate score metrics, versatility of Bayesian network and its
structure learning can be expanded by making it applicable to diﬀerent types of
distribution. There already have been done for learning BN structure on con-
tinous distribution, but mostly based on conditional independence information.
Figuring out ways to allow more types of distribution, especially for the ILP
formulation, would be an interesting and worthwhile challenge.

6.1.2 Optimisation

Leveraging the Graph Structure

While we did not directly make much use of the fact that the structure of
Bayesian network is DAG, there were few researches that did in the last few
years, including the one by Studeny et al.[32] that introduced the concept of char-
acteristic imset, which stems from the property of Markov equivalent structures
described in subsection 2.1.2, and another with a set of additional treewidth con-
straints by Parviainen et al.[30]. Empirical results on these approaches showed
to be signiﬁcantly slower than ours, but it would be interesting to go further
with these leads from combinatorial optimisation perspective.

Advanced Modelling Techniques

We benchmarked on the pre-calculated local score ﬁles that have parent set
size limit of either 2 or 3. While our formulation theoretically works on bigger
parent set sizes, we currently haven’t employed more advanced techniques such
as column generation that could make it possible to deal with extremely large
number of variables. Further development of incorporating such techniques to
Bayene would allow us to handle larger datasets.

38

Alternate Optimisation Paradigm

Aside from the integer linear programming, there have been eﬀorts to use alter-
nate optimsation scheme such as constraint programming.[3] While they were
not decisively better than our ILP approach, they did show some promising re-
sults. It would be worthwhile to examine the inner workings of their approach
in order to improve our formulation.

Deeper Integration with Branch-and-Cut

As seen from section 5.1, we had some issues with adjusting the solver’s branch-
and-cut algorithm to our needs, as there were complications resulting from var-
ious techniques and restrictions involved with the solver programs. In order to
make Bayene suitable for more learning tasks, thorough inspection of how the ILP
solvers perform optimisation would be needed to prevent ineﬃcent operations.

Appendix A

Software Instructions

Bayene can be downloaded from the following link: https://link.iamblogger.
net/4khv1. Bayene itself is not a standalone program but rather a Python library,
so the user needs to inherit the class from the package to his or her application.
For evaluation purposes, we provide a test script ﬁle sample˙script.py that allows
the user to testdrive Bayene.

There’s no formal install functionality yet on Bayene, so the user needs to
install all the dependencies manually. Bayene was written for CPython 2.7 series,
and will not work on any other implementation of Python such as Python 3 or
PyPy. Please download the appropriate version of Python 2.7 for your platform
from https://www.python.org/downloads/.

If your system does not have pip, please refer to https://pip.pypa.io/en/
latest/installing.html for install instructions. After installing pip, please
turn on Command Prompt on Windows or Terminal on OS X or Linux with admin-
istrator or root access and type pip install pyomo numpy scipy, which will install
In addition, you need to install gurobipy
all the required libaries for Bayene.
package included with the installation of Gurobi solver, which the instructions
are provided in http://www.gurobi.com/documentation/.

After installing all the dependencies, please open sample˙script.py with a plain
text editor. Please edit the string on line 12 to specify the score ﬁles that
needs to be tested. Please note that all the score ﬁles used for this disserta-
tion is available in http://www-users.cs.york.ac.uk/~jc/research/uai11/
ua11_scores.tgz.

Lastly, go back to Command Prompt or Terminal, navigate to the directory
where sample˙script.py is located and type python sample˙script.py. Please refer to
the source code for further information.

39

Bibliography

[1] Tobias Achterberg et al. “Constraint integer programming: A new approach
to integrate CP and MIP”. In: Integration of AI and OR techniques in
constraint programming for combinatorial optimization problems. Springer,
2008, pp. 6–20.

[2] Mark Bartlett and James Cussens. “Integer Linear Programming for the
Bayesian network structure learning problem”. In: Artiﬁcial Intelligence
(2015).

[3] Peter van Beek and Hella-Franziska Hoﬀmann. “Machine learning of Bayesian

networks using constraint programming”. In: ().

[4] Eliot Brenner and David Sontag. “SparsityBoost: A New Scoring Func-
tion for Learning Bayesian Network Structure”. In: CoRR abs/1309.6820
(2013). url: http://arxiv.org/abs/1309.6820.

[5] Wray Buntine. “Theory reﬁnement on Bayesian networks”. In: Proceedings
of the Seventh conference on Uncertainty in Artiﬁcial Intelligence. Morgan
Kaufmann Publishers Inc. 1991, pp. 52–60.

[6] Cassio Polpo de Campos and Qiang Ji. “Properties of Bayesian Dirichlet
Scores to Learn Bayesian Network Structures.” In: AAAI. 2010, pp. 431–
436.

[7] Alexandra M Carvalho. “Scoring functions for learning Bayesian networks”.

In: ().

[8] Eugene Charniak. “Bayesian networks without tears.” In: AI magazine 12.4

(1991), p. 50.

[9] David Maxwell Chickering. “Learning Bayesian networks is NP-complete”.

In: Learning from data. Springer, 1996, pp. 121–130.

[10] David Maxwell Chickering, David Heckerman, and Christopher Meek. “Large-

sample learning of Bayesian networks is NP-hard”. In: The Journal of Ma-
chine Learning Research 5 (2004), pp. 1287–1330.

[11] Jens Clausen. “Branch and bound algorithms-principles and examples”.
In: Department of Computer Science, University of Copenhagen (1999),
pp. 1–30.

BIBLIOGRAPHY

[12] M. Conforti, G. Cornuejols, and G. Zambelli. Integer Programming. Grad-
uate Texts in Mathematics. Springer International Publishing, 2014. isbn:
9783319110080.

[13] Gregory F Cooper and Edward Herskovits. “A Bayesian method for the
induction of probabilistic networks from data”. In: Machine learning 9.4
(1992), pp. 309–347.

[14] James Cussens. “Bayesian network learning with cutting planes”. In: arXiv

preprint arXiv:1202.3713 (2012).

[15] James Cussens. “Maximum likelihood pedigree reconstruction using integer

programming”. In: Proceedings of WCB 2010 (), p. 9.

[16] James Cussens and Mark Bartlett. “GOBNILP 1.6.1 User/Developer Man-

ual”. In: (2015).

[17] James Cussens, Brandon Malone, and Changhe Yuan. “Tutorial on Opti-

mal Algorithms for Learning Bayesian Networks”. In: ().

[18] R´on´an Daly, Qiang Shen, and Stuart Aitken. “Learning Bayesian net-
works: approaches and issues”. In: The Knowledge Engineering Review
26.02 (2011), pp. 99–157.

[19] Sanjoy Dasgupta. “Learning polytrees”. In: Proceedings of the Fifteenth
conference on Uncertainty in artiﬁcial intelligence. Morgan Kaufmann Pub-
lishers Inc. 1999, pp. 134–141.

[20] Dan Geiger, Tom S Verma, and Judea Pearl. “d-separation: From theorems

to algorithms”. In: arXiv preprint arXiv:1304.1505 (2013).

[21] Ralph E Gomory. “An algorithm for integer solutions to linear programs”.
In: Recent advances in mathematical programming 64 (1963), pp. 260–302.

[22] Martin Gr¨otschel, Michael J¨unger, and Gerhard Reinelt. “On the acyclic
subgraph polytope”. In: Mathematical Programming 33.1 (1985), pp. 28–
42.

[23] Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. “Exploring network
structure, dynamics, and function using NetworkX”. In: Proceedings of the
7th Python in Science Conference (SciPy2008). Pasadena, CA USA, Aug.
2008, pp. 11–15.

[24] William E Hart, Jean-Paul Watson, and David L Woodruﬀ. “Pyomo: mod-
eling and solving mathematical programs in Python”. In: Mathematical
Programming Computation 3.3 (2011), pp. 219–260.

[25] David Heckerman and Dan Geiger. Likelihoods and parameter priors for

Bayesian networks. Tech. rep. Citeseer, 1995.

BIBLIOGRAPHY

[26] David Heckerman, Dan Geiger, and David M Chickering. “Learning Bayesian

networks: The combination of knowledge and statistical data”. In: Machine
learning 20.3 (1995), pp. 197–243.

[27] Tommi Jaakkola et al. “Learning Bayesian network structure using LP
relaxations”. In: International Conference on Artiﬁcial Intelligence and
Statistics. 2010, pp. 358–365.

[28] Donald B Johnson. “Finding all the elementary circuits of a directed graph”.

In: SIAM Journal on Computing 4.1 (1975), pp. 77–84.

[29] Ailsa H Land and Alison G Doig. “An automatic method of solving discrete
programming problems”. In: Econometrica: Journal of the Econometric
Society (1960), pp. 497–520.

[30] Pekka Parviainen, Hossein Shahrabi Farahani, and Jens Lagergren. “Learn-
ing bounded tree-width Bayesian networks using integer linear program-
ming”. In: Proc. 17th Int. Conf. on AI and Stat. 2014, pp. 751–759.

[31] Judea Pearl. d-SEPARATION WITHOUT TEARS (At the request of many
readers). url: http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html.

[32] Milan Studen´y and David Haws. “Learning Bayesian network structure:
Towards the essential graph by integer linear programming tools”. In: In-
ternational Journal of Approximate Reasoning 55.4 (2014), pp. 1043–1071.

[33] Thomas Verma and Judea Pearl. “Equivalence and synthesis of causal
models”. In: Proceedings of the Sixth Annual Conference on Uncertainty
in Artiﬁcial Intelligence. Elsevier Science Inc. 1990, pp. 255–270.

[34] H Paul Williams. “Model building in mathematical programming”. In:

(1999).

[35] Harry Zhang. “The optimality of naive Bayes”. In: AA 1.2 (2004), p. 3.

