0
2
0
2

v
o
N
0
3

]

C
H
.
s
c
[

1
v
0
5
0
5
1
.
1
1
0
2
:
v
i
X
r
a

A Novice-Reviewer Experiment to
Address Scarcity of Qualiﬁed Reviewers in Large Conferences

Ivan Stelmakh♠, Nihar B. Shah♠, Aarti Singh♠ and Hal Daumé III♥♦

♠School of Computer Science, Carnegie Mellon University
♥University of Maryland, College Park
♦Microsoft Research, New York
{stiv,nihars,aarti}@cs.cmu.edu, me@hal3.name

Abstract

Conference peer review constitutes a human-computation process whose importance cannot be over-
stated: not only it identiﬁes the best submissions for acceptance, but, ultimately, it impacts the future
of the whole research area by promoting some ideas and restraining others. A surge in the number of
submissions received by leading AI conferences has challenged the sustainability of the review process by
increasing the burden on the pool of qualiﬁed reviewers which is growing at a much slower rate. In this
work, we consider the problem of reviewer recruiting with a focus on the scarcity of qualiﬁed reviewers
in large conferences. Speciﬁcally, we design a procedure for (i) recruiting reviewers from the population
not typically covered by major conferences and (ii) guiding them through the reviewing pipeline.
In
conjunction with ICML 2020 — a large, top-tier machine learning conference — we recruit a small set of
reviewers through our procedure and compare their performance with the general population of ICML re-
viewers. Our experiment reveals that a combination of the recruiting and guiding mechanisms allows for
a principled enhancement of the reviewer pool and results in reviews of superior quality compared to the
conventional pool of reviews as evaluated by senior members of the program committee (meta-reviewers).

1

Introduction

Over the last few years, Machine Learning (ML) and Artiﬁcial Intelligence (AI) conferences have been
experiencing rapid growth in the number of submissions: for example, the number of submissions to AAAI
and NeurIPS — popular AI and ML conferences — more than quadrupled in the last ﬁve years. The
explosion in the number of submissions has challenged the sustainability of the peer-review process as the
number of qualiﬁed reviewers is growing at a much slower rate (Sculley et al., 2019; Shah, 2019b). While
especially prominent in ML and AI, the problem is present in many other ﬁelds where “submissions are up,
reviewers are overtaxed, and authors are lodging complaint after complaint” (McCook, 2006).

The disparity between growth rates of the submission and reviewer pools increases the burden on the
reviewers, thereby putting a severe strain on the review process. According to the president of the Interna-
tional Conference on Machine Learning (ICML) board John Langford (Langford, 2018), “There is signiﬁcant
evidence that the process of reviewing papers in machine learning is creaking under several years of exponen-
tiating growth.” Hence, it is important to increase the number of qualiﬁed reviewers in the system to keep
up with the growing number of submissions.

When the size of a conference is small, program chairs can extend the pool of reviewers by manually
selecting new reviewers among researchers who have enough expertise in the area. The selection can be guided
by the program chairs’ understanding of who might be a good reviewer or by personal recommendations
made by other senior members of the program committee. In what follows, we refer to the pool of reviewers
manually constructed by the program chairs as the curated pool. However, with a massive increase in

1

 
 
 
 
 
 
the scale of the conference, such a manual addition to the curated pool does not allow bringing in enough
reviewers to cover the demand of the conference. The program chairs must then rely on alternative ways of
reviewer recruiting.

With this motivation in mind, in the present paper we aim to design and evaluate modiﬁcations to the

reviewer recruiting process that simultaneously address two challenges:

• Challenge 1. To avoid overloading reviewers, conferences need to ﬁnd new sources of reviewers as there

are not enough curated reviewers to review all papers.

• Challenge 2. Conferences need to ensure that newly added reviewers do not compromise the quality of
the process, that is, are able to write reviews of quality at least comparable to the curated reviewer pool.

In the past, conference organizers have been trying to expand the reviewer pool by relaxing the qualiﬁ-
cation bar, that is, by allowing researchers who meet some minimal requirements such as having one or two
relevant publications to join the pool of reviewers without further screening. For example, 1176 out of 3242
(that is, 36%) of the reviewers in the NeurIPS 2016 conference were recruited by requesting authors of each
submission to name at least one author who is willing to become a reviewer, and 70% of these reviewers were
PhD students: researchers at very early stages of their careers (Shah et al., 2018). Such practices have now
become conventional and are adopted by many other conferences, including a ﬂagship conference in artiﬁcial
intelligence AAAI that in 2020 invited self-nominated individuals with publication history in top venues,
and in 2021 requires authors of submissions to be willing to become reviewers on request. Similarly, ICML
2020 — a ﬂagship ML conference — distributed a public call for reviewers and accepted self-nominated
individuals with publication and reviewing history in top venues.

While the aforementioned innovations allow to enlarge the reviewer pool, little scientiﬁc evidence exists
on the quality of reviews written by reviewers recruited through these novel procedures. NeurIPS 2016
compared the reviews written by curated reviewers with reviews sourced from authors of submissions in
terms of numeric scores (overall score and several criteria scores) and inter-reviewer agreement (Shah et al.,
2018). The analysis did not reveal a signiﬁcant diﬀerence between populations, only showing that author-
sourced reviews were slightly harsher in scoring the clarity of submissions. However, we note that this analysis
only operates with scores given by reviewers and does not address the quality of reviews — perhaps the most
important metric for success of the conference peer-review process — which is largely determined by the
textual part of the review. Other works provide anecdotal and empirical evidence that junior reviewers are
more critical than their senior counterparts (Mogul, 2013; Toor, 2009; Tomiyama, 2007) and that “graduate
students seem to be unable to provide very useful comments” (Patat et al., 2019). Thus, while the methods
employed by leading conferences address the ﬁrst challenge, it remains unclear if and how they address the
second challenge of high quality reviewing.1

In this work, in conjunction with the review process of ICML 2020 we conduct a threefold experiment:
• First, we recruit reviewers from the population not typically covered by the reviewer-selection process
of major conferences. In that, we target the population of very junior researchers with limited or no
publication/reviewing history most of whom do not pass the recruiting ﬁlters of ICML. Conceptually, in
contrast to the standard approach of selecting reviewers based on some proxy towards reviewing ability
(e.g., prior publication and reviewing history), we evaluate candidates’ abilities to review in an auxiliary
peer-review process organized for the experiment.

• Second, we add a select set of reviewers recruited through our experiment to the reviewer pool of the

ICML conference and guide them through the peer-review process by oﬀering mentoring.

• Finally, we evaluate the performance of these novice reviewers by comparing them with the general
population of the ICML reviewer pool on multiple aspects. In doing so, we augment the past analysis
of Shah et al. (2018) by using an explicit measure of review quality (evaluated by meta-reviewers) in
addition to indirect proxies.

1After the experiment described in this paper was completed, the NeurIPS 2020 conference released an analysis of their
review process (Lin et al., 2020) which compared quality of reviews sourced from authors of submissions and reviews written
by the curated pool of reviewers. In Section 4 we compare the results presented therein with those of the present study.

2

An important aspect of our experiment is that most of the reviewers brought to the reviewer pool through
our experiment would not have been considered in standard ways of recruiting. Hence, our experiment oﬀers
a principled way to enlarge the reviewer pool. As a by-product, the new pool of reviewers contributes to
diversity of peer review resonating with the virtues such as increased scrutiny and variety of opinions out-
lined by Garisto (2019). Moreover, we oﬀer the new reviewers a more guided introduction to the reviewing
process which is known to help novice reviewers to write better reviews (Patat et al., 2019) and improve
their own writing skills (Kerzendorf et al., 2020). From the perspective of training reviewers, our experi-
ment is conceptually similar to the initiative of Journal of Neuroscience (Picciotto, 2018) and SIGCOMM
conference (Feldmann, 2005) that attempt to help novices in becoming reviewers.

This work falls in the line of empirical works that study various behavioral aspects of human com-
putation, including motivational aspect (Kaufmann et al., 2011) and the impact of the task framing on
performance (Kotturi et al., 2020; Levy and Sarne, 2018; Chandler and Kapelner, 2013). The ﬁndings we
report in this paper can be combined with insights from the aforementioned works to improve the design
of the review process with a goal of achieving better eﬃciency and engagement of reviewers. Additionally,
this work is complementary to a direction of research that aims at improving computational and statistical
aspects of peer review (Kurokawa et al., 2015; Wang and Shah, 2019; Xu et al., 2019; Lian et al., 2018;
Stelmakh et al., 2019; Fiez et al., 2020; Jecmen et al., 2020; Noothigattu et al., 2020) and results of the
present study can be used to motivate future theoretical research.

The rest of the paper is structured as follows. In Section 2 we discuss the methodology of each component
of the novice-reviewer experiment. We then present the main results in Section 3. Finally, in Section 4 we
conclude the paper with a discussion of various aspects of the experiment.

2 Methodology

In this section we discuss the setup of our experiment. Speciﬁcally, we introduce the selection and mentoring
mechanisms and explain the methodology of evaluation of reviewers recruited through our experiment in the
ICML 2020 conference — a large venue that receives thousands of paper submissions and has more than
three thousand reviewers.

2.1 Selection Mechanism

The high-level idea of our selection mechanism is to pretest abilities of candidates to write high-quality
reviews. To this end, we frame the experiment as an auxiliary peer-review process that mimics the pipeline
of the real ML conferences as explained below and ask participants to serve as reviewers in this process. Let
us now describe the experiment in detail by discussing the pools of participants and papers, the organization
of the auxiliary review process, and the selection criteria we used to identify the best reviews whose authors
were invited to join the ICML reviewer pool.

Papers. We solicited 19 anonymized preprints in various sub-areas of ML from colleagues at various research
labs, ensuring that authors of these manuscripts do not participate in the experiment as subjects. Some
ML and AI conferences publicly release reviews for accepted/submitted papers, making these papers inap-
propriate for our experiment as our goal is to elicit independent reviews from participants. Thus, we used
only those papers that did not have reviews publicly available. The ﬁnal pool of papers consisted of working
papers, papers under review at other conferences, workshop publications and unpublished manuscripts. The
papers were 6–12 pages long excluding references and appendices (a standard range for many ML confer-
ences) and were formatted in various popular journals’ and conferences’ templates with all explicit venue
identiﬁers removed.

Participants. Since we had a small quota of approximately 50 reviewers allocated for the experiment in
the reviewer pool of the ICML 2020 conference, in this positional experiment we limited the target study
population to graduate students or recent graduates of ﬁve large, top US universities (CMU, MIT, UMD,
UC Berkeley and Stanford). To recruit participants, we messaged mailing lists of these universities and

3

targeted master’s and junior PhD students working in ML-related ﬁelds. The invitation also propagated to
a small number of students outside of these schools through the word of mouth. The recruiting materials
contained an invitation to participate in the ICML reviewer-selection experiment. Speciﬁcally, we notiﬁed
participants that they will need to review one paper and that those who write strong reviews will be invited
to join the the ICML reviewer pool. Being a reviewer in the top ML conference is a recognition of one’s
expertise and we envisaged that this potential beneﬁt is a good motivation for junior researchers to join
our experiment. As a result, we received responses from 200 candidates, more than 90% of whom were
students/recent graduates from the aforementioned schools. All of these candidates were added to the pool
of participants without further screening. We provide additional discussion of the demography of participants
(including their research and reviewing experience) in Section 4.

Auxiliary peer-review process The selection procedure closely followed the initial stages of the standard
double-blind ML conference peer-review pipeline and was hosted using Microsoft Conference Management
Toolkit (https://cmt3.research.microsoft.com) which is also used in ICML. First, we asked participants
to indicate their preferences in what papers they would like to review by entering bids that take the following
values: “Not Willing”, “In a Pinch”, “Willing” and “Eager”. Thirteen participants did not enter any bids and
were removed from the pool. The remaining 187 participants were active in bidding (mean number of
“Willing” and “Eager” bids is 4.7) and we assigned all of them to 1 paper each, where we tried to satisfy
reviewer bids, subject to a constraint that each paper is assigned to at least 8 reviewers.2 As a result, 186
participants were assigned to a paper they bid either “Willing” or “Eager” and 1 participant was assigned to
a paper they bid “In a Pinch” (this participant did not bid “Eager” or “Willing” on any paper).

Finally, we instructed participants that they should review the paper as if it was submitted to the real
ICML conference with the exception that the relevance to ICML, formatting issues (e.g., page limit, margins)
and potential anonymity issues should not be considered as criteria. To help participants in writing their
reviews, we provided reviewer guidelines (included in supplementary materials on the ﬁrst author’s website)
that discuss the best practices of reviewing. We gave participants 15 days to complete the review and then
extended the deadline for 16 more days to accommodate late reviews as our original deadline interfered with
the ﬁnal exams at various US universities and the US holiday period.
Selection of participants Out of 187 participants who were assigned a paper for review, 134 handed in the
reviews (response rate of 71.7%). Upon receipt of reviews, we removed numeric scores given by participants
to the papers and relied on the combination of the following approaches to identify individuals to be invited
to join the ICML reviewer pool:

• Author evaluation We asked authors of papers used in the experiment to read the reviews and

rate/comment on their qualities. Authors of 14 of the 19 submissions responded to our request.

• Internal evaluation We analyzed reviews for 17 papers falling in the study team members’ areas of

expertise.

• External evaluation We called upon an independent domain expert to help with 2 papers that are

outside of the study team members’ areas of expertise.

It is natural to assume that authors are at the best position to evaluate the reviews written for their
papers. Indeed, they know all the technical details of their papers, thereby being able to evaluate objective
points made by reviewers. Additionally, we hypothesize that the non-competitive nature of the auxiliary
review process may reduce potential biases related to a more negative perception of critical reviews which
in the past were observed in some real conferences (Weber et al., 2002; Papagiannaki, 2007; Khosla et al.,
2013). With this motivation, we requested authors to provide feedback on the received reviews and most of
the authors fulﬁlled our request.

To validate our expectations regarding the quality of the author feedback, all the reviews together with
the author feedback (when available) were additionally analyzed by the study team members and the afore-
mentioned domain expert. We qualitatively observed that the author feedback is helpful to identify the

2The constraint on the number of reviewers per paper was enforced to facilitate another experiment conducted in parallel

with the present study and described in the companion paper (Stelmakh et al., 2020).

4

strongest reviews and our selection decisions were well-aligned with the authors’ evaluations. Overall, we
invited 52 participants whose reviews received excellent feedback from all the evaluators who read the review
to join the ICML reviewer pool; all 52 accepted the invitation. For the rest of the paper, we will refer to
these reviewers as experimental reviewers.

2.2 Mentoring Mechanism

Throughout the conference review process, the experimental reviewers were oﬀered additional mentorship:
• The reviewers were provided with a senior researcher as a point of contact, and were oﬀered to ask any
questions pertaining to the review process at any point in the process. There were several questions asked
and answered as a part of the mentorship.

• The reviewers were provided with examples on various parts of the process, for instance, on how to lead

a discussion among the reviewers.

• When the initial reviews were submitted, certain issues were identiﬁed that were common across many
reviews from the experimental pool (e.g., many reviews were initially written about the authors rather
than the paper). The experimental reviewers were requested to address these issues.

• The experimental reviewers were sent a few more reminders than the conventional reviewers.

The total amount of time and eﬀort in the mentorship (across all 52 experimental reviewers) was equal to
about half the time and eﬀort for a meta-reviewer’s job.

2.3 Methodology of Evaluation

The main pool of the ICML 2020 reviewers was recruited through a combination of conventional approaches
and consisted of 3,012 reviewers3 belonging to two disjoint groups. The ﬁrst group, which we refer to as
curated, made up about 68% of the main pool and included reviewers who were invited by program chairs
based on satisfaction of at least one of the following criteria: (i) several years of reviewing and publishing
experience for top ML venues, (ii) above-average performance in reviewing for NeurIPS 2019 or (iii) personal
recommendation by a meta-reviewer. The remaining 32% of reviewers constituted the second group that
we call self-nominated: this group comprised individuals who self-nominated and satisﬁed the selection
criteria of (i) having at least two papers published in some top ML venues, and (ii) being a reviewer for at
least one top ML conference in the past. On average, the curated group consisted of more senior researchers
while the self-nominated pool mostly comprised researchers at early stages of their careers.

In the sequel, we compare the performance of 52 experimental reviewers who joined the ICML reviewer
pool through our experiment with the performance of the reviewers from the main pool. Let us now discuss
some important details of the evaluation.

Aﬃliation caveat 51 out of 52 experimental reviewers recruited through our selection procedure are
current master’s and PhD students or recent graduates of the aforementioned universities (one reviewer is a
graduate of another US school), whereas reviewers from the main pool represent universities as well as private
companies, government organizations, non-proﬁts and more, from all over the world. Hence, the reviewers
in the main pool have diﬀerent backgrounds from the experimental reviewers and this diﬀerence can serve
as an undesirable confounder (orthogonal to the selection procedure and mentoring) in our analysis.

To counteract this confounding factor, we identify a subset of the main pool of reviewers, whom we call
colleague reviewers. The colleague group comprises 305 reviewers from the main pool who share an
aﬃliation (i.e., email domain or aﬃliation listed on the conference management system) with the 5 schools
mentioned above. In our evaluations subsequently, we additionally juxtapose the experimental reviewers
to this group to evaluate how they compare to reviewers of similar background, thereby alleviating the
aﬃliation confounder.

3Some reviewers who initially accepted the invitation dropped out in the early stages of the review process and are not

included in this number and in the subsequent analysis.

5

Criteria (R = reviewer, P = paper)

Range

Experimental Main Pool

1.* Mean number of positive bids per R
2.* Frac. of Rs with > 0 reviews completed in time
3.* Mean review length (in symbols)
4. Mean initial overall score given by Rs
5. Mean self-reported confidence
6.* Mean self-reported expertise
7.* Frac. of (P, R) pairs with R active in P discussion
8.* Frac. of (P, R) pairs with post-rebuttal review update
9.* Mean review quality evaluated by meta-R

[0, 5052]
[0, 1]
[0, ∞)
[1, 6]
[1, 4]
[1, 4]
[0, 1]
[0, 1]
[1, 3]

34.6
0.92
4759
3.34
3.05
2.83
0.68
0.61
2.26

27.4
0.81
2858
3.25
3.03
2.98
0.58
0.43
2.08

P val.

.043
.041
< .001
.373
.841
.026
.033
< .001
< .001

Table 1: Performance comparison of reviewers from the main pool and experimental reviewers on various
criteria. Asterisks indicate criteria with signiﬁcant diﬀerence at the level 0.05.

Metrics and tools of comparison We use a set of indirect indicators of review quality (e.g., review
length and discussion participation) as well as direct evaluations of review quality made by meta-reviewers
of the ICML conference — senior reviewers, each of whom is in charge of overseeing the review process
for approximately 20 submissions. To quantify signiﬁcance of the diﬀerence in these metrics, we use the
permutation test (Fisher, 1935), treating each paper-reviewer pair as a unit of analysis. Error bars presented
in ﬁgures below represent bootstrapped 90% conﬁdence intervals unless stated otherwise.

Finally, throughout the review process, meta-reviewers were calling upon additional external reviewers to
help with some submissions or asking reviewers from the main pool to review additional papers; these paper-
reviewer pairs are not included into comparison because new reviewers typically had less time to complete
the assigned reviews.

3 Evaluation

In the previous section we described our approach towards recruiting novice reviewers and mentoring them.
In this section we move to the real ICML conference and evaluate the beneﬁt of the proposal by juxtaposing
the performance of experimental reviewers to the main reviewer pool which consists of self-nominated
and curated reviewers, some of whom belong to the group of colleague reviewers. For this, we compare
performance of reviewers at diﬀerent stages of the review process: bidding, reviewing (in-time submission,
review length, self-assessed conﬁdence and others) and discussion (activity, attention to the author feedback).
Finally, we complement the comparison by overall evaluation of the review quality made by meta-reviewers.
Table 1 summarizes the results of comparison of experimental reviewers with reviewers from the main
pool; subsequently, we will present a more detailed analysis with breakdown by reviewer groups. The
main message of Table 1 is that from various angles the reviews written by experimental reviewers are
comparable to or sometimes even better than reviews written by reviewers from the main pool. With this
general observation, we now provide details and background for each row of Table 1.

Bidding activity (Row 1 of Table 1) Algorithms for automated paper-reviewer matching signiﬁcantly
rely on reviewer bids (Fiez et al., 2020). Hence, activity of reviewers in the bidding stage is crucial to ensure
that submissions are assigned to reviewers with appropriate expertise. To give matching algorithms enough
ﬂexibility, ICML program chairs requested reviewers to positively bid (i.e., indicate papers they are “Willing”
or “Eager” to review) on at least 30-40 submissions (out of approximately 5,000 submitted for review).

Figure 1 compares mean numbers of positive and non-negative (“Willing”, “Eager” and “In a Pinch”)
bids made by reviewers from diﬀerent groups. Note that to compare non-negative bids we remove one
reviewer who bulk bid “In a Pinch” on all non-conﬂicting submissions. Several reviewers bid on hundreds
of submissions (possibly by bulk bidding on some speciﬁc areas or keywords), but we do not exclude such
reviewers from the analysis.

6

Figure 1: Mean number of positive/non-negative bids per reviewer. Experimental reviewers positively bid
on more papers than reviewers from each of the comparison groups.

Bids

Experimental Main Pool Curated

Self-Nominated Colleague

Positive

Non-
Negative

Sample Size
Mean Value
90% CI
P value
Sample Size
Mean Value
90% CI
P value

52
34.6
[31.4; 38.1]
–

52
41.1
[37.6; 44.7]
–

3012
27.4
[26.8; 28.1]
.043

3011
32.4
[31.7; 33.2]
.046

2060
25.7
[25.1; 26.3]
.003

2059
30.7
[30.0; 31.5]
.007

952
31.2
[29.8; 33.0]
.221

952
36.1
[34.6; 37.9]
.165

305
26.3
[24.6; 28.3]
.011

305
31.2
[29.3; 33.3]
.005

Table 2: Comparison of bidding activity of the reviewers. P values are for the test of the diﬀerence of means
between experimental and each of the other groups of reviewers.

Overall, we observe that experimental reviewers are more active than other categories of reviewers
with a qualiﬁcation that the diﬀerence with self-nominated reviewers (∆positive = 3.4, ∆non-negative = 5.0)
is not statistically signiﬁcant at the 0.05 signiﬁcance level as demonstrated in Table 2 that summarizes the
results of comparison.4

Timely review submission (Row 2 of Table 1) A typical conference timeline is very tight and it is
crucial that reviewers complete their reviews in a timely manner. We now compare how diﬀerent groups
of reviewers respect the deadlines. For this, we use two metrics: ﬁrst, Figure 2a juxtaposes engagement
rates — fractions of reviewers who submitted at least one review by a given date — of diﬀerent reviewer
groups. Second, Figure 2b compares completion rates — the total number of submitted reviews divided
by the total number of assigned papers. While the completion rate is perhaps a more intuitive choice of
metric, it is artiﬁcially favourable to experimental reviewers due to a diﬀerence in the reviewer loads
between experimental reviewers and reviewers from the main pool (see more discussion in Section 4). To
counteract the bias in objective, for formal comparisons presented in Tables 1 and 3 we use the engagement
rate which is less impacted by the diﬀerence in loads.

Looking at Figure 2, we again observe the trend of junior self-nominated and experimental review-
ers being consistently more active than their senior curated counterparts throughout the whole review-
submission period, with experimental reviewers achieving the highest engagement and completion rates
across all reviewer groups. Note that due to the impact of the COVID-19 pandemic, the initial deadline for

4We also observe that self-nominated reviewers are more active than curated reviewers, suggesting that the bidding

activity may be decreasing with seniority.

7

CuratedSelf-nominated   ColleagueExperimentalReviewer group010203040Mean number of bids"Willing" and "Eager""In a Pinch", "Willing" and "Eager"(a) Engagement rates.

(b) Completion rates.

Figure 2: Timely review submission. Bold labels indicate dates at which deadlines were set with the original
deadline on day X and two extensions. 90% conﬁdence intervals are computed using the method of Wilson
(1927). Experimental reviewers have higher engagement and completion rates than other reviewers.

Experimental Main Pool Curated

Self-Nominated Colleague

Sample Size
Mean Value
90% Wilson CI
P value

52
0.92
[0.84; 0.96]
–

3007
0.81
[0.80; 0.82]
.041

2055
0.79
[0.78; 0.81]
.028

952
0.84
[0.82; 0.86]
.140

305
0.76
[0.71; 0.80]
.006

Table 3: Comparison of engagement rates of reviewers on the ﬁrst extended deadline. P values are for the
test of the diﬀerence of means between experimental and each of the other groups of reviewers.

review submission on day X was extended twice (deadline dates are highlighted in bold in Figure 2) with
the ﬁrst extension announced well in advance and hence only a small fraction of reviews was submitted by
day X. Thus, in Table 1 we use data for the ﬁrst extended deadline on day X+3.

Table 3 extends the comparison reported in Table 1 by displaying a breakdown by reviewer groups.5
The results of the permutation test qualify the observations we made from Figure 2 by showing that the
diﬀerence between experimental and self-nominated reviewers (∆ = 0.08), who represent the more
junior population of the main reviewer pool, is not signiﬁcant at the requested level.

Before we proceed to other dimensions of comparison, we note that a small number of reviewers from the
main pool never submitted reviews for some of the assigned papers (less than 5% of paper-reviewer pairs had
no review submitted). Corresponding paper-reviewer pairs are excluded from the analysis of various aspects
of review quality we perform below.

Review length (Row 3 of Table 1) We continue the analysis by juxtaposing the lengths of textual
comments submitted by reviewers in Figure 3. We observe that diﬀerent categories of reviewers from the
main pool appear to write reviews of comparable length whereas experimental reviewers write considerably
longer reviews. The distribution of lengths of reviews written by reviewers from the main pool is very similar
to that of several major ML conferences (Beygelzimer et al., 2019), and thus we conclude that experimental
reviewers produced longer reviews than standard in the ﬁeld. Table 4 compares mean lengths of reviews
written by reviewers from diﬀerent groups and conﬁrms the intuition represented in Figure 3.

5The number of reviewers used in the comparison is smaller than the total number of reviewers because we only use paper-
reviewer pairs that were in the assignment from the beginning of the review period and 5 reviewers from the curated group
with small initial loads had a set of their papers fully changed throughout the process.

8

XX+2X+3X+5X+6X+8X+12Date0.00.20.40.60.81.0Engagement rateCuratedSelf-nominatedColleagueExperimentalXX+2X+3X+5X+6X+8X+12Date0.00.20.40.60.81.0Completion rateFigure 3: Distribution of review lengths. Experimental reviewers write longer reviews than other reviewers.

Experimental Main Pool

Curated

Self-Nominated Colleague

Sample Size
Mean Value
90% CI
P value

154
4759
[4432; 5089]
–

15206
2858
[2836; 2880]
< .001

10502
2953
[2926; 2979]
< .001

4704
2647
[2609; 2685]
< .001

1593
2985
[2924; 3047]
< .001

Table 4: Comparison of mean lengths (in symbols) of reviews. P values are for the test of the diﬀerence of
means between experimental and each of the other groups of reviewers.

Hypercriticality (Row 4 of Table 1) Although junior reviewers are often perceived to be more crit-
ical (Mogul, 2013; Tomiyama, 2007; Toor, 2009), the analysis of the NeurIPS 2016 conference conducted
by Shah et al. (2018) does not reveal a signiﬁcant diﬀerence between overall scores given by junior and senior
reviewers. We now perform such an analysis for the ICML conference: in ICML 2020 reviewers were asked to
give the overall score on a 6-point Likert item and we encode the options with integers from 1 to 6 such that
the larger number indicates the better score. The mean overall scores given in initial reviews (i.e., before
reviewers got to see other reviews and the author rebuttal) are compared across diﬀerent groups of reviewers
in Figure 4.

Mean initial overall scores given by diﬀerent groups of reviewers appear to be comparable; self-nominated

and experimental reviewers seem to be slightly more lenient than curated reviewers, but the sample size

Figure 4: Mean initial overall scores. Self-nominated and experimental reviewers appear to be more
lenient than curated reviewers.

9

0.00.20.40.60.81.0Fraction of reviews05101520Length ×103 symbolsCuratedSelf-nominatedColleagueExperimentalCuratedSelf-nominatedColleagueExperimentalReviewer group1.01.52.02.53.03.5Mean initialoverall scoreExperimental Main Pool Curated

Self-Nominated Colleague

Sample Size
Mean Value
90% CI
P value

154
3.34
[3.17; 3.51]
–

15206
3.25
[3.24; 3.27]
.373

10502
3.21
[3.19; 3.23]
.199

4704
3.35
[3.33; 3.38]
.895

1593
3.20
[3.16; 3.25]
.175

Table 5: Comparison of mean initial overall scores. P values are for the test of the diﬀerence of means
between experimental and each of the other groups of reviewers.

of experimental reviewers is not suﬃcient to draw deﬁnitive conclusions (Table 5 summarizes the compar-
ison). However, we note that self-nominated reviewers are indeed more lenient than curated reviewers
(∆ = 0.14, P < .001), contradicting the aforementioned observations of hypercriticality in junior reviewers.
This misalignment may be speciﬁc to the ﬁeld of computer science where hypercriticality is not limited
to junior reviewers, but is prevalent in the whole area (Vardi, 2010), or, alternatively, it is possible that
hypercriticality of junior reviewers manifests not in numeric scores but in textual reviews.

Expertise and conﬁdence (Rows 5 and 6 of Table 1) We now continue with the analysis of self-
assessed conﬁdence and expertise of diﬀerent reviewer groups (using values reported in initial reviews). The
reviewer form of the ICML 2020 conference contained two questions in which reviewers were asked to evaluate
their expertise and conﬁdence in their review on 4-point Likert items. We encode the options of the Likert
items with integer numbers from 1 to 4 such that larger numbers indicate higher expertise/conﬁdence. We
then compare mean scores across reviewer groups and report the results in Figure 5 and Table 6.

Not surprisingly, experimental reviewers reported lower self-assessed expertise (Figure 5a) with a caveat
that the diﬀerence with self-nominated reviewers is not statistically signiﬁcant. That said, the diﬀerence
in expertise does not seem to result in the diﬀerence in self-assessed conﬁdence (Figure 5b).

Rebuttals and discussion (Rows 7 and 8 of Table 1) The review process of ICML allows authors
to respond to initial reviews written for their papers by submitting a short rebuttal that is followed by a
private discussion between reviewers and the meta-reviewer. Past analysis (Shah et al., 2018; Gao et al.,
2019; ACLCommittee, 2018) provide mixed evidence regarding the usefulness of rebuttals, and in this work
we do not aim to judge the overall eﬃcacy of the rebuttal process. However, in order for the rebuttal or
discussion to change the reviewer’s opinion, reviewers at the very least need to consider the rebuttal and be

(a) Self-assessed expertise.

(b) Self-assessed conﬁdence.

Figure 5: Comparison of self-assessed expertise and conﬁdence. Experimental reviewers report consid-
erably lower expertise than other groups of reviewers, but there is no signiﬁcant diﬀerence in self-assessed
conﬁdence.

10

CuratedSelf-nominatedColleagueExperimentalReviewer group1.01.52.02.53.03.5Mean self-reportedexpertiseCuratedSelf-nominatedColleagueExperimentalReviewer group1.01.52.02.53.03.5Mean self-reportedconfidenceCriteria

Experimental Main Pool Curated

Self-Nominated Colleague

Expertise

Confidence

Sample Size
Mean Value
90% CI
P value
Sample Size
Mean Value
90% CI
P value

154
2.83
[2.73; 2.94]
–

154
3.05
[2.97; 3.13]
–

15206
2.98
[2.97; 2.99]
.026

15206
3.03
[3.02; 3.04]
.841

10502
3.01
[3.00; 3.02]
.005

10502
3.04
[3.03; 3.05]
.829

4704
2.92
[2.90; 2.93]
.204

4704
3.02
[3.00; 3.04]
.646

1593
2.98
[2.95; 3.01]
.021

1593
3.02
[2.99; 3.05]
.579

Table 6: Comparison of self-assessed expertise (ﬁrst 4 rows) and conﬁdence (last 4 rows). P values are for
the test of the diﬀerence of means between experimental and each of the other groups of reviewers.

engaged in the discussion. We now investigate this aspect, conditioning on papers whose authors supplied a
response to initial reviews (approximately 80% of submissions had the author response provided).

Figure 6 compares the fractions of paper-reviewer pairs such that the reviewer posted at least one message
in the discussion thread (discussion activity rate, Figure 6a) / updated the textual review after the rebuttal
(review update rate, Figure 6b), formal results of comparison are summarized in Table 7. We note that in
both dimensions experimental reviewers are more active than other categories of reviewers.6
Review quality (Row 9 of Table 1) So far we have observed that experimental reviewers are more
active in all stages of the review process than reviewers from the main pool. However, the comparisons
above do not decisively answer the question of quality of reviews written by the new reviewers. To bridge
this gap, we now report the evaluations of review quality made by meta-reviewers. At the end of the review
process, meta-reviewers were asked to evaluate the quality of each review on a 3-point Likert item with
the following options:
“Failed to meet expectations” (Score 1), “Met expectations” (Score 2), “Exceeded
expectations” (Score 3). Not all meta-reviewers completed the evaluation (approximately 25% of all paper-
reviewer pairs did not have a meta-reviewer evaluation) so for comparison of review quality we limit the
attention to paper-reviewer pairs for which the corresponding meta-reviewer rated the quality of the review.

(a) Participation in discussion.

(b) Post-rebuttal review update.

Figure 6: Activity in the last stage of the review process. Experimental reviewers participate in the
discussion and update textual reviews more often than other reviewers.

6Interestingly, Figure 6 shows that self-nominated reviewers are less engaged in the last stage of the review process
than senior curated reviewers. This observation suggests that the relative engagement of junior self-nominated reviewers
decreases as the review process progresses. We do not see this in the experimental reviewers and hypothesize that more
tailored mentoring leads to a consistent engagement of experimental reviewers.

11

CuratedSelf-nominatedColleagueExperimentalReviewer group0.00.20.40.60.81.0Discussion activity rateCuratedSelf-nominatedColleagueExperimentalReviewer group0.00.20.40.60.81.0Review update rateCriteria

Experimental Main Pool Curated

Self-Nominated Colleague

Sample Size
Discussion Mean Value
Activity

90% CI
P value
Sample Size
Mean Value
90% CI
P value

123
0.68
[0.61; 0.75]
–

123
0.61
[0.54; 0.68]
–

11985
0.58
[0.58; 0.59]
.033

11985
0.43
[0.42; 0.44]
< .001

8298
0.60
[0.59; 0.61]
.083

8298
0.47
[0.46; 0.48]
.005

3687
0.54
[0.53; 0.56]
.004

3687
0.34
[0.33; 0.35]
< .001

1262
0.60
[0.57; 0.62]
.077

1262
0.46
[0.44; 0.48]
.003

Review
Update

Table 7: Comparison of post-rebuttal reviewer activity: participation in discussion (ﬁrst 4 rows) and review
update rate (last 4 rows). P values are for the test of the diﬀerence of means between experimental and
each of the other groups of reviewers.

Importantly, meta-reviewers were not aware of the group aﬃliation of reviewers. In the corresponding row of
Table 1, we compare mean quality scores between diﬀerent categories of reviewers and Table 8 complements
the comparison by additionally presenting a breakdown by reviewer groups, conﬁrming that experimental
reviewers receive higher scores than their counterparts.

In contrasts to Tables 1 and 8 that compare mean ratings, Figure 7 visualizes the fraction of reviews
below and above the expectations of meta-reviewers within each group. Observe that reviews written by
experimental reviewers exceed expectations of meta-reviewers more often than reviews of curated and
self-nominated reviewers, and conditioning on the aﬃliation does not impact the comparison. Figure 7
also shows that experimental reviewers produced substandard reviews less often than other reviewers.

4 Discussion

In this work we designed and executed an experimental procedure for novice reviewer recruiting and mentor-
ing with a goal to address scarcity of qualiﬁed reviewers in large conferences. We evaluated the results of the
experiment by juxtaposing the performance of the new reviewers to the traditional reviewer pool in the real
ICML 2020 conference. We now provide additional discussion of the recruiting and evaluation procedures,
comment on the scalability of the experiment, and suggest directions for future work.

4.1 Discussion of the Recruiting Experiment

We begin from some important aspects of the auxiliary peer-review process we conducted to recruit reviewers.
First, we perform the analysis of the demography of participants. After that, we mention another potentially
useful aspect of the experiment related to reviews written in this auxiliary review process.

Experimental Main Pool Curated

Self-Nominated Colleague

Sample Size
Mean Value
90% CI
P value

111
2.26
[2.18; 2.34]
–

11624
2.08
[2.07; 2.09]
< .001

8035
2.11
[2.10; 2.12]
.002

3589
2.02
[2.00; 2.03]
< .001

1179
2.11
[2.09; 2.14]
.003

Table 8: Comparison of mean review qualities as evaluated by meta-reviewers. P values are for the test of
the diﬀerence of means between experimental and each of the other groups of reviewers.

12

Figure 7: Evaluation of review quality by meta-reviewers. The closer the point to the upper-left corner, the
better. Experimental reviewers dominate other groups of reviewers.

Demography of participants Recall that self-nominated individuals had to pass a publication and review-
ing ﬁlters (see Section 2.3 for details) to join the self-nominated reviewer pool of ICML 2020. We now
test whether the subjects of our experiment satisfy these criteria. Table 9 comprises relevant demographic
information for 134 subjects of the selection experiment who completed the participation (that is, submitted
the review of the assigned paper) and for 52 subjects who were eventually invited to join the ICML 2020
experimental reviewer pool. Importantly, this demographic information was hidden from evaluators who
performed the selection. Observe that most of the participants of our experiment (including those that
were selected to join the ICML reviewer pool) do not pass at least one of the ﬁlters mandatory for the
self-nominated reviewers. Similarly, none of the participants was invited to join the curated reviewer
pool. Therefore, we conclude that most of the participants of our experiment would not have been invited
to ICML 2020 through conventional ways of reviewer recruiting.

Total number

With prior review experience
With publications
Pass reviewing filter
Pass publication filter
Pass self-nominated filters

All

Invited

134

52

36%
37%
72%
77%
21%
21%
23%
24%
13% 12%

Table 9: Demography of subjects of our experiment.

Reviews As a byproduct of our experiment, authors of manuscripts we used in the auxiliary peer-review
process received a set of reviews. They generally admitted a high quality of reviews: several authors men-
tioned that reviewers found some errors/important typos in their papers or suggested some ways to improve
the presentation. An author of one paper comments:

“Very high quality reviews, in my opinion. Most of them [...] are clearly more detailed and more
useful than reviews we received at [another top ML venue] ”

and an author of another manuscript says:

“Those [reviews] were super informative and helpful! ”

As a minor qualiﬁcation, we underscore that while the overall feedback was positive, authors also helped us
to identify a number of reviews of substandard quality for ICML (with serious factual errors or dismissive

13

0.000.020.040.060.080.100.12Fraction of "below expectation" reviews0.000.050.100.150.200.250.300.350.40Fraction of"above expectation" reviewsReviewer groupExperimentalCuratedSelf-nominatedColleagueMain Pool Experimental

# Reviewers
Mean reviewer load
Fraction of positive bids
Mean paper↔reviewer similarity

3012
5.4
0.88
0.77

52
3.0
0.99
0.88

Table 10: Assignment quality.
“Fraction of positive bids” represents a fraction of paper-reviewer pairs in
the assignment such that the reviewer has bid positively on the paper. Similarities between papers and
reviewers take values in the interval [0, 1] and are computed using the TPMS system (Charlin and Zemel,
2013). 706 reviewers in the main pool and 4 experimental reviewers did not have similarities computed
and are excluded from the computation of mean similarity.

criticism) and we found the author feedback to be very useful for the selection. That said, the positive
overall feedback from authors hints that a large scale version of our experiment can give researchers a set of
useful reviews before they submit a paper to a real conference, potentially decreasing the load on the actual
conferences.

4.2 Discussion of the Evaluation

We now discuss some aspects important for interpretation of the results of the comparison between exper-
imental reviewers and the main reviewer pool of ICML 2020.

Aspect 1. Assignment procedure Each paper submitted to the ICML conference was ﬁrst automatically
assigned to three reviewers from the main pool, two of whom were curated reviewers and one was a self-
In that, we tried to satisfy reviewer bids and optimize for the notion of textual
nominated reviewer.
similarity (Charlin and Zemel, 2013) between submissions and assigned reviewers, subject to a requirement
that each reviewer is assigned at most six papers (a small set of reviewers requested a lower quota) and under
a fairness constraint that aims at balancing assignment quality across submissions (Stelmakh et al., 2018).
After that, each experimental reviewer was manually assigned to three submissions as a fourth reviewer.
All assignments were ﬁnally adjusted by meta-reviewers before being released to the reviewers. Given the
small number of experimental reviewers and the large number of submissions, the constraints we had to
satisfy in the manual assignment were mild as compared to the main assignment. Hence, experimental
reviewers could receive submissions that better ﬁt their expertise than reviewers from the main pool.

Table 10 compares several metrics of assignment quality across reviewers from the main pool and exper-
imental reviewers. First, we note that experimental reviewers were intentionally assigned fewer papers
than reviewers from the main pool to ensure a gentle introduction to the review process. We underscore that
in this study we aim to show that reviewers from the population not covered by current recruiting methods
can usefully augment the reviewer pool given some special treatment (careful recruiting, reduced load and
mentoring). Hence, we do not consider the diﬀerence in load to be a confounder in our comparisons.

Second, experimental reviewers were assigned to papers they positively bid on and to papers with
high textual similarity more often than reviewers from the main pool. Hence, we caveat that the quality
of the assignment diﬀers signiﬁcantly between reviewers from the main pool and experimental reviewers,
introducing a confounding factor that can potentially impact the comparison. On the other hand, the
diﬀerence in the assignment quality may in part be due to the diﬀerence in bidding activity that we outlined
in Section 3: a large number of positive bids made by experimental reviewers gives more ﬂexibility in
satisfying them, thereby increasing the quality of the assignment. It will be of interest to investigate, in any
future larger scale studies, whether a larger number of experimental reviewers also continue to have such
a higher quality of assignment due to higher bidding activity, and if not, then it will be of interest to observe
how that impacts the other metrics.

14

Aspect 2. Quality evaluation Following a standard approach in AI/ML conferences that conduct
a survey of meta-reviewers on the review quality, when asking meta-reviewers to evaluate the quality of
reviews, we left it for the meta-reviewers to decide on their expectations and did not precisely deﬁne the
term “quality”. As a result, diﬀerent meta-reviewers could have diﬀerent standards in mind, leading to
some inconsistency in evaluations. To account for this issue, in the Appendix we complement the above
analysis of review quality and some other metrics with additional comparisons performed on a restricted
set of submissions that had at least one experimental reviewer assigned (by doing so we equalize the
sets of meta-reviewers who rate experimental reviewers and other categories of reviewers as well as other
characteristics of papers assigned to diﬀerent groups of reviewers). Importantly, this analysis leads to the
same conclusions as the analysis we described in Section 3.

Another related caveat is that the absence of a well-deﬁned notion of quality could result in the substi-
tution bias in meta-reviewers’ judgments of review quality. For example, meta-reviewers’ evaluations could
be driven by the length of the review or some other computationally inexpensive, but suboptimal, proxy,
resulting in a biased evaluation of quality. We urge the reader to be aware of this issue when interpreting
the results of the quality evaluations.

Aspect 3. Insights from NeurIPS 2020 review process After the experiment we describe in this
paper was completed, the NeurIPS 2020 conference released the analysis of its review process (Lin et al.,
2020) in which they compared the quality of reviews written by curated and author-sourced reviewers (to
avoid ambiguity, in this section we refer to curated NeurIPS reviewers as invited reviewers). In terms of the
selection criteria, these groups of reviewers roughly correspond to curated and self-nominated groups
we consider in the present paper, with an important exception that in our case self-nominated reviewers
were not required to have their paper submitted to ICML 2020. There may also be some subtle diﬀerences
in how the pools of invited NeurIPS reviewers and curated ICML reviewers were constructed. With these
caveats, the analysis of NeurIPS 2020 data presents two key insights relevant to our study that we now
discuss.

First, NeurIPS data suggests that the quality (as measured by meta-reviewers) of reviews written by
author-sourced reviewers is only marginally worse than that of invited reviewers. This observation agrees
with what we report in Table 8 where curated reviewers have slightly higher mean review quality than
their self-nominated counterparts, but the diﬀerence is somewhat more pronounced in our case.

Second, and perhaps more importantly, it appears that in NeurIPS the quality of reviews was negatively
correlated with experience of reviewers: reviewers for whom NeurIPS 2020 was the ﬁrst big ML conference
they serve for appear to produce reviews of higher quality than their counterparts who have reviewed for
major conferences before. In ICML 2020, all but perhaps 5–10% of members of the main reviewer pool had
past experience of being a reviewer for some top ML venues,7 while most of experimental reviewers did
not have such experience. Hence, the result of NeurIPS analysis highlights another potential confounding
factor in our study: past reviewing experience. Indeed, hypothetically the results of our experiment can be
explained by the fact that reviewers put more eﬀorts into their ﬁrst reviews and then become less engaged
in future conferences.

Let us now qualify the above caveat. First, in our experiment the diﬀerence between experimental
reviewers and reviewers from the main ICML reviewer pool appears to be considerably larger than the
diﬀerence between the ﬁrst-time reviewers and those who have reviewed before observed in NeurIPS. The
larger eﬀect size hints at the potential eﬀect of our intervention. Second, in the present work we carefully
account for the aﬃliation confounding factor and additionally juxtapose the experimental reviewers with
the colleague reviewers who share the same aﬃliation. The NeurIPS analysis does not provide such
comparison and hence we cannot remove this confounding factor. Finally, in the present paper we compare
the performance of reviewers on various metrics beyond the review quality and it would be interesting to see
how novice NeurIPS reviewers perform on these metrics.

7Some curated reviewers were recommended by the meta-reviewers and are not guaranteed to have the past review

experience

15

Aspect 4. Additional caveats
other remarks:

In addition to the caveats mentioned above, we would like to make several

• First, while we measured a number of metrics that were possible to measure and have been considered
in the literature, we cannot exclude the possibility that experimental reviewers may be worse than the
main pool of reviewers in some other aspect not considered here.

• Second, it is possible that behavior of experimental reviewers was aﬀected by demand characteris-
tics McCambridge et al. (2012), that is, experimental reviewers could hypothesize that we want them
to perform better than reviewers from the main pool and hence they could adjust their behaviour to meet
these perceived expectations.

• Third, the review process of the ICML 2020 conference was impacted by the COVID-19 pandemic and the
impact of the pandemic on diﬀerent reviewer groups could be unequal. For example, senior reviewers from
the curated pool could have more family-related duties (and hence could be more restricted in reviewing
ability) than junior experimental reviewers.

• Finally, in extrapolating any results to other conferences, one should carefully consider any idiosyncrasies

of speciﬁc conferences.

All the aforementioned caveats coupled with sensitivity of the subject matter underscore the importance
of a careful experimentation with the proposed procedure before its implementation in the routine review
process.
Aspect 5. The role of reviewers With the above caveats, the experiment demonstrated that exper-
imental reviewers are comparable to and sometimes even better than reviewers recruited in conventional
ways in terms of various metrics analyzed in Section 3. However, we qualify that this observation absolutely
does not imply that experimental reviewers can entirely substitute the pool of experienced reviewers.
Instead, we conclude that if recruited and mentored appropriately, experimental reviewers can form a
useful augmentation to the traditional pool. The experimental and experienced reviewers may have dif-
ferent strengths that can be combined to achieve an overall improvement of the peer-review quality. For
instance (Shah, 2019a), experimental and, more generally, junior reviewers can be used to evaluate nu-
anced technical details of submissions (e.g., proofs) while senior researchers can focus on the broader picture
and more subjective criteria such as impact where their expertise is extremely important.

4.3 Scalability of the Experiment

In this section we provide some ideas on how the experiment we described in this paper can be scaled to
increase the number of recruited reviewers from 52 to several hundred. To this end, recall that the experiment
is based on the two major components: the selection and mentoring mechanisms. We now comment on how
to scale each of these components.

Selection Mechanism The current selection pipeline requires an amount of work equivalent to 2 to 4 days
of the conference workﬂow chair’s work and 2 to 4 hours of the conference program chairs’ work to execute
the experiment. Hence, it is important to design a version of the selection mechanism that can handle more
reviewers while not resulting in a proportional increase of the load on the organizers.

First, we note that a large share of work in the selection stage was spent on ﬁnding papers to use in
the auxiliary review. For this initial experiment, we had not made the call for papers public and instead
personally reached to dozens of colleagues asking them to contribute their manuscripts which resulted in
a large amount of communication-related work. However, this initial experiment demonstrated a lot of
enthusiasm from authors of the papers used in the experiment who appreciated a set of useful reviews they
received. Hence, we believe that we can easily extend the pool of papers by widely distributing the call for
papers.

Similarly, participants of the selection mechanism executed in the present study were active in signing up
for the experiment and appreciated an opportunity to join the ICML reviewer pool. In this initial experiment,

16

the population of participants was limited to students of 5 large US universities. Hence, by making an open
call for participants on behalf of a large ML conference, we expect to increase the pool of candidates to
several hundred participants without much additional eﬀorts.

The major part of the selection mechanism that requires a close attention of organizers is the review
evaluation and ﬁnal decision making. As noted in Section 2.1, we found author feedback to be very helpful
for evaluating the quality of reviews and hence the selection part can be streamlined if the authors are
required to evaluate all the reviews received in the experiment. Given that authors of papers used in the
experiment generally found these reviews useful, we think that such a requirement is feasible as these reviews
serve as a good incentive for authors to put some eﬀorts in the experiment.

Mentoring As mentioned in Section 2.2, the total amount of time and eﬀort in the mentorship of 52
experimental reviewers was equal to about half the time and eﬀort for a meta-reviewer’s job. We note
that the time demand for mentorship does not increase proportionally to the number of reviewers as some
parts of the mentorship have a ﬁxed cost (e.g., sending general guideline emails or multiple reminders to
non-responsive reviewers). Thus, several additional committee members recruited speciﬁcally for mentoring
will allow to handle several hundred novice reviewers in the real conference. Alternatively, mentoring can
be distributed across many senior researchers who do not have time for meta-reviewing, but can contribute
a smaller amount to mentoring.

Overall, we believe that the procedure outlined above can produce several hundred experimental reviewers

without overburdening the organizers of the experiment.

4.4 Future Work

An important direction for future work is to compare experimental reviewers with the main reviewer pool
in a larger scale study whose design we outlined above. A larger experiment would enable a deeper analysis
of textual reviews written by diﬀerent reviewer groups.
It will also be of interest to design and execute
experiments that address the caveats discussed above.

Another important direction is a principled design of a mentoring protocol to support novice reviewers.
Since ML/AI conferences have hundreds of meta-reviewers, it may be prudent to assign a small number of
meta-reviewers as mentors for junior reviewers and reduce their meta-reviewer workload accordingly. Future
editions could also involve sharing more material on how to review with reviewers (e.g., Köhler et al., 2020)
and holding webinars with Q&A sessions.

Finally, the feedback from the experimental reviewers was that it was helpful for them to experience
and gain insights into the review process, which will also help in their own research dissemination in the
future. It would be interesting to measure the impact of the guided introduction to the review process in
the early stages of career on the future trajectory of the individuals as researchers and reviewers.

Acknowledgments

We thank authors of papers used in the selection experiment for contributing their works to this study
and for helping us with identifying strong reviews. We also thank Devendra Singh Chaplot who served
as a domain expert in the external evaluation of reviews. We are grateful to the support team of the
Microsoft Conference Management Toolkit (CMT) who hosted our selection experiment and helped with
many customization requests during the experiment and the actual ICML 2020 conference. Finally, we
appreciate the eﬀorts of all reviewers and meta-reviewers involved in the ICML 2020 review process. This
study was approved by Carnegie Mellon University Institutional Review Board.

This work was supported in part by NSF CAREER award 1942124 and in part by NSF CIF 1763734.

17

References

ACLCommittee (2018). A report on the review process of ACL 2018.

https://acl2018.org/2018/05/19/

how-decisions-made/ [Accessed: 9/7/2020].

Beygelzimer, A., Fox, E., dÁlche Buc, F., and Larochelle, H. (2019). What we learned from NeurIPS 2019 data. https:
//medium.com/@NeurIPSConf/what-we-learned-from-neurips-2019-data-111ab996462c [Accessed: 9/7/2020].

Chandler, D. and Kapelner, A. (2013). Breaking monotony with meaning: Motivation in crowdsourcing markets.

Journal of Economic Behavior & Organization, 90:123 – 133.

Charlin, L. and Zemel, R. S. (2013). The Toronto Paper Matching System: An automated paper-reviewer assignment

system.

Feldmann, A. (2005). Experiences from the sigcomm 2005 european shadow pc experiment. ACM SIGCOMM

Computer Communication Review, 35(3):97–102.

Fiez, T., Shah, N., and Ratliﬀ, L. (2020). A SUPER* algorithm to optimize paper bidding in peer review.

In

Conference on Uncertainty in Artiﬁcial Intelligence.

Fisher, R. A. (1935). The design of experiments. Oliver & Boyd, Oxford, England.

Gao, Y., Eger, S., Kuznetsov, I., Gurevych, I., and Miyao, Y. (2019). Does my rebuttal matter? Insights from a

major NLP conference. CoRR, abs/1903.11367.

Garisto, D. (2019). Diversifying peer review by adding junior scientists. https://www.natureindex.com/news-blog/

diversifying-peer-review-by-adding-junior-scientists [Accessed: 9/7/2020].

Jecmen, S., Zhang, H., Liu, R., Shah, N. B., Conitzer, V., and Fang, F. (2020). Mitigating manipulation in peer

review via randomized reviewer assignments. In NeurIPS.

Kaufmann, N., Schulze, T., and Veit, D. (2011). More than fun and money. worker motivation in crowdsourcing–a

study on mechanical turk.

Kerzendorf, W. E., Patat, F., Bordelon, D., van de Ven, G., and Pritchard, T. A. (2020). Distributed peer review

enhanced with natural language processing and machine learning.

Khosla, A., Hoiem, D., and Belongie, S. (2013). Analysis of reviews for CVPR 2012.

Köhler, T., González-Morales, M. G., Banks, G. C., O’Boyle, E. H., Allen, J. A., Sinha, R., Woo, S. E., and Gulick,
L. M. (2020). Supporting robust, rigorous, and reliable reviewing as the cornerstone of our profession: Introducing
a competency framework for peer review. Industrial and Organizational Psychology, 13(1):1–27.

Kotturi, Y., Kahng, A., Procaccia, A. D., and Kulkarni, C. (2020). Hirepeer: Impartial peer-assessed hiring at scale
in expert crowdsourcing markets. In Proceedings of the Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence,
AAAI’20. AAAI Press.

Kurokawa, D., Lev, O., Morgenstern, J., and Procaccia, A. D. (2015). Impartial peer review. In Proceedings of the

24th International Conference on Artiﬁcial Intelligence, IJCAI’15, page 582–588. AAAI Press.

Langford, J. (2018). When the bubble bursts. . . . https://hunch.net/?p=9604328 [Accessed: 9/7/2020].

Levy, P. and Sarne, D. (2018). Understanding over participation in simple contests. In Proceedings of the Thirty-

Second AAAI Conference on Artiﬁcial Intelligence, AAAI’18. AAAI Press.

Lian, J. W., Mattei, N., Noble, R., and Walsh, T. (2018). The conference paper assignment problem: Using order
weighted averages to assign indivisible goods. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, AAAI’18. AAAI Press.

Lin,

H.-T.,

learned
what-we-learned-from-neurips-2020-reviewing-process-e24549eea38f [Accessed: 11/23/2020].

reviewing

from

Hadsell,

R.,

and
process.

Ranzato, M.

we
https://medium.com/@NeurIPSConf/

(2020).

What

Balcan, M.
NeurIPS

F.,
2020

18

McCambridge, J., De Bruin, M., and Witton, J. (2012). The eﬀects of demand characteristics on research participant

behaviours in non-laboratory settings: a systematic review. PloS one, 7(6):e39116.

McCook, A. (2006). Is peer review broken? submissions are up, reviewers are overtaxed, and authors are lodging
complaint after complaint about the process at top-tier journals. what’s wrong with peer review? The scientist,
20(2):26–35.

Mogul, J. C. (2013). Towards more constructive reviewing of SIGCOMM papers. SIGCOMM Comput. Commun.

Rev., 43(3):90–94.

Noothigattu, R., Shah, N. B., and Procaccia, A. D. (2020). Loss functions, axioms, and peer review.

In ICML

workshop on Incentives in Machine Learning.

Papagiannaki, K. (2007). Author feedback experiment at pam 2007. SIGCOMM Comput. Commun. Rev., 37(3):73–78.

Patat, F., Kerzendorf, W., Bordelon, D., Van de Ven, G., and Pritchard, T. (2019). The Distributed Peer Review

Experiment. The Messenger, 177:3–13.

Picciotto, M. (2018). New reviewer mentoring program. Journal of Neuroscience, 38(3):511–511.

Sculley, D., Snoek, J., and Wiltschko, A. B. (2019). Avoiding a tragedy of the commons in the peer review process.

CoRR, abs/1901.06246.

Shah, N. (2019a). Double decker peer review. Research on Research blog. https://researchonresearch.blog/2019/

02/23/double-decker-peer-review/.

Shah, N. B. (2019b). Principled methods to improve peer review.

Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and Von Luxburg, U. (2018). Design and analysis of the NIPS

2016 review process. The Journal of Machine Learning Research, 19(1):1913–1946.

Stelmakh, I., Shah, N., and Singh, A. (2019). On testing for biases in peer review. In NeurIPS.

Stelmakh, I., Shah, N. B., and Singh, A. (2018). PeerReview4All: Fair and accurate reviewer assignment in peer

review. arXiv preprint arXiv:1806.06237.

Stelmakh, I., Shah, N. B., Singh, A., and Daumé III, H. (2020). Prior and prejudice: The novice reviewers’ bias against
resubmissions in conference peer review. Preprint http://www.cs.cmu.edu/afs/cs.cmu.edu/user/istelmak/www/
icml/prior.pdf.

Tomiyama, A. J. (2007). Getting involved in the peer review process. https://www.apa.org/science/about/psa/

2007/06/student-council [Accessed: 9/7/2020].

Toor,

R.

graduate
Reading-Like-a-Graduate/47922 [Accessed: 9/7/2020].

Reading

(2009).

like

a

student.

https://www.chronicle.com/article/

Vardi, M. Y. (2010). Hypercriticality. Communications of the ACM, 53(7):5–5.

Wang, J. and Shah, N. B. (2019). Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings. In

AAMAS.

Weber, E. J., Katz, P. P., Waeckerle, J. F., and Callaham, M. L. (2002). Author perception of peer review: impact

of review quality and acceptance on satisfaction. JAMA, 287(21):2790–2793.

Wilson, E. B. (1927). Probable inference, the law of succession, and statistical inference. Journal of the American

Statistical Association, 22(158):209–212.

Xu, Y., Zhao, H., Shi, X., and Shah, N. (2019). On strategyproof conference review. In IJCAI.

19

Appendix: Additional Evaluation Details

In this section we provide additional details for comparison of experimental reviewers with reviewers from
the main reviewer pool of the ICML 2020 conference. Speciﬁcally, where applicable we replicate comparisons
described in the main paper, conditioning on a target set of papers with at least one experimental reviewer
assigned. Note that this conditioning signiﬁcantly reduces the sample size and hence we may not always
have enough data to establish statistically signiﬁcant diﬀerences. Nevertheless, the results we present below
allow to make some qualitative conclusions.

Before we proceed, recall that the main reviewer pool consists of complementary sets of curated and
self-nominated reviewers and we additionally consider a subset of colleague reviewers who are aﬃliated
with one of the 5 US schools we were recruiting experimental reviewers from.

Review length (Row 3 of Table 1) Bidding activity as well as in-time review submission are independent
of a set of papers assigned to reviewers so we begin our additional analysis from a comparison of mean
review lengths. Figure 8 compares mean lengths of initial reviews across diﬀerent reviewer groups, where
mean values are computed using all reviews and also using reviews written for papers that have at least one
experimental reviewer assigned. Overall, we observe that experimental reviewers write considerably
longer reviews than other reviewers even after conditioning on the aforementioned subset of paper. Table 11
mimics Table 4 with the exception that only papers with experimental reviewers assigned are used for the
comparison.

Figure 8: Mean review lengths (in symbols). Experimental reviewers write longer reviews than other
reviewers.

Experimental Main Pool

Curated

Self-Nominated Colleague

Sample Size
Mean Value
90% CI
P value

154
4759
[4432; 5089]
–

423
2959
[2827; 3098]
< .001

294
3073
[2908; 3248]
< .001

129
2700
[2487; 2928]
< .001

51
2917
[2592; 3261]
< .001

Table 11: Comparison of mean lengths (in symbols) of reviews using papers with at least one experimental
reviewer assigned. P values are for the test of the diﬀerence of means between experimental and each of
the other groups of reviewers.

Hypercriticality (Row 4 of Table 1) Analysis presented in the main paper did not reveal hypercriticality
in experimental reviewers. Table 12 replicates the analysis displayed in Table 5 on the target subset of
papers and also does not show any evidence of hypercriticality in experimental reviewers.

20

CuratedSelf-nominatedColleagueExperimentalReviewer group012345Mean review length×103 symbolsPapersAllWith experimentalreviewersExperimental Main Pool Curated

Self-Nominated Colleague

Sample Size
Mean Value
90% CI
P value

154
3.34
[3.17; 3.51]
–

423
3.22
[3.13; 3.32]
.361

294
3.20
[3.10; 3.32]
.282

129
3.27
[3.09; 3.46]
.691

51
3.12
[2.86; 3.37]
.325

Table 12: Comparison of mean initial overall scores using papers with at least one experimental reviewer
assigned. P values are for the test of the diﬀerence of means between experimental and each of the other
groups of reviewers.

Expertise and conﬁdence (Rows 5 and 6 of Table 1) Figure 9 juxtaposes the mean self-assessed
conﬁdence and expertise of reviewers computed over all papers and over papers with at least one experi-
mental reviewer assigned. Observe that conditioning on the target subset of papers does not change the
conclusions we made in the main paper (see Table 13 for formal comparison).

(a) Mean self-reported expertise. Experimental re-
viewers report considerably lower expertise than other
groups of reviewers.

(b) Mean self-reported conﬁdence. We do not observe
signiﬁcant diﬀerence in mean conﬁdence of diﬀerent re-
viewer groups.

Figure 9: Comparison of self-assessed expertise and conﬁdence.

Criteria

Experimental Main Pool Curated

Self-Nominated Colleague

Expertise

Confidence

Sample Size
Mean Value
90% CI
P value
Sample Size
Mean Value
90% CI
P value

154
2.83
[2.73; 2.94]
–

154
3.05
[2.97; 3.13]
–

423
2.96
[2.91; 3.02]
.064

423
3.05
[3.00; 3.11]
.993

294
3.01
[2.94; 3.07]
.021

294
3.06
[3.00; 3.13]
.878

129
2.87
[2.77; 2.97]
.737

129
3.03
[2.92; 3.14]
.863

51
3.00
[2.84; 3.16]
.211

51
2.98
[2.82; 3.14]
.556

Table 13: Comparison of self-assessed expertise (ﬁrst 4 rows) and conﬁdence (last 4 rows) using papers with
at least one experimental reviewer assigned. P values are for the test of the diﬀerence of means between
experimental and each of the other groups of reviewers.

Rebuttals and discussion (Rows 7 and 8 of Table 1) We now provide additional details on comparison
of reviewers’ activity in the post-rebuttal stage of the conference peer-review process. Recall that for this
comparison we use only paper-reviewer pairs such that the authors of the paper supplied the response to the
initial reviews. Figure 10 replicates Figure 6 with the exception that it is computed using papers that have

21

CuratedSelf-nominatedColleagueExperimentalReviewer group1.01.52.02.53.03.5Mean self-reportedexpertisePapersAllWith experimentalreviewersCuratedSelf-nominatedColleagueExperimentalReviewer group1.01.52.02.53.03.5Mean self-reportedconfidence(a) Participation in discussion.

(b) Post-rebuttal review update.

Figure 10: Activity of reviewers in the last stage of the review process conditioned on papers with at least
one experimental reviewer assigned. Experimental reviewers participate in the discussion and update
reviews more actively than other reviewers.

Criteria

Experimental Main Pool Curated

Self-Nominated Colleague

Sample Size
Discussion Mean Value
Activity

90% CI
P value
Sample Size
Mean Value
90% CI
P value

123
0.68
[0.61; 0.75]
–

123
0.61
[0.54; 0.68]
–

337
0.56
[0.52; 0.61]
.026

337
0.43
[0.39; 0.48]
.002

238
0.59
[0.54; 0.64]
.119

238
0.50
[0.44; 0.55]
.050

99
0.48
[0.40; 0.57]
.003

99
0.28
[0.21; 0.35]
< .001

41
0.59
[0.46; 0.71]
.338

41
0.41
[0.29; 0.54]
.045

Review
Update

Table 14: Comparison of post-rebuttal reviewer activity using papers with at least one experimental
reviewer assigned: participation in discussion (ﬁrst 4 rows) and review update rate (last 4 rows). P values
are for the test of the diﬀerence of means between experimental and each of the other groups of reviewers.

at least one experimental reviewer assigned. Note that even after conditioning on this subset of papers,
experimental reviewers remain to be more active in the post-rebuttal stage of the review process than
other categories of reviewers. (see Table 14 for the formal comparison).

Review quality (Row 9 of Table 1) We conclude the analysis with comparison of review quality
evaluated by meta-reviewers. First, Figure 11 replicates Figure 7 with the exception that we condition on
papers that have at least one experimental reviewer assigned. As before, conditioning on this subset of
papers does not change the qualitative relationship between diﬀerent groups of reviewers with experimental
pool dominating others. Getting back to the comparison of the mean quality scores that we reported in
Tables 1 and 8, we now complement this analysis with results reported in Table 15. Again, we conclude that
experimental reviewer remain to have higher mean quality of reviews even after we equalize the sets of
meta-reviewers who rate the reviews written by diﬀerent groups of reviewers.

22

CuratedSelf-nominatedColleagueExperimentalReviewer group0.00.20.40.60.81.0Discussion activity rateCuratedSelf-nominatedColleagueExperimentalReviewer group0.00.20.40.60.81.0Review update rateFigure 11: Evaluation of mean review quality by meta-reviewers conditioned on papers with at least one
experimental reviewer assigned. The closer the point to the upper-left corner, the better. Experimental
reviewers dominate other groups of reviewers.

Experimental Main Pool Curated

Self-Nominated Colleague

Sample Size
Mean Value
90% CI
P value

111
2.26
[2.18; 2.34]
–

310
2.11
[2.06; 2.15]
.008

214
2.14
[2.08; 2.20]
.058

96
2.03
[1.95; 2.11]
.002

32
2.16
[2.00; 2.31]
.431

Table 15: Comparison of mean review qualities as evaluated by meta-reviewers using papers with at least one
experimental reviewer assigned. P values are for the test of the diﬀerence of means between experimental
and each of the other groups of reviewers.

23

0.000.020.040.060.080.100.120.140.16Fraction of "below expectation" reviews0.000.050.100.150.200.250.300.350.40Fraction of"above expectation" reviewsReviewer groupExperimentalCuratedSelf-nominatedColleague