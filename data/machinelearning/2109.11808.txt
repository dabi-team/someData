2
2
0
2

n
a
J

0
2

]

G
L
.
s
c
[

3
v
8
0
8
1
1
.
9
0
1
2
:
v
i
X
r
a

A DYNAMIC PROGRAMMING ALGORITHM FOR FINDING AN
OPTIMAL SEQUENCE OF INFORMATIVE MEASUREMENTS

A PREPRINT

Peter N. Loxley
University of New England
Armidale, NSW, Australia.

Ka Wai Cheung
University of New England
Armidale, NSW, Australia.

ABSTRACT

An informative measurement is the most efﬁcient way to gain information about an unknown state.
We give a ﬁrst-principles derivation of a general-purpose dynamic programming algorithm that re-
turns an optimal sequence of informative measurements by sequentially maximizing the entropy
of possible measurement outcomes. This algorithm can be used by an autonomous agent or robot
to decide where best to measure next, planning a path corresponding to an optimal sequence of
informative measurements. The algorithm is applicable to states and controls that are continuous
or discrete, and agent dynamics that is either stochastic or deterministic; including Markov deci-
sion processes and Gaussian processes. Recent results from approximate dynamic programming
and reinforcement learning, including on-line approximations such as rollout and Monte Carlo tree
search, allow the measurement task to be solved in real-time. The resulting solutions include non-
myopic paths and measurement sequences that can generally outperform, sometimes substantially,
commonly used greedy approaches. This is demonstrated for a global search problem, where on-
line planning with an extended local search is found to reduce the number of measurements in the
search by approximately half. A variant of the algorithm is derived for Gaussian processes for active
sensing.

Keywords Information theory · Approximate dynamic programming · Active learning · Autonomous agent

1

Introduction

Observing the outcomes of a sequence of measurements usually increases our knowledge about the state of a particular
system we might be interested in. An informative measurement is the most efﬁcient way of gaining this information,
having the largest possible statistical dependence between the state being measured and the possible measurement
outcomes. Lindley ﬁrst introduced the notion of the amount of information in an experiment, and suggested the
following greedy rule for experimentation: perform that experiment for which the expected gain in information is the
greatest, and continue experimentation until a preassigned amount of information has been attained [Lindley, 1955].

Greedy methods are still the most common approaches for ﬁnding informative measurements, being both simple to
implement and efﬁcient to compute. For example, in a weighing problem where an experimenter has a two-pan
balance and is given a set of balls of equal weight except for a single odd ball that is heavier or lighter than the others
(see Figure 1), the experimenter would like to ﬁnd the odd ball in the fewest weighings. MacKay suggested that
for useful information to be gained as quickly as possible, each stage of an optimal measurement sequence should
have measurement outcomes as close as possible to equiprobable [MacKay, 2003]. This is equivalent to choosing the
measurement at each stage (i.e., the distribution of balls on pans) corresponding to the measurement outcome with
largest entropy.

 
 
 
 
 
 
A dynamic programming algorithm for informative measurement sequences A PREPRINT

Figure 1: In the weighing problem, an experimenter has a two-pan balance and a set of balls of equal weight except
for a single odd ball that is heavier or lighter than the others. The unknown state is the identity of the odd ball, and the
possible measurement outcomes are “left-pan heavier”, “right-pan heavier”, or “balanced” if the odd ball is not on a
pan. The objective is to ﬁnd the odd ball in the fewest weighings.

Less well-recognized is the fact that greedy approaches can sometimes lead to suboptimal measurement sequences.
This is not usually the case for simple systems such as in the weighing problem described above. However, in other
cases, an optimal sequence of measurements may involve trade-offs where earlier measurements in the sequence lead
to modest information gains, rather than the maximum gains attainable at those stages, so that later measurements
can access larger information gains than would otherwise be possible. A greedy approach never allows for such
trade-offs. This is seen in the robotics literature, such as for path planning and trajectory optimization for robots
building maps from sensor data [Placed and Castellanos, 2020, Kollar and Roy, 2008], path planning for active sensing
[Low et al., 2009, Cao et al., 2013], and robots exploring unknown environments [Ay et al., 2008, Kollar and Roy,
2008]. These approaches generally have in common the application of information theory objectives, and the use of
sequential optimization methods such as dynamic programming or reinforcement learning [Bertsekas, 2017, 2020,
Sutton and Barto, 2018]. Such methods speciﬁcally allow for the possibility of delayed information gains and non-
myopic paths, and attempt to deal with the combinatorial nature of the underlying optimization problem. Exact
dynamic programming is often computationally prohibitive or too time-consuming due to the need to consider all
states of the problem. However, introducing efﬁcient parametric approximations [Bertsekas and Tsitsiklis, 1996,
Bertsekas, 2017, Loxley, 2021], on-line approximations [Tesauro and Galperin, 1997, Bertsekas, 2017, 2020], or real-
time heuristics [Barto et al., 1995], allows very good suboptimal solutions (also called near-optimal solutions) to be
found.

The aim of this work is to develop from ﬁrst-principles a general-purpose dynamic programming algorithm for ﬁnding
optimal sequences of informative measurements. We ignore measurement problems with hidden states, such as hidden
Markov models, that can only be observed indirectly through noisy measurements. This leads to a tractable algorithm
that constructs an optimal sequence of informative measurements by sequentially maximizing the entropy of mea-
surement outcomes. In addition to planning a sequence of measurements for a particular measurement problem, our
algorithm can also be used by an autonomous agent or robot exploring a new environment to plan a path giving a near-
optimal sequence of informative measurements. The framework we use for dynamic programming is very general,
and includes Markov decision processes (MDPs) as a special case: allowing the agent dynamics to be either stochastic
or deterministic, and the states and controls to be continuous or discrete. Our general dynamic programming frame-
work also allows suboptimal solution methods from approximate dynamic programming and reinforcement learning
to be applied in a straightforward manner, avoiding the need to develop new approximation methods for each speciﬁc
situation.

The closest related previous work is that of Low et al. [2009] and Cao et al. [2013] in the active-sensing domain.
Active sensing is the task of deciding what data to gather next [MacKay, 1992], and is closely related to explorative
behavior [Little and Sommer, 2011, 2013]: for example, should an animal or agent gather more data to reduce its
ignorance about a new environment, or should it omit gathering some data that are expected to be least informative. The
work of Low et al. [2009] and Cao et al. [2013] models spatially-varying environmental phenomena using Gaussian
processes [Rasmussen and Williams, 2006, Gibbs, 1997, MacKay, 2003]. Informative sampling paths are planned
using various forms of dynamic programming. Gaussian processes are also described within our general dynamic
programming framework. However, in the work of Low et al. [2009] and Cao et al. [2013], it is not clear that dynamic
programming is necessary for ﬁnding informative paths, as greedy algorithms are shown to achieve comparable results
for the examples shown [Low et al., 2009, Cao et al., 2013]. Krause et al. [2008] showed that ﬁnding optimal static
sensor placements for a Gaussian process requires solving a combinatorial optimization problem that is NP-complete
[Krause et al., 2008]. Nevertheless, a greedy algorithm can achieve near-optimal performance when the information

2

A dynamic programming algorithm for informative measurement sequences A PREPRINT

objective has the property of being submodular and monotonic [Krause et al., 2008]. Further, for a particular path
planning problem involving a Gaussian process, Singh et al. [2009] presented an efﬁcient recursive-greedy algorithm
that attains near-optimal performance. In this work, we demonstrate a simple path planning problem that cannot be
solved efﬁciently using a greedy algorithm. We also demonstrate the existence of delayed information gains in this
example, implying that solution methods must take into account trade-offs in information gains to ﬁnd optimally
informative paths.

The structure of the paper is as follows.
In Section 2, the general dynamic programming algorithm is developed
from ﬁrst-principles. The algorithm is then applied to two well-known examples in Section 3 in order to illustrate
the approach and present some exact solutions given by our algorithm. In Section 4, the algorithm is approximated
by making use of approximate dynamic programming methodology to allow for real-time behavior of an autonomous
agent or robot, and it is applied to a scaled-up on-line example where a greedy approach is shown to fail. In Section
5, a slight modiﬁcation to the algorithm is given to describe Gaussian processes for active sensing and robot path
planning. A conclusion is presented in Section 6.

2 Sequential maximization of entropy

To determine the kinds of measurements that are most informative, we introduce two random variables X and M ;
where X describes the state of the system of interest and is deﬁned over some suitable state-space, and M describes
the outcome of a measurement made on that system and is deﬁned over the space of possible measurement outcomes.
These random variables are dependent, because we hope a measurement outcome M provides us with some infor-
mation about the state X. This dependence can be described using the mutual information [Shannon, 1948], given
by

I(X; M ) = H(X) − H(X|M ),

(1)

where the entropy H(X) is the average uncertainty in the random variable X describing the state, and H(X|M ) is
the average uncertainty in X that remains after observing the outcome M of a measurement performed on X. This
means I(X; M ) is the average reduction in uncertainty about X that results from knowledge of M [MacKay, 2003].
Its minimum value given by I(X; M ) = 0 is obtained when X and M are independent random variables, while its
maximum value: I(X; M ) = H(X) is obtained when H(X|M ) = 0; so that knowing M tells you everything there
is to know about X.

Given deﬁnitions for X and M corresponding to a particular system of interest, we would like to choose the mea-
surement that maximizes I(X; M ) in order to reduce our uncertainty about X and obtain the maximum amount of
information available. To address this problem, we will now focus exclusively on measurement outcomes satisfying

H(M |X) = 0,

(2)

– so that given knowledge of the state X, the measurement outcome M of that state is fully determined (i.e., the
uncertainty over M is zero). For example, in the weighing problem shown in Figure 1, if you know the state of the
odd ball is X = “heavy ball on left pan”, then you know with absolute certainty that the measurement outcome will
be M = “left-pan heavier”. This assumption means the unknown state is always visible and can be observed directly,
unlike the case of a hidden Markov model or state-space model where the unknown state is hidden and can only
be observed indirectly through a noisy measurement. The mutual information I(X; M ) can be written equivalently
as I(X; M ) = H(M ) − H(M |X), so that maximizing I(X; M ) over the probability distribution of measurement
outcomes, pM (m), leads to

max
pM

I(X; M ) = max
pM

H(M ),

(3)

when H(M |X) = 0. Therefore, pM (m) is given by a maximum entropy distribution [Jaynes, 1957].

The entropy maximization will usually need to be done over a sequence of measurements, as it is unlikely that a
single measurement will be sufﬁcient to determine X precisely when either a large number of states are present, or
when measurement resolution is limited. For example, using a two-pan balance to ﬁnd the odd ball in the weighing
problem usually requires a sequence of weighings. Extending our approach from a single measurement with outcome
M , to a sequence of measurements with outcomes M0, ..., MN −1, we look for a sequence of probability distributions
{pM0, ..., pMN −1} that maximize the joint entropy H(M0, ..., MN −1). The key observation is that in many cases of
interest this maximization can be carried out sequentially. Applying the chain rule for entropy [?], leads to

max
{pM0 ,..., pMN −1 }

N −1
(cid:88)

k=0

Hk(Mk|Mk−1, ..., M0).

(4)

3

A dynamic programming algorithm for informative measurement sequences A PREPRINT

It is now straightforward to see that if the Mk can be modelled as independent random variables, then we only need to
ﬁnd a sequence of probability distributions that maximize a sum of independent entropies:

max
{pM0 ,..., pMN −1 }

N −1
(cid:88)

k=0

Hk(Mk),

(5)

which is a much simpler task that can be done sequentially. The maximization in (5) will be written as a dynamic
program in the next section. Alternatively, if the Mk are dependent random variables, sequential maximization of
equation (4) can still be done approximately using the methods of approximate dynamic programming. A slight
modiﬁcation of the dynamic programming algorithm developed in the next sections will allow for this, as shown in
Section 5 for the case of a Gaussian process.

2.1 Dynamic programming algorithm

Dynamic programming [Bertsekas, 2017] is a general technique for carrying out sequential optimization. Provided
the objective function can be decomposed as a sum over independent stages as in (5), the principle of optimality
guarantees that optimization can be performed sequentially by starting at the ﬁnal stage (the tail subproblem) and
working backwards towards the initial stage, at each stage using the solution of the previous subproblem to help ﬁnd
the solution to the current subproblem (for this reason they are called overlapping subproblems). The equation (5) is
close to “the basic problem” of dynamic programming (DP), and we will adopt the notation commonly used in DP.

In order to maximize the sum of entropies in equation (5), we will need to introduce a set of parameters to maximize
over. The probability distribution pM (m) is now assumed to depend on the parameters x and u (note that x is not
related to the random variable X, but is standard notation in DP), giving pM (m) = p(m|x, u). For a sequence of N
independent measurements, the kth probability distribution then becomes,

pMk (mk) = pk(mk|xk, uk),

(6)

k=0 and {uk}N −1

where {xk}N −1
k=0 are sets of parameters allowing the sequence of probability distributions {p0, ..., pN −1}
to vary according to the measurement chosen at each stage of the sequence. Each parameter uk ∈ Uk(xk) is chosen
from the set Uk(xk) of all measurements possible at stage k and xk, while each parameter xk ∈ Sk is then updated
according to the discrete dynamical system:

xk+1 = fk(xk, uk, mk).

(7)

In other words, the “measurement state” xk+1 determines how the set of possible measurements Uk(xk) changes
to Uk+1(xk+1) when going from stage k to stage k + 1 as a result of the measurement uk chosen at stage k, and
the measurement outcome mk that is realized. Allowing the set of possible measurements to change at each stage
in a predictable manner is a deﬁning feature of our model. To allow for closed-loop maximization where this extra
information can be used at each stage, we deﬁne a sequence of functions µk that map xk into uk = µk(xk). A policy
or a design is then given by a sequence of these functions, one for each measurement:

Maximizing over policies, and adding a terminal entropy HN for stage N , allows equation (5) to be written as

π = {µ0(x0), ..., µN −1(xN −1)}.

max
π

N −1
(cid:88)

k=0

Hk(Mk) + HN .

(8)

The ﬁnal step is to write the objective in (8) as an expectation. This can be done using the fact that entropy is the
expected value of the information content:

H(M ) = E

(cid:18)

log2

(cid:19)

,

1
p(m)

where the expectation is taken over all measurement outcomes m ∈ Im M , and log2 (1/p(m)) is the information
content of measurement outcome M = m. Expressing the entropies in (8) in terms of expectations, then using the
linearity of expectation and the fact that N is ﬁnite to interchange the summation and the expectation, now leads to an
expression for the maximum expected value of N information contents:

E

max
π

(cid:40)N −1
(cid:88)

k=0

hk(xk, µk(xk), mk) + hN (xN )

,

(9)

(cid:41)

4

A dynamic programming algorithm for informative measurement sequences A PREPRINT

where the expectation is over m0, ..., mN −1, and the information content of the kth measurement is given by

hk(xk, µk(xk), mk) = log2

1
pk(mk|xk, µk(xk))

.

(10)

We can now invoke the Dynamic Programming Algorithm [Bertsekas, 2017] in order to perform the maximization
in (9). Starting with the terminal condition JN (xN ) = hN (xN ), the DP algorithm corresponding to (9) proceeds
backwards in time by evaluating the recurrence relation:

Jk(xk) = max

uk∈Uk(xk)

E
mk

(cid:110)

hk(xk, uk, mk) + Jk+1(fk(xk, uk, mk))

(11)

(cid:111)
,

from the ﬁnal stage k = N − 1 to the initial stage k = 0. This procedure is given in Algorithm 1. The maximization
in equation (11) is over all measurements uk ∈ Uk(xk) possible at xk and stage k, while the expectation is over all
measurement outcomes mk ∈ Im Mk, and the function hk(xk, uk, mk) is the information content of measurement
outcome Mk = mk:

hk(xk, uk, mk) = log2

.

(12)

1
pk(mk|xk, uk)

The last step of the algorithm returns J0(x0), giving the maximum entropy of N measurements. The optimal mea-
surement sequence is given by the sequence u∗
k(xk) that maximizes the right hand side of equation (11) for each
xk and k.

k = µ∗

Algorithm 1 Dynamic Programming Algorithm

JN (xN ) ← hN (xN )
for k = N − 1 to 0 do
for all xk ∈ Sk do

Jk(xk) ← max

uk∈Uk(xk)

E
mk

k(xk) ← u∗
µ∗
k

end for

end for

(cid:110)

hk(xk, uk, mk) + Jk+1(fk(xk, uk, mk))

(cid:111)

There are two alternative ways to apply the DP algorithm that are both consistent with optimizing the objective in
equation (9). If we know the number of measurements N , we can iterate the DP recurrence relation over N stages to
determine the maximum information gained from N measurements. In this case, the maximum information gained,
J0, is unknown until the algorithm completes. Alternatively, if we know how much information we need to gain but
would like to do so using the minimum number of measurements, we can iterate the DP recurrence relation over a
number of stages until we ﬁrst reach this pre-assigned amount of information; whereupon we terminate the algorithm
and read off the value of N . In this case, the number of measurements N is unknown until the algorithm is terminated.
It is easy to see that if we now use this value of N in the DP algorithm, J0 will equal the pre-assigned amount of
information. Are there smaller values of N that would also achieve this? By construction we stopped the algorithm
at the ﬁrst stage we gained the required amount of information, so stopping before this would lead to less information
than required. We use both of these alternatives in Sections 3 and 4.

2.2 Extended dynamic programming algorithm for an autonomous agent

The previous DP algorithm allows us to ﬁnd an optimal sequence of independent measurements by maximizing the
entropy of N independent measurement outcomes. We now include a simple extension that allows an autonomous
agent to ﬁnd an optimal sequence of independent measurements as it explores a new environment. This opens up
additional possibilities where dynamic programming may play a more substantial role.
An agent moving through an environment is described by its position x(cid:48)
to take control u(cid:48)
system:

k at time k. The agent then decides
k+1 at time k + 1 according to the following dynamical

k), moving it to a new position x(cid:48)

k ∈ S(cid:48)

k ∈ U (cid:48)

k(x(cid:48)

(13)
where wk ∈ Dk describes a random “disturbance” to the agent dynamics if the dynamics is stochastic. Coupling the
agent dynamics to a sequence of measurements is achieved by making the following replacement:

k, wk),

k+1 = vk(x(cid:48)
x(cid:48)

k, u(cid:48)

Uk(xk) (cid:55)→ Uk(x(cid:48)

k, xk),

(14)

5

A dynamic programming algorithm for informative measurement sequences A PREPRINT

– so the set of all measurements possible at stage k now also depends on the position of the agent in the environment
at time k. The agent is assumed to take one measurement at each time step, so the horizon of the agent dynamics
is equivalent to the number of measurements in a measurement sequence. It is possible to relax this assumption by
introducing an extra index k(cid:48) that distinguishes the horizon of the agent dynamics from the horizon of the measurement
sequence, but we choose not to do this here. The DP recurrence relation given by (11) now becomes,

Jk(x(cid:48)

k, xk) = max
k(x(cid:48)
k)
+Jk+1(vk(x(cid:48)

k∈U (cid:48)
u(cid:48)

E
wk

(cid:110)

max
uk∈Uk(x(cid:48)

k,xk)

(cid:110)

hk(xk, uk, mk)

E
mk

k, u(cid:48)

k, wk), fk(xk, uk, mk))

(cid:111)(cid:111)
,

(15)

k, xk) now depends on both x(cid:48)
k, in order to allow the agent to move from x(cid:48)

where Jk(x(cid:48)
tion over u(cid:48)
JN (x(cid:48)
k = N − 1 to stage k = 0. This procedure is given in Algorithm 2. The last step of the algorithm returns J0(x(cid:48)

k and xk; and there is an additional expectation over wk, and maximiza-
k+1. The corresponding DP algorithm starts with
N , xN ), and proceeds backwards in time by evaluating the recurrence relation (15) from stage
0, x0),

N , xN ) = hN (x(cid:48)

k to x(cid:48)

Algorithm 2 Extended Dynamic Programming Algorithm
N , xN )

N , xN ) ← hN (x(cid:48)
JN (x(cid:48)
for k = N − 1 to 0 do
k, xk) ∈ S(cid:48)
k, xk) ← max
u(cid:48)
k∈U (cid:48)
k(x(cid:48)
k, xk) ← u∗
k
k) ← u(cid:48)∗
k

for all (x(cid:48)
Jk(x(cid:48)
k(x(cid:48)
µ∗
k (x(cid:48)
µ(cid:48)∗

k × Sk do
E
wk

k)

(cid:110)

max
uk∈Uk(x(cid:48)

k,xk)

(cid:110)

E
mk

hk(xk, uk, mk) + Jk+1(vk(x(cid:48)

k, u(cid:48)

k, wk), fk(xk, uk, mk))

(cid:111)(cid:111)

end for

end for

0, x0) should also be maximized over x(cid:48)
k(x(cid:48)

the maximum entropy of N measurements made by an autonomous agent. Given a choice of values for x(cid:48)
J0(x(cid:48)
quence is given by the sequence u∗
x(cid:48)
k and xk at each k, and the autonomous agent dynamics is determined by the sequence u(cid:48)∗
the right hand side of equation (15) for each x(cid:48)

0, then
0, x0). The optimal measurement se-
0 to give: J0(x0) = maxx(cid:48)
k, xk) that jointly maximizes the right hand side of equation (15) for each
k) that maximizes

k at each k. The objective maximized by Algorithm 2 is given by

k = µ(cid:48)∗

k = µ∗

J0(x(cid:48)

k (x(cid:48)

0

(cid:40)

E(cid:48)

max
π(cid:48)

E

max
π

(cid:40)N −1
(cid:88)

k=0

hk(xk, µk(x(cid:48)

k, xk), mk) + hN (x(cid:48)

N , xN )

,

(16)

(cid:41)(cid:41)

as shown in the Appendix. In (16), the policies π(cid:48) = {µ(cid:48)
functions, where

0, ..., µ(cid:48)

N −1} and π = {µ0, ..., µN −1} are sequences of

k(x(cid:48)
µ(cid:48)

k) = u(cid:48)
k,

and µk(x(cid:48)

k, xk) = uk,

while E(cid:48) is an expectation over w0, ..., wN −1, and E is an expectation over m0, ..., mN −1.
The extended DP algorithm in Algorithm 2 looks computationally formidable due to the potentially large number
of evaluations of (15) required. Fortunately, the constraint given by uk ∈ U (x(cid:48)
k, xk) will often limit the number of
feasible measurement states xk for a given agent position x(cid:48)
k, so that instead of |S(cid:48)
k × Sk| potential evaluations of (15),
there will be some smaller multiple of |S(cid:48)

k| evaluations required.

3

Illustrative examples and exact solutions

In this section, we work through two examples called the weighing problem, and ﬁnd the submarine using the proposed
dynamic programming algorithm (a third example, guess my number, is provided in the appendix). The ﬁrst example
illustrates Algorithm 1, while the second example illustrates the extension given by Algorithm 2. These examples only
cover the cases of discrete states and controls, and deterministic agent dynamics; whilst Algorithms 1 and 2 can also
be used when states and controls are continuous, and when the agent dynamics is stochastic. All three examples are
taken from Chapter 4 of MacKay [2003], with slight modiﬁcation.

6

A dynamic programming algorithm for informative measurement sequences A PREPRINT

3.1 The weighing problem

The ﬁrst example is the weighing problem we considered in Figure 1. In the general version of this problem the
unknown odd ball can be either heavy or light. Here, we simplify the problem so that the odd ball is always a heavy
ball. The weighting problem is now this: given a set of balls of equal weight except for a single heavy ball, determine
the minimum number of weighings of a two-pan balance that identiﬁes the heavy ball.

Let X ∈ {1, ..., n} be the label of the heavy ball, and let M be the outcome of a weighing, taking one of the values:
“left-pan heavier”, “right-pan heavier”, or “balanced”. If the outcome of a particular weighing is “balanced”, then the
heavy ball is one of the balls left off the two-pan balance. We also make the following deﬁnitions:

xk = total number of balls to be weighed at stage k,
uk = number of balls on both balance pans at stage k,

– as well as assuming there are an equal number of balls on each pan so that uk is even (otherwise, a weighing exper-
iment leads to a trivial result). If every ball is equally likely to be the heavy ball, then the following parameterizations
hold:

pk(mk|xk, uk) =

(cid:26)uk/2xk

mk = “left-pan heavier” or “right-pan heavier”,

(xk − uk)/xk mk = “balanced”,

fk(xk, uk, mk) =

(cid:26)uk/2

mk = “left-pan heavier” or “right-pan heavier”,

xk − uk mk = “balanced”.

Here, pk(mk|xk, uk) is simply the number of balls leading to measurement outcome M = mk, divided by the
total number of balls weighed at stage k. The number of balls to be weighed at the next stage, xk+1, is then
fk(xk, uk, mk) = pk(mk|xk, uk)xk. With these deﬁnitions and parameterizations, equations (11) and (12) lead to
the DP recurrence relation:

Jk(xk) = max
uk∈U +

E (xk)

(cid:18)

(cid:26) uk
xk

log2

2xk
uk

+ Jk+1

(cid:17)(cid:19)

(cid:16) uk
2

+

xk − uk
xk

(cid:18)

log2

xk
xk − uk

+ Jk+1 (xk − uk)

,

(17)

(cid:19)(cid:27)

where U +

E (xk) is the set {2, 4, ..., xk} if xk is even, and {2, 4, ..., xk − 1} if xk is odd.

Following the principle of optimality, the DP algorithm given by Algorithm 1 starts at the ﬁnal stage with terminal
condition JN (1) = 0 bits, and proceeds backwards in time. The subproblem for measurement N − 1 can be solved by
evaluating equation (17) for xN −1 = 2,

JN −1(2) = log2 2 + JN (1) ,

= 1 bit. (u∗

N −1 = 2)

In a similar manner,

JN −1(3) =

2
3

(log2 3 + JN (1)) +

(log2 3 + JN (1)) ,

= log2 3 bits. (u∗

1
3
N −1 = 2)

The subproblem for xk = 4 now requires Jk+1(2) according to equation (17), so that JN −1(2) becomes an overlapping
subproblem if we move to measurement k = N − 2, giving:
(cid:26) 1
2

(log2 4 + JN −1 (1)) +

(log2 2 + JN −1 (2)) ,

log2 2 + JN −1 (2)

JN −2(4) = max

(cid:27)

,

= 2 bits. (u∗

N −2 = 2 or u∗

1
2
N −2 = 4)

(18)

We now have the exact DP solution to the weighing problem for four balls. The DP solution can be continued in
this way by increasing both the number of balls and the number of measurements. As there are three states of the
balance, and n states for the n possibilities where one of the balls is the heavy ball, the upper bound on the entropy
(corresponding to equiprobable outcomes) is log2 3 bits per weighing for M , and log2 n bits for X. Therefore, the
heavy ball is guaranteed to be found after a number of weighings equal to (cid:100)log2 n/ log2 3(cid:101), where the ceiling function
(cid:100).(cid:101) rounds up to the closest integer.
The new contribution from DP can be seen in the two alternative solutions for JN −2(4) in (18). In Solution 1 (u∗
N −2 =
2), we place one ball on each pan in the ﬁrst weighting, and two balls are kept off the two-pan balance. With probability

7

A dynamic programming algorithm for informative measurement sequences A PREPRINT

×
× × ×
×

1

4

7

2

5

8

3

6

9

Figure 2: (Left) Sonar search pattern for a ship in “ﬁnd the submarine”. The red square is the position of the ship, and
the “×” symbols indicate the grid squares that are searched in a single measurement. (Right) Grid coordinates used
for the position of the ship and the submarine.

0.5, we will “get lucky” by ﬁnding the heavy ball on the ﬁrst weighing and immediately gain 2 bits of information.
If not, then the heavy ball will be found on the second weighing.
In addition to maximizing the entropy of two
measurements, this solution also minimizes the average number of weighings to ﬁnd the heavy ball.
In Solution 2 (u∗
N −2 = 4), we place two balls on each pan in the ﬁrst weighing, the outcome informing us which
pair contains the heavy ball. The identity of the heavy ball is then resolved by placing each ball from this pair on a
separate pan in the second weighing. However, there is no chance to “get lucky” since this solution always requires
two weighings.

Both of these solutions are optimal and maximize the entropy over two measurements (each giving 2 bits of infor-
mation); however, Solution 1 has a larger entropy for the ﬁrst measurement (1.5 bits versus 1 bit), while Solution
2 spreads the entropy more evenly between the two measurements. Therefore, seeking the most informative set of
weighings by maximizing the entropy of each measurement, as in MacKay [2003], would lead only to Solution 1.
Our DP algorithm ﬁnds all of the optimal solutions and therefore provides a more rigorous approach to solving this
problem.

3.2 Find the submarine

In the second example, a ship (treated as an autonomous agent) moves on a 3 × 3 grid and uses its sonar to attempt
to locate a stationary submarine positioned at a hidden location. To make the problem more interesting, we allow the
sonar to search ﬁve neighbouring squares in a single measurement using the extended local search pattern shown in
Figure 2. If the ship is located on a grid boundary, or if one of the neighbouring squares has already been searched in a
previous measurement, then fewer than ﬁve squares will contribute new information to the current search (see Figures
3-5). Further, if the submarine is located in any one of the ﬁve sonar search squares, we assume its precise location
has been successfully determined. In these type of games the ship can usually move to any square on the grid in one
move. Instead, we choose a more realistic agent dynamics that obeys

k + u(cid:48)
k,
k is the position of the ship on the 3 × 3 grid at time k, and u(cid:48)

where x(cid:48)
directions by two squares, or along a diagonal by one square.

k+1 = x(cid:48)
x(cid:48)

k is a movement either along one of the Cartesian

(19)

A reasonable approach to this problem might be to position the ship to search the largest possible area in the ﬁrst
measurement, corresponding to a greedy approach. Instead, the DP algorithm proposed here will maximize the entropy
over the whole sequence of measurements, allowing for trade-offs in measurements over different stages. Since the
agent dynamics given by (19) is deterministic, the expectation over wk in equation (15) vanishes. It is also the case
that the maximum over uk in (15) is unnecessary in this example because there is no set of measurements to maximize
over at each stage, only the sonar with its ﬁxed search pattern. The DP recurrence relation (15) therefore simpliﬁes to:

Jk(x(cid:48)

k, xk) = max
k(x(cid:48)

k∈U (cid:48)
u(cid:48)

k)

(cid:110)

hk(xk, uk(x(cid:48)

k), mk) + Jk+1(vk(x(cid:48)

k, u(cid:48)

k), fk(xk, uk(x(cid:48)

k), mk))

(cid:111)
.

(20)

E
mk

Let the random variable X be the position of the submarine on the 3 × 3 grid using the grid coordinates shown in
Figure 2, and let M return “yes” or “no” to the question: “Is the submarine detected by the sonar?”. If the answer is
“yes”, we assume the agent knows which particular square the submarine is located on, as previously mentioned. We
then make the following deﬁnitions:

xk = number of possible locations of submarine at stage k,
uk = number of new locations searched by sonar at stage k,

8

A dynamic programming algorithm for informative measurement sequences A PREPRINT

×
× × ×
×

×
× × ×
××

×
× × ×
×× ×

×
×
× × ×
×× ×

Figure 3: A search pattern initiated by a ship in the center of the grid. The measurement sequence starts at the left-
most grid illustration at time k = 0, and ﬁnishes at the right-most grid illustration at time k = 3. The uk sequence is
5, 1, 1, 1 so that four measurements are guaranteed to locate the submarine. See text for details.

×
× ×
×

×
× ×
×

×
×
×

×
× ×
×

×
×
××

Figure 4: A search pattern initiated by a ship at the left edge of the grid. In this case, the uk sequence is 4, 3, 1 so that
three measurements are guaranteed to locate the submarine.

and the following parameterizations:

pk(mk|xk, uk) =

(cid:26)uk/xk

mk = “yes”,
(xk − uk)/xk mk = “no”,

fk(xk, uk, mk) =

(cid:26)uk

mk = “yes”,
xk − uk mk = “no”.

With these deﬁnitions and parameterizations, equations (12) and (20) lead to the following DP recurrence relation:

Jk(x(cid:48)

k, xk) = max

u(cid:48)
k

(cid:26) uk
xk

(cid:18)

log2

xk
uk
xk − uk
xk
xk − uk
xk

+

+ Jk+1 (x(cid:48)

k + u(cid:48)

k, uk)

(cid:19)

(cid:18)

log2

xk
xk − uk
(cid:0) log2 (xk − uk) − max

u(cid:48)
k

+ Jk+1 (x(cid:48)

k + u(cid:48)

k, xk − uk)

(cid:19)(cid:27)

,

= log2 xk −

Jk+1 (x(cid:48)

k + u(cid:48)

k, xk − uk) (cid:1),

(21)

k + u(cid:48)

k) ∈ {0, 1, ..., 5} depends on the location of the ship x(cid:48)

where uk = uk(x(cid:48)
k at stage k relative to the grid boundaries,
and on how many new locations can be searched by the sonar from that location. In the second equation, the term
Jk+1(x(cid:48)
k, uk) has been replaced with log2 uk because an answer of “yes” implies the precise location of the
submarine has been determined, immediately yielding log2 uk bits of information.
Before applying the DP algorithm, let’s consider some possible ship paths and search patterns. In Figure 3, the ship
moves to the center square and searches the largest possible area with its ﬁrst measurement at time k = 0, giving
x(cid:48)
0 = 5, x0 = 9, and u0 = 5. If the submarine is not found, the ship then searches the remaining squares in the
next time periods using further measurements. For example, at time k = 1, the ship moves diagonally to the bottom-
left corner, giving x(cid:48)
1 = 7, x1 = 4, and u1 = 1. At time k = 2, the ship moves two squares to the right, giving
2 = 9, x2 = 3, and u2 = 1. And at time k = 3, the ship moves two squares up, giving x(cid:48)
x(cid:48)
3 = 3, x3 = 2, and u3 = 1.
The position of the submarine is now guaranteed to be known in four measurements.
In the second case (shown in Figure 4), the ship moves to position x(cid:48)
0 = 4 at time k = 0 and searches four squares,
giving x0 = 9 and u0 = 4. This choice allows it to search the remaining squares in fewer measurements than the ﬁrst
case, so that the position of the submarine is guaranteed to be known in three measurements instead of four.
In the third case (shown in Figure 5), the ship moves to position x(cid:48)
0 = 7 at time k = 0 and searches thee squares,
giving x0 = 9 and u0 = 3. Compared with the other cases, the ship searches a smaller area in the ﬁrst measurement.
The ship then moves to each other corner at later time periods, until completing the search in four measurements.
The DP algorithm given by Algorithm 2 starts with the terminal condition JN (x(cid:48)
N , xN ) = 0 bits. Proceeding back-
wards in time, we evaluate (21) at xN −1 for the tail subproblem at time N − 1. Inspection of the ﬁgures indicates

9

A dynamic programming algorithm for informative measurement sequences A PREPRINT

×
× ×

× ×

×
× × ×

××
×
× × ×

×××
×
×
× × ×

Figure 5: A search pattern initiated by a ship at the bottom-left corner of the grid. In this case, the uk sequence is
3, 2, 2, 1 so that four measurements are guaranteed to locate the submarine.

that the ship can be in any of the corners in cases 1 and 3, and at x(cid:48)
four-fold symmetry. In all cases we have xN −1 = 2 and uN −1(x(cid:48)
and the positions of the unsearched squares, leading to

N −1 ∈ {2, 4, 6, 8} for case 2, due to the existing
N −1) = 1 due to the geometry of the sonar pattern,

JN −1(x(cid:48)

N −1, 2) = log2 2 −

1
2

(log2 1 − JN (x(cid:48)

N , xN )) ,

= 1 bit.

The subproblem for time N − 2 depends on each case. In case 1, xN −2 = 3 and uN −2(x(cid:48)

N −2) = 1. This leads to

JN −2(x(cid:48)

N −2, 3) = log2 3 −

(cid:0) log2 2 − max

JN −1

(cid:0)x(cid:48)

N −2 + u(cid:48)

N −2, 2(cid:1) (cid:1),

u(cid:48)

N −2

2
3
= log2 3 bits,

provided that:

In case 2, xN −2 = 5 and uN −2(x(cid:48)

N −1 = x(cid:48)
x(cid:48)
N −2.
N −2) = 3. Similar reasoning leads to

N −2 + u(cid:48)∗

JN −2(x(cid:48)

(cid:0) log2 2 − max

u(cid:48)

N −2

N −2, 5) = log2 5 −

2
5
= log2 5 bits.
N −2) = 2. This leads to

In case 3, xN −2 = 4 and uN −2(x(cid:48)

JN −2(x(cid:48)

N −2, 4) = log2 4 −

(cid:0) log2 2 − max

u(cid:48)

N −2

JN −1

JN −1

(cid:0)x(cid:48)

N −2 + u(cid:48)

N −2, 2(cid:1) (cid:1),

(cid:0)x(cid:48)

N −2 + u(cid:48)

N −2, 2(cid:1) (cid:1),

We now solve the subproblem for time N − 3. In case 1, xN −3 = 4 and uN −3 = 1, leading to
N −3, 3(cid:1) (cid:1),

(cid:0) log2 3 − max

N −3, 4) = log2 4 −

N −3 + u(cid:48)

JN −3(x(cid:48)

JN −2

(cid:0)x(cid:48)

u(cid:48)

N −3

1
2
= log2 4 bits.

3
4
= log2 4 bits,

provided that:

N −2 = x(cid:48)
x(cid:48)

N −3 + u(cid:48)∗

N −3.

In case 2, xN −3 = 9 and uN −3(x(cid:48)

N −3) = 4, leading to

JN −3(x(cid:48)

(cid:0) log2 5 − max

u(cid:48)

N −3

N −3, 9) = log2 9 −

5
9
= log2 9 bits.
N −2) = 2. This leads to

In case 3, xN −3 = 6 and uN −3(x(cid:48)

JN −3(x(cid:48)

N −3, 6) = log2 6 −

2
3
= log2 6 bits.

(cid:0) log2 4 − max

u(cid:48)

N −3

JN −2

JN −2

(cid:0)x(cid:48)

N −3 + u(cid:48)

N −3, 5(cid:1) (cid:1),

(cid:0)x(cid:48)

N −3 + u(cid:48)

N −3, 4(cid:1) (cid:1),

(22)

(23)

(24)

Since the total number of possible submarine locations is initially nine, and xN −3 = 9 in equation (24), we can now
terminate the algorithm and set N = 3.

10

A dynamic programming algorithm for informative measurement sequences A PREPRINT

u(cid:48)∗
0 u(cid:48)∗
x(cid:48)
1
0
−4 or − 2
6
2
4
−4 or 2
2
6 −2 −2 or 4
8 −6

2 or 4

Table 1: Optimal controls for ship in “ﬁnd the submarine”.

0, x0) over x(cid:48)

0. Comparing the values for J0(x(cid:48)

The ﬁnal step is to maximize J0(x(cid:48)
0, 9) yields
J0(9) = log2 9 bits of information from three measurements, and x(cid:48)
0 ∈ {2, 4, 6, 8} for the initial position of the ship:
each of these positions leads to u0(x(cid:48)
0) = 4 due to the four-fold symmetry of the grid. The optimal ship movements
are given by x(cid:48)
0, and equations (22) and (23) with the optimal controls in Table 1. A greedy approach that maximizes
the entropy of each measurement is equivalent to the suboptimal search pattern shown in Figure 3. A DP solution
leading to an optimal search pattern is shown in Figure 4. These two ﬁgures bear a resemblance to the greedy and
non-myopic schematics shown in Figure 3 of Bush et al. [2008]. This example will be scaled up and solved using
approximate dynamic programming in the next section.

0, 6), and J0(x(cid:48)

0, 4), J0(x(cid:48)

4 Real-time approximate dynamic programming

The exact DP approach given in Algorithms 1 and 2 requires all states of the problem to be taken into account.
For many problems, the number of states can be very large, and fast computation of an exact solution in real-time is
therefore not possible. A useful approximation that allows for real-time solutions is to look ahead a few stages, simulate
some possible paths going forwards in time all the way out to the horizon, then choose the path corresponding to the
largest entropy. This is repeated at each stage. We do not need to consider more states than those actually visited during
the simulation – a considerable saving when the number of states in the problem is large. This “on-line” approach leads
to an efﬁcient algorithm that approximates the problem, while hopefully also leading to good suboptimal solutions.
For the special case of problems with deterministic dynamics, efﬁcient algorithms already exist; including Dijkstra’s
shortest-path algorithm for discrete states [Bertsekas, 2017] and an extension for continuous states [Tsitsiklis, 1995],
and the A∗ algorithm for discrete states [Hart et al., 1968, 1972]. In the more general case of stochastic dynamics,
the rollout algorithm [Tesauro and Galperin, 1997, Bertsekas, 2017, 2020], combined with adaptive Monte Carlo
sampling techniques such as Monte Carlo Tree Search [Chang et al., 2005, 2013], leads to efﬁcient algorithms. Other
possibilities also include sequential Monte Carlo approaches [Zheng et al., 2018]. In this section, we develop an on-
line algorithm for stochastic dynamics that allows for real-time behavior of an autonomous agent or path-planning
robot seeking an optimal set of measurements as the measurement task is unfolding.

The ﬁrst approximation is to restrict attention to limited lookahead. This can be done, for example, by introducing
a one-step lookahead function ˜Jk+1 that approximates the true function Jk+1. Denoting ˆJk as the general one-step
lookahead approximation of Jk, we write the one-step lookahead approximation of equation (15) as,

ˆJk(x(cid:48)

k, xk) = max
k(x(cid:48)
k)
+ ˜Jk+1(vk(x(cid:48)

k∈U (cid:48)
u(cid:48)

E
wk

(cid:110)

max
uk∈Uk(x(cid:48)

k,xk)

(cid:110)

hk(xk, uk, mk)

E
mk

k, u(cid:48)

k, wk), fk(xk, uk, mk))

(cid:111)(cid:111)
.

(25)

k+1(x(cid:48)

k+1), ..., ˆµ(cid:48)

Given some base policies

k+1, xk+1), ..., ˆµN −1(x(cid:48)

(also called base heuristics) {ˆµ(cid:48)

The one-step lookahead function ˜Jk+1 can be found using an on-line approximation, as we now de-
N −1)} and
scribe.
{ˆµk+1(x(cid:48)
N −1, xN −1)}, it is possible to simulate the dynamics using equations (7) and (13)
from k + 1 all the way to the horizon at N − 1. This idea is used in Algorithm 3 describing the stochastic rollout
algorithm. During each stage of the rollout algorithm, simulation is used to ﬁnd ˜Jk+1 for each control u(cid:48)
k),
and each measurement uk ∈ Uk(x(cid:48)
k and uk that maximize the
right-hand-side of equation (25) are chosen, leading to the rollout policies ¯µ(cid:48)
k, xk) (lines 16 and 17).
Rollout policies are guaranteed to be no worse in performance than the base policies they are constructed from, at least
for base policies that are sequentially improving [Bertsekas, 2017, 2020]. In practice, rollout policies are often found
to perform dramatically better than this [Bertsekas, 2017].
To ﬁnd ˜Jk+1 for each pair (u(cid:48)
k, uk) we use simulation and Monte Carlo sampling in Algorithm 3. Firstly, the samples
wk and mk are drawn from the probability distributions for Wk and Mk in line 5, and used to simulate the dynamics
of (x(cid:48)
k+1, xk+1) in line 6. The rollout phase then takes place in lines

k, xk) (lines 3–15). Following this, the values of u(cid:48)

k, xk) for the control pair (u(cid:48)

k, uk), to give (x(cid:48)

k) and ¯µk(x(cid:48)

N −1(x(cid:48)

k ∈ U (cid:48)

k(x(cid:48)

k(x(cid:48)

11

A dynamic programming algorithm for informative measurement sequences A PREPRINT

k × Sk

0, x0) ∈ S(cid:48)

Algorithm 3 Stochastic Rollout Algorithm
1: Input: (x(cid:48)
2: for k = 0 to N − 1 do
for each (u(cid:48)
3:
repeat
4:
5:

k, uk) ∈ U (cid:48)

k(x(cid:48)

k) × Uk(x(cid:48)

k, xk) do

k, u(cid:48)

wk ∼ pWk , mk ∼ pMk
x(cid:48)
k+1 ← vk(x(cid:48)
for i = k + 1 to N − 1 do
{ˆµ(cid:48)
i), ˆµi(x(cid:48)
wi ∼ pWi, mi ∼ pMi
i+1 ← vi(x(cid:48)
x(cid:48)

i, ˆµ(cid:48)

i(x(cid:48)

i(x(cid:48)

k, wk), xk+1 ← fk(xk, uk, mk)

i, xi)} ← Generate_base_policies(x(cid:48)

i, xi)

i), wi), xi+1 ← fi(xi, ˆµi(x(cid:48)

i, xi), mi)

end for
Store: hk(xk, uk, mk) + ˜Jk+1(x(cid:48)

k+1, xk+1)

until a selected criterion is met
(cid:110)
˜Qk(x(cid:48)
hk(xk, uk, mk) + ˜Jk+1(x(cid:48)

k, xk, u(cid:48)

(cid:110)

k, uk) ← E
wk

E
mk

(cid:111)(cid:111)

k+1, xk+1)

end for
ˆJk(x(cid:48)

k, xk) ← max
u(cid:48)
k∈U (cid:48)
k(x(cid:48)
k)
k(x(cid:48)
k, ¯µ(cid:48)
k, xk) ← u∗
¯µk(x(cid:48)
wk ∼ pWk , mk ∼ pMk
k+1 ← vk(x(cid:48)
x(cid:48)

k, ¯µ(cid:48)

k(x(cid:48)

˜Qk(x(cid:48)

k, xk, u(cid:48)

k, uk)

max
uk∈Uk(x(cid:48)
k) ← u(cid:48)∗
k

k,xk)

k), wk), xk+1 ← fk(xk, ¯µk(x(cid:48)

k, xk), mk)

6:

7:

8:

9:

10:

11:
12:

13:

14:

15:

16:

17:

18:

19:

20: end for

k, xk, u(cid:48)

7–11, where (x(cid:48)
k+1, xk+1) is simulated by generating a pair of base policies, drawing samples for wi and mi, and
then applying equations (7) and (13), stage-by-stage until the horizon is reached. At each stage the information
content hi(xi, ˆµi, mi) is collected, and added to the other stages to produce an estimate for ˜Jk+1(x(cid:48)
k+1, xk+1). These
steps are repeated many times, and the estimates for hk(xk, uk, mk) + ˜Jk+1(x(cid:48)
k+1, xk+1) are then averaged to give
˜Qk(x(cid:48)
There are several steps in Algorithm 3 that can be made more efﬁcient by using adaptive sampling methods such as
Monte Carlo Tree Search. In line 3, some less worthwhile controls can either be sampled less often in lines 4–13,
the simulation of those controls in lines 7–11 can be terminated early before reaching the horizon, or those controls
may be discarded entirely. This can be done adaptively by using, for example, statistical tests or heuristics. There
are also other options available, such as rolling horizons and terminal cost approximations. See Bertsekas [2017] and
references therein for a more complete discussion.

k, uk); where the expectations on Line 14 are approximated by their sample averages.

Algorithm 3 makes use of the subroutine Generate_base_policies. For rollout to work, a base policy must
be fast to evaluate. Here, we use the idea of multistep lookahead to generate base policies. Setting ˜Jk+1 to zero in
equation (25), gives the zero-step lookahead solution:

ˆJk(x(cid:48)

k, xk) =

max
uk∈Uk(x(cid:48)

k,xk)

E
mk

(cid:110)

hk(xk, uk, mk)

(cid:111)
,

(26)

which corresponds to maximizing the entropy of the current measurement only. The next simplest choice is to approx-
imate ˜Jk+1 itself with a one-step lookahead:

˜Jk+1(x(cid:48)

k+1, xk+1) = max

u(cid:48)

k+1

E
wk+1

(cid:110)

max
uk+1

E
mk+1

(cid:110)

hk+1(xk+1, uk+1, mk+1)

+ ˜Jk+2(vk+1(x(cid:48)

k+1, u(cid:48)

k+1, wk+1), fk+1(xk+1, uk+1, mk+1))

(cid:111)(cid:111)
,

12

A dynamic programming algorithm for informative measurement sequences A PREPRINT

where ˜Jk+2 is now an approximation of Jk+2. Setting ˜Jk+2 to zero, leads to the following closed-form expression for
one-step lookahead:

ˆJk(x(cid:48)

k, xk) =

max
uk∈Uk(x(cid:48)

k,xk)

+ max
k∈U (cid:48)
u(cid:48)

k(x(cid:48)

k)

E
wk

E
mk
(cid:110)

(cid:110)

hk(xk, uk, mk)

max
uk+1∈Uk+1(vk,fk)

(cid:110)

hk+1(fk, uk+1, mk+1)

(cid:111)(cid:111)(cid:111)
.

E
mk+1

(27)

This equation gives the ﬁrst correction to the zero-step lookahead result (26), so that ˆJk now depends on the informa-
tion content at k and k + 1. We now have a closed-form expression that depends on both u(cid:48)
k appears
through vk(x(cid:48)
k, wk) in the argument of Uk+1), so that equation (27) can be used to generate the base policies
needed in Algorithm 3. The subroutine is given in Algorithm 4. Instead of approximating the expectations in equation

k and uk (where u(cid:48)

k, u(cid:48)

Algorithm 4 Generate base policies (one possibility based on an optimistic one-step lookahead)
1: Input: x(cid:48)
2: wk ∼ pWk , mk ∼ pMk , mk+1 ∼ pMk+1

k, xk

(cid:110)

3:

max
uk∈Uk(x(cid:48)

k,xk)

hk(xk, uk, mk) + max
k(x(cid:48)

u(cid:48)
k∈U (cid:48)

k)

max
uk+1∈Uk+1(vk,fk)

(cid:110)

hk+1(fk(xk, uk, mk), uk+1, mk+1)

(cid:111)(cid:111)

4: ˆµk(x(cid:48)

k, xk) ← u∗

k, ˆµ(cid:48)

k(x(cid:48)

k) ← u(cid:48)∗
k

(27) by their sample averages, we apply an “optimistic approach” and use the single-sample estimates wk, mk, and
mk+1. The expression on Line 3 is a closed-form expression, so the maximizations leading to the control u(cid:48)∗
k and the
measurements u∗
k+1 is discarded (only the ﬁrst stage is approximated for
limited lookahead) to return a pair of base policies ˆµ(cid:48)

k+1 can be done very quickly. Now u∗

k and u∗

k(x(cid:48)

k) and ˆµk(x(cid:48)

k, xk).

The time efﬁciency of Algorithm 3 strongly depends on how Monte Carlo sampling is performed. If it cannot be
carried out within the time constraints of the real-time problem, then adaptive sampling techniques such as Monte
Carlo tree search must be used. This may lead to some degradation in the quality of solutions, but the aim of these
techniques is to reduce the risk of degradation while gaining substantial computational efﬁciencies. In some cases, the
principle of certainty equivalence may hold for the agent dynamics and single-sample estimates may be sufﬁcient for
approximating expectations. In other cases, such as for Gaussian processes discussed in Section 5, a model for pMk
allows closed-form expressions for expectations instead of requiring expensive sampling techniques. In the limit of
pure rollout (i.e., with no Monte Carlo sampling), the time complexity of Algorithm 3 is O(N 2C); where N is the
k × Uk| is the maximum number
number of measurements (or number of stages to reach the horizon), and C = max
of agent controls and measurement choices per stage. If N is too large for real-time solutions, then further options are
available from approximate dynamic programming and reinforcement learning, such as rolling horizons with terminal
cost approximation, or various other forms of approximate policy iteration [Bertsekas, 2017]. Algorithms 3 and 4 are
now demonstrated using an example with deterministic agent dynamics.

|U (cid:48)

k

4.1 Find the submarine on-line

In this section, we compare a greedy policy to one given by real-time approximate dynamic programming (Algorithms
3 and 4) for the example “ﬁnd the submarine” previously discussed. A greedy policy is the most appropriate com-
parison here, since any improvement in performance beyond a greedy policy will demonstrate planing in a real-time
environment. The greedy policy is found to give optimal behavior up to a certain grid size, beyond which it completely
fails. The approximate DP (rollout) policy continues to give (near) optimal performance for much larger grids, where
planning is demonstrated to take place.

Algorithms 3 and 4 are appropriately modiﬁed to include a parametric model for pMk , deterministic agent dynamics,
and only a single choice of measurement at each stage. In Algorithm 3, this means lines 4, 5, 9, 13, 18, and the
expectation with respect to wk on line 14 are no longer required. Following from equation (21), equation (25) now
becomes

ˆJk(x(cid:48)

k, xk) = log2 xk −

xk − uk
xk

(cid:0) log2 (xk − uk) − max

u(cid:48)
k

13

˜Jk+1 (x(cid:48)

k + u(cid:48)

k, xk − uk) (cid:1),

(28)

A dynamic programming algorithm for informative measurement sequences A PREPRINT

×

×
××
×

×

×
××
×

×
×
×

×

×
××
×

×
×
×
××

×

×
×
×
××
×
×
××××

×
×
×
×
××
×
×
×
××××

×
×
×
×
××
× ×
×
×
××××

×
×
×
××
×
×
× ×
×
×
××××

Figure 6: A search pattern used by a ship following the greedy base policy for “ﬁnd the submarine”. The measurement
sequence starts at the top-left grid illustration, then moves from left to right, top to bottom, before ﬁnishing at the
bottom-right grid illustration. The uk sequence is 5, 3, 2, 2, 1, 1, 1 so that seven measurements are guaranteed to locate
the submarine.

where uk = uk(x(cid:48)
equation (27), yielding the closed-form expression:

k). To generate base policies for use in the rollout algorithm we use the same approach that led to

ˆJk(x(cid:48)

k, xk) = log2 xk − min

u(cid:48)
k

xk − uk − uk+1
xk

log2 (xk − uk − uk+1),

(29)

where uk = uk(x(cid:48)
maximizing the terms uk + uk+1. Therefore, instead of equation (29), we equivalently have,

k). The minimization over u(cid:48)

k), and uk+1 = uk+1(x(cid:48)

k + u(cid:48)

k in equation (29) is equivalent to

ˆJk(x(cid:48)

k) = uk(x(cid:48)

k) + max

u(cid:48)
k

uk+1(x(cid:48)

k + u(cid:48)

k).

The equation (30) can be derived from a DP algorithm with recurrence relation:

Jk(x(cid:48)

k) = uk(x(cid:48)

k) + max

u(cid:48)
k

Jk+1(x(cid:48)

k + u(cid:48)

k),

(30)

(31)

that maximizes the objective: (cid:80)N −1
k=0 uk. A moment’s reﬂection should convince the reader that this DP algorithm
also solves “ﬁnd the submarine”. Therefore, instead of approximating equation (21) to get (28), we now approximate
equation (31) to get

ˆJk(x(cid:48)

k) = uk(x(cid:48)

k) + max

u(cid:48)
k

˜Jk+1(x(cid:48)

k + u(cid:48)

k),

(32)

k(x(cid:48)

where the base policy ˆµ(cid:48)
k) can be generated using equation (30). Algorithms 3 and 4 can now be appropriately
modiﬁed to suit equations (30) and (32). During each stage of rollout, simulation is used to ﬁnd ˜Jk+1 for each control
u(cid:48)
k) taken at state x(cid:48)
k ∈ U (cid:48)
k that maximizes the right-hand-side of equation (32) is chosen. This
leads to the rollout policy ¯µ(cid:48)
k), and describes the path followed by the ship as it plans an optimal sequence of sonar
measurements.

k, and the value of u(cid:48)
k(x(cid:48)

k(x(cid:48)

The base policy generated using Algorithm 4 with equation (30) is shown in Figure 6 for a 4 × 4 grid. This policy is
greedy after the ﬁrst stage: after the initial condition has been chosen, the policy seeks the maximum value of uk at
each stage. Nevertheless, the greedy base policy turns out to be optimal for the 3 × 3 grid shown in Figures 3–5, as
well as for all grids up to 6 × 6.

For grids larger than 6×6 the greedy base policy no longer works, and it becomes necessary to plan each measurement
to obtain an optimal search pattern. The reason can be seen in Figure 7, which shows a ship following the greedy base
policy on a 7 × 7 grid. The grid is now large enough that a ship can move out of reach of a region of unsearched
squares, as shown in Figure 7. This is not possible for smaller grids such as those in Figure 6, because any unsearched
squares will always be within reach of an admissible control (i.e., a control satisfying u(cid:48)
k)). As the grid gets
larger, however, moving the ship back to a region of unsearched squares becomes more and more difﬁcult under the
greedy base policy, since it becomes possible for all controls to maximize the entropy of the next stage (see Figure 7).

k ∈ U (cid:48)

k(x(cid:48)

14

A dynamic programming algorithm for informative measurement sequences A PREPRINT

××× ×
×
× ×××
×
×
×××××××
×
××××××
×××××××
××××××
×××××××

××× ×
×
× ×××
×
×
×××××××
×
××××××
×
×××××××
××××××
×××××××

××× ×
×
× ×××
×
×
×××××××
×
××××× ××
×
×××××××
××××××
×
×××××××

Figure 7: A ship following the greedy base policy moves away from a region of unsearched squares during its search
(grids left to right).
In the right-most grid illustration, all controls now equally maximize the next-stage entropy
from this position, so a return to the region of unsearched squares is not guaranteed (for example, the ship can move
horizontally, forwards and backwards in an endless cycle, without encountering an unsearched square).

×××××××
×××××××
×××××××
×××××××
× × × ×
×××××××
× × × ×

×××××××
×××××××
×××××××
×××××××
× × ×××
×××××××
× × × ×

×××××××
×××××××
×××××××
×××××××
× ×××××
×××××××
× × × ×

Figure 8: A ship following the rollout policy conducts its search systematically (grid illustrations left to right), follow-
ing a planned sequence of moves that avoids the ship moving out of reach of a region of unsearched squares.

In this case, the control that is chosen no longer depends on the entropy of a measurement, but rather, on the order the
admissible controls are processed in.

Surprisingly, although the greedy base policy does not exhibit optimal behavior for larger grids, it can be used to
improve a policy that subsequently attains optimal behavior. This improved policy (the rollout policy) is used to plan
an optimal search pattern. A ship following the rollout policy is shown in Figure 8 for a 7 × 7 grid. During the
late stages of this policy the ship searches the remaining squares systematically by following a planned sequence of
moves. Regardless of the initial condition, the planning done by the rollout policy avoids the ship moving out of reach
of a region of unsearched squares as it does in Figure 7. As the grid increases in size, this behavior continues, and the
minimum number of measurements guaranteed to ﬁnd the submarine is shown in Table 2. In order to guarantee ﬁnding
the submarine it is necessary to search each square of the grid except for the last square (if we have not already found
the submarine, it is guaranteed to be on the last square). However, according to the results in Table 2, the minimum
number of measurements is approximately half the total number of squares that are searched. In other words, using
a local search pattern such as the one shown in Figure 2 can substantially reduce the number of measurements taken

Grid Size
7 × 7
8 × 8
9 × 9
10 × 10
11 × 11
12 × 12
13 × 13
14 × 14

Squares Number of Measurements

49
64
81
100
121
144
169
196

23
31
39
49
60
71
84
98

Percentage
47.9
49.2
48.8
49.5
50.0
49.7
50.0
50.3

Table 2: The minimum number of measurements guaranteed to ﬁnd the submarine using the rollout policy, shown for
different grid sizes. Percentage is given by number of measurements divided by total number of squares searched.

15

A dynamic programming algorithm for informative measurement sequences A PREPRINT

Figure 9: (Left) Bar graph of uk versus k for the rollout policy on an 8 × 8 grid with 31 measurements. The two peaks
at k = 5 and k = 14 are delayed information gains due to planned trade-offs where early low-entropy measurements
lead to later higher-entropy measurements. (Right) Bar graph of uk versus k for the greedy base policy on a 6 × 6 grid
with 17 measurements. In this case there are no planning trade-offs, and uk is a strictly decreasing function of k.

to ﬁnd the submarine by using dynamic programming to plan a path that includes a (near) optimal measurement
sequence.

In Figure 9, the time series of uk is shown for both the rollout and greedy base policies. Trade-offs leading to delayed
information gains can clearly be seen for the rollout policy, where early measurements with lower entropy lead to later
measurements with higher entropy, but not for the greedy base policy. These trade-offs are a sign of planning taking
place in the rollout policy. It is interesting to note that while maximizing entropy always leads to a uniform probability
distribution in the absence of external constraints, maximizing the entropy sequentially generally leads to a sequence
of non-uniform probability distributions.

The rollout policy eventually fails when the grid size is increased beyond a certain range and the approximations lose
their effectiveness. Note that dividing the grid into smaller sub-grids, then searching these sub-grids instead, generally
doesn’t help and will usually lead to more than the minimum number of measurements. This is due to redundant
measurements from overlapping subproblems at the sub-grid boundaries. Dynamic programming speciﬁcally takes
advantage of these overlapping subproblems to achieve optimality. However, the approximations used in approximate
dynamic programming can be improved systematically: either by increasing the lookahead in the rollout algorithm,
or by including additional steps of policy evaluation and policy improvement to go beyond rollout. Improvements to
approximations are expected to extend planning to larger and larger grid sizes.

5 Dynamic programming for Gaussian processes

Gaussian processes [Rasmussen and Williams, 2006, Gibbs, 1997, MacKay, 2003] are widely used for active sensing
of environmental phenomena. Here, we give a variant of our DP algorithm that is applicable to Gaussian processes by
speciﬁcally considering a robot transect sampling task similar to that described in Cao et al. [2013].
In a robot transect sampling task, a robot changes its position x(cid:48) ∈ R2 continuously while taking sensor measurements
at a discrete set of locations x(cid:48)
k (i.e., one measurement per stage k). The outcome of each measurement is determined
by the random ﬁeld M (x(cid:48)), which is a continuous random variable that varies continuously with robot position x(cid:48). The
probability distribution of M (x(cid:48)) is assumed to be a Gaussian process (GP), meaning that any discrete set of outcomes
M (x(cid:48)
k) = mk has a Gaussian probability density. In order to derive a DP algorithm that is useful
for GPs it is therefore necessary to go beyond independent measurement outcomes. Fortunately, this can be done with
a simple modiﬁcation to Algorithm 3 using the method of state augmentation [Bertsekas, 2017].

0) = m0, ..., M (x(cid:48)

A GP model for regression allows us to predict the location of where the next measurement should be taken, given all
previous measurement locations. In this case, the probability density of measurement outcome mk is:

pk(mk|m0:k−1) = N (mk, µk, σ2

k),

(33)

16

0102030k012345uk051015k012345A dynamic programming algorithm for informative measurement sequences A PREPRINT

where

µk = pT C−1mk−1,
k = κ − pT C−1p.
σ2
In Eq. (33), we use the notation m0:k−1 = m0, ..., mk−1 to denote the sequence of all previous measurement out-
comes, which are also used to form the column vector mk−1 = (m0, ..., mk−1)T in Eq. (34). The covariance matrix
C depends on the covariance function C(x, y) of the GP, and has elements Cij = C(x(cid:48)
νδij; where indices i
and j run from 0 to k − 1, and σ2
k), and the column
k); where p is a column vector, and its transpose pT is a row vector. The differen-
vector p has elements pi = C(x(cid:48)
tial entropy of the GP is given by Hk(Mk|σ2
0:k), and depends only on past measurement locations but
not on past measurement outcomes.

ν is the noise variance. The parameter κ is given by κ = C(x(cid:48)
i, x(cid:48)

j) + σ2
k, x(cid:48)

k) = Hk(Mk|x(cid:48)

i, x(cid:48)

(34)

(35)

The DP recurrence relation can now be derived with reference to Eq (15). Assumptions equivalent to Cao et al. [2013]
include only one choice of measurement at each stage, and robot dynamics that is deterministic. These assumptions
reduce Eq (15) to Eq (20), as in the case of “ﬁnd the submarine”. Further, xk and fk play no role in this model, and so
Eq (20) further reduces to:

(cid:110)

Jk(x(cid:48)

k) = E
mk

hk(uk(x(cid:48)

k), mk)

(cid:111)

+ max
k∈U (cid:48)
u(cid:48)

k(x(cid:48)

k)

Jk+1(vk(x(cid:48)

k, u(cid:48)

k)).

(36)

However, this recurrence relation is not quite right because we assumed independence to derive it. In particular, the
entropy (the ﬁrst term on the right-hand side) should be replaced by Hk(Mk|x(cid:48)
0:k) from our GP model. This means
the state given by x(cid:48)
0:k−1 to give the new state
0:k−1, x(cid:48)
x(cid:48)

k in Eq. (36) is no longer sufﬁcient and must now be augmented by x(cid:48)

0:k. The corresponding DP recurrence relation is now written as

k = x(cid:48)

Jk(x(cid:48)

0:k) = Hk(Mk|x(cid:48)

0:k) + max
u(cid:48)
k∈U (cid:48)
k(x(cid:48)

k)

Jk+1(x(cid:48)

0:k, vk(x(cid:48)

k, u(cid:48)

k)).

(37)

The DP algorithm corresponding to Eq. (37) now takes into account all past measurement locatons x(cid:48)
k so that the entropy at stage k may be found. At stage k, the robot then chooses control u(cid:48)
to x(cid:48)
informative measurement location x(cid:48)

k+1 at the next stage. This leads to x(cid:48)

0:k, vk = x(cid:48)

0:k−1 leading
k to reach the most

0:k+1 for the argument of Jk+1.

The DP recurrence relation given by Eq. (37) is expected to give similar results to the “approximate maximum entropy
path planning” presented in Cao et al. [2013] when it is used in the rollout algorithm. However, the strength of our DP
framework is that it is general, and therefore can be used to describe more diverse situations. For example, if the robot
dynamics is stochastic instead of deterministic, we can simply appeal to Eq (15) to get the following DP recurrence
relation:

Jk(x(cid:48)

0:k) = Hk(Mk|x(cid:48)

0:k) + max
u(cid:48)
k∈U (cid:48)
k(x(cid:48)
k)
Alternatively, instead of considering a single random ﬁeld, we might be interested in sensing several random ﬁelds
simultaneously; such as the salinity and temperature of a water body. We then have more than one choice of sensor
measurement available at each stage. Again, appealing to Eq (15), we might choose to model this using the following
DP recurrence relation:
Jk(x(cid:48)

Jk+1(x(cid:48)

k, wk))

(39)

(38)

0:k, vk(x(cid:48)

Jk+1(x(cid:48)

k, u(cid:48)

(cid:88)

E
wk

0:k, vk(x(cid:48)

k(M i

k|x(cid:48)

kH i
ui

k, u(cid:48)

k)),

(cid:111)
.

(cid:110)

0:k) = max
uk∈Uk(x(cid:48)

k)

i

0:k) + max
k(x(cid:48)
k∈U (cid:48)
u(cid:48)

k)

k (x(cid:48)) and M 2

where M 1
k (x(cid:48)) might be the salinity and temperature ﬁelds, for example. In this case, possible measure-
ment choices at each stage would include uk = (1, 0), uk = (0, 1), or uk = (1, 1). Presumably, the default case is the
measurement uk = (1, 1) where both salinity and temperature are measured simultaneously at each stage. However,
in some circumstances there may be hard constraints on either the number of measurements possible at each stage,
or the type of measurement that can be taken at each stage. This could be due to constraints on power consumption,
storage of samples, sensor response times, etc. The DP recurrence relation given by Eq (39) is able to properly ac-
count for these types of measurement constraints, as well as any kinematic constraints on the robot or vehicle. This is
done through the constraint sets Uk(x(cid:48)
k at time k. These are just
three examples, but other possibilities for DP recurrence relations can also be derived from Eq (15) under different
modelling assumptions.

k), which depend on the robot position x(cid:48)

k) and U (cid:48)

k(x(cid:48)

A modiﬁed version of Algorithm 3 is now proposed for solving a GP. Speciﬁcally, Lines 7–12 in Algorithm 3 are
replaced with Lines 7–13 in Algorithm 3.1. The main change is the extra assignment on Line 8, which is necessary
for prediction of the ith stage entropy, Hi(Mi|σ2
i ). On Line 13, the entropy predictions from stages k to N −1 are then
added together and stored. An additional evaluation of σ2
k following Line 2 in Algorithm 3 is also required in order to

17

A dynamic programming algorithm for informative measurement sequences A PREPRINT

Algorithm 3.1 Modiﬁed Stochastic Rollout for GPs
7: for i = k + 1 to N − 1 do

0:i) = κ − pT C−1p
i) ← Generate_base_policy(x(cid:48)
i)

8:

9:

10:

11:

σ2
i (x(cid:48)
i(x(cid:48)
ˆµ(cid:48)
wi ∼ pWi
i+1 ← vi(x(cid:48)
x(cid:48)

i, ˆµ(cid:48)

i(x(cid:48)

i), wi)

12: end for
13: Store: Hk(Mk|x(cid:48)

0:k) + ˜Jk+1(x(cid:48)

k+1)

deﬁne Hk(Mk|x(cid:48)
0:k) on Line 13. Further slight modiﬁcations of Algorithm 3 may also be required, depending on the
precise form of the DP recurrence relation considered. The assignment on Line 8 requires computation of C−1, which
takes O(i3) time using exact matrix inversion. However, a tighter bound of O((i − j)3) is possible by recognizing that
dependencies are only appreciable between a subset of locations x(cid:48)
0:i−1: potentially
leading to a much smaller C matrix. The size of this subset will depend on the length-scale hyperparameters in
the covariance function, as well as the distance between each sampling location. In the best case, we can hope to
gain a small constant-time overhead with each iteration, and the modiﬁed Algorithm 3 still scales as O(N 2C) in the
deterministic case. If not, a further reduction in computational time is possible by replacing exact matrix inversion
with one of the approximation methods discussed in Rasmussen and Williams [2006].

j:i−1, rather than all locations x(cid:48)

6 Conclusion

The outcome of this work was the development of a general-purpose dynamic programming algorithm for ﬁnding
an optimal sequence of informative measurements. This algorithm uniﬁes the design of informative measurements
with efﬁcient path planning for robots and autonomous agents. While greedy methods are still the most common
approach for ﬁnding informative measurements, we showed that an essential characteristic of some types of optimal
measurement sequences includes planning for delayed information gains. This seems especially true for path planning
in artiﬁcial intelligence and robotics, where ﬁnding optimal sequences of informative measurements is more likely to
lead to a combinatorial optimization problem. We demonstrated a simple path planning problem involving a deter-
ministic agent taking measurements on visible states that could not be solved efﬁciently using a greedy method. We
also showed an approximate dynamic programming solution to this problem that clearly exhibited delayed information
gains due to planning trade-offs taking place.

An obvious application of the proposed algorithm is to make efﬁcient use of sensors on an autonomous robot or
vehicle that is exploring a new environment. A major strength of our dynamic programming algorithm is that it can
simultaneously take into account sensor constraints and kinematic constraints of robots and autonomous vehicles.
Continuous states, controls, and hard constraints can also be handled using a version of the rollout algorithm called
model predictive control (MPC). This requires having a map of the environment, as well as an accurate method for
localizing the position of the robot or vehicle on the map. Another application of our algorithm is efﬁcient active
sensing of spatially continuous phenomena via a Gaussian process model. We showed how to include different types
of sensor constraints while simultaneously including the dynamics and kinematic constraints of the sensor platform.
This application will be explored further in a future paper.

Acknowledgements

PL would like to thank Fritz Sommer for informative discussions on this topic.

Appendix A1: Objective maximized by extended DP algorithm

Deﬁne the maximum entropy of the (N − k)-stage problem to be

(cid:40)

k (x(cid:48)
J ∗

k, xk) = max
π(cid:48)k

E(cid:48)

E

max
πk

(cid:40)N −1
(cid:88)

i=k

hi(xi, µi(x(cid:48)

i, xi), mi) + hN (x(cid:48)

N , xN )

,

(40)

(cid:41)(cid:41)

18

A dynamic programming algorithm for informative measurement sequences A PREPRINT

where π(cid:48)k = {µ(cid:48)
expectation over mk, ..., mN −1.

k, ..., µ(cid:48)

N −1}, πk = {µk, ..., µN −1}, and where E(cid:48) is an expectation over wk, ..., wN −1, and E is an

Proposition: The maximum entropy of the N -stage problem, J ∗
of a DP algorithm that starts with the terminal condition: JN (x(cid:48)
time by evaluating the following recurrence relation:

0, x0), is equal to J0(x(cid:48)

0 (x(cid:48)
N , xN ) = hN (x(cid:48)

0, x0), given by the last step
N , xN ), and proceeds backwards in

Jk(x(cid:48)

k, xk) = max
k(x(cid:48)
k)
+Jk+1(vk(x(cid:48)

k∈U (cid:48)
u(cid:48)

E
wk

(cid:110)

max
uk∈Uk(x(cid:48)

k,xk)

(cid:110)

hk(xk, uk, mk)

E
mk

k, u(cid:48)

k, wk), fk(xk, uk, mk))

(cid:111)(cid:111)
,

(41)

from stage k = N − 1 to stage k = 0.

N (x(cid:48)

N , xN ) = hN (x(cid:48)

N , xN ) = JN (x(cid:48)

N , xN ), proving the base

k+1, xk+1), that J ∗

k+1(x(cid:48)

k+1, xk+1) = Jk+1(x(cid:48)

k+1, xk+1). We need to

Proof: We use induction. At stage k = N , we have J ∗
case to be true.
Now assume for some k ≤ N − 1, and all (x(cid:48)
k, xk) = Jk(x(cid:48)
show that J ∗
k (x(cid:48)
k (x(cid:48)
J ∗
k, xk) = max

E(cid:48)(cid:110)

E

k, xk) to complete the proof. We have,
(cid:110)

hk(xk, µk(x(cid:48)

k, xk), mk)

max
(µk,πk+1)

(µ(cid:48)

k,π(cid:48)k+1)
N −1
(cid:88)

+

hi(xi, µi(x(cid:48)

i, xi), mi) + hN (x(cid:48)

N , xN )

(cid:111)(cid:111)
,

i=k+1
(cid:110)

= max
µ(cid:48)
k

E
wk

max
µk

E
mk

(cid:110)

hk(xk, µk(x(cid:48)

k, xk), mk)

+ max
π(cid:48)k+1

E
{wk+1,..}

max
πk+1

E
{mk+1,..}

(cid:110)

(cid:110) N −1
(cid:88)

i=k+1

hi(xi, µi(x(cid:48)

i, xi), mi) + hN (x(cid:48)

N , xN )

(cid:111)(cid:111)(cid:111)(cid:111)
,

= max
µ(cid:48)
k

= max
µ(cid:48)
k

E
wk

E
wk

(cid:110)

max
µk

(cid:110)

max
µk

E
mk

E
mk

(cid:110)

hk(xk, µk(x(cid:48)

k, xk), mk) + J ∗

k+1(x(cid:48)

k+1, xk+1)

(cid:110)

hk(xk, µk(x(cid:48)

k, xk), mk) + Jk+1(x(cid:48)

k+1, xk+1)

(cid:111)(cid:111)
,

(cid:111)(cid:111)
,

(cid:110)

E
wk

= max
u(cid:48)
k∈U (cid:48)
k(x(cid:48)
k)
+ Jk+1(vk(x(cid:48)

max
uk∈Uk(x(cid:48)

k,xk)

(cid:110)

hk(xk, uk, mk)

E
mk

k, u(cid:48)

k, wk), fk(xk, uk, mk))

(cid:111)(cid:111)
,

= Jk(x(cid:48)

k, xk).

In the ﬁrst equation above, we used the deﬁnition of J ∗
k , π(cid:48)k, and πk. In the second equation, we interchanged the
maxima over π(cid:48)k+1 and πk+1 with hk because the tail portion of an optimal policy is optimal for the tail subproblem.
We also used the linearity of expectation. In the third equation we used the deﬁnition of J ∗
k+1, and in the fourth
equation we used the inductive hypothesis: J ∗
k+1 and
(cid:4)
xk+1, and u(cid:48)

k+1 = Jk+1. In the ﬁfth equation, we substituted vk and fk for x(cid:48)

k and µk. In the sixth equation, we used the deﬁnition of Jk.

k and uk for µ(cid:48)

Appendix A2: Guess my number

In this example, an integer is selected uniformly at random in the range [0, n − 1], and the problem is to ﬁnd the
minimum number of yes/no questions guaranteed to determine this integer. The bisection method and binary search
algorithm are efﬁcient methods for solving this type of problem. Here, we use DP to demonstrate the optimality of
these methods for maximizing the entropy of a set of binary comparisons represented by the yes/no questions.

Let X be the unknown random integer between 0 and n − 1. Given a proper subinterval of consecutive integers
between 0 and n − 1, let M return “yes” or “no” to the question: “Is the unknown integer within this subinterval?”.
We also make the following deﬁnitions:

xk = size of integer range considered at stage k,
uk = size of proper subinterval containing the unknown integer at stage k,

19

A dynamic programming algorithm for informative measurement sequences A PREPRINT

and the following parameterizations:

pk(mk|xk, uk) =

(cid:26)uk/xk

mk = “yes”,
(xk − uk)/xk mk = “no”,

fk(xk, uk, mk) =

(cid:26)uk

mk = “yes”,
xk − uk mk = “no”.

With these deﬁnitions and parameterizations, equations (11) and (12) lead to the following DP recurrence relation:

Jk(xk) = max

uk∈U +(xk)

(cid:18)

(cid:26) uk
xk

log2

xk
uk

(cid:19)

+ Jk+1 (uk)

+

xk − uk
xk

(cid:18)

log2

xk
xk − uk

+ Jk+1 (xk − uk)

,

(42)

(cid:19)(cid:27)

where U +(xk) is the set {1, 2, ..., xk − 1}.
The DP algorithm starts with the terminal condition JN (1) = 0 bits, as before. Considering the tail subproblem for
measurement N − 1 and xN −1 = 2, leads to
1
2

(log2 2 + JN (1)) +

(log2 2 + JN (1)),

JN −1(2) =

1
2

N −1 = 1)
Now the subproblem for xk = 3 requires the value for JN −1(2), which only becomes an overlapping subproblem if
we move to measurement N − 2:

= 1 bit. (u∗

JN −2(3) = max

(cid:26) 1
3

(log2 3 + JN −1 (1)) +

(cid:18)

log2

3
2

(cid:19)

+ JN −1 (2)

,

(cid:18)

2
3

log2

3
2

+ JN −1 (2)

+

1
3
N −2 = 2)

= log2 3 bits. (u∗

N −2 = 1 or u∗

(log2 3 + JN −1 (1))

,

(cid:27)

2
3
(cid:19)

This solution tells us that if we start with three integers (xN −2 = 3) and choose the ﬁrst subinterval to be length 1
(u∗
N −2 = 1), then with probability 1/3 we can determine the unknown integer with one question. Otherwise, two
questions will be necessary. Now consider the DP for xN −2 = 4,

JN −2(4) = max

(cid:26) 1
4

(log2 4 + JN −1 (1)) +

(cid:18)

3
4

log2

4
3

(cid:19)

+ JN −1 (3)

,

1
2

(log2 2 + JN −1 (2)) +

(log2 2 + JN −1 (2)) ,

(cid:18)

3
4

log2

4
3

+ JN −1 (3)

+

(log2 4 + JN −1 (1))

,

(cid:27)

1
4

1
2
(cid:19)

= 2 bits. (u∗

N −2 = 2)

In the ﬁrst equation, JN −1(3) is replaced with log2 3 − 2/3 to get the ﬁnal result because JN −1(3) cannot be fully
resolved in a single measurement. This result can be derived in a similar way to JN −2(3), but instead, using JN (2) =
0. From the solution for JN −2(4), it is seen that two bits of information can be gained from two binary questions
provided each subinterval divides the previous subinterval in half: u∗
N −1 = 1 when
xN −1 = 2. This solution can be continued, giving an upper bound on the entropy as 1 bit per question for M . Then
log2 n bits for X means the unknown random integer is guaranteed to be found after a number of yes/no questions
equal to (cid:100)log2 n(cid:101). This result demonstrates that methods such as the binary search algorithm and the bisection method
maximize the entropy of a set of binary comparisons.

N −2 = 2 when xN −2 = 4, and u∗

References

N. Ay, N. Bertschinger, R. Der, F. G¨uttler, and E. Olbrich. Predictive information and explorative behavior of au-

tonomous robots. Eur. Phys. J. B, 63:329–339, 2008.

A. G. Barto, S. J. Bradtke, and S. P. Singh. Learning to act using real-time dynamic programming. Artiﬁcial Intelli-

gence, 72:81–138, 1995.

D. P. Bertsekas. Dynamic programming and optimal control vol 1, 4th ed. Athena Scientiﬁc, 2017.

20

A dynamic programming algorithm for informative measurement sequences A PREPRINT

D. P. Bertsekas. Rollout, policy iteration, and distributed reinforcement learning. Athena Scientiﬁc, 2020.
D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc, 1996.
L. A. Bush, B. Williams, and N. Roy. Computing exploration policies via closed-form least-squares value iteration.

In International Conference on Planning and Scheduling, 2008.

N. Cao, K. H. Low, and J. M. Dolan. Multi-robot informative path planning for active sensing of environmental
phenomena: a tale of two algorithms. In Proceedings of the 2013 International Conference on Autonomous Agents
and Multi-Agent Systems, pages 7–14, Richland, SC, 2013. International Foundation for Autonomous Agents and
Multiagent Systems.

H. Chang, J. Hu, M. Fu, and S. Marcus. Simulation-Based Algorithms for Markov Decision Processes. 01 2013. ISBN

978-1-4471-5021-3. doi: 10.1007/978-1-4471-5022-0.

H. S. Chang, M. C. Fu, J. Hu, and S. I. Marcus. An adaptive sampling algorithm for solving markov decision processes.

Operations Research, 53(1):126–139, 2005.

M. N. Gibbs. Bayesian Gaussian processes for regression and classiﬁcation. PhD thesis, Cambridge University, 1997.
P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE

Transactions on Systems Science and Cybernetics, 4(2):100–107, 1968.

P. E. Hart, N. J. Nilsson, and B. Raphael. Correction to “a formal basis for the heuristic determination of minimum

cost paths”. ACM SIGART Bulletin, 37:28–29, 1972.

E. T. Jaynes. Information theory and statistical mechanics. Physical Review, 106:620–630, 1957.
T. Kollar and N. Roy. Trajectory optimization using reinforcement learning for map exploration. Int. J. Rob. Res., 27:

175–196, 2008.

A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efﬁcient

algorithms and empirical studies. J. Mach. Learn. Res., 9:235–284, 2008.

D. V. Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics,

27:986–1005, 1955.

D. Y. Little and F. T. Sommer. Learning in embodied action-perception loops through exploration. arXiv:1112.1125v2,

2011.

D. Y. Little and F. T. Sommer. Learning and exploration in action-perception loops. Frontiers in Neural Circuits, 7:

1–19, 2013.

K. H. Low, J. M. Dolan, and P. Khosla. Information-theoretic approach to efﬁcient adaptive path planning for mobile
robotic environmental sensing. In Proceedings of the Nineteenth International Conference on Automated Planning
and Scheduling, page 233–240. AAAI Press, 2009.

P. N. Loxley. A sparse code increases the speed and efﬁciency of neuro-dynamic programming for optimal control

tasks with correlated inputs. Neurocomputing, 426:1–13, 2021.

D. J. C. MacKay. Information-based objective functions for active data selection. Neural Comput., 4:590–604, 1992.
D. J. C. MacKay. Information theory, inference, and learning algorithms. Cambridge University Press, 2003.
J. A. Placed and J. A. Castellanos. A deep reinforcement learning approach for active slam. Appl. Sci, 10:8386, 2020.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
C. J. Shannon. A mathematical theory of communication. Bell Systems Technical Journal, 27:379–423, 623–656,

1948.

A. Singh, A. Krause, C. Guestrin, and W. J. Kaiser. Efﬁcient informative sensing using multiple robots. J. Artif. Int.

Res., 34:707–755, 2009.

R. S. Sutton and A. G. Barto. Reinforcement learning, second edition: an introduction. MIT Press, 2018.
G. Tesauro and G. Galperin. On-line policy improvement using monte-carlo search. In M. C. Mozer, M. Jordan, and

T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9. MIT Press, 1997.

J.N. Tsitsiklis. Efﬁcient algorithms for globally optimal trajectories. IEEE Transactions on Automatic Control, 40(9):

1528–1538, 1995.

S. Zheng, J. Pacheco, and J. Fisher. A robust approach to sequential information theoretic planning. In Jennifer Dy
and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pages 5941–5949. PMLR, 10–15 Jul 2018.

21

