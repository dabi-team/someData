. ARTICLES .

doi:

Low-Complexity Algorithm for Restless Bandits with
Imperfect Observations

Keqin Liu1, Richard Weber2, Ting Wu3 & Chengzhong Zhang4

1Department of Mathematics, Nanjing University, Nanjing 210093, China;
2Department of Mathematics, University of Cambridge, Cambridge CB3 0WB, UK;
3Department of Mathematics, Nanjing University, Nanjing 210093, China;
4Department of Mathematics, Nanjing University, Nanjing 210093, China
Email:kqliu@nju.edu.cn, rrw1@cam.ac.uk, tingwu@nju.edu.cn, 171840780@smail.nju.edu.cn

Abstract We consider a class of restless bandit problems that ﬁnds a broad application area in stochastic
optimization, reinforcement learning and operations research. We consider N independent discrete-time Markov
processes, each of which had two possible states: 1 and 0 (‘good’ and ‘bad’). Only if a process is both in state
1 and observed to be so does reward accrue. The aim is to maximize the expected discounted sum of returns
over the inﬁnite horizon subject to a constraint that only M (< N ) processes may be observed at each step.
Observation is error-prone: there are known probabilities that state 1 (0) will be observed as 0 (1). From this
one knows, at any time t, a probability that process i is in state 1. The resulting system may be modeled
as a restless multi-armed bandit problem with an information state space of uncountable cardinality. Restless
bandit problems with even ﬁnite state spaces are PSPACE-HARD in general. We propose a novel approach for
simplifying the dynamic programming equations of this class of restless bandits and develop a low-complexity
algorithm that achieves a strong performance and is readily extensible to the general restless bandit model with
observation errors. Under certain conditions, we establish the existence (indexability) of Whittle index and
its equivalence to our algorithm. When those conditions do not hold, we show by numerical experiments the
near-optimal performance of our algorithm in the general parametric space. Last, we theoretically prove the
optimality of our algorithm for homogeneous systems.

Keywords

restless bandits, continuous state space, observation errors, index policy

MSC(2020)

65K05, 90B18, 90C15, 90C40

1 Introduction

The exploration-exploitation (EE) dilemma is well posed in optimization-over-time problems and math-
ematically modeled in various forms for reinforcement learning, to which a major category, multi-armed
bandits (MAB) belongs. In the classical MAB model, a player chooses one out of N statistically indepen-
dent arms to pull and possibly accrues reward determined by the state of the chosen arm which transits
to a new state according to a known Markovian rule (Gittins et al. [5]). The states of other arms remain

2
2
0
2

g
u
A
9

]

G
L
.
s
c
[

2
v
2
1
8
3
0
.
8
0
1
2
:
v
i
X
r
a

©

 
 
 
 
 
 
2

Liu K Q et al. arXiv version

frozen. The objective is to maximize the expected total discounted reward summed over times t = 1, 2, . . .
to an inﬁnite time horizon with discount factor β ∈ (0, 1),

max
π∈Π

Eπ

(cid:34) ∞
(cid:88)

(cid:35)
βt−1R(t) | S(t), A(t)

.

t=1

(1.1)

The expectation is taken under some policy π, chosen from the set of all feasible policies Π; S(t) is the
joint state of all arms at time t, A(t) ∈ {1, 2, . . . , N } is the arm pulled at t and R(t) is the reward thus
obtained. It follows from standard theory of Markov decision processes that there must exist an optimal
stationary policy π∗, independent of time t. If each arm’s state space has cardinality K, then the joint
state space has size K N . This means that a dynamic programming solution to the problem will have
running time that grows geometrically as the number of arms increases. Gittins [4] solved the problem by
showing the optimality of an index policy, i.e., for each state of each arm there exists an index depending
solely on the parameters of that arm; it is then optimal at each time to choose the arm whose state
has highest index. The running time of the Gittins index policy grows only linearly with the number
of arms as they are decoupled when computing the index function(of the states of each arm). Whittle
[17] generalized Gittins index to the restless MAB model in which those arms that are not chosen may
also change states and produce reward. Whittle’s generalization has been shown to perform very well in
theoretical and numerical studies (see, e.g., Brown and Smith [1], Chen et al. [2], Gast et al. [3], Hu and
Frazier [6], Liu and Zhao [11], Weber and Weiss [15, 16], Zayas-Cab´an et al. [18]). In general, however, it
is diﬃcult to establish the condition that is necessary for the Whittle index to exist (so called indexability)
and to compute the Whittle index when it does exists. Papadimitriou and Tsitsiklis [13] have shown that
the restless MAB with a ﬁnite state space is PSPACE-HARD.

In this paper, we extend the work in Liu and Zhao [11] (for a perfect observation model) and Liu et
al. [12] (for the myopic policy on stochastically identical arms) to build a near-optimal algorithm with
low complexity for a class of restless bandits with an inﬁnite state space and an imperfect observation
model. Our model also belongs to the general framework of partially observable Markov decision processes
(POMDP) (Sondik [14]). Consider N processes each of which evolves on a 2-state Markov chain whose
state is observed if and only if the process is chosen. Furthermore, the observation is error-prone: state 1
may be observed as 0 and vice versa. Each process is referred to as an arm. At time t, the player obtains
reward of amount Bn if and only if arm n is currently chosen and accurately observed in state 1. Under
resource constraints, the player’s goal is to select M (M < N ) arms at each time and maximize the
long-term reward. By formulating the belief vector as the system state for decision making, we show that
the indexability is satisﬁed under certain conditions. Furthermore, we propose an eﬃcient algorithm to
compute an approximate Whittle index that achieves a near-optimal performance in general, even if the
conditions for indexability do not hold.

The paper is organized as follows. Our main results are in Section 2, our new algorithm is in Section 3,
experimental results are in Section 4, the optimality proof is in Section 5, and the conclusions follow in
Section 6.

2 Main results

Consider a restless MAB having N internal 2-state Markov chains (arms) of potentially diﬀerent transition
probabilities. At each time t, the player chooses to observe the states of M (< N ) arms. Let S ∈
{0 (bad), 1 (good)} denote the current state of an arm and let O denote its observation outcome (detection
outcome). The error probabilities are δ = Pr(O = 1 | S = 0) and (cid:15) = Pr(O = 0 | S = 1), i.e., the
probabilities of miss detection and false alarm, respectively, in the observation model. Under the optimal

Liu K Q et al. arXiv version

3

detector, the error probabilities δ and (cid:15) follow the curve of receiver operating characteristics (ROC) (Levy
[8]). Since one cannot minimize both δ and (cid:15) simultaneously, we ﬁx a constraint on miss detection δ (cid:54) δ0
for some δ0 ∈ (0, 1) which yields the optimal false arm probability (cid:15)0, simply denoted by (cid:15). If arm n in
state S = 1 is observed in state 1 (i.e., S = O = 1), then the player accrues Bn units of reward from
this arm. One of many application examples of this observation model is to cognitive radios, where a
secondary user aims to utilize a frequency band (channel/arm) currently unused by the primary users.
Due to energy and policy constraints on the sensor of the secondary user, only a subset of channels can
be sensed at each time and if any of them is sensed idle (O = 1), the user can send certain packets over it
to its receiver and obtain an ACK (acknowledgement) in the end of the time slot if the channel is indeed
idle (S = 1); otherwise no ACK from this channel would be received. Then the reward Bn is just the
bandwidth of channel n. Clearly, the hard constraint here should be on the miss detection probability δ
to guarantee the satisfaction of the primary users, i.e., the disturbance (when a secondary user senses a
busy channel as idle and subsequently sends data over it) to the primary users should be capped.

2.1 System Model and Belief Vector

At each discrete time t, the internal state (0/1) of an arm cannot be observed before deciding whether
or not to observe the arm. Therefore, we cannot use the states of the Markov chains as the system state
for decision making. Applying the general POMDP theory to our model the belief state vector consisting
of probabilities that arms are in state 1 given all past observations is a suﬃcient statistics for making
future decisions (Sondik [14]):

ω(t) = (ω1(t), ω2(t), · · · , ωN (t)),
ωn(t) = Pr(Sn(t) = 1 | past observations on arm n), ∀ n ∈ {1, · · · , N },

(2.1)
(2.2)

where ωn(t) is the belief state of arm n at time t and Sn(t) its internal state. According to the Bayes’
rule, the belief state (of any arm) itself evolves as a Markov chain with an inﬁnite state space:

ωn(t + 1) =






p(n)
11 ,
(cid:16)
Tn
Tn(ωn(t)),

(cid:15)ωn(t)
(cid:15)ωn(t)+1−ωn(t)

(cid:17)

n ∈ A(t), ACKn(t) (Sn(t) = 1, On(t) = 1)

, n ∈ A(t), no ACKn(t)

n /∈ A(t)

Tn(ωn(t)) = ωn(t)p(n)

11 + (1 − ωn(t))p(n)
01 ,

,

(2.3)

(2.4)

where A(t) ⊂ {1, 2, . . . , N } is the set of arms chosen at time t with |A(t)| = M , Sn(t) and On(t) are
respectively the state and observation from arm n at time t if n ∈ A(t), ACKn(t) the acknowledgement
of successful utilization of arm n for slot t, Tn(·) the one-step belief update operator without observation,
and P(n) = {p(n)
ij , i, j ∈ {0, 1}} the transition matrix of the internal Markov chain of arm n. Furthermore,
the k-step belief update of an unobserved arm for k consecutive slots starting from any belief state ω is

T k
n (ω) =

01 − (p(n)
p(n)

11 − p(n)

01 )k(p(n)
1 + p(n)

01 − (1 + p(n)
01 − p(n)

11

01 − p(n)

11 )ω)

.

(2.5)

For simplicity of notations, we denote T 1

n (·) by Tn(·).

At time t = 1, the initial belief state ωn(1) of arm n can be set as the stationary distribution ωn,o

of the internal Markov chain1) :

1) Here we assume the internal Markov chain with transition matrix P(n) is irreducible and aperiodic.

4

Liu K Q et al. arXiv version

ωn(1) = ωn,o = lim
k→∞

n (ω(cid:48)) =
T k

p(n)
01
01 + p(n)
p(n)

10

,

(2.6)

where ωn,o is the unique solution to Tn(ω) = ω and ω(cid:48) ∈ [0, 1] an arbitrary probability. Given the
initial belief vector ω(1) = (ω1(1), ω2(1), . . . , ωN (1)), we arrive at the following constrained optimization
problem:

max
π:ω(t)→A(t)

Eπ

(cid:20) ∞
(cid:80)
t=1

βt−1R(t)

subject to |A(t)| = M,

,

(cid:21)
(cid:12)
(cid:12)
(cid:12) ω(1)
t (cid:62) 1.

(2.7)

(2.8)

It is clear that ﬁxing ω(1), the action-dependent belief vector ω(t) takes possible values growing geo-
metrically with time t, leading to a high-complexity in solving the problem; this is the so-called curse of
dimensionality. In the following, we adopt Whittle’s original idea of Lagrangian relaxation to decouple
arms for an index policy and show some crucial properties of the value functions of a single arm.

2.2 Arm Decoupling by Lagrangian Relaxation

max
π:ω(t)→A(t)

Eπ

subject to Eπ

(cid:34) ∞
(cid:88)

t=1
(cid:34) ∞
(cid:88)

t=1

βt−1

βt−1

N
(cid:88)

n=1

N
(cid:88)

n=1

1(n∈A(t)) · Sn(t) · On(t) · Bn

(cid:35)
(cid:12)
(cid:12)
(cid:12) ω(1)

1(n /∈A(t))

(cid:35)

(cid:12)
(cid:12)
(cid:12) ω(1)

=

N − M
1 − β

.

(2.9)

(2.10)

Clearly constraint (2.10) is a relaxation on the player’s action A(t) from (2.8). Applying the Lagrangian
multiplier µ to constraint (2.10), we arrive at the following unconstrained optimization problem:

max
π:ω(t)→A(t)

Eπ

(cid:34) ∞
(cid:88)

t=1

βt−1

N
(cid:88)

n=1

(cid:2)1(n∈A(t))Sn(t)On(t)Bn + µ · 1(n /∈A(t))

(cid:35)
(cid:3) (cid:12)
(cid:12)
(cid:12) ω(1)

.

(2.11)

Fixing µ, the above optimization is equivalent to N independent unconstraint optimization problem as
shown below: for each n ∈ {1, 2, . . . , N },

max
π:ωn(t)→{0,1}

Eπ

(cid:34) ∞
(cid:88)

t=1

βt−1 (cid:2)1(n∈A(t))Sn(t)On(t)Bn + µ · 1(n /∈A(t))

(cid:35)
(cid:3) (cid:12)
(cid:12)
(cid:12) ωn(1)

.

(2.12)

Here π is a single-arm policy that maps the belief state of the arm to the binary action u = 1 (cho-
sen/activated) or u = 0 (unchosen/made passive). It is thus suﬃcient to consider a single arm for solving
problem (2.11). For simplicity, we will drop the subscript n in consideration of a single-armed bandit
problem without loss of generality. Let Vβ,m(ω) denote the value of (2.12) with µ = m and ωn(1) = ω,
it is straightforward to write out the dynamic equation of the single-armed bandit problem as follows:

Vβ,m(ω) = max{Vβ,m(ω; u = 1); Vβ,m(ω; u = 0)},

(2.13)

where Vβ,m(ω; u = 1) and Vβ,m(ω; u = 0) denote, respectively, the maximum expected total discounted
reward that can be obtained if the arm is activated or made passive at the current belief state ω, followed

Liu K Q et al. arXiv version

5

by an optimal policy in subsequent slots. Since we consider the inﬁnite-horizon problem, a stationary op-
timal policy can be chosen and the time index t is not needed in (2.13). Deﬁne the nonlinear operator φ(·)
as

φ(ω) =

(cid:15)ω
(cid:15)ω + 1 − ω

.

It is easy to see that T ◦ φ(·) is Lipschitz continuous on [0, 1]:

(cid:16)

(cid:12)
(cid:12)
(cid:12)T

(cid:15)ω
(cid:15)ω+1−ω

(cid:17)

− T

(cid:16)

(cid:15)ω(cid:48)
(cid:15)ω(cid:48)+1−ω(cid:48)

(cid:17)(cid:12)
(cid:12)
(cid:12) =

=

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

p11(cid:15)ω+(1−ω)p01
(cid:15)ω+1−ω

− p11(cid:15)ω(cid:48)+(1−ω(cid:48))p01
(cid:15)ω(cid:48)+1−ω(cid:48)

(cid:12)
(cid:12)
(cid:12)

(cid:15)(p11−p01)(ω−ω(cid:48))
(1−(1−(cid:15))ω)(1−(1−(cid:15))ω(cid:48))

(cid:12)
(cid:12)
(cid:12)

(cid:54) |p11 − p01|
(cid:15)

|ω − ω(cid:48)|.

(2.14)

(2.15)

We assume that (cid:15) (cid:54)= 0 (otherwise the problem is reduced to that considered in Liu and Zhao [11])
and p11 (cid:54)= p01 (otherwise the belief update is independent of observations or actions and the problem
becomes trivial). Without loss of generality, set B = 1. We have






Vβ,m(ω; u = 1) = (1 − (cid:15))ω + β(cid:2)(1 − (cid:15))ωVβ,m(p11)
+(1 − (1 − (cid:15))ω)Vβ,m(T (φ(ω)))(cid:3),

Vβ,m(ω; u = 0) = m + βVβ,m(T (ω)).

(2.16)

Deﬁne passive set P (m) as the set of all belief states in which taking the passive action u = 0 is optimal:

P (m) ∆= {ω : Vβ,m(ω; u = 1) (cid:54) Vβ,m(ω; u = 0)}.

(2.17)

It is clear that the passive set P (m) changes from the empty set to the closed interval [0, 1] as m increases
from −∞ to ∞. However, such change may not be monotonic as m increases. But if P (m) does increase
monotonically with m, then for each value ω of the belief state, one can deﬁne the unique m that makes
it join P (m) and stay in the set forever.
Intuitively, such m measures in a well-ordered manner the
attractiveness of activating the arm in belief state ω compared to other belief states: the larger is the m
that is required for it to be passive, the more is the incentive to activate the arm in belief state ω, even
in the problem without m. This Lagrangian multiplier m is thus called ‘subsidy for passivity’ by Whittle
who formalized the following deﬁnition of indexability and Whittle index (Whittle [17]).
Deﬁnition 2.1. A restless multi-armed bandit is indexable if for each single-armed bandit in a problem
with subsidy m for passivity, the set of arm states P (m) in which passivity is optimal increases monoton-
ically from the empty set to the whole state space as m increases from −∞ to +∞. Under indexability,
the Whittle index of an arm state is deﬁned as the inﬁmum subsidy m such that the state remains in the
passive set.

For our model in which the arm state is given by the belief vector, the indexability is equivalent to the

following:

If Vβ,m(ω; u = 1) (cid:54) Vβ,m(ω; u = 0), then
∀ m(cid:48) > m, Vβ,m(cid:48)(ω; u = 1) (cid:54) Vβ,m(cid:48)(ω; u = 0).

Under indexability, the Whittle index W (ω) of arm state ω is deﬁned as

W (ω) ∆= inf{m : Vβ,m(ω; u = 1) (cid:54) Vβ,m(ω; u = 0)}.

(2.18)

(2.19)

6

Liu K Q et al. arXiv version

In the following we derive useful properties of the value functions Vβ,m(ω; u = 1), Vβ,m(ω; u = 0) and
Vβ,m(ω). Our strategy is to ﬁrst establish those properties for ﬁnite horizons and then extend them to
the inﬁnite horizon by the uniform convergence of the value functions of the former to the latter. Deﬁne
the T -horizon value function V1,T,β,m(ω) as the maximum expected total discounted reward achievable
over the next T time slots starting from the initial belief state ω. Then

V1,T,β,m(ω) = max{V1,T,β,m(ω; u = 1); V1,T,β,m(ω; u = 0)},

(2.20)

where V1,T,β,m(ω; u = 1) and V1,T,β,m(ω; u = 0) denote, respectively, the maximum expected total dis-
counted reward achievable given the initial active and passive actions over the next T time slots starting
from the initial belief state ω:

V1,T,β,m(ω; u = 1) = (1 − (cid:15))ω + (1 − (cid:15))ωβV1,T −1,β,m(p11)

+ (1 − (1 − (cid:15))ω)βV1,T −1,β,m

(cid:16)

T (

(cid:17)
(cid:15)ω
1−(1−(cid:15))ω )

,

V1,T,β,m(ω; u = 0) = m + βV1,T −1,β,m(T (ω)),

V1,0,β,m(·) ≡ 0.

(2.21)

(2.22)
(2.23)

From the above recursive equations, we can analyze V1,T,β,m(ω) by backward induction on T . It is easy
to see that for any ω,

V1,1,β,m(ω; u = 1) = (1 − (cid:15))ω,

V1,1,β,m(ω; u = 0) = m.

(2.24)

Therefore V1,T,β,m(ω) is the maximum of two linear equations and thus piecewise linear and convex
for T = 1 (in both ω and m). Exploiting the Bayes’ rule that leads to the following term

(1 − (1 − (cid:15))ω)βV1,T −1,β,m

(cid:16)

T (

(cid:17)
(cid:15)ω
1−(1−(cid:15))ω )

,

(2.25)

which has a coeﬃcient (1 − (1 − (cid:15))ω) also appeared as the denominator of the expression inside the linear
operator T , the recursive equation set (2.21) and (2.22) shows that V1,T,β,m(ω) is the maximum of two
convex and piecewise linear functions and thus piecewise linear and convex for any T > 1 (in both ω
and m). Motivated by the Lipschitz continuity of T ◦ φ, we show in Lemma 2.3 that V1,T,β,m(ω) is also
Lipschitz continuous under certain conditions. In the following, we ﬁrst establish a monotonic property
of V1,T,β,m(ω) in the case of p11 > p01 (positively correlated Markov chain).
Lemma 2.2.

If p11 > p01, then V1,T,β,m(ω) is monotonically increasing with ω ∈ [0, 1] for any T (cid:62) 1.
Proof. Since V1,T,β,m(ω) is piecewise linear, it is diﬀerentiable almost everywhere except on a null set
(under the Lebesgue measure on R) consisting of ﬁnite points among which both the left and right deriva-
tives at any point exist but not equal. To prove that the continuous function V1,T,β,m(ω) is monotonically
increasing with ω, we only need to show

1,T,β,m(ω) (cid:62) 0,
V (cid:48)

∀ω ∈ (0, 1),

(2.26)

where V (cid:48)
1,T,β,m(ω) denotes the right derivative of V1,T,β,m(·) as a function of the belief state with m
ﬁxed. From (2.24), the value function V1,1,β,m(ω) = max{(1 − (cid:15))ω, m} is monotonically increasing with
nonnegative right derivative 1 − (cid:15) or 0. Assume (2.26) is true for T (cid:62) 1, then for T + 1 we have
V1,T +1,β,m(ω) = max{fT (ω), gT (ω)} with

Liu K Q et al. arXiv version

fT (ω) = (1 − (cid:15))ω + (1 − (cid:15))ωβV1,T,β,m(p11) + (1 − (1 − (cid:15))ω)βV1,T,β,m(T ◦ φ(ω)),

gT (ω) = m + βV1,T,β,m(T (ω)).

From the above, we have

f (cid:48)
T (ω) = (1 − (cid:15)) + (1 − (cid:15))βV1,T,β,m(p11) − (1 − (cid:15))βV1,T,β,m(T ◦ φ(ω))

+ V (cid:48)
T (ω) = β(p11 − p01)V (cid:48)
g(cid:48)

1,T,β,m(T ◦ φ(ω)) (cid:15)β(p11−p01)
1−(1−(cid:15))ω ,

1,T,β,m(T (ω)),

7

(2.27)

(2.28)

T (·), g(cid:48)

T (·) and V (cid:48)

where f (cid:48)
1,T,β,m(·) denote the right derivatives of the corresponding functions. We have
used the fact that φ(·) is monotonically increasing and when p11 > p01, T (·) is also monotonically
increasing and that

φ(cid:48)(ω) =

(cid:15)
(1 − (1 − (cid:15))ω)2 .

(2.29)

By the induction hypothesis and (2.28), if p11 > p01 then gT (ω) is monotonically increasing (since
T (ω) (cid:62) 0) and
g(cid:48)

f (cid:48)
T (ω) = (1 − (cid:15)) + (1 − (cid:15))βV1,T,β,m(p11) − (1 − (cid:15))βV1,T,β,m(T (
))(T ◦ φ)(cid:48)(ω)

+ (1 − (1 − (cid:15))ω)βV (cid:48)

1,T,β,m(T (

(cid:15)ω
(cid:15)ω+1−ω ))

(2.30)

(cid:62) (1 − (cid:15)) + (cid:15)β(p11−p01)

1−(1−(cid:15))ω V (cid:48)

1,T,β,m(T (

(cid:15)ω+1−ω )) > 0,

(cid:15)ω
(cid:15)ω + 1 − ω
(cid:15)ω

where both the ﬁrst and second inequalities are due to the monotonically increasing property of V1,T,β,m(·)
under the assumption that p11 > p01 by our induction hypothesis and

p01 (cid:54) T (ω) (cid:54) p11, 0 (cid:54) φ(ω) (cid:54) 1,

∀ ω ∈ [0, 1].

(2.31)

This proves the monotonically increasing property of fT (ω). Thus V1,T +1,β,m(ω) = max{fT (ω), gT (ω)}
is also monotonically increasing and the proof by induction is ﬁnished.

Now we show that under a constraint on the discount factor β ∈ (0, 1), the value function V1,T,β,m(ω)

is a Lipschitz function:
Lemma 2.3.

Suppose the discount factor β ∈ (0, 1) satisﬁes

Then ∀ T (cid:62) 1 and ∀ ω, ω(cid:48) ∈ [0, 1],

β <

1
(2−(cid:15))|p11−p01| .

|V1,T,β,m(ω) − V1,T,β,m(ω(cid:48))| (cid:54) C|ω − ω(cid:48)|, where

C =

1−(cid:15)
1−(2−(cid:15))β|p11−p01| .

(2.32)

(2.33)

(2.34)

Proof. We prove this by induction. Without loss of generality, assume ω < ω(cid:48). For the case of T = 1,

|V1,1,β,m(ω) − V1,1,β,m(ω(cid:48))| =






0,
(1 − (cid:15))ω(cid:48) − m,
(1 − (cid:15))|ω − ω(cid:48)|,

m (cid:62) (1 − (cid:15))ω(cid:48);
if (1 − (cid:15))ω (cid:54) m < (1 − (cid:15))ω(cid:48);
if m < (1 − (cid:15))ω.

8

Liu K Q et al. arXiv version

Thus |V1,1,β,m(ω) − V1,1,β,m(ω(cid:48))| (cid:54) (1 − (cid:15))|ω − ω(cid:48)| (cid:54) C|ω − ω(cid:48)|. Assume that for T (cid:62) 1, |V1,T,β,m(ω) −
V1,T,β,m(ω(cid:48))| (cid:54) C|ω − ω(cid:48)| holds, i.e., neither the left nor the right derivative of V1,T,β,m(·) can exceed C.
To prove |V1,T +1,β,m(ω) − V1,T +1,β,m(ω(cid:48))| (cid:54) C|ω − ω(cid:48)|, recall the deﬁnitions of fT (ω) and gT (ω) in (2.27)
and their right derivatives f (cid:48)

T (ω) in (2.28) and observe the following inequalities:

T (ω) and g(cid:48)

|V1,T,β,m(p11) − V1,T,β,m(T (

(cid:15)ω

(cid:15)ω+1−ω ))| (cid:54) C|p11 − T (
(cid:54) C|p11 − p01|,

(cid:15)ω
(cid:15)ω+1−ω )|

(cid:15)
1 − (1 − (cid:15))ω

|V (cid:48)

1,T,β,m(T (

(cid:15)ω

(cid:15)ω+1−ω ))| (cid:54) C.

(2.35)

Thus we have the following lower and upper bounds on f (cid:48)

T (ω) and g(cid:48)

T (ω):

(1 − (cid:15)) − (2 − (cid:15))βC|p11 − p01| (cid:54) f (cid:48)
−Cβ|p11 − p01| (cid:54) g(cid:48)

T (ω) (cid:54) (1 − (cid:15)) + (2 − (cid:15))βC|p11 − p01|,
T (ω) (cid:54) Cβ|p11 − p01|.

(2.36)

Note that for the case of p11 < p01, the left derivative of V1,T,β,m(·) has been used due to the monotonically
decreasing property of T (·). From (2.28) and (2.36), we have

|V (cid:48)

1,T +1,β,m(ω)| (cid:54) (1 − (cid:15)) + (2 − (cid:15))βC|p11 − p01|

= (1 − (cid:15)) + (2 − (cid:15))β|p11 − p01|

1−(cid:15)
1−(2−(cid:15))β|p11−p01|

=

1−(cid:15)

1−(2−(cid:15))β|p11−p01| = C.

Since V1,T +1,β,m(ω) is absolutely continuous, the above implies that

|V1,T +1,β,m(ω) − V1,T +1,β,m(ω(cid:48))| (cid:54) C|ω − ω(cid:48)|.

The proof is thus ﬁnished by the induction process.
Last, we give a lemma establishing the order of V (cid:48)

1,T,β,m( · ; u = 0) under certain
conditions which further leads to a threshold structure of the optimal single-arm policy as detailed in
Section 2.3.
Lemma 2.4.

Suppose that p11 > p01 and β (cid:54) 1/[(3 − (cid:15))(p11 − p01)], we have

1,T,β,m( · ; u = 1) and V (cid:48)

1,T,β,m(ω; u = 1) (cid:62) V (cid:48)
V (cid:48)

1,T,β,m(ω; u = 0),

(2.37)

1,T,β,m(ω; u = i) denotes the right derivative of V1,T,β,m( · ; u = i) at ω for i ∈ {0, 1}. The above

where V (cid:48)
inequality is also true if p01 > p11 and β (cid:54) 1/[(5 − 2(cid:15))(p01 − p11)].
Proof. Again, we prove by induction on the time horizon T . When T = 1, it is clear that V1,1,β,m(ω; u =
1) = (1 − (cid:15))ω and V1,1,β,m(ω; u = 0) = m:

1,1,β,m(ω; u = 1) = 1 − (cid:15) > V (cid:48)
V (cid:48)

1,1,β,m(ω; u = 0) = 0.

(2.38)

Assume that V (cid:48)
and β (cid:54)
1

1,T,β,m(ω; u = 1) (cid:62) V (cid:48)

(5−2(cid:15))(p01−p11) , Cβ(p01 − p11) (cid:54) (1 − (cid:15)) − (2 − (cid:15))βC(p01 − p11), which shows that f (cid:48)

1,T,β,m(ω; u = 0) for T (cid:62) 1. From (2.36), we have, in case of p01 > p11
T (ω).

T (ω) (cid:62) g(cid:48)

Liu K Q et al. arXiv version

9

When p11 > p01, V1,T,β,m(ω) is increasing with ω with nonnegative right derivatives by Lemma 2.2. We
can thus obtain tighter bounds on f (cid:48)

T (ω) and g(cid:48)

T (ω):

(1 − (cid:15)) (cid:54) f (cid:48)
0 (cid:54) g(cid:48)

T (ω) (cid:54) (1 − (cid:15)) + (2 − (cid:15))βC(p11 − p01),
T (ω) (cid:54) Cβ(p11 − p01).

If β (cid:54)
thus complete.

(3−(cid:15))(p11−p01) , we have Cβ(p11 − p01) (cid:54) (1 − (cid:15)), which shows that f (cid:48)

1

T (ω) (cid:62) g(cid:48)

T (ω). The proof is

2.3 Threshold Policy and Indexability

In this section, we show that the optimal single-arm policy is a threshold policy under the constraints
on the discount factor β speciﬁed in Section 2.2 and analyze the conditions for indexability. First, for
a ﬁnite-horizon single-armed bandit, a threshold policy π is deﬁned by a time-dependent real number
ωT,β(m) such that

uT,m(ω) =

(cid:26)1,
0,

if ω > ωT,β(m);
if ω (cid:54) ωT,β(m).

(2.39)

In the above uT,m(ω) ∈ {0, 1} is the action taken under π at the current state ω with T slots remaining.
Intuitively, the larger ω is, the larger expected immediate reward to accrue and thus more attractive to
activate the arm. We formalize this intuition under certain conditions in the following theorem.
Theorem 2.5. Suppose that p11 > p01 and β (cid:54)
policy π∗ is a threshold policy, i.e., there exists ω∗

(3−(cid:15))(p11−p01) . For any T (cid:62) 1, the optimal single-arm
T,β(m) ∈ R such that under π∗, the optimal action is

1

u∗
T,m(ω) =

(cid:26)1,
0,

if ω > ω∗
if ω (cid:54) ω∗

T,β(m);
T,β(m).

Furthermore, at the threshold ω∗

T,β(m),

V1,T,β,m(ω∗

T,β(m); u = 0) = V1,T,β,m(ω∗

T,β(m); u = 1).

(2.40)

The conclusion is also true for the case of p01 > p11 and β (cid:54)

1
(5−2(cid:15))(p01−p11) .

Proof. At T = 1, V1,1,β,m(ω; u = 1) = (1 − (cid:15))ω, V1,1,β,m(ω; u = 0) = m. Thus we can choose ω∗
follows:

1,β(m) as

ω∗

1,β(m) =






c,
m
1−(cid:15) ,
b,

if m (cid:62) 1 − (cid:15);
if 0 (cid:54) m < 1 − (cid:15);
if m < 0,

where b < 0, c > 1 are arbitrary constants. For T (cid:62) 1, when the condition on β is satisﬁed, Lemma 2.4
shows that

hT (ω) ∆= V1,T,β,m(1; u = 1) − V1,T,β,m(1; u = 0),

T (ω) (cid:62) 0,
h(cid:48)

∀ ω ∈ (0, 1).

(2.41)

(2.42)

This shows that hT (·) is monotonically increasing and either has no zeros in the interval [0, 1] or intersects
with it over a closed interval (which can be a single point) only. Specially,

10

Liu K Q et al. arXiv version

V1,T,β,m(0; u = 1) = βV1,T −1,β,m(p01),
V1,T,β,m(0; u = 0) = m + βV1,T −1,β,m(p01),
V1,T,β,m(1; u = 1) = (1 − (cid:15)) + βV1,T −1,β,m(p11),
V1,T,β,m(1; u = 0) = m + βV1,T −1,β,m(p11).

Consider the following three regions of m.

(i) 0 (cid:54) m < 1 − (cid:15).

In this case, V1,T,β,m(0; u = 1) (cid:54) V1,T,β,m(0; u = 0) and V1,T,β,m(1; u = 1) >
V1,T,β,m(1; u = 0). Therefore hT (·) intersects over (at least) one point in [0, 1]. This point can thus
be chosen as ω∗

T,β(m).

(ii) m < 0. In this case, V1,T,β,m(0; u = 1) > V1,T,β,m(0; u = 0) and V1,T,β,m(1; u = 1) > V1,T,β,m(1; u =

0). So hT (·) is strictly positive over [0, 1] and we can choose ω∗

T,β(m) = b with any b < 0.

(iii) m (cid:62) (1 − (cid:15)). In this case, always choosing the passive action is clearly optimal as the expected
immediate reward is uniformly upper-bounded by m over the whole belief state space. We can thus
choose ω∗

T,β(m) = c with any c > 1.

In conclusion, when the conditions in the theorem are satisﬁed, the optimal ﬁnite-horizon single-arm
policy is a threshold policy for any horizon length T (cid:62) 1.

In the next theorem, we show that the optimal single-arm policy over the inﬁnite horizon is also a

threshold policy under the same conditions.
Theorem 2.6.
Fix the subsidy m. The ﬁnite-horizon value functions V1,T,β,m(·), V1,T,β,m( · ; u =
1) and V1,T,β,m( · ; u = 0) uniformly converge to the inﬁnite-horizon value functions Vβ,m(·), Vβ,m( · ; u =
1) and Vβ,m( · ; u = 0) which are consequently obedient to the same properties established in Lemmas 2.2
and 2.3 and Theorem 2.5.

Proof. Consider the single-armed bandit with subsidy. We will apply two diﬀerent scenarios to analyze
the relation between V1,T,β,m(ω) and Vβ,m(ω). First, we apply a T -horizon optimal policy to the ﬁrst T
time slots and the inﬁnite-horizon optimal policy to the future slots. Suppose that the initial belief state
is ω and after T slots the belief state becomes to a policy-dependent random variable ω(T + 1). Then
the maximum expected total discounted reward under this policy is V1,T,β,m(ω) + βT E[Vβ,m(ω(T + 1))]
where the expectation is taken with respect to ω(T + 1). By the deﬁnition of Vβ,m(ω), we have

V1,T,β,m(ω) + βT E[Vβ,m(ω(T + 1))] (cid:54) Vβ,m(ω).

In the second scenario, we apply the inﬁnite-horizon optimal policy to the RMAB and the expected total
discounted reward over the ﬁrst T slots is
Vβ,m(ω) − βT E[Vβ,m(ω(T + 1))]. By the deﬁnition of V1,T,β,m(ω), we have

Vβ,m(ω) − βT E[Vβ,m(ω(T + 1))] (cid:54) V1,T,β,m(ω).

Combining the two inequalities above, we obtain

0 (cid:54) Vβ,m(ω) − V1,T,β,m(ω) (cid:54) βT max{1, m}

1 − β

→ 0,

as T → ∞.

Thus V1,T,β,m(·) uniformly converges to Vβ.m(·) as T → ∞. The uniform convergence of V1,T,β,m( · ; u = 1)

Liu K Q et al. arXiv version

11

and V1,T,β,m( · ; u = 0) to Vβ,m( · ; u = 1) and Vβ,m( · ; u = 0) respectively can be shown similarly. For any
(cid:15) > 0, there exists a T0 (cid:62) 1 such that |Vβ,m(ω) − V1,T,β,m(ω)| < (cid:15) for any T (cid:62) T0 and any ω ∈ [0, 1].
Therefore the properties of monotonicity and Lipschitz-continuity of V1,T,β,m(ω) established under certain
conditions in Lemmas 2.2 and 2.3 and the convexity of the ﬁnite-horizon value functions also hold for
Vβ,m(ω). Recall the deﬁnition of hT (ω) in (2.41) and deﬁne the continuous function (as the diﬀerence of
two convex functions Vβ,m( · ; u = 1) and Vβ,m( · ; u = 0)):

h(·) = Vβ,m( · ; u = 1) − Vβ,m( · ; u = 0) = lim
T →∞

hT (·).

(2.43)

The above limit is well-deﬁned since V1,T,β,m( · ; u = i) converges uniformly to Vβ,m( · ; u = i) as T → ∞,
for i ∈ {0, 1}. Since hT (ω) is monotonically increasing with ω for any T (cid:62) 1 according to Lemma 2.4,
h(ω) is also monotonically increasing; it follows that the inﬁnite-horizon optimal policy is a threshold
policy.

Thus far we have established the threshold structure of the optimal single-arm policy with subsidy based
on the analysis of Vβ,m(ω) as a function of the belief state ω with m ﬁxed. To study the indexability
condition, we now analyze the properties of Vβ,m(ω) as a function of the subsidy m with the starting
belief ω ﬁxed. From Deﬁnition 2.1 and the threshold structure of the optimal policy, the indexability of
our model is reduced to requiring that the threshold ω∗
β(m) is monotonically increasing with m (if the
threshold is a closed interval then the right end is selected). Note that for the inﬁnite-horizon problem, the
β(m) is independent of time. Furthermore, Vβ,m(ω) is also convex in m as for any m1, m2 ∈ R
threshold ω∗
and θ ∈ (0, 1) the optimal policy π∗
β(θm1 + (1 − θ)m2) achieving Vβ,θm1+(1−θ)m2 (ω) applied respectively
on the problem with subsides m1 and m2 cannot outperform those achieving Vβ,m1(ω) and Vβ,m2(ω).
Speciﬁcally, let ra be the expected total discounted reward from the active action and rp(m) that from
the passive action under π∗

β(θm1 + (1 − θ)m2) applied to the problem with subsidy m, then

θVβ,m1(ω) + (1 − θ)Vβ,m2 (ω) (cid:62) ra + θrp(m1) + (1 − θ)rp(m2)

= ra + rp(θm1 + (1 − θ)m2)
= Vβ,θm1+(1−θ)m2(ω).

β(m1) and π∗

Since Vβ,m(ω) is convex in m, its left and right derivatives with m exist at every point m0 ∈ R. Further-
β(m2) achieving Vβ,m1(ω) and Vβ,m2(ω) for any m1, m2 ∈ R,
more, consider two policies π∗
respectively. With a similar interchange argument of π∗
β(m2) as above, we have |Vβ,m1(ω) −
Vβ,m2 (ω)| (cid:54) 1
1−β |m1 − m2| and Vβ,m(ω) is Lipschitz continuous in m. By the Rademacher theorem (see
Heinonen [7]), Vβ,m(ω) is diﬀerentiable almost everywhere in m. For a small increase of m, the rate at
which Vβ,m(ω) increases is at least the expected total discounted passive time under any optimal policy
for the problem with subsidy m starting from the belief state ω. In the following theorem, we formalize
this relation between the value function and the passive time as well as a suﬃcient condition for the
indexability of our model.
Let Π∗
Theorem 2.7.
initial belief state ω. Deﬁne the passive time

β(m) denote the set of all optimal single-arm policies achieving Vβ,m(ω) with

β(m1) and π∗

Dβ,m(ω) ∆=

max
β (m)∈Π∗
π∗

β (m)

Eπ∗

β (m)

(cid:34) ∞
(cid:88)

t=1

βt−11(u(t)=0)

(cid:12)
(cid:12)
(cid:12) ω(1) = ω

(cid:35)

.

(2.44)

12

Liu K Q et al. arXiv version

The right derivative of the value function Vβ,m(ω) with m, denoted by dVβ,m(ω)
and

(dm)+ , exists at every value of m

dVβ,m(ω)
(dm)+

(cid:12)
(cid:12)
(cid:12)
(cid:12)m=m0

= Dβ,m0(ω).

(2.45)

Furthermore, the single-armed bandit is indexable if at least one of the following condition is satisﬁed:

i. for any m0 ∈ [0, 1 − (cid:15)) the optimal policy is a threshold policy with threshold ω∗

β(m0) ∈ [0, 1) (if the

threshold is a closed interval then the right end is selected) and

dVβ,m(ω∗

β(m0); u = 0)
(dm)+

(cid:12)
(cid:12)
(cid:12)
(cid:12)m=m0

>

dVβ,m(ω∗

β(m0); u = 1)
(dm)+

(cid:12)
(cid:12)
(cid:12)
(cid:12)m=m0

.

ii. for any m0 ∈ R and ω ∈ P (m0), we have

dVβ,m(ω; u = 0)
(dm)+

(cid:12)
(cid:12)
(cid:12)
(cid:12)m=m0

(cid:62) dVβ,m(ω; u = 1)
(dm)+

(cid:12)
(cid:12)
(cid:12)
(cid:12)m=m0

.

(2.46)

(2.47)

Proof. The proof of (2.45) follows directly from the argument in Theorem 1 in Liu [9] and is omitted
here. To prove the suﬃciency of (2.46), we note that if it is true then there exists a ∆m > 0 such that
∀ m ∈ (m0, m0 + ∆m),

Vβ,m(ω∗
> Vβ,m(ω∗

β(m0); u = 0) − Vβ,m0 (ω∗
β(m0); u = 1) − Vβ,m0 (ω∗

β(m0); u = 0)
β(m0); u = 1).

β(m0); u = 0) = Vβ,m0(ω∗

β(m0); u = 1) which implies that the threshold ω∗

Since Vβ,m0(ω∗
> Vβ,m(ω∗
β(m0) remains in the passive set as m con-
tinuously increases so P (m) is monotonically increasing with m. This conclusion is clearly true for the
trivial case of m < 0 or m (cid:62) 1 − (cid:15). The suﬃciency of (2.47) is obvious as any ω ∈ P (m) is impossible to
escape from P (m) as m increases due to the nondecreasing property of Vβ,m(ω; u = 0) − Vβ,m(ω; u = 1)
enforced by (2.47).

β(m0); u = 1), we have Vβ,m(ω∗

β(m0); u = 0)

Theorem 2.7 essentially provides a way for checking the indexability condition in terms of the passive

times. For example, equation (2.46) is equivalent to for any m ∈ [0, 1 − (cid:15)),

(cid:104)

β

(1 − (cid:15))ω∗

β(m)Dβ,m(p11) + (1 − (1 − (cid:15))ω∗

β(m))Dβ,m

(cid:16)

T (cid:0)

β (m)

(cid:15)ω∗
β (m)+1−ω∗

(cid:15)ω∗

β (m)

(cid:1)(cid:17)(cid:105)

(2.48)

< 1 + βDβ,m(T (ω∗

β(m))).

1−β ] for any m ∈ R. When β = 0.5,
The above strict inequality clearly holds if β < 0.5 since Dβ,m(·) ∈ [0,
we prove by contradiction that the strict inequality (2.49) must hold under the threshold structure of the
optimal policy. If ω∗
β(m) = 0 then (2.49) is clearly true. Assume that the left and right sides of (2.49)
are equal and ω∗

β(m) (cid:54)= 0. In this case, we have

1

Dβ,m(p11) = 1
Dβ,m(T (ω∗

1−β ,
β(m))) = 0.

(2.49)

(2.50)

Equation (2.50) implies that starting from T (ω∗
means that the threshold ω∗

β(m) is strictly below p11 and we have a contradiction to (2.49). Another

β(m)), always activating the arm is strictly optimal. This

Liu K Q et al. arXiv version

13

easier way to see that the bandit is indexable if β (cid:54) 0.5 is that (2.47) would be satisﬁed since no strict
inequality is required. However, condition (2.49) provides a convenient way for approximately computing
the passive times as well as the value functions which leads to an eﬃcient algorithm for evaluating the
indexability and solving for the Whittle index function for any β ∈ (0, 1), as detailed in the next section.
Corollary 2.8.

The restless bandit is indexable if β (cid:54) 0.5.

2.4 The Approximated Whittle Index

The threshold structure of the optimal single-arm policy under certain conditions yields the following
iterative nature of the dynamic equations for both Dβ,m(ω) and Vβ,m(ω). Deﬁne the ﬁrst crossing time

L(ω, ω(cid:48)) = min

0(cid:54)k<∞

{k : T k(ω) > ω(cid:48)}.

(2.51)

In the above T 0(ω) ∆= ω and we set L(ω, ω(cid:48)) = +∞ if T k(ω) (cid:54) ω for all k (cid:62) 0. Clearly L(ω, ω(cid:48)) is
the minimum time slots required for a belief state ω to stay in the passive set P (m) before the arm is
activated given a threshold ω(cid:48) ∈ [0, 1). Consider the nontrivial case where p01, p11 ∈ (0, 1) and p01 (cid:54)= p11
such that the Markov chain of the internal arm states is aperiodic and irreducible and that the belief
update is action-dependent. From (2.5), if p11 > p01 then

L(ω, ω(cid:48)) =


0,

(cid:22)



∞,

log

p01−ω(cid:48) (1−p11+p01)
p01−ω(1−p11+p01)
p11−p01

(cid:23)

ω > ω(cid:48)

+ 1, ω (cid:54) ω(cid:48) < ωo

;

ω (cid:54) ω(cid:48), ω(cid:48) (cid:62) ωo

(2.52)

or if p11 < p01 then

L(ω, ω(cid:48)) =






ω > ω(cid:48)
0,
ω (cid:54) ω(cid:48), T (ω) > ω(cid:48)
1,
∞, ω (cid:54) ω(cid:48), T (ω) (cid:54) ω(cid:48)

.

(2.53)

Suppose that the following conditions are satisﬁed such that the optimal single-arm policy is a threshold
policy and the indexability holds:

β (cid:54)




min



min

(cid:110)

(cid:110)

(cid:111)

1

(3−(cid:15))(p11−p01) , 0.5
(5−2(cid:15))(p01−p11) , 0.5

1

,
(cid:111)

if p11 > p01

,

if p11 < p01

.

(2.54)

To solve for the Whittle index function W (ω), given the current arm state ω, we aim to ﬁnd out the
minimum subsidy m that makes it as a threshold:

Vβ,m(ω) = Vβ,m(ω; u = 1) = Vβ,m(ω; u = 0),
Vβ,m(ω; u = 1) = (1 − (cid:15))ω + β[(1 − (cid:15))ωVβ,m(p11)

+ (1 − (1 − (cid:15))ω)Vβ,m(T ◦ φ(ω))],

Vβ,m(ω; u = 0) = m + βVβ,m(T (ω))

.

(2.55)

(2.56)

(2.57)

Given a threshold ω∗
ﬁrst crossing time as

β(m) ∈ [0, 1) and any ω ∈ [0, 1], the value function Vβ,m(ω) can be expanded by the

14

Liu K Q et al. arXiv version

Vβ,m(ω) =

=

β (m))

β (m))

1 − βL(ω,ω∗
1 − β
1 − βL(ω,ω∗
1 − β

m + βL(ω,ω∗

β (m))Vβ,m(T L(ω,ω∗

β (m))(ω); u = 1)

m + βL(ω,ω∗

β (m))

(cid:26)

(1 − (cid:15))T L(ω,ω∗

β (m))(ω) + β

(cid:20)
(1 − (cid:15))T L(ω,ω∗

β (m))(ω)Vβ,m(p11)

+ (1 − (1 − (cid:15))T L(ω,ω∗

β (m))(ω))Vβ,m

(cid:18)

(cid:18)

T

L(ω,ω∗
β

(cid:15)T

(cid:15)T
(m))

L(ω,ω∗

β (m))

(ω)+1−T

(ω)
L(ω,ω∗
β

(cid:19)(cid:19)(cid:21)(cid:27)
.

(m))

(ω)

(2.58)

There is no doubt that the last item of the above equation has caused us trouble in solving for Vβ,m(ω).
However, if we let

f (ω, ω∗

β(m)) = T

(cid:18)

(cid:15)T
(m))

L(ω,ω∗

β (m))

(ω)+1−T

(ω)
L(ω,ω∗
β

(cid:19)

(m))

(ω)

L(ω,ω∗
β

(cid:15)T
p11(cid:15)T L(ω,ω∗
(cid:15)T L(ω,ω∗

=

β (m))(ω) + p01(1 − T L(ω,ω∗
β (m))(ω) + 1 − T L(ω,ω∗

β (m))(ω)

β (m))(ω))

(2.59)

and construct iteratively the sequence {kn} as kn+1 = f (kn, ω∗
following sequence of equations:

β(m)) with k0 = ω. We then get the

Vβ,m(k0) =

Vβ,m(k1) =

β (m))

β (m))

1 − βL(k0,ω∗
1 − β
× T L(k0,ω∗
1 − βL(k1,ω∗
1 − β
× T L(k1,ω∗

m + βL(k0,ω∗

β (m))(cid:8)(1 − (cid:15))T L(k0,ω∗

β (m))(k0)Vβ,m(p11) + (1 − (1 − (cid:15))T L(k0,ω∗

m + βL(k1,ω∗

β (m))(cid:8)(1 − (cid:15))T L(k1,ω∗

β (m))(k1)Vβ,m(p11) + (1 − (1 − (cid:15))T L(k1,ω∗

β (m))(k0) + β(cid:2)(1 − (cid:15))
β (m))(k0))Vβ,m(k1)(cid:3)(cid:9)

β (m))(k1) + β(cid:2)(1 − (cid:15))
β (m))(k1))Vβ,m(k2)(cid:3)(cid:9)

.

· · ·

Vβ,m(kn) =

1 − βL(kn,ω∗
1 − β

β (m))

m + βL(kn,ω∗

β (m))(cid:8)(1 − (cid:15))T L(kn,ω∗

β (m))(kn)

+ β(cid:2)(1 − (cid:15)) · T L(kn,ω∗
× T L(kn,ω∗

β (m))(kn))Vβ,m(kn+1)(cid:3)(cid:9)

β (m))(kn)Vβ,m(p11) + (1 − (1 − (cid:15))

· · ·

For suﬃciently large n, we can get an estimation of Vβ,m(ω) = Vβ,m(k0) with an arbitrarily small
error by setting Vβ,m(kn+1) = 0 whose error is discounted by β in computing Vβ,m(kn) thus causing a
geometrically decreasing error propagation in the backward computation process for Vβ,m(k0). Note that
we ﬁrst compute Vβ,m(p11) in the same way by setting k0 = p11 in the above equation set. Therefore
we can have an estimation of Vβ,m(ω) with arbitrarily high precision for any ω ∈ [0, 1]. Interestingly,
extensive numerical results found that {kn} quickly converges to a limit belief state k (independent of
k0) or oscillates within a small neighborhood of k. Speciﬁcally, after 4 iterations, the diﬀerence |k4 − k|
becomes quite small so we can set Vβ,m(k5) = Vβ,m(k4) and eﬃciently solve the ﬁnite linear equation
set (up to Vβ,m(k4)). In general, the n-iteration Whittle index is based on the solution of the following
equations:

Liu K Q et al. arXiv version

15

Vβ,m(p11) =

Vβ,m(k1) =

...

Vβ,m(kn) =

1 − βL(p11,ω∗

β (m))

1 − β
+ β(cid:2)(1 − (cid:15))T L(p11,ω∗
× T L(p11,ω∗
β (m))

1 − βL(k1,ω∗
1 − β
× T L(k1,ω∗

β (m))

1 − βL(kn,ω∗
1 − β
+ β(cid:2)(1 − (cid:15))T L(kn,ω∗
× T L(kn,ω∗

m + βL(p11,ω∗

β (m))(cid:8)(1 − (cid:15))T L(p11,ω∗

β (m))(p11)

β (m))(p11)Vβ,m(p11) + (1 − (1 − (cid:15))

β (m))(p11))Vβ,m(k1)(cid:3)(cid:9)

m + βL(k1,ω∗

β (m))(cid:8)(1 − (cid:15))T L(k1,ω∗

β (m))(k1)Vβ,m(p11) + (1 − (1 − (cid:15))T L(k1,ω∗

β (m))(k1) + β(cid:2)(1 − (cid:15))
β (m))(k1))Vβ,m(k2)(cid:3)(cid:9)

m + βL(kn,ω∗

β (m))(cid:8)(1 − (cid:15))T L(kn,ω∗

β (m))(kn)

β (m))(kn)Vβ,m(p11) + (1 − (1 − (cid:15))

β (m))(kn))Vβ,m(kn)(cid:3)(cid:9).

According to Theorem 2.7, the passive time Dβ,m(ω) can also be approximately solved based on the
following equations:

Dβ,m(p11) =

1 − βL(p11,ω∗

β (m))

1 − β

+ βL(p11,ω∗

β (m))+1(cid:8)(1 − (cid:15))T L(p11,ω∗

β (m))(p11)

× Dβ,m(p11) + (1 − (1 − (cid:15))T L(p11,ω∗
1 − βL(k1,ω∗
1 − β

+ βL(k1,ω∗

β (m))

β (m))(p11))Dβ,m(k1)(cid:9)

β (m))+1(cid:8)(1 − (cid:15))T L(k1,ω∗

β (m))(k1)

× Dβ,m(p11) + (1 − (1 − (cid:15))T L(k1,ω∗

β (m))(k1))Dβ,m(k2)(cid:9)

.

Dβ,m(k1) =

...

Dβ,m(kn) =

1 − βL(kn,ω∗
1 − β

β (m))

+ βL(kn,ω∗

β (m))+1(cid:8)(1 − (cid:15))T L(kn,ω∗

β (m))(kn)

× Dβ,m(p11) + (1 − (1 − (cid:15))T L(kn,ω∗

β (m))(kn))Dβ,m(kn)(cid:9)

Substituting ω for ω∗
β(m) in the above n + 1 linear equations with n + 1 unknowns (ﬁrst solving for
ω = p11), we can obtain Vβ,m(ω(cid:48)) and Dβ,m(ω(cid:48)) for any ω(cid:48) ∈ [0, 1] according to the linear equation sets.
The indexability condition (2.46) in Theorem 2.7 can be checked online: for the original multi-armed
bandit problem and for each arm at state ω(t) at time t, we compute its approximated Whittle index
W (ω(t)) by solving a set of linear equations, which has a polynomial complexity of the iteration number
n, independent of the decision time t. At time t, for each arm, if W (·) is found to be nondecreasing with
the arm states (ω(1), ω(2), . . . , ω(t)) appeared so far starting from the initial belief vector ω(1) deﬁned
in (2.1), then the indexability has not been violated.
Interestingly, extensive numerical studies have
shown that the indexability is always satisﬁed as illustrated in Section 4.

For large β ∈ (0, 1) where the threshold structure of the optimal policy or the indexability may not
hold (i.e., condition (2.54) is not satisﬁed), we can still use the above process to solve for the subsidy m
that makes (2.55) true if it exists. Note that after computing the value functions appeared in (2.55) in
terms of m, both Vβ,m(ω; u = 1) and Vβ,m(ω; u = 0) are linear (aﬃne) in m and their equality gives a
unique solution of m if their linear coeﬃcients are not equal. This m, if exists, can thus be used as the

16

Liu K Q et al. arXiv version

approximated Whittle index W (ω) without requiring indexability or threshold-based optimal policy. If
it does not exist, we can simply set W (ω) = ωB. The existence of such an m is deﬁned as the relaxed
indexability in Liu [9]. Note that extensive numerical studies have shown that the relaxed indexability of
our model with imperfect state observations is always satisﬁed as well. Before summarizing our general
algorithm for all β ∈ (0, 1) in Section 3, we solve for the approximated Whittle index function in closed-
form for the simplest case of 0-iteration, which is referred to as the imperfect Whittle index. Note that if
(cid:15)ω
(cid:15)ω+1−ω ))
(cid:15) → 0 then T (
by Vβ,m(p01). Under this approximation, we have, for any ω ∈ [0, 1],

(cid:15)ω+1−ω ) → p01. Thus when (cid:15) is suﬃciently small, we can approximate Vβ,m(T (

(cid:15)ω

Vβ,m(ω; u = 1) = (1 − (cid:15))ω + β(cid:2)(1 − (cid:15))ωVβ,m(p11) + (1 − (1 − (cid:15))ω)Vβ,m(p01)(cid:3)
Vβ,m(ω; u = 0) = m + βVβ,m(T (ω))
1 − βL(ω,ω∗
1 − β

β (m))(cid:8)(1 − (cid:15))T L(ω,ω∗

m + βL(ω,ω∗

Vβ,m(ω) =

β (m))(ω)

β (m))

.

+ β(cid:2)(1 − (cid:15))T L(ω,ω∗

β (m))(ω)Vβ,m(p11) + (1 − (1 − (cid:15))T L(ω,ω∗

β (m))(ω))Vβ,m(p01)(cid:3)(cid:9)

By using the above three equations, we can directly solve for Vβ,m(p01) and Vβ,m(p11) in closed-form.
When p11 > p01, Vβ,m(p01) =






(1−(cid:15))p01
(1−β)(1−β(1−(cid:15))p11+β(1−(cid:15))p01) ,

if ω∗

β(m) < p01

(1−β(1−(cid:15))p11)(1−β

(1−β(1−(cid:15))p11)(1−β)(1−β
m
1−β ,

β (m))

L(p01,ω∗
L(p01,ω∗
β

)m+(1−(cid:15))(1−β)β

(m))+1

)+(1−(cid:15))(1−β)2β

L(p01,ω∗
β

L(p01 ,ω∗

β (m))

L(p01,ω∗

T
(m))+1

T

β (m))
(p01)
L(p01,ω∗
(m))
β

(p01)

,

β(m) < ω0

if p01 (cid:54) ω∗
if ω∗

β(m) (cid:62) ω0

,

Vβ,m(p11) =

(cid:40) (1−(cid:15))p11+β(1−(1−(cid:15))p11)Vβ,m(p01)
1−β(1−(cid:15))p11

m
1−β ,

,

if ω∗
if ω∗

β(m) < p11
β(m) (cid:62) p11

.

The approximate Whittle index is given by

W (ω) =





ω(1−(cid:15))(1−βp11+βp01)
1−β(1−(cid:15))p11+β(1−(cid:15))p01
(1−(cid:15))(ω−βT (ω))+C2(1−β)β[(1−β(1−(cid:15))p11)−(1−(cid:15))(ω−βT (ω))]
1−β(1−(cid:15))p11−C1β[(1−β(1−(cid:15))p11)−(1−(cid:15))(ω−βT (ω))]

,

(1−(cid:15))ω
1−β(1−(cid:15))p11+β(1−(cid:15))ω ,
(1 − (cid:15))ω,

,

if ω (cid:54) p01
if p01 < ω (cid:54) ω0
if ω0 < ω (cid:54) p11
if ω > p11

,

where

C1 =

C2 =

(1 − β(1 − (cid:15))p11)(1 − βL(p01,ω))
(1 − β(1 − (cid:15))p11)(1 − βL(p01,ω)+1) + (1 − (cid:15))(1 − β)βL(p01,ω)+1T L(p01,ω)(p01)
(1 − (cid:15))βL(p01,ω)T L(p01,ω)(p01)
(1 − β(1 − (cid:15))p11)(1 − βL(p01,ω)+1) + (1 − (cid:15))(1 − β)βL(p01,ω)+1T L(p01,ω)(p01)

,

.

Similarly, when p01 > p11, we have

Vβ,m(p11) =





(1−(cid:15))(p11(1−β)+βp01)
(1−β)(1−β(1−(cid:15))p11+β(1−(cid:15))p01) ,
(1−β(1−(1−(cid:15))p01))m+β(1−(cid:15))T (p11)(1−β)+β2(1−(cid:15))p01
(1−β)(1+β(1+β)(1−(cid:15))p01−β2(1−(cid:15))T (p11))

,

m
1−β ,

if ω∗

β(m) < p11

if p11 (cid:54) ω∗

β(m) < T (p11)

,

if ω∗

β(m) (cid:62) T (p11)

Liu K Q et al. arXiv version

17

Vβ,m(p01) =

(cid:40) (1−(cid:15))p01+β(1−(cid:15))p01Vβ,m(p11)
1−β(1−(1−(cid:15))p01)

m
1−β ,

,

if ω∗
if ω∗

β(m) < p01
β(m) (cid:62) p01

.

The approximate Whittle index is given by

W (ω) =






,

ω(1−(cid:15))(1−βp11+βp01)
1−β(1−(cid:15))p11+β(1−(cid:15))p01
(1−(cid:15))(1−β+C4β)(βp01+ω−βT (ω))
1−β(1−(1−(cid:15))p01)+(1−(cid:15))C3β(βT (ω)−βp01−ω) ,
(1−(cid:15))(1−β+βC4)(βp01+ω(1−β))
1−β(1−(1−(cid:15))p01)−(1−(cid:15))βC3(βp01+ω−βω) ,
(1−(cid:15))(βp01+(1−β)ω)
,
1+(1−(cid:15))β(p01−ω)

(1 − (cid:15))ω,

if ω (cid:54) p11

if p11 < ω < ω0

if ω0 (cid:54) ω < T (p11)

,

if T (p11) (cid:54) ω < p01
if ω (cid:62) p01

where

3 Algorithm

1−β(1−(1−(cid:15))p01)
1+β(1+β)(1−(cid:15))p01−β2(1−(cid:15))T (p11) ,

C3 =
C4 = β(1−(cid:15))T (p11)(1−β)+β2(1−(cid:15))p01

1+β(1+β)(1−(cid:15))p01−β2(1−(cid:15))T (p11) .

Our analysis leads to the algorithm for the RMAB model with imperfect observations in Algorithm 1 for
all β ∈ (0, 1).

Algorithm 1 Whittle Index Policy
Input: β ∈ (0, 1), T (cid:62) 1,N (cid:62) 2, 1 (cid:54) M < N , iteration number k
Input: initial belief state ωn(1), P(n), Bn, n = 1, ..., N
1: for t = 1, 2, ..., T do
2:
3:

Set the threshold ω∗

for n = 1, ..., N do

(cid:16)

β(m) = ωn(t) in (2.58)
and set ω = p(n)

(cid:17)

p(n)
11 , ωn(t)

β,m(T ◦ φ(ω)) from V (n)

β,m(p(n)
11 )

11 in (2.58)

Compute L
Expand (2.58) to the kth step and solve for V (n)
Compute L (T ◦ φ(ω), ωn(t)) and set ω = T ◦ φ(ω) in (2.58)
Expand (2.58) to the kth step and solve for V (n)
Compute L (T (ω), ωn(t)) and set ω = T (ω) in (2.58)
Expand (2.58) to the kth step and solve for V (n)
β,m(p(n)
Solve for V (n)
11 ) and V (n)
Solve for V (n)
β,m(T (ω)) as in (2.57)
Evaluate the solvability of the linear equation of m: V (n)
= V (n)
Set W (ωn(t)) = ωn(t)Bn and skip Step 14 if the above is unsolvable
Compute W (ωn(t)) as the solution to V (n)

β,m(ω; u = 1) by V (n)
β,m(ω; u = 0) by V (n)

β,m(ω; u = 1) = V (n)

β,m(ω; u = 1)

β,m(ω; u = 0)

β,m(p(n)
β,m(T (ω)) from V (n)
11 )
β,m(T ◦ φ(ω)) as in (2.56)

β,m(ω; u = 0)

β,m(p(n)
11 )

Choose the top M arms with the largest Whittle Indices W (ωn(t))
Observe the selected M arms and accrue reward On(t)Sn(t)Bn from each
observed arm
for n = 1, ..., N do

Update the belief state ωn(t) according to (2.3)

4:

5:
6:

7:
8:

9:

10:

11:

12:

13:

14:

15:
16:

17:
18:

4 Experimental results

We use the 4-iteration approximation algorithm to plot the approximated Whittle index function W (ω)
for the following parameters:
in Fig. 1 p11 = 0.2, p01 = 0.9, β = 0.9, (cid:15) = 0.1, B = 1; in Fig. 2 p11 =
0.6, p01 = 0.3, β = 0.9, (cid:15) = 0.1, B = 1. Note that the monotonic increasing property of W (ω) implies

18

Liu K Q et al. arXiv version

the indexability numerically while the nonlinearity of W (ω) illustrates its diﬀerence to the myopic policy
(with index ωB).

We further compare the performance of Whittle index policy with the optimal policy which is computed
by dynamic programming over a ﬁnite horizon (thus an upper bound of the optimal inﬁnite-horizon
optimal policy for this horizon). The performance of the myopic policy that chooses the M arms with the
largest ωnBn is also illustrated. From Figures 3-18, we observe that Algorithm 1 with iteration number
k = 4 achieves a near-optimal performance and outperforms the myopic policy.

5 Optimality for Homogeneous Systems

A space-wise homogeneous system for a restless bandit is deﬁned as the system with N stochastically
identical arms, i.e., the parameters P(n) and Bn do not depend on n.
In this case, our algorithm is
equivalent to the myopic policy that chooses the arms with the largest belief values and is optimal.
Theorem 5.1. Consider a space-wise homogeneous model with positively correlated arms (p11 (cid:62) p01)
and (cid:15) satisfying

(cid:15) (cid:54) p01(1 − p11)
p11(1 − p01)

=

p01p10
p11p00

,

(5.1)

the myopic policy is optimal over both ﬁnite and inﬁnite horizons.

Proof. We adopt notations similar to that in Liu et al. [10] for the case of perfect observation ((cid:15) = δ = 0)
but need several non-trivial diﬀerences due to the additional complexity introduced by observation errors.
Consider N arms in total and we choose K arms to active at each step. Let Ws(ω1, . . . , ωN ) denote the
expected total discounted reward over s steps when all arms are ordered so the probabilities that the
underlying random processes are in state 1 are ω1 (cid:62) · · · (cid:62) ωN . In Liu et al. [12], it has been proved that
the myopic policy has a dynamic queuing structure if the error probability (cid:15) satisﬁes (5.1). Then we have

Ws+1(ω1, . . . , ωN ) = (1 − (cid:15))

K
(cid:88)

i=1

ωi + βE[Ws(p11, . . . , p11, τ (ωK+1), . . . , τ (ωN ), σ(·), · · · , σ(·))],

(cid:15)ω+1−ω ), and the expectation is taken over possible
where W0(·) = 0, τ (ω) = p11ω + p01(1 − ω), σ(ω) = τ (
outcomes that can occur when the K arms that are observed are those at the left end (i.e., having
probabilities ω1, . . . , ωK that the underlying random processes are in state 1). We will describe it more
speciﬁcally. For a belief state sequence (ω1, . . . , ωK), we call (ωi1, . . . , ωis ) and (ωj1 , . . . , ωjt) a partition
of (ω1, . . . , ωK) if they satisfy:

(cid:15)ω

(i) (ωi1, . . . , ωis , ωj1, . . . , ωjt) is a rearrangement of (ω1, . . . , ωK);

(ii) i1 < · · · < is and j1 < · · · < jt.

Let P be the collection of all partitions of (ω1, . . . , ωK), the expectation above can be written as follows:

Liu K Q et al. arXiv version

19

E[Ws(p11, . . . , p11, τ (ωK+1), . . . , τ (ωN ), σ(·), . . . , σ(·))]

s
(cid:89)

(cid:88)
(

=

P

m=1

(1 − (cid:15))ωim)(

t
(cid:89)

(1 − (1 − (cid:15))ωjn ))Ws(p11, . . . , p11, τ (ωK+1), . . . , τ (ωN ), σ(ωj1), . . . , σ(ωjt)).

n=1

We can see that when ω1 (cid:62) ω2 (cid:62) · · · (cid:62) ωN , Ws(ω1, . . . , ωN ) is the value function for the myopic policy.
To prove the theorem, we ﬁrst prove that ∀s, Ws(ω1, . . . , ωN ) is linear in ωi(1 (cid:54) i (cid:54) N ). We will prove
it by induction. It is obvious that W0 = 0, W1(ω1, . . . , ωN ) = (1 − (cid:15)) (cid:80)K
i=1 ωi are linear in ω1, . . . , ωN .
Assume it is true for s, i.e., Ws(ωl) = Ws(ω1, . . . , ωl, . . . , ωN ) = alωl + bl (∀1 (cid:54) l (cid:54) N ), where al and bl
are constants independent of ωl. Now consider Ws+1(ωl) = Ws+1(ω1, . . . , ωl, . . . , ωN ). When l > K,

Ws+1(ω1, . . . ,ωl, · · · , ωN ) = (1 − (cid:15))

K
(cid:88)

i=1

ωi

+ βE[Ws(p11, . . . , p11, τ (ωK+1), . . . , τ (ωl), . . . , τ (ωN ), σ(·), . . . , σ(·))]

In this case, probability terms in expectation are only related to ω1, . . . , ωK, τ (ωl) is linear in ωl and Ws
is linear in τ (ωl), thus Ws+1 is also linear in ωl.
When l (cid:54) k, for any partition (ωi1, . . . , ωis) and (ωj1 , . . . , ωjt) of (ω1, . . . , ωK), if ωl ∈ {ωi1, . . . , ωis }, the
corresponding term

s
(cid:89)

(

m=1

(1 − (cid:15))ωim )(

t
(cid:89)

(1 − (1 − (cid:15))ωjn ))Ws(p11, . . . , p11, τ (ωK+1), . . . , τ (ωN ), σ(ωj1 ), . . . , σ(ωjt))

n=1

in the expectation is linear in ωl. If ωl ∈ {ωj1, . . . , ωjt }, by inductive hypothesis there exists ˜a, ˜b,

(1 − (1 − (cid:15))ωl)Ws(p11, . . . , p11, τ (ωK+1), . . . , τ (ωN ), σ(ωj1 ), . . . , σ(ωl), . . . , σ(ωjt ))

= (1 − (1 − (cid:15))ωl)(˜aσ(ωl) + ˜b) = ˜a((cid:15)p11ωl + p01(1 − ωl)) + ˜b(1 − (1 − (cid:15))ωl).

The equation above shows that Ws+1 is linear in ωl, thus the proposition is proved.

From above, we can assume Ws(ω1, . . . , x, . . . , y, . . . , ωN ) = ax + by + cxy + d, where a, b, c, d are

constants. If we swap the positions of x and y and make diﬀerences between the two, we have

Ws(ω1, . . . , x, . . . , y, . . . , ωN ) − Ws(ω1, . . . , y, . . . , x, . . . , ωN )

=(x − y)[Ws(ω1, . . . , 1, . . . , 0, . . . , ωN ) − Ws(ω1, . . . , 0, . . . , 1, . . . , ωN )].

Next we will prove two important properties of Ws. We let ¯ωi denote any sequence of ωis, possibly empty.
We still adopt induction to prove next two properties:
(A) 1 − (cid:15) + Ws( ¯ω1, y, ¯ω2, ¯ω3) − Ws( ¯ω1, ¯ω2, y, ¯ω3) (cid:62) 0.
(B) ∀y > x, Ws( ¯ω1, y, ¯ω2, x, ¯ω3) − Ws( ¯ω1, x, ¯ω2, y, ¯ω3) (cid:62) 0.
These are clearly true for s = 1. We will begin by proving an induction step for (B). As above, the
expression in (B) is equal to (y − x)[Ws( ¯ω1, 1, ¯ω2, 0, ¯ω3) − Ws( ¯ω1, 0, ¯ω2, 1, ¯ω3)].
Suppose the position exchange occur in the ith and jth place, i < j. If i, j (cid:54) K, for some ¯ω1
(cid:48)(which
are stochastically determined by the observations from the top K arms in the queue), by inductive hy-
pothesis,

(cid:48), ¯ω3

(cid:48), ¯ω2

20

Liu K Q et al. arXiv version

Ws( ¯ω1, 1, ¯ω2, 0, ¯ω3) − Ws( ¯ω1, 0, ¯ω2, 1, ¯ω3)
(cid:48), p01, ¯ω3

(cid:48)) − Ws−1( ¯ω1

= βE[Ws−1( ¯ω1

(cid:48), ¯ω2

(cid:48), p01, ¯ω2

(cid:48), ¯ω3

(cid:48))] (cid:62) 0.

Similarly if i, j > K,

Ws( ¯ω1, 1, ¯ω2, 0, ¯ω3) − Ws( ¯ω1, 0, ¯ω2, 1, ¯ω3)

= βE[Ws−1( ¯ω1

(cid:48), p11, ¯ω2

(cid:48), p01, ¯ω3

(cid:48)) − Ws−1( ¯ω1

(cid:48), p01, ¯ω2

(cid:48), p11, ¯ω3

(cid:48))] (cid:62) 0.

The interesting case is i (cid:54) K < j. In this case,

Ws( ¯ω1, 1, ¯ω2, 0, ¯ω3) − Ws( ¯ω1, 0, ¯ω2, 1, ¯ω3)
(cid:48), ¯ω4
(cid:48), ¯ω4

= 1 − (cid:15) + βE[Ws−1( ¯ω1
(cid:62) 1 − (cid:15) + βE[Ws−1( ¯ω1
= (1 − (cid:15))(1 − β) + βE[(1 − (cid:15)) + Ws−1( ¯ω1
(cid:62) 0,

(cid:48), p01, ¯ω3
(cid:48), p11, p01, ¯ω3

(cid:48), p11, ¯ω2
(cid:48), ¯ω2

(cid:48)) − Ws−1( ¯ω1
(cid:48)) − Ws−1( ¯ω1
(cid:48), p11, p01, ¯ω3

(cid:48), ¯ω2
(cid:48), ¯ω2
(cid:48), ¯ω4

(cid:48), p01, ¯ω4
(cid:48), p01, ¯ω4

(cid:48), p11, ¯ω3
(cid:48), p11, ¯ω3
(cid:48)) − Ws−1( ¯ω1

(cid:48))]
(cid:48))]
(cid:48), ¯ω2

(cid:48), ¯ω2

(cid:48), p11, ¯ω3

(cid:48), p01, ¯ω4

(cid:48))]

where the ﬁrst inequality follows from the inductive hypothesis for (B) and the second follows from (A).
Next we will prove (A) by induction. Suppose that y occurs within the two expressions in the ith and
jth place, i < j. If i, j (cid:54) K, similarly for some ¯ω1
(cid:48), ¯ω2

(cid:48) (they depend on the observations),

(cid:48), ¯ω3

1 − (cid:15) + Ws( ¯ω1, y, ¯ω2, ¯ω3) − Ws( ¯ω1, ¯ω2, y, ¯ω3)

= 1 − (cid:15) + βE[Ws−1( ¯ω1
= (1 − (cid:15))(1 − β) + βE[(1 − (cid:15)) + Ws−1( ¯ω1
(cid:62) 0.

(cid:48), σ(y), ¯ω2

(cid:48), ¯ω3

(cid:48)) − Ws−1( ¯ω1
(cid:48), σ(y), ¯ω2

(cid:48), ¯ω2
(cid:48), ¯ω3

(cid:48), σ(y), ¯ω3
(cid:48)) − Ws−1( ¯ω1

(cid:48))]

(cid:48), ¯ω2

(cid:48), σ(y), ¯ω3

(cid:48))]

If i, j > K, we have

1 − (cid:15) + Ws( ¯ω1, y, ¯ω2, ¯ω3) − Ws( ¯ω1, ¯ω2, y, ¯ω3)

= (1 − (cid:15))(1 − β) + βE[(1 − (cid:15)) + Ws−1( ¯ω1
(cid:62) 0.

(cid:48), τ (y), ¯ω2

(cid:48), ¯ω3

(cid:48)) − Ws−1( ¯ω1

(cid:48), ¯ω2

(cid:48), τ (y), ¯ω3

(cid:48))]

The interesting case is i (cid:54) K < j. Let ¯ω2 = ( ¯ω21, x, ¯ω22), where ¯ω1 and ¯ω21 represent K − 1 states in
total. Then

1 − (cid:15) + Ws( ¯ω1, y, ¯ω2, ¯ω3) − Ws( ¯ω1, ¯ω2, y, ¯ω3)

= 1 − (cid:15) + Ws( ¯ω1, y, ¯ω21, x, ¯ω22, ¯ω3) − Ws( ¯ω1, ¯ω21, x, ¯ω22, y, ¯ω3).

The above expression is a function of x and y, of the form ax + by + cxy + d. To prove the expres-
sion above is nonnegative for all x, y ∈ [0, 1], we just need to check out that it is true for (x, y) ∈
{(0, 0), (0, 1), (1, 0), (1, 1)}.
If x = y = 0, then

Liu K Q et al. arXiv version

21

1 − (cid:15) + Ws( ¯ω1, 0, ¯ω21, 0, ¯ω22, ¯ω3) − Ws( ¯ω1, ¯ω21, 0, ¯ω22, 0, ¯ω3)
(cid:48), p01, τ ( ¯ω22), ¯ω3
(cid:48)) − Ws−1( ¯ω1

(cid:48), p01, ¯ω4

(cid:48), τ ( ¯ω22), p01, ¯ω3

(cid:48), ¯ω4

(cid:48), p01)]

= 1 − (cid:15) + βE[Ws−1( ¯ω1
(cid:62) (1 − (cid:15))(1 − β) + βE[(1 − (cid:15))
(cid:48), p01, τ ( ¯ω22), ¯ω3

+Ws−1( ¯ω1

(cid:48), p01, ¯ω4

(cid:48)) − Ws−1( ¯ω1

(cid:48), τ ( ¯ω22), ¯ω3

(cid:48), p01, ¯ω4

(cid:48), p01)]

(cid:62) 0.

If x = y = 1, then

1 − (cid:15) + Ws( ¯ω1, 1, ¯ω21, 1, ¯ω22, ¯ω3) − Ws( ¯ω1, ¯ω21, 1, ¯ω22, 1, ¯ω3)
(cid:48)) − Ws−1( ¯ω1

(cid:48), p11, τ ( ¯ω22), ¯ω3

(cid:48), p11, ¯ω2
= 1 − (cid:15) + βE[Ws−1( ¯ω1
(cid:62) (1 − (cid:15))(1 − β) + βE[(1 − (cid:15))
(cid:48), ¯ω2

+Ws−1( ¯ω1

(cid:48), p11, p11, τ ( ¯ω22), ¯ω3

(cid:48)) − Ws−1( ¯ω1

(cid:48), ¯ω2

(cid:48), p11, τ ( ¯ω22), p11, ¯ω3

(cid:48))]

(cid:48), ¯ω2

(cid:48), p11, τ ( ¯ω22), p11, ¯ω3

(cid:48))]

(cid:62) 0.

If x = 0, y = 1, then

1 − (cid:15) + Ws( ¯ω1, 1, ¯ω21, 0, ¯ω22, ¯ω3) − Ws( ¯ω1, ¯ω21, 0, ¯ω22, 1, ¯ω3)

(cid:48), p11, ¯ω3

(cid:48), p01, τ ( ¯ω22), ¯ω4

(cid:48)) − Ws−1( ¯ω1

(cid:48), ¯ω3

(cid:48), τ ( ¯ω22), p11, ¯ω4

(cid:48), p01)]

= 2(1 − (cid:15)) + βE[Ws−1( ¯ω1
(cid:62) 2(1 − (cid:15))(1 − β) + βE[2 − 2(cid:15)
(cid:48), ¯ω3

+Ws−1( ¯ω1

(cid:48), p01, τ ( ¯ω22), p11, ¯ω4

(cid:48)) − Ws−1( ¯ω1

(cid:48), ¯ω3

(cid:48), τ ( ¯ω22), p11, ¯ω4

(cid:48), p01)]

> 0.

If x = 1, y = 0, then

1 − (cid:15) + Ws( ¯ω1, 0, ¯ω21, 1, ¯ω22, ¯ω3) − Ws( ¯ω1, ¯ω21, 1, ¯ω22, 0, ¯ω3)

= βE[Ws−1( ¯ω1
(cid:62) 0.

(cid:48), p11, τ ( ¯ω22), ¯ω3

(cid:48), p01, ¯ω4

(cid:48)) − Ws−1( ¯ω1

(cid:48), p11, τ ( ¯ω22), p01, ¯ω3

(cid:48), ¯ω4

(cid:48))]

Thus (A) is true. In fact, (B) shows that the myopic policy is optimal over ﬁnite horizons. By contra-
diction, it is easy to show that the myopic policy also maximizes the expected total discounted reward
and the expected average reward over the inﬁnite horizon. Furthermore, our proof does not depend on
the time-homogeneousness of the system so the optimality result holds even if the system parameters are
time-varying as long as p11(t) (cid:62) p01(t) and (cid:15) satisﬁes (5.1).

6 Conclusions

In this paper, we proposed a low-complexity algorithm based on the idea of relaxed indexability to
achieve a near-optimal performance for a class of RMAB with an inﬁnite state space and an imperfect
observation model. Future work includes the extension of the construction of linear equation sets by
the idea of threshold-based ﬁrst crossing time and the computation of index functions under the relaxed
indexability to more general state and observation models (see, e.g., the non-Markovian state model
considered in Liu et al. [10]).

22

Liu K Q et al. arXiv version

Acknowledgements The authors thank the anonymous referees for their helpful and constructive comments
that improved this paper.

System-1

System-2

System-3

System-4

i=1

i=1

i=1

{p(i)
11 }7
{p(i)
01 }7
{Bi}7
{p(i)
11 }7
{p(i)
01 }7
{Bi}7
{p(i)
11 }7
{p(i)
01 }7
{Bi}7
{p(i)
11 }7
{p(i)
01 }7
{Bi}7

i=1

i=1

i=1

i=1

i=1

i=1

i=1

i=1

i=1

Table 1 Experiment Setting

{0.3, 0.6, 0.4, 0.7, 0.2, 0.6, 0.8}
{0.1, 0.4, 0.3, 0.4, 0.1, 0.3, 0.5}
{0.8800, 0.2200, 0.3300, 0.1930, 1.0000, 0.2558, 0.1549}
{0.6, 0.4, 0.2, 0.2, 0.4, 0.1, 0.3}
{0.8, 0.6, 0.4, 0.9, 0.8, 0.6, 0.7}
{0.5150, 0.6666, 1.0000, 0.6296, 0.5833, 0.8100, 0.6700}
{0.1, 0.4, 0.3, 0.4, 0.1, 0.3, 0.5}
{0.3, 0.6, 0.4, 0.7, 0.2, 0.6, 0.8}
{0.7273, 0.3636, 0.5000, 0.3377, 1.0000, 0.3939, 0.2955}
{0.6, 0.7, 0.2, 0.6, 0.4, 0.5, 0.3}
{0.8, 0.4, 0.9, 0.5, 0.7, 0.2, 0.6}
{0.4286, 0.5000, 0.5397, 0.5143, 0.5306, 1.0000, 0.6190}

Table 2 Experiment Setting (continued)

System Example

(cid:15)

β

Meet threshold Meet indexability

System-1

System-2

System-3

System-4

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

0.3
0.1
0.3
0.1
0.3
0.1
0.3
0.1
0.3
0.1
0.3
0.1
0.3
0.1
0.3
0.1

0.999
0.999
0.29
0.29
0.48
0.48
0.69
0.69
0.48
0.48
0.29
0.29
0.48
0.48
0.999
0.999

conditions?
yes
yes
yes
yes
no
no
yes
yes
yes
yes
yes
yes
no
no
no
no

conditions?
no
no
yes
yes
yes
yes
no
no
yes
yes
yes
yes
yes
yes
no
no

Liu K Q et al. arXiv version

23

Figure 1 Approximated W (ω) (p11 < p01)

Figure 2 Approximated W (ω) (p11 > p01)

Figure 3 Example-1

Figure 4 Example-2

Figure 5 Example-3

Figure 6 Example-4

24

Liu K Q et al. arXiv version

Figure 7 Example-5

Figure 8 Example-6

Figure 9 Example-7

Figure 10 Example-8

Figure 11 Example-9

Figure 12 Example-10

Liu K Q et al. arXiv version

25

Figure 13 Example-11

Figure 14 Example-12

Figure 15 Example-13

Figure 16 Example-14

Figure 17 Example-15

Figure 18 Example-16

26

References

Liu K Q et al. arXiv version

1 Brown DB, Simth JE (2020) Index policies and performance bounds for dynamic selection problems. Management

Science 66(7):3029–3050.

2 Chen M, Wu K, Song L (2021) A Whittle index approach to minimizing age of multi-packet information in IoT network.

IEEE ACCESS 9: 31467 - 31480.

3 Gast N, Gaujal B, Yan C (2021, working paper) (Close to) Optimal policies for ﬁnite horizon restless bandits. https:

//hal.inria.fr/hal-03262307/file/LP_paper.pdf.

4 Gittins JC (1979) Bandit processes and dynamic allocation indices. J. R. Stat. Soc. 41(2):148–177.

5 Gittins JC, Glazebrook KD, Weber RR (2011) Multi-Armed Bandit Allocation Indices (Wiley, Chichester).

6 Hu W, Frazier PI (2017, working paper) An asymptotically optimal index policy for ﬁnite-horizon restless bandits.

https://arxiv.org/abs/1707.00205.

7 Heinonen J (2005) Lectures on Lipschitz analysis. www.math.jyu.fi/research/reports/rep100.pdf.

8 Levy BC (2008) Principles of Signal Detection and Parameter Estimation (Springer, Verlag).

9 Liu K (2021) Index policy for a class of partially observable Markov decision processes. https://arxiv.org/abs/2107.

11939.

10 Liu K, Weber RR, Zhao Q (2011) Indexability and Whittle index for restless bandit problems involving reset processes.

Proc. of the 50th IEEE Conference on Decision and Control 7690–7696.

11 Liu K, Zhao Q (2010) Indexability of restless bandit problems and optimality of Whittle index for dynamic multichannel

access. IEEE Trans. Inform. Theory 56(11):5547–5567.

12 Liu K, Zhao Q, Krishnamachari B (2010) Dynamic multichannel access with imperfect channel state detection. IEEE

Trans. Sig. Proc. 2795–2808.

13 Papadimitriou CH, Tsitsiklis JN (1999) The complexity of optimal queueing network control. Math. Oper. Res.

24(2):293–305.

14 Sondik EJ (1978) The optimal control of partially observable Markov processes over the inﬁnite horizon: discounted

costs. Operations Research 26(2):282–304.

15 Weber RR, Weiss G (1990) On an index policy for restless bandits. J. Appl. Probab. 27:637–648.

16 Weber RR, Weiss G (1991) Addendum to ‘On an index policy for restless bandits’. Adv. Appl. Prob. 23:429–430.

17 Whittle P (1988) Restless bandits: Activity allocation in a changing world. J. Appl. Probab. 25:287–298.

18 Zayas-Cab´an G, Jasin S, Wang G (2019) An asymptotically optimal heuristic for general nonstationary ﬁnite-horizon

restless multi-armed, multi-action bandits. Advanced in Applied Probability 51:745–772.

