0
2
0
2

g
u
A
9

]

C
O
.
h
t
a
m

[

1
v
7
0
7
3
0
.
8
0
0
2
:
v
i
X
r
a

Risk-Sensitive Markov Decision Processes with Combined

Metrics of Mean and Variance∗

Li Xia†

Abstract

This paper investigates the optimization problem of an inﬁnite stage discrete time

Markov decision process (MDP) with a long-run average metric considering both mean

and variance of rewards together. Such performance metric is important since the mean

indicates average returns and the variance indicates risk or fairness. However, the vari-

ance metric couples the rewards at all stages, the traditional dynamic programming is

inapplicable as the principle of time consistency fails. We study this problem from a

new perspective called the sensitivity-based optimization theory. A performance diﬀer-

ence formula is derived and it can quantify the diﬀerence of the mean-variance combined

metrics of MDPs under any two diﬀerent policies. The diﬀerence formula can be utilized

to generate new policies with strictly improved mean-variance performance. A necessary

condition of the optimal policy and the optimality of deterministic policies are derived.

We further develop an iterative algorithm with a form of policy iteration, which is proved

to converge to local optima both in the mixed and randomized policy space. Specially,

when the mean reward is constant in policies, the algorithm is guaranteed to converge to

∗This paper was supported in part by the National Natural Science Foundation of China (61573206,

11931018). This paper was initially submitted to the journal of Production and Operations Management

(POMS) in July 2019, revised in December 2019, March and May 2020, and accepted in July 2020.

†Li Xia is with the Business School, Sun Yat-Sen University, Guangzhou 510275, P. R. China. (email:

xiali5@sysu.edu.cn. Corresponding Author.)

1

 
 
 
 
 
 
the global optimum. Finally, we apply our approach to study the ﬂuctuation reduction of

wind power in an energy storage system, which demonstrates the potential applicability

of our optimization method.

Keywords: Markov decision process, risk-sensitive, mean and variance, sensitivity-based

optimization

1

Introduction

The theory of Markov decision processes (MDPs) is widely used to analyze and optimize

the performance of a stochastic dynamic system.

In the framework of MDPs, RT is de-

noted as the accumulated (discounted or undiscounted) rewards until time T , which is a

random variable. Many studies of MDPs focus on the discounted or long-run average per-
formance criteria, i.e., maximization of E[RT ]. However, its variance σ2[RT ] is also an im-

portant metric in practical problems, which can reﬂect the factors of risk, fairness, quality or

safety. For example, the mean-variance analysis for the risk management in ﬁnancial port-

folio or hedging (Kouvelis et al., 2018), the fairness of customers served in queueing systems

(Avi-Itzhak and Levy, 2004), the power quality and safety of renewable energy to electricity

grid (Li et al., 2014), the optimal advertising of fashion markets and supply chain manage-

ment (Chiu et al., 2018; Zhang et al., 2020), etc. Optimization theory of risk-sensitive MDPs

considering not only the average metric but also the variance metric is an important problem

attracting continued attention in the literature.

However, the optimization of the variance metric does not ﬁt a standard model of MDPs

because of the non-additive variance function. The value of the variance function will be

aﬀected by actions of not only the current stage but also the future stages (Bertsekas, 2005;

Feinberg and Schwartz, 2002; Puterman, 1994). Thus, the Bellman optimality equation does

not hold and the principle of time consistency (roughly speaking, if a decision at time t + 1

is optimal, then it should constitute part of optimal decisions at time t) in dynamic pro-

gramming fails (Ruszczy´nski, 2010; Shapiro, 2009). One main thread of studying the variance

2

optimization problem lays in the framework of the MDP theory, which aims at minimizing

the variance after the mean performance already achieves optimum. For the policy subset

where the mean already achieves optimum, the variance minimization problem is equivalent

to another standard MDP with a new average cost criterion. Interested readers can refer to

related papers in the literature, such as the work by Cao and Zhang (2008) about the n-bias

optimality of MDPs, the works by Guo and Song (2009); Guo et al. (2012) about discounted

MDPs, and the work by Hernandez-Lerma et al. (1999) about average MDPs, just name a

few. For these equivalent MDPs, traditional approaches like dynamic programming or policy

iteration can be used to minimize the variance of MDPs.

Another thread of studying the variance-related optimization problem is the policy gra-

dient approach for risk-sensitive MDPs (Prashanth and Ghavamzadeh, 2013; Tamar et al.,

2013), which is more from the algorithmic viewpoint in reinforcement learning. In this sce-

nario, the policy is usually parameterized by sophisticated functions, e.g., neural networks.

In general, such optimization problem is not a standard MDP (a parameterized one) and the

policy iteration is not applicable. For such scenarios, stochastic gradient-based algorithms are

usually adopted to approach to a local optimal solution. Although neural networks have strong

policy representation capability, such gradient-based algorithms usually converge slowly and

suﬀer from some intrinsic deﬁciencies, such as trap of local optimum, diﬃculty of selecting step

sizes, and sensitivity to initial points. Seeking policy iteration type algorithms is a promising

attempt to study the MDPs with variance criteria. A recent study investigates the variance

minimization problem regardless of the optimality of mean performance from the perspec-

tive of the sensitivity-based optimization, where algorithms with a policy iteration type are

developed for the variance minimization problem of MDPs with averages and parameterized

policies, respectively (Xia, 2016, 2018b). It is worth noting that there exist other risk-sensitive

metrics besides the variance-related one. For example, exponential utility function is also a

risk-sensitive metric since the ﬁrst two orders of Taylor expansions of exponential utility in-

clude the mean and the variance (B¨auerle and Ja´skiewicz, 2015; Borkar, 2002; Guo and Zhang,

2019). Percentile-related criterion, VaR (Value at Risk), and CVaR (Conditional VaR) are

3

other widely studied risk-sensitive metrics in the ﬁeld of ﬁnance and economy (Chow et al.,

2015; Delage and Mannor, 2010; Fu et al., 2009; Gao et al., 2017; Hong et al., 2014). Moti-

vated by the modern theory of coherent risk measures (Artzner et al., 1999), time-consistent

Markov risk measures are also studied from another point of view in risk-aware MDPs litera-

ture (Ruszczy´nski, 2010; Ruszczy´nski and Shapiro, 2006). Haskell and Jain (2013) study the

stochastic dominance of accumulated rewards rather than the expectation of rewards, which

is another interesting way to study the risk-aware metrics in MDPs.

In the literature, some studies consider the performance optimization of mean and vari-

ance together, which is called the mean-variance optimization. The pioneering work of mean-

variance optimization is initiated by Markowitz (1952), the Nobel Laureate in Economics.

Intensive attention has been paid during past decades and the mathematical model is ex-

tended from single-stage static problem to multi-stage dynamic one. Markov models are usu-

ally adopted when we study the mean-variance optimization in a stochastic dynamic scenario.

The related works on the dynamic mean-variance optimization can be categorized into three

classes. The ﬁrst one is to maximize the mean performance at the condition that the variance

is less than a given amount. The second one is to minimize the variance while keeping the

mean performance larger than a given amount. The third one is to optimize the combination of

the mean and the variance together, such as maximizing the Sharpe ratio
combination E[RT ]

E[RT ]
σ[RT ] or the weighted
βσ2[RT ]. Chung (1994) and Sobel (1994) study the mean-variance opti-

−

mization in a regime of long-run average MDPs, by converting the constrained MDP problem

to a mathematical programming problem. Numerical algorithms are also studied for the multi-

stage mean-variance optimization problem from the viewpoint of mathematical programming

(Parpas and Rustem, 2007). For discounted MDPs, the variance minimization problem with

a given discounted average performance can be converted to an equivalent discounted MDP

with a new cost function (Huang, 2018; Huo et al., 2017; Xia, 2018a), and traditional MDP

methods such as policy iteration are applicable. In some general settings, it has been shown

that the mean-variance optimization problem in MDPs is NP-hard (Mannor and Tsitsiklis,

2013). Another way to study the mean-variance optimization is from the perspective of op-

4

timal control, in which the model is usually continuous time or even continuous state and

the optimality structures of control laws are investigated for speciﬁc scenarios (Zhou and Li,

2000; Zhou and Yin, 2004). In recent years, reinforcement learning attracts intensive atten-

tion for the great success of AlphaGo. Although most reinforcement learning algorithms focus

on discounted criterion, some policy gradient algorithms are also investigated to optimize the

combined metrics of mean and variance (Borkar, 2010; Tamar et al., 2012). However, such

gradient-based approach still suﬀers from the deﬁciency of local optimum and slow conver-

gence. On the other hand, as we know, there are three categories of approaches to solve MDP

problems: policy iteration, value iteration, and policy gradient, where the policy iteration is a

classical and important approach. Although the algorithmic complexity of the policy iteration

is still an open problem (Littman et al., 1995), it is observed that the policy iteration usually

converges very fast in most cases. Therefore, it is promising to develop policy iteration type

algorithms to solve the mean-variance optimization problem.

In this paper, we study the optimization of the mean-variance combined metric in the

framework of MDPs. The objective is to ﬁnd the optimal policy that maximizes the mean
minus variance metrics, E[RT ]

βσ2[RT ]. Diﬀerent coeﬃcients β tradeoﬀ the weights between

−

the mean and the variance. Since the variance-related metric is not additive, this dynamic

optimization problem does not ﬁt the standard model of MDPs. The time consistency of

dynamic programming is not valid and the Bellman optimality equation does not hold. We

study this problem from a new perspective called the theory of sensitivity-based optimization

(Cao, 2007), which is rooted from the theory of perturbation analysis (Ho and Cao, 1991) and

largely extended to stochastic dynamic systems with Markov models. The key idea of the

sensitivity-based optimization is to utilize the performance diﬀerence and derivative informa-

tion to search the optimal policy. This theory is applicable for general Markov systems and it

does not require an additive cost function. For the mean-variance combined metric, we derive

a diﬀerence formula which quantiﬁes the diﬀerence of the combined metrics under any two

policies. The coupling eﬀect caused by the non-additive variance function can be decoupled by

a square term in the diﬀerence formula. Based on the diﬀerence formula, we derive a method

5

to strictly improve the system mean-variance metrics. A necessary condition of the optimal

policy is also derived. We further prove that the optimal policy with the maximal mean-

variance combined metric can always be achieved in the deterministic policy space. A policy

iteration type algorithm is developed to ﬁnd the optimal policy, which is proved to converge

to a local optimum both in the mixed and randomized policy space. Moreover, this algorithm

can ﬁnd the global optimum if the mean reward remains constant for diﬀerent policies. Some

exploration techniques are also discussed to improve the global search capability of the algo-

rithm. Finally, we use an example in a renewable energy storage system to demonstrate that

our approach can eﬀectively reduce the ﬂuctuation of the total output power, which improves

the output power quality and reduces the stability risk of power grid. The local convergence

and the global convergence of our approach are both demonstrated in diﬀerent scenarios of

this example.

The main contributions of the paper are as follows. We use the sensitivity-based opti-

mization theory to study the performance optimization of mean-variance combined metrics in

Markov systems, which is a new perspective diﬀerent from the traditional dynamic program-

ming. Although the principle of dynamic programming is not applicable, we derive a diﬀerence

formula to quantify the changing behaviors of mean-variance combined metrics with respect

to diﬀerent policies. Optimality conditions and a policy iteration type algorithm for optimal

policies are also derived. To the best of our knowledge, our paper is the ﬁrst work that a

policy iteration type algorithm is developed to optimize the mean-variance combined metrics

in MDPs. Compared with traditional gradient-based algorithms, policy iteration usually has

a much faster convergence speed, which is partly demonstrated by our numerical experiments.

This paper is substantially extended based on its conference version (Xia and Yang, 2019),

where the main theorems, algorithms, and experiments are not presented. The diﬀerence of

this paper from our previous work (Xia, 2016) is that this paper optimizes both the mean and

variance metrics together, while Xia (2016) only minimizes the variance metric without any

consideration of the mean performance. It is important to consider the mean and variance

metrics together in real life. Many works about dynamic mean-variance optimization focus

6

on the variance of accumulated discounted rewards at a terminal stage, where the contraction

operator is a key tool. There are much less works studying the mean-variance optimization

of rewards in the sense of long-run averages, where the contraction operator is not applicable.

Therefore, our work on the long-run mean-variance combined metrics could give a comple-

mentary result to the whole theory of mean-variance optimization. We hope that this paper

could shed light on studying the dynamic mean-variance optimization, especially in the sce-

nario with long-run average metrics (instead of discounted ones) and policy iteration type

algorithms (instead of gradient ones).

The remainder of the paper is organized as follows. In Section 2, we formulate the per-

formance optimization problem of mean-variance combined metrics under the framework of

MDPs. The main challenge of this problem is also discussed.

In Section 3, we use the

sensitivity-based optimization theory to derive the diﬀerence formula for the mean-variance

combined metrics. Optimality structures and necessary condition of the optimal policy are

also derived. Furthermore, we develop an iterative algorithm to ﬁnd the optimal policy in

Section 4. The convergence analysis of the algorithm is also studied. In Section 5, we use

an example of renewable energy storage systems to numerically illustrate the local and global

convergence of our approach, respectively. Finally, we conclude this paper and discuss some

future research topics in Section 6.

2 Problem Formulation

We consider a discrete time Markov decision process. The state space and the action space

are both ﬁnite and denoted as

:=

1, 2,

{

· · ·

S

general,

A

can also be state-dependent, i.e.,

A

, S

and

}
(i) and i

A

∈ S

:=

a1, a2,

, aA}
, but we omit this case for simplicity

, respectively. In

· · ·

{

of representation. A stationary policy is a mapping from the state space to the action space,

d :

S → A

, where d is deterministic since its optimality is proved in the later section. The

stationary policy space is denoted as

. At each state i

D

∈ S

, an action a = d(i) will be

adopted if policy d

∈ D

is being employed. Then the system will transit to the next state j

7

with transition probability pa(i, j) and an instant reward r(i, a) will be incurred, i, j

denote P d as the transition probability matrix whose elements are pd(i)(i, j)’s, i, j

. We

∈ S

. We

∈ S

know that P d is an S-by-S matrix and P d1 = 1, where 1 is an S-dimensional column vector

with all elements being 1. We further denote rd as an S-dimensional column vector whose

elements are r(i, d(i))’s, i

∈ S

. Note that we may also remove the superscript “d” and use P

and r for notation simplicity. We make the following assumption about the MDP model in

this paper.

Assumption 1. We only focus on stationary policies in this paper and the MDP under any

policy d

∈ D

is always ergodic.

We denote πd(i) as the stationary probability that the system stays at state i when policy d

is employed. Therefore, the vector of steady state distribution is denoted as an S-dimensional

row vector πd := (πd(1), πd(2),

· · ·

tions

, πd(S)), which can be determined by the following equa-

πdP d = πd,

πd1 = 1.

We deﬁne the long-run average performance of the MDP

µ := πdrd = lim
J d
T →∞

Ed

1
T

T −1

(

t=0
X

r(Xt, At)

,

)

(1)

(2)

where Ed indicates the expectation under policy d, Xt is the system state at time t, At = d(Xt)

and it is the action at time t. With Assumption 1, the MDP under any policy d is always

ergodic, thus J d

µ is independent of the initial state X0.

We also deﬁne the steady state variance (or called long-run variance) of rewards of the

MDP (Chung, 1994; Sobel, 1994)

J d
σ := lim
T →∞

Ed

1
T

T −1

(

t=0
X

(cid:2)

r(Xt, At)

2

J d
µ

−

(cid:3)

.

)

(3)

Note that the steady state variance J d

accumulated rewards RT :=

σ deﬁned above is diﬀerent from the variance of the
t=0 αtr(Xt, At) with discount factor α. The latter deﬁnition is

T

P

8

also widely studied in the MDP literature and it reﬂects the variation of the terminal wealth

at time T . However, it does not reﬂect the variation during the reward process.

In this paper, we study the following mean-variance combined metric

µ,σ := J d
J d

µ −

βJ d
σ,

(4)

where β > 0 and it is a weight between J d

µ and J d

σ. With deﬁnitions (2) and (3), we can

rewrite (4) as

J d
µ,σ = lim
T →∞

Ed

1
T

T −1

(

t=0
X

r(Xt, At)

−

β[r(Xt, At)

J d
µ]2

−

)

= πdf d

µ,σ,

where f d

µ,σ is a column vector with S-dimension and its element f d

µ,σ(i) is deﬁned as

f d
µ,σ(i) := r(i, d(i))

β[r(i, d(i))

−

J d
µ]2,

−

i

.

∈ S

(5)

(6)

Thus, the optimization of the mean-variance combined metric is represented by the long-run

average performance of the MDP with the new cost function (6). The objective is to ﬁnd the
optimal policy d∗ which maximizes the associated value of J d∗

µ,σ, i.e.,

d∗ = argmax
d∈D {

J d
µ,σ}

.

(7)

The problem (7) can also be viewed as a multi-objective optimization problem, one objec-

tive is the mean, and the other one is the variance. The value of (J d∗
σ ) associated with
the optimal policy d∗ derived in (7) is a Pareto optimum of the multi-objective optimization
problem. Diﬀerent β will induce diﬀerent solution to (7). All the associated pairs (J d∗
µ , J d∗
σ )
compose the Pareto frontier (or eﬃcient frontier) of the mean-variance optimization problem.

µ , J d∗

The optimization problem (7) is of signiﬁcance in practical applications, as the variance-

related metric has rich physical meanings. In ﬁnancial systems, the variance metric can be

used to quantify the risk of assets, which is widely used in portfolio management and ﬁnancial

hedging (Kouvelis et al., 2018). In service systems, the variance of customers’ waiting time

can be used to quantify the fairness of service. The study of service fairness is an interesting

topic in queueing theory (Avi-Itzhak and Levy, 2004). In industrial engineering, the variance

9

can be used to quantify the quality of products, such as the variation minimization control

in chemical process industry (Harrison and Qin, 2009). In power grid, the variance can be

used to quantify the stability of output power, which is critical for the safety of the grid

(Li et al., 2014). We also use an example of power grid to demonstrate the applicability of

our optimization approach in Section 5.

However, it is challenging to solve (7). The optimization problem (7) does not ﬁt the

standard model of MDPs, because its cost function (6) includes a term of J d

µ. The value of J d
µ

is aﬀected by not only the action selection at the current stage, but also those at future stages.

Thus, the action selection in future stages will aﬀect the “instant” cost (6) at the current stage,

which indicates that (6) is not additive and violates the requirement of a standard MDP model.

The Bellman optimality equation does not hold and the principle of dynamic programming

fails for this problem (Puterman, 1994). In the next section, we will study this optimization

problem with mean-variance combined metrics from a new perspective, called the sensitivity-

based optimization theory.

3 Sensitivity-Based Optimization

The theory of sensitivity-based optimization is proposed by Cao (2007) and it provides a

new perspective to study the performance optimization of Markov systems. The origin of

this theory can be traced back to the theory of perturbation analysis (Ho and Cao, 1991)

and its basic idea is to exploit sensitivity information from system sample paths, thus to

guide the system optimization. The sensitivity information includes not only performance

gradients, but also performance diﬀerence. A key result of the sensitivity-based optimization

is the performance diﬀerence formula which quantiﬁes the diﬀerence of the system performance

under any two policies or parameter settings. This theory does not require a standard model

of MDPs and it is valid for Markov systems with general controls, even works in some scenarios

where the traditional dynamic programming fails (Cao, 2007; Xia et al., 2014). One of the

main contributions of our paper is to derive a mean-variance combined performance diﬀerence

10

formula for solving (7), which has an elegant form with a square term to handle the failure of

dynamic programming.

3.1 Performance Diﬀerence of Mean-Variance Combined Metrics

The theory of sensitivity-based optimization has a fundamental quantity called performance

potential gd

µ,σ(i), which is deﬁned as below (Cao, 2007)

T

gd
µ,σ(i) := lim
T →∞

Ed

[f d

µ,σ(Xt)

J d
µ,σ]

−

,

i

, d

.

∈ D

∈ S

(8)

(

t=0
X

X0 = i
(cid:12)
(cid:12)
(cid:12)

)

We further denote gd

µ,σ as an S-dimensional column vector whose elements are gd

∈ S
µ,σ(i) can be understood as a quantity which accumulates the advantages of the

µ,σ(i)’s, i

.

From (8), gd

system rewards over the average reward J d

µ,σ, caused by the condition of the initial state i. If the

value of gd

µ,σ(i) is positively large, it indicates that the initial state i has a good position with

a high potential, compared with the long-run average level J d

µ,σ. Therefore, gd

µ,σ is also called

bias or relative value function in the classical MDP theory (Puterman, 1994). To compute

the value of gd

µ,σ(i) in (8), we have to obtain the long-run average J d

µ,σ ﬁrst. Meanwhile, the

value of J d

µ,σ depends on the calculation of J d

µ and J d

σ according to (4). Substituting (4) and

(6) into (8), we can further derive

[r(Xt)

[r(Xt)

−

−

T

(

t=0
X
T

(
βgd

t=0
X
σ(i),

β(r(Xt)

J d
µ)2

−

−

J d
X0 = i
µ]
(cid:12)
(cid:12)
(cid:12)

) −

X0 = i

)

µ + βJ d
J d
σ]
(cid:12)
(cid:12)
T
(cid:12)

Ed

β lim
T →∞

(

t=0
X

[(r(Xt)

J d
µ)2

−

−

J d
X0 = i
σ]
(cid:12)
(cid:12)
(cid:12)

)
(9)

gd
µ,σ(i) = lim
T →∞

Ed

= lim
T →∞

Ed

= gd

µ(i)

−
µ(i) and gd

where gd

functions r(i) and (r(i)

−

σ(i) are the corresponding performance potentials of the MDP with cost
J d
µ)2, respectively, using similar deﬁnitions in (8) as follows

gd
µ(i) := lim
T →∞

Ed

T

T

(

t=0
X

[r(Xt)

−

gd
σ(i) := lim
T →∞

Ed

[(r(Xt)

−

(

t=0
X

J d
µ)2

J d
µ]
(cid:12)
(cid:12)
(cid:12)

−

X0 = i

)

,

i

, d

,

∈ D

∈ S

,

i

, d

.

∈ D

∈ S

J d
σ]

X0 = i
(cid:12)
(cid:12)
(cid:12)

)

11

Equation (9) indicates that the performance potentials conserve the linear additivity if the

cost function can be linearly separated into diﬀerent parts.

Using the strong Markov property and extending the summation terms of (8) at time t = 0,

we derive a recursive equation

µ,σ(i) = f d
gd

µ,σ(i)

= f d

µ,σ(i)

−

−

T

Ed

[f d

µ,σ(Xt)

(
t=1
X
pd(i)(i, j)gd

µ,σ(j),

J d
µ,σ + lim
T →∞

J d
µ,σ +

j∈S
X

J d
µ,σ]

−

X0 = i
(cid:12)
(cid:12)
(cid:12)

)

(10)

where the second equality is derived by recursively applying (8). We can further rewrite (10)

in a matrix form and derive the following Poisson equation:

µ,σ = f d
gd

µ,σ −

µ,σ1 + P dgd
J d

µ,σ.

(11)

It is known that P d is a stochastic matrix and its rank is S

1. Thus, we can set gd

µ,σ(1) = c

−

and (11) can be numerically solved with a unique solution, where c is any ﬁxed constant.

That is, gd

µ,σ + c1 is also a solution to (11) for any constant c. Moreover, we can also use the

deﬁnition (8) to online learn or estimate the value of gd

µ,σ based on system sample paths (see

details in Chapter 3 of (Cao, 2007) and reinforcement learning (Sutton and Barto, 2018)).

Next, we are ready to study the mean-variance combined performance diﬀerence between

µ,σ and J d′
J d
omit the superscript “d” by default and use the superscript “

µ,σ when the policy of MDPs is changed from d to d′. To simplify notations, we

′

” instead of “d′

” in the rest of

the paper if applicable. We write the cost function (6) in a vector form

fµ,σ = r

β(r

−

−

Jµ1)2
⊙,

where (r

−

Jµ1)2

⊙ is the component-wise square of vector (r

Jµ1), i.e.,

−

(r

−

Jµ1)2

⊙ := [(r(1)

Jµ)2, (r(2)

Jµ)2,

−

· · ·

, (r(S)

−

Jµ)2]T .

−

Thus, the mean-variance combined performance metric under policy d is

Jµ,σ = πfµ,σ = π[r

β(r

−

−

Jµ1)2

⊙].

12

(12)

(13)

Similarly, the mean-variance combined performance metric under policy d′ is

µ,σ = π′f ′
J ′

µ,σ = π′[r′

β(r′

−

−

J ′
µ1)2

⊙].

Substituting (12) into (11), we can rewrite the Poisson equation as

gµ,σ = r

β(r

Jµ1)2

⊙ −

−

−

Jµ,σ1 + P gµ,σ.

By left-multiplying π′ on both sides of (15), we have

π′P ′gµ,σ = π′r

βπ′(r

Jµ1)2

⊙ −

−

−

Jµ,σ + π′P gµ,σ,

where we use the fact π′P ′ = π′ and π′1 = 1. By letting (14) minus (16), we obtain

(14)

(15)

(16)

J ′
µ,σ −

Jµ,σ = π′

(P ′

h

P )gµ,σ + r′

β(r′

J ′
µ1)2

⊙ −

−

−

r + β(r

−

−

Jµ1)2
⊙

.

(17)

i

Furthermore, we extend π′(r′

J ′
µ1)2

⊙ and derive

−

π′(i)r′2(i)

2π′(i)r′(i)J ′

µ + π′(i)J ′2
µ

π′(i)r′(i) + J ′2
µ

π′(i)

i

i∈S
X

π′(r′

−

J ′
µ1)2

⊙ =

=

=

=

i∈S h
X

π′(i)r′2(i)

i∈S
X

i∈S
X

π′(i)r′2(i)

π′(i)r′2(i)

−

i∈S
X

2J ′
µ

J ′2
µ

2Jµ

−

−

−

i∈S
X
= π′(r′

Jµ1)2

⊙ −

−

(J ′

i∈S
X
µ −

Jµ)2,

π′(i)r′(i) + J 2
µ

π′(i)

(J ′

µ −

−

Jµ)2

i∈S
X

(18)

where the fact

i∈S π′(i)r′(i) = J ′

µ is utilized. By substituting (18) into (17), we directly derive

the following lemma about the mean-variance combined performance diﬀerence formula.

P

Lemma 1. If an MDP’s policy is changed from d to d′, the associated (P , r) is changed to

(P ′, r′), then the diﬀerence of the system mean-variance combined metrics is quantiﬁed by

J ′
µ,σ −

Jµ,σ = π′

(P ′

h

P )gµ,σ + r′

β(r′

Jµ1)2

⊙ −

−

−

r + β(r

Jµ1)2
⊙

−

−

+ β(J ′

µ −

Jµ)2. (19)

i

An important feature of (19) is that every element in the square brackets is given or

computable under the current policy d, although the explicit values of every possible π′ and

13

J ′
µ are unknown. The mean-variance combined performance diﬀerence formula (19) is a key

result of our paper to solve (7). It clearly quantiﬁes the relationship between the long-run

mean-variance combined performance and the adopted policies, where diﬀerent policies are

represented by diﬀerent (P , r)’s. It is worth noting that the square term β(J ′

µ −
for the optimization analysis in the rest of the paper because of the fact β(J ′

Jµ)2 is critical

µ −

Jµ)2

0.

≥

For the current system with policy (P , r), we can compute or learn the associated values of

Jµ and gµ,σ. Although the value of π′(i) is unknown, we always have the fact π′(i) > 0 for

ergodic MDPs with Assumption 1. If we choose a new policy d′ with proper (P ′, r′) such that

all the elements of the column vector represented by the square brackets of (19) are positive,

then we have π′

(P ′

P )gµ,σ + r′

β(r′

Jµ1)2

r + β(r

Jµ1)2
⊙

> 0, which indicates

−

−

−

⊙ −

−

Jµ,σ > 0 and the system performance under this new policy is improved. Repeating

J ′
µ,σ −
such operations, we can iteratively improve the policy, which also motivates the development

h

i

of an optimization algorithm in the next section.

Remark 1. To investigate the system behavior under a new policy (P ′, r′), we usually need to

obtain π′, J ′

µ, and J ′

by utilizing the fact that π′ and (J ′

µ,σ with high computation burdens. Fortunately, (19) avoids such burden
Jµ)2 are always nonnegative (although we do not

µ −

know their explicit values). This also reveals sensitivity information to guide the optimization

algorithm, which we call diﬀerence sensitivity (compared with derivative sensitivity).

With the performance diﬀerence formula (19), we can derive the following theorem about

generating improved policies.

Theorem 1. If a new policy d′ with (P ′, r′) satisﬁes

p′(i, j)gµ,σ(j)+r′(i)

β(r′(i)

Jµ)2

−

≥

−

j∈S
X

j∈S
X

p(i, j)gµ,σ(j)+r(i)

β(r(i)

Jµ)2,

−

−

i
∀

∈ S

, (20)

then we have J ′

µ,σ ≥

J ′
µ,σ > Jµ,σ.

Jµ,σ. If the inequality strictly holds for at least one state i, then we have

The proof of Theorem 1 is straightforward based on (19). We omit the detailed proof for

simplicity. Theorem 1 indicates an approach by which we can generate an improved policy

14

d′ based on the condition (20). That is, after we obtain the value of gµ,σ and Jµ under

the current policy d, we can ﬁnd a new policy with proper (P ′, r′) such that the value of

j∈S p′(i, j)gµ,σ(j) + r′(i)

β(r′(i)

−

−

Jµ)2 at each state i is as large as possible. The mean-

variance combined performance of the system under this new policy will be improved, as
P

guaranteed by Theorem 1.

With Theorem 1 and diﬀerence formula (19), we can further derive the following theorem

about a necessary condition of the optimal policy for (7).

Theorem 2. For the optimization problem (7), the optimal policy d with (P , r) must satisfy

p′(i, j)gµ,σ(j)+r′(i)

β(r′(i)

Jµ)2

−

≤

−

j∈S
X

j∈S
X

p(i, j)gµ,σ(j)+r(i)

β(r(i)

Jµ)2,

−

−

i
∀

∈ S

(21)

for any policy d′

.

∈ D

Proof. This theorem can be proved by using contradiction. Assume that the condition (21)

does not hold for the optimal policy d with (P , r). That is, for some state, say state k, there

exists an action a′ such that

p′(k, j)gµ,σ(j) + r′(k)

β(r′(k)

Jµ)2 >

−

−

j∈S
X

j∈S
X

p(k, j)gµ,σ(j) + r(k)

β(r(k)

−

−

Jµ)2. (22)

Therefore, we can construct a new policy d′ as follows: for state k, choose action a′; for other

states, choose exactly the same action as that of the optimal policy d. Therefore, substituting

(22) into (19), we obtain

J ′
µ,σ −

Jµ,σ = π′(k)

"
+β(J ′

j∈S
X
µ −
> 0 + β(J ′

Jµ)2

0.

≥

µ −

p(k, j))gµ,σ(j) + r′(k)

β(r′(k)

Jµ)2

−

−

−

r(k) + β(r(k)

Jµ)2

−

#

(p′(k, j)

−

Jµ)2

Therefore, we have J ′

µ,σ > Jµ,σ, which means that the policy d′ is better than the optimal

policy d. This contradicts the assumption that d is the optimal policy. Thus, the assumption

does not hold and the theorem is proved.

15

Note that the condition (21) is only a necessary condition for the optimal policy, not a

suﬃcient condition. The reason comes from the square term β(J ′

Jµ)2 in the diﬀerence

µ −

formula (19). It is possible to construct a new policy d′ such that the ﬁrst part of the right-

hand-side of (19) is negative, while the second part β(J ′

µ −

is better than d, while the condition (21) is unsatisﬁed.

Jµ)2 is relatively larger. Thus, d′

Remark 2. If the system mean rewards under any policy in

are the same, (21) becomes

D

the necessary and suﬃcient condition of the optimal policy. This is also consistent with the

existing results in the literature in which the optimization objective is to minimize the variance

after the mean reward is already optimal (Cao and Zhang, 2008; Guo and Song, 2009).

3.2 Performance Derivative of Mean-Variance Combined Metrics

In the theory of sensitivity-based optimization, performance derivative formula is another fun-

damental concept compared with the diﬀerence formula. Based on the performance derivative,

gradient-based methods can be utilized to solve the mean and variance optimization problems

in the literature since the traditional dynamic programming method cannot be directly applied

(Prashanth and Ghavamzadeh, 2013; Tamar et al., 2012). Below, we study the performance

derivative formula of this mean-variance combined metric with respect to randomized param-

eters.

First, we deﬁne a special case of randomized policy by using the concept of mixed policy

in MDPs and game theory (Feinberg and Schwartz, 2002). Consider any two deterministic
policies d and d′ with (P , r) and (P ′, r′), respectively. We deﬁne a mixed policy dδ,d′

which

adopts policy d with probability 1
−
= d and d1,d′
Obviously, we have d0,d′
matrix and the reward function under this mixed policy as P δ and rδ, respectively. We can

= d′. For simplicity, we denote the transition probability

δ and adopts policy d′ with probability δ, 0

≤

≤

1.

δ

verify that

The steady state distribution and the mean reward of the system under this mixed policy are

P δ = P + δ(P ′

rδ = r + δ(r′

−

P ),

−
r).

(23)

16

denoted as πδ and J δ

µ, respectively. We have

πδP δ = πδ,

µ = πδrδ = πδ[r + δ(r′
J δ

r)].

−

(24)

With this mixed policy dδ,d′

, the cost function of the mean-variance combined metric can be

written as

f δ(i) = (1

δ)[r(i)

β(r(i)

−

−

−

µ)2] + δ[r′(i)
J δ

β(r′(i)

J δ
µ)2],

−

−

i
∀

.

∈ S

The vector of the cost function is denoted as f δ. The long-run average performance of the
mean-variance combined metric under this mixed policy dδ,d′

can be written as

J δ
µ,σ =

πδ(i)f δ(i) = πδf δ.

i∈S
X

Similar to the derivation procedure of (17), left-multiplying πδ on both sides of (15), we

derive the performance diﬀerence formula between policies dδ,d′

and d as follows

J δ
µ,σ −

Jµ,σ = πδ

(P δ

n

P )gµ,σ +(1

−

δ)[r

β(r

−

−

−

J δ
µ1)2

⊙]+δ[r′

β(r′

−

−

J δ
µ1)2
⊙]

−

r+β(r

−

Jµ1)2
⊙
o(25)

Substituting (24) into (25), we derive

J δ
µ,σ −

Jµ,σ = πδ

(P δ

P )gµ,σ + δ[r′

−

−

−

n
+πδβ[(r

= πδ

(P δ

−

(r

Jµ1)2

⊙ −
−
P )gµ,σ + δ[r′

n

+πδβδ[2r′J δ

2rJ δ

µ −
µ −
P )gµ,σ + δ[r′

= πδ

(P δ

n

+2πδβ[r + δ(r′

r)]J δ

−
P )gµ,σ + δ[r′

µ −

−
2JµJ δ

µ + (J δ

µ)2]

P )gµ,σ + δ[r′

= πδ

(P δ

n
+β[J 2

µ −
(P δ

= πδ

−
Jµ)2.

n
+β(J δ

µ −

β(r′

−
J δ
µ1)2
⊙]

−

J δ
µ1)2
⊙]

δ[r

−

−

β(r

−

J δ
µ1)2
⊙]

o

β(r′

Jµ1)2
⊙]

δ[r

−

−
2r′Jµ + 2rJµ] + πδβ[(r

−

β(r

−
Jµ1)2

−

−

β(r′

Jµ1)2
⊙]

−

−
2πδβ[r + δ(r′

δ[r

β(r

−

−
r)]Jµ + β[J 2

Jµ1)2
⊙]

⊙ −

(r

o
J δ
µ1)2
⊙]
−
Jµ1)2
⊙]

−

−

−

β(r′

−

−

Jµ1)2
⊙]

δ[r

−

β(r

−

β(r′

−

−

Jµ1)2
⊙]

δ[r

−

−

β(r

−

17

(J δ

o
µ)2]

µ −
Jµ1)2
⊙]

o

Jµ1)2
⊙]

o

(26)

Substituting (23) into (26), we derive the mean-variance combined performance diﬀerence
formula between mixed policy dδ,d′

and deterministic policy d as below

J δ
µ,σ −

Jµ,σ = δπδ

(P ′

h

P )gµ,σ + r′

β(r′

Jµ1)2

⊙ −

−

−

r + β(r

Jµ1)2
⊙

−

−

+ β(J δ

µ −

Jµ)2. (27)

i

Comparing (27) with (19), we can see that the ﬁrst parts of the right-hand-side of these

two diﬀerence formulas have a linear factor δ. When δ = 1, we can see that (27) becomes (19).

With (27), we further derive the following lemma about the performance derivative formula

of the mean-variance combined metrics in the mixed policy space.

Lemma 2. For an MDP with the current deterministic policy d and associated (P , r), if

we choose any new deterministic policy d′ with (P ′, r′), the derivative of the mean-variance

combined performance Jµ,σ with respect to the mixed probability δ is quantiﬁed by

dJµ,σ
dδ

= π

(P ′
h

−

P )gµ,σ + r′

β(r′

Jµ1)2

⊙ −

−

−

r + β(r

−

Jµ1)2
⊙

.

i

(28)

Proof. With (27), taking the derivative operation with respect to δ on both sides and letting

0, we directly obtain

δ

→

dJµ,σ
dδ

= lim

πδ

(P ′

P )gµ,σ + r′

β(r′

Jµ1)2

⊙ −

−

−

r + β(r

Jµ1)2
⊙

−

+ 2β(J δ

µ −

δ→0 (

−

= π

(P ′

= π

h
(P ′

h

−

h
P )gµ,σ + r′

P )gµ,σ + r′

−

β(r′

β(r′

Jµ1)2

⊙ −

Jµ1)2

⊙ −

−

−

−

−

r + β(r

r + β(r

−

−

Jµ1)2
⊙

+ 2β

Jµ1)2
⊙

i
,

i

Jµ)

dJ δ
µ
dδ )

i
dJµ
dδ

lim
δ→0

J δ
µ −

Jµ

(cid:8)

(cid:9)

where we utilize the fact that lim
δ→0

πδ = π and lim
δ→0

(J δ

µ −

Jµ) = 0. The lemma is proved.

Therefore, performance derivative formula (28) can directly quantify the performance gra-

dient in the mixed policy space along with a policy perturbation direction between any two

deterministic policies d and d′.

Second, we focus on a more general form of randomized policies where the policy is pa-

rameterized by θ := (θi,a)’s, parameter θi,a indicates the probability of choosing action a

at state i

∈ S

. We deﬁne the randomized policy space as Θ :=

all θ

θi,a ≥

|

0,

{

∈ A
a∈A θi,a =

18

P

1,

i
∀

,

a

∈ S

∀

∈ A}

. Similarly, we consider the performance diﬀerence of the mean-variance

combined metrics under two diﬀerent randomized policies θ and θ′.

From the deﬁnition of θi,a’s, we can derive that the transition probabilities under θ and θ′

are as follows, respectively

P θ(i, j) :=
P θ′

(i, j) :=

a∈A pa(i, j)θi,a,
a∈A pa(i, j)θ′
i,a,

P

i, j

i, j

,

∈ S

.

∈ S

The mean-variance combined cost function under policy θ has the following deﬁnition

P

f θ
µ,σ(i) :=

θi,a

r(i, a)

a∈A
X
(cid:2)
where the mean performance J θ
µ is deﬁned as

−

β(r(i, a)

J θ
µ )2

,

−

i

,

∈ S

(cid:3)

(29)

(30)

J θ
µ :=

πθ(i)

θi,ar(i, a),

and πθ(i) is the steady state distribution at state i under policy θ. Similarly, we obtain the

i∈S
X

a∈A
X

cost function under policy θ′ as

f θ′
µ,σ(i) :=

θ′
i,a

r(i, a)

a∈A
X

h

β(r(i, a)

−

J θ′
µ )2

−

,

i

,

∈ S

i

(31)

and

J θ′
µ :=

πθ′

(i)

θ′
i,ar(i, a).

Similar to the derivation of the performance diﬀerence formula (17), we can also obtain the

i∈S
X

a∈A
X

following diﬀerence formula under two randomized policies θ and θ′

J θ′
µ,σ −

J θ
µ,σ =

i∈S
X

πθ′

(i)

f θ′
µ,σ(i)

(

f θ
µ,σ(i) +

−

j∈S
X

(cid:2)

P θ′

(i, j)

−

P θ(i, j)

gθ
µ,σ(j)

,

)

(32)

(cid:3)

where gθ

µ,σ(j) is the performance potential deﬁned in (10) with P θ(i, j)’s and f θ

µ,σ(i)’s under

policy θ. Substituting (29), (30), and (31) into the above equation, we can further derive the

following formula

J θ′
µ,σ −

J θ
µ,σ =

πθ′

(i)

(θ′

i,a −

i∈S
X
+β(J θ′

µ −

a∈A
X
J θ
µ )2.

θi,a)

(

j∈S
X

pa(i, j)gθ

µ,σ(j) + r(i, a)

−

βr2(i, a) + 2βJ θ

µ r(i, a)

)

(33)

19

Therefore, with (33), we directly derive the following lemma about the mean-variance com-

bined performance derivative with respect to the randomized policy.

Lemma 3. For a randomized MDP with parameterized policy θ, where θi,a indicates the proba-

bility of choosing action a at state i, the derivative of the mean-variance combined performance

with respect to the parameter θ is quantiﬁed by

dJ θ
µ,σ
dθi,a

= πθ(i)

(

j∈S
X

pa(i, j)gθ

µ,σ(j) + r(i, a)

−

βr2(i, a) + 2βJ θ

µ r(i, a)

,

i

, a

∈ S

∈ A

.

(34)

)

Proof. Taking the derivative operation with respect to θi,a on both sides of (33) and letting

θ′

→
dJ θ
µ,σ
dθi,a

θ, we have

= lim
θ′→θ

πθ′

(i)

(

j∈S
X
pa(i, j)gθ

= πθ(i)

(

j∈S
X

pa(i, j)gθ

µ,σ(j) + r(i, a)

βr2(i, a) + 2βJ θ

µ r(i, a)

−

dJ θ
µ
dθi,a

lim
θ′→θ

+ 2β

)

J θ′
µ −

J θ
µ

n

o

µ,σ(j) + r(i, a)

βr2(i, a) + 2βJ θ

µ r(i, a)

−

,

)

where we utilize the fact that lim
θ′→θ

πθ′

(i) = πθ(i) and lim
θ′→θ

J θ′
µ −

J θ
µ

= 0. The lemma is

proved.

(cid:8)

(cid:9)

Based on (34), we can further develop policy gradient algorithms to ﬁnd the optimal θ∗.

With Lemma 3, we derive the following theorem about the optimality of deterministic policies.

Theorem 3. For the MDP optimization problem of mean-variance combined metrics deﬁned

in (7), a deterministic policy can achieve the optimal value.

Proof. We choose any randomized policy deﬁned by parameters θi,a, i

, a

∈ S

∈ A

. The

associated transition probability and cost function are deﬁned by (29) and (30), respectively.

With (33) and the necessary condition in Theorem 2, we directly derive the following result.

For any i

∈ S

, if θ∗

i := (θ∗

i,a1, θ∗

i,a2,

, θ∗

i,aA) is optimal, it must satisfy the following conditions

· · ·
j∈S pa(i, j)gθ∗

µ,σ(j) + r(i, a)

−

βr2(i, a) + 2βJ θ∗

µ r(i, a)

,

o

(35)

θ∗
i = argmax

a∈A θi,a

s.t.,

θi
P
a∈A θi,a = 1,

nP

θi,a ≥
P

0,

a

∀

.

∈ A

20

In the above problem, the values in the braces are given and the θi,a’s are optimization vari-

ables. Obviously, (35) is a linear program. With the well known results of linear programming,

an optimal solution θ∗

i,a can be found on the vertexes of the multidimensional polyhedron com-

posed by the value domain of θi,a’s (Chong and Zak, 2013). We further look at the constraints

deﬁned in (35). It is easy to ﬁnd that the feasible domain of θi,a’s is [0, 1]. Therefore, the

vertexes are either 0 or 1, so are the optimal solution θ∗

i,a’s. This indicates that the optimal

policy can be deterministic and the theorem is proved.

Remark 3. For many risk-aware MDPs, a deterministic policy cannot achieve the optimal

value (Chung, 1994). This is partly because that those risk-aware MDPs have a constrained

form. It is well known that constrained MDPs may not achieve optimum at deterministic poli-

cies (Altman, 1999). However, our optimization problem deﬁned in (7) is not of a constrained

form, which partly supports the result in Theorem 3.

With the closed-form solution of gradients represented by (28) or (34), we can further de-

velop policy gradient-based algorithms to solve our optimization problem (7). Gradient-based

algorithms are widely adopted in the community of reinforcement learning (Prashanth and Ghavamzadeh,

2013; Tamar et al., 2012). It is worth noting that Lemma 3 is about a special case of param-

eterized policy θ, where θi,a indicates the probability of choosing action a

at state i

.

∈ S
= SA. In general, the space

∈ A

Thus, the dimension of such parameterized policy is

=

θ

|

|

|S||A|

of parameters θ can have a much lower dimension than the original policy space mapping

from

to

S

A

. For example, θ can be the weights of kernel functions or the parameters of a

neural network, such as the actor network (or policy network) widely used in deep reinforce-

ment learning. We can also develop similar gradients like (34). However, such gradient-based

method usually suﬀers from intrinsic deﬁciencies, such as being trapped into a local optimum,

diﬃculty of selecting learning step sizes. In the next section, we will further develop a policy

iteration type algorithm to solve (7), which usually has a fast convergence speed in practice.

21

4 Optimization Algorithm

In this section, we develop an iterative algorithm to solve the optimization problem (7) based

on the performance diﬀerence formula (19). This algorithm is of a policy iteration type. We

also prove that the algorithm can converge to a local optimum in both the mixed and the

randomized policy spaces.

The performance diﬀerence formula (19) and Theorem 1 directly indicate an approach to

generate improved policies. Therefore, we can develop an iterative procedure to optimize the

system performance of mean-variance combined metrics, which is stated in Algorithm 1.

Algorithm 1 An iterative algorithm to ﬁnd the optimal mean-variance combined metric.
1: arbitrarily choose an initial policy d(0)

and set l = 0;

∈ D

2: repeat

3:

4:

for the current policy d(l), compute or estimate the values of Jµ, Jµ,σ, and gµ,σ based

on their deﬁnitions (2), (4), and (8), respectively;

generate a new policy d(l+1) as follows:

d(l+1)(i) := argmax

r(i, a)

a∈A (

β[r(i, a)

−

−

Jµ]2 +

pa(i, j)gµ,σ(j)

j∈S
X

(36)

)

for all i

. While breaking ties to avoid policy oscillations, we ensure d(l+1)(i) = d(l)(i)

∈ S

if possible (if d(l)(i) can already achieve max in (36));

5:

set l := l + 1;

6: until d(l) = d(l−1)

return d(l).

As we discussed at the end of Section 2, because the cost function (6) is not additive, our

optimization problem (7) is not a standard MDP problem and the dynamic programming is

not applicable. Algorithm 1 treats the original problem (7) as if it is a standard MDP with

new cost function r(i, a)

β[r(i, a)

−

−

Jµ]2, where Jµ is a constant at the current iteration

and will be updated at next iterations. With Theorem 1, we can see that the new policy

generated by (36) is better than the current policy. Therefore, the policy will be improved

22

continually in Algorithm 1. We can also observe that Algorithm 1 is of a policy iteration type.

However, the global convergence of the traditional policy iteration cannot be directly extended

to Algorithm 1 since our problem (7) is not a standard MDP. Below, we ﬁrst give deﬁnitions

of local optimum in the mixed policy space and the randomized policy space, respectively.

Then, we study the convergence property of Algorithm 1.

Deﬁnition 1. For a policy d

, if there exists ∆

∈

∈ D

(0, 1), we always have J d

µ,σ ≥

′

J dδ,d
µ,σ

for

(0, ∆) , then we say d is a local optimal policy in the mixed policy space.

any d′

and δ

∈

∈ D

If we extend our policy space to the randomized policy space with parameters θ, we can

also have the following deﬁnition of local optimal policy.

Deﬁnition 2. For a randomized policy θ
J θ′
µ,σ for any θ′
−
θ is a local optimal policy in the randomized policy space.

∈
< ∆ where

Θ and

|| · ||

θ′

∈

θ

||

||

Θ, if there exists ∆ > 0, we always have J θ

µ,σ ≥
can be an Euclidean distance, then we say

Since

D

has a ﬁnite set of policies, we need the randomization to make the space continuous

such that a local optimum can be deﬁned. Deﬁnition 1 indicates that if d is a local optimum,

then it is not worse than any mixed policy in its small enough neighborhood, along with any

perturbation direction from d to d′
policy space, then we always have dJ d

∈ D

µ,σ

dδ ≤

. In other words, if d is a local optimum in the mixed

0 along with any perturbation direction. Deﬁnition 2

is more natural since it is deﬁned in the fully randomized policy space, which is continuous.

With Deﬁnitions 1 & 2 , we further derive the following theorem about the local optimal

convergence of Algorithm 1.

Theorem 4. Algorithm 1 converges to a local optimum, both in the mixed policy space and

the randomized policy space.

Proof. First, we prove the convergence of Algorithm 1. From the policy improvement step in

(36), we can see that the newly generated policy d(l+1) is not worse than d(l), based on the

result of Theorem 1. More speciﬁcally, if d(l+1)

= d(l), we can see that for at least one state

i, the max operator in (36) is achieved and its value in the braces of (36) is strictly larger

23

6
than the associated value with the current action d(l)(i). Therefore, with the last part of
Theorem 1, we have J d(l+1)

µ,σ and the newly generated policy is strictly improved. Since

µ,σ > J d(l)

the policy space

D

is ﬁnite, Algorithm 1 will stop after a ﬁnite number of iterations. Thus,

the convergence of Algorithm 1 is proved.

Second, we prove that the convergence point is a local optimum of the mixed policy space

in Deﬁnition 1. From Algorithm 1, we can see that when the algorithm stops we cannot ﬁnd a

new diﬀerent policy generated by (36). In other words, any other policy with (P ′, r′) cannot

make the term in the braces of (36) strictly bigger than that of the current policy with (P , r).

That is, when Algorithm 1 stops, we have

r(i)

−

β[r(i)

−

Jµ]2 +

j∈S
X

p(i, j)gµ,σ(j)

r′(i)

−

≥

β[r′(i)

−

Jµ]2 +

p′(i, j)gµ,σ(j),

i
∀

∈ S

, (37)

j∈S
X

for any policy with (P ′, r′). Substituting the above inequality into the derivative formula (28)

and using the fact that the elements of π are always positive, we have

dJµ,σ

dδ ≤

0,

along with any policy perturbation direction in the mixed policy space. Therefore, with

Deﬁnition 1 and the ﬁrst order of Taylor expansion of Jµ,σ with respect to δ, we can see that

the policy with (P , r) converged to is a local maximum in the mixed policy space.

Third, we further prove that the convergence point is also a local optimum of the random-

ized policy space in Deﬁnition 2. Suppose Algorithm 1 stops at policy d with (P , r), we have

(37) and rewrite it as

r(i)

−

βr2(i) + 2βJµr(i) +

p(i, j)gµ,σ(j)

j∈S
X

r′(i)

−

≥

βr′2(i) + 2βJµr′(i) +

p′(i, j)gµ,σ(j),

j∈S
X

for any (P ′, r′) and i

∈ S

maximal value of r(i, a)

−

i.e.,

. In other words, at each state i, the current policy d always has the

βr2(i, a) + 2βJµr(i, a) +

j∈S pa(i, j)gµ,σ(j) over all actions a

,

∈ A

P

d(i) = argmax

r(i, a)

a∈A (

−

βr2(i, a) + 2βJµr(i, a) +

pa(i, j)gµ,σ(j)

j∈S
X

24

,

i

.

∈ S

)

Substituting the above equation into the policy gradient (34) in Lemma 3, we can see that

the converged policy d (also denoted by θ in the randomized policy space) has

dJ θ
µ,σ
dθi,a ≥

dJ θ
µ,σ
dθi,a′

,

a = d(i),

a′
∀

.

∈ A

(38)

That is, at each state i, the converged policy d always has the maximal gradient at θi,a than

other gradients at θi,a′, a′

. Since the current policy d is deterministic and it has θi,a = 1

for a = d(i) and θi,a′ = 0 for other a′

∈ A

speciﬁcally, θi,a is perturbed from 1 to 1

satisfy

a′6=a ∆a′

= ∆. We have

−

∈ A

, we perturb in a small neighborhood of θ. More
∆, θi,a′’s are perturbed from 0 to ∆a′

, and it must

P

J θ
µ,σ −

J θ+∆
µ,σ =

dJ θ
µ,σ
dθi,a

∆

−

=

a′∈A,a′6=a "
X

dJ θ
µ,σ
dθi,a′

∆a′

+ o(∆)

dJ θ
µ,σ
dθi,a′ #

∆a′

+ o(∆).

a′∈A,a′6=a
X
dJ θ
µ,σ
dθi,a −

Substituting (38) into the above equation, we always have J θ

J θ+∆
µ,σ ≥
enough neighborhood of θ. This analysis procedure is valid for each state i

µ,σ −

0 for any small

. Therefore,

∈ S

the converged policy d is a local optimum in the randomized policy space. The theorem is

proved.

Because of the quadratic form of the variance related metrics, our optimization problem

(7) usually is a multi-modal function in the policy space, which is hard to ﬁnd the global

optimum. This is also one of the reasons that Algorithm 1 may only converge to a local

optimum. Similar to the condition discussed in Remark 2, we also have the following remark

to clarify the algorithm’s global convergence.

Remark 4. If the system mean reward is the same for all policy d

, i.e., J d

µ is independent

∈ D

of d, Algorithm 1 will converge to the global optimum.

One example satisfying the condition in Remark 4 can be found in the experiment of the

next section, where the global convergence is guaranteed. Although Algorithm 1 may only

converge to a local optimum, it has a form of policy iteration, which is similar to the classical

25

policy iteration in the traditional MDP theory. Therefore, it is expected that Algorithm 1 has

a similar convergence behavior as that of the classical policy iteration. However, a speciﬁc

analysis of the algorithmic complexity of Algorithm 1 is diﬃcult to derive. It is because the

algorithmic complexity of the classical policy iteration is still an open problem (Littman et al.,

1995). Nevertheless, it is often observed that the policy iteration converges very fast. With

the diﬀerence formula (17), we can see that each iteration in (36) will strictly improve the

system performance. However, the gradient-based approach has to carefully select step sizes,

otherwise it may jump to a worse policy. Such merits of the policy iteration are desirable

for policy gradient algorithms. For example, the proximal policy optimization (PPO) is an

eﬃcient reinforcement learning algorithm emerging in recent years (Schulman et al., 2015). It

outperforms the traditional policy gradient algorithms in large-scale reinforcement learning

problems, such as optimizing policy to drive a complex game called Dota 2 in the project

OpenAI Five. The PPO algorithm can also be viewed as an attempt to use approximated

policy iteration to guarantee a strict improvement of each policy update, compared with the

policy gradient method. Therefore, it is reasonable to argue that Algorithm 1 also has a fast

convergence speed for small or medium size risk-sensitive MDP problems.

For large-scale risk-sensitive MDP problems, we can utilize the widely used approximation

techniques to improve the performance of Algorithm 1, such as neuro-dynamic programming

(Bertsekas and Tsitsiklis, 1996), approximate dynamic programming (Powell, 2007; Yu et al.,

2017), deep neural networks (Silver et al., 2016), and other data-driven learning techniques.

We may call it risk-sensitive reinforcement learning (Borkar, 2010; Huang and Haskell, 2017;

Prashanth and Ghavamzadeh, 2013), which is still a new research direction of reinforcement

learning deserving further investigations.

Interesting topics may include the eﬃcient esti-

mation of key quantities gµ,σ, the robustness analysis of algorithms with respect to the in-

accurate values of P or gµ,σ (see robust MDPs by Chow et al. (2015); Lim et al. (2013);

Nilim and El Ghaoui (2005)), etc.

26

5 Numerical Experiments

In this section, we use an example about the ﬂuctuation reduction of wind power in energy

storage systems to demonstrate the applicability of our optimization method. The parameter

setting and engineering constraints have been largely simpliﬁed such that the key idea of this

example is concise and easy to follow.

5.1 Wind Abandonment Not Allowed

We consider a wind farm with a battery energy storage system connected to the main power

grid. Note that all the continuous variables in this problem are discretized properly. The wind

power is assumed as a stationary stochastic process. We use a Markov chain X :=

Xt}

{

to

model the dynamics of wind power, where Xt is the wind power at time epoch t (hourly in

this paper), t = 1, 2,

· · ·

. The transition probability matrix of Xt is denoted as P , which can

be estimated from statistics (Luh et al., 2014; Yang et al., 2018). The battery storage has a

capacity B and bt denotes the remaining battery energy level at time t. The system state is

deﬁned as (Xt, bt). The action is denoted as At which indicates the charging or discharging

power of the battery at time t. The battery has a maximal charging and discharging power and

the value domain of At is

:=

2,

1, 0, 1, 2

. Positive element of

indicates discharging

−

A

{−

}
power and negative one indicates charging power. When we select At, it should be constrained
by the remaining capacity of the battery, i.e., bt −
At ≤
energy level at the next time epoch is updated as bt+1 = bt −

bt. If At is adopted, the battery

At.

A

≤

B

The parameter setting of this problem is summarized in the following tables. The number

of wind power states is 6, and the correspondence between the state and the wind power is

shown in Table 1. The battery capacity is B = 5, and the correspondence between the battery

states and battery energy level is shown in Table 2. Table 3 shows the actions and their

corresponding operations of the battery energy storage system.

The wind state transition probability matrix P is calculated based on the real data provided

by the Measurement and Instrumentation Data Center (MIDC) in the National Renewable

27

Table 1: States of the wind power output

State

1

Wind power/MW 0

2

1

3

2

4

3

5

4

6

5

Table 2: States of the battery energy level

State

1

Battery energy level/MWh 0

2

1

3

2

4

3

5

4

6

5

Energy Laboratory (NREL, online), in which the wind speed is measured since 1996.

0.53 0.18 0.19 0.04 0.01 0.05

0.51 0.08 0.20 0.08 0.02 0.11

P =

0.35 0.11 0.19 0.11 0.03 0.21

0.27 0.15 0.15 0.14 0.03 0.26

0.14 0.11 0.13 0.15 0.05 0.42

0.09 0.03 0.06 0.06 0.03 0.73

































.

(39)

It is easy to verify that the ergodicity in Assumption 1 is satisﬁed in this experiment according

to the value of the transition probability matrix (39) and the feasible actions.

In this subsection, we assume that the wind abandonment is not allowed, i.e., all the wind

power generated should go to either the battery or the grid. The total output power of the

system is denoted as Yt and we have Yt = Xt + At. The control policy d of the battery is a

mapping from state (Xt, bt) to action At, i.e., the charging or discharging power of the battery

is determined by At = d(Xt, bt). The optimization objective includes two parts. One is the
average power generated E[Yt], which reﬂects the economic beneﬁt of the system. We can also

further include other economic metrics if needed, such as the operating cost of the battery

Table 3: Scheduling actions of the battery

Action

Battery (dis)charging power/MW

2
−
2
−

1
−
1
−

0

1

2

0 +1 +2

28

system. The other is the ﬂuctuation of the output power σ2[Yt], which reﬂects the power

quality or system safety. If σ2[Yt] is smaller, it indicates that the output power is more stable

and the power quality is better. The coeﬃcient β can be viewed as a shadow price of the

power quality or safety for the grid system, which has been attracting more attention by grid
company recently (Li et al., 2014). We aim at maximizing the average output power E[Yt]

while reducing the power ﬂuctuation σ2[Yt]. Such an objective is exactly a combined metric

of mean and variance. Thus, we can apply the approach proposed in this paper to study

this ﬂuctuation reduction problem of renewable energy with storage systems. An illustrative

diagram of this optimization problem is shown in Fig. 1.

wind
power

"$

max
&

{'  ! ( )*,[ !]}

s.t.

output
power

 $

C

dis/charging
+
power
#$
%+

-$ energy level

 ! = "! + #! ;

#! = 78"!/ -!9; -!45 = -! ( #!;

#! . {(2/ (1/0/1/2}; 3 "!45 "! ~6

Figure 1: The control of renewable energy with storage systems without wind abandonment.

We apply Algorithm 1 to ﬁnd the optimal control policy of the battery. Note that since
the wind abandonment is not allowed, the long-run average output power E[Yt] is not aﬀected
by actions. That is, the value of E[Yt] is independent of policies and it is determined by

the statistics of wind power. Therefore, this scheduling problem is a special case of our

optimization problem (7) for mean-variance combined metrics and it satisﬁes the condition

discussed in Remarks 2&4. The necessary condition in Theorem 2 is also a suﬃcient condition

for this case and Algorithm 1 converges to the global optimum, as indicated by Remark 4. For

demonstration, we choose β = 0.1 and the optimization results are illustrated by Fig. 2. From
Fig. 2, we can see that the variance σ2[Yt] is continually reduced while E[Yt] remains unvaried
as 2.3605. The mean-variance combined metric E[Yt]

βσ2[Yt] is continually improved until

−
converged after 4 iterations and its optimum is 2.0826.

Moreover, we randomly choose diﬀerent initial policies and ﬁnd that Algorithm 1 always

29

 = 0.1

E[Y]
2[Y]
E[Y]-

2[Y]

4.4

4.2

4

3.8

3.6

3.4

3.2

3

2.8

2.6

2.4

2.2

2

s
c
i
r
t
e
M

1.8

0

1

2

3
Number of iterations

4

5

6

Figure 2: The convergence procedure of Algorithm 1 without wind abandonment.

converges to the same optimal policy, which is truly the global optimum. This is also consis-

tent with the results in Remarks 2&4. The convergence results under some diﬀerent initial

policies are illustrated in Fig. 3, where diﬀerent colors and notations indicate the convergence

procedure from diﬀerent initial policies. With Fig. 3, we can see that the algorithm converges

to the same point with the minimal variance, i.e., σ2[Yt] = 2.7793.

To demonstrate the convergence eﬃciency of Algorithm 1, we further conduct an exper-

iment of traditional gradient-based algorithm as a comparison. Since the control variable is

the charging or discharging power, it is denoted as a

=

2,

{−

∈ A

1, 0, 1, 2

−

}

. We use the ran-

domized policy θ such that the parameters can be updated with gradient-descent algorithms.
With the performance derivative formula (34), we can compute the gradient dJµ,σ
dθi,a

for each

a

∈ A

and each system state i = (Xt, bt), where θi,a is the probability of selecting action a at

state i. We ﬁrst choose an arbitrary initial policy, say θi = (0, 0, 1, 0, 0), i.e., the initial action

∈ S

is always a = 0. At each state i

, after computing the value of gradient dJµ,σ
dθi,a
action am whose gradient is maximal. Then, we update the parameters as θl+1
i,am = θl
and θl+1

a relatively large amount, say αl = 1/√l. Then, we do normalization θl+1
such that the sum of θl+1

= am, where αl is the step size at the lth iteration and we set it as
a∈A θl+1
equals 1. We update the parameters θ repeatedly until the diﬀerence

i,a for all other a

i,a = θl+1
i,a /

, we ﬁnd the

i,am + αl

i,a = θl

i,a

P

i

30

6
=0.1

converge

]

Y

[

2

5.6

5.4

5.2

5

4.8

4.6

4.4

4.2

4

3.8

3.6

3.4

3.2

3

2.8

2.6

2.4

2

2.2

2.4

2.6

E[Y]

Figure 3: The same convergence point of Algorithm 1 under diﬀerent initial policies, where

Remarks 2&4 work.

of θ between two successive iterations is smaller than a given threshold, say 0.1% in ratio. The

algorithm stops and outputs the current θ∗ as the optimal randomized policy. The experiment

results are illustrated by Fig. 4.

The two sub-ﬁgures in Fig. 4 illustrate the convergence curves of performance metrics and

randomized parameters, respectively. Here we only choose θi as an example, where i = (0, 1)

means that the wind power is 0 and the battery energy level is 1. From Fig. 4 we can

see that the gradient-descent algorithm converges after 33 iterations, although we choose a

relatively large step size as αl = 1/√l. At the convergence point, the performance metrics
are E[Yt] = 2.3605, σ2[Yt] = 2.7802, and E[Yt]

βσ2[Yt] = 2.0825, which are close to and

−

slightly worse than our previous results of Algorithm 1. We can also see that the convergence

speed of this gradient-based algorithm is much slower than that of our policy iteration type

algorithm. Moreover, from the convergence curves of randomized parameters θi in Fig. 4, we

can see that θi converges to a deterministic action (0, 0, 0, 1, 0). It means that the optimal

action is discharging the battery with power a = 1 when the state is i = (0, 1), which looks

reasonable. This also veriﬁes the optimality of deterministic policies, as stated in Theorem 3.

31

β = 0.1

E[Y]
σ2[Y]
E[Y]-βσ2[Y]

0

5

10

15

20

25

30

Number of iterations

β = 0.1

θ

θ

θ

θ

θ

i,a1

i,a2

i,a3

i,a4

i,a5

30

0

0

5

10
Number of iterations at state i=(0,1)

15

20

25

4.6

4.2

3.8

3.4

3

2.6

2.2

1.8

1

0.8

0.6

0.4

0.2

s
c
i
r
t
e
M

e
a

t

t
s

t

a
n
o

l

i
t
c
e
e
s
n
o

i
t
c
a
f
o
y
t
i
l
i

b
a
b
o
r
p
e
h
T

Figure 4: The experiment results of the gradient-based algorithm.

5.2 Wind Abandonment Allowed

In this subsection, we study another scenario in which the wind abandonment is allowed. All

the parameter settings are the same as those in the previous subsection. The diﬀerence is

Vt, where 0

that we can abandon an extra power Vt if necessary. In this scenario, we deﬁne the action as
Ut := At −
≤
subsection, i.e., At ∈ A
can only use Ut as the decision variable after using the following reasonable assumptions. If

Xt. Variable At has the same constraint as that in the previous

bt. Although At and Vt are both variables, we

Vt ≤
and bt −

At ≤

≤

B

0

Ut ≤

≤

min

{

max(

), bt}

A

, then we have Vt = 0 and At = Ut, which means that the battery is

discharging and the wind power should not be abandoned. If max
{

min(

), bt −

A

B

} ≤

Ut < 0,

then we have Vt = 0 and At = Ut, which means that the battery has potential charging capacity

unused and the wind power should not be abandoned. If
), bt −
then we have Vt = At −
using full charging power capacity and some wind power is abandoned.

Xt ≤
−
}
B
, which means that the battery is
}

Ut < max
{

Ut and At = max

), bt −

min(

min(

A

A

B

{

,

In summary, the value domain of the decision variable Ut is

Xt ≤
−

Ut ≤

min

{

max(

), bt}

A

32

 
 
 
 
 
 
and it has the rule as follows:
Xt ≤
−

Vt = 0 and At = Ut; if

if max
{

min(

Ut < max

{

A
min(

), bt −
A

B
), bt −

} ≤
B

}

Ut ≤
), bt}
min
max(
A
{
Ut and At =
, then Vt = At −

, then

max

min(

), bt −

A

B

}

{

. Therefore, we only need to optimize the variable Ut. Variables At

and Vt can be determined by the aforementioned rule. The control policy d is a mapping

from (Xt, bt) to Ut, i.e., Ut = d(Xt, bt). The output power of the system is determined by

Yt = Xt + Ut. Our goal is to ﬁnd the optimal value of Ut at every state (Xt, bt) such that the
mean-variance combined metric E[Yt]

βσ2[Yt] can be maximized. The control procedure of

−

this problem is illustrated in Fig. 5.

wind
power

":

dis/charging
+
power
$:
<+
- 5:

8: energy level

output
power

 :

abandoned 
power

C

max
-

{.  ! & /34[ !]} s.t.

 ! = "! + #!;

#! = 67"!, 8!9;

5! = $! & #!;

8!() = 8! & $!;

$! % {&2, &1,0,1,2}; ' "!() "! ~*

Figure 5: The control of renewable energy with storage systems with wind abandonment.

We use Algorithm 1 to maximize the mean-variance combined metric of this problem. We

arbitrarily choose an initial policy from the policy space. For diﬀerent initial policies, Al-

gorithm 1 may converge to diﬀerent local optima. Fig. 6 shows the convergence procedure

when β = 0.5. It can be observed that the variance of power output σ2[Yt] is strictly reduced
during each iteration, while the average power output E[Yt] has an increasing trend. The com-
bined metric E[Yt]

βσ2[Yt] is continually improved after several iterations until Algorithm 1

−

converges. Algorithm 1 makes a balance between the wind abandonment and the ﬂuctuation
reduction, which demonstrates the eﬀectiveness of Algorithm 1 for maximizing E[Yt]

βσ2[Yt].

−

It is also observed that Algorithm 1 converges after 5 to 7 iterations in most cases, which shows

the fast convergence speed of Algorithm 1.

Moreover, we investigate the convergence results of Algorithm 1 under diﬀerent initial

policies and diﬀerent coeﬃcients β. In each subgraph of Fig. 7, we randomly choose 5 diﬀerent

initial policies to implement Algorithm 1. Diﬀerent curves indicate diﬀerent convergence

33

s
c
i
r
t
e
M

2.4

2.2

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0

1

2

 = 0.5

E[Y]
2[Y]
E[Y]-

2[Y]

3

4
Number of iterations

5

6

7

8

Figure 6: The convergence procedure of Algorithm 1 with wind abandonment.

trajectories under diﬀerent initial policies. For the case of β = 1, we observe that Algorithm 1

converges to two diﬀerent local optima; For other cases, diﬀerent initial policies converge to

the same local optimum. Every circle in Fig. 7 represents a feasible solution. We prefer the
solutions with large E[Yt] and small σ2[Yt]. Some of the feasible solutions are the Pareto
solutions, which dominate other solutions in either E[Yt] or σ2[Yt]. All the Pareto solutions

form the Pareto frontier, which can be partly reﬂected in Fig. 7.

Note that the solution space is too large for us to enumerate every initial policies in Fig. 7.

We only choose some representative solutions to outline the convergence procedures. The top-

left corner of each subgraph is the zooming-in area around the convergence points. In future

research, it is valuable to further apply the exploration mechanisms such that the algorithm

has capability to jump out from local optima.

One intuitive way of exploration is to randomly sample the initial policy such that Algo-

rithm 1 can start at diﬀerent initial points and converge to diﬀerent local optima. We can

record the optimal policy best so far until the computation budget is depleted. In order to

make the initial points as diverse as possible, we may deﬁne proper metrics to measure the

diversity of a policy set. One example of heuristic ways to deﬁne the diversity metric of a

34

β =0.5

1.86

1.88

1.9

1.92

1.4

1.2

1

7

6

5

4

3

2

]

Y
2

[

σ

2
E[Y]

β =5

1
1.6

1.8

local optimum

0.4

0.2

0

-0.2

7

6

5

4

3

2

1

]

Y
2

[

σ

0.9 0.95

1

1.05

β =1

0.6

0.4

0.2

1.35 1.4 1.45 1.5 1.55

5

4

3

2

1

]

Y
2

[

σ

2

2.5

local optimum

1.5
E[Y]

β =10

2.2

2.4

0
0.5

1

local optimum

0.6
0.4
0.2
0
-0.2
-0.4

0.8

0.9

1

1.1

7

6

5

4

3

2

1

]

Y
2

[

σ

0
0.5

1

local optimum

1.5
E[Y]

2

2.5

0
0.5

1

1.5
E[Y]

2

2.5

local optimum

Figure 7: The convergence results of diﬀerent initial policies and diﬀerent β.

policy set

D0 ⊆ D

is as follows.

Ψ(

D0) :=

d(s)

,

(40)

where

d∈D0
[
is the number of elements in the set and (40) means the sum of the numbers of
D0 is more diverse,
we expect that Algorithm 1 may converge to diﬀerent local optima and the best one is more

diﬀerent actions at all states. Thus, given the size of initial policy set

s∈S (cid:12)
X
(cid:12)
(cid:12)

D0, if

| · |

(cid:12)
(cid:12)
(cid:12)

likely the global optimum.

Another intuitive way is to introduce exploration schemes during the step of policy gener-

ation in Algorithm 1. That is, we modify (36) such that the generated new policy can have

more diversity. One feasible way is to use the ǫ-greedy scheme which is widely adopted in

reinforcement learning (Sutton and Barto, 2018). At each state s, we adopt the action deter-

mined by (36) with probability 1

−

ǫ and randomly select actions from

with probability ǫ,

A

35

where ǫ is a small positive number such as ǫ = 0.05. We can record and output the best so

far Jµ,σ until the computation budget is depleted. Another more eﬀective way is to use the

UCB (upper conﬁdent bound) method to balance the exploitation and exploration during the

search procedure (Agrawal, 1995; Auer, 2002). We can deﬁne a counter n(s, a) which records

the number of pair (s, a) evaluated during algorithm execution. We replace (36) with the

following scheme

d(l+1)(i) := argmax

r(i, a)

a∈A (

β[r(i, a)

−

−

Jµ]2 +

pa(i, j)gµ,σ(j) + γ

j∈S
X

2 ln(

a n(s, a))

s

n(s, a)
P

,
)

where the square root part reﬂects the exploration beneﬁt and γ is a coeﬃcient balancing the

exploitation and exploration. Such scheme is eﬀective especially considering the estimation

errors of gµ,σ(j)’s. We can let the coeﬃcient γ vanishing when the estimation of gµ,σ(j) becomes

more accurate. Similar schemes are widely used in sample-based optimization algorithms, such

as Monte-Carlo tree search in AlphaGo (Silver et al., 2016) and adaptive sampling in MDPs

(Chang et al., 2007). We can also resort to other exploration techniques such as local search

and global search techniques widely used in evolutionary algorithms.

6 Discussion and Conclusion

The mean-variance combined metrics reﬂect both the average performance and the risk-related

performance. Since the variance function is not additive, this mean-variance combined opti-

mization problem does not ﬁt the standard model of MDPs. The classical method of dynamic

programming is not applicable. We study this problem from a new perspective called the

theory of sensitivity-based optimization. The performance diﬀerence formula is established

to directly quantify the diﬀerence of the mean-variance combined metrics under any two

policies. The necessary condition of the optimal policy is derived. The optimality of deter-

ministic policies is proved. We also develop a policy iteration type algorithm to optimize the

mean-variance combined metric. The convergence of the algorithm is studied. Similar to the

traditional policy gradient approach, our approach also converges to a local optimum in the

36

mixed and randomized policy space, but with a more eﬃcient way as it has a form of policy

iteration. The global convergence of the algorithm is also discussed with the special condition

in Remarks 2&4 or adopting some exploration and sampling techniques. Experiment exam-

ples of the ﬂuctuation reduction of wind power with battery energy storage are conducted to

demonstrate the eﬀectiveness of our approach.

One of the future research topics is to implement our approach in a data-driven mode. This

is a promising direction to study the risk-sensitive reinforcement learning algorithm, since the

variance metric can reﬂect the risk-related factors. The online algorithm implementation and

the integration with neural networks deserve further investigations, which is important to han-

dle the issues of model-absence and the curse of dimensionality. Another future research topic

is to extend our approach to optimize higher moment metrics or even distribution optimiza-

tion, since the variance is only a second moment metric. For example, the third and the fourth

moment of rewards are also interesting metrics in statistics, which reﬂect the skewness and

kurtosis of reward distributions. One recent work makes a good initiate on the mean-variance-

skewness-kurtosis analysis for the classic newsvendor problem in a static optimization regime

(Zhang et al., 2020), while a more general study in a wider ﬁeld and dynamic optimization

regime will be a signiﬁcant and challenging research topic.

References

Altman, E. Constrained Markov Decision Processes. Chapman and Hall, 1999.

Agrawal, R. (1995). Sample mean based index policies with O(log n) regret for the multi-armed

bandit problem. Advances in Applied Probability 27, 1054-1078.

Artzner, P., Delbaen, F., Eber, J. M., Heath, D. (1999). Coherent measures of risk. Math.

Finance 9, 203-228.

Auer, P. (2002). Using conﬁdence bounds for exploitation-exploration trade-oﬀs. Journal of

Machine Learning Research 3, 397-422.

37

Avi-Itzhak, B. and Levy, H. (2004). On measuring fairness in queues. Advances in Applied

Probability 36(3), 919-936.

B¨auerle, N. and Ja´skiewicz, A. (2015). Risk-sensitive dividend problems. European Journal of

Operations Research 242, 161-171.

Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control–Vol. I. Athena Scientiﬁc.

Bertsekas, D. P. and Tsitsiklis J. N. (1996). Neuro-Dynamic Programming. Athena Scientiﬁc,

Belmont, Massachusetts.

Borkar, V. (2002). Q-learning for risk-sensitive control. Mathematics of Operations Research

27, 294-311.

Borkar, V. (2010). Learning algorithms for risk-sensitive control. Proceedings of the 19th In-

ternational Symposium on Mathematical Theory of Networks and Systems (MTNS’2010),

July 5-9, 2010, Budapest, Hungary, pp. 1327-1332.

Cao, X. R. (2007). Stochastic Learning and Optimization – A Sensitivity-Based Approach. New

York: Springer.

Cao, X. R. and Zhang, J. (2008). The nth-order bias optimality for multi-chain Markov decision

processes. IEEE Transactions on Automatic Control 53, 496-508.

Chang, H. S., Fu, M. C., Hu, J., and Marcus, S. I. (2007). Simulation-Based Algorithms for

Markov Decision Processes. Springer.

Chiu, C. H., Choi, T. M., Dai, X., Shen, B., and Zheng, J. H. (2018). Optimal advertising

budget allocation in luxury fashion markets with social inﬂuences: A mean-variance analysis.

Production and Operations Management 27, 1611-1629.

Chong, E. K. P. and Zak, S. H. (2013). An Introduction to Optimization, 4th Edition. Wiley.

Chow, L. M., Tamar, A., Mannor, S., and Pavone, M. (2015) Risk-sensitive and robust

decision-making: a CVaR optimization approach. Proceedings of the 28th International

38

Conference on Neural Information Processing Systems (ICML’2015), Montreal, Canada,

December 07-12, 2015, pp. 1522-1530.

Chung, K. J. (1994). Mean-variance tradeoﬀs in an undiscounted MDP: the unichain case.

Operations Research 42, 184-188.

Delage, E. and Mannor, S. (2010). Percentile optimization for Markov decision processes with

parameter uncertainty. Operations Research 58, 203-213.

Feinberg, E. and Schwartz, A. (2002). Handbook of Markov Decision Processes: Methods and

Applications. Boston, MA: Kluwer Academic Publishers.

Fu, M. C., Hong, L. J., and Hu, J. Q. (2009). Conditional Monte Carlo estimation of quantile

sensitivities. Management Science 55 (12), 2019-2027.

Gao, J., Zhou, K., Li, D., and Cao, X. R. (2017). Dynamic mean-LPM and mean-CVaR

portfolio optimization in continuous time. SIAM Journal on Control and Optimization 55(3),

1377-1397.

Guo, X. and Zhang, J. (2019). Risk-sensitive continuous-time Markov decision processes with

unbounded rates and Borel spaces. Discrete Event Dynamic Systems: Theory and Applica-

tions 29(4), 445-471.

Guo, X. and Song, X. Y. (2009). Mean-variance criteria for ﬁnite continuous-time Markov

decision processes. IEEE Transactions on Automatic Control 54, 2151-2157.

Guo, X., Ye, L., and Yin, G. (2012). A mean-variance optimization problem for discounted

Markov decision processes. European Journal of Operational Research 220, 423-429.

Haskell, W. B. and Jain, R. (2013). Stochastic dominance-constrained Markov decision pro-

cesses. SIAM Journal on Control and Optimization 51(1), 273-303.

Harrison, C. A. and Qin, S. J. (2009). Minimum variance performance map for constrained

model predictive control. Journal of Process Control 19, 1199-1204.

39

Hernandez-Lerma, O., Vega-Amaya, O., and Carrasco, G. (1999). Sample-path optimality and

variance-minimization of average cost Markov control processes. SIAM Journal on Control

and Optimization 38, 79-93.

Ho, Y. C. and Cao, X. R. (1991). Perturbation Analysis of Discrete Event Dynamic Systems,

Springer.

Hong, L. J., Hu, Z., and Liu, G. (2014). Monte Carlo methods for value-at-risk and conditional

value-at-risk: A review. ACM Transactions on Modeling and Computer Simulation 24, 1-37.

Huang, W. and Haskell, W. B. (2017). Risk-aware Q-Learning for Markov decision processes.

Proceedings of the 56th IEEE Conference on Decision and Control (CDC’2017), December

12-15, 2017, Melbourne, Australia, pp. 4928-4933.

Huang, Y. (2018). Finite horizon continuous-time Markov decision processes with mean and

variance criteria. Discrete Event Dynamic Systems: Theory and Applications 28(4), 539-564.

Huo, H., Zou, X., and Guo, X. (2017). The risk probability criterion for discounted continuous-

time Markov decision processes. Discrete Event Dynamic Systems: Theory and Applications

27(4), 675-699.

Kouvelis, P., Pang, Z., and Ding, Q. (2018). Integrated commodity inventory management

and ﬁnancial hedging: A dynamic mean-variance analysis. Production and Operations Man-

agement 27(6), 1052-1073.

Li, Y. Z., Wu, Q. H., Li, M. S., Zhan, J. P. (2014). Mean-variance model for power system

economic dispatch with wind power integrated. Energy 72, 510-520.

Lim, S., Xu, H., and Mannor, S. (2013). Reinforcement learning in robust Markov decision

processes. Proceedings of the NIPS’2013, Lake Tahoe, CA, The USA.

Littman, M. L., Dean, T. L., and Kaelbling, L. P. (1995). On the complexity of solving

Markov decision problems. Proceedings of the 11th Conference on Uncertainty in Artiﬁcial

Intelligence, Morgan Kaufmann Publishers Inc.

40

Luh, P. B., Yu, Y., Zhang, B., Litvinov, E., Zheng, T., Zhao, F., Zhao, J., and Wang, C.

(2014). Grid integration of intermittent wind generation: A Markovian approach. IEEE

Transactions on Smart Grid 5, 732-741.

Markowitz, H. (1952). Portfolio selection. The Journal of Finance 7, 77-91.

Mannor, S. and Tsitsiklis, J. N. (2013). Algorithmic aspects of mean-variance optimization in

Markov decision processes. European Journal of Operational Research 231, 645-653.

Nilim, A. and El Ghaoui, L. (2005). Robust control of Markov decision processes with uncertain

transition matrices. Operations Research 53, 780-798.

NREL.

National

wind

technology

center.

[online].

Available

on

website

http://www.nrel.gov/midc/nwtcm2.

Parpas, P. and Rustem, B. (2007). Computational assessment of nested benders and aug-

mented lagrangian decomposition for mean-variance multistage stochastic problems. IN-

FORMS Journal on Computing 19, 149-312.

Powell, W. B. (2007). Approximate Dynamic Programming: Solving the Curses of Dimension-

ality. John Wiley & Sons.

Prashanth, L. A. and Ghavamzadeh, M. (2013). Actor-critic algorithms for risk-sensitive

MDPs. Proceedings of the 26th International Conference on Neural Information Process-

ing Systems (NIPS’2013), 252-260.

Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Program-

ming. New York: John Wiley & Sons.

Ruszczy´nski, A. (2010). Risk-averse dynamic programming for Markov decision processes.

Mathematical Programming 125, 235-261.

Ruszczy´nski, A. and Shapiro, A. (2006). Conditional risk mappings. Mathematics of Operations

Research 31, 544-561.

41

Schulman, J., Levine, S., Moritz, P., Jordan, M., and Abbeel, P. (2015). “Trust region pol-

icy optimization,” Proceedings of the 31st International Conference on Machine Learning

(ICML’2015), Lille, France, pp. 1889-1897.

Shapiro, A. (2009). On a time consistency concept in risk averse multistage stochastic pro-

gramming. Oper. Res. Lett. 37, 143-147.

Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., and Van, D. G. (2016). Mastering

the game of go with deep neural networks and tree search. Nature 529(7587), 484-489.

Sobel, M. J. (1994). Mean-variance tradeoﬀs in an undiscounted MDP. Operations Research

42, 175-183.

Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction, 2nd Edition.

MIT Press, Cambridge, MA.

Tamar, A., Castro, D. D., and Mannor, S. (2012). Policy gradients with variance re-

lated risk criteria. Proceedings of the 29th International Conference on Machine Learning

(ICML’2012), Edinburgh, Scotland.

Tamar, A., Castro, D. D., and Mannor, S. (2013). Temporal diﬀerence methods for the variance

of the reward to go. Proceedings of the 13rd International Conference on Machine Learning

(ICML’2013), pp. 495-503.

Xia, L. (2016). Optimization of Markov decision processes under the variance criterion. Auto-

matica 73, 269-278.

Xia, L. (2018a). Mean-variance optimization of discrete time discounted Markov decision pro-

cesses. Automatica 88, 76-82.

Xia, L. (2018b). Variance minimization of parameterized Markov decision processes. Discrete

Event Dynamic Systems: Theory and Applications 28, 63-81.

42

Xia, L., Jia, Q. S., and Cao, X. R. (2014). A tutorial on event-based optimization – A new

optimization framework. Discrete Event Dynamic Systems: Theory and Applications 24(2),

103-132.

Xia, L. and Yang, Z. (2019). A new method for mean-variance optimization of stochastic

dynamic systems. Proceedings of the 2019 IEEE Conference on Control Technology and

Applications (CCTA’2019), August 19-21, 2019, Hong Kong, pp. 856-859.

Yang, Z., Xia, L., and Guan, X. (2018). Fluctuation reduction of wind power with battery

energy storage systems. Proceedings of the 2018 IEEE International Conference on Au-

tomation Science and Engineering (CASE’2018), August 20-23, 2018, Munich, Germany,

pp. 762-767.

Yu, P., Haskell, W. B., and Xu, H. (2017). Approximate value iteration for risk-aware Markov

decision processes. IEEE Transactions on Automatic Control 63(9), 3135-3142.

Zhang, J., Sethi, S. P., Choi, T. M., and Cheng, T. C. E. (2020). Supply chains involving

a mean-variance-skewness-kurtosis newsvendor: Analysis and coordination. Production and

Operations Management 29(6), 1397-1430.

Zhou, X. Y. and Li, D. (2000). Continuous-time mean-variance portfolio selection: a stochastic

LQ framework. Applied Mathematics & Optimization 42, 19-33.

Zhou, X. Y. and Yin, G. (2004). Markowitz’s mean-variance portfolio selection with regime

switching: A continuous-time model. SIAM Journal on Control and Optimization 42, 1466-

1482.

43

