1
2
0
2

v
o
N
6
1

]

C
O
.
h
t
a
m

[

1
v
4
7
6
8
0
.
1
1
1
2
:
v
i
X
r
a

Multiclass Optimal Classiﬁcation
Trees with SVM-splits

V´ıctor Blanco†, Alberto Jap´on‡ and Justo Puerto‡

†Institute of Mathematics (IMAG), Universidad de Granada
‡Institute of Mathematics (IMUS), Universidad de Sevilla
vblanco@ugr.es, ajapon1@us.es, puerto@us.es

Abstract. In this paper we present a novel mathematical optimization-
based methodology to construct tree-shaped classiﬁcation rules for multiclass
instances. Our approach consists of building Classiﬁcation Trees in which,
except for the leaf nodes, the labels are temporarily left out and grouped
into two classes by means of a SVM separating hyperplane. We provide a
Mixed Integer Non Linear Programming formulation for the problem and
report the results of an extended battery of computational experiments to
assess the performance of our proposal with respect to other benchmarking
classiﬁcation methods.

1. Introduction

Interpretability is a crucial requisite demanded to machine learning meth-
ods provoked by the tremendous amount of methodologies that have arised in
the last decade [21].
It is expected that the model that results when apply-
ing a machine learning methodology using a training sample, apart from being
able to adequately predict the behaviour of out-of-sample observations, can be
interpreted. Diﬀerent tools have been applied to derive interpretable machine
learning methods. One of the most popular strategies to simplify the obtained
models is feature selection, in which a reduced set of attributes is to be selected
without loosing quality in the predictions. Reducing the number of parameters
to analyze, the models can be easier to understand, yielding higher descriptive
accuracy. One could also consider models that can be modulated, in the sense
that a great proportion of its prediction-making process can be interpreted in-
dependently. This is the case of generalized linear models [29]. Other methods
incorporate interpretability as a synonym of being able to be reproduced by
humans in its entire construction [15, 34]. This is the case of Decision Trees
with small depth which can be visualized and interpreted easily by users even

Date: November 17, 2021.

1

 
 
 
 
 
 
2

V. BLANCO, A. JAP ´ON and J. PUERTO

not familiar with the tools behind their construction. We adopt a tree-based
methodology through this paper.

Among the wide variety of strands derived under the lens of Machine Learning,
classiﬁcation is one that has attracted a lot of attention because of its applica-
bility in many diﬀerent ﬁelds [4, 28, 31, 36, 41]. Classiﬁcation methodologies
aim to adequately predict the class of new observations provided that a given
sample has been used to construct the classiﬁcation rule. The role of Mathemat-
ical Programming in the construction of classiﬁcation models has been widely
recognized, and some of the most popular methods to derive classiﬁcation rules
are based on solving optimization problems [13, 17, 16, 11, 27]. Moreover, Math-
ematical Programming has also been proven to be a ﬂexible and accurate tool
when requiring interpretability to the obtained models [5, 6, 12, 25].

However, most of the optimization tools derived to construct classiﬁers as-
sume instances with only two classes. In this paper, we provide a novel classi-
ﬁcation method in which the instances are allowed to be classiﬁed into two or
more classes. The method is constructed using one of the most interpretable
classiﬁcation method, Classiﬁcation Trees, but combined with Support Vector
Machines, which provide highly predictive models.

We have developed a Mathematical Programming model that allows to con-
struct an Optimal Classiﬁcation Tree for a given training sample, in which each
split is generated by means of a SVM-based hyperplane. When building the tree,
the labels of the observations are ignored in the branch nodes, and they are only
accounted for in the leaf nodes where misclassiﬁcation errors are considered. The
classiﬁcation tree is constructed to minimize the complexity of the tree (assuring
interpretability) and also the misclassiﬁcation risk (assuring predictive power).

1.1. Related Works. Several machine learning methodologies have been pro-
posed in the literature in order to construct highly predictive classiﬁcation rules.
The most popular ones are based on Deep Learning mechanisms [1], k-Nearest
Neighborhoods [18, 42], Na¨ıve Bayes [35], Classiﬁcation Trees (CT) [15, 23] and
Support Vector Machines (SVM) [17]. Among them, CT and SVM, which are,
by nature, optimization-based methodologies, apart from producing highly pre-
dictive classiﬁers have been proven to be very ﬂexible tools since both allow the
incorporation of diﬀerent elements (through the adequate optimization models
by means of constraints and objective functions) to be adapted to diﬀerent situa-
tions, as Feature Selection [27, 5, 6, 30], accuracy requirements [7, 24] or dealing
with unbalanced or noisy instances [22, 10, 14], amongst others.

Support Vector Machines were originally introduced by Cortes and Vapnik [17]
as a binary classiﬁcation tool that builds the decision rule by means of a separat-
ing hyperplane with large separation between the two classes. This hyperplane is

3

obtained by solving a convex quadratic optimization problem, in which the goal
is to separate data by their two diﬀerentiated classes, maximizing the margin
between them and minimizing the misclassiﬁcation errors. Duality properties of
this optimization problem allow one to extend the methodology to ﬁnd nonlin-
ear separators by means of kernels. Classiﬁcation Trees were ﬁrstly introduced
by Breiman et. al [15], and the decision rule is based on a hierarchical relation
among a set of nodes which is used to deﬁne paths that lead observations from
the root node (highest node in the hierarchical relation), to some of the leaves
in which a class is assigned to the data. These paths are obtained according to
diﬀerent optimization criteria over the predictor variables of the training sample.
The decision rule comes up naturally, the classes predicted for new observations
are the ones assigned to the terminal nodes in which observations fall in. Clearly,
the classiﬁcation rules derived from CTs are easily interpretable by means of the
splits that are constructed at the tree nodes. In [15], a greedy heuristic proce-
dure, the so-called CART approach, is presented to construct CTs. Each level
of the tree is sequentially constructed: starting at the root node and using the
whole training sample, the method minimizes an impurity measure function ob-
taining as a result a split that divides the sample into two disjoint sets which
determine the two descendant nodes. This process is repeated until a given ter-
mination criteria is reached (minimum number of observations belonging to a
leaf, maximum depth of the tree, or minimum percentage of observations of the
same class on a leaf, amongst others). In this approach, the tree grows following
a top-down greedy approach, an idea that is also shared in other popular decision
tree methods like C4.5 [40] or ID3 [39]. The advantage of these methods is that
the decision rule can be obtained rather quickly even for large training samples,
since the whole process relies on solving manageable problems at each node.
Nevertheless, these types of heuristic approaches may not obtain the optimal
classiﬁcation tree, since they look for the best split locally at each node, not tak-
ing into account the splits that will come afterwards. Thus, these local branches
may not capture the proper structure of the data, leading to misclassiﬁcation
errors in out-of-sample observations. Furthermore, the solutions provided by
these methods can result into very deep (complex) trees, resulting in overﬁtting
and, at times, loosing interpretability of the classiﬁcation rule. This diﬃculty
is usually overcome by pruning the tree as it is being constructed by comparing
the gain on the impurity measure reduction with respect to the complexity cost
of the tree.

The recent advances on modeling and solving diﬃcult Optimization problems
together with the ﬂexibility and adaptability of these models have motivated the
use of optimization tools to construct supervised classiﬁcation methods with a
great success [9, 16]). In particular, recently, Bertsimas and Dunn [8] introduced

4

V. BLANCO, A. JAP ´ON and J. PUERTO

the notion of Optimal Classiﬁcation Trees (OCT) by approaching Classiﬁcation
and Regression Trees under optimization lens, providing a Mixed Integer Linear
Programming formulation for its optimal construction. Moreover, the authors
proved that this model can be solved for reasonable size datasets, and equally
important, that for many diﬀerent real datasets, signiﬁcant improvements in
accuracy with respect to CART can be obtained. In contrast to the standard
CART approach, OCT builds the tree by solving a single optimization problem
taking into account (in the objective function) the complexity of the tree, avoid-
ing post pruning processes. Moreover, every split is directly applied in order to
minimize the misclassiﬁcation errors on the terminal nodes, and hence, OCTs
are more likely to capture the hidden patterns of the data.

−

∈ {

1, . . . , k

While SVM were initially designed to deal only with bi-class instances, some
extensions have been proposed in the literature for multiclass classiﬁcation. The
most popular multiclass SVM-based approaches are One-Versus-All (OVA) and
One-Versus-One (OVO). The former, namely OVA, computes, for each class
, a binary SVM classiﬁer labeling the observations as 1, if the
r
}
observation is in the class r, and
1 otherwise. The process is repeated for all
classes (k times), and then each observation is classiﬁed into the class whose
constructed hyperplane is the furthest from it in the positive halfspace. In the
OVO approach, classes are separated with (cid:0)k
(cid:1) hyperplanes using one hyperplane
2
for each pair of classes, and the decision rule comes from a voting strategy in
which the most represented class among votes becomes the class predicted. OVA
and OVO inherit most of the good properties of binary SVM. In spite of that,
they are not able to correctly classify datasets where separated clouds of obser-
vations may belong to the same class (and thus are given the same label) when
a linear kernel is used. Another popular method is the directed acyclic graph
SVM, DAGSVM [1]. In this technique, although the decision rule involves the
same hyperplanes built with the OVO approach, it is not given by a unique vot-
ing strategy but for a sequential number of votings in which the most unlikely
class is removed until only one class remains. In addition, apart from OVA and
OVO, there are some other methods based on decomposing the original multi-
class problem into several binary classiﬁcation ones. In particular, in [2] and [20],
this decomposition is based on the construction of a coding matrix that deter-
mines the pairs of classes that will be used to build the separating hyperplanes.
Alternatively, other methods such as CS ([19]), WW ([44]) or LLW ([33]), do
not address the classiﬁcation problem sequentially but as a whole considering all
the classes within the same optimization model. Obviously, this seems to be the
correct approach. In particular, in WW, k hyperplanes are used to separate the
k classes, each hyperplane separating one class from the others, using k
1 mis-
classiﬁcation errors for each observation. The same separating idea, is applied

−

5

in CS but reducing the number of misclassiﬁcation errors for each observation
to a unique value. In LLW, a diﬀerent error measure is proposed to cast the
Bayes classiﬁcation rule into the SVM problem implying theoretical statistical
properties in the obtained classiﬁer. These properties cannot be ensured in WW
or CS.

We can also ﬁnd a quadratic extension based on LLW proposed by [26]. In
[43], the authors propose a multiclass SVM-based approach, GenSVM, in which
the classiﬁcation boundaries for a problem with k classes are obtained in a (k
−
1)-dimensional space using a simplex encoding. Some of these methods have
become popular and are implemented in most software packages in machine
learning as e1071 [37], scikit-learn [38] or [32]. Finally, in the recent work [11]
the authors propose an alternative approach to handle multiclass classiﬁcation
extending the paradigm of binary SVM classiﬁers by construnting a polyhedral
partition of the feature space and an assignment of classes to the cells of the
partition, by maximizing the separation between classes and minimizing two
intuitive misclassiﬁcation errors.

1.2. Contributions. In this paper, we propose a novel approach to construct
Classiﬁcation Trees for multiclass instances by means of a mathematical pro-
gramming model. Our method is based on two main ingredients: (1) An optimal
binary classiﬁcation tree (with oblique cuts) is constructed in the sense of [8],
in which the splits and pruned nodes are determined in terms of the misclassi-
ﬁcation errors at the leaf nodes; (2) The splits generating the branches of the
tree are build by means of binary SVM-based hyperplanes separating ﬁctitious
clases (which are also decided by the model), i.e., maximizing separation between
classes and minimizing the distance-based misclassiﬁcation errors.
Our speciﬁc contributions include:

(1) Deriving an interpretable multiclass classiﬁcation rule which combines
two of the most powerful tools in supervised classiﬁcation, namely OCT
and SVM.

(2) The classiﬁer is constructed using a mathematical programming model
that can be formulated as a Mixed Integer Second Order Cone Program-
ming problem. The classiﬁer is simple to apply and interpretable.
(3) Several valid inequalities are presented for the formulation that allow
one to strengthen the model and to solve larger size instances in smaller
CPU times.

(4) An extensive battery of computational experiments on realistic datases
from UCI is reported showing that our approach outperforms other de-
cision tree-based methodologies as CART, OCT and OCT-H.

6

V. BLANCO, A. JAP ´ON and J. PUERTO

1.3. Paper structure. Section 2 is devoted to ﬁx the notation and to recall
the tools that are used to derive our method. In Section 3 we detail the main
ingredients of our approach and illustrate its performance on a toy example.
The mathematical programming model that allows us to construct the classiﬁer
is given in Section 4, where we include all the elements involved in the model:
parameters, variables, objective function and constraints. In Section 5 we report
the results of our experiments to assess the performance of our method compared
with other tree-shaped classiﬁers. Finally, Section 6 is devoted to draw some
conclusions and future research lines on the topic.

2. Preliminaries

This section is devoted to introduce the problem under study and to ﬁx the
notation used through this paper. We also recall the main tools involved in our
proposed approach namely, Support Vector Machines and Optimal Classiﬁcation
Trees. These methods are adequately combined to develop a new method, called
Multiclass Optimal Classiﬁcation Trees with Support Vector Machines based
splits (MOCTSVM).

=

We are given a training sample,

,
}
{
which comes up as the result of measuring p features over a set of n observations
for each of them (y1, . . . , yn). The
1, . . . , K
(x1, . . . , xn) as well as a label in
{
goal of a classiﬁcation method is to build a decision rule so as to accurately
assign labels (y) to data (x) based on the behaviour of the given training sample

(x1, y1), . . . , (xn, yn)

1, . . . , K

} ⊆

×{

X

}

Rp

X

.
The ﬁrst ingredient that we use in our approach is the Support Vector Machine
method. SVM is one of the most popular optimization-based methods to design
a classiﬁcation rule in which only two classes are involved, usually referred as the
positive (y = +1) and the negative class (y =
1). The goal of linear SVM is to
construct a hyperplane separating the two classes by maximizing their separation
and simultaneously minimizing the misclassiﬁcation and margin violation errors.
Linear SVM can be formulated as the following convex optimization problem:

−

min

1
2 (cid:107)

ω

2
2 + c
(cid:107)

(cid:88)

ei

i∈N
s.t. yi(ω(cid:48)xi + ω0)

Rp, ω0
R+,

∈

ω

ei

∈

∈

≥
R,

ei,

1

−

N,

i
∀

∈

N.

i
∀

∈

where c is the regularization parameter that states the trade-oﬀ between training
errors and model complexity (margin), ω(cid:48) is the transpose of the vector ω and
2 is the Euclidean norm in Rp (other norms can also be considered but still

(cid:107) · (cid:107)

7

keeping similar structural properties of the optimization problem [13]). Note
that with this approach, the positive (resp. negative) class will tend to lie on the
Rp :
positive (resp. negative) half space induced by the hyperplane
ω(cid:48)z + ω0 = 0
. On the other hand, the popularity of SVM is mostly due to the so
}
call kernel trick. This allows one to project the data onto a higher dimensional
space in which a linear separation is performed in a most accurate way with no
need of knowing such a space, but just knowing the form of its inner products,
and maintaining the computational complexity of the optimization problem (see
[17] for further details).

H

=

∈

{

z

The second method that we combine in our approach is Classiﬁcation Trees.
CTs are a family of classiﬁcation methods based on a hierarchical relationship
among a set of nodes. These methods allow one to create a partition of the
feature space by means of hyperplanes that are sequentialy built. CT starts
on a node containing the whole sample, that is called the root node, in which
the ﬁrst split is applied. When applying a split on a node (by means of a
hyperplane separating the observations, two new branches are created leading
to two new nodes, which are referred to as its child nodes. The nodes are
usually distinguished into two groups: branch nodes, that are nodes in which a
split is applied, and on the other hand the leaf nodes, which are the terminal
nodes of the tree. Given a branch node and a hyperplane split in such a node,
their branches (left and right) are deﬁned as each of the two halfspaces deﬁned
by the hyperplane. The ﬁnal goal of CT is to construct branches in order to
obtain leaf nodes as pure as possible with respect to the classes. In this way,
the classiﬁcation rule for a given observation consists of assigning it to the most
popular class of the leaf where it belongs to.

There is a vast amount of literature on CTs since they provide an easy inter-
pretable classiﬁcation rule. One of the most popular methods to construct CT
is known as CART, introduced in [15]. CART is a greedy heuristic approach
that starts at the root node looking for the split in a single feature that mini-
mizes a given impurity function, creating two new nodes. The same procedure is
sequentially applied until a stop criteria is reached (maximal depth of the tree,
proportion of observations of a single class in a node, etc). The main advantage
of CART is its low computational cost, since nowadays very deep trees can be
obtained within a few seconds. However, CART does not guarantee the opti-
mality of the classiﬁcation tree, in the sense that more accurate trees could be
obtained if instead of locally constructing the branches one looks at the ﬁnal
conﬁguration of the leaf nodes. For instance, in Figure 1(left) we show a CT
constructed by CART for a biclass problem with maximal depth 2. We draw the
classiﬁcation tree, and also in the top right corner, the partition of the feature
space (in this case R2). As can be observed, the obtained classiﬁcation is not

8

V. BLANCO, A. JAP ´ON and J. PUERTO

perfect (not all leaf nodes are composed by pure classes) while in this case is not
diﬃcult to construct a CT with no classiﬁcation errors. This situation is caused
by the myopic construction done by the CART approach that, at each node only
cares on better classiﬁcation at their children, but not at the ﬁnal leaf nodes,
while subsequent branching decisions clearly aﬀect the overall shape of the tree.
Motivated by this drawback of CART, in [8], the authors propose an approach
to build an Optimal Classiﬁcation Tree (OCT) by solving a single Mathematical
Programming problem in which not only single-variable splits are possible but
oblique splits involving more than one predictive variable (by means of general
hyperplanes in the feature space) can be constructed. In Figure 1(right) we show
a solution provided by OCT with hyperplanes (OCT-H) for the same example.
One can observe that when splitting the root node (orange branches) a good
local split is not obtained (the nodes contain half of the observations in diﬀerent
classes), however, when adding the other two splits, the ﬁnal leaves only have
observations of the same class, resulting in a perfect classiﬁcation rule for the
training sample.

Figure 1. Example of a CT obtained with CART (left) and
OCT-H (right) approaches for the same instance.

Both approaches, OCTs and SVMs can be combined in order to construct
classiﬁcation trees in which the classes separated by the hyperplanes determined
in the CT are maximally separated, in the sense of the SVM approach. This idea
is not new and has been proven to outperform standard optimal decision trees
methods amongst many diﬀerent biclass classiﬁcation problems, as for instance,
In Figure 2 we show how
in [12] where the OCTSVM method is proposed.
one could construct OCTs with larger separations between the classes using
OCTSVM but still with the same 100% accuracy in the training sample as in
OCT-H, but more protected to misclassiﬁcation in out-sample observations.

Nevertheless, as far as we know, the combination of OCT and SVM has only
been analyzed for biclass instances. The extension of this method to multiclass
settings (more than two classes) is not trivial, since one could construct more
complex trees or use a multiclass SVM-based methodology (see e.g. [19, 44, 33]).

9

Figure 2. Example of a CT obtained with OCTSVM.

However, these adaptations of the classical SVM method have been proved to
fail in real-world instances (see [11]). In the rest of the paper we describe a novel
methodology to construct accurate multiclass tree-shaped classiﬁers based on a
diﬀerent idea: constructing CTs with splits induced by bi-class SVM separators
in which the classes of the observations at each one of the branch nodes are
determined by the model, but adequately chosen to provide small classiﬁcation
errors at the leaf nodes. The details of the approach are given in the next section.

3. Multiclass OCT with SVM splits

In this section we describe the method that we propose to construct classiﬁ-
cation rules for multiclass instances, in particular Classiﬁcation Trees in which
splits are generated based on the SVM paradigm.

As already mentioned, our method is based on constructing OCT with SVM
splits, but where the classes of the observations are momentarily ignored and
only accounted for at the leaf nodes. In order to illustrate the idea under our
method, in Figure 3 we show a toy instance with a set of points with four diﬀerent
classes (blue, red, orange and green).

Figure 3. Instance for a 4-class problem.

10

V. BLANCO, A. JAP ´ON and J. PUERTO

First, at the root node (the one in which all the observations are involved),
our method constructs a SVM separating hyperplane for two ﬁctitious classes
(which have to be also decided). A possible separation could be the one shown
in Figure 4, in which the training dataset has been classiﬁed into two classes
(black and white). This separation allows one to generate two child nodes, the

Figure 4. Root split on the 4-class classiﬁcation problem

black and the white nodes. At each of these nodes, the same idea is applied until
the leaf nodes are reached. In Figure 5 we show the ﬁnal partition of the feature
space according to this procedure.

Figure 5. Child node splits on the 4-class classiﬁcation problem
higheleted as the ﬁctitious classes decided in our model.

Clearly, ignoring the original classes of the training sample in the whole pro-
cess would result in senseless trees, unless one accounts for the goodness in the
classiﬁcation rule in the training sample at the leaf nodes. Thus, at the ﬁnal
leaf nodes, the original labels are recovered and the classiﬁcation is performed
according to the generated hyperplanes. The ﬁnal result of this tree is shown
in Figure 6 where one can check that the constructed tree achieves a perfect
classiﬁcation of the training sample.

Once the tree is constructed with this strategy, the decision rule comes up
naturally as it is usually done in decision trees methods, that is, out of sample
observations will follow a path on the tree according to the splits and they will
be assigned to the class of the leaf where they lie in (the most represented class

11

Figure 6. Child node splits on the 4-class classiﬁcation problem
with their original labels (colors).

In case a branch is pruned when building
of the leaf over the training set).
the tree, observations will be assigned to the most represented class of the node
where the prune took place.

4. Mathematical Programming Formulation for MOCTSVM

In this section we derive a Mixed Integer Non Linear Programming formulation

for the MOCTSVM method described in the previous section.

1, . . . , K

We assume to be given a training sample

(x1, y1), . . . , (xn, yn)
×
the index set for the observations in
{
the training sample. We also consider the binary representation of the labels y
as:

. We denote by N =
}

1, . . . , n

} ⊆

=

X

{

{

}

Rp

(cid:40)

Yik =

1 if yi = k,
0 otherwise,

for all i

∈

N, k = 1, . . . K.

∈

Moreover, without loss of generality we will assume the features to be normalized,
i.e., x1, . . . , xn

[0, 1]p.

τ

}

∈

−

For any node t

1, . . . , T
{

We will construct decision trees with a ﬁxed maximum depth D. Thus, the
1 nodes. We denote by
the index set for the tree nodes, where node 1 is the root node

classiﬁcation tree is formed by at most T = 2D+1
τ =
and nodes 2D, . . . , 2D+1
−
1
}

1 are the leaf nodes.
, we denote by p(t) its (unique) parent node. The tree
nodes can be classiﬁed in two sets: branching and leaf nodes. The branching
nodes, that we denote by τb, will be those in which the splits are applied. In
constrast, in the leaf nodes, denoted by τl, no splits are applied but is where
predictions take place. The branching nodes can be also classiﬁed into two sets:
τbl and τbr depending on whether they follow the left or the right branch on
the path from their parent nodes, respectively. τbl nodes are indexed with even
numbers meanwhile τbr nodes are indexed with odd numbers.

\{

We deﬁne a level as a set of nodes which have the same depth within the
tree. The number of levels in the tree to be built is D + 1 since the root node is
be the set of levels of the tree,
assumed as the zero-level. Let U =

u0, . . . , uD

{

}

12

V. BLANCO, A. JAP ´ON and J. PUERTO

where each us
notation, the root node is u0 while uD represent the set of leaf nodes.

U is the set of nodes at level s, for s = 0, . . . , D. With this

∈

In Figure 7 we show the above mentioned elements in a 3-depth tree.

Figure 7. Elements in a depth D = 3 tree.

Apart from the information about the topological structure of the tree, we
also consider three regularization parameters that have to be calibrated in the
validation process that allow us to ﬁnd a trade-oﬀ between the diﬀerent goals
that we combine in our model: margin violation and classiﬁcation errors of
the separating splitting hyperplanes, correct classiﬁcation at the leaf nodes and
complexity of the tree. These parameters are the following:

c1: unit misclassiﬁcation cost at the leaf nodes.

c2: unit distance based misclassiﬁcation errors for SVM splits.

c3: unit cost for each splitting hyperplane introduced in the tree.

The complete list of index sets and parameters used in our model are summa-

rized in Table 1.

4.1. Variables. Our model uses a set of decision and auxiliary variables that
are described in Table 2. We use both binary and continuous decision variables
to model the MOCTSVM. The binary variables allow us to decide the alloca-
tion of observations to the decision tree nodes, or to decide whether a node is
splited or not in the tree. The continuous variables allow us to determine the
coeﬃcients of the splitting hyperplanes or the misclassiﬁcation errors (both in th
SVM separations or at the leaf nodes). We also use auxiliary binary and integer
variables that are useful to model adequately the problem.

In Figure 8 we illustrate the use of these variables in a feasible solution of a

toy instance with three classes (red,blue and green).

t=1(root)t=2t=4t=8t=9t=5t=10t=11t=3t=6t=12t=13t=7t=14t=15u0u1u2u3p(4)==p(5)=p(7)p(6)=τb3τb3τb3τl3τblτbrτblτbrτbrτbl13

N =

τ =

Index set for the observations in the training sample.

}
D Maximal depth of the tree.

1, . . . , n
{
T = 2D+1 maximal number of nodes in a D-depth tree.
1, . . . , T

Index set for the set of nodes of the tree.

{

}

p(t) Parent of node t, for t

1
\{
τ Branching nodes of the tree.
τl Leaf nodes of the tree.
τb Nodes that follow the left branch on the path from

.
}

∈

τ

their parent nodes.

τb Nodes whose right branch has been followed on the

path from their parent nodes.

τb

∈

τbl

τbr

∈

∈

us Nodes at level s of the tree, for s = 0, . . . , D.

U =

{

u0, . . . , ud

Sets of levels of the tree.
}
c1 Unit misclassiﬁcation cost.
c2 Unit distance based missclassiﬁcation errors for SVM

splits.

c3 Unit cost for splitting hyperplanes.

Table 1. Index sets and parameters used in our model.

ωt
∈
ωt0 ∈
eit
∈
δ
∈

Continuous Decision Variables

Rp Coeﬃcients of the separating hyperplane of node t.
R Intercept of the separating hyperplane of node t.
R+ Misclassiﬁcation error of observation i at node t.
R+ Inverse of the minimum margin between splitting hy-

perplanes.

Binary Decision Variables

Is one if observation i belongs to node t and zero oth-
erwise.
Is one if a split is applied at node t and zero otherwise.

zit

dt

0, 1

∈ {

0, 1

∈ {

}

}

Auxiliary Variables

αit

hit

0, 1

∈ {

Lt

Z+ Number of misclassiﬁed observations at leaf node t.
∈
0, 1
∈ {

}

Is one if observation i belongs to the reference ﬁctitious
class in node t and zero otherwise.
Is one if observation i is in node t and lies on the
positive half space of the hyperplane of node t, and
zero otherwise.
Is one if not all observations in node t lie on the pos-
itive half space of the hyperplane in node t and zero
otherwise.
Is one if class k is the most represented one in leaf
node t and zero otherwise.

vt

0, 1

∈ {

qkt

0, 1

∈ {

}

}

}

Table 2. Summary of the variables used in our model.

The whole set of training observation is considered at the root node (node
t = 1). There, the original labels are ignored and to determine the ﬁctitious class
of each observation a SVM-based hyperplane is constructed. Such a hyperplane

14

V. BLANCO, A. JAP ´ON and J. PUERTO

Figure 8. Illustration of the sets of variables used in our model
in a toy example.

∈

∈

2
(cid:107)ω1(cid:107)2

Rp and ω10

R (hyperplane/line drawn with
is deﬁned by the coeﬃcients ω1
) and
a dotted line in the picture) and it induces a margin separation (
misclassiﬁcation errors ei1.
In the feasible solution drawn in the ﬁgure, only
three observations induce positive errors (those that are classiﬁed either in the
margin area or in the opposite side of the hyperplane). Such a hyperplane also
determines the splitting rule for the deﬁnition of the children of that node. Since
the node is split (d1 = 1), the observations that belongs to the positive side of the
hyperplane are assigned to the left node (node t = 2) while those in the negative
side are assigned to the right node (node t = 3) through the z-variables. At
node t = 2, the same scheme is applied, that is, the hyperplane deﬁned by ω2
is constructed, inducing SVM-based margin and errors and since d2 = 1, also
the splitting rule applies to deﬁne nodes t = 4 and t = 5. At node t = 2, one
must control the observations in that node to quantify the misclassifying errors,
ei2, only for those observations in the objective function. Speciﬁcally, we only
account for these errors for the observations that belong to the node (zi2 = 1)
and either belong to the positive (αi2 = 1) or the negative (αi2 = 0) side of the
hyperplane. Also, in order to control the complexity of the tree, the h-variables
are used to know whether an observation belongs to the node and to the positive

t=12kω1k2ei1ei1ei1+-αi1=1αi1=0ω11z+ω10t=22kω2k2−+ω21z+ω20αi1=1(hi1=1)αi2=0(hi2=0)αi3=1(hi2=0)αi1=1(hi1=1)αi2=1(hi1=1)αi3=1(hi1=0)ei2=0t=4qblue4=1t=5qred5=1t=3ω31z+ω30αi1=0(hi1=0)αi2=1(hi2=0)αi3=0(hi3=0)+−t=6qblue6=1t=7qgreen7=1d1=1d2=1d3=0L4=1L5=2L6=0L7=3zi2=1zi3=1zi4=1zi5=1zi6=1zi7=1v1=1v2=1v3=115

side of the SVM-hyperplane. If all observations in a node belong to the positive
side of the hyperplane, the variable v assumes the value 0. Otherwise, in case v
takes value 1, two situations are possible: 1) there are observations in both sides
of the hyperplane (as in node t = 2) inducing a new split (d2 = 1), and 2) all
observations belong to the negative side (as in node t = 3) determining that the
tree is pruned at that node (d3 = 0).

Concerning the leaf nodes, node t = 2 is split into nodes t = 4 and t = 5
and node t = 3, which was decided to be no longer split, is ﬁctitiously split in
two leaf nodes, although one of them is empty and the other one receives all
the observations of the parent node (node t = 3). The allocation of any leaf
node τl to a class is done through the q-variables (to the most popular class in
the node or arbitrarily in case the node has no observations) and the number of
misclassiﬁed observations is accounted for by the L-variables.

4.2. Objective Function. As already mentioned, our method aims to con-
struct classiﬁcation trees with small misclassiﬁcation errors at the leaf nodes,
but at the same time with maximal separation between the classes with the
SVM-based hyperplanes and minimum distance based errors.

Using the variables described in the previous section, the four terms that are

included in the objective functions are the following:

Margins of the splitting hyperplanes:: The separating hyperplane of
. Thus, our method aims to
branching node t
maximize the minimum of these margins. This is equivalent to minimize
the maximum among the inverse margins
which is
represented by the auxiliary variable δ.

τb has margin

2
(cid:107)ωt(cid:107)2

2
2 : t

1
2 (cid:107)

ωt

τb

∈

∈

{

(cid:107)

}

Misclassiﬁcation Errors at the leaf nodes:: Variable Lt accounts for
the number of misclassiﬁed observations in leaf node t, i.e., the number
of observations that do not belong to the most represented class in that
leaf node. These variables allow us to count the overall number of mis-
classiﬁed observations in the training sample. Therefore, the amount to
be minimized by the model is given by the following sum:

c1

(cid:88)

t∈τl

Lt

Distance-based Errors at branching nodes:: Each time a split is added
to the tree, a SVM-based hyperplane in which the labels are assigned
based on the global convenience of for the overall tree is incorporated.
Thus, we measure, at each branching node in τb, the distance-based errors
incurred by the SVM classiﬁer at that split. This amount is measured

16

V. BLANCO, A. JAP ´ON and J. PUERTO

by the eit variables and is incorporated to the model through the sum:

(cid:88)

(cid:88)

c2

eit

i∈N

t∈τb

Complexity of the tree: The simplicity of the resulting tree is measured
by the number of splits that are done in its construction. Since the dt
variable tells us whether node t is split or not, this term is accounted for
in our model as:

(cid:88)

dt

c3

Summarizing, the overall objective function of our model is:

t∈τb

min δ + c1

Lt + c2

(cid:88)

t∈τl

(cid:88)

(cid:88)

i∈N

t∈τb

eit + c3

(cid:88)

t∈τb

dt.

(OBJ)

Note that the coeﬃcients c1, c2 and c3 trade-oﬀ the misclassiﬁcation of the
training sample, the separation between classes and the complexity of the tree,
respectively. These parameters should be carefully calibrated in order to con-
struct simple decision trees with high predictive power, as can be seen in our
computational experiments.

4.3. Constraints. The requirements on the relationships between the variables
and the rationale of our model are described through the following constraints
that deﬁne the mathematical programming model.

First of all, in order to adequately represent the maximum among the inverse

margins of the sppliting hyperplanes, we require:

δ

1
2 (cid:107)

ωt

2
2,

t
∀

τb.

∈

≥

(cid:107)
Next, we impose how the splits are performed in the tree. To this end, we need
to know which observations belong to a certain node t (z-variable) and how
these observations are distributed with respect to the two ﬁctitious classes to
be separated (α-variables). Gathering all these elements together, we use the
following constraints to deﬁne the splits of the decision tree:

(C1)

ω(cid:48)
txi + ωt0
ω(cid:48)
txi + ωt0

1

≥

≤ −

eit

(1

hit)

−
−
−
1 + eit + (1

zit + αit)

−

N, t

N, t

i
∀
i
∀

∈

∈

∈

∈

τb,

τb.

(C2a)

(C2b)

According to this, constraint (C2a) is activated just in case the observation i
belongs to the reference class and it is in node t (hit = 1). On the other hand,
(C2b) is activated if i is allocated to node t (zit = 1) but it does not belong
to the reference class (αit = 0). Therefore, the reference class is located on the
positive half space of hyperplane
t, while the other class is positioned in the
negative half space, and at the same time, margin violations are regulated by
the eit variables.

H

17

To ensure the correct behaviour of the above constraints, we must correctly
deﬁne the zit variables. First, it is required that each observation belongs to
exactly one node per level in the tree. This can be easily done by adding the
usual assingment constraints to the problem at each of the levels, u
U , of the
tree:

∈

(cid:88)

t∈u

zit = 1

N, u

i
∀

∈

U.

∈

(C3)

Furthermore, we should enforce that if observation i is in node t (zit = 1),
then observation i must also be in the parent node of t, p(t) (zip(t) = 1), and
also observation i can not be in node t if it is not in its parent node (zip(t) =
zit = 0). These implications can be obtained by means of the following
0
constraints:

⇒

zit

≤

zip(t)

N, t = 2, . . . , T.

(C4)

i
∀

∈

Nevertheless, the way observations descend through the tree needs a further
analysis, since at this point they could just randomly deﬁne a path in the tree.
Whenever an observation i is in the positive half space of the splitting hyperplane
t, this observation should follow the right branch connecting to the
at node t,
child node of t. Otherwise, in case i is on the negative half space, it should follow
the left branch. The knowledge on the side of the splitting hyperplane where an
observation belongs to is encoded in the α-variables. Then, in case i lies on the
positive half space of
t, αit will never be equal to zero since it would lead to a
value of eit greater than one, while eit < 1 is guaranteed in case αit = 1.

H

H

With the above observations, the constraints that assure the correct construc-
tion of the splitting hyperplanes with respect to the side of them where the
observations belong to are the following:

zip(t) −
zip(t) −

zit

zit

≤

≤

αip(t)

αip(t)

1

−

N, t

N, t

i
∀
i
∀

∈

∈

∈

∈

τbl,

τbr.

(C5a)

(C5b)

Constraints (C5a) assure that if observation i is on the parent node of an even
node t (zip(t) = 1), and i lies on the negative half space of
Hp(t) (αip(t) = 0),
then zit is enforced to be equal to one. As a result, αip(t) = 0 forces observation
i to take the left branch in node t. Note that in case zip(t) = 1, and at the same
τbl), then
time observation i is not in the left child node of t (zit = 0 for i
αip(t) = 1, which means that observation i lies on the positive half space of
Hp(t).
Constraints (C5b) are analogous to (C5a) but allowing to adequately represent
right branching nodes.

∈

18

V. BLANCO, A. JAP ´ON and J. PUERTO

(cid:107)

2
2 ≤

Moreover, two additional important elements need to be incorporated to com-
plete our model: the tree complexity and the correct deﬁnition of misclassi-
ﬁed observations. Note that in usual Optimal Classiﬁcation Trees that do not
use SVM-based splits, the complexity can be easily regulated by just imposing
M dt (for a big enough M constant) in all the branch nodes, since in case
ωt
(cid:107)
a node is no further branched (dt = 0), the coeﬃcients of the splitting hyper-
plane are set to zero. However, in our case, in which the splitting hyperplanes are
SVM-based hyperplanes, these constraints are in conﬂict with constraints (C2a)
and (C2b), since in case dt = 0 (and therefore ωt = 0) it would not only imply
that the coeﬃcients ωt are equal to zero, but also that the distance based errors
would be set to the maximum value of 1, i.e., eit = 1 for every observation i in
the node, even though these errors would not make any sense since observations
would not be separated at the node. To overcome this issue, we consider the
auxiliary binary variables hit = zitαit (hit takes value 1 if observation i belongs
to node t and lies in the positive half-space of the splitting hyperplane applied at
node t) and vt (that takes value zero in case all the points in the node belong to
the positive halfspace and one otherwise). The variables are adequatelly deﬁned
if the following constraints are incorporated to the model:

hit

≥

zit + αit

1,

−
αit + 1,

hit
(cid:88)

zit

≤
(zit

−

hit)

≤

nvt,

n(1 + dt

vt),

−

−

≤

i∈N
(cid:88)

i∈N

hit

N, t

N, t

i
∀
i
∀

∈

∈

t
∀

τb,

τb,

τb,

∈

∈

∈

t
∀

∈

τb,

(C6a)

(C6b)

(C6c)

(C6d)

where constraints (C6a) and (C6b) are the linearization of the bilinear constraint
hit = zitαit. On the other hand, Constraints (C6c) assure that in case vt =
0, then all observations in node t belong to the positive halfspace of
t, and
constraints (C6d) assure that if vt = 1 and the tree is pruned at node t (dt = 0),
then those observations allocated to node t are placed in the negative halfspace
deﬁned by the splitting hyperplane. Thus, it implies that dt takes value one
if and only if the observations in node t are separated by
t, and therefore
producing an eﬀective split at the node.

H

H

Finally, in order to adequately represent the Lt variables (the ones that mea-
sure the number of misclassiﬁed observations at the leaf nodes) we use the con-
straints already incorporated in the OCT-H model in [8]. On the one hand, we
assign each leaf node to a single class (the most popular class of the observations
that belong to that node). We use the binary variable qkt to check whether leaf
τl is assigned to class k = 1, . . . , K. The usual assignment constraints
node t

∈

are considered to assure that each node is assigned to exactly one class:

K
(cid:88)

k=1

qkt = 1,

t
∀

∈

τl.

19

(C7)

The correct deﬁnition of the variable Lt is then guaranteed by the following

set of constraints:

Lt

≥

(cid:88)

i∈N

zit

−

(cid:88)

i∈N

Yikzit

n(1

−

−

qkt),

k = 1, . . . , K, t

∀

τl,

∈

(C8)

These constraints are activated if and only if qkt = 1, i.e., if observations in node t
are assigned to class k. In such a case, since Lt is being minimized in the objective
function, Lt will be determined by the number of training observations in node
t except those whose label is k, i.e., the number of missclasiﬁed observations in
node t according to the k-class assignment.

Observe that the constant n in (C8) can be decreased and ﬁxed to the maxi-
mum number of misclassiﬁed observations in the training sample. This number
coincide with the diﬀerence between the number of observations in the training
sample (n) and the number of observations in the most represented class in the
sample.

Summarizing the above paragraphs, the MOCTSVM can be formulated as the

following MINLP problem:

min δ + c1

(cid:88)

t∈τl

Lt + c2

(cid:88)

(cid:88)

i∈N

t∈τ

eit + c3

(cid:88)

t∈τ

dt

s.t. δ

ωt

≥

1
2 (cid:107)
ω(cid:48)
txi + ωt0
ω(cid:48)
txi + ωt0
(cid:88)

,

(cid:107)

≥

1

≤ −

zit = 1,

eit

(2

−
−
−
1 + eit + (1

zit

αit),

−
zit + αit),

−

zip(t),

t∈u

zit
≤
zip(t) −
zip(t) −
hit
≥

zit

zit

≤

≤

αip(t),

αip(t),

1

−

zit + αit

1,

−
αit + 1,

hit
(cid:88)

zit

≤
(zit

−

i∈N
(cid:88)

i∈N

hit

hit)

≤

nvt,

n(1 + dt

vt),

−

−

≤

(OBJ)

(C1)

(C2a)

(C2b)

(C3)

t
∀
N, t

N, t

N, u

τb,

τb,

τb,

U,

∈

∈

∈

∈

i
∀
i
∀
i
∀

∈

∈

∈

N, t = 2, . . . , T,

(C4)

i
∀

∈

N, t

∈
N, t

τbl,

∈
τbr, .

i
∀
i
∀

∈
i
∀
i
∀

∈

∈

∈
N, t

N, t

t
∀

τb,

τb,

τb,

∈

∈

∈

(C5a)

(C5b)

(C6a)

(C6b)

(C6c)

t
∀

∈

τb,

(C6d)

20

V. BLANCO, A. JAP ´ON and J. PUERTO

K
(cid:88)

k=1

qkt = 1,

t
∀

∈

τl,

Yikzit

n(1

−

−

qkt),

k = 1, . . . , K, t

∀

(cid:88)

(cid:88)

zit

−

i∈N

i∈N
R+, αit, hit

≥

∈

Lt

eit

zit

qkt

ωt

0, 1

∈ {

,
}

N, t

i
∀

∈

∈ {

∈ {

,

0, 1
}
,
0, 1
}
Rp, ωt0

∈

R, dt

∈

0, 1
}

∈ {

,

N, t = 1, . . . , T,

i
∀

∈

k = 1 . . . , K, t

∀

τl,

∈

t = 1, . . . , T.

∀

(C7)

(C8)

τl,

τb,

∈

∈

4.4. Strengthening the model. The MINLP formulation presented above is
valid for our MOCTSVM model. However, it is a computationally costly prob-
lem, and although it can be solved by most of the oﬀ-the-shelf optimization
solvers (as Gurobi, CPLEX or XPRESS), it is able to solve optimally only small
to medium size instances. To improve its performance, the problem can be
strengthen by means of valid inequalities which allows one to reduce the gap be-
tween the continuous relaxation of the problem and its optimal integer solution,
being then able to solve larger instances in smaller CPU times. In what follows
we describe some of these inequalities that we have incorporated to the MINLP
formulation:

•

If observations i and i(cid:48) belongs to diﬀerent nodes, they cannot be assigned
to the same node for the remainder levels of the tree:

zis + zi(cid:48)s

zit + zi(cid:48)t,

u, s

u(cid:48)u

u(cid:48)

∈
If leaf nodes t and s are the result of proper splitting hyperplanes, then,
both nodes cannot be assigned to the same class:

≤

≤

∈

t
∀

qkt + qks

2

−

≤

dp(t),

t, s = 2, . . . , T (t

∀

= s) with p(t) = p(s), k = 1, . . . , K.

Variable αit is enforced to take value 0 in case zit = 0:

∈
Variable hit is not allowed to take value one if αit takes value zero:

≤

∈

αit

zit,

i
∀

N, t

τb.

hit

αit,

i
∀

∈

≤

N, t

∈

τb.

There should be at least a leaf node to which each class is assigned to
(assuming that each class is represented in the training sample). It also
implies that the number of nodes to which a class is assigned is bounded
as:

1

≤

(cid:88)

t∈τl

qkt

2D

1,

−

≤

k = 1, . . . , K.

∀

•

•

•

•

(cid:54)
21

(cid:107)

∈

∈

−

ai

ai(cid:48)

N :

arg max

ε and yi = yi(cid:48)

i(cid:48)
i∈N | {

In order to reduce the dimensionality and also to avoid symmetries of the
MINLP problem, one can also apply some heuristic strategies to ﬁx the values of
some of the binary variables in a preprocessing phase. For instance, we choose
i0
, that is, the observation
with a maximum amount of observations in the same class close enough to it.
Then, we ﬁx to one all the variables zit0 with i
ε and yi =
yi(cid:48)
, being t0 the ﬁrst left leaf node of the tree (and ﬁxing to zero the allocation
of these points to the rest of the leaf nodes). Analogously, we ﬁx also to one all
the z-variables allocating observations to the last right leaf node of the tree for a
subset of observations in the same class which are far enough from i0, i.e., zitf = 1
ai
ε and yif = yi(cid:48)
for all i

where if = arg max

i(cid:48)
∈ {

ai
(cid:107)

(cid:107) ≤

(cid:107) ≤

N :

N :

} |

ai(cid:48)

ai(cid:48)

−

i(cid:48)

∈

}

∈ {

∈

aif −
(cid:107)

(cid:107) ≤

i∈N (cid:107)

ai0 −

(cid:107)

(and ﬁxing to zero the allocation of these points to the rest of the leaf nodes).

}

5. Experiments

In order to analyze the performance of this new methodology we have run a
series of experiments among diﬀerent real datasets from UCI Machine learning
Repository [3]. We have chosen twelve datasets with number of classes between
two and seven. The dimension of these problems is reported in Table 3 by the tu-
ple (n : number of observations, p : number of features, K : number of classes).

We have compared the MOCTSVM model with three other Classiﬁcation
Tree-based methodologies, namely CART, OCT and OCT-H. The maximum
tree depth, D, for all the models was equal to 3, and the minimum number of
observations per node in CART, OCT and OCT-H was equal to the 5% of the
training size.

We have performed, for each instance a 5-fold cross validation scheme, i.e.,
datasets have been splited into ﬁve random train-test partitions where one of
the folds is used to build the model and the remaining are used to measure the
accuracy of the predictions. Moreover, in order to avoid taking advantage of
beneﬁcial initial partitions, we have repeated the cross-validation scheme ﬁve
times for all the datasets.

The CART method was coded in R using the rpart library. On the other
hand, MOCTSVM, OCT and OCT-H were coded in Python and solved using
the optimization solver Gurobi 8.1.1. All the experiments were run on a PC Intel
Xeon E-2146G processor at 3.50GHz and 64GB of RAM. A time limit of 300
seconds was set for training the training folds. Although not all the problems
were optimally solved within the time limit, as can be observed in Table 3, the
results obtained with our model already outperform the other methods.

In order to calibrate the parameters of the diﬀerent models that regulate the
complexity of the tree, we have used diﬀerent approaches. On the one hand,

22

V. BLANCO, A. JAP ´ON and J. PUERTO

−

for CART and OCT, since the maximum number of nodes for such a depth
is 2D
1 = 7, one can search for the tree with best complexity by searching
1(cid:9) of possible active nodes. For OCT-H, we search
in the grid (cid:8)1, . . . , 2D
−
5, . . . , 5(cid:9). Finally,
the complexity regularization factor in the grid (cid:8)10i : i =
5, . . . , 5(cid:9) for c1 and c2, and
in MOCTSVM we used the same grid (cid:8)10i : i =
(cid:8)10i : i =

2, . . . , 2(cid:9) for c3.

−

−

In Table 3 we report the results obtained in our experiments for all the models.
The ﬁrst column of the table indicates the identiﬁcation of the dataset (together
with its dimensionality). Second, for each of the methods that we have tested, we
report the obtained average test accuracy and the standard deviation. We have
highlighted in bold the best average test accuracies obtained for each dataset.

−

As can be observed, our method clearly outperforms in most of the instances
the rest of the methods in terms of accuracy. Clearly, our model is designed
to construct Optimal Classiﬁcation Trees with larger separations between the
classes, which results in better accuracies in the test sample. The datasets
Australian and BalanceScale obtain their better results with OCT-H, but, as
can be observed, the diﬀerences with respect the rest of the methods are tiny (it
is the result of correctly classifying in the test sample just a few more observations
than the rest of the methods). In that case, our method gets an accuracy almost
as good as OCT-H. In the rest of the datasets, our method consistently gets
better classiﬁers and for instance for Dermatology the diﬀerence with respect
to the best classiﬁers among the others ranges in [4%, 19%], for Parkinson the
accuracy with our model is at least 6% better than the rest, for Wine we get 5%
more accuracy than OCTH and 10% more than CART and for Zoo the accuracy
of our model is more than 17% greater than the one obtained with CART.

Concerning the variability of our method, the standard deviations reported in
Table 3 show that our results are, in average, more stable than the others, with
small deviations with respect to the average accuracies. This behaviour diﬀers
from the one observed in CART or OCT, where larger deviations are obtained,
implying that the accuracies highly depends of the test folder where the method
is applied.

6. Conclusions and Further Research

We have presented in this paper a novel methodology to construct classiﬁers
for multiclass instances by means of a Mathematical Programming model. The
proposed method outputs a classiﬁcation tree were the splits are based on SVM-
based hyperplanes. At each branch node of the tree, a binary SVM hyperplane
is constructed in which the observations are classiﬁed in two ﬁctitious classes
(the original classes are ignored in all the splitting nodes), but the global good-
ness of the tree is measured at the leaf nodes, where misclassiﬁcation errors

CART

OCT

OCT-H

MOCTSVM

Diﬀ

23

Australian
(690,14,2)
BalanceScale
(625,4,3)
Banknote
(1372,5,2)
BreastCancer
(683,9,2)
Dermatology
(358,34,6)
Heart
(294,13,5)
Iris
(150,4,3)
Parkinson
(240,40,2)
Seeds
(210,7,3)
Teaching
(150,5,3)
Thyroid
(215,5,3)
Wine
(178,13,3)
Zoo
(101,16,7)

85.54

69.55

89.27

92.69

75.69

64.37

94.26

72.29

86.36

41.91

89.77

84.52

±

±

±

±

±

±

±

±

±

±

±

±

0.81 85.22

1.76 73.30

0.95 88.50

1.01 94.16

3.60 77.82

1.48 65.14

1.90 95.37

4.05 73.53

4.02 88.52

5.64 48.35

2.37 92.43

2.66 92.22

±

±

±

±

±

±

±

±

±

±

±

±

1.27 85.65

1.20 90.43

1.17

98.89

0.54

95.10

4.34

91.41

1.57

64.30

0.97

95.64

2.26

74.92

2.69

91.12

3.80

48.09

2.12

92.46

3.41

89.35

1.02

85.27

1.07

89.53

0.33

98.91

1.26

96.27

2.83

95.39

1.79

66.41

1.46

95.72

3.01

80.83

2.99

92.98

2.92

48.62

2.49

94.57

3.71

94.13

2.58

92.31

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

1.11

-0.38

1.28

-0.90

0.63

1.69

±

±

0.46

0.02

0.64

1.17

1.47

3.98

1.54

1.26

1.79

0.08

1.89

5.91

1.82

1.85

3.37

0.26

2.08

2.11

1.78

1.90

2.15

3.20

0.43

1.39

2.74

1.38

1.70

3.06

2.23

4.60

3.10

3.37

2.95

±

±

±

±

±

±

±

±

±

±

±

±

74.96

5.79 87.75

1.99
Table 3. Average accuracies (
±
in our computational experiments.

±

89.11

standard deviations) obtained

are minimized. Also, the model minimizes the complexity of the tree together
with the two elements that appear in SVM-approaches: margin separation and
distance-based misclassifying errors. We have run an extensive battery of com-
putational experiments that shows that our method outperforms most of the
Decision Tree-based methodologies both in accuracy and stability.

Future research lines on this topic include the analysis of nonlinear splits when
branching in MOCTSVM, both using kernel tools derived from SVM classiﬁers
or speciﬁc families of nonlinear separators. This approach will result into more
ﬂexible classiﬁers able to capture the nonlinear trends of many real-life datasets.
Additionally, we also plan to incorporate features selection in our method in
order to construct highly predictive but also more interpretable classiﬁcation
tools.

24

V. BLANCO, A. JAP ´ON and J. PUERTO

Acknowledgements

This research has been partially supported by Spanish Ministerio de Ciencia
e Innovaci´on, Agencia Estatal de Investigaci´on/FEDER grant number PID2020-
114594GBC21, Junta de Andaluc´ıa projects P18-FR-1422 and projects FEDERUS-
1256951, BFQM-322-UGR20, CEI-3-FQM331 and NetmeetData-Ayudas Fun-
daci´on BBVA a equipos de investigaci´on cient´ıﬁca 2019. The ﬁrst author was
also partially supported by the IMAG-Maria de Maeztu grant CEX2020-001105-
M /AEI /10.13039/501100011033.

References

[1] Agarwal, N., Balasubramanian, V. N., and Jawahar, C. Improving multiclass clas-
siﬁcation by deep networks using dagsvm and triplet loss. Pattern Recognition Letters 112
(2018), 184–190.

[2] Allwein, E. L., Schapire, R. E., and Singer, Y. Reducing multiclass to binary: A uni-
fying approach for margin classiﬁers. Journal of machine learning research 1, Dec (2000),
113–141.

[3] Asuncion, A., and Newman, D. Uci machine learning repository, 2007.
[4] Bahlmann, C., Haasdonk, B., and Burkhardt, H. Online handwriting recognition
with support vector machines-a kernel approach. In Proceedings Eighth International
Workshop on Frontiers in Handwriting Recognition (2002), IEEE, pp. 49–54.

[5] Baldomero-Naranjo, M., Mart´ınez-Merino, L. I., and Rodr´ıguez-Ch´ıa, A. M.
Tightening big ms in integer programming formulations for support vector machines with
ramp loss. European Journal of Operational Research 286, 1 (2020), 84–100.

[6] Baldomero-Naranjo, M., Mart´ınez-Merino, L. I., and Rodr´ıguez-Ch´ıa, A. M. A
robust svm-based approach with feature selection and outliers detection for classiﬁcation
problems. Expert Systems with Applications 178 (2021), 115017.

[7] Ben´ıtez-Pe˜na, S., Blanquero, R., Carrizosa, E., and Ram´ırez-Cobo, P. Cost-
sensitive feature selection for support vector machines. Computers & Operations Research
106 (2019), 169–178.

[8] Bertsimas, D., and Dunn, J. Optimal classiﬁcation trees. Machine Learning 106, 7

(2017), 1039–1082.

[9] Bertsimas, D., and Dunn, J. W. Machine learning under a modern optimization lens,

2019.

[10] Blanco, V., Jap´on, A., and Puerto, J. A mathematical programming approach to
binary supervised classiﬁcation with label noise. arXiv preprint arXiv:2004.10170 (2020).
[11] Blanco, V., Jap´on, A., and Puerto, J. Optimal arrangements of hyperplanes for svm-
based multiclass classiﬁcation. Advances in Data Analysis and Classiﬁcation 14, 1 (2020),
175–199.

[12] Blanco, V., Jap´on, A., and Puerto, J. Robust optimal classiﬁcation trees under noisy

labels. Advances in Data Analysis and Classiﬁcation (2021).

[13] Blanco, V., Puerto, J., and Rodriguez-Chia, A. M. On lp-support vector machines

and multidimensional kernels. J. Mach. Learn. Res. 21 (2020), 14–1.

[14] Blanquero, R., Carrizosa, E., Molero-R´ıo, C., and Morales, D. R. Optimal
randomized classiﬁcation trees. Computers & Operations Research 132 (2021), 105281.

25

[15] Breiman, L., Friedman, J., Olshen, R., and Stone, C. Classiﬁcation and regression

trees, 1984.

[16] Carrizosa, E., Molero-R´ıo, C., and Morales, D. R. Mathematical optimization in

classiﬁcation and regression trees. Top 29, 1 (2021), 5–33.

[17] Cortes, C., and Vapnik, V. Support-vector networks. Machine learning 20, 3 (1995),

273–297.

[18] Cover, T., and Hart, P. Nearest neighbor pattern classiﬁcation. IEEE transactions on

information theory 13, 1 (1967), 21–27.

[19] Crammer, K., and Singer, Y. On the algorithmic implementation of multiclass kernel-
based vector machines. Journal of machine learning research 2, Dec (2001), 265–292.
[20] Dietterich, T. G., and Bakiri, G. Solving multiclass learning problems via error-
correcting output codes. Journal of artiﬁcial intelligence research 2 (1994), 263–286.
[21] Du, M., Liu, N., and Hu, X. Techniques for interpretable machine learning. Communi-

cations of the ACM 63, 1 (2019), 68–77.

[22] Eitrich, T., and Lang, B. Eﬃcient optimization of support vector machine learning
parameters for unbalanced datasets. Journal of computational and applied mathematics
196, 2 (2006), 425–436.

[23] Friedman, J., Hastie, T., and Tibshirani, R. The elements of statistical learning,

2001.

[24] Gan, J., Li, J., and Xie, Y. Robust svm for cost-sensitive learning. Neural Processing

Letters (2021), 1–22.

[25] Gaudioso, M., Gorgone, E., Labb´e, M., and Rodr´ıguez-Ch´ıa, A. M. Lagrangian
relaxation for svm feature selection. Computers & Operations Research 87 (2017), 137–145.
[26] Guermeur, Y., and Monfrini, E. A quadratic loss multi-class svm for which a radius–

margin bound applies. Informatica 22, 1 (2011), 73–96.

[27] G¨unl¨uk, O., Kalagnanam, J., Menickelly, M., and Scheinberg, K. Optimal deci-
sion trees for categorical data via integer programming. arXiv preprint arXiv:1612.03225
(2018).

[28] Harris, T. Quantitative credit risk assessment using support vector machines: Broad
versus narrow default deﬁnitions. Expert Systems with Applications 40, 11 (2013), 4404–
4413.

[29] Hastie, T. J., and Tibshirani, R. J. Generalized additive models, 2017.
[30] Jim´enez-Cordero, A., Morales, J. M., and Pineda, S. A novel embedded min-max
approach for feature selection in nonlinear support vector machine classiﬁcation. European
Journal of Operational Research 293, 1 (2021), 24–35.

[31] Kaˇs´celan, V., Kaˇs´celan, L., and Novovi´c Buri´c, M. A nonparametric data mining
approach for risk prediction in car insurance: a case study from the montenegrin market.
Economic research-Ekonomska istraˇzivanja 29, 1 (2016), 545–558.

[32] Lauer, F., and Guermeur, Y. Msvmpack: a multi-class support vector machine package.

The Journal of Machine Learning Research 12 (2011), 2293–2296.

[33] Lee, Y., Lin, Y., and Wahba, G. Multicategory support vector machines: Theory and
application to the classiﬁcation of microarray data and satellite radiance data. Journal of
the American Statistical Association 99, 465 (2004), 67–81.

[34] Letham, B., Rudin, C., McCormick, T. H., and Madigan, D. Interpretable classiﬁers
using rules and bayesian analysis: Building a better stroke prediction model. The Annals
of Applied Statistics 9, 3 (2015), 1350–1371.

26

V. BLANCO, A. JAP ´ON and J. PUERTO

[35] Lewis, D. D. Naive (bayes) at forty: The independence assumption in information re-

trieval. In European conference on machine learning (1998), Springer, pp. 4–15.

[36] Majid, A., Ali, S., Iqbal, M., and Kausar, N. Prediction of human breast and colon
cancers from imbalanced data using nearest neighbor and support vector machines. Com-
puter methods and programs in biomedicine 113, 3 (2014), 792–808.

[37] Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., Leisch, F., Chang,
C., and Lin, C. Misc functions of the department of statistics, probability theory group
(formerly: E1071). Package e1071. TU Wien (2015).

[38] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research 12 (2011), 2825–
2830.

[39] Quinlan, J. Machine learning and id3. Los Altos: Morgan Kauﬀman (1996).
[40] Quinlan, R. C4. 5. Programs for machine learning (1993).
[41] Radhimeenakshi, S. Classiﬁcation and prediction of heart disease risk using data mining
techniques of support vector machine and artiﬁcial neural network. In 2016 3rd Interna-
tional Conference on Computing for Sustainable Global Development (INDIACom) (2016),
IEEE, pp. 3107–3111.

[42] Tang, X., and Xu, A. Multi-class classiﬁcation using kernel density estimation on k-

nearest neighbours. Electronics Letters 52, 8 (2016), 600–602.

[43] van den Burg, G., and Groenen, P. Gensvm: A generalized multiclass support vector

machine. Journal of Machine Learning Research 17 (2016), 1–42.

[44] Weston, J., and Watkins, C. Support vector machines for multi-class pattern recogni-

tion. In Esann (1999), vol. 99, pp. 219–224.

