1
2
0
2

y
a
M
7

]
E
S
.
s
c
[

1
v
1
3
1
3
0
.
5
0
1
2
:
v
i
X
r
a

Code2Image: Intelligent Code Analysis by Computer Vision
Techniques and Application to Vulnerability Prediction

Zeki Bilgin
Arcelik Research, Istanbul, Turkey
zeki.bilgin@arcelik.com

Abstract

Intelligent code analysis has received increasing at-
tention in parallel with the remarkable advances in
the ﬁeld of machine learning (ML) in recent years.
A major challenge in leveraging ML for this purpose
is to represent source code in a useful form that ML
algorithms can accept as input.
In this study, we
present a novel method to represent source code as
image while preserving semantic and syntactic prop-
erties, which paves the way for leveraging computer
vision techniques to use for code analysis. Indeed the
method makes it possible to directly enter the result-
ing image representation of source codes into deep
learning (DL) algorithms as input without requiring
any further data pre-processing or feature extraction
step. We demonstrate feasibility and eﬀectiveness of
our method by realizing a vulnerability prediction use
case over a public dataset containing a large number
of real-world source code samples with performance
evaluation in comparison to the state-of-art solutions.
Our implementation is publicly available1.

1

Introduction

Some exciting applications that fall under the scope
of intelligent code analysis are vulnerability predic-
tion [5, 20, 26], semantic-based code search [8], code
summarization [3, 8], code captioning [3], code classi-
ﬁcation and clone detection [30], semantic error iden-
tiﬁcation, code review and completion, synthesis, re-

1https://github.com/ArcelikHMI/Code2Image.git

pairs, documentation and more [14, 3, 18]. These
are mostly developed based on Artiﬁcial Intellifence
(AI) in general and Machine Learning (ML) or Deep
Learning (DL) in particular.

In traditional ML-based approaches, a signiﬁcant
challenge to perform intelligent code analysis is to ex-
tract useful features from source code, which mostly
requires hand-crafted feature engineering and judi-
cious data pre-processing steps. DL, which is a spe-
ciﬁc kind of ML, eliminates this diﬃculty by enabling
to learn features from data automatically [17]. Yet
there remains an issue to be overcome in order to take
advantage of DL: presenting the data in a format that
DL algorithm can accept [30].

The high-level structure of source code is naturally
text-based to facilitate the code development process
which is usually fulﬁlled by software developers fol-
lowing the underlying grammar and syntax rules of
the programming language. Software development
processes contains useful information about the re-
sulting code, and from this perspective, some studies
[25, 7, 9] investigate the usefulness of several software
metrics such as code churn, developer activity, char-
acter diversity, string entropy, and function length
for code analysis. Embracing the code naturalness
hypothesis that claims that code is a natural prod-
uct of human eﬀort and therefore should have com-
mon statistical properties with natural human lan-
guage [19], some other studies [14, 24] leverage nat-
ural language processing (NLP) techniques for code
representation and feature extraction by treating the
source code as regular text. However, treating source
code as a natural language-based text may have some

1

 
 
 
 
 
 
drawbacks in capturing comprehensive code seman-
tics because code is more structural and logical than
natural languages [30, 31]. Therefore, some works
[3, 4, 5, 8, 26, 30] seek to develop more eﬃcient code
representation methods for intelligent code analysis,
among which a promising approach is to beneﬁt from
Abstract Syntax Tree (AST) representation of source
code. Due to fact that AST is a useful mid-level rep-
resentation of source code and holds rich structural
and semantic information about the code [8], there is
a spreading interest in AST-based code analysis.

Nevertheless, it is still a challenge to transfer the
structural and semantic information hidden in AST
to the ML/DL models with minimal loss.
In this
study, we present a novel AST-based code represen-
tation method to leverage DL algorithms for intel-
ligent code analysis. The main novelty in our ap-
proach is that the code representation we generate is
in the form of image which paves the way for lever-
aging AI-based computer vision techniques to use for
intelligent code analysis. This is a very important
achievement as the DL algorithms exhibit spectacu-
lar successes especially in the ﬁeld of computer vision.
Thus, the gains achieved in the computer vision ﬁeld
with DL algorithms can be transferred to the intelli-
gent code analysis area. As the saying goes, a picture
is worth a thousand words.

To demonstrate the feasibility and eﬀectiveness of
our method, we set up a use case of vulnerability
prediction from source code on a public dataset in-
cluding a large number of function-level real-world
code samples. Detecting software vulnerabilities be-
fore they are exposed is a highly important task
to protect people and companies against malicious
attempts through the exploitation of vulnerabilities
[15]. It is more diﬃcult to detect vulnerabilities in
code than to detect bugs, because vulnerabilities are
often not realized by users or developers during the
normal operation of the system while bugs or defects
are more easily and naturally noticed [20]. More-
over, vulnerability prediction should not raise high
false alarms given that the rate of vulnerable code
In our vulnerability predic-
samples is very small.
tion use case, we make performance comparison be-
tween our method and state-of-art solutions on the
same dataset. Notice that although we demonstrate

a vulnerability prediction use case in the scope of
this study, it is possible to develop many alternative
promising use cases based on our code representation
method.

The main contributions of the paper are summa-

rized as follows:

• A novel visual code representation method that
has the potential to leverage computer vision
techniques for intelligent code analysis,

• Enabling automatic feature extraction from
source code thanks to presented image represen-
tation method,

• Converting source code’s AST into a useful form
while preserving syntactic and semantic infor-
mation, which can directly be accepted by DL
models, and

• Providing a use-case of vulnerability prediction
on real-world code samples in comparison with
state-of-art approaches.

The rest of the paper is organized as follows: In
Section 2, we overview prior studies in the areas
of code representation and vulnerability prediction.
Then we provide some background information about
AST in Section 3. We present our code representation
method in Section 4. Section 5 covers our use-case of
vulnerability prediction from source code using our
code representation method. Finally, Section 6 in-
cludes our conclusion remarks.

2 Related Work

ML/DL-based intelligent code analysis, in general,
relies on extracting useful features from source code.
Advanced DL algorithms can extract such features
automatically from code provided that the code is
in a form that is directly acceptable by the algo-
rithm, which is unfortunately not easy given the cur-
rent state of technology [12]. Some studies [25, 7, 9]
use certain software metrics such as complexity, code
churn, character diversity, string entropy, function

2

length, and developer activity to perform code anal-
ysis. Another group of studies leverage NLP tech-
niques for code representation and feature extrac-
tion. For example, the authors of [24] adapt a
method, which was initially developed for sentence
sentiment classiﬁcation, to classify “vulnerable” and
“non-vulnerable” source code components by em-
ploying deep feature representation learning over a
one-hot encoding of the tokens obtained from the
lexed source code.

From a diﬀerent perspective, the code2vec [4] uses
a neural network model to represent a code snippet as
a single ﬁxed-length code vector, by decomposing the
code into a collection of paths in its AST, called path
contexts, and then the network learns the atomic rep-
resentation of each path contexts while simultane-
ously learning how to aggregate a set of them. The
authors demonstrate the eﬀectiveness of the code2vec
by using it to predict a method’s (i.e.
function’s)
name from the vector representation of its body. Al-
though it seems that the code2vec performs well for
predicting what a code fragment does, it is shown in
[5] that it does not perform well for the task of vul-
nerability prediction. Another study beneﬁting from
AST is [30], where the authors propose an AST-based
Neural Network (ASTNN) for source code represen-
tation, which splits each large AST into a sequence
of small statement trees, and encodes the statement
trees to vectors by capturing the lexical and syntacti-
cal knowledge of statements. Based on the sequence
of statement vectors, a bidirectional RNN model is
used to leverage the naturalness of statements and
ﬁnally produce the vector representation of a code
fragment. The eﬀectiveness of the proposed code rep-
resentation is evaluated on the tasks of source code
classiﬁcation and code clone detection.

In a recent study [5] , the authors propose a source
code representation method that is capable of charac-
terizing source code into a proper format for further
processes in ML algorithms. The presented method
in [5] converts partial AST of a given source code
into a numerical array representation while preserv-
ing structural and semantic information contained in
the source code. The authors of [5] validated their
code representation approach with a use case of vul-
nerability prediction. We make performance evalua-

tion of our method in comparison with this study on
the same dataset they used.

The authors of [2] employ Gated Graph Neural
Networks (GGNN) on program graphs that track the
dependencies of the same variables and functions to
predict variable names and detect variable misuses.
Another study using the GGNN is [26], where source
code is encoded into the Code Property Graph [29]
and then vectorized to train the GGNN for vulner-
ability detection. The study [12] presents a method
to automatically convert program source-codes to vi-
sual images, which could be then utilized for auto-
mated classiﬁcation by convolutional neural networks
(CNNs). Speciﬁcally, in [12], an intermediate repre-
sentation (IR) of code is created by the LLVM com-
piler, and then redundant part of the IR code (such as
comments or initializations) are cleaned, and ﬁnally
ASCII value of the remaining characters are treated
as a pixel value in a predeﬁned empty image canvas.
This method may not capture all semantic informa-
tion in the code because the resulting image relies on
the ASCII value of characters in the IR of code.

To the best of our knowledge, this is the ﬁrst study
that proposes a method to represent AST of source
code in the form of image data that can be directly
accepted by DL algorithms.

3 Background

3.1 Abstract Syntax Tree (AST)

AST is a code representation in the form of a tree-
type data structure. It is usually created by compil-
ers or parsers through lexical and syntactic analysis of
source code, which is a kind of tokenization and pars-
ing process. Speciﬁcally, while generating AST rep-
resentation of source code, ﬁrstly the inessential ele-
ments of code such as comments, whitespaces, tabs,
and newlines are eliminated, and then the remaining
part is converted into a series of tokens, where a to-
ken is a sequence of characters that can be treated
as a unit in the grammar of the corresponding pro-
gramming language. This can be achieved by using
a lexer developed explicitly for the language of the
source code. Based on the extracted tokens, AST is

3

(a) Source code

(b) AST

Figure 1: A sample source code in C language with
its AST

generated as a tree-type graph data with hierarchi-
cal parent-child relations between the tokens. AST is
an important representation of source code as it car-
ries rich semantic and syntactic information of source
code. As an example, Figure 1 shows a sample source
code written in C language and its AST.

As seen in Figure 1, while generating AST, the
code is cleared of unnecessary information such as
comments and tabs. Some tokens in AST shown
[],
in Figure 1b are (FuncDef ), (Decl: main,
[]), (IdentiﬁerType: [’int’]), (Compound), (Constant:
int, 5), (FuncCall), (ID: printf ), (ExprList), (Bina-
ryOp: +), and (ID: a). As seen in this examples,
some tokens have no parameters, while some may
have one, two or three parameters. Since AST is
a tree-type data, there is a parent-child relationship
between tokens. For example, FuncDef is the root,

[],

and (Compound) is the parent of (Decl:a, [], [], []),
(Decl:b, [], [], []), and (FuncCall). All this informa-
tion (i.e.
token types, token contents, and parent-
child relationships of tokens) provide rich and impor-
tant information about the code.

3.2 Code Vulnerabilities

Vulnerability is deﬁned as a weakness in an infor-
mation system, system security procedures, internal
controls, or implementation that could be exploited
by a threat source [21], whereas a ﬂaw or bug is a de-
fect in a system that may (or may not) lead to a vul-
nerability [11]. Thus, vulnerabilities are the subclass
of software bugs that can be exploited for malicious
purposes [13, 20].

Detection of possible code vulnerabilities has been
traditionally tried to be done by static analysis
and/or dynamic analysis.
In static analysis, the
code is examined for weaknesses without executing
it, which does not take into account the potential
impact of the executable environment such as the op-
erating system and hardware during the analysis [1].
On the other hand, in dynamic analysis, the code is
executed to check how the software will behave in a
run-time environment, but this can only reason about
the observed execution paths and not all possible pro-
gram paths [1]. Hence, both static and dynamic code
analyses have some problems on their own. Some
tools used as source code security analyzer are given
along with their basic capabilities by the National
Institute of Standards and Technology (NIST) in the
scope of Software Assurance Metrics And Tool Eval-
uation (SAMATE) project2. Given that both static
and dynamic analysis may be ineﬀective in detecting
some vulnerabilities in certain situations [11, 22], the
SAMATE project presents a highly useful overview,
evaluations, and test results about the eﬀectiveness
of several static code analysis tools based on a pub-
lic dataset that includes real and synthetic test cases
with a set of known security ﬂaws [11].

2https://samate.nist.gov/Main Page.html

4

4 Representing Source Code as

Image

We present a novel technique to generate AST-based
color image representation of a given source code,
which can enable to leverage deep learning-based im-
age classiﬁcation and other analysis techniques for
intelligent source code analysis. The resulting AST-
based image data can be readily and eﬀectively pro-
cessed, treated, and visualized as image for any in-
tended purpose of software analysis.

4.1 Image Representation

As mentioned in the previous section, AST contains
rich information about source code in the form of to-
kens and their parent-child relations. It is important
to preserve all this information in AST while gener-
ating its image representation. To achieve this, we
exploit ﬁgural, chromatic, and spatial properties of
the image as follows:

• (ﬁgural) We plot the tokens in a particular
shape, preferred as rectangular in our implemen-
tation but it could be any other shape as well,

• (chromatic) We use colors to encode the con-
tent of AST tokens into image such that the rect-
angular token shapes are ﬁlled with speciﬁc col-
ors predetermined depending on the type and
content of the tokens, and

• (spatial) We use black lines connecting parent-
child tokens such that these lines do not cross
each other as in tree-based graphs.

To better explain this with a concrete example, we
generate an image representation of the code given
in Figure 1a by our method and visualized it in Fig-
ure 2. The image in Figure 2 represents AST of the
aforementioned code sample such that the rectangle
boxes represent the AST tokens and are colored ac-
cording to the contents of tokens. The black lines in
the image indicate the parent-child relationship be-
tween the tokens. Notice that the generated image
representation is actually in the form of RGB image
data composed of 8-bit unsigned integers specifying

the color of each pixel. Figure 2 is the visualization
of resulting image data, where the numbers on the x-
axis and y-axis are indexes of pixels. As illustrated in
this example, the original text-based AST of the code
is represented in image format ready for further pro-
cessing such as DL-based image classiﬁcation tasks.

Figure 2: AST-based visual image representation of
the code given in Figure 1a

More speciﬁcally, a digital color image contains
color information for each pixel. Typically, colors in
an RGB image are speciﬁed by a tuple of three integer
values between 0 and 255, which indicate the inten-
sity and chroma (color) of light. As mentioned in
the previous part, an AST token may have one, two,
or three content parameters. Thus, these contents
can be mapped to three color values while generating
the image representation of an AST. For example, as
shown in Figure 3, we encode the token of FuncDef
as [255, 0, 0] which corresponds to a speciﬁc shade
of Red color. Similarly, other token types can be en-
coded to a diﬀerent triplet of integers, between 0 and
255, corresponding to diﬀerent colors as exempliﬁed
in Figure 3. Notice that the colors corresponding to
AST tokens can be chosen as desired provided that
they are consistent across the entire dataset.

To generate this kind of image representation ac-
curately and automatically for any code sample, it
is necessary to overcome some technical diﬃculties
during the implementation phase as explained in the
following parts.

5

1. (Drawing) We start drawing the image from
the upper left corner of the drawing canvas (i.e.
3d array initialized with the pixel value of white
color) by adjusting the space between the rect-
angular boxes of tokens to the longest distance
possible to avoid edge intersections, and

2. (Compacting) We make the produced image
as compact as possible by removing inessential
parts such as all-white columns & rows of pixels
and recurring rows of pixels.

We explain these steps in detail in the following

subsections.

4.2.1 Drawing

To determine the upper limit of the number of edges
passing through between the token boxes in the image
representation, we performed some statistical analy-
sis on a large number of real-world code samples. For
this purpose, we beneﬁted from the public Draper
VDISC Dataset3 [24] that consists of more than 1 mil-
lion function-level C and C++ code samples mined
from real-world projects such as Debian Linux distri-
bution [10], public Git repositories on GitHub [16],
and SATE IV Juliet Test Suite [6].

In our statistical analyses, we examined approxi-
mately 50000 function-level code samples written in
C language from the Draper dataset. We ﬁrst ex-
tracted ASTs of those code samples by using the
Pycparser[23], which is a Python-based parser for the
C language (C99), and explored this ASTs’ structure.
We noted both AST depth (i.e. level of the deepest
node) and the number of nodes (i.e. tokens) at each
level of ASTs to reveal the expected size of ASTs.
Figure 4 shows the statistical distribution of these
features for the code samples we investigated. Ac-
cording to Figure 4a, the most frequent AST depths
encountered are 8, 9, 10, 11, 7, 12, 13, and so on from
most frequent to least. On the other hand, Figure
4b shows the statistical distribution of the maximum
number of nodes seen at an AST level, which indi-
cates that the highest number of nodes at AST levels
is 11 for most code samples and gradually decreases

Figure 3: Some examples for encoding and color map-
ping of AST tokens

4.2 Image Size

Trees are planar graphs that can be embedded in the
plane. In other words, a tree can be drawn on the
plane in such a way that its edges intersect only at
their endpoints [28], which requires us to draw AST in
our image representation in such a way that no edges
cross each other.
It is a challenge to achieve this
while automatically drawing ASTs of a large number
of code samples in a bounded drawing space because
ASTs may have arbitrary depth length and number of
nodes. Put it another way, we draw AST by coloring
associated pixels in an image canvas, and even when
we set the line (i.e. edge) thickness as the minimum
value of 1 pixel, the number of edges that can pass
rectangular color
through between the nodes (i.e.
boxes in our case) without overlapping is bounded
by the space between nodes. To overcome this is-
sue, we need to leave enough length of space between
rectangular token boxes in both horizontal and verti-
cal directions while generating the image representa-
tion of source code. But, what would be the optimal
length of such a blank space? If the space becomes
unnecessarily long, the resulting image sizes would
be needlessly big, increasing space and processing re-
quirements. On the other hand, if this space becomes
not long enough, the black connecting lines may in-
tersect in the resulting image representation for some
source code samples.

We develop a 2-step process to overcome this issue

as follows:

3https://osf.io/d45bw/

6

(a)

(b)

Figure 4: Statistical distributions of some AST prop-
erties

for other values. The maximum highest node number
seen is 180 in just one sample. Taking into account
these statistical values, we optimize our implementa-
tion by leaving enough space both horizontally be-
tween nodes at the same level and vertically between
subsequent levels.

Figure 5: Illustration of the impact of the compact-
ing step. The image representation given in Figure 2
would look like this if the compacting step was not
applied.

of pixels that sequentially repeats themselves, which
would unnecessarily lengthen the height of the image.
Also, there may occur all-white (i.e. blank) columns
and rows of pixels on the right and bottom side of
the image, which would unnecessarily increase the
image size. The resulting image can be compacted
by deleting this kind of inessential parts. Figure 5 il-
lustrates the impact of compacting step by depicting
the state of the image representation given in Figure
2 when the compacting step is not applied. When
we compare Figure 2 and Figure 5, it is seen that
the compacting step brings the resulting image rep-
resentation to the smallest possible size. Notice that
size of the rectangular token boxes and thickness of
the edges in both drawings are the same in fact but
appear large in one and small in the other due to the
scale diﬀerence as seen on the numbers on x-axis and
y-axis.

Our implementation is summarized in the pseu-

docode given in Algorithm 1.

4.2.2 Compacting

4.2.3 Image Format

When the blank space left between the rectangular
token boxes is more than necessary, inessential rows
and columns of pixels may occur in the resulting im-
age representation. Speciﬁcally, there may be rows

The generated image representation is actually in the
form of 3-dimensional array, where each dimension
corresponds to 1 color channel as explained previ-
ously. This data can be converted to any intended

7

Algorithm 1 Generating Image Representation

Input: AST
Output: Image Representation

(cid:46) AST of source code

1: procedure Encode(node)

(cid:46) specify colour based on

token

colour ← node
return colour

2:
3:
4: end procedure
5: procedure Drawing(root,x,y,3)

dimension

(cid:46) specify image

6:

image ← array[x][y] = [255, 255, 255]

(cid:46) initialize:

7:
8:
9:
10:
11:
12:
13:

14:
15:
16:
17:
18:
19:

20:
21:

all pixels are white
queue ← root
level ← 0
while queue (cid:54)= ∅ do

box

node ← queue.pop(0)
level ← node.level
index ← placeholder
if currentlevel == level then

(cid:46) draw token

image[currentlevel][index] ← encode(node)

else

level ← currentlevel
image[currentlevel][index] ← encode(node)

end if
if currentlevel (cid:54)= 0 then

child to parent

(cid:46) draw line from

for each (indexi, indexj ) ∈ line do

image[indexi][indexj ] ← 0

(cid:46) colour

black

end for

end while

end if
queue ← nextnode

22:
23:
24:
25:
26: end procedure
27: procedure Compact(image)
28:
29:
30: end procedure

image ← image.delete(subsequently repeating rows)
image ← image.delete(allwhite columns&rows)

image format such as jpeg, eps, png. Either raw form
or converted form can then be directly used for fur-
ther processes in machine learning applications.

5 Vulnerability Prediction

We claim that our method of generating the image
representation of source code enables us to leverage
AI-based computer vision techniques for intelligent
code analysis. To validate this, we apply our code
representation method for the use case of vulnera-
bility prediction from source code and make perfor-

8

mance evaluation in comparison with state-of-art so-
lutions.

5.1 DataSet

The public Draper VDISC Dataset [24] provides a
large number of useful function-level real-world code
samples that are labeled according to whether they
contain any of the certain vulnerabilities based on
the examinations conducted by several static code
analyzers. The authors of [24] also split the whole
dataset into 3 disjoint subsets as training, validation,
and test sets with the percentage of 80, 10, and 10 re-
spectively. From this dataset, we extracted the code
samples parseable by the Pycparser[23], a Python-
based parser for the C language (C99).

Table 1 shows the details of the 5 vulnerability cat-
egories considered in this study. The ﬁrst, third, and
fourth categories contain only one type of vulnera-
bility, which is CWE-119, CWE-469, and CWE-476
respectively. The second category includes three vari-
ants of buﬀer overﬂow, which are CWE-120 (Clas-
sic Buﬀer Overﬂow), CWE-121 (Stack-based Buﬀer
Overﬂow) and CWE-122 (Heap-based Buﬀer Over-
ﬂow). The ﬁfth category includes a few variants
of improper input validation such as CWE-20 (Im-
proper Input Validation), CWE-457 (Use of Unini-
tialized Variable), and CWE-805 (Buﬀer Access with
Incorrect Length Value). These are serious vulnera-
bilities that can cause irreparable damage to system,
businesses, or users, and therefore it is crucial to de-
tect them as early as possible.

5.2 Experimental Setup

To realize the vulnerability prediction use case, we
ﬁrst generated the image representation of the code
samples that we extracted from the Draper dataset.
As explained earlier, we followed a two-step process
for this purpose, i.e. (i) drawing and (ii) compacting.
In the drawing step, to avoid the intersection of black
lines connecting parent and child nodes in AST, we
left long enough space horizontally and vertically be-
tween rectangular token boxes. We found the optimal
length of these spaces for the whole dataset based on
our statistical analysis of the ASTs extracted from a

Table 1: Types and frequencies of the vulnerabilities investigated in the experimental work

CWE ID
119
120/121/122
469
476
20,457,805 etc

CWE Description
Improper Restriction of Operators within the Bounds of a Memory Buﬀer
Buﬀer Overﬂow
Use of Pointer Subtraction to Determine Size
NULL Pointer Dereference
Improper Input Validation, Use of Uninitialized Variable, Buﬀer Access with Incorrect Length Value etc.

Frequency (%)
21.34
40.69
2.57
9.22
26.18

large number of real-world code samples as explained
earlier in Section 4.2.1. In the compacting step, we
removed inessential columns and rows in the result-
ing image, and thus compacted them. The Appendix
includes more code samples along with their AST and
resulting image representations.

5.2.1 Problem Modeling

We model the vulnerability prediction task as a bi-
nary image classiﬁcation problem such that we train
our DL-based model with the training data includ-
ing code samples that are labeled either as vulnera-
ble or non-vulnerable. The model learns how to dis-
tinguish vulnerable and non-vulnerable code samples
from training data and then we make performance
evaluation on test dataset including both vulnerable
and non-vulnerable code samples. We train a sepa-
rate DL model for each of the 5 diﬀerent vulnerability
types, the details of which are given in Table 1.

5.2.2 Model Architecture

As highlighted earlier, DL algorithms demonstrate
spectacular performance in image classiﬁcation,
which is mostly done by using convolutional neural
networks (CNNs). We can leverage this for vulner-
ability prediction in source code by using our code
representation method which converts a given code
fragment into image data. However, it is known that
traditional CNN models accept only images of the
same size due to fully connected (dense) layers, which
is a requirement that is not satisﬁed in our case. As
discussed in Section 4.2, the ASTs of code samples
collected from real-world projects have varying di-
mensions, and as a result of this, our method gen-
erates images in varying dimensions. To overcome

this challenge, we initially tried to use a fully con-
volutional neural network (FCN) that does not con-
tain any dense layer, and thus accepts varying input
sizes. However, we observed that the classiﬁcation
performance of FCN was not satisfactory. Then we
modiﬁed our convolutional network to include a dense
layer with the objective of achieving better classiﬁca-
tion performance and found another way to solve the
issue of varying input size as explained below.

In the training process of a neural network, all sam-
ples in the training dataset are not processed at once.
Instead, they are processed in batches containing a
certain number of samples to reduce memory require-
ments. To accept images in varying size while there
is a fully connected layer in our network, we perform
batch-based size equalization such that we ﬁrst ﬁnd
the max height and width of images in a batch and
pad every other image with a pixel value of 255 (i.e.
white color in our case) in the right and down direc-
tions so that every image in the batch has an equal di-
mension. The padded white pixels in the image have
no impact on code semantic because they are iden-
tical with the white background of the image. The
model can extract features from the intended portion
from the padded image. Thus we have a batch with
equal image dimensions but every batch has a diﬀer-
ent shape (due to diﬀerence in max height and width
of images across batches) [27].

In our convolutional network, we specify input
shape as (None, None, 3) since the dimensions of our
input images are variable. The 3 is for the number of
channels in our image which is ﬁxed for colored im-
ages (RGB). As illustrated in Figure 6, there are two
convolution blocks in our network, each of which con-
sists of a 2D convolution layer (Conv2D) with 32 ﬁl-
ters and kernel size of 3 to extract features from input
image and an activation layer (Relu) to incorporate

9

Table 2: Distribution of positive and negative samples in the original and oversampled cases

Class

CWE119

CWE120

CWE469

CWE476

CWEOther

original
oversampled
original
oversampled
original
oversampled
original
oversampled
original
oversampled

Training (# of samples:46093)
Vulnerable
2684 (5.82%)
14470 (25.00%)
5119 (11.10%)
13658 (25.00%)
323 (0.70%)
15257 (25.00%)
1160 (2.52%)
14978 (25.00%)
3294 (7.15%)
14266 (25.00%)

Non-Vulnerable
43409 (94.18%)
43409 (75.00%)
40974 (88.89%)
40974 (75.00%)
45770 (99.30%)
45770 (75.00%)
44933 (97.48%)
44933 (75.00%)
42799 (92.85%)
42799 (75.00%)

Validation(# of samples:5736)
Non-Vulnerable
Vulnerable
5401 (94.16%)
335 (5.84%)
-
-
5095 (88.82%)
641 (11.18%)
-
-
5700 (99.37%)
36 (0.63%)
-
-
5590 (97.45%)
146 (2.55%)
-
-
5317 (92.70%)
419 (7.30%)
-
-

Test(# of samples:5765)

Vulnerable
355 (6.16%)
-
684 (11.86%)
-
32 (0.56%)
-
140 (2.43%)
-
399 (6.92%)
-

Non-Vulnerable
5410 (93.84%)
-
5081 (88.14%)
-
5733 (99.44%)
-
5625 (97.57%)
-
5366 (93.08%)
-

non-linearity. To avoid overﬁtting we also added reg-
ularization layer (Dropout and BatchNormalization)
after Conv2D. There is a max pool layer after each
convolution block, and then a full connected layer
(size of 32) with an activation layer to perform the
classiﬁcation process. Finally, there is a softmax layer
to calculate class probabilities for the corresponding
input.

5.2.3 Dealing With Data Imbalance

The Draper VDISC Dataset is highly imbalanced as
the number of positive (i.e. vulnerable) samples is
far less than the number of negative (non-vulnerable)
samples due to fact that they are collected from real-
world projects and thus reﬂect the natural distribu-
tion of the targeted vulnerabilities. It is inherently

Figure 6: Architecture of our convolutional neural
network

challenging to deal with classiﬁcation problems on
highly imbalanced sets because they require to ac-
curately detect positive test samples constituting a
very small portion of the whole test samples with-
out raising false alarms for negative samples. When
an ML/DL algorithm is trained with such an imbal-
anced dataset, the impact of minority class over the
trained model is damped in general due to the dom-
inance of the majority class. As a result of this, the
model can not learn enough from minority class sam-
ples, which degrades the classiﬁcation performance in
terms of P recision, Recall, and other metrics. As a
remedy for this, we apply the oversampling technique
to increase the number of vulnerable code samples
only in the training dataset while not touching the
test dataset. To do this, we create multiple copies
of vulnerable code samples to soften the imbalance
rate between classes.
In this way, we adjusted the
ratio of vulnerable code samples to 25% in the train-
ing dataset for all vulnerability types. Table 2 shows
the number of vulnerable and non-vulnerable code
samples before and after oversampling process.

5.3 Experimental Results

We trained a separate DL model for each vulnera-
bility type (i.e.
totally 5 models) on the oversam-
pled training data to perform binary classiﬁcation,
and made performance evaluation on the test dataset
given in Table 2. We observed the performance of
our model for each epoch during the training process
and saved the model state whenever a lower valida-

10

tion loss is obtained. We used the model state with
minimum validation loss for performance evaluation
on the test dataset. Also, we compared the perfor-
mance of our vulnerability prediction approach with
the Ast2vec [5] on the same dataset. The Ast2vec is
a state-of-art code representation and vulnerability
prediction method as shortly explained in Section 2.
We preferred to use F1, MMC, and AUC as per-
formance metrics because our dataset is highly im-
balanced. Notice that our classiﬁcation models are
naturally probabilistic, which means they actually re-
turn probabilities indicating the likelihood of a sam-
ple belonging to each class label (i.e. vulnerable or
non-vulnerable). These probabilities are converted to
class labels based on decision threshold, i.e., a value
above that threshold indicates “vulnerable”; a value
below indicates “non-vulnerable”. A model can be
operated at the optimum point (usually where the F1
score is maximum) by adjusting the decision thresh-
old. The precision-recall curves below were obtained
in this way by varying decision threshold and observ-
ing model performance in terms of precision and re-
call.

CWE-119:

Figure 7a shows precision-recall
curves of both our Code2image method and the
Ast2vec for vulnerability type of CWE-119, where
it obviously seems that the Code2image outperforms
the Ast2vec.
Indeed the Code2image has higher
F1, MMC, and AUC scores than the Ast2vec as
indicated in Table 3, where F1, MMC, and AUC
scores are 0.668, 0.648, and 0.554 respectively for the
Code2image, and 0.527, 0.496, and 0.424 respectively
for the Ast2vec. The Code2image reaches its maxi-
mum F1 score of 0.668 at precision and recall values
of 0.614 and 0.734 respectively.

CWE-120: Figure 7b depicts precision-recall
curves of our Code2image method and the Ast2vec
for the vulnerability group of CWE-120. As clearly
seen in Figure 7b, the Code2image outperforms
the Ast2vec by reaching higher F1, MMC, and
AUC scores. Table 3 provides exact values of this
metrics as 0.528, 0.463, 0.477 respectively for the
Code2image, and as 0.424, 0.357, and 0.385 respec-
tively for the Ast2vec. The Code2image reaches its
maximum F1 score of 0.528 when precision is 0.509
and recall is 0.549, while the Ast2vec reaches its max-

imum F1 score of 0.424 when precision is 0.373 and
recall is 0.493.

CWE-469: Figure 7c presents precision-recall
curves for both compared methods. As can be in-
ferred from Figure 7c, predicting this vulnerability
type is the most challenging one with respect to other
categories because this category has the highest im-
balance ratio on the test dataset. Indeed, it is seen
in Table 2 that there are only 32 positive samples in
the test dataset, making up approximately 0.56% of
the whole test group. Still, our method shows bet-
ter performance than the Ast2vec, with F1, MMC,
and AUC values of 0.140, 0.166, and 0.054 respec-
tively, whereas the same metrics for the Ast2vec are
0.082, 0.115, and 0.031 respectively, as given in Table
3. The Code2image achieves its maximum F1 value
of 0.140 when precision is 0.273 and recall is 0.094.
The Ast2vec achieves its maximum F1 value of 0.082
when precision is 0.118 and recall is 0.063.

CWE-476: Figure 7d depicts precision-recall
curves for both methods.
It seems from Figure 7d
that both methods performed similarly for this vul-
nerability type, and it is not clear whether one out-
performs the other. According to Table 3, there are
slight diﬀerences in the F1, MMC and AUC scores of
both methods such that they are 0.617, 0.613 and
0.524 respectively for the Code2image, and 0.599,
0.622, and 0.535 respectively for the Ast2vec. The
Code2image achieves its maximum F1 value of 0.617
when precision is 0.747 and recall is 0.525. The
Ast2vec achieves its maximum F1 value of 0.599 when
precision is 0.833 and recall is 0.468.

CWE-others: Figure 7e exhibits precision-recall
curves of both compared methods for last vulnera-
bility category.
It is obvious from Figure 7e that
our Code2image method is superior to the Ast2vec.
This is also seen in Table 3, where the Code2image
has F1, MMC, and AUC values of 0.335, 0.282, and
0.277 respectively, while the Ast2vec has 0.283, 0.224,
and 0.186 for the same metrics respectively. The
Code2image reaches its maximum F1 score of 0.335
when precision is 0.301 and recall is 0.379. The
Ast2vec reaches its maximum F1 score of 0.283 when
precision is 0.245 and recall is 0.335.

Based on the F1 scores given in Table 3, the
Code2image showed its best performance for CWE-

11

(a) CWE-119

(b) CWE-120

(c) CWE-469

(d) CWE-476

(e) CWE-others

Figure 7: Performance comparison of the presented method with the Ast2vec [5]

Table 3: Performance comparison of the presented method with the Ast2vec

Method

Code2Image
Ast2Vec [5]

Vulnerability

CWE-119

CWE-120

CWE-469

CWE-476

CWE-others

F1 MCC AUC
0.554
0.648
0.424
0.496

0.668
0.527

F1 MCC AUC
0.477
0.463
0.385
0.357

0.528
0.424

F1 MCC AUC
0.054
0.166
0.031
0.115

0.140
0.082

F1 MCC AUC
0.524
0.613
0.535
0.622

0.617
0.599

F1 MCC AUC
0.277
0.282
0.186
0.224

0.335
0.283

119, whereas the Ast2vec showed its best perfor-
mance for CWE-476. Both methods performed worst
for CWE-469 with respect to other vulnerability cat-
egories, which is probably due to fact that CWE-469
has the highest imbalance ratio that makes the clas-
siﬁcation task very challenging.

The Code2image shows better performance than
the Ast2vec in general according to overall experi-
mental results. One reason for this could be that the
Ast2vec uses partial AST (not whole AST) for code
representation due to some scalability issues, which
causes information loss, while the Code2image gen-

12

erates code representation based on whole AST. An-
other reason is that we build a separate model for
each vulnerability type in our method such that mod-
els are optimized to minimize validation loss, whereas
a common model is built for all vulnerability cate-
gories in the Ast2vec.

5.3.1

Inference Time

We trained 5 diﬀerent models, each of which was ded-
icated to a particular vulnerability type, up to the
epoch number with the minimal validation loss as
explained earlier. The models reached the minimal
validation loss in diﬀerent epochs (e.g.
in epoch no
48 or 27), which resulted in diﬀerent training time
for each model. But, the epoch-based training time
was identical for all of them, which was observed as
around 6 minutes per epoch on a machine with Nvidia
GP106GL [Quadro P2000] GPU. On the other hand,
we measured inference time by executing our trained
models on the test dataset containing 5765 code sam-
ples, and found the average inference time per sample
as 20.8 milliseconds.

6 Conclusion

We present a code representation method that con-
verts AST of code to image form which can directly
be entered into convolution neural networks. Thus
it enables automatic feature extraction from source
code by using deep learning algorithms. While gen-
erating such an image representation of code, syntac-
tic and semantic information hidden in code is pre-
served thanks to AST. We implemented our method
so that the size of the resulting image is optimized
(i.e. compacted) as much as possible while retain-
ing tree graph properties in visualization. For exam-
ple, the lines connecting parent-child nodes should be
drawn so that they do not intersect with each other
except their common vertex if any. For validation, we
realized a vulnerability prediction use case, and ex-
perimentally showed the superiority of our approach
to state-of-art solutions.

References

[1] Tamas Abraham and Olivier de Vel. A review
of machine learning in software vulnerability re-
search. 2017.

[2] Miltiadis Allamanis, Marc Brockschmidt, and
Mahmoud Khademi. Learning to represent pro-
In Proceedings of the 7th
grams with graphs.
International Conference on Learning Represen-
tations, 2018.

[3] Uri Alon, Shaked Brody, Omer Levy, and
code2seq: Generating sequences
Eran Yahav.
from structured representations of code. arXiv
preprint arXiv:1808.01400, 2018.

[4] Uri Alon, Meital Zilberstein, Omer Levy, and
Eran Yahav. Code2vec: Learning distributed
representations of code. Proc. ACM Program.
Lang., 3(POPL), January 2019.

[5] Z. Bilgin, M. A. Ersoy, E. U. Soykan, E. Tomur,
P. C¸ omak, and L. Kara¸cay. Vulnerability pre-
diction from source code using machine learning.
IEEE Access, 8:150672–150684, 2020.

[6] Paul E Black and Paul E Black. Juliet 1.3 Test
Suite: Changes From 1.2. US Department of
Commerce, National Institute of Standards and
Technology, 2018.

[7] Amiangshu Bosu, Jeﬀrey C. Carver, Munawar
Iden-
Haﬁz, Patrick Hilley, and Derek Janni.
tifying the characteristics of vulnerable code
changes: An empirical study. In Proceedings of
the 22nd ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering,
FSE 2014, page 257–268, New York, NY, USA,
2014. Association for Computing Machinery.

[8] Long Chen, Wei Ye, and Shikun Zhang. Cap-
turing source code semantics via tree-based con-
volution over api-enhanced ast. In Proceedings
of the 16th ACM International Conference on
Computing Frontiers, pages 174–182, 2019.

13

[9] Boris Chernis and Rakesh Verma. Machine
learning methods for software vulnerability de-
tection. In Proceedings of the Fourth ACM In-
ternational Workshop on Security and Privacy
Analytics, pages 31–39, 2018.

[19] Abram Hindle, Earl T Barr, Mark Gabel, Zhen-
dong Su, and Premkumar Devanbu. On the
naturalness of software. Communications of the
ACM, 59(5):122–131, 2016.

[20] Matthieu Jimenez. Evaluating Vulnerability Pre-

[10] Debian. Debian - the universal operating system.

diction Models. PhD thesis, 10 2018.

https://www.debian.org/.

[11] A. M. Delaitre, B. C. Stivalet, P. E. Black,
V. Okun, T. S. Cohen, and A. Ribeiro. Sate
V report: Ten years of static analysis tool expo-
sitions. Technical report, NIST Special Publica-
tion - 500-326, 2018.

[12] Somdip Dey, Amit Kumar Singh, Dilip Ku-
mar Prasad, and Klaus Dieter Mcdonald-Maier.
Socodecnn: Program source code for visual cnn
classiﬁcation using computer vision methodol-
ogy. IEEE Access, 7:157158–157172, 2019.

[13] Mark Dowd, John McDonald, and Justin Schuh.
The art of software security assessment: Iden-
tifying and preventing software vulnerabilities.
Addison Wesley Professional, 2006.

[14] V. Efstathiou and D. Spinellis. Semantic source
code models using identiﬁer embeddings. In 2019
IEEE/ACM 16th International Conference on
Mining Software Repositories (MSR), pages 29–
33, May 2019.

[15] Tiago Espinha Gasiba, Ulrike Lechner, Maria
Pinto-Albuquerque, and Daniel Mendez Fernan-
dez. Awareness of secure coding guidelines in the
industry - a ﬁrst data analysis. In 2020 IEEE
19th International Conference on Trust, Secu-
rity and Privacy in Computing and Communi-
cations (TrustCom), pages 345–352, 2020.

[16] Github. Github. https://github.com/.

[17] Ian Goodfellow, Yoshua Bengio, and Aaron
Courville. Deep Learning. MIT Press, 2016.
http://www.deeplearningbook.org.

[18] Anshul Gupta and Neel Sundaresan. Intelligent

code reviews using deep learning. 2018.

[21] Joint Task Force Transformation Initiative. In-
formation security. Technical report, NIST Spe-
cial Publication 800-30, 2012.

[22] Bruce McCorkendale, Xue Feng Tian, Sheng
Gong, Xiaole Zhu, Jun Mao, Qingchun Meng,
Ge Hua Huang, and Wei Guo Eric Hu. Sys-
tems and methods for combining static and dy-
namic code analysis, May 13 2014. US Patent
8,726,392.

[23] Pycparser. Parser for the C language. https:

//github.com/eliben/pycparser.

[24] R. Russell, L. Kim, L. Hamilton, T. Lazovich,
J. Harer, O. Ozdemir, P. Ellingwood, and
M. McConley. Automated vulnerability detec-
tion in source code using deep representation
learning. In 2018 17th IEEE International Con-
ference on Machine Learning and Applications
(ICMLA), pages 757–762, Dec 2018.

[25] Y. Shin, A. Meneely, L. Williams, and J. A. Os-
borne. Evaluating complexity, code churn, and
developer activity metrics as indicators of soft-
ware vulnerabilities. IEEE Transactions on Soft-
ware Engineering, 37(6):772–787, 2011.

[26] Sahil Suneja, Yunhui Zheng, Yufan Zhuang,
Jim Laredo, and Alessandro Morari. Learn-
ing to map source code to software vulnera-
bility using code-as-a-graph.
arXiv preprint
arXiv:2006.08614, 2020.

[27] Towardsdatascience. Understanding and im-
plementing a fully convolutional network (fcn).
https://towardsdatascience.com/impleme
nting-a-fully-convolutional-network-fc
n-in-tensorflow-2-3c46fb61de3b.

14

[28] Richard J Trudeau. Introduction to graph theory.

Courier Corporation, 2013.

[29] Fabian Yamaguchi, Nico Golde, Daniel Arp,
and Konrad Rieck. Modeling and discovering
vulnerabilities with code property graphs.
In
2014 IEEE Symposium on Security and Privacy,
pages 590–604. IEEE, 2014.

[30] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong
Sun, Kaixuan Wang, and Xudong Liu. A novel
neural source code representation based on ab-
stract syntax tree. In 2019 IEEE/ACM 41st In-
ternational Conference on Software Engineering
(ICSE), pages 783–794. IEEE, 2019.

[31] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xi-
aoning Du, and Yang Liu. Devign: Eﬀective
vulnerability identiﬁcation by learning compre-
hensive program semantics via graph neural net-
works. In Advances in Neural Information Pro-
cessing Systems 32, pages 10197–10207. Curran
Associates, Inc., 2019.

A Additional examples

In the following pages, we provide two more examples
for source code, AST and corresponding visualised
image representation generated by our code repre-
sentation method.

15

Figure 8: A real-world code sample from the Draper VDISC Dataset with its AST and corresponding visual
image representation generated by our method.

16

Figure 9: A real-world code sample from the Draper VDISC Dataset with its AST and corresponding visual
image representation generated by our method.

17

