1

Learning Semantic Program Embeddings with Graph Interval
Neural Network

YU WANG, Nanjing University
FENGJUAN GAO, Nanjing University
LINZHANG WANG, Nanjing University
KE WANG, Visa Research.

Learning distributed representations of source code has been a challenging task for machine learning models.
Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately,
such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph
Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations.
Due to the homogeneous (i.e. do not take advantage of the program-specific graph characteristics) and
expensive (i.e. require heavy information exchange among nodes in the graph) message-passing procedure,
GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs.
In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN),
to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated
graph representation obtained through an abstraction method designed to aid models to learn. In particular,
GINN focuses exclusively on intervals (generally manifested in looping construct) for mining the feature
representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning
to large graphs.

We evaluate GINN for two popular downstream applications: variable misuse prediction and method name
prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin.
We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code.
While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly
outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based
bug detector and Facebook Infer, arguably the state-of-the-art static analysis tool, to scan the codebase of 20
highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings
raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer. We have
reported 38 bugs GINN caught to developers, among which 11 have been fixed and 12 have been confirmed
(fix pending). GINN has shown to be a general, powerful deep neural network for learning precise, semantic
program embeddings.

0
2
0
2

y
a
M
7
2

]
E
S
.
s
c
[

2
v
7
9
9
9
0
.
5
0
0
2
:
v
i
X
r
a

1 INTRODUCTION
Learning distributed representations of source code has attracted growing interest and atten-
tion in program language research over the past several years. Inspired by the seminal work
word2vec (Mikolov et al. 2013), many methods have been proposed to produce vectorial representa-
tion of programs. Such vectors, commonly known as program embeddings, capture the semantics
of a program through their numerical components such that programs denoting similar semantics
will be located in close proximity to one another in the vector space. A benefit of learning program

2020. XXXX-XXXX/2020/1-ART1 $15.00
DOI:

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

 
 
 
 
 
 
1:2

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

embedding is to enable the application of neural technology to a board range of programming
language (PL) tasks, which were exclusively approached by logic-based, symbolic methods before.
Existing Models of Source Code. Earlier works predominately considered source code as text,
and mechanically transferred natural language methods to discover their shallow, textual pat-
terns (Gupta et al. 2017; Hindle et al. 2012; Pu et al. 2016). Following approaches aim to learn
semantic program embeddings from Abstract Syntax Tree (AST) (Alon et al. 2019a,b; Maddison and
Tarlow 2014). Despite the significant progress, tree-based models are still confined to learning syn-
tactic program features due to the limited offering from AST. Recently, another neural architecture,
called Graph Neural Network (GNN), has been introduced to the programming domain (Allamanis
et al. 2018; Li et al. 2016). The idea is to reduce the problem of learning embeddings from source
code to learning from their graph representations. Powered by a general, effective message-passing
procedure (Gilmer et al. 2017), GNN has achieved state-of-the-art results in a variety of problem
domains: variable misuse prediction (Allamanis et al. 2018), program summarization (Fernandes
et al. 2019), and bug detection and fixing (Dinella et al. 2020). Even though GNN has substantially
improved the prior works, their precision and efficiency issues remain to be dealt with. First, GNN
is indiscriminate in which types of graph data they deal with. In other words, once programs are
converted into graphs, they will be processed in the same manner as any other graph data (e.g.
social networks, molecular structures). As a result, it misses out on the opportunity to capitalize on
the unique graph characteristics possessed by programs. Second, perhaps more severely, GNN has
difficulties in learning from large graphs due to the high cost of its underlying message-passing
procedure. Ideally, every node should pass messages directly or indirectly to every other node in
the graph to allow sufficient information exchange. However, such an expensive propagation is
hard to scale to large graphs without incurring a significant precision loss.
Graph Interval Neural Network. In this paper, we present a novel, general neural architecture
called Graph Interval Neural Network (GINN) for learning semantic embeddings of source code.
The design of GINN is based on a key insight that by learning from abstractions of programs, models
can focus on code constructs of greater importance, and ultimately capture the essence of program
semantics in a precise manner. At the technical level, we adopt GNN’s learning framework for
their cutting-edge performance. An important challenge we need to overcome is how to abstract
programs into more efficient graph representations for GINN to learn. To this end, we develop
a principled abstraction method directly applied on control flow graphs. In particular, we derive
from the control flow graph a hierarchy of intervals — subgraphs that generally represent looping
constructs — as a new form of program graphs. Under this abstracted graph representation, GINN
not only focuses exclusively on intervals for extracting deep, semantic program features but also
scales its generalization to large graphs efficiently. Note that GINN’s scalability-enhancing approach
is fundamentally different from the existing techniques (Allamanis et al. 2018), where extra edges
are required to link nodes that are far apart in a graph. Moreover, Allamanis et al. (2018) have not
quantified the improvement of model scalability due to the added edges, let alone identify which
edges to add w.r.t. the different graphs and downstream tasks.

We have realized GINN as a new model of source code and extensively evaluated it. To demon-
strate its generality, we evaluate GINN in multiple downstream tasks. First, we pick two prediction
problems defined by the prior works (Allamanis et al. 2018; Alon et al. 2019b) in which GINN signif-
icantly outperforms RNN Sandwich and sequence GNN, the state-of-the-art models in predicting
variable misuses and method names respectively. Second, we evaluate GINN in detecting null
pointer dereference bugs in Java code, a task that is barely attempted by learning-based approaches.
Results show GINN-based bug detector is significantly more effective than the baseline built upon
GNN. We also deploy our trained bug detector and Facebook Infer (Berdine et al. 2006; Calcagno

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:3

et al. 2015), arguably the state-of-the-art static analysis tool, to search bugs in 20 Java projects that
are among the most starred on GitHub. After manually inspecting the alarms raised by both tools,
we find that our bug detector yields a higher precision (38 bugs out of 102 alarms) than Infer (34
bugs out of 129 alarms). We have reported the 38 bugs detected by GINN to developers, out of
which 11 are fixed and 12 are confirmed (fix pending). Our evaluation suggests GINN is a general,
powerful deep neural network for learning precise, semantic program embeddings.
Contributions. Our main contributions are:

• We propose a novel, general deep neural architecture, Graph Interval Neural Network,
which generalizes from a curated graph representation, obtained through a graph abstrac-
tion method on the control-flow graph.

• We realize GINN as a new deep model of code, which readily serves multiple downstream
PL tasks. More importantly, GINN outperforms the state-of-the-art model by a comfortable
margin in each downstream task.

• We present the details of our extensive evaluation of GINN in variable misuse prediction,

method name prediction, and null pointer dereference detection.

• We publish our code and data at https://figshare.com/articles/datasets tar gz/8796677 to

aid future research activity.

2 PRELIMINARY
In this section, we briefly revisit connected, directed graphs, interval (Allen 1970) and GNN (Gori
et al. 2005), which lay the foundation for our work.

2.1 Graph
A graph G = (V , E) consists of a set of nodes V = {v1, ..., vn }, and a list of directed edge sets
E = (E1, ..., EK ) where K is the total number of edge types and Ek (1 ≤ k ≤ K) is a set of edges
of type k. We denote by (vs , vd , k) ∈ Ek an edge of type k directed from node vs to node vd . For
graphs with only one edge type, an edge is represented as (vs , vd ).

The immediate successors of a node vi , denoted by post (vi ), are all of the nodes vj for which
(vi , vj ) is an edge in one edge set in E. The immediate predecessors of node vj , denoted by pre (vj ),
are all of the nodes vi for which (vi , vj ) is an edge in one edge set in E.

A path is an ordered sequence of nodes (vp, ..., vq) and their connecting edges, in which each
node vt , t ∈ (p, . . . , q − 1), is an immediate predecessor of vt +1. A closed path is a path in which
the first and last nodes are the same. The successors of a node vt , denoted by post ∗(vt ), are all of
the nodes vx for which there exists a path from vt to vx . The predecessors of a node vt , denoted by
pre ∗(vt ), are all of the nodes vy for which there exists a path from vy to vt .

2.2 Interval
Introduced by Allen (1970), an interval I(h) is the maximal, single entry subgraph in which h
is the only entry node and all closed paths contain h. The unique interval node h is called the
interval head or simply the header node. An interval can be expressed in terms of the nodes in it:
I (h) = {vl , v2, ..., vm }.

By selecting the proper set of header nodes, a graph can be partitioned into a set of disjoint
intervals. We show the partition procedure proposed by Allen (1970) in Algorithm 1. The key is to
add to an interval a node only if all of its immediate predecessors are already in the interval (Line 8
to 9). The intuition is such nodes when added to an interval keep the original header node as the
single entry of an interval. To find a header node to form another interval, a node is picked that is
not a member of any existing intervals although it must have a (but not all) immediate predecessor

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:4

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

being a member of the interval that is just computed (Line 11 to 12). We repeat the process until
reaching the fixed-point where all nodes are members of an interval.

The intervals on the original graph are called the first order intervals, denoted by I 1(h), and
the graph from which they were derived is called first order graph G1. Partitioning the first order
graph results in a set of first order intervals, denoted by S1 s.t. I 1(h) ∈ S1. By making each first
order interval into a node and each interval exit edge into an edge, the second order graph can be
derived, from which the second order intervals can also be defined. The procedure can be repeated
to derive successively higher order graphs until the n-th order graph consists of a single interval.
Figure 1 illustrates such a sequence of derived graphs.
Algorithm 1: Finding intervals for a given graph
Input: a set of nodes V on a graph
Output: a set of intervals S

1 // Assume v0 is the unique entry node of the graph
2 H = {v0};
3 while H (cid:44) ∅ do
4

// remove next h from H
h = H.pop();
I(h) = {h};
// only nodes that are neither in the current interval nor any other interval will be considered
while {v ∈ V | v (cid:60) I (h) ∧ (cid:64)s(s ∈ S ∧ v ∈ s) ∧ pre(v) ⊆ I (h)} (cid:44) ∅ do

I(h) = I(h) ∪ { v };

// find next headers
while {v ∈ V | (cid:64)s1(s1 ∈ S ∧ v ∈ s1) ∧ ∃m1, m2(m1, m2 ∈ pre(v) ∧ m1 ∈ I (h) ∧ m2 (cid:60) I (h))} (cid:44) ∅ do

H = H ∪ { v };

S = S ∪ I(h);

5

6

7

8

9

10

11

12

13

are I

and S4

respectively.

2(1)={1} and I

are I
2(2)={2,7,8}. I

Fig. 1. n-th order intervals and graphs. The set of intervals on S1
1(7)={7}. The set of intervals on S2
I
only intervals on S3
2.3 Graph Neural Network
Graph Neural Network (GNN) (Gori et al. 2005) is a specialized machine learning model designed to
learn from graph data. The intuitive idea underlying GNN is that nodes in a graph represent objects
and edges represent their relationships. Thus, each node v can be attached to a vector, called state,
which collects a representation of the object denoted by v. Naturally, the state of v can be specified

1(3)={3,4,5,6},
4(10)={10} are the

1(1)={1}, I
3(1)={1,9} and I

1(2)={2}, I

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:5

using the information provided by nodes in the neighborhood of v. Technically, there are many
realizations of this idea. Here, we describe the message-passing technique (Gilmer et al. 2017), a
widely applied method for GNN to compute the states of nodes.

We reuse the terminology of graph introduced in Section 2.1. Given a graph G = (V , E) where V
and E are the inputs, a neural message-passing GNN computes the state vector for each node in a
sequence of steps. In each step, every node first sends messages to all of its neighbors, and then
update its state vector by aggregating the information received from its neighbors.

µ(l +1)

v

= ϕ({µ(l )

u }u ∈N(v))

(1)

µ(l +1)
denotes the state of v in l + 1 steps, which is determined by the state of its neighbors in the
v
previous step. N (v) denotes the neighbors that are connected to v. Formally, N (v) = {u|(u, v, k) ∈
Ek , ∀k ∈ {1, 2, ..., K }}. ϕ(·) is a non-linear function that performs the aggregation. After the state
of each node is computed with many rounds of message passing, a graph representation can also
be obtained through various pooling operations (e.g. average or sum).

Some GNN (Si et al. 2018) computes a separate node state w.r.t. an edge type (i.e. µ(l +1),k

in

v

Equation 2) before aggregating them into a final state (i.e. µ(l +1)

v

in Equation 3).

µ(l +1),k

v

= ϕ1(

(cid:213)

W1µ(l )

u ), ∀k ∈ {1, 2, ..., K }

(2)

µ(l +1)

v

= ϕ2(W2[µ(l ),1

v

, µ(l ),2
v

, ..., µ(l ), K
v

])

(3)

u ∈Nk (v)

W1 and W2 are variables to be learned, and ϕ1 and ϕ2 are some nonlinear activation functions.

Li et al. (2016) proposed Gated Graph Neural Network (GGNN) as a new variant of GNN. Their
major contribution is a new instantiation of ϕ(·) using Gated Recurrent Units (Cho et al. 2014). The
following equations describe how GGNN works:

ml
v

= (cid:213)
u ∈N(v)

f (µ(l )
u )

(4)

µ(l +1)

v

= GRU (ml

v , µ(l )
v )

(5)

v

and µ(l )

(Equation 5).

using f (·) (e.g. a linear function)
v — the current state

To update the state of node v, Equation 4 computes a message ml
v
from the states of its neighboring nodes N (v). Next, a GRU takes ml
v
of node v — to compute the new state µ(l +1)
GNN’s Weakness. Although the message-passing technique has highly empowered GNN, it
severely hinders GNN’s capability of learning patterns formed by nodes that are far apart in a
graph. From a communication standpoint, a message sent out by the start node needs to go through
many intermediate nodes en route to the end node. Due to the aggregation operation (regardless of
which implementation in Equation 1, 3, or 5 is adopted), the message gets diluted whenever it is
absorbed by an intermediate node for updating its own state. By the time the message reaches the
end node, it is too imprecise to bear any reasonable resemblance to the start node. This problem
is particularly challenging in the programming domain where long-distance dependencies are
common, important properties models have to capture. Allamanis et al. (2018) designed additional
edges to connect nodes that exhibit a relationship of interest (e.g. data or control dependency)
but are otherwise far away from each other in an AST. The drawbacks are: first, their idea, albeit
conceptually appealing, has not been rigorously tested in practice, therefore, the extent to which
it addresses the scalability issue is unknown; second, they have not offered a principled solution
regarding which edges to add in order to achieve the best scalability. Instead, they universally
added a predefined set of edges to any graph, an approach that is far from satisfactory.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:6

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

Fig. 2

Fig. 3

Fig. 4

Fig. 5

Fig. 6

3 METHODOLOGY AND APPROACH
Our intuition is that generalization can be made easier when models are given abstractions (of
programs) to learn. Considering its generality and capacity, we choose GNN’s learning framework
to incorporate our abstraction methodology. A key challenge arises: how to abstract programs into
favorable graph representations for GNN to learn. A natural idea would be abstracting programs
at the level of source code and then reducing the abstracted programs into graphs. On the other
hand, a simpler and perhaps more elegant approach is to abstract programs directly on their graph
representation, which this paper explores. Specifically, we propose a new graph model, called Graph
Interval Neural Network (GINN), which uses control flow graph to represent an input program,1
and abstracts it with three primitive operators: partitioning, heightening, and lowering. Each
primitive operator is associated with an extra operator for computing the state of each node on the
resultant graph after the primitive operator is applied. Using the control flow graph depicted in
Figure 2 as our running example, we explain in details of GINN’s learning approach.

To start with, GINN uses the partitioning operator to split the graph into a set of intervals.
Figure 3 adopts different styling of the nodes to indicate the four intervals on the graph. Then,
it proceeds to compute the node states using the extra operator. Unlike the standard message-
passing mechanism underpinning the existing GNN, GINN restricts messages to be passed within
an interval. If an interval is a node itself, its state will not get updated by the extra operator. For
example, node 3, 4, 5, and 6 in Figure 3 will freely pass messages to the neighbors while evolving
their own states. On the other hand, node 1 and 2, which are alone in their respective intervals, keep
their states as they were. The propagation that occurs within an interval is carried out in the same
manner as it is in the existing GNN (Equation 1, 3, or 5 depending on the actual implementation).
We formalize the partitioning operator as follows:

Definition 3.1. (Partitioning) Partitioning operator, denoted by ρ, is a sequence (ρabs, ρcom) where
ρabs : Gn → Sn is a function that maps an n-th order graph into a set of n-th order intervals, and
ρcom : ∀I n(h) ∈ Sn → µv1, µv2, . . . µvT ∈ Re (v1, v2, . . . , vT ∈ I n(h)) is a function that computes a
e-dimensional vector for each node in each n-th order interval. Equation 6 defines ρcom.

Depends on how many nodes I n(h) is composed of, two cases to be considered:

(cid:40)

v

µ(l +1)
µ(l +1)

v

= ϕ({µ(l )
= µ(l )
v

u }u ∈N(v)) v ∈ I n(h) ∧ u ∈ I n(h)
v ∈ I n(h) ∧ |I n(h)| = 1

(6)

Next, GINN applies the heightening operator to convert the first order graph into the second
order graph (Figure 4). In particular, the heightening operation replaces each active interval (of

1We discuss a few variations in Section 4.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:7

multiple nodes) on the lower order graph with a single node on the higher order graph (e.g. node
3, 4, 5, and 6 are replaced with node 8). In terms of the states of the freshly created nodes, GINN
initializes them to be the weighted sum of the states of replaced nodes. To motivate this design
decision, we also consider simply averaging the states of replaced nodes in Section 4.3.5 and 4.4.2.
The heightening operator is defined below.

Definition 3.2. (Heightening) Heightening operator, denoted by η, is a sequence (ηabs, ηcom) where
ηabs : Gn → Gn+1 is a function that maps an n-th order graph into an (n+1)-th order graph, and
ηcom : µv n
} denotes the set of
nodes on Gn; {vn+1
N (cid:48) } denotes the set of nodes on Gn+1) is a function that maps a set
of e-dimensional vectors for each node on Gn to another set of e-dimensional vectors for each node
on Gn+1. Equation 7 defines ηcom.

∈ Re → µv n+1
, . . . vn+1

, . . . µv n+1
N (cid:48)

2 , . . . vn
N

∈ Re ({vn

2 , . . . µv n

, vn+1
2

, µv n+1

1 , µv n

1 , vn

1

N

1

2

Like Equation 6, we consider two cases:

µ(0)
v n+1
i
µ(0)
v n+1
h





= µ(l )
v n
i
v ∈I n (h) αv µ(l )
= (cid:205)

v

|I n(h)| = 1 ∧ vn
|I n(h)| > 1

i ∈ I n(h)

(7)

αv = exp (||µ(l )
v ||)
exp (||µ(l )
v (cid:48) ||)

(cid:205)
v (cid:48) ∈I n (h)

(8)

on the n-th order graph, and
is the node that replaces the interval I n(h). The weight of each state vector, αv , is determined

on the (n+1)-th order graph is the same node as vn
i

given that vn+1
vn+1
h
based on its length (Equation 8).

i

Now GINN uses the partitioning operator again to produce a set of intervals from the second
order graph (Figure 5). Due to the restriction outlined earlier, messages are only passed among
node 2, 7, and 8. However, since node 8 can be deemed as a proxy of node 3˜6, this exchange, in
essence, covers all but node 1 on the first order graph. In other words, even though messages
are always exchanged within an interval, heightening abstraction expands the region an interval
covers, thus enabling the communication of nodes that were otherwise far apart.

Then, GINN applies the heightening operator the second time, as a result, the third order graph
emerges (Figure 6). Since the third order graph is an interval itself, GINN computes the state of
node 1 and 9 in the same way as existing GNN using partitioning operator. Now, GINN reaches
an important point, which we call sufficient propagation. That is every node has passed messages
either directly or indirectly to every other reachable node in the original control flow graph. In
other words, the heightening operator is no longer needed.

Upon completion of the message-exchange between node 1 and 9, GINN applies for the first time
the lowering operator to convert the third order graph back to the second order graph. In particular,
nodes that were created on the higher order graphs for replacing an interval on the lower order
graph will be split back into the nodes of which the interval previously consists (e.g. node 9 to node
2, 7, and 8). The idea is, after reaching sufficient propagation, GINN begins to recover the state of
each node on the original control flow graph. We define the lowering operator below.

Definition 3.3. (Lowering) Lowering operator, denoted by λ, is a sequence (λabs, λcom) where
λabs : Gn+1 → Gn is the inverse function of ηabs, which maps an (n+1)-th order graph into an n-th
, . . . vn+1
order graph, and λcom : µv n+1
N (cid:48) }
, . . . µv n+1
N (cid:48)
denotes the set of nodes on Gn+1; {vn
} denotes the set of nodes on Gn) is a function
2 , . . . vn
1 , vn
N
that maps a set of e-dimensional vectors for each node on Gn+1 to another set of e-dimensional
vectors for each node on Gn. Equation 9 defines λcom.

∈ Re → µv n

∈ Re ({vn+1

2 , . . . µv n

, vn+1
2

, µv n+1

1 , µv n

1

N

2

1

In general, λcom can be considered as a reversing operation of ηcom:

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:8

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

µ(0)
= µ(l )
v n+1
v n
i
i
µ(0)
v = αv µ(l )
v n+1
h

|I n+1(h)| = 1 ∧ vn+1
|I n+1(h)| > 1

i

∈ I n+1(h)

∗ |I n+1(h)|

(9)





on the n-th order graph is the same node as vn+1

on the (n+1)-th order graph, and
given that vn
i
vn+1
is the node that is split into a multitude of nodes, each of which is assigned αv , the weight
h
|I n+1(h)| is the regulation term that aims to bring node v’s initial state
defined in Equation 8.
and its final state (prior to being replaced by the heightening operator) within the same order of
magnitude.

i

As GINN revisits the second order graph, it re-applies the partitioning operator to produce
the only interval to be processed (i.e. node 2, 7, and 8). However, there lies a crucial distinction
in computing the states of node 2, 7, and 8 this time around. That is the communication among
the three nodes is no longer local to their own states, but also inclusive of node 1’s thanks to
the exchange between node 1 and 9 on the third order graph. In general, such recurring interval
propagation improves the precision of node representations by incorporating knowledge gained
about the entire graph.

Next, GINN applies the lowering operator again to recover the original control flow graph. After
the partitioning operation, GINN computes the state vector for every node on the graph, signaling
the completion of a whole learning cycle. The process will be repeated for a fixed number of steps.
Then, we use the state vectors from the last step as the node representations. We generalize the
above explanation with the running example to formalize GINN’s abstraction cycle.

Definition 3.4. (Abstraction Cycle) GINN’s abstraction cycle is a sequence composed of three
primitive operators: partitioning, denoted by ρ; heightening, denoted by η; and lowering, denoted
by λ. The sequence can be written in the form of ((ρ → η)∗ → ρ (cid:48) → (λ → ρ)∗)∗ where
|(ρ → η)∗| = |(λ → ρ)∗|. ρ (cid:48) denotes the partitioning operator on the highest order graph in which
case the graph is a single interval itself.

To sum up, GINN adopts control flow graphs as the program representation. At first, it uses
heightening operator to increase the order of graphs, as such, involving more nodes to communicate
within an interval. Later, by applying the lowering operator, GINN restores local propagation on the
reduced order of graphs, and eventually recovers the node states on the original control flow graph.
Worth mentioning heightening and lowering operators are complementary and their cooperation
benefits the overall GINN’s learning objective. Specifically, heightening operator enables GINN to
capture the global properties of a graph, with which lowering operator helps GINN to compute
precise node representations. As the interleaving between heightening and lowering operator goes
on, node representations will continue to be refined to reflect the properties of a graph.
Abstracting the Source Code. We aim to demystify what the above graph abstraction method
translates to at the source code level, and why it helps GINN to learn. Since intervals play a pivotal
role in all three operators of the graph abstraction method, it is imperative that we understand
what an interval represents in a program. As a simple yet effective measure, we use a program,
which has the same control flow graph as the one in Figure 2, to disclose what code construct an
interval correlates to, and more importantly, how the abstraction method works at the source code
level. The program is depicted in Figure 7, which computes the starting indices of the substring
s1 in string s2. Recall the only interval GINN processed on the first order graph, the one that
consists of node 3˜6, which represents the inner do-while loop in the function. In other words,
GINN focuses exclusively on the inner loop when learning from the first order graph. Similarly,
node 2, node 7, and node 8 consist of the only interval from which GINN learns at the second order

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:9

void substringIndices (String s1 ,

void substringIndices (String s1 ,

void substringIndices (String s1 ,

String s2 , int attempts , int [] arr )

String s2 , int attempts , int [] arr )

String s2 , int attempts , int [] arr )

{

}

int M = s1.length();
int N = s2.length();
int i = 0,j = 0,res = 0,pos = 0;

{

int M = s1.length();
int N = s2.length();
int i = 0,j = 0,res = 0,pos = 0;

{

int M = s1 . length ();
int N = s2 . length ();
int i = 0,j = 0, res = 0, pos = 0;

do {

if (pos < attempts) {

do {

do {

do {

if ( pos < attempts ) {

if ( pos < attempts ) {

do {

do {

log (" Begin at index : " + i );
if ( s2 . charAt (i+j) !=

s1 . charAt (j )) {

res =
log (" Differs at : " + j );}

-1;

else

log (" Begin at index : " + i );
if ( s2 . charAt (i+j) !=

s1 . charAt (j )) {

res =
log (" Differs at : " + j );}

-1;

else

log (" Begin at index : " + i );
if ( s2 . charAt (i+j) !=

s1 . charAt (j )) {

res =
log (" Differs at : " + j );}

-1;

else

log (" Equal at : " + j );

log (" Equal at : " + j );

log (" Equal at : " + j );

j ++;

} while ( j < M );

}

arr[pos++] = res;
i++; j = 0;
res = pos < attempts ? i : 0;

} while (i <= N - M);

j ++;

} while (j < M );

}

j ++;

} while (j < M );

}

arr [ pos ++] = res ;
i ++; j = 0;
res = pos < attempts ? i : 0;

arr [ pos ++] = res ;
i ++; j = 0;
res = pos < attempts ? i : 0;

} while ( i <= N - M );

} while (i <= N - M );

}

}

Fig. 7. GINN’s focus in source code when transitioning back and forth between different order graphs.

graph. This interval happens to be the nested do-while loop. Finally, GINN spans across the whole
program as node 1 and node 9 cover every node in the first order graph. Figure 7 illustrates GINN’s
transition among the first, second, and third order graph, in particular, the distinct focus on each
order graph. Inspired from the exposition above, we present Theorem 3.5 and Corollary 3.6.

Theorem 3.5 (Interval Constituents). Given a loop structure l on a control flow graph, l’s loop
header, denoted by h, is the entry node of an interval that does not contain any predecessor of h outside
of l. However, the interval contains every node inside of l assuming l does not contain an inner loop
construct.
Proof. First, we prove the non-coexistence of h and its predecessors outside of l. Assume otherwise,
there is an interval I containing at least one predecessor of h outside of l and h itself. By construction,
I already had an entry node h(cid:48). Due to the loop-forming back edge connecting node m back to h
within l, I now has two entry nodes, h(cid:48) and h, if m is not already in I , which violates the single-entry
rule of an interval. For the other possibility that if m is in I , then by construction all immediate
predecessors of m are also in I given that l does not contain an inner loop, recursively all nodes in
l are in I , meaning there will be closed paths within I that do not go through h(cid:48) (i.e. those going
through h), which violates the definition of interval.
Second, we prove h is the entry node of the interval it is in. Assume otherwise, there exists an
interval I , which h is a part of, having a different entry node h(cid:48). Since none of h’s predecessors
outside of l are part of I , at least one of them will connect to h, thus making h the other entry node
of I in addition to h(cid:48). Therefore, the single-entry rule of an interval is violated.
Finally, we prove that all nodes of l are in the same interval as h if l does not contain an inner
loop construct. Assume otherwise, there exists at least one node in l that is not in the interval, I ,
for which h is the entry node. By construction, at least one immediate predecessor of the node
not in I is also not in I . Since l does not contain an inner loop construct, none of the nodes
in l has a predecessor outside of l. By recursion, ultimately, the loop header h will be the only
immediate predecessor of a node that is not in I . Therefore, h is also not in I , which contradicts the
(cid:3)
assumption.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:10

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

var clazz = classTypes [" Root " ]. Single () as ...
Assert . NotNull ( clazz );

public string [] f( string [] array )
{

var first = classTypes [" RecClass " ]. Single () as ...
Assert . NotNull ( clazz );

string [] newArray = new string [ array . Length ];
for ( int index = 0; index < array . Length ; index ++)
newArray [ array . Length - index -1] = array [ index ];

Assert . Equal (" string " , first . Properties [" Name " ]. Name );
Assert . False ( clazz . Properties [" Name " ]. IsArray );

}

return newArray ;

Fig. 8. An example (extracted from (Allamanis et al.
2018)) of variable misuse. clazz highlighted in red is
misused. Instead, first should have been used.

Fig. 9. An example function extracted from (Alon et al.
2019b) whose name is left out for models to predict.
The answer in this case is reverseArray.

Corollary 3.6 (Hierarchical Intervals for Nested Loops). Given a nested loop structure on

a control flow graph, nodes of the n-th most inner loop lie in an interval on the n-th order graph.

Proof. Let P(n) be the statement that nodes of the n-th most inner loop lie in an interval on the
n-th order graph. We give a proof by induction on n.
Base case: Show that the statement holds for the smallest number n = 1.
According to Theorem 3.5, P(0) is clearly true: nodes of the most inner loop lie in an interval on the
first order graph.
Inductive step: Show that for any k ≥ 1, if P(k) holds, then P(k+1) also holds.
Assume the induction hypothesis that for a particular k, the single case n = k holds, meaning P(k)
is true: nodes of the k-th most inner loop lie in an interval on the k-th order graph. By definition,
nodes of the k-th most inner loop will be rendered into a single node within the (k+1)-th most inner
loop on the (k+1)-th order graph. According to Theorem 3.5, we deduce that nodes of the (k+1)-th
most inner loop will also lie in an interval on the (k+1)-th order graph. That is, the statement P(k+1)
also holds, establishing the inductive step.
Conclusion: Since both the base case and the inductive step have been shown, by mathematical
(cid:3)
induction the statement P(n) holds for every number n.

GINN’s Strengths. To conclude, the graph abstraction method translates to a loop-based ab-
straction scheme at the source code level. In particular, it sorts programs into hierarchies, where
each level presents unique looping constructs for models to learn. Since looping constructs are
integral parts of a program, this loop-based abstraction scheme helps GINN to focus on the core
of a program for learning its feature representation. In scalability regard, the graph abstraction
method also offers a crucial advantage. Given the abstracted program graphs, even without the
manually designed edges, GINN mostly deals with small, concise intervals. As a result, information
rarely needs to be propagated over long distances, thus significantly enhancing GINN’s capability
in generalizing from the large graphs, an aspect that the existing GNN struggles with.

4 EVALUATION
This section presents an extensive evaluation of GINN on three PL tasks. For each task, we compare
GINN against the state-of-the-art.

4.1 Selection Criteria
To select the right downstream applications for evaluating GINN, we devise a list of criteria below
in the order of importance.

(1) We prefer tasks for which datasets are publicly accessible and baseline models are open
sourced. Furthermore, we only consider models that are built upon Tensorflow, a widely
used deep learning library, for reducing the engineering load.

(2) We prefer tasks solved by an influential work that is highly cited in the literature.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:11

(3) We prefer tasks where implementations of GNN-based models are publicly accessible so that
we can save our effort in building the baseline with GNN. The reason is it’s not sufficient to
only compare GINN with the state-of-the-art, which GNN may also outperform.

4.2 Evaluation Tasks
In the end, we select variable misuse prediction task (Allamanis et al. 2018) (150+ citations) in which
models aim to infer which variable should be used in a program location. As shown in Figure 8,
the variable clazz highlighted in red is misused. Instead, variable first should have been passed
as the argument of the function Assert.NotNull. At first, Allamanis et al. (2018) use GGNN to
predict the misuse variable from AST-based program graphs (more precisely AST with additional
edges). Later, the joint model proposed by (Hellendoorn et al. 2020; Vasic et al. 2019) has achieved
better results and thus becomes the new state-of-the-art.

In addition, we select method name prediction in which models aim to infer the name of a
method when given its body. Figure 9 shows the problem setting where the function name has been
stripped for models to predict. The correct answer in this case is reverseArray. Alon et al. (2019b)
(120+ citations) create the first large-scale, cross-project classification task in method names. Later
works adopted a generative approach that generates method names as sequences of words (Alon
et al. 2019a; Fernandes et al. 2019; Wang and Su 2020).

Last, we design a new task to evaluate GINN. Granted, machine learning has inspired a distinctive
approach to a board range of problems in program analysis. However, most problems are catered to
the strength of machine learning models in mining statistical patterns from data. It’s unclear how
well they can solve the long-standing problems that are traditionally tackled by symbolic, logic-
based methods. In this task, we examine if models can accurately detect null pointer dereference, a
type of deep, semantic, and prevalent bugs in software. In particular, we compare GINN against
not only the standard GNN but also arguably the state-of-the-art static analysis tool Facebook
Infer (Berdine et al. 2006; Calcagno et al. 2015).

4.3 Variable Misuse Prediction
The state-of-the-art model for predicting variable misuse bugs is proposed by Hellendoorn et al.
(2020). Their model is an instantiation of the conceptual framework invented by Vasic et al. (2019).
In this section, we first describe this framework, then we explain how to instantiate it using GINN.

4.3.1

Joint Model for Localization and Repair. Vasic et al. (2019) propose a joint model structure
that learns to localize and repair variable misuse bugs simultaneously. Intuitively, their design
exploits an important property of variable misuse bugs. That is both the misuse (i.e. incorrectly
used) and the repair (i.e. should have been used) variable have already been defined somewhere in
the program, therefore, the key is to identify their locations. Vasic et al. (2019) learned probability
distributions over the sequence of tokens of a buggy program from which they pick the token of
the highest probability to be the misuse or repair variable. At a high-level, the joint model consists
of three major components: initial embedding layer, core model, and two separate classifiers. Below
we describe each component in detail.
Initial Embedding Layer. Initial embedding layer is responsible for turning each symbol from
the input vocabulary (e.g. a token or a type of AST node) into a numerical vector, the format that is
amenable to deep neural networks. A simple, crude method is to use the one-hot vectors, a N × N
matrix representing every word in a vocabulary (N denotes the size of the vocabulary). Each row
corresponds to a word, which consists of 0s in all cells with the exception of a single 1 in a cell
used uniquely to identify the word. For example, given a vocabulary, {a, +, b, binary-exp}, the
matrix M in Equation 10 depicts the one-hot vectors of each token. A drawback of one-hot vectors

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:12

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

is they don’t capture the semantic relationship of words in the vocabulary as vectors are uniformly
scattered in a high dimensional space (i.e. the Euclidean distance between any vector with every
other vector is a constant). A common remedy is to have the one-hot vectors multiply another
matrix W1 ∈ RN ×d to produce E ∈ RN ×d . The intuition is to expand the network’s capacity with
more learnable parameters in W1 for searching a precise embedding matrix E for each word in
the vocabulary. d denotes the number of columns in W1, which is also the size of the embedding
vectors. Equation 10 gives an example where d equals to 3. The values of W1 is initialized randomly
and will be learned simultaneously with the network during training.
Core Model. Given the embedding vector of each token, the core model computes the numerical
representation of the input program, expressed as another matrix C where each row represents
a token. Many neural architectures that can play the role of core model. Vasic et al. (2019) used
Long Short Term Memory networks (LSTM) (Hochreiter and Schmidhuber 1997), a specialization of
Recurrent Neural Networks (RNN). Hellendoorn et al. (2020) explored other alternatives including
GGNN, Transformer (Vaswani et al. 2017), the state-of-the-art sequence model, and RNN Sandwich,
a mixture of sequence and graph models, which achieves the state-of-the-art results. For now, a
core model can be considered as a black-box with an abstract interface ϕ : E → C. We defer the
discussion of two instantiations of the core model to later sections.

(10)

Classifiers for Misuse and Repair Variables. This layer is responsible for predicting the location
of misuse and repair variable2. It distributes two probabilities to each token in the program: one
for identification of the misuse variable and the other for the repair variable. The token with the
highest probability of being the misuse or repair variable will be picked as the output. At a technical
level, Vasic et al. (2019) performed a linear transformation on matrix C ∈ Rk ×h using another
matrix W2 ∈ Rh×2 to produce Pval ∈ Rk ×2. As explained before, k is the number of tokens in a
program; h is the size of the embedding vector produced by a core model.

Pval = CW2
Since Pval is a non-normalized output, we apply softmax function, a common practice in machine
learning, to map Pval into probability distributions over the token sequence of an input program.
Note that axis = 1 in Equation 11 means the normalization happens along each column in which
each value denotes the probability of one token being the misuse or repair variable.

Ppro = softmax(Pval , axis = 1)
Finally, the model is trained to nudge Ppro as close to the true distribution of misuse and repair
variables as possible. In particular, we minimize the cross-entropy loss expressed by the difference
between Ppro and the true distributions, denoted by Loc and Rep. Similar to the one-hot vectors, Loc
(resp. Rep) assigns a value of 1 to the actual index of misuse (resp. repair) variable among all tokens
of the input program and 0 otherwise. We give the details of the loss function in Appendix A.

(11)

2We followed Hellendoorn et al. (2020)’s approach to consider a single variable misuse bug per program.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:13

4.3.2

Instantiations of the Core Model. In this section, we review two instantiations of the core
model presented in (Hellendoorn et al. 2020): GGNN and RNN Sandwich. In particular, we discuss
how each of them computes the matrix C introduced earlier.

As explained in Section 4.2, GGNN works with AST as the backbone of graphs. To collect the
numerical representations of each token in the program, Hellendoorn et al. (2020) extracted states
of terminals nodes after GGNN had completed the message-passing routine, and then stacked them
into matrix C (Equation 12).

(12)

C = stack([µv1, . . . , µvn ], axis = 0), ∀vi ∈ Σ
where Σ denotes the set of terminal nodes in AST; axis = 0 indicates each µv forms a row in C.

Next, we discuss RNN Sandwich (Hellendoorn et al. 2020), the state-of-the-art model in pre-
dicting variable misuse bugs. In a nutshell, RNN Sandwich adopts a hybrid neural architecture
combining a sequence model RNN and a graph model GGNN intending to get the best of both
worlds. That is, not only exploiting the semantic code structure using GGNN but also streamlining
information flow through token sequences using RNN. Technically, the way it works is the follow-
ing: (1) first, the matrix E produced in the initial embedding layer will be fed into an RNN. The
results are a list of vectors, h1, . . . , hn, each of which represents a token. Compared to the initial
embedding matrix E, h1, . . . , hn increases the precision of the token representation by capturing
the temporal properties they display in a sequence. Equation 13 computes h1, . . . , hn. In simplest
terms, RNN takes two vectors at each time step — the embedding vector of the t-th token in the
sequence, denoted by E[t] (assuming the t-th row in E is the embedding of the t-th token), and
RNN’s current hidden state after consuming the embedding of the previous token. The output is
the new hidden state ht . For interested readers, Appendix B provides a more detailed description
ht = RNN (E[t], ht −1)
(13)
of RNN’s computation model. (2) Next, GGNN takes over and computes the state vector for each
node via the message-passing protocol. For node initialization, the token representations computed
in Step (1) are assigned to the terminal nodes in an AST (Equation 14) while the non-terminal nodes

µvi

= hi , ∀vi ∈ Σ, ∀hi ∈ [h1, . . . , hn]

(14)

keep their representations computed from the initial embedding layer. (3) Finally, after GGNN
has computed the state of each node in an AST, RNN takes back those of the terminal nodes and
, according to Equation 15 where µvt
computes a new representation for each token, h(cid:48)
denotes the state vector of the node corresponding to the t-th token in the program. Finally, the
= RNN (µvt , h(cid:48)
(15)

1, . . . , h(cid:48)
n

t −1)

h(cid:48)
t

matrix C can be computed by Equation 16.

C = stack([h(cid:48)

n], axis = 0)

1, . . . , h(cid:48)
4.3.3 New Instantiations of Core Models Using GINN. We propose two new instantiations built
upon GINN to pair with GGNN and RNN Sandwich. In order to adapt GINN as an instantiation of
the core model, an important issue needs to be addressed. Recall the interface defined for the core
model: ϕ : E → C, since GINN can not compute representations of tokens from a standard control
flow graph, matrix C can’t be produced. Therefore, we modify control flow graphs as follows: (1)
first we split a graph node representing a basic block into multiple nodes, each of which represents
a single statement. Subsequently, we add additional edges to connect every statement with its
immediate successor within the same basic block. For edges on the original control flow graphs,
we change their start (resp. end) nodes from a basic block to its last (resp. first) statement after the
split; (2) next, we replace each statement node with a sequence of nodes, each of which represents

(16)

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:14

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

a token of the statement. Specifically, every token node is connected to its immediate successor,
and the first token node will become the new start or end node of edges that were connecting the
statement nodes before.

Figure 10b depicts an example of the new graph representation given the function in Figure 10a.
Note that our modification will not alter the partitioning of a graph into intervals. Instead, we
only replace nodes denoting basic blocks with those denoting token as the new constituents of
an interval. We have included a detailed explanation in Appendix C for readersfi perusal. Under
the new graph representation, GINN can compute the state vector for every token in the program,
and in turn the matrix C. Therefore, we can now instantiate the core model with GINN. Similarly,
we can easily construct a variant of RNN Sandwich by swapping out GGNN for GINN as the new
graph component.

static bool FindIndexOfMaxElement ( int [] arr )
{

int max = int . MinValue , idx = 0;

for ( int i = 0; i < arr . Length ; i ++)
{

if ( arr [i ] > max )
{

max = arr [i ];
idx = i;

}

}

return idx ;

}

(a)

(b)

Fig. 10. The program graph in (b) that represents the function in (a). Edges with hallow arrow heads represent
the control flows between program statements. The other edges connect tokens in a sequence.

4.3.4 Experimentation. Given the conceptual framework of the joint model and various instanti-

ations discussed above, we now describe the experiment setup.
Dataset, Metric, and Baseline. Hellendoorn et al. (2020) worked with ETH Py150 (Raychev et al.
2016), a publicly accessible dataset of python programs, on top of which they introduced buggy
programs to suit the task of variable misuse prediction. Since the actual dataset used in their
experiment is not publicly available, we follow their pre-processing steps in an attempt to replicate
their dataset. Major tasks include the deduplication of ETH Py150, generation of buggy programs,
and preservation of original programs as correct examples. Like their dataset, we maintain a balance
between buggy and correct programs. In the end, we have 3M programs in total, among which
We use 2M for training, 245K for validation, and 755K for test. A detailed description of the data
generation is provided in Appendix D. Similarly, we adopt the metrics proposed in (Vasic et al.
2019) to measure the model performance, such as (1) classification accuracy, the percentage of
programs in the test set that are correctly classified as either buggy or correct; (2) localization
accuracy, the percentage of buggy programs for which the bug location is correctly predicted; and
(3) localization+repair accuracy (or joint accuracy), the percentage of buggy programs for which
both the location and repair are correctly predicted. Regarding baselines, we use GGNN and RNN
Sandwich as the two instantiations of the core model.
Implementation. Since ETH Py150 provides AST for programs in the dataset, we convert each AST
into the graph representation that GINN consumes (Appendix E). We also implement Algorithm 1
to partition the graphs into intervals. Since prior works have not open-sourced the joint model
framework, we implemented our own in Tensorflow. Regarding the core models, we use Allamanis

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:15

et al. (2018)’s implementation of GGNN, on top of which we realize GINN’s graph abstraction
method while keeping all GGNN’s default parameters (e.g. size of node embeddings, number of
layers, etc.) intact. We only implement one abstraction cycle of GINN’s (original graph→highest
order graphs→original graphs) as more cycles do not make a significant difference. For RNN
Sandwich, we use the large sandwich model (Hellendoorn et al. 2020), which is shown to work
better than the vanilla model presented in Section 4.3.2. The improvement is due to the increased
frequency of the alternation between sequence and graph models. That is, instead of inserting
RNN only before and after GNN’s computation, they wrap every few rounds of message-passing
inside of GNN with an RNN to facilitate a thorough exchange between two neural architectures.
We couple GINN (resp. GGNN) with an RNN to implement the GINN- (resp. GGNN-) powered RNN
Sandwich. Note that Hellendoorn et al. (2020) also propose a lightweight graph representation by
keeping only the terminal nodes from an AST as the input for both GGNN and RNN Sandwich.
Even though these new program graphs have made models faster to train, their accuracy often
decreases due to the lesser information contained in the input graphs. Hence, we don’t consider
this lightweight graphs in our evaluation.

Table 1. Results for the different instantiations of core models. For GINN and GINN-powered RNN Sandwich,
we also include their results of an ablation study and an evaluation on an alternative design.

Models Configuration

GGNN

GINN

Sandwich (GGNN)

Sandwich (GINN)

Original
Original
Ablated
Alternative
Original
Original
Ablated
Alternative

Classification
Accuracy
74.0%
74.9%
74.7%
74.0%
74.7%
75.8%
75.1%
75.0%

Localization
Accuracy
58.1%
60.5%
58.8%
59.5%
62.9%
69.3%
62.4%
63.1%

Localization+Repair
Accuracy
56.0%
60.0%
57.6%
58.7%
61.1%
68.4%
62.3%
62.0%

All experiments including those to be presented later are performed on a desktop that runs
Ubuntu 16.04 having 3.7GHz i7-8700K CPU, 32GB RAM, and NVIDIA GTX 1080 GPU. As a pre-test,
we repeat the experiment presented in (Hellendoorn et al. 2020), and observed comparable model
performance for both GGNN and GGNN-based RNN Sandwich, signaling the validity of both our
model implementation and data curation. Interested readers may refer to Appendix F for details.

4.3.5 Results. First, we compare the performance of GINN (resp. GINN-powered RNN Sandwich)
against GGNN (resp. GGNN-powered RNN Sandwich). Then we investigate the scalability of these
four neural architectures. Later, we conduct ablation studies to gain a deeper understanding of
GINN’s inner workings. Finally, we evaluate an alternative design of GINN.
Accuracy. Table 1 depicts the results of all four models using the aforementioned metrics (Appen-
dix G shows their training time). We focus on rows for the original configuration of each model,
and discuss the rest later. Regarding the classification accuracy, all models perform reasonably
well, and GINN-powered RNN Sandwich and GINN are the top two core models despite by a small
margin. For more challenging tasks like localization or repair, GINN-powered RNN Sandwich now
displays a significant advantage over all other models, in particular, it outperforms GGNN-powered
RNN Sandwich, the state-of-the-art model in variable misuse prediction, by 6.4% in localization and
7.3% in joint accuracy. To dig deeper, We manually inspect the predictions made by each model and

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:16

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

find that GINN-powered RNN Sandwich is considerably more precise at reasoning the semantics of
a program. Below we gave two examples to illustrate our findings.

For Figure 11a, GINN-powered RNN Sandwich is the only model that not only locates but also
fixes the misused variable (i.e. orig sys path highlighted within the shadow box). In contrast,
all baseline models consider orig sys path as the correctly used variable, which is not totally
unreasonable. In fact, appending an item (item) when it is not yet in the list (orig sys path) is a
very common pattern models need to learn. However, in this case, if orig sys path was the correct
variable, new sys path would have been an empty list when being assigned to sys.path[:0] in
the last line. GINN-powered RNN Sandwich is capable of capturing the nuance of the semantics
this program denotes, and not getting trapped by the common programming paradigms. As for
Figure 11b, GINN and GGNN-powered RNN Sandwich also correctly localizes the misused variable
request, but none of them predicts the right repair variable sq. The signal there is the fact that
fts is a list, which does not have keys() method. GINN-powered RNN Sandwich is again the
only neural architecture that produces the correct prediction end-to-end, demonstrating its higher
precision in reasoning the semantics of a program.

def add_vendor_lib ():

orig_sys_path = set ( sys . path )

def list_stored_queries ( self , request ):
sq = super ( GeoDjangoWFSAdapter ,

new_sys_path = []
for item in list ( sys . path ):

self ). list_stored_queries ( request )

fts = list ( self . models . keys ())

if item not in orig_sys_path :

for k in request . keys ():

orig_sys_path . append ( item )
sys . path . remove ( item )

sq [k ] = StoredQueryDescription ( name =k , feature_types = fts ,

title =k , parameters =[])

sys . path [:0] = new_sys_path

return sq

(a)

(b)

Fig. 11. Two programs only GINN-powered RNN Sandwich makes the correct prediction end-to-end.

Scalability. We further analyze the results we obtained from the previous experiment to investigate
the scalability of the four neural architectures. In particular, we divide the entire test set into ten
subsets, each of which consists of graphs of similar size. We then record the performance of
all models on each subset starting from the smallest to the largest graphs. Note that GGNN,
at the core of both compared baselines, has already had additional edges incorporated into the
program graphs to alleviate the scalability concern (Allamanis et al. 2018). However, as explained
in Section 1, Allamanis et al. (2018) do not provide a principled guideline as to where exactly to
add the edges given a program graph specifically. Therefore, we can only follow their approach by
universally adding all types of edges they proposed (9 in total which we list in Appendix H) to all
program graphs. In contrast, we do not incorporate any additional edge apart from those that are
present in the graph representation for GINN.

Figure 12 shows how the performance of each model varies with the size of graphs. We skip the
metric of classification accuracy under which models are largely indistinguishable. For localization
and joint accuracy, we make several observations of the performance trend for each model. First,
GGNN suffers the largest performance drop among all models — more than 10% (resp. 20%) under
location (resp. joint) accuracy. This phenomenon shows there is indeed a scalability issue with
GGNN. Second, the combination of sequence and graph model helps to make the neural architecture
more scalable than GGNN alone. Furthermore, by replacing GGNN with GINN in RNN Sandwich,
we obtain a model that is clearly the most scalable. Barring few sets of graphs, GINN-powered RNN
Sandwich is considerably more accurate than any other model, especially under the joint accuracy.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:17

y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
l
C

1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55

GGNN
Sand

GINN
I-Sand

1

2

3

4

5

6

7

8

9

10

y
c
a
r
u
c
c
A
n
o
i
t
a
c
o
L

1.1
1

0.9
0.8
0.7
0.6
0.5
0.4

GGNN
Sand

GINN
I-Sand

1

2

3

4

5

6

7

8

9

10

y
c
a
r
u
c
c
A
r
i
a
p
e
R
+
n
o
i
t
a
c
o
L

1.1
1

0.9
0.8
0.7
0.6
0.5
0.4
0.3

GGNN
Sand

GINN
I-Sand

1

2

3

4

5

6

7

8

9

10

N-th Set (Ranked by Size of Graphs)

N-th Set (Ranked by Size of Graphs)

N-th Set (Ranked by Size of Graphs)

Fig. 12. Investigating the scalability of all four neural architectures. I-Sand (resp. Sand) denotes the GINN-
powered RNN Sandwich (resp. GGNN-powered RNN Sandwich).

Finally, GINN alone also manages to improve the scalability of GGNN. As described earlier, the
improvement is magnified under the assistance of the RNN in the sandwich model.
Ablation Study. Unlike the prior work (Allamanis et al. 2018), all edges in our graph representation
are indispensable, thus can not be ablated. Therefore, the goal of this ablation study is to quantify
the influence of the graph abstraction method, the crux of GINN’s learning approach, on GINN’s
performance. Because GINN is identical to GGNN without the graph abstraction operators, we
evaluate GGNN when GINN’s program graphs — a variant of control flow graph — are provided
as input data. Rows for ablated configuration in Table 1 show GINN (resp. GINN-powered RNN
Sandwich) becomes only slightly more accurate than GGNN (resp. GGNN-powered RNN Sandwich)
after the ablation, in other words, our graph abstraction method indeed helps models to learn.
Alternative Design. As mentioned in Section 3, we evaluate an alternative design of GINN in
which freshly created nodes — due to the heightening operator — are initialized to be the average
of the replaced nodes; conversely, for nodes that are created by the lowering operator, they receive
an equal share of the split node that they emerge from. In essence, we assign the weight, αv , used
in Equation 7 and 9 to be 1/|I n
|. Rows of alternative configuration in Table 1 show that the new
h
design results in a notable decrease in model accuracy, which motivates the original design of
GINN.

4.4 Method Name Prediction
The state-of-the-art model in method name prediction, sequence GNN (Fernandes et al. 2019),
adopts a typical encoder-decoder architecture (Cho et al. 2014; Devlin et al. 2014).3 In a similar
vein to RNN Sandwich, sequence GNN also employs a combination of graph and sequence model
to encode a program, however, the synergy between the two neural architectures in this case is
considerably simpler. They only perform the first two steps of the vanilla joint model presented
in Section 4.3.2. That is, RNN first learns the sequence representation of each token in a program
before GGNN computes the state for every node in the AST. For decoder, they use another RNN,
which generates the method name as a sequence of words. While decoding, it also attends to the
states of nodes in the AST, a common technique for improving the accuracy of models adopting
encoder-decoder architecture (Bahdanau et al. 2015).

Instantiating the framework of sequence GNN’s with GINN is a straightforward task. We use
GINN as the new graph model for the encoder while keeping the remaining parts of the framework

3Wang and Su (2020) proposed a model to solve the same problem. Since (1) they did not perform a head-to-head comparison
against sequence GNN; and (2) their methodology requires program executions, we do not consider their model for this task.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:18

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

private int extractModifiers ( GroovySourceAST ast ) {

GroovySourceAST modifiers = ast . childOfType ( MODIFIERS );

public void applyRules (@Nullable String pluginId , Class <? > clazz ) {

ModelRegistry modelRegistry = target . getModelRegistry ();

if ( modifiers == null ) return 0;
int modifierFlags = 0;
for ( GroovySourceAST child =

( GroovySourceAST ) modifiers . getFirstChild ();
child != null ;
child = ( GroovySourceAST ) child . getNextSibling ()) {

switch ( child . getType ()) {
case LITERAL_private :

modifierFlags |= Modifier . PRIVATE ; break ;

case LITERAL_protected :

modifierFlags |= Modifier . PROTECTED ; break ;

case LITERAL_public :

modifierFlags |= Modifier . PUBLIC ; break ;

case FINAL :

modifierFlags |= Modifier . FINAL ; break ;

case LITERAL_static :

modifierFlags |= Modifier . STATIC ; break ;

Iterable < Class <? extends RuleSource > > declaredSources =

ruleDetector . getDeclaredSources ( clazz );

for ( Class <? extends RuleSource > ruleSource : declaredSources ) {

Iterable < ExtractedModelRule > rules =

ruleInspector . extract ( ruleSource ). getRules ();

for ( ExtractedModelRule rule : rules ) {

for ( Class <? > dependency : rule . getRuleDependencies ()) {

target . getPluginManager (). apply ( dependency );

}

rule . apply ( modelRegistry , ModelPath . ROOT );

}}

return modifierFlags ;

}

}

}

}

(a) Predicted to be SelectModifier by the baseline.

(b) Predicted to be CheckSource by the baseline.

Fig. 13. Two programs sequence GINN predicts correctly but not the baseline.

intact. Like prior experiments, GINN works with the same graph format derived from control flow
graphs and customized to include nodes of tokens.

4.4.1 Experimentation. Given the sequence GNN’s framework and the instantiation with GINN,

we describe the setup of our experiment.
Dataset, Metric, and Baseline. We consider Java-small proposed by Alon et al. (2019a), a publicly
available dataset used in (Fernandes et al. 2019), adopting the same train-validation-test splits they
have defined. We also follow Fernandes et al. (2019)’s approach to measure performance using F1
and ROGUE score over the generated words (Appendix I). For sequence GNN baseline, we use the
model that achieves state-of-the-art results on Java-small, which employs bidirectional-LSTM as
the sequence and GGNN as the graph model for encoder and another LSTM for decoder. Apart
from the aforementioned attention mechanism, decoder also integrates a pointer network (Vinyals
et al. 2015) to directly copy tokens from the input program.
Implementation. We use JavaParser to extract AST out of programs in Java-small. We adopt
the same procedure as before to convert AST into the graph representation that GINN consumes.
Regarding the model implementation, we use the model code of sequence GNN open-sourced
on GitHub, within which we implemented GINN’s graph abstraction methods. Like before, all
default model parameters in their implementation are kept as they are and we only implement one
abstraction cycle within GINN. We name our model built out of GINN sequence GINN.

4.4.2 Results. In this section, we first compare the performance of sequence GINN against
sequence GNN. Then we investigate the scalability of both neural architectures. Finally, we conduct
a similar ablation study and alternative design evaluation with those presented in the previous task.
Accuracy. Table 2 depicts the results of the two models. Sequence GINN significantly outperforms
the sequence GNN across all metrics (e.g. close to 10% in F1). Through our manual inspection, we
find that sequence GINN’s strength stands out when dealing with programs of greater complexity.
Take programs in Figure 13 as examples, even though both of them denote relatively simple
semantics, sequence GNN struggles with their complexity (e.g. large program size for Figure 13a4,
and nested looping construct for Figure 13b). Therefore, its predictions do not precisely reflect
the semantics of either program. sequence GNN predicts the name of the program in Figure 13a

4The average size of the AST in Java-small is around 100 while this program has 150+ nodes in its AST.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:19

(resp. 13b) to be SelectModifier (resp. CheckSoucre) whereas sequence GINN produces correct
predictions for both programs, which are highlighted within the shadow boxes.

Table 2. The first two rows show the results for sequence GINN and sequence GNN; next two rows are the
results of the ablation study and the evaluation on alternative design.

Models
Sequence GNN

Sequence GINN

Configuration
Original
Original
Ablated
Alternative

F1
51.3%
60.2%
56.3%
52.2%

ROGUE-2 ROGUE-L

24.9%
29.5%
26.3%
24.8%

49.8%
54.7%
51.6%
50.4%

Scalability. In the same setup as the scalability experiment of variable misuse prediction task,
we investigate the scalability of both models. To strengthen sequence GNN, we incorporate all
additional edges to its program graphs (Allamanis et al. 2018). As depicted in Figure 14, under all
three metrics, sequence GINN outperforms sequence GNN throughout the entire test set, especially
in F1 where sequence GINN displays a wider margin over sequence GNN on larger graphs (i.e. last
5 sets of the graphs except the seventh set) than it does on smaller graphs (i.e. first 5 sets of graphs).

1
F

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Baseline

GINN

1

2

3

4

5

6

7

8

9

10

2
-
E
U
G
O
R

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Baseline

GINN

1

2

3

4

5

6

7

8

9

10

L
-
E
U
G
O
R

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Baseline

GINN

1

2

3

4

5

6

7

8

9

10

N-th Set (Ranked by Size of Graphs)

N-th Set (Ranked by Size of Graphs)

N-th Set (Ranked by Size of Graphs)

Fig. 14. Investigating the scalability of all four neural architectures.

Ablation Study. We conduct the same ablation study to quantify the influence of the graph
abstraction method on the performance of sequence GINN. As shown in Table 2, without the graph
abstraction operators, sequence GINN still performs somewhat better than the baseline but notably
worse than the original configuration, indicating the considerable influence of the abstraction
method on sequence GINN’s performance.
Alternative Design. Following the evaluation of the alternative design in variable misuse predic-
tion, we adopt the same approach to initializing the state of freshly created nodes. The last row in
Table 2 shows that this design causes GINN to be less accurate by a fairly wide margin.

4.5 Detection of Null Pointer Dereference
First, we present a framework for creating neural bug detectors in catching null pointer dereference.
Then, we describe our experiment setup. Finally, we report our evaluation results.
Neural Bug Detection Framework. We devise a simpler program representation for this task.
Given a control flow graph, we only split a node of basic block into multiple nodes of statement
to aid the localization of bugs at the level of lines. When checking a method, we also include its

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:20

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

context by stitching the graphs of all its callers and callees to that of itself. Regarding the initial
state of each statement node, we use RNN to learn a representation of its token sequence. As
explained in Section 4.3.1, after each token is embedded into a numerical vector (by the initial
embedding layer), we feed the embeddings of all tokens in a sequence to the network whose final
hidden state (i.e. the last ht in Equation 13) will be extracted as the initial node state.

After GINN computes the state vector for each node, and in turn the entire graph (by aggregating
the states of all nodes), we train a feed-forward neural network (Svozil et al. 1997) to predict (using
the representation of the entire graph) whether or not a method is buggy. If it is, we will use the
same network to predict the bug locations. Our rationale is a graph as a whole provides stronger
signals for models to determine the correctness of a method. Correct methods will be refrained
from further predictions, leading to fewer false warnings. On the other hand, if a method is indeed
predicted to be buggy, our model will then pick the top N statements ranked by their probabilities
as potential bug locations. This prediction mode intends to provide flexibility for developers to
tune their analysis toward either producing less false warnings or identifying more bugs.
Dataset. We target Java code due to the popularity and availability of Java datasets. In addition to
bugs provided by existing datasets (Just et al. 2014; Saha et al. 2018; Ye et al. 2014), we extract addi-
tional bugs from Bugswarm projects (Tomassi et al. 2019) to enrich our dataset for this experiment.
For example, given a bug description ”Issue #2600…” contained in a commit message, we search
in the bug tracking system (e.g. Bugzilla) using id #2600 to retrieve the details of the bug, against
which we match pre-defined keywords to determine if it’s a null pointer dereference bug. Next,
we refer back to the commit to find out bug locations, specifically, we consider the lines that are
modified by this commit to be buggy and the rest to be clean. We acknowledge the location of a
bug and its fix may not be precisely the same thing. If there are ever cases where two locations
significantly differ, GINN and the compared baseline will be equally affected.

Any detection is in nature a classification problem having subjects of interest one class of the
data, and the rest the other. Therefore, in the context of bug detection, it’s also necessary to supply
the correct methods apart from the buggy methods for training. A natural approach would be
directly taking the fixed version of each buggy method. However, this approach is unlikely to work
well in practice. Because in a real problem setting, an analyzer will never set out to differentiate
the two versions (i.e. correct and buggy) of the same program, instead, it has to separate buggy
programs from correct programs that are almost guaranteed to be functionally different. For this
reason, we pair each buggy method with a correct method that is syntactically the closest (defined
by the tree edit distance between their AST) from the same project. However, due to the shortage
of bug instances in existing datasets even after taking into account the additional bugs we extracted
ourselves, we include more correct methods w.r.t. each buggy method to further enlarge our dataset.
Table 7 in Appendix J shows the details including the projects from which we extract the code
snippet, a brief description of their functionality, size of their codebase, and the number of buggy
methods we extract from each project. We maintain an approximately 3:1 ratio between the correct
and buggy methods for each project. In total, we use 64 projects for training and 13 for test.
Objective, Metric, and Baseline. We evaluate how accurately the neural bug detector can localize
a null pointer dereference within a method. Because warning developers about buggy functions
hardly makes a valuable tool. As we deal with an unbalanced dataset (i.e. cleaning lines are more
than buggy lines), we forgo the accuracy metric since a bug detector predicting all lines to be correct
would yield a decent accuracy but make a totally useless tool. Instead, we opt for precision, recall,
and F1 score (Equation 18–20 in Appendix K), metrics are commonly used in defect prediction
literature (Pradel and Sen 2018; Wang et al. 2016). To show our interval-based graph abstraction
method can improve a variety of graph models, we use the standard GNN to build a baseline bug

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:21

Table 3. Precision.
Methods Top-1 Top-3 Top-5
0.147
0.209
0.117
GNN (0)
0.166
0.131
GNN (1)
0.285
0.138
0.218
0.101
GNN (2)
0.174
0.326
0.138
GINN (0)
0.167
0.130
GINN (1)
0.351
0.195
0.305
0.136
GINN (2)
0.351 - 0.285 = 0.066
Gain

Table 4. Recall.
Methods Top-1 Top-3 Top-5
0.373
0.243
GNN (0)
0.450
0.252
0.171
0.302
GNN (1)
0.337
0.210
0.375
GNN (2)
0.374
0.282
0.447
GINN (0)
0.431
0.329
GINN (1)
0.507
0.428
0.304
0.500
GINN (2)
0.507 - 0.450 = 0.057
Gain

Table 5. F1 score.
Methods Top-1 Top-3 Top-5
0.211
0.186
GNN (0)
0.224
0.201
0.214
0.183
GNN (1)
0.196
0.214
0.159
GNN (2)
0.238
0.303
0.211
GINN (0)
0.241
0.207
GINN (1)
0.339
0.268
0.304
0.214
GINN (2)
0.339 - 0.224 = 0.115
Gain

detector for this experiment. We do not include classical static analysis tools as additional baselines
since comparing them against learning-based bug detectors is likely to be unfair due to the noise
issue raised earlier. That is static analyzers could very well discover hidden bugs or report different
locations of the same bug. Properly handling those situations require manual inspection, which is
hard to scale. On the other hand, machine learning models are less susceptible to this problem as
the training set yield in principle the same distribution of the test set.
Implementation. We construct the control flow and interval graphs using Spoon (Pawlak et al.
2015), an open-source library for analyzing Java source code. To efficiently choose correct methods
to pair with a buggy method, we run DECKARD (Jiang et al. 2007), a clone detection tool adopting
an approximation of the shortest tree edit distance algorithm to identify similar code snippets.
To realize the neural bug detector powered by GNN, we make two major changes to Allamanis
et al. (2018)’s implementation of GGNN. First, we adopt the method defined in Equation 2 and 3
for updating the states of nodes in the message-passing procedure. Second, we design two loss
functions both in the form of cross-entropy for the bug detection task. The first one is designed
for the classification of buggy and correct methods and the other for the classification of buggy
and clean lines inside of a buggy method. Next, we realize the graph abstraction method within
the implementation of GNN-powered bug detector to build GINN-powered bug detector. Similar
to prior tasks, we implement only one abstraction cycle for GINN. For the remaining GGNN’s
parameters after our re-implementation, we keep them in both bug detectors.
Performance. Table 3–5 depict the precision, recall, and F1 for both bug detectors. We also
examine the impact of the program context — denoted by the number in parenthesis — on the
performance of each bug detector. More concretely, (0) means each method will be handled by itself.
(1) stitches control flow graphs of all callers and callees (denoted by F ) to that of the target method,
and (2) further integrates the graphs of all callers and callees of every method in F . Exceeding 2
has costly consequences. First, most data points will have to be filtered out for overflowing the
GPU memory. Moreover, a significant portion of the remaining ones would also have to be placed
in a batch on its own, resulted in a dramatic increase in training time. We provide the same amount
of context for each method in the test set as we do in the training set to maintain a consistent
distribution across the entire dataset. Columns in each table correspond to three prediction modes
in which models pick 1, 3, or 5 statements to be buggy after a method is predicted buggy.

Overall, the neural bug detector built out of GINN consistently outperforms the baseline using
all proposed metrics, especially in F1 score where GINN beats GNN by more than 10%. Regarding
the impact of the context, we find that more context mostly but not always leads to improved

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:22

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

performance of either bug detector. The reason is, on one hand, more information will always be
beneficial. On the other hand, exceedingly large graphs hinders the generalization of graph models,
resulted in the degraded performance of the neural bug detectors.
Scalability. Similar to the prior studies, we investigate the scalability of both neural bug detectors.
Again, we have strengthened the baseline by adding extra edges to its program graphs (Allamanis
et al. 2018). We fix the context for each method to be 2, which provides the largest variation in
graph size. Figure 15 depicts the precision, recall, and F1 score for GINN and the baseline under
top-1 prediction mode (top-3 and top-5 yield similar results). Even though GINN was beaten by
GNN on few subsets of small graphs, it consistently outperforms the baseline as the size of graphs
increases. Overall, we conclude bug detectors built out of GINN are more scalable.

n
o
i
s
i
c
e
r
P
1
-
p
o
T

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

GNN

GINN

2

3

4

5

6

7

8

9

10

l
l
a
c
e
R
1
-
p
o
T

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

GNN

GINN

2

3

4

5

6

7

8

9

10

e
r
o
c
S
1
F
1
-
p
o
T

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

GNN

GINN

2

3

4

5

6

7

8

9

10

N-th Set (Ranked by Size of Graphs)

N-th Set (Ranked by Size of Graphs)

N-th Set (Ranked by Size of Graphs)

Fig. 15. Investigating the scalability of baseline and GINN.

Catching Bugs in the Wild. We conduct another study to test if the GINN-based bug detector
can discover new bugs in the real world. We set up both GINN-based bug detector (previously
trained on 64 projects and configured with top-1 prediction mode) and Facebook Infer (Berdine
et al. 2006; Calcagno et al. 2015), arguably the state-of-the-art static analysis tool for Java code, to
scan the codebase of 20 projects on GitHub that are highly starred and actively maintained. Note
that although Infer is sound by design (based on abstract interpretation (Cousot and Cousot 1977)),
it makes engineering compromises for the sake of usability, thus triggering both false positives
and false negatives in practice. On the other hand, GINN-powered bug detector is by nature an
unsound tool, therefore we only study the utility of both tools in engineering terms.

After manually inspecting all alarms produced by both tools, We confirmed 38 bugs out of 102
alarms GINN-based bug detector raises, which rounds up to a precision of 37.3%. In comparison,
Facebook Infer emitted 129 alarms out of which 34 are confirmed (i.e. precision = 26.3% ). To
understand why our bug detector is more precise, we make several important observations about
the behavior of both tools through our manual inspection. All examples are included in Appendix L.
First, Infer in general raised more alarms that are provably false (e.g. Figure 19, 21, and 23) and
missed more bugs that are demonstrably real (e.g. Figure 25 and 26, both of which have been fixed
after our reporting) than GINN-based bug detector. Second, Infer raised many alarms due to its
confusion of some common APIs (e.g. Figure 27 and 29). Third, regarding path-sensitivity, there’s
still much room for infer to improve. For example, the path along which the bug is reported by
Infer (Figure 31) is clearly unsatisfiable. Specifically, the branch (Line 112) that triggers the null
pointer exception is guarded against the same condition under which the supposed null pointer is
in fact instantiated properly (Line 100). We have manually verified that no code in between changes
the evaluation of the path condition at Line 111. Unlike Facebook Infer, GINN-based bug detector

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:23

searches for bug patterns it learned from the training data. Because a large number of the false
alarms Infer reports clearly deviate from the norm exhibited in the training data, GINN-based bug
detector is better at suppressing them, resulted in a higher precision.

We also look into wrong predictions our bug detector made, and find that it tends to struggle
when a bug pattern is not local — the dereference is quite distant from where the null pointer is
assigned. We conjecture that the cause is the signal networks receive gets weaker when a scattered
bug pattern is overwhelmed by irrelevant statements. To address this issue, we apply the slicing
technique (Weiser 1981) and obtain encouraging results in a preliminary study. By simply slicing
the graph of each tested method, the same model (without retraining) can now detect 39 bugs out
of 93 alarms, a moderate improvement over the previous result. We plan to test this idea out more
systematically and at a larger scale.

We reported the confirmed bugs (i.e. 38 in total mentioned above) that GINN-based bug detector
found, out of which 11 are fixed and another 12 are confirmed (fixes pending). Reasons for
unconfirmed bugs are: (1) no response from developers; (2) requiring actual inputs that trigger the
reported bugs; or (3) third party code developers did not write themselves. Our findings show that
GINN-based bug detector is capable of catching new bugs in real-world Java projects.

4.6 Discussion
We summarize the lessons learned from our extensive evaluation. First and foremost, the interval-
based graph abstraction method has shown to improve the generalization of various graph-based
models (e.g. standard GNN, GGNN, RNN Sandwich) across all three downstream PL tasks. In each
task, the GINN-based model outperforms a GNN-based model — on a dataset that the GNN-based
model achieves state-of-the-art results — by around 10% in a metric adopted by the GNN-based
model. Based on the collective experience of the community (Appendix M), it is evident GINN has
significantly advanced the state-of-the-art GNN as a general, powerful model in learning semantic
program embeddings. Second, our evaluation reveals the scalability issues existing graph models
suffer from — manifested in the significant performance drop against the increasing size of graphs
— and suggests an effective solution built out of the graph abstraction method. In fact, the overall
accuracy improvement mentioned earlier is in large part attributed to GINN’s efficient handling
of the large graphs. Finally, our evaluation also shows model-based static analyzers can be a
promising alternative to the classical static analysis tools for catching null pointer dereference bugs.
Even though our results are still preliminary for drawing a general conclusion on the comparison
between statistical-based and logic-based tools, it shows our neural bug detector is not only notably
more precise than Infer but also capable of catching new bugs on many popular codebases on
GitHub.

5 RELATED WORK
In this section, we survey prior works on learning models of source code. Hindle et al. (2012)
pioneer the field of machine learning for source code. In particular, Hindle et al. find that programs
that people write are mostly simple and rather repetitive, and thus they are amenable to statistical
language models. Their finding is based on the evidence that the n-gram language model captures
regularities in software as well as it does in English.

Later, many works propose to elevate the learning from the level of token sequences (Gupta et al.
2017; Hindle et al. 2012; Nguyen et al. 2013; Pu et al. 2016) to that of abstract syntax trees (Alon et al.
2019a,b; Maddison and Tarlow 2014) in an attempt to capture the structure properties exhibited in
source code. Notably, Alon et al. (2019b) present a method for function name prediction. Specifically,

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:24

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

it decomposes a program to a collection of paths in its abstract syntax tree, and learns the atomic
representation of each path simultaneously with learning how to aggregate a set of them.

Nowadays, graph neural networks have become undoubtedly the most popular deep model of
source code. Since the introduction of graph neural networks to the programming domain (Alla-
manis et al. 2018; Li et al. 2016), they have been applied to a variety of PL tasks, such as program
summarization (Fernandes et al. 2019), bug localization and fixing (Dinella et al. 2020), and type
inference (Wei et al. 2020). Apart from being thoroughly studied and constantly improved in the
machine learning field, GNN’s capability of encoding semantic structure of code through a graph
representation is a primary contributor to their success. While our work also builds upon GNN, we
offer a fundamentally different perspective which no prior works have explored. That is program
abstraction helps machine learning models to capture the essence of the semantics programs denote,
and in turn facilitating the execution of downstream tasks in a precise and efficient manner.

In parallel to all the aforementioned works, a separate line of works has emerged recently
that use program executions (i.e. dynamic models) (Wang 2019; Wang et al. 2018; Wang and Su
2020) rather than source code (i.e. static models) for learning program representations. Their
argument is that source code alone may not be sufficient for models to capture the semantic
program properties. Wang and Christodorescu (2019) show simple, natural transformations, albeit
semantically-preserving, can heavily influence the predictions of models learned from source
code. In contrast, executions that offer direct, precise, and canonicalized representations of the
program behavior help models to generalize beyond syntactic features. On the flip side, dynamic
models are likely to suffer from the low quality of training data since high-coverage executions are
hard to obtain. Wang and Su (2020) address this issue by blending both source code and program
executions. Our work is set to improve static models via program abstraction, therefore we don’t
consider runtime information as a feature dimension, which can be an interesting future direction
to explore.

6 CONCLUSION
In this paper, we present a new methodology of learning models of source code. In particular, we
argue by learning from abstractions of source code, models have an easier time to distill the key
features for program representation, thus better serving the downstream tasks. At a technical
level, we develop a principled interval-based abstraction method that directly applies to control
flow graph. This graph abstraction method translates to a loop-based program abstraction at
the source code level, which in essence makes models focus exclusively on looping construct for
learning feature representations of source code. Through a comprehensive evaluation, we show
our approach significantly outperforms the state-of-the-art models in two highly popular PL tasks:
variable misuse and method name prediction. We also evaluate our approach in catching null
pointer dereference bugs in Java programs. Results again show GINN-based bug detector not only
beats the GNN-based bug detector but also yields a lower false positive ratio than Facebook Infer
when deployed to catch null pointer deference bugs in the wild. We reported 38 bugs found by
GINN to developers, among which 11 have been fixed and 12 have been confirmed (fixing pending).
GINN is a general, powerful deep neural network that we believe is readily available to tackle a
wide range of problems in program analysis, program comprehension, and developer productivity.

REFERENCES
Miltiadis Allamanis. 2019. The Adverse Effects of Code Duplication in Machine Learning Models of Code. In Proceedings of
the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software (Onward! 2019).

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:25

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to represent programs with graphs.

International Conference on Learning Representations (2018).

Frances E. Allen. 1970. Control Flow Analysis. In Proceedings of a Symposium on Compiler Optimization.
Uri Alon, Omer Levy, and Eran Yahav. 2019a. code2seq: Generating sequences from structured representations of code.

International Conference on Learning Representations (2019).

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019b. Code2Vec: Learning Distributed Representations of Code.

Proc. ACM Program. Lang. POPL (2019).

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and

translate. International Conference on Learning Representations (2015).

Josh Berdine, Cristiano Calcagno, and Peter W. O’Hearn. 2006. Smallfoot: Modular Automatic Assertion Checking with
Separation Logic. In Proceedings of the 4th International Conference on Formal Methods for Components and Objects
(FMCO’05).

Cristiano Calcagno, Dino Distefano, Jeremy Dubreil, Dominik Gabi, Pieter Hooimeijer, Martino Luca, Peter O’Hearn, Irene
Papakonstantinou, Jim Purbrick, and Dulma Rodriguez. 2015. Moving Fast with Software Verification. In NASA Formal
Methods, Klaus Havelund, Gerard Holzmann, and Rajeev Joshi (Eds.).

Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).

P. Cousot and R. Cousot. 1977. Abstract interpretation: a unified lattice model for static analysis of programs by construction
or approximation of fixpoints. In Conference Record of the Fourth Annual ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and
robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers).

Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. 2020. Hoppity: Learning Graph Transforma-

tions to Detect and Fix Bugs in Programs. International Conference on Learning Representations (2020).

Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured Neural Summarization. International

Conference on Learning Representations (2019).

Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. 2017. Neural Message Passing for
Quantum Chemistry. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 (ICMLfi17).
Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in graph domains. In Proceedings.

2005 IEEE International Joint Conference on Neural Networks, 2005.

Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix: Fixing Common C Language Errors by Deep

Learning. In Thirty-First AAAI Conference on Artificial Intelligence.

Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. 2020. Global Relational Models

of Source Code. International Conference on Learning Representations (2020).

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the Naturalness of Software. In

Proceedings of the 34th International Conference on Software Engineering (ICSE fi12).

Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long Short-term Memory. Neural computation (1997).
L. C. Jain and L. R. Medsker. 1999. Recurrent Neural Networks: Design and Applications (1st ed.). CRC Press, Inc., USA.
L. Jiang, G. Misherghi, Z. Su, and S. Glondu. 2007. DECKARD: Scalable and Accurate Tree-Based Detection of Code Clones.

In 29th International Conference on Software Engineering (ICSE’07).

Ren´e Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J: A database of existing faults to enable controlled testing
studies for Java programs. In Proceedings of the 2014 International Symposium on Software Testing and Analysis. ACM,
437–440.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2016. Gated graph sequence neural networks. International

Conference on Learning Representations (2016).

Chris Maddison and Daniel Tarlow. 2014. Structured generative models of natural source code. In International Conference

on Machine Learning.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and

Phrases and Their Compositionality. In Neural Information Processing Systems (NIPS).

Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2013. A Statistical Semantic Language
Model for Source Code. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE
2013).

Renaud Pawlak, Martin Monperrus, Nicolas Petitprez, Carlos Noguera, and Lionel Seinturier. 2015. Spoon: A Library for

Implementing Analyses and Transformations of Java Source Code. Software: Practice and Experience (2015).

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:26

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

Michael Pradel and Koushik Sen. 2018. Deepbugs: a learning approach to name-based bug detection. Proceedings of the

ACM on Programming Languages OOPSLA (2018).

Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay. 2016. Sk P: A Neural Program Corrector
for MOOCs. In Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming,
Languages and Applications: Software for Humanity (SPLASH).

Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic Model for Code with Decision Trees. In Proceedings of
the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications
(OOPSLA 2016).

Ripon Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul Prasad. 2018. Bugs. jar: a large-scale, diverse dataset of
real-world java bugs. In 2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR). IEEE, 10–13.
Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, and Le Song. 2018. Learning Loop Invariants for Program
Verification. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPSfi18).
Daniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal. 1997. Introduction to multi-layer feed-forward neural networks.

Chemometrics and intelligent laboratory systems (1997).

David A Tomassi, Naji Dmeiri, Yichen Wang, Antara Bhowmick, Yen-Chuan Liu, Premkumar T Devanbu, Bogdan Vasilescu,
and Cindy Rubio-Gonz´alez. 2019. Bugswarm: mining and continuously growing a dataset of reproducible failures and
fixes. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 339–349.

Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh singh. 2019. Neural Program Repair by Jointly

Learning to Localize and Repair. International Conference on Learning Representations (2019).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser, and Illia

Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer Networks. In Advances in Neural Information Processing

Systems 28.

Ke Wang. 2019. Learning Scalable and Precise Representation of Program Semantics. arXiv preprint arXiv:1905.05251 (2019).
Ke Wang and Mihai Christodorescu. 2019. COSET: A Benchmark for Evaluating Neural Program Embeddings. arXiv preprint

arXiv:1905.11445 (2019).

Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Dynamic Neural Program Embedding for Program Repair. International

Conference on Learning Representations (2018).

Ke Wang and Zhendong Su. 2020. Blended, Precise Semantic Program Embeddings. In Proceedings of the 41st ACM SIGPLAN

International Conference on Programming Language Design and Implementation (PLDI ’20).

Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic features for defect prediction. In 2016 IEEE/ACM

38th International Conference on Software Engineering (ICSE). IEEE.

Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. 2020. LambdaNet: Probabilistic Type Inference using Graph Neural

Networks. In International Conference on Learning Representations.

Mark Weiser. 1981. Program Slicing. In Proceedings of the 5th International Conference on Software Engineering (ICSE fi81).
Xin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to rank relevant files for bug reports using domain knowledge. In
Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM, 689–699.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:27

Appendix

A LOSS FUNCTION
We present the loss function of the joint model below. First, we use the cross-entropy loss to specify
the error between the predicted location and the true distribution, Loc, of the misuse variable.

H (Loc, Ppro) = −

(cid:213)

i ∈[0,k )

Loc[i]logPpro[i][0] = −logPpro[it r ue ][0]

where it rue is the actual index of the misuse variable in an input program. That is, the loss is the
negative logarithm of Ppro[it rue ][0], the probability that the model assigns to the token at index it r ue .
As Ppro[it rue ][0] tends to 1, the loss approaches zero. The further Ppro[it r ue ][0] goes below 1, the
greater the loss becomes. Thus, minimizing this loss is equivalent to maximizing the log-likelihood
that the model assigns to the true labels it rue . Similarly, the cross-entropy loss for repair variable is:

H (Rep, Ppro) = −

(cid:213)

i ∈[0,k )

Rep[i]logPpro[i][1] = −logPpro[it rue ][1]

The network will be trained to minimize both H (Loc, Ppro) and H (Rep, Ppro). For inference, the
network predicts the token at iLoc (resp. iRep) to be the misuse (resp. repair) variable.

iLoc = argmax
i ∈[0,k)
iRep = argmax
i ∈[0,k )

Ppro[i][0]

Ppro[i][1]

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:28

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

B RECURRENT NEURAL NETWORK
A recurrent neural network (RNN) (Jain and Medsker 1999) is a class of artificial neural networks
that are distinguished from feedforward networks by their feedback loops. This allows RNNs to
ingest their own outputs as inputs. It is often said that RNNs have memory, enabling them to
process sequences of inputs.

Here we briefly describe the computation model of a vanilla RNN. Given an input sequence,
embedded into a sequence of vectors x = (x1, · · ·, xTx ), an RNN with N inputs, a single hidden layer
with M hidden units, and Q output units. We define the RNN’s computation as follows:

ht = f (W ∗ xt + V ∗ ht −1)
ot = softmax(Z ∗ ht )

(17)

where xt ∈ RN , ht ∈ RM , ot ∈ RQ is the RNN’s input, hidden state and output at time t, f is a
non-linear function (e.g. tanh or sigmoid), W ∈ RM ∗N denotes the weight matrix for connections
from input layer to hidden layer, V ∈ RM ∗M is the weight matrix for the recursive connections (i.e.
from hidden state to itself) and Z ∈ RQ ∗M is the weight matrix from hidden to the output layer.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:29

C PARTITIONING THE NEW GRAPHS INTO INTERVALS
We split our exposition into two steps: convert standard control flow graph to statement-based
control flow graph and from statement-based control flow graph to token-based control flow graphs.
Recall the definition of intervals: one node, called header, is the only entry node of a subgraph
in which all closed paths contain the header node. In both steps, the intervals on the original
control flow graph will be preserved because (1) there is no external node that connects to any
statement or token node (2) the closed path is fundamentally the same as before except there will be
more statement/token nodes on the path. Therefore, we say intervals consist of the same program
statements regardless of the conversion from the standard to token-based control flow graph.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:30

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

D DATASET GENERATION
We used the ETH Py150 dataset (Raychev et al. 2016), which is based on GitHub Python code,
and already partitioned into train and test splits (100K and 50K files, respectively). We further
split the 100K train files into 90K train and 10K validation examples and applied a deduplication
step on that dataset (Allamanis 2019). We extracted all top-level function definitions from these
files; any function that uses multiple variables can be turned into a training example by randomly
replacing one variable usage with another. As there may be many candidates for such bugs in a
function, Hellendoorn et al. (2020) limited their extraction to up to three samples per function to
avoid biasing the dataset too strongly towards longer functions. Since an important goal of our
work is to improve the model scalability, we included more longer functions by creating 4 samples
per function. To keep our dataset the same size as Hellendoorn et al. (2020)’s (only for training and
testing as they did not report the size of their validation set), we trimmed small functions from the
other end. For every synthetically generated buggy example, an unperturbed, bug-free example of
the function is included as well to keep our dataset balanced. Our dataset contains 2M programs
for training, 245K for validation, and 755K for testing.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:31

E CONVERTING AST TO GINN’S GRAPH REPRESENTATION
First, we walk each AST to distill all the statement nodes; then, for each statement node, we
identify its immediate successor, such as the next statement within the same basic block, or the first
statement of another basic block (due to control constructs like if statement or for loop). Next,
we add an edge from each statement node to its immediate successor. Finally, we replace statement
nodes with sequences of token nodes, and move the start and end node for each control flow edge
from statement nodes to their first token nodes.

Note that all programs used in (Hellendoorn et al. 2020) consist of single functions only, therefore

we don’t need to consider method calls when converting AST to CFG.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:32

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

F REPRODUCING THE RESULTS REPORTED IN PRIOR WORKS
We compare the performance of our own model implementations again those reported in (Hel-
lendoorn et al. 2020). In particular, we measure the independent localization and repair accuracy,
the metrics adopted by (Hellendoorn et al. 2020), of the core models instantiated by GGNN and
RNN Sandwich. Table 6 shows the model performance is close across both neural architectures,
signaling the validity of both our model implementations and data collection.

Note that the accuracy below is what models achieved on the training set. Table 1 in the paper
shows the results of both GGNN and GGNN-powered RNN Sandwich on the test set, which is around
5%–10% lower than what’s reported in (Hellendoorn et al. 2020).5 We conjecture the accuracy drop
is likely caused by the higher difficulty level of our testing set. As explained in Appendix D, our
dataset contains more longer functions. Specifically, programs exceeding 250 tokens only make up
6.5% of their test set (Hellendoorn et al. 2020) whereas programs of the same size take around 14% in
our test set. In addition, our experiment mainly focuses on the improvement of GINN over GGNN
(or GNN). Since both models are implemented and executed with the exact same configurations (i.e.
server configuration, number of GPU cores, Tensorflow version, etc.), we believe our experimental
results for variable misuse prediction task is legitimate.

Table 6

Models

GGNN (reported)
GGNN (self-implemented)
RNN Sandwich
(reported)
RNN Sandwich
(self-implemented)

Localization
Accuracy
79%
77%

Repair
Accuracy
74%
71%

81%

80%

86%

84%

5Hellendoorn et al. (2020) used classification accuracy and location+repair accuracy only for the test set.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:33

G TRAINING TIME
In this section, we report the training time for every evaluated model under each program analysis
task. We ignore the inference time as all models make predictions instantaneously.
Variable Misuse Prediction Figure 16 presents the training time of all evaluated models in variable
misuse prediction task. We use the localization accuracy as other metrics show the same trend.

y
c
a
r
u
c
c
a
n
o
i
t
a
z
i
l
a
c
o
l

t
s
e
T

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

GGNN
Sand
30

20

GINN
I-Sand
50

60

40

0

10

Training Time (Hours)

Fig. 16. Models’ training time for variable misuse prediction.

Method Name Prediction Figure 17 presents the training time of all evaluated models in the
method name prediction task.

GGNN

GINN

0.8

0.6

0.4

0.2

t
e
s

t
s
e
t
n
o
d
e
v
e
i
h
c
a
1
F

0

0

20

40

60

80

100

120

Training Time (Minutes)

Fig. 17. Models’ training time for method name prediction.

Null Pointer Dereference Figure 18 presents the training time of all evaluated models in null
pointer dereference detection task. Y-axis denotes precision score models achieved on the test set
(with context (1)). Recall and F1 shows a similar trend.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:34

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

1
-
p
o
T
n
i
n
o
i
s
i
c
e
r
P
t
s
e
T

0.5

0.4

0.3

0.2

0.1

0

0

GNN

GINN

10

20

30

40

50

60

Training Time (Minutes)

Fig. 18. Models’ training time for null pointer dereference detection.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:35

H ADDITION EDGES PROPOSED BY ALLAMANIS ET AL.
Below we list the edges Allamanis et al. (2018) designed on top of AST.

• NextToken connects each terminal node (syntax token) to its successor.
• LastRead connects a terminal node of a variable to all elements of the set of terminal

nodes at which the variable could have been read last.

• LastWrite: connects a terminal node of a variable to all elements of the set of syntax

tokens at which the variable was could have been last written to.

• ComputedFrom connects a terminal node of a variable v to all variable tokens occurring

in expr when expr is assigned to v.

• LastLexicalUse chains all uses of the same variable.
• ReturnsTo connects return tokens to the method declaration.
• FormalArgName connects arguments in method calls to the formal parameters that they

are matched to.

• GuardedBy connects every token corresponding to a variable (in the true branch of a if

statement) to the enclosing guard expression that uses the variable.

• GuardedByNegation connects every token corresponding to a variable (in the false branch

of a if statement) to the enclosing guard expression that uses the variable.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:36

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

I METRICS USED IN METHOD NAME PREDICTION
We adopted the measure used by previous works (Alon et al. 2019a,b; Fernandes et al. 2019; Wang and
Su 2020) in method name prediction, which measured F1 score (i.e. the harmonic mean of precision
and recall) over subtokens, case-insensitive. The intuition is the quality of a method name prediction
largely depends on the constituent sub-words. For example, for a method called computeDiff,
a prediction of diffCompute is considered as an exact match, a prediction of compute has a full
precision but low recall, and a prediction of computeFileDiff has a full recall but low precision.

Below is extracted from https:// en.wikipedia.org/wiki/ ROUGE (metric).
ROUGE, which stands for Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics
for evaluating automatic summarization and machine translation in natural language processing.
The metrics compare an automatically produced summary or translation against a reference or a
set of references (human-produced) summary or translation.

• ROUGE-1 refers to the overlap of unigram (each word) between the system and reference

summaries.

• ROUGE-2 refers to the overlap of bigrams between the system and reference summaries.
• ROUGE-L: Longest Common Subsequence based statistics. Longest common subsequence
problem takes into account sentence level structure similarity naturally and identifies
longest co-occurring in sequence n-grams automatically.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:37

J DATASET FOR NPE DETECTION TASK
We give the details about the dataset we assembled for the evaluation of detecting null pointer
dereference.

Size (KLoC) Number of Buggy Methods

Table 7

Dataset

Test

Projects
Lang
Closure
Chart
Mockito
Math
Accumulo
Camel
Flink
Jackrabbit-oak
Log4j2
Maven
Wicket
Birt

Description
Java lang library
A JavaScript checker and optimizer
Java chart library
Mocking framework for unit tests
Mathematics and statistics components
Key/value store
Enterprise integration framework
System for data analytics in clusters
hierarchical content repository
Logging library for Java
Project management and comprehension tool
Web application framework
Data visualizations platform

50
260
149
45
165
194
560
258
337
70
62
206
1,093

Validation

SWT
Tomcat

Eclipse Platform project repository
Web server and servlet container

460
222

Number of Buggy Methods in Total for Test
Number of Methods in Total for Test

Training

JDT UI
Platform UI
AspectJ
from BugSwarm

Number of Buggy Methods in Total for Validation
Number of Methods in Total for Validation

User interface for the Java IDE
User interface and help components of Eclipse
An aspect-oriented programming extension
61 projects on GitHub
Number of Buggy Methods in Total for Training
Number of Methods in Total for Training

508
595
289
5,191

7
18
17
14
16
6
17
13
23
29
6
7
678
793
3,000
276
111
387
1,000
897
920
151
156
2,124
9,000

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:38

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

K PRECISION, RECALL AND F1 SCORE
Below we explain precision, recall and F1 score for bug detection.

Precision =

Recall =

F1 Score =

TP
TP + FP
TP
TP + FN
2 ∗ Precision ∗ Recall
Precision + Recall

(18)

(19)

(20)

TP, FP, and FN denote true positive, false positive, and false negative respectively. True positive
is the number of the predicted buggy lines that are truly buggy, while false positive is the number
of predicted buggy line that are not buggy. False negative records the number of buggy lines that
are predicted as non-buggy. A higher precision suggests a relatively low number of false alarms
while high recall indicates a relatively low number of missed bugs. F1 takes both precision and
recall into consideration.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:39

L MANUAL INVESTIGATION OF THE BUG REPORTS
Below is the program extracted from https://github.com/crossoverJie/JCSprout/blob/master/src/m
ain/java/com/crossoverjie/algorithm/TwoSum.java.

int [] result = new int [2] ;
Map < Integer , Integer > map = new HashMap < >(2) ;
for ( int i =0; i < nums . length ;i ++){

48 public int [] getTwo2 ( int [] nums , int target ){
49
50
51
52
53
54
55
56
57
58
59 }

}
map . put ( target - nums [i],i) ;

}
return result ;

if ( map . containsKey ( nums [i ])){

result = new int []{ map . get ( nums [i ]) , i} ;

Fig. 19

Below is the report produced by Facebook Infer for the program in Figure 19. The alarm is proven
to be false.

Fig. 20

Below is the program extracted from https://github.com/alibaba/Sentinel/blob/master/sentinel-cor
e/src/main/java/com/alibaba/csp/sentinel/node/metric/MetricTimerListener.java.

for ( Entry < Long , MetricNode > entry : metrics . entrySet ()) {

long time = entry . getKey ();
MetricNode metricNode = entry . getValue ();
metricNode . setResource ( node . getName ());
metricNode . setClassification ( node . getResourceType ());
if ( maps . get ( time ) == null ) {

59 private void aggregate ( Map < Long , List < MetricNode >> maps , Map < Long , MetricNode > metrics , ClusterNode node ) {
60
61
62
63
64
65
66
67
68
69
70
71 }

}
List < MetricNode > nodes = maps . get ( time );
nodes . add ( entry . getValue ());

maps . put ( time , new ArrayList < MetricNode >());

}

Fig. 21

Figure 22 shows the report produced by Facebook Infer for the program in Figure 21. This alarm is
also proven to be false.
Figure 23 shows the program extracted from https://github.com/stagemonitor/stagemonitor/blob/0.
89.0/stagemonitor-tracing/src/main/java/org/stagemonitor/tracing/profiler/CallStackElement.ja
va.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:40

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

Fig. 22

return ;

child . recycle ();

if (! useObjectPooling ) {

}
children . clear ();
objectPool . offer ( this );

}
parent = null ;
signature = null ;
executionTime = 0;
for ( CallStackElement child : children ) {

75 public void recycle () {
76
77
78
79
80
81
82
83
84
85
86
87 }
88
89 public void removeCallsFasterThan ( long thresholdNs ) {
90
91
92
93
94
95
96
97
98
99 }
100
101 public boolean isIOQuery () {
102
103
104 }

child . removeCallsFasterThan ( thresholdNs );

iterator . remove ();
child . recycle ();

} else {

}

}

for ( Iterator < CallStackElement > iterator = children . iterator (); iterator . hasNext (); ) {

CallStackElement child = iterator . next ();
if ( child . executionTime < thresholdNs && ! child . isIOQuery () ) {

// that might be a bit ugly , but it saves reference to a boolean and thus memory
return signature . charAt ( signature . length () - 1) == ' ';

Fig. 23

Figure 24 is the report produced by Facebook Infer for the program in Figure 23. The reason Infer
think a npe is thrown by child.isIOQuery on line 92 is because inside of child.recycle(),
called at line 94, signature will be assigned as null (line 80). Then in the next iteration when
child.isIOQuery is invoked charAt function will be called on a null pointer at line 103. However,
this report is provably false because the statement at line 91 guarantees child object refers to
different memory cells in an array during each iteration, meaning recyle function will never be
called on the same object as isQuery.

Fig. 24

Figure 25 shows the program extracted from https://github.com/winder/Universal-G-Code-Sender/
blob/54a1db413980d4c90b5b704c2723f0e740de20be/ugs-core/src/com/willwinder/universalgcode
sender/i18n/Localization.java. When region is null at line 7 inside of function getString, a null

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:41

pointer exception will be thrown in function initialize at line 2. This bug, which has been fixed
after our reporting, is missed by Facebook Infer but caught by GINN-based bug detector.

String [] lang = language . split ("_" );
return initialize ( lang [0] , lang [1]);

1 synchronized public static boolean initialize ( String language ) {
2
3
4 }
5
6 public static String getString ( String id , String region ) {
7
8
9
10
11 }

if ( region == null || ! region . equals ( Localization . region )) {

}
return getString ( id );

initialize ( region );

Fig. 25

Figure 26 is the program extracted from https://github.com/brianfrankcooper/YCSB/blob/cd1589ce
6f5abf96e17aa8ab80c78a4348fdf29a/jdbc/src/main/java/site/ycsb/db/JdbcDBClient.java. If driver
is checked against null at line 9, it is reasonable to add a check before calling driver.contains at
line 3. Like the previous bug, this one is also missed by Facebook Infer but caught by GINN-based
bug detector, in addition, it is fixed after we report the bug.

sqlserver = true ;

1 String driver = props . getProperty ( DRIVER_CLASS );
2
3 if ( driver . contains ( " sqlserver " )) {
4
5 }
6
7 ...
8 try {
9
10
11
12
13 ...
14 }

Class . forName ( driver );

if ( driver != null ) {

}

Fig. 26

Figure 27 is the program extracted from https://github.com/ctripcorp/apollo/blob/master/apollo-c
ore/src/main/java/com/ctrip/framework/foundation/internals/Utils.java. This alarm is provably
false, and the reason seems that Facebook Infer is confused by the API Strings.nullToEmpty in
com.google.common.base.Strings.
Figure 28 shows the report produced by Facebook Infer for the program in Figure 27. This is a false
alarm.
Figure 29 is the program extracted from https://github.com/brianfrankcooper/YCSB/blob/master/s
3/src/main/java/site/ycsb/db/S3Client.java. This alarm is provably false, and the reason seems that
Facebook Infer is confused by the API propsCL.getProperty("s3.protocol", "HTTPS"); (a
method in java.util.Properties) at line 210, which assigns a default value HTTPS to protocol.
Figure 30 shows the report produced by Facebook Infer for the program in Figure 29. This is another
false alarm.
Figure 31 is the program extracted from https://github.com/stagemonitor/stagemonitor/b
lob/0.89.0/stagemonitor- web- servlet/src/main/java/org/stagemonitor/web/servlet/f
ilter/HttpRequestMonitorFilter.java. The path on which Infer reports a NPE is infeasible.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:42

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

public static boolean isBlank ( String str ) {

return Strings . nullToEmpty ( str ). trim (). isEmpty ();

}

5 public class Utils {
6
7
8
9
10
11
12
13
14
15
16
17 }

return false ;

}

public static boolean isOSWindows () {

String osName = System . getProperty (" os . name " );
if ( Utils . isBlank ( osName )) {

}
return osName . startsWith (" Windows " );

Fig. 27

Fig. 28

protocol = propsCL . getProperty (" s3 . protocol " , " HTTPS " );

208 protocol = props . getProperty (" s3 . protocol " );
209 if ( protocol == null ){
210
211 }
212
213 ...
214 clientConfig = new ClientConfiguration ();
215 clientConfig . setMaxErrorRetry ( Integer . parseInt ( maxErrorRetry ));
216 if ( protocol . equals (" HTTP " )) {
217
218 } else {
219
220 }

clientConfig . setProtocol ( Protocol . HTTPS );

clientConfig . setProtocol ( Protocol . HTTP );

Fig. 29

Fig. 30

isInjectContentToHtml(request) if evaluated to be true, httpServletResponseBufferWrapper
was instantiated properly under the same condition. We have manually verified that no code
in between the two isInjectContentToHtml(request) (line 99 and 111) affects the output of
isInjectContentToHtml(request). We have attached a detailed call chain for monitorRequest
in the supplemental material. The remaining part of the code is straight-forward, and can be easily
checked.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:43

}

} else {

responseWrapper = new StatusExposingByteCountingServletResponse ( response );

httpServletResponseBufferWrapper = new HttpServletResponseBufferWrapper ( response );
responseWrapper = new StatusExposingByteCountingServletResponse ( httpServletResponseBufferWrapper );

final StatusExposingByteCountingServletResponse responseWrapper ;
HttpServletResponseBufferWrapper httpServletResponseBufferWrapper = null ;
if ( isInjectContentToHtml ( request )) {

95 private void doMonitor ( HttpServletRequest request , HttpServletResponse response , FilterChain filterChain ) {
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115 }

injectHtml ( response , request , httpServletResponseBufferWrapper );

monitorRequest ( filterChain , request , responseWrapper );

} catch ( Exception e) {
handleException (e );

if ( isInjectContentToHtml ( request )) {

} finally {

try {

}

}

Figure 32 shows the report produced by Facebook Infer for the program in Figure 31. This is a false
alarm.

Fig. 31

Fig. 32

List of fixed and confirmed bugs:
https://github.com/ctripcorp/apollo/issues/3012
https://github.com/spring-projects/spring-boot/issues/20890
https://github.com/SynBioDex/libSBOLj/issues/601
https://github.com/SynBioDex/libSBOLj/issues/603
https://github.com/konsoletyper/teavm/issues/438
https://github.com/brianfrankcooper/YCSB/issues/1371
https://github.com/ome/bioformats/issues/3464
https://github.com/nutzam/nutz/issues/1532
https://github.com/winder/Universal-G-Code-Sender/issues/1304
https://github.com/SeleniumHQ/selenium/issues/8183
https://github.com/SeleniumHQ/selenium/issues/8184
https://github.com/SeleniumHQ/selenium/issues/8185
https://github.com/kdn251/interviews/pull/159
https://github.com/spring-projects/spring-boot/issues/20901
https://github.com/crossoverJie/JCSprout/issues/195

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

1:44

Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang

M PUBLICATION HISTORY
Variable Misuse Prediction GGNN (Allamanis et al. 2018) is the first notable model, which
achieves 74.1% accuracy on a subset of MSR-VarMisuse. As depicted in Table 8, on the exact same
dataset using Allamanis et al. (2018)’s metric, the joint model proposed by Vasic et al. (2019) is in
fact notably worse than GGNN. The joint model performed better than GGNN on a new dataset
with a set of new metrics.

Table 8

Model

GGNN
Joint Model

MSR-VarMisuse
end-to-end
Accuracy
71.4%
62.3%

ETH-Py150

Classification
Accuracy
71.1%
82.4%

Localization
Accuracy
64.6%
71%

Localization+Repair
Accuracy
55.8%
65.7%

Improvement

-9.1%

11.3%

6.4%

9.9%

Given the conceptual framework laid out by the joint model, Hellendoorn et al. (2020) proposed
an engineering upgrade by simply replacing the RNN with a Transformer (Vaswani et al. 2017)
as the new instantiation of the core model. Table 9 shows how RNN Sandwich (the state-of-the-
art) compared to the engineering upgrade of the model proposed by Vasic et al. (2019). Note
that Hellendoorn et al. (2020) curated a different dataset than Vasic et al. (2019)’s from ETH-Py150.
≤250 (resp. 1000) represents the set of programs that consist of less than 250 (resp. 1000) tokens.

Table 9

Model

Transformer
RNN Sandwich

Classification Accuracy
≤ 250
75.9%
82.5%

≤ 1000
73.2%
81.9%

Location+Repair Accuracy
≤ 250
67.7%
75.8%

≤ 1000
63.0%
73.8%

Improvement

6.6%

8.7%

8.1%

10.8%

Method Name Prediction code2seq (Alon et al. 2019a) set the bar for large-scale, cross-project
method name prediction. Table 10 shows how it compares against a simple 2-layer bidirectional
LSTM on code2seq’s datasets: Java-small, Java-med, and Java-large.

Model

Precision

2-layer bi-LSTM 42.63%
50.64%
code2seq

Java-small
Recall
29.97% 35.20%
37.40% 43.02%

F1

Table 10

Precision
55.15%
61.24%

Java-med
Recall
41.75% 47.52%
47.07% 53.23%

F1

Java-large
Recall
48.77% 55.18%
55.02% 59.19%

F1

Precision
63.53%
64.03%

Improvement

8.01%

7.43%

7.82%

6.09%

5.32%

5.71%

0.50%

6.25%

4.01%

Table 11 shows how Sequence GNN (Fernandes et al. 2019) compares against code2seq on

Java-small.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

Learning Semantic Program Embeddings with Graph Interval Neural Network

1:45

Table 11

Model

F1
code2seq
43.0%
Sequence GNN 51.4%

Improvement

8.4%

Java-small

ROGUE-2 ROGUE-L

-
25.0
-

-
50.0
-

Table 12

Model

code2seq
Liger

Precision
32.95%
39.88%

Java-med
Recall
20.23% 25.07%
27.14% 32.30%

F1

Java-large
Recall
22.51% 27.84%
31.43% 36.42%

F1

Precision
36.49%
43.28%

Improvement

6.93%

6.91%

7.23%

6.79%

8.92%

8.58%

Wang and Su (2020) proposed a blended model, called Liger, that combines both static and
dynamic program features for predicting method names. Table 12 shows how Liger compares with
code2seq on a subset of Java-med and Java-large.

In conclusion, the improvement the GINN-based model made over the GNN-based model in
each PL task presented in this paper is comparable to that each aforementioned model made over
the prior state-of-the-art. More importantly, GINN-based models only work with the datasets on
which GNN-based models achieved state-of-the-art results and they are measured using the metrics
proposed by the GNN-based models.

Proc. ACM Program. Lang., Vol. 1, No. OOPSLA, Article 1. Publication date: January 2020.

