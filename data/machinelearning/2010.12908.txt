1
2
0
2

n
a
J

2
2

]
I

A
.
s
c
[

2
v
8
0
9
2
1
.
0
1
0
2
:
v
i
X
r
a

Deep Graph Matching and Searching for Semantic Code
Retrieval

XIANG LINGâˆ—, Zhejiang University, China
LINGFEI WUâˆ—, IBM T. J. Watson Research Center, USA
SAIZHUO WANG, Zhejiang University, China
GAONING PAN, Zhejiang University, China
TENGFEI MA, IBM T. J. Watson Research Center, USA
FANGLI XU, Squirrel AI Learning, USA
ALEX X. LIU, Ant Group, China
CHUNMING WUâ€ , Zhejiang University, China and Zhejiang Lab, China
SHOULING JIâ€ , Zhejiang University, China

Code retrieval is to find the code snippet from a large corpus of source code repositories that highly matches
the query of natural language description. Recent work mainly uses natural language processing techniques
to process both query texts (i.e., human natural language) and code snippets (i.e., machine programming
language), however neglecting the deep structured features of query texts and source codes, both of which
contain rich semantic information. In this paper, we propose an end-to-end deep graph matching and searching
(DGMS) model based on graph neural networks for the task of semantic code retrieval. To this end, we first
represent both natural language query texts and programming language code snippets with the unified
graph-structured data, and then use the proposed graph matching and searching model to retrieve the best
matching code snippet. In particular, DGMS not only captures more structural information for individual
query texts or code snippets but also learns the fine-grained similarity between them by cross-attention based
semantic matching operations. We evaluate the proposed DGMS model on two public code retrieval datasets
with two representative programming languages (i.e., Java and Python). Experiment results demonstrate that
DGMS significantly outperforms state-of-the-art baseline models by a large margin on both datasets. Moreover,
our extensive ablation studies systematically investigate and illustrate the impact of each part of DGMS.

CCS Concepts: â€¢ Information systems â†’ Retrieval models and ranking; Multimedia and multimodal
retrieval; â€¢ Computing methodologies â†’ Natural language processing; Neural networks.

âˆ—Xiang Ling and Lingfei Wu contribute equally to this research.
â€ Chunming Wu and Shouling Ji are the co-corresponding authors.

This work was partly supported by the National Key R&D Program of China Under No. 2018YFB0804102, 2020YFB1804705,
and 2020YFB2103802; NSFC under No. 61772466, U1936215, and U1836202; the Key R&D Program of Zhejiang Province
under No. 2020C01021, 2020C01077, and 2021C01036; the Major Scientific Project of Zhejiang Lab under No. 2018FD0ZX01;
the Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars under No. LR19F020003; and the
Fundamental Research Funds for the Central Universities (Zhejiang University NGICS Platform).
Authorsâ€™ addresses: Xiang Ling, Zhejiang University, China, lingxiang@zju.edu.cn; Lingfei Wu, IBM T. J. Watson Research
Center, USA, lwu@email.wm.edu; Saizhuo Wang, Zhejiang University, China, szwang@zju.edu.cn; Gaoning Pan, Zhejiang
University, China, szwang@zju.edu.cn; Tengfei Ma, IBM T. J. Watson Research Center, USA, Tengfei.Ma1@ibm.com; Fangli
Xu, Squirrel AI Learning, USA, lili@yixue.us; Alex X. Liu, Ant Group, China, alexliu@antfin.com; Chunming Wu, Zhejiang
University, China, Zhejiang Lab, China, wuchunming@zju.edu.cn; Shouling Ji, Zhejiang University, China, sji@zju.edu.cn.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2021 Association for Computing Machinery.
1556-4681/2021/1-ART111 $15.00
https://doi.org/10.1145/1122445.1122456

111

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

 
 
 
 
 
 
111:2

Ling and Wu, et al.

Additional Key Words and Phrases: Neural networks, graph representation, source code retrieval

ACM Reference Format:
Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli Xu, Alex X. Liu, Chunming Wu,
and Shouling Ji. 2021. Deep Graph Matching and Searching for Semantic Code Retrieval. ACM Trans. Knowl.
Discov. Data. 0, 0, Article 111 (January 2021), 21 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
With the advent of massive source code repositories such as GitHub1, Bitbucket2, and GitLab3, code
retrieval over billions of lines of source codes has become one of the key challenges in software
engineering [2]. Given a natural language query (i.e., human natural language) that describes
the intent of program developers, the goal of code retrieval is to find the best matching code
snippet (i.e., machine programming language) from a large corpus of code snippets in source code
repositories. Code retrieval tools can not only help program developers to find standard syntax
or semantic usages of a specific programming language or library, but also help them to quickly
retrieve previously written code snippets for certain functionality and then reuse those written
code snippets, which largely accelerate software development for program developers.

To deal with the task of code retrieval, traditional approaches [28, 39, 40, 42, 51] mainly employ
information retrieval techniques that treat source codes as a collection of documents and perform
keyword searching over them. For example, Sindhgatta [51] developed a source code search tool -
JSearch, which employs text based information retrieval techniques and indexes source code of Java
programming languages by extracting features of syntactic entities. Linstead et al. [39] proposed
sourcerer, an information retrieval based tool that incorporates both textual information and
structural fingerprints (i.e., code-specific heuristics) of source code and exploits the PageRank [44]
algorithm for ranking in code retrieval. Lv et al. [40] implemented CodeHow that applies the
Extended Boolean model to combine the impact of both text similarity and potential APIs expanding
on code retrieval. However, all those information retrieval-based approaches have difficulty in
understanding the semantics of both query texts and source codes.

More recently, advanced deep learning techniques, especially natural language processing meth-
ods, have been applied to learn deeper semantic information of both query and source code for
code retrieval tasks. One representative approach is DeepCS [25] that applies deep neural networks
to solve code retrieval tasks for the first time in 2018. DeepCS first uses Recurrent Neural Network
(RNN) [15] or multi-layer perceptron (MLP) [17] to encode both code snippets and query texts
into a vector representation respectively and then compute the similarity score between their
representations. Other follow-up work [13, 25, 26, 30, 48, 55, 62] is similar to DeepCS with only a
slight difference in determining the encoding models. Conceptually, these approaches usually apply
two individual sequence encoder models (e.g., MLP, 1D-CNN [33], LSTM [29], or even BERT [19]
encoder) for both the query text and the code snippets, and then rank these code snippets accord-
ing to the similarity score between the learned distributed representations of the query text and
every code snippet in the candidate set. However, we argue that these approaches still suffer from
two major challenges: 1) sequence encoding models cannot capture the structural information
behind, especially for the source codes in which various dependency features include long-range
dependencies may exist (e.g., the same identifiers may be operated in many places of source code);
2) lack of exploration of the semantic relationship between query texts and code snippets makes
these models unable to align their distributed representation with fine-granularity.

1https://www.github.com/
2https://www.bitbucket.org/
3https://about.gitlab.com/

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:3

To address these aforementioned challenges, in this paper, we propose a novel Deep Graph
Matching and Searching (DGMS) model for representation learning and matching of both query
texts and source codes.4 Our proposed DGMS model is based on two key insights. Firstly, since
higher abstraction requires capturing more semantic information, if the representation learning of
both query texts and source codes has a higher abstraction, it would be more powerful to learn
their semantic information. Intuitively, graph-structured data represents a much higher abstraction
than plain sequences or tokens, and hence the proposed DGMS model use graph-structured data as
the basis of the representation learning for code retrieval tasks. More importantly, we implement a
novel graph generation method that represents both query texts and source codes into a unified
graph-structured data in which both structural and semantic information could be largely retained.
Secondly, instead of simply encoding each graph-structured data as a graph-level embedding vector
and calculating the cosine similarity between them, we first leverage the power of graph neural
networks (GNN) to learn all node embeddings for graphs to capture semantic information for
individual text graph or code graph. Then, we propose a semantic matching operation based on the
cross-attention mechanism to explore more fine-grained semantic relations between the text graph
and the corresponding code graph for updating the embedding of each node in both graphs.

Specifically, as depicted in Figure 2, DGMS consists of three key modules: â¶ Graph encoding
module extracts deep structural representation from both query texts and source codes. It em-
ploys graph neural networks to encode both text and code graphs individually with each node
encoded with contextual embedding. â· Semantic matching module utilizes a cross-attention based
matching operation to obtain the semantic intersection relationship between the text graph and
corresponding code graph and update the embedding of each node. â¸ Code searching is performed
by first aggregating all node embeddings for both text and code graphs to obtain two graph-level
embeddings and then computing their ultimate similarity score. Finally, we leverage the ranking
loss to train the model in an end-to-end fashion.

To demonstrate the effectiveness of the proposed DGMS model, we systematically investigate
the performance of DGMS compared with 7 state-of-the-art baseline models on two public code
retrieval datasets: FB-Java and CSN-Python that are built from two different and representative
programming languages i.e., Java and Python, respectively. The experimental results show that
our model significantly outperforms baselines by a large margin on both datasets in terms of all
evaluation metrics. We also conduct ablation studies to evaluate the contribution of each part of
our DGMS model. Our code can be available at https://github.com/kleincup/DGMS. To summarize,
we highlight the main contributions of our work as follows:

â€¢ To the best of our knowledge, it is the first approach using the unified graph-structured data
to represent both query texts and code snippets whose structural and semantic information
is largely preserved and learned.

â€¢ We propose DGMS, a deep graph matching and searching model for semantic representation
learning of both query texts and code snippets. DGMS can not only capture semantic infor-
mation for individual the query text or code snippet, but also explore fine-grained semantic
relations between them.

â€¢ We conduct extensive experiments on two public code retrieval datasets from two representa-
tive programming languages (i.e., Java and Python) and our results demonstrate the superior
performance of our model over the state-of-the-art baseline methods.

Roadmap: The rest of the paper is organized as follows. We represent both query texts and
source codes into a unified graph-structured data in Section 2, and describe the proposed DGMS

4For brevity, we interchangeably use â€œsource codeâ€, â€œcodeâ€ and â€œcode snippetâ€ for the source code, as well as â€œquery textâ€,
â€œqueryâ€ and â€œtextâ€ for the natural language description of the query.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

111:4

Ling and Wu, et al.

model for graph representation matching and searching in Section 3. The performance of the
proposed DGMS model is systematically evaluated and analyzed in Section 4. Section 5 surveys
related work and Section 6 finally concludes this work.

2 SEMANTIC CODE RETRIEVAL: FROM THE UNIFIED GRAPH PERSPECTIVE
Before introducing the proposed DGMS model architecture for semantic code retrieval tasks, in this
section, we first introduce how we generate graph-structured data for both query texts and code
snippets (Section 2.1) and then define the code retrieval task from the unified graph perspective
(Section 2.2).

2.1 Graph Generation
Instead of simply treating both query texts and code snippets as plain tokens or sequences, we argue
that both of them have rich important semantic structure information (e.g., various dependency
features). Thus, we propose to represent both the query text and code snippet with graph-structured
data. One real example5 of query text and code snippet as well as their corresponding graph
representations are shown in Figure 1. Below, we will introduce how we generate the text graph
(Section 2.1.1) and the code graph (Section 2.1.2) from the corresponding query text and code
snippet in detail.

2.1.1 Generating Text Graphs. For the query text in the form of natural languages, we build the
text graph based on the constituency parse tree [32] and word ordering features, which provide
both constituent and ordering information of sentences. We argue that the constituency features
from the constituency parse tree actually represent the phrase structure of sentences and thus
semantic information can be obtained. In general, the most widely used formal system for modeling
constituent structure in natural languages is context-free grammar [16]. A context-free grammar
consists of a lexicon of symbols (e.g., words) and a set of grammar rules. Each of the rules expresses
the ways that symbols of the language can be grouped and ordered together.

Specifically, the context-free grammar for natural languages can be formally defined as a four-
tuple âŸ¨N, T, R, ğ‘ âŸ©. N is a set of non-terminal symbols; T is a set of terminal symbols that disjoint
from N; R is a set of grammar rules R : N â†’ (T âˆª N)âˆ— that map a non-terminal symbol to a list
of its children (a string of symbols from the infinite set of strings (T âˆª N)âˆ—); and ğ‘  âˆˆ N is the
designated root symbol. Taking Figure 1(b) as an example, N is the set of {S, VP, NP} and T is the set
of {â€œConfigureâ€, â€œtheâ€, â€œwindowâ€, â€œsizeâ€}. One grammar rule in R can be â€œVP â†’ Verb NPâ€, expressing
that one kind of verb phrase (VP) can be composed of a verb in English followed by a noun phrase
(NP). Our text graph generation is based on the constituency parse tree where nodes express the
terminal or non-terminal symbols and edges express its grammar rules, i.e., constituency edges
(with black color) in Figure 1(b).

On the other hand, we also incorporate the word ordering information of sentences into the text
graph. Specifically, we link the words (i.e., terminal symbols in the constituency tree) of sentences in
a chain, which could capture the forward and backward contextual information of sentences. That
is, we add this kind of word ordering edges (with blue color in Figure 1(b)) into the constituency
tree. In summary, by combining both constituent and word ordering information of sentences into
graph-structured data, we can generate a more informative text graph representation as shown in
Figure 1(b).

5Source: https://github.com/onyxbits/raccoon4/blob/master/src/main/java/de/onyxbits/weave/swing/WindowBuilder.java#
L135 (last accessed on January 2020).

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:5

(a) An example java function with its natural language description
and the code snippet.

(b) The generated text graph of the natural language
description in Figure 1(a), in which there are 3 con-
stituency symbols: S (the simple declarative clause), VP
(verb phrase), and NP (noun phrase).

(c) The generated code graph of the java function in Figure 1(a).

Fig. 1. An example java function is shown in Figure 1(a) with its generated text graph in Figure 1(b) and the
generated code graph in Figure 1(c).

2.1.2 Generating Code Graphs. Inspired by recent advances in solving source code related tasks, like
code completion, variable naming, and code generation [3, 10, 18, 22], we generate the code graphs
using the program graph structure [3], which is mainly based on the abstract syntax tree (AST)
representation of source codes [52]. In fact, the AST of source codes is in analogy to the constituency
parse tree of natural language texts. Both of them share a similar spirit for representing either source
codes or query texts. In particular, an AST for a code snippet is a tuple âŸ¨N, T, X, Î”, ğœ™, ğ‘ âŸ©, where N is
a set of non-terminal nodes, T is a set of terminal nodes, X is a set of values, Î” : N â†’ (N âˆª T)âˆ— is a
function that maps a non-terminal node to a list of its children, ğœ™ : T â†’ X is a function that maps a
terminal node to an associated value, and ğ‘  âˆˆ N is the root node.

Specifically, a program graph consists of syntax nodes (corresponding to terminal/non-terminal
nodes in AST) and syntax tokens (values of terminal nodes in the original code). Program graphs use
different kinds of edge types to model the syntactic and semantic relationship between nodes/tokens.
As shown in Figure 1(c), the program graph explores three edge types: Child edges are to connect
syntax nodes in AST, NextToken edges are to connect each syntax token to its successor in the
original code, LastLexicalUse edges are to connect identifiers to their most recent lexical usage.
More introduction and implementation details can be found in [3, 22].

Why we choose code graphs rather than other graphs like control-flow graphs? In short,
the main reason we choose to represent the code snippet with the program graph is that it shares a
similar spirit to the text graph of the query texts, which allows us to represent both query texts
and source codes from a unified graph perspective. On the other hand, the reason why we do

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

PublicWindowBuilerwithSizeintwidthintheightthissizenewDimensionwidthheightreturnthisMethod DeclarationClassOrInterfaceTypeParameterBlockStmtParameterPrimitiveTypeReturnStmtThisExprExpressionStmtAssign ExprFieldAccessExprThisExprObjectCreationExprNameExprClassOrInterfaceTypeNameExprPrimitiveTypeConfigurethewindowsizeNPVPS/** * Configure the window size** @paramwidth width* @paramheight height* @returnthis reference for chaining.     */publicWindowBuilderwithSize(intwidth,intheight){this.size=newDimension(width,height);returnthis;}Constituency EdgesWord Ordering EdgesNatural Language DescriptionCode SnippetConstituency SymbolsWord TokensChildNextTokenSyntax NodesSyntax TokensLastLexicalUsePublicWindowBuilerwithSizeintwidthintheightthissizenewDimensionwidthheightreturnthisMethod DeclarationClassOrInterfaceTypeParameterBlockStmtParameterPrimitiveTypeReturnStmtThisExprExpressionStmtAssign ExprFieldAccessExprThisExprObjectCreationExprNameExprClassOrInterfaceTypeNameExprPrimitiveTypeConfigurethewindowsizeNPVPS/** * Configure the window size** @paramwidth width* @paramheight height* @returnthis reference for chaining.     */publicWindowBuilderwithSize(intwidth,intheight){this.size=newDimension(width,height);returnthis;}Constituency EdgesWord Ordering EdgesDescription TextFunction CodeConstituency SymbolsWord TokensChildNextTokenSyntax NodesSyntax TokensLastLexicalUsePublicWindowBuilerwithSizeintwidthintheightthissizenewDimensionwidthheightreturnthisMethod DeclarationClassOrInterfaceTypeParameterBlockStmtParameterPrimitiveTypeReturnStmtThisExprExpressionStmtAssign ExprFieldAccessExprThisExprObjectCreationExprNameExprClassOrInterfaceTypeNameExprPrimitiveTypeConfigurethewindowsizeNPVPS/** * Configure the window size** @paramwidth width* @paramheight height* @returnthis reference for chaining.     */publicWindowBuilderwithSize(intwidth,intheight){this.size=newDimension(width,height);returnthis;}Constituency EdgesWord Ordering EdgesNatural Language DescriptionCode SnippetConstituency SymbolsWord TokensChildNextTokenSyntax NodesSyntax TokensLastLexicalUse111:6

Ling and Wu, et al.

Table 1. Important symbols and notations

Symbols Definitions or Descriptions

ğ‘
ğ‘’
ğºğ‘
ğºğ‘’
ğ‘ğ‘–
ğ‘’ ğ‘—
qğ‘– âˆˆ Rğ‘‘
eğ‘— âˆˆ Rğ‘‘
Hğ‘ âˆˆ Rğ‘‘â€²
Hğ‘’ âˆˆ Rğ‘‘â€²
ğ‘€
ğ‘

query text
code snippet
graph representation of query text ğ‘
graph representation of code snippet ğ‘’
ğ‘–-th node in graph ğºğ‘
ğ‘—-th node in graph ğºğ‘’
node embedding vector of ğ‘ğ‘– in graph ğºğ‘ with ğ‘‘ dimensions
node embedding vector of ğ‘’ ğ‘— in graph ğºğ‘’ with ğ‘‘ dimensions
graph-level embedding vector of graph ğºğ‘ with ğ‘‘ â€² dimensions
graph-level embedding vector of graph ğºğ‘’ with ğ‘‘ â€² dimensions
the number of nodes in graph ğºğ‘
the number of nodes in graph ğºğ‘’

not represent source code as a control-flow graph (CFG) is mainly because control-flow related
graphs are too coarse-grained (i.e., each node is a sequence of instructions without jumps where
one instruction consists of multiple lines of sequential tokens) to properly represent source codes
for code retrieval tasks.

2.2 Code Retrieval Task Definition: A Unified Graph Perspective
In this subsection, we define the code retrieval task from a unified graph perspective. Specifically,
given a corpus of source codes ğ¸ with a total number of |ğ¸| code snippets, the goal of code retrieval
is to find the best matching code snippet Ë†ğ‘’ from ğ¸ according to the query text ğ‘ in the form of
natural language. Thus, we give the formulation of the definition of code retrieval task as follows.

Ë†ğ‘’ = arg max

ğ‘’ âˆˆğ¸

ğ‘ ğ‘–ğ‘š(ğ‘, ğ‘’) = arg max

ğ‘’ âˆˆğ¸

ğ‘ ğ‘–ğ‘š(ğºğ‘, ğºğ‘’ )

(1)

The core of this code retrieval task is to compute the similarity score ğ‘ ğ‘–ğ‘š(ğ‘, ğ‘’) between the code
snippet ğ‘ and query text ğ‘’. As described in above Section 2.1, both code and text can be represented
with graph-structured data using our graph generation methods. Thus, we further formulate
ğ‘ ğ‘–ğ‘š(ğ‘, ğ‘’) as ğ‘ ğ‘–ğ‘š(ğºğ‘, ğºğ‘’ ), in which ğºğ‘, ğºğ‘’ are graph representations for the text ğ‘ and code ğ‘’,
respectively. In this paper, both ğºğ‘ and ğºğ‘’ are represented as directed and labeled multi-graphs
in which different edge types are encoded by labeled edges. Specifically, the text graph ğºğ‘ is
represented as (Vğ‘, Eğ‘, Rğ‘) with nodes ğ‘ğ‘– âˆˆ Vğ‘ and edges (ğ‘ğ‘–, ğ‘Ÿ, ğ‘ ğ‘— ) âˆˆ Eğ‘, where ğ‘Ÿ âˆˆ Rğ‘ denotes
edge type. Similarly, the code graph ğºğ‘’ is represented as (Vğ‘’, Eğ‘’, Rğ‘’ ). The number of nodes of ğºğ‘
and ğºğ‘’ is ğ‘€ and ğ‘ , respectively. Furthermore, the important symbols and notations throughout
the following sections of our paper are summarized in Table 1.

3 DEEP GRAPH MATCHING AND SEARCHING
In this section, we introduce our proposed deep graph matching and searching (DGMS) model,
which performs fine-grained text-code semantic matching and searching based on text graphs and
code graphs. Figure 2 shows the overall DGMS architecture, consisting of three modules: graph
generation, graph encoding, semantic matching, and code searching. As graph generation has been
introduced in previous Section 2, we will detail the other three modules as follow.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:7

Fig. 2. The general framework of deep graph matching and searching (DGMS) model.

3.1 Graph Encoding
In order to capture the structure and semantic information for individual text graphs and code
graphs, in this graph encoding module, we first need to learn the node embeddings for each node
in both text and code graphs.

Recently, the graph neural network (GNN) that adapts deep learning from image to graph-
structured data has received unprecedented attention from both machine learning and data mining
communities [47, 59]. The main goal of GNN is to learn information representations (e.g., node or
(sub)graph, etc) of graph-structured data in an end-to-end manner [12]. Specifically, a GNN model
takes a graph (i.e., its topological structures, node and/or edge attributes) as input, and output an
embedding vector for each node of the input graph. There is a large body of GNN models designed
to learn node representations [14, 24, 27, 34, 37, 49, 54, 61]. With the learned node representation,
various tasks on graphs can be performed such as link prediction [63], graph classification [20, 58],
and graph matching and similarity [7, 38, 64], to name just a few.

Since both text graphs and code graphs are represented as directed and labeled multi-graphs
(i.e., graphs with multiple labeled edges), we adopt one variant of GNNs â€“ Relational Graph
Convolutional Networks (RGCNs) [50] to learn their node embedding in this graph encoding
module. Other variants of graph neural networks can also be applied for the graph encoding as
long as these variants could learn node embeddings of a graph with different edge types (see details
in Section 4.4.2).

In particular, taking the text graph ğºğ‘ = (Vğ‘, Eğ‘, Rğ‘) as an example, RGCN defines the propaga-
tion model for calculating the updated embedding vector qğ‘– of each node ğ‘ğ‘– âˆˆ Vğ‘ in the text graph
ğºğ‘ as follows,

q(ğ‘™+1)
ğ‘–

= ReLU (cid:0)ğ‘Š (ğ‘™)

Î˜ q(ğ‘™)

ğ‘– +

âˆ‘ï¸

âˆ‘ï¸

ğ‘Ÿ âˆˆRğ‘

ğ‘— âˆˆNğ‘Ÿ
ğ‘–

1
|Nğ‘Ÿ
ğ‘– |

ğ‘Š (ğ‘™)

ğ‘Ÿ q(ğ‘™)
ğ‘—

(cid:1)

(2)

ğ‘–

where q(ğ‘™+1)
represents the set of relations (i.e., edge types), Nğ‘Ÿ
ğ‘–
edge type ğ‘Ÿ âˆˆ Rğ‘, ğ‘Š (ğ‘™)
Î˜ and ğ‘Š (ğ‘™)
the activation function.

denotes the updated embedding vector of node ğ‘ğ‘– in the (ğ‘™ + 1)-th layer of RGCN, Rğ‘
is the set of the neighbors of node ğ‘ğ‘– under the
are parameters of the RGCN model to be learned, ReLU denotes

ğ‘Ÿ

By encoding the graph-structured data for both the query text and code snippet with above
ğ‘–=1 âˆˆ R(ğ‘€,ğ‘‘) for text graph ğºğ‘ and
ğ‘—=1 âˆˆ R(ğ‘ ,ğ‘‘) for code graph ğºğ‘’ , in which ğ‘‘ represents the embedding

RGCN model, we thus obtain both node embeddings Xğ‘ = {qğ‘– }ğ‘€
node embeddings Xğ‘’ = {eğ‘— }ğ‘
dimensions of each node.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

qğ‘–ğ‘–ğ‘’ğ‘’ğ‘—ğ‘—Î±i,j=attention(qi,ej)ğ‘’ğ‘’ğºğºğ‘–ğ‘–=1ğ‘ğ‘ï¿½ğ‘—ğ‘—NÎ±i,jejRanking for RecommendationRGCNRGCNGraph Construction & EncodingSemantic MatchingCode SearchingText Graph qCodeGraph eğ’‡ğ’‡ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’‡ğ’‡ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’ğ’ï¿½qğ‘–ğ‘–ğ—ğ—eğ—ğ—qğ‡ğ‡eğ‡ğ‡qSimilaritycosine(ğ‡ğ‡q, ğ‡ğ‡e)AggregationAggregationCode SnippetsNatural Language Description111:8

Ling and Wu, et al.

3.2 Semantic Matching
After obtaining node embeddings with rich semantic information for each individual text graph
or code graph, we propose a cross-attention based semantic matching operation ğ‘“match for
comparing and aligning every node embedding in one graph against the graph-level contextual
embedding of the other graph. The key idea of this semantic matching operation is to explore more
fine-grained semantic relations between text graphs and the corresponding code graphs, and then
update and enrich the node embeddings for both graphs. We detail each step for semantic matching
as follows.

Attention: we first compute the cosine attention similarity between all pairs of each node in
one graph and all nodes in another graph. Taking the node ğ‘ğ‘– in ğºğ‘(i.e., ğ‘ğ‘– âˆˆ Vğ‘) as an example,
we calculate a cross attention similarity between its node embedding qğ‘– of node ğ‘ğ‘– and the node
embedding eğ‘– of each node ğ‘’ ğ‘— in graph ğºğ‘’ ,

ğ›¼ğ‘–,ğ‘— = cosine(qğ‘–, eğ‘— ), âˆ€ğ‘— = 1, . . . , ğ‘

(3)

where cosine denotes the cosine similarity function.

Context representation: We then take the cosine attention (i.e., ğ›¼ğ‘–,ğ‘— ) as the weight of eğ‘— and
compute a weighted-average embedding over all node embeddings of ğºğ‘’ by the corresponding
cross attention scores with node ğ‘ğ‘– in ğºğ‘, which yields a contextual global-level representation eğ‘–
ğº
of ğºğ‘’ with respect to node ğ‘ğ‘– in ğºğ‘,

eğ‘–
ğº =

1
ğ‘

ğ‘
âˆ‘ï¸

ğ‘—

ğ›¼ğ‘–,ğ‘— eğ‘—

(4)

Comparison: Inspired by previous work of textual entailment in natural language process-
ing [56], we consider two following matching operations Sub and Mul, both of which transform qğ‘–
and eğ‘–
ğº into a vector Ë†qğ‘– to represent the comparison result. We argue that the comparison result
measures, to some extent, the alignment information between each pair of node embeddings in two
graphs and thus can be used to update node embeddings for latter computing similarity between
two graphs.

Ë†qğ‘– = Sub(qğ‘–, eğ‘–
Ë†qğ‘– = Mul(qğ‘–, eğ‘–

ğº ) = (qğ‘– âˆ’ eğ‘–
ğº ) = qğ‘– âŠ™ eğ‘–
ğº

ğº ) âŠ™ (eğ‘–

ğº âˆ’ qğ‘– )

(5)

(6)

where âŠ™ denotes the element-wise multiplication operation; the resulting Ë†qğ‘– is the updated node
embedding vector and has the same embedding size as qğ‘– or eğ‘–
ğº .

Alternatively, the results of these two matching operations Sub and Mul can be concatenated to
assemble another matching operation called SubMul, which is finally adopted by our final model
DGMS,

Ë†qğ‘– = SubMul(qğ‘–, eğ‘–

ğº ) = Concat (cid:2) Sub(qğ‘–, eğ‘–

ğº ), Mul(qğ‘–, eğ‘–

ğº )(cid:3)

(7)

where Concat denotes the concatenation operation and the resulting Ë†qğ‘– is twice the node embedding
size of qğ‘– or eğ‘–
ğº .

To sum up, after performing the above semantic matching operation ğ‘“ğ‘šğ‘ğ‘¡ğ‘â„ for both text graph
ğ‘–=1 âˆˆ R(ğ‘€,ğ‘‘â€²) and
ğ‘—=1 âˆˆ R(ğ‘ ,ğ‘‘â€²) , where ğ‘‘ â€² denotes the updated node embedding size (i.e., ğ‘‘ â€² = ğ‘‘ for Sub/Mul,

ğºğ‘ and code graph ğºğ‘’ , we further update their node embeddings as Xğ‘ = { Ë†qğ‘– }ğ‘€
Xğ‘’ = { Ë†eğ‘— }ğ‘
ğ‘‘ â€² = 2ğ‘‘ for SubMul.).

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:9

3.3 Code Searching
As shown in Equation (1), the core of code retrieval tasks is to calculate a similarity score between
the code and query text, and thus to calculate the similarity between the two graph representations
of both the query text and code snippet.

To perform code searching over graph representations of both codes and texts, we first aggre-
gate the unordered node embeddings for each graph and obtain the corresponding graph-level
representation vector for either the code graph or the text graph. A simple and straightforward
aggregation method is the max-pooling operation that calculates the maximum value for all node
embeddings in one graph. In this paper, we apply another pooling operation FCMax on their
node embeddings (i.e., Xğ‘ and Xğ‘’ ). Particularly, FCMax is a variant of the max-pooling operation
following a fully-connected layer transformation.

Hğ‘ = FCMax(Xğ‘) = max-pooling(FC({ Ë†qğ‘– }ğ‘€
Hğ‘’ = FCMax(Xğ‘’ ) = max-pooling(FC({ Ë†eğ‘– }ğ‘

ğ‘–=1))
ğ‘–=1))

(8)

(9)

where FC and max-pooling denote the fully-connected layer and max-pooling operation. It is
evident that FCMax offers a larger model capacity than max-pooling operation in theory, which is
also supported by the latter experimental evaluation in Section 4.4.5. The output dimension size
of Hğ‘ depends on the hidden size of the fully-connected layer which is set the same as ğ‘‘ â€² (i.e.,
Hğ‘ âˆˆ Rğ‘‘â€²

and Hğ‘’ âˆˆ Rğ‘‘â€²

).

Next, to measure the similarity score between the query text and code snippet, i.e., sim(ğ‘, ğ‘’) in

Equation (1), we compute the cosine similarity between Hğ‘ and Hğ‘’ ,

sim(ğ‘, ğ‘’) = sim(ğºğ‘, ğºğ‘’ ) = cosine(Hğ‘, Hğ‘’ )

(10)

According to Equation (1), we can perform code searching based on the similarity score between
the two learned distributed representations.

3.4 Model Training
In principle, our model can be trained in an end-to-end fashion on a large corpus of paired query
texts and code snippets. However, we use the document description of code snippet instead of the
query text for model training, since there is no benchmark dataset that contains a large corpus
of paired query texts and code snippets. For example, the dataset in [36] only contains 287 Stack
Overflow question/answer pairs and the dataset in [30] contains 99 human-annotated query/code
pairs per programming language. Hence, it is impossible to train a code retrieval model based on
those limited paired query texts and code snippets.

Specifically, each training sample in the training corpus T is a triple âŸ¨ğ‘, ğ‘’, (cid:165)ğ‘’âŸ©, which is constructed
as follows: for each code snippet ğ‘’ and its corresponding documental text description ğ‘, we randomly
select a negative sample code snippet (cid:165)ğ‘’ from other code snippets in the corpus T. The goal of our
model is to predict a higher cosine similarity ğ‘ ğ‘–ğ‘š(ğ‘, ğ‘’) than ğ‘ ğ‘–ğ‘š(ğ‘, (cid:165)ğ‘’). According to Equation 10,
we use the margin ranking loss [8] for model optimization as follows,

L (ğœƒ ) =

âˆ‘ï¸

max(cid:0)0, ğ›¿ âˆ’ sim(ğ‘, ğ‘’) + sim(ğ‘, (cid:165)ğ‘’)(cid:1)

âŸ¨ğ‘,ğ‘’, (cid:165)ğ‘’ âŸ© âˆˆT
âˆ‘ï¸

âŸ¨ğ‘,ğ‘’, (cid:165)ğ‘’ âŸ© âˆˆT

=

max(cid:0)0, ğ›¿ âˆ’ cosine(Hğ‘, Hğ‘’ ) + cosine(Hğ‘, H (cid:165)ğ‘’ )(cid:1)

where ğœƒ denotes the model parameters to be learned and ğ›¿ is the margin parameter of margin
ranking loss.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

111:10

Ling and Wu, et al.

It is noted that our model is trained based on the siamese network [11] that uses the shared
RGCN model to learn representations for both text graph and code graph. The feature of sharing the
parameters of the siamese network makes our model smaller, thus mitigating possible over-fitting
and making the training process easier.

4 EXPERIMENTS
In this section, we first introduce the benchmark datasets to be evaluated in Section 4.1, describe the
baseline models to be compared with our models in Section 4.2, provide details of the experimental
settings in Section 4.3, and finally present experimental results and discussion of the results in
Section 4.4.

4.1 Datasets Description & Preprocessing
We evaluate our DGMS model on two public code retrieval datasets: FB-Java and CSN-Python that
are built from two different and representative programming languages, i.e., Java and Python.
The first FB-Java dataset is built on the recently released benchmark dataset that the Facebook
research team has been used to assess the performance of neural code search models [21, 36].
As the released dataset provides the downloadable links of GitHub repositories and the original
repositories might be deleted or removed by their owner for some reasons, we actually collect a total
of 24,234 repositories with 4,679,758 functions or methods on the download date of October 11, 2019.
To make the dataset can be utilized to evaluate code retrieval tasks, we apply some preparatory
operations as follows.

(1) For each downloaded method or function, we parse it to a pure code snippet and the corre-

sponding docstrings (i.e., method-level documental description) if it has;

(2) We remove all methods without docstrings, as we treat the docstring as the query text in our

evaluation;

(3) To make the dataset more realistic to evaluate the code retrieval tasks, we remove all methods

whose pure code snippet contains less than 3 lines;

(4) We remove all methods whose docstring contains less than 3 words or contains non-English

words;

(5) As some of those methods have duplicate docstrings (e.g., method overloading or overriding,

etc.), we only keep one of the methods with duplicate docstrings.

After that, we get a total of 249,072 pairs of source code and corresponding documental descriptions
for the FB-Java dataset.

To further show the effectiveness of our model on other programming languages, we also choose
another dataset CSN-Python from CodeSearchNet [30]. We apply similar preparatory operations
on CSN-Python as above, which essentially results in a total of 364,891 pairs of source code and
documental description.

Since our model is built on the graph-structured data, we need to produce graph representations
for both code snippets and text descriptions. To produce the text graphs, we first generate the
constituency tree of text description using the Stanford CoreNLP toolkit [41] and then link the
terminal nodes in the constituency tree in a chain with bi-direction edges. We build code graphs
based on the open-sourced code in [22]. As illustrated in Figure 3, the number of nodes for both
text graphs and code graphs in both datasets follows a typical long-tail distribution, we limit the
number of nodes of all graphs to no more than 300, which keeps more than 90% of the total dataset.
It is also obviously observed that the average number of nodes of text graphs is much less than
that of code graphs for both datasets. Finally, we split the dataset into training/validation/testing
sets with statistics shown in Table 2.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:11

(a) FB-Java

(b) CSN-Python

Fig. 3. Graph size distribution for both datasets.

Table 2. Datasets statistics.

Datasets

# Training # Validation # Testing

FB-Java
CSN-Python

216,259
312,189

9,000
17,215

1,000
1,000

4.2 Baseline Models
To evaluate the effectiveness of our model, we consider 7 state-of-the-art deep learning based
models as baseline models for comparison. Details of these baselines as well as their experimental
settings are as follows.

â€¢ Neural Bag of Words (Neural BoW), RNN, 1D-CNN, and Self-Attention are 4 baseline
models provided by [30] that first encode both the code and text with the corresponding
neural network and then compute a similarity score between the representations of both code
and text. In particular, the open-sourced implementation of the RNN baseline employs LSTM
as the encoder, and Self-Attention adopts the BERT encoder [19] in their implementation. To
avoid over-tuning, we use their original hyper-parameters and experimental settings.

â€¢ DeepCS [25] captures the semantic information of code from 3 perspectives: method name,
API call sequence, and code tokens. It first separately encodes the 3 different sequences with
RNN or MLP, and then fuse them to get code representation. For the text, the authors simply
encode its tokens with RNN. Finally, DeepCS computes the cosine similarity score between
the representations of code snippets and texts. However, the extraction method of API call
sequences is based on heuristic approaches that are specific to the Java language and do not
work for other programming languages. Thus, we only use method names and code tokens
as the semantic features of source codes in our experiments.

â€¢ UNIF maps both code and query tokens with learnable embeddings [13]. To generate their rep-
resentations, all token embeddings of the code snippet are aggregated by attention weighted
sum with learnable attention vectors, while query token embeddings are simply pooled on
average. The cosine distance is used as the similarity metric. In our evaluations, we set the
token embedding dimension to 100.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

111:12

Ling and Wu, et al.

â€¢ CAT [26] propose a CAT model that uses sequence encoders to represent code snippets
based on both raw tokens and the converted string sequence of AST. The authors also
introduce multi-perspective matching operations [57] into CAT and build a combined model
call MPCAT. However, we argue that MPCAT is very time-consuming and unrealistic for
code retrieval tasks with datasets like ours. The reason is that our training dataset is roughly
100 times the size of the dataset in [26]. If we train MPCAT with 10 epochs (i.e., the same as
our experiment settings for a fair comparison) upon our dataset, it is expected to take about
660 hours (27 days), which is unacceptable in our evaluation. Thus, we only consider CAT as
the baseline model in our evaluation.

4.3 Experimental Settings
4.3.1 Parameter Settings. To set up our model, we use one layer of RGCN with the output node
dimension of 100 and use ReLU as the activation function. Since each node in the graph contains
one word token, we initialize each node with the pre-trained embeddings from GloVe [46] and the
dimension of one word embedding is 300. For those tokens that cannot be initialized from GloVe,
we try to split them into sub-tokens (e.g., CamelCase) and use the average of GloVe pre-trained
embeddings of sub-tokens for initialization. Otherwise, we initialize them with all zeros. We set the
output size of FCMax to 100.

Our implementation is built using the PyTorch [45] and PyTorch_Geometric [23] toolkits. To
train our model, we fix the margin ğ›¿ to 0.5, set the batch size to 10, and use Adam optimizer with a
learning rate of 0.0001. We train our model for 10 epochs and select the best model based on the
lowest validation loss. In general, it takes approximately 2.5 hours to train, validate and test our
model. Note that, all experiments are conducted on a PC equipped with 8 Intel Xeon 2.2GHz CPU,
256GB memory, and one NVIDIA GTX 1080Ti GPU.

4.3.2 Evaluation Metrics. In our evaluation, we consider the document description of code as the
query text and the code itself as the ground-truth result of code retrieval tasks, which is similar
to [26] but different from [25, 36]. Since the evaluation queries in [25] and [36] do not have the
ground-truth code snippets, they use manually labeled ground-truth or utilize another code-to-code
similarity tool for judgment. We argue that both these evaluation methods introduce human bias
and manual threshold settings.

Specifically, for each pair of code snippets and descriptions in the testing dataset, we treat the
description as the query and take the corresponding code snippet together with the other 99
randomly selected code snippets as the searching candidates for code retrieval tasks. We adopt
two kinds of evaluation metrics that are commonly used in information retrieval to measure the
performance of our models and baseline models - mean reciprocal rank (MRR) and success at k
(S@k). Specifically, MRR is the average of the reciprocal ranks of results for a set of queries that
denoted as ğ‘„,

MRR =

1
|ğ‘„ |

|ğ‘„ |
âˆ‘ï¸

ğ‘=1

1
FRankq

(11)

where FRankq refers to the rank position of the first result for the ğ‘-th query, |ğ‘„ | is the number of
queries in ğ‘„. In addition, S@k denotes the percentage of queries for which more than one correct
result exists in the top ğ‘˜ ranked results.

S@k =

1
|ğ‘„ |

|ğ‘„ |
âˆ‘ï¸

ğ‘=1

Î“(FRankq â‰¤ ğ‘˜)

(12)

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:13

where Î“ returns 1 if the input is true and returns 0 otherwise. For both evaluation metrics, a higher
metric value implies better performance.

It is noted that for the most complex final model DGMS with SubMul on both datasets, it averagely

takes about 3 hours for training, validation, and testing in our experiments.

4.4 Experimental Results & Discussion
In this subsection, we empirically compare the proposed model DGMS with 7 state-of-the-art
baseline methods and illustrate the effectiveness and generality of the proposed DGMS model.
We also systematically investigate the impact and contribution of each part (i.e., different graph
encoding models rather than RGCN, different feature dimensions of RGCN, different semantic
matching operations, and different aggregation operations) in the proposed DGMS model by using
ablation studies.

4.4.1 Comparison of Baseline Methods. To demonstrate the effectiveness of the proposed DGMS
model, we quantitatively evaluate the DGMS model by measuring and comparing MRR and S@k
performance on the testing samples of both FB-Java and CSN-Python datasets. Table 3 presents
the experimental results of our final model DGMS compared against 7 baseline methods on both
datasets. It is noted that DGMS refers to the final model that applies the SubMul matching operation
in the semantic matching module.

Among the 7 baseline methods, it is clearly observed from Table 3 that UNIF shows the best
performance on the FB-Java dataset, while CAT performs the best on CSN-Python. Interestingly,
CAT performs the worst on FB-Java, which implies that different baselines methods tend to have
different performance on different datasets and some baselines might bias the performance for
some datasets. On the other hand, 1D-CNN performs very poorly on both datasets in terms of all
evaluation metrics. One possible reason we conjecture is that 1D-CNN cannot capture the semantic
information for both query texts and code snippets with rich structural and dependency features
(e.g., long-range dependencies may exist in code snippets). Therefore, we suggest that subsequent
work should not use 1D-CNN as the encoding model, especially for code snippets.

Compared with all 7 baseline methods, our proposed DGMS model achieves state-of-the-art
performance on both datasets in terms of all the four evaluation metrics, i.e., MRR, S@1, S@5, and
S@10. For both FB-Java and CSN-Python datasets, our model has the performance of over 85% MRR,
over 80% S@1, and over 95% S@5, which implies low inspection effort of DGMS to retrieve the
desired result for code retrieval tasks. In particular, for the CSN-Python dataset, the DGMS model
has significantly higher performance than the best results of the other 7 baseline methods by a large
margin up to 22.1, 27.9, 13.9, and 8.6 absolute value on MRR, S@1, S@5, and S@10, respectively.
These correspond to increase rates of performance in MRR, S@1, S@5, and S@10 are high as 31.5%,
46.7%, 16.6%, and 9.5%, respectively.

S@k is an important evaluation metric for code retrieval tasks. A higher S@k value implies
more likely the correct results exist in the top ğ‘˜ ranked returned results. From Table 3, we observe
that the S@1 score of our DGMS model is 81.7% for FB-Java and 87.6% for CSN-Python. For both
datasets, the S@5 scores of DGMS are over 95%. These observations suggest that our DGMS model
is more likely (i.e., over 80% statistically) to get the correct code snippet from the top 1 returned
ranked results. Otherwise, DGMS can easily get the correct code snippet from the top 5 returned
ranked results with over 95% probability statistically.

Impact of different graph encoding models. We investigate the impact of different relational
4.4.2
GNNs employed in the graph encoding module of DGMS. Specifically, we replace RGCN with two
variants: Message Passing Network Network (MPNN) [24] and Crystal Graph Convolutional Neural
(CGCN) [60], as both of them can encode graphs with different types of edges. It is noted that we

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

111:14

Ling and Wu, et al.

Table 3. Experimental results compared with 7 baseline methods.

Dataset

Model

Result (%)
S@5

MRR S@1

S@10

FB-Java

CSN-Python

Neural BoW
RNN
1D-CNN
Self-Attention
DeepCS
UNIF
CAT

77.7
71.7
22.6
65.3
78.9
84.8
20.6

71.3
63.0
12.3
54.4
70.6
78.1
10.1

85.3
83.2
32.7
79.1
89.6
92.5
28.9

88.5
88.6
45.7
84.2
94.2
95.7
41.5

DGMS

87.9

81.7

95.5

96.7

Neural BoW
RNN
1D-CNN
Self-Attention
DeepCS
UNIF
CAT

66.0
62.7
18.4
63.9
64.4
59.3
70.1

56.2
52.8
10.5
54.5
52.2
47.0
59.7

78.3
73.1
25.1
75.3
78.2
73.5
83.8

83.2
81.6
33.6
82.1
88.3
83.2
90.3

DGMS

92.2

87.6

97.7

98.9

Table 4. Impact of different graph encoders.

Dataset

Model

Result (%)
S@5

MRR S@1

S@10

FB-Java

CSN-Python

DGMS (MPNN)
DGMS (CGCN)
DGMS

DGMS (MPNN)
DGMS (CGCN)
DGMS

85.8
85.6
87.9

91.8
92.5
92.2

79.0
78.6
81.7

87.3
88.2
87.6

94.3
94.4
95.5

97.9
97.9
97.7

96.7
96.0
96.7

98.9
99.1
98.9

do not fine-tune any hyper-parameters of the three models and all other experimental settings (e.g.,
the output dimension of graph encoding, SubMul matching, and FCMax aggregation operations
as well as training hyper-parameters) of DGMS (MPNN) and DGMS (CGCN) models are kept the
same with DGMS as the previous evaluation.

Table 4 presents the results of RGCN versus MPNN/CGCN in the DGSM architecture. We can
observe that all three models achieve similar and stable performance on both datasets, implying
our model architecture is not sensitive to different relational GNNs in the graph encoding module.
Interestingly, DGMS (CGCN) slightly improves DGMS on the CSN-Python dataset, which indicates
that our model can be further improved by carefully choosing other graph encoding models
according to different application tasks.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:15

(a) MRR

(b) S@1

(c) S@5

(d) S@10

Fig. 4. The impact of different feature dimensions in RGCN for the FB-Java dataset with regard to MRR, S@1,
S@5, and S10.

(a) MRR

(b) S@1

(c) S@5

(d) S@10

Fig. 5. The impact of different feature dimensions in RGCN for the CSN-Python dataset with regard to MRR,
S@1, S@5, and S10.

Impact of different feature dimensions of RGCN. We further study the impact of different
4.4.3
feature dimensions in graph encoder (i.e., RGCN) on the performance of our DGMS models. Follow-
ing the default and same parameter settings of previous experiments, we only change the output
dimension of RGCN (ğ‘‘ = 50/75/100/125/150) and observe their impacts on the performance of
DGMS-Sub, DGMS-Mul, and DGMS-SubMul. The main results for both FB-Java and CSN-Python
datasets are shown in Figure 4 and Figure 5, respectively.

It is clearly seen that the performance (i.e., MRR, S@1, S@5, and S@10) of DGMS-Sub, DGMS-Mul,
and DGMS-SubMul improves as the feature dimension in the graph encoder grows. The reason is
intuitive that increasing feature dimensions in the graph encoder imply increasing model capacities,
which usually show better performance in supervised learning. For DGMS-Sub and DGMS-Mul,
their performance increases rapidly with the increasing feature dimensions, while our final DGMS-
SubMul model shows a relatively slower increasing trend. We conjecture this is because the model
capacity of DGMS-SubMul is comparably more than DGMS-Sub and DGMS-Mul. However, it is a
trade-off between model performance and time/memory consumption since the increasing features
dimensions of the three models inevitably bring more time and memory consumption. We thus set
the default feature dimension in RGCN of all three models to be 100 for other evaluations.

Impact of different semantic matching operations. In order to assess the impact of different
4.4.4
operations of the semantic matching module in our model, we conduct ablation studies to evaluate
the performance of the following model variants with different semantic matching operations:

â€¢ DGMS-No refers to the model that does not employ any semantic matching operation and

we apply FCMax directly after the graph encoding;

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

507510012515060708090DimensionsMRR (%)java_MRRSubMulSubMul50751001251505060708090DimensionsS@1 (%)java_S1SubMulSubMul507510012515080859095100DimensionsS@5 (%)java_S5SubMulSubMul5075100125150859095100DimensionsS@10 (%)java_S10SubMulSubMul5075100125150708090100Dimensions MRR (%)python_MRRSubMulSubMul507510012515060708090100DimensionsS@1 (%)python_S1SubMulSubMul507510012515080859095100DimensionsS@5 (%)python_S5SubMulSubMul5075100125150859095100DimensionsS@10 (%)python_S10SubMulSubMul111:16

Ling and Wu, et al.

Table 5. Impact of different matching operations.

Dataset

Model

Result (%)
S@5

MRR S@1

S@10

FB-Java

CSN-Python

DGMS-No
DGMS-Sub
DGMS-Mul
DGMS

DGMS-No
DGMS-Sub
DGMS-Mul
DGMS

75.5
78.0
78.2
87.9

79.2
82.5
85.4
92.2

64.3
68.9
68.1
81.7

70.7
74.9
77.5
87.6

89.6
89.7
91.4
95.5

90.5
92.1
95.5
97.7

93.8
94.5
95.3
96.7

94.8
95.9
97.9
98.9

â€¢ DGMS-Sub refers to the model with the Sub matching operation as Equation (5);
â€¢ DGMS-Mul is a variant model with the Mul matching operation as Equation (6);
â€¢ DGMS is the final model using the SubMul matching operation as Equation (7).

As shown in Table 5, models that employ matching operations (i.e., Sub, Mul, SubMul) achieve
noticeably better performance than that without the matching operation (DGMS-No). These ob-
servations highlight the importance of the semantic matching module, which could significantly
improve the effectiveness of our DGMS model. On the other hand, compared with both DGMS-Sub
and DGMS-Mul models, the final DGMS model (DGMS-SubMul) achieves better performance, indi-
cating the concatenation of Sub and Mul matching operations captures more interaction features
than each of them individually.

Interestingly, compared with all 7 baselines in Table 3, even DGMS-No performs better than
most baselines, especially on the CSN-Python dataset. This observation indicates the usefulness of
the graph-structured data and the encoder for representing both the query text and code snippets
in our model, as the graph representation learning captures more semantic information.

Impact of different aggregation operations. We further investigate the impact of different
4.4.5
aggregation operations on the performance of our final DGMS models. Keeping all other experi-
mental settings the same as previous experiments, we employ four different aggregation operations
to aggregate a graph-level embedding from the learned node embeddings of the code graph or the
text graph. In general, an aggregation operation must operate over an unordered set of vectors and
be invariant to permutations of its inputs. Average and Max are two simple and straightforward
pooling operations that take the element-wise average operation and the element-wise max opera-
tion of the input node embeddings, respectively. FCAvg and FCMax are two variants of Average
pooling and Max pooling operations following a fully connected layer transformation. The main
results are summarized in Table 6.

Obviously, it can be seen that all three models (i.e., DGMS-Sub, DGMS-Mul, and DGMS-SubMul)
with both FCAvg and FCMax aggregation operations demonstrate superior performance on both
datasets in terms of all evaluation metrics. This implies that the fully connected layer transformation
after the max/average pooling plays an important role in aggregating the graph-level embedding.
In addition, FCMax shows better performance than FCAvg for all three models on both FB-Java and
CSN-Python datasets. We conjecture this is because the max pooling operation could easily compare
and find the similarity and dissimilarity of nodes of the text and code graphs, while the average
pooling operation might â€œflattenâ€ the aggregated graph-level embedding from node embeddings.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:17

Table 6. The impact of different aggregation operations on both datasets.

Datasets

Models

Aggregations MRR S@1

S@5

S@10

DGMS-Sub

FB-Java

DGMS-Mul

DGMS-SubMul

DGMS-Sub

CSN-Python

DGMS-Mul

DGMS-SubMul

Average
Max
FCAvg
FCMax

Average
Max
FCAvg
FCMax

Average
Max
FCAvg
FCMax

Average
Max
FCAvg
FCMax

Average
Max
FCAvg
FCMax

Average
Max
FCAvg
FCMax

64.4
66.6
73.7
78.0

45.8
51.5
78.1
78.2

83.8
81.8
85.6
87.9

62.4
65.6
74.9
82.5

46.7
48.0
81.6
85.4

89.5
85.7
89.7
92.2

51.7
53.9
62.3
68.9

28.3
33.6
68.2
68.1

74.9
71.3
78.1
81.7

49.2
53.5
64.1
74.9

27.7
28.9
72.4
77.5

83.1
76.7
84.0
87.6

79.7
83.0
87.5
89.7

70.0
75.2
91.0
91.4

95.9
95.5
94.8
95.5

79.5
82.4
88.5
92.1

71.9
73.7
92.4
95.5

97.2
97.0
96.8
97.7

89.7
90.4
92.6
94.5

84.5
87.6
95.1
95.3

96.8
96.0
97.0
96.7

89.3
90.7
94.8
95.9

86.6
88.0
96.6
97.9

98.7
98.7
98.4
98.9

5 RELATED WORK

5.1 Deep Learning for Code Retrieval Tasks
Recently, with the enormous success achieved in many different fields, deep learning techniques
have been gradually studied to improve the performance of source code retrieval techniques [13, 25,
26, 30, 48, 55, 62]. [25] is the first work that applies deep neural networks for the code retrieval task.
It first simply encodes both the query text and code snippet into a vector representation using MLP
or RNN and then try to rank these code snippets n the candidate set according to the similarity
score between the learned representations. Other following work is similar to [25] with only a
slight difference in choosing the encoding models. For instance, Cambronero et al. [13] utilized
FastText [9] to initialize the embeddings of all the tokens in queries and codes, and then aggregated
them with learnable attention weight or simply averages them; Husain et al. [30] learned the tokens
embeddings using different standard sequence models (e.g., Neural BoW, RNN, 1D-CNN, and BERT).
Yao et al. [62] explored a novel perspective of generating code annotations for code retrieval based
on the reinforcement learning framework. It first trains a code annotation model via reinforcement
learning to generate a natural language annotation and then uses the generated annotation to
better distinguish relevant code snippets from others. Haldar et al. [26] used the LSTM encoders to

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

111:18

Ling and Wu, et al.

represent code snippets based on both raw tokens and the converted string sequence of AST and
explored a bilateral multi-perspective matching model for semantic code searching.

All these work share a similar spirit that first maps both code and natural language description
into vectors in the same embedding space with sequence encoders (e.g., RNN, LSTM, Attention,
etc.), and then computes the cosine or L2 similarity of these vectors. However, DGMS differs from
previous work in two major dimensions: 1) we first propose to represent both code snippets and
query texts with the unified graph-structured data, which largely preserves their structural and
semantic information and can also be learned with graph neural networks; 2) DGMS not only
captures the semantic information for individual code snippets or query texts, but also explore
more fine-grained semantic relation between them for better representation.

5.2 Other Source Code Related Tasks
Other active research areas that involve machine learning in source code related tasks include code
summarization [1, 4, 6, 22, 31, 35], code generation [10, 43], etc. For code summarization, Iyer et
al. [31] proposed the first completely data-driven approach for generating high-level summaries of
code snippets by employing LSTM with the attention mechanism. Fernandes et al. [22] presented a
hybrid model that extends standard sequence encoder models with graph neural networks that
leverage additional structure in sequence data to summarize source codes. Alon et al. [4] presented
a novel code-to-sequence (code2seq) model which samples paths in the abstract syntax tree of
code snippets, encodes these paths with LSTM networks, and attends to them while generating the
target sequence. Alon et al. [6] also proposed a new attention-based neural network - code2vec -
for representing arbitrary-sized source codes using a learned fix-length continuous vector. The
code2vec uses the soft-attention mechanism over syntactic paths of the abstract syntax tree of the
source code and aggregates all of their vector representation into a contextual vector. Ahmad et
al. [1] explored the Transformer [53] model that uses the self-attention mechanism to capture
the long-range dependencies in code tokens. LeClair et al. [35] also exploited improving code
summarization via a graph neural network. For code generation, Oda et al. [43] leveraged the
statistical machine translation framework to automatically generate pseudo-code. Brockschmidt et
al. [10] proposed a novel generative model that uses graphs to represent intermediate states of
the generated output. It generates the code snippet by interleaving grammar-driven expansion
steps with graph augmentation and neural message passing steps. Alon et al. [5] presented a novel
structural language model that estimates the probability of AST of code snippet by decomposing it
into a product of conditional probabilities over the nodes.

6 CONCLUSION
We propose DGMS, a deep graph matching and searching model for semantic code retrieval tasks.
In particular, we represent both source codes and query texts with graph-structured data and then
encode and match them with the proposed DGMS model. Our model makes better use of the rich
structural information in source codes and query texts as well as the interaction semantic relations
between each other. Extensive experiments demonstrate that DGMS significantly outperforms
the state-of-the-art baseline models by a large margin on two benchmark datasets from two
representative programming languages (i.e., Java and Python). One limitation of our work is that
we consider the document descriptions of code snippets as the query texts to train our model as
there is no benchmark dataset that contains a large corpus of paired human-written questions and
corresponding code snippets. In future work, we hope to find more datasets with large amounts of
pairs of human-written query questions and corresponding code snippets, and then train and test
our model with these datasets from a more realistic scenario. Another future work is to explore
more effective ways to construct text graphs and code graphs for other programming languages

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:19

(i.e., C/C++, C#, PHP, etc.) as well as develop better graph comparison functions to further improve
the performance of semantic code retrieval tasks.

REFERENCES
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for Source
Code Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
(Short). ACL, Virtual Event, 4998â€“5007.

[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big

code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 1â€“37.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs.

In International Conference on Learning Representations. OpenReview.net, Vancouver, BC, Canada.

[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences from Structured Rep-
resentations of Code. In International Conference on Learning Representations. OpenReview.net, New Orleans, LA,
USA.

[5] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural language models of code. In Thirty-seventh

International Conference on Machine Learning. PMLR, Virtual Event, 245â€“256.

[6] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learning distributed representations of code.

Proceedings of the ACM on Programming Languages 3, POPL (2019), 40:1â€“40:29.

[7] Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. 2019. Simgnn: A neural network approach
to fast graph similarity computation. In Proceedings of the Twelfth ACM International Conference on Web Search and
Data Mining. ACM, Melbourne, Australia, 384â€“392.

[8] Vassileios Balntas, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. 2016. Learning local feature descriptors with
triplets and shallow convolutional neural networks. In Proceedings of the British Machine Vision Conference (BMVC).
BMVA Press, York, UK, 3.

[9] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword

information. Transactions of the Association for Computational Linguistics 5 (2017), 135â€“146.

[10] Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr Polozov. 2019. Generative code modeling
with graphs. In International Conference on Learning Representations. OpenReview.net, New Orleans, LA, USA.
[11] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard SÃ¤ckinger, and Roopak Shah. 1993. Signature verification using a
â€œsiameseâ€ time delay neural network. In Advances in neural information processing systems. Morgan-Kaufmann, Denver,
Colorado, USA, 737â€“744.

[12] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. 2017. Geometric deep

learning: going beyond euclidean data. IEEE Signal Processing Magazine 34, 4 (2017), 18â€“42.

[13] Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When deep learning met code
search. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. ACM, Tallinn, Estonia, 964â€“974.

[14] Yu Chen, Lingfei Wu, and Mohammed Zaki. 2020. Iterative Deep Graph Learning for Graph Neural Networks: Better
and Robust Node Embeddings. In Advances in Neural Information Processing Systems. Curran Associates, Inc., Virtual
Event.

[15] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL, Doha, Qatar,
1724â€“1734.

[16] Noam Chomsky. 1956. Three models for the description of language. IRE Transactions on information theory 2, 3 (1956),

113â€“124.

[17] Ronan Collobert and Samy Bengio. 2004. Links between perceptrons, MLPs and SVMs. In Proceedings of the Twenty-first

International Conference on Machine learning. ACM, Banff, Alberta, Canada, 23.

[18] Milan Cvitkovic, Badal Singh, and Animashree Anandkumar. 2019. Open Vocabulary Learning on Source Code with a
Graph-Structured Cache. In Proceedings of the 36th International Conference on Machine Learning. PMLR, Long Beach,
California, USA, 1475â€“1485.

[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies. ACL, Minneapolis, Minnesota, 4171â€“4186.
[20] Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. 2020. A Fair Comparison of Graph Neural Networks
for Graph Classification. In International Conference on Learning Representations. OpenReview.net, Addis Ababa,
Ethiopia. https://openreview.net/forum?id=HygDF6NFPB

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

111:20

Ling and Wu, et al.

[21] Facebook Research. 2019. Releasing a new benchmark and data set for evaluating neural code search models. https:

//ai.facebook.com/blog/neural-code-search-evaluation-dataset/.

[22] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured Neural Summarization. In Interna-

tional Conference on Learning Representations. OpenReview.net, New Orleans, LA, USA.

[23] Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with PyTorch Geometric. In ICLR Workshop

on Representation Learning on Graphs and Manifolds. OpenReview.net, New Orleans, LA, USA.

[24] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing
for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning. PMLR, Sydney, NSW,
Australia, 1263â€“1272.

[25] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep Code Search. In Proceedings of the 40th International

Conference on Software Engineering. Association for Computing Machinery, Gothenburg, Sweden, 933â€“944.

[26] Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, and Julia Hockenmaier. 2020. A Multi-Perspective Architecture for Semantic
Code Search. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. ACL, Virtual
Event, 8563â€“8568.

[27] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances

in Neural Information Processing Systems. Curran Associates, Inc., Long Beach, CA, USA, 1024â€“1034.

[28] Emily Hill, Lori Pollock, and K Vijay-Shanker. 2011. Improving source code search with natural language phrasal rep-
resentations of method signatures. In 2011 26th IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE Computer Society, Lawrence, KS, USA, 524â€“527.

[29] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735â€“1780.
[30] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet

challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

[31] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural
attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). ACL, Berlin, Germany, 2073â€“2083.

[32] Daniel Jurafsky and James H. Martin. 2019. Speech and Language Processing: An Introduction to Natural Language
Processing, Computational Linguistics, and Speech Recognition (3rd draft ed.). Online, https://web.stanford.edu/~jurafsky/
slp3/.

[33] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of the 2014 Conference on

Empirical Methods in Natural Language Processing (EMNLP). ACL, Doha, Qatar, 1746â€“1751.

[34] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th

International Conference on Learning Representations. OpenReview.net, Toulon, France.

[35] Alexander LeClair, Sakib Haque, Linfgei Wu, and Collin McMillan. 2020. Improved code summarization via a graph

neural network. arXiv preprint arXiv:2004.02843 (2020).

[36] Hongyu Li, Seohyun Kim, and Satish Chandra. 2019. Neural Code Search Evaluation Dataset. arXiv preprint

arXiv:1908.09804 (2019).

[37] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated Graph Sequence Neural Networks. In
4th International Conference on Learning Representations, Yoshua Bengio and Yann LeCun (Eds.). OpenReview.net, San
Juan, Puerto Rico.

[38] Xiang Ling, Lingfei Wu, Saizhuo Wang, Tengfei Ma, Fangli Xu, Alex X Liu, Chunming Wu, and Shouling Ji. 2020.
Hierarchical graph matching networks for deep graph similarity learning. arXiv preprint arXiv:2007.04395 (2020).
[39] Erik Linstead, Sushil Bajracharya, Trung Ngo, Paul Rigor, Cristina Lopes, and Pierre Baldi. 2009. Sourcerer: mining

searching internet-scale software repositories. Data Mining and Knowledge Discovery 18, 2 (2009), 300â€“336.

[40] Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei Wang, Dongmei Zhang, and Jianjun Zhao. 2015. Codehow: Effective
code search based on api understanding and extended boolean model. In 2015 30th IEEE/ACM International Conference
on Automated Software Engineering (ASE). IEEE Computer Society, Lincoln, NE, USA, 260â€“270.

[41] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The
Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations. ACL, Baltimore, MD, USA, 55â€“60.

[42] Collin McMillan, Mark Grechanik, Denys Poshyvanyk, Qing Xie, and Chen Fu. 2011. Portfolio: finding relevant
functions and their usage. In Proceedings of the 33rd International Conference on Software Engineering. ACM, Waikiki,
Honolulu , HI, USA, 111â€“120.

[43] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura.
2015. Learning to generate pseudo-code from source code using statistical machine translation (t). In 2015 30th
IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE Computer Society, Lincoln, NE,
USA, 574â€“584.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

Deep Graph Matching and Searching for Semantic Code Retrieval

111:21

[44] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank Citation Ranking: Bringing
Order to the Web. Technical Report 1999-66. Stanford InfoLab. http://ilpubs.stanford.edu:8090/422/ Previous number =
SIDL-WP-1999-0120.

[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An
imperative style, high-performance deep learning library. In Advances in neural information processing systems. Curran
Associates, Inc., Vancouver, BC, Canada, 8026â€“8037.

[46] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). ACL, Doha, Qatar,
1532â€“1543.

[47] Yu Rong, Tingyang Xu, Junzhou Huang, Wenbing Huang, Hong Cheng, Yao Ma, Yiqi Wang, Tyler Derr, Lingfei Wu,
and Tengfei Ma. 2020. Deep Graph Learning: Foundations, Advances and Applications. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, Virtual Event, 3555â€“3556.

[48] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish Chandra. 2018. Retrieval on source
code: a neural code search. In Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and
Programming Languages. ACM, Philadelphia, PA, USA, 31â€“41.

[49] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural

network model. IEEE Transactions on Neural Networks 20, 1 (2008), 61â€“80.

[50] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling
relational data with graph convolutional networks. In European Semantic Web Conference. Springer, Heraklion, Crete,
Greece, 593â€“607.

[51] Renuka Sindhgatta. 2006. Using an information retrieval system to retrieve source code samples. In Proceedings of the

28th international conference on software engineering. ACM, Shanghai, China, 905â€“908.

[52] Kenneth Slonneger and Barry L Kurtz. 1995. Formal syntax and semantics of programming languages. Addison-Wesley

Reading.

[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. Curran Associates,
Inc., Long Beach, CA, USA, 5998â€“6008.

[54] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio. 2018. Graph
Attention Networks. In 6th International Conference on Learning Representations. OpenReview.net, Vancouver, BC,
Canada.

[55] Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and Philip Yu. 2019. Multi-modal attention
network learning for semantic source code retrieval. In 2019 34th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE, San Diego, CA, USA, 13â€“25.

[56] Shuohang Wang and Jing Jiang. 2017. A Compare-Aggregate Model for Matching Text Sequences. In International

Conference on Learning Representations. OpenReview.net, Toulon, France.

[57] Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral Multi-Perspective Matching for Natural Language
Sentences. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17. ijcai.org,
Melbourne, Australia, 4144â€“4150.

[58] Lingfei Wu, Ian En-Hsu Yen, Zhen Zhang, Kun Xu, Liang Zhao, Xi Peng, Yinglong Xia, and Charu Aggarwal. 2019.
Scalable Global Alignment Graph Kernel Using Random Features: From Node Embedding to Graph Embedding. In
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, Anchorage,
AK, USA, 1418â€“1428.

[59] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2021. A comprehensive
survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems 32, 1 (2021), 4â€“24.
[60] Tian Xie and Jeffrey C Grossman. 2018. Crystal graph convolutional neural networks for an accurate and interpretable

prediction of material properties. Physical review letters 120, 14 (2018), 145301.

[61] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful are Graph Neural Networks?. In 7th

International Conference on Learning Representations. OpenReview.net, New Orleans, LA, USA.

[62] Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. CoaCor: code annotation for code retrieval with
reinforcement learning. In The World Wide Web Conference (WWW). ACM, San Francisco, CA, USA, 2203â€“2214.
[63] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. In Advances in Neural Information

Processing Systems. Curran Associates, Inc., MontrÃ©al, Canada, 5165â€“5175.

[64] Zhen Zhang, Yijian Xiang, Lingfei Wu, Bing Xue, and Arye Nehorai. 2019. KerGM: Kernelized graph matching. In
Advances in Neural Information Processing Systems. Curran Associates, Inc., Vancouver, BC, Canada, 3335â€“3346.

ACM Trans. Knowl. Discov. Data., Vol. 0, No. 0, Article 111. Publication date: January 2021.

