1
2
0
2

l
u
J

2
2

]

G
L
.
s
c
[

1
v
0
4
6
0
1
.
7
0
1
2
:
v
i
X
r
a

Hash-based Tree Similarity and Simpliﬁcation in
Genetic Programming for Symbolic Regression

Bogdan Burlacu1,2, Lukas Kammerer1,2
Michael Aﬀenzeller2,3, and Gabriel Kronberger1,2

1 Josef Ressel Centre for Symbolic Regression
2 Heuristic and Evolutionary Algorithms Laboratory
University of Applied Sciences Upper Austria, Softwarepark 11, 4232 Hagenberg
3 Institute for Formal Models and Veriﬁcation
Johannes Kepler University, Altenbergerstr. 69, 4040 Linz
bogdan.burlacu@fh-hagenberg.at

Abstract. We introduce in this paper a runtime-eﬃcient tree hashing
algorithm for the identiﬁcation of isomorphic subtrees, with two important
applications in genetic programming for symbolic regression: fast, online
calculation of population diversity and algebraic simpliﬁcation of symbolic
expression trees. Based on this hashing approach, we propose a simple
diversity-preservation mechanism with promising results on a collection
of symbolic regression benchmark problems.

1

Introduction

Tree isomorphism algorithms play a fundamental role in pattern matching for
tree-structured data. We introduce a fast inexact 4 tree matching algorithm that
processes rooted, unordered, labeled trees into sequences of integer hash values,
such that the same hash value indicates isomorphism. We deﬁne a distance
measure between two trees based on the intersection of their corresponding hash
value sequences. We are then able to eﬃciently compute a distance matrix for all
trees by hashing each tree exactly once, then cheaply computing pairwise hash
sequence intersections in linear time.

Genetic Programming (GP) for Symbolic Regression (SR) discovers math-
ematical formulae that best ﬁt a given target function by means of evolving a
population of tree-encoded solution candidates. The algorithm performs a guided
search of the space of mathematical expressions by iteratively manipulating
and evaluating a large number of tree genotypes under the principles of natural
selection.

Dynamic properties such as exploratory or exploitative behaviour and conver-
gence speed have a big inﬂuence on GP performance [2]. Exploration refers to

0 The ﬁnal publicaiton is available at https://link.springer.com/chapter/10.1007%

2F978-3-030-45093-9_44

4 Inexact due to the possibility of hash collisions causing the algorithm to return the
wrong answer. With a reasonable hahs function, collision probability is negligible.

 
 
 
 
 
 
the ability to probe diﬀerent areas of the search space, and exploitation refers
to the ability to produce improvements in the local neighbourhood of existing
solutions. The algorithm’s success depends on achieving a good balance between
exploration and exploitation over the course of the evolutionary run [11].

Population diversity (measured either at the structural-genotypic or semantic-
phenotypic level) is typically used as an indirect measure of the algorithm’s
state-of-convergence, following the reasoning that the exploratory phase of the
search is characterized by high diversity and the exploitative phase is characterized
by low diversity. Premature convergence can occur at both genotype or phenotype
levels, leading to a large amount of shared genetic material in the ﬁnal population
and a very concentrated set of behaviours [3].

Based on tree node hash values, we propose a new diversity measure deﬁned
as an individual’s average distance to the rest of the population. We show how
the resulting diversity score associated with each individual can be used to shift
the focus of selection towards ﬁt-and-diverse individuals.

Section 2 describes the methodology in detail. Section 3 summarizes the
empirical results of our diversity-focused algorithmic improvements, and in
Section 4 we oﬀer our ﬁnal remarks and conclusions.

2 Methodology

In this section we describe the tree hashing algorithm in detail and introduce
a tree distance measure based on the number of common hash values between
two tree individuals. The method lends itself well to the eﬃcient computation of
distance matrices (for the entire population) since each individual needs only be
hashed once and pairwise distances can be subsequently computed using only
the corresponding hash value sequences.

An important aspect of our methodology is the ability of the proposed diversity
measure to implicitly capture an individual’s semantics by hashing the numerical
coeﬃcients associated to the tree’s leaf nodes. This allows us to diﬀerentiate (in
terms of tree distance) between individuals with similar structure but diﬀerent
semantics.

Tree hashing The hashing procedure shares some common aspects with the
earlier algorithm by Merkle [7] where data blocks represented as leaves in the
tree are hashed together in a bottom-up manner.

In our approach an associated data block for each tree node is aggregated
together with child node data as input to the hash function to a general-purpose
non-cryptographic hash function ⊕. This concept is illustrated in Figure 1, where
leaf nodes L1, ..., L4 represent terminal symbols and internal nodes represent
mathematical operations. Hash values are computed by the ⊕ operator taking as
arguments the node’s own data hash and its child hash values.

The procedure given as pseudocode in Algorithm 1 relies on the linearization
of input tree T , such that the resulting array of nodes corresponds to a postorder
traversal of T . The beneﬁt of linearization is that subtrees are represented by

Root
H = ⊕(Root, H0, H1)

N0
H0 = ⊕(N0, H0,0, H0,1)

N1
H1 = ⊕(N1, H1,0, H1,1)

N0,0
H0,0 = ⊕(N0,0, L1)

N0,1
H0,1 = ⊕(N0,1, L2)

N1,0
H1,0 = ⊕(N1,0, L3)

N1,1
H1,1 = ⊕(N1,1, L4)

L1

L2

L3

L4

Fig. 1. Example hash tree where each parent hash value is aggregated from its initial
hash value and the child node hash values. Leaf nodes L1, ..., L4 represent data blocks.

continuous array regions, thus facilitating indexing and sorting operations. For
example, tree node n with postorder index i will ﬁnd its ﬁrst child at index
j = i − 1, its second child at index k = j − Size(j) and so on, where Size(j)
returns the size of the subtree whose root node has index j.

Sorting the child nodes of commutative symbols is a key part of this procedure.
In the general case, this requires a reordering of the corresponding subarrays
using an auxiliary buﬀer. Sorting without an auxiliary buﬀer is possible when
all child nodes are leafs, that is when Size(n) = Arity(n) for a parent node n.
Child order is established according to the calculated hash values.

We illustrate the sorting procedure in Figure 2, where a postﬁx expression
with root symbol S at index 9 contains three child symbols with indices 2, 5, 8
and hash values H2, H5, H8, respectively. In order to calculate ⊕(S) the child
order is ﬁrst established according to symbol hash values. Leaf nodes are assigned
initial hash values that do not change during the procedure. Assuming that
sorting produces the order {S5, S2, S3}, the child symbols and their respective
subarrays need to be reordered in the expression using a temporary buﬀer. After
sorting, the original expression becomes:

{[S0, S1, S2], [S3, S4, S5], [S6, S7, S8], S9} → {[S4, S5, S6], [S1, S2, S3], [S7, S7, S8], S9}

The sorted child hash values are then aggregated with the parent label in
order to produce the parent hash value H9 = ⊕({H5, H2, H8, S9}). The hashing
algorithm alternates hashing and sorting steps as it moves from the bottom level
of the tree towards the root node. Finally, each tree node is assigned a hash value
and the hash value of the root node is returned as the expression’s hash value.

Hash-based tree simpliﬁcation Our hash-based approach to simpliﬁcation
enables structural transformations based on tree isomorphism and symbolic
equivalence relations. Figure 3 illustrates the simpliﬁcation of an addition function
node with two identical child nodes.

In the expression E = c1x + c2yz + c3x represented in postﬁx notation, the
terms c1x and c3x are isomorphic and hash to the same value. The simpliﬁcation
algorithm identiﬁes the possibility of folding the constants c1 and c3 so that the

0

S0

1

S1

2

S2, H2

3

S3

4

S4

5

S5, H5

6

S6

7

S7

8

9

Indices

S8, H8

Child 1

Child 2

Child 3

Size of the whole subtree rooted at n

Parent symbol s
(at index i)

Fig. 2. Example internal node with three child subtrees in postﬁx representation. Child
subarrays are reordered according to priority rules and their respective root hash values
H2, H5, H8.

Algorithm 1: Expression hashing
input : A symbolic expression tree T
output : The corresponding sequence of hash values

1 hashes ← empty list of hash values;
2 nodes ← post-order list of T ’s nodes;
3 foreach node n in nodes do
4

H(n) ← an initial hash value;
if n is a function node then

5

6

7

8

9

if n is commutative then

Sort the child nodes of n;

child hashes ← hash values of n’s children;
H(n) ← ⊕(child hashes, H(n));

10

hashes.append(H(n));

11 return hashes;

two terms are simpliﬁed to a single term c4x where c4 = c1 + c3. The simpliﬁed
expression then becomes E(cid:48) = c4x + c2yz ≡ E.

c3

z
29375 27012 29320 29375 65245 52308 29320 29375 27012 29320 29319

c1

c2

×

×

×

+

x

x

y

⇓ Sort and hash

c1

×
29375 27012 75236 29320 29375 75236 29320 65245 52308 47983 31738

c3

c2

×

×

+

x

x

y

z

⇓ Simplify

c4

x

×

c2

y

z

×

+

29375 27012 75236 29320 65245 52308 47983 31040

Fig. 3. Expression simpliﬁcation. The original expression (top) is sorted and hashed
according to Algorithm 1. Isomorphic subtrees with the same hash value are simpliﬁed.

Hash-based population diversity Using the algorihm described in Section 2,
we convert each tree individual into a sequence of hash values corresponding to a
post-order traversal of its structure. After this conversion, the distance between
two trees can be deﬁned using the intersection between the two sequences5:

D(T1, T2) =

2 · |H1 ∩ H2|
|H1| + |H2|

(1)

5 The Sørensen-Dice coeﬃcient (Equation 1) returns a value in the interval [0, 1]

where H1, H2 represent the hash value sequences of T1 and T2, respectively.

Computing the distance matrix for the entire population can be further
optimized by hashing all trees in an initial pass, then using the resulting hash
value sequences for the calculation of pairwise distances. This leads to the
following algorithmic steps:

1. Convert every tree Ti in the population to hash value sequence Hi
2. Sort each hash value sequence Hi in ascending order
3. For every pair (Hi, Hj) compute distance using Equation 1.

Sorting in step 2 allows us to eﬃciently compute |Hi ∩ Hj| in linear time. As
shown in 1, in this particular scenario, these optimizations lead to a two order of
magnitude improvement in runtime performance over similar methods [10].

Tree distance method Elapsed time (s) Speed-up

Bottom-up
Hash-based

1225.751
3.677

1.0x
333.3x

Table 1. Elapsed time computing average distance for 5000 trees

Diversity as an explicit search objective Diversity maintenance strategies
have been shown to improve GP performance [11,3]. A plethora of diversity
measures have already been studied: history-based, distance-based, diﬀerence-
based, entropy-based, etc. [11]. However, due to high computational requirements,
tree distances like the tree edit distance have seldomly been used. Our hash-based
approach overcomes this limitation making it feasible to compute the population
distance matrix every generation.

We deﬁne an individual’s diversity score as its average distance to the rest
of the population. The score is easily computed by averaging the corresponding
distance matrix row for a given individual. In eﬀect, this causes selection to favor
individuals “farther away” from the crowd, reducing the eﬀects of local optima
as attractors in the search space.

We integrate this new objective into a single-objective approach using standard

GP and a multi-objective approach using the NSGA-2 algorithm [4].

In the single-objective case, we extent the standard ﬁtness function with
an additional diversity term d, such that the new ﬁtness used during selection
becomes: f (cid:48) = f + d. Since ﬁtness is normalized between [0, 1] both terms have
the same scale and no additional weighting is used.

In the multi-objective case the diversity score is used as a secondary objective
along with ﬁtness. The NSGA-2 algorithm performs selection using crowding
distance within Pareto fronts of solutions.

3 Empirical Results

We test the proposed approach on a collection of synthetic symbolic regres-
sion benchmarks: Vladislavleva [12], Poly-10 [9], Spatial Coevolution [8], Fried-

man [5] and Breiman [1]. We conﬁgure all algorithms to evolve a popula-
tion of 1000 individuals over 500 generations with a function set consisting of
(+, −, ×, ÷, exp, log, sin, cos, square). Diﬀerent types of mutation (remove branch,
replace branch, change node type, one-point mutation) are applied with a proba-
bility of 25%. Tree individuals are initialized using the Probabilistic Tree Creator
(PTC2) [6].

The results summarized in Table 2 show the beneﬁts of diversity maintenance.
Selecting for diversity enables both the GA and NSGA-2 algorithms to better
exploit population diversity and achieve better results in comparison with the
standard GA approach. The NSGA-2 algorithm in particular is able to avoid
overﬁtting and produce more generalizable models on the Vladislavleva-6 and
Vladislavleva-8 problems. In all tested problem instances, both GA Diversity
and NSGA-2 outperform standard GA on both training and test data. Figure 4
shows that GA Diversity and NSGA-2 are able to maintain higher diversity and
promote smaller, less bloated individuals.

The overhead incurred by tree distance calculation depends on the size of the
training data. For large data, this overhead becomes negligible as the algorithm
will spend most of its runtime evaluating ﬁtness. In our tests GA Diversity is
approximately 20-25% slower than Standard GA. A direct runtime comparison
with NSGA-2 is not possible due to diﬀerent algorithmic dynamics.

Fig. 4. Evolution of average diversity and average tree length

4 Summary

We described a hashing algorithm for GP trees with applications to distance
calculation and expression simpliﬁcation. The approach is highly eﬃcient, making
it feasible to measure diversity on a generational basis, as after an initial pre-
processing step tree distance is reduced to a simple co-ocurrence count between
sorted hash value sequences. With this information new diversity preservation
strategies become possible at low computational cost.

A simple strategy illustrated in this work is to bias selection towards individ-
uals that are more distant from the rest of the population. Experimental results
using the standard GA and NSGA-2 algorithms showed increased model accuracy
on all tested problem instances, correlated with increased diversity and lower

0100200300400500Generation0.70.80.91.0Population diversityNSGA-2GAGA Diversity0100200300400500Generation25303540Average tree lengthNSGA-2GAGA Diversityaverage tree size. Although easily integrated with all GA ﬂavours, we conclude
empirically that the strategy is more eﬀective in the multi-objective case when
diversity is separately considered.

Future work in this area will focus on mining common subtrees in the pop-
ulation based on hash value frequencies and designing more complex diversity
preservation strategies.

Acknowledgement

The authors gratefully acknowledge support by the Christian Doppler Research
Association and the Federal Ministry of Digital and Economic Aﬀairs within the
Josef Ressel Centre for Symbolic Regression.

References

1. Breiman, L., Friedman, J., Stone, C., Olshen, R.: Classiﬁcation and Regression Trees.
The Wadsworth and Brooks-Cole statistics-probability series, Taylor & Francis
(1984)

2. Burke, E.K., Gustafson, S., Kendall, G.: Diversity in genetic programming: An
analysis of measures and correlation with ﬁtness. IEEE Transactions on Evolutionary
Computation 8(1), 47–62 (Feb 2004)

3. Burks, A.R., Punch, W.F.: An analysis of the genetic marker diversity algorithm
for genetic programming. Genetic Programming and Evolvable Machines 18(2),
213–245 (Jun 2017)

4. Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A fast and elitist multiobjective
genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation 6(2),
182–197 (April 2002)

5. Friedman, J.H.: Multivariate adaptive regression splines. Ann. Statist. 19(1), 1–67

(03 1991)

6. Luke, S.: Two fast tree-creation algorithms for genetic programming. Evolutionary

Computation, IEEE Transactions on 4(3), 274–283 (2000)

7. Merkle, R.C.: A digital signature based on a conventional encryption function.
In: Pomerance, C. (ed.) Advances in Cryptology — CRYPTO ’87. pp. 369–378.
Springer Berlin Heidelberg, Berlin, Heidelberg (1988)

8. Pagie, L., Hogeweg, P.: Evolutionary consequences of coevolving targets. Evolution-

ary Computation 5(4), 401–418 (1997)

9. Poli, R.: A simple but theoretically-motivated method to control bloat in genetic
programming. In: Genetic Programming. pp. 204–217. Springer Berlin Heidelberg,
Berlin, Heidelberg (2003)

10. Valiente, G.: An eﬃcient bottom-up distance between trees. In: Proc. 8th Int.
Symposium on String Processing and Information Retrieval. pp. 212–219. IEEE
Computer Science Press (2001)

11. ˇCrepinˇsek, M., Liu, S.H., Mernik, M.: Exploration and exploitation in evolutionary

algorithms: A survey. ACM Comput. Surv. 45(3), 35:1–35:33 (Jul 2013)

12. Vladislavleva, E.J., Smits, G.F., den Hertog, D.: Order of nonlinearity as a com-
plexity measure for models generated by symbolic regression via pareto genetic
programming. IEEE Transactions on Evolutionary Computation 13(2), 333–349
(Apr 2009)

Problem data

Algorithm

R2 (training)

R2 (test)

Time (s)

Breiman - I
Breiman - I
Breiman - I

Friedman - I
Friedman - I
Friedman - I

Friedman - II
Friedman - II
Friedman - II

Pagie-1
Pagie-1
Pagie-1

Poly-10
Poly-10
Poly-10

GA
GA Diversity
NSGA-2

GA
GA Diversity
NSGA-2

GA
GA Diversity
NSGA-2

GA
GA Diversity
NSGA-2

GA
GA Diversity
NSGA-2

0.865 ± 0.037
0.870 ± 0.040
0.875 ± 0.034
0.870 ± 0.029
0.885 ± 0.013 0.879 ± 0.012

0.860 ± 0.006
0.859 ± 0.007
0.860 ± 0.007
0.860 ± 0.005
0.863 ± 0.002 0.863 ± 0.003

0.942 ± 0.041
0.944 ± 0.038
0.957 ± 0.023
0.957 ± 0.025
0.958 ± 0.008 0.957 ± 0.010

0.889 ± 0.107
0.990 ± 0.012
0.994 ± 0.005
0.912 ± 0.070
0.998 ± 0.002 0.932 ± 0.075

0.764 ± 0.478
0.820 ± 0.293
0.840 ± 0.089
0.838 ± 0.120
0.879 ± 0.072 0.850 ± 0.099

Vladislavleva-1 GA
Vladislavleva-1 GA Diversity
Vladislavleva-1 NSGA-2

0.946 ± 0.120
0.999 ± 0.001
0.999 ± 0.001
0.980 ± 0.106
1.000 ± 0.000 0.987 ± 0.026

Vladislavleva-2 GA
Vladislavleva-2 GA Diversity
Vladislavleva-2 NSGA-2

0.987 ± 0.028
0.995 ± 0.012
0.992 ± 0.016
0.995 ± 0.012
0.999 ± 0.001 0.998 ± 0.002

Vladislavleva-3 GA
Vladislavleva-3 GA Diversity
Vladislavleva-3 NSGA-2

0.932 ± 0.508
0.968 ± 0.062
0.979 ± 0.048
0.975 ± 0.053
0.995 ± 0.014 0.989 ± 0.042

Vladislavleva-4 GA
Vladislavleva-4 GA Diversity
Vladislavleva-4 NSGA-2

0.918 ± 0.053
0.951 ± 0.036
0.968 ± 0.023
0.936 ± 0.049
0.982 ± 0.017 0.966 ± 0.027

Vladislavleva-5 GA
Vladislavleva-5 GA Diversity
Vladislavleva-5 NSGA-2

0.997 ± 0.012
0.933 ± 0.144
0.995 ± 0.015
0.999 ± 0.002
1.000 ± 0.000 0.997 ± 0.023

Vladislavleva-6 GA
Vladislavleva-6 GA Diversity
Vladislavleva-6 NSGA-2

0.072 ± 0.329
0.869 ± 0.155
0.939 ± 0.143
0.255 ± 0.978
1.000 ± 0.023 1.000 ± 0.347

Vladislavleva-7 GA
Vladislavleva-7 GA Diversity
Vladislavleva-7 NSGA-2

0.878 ± 0.100
0.895 ± 0.048
0.904 ± 0.029
0.892 ± 0.058
0.917 ± 0.017 0.899 ± 0.033

Vladislavleva-8 GA
Vladislavleva-8 GA Diversity
Vladislavleva-8 NSGA-2

0.541 ± 0.569
0.962 ± 0.080
0.986 ± 0.033
0.787 ± 0.436
0.992 ± 0.012 0.817 ± 0.412

1084.5
1207.5
1584.0

1090.5
1207.0
1554.0

1092.0
1248.0
1474.0

442.5
540.5
880.0

405.0
518.0
902.5

371.5
450.0
784.5

355.0
462.0
802.0

445.0
570.0
920.5

527.0
651.0
973.5

407.0
502.5
871.0

369.0
491.0
930.0

395.0
501.5
843.5

362.5
489.5
799.0

Table 2. Empirical results expressed as median R2± interquartile range over 50
algorithm repetitions

