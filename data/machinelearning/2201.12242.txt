Large Scale Generation of Labeled Type Data for Python

Ibrahim Abdelaziz
IBM TJ Watson Research Center
Yorktown Heights, NY, United States
ibrahim.abdelaziz1@ibm.com

Julian Dolby
IBM TJ Watson Research Center
Yorktown Heights, NY, United States
dolby@us.ibm.com

Kavitha Srinivas
IBM TJ Watson Research Center
Yorktown Heights, NY, United States
kavitha.srinivas@ibm.com

2
2
0
2

b
e
F
6

]
L
P
.
s
c
[

2
v
2
4
2
2
1
.
1
0
2
2
:
v
i
X
r
a

ABSTRACT
Recently, dynamically typed languages, such as Python, have gained
unprecedented popularity. Although these languages alleviate the
need for mandatory type annotations, types still play a critical
role in program understanding and preventing runtime errors. An
attractive option is to infer types automatically to get static guar-
antees without writing types. Existing inference techniques rely
mostly on static typing tools such as PyType for direct type in-
ference; more recently, neural type inference has been proposed.
However, neural type inference is data hungry, and depends on col-
lecting labeled data based on static typing. Such tools, however, are
poor at inferring user defined types. Furthermore, type annotation
by developers in these languages is quite sparse. In this work, we
propose novel techniques for generating high quality types using
1) information retrieval techniques that work on well documented
libraries to extract types and 2) usage patterns by analyzing a large
repository of programs. Our results show that these techniques are
more precise and address the weaknesses of static tools, and can
be useful for generating a large labeled dataset for type inference
by machine learning methods. F1 scores are 0.52-0.58 for our tech-
niques, compared to static typing tools which are at 0.06, and we
use them to generate over 37,000 types for over 700 modules.

KEYWORDS
Dynamically typed languages, type inference, static analysis, Python,
big code, mining software repositories

1 INTRODUCTION
Dynamically typed languages such as Python have become very
popular1, thanks in part to the unprecedented growth of Artificial
Intelligence (AI) and the wide adoption of Python for AI frame-
works. Python, like many dynamic programming languages, does
not enforce types statically, but discovers errors only at runtime,
which is popular because it allows programmers to build prototypes
quickly. Types, however, are useful for program understanding, for
finding errors early and improving program correctness. Python
3 introduced optional type declarations with PEP484 [14], but so
far there has been little adoption: a recent study showed that fewer
than 4% of repositories from a sample of GitHub had any anno-
tations, and, even in those that did, 80% of files did not contain a
single annotation [12]. Furthermore, traditional type inference has
so far proved largely ineffective: the most common tools are MyPy
and Pytype, and they infer only 6% of user provided annotations
for types that are not builtins nor primitives. As shown in Figure 1,
they frequently produce Any as a type, which is equivalent to no
information. Furthermore, only 14% of the types they produce are

1https://stackoverflow.blog/2017/09/06/incredible-growth-python/

user defined or library types, which tend to be much more prevalent
in user code [12].

In this situation, machine learning has become a promising ap-
proach; recent systems include Typilus [2] and TypeWriter [11]
perform type inference using neural networks. However, learning
approaches require large amounts of type-annotated code for train-
ing, which as we have seen, does not exist. In fact, neural systems
currently rely on tools such as Pytype and MyPy [2] or user speci-
fied annotations [11] for their gold standard. This labeled data is
skewed in ways that will affect the quality of the model that is built,
and will provide potentially misleading estimates of accuracy when
used as a gold standard.

Figure 1: PyType predictions for a dataset of 408 repositories

In this work, we explore techniques to generate high quality
types for methods, which can serve as labeled data for data-hungry
machine learning approaches. We do believe machine learning
techniques have promise for the problem of general type inference.
However, we definitely need better mechanisms to address the
problem of obtaining good quality labeled data. In particular, we
explore whether it is possible to (a) extract high quality types from
well documented framework code to infer types, and (b) extract high
quality types from their usage in open source code. Our goal here
is not so much to propose a mechanism for general type inference
as to produce better datasets for use in building better probabilistic
type inference systems.

We start with framework data because they are both well used,
and well documented. To infer types from documentation, we use
techniques from information retrieval to gather possible types spec-
ified in documentation and map them to a set of classes we index. To
infer types from usage, we mine usage from millions of programs

 
 
 
 
 
 
on GitHub, and explore duck typing based on program analysis.
Duck typing refers to the idea that if a class contains all the meth-
ods called on a given object, then it is a likely candidate for the
type of that object. In other words, if the object walks like a duck
and quacks like a duck then it is quite possibly a duck. Although
the idea of duck typing is not new, we apply it in a novel way.
Specifically, we analyze a large repository of 1.3 million code files
from GitHub, and combine usage of the same libraries across them.
While we analyze individual programs, we observe how data flows
from common API calls to objects returned by the calls across all
programs. The duck typing method has the potential to infer user
defined types; it cannot infer primitives or builtin types since they
do not correspond to classes that we know about. Type inference
from documentation has the potential to do better on builtins and
primitives. Our hypothesis is that by combining these techniques
we could infer a greater variety of types, and offset weaknesses in
each technique.

We focus on three research questions:

(1) Do these two techniques yield types that are precise enough

to provide high quality labeled data?

(2) Do they yield types that address some of the weaknesses of
tools such as Pytype which currently are the state-of-the-art
for obtaining labeled type inference data in Python?

(3) Do the two techniques provide non-overlapping sets of types,
such that the union of the two approaches increases the size
of the labeled set?

Our experiments show that the two techniques produce types
for over 37,000 methods and functions in 756 Python modules. We
compare the precision and recall of our type inference techniques
against a set of types inferred from dynamic techniques, as well
as manual annotations for sample sizes of over 200 functions. Our
F1 scores were .52 and .58 for static analysis and documentation
inference, compared to PyType which was .06. We note that state-
of-the-art neural prediction systems such as TypeWriter achieve
0.64 from a trained model based on type annotations. Our approach
is completely unsupervised and we hope that this method of pro-
ducing labeled data will be helpful for building better neural models
for type inference. The data and code used to create it are publicly
available https://github.com/wala/graph4code.

2 A RUNNING EXAMPLE
Figure 2 shows the core ideas behind large scale generation of
labeled types for API calls. Script 1 in the example calls a function
to read a pandas.Dataframe object from the pandas library, and
then passes the return value into a function, where the object is used
as a receiver for the drop and drop_na calls. Script 2 has a more
direct relation between the read_csv call and the to_csv call on the
returned object. From the perspective of duck typing across multiple
scripts, it is clear that the type of returned objects from read_csv
calls must support drop, drop_na, head and the to_csv. From the
type definitions of classes of APIs, pandas.Dataframe is clearly a
candidate class. Figure 2 shows the documentation associated with
the read_csv function, and it is clear that the documentation is
not formal enough to clearly denote the class being referred to, so
we need additional processing to infer the possible type. Classes
are mentioned informally, with no reference to their fully qualified

Trovato and Tobin, et al.

Figure 2: Code and documentation example for read_csv

name. They are referred to with natural language using phrases
such as DataFrame or TextParser, so we need some mechanism to
resolve the two classes mentioned here to their fully qualified names
(e.g. pandas.core.frame.DataFrame). In our current work, we use
simple techniques from information retrieval to find potential types
from documentation.

3 DATASET
Our dataset is based on 1.3 million Python programs on GitHub,
which we gathered from Google’s public datasets of Python pro-
grams from BigQuery with a query to Google BigQuery that focused
on Python repositories that more than watch one event in the last
year. The query was issued in August 2019, but reflected a snapshot
of GitHub from March 20, 2019, by Google.

To gather relevant classes and methods, we identified the top
500 modules imported in these 1.3 million Python programs. For
each of these modules, we tried to programmatically create a virtual
environment, install the module using pip, and then used the python
inspect APIs to gather all the classes in the loaded modules, as well
as their methods and relevant docstrings. Python introspect APIs
do not just provide classes from the loaded module, they gather
classes from the modules that are in the dependency tree of the
loaded module. Furthermore, a quirk of the Python inspect API is
that it specifies numerous classes that alias to the same class, based
on the dependency of the module. Table 1 shows such an example

Large Scale Generation of Labeled Type Data for Python

Class

Aliases To

statsmodels.datasets.utils.DataFrame
statsmodels.stats.anova.DataFrame
bokeh.core.properties.PandasDataFrame
bokeh.core.properties.PandasDataFrame

pandas.core.frame.DataFrame
pandas.core.frame.DataFrame
bokeh.core.property.pandas.PandasDataFrame
bokeh.core.property.pandas.PandasDataFrame

Table 1: Examples of classes that alias to a different class

- the first two DataFrame classes from statsmodels actually map
to a class in an entirely different module pandas. Furthermore,
because of Python packaging, multiple Python classes from within
a module appear with different qualified names (as shown by the
bokeh classes).

From the seed set of 500 modules that we started with, we ended
up with a result set of 1017 modules, 167,872 classes and 164,134
functions. To cleanse the dataset, we loaded each of 167,872 classes
returned by the inspect API in a virtual environment, loaded the
class using the name returned by the API, and then noted its actual
name when we printed a string representation of the class. We
derived a map of classes to the class they were really aliased to
as shown in Table 1, which resulted in 92,277 unique classes after
aliasing. We employed a similar approach to alias function names;
we loaded 164,134 function names, that ended up aliasing to 91,818
functions.

Figure 3 describes the distribution of classes in the top 25 mod-
ules. As one can see from the figure, the modules cover a diverse set
of functionality; it contains libraries from visualization (e.g. plotly)
to cloud management (e.g. kubernetes) to data science libraries
(e.g. sklearn and pandas). In total we had 26,800 class methods and
53,441 functions with docstrings.

4 TYPE INFERENCE WITH DOCSTRINGS
4.1 Extraction of types
As shown in Figure 2, documentation in API libraries is well struc-
tured, and tends to be written using rich structured text to en-
able documentation generation from packages like Sphinx. One
question we addressed was how one might leverage information
retrieval techniques to infer type information from such documenta-
tion. We focus here on returns, to illustrate our method, as described
in Algorithm 1. Given a set of modules 𝑙, we gather all the functions
and methods declared in the module into 𝑙 𝑓 . For each 𝑓 , we collect
its class (if it is a method) into a set 𝐶, and get the correspond-
ing docstring 𝑟 . We use the sphinx library in Python to parse the
docstring into restructured text. In our example, this will strip the
‘Returns’ portion off the entire method docstring, so we have the
text shown in Figure 2. This structured text 𝑟 contains each class
and function’s return value in an informal manner; for instance, in
Figure 2, it is stated that the return value is either a DataFrame or a
TextParser. To infer the qualified type, we create a ‘document’ 𝑑 for
each function or method, setting the fields of function and content,
and index 𝑑 in an ElasticSearch text index. At the end of inspection
of all modules, every method’s return type has been added to the
index. We then loop through all classes in 𝐶 and search the Elas-
ticSearch index for all 𝑑 documents that have this class mentioned

in them in their return text. Each 𝑑 has an inferred type set where
the fully qualified classname 𝑐 is added the function’s return type.
Every 𝑑 in the index at the end of the extraction process is then a
function for which we have inferred a type based on docstrings, if
𝑑 has an inferred type field set.

4.2 Cleansing
Because type inference with this mechanism can be quite noisy, we
employ a postprocessing step to filter out erroneous annotations.
In particular, for a method and its list of inferred types returned
from the above step, we perform the following:

• Using the map of classes to the class they were really aliased
to (see Table 1), we map each return user-defined type to
its correct alias. For example, the class pandas.DataFrame
gets mapped to pandas.core.frame.DataFrame. We note
that both forms are valid in Python, and in fact,
frequently contain imports of pan-
user code will
das.DataFrame, but at runtime the interpreter will return
pandas.core.frame.DataFrame.

• Remove any type that can not be resolved to any valid type,
based on classes that the inspect API provides us, but they
fail when one tries to load them at runtime because they do
not exist.

• Remove user defined types from different libraries, when we
have classes as return types which are candidates for the
type within the same library. This last approach is based on
the heuristic that if a class with the same name is present
in the same library it is more likely to be a candidate for
return than a class with the same name from another library.
We note that existing systems for type inference such as
TypeWriter [11] ignore the fully qualified name of the class,
which is problematic because we observed this as an issue
in our work.

• Remove all other classes if a builtin or a primitive is a match.
This step is necessary to avoid matches to classes which
have the same name as a builtin or a primitive (e.g., Dict)
but clearly are unlikely matches.

5 TYPE INFERENCE WITH ANALYSIS
One method to infer types is to perform dataflow over millions of
scripts in GitHub, and observe what methods get called on objects
returned by a specific method call. We outline in 5.1 a novel set of
changes we introduced into static analysis infrastructure to support
this type of analysis. We then describe how to actually perform
duck typing in Section 5.2.

Trovato and Tobin, et al.

Figure 3: Class distribution for top 25 modules

Algorithm 1 Docstring extraction algorithm

⊲ The list of modules
⊲ set of all classes

⊲ list of methods

⊲ inspect function
⊲ Sphinx to parse text
⊲ doc for ElasticSearch

⊲ ElasticSearch index

⊲ search for all mentions of c

3:

1: procedure Extract(𝑙)
𝐶 ← new set
2:
for each 𝑚 in 𝑙 do
𝑙 𝑓 ← inspect(𝑚)
for each 𝑓 in 𝑙 𝑓 do

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

add f.class to 𝐶
𝑟 ← inspect(𝑓 )
𝑡 ← parse(𝑟 )
𝑑 ← new JSON
𝑑.𝑓 𝑢𝑛𝑐𝑡𝑖𝑜𝑛 ← 𝑓
𝑑.𝑐𝑜𝑛𝑡𝑒𝑛𝑡 ← 𝑡
index(d)

end for

end for
for each 𝑐 in 𝐶 do
𝑙𝑑 ← search(𝑐)
for each 𝑑 in 𝑙𝑑 do
𝑑.𝑟𝑒𝑡𝑡𝑦𝑝𝑒 ← 𝑐

end for

end for

20:
21: end procedure

5.1 Extended Analysis Approach
To perform this dataflow, we confined the scope of our analysis
to the level of each Python file in GitHub. Analysis needs starting
points. We used each method in the script as a starting point, as
well as the script itself to ensure maximal coverage of the code in
the script. Our analysis was inter-procedural, so that as shown in

Figure 2a we followed the dataflow into the procedure massage_-
data to find that the return value of pandas.read_csv has both
dropna and drop called on it, followed by a call to head guarded
by a conditional.

No Python script is self-contained; it always includes imports
of libraries and API calls, or user modules with code contained in
other files. To perform analysis on a large number of files under
such circumstances, it was important to not assume that we would
be able to create a large number of stubs for such calls, or assume
that we could analyze the library code. We created a mechanism
we termed ‘turtles‘ to handle such imports or calls on functions
that were not part of the script. The basic approach is that all
returns from API calls are represented as instances of a single
“turtle” type and all calls on such objects return new instances of
that type. Similarly, access to properties of those objects return
the object itself. This can be expressed easily in common analysis
frameworks and formalisms, as it requires customization of three
aspects of analysis. We present these three in terms of the analysis
abstractions that need to be customized for any analysis framework.
We also make an actual implementation available as open source
for the larger community2.

Overall, there are 3 key changes required for any analysis frame-

work to allow a turtle based analysis of the program:

(1) The imports of the required APIs need to be replaced by turtle
creations. The way import calls are represented will vary
amongst analysis frameworks but in our implementation, we
modeled the import call itself directly as a call to a synthetic
function that returns a newly-allocated object of “turtle”
type. This function is analyzed using call-site sensitivity,
i.e. a separate analysis for each call, so that each API import

2URL will be provided should the paper be accepted

Large Scale Generation of Labeled Type Data for Python

creates a different turtle object. In Figure 2, read_csv is
imported, so the return of the call on it is represented by a
turtle.

(2) The semantics of property reads need to be changed so that
any property read of a turtle returns the container object
itself. We model this by performing field-insensitive analysis
for objects of turtle type, i.e. by modeling all properties of
those objects with a single property. And, when turtle objects
are created, we assign the turtle object itself to its single
property.

(3) The semantics of function calls must be augmented such
that any call on an object of turtle type to be a synthetic
function that returns a new turtle object. For function calls,
we simply model every function with the same synthetic
function that returns a new turtle. In Python, a call like
pd.read_csv consists of first a property read and then a
call. Since property reads on turtles return the same object
already, the synthetic model of function calls suffices for
method calls too.

Figure 4: Dataflow for script 1

Our extended analysis framework performs a standard combined
call graph construction and pointer analysis that computes, for each
call site, what functions may be called, and for each value, what
objects it may hold. Analysis starts at a root function, analyzing
each instruction, adding functions found at call sites to a work
queue. To make the workings of the analysis more concrete, we
will use the IR for the script of Figure 2a. The code is organized as a
sequence of basic blocks (denoted BB0, BB1, etc) of operations such
as property reads, and all values are in Static Single Assignment
form. We show how the analysis works for turtles by stepping
through what the analysis does when the script is analyzed:

• instruction 2 is the import corresponding to line 1 of Fig-
ure 2a. This assigns the imported script to v40, which we
represent with turtle 𝑡1.

• instructions 3-5 create the inner function massage_data
from lines 3 to 8. Functions are represented as objects in our
analysis, since they can be first class.

• instruction 6 reads the property read_csv from v40, which
holds the imported pandas script, and assigns it to v47. This
is also 𝑡1.

• instruction 7 calls v47 as a function. Since v47 holds 𝑡1 and
the semantics of function calls on turtles is to create a new
turtle, we assign the new turtle 𝑡2 to v46.

The rest of the instructions are mostly analogous, except one

• instruction 9 calls v44, which is massage_data. This is not
a turtle, so the code for that function is added to the work
queue of the analysis. 𝑣46 is passed as an argument, corre-
sponding to passing the result of the read_csv.

There is one aspect of analysis not illustrated by this code snippet:
at line 10 of Figure 2a, the built in len call will be passed a turtle
returned by read_csv and ultimately massage_data. Since the
analysis makes no assumption about the meaning of a turtle, we
treat calls to primitives as simply returning any of the turtles that
are used as arguments.

5.2 Duck Typing
As described above, our analysis is neither sound nor complete.
Traditional approaches to duck typing require that for every ob-
ject 𝑂 that is returned from a method call 𝑀, you observe the set
of method calls on 𝑂 which we call 𝐹 , and 𝐹 must be defined in
a given class 𝐶 in order to infer that 𝐶 is a return type 𝑀. Be-
cause we may have imprecision in analysis, it is possible that we
have methods in 𝐹 that are incorrect. For instance, in Figure 2, the
call to head is under an if, so it might not be called. This code
would work even if small tables returned a type that did not sup-
port head. To handle this situation, we approximate duck typing
by instead computing the size of 𝐹 ∪ 𝐷 where 𝐷 is the set of all
methods defined for 𝐶. The likelihood that type inference was cor-
rect is governed by two factors: (a) the size of 𝐹 ∪ 𝐷, and (b) the
number of classes that are possible types for a given method re-
turn value. Clearly, as (a) increases, confidence in type inference
grows. However, a small number of classes in (b) in combination
with a small number of shared methods in (a) can sometimes still
imply a valid inference. An example of such a case is shown in Ta-
ble 2, where, for instance, we find that pandas.array returns pan-
das.core.arrays.base.ExtensionArray correctly, and in fact
pandas.core.arrays.sparse.array.SparseArray is a subclass
of pandas.core.arrays.base.ExtensionArray. We discuss how
to cleanse the types next.

Table 2: Example of sharing small number of methods

Method

Return

pandas.array
pandas.array
pandas.array

pandas.core.arrays.base.ExtensionArray
pandas.core.base.IndexOpsMixin
pandas.core.arrays.sparse.array.SparseArray

#

2
2
1

1import pandaspandas.read_csvReceiver 0Receiver 0pandas.read_csv.dropna234pandas.read_csv.dropReceiver 05pandas.read_csv.headReceiver 0Trovato and Tobin, et al.

s . py [ 2 : 7 ] − > [ 2 : 1 3 ]
s . py [ 2 : 0 ] − > [ 1 4 : 1 7 ]
s . py [ 2 : 0 ] − > [ 1 4 : 1 7 ]
s . py [ 2 : 0 ] − > [ 1 4 : 1 7 ]
s . py [ 9 : 7 ] − > [ 9 : 2 2 ]
s . py [ 9 : 7 ] − > [ 9 : 3 5 ]

[ 4 0 = [ p a n d a s ] ]

[ 4 4 = [ m a s s a g e _ d a t a ] ]
[ 4 4 = [ m a s s a g e _ d a t a ] ]
[ 4 4 = [ m a s s a g e _ d a t a ] ]

[ 4 0 = [ p a n d a s ] ]
[ 4 6 = [ d a t a ,

s a m p l e ] ]

1
2
3
4
5
6
7
8
9

10
11
12
13
14
15
16
17
18
19

. . .
i n i t i a l i z a t i o n c o d e
v40 = i n v o k e s t a t i c < Lpandas ,
v44 = new < L s c r i p t

. . .

import ( ) Lp and as ; >

s . py / m a s s a g e _ d a t a >@21

s c r i p t

s . py / m a s s a g e _ d a t a = v44

g l o b a l
p u t f i e l d v1 . < m a s s a g e _ d a t a > = v44
v47 = g e t f i e l d < r e a d _ c s v > v40
v46 = i n v o k e F u n c t i o n
BB2
v51 = i n v o k e F u n c t i o n

v47 , v48 : # i r i s . d a t a

s a m p l e ] ]

BB3
v56 = i n v o k e F u n c t i o n
BB4
v55 = b i n a r y o p ( g t ) v56
c o n d i t i o n a l b r a n c h ( eq ,
BB5
v60 = g e t f i e l d < head > v51
f i e l d r e f v61 : # n . v 5 8 : # 0 = v 5 3 : # 1 0
v59 = i n v o k e F u n c t i o n v60 n : 5 3
BB6

v44 , v46 s . py [ 1 0 : 7 ] −> [ 1 0 : 2 5 ]

[ 5 1 = [ d a t a ,

s a m p l e ] 4 4 = [ m a s s a g e _ d a t a ] 4 6 = [ d a t a ,

v5 , v51 s . py [ 1 1 : 3 ] − > [ 1 1 : 1 2 ]

, v53 : # 1 0
t o i i n d e x = 3 8 ) v55 , v58 : # 0

[ 5 = [ len ] 5 1 = [ d a t a ,

s a m p l e ] ]

s . py [ 1 1 : 3 ] − > [ 1 1 : 1 7 ]
s . py [ 2 : 0 ] − > [ 1 4 : 1 7 ]

[ 5 3 = [ cmp0 ] ]

s . py [ 1 2 : 1 3 ] − > [ 1 2 : 2 2 ]

[ 5 1 = [ d a t a ,

s . py [ 1 2 : 2 3 ] −> [ 1 2 : 2 4 ]

s a m p l e ] ]
[ 5 3 = [ cmp0 ] ]

s . py [ 1 2 : 1 3 ] − > [ 1 2 : 3 0 ]

[ 5 9 = [ s a m p l e ] 5 3 = [ cmp0 ] ]

Figure 5: IR of script 1 from example code

5.3 Analysis Cleansing
We often find a large number of spurious types from our initial
duck typing of code, and we filter them in a series of steps:

Table 3: Summary of number of passed and failed tests and
number of methods inferred for each module

• Since our duck typing is not entirely precise, the first step
is to filter candidates types to those that match the largest
number of methods called in the code.

• There are often many concrete types that share a common
supertype that is also present in the set of types. In this
case, we remove the subtypes, since they are covered by the
supertype.

• Sometimes most of the types in a set share a supertype 𝑆
that is not itself in the set. In this case, we remove types that
are not subtypes of 𝑆, since they are often due to analysis
imprecision.

• We use lists of functions and classes to remove items that
are in fact modules, but appear ambiguous due to the fact
imports can be of anything.

• We eliminate classes and functions that were not valid as

before, and use their aliases.

6 EVALUATION
6.1 How precise are labeled types?
6.1.1 Evaluation against dynamic types. To develop a gold stan-
dard for our evaluation, we collected a set of types by observing
their runtime types. We targeted 5 repositories from our set of 408
repositories that (a) used pytest for unit testing, (b) seemed to be
set up relatively easily without a set of additional dependencies on
databases, servers etc. For each function invoked by pytest in the
tests, we inserted a wrapper function which would log its return
type before return. We leveraged monkey patching in pytest and
pytest fixtures to insert our wrapper. Table 3 shows the number of

Module

Passed

Failed Methods

Flask
Numpy
Scikit-learn
Sympy
Pandas

408
4,881
17,900
1,553
48,625

29
5,139
1,142
214
6,453

255
262
371
785
624

tests that passed or failed in each package3. The types gathered by
monkey-patching are always sound, but not necessarily complete.
We gathered the 2284 distinct methods for which we had types.

Each method was annotated often with multiple types. We man-
ually inspected some of the cases and augmented the set of dynamic
types when we could based on documentation, and running the
code. We tried to extend this beyond the libraries in Table 3 but
for many libraries we tried, it was not possible to set tests up even
when they did support pytest.

Table 4 shows the total number of tests we ran to get the 2,284
methods we gathered from dynamic typing. The number of matches
were quite low for each type inference technique, but investigation
showed that this was because we frequently received from the run-
time, method names for functions that dropped the class name4. Of
the three methods, extraction from docstrings retrieved the types
for most functions. Analysis was next followed by PyType. Doc-
strings when combined with analysis yielded return types for 203

3We note that monkey patching caused a larger number of test failures, for various
reasons which were not easy to fix.
4We used dill to get the fully qualified name of a function in logging return types.

Large Scale Generation of Labeled Type Data for Python

Table 4: Statistics about dynamic types found

Number of
dynamic
types found

Mean number
of types

PyType
Docstrings
Static Analysis
Docstring + analysis

105
168
132
203

1.0
1.125
1.96
1.69

Table 5: Precision of docstrings and analysis based type in-
ference versus PyType

Precision Recall

F1-score

PyType
Docstrings
Static Analysis

0.067
0.661
0.489

0.067
0.529
0.549

0.067
0.587
0.517

methods, which is 9% of the methods we had dynamic information
for.

Table 5 shows the results of precision and recall for PyType, and
separately for type inference based on docstrings and type inference
based on static analysis and duck typing. PyType’s 𝐹 1 score was
very surprisingly low (.067), but this result is in consistent with the
6.1% accuracy reported by [12] when the type is a user defined type.
In contrast, the 𝐹 1 score for type inference based on docstrings
was 0.587, and 0.517 with static analysis; a significant improvement
over PyType.

6.1.2 Evaluation of class constructors. Dynamic typing is one method
to analyze the precision of our type inference. We exploit a feature
of the Python language as a type of sanity test for the precision of
static analysis based type inference. In Python, as in many dynamic
languages, a constructor is simply another method. We used this
fact to generate a gold standard of methods for which we know the
return type. We gathered all classes 92,277 classes from inspect,
and asked about whether their constructors were inferred correctly
by our technique for type inference using static analysis5. Recall
for constructors was 0.0459, indicating that only a small percentage
of classes were used in practice. Of those, static analysis based duck
typing produced the correct type for 4,236 types, and an incor-
rect value for 130 types, for a precision that was 0.97. The errors
were due to errors in gathering class definitions. As an example
QtNetwork.QLocalSocket is a class that we see in usage, and it
has a method waitForConnected called on it in code. However,
in the inspect output, no method waitForConnected was found,
and hence it was not associated with any class. We note that in
general, the inspect API from Python had several inaccuracies
which added noise to the process. Nevertheless, the test with class
constructors suggests the analysis and duck typing approach does
work.

5This technique cannot be applied to test the precision of the method which infers
types based on docstrings because init methods do not have docstrings which specify
return values correctly.

Figure 6: Percentage of predicted types using Dynamic Typ-
ing, PyType and our approach

6.1.3 Manual annotation. To evaluate the type inference for the
two techniques further, we selected a random sample of methods for
each technique, and we tried to manually evaluate if the return type
was correct. Note in this case, we cannot actually evaluate recall or
F1, but this sort of qualitative assessment is useful to understand
where the weaknesses of each method are. For analysis, we tried to
find as much information as we could from documentation on the
web or what we had gleaned from inspection to make the decision
on whether the returned type was correct or not.
Static Analysis Sample: For 25/108 methods, we could not find
enough documentation to infer the return type correctly. For the
remaining methods, we often returned multiple types. Across all
those returned types, we were correct on 71/163 (43.56%) cases
(where each case reflects a specific type inference), which is
lower than what we observed with dynamic typing, which may
just reflect sampling noise. One observation from this exercise
is that we often find classes that are conceptually very similar
but they are not related from a type perspective. As an exam-
ple, we found scipy.spatial.kdtree.KDTree as a return type
for sklearn.neighbors.BallTree. Both are conceptually related,
both are derived from BinaryTree, but of course one cannot be sub-
stituted for another. This is a weakness of the duck typing approach
in general.
Docstrings Sample: We created another random sample of 200
methods from docstrings type annotations. We could not manually
verify the return type of 67 methods which were mostly internal
setter functions inside libraries like plotly. For the rest of the meth-
ods, we predicted the return type correctly for 103/133 (77%). One
common issue with docstring-based types is its impreciseness when
the documentation is not sufficient or vague. In numpy for instance,
documentation would frequently state that the return value is an
array, but what was being returned was numpy.ndarray. In such
cases, relying on usage patterns could infer better types.

6.2 Weaknesses of static typing in PyType
The next question we evaluated was whether our methods for type
inference addressed some of the weaknesses we referred to in the
Introduction with static typing tools such as PyType. Similar to [12],
we chose to compare against PyType because of the observation
that PyType is slightly better than MyPy in type inference.

Table 6: Confusion matrix for PyType Against Dynamic
types

Table 9: Confusion matrix for PyType vs. Static Analysis- the
number for Class reflects differences (agreement in paren-
theses)

Trovato and Tobin, et al.

Dynamic

Primitive None Any BuiltIn Class

Class
Builtin
Primitive
None

0
0
3
0

0
0
0
4

270
9
17
1

0
4
0
0

0
3
0
0

Table 7: Confusion matrix for Static Analysis- the number
for Class reflects errors (correct answers is in parentheses)

Dynamic

Primitive BuiltIn Class

Class
Builtin
Primitive

0
0
2

0
0
0

124 (141)
13
3

Table 8: Confusion matrix for Docstrings - the number for
Class reflects errors (correct answers is in parentheses)

Dynamic

Primitive BuiltIn Class

Class
Builtin
Primitive

24
0
23

1
1
0

56 (134)
2
0

Figure 6 shows the distribution of types for dynamic typing,
versus PyType and our methods on this gold standard we developed.
Once again, as we discussed in Figure 1, PyType tends to produce
less user defined types, and produces a large percentage of types
that are labeled Any which is not very precise type information.
Our method is biased against void types, unless we infer those from
documentation. For the purposes of harvesting high quality labeled
data, it is less important to model ‘void’ correctly. For all other
categories, we seem to infer as many types as produced by dynamic
types.

To examine the nature of each typing method, and its errors
against the dynamic types, we computed a confusion matrix for
each method. Table 6 shows the same behavior as observed in
Figure 1 for PyType, and we note that tendency to respond with
Any in this system is true across all types, but exacerbated for user
defined types, to the point where none of the user defined classes
were ever inferred correctly. In fact, PyType frequently returned
the name of a module (e.g., sympy for user defined classes such as
sympy.core.power.Pow). The inference techniques we used had
the exact opposite bias. Table 7 shows that analysis tends to err on
the side of providing user defined types. The confusion for builtins
reflects coarseness in how we modeled flow - if some object was
retrieved from a tuple or a list and then a method was called on
the object, we falsely assumed a direct data flow. Table 8 shows the
confusion matrix for docstrings, which also shows a similar error
pattern as analysis, frequently confusing primitives and built-ins
with user defined types. Most of those errors in the docstring case
came from the fact that the docstring for numpy methods frequently

Pytype

Primitive BuiltIn Class

Any
Class
Builtin
Primitive

0
0
0
2

0
0
0
0

658
162 (47)
100
1

Table 10: Confusion matrix for PyType vs Docstrings - the
number for Class reflects differences (agreement in paren-
theses)

PyType

Primitive BuiltIn Class

Any
Class
Builtin
Primitive

617
36
133
247

183
17
124
0

1051
77 (47)
66
14

return a user defined class numpy.bool_ but state that they return
a bool type. Similarly for builtin, when tuples were returned but
the documentation stated the types being returned in the tuple, we
incorrectly stated that the return type was one of the mentioned
types.

We also examined to what extent our techniques and PyType
agree on the types returned from static analysis, as shown in Table 9.
The agreement is small (22%) even when PyType returns a type
that looks like a class, as shown in Table 9. As we discuss shortly
this agreement is much worse than agreement between our two
techniques (61%). In many cases, PyType does not return a fully
qualified name of the class, so our measure of the overlap of 47 cases
was adjusted to consider cases when the class name matched. In 36
cases, PyType returned a module as a returned type, which means
in 36/209 cases (17%) PyType is returning imprecise information
about types when it infers a class.

A similar comparison with type inference based on docstrings
is shown in Table 10. When PyType produced a class, it matched a
docstring based class in 47 cases; with an agreement of about 38%.
Docstring based inference seemed especially prone to disagreeing
with Pytype on builtins, most likely because documentation often
refers to both the data structure and the types held in it (e.g. list of
int. This is currently a weakness of docstring extraction that could
be addressed with better natural language processing techniques,
but this is for future work. Once again, PyType returned a module
instead of a class 34 times, which is 34/124 (27%).

6.3 Properties of the inferred types dataset
Table 11 shows some summary statistics of the two methods of
type inference. As shown in the table, together the two techniques
yield over 37,000 labeled types. The degree of intersection between
the two was small (410) because the focus of each is quite different.
When they did produce types for the same methods, they agreed in
249/410 cases (61%). We also show in Figure 7 the distribution of

Large Scale Generation of Labeled Type Data for Python

Table 12: Top-20 predicted types compared to TypeWriter

TypeWriter

This Work

str
bool
int

list
float
str
array
bool
plotly.basedatatypes.BasePlotlyType
int
pandas.core.frame.DataFrame
pandas.core.series.Series
tensorflow.python.ops.variables.Variable
tensorflow.python.ops.variables.VariableV1
scipy.sparse.base.spmatrix
numpy.ndarray
mne.io.fiff.raw.Raw
pandas.core.indexes.base.Index

1 None
2 Unknown
3
4
5
6 Any
Response
7
dict
8
9
Tensor
10 Optional[str]
11 Dict[str, Any]
List[str]
12
float
13
ndarray
14
15
bytes
16 CommonTestData mne.epochs.Epochs
17 HttpResponse
18 Dict
19 DataFrame
20

scipy.sparse.data._data_matrix
numpy.ma.core.MaskedArray
sympy.core.expr.Expr
mne.evoked.Evoked

list

more user defined types in the top 20 set, and our types are fully
qualified which is helpful for inference.

7 RELATED WORK
In many languages, type inference is a well understood problem.
For statically-typed languages, it is a convenience to reduce the
amount of typing at the keyboard to get typing of the program.
Recent versions of Java, for instance, do significant amounts of
type inference in the context of generics and lambda functions [5].
Historically, languages such as ML [10, 13] pioneered type inference
such that writing types was usually unnecessary, except for some
tricky polymorphic cases.

Type inference for dynamic languages has always been more
approximate. In part, that is because the dynamic nature of the
language permits flexibility such as the same variable having differ-
ent types in different places; in fact, this is one place where MyPy
can have trouble. Type analysis has been done in Python, but most
of the approaches developed so far cannot produce high quality
labeled data on a large scale. Some approaches handle restricted
versions of the language (e.g. [8]), but these approaches have lim-
ited applicability for producing large scale unsupervised data from
our large collection of public Python code; we have no control over
that code, and it uses all aspects of the language freely.

There have been some approaches for traditional inference, but
they all suffer from the extreme difficulty of doing precise analysis
in Python, and blur the distinction between type inference and
program analysis. Fritz and Hage [4] implement type inference via
abstract interpretation and experiment with tuning the precision
by modifying flow sensitivity and context sensitivity. Moving fully
into more traditional static program analysis, Dolby et al. [3] use
type inference based on the WALA program analysis framework to
find bugs in Python-based deep learning code by inferring tensor
shapes and dimensions. Beyond purely static analysis, Hassan et

Figure 7: Static Analysis and Docstrings-based predictions
per type category

Table 11: Summary of types inferred by the two methods

Total Average types per method

Docstrings
Static Analysis

22,041
15,486

1.12
1.50

predictions per category type. Compared to PyType (see Figure 1,
our approach clearly complements PyType by producing more user
defined types instead of None and Any types from PyType. This
and the accuracy results shown earlier support the claim that our
work can indeed produce better quality type annotations (turning
inconclusive types like Any and None to real types) that can further
improve existing type inference techniques.

Figure 8 shows the distribution of the top 25 modules for which
the two methods inferred types. Some of these modules had the
most classes as shown in Figure 3, but not all. This is in keeping
with the fact that the two techniques have different strengths, so
modules with a larger number of classes do not completely govern
the effectiveness of type inference.

6.3.1 Comparison with TypeWriter. We also considered evaluating
our type predictions against TypeWriter [11]; a recent neural model
that leverages code and the natural language tokens around it. How-
ever, we found it hard to run TypeWriter on our datasets since the
pre-trained model was not given and TypeWriter does not output
the method qualified name, rather it simply names a source file and
line number. Therefore, we only focused on an overall analysis of
the output of TypeWriter against our predicted type categories. On
its test set, TypeWriter outputs the following categories of Any and
Unknown (12%), None (38%), primitives (18%), built-ins (2%) and 30%
for the rest. Note that the production of Any reflects again the re-
liance of such systems on labeled data from static typing tools such
as MyPy. With the caveat that our results are on a different dataset,
our results show in Figure 7 shows that our work produces more
user-defined types (75%). Table 12 shows the top types reported by
TypeWriter compared to our technique. As one can see. we produce

Trovato and Tobin, et al.

Figure 8: Distribution of inference for top 25 modules

al. [6] implement type inference via a MaxSMT solver, maximizing
optional equality constraints while satisfying all mandatory type
constraints.

More approximate techniques have been tried as well. For in-
stance, Xu et al. [15] augment standard type rules with a probabilis-
tic approach to make use of additional information such as identifier
naming conventions. While not sound like more traditional type
inference, this can yield additional results.

For all these techniques, they are not designed for our use case.
They will not scale to the enormous collection of libraries that our
collection of code uses, and so we attempt to collect information at
the boundary of these libraries with duck typing on the application
side and gathering information from documentation for the API
side.

Beyond that, machine learning approaches are gaining popu-
larity. TypeWriter [11] trained a neural model using a corpus of
code, with labeled data derived from user annotations. The trained
model is used to predict likely types which are used by feedback
directed search and a static type checker (MyPy) to find consistent
type assignments. TypeWriter does consider comments in code, as
we do, but as inputs to the neural model. A key problem with Type-
Writer’s type inference is that the types the system infers are not
qualified. For user specified types, this is a serious problem because
many names are re-used across libraries (e.g., numpy.ndarray is
used heavily by several libraries, so returning ndarray alone is
unhelpful). Another neural system Typilus [2] represents code as a
graph and trains a Graph Neural Network to find type embeddings.
The labeled data it uses are from PyType and MyPy, which have
serious shortcomings, because they often produce the type Any for
most user defined types.

Other dynamic languages face a similar issue, and machine learn-
ing has been employed for them too. DeepType [7] and NL2Type [9]
build deep learning models for JavaScript type annotations. How-
ever, as shown in [12], these are sparsely populated in Python
repositories, and hence it would be hard to apply this line of work
directly to Python. Our work aims for producing high quality anno-
tated data in an unsupervised manner to allow for building better
deep learning models.

8 CONCLUSIONS AND FUTURE WORK
In this work, we have shown that one can leverage documentation
as well as usage information to produce reasonably high quality
labeled data for Python at a large scale. Our techniques achieve
significantly better performance than static type checkers. It also
produces high quality labeled data, enabling better probabilistic
type inference systems. A next step for future work is to leverage
these large scale type annotated data for building better neural
models for type inference.

REFERENCES
[1] Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, and Kavitha Srinivas. 2021.
A toolkit for generating code knowledge graphs. In Proceedings of the 11th on
Knowledge Capture Conference. 137–144.

[2] Miltiadis Allamanis, Earl T Barr, Soline Ducousso, and Zheng Gao. 2020. Typilus:
neural type hints. In Proceedings of the 41st acm sigplan conference on programming
language design and implementation. 91–105.

[3] Julian Dolby, Avraham Shinnar, Allison Allain, and Jenna Reinen. 2018. Ariadne:
Analysis for Machine Learning Programs. In Workshop on Machine Learning
and Programming Languages (MAPL). 1–10. http://doi.acm.org/10.1145/3211346.
3211349

[4] Levin Fritz and Jurriaan Hage. 2017. Cost versus Precision for Approximate
Typing for Python. In Workshop on Partial Evaluation and Program Manipulation
(PEPM). 89–98. https://doi.org/10.1145/3018882.3018888

Large Scale Generation of Labeled Type Data for Python

[5] James Gosling, Bill Joy, Guy L. Steele, Gilad Bracha, and Alex Buckley. 2014.
The Java Language Specification, Java SE 8 Edition (1st ed.). Addison-Wesley
Professional.

[6] Mostafa Hassan, Caterina Urban, Marco Eilers, and Peter Müller. 2018. MaxSMT-
Based Type Inference for Python 3. In Conference on Computer Aided Verification
(CAV). 12–19. https://doi.org/10.1007/978-3-319-96142-2_2

[7] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. 2018.
Deep learning type inference. In Proceedings of the 2018 26th acm joint meeting
on european software engineering conference and symposium on the foundations of
software engineering. 152–162.

[8] Eva Maia, Nelma Moreira, and Rogério Reis. 2011. A Static Type Inference for
Python. In Workshop on Dynamic Languages and Applications (DYLA). http:
//scg.unibe.ch/download/dyla/2011/dyla11_submission_3.pdf

[9] Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: inferring
JavaScript function types from natural language information. In 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE). IEEE, 304–315.

[10] Robin Milner, Mads Tofte, and David Macqueen. 1997. The Definition of Standard

ML. MIT Press, Cambridge, MA, USA.

[11] Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. 2020. Type-
writer: Neural type prediction with search-based validation. In Proceedings of
the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 209–220.

[12] Ingkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and
Julian Dolby. 2020. Python 3 Types in the Wild: A Tale of Two Type Systems.
Association for Computing Machinery, New York, NY, USA, 57–70. https://doi.
org/10.1145/3426422.3426981

[13] Martin Franz Sulzmann and Paul Hudak. 2000. A General Framework for Hind-
ley/Milner Type Systems with Constraints. Ph.D. Dissertation. USA. AAI9973781.
[14] G. van Rossum, J. Lehtosalo, and L. Langa. [n.d.]. PEP484: Type Hints. https:

//www.python.org/dev/peps/pep-0484/. [Online; accessed 8-February-2021].

[15] Zhaogui Xu, Xiangyu Zhang, Lin Chen, Kexin Pei, and Baowen Xu. 2016. Python
Probabilistic Type Inference with Natural Language Support. In Foundations of
Software Engineering (FSE). 607–618. http://doi.acm.org/10.1145/2950290.2950343

