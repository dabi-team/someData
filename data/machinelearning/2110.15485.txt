Location-routing Optimisation for Urban Logistics Using Mobile Parcel Locker Based on 
Hybrid Q-Learning Algorithm 

Yubin Liu 
Department of Civil and Environmental Engineering 
Imperial College London, London, UK, SW7 2AZ 
Email: y.liu20@imperial.ac.uk 

Qiming Ye 
Department of Civil and Environmental Engineering 
Imperial College London, London, UK, SW7 2AZ 
Email: qiming.ye18@imperial.ac.uk 

Yuxiang Feng 
Department of Civil and Environmental Engineering 
Imperial College London, London, UK, SW7 2AZ 
Email: y.feng19@imperial.ac.uk 

Jose Escribano-Macias 
Department of Civil and Environmental Engineering 
Imperial College London, London, UK, SW7 2AZ 
Email: jose.escribano-macias11@imperial.ac.uk 

Panagiotis Angeloudis 
Department of Civil and Environmental Engineering 
Imperial College London, London, UK, SW7 2AZ 
Email: p.angeloudis@imperial.ac.uk 

Word Count: 7,229 words + 250 (1 tables) = 7,479 words 

Submitted 1 August 2021 

1 

 
 
 
 
 
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

ABSTRACT 
Mobile  parcel  lockers  (MPLs)  have  been  recently  introduced  by  urban  logistics  operators  as  a 
means to reduce traffic congestion and operational cost. Their capability to relocate their position 
during the day has the potential to improve customer accessibility and convenience (if deployed 
and planned accordingly), allowing customers to collect parcels at their preferred time among one 
of the multiple locations. This paper proposes an integer programming model to solve the Location 
Routing Problem for MPLs to determine the optimal configuration and locker routes. In solving 
this model, a Hybrid Q-Learning algorithm-based Method (HQM) integrated with global and local 
search mechanisms is developed, the performance of which is examined for different problem sizes 
and  benchmarked  with  genetic  algorithms.  Furthermore,  we  introduced  two  route  adjustment 
strategies to resolve stochastic events that may cause delays. The results show that HQM achieves 
443.41%  improvement  on  average  in  solution  improvement,  compared  with  the  94.91% 
improvement of heuristic counterparts, suggesting HQM enables a more efficient search for better 
solutions. Finally, we identify critical factors that contribute to service delays and investigate their 
effects. 
Keywords: Mobile Parcel Lockers, Urban Logistics, Location Routing Problem, Reinforcement 
Q-Learning  

2 

 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

INTRODUCTION 

The growth of e-commerce stimulates the demand for timely parcel delivery and collection, 
contributing  to  intensive  logistics  operations  in  urban  areas.  However,  the  increasing  logistics 
activity  had  led  to  adverse  effects  on  urban  mobility,  such  as  limited  parking  space  and  traffic 
congestion [1]. To address these concerns, novel last-mile concepts have been proposed to improve 
convenience and efficiency, such as deploying mobile parcel lockers (MPLs). These can facilitate 
flexible delivery according to customers’ location and increase accessibility for customers while 
fewer lockers are required [2].  

The range of algorithms that have been previously used to solve the mobile parcel locker 
problem  (MPLP)  includes  exact  algorithms  [3],  classic  heuristics  [2],  and  meta-heuristics  [4]. 
However, a key disadvantage of implementing heuristic algorithms is that they are vulnerable to 
the  presence  of  local  optimums  when  large  instances  are  deployed—an  issue  that  this  study 
attempts to improve.  

The motivation for this study is twofold. First, different studies on MPLP have identified 
the lockers' location as the most critical factor that influences customers' acceptance using locker 
services [5, 6]. As a result, the effectiveness of stationary parcel lockers is limited by the spatial 
variability in customer demand. Hence, deploying mobile lockers that consider dynamic customer 
locations  has  the  potential  of  enabling  couriers  to  respond  to  demand  changes  in  a  more  agile 
manner. The second motivation is the potential of our adopted solution approach (Q-Learning) to 
improve  the  quality  of  solutions  given  its  ability  to  consider  a  wide  spectrum  of  individual 
behaviour  [7].  Compared  with  meta-heuristic  algorithms,  Q-Learning  can  better  utilise  the 
information  feedback  obtained  from  previous  actions,  and  is  capable  of  improving  previously 
determined solutions while retaining the elements that contribute to its determined effectiveness.  
Given these considerations, this study pursues the following objectives: 1) To develop task 
and route generation algorithms to define the task sequence considering customers' locations and 
time windows constraints, 2) To develop a Hybrid Q-Learning algorithm-based Method (HQM) 
resolving the disadvantage of falling into local optimum for large-sized networks faced by most 
evolutionary  algorithms,  and  3)  To  evaluate  the  performance  of  HQM  comparing  to  a  meta-
heuristic  algorithm  (Genetic  Algorithm)  using  simulated  data  and  define  the  effect  of  critical 
factors against service delay. 

This paper contributes to the existing literature in the following respects. First, the proposed 
HQM  enables  deriving  better  solutions  than  meta-heuristic  algorithms  for  solving  large-scale 
MPLP problems. Second, two route adjustment strategies are developed to dynamically resolve 
the  time  windows  conflicts  caused  by  service  delay  and  traffic  conditions.  Third,  our  analysis 
reveals that HQM is an effective framework to solve the MPLP regardless of the network type. 
This is beneficial to customers and express companies, allowing them to handle parcel deliveries' 
timeliness and accessibility problems in a coordinated and flexible fashion. Finally, this research 
enriches the application of the Q-Learning method in the field of urban logistics optimisation. 

The rest of the paper is structured as follows. It begins with a brief review of parcel locker-
related  studies.  The  problem  statement  introduces  the  MPLP  and  formulates  an  integer 
programming model. The methodology section presents our proposed solution approach for the 
MPLP instances, consisting of three parts: i) task generation procedure, ii) route generation and 
adjustment procedure, and iii) HQM method integrated with global and local search. The results 
and discussion section discusses the results of our experiments and identifies the effects of critical 
factors  against  service  delay.  Finally,  the  conclusion  provides  a  summary  of  findings  and 
highlights the potential application of our MPLP solution approaches. 

3 

 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

LITERATURE REVIEW 

The research of optimising last-mile deliveries has expanded substantially in recent years. 
For the purposes of this review, we focus on more recent papers on novel trends in delivery with 
autonomous vehicles [8], pickup stations [9], and e‐fulfilment with shared reception boxes [10].   
To the best of the authors' knowledge, literature on location-routing planning for MPLs is 
limited: only four pieces of literature focus on the last-mile deliveries using MPLs [2, 3, 4, 11]. 
Since  the  MPLP  involves  aspects  of  facility  location  selection  and  vehicle  route  planning,  we 
analyse  the  following  fields  related  to  our  research:  (i)  MPLP,  (ii)  location-routing  problems 
(LRPs), and (iii) vehicle routing problem (VRPs) with reinforcement learning (RL). 

(i) MPLP: Schwerdfeger and Boysen [3] introduced the first MPLP, the formulation which 
bears a significant degree of similarity with our proposed model. A mixed-integer programming 
model was formulated to minimise the fleet size of MPLs while covering all customers through 
MPLs relocation. While their exact solution approach shows substantial improvements compared 
to stationary locker systems, several key limitations arise. Firstly, there is a lack of consideration 
of stochastic factors (e.g., service time) in the study, which affect the dynamic adaption of the 
scheduling strategy. Moreover, the operational details of a single mobile locker cannot be tracked, 
such  as  the  service  starting/ending  time  and  the  duration  at  parking  spaces.  Those  operational 
details  have  a  significant  impact  on  improving  scheduling  policies  and  customer  satisfaction. 
Orenstein  et  al.  [11]  introduced  a  flexible  parcel  delivery  problem  that  enables  customers  to 
provide couriers with multiple delivery locations; each parcel will only be delivered to a subset of 
the  locker  network.  The  solution  approach  is  based  on  a  savings  heuristic,  the  petal  method, 
combined with tabu search with a joint objective of large neighbourhoods to optimise the delivery 
task with lower cost and shorter delivery times. The results show that the proposed method could 
be adapted to dynamic and stochastic environments. However, the model was not adapted to the 
occasions where customers may change their prefered destinations if parcels fail to be delivered. 
This limitation is worth discussing since customers may request delivery again, which requires the 
model to respond dynamically to requirements.  

Wang  et  al.  [4]  integrated  the  LRPs  with  MPLs,  and  established  a  non-linear  integer 
programming model to minimise operating costs. An embedded GA was developed to determine 
the  locations  of  depots,  the  number  of  mobile  parcel  lockers  deployed,  and  routes  of  lockers 
simultaneously. The results show that a policy that considers demand aggregation can significantly 
reduce delivery time while the scheme without demand aggregation saves the number of vehicles 
deployed. Li et al. [2] introduced a Two-Echelon MPLP, in which locker travel to couriers in the 
field  and  transfer  parcels  between  the  couriers  and  the  depot,  while  couriers  perform  the  final 
delivery without making repeated returns to the depot. A hybrid Clark and Wright heuristic was 
developed  to  solve  the  mixed-integer  programming  model.  However,  the  proposed  method 
becomes  computationally  infeasible  for  larger  problem  instances.  Both  these  studies  lack  the 
consideration of dynamic cases, where locker routes are adjusted to respond to stochastic events 
(e.g., demand changes and traffic congestions).   

This  study  will  resolve  the  limitation  mentioned  above.  Concretely,  the  dynamic  route 
adjustment for stochastic events and the computing complexity for large network sizes. The HQM 
embedded with route adjustment strategies is developed to solve these limitations.    

(ii) LRPs combine two decision tasks: (1) the location of varying types of facilities, and (2) 
the allocation scheme in which customer fulfilled by which facility and the corresponding delivery 
route. The common objectives of LRPs include minimising the total travel cost, the acquisition of 
vehicles, and the cost of locating and operating depots [12, 13].  

4 

 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

Most  literature  studying  multi-echelon  LRPs  focuses  on  two-echelon  cases  and  ignores 
temporal  aspects  such  as  time  windows  or  the  synchronisation  of  transshipments  [14,  15].  The 
heuristics proposed by Perboli et al. [14] and Gonzalez Feliu et al. [15] indicate the fact that once 
a customer is assigned to a facility at the previous echelon, the problem can be decomposed into a 
single VRP for a facility at a higher echelon and another VRP for each facility at a sub-echelon.  
Our  proposed  model  is  a  single-echelon  LRPs  model.  Although  the  echelon  of  the 
customer-to-locker network can be regarded as the second echelon of the model, we do not plan 
the route between customers and MPLs. To the best of the authors' knowledge, this aspect has not 
been treated within the area of two-echelon LRPs. Instead, we determine the locker routes from 
the central depot towards parking spaces and the routes between parking spaces.    

(iii)  VRPs  with  RL:  VRPs  are  combinatorial  optimisation  problems 

that  are 
computationally expensive. Reinforcement learning provides a compelling choice to resolve the 
computational  complexities  faced  by  classic  heuristic  algorithms  in  the  pursuit  of  an  optimal 
solution. The advantage of RL lies in its learning ability as it interacts with the environment, thus 
improving the solution more efficiently. The optimal solution can be regarded as a sequence of 
decisions in RL following the Markov Decision Process (MDP) of the problem. Hence, decision-
makers can apply RL to obtain near-optimal solutions by increasing the probability of decoding 
expected  sequences  [16].  Bello  et  al.  [17]  were  the  first  to  apply  policy  gradient  algorithms  to 
Traveling Salesman Problem (TSP). The MDP can be represented as follows: a state is denoted as 
a !-dimensional graph embedding vector, representing the current tour of the nodes at time step ". 
The action is whether to select another unselected node or not for the current state. The transition 
function #(%, ', %!) returns the next node from a set of the constructed tour until all nodes have 
been visited. The reward function is the negative tour length.  

Nazari  et  al.  [18]  proposed  a  VRP  model,  in  which  state % is  represented  as  a  vector  of 
tuples, including customer's location and demands. The action represents whether to add a node to 
the current route or not, such that vehicles will visit. The reward is denoted as the negative value 
of the total travel distance. Since then, hybrid RL were gradually used to solve VRP.  

Cappart et al. [19] combined RL and constraint programming (CP) to solve VRP with time 
windows.  Deep  Q-Networks  and  Proximal  Policy  Optimization  algorithms  were  trained  to 
formulate MDP to determine efficient branching policies for different CP search strategies. Yu et 
al. [20] developed a deep RL-based neural combinatorial optimisation strategy to transform online 
VRP into a vehicle route generation problem. They proposed a structural graph embedded pointer 
network  to  develop  vehicle  routes.  A  deep  RL  mechanism  based  on  an  unsupervised  auxiliary 
network was used to train model parameters. The result shows that the proposed strategy achieves 
a fast online route generation speed due to the offline parameter training process. Zhao et al. [21] 
proposed a novel deep reinforcement learning (DRL) model, which composes an actor, an adaptive 
critic,  and  a  routing  simulator.  The  actor  is  designed  to  generate  routing  strategies  based  on 
attention mechanism. An adaptive critic is developed to adjust the network structure accordingly 
such that improving solution. The output of the DRL model is used as the initial solution of the 
following local search method from which the final solution can be obtained. 

Since our model does not rely on historical training samples and prior knowledge, states 
are also generated randomly according to probability distributions, we propose a novel hybrid RL 
method that integrates Q-Learning with global and local search mechanisms to solve MPLP. 

5 

 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

PROBLEM STATEMENT    

A set of customer nodes ) serve as the recipients of parcels from a group of MPLs. For 
each customer * ∈ ), customer * creates a set of location ," during the planning horizon. Each 
location that customer visited within the location set (- ∈ {1, 2, ⋯ , ‖,"‖}) is defined by a time 
interval [5"#, ,"#] and a corresponding position [7"#, 8"#].   

Customer * will  pick  up  a  parcel  from  a  specific  parcel  locker  if  the  locker's  current 
stopover  is  within  the  customer's  maximum  walking  range 9".  We  assume  that  the  maximum 
accepted walking ranges 9" can vary from customer to customer and can be changed at different 
locations (e.g., a customer tends to have a higher possibility to decline large 9" after work).  

A  set  of  mobile  lockers  :  change  their  locations  to  facilitate  parcel  pickups  by  the 
customer set ). Since MPLs cannot park randomly in urban areas, we predefine a given set of 
parking spaces ; where mobile locker < can potentially be parked. For each parking space = ∈ ;, 
we predefine a position [7$, 8$], and the available parking time windows[5$, ,$]. We define >$ to 
denote the service time of parking space =. Therefore, for each parking space =, the parking time 
windows [5$, ,$] can  be  divided  into ?$	(?$ = |,$ − 5$| >$⁄ ) sub-intervals.  Each  sub-interval  has 
an  equal  length  of  time  span.  We  introduce [E$%, F$%] to  represent  the 'th	(' = 1, … , ?$) sub- 
interval  within[5$, ,$] in  parking  space =.  The  demand  within  each  sub-interval [E$%, F$%] to  be 
fulfilled is define as a single task. To avoid missed deliveries (e.g., due to traffic congestion) and 
guarantee feasible parcel handovers at selected stopovers, we define a buffer time J to denote the 
minimum overlap between [E$%, F$%] and [5"#, ,"#].  

Figure 1 illustrates an example of MPLP with four customers and one parcel locker. The 
four customers are denoted by the solid circle with green, purple, orange, and yellow, and their 
corresponding pickup-interval time are represented as a blue rectangle. Customers may change 
location throughout the day as denoted with the dotted coloured arrows. Note that the customer 
time windows at different locations shall never overlap. The proposed MPLP aims to allocate the 
least number of MPLs departed from depots, fulfil all customers at a specific time interval within 
a  day,  and  plan  locker  routes.  The  optimal  solution  here  is  deploying  one  MPL  to  satisfy  all 
customers, as shown by the solid black arrow (since the available parking time intervals satisfy 
both customer time windows and service buffer time).  

FIGURE 1 Instance of MPLP 

6 

 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

We aim to minimise cost, which consists of the number of lockers deployed and the total 
distance travelled while satisfying all customers and time windows constraints from both customer 
and parking space. The notation and the mathematical model are as follows: 

Indices: 
< 
* 
=, K 

mobile parcel locker <	(< ∈ :) 
customer n	(* ∈ )) 
two adjacent parking spaces(=, K ∈ ;) 

Sets: 
) 
: 
;\{0} 

N 

set of customer nodes 
set of MPLs 
set  of  parking  space,  where  0 
represent depot 
set of nodes within the network (N =
{0,1, … , ‖;‖} ),  where  0  represent 
depot 

Parameters: 
Q 

the capacity of MPLs 

Decision Variables: 
P$&' 

Boolean: 1, if locker < visits parking 
space K after  fulfill  the  demands  at =; 
0, otherwise 
Boolean:  1,  if  locker  <  serves  the 
customer covered by parking space =; 
0, otherwise 
Boolean: 1, if locker < starts service 
within the	'th sub-interval of parking 
space =; 0, otherwise 

Q$% 

the total demand to be fulfilled within 
the 'th sub-interval of parking space = 

R$' 

[E$%, F$%] 

the  'th  sub-interval  within  [5$, ,$]  at 
parking space = (' = 1, … ‖?$‖) 

S$'% 

"$& 

>$ 
T$'% 

U' 
V$& 

W 

time  consumption  travelling  from = to 
K 
service time of parking space = 
the  time  when  locker < arrives  at  the 
'th sub-interval of parking space = 
fixed cost of locker < 
unit cost travelling from parking space 
= to K 
a large indefinite number 

Mathematical Model 

:=*=<=%E	XYP$&', R$', S$'%Z = 	 [( × U' × ] ] P$)'

+ [- × ] ] ] P$&'

×

V$&					(1) 

'+(

$+(

'+(

$+)

&+)

,

*

,

*

*

Subject to: 

,
*
] ] P$&'
'+(
$+)

,
*
] ] P$&'
'+(
&+)

= 1						∀= ∈ ;																																																														(2) 

= 1						∀K ∈ ;																																																															(3) 

7 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

*
] P$)'
$+(

*
] P)&'
&+(

= 1						∀< ∈ :																																																															(4) 

= 1						∀< ∈ :																																																															(5) 

.!

*
] ] Q$%
$+(
%

R$'

≤ d						∀< ∈ :																																																								(6) 

E$% − 	W(1 − S$'%) ≤ 	 T$'%			∀< ∈ :; = ∈ ;; ' ∈ {1, … ‖?$‖}																										(7) 
F$% − 	W(1 − S$'%) ≥ 	 T$'%			∀< ∈ :; = ∈ ;; ' ∈ {1, … ‖?$‖}																										(8) 
T$'% + >$P$&' + "$&P$&' − 	WY1 − P$&'Z ≤ T&'%			∀< ∈ :; 	=, K ∈ ;																	(9) 

E&% − 	WY1 − S&'%Z ≤ 	 T$'% + >$P$&' + "$&P$&'			∀< ∈ :, = ∈ ;																				(10) 

F&% − 	WY1 − S&'%Z ≥ 	 T$'% + >$P$&' + "$&P$&'			∀< ∈ :, = ∈ ;																				(11) 

P$&', R$', S$'% ∈ {0, 1}								∀< ∈ :; =, K ∈ ;																																						(12) 

The objective function (1) minimises the fleet size and the total travel distance, where [( 
and [- represent the weights of two cost units. Constraints (2) and (3) ensure that parking space = 
and K can  only  be  visited  once  within  the  same  sub-interval.  Constraints  (4)  and  (5)  guarantee 
MPLs depart from a central depot and return when the task is completed. Constraint (6) represents 
that the customer fulfilled by locker < will not exceed the locker's capacity. Constraints (7) and 
(8) ensure that the locker < arrives at parking space = within the 'th sub-interval if locker serves 
the  corresponding  customers.  Constraints  (9)  to  (11)  represents  that  if  locker < visits  parking 
space K after fulfilling the customer at =, the time arriving at K needs to meet the time window of K. 
Constraint (12) defines the domain of the binary variables.   

METHODOLOGY 

The structure of our proposed solution approach is illustrated in Figure 2. First, we assign 
different  customer  locations  to  the  nearest  parking  space  using  K-means  clustering,  such  that 
MPLs can fulfil customers at one of his/her potential locations within a day. Next, we derive a 
finite  set  of  tasks  for  each  parking  space  to  compose  the  original  problem  into  defining  task 
allocation and the sequence of executing tasks by different lockers. The task set of parking space 
= is constituted by multiple sub-tasks within [5$, ,$]. Notably, the sub-task is defined as the demand 
to be fulfilled within each sub-interval [E$%, F$%] (see Problem statement). MPLs are then allocated 
to perform the task set of all parking spaces.  

By  obtaining  a  finite  set  of  tasks,  the  HQM  algorithm  is  designed  to  determine  the 
following set of decisions: i) task allocation schemes (denoted as P() that determine sub-task and 
locker pairings, and ii) the locker task execution sequence (referred as P-). A state % = (P(, P-) 
can be regarded as a feasible solution of the proposed MPLP. An agent is regarded as a set of state 
%, and is used to accelerate the search for an optimal solution.  

Randomly generated agents serve as inputs to the HQM, which generates an optimal state 
of each agent based on its current knowledge of the problem. Thus, the task allocation scheme and 
the task execution sequence are determined. The sequence of tasks performed by different MPLs 

8 

 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

determines the locker routes. The essential elements and structure of the HQM approach will be 
demonstrated in the following sections.  

FIGURE 2 Solution Framework of Model 

Task Generation 
Any planning horizon and availability interval of each parking space can be divided into a finite 
set  of  sub-intervals  once  a  service  time  >$	 of  parking  space  has  been  defined  (see  Problem 
statement).  We  first  reduce  the  redundant  length  of  parking  spaces'  availability  intervals.  For 
instance, if a parking space = is available for a mobile locker from 10:00 to 16:00, but no customer 
time windows exist between 14:00 and 16:00, we can reduce the available time intervals to only 
last  from  10:00  to  14:00  without  excluding  optimal  solutions.  The  available  time  interval  of 
parking space = can be reduced to k$: 

9 

 
 
      
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

k$ = {[5$, ,$] ∩ [5"#, ,"#]		∀* ∈ ); 	- ∈ {1, 2, ⋯ , ‖,"‖}; 	= ∈ ;}																		(13) 

To  further  reduce  the  range  of  available  time  intervals,  we  determine  the  earliest  time 
window and the latest time window among all customers within each parking space. The optimal 
/01, we obtain a set of 
range of available time intervals will be updated as k$
/01  divided  by 
equal-length  sub-intervals  (Equation  15)  of  parking  space =  when  executing k$
service time >$ 

/01. By defining k$

/01 = m[<=*{5(#, ⋯ , 5"#} , <'P{,(#, ⋯ , ,"#}] ∩ k$		∀* ∈ ); - ∈ {1, ⋯ , ‖,"‖}n			(14) 

k$

[E$%, F$%] = k$

/01/	>$			∀= ∈ ;; ' ∈ {1,2, ⋯ , ?$};	[E$%, F$%] ∈ k$

/01																(15) 

Thus, the set of demands to be fulfilled by mobile locker < at parking space = within the 
'th sub-interval  can  be  denoted  by d$'% ,  where Q$% 	denotes  the  total  demand  at  the 'th sub- 
interval at =. Each d$'% is regarded as a single task. The task list of locker < is denoted as #p' =
{d('%, ⋯ , d$'%}. 

d$'% = {(Q$%, <, =, [E$%, F$%])			∀= ∈ ;; 	< ∈ :; 	' ∈ {1,2, ⋯ , ?$}	}																(16) 

Route Generation 
Mechanism of Route Generation 
We propose a route generation algorithm to define the sequence of mobile lockers executing the 
tasks generated in Task Generation. The generated routes will be applied in the hybrid Q-Learning 
model in the later section; The evaluation of the generated route is measured by Equation (1). For 
each parking space, it consists of multiple tasks within the available parking time intervals. The 
locker  will  perform  the  corresponding  task  within  the  sub-intervals.  Considering  the  optimal 
scheduling strategy, a locker may also visit the same parking space multiple times within different 
sub-intervals to perform corresponding tasks. To capture the route generation process of mobile 
lockers, we modified a lemma developed by Schwerdfeger and Boysen [3] and prove that for any 
given task d$'%, the task starting/ending time can be adjusted to satisfy constraints (7) to (11).  

Lemma  1.  For  any d$'% (= ∈ ;, ' ∈ {1,2, ⋯ , ?$}),  there  is  an  optimal  task  starting  and  ending 
time to fulfil the corresponding customer/demand, while constraints (7) to (11) are satisfied. 

We assume that a locker arrives at its first parking space at the earliest time within [E(%, F(%] 
of d('% and leave it once the last task d$'% is fulfilled by the earliest time. Then, the locker moves 
to the next parking space and starts executing the task when it arrives (if the arriving time T$'% is 
within  the  corresponding  sub-interval  of  the  next  task).  As  such,  the  locker  leaves  the  current 
parking  space  and  moves  towards  the  next  one  earlier.  Following  this  logic,  Lemma  1  can  be 
proven. 

Proof To simplify the process, we assume that each customer only has a single location (," = 1), 
but our proof is still valid for the multiple-locations situation. Following the symbols defined in 
problem  statement  section,  we  define T$'%  as  the  time  when  locker < arrives  at  the 'th sub-

10 

 
 
 
 
 
 
 
 
 
 
  
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

interval of parking space =. T2'%
"$& is the travelling time from = to K. At the first parking space = of locker <, we can set: 

qqqqqqqqq⃗ is the leaving time, >$ is the service time of parking space =, and 

T('% = max{E(%, ")(}	∀< ∈ :; ' ∈ {1, ⋯ , ?$};	E(% ∈ [E(%, F(%]																			(17) 

We assume that the locker serves the customers as early as possible, because a task can 
start immediately at the beginning of the sub-interval. Thus, the locker can leave at the earliest at: 

qqqqqqqqqq⃗ = {max(T('%, 5"#) + >(}		∀< ∈ :; * ∈ ); ' ∈ {1, ⋯ , ?$};	5"# ∈ [5"#, ,"#]	(18) 
T('%

because the last customer is served as soon as possible. 5"# denotes the earliest time among 
all customers within the current sub-interval. The earliest task starting time of the second parking 
space is: 

T-'% = maxmT('%

qqqqqqqqqq⃗ + "(-, E-%n		∀< ∈ :; 	' ∈ {1, ⋯ , ?$}, E-% ∈ [E-%, F-%]						(19) 

Therefore, the earliest task starting and ending time for each d$'% can be represented as: 

T$'% = maxmT(24()'%

qqqqqqqqqqqqqqqqq⃗ + "($4()$, E$%n		 ∀< ∈ :; 	' ∈ {1, ⋯ , ?$}, E$% ∈ [E$%, F$%]					(20) 

qqqqqqqqq⃗ = max{max	{T$'%, 5"#} + >$}	∀< ∈ :; * ∈ ); ' ∈ {1, ⋯ , ?$};	5"# ∈ [5"#, ,"#]		(21) 
T2'%

Route adjustment strategy 
The current load of the locker may reach its maximum capacity before performing the following 
task and violating constraint (6), meaning that the locker must return to the depot to unload and 
then  execute  the  next  task.  Additionally,  the  task  ending  time  (T$'% + >$)  within  the 'th sub-
interval of parking space = plus the travelling time "$& towards parking space K may less than the 
earliest time E&% of K due to the stochastic driving time (violate constraints (7) to (11)). We propose 
two strategies to postpone the arrival time to satisfy constraints: 1) the locker moves to the depot 
and then visits the next parking space (refer as BTD); 2) the locker remains at the current parking 
space  until  the  latest  ending  time F$%  of  the 'th sub-interval  (refer  as  HCPS).  The  following 
relationships capture the situation where a locker travels from parking space = to K using the above 
adaption strategies: 

T&'% = v

T$'% + >$ + "$) + ")&				∀=, K ∈ ;; < ∈ :; ' ∈ {1,2, ⋯ , ?$}						(22)
F$% + ")&				∀=, K ∈ ;; < ∈ :; ' ∈ {1,2, ⋯ , ?$}																														(23)

Equation  (22)  represents  the  task  starting  time  adapted  by  implementing  BTD,  which 
equals the previous task ending time plus the time consumption travelling between the depot and 
two parking spaces. Equation (23) represents the situation implementing HCPS, which equals the 
latest ending time of the previous task plus the time consumption travelling to the next parking 
space. By using different strategies, the starting time of the next task satisfies E&% < T&'% < F&%. 
Code Listing 1 outlines the task and route generation process and route adjustment procedure. 

11 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

Algorithm 1: Route generation and adjustment algorithm  
Input: Number of lockers :; Task list #p' for < ∈ :; Locker speed  x 
output: The reward of the generated routes 
1 Initialise corresponding parameters and matrix 
2 Calculate travel time "$& between parking space = and K  
3 for each locker < in : do 
4       index = argsort(#p') 
5       #p' = #p'[index] 
6       total_dispatch += 1 // Record the dispatched locker 
7 end 
8 for each sub-task "- in #p' do 
9       Add depot to current task list #p' // Depart from depot 
10     Create blank list route_list  // Record visited parking spaces 
11     Initialise T$'%, Q$%,  E$%, F$%, 5$, ,$, >$     
12     for next in "- do 
13           current_parking_space = #p'["-] 
14           next_parking_space = p'[*EP"] 
15           Add current_parking_space to route_list 
16           Update T$'%,	Q$%,		E$%, F$% 
17           travel_time = lialg.norm(current_parking_space – next_parking_space) / x 
18           if T$'% + >$ + "$& < E&% and F&% + "$& ≥ E&% then // Violate constraints (7) to (11) 
19                 Adjust task starting and ending time based on Equation (23) // Apply HCPS 
20           else if T&'% < E&% or 	Q$% > d then // Violate constraints (7) to (11) or constraint (6) 
21                 Adjust task starting and ending time based on Equation (22) // Apply BTD 
22                 Update T&'%, Q&%,  E&%, F&%, 5&, ,&, >& 
23                 Add next_parking_space to route_list 
24           else if satisfy constraints then 
25                 Update T&'%, Q&%,  E&%, F&%, 5&, ,&, >& 
26                 Add next_parking_space to route_list 
27           end 
28      end 
29 end 
30 Calculate the total route length from route_list 
31 Calculate the value of objective function X(P$&', R$', S$'%) 
32 Reward	← 1/X(P$&', R$', S$'%) 
33 return Reward 

CODE LISTING 1 Route Generation and Adjustment Algorithm 

Hybrid Q-Learning Algorithm-based Model (HQM) 
Code Listing 1 obtains locker routes that can be used to execute delivery tasks, and calculates the 
corresponding reward. We further introduce a hybrid Q-Learning model that combines global and 
local search to generate a set of feasible solutions and obtains the optimal result by updating Q-
Value iteratively. 

Given  as the integration of task allocation scheme and the tasks execution sequence, our 

goal is to maximise the reward Ä obtained from a set of generated locker route Å: 

12 

 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

Ä(|Å) = max v

1
X6"YP$&', R$', S$'%Z

, ⋯ ,

1
X6#YP$&', R$', S$'%Z

Ç	" ∈ ‖TÉE*"‖		(24) 

The proposed HQM, parameterised by Ñ( (correspond to task allocation scheme P() and 
Ñ- (correspond to task executing sequence P-), needs to define a strategy !(|Å) that improve the 
solution . It is a stochastic policy and can be factorised as: 

1

!7",7$(|Å) = Ö !7",7$((")|(< "), Å)

																																					(25) 

$+(

To maximise the objective in Equation (24), we use HQM to update every component on 
the  right-hand  side  of  Equation  (25)  in  every  timestep ".  The  essential  elements  of  HQM  are 
defined as follows: 

A.  State 
State % = (P(, P-) is  determined  as  a  vector,  consisting  of  two  variables:  1)  the  task  allocation 
scheme (P() that defines which task is assigned to which locker, and 2) the task execution sequence 
of MPLs (P-). Each state is regarded as a feasible solution, and different states form an agent. An 
illustration is shown in the upper right of Figure 2. 

B.  Agent  
We consider a set of states %" = (P(, P-) as an agent (TÉE*" = {%(, ⋯ , %"}). At every timestep ", 
the agent interacts with the environment and selects an action	(") based on its policy. By selecting 
an  action ("),  the  state  within  the  agent  will  be  updated  accordingly  based  on  the  Q-Value  at 
every timestep ". The larger the reward a state obtains, the larger the Q-Value it has. The new state 
corresponding to the agent generated in the next timestep/action will be similar to the current state 
with the maximum Q-value in the current timestep/action, meaning that the new state retains the 
elements that make a significant contribution to the optimal solution in the current timestep. We 
generate multiple agents for the later global search to speed up the search for the optimal solution. 
Receiving a new reward Ä from the environment, the agents are iteratively updated with improved 
states.  

C.  Action 
The action π(t) can be defined as a decisional behaviour that agents select a policy searching for a 
better  solution  in  the  next  timestep. (") is  sampled  from  the  right-hand  side  of  Equation  (25) 
during  training  time,  and  obtained  by Ü − ÉáEEàR search  of  the  Q-Learning  and  global-local 
search strategies. 

D.  Reward 
The reciprocal  of  Equation (1) is calculated as  a  reward  when all tasks are executed,  since  the 
HQM algorithm is set to maximise the reward (minimise Equation (1)).  

Decomposition of State-Action Space 
A lookup table represents the state-action space of HQM. The size of the lookup table equals the 
Cartesian product of > × T, where > equals the length of elements (feasible solution) within the 

13 

 
 
 
 
 
 
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

agent, while T equals the number of iteration (step size). Hence, the size of the lookup table will 
expand exponentially when the problem size becomes larger, causing computing complexity.  

To  tackle  such  a  curse  of  dimensionality,  we  decomposed  the  state-action  space  into 
multiple  low-dimensional  state-action  combination  chains  linked  by  Q-value  matrix;  we  define 
two Q-Value matrices, which correspond to the variable P( and P- within a state respectively. The 
two Q-Value matrices are captured by Equations (26) and (27), where < equals the number of 
lockers deployed and * equals the number of tasks. Each matrix value reflects the evaluation of 
the variable, such that the current task allocation scheme and the task executing sequence can be 
evaluated. Additionally, each value within the matrix reflects the interdependence between two 
adjacent actions. The larger the value is, the more closely the two adjacent actions are connected. 
The new state corresponding to the agent updated in the next action will inherit the elite fragment 
of the state with the larger Q-Value in the current action. Therefore, it is possible to improve the 
solution while inheriting an elite fragment of the current state. 

!(Ñ() = â

d(((Ñ() ⋯ d('(Ñ()
⋱
d'((Ñ() ⋯ d''(Ñ()

⋮

⋮

å																																											(26) 

!(Ñ-) = â

d(((Ñ-) ⋯ d("(Ñ-)
⋱
d"((Ñ-) ⋯ d""(Ñ-)

⋮

⋮

å																																													(27) 

The iteration procedure can be regarded as an MDP that derives satisfactory results through 
updating  the  Q-value  between  two  adjacent  actions.  Figure  3  shows  the  structure  of  the > × T 
dimension  lookup  table;  For  each  action T$	(= ∈ {1, ⋯ , "}),  there  is  a  corresponding  Q-Value 
(!(Ñ() and !(Ñ-)) that interacts with it. Each action selects the direction of migration based on the 
Q-Value:  the  action  T$  is  taken  as  the  previous  iteration  of  the  next  action  T$9(  once  T$  is 
determined, and the next action will be selected according to d$9(.  

            FIGURE 3 State-Action Space 

The update of the two Q-Value matrices follows the following equation: 

(! "#%
!%&'

(!), %%

(!)& = !%

(! "#%

(!), %%

(!)& + ) *+(!) "#%

(!), #%&'

(!) , %%

(!)& + , max
*"!#∈,$

(! "#%&'
!%

(!) , %(!)& − !%

(! "#%

(!), %%

(!)&1 (28) 

14 

 
 
 
 
 
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

where P$ represents the type of variable within the state (P$ ∈ {P(, P-}), K represent the Kth 
:!&executing  action ':!& 

:!&Z represents  the  reward  obtained  from  state %1

:!&, %19(

:!& , '1

:!&  in timestep ", ç represents the learning rate, and é represents the discount factor. 

agent, Ä:!&Y%1
towards %19(

Policy for Action Selection 
The  trade-off  between  exploitation  and  exploration  of  the  information  feedback  obtained  from 
previous actions is challenging for traditional Q-Learning. Emphasising information exploration 
may obtain the optimal solution but sacrifice convergence speed. While emphasising information 
exploitation improves convergence speed at the cost of optimality. We propose to integrate the Q-
Value-based global search with a local search approach to define the policy for action selection, 
such that obtains a near-optimal solution while improves convergence speed. 

Global Search Based on Q-Value Matrix 
The transfer between two adjacent actions updates agents according to the Q-Value obtained from 
the last action. Since the new agents consist of multiple new states, each composed of variables P( 
and P-, we first apply a global search to produce elements for each state variable parameter. The 
policy of action selection is based on the Ü − ÉáEEàR search which can be defined as follows: 

:!& = è
'19(

:!& , ':!&Z,

d:! Y%19(

argmax
%-!.∈</
':!&′	,																																											í ≥ Ü

í < Ü

																																							(29) 

Where Ü denotes the greedy factor, í denotes the random value between 0 and 1, and ':!&′ 
denotes  the  action  that  is  being  selected  based  on  the  normalised  Q-Value  matrices  within  the 
global  scope.  The  greater  the  Q-Value  is,  the  greater  possibility  of  the  corresponding  element 
inheriting an elite fragment of the current state.   

Local search 
The local search operation focuses on the adjustment of individual state. The generation of the new 
state is represented as follows: 

&
%"=>

(P(, P-) = %/?@

& (P(, P-) + î × ï%/?@

:!&(P(, P-) − %"=$ABC/D

:!&

(P(, P-)ñ									(30) 

Where  %"=>

&

(P(, P-)  denotes  the  new  state,  %/?@

& (P(, P-)  denotes  the  current  state,  and 

(P(, P-) denotes the neighbourhood state. î is a random value between -1 and 1.  

:!&
%"=$ABC/D

By  implementing  a  combined  global  and  local  search  approach,  the  agent  and 
corresponding Q-Value matrices will be updated accordingly. The iteration stops once the error of 
the Q-value matrices generated by two adjacent actions is less than 1 × 104E. The state with the 
greatest reward among all agents is defined as the optimal solution of MPLP. The implementation 
procedure of HQM is shown in Code Listing 2. 

15 

 
 
 
 
 
 
 
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

Algorithm 2: Hybrid Q-Learning algorithm-based method (HQM)  
Input: Number of lockers :; All task list #p; Number of agent TÉE*"; Timestep #  
output: The optimal state % = (P(, P-) 
1 Initialise learning rate ç, discount factor é, random value í, î, greedy factor ó, Q-value 
matrices d:", d:$ 
2 for = in range(TÉE*") do // Initialise agents 
3       P( = random. randint(0, :, #p) 
4       P- = random. permutation(#p) 
5       Combine P( and P- as a state and run Algorithm 1 // Generate initial state 
6 end 
7 Rank and record states based on the reward obtained 
8 for " in range(#) do 
9       Record the best state and the corresponding reward of the agent 
10     for tk in range(#p) do 
11           Normalise Q-Value matrices d:", d:$ 
12     end 
13     for = in range(TÉE*") do // Global search 
14           for "- in range(#p) do 
15                 Implement global search based on Equation (29) and obtain new variable P(
!  as new state %! = (P(
16                 Combine P(
17           end 
18           Record the reward obtained by state %! = (P(
19           if new_reward > current_reward then 
20                 Update the best state and reward as %! = (P(
21           end  
22      end 
23      for = in range(TÉE*") do // Local search 
24           Implement global search based on Equation (30) and obtain new variable P(
!!, P-
25           Combine P(
26           Record the reward obtained by state %!! = (P(
27           if *Eú_áEú'áà! > new_reward then 
28                 Update the best state and reward as %!! = (P(
29           end 
30      end 
31      Update Q-Value matrices d:", d:$ according to Equation (28) 
32      if satisfy convergence constraints then 
33           break 
34      end 
35 end   
36 return The optimal state % = (P(, P-) 

!!) and run Algorithm 1 
!!) as *Eú_áEú'áà! 
!!, P-

!! as new state %!! = (P(

! ) and run Algorithm 1 

!!) and *Eú_áEú'áà! 

! ) as new_reward  

!! and P-

!  and P-

!! 
!!, P-

!!, P-

! , P-

! , P-

! , P-

! ) and new_reward respectively 

!  
! , P-

CODE LISTING 2 Pseudo Code of HQM 

RESULTS AND DISCUSSION 

In  this  section,  we  discuss  the  application  of  our  HQM  approach  and  compare  its 
effectiveness with a Genetic Algorithm (GA). We first evaluate the optimal configuration of MPLs 

16 

 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

based  on  two  route  adjustment  strategies  under  two  algorithms  with  different  network  sizes. 
Additionally,  we  obtain  and  analyse  the  optimisation  performance  of  HQM.  We  analyse  the 
performance of the HQM and GA based on their reward acquisition capability and convergence 
speed. Finally, we investigate the crucial factors influencing the service delay. 

Optimal Configuration of MPLs 
Testing is based on four types of whereabout settings (the total number of locations covered by 
each parking space equals 5, 10, 15, 20) with two route adjustment strategies applied. The parking 
space set for the proposed MPLP ranges from 5 to 10. The HQM and GA code are implemented 
in Python 3.8.5 on an Apple M1 (3.20 GHz) processor with 16GB RAM. The corresponding HQM 
and  GA  parameters  are  presented  in  Table  1.  To  ensure  that  the  results  are  comparable,  we 
configured the two algorithms implementation with similar parameters, to the degree possible. 

We compare the performance of HQM with GA based on four aspects: i) locker deployed; 
ii) total travel distance; iii) average service delay; iv) reward obtained. The implemented result of 
the different parameter settings based on two algorithms are presented in Table 1.  

In terms of the number of locker deployed, there is no significant difference between the 
two algorithms in achieving optimal deployment of lockers. The difference of locker deployments 
between the two algorithms fluctuates within the interval of [-1, 1].  

The average driving distance of two algorithms based on two route policies are 308.176km 
(HQM-BTD), 302.748km (HQM-HCPS), 275.740km (GA-BTD), and 272.569km (GA-HCPS), 
respectively.  Therefore,  the  GA  achieves  a  better  performance  than  HQM  when  it  causes  to 
minimise driving distances, due to the GA's better local search characteristic. With the increase of 
parking spaces deployed and customer locations, applying the HCPS strategy saves more driving 
distance than BTD since the round trips between parking spaces and depots are reduced.  

Concerning the average service delay, the results obtained from the two algorithms based 
on two route policies are 10.104min (HQM-BTD), 8.907min (HQM-HCPS), 121.029min (GA-
BTD),  and  113.765min  (GA-HCPS),  respectively.  Hence,  the  average  service  delay  from  the 
approach  HQM  is  significantly  lower  than  GA  under  the  two  route  adjustment  strategies, 
contributing  to  greater  reward  acquisition.  Additionally,  the  HCPS  strategy  reduces  delays  by 
11.847% and 6% respectively, compared to BTD implemented by HQM and GA. The average 
reward  of  two  algorithms  based  on  two  route  policies  are 1.901 × 104F  (HQM-BTD), 
1.965 × 104F(HQM-HCPS), 0.972 × 104F (GA-BTD), and 1 × 104F (GA-HCPS) respectively. 
The  result  shows  that  the  proposed  HQM  algorithm  combined  with  the  HCPS  strategy  derives 
better solutions than applying the BTD strategy in two algorithms.   

17 

 
 
 
Number of 
parking 
spaces 

Locations of 
each parking 
space 

Route 
adjustment 
policy 

5 

6 

7 

8 

5 

10 

15 

20 

BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 

Mean value (BTD) 
Mean value (HCPS) 

5 

10 

15 

20 

BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 

Mean value (BTD) 
Mean value (HCPS) 

5 

10 

15 

20 

BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 

Mean value (BTD) 
Mean value (HCPS) 

5 

10 

15 

20 

BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 

Locker 
deployed 
(unit) 
9 
9 
13 
16 
15 
16 
17 
15 
14 
14 
11 
11 
15 
16 
18 
17 
23 
20 
17 
16 
12 
12 
20 
19 
23 
25 
23 
28 
20 
21 
15 
15 
19 
19 
22 
23 
32 
33 

TABLE 1 Implement Result 

Travel 
distance 
(km) 
115.196 
103.842 
188.929 
179.039 
203.999 
184.857 
230.168 
213.614 
184.573 
170.338 
148.131 
131.085 
214.336 
234.621 
268.043 
260.924 
274.511 
283.338 
226.255 
227.492 
179.269 
168.145 
271.938 
285.177 
345.110 
337.984 
371.527 
475.139 
291.961 
316.611 
188.846 
183.766 
315.499 
302.573 
379.610 
364.222 
459.328 
454.509 

HQM 

Average 
service 
delay (min) 
0 
0 
0 
0 
0 
0.044 
5.706 
8.653 
1.427 
2.174 
0 
0 
1.680 
0.250 
1.378 
3.788 
0.991 
0.220 
1.012 
1.065 
0 
0 
11.210 
10.942 
15.170 
16.348 
17.891 
10.100 
11.068 
9.348 
0 
0 
11.590 
9.458 
24.673 
17.830 
20.959 
18.218 

18 

Reward 
(× "#!") 

Reward 
Gap 

6.242 
6.719 
3.938 
3.860 
3.584 
3.726 
1.249 
1.067 
3.753 
3.843 
4.923 
5.374 
2.407 
2.987 
2.075 
1.972 
1.986 
2.467 
2.848 
3.200 
4.179 
4.383 
0.670 
0.704 
0.454 
0.399 
0.393 
0.518 
1.424 
1.501 
3.790 
3.865 
0.662 
0.772 
0.312 
0.395 
0.252 
0.276 

-0.477 

0.078 

-0.142 

0.182 

- 
- 

-0.451 

-0.580 

0.103 

-0.481 

- 
- 

-0.204 

-0.034 

0.055 

-0.125 

- 
- 

-0.075 

-0.110 

-0.083 

-0.024 

Locker 
deployed 
(unit) 
12 
15 
16 
16 
16 
14 
16 
15 
15 
15 
11 
11 
17 
14 
17 
17 
21 
21 
17 
16 
12 
15 
19 
18 
23 
23 
25 
24 
20 
20 
16 
15 
19 
19 
24 
23 
30 
31 

Travel 
distance 
(km) 
107.922 
125.609 
168.477 
175.058 
178.342 
162.803 
199.032 
216.142 
163.443 
169.903 
135.441 
127.910 
205.476 
196.877 
227.272 
214.830 
277.209 
261.223 
211.350 
200.210 
157.566 
170.794 
236.719 
224.509 
281.598 
279.518 
346.943 
373.877 
255.707 
262.175 
163.958 
174.367 
248.248 
251.514 
361.005 
333.354 
414.339 
406.337 

GA 
Average 
service 
delay (min) 
77.583 
0 
111.319 
26.944 
49.013 
158.607 
143.638 
151.553 
95.388 
84.276 
109.518 
81.355 
78.547 
121.986 
163.194 
115.688 
102.267 
91.843 
113.382 
102.718 
191.808 
33.413 
93.363 
141.294 
168.470 
145.034 
108.664 
120.663 
140.576 
110.101 
73.506 
103.687 
172.337 
154.026 
85.242 
144.870 
135.767 
113.394 

Reward 
(× "#!") 

Reward 
Gap 

5.331 
6.111 
1.438 
1.211 
0.670 
0.750 
0.435 
0.336 
1.969 
2.102 
4.489 
4.551 
0.480 
0.655 
0.676 
0.685 
0.792 
0.603 
1.609 
1.624 
3.775 
3.833 
0.246 
0.257 
0.229 
0.161 
0.248 
0.246 
1.125 
1.124 
1.297 
0.956 
0.320 
0.257 
0.206 
0.292 
0.152 
0.135 

-0.780 

0.227 

-0.080 

0.099 

- 
- 

-0.062 

-0.175 

-0.009 

0.189 

- 
- 

-0.058 

-0.011 

0.068 

0.002 

- 
- 

0.341 

0.063 

-0.086 

0.017 

 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

Mean value (BTD) 
Mean value (HCPS) 

5 

10 

15 

20 

BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 

Mean value (BTD) 
Mean value (HCPS) 

5 

10 

15 

20 

BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 
BTD 
HCPS 

Mean value (BTD) 
Mean value (HCPS) 

22 
23 
15 
15 
22 
22 
29 
31 
32 
30 
25 
25 
18 
19 
24 
23 
31 
31 
34 
32 
27 
27 

335.821 
326.268 
202.043 
205.974 
378.835 
388.371 
432.421 
410.161 
480.981 
460.842 
373.570 
366.337 
254.706 
249.439 
420.902 
383.965 
491.675 
461.697 
580.228 
542.657 
436.878 
409.440 

14.306 
11.377 
0 
0.240 
23.700 
23.241 
6.052 
5.174 
21.338 
18.043 
12.773 
11.675 
0 
1.211 
20.396 
18.270 
18.826 
20.048 
40.929 
31.684 
20.038 
17.803 

1.254 
1.327 
3.610 
3.346 
0.323 
0.327 
0.687 
0.731 
0.247 
0.301 
1.217 
1.176 
2.901 
2.176 
0.335 
0.385 
0.281 
0.269 
0.130 
0.147 
0.912 
0.744 

- 
- 

0.264 

-0.004 

-0.044 

-0.054 

- 
- 

0.725 

-0.050 

0.012 

-0.017 

- 
- 

23 
22 
15 
16 
22 
21 
30 
28 
32 
30 
25 
24 
16 
16 
24 
23 
29 
30 
31 
32 
25 
26 

296.888 
291.393 
192.197 
194.847 
360.514 
329.978 
411.984 
387.925 
450.828 
419.316 
353.881 
333.017 
204.258 
217.141 
345.720 
335.836 
435.426 
446.390 
507.270 
515.494 
373.169 
378.715 

116.713 
128.994 
76.660 
69.625 
153.209 
243.752 
24.620 
67.168 
122.241 
114.243 
94.183 
123.697 
143.019 
71.606 
198.750 
138.435 
112.783 
147.523 
209.165 
173.650 
165.929 
132.804 

0.494 
0.410 
1.091 
0.928 
0.136 
0.157 
0.262 
0.254 
0.130 
0.218 
0.405 
0.389 
0.480 
1.011 
0.176 
0.146 
0.156 
0.164 
0.094 
0.099 
0.227 
0.355 

- 
- 

0.163 

-0.021 

0.008 

-0.088 

- 
- 

-0.531 

0.030 

-0.008 

-0.005 

- 
- 

9 

10 

Notes: 1.The comparison test was implemented based on the following parameters: a) service radius = 5km; b) quantity range of locations for each customer = [1, 4]; c) 
working hours = 9:00-18:00; d) demand range of customer = [1,4]; e) walking speed = 0.08km/min; f) customer walking range (km) = [0.1, 1]); g) range of time span of 
customer (min) = [30,90]; h) range of available parking time = [30, 70]; i) locker speed = 0.7 km/hr; j) capacity of locker = 20 units. 
2. HQM parameters: a) weights of the objective function !!: !" = 5: 1; b) learning rate & = 0.9 × +#(
continuous uniform distribution /(0,1); e) episode/iteration = 1000; f) population size = 100; g) random variable 3 and 4 follow /(0,1). 
2. GA parameters: a) elite size = 5%; b) possibility of crossover = 50%; c) mutation rate = 5%. 
3. Abbreviations: a) BTD = Back to Depot; b) HCPS = Holding at Current Parking Space. 
4. Improvement rate = (final best reward –best initial reward) / best initial reward ×100%. 
5. Reward gap = Reward (BTD) – Reward (HCPS). 
6. The mean value of the number of lockers deployed is rounded upward. 

!"##$%&	(&$#)&(*%
&*&)+	(&$#)&(*%

); c) discount factor , = 0.9; d) greedy factor -		follow the 

1 

19 

 
 
Optimisation Performance 
We further investigated concrete improvements in rewards acquired through implementing HQM 
and  GA  with  two  route  adjustment  strategies.  The  improvement  rate  is  calculated  based  on 
( !"#$%&(()*$+) − !"#$%&()*).)$+))/!"#$%&()*).)$+) .  The  broader  gap  between  the  two 
surfaces in Figure 4b indicates that the HCPS obtains a more significant improvement in reward 
acquisition  than  BTD  (Figure  4a)  when  implementing  HQM.  Specifically,  HCPS  achieves  an 
average of 453.58% improvement in final rewards than initial rewards, while BTD achieved an 
average of 433.23% improvement. The average improvement of the two strategies is 443.41%. 

Compared  with  HQM,  the  GA  performs  poorly  when  it  comes  to  the  route  adjustment 
strategies since the gap between the two surfaces became significantly narrower in Figure 4c and 
4d. In this case, HCPS achieves an average of 97.308% improvement in final rewards compared 
to  initial  rewards,  while  BTD  achieved  an  average  of  92.504%  improvement.  On  average,  the 
optimisation ability of HQM is 348.49% greater than that of GA, suggesting HQM enables a more 
efficient search for better solutions. 

(a)                                                                                   (b) 

(c)                                                                         (d) 
FIGURE 4 Optimisation Ability of HQM and GA 

Reward Acquisition and Convergence Comparison 
Figure  5  shows  the  reward  acquisition  and  convergence  of  HQM  and  GA  under  the  two  route 
adjustment policies. Algorithms are implemented under a relatively large instance setting (Number 

20 

 
 
   
 
    
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

of parking spaces = 10, Number of locations of each parking space = 20). Figure 5a indicates that 
HQM  achieves  greater  final  rewards  (BTD= 1.499 × 10!" ;  HCPS= 1.373 × 10!" )  than  GA 
(BTD=1.188 × 10!"; 1.076 × 10!") under two strategies. Furthermore, HQM steadily obtains a 
better reward through iteration, while GA tends to fall into local optimum during earlier iteration, 
resulting in a lower reward. This can be explained by Figure 5b, which capture the convergence 
performance of algorithms: the GA's value of objective function converged sharply at generation 
200 (±20), whilst HQM iterates relatively smoothly and converged at generation 250 (±50) even 
though there are fluctuations in the subsequent iterations. The reason for this phenomenon is that, 
compared  with  GA,  HQM  makes  better  utilisation  of  the  information  feedback  obtained  by 
performing the previous action, to retain the elements that make a significant contribution to the 
optimal solution in the previous state, thus providing potential for obtaining better solutions.   

(a)                                                                                 (b) 
FIGURE 5 Reward Acquisition and Convergence Comparison 

Effects of Critical Factors against Service Delay  
The study finally investigated how critical factors affect service delay and provide operational and 
managerial insight for logistics and administration professionals. Critical factors are categorised 
into four dimensions, depending on the attributes of factors: i) time windows from customer and 
parking space perspective, ii) locker properties, iii) network density, and iv) locker service radius 
and customer maximum walking range. The effects of different factors are summarised in Figure 
6. They suggest the following findings:  

Figure 6a demonstrates how customers' time windows and parking spaces' time windows 
affect service delays. To be more realistic, the customer residence time and available parking time 
are generated randomly following a normal distribution interval with a median value of :# and :$, 
namely  as  [:# − 20, :# + 20] ,  and  [:$ − 20, :$ + 20]  respectively.  The  result  shows  that  the 
service delay decreased with customer residence time increased, meaning that if customers provide 
more generous time windows, the locker can be more flexibly scheduled, thus reducing delays. 
Specifically,  the  median  threshold  ( :# )  of  customer  residence  time  is  50,  corresponding  to 
customer time windows' distribution [30,80], and the delay will be significantly reduced once :# ≥
50. Regarding the available parking time, it suffers fluctuation with :$ = 55. However, with the 
increase of :#, the effect of available parking time :$ on delay will gradually decrease. Based on 

21 

 
 
 
    
   
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

the above analysis, customer stay time :# has more significant impacts on service delay compared 
to available parking time :$, and the delay will decrease as :# and :$ increase.       

Figure 6b indicates the effect of locker attributes against delay. Generally, service delay 
will be improved as locker speed C% increased or locker capacity D decreased. The reduced locker 
capacity  means  fewer  customers  can  be  fulfilled  in  a  single  delivery  round,  which  saves  more 
travel distance, thus reducing the potential delays. However, more lockers are required to satisfy 
the remaining demands.  

Conversely, the greater capacity may cause the algorithm to assign more customers to the 
same locker, reducing the number of lockers deployed and obtaining greater rewards, but may lead 
to higher service delays. As locker speed increases, the delay tends to be improved. However, if 
the moving speed is too fast, the travel time between two adjacent parking spaces may be too short 
for the available parking time restriction. In this case, the lockers have to adopt route adjustment 
strategies  to  resolve  time  windows  conflicts,  resulting  in  higher  delays.  Based  on  the  above 
analysis, the combination of D = 25(±5)	unit and C% = 0.7KL/ℎ may achieve satisfactory delay 
improvement.    

Figure 6c shows that the parking spaces and customers' locations have conspicuous impacts 
on  service  delay,  proving  from  the  steeply  curved  surface.  The  increase  of  parking  spaces  and 
customer  locations  leads  to  more  locker  deployments  and  longer  travel  distances,  resulting  in 
higher delays. Finally, Figure 6d presents that the greater locker service radius or customer walking 
distance will cause higher delay since more customers should be covered within the service radius. 
Under  this  circumstance,  the  combination  of N% = 5.5(±0.5)	KL and N& = 0.6(±0.1)	KL may 
obtain lower delays. 

In summary, the time windows constraints and network density are still the main factors 
affecting service delay. This requires coordination between logistics professionals, customers, and 
urban planners to determine a better combination of critical factors to improve timeliness more 
economically.  Additionally,  for  carriers,  defining  a  reasonable  service  radius  and  effective 
scheduling policies based on customer preferences will increase customer satisfaction. 

(a)                                                                         (b) 

22 

 
 
 
    
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

(c)                                                                          (d) 

FIGURE 6 Effects of Critical Factors against Service Delay 

CONCLUSIONS 

This  study  provides  a  novel  approach  for  MPLP,  considering  customer  locations  and 
stochastic  events.  A  hybrid  Q-Learning  algorithm  that  integrates  both  global  and  local  search 
mechanisms  has  been  developed  to  optimise  locker  deployed  and  service  delays.  Two  route 
adjustment strategies are designed to resolve stochastic events. To assess the performance of the 
HQM, the study proposes a GA as a baseline case and tests the approach under different network 
density and route adjustment strategies. 

The results demonstrate that our proposed HQM combined with HCPS strategy achieves 
better  locker  configuration  and  service  delay  reduction  than  BTD  counterparts.  The  larger  gap 
region  of  the  optimisation  performance  surface  plot  shows  that  HQM  has  better  optimisation 
ability  than  GA,  especially  in  the  service  delay  improvement  aspect.  It  demonstrates  that  the 
proposed HQM can improve the local optimum issues faced by most evolutionary algorithms.  

This  study  provides  a  new  scope  to  solve  the  MPLP,  which  can  be  applied  to  urban 
deliveries with varied scales. Noticeably, it provides a novel and effective approach to flexibly 
schedule the autonomous delivered vehicles to urban logistics in the future. 

Further work will focus on optimising more complex situations by introducing dynamic 
cases,  where  more  flexible  route  adjustment  approaches  are  developed  in  response  to  demand 
changes and delay issues. It would also be beneficial to investigate the multi-echelon MPLP, in 
which transhipment centres or feeder vehicles are introduced to shorten the travel distance between 
lockers and the central depot.     

ACKNOWLEDGMENTS 

I would like to express my sincere gratitude to my supervisor and all co-authors, for their 
pertinent suggestions and encouragement during my study and paper creation. Specially, thanks to 
Miss. Xu Fang for her encouragement and support. 

AUTHOR CONTRIBUTIONS 
The  authors  confirm  contribution  to  the  paper  as  follows:  Conceptualisation,  algorithm  design, 
writing:  Yubin  Liu;  Data  visualisation,  data  curation:  Qiming  Ye;  Methodology  review,  code 
review: Yuxiang Feng; Editing, manuscript review: Jose Escribano-Macias; Supervision, review 

23 

 
 
    
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

& editing: Panagiotis Angeloudis. All authors reviewed the results and approved the final version 
of the manuscript. 

24 

 
 
 
REFERENCES 
[1]  Savelsbergh,  M.  and  Van  Woensel,  T.  50th  Anniversary  Invited  Article—City  Logistics: 
Challenges and Opportunities. Transportation Science, 2016. 50:579-590. 

[2] Li, J., Ensafian, H., Bell, M. and Geers, D. Deploying autonomous mobile lockers in a two-
echelon  parcel  operation. Transportation  Research  Part  C:  Emerging  Technologies,  2021. 
128:103155. 

[3] Schwerdfeger, S. and Boysen, N. Optimising the changing locations of mobile parcel lockers 
in last-mile distribution. European Journal of Operational Research, 2020. 285:1077-1094. 

[4] Wang, Y., Bi, M. and Chen, Y. A Scheduling Strategy of Mobile Parcel Lockers for the Last 
Mile Delivery Problem. Promet - Traffic&Transportation, 2020. 32:875-885. 

[5] Iwan, S., Kijewska, K. and Lemke, J. Analysis of Parcel Lockers’ Efficiency as the Last Mile 
Delivery Solution – The Results of the Research in Poland. Transportation Research Procedia, 
2016. 12:644-655. 

[6] Morganti, E., Seidel, S., Blanquart, C., Dablanc, L. and Lenz, B. The Impact of E-commerce 
on Final Deliveries: Alternative Parcel Delivery Services in France and Germany. Transportation 
Research Procedia, 2014. 4:178-190. 

[7]  Sutton  R,  Bach  F,  Barto  A.  Reinforcement  Learning.  Massachusetts:  MIT  Press  Ltd., 
Cambridge, 2018. 

[8] Pelletier S, Jabali O, Laporte G. 50th Anniversary Invited Article—Goods Distribution with 
Electric Vehicles: Review and Research Perspectives. Transportation Science, 2016. 50:3-22. 

[9]  Ulmer  M,  Streng  S.  Same-Day  delivery  with  pickup  stations  and  autonomous  vehicles. 
Computers & Operations Research, 2019. 108:1-19. 

[10]  Punakivi  M,  Tanskanen  K.  Increasing  the  cost  efficiency  of  e‐fulfilment  using  shared 
reception boxes. International Journal of Retail & Distribution Management, 2002. 30:498-507. 

[11] Orenstein I, Raviv T, Sadan E. Flexible parcel delivery to automated parcel lockers: models, 
solution methods and analysis. EURO Journal on Transportation and Logistics, 2019. 8:683-711. 

[12]  Tuzun  D,  Burke  L.  A  two-phase  tabu  search  approach  to  the  location  routing  problem. 
European Journal of Operational Research, 1999. 116:87-99. 

[13] Govindan K, Jafarian A, Khodaverdi R, Devika K. Two-echelon multiple-vehicle location–
routing  problem  with  time  windows  for  optimisation  of  sustainable  supply  chain  network  of 
perishable food. International Journal of Production Economics, 2014. 152:9-28. 

[14] Perboli G, Tadei R, Vigo D. The Two-Echelon Capacitated Vehicle Routing Problem: Models 
and Math-Based Heuristics. Transportation Science, 2011. 45:364-380. 

25 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
Liu, Ye, Feng, Escribano-Macias, and Angeloudis 

[15] Gonzalez Feliu, J., Perboli, G., Tadei, R., and Vigo, D. The two-echelon capacitated vehicle 
routing problem. Technical report, Control and Computer Engineering Department, Politecnico di 
Torino, Italy, 2008 

[16] Mazyavkina N, Sviridov S, Ivanov S, Burnaev E. Reinforcement learning for combinatorial 
optimisation: A survey. Computers & Operations Research, 2021. 134:105400. 

[17] Bello, I., Pham, H., Le, Q. V., Norouzi, M., & Bengio, S. Neural combinatorial optimisation 
with reinforcement learning, 2016. arXiv preprint arXiv:1611.09940. 

[18] Nazari, M., Oroojlooy, A., Snyder, L. V., & Takáč, M. Reinforcement learning for solving 
the vehicle routing problem, 2018. arXiv preprint arXiv:1802.04240. 

[19]  Cappart,  Q.,  Moisan,  T.,  Rousseau,  L.  M.,  Prémont-Schwarz,  I.,  &  Cire,  A.  Combining 
reinforcement learning and constraint programming for combinatorial optimization, 2020. arXiv 
preprint arXiv:2006.01610. 

[20] Yu J, Yu W, Gu J. Online Vehicle Routing with Neural Combinatorial Optimization and Deep 
Reinforcement  Learning.  IEEE  Transactions  on  Intelligent  Transportation  Systems,  2019. 
20:3806-3817. 

[21] Zhao J, Mao M, Zhao X, Zou J. A Hybrid of Deep Reinforcement Learning and Local Search 
for  the  Vehicle  Routing  Problems.  IEEE  Transactions  on  Intelligent  Transportation  Systems, 
2020. :1-11. 

26 

 
 
 
 
 
 
 
 
 
 
 
