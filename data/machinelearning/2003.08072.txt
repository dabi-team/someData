Speeding up Linear Programming using Randomized Linear Algebra

Agniva Chowdhury∗

Palma London†

Haim Avron‡

Petros Drineas§

0
2
0
2

r
a

M
8
1

]
S
D
.
s
c
[

1
v
2
7
0
8
0
.
3
0
0
2
:
v
i
X
r
a

Abstract

Linear programming (LP) is an extremely useful tool and has been successfully applied to solve
various problems in a wide range of areas, including operations research, engineering, economics,
or even more abstract mathematical areas such as combinatorics. It is also used in many machine
learning applications, such as ℓ1-regularized SVMs, basis pursuit, nonnegative matrix factorization,
etc. Interior Point Methods (IPMs) are one of the most popular methods to solve LPs both in theory
and in practice. Their underlying complexity is dominated by the cost of solving a system of linear
equations at each iteration. In this paper, we consider infeasible IPMs for the special case where the
number of variables is much larger than the number of constraints. Using tools from Randomized
Linear Algebra, we present a preconditioning technique that, when combined with the Conjugate
Gradient iterative solver, provably guarantees that infeasible IPM algorithms (suitably modiﬁed to
account for the error incurred by the approximate solver), converge to a feasible, approximately
optimal solution, without increasing their iteration complexity. Our empirical evaluations verify
our theoretical results on both real-world and synthetic data.

1

Introduction

Linear programming (LP) is one of the most useful tools available to theoreticians and practition-
ers throughout science and engineering. It has been extensively used to solve various problems in a
wide range of areas, including operations research, engineering, economics, or even in more abstract
mathematical areas such as combinatorics. Also in machine learning and numerical optimization, LP
appears in numerous settings, including ℓ1-regularized SVMs [54], basis pursuit (BP) [51], sparse in-
verse covariance matrix estimation (SICE) [52], the nonnegative matrix factorization (NMF) [42], MAP
inference [34], etc. Not surprisingly, designing and analyzing LP algorithms is a topic of paramount
importance in computer science and applied mathematics.

One of the most successful paradigms for solving LPs is the family of Interior Point Methods (IPMs),
pioneered by Karmarkar in the mid 1980s [23]. Path-following IPMs and, in particular, long-step path
following IPMs, are among the most practical approaches for solving linear programs. Consider the
standard form of the primal LP problem:

T

min c

x , subject to Ax = b , x

0 ,

≥

(1)

where A
×
The associated dual problem is

∈

∈

Rm, and c

n, b

Rm

Rn are the inputs, and x

∈

∈

Rn is the vector of the primal variables.

T

max b

y , subject to A

T

y + s = c , s

0 ,

≥

(2)

∗Department of Statistics, Purdue University, West Lafayette, IN, USA, chowdhu5@purdue.edu.
†Department of Computer Science, California Institute of Technology, Pasadena, CA, USA, plondon@caltech.edu.
‡School of Mathematical Sciences, Tel Aviv University, Tel Aviv, Israel, haimav@tauex.tau.ac.il.
§Department of Computer Science, Purdue University, West Lafayette, IN, USA, pdrineas@purdue.edu.

1

 
 
 
 
 
 
∈

∈

Rm and s

Rn are the vectors of the dual and slack variables respectively. Triplets (x, y, s)
where y
that uphold both (1) and (2) are called primal-dual solutions. Path-following IPMs typically converge
towards a primal-dual solution by operating as follows: given the current iterate (xk, yk, sk), they
compute the Newton search direction (∆x, ∆y, ∆s) and update the current iterate by following a step
towards the search direction. To compute the search direction, one standard approach [38] involves
solving the normal equations1:

AD2A

T

∆y = p.

(3)

Here, D = X1/2S−
1/2 is a diagonal matrix, X, S
entries are equal to xi and si, respectively, and p
eqn. (22)2. Given ∆y, computing ∆s and ∆x only involves matrix-vector products.

Rn
n are diagonal matrices whose i-th diagonal
Rm is a vector whose exact deﬁnition is given in

∈
∈

×

The core computational bottleneck in IPMs is the need to solve the linear system of eqn. (3)
at each iteration. This leads to two key challenges: ﬁrst, for high-dimensional matrices A, solving
the linear system is computationally prohibitive. Most implementations of IPMs use a direct solver;
see Chapter 6 of [38]. However, if AD2AT is large and dense, direct solvers are computationally
If AD2AT is sparse, specialized direct solvers have been developed, but these do not
impractical.
apply to many LP problems, especially those arising in machine learning applications, due to irregular
sparsity patterns. Second, an alternative to direct solvers is the use of iterative solvers, but the
situation is further complicated since AD2AT is typically ill-conditioned. Indeed, as IPM algorithms
approach the optimal primal-dual solution, the diagonal matrix D becomes ill-conditioned, which also
results in the matrix AD2AT becoming ill-conditioned. Additionally, using approximate solutions
for the linear system of eqn. (3) causes certain invariants, which are crucial for guaranteeing the
convergence of IPMs, to be violated; see Section 1.1 for details.

≪

In this paper, we address the aforementioned challenges, for the special case where m

n, i.e., the
number of constraints is much smaller than the number of variables; see Section 5 for a generalization.
This is a common setting in many applications of LP solvers. For example, in machine learning, ℓ1-
SVMs and basis pursuit problems often exhibit such structure when the number of available features
(n) is larger than the number of objects (m). Indeed, this setting has been of interest in recent work
on LPs [16, 3, 29]. For simplicity of exposition, we also assume that the constraint matrix A has full
rank, equal to m. First, we propose and analyze a preconditioned Conjugate Gradient (CG) iterative
solver for the normal equations of eqn. (3), using matrix sketching constructions from the Randomized
Linear Algebra (RLA) literature. We develop a preconditioner for AD2AT using matrix sketching
which allows us to prove strong convergence guarantees for the residual of CG solvers. Second, building
upon the work of [35], we propose and analyze a provably accurate long-step infeasible IPM algorithm.
The proposed IPM solves the normal equations using iterative solvers. In this paper, for brevity and
clarity, we primarily focus our description and analysis on the CG iterative solver. We note that a non-
trivial concern is that the use of iterative solvers and matrix sketching tools implies that the normal
equations at each iteration will be solved only approximately. In our proposed IPM, we develop a novel
way to correct for the error induced by the approximate solution in order to guarantee convergence.
Importantly, this correction step is relatively computationally light, unlike a similar step proposed
in [35]. Third, we empirically show that our algorithm performs well in practice. We consider solving
LPs that arise from ℓ1-regularized SVMs and test them on a variety of synthetic and real-world data
sets. Several extensions of our work are discussed in Section 5.

1Another widely used approach is to solve the augmented system [38]. This approach is less relevant for this paper.
2The superscript k in eqn. (22) simply indicates iteration count and is omitted here for notational simplicity.

2

1.1 Our contributions

Our point of departure in this work is the introduction of preconditioned, iterative solvers for solving
eqn. (3). Preconditioning is used to address the ill-conditioning of the matrix AD2AT. Iterative solvers
allow the computation of approximate solutions using only matrix-vector products while avoiding
matrix inversion, Cholesky or LU factorizations, etc. A preconditioned formulation of eqn. (3) is:

Q−

1AD2A

T

∆y = Q−

1p,

(4)

Rm

m is the preconditioning matrix; Q should be easily invertible (see [2, 20] for back-
where Q
ground). An alternative yet equivalent formulation of eqn. (4), which is more amenable to theoretical
analysis, is

∈

×

Q−

1/2AD2A

T

Q−

1/2z = Q−

1/2p,

(5)

∈

Rm is a vector such that ∆y = Q−

1/2z. Note that the matrix in the left-hand side of the
where z
above equation is always symmetric, which is not necessarily the case for eqn. (4). We do emphasize
that one can use eqn. (4) in the actual implementation of the preconditioned solver; eqn. (5) is much
more useful in theoretical analyses.

Recall that we focus on the special case where A

n, i.e., it is a short-and-fat
matrix. Our ﬁrst contribution starts with the design and analysis of a preconditioner for the Conjugate
Gradient solver that satisﬁes, with high probability,

≪

∈

×

Rm

n has m

2

2 + ζ ≤

σ2
min(Q−

1
2 AD)

≤

σ2
max(Q−

1
2 AD)

2

−

,

ζ

≤

2

(6)

∈

for some error parameter ζ
) correspond to the smallest and
) and σmax(
[0, 1]. In the above, σmin(
·
·
largest singular value of the matrix in parentheses. The above condition says that the preconditioner
eﬀectively reduces the condition number of AD to a constant. We note that the particular form of
the lower and upper bounds in eqn. (6) was chosen to simplify our derivations. RLA matrix-sketching
techniques allow us to construct preconditioners for all short-and-fat matrices that satisfy the above
inequality and can be inverted eﬃciently. Such constructions go back to the work of [1]; see Section 3
for details on the construction of Q and its inverse. Importantly, given such a preconditioner, we then
prove that the resulting CG iterative solver satisﬁes

1/2AD2A

T

Q−

1/2˜zt

Q−
k

Q−

−

1/2p

k2 ≤

ζ t

Q−
k

1/2p

k2.

(7)

Here ˜zt is the approximate solution returned by the CG iterative solver after t iterations. In words,
the above inequality states that the residual achieved after t iterations of the CG iterative solver drops
exponentially fast. To the best of our knowledge, this result is not known in the CG literature: indeed,
it is actually well-known that the residual error of CG may oscillate, even in cases where the energy
norm of the solution error decreases monotonically. However, we prove that if the preconditioner is
suﬃciently good, i.e., it satisﬁes the constraint of eqn. (6), then the residual error decreases as well.

Our second contribution is the analysis of a novel variant of a long-step infeasible IPM algorithm
proposed by [35]. Recall that such algorithms can, in general, start with an initial point that is not
necessarily feasible, but does need to satisfy some, more relaxed, constraints. Following the lines
of [53, 35], let
be the set of feasible and optimal solutions of the form (x∗, y∗, s∗) for the primal
is not empty. Then, long-step infeasible
and dual problems of eqns. (1) and (2) and assume that
IPMs can start with any initial point (x0, y0, s0) that satisﬁes (x0, s0) > 0 and (x0, s0)
(x∗, s∗), for
some feasible and optimal solution (x∗, s∗)
. In words, the starting primal and slack variables must
be strictly positive and larger (element-wise) when compared to some feasible, optimal primal-dual

∈ S

≥

S

S

3

solution. See Chapter 6 of [49] for a discussion regarding why such choices of starting points are also
relevant to computational practice.

The ﬂexibility of infeasible IPMs comes at a cost: long-step feasible IPMs converge in

(n log 1/ǫ)
(n2 log 1/ǫ) iterations to converge [53, 35]. Here ǫ
iterations, while long-step infeasible IPMs need
O
is the accuracy of the approximate LP solution returned by the IPM; see Algorithm 2 for the exact
deﬁnition. Let

O

Ax0
y0 + s0

T

A

b = r0
p,
c = r0
d,

−

−

(8)

(9)

d ∈

p ∈

Rn and r0

Rm are the primal and dual residuals, respectively, and characterize how far
where r0
the initial point is from being feasible. As long-step infeasible IPM algorithms iterate and update the
Rn+m be the primal
primal and dual solutions, the residuals are updated as well. Let rk = (rk
and dual residual at the k-th iteration:
it is well-known that the convergence analysis of infeasible
long-step IPMs critically depends on rk lying on the line segment between 0 and r0. Unfortunately,
using approximate solvers (such as the CG solver proposed above) for the normal equations violates
this invariant.Aa simple solution to ﬁx this problem by adding a perturbation vector v to the current
primal-dual solution that guarantees that the invariant is satisﬁed is proposed in [35]. Again, we use
RLA matrix sketching principles to propose an eﬃcient construction for v that provably satisﬁes the
invariant. Next, we combine the above two primitives to prove that Algorithm 2 in Section 4 satisﬁes
the following theorem.

p, rk
d)

∈

ǫ

Theorem 1. Let 0
1 be an accuracy parameter. Consider the long-step infeasible IPM Algo-
rithm 2 (Section 4) that solves eqn. (5) using the CG solver of Algorithm 1 (Section 3). Assume that
the CG iterative solver runs with accuracy parameter ζ = 1/2 and iteration count t =
(log n). Then,
with probability at least 0.9, the long-step infeasible IPM converges after

O
(n2 log 1/ǫ) iterations.

≤

≤

O

We note that the 0.9 success probability above is for simplicity of exposition and can be easily
ampliﬁed using standard techniques. Also, at each iteration of our infeasible long-step IPM algorithm,
((nnz(A) + m3) log n). See Section 4 for a detailed discussion of the overall
the running time is
running time.

O

Our empirical evaluation demonstrates that our algorithm requires an order of magnitude much
fewer inner CG iterations than a standard IPM using CG, while producing a comparably accurate
solution (see Section 6). In practice, our empirical evaluation also indicates that using a CG solver with
our sketching-based preconditioner does not increase the number of (outer) iterations of the infeasible
IPM, compared to unpreconditioned CG or a direct linear solver. In particular, there are instances
where our solver performs much better than unpreconditioned CG in terms of (outer) iteration count.

1.2 Comparison with Related Work

There is a large body of literature on solving LPs using IPMs. We only review literature that is
immediately relevant to our work. Recall that we solve the normal equations inexactly at each iteration,
and develop a way to correct for the error incurred. We also focus on IPMs that can use an suﬃciently
positive, infeasible initial point (see Section 1.1). We discuss below two papers that present related
ideas.

The use of an approximate iterative solver for eqn. (3), followed by a correction step to “ﬁx” the
approximate solution was proposed in [35] (see our discussion in Section 1.1). We propose eﬃcient,
RLA-based approaches to precondition and solve eqn. (3), as well as a novel approach to correct for
the approximation error in order to guarantee the convergence of the IPM algorithm. Speciﬁcally, [35]
propose to solve eqn. (3) using the so-called maximum weight basis preconditioner [43]. However,

4

computing such a preconditioner needs access to a maximal linearly independent set of columns of
(m2n) time in the worst-case. More importantly,
AD in each iteration, which is costly, taking
while [36] was able to provide a bound on the condition number of the preconditioned matrix, that
depends only on properties of A, and is independent of D, this bound might, in general, be very large.
In contrast, our bound is a constant and it does not depend on properties of A or its dimension. In
addition, [35] assumed a bound on the two-norm of the residual of the preconditioned system, but it is
unclear how their preconditioner guarantees such a bound. Similar concerns exist for the construction
of the correction vector v proposed by [35], which our work alleviates.

O

The line of research in the Theoretical Computer Science literature that is closest to our work is [14],
who presented an IPM that uses an approximate solver in each iteration. However, their accuracy
guarantee is in terms of the ﬁnal objective value which is diﬀerent from ours. More importantly, [14]
focuses on short-step, feasible IPMs, whereas ours is long-step and does not require a feasible starting
point. Finally, the approximate solver proposed by [14] works only for the special case of input matrices
that correspond to graph Laplacians, following the lines of [44, 45].

We also note that in the Theoretical Computer Science literature, [24, 25, 26, 27, 28, 11] proposed
and analyzed theoretically ground-breaking algorithms for LPs based on novel tools such as the so-
called inverse maintenance for accelerating the linear system solvers in IPMs. However, all these
endeavors are primarily focused on the theoretically fast but practically ineﬃcient short-step feasible
IPMs.
In contrast, our work is focused on infeasible long-step IPMs, known to work eﬃciently in
practice. Very recently, [6] proposed another fast, short-step, feasible IPM for solving tall and dense
LPs. The output of their algorithm does not satisfy the linear constraints exactly (similar to [14]) and
the ﬁnal convergence guarantee is somewhat diﬀerent from our work.

Another relevant line of research is the work of [13], which proposed solving eqn. (3) using pre-
conditioned Krylov subspace methods, including variants of generalized minimum residual (GMRES)
or CG methods. Indeed, [13] conducted extensive numerical experiments on LP problems taken from
standard benchmark libraries, but did not provide any theoretical guarantees.

From a matrix-sketching perspective, our work was partially motivated by [7], which presented
an iterative, sketching-based algorithm to solve under-constrained ridge regression problems, but did
not address how to make use of such approaches in an IPM-based framework, as we do here. Recent
papers proposed the so-called Newton sketch [40, 50] to construct an approximate Hessian matrix for
more general convex objective functions of which LP is a special case. Nevertheless, these randomized
second-order methods are signiﬁcantly faster than the conventional approach only when the data
n. It is unclear whether the approach of [40, 50] is faster than
matrix is over-constrained, i.e. m
IPMs when the optimization problem to be solved is linear. A probabilistic algorithm to solve LP
approximately in a random projection-based reduced feature-space was proposed in [46]. A possible
drawback of this paper is that the approximate solution is infeasible with respect to the original region.
Finally, we refer the interested reader to the surveys [48, 18, 22, 31, 17] for more background on

≫

Randomized Linear Algebra.

2 Notation and Background

A
k

A, B, . . . denote matrices and a, b, . . . denote vectors. For vector a,
k2 denotes its Euclidean norm;
k2 denotes its spectral norm and
for a matrix A,
kF denotes its Frobenius norm. We use 0
to denote a null vector or null matrix, dependent upon context, and 1 to denote the all-ones vector.
Rm
n of rank m a thin Singular Value Decomposition (SVD) is a
For any matrix X
×
product UΣVT , with U
m( the matrix of
m a diagonal matrix whose entries are equal to the
the top-m right singular vectors), and Σ
) to denote the i-th singular value of the matrix in parentheses.
singular values of X. We use σi(
·

m (the matrix of the left singular vectors), V

n with m
Rm

A
k

a
k

Rm

Rn

≤

∈

∈

∈

∈

×

×

×

5

For any two symmetric positive semideﬁnite (resp. positive deﬁnite) matrices A1 and A2 of
A1 is positive semideﬁnite (resp.

appropriate dimensions, A1 4 A2 (A1 ≺
positive deﬁnite).

A2) denotes that A2 −

We now brieﬂy discuss a result on matrix sketching [12, 10] that is particularly useful in our
n, there exists a

In our parlance, [12] proved that, for any matrix Z
Rn

theoretical analyses.
sketching matrix W

w such that

Rm

∈

×

×

∈

T

ZWW

T

Z

T

ZZ

−

ζ
4

2
2 + k

2
Z
F
k
r

k

Z
k

(cid:17)

2 ≤

(cid:13)
(cid:16)
(cid:13)
1. Here ζ
(cid:13)

(cid:13)
(cid:13)
holds with probability at least 1
(cid:13)
Ignoring constant terms, w =
product ZW can be computed in time

δ for any r

−
(r log(r/δ)); W has
O

≥

(log(r/δ)

∈
O
nnz(Z)).

O

·

[0, 1] is a (constant) accuracy parameter.
(log(r/δ)) non-zero entries per row; and the

(10)

3 Conjugate Gradient Solver

In this section, we discuss the computation of the preconditioner Q (and its inverse), followed by a
discussion on how such a preconditioner can be used to satisfy eqns. (6) and (7).

Rn

×

w, iteration count t;

m be the matrix of its left singular vectors and let

×

Algorithm 1 Solving eqn. (5) via CG
Rm

Rm, sketching matrix W

n, p
Input: AD
1. Compute ADW and its SVD: let UQ
Σ

×
m be the matrix of its singular values;
Q UT
1/2
Q;

1/2 = UQΣ−

1/2
Q ∈

Rm

Rm

∈

∈

∈

×

∈

2. Compute Q−
3. Initialize ˜z0

←

0m and run standard CG on the preconditioned system of eqn. (5) for t iterations;

Output: return ˜zt;

Algorithm 1 takes as input the sketching matrix W
Section 2. Our preconditioner Q is equal to

∈

Rn

×

w, which we construct as discussed in

Q = ADWW

T

T

DA

.

(11)

Notice that we only need to compute Q−
ﬁrst compute the sketched matrix ADW
let UQ be the matrix of its left singular vectors and let Σ
that the left (and right) singular vectors of Q−
Σ−

1/2 in order to use it to solve eqn. (5). Towards that end, we
w. Then, we compute the SVD of the matrix ADW:
1/2
Q be the matrix of its singular values. Notice
1/2 are equal to UQ and its singular values are equal to

Q . Therefore, Q−

1/2
Let AD = UΣVT be the thin SVD representation of AD. We apply the results of [12] (see

1/2 = UQΣ−

Q UT
Q.

Rm

1/2

∈

×

Section 2) to the matrix Z = VT

n with r = m to get that, with probability at least 1

Rm

×

∈

T

V

T

WW

V

Im

−

2 ≤

ζ
4

2
2 + k

V
k

k

2
V
F
k
m

ζ
2

.

≤

(cid:13)
(cid:13)
In the above we used
V
(cid:13)
k
ADW is equal to (ignoring constant factors)
·
The cost of computing the SVD of ADW (and therefore Q−
Q−

(cid:13)
(cid:13)
k2 = 1 and
(cid:13)

1/2 can be done in time

V
k

O

k

(cid:16)

2
F = m. The running time needed to compute the sketch
log(m/δ)). Note that nnz(AD) = nnz(A).
(m3 log(m/δ)). Overall, computing
1/2) is

(nnz(A)

(cid:17)

O

δ,

−

(12)

(nnz(A)

·

O

log(m/δ) + m3 log(m/δ)).

(13)

6

Given these results, we now discuss how to satisfy eqns. (6) and (7) using the sketching matrix W.
We start with the following bound, which is relatively straight-forward given prior RLA work.

Lemma 2. If the sketching matrix W satisﬁes eqn. (12), then, for all i = 1 . . . m,

Proof. Consider the condition of eqn. (12):

(1 + ζ/2)−

1

σ2
i (Q−

1/2AD)

(1

−

≤

ζ/2)−

1.

≤

T

WW

T

V
k

ζ
2

⇔ −

AD2A

V

ζ
Imk2 ≤
2 ⇔ −
T
T 4 ADWW

DA

−

T

T

Im 4 V

ζ
2
AD2A

T 4

T

WW

V

−
T
AD2A

Im 4

ζ
2

Im

−

1
⇔ (cid:18)

−

ζ
2

(cid:19)

AD2A

T
T 4 ADWW
Q

T

DA

4

1 +

(cid:18)

T
AD2A

.

ζ
2
ζ
2

(cid:19)

(14)

(15)

(16)

|

{z

}

We obtain eqn. (15) by pre- and post-multiplying the previous inequality by UΣ and ΣUT respectively
and using the facts that AD = UΣVT and AD2AT = UΣ2UT. Also, from eqn. (14), note that all
the eigenvalues of VTWWTV lie between (1
2 ) and thus rank(VTW) = m. Therefore,
rank(ADW) = rank(UΣVTW) = m, as UΣ is non-singular and we know that the rank of a matrix
remains unaltered by pre- or post-multiplying it by a non-singular matrix. So, we have rank(Q) = m;
1/2 = Im .
in words Q has full rank. Therefore, all the diagonal entries of ΣQ are positive and Q−
1/2, we get

Using the above arguments, pre- and post- multiplying eqn. (16) by Q−

2 ) and (1 + ζ

1/2QQ−

−

ζ

Q−

1/2AD2A

T

Q−

1/2 4 Im 4

1 +

1

−

(cid:18)

1 +

⇒ (cid:18)

ζ
2
ζ
2

(cid:19)

−

1

(cid:19)

Im 4 Q−

1/2AD2A

T

(cid:18)
1/2 4

Q−

1
(cid:18)
1/2AD2ATQ−

ζ
2

−

(cid:19)
ζ
2

Q−

1/2AD2A

T

1/2

Q−

1

−

Im .

(cid:19)

1/2 are bounded between

Eqn. (17) implies that all the eigenvalues of Q−

1

−

ζ
2

, which concludes the proof of the lemma.

1

−

(17)

1

−

and

1 + ζ
2
(cid:16)

(cid:17)

(cid:17)

(cid:16)
The above lemma directly implies eqn. (6). We now proceed to show that the above construction for
1/2, when combined with the conjugate gradient solver to solve eqn. (5), indeed satisﬁes eqn. (7)3.
Q−
We do note that in prior work most of the convergence guarantees for CG focus on the error of the
approximate solution. However, in our work, we are interested in the convergence of the residuals
and it is known that even if the energy norm of the error of the approximate solution decreases
monotonically, the norms of the CG residuals may oscillate. Interestingly, we can combine a result on
the residuals of CG from [5] with Lemma 2 to prove that in our setting the norms of the CG residuals
also decrease monotonically.

Let ˜f (j) be the residual at the j-th iteration of the CG algorithm:

˜f (j) = Q−

1/2AD2A

T

Q−

1/2˜zj

Q−

1/2p.

−
1/2p. In our parlance, Theorem 8 of [5]

Q−

Recall from Algorithm 1 that ˜z0 = 0 and thus ˜f (0) =
proved the following bound.

−

3See Chapter 9 of [30] for a detailed overview of CG.

7

Lemma 3 (Theorem 8 of [5]). Let ˜f (j
j

1 and j. Then,

−

1) and ˜f (j) be the residuals obtained by the CG solver at steps

−

˜f (j)
k

k2 ≤

κ2(Q−

1/2AD)
2

1

−

1)

−

˜f (j
k

k2 ,

where κ(Q−

1/2AD) is the condition number of Q−

1/2AD.

From Lemma 2, we get

κ2(Q−

1/2AD) =

σ2
max(Q−
σ2
min(Q−

1/2AD)
1/2AD) ≤

1 + ζ/2
ζ/2
1

−

.

Combining eqn. (18) with Lemma 3,

˜f (j)
k

k2 ≤

1+ζ/2
1

−

ζ/2 −
2

1

1)

−

˜f (j
k

k2 =

ζ

−

2

˜f (j

ζ k

1)

−

k2 ≤

ζ

˜f (j
k

1)

−

k2 ,

(18)

(19)

where the last inequality follows from ζ

˜f (t)
k

k2 ≤

ζ

˜f (t
−
k

which proves the condition of eqn. (7).

≤
1)

1. Applying eqn. (19) recursively, we get

k2 ≤ · · · ≤

ζ t

˜f (0)
k

k2 = ζ t

Q−
k

1/2p

k2 ,

We remark that one can consider using MINRES [39] instead of CG. Our results hinges on bounding
the two-norm of the residual. MINRES ﬁnds, at each iteration, the optimal vector with respect the
two-norm of the residual inside the same Krylov subspace of CG for the corresponding iteration. Thus,
the bound we prove for CG applies to MINRES as well.

4 The Infeasible IPM algorithm

In order to avoid spurious solutions, primal-dual path-following IPMs bias the search direction towards
the central path and restrict the iterates to a neighborhood of the central path. This search is controlled
[0, 1]. At each iteration, given the current solution (xk, yk, sk), a
by the centering parameter σ
standard infeasible IPM obtains the search direction (∆xk, ∆yk, ∆sk) by solving the following system
of linear equations:

∈

AD2A

T

∆yk = pk ,
rk
∆sk =
A
d −
xk + σµkS−
∆xk =

−

T

∆yk ,

−

11n −

D2∆sk.

(20a)

(20b)

(20c)

Here D and S are computed given the current iterate (xk and sk); we skip the indices on D and S
for notational simplicity. After solving the above system, the infeasible IPM Algorithm 2 proceeds by
computing a step-size ¯α to return:

(xk+1, yk+1, sk+1) = (xk, yk, sk) + ¯α(∆xk, ∆yk, ∆sk).

(21)

p = Axk
Recall that rk = (rk
residuals). We also use the duality measure µk = xk

d) is a vector with rk

p, rk

−
T

d = ATyk + sk

b and rk
sk/n and the vector

c (the primal and dual

−

pk =

rk
p −

−

σµkAS−

11n + Axk

AD2rk
d.

−

(22)

8

Given ∆yk from eqn. (20a), ∆sk and ∆xk are easy to compute from eqns. (20b) and (20c), as they only
involve matrix-vector products. However, since we use Algorithm 1 to solve eqn. (20a) approximately
using the sketching-based preconditioned CG solver, the primal and dual residuals do not lie on the
line segment between 0 and r0. This invalidates known proofs of convergence for infeasible IPMs.

For notational simplicity, we now drop the dependency of vectors and scalars on the iteration
1/2˜zt be the approximate solution to eqn. (20a). In order to account for the

counter k. Let ˆ∆y = Q−
loss of accuracy due to the approximate solver, we compute ˆ∆x as follows:

ˆ∆x =

x + σµS−

−

11n −

D2 ˆ∆s

−

S−

1v.

(23)

Here v
iteration of the infeasible IPM:

∈

Rn is a perturbation vector that needs to exactly satisfy the following invariant at each

We note that the computation of ˆ∆s is still done using, essentially, eqn. (20b), namely

AS−

1v = AD2A

T ˆ∆y

p .

−

(24)

∆ˆsk =

rk
d −
In [35] it is argued that if v satisﬁes eqn. (24), the primal and dual residuals lie in the correct line
segment.
Construction of v. There are many choices for v satisfying eqn. (24). To prove convergence, it is
desirable for v to have a small norm, hence a general choice is

(25)

A

−

.

k
T ˆ∆y

v = (AS−

1)†(AD2A

T ˆ∆y

p),

−

(m2n).
which involves the computation of the pseudoinverse (AS−
Instead, we propose to construct v using the sketching matrix W of Section 2. More precisely, we
construct the perturbation vector

1)†, which is expensive, taking time

O

−
The following lemma proves that the proposed v satisﬁes eqn. (24).

v = (XS)1/2W(ADW)†(AD2A

T ˆ∆y

p).

(26)

Rn

×

w be the sketching matrix of Section 2 and v be the perturbation vector of

Lemma 4. Let W
∈
eqn. (26). Then, with probability at least 1
Proof. Let AD = UΣVT be the thin SVD representation of AD. We use the exact same W as
discussed in Section 3. Therefore, eqn. (12) holds with probability 1
δ and it directly follows
from the proof of Lemma 2 that rank(ADW) = m. Recall that ADW has full row-rank and thus
ADW (ADW)† = Im. Therefore, taking v = (XS)1/2W(ADW)†(AD2AT ˆ∆y

δ, rank(ADW) = m and v satisﬁes eqn. (24).

p), we get

−

−

−

AS−

1 v = AS−

1(XS)1/2W(ADW)†(AD2A

T ˆ∆y

p)

−

= ADW(ADW)†(AD2A
T ˆ∆y
= AD2A

p ,

−

T ˆ∆y

where the second equality follows from D = X1/2S−

1/2.

p)

−

w to form the preconditioner
We emphasize here that we use the same exact sketching matrix W
used in the CG algorithm of Section 3 as well as the vector v in eqn.(26). This allows us to sketch AD
only once, thus saving time in practice. Next, we present a bound for the two-norm of the perturbation
vector v of eqn. (26).

∈

×

Rn

9

k2:
v
k

(28)

Lemma 5. With probability at least 1

−

δ, our perturbation vector v in Lemma 4 satisﬁes

with ˜f (t) = Q−

1/2AD2ATQ−

1/2˜zt

Q−

−

v
k2 ≤
k
1/2p.

3nµ

˜f (t)
k

k2,

p

(27)

Proof. Recall that Q = ADW(ADW)T = UQΣQUT
matrices of the left singular vectors and the singular values of ADW. Now, let
vector of ADW. Therefore, ADW = UQΣ
from Lemma 2, we know that Q has full rank. Therefore, Q1/2Q−

1/2
Q are (respectively) the
V be the right singular
VT is the thin SVD representation of ADW. Also,

b
1/2 = Im. Next, we bound

Q. Also, UQ and Σ

1/2
Q

b

(cid:13)
(cid:13)
(cid:13)

k2 =
v
k
=

(XS)1/2W(ADW)†(AD2A
k
(XS)1/2W(ADW)†Q1/2Q−
k
(XS)1/2W(ADW)†Q1/2

−

T ˆ∆y
p)
1/2(AD2A
˜f (t)
k2.

k2 k

k2
T ˆ∆y

p)

k2

−

≤ k
1/2(AD2AT ˆ∆y
Q UT
Q UQΣ1/2

In the above we used Q−
Q UT
(ADW)†Q1/2 =
V
matrix and
k

−
Q =
k2 = 1. Therefore, combining with eqn. (28) yields

VΣ−

1/2

b

p) = ˜f (t). Using the SVD of ADW and Q, we get
VUT
m is an orthogonal

Q. Now, note that UQ

Rm

×

∈

b

v
k2 ≤ k
k

(XS)1/2W
(XS)1/2W

VU

k2k
b

≤k

b
T
Qk2k
˜f (t)
k2.

˜f (t)

k2 =

(XS)1/2W
k

V

˜f (t)

k2k

k2

b

(29)

The ﬁrst equality follows from the unitary invariance property of the spectral norm and the second
inequality follows from the sub-multiplicativity of the spectral norm and
k2 = 1. Our construction
for W implies that eqn. (10) holds for any matrix Z and, in particular, for Z = (XS)1/2. Eqn. (10)
implies that

V
k

b

T
(XS)1/2WW
(cid:13)
(cid:13)
(cid:13)

holds with probability at least 1
we get

−

(XS)1/2

−

(XS)

ζ
4  k

(XS)1/2

2
2 + k
k

(XS)1/2
m

2
F
k

!

2 ≤

(30)

δ. Applying Weyl’s inequality on the left hand side of the eqn. (30),

2

(XS)1/2W
(XS)1/2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(XS)1/2
k2 ≤ k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(XS)1/2
k

kF ≤

2 −

Using ζ

1 and

≤

2

ζ
4  k

(XS)1/2

≤

2
(cid:13)
(cid:13)
(cid:13)
xTs = nµ, we get4

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2
2 + k
k

(XS)1/2
m

Finally, combining eqns. (29) and (32), we conclude

2

2 ≤

(XS)1/2
3
k

2
F = 3nµ.
k

(XS)1/2W
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
F
k

.

!

(31)

(32)

v
k2 ≤
k

3nµ

˜f (t)
k

k2.

p

4The constant 3 in eqn. (32) could be slightly improved to 3/2; we chose to keep the suboptimal constant to make our
results compatible with the work in [35] and avoid reproving theorems from [35]. The better constant does not result in
any signiﬁcant improvements in the number of iterations of the Algorithm 2.

10

The above result is particularly useful in proving the convergence of Algorithm 2. More precisely,
combining a result from [35] with our preconditioner Q−
(n)√µ.
This bound allows us to prove that if we run Algorithm 1 for

1/2p
(log n) iterations, then

1/2, we can prove that

k2 ≤ O

Q−
k

˜f (t)
k

k2 ≤

γσ
4√n

√µ and

v
k2 ≤
k

O

γσ
4

µ.

The last two inequalities are critical in the convergence analysis of Algorithm 2; see Appendix C for
details.

We are now ready to present the infeasible IPM algorithm. We will need the following deﬁnition

for the neighborhood

N

(γ) =

k
(x

k
, y

k

k
) : x
i s

k
i

, s

n

(1

−

≥

rk
r0

γ)µ and k
k

2
k
2 ≤
k

µk
µ0

,

.

o

Here γ

∈

(0, 1) and we note that the duality measure µk steadily reduces at each iteration.

Algorithm 2 Infeasible IPM

Rm

Input: A

∈
Initialize: k
←
while µk > ǫ do

Rm, c

n, b
×
0; initial point (x0, y0, s0);

Rn, γ

∈

∈

∈

(0, 1), tolerance ǫ > 0, centering parameter σ

(0, 4/5);

∈

(a) Compute sketching matrix W
(b) Compute rk
(c) Solve the linear system of eqn. (5) for z using Algorithm 1 with W from step (a) and

w (Section 2) with ζ = 1/2 and δ = O(n−
c; and pk from eqn. (22);

Rn
d = ATyk + sk

p = Axk

b; rk

2);

−

−

∈

×

t =

(log n). Compute ˆ∆y = Q−

1/2z;

O
(d) Compute v using eqn. (26) with W from step (a); ˆ∆s using eqn. (20b); ˆ∆x using eqn. (23);

(e) Compute ˜α = argmax

α
{
(f) Compute ¯α = argmin
α
{
(g) Compute (xk+1, yk+1, sk+1) = (xk, yk, sk) + ¯α( ˆ∆x

[0, 1] : (xk, yk, sk) + α( ˆ∆x
∈
[0, ˜α] : (xk + α ˆ∆x

, ˆ∆y
k
)T(sk + α ˆ∆s
k
k
, ˆ∆y

∈

k

)
.
}
k
, ˆ∆s

k

k

, ˆ∆s

k

)

.
(γ)
}

∈ N

); set k

k + 1;

←

end while

Running time. We start by discussing the running time to compute v. As discussed in Section 3,
(log(m/δ))
(ADW)† can be computed in
n).
non-zero entries per row, pre-multiplying by W takes
log(m/δ) + m3 log(m/δ)) time,
Since X and S are diagonal matrices, computing v takes
which is asymptotically the same as computing Q−

log(m/δ) + m3 log(m/δ)) time. Now, as W has

(nnz(A) log(m/δ)) time (assuming nnz(A)

(nnz(A)
O
·
1/2 (see eqn. (13)).

(nnz(A)

O

O

O

≥

·

1/2 and the vector v can be computed in

We now discuss the overall running time of Algorithm 2. At each iteration, with failure probability
log(m/δ) + m3 log(m/δ))
O
(log n) iterations of Algorithm 1, all the matrix-vector products in the
log n) time. Therefore, the computational time for steps
(log n + log(m/δ)) + m3 log(m/δ)). Finally, taking a union bound over
·
2) (ignoring constant factors), Algorithm 2 converges with probability at

δ, the preconditioner Q−
time. In addition, for t =
CG solver can be computed in
(a)-(d) is given by
all iterations with δ =
least 0.9. The running time at each iteration is given by

O
(nnz(A)
(n−

((nnz(A) + m3) log n).

(nnz(A)

(nnz(A)

O

O

O

·

·

O

5 Extensions

We brieﬂy discuss extensions of our work. First, there is nothing special about using a CG solver for
solving eqn. (5). We analyze two more solvers that could replace the proposed CG solver without any

11

loss in accuracy or any increase in the number of iterations for the long-step infeasible IPM Algorithm 2
of Section 4. In Appendix A, we analyze the performance of the preconditioned Richardson Iteration
and in Appendix B, we analyze the performance of the preconditioned Steepest Descent. In both cases,
if the respective preconditioned solver (with the preconditioner of Section 3) runs for t =
(log n) steps,
Theorem 1 still holds, with small diﬀerences in the constant terms. While preconditioned Richardson
iteration and preconditioned Steepest Descent are interesting from a theoretical perspective, they
are not particularly practical.
In future work, we will also consider the preconditioned Chebyshev
semi-iterative method, which oﬀers practical advantages compared to PCG in parallel settings.

O

Second, recall that our approach focused on full rank input matrices A

n. Our
≪
overall approach still works if A in any m
.
m, n
min
{
}
In that case, using the thin SVD of A, we can rewrite the linear constraints as follows UAΣAVT
Ax = b,
k are the matrices of left and right singular vecors of A respectively;
where UA
∈
Rk
k is the diagonal matrix with the k non-zero singular values of A as its diagonal elements.
ΣA
The LP of eqn. (1) can be restated as

n matrix that is low-rank, e.g., rank(A) = k

k and VA

Rm

Rn

≪

×

∈

∈

∈

×

×

×

×

Rm

n with m

T

min c

x , subject to V

T
Ax =

b , x

0 ,

≥

(33)

1

A UT

b = Σ−

Ab. Note that, rank(VA) = k

where
n and therefore eqn. (33) can be solved using
our framework. The matrices UA, VA, and ΣA can be approximately recovered using the fast SVD
algorithms of [22, 4, 9]. However, the accuracy of the ﬁnal solution will depend on the accuracy of the
approximate SVD and we defer this analysis to future work.

≪

e

e

Rn

(m

w, where every entry is a

Third, even though we chose to use the Count-Min sketch and its analysis from [12] (Section 2),
there are many other alternative sketching matrix constructions that would lead to similar results. A
(0, 1)
particularly simple one is the Gaussian sketching matrix WG ∈
×
random variable. Setting w =
(m+log(1/δ)/ζ 2) would result in the same accuracy guarantees as the
O
sketching matrix of Section 2. However, the (theoretical) running time needed to compute ADW
nnz(A)). In practice, at least for relatively small matrices, using Gaussian sketching
increases to
matrices is a reasonable alternative; see the discussion in [33] which argued that the Gaussian matrix
sketching-based solvers are considerably better than direct solvers. We also opted to use Gaussian
matrices in our empirical evaluation, since we primarily interested in measuring the accuracy of the
ﬁnal solution as a function of the number of iterations of the solver and the IPM algorithm. Other
known constructions of sketching matrices that are also applicable in our setting include (any) sub-
gaussian sketching matrix; the Subsampled Randomized Hadamard transform (SRHT); and any of
the Sparse Subspace Embeddings of [8, 37, 32, 10].

N

O

·

6 Experiments

Here we demonstrate the empirical performance of our algorithm on a variety of real-world data
sets from the UCI ML Repository [19], such as ARCENE, DEXTER [21], DrivFace [15], and a gene
expression cancer RNA-Sequencing dataset that is part of the PANCAN dataset [47]. See Table 1 for
a description of the datasets. The experiments were implemented in Python and we observed that the
results for both synthetic data (generated as described in Appendix D.2) and real-world data were
qualitatively similar. Thus, we highlight results on several representative real datasets.

n, where n is the number of features.

As an application, we consider ℓ1-regularized SVMs. All of the data sets are concerned with binary
In Appendix D.1, we describe the
classiﬁcation with m
ℓ1-SVM problem and how it can be formulated as an LP. Here, m is the number of training points, n
is the feature dimension, and the size of the constraint matrix in the LP becomes m
Comparisons and Metrics. We compare our Algorithm 2 with a standard IPM (see Chapter 10, [41])
using CG, and a standard IPM using a direct solver. We also use CVXPY as a benchmark to compare

(2n + 1).

≪

×

12

103

102

s
n
o
i
t
a
r
e
t
I

G
C
r
e
n
n

I

101

103

s
n
o
i
t
a
r
e
t
I

102

G
C
r
e
n
n

I

Stand. IPM
Sk. IPM w=200
Sk. IPM w=400
Sk. IPM w=1000

Stand. IPM
Sk. IPM w=200
Sk. IPM w=400
Sk. IPM w=1000

108

106

104

102

r
e
b
m
u
N
n
o
i
t
i
d
n
o
C

0

20

40
Outer Iterations

60

0

20

40
Outer Iterations

60

Stand. IPM
Sk. IPM w=500
Sk. IPM w=1000
Sk. IPM w=2000

Stand. IPM
Sk. IPM w=500
Sk. IPM w=1000
Sk. IPM w=2000

1010

r
e
b
m
u
N
n
o
i
t
i
d
n
o
C

108

106

104

102

101

0

10
20
Outer Iterations

30

0

10
20
Outer Iterations

30

(a)

(b)

Figure 1: ARCENE (top row) and DEXTER (bottom row) data sets: Our algorithm (Sk. IPM) requires
an order of magnitude fewer inner iterations than the Standard IPM with CG at each outer iteration,
1/2 com-
as demonstrated in (a). This is possibly due to the improved conditioning of Q−
pared to AD2AT , as shown in (b). For all experiments tolCG = 10−

1/2AD2ATQ−

5 and τ = 10−

9.

x⋆

−

2/
x⋆
k
k

ˆx
the accuracy of the solutions; we deﬁne the relative error k
2, where ˆx is our solution and
k
x⋆ is the solution generated by CVXPY. We also consider the number of outer iterations, namely
the number of iterations of the IIPM algorithm, as well as the number of inner iterations, namely the
number of iterations of the CG solver. We denote the relative stopping tolerance for CG by tolCG and
5, and σ = 0.5.
we denote the outer iteration residual error by τ . If not speciﬁed: τ = 10−
We evaluated a Gaussian sketching matrix, and the initial triplet (x, y, s) for all IPM algorithms was
set to be all ones.
Experimental Results. Figure 1(a) shows that our Algorithm 2 uses an order of magnitude fewer
inner iterations than the un-preconditioned standard solver. This is due to the improved conditioning
of the respective matrices in the normal equations, as demonstrated in Figure 1(b). Across various
real-world and synthetic data sets, the results were qualitatively similar to those shown in Figure 1.
Results for several real-world data sets are summarized in Table 1.

9, tolCG = 10−

In general, our preconditioned CG solver used in Algorithm 2 does not increase the total number
of outer iterations as compared to the standard IPM with CG, and the standard IPM with a direct
linear solver (denoted IPM w/Dir), as seen in Table 1. Actually, for unpreconditioned CG there is
clearly more outer iterations, especially for Gene RNA, which has x5 outer iterations. Figure 1 also
demonstrates the relative insensitivity to the choice of w (the sketching dimension, i.e., the number
of columns of the sketching matrix W of Section 2). For smaller values of w, our algorithm requires

13

 
 
 
 
 
 
Max. Inner Iterations

Max. Cond. Num.

w

.

i

m
D
h
c
t
e
k
S

200

300

500

1000

30

25

20

15

w

.

i

m
D
h
c
t
e
k
S

200

300

500

1000

3e-06 6e-06 1e-05 3e-05
Rel. Tolerance CG

3e-06 6e-06 1e-05 3e-05
Rel. Tolerance CG

(a) Max. Inner CG Iterations.

(b) Max. Condition Number.

40

35

30

25

20

15

10

5

for various (w, tolCG) settings, (a) the maximum number of inner
Figure 2: ARCENE data set:
1/2,
iterations used by our algorithm and (b) the maximum condition number of Q−
across outer iterations. The standard IPM, across all settings, needed on the order of 1,000 iterations
and κ(AD2AT ) was on the order of 108. The relative error was ﬁxed to 0.04%.

1/2AD2ATQ−

Table 1: Comparison of (our) sketched IPM with CG, standard IPM with CG, and Standard IPM with
a direct solver, for the ℓ1-SVM problem on UCI Machine Learning Repository [19] data sets. Across all,
1/2) and
τ = 10−
κStan = κ(AD2AT ).

3 or less was achieved. We deﬁne κSk = κ(Q−

9 and a relative error of 10−

1/2AD2ATQ−

Problem

Size

Sketch IPM w/ Precond. CG Stand. IPM w/ Unprec. CG IPM w/ Dir.

(m
(100
ARCENE
(300
DEXTER
DrivFace
(606
Gene RNA (801

N )
10K)
20K)
6400)
20531)

×
×
×
×
×

w
200
500
1000
2000

In. It. Out. It.

30
39
50
27

50
39
42
44

κSk
38.09
75.42
68.87
20.03

In. It. Out. It.
1.1K
4.6K
139K
101K

59
39
43
208

κStan

108
109
1012
1012

4.4
7.6
17
4.7

×
×
×
×

Out. It.
50
39
42
44

more inner iterations. However, across various choices of w, the number of inner iterations is always
an order of magnitude smaller than the number required by the standard solver.

Figure 2 shows the performance of our algorithm for a range of (w, tolCG) pairs. Figure 2(a)
demonstrates that the number of the inner iterations is robust to the choice of tolCG and w. The
number of inner iterations varies between 15 and 35 for the ARCENE data set, while the standard
IPM took on the order of 1, 000 iterations across all parameter settings. Across all settings, the
relative error was ﬁxed at 0.04%. In general, our sketched IPM is able to produce an extremely high
accuracy solution across parameter settings. Thus we do not report additional numerical results for
3 or less. Figure 2(b) demonstrates a trade-oﬀ of our
the relative error, which was consistently 10−
1/2) decreases,
approach: as both tolCG and w are increased, the condition number κ(Q−
corresponding to better conditioned systems. As a result, fewer inner iterations are required. In this
context, Figure 3 shows that how the number of inner CG iterations (Figure 3(a)) or the condition
1/2 (Figure 3(b)) decreases with the increase in sketching dimension w for
number of Q−
various tolCG .

1/2AD2ATQ−

1/2AD2ATQ−

14

 
 
 
 
30

25

20

15

s
n
o
i
t
a
r
e
t
I

r
e
n
n

I

CG tol  = 3e − 06
CG tol  = 6e − 06
CG tol  = 1e − 05
CG tol  = 3e − 05

)
2
/
1
−
Q
T
A
2
D
A
2
/
1
−
Q
(
κ

101

CG tol  = 3e − 06
CG tol  = 6e − 06
CG tol  = 1e − 05
CG tol  = 3e − 05

10

200

400

600
Sketch Dim. w

800

1000

200

400

600
Sketch Dim. w

800

1000

(a)

(b)

Figure 3: ARCENE data set: As w increases, (a) the number of inner iterations decreases, and is
relatively robust to tolCGand (b) the condition number decreases as well.

7 Conclusions

We proposed and analyzed an infeasible IPM algorithm using a preconditioned conjugate gradient
solver for the normal equations and a novel perturbation vector to correct for the error due to the
approximate solver. Thus, we speed up each iteration of the IPM algorithm, without increasing
the overall number of iterations. We demonstrate empirically that our IPM requires an order of
magnitude fewer inner iterations within each linear solve than standard IPMs. It would be interesting
to extend our work to analyze feasible IPMs. More precisely, we would like to apply Algorithm 2 of
Section 4 starting with a strictly feasible point. In that case, the analysis should be simpler and the
iteration complexity of the IPM algorithm should reduce to
(n log(1/ǫ)), which is the best known for
feasible long-step path following IPM algorithms. We chose to present the more technically challenging
approach in this paper and delegate the feasible case to future work.

O

Acknowledgements

AC and PD were partially supported by NSF FRG 1760353 and NSF CCF BSF 1814041. HA was
partially supported by BSF grant 2017698. PL was supported by an Amazon Graduate Fellowship in
Artiﬁcial Intelligence.

References

[1] Haim Avron, Petar Maymounkov, and Sivan Toledo. Blendenpik: Supercharging LAPACK’s

least-squares solver. SIAM Journal on Scientiﬁc Computing, 32(3):1217–1236, 2010.

[2] Owe Axelsson and Vincent A. Barker. Finite element solution of boundary value problems: Theory

and computation, volume 35. Society for Industrial and Applied Mathematics, 1984.

[3] Daniel Bienstock and Garud Iyengar. Approximating fractional packings and coverings in

iterations. SIAM Journal on Computing, 35(4):825–854, 2006.

(1/ǫ)

O

[4] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based matrix

reconstruction. SIAM Journal on Computing, 43(2):687–717, 2014.

15

 
[5] R. Bouyouli, Gérard Meurant, Laurent Smoch, and Hassane Sadok. New results on the conver-
gence of the conjugate gradient method. Numerical Linear Algebra with Applications, 16(3):223–
236, 2009.

[6] Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense linear programs

in nearly linear time. arXiv preprint arXiv:2002.02304, 2020.

[7] Agniva Chowdhury, Jiasen Yang, and Petros Drineas. An iterative, sketching-based framework
for ridge regression. In Proceedings of the 35th International Conference on Machine Learning,
volume 80, pages 988–997, 2018.

[8] Kenneth L. Clarkson and David P. Woodruﬀ. Low rank approximation and regression in input
sparsity time. In Proceedings of the 45th Annual ACM symposium on Theory of Computing, pages
81–90, 2013.

[9] Kenneth L Clarkson and David P Woodruﬀ. Low-rank approximation and regression in input

sparsity time. Journal of the ACM (JACM), 63(6):54, 2017.

[10] Michael B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In Proceed-
ings of the 27th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 278–287, 2016.

[11] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix
multiplication time. In Proceedings of the 51st Annual ACM Symposium on Theory of Computing,
pages 938–942, 2019.

[12] Michael B. Cohen, Jelani Nelson, and David P. Woodruﬀ. Optimal approximate matrix prod-
In 43rd International Colloquium on Automata, Languages, and

uct in terms of stable rank.
Programming, pages 11:1–11:14, 2016.

[13] Yiran Cui, Keiichi Morikuni, Takashi Tsuchiya, and Ken Hayami. Implementation of interior-point
methods for LP based on krylov subspace iterative solvers with inner-iteration preconditioning.
Computational Optimization and Applications, 74(1):143–176, 2019.

[14] Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized ﬂow via interior
point algorithms. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing,
pages 451–460, 2008.

[15] Katerine Diaz-Chito, Aura Hernández-Sabaté, and Antonio M. López. A reduced feature set for

driver head pose estimation. Applied Soft Computing, 45:98–107, 2016.

[16] David L. Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear
In Proceedings of the National Academy of Sciences of the

equations by linear programming.
United States of America, pages 9446–9451, 2005.

[17] Petros Drineas and Michael W. Mahoney. RandNLA: Randomized numerical linear algebra. Com-

munications of the ACM, 59(6):80–90, 2016.

[18] Petros Drineas and Michael W. Mahoney. Lectures on randomized numerical linear algebra, vol-
ume 25 of The Mathematics of Data, IAS/Park City Mathematics Series. American Mathematical
Society, 2018.

[19] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.

[20] Gene H. Golub and Charles F. Van Loan. Matrix computations, volume 3. Johns Hopkins

University Press, 2012.

[21] Isabelle Guyon, Steve Gunn, Asa Ben-Hur, and Gideon Dror. Result analysis of the NIPS 2003
feature selection challenge. In Advances in Neural Information Processing Systems, pages 545–552,

16

2005.

[22] Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2):217–288, 2011.

[23] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings

of the 16th Annual ACM Symposium on Theory of Computing, pages 302–311, 1984.

[24] Yin Tat Lee and Aaron Sidford. Path ﬁnding I: Solving linear programs with ˜
O

(√rank) linear

system solves. arXiv preprint arXiv:1312.6677, 2013.
[25] Yin Tat Lee and Aaron Sidford. Path ﬁnding II: An ˜
O
ﬂow problem. arXiv preprint arXiv:1312.6713, 2013.

(m√n) algorithm for the minimum cost

[26] Yin Tat Lee and Aaron Sidford. Path ﬁnding methods for linear programming: Solving linear
(√rank) iterations and faster algorithms for maximum ﬂow. In Proceedings of the

programs in ˜
O
55th IEEE Symposium on Foundations of Computer Science, pages 424–433, 2014.

[27] Yin Tat Lee and Aaron Sidford. Eﬃcient inverse maintenance and faster algorithms for linear
programming. In Proceedings of the 56th IEEE Symposium on Foundations of Computer Science,
pages 230–249, 2015.

[28] Yin Tat Lee and Aaron Sidford. Solving linear programs with ˜
O

arXiv preprint arXiv:1910.08033, 2019.

(√rank) linear system solves.

[29] Palma London, Shai Vardi, Adam Wierman, and Hanling Yi. A parallelizable acceleration frame-
In Proceedings of the 32nd AAAI Conference on Artiﬁcial

work for packing linear programs.
Intelligence, pages 3706 – 3713, 2018.

[30] David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer Publishing

Company, Incorporated, 3rd edition, 2008.

[31] Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends

in Machine Learning, 3(2):123–224, 2011.

[32] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the 45th Annual ACM Sym-
posium on Theory of Computing, pages 91–100, 2013.

[33] Xiangrui Meng, Michael A. Saunders, and Michael W. Mahoney. LSRN: A parallel iterative
solver for strongly over- or underdetermined systems. SIAM Journal on Scientiﬁc Computing,
36(2):95–118, 2014.

[34] Ofer Meshi and Amir Globerson. An alternating direction method for dual MAP LP relaxation.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages
470–483. Springer, 2011.

[35] Renato D. C. Monteiro and Jerome W. O’Neal. Convergence analysis of a long-step primal-
dual infeasible interior-point LP algorithm based on iterative linear solvers. Georgia Institute of
Technology, 2003.

[36] Renato D. C. Monteiro, Jerome W. O’Neal, and Takashi Tsuchiya. Uniform boundedness of a
preconditioned normal matrix used in interior-point methods. SIAM Journal on Optimization,
15(1):96–100, 2004.

[37] Jelani Nelson and Huy L. Nguyên. OSNAP: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In Proceedings of the 54th IEEE Symposium on Foundations of Computer

17

Science, pages 117–126, 2013.

[38] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,

2006.

[39] Christopher C. Paige and Michael A. Saunders. Solution of sparse indeﬁnite systems of linear

equations. SIAM Journal on Numerical Analysis, 12(4):617–629, 1975.

[40] Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algo-

rithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205–245, 2017.

[41] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical
In The Oxford Handbook of Innovation,

recipes 3rd edition: The art of scientiﬁc computing.
chapter 10. Cambridge University Press, 2007.

[42] Ben Recht, Christopher Re, Joel Tropp, and Victor Bittorf. Factoring nonnegative matrices with
linear programs. In Advances in Neural Information Processing Systems, pages 1214–1222, 2012.

[43] Mauricio G. C. Resende and Geraldo Veiga. An implementation of the dual aﬃne scaling algorithm
for minimum-cost ﬂow on bipartite uncapacitated networks. SIAM Journal on Optimization,
3(3):516–537, 1993.

[44] Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partition-
ing, graph sparsiﬁcation, and solving linear systems. In Proceedings of the 36th Annual ACM
Symposium on Theory of Computing, volume 4, pages 81–90, 2004.

[45] Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and
solving symmetric, diagonally dominant linear systems. SIAM Journal on Matrix Analysis and
Applications, 35(3):835–885, 2014.

[46] Ky Vu, Pierre-Louis Poirion, and Leo Liberti. Random projections for linear programming. Math-

ematics of Operations Research, 43(4):1051–1071, 2018.

[47] John N. Weinstein, Eric A. Collisson, Gordon B. Mills, Kenna R. Mills Shaw, Brad A. Ozenberger,
Kyle Ellrott, Ilya Shmulevich, Chris Sander, Joshua M. Stuart, et al. The cancer genome atlas
pan-cancer analysis project. Nature Genetics, 45(10):1113–1120, 2013.

[48] David P. Woodruﬀ. Sketching as a tool for numerical linear algebra. Foundations and Trends in

Theoretical Computer Science, 10(1-2), 2014.

[49] Stephen J. Wright. Primal-dual interior-point methods, volume 54. Society for Industrial and

Applied Mathematics, 1997.

[50] Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher Ré, and Michael W. Mahoney.
Sub-sampled Newton methods with non-uniform sampling. In Advances in Neural Information
Processing Systems, pages 3000–3008, 2016.

[51] Junfeng Yang and Yin Zhang. Alternating direction algorithms for ℓ1-problems in compressive

sensing. SIAM Journal on Scientiﬁc Computing, 33(1):250–278, 2011.

[52] Ming Yuan. High dimensional inverse covariance matrix estimation via linear programming. Jour-

nal of Machine Learning Research, 11(Aug):2261–2286, 2010.

[53] Yin Zhang. On the convergence of a class of infeasible interior-point methods for the horizontal

linear complementarity problem. SIAM Journal on Optimization, 4(1):208–227, 1994.

[54] Ji Zhu, Saharon Rosset, Robert Tibshirani, and Trevor J. Hastie. 1-norm support vector machines.

In Advances in Neural Information Processing Systems, pages 49–56, 2004.

18

Appendix A Richardson Iteration

Here, we show that all our analyses still hold if we replace Step 4 of Algorithm 1 (CG solver) with
Richardson’s iteration. Basically, all we need to show is that the condition of eqn. (7) holds. Note
w
that the condition of eqn. (6) already holds from Lemma 2, as we use the sketching matrix W
discussed in Section 3.

Rn

∈

×

Algorithm 3 Richardson Iteration Solver

Rm

Input: AD
∈
Initialize: ˜z0
←
for j = 1 to t do
1 + Q−
˜zj

˜zj
←
end for
Output: return ˜zt;

−

n, p

×
0m;

1/2(p

Rm; number of iterations t > 0; sketching matrix W

Rn

w;

×

∈

∈

AD2ATQ−

1/2˜zj

1);

−

−

Our ﬁrst result expresses the residual vector ˜f (j) in terms of ˜f (j

1) for j = 1 . . . t.

−

Lemma 6. Let ˜f (j), j = 1 . . . t be the residual vectors at each iteration.Then,

˜f (j) =

Q−

1/2AD2A

T

1/2

Q−

˜f (j

1) .

−

(34)

In −

(cid:16)

Recall that Q = ADWWTDAT and ˜f (j) = Q−
Proof. Using Algorithm 3, we express ˜f (j) as

1/2(AD2ATQ−

(cid:17)
1/2˜zj

p).

−

˜f (j) = Q−

= Q−

1/2AD2A
1/2AD2A

T

Q−

T

Q−

1/2˜zj
1/2

1/2p
Q−
1 + Q−

−
˜zj
−

=

Q−

T
1/2AD2A

Q−

(cid:16)
1/2˜zj

1

−

Q−

−
1/2AD2A

T

1/2p
(cid:17)
1/2

Q−

Q−

−

1/2(p

−

AD2A

T

Q−

1/2˜zj

−

Q−

1/2p

1)
(cid:17)

−

Q−

1/2AD2A

T

Q−

1/2˜zj

−

1

Q−

−

1/2p
(cid:17)

Q−

1/2AD2A

Q−

1/2AD2A

Im −
Im −

T

T

1/2

Q−

(cid:16)
1/2AD2A

T

Q−

Q−

1/2˜zj

−

1

1/2

Q−

(cid:17) (cid:16)
˜f (j

1) ,

−

(cid:17)

Q−

−

1/2p
(cid:17)

(cid:16)

(cid:16)

=

=

(cid:16)

which concludes the proof.

In the next result, we show that the spectral norm of Im −
Lemma 7. Let the condition of eqn. (6) holds for the sketching matrix W

Q−

1/2AD2ATQ−

1/2 is upper bounded by ζ.

Rn

w. Then

×

∈

1/2AD2A

T

1/2

Q−

Q−
k

Imk2 ≤

−

ζ .

Proof. As the condition in eqn. (6) holds, we can go backwards in the proof of Lemma 2 and note that
eqn. (17) holds. So, we subtract Im from each side of eqn. (17) to get

2

1
(cid:19)
Im 4 Q−

Im 4 Q−

(cid:18)

2 + ζ −
ζ
2 + ζ
ζ

⇔ −

⇒ −

2

ζ

−

Im 4 Q−

T
1/2AD2A

Q−

1/2

Im 4

−

1/2AD2A

T

1/2

Q−

1/2AD2A

T

1/2

Q−

Im 4

Im 4

−

−

ζ

−
ζ

−

2

2

19

2

−

ζ −

Im

1
(cid:19)

2

(cid:18)
Im

ζ

Im

ζ

(35)

Q−

⇔ k

1/2AD2A

T

1/2

Q−

ζ

Imk2 ≤

−

2

ζ ≤

ζ.

(36)

Eqn. (35) holds as

ζ
2+ζ ≤

ζ

2

−

−
ζ and the last inequality of eqn. (36) follows from ζ < 1.

Satisfying eqn. (6). Note that the condition in eqn. (6) already holds from Lemma 2, as we use the
w.
exact same sketching matrix W
Satisfying eqn. (7). Using Lemma 7 and applying Lemma 6 recursively, we get

Rn

∈

×

˜f (t)
k

k2 ≤

1)

ζ

˜f (t
−
k

k2 ≤ · · · ≤

ζ t

˜f (0)
k

k2 = ζ t

Q−
k

1/2p

k2 .

Appendix B Steepest Descent

We now replace Step 4 of Algorithm 1 (our proposed PCG solver) by preconditioned steepest descent.
We again prove that our analysis of the proposed infeasible long-step IPM remains essentially the
same.

First, we construct the sketching matrix W as discussed in Section 2, with a slightly more stringent

accuracy guarantee. More speciﬁcally, we necessitate that

T

V

T

WW

V

Im

−

2 ≤

ζ(1

ζ)

−
2

(37)

(cid:13)
(cid:13)
δ for a constant ζ
(cid:13)

(cid:13)
(cid:13)
(cid:13)
−

O

(m log(m/δ)) and the running time needed to compute Q−

[0, 1]. Notice that the sketching dimension
holds with probability at least 1
log(m/δ) +
w =
m3 log(m/δ))) remain, asymptotically, the same.
In the case of steepest descent, it turns out that
at each iteration the search direction is the negative of the gradient, which is equal to the residual
˜f (j). Moreover, the step size αj is determined by an exact line search that minimizes the underlying
quadratic function:

1/2 (which is

(nnz(A)

O

∈

·

αj =

˜f (j)T

Q−

˜f (j)T˜f (j)
1/2 AD2ATQ−

.

1/2˜f (j)

For this choice of αj, it is easy to verify that the current gradient is orthogonal to the previous one.

n, p

Rm; number of iterations t > 0; sketching matrix W

Algorithm 4 Steepest Descent Solver
Rm

Input: AD
∈
Initialize: ˜z0
for j = 0 to t

∈

×
0m;
1 do
T

←
−

˜f (j)

˜f (j)
1/2 AD2ATQ−
αj˜f (j);

1/2˜f (j) ;

αj =

T

˜f (j)

Q−

˜zj

←

˜zj+1
end for
Output: return ˜zt;

−

Rn

w;

×

∈

Similar to Lemma 6, our next result reveals a recursive relation between the search directions which
will be instrumental in bounding ˜f (t).
Lemma 8. Let ˜f (j), j = 1 . . . t be the residual vectors at each iteration and let αj be as in Algorithm 4.
Then,

˜f (j+1) =

αjQ−

1/2AD2A

T

1/2

Q−

˜f (j).

(38)

Recall that Q = ADWWTDAT and ˜f (j) = Q−

In −
(cid:16)

1/2(AD2ATQ−

1/2˜zj

(cid:17)

−

p) .

20

Proof. In Algorithm 4, we pre-multiply ˜zj+1 by Q−

1/2AD2ATQ−

1/2 and then subtract Q−

1/2p to get

˜f (j+1) = Q−

= Q−
= ˜f (j)

T

1/2AD2A
T
1/2AD2A

αjQ−

−

Q−

1/2p

1/2˜zj+1
1/2˜zj
Q−
−
T
1/2AD2A
Q−

Q−
−
1/2p
Q−
−
1/2˜f (j) =

which concludes the proof.

1/2AD2A

αjQ−

T

1/2˜f (j)
T

Q−
1/2AD2A

αjQ−
Im −
(cid:16)

Q−

1/2

˜f (j) ,

Next, using this new condition in eqn. (37), we bound

αjQ−

1/2AD2ATQ−

1/2

Lemma 9. If eqn. (37) is satisﬁed, then

αj −

|

1
| ≤

Proof. First, we rewrite eqn. (37) as follows,

Im −
(cid:13)
(cid:13)
ζ(1
(cid:13)
.
−
2

ζ)

(cid:17)

.

2

(cid:13)
(cid:13)
(cid:13)

ζ(1

ζ)

−
2

−

Im 4 V

T

T

WW

V

Im 4

−

ζ(1

ζ)

Im.

−
2

Next, we pre- and post-multiply the above expression by UΣ and ΣUT to get

ζ(1

−
2

−

ζ)

AD2A

T
T 4 ADWW
Q

T

DA

−

AD2A

T 4

ζ(1

−
2

ζ)

T
AD2A

.

(39)

}
1/2, we get
Now, pre and post-multiplying eqn. (39) again by Q−

{z

|

1
(cid:18)
1
⇒ (cid:18)

−

−

1
⇒ (cid:18)

−

ζ)

ζ)

ζ)

(cid:19)

(cid:19)

ζ(1

ζ(1

ζ(1

−
2

−
2

−
2

αj −

1
| ≤

⇒ |

ζ(1

(cid:19) ≤
ζ)
−
2

Q−

1/2AD2A

T

Q−

1/2 4 Im 4

˜f (j)T

Q−

1/2AD2A

T

Q−

1/2˜f (j)

ζ(1

1 +

−
2
˜f (j)T˜f (j)

(cid:18)

≤

˜f (j)T˜f (j)
1/2AD2ATQ−

Q−

1 +

1/2˜f (j) ≤ (cid:18)

˜f (j)T

(cid:19)
1 +

≤ (cid:18)
ζ(1

ζ)

−
2

(cid:19)

ζ)

Q−

T
1/2AD2A

Q−

1/2

ζ(1

ζ)

−
2

(cid:19)

˜f (j)T

Q−

1/2AD2A

T

Q−

1/2˜f (j)

, for j = 1 . . . t .

(40)

Our next result shows that if eqn. (37) holds, then
by a small quantity for j = 1 . . . t.

Lemma 10. If eqn. (37) is satisﬁed, then

1/2AD2ATQ−

ζ , for j = 1 . . . t.

Proof. We note that eqn. (37) directly implies

αjQ−

1/2AD2ATQ−

1/2

is upper bounded

2
(cid:13)
(cid:13)
(cid:13)

Im −
(cid:13)
(cid:13)
(cid:13)
αjQ−

Im −
(cid:13)
(cid:13)
(cid:13)

1/2

2 ≤

(cid:13)
(cid:13)
(cid:13)

Now, as eqn. (41) holds, from eqn. (17) in the proof of Lemma 2,we have

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

T

V

T

WW

V

Im

−

2 ≤

ζ
2

.

(41)

1 +

ζ
2
2αj
2 + ζ −

(cid:19)

(cid:18)

⇔ (cid:18)

−

1

Im 4 Q−

1/2AD2A

T

Q−

1/2 4

1

1

−

Im

Im 4 αjQ−

1/2AD2A

T

Q−

1
(cid:19)

(cid:18)
1/2

−

21

ζ
2

−

(cid:19)
Im 4

2αj
2

−

ζ −

(cid:18)

Im

1
(cid:19)

2(αj −

1)
2 + ζ

ζ

−

⇔

Im 4 αjQ−

1/2AD2A

T

1/2

Q−

Im 4

−

2(αj −
2
−

1) + ζ
ζ

Im.

(42)

The above expression follows by multiplying eqn. (17) by αj and then subtracting Im. Now, from
Lemma 9, we get
ζ) for j = 1 . . . t. Using this in eqn. (42), we further
have

2(αj −

ζ(1

ζ(1

ζ)

1)

−

≤

≤

−

−

ζ(1

−

−
2 + ζ
ζ)

ζ(2

−
2 + ζ

⇔ −

ζ) + ζ

Im 4 αjQ−

T
1/2AD2A

Q−

1/2

ζ(1

Im 4

−

−
2

ζ) + ζ
ζ

−

Im 4 αjQ−

1/2AD2A

T

Q−

1/2

Im 4 ζ Im

−
Im 4 ζ Im

T

1/2

⇒

1/2AD2A

Q−

⇒ −

αjQ−

1/2AD2A

ζ Im 4 αjQ−
Im −
(cid:13)
(cid:13)
where eqn. (43) is due to the fact that 2
ζ
(cid:13)
−
2+ζ ≤
Satisfying eqn. (6). As eqn. (41) holds, eqn. (6) directly follows from Lemma 2.
Satisfying eqn. (7). Using Lemma 10 and applying Lemma 8 recursively, we get

−
ζ ,

(cid:13)
(cid:13)
(cid:13)
1.

Q−

2 ≤

1/2

T

Im

(43)

˜f (t)
k

k2 ≤

1)

ζ

˜f (t
−
k

k2 ≤ · · · ≤

ζ t

˜f (0)
k

k2 = ζ t

Q−
k

1/2p

k2 .

Appendix C Convergence analysis of Algorithm 2

C.1 Additional notation
For any two vectors a = (a1, . . . , aℓ)T and b = (b1, . . . , bℓ)T let a
vector a
norm is deﬁned as
∞
inequality to prove results in this section:

= maxi |

Rn its ℓ

ai|

a
k

k∞

∈

b = (a1b1, . . . , aℓbℓ)T. For any
. We heavily use the following standard

◦

a

≤ k

a

k∞ ≤ k

k2.

(44)

aT1n
n (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C.2 Number of iterations for the CG solver

In this section, most of the proofs follow [35] except for the fact that we used our sketching based
preconditioner Q−

is the set of optimal and feasible solutions for the proposed LP.

1/2. Recall that

S

Lemma 11. Let (x0, y0, s0) be the initial point with (x0, s0) > 0 and (x∗, y∗, s∗)
(x∗, s∗)
0

. Then, for any point (x, y, s)
c
|

≥ |
, we get

≤
min

ATy0

∈ N

−

η

such that
∈ S
(γ) such that r = η r0 and

(x0, s0) with s0
1, sTx
s0Tx0
n

o

≤

≤

3nµ ,

T

T
s0 + s

(ii) η

(i) η (x

x0)
≤
x0)
S(x∗
k2 ≤
k
−
T
y0
X(s0 + A
(iii) η
−
k

η

Sx0
k
c)

k2 ≤

k2 ≤
2η

T

x0
ηs
Xs0
k

≤
k2 ≤

3nµ ,

2η x

T

s0

6nµ .

≤

(45a)

(45b)

(45c)

Proof. We prove eqns. (45a)–(45c) below.
Proof of eqn. (45a). For completeness, we provide a proof of eqn. (45a) following [35]. Since
(x∗, s∗, y∗)

, the following equalities hold:

∈ S

Ax∗ = b

22

(46a)

Furthermore, r = ηr0 implies

T

A

y∗ + s∗ = c.

Ax

T

A

y + s

−

−

b = η(Ax0
T

c = η(A

b)
−
y0 + s0

c).

−

Combining eqn. (46a) with eqn. (47a) and eqn. (46b) with eqn. (47b), we get

ηy0

(1

−

−

η)y∗) + (s

(cid:0)

A

x

ηx0
ηs0

−

−

(1

(1

−

−

−

−

η)x∗

= 0

η)s∗) = 0.
(cid:1)

T

A

(y

Multiplying eqn. (48b) by

x

−

−

ηx0

(1

−

−

η)x∗

T

(cid:1)
T

(cid:0)
x

(cid:16)

ηx0

−

(1

−

−

η)x∗

Expanding we get

ηs0

s

−

(1

−

−

η)s∗

= 0.

(cid:17)

(cid:17)

(cid:16)

on the left and using eqn. (48a), we get

(46b)

(47a)

(47b)

(48a)

(48b)

x0T

s + x

T

s0

= η2x0T

η

s0 + (1

(cid:16)

(cid:17)

+ η(1

−

s∗ + x

T

s

T
η)2(x∗)
x0T

−
η)

s∗ + (x∗)

(cid:16)

Next, we use the given conditions and rewrite eqn. (49) as

T

s0

−

(cid:17)

(1

−

η)

T

(x∗)

s + x

T

s∗

.

(49)

(cid:16)

(cid:17)

s + s0T

x

η

x0T
(cid:16)

η2x0T
η2x0T
2ηx0T

(cid:17)

≤

≤

≤

−

x0T
η)
(cid:16)
η)x0T

s∗ + s0T
s0

s0 + x

T

s + η(1

T

s0 + x
s0 + x

s + 2η(1
T

3x

T

s

−
s = 3nµ.

≤

x∗

(cid:17)

(50)

≥

0 and (x0, s0)

0. Second, as (x∗, s∗, y∗)

≥
0; combining them we get (x0T

The ﬁrst inequality in eqn. (50) follows from the following facts. First, (1
0
−
s∗ = 0), we have
as (x∗, s∗)
(which implies x∗ ◦
(x∗)Ts∗ = 0. The second inequality in eqn. (50) holds as x∗ ≤
x0, s∗ ≤
0, and
s0. Third inequality in eqn. (50) is true
(x0, s0)
≥
2ηx0T
as we have η2x0T
s0. The ﬁnal inequality holds as
η
s0 .
Proof of eqn. (45b). The last inequality follows from eqn. (45a). The second to last inequality is
also easy to prove as

∈ S
2 x0T
s0

s∗ + s0T
s0

s0 = 2ηx0T

x∗)
≤
η2x0T

s0, (x∗, s∗)

η)x0T

xTs
T
x0

+ 2η(1

−

≥

−

≤

≥

≤

η)((x∗)Ts + xTs∗)

Sx0
k

k2 =

s

(six0

i )2

v
u
u
t

Xi=1

2

six0
i

!

s

Xi=1

≤ v
u
u
t

T

= s

x0 .

(51)

To prove the ﬁrst inequality in eqn. (45b), we use the fact x0

Sx0
k

2
2 − k
k

S(x∗

2
x0)
2 =
k

−

(six0

i )2

−

n

Xi=1
n

n

Xi=1

x∗ as follows:

≥

2x∗i x0
i

−

(cid:17)

i )2

s2
i

(x∗i )2 + (x0
(cid:16)
(x∗i )2

0 .

≥

(cid:17)

=

s2
i

Xi=1

(cid:16)

2x∗i x0

i −

23

 
Proof of eqn. (45c). To prove this we use a similar approach as in eqn. (45b). The last inequality
directly follows from eqn. (45a); the second to last inequality is also easy to prove as

Xs0
k

k2 =

(xis0

i )2

n

Xi=1

≤ v
u
u
t

2

xis0
i

!

n

Xi=1

= x

T

s0 .

v
u
u
t

For the ﬁrst inequality, we proceed as follows:

X(s0 + A
k

T

y0

2
2 =
c)
k

−

=

Xs0
k
Xs0
k

2
2 +
k
2
2 +
k

Xs0

≤ k

2
2 +
k

=

Xs0
k

2
2 +
k

T

y0

X(A
k
n

x2
i (A

Xi=1
n

−
y0

T

(xis0

i )2 + 2

2 + 2s0T
2
c)
k
c)2

i + 2

−

X
n

n

Xi=1
i )2

(xis0

Xi=1
Xs0
k

Xi=1
Xs0
2
Xs0
2
2 = 4
2 + 2
k
k
k
k
(ATy0

0 and

(52)

c)

T

T

X(A

y0

i s0
x2

i (A

T

−
y0

c)i

−

2
2.
k

(53)

s0
i for all i = 1 . . . n.

0, s0

The inequality in eqn. (53) follows from xi ≥
Our next result bounds

(cid:12)
(cid:12)
k2 which is instrumental in proving the ﬁnal bound.
(cid:12)
Lemma 12. Let (x0, y0, s0) be the initial point with (x0, s0) > 0 such that x0
max
c
s∗,
|
{
for some 0

for some (x∗, y∗, s∗)
1. If the sketching matrix W

. Furthermore, let (x, y, s)

ATy0
η

Q−
k

∈ N

1/2p

i ≥

c)i

Rn

−

≤

|}

(cid:12)
(cid:12)
(cid:12)

w satisﬁes the condition in eqn. (6), then

×

x∗ and s0
≥
(γ) with r = η r0

≥

−
≤

≤

Recall that r = (rp, rd) = (Ax

−

1/2p

Q−
k

k2 ≤

√2

+ σ

√1

γ

(cid:18)
b, ATy + s

r

−

−
c) and r0 = (r0
p, r0

−

n

1

γ

+ √n

√µ .

(cid:19)

d) = (Ax0

b, ATy0 + s0

−

c) .

−

∈ S
∈
9n

Proof. Note that after correcting the approximation error of the CG solver using v, the primal and
(γ) always lie on the line segment
dual residuals r = (rp, rd) corresponding to an iterate (x, y, s)
between zero and r(0). In other words, r = ηr(0) always holds for some η
[0, 1]. This was formally
k2, ﬁrst we express p as in eqn. (3) and rewrite
proven in Lemma 3.3 of [35]. In order to bound
11n + Ax
(54)
.

1/2p = Q−

AD2rd

σµAS−

1/2p

∈ N

Q−

1/2

∈

Q−
k
rp −

−

(cid:16)
Then, applying the triangle inequality to

−

(cid:17)

Q−
k

1/2p

k2 in eqn. (54), we get

Q−
k

1/2p

k2 ≤

∆1 + ∆2 + ∆3 + ∆4 ,

(55)

where

∆1 =

Q−
k

1/2rpk2 ,
Q−
k
1/2ADD−
1x
Q−
k
1/2AD2rdk2 .
Q−
k

1/2AD(XS)−
k2 ,

∆2 = σµ

∆3 =

∆4 =

1/21nk2 ,

To bound ∆1, ∆2, ∆3 and ∆4 we heavily use the condition of eqn. (6).

24

 
Bounding ∆1. Using rp = η r0

p, r0

p = Ax0

b and b = Ax∗, we rewrite ∆1 as

−
1/2A(x0
−
1/2ADD−
1/2AD
1(x0

k2k

k2

x∗)
1(x0

−
1(x0

D−

x∗)

k2
x∗)

k2

−

k2

D−
k
(XS)−
k
(XS)−
k

x∗)
−
1/2S(x0
1/2

k2 k

−
S(x0

x∗)

−

k2
x∗)

k2 ,

∆1 = η

≤

= η

Q−
k
Q−
k
η
Q−
k
√2η
≤
= √2η
√2η

≤

(56)

where the above steps follow from submultiplicativity and eqn. (6). From eqn. (6), note that we have
, we further
Q−
k
have

1 . Now, applying eqn. (45b) and

k2 = max1

(XS)−
k

√2 as ζ

1/2AD

1
√xisi

k2 ≤

i
≤

≤

1/2

≤

n

∆1 ≤

≤

√2 max
n
i
1
≤
≤
3√2 n

1
√xisi ·
µ

,

γ

r

1

−
(γ).

where the last inequality follows from (x, y, s)

∈ N

Bounding ∆2. Applying submultiplicativity, we get

3nµ

(57)

∆2 = σµ

Q−
k
σµ
Q−
k
√2 σµ

≤

≤

1/2 AD (XS)−
1/2 AD

(XS)−

1/21nk2

1/21nk2

= √2 σµ

√2 σµ

k2k
1/21nk2
1

xisi ≤

(XS)−
k

n

v
u
u
t

Xi=1
n µ

= √2 σ

(1

s

γ)

−

n

1

v
u
u
t

Xi=1

γ)µ

(1

−

,

(58)

where the second to last inequality follows from eqn. (6) and the last inequality holds as (x, y, s)

(γ).

N

Bounding ∆3. Using D = S−

1/2X1/2 and x = X 1n we get

∈

∆3 =

=

Q−
k
Q−
k
Q−

1/2) X 1nk2

1/2 AD (S1/2X−
1/2 AD (SX)1/2 1nk2
1/2 AD
k2k

(SX)1/2 1nk2

≤ k

where the inequalities follow from submultiplicativity and eqn. (6).

√2

≤

n

v
u
u
t

Xi=1

xisi =

2n µ ,

p

(59)

25

Bounding ∆4. Using rd = η r0

d, we have

∆4 = η

Q−
k
η
Q−
k
√2η
√2η

≤

≤

1/2 A D2r0
1/2 AD

(XS)−
k
(XS)−
k

dk2
(XS)−
k2k
T
1/2X(A
1/2

1/2Xr0
y0 + s0
T

X(A

dk2

c)
−
y0 + s0

k2

c)

k2 ,

k2 k
where the above inequalities follow from submultiplicativity and eqn. (6). Now, applying eqn. (45c)
and

, we further have

−

≤

1/2

(XS)−
k

k2 ≤

1
√(1

−

γ)µ

∆4 ≤

6√2n

µ

−

.

γ

1

r

Final bound. Combining eqns. (55), (57), ,(58), (59) and (60), we get

Q−
k

1/2p

k2 ≤

√2

9n

√1

(cid:18)

γ

−

+ σ

1

r

n

−

γ

+ √n

√µ .

(cid:19)

This concludes the proof of Lemma 12.

(60)

(61)

Lemma 13. Let the sketching matrix W satisfy the conditions of eqns. (6) and (7). Then, after
t

iterations of the CG solver in Algorithm 1,

log(4√6n ψ/γσ)
log(1/ζ)

≥

˜f (t)
k

k2 ≤

γσ
4√n

√µ and

v
k2 ≤
k

γσ
4

µ.

Here ψ =

CG solver.

9n

√1

−

γ + σ

(cid:18)

q

γ + √n

n

1

−

(cid:19)

and ˜f (t) = Q−

1/2AD2ATQ−

1/2˜zt

Q−

1/2p is the residual of the

−

Proof. Combining Lemma 12 and the condition in eqn. (7), we get

Now,
≥
holds for our choice of t. Next, combining Lemma 5 and eqn. (62) we get

4√n √µ holds if √2ψ ζ t√µ

k2 ≤

≤

(cid:16)

(cid:17)

˜f (t)
k

γσ

1
ζ

p
γσ
4√n √µ, which holds if

t

˜f (t)
k

k2 ≤

ζ tψ

2µ.

(62)

4√2 n ψ
γσ

. The last inequality

v
k2 ≤
k

3nµ

k2 ≤

√6n ζ tψµ

˜f (t)
k
γσµ
4 , which holds for our choice of t. Now, ﬁxing γ, σ,

p
γσµ
4 holds if √6nψ ζ tψµ

≤

(log n) iterations of Algorithm 1 the conclusions of the lemma hold.

Therefore,
and ζ, after t =

v
k2 ≤
k
O

C.3 Determining step-size, bounding the number of iterations, and proof of The-

orem 1

Assume that the triplet ( ˆ∆x, ˆ∆y, ˆ∆s) satisﬁes eqns. (23), (24) and (25). We rewrite this system in
the following alternative form:

A ˆ∆x =
T ˆ∆y + ˆ∆s =

A

rp,

rd,

−

−

26

(63a)

(63b)

X ˆ∆s + S ˆ∆x =

XS 1n + σµ 1n −

−

v.

(63c)

Indeed, we now show how to derive eqns. (23), (24) and (25) from eqn. (63). Pre-multiplying both
sides of eqn. (63c) by AS−

1 and noting that D2 = XS−

1, we get

AX1n + σµAS−
11n −
Eqn. (64) holds as AX1n = Ax and, from eqn. (63a), A ˆ∆x =
by AD2, we get

AD2 ˆ∆s + A ˆ∆x =
AD2 ˆ∆s =

Ax + rp + σµAS−

⇒

−

−

−

1v

AS−

11n −
1v.
AS−

(64)

rp. Next, pre-multiplying eqn. (63b)

AD2A
AD2A

T ˆ∆y + AD2 ˆ∆s =
T ˆ∆y =

−
σµAS−

AD2rd

rp −

−

11n + Ax

−

AD2rd + AS−

1v = p + AS−

1v.

(65)

⇒

The ﬁrst equality in eqn. (65) follows from eqn. (64) and the deﬁnition of p. This establishes eqn. (24).
Eqn. (25) directly follows from eqn. (63b). Finally, we get eqn. (23) by pre-multiplying eqn. (63c) by
S−

1.
Next, we deﬁne each new point traversed by the algorithm as (x(α), y(α), s(α)), where

(x(α), y(α), s(α)) = (x, y, s) + α( ˆ∆x, ˆ∆y, ˆ∆s),

µ(α) = x(α)

T

s(α)/n,

r(α) = r (x(α), s(α), y(α)) .

(66)

(67)

(68)

The goal in this section is to bound the number of iterations required by Algorithm 2. Towards that
end, we bound the magnitude of the step size α. First, we provide an upper bound on α, which
allows us to show that each new point (x(α), s(α), y(α)) traversed by the algorithm stays within the
neighborhood
(γ). Second, we provide a lower bound on α, which allows us to bound the number of
iterations required. We use multiple lemmas from [35], which we reproduce here, without their proofs.
First, we provide an upper bound on α, ensuring that each new point (x(α), y(α), s(α)) traversed
(γ).

by the algorithm stays within the neighborhood

N
Lemma 14 (Lemma 3.5 of [35]). Assume ( ˆ∆x, ˆ∆y, ˆ∆s) satisﬁes eqns. (63) for some σ > 0, (x, y, s)
γσµ
4 . Then, (x(α), y(α), s(α))

∈
(γ) for every scalar α such that

(0, 1)), and

(γ) (for γ

N

N

∈

v
k2 ≤
k

∈ N

α

0

≤

≤

min

1,

(

γσµ

ˆ∆x
4
k

◦

ˆ∆s

k∞

.

)

(69)

We now provide a lower bound on the values of ¯α and the corresponding µ(¯α); see Algorithm 2.

Lemma 15 (Lemma 3.6 of [35]). In each iteration of Algorithm 2, if
¯α satisﬁes

v
k2 ≤
k

γσµ
4 , then the step size

min

¯α

≥

1,

(

min

γσ, (1
{
ˆ∆x
4
k

◦

−
ˆ∆s

5
µ
4 σ)
}

k∞

)

and

µ(¯α) =

¯α
2

(1

−

5
4

−

1
h

µ.

σ)
i

(70)

(71)

At this point, we have provided a lower bound (eqn. (70)) for the allowed values of the step size ¯α.
Next, we show that this lower bound is bounded away from zero. From eqn. (70) this is equivalent to
showing that

is bounded.

ˆ∆s

ˆ∆x
k

◦

k∞

27

(74)

(γ),

∈ N

(75)

Lemma 16 (Lemma 3.7 of [35] (slightly modiﬁed)). Let (x0, y0, s0) be the initial point with (x0, s0) > 0
and (x0, s0)
(γ) be such that r = ηr0 for some
(x∗, s∗) for some (x∗, y∗, s∗)
≥
4 . Then, the search direction ( ˆ∆x, ˆ∆y, ˆ∆s) produced by Algorithm 2 at each
η
v
[0, 1] and
k2 ≤
k
iteration satisﬁes

. Let (x, y, s)

∈ N

∈ S

γσµ

∈

max

D−

{k

1 ˆ∆x

k2,

D ˆ∆s
k

k2} ≤  

1 +

1/2

γ −

2σ

!

√nµ +

σ2

−

1

6n

γ)

(1

−

√µ +

γσ
4 √1

−

√µ.

γ

(72)

We should note here that the above lemma is slightly diﬀerent than Lemma 3.7 of [35]. Indeed, Lemma
3.7 of [35] actually proves the following bound:

p

max

D−

{k

1 ˆ∆x

k2,

D ˆ∆s
k

k2} ≤  

1 +

1/2

γ −

2σ

!

√nµ +

σ2

−

1

6n

(1

−

√µ +

γ)

γσ
4√n

√µ .

(73)

Notice that there is slight diﬀerence in the last term in the right-hand side, which does not asymptoti-
cally change the bound. The underlying reason for this diﬀerence is the fact that [35] constructed the
vector v diﬀerently. In our case, we need to bound

1/2v

p

(XS)−
k

1/2v

k2 ≤ k

(XS)−

1/2

(XS)−
k

k2, which we do as follows:
,

v
k2 ≤
k2 k
1/2

(XS)−
k

γσµ
4

1
mini √xisi
k2 =

1
mini √xisi

. Now as (x, y, s)

where in the above expression we use the fact that
(1
we further have xisi ≥

−

γ)µ for all i = 1 . . . n. Combining this with eqn. (74), we get

(XS)−
k

1/2v

k2 ≤

4

γσµ
(1

−

=

γ)µ

γσ
4 √1

−

√µ.

γ

On the other hand, [35] had a diﬀerent construction of v for which
Therefore they had the following bound:

p

(XS)−
k

1/2v

k2 =

˜f (t)
k

k2 ≤

γσ
4√n

√µ.

(XS)−
k

1/2v

k2 =

˜f (t)
k

k2 holds.

The next lemma bounds the number of iterations that Algorithm 2 needs when started with an
infeasible point that is suﬃciently positive.

1

1, (1

Lemma 17 (Theorem 2.6 of [35]). Assume that the constants γ and σ are such that max
1, σ−
γ)−
(x∗, s∗, y∗)
ǫ
k2 ≤

γ−
{
−
(1). Let the initial point (x0, s0, y0) satisfy (x0, s0)
(x∗, s∗) for some
γσµ
4 . Algorithm 2 generates an iterate (xk, sk, yk) satisfying µk ≤
ǫµ0 and

5
4 σ)−
=
}
O
−
and
v
k2 ≤
∈ S
k
(n2 log 1/ǫ) iterations.
r0
k2 after
O
k

rk
k
Finally, Theorem 1 follows from Lemmas 13 and 17.

≥

1, (1

Appendix D Additional notes on experiments

D.1 Support Vector Machines (SVMs)

The classical ℓ1-SVM problem is as follows. We consider the task of ﬁtting an SVM to data pairs
. Here, m is the number of training points, and n
1
+1,
S =
}
is the feature dimension. The SVM problem with an ℓ1 regularizer has the following form:

m
i=1, where xi ∈
(xi, yi)
}
{

Rn and yi ∈ {

−

minimize
w
subject to

w
k
yi(wT xi + b′)
This problem can be written as an LP by introducing the variables w+ and w−, where w = w+
j + w−j , and we constrain w+
The objective becomes
the constraint matrix in the LP becomes m

0 and w−i ≥

i = 1 . . . m.

j=1 w+
n

(2n + 1).

w−.
0. Note that the size of

i ≥

(76)

k1

≥

−

1,

P

×

28

D.2 Random data

n, we
We generate random synthetic instances of linear programs as follows. To generate A
×
∈
i.i.d. draws
m, n
set aij ∼i.i.d. U (0, 1) with probability p and aij = 0 otherwise. We then add min
{
from U (0, 1) to the main diagonal, to ensure each row of A has at least one nonzero entry. We set
b = Ax + 0.1z, where x and z are random vectors drawn from N (0, 1). Finally, we set c

N (0, 1).

}

Rm

∼

D.3 Real-world data

We used a gene expression cancer RNA-Sequencing dataset, taken from the UCI Machine Learning
repository. It is part of the RNA-Seq (HiSeq) PANCAN data set [47] and is a random extraction of
gene expressions from patients who have diﬀerent types of tumors: BRCA, KIRC, COAD, LUAD, and
PRAD. We considered the binary classiﬁcation task of identifying BRCA versus other types.
We also used the DrivFace dataset taken from the UCI Machine Learning repository.

In the
DrivFace dataset, each sample corresponds to an image of a human subject, taken while driving
in real scenarios. Each image is labeled as corresponding to one of three possible gaze directions:
left, straight, or right. We considered the binary classiﬁcation task of identifying two diﬀerent gaze
directions: straight, or to either side (left or right).

29

