1
2
0
2

b
e
F
8
1

]

G
L
.
s
c
[

1
v
4
0
1
9
0
.
2
0
1
2
:
v
i
X
r
a

Distributed Algorithms for Linearly-Solvable Optimal Control in

Networked Multi-Agent Systems

Neng Wan1, Aditya Gahlawat1, Naira Hovakimyan1,
Evangelos A. Theodorou2, and Petros G. Voulgaris3

Abstract

Distributed algorithms for both discrete-time and continuous-time linearly solvable optimal con-

trol (LSOC) problems of networked multi-agent systems (MASs) are investigated in this paper. A

distributed framework is proposed to partition the optimal control problem of a networked MAS

into several local optimal control problems in factorial subsystems, such that each (central) agent

behaves optimally to minimize the joint cost function of a subsystem that comprises a central agent

and its neighboring agents, and the local control actions (policies) only rely on the knowledge of

local observations. Under this framework, we not only preserve the correlations between neighboring

agents, but moderate the communication and computational complexities by decentralizing the sam-

pling and computational processes over the network. For discrete-time systems modeled by Markov

decision processes, the joint Bellman equation of each subsystem is transformed into a system of

linear equations and solved using parallel programming. For continuous-time systems modeled by

Itˆo diﬀusion processes, the joint optimality equation of each subsystem is converted into a linear

partial diﬀerential equation, whose solution is approximated by a path integral formulation and a

sample-eﬃcient relative entropy policy search algorithm, respectively. The learned control policies

are generalized to solve the unlearned tasks by resorting to the compositionality principle, and illus-

trative examples of cooperative UAV teams are provided to verify the eﬀectiveness and advantages

of these algorithms.

Keywords: Multi-Agent Systems, Linearly-Solvable Optimal Control, Path Integral Control, Rela-

tive Entropy Policy Search, Compositionality.

1 Introduction

The research of control and planning in multi-agent systems (MASs) has been developing rapidly

during the past decade with the growing demands from areas, such as cooperative vehicles [1, 2],

Internet of Things [3], intelligent infrastructures [4–6], and smart manufacturing [7]. Distinct

1Neng Wan, Aditya Gahlawat, and Naira Hovakimyan are with the Department of Mechanical Science and Engineer-
ing, University of Illinois at Urbana-Champaign, Urbana, IL 61801. {nengwan2, gahlawat, nhovakim}@illinois.edu.
2Evangelos A. Theodorou is with the Department of Aerospace Engineering, Georgia Institute of Technology, Atlanta,
GA 30332. evangelos.theodorou@gatech.edu.
3Petros G. Voulgaris is with the Department of Mechanical Engineering, University of Nevada, Reno, NV 89557.
pvoulgaris@unr.edu.

1

 
 
 
 
 
 
from other control problems, control of MASs is characterized by the issues and challenges, which

include, but are not limited to, a great diversity of possible planning and execution schemes,

limited information and resources of the local agents, constraints and randomness of communication

networks, optimality and robustness of joint performance. A good summary of recent progress in

multi-agent control can be found in [8–14]. Building upon these results and challenges, this paper

puts forward a distributed optimal control scheme for stochastic MASs by extending the linearly-

solvable optimal control algorithms to MASs subject to stochastic dynamics in the presence of an

explicit communication network and limited feedback information.

Linearly-solvable optimal control (LSOC) generally refers to the model-based stochastic opti-

mal control (SOC) problems that can be linearized and solved with the facilitation of Cole-Hopf

transformation, i.e. exponential transformation of value function [15, 16]. Compared with other

model-based SOC techniques, since LSOC formulates the optimality equations in linear form, it en-

joys the superiority of analytical solution [17] and superposition principle [18], which makes LSOC

a popular control scheme for robotics [19,20]. LSOC technique was ﬁrst introduced to linearize and

solve the Hamilton–Jacobi–Bellman (HJB) equation for continuous-time SOC problems [15], and

the application to discrete-time SOC, also known as the linearly-solvable Markov decision process

(LSMDP), was initially studied in [16]. More recent progress on single-agent LSOC problems can

be found in [17, 21–25].

Diﬀerent from many prevailing distributed control algorithms [10–13], such as consensus and

synchronization that usually assume a given behavior, multi-agent SOC allows agents to have dif-

ferent objectives and optimizes the action choices for more general scenarios [8]. Nevertheless, it

is not straightforward to extend the single-agent SOC methods to multi-agent problems. The ex-

ponential growth of dimensionality in MASs and the consequent surges in computation and data

storage demand more sophisticated and preferably distributed planning and execution algorithms.

The involvement of communication networks (and constraints) requires the multi-agent SOC al-

gorithms to achieve stability and optimality subject to local observation and more involved cost

function. While the multi-agent Markov decision process (MDP) problem has received plenty of

attention from both the ﬁelds of computer science and control engineering [8, 14, 26–29], there are

relatively fewer results focused on multi-agent LSMDP. A recent result on multi-agent LSMDP rep-

resented the MAS problem as a single-agent problem by stacking the states of all agents into a joint

state vector, and the scalability of the problem was addressed by parameterizing the value func-

tion [30]; however, since the planning and execution of the control action demand the knowledge of

global states as well as a centralized coordination, the parallelization scheme of the algorithm was

postponed in that paper. While there are more existing results focused on the multi-agent LSOC

in continuous-time setting, most of these algorithms still depend on the knowledge of the global

states, i.e. a fully connected communication network, which may not be feasible or aﬀordable to

attain in practice. Some multi-agent LSOC algorithms also assume that the joint cost function can

be factorized over agents, which basically simpliﬁes the multi-agent control problem into multiple

2

single-agent problems, and some features and advantages of MASs are therefore forfeited. Broek

et al.

investigated the multi-agent LSOC problem for continuous-time systems governed by Itˆo

diﬀusion process [31]; a path integral formula was put forward to approximate the optimal control

actions, and a graphical model inference approach was adopted to predict the optimal path distri-

bution; nonetheless, the optimal control law assumed an accurate and complete knowledge of global

states, and the inference was performed on the basis of mean-ﬁeld approximation, which assumes

that the cost function can be disjointly factorized over agents and ignores the correlations between

agents. A distributed LSOC algorithm with inﬁnite-horizon and discounted cost was studied in [32]

for solving a distance-based formation problem of nonholonomic vehicular network without explicit

communication topology. The multi-agent LSOC problem was also recently discussed in [25] as

an accessory result for a novel single-agent LSOC algorithm; an augmented dynamics was built by

piling up the dynamics of all agents, and a single-agent LSOC algorithm was then applied to the

augmented system. Similar to the discrete-time result in [30], the continuous-time result resorting

to augmented dynamics also presumes the fully connected network and faces the challenge that

the computation and sampling schemes that originated from single-agent problem may become

ineﬃcient and possibly fail as the dimensions of augmented state and control grow exponentially

in the number of agents.

To address the aforementioned challenges, this paper investigates the distributed LSOC algo-

rithms for discrete-time and continuous-time MASs with consideration of local observation, cor-

relations between neighboring agents, eﬃcient sampling and parallel computing. A distributed

framework is put forward to partition the connected network into multiple factorial subsystems,

each of which comprises a (central) agent and its neighboring agents, such that the local control

action of each agent, depending on the local observation, optimizes the joint cost function of a

factorial subsystem, and the sampling and computational complexities of each agent are related

to the size of the factorial subsystem instead of the entire network. Sampling and computation

are parallelized to expedite the algorithms and exploit the resource in network, with state mea-

surements, intermediate solutions, and sampled data exchanged over the communication network.

For discrete-time multi-agent LSMDP problem, we linearize the joint Bellman equation of each

factorial subsystem into a system of linear equations, which can be solved with parallel program-

ming, making both the planning and execution phases fully decentralized. For continuous-time

multi-agent LSOC problem, instead of adopting the mean-ﬁeld assumption and ignoring the cor-

relations between neighboring agents, joint cost functions are permitted in the subsystems; the

joint optimality equation of each subsystem is ﬁrst cast into a joint stochastic HJB equation, and

then solved with a distributed path integral control method and a sample-eﬃcient relative entropy

policy search (REPS) method, respectively. The compositionality of LSOC is utilized to eﬃciently

generate a composite controller for unlearned task from the existing controllers for learned tasks.

Illustrative examples of coordinated UAV teams are presented to verify the eﬀectiveness and ad-

vantages of multi-agent LSOC algorithms. Building upon our preliminary work on distributed path

3

integral control for continuous-time MASs [33], this paper not only integrates the distributed LSOC

algorithms for both discrete-time and continuous-time MASs, but supplements the previous result

with a distributed LSMDP algorithm for discrete-time MASs, a distributed REPS algorithm for

continuous-time MASs, a compositionality algorithm for task generalization, and more illustrative

examples.

The paper is organized as follows: Section 2 introduces the preliminaries and formulations of

multi-agent LSOC problems; Section 3 presents the distributed LSOC algorithms for discrete-time

and continuous-time MASs, respectively; Section 4 shows the numerical examples, and Section 5

draws the conclusions. Some notations used in this paper are deﬁned as follows: For a set S, |S|

represents the cardinality of the set S; for a matrix X and a vector v, det X denotes the determinant
of matrix X, and weighted square norm (cid:107)v(cid:107)2
X := v(cid:62)Xv.

2 Preliminaries and Problem Formulation

Preliminaries on MASs and LSOC are introduced in this section. The communication networks

underlying MASs are represented by graphs, and the discrete-time and continuous-time LSOC prob-

lems are extended from single-agent scenario to MASs under a distributed planning and execution

framework.

2.1 Multi-Agent Systems and Distributed Framework

For a MAS consisting of N ∈ N agents, D = {1, 2, · · · , N } denotes the index set of agents, and
the communication network among agents is described by an undirected graph G = {V, E}, which

implies that the communication channel between any two agents is bilateral. The communication

network G is assumed to be connected. An agent i ∈ D in the network is denoted as a vertex vi ∈
V = {v1, v2, · · · , vN }, and an undirected edge (vi, vj) ∈ E ⊂ V ×V in graph G implies that the agents
i and j can measure the states of each other, which is denoted by xi = [xi(1), xi(2), · · · , xi(M )](cid:62) ∈ RM
for agent i ∈ D. Agents i and j are neighboring or adjacent agents, if there exists a communication

channel between them, and the index set of agents neighboring to agent i is denoted by Ni with
](cid:62) ∈ RM ·| ¯Ni| to denote the joint state of
¯Ni = Ni ∪ {i}. We use column vectors ¯xi = [x(cid:62)
agent i and its adjacent agents Ni, which together group up the factorial subsystem ¯Ni of agent i,
n ](cid:62) ∈ RM ·N denotes the global states of MAS. Figure 1 shows a MAS and
1 , x(cid:62)
and x = [x(cid:62)
all its factorial subsystems.

2 , · · · , x(cid:62)

i , x(cid:62)

j∈Ni

To optimize the correlations between neighboring agents while not intensifying the communica-

tion and computational complexities of the network or each agent, our paper proposes a distributed

planning and execution framework, as a trade-oﬀ scheme between the easiness of implementation

and optimality of performance, for multi-agent LSOC problems. Under this distributed framework,

the local control action ui of agent i ∈ N is computed by solving the local LSOC problem deﬁned
in factorial subsystem ¯Ni. Instead of requiring the cost functions fully factorized over agents [31]
or the knowledge of global states [25, 30], joint cost functions are permitted in every factorial

4

subsystem, which captures the correlations and cooperation between neighboring agents, and the

local control action ui only relies on the local observation ¯xi of agent i, which also simpliﬁes the
structure of communication network. Meanwhile, the global computational complexity is no longer

exponential with respect to the total amount of agents in network |D|, but becomes linear with

respect to it in general, and the computational complexity of local agent i is only related to the
number of agents in factorial subsystem ¯Ni. However, since the local control actions are computed
from the local observations with only partial information, the distributed LSOC law obtained under

this framework is usually a sub-optimal solution, despite that it coincides with the global optimal

solution when the communication network is fully connected. More explanations and discussions

on this will be given at the end of this section. Before that, we ﬁrst reformulate the discrete-time

and continuous-time LSOC problems from the single-agent scenario to a multi-agent setting that

is compatible with our distributed framework.

Figure 1: An example of MAS and factorial subsystems. MAS G with four agents can be partitioned into
four factorial subsystems ¯N1, ¯N2, ¯N3, and ¯N4, and each subsystem is assumed to be fully connected.

2.2 Discrete-Time Dynamics

Discrete-time SOC for single-agent systems, also known as the single-agent MDP, is brieﬂy

reviewed and then generalized to the networked MAS scenario. We consider the single-agent MDPs

with ﬁnite state space and continuous control space in the ﬁrst-exit setting. For a single agent i ∈ D,
the state variable xi belongs to a ﬁnite set Si = {si
2, · · · } = Ii ∪ Bi, which may be generated
from an inﬁnite-dimensional state problem by an appropriate coding scheme [34]. Ii ⊂ Si denotes
the set of interior states of agent i, and Bi ⊂ Si denotes the set of boundary states. Without
communication and interference from other agents, the passive dynamics of agent i follows the

1, si

probability distribution

x(cid:48)
i ∼ pi(·|xi),

5

1233412341231234111:311:3111:(,)xxuucxuN1:4:xxG443:443:4444:(,)xxuucxuN221:321:3222:(,)xxuucxuN331:431:4333:(,)xxuucxuNi ∈ Si, and pi(x(cid:48)

where xi, x(cid:48)
i. When taking
control action ui at state xi, the controlled dynamics of agent i is described by the distribution
mapping

i|xi) denotes the transition probability from state xi to x(cid:48)

x(cid:48)
i ∼ ui(·|xi) = pi(·|xi, ui),

(1)

i|xi) or pi(x(cid:48)

i|xi, ui) denotes the transition probability from state xi to state x(cid:48)

where ui(x(cid:48)
i subject to
control ui that belongs to a continuous space. We require that ui(x(cid:48)
i|xi) = 0,
to prevent the direct transitions to the goal states. When xi ∈ Ii ⊂ Si, the immediate or running
cost function of LSMDPs is designed as:

i|xi) = 0, whenever pi(x(cid:48)

ci(xi, ui) = qi(xi) + KL(ui(·|xi) (cid:107) pi(·|xi)),

(2)

where the state cost qi(xi) can be an arbitrary function encoding how (un)desirable diﬀerent states
are, and the KL-divergence∗ measures the cost of control actions. When xi ∈ Bi ⊂ Si, the ﬁnal
cost function is deﬁned as φi(xi) ≥ 0. The cost-to-go function of ﬁrst-exit problem starting at
state-time pair (xt0

i , t0) is deﬁned as

J ui
i (xt0

i , t0) = Eui

(cid:20)

φi(xtf

i ) +

tf −1
(cid:88)

τ =t0

ci(xτ

i , uτ
i )

(cid:21)
,

(3)

i , uτ

i ) is the state-action pair of agent i at time step τ , xtf

where (xτ
is the terminal or exit state, and
the expectation Eui is taken with respect to the probability measure under which xi satisﬁes (1)
given the control law ui = (ut0
) and initial condition xt0
i . The objective of discrete-
time stochastic optimal control problem is to ﬁnd the optimal policy u∗
i and value functions Vi(xi)
by solving the Bellman equation

i , · · · , utf −1

i , ut1

i

i

Vi(xi) = min
ui

(cid:110)
ci(xi, ui) + Ex(cid:48)

i∼ui(·|xi)[Vi(x(cid:48)
i)]

(cid:111)

,

(4)

where the value function Vi(xi) is deﬁned as the expected cumulative cost for starting at state xi
and acting optimally thereafter, i.e. Vi(xi) = minui J ui

i (xt0

i , t0).

Based on the formulations of single-agent LSMDP and augmented dynamics, we introduce a

multi-agent LSMDP formulation subject to the distributed framework of the factorial subsystem.

For simplicity, we assume that the passive dynamics of agents in MAS are homogeneous and

mutually independent, i.e. agents without control are governed by identical dynamics and do not

∗The KL-divergence (relative entropy) between two discrete probability mass functions p(x) and q(x) is deﬁned as

KL(p (cid:107) q) =

(cid:88)

x∈X

p(x) log[p(x)/q(x)],

which has an absolute minimum 0 when p(x) = q(x), ∀x ∈ X . For two continuous probability density functions p(x)

and q(x), the KL-divergence is deﬁned as

KL(p (cid:107) q) =

(cid:90)

x∈χ

p(x) log[p(x)/q(x)]dx.

6

interfere or collide with each other. This assumption is also posited in many previous papers on

multi-agent LSOC or distributed control [11, 25, 31, 35]. Since agent i ∈ D can only observe the
states of neighboring agents Ni, we are interested in the subsystem ¯Ni = Ni ∪ {i} when computing
the control law of agent i. Hence, the autonomous dynamics of subsystem ¯Ni follow the distribution
mapping

¯x(cid:48)
i ∼ ¯pi(·|¯xi) =

(cid:89)

pj(·|xj),

(5)

j∈ ¯Ni
1, ¯si

i ∈ (cid:81)

j∈ ¯Ni

Sj = ¯Si = {¯si

2, · · · } = ¯Ii ∪ ¯Bi, and distribution information
where the joint state ¯xi, ¯x(cid:48)
¯pi(·|¯xi) are generally only accessible to agent i. Similarly, the global state of all agents x(cid:48) ∼ p(·|x) =
(cid:81)N
i=1 pi(·|xi), which is usually not available to a local agent i unless it can receive the information
from all other agents D\{i}, e.g. agent 3 in Figure 1. Since the local control action ui only
relies on the local observation of agent i, we assume that the joint posterior state ¯x(cid:48)
i of subsystem
¯Ni is exclusively determined by the joint prior state ¯xi and joint control ¯ui when computing the
optimal control actions in subsystem ¯Ni. More intuitively, under this assumption, the local LSOC
algorithm in subsystem ¯Ni only requires the measurement of joint state ¯xi and treats subsystem
¯Ni as a complete connected network, as shown in Figure 1. When each agent in ¯Ni samples their
control action independently, the joint controlled dynamics of the factorial subsystem ¯Ni satisﬁes

¯x(cid:48)
i ∼ ¯ui(·|¯xi) =

uj(·|¯xi) =

(cid:89)

j∈ ¯Ni

(cid:89)

j∈ ¯Ni

pj(·|xi, xj∈Ni, uj),

(6)

where the joint state ¯xi and joint distribution ¯ui(·|¯xi) are only accessible to agent i in general. Once
we ﬁgure out the joint control distribution ¯ui(·|¯xi) for subsystem ¯Ni, the local control distribution
ui(·|¯xi) of agent i can be retrieved by calculating the marginal distribution. The joint immediate
cost function for subsystem ¯Ni when ¯xi ∈ ¯Ii is deﬁned as follows

ci(¯xi, ¯ui) = qi(¯xi) + KL(¯ui(·|¯xi) (cid:107) ¯pi(·|¯xi)) = qi(¯xi) +

KL(uj(·|¯xi) (cid:107) pj(·|xj)),

(7)

(cid:88)

j∈ ¯Ni

where the state cost qi(¯xi) can be an arbitrary function of joint state ¯xi, i.e. a constant or the
norm of disagreement vector, and the second equality follows from (5) and (6), which implies that
the joint control cost is the cumulative sum of the local control costs. When ¯xi ∈ ¯Bi, the exit cost
function φi(¯xi) = (cid:80)
j > 0 is a weight measuring the priority of assignment
on agent j. In order to improve the success rate in application, it is preferable to assign ωi
i as the
largest weight when computing the control distribution ¯ui in subsystem ¯Ni. Subsequently, the joint
cost-to-go function of ﬁrst-exit problem in subsystem ¯Ni becomes

j · φj(xj), where ωi
ωi

j∈ ¯Ni

J ¯ui
i (¯xt0

i , t0) = E¯ui

(cid:20)
φi(¯xtf

i ) +

tf −1
(cid:88)

τ =t0

ci(¯xτ

i , ¯uτ
i )

(cid:21)
.

(8)

Some abuses of notations occur when we deﬁne the cost functions ci and φi, cost-to-go function Ji,
and value function Vi in single-agent setting and the factorial subsystem; one can diﬀerentiate the

7

diﬀerent settings from the arguments of these functions. Derived from the single-agent Bellman
equation (4), the joint optimal control action ¯u∗
i (·|¯xi) subject to the joint cost function (7) can be
solved from the following joint Bellman equation in subsystem ¯Ni

Vi(¯xi) = min
¯ui

(cid:110)
ci(¯xi, ¯ui) + E¯x(cid:48)

i∼¯ui(·|¯xi)[Vi(¯x(cid:48)
i)]

(cid:111)

,

(9)

where Vi(¯xi) is the (joint) value function of joint state ¯xi. A linearization method as well as a
parallel programming method for solving (9) will be discussed in Section 3.

2.3 Continuous-Time Dynamics

For continuous-time LSOC problems, we ﬁrst consider the dynamics of single agent i described

by the following Itˆo diﬀusion process

dxi = fi(xi, t)dt + Bi(xi)[ui(xi, t)dt + σidwi],

(10)

where xi ∈ RM is agent i’s state vector from an uncountable state space; fi(xi, t)+Bi(xi)·ui(xi, t) ∈
RM is the deterministic drift term with passive dynamics fi(xi, t), control matrix Bi(xi) ∈ RM ×P
and control action ui(xi, t) ∈ RP ; noise dwi ∈ RP is a vector of possibly correlated† Brownian
components with zero mean and unit rate of variance, and the positive semi-deﬁnite matrix σi ∈
RP ×P denotes the covariance of noise dwi. When xi ∈ Ii, the running cost function is deﬁned as

ci(xi, ui) = qi(xi) +

1
2

ui(xi, t)(cid:62)Riui(xi, t),

(11)

where qi(xi) ≥ 0 is the state-related cost, and u(cid:62)
Ri ∈ RP ×P being positive deﬁnite. When xtf
is the exit time. Hence, the cost-to-go function of ﬁrst-exit problem is deﬁned as

i Riui is the control-quadratic term with matrix
i ), where tf

i ∈ Bi, the terminal cost function is φi(xtf

J ui
i (xt

i, t) = Eui
xt
i,t

(cid:20)

φi(xtf

i ) +

(cid:90) tf

t

ci(xi(τ ), ui(τ )) dτ

,

(cid:21)

(12)

where the expectation is taken with respect to the probability measure under which xi is the solution
to (10) given the control law ui and initial condition xi(t). The value function is deﬁned as the
minimal cost-to-go function Vi(xi, t) = minui J ui
i (xt
i, t). Subject to the dynamics (10) and running
cost function (11), the optimal control action u∗
i can be solved from the following single-agent
stochastic Hamilton–Jacobi–Bellman (HJB) equation:

−∂tVi(xi, t) = min
ui

(cid:110)
ci(xi, ui) + [fi(xi, t) + Bi(xi)ui(xi, t)](cid:62) · ∇xiVi(xi, t)
(cid:104)
Bi(xi)σiσ(cid:62)

i Bi(xi)(cid:62) · ∇2

tr

+

1
2

xixiVi(xi, t)

(13)

(cid:105) (cid:111)
,

†When the components of d ˜wi = [d ˜wi,(1), · · · , d ˜wi,(P )](cid:62) are correlated and satisfy a multi-variate normal distribution
N (0, Σi), by using the Cholesky decomposition Σi = σiσ(cid:62)
i , we can rewrite d ˜wi = σidwi, where dwi is a vector of
Brownian components with zero drift and unit-variance rate.

8

where ∇xi and ∇2
xixi respectively refer to the gradient and Hessian matrix with ∇xiVi = [∂Vi/∂xi(1),
· · · , ∂Vi/∂xi(M )](cid:62) and elements [∇2
xixiVi]m,n = ∂2Vi/∂xi(m)∂xi(n). A few methods have been pro-
posed to solve the stochastic HJB in (13), such as the approximation methods via discrete-time

MDPs or eigenfunction [36] and path integral approaches [22, 31, 37].

Similar to the extension of discrete-time LSMDP from single-agent setting to MAS, the joint

continuous-time dynamics for factorial subsystem ¯Ni is described by

d¯xi = ¯fi(¯xi, t)dt + ¯Bi(¯xi) [¯ui(¯xi, t)dt + ¯σid ¯wi] ,

(14)

j∈Ni

i , dw(cid:62)

the joint control matrix is denoted by

where the joint passive dynamics vector is denoted by ¯fi(¯xi, t) = [fi(xi, t)(cid:62), fj∈Ni(xj, t)(cid:62)](cid:62) ∈
RM ·| ¯Ni|,
¯Bi(¯xi) = diag{Bi(xi), Bj∈Ni(xj)} ∈
RM ·| ¯Ni|×P ·| ¯Ni|, ¯ui(¯xi, t) = [ui(¯xi, t)(cid:62), uj∈Ni(¯xi, t)(cid:62)](cid:62) ∈ RP ·| ¯Ni| is the joint control action, d ¯wi =
](cid:62) ∈ RP ·| ¯Ni| is the joint noise vector, and the joint covariance matrix is denoted by
[dw(cid:62)
¯σi = diag{σi, σj∈Ni} ∈ RP ·| ¯Ni|×P ·| ¯Ni|. Analogous to the discrete-time scenario, we assume that the
passive dynamics of agents are homogeneous and mutually independent, and for the local planning
algorithm on agent i or subsystem ¯Ni, which computes the local control action ui(¯xi, t) for agent i
and joint control action ¯ui(¯xi, t) in subsystem ¯Ni, the evolution of joint state ¯xi only depends on
the current values of ¯xi and joint control ¯ui(¯xi, t). When ¯xi ∈ ¯Ii, the joint immediate cost function
for subsystem ¯Ni is deﬁned as

ci(¯xi, ¯ui) = qi(¯xi) +

1
2

¯ui(¯xi, t)(cid:62) ¯Ri ¯ui(¯xi, t),

(15)

where the state-related cost qi(¯xi) can be an arbitrary function measuring the (un)desirability of dif-
¯Ri ¯ui is the control-quadratic term with matrix ¯Ri ∈ RP ·| ¯Ni|×P ·| ¯Ni|
ferent joint states ¯xi ∈ ¯Si, and ¯u(cid:62)
i
being positive deﬁnite. When ¯Ri = diag{Ri, Rj∈Ni} with Ri and Rj deﬁned in (11), the joint con-
u(cid:62)
trol cost term in (15) satisﬁes ¯u(cid:62)
j Rjuj, which is symmetric with respect to the
i
relationship of discrete-time control costs in (7). When ¯xi ∈ ¯Bi, the terminal cost function is deﬁned
as φi(¯xi) = (cid:80)
j > 0 is the weight measuring the priority of assignment
on agent j, and we let the weight ωi
to improve the success rate.
Compared with the cost functions fully factorized over agents, the joint cost functions in (15) can

i dominate other weights ωi

j · φj(xj), where ωi
ωi

¯Ri ¯ui = (cid:80)

j∈ ¯Ni

j∈ ¯Ni

j∈Ni

gauge and facilitate the correlation and cooperation between neighboring agents. Subsequently, the
joint cost-to-go function of ﬁrst-exit problem in subsystem ¯Ni is deﬁned as

J ¯ui
i (¯xt

i, t) = E¯ui
¯xt
i,t

(cid:20)

φi(¯xtf

i ) +

(cid:90) tf

t

ci(¯xi(τ ), ¯ui(τ )) dτ

.

(cid:21)

Let the (joint) value function Vi(¯xi, t) be the minimal cost-to-go function, i.e. Vi(¯xi, t) = min¯ui J ¯ui
We can then compute the joint optimal control action ¯u∗
joint optimality equation

i (¯xt
i of subsystem ¯Ni by solving the following

i, t).

Vi(¯xi, t) = min
¯ui

E¯ui
¯xt
i,t

(cid:20)

φi(¯xtf

i ) +

(cid:90) tf

t

ci(¯xi(τ ), ¯ui(τ )) dτ

.

(cid:21)

(16)

9

A distributed path integral control algorithm and a distributed REPS algorithm for solving (16)

will be respectively discussed in Section 3. Discussion on the relationship between discrete-time

and continuous-time LSOC dynamics can be found in [16, 38].

Remark 1. Although each agent i ∈ D under this framework acts optimally to minimize a joint
cost-to-go function deﬁned in their subsystem ¯Ni, the distributed control law obtained from solving
the local problem (9) or (16) is still a sub-optimal solution unless the communication network G

is fully connected. Two main reasons account for this sub-optimality. First, when solving (9) or
(16) for the joint (or local) optimal control ¯u∗
i (or u∗
i ), we ignore the connections of agents outside
the subsystem ¯Ni and assume that the evolution of joint state ¯xi only relies on the current values
of ¯xi and joint control ¯ui. This simpliﬁcation is reasonable and almost accurate for the central
agent i of subsystem ¯Ni, but not for the non-central agents j ∈ Ni, which are usually adjacent to
other agents in Nj\Ni. Therefore, the local optimal control actions ¯u∗
j of other agents j ∈ D\{i}
are respectively computed from their own subsystems ¯Nj, which may contradict the joint optimal
control ¯ui solved in subsystem ¯Ni and result in a sub-optimal solution. Similar conﬂicts widely
exist in the distributed control and optimization problems subject to limited communication and

partial observation, and some serious and heuristic studies on the global- and sub-optimality of

distributed subsystems have been conducted in [11, 39–41]. We will not dive into those technical

details in this paper, as we believe that the executability of a sub-optimal plan with moderate

communication and computational complexities should outweigh the performance gain of a global-

optimal but computationally intractable plan in practice. In this regard, the distributed framework

built upon factorial subsystems, which captures the correlations between neighboring agents while

ignoring the further connections outside subsystems, provides a trade-oﬀ alternative between the

optimality and complexity and is analogous to the structured prediction framework in supervised

learning.

3 Distributed Linearly-Solvable Optimal Control

Subject to the multi-agent LSOC problems formulated in Section 2, the linearization meth-

ods and distributed algorithms for solving the joint discrete-time Bellman equation (9) and joint

continuous-time optimality equation (16) are discussed in this section.

3.1 Discrete-Time Systems

We ﬁrst consider the discrete-time MAS with dynamics (6) and immediate cost function (7),

which give the joint Bellman equation (9) in subsystem ¯Ni

Vi(¯xi) = min
¯ui

(cid:110)
ci(¯xi, ¯ui) + E¯x(cid:48)

i∼¯ui(·|¯xi)[Vi(¯x(cid:48)
i)]

(cid:111)

.

In order to compute the value function Vi(¯xi) and optimal control action ¯u∗
i (·|¯xi) from equation (9),
an exponential or Cole-Hopf transformation is employed to linearize (9) into a system of linear

10

equations, which can be cast into a decentralized programming and solved in parallel. Local
optimal control distribution u∗
i (·|¯xi) that depends on the local observation of agent i is then derived
by marginalizing the joint control distribution ¯u∗

i (·|¯xi).

A. Linearization of Joint Bellman Equation

Motivated by the exponential transformation employed in [38] for single-agent system, we deﬁne

the desirability function Zi(¯xi) for joint state ¯xi ∈ ¯Ii in subsystem ¯Ni as

Zi(¯xi) = exp[−Vi(¯xi)],

(17)

which implies that the desirability function Zi(¯xi) is negatively correlated with the value function
Vi(¯xi), and the value function can also be written conversely as a logarithm of the desirability
function, Vi(¯xi) = log 1/Zi(¯xi). For boundary states ¯xi ∈ ¯Bi, the desirability functions are deﬁned
as Zi(¯xi) = exp[−φi(¯xi)]. Based on the transformation (17), a linearized joint Bellman equation (9)
along with the joint optimal control distribution is presented in Theorem 1.

Theorem 1. With exponential transformation (17), the joint Bellman equation (9) for subsystem
¯Ni is equivalent to the following linear equation with respect to the desirability function

Zi(¯xi) = exp(−qi(¯xi)) ·

(cid:88)

¯x(cid:48)
i

¯pi(¯x(cid:48)

i|¯xi)Zi(¯x(cid:48)

i),

(18)

where qi(¯xi) is the state-related cost deﬁned in (15), and ¯pi(¯x(cid:48)
passive dynamics in (5). The joint optimal control action ¯u∗

i|¯xi) is the transition probability of

i (·|¯xi) solving (9) satisﬁes

¯u∗
i (·|¯xi) =

¯pi(·|¯xi)Zi(·)
¯pi(¯x(cid:48)

i|¯xi)Zi(¯x(cid:48)
i)

(cid:80)

¯x(cid:48)
i

,

(19)

where ¯u∗

i (¯x(cid:48)

i|¯xi) is the transition probability from ¯xi to ¯x(cid:48)

i in controlled dynamics (6).

Proof. See Appendix A for the proof.

Joint optimal control action ¯u∗

i|¯xi) in (19) only relies on the joint state ¯xi of subsystem ¯Ni, i.e.
local observation of agent i. To execute this control action (19), we still need to ﬁgure out the values

i (¯x(cid:48)

of desirability functions or value functions, which can be solved from (18). A conventional approach

for solving (18), which was adopted in [16, 18, 38] for single-agent LSMDP, is to rewrite (18) as a

recursive formula and approximate the solution by iterations. We can approximate the desirability
functions of interior states ¯xi ∈ ¯Ii by recursively executing the following update law

ZI = ΘZI + ΩZB,

(20)

where ZI and ZB are respectively the desirability vectors of interior states and boundary states; the
diagonal matrix Θ = diag{exp(−qI)} · PII with qI denoting the state-related cost of interior states,
PII = [pmn] denoting the transition probability matrix between interior states, and pmn = ¯pi(¯x(cid:48)
i =

11

n, ¯si

m) for ¯si

m ∈ ¯Ii; the matrix Ω = diag{exp(−qI)} · PIB with PIB = [pmn] denoting the
n | ¯xi = ¯si
¯si
transition probability matrix from interior states to boundary states, and pmn = ¯pi(¯x(cid:48)
n | ¯xi =
n ∈ ¯Bi. Assigning an initial value to the desirability vector ZI, the recursive
m) for ¯si
¯si
formula (20) is guaranteed to converge to a unique solution, since the spectral radius of matrix Θ is

m ∈ ¯Ii and ¯si

i = ¯si

less than 1. More detailed convergence analysis on this iterative solver of LSMDPs has been given

in [16]. However, this centralized solver is ineﬃcient when dealing with the MAS problems with

high-dimensional state space, which requires the development of a distributed solver that exploits

the resource of network and expedites the computation.

B. Distributed Planning Algorithm

While most of the distributed SOC algorithms are executed by local agents in a decentralized

approach, a great number of these algorithms still demand a centralized solver in planning phase,

which becomes a bottleneck for their implementations when the amount of agents scales up [8]. For

a fully connected MAS with N agents, when each agent has |I| interior states, the dimension of the
vector ZI in (20) is |I|N , and as the number of agents N grows up, it will become more intractable
for a central computation unit to store all the data and execute all the computation required

by (20) due to the curse of dimensionality. Although the subsystem-based distributed framework

can alleviate this problem by demanding a less complex network and making the dimension and

computational complexity only related to the sizes of factorial subsystems, it is still preferable

to utilize the resources of MAS by distributing the data and computational task of (20) to each
local agent in ¯Ni, instead of relying on a central planning agent. Hence, we rewrite the linear
equation (20) in the following form

(I − Θ)ZI = ΩZB,

(21)

and formulate (21) into a parallel programming problem. In order to solve the desirability vector
ZI from (21) via a distributed approach, each agent in subsystem ¯Ni only needs to know (store)
a subset (rows) of the partitioned matrix [I − Θ, ΩZB]. Subject to the equality constraints laid
by its portion of coeﬃcients, agent j ∈ ¯Ni ﬁrst initializes its own version of solution Z(0)
I,j to (21).
Every agent has access to the solutions of its neighboring agents, and the central agent i can
access the solutions of agents in Ni. A consensus of ZI in (21) can be reached among ZI,j∈ ¯Ni
when implementing the following synchronous distributed algorithm on each computational agent
in ¯Ni [42]:

,

I,j = Z(n)
Z(n+1)

I,j − Pj


Z(n)

I,j −

1
dj



(cid:88)

Z(n)
I,k

 ,

k∈Nj ∩ ¯Ni

(22)

where Pj is the orthogonal projection matrix on the kernel of [I − Θ]j, rows of the matrix I − Θ
stored in agent j; dj = |Nj ∩ ¯Ni| is the amount of neighboring agents of agent j in subsystem ¯Ni;
and n is the index of update iteration. An asynchronous distributed algorithm [43], which does not

require agents to concurrently update their solution ZI,i, can also be invoked to solve (21). Under

12

these distributed algorithms, diﬀerent versions of solution ZI,j are exchanged across the network,
and the requirements of data storage and computation can be allocated evenly to each agent in

network, which improve the overall eﬃciency of algorithms. Meanwhile, these fully parallelized

planning algorithms can be optimized and boosted further by naturally incorporating some parallel

computing and Internet of Things (IoT) techniques, such as edge computing [44]. Nonetheless, for

MASs with massive population, i.e. N → ∞, most of the control schemes and algorithms introduced

in this paper will become fruitless, and we may resort to the mean-ﬁeld theory [9, 45, 46], which

describes MASs by probability density model rather than connected graph model.

C. Local Control Action

After we ﬁgure out the desirability functions Zi(¯xi) for all the joint states ¯xi ∈ ¯Ii ∪ ¯Bi in
i|¯xi) for central agent i is derived by

subsystem ¯Ni, the local optimal control distribution u∗
calculating the marginal distribution of ¯u∗

i (x(cid:48)

i (¯x(cid:48)

i|¯xi)

i (x(cid:48)
u∗

i|¯xi) =

(cid:88)

j∈Ni

i (x(cid:48)
¯u∗

i, x(cid:48)

j∈Ni|¯xi),

i (¯x(cid:48)

i|¯xi) and local distribution u∗

where both the joint distribution ¯u∗
i|¯xi) rely on the local obser-
i (x(cid:48)
vation of central agent i. By sampling control action from marginal distribution u∗
i|¯xi), agent i
behaves optimally to minimize the joint cost-to-go function (8) deﬁned in subsystem ¯Ni, and the
local optimal control distribution u∗
k|¯xk) of other agents k ∈ D\{i} in network G can be derived
by repeating the preceding procedures in subsystems ¯Nk. The procedures of multi-agent LSMDP
algorithm are summarized as Algorithm 1 in Appendix E.

k(x(cid:48)

i (x(cid:48)

3.2 Continuous-Time Systems

We now consider the LSOC for continuous-time MASs subject to joint dynamics (14) and joint

immediate cost function (15), which can be formulated into the joint optimality equation (16) as

follows

Vi(¯xi, t) = min
¯ui

E¯ui
¯xt
i,t

(cid:20)
φi(¯xtf

i ) +

(cid:90) tf

t

ci(¯xi(τ ), ¯ui(τ )) dτ

.

(cid:21)

In order to solve (16) and derive the local optimal control action u∗
i (¯xi, t) for agent i, we ﬁrst cast
the joint optimality equation (16) into a joint stochastic HJB equation that gives an analytic form
for joint optimal control action ¯u∗
i (¯xi, t). To solve for the value function, by resorting to the Cole-
Hopf transformation, the stochastic HJB equation is linearized into a partial diﬀerential equation

(PDE) with respect to desirability function. Feynman-Kac formula is then invoked to formulate

the solution of the linearized PDE and joint optimal control action as the path integral formulae

forward in time, which are later approximated respectively by a distributed Monte Carlo (MC)

sampling method and a sample-eﬃcient distributed REPS algorithm.

13

A. Linearization of Joint Optimality Equation

Similar to the transformation (17) for discrete-time systems, we adopt the following Cole-Hopf

or exponential transformation in continuous-time systems

Z(¯xi, t) = exp[−Vi(¯xi, t)/λi],

(23)

where λi ∈ R is a scalar, and Z(¯xi, t) is the desirability function of joint state ¯xi at time t.
Conversely, we also have Vi(¯xi, t) = λi log Z(¯xi, t) from (23). In the following theorem, we convert
the optimality equation (16) into a joint stochastic HJB equation, which reveals an analytic form
of joint optimal control action ¯u∗
i (¯xi, t), and linearize the HJB equation into a linear PDE that has
a closed-form solution for the desirability function.

Theorem 2. Subject to the joint dynamics (14) and immediate cost function (15), the joint
optimality equation (16) in subsystem ¯Ni is equivalent to the joint stochastic HJB equation

−∂tVi(¯xi, t) = min
¯ui

E¯ui

¯xi,t

(cid:20) (cid:88)

j∈ ¯Ni

[fj(xj, t) + Bj(xj)uj(¯xi, t)](cid:62) · ∇xj Vi(¯xi, t) + qi(¯xi, t)

(24)

+

1
2

¯ui(¯xi, t)(cid:62) ¯Ri ¯ui(¯xi, t) +

1
2

(cid:88)

(cid:16)

tr

j∈ ¯Ni

Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62) · ∇xj xj Vi(¯xi, t)

(cid:17) (cid:21)
,

with boundary condition Vi(¯xi, tf ) = φi(¯xi), and the optimum of (24) can be attained with the
joint optimal control action

i (¯xi, t) = − ¯R−1
¯u∗

i

¯Bi(¯xi)(cid:62)∇¯xiVi(¯xi, t).

(25)

Subject to the transformation (23), control action (25) and condition ¯Ri = (¯σi¯σ(cid:62)
stochastic HJB equation (24) can be linearized as

i /λi)−1, the joint

∂tZi(¯xi, t) =

(cid:20) qi(¯xi, t)
λi

(cid:88)

−

j∈ ¯Ni

fj(xj, t)(cid:62)∇xj −

1
2

(cid:88)

(cid:16)

tr

j∈ ¯Ni

Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62)∇xj xj

(cid:17) (cid:21)

Zi(¯xi, t) (26)

with boundary condition Zi(¯xi, tf ) = exp[−φi(¯xi)/λi], which has a solution

Zi(¯xi, t) = E¯xi,t

(cid:20)

(cid:18)

exp

−

1
λi

φi(¯ytf

i ) −

1
λi

(cid:90) tf

t

(cid:19)(cid:21)

qi(¯yi, τ ) dτ

,

(27)

where the diﬀusion process ¯y(t) satisﬁes the uncontrolled dynamics d¯yi(τ ) = ¯fi(¯yi, τ )dτ + ¯Bi(¯yi)¯σi ·
d ¯wi(τ ) with initial condition ¯yi(t) = ¯xi(t).

Proof. See Appendix B for the proof.

Based on the transformation (23), the gradient of value function satisﬁes ∇¯xiVi(¯xi, t) = −λi ·

∇¯xiZi(¯xi, t)/Zi(¯xi, t). Hence, the joint optimal control action (25) can be rewritten as

i (¯xi, t) = λi ¯R−1
¯u∗

i

¯B(cid:62)

i (¯xi) ·

∇¯xiZi(¯xi, t)
Zi(¯xi, t)

= ¯σi¯σ(cid:62)
i

¯Bi(¯xi)(cid:62) ·

∇¯xiZi(¯xi, t)
Zi(¯xi, t)

,

(28)

14

where the second equality follows from the condition ¯Ri = (¯σi¯σ(cid:62)
i /λi)−1. Meanwhile, by virtue of
Feynman-Kac formula, the stochastic HJB equation (24) that must be solved backward in time can

now be solved by an expectation of diﬀusion process evolving forward in time. While a closed-form
solution of the desirability function Zi(¯xi, t) is given in (27), the expectation E¯xi,t(·) is deﬁned on the
sample space consisting of all possible uncontrolled trajectories initialized at (¯xi, t), which makes
this expectation intractable to compute. A common approach in statistical physics and quantum

mechanics is to ﬁrst formulate this expectation as a path integral [37,47,48], and then approximate

the integral or optimal control action with various techniques, such as MC sampling [37] and policy

improvement [22]. In the following subsections, we ﬁrst formulate the desirability function (26) and

control action (28) as path integrals and then approximate them with a distributed MC sampling

algorithm and a sample-eﬃcient distributed REPS algorithm, respectively.

B. Path Integral Formulation

i(n), ¯x(cid:62)

Before we show a path integral formula for the desirability function Zi(¯xi, t) in (27), some
manipulations on joint dynamics (14) are performed to avoid singularity problems [22]. By rear-
ranging the components of joint states ¯xi in (14), the joint state vector ¯xi in subsystem ¯Ni can be
i(d)](cid:62), where ¯xi(n) ∈ RU ·| ¯Ni| and ¯xi(d) ∈ ¯RD·| ¯Ni| respectively indicate the joint
partitioned as [¯x(cid:62)
non-directly actuated states and joint directly actuated states of subsystem ¯Ni; U and D denote
the dimensions of non-directly actuated states and directly actuated states for a single agent. Con-
sequently, the joint passive dynamics term ¯fi(¯xi, t) and the joint control transition matrix ¯Bi(¯xi) in
i(n), ¯f (cid:62)
(14) are partitioned as [ ¯f (cid:62)
i(d)(¯xi)](cid:62), respectively. Hence, the joint dynamics (14)
can be rewritten in a partitioned vector form as follows

i(d)](cid:62) and [0, ¯B(cid:62)

(cid:32)

d¯xi(n)
d¯xi(d)

(cid:33)

=

(cid:33)

(cid:32) ¯fi(n)(¯xi, t)
¯fi(d)(¯xi, t)

dt +

(cid:32)

0
¯Bi(d)(¯xi)

(cid:33)

[¯ui(¯xi, t)dt + ¯σid ¯wi] .

(29)

With the partitioned dynamics (29), the path integral formulae for the desirability function (27)

and joint optimal control action (28) are given in Proposition 3.

Proposition 3. Partition the time interval from t to tf into K intervals of equal length ε > 0, t =
t0 < t1 < · · · < tK = tf , and let the trajectory variable ¯x(k)
i(d) ](cid:62) denote the segments
of joint uncontrolled trajectories on time interval [tk−1, tk), governed by joint dynamics (29) with
¯ui(¯xi, t) = 0 and initial condition ¯xi(t) = ¯x(0)
. The desirability function (27) in subsystem ¯Ni can
then be reformulated as a path integral

i = [¯x(k)(cid:62)

i(n) , ¯x(k)(cid:62)

i

Zi(¯xi, t) = lim
ε↓0

(cid:90)

(cid:16)

exp

− ˜Sε,λi
i

(¯x(0)
i

(cid:17)
, ¯(cid:96)i, t0) − KD| ¯Ni|/2 · log(2πλiε)

d¯(cid:96)i,

(30)

where the integral is over path variable ¯(cid:96)i = (¯x(1)
initialized at (¯xi, t), and the generalized path value

i

, · · · , ¯x(K)

i

), i.e. set of all uncontrolled trajectories

˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) =

)

φi(¯x(K)
i
λi

+

ε
λi

K−1
(cid:88)

k=0

qi(¯x(k)
i

, tk) +

1
2

K−1
(cid:88)

k=0

log det(H (k)

i

)

(31)

15

+

ε
2λi

K−1
(cid:88)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i(d) − ¯x(k)
¯x(k+1)
ε

i(d)

− ¯fi(d)(¯x(k)

i

, tk)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
H (k)
i

(cid:17)−1

i = ¯Bi(d)(¯x(k)

¯Bi(d)(¯x(k)
with H (k)
i
control action in subsystem ¯Ni can be reformulated as a path integral

)(cid:62) = λi ¯Bi(d)(¯x(k)

¯Bi(d)(¯x(k)

)¯σi¯σ(cid:62)
i

) ¯R−1
i

i

i

i

)(cid:62). Hence, the joint optimal

i (¯xi, t) = λi ¯R−1
¯u∗

i

¯Bi(d)(¯xi)(cid:62) · lim
ε↓0

(cid:90)

i (¯(cid:96)i|¯x(0)
˜p∗

i

, t0) · ˜ui(¯x(0)

i

, ¯(cid:96)i, t0) d¯(cid:96)i,

where

i (¯(cid:96)i|¯x(0)
˜p∗

i

, t0) =

is the optimal path distribution, and

exp(− ˜Sε,λi
i
(cid:82) exp(− ˜Sε,λi

(¯x(0)
i
(¯x(0)
i

, ¯(cid:96)i, t0))
, ¯(cid:96)i, t0)) d¯(cid:96)i

i

(32)

(33)

˜ui(¯x(0)
i

, ¯(cid:96)i, t0) = −

ε
λi

∇

¯x(0)
i(d)

qi(¯x(0)
i

, t0) +

(cid:17)−1

(cid:16)

H (0)
i





i(d) − ¯x(0)
¯x(1)
ε

i(d)



− ¯fi(d)(¯x(0)
i

, t0)



(34)

is the initial control variable.

Proof. See Appendix C for the proof.

While Proposition 3 gives the path integral formulae for desirability (value) function Zi(¯xi, t) and
joint optimal control action ¯u∗
i (¯xi, t), one of the challenges when implementing these formulae is the
approximation of optimal path distribution (33) and integrals in (30) and (32), since these integrals
are deﬁned on the set of all possible uncontrolled trajectories initialized at (¯xi, t) or (¯x(0)
, t0), which
are intractable to exhaust and might become computationally expensive to sample as the state

i

dimension and the amount of agents scale up. An intuitive solution is to formulate this problem as
a statistical inference problem and predict the optimal path distribution ˜p∗
, t0) via various
inference techniques, such as the easiest MC sampling [37] or Metropolis-Hastings sampling [31].
Provided a batch of uncontrolled trajectories Yi = {(¯x(0)
i )}y=1,··· ,Y and a ﬁxed ε, the Monte
Carlo estimation of optimal path distribution in (33) is

i (¯(cid:96)i|¯x(0)

, ¯(cid:96)[y]

i

i

i (¯(cid:96)[y]
˜p∗

i

|¯x(0)
i

, t0) ≈

(¯x(0)
exp(− ˜Sε,λi
i
y=1 exp(− ˜Sε,λi

, ¯(cid:96)[y]
i
(¯x(0)
i

, t0))
, ¯(cid:96)[y]
i

i

i

(cid:80)Y

, t0))

,

(35)

where ˜Sε,λi
estimation of joint optimal control action in (32) is

(¯x(0)
i

, ¯(cid:96)[y]
i

i

, t0) denotes the generalized path value (31) of trajectory (¯x(0)

, ¯(cid:96)[y]

i ), and the

i

i (¯xi, t) = λi ¯R−1
¯u∗

i

¯Bi(d)(xi)(cid:62) ·

Y
(cid:88)

y=1

i (¯(cid:96)[y]
˜p∗

i

|¯x(0)
i

, t0) · ˜ui(¯x(0)

i

, ¯(cid:96)[y]
i

, t0),

(36)

where ˜ui(¯x(0)
i ). To expedite the process
of sampling, the sampling tasks of Yi∈D can be distributed to diﬀerent agents in the network, i.e.

, t0) is the initial control of sample trajectory (¯x(0)

, ¯(cid:96)[y]
i

, ¯(cid:96)[y]

i

i

16

each agent j in MAS G only samples its local uncontrolled trajectories {(x(0)
j )}y=1,··· ,Y , and the
optimal path distributions of each subsystem ¯Ni are approximated via (35) after the central agent
i restores Yi by collecting the samples from its neighbors j ∈ ¯Ni. Meanwhile, the local trajectories
{(x(0)
j )}y=1,··· ,Y of agent j not only can be utilized by the local control algorithm of agent j
j
in subsystem ¯Nj, but also can be used by the local control algorithms of the neighboring agents
k ∈ Nj in subsystem ¯Nk, and the parallel computation of GPUs can further facilitate the sampling
processes by allowing each agent to concurrently generate multiple sample trajectories [25]. The

, (cid:96)[y]

, (cid:96)[y]

j

continuous-time multi-agent LSOC algorithm based on estimator (35) is summarized as Algorithm 2

in Appendix E.

However, since the aforementioned auxiliary techniques are still proposed on the basis of pure

sampling estimator (35), the total amount of samples required for a good approximation is not re-

duced, which will hinder the implementations of Proposition 3 and estimator (35) when the sample

trajectories are expensive to generate. Various investigations have been conducted to mitigate this

issue. For the sample-eﬃcient approximation of desirability (value) function Zi(¯xi, t), parameter-
ized desirability (value) function, Laplace approximation [37], approximation based on mean-ﬁeld

assumption [31], and a forward-backward framework based on Gaussian process can be used [17].

Meanwhile, it is more common in practice to only eﬃciently predict the optimal path distribution
i (¯(cid:96)i|¯x(0)
˜p∗
i (¯xi, t), while omitting the value of desirability function.
Numerous statistical inference algorithms, such as Bayesian inference and variational inference [49],

, t0) and optimal control action ¯u∗

i

can be adopted to predict the optimal path distribution, and diversiﬁed policy improvement and
policy search algorithms, such as PI2 [22] and REPS [23], can be employed to update the param-
eterized optimal control policy. Hence, in the following subsection, a sample-eﬃcient distributed

REPS algorithm is introduced to update the parameterized joint control policy for each subsystem

and MAS.

C. Relative Entropy Policy Search in MAS

Since the initial control ˜ui(¯x(0)

, ¯(cid:96)i, t0) can be readily calculated from sample trajectories and
network communication, we only need to determine the optimal path distribution ˜p∗
, t0)
before evaluating the joint optimal control action from (32). To approximate this distribution more

i (¯(cid:96)i|¯x(0)

i

i

eﬃciently, we extend the REPS algorithm [21,23] for path integral control in MAS. REPS is a model-

(¯u(k)
i

based algorithm with competitive convergence rate, whose objective is to search for a parametric
i (¯(cid:96)i|¯x(0)
policy π(k)
|¯x(k)
, t0) to approximate the optimal
i
i
path distribution ˜p∗
, t0). Compared with reinforcement learning algorithms and model-free
policy improvement or search algorithms, since REPS is built on the basis of model, the policy and

) that generates a path distribution ˜pπ
i (¯(cid:96)i|¯x(0)

i

i

trajectory generated from REPS can automatically satisfy the constraints of the model. Moreover,

for the MAS problems, REPS algorithm allows to consider a more general scenario when the
initial condition ¯xi(t) = ¯x(0)
, t0). REPS algorithm
alternates between two steps: a) Learning step that approximates the optimal path distribution

is stochastic and satisﬁes a distribution µi(¯x(0)

i

i

17

from samples generated by the current or initial policy, i.e. ˜pπ
b) Updating step that updates the parametric policy π(k)
|¯x(k)
i
distribution ˜pπ
i
approximate path distribution converge.

(¯u(k)
i
, t0) generated in step a). The algorithm terminates when the policy and

, t0), and
) to reproduce the desired path

, t0) → ˜p∗

i (¯(cid:96)i|¯x(0)

i

i

i

i (¯(cid:96)i|¯x(0)

i (¯(cid:96)i|¯x(0)

During the learning step, we approximate the joint optimal distribution ˜p∗

i (¯x(0)

i

, ¯(cid:96)i) = ˜p∗

i (¯(cid:96)i|¯x(0)

i

, t0)·

µi(¯x(0)
, t0) by minimizing the relative entropy (KL-divergence) between an approximate distribution
i
˜pi(¯x(0)
, ¯(cid:96)i) subject to a few constraints and a batch of sample trajectories Yi = {(¯x(0)
, ¯(cid:96)i) and ˜p∗
,
i
i )}y=1,··· ,Y generated by the current or initial policy related to old distribution ˜qi(¯x(0)
¯(cid:96)[y]
, ¯(cid:96)i) from
prior iteration. Similar to the distributed MC sampling method, the computation task of sample

i (¯x(0)

i

i

i

trajectories Yi can either be exclusively assigned to the central agent i or distributed among the
agents in subsystem ¯Ni and then collected by the central agent i. However, unlike the local uncon-
trolled trajectories {(x(0)
i )}y=1,··· ,Y in the MC sampling method, which can be interchangeably
used by the control algorithms of all agents in subsystem ¯Ni, since the trajectory sets Yi∈D of the
REPS approach are generated subject to diﬀerent policies π(k)
) for i ∈ D, the sample data
in Yi can only be speciﬁcally used by the local REPS algorithm in subsystem ¯Ni. We can then
update the approximate path distribution ˜pi(¯x(0)
, ¯(cid:96)i) by solving the following optimization problem

(u(k)
i

|¯x(k)
i

, (cid:96)[y]

i

i

i

arg max
˜pi(¯x(0)
i

,¯(cid:96)i)

s.t.

(cid:90)

(cid:90)

(cid:90)

(cid:90)

˜pi(¯x(0)
i

, ¯(cid:96)i)

(cid:104)
− ˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) − log ˜pi(¯x(0)

i

, ¯(cid:96)i)

(cid:105)

d¯x(0)

i d¯(cid:96)i,

(37)

˜pi(¯x(0)
i

, ¯(cid:96)i) log

˜pi(¯x(0)
i

, ¯(cid:96)i)ψ(¯x(0)

i d¯(cid:96)i ≤ δ,

d¯x(0)

˜pi(¯x(0)
, ¯(cid:96)i)
i
˜qi(¯x(0)
, ¯(cid:96)i)
i
i d¯(cid:96)i = ˆψ(0)
i ) d¯x(0)

i

,

˜pi(¯x(0)
i

, ¯(cid:96)i) d¯x(0)

i d¯(cid:96)i = 1,

where δ > 0 is a parameter conﬁning the update rate of approximate path distribution; ψi(¯x(0)
i )
is a state feature vector of the initial condition; and ˆψ(0)
is the expectation of state feature vector
subject to initial distribution µi(¯x(0)
to an optimization problem for path distribution ˜pi(¯(cid:96)i|¯x(0)
, t0), and the second constraint in (37)
can be neglected. The optimization problem (37) can be solved analytically with the method of

i ). When the initial state ¯x(0)

is deterministic, (37) degenerates

i

i

i

Lagrange multipliers, which gives

˜pi(¯x(0)
i

, ¯(cid:96)i) = exp

(cid:18)

−

1 + κ + η
1 + κ

(cid:19)

· ˜qi(¯x(0)

i

, ¯(cid:96)i)

κ
1+κ · exp

(cid:32)

−

˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) + θ(cid:62)ψi(¯x(0)
i )

(cid:33)

1 + κ

,

(38)

where κ and θ are the Lagrange multipliers that can be solved from the dual problem

arg min
κ,θ

g(κ, θ),

s.t. κ > 0,

(39)

18

with objective function

g(κ, θ) = κδ + θ(cid:62) ˆψ(0)

i + (1 + κ) · log

(cid:90)

˜qi(¯x(0)
i
(cid:32)

˜Sε,λi
i

× exp

−

, ¯(cid:96)i)

κ
1+κ

(40)

(¯x(0)
i

, ¯(cid:96)i, t0) + θ(cid:62)ψi(¯x(0)
i )

(cid:33)

1 + κ

d¯x(0)

i d¯(cid:96)i;

and the dual variable η can then be determined by the constraint obtained by substituting (39) into

the normalization (last) constraint in (37). The approximation of the dual function g(κ, θ) from
sample trajectories Yi, calculation of the old (or initial) path distribution ˜qi(¯(cid:96)i, ¯x(0)
i ) from the current
(or initial) policy π(k)
), and a brief interpretation on the formulation of optimization
i
problem (37) are explained in Appendix D.

(¯u(k)
i

|¯x(k)
i

i

i

i

ap-

|¯x(k)
i

(¯u(k)
i

) · π(k)
i

) d¯u(k)
i

i (¯x(k+1)

) = (cid:82) p(¯x(k+1)

such that the joint dis-

i
generated by policy π(k)

In the updating step of REPS algorithm, we update the policy π(k)
, ¯u(k)
|¯x(k)
|¯x(k)
tribution ˜pπ
i
i
i
proximates the path distribution ˜pi(¯(cid:96)i|¯x(0)
, t0) generated in the learning step (37) and ultimately
i (¯(cid:96)i|¯x(0)
converges to the optimal path distribution ˜p∗
, t0) in (33). More explanations on distribution
i
˜pπ
i and old distribution ˜qi can be found in (84) of Appendix D. In order to provide a concrete
example, we consider a set of parameterized time-dependent Gaussian policies that are linear in
states, i.e. π(k)
i + ˆb(k)
|¯x(k)
) at time step tk < tf , where
i
i
, ˆb(k)
i = (ˆa(k)
χ(k)
) are the policy parameters to be updated. For simplicity, one can also con-
i
struct a stationary Gaussian policy ˆπi(¯ui|¯xi, ˆai, ˆbi, ˆΣi) ∼ N (¯ui|ˆai ¯xi + ˆbi, ˆΣi), which was employed
in [50] and shares the same philosophy as the parameter average techniques discussed in [22, 31].
The parameters χ∗(k)
i (¯x(0)
˜pi(¯x(0)
, ¯(cid:96)i) and ˜pπ
i

can be updated by minimizing the relative entropy between

in policy π(k)
, ¯(cid:96)i), i.e.

(¯u(k)
i
, ˆΣ(k)
i

) ∼ N (¯u(k)

i ¯x(k)

, ˆΣ(k)
i

, ˆΣ(k)
i

, ˆa(k)
i

, ˆb(k)
i

|ˆa(k)

i

i

i

i

i

i

i

χ∗(k)
i = arg max

χ(k)
i

(cid:90)

˜pi(¯x(0)
i

, ¯(cid:96)i) · log π(k)

i

(¯u∗(k)
i

|¯x(k)
i

, χ(k)
i

) d¯x(0)

i d¯(cid:96)i,

(41)

i

is the optimal control action. More detailed interpretation on policy update as well as

where ¯u∗(k)
approximation of (41) from sample data Yi can be found in Appendix D. When implementing the
REPS algorithm introduced in this subsection, we initialize each iteration by generating a batch

of sample trajectories Yi from the current (or initial) policy, which was updated to restore the old
approximate path distribution ˜qi(¯x(0)
, ¯(cid:96)i) in the prior iteration. With these sample trajectories,
we update the path distribution through (37)-(40) and update the policy again through (41) till

i

convergence of the algorithm. This REPS-based continuous-time MAS control algorithm is also

summarized as Algorithm 3 in Appendix E.

D. Local Control Action

The preceding two distributed LSOC algorithms will return either a joint optimal control action
i ) for subsystem ¯Ni at joint state ¯xi(t).
i (¯xi, t) or a joint optimal control policy π∗
¯u∗
Similar to the treatment of distributed LSMDP in discrete-time MAS, only the central agent i

i (t)|¯xi(t), χ∗

i (¯u∗

19

selects or samples its local control action u∗
agents j ∈ D\{i} are guided by the control law, ¯u∗
own subsystems ¯Nj.

i (¯xi, t) from ¯u∗

i (¯xi, t) or π∗
j (¯u∗

i (¯u∗
j (t)|¯xj(t), χ∗

i (t)|¯xi(t), χ∗

i ), while other
j ), computed in their

j (¯xj, t) or π∗

|ˆa(k)

, ˆΣ(k)
i

i ¯x(k)

i + ˆb(k)

i (¯xi, t) can be directly selected from the joint control action ¯u∗

For distributed LSOC based on the MC sampling estimator (36), the local optimal control ac-
i (¯xi, t). For distributed LSOC
(¯u(k)
) ∼
i
) for each time step t = t0 < t1 < · · · < tK = tf per iteration.
), we

tion u∗
based on the REPS method, the recursive algorithm generates control policy π(k)
N (¯u(k)
i
When generating the trajectory set Yi from the current control policy π(k)
sample the local control action of agent i from the marginal distribution π(k)
(cid:80)

) =
). After the convergence of local REPS algorithm in subsystem ¯Ni,
i (¯xi, t) of the central agent i at state xi(t) can be sampled from

, u(k)
the local optimal control action u∗
the following marginal distribution

, χ(k)
i
, χ(k)
i

(¯u(k)
i
(u(k)
i

|¯x(k)
i
|¯x(k)
i

, χ(k)
i

, χ(k)
i

(u(k)
i

|¯x(k)
i

|¯x(k)
i

π(k)
i

j∈Ni

j∈Ni

i

i

i

i

π∗(0)
i

(u∗(0)
i

|¯x(0)
i

, χ∗(0)
i

) =

(cid:88)

j∈Ni

π∗(0)
i

(u∗(0)
i

, u∗(0)
j∈Ni

|¯x(0)
i

, χ∗(0)
i

),

(42)

which minimizes the joint cost-to-go function (12) in subsystem ¯Ni and only relies on the local
observation of agent i.

3.3 Generalization with Compositionality

While several accessory techniques have been introduced in Section 3.1 and Section 3.2 to facil-

itate the computation of optimal control law, we have to repeat all the aforementioned procedures

when a new optimal control task with diﬀerent terminal cost or preferred exit state is assigned. A

possible approach to solve this problem and generalize LSOC is to resort to contextual learning,

such as the contextual policy search algorithm [50], which adapts to diﬀerent terminal conditions

by introducing hyper-parameters and an additional layer of learning algorithm. However, this sup-

plementary learning algorithm will undoubtedly increase the complexity and computational task

for the entire control scheme. Thanks to the linearity of LSOC problems, a task-optimal controller

for a new (unlearned) task can also be constructed from existing (learned) controllers by utilizing

the compositionality principle [17, 18, 31, 51]. Suppose that we have F previously learned (compo-

nent) LSOC problems and a new (composite) LSOC problem with diﬀerent terminal cost from the

previous F problems. Apart from diﬀerent terminal costs, these F + 1 multi-agent LSOC problems
share the same communication network G, state-related cost q(¯xi), exit time tf , and dynamics,
which means that the state space, control space, and sets of interior and boundary states of these
F + 1 problems are also identical. Let φ{f }
(¯xi) with f ∈ {1, 2, · · · , F } be the terminal costs of F
component problems in subsystem ¯Ni, and φi(¯xi) denotes the terminal cost of composite or new
problem in subsystem ¯Ni. We can eﬃciently construct a joint optimal control action ¯u∗
i|¯xi) for
the new task from the existing component controllers ¯u∗{f }
i|¯xi) by the compositionality principle.
such

For multi-agent LSMDP in discrete-time MAS, when there exists a set of weights ¯ω{f }

i (¯x(cid:48)

(¯x(cid:48)

i

i

i

20

that

φi(¯xi) = − log

¯ω{f }
i

· exp

(cid:16)

− φ{f }
i

(¯xi)

(cid:17)(cid:21)
,

(cid:20) F

(cid:88)

f =1

by the deﬁnition of discrete-time desirability function (17), we can imply that

Zi(¯xi) =

F
(cid:88)

f =1

¯ω{f }
i

· Z{f }
i

(¯xi)

(43)

for all ¯xi ∈ ¯Bi. Due to the linear relation in (20), identity (43) should also hold for all interior states
¯xi ∈ ¯Ii. Substituting (43) into the optimal control action (19), the task-optimal control action for
the new task with terminal cost φi(¯xi) can be immediately generated from the existing controllers

i (¯x(cid:48)
¯u∗

i|¯xi) =

F
(cid:88)

f =1

¯W {f }
i

(¯xi) · ¯u∗{f }

i

(¯x(cid:48)

i|¯xi),

(44)

i

i W {f }

(¯xi)/((cid:80)F

(¯xi) = ¯ω{f }

where ¯W {f }
i). For
LSOC in MAS, compositionality principle can be used for not only the generalization of controllers
for the same subsystem ¯Ni, but the generalization of control law across the network. For any two
subsystems that satisfy the aforementioned compatible conditions, the task-optimal controller of

(¯xi)) and W {f }

(¯xi) = (cid:80)
¯x(cid:48)
i

i|¯xi)Z{f }

i W {e}

e=1 ¯ω{e}

¯pi(¯x(cid:48)

(¯x(cid:48)

i

i

i

i

one subsystem can also be directly constructed from the existing computational result of the other

subsystem by resorting to (44).

The generalization of continuous-time LSOC problem can be readily inferred by symmetry. For

F + 1 continuous-time LSOC problems that satisfy the compatible conditions, when there exist
scalars λi, λ{f }

and weights ¯ω{f }

such that

i

i

φi(¯xi) = −λi log

(cid:20) F

(cid:88)

f =1

¯ω{f }
i

· exp

(cid:16)

−

1
λ{f }
i

(cid:17)(cid:21)
,

(¯xi)

· φ{f }
i

(45)

by the deﬁnition of continuous-time desirability function (23), we readily have Zi(¯xi, tf ) = (cid:80)F
Z{f }
i
(26), the linear combination of desirability function

f =1 ¯ω{f }
(¯xi, τ ) is the solution to linearized stochastic HJB equation

(¯xi, tf ) for all ¯xi ∈ ¯Bi. Since Z{f }

i

i

·

Zi(¯xi, τ ) =

F
(cid:88)

f =1

¯ω{f }
i

· Z{f }
i

(¯xi, τ )

(46)

holds everywhere from t to tf on condition that (46) holds for all terminal states ¯Bi, which is
guaranteed by (45). Substituting (46) into the continuous-time optimal controller (28), the task-
optimal composite controller ¯u∗
by

i (¯xi) can be constructed from the component controllers ¯u{f }∗

(¯xi)

i

¯u∗
i (¯xi, t) =

F
(cid:88)

f =1

¯W {f }
i

(¯xi, t) · ¯u{f }∗

i

(¯xi, t),

21

i

i Z{f }

(¯xi, t)/((cid:80)F

(¯xi, t) = ¯ω{f }

i Z{f }
where ¯W {f }
(¯xi, t)). However, this generalization tech-
nique rigorously does not apply to the control policies based on the trajectory optimization or
policy search methods, such as PI2 [22] and REPS in Section 3.2, since these policy approximation
methods usually only predict the optimal path distribution or optimal control policy, while leaving

f =1 ¯ω{f }

i

i

the value or desirability function unknown.

4 Illustrative Examples

Distributed LSOC can be deployed on a variety of MASs in reality, such as the distributed HVAC

system in smart buildings, cooperative unmanned aerial vehicle (UAV) teams, and synchronous

wind farms. Motivated by the experiments in [25, 30, 31], we demonstrate the distributed LSOC

algorithms with a cooperative UAV team in cluttered environment. Compared with these preceding

experiments, the distributed LSOC algorithms in this paper allow to consider an explicit and simpler

communication network underlying the UAV team, and the joint cost function between neighboring

agents can be constructed and optimized with less computation. Consider a cooperative UAV team

consisting of three agents illustrated in Figure 2. UAV 1 and 2, connected by a solid line, are

strongly coupled via their joint cost functions q1(¯x1) and q2(¯x2), and their local controllers u1(¯x1)
and u2(¯x2) are designed to drive UAV 1 and 2 towards their exit state while minimizing the distance
between two UAVs and avoiding obstacles. These setups are useful when we need multiple UAVs to

ﬂy closely towards an identical destination, e.g. carrying and delivering a heavy package together

or maintaining communication channels/networks. By contrast, although UAV 3 is also connected

with UAV 1 and 2 via dotted lines, the immediate cost function of UAV 3 is designed to be

independent or fully factorized from the states of UAV 1 and 2 in each subsystem, such that UAV 3

is only loosely coupled with UAVs 1 and 2 through their terminal cost functions, which restores the

scenario considered in [25, 31]. The local controller of UAV 3 is then synthesized to guide the UAV

to its exit state with minimal cost. In the following subsections, we will verify both discrete-time

and continuous-time distributed LSOC algorithms subject to this cooperative UAV team scenario.

Figure 2: Communication network of UAV team. UAV 1 and 2 are strongly coupled through their running

cost functions. UAV 3 is loosely coupled with UAV 1 and 2 through their terminal cost functions.

4.1 UAVs wtih Discrete-Time Dynamics

In order to verify the distributed LSMDP algorithm, we consider a cooperative UAV team

described by the probability model introduced in Section 2.2. The ﬂight environment is described

22

123by a 5×5 grid with total 25 cells to locate the positions of UAVs, and the shaded cells in Figure 3(a)
represent the obstacles. Each UAV is described by a state vector xi = [ri, ci](cid:62), where ri and
ci ∈ {1, 2, 3, 4, 5} are respectively the row and column indices locating the ith UAV. The cell with
circled index i denotes the initial state of the ith UAV, and the one with boxed index i indicates
the exit state of the ith UAV. The passive transition probabilities of interior, edge, and corner cells
are shown in Figure 3(b), Figure 3(c), and Figure 3(d), respectively, and the passive probability of

a UAV transiting to an adjacent cell can be interpreted as the result of random winds. To fulﬁll

the requirements of aforementioned scenario, the controlled transition distributions should i) drive

UAV 1 and 2 to their terminal state (5, 5) while shortening the distance between two UAVs, and

ii) guide UAV 3 to terminal state (1, 5) with minimum cost, which is related to obstacle avoidance,

control cost, length of path, and ﬂying time.

Figure 3: Flight environment and UAV’s passive transition dynamics.

(a) Flight environment, initial

states, and exit states of UAVs. (b) The passive transition probability of UAVs in the interior cells. (c)

The passive transition probability of UAVs in the edge cells. (d) The passive transition probability of

UAVs in the corner cells.

In order to realize these objectives, we consider the state-related cost functions as follows

q1(¯x1) = w12 · (|r1 − r2| + |c1 − c2|) + o1(x1) · o2(x2),

q2(¯x2) = w21 · (|r2 − r1| + |c2 − c1|) + o1(x1) · o2(x2),

(47)

q3(¯x3) = o3(x3),

where in the following examples, weights on relative distance (cid:107)x1 − x2(cid:107)1, deﬁned by Manhattan
distance, are w12 = w21 = 3.5; state and obstacle cost oi(xi) = 30 when the ith UAV is in an
obstacle cell, and oi(xi) = 2.2 when UAV i is in a regular cell; and terminal cost qi(¯xi) = 0 for
¯xi ∈ ¯Bi. State-related cost q1(¯x1) and q2(¯x2), which involve both the states of UAV 1 and 2,
can measure the joint performance between two UAVs, while cost q3(¯x3) only contains the state
of UAV 3. Without the costs on relative distance, i.e. w12 = w21 = 0, this distributed LSMDP
problem will degenerate to three independent shortest path problems with obstacle avoidance as

23

1234512345① ③②1rc1rr1c1c(a)Passive Dynamics:(b)(d).80.05.05.05(c)r1r.05.85.05.05.05r1r.05.05.90c1cc1c1c123considered in [25, 30, 31], and the optimal trajectories are straightforward‡. Desirability function
Zi(¯xi) and local optimal control distribution u∗
i (·|¯xi) can then be computed by following Algorithm 1
in Appendix E. Figure 4 shows the maximum likelihood controlled trajectories of UAV team subject

to passive dynamics in Figure 3 and cost functions (47). Some curious readers may wonder why

UAV 1 in Figure 4(a) decides to stay in (1, 4) cell for two consecutive time steps rather than

moving forward to (1, 3) and then ﬂying along with UAV 2, which generates a lower state-related

cost. While this alternative corresponds to a lower state-related cost, it may not be the optimal

trajectory minimizing the control cost and the overall immediate cost in (7). In order to verify this

speculation, we increase the passive transition probabilities pi(·|¯xi) to surpass certain thresholds
in Figure 5(a)-(c), which can be interpreted as stronger winds in reality. With this altered passive

dynamics and cost functions in (47), the maximum likelihood controlled trajectory of UAV 1 is

shown in Figure 5(d), which veriﬁes our preceding reasoning. Trajectories of UAV 2 and 3 subject

to altered passive dynamics are identical to the results in Figure 4. To provide an intuitive view

on the eﬃciency improvements of our distributed LSMDP algorithm, Figure 6 presents the average

data size and computational complexity on each UAV (|Si| = 25), gauged by the row number m
of matrices and vectors in (20) and (22), subject to the centralized programming (20) and parallel

programming (22) in diﬀerent communication network, such as line, ring, complete binary tree,

and fully connected topology, which restores the scenario with exponential complexity considered

in [30, 31].

Figure 4: UAVs’ trajectories subject to optimal control policy.

(a) Controlled trajectory of UAV 1:

(1, 5) → (1, 4) → (1, 4) → (1, 4) → (2, 4) → (3, 4) → (3, 5) → (4, 5) → (5, 5). (b) Controlled trajectory

of UAV 2: (1, 1) → (1, 2) → (1, 3) → (1, 4) → (2, 4) → (3, 4) → (3, 5) → (4, 5) → (5, 5). (c) Controlled

trajectory of UAV 3: (1, 5) → (1, 4) → (2, 4) → (3, 4) → (3, 3) → (4, 3) → (5, 3) → (5, 2) → (5, 1).

‡The shortest path for UAV 1 is (1, 5) → (1, 4) → (2, 4) → (3, 4) → (3, 5) → (4, 5) → (5, 5). There are three diﬀerent
shortest paths for UAV 2, and the shortest path for UAV 3 is shown in Figure 4 (c).

24

1234512345①←(a)②→③←↺ ↺↑↑→↑↑1→→↑↑→↑↑2↑↑←↑↑←←3(b)1234512345(c)1234512345Figure 5: Altered passive transition dynamics and controlled trajectory of UAV 1 subject to new dynamics.

(a-c) are respectively the passive transition probabilities for interior, cell, and corner cells. (d) Controlled

trajectory of UAV 1 subject to the altered passive dynamics: (1, 5) → (1, 4) → (1, 3) → (1, 4) → (2, 4) →

(3, 4) → (3, 5) → (4, 5) → (5, 5).

Figure 6: Relationship between amount of agents N and data dimension and computational complexity

O(m). Vertical axis is the value of m. Solid and dashed lines respectively represent the results attained by

centralized algorithm (20) and parallel algorithm (22). Blue asterisk, red cross, green square, and magenta

triangle respectively denote the results associated with fully connected, line, ring, and complete binary

tree communication networks.

4.2 UAVs wtih Continuous-Time Dynamics

In order to verify the continuous-time distributed LSOC algorithms, consider the continuous-

time UAV dynamics described by the following equation [31, 52]:









=









dxi
dyi
dvi
dϕi









vi cos ϕi
vi sin ϕi
0

0









dt +



0 0







0 0

1 0

0 1









(cid:33)

(cid:34)(cid:32)

ui
ωi

dt +

(cid:32)

σi
0

(cid:33)

(cid:35)

dwi

,

0

νi

(48)

25

Passive Dynamics:(a)(c).72.07.07.07.07.79.07.07.07.07.07.8612345①←(d)←↑↑→↑↑1(b)12345→1r+r1r−1r+r1r+r1c−c1c+1c−c1c+c1c+Dimension and Complexity O(m)Amount of agents Nwhere (xi, yi), vi, and ϕi respectively denote the position, forward velocity, and direction angle of the
ith UAV; forward acceleration ui and angular velocity ωi are the control inputs, and disturbance wi
is a standard Brownian motion. Control transition matrix ¯Bi(xi) is a constant matrix in (48), and
we set noise level parameters σi = 0.1 and νi = 0.05 in simulation. The communication network
underlying the UAV team as well as the control objectives are the same as in the discrete-time

scenario. Instead of adopting the Manhattan distance, the distance in continuous-time problem is

associated with L2 norm. Hence, we consider the state-related costs deﬁned as follows

q1(¯x1) = w11 · ((cid:107)(x1, y1) − (xtf
q2(¯x2) = w22 · ((cid:107)(x2, y2) − (xtf
q3(¯x3) = w33 · ((cid:107)(x3, y3) − (xtf

1 , ytf
2 , ytf
3 , ytf

1 )(cid:107)2 − dmax
2 )(cid:107)2 − dmax
3 )(cid:107)2 − dmax

2

3

1

) + w12 · ((cid:107)(x1, y1) − (x2, y2)(cid:107)2 − dmax
) + w21 · ((cid:107)(x2, y2) − (x1, y1)(cid:107)2 − dmax

12 ),

21 ),

(49)

),

where wii is the weight on distance between the ith UAV and its exit position; wij is the weight
on distance between the ith and jth UAVs; dmax
usually denotes the initial distance between the
ith UAV and its destination, and dmax
denotes the initial distance between the ith and jth UAVs.
The parameters dmax
are chosen to regularize the numerical accuracy and stability of
algorithms.

and dmax

ij

ij

i

i

To show an intuitive improvement brought by the joint state-related cost, we ﬁrst verify

1 = (5, 5, 0.5, 0)(cid:62), x0

3 = (5, 25, 0.5, 0)(cid:62) and identical exit state xtf

the continuous-time LSOC algorithm in a simple ﬂight environment without obstacle. Consider
three UAVs forming the network in Figure 2 and with initial states x0
2 =
(5, 45, 0.5, 0)(cid:62), x0
i = (45, 25, 0, 0)(cid:62) for i = 1, 2, 3.
The exit time is tf = 25, and the length of each control cycle is 0.2. When sampling trajectory
roll-outs Yi, the time interval from t to tf is partitioned into K = 7 intervals of equal length ε,
i.e. εK = tf − t, until ε becomes less than 0.2. Meanwhile, to make the exploration process more
aggressive, we increase the noise level parameters σi = 0.75 and νi = 0.65 when sampling trajectory
roll-outs. The size of data set Yi for estimator (35) in each control cycle is 400 sample trajecto-
ries, which can be generated concurrently by GPU [25]. Control weight matrices ¯Ri are selected
as identity matrices. Guided by the sampling-based distributed LSOC algorithm, Algorithm 2 in

Appendix E, UAV trajectories and the relative distance between UAV 1 and 2 in condition of both

joint and independent state-related costs are presented in Figure 7. Letting update rate constraint

be δi = 25 and the size of trajectory set be |Yi| = 400 and 150 for the initial and subsequent policy
iterations in every control period, simulation results obtained from the REPS-based distributed

LSOC algorithm, Algorithm 3 in Appendix E, are given in Figure 8. Figure 7 and Figure 8 imply

that joint state-related costs can signiﬁcantly inﬂuence the trajectories and shorten the relative

distance between UAV 1 and 2, which fulﬁlls our preceding control requirements.

26

Figure 7: UAV trajectories and the distance between UAV 1 and 2 from 50 trails subject to the sampling-

based distributed LSOC algorithm. (a) Trajectories of UAVs. Red, blue and green lines are trajectories

for UAV 1, 2 and 3, respectively. Dashed lines are from a trail with factorized (or independent) state costs

(w11 = w22 = 0.75, w33 = 1, w12 = w21 = 0), and solid (transparent) lines are from trails with joint state
costs (w11 = w22 = 0.75, w33 = 1, w12 = w21 = 1.5). (b) Distance between UAV 1 and 2. Red dashed line
and blue solid line are respectively the mean distances from tails with factorized state cost and joint state

cost. Height of strip represents one standard deviation.

Figure 8: UAV trajectories and distance between UAV 1 and 2 from 50 trails subject to the REPS-based

distributed LSOC algorithm. (a) Trajectories of UAVs. Red, blue and green lines are trajectories for UAV

1, 2 and 3, respectively. Dashed lines are from a trail with factorized state costs (w11 = w22 = w33 = 0.1,
w12 = w21 = 0), and solid (transparent) lines are from trails with joint state costs (w11 = w22 = w33 = 0.1,
w12 = w21 = 0.2). (b) Red dashed line and blue solid line are respectively the mean distances from tails
with factorized state cost and joint state cost. Height of strip represents one standard deviation.

We then consider a cluttered ﬂight environment as the discrete-time example. Suppose three

UAVs forming the network in Figure 2 and with initial states x0
3 = (45, 5, 0.5, π)(cid:62) and exit states xtf
x0
3 = (5, 45, 0, π)(cid:62). The exit time
is tf = 30, and the length of each control cycle is 0.2. When sampling trajectory roll-outs Yi,
the time interval from t to tf is partitioned into K = 18 intervals of equal length. The size of
Yi is 400 trajectory roll-outs when adopting random sampling estimator, and the other param-
eters are the same as in the preceding continuous-time example. Subject to the sampling-based

2 = (45, 45, 0, π/2)(cid:62), xtf

1 = (45, 5, 0.35, π)(cid:62), x0

1 = xtf

2 = (5, 5, 0.65, 0)(cid:62),

27

①②③y [m]x [m]Time [sec.]Distance between UAV 1 and 2 [m](a)(b)223311①②③y [m]x [m]Time [sec.]Distance between UAV 1 and 2 [m](a)(b)223311distributed LSOC algorithm, UAV trajectories and the relative distance between UAVs 1 and 2

subject to both joint and independent state-related costs are presented in Figure 9. Letting the

update rate constraint be δi = 50 and the size of trajectory set be |Yi| = 400 and 150 for the initial
and subsequent policy iterations in every control period, experimental results obtained from the

REPS-based distributed LSOC algorithm are given in Figure 10. Figure 9 and Figure 10 show

that our continuous-time distributed LSOC algorithms can guide UAVs to their terminal states,

avoid obstacles, and shorten the relative distance between UAVs 1 and 2. It is also worth noticing

that since there exist more than one shortest path for UAV 2 in condition of factorized state cost

(see the footnote in Section 4.1), the standard variations of distance in Figure 9 and Figure 10

are signiﬁcantly larger than other cases. Lastly, we compare the sample-eﬃciency between the

sampling-based and REPS-based distributed LSOC algorithms in preceding two continuous-time
examples. Figure 11 shows the value of immediate cost function c2(¯x2, ¯u2) from subsystem ¯N2 versus
the amount of trajectory roll-outs, and the maximum numbers of trajectory roll-outs on horizontal

axes are determined by the REPS-based trails with minimum amounts of sample roll-outs.

In

Figure 11, we can tell that the REPS-based distributed LSOC algorithm is more sample-eﬃcient

than the sampling-based algorithm.

Figure 9: UAV trajectories and relative distance between UAV 1 and 2 from 100 trails based on random

sampling estimator. (a) Trajectories of UAVs. Red, blue and green lines are trajectories for UAV 1, 2

and 3, respectively. Dashed lines are from a trail with factorized (or independent) state costs (w11 =
w22 = w33 = 1, w12 = w21 = 0), and solid (transparent) lines are from trails with joint state costs
(w11 = w22 = w33 = 1, w12 = 1.5, w21 = 0.5). (b) Distance between UAV 1 and 2. Red dashed line and
blue solid line are respectively the mean distances from tails with independent state cost and joint state

cost. Height of strip represents one standard deviation.

28

Time [sec.]Distance between UAV 1 and 2 [m](b)y [m]x [m](a)①②③223311Figure 10: UAV trajectories and relative distance between UAV 1 and 2 from 100 trails based on REPS.

(a) Trajectories of UAVs. Red, blue and green lines are trajectories for UAV 1, 2 and 3, respectively. Dashed

lines are from a trail with factorized state costs (w11 = w22 = w33 = 0.18, w12 = w21 = 0), and solid
(transparent) lines are from trails with joint state costs (w11 = w22 = w33 = 0.18, w12 = 0.27, w21 = 0.1).
(b) Distance between UAV 1 and 2. Red dashed line and blue solid line are respectively the mean distances

from tails with independent state cost and joint state cost. Height of strip is one standard deviation.

Figure 11: Sample-eﬃciency of continuous-time algorithms from 100 trails. (a) Immediate cost c2(¯x2, ¯u2)
in simple scenario without obstacle. Red dashed line is the mean of immediate cost subject to random

sampling approach. Blue solid line is the mean of immediate cost subject to REPS algorithm. The height

of strip is one standard deviation.

(b) Immediate cost c2(¯x2, ¯u2) in complex scenario with obstacles.

Interpretation is the same as (a).

To verify the eﬀectiveness of distributed LSOC algorithms in larger MAS with more agents, we

consider a line-shape network consisting of nine UAVs as shown in Figure 12. These nine UAVs
i = {1, 2, · · · , 9} are initially distributed at x0
i = (10, 100 − 10i, 0.5, 0) as shown in Figure 13, and
they can be divided into three groups based on their exit states. UAV 1 to 6 share a same exit
state A at xtf
7:8 = (90, 25, 0, 0), and UAV
9 is expected to exit at state C, xtf
9 = (90, 10, 0, 0), where the exit time tf = 40 sec. As exhibited
in Figure 12, UAVs from diﬀerent groups are either loosely coupled through their terminal cost

1:6 = (90, 65, 0, 0); the exit state B of UAV 7 and 8 is at xtf

functions or mutually independent, where the latter scenario does not require any communication

29

Time [sec.]Distance between UAV 1 and 2 [m](b)y [m]x [m](a)①②③223311Number of trajectory roll-outsImmediate cost(b)Number of trajectory roll-outs(a)222(,)cxu222(,)cxuImmediate costbetween agents. A state-related cost function qi(¯xi) in subsystem ¯Ni is designed to consider and
optimize the distances between neighboring agents and towards the exit state of agent i:

qi(¯xi) = wii · ((cid:107)(xi, yi) − (xtf

i , ytf

i )(cid:107)2 − dmax

i

) + wi,i−1 · ((cid:107)(xi, yi) − (xi−1, yi−1)(cid:107) − dmax
+ wi,i+1 · ((cid:107)(xi, yi) − (xi+1, yi+1)(cid:107) − dmax

i,i−1)

i,i+1),

where wi,j is the weight related to the distance between agent i and j; wi,j = 0 when j = 0 or
10; dmax
is the regularization term for numerical stability, which is assigned by the initial distance
between agents i and j in this demonstration, and the remaining notations and parameters are the

i,j

same as the assignments in (49) and the ﬁrst example in this subsection if not explicitly stated. Tra-

jectories of UAV team subject to two distributed LSOC algorithms, Algorithm 2 and Algorithm 3

in Appendix E, are presented in Figure 13. For some network structures, such as line, loop, star

and complete binary tree, in which the scale of every factorial subsystem is tractable, increasing

the total number of agents in network will not dramatically boost the computational complexity on

local agents thanks to the distributed LSOC framework proposed in this paper. Veriﬁcation along

with more simulation examples on the generalization of distributed LSOC controllers discussed in

Section 3.3 is supplemented in [53].

Figure 12: Communication network of a UAV team with nine agents. UAV 1 to 6, as well as UAV 7 and

8 are strongly coupled (represented by solid lines) through their immediate cost functions. UAV 6 and

7, as well as UAV 8 and 9 are loosely coupled (represented by dashed lines) through their terminal cost

functions.

Figure 13: UAV trajectories subject to two distributed LSOC algorithms. (a) Trajectories of UAV team

controlled by the sampling-based distributed LSOC algorithm with w1,2 = w2,3 = w3,4 = w6,5 = w7,8 =
w8,7 = w99 = 1, wii = w4,3 = w4,5 = w5,4 = w5,6 = 0.5, and w2,1 = w3,2 = w6,7 = w7,6 = w9,8 = 0. (b)
Trajectories of UAV team controlled by the REPS-based distributed LSOC algorithm with w1,2 = w2,3 =
w3,4 = w6,5 = w7,8 = w8,7 = w99 = 0.2, wii = w4,3 = w4,5 = w5,4 = w5,6 = 0.1, and w2,1 = w3,2 = w6,7 =
w7,6 = w9,8 = 0.

30

123456789⑨②③ABCy [m]x [m]①④⑤⑥⑦⑧⑨②③y [m]x [m]①④⑤⑥⑦⑧(a)(b)ABCMany interesting problems remain unsolved in the area of distributed (linearly solvable) stochas-

tic optimal control and deserve further investigation. Most of existing papers, including this paper,

assumed that the passive and controlled dynamics of diﬀerent agents are mutually independent.

However, when we consider some practical constraints, such as the collisions between diﬀerent

UAVs, the passive and controlled dynamics of diﬀerent agents are usually not mutually dependent.

Meanwhile, while this paper considered the scenario when all states of local agents are fully ob-

servable, it will be interesting to study the MAS of partially observable agents with hidden states.

Lastly, distributed LSOC algorithms subject random network, communication delay, inﬁnite hori-

zon or discounted cost are also worth our attention.

5 Conclusion

Discrete-time and continuous-time distributed LSOC algorithms for networked MASs have been

investigated in this paper. A distributed control framework based on factorial subsystems has been

proposed, which allows to optimize the joint state or cost function between neighboring agents

with local observation and tractable computational complexity. Under this distributed framework,

the discrete-time multi-agent LSMDP problem was addressed by respectively solving the local sys-

tems of linear equations in each subsystem, and a parallel programming scheme was proposed

to decentralize and expedite the computation. The optimal control action/policy for continuous-

time multi-agent LSOC problem was formulated as a path integral, which was approximated by

a distributed sampling method and a distributed REPS method, respectively. Numerical exam-

ples of coordinated UAV teams were presented to verify the eﬀectiveness and advantages of these

algorithms, and some open problems were given at the end of this paper.

Acknowledgments

This work was supported in part by NSF-NRI, AFOSR, and ZJU-UIUC Institute Research

Program. The authors would like to appreciate the constructive comments from Dr. Hunmin Lee

and Gabriel Haberfeld. In this arXiv version, the authors would also like to thank the readers and

staﬀ on arXiv.org.

Conﬂict of Interest Statement

The authors have agreed to publish this article, and we declare that there is no conﬂict of

interests regarding the publication of this article.

31

Appendix A: Proof for Theorem 1

Proof for Theorem 1: Substituting the joint running cost function (7) into the joint Bellman

equation (9), and by the deﬁnitions of KL-divergence and exponential transformation (17), we have

Vi(¯xi) = min
¯ui

= min

¯ui

= min

¯ui

(cid:110)
qi(¯xi) + KL(¯ui(·|¯xi) (cid:107) ¯pi(·|¯xi)) + E¯x(cid:48)
(cid:26)

(cid:20)

(cid:21)

i∼¯ui(·|¯xi)[Vi(¯x(cid:48)
i)]
(cid:20)

(cid:111)

qi(¯xi) + E¯x(cid:48)

i∼¯ui(·|¯xi)

(cid:26)

qi(¯xi) + E¯x(cid:48)

i∼¯ui(·|¯xi)

log

(cid:20)

log

¯ui(¯x(cid:48)
¯pi(¯x(cid:48)

i|¯xi)
i|¯xi)
¯ui(¯x(cid:48)
i|¯xi)
i|¯xi)Zi(¯x(cid:48)
i)

¯pi(¯x(cid:48)

+ E¯x(cid:48)∼¯ui(·|¯xi)
(cid:21)(cid:27)

.

log

(cid:21)(cid:27)

1
Zi(¯x(cid:48)
i)

(50)

The optimal policy will be straightforward if we can rewrite the expectation on the RHS of (50) as

a KL-divergence and exploit the minimum condition of KL-divergence. While the control mapping
i|¯xi) in (50) is a probability distribution, the denominator p(x(cid:48)|x)Z(x(cid:48)) is not necessarily a
¯ui(¯x(cid:48)
probability distribution. Hence, we deﬁne the following normalized term

Wi(¯xi) =

(cid:88)

¯x(cid:48)
i

¯pi(¯x(cid:48)

i|¯xi)Zi(¯x(cid:48)

i).

(51)

Since p(x(cid:48)|x)Z(x(cid:48))/Wi(¯xi) is a well-deﬁned probability distribution, we can rewrite the joint Bell-
man equation (50) as follows

(cid:26)

Vi(¯xi) = min
¯ui

qi(¯xi) + E¯x(cid:48)

i∼¯ui(·|¯xi)

(cid:20)

log

(cid:40)

= min

¯ui

qi(¯xi) − log Wi(¯xi) + KL

i|¯xi)

¯pi(¯x(cid:48)
(cid:32)

¯ui(¯x(cid:48)
i|¯xi)Zi(¯x(cid:48)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯ui(·|¯xi)

i)/Wi(¯xi)
¯pi(·|¯xi)Zi(·)
Wi(¯xi)

(cid:21)(cid:27)

− log Wi(¯xi)

(cid:33)(cid:41)

,

(52)

where only the last term depends on the joint control action ¯ui(·|¯xi). According to the minimum
condition of KL-divergence, the last term in (52) attains its absolute minimum at 0 if and only if

¯u∗
i (·|¯xi) =

¯pi(·|¯xi)Zi(·)
Wi(¯xi)

,

which gives the optimal control action (19) in Theorem 1. By substituting (19) into (52), we can

minimize the RHS of joint Bellman equation (9) and remove the minimum operator:

Vi(¯xi) = qi(¯xi) − log Wi(¯xi).

(53)

Exponentiating both sides of (53) and substituting (17) and (51) into the result, the Bellman

equation (53) can be rewritten as a linear equation with respect to the desirability function

Zi(¯xi) = exp[−Vi(¯xi)] = exp[−qi(¯xi)] · Wi(¯xi) = exp[−qi(¯xi)] ·

(cid:88)

¯x(cid:48)
i

which implies (18) in Theorem 1. This completes the proof.

32

¯pi(¯x(cid:48)

i|¯xi)Zi(¯x(cid:48)

i),

(54)

Appendix B: Proof for Theorem 2

Before we present the proof for Theorem 2, Feynman–Kac formula that builds an important

relationship between parabolic PDEs and stochastic processes is introduced as Lemma 4.

Lemma 1. [Feynman–Kac formula] Consider the Kolmogorov backward equation (KBE) described

as follows

∂tZ(x, t) =

q(x, t)
λ

· Z(x, t) − f (x, t)(cid:62) · ∇xZ(x, t) −

(cid:16)

(cid:17)
B(x)σσ(cid:62)B(x)(cid:62) · ∇xxZ(x, t)

,

tr

1
2

where the terminal condition is given by Z(x, tf ) = exp[−φ(x(tf ))/λ]. Then the solution to this
KBE can be written as a conditional expectation

Z(x, t) = Ex,t

(cid:20)

(cid:18)

exp

−

1
λ

(cid:19)

φ(y(tf ))

−

1
λ

(cid:90) tf

t

q(y, τ ) dτ

(cid:12)
(cid:12)
(cid:12) y(t) = x

(cid:21)

,

under the probability measure that y is an Itˆo diﬀusion process driven by the equation dy(τ ) =

f (y, τ )dτ + B(y)σ · dw(τ ) with initial condition y(t) = x.

We now show the proof for Theorem 2.

Proof for Theorem 2: First, we show that the joint optimality equation (16) can be formulated

into the joint stochastic HJB equation (24) that gives an analytic expression of optimal control

action (25). Substituting immediate cost function (15) into optimality equation (16) and letting s
be a time step between t and tf , optimality equation (16) can be rewritten as

Vi(¯xi, t) = min
¯ui

E¯ui

¯xi,t

= min

¯ui

E¯ui

¯xi,t

(cid:20)

(cid:20)

(cid:90) tf

φi(¯xi, tf ) +

Vi(¯xi, s) +

t
(cid:90) s

t

qi(¯xi, τ ) +

1
2

qi(¯xi, τ ) +

1
2

¯ui(¯xi, τ )(cid:62) ¯Ri ¯ui(¯xi, τ ) dτ
(cid:21)

¯ui(¯xi, τ )(cid:62) ¯Ri ¯ui(¯xi, τ ) dτ

(cid:21)

.

(55)

With some rearrangements and dividing both sides of (55) by s − t > 0, we have

0 = min

¯ui

E¯ui

¯xi,t

(cid:20) Vi(¯xi, s) − Vi(¯xi, t)
s − t

+

1
s − t

(cid:90) s

t

qi(¯xi, τ ) +

1
2

¯ui(¯xi, τ )(cid:62) ¯Ri ¯ui(¯xi, τ ) dτ

(cid:21)

.

(56)

By letting s → t, the optimality equation (56) becomes

0 = min

¯ui

E¯ui

¯xi,t

(cid:20) dVi(¯xi, t)
dt

+ qi(¯xi, t) +

¯ui(¯xi, t)(cid:62) ¯Ri ¯ui(¯xi, t)

(cid:21)

.

1
2

(57)

Applying Itˆo’s formula [54], the diﬀerential dVi(¯xi, t) in (57) can be expanded as

dVi(¯xi, t) =

(cid:88)

M
(cid:88)

j∈ ¯Ni

m=1

∂Vi(¯xi, t)
∂xj(m)

dxj(m)+

∂Vi(¯xi, t)
∂t

dt+

1
2

(cid:88)

M
(cid:88)

j,k∈ ¯Ni

m,n=1

∂2Vi(¯xi, t)
∂xj(m)∂xk(n)

dxj(m)dxk(n). (58)

For conciseness, we will omit the indices (or subscripts) of state components, (m) and (n), in the

following derivations. Dividing both sides of (58) by dt, taking the expectation over all trajectories

33

that initialized at (¯xt
into the result, we have

i, t) and subject to control action ¯ui, and substituting the joint dynamics (14)

E¯ui

¯xi,t

(cid:21)

(cid:20) dVi(¯xi, t)
dt

=

(cid:88)

j∈ ¯Ni

[fj(xj, t) + Bj(xj)uj(¯xi, t)](cid:62) · ∇xj Vi(¯xi, t) +

∂Vi(¯xi, t)
∂t

+

1
2

(cid:88)

(cid:16)

tr

j∈ ¯Ni

Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62) · ∇xj xj Vi(¯xi, t)

(cid:17)

,

(59)

where the identity E¯ui
¯xi,t[dxj(m)dxk(n)] = (σjσ(cid:62)
j )mmδjkδmndt derived from the property of standard
Brownian motion E¯ui
¯xi,t[dwj(m)dwk(n)] = δjkδmndt is invoked, and operators ∇xj and ∇xj xj follow
the same deﬁnitions in (13). Substituting (59) into (57), the joint stochastic HJB equation (24) in

Theorem 2 is obtained

−∂tVi(¯xi, t) = min
¯ui

E¯ui

¯xi,t

(cid:20) (cid:88)

j∈ ¯Ni

[fj(xj, t) + Bj(xj)uj(¯xi, t)](cid:62) · ∇xj Vi(¯xi, t) + qi(¯xi, t)

+

1
2

¯ui(¯xi, t)(cid:62) ¯Ri ¯ui(¯xi, t) +

1
2

(cid:88)

(cid:16)

tr

j∈ ¯Ni

Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62) · ∇xj xj Vi(¯xi, t)

(cid:17) (cid:21)
,

where the boundary condition is given by Vi(¯xi, tf ) = φi(¯xi). The joint optimal control action
¯u∗
i (¯xi, t) can be obtained by setting the derivative of (24) with respect to ¯ui(¯xi, t) equal to zero.
When the control weights Rj of each agent j ∈ ¯Ni are coupled, i.e. the joint control weight matrix
¯Ri cannot be formulated as a block diagonal matrix, the joint optimal control action for subsystem
¯Ni is given as (25) in Theorem 2, ¯u∗
¯Bi(¯xi)(cid:62) · ∇¯xiVi(xi, t), where ∇¯xi denotes the
gradient with respect to the joint state ¯xi. However, it is more common in practice that the joint
control weight matrix is given by ¯Ri = diag{Ri, Rj∈Ni} as in (15), and the joint control cost satisﬁes
1
2 ¯u(cid:62)
j Rjuj. Setting the derivatives of (24) with respect to uj(¯xi, t) equal to zero
i
in the latter case gives the local optimal control action of agent j ∈ ¯Ni

i (¯xi, t) = − ¯R−1

¯Ri ¯ui = (cid:80)

1
2 u(cid:62)

j∈ ¯Ni

i

j (¯xi, t) = −R−1
u∗

j Bj(xj)(cid:62) · ∇xj Vi(¯xi, t).

(60)

For conciseness of derivations and considering the formulation of (24), we will mainly focus on the
latter scenario, ¯Ri = diag{Ri, Rj∈Ni}, in the remaining part of this proof.

In order to solve the stochastic HJB equation (24) and evaluate the optimal control action (25),

we consider to linearize (24) with the Cole-Hopf transformation (23), Vi(¯xi, t) = λi log Z(¯xi, t).
Subject to this transformation, the derivative and gradients in (24) satisfy

∂tVi(¯xi, t) = −λi ·

∂tZi(¯xi, t)
Zi(¯xi, t)

,

∇xj Vi(¯xi, t) = −λi ·

∇xj xj Vi(¯xi, t) = −λi ·

(cid:34)

∇xj xj Zi(¯xi, t)
Zi(¯xi, t)

34

,

∇xj Zi(¯xi, t)
Zi(¯xi, t)
∇xj Zi(¯xi, t) · ∇xj Zi(¯xi, t)(cid:62)
Zi(¯xi, t)2

−

(61)

(62)

(63)

(cid:35)

.

Equalities (62) and (63) will still hold when we replace the agent’s state xj in ∇xj and ∇xj xj by
the joint state ¯xi and reformulate the joint HJB (24) in a more compact form. Substituting local
optimal control action (60), and gradients (62) and (63) into the stochastic HJB equation (24), the
corresponding terms of each agent j ∈ ¯Ni in (24) satisfy

[Bj(xj)uj(¯xi, t)](cid:62)∇xj Vi(¯xi, t) +

1
2

uj(¯xi, t)(cid:62)Rjuj(¯xi, t)

= −

=

∇xj Vi(¯xi, t)(cid:62) · Bj(xj)R−1
−λ2
i

1
2
2 · Zi(¯xi, t)2 · ∇xj Zi(¯xi, t)(cid:62) · Bj(xj)R−1

j Bj(xj)(cid:62) · ∇xj Vi(¯xi, t)

j Bj(xj)(cid:62) · ∇xj Zi(¯xi, t),

tr(cid:0)Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62) · ∇xj xj Vi(¯xi, t)(cid:1)

1
2

=

(cid:16)

· tr

−λi
2 · Zi(¯xi, t)
λi
2 · Zi(¯xi, t)2 · tr

+

Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62) · ∇xj xj Zi(¯xi, t)

(cid:17)

(cid:16)

Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62) · ∇xj Zi(¯xi, t) · ∇xj Zi(¯xi, t)(cid:62)(cid:17)

.

By the properties of trace operator, the quadratic terms in (64) and (65) will be canceled if

σjσ(cid:62)

j = λiR−1
j

,

(64)

(65)

(66)

i.e. Rj = (σjσ(cid:62)
i /λi)−1
subject to joint dynamics (14). Substituting (25), (61), (64) and (65) into stochastic HJB equa-

j /λi)−1, which is equivalent to the condition ¯σi¯σ(cid:62)

or ¯Ri = (¯σi¯σ(cid:62)

i = λi ¯R−1

i

tion (24), we then remove the minimization operator and obtain the linearized PDE as (26) in

Theorem 2

∂tZi(¯xi, t) =





qi(¯xi, t)
λi

(cid:88)

−

j∈ ¯Ni

fj(xj, t)∇xj −

1
2

(cid:88)

(cid:16)

tr

j∈ ¯Ni

Bj(xj)σjσ(cid:62)

j Bj(xj)(cid:62)∇xj xj

(cid:17)



 Zi(¯xi, t),

where the boundary condition is given by Zi(¯xi, tf ) = exp[−φi(¯xi)/λi]. Once the value of desir-
ability function Zi(¯xi, t) in (26) is solved, we can readily ﬁgure out the value function Vi(¯xi, t) and
joint optimal control action from (23) and (25), respectively. Invoking the Feynman–Kac formula

introduced in Lemma 1, a solution to (26) can be formulated as (27) in Theorem 2

Zi(¯xi, t) = E¯xi,t

(cid:20)

(cid:18)

exp

−

1
λi

φi(¯ytf

i ) −

1
λi

(cid:90) tf

t

(cid:19)(cid:21)

qi(¯yi, τ ) dτ

,

where ¯y(t) satisﬁes the uncontrolled dynamics d¯yi(τ ) = ¯fi(¯yi, τ )dτ + ¯Bi(¯yi)¯σi · d ¯wi(τ ) with initial
condition ¯yi(t) = ¯xi(t). This completes the proof.

Appendix C: Proof of Proposition 3

Proof of Proposition 3: First, we formulate the desirability function (27) as a path integral shown
in (30). Partitioning the time interval from t to tf into K intervals of equal length ε > 0, t = t0 <

35

t1 < · · · < tK = tf , we can rewrite (27) as the following path integral

Zi(¯xi, t) = E¯xi,t

(cid:20)

(cid:18)

exp

−

1
λi

φi(¯ytf

i ) −

1
λi

(cid:90) tf

t

(cid:19)(cid:21)

qi(¯yi, τ ) dτ

(cid:90)

=

d¯x(1)
i

· · ·

(cid:90)

(cid:18)

exp

−

1
λi

φi(¯x(K)
i

)

K−1
(cid:89)

(cid:19)

·

k=0

Zi(¯x(k+1)
i

, tk+1; ¯x(k)

i

, tk) d¯x(K)

i

,

(67)

where the integral of variable ¯x(k)
interval [tk−1, tk) and with initial condition ¯x(0)
i at initial time t, and the function Zi(¯x(k+1)

is over the set of all joint uncontrolled trajectories ¯xi(τ ) on time
i = ¯xi(t0) = ¯xi(t), which can be measured by agent
, tk+1; ¯x(k)

, tk) is implicitly deﬁned by

i

i

i

(cid:90)

f (¯x(k+1)
i

)·Zi(¯x(k+1)
i

i

, tk+1; ¯x(k)
(cid:20)
f (¯x(k+1)
i

, tk) d¯x(k+1)
i
(cid:18)

) · exp

−

= E

¯x(k)
i

,tk

(cid:90) ti+1

ti

qi(¯yi, τ ) dτ

(cid:19) (cid:12)
(cid:12) ¯yi(tk) = ¯x(k)
(cid:12)

i

(cid:21)

(68)

1
λi

for arbitrary functions f (¯x(k+1)
function Zi(¯x(k+1)

i
, tk+1; ¯x(k)

i

i

, tk) can be approximated by

). Based on deﬁnition (68) and in the limit of inﬁnitesimal ε, the

Zi(¯x(k+1)
i

, tk+1; ¯x(k)

i

, tk) = pi(¯x(k+1)

i

, tk+1|¯x(k)

i

(cid:18)

, tk) · exp

−

· qi(¯x(k)

i

(cid:19)

, tk)

,

ε
λi

(69)

where pi(¯x(k+1)
pair (¯x(k)

, tk+1|¯x(k)
i
i
, tk) to (¯x(k+1)
i
, tk+1|¯x(k)

i
pi(¯x(k+1)
i

i

, tk) is the transition probability of uncontrolled dynamics from state-time
, tk+1) and can be factorized as follows
, tk+1|¯x(k)
, tk) = pi(¯x(k+1)
, tk) · pi(¯x(k+1)
i(n)
, tk+1|¯x(k)
= pi(¯x(k+1)
i(n)
, tk+1|¯x(k)
∝ pi(¯x(k+1)

, tk)
i
, tk+1|¯x(k)

i(n), tk) · pi(¯x(k+1)

i(d), ¯x(k)
, tk),

, tk+1|¯x(k)

i(d), ¯x(k)

i(n), tk)

(70)

i(d)

i(d)

i

i(d)

i

where pi(¯x(k+1)
i(n)
calculated from ¯x(k)

, tk+1|¯x(k)

i(d), ¯x(k)
i(n), tk) is a Dirac delta function, since ¯x(k+1)
i(n) and ¯x(k)
i(d). Provided the directly actuated uncontrolled dynamics from (29)
i(d) − ¯x(k)
¯x(k+1)

, tk)ε + ¯Bi(d)(¯x(k)

i(d) = ¯fi(d)(¯x(k)

can be deterministically

) · ¯σi ¯wi

i(n)

i

i

i(d) ∼ N (¯x(k)

with Brownian motion ¯wi ∼ N (0, εIM ), the directly actuated states ¯x(k)
i(d) + ¯fi(d)(¯x(k)
sian distribution ¯x(k+1)
¯Bi(d)(¯x(k)
i = λi ¯R−1
)(cid:62). When condition ¯σi¯σ(cid:62)
i
ελi ¯Bi(d)(¯x(k)
i with H (k)
¯Bi(d)(¯x(k)
i
Hence, the transition probability in (70) satisﬁes

, tk)ε, Σ(k)
in Theorem 2 is fulﬁlled, the covariance is Σ(k)
i = λi ¯Bi(d)(¯x(k)

satisfy Gaus-
)¯σi¯σ(cid:62)
i
i =
)¯σi¯σ(cid:62) ¯Bi(d)(¯x(k)

) with covariance Σ(k)

i = ε ¯Bi(d)(¯x(k)

i(d) and ¯x(k+1)

)(cid:62) = ¯Bi(d)(¯x(k)

· ¯Bi(d)(¯x(k)

)(cid:62) = εH (k)

)· ¯R−1
i

) ¯R−1
i

i(d)

·

i

i

i

i

i

i

i

i

i

)(cid:62).

pi(¯x(k+1)
i(d)

,tk+1|¯x(k)

i

, tk) =

1

[det(2πΣ(k)


i

=

1

[det(2πΣ(k)

i

)]1/2

· exp


−

ε
2

)]1/2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)

exp

−

1
2

(cid:13)
(cid:13)¯x(k+1)
(cid:13)

i(d) − ¯x(k)

i(d) − ¯fi(d)(¯x(k)


i

, tk)ε

(cid:33)

(cid:13)
2
(cid:13)
(cid:16)
(cid:13)
Σ(k)
i

(cid:17)−1

i(d) − ¯x(k)
¯x(k+1)
ε

i(d)

− ¯fi(d)(¯x(k)

i

, tk)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
H (k)
i


 .

(cid:17)−1

(71)

36

Substituting (69), (70) and (71) into (67) and in the limit of inﬁnitesimal ε, the desirability function

Zi(¯xi, t) can then be rewritten as a path integral

Zi(¯xi, t) = lim
ε↓0

Z(ε)
i

(¯x(0)
i

, t0).

(72)

Deﬁning a path variable ¯(cid:96)i = (¯x(1)
expressed as

i

, · · · , ¯x(K)

i

), the discretized desirability function in (72) can be

Z(ε)
i

(¯x(0)
i

, t0) =

=

=

(cid:90)

(cid:90)

(cid:90)

(cid:32)

exp

−Sε,λi
i

(cid:32)

exp

−Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) −

1
2

K−1
(cid:88)

log det(2πΣ(k)

i

)

(cid:33)

d¯(cid:96)i

(73)

(¯x(0)
i

, ¯(cid:96)i, t0) −

log det(H (k)

i

) −

KD| ¯Ni|
2

(cid:33)

log(2πε)

d¯(cid:96)i

(cid:18)

− ˜Sε,λi
i

exp

(¯x(0)
i

, ¯(cid:96)i, t0) −

(cid:19)

log(2πε)

d¯(cid:96)i,

k=0
K−1
(cid:88)

1
2
k=0
KD| ¯Ni|
2

(¯x(0)
where Sε,λi
i
i
(¯xi, t) or (¯x(0)
, t0) and takes the form of

, ¯(cid:96)i, t0) is the path value for a trajectory (¯x(0)

i

i

, · · · , ¯x(K)

i

) starting at space-time pair

Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) =

)

φi(¯x(K)
i
λi

+ ε

(cid:34)

K−1
(cid:88)

k=0

, tk)

qi(¯x(k)
i
λi

+

1
2

i(d) − ¯x(k)
¯x(k+1)
ε

i(d)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

− ¯fi(d)(¯x(k)

i

, tk)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
H (k)
i

(cid:35)

;

(cid:17)−1

i

(¯x(0)
i
(cid:80)K−1

k=0 log det(H (k)

, ¯(cid:96)i, t0) is the generalized path value and satisﬁes ˜Sε,λi

˜Sε,λi
, ¯(cid:96)i, t0)
i
), and the constant KD| ¯Ni|/2 · log(2πε) in (73) is related to the numeri-
+ 1
2
cal stability, which demands a careful choice of ε and a ﬁne partition over [t, tf ). Identical to the
expectation in (27) and the integral in (67), the integral in (73) is subject to the set of all uncon-
trolled trajectories ¯xi(τ ) initialized at (¯xi, t) or (¯x(0)
(30) and (31) in Proposition 3 can be restored.

, t0). Summarizing the preceding deviations,

, ¯(cid:96)i, t0) = Sε,λi

(¯x(0)
i

(¯x(0)
i

i

i

i

Substituting gradient (62) and discretized desirability function (72) into the joint optimal control

action (25), we have

i (¯xi, t) = λi ¯R−1
¯u∗

i

¯Bi(¯xi)(cid:62) ∇¯xiZi(¯xi, t)
Zi(¯xi, t)

= ¯σi¯σ(cid:62)
i

¯Bi(¯xi)(cid:62) ∇¯xiZi(¯xi, t)
Zi(¯xi, t)

(74)

= λi ¯R−1

i

¯Bi(¯xi)(cid:62) · lim
ε↓0

∇

(cid:82) exp[− ˜Sε,λi

¯x(0)
i
i
(cid:82) exp[− ˜Sε,λi
(¯x(0)
i

i

, ¯(cid:96)i, t0) − KD| ¯Ni|/2 · log(2πε)] d¯(cid:96)i

(¯x(0)
i
, ¯(cid:96)i, t0) − KD| ¯Ni|/2 · log(2πε)] d¯(cid:96)i

(a)= λi ¯R−1

i

¯Bi(¯xi)(cid:62) · lim
ε↓0

exp[−KD| ¯Ni|/2 · log(2πε)] · ∇

¯x(0)
i

(cid:82) exp[− ˜Sε,λi

exp[−KD| ¯Ni|/2 · log(2πε)] · (cid:82) exp[− ˜Sε,λi

i

, ¯(cid:96)i, t0)] d¯(cid:96)i

(¯x(0)
i
, ¯(cid:96)i, t0)] d¯(cid:96)i

i
(¯x(0)
i

37

(b)= λi ¯R−1

i

¯Bi(¯xi)(cid:62) · lim
ε↓0

(cid:82) exp[− ˜Sε,λi

i

(c)= λi ¯R−1

i

¯Bi(¯xi)(cid:62) · lim
ε↓0

(cid:90)

i (¯(cid:96)i|¯x(0)
˜p∗

i

, t0) · ∇

¯x(0)
i

, ¯(cid:96)i, t0)] · ∇

(¯x(0)
i
(cid:82) exp[− ˜Sε,λi

¯x(0)
i
(¯x(0)
i
i
[− ˜Sε,λi
i

(¯x(0)
[− ˜Sε,λi
i
i
, ¯(cid:96)i, t0)] d¯(cid:96)i
(¯x(0)
i

, ¯(cid:96)i, t0)] d¯(cid:96)i

, ¯(cid:96)i, t0)] d ¯(cid:96)i

(d)= λi ¯R−1

i

¯Bi(d)(¯xi)(cid:62) · lim
ε↓0

(cid:90)

i (¯(cid:96)i|¯x(0)
˜p∗

i

, t0) · ˜ui(¯x(0)

i

, ¯(cid:96)i, t0) d¯(cid:96)i.

(a) follows from the fact that exp[−KD| ¯Ni|/2 · log(2πε)] is independent from the path variables
(¯x(0)
, ¯(cid:96)i); (b) employs the diﬀerentiation rule for exponential function and requires that the inte-
i
grand exp[− ˜Sε,λi
, ¯(cid:96)i);
(c) follows from the optimal path distribution ˜p∗

, ¯(cid:96)i, t0)] be continuously diﬀerentiable in ε and along the trajectory (¯x(0)

, t0) that satisﬁes

(¯x(0)
i

i

i

i (¯(cid:96)i|¯x(0)

i

i (¯(cid:96)i|¯x(0)
˜p∗

i

, t0) =

exp[− ˜Sε,λi
i
(cid:82) exp[− ˜Sε,λi

(¯x(0)
i
(¯x(0)
i

, ¯(cid:96)i, t0)]
, ¯(cid:96)i, t0)] d¯(cid:96)i

i

;

(d) follows the partitions ¯Bi(¯xi) = [0, ¯Bi(d)(¯xi)(cid:62)](cid:62) and −∇
¯(cid:96)i, t0)(cid:62), −∇

, ¯(cid:96)i, t0)(cid:62)](cid:62), and the initial control variable ˜ui(¯x(0)

˜Sε,λi
i

(¯x(0)
i

¯x(0)
i

˜Sε,λi
i

(¯x(0)
i

i

, ¯(cid:96)i, t0) = [−∇

˜Sε,λi
i

(¯x(0)
i

,

¯x(0)
i(n)
, ¯(cid:96)i, t0) is determined by

¯x(0)
i(d)

∇

¯x(0)
i(d)

˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) = ∇

(cid:20) φi(¯x(K)
i
λi

)

+

ε
λi

¯x(0)
i(d)

K−1
(cid:88)

qi(¯x(k)
i

, tk) +

1
2

K−1
(cid:88)

k=0

log det(H (k)

i

)

(75)

K−1
(cid:88)

k=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=0

+

ε
2

i(d) − ¯x(k)
¯x(k+1)
ε

i(d)

− ¯fi(d)(¯x(k)

i

, tk)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:13)
H (k)
i

(cid:21)

.

(cid:17)−1

In the following, we calculate the gradients in (75). Since the terminal cost φi(¯x(K)
constant, the ﬁrst gradient in (75) is zero, i.e. ∇

i

)/λi] = 0. When the immediate cost

) usually is a

[φi(¯x(K)
i

¯x(0)
i(d)

qi(¯x(0)
i

, t0) is a function of state ¯x(0)

i(d), the second gradient in (75) can be computed as follows

∇

¯x(0)
i(d)

ε
λi

K−1
(cid:88)

k=0

qi(¯x(k)
i

, tk) =

ε
λi

∇

¯x(0)
i(d)

qi(¯x(0)
i

, t0);

(76)

, t0) is not related to the value of ¯x(0)
when qi(¯x(0)
second gradient is then zero. The third gradient in (75) follows

i

i

, i.e. a constant or an indicator function, the

∇

¯x(0)
i(d)

1
2

K−1
(cid:88)

k=0

log det(H (k)

i

) =

1
2

∇

¯x(0)
i(d)

log det(H (0)

i

).

(77)

Letting α(k)

i = (¯x(k+1)

i(d) − ¯x(k)

i(d))/ε − ¯fi(d)(¯x(k)

i

, tk) and β(k)

i = (H (k)

i

)−1α(k)
i

, the gradient of the fourth

38

term in (75) satisﬁes

∇

¯x(0)
i(d)

=

ε
2

= −

= −

1
2
(cid:16)

K−1
(cid:88)

k=0

ε
2
(cid:20)(cid:18)

∇

(cid:13)
(cid:13)α(k)
(cid:13)

i

(cid:13)
2
(cid:13)
(cid:16)
(cid:13)
H (k)
i

(cid:17)−1 =

ε
2

· ∇

α(0)
i

(cid:19)

β(0)
i +

(cid:18)

∇

¯x(0)
i(d)

(α(0)

i )(cid:62)β(0)

i

¯x(0)
i(d)
(cid:19)

β(0)
i

(cid:21)

α(0)
i

β(0)
i −

¯x(0)
i(d)
ε
2
(cid:17)−1

(cid:20)(cid:18)

∇

(cid:19)

, t0)

¯fi(d)(¯x(0)
¯x(0)
i
i(d)
(cid:20)

i ∇

i − α(0)
β(0)
(cid:21) (cid:16)

(cid:17)−1

β(0)
i

¯x(0)
i(d)

¯fi(d)(¯x(0)
i )

H (0)
i

α(0)
i +

∇

¯x(0)
i(d)

H (0)
i

α(0)
i − ε

(78)

(cid:21)

ε
2

(cid:16)

α(0)
i

(cid:17)(cid:62) (cid:20)

∇

¯x(0)
i(d)

(cid:17)−1(cid:21)

(cid:16)

H (0)
i

α(0)
i

.

Detailed interpretations on the calculation of (78) can be found in [22, 55]. Meanwhile, after

substituting (78) into (74), one can verify that the integrals in (74) satisfy

(cid:90)

i (¯(cid:96)i|¯x(0)
˜p∗

i

(cid:20)

, t0) · ε

∇

(cid:21) (cid:16)

(cid:17)−1

H (0)
i

α(0)
i d(cid:96) = 0,

(cid:90)

i (¯(cid:96)i|¯x(0)
˜p∗

i

, t0) ·

(cid:16)

α(0)
i

ε
2

(cid:17)(cid:62) (cid:20)

∇

¯x(0)
i(d)

α(0)
i d(cid:96) = −

1
2

∇

¯x(0)
i(d)

log det(H (0)

i

).

¯fi(d)(¯x(0)
i )
(cid:17)−1(cid:21)

¯x(0)
i(d)
(cid:16)

H (0)
i

(79)

(80)

Substituting (75-80) into (74), we obtain the initial control variable ˜ui(¯x(0)

i

, ¯(cid:96)i, t0) as follows

˜ui(¯x(0)
i

, ¯(cid:96)i, t0) = −

ε
λi

∇

¯x(0)
i(d)

qi(¯x(0)
i

, t0) +

(cid:17)−1

(cid:16)

H (0)
i





i(d) − ¯x(0)
¯x(1)
ε

i(d)



− ¯fi(d)(¯x(0)
i

, t0)

 .

This completes the proof.

Appendix D: Relative Entropy Policy Search

The local REPS algorithm in subsystem ¯Ni alternates between two steps, learning the optimal
path distribution and updating the parameterized policy, till the convergence of the algorithm.

First, we consider the learning step realized by the optimization problem (37). Since we want to
minimize the relative entropy between the current approximate path distribution ˜pi(¯x(0)
, ¯(cid:96)i) and
, ¯(cid:96)i), the objective function of the learning step follows
the optimal path distribution ˜p∗

i

i (¯x(0)

i

arg min

˜pi

KL(˜pi(¯x(0)
(cid:90)

i

, ¯(cid:96)i) (cid:107) ˜p∗

i (¯x(0)

i

, ¯(cid:96)i))

(a)= arg max
˜pi
(b)= arg max
˜pi

˜pi(¯x(0)
i

, ¯(cid:96)i)

(cid:104)
log ˜p∗

i (¯(cid:96)i|¯x(0)

i ) + log µ(¯x(0)

i ) − log ˜pi(¯x(0)

i

(cid:105)

, ¯(cid:96)i)

d¯x(0)

i d¯(cid:96)i

(cid:90)

˜pi(¯x(0)
i

, ¯(cid:96)i)

(cid:104)

− ˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) − log ˜pi(¯x(0)

i

(cid:105)
, ¯(cid:96)i)

d¯x(0)

i d¯(cid:96)i.

i

, ¯(cid:96)i) = ˜p∗

(a) transforms the minimization problem to a maximization problem and adopts the identity
i (¯x(0)
˜p∗
i ), where time arguments are generally omitted in this appendix
for brevity; and (b) employs identity (33) and omits the terms that are independent from the path
variable ¯(cid:96)i, since these terms have no inﬂuence on the optimization problem. In order to construct

i ) · µi(¯x(0)

i (¯(cid:96)i|¯x(0)

39

the information loss from old distribution and avoid overly greedy policy updates [21,23], we restrict

the update rate with constraint

(cid:90)

, ¯(cid:96)i) log

˜pi(¯x(0)
i

, ¯(cid:96)i)
, ¯(cid:96)i)

˜pi(¯x(0)
i
˜qi(¯x(0)
i
where δ > 0 can be used as a trade-oﬀ between exploration and exploitation, and LHS is the relative
entropy between the current approximate path distribution ˜pi(¯x(0)
, ¯(cid:96)i) and the old approximate path
i ) = (cid:82) ˜pi(¯x(0)
distribution ˜qi(¯x(0)
, ¯(cid:96)i) d¯(cid:96)i needs to
match the initial distribution µi(¯x(0)
i ), which is known to the designer. However, this condition
could generate an inﬁnite number of constraints in optimization problem (37) and is too restrictive

, ¯(cid:96)i). Meanwhile, the marginal distribution ˜pi(¯x(0)

i d¯(cid:96)i ≤ δ,

d¯x(0)

i

i

i

for practice [23, 34, 50]. Hence, we relax this condition by only considering to match the state

feature averages of initial state
(cid:90)

˜pi(¯x(0)
i

, ¯(cid:96)i) · ψi(¯x(0)

i ) d¯x(0)

i d¯(cid:96)i =

(cid:90)

µi(¯x(0)

i ) · ψi(¯x(0)

i ) d¯x(0)

i = ˆψ(0)

i

,

i ) is a feature vector of initial state, and ˆψ(0)

where ψi(¯x(0)
vector subject to initial distribution µi(¯x(0)
and quadratic terms of initial states, ¯x(0)
of marginal distribution ˜pi(¯x(0)
following normalization constraint

i ). In general, ψ(¯x(0)
i(m) and ¯x(0)

i(m) ¯x(0)

i

is the expectation of the state feature
i ) can be a vector made up with linear
i(n), such that the mean and the covariance
i ). Lastly, we consider the

i ) match those of initial distribution µi(¯x(0)

(cid:90)

˜pi(¯x(0)
i

, ¯(cid:96)i) d¯x(0)

i d¯(cid:96)i = 1,

(81)

which ensures that ˜pi(¯x(0)

i

, ¯(cid:96)i) deﬁnes a probability distribution.

The optimization problem (37) can be solved analytically by the method of Lagrange multipliers.

Deﬁning the Lagrange multipliers κ > 0, η ∈ R and vector θ, the Lagrangian is

L = η + κδ + θ(cid:62) ˆψ(0)

i +

(cid:90)

(cid:34)
, ¯(cid:96)i)

˜pi(¯x(0)
i

− ˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i) − log ˜pi(¯x(0)

i

, ¯(cid:96)i) − η

(82)

− θ(cid:62)ψi(¯x(0)

i ) − κ log

(cid:35)

d¯x(0)

i d¯(cid:96)i.

, ¯(cid:96)i)
, ¯(cid:96)i)

˜pi(¯x(0)
i
˜qi(¯x(0)
i
, ¯(cid:96)i) in (37) by letting ∂L/∂ ˜pi(¯x(0)
i
i
i ) if and only if the derivative of

, ¯(cid:96)i) =

We can maximize the Lagrangian L and derive the maximizer ˜pi(¯x(0)
0. This condition will hold for arbitrary initial distributions µi(¯x(0)
the integrand in (82) is identically equal to zero, i.e.
(cid:105)

(cid:104)

− ˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i, t0) − (1 + κ)

1 + log ˜pi(¯x(0)

, ¯(cid:96)i)

i

− η − θ(cid:62)ψi(¯x(0)

i ) + κ log ˜qi(¯x(0)

i

¯(cid:96)i) = 0,

from which we can ﬁnd the maximizer ˜pi(¯x(0)
, ¯(cid:96)i)
in (38), we then determine the values of dual variables κ, η and θ by solving the dual problem.

, ¯(cid:96)i) as shown in (38). In order to evaluate ˜pi(¯x(0)

i

i

Substituting (38) into the normalization constraint (81), we have the identity
, ¯(cid:96)i) + θ(cid:62)ψi(¯x(0)
i )
1 + κ

(cid:18) 1 + κ + η
1 + κ

κ
1+κ · exp

˜qi(¯x(0)
i

(¯x(0)
i

˜Sε,λi
i

, ¯(cid:96)i)

exp

(cid:32)

=

−

(cid:19)

(cid:90)

(cid:33)

d¯x(0)

i d¯(cid:96)i,

(83)

40

which can be used to determine η provided the values of κ and θ. To ﬁgure out the values of κ

and θ, we solve the dual problem (39), where the objective function g(κ, θ) in (40) is obtained by

substituting (38) and (83) into (82)

g(κ,θ) (a)= η + κδ + θ(cid:62) ˆψ(0)

i + 1 + κ

= κδ + θ(cid:62) ˆψ(0)

i + (1 + κ)

1 + κ + η
1 + κ
(cid:90)

(b)= κδ + θ(cid:62) ˆψ(0)

i + (1 + κ) log

˜qi(¯x(0)
i

, ¯(cid:96)i)

κ
1+κ · exp

(cid:32)

−

˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)i) + θ(cid:62)ψi(¯x(0)
i )
1 + κ

(cid:33)

d¯x(0)

i d¯(cid:96)i.

(a) substitutes (38) into (82), and (b) substitutes (83) into the result. By applying the Monte
Carlo method and using the data set Yi = {(¯x(0)
i )}y=1,··· ,Y , the objective function (40) can be
approximated from sample trajectories by

, ¯(cid:96)[y]

i

g(κ, θ) = κδ + θ(cid:62) ˆψ(0)

i + (1 + κ) log

(cid:34)

1
Y

Y
(cid:88)

y=1

˜qi(¯x(0)
i

, ¯(cid:96)[y]
i )

κ
1+κ exp

(cid:32)

−

˜Sε,λi
i

(¯x(0)
i

, ¯(cid:96)[y]

i ) + ψ(cid:62)
1 + κ

i (¯x(0)

i ) · θ

(cid:33) (cid:35)
,

i

, ¯(cid:96)[y]

(¯x(0)
i

i ) and ψi(¯x(0)
where ˜Sε,λi
, ¯(cid:96)[y]
vector of sample trajectory (¯x(0)
with the current or initial policy by

i

i ) are respectively the generalized path value and state feature
i ) can be evaluated

i ), and the old path probability ˜qi(¯x(0)

, ¯(cid:96)[y]

i

˜qi(¯x(0)
i

, ¯(cid:96)[y]

i ) = µi(¯x(0)

i ) ·

K−1
(cid:89)

(cid:90)

k=0

pi(¯x(k+1)
i

, tk+1|¯x(k)

i

, ¯u(k)
i

, tk) · π(k)

i

(¯u(k)
i

|¯x(k)
i

) d¯u(k)
i

,

(84)

i

(¯u(k)
i

, ¯x(k)
i

i
|¯x(k)
i

and ¯x(k+1)
i

in (84) are from sample trajectory (¯x(0)

where the state variables ¯x(0)
the control policy π(k)
from updating step (85), and the controlled transition probability pi(¯x(k+1)
¯x(k+1)
)¯ui(¯x(k)
i
i
steps when deriving the uncontrolled transition probability in (71). When policy π(k)
sian, (84) can be analytically evaluated.

i );
) is given either as an initialization or an optimization result
, tk) with
) can be obtained by following the similar

, tk)ε + ¯Bi(¯x(k)

i + ¯fi(¯x(k)

, tk)ε, Σ(k)

, tk+1|¯x(k)

∼ N (¯x(k)

is Gaus-

, ¯u(k)
i

, ¯(cid:96)[y]

i

i

i

i

i

i

i

In the policy updating step, we can ﬁnd the optimal parameters χ∗(k)

minimizing the relative entropy between the joint distribution ˜pi(¯x(0)
i
(¯u(k)
joint distribution ˜pπ
i
, ˆΣ(k)
ˆb(k)
i
i
the following optimization problem, which is also a weighted maximum likelihood problem

, ¯(cid:96)i) generated by parametric policy π(k)
i
, ˆb(k)
i

). To determine the policy parameters χ(k)

i = (ˆa(k)

for Gaussian policy by
, ¯(cid:96)i) from learning step (37) and
) ∼ N (¯u(k)
|¯x(k)
i +
i
) at time tk, we need to solve

i ¯x(k)

i (¯x(0)

, ˆΣ(k)
i

, χ(k)
i

|ˆa(k)

i

i

i

i

χ∗(k)
i = arg min

χ(k)
i

KL(˜pi(¯x(0)

i

, ¯(cid:96)i) (cid:107) ˜pπ

i (¯x(0)

(a)= arg max

χ(k)
i

(b)
≈ arg max

χ(k)
i

(cid:90)

(cid:90)

˜pi(¯x(0)
i

, ¯(cid:96)i) · log

d¯x(0)

i d¯(cid:96)i

i

, ¯(cid:96)i))
i (¯x(k+1)
˜pπ
i
˜pi(¯x(k+1)
i
(¯u∗(k)
i

i

)

|¯x(k)
i
|¯x(k)
)
i
|¯x(k)
i

˜pi(¯x(0)
i

, ¯(cid:96)i) · log π(k)

, χ(k)
i

) d¯x(0)

i d¯(cid:96)i

(85)

41

(c)= arg max

χ(k)
i

Y
(cid:88)

y=1

˜pi(¯x(0)
i
˜qi(¯x(0)
i

, ¯(cid:96)[y]
i )
, ¯(cid:96)[y]
i )

· log π(k)

i

(¯u∗(k)
i

|¯x(k)
i

, χ(k)
i

)

(d)= arg max

χ(k)
i

Y
(cid:88)

y=1

d[y]
i

· log π(k)

i

(¯u∗(k)
i

|¯x(k)
i

, χ(k)
i

).

(a) converts the minimization problem to a maximization problem and replaces the joint distribu-

tions by the products of step-wise transition distributions; (b) employs the assumption that the

distribution of controlled transition equals the product of passive transition distribution and control
) = ˜pi(¯x(k+1)
, χ(k)
policy distribution [23], i.e. ˜pπ
), and the control
i
i = [ ¯Bi(d)(¯x(k)
action ¯u∗(k)
, tk)]/ε from control
aﬃne system dynamics maximizes the likelihood function; (c) approximates the integral by using
sample trajectories from ˜qi(¯x(0)
i ); (d) substitutes (38), and the weight of likelihood function is

|¯x(k)
) · π(k)
i
i
i
i(d) − ¯x(k)
)(cid:62)[¯x(k+1)

|¯x(k)
i
)]−1 ¯Bi(d)(¯x(k)

i (¯x(k+1)
)(cid:62) ¯Bi(d)(¯x(k)

i(d) − ε ¯fi(d)(¯x(k)

(¯u∗(k)
i

|¯x(k)
i

, ¯(cid:96)[y]

i

i

i

i

i

i

i = ˜qi(¯x(0)
d[y]

i

, ¯(cid:96)[y]
i )

−1
1+κ · exp

(cid:32)

−

˜Sε,λi
i

(¯x(0)
i

Constant terms are omitted in steps (a) to (d).

, ¯(cid:96)[y]

i ) + ψ(cid:62)
1 + κ

i (¯x(0)

i ) · θ

(cid:33)

.

Appendix E: Multi-Agent LSOC Algorithms

Algorithm 1 gives the procedures of distributed LSMDP algorithm introduced in Section 3.1.

Algorithm 1 Distributed LSMDP based on factorial subsystems
Input agent set D, communication network G, initial time t0, exit time tf , initial states xt0

i , exit states xtf
i ,

joint state-related costs qi(¯xi), exit costs φ(xtf

t ), weights on exit costs wi

j, and error bound (cid:15).

Initialize factorial subsystems ¯Ni∈D, and joint exit costs φi(¯xi).
Planning:

1: for i ∈ D = {1, · · · , N } do

/*Calculate joint desirability Zi(·) for subsystem ¯Ni*/

2:

Compute coeﬃcients Θ, Ω and ZB in (20) for subsystem ¯Ni. For distributed planning (22), partition
the coeﬃcients [I − Θ, ΩZB]j and calculate the projection matrices Pj for j ∈ ¯Ni.

3: while (cid:107)Z (n+1)
4:

I

− Z (n)

I (cid:107) > (cid:15) do

Update desirability ZI with (20). For distributed planning, exchange local solutions Z (n)
boring agents k ∈ Nj ∩ ¯Ni and update desirability Z (n+1)

with (22).

I

I,j with neigh-

5:

end while

/*Calculate control distribution for agent i*/

Compute the joint optimal control distribution ¯u∗

6:
7: Derive the local optimal control distribution u∗
8: end for

i (·|¯xi) of subsystem ¯Ni by (19).
i (·|¯xi) for agent i by marginalizing ¯u∗

i (·|¯xi).

Execution:
9: while t < tf or x /∈ B do
10:

for i ∈ D = {1, · · · , N } do

11:

Measure joint state ¯xi(t) by collecting state information from neighboring agents j ∈ Ni.

42

12:

13:

Sample control action or posterior state x(cid:48)

i from u∗

i (·|¯xi).

end for

14: end while

Algorithm 2 illustrates the procedures of sampling-based distributed LSOC algorithm introduced

in Section 3.2.

Algorithm 2 Distributed LSOC based on sampling estimator
Input agent set D, communication network G, initial time t0, exit time tf , initial states xt0
xtf
i , joint state-related costs qi(¯xi), control weight matrices ¯Ri, exit costs φ(xtf
costs wi
j

i , exit states
t ), and weights on exit

Initialize factorial subsystems ¯Ni∈D, and joint exit costs φi(¯xi).
Planning & Execution:
1: while t < tf or x /∈ B do
2:

for i ∈ D = {1, · · · , N } do

3:

4:

5:

6:

7:

8:

Measure joint state ¯xi(t) by collecting state information from neighboring agents j ∈ Ni.
Generate uncontrolled trajectory set Yi by sampling or collecting data from neighboring agents.
Evaluate generalized path value ˜Sε,λi
trajectory (¯x(0)
Approximate the optimal path distribution ˜p∗
by (35), (36) or other sampling techniques.
Select and execute local control action u∗

(¯x(0)
i
i ) in Yi by (31) and (34).

, t0) and joint optimal control action ¯u∗

, t0) and initial control ˜ui(¯x(0)

i (¯xi, t) from joint optimal control action ¯u∗

i (¯xi, t).

i (¯(cid:96)[y]

|¯x(0)
i

, ¯(cid:96)[y]
i

, ¯(cid:96)[y]
i

, ¯(cid:96)[y]

i

i

i

i

, t0) of each sample

i (¯xi, t)

end for

9: end while

Algorithm 3 illustrates the procedures of REPS-based distributed LSOC algorithm introduced in

Section 3.2.

Algorithm 3 Distributed LSOC based on REPS
Input agent set D, communication network G, initial time t0, exit time tf , initial states xt0
joint state-related costs qi(¯xi), control weight matrices ¯Ri, exit costs φ(xtf
and initial policy π(k)
).

i , exit states xtf
i ,
t ), weights on exit costs wi
j,

i

|¯x(k)
i

, χ(k)
i

(¯u(k)
i
Initialize factorial subsystems ¯Ni∈D, and joint exit costs φi(¯xi).
Planning & Execution:
1: while t < tf or x /∈ B do
2:

for i ∈ D = {1, · · · , N } do

3:

4:

5:

6:

7:

8:

9:

10:

Measure joint state ¯xi(t) by collecting state information from neighboring agents j ∈ Ni.
repeat

Generate trajectory set Yi by sampling with (initial) policy π(k)
neighboring agents j ∈ Ni.
Solve dual variables κ, θ and η from dual problem (39) and condition (83).
Compute path distribution ˜q(¯x(0)
Update parameter χ(k)

i ) of each trajectory in Yi by (84).

by solving weighted maximum likelihood problem (85).

, χ(k)
i

(u(k)
i

|¯x(k)
i

, ¯(cid:96)[y]

i

i

i
until convergence of parametric policy π(k)
Marginalize joint optimal control policy π(0)

i

(¯u(k)
i
(¯u(0)
i

|¯x(k)
i
|¯x(0)
i

, χ(k)
).
i
, χ(0)
i

i

) by (42).

) or collecting data from

43

11:

12:

Sample
π∗(0)
i
end for

(u∗(0)
i

and execute
, χ∗(0)
|¯x(0)
i
i

).

local

control

action u∗

i (¯xi, t)

from local

optimal

control policy

13: end while

References

[1] R. Mahony, V. Kumar, and P. Corke, “Multirotor aerial vehicles: Modeling, estimation, and

control of quadrotor,” IEEE Robotics & Systems Magazine, vol. 19, no. 3, pp. 20–32, 2012.

[2] V. Cichella, R. Choe, S. B. Mehdi, E. Xargay, N. Hovakimyan, V. Dobrokhodov, I. Kaminer,

A. M. Pascoal, and A. P. Aguiar, “Safe coordinated maneuvering of teams of multirotor un-

manned aerial vehicles: A cooperative control framework for multivehicle, time-critical mis-

sions,” IEEE Control Systems Magazine, vol. 36, no. 4, pp. 59–82, 2016.

[3] Y. Ota, H. Taniguchi, T. Nakajima, K. M. Liyanage, J. Baba, and A. Yokoyama, “Au-

tonomous distributed V2G (Vehicle-to-Grid) satisfying scheduled charging,” IEEE Transac-

tions on Smart Grid, vol. 3, no. 1, pp. 559–564, 2012.

[4] F. Blaabjerg, R. Teodorescu, M. Liserre, and A. V. Timbus, “Overview of control and grid

synchronization for distributed power generation systems,” IEEE Transactions on Industrial

Electronics, vol. 53, no. 5, pp. 1398–1409, 2006.

[5] J. M. Guerrero, M. Chandorkar, T. L. Lee, and P. C. Loh, “Advanced control architectures for

intelligent microgrids — Part I: Decentralized and hierarchical control,” IEEE Transactions

on Industrial Electronics, vol. 60, no. 4, pp. 1254–1262, 2013.

[6] F. Dorﬂer, J. W. Simpson-Porco, and F. Bullo, “Breaking the hierarchy: Distributed control

and economic optimality in microgrids,” IEEE Transactions on Control of Network Systems,

vol. 3, no. 3, pp. 241–253, 2016.

[7] P. Leitao, “Agent-based distributed manufacturing control: A state-of-the-art survey,” Engi-

neering Applications of Artiﬁcial Intelligence, vol. 22, no. 7, pp. 979–991, 2009.

[8] C. Amato, G. Chowdhary, A. Geramifard, N. K. Ure, and M. J. Kochenderfer, “Decentral-

ized control of partially observable Markov decision processes,” in 52nd IEEE Conference on

Decision and Control, Florence, Italy, 2013.

[9] A. Bensoussan, J. Frehse, and P. Yam, Mean Field Games and Mean Field Type Control

Theory. Springer, 2013.

[10] Y. Cao, W. Yu, W. Ren, and G. Chen, “An overview of recent progress in the study of

distributed multi-agent coordination,” IEEE Transactions on Industrial Informatics, vol. 9,

no. 1, pp. 427–438, 2013.

44

[11] F. L. Lewis, H. Zhang, K. Hengster-Movric, and A. Das, Cooperative Control of Multi-Agent

Systems: Optimal and Adaptive Design Approaches. Springer, 2013.

[12] K. K. Oh, M. C. Park, and H. S. Ahn, “A survey of multi-agent formation control,” Automatica,

vol. 53, pp. 424–440, 2015.

[13] J. Qin, Q. Ma, Y. Shi, and L. Wang, “Recent advances in consensus of multi-agent systems:

A brief survey,” IEEE Transactions on Industrial Electronics, vol. 64, no. 6, pp. 4972–4983,

2017.

[14] K. Zhang, Z. Yang, and T. Basar, “Multi-agent reinforcement learning: A selective overview

of theories and algorithms,” arXiv, no. 1911.10635, 2019.

[15] W. H. Fleming, “Exit probabilities and optimal stochastic control,” Applied Mathematics and

Optimization, vol. 4, no. 1, pp. 329–346, 1977.

[16] E. Todorov, “Linearly-solvable Markov decision problems,” in Advances in Neural Information

Processing Systems, Vancouver, Canada, 2007.

[17] Y. Pan, E. A. Theodorou, and M. Kontitsi, “Sample eﬃcient path integral control under

uncertainty,” in Advances in Neural Information Processing Systems, Montreal, Canada, 2015.

[18] E. Todorov, “Compositionality of optimal control laws,” in Advances in Neural Information

Processing Systems, Vancouver, Canada, 2009.

[19] A. Kupcsik, M. P. Deisenroth, J. Peters, L. A. Poh, P. Vadakkepat, and G. Neumann, “Model-

based contextual policy search for data-eﬃcient generalization of robot skills,” Artiﬁcial Intel-

ligence, vol. 247, p. 415–439, 2017.

[20] G. Williams, P. Drews, B. Goldfain, J. M. Rehg, and E. A. Theodorou, “Information-theoretic

model predictive control: Theory and applications to autonomous driving,” IEEE Transactions

on Robotics, vol. 34, no. 6, pp. 1603–1622, 2018.

[21] J. Peters, K. Mulling, and Y. Altun, “Relative entropy policy search,” in 24th AAAI Conference

on Artiﬁcial Intelligence, Atlanta, USA,, 2010.

[22] E. A. Theodorou, “A generalized path integral control approach to reinforcement learning,”

Journal of Machine Learning Research, no. 11, pp. 3137–3181, 2010.

[23] V. Gomez, H. J. Kappen, J. Peters, and G. Neumann, “Policy search for path integral control,”

in Joint European Conference on Machine Learning and Knowledge Discovery in Databases,

Dublin, Ireland, 2014.

45

[24] P. Guan, M. Raginsky, and R. M. Willett, “Online Markov decision processes with Kull-

back–Leibler control cost,” IEEE Transactions on Automatic Control, vol. 59, no. 6, pp. 1423–

1438, 2014.

[25] G. Williams, A. Aldrich, and E. A. Theodorou, “Model predictive path integral control: From

theory to parallel computation,” Journal of Guidance, Control, and Dynamics, vol. 40, no. 2,

pp. 344–357, 2017.

[26] C. Guestrin, D. Koller, and R. Parr, “Multiagent planning with factored MDPs,” in Advances

in Neural Information Processing Systems, Vancouver, Canada, 2002.

[27] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman, “Solving transition independent

decentralized Markov decision processes,” Journal of Artiﬁcial Intelligence Research, vol. 22,

p. 423–455, 2004.

[28] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforce-

ment learning with networked agents,” Proceedings of Machine Learning Research, vol. 80, pp.

5872–5881, 2018.

[29] K. Zhang, Z. Yang, and T. Basar, “Decentralized multi-agent reinforcement learning with

networked agents: Recent advances,” arXiv, no. 1912.03821, 2019.

[30] B. C. Daniel, “Cross-entropy method for Kullback-Leibler control in multi-agent systems,”

Master’s thesis, Universitat Pompeu Fabra, 2017.

[31] B. Broek, W. Wiegerinck, and B. Kappen, “Graphical model inference in optimal control of

stochastic multi-agent systems,” Journal of Artiﬁcial Intelligence Research, no. 32, pp. 95–122,

2008.

[32] R. P. Anderson and D. Milutinovic, “Stochastic optimal enhancement of distributed formation

control using Kalman smoothers,” Robotica, vol. 32, pp. 305–324, 2014.

[33] N. Wan, A. Gahlawat, N. Hovakimyan, E. A. Theodorou, and P. G. Voulgaris, “Cooperative

path integral control for stochastic multi-agent systems,” arXiv, no. 2009.14775, 2020.

[34] R. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT Press, 2018.

[35] R. Olfati-Saber, J. A. Fax, and R. M. Murray, “Consensus and cooperation in networked

multi-agent systems,” Proceedings of the IEEE, vol. 95, no. 1, pp. 215–233, 2007.

[36] E. Todorov, “Eigenfunction approximation methods for linearly-solvable optimal control prob-

lems,” in IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,

Nashville, USA, 2009.

46

[37] H. J. Kappen, “Linear theory for control of nonlinear stochastic systems,” Physical Review

Letters, no. 95, pp. 200 201–1–4, 2005.

[38] E. Todorov, “Eﬃcient computation of optimal actions,” Proceedings of the National Academy

of Sciences, vol. 106, no. 28, pp. 11 478–11 483, 2009.

[39] R. Johari and J. N. Tsitsiklis, “Eﬃciency loss in a network resource allocation game,” Mathe-

matics of Operations Research, vol. 29, no. 3, pp. 407–435, 2004.

[40] A. Nedic, A. Ozdaglar, and P. A. Parrilo, “Constrained consensus and optimization in multi-

agent networks,” IEEE Transactions on Automatic Control, vol. 55, no. 4, pp. 922–938, 2010.

[41] P. G. Voulgaris and N. Elia, “Social optimization problems with decentralized and selﬁsh

optimal strategies,” in 56th IEEE Conference on Decision and Control, Melbourne, Australia,

2017.

[42] S. Mou, J. Liu, and A. S. Morse, “A distributed algorithm for solving a linear algebraic

equation,” IEEE Transactions on Automatic Control, vol. 60, no. 11, pp. 2863–2878, 2015.

[43] J. Liu, S. Mou, and A. S. Morse, “Asynchronous distributed algorithms for solving linear

algebraic equations,” IEEE Transactions on Automatic Control, vol. 63, no. 2, pp. 372–385,

2018.

[44] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision and challenges,” IEEE

Internet of Things Journal, vol. 3, no. 5, pp. 637 – 646, 2016.

[45] K. Bakshi, D. Fan, and E. A. Theodorou, “Schr¨odinger approach to optimal control of large-

size populations,” arXiv, no. 1810.06064, 2018.

[46] K. Bakshi, P. Grover, and E. A. Theodorou, “On mean ﬁeld games for agents with Langevin

dynamics,” IEEE Transactions on Control Systems Technology, 2019.

[47] P. D. Moral, Feynman-Kac Formulae: Genealogical and Interacting Particle Systems with

Applications. Springer, 2004.

[48] E. A. Theodorou, “Nonlinear stochastic control and information theoretic dualities: Connec-

tions, interdependencies and thermodynamic interpretations,” Entropy, vol. 17, no. 5, pp.

3352–3375, 2015.

[49] G. Boutselis, M. Pereira, and E. A. Theodorou, “Variational inference for stochastic control

of inﬁnite dimensional systems,” arXiv, no. 1809.03035, 2018.

[50] A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann, “Data-eﬃcient generalization

of robot skills with contextual policy search,” in AAAI Conference on Artiﬁcial Intelligence,

Bellevue, USA, 2013.

47

[51] F. D. M. Da Silva and J. Popovic, “Linear Bellman combination for control of character

animation,” ACM Transactions on Graphics, vol. 28, no. 3, pp. 82:1–10, 2009.

[52] W. Yao, N. Qi, N. Wan, and Y. Liu, “An iterative strategy for task assignment and path

planning of distributed multiple unmanned aerial vehicles,” Aerospace Science and Technology,

vol. 86, pp. 455–464, 2019.

[53] L. Song, N. Wan, A. Gahlawat, N. Hovakimyan, and E. A. Theodorou, “Compositionality of

linearly solvable optimal control in networked multi-agent systems,” arXiv, no. 2009.13609,

2020.

[54] J. F. Le Gall, Brownian Motion, Martingales, and Stochastic Calculus. Springer, 2016.

[55] E. A. Theodorou, “Iterative path integral stochastic optimal control: Theory and applications

to motor control,” Ph.D. dissertation, Dept. of Computer Sci., Univ. of Southern California,

Los Angeles, California, USA, 2011.

48

