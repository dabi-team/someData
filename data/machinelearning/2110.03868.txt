Towards Learning (Dis)-Similarity of Source Code from Program
Contrasts

Yangruibo Ding§, Luca Buratti†, Saurabh Pujar†, Alessandro Morari†,
Baishakhi Ray§, and Saikat Chakraborty§
§ Columbia University
† IBM Research
§{yrbding, rayb, saikatc}@cs.columbia.edu
†{luca.buratti1, saurabh.pujar}@ibm.com, amorari@us.ibm.com

Abstract

Understanding the functional (dis)-similarity
of source code is signiﬁcant for code modeling
tasks such as software vulnerability and code
clone detection. We present DISCO (DIS-
similarity of COde), a novel self-supervised
model focusing on identifying (dis)similar
functionalities of source code. Different from
existing works, our approach does not re-
quire a huge amount of randomly collected
datasets. Rather, we design structure-guided
code transformation algorithms to generate
synthetic code clones and inject real-world se-
curity bugs, augmenting the collected datasets
in a targeted way. We propose to pre-train the
Transformer model with such automatically
generated program contrasts to better identify
similar code in the wild and differentiate vul-
nerable programs from benign ones. To better
capture the structural features of source code,
we propose a new cloze objective to encode
the local tree-based context (e.g., parents or
sibling nodes). We pre-train our model with
a much smaller dataset, the size of which is
only 5% of the state-of-the-art models’ train-
ing datasets, to illustrate the effectiveness of
our data augmentation and the pre-training ap-
proach. The evaluation shows that, even with
much less data, DISCO can still outperform
the state-of-the-art models in vulnerability and
code clone detection tasks.

1

Introduction

2
2
0
2

r
a

M
0
2

]
L
P
.
s
c
[

2
v
8
6
8
3
0
.
0
1
1
2
:
v
i
X
r
a

the

functional

similar-
Understanding
ity/dissimilarity of source code is at the core of
several code modeling tasks such as software
vulnerability and code clone detection, which
are important for software maintenance (Kim
et al., 2017; Li et al., 2016). Existing pre-trained
Transformer models (Guo et al., 2021; Feng
et al., 2020; Ahmad et al., 2021) show promises
for understanding code syntax (i.e., tokens and
structures). However, they still get confused when
trying to identify functional (dis)-similarities. For

instance, syntax-based models can embed two code
fragments with identical functionality but very
different tokens and structures as distinct vectors
and fail to identify them as semantically similar.
Likewise, these models cannot distinguish between
two code fragments that differ in functionalities but
share a close syntactic resemblance. For example,
consider an if statement if(len(buf) < N)
checking buffer length before accessing the buffer.
Keeping the rest of the program the same, if we
simply replace the token ‘<’ with ‘≤,’ the modiﬁ-
cation can potentially trigger security vulnerability,
e.g., buffer overﬂow bug1. It is challenging for
existing pre-training techniques to tell apart such
subtle differences in the functionalities.

In addition, existing pre-training techniques rely
on a huge volume of training corpus that is ran-
domly selected. For ﬁne-tuning tasks like code
clone detection or vulnerability detection, such ran-
dom selection of training data is never tailored to
teach the model about code functionalities.

To address these limitations, we present DISCO,
a self-supervised pre-trained model that jointly
learns the general representations of source code
and speciﬁc functional features for identifying
source code similarity/dis-similarity.
Similar
to state-of-the-art pre-trained Transformer mod-
els (Devlin et al., 2019; Liu et al., 2019), we apply
the standard masked language model (MLM) to
capture the token features of source code. To learn
about the structural code properties, we propose
a new auxiliary pre-training task that consumes
additional inputs of local tree-based contexts (e.g.,
parent or sibling nodes in abstract syntax trees)
and embeds such structural context, together with
the token-based contexts, into each token represen-
tation. On top of such well-learned general code
representations, we further incorporate prior knowl-
edge of code clones and vulnerable programs into
the pre-training to help the model learn the func-

1https://en.wikipedia.org/wiki/Buffer_overﬂow

 
 
 
 
 
 
tional (dis)-similarity. We design structure-guided
code transformation heuristics to automatically aug-
ment each training sample with one synthetic code
clone (i.e., positive samples) that is structurally dif-
ferent yet functionally identical and one vulnerable
contrast (i.e., hard negative samples) that is syn-
tactically similar but injected with security bugs.
During the pre-training, DISCO learns to bring
similar programs closer in the vector space and dif-
ferentiate the benign code from its vulnerable con-
trast, using a contrastive learning objective. Since
we augment the dataset in a more targeted way
than existing works and the model explicitly learns
to reason about a code w.r.t. its functional equiva-
lent and different counterparts during pre-training,
DISCO can learn sufﬁcient knowledge for down-
stream applications from a limited amount of data,
consequently saving computing resources. In par-
ticular, we evaluate DISCO for clone detection
and vulnerability detection, as the knowledge of
similar/dissimilar code fragments is at the core of
these tasks.

To this end, we pre-train DISCO on a small
dataset, with only 865 MB of C code and 992 MB
Java code from 100 most popular GitHub repos-
itories, and evaluate the model on four different
datasets for vulnerability and code clone detection.
Experiments show that our small models outper-
form baselines that are pre-trained on 20× larger
datasets. The ablation study (§5.4) also reveals that
pre-training our model with 10× larger datasets
further improves the performance up to 8.2%, out-
performing state-of-the-art models by 1% for iden-
tifying code clones and up to 9.6% for vulnerability
detection, even if our dataset is still smaller.

In summary, our contributions are: 1) We de-
sign structure-guided code transformation heuris-
tics to automatically augment training data to inte-
grate prior knowledge of vulnerability and clone
detection without human labels. 2) We propose
a new pre-training task to embed structural con-
text to each token embedding. 3) We develop
DISCO, a self-supervised pre-training technique
that jointly and efﬁciently learns the textual, struc-
tural, and functional properties of code. Even
though pre-trained with signiﬁcantly less data,
DISCO matches or outperforms the state-of-the-art
models on code clone and vulnerability detection.

2 Related Works

Pre-training for Source Code. Researchers have
been passionate about pre-training Transformer

models (Vaswani et al., 2017) for source code
with two categories: encoder-only and encoder-
decoder (Ahmad et al., 2021; Wang et al., 2021;
Rozière et al., 2021; Phan et al., 2021). Our work
focuses on pre-training encoder-only Transformer
models to understand code. Existing models are
pre-trained with different token level objectives,
such as masked language model (MLM) (Kanade
et al., 2020; Buratti et al., 2020), next sentence pre-
diction (NSP) (Kanade et al., 2020), replaced token
detection, and bi-modal learning between source
code and natural languages (Feng et al., 2020).
However, these approaches ignore the underlying
structural information to fully understand the syn-
tax and semantics of programming languages. Re-
cently, more works aimed to understand the strict-
deﬁned structure of source code leveraging abstract
syntax tree (AST) (Zügner et al., 2021; Jiang et al.,
2021), control/data ﬂow graphs (CFG/DFG) (Guo
et al., 2021). DISCO leverages code structures
differently from existing works in two ways: (a.)
with AST/CFG/DFG, we automatically generate
program contrasts to augment the datasets target-
ing speciﬁc downstream tasks. (b.) DISCO takes
an additional input of local AST context, and we
propose a new cloze task to embed local structural
information into each token representation.
Self-supervised Contrastive Learning.
Self-
supervised contrastive learning, originally pro-
posed for computer vision (Chen et al., 2020),
has gained much interest in language process-
ing (Giorgi et al., 2021; Wu et al., 2020; Gao et al.,
2021). The common practice of self-supervised
contrastive learning is building similar counter-
parts, without human interference, for the original
samples and forcing the model to recognize such
similarity from a batch of randomly selected sam-
ples. Corder (Bui et al., 2021) leverages contrastive
learning to understand the similarity between a pro-
gram and its functionally equivalent code. While
Corder approach will help code similarity detec-
tion type of applications, their pre-training does not
learn to differentiate syntactically very close, but
functionally different programs. Such differentia-
tion is crucial for models to work well for bug de-
tection (Ding et al., 2020). ContraCode (Jain et al.,
2020) also leverages contrastive learning. However,
they generate negative contrast for a program from
unrelated code examples, not from variants of the
same code. They also do not encode the structural
information into the code as we do. Inspired by the
empirical ﬁndings that hard negative image and text

samples are beneﬁcial for contrastive learning (Gao
et al., 2021; Robinson et al., 2021), DISCO learns
both from equivalent code as the positive contrast,
and functionally different yet syntactically close
code as the hard-negative contrast. We generate
hard-negative samples by injecting small but cru-
cial bugs in the original code (§3.1).

3 Data Augmentation Without Human

Labels

Our pre-training aims to identify similar programs
that can be structurally different (positive sample)
and differentiate the buggy programs (negative sam-
ple) that share structural resemblances with the be-
nign ones. Thus, we need a labeled positive and a
negative example for each original sample. Manu-
ally collecting them is expensive, especially at the
scale of pre-training. To this end, we design code
transformation heuristics to automatically gener-
ate such positive and negative samples so that the
transformation can be applied to any amount of
programs without human efforts.

We ﬁrst represent a code sample as Abstract Syn-
tax Tree (AST), and build a control/data ﬂow graph
from the AST. The code transformation heuristics
are then applied to this graph. For every original
code sample (x), we apply semantic preserving
transformation heuristics (§3.2) to generate a pos-
itive sample (x+) and a bug injection heuristics
(§3.1) to generate a hard-negative code example
(x−). We design the heuristics in a way that makes
x+ be the functional equivalent or semantic clone
of x and x− be the buggy/noisy version of x. Noted
that not all heuristics are applicable to all code sam-
ples; we decide on applicable heuristics based on
the ﬂow graph of the original code. Figure 1 shows
an example of the code transformation.

3.1 Bug Injection
To generate a hard negative sample (x−) from a
given code (x), we deﬁne six categories of bug in-
jection heuristics. Here our goal is to maintain max-
imum token-level similarity to the original code,
so that the model can learn to analyze source code
beyond token-level similarity. These heuristics are
inspired by the buggy code patterns from a wide
range of Common Weakness Enumeration (CWE)
types (Appendix A.1). While it is challenging to
guarantee that x− will exhibit vulnerability or se-
curity bug, our heuristics will force x− to exhibit
different functionality than x. Compared with a

concurrent work from Allamanis et al. (2021), our
methods are signiﬁcantly different. First, we focus
on concrete types of security bugs that have been
identiﬁed by the security experts, while they mainly
target regular bugs. Second, our scope is not only
bug detection but clone detection as well, and we
apply contrastive learning to differentiate the code
functionalities of code clones and vulnerabilities.

Misuse of Data Type. Usage of the wrong data
type can trigger several security ﬂaws. For instance,
using a smaller data type (e.g., short) to replace
a larger one (e.g., long) may result in an over-
ﬂow bug (e.g., CVE-2021-38094 (2021)). Such
errors are complicated to track since they are usu-
ally exhibited in input extremities (i.e., very large
or very small values). For languages allowing im-
plicit typecasting, such an incorrect type may even
cause imprecision, resulting in the unpredictable
behavior of the code. We intentionally change the
data types in x to inject potential bugs, while en-
suring the code can still be compiled (e.g., we will
not replace int with char).
Misuse of Pointer. Incorrect pointer usage is a ma-
jor security concern. Accessing uninitialized point-
ers may lead to unpredictable behavior. A NULL
pointer or freed pointer could lead to Null Pointer
Dereferencing vulnerability (e.g., CVE-2021-3449
(2021)). To inject such bugs, we randomly remove
the initialization expression during pointer declara-
tion, or set some pointers to NULL.
Change of Conditional Statements. Program-
mers usually check necessary preconditions using
if-statement before doing any safety-critical
operation. For instance, before accessing an array
with an index, a programmer may add a condi-
tion checking the validity of the index. Lack of
such checks can lead to buffer-overﬂow bugs in
code (e.g., CVE-2020-24020 (2020)). We intro-
duce bugs in the code by removing such small
if-statements.
In addition, we also inject
bugs by modifying randomly selected arithmetic
conditions— replace the comparison operator (<,
>, ≤, ≥, ==, ! =) with another operator, to inject
potential out-of-bound access, forcing the program
to deviate from its original behavior.

Misuse of Variables. When there are multiple
variables present in a code scope, incorrect use
of variables may lead to erroneous behavior of the
program. Such errors are known as VARMISUSE
bug (Allamanis et al., 2018). We induce code with
such bugs by replacing one variable with another.

(a) Original Code

(b) Functionally Equivalent Code

(c) Bug Injected Code

Figure 1: An example illustrating data augmentation. 1a shows the original code that is adapted from the CVE-
2021-38094 patch. 1b shows functionality equivalent code of 1a where the original code is transformed by renam-
ing and statements permutation. 1c shows a variation from 1a where a potential integer overﬂow bug is injected.

To keep the resultant code compilable, we perform
scope analysis on the AST and replace a variable
with another variable reachable in the same scope.
Misuse of Values. Uninitialized variables or vari-
ables with wrong values may alter the program
behaviors and consequently cause security ﬂaws
(e.g., CVE-2019-12730 (2019)). We modify the
original code by removing the initializer expression
of some variables. In addition, to induce the code
with divide-by-zero vulnerability, we iden-
tify the potential divisor variables from the ﬂow
graph and forcefully assign zero values to them
immediately before the division.
Change of Function Calls. We induce bugs in the
code by randomly changing arguments of function
calls. For a randomly selected function call, we add,
remove, swap, or assign NULL value to arguments,
forcing the code to behave unexpectedly.

3.2 Similar Code Generation
To generate positive samples (x+) from a given
code, we use three different heuristics. In this case,
our goal is to generate functionally equivalent code
while inducing maximum textual difference. These
heuristics are inspired by code clone literature (Fu-
naro et al., 2010; Sheneamer et al., 2018).
Variable Renaming. Variable renaming is a typi-
cal code cloning strategy and frequently happens
during software development (Ain et al., 2019). To
generate such a variant of the original code, we
either (a.) rename a variable in the code with a
random identiﬁer name or (b.) with an abstract
name such as VAR_i (Rozière et al., 2021). While
choosing random identiﬁer names, we only select
available identiﬁers in the dataset. We ensure that
both the deﬁnition of the variable and subsequent
usage(s) are renamed for any variable renaming.
We also ensure that a name is not used to rename
more than one variable.
Function Renaming. We rename function calls
with abstract names like FUNC_i. By doing this,

we make more tokens different compared with the
original code but keep the same syntax and seman-
tics. We do not rename library calls for the code
(e.g., memcpy() in C).

Noted that even if tokens like VAR_i and
FUNC_i are rare in normal code, the model will
not bias towards identifying samples with these
tokens as positive samples. The reason is that, as
shown in Figure 2, x+, y+ and z+ all potentially
have these abstract tokens, but the model learns to
move EM Bx closer to EM Bx+ and further from
EM By+ and EM Bz+, regardless of the existence
of abstract tokens.

Statement Permutation.
The relative order
among the program statements that are indepen-
dent of each other can be changed without altering
the code functionality. More speciﬁcally, we focus
on the variable declaration or initialization state-
ments. We ﬁrst conduct the dependency analysis to
identify a set of local variables that do not depend
on other values for initialization. Then we move
their declaration statements to the beginning of the
function and permute them.

4 DISCO

This section presents the model architecture, input
representation, and pre-training tasks. DISCO uses
a 12-layered Transformer encoder model similar to
BERT. We feed the model with both source code
text and structure (AST) information (§4.1). We
pre-train DISCO using three different pre-training
tasks (§4.2). Figure 2 depicts an example workﬂow
of DISCO. We randomly select tokens in the origi-
nal sample, mask them and their node types, and
then use the embedding of these masks to predict
them back. We further extract the sequence embed-
dings within a minibatch and contrast them based
on the code functionality.

Figure 2: An illustration of DISCO pre-training with a minibatch of three. The original code and its node types
will be randomly masked with [M ASK], and the ﬁnal representation of masked tokens will be used to recover
their source tokens and node types. The original code, say x, will also be transformed to build (x, x+, x−). Then
the triplet will be fed into the same Transformer encoder and get the embedding of each sequence with [CLS]
tokens for contrastive learning.

4.1

Input Representation

Source Code. Given a program (x), we apply a
lexical analyzer to tokenize it based on the lan-
guage grammar and ﬂatten the program as a to-
ken sequence (x1x2...xm, where xi is ith token in
the code). We further train a sentencepiece (Kudo
and Richardson, 2018) tokenizer based on such
ﬂattened code token sequences with vocabulary
size 20,000. We use this tokenizer to divide the
source code tokens into subtokens. We prepend
the subtoken sequence with a special token [CLS]
and append with a special token [SEP]. Finally,
DISCO converts the pre-processed code sequence
C = {[CLS], c1, c2, ..., ck, [SEP ]} to vectors
V src = {vsrc
[SEP ]} with
a token embedding layer.

2 , ..., vsrc

[CLS], vsrc

k , vsrc

1 , vsrc

Local AST Types. For every token in the input
code, we extract the node type (tt) from the syn-
tax tree. Since such types are all terminal node
types (e.g., keyword, identiﬁer, punctuation), we
do not get enough information about the structure
only with these types. In order to add more infor-
mation about the tree, we also extract its parent
type (pt) for each token. Such parent type provides
us with information about the structural context
of a token. For instance, when the parent type of
an identifier is Function-Declarator,
we know that the identiﬁer is a function name.
In contrast, when the identifier’s parent is
a Binary Expression, it should be a vari-
able. Consequently, we annotate each code sub-
token ci with a local AST-type token t = tt#pt.
It is worth noting that sub-tokens coming from

[CLS], vtype

the same code token will all have the same
type. Therefore, we have the AST-type sequence
for the code T = {[CLS], t1, t2, ..., tk, [SEP ]},
and DISCO converts it as vectors V type =
, ..., vtype
{vtype
[SEP ]} with a type
k
embedding layer. Appendix Table 7 shows an ex-
ample of code tokens and their AST types. DISCO
generates token representation vi of subtoken ci as
a sum of token embedding vsrc
and type embed-
ding vtype

. Thus, V = V src + V type.

, vtype
2

, vtype

1

i

i

4.2 Pre-training

We aim to pre-train the DISCO to learn the repre-
sentation of source code based on (a.) token-based
context, (b.) AST-based context, and (c.) code func-
tionality. In that spirit, we pre-train DISCO to opti-
mize on three different objectives, i.e., masked lan-
guage model (MLM), local AST node type-MLM
(NT-MLM), and Contrastive Learning (CLR).

For a given program x, we ﬁrst embed
the tokens and node-types to vectors V =
{v[CLS], v1, ..., v[SEP ]}. We optimize MLM loss
(LM LM ) (§4.2.1) and NT-MLM loss (LN T −M LM )
(§4.2.2) based on x. These two loss functions learn
about the textual and syntactic context of source
code. For every code sample x in a minibatch of in-
put, we generate a positive example x+ and a hard-
negative example x− using the heuristics described
in Section 3. We optimize CLR loss ( LCLR)
(§4.2.3) considering the original code and its posi-
tive and hard-negative counterparts. The ﬁnal loss
function to optimize for pre-training DISCO is

L(θ) = LM LM (θ) + LN T −M LM (θ) + LCLR(θ)

4.2.1 Encoding Token-based Context
We apply the standard masked language model to
the original code (x). Given a source code sequence
C, we randomly choose 15% of tokens and replace
them with a special token [M ASK] for 80% of
the time and a random token for 10% of the time
and leave the rest 10% unchanged. We record the
indices of masked token as locm, replaced token
as locr and unchanged tokens as locu for node-
type MLM. We deﬁne the union of these indices as
M = locm∪locr∪locu. MLM will learn to recover
the masked source code {ci|i ∈ M } given the
Transformer encoder’s output hi. We present the
loss for MLM as LM LM = (cid:80)
i∈M −logP (ci|hi)

4.2.2 Encoding AST-based Context
Token-based MLM re-builds the token using its
surrounding tokens and successfully encodes the
contextual information into each token represen-
tation. Motivated by MLM, we propose the tree-
based context-aware pre-training task, to encode
the structural context, such as parent, sibling, and
children nodes. As we have shown in Figure 2, we
ﬂatten the ASTs as sequences and we expect the
ﬂattened trees can preserve the local structure infor-
mation (i.e., sub-trees containing terminal nodes),
and existing work (Chakraborty et al., 2020; Hellen-
doorn et al., 2020) has empirically shown such po-
tentials. To this end, we introduce AST node-type
masked language model (NT-MLM). Given the cor-
responding AST-type sequence T of source code
C, we mask the AST types {tp|p ∈ locm} with
the special token [M ASK], and replace the AST
types {tq|q ∈ locr} with random tokens. Speciﬁ-
cally, by doing this, we make sure that if a source
code token is chosen to be masked or replaced, its
corresponding AST type will perform the same op-
eration. NT-MLM will learn to recover the masked
AST type {ti|i ∈ M } given the Transformer en-
coder’s output hi. We present the loss for NT-MLM
as LN T −M LM = (cid:80)

i∈M −logP (ti|hi)

A recent work, CodeT5 (Wang et al., 2021),
proposes to predict token type as well. However,
our new objective is different from them in both
high-level designs and the detailed implementation.
First, their objective only predicts one single token
type: identiﬁers, while our approach predicts all
possible AST types. Also, we do not only consider
the AST node type of tokens, but also include their
AST parents to embed the local sub-tree context
(§4.1). Second, CodeT5 implements the identiﬁer
tagging task as a binary classiﬁcation (0/1) for each

token, while our NT-MLM reconstructs the local
ASTs out of hundreds of distinct types.

4.2.3 Contrastive Learning
We adopt contrastive learning to focus on the func-
tional characteristics of code. With the structure-
guided code transformation algorithms in Section 3,
we are able to generate a positive sample (x+ in Fig-
ure 2) and a hard negative sample (x− in Figure 2)
for each program in the dataset. More speciﬁcally,
we have a minibatch of N programs, and for each
program, we extract the sequence representation
from the Transformer outputs h = h[CLS]. We will
augment every sequence in the minibatch with pos-
itive and negative samples, and then the minibatch
is extended to N triplets of (h, h+, h−). We refer
to the contrastive loss with hard negative samples
from Gao et al. (2021) and we adapt it to our scope
as follows. We use cosine similarity as the sim()
function and τ is the temperature parameter to scale
the loss, and we use τ = 0.05.

LCLR = − log

(cid:16)

(cid:80)N

n=1

esim(h,h+)/τ

esim(h,h+

n )/τ + esim(h,h−

n )/τ (cid:17)

We also consider to pre-train the model with only
positive counterparts as a variation. In such a case,
the minibatch will contain N pairs of (h, h+) and
the loss is computed as

esim(h,h+)/τ
esim(h,h+

(cid:16)

n )/τ (cid:17)

(cid:80)N

n=1

LCLR = − log

5 Experiments

In this section, we will explain our experimental
settings and report the results. We evaluate our
model on vulnerability and code clone detection.

5.1 Experimental Settings

Data. We collect our pre-training corpus from
open-source C and Java projects. We rank Github
repositories by the number of stars and focus on
the most popular ones. After ﬁltering out forks
from existing repositories, we collect the dataset
for each language from top-100 repositories. We
only consider the “.java” and “.c” ﬁles for Java and
C repositories respectively, and we further remove
comments and empty lines from these ﬁles. The
corresponding datasets for Java and C are of size
of 992MB and 865MB, respectively. Our datasets
are signiﬁcantly smaller than existing pre-training
models (Feng et al., 2020; Ahmad et al., 2021; Guo

et al., 2021). For example, while CodeBERT and
GraphCodeBERT are trained on 20GB data, we
used an order of magnitude less data. Details of
our datasets and the comparison can be found in
Appendix Table 5.
Models. To study the different design choices,
we
(i)
train four variations of DISCO.
MLM+CLR±+NT-MLM is trained by all
three tasks with hard negative samples.
(ii)
MLM+CLR±. The input of this model only
considers the source code sequence and ignores the
AST-type sequence. This model helps us under-
stand the impact of NT-MLM. (iii) MLM+CLR+.
This variant evaluates the effectiveness of hard neg-
ative code samples, by contrasting its performance
with MLM+CLR±. (iv) MLM. This is the baseline
trained with only MLM objective. We provide
detailed model conﬁguration in Appendix A.4 to
ensure the reproducibility.
Baselines. We consider two types of baselines:
encoder-only pre-trained Transformers and exist-
ing deep-learning tools designed for code clone
and vulnerability detection. We do not consider
encoder-decoder pre-trained Transformers as base-
lines, since such generative models always need
much more pre-training data and training steps to
converge, so it is unfair to compare our model with
them. For example, PLBART uses 576G source
code for pre-training, while we only use less than
1G. Based on the data size. As future work, we
plan to pre-train the model on much larger datasets.

5.2 Vulnerability Detection (VD)

VD is the task to identify security bugs: given a
source code function, the model predicts 0 (benign)
or 1 (vulnerable) as binary classiﬁcation.
Dataset and Metrics. We consider two datasets
for VD task: REVEAL (Chakraborty et al., 2021)
and CodeXGLUE (Lu et al., 2021; Zhou et al.,
2019). In the real-world scenario, vulnerable pro-
grams are always rare compared to the normal ones,
and Chakraborty et al. (2021) have shown such im-
balanced ratio brings challenges for deep-learning
models to pinpoint the bugs. To imitate the real-
world scenario, they collect REVEAL dataset from
Chromium (open-source project of Chrome) and
Linux Debian Kernel, which keeps the ratio of vul-
nerable to benign programs to be roughly 1:10.
Following Chakraborty et al. (2021), we consider
precision, recall and F1 as the metrics.

CodeXGLUE presents another dataset of secu-

rity vulnerabilities. It is less real-world than RE-
VEAL, since it a balanced dataset, but it has been
frequently used by existing Transformer-based
models to evaluate their tools for VD task. To com-
pare with these baselines, we use CodeXGLUE
train/valid/test splits for training and testing. We
use accuracy as the metric, following the design of
the benchmark.
REVEAL. Table 1 shows the results. We compare
with four deep-learning-based VD tools. VulDeeP-
ecker (Li et al., 2018b) and SySeVR (Li et al.,
2018a) apply program slices and sequence-based
RNN/CNN to learn the vulnerable patterns. De-
vign (Zhou et al., 2019) uses graph-based neural
networks (GNN) to learn the data dependencies of
program. REVEAL (Chakraborty et al., 2021) ap-
plies GNN + SMOTE (Chawla et al., 2002) + triplet
loss during training to handle the imbalanced dis-
tribution. We also consider pre-trained RoBERTa,
CodeBERT and GraphCodeBERT, and a 12-Layer
Transformer model trained from scratch.

Table 1: Vulnerability Detection Results on REVEAL.

Model
VulDeePecker
SySeVR
Devign
REVEAL
Transformer
RoBERTa
CodeBERT
GraphCodeBERT
DISCO
MLM
MLM+CLR+
MLM+CLR±
MLM+CLR±+NT-MLM

Prec. (%) Rec. (%)

17.7
24.5
34.6
30.8
41.6
44.5
44.6
47.9

45.5
38.6
39.4
48.3

13.9
40.1
26.7
60.9
45.3
39.0
45.8
43.9

31.0
47.7
50.5
44.6

F1 (%)
15.7
30.3
29.9
41.3
43.4
41.6
45.2
45.8

36.9
42.6
44.2
46.4

Table 2: Results on CodeXGLUE for vulnerability detection

Model
Transformer
RoBERTa
CodeBERT
GraphCodeBERT
C-BERT
DISCO
MLM
MLM+CLR+
MLM+CLR±
MLM+CLR±+NT-MLM

Acc (%)
62.0
61.0
62.1
63.2
63.6∗

61.8
64.4
63.6
63.8

*We take this result from Buratti et al. (2020). They did not use CodeXGLUE
splits, so the test data can be different with other baselines.

In our case, the best DISCO variation with con-
trastive learning and NT-MLM objective outper-
forms all the baselines, including the graph-based
approaches and models pre-trained with larger
datasets. This empirically proves that DISCO can

efﬁciently understand the code semantics and data
dependencies from limited amount of data, helping
the identiﬁcation of the vulnerable patterns. We
notice that hard negative samples (i.e., buggy code
contrasts) helps DISCO improve the performance.
The reason is that REVEAL contains thousands of
(buggy version, ﬁxed version) pairs for the same
function. Two functions in such a pair are different
by only one or a few tokens. Such real-world chal-
lenges align well with our automatically generated
buggy code, and pre-training with these examples
teaches the model better distinguish the buggy code
from the benign ones. We provide an example in
Appendix Figure 3 to illustrate this.
CodeXGLUE. We consider four pre-trained mod-
els: RoBERTa, CodeBERT, GraphCodeBERT and
C-BERT. The ﬁrst three are pre-trained on much
larger datasets than ours. However, even trained
with small dataset, three variations of DISCO
outperforms the baselines. Unlike REVEAL,
CodeXGLUE does not have those challenging pairs
of functions’ buggy and patched version; thus the
hard negative contrast in DISCO does not help the
model much.
Table 3: Clone detection on POJ104 and BigCloneBench

Model

Transformer
MISIM-GNN
RoBERTa
CodeBERT∗
GraphCodeBERT∗
DISCO
MLM
MLM+CLR+
MLM+CLR±
MLM+CLR±+NT-MLM

POJ104

BigCloneBench

MAP@R Prec.(%) Rec.(%)
-
-
-
93.4
95.2

62.11
82.45
76.67
82.67
-

-
-
-
94.7
94.8

83.32
82.44
82.73
82.77

93.4
93.9
95.1
94.2

93.8
93.7
93.3
94.6

F1(%)
-
-
-
94.1
95.0

93.6
93.8
94.2
94.4

*The authors of both works ﬁxed bugs in their evaluation tool and updated the
results in their Github repositories. We directly take their latest results and use
their latest evaluation tool for fair comparisons.

5.3 Clone Detection

Clone detection aims to identify the programs with
similar functionality. It also can help detecting se-
curity vulnerabilities—given a known vulnerability,
we can scan the code base with clone detector and
check for similar code snippets.
Dataset and Metrics. We consider POJ-104 (Mou
et al., 2016) and BigCloneBench (Svajlenko et al.,
2014) as the evaluation datasets. We again strictly
follow the CodeXGLUE train/dev/test splits for ex-
periments. Following CodeXGLUE’s design, we
use MAP@R as the metric for POJ-104 and preci-
sion/recall/F1 as the metric for BigCloneBench.
POJ-104. We consider three pre-trained models,
one graph-based model (Ye et al., 2020) and one

12-layer Transformer model trained from scratch
as baselines. Table 3 shows that, with hard nega-
tive contrast and NT-MLM, DISCO outperforms
all baselines including CodeBERT, which is pre-
trained on much larger datasets. This highlights
the signiﬁcance of learning the code contrasts to-
gether with syntactical information to better cap-
ture the functional similarities. Interestingly, we
notice that DISCO-MLM performs the best among
all variations. This indicates that our current pos-
itive heuristics might not align with all the clone
patterns in this benchmark. As future work, we will
propose more code transformation rules to imitate
more real-world clone patterns.

BigCloneBench. Our best model achieves slightly
better precision than the baselines indicating that
our designs with contrastive learning and structure
information can compensate the loss brought by
less data. However, our recall is slightly worse
than GraphCodeBERT, since they are pre-trained
on large datasets with code graph. We conclude
that enlarging our Java pre-training dataset is nec-
essary for code clone detection and we regard this
as future work.

5.4 Medium Pre-trained Model

As shown in Section 5, DISCO trained on a small
dataset achieves comparable or even better perfor-
mance than models pre-trained on large datasets
in vulnerability and clone detection (Let’s call this
version DISCOsmall). We further explore the bene-
ﬁts of pre-training using larger data. We pre-train a
MEDIUM model, DISCOmedium, on our extended
datasets with more C-language Github reposito-
ries (13G). Note that our medium dataset is still
smaller than the large dataset of the baseline mod-
els (13G vs. 20G). We evaluate DISCOmedium on
C-language tasks. The results are shown in Table 4.
Increasing the pre-training dataset improves the
performance of downstream tasks, outperforming
the best baselines’ results.

Table 4: Results for the best baselines, small- and medium-
DISCO. POJ-104 is for code clone task; VD-CXG and VD-
RV are VD tasks for CodeXGLUE and REVEAL datasets.

Model

DISCOsmall
DISCOmedium
Baselinelarge

POJ-104 VD-CXG VD-RV
(F1)
46.4
50.2
45.8

(MAP@R)
82.8
83.8
82.7

(Acc)
63.8
64.6
63.6

6 Conclusion

In this work, we present DISCO, a self-supervised
contrastive learning framework to both learn the
general representations of source code and speciﬁc
characteristics of vulnerability and code clone de-
tections. Our evaluation reveals that DISCO pre-
trained with smaller dataset can still outperform
the large models’ performance and thus prove the
effectiveness of our design.

Acknowledgements

We would appreciate the insightful feedback and
comments from the anonymous reviewers. This
work was partially done when Yangruibo Ding was
an intern at IBM Research. This work is also sup-
ported in part by NSF grants CCF-2107405, CCF-
1845893, IIS-2040961, and IBM.

Ethical Considerations

The main goal of DISCO is
to generate
functionality-aware code embeddings, producing
similar representations for code clones and differ-
entiating security bugs from the benign programs.
Our data is collected from either the open-source
projects, respecting corresponding licences’ restric-
tions, or publicly available benchmarks. Mean-
while, throughout the paper we make sure to sum-
marize the paper’s main claims. We also discussed
DISCO’s limitation and potential future work for
clone detection in Section 5.3. We report our
model conﬁgurations and experiment details in Ap-
pendix A.4.

References

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-Wei Chang. 2021. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 2655–2668,
Online. Association for Computational Linguistics.

Qurat Ul Ain, Wasi Haider Butt, Muhammad Waseem
Anwar, Farooque Azam, and Bilal Maqbool. 2019.
A systematic review on code clone detection. IEEE
Access, 7:86121–86144.

Miltiadis Allamanis, Marc Brockschmidt, and Mah-
moud Khademi. 2018. Learning to represent pro-
grams with graphs. In International Conference on
Learning Representations.

Miltiadis Allamanis, Henry Jackson-Flux, and Marc
Brockschmidt. 2021. Self-supervised bug detection
and repair. In NeurIPS.

Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021.
Self-supervised contrastive learning for code re-
trieval and summarization via semantic-preserving
transformations. In SIGIR ’21, page 511–521.

Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott
McCarley, Yunhui Zheng, Gaetano Rossiello,
Alessandro Morari, Jim Laredo, Veronika Thost,
Yufan Zhuang, and Giacomo Domeniconi. 2020.
Exploring software naturalness through neural lan-
guage models.

S. Chakraborty, Y. Ding, M. Allamanis, and B. Ray.
2020. Codit: Code editing with tree-based neural
models. IEEE Transactions on Software Engineer-
ing, pages 1–1.

Saikat Chakraborty, Rahul Krishna, Yangruibo Ding,
and Baishakhi Ray. 2021. Deep learning based vul-
nerability detection: Are we there yet. IEEE Trans-
actions on Software Engineering, pages 1–1.

Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall,
and W. Philip Kegelmeyer. 2002. Smote: Synthetic
minority over-sampling technique. J. Artif. Int. Res.,
16(1):321–357.

Ting Chen, Simon Kornblith, Mohammad Norouzi,
and Geoffrey Hinton. 2020. A simple framework
for contrastive learning of visual representations. In
Proceedings of the 37th International Conference on
Machine Learning, pages 1597–1607. PMLR.

CVE-2019-12730.

2019.

https://nvd.nist.gov/vuln/detail/CVE-2019-12730.

CVE-2020-24020.

2020.

https://nvd.nist.gov/vuln/detail/CVE-2020-24020.

CVE-2021-3449.

2021.

https://nvd.nist.gov/vuln/detail/CVE-2021-3449.

CVE-2021-38094.

2021.

https://nvd.nist.gov/vuln/detail/CVE-2021-38094.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1, pages 4171–4186, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.

Yangruibo Ding, Baishakhi Ray, Devanbu Premkumar,
Patching as
and Vincent J. Hellendoorn. 2020.
In 35th
translation:
IEEE/ACM International Conference on Automated
Software Engineering, ASE ’20.

the data and the metaphor.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
BERT: A pre-trained model for programming and
In Findings of the Association
natural languages.
for Computational Linguistics: EMNLP 2020, pages
1536–1547, Online. Association for Computational
Linguistics.

Marco Funaro, Daniele Braga, Alessandro Campi, and
Carlo Ghezzi. 2010. A hybrid approach (syntactic
In Proceedings of
and textual) to clone detection.
the 4th International Workshop on Software Clones,
pages 79–80.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence
embeddings. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.
2021. DeCLUTR: Deep contrastive learning for
In Proceed-
unsupervised textual representations.
ings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 879–895,
Online. Association for Computational Linguistics.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie LIU, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin Clement, Dawn Drain, Neel
Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.
2021. Graphcode{bert}: Pre-training code represen-
tations with data ﬂow. In International Conference
on Learning Representations.

Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh,
Petros Maniatis, and David Bieber. 2020. Global
In International
relational models of source code.
Conference on Learning Representations.

Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel,
Joseph E. Gonzalez, and Ion Stoica. 2020. Con-
arXiv
trastive code representation learning.
preprint.

Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li,
and Lei Lyu. 2021. Treebert: A tree-based pre-
trained model for programming language. ArXiv,
abs/2105.12485.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,
and Kensen Shi. 2020. Learning and evaluating con-
textual embedding of source code. In ICML 2020.

Methods in Natural Language Processing: System
Demonstrations, pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.

Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Hanchao
Qi, and Jie Hu. 2016. Vulpecker: An automated vul-
nerability detection system based on code similarity
In Proceedings of the 32nd Annual Con-
analysis.
ference on Computer Security Applications, ACSAC
’16, page 201–213.

Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei
Zhu, and Zhaoxuan Chen. 2018a. Sysevr: A frame-
work for using deep learning to detect software vul-
nerabilities.

Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai
Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong.
2018b. Vuldeepecker: A deep learning-based sys-
In Proceedings of
tem for vulnerability detection.
the 25th Annual Network and Distributed System Se-
curity Symposium (NDSS‘2018).

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR, abs/1907.11692.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
CoRR, abs/2102.04664.

Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.
2016. Convolutional neural networks over tree struc-
tures for programming language processing. In Pro-
ceedings of the Thirtieth AAAI Conference on Artiﬁ-
cial Intelligence, pages 1287–1293.

Long N. Phan, Hieu Tran, Daniel Le, Hieu Nguyen,
James T. Anibal, Alec Peltekian, and Yanfang Ye.
2021. Cotext: Multi-task learning with code-text
transformer. CoRR, abs/2105.08645.

Joshua David Robinson, Ching-Yao Chuang, Suvrit
Sra, and Stefanie Jegelka. 2021. Contrastive learn-
In International
ing with hard negative samples.
Conference on Learning Representations.

Seulbae Kim, Seunghoon Woo, Heejo Lee, and Hakjoo
Oh. 2017. Vuddy: A scalable approach for vulnera-
ble code clone discovery. In 2017 IEEE Symposium
on Security and Privacy (SP), pages 595–614.

Baptiste Rozière, Marie-Anne Lachaux, Marc
Szafraniec, and Guillaume Lample. 2021. DOBF:
A deobfuscation pre-training objective for program-
ming languages. CoRR, abs/2102.07492.

Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical

Abdullah Sheneamer, Swarup Roy, and Jugal Kalita.
2018. A detection framework for semantic code
clones and obfuscated code. Expert Systems with
Applications, 97:405–420.

Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo,
Chanchal K Roy, and Mohammad Mamun Mia.
2014. Towards a big data curated benchmark of
In 2014 IEEE Interna-
inter-project code clones.
tional Conference on Software Maintenance and
Evolution, pages 476–480. IEEE.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 31st International
Conference on Neural Information Processing Sys-
tems, NIPS’17, page 6000–6010.

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H.
Hoi. 2021. Codet5:
Identiﬁer-aware uniﬁed pre-
trained encoder-decoder models for code under-
standing and generation. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2021.

Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian
Khabsa, Fei Sun, and Hao Ma. 2020. CLEAR: con-
trastive learning for sentence representation. CoRR,
abs/2012.15466.

Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Mar-
cus, Nesime Tatbul, Jesmin Jahan Tithi, Paul Pe-
tersen, Timothy G. Mattson, Tim Kraska, Pradeep
Dubey, Vivek Sarkar, and Justin Gottschlich. 2020.
MISIM: an end-to-end neural code similarity system.
CoRR, abs/2006.05265.

Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning
Du, and Yang Liu. 2019. Devign: Effective vul-
nerability identiﬁcation by learning comprehensive
program semantics via graph neural networks.
In
Advances in Neural Information Processing Systems,
pages 10197–10207.

Daniel Zügner, Tobias Kirschstein, Michele Catasta,
Jure Leskovec, and Stephan Günnemann. 2021.
Language-agnostic representation learning of source
In International
code from structure and context.
Conference on Learning Representations (ICLR).

A Appendix

A.1 Bug Injection Heuristics and Common

Weakness Enumeration Types

Our automated bug injection heuristics are moti-
vated by the real-world security bugs that are al-
ways small but hazardous. We empirically con-
clude the frequently happened vulnerable patterns
based on the concrete CWE types. Table 6 shows
that each of our operation is relating to several
CWE types. We inject all these security issues au-
tomatically and ask model to distinguish them with
the benign samples.

Table 5: Comparison of pre-training dataset size be-
tween ours and other related work

Dataset
DISCO
Java SMALL
C SMALL
C MEDIUM
CodeBERT
GraphCodeBERT
CuBERT
DOBF
PLBART

Instance Count Total Size

187 K
64 K
860 K
8.5 M
2.3 M
7.4 M
-
-

992 MB
865 MB
12 GB
20 GB
20 GB
-
45 GB
576 GB

A.2 Node Type Details
We parse the source code into ASTs and extract
the node type and parent node type for each token.
Table 7 shows an example after parsing. We can
see that, with the parent node type, each token can
be well embedded with its local structure contexts.
Considering two tokens that are distant from each
other: if and else. With only node types, we
just know these two tokens are keywords, but with
parent node type, we can easily know that they
are from the same if-statement and they are
siblings in the AST.

A.3 Dataset
Pre-training We collect our dataset from C and
Java Github repositories. Our main dataset is the
combination of Java SMALL and C SMALL. From
Table 5, we can see that our dataset is signiﬁcantly
smaller than the existing pre-trained models. For
an ablation study (§ 5.4) with enlarged dataset, we
collect a MEDIUM dataset of C language. We have
seen the improvement using such larger dataset,
but even MEDIUM dataset is still much smaller than
other datasets.
Datasets for downstream tasks We provide
dataset details of our downstream tasks in Table 8.
Noted that for POJ-104 (Mou et al., 2016), Table 8
only shows the number of code samples, and we fol-
low the design of CodeXGLUE that build positive
and negative pairs during the minibatch generation.
The amount of pairs for training is much larger than
the number of samples.

A.4 Conﬁguration
DISCO is built based on a stack of 12 layers
transformer encoder with 12 attention heads and
768 hidden sizes. The model is implemented
with PyTorch-1.9.0 and Huggingface-transformer-
4.12.3 2. Longer sequences are disproportionately

2https://github.com/

huggingface/transformers/tree/
c439752482759c94784e11a87dcbf08ce69dccf3

Table 6: Common Weakness Enumeration (CWE) types covered by our bug injection heuristic

Operation

Misuse of Data Type

Misuse of Pointer

Change of Conditional Statements

Misuse of Values

Change of Function Calls

Potential CWE types
CWE-190: Integer overﬂow
CWE-191: Integer Underﬂow
CWE-680: Integer Overﬂow to Buffer Overﬂow
CWE-686: Function Call With Incorrect Argument Type
CWE-704: Incorrect Type Conversion or Cast
CWE-843: Access of Resource Using Incompatible Type
CWE-476: NULL Pointer Dereference
CWE-824: Access of Uninitialized Pointer
CWE-825: Expired Pointer Dereference
CWE-120: Buffer Overﬂow
CWE-121: Stack-based Buffer overﬂow
CWE-122: Heap-based Buffer overﬂow
CWE-124: Buffer Underﬂow
CWE-125: Out-of-bounds Read
CWE-126: Buffer Over-read
CWE-129: Improper Validation of Array Index
CWE-787: Out-of-bounds Write
CWE-788: Access of Memory Location After End of Buffer
CWE-823: Use of Out-of-range Pointer Offset
CWE-369: Divide By Zero
CWE-456: Missing Initialization of a Variable
CWE-457: Use of Uninitialized Variable
CWE-908: Use of Uninitialized Resource
CWE-683: Function Call With Incorrect Order of Arguments
CWE-685: Function Call With Incorrect Number of Arguments
CWE-686: Function Call With Incorrect Argument Type
CWE-687: Function Call With Incorrectly Speciﬁed Argument Value
CWE-688: Function Call With Incorrect Variable or Reference

token
int
foo
(
int
bar
)
{
if
(
bar
<
5

Task

Table 7: Examples for tokens and their AST node types

node type
type
identiﬁer
punctuation
type
identiﬁer
punctuation
punctuation
keyword
punctuation
identiﬁer
operator
number_literal

parent node type
func_deﬁnition
func_declarator
param_list
parameter_declaration
parameter_declaration
parameter_list
compount_stmt
if_stmt
parenthesized_expr
binary_expr
binary_expr
binary_expr

token
)
return
(
1
)
;
else
return
(
0
)
;
}

node type
punctuation
keyword
punctuation
number_literal
punctuation
punctuation
keyword
keyword
punctuation
number_literal
number_literal
punctuation
punctuation

parent node type
parenthesized_expr
return_stmt
parenthesized_expr
parenthesized_expr
parenthesized_expr
return_stmt
if_stmt
return_stmt
parenthesized_expr
parenthesized_expr
parenthesized_expr
return_stmt
compount_stmt

Table 8: Details of downstream tasks datasets.

Dataset
Chakraborty et al. (2021)
Zhou et al. (2019)
Mou et al. (2016)
Svajlenko et al. (2014)

Language
C/C++
C/C++
C/C++
Java

Train
15,867
21,854
32,000
901,028

Valid
2,268
2,732
8,000
415,416

Test
4,535
2,732
12,000
415,416

Vulnerability Detection

Clone Detection

Figure 3: An example in REVEAL dataset. The patched code happens to be in the train split and the buggy code is
in the test split. During inference, DISCO MLM+CLR± model can correctly predict the buggy code as vulnerable,
while MLM+CLR+predicts it as benign.

... if (( ret = ff_get_buffer ( avctx, frame )) < 0 ) {     av_log ( avctx, AV_LOG_ERROR, STR ) ;     return ret ; } ... Buggy Code... if (( ret = ff_get_buffer ( avctx, frame , 0 )) < 0 ) {     av_log ( avctx, AV_LOG_ERROR, STR ) ;     return ret ; } ... Patched Codeexpensive so we follow the original BERT de-
sign by pre-training the model with short sequence
length for ﬁrst 70% steps and long sequence length
for the rest 30% steps to learn the positional em-
beddings. DISCO is trained with Java SMALL
and C SMALL for 24 hours in total with two 32GB
NVIDIA Tesla V100 GPUs, using batch size of 128
with max sequence length of 256 tokens and batch
size of 64 sequences with max sequence length 512
tokens. DISCO is also trained with C MEDIUM for
3 days, using batch size of 1024 sequences with
max sequence length of 256 tokens and batch size
of 512 sequences and max sequence length of 512
tokens. We use the Adam optimizer and 1e-4 as
the pre-training learning rate. For ﬁne-tuning tasks,
we use batch size of 8 and the learning of 8e-6.
We train the model with train split and evaluate the
model during the training using validation split. We
pick the model with best validation performance
for testing.

A.5 Case Study

We studied the model performance on REVEAL
dataset for vulnerability detection. Figure 3 shows
two samples inside REVEAL. We can recognize
that they are from the same program. We fur-
ther checked the details of these two example
and we found the code on the left is a buggy
version, and it is ﬁxed by adding an argument
of value 0 to the function call. This real-world
situation actually matches our "Change of Func-
tion Calls" (§ 3.1) bug injection operation.
In
the REVEAL dataset, the patched code is in the
training corpus while the buggy one is in the test
split. Interestingly, during the inference, DISCO
MLM+CLR± can correctly predict the buggyiess
while MLM+CLR+fails. This empirically prove
our bug injected samples can help the model iden-
tify small but siginicant real-world vulnerabilities.

