0
2
0
2

n
u
J

9
1

]

A
N
.
h
t
a
m

[

2
v
1
5
2
4
0
.
7
0
9
1
:
v
i
X
r
a

A divide-and-conquer algorithm for binary matrix
completion

Melanie Beckerleg1

Mathematical Institute, University of Oxford, Andrew Wiles Building, Woodstock Road,
Oxford, OX2 6GG, UK.

Andrew Thompson1,∗

Mathematical Institute, University of Oxford, Andrew Wiles Building, Woodstock Road,
Oxford, OX2 6GG, UK.

Abstract

We propose a practical algorithm for low rank matrix completion for matrices
with binary entries which obtains explicit binary factors and show it performs
well at the recommender task on real world datasets. The algorithm, which we
call TBMC (Tiling for Binary Matrix Completion), gives interpretable output
in the form of binary factors which represent a decomposition of the matrix into
tiles. Our approach extends a popular algorithm from the data mining com-
munity, PROXIMUS, to missing data, applying the same recursive partitioning
approach. The algorithm relies upon rank-one approximations of incomplete bi-
nary matrices, and we propose a linear programming (LP) approach for solving
this subproblem. We also prove a 2-approximation result for the LP approach
which holds for any level of subsampling and for any subsampling pattern, and
show that TBMC exactly solves the rank-k prediction task for a underlying
block-diagonal tiling structure with geometrically decreasing tile sizes, provid-
ing the ratio between successive tiles is less than 1/
2. Our numerical experi-
ments show that TBMC outperforms existing methods on recommender systems
arising in the context of real datasets.

√

Keywords: Binary matrix completion, linear programming, recommender
systems.
2010 MSC: 65K99.

∗Corresponding author
Email addresses: beckerleg@maths.ox.ac.uk (Melanie Beckerleg),

thompson@maths.ox.ac.uk (Andrew Thompson)

1This publication is based on work partially supported by the EPSRC Centre For Doctoral
Training in Industrially Focused Mathematical Modelling (EP/L015803/1) in collaboration
with e-Therapeutics plc.

2In compliance with EPSRC’s open access inititive, the data in this paper is available from

https://doi.org/10.5061/dryad.51c59zw5r

Preprint submitted to Elsevier

June 23, 2020

 
 
 
 
 
 
1. Introduction

1.1. Matrix completion

Matrix completion is an area of great mathematical interest and has numer-
ous applications including recommender systems for e-commerce, bioactivity
prediction and models of online content, such as the famous Netﬂix problem.

The recommender problem in the e-commerce setting is the following: given
a database where rows are users and column entries indicate user preferences
for certain products, ﬁll in the entries of the database so as to be able to rec-
ommend new products based on the preferences of other users. Typically these
matrices are highly incomplete, since most users will only have experienced
a small fraction of all available products [2]. Similarly, in bioactivity predic-
tion, compound-protein interaction (CPI) databases record bioactivity between
potential drug compounds (rows, or users) and target proteins (columns, or
products). Obtaining experimental evidence of all possible interactions is pro-
hibitively expensive however, and therefore there are few known entries relative
to the overall size of the database, see for example [20].

Low rank approaches to matrix completion have been the focus of a great
deal of theoretical and algorithmic exploration ever since the seminal work in [9,
7].
In many cases, the problem is formulated as follows. Given a database
A ∈ Rm×n, with observed entries in Ω, ﬁnd a matrix X ∈ Rm×n with minimal
rank that matches the observed entries up to a given tolerance, i.e. which solves

min ||PΩ(A − X)||2

2 s.t. rank(X) ≤ r

(1)

where r is some small integer and PΩ is the projection to the space of known

entries, such that the error is evaluated only for the (i, j) ∈ Ω.

A variety of algorithms have been proposed to solve this non-convex problem,
see [28] and references therein. Another popular approach is to solve a convex
relaxation involving the nuclear norm [7]. Such algorithms have been applied
successfully in a wide range of applications, including recommender systems [15].

1.2. Binary matrix completion

The recommender problem can be seen as a binary decision problem. When
applied to a binary matrix, A ∈ Bm×n, the output of the matrix completion
algorithms described above cannot be guaranteed to be binary, and a typical
approach for matrix completion with binary data is to threshold the output
from an algorithm for completion with real data [29, 3]. As highlighted in [31],
where databases follow model assumptions and recovery from solving is exact,
factorisation reduces to assigning identical rows to the same cluster. However,
this approach is not robust to the violation of assumptions and breaks down
when recovery is not exact. This motivates the search for algorithms which
explicitly seek binary solutions.

The problem (1) can also be formulated as one of approximating X as the

product of factors, namely

min ||PΩ(A − UVT )||2

2 s.t. U ∈ Rm×k, V ∈ Rn×k .

(2)

2

We are interested in this paper in approximating our database with binary
factors, due to the greater interpretability of the output. To appreciate this,
note that a binary factorisation

UVT =

k
(cid:88)

i=1

U:,iVT
:,i

decomposes a binary matrix into biclusters of rows and columns, often referred
to in the itemset mining community as tiles, see for example [10]. This decompo-
sition provides an explicit characterization of the row/column clusters that best
explain the database. Low rank decompositions designed for real matrices, on
the other hand, tend to be SVD-like and orthogonal, with negative entries, and
it is less clear how to interpret these. Low rank matrix completion with nonneg-
ativity constraints, for example in [33], enforces non-negativity of factors, but
does not address the issue of rounding errors induced by rounding non-integer
values. Matrix completion algorithms were proposed in [32, 30] which obtain
decompositions in which one factor is composed of rows of the matrix, and is
by consequence binary, although the other factor is generally non-integer.

However, in the case where both factors are required to binary, research
to date has largely focused on the case of binary matrix factorisation (BMF)
in which the matrix is fully observed. BMF has found applications in itemset
mining of transactional and textual data [10], and also in the analysis of gene
expression data [35]. In these applications, entries are in general fully observed,
though possibly with noise.

Various algorithms for BMF have been proposed. The problem can be for-
mulated as an integer program, but the use of an integer programming solver
is only tractable for very small problem sizes. An approach combining con-
vex quadratic programming relaxations and binary thresholding was proposed
in [35]. For the special case of rank-one approximation, linear programming
relaxations were proposed in [27, 21, 35], the ﬁrst two of which also both proved
that the linear programming solution yields a 2-approximation to the optimal
objective. An approach to binary factorisation based on k-means clustering was
considered in [13]. A local search heuristic capable of improving solutions ob-
tained by other methods was proposed in [25]. Of particular relevance to this
paper is the PROXIMUS algorithm, proposed in [17], which identiﬁes patterns
within the data using recursive partitioning based on rank-one approximations.
Closely related to BMF is the Boolean matrix factorisation problem, in which
UVT is replaced by the OR operator, U∧V, which allows the tiles to overlap. A
number of algorithms also exist for Boolean factorisation, including the iterative
ASSO algorithm [24] and integer programming [16].

1.3. Our contribution and related work

The novelty of this paper is the use of a partitioning approach to solve the
problem of BMF with missing data, which can be formulated as follows. For
A ∈ Bm×n, solve

3

||PΩ(A − UVT )||2

2 .

(3)

min
U ∈ Bm×r
V ∈ Bn×r

The contributions of this paper are as follows.

• We propose TBMC (Tiling for Binary Matrix Completion), a low rank
binary matrix completion algorithm (Section 2). The algorithm extends
the recursive partitioning approach of [17] for BMF by means of rank-one
approximations.

• In particular, we propose using an LP rank-one approximation for miss-
ing data. We support this choice with a guarantee that it provides a
2-approximation to the optimal objective value, showing that the reason-
ing of [27] holds in the missing data case (Section 3).

• We show that, under the assumption of a block diagonal model for our
data, this algorithm correctly identiﬁes tiles for a rank-k database with
geometrically decreasing tiles, where the ratio between successive tiles is
less than 1/

√

2.

• We show that our algorithm outperforms alternatives based on related
heuristics and techniques for non-negative matrix completion and binary
matrix completion, when tested on synthetic and real life data (Section 5).

The most closely related work we are aware of is the Spectral method pro-
posed in [31] for bi-clustering databases with missing data. The authors use
low rank completion to cluster neighbour rows and then redeﬁne the column
clusters based on cluster membership, in a similar fashion to k-means. We show
that our algorithm outperforms the Spectral method when solving Problem (3)
for real world datasets.
The authors of

[34] extend the ASSO algorithm for Boolean matrix fac-
torisation to deal with missing data. However, it is worth pointing out that
their setup, as well as solving a diﬀerent problem, is primarily intended for only
small amounts of missing data. Methods that involve linear programs can be
straightforwardly extended to the missing data case, by evaluating the objective
and enforcing constraints only for (i, j) ∈ Ω, however we are not aware of any
attempts to analyse their predictive power for the recommender problem.

1.4. Relation to graph-theoretic problems

Viewing the database as the adjacency matrix of a graph, we can view (3)
as a clustering problem; in particular the problem is that of approximating the
edge set of a partially observed graph as a union of bicliques. The factors U and

4

Figure 1: A row-wise partition is formed from a rank-one approximation (a tile) by splitting
according to which rows are included in the tile.

V can be interpreted as indicating the rows and columns of these bicliques. col-
orredThe authors of [29] solve the related problem for the diagonal block model,
expressing the database as a sum of a low rank matrix plus a sparse compo-
nent to account for noise. Our approach diﬀers as we allow column overlap of
clusters. Bi-clustering approaches, in particular for clustering gene expression
data, have been used to solve formulations similar to the BMF problem, with
diﬀerent assumptions about the underlying data, equivalent to considering dif-
ferent constraints on U and V. However these have focused primarily on cluster
recovery; our focus is on exploring the predictive power of diﬀerent algorithms
for solving Equation (3).

2. The TBMC algorithm

We present an algorithm for low rank binary matrix completion inspired by
the partitioning approach of [17] that generates a binary factorisation based on
recursive rank-one partitions.

2.1. Partitioning

Our algorithm is inspired by the recursive partitioning approach of the
PROXIMUS algorithm for binary matrix factorisation [17]. For a given subma-
trix, B, the algorithm ﬁrst calculates a binary rank-one approximation {u, v}.
The matrix B is then partitioned based upon the rows included in the tile
{u, v}: if the ith entry of u is positive, then the ith row is included in B1, else
it is included in B0. Both submatrices are added to the set of submatrices to
be partitioned.

j:(i,j)∈Ω(vij (cid:54)= Aij)/ (cid:80)

The vector v is used as the pattern vector for B1. If the scaled Hamming
distance, deﬁned as H(v, Ai,:) = (cid:80)
j:(i,j)∈Ω 1, from v to
any of the rows in B1 is less than some tolerance t, for 0 < t < 1, then the tile
uvT is included in the decomposition and B1 is not partitioned any further. In
addition, if the rank-one approximation returns a row vector of 1s, the tile is
added to the decomposition. This process is repeated on successive submatrices
until convergence. The algorithm terminates when no partitions remain. An
illustration of the splitting process can be seen in Figure 1.

5

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)11(cid:1)1(cid:2)(cid:1)21111111111111111111111111(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)11(cid:1)(cid:1)(cid:2)11(cid:1)(cid:1)(cid:2)11(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)2.2. Rank-one approximation with missing data

The partitioning method outlined above relies on rank-one approximations

of the database. We outline two methods for this sub-problem.

2.2.1. Approximate rank-one using a linear program

Inspired by the work of [27], we propose a linear programming relaxation of

the rank-one approximation problem with missing data.

The rank-one problem can be formulated as an integer linear program for
the case where no data is missing [25, 16]. The authors of [27] show that a linear
program relaxation of a related problem has no more than twice the error of
the integer solution. In Section 3 we show that a similar result holds for the
missing data case.

We can formulate the best binary rank-one approximation problem as a

linear program as follows:

min
u ∈ {0, 1}m
v ∈ {0, 1}n

||PΩ(A − uvT )||2

2 =

=

min
u ∈ {0, 1}m
v ∈ {0, 1}n
min
u ∈ {0, 1}m
v ∈ {0, 1}n

(cid:80)

(Aij − uivj)2

(i,j)∈Ω
(cid:80)

(i,j)∈Ω

A2

ij − 2(Aij − 1)uivj .

Since A is ﬁxed, (4) is equivalent to solving

max
u ∈ {0, 1}m
v ∈ {0, 1}n

(cid:88)

uivj −

(cid:88)

uivj.

(i,j):Aij =1

(i,j):Aij =0

(4)

(5)

We translate Problem (5) to a linear program as follows: introducing dummy

variables zij which serve as indicator variables for uivj we can rewrite (5) as

max
u ∈ {0, 1}m
v ∈ {0, 1}n
z ∈ {0, 1}n×m

(cid:80)

zij −

(cid:80)

zij

(i,j)∈Ω:Aij =1

(i,j)∈Ω:Aij =0

s.t.

ui + vj − zij ≤ 1
2zij ≤ ui + vj.

(cid:41)

(i, j) ∈ Ω

(6)

If we relax the integer constraints, then for optimal zij, the second constraint
will be satisﬁed as an equality for Aij = 1, so we can drop the corresponding

(ui + vj). Thus
dummy variables and replace their value in the objective with
we consider the following formulation for approximating the solution to Equa-
tion (6)

1
2

max
u ∈ Rm
v ∈ Rn

s.t.

(cid:80)

(i,j)∈Ω:Aij =1

1
2

(ui + vj) −

(cid:80)

zij

(i,j)∈Ω:Aij =0

(cid:41)

(i, j) ∈ Ω : Aij = 0

(7)

ui + vj − zij ≤ 1
0 < zij < 1
0 < ui, vj < 1

(i, j) ∈ Ω

6

The corresponding constraint matrix is totally unimodular (as it is a sub-
matrix of the constraint matrix in [27]), and so solving will give integral values
for {u, v, z}. We can obtain the corresponding rank-one approximation as uvT .
Note that zij is only deﬁned for (i, j) : Aij = 0. The remaining z values can be
calculated as 1/2(ui + vj) but are no longer guaranteed to be integers.

2.2.2. Alternating minimisation

Alternating minimisation has been widely used for matrix completion, see for
example [12]. Given a ﬁxed v we want to ﬁnd u to minimise the approximation
error

||PΩ(A − uvT )||2

2 =

A2

ij − ui(2Aijvj − v2
j )

(cid:88)

where we have used the fact that u and v are binary. Writing

i,j∈Ω

(cid:40)

Wij =

2Aij − 1
0

if i, j ∈ Ω
otherwise,

we can write the right-hand side of (8) as

(cid:88)

(cid:88)

i

j:(i,j)∈Ω

A2

ij − uiWijvj.

(8)

(9)

(10)

Suppose we are at iteration k and we are ﬁxing vk. Then it follows from (10)
that the updated binary uk+1 is given by

uk+1
i =

(cid:40)

1
0

if (Wvk)i > 0
else.

(11)

Then vk+1 can be calculated in a similar way. Typically the process con-

verges in only a few iterations.

The authors of [17] outline a number of heuristics for initialising the iterative
scheme. In particular, if not initialised correctly, this method can lead to the
empty tile, which is always sub-optimal. We propose using alternating minimi-
sation as an optional post-processing step for the TBMC algorithm stated in
Section 2.3.

2.3. Statement of algorithm

Using the ideas of Section 2.1 and Section 2.2 we now propose an algorithm

for binary matrix completion for recommender systems.

Starting with the full database, we calculate {u∗, v∗} the rank-one approxi-
mation obtained by solving Problem (7) for the current partition, then partition
into the rows that are included in the tile and those that are not, and repeat for
both sides of the partition. As in [17], when the partitioning process generates
rows that are all within a given radius of v∗ or when u∗ is the vector of all 1s
or all 0s, then {u∗, v∗} is added to the factorisation. The algorithm is stated in

7

detail in Algorithm 1. We could also consider using the alternating minimisation
outlined in Section 2.2.2 to improve the approximation found by solving (7). We
refer to this approach as TBMC-AM. In its current form, the algorithm only
partitions by rows. We also considered a variant by which each tile identiﬁed
by the algorithm was then partitioned in the same manner column-wise. It is
possible to construct examples for which this would improve the accuracy but
as we found no change in the output for any of our experiments, we speculate
that these examples are rarely found in practise.

Algorithm 1 Tiling for Binary Matrix Completion (TBMC)

1: Initialise S = {A}, U = ∅, V = ∅, i = 1
2: while S (cid:54)= ∅, k < kmax do
3:
4:
5:

Find {u, v} as the solution to 7

select B ∈ S, set S ← S\B
procedure find rank-one approximation for B:

6:
7:

8:

9:
10:

11:

12:

procedure Partition B

Deﬁne J = (cid:8)j : uk(j) = 1(cid:9)
Set B1 = B(J, :), B0 = B(J, :)

procedure Update

if B0 (cid:54)= 0 and u (cid:54)= 0 then S ← B0
if maxj ||B1(j, :) − vk|| < t, or u = 1

then

U ← uk,V ← vk, k → k + 1

else S ← S ∪ B1

2.4. Algorithm complexity

The most computationally demanding step in Algorithm 1 is the solution
of (7). To solve a non-degenerate LP with a simplex method, the worst case
complexity is exponential in the number of variables (equal to N = n + m +
|{(i, j) : (i, j) ∈ Ω, Aij = 0}|) and the number of constraints (equal to M =
2N + |{(i, j) : (i, j) ∈ Ω, Aij = 0}|), since we have one constraint for each
zij, plus constraints that each variable be between 0 and 1. This upper bound
arises because the method will terminate when it has checked every vertex of
the feasible region deﬁned by the problem. There are M + N possible vertices,
so there are 2M +N possible intersections. However, since the constraint matrix
of (7) is totally unimodular (see Section 2.2), a polynomial bound in M and N
can be obtained. By Corollary 4.1 of [14], solving (7) using Bland’s rule [5] (or
another method for avoiding cycles) will terminate in at most 2N (M log N + 1)
operations.

The other steps in the algorithm have lower complexity. The alternating
minimisation scheme requires one left and one right matrix vector multiplication
per iteration, hence, since we impose a limit on the number of iterations, it
takes O(n + m) steps. Calculation of the error (step 11) requires at most nm
operations.

8

In order to determine the complexity of the algorithm as a whole, we multiply
the complexity of the rank-one case by m, the worst case for the number of parti-
tions that need to be checked. So Algorithm 1 requires at most O(mN M logN )
operations, i.e. polynomial in m and n.

We note that solving the problem directly involves checking k options for the
membership of each row and column, i.e. km+n possibilities. Hence Algorithm 1
oﬀers a signiﬁcant improvement over solving the problem directly.

3. Approximation bounds for the rank-1 subproblem

In Section 2.2, we proposed a linear program method for solving the rank-
one step of (1). By closely following the approach in [27] for the case in which
all the entries in A are known, we show that the approximation error of the tile
found by solving Problem (7) is no more than twice the optimal.

Theorem 1. Denoting by (u∗, v∗, z∗) the optimal solution for Problem (7) ob-
tained using a simplex method, then the approximation error E = ||PΩ(A −
u∗v∗T )||2

2 satisﬁes

E ≤ 2

min
u ∈ {0, 1}m
v ∈ {0, 1}n

||PΩ(A − uvT )||2
2

(12)

Proof. The minimal error for a single tile approximation is given by

||PΩ(A − uvT )||2
2

min
u ∈ {0, 1}m

v ∈ {0, 1}n

=

min
u ∈ {0, 1}m

v ∈ {0, 1}n

(cid:80)

(Aij − uivj)2

(i,j)∈Ω

= (cid:80)

(i,j)∈Ω

A2

ij − max

u ∈ {0, 1}m

(cid:32)

(cid:80)

uivj − (cid:80)

uivj

(cid:33)

(i,j):Aij =1

(i,j):Aij =0

v ∈ {0, 1}n
(cid:32)

≥ (cid:80)

(i,j)∈Ω

A2

ij −

(cid:80)

(u∗

i + v∗

j ) − (cid:80)

(i,j):Aij =1

(i,j):Aij =0

1
2

(cid:33)

z∗
ij

(13)
where the inequality is a result of the fact that the optimal solution for
Problem (6) is bounded above by the objective for Problem (7) since relaxing
constraints does not decrease the value at optimality.

For (i, j) such that Aij = 0, we know that ui, vj and zij are integers. Since
j −1, we
j = 2. Thus we can split the summation

the objective of (7) is to minimise z∗
have that z∗
terms on the right hand side of the inequality to obtain

ij, which is bounded below by u∗

ij = 1 if and only if u∗

i + v∗

i +v∗

9

LHS ≥

(cid:88)

A2

ij−

(cid:88)

1−

(cid:88)

(i,j)∈Ω

(i, j) : Aij = 1

(i, j) : Aij = 1

ui + vj = 2

ui + vj = 1

1
2

+

(cid:88)

1

(i, j) : Aij = 0

ui + vj = 2

(14)

Since A is binary, we have that

(cid:88)

A2

ij ≥

(cid:88)

1 +

(cid:88)

1

(15)

(i,j)∈Ω

(i, j) : Aij = 1

(i, j) : Aij = 1

ui + vj = 1

ui + vj = 2

which we can use to replace the sum in (14) corresponding to values where

exactly one of ui and vj is 1 to derive

||PΩ(A − uvT )||2

2 ≥

min
u ∈ {0, 1}m

v ∈ {0, 1}n

≥











1
2

1
2

(cid:80) A2

ij −

(cid:80)

(i, j) : Aij = 1

ui + vj = 2



1


 +

(cid:80)

1

(i, j) : Aij = 0

ui + vj = 2

(cid:80) A2

ij −

(cid:80)

1 +

(cid:80)

(i, j) : Aij = 1

(i, j) : Aij = 0

ui + vj = 2

ui + vj = 2




1


(16)
Now since 2Aij − 1 is equal to 1 if Aij is positive and −1 if Aij is zero we can
rewrite (16) as

min
u ∈ {0, 1}m
v ∈ {0, 1}n

||PΩ(A − uvT )||2

2 ≥ =

=

1
2
1
2

(cid:80)

(i,j)∈Ω

(cid:0)A2

ij − (2Aij − 1) uivj

(cid:1)

(cid:80)

(Aij − uivj)2 =

(i,j)∈Ω

1
2

E .

(17)

This gives us our bound.

Note that in the case where the true best solution has zero error, the LP
relaxation will also have zero error, and therefore for a database consisting of
a single planted tile, solving (7) will recover this tile provided that, for each
row, the sampling operator sees a positive and a negative entry for each row or
column.

3.1. Verifying Theorem 1

We ﬁrst perform experiments to illustrate numerically that the 2-approximation

result given in (12) is not violated.

We evaluate performance relative to the optimal solution, which we obtain
by solving (6) directly as an integer program. We generate binary matrices
with (a) a single planted tile and (b) 3 planted tiles, with τ n positive entries

10

per row in each tile. We simulate noisy data by randomly ﬂipping a fraction
(cid:15) of entries and remove a proportion 1 − ρ of entries. We set τ = 0.7 and
(cid:15) = 0.03, ρ = 0.7. Note that choice of parameters is to provide an illustrative
example, as we observe similar behaviour for other parameter conﬁgurations.
To make it tractable to solve the problem directly, we consider databases of size
100 × 100 for the single tile case and 10 × 10 for the 3 tile case, since the latter is
more computationally intensive to solve. In both cases, we calculate the ratio,
R, of squared l2 approximation error to the optimal squared l2 error. The mean
value of R and the proportion of cases for which R is greater than 1 is recorded
in Section 3.1.

We solve the LP relaxation using the simplex method implemented by MAT-
LAB’s linprog solver [8]. In addition, we compare against other methods, aver-
age, partition, and nmf for generating a rank-1 approximation, further details
of which can be found in Section 5. We observe that while the other methods
all violate the 2-approximation bound for the 3-tile model, the LP does not.

LP
(AM)
NMF
(AM)
Partition
(AM)
Avg
(AM)

Mean R
1.0
(1.0)
0.0
(0.0)
2.42
(1.26)
2.56
(1.32)

P0
0
(0)
0.0
(0.0)
0.87
(0.0)
0.62
(0.0)

LP
(AM)
NMF
(AM)
Partition
(AM)
Avg
(AM)

Mean R
1.06
(1.04)
2.46
(1.21)
2.94
(1.37)
2.96
(1.21

P0
0
(0)
0.43
(0.01)
0.58
(0.12)
0.74
(0.06)

Table 1: Mean value of R and P0, the
proportion of cases test cases for which
R > 2, for matrices with a single planted
tile, m = 100.

Table 2: Mean value of R and proportion
of cases test cases for which R > 2, for
matrices with 3 planted tiles with density
τ = 0.7 with m = 10.

4. Recovery guarantees for TBMC

The results of Section 3 hold for any binary matrix and give guarantees for
solving the rank-1 problem. We now explore the conditions for which Algo-
rithm 1 will recover a database generated according to a planted tile model. We
let A ∈ Bm×m have a
consider the following model for the underlying data:
symmetric block diagonal structure, with the size of the lth tile equal to (mτl)2
where τl+1 ≤ τl as illustrated in Figure 2.

11

Figure 2: We consider recovery guarantees
for case of a block diagonal model with
decreasing tile size.

Figure 3: On the ith iteration of Algo-
rithm 1, empty columns can be ignored,
as setting βj = 1 for j < i will never in-
crease the objective.

The heart of the proof is in showing that, in the case of no missing data,
solving Problem (7) recovers the largest tile. We then show that for the case of
no missing data, if the algorithm recovers the ﬁrst i tiles then it will recover the
(i + 1)th tile and conclude. Finally, we show numerically that the performance
for the case of missing data is in close agreement with the theoretical result for
no missing data. We deﬁne Tl to be the set of indices in the lth tile and

Sl,l(cid:48) = {(i, j) : i ∈ Tl, j ∈ Tl(cid:48)}.

We rely on the following observations:

Lemma 2. We can rewrite the objective of (7) as

f (u, v) =









(cid:88)

l

−

(cid:88)

l(cid:54)=l(cid:48)

1
2

+

1.

(cid:88)

(i, j) ∈ Sl,l
ui + vj = 1
(cid:88)

(i, j) ∈ Sl,l(cid:48)
ui + vj = 2





1




(cid:88)

(i, j) ∈ Sl,l
ui + vj = 2

(18)

Proof. The constraints of (7) enforce the condition that zij = uivj. Hence, if
we split the second term of the objective of Problem (7) by the values of ui + vj
the only terms that will have a contribution are the ones for which ui + vj = 2.
The rest follows by splitting the summation terms according to the values of
ui + vj.

Lemma 3. Let αl, βl be the proportion of rows and columns of tile l included
in a feasible solution to (7). Then we can write (18) as

12

m(cid:2)(cid:1)m(cid:2)(cid:1)m(cid:2)2m(cid:2)km(cid:2)2m(cid:2)km(cid:2)km(cid:2)im(cid:2)km(cid:2)im(cid:1)i(cid:1)(cid:1)l(cid:2)lf (α, β) = m2 (cid:88)

τ 2
l

−m2

l
k
(cid:88)

l=1

(cid:26) 1
2
k
(cid:88)

l(cid:48)=1

l(cid:48)(cid:54)=l

[αl(1 − βl) + (1 − αl)βl] + αlβl

(cid:27)

αl(cid:48)βlτl(cid:48)τl.

(19)

In addition, there is an optimal solution for which αl, βl ∈ {0, 1} for each

tile.

Proof. This follows from Equation (18). The objective can be split into sums
by row; rows in the same tile will have identical contribution to the objective,
leading to the formulation in (19). Since this expression is linear in the αl for
each l, we can conclude that there must be an optimal solution for either αl = 0
or αl = 1; to determine which requires determining the sign of the coeﬃcient of
each αl: if it is positive then αl = 0 is optimal, and if it is negative then αl = 1
is optimal. The same reasoning applies to each βl.

We analyse tile recovery for the case where all entries are known, obtaining
a phase transition for recovery in terms of relative tile sizes, and then show
that, in practice, we get agreement with this phase transition for sub-sampled
matrices, provided the tiles are large enough.

Theorem 4. For a binary matrix A with a symmetric block diagonal structure
with k tiles, having size mτ 2
2, Algorithm 1
recovers the tiles exactly in the case where all entries are known (in this case
Algorithm 1 reduces to the PROXIMUMS algorithm [17]).

l where τl+1 = aτl, for a ≤ 1/

√

Proof. Using Lemma 3, we may consider four cases depending on the integer
values of α1, β1.

Case (i): For α1 = β1 = 1, we calculate

1
m2

∂f
∂αj

= 1

2 τ 2

j − τjτ1 − (cid:80)

τjτl

for j > 1

l(cid:54)=j

l>1

(20)

≤ 1

2 τ 2

j − τjτ1

This is strictly negative, since τ1 > τj, hence αj = 0 for all j > 1. Similarly,

βj = 0 for all j > 1. The corresponding objective value is m2τ 2
1 .

Cases (ii) and (iii): For α1 = 1, β1 = 0,

1
m2

∂f
∂βj

≤

1
2

τ 2
j − τjτ1 for j > 1 .

(21)

Hence βj = 0 for all j > 1. We can then substitute this to ﬁnd

13

f (α, β) = 1

2 m2τ 2

1

(cid:32)

τ 2
1 +

(cid:88)

l>1

(cid:33)

αlτ 2
l

.

(22)

So for optimality in this case, αl = 1, βl = 0 for all l. By symmetry, for
α1 = 0, β1 = 1, optimality in this case is obtained for αl = 0, βl = 1 for all l.
In both cases, the objective value is f = 1

Case (iv): For α1 = 0, β1 = 0, the objective is at most m2 (cid:80)

l /2. To
see this, consider that the lth term of the ﬁrst summation is increasing in αl
and βl, so obtains its maximum for αl = βl = 1, and the second summation is
always negative.

2 m2 (cid:80)

l>1 τ 2

l τ 2
l .

We can now compare the objectives for each case. Case (i) is optimal over

the other cases, provided

which is true if and only if

τ 2
1 >

(cid:88)

l>1

1
2

(cid:88)

τ 2
l ,

l

τ 2
1 >

τ 2
l

.

(cid:88)

l>1

(23)

(24)

Since this is a strict inequality, the optimal solution will be unique.
Note that on subsequent iterations of step 2 of Algorithm 1, the input matrix
is no longer square, but has dropped out the rows corresponding to the largest i
tiles. The resulting matrix contains the remaining k −i tiles and empty columns
corresponding to the dropped tiles. Hence we are no longer tracking the αj for
j ≤ i. Therefore, we rewrite the objective as

k
(cid:80)
l=i+1
k
(cid:88)

−m2

f (α, β) = m2

τ 2
l

2 [αl(1 − βl) + (1 − αl)βl] + (1 − αl)(1 − βl)(cid:9)
(cid:8) 1

k
(cid:88)

αl(cid:48)βlτl(cid:48)τl − m2

i
(cid:88)

l=1

k
(cid:88)

l(cid:48)=i+1
l(cid:48)(cid:54)=l

αl(cid:48)βlτl(cid:48)τl

(25)

l=i+1

l(cid:48)=i+1
l(cid:48)(cid:54)=l

where we have dropped out the terms corresponding to the dropped rows,
and isolated terms corresponding to the empty columns (l ≤ i). In this case,
the coeﬃcient of βj for j ≤ i will be non-positive, and negative provided at least
one of the αj is positive for j > i. This means we can consider optimality for
the reduced matrix formed by dropping out the columns corresponding to the
ﬁrst i tiles and therefore if

(cid:88)

τ 2
i >

τ 2
l

.

(26)

we have that at each iteration, the algorithm will successfully identify the

right tile, and therefore 1 will output the exact tiling after k iterations.

l>i

14

In particular, if we consider the geometric model for tile size, τl+1 = aτl,

then imposing (26) leads to the condition

1 > (cid:80)k−i

l=1 a2l = a2 (cid:80)k−i

l=1 a2(l−1) = a2 1 − a2(k−i)
1 − a2

for all 0 ≤ i ≤ k − 1, from which we obtain

In particular, for a ≤ 1/

√

2a2 − a2(k−i+1) < 1

2 this requirement is satisﬁed for all i.

(27)

(28)

4.1. Observed behaviour with missing entries

Now suppose that entries are erased independently with probability 0 ≤ ρ <
1. We are interested in whether the phase transition identiﬁed above can be
extended to this case. For a given realisation, we can no longer consider each
row as identical, hence the theoretical result in this section does not precisely
extend. However, our numerical observations suggest that we do observe a
behaviour which is a close approximation, provided the database size is large
enough.

We generate symmetric block-diagonal matrices with decreasing tile size ac-
cording to the model in Section 4, setting k = 4, for a = 0.1 : 1 and ρ = 0.1 to
0.9. For 100 random trials, we calculate the proportion of matrices recovered
exactly from Algorithm 1 and plot the results in the top row of Figure 4. We
also plot, in the bottom row of Figure 4, the proportion for which Algorithm 1
is able to recover all but 3% of the entries; accounting for the diﬃculty of re-
covery of the smallest tiles. According to (28), we would expect to see recovery
for a ≤ 0.72. In both cases, we see that there is agreement with our bound on
a, and that as we increase the size of our database, the value of maximum ρ
for which the algorithm recovers all of the tiles also increases. The small gap
between the dashed line indicating a = 0.72 and the line of failure to recover
is an artefact of discretisation error for smaller databases; it is not possible to
generate tiles that are exactly a factor of 0.72 smaller than the previous, since
a row is either included or not.

15

Figure 4: Proportion of matrices, generated according to Section 4, which are (top) recovered
by TBMC and (bottom) for which 97% of the entries are recovered (bottom). Dark blue
regions are fully recovered. The black dashed line indicate the theoretical bound on a.

5. Numerical Results

We next demonstrate that the partitioning approach, and in particular our
TBMC algorithm is a practical method for generating interpretable rules for
inferring missing information in real data sets (recommender systems), that
outperforms state of the art alternative methods for binary matrix completion.

5.1. Other algorithms

We compare Algorithm 1 to other methods for rank-k approximation: two
alternative heuristics for partitioning similar to those used in [17]; NMF with
missing values implemented using the NMF MATLAB Toolbox [26], based on
the multiplicative scheme of [19] with zero weighting of missing entries; and a
method that follows a k-means approach to cluster the observed data based on
projections onto the k largest singular vectors of the observed data. In particular
we consider

• ‘average’ : setting vj = 0 where the jth column has more than half of its
entries as positive and ui = 1 if any of the entries of the ith row have a
positive entry in one of those columns ;

• ‘partition’ : selecting a column at random to be u and calculating v as the
average of all the rows that have positive entries in that column, binarised
with a threshold of 1/2;

• ‘NMF’ : a non-negative rank-one factorisation is obtained using a multi-
plicative update scheme with missing data and the factors are binarised
using a threshold of 0.5. We use the approach outlined in [35] to normalize
the factors such that the 0.5 threshold is appropriate.

16

00.20.40.60.8100.20.40.60.800.20.40.60.8100.20.40.60.800.20.40.60.8100.20.40.60.800.20.40.60.8100.20.40.60.800.20.40.60.8100.20.40.60.800.20.40.60.8100.20.40.60.800.20.40.60.8100.20.40.60.800.20.40.60.8100.20.40.60.8• ‘Spectral’ : We obtain a rank-one approximation by implementing the
method outlined in [31], which follows a k-means approach. Brieﬂy, the
factors are initially generated using a projection onto the ﬁrst k singular
values; viewing the factorisation as a clustering, the footprints in Vi are
updated using the clustering given by Ui, before reassigning rows to the
cluster whose footprint most closely matches their own.

For TBMC, average and partition we also consider using an alternating min-
imisation step, outlined in Section 2.2.2 to improve the rank-one approximations.
We refer to these approaches as TBMC-AM, average-AM and partition-AM.

5.2. Real world datasets

We consider performance on a series of real world recommender systems.
It is worth pointing out that whilst many of the relevant datasets have non-
binary entries, often categorical ratings for example, the aim of the recommender
problem is to decide whether or a particular entry will be a positive interaction
or not (i.e. whether a user will like a particular ﬁlm, or a drug will bind to
a particular target). Hence it is necessary to make decisions about how these
datasets are binarised.

In all of the datasets we consider, we ﬁnd optimum performance for partition
based methods, and in particular for four of the ﬁve data sets that we investigate,
we ﬁnd that TBMC outperforms other state of the art methods upon this task.
We consider the performance of rank-k binary matrix factorisation on the

following datasets.

• ChEMBL: Data of protein-ligand interactions, obtained from ChEMBL
[4], version 21. Assay values for measurement types POTENCY, IC50,
KI, MIC, EC50, KD, AC50 were binarised using a threshold of 2µM to
obtain positive and negative activity categories, where negative represents
no discernible eﬀect on protein behaviour. The resulting matrix was then
ﬁltered by counting the positive and negative interactions, ranking the
compounds based on the total number of interactions and taking subsets
from the top 50000 ligands.

• ML: MovieLens 100k [22], which contains 100, 000 ratings from 943 users
across 1682 ﬁlms. We threshold to consider only 4 or 5 star ratings as
positive.

• RC: customer reviews from 138 customers for 130 restaurants [23]. Since
ratings are in {1, 2, 3}, we map to binary data by converting 3 star ratings
to one, and converting 1 or 2 star ratings to zero.

• ALL-AML (GE): gene expression data [11], containing information from
human tumour samples of diﬀerent leukemia types. This dataset has been
widely used for cancer classiﬁcation, and for the evaluation of BMF tech-
niques in the case of no missing data [6]. We normalize the data by
column.

17

• Netﬂix: a subset of data obtained from Kaggle [18] from the Netﬂix prize
data [1] of ratings by users on diﬀerent movies. The sparsely reviewed
movies (< 2000 ratings) and sparsely active users (< 52 reviews) are
removed. The resulting dataset reﬂects the 70th percentile for density of
reviews. In order to apply the algorithms below, we consider a random
subset of 1000 users and 1000 movies. We threshold to consider only 4 or
5 star ratings as positive.

We compare the performance of TBMC and TBMC-AM against NMF and the
Spectral method (see Section 3.1). The rank parameter was optimised for NMF,
with 5 being found to be a good choice. The maximum rank was set to be 5 in
the Spectral method, although the method was found to often terminate before
ﬁve tiles were found. Although increasing the rank initially leads to an improve
in approximation accuracy, none of these methods are guaranteed to ﬁnd a zero
error approximation simply by increasing the rank. In addition, there is a risk
of over-ﬁtting for a rank parameter that is too high. We also compare with a
variant of the recursive partitioning approach of TBMC in which the LP-based
rank-one approximation step is replaced by the average and partition heuristics
(see Section 3.1), again with and without alternating minimisation.

5.3. Performance of TBMC for rank-k decomposition

We consider a 70/30 split between known training entries and unseen test
entries (ρ = 0.7) and set t = 0.05, although we ﬁnd the impact of changing t
has little impact below 0.1. Note that MovieLens, RC, ChEMBL and Netﬂix
have a low density of known entries ρD. In all cases we record the percentage
approximation error P. We give approximation errors upon both the test set,
and also upon the training set, averaged over 100 random trials. The ﬁrst score
tells us how well each method is able to predict missing entries. The second
score tells us how well the method is able to generate a tiling model for the
known data.

From Table 3, we see that TBMC outperforms all other algorithms as a
predictive method on all ﬁve data sets. The alternating minimisation (AM)
step leads to improved results in some cases, but not always. The improvement
over other methods is signiﬁcant in the case of the gene expression (GE) and
restaurant customer (RC) data. On the other hand, in the case of the Movie-
Lens data, the improvement gained by using TBMC over the NMF method is
marginal and for both MovieLens and Netﬂix, the gain over simpler partitioning
using simpler heuristics, average and partition heuristics is also marginal.

Approximation error on the training set allows us to compare how well the
database tilings of the various methods are able to capture the known data. We
see from Table 3 that TBMC outperforms all other methods on this task for the
RC and GE datasets. For MovieLens, replacing the LP rank-one approximation
in TBMC with the averaging heuristic is seen to give the best performance while
for ChEMBL partition gives the best performance. These results support the
use of a partitioning for the approximation task.

18

m

n

ρ

ρD

ChEMBL 7500

2183

0.7

0.01

ML

1600

943

0.7

0.06

RC

138

130

0.7

0.065

GE

5000

38

0.7

1.0

Netﬂix

1000

1000

0.7

0.06

P
Test (AM)
Train (AM)
Partition NMF Spectral

29.7
(17.1)
15.7
(14.7)
33.8
(30.1)
33.3
(23.1)
23.3
(29.7)
23.4
(17.7)
20.4
(15.2)
18.7
(12.2)
19.9
(18.6)
20.0
(14.2)

27.7

29.9

26.4

30.7

21.1

21.1

19.5

21.3

22.2

22.0

16.1

22.2

14.3

21.2

12.6

21.5

20.4

19.9

16.4

20.0

TBMC Avg
30.7
(16.0)
18.0
(18.0)
21.3
(23.3)
17.7
(19.0)
22.2
(31.4)
17.2
(18.2)
21.2
(21.2)
21.5
(21.5)
18.9
(21.0)
17.0
(13.9)

19.0
(15.4)
16.5
(14.2)
20.8
(22.7)
20.6
(19.5)
19.5
(35.5)
4.8
(5.5)
11.6
10.9
11.3
(10.4)
18.3
(22.8)
15.7
(13.57)

Table 3: Proportional error P for diﬀerent datasets, on both test and training sets, with and
without alternating minimisation (AM).

We speculate that the varying results for diﬀerent datasets are due in large
part to the degree to which each database can be modelled as a low-rank binary
factorisation (tiling). It should be emphasised that the prediction algorithms
considered here are restricted to those which generate interpretative binary fac-
torisations. We are not claiming that TBMC competes favourably with all pre-
diction algorithms (such as neural networks) if the interpretability requirement
is removed. That said, our results show clearly that partitioning algorithms, and
in particular TBMC, perform well compared to other algorithms for low-rank
matrix completion with binary factors.

We conclude by making two further observations. Firstly, the fact that
simple heuristics can perform well highlights the importance of balancing higher
accuracy with constraints on computational time. Secondly, the algorithm shows
evidence of over-ﬁtting the RC data as the percentage error on the known values
(training set) is 5.5%.

The RC database appears to be an especially good ﬁt for approximation
by binary factors (tiles). The clustering found by TBMC on the RC database
is provided as an illustration in Figure 5. The rows and columns have been
reordered for visual eﬀect.

19

Figure 5: The RC dataset (left) with rows and columns reordered according to the factorisation
found by TBMC (right).

6. Conclusion and future directions

Binary matrix completion for recommender systems has numerous applica-
tions. Inﬂuenced by approaches to binary matrix factorisation in the itemset
mining literature, we have proposed the TBMC algorithm for binary matrix
completion and shown that it outperforms alternatives on both synthetic and
real data sets for certain regimes. These results make the case for the con-
sideration of heuristic methods where typical assumptions for low rank matrix
completion are violated and exact recovery is not guaranteed.

We have presented a theoretical recovery guarantee for TBMC for geomet-
rically decaying diagonal blocks. It would interesting to extend the result to
incorporate random models for missing data and noise, and also to consider
alternative block models.

References

[1] URL www.netflixprize.com.

[2] Charu C Aggarwal. Recommender systems. Springer, 2016.

[3] Brendan PW Ames and Stephen A Vavasis. Nuclear norm minimization
for the planted clique and biclique problems. Mathematical Programming,
129(1):69–89, 2011.

[4] A Patr´ıcia Bento, Anna Gaulton, Anne Hersey, Louisa J Bellis, Jon Cham-
bers, Mark Davies, Felix A Kr¨uger, Yvonne Light, Lora Mak, Shaun
McGlinchey, et al. The chembl bioactivity database: an update. Nucleic
acids research, 42(D1):D1083–D1090, 2014.

[5] Robert G Bland. New ﬁnite pivoting rules for the simplex method. Math-

ematics of operations Research, 2(2):103–107, 1977.

20

020406080100120020406080100120020406080100120020406080100120[6] Jean-Philippe Brunet, Pablo Tamayo, Todd R Golub, and Jill P Mesirov.
Metagenes and molecular pattern discovery using matrix factorization. Pro-
ceedings of the National Academy of Sciences, 101(12):4164–4169, 2004.

[7] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via
convex optimization. Foundations of Computational mathematics, 9(6):717,
2009.

[8] George B Dantzig, Alex Orden, Philip Wolfe, et al. The generalized simplex
method for minimizing a linear form under linear inequality restraints.
Paciﬁc Journal of Mathematics, 5(2):183–195, 1955.

[9] Maryam Fazel. Matrix rank minimization with applications. PhD thesis,

PhD thesis, Stanford University, 2002.

[10] Floris Geerts, Bart Goethals, and Taneli Mielik¨ainen. Tiling databases. In
International Conference on Discovery Science, pages 278–289, 2004.

[11] Todd R Golub, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle
Gaasenbeek, Jill P Mesirov, Hilary Coller, Mignon L Loh, James R Down-
ing, Mark A Caligiuri, et al. Molecular classiﬁcation of cancer: class dis-
covery and class prediction by gene expression monitoring. Science, 286
(5439):531–537, 1999.

[12] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix
completion using alternating minimization. In Proceedings of the forty-ﬁfth
annual ACM symposium on Theory of computing, pages 665–674. ACM,
2013.

[13] Peng Jiang, Jiming Peng, Michael Heath, and Rui Yang. A clustering
approach to constrained binary matrix factorization. In Data Mining and
Knowledge Discovery for Big Data, pages 281–303. Springer, 2014.

[14] Tomonari Kitahara and Shinji Mizuno. A bound for the number of diﬀerent
basic solutions generated by the simplex method. Mathematical Program-
ming, 137(1-2):579–586, 2013.

[15] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization tech-

niques for recommender systems. Computer, pages 30–37, 2009.

[16] R´eka Kov´acs, Oktay G¨unl¨uk, and Raphael Hauser. Low-rank Boolean ma-
trix approximation by integer programming. In NIPS Workshop on Opti-
mization for Machine Learning, Long Beach, CA, 2017.

[17] Mehmet Koyut¨urk, Ananth Grama, and Naren Ramakrishnan. Nonorthog-
onal decomposition of binary matrices for bounded-error data compression
and analysis. ACM Transactions on Mathematical Software, 32(1):33–69,
2006.

21

[18] D Lao. Netﬂix movie recommendation, 2019. URL \url{https://www.

kaggle.com/laowingkin/netflix-movie-recommendation}.

[19] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by

non-negative matrix factorization. Nature, 401(6755):788, 1999.

[20] Hui Liu, Jianjiang Sun, Jihong Guan, Jie Zheng, and Shuigeng Zhou. Im-
proving compound–protein interaction prediction by building up highly
credible negative samples. Bioinformatics, 31(12):i221–i229, 2015.

[21] Haibing Lu, Jaideep Vaidya, Vijayalakshmi Atluri, Heechang Shin, and Lili
Jiang. Weighted rank-one binary matrix factorization. In Proceedings of
the 2011 SIAM International Conference on Data Mining, pages 283–294.
SIAM, 2011.

[22] F. Maxwell Harper and J. Konstan. The MovieLens datasets: History and
context. ACM transactions on interactive intelligent systems (TiiS).

[23] Rafael Medelln, Juan Gonzlez Sern, and Blanca Vargas-Govea. https:
//www.kaggle.com/uciml/restaurant-data-with-consumer-ratings.
Accessed: 2018-11-30.

[24] Pauli Miettinen, Taneli Mielik¨ainen, Aristides Gionis, Gautam Das, and
Heikki Mannila. The discrete basis problem. IEEE Transactions on Knowl-
edge and Data Engineering, 20(10):1348–1362, 2008.

[25] Seyed Hamid Mirisaee, Eric Gaussier, and Alexandre Termier. Improved
local search for binary matrix factorization. In AAAI, pages 1198–1204,
2015.

[26] Qihao Qi, Yingdong Zhao, MingChung Li, and Richard Simon. Non-
negative matrix factorization of gene expression proﬁles: a plug-in for brb-
arraytools. Bioinformatics, 25(4):545–547, 2009.

[27] Bao-Hong Shen, Shuiwang Ji, and Jieping Ye. Mining discrete patterns
via binary matrix factorization. In Proceedings of the 15th ACM SIGKDD
international conference on Knowledge discovery and data mining, pages
757–766, 2009.

[28] Jared Tanner and Ke Wei. Low rank matrix completion by alternating
steepest descent methods. Applied and Computational Harmonic Analysis,
40(2):417–429, 2016.

[29] Ramya Korlakai Vinayak, Samet Oymak, and Babak Hassibi. Graph clus-
tering with missing data: Convex algorithms and analysis. In Advances in
Neural Information Processing Systems, pages 2996–3004, 2014.

[30] Yining Wang and Aarti Singh. Provably correct algorithms for matrix
column subset selection with selectively sampled data. The Journal of
Machine Learning Research, 18(1):5699–5740, 2017.

22

[31] Jiaming Xu, Rui Wu, Kai Zhu, Bruce Hajek, R Srikant, and Lei Ying.
Jointly clustering rows and columns of binary matrices: Algorithms and
trade-oﬀs. In ACM SIGMETRICS Performance Evaluation Review, vol-
ume 42, pages 29–41. ACM, 2014.

[32] Miao Xu, Rong Jin, and Zhi-Hua Zhou. Cur algorithm for partially ob-
served matrices. In International Conference on Machine Learning, pages
1412–1421, 2015.

[33] Yangyang Xu, Wotao Yin, Zaiwen Wen, and Yin Zhang. An alternating di-
rection algorithm for matrix completion with nonnegative factors. Frontiers
of Mathematics in China, 7(2):365–384, 2012.

[34] Prashant Yadava. Boolean matrix factorization with missing values. Mas-

ter’s thesis, Universitat des Saarlandes, 2012.

[35] Zhong-Yuan Zhang, Tao Li, Chris Ding, Xian-Wen Ren, and Xiang-Sun
Zhang. Binary matrix factorization for analyzing gene expression data.
Data Mining and Knowledge Discovery, 20(1):28, 2010.

23

