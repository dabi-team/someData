1
2
0
2

n
u
J

2

]
S
D
.
s
c
[

2
v
1
6
2
9
0
.
7
0
0
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

1

Frequency Estimation in Data Streams:
Learning the Optimal Hashing Scheme

Dimitris Bertsimas and Vassilis Digalakis Jr.

Abstract—We present a novel approach for the problem of frequency estimation in data streams that is based on optimization and
machine learning. Contrary to state-of-the-art streaming frequency estimation algorithms, which heavily rely on random hashing to
maintain the frequency distribution of the data steam using limited storage, the proposed approach exploits an observed stream preﬁx
to near-optimally hash elements and compress the target frequency distribution. We develop an exact mixed-integer linear optimization
formulation, which enables us to compute optimal or near-optimal hashing schemes for elements seen in the observed stream preﬁx;
then, we use machine learning to hash unseen elements. Further, we develop an efﬁcient block coordinate descent algorithm, which,
as we empirically show, produces high quality solutions, and, in a special case, we are able to solve the proposed formulation exactly in
linear time using dynamic programming. We empirically evaluate the proposed approach both on synthetic datasets and on real-world
search query data. We show that the proposed approach outperforms existing approaches by one to two orders of magnitude in terms
of its average (per element) estimation error and by 45-90% in terms of its expected magnitude of estimation error.

Index Terms—Data streams, streaming frequency estimation, learning to hash, optimal hashing scheme.

(cid:70)

1 INTRODUCTION

W E consider a streaming model of computation [1], [2],

where the input is represented as a ﬁnite sequence of
elements from some ﬁnite universe (domain) which is not
available for random access, but instead arrives dynamically
and one at a time in a stream. We further assume that each
element is identiﬁed by a unique key and is also associated
with a set of features. One of the most fundamental prob-
lems in the streaming model is frequency estimation, i.e.,
given an input stream, estimate the frequency (number of
occurrences) of each element. Notice that this can trivially
be computed in space equal to the minimum of the universe
and the stream size, by simply maintaining a counter for
each element or by storing the entire stream, respectively.
Nevertheless, data streams are typically characterized by
large volume and, therefore, streaming frequency estimation
algorithms should require small space, sublinear in both
the universe and the stream size. Furthermore, streaming
algorithms should generally be able to operate in a single
pass (each element should be examined at most once in ﬁxed
arrival order) and in real-time (each element’s processing
time must be low).

Example. Consider a stream of queries arriving on a server. The
universe of all elements is the set of all possible queries (of bounded
length) and each element is uniquely identiﬁed by the query text.
Note that any unique query may appear multiple times in the
stream. The features associated with a query could include, e.g.,
the query length, the unigram of the query text (possibly after

• D. Bertsimas is with the Sloan School of Management and the Operations
Research Center, Massachusetts Institute of Technology, Cambridge, MA,
02139.
E-mail: dbertsim@mit.edu.

• V. Digalakis Jr. is with the Operations Research Center, Massachusetts

Institute of Technology, Cambridge, MA, 02139.
E-mail: vvdig@mit.edu.

Submitted: 07/2020. Revised: 05/2021.

some pre-processing), etc. The goal is to estimate the frequency
distribution of the queries, that is, the number of times each
query appears in the stream, in space much smaller than the total
number of unique queries.

Massive data streams appear in a variety of applications.
For example, in search query monitoring, Google received
more than 1.2 trillion queries in 2012 (which translates to 3.5
billion searches per day) [3]. In network trafﬁc monitoring,
AT&T collects over one terabyte of NetFlow [4] measure-
ment data from its production network each day [2]. More-
over, the IPV6 protocol provides nearly 2128 addresses, mak-
ing the universe of possible IP addresses gigantic, especially
considering that, in many applications, we are interested
in monitoring active IP network connections between pairs
(source/destination) of IP addresses. Thus, being able to
process a data stream in sublinear space is essential.

Maintaining the frequency distribution of a stream of el-
ements is useful, not only as a sufﬁcient statistic for various
empirical measures and functionals (e.g., entropy [5]), but
also to identify interesting patterns in the data. An example
are the so-called “heavy-hitters” [6], [7], that is, the elements
that appear a big number of times, which, e.g., could be
indicative of denial of service attacks in network trafﬁc
monitoring (see [2] for a detailed discussion of applications).
In this paper, we address the problem of frequency
estimation in data streams, under the additional assump-
tion that a preﬁx of the input stream has been observed.
Along the lines of [8], who address the same problem and
extend classical streaming frequency estimation algorithms
with a machine learning component, we aim to exploit the
observed preﬁx and the features associated with each ele-
ment, and develop data-driven streaming algorithms. The
proposed algorithms satisfy the small-space requirement,
as they signiﬁcantly compress the input frequency vector,
and do operate in a single pass and in real-time, as their

 
 
 
 
 
 
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

2

update and query times are constant (except for the training
phase, which is more computationally demanding, since we
perform optimization and machine learning).

Instead, we use optimization to learn an optimal (or near-
optimal) hashing scheme from (training) data, and machine
learning to hash “unseen elements,” which did not appear
in the training data.

1.1 Streaming Frequency Estimation Algorithms

A rich body of research has emerged in the streaming
model of computation [1], [2]; the ﬁrst streaming algorithms
appeared in the early 1980s, to address, in limited space,
problems such as ﬁnding the most frequently occurring
elements in a stream [6]. A vast literature has since been de-
veloped, especially since the 1990s, and numerous problems,
including complex machine learning tasks, such as decision
tree induction [9], can now be solved in streaming settings.
Sketches [10] are among the most powerful tools to
process streaming data. A sketch is a data structure which
can be represented as a linear transform of the input. For
example, in the context of frequency estimation, the input
is the vector of frequencies (or frequency distribution) of
the input elements and the sketch is computed by multiply-
ing the frequency distribution by a ﬁxed, “fat” matrix. Of
course, for compactness, the matrix that performs the sketch
transform is never explicitly materialized and is implicitly
implemented via the use of random hash functions.

Any given sketch transform is deﬁned for a particular
task. Among the most popular sketching methods for the
task of frequency estimation, are the Count-Min Sketch [11]
and the Count Sketch [12], which both rely on random
hashing and differ in their frequency estimation procedure.
Historically, the so-called AMS Sketch [13], which addresses
the task of estimating the sum of the squares of the fre-
quencies of the input stream, was among the ﬁrst sketching
algorithms that have been proposed. Sketching algorithms
have found numerous applications, including in measuring
network trafﬁc [14], in natural language processing [15],
in signal processing and compressed sensing [16], and in
feature selection [17].

1.2 Learning-Augmented Streaming Algorithms

The abundance of data that is available today has moti-
vated the development of the ﬁeld of learning-augmented
algorithms, whereby traditional algorithms are modiﬁed to
leverage useful patterns in their input data. More speciﬁ-
cally, in the context of streaming algorithms, [18] and [19]
augment with a machine learning oracle the Bloom ﬁlter
[20], [21], a widely used probabilistic data structure that tests
set membership. [8] develop learning-based versions of the
Count-Min Sketch and the Count Sketch; a similar approach
is taken by [22], who focus on network monitoring and
develop a generalized framework to augment sketches with
machine learning. The latter two approaches use machine
learning in parallel with a standard (random) sketch, that
is, they combine a machine learning oracle with standard
(conventional) streaming frequency estimation algorithms,
such as the Count-Min Sketch.

In this paper, we consider the same problem as in
[8], namely learning-based streaming frequency estimation.
However, our approach fundamentally differs in that we
combine a (non-random) sketch (i.e., the optimal hashing
scheme) and machine learning into a new estimator and
hence our approach does not rely on random hashing at all.

1.3 Learning to Hash

The proposed approach has connections with the ﬁeld of
learning to hash, a data-dependent hashing approach which
aims to learn hash functions from a speciﬁc dataset (see [23]
for a comprehensive survey). Learning to hash has mostly
been considered in the context of nearest neighbor search,
i.e., learning a hashing scheme so that the nearest neighbor
search result in the hash coding space is as close as possible
to the search result in the original space. Optimization-
based learning to hash approaches include the works [24],
[25], [26]. [27] develop an optimal data-dependent hashing
scheme for the approximate nearest neighbor problem. To
the best of our knowledge, our approach is the ﬁrst that con-
siders learning hashing schemes for the streaming frequency
estimation problem, whereby the objective is different.

1.4 Learning-Augmented Algorithms beyond Stream-
ing and Hashing

[28] use
Beyond streaming and hashing algorithms,
machine-learned predictions to improve the performance
of online algorithms. [29] use reinforcement learning and
neural networks to learn workload-speciﬁc scheduling algo-
rithms that, e.g., aim to minimize the average job completion
time. Machine learning has also been used outside the ﬁeld
of algorithm design, e.g., in signal processing and, speciﬁ-
cally, in the context of “structured” (instead of sparse) signal
recovery [30] and in optimization. [31] and [32] propose ma-
chine learning-based approaches for variable branching in
mixed-integer optimization, [33] use reinforcement learning
to learn combinatorial optimization algorithms over graphs,
[34] use interpretable machine learning methods to learn
strategies behind the optimal solutions in continuous and
mixed-integer convex optimization problems as a function
of their key parameters, and [35] focus speciﬁcally on online
mixed-integer optimization problems. Machine learning has
also been popularized in the context of data management
and, in particular, in tasks such as learning index structures
[18] and query optimization [36], [37].

1.5 Contributions

Our key contributions can be summarized as follows:

- We develop a novel approach for the problem of
frequency estimation in data streams that is based on
optimization and machine learning. By exploiting an
observed stream preﬁx, the proposed learning-based
streaming frequency estimation algorithm achieves
superior performance compared to conventional
streaming frequency estimation algorithms.

- We present an exact mixed-integer linear optimiza-
tion formulation, as well as an efﬁcient block coor-
dinate descent algorithm, that enable us to compute
near-optimal hashing schemes and provide a smart
alternative to oblivious random hashing schemes.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

3

This part of our work could be of independent inter-
est, beyond the problem of frequency estimation in
data streams. Further, in a special case, we are able
to solve the proposed formulation exactly in linear
time using dynamic programming.

- We apply the proposed approach to the problem of
search query frequency estimation and evaluate it
using both synthetic and real-world data. Computa-
tional results indicate that the proposed approach no-
tably outperforms state-of-the-art non-learning and
learning-based approaches in terms of its estima-
tion error. Moreover, the proposed approach is by
construction interpretable and enables us to get ad-
ditional insights into the problem of search query
frequency estimation.

The rest of the paper is organized as follows. In Section 2,
we formalize the streaming frequency estimation problem
and present, at a high level, the Count-Min Sketch, the most
widely used random hashing-based streaming frequency
estimation algorithm, and the Learned Count-Min Sketch,
a learning-augmented version of the Count-Min Sketch.
Section 3 gives an overview of the proposed approach.
In Section 4, we formulate the problem of learning the
optimal hashing scheme using the observed stream preﬁx
and develop efﬁcient optimization algorithms. Section 5 de-
scribes the frequency estimation procedure we apply, after
the optimal hashing scheme is learned. In Section 6, we use
synthetic data to explore the performance and scalability
of the proposed algorithms and investigate the impact of
various design choices on the proposed approach. Section 7
empirically evaluates the proposed approach on real-world
search query data. Section 8 concludes the paper.

2 PRELIMINARIES

In this section, we formally describe the problem of fre-
quency estimation in data streams and present the state-
of-the-art approaches to solving it. Although we explain the
notation that we use in the main text, for convenience, we
also gather the basic notations in Table 2 in Appendix C.

Formally, we are given input data in the form of an

ordered set of elements

S = (u1, u2, . . . , u|S|),

where ut ∈ U, ∀t ∈ [|S|]
:= {1, ..., |S|}, and U is the
universe of input elements. Each element u ∈ U is of the
form

u = (k, x),

where (without loss of generality) k ∈ [|U|] is a unique ID
and x ∈ X is a set of features associated with u. The goal
is, at the end of S, given an element u ∈ U , to output an
estimate ˜fu of the frequency

fu =

|S|
(cid:88)

t=1

1(ut=u)

min{|S|, |U|}. We work under the additional assumption
that a preﬁx S0 = (u1, u2, . . . , u|S0|) (where |S0| (cid:28) |S|)
of the input stream has already been observed.

2.1 Conventional Approach: Random Sketches

The standard approach to attack this problem is the well-
known Count-Min Sketch (CMS) [11], a probabilistic data
structure based on random hashing that serves as the fre-
quency table of S. In short, CMS randomly hashes (via a
random linear hash function hash(·)) each element u ∈ U
to a bucket in an array φ of size w (cid:28) min{|S|, |U|};
whenever element u occurs in S, the corresponding counter
φhash(u) is incremented. Since w (cid:28) |U|, multiple elements
are mapped to the same bucket and φhash(u) overestimates
fu. In practice, multiple arrays φ1, ..., φd are maintained
(each array is referred to as a “level”) and the ﬁnal estimate
for fu is

˜fu = min
l∈[d]

φl

hashl(u),

where hashl(·) is the hash function that corresponds to the
l-th level. Intuitively, by repeating the estimation procedure
multiple times and taking the minimum of the estimated
frequencies (all of which overestimate the actual frequency),
the resulting estimator’s accuracy will improve. CMS pro-
vides probabilistic guarantees on the accuracy of its esti-
mates, namely, for each u ∈ U, with probability 1 − δ,

| ˜fu − fu| ≤ (cid:15)||f ||1,

where (cid:15) = e
w × d buckets.

w and δ = e−d. In total, CMS consists of b =

2.2 Learning-Based Approach: Learned Sketches

To leverage the observed stream preﬁx, [8] augment the clas-
sical CMS algorithm as follows. Noticing that the elements
that affect the estimation error the most are the so-called
heavy-hitters (i.e., elements that appear many times), they
propose to train a classiﬁer

hHH : X → {heavy, heavy}

that predicts whether an element u = (k, x) is going to be a
heavy-hitter or not. 1 Then, they allocate bheavy unique buck-
ets to elements identiﬁed as heavy-hitters, and randomly
allocate the remaining brandom = b − 2bheavy buckets to the
rest of the universe, using, e.g., the standard CMS. We call
their algorithm the Learned Count-Min Sketch (LCMS).

An important remark is that each of the bheavy unique
buckets allocated to heavy-hitters should maintain both
the frequency and the ID of the associated element. As
explained, this can be achieved by using hashing with
open addressing, whereby it sufﬁces to store IDs hashed
into log bheavy + t bits (instead of whole IDs which could
be arbitrarily large) to ensure there is no collision with
probability 1 − 2−t. Noticing that log bheavy + t is com-
parable to the number of bits per counter, the space for

of that element, i.e., the number of times the element ap-
pears in S; here, 1A denotes the indicator function of event
A. We assume that both S and U are huge, so we wish
to produce accurate estimates in space much smaller than

1.

[8] identify the heavy-hitters by ﬁrst predicting the element
frequencies (or log-frequencies) using machine learning and then select-
ing, using validation data, the optimal cutoff threshold for an element to
be considered a heavy-hitter. In their experiments, they predict whether
an item is in the top 1% of the frequencies.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

4

a unique bucket is twice the space of a normal bucket.
The learning augmented algorithm is shown to outperform,
both theoretically and empirically, its conventional, fully-
random counterpart. Additionally, they prove that under
certain distributional assumptions, allocating unique buck-
ets to heavy-hitters is asymptotically optimal [8], [38]. In
general, however, their approach remains heuristic, does not
guarantee optimal performance, and possibly throws away
information by taking hard, binary decisions.

3 OVERVIEW OF THE PROPOSED APPROACH
Motivated by the success of LCMS, we investigate an alter-
native, optimization-based approach in using the observed
stream preﬁx to enhance the performance of the frequency
estimator.

At a high level, the proposed two-phase approach works
as follows. In the ﬁrst phase, the elements that appeared in
the stream preﬁx are optimally allocated to buckets based on
their observed frequencies so that the frequency estimation
error is minimized and, at the same time, similar elements
are mapped to the same bucket. Importantly, contrary to
CMS-based approaches, in the proposed approach, the es-
timate for an element’s frequency is the average of the
frequencies of all elements that are mapped to the same
bucket. Therefore, we aim to assign “similar” elements to
the same bucket. In the second phase, once we have an
optimal allocation of the elements that appeared in the
preﬁx to buckets, we train a classiﬁer mapping elements to
buckets based on their features. By doing so, we are able to
provide estimates for unseen elements that did not appear
in the preﬁx and hence their frequencies are not recorded.

The proposed hashing scheme consists of a hash table
mapping IDs of elements that appeared in the preﬁx to
buckets and the learned classiﬁer. In addition, for each
bucket, we need to maintain the sum of frequencies of all
elements mapped therein. During stream processing, that is,
once the estimator is ready, whenever an element that had
appeared in the preﬁx re-appears, we increment the counter
(i.e., the aggregated frequency) of the bucket to which the
element was mapped. Finally, to answer count-queries for
any given element, we simply output the current average
frequency of the bucket where the element is mapped (either
via the hash table or via the classiﬁer).

Appendix B provides ﬂowcharts for the proposed ap-
proach, which further explain the learning phase, where the
stream preﬁx is used to learn the optimal hashing scheme
and the classiﬁer, illustrate how the proposed approach
answers count queries for any input element, and show the
update mechanism of the proposed approach.

4 LEARNING THE OPTIMAL HASHING SCHEME
In this section, we develop the proposed approach in learn-
ing the optimal hashing scheme.

4.1 Exact Formulation
Let S0 = (u1, ..., u|S0|) be the observed stream preﬁx. We
denote by f 0
u the empirical frequency of element u in S0,
i.e.,

f 0
u =

1(ut=u),

|S0|
(cid:88)

t=1

and by f 0(S0) the entire frequency distribution after observ-
ing S0. Moreover, U0 = {u ∈ U : f 0
u > 0} is the set of all
distinct elements that appeared in S0 and let |U0| = n. We
introduce n × b binary variables, where b is the total number
of available buckets, deﬁned as

zij =

(cid:26) 1,
0,

if ith element of U0 is mapped to bucket j,
otherwise.

Each row zi of Z (where we denote [Z]ij = zij) can be
viewed as an one-hot binary hash code mapping element i
to one of the buckets. At the end of the stream and given a
ﬁxed assignment for the variables zij, the ﬁnal estimate of
the frequency of element i ∈ [n] is

˜fi =

(cid:88)

zij

j∈[b]

(cid:32) (cid:80)

k∈[n] zkjfk
(cid:80)
k∈[n] zkj

(cid:33)

.

(cid:80)

(cid:80)

(cid:80)

1
k∈[n] fk

The resulting, e.g., absolute estimation error is (cid:80)
i∈[n] | ˜fi −
fi|; a natural objective is to pick the variables zij that
minimize this absolute error in the observed stream preﬁx.
An alternative objective we could pick is the expected
i∈[n] fi · | ˜fi − fi|,
magnitude of the absolute error
whereby it is assumed that the probability pi of observing
element i is equal to its empirical probability in the observed
fi
stream preﬁx, i.e., pi :=
. In fact, this metric is used
k∈[n] fk
by [8] in their theoretical analysis. However, such an ap-
proach would heavily weigh the most frequently occurring
elements and would probably produce highly inaccurate
estimates for less frequent elements. As we would like to
achieve a uniformly small estimation error, we stick to the
former objective and select the variables zij that solve the
optimization formulation which we will present shortly. We
incorporate an additional term in the objective function of
the proposed formulation, to take the features associated
with each element into account when computing the opti-
mal mapping of elements to buckets. For λ ∈ [0, 1], we have:

min
Z∈{0,1}n×b

(cid:88)

(cid:88)

i∈[n]

j∈[b]

(cid:34)

zij

λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f 0
i −

(cid:80)

k∈[n] zkjf 0
k
(cid:80)
k∈[n] zkj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

zkj(cid:107)xi − xk(cid:107)2





+(1 − λ)

(cid:88)

k∈[n]

zij = 1,

∀i ∈ [n].

s.t.

(cid:88)

j∈[b]

(1)

The parameter λ ∈ [0, 1] controls the trade-off between
hashing schemes that map to the same bucket elements
that are similar in terms of their observed frequencies in the
preﬁx (λ → 1) and hashing schemes that put more weight
on the elements’ feature-wise similarity (λ → 0). Therefore,
we refer to the ﬁrst term in the objective as the estimation
error and to the second term as the similarity error.

Problem (1) is a nonlinear binary optimization problem,
so it is, in principle, hard to solve. Therefore, we next
develop different approaches that can be used to solve it
to optimality or near-optimality in different regimes.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

5

4.2 Mixed-Integer Linear Reformulation

As we show next, Problem (1) can be as reformulated as a
mixed integer linear optimization problem by introducing
auxiliary variables and new constraints. Formally, we have
the following theorem:

Theorem 1. Problem (1) is equivalent with the following mixed-
integer linear optimization problem:

(cid:88)

(cid:88)

i∈[n]

j∈[b]


λθiij + (1 − λ)

(cid:88)

k∈[n]

δikj(cid:107)xi − xk(cid:107)2





min
Z∈{0,1}n×b,
E∈Rn×b
≥0 ,
Θ∈Rn×n×b
,
≥0
∆∈[0,1]n×n×b

s.t.

(cid:88)

j∈[b]

zij = 1,

∀i ∈ [n],

(cid:88)

k∈[n]

(cid:88)

k∈[n]

θikj − f 0
i

(cid:88)

zkj +

(cid:88)

f 0
k zkj ≥ 0,

k∈[n]

k∈[n]

θikj + f 0
i

(cid:88)

zkj −

(cid:88)

∀i ∈ [n], ∀j ∈ [b],
f 0
k zkj ≥ 0,

k∈[n]

k∈[n]

∀i ∈ [n], ∀j ∈ [b],

θikj ≥ eij − M (1 − zkj),

∀i ∈ [n], ∀k ∈ [n], ∀j ∈ [b],

θikj ≤ eij,

θikj ≤ M zkj,

∀i ∈ [n], ∀k ∈ [n], ∀j ∈ [b],

∀i ∈ [n], ∀k ∈ [n], ∀j ∈ [b],

δikj ≥ zij + zkj − 1,

δikj ≤ zij,

δikj ≤ zkj,

∀i ∈ [n], ∀k ∈ [n], ∀j ∈ [b],

∀i ∈ [n], ∀k ∈ [n], ∀j ∈ [b],

∀i ∈ [n], ∀k ∈ [n], ∀j ∈ [b],

where M is a constant that satisﬁes M ≥ maxi∈[n] f 0
i .

(2)

Proof. The proof is presented in Appendix A.

Problem (2) consists of O(n2b) variables and constraints.
As our computational study in Section 6 suggests, by solv-
ing the reformulated Problem (2), we are able to compute
optimal hashing schemes for problems with thousands of
elements. Nevertheless, solving a mixed integer linear op-
timization problem of that size can still be prohibitive in
the applications we consider. For example, in the real-world
case study in Section 7, we map up to tens of thousands of
elements to up to thousands of buckets, so Formulation (2)
would consist of variables and constraints in the order of
1011. Therefore, we next develop a tailored block coordinate
descent algorithm that works well in practice.

(cid:105)

(cid:104) U0
b

1) that can be used to either heuristically solve Problem (1)
or compute high-quality warm starts for Problem (2).

Concerning the algorithm’s initialization, we start from
a random allocation of elements to buckets. Alternatively,
we could sort elements in U0 in terms of their observed
frequencies and allocate the ﬁrst
elements to the ﬁrst

(cid:105)

(cid:104) U0
b

to the second bucket, and so forth,
bucket, the next
or we could even use the heavy-hitter heuristic (that is,
assign heavy-hitters to their own bucket and the remaining
elements at random).

In our implementation, we maintain, for each bucket,
the set of elements Ij mapped therein, its cardinality cj
and mean frequency µj, as well as the associated esti-
(cid:12)
(cid:12)
mation error ej = (cid:80)
(cid:12)f 0
(cid:12) and similarity error
sj = (cid:80)
(cid:107)xi − xk(cid:107)2. After any update performed
by Algorithm 1 we only need to update the above quantities,
instead of having to recompute them from scratch and,
therefore, we can directly evaluate the objective function
value ε associated with any particular mapping of elements
to buckets.

(i,k)∈Ij ×Ij

i − µj

i∈Ij

In each iteration, Algorithm 1 examines sequentially and
in random order all n blocks of b variables zi, i ∈ [n].
Notice that each block contains all possible mappings of
a particular element to any bucket. For each element i,
we greedily select the mapping that minimizes the overall
estimation error. To do so, we remove element i from its
current bucket and compute the estimation error associated
with each bucket j, ﬁrst with element i allocated to bucket
j and then without element i. We allocate element i to the
bucket j(cid:63) that minimizes the sum of all error terms.

The algorithm terminates when the improvement in
estimation error is negligible; in case we are willing to obtain
an intermediate solution faster, the termination criterion can
be set to a user-speciﬁed maximum number of iterations.
As we empirically show, Algorithm 1 converges to a local
optimum after a few tens of iterations and produces high-
quality solutions. Given that algorithm is not guaranteed to
converge to a globally optimum solution, the process can be
repeated multiple times from different starting points.

Algorithm 1 can be efﬁciently implemented so that the
complexity of each iteration is O(n2b). This is to be expected
since, for each bucket, we need to compute the similarity
error between all pairs of elements mapped therein, which
requires O(n2b) operations.

4.4 The λ = 1 Case: Efﬁcient Dynamic Programming
Algorithm
In the special case where we set λ = 1, that is, we do not
take the features into account when computing the optimal
hashing scheme, we obtain the following formulation:

min
Z∈{0,1}n×b

s.t.

(cid:88)

(cid:88)

zij

j∈[b]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f 0
i −

(cid:80)

k∈[n] zkjf 0
k
(cid:80)
k∈[n] zkj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(3)

zij = 1,

∀i ∈ [n].

i∈[n]
(cid:88)

j∈[b]

4.3 Efﬁcient Block Coordinate Descent Algorithm

By exploiting the problem structure, we propose the follow-
ing efﬁcient block coordinate descent algorithm (Algorithm

Problem (3) is an one-dimensional k-median clustering
problem and has been thoroughly studied in the literature.
It is fairly straightforward to develop an O(n2b) dynamic

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

Algorithm 1: Block Coordinate Descent Algorithm.

6

Input: Observed frequency vector f 0 ∈ Nn, number of buckets b ∈ N, hyperparameter λ ∈ [0, 1].
Output: Learned one-hot hashing scheme Z ∈ {0, 1}n×b.
1: Initialize Z satisfying (cid:80)
2: ε0 ← 0
3: for j ∈ [b] do

(cid:46) Objective function value for initial map

j∈[b] zij = 1, ∀i ∈ [n]

(cid:46) Find set of elements, cardinality, and mean for bucket j in initial map:

Ij, cj, µj ← {i ∈ [n] : zij = 1}, |Ij|,

f 0
i

(cid:80)

i∈Ij
cj

(cid:46) Compute estimation error ej and similarity error sj for bucket j in initial map:
(cid:12)
ej, sj ← (cid:80)
(cid:12) , (cid:80)
i∈Ij
ε0 ← ε0 + [λej + (1 − λ)sj]

(cid:107)xi − xk(cid:107)2

(i,k)∈Ij ×Ij

i − µj

(cid:12)
(cid:12)f 0

8:
9: end for

10: t ← 0
11: repeat
12: Draw a random permutation σ of the set [n]
13:
14:

for i ∈ [n] do

for j ∈ [b] do

(cid:46) Check if σi is already in bucket j and compute error with and without σi:
if σi ∈ Ij then

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

f 0
k −

k∈Ij \{σi}

εσi,j ← λej + (1 − λ)sj
(cid:18)
cj µj −f 0
σi
ε−σi,j ← λ
cj −1
(cid:46) Update bucket j stats and errors after removing σi:
Ij, cj, µj ← Ij \ {σi}, cj − 1,
ej, sj ← (cid:80)
(cid:18)

cj µj −f 0
σi
cj −1
(cid:12)
(cid:12) , sj − 2 (cid:80)

(cid:16)
sj − 2 (cid:80)

+ (1 − λ)

(cid:12)
(cid:12)f 0

k∈Ij

k∈Ij

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)xσi − xk(cid:107)2
k∈Ij
(cid:12)
(cid:19)
(cid:12)
(cid:12)
(cid:12)

+ (1 − λ)

(cid:16)

f 0
k −

cj µj +f 0
σi
cj +1

else

k − µj
(cid:12)
(cid:12)
εσi,j ← λ
(cid:12)
(cid:12)
ε−σi,j ← λej + (1 − λ)sj

k∈Ij ∪{σi}

(cid:80)

end if

sj + 2 (cid:80)

k∈Ij

(cid:107)xσi − xk(cid:107)2(cid:17)

(cid:107)xσi − xk(cid:107)2(cid:17)

end for
(cid:46) Find best bucket j(cid:63) and update stats and errors after mapping σi to it:
j(cid:63) ← argminj∈[b]εσi,j + (cid:80)
zi ← ej(cid:63)
Ij(cid:63) , cj(cid:63) , µj(cid:63) ← Ij(cid:63) ∪ {σi}, cj(cid:63) + 1,
ej(cid:63) , sj(cid:63) ← (cid:80)

(cid:46) ej(cid:63) denotes the j(cid:63)-th standard unit vector
cj(cid:63) µj(cid:63) +f 0
σi
cj(cid:63) +1
k∈Ij(cid:63) (cid:107)xσi − xk(cid:107)2

(cid:96)∈[b]\{j} ε−σi,(cid:96)

(cid:12)
(cid:12) , sj(cid:63) + (cid:80)

k − µj(cid:63)

(cid:12)
(cid:12)f 0

k∈Ij(cid:63)

4:

5:

6:
7:

15:
16:
17:

18:

19:

20:

21:
22:

23:

24:
25:

26:

27:
28:
29:

30:

31:
end for
32:
t ← t + 1
33:
εt ← (cid:80)
34:
35: until εt−1 − εt < (cid:15)
36: return Z

j∈[b] [λej + (1 − λ)sj]

programming algorithm to solve Problem (3) to provable
optimality as per [39]. An even more efﬁcient solution
method for Problem (3) has been developed in the context
of optimal quantization; using dynamic programming in
combination with a matrix searching technique, [40] solves
Problem (3) to optimality in O(nb) time. We refer the inter-
ested reader to [41] for a detailed and uniﬁed presentation
of the above methods.

Given that we can obtain an optimal solution to Problem
(3) very fast, in O(nb) time, we propose to use it as a warm
start for the general λ ∈ [0, 1) case. Therefore, we provide
another alternative for the initialization step of Algorithm 1,
in addition to the ones discussed in Section 4.3.

5 FREQUENCY ESTIMATION

In this section, we describe the frequency estimation compo-
nent of the proposed estimator, which, in its simplest form,
consists of a multi-class classiﬁer.

5.1 Frequency Estimation for Elements Seen in the Pre-
ﬁx

Once the optimal assignment Z is computed, we essentially
have a hash code hi = (cid:80)
j∈[b] j · 1(zij =1), i ∈ [n], for each

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

7

element u ∈ U0. Therefore, for element u ∈ U0, indexed by
i ∈ [n], we simply estimate its frequency as
(cid:80)

˜fi =

(cid:80)

k∈[n]:hk=hi

k∈[n]:hk=hi

fk
1

= µj.

We denote by hS : U0 → [b] the function that maps elements
seen in the preﬁx to buckets according to the learned hash
code.

5.2 Similarity-Based Frequency Estimation for Unseen
Elements

To be able to produce frequency estimates for elements that
did not appear in the preﬁx, i.e., u ∈ U \ U0, we formulate
a multi-class classiﬁcation problem, mapping elements to
buckets based on their features. Formally, we search for a
function

hU : X → [b].

The training set consists of all data points in

{(xi, hi) : ui = (ki, xi) ∈ U0},

that is, all feature-hash code tuples for elements that ap-
peared in the preﬁx. Such a classiﬁer will allow us to
estimate the frequencies of unseen elements based on the
average of the frequencies of elements that “look” similar.
The estimate for element u = (k, x) ∈ U \ U0 is then

˜fu =

(cid:80)

fk

k∈[n]:hk=hU(x)
(cid:80)

1
k∈[n]:hk=hU(x)

.

5.3 Adaptive Counting Extension: Keeping Track of the
Frequencies of Unseen Elements

So far, we have described a static approach; we learn the
optimal hashing scheme for the elements that appear in the
stream preﬁx and then keep track only of their frequencies.
The estimated frequencies for all elements are based only
on the frequencies of elements in U0 (which appeared in
S0). We next describe a dynamic approach, that keeps track
of the frequencies of elements beyond the ones in U0. At a
high level, the adaptive approach is based on approximately
counting the distinct elements in each bucket. We work as
follows.

1) We learn the optimal hashing scheme based on
the observed stream preﬁx and train a classiﬁer
mapping elements to buckets, as outlined above. For
each bucket, we only record the number of elements
that are mapped therein (instead of storing the IDs
of the elements that are mapped to this bucket).
We use the classiﬁer to determine which bucket any
element is mapped to.

2) We maintain a Bloom ﬁlter [20] BF, i.e., a probabilis-
tic data structure that, given a universe of elements
U and a set U (cid:48) ⊆ U , probabilistically tests, for
any element u ∈ U , whether u ∈ U (cid:48) (here, U (cid:48)
corresponds to the elements that have appeared in
the stream). If u ∈ U (cid:48), then we deterministically
have that BF(u) = 1. However, if u (cid:54)∈ U (cid:48), then
it need not be the case that BF(u) = 0 (therefore

a Bloom ﬁlter is prone to false positives - we will
explain the impact of those in the sequel).

4)

3) We initialize the Bloom ﬁlter based on the elements
u ∈ U0. Therefore, all elements u ∈ U0 will initially
have BF(u) = 1. On the other hand, elements u (cid:54)∈ U0
may initially have either BF(u) = 0 or BF(u) = 1.
For every subsequent element u that appears in
the stream after the stream preﬁx S0 has been pro-
cessed, we map it to a bucket j ∈ [b] using the
trained classiﬁer. Then, we test whether we have
already seen u, using the Bloom ﬁlter. If BF(u) = 0,
we increase both the frequency φj and the number
of elements cj in the bucket j, and we set BF(u) = 1.
If BF(u) = 1, we only increase the frequency φj.
5) When queried for the frequency of any element u ∈
U , regardless of whether it appeared in U0 or not,
we estimate

˜fu =

φj
cj
where j is the bucket in which u is mapped using
the classiﬁer.

BF (u),

The impact of Bloom ﬁlters’ false positives is that the
proposed approach will mark as seen elements that have
not appeared in the stream. When one such element actually
appears in the stream, we will not increase the counter cj
that tracks the number of elements in the bucket j where
this element is mapped. Therefore, the estimated number of
elements cj in bucket j will be less than the actual number.
As a result, the adaptive counting extension will generally
overestimate elements’ frequencies.

The ﬂowchart for the adaptive counting extension of the

proposed approach is given in Figure 9d in Appendix B

6 EXPERIMENTS ON SYNTHETIC DATA
In this section, we empirically evaluate the proposed ap-
proach on synthetic data. We investigate the performance
and scalability of the optimization approaches discussed
in Section 4, and explore the possibility of using different
classiﬁers for unseen elements (as per Section 5).

6.1 Data Generation Methodology

The data that we use in our synthetic experiments are
generated according to the following methodology:

- Elements: We parameterize the universe of elements
U by a positive integer G ∈ Z>0 that controls the
problem size in the way that we explain next. We
generate G groups of elements G1, . . . , GG of expo-
nentially increasing sizes 2G0+1, . . . , 2G0+G (where
G0 ∈ Z≥0 is an additional parameter that determines
the size of the smallest group; we use G0 = 2 in our
experiments). We associate each group Gg, g ∈ [G],
with a p-dimensional normal distribution (we use
p = 2 in our experiments to enable visualization)
with mean µg selected uniformly at random from
[−10, 10]p and covariance matrix equal to the iden-
tity. We draw the features associated with each ele-
ment u ∈ Gg as a realization of the p-dimensional
normal distribution N (µg, I) that corresponds to the
element’s group.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

8

-

Stream: We generate the data stream S according
to the following process. We associate each group
Gg, g ∈ [G] with an arrival probability that is pro-
portional to 1
g . Within group Gg, we assign to each
element u ∈ Gg a uniform probability of arrival
1
|Gg| . Thus, smaller groups are more likely to appear
and elements therein have a larger probability of
selection so that they represent the heavy hitters. We
construct the stream by ﬁrst selecting the group that
each new arrival belongs to and then selecting the
actual element from within that group. As far as the
stream preﬁx S0 is concerned, we would want to
mimic a real-world scenario where not all elements
from within each group start appearing since the be-
ginning of the stream. Therefore, when we generate
the preﬁx, we only allow for a fraction g0 ∈ [0, 1]
of elements to be selected from within each group
Gg, g ∈ [G], each with probability
g0|Gg| . Finally, we
remark that, in our experiment, we generate a stream
preﬁx of size |S0| = 10 · 2G.

1

For example, by setting G = 10 and g0 = 0.5, we obtain a
problem with 8, 192 elements, out of which we only allow
for 4, 096 to appear in the preﬁx, which in turn has size
10, 240. Therefore, we aim to learn a hashing scheme that
maps at most 4, 096 elements to 10 buckets; the memory
requirements of such a hashing scheme would be ≈ 20 KB.

6.2 Algorithms and Software

We next summarize the algorithms and software that we use
in our experiments. We note that all algorithms were imple-
mented in Python 3 and all experiments were performed
on a standard Intel(R) Xeon(R) CPU E5-2690 @ 2.90GHz
running CentOS release 7. We independently repeat each
experiment 10 times and report the averaged error, as well
as its standard deviation.

We implement and refer to the optimization algorithms

presented in Section 4 as follows:

-

-

-

milp: Solves the mixed-integer linear optimization
problem (Problem (2)) from Section 4.2 using the
commercial MIO solver Gurobi [42].
bcd: Implements the block coordinate descent algo-
rithm (Algorithm 1) from Section 4.3.
dp: Solves Problem (3) in linear time via dynamic
programming (Section 4.4) using a Python wrapper
for the R package Ckmeans.1d.dp [39].

The machine learning algorithms that we examine in-
clude a linear classiﬁer, namely, multinomial logistic regres-
sion (logreg), a tree-based classiﬁer, namely, CART (cart)
[43], and an ensemble classiﬁer, namely, random forest (rf)
[44]. All methods are tuned using 10-fold cross validation;
the hyperparameters that we tune are the weight of a ridge
regularization term for logreg, the minimum impurity
decrease and the maximum depth for cart, the maximum
number of features in each split and the maximum depth
for rf. Unless stated otherwise, we use cart as the under-
lying classiﬁer in our experiments. We use the Scikit-learn
machine learning package’s implementation of all the above
algorithms [45].

Finally, we use the following notation for the frequency
estimation algorithms presented in this paper. We refer to
the proposed estimator as opt-hash. We refer to CMS
(the standard Count-Min Sketch) as count-min and to
LCMS (the learned Count-Min Sketch with the heavy-hitter
heuristic) as heavy-hitter. We implement all the above
estimators in Python.

6.3 Visualization: Learned Hash Code for Seen and Un-
seen Elements

In Figure 1, we show an instance of a synthetically generated
problem with G = 10 groups (Figure 1a colors elements
depending on their actual group). Figure 1b shows the
logarithm of the frequency of each element that appeared
in a preﬁx of length |S0| = 1, 000; we assume that a fraction
of g0 = 0.33 elements from each group can appear in the
preﬁx. In Figure 1c, we present the learned hash code for
elements that actually appeared in the preﬁx (using the
bcd algorithm), whereas Figure 1d illustrates the hash code
predicted for unseen elements (using cart).

6.4 Results

k

(cid:80)

(cid:80)

i∈[n]

i∈[n]
(cid:80)

j∈[b] zij

j∈[b] zij
(cid:80)

k∈[n] zkj f 0
(cid:80)
k∈[n] zkj

We next present the results from our computational study
on synthetic data. Let Z denote the learned hash code;
for elements i ∈ S0, zi
is obtained using one of the
algorithms presented in Section 4; for elements i (cid:54)∈ S0,
zi is obtained using machine learning, as per Section 5.2.
Then, the metrics that we consider are the estimation er-
ror (cid:80)
(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
f 0
i −
, the similarity error
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k∈[n] zkj(cid:107)xi−xk(cid:107)2, and the overall error,
i.e., the convex combination of the above two error terms,
weighted by λ and 1 − λ, respectively, which is exactly the
objective function that we use in the proposed formulation.
We separately study the two error terms to shed light on
the trade-off that the proposed approach is faced with.
Moreover, we distinguish between the error on elements
which appeared in the preﬁx (and hence their estimate is
extracted from the learned hashing scheme) and the error
on unseen elements which did not appear in the preﬁx (and
hence their estimate is inferred using machine learning) to
examine the individual performance of each component of
the proposed approach. We also measure the running time
(in seconds) of the algorithms (note that the running time
includes the time to learn both the hashing scheme and the
classiﬁer).

Experiment 1: Impact of hyperparameter λ. In this
experiment, we study the impact of the hyperparameter
λ on the learned hashing scheme. We set G = 6 and run
three different versions of opt-hashfor varying λ: one
that uses milp to learn the hashing scheme, one uses bcd,
and one uses dp. We record the estimation, similarity, and
overall error on the preﬁx, as well as the running time of
each algorithm. To examine the degree of sub-optimality
of bcd, we present the actual values of the error terms
that constitute the objective function (estimation, similarity,
and overall error), i.e., we do not convert them in a per
element/per pair of elements scale, which would be more

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

9

(a) Element groups.

(b) Preﬁx element frequencies.

(c) Hash code for elements that
appeared in the preﬁx.

(d) Hash code for unseen ele-
ments.

Fig. 1. Visualization of element groups and hash codes.

interpretable. The results are presented in Figure 2. The key
takeaways from this experiment are as follows:

-

milp achieves the smallest overall error at the cost of
increased running times. Its edge over the heuristic
bcd approach can be veriﬁed in terms of the esti-
mation error, as it almost always improves over the
solution obtained by bcd.

- The solutions obtained by bcd are of high quality;
the improvement achieved by applying the exact
milp approach is often negligible. For small problem
sizes, the runtime of bcd is less than a second.

- As expected, dp achieves the smallest estimation
error, since it optimizes only for the estimation error
independently of the value of λ. In terms of the
similarity and the overall, the performance of dp is
notably worse, whereas its running time is less than
a second.

Note that, in the λ = 1 case, all three methods are able to
ﬁnd comparable near-optimal solutions. The small deviation
is due to suboptimality tolerances of the algorithms used.

Experiment 2: Comparison between bcd and dp in
the λ = 1 case. In this experiment, we focus on the
λ = 1 case and compare, for increasing values of G, bcd
with dp; in this case, the latter is guaranteed to ﬁnd the
optimal hashing scheme. We again record the estimation,
similarity, and overall error on the preﬁx, as well as the
running time of each algorithm. In this and in subsequent
experiment, we convert the errors in a per element/per pair
of elements scale. The results are presented in Figure 3. We
observe that, for problems with G ≤ 10, bcd computes near-
optimal solutions fast; however, as G further increases, the
performance of bcd deteriorates.

Experiment 3: bcd from multiple starting points in the
general λ case. In this experiment, we set λ = 0.5 and
run bcd multiple times from different starting points and
for increasing values of G to examine the stability of the
solutions obtained. We again record the estimation, similar-
ity, and overall error on the preﬁx, as well as the running
time of each algorithm. The results, presented in Figure 4,
indicate that bcd is robust to the (random) initialization of
the algorithm and computes stable solutions.

Experiment 4: Impact of the fraction of elements seen
in the preﬁx. In this experiment, we set G = 10 and vary
the value of g0, which controls the fraction of elements that

appear in the preﬁx. We explore two approaches for learning
the hashing scheme: ﬁrst, we set λ = 0.5 and run bcd;
then, we run dp (which implies λ = 1). We now record
the estimation and similarity error both on the preﬁx S0
and on elements that did not appear in S0 but did appear
within |S| = 10|S0| arrivals after S0. Figure 5 suggests that
observing more elements in the preﬁx results in a decrease
of the estimation error on both seen and unseen elements at
the cost of an increased similarity error.

Experiment 5: Comparison between classiﬁcation
methods. In this experiment, we set g0 = 0.33 and λ = 0.5,
vary the value of G, and explore the impact of using
different types of classiﬁers (logreg, cart, rf) as part of
opt-hash. We record the estimation, similarity, and overall
error on elements that did not appear in S0 but did appear
within |S| = 10|S0| arrivals after S0. We also report the
training time for each method. In Figure 6, we see that
there is indeed merit in using non-linear classiﬁers. We
remark, however, that the results heavily depend on the data
generating process.

7 EXPERIMENTS
SEARCH QUERY ESTIMATION

ON

REAL-WORLD

DATA:

In this section, we empirically evaluate the proposed ap-
proach on real-world search query data. The task of search
query frequency estimation seems particularly suited for
the proposed learning-based approach, given that popular
search queries tend to appear consistently across multiple
days.

7.1 Dataset

In the lines of [8], we use the AOL query log dataset, which
consists of 21 million search queries (with 3.8 million unique
ones) collected from 650 thousand anonymized users over
90 days in 2006. Each query is a search phrase in free
text; for example, the 1st most common query is “google”
and appears 251,463 times over the entire 90-day period,
the 10th is “www.yahoo.com” and its frequency is 37,436,
the 100th is “mys” and its frequency is 5,237, the 1000th
is “sharon stone” and its frequency is 926, the 10000th is
“online casino” and its frequency is 146, and so forth. As
shown in [8], the distribution of search query frequency
indeed follows the Zipﬁan law and hence the setting seems
ideal for their proposed algorithm (LCMS).

10505101050510105051010.07.55.02.50.02.55.07.510.00123410.07.55.02.50.02.55.07.510.010.07.55.02.50.02.55.07.510.010505101050510IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

10

(a) Estimation error on S0.

(b) Similarity error on S0.

(c) Overall error (objective function
value) on S0.

(d) Elapsed time (in sec).

Fig. 2. Impact of hyperparameter λ for G = 6.

(a) Estimation error on S0.

(b) Similarity error on S0.

(c) Overall error (objective function
value) on S0.

(d) Elapsed time (in sec).

Fig. 3. Comparison between dp and bcd for λ = 1.

(a) Estimation error on S0.

(b) Similarity error on S0.

(c) Overall error (objective function
value) on S0.

(d) Elapsed time (in sec).

Fig. 4. Comparison between bcd from multiple starting points for λ = 0.5.

(a) Estimation error on S0.

(b) Similarity error on S0.

(c) Estimation error on elements
u (cid:54)∈ S0 after |S| = 10|S0| arrivals.

(d) Similarity error on elements
u (cid:54)∈ S0 after |S| = 10|S0| arrivals.

Fig. 5. Impact of fraction of seen elements in the preﬁx (g0) for G = 10.

7.2 Baselines

As baselines, we use count-min and heavy-hitter. For
each method, we maintain multiple versions correspond-
ing to different values of the method’s hyperparameters
and report the best performing version. More speciﬁcally,
total number of buckets b),
for ﬁxed sketch size (i.e.,
we report the best performing for count-min’s depth
from the set d ∈ {1, 2, 4, 6} and for heavy-hitter’s
depth d ∈ {1, 2, 4, 6} and number of heavy-hitter buckets
bheavy ∈ {10, 102, 103, 104} (provided that bheavy ﬁts within
the available memory, i.e., bheavy ≤ b/2). Additionally, we

assume that heavy-hitter has access to an ideal heavy-
hitter oracle, i.e., the IDs of the heavy-hitters in the test set
(over the entire 90-day period) are known. Therefore, we
compare the proposed method with the ideal version of the
method proposed in [8], which was in fact shown to signif-
icantly outperform any realistically implementable version
of heavy-hitter that relied upon non-ideal heavy-hitter
oracles (e.g., recurrent neural network classiﬁer).

0.00.20.40.60.81.0lambda100200300400500prefix_estimation_errorExperiment 1bcddpmilp0.00.20.40.60.81.0lambda10000200003000040000prefix_similarity_errorExperiment 1bcddpmilp0.00.20.40.60.81.0lambda010000200003000040000prefix_overall_errorExperiment 1bcddpmilp0.00.20.40.60.81.0lambda0200400600800elapsed_timeExperiment 1bcddpmilp456789101112num_groups0.51.01.52.02.53.0prefix_estimation_errorExperiment 2456789101112num_groups0.000.250.500.751.001.251.501.75prefix_similarity_errorExperiment 2456789101112num_groups0.51.01.52.02.53.0prefix_overall_errorExperiment 2456789101112num_groups0100200300400500600700elapsed_timeExperiment 2456789101112num_groups2468prefix_estimation_errorExperiment 3456789101112num_groups0.050.100.150.200.250.30prefix_similarity_errorExperiment 3456789101112num_groups1234prefix_overall_errorExperiment 3456789101112num_groups0100200300400500600700800elapsed_timeExperiment 30.10.20.30.40.50.60.70.80.9fraction_seen024681012prefix_estimation_errorExperiment 4 - epoch 0bcddp0.10.20.30.40.50.60.70.80.9fraction_seen0.10.20.30.40.5prefix_similarity_errorExperiment 4 - epoch 0bcddp0.10.20.30.40.50.60.70.80.9fraction_seen0.00.10.20.30.40.50.60.7unseen_estimation_errorExperiment 4 - epoch 10bcddp0.10.20.30.40.50.60.70.80.9fraction_seen0.20.00.20.40.60.81.01.21.4unseen_similarity_errorExperiment 4 - epoch 10bcddpIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

11

(a) Estimation error on elements
u (cid:54)∈ S0 after |S| = 10|S0| arrivals.

(b) Similarity error on elements
u (cid:54)∈ S0 after |S| = 10|S0| arrivals.

(c) Overall error (objective function
value) on elements u (cid:54)∈ S0 after
|S| = 10|S0| arrivals.

(d) Elapsed time (in sec).

Fig. 6. Comparison between classiﬁcation methods.

7.3 Remarks on the Learned Hashing Scheme
As far as opt-hash is concerned, we make the following
remarks:

- We consider the ﬁrst day to be the observed stream
preﬁx S0 and use (part of) the queries u ∈ U (cid:48)
0 ⊆
U0 therein (along with their number of occurrences
during the ﬁrst day) to learn the optimal hashing
scheme via Algorithm 1 and for λ = 1.

- The ﬁrst day consists of over 200,000 unique queries
and just storing their IDs would require 200,000
buckets. Thus, we randomly sample a subset of the
observed queries, with probabilities proportional to
their observed frequencies. We use the sampled sub-
set of queries as input to Algorithm 1.
For ﬁxed number of total buckets btotal, we need to
determine the ratio c between the number of buckets
b that the learned hashing scheme will consist of and
the number of queries n whose IDs we will store.
Therefore, for user-speciﬁed btotal and c, we pick b
and n according to

-

n = btotal/1+c,

b = btotal − n.

-

In our experiments, we examine c ∈ {0.03, 0.3}.
For the classiﬁer g : X → [b], mapping unseen
queries u ∈ U \ U (cid:48)
0 to buckets (as per Section 5.2),
we found that rf achieves the best trade-off between
training time and classiﬁcation accuracy and use this
model in the results we report.

- To create input features for the classiﬁer g, we follow
a simple bag-of-words approach and only keep the
500 most common words in the training queries.
We also include as features the number of ASCII
characters in the query text, the number of punctu-
ation marks, the number of dots, and the number
of whitespaces. As a result, the proposed approach
is simple and interpretable, yet strong (as we show
next).

7.4 Results

We implement our experiments in Python 3 and use the
Scikit-learn machine learning package [45]. We indepen-
dently repeat each experiment 5 times and report the av-
eraged error, as well as its standard deviation. We remark
that each bucket consumes 4 bytes of memory and hence
the total number of buckets used in each experiment can be
calculated as b = m·103
, where m is the size of the estimator

4

in KB. Moreover, we denote by Ut the set of queries that
appear in day t, and by f t
their aggregated
true frequencies and estimated frequencies, respectively,
between days 0 and t.

Ut and ˜f t
Ut

In Figure 7, we show the estimation error as function of
the estimator’s size in KB, after the 30th and the 70th day.
On the the left (Figures 7a and 7c), we plot the average (per
element) estimation error

1
|Ut|

(cid:88)

u∈Ut

|f t

u − ˜f t
u|.

On the the right (Figures 7b and 7d), we plot the expected
magnitude of the absolute estimation error

1

(cid:80)

u∈Ut

fu

(cid:88)

u∈Ut

u · |f t
f t

u − ˜f t
u|.

Notice that the former metric is expressed in a per element
scale, that is, we normalize the overall error by the total
number of elements |Ut| and hence all elements are penal-
ized uniformly, whereas the second metric, the expected
magnitude of the absolute estimation error, penalizes el-
ements proportionally to their actual frequencies, as per
Section 4.1.

We observe that the trend in the estimation error is
very similar after the 30th and the 70th day. What changes
is the absolute value of the estimation error, which, as
expected, deteriorates with time, uniformly for all methods.
The proposed method opt-hash consistently outperforms
its competitors, in terms of both metrics. Unsurprisingly, as
the size of all estimators increases, their errors drop. This is
the case with both the average and the expected estimation
error. We make the following additional remarks:

- The superiority of opt-hash is most notable in
terms of average (per element) error. This is partly
due to the fact that opt-hash does a substantially
better job at estimating the frequencies of rarely
occurring queries. In particular, queries that appear
very few times are placed in the same bucket and
hence the estimation error on them is small. In con-
trast, heavy-hitter and count-min often place
such queries in the same bucket with queries of
medium or even high frequencies, which produces
big estimation error.

- The expected magnitude of the estimation error
of heavy-hitter and count-min does seem to
slowly converge towards that of opt-hash when the

45678910num_groups24681012141618unseen_estimation_errorExperiment 5 - epoch 10cartlogregrf45678910num_groups0.10.20.30.40.5unseen_similarity_errorExperiment 5 - epoch 10cartlogregrf45678910num_groups246810121416unseen_overall_errorExperiment 5 - epoch 10cartlogregrf45678910num_groups010203040506070elapsed_timeExperiment 5 - epoch 10cartlogregrfIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

12

(a) Average per element absolute
error after the 30th day.

(b) Expected magnitude of abso-
lute error after the 30th day.

(c) Average per element absolute
error after the 70th day.

(d) Expected magnitude of abso-
lute error after the 70th day.

Fig. 7. Estimation error as function of the estimator’s size (in KB).

(a) Average per element absolute
error using 4 KB of memory.

(b) Expected magnitude of abso-
lute error using 4 KB of memory.

(c) Average per element absolute
error using 120 KB of memory.

(d) Expected magnitude of abso-
lute error using 120 KB of memory.

Fig. 8. Estimation error as function of time (in days).

estimators’ size becomes sufﬁciently large. This indi-
cates that opt-hash is particularly suited for low-
space regimes and can achieve much more effective
compression of the frequency vector.

- As far as heavy-hitter and count-min are con-
cerned, the former does produce better estimates,
which is in agreement with the results in [8]. The
improvement is much more notable in terms of
the expected magnitude of the estimation error.
This observation is to be expected as well, given
that heavy-hitter makes zero error on the most
frequently occurring elements, which are heavily
weighed in this metric.

Figure 8 reports the estimation error as function of time
(in days), for two different memory conﬁgurations (4 KB
in Figures 8a and 8b, 120 KB in Figures 8c and 8d). The
superiority of opt-hash is preserved over time, in terms
of both metrics. Moreover, we observe opt-hash achieves
the smallest standard deviation in its estimation error. This
can be attributed to the fact that the mappings of elements
to buckets are more stable than those of heavy-hitter
and count-min, as they are obtained via optimization
instead of randomization; the main source of randomness
for opt-hash is the classiﬁer.

We next experiment with memory conﬁgurations that
vary between 1.2 KB and 120 KB, and compare opt-hash
with count-min and heavy-hitter. The proposed ap-
proach provides an average improvement (over the entire
90-day period) by one to two orders of magnitude, in terms
of its average (per element) absolute estimation error, and
by 45-90%, in terms of its expected magnitude of estimation
error. For example, with 120 KB of memory, opt-hash
makes an average absolute estimation error of ∼ 29 in
estimating the frequency of each query, whereas the error of
heavy-hitter is ∼ 479 (Figure 7a). With 4 KB of memory,

the errors of opt-hash and heavy-hitter are ∼ 167
and ∼ 14, 661, respectively (Figure 7c). Table 1 shows the
average (per element) error after the entire 90-day period
as a percentage of each query’s frequency for the 1st, the
10th, the 100th, the 1, 000th, and the 10, 000th most common
queries.

Query rank (by frequency) Query frequency Average error percentage (%)
1
10
100
1,000
10,000

251,463
37,436
5,237
926
146

0.01
0.08
0.55
3.13
19.86

TABLE 1
Average (per element) error as percentage of query’s frequency.

An additional feature of opt-hash is that, by using
interpretable features in its machine learning component, it
provides insights into the underlying frequency estimation
problem. In particular, the features that were consistently
marked as most important are the four counts (i.e., number
of ASCII characters in the query text, the number of punctu-
ation marks, the number of dots, and the number of whites-
paces), as well as the words “com,” “www,” “google,” and
“yahoo.” Intuitively, this observation makes sense. For in-
stance, a large number of ASCII characters and whitespaces
would be indicative of a big query with multiple words,
making it more likely to be rare. On the other hand, a query
containing the word “google” would be more likely to be
common, given that “google” is consistently part of the most
frequently occurring queries.

8 CONCLUSION

In this paper, we developed a novel approach for the prob-
lem of frequency estimation in data streams that relies on the
use of optimization and machine learning on an observed

020406080100120Size (in KB)102103104Average Absolute ErrorDay 30.count-minheavy-hitteropt-hash020406080100120Size (in KB)103104Expected Absolute ErrorDay 30.count-minheavy-hitteropt-hash020406080100120Size (in KB)102103104105Average Absolute ErrorDay 70.count-minheavy-hitteropt-hash020406080100120Size (in KB)103104Expected Absolute ErrorDay 70.count-minheavy-hitteropt-hash020406080Time (in days)0500010000150002000025000Average Absolute ErrorSize = 4.0 KB.count-minheavy-hitteropt-hash020406080Time (in days)0500010000150002000025000Expected Absolute ErrorSize = 4.0 KB.count-minheavy-hitteropt-hash020406080Time (in days)0200400600800Average Absolute ErrorSize = 120.0 KB.count-minheavy-hitteropt-hash020406080Time (in days)0200400600800Expected Absolute ErrorSize = 120.0 KB.count-minheavy-hitteropt-hashIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

13

stream preﬁx. First, we formulated and efﬁciently solved the
problem of optimally (or near-optimally) hashing the ele-
ments seen in the preﬁx to buckets, hence providing a smart
alternative to oblivious random hashing schemes. To this
end, we reformulated the problem as a mixed-integer lin-
ear optimization problem, we developed an efﬁcient block
coordinate descent algorithm, and, in a special case, we
used dynamic programming to solve the problem in linear
time. Next, we trained a classiﬁer mapping unseen elements
to buckets. As we discussed, during stream processing,
we only keep track of the frequencies of those elements
that appeared in the preﬁx; the estimate the frequency of
any element (either seen or unseen) is the average of the
frequencies of all elements that map to the same bucket.
We also described an adaptive approach that enables us to
update the compressed frequency vector and keep track of
the frequencies of all elements. We used synthetic data to
investigate the performance, the scalability, and the impact
of various design choices for the proposed approach; our
study suggested that the proposed algorithms can compute
optimal hashing schemes for problems with thousands of
elements, using the mixed-integer linear optimization re-
formulation or the dynamic programming approach, and
high quality hashing schemes for problems with tens of
thousands of elements using the block coordinate descent
algorithm. Finally, we applied the proposed approach to
the problem of search query frequency estimation and eval-
uated it using real-world data and empirically showed that
the proposed learning-based streaming frequency estima-
tion algorithm achieves superior performance compared to
existing streaming frequency estimation algorithms.

APPENDIX A
PROOF OF THEOREM 1
Proof. We introduce variables E ∈ Rn×b
such that eij
≥0
corresponds to the absolute estimation error associated with
mapping element i to bucket j. Since we are minimizing a
nonnegatively weighed sum of such nonnegative terms, it
sufﬁces to require that

eij ≥ f 0

i −

(cid:80)

k∈[n] zkjf 0
k
(cid:80)
k∈[n] zkj

,

eij ≥ −f 0

i +

(cid:80)

k∈[n] zkjf 0
k
(cid:80)
k∈[n] zkj

,

(cid:80)

(4)
for all i ∈ [n], j ∈ [b]. To get rid of the fractional term in (4),
we multiply both equations with (cid:80)
k∈[n] zkj; this results in
k∈[n] zkj. To linearize those,
bilinear terms of the form eij
we introduce variables Θ ∈ Rn×n×b
such that θikj = eijzkj
can be interpreted as the error associated with mapping ele-
ment i to bucket j when k is also mapped therein. Since θikj
is the product of a binary variable and a continuous variable,
we can linearize the constraint θikj = eijzkj by introducing
a big-M constant such that, for all i ∈ [n], j ∈ [b], eij ≤ M .
We then require that

≥0

θikj ≥ eij − M (1 − zkj),

θikj ≤ eij,

θikj ≤ M zkj,

(5)

[6]

for all i ∈ [n], k ∈ [n], j ∈ [b]. Thus, (4) can be rewritten as
(cid:88)

(cid:88)

(cid:88)

θikj ≥ f 0
i

zkj −

f 0
k zkj,

k∈[n]
(cid:88)

k∈[n]

k∈[n]

k∈[n]

θikj ≥ −f 0
i

(cid:88)

zkj +

(cid:88)

f 0
k zkj,

k∈[n]

k∈[n]

(6)

which is linear in all variables. To linearize the other bilinear
term that appears in the objective function, we introduce
another set of auxiliary variables ∆ ∈ [0, 1]n×n×b such
that δikj = zijzkj indicates whether elements i and k are
mapped together to bucket j. We then have the constraints

δikj ≥ zij + zkj − 1,

δikj ≤ zij,

δikj ≤ zkj,

(7)

for all i ∈ [n], k ∈ [n], j ∈ [b]. Using the above new
variables, the objective function can be written as

(cid:88)

(cid:88)

i∈[n]

j∈[b]


λθiij + (1 − λ)

(cid:88)

k∈[n]

δikj(cid:107)xi − xk(cid:107)2


 .

(8)

Finally, we have to properly select the constant M in (5)
so that it is a valid upper bound for the variables E; such
a bound can be obtained by setting M ≥ maxi∈[n] f 0
i , i.e.,
the estimation error associated with any element cannot be
greater than the largest frequency observed in the preﬁx.

APPENDIX B
FLOWCHARTS FOR THE PROPOSED APPROACH

Figure 9 provides the ﬂowcharts for the proposed approach.
In particular, Figure 9a corresponds to the learning phase,
where the stream preﬁx is used to learn the optimal hashing
scheme and the classiﬁer; Figure 9b illustrates how the
proposed approach answers count queries for any input
element; Figures 9c and 9d show the update mechanism of
the proposed approach without and with the use of Bloom
ﬁlters, respectively.

APPENDIX C
TABLE OF NOTATIONS

Table 2 includes the basic notations that are used repeatedly
throughout the paper; we explain notations that are not used
repeatedly in the main text. We note that we generally use
the index u to refer to elements, the indices i and k to refer to
element IDs, the index j to refer to buckets. For simplicity in
notation, elements are referred to using either their symbol u
or their ID i, depending on the context; similarly, frequencies
are indexed using either of the two approaches.

REFERENCES

[1]

S. Muthukrishnan, Data streams: Algorithms and applications. Now
Publishers Inc, 2005.

[2] M. Garofalakis, J. Gehrke, and R. Rastogi, Data stream management:

[3]

[4]

live

processing high-speed data streams. Springer, 2016.
“Internet
stats,”
google-search-statistics, accessed: 2020-07-01.
“Netﬂow services and applications,” Cisco systems white paper
(1999) http://www.cisco.com.

https://www.internetlivestats.com/

[5] L. Bhuvanagiri and S. Ganguly, “Estimating entropy over data
Springer, 2006,

streams,” in European Symposium on Algorithms.
pp. 148–159.
J. Misra and D. Gries, “Finding repeated elements,” Science of
computer programming, vol. 2, no. 2, pp. 143–152, 1982.

[7] G. Cormode and M. Hadjieleftheriou, “Finding the frequent items
in streams of data,” Communications of the ACM, vol. 52, no. 10, pp.
97–105, 2009.

[8] C.-Y. Hsu, P. Indyk, D. Katabi, and A. Vakilian, “Learning-based
frequency estimation algorithms,” in International Conference
on Learning Representations, 2019.
[Online]. Available: https:
//openreview.net/forum?id=r1lohoCqY7

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

14

(a) Learning the optimal hashing scheme.

(b) Answering count queries for element u ∈ U .

(c) Updating sketch at time t upon arrival of element ut ∈ U .

(d) Updating sketch with Bloom ﬁlter extension at time t upon arrival of
element ut ∈ U .

Fig. 9. Flowcharts for the proposed approach.

[9] P. Domingos and G. Hulten, “Mining high-speed data streams,”
in Proceedings of the sixth ACM SIGKDD international conference on
Knowledge discovery and data mining, 2000, pp. 71–80.

[10] G. Cormode, M. Garofalakis, P. J. Haas, and C. Jermaine, “Syn-
opses for massive data: Samples, histograms, wavelets, sketches,”
Foundations and Trends in Databases, vol. 4, no. 1–3, pp. 1–294, 2012.
[11] G. Cormode and S. Muthukrishnan, “An improved data stream
summary: the count-min sketch and its applications,” Journal of
Algorithms, vol. 55, no. 1, pp. 58–75, 2005.

[12] M. Charikar, K. Chen, and M. Farach-Colton, “Finding frequent
items in data streams,” in International Colloquium on Automata,
Languages, and Programming. Springer, 2002, pp. 693–703.

[13] N. Alon, Y. Matias, and M. Szegedy, “The space complexity of
approximating the frequency moments,” Journal of Computer and
system sciences, vol. 58, no. 1, pp. 137–147, 1999.

[14] M. Yu, L. Jose, and R. Miao, “Software deﬁned trafﬁc measurement
with opensketch,” in Presented as part of the 10th {USENIX} Sympo-
sium on Networked Systems Design and Implementation ({NSDI} 13),
2013, pp. 29–42.

[15] A. Goyal, H. Daumé III, and G. Cormode, “Sketch algorithms for
estimating point queries in nlp,” in Proceedings of the 2012 joint
conference on empirical methods in natural language processing and
computational natural language learning, 2012, pp. 1093–1103.
[16] A. Gilbert and P. Indyk, “Sparse recovery using sparse matrices,”

Proceedings of the IEEE, vol. 98, no. 6, pp. 937–947, 2010.

[17] A. Aghazadeh, R. Spring, D. Lejeune, G. Dasarathy, A. Shrivastava
et al., “Mission: Ultra large-scale feature selection using count-
sketches,” in International Conference on Machine Learning, 2018, pp.
80–88.

[18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case
for learned index structures,” in Proceedings of the 2018 International
Conference on Management of Data, 2018, pp. 489–504.

[19] M. Mitzenmacher, “A model for learned bloom ﬁlters and optimiz-
ing by sandwiching,” in Advances in Neural Information Processing
Systems, 2018, pp. 464–473.

[20] B. H. Bloom, “Space/time trade-offs in hash coding with allowable
errors,” Communications of the ACM, vol. 13, no. 7, pp. 422–426,
1970.

[21] A. Broder and M. Mitzenmacher, “Network applications of bloom
ﬁlters: A survey,” Internet mathematics, vol. 1, no. 4, pp. 485–509,
2004.

[22] T. Yang, L. Wang, Y. Shen, M. Shahzad, Q. Huang, X. Jiang,
K. Tan, and X. Li, “Empowering sketches with machine learning
for network measurements,” in Proceedings of the 2018 Workshop
on Network Meets AI & ML, ser. NetAI’18. New York, NY, USA:
Association for Computing Machinery, 2018, p. 15–20. [Online].
Available: https://doi.org/10.1145/3229543.3229545

[23] J. Wang, T. Zhang, N. Sebe, H. T. Shen et al., “A survey on
learning to hash,” IEEE transactions on pattern analysis and machine
intelligence, vol. 40, no. 4, pp. 769–790, 2017.

Training data:𝒙𝑢,ℎ!𝑢,∀𝑢∈𝑆"Learn hash function:ℎ!:𝑢:𝑢∈𝑆"→[𝑏]Learn classifier:ℎ#:𝑋→[𝑏]Input stream prefix:𝑆!=(𝑢",…,𝑢|$!|)Element u’s features: 𝒙(𝑢)StartEndInputquery:𝑢∈𝑈𝑢∈𝑆!Find u’s bucket using learned hash function:𝑗=ℎ"(𝑢)Output estimate:𝑓#𝑐#Find u’s bucket using learned classifier:j =ℎ$(𝒙(𝑢))YesNoStartEndInput arrival:𝑢!∈𝑈𝑢!∈𝑆"Find 𝑢’s bucket using learned hash function:j ∶=ℎ#(𝑢)YesNo𝑓$=𝑓$+1StartEndInputarrival:𝑢!∈𝑈𝑢!∈𝑆"Find 𝑢’s bucket using learned hash function:j =ℎ#(𝑢)Find 𝑢’s bucket using learned classifier:𝑗=ℎ$(𝒙(𝑢))YesNo𝑓%=𝑓%+1Check𝐵𝐹𝑢!.Has 𝑢!appeared before?𝑐%=𝑐%+1NoYesUpdate Bloom filter so that 𝑢_𝑡is marked as seen:𝐵𝐹(𝑢!)=1StartEndIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING

15

Explanation

Symbol
General symbols:
U
U0

n
u ∈ U
k ∈ [|U|]
X
x ∈ X
S = (u1, . . . , u|S|) Data stream
S0
fu
f 0
u
˜fu
b
Symbols related to CMS and LCMS:
w and d
φj (or φl
j )

hHH (·)

Symbols related to the proposed approach:
Ij
cj
µj
zi

Universe of elements
Set of elements that appeared in the stream
preﬁx
|U0|
Element
Element’s unique ID
Feature space
Element’s features

Data stream preﬁx
Frequency of element u in S
Frequency of element u in S0
Estimate of frequency of element u in S
Sketch’s total buckets

Sketch width and depth
Aggregate frequency in bucket j (or bucket j
in level l); this is used in CMS and LCMS
Classiﬁer that decides whether element u is a
heavy hitter

Set of elements in bucket j
Number of elements in bucket j
Mean of frequencies of elements in bucket j
One-hot binary hash code for element with
ID i
Integer hash code for element with ID i
Hyperparameter that controls the trade-off
between estimation error and similarity error
Function that maps elements that appeared
in the preﬁx to buckets based on the learned
hash code
Classiﬁer that maps elements to buckets

hi
λ

hS (·)

hU (·)

TABLE 2
Notations.

[24] B. Kulis and T. Darrell, “Learning to hash with binary recon-
structive embeddings,” in Advances in neural information processing
systems, 2009, pp. 1042–1050.

[25] G. Lin, C. Shen, D. Suter, and A. Van Den Hengel, “A general
two-step approach to learning-based hashing,” in Proceedings of
the IEEE international conference on computer vision, 2013, pp. 2552–
2559.

[26] G. Lin, C. Shen, Q. Shi, A. Van den Hengel, and D. Suter, “Fast su-
pervised hashing with decision trees for high-dimensional data,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2014, pp. 1963–1970.

[27] A. Andoni and I. Razenshteyn, “Optimal data-dependent hashing
for approximate near neighbors,” in Proceedings of the forty-seventh
annual ACM symposium on Theory of computing, 2015, pp. 793–801.
[28] M. Purohit, Z. Svitkina, and R. Kumar, “Improving online al-
gorithms via ml predictions,” in Advances in Neural Information
Processing Systems, 2018, pp. 9661–9670.

[29] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan, Z. Meng, and
M. Alizadeh, “Learning scheduling algorithms for data processing
clusters,” in Proceedings of the ACM Special Interest Group on Data
Communication, 2019, pp. 270–288.

[30] A. Mousavi, A. B. Patel, and R. G. Baraniuk, “A deep learning
approach to structured signal recovery,” in 2015 53rd annual aller-
ton conference on communication, control, and computing (Allerton).
IEEE, 2015, pp. 1336–1343.

[31] E. B. Khalil, P. Le Bodic, L. Song, G. Nemhauser, and B. Dilkina,
“Learning to branch in mixed integer programming,” in Thirtieth
AAAI Conference on Artiﬁcial Intelligence, 2016.

[32] M.-F. Balcan, T. Dick, and T. Sandholm, “Learning to branch,” in

International Conference on Machine Learning, 2018.

[33] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song, “Learning
combinatorial optimization algorithms over graphs,” in Advances
in Neural Information Processing Systems, 2017, pp. 6348–6358.
[34] D. Bertsimas and B. Stellato, “The voice of optimization,” Machine

Learning, vol. 110, no. 2, pp. 249–277, 2021.

[35] ——, “Online mixed-integer optimization in milliseconds,” arXiv

preprint arXiv:1907.02206, 2019.

[36] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica,
“Learning to optimize join queries with deep reinforcement learn-
ing,” arXiv preprint arXiv:1808.03196, 2018.

[37] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi, “Learning state
representations for query optimization with deep reinforcement
learning,” in Proceedings of the Second Workshop on Data Management
for End-To-End Machine Learning, 2018, pp. 1–4.

[38] A. Aamand, P. Indyk, and A. Vakilian, “Learned frequency es-
timation algorithms under zipﬁan distribution,” arXiv preprint
arXiv:1908.05198, 2019.

[39] H. Wang and M. Song, “Ckmeans. 1d. dp: optimal k-means
clustering in one dimension by dynamic programming,” The R
journal, vol. 3, no. 2, p. 29, 2011.

[40] X. Wu, “Optimal quantization by matrix searching,” Journal of

algorithms, vol. 12, no. 4, pp. 663–673, 1991.

[41] A. Grønlund, K. G. Larsen, A. Mathiasen, J. S. Nielsen, S. Schnei-
der, and M. Song, “Fast exact k-means, k-medians and bregman
divergence clustering in 1d,” arXiv preprint arXiv:1701.07204, 2017.
[42] Gurobi Optimization Inc., “Gurobi optimizer reference manual;

2016,” http://www. gurobi. com, 2016.

[43] L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classiﬁcation
and regression trees,” Wadsworth and Brooks, vol. 37, no. 15, pp.
237–251, 1984.

[44] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.

5–32, 2001.

[45] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel et al., “Scikit-
learn: Machine learning in Python,” Journal of Machine Learning
Research, vol. 12, pp. 2825–2830, 2011.

Dimitris Bertsimas is the Associate Dean of
Business Analytics, Boeing Professor of Opera-
tions Research and faculty director of the Master
of Business Analytics at MIT. He received his
SM and PhD in Applied Mathematics and Oper-
ations Research from MIT in 1987 and 1988 re-
spectively. He has been MIT faculty since 1988.
His research interests include optimization, ma-
chine learning, and applied probability, and their
applications in health care, ﬁnance, operations
management, and transportation. He has co-
authored more than 200 scientiﬁc papers and ﬁve graduate level text-
books and has received numerous awards, with the most recent being
the John von Neumann Theory Prize, INFORMS, and the President’s
award, INFORMS, both in 2019.

Vassilis Digalakis Jr. is a PhD candidate at
MIT’s Operations Research Center, advised by
Prof. Dimitris Bertsimas. Prior to joining MIT,
he earned his Diploma in Electrical and Com-
puter Engineering from the Technical University
of Crete, Greece, in 2018. His research inter-
ests lie at the intersection of machine learning
and optimization, with application to big-data set-
tings.

