Neurocomputing (2021) 1–11

Procedia
Computer
Science

1
2
0
2

p
e
S
8
2

]
E
N
.
s
c
[

5
v
3
7
1
7
0
.
3
0
1
2
:
v
i
X
r
a

Neural Architecture Search based on the Cartesian Genetic
Programming

Xuan Wua,b, Xiuyi Zhanga,b, Linhan Jiaa,b, Liang Chenc, Yanchun Liangd, You Zhoua,b,c,∗,
Chunguo Wua,b,∗

aKey Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education,
bCollege of Computer Science and Technology, Jilin University, Changchun, 130012, China
cCollege of Software, Jilin University, Changchun, 130012, China
dSchool of Computer Science, Zhuhai College of Science and Technology, Zhuhai, 519041, China

Abstract

Neural architecture search (NAS) is a hot topic in the ﬁeld of automated machine learning and outperforms humans in designing
neural architectures on quite a few machine learning tasks. Motivated by the natural representation form of neural networks by
Cartesian Genetic Programming (CGP), we propose an evolutionary approach of NAS based on CGP, called CGPNAS, to solve
sentence classiﬁcation task. To evolve the architectures under the framework of CGP, the operations such as convolution are
identiﬁed as the types of function nodes of CGP, and the evolutionary operations are designed based on Evolutionary Strategy. The
experimental results show that the searched architectures are comparable with the performance of human-designed architectures.
We verify the ability of domain transfer of our evolved architectures and the transfer experimental results show that the accuracy
deterioration is lower than 2-5%. Finally, the ablation study identiﬁes the Attention function as the single key function node and the
linear transformations along could keep the accuracy similar with the full evolved architectures, which is worthy of investigation
in the future.

©

Keywords: Neural architecture search, Cartesian genetic programming, Attention mechanism, Sentence classiﬁcation

1. INTRODUCTION

As a core technique in modern data-driven artiﬁcial intelligence, Deep Neural Networks (DNNs) have surpassed
the achievement of former methods in many typical problems and have made excellent solutions to questions in
interdisciplinary research. However, the architecture designing of DNNs is limited by the existing knowledge of
designers, which makes it hard to ﬁnd the global best architecture for a given task. Hence, much attention has been
paid to the Neural Architecture Search (NAS) to relieve the burden of researchers from architecture design for DNNs
and to best explore the architecture searching space [1, 2]. There are many methods proposed to search architecture,
among which Reinforcement Learning (RL) and Evolutionary Algorithm (EA) are the most popular.

Zoph et al. (2017) [3] ﬁrstly use the policy gradient algorithm, a RL approach, as the Recurrent Neural network
(RNN) controller to produce new architectures of Convolution Neural Network (CNN). Subsequently, Zoph et al.

∗{wucg, zyou}@jlu.edu.cn

1

 
 
 
 
 
 
/ Neurocomputing (2021) 1–11

2

(2018) [4] use the RL with proximal policy optimization as the RNN controller. Baker et al.
(2017) [5] use Q-
learning with the (cid:15)-greedy exploration strategy to sequentially search for neural architectures. To relieve expensive
calculations on GPUs, several speed-up methods and eﬃcient solutions are proposed based on the RNN controller.
Pham et al. (2018) [6] propose Eﬃcient Neural Architecture Search (ENAS), in which the controller searches for the
best subgraph within a larger graph in the ﬁrst stage and shares parameters between subgraphs in the second stage.
Compared with the original work in [3], ENAS accelerates the eﬃciency of NAS up to a thousand times.

As another popular method, NAS based on EA has a history of more than 30 years. Gruau (1993) [7] proposes
Cellular Encoding (CE), which is a grammatical inference process to search neural networks with Genetic Program-
ming (GP). Yao and Liu (1997) [8] propose Evolutionary Programming Network (EPNet), which evolves the network
architecture and connection weights with Evolutionary Programming. To evolve neurons of a network, Stanley and
Miikkulainen (2002) [9] propose NeuroEvolution of Augmenting Topologies (NEAT), which encodes the neurons into
Node genes and Connection genes and uses Genetic Algorithm (GA) to update Node genes and Connection genes.

With the emergence of Automated Machine Learning, many NAS methods based on EA are proposed in recent
years. Xie et al. (2017) [10] propose GeNet based on GA to choose CNNs, where CNN is divided into diﬀerent stages
with pooling operation as the boundary, and all convolution operations in the same stage have the same convolution
kernel and channel number. Suganuma et al. (2019) [11] use Cartesian Genetic Programming (CGP), a graph form of
GP, to encode the CNN architectures (CGP-CNN). CGP-CNN adopts highly functional Block as the node functions,
for example, ConvBlock including convolution, batch normalization, and ReLU. Bi et al. (2019) [12] propose Feature
Learning GP (FLGP) to evolve convolution operators for feature learning on image classiﬁcations. Sun et al. (2019)
[13] use PSO to search Flexible Convolutional Auto Encoders (FCAE) with chain structure. To search image classiﬁer,
Real et al. (2019) [14] modify the tournament selection evolution by introducing an age property to favor the younger
genotypes (named Aging Evolution or regularized evolution), which keeps as many young individuals as possible.

Most of the NAS methods are proposed to solve Computer Vision (CV) problems [15] and focus on evolving
CNN architectures. Nowadays, researchers make eﬀorts to enable NAS to solve problems in the ﬁeld of Natural
Language Processing (NLP). Since Transformer [16] has become the state-of-the-art model in NLP, David et al.
(2019) [17] use Transformer as initial, design a new searching space for NLP problems, and search for the best
candidate Transformers, named Evolved Transformer. Ramakanth et al. (2020) [18] propose Flexible and Expressive
Neural Architecture Search (FENAS), dividing the search process into two stages similar to ENAS[6]. The results
show FENAS can reproduce Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) structures.

Sentence classiﬁcation task is a classical and fundamental task in the ﬁeld of NLP. Motivated by the eﬀectiveness
of Transformer for NLP problems and the natural representation of DNN with CGP, this paper proposes a CGP
encoding-based NAS (CGPNAS) method to deal with sentence classiﬁcation task.

The remaining parts are organized as follows: Section 2 introduces the related work brieﬂy; Section 3 proposes the
CGPNAS; Section 4 presents the experimental results to evaluate the performance of CGPNAS; and ﬁnally, Section
5 presents the conclusion.

2. RELATED WORK

In this Section, we ﬁrst introduce how EA applied to NAS in Part 2.1. Next, we brieﬂy review the research on
sentence classiﬁcation task in Part 2.2. Finally, we introduce CGP method in Part 2.3, which is the encoding method
of this paper .

2.1. Neural architecture search based on Evolutionary Algorithms

NASs based on EA mainly focus on the following two aspects: encoding method and genetic operator. The
encoding method is to convert the phenotype into the genotype of a given DNN. The genetic operator is to produce
new genotypes in each iteration. Except encoding method and genetic operator, there are also a small number of
studies on survival selection strategy and parental selection strategy [14, 19]. In EA, there are two classic kinds of
genetic operators: Crossovers and Mutations. Crossovers combine the genotype of two or more parents to get one
or more oﬀspring genotypes. Mutations change the genotype of a parent to get a new genotype. To produce new
genotypes, diﬀerent NASs use one or both two kinds of genetic operators. CGP-CNN [11] use mutation as genetic
operator only. NEAT [9], GeNet [10], FLGP [12], AmoebaNet-A [14] and DCNN designer [20] use both crossover
and mutation as genetic operators.

2

/ Neurocomputing (2021) 1–11

3

There are two types of encoding methods: direct and indirect. As a widely used method, the direct encoding
methods explicitly specify neural architecture information with genotypes. In NEAT [9], the genotype is composed
of Node genes and Connection genes. Node genes store the node type, indicating input (or sensor) node, output node
or hidden node. Connection genes store the numbers of in-nodes and out-nodes, and the weights, states (Enabled or
Disenabled) and innovation numbers of the connections. Because FCAE is used to evolve a chain architecture without
explicit topological connection information, Sun et al. [13] only encode node type and its parameter information into
genotype. The indirect encoding methods specify only a generating rule of genotypes. CE [7] is a classical indirect
encoding method. The entire neural network evolves from a single ancestor cell where the evolutionary DNA is stored
in a tree structure. The tree structure deﬁnes the method of cell division, generating the ﬁnal network topology with
cell development.

2.2. Sentence classiﬁcation task

Sentence classiﬁcation is a classical and fundamental task in NLP. Traditional classiﬁcation methods often use
human-designed features, which could learn only the shallow representation of sentences. With the development of
deep learning, CNN, RNN and Attention [21] are widely used in sentence classiﬁcation tasks. Hochreiter and Schmid-
huber (1997) [22] propose Long Short-term Memory (LSTM) as a special RNN for long-term dependencies learning,
which relieves the gradient disappearance eﬀectively in the process of back propagation by gating mechanism. After
that, there is great success in dealing with NLP problems with LSTM. Kim (2014) [23] apply a simple CNN to sen-
tence classiﬁcation task, and achieve excellent results on multiple benchmarks. Compared with traditional machine
learning methods, Kim’s method is good at capturing location features of sentences. Vaswani et al. [16] (2017) pro-
pose the Transformer architecture based on Attention mechanisms, which could be widely used in NLP tasks. BERT
[24] is a pre-trained architecture, characterized by Masked Language Model and Next Sentence Prediction, which
could create the state-of-the-art performance for a wide range of tasks by ﬁnetuning just the output layer.

Since diﬀerent methods have their own advantages, many scholars combine multiple methods to achieve competi-
tive results than a single method. Lai et al. [25] (2015) propose Recurrent Convolutional Neural Networks, combining
the bidirectional RNN and max-pooling layer in CNN. Liu and Guo [26] (2019) propose AC-BiLSTM, combining
the Attention mechanism, convolutional layer and bidirectional LSTM. Zhang et al. [27] (2019) propose 3W-CNN,
combining deep learning methods and traditional feature-based methods. Zhang et al. design a conﬁdence function to
divide the outputs of CNN into 2 parts with strong and weak conﬁdence, respectively. The CNN classiﬁcation outputs
with weak conﬁdence will be reclassiﬁed by NB-SVM proposed in [28].

2.3. Cartesian genetic programming

As a graph form of GP, CGP is initially proposed to optimize digital circuits [29], and hence, each intermediate
node has two inputs. Subsequently, CGP is applied to many problems, such as image processing and molecular
docking [30, 31, 32, 33].

As shown in Fig. 1, CGP is represented by a directed graph with n input nodes, m output nodes. Except input
nodes and output nodes, CGP has r ∗ c intermediate nodes, also known as function nodes, where r and c denote rows
size and Columns size, respectively. CGP could set the number of inputs for each function node, for example, Ref.
[29] sets 2 as the number of inputs and the number of outputs is usually set 1. In addition, CGP forbids the links in
the same grid columns, and usually sets a max stride of connection between columns, called “levels-back”, which can
increase or reduce the size of searching space. Borrowing the words in genetics, some function nodes, e.g., Node ar,c,
is not be used as input for subsequent nodes, called inactive nodes [11].

3

/ Neurocomputing (2021) 1–11

4

Figure 1. Illustration of Cartesian Genetic Programming.

3. OUR METHOD

Motivated by the natural representation form of neural networks by CGP, we propose a novel NAS method based
on CGP, named CGPNAS, to deal with sentence classiﬁcation task. In Part 3.1, we introduce CGP coding method
applied to NAS. In Part 3.2, we present the function nodes used in this paper. In Part 3.3, we design an evolution
method for CGPNAS.

3.1. CGP coding method

For NAS problem, CGP uses a two-dimensional grid as the phenotype of neural networks, as shown in Fig. 1,
which is a natural presentation of neural networks due to the topological similarity between CGP and neural networks.
The links represent the data ﬂow and the function nodes represent basic operations of the neural networks, such as
Convolution, Attention and so on.

The encoding structure of CGP is a triplet shown at the bottom in Fig. 2-a, indicating the function name and the
two numbers of input nodes. An illustrating genotype, with 10 function nodes, is shown above the encoding structure
in Fig. 2-a. For the genotype, each gene corresponds to a node in Fig. 2-b, which is the intermediate phenotype
with both inactive and active links in dashed and solid arrows, respectively. In addition, Node 6 and Node 7 are both
inactivate nodes. Fig. 2-c is the ﬁnal phenotype with only active links in solid arrows and can be used as a DNN to
solve problems.

4

Input Node Function NodeOutput NodeColumns SizeRows Size𝑚𝑛𝑎𝑐,𝑐𝑎1,1𝑎2,1𝑎2,1𝑎3,1𝑎𝑟,1𝑎2,2𝑎3,2𝑎𝑟,2𝑎2,𝑐𝑎3,𝑐𝑎𝑟,𝑐/ Neurocomputing (2021) 1–11

5

Figure 2. Illustration of Cartesian Genetic Programming.

3.2. Function Node Design

The set of function nodes is important for the evolved neural architectures. Hence, for the task of sentence

classiﬁcation, we design the set of function nodes as follows, denoted as S:

S = {Conv, Atte, Linear, S um, ReLU, LNorm, GLU}

where the enumerated symbols mean the operations of Convolution, Attention, Linear, Sum, ReLU, Layer Normaliza-
tion [35] and Gated Linear Units (GLU) [36]. The function node types, the number of input nodes, parameter name,
candidate parameter values, input and outputs dimensions are shown sequentially in Table 1, from the ﬁfth to the sixth
column, where b, l, d, and d(cid:48) denote batch size, max sentence length, input dimension of word vectors and output
dimension of word vectors. It is worth emphasizing that when the node has two inputs, we use d1 and d2 to represent
the word vector dimensions of the two inputs respectively.

Table 1. THE TYPES OF FUNCTION NODES AND THEIR CANDIDATE PARAMETER VALUES.

Node Type
Conv.

# Input Nodes
1

Atte.
Linear.
Sum.

ReLU.
LNorm.
GLU.

1
1
2

1
1
1

Para. Name
Channel
Kernel
Head
Channel
-

Para. Value
{16, 32}
{1, 3, 5}
{4, 8, 16}
{32, 128}
-

-
-
-

-
-
-

Input Dim. Output Dim.
b × l × d

b × l × d(cid:48)

b × l × d
b × l × d
b × l × d1
b × l × d2
b × l × d
b × l × d
b × l × d

b × l × d
b × l × d(cid:48)
b × l × d(cid:48)

b × l × d
b × l × d
b × l × d(cid:48)

For the task of sentence classiﬁcation, one-dimensional convolutions and multi-head attention are used. The Linear
node represents a linear transformation. The function of Sum node is to merge two branches. When two branches
with diﬀerent dimensions of word vectors are going to be merged, the smaller word vector would be ﬁlled with 0
at its end to force it into the same size as the larger one. Although Sum node has two input nodes formally, it is
allowed to receive the same input two times from a single precursor Sum node, such as Node 1 and Node 5, shown in
Fig. 2-a. The Layer Normalization is proposed by Ba et, al. [35] for RNN, which is normalized in the channels and
features of samples. The Linear node represents a linear transformation. GLU node is a variant of Convolution with
gate-controlled outputs.

5

InputLinearLNormSumSumSumLNormFullInputLinearLNormSumSumSumLNormFullAtteConvbcInactive LinkActive Link2LNorm00Sum0Linear31SumInput6Conv5Atte5LNorm7Full44Sum1stinput node numberFunction ID2ndinput node numberNot expressed in the phenotypea3124867950/ Neurocomputing (2021) 1–11

6

3.3. Evolution strategy design

CGP usually uses the 1 + λ Evolutionary Strategy (ES) to update and select the population, meaning that one
parental individual and λ oﬀspring individuals compete to survive into the next generation. Through mutation opera-
tion and adaptive selection, the population evolves towards the optimal goal. According to [11], there are two kinds
of mutations in 1+λ ES, named forced mutation and neutral mutation, respectively. The forced mutation works on
all parental nodes to generate oﬀspring, and the neutral mutation works only on inactive parental nodes to contribute
potentially new nodes for the next generation. Both forced mutation and neutral mutation are point mutations, which
means that the function and connection of nodes randomly change to valid values according to the mutation rate.
To enhance exploration and overcome the local optimal traps, we double the initial mutation rate for the late 25%
generation.

The algorithm is described as follows. Firstly, the λ oﬀspring individuals are produced by the current parental
If all ﬁtness of the λ oﬀspring individuals are worse than their parental
individual through the forced mutation.
individual, the inactive nodes of the parental individual are mutated by neutral mutation, and the λ oﬀspring individuals
are eliminated. Otherwise, the oﬀspring individual with the highest ﬁtness is selected as the parental individual of the
next generation. The pseudocode is presented as follows:

Algorithm 1 Evolution Strategy
1: Create a parent randomly
2: Evaluate the ﬁtness of parent
3: while generation < Max generation do
4:
5:
6:

Double the mutation rate for late 25% generation
λ oﬀspring are produced by forced mutation.
Evaluate the ﬁtness of λ oﬀspring individuals
if the λ oﬀspring individuals are all worse than the parent then
Mutate the inactive nodes of parent with neutral mutation

7:
8:
else oﬀspring with the best ﬁtness become the new parent for the next iteration
9:
end if
10:
11: end while
12: End

The accuracy of sentence classiﬁcation task corresponding to each architecture is taken as the individual ﬁtness.
The neutral mutation acts on inactive nodes, it does not change the parental ﬁtness, so we do not need to evaluate the
altered parent by the neutral mutation.

4. EXPERIMENT

In this Section, we ﬁrst introduce datasets, hyperparameter and experimental setting details in Part 4.1 and 4.2.
Next, we compare the searched architecture obtained by CGPNAS and CGPNAS(GloVe) with the classical architec-
ture in Part 4.3. GloVe [37] is an embedding method, that allows neural networks not to learn the correlation between
words from scratch, so GloVe can improve the performance of the network. And then we verify the transfer ability
of the searched architecture on diﬀerent datasets in Part 4.4. Finally, we implement ablation testing to analyze the
impact of function nodes on the searched architecture in Part 4.5.

4.1. Datasets

The following datasets are used in our experiments, shown in Table 2. There are 3 datasets labeled with positive
and negative, including SST2 [38] (Binary labeled version of Stanford sentiment treebank), MR [39] (a large movie
review dataset extracted from Rotten Tomatoes web) and IMDB [40]. Samples of SST5 [38] (Stanford Sentiment
Treebank) are labeled with 5 levels, i.e., very positive, positive, neutral, negative and very negative. Samples of
AG news [41], extracted by ComeToMyHead website, are labeled with 4 kinds of tags, i.e., World, Sports, Business
and Sci/Tech.

6

/ Neurocomputing (2021) 1–11

7

Table 2. PROPERTIES OF THE EXPERIMENTAL DATASETS.

Dataset
Label levels
max sentence length
word vector dimension

SST2
5
50

SST5 MR IMDB Ag news
2
2
400
50

4
50

2
50

300

4.2. Hyperparameter and experiment details

The CGP parameters are shown in Table 3. Initially, we set the CGP grid by 5×20 and use a relatively large number
of columns size to generate deep architectures. To leverage searching space complexity and models’ generalization
ability, Levels-back is set to 3. The lower and upper bounds of numbers of active nodes are 10 and 60, respectively.
To enhance the exploration ability, the oﬀspring size is set to 4.

Table 3. EXPERIMENTAL PARAMETERS.

Parameters
Input nodes number
Input nodes number
Rows Size r
Columns Size c
Levels-back
Activate nodes number
Mutation rate
Oﬀspring Size λ
Max generation

Values
1
1
5
20
3
[10, 60]
{0.1, 0.2, 0.4}
4
1000

To enhance exploration, we set the initial mutation rate of the early 75% generations as 0.1, and double it into 0.2
for the late 25% generations. However, the mutation rate of SUM function nodes should be larger than that of other
functional nodes to decrease the probability of single-chain architectures. Hence, we set the mutation rate of SUM
function nodes as 0.2 and 0.4 by trials, respectively, in the early and late generations.

Taking the time consumption into account, we try to use the small values for the max sentence length and the word
vector dimension as shown in Table 2. However, due to the average sentence length of IMDB is 8 times larger than
the other dataset, the max sentence length is set as 400. For all experimental datasets, the word vector dimension is
set uniformly as 300.

In Parts 4.3,4.4 and 4.5 we train CGPNAS and CGPNAS(GloVe) with Adam Optimizer for 50 epochs and the
learning rate is 0.01. In Part 4.3, the classic architectures in comparison include TextCNN [23], Transformer [16],
BERT [24], Evolved Transformer [17], AC-BiLSTM [26], 3W-CNN [27] and FENAS [18].

In this paragraph, we introduce the training details of the comparison algorithm in Part 4.3. Similar to CGPNAS,
we also train TextCNN, Transformer, BERT and Evolved Transformer with Adam Optimizer for 50 epochs and the
learning rate is 0.01. In addition, We train a 6 layers Transformer encoder [17] and the number of attention heads is
set to 6. We follow the oﬃcial guide from [42] to ﬁnetune the BERT-Base-Uncased model [24] for downstream tasks.
We use the searched network from [17], training a 6 layer Evolved Transformer encoder with a linear layer to perform
classiﬁcation task at last.

4.3. Comparation with other algorithms

To present the performance of CGPNAS and CGPNAS(GloVe) on diﬀerent datasets and perform statistical tests,
we execute CGPNAS and CGPNAS(GloVe) 10 times on each dataset, respectively. As an example, one of the searched
architectures on IMBD dataset is shown in Table 4.

As shown in Table 5, with the help of GloVe, CGPNAS(GloVe) knows the correlation between words in the initial
stage and improves the accuracy by 2-5% on diﬀerent datasets, compared with CGPNAS. Hence, the performance
of CGPNAS is similar to TextCNN and the performance of CGPNAS(GloVe) is similar to Transformer and Evolved
Transformer. As the human-designed architectures, BERT and AC-BiLSTM get the best accuracy on 2 and 3 datasets,
7

respectively. It can be said that for the sentence classiﬁcation task, even if the existing NASs can reach the human-
designed level, they are still diﬃcult to outperform the best human-designed methods.

/ Neurocomputing (2021) 1–11

8

Table 4. DIMENSION CHANGE OF THE SEARCHED NEURAL NETWORK.
8 × 400 × 300

Input

Sum
Conv (Channel: 32 Kernel: 1)

8 × 400 × 300
8 × 400 × 32

Linear (Channel: 128)
LNorm

8 × 400 × 128
8 × 400 × 128

Sum
Sum
Atte (Head: 16)
LNorm
Atte (Head: 4)
Conv (Channel: 32 Kernel: 3)
Conv (Channel: 16 Kernel: 5)

8 × 400 × 128
8 × 400 × 128
8 × 400 × 128
8 × 400 × 128
8 × 400 × 128
8 × 400 × 32
8 × 400 × 16

Table 5. COMPARISON OF DIFFERENT ALGORITHMS. (“*” RESULTS FROM THE ORIGINAL PAPERS. THE CELLS HIGHLIGHTED
IN BOLD INDICATE THE BEST ACCURACY)

Dataset

Architecture
TextCNN (2014)
Transformer (2017)
BERT(2019)
Evolved Transformer (2019)
AC-BiLSTM* (2019)
3W-CNN* (2019)
FENAS* (2020)
CGPNAS
CGPNAS (GloVe)

4.4. Transfer ability study

SST2

SST5

MR

IMDB

Ag news

0.812
0.855
0.915
0.769
0.883
-
0.866
0.733 ± 0.027
0.788 ± 0.013

0.372
0.365
0.423
0.385
0.489
-
-
0.362 ± 0.006
0.413 ± 0.013

0.713
0.746
0.821
0.717
0.832
0.823
-
0.704 ± 0.015
0.744 ± 0.015

0.84
0.863
0.912
0.873
0.918
-
-
0.844 ± 0.012
0.864 ± 0.011

0.817
0.853
0.892
0.812
-
-
-
0.843 ± 0.017
0.864 ± 0.018

To verify the transfer ability of the searched architecture, we transfer all the architectures searched on one dataset

to the other datasets. The results are shown in Table 6.

We can see that the architectures searched on Ag news still perform better on the target dataset; the mean of
accuracy improves 1% on target datasets SST2 and MR, reduces by 1% on target dataset SST5 and reduces by 3%
on target dataset IMDB. But the architectures searched on SST2, SST5, MR and IMDB perform slightly poorly on
target datasets, the mean of accuracy is reduced by 2-5%. In particular, on the target dataset Ag news, the mean
of most accuracy is reduced by 7-8%, especially 15% of architectures searched on SST5. The results show that the
architecture searched by CGPNAS has transfer ability and can be applied to most target datasets, but the accuracy of
some target datasets has decreased signiﬁcantly.

Target

SST2

SST5

MR

IMDB

Ag news

Table 6. TRANSFER TESTING OF CGPNAS.

0.733 ± 0.027
0.673 ± 0.015
0.706 ± 0.018
0.689 ± 0.044
0.742 ± 0.026

0.324 ± 0.022
0.362 ± 0.006
0.324 ± 0.013
0.341 ± 0.011
0.351 ± 0.020

0.661 ± 0.020
0.654 ± 0.017
0.704 ± 0.015
0.674 ± 0.014
0.711 ± 0.018

0.814 ± 0.009
0.797 ± 0.018
0.813 ± 0.008
0.844 ± 0.012
0.819 ± 0.011

0.762 ± 0.057
0.689 ± 0.021
0.769 ± 0.046
0.776 ± 0.044
0.843 ± 0.017

Origin
SST2
SST5
MR
IMDB
Ag news

8

/ Neurocomputing (2021) 1–11

9

4.5. Ablation study

To investigate the key component that has a remarkable contribution to the performance, the ablation testing is
presented in this part. For this purpose, we reduce the diversity of functions in the set of function nodes and create
three new sets of function nodes. The ﬁrst one is denoted as S \{Conv}, which removes Convolution in the set of
function node. The second one is denoted as S \{Atte}, which removes Attention in set of function node. The third one
is denoted as S \{Conv, Atte}, which removes both Convolution and Attention in the set of function node. We execute
CGPNAS 10 times on each set of function nodes, respectively. Schematic architectures of ablation testing are shown
in Fig. 3.

It can be seen from Table 7. that even if the Convolution is removed, the accuracy improves 0.6% on IMDB and
drops only by 1-2% on the rest datasets. However, if the Attention is removed, the average accuracy drops by 1.1%
on Ag news but by 4-6% on the other datasets. The experimental results show that the Attention function node is
vital for the searched architecture. While it is also noted that even if all Convolution and Attention nodes are both
removed, the accuracy drops by 4-5%. However, the accuracies of the evolved architectures, excluding Convolution
and Attention nodes, are higher than those only Attention excluded on SST2, SST5 and MR. It can be known that
the architecture shown in Fig. 3-c performs mainly the linear transformation from its input, but it still achieves better
accuracy than S {Atte}. The detailed mechanism is worthy of investigation in the future.

Search Space
S \{Conv}
S \{Atte}
S \{Conv, Atte}
S

Table 7. ABLATION TESTING OF CGPNAS.

Dataset

SST2

SST5

MR

IMDB

Ag news

0.717 ± 0.018
0.678 ± 0.022
0.690 ± 0.019
0.733 ± 0.027

0.348 ± 0.003
0.319 ± 0.003
0.325 ± 0.003
0.362 ± 0.006

0.678 ± 0.029
0.647 ± 0.008
0.663 ± 0.027
0.704 ± 0.015

0.850 ± 0.005
0.798 ± 0.016
0.795 ± 0.009
0.844 ± 0.012

0.838 ± 0.006
0.832 ± 0.007
0.823 ± 0.006
0.843 ± 0.017

Figure 3. Schematic architectures of ablation testing.

9

InputLinearLNormSumConvSearch Space: SSumSumAtteLNormAtteConvConvFullInputLinearReluLNormLinearReluSumConvConvSumGLUConvConvFullSearch Space: S\{Atte}InputSearch Space: S\{Conv}SumLinearLNormLinearLinearAtteReluLNormAtteGLUSumSumSumAtteAtteAtteSumAtteSumAtteFullInputLinearSearch Space: S\{Conv,Atte}ReluLNormReluSumLinearReluLNormGLUGLUGLULinearFullabcd/ Neurocomputing (2021) 1–11

10

5. CONCLUSION

CGP is a natural representation of neural networks and can evolve the structure and parameters of neural architec-
tures at the same time. For this reason, we propose CGPNAS which can reach the state-of-the-art of human-designed
architectures for sentence classiﬁcation tasks. The transfer study proves that the evolved architectures have transfer
ability and can be applied to diﬀerent target domains. According to the ablation testing, the attention mechanism is
very important for CGPNAS, which also proves the reason why the attention mechanism is widely used in NLP.

NAS is still worthy of in-depth study on NLP and subsequent work can increase the diversity of function, such as
adding LSTM in the set of function node. To give the design speciﬁcation of the neural network, a large number of
experiments can be carried out to give which combinations are more likely to appear in the network. In addition, the
basic mathematical operations can be considered as function nodes to expand the representation ability of the evolved
architectures.

Acknowledgement

This work is supported by the National Natural Science Foundation of China (61876069, 61972174 and 61972175),
the Jilin Natural Science Foundation (20200201163JC), and the Science and Technology Planning Project of Guang-
dong Province (2020A0505100018), the Guangdong Key-Project for Applied Fundamental Research (2018KZDXM076).

References

[1] H. Gu, G. Fu, J. Li, and J. Zhu, “Auto-ReID+: Searching for a multi-branch ConvNet for person re-identiﬁcation,” Neurocomputing, vol.

435, pp. 53–66, May 2021, doi: 10.1016/j.neucom.2020.12.105.

[2] D. Tian, J. Deng, G. Vinod, T. V. Santhosh, and H. Tawﬁk, “A constraint-based genetic algorithm for optimizing neural network archi-
tectures for detection of loss of coolant accidents of nuclear power plants,” Neurocomputing, vol. 322, pp. 102–119, Dec. 2018, doi:
10.1016/j.neucom.2018.09.014.

[3] B. Zoph and Q. V. Le, “Neural Architecture Search with Reinforcement Learning,” in 5th International Conference on Learn-
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [Online]. Available:

ing Representations,
https://openreview.net/forum?id=r1Ue8Hcxg

[4] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning Transferable Architectures for Scalable Image Recognition,” in 2018 IEEE/CVF

Conference on Computer Vision and Pattern Recognition, Jun. 2018, pp. 8697–8710. doi: 10.1109/CVPR.2018.00907.

[5] B. Baker, O. Gupta, N. Naik, and R. Raskar, “Designing Neural Network Architectures using Reinforcement Learning,” in 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [Online].
Available: https://openreview.net/forum?id=S1c2cvqee

[6] H. Pham, M. Y. Guan, B. Zoph, and Q. V. Le, “Eﬃcient Neural Architecture Search via parameter Sharing,” in 35th International Conference

on Machine Learning, ICML 2018, 2018, vol. 9, pp. 6522–6531.

[7] F. Gruau, “Cellular encoding as a graph grammar,” in IEE Colloquium on Grammatical Inference: Theory, Applications and Alternatives,

1993, p. 17/1-1710.

[8] X. Yao and Y. Liu, “A new evolutionary system for evolving artiﬁcial neural networks,” IEEE Trans. Neural Netw., vol. 8, no. 3, pp. 694–713,

1997, doi: 10.1109/72.572107.

[9] K. O. Stanley and R. Miikkulainen, “Evolving Neural Networks through Augmenting Topologies,” Evol. Comput., vol. 10, no. 2, Art. no. 2,

Jun. 2002, doi: 10.1162/106365602320169811.

[10] L. Xie and A. Yuille, “Genetic CNN,” in 2017 IEEE International Conference on Computer Vision (ICCV), Oct. 2017, pp. 1388–1397. doi:

10.1109/ICCV.2017.154.

[11] M. Suganuma, M. Kobayashi, S. Shirakawa, and T. Nagao, “Evolution of Deep Convolutional Neural Networks Using Cartesian Genetic

Programming,” Evol. Comput., pp. 1–23, Mar. 2019, doi: 10.1162/evco a 00253.

[12] Y. Bi, B. Xue, and M. Zhang, “An Evolutionary Deep Learning Approach Using Genetic Programming with Convolution Operators for Image
Classiﬁcation,” in 2019 IEEE Congress on Evolutionary Computation (CEC), 2019, pp. 3197–3204. doi: 10.1109/CEC.2019.8790151.
[13] Y. Sun, B. Xue, M. Zhang, and G. G. Yen, “A Particle Swarm Optimization-Based Flexible Convolutional Autoencoder for Image Classiﬁca-

tion,” IEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 8, pp. 2295–2309, Aug. 2019, doi: 10.1109/TNNLS.2018.2881143.

[14] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, “Regularized Evolution for Image Classiﬁer Architecture Search,” Proc. AAAI Conf. Artif.

Intell., vol. 33, pp. 4780–4789, Jul. 2019, doi: 10.1609/aaai.v33i01.33014780.

[15] X. He, K. Zhao, and X. Chu, “AutoML: A survey of the state-of-the-art,” Knowl.-Based Syst., vol. 212, p. 106622, Jan. 2021, doi:

10.1016/j.knosys.2020.106622.

[16] A. Vaswani et al., “Attention is All You Need,” in Proceedings of the 31st International Conference on Neural Information Processing Systems,

Red Hook, NY, USA, 2017, pp. 6000–6010.

[17] D. So, Q. Le, and C. Liang, “The Evolved Transformer,” in Proceedings of the 36th International Conference on Machine Learning, Jun.

2019, vol. 97, pp. 5877–5886. [Online]. Available: http://proceedings.mlr.press/v97/so19a.html

[18] R. Pasunuru and M. Bansal, “FENAS: Flexible and Expressive Neural Architecture Search,” in Findings of the Association for Computational

Linguistics: EMNLP 2020, Online, Nov. 2020, pp. 2869–2876. doi: 10.18653/v1/2020.ﬁndings-emnlp.258.

10

/ Neurocomputing (2021) 1–11

11

[19] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu, “Hierarchical Representations for Eﬃcient Architecture Search,” in 6th
International Conference on Learning Representations, ICLR 2018, 2018. [Online]. Available: https://openreview.net/forum?id=BJQRKzbA-
[20] B. Ma, X. Li, Y. Xia, and Y. Zhang, “Autonomous deep learning: A genetic DCNN designer for image classiﬁcation,” Neurocomputing, vol.

379, pp. 152–161, Feb. 2020, doi: 10.1016/j.neucom.2019.10.007.

[21] D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” in 3rd International Con-
ference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [Online].
Available: http://arxiv.org/abs/1409.0473

[22] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997.
[23] Y. Kim, “Convolutional Neural Networks for Sentence Classiﬁcation,” in Proceedings of the 2014 Conference on Empirical Methods in

Natural Language Processing (EMNLP), Doha, Qatar, 2014, pp. 1746–1751. doi: 10.3115/v1/D14-1181.

[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,”
in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, 2019, pp. 4171–4186. doi: 10.18653/v1/N19-1423.

[25] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent Convolutional Neural Networks for Text Classiﬁcation,” in AAAI, 2015, pp. 2267–2273.

[Online]. Available: http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745

[26] G. Liu and J. Guo, “Bidirectional LSTM with attention mechanism and convolutional layer for text classiﬁcation,” Neurocomputing, vol. 337,

pp. 325–338, Apr. 2019, doi: 10.1016/j.neucom.2019.01.078.

[27] Y. Zhang, Z. Zhang, D. Miao, and J. Wang, “Three-way enhanced convolutional neural networks for sentence-level sentiment classiﬁcation,”

Inf. Sci., vol. 477, pp. 55–64, Mar. 2019, doi: 10.1016/j.ins.2018.10.030.

[28] S. Wang, “An improved PTAS approximation algorithm for k-means clustering problem,” in 2012 2nd International Conference on Uncer-

tainty Reasoning and Knowledge Engineering, Jalarta, Indonesia, Aug. 2012, pp. 90–94. doi: 10.1109/URKE.2012.6319592.

[29] J. MILLER, “Designing electronic circuits using evolutionary algorithms. arithmetic circuits: A case study,” Genet. Algorithms Evol. Strateg.

Engineeing Comput. Sci., 1998.

[30] J. Rothermich and J. Miller, “Studying the Emergence of Multicellularity with Cartesian Genetic Programming,” in Late Breaking Papers at

the Genetic and Evolutionary Computation Conference (GECCO-2002, 2002, pp. 397–403.

[31] T. Arslan, “Evolvable Components—From Theory to Hardware Implementations,” Genet. Program. Evolvable Mach., vol. 6, no. 4, Art. no.

4, Dec. 2005, doi: 10.1007/s10710-005-3718-x.

[32] A. B. Garmendia-Doval, S. D. Morley, and S. Juhos, “Post Docking Filtering Using Cartesian Genetic Programming,” in Artiﬁcial Evolution,

Berlin, Heidelberg, 2004, pp. 189–200. doi: 10.1007/978-3-540-24621-3 16.

[33] A. B. Garmendia-Doval, J. F. Miller, and S. D. Morley, “Cartesian Genetic Programming and the Post Docking Filtering Problem,” in Genetic
Programming Theory and Practice II, U.-M. O’Reilly, T. Yu, R. Riolo, and B. Worzel, Eds. Boston, MA: Springer US, 2005, pp. 225–244.
doi: 10.1007/0-387-23254-0 14.

[34] E. Real, C. Liang, D. So, and Q. Le, “AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,” in Proceed-
[Online]. Available:

the 37th International Conference on Machine Learning,

Jul. 2020, vol. 119, pp. 8007–8019.

ings of
https://proceedings.mlr.press/v119/real20a.html

[35] L. J. Ba,

J. R. Kiros, and G. E. Hinton, “Layer Normalization,” CoRR, vol. abs/1607.06450, 2016,

[Online]. Available:

http://arxiv.org/abs/1607.06450

[36] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language Modeling with Gated Convolutional Networks,” in Proceedings of the 34th In-
ternational Conference on Machine Learning, International Convention Centre, Sydney, Australia, Aug. 2017, vol. 70, pp. 933–941. [Online].
Available: http://proceedings.mlr.press/v70/dauphin17a.html

[37] J. Pennington, R. Socher, and C. Manning, “Glove: Global Vectors for Word Representation,” in Proceedings of the 2014 Conference on

Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, 2014, pp. 1532–1543. doi: 10.3115/v1/D14-1162.

[38] R. Socher et al., “Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,” in Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language Processing, Seattle, Washington, USA, 2013, pp. 1631–1642. [Online]. Available:
https://www.aclweb.org/anthology/D13-1170

[39] B. Pang and L. Lee, “Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales,” in Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics - ACL ’05, Ann Arbor, Michigan, 2005, pp. 115–124. doi:
10.3115/1219840.1219855.

[40] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, “Learning Word Vectors for Sentiment Analysis,” in Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, USA, 2011, pp.
142–150.

[41] X.

Zhang,

J.

Zhao,

Advances
28,
https://proceedings.neurips.cc/paper/2015/ﬁle/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf

Neural

2015,

vol.

in

and Y.
Information

LeCun,
Processing

“Character-level
Systems,

Convolutional Networks

for

Text

Classiﬁcation,”

pp.

649–657.

[Online].

in
Available:

[42] T. Wolf et al., “Transformers: State-of-the-Art Natural Language Processing,” in Proceedings of the 2020 Conference on Empirical Methods

in Natural Language Processing: System Demonstrations, Online, 2020, pp. 38–45. doi: 10.18653/v1/2020.emnlp-demos.6.

11

