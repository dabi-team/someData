SMT + ILP∗

Vaishak Belle
University of Edinburgh & Alan Turing Institute
vaishak@ed.ac.uk

0
2
0
2

n
a
J

5
1

]
I

A
.
s
c
[

1
v
8
0
2
5
0
.
1
0
0
2
:
v
i
X
r
a

Abstract

Inductive logic programming (ILP) has been a deeply inﬂuen-
tial paradigm in AI, enjoying decades of research on its theory
and implementations. As a natural descendent of the ﬁelds of
logic programming and machine learning, it admits the incor-
poration of background knowledge, which can be very useful
in domains where prior knowledge from experts is available
and can lead to a more data-eﬃcient learning regime.
Be that as it may, the limitation to Horn clauses composed
over Boolean variables is a very serious one. Many phenom-
ena occurring in the real-world are best characterized using
continuous entities, and more generally, mixtures of discrete
and continuous entities. In this position paper, we motivate
a reconsideration of inductive declarative programming by
leveraging satisﬁability modulo theory technology.

Introduction

its

on

has

and

been

logic

(ILP)

research

programming

Inductive
a
deeply inﬂuential paradigm in AI, enjoying decades
implementations
of
theory
(Muggleton and De Raedt 1994;
De Raedt et al. 2008;
Muggleton et al. 2012). ILP continues to be applied in
domains ranging from robotics to biology. As a natural
descendent of the ﬁelds of logic programming and machine
learning, it admits the incorporation of complex background
knowledge, which can be very useful in domains where
prior knowledge from experts is available and can lead
to a more data-eﬃcient learning regime. In essence, the
semantic theory attempts to construct a hypothesis based
on entailment judgements wrt a (possibly small) set of
examples. The construction itself may appeal to principled
notions such as inverse entailment (Muggleton 1991).

Be that as it may, the limitation to Horn clauses composed
over Boolean variables is a very serious one. Many phenom-
ena occurring in the real-world, from gravitational and quan-
tum mechanics to stock price ﬂuctuations, are best character-
ized using continuous entities, and more generally, mixtures
of discrete and continuous entities. While many notable pro-
posals are treating the issue of modeling hybrid phenomena

∗The author was supported by a Royal Society University Re-

search Fellowship.
Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

in logic programming and inductive logic programming set-
tings (Nitti 2016; Speichert and Belle 2018), the underpin-
ning semantics still largely reduces to classical notions with
continuous concepts carefully (but not generally) integrated.
Thus, we consider whether we should upgrade the logical
basis for ILP to natively handle continuous concepts.

in

such

operations

(Barrett et al. 2009),

In recent years, satisﬁability modulo theories (SMT)
has emerged as a pragmatic logical framework for rea-
inequalities and other
soning about complex terms,
arithmetic
as
linear constraints. For ex-
testing the satisﬁability of
ample,
(Chistikov, Dimitrova, and Majumdar 2015;
Belle, Passerini, and Van den Broeck 2015), SMT solvers
were used to generalize model counting to hybrid domains
by computing the volume of the polytope encoded as a
linear constraint over the reals. Other signiﬁcant advances
in SMT have included the handling of inductive constraints
and
(Reynolds and Kuncak 2015;
Gao, Kong, and Clarke 2013). Thus, we argue that SMT
and similar technologies could serve as a reasonable basis
to upgrade ILP for hybrid domains. We do not suggest the
proposal should aim to subsume classical ILP, as it is very
likely that Horn logic over Boolean atoms will be both
suﬃcient and eﬃcient for many problem domains. Rather,
it is meant to be complementary, to potentially tackle a
diﬀerent set of problem domains while beneﬁting from
decades of developments in ILP theory.

non-linear

theories

It is possible, of course, that a reasonable middle ground
could be achieved by appealing to constraint logic program-
ming (Jaﬀar and Maher 1994), as seen in some early work
(Martin and Vrain 1997; Sebag and Rouveirol 1996). How-
ever, it is not entirely obvious such proposals capture the en-
tire range of expressivity that one would with a SMT basis.
In that regard, note that because we are not insisting on logic
programming syntax, we are essentially motivating a recon-
sideration of inductive declarative programming, as opposed
to purely inductive logic programming.

It is also worth remarking that a number of recent devel-
opments relate to the motivation here, such as in the ﬁeld
of constraint learning (De Raedt, Passerini, and Teso 2018),
all of which could be leveraged to strengthen the theoret-
ical and algorithmic foundations of the proposed frame-
work. In (Kolb et al. 2018), a heuristic approach to learn
SMT formulas capturing positive-only examples is consid-

 
 
 
 
 
 
ered. In (Mocanu, Belle, and Juba 2019), the implicit learn-
ing of SMT formulas is investigated via a PAC formula-
tion, while also allowing for noisy examples. The work in
(Molina et al. 2018; Bueﬀ, Speichert, and Belle 2018) can
be seen as learning weighted SMT formulas, albeit simple
ones corresponding to the diﬀerence logic fragment over bi-
nary connectives.

Classical Setup

The basic concepts of logic programming are deﬁned wrt
a ﬁrst-order language, where we have: atoms p(t1, . . . , tn),
consisting of predicates p and terms t1, . . . , tb, understood as
usual. Literals, clauses, deﬁnite clauses and grounding are
understood as usual. Then, in the simplest instance, we have:
Given a set of examples (or observations) E = {e1, . . . , en},
where ei is a ground fact for the unknown target predicate
p, a background theory B as a set of deﬁnition clauses, a
space of clauses Lh speciﬁed using a declarative bias, ﬁnd
a hypothesis H ⊆ Lh where B ∧ H |= E.
The hypothesis would additionally need to satisfy certain ra-
tional generality properties, in the sense of maximally com-
pressing E relative to B (Muggleton 1991). For example,
consider an empty background theory with observations:
parent(f,c), parent(m,c), parent(g,f),grandparent(g,c), and
Lh being the set of deﬁnite clauses. Then we may obtain the
hypothesis: grand parent(x, y) ← parent(x, z), parent(z, y).

Revised Setup

Satisﬁability modulo theories (SMT) is a generalization to
SAT for deciding satisﬁability for fragments and extensions
of ﬁrst-order logics with equality, for which specialized de-
cision procedures have been developed. Deciding satisﬁa-
bility for these logics is done with respect to some decidable
background theory which ﬁxes the interpretations of func-
tions and predicates (Barrett et al. 2009). Brieﬂy, we have:

Syntax We assume a logical signature consisting of the
set of predicates denoted as P, and a set of functions symbols
F , including 0-ary functions, logical variables, and standard
connectives. An atom is one of the form: b (a propositional
symbol), p(t1, ..., tk), t1 = t2, ⊥ (false), ⊤ (true). Literals and
clauses are understood as usual. A ground expression is one
where all the variables are replaced by the domain of dis-
course (e.g., integers, reals, ﬁnite set of named objects).

Semantics Formulas are given a truth value from the set
{true, f alse} by means of ﬁrst order models. A model ρρρ is
a pair consisting of a non-empty set Σ, the universe of the
model and a mapping assigning to each constant symbol a
an element a ∈ Σ (the domain), to each function symbol
f ∈ F of arity n > 0 a total function f : Σn → Σ, to each
propositional symbol b an element b ∈ {true, f alse} and to
each predicate p ∈ P of arity n > 0 a total function p :
Σn → {true, f alse}. Terms are interpreted as usual, as is the
satisfaction relation that is deﬁned inductively. We assume
entailment wrt a suitable background theory (e.g., reals).

A general setting for induction can be taken to be de-
ﬁned over the following languages (Muggleton 1991): Le

(the language of examples), Lb (the language for the back-
ground knowledge) and Lh (the language for the hypothe-
sis), and as can be inferred from above, given B ⊆ Lb and
E ⊆ Le, the task is to ﬁnd H ⊆ Lh, such that H ∧ B |= E.
As far as the background knowlege and hypothesis is con-
cerned, for the SMT setting, we could now imagine Lb, Lh
being fragments of linear real arithmetic, for example, but
this is not necessary. Lb could involve inductive constraints,
and both Lb and Lh could involve non-linear constraints.
The search for the hypothesis could be achieved in the ﬁrst
instance by appealing to reductions of the induction step
to satisﬁability (Evans and Grefenstette 2018). So, not very
much changes at ﬁrst glance, which is a positive develop-
ment for bridging existing ILP theory and frameworks with
this new setting. Moreover, other entailment judgements for
strengtening ILP, e.g., that negative examples where pro-
vided should never be entailed by B ∧ H could be applied
here too. Nonetheless, note that Lb, Lh are richer in some
regards, and not restricted to Horn clauses. We will now dis-
cuss some variants below for Le.

Le = partial models In the simplest instance, we have
observations that are partial models. For example, in a lan-
guage with 0-ary functions {x, y, . . . , z} over the reals, a full
model may be of the form (x = 1, y = 2.3, . . . , z = 6). In
this case, an example might be a partial model of the form
(x = 1), and another might be of the form (y = 2.3). Clearly,
if the conjunction of the partial models is taken as H, then
trivially B ∧ H |= E. But this is a not a very interesting
hypothesis. Like in the classical setting, we would need to
specify Lh in a way to maximally compress the examples
wrt B; so, for example, if B is x + y > z, we might infer H as
y > x by specifying length/syntax restrictions on H wrt E.

Le = sets of partial models It is natural to imagine that
the observations are, in fact, sets of partial models. For ex-
ample, if we were unable to measure x precisely, we may
need to contend with x > 0. So, given examples x > 0 and
z = 0, we might infer the hypothesis x > z.

Le = k-ary functions We have discussed the use of 0-ary
functions above, and that is the case for much of the learning
literature (Kolb et al. 2018; Mocanu, Belle, and Juba 2019).
The natural analogue of the kind of examples seen in clas-
sical ILP might better correspond to the use of k-ary func-
tions with logical variables. Incidentally, this level of expres-
siveness is supported by SMT solvers (Barrett et al. 2009)
for capturing arrays, and so on. As an example, suppose
B includes the expression bmi(x) = weight(x)/height(x)2,
and given examples (partial models) weight(john) = 100,
height(john) = 1.9, weight(mary) = 70, height(mary) = 1.8,
we might infer a hypothesis bmi(x) > 21.

Conclusions
We motivated a reconsideration of inductive declarative pro-
gramming by leveraging SMT. SMT solvers have emerged
to deftly handle arithmetic constraints and inductive deﬁ-
nitions. As a result, the languages that we could consider
for B and H would be signiﬁcantly richer so as to capture
a broad range of problems, and potentially impact the nu-
merous applications areas of SMT (Barrett et al. 2009). Of
course, understanding how ideas from ILP systems can be

lifted for this methodology is an open but exciting ques-
tion. Treating probabilistic concepts (De Raedt et al. 2008)
is another exciting direction, which would relate this
framework to learning in hybrid probabilistic models
(Belle, Passerini, and Van den Broeck 2015) and statistical
relational
learning (Kersting, Natarajan, and Poole 2011)
more generally. It would allow one to express that each
learned clause holds with a certain probability, but not cate-
gorically, for example.

[Belle, Passerini, and Van den Broeck 2015] Belle,

References
[Barrett et al. 2009] Barrett, C.; Sebastiani, R.; Seshia, S. A.;
and Tinelli, C. 2009. Satisﬁability modulo theories.
In
Handbook of Satisﬁability. IOS Press. chapter 26, 825–885.
V.;
Passerini, A.; and Van den Broeck, G. 2015. Probabilistic
inference in hybrid domains by weighted model integration.
In IJCAI.
[Bueﬀ, Speichert, and Belle 2018] Bueﬀ, A.; Speichert, S.;
and Belle, V. 2018. Tractable querying and learning in hy-
brid domains via sum-product networks. In KR Workshop
on Hybrid Reasoning and Learning.

[Chistikov, Dimitrova, and Majumdar 2015] Chistikov, D.;
Dimitrova, R.; and Majumdar, R.
2015. Approximate
counting in SMT and value estimation for probabilistic
programs. In TACAS, volume 9035. 320–334.

[De Raedt et al. 2008] De Raedt, L.; Frasconi, P.; Kersting,
K.; and Muggleton, S., eds. 2008. Probabilistic inductive
logic programming: theory and applications. Berlin, Hei-
delberg: Springer-Verlag.

on Knowledge Representation & Reasoning Meets Machine
Learning.

[Molina et al. 2018] Molina, A.; Vergari, A.; Di Mauro, N.;
Natarajan, S.; Esposito, F.; and Kersting, K. 2018. Mixed
sum-product networks: A deep architecture for hybrid do-
mains. In Thirty-second AAAI conference on artiﬁcial intel-
ligence.

[Muggleton and De Raedt 1994] Muggleton,
1994.

and
De Raedt, L.
Inductive logic programming:
Theory and methods. The Journal of Logic Programming
19:629–679.

S.,

[Muggleton et al. 2012] Muggleton, S.; De Raedt, L.; Poole,
D.; Bratko, I.; Flach, P.; Inoue, K.; and Srinivasan, A. 2012.
Ilp turns 20. Machine learning 86(1):3–23.

[Muggleton 1991] Muggleton, S. 1991. Inductive logic pro-

gramming. New Gen. Comput. 8(4):295–318.

[Nitti 2016] Nitti, D. 2016. Hybrid Probabilistic Logic Pro-

gramming. Ph.D. Dissertation, KU Leuven.

[Reynolds and Kuncak 2015] Reynolds, A., and Kuncak, V.
2015. Induction for smt solvers. In International Workshop
on Veriﬁcation, Model Checking, and Abstract Interpreta-
tion, 80–98. Springer.

[Sebag and Rouveirol 1996] Sebag, M., and Rouveirol, C.

1996. Constraint inductive logic programming.

[Speichert and Belle 2018] Speichert, S., and Belle, V. 2018.
Learning probabilistic logic programs in continuous do-
mains.

[De Raedt, Passerini, and Teso 2018] De

L.;
Passerini, A.; and Teso, S. 2018. Learning constraints from
examples. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence.

Raedt,

[Evans and Grefenstette 2018] Evans, R., and Grefenstette,
E. 2018. Learning explanatory rules from noisy data. Jour-
nal of Artiﬁcial Intelligence Research 61:1–64.

[Gao, Kong, and Clarke 2013] Gao, S.; Kong, S.;

and
Clarke, E. M. 2013. dreal: An smt solver for nonlinear
theories over the reals. In CADE, 208–214.
[Jaﬀar and Maher 1994] Jaﬀar, J., and Maher, M. J. 1994.
Constraint logic programming: A survey. The journal of
logic programming 19:503–581.

[Kersting, Natarajan, and Poole 2011] Kersting, K.; Natara-
jan, S.; and Poole, D. 2011. Statistical relational AI: Logic,
probability and computation.

[Kolb et al. 2018] Kolb, S.; Teso, S.; Passerini, A.; and
De Raedt, L. 2018. Learning smt (lra) constraints using
smt solvers. In IJCAI, 2333–2340.

[Martin and Vrain 1997] Martin, L., and Vrain, C.

1997.
Learning linear constraints in inductive logic programming.
In Proceedings of the 9th European Conference on Machine
Learning, ECML ’97, 162–169. London, UK, UK: Springer-
Verlag.

[Mocanu, Belle, and Juba 2019] Mocanu, I. G.; Belle, V.;
In NeurIPS Workshop

2019. Pac+ smt.

and Juba, B.

