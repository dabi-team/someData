0
2
0
2

p
e
S
1
2

]

G
L
.
s
c
[

5
v
9
3
2
9
0
.
5
0
9
1
:
v
i
X
r
a

Optimal Decision Making Under Strategic Behavior

Stratis Tsirtsis†∗, Behzad Tabibian§∗, Moein Khajehnejad‡∗, Adish Singla†,
Bernhard Sch¨olkopf¶, and Manuel Gomez Rodriguez†

†MPI for Software Systems,

stsirtsis, adishs, manuelgr

{

§Reasonal, Inc., behzad@reason.al
‡Monash University, moein.khajehnejad@monash.edu
¶MPI for Intelligent Systems, bs@tuebingen.mpg.de

@mpi-sws.org
}

Abstract

We are witnessing an increasing use of data-driven predictive models to inform decisions. As decisions
have implications for individuals and society, there is increasing pressure on decision makers to be
transparent about their decision policies. At the same time, individuals may use knowledge, gained by
transparency, to invest eﬀort strategically in order to maximize their chances of receiving a beneﬁcial
decision. Our goal is to ﬁnd decision policies that are optimal in terms of utility in such a strategic setting.
To this end, we ﬁrst characterize how strategic investment of eﬀort by individuals leads to a change in
the feature distribution. Using this characterization, we ﬁrst show that, in general, we cannot expect to
ﬁnd optimal decision policies in polynomial time and there are cases in which deterministic policies are
suboptimal. Then, we demonstrate that, if the cost individuals pay to change their features satisﬁes a
natural monotonicity assumption, we can narrow down the search for the optimal policy to a particular
family of decision policies with a set of desirable properties, which allow for a highly eﬀective polynomial
time heuristic search algorithm using dynamic programming. Finally, under no assumptions on the cost
individuals pay to change their features, we develop an iterative search algorithm that is guaranteed to
ﬁnd locally optimal decision policies also in polynomial time. Experiments on synthetic and real credit
card data illustrate our theoretical ﬁndings and show that the decision policies found by our algorithms
achieve higher utility than those that do not account for strategic behavior.

1

Introduction

Consequential decisions across a wide variety of domains, from banking and hiring to insurances, are
increasingly informed by data-driven predictive models. In all these domains, the decision maker aims to
employ a decision policy that maximizes a given utility function while the predictive model aims to provide
an accurate prediction of the outcome of the process from a set of observable features. For example, in loan
decisions, a bank may decide whether or not to oﬀer a loan to an applicant on the basis of a predictive
model’s estimate of the probability that the individual would repay the loan.

In this context, there is an increasing pressure on the decision makers to be transparent about the decision
policies, the predictive models, and the features they use. However, individuals are incentivized to use this
knowledge to invest eﬀort strategically in order to receive a beneﬁcial decision. With this motivation, there
has been a recent ﬂurry of work on strategic classiﬁcation [3–5, 9–11, 15, 17, 22, 23]. This line of work has
focused on developing accurate predictive models and it has shown that, under certain technical conditions,
it is possible to protect predictive models against misclassiﬁcation errors that would have resulted from

∗Authors contributed equally. Moein Khajehnejad contributed to this work during his internship at the Max Planck Institute for
Software Systems. Behzad Tabibian contributed to this work while he was a graduate student at the Max Planck Institute for Intelligent
Systems and the Max Planck Institute for Software Systems.

1

 
 
 
 
 
 
this strategic behavior. In this work, rather than accurate predictive models, we pursue the development
of decision policies that maximize utility in this strategic setting. Our work is most closely related to a
recent line of work on incentive-aware evaluation mechanisms [1, 20], which aims to design scoring rules
that incentivize agents to invest eﬀort in speciﬁc actions. However, in these works, the decision maker does
not employ a predictive model and the feature distribution of a population of agents to design her decision
policy. As a result, the technical contributions of these works are orthogonal to ours. More broadly, our work
also relates to recent work on the long-term consequences of machine learning algorithms [16, 21, 24, 29],
recommender systems [26, 28] and counterfactual explanations [30, 31, 34].

Once we focus on the utility of a decision policy, it is overly pessimistic to always view an individual’s
strategic eﬀort as some form of gaming, and thus undesirable—an individual’s eﬀort in changing their features
may actually lead sometimes to self-improvement, as noted by several studies in economics [7, 12, 16] and,
more recently, in the theoretical computer science literature [1, 13, 20]. For example, in car insurance decisions,
if an insurance company uses the number of speeding tickets a driver receives to decide how much to charge
the driver, she may feel compelled to drive more carefully to pay a lower price, and this will likely make her a
better driver. In loan decisions, if a bank uses credit card debt to decide about the interest rate it oﬀers
to a customer, she may feel compelled to avoid overall credit card debt to pay less interest, and this will
improve her ﬁnancial situation. In hiring decisions, if a law ﬁrm uses the number of internships to decide
whether to oﬀer a job to an applicant, she may feel compelled to do more internships during her studies to
increase their chances of getting hired, and this will improve her job performance. In all these scenarios, the
decision maker—insurance company, bank, or law ﬁrm—would like to ﬁnd a decision policy that incentivizes
individuals to invest in forms of eﬀort that increase the utility of the policy—reduce payouts or default rates,
or increase job performance.

In this work, we cast the problem as a Stackelberg game in which the decision maker moves ﬁrst and shares
her decision policy before individuals best-respond and invest eﬀort to maximize their chances of receiving a
beneﬁcial decision under the policy. Here, we assume that the decision maker takes decisions based on low
dimensional feature vectors since, in many realistic scenarios, the data is summarized by just a small number
of summary statistics (e.g., FICO scores) [14, 21]. Then, we characterize how this strategic investment of
eﬀort leads to a change in the feature distribution at a population level. More speciﬁcally, we derive an
analytical expression for the feature distribution induced by any policy in terms of the original feature
distribution by solving an optimal transport problem [33]. Based on this analytical expression, we make the
following contributions:

I. We show that the problem of ﬁnding the optimal decision policy is NP-hard by using a novel reduction

of the Boolean satisﬁability (SAT) problem [18].

II. We show that there are cases in which deterministic policies are suboptimal in terms of utility. This is in

contrast with the non-strategic setting, where deterministic threshold rules are optimal [8, 32].

III. Under a natural monotonicity assumption on the cost individuals pay to change features [15, 17], we
show that we can narrow down the search for the optimal policy to a particular family of decision policies
with a set of desirable properties. Moreover, these properties allow for the development of a highly
eﬀective polynomial time heuristic search algorithm using dynamic programming (refer to Algorithm 1).

IV. Under no assumptions on the cost individuals pay to change features, we introduce an iterative search
algorithm that is guaranteed to ﬁnd locally optimal decision policies also in polynomial time (refer to
Algorithm 2)

Finally, we experiment with synthetic and real credit card data to illustrate our theoretical ﬁndings
and show that the decision policies found by our algorithms achieve higher utility than several competitive
baselines1.

1To facilitate research in this area, we release an open-source implementation of our algorithms at https://github.com/Networks-

Learning/strategic-decisions.

2

2 Decision policies, utilities, and beneﬁts

}

∈ {

0, 1

∈ {

1, . . . , n

, a decision
Given an individual with a feature vector x
}
controls whether the label y is realized 2 . This setting ﬁts a variety of real-world scenarios,
d(x)
where continuous features are often discretized into (percentile) ranges. As an example, in a loan decision, the
decision speciﬁes whether the individual receives a loan (d(x) = 1) or her application is rejected (d(x) = 0);
the label indicates whether an individual repays the loan (y = 1) or defaults (y = 0) upon receiving it; and
the feature vector (x) may include an individual’s salary percentile, education, or credit history. Moreover,
we denote the number of feature values using m = nd, assuming that the number of features d is small as
discussed in Section 1.

d and a (ground-truth) label y
}

0, 1

∈ {

Each decision d(x) is sampled from a decision policy d(x)

x) and, for each individual, the labels y
are sampled from P (y
x) and we will
say that the decision policy satisﬁes outcome monotonicity if the higher an individual’s outcome, the higher
their chances of receiving a positive decision, i.e.,

x). Throughout the paper, for brevity, we will write π(x) = π(d = 1

π(d

∼

|

|

|

P (y = 1

|

xi) < P (y = 1

xj)

|

⇔

π(xi) < π(xj)

(1)

Moreover, we adopt a Stackelberg game-theoretic formulation in which the decision maker publishes her
decision policy π before individuals (best-)respond. As it will become clearer in the next section, individual
best responses lead to a change in the feature distribution at a population level—we will say that the new
feature distribution P (x
π) is induced by the policy π. Then, we measure the (immediate) utility a decision
maker obtains using a policy π as the average overall proﬁt she obtains [8, 19, 32], i.e.,

|

u(π, γ) = Ex∼P (x | π), y∼P (y | x), d∼π(d | x)[y d(x)
−
= Ex∼P (x | π), d∼π(d | x)[P (y = 1
x) d(x)

γ d(x)]

|

−

γ d(x)].

(2)

∈

(0, 1) is a given constant reﬂecting economic considerations of the decision maker. For example,
where γ
x) is proportional to the expected number of individuals who
in a loan scenario, the term d(x)P (y = 1
receive and repay a loan, the term d(x)γ is proportional to the number of individuals who receive a loan,
and γ measures the cost of oﬀering a loan in units of repaid loans. Here, note that γ is bounded by the
collateral against the loan, which caps the maximum potential cost to the loan provider. Finally, we deﬁne
the (immediate) individual beneﬁt an individual with features x obtains from a policy π as

|

b(x) = Ed∼π(d | x)[f (d(x))],

(3)

) is problem dependent. Here, for ease of exposition, we will assume that f (d(x)) = d(x)
·
) that is

where the function f (
and thus b(x) = Ed∼π(d | x)[d(x)] = π(x), however, our results can be extended to any function f (
·
proportional to the probability that individuals receive a positive decision.
Remarks on strategic classiﬁcation. Due to Goodhart’s law, if x are noncausal, then x can lose predictive
x) may change. This has been a key insight by previous
power for y after individuals (best-)respond, i.e., P (y
|
x)
work on strategic classiﬁcation [2, 15, 17, 23, 27], which aims to develop accurate predictive models Pθ(y
x) does not change after best-response, a predictive
in a strategic setting. Even if x are causal and P (y
Ex∼P (x), y∼P (y | x)[(cid:96)(x, y, θ)],
model Pθ(y
where (cid:96)(
) is a given loss function, may decrease its accuracy after best-response. This is because, once
·
individuals best respond to a decision policy π, the parameters θ∗ may be suboptimal with respect to the
feature distribution induced by the policy, i.e.,

x) trained using empirical risk minimization, i.e., θ∗ = argminθ

|

|

|

Ex,y∼P (x | π)P (y | x)[(cid:96)(x, y, θ∗)]

min
θ

≥

Ex,y∼P (x | π)P (y | x)[(cid:96)(x, y, θ)].

In the context of strategic classiﬁcation, Miller et al. [22] have very recently argued that (best-)responses
to noncausal and causal features correspond to gaming and improvement, respectively. In this work, for
x), however, the development of
simplicity, we assume that P (y
optimal policies that account for changes on P (x), P (y
x) after individuals best-respond is a
very interesting direction for future work [22, 27].

x) does not change and Pθ(y

|
x) and Pθ(y

x) = P (y

|

|

|

|

2For simplicity, we assume features are discrete and, without loss of generality, we assume each feature takes n discrete values.

3

a) P (x)

c) P (x

|

π), α = 1.1

e) P (x

|

π), α = 0.5

g) P (x

|

π), α = 0.1

b) P (y = 1

x)

d) π(x), α = 1.1

f) π(x), α = 0.5

h) π(x), α = 0.1

|

Figure 1: Optimal decision policies and induced feature distributions. Panels (a) and (b) visualize P (x)
x), respectively.
and P (y = 1
In Panels (c-h), we set the cost to change feature values c(xi, xj) =
|
α[
], where α is a given parameter and γ = 0.2. In all panels, each cell corresponds to
xj1
xi1
+
xi0
|
|
|
a diﬀerent feature value xi and darker colors correspond to higher values. As the cost of moving to further
feature values for individuals decreases, the decision policy only provides positive decisions for a few x values
with high (P (y = 1

γ), encouraging individuals to move to those values.

xj0

−

−

|

x)

|
3 Problem Formulation

−

Similarly as in most previous work in strategic classiﬁcation [4, 9, 10, 15, 17], we consider a Stackelberg
game in which the decision maker moves ﬁrst before individuals best-respond. Moreover, we assume every
individual is rational and aims to maximize her individual beneﬁt. However, in contrast with previous work,
we assume the decision maker shares her decision policy rather than the predictive model. Then, our goal is
to ﬁnd the (optimal) policy that maximizes utility, as deﬁned in Eq. 2, i.e.,

under the assumption that each individual best responds. For each individual, her best response is to change
from her initial set of features xi to a set of features

π∗ = argmax

u(π, γ),

π

(4)

xj = argmax

b(xk)

k∈[m]

c(xi, xk),

−

(5)

where c(xi, xk) is the cost3 she pays for changing from xi to xk. Throughout the paper, we will assume that
(i) it holds that c(xi, xj) > 0 for all i
xi) and (ii) if there are ties in Eq. 5,
the individual chooses to move to the set of features xj with highest P (y

= j such that P (y

xj).

P (y

xj)

≥

|

|

At a population level, this best response results into a transportation of mass between the original
distribution and the induced distribution, i.e., from P (xi) to P (xj
π), as exempliﬁed by Figure 1. In
particular, we can readily derive an analytical expression for the induced feature distribution in terms of the

|

|

3In practice, the cost c(xi, xk) for each pair of feature values may be given by a parameterized function.

4

0.000.050.100.150.000.050.100.150.00.10.20.30.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0(cid:54)
original feature distribution:

P (xj

π) =

|

(cid:88)

i∈[m]

P (xi)I(xj = argmax
k∈[m]

b(xk)

−

c(xi, xk)).

(6)

Note that the transportation of mass between the original and the induced feature distribution has a natural
interpretation in terms of optimal transport theory [33]. More speciﬁcally, the induced feature distribution is
given by P (xj
π) and it is the solution
i∈[m] fij, where fij denotes the ﬂow between P (xi) and P (xj
to the following optimal transport problem:

π) = (cid:80)

|

|

maximize
{fij }

(cid:88)

i,j∈[m]

fij[b(xj)

−

c(xi, xj)]

subject to fij

0

≥

i, j

∀

and

(cid:88)

j∈[m]

fij = P (xi).

Finally, we can combine Eqs. 4-6 and rewrite our goal as follows:

π∗ = argmax

π

(cid:34)

(cid:88)

i,j∈[m]

(P (y = 1

xj)

|

−

γ)π(xj)

(cid:35)

P (xi)I(xj = argmax
k∈[m]

×

b(xk)

−

c(xi, xk))

(7)

where note that, by deﬁnition, 0
≤
practice, the distribution P (x) and the conditional distribution P (y
trained on historical data (see remarks on gaming in Section 2).

π(xj)

≤

1 for all j, the optimal policy π∗ may not be unique and, in
x) may be approximated using models

|

Unfortunately, the following Theorem tells us that, in general, we cannot expect to ﬁnd the optimal policy
that maximizes utility in polynomial time (proven in Appendix A.1 using a novel reduction of the Boolean
satisﬁability (SAT) problem [18]):

Theorem 1. The problem of ﬁnding the optimal decision policy π∗ that maximizes utility in a strategic
setting is NP-hard.

The above result readily implies that, in contrast with the non strategic setting, optimal decision policies

are not always deterministic threshold rules [8, 32], i.e.,

π∗(d = 1

x) =

|

(cid:40)
1
0

if P (y = 1
otherwise.

|

x)

γ

≥

(8)

Even more, in a strategic setting, there are many instances in which the optimal decision policies are not
deterministic, even under outcome monotonic costs. For example, assume x

with γ = 0.1,

1, 2, 3
}

∈ {

P (x) = 0.1 I(x = 1) + 0.4 I(x = 2) + 0.5 I(x = 3)
x) = 1.0 I(x = 1) + 0.7 I(x = 2) + 0.4 I(x = 3)

P (y = 1

|

c(xi, xj) =





0.0
0.3
1.2

0.0
0.0
0.3





0.0
0.0
0.0

|

In the non-strategic setting, the optimal policy is clearly π∗(d = 1
x = 2) = 1 and
π∗(d = 1
x = 3) = 1. However, in the strategic setting, a brute force search reveals that the optimal policy is
stochastic and it is given by π∗(d = 1
x = 3) = 0, inducing
a transportation of mass from P (x = 3) to P (x = 2
π). Moreover, note
that the optimal policy in the strategic setting achieves higher utility than its counterpart in the non-strategic
setting.

|
π) and from P (x = 2) to P (x = 1

x = 2) = 0.7 and π∗(d = 1

x = 1) = 1, π∗(d = 1

x = 1) = 1, π∗(d = 1

|

|

|

|

|

|

5

4 Outcome Monotonic Costs

In this section, we show that, if the cost individuals pay to change features satisﬁes outcome monotonicity [17,
23], we can narrow down the search for the optimal policy to a particular family of decision policies with
a set of desirable properties. A cost satisﬁes outcome monotonicity if improving an individual’s outcome
requires increasing amount of eﬀort, i.e.,

P (y = 1

|

xi) < P (y = 1

xj) < P (y = 1

|

[c(xi, xj) < c(xi, xk)]

xk)
|
[c(xj, xk) < c(xi, xk)]

⇔

∧

P (y

xi) > P (y = 1

and worsening an individual’s outcome requires no eﬀort, i.e., P (y = 1
c(xi, xj) = 0.
Here, without loss of generality, we will index the feature values in decreasing order with respect to their
corresponding outcome, i.e., i < j

xj).
|
Given any instance of the utility maximization problem, as deﬁned in Eq. 4, it is easy to show that the
optimal policy will always decide positively about the feature value with the highest outcome4, i.e., π∗(x1) = 1,
π∗(xi) = 0. However,
and negatively about the feature values with outcome lower than γ, i.e., P (y
if the cost individuals pay to change features satisﬁes outcome monotonicity, we can further characterize a
particular family of decision policies that is guaranteed to contain a policy that achieves the optimal utility.
In particular, we start by showing that there exists an optimal policy that is outcome monotonic (proven in
Appendix A.2):

xi) < γ

P (y

xj)

xi)

⇒

⇒

⇔

≥

|

|

|

|

Proposition 2. Let π∗ be an optimal policy that maximizes utility. If the cost c(xi, xj) is outcome monotonic
then there exists an outcome monotonic policy π such that u(π, γ) = u(π∗, γ).

In the above, note that, given an individual with an initial set of features xi, an outcome monotonic
xi). Otherwise, a contradiction would
policy always induces a best response xj such that P (y
c(xi, xj). Next, we
occur since, by assumption, it would hold that π(xi)
consider additive costs, i.e., c(xi, xj) + c(xj, xk) = c(xi, xk), and afterwards move on to subadditive costs,
i.e., c(xi, xj) + c(xj, xk)
Additive costs. If the cost is additive, we ﬁrst show that we can narrow down the search for the optimal
policy to the policies π that satisfy that

xj)
P (y
π(xj) and π(xj)

c(xi, xk)

π(xj)

|
≥

−

≥

≥

≥

|

π(xi) = π(xi−1)

π(xi) = max(0, π(xi−1)

c(xi, xi−1))

−
for all i > 1 such that P (y
xi) > γ. In the remainder, we refer to any policy with this property as an
outcome monotonic binary policy. More formally, we have the following Theorem (proven in Appendix A.3):

∨

|

(9)

Theorem 3. Let π∗ be an optimal policy that maximizes utility. If the cost c(xi, xj) is additive and outcome
monotonic then there exists an outcome monotonic binary policy π such that u(π, γ) = u(π∗, γ).

Moreover, we can further characterize the best-responses of individuals under outcome monotonic binary

policies and additive costs (proven in Appendix A.4):

Proposition 4. Let π be an outcome monotonic binary policy, c(xi, xj) be an additive and outcome monotonic
.
k
cost, xi be an individual’s initial set of features, and deﬁne j = max
k
|
{
}
∨
If P (y
γ, the individual’s best response is xj
xi)
≤
|
if π(xj)

xi) > γ, the individual’s best response is xj and, if P (y

c(xi, xj) and xi otherwise.

π(xk) = π(xk−1)

i, π(xk) = 1

≤

|

≥

This proposition readily implies that P (xi

= π(xi−1) with π(xi) > 0.
Therefore, it lets us think of the feature values xi with π(xi) = π(xi−1) as blocking states and those
with π(xi)
= π(xi−1) as non-blocking states. Moreover, the above results facilitate the development of a
highly eﬀective heuristic search algorithm based on dynamic programming to ﬁnd close to optimal (outcome
monotonic binary) policies in polynomial time, which we describe later in this section.

π) = 0 for all xi such that π(xi)

|

4As long as P (y | x1) > γ.

6

(cid:54)
(cid:54)
(a) Optimal policy π∗(x)

(b) Subpolicy π5,3(x)

(c) Subpolicy π4,2(x)

(d) Subpolicy π2,1(x)

Figure 2: Optimal policy and subpolicies after Algorithm 1 performs its ﬁrst round. Panel (a) shows the
optimal subpolicy π∗(x), which contains blocking states in x3 and x5. Panel (b) shows the subpolicy π5,3(x),
which is a base subpolicy that can be computed without recursion. Panel (c) shows the subpolicy π4,2(x),
which contains a blocking state in x4 and uses a lowered version of the subpolicy π5,4(x) to set the feature
value x5. Since π4,2(x4)
c(x5, x4) < 0, this value is set equal to π4,2(x4). Panel (d) shows the subpolicy
π2,1(x), which contains a blocking state in x3 and uses a lowered version of the subpolicy π5,3(x) to set the
feature values x4 and x5. Since in π2,1(x), the feature value x5 became negative and was set as blocking, the
algorithm will perform a second round, starting from x3, which is the last blocking state before d = 4.

−

Subadditive costs. If the cost is subadditive, we can show that we need to instead narrow down the search
for the optimal policy to the policies π that satisfy that

π(xi) = π(xi−1)

(cid:95)

∨

j

π(xi) = max(0, π(xi−1)

c(xj, xi−1))

−

(10)

for all i > 1 such that P (y
. More for-
}
mally, we have the following proposition, which can be easily shown using a similar reasoning as the one used
in the proof of Theorem 3:

xi) > γ and j = i, . . . k with k = max

c(xj, xi−1) > 0

π(xi−1)

j
{

−

|

|

Proposition 5. Let π∗ be an optimal policy that maximizes utility. If the cost c(xi, xj) is subadditive and
outcome monotonic then there exists an outcome monotonic policy π satisfying Eq. 10 such that u(π, γ) =
u(π∗, γ).

Similarly as in the case of additive costs, it is possible to characterize the best response of the individuals5
and adapt the above mentioned heuristic search algorithm to ﬁnd optimal (outcome monotonic binary)
policies with subadditive costs, however, the resulting algorithm is rather impractical due to its complexity
and therefore we omit the details.
Dynamic programming algorithm. The key idea behind our algorithm (refer to Algorithm 1) is to
recursively create a set of decision subpolicies Π = (πi,j(xk)) where i, j = 1, . . . , m with j < i, k = j, . . . , m,
which we later use to build the entire decision policy π. Moreover, depending on the structure of the costs
and feature and label distributions, we may need to perform several rounds and, in each round, create a new
set of decision subpolicies, which are used to set only some values of the decision policy (lines 31–33).

|

≤

xk)

More speciﬁcally, in each round, we proceed in decreasing order of i and j (lines 5–6) until the feature
value index s, which is computed in the previous round (lines 20, 27). For each subpolicy πi,j: (i) we ﬁx
πi,j(xj) = π2,1(xs), πi,j(xk) = π(xk−1)
c(xk, xk−1) for all j < k < i and πi,j(xk) = 0 for all k such that
γ) (line 4); (ii) we decide whether to block or not to block the feature value xi, i.e., set πi,j(xi) to
P (y
either πi,j(xi−1) or πi,j(xi−1)
c(xi, xi−1), based on previously computed subpolicies within the round (lines
13–14); and, (iii) if we decide to block the feature value xi, we set the remaining policy values by appending the
best of these previously computed subpolicies in terms of overall utility (lines 15–19 and 22–26). Here, note that
0,
there is a set of base subpolicies, those with i = r where r = max
{
which can be computed directly, without recursion (line 4). Moreover, if we decide to block xi in a subpolicy
πi,j, we need to lower the values of the previously computed subpolicies down (line 13) by σ = c(xi−1, xj)
c(xi−1, xj) eventually. However, some of these values may
before appending them so that πi,j(xi) = π2,1(xs)

c(xi−1, xj)

xk) > γ

k : P (y

and 1

−

≥

−

−

}

|

−

5In this case, however, each possible decision policy value blocks zero, one or more feature values.

7

Algorithm 1 DynamicProgramming: It searches for the optimal decision policy that maximizes utility
under additive and outcome monotonic costs.
Require: Number of feature values m, constant c, distributions P = [P (xi)] and Q = [P (y | xi)], and cost C =

[c(xi, xj)]

1: Π ← InitializeSubpolicies()
2: s ← 1
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:
12:

continue

r, Π, F ← ComputeBaseSubpolicies(C, P , Q, π2,1(xs))
for i = r − 1, . . . , s + 1 do
for j = i − 1, . . . , s do

if c(xi−1, xj) > π2,1(xs) then

end if
V (i, j) = m
σ ← c(xi−1, xj)
G ← (P (y | xj) − c) (cid:80)
π(cid:48), F (cid:48), v(cid:48) ← Lower(πi+1,i, F (i + 1, i), σ) + G)
if F (i + 1, j) ≥ F (cid:48) and c(xi, xj) ≤ π2,1(xs) then

j : j≤k<i P (xk)

13:

14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:

29:
30:

F (i, j) ← F (i + 1, j)
πi,j(xi) ← πi,j(xi−1) − c(xi, xi−1)
for l = i + 1, · · · , m do
πi,j(xl) = πi+1,j(xl)

end for
V (i, j) = V (i + 1, j)

else

F (i, j) ← F (cid:48)
πi,j(xi) ← πi,j(xi−1)
for l = i + 1, · · · , m do
πi,j(xl) = π(cid:48)(xl)

end for
V (i, j) ← min(v(cid:48), V (i + 1, i))

end if

end for

end for

for l = s, . . . , V (s + 1, s) do

π(xl) ← πs+1,s(xl)

31:
32:
end for
33:
s ← V (s + 1, s)
34:
35: until V (s + 1, s) = m
36: Return π, u(π, γ)

0

σ

−

≥

l : πi+1,i(xl)
{

become negative and are thus decided to be blocking sates, i.e., π(cid:48)(xk) = πi+1,i(xd)
k > d where
d = max
. If during this procedure the lowered policy makes some individual change
}
their best-response, the policy values starting from the last blocking state v(cid:48) before d will be revisited in
another round (line 35). Figure 2 shows several examples of subpolicies πi,j, showing the lowering procedure.
,
}
, the base subpolicies and their
}
σ if that quantity
σ otherwise, its corresponding utility F (cid:48), and calculates the index

ComputeBaseSubpolicies(. . .) computes r = max
utilities, and Lower(πi+1,i, F (i + 1, i), σ) computes a policy π(cid:48) with π(cid:48)(xk) = πi+1,i(xk)
is non-negative and π(cid:48)(xk) = πi+1,i(xd)
v(cid:48) of the last blocking state before d as described in the previous paragraph.

Within the algorithm, the function InitializeSubpolicies() initializes the subpolicies Π =

k : P (y
{

xk) > γ

πi,j
{

k : r

≥

−

−

−

σ

∀

|

As mentioned above, the algorithm might need more than one round to terminate. Since each round
consists of one dynamic programming execution, an array of utility values of all subpolicies needs to be

8

Algorithm 2 Iterative: It approximates the optimal decision policy that maximizes utility under general
cost.
Require: Number of feature values m, constant c, distributions P = [P (xi)] and Q = [P (y | xi)], and cost C =

[c(xi, xj)]

π(cid:48) ← π
for i = 1, . . . , m do

1: π ← InitializePolicy()
2: repeat
3:
4:
5:
6:
7: until π = π(cid:48)
8: Return π(cid:48), u(π(cid:48), c)

end for

π(xi) ← solve(i, π, C, P , Q)

(m2), considering that each state variable i, j takes values from the set

computed, having a size of
.
}
Given an outcome monotonic binary policy π, according to Proposition 4, we can easily characterize the
best-response of each individual and it can be easily seen that the overall utility u(π, γ) can be computed
with a single pass over the feature values. Therefore, computing each entry’s value in the aforementioned
array takes

(m) time, leading to a total round complexity of

1, 2, .., m
{

(m3).

O

Now, consider the total number of rounds. It can be observed that a second round is executed iﬀ s

= m at
the end of the ﬁrst one, implying that at least one feature value was blocked since the value of V (i, j) might
get altered only when choosing to block a feature value because of the Lower operation. Therefore, we can
(m)
deduce that during each round ending with s
O
(m4).
bound on the total number of rounds. As a consequence, the overall complexity of the algorithm is

= m, at least one feature value gets blocked, leading to a

O

O

O

5 General costs

In this section, we ﬁrst show that, under no assumptions on the cost people pay to change features, the
optimal policy may violate outcome monotonicity. Then, we introduce an eﬃcient iterative algorithm that it
is guaranteed to terminate and ﬁnd locally optimal decision policies. Finally, we propose a variation of the
algorithm that can signiﬁcantly reduce its running time when working with real data.
There may not exist an optimal policy that satisﬁes outcome monotonicity under general costs.
Our starting point is the toy example introduced at the end of Section 3. Here, we just modify the cost
individuals pay to change features so that it violates outcome monotonicity of the costs. More speciﬁcally,
assume x

with γ = 0.1,

1, 2, 3

∈ {

}

P (x) = 0.1 I(x = 1) + 0.4 I(x = 2) + 0.5 I(x = 3)
x) = 1.0 I(x = 1) + 0.7 I(x = 2) + 0.4 I(x = 3)

P (y = 1

|

c(xi, xj) =





0.0
0.3
1.2

0.2
0.0
1.1





0.3
0.7
0.0

x = 2)

0.7 and π∗(d = 1

Now, in the strategic setting, it is easy to see that every policy given by π∗(d = 1

x = 1) = 1,
x = 3) = 1 is optimal and induces a transportation of mass from
π). Therefore, optimal policies are not necessarily outcome monotonic under general

π∗(d = 1
≤
|
P (x = 2) to P (x = 1
costs.
An iterative algorithm for general costs. The iterative algorithm is based on the following key insight:
ﬁx the decision policy π(x) for all feature values x = xk except x = xi. Then, Eq. 7 reduces to searching
over a polynomial number of values for π(xi).

|

|

|

Exploiting this insight, the iterative algorithm proceeds iteratively and, at each each iteration, it optimizes
the decision policy for each of the feature values while ﬁxing the decision policy for all other values. Algorithm 2
summarizes the iterative algorithm. Within the algorithm, InitializePolicy() initializes the decision policy

9

(cid:54)
(cid:54)
to π(x) = 0 for all x, Solve(i, π, C, P , Q) ﬁnds the best policy π(xi) for xi given π(xk) for all xk
= xi,
xi)] by searching over a polynomial number of values. In
C = [c(xi, xj)], P = [P (xi)] and Q = [P (y
practice, we proceed over feature values in decreasing order with respect to P (y
xi) because we have observed
it improves performance. However, our theoretical results do not depend on such ordering. In the following,
we refer to lines 2-7 of Algorithm 2 as one iteration and line 5 as one step.
Theoretical guarantees of the iterative algorithm. We start our theoretical analysis with the following
Proposition, which shows that our algorithm is guaranteed to terminate and that the number of steps is, at
most polynomial, in the number of feature values m (proven in Appendix A.5):

|

|

1 steps, where ¯u is the greatest common
xi, xj, xk

1, . . . , m

16.

Proposition 6. Algorithm 2 terminates after at most m1+ 1
denominator of all elements in the set A =

¯u

c(xi, xj)
{

−
c(xi, xk)
|

−
It readily follows that, at each step, our iterative algorithm is guaranteed to ﬁnd a better policy π, i.e.,
u(π, γ) > u(π(cid:48), γ). This is because Solve(i, π, C, P ) always returns a better policy π and, by deﬁnition, at
= π(cid:48). As a direct consequence of the above results, we can conclude that Algorithm 2
the end of each step, π
ﬁnds locally optimal decision policies in polynomial time.

}} ∪

∈ {

O

O

O

< 30, 30

= xi and they can all be evaluated in

Moreover, we can characterize the computational complexity of the algorithm as follows. At each iteration,
(m) candidate values for π(xi) when
(m2). Therefore, the iteration complexity of

the algorithm calls Solve m times and, within Solve, there are
π(xk) is ﬁxed for all xk
(m3).
Algorithm 2 is
Speeding up the iterative algorithm in the presence of non-actionable features. In this section,
we discuss a highly eﬀective strategy to speed up the iterative algorithm whenever some of the features are
non-actionable, which is amenable to parallelization. As an example, assume there is an Age Group feature
which takes values
. Now, consider two individuals with initial feature values xi, xj
}
such that xi,AgeGroup = < 30 and xj,AgeGroup = > 60. Since individuals cannot change their age, it holds
be an undirected graph where each node vi represents a feature value
that c(xi, xj) = c(xj, xi) =
xi and there is an edge ei,j between two nodes vi and vj iﬀ c(xi, xj)
1. Then, if there
are non-actionable features, it is easy to see that the graph
may be composed of several independent
connected components. Assume vi and vj belong to two diﬀerent connected components. Then, whatever
value is picked for π(xi), the best-response of individuals with initial features xj will never be xi since
π(xi)
π(xj) and therefore xj will always be a better response.
Similarly, the best-response of individuals with initial features xi will never be xj independently of the value
of π(xj). As a consequence, we can ﬁnd the values of the optimal policy by running the iterative algorithm
independently on each independent component.

c(xj, xi) < 0

c(xj, xi)

c(xj, xi)

60, > 60

π(xi)

. Let

∞

⇒

≤

≤

≤

≤

−

≤

−

−

∨

G

G

1

{

1

1

6 Experiments on Synthetic Data

In this section, we evaluate both our dynamic programming algorithm (Algorithm 1) and our iterative
algorithm (Algorithm 2) on outcome monotonic and general costs. We ﬁrst compare the utility achieved by
the decision policies found by our algorithms and those found by several competitive baselines. Then, we
compare their computational complexity both in terms of running time and number of rounds (or iterations)
to termination.
Performance evaluation. We compare the utility achieved by the decision policies found by our algorithms
and those found by several baselines. More speciﬁcally, we consider:

(i) the optimal deterministic threshold rule in a non-strategic setting (Eq. 8) [Non-Strategic];

(ii) the optimal deterministic threshold rule in a strategic setting, found via bruteforce search over all

deterministic threshold rules [Threshold ];

(iii) the optimal (stochastic) decision policy in a strategic setting, found via brute force search [Bruteforce];

6The common denominator ¯u satisﬁes that a

¯u ∈ Z ∀a ∈ A ∪ {1}. Such ¯u exists if and only if a

b is rational ∀a, b ∈ A.

10

(cid:54)
(cid:54)
(cid:54)
(a) Outcome monotonic additive costs

(b) General costs

Figure 3: Performance evaluation on synthetic data. Panels show the utility obtained by several decision
policies against the number of feature values m. Here, note that the dynamic programming (DP) algorithm
(Algorithm 1) only works with outcome monotonic additive costs and thus only appears in Panel (a). In
Panel (a), we set κ = 0.1 and, in Panel (b), we set κ = 0.75.

(a) Running time vs. # feature values

(b) # iterations vs. # feature values

(c) # rounds vs. # feature values

Figure 4: Running time analysis on synthetic data with outcome monotonic and additive costs. Panel (a)
shows the running time of the brute force search, the threshold policy baseline, our iterative algorithm and
our dynamic programming algorithm. Panels (b) and (c) show the number of iterations and rounds required
by the iterative and dynamic programming algorithms until termination, respectively, for diﬀerent κ values.
In Panel (a), we set κ = 0.1.

(iv) the (stochastic) decision policy found by our dynamic programming algorithm (Algorithm 1), which we

can only run for instances with outcome monotonic additive costs [DP ].

(v) the (stochastic) decision policy found by our iterative algorithm (Algorithm 2) [Iterative];

x)

|

∼

Here, for simplicity, we consider unidimensional features with m discrete values x
compute P (x = i) = pi/ (cid:80)
truncated from below at zero. Then, we sample P (y = 1

and
j pj, where pi is sampled from a Gaussian distribution N (µ = 0.5, σ = 0.1)

U [0, 1] and we set γ = 0.3.

0, . . . , m

∈ {

−

1

}

|

∀

∈

For instances with outcome monotonic additive costs, we initially set c(xi, xj) = 0

−
i < j and κ

xi). Then, we take m

∀
1 samples from U [0, 1/κ] and assign them to c(xm, xi)

≤
i < m such that
P (y
(0, 1]. Finally, we set the remaining values c(xi, xj), in decreasing
c(xm, xi) > c(xm, xj)
order of i and j such that c(xi, xj) = c(xi−1, xj)
It is easy to observe that, proceeding
−
this way, individuals with feature values xi can move (on expectation) to at most κm better states, i.e.,
j < i. For instances with general costs, we sample the cost between
c(xi, xj)
1
for the remaining pairs.
feature values c(xi, xj)
Figure 3 summarizes the results for both outcome monotonic and general costs, where we repeat each
experiment 10 times to obtain error bars. In both cases, we observe that the optimal decision policy in a
non-strategic setting has an underwhelming performance. For outcome monotonic additive costs, we observe
that the policies found using our dynamic programming algorithm and brute force search closely match each

U [0, 1] for a fraction κ of all pairs and set c(xi, xj) =

xi, xj : max(1, i

c(xi−1, xi).

xi, xj : P (y

κm)

xj)

∞

≤

≤

−

∼

∀

∀

|

11

456789100200300400500600m0.150.200.250.30Utility,u(π,γ)Non-StrategicThresholdBruteforceIterativeDP456789100200300400500600m0.00.20.40.60.81.0Utility,u(π,γ)Non-StrategicThresholdBruteforceIterative456789100200300400500600m10−310−1101103Time(s)ThresholdBruteforceIterativeDP456789100200300400500600m12345#iterationsκ=0.1κ=0.25κ=0.5456789100200300400500600m123#roundsκ=0.1κ=0.25κ=0.5other in terms of utility and they consistently outperform the policies found by the iterative algorithm. For
general costs, we ﬁnd that our iterative algorithm and the threshold policy baseline are the top performers.
Running time and number of iterations/rounds. To compare the running time of all the aforementioned
algorithms7, we consider the same conﬁguration as in the performance evaluation with outcome monotonic
and additive costs. Figure 4a summarizes the results, which show several interesting insights. We ﬁnd that
brute force search quickly becomes computationally intractable. Moreover, we observe that the dynamic
programming algorithm, is signiﬁcantly faster than the iterative algorithm, making it the most eﬃcient of the
proposed algorithms. To understand why, we compute the number of iterations/rounds the two algorithms
take to terminate in Figures (4b,4c). Recall that the complexity of one round in the dynamic programming
(m3). The results show that, although in theory,
algorithm and one iteration in the iterative algorithm is
the dynamic programming algorithm needs
(m) rounds to terminate, in practice, it rarely needs more than
two rounds. This is in contrast with the iterative algorithm which might need a larger number of iterations
to converge, especially for large values of m. Overall, the above results let us conclude that, under outcome
monotonic additive costs, the dynamic programming algorithm is a highly eﬀective and eﬃcient heuristic.

O

O

7 Experiments on Real Data

In this section, we evaluate our iterative algorithm using real credit card data. Since in our experiments, the
cost individuals pay to change features is not always monotonic, we cannot experiment with our dynamic
programming algorithm.
Experimental setup. We use the publicly available credit dataset [35], which contains information about a
bank’s credit card payoﬀs8. For each accepted credit card holder, the respective dataset contains various
demographic characteristics and ﬁnancial status indicators which serve as features x and the current credit
payoﬀ status which serves as label y. Among the features, we distinguish both numerical and discrete-valued
features as well as actionable (e.g., most recent bill amount) and non-actionable (e.g., marital status) features.
Refer to Appendix B.1 for more details on the speciﬁc features we used.

To approximate the conditional distribution P (y

x), we follow the same procedure as in Tsirtsis and
|
Gomez-Rodriguez [30], which we describe next for completeness. First, we cluster the credit card holders into
k groups based on the original numerical features using k-means clustering and then, for each credit card
holder, we replace their initial numerical features with the respective identiﬁer of the cluster they belong to,
represented using a one-hot encoding. After this preprocessing step, the discrete feature values xi consist of
all possible value combinations of discrete non-actionable features, if any, and cluster identiﬁers. Then, we
train four types of classiﬁers (Multi-layer perceptron, support vector machine, logistic regression, decision
tree) using scikit-learn [6] with default parameters. Finally, we choose the pair of classiﬁer type and number
of clusters k that maximizes accuracy, estimated using 5-fold cross validation, to approximate the values of
P (y

|
To set the cost function c(xi, xj) values, we use the maximum percentile shift [31]. More speciﬁcally, let
be the set of actionable (numerical) features and ¯
be the set of non-actionable (discrete-valued) features.
L

L
Then, for each pair of feature values, we set the cost function c(xi, xj) to:

x).

(cid:40)

α

maxl∈L

Ql(xj,l)
|

Ql(xi,l)

if xi,l = xj,l
otherwise,

|

−

c(xi, xj) =

∈ L

and α

·
∞
where xj,l is the value of the l-th feature for the feature value xj, Ql(
·
l
set the cost c(xi, xj) between two feature values to
Total months overdue
}

if Ql(xj,l) < Ql(xi,l) for l
, not allowing the history of overdue payments to be erased.

) is the CDF of the numerical feature
1 is a scaling factor which controls the diﬃculty of changing features. As an exception, we always
Total overdue counts,

Finally, we set the parameter γ to the 50-th percentile of all the individuals’ P (y = 1

x), such that 50% of
|
the population is accepted by the optimal threshold policy in the non strategic setting. Refer to Appendix B.2
for further details on the experimental setup.

∈ {

∞

≥

l
∀

∈

¯
L

(11)

7We ran all experiments on a machine equipped with 48 Intel(R) Xeon(R) 3.00GHz CPU cores and 1.2TB memory.
8We used a preprocessed version of the credit dataset by Ustun et al. [31]

12

(a) Non-strategic setting

(b)

Strategic setting
with α = 10

(c)

Strategic setting
with α = 3.3

(d)

Strategic setting
with α = 1

Figure 5: Transportation of mass induced by the policies found via the iterative algorithm (Algorithm 2) in
the credit dataset for several values of α, which controls the diﬃculty of changing features. For each individual
in the population, we record her outcome P (y = 1
x)) and
x)). In each panel, the color illustrates the percentage of individuals
after the best response (Final P (y = 1
with the corresponding initial and ﬁnal P (y = 1

x) before the best-response (Initial P (y = 1

x) values.

|

|

|

|

(a) Utility vs. 1/α

(b) Running time vs. 1/α

(c) # components vs. 1/α

Figure 6: Eﬀectiveness and eﬃciency of the proposed algorithms. Panel (a) shows the utility achieved by
three types of decision policies in the credit dataset, against the value of the parameter α, which controls how
diﬃcult it is for the individuals to change their features. Panel (b) shows the running time of the threshold
policy baseline algorithm and our iterative algorithm, with and without the speed-up discussed in Section 5.
. Note that, whenever we implement the
Panel (c) shows the number of connected components in the graph
iterative algorithm with the speed up, we solve the subproblems corresponding to independent components
sequentially, however, the procedure is amenable to parallelization.

G

Results. We ﬁrst look into the transportation of mass induced by the decision policy found by our iterative
algorithm for diﬀerent α values in Figure 5. We observe that, as the cost of changing features increases, there
is a higher transportation of mass towards feature values with the highest outcomes P (y = 1
x). Moreover,
whenever individuals can arbitrarily change actionable features (i.e., α = 1), the best-response of individuals
is either feature values with the highest outcomes or their initial features if their recourse may be limited due
to non-actionable features (e.g., history of overdue payments).

|

Next, we compare the utility of the decision policy found by our iterative algorithm and the policies found
by the same baselines used in Section 6. Here, we do not compare with the optimal (stochastic) decision
policy because brute force search does not scale to the size of the dataset. Figure 6a summarizes the results
for several values of the cost scaling factor α, which show that the decision policy found by the iterative
algorithm outperforms the baselines and, as the cost of changing features becomes smaller (α decreases), the
utility value increases.

Finally, we compare the running time of the threshold baseline and the iterative algorithm with and without
the speed up that exploits the presence of non-actionable features, described in Section 5. Figures 6b, 6c
summarize the results. We observe that, whenever the cost to change features is high, there exist many
independent connected components and the speed up provides a signiﬁcant advantage. In those cases, the

13

0.5020.7630.8580.8870.916FinalP(y=1|x)0.5020.7630.8580.8870.916InitialP(y=1|x)0.000.020.040.060.080.100.5020.7630.8580.8870.916FinalP(y=1|x)0.5020.7630.8580.8870.916InitialP(y=1|x)0.000.020.040.060.080.100.5020.7630.8580.8870.916FinalP(y=1|x)0.5020.7630.8580.8870.916InitialP(y=1|x)0.000.020.040.060.080.100.5020.7630.8580.8870.916FinalP(y=1|x)0.5020.7630.8580.8870.916InitialP(y=1|x)0.000.020.040.060.080.1010%20%30%50%100%Allowedpercentileshift,1/α0.020.030.040.050.06Utility,u(π,γ)Non-StrategicThresholdIterative10%20%30%50%100%Allowedpercentileshift,1/α101102103104105Time(s)ThresholdIterativeIterative(Optimized)10%20%30%50%100%Allowedpercentileshift,1/α050010001500Connectedcomponentsiterative algorithm with the speed up performs faster than the threshold baseline while the running time of
the two algorithms remains comparable, even when the cost to change features is low.

8 Conclusions

In this paper, we have studied the problem of ﬁnding optimal decision policies that maximize utility in a
strategic setting. We have shown that, in contrast with the non-strategic setting, optimal decision policies
that maximize utility are hard to ﬁnd. However, if the cost individuals pay to change their features satisﬁes
a natural monotonicity assumption, we have demonstrated that we can narrow down the search for the
optimal policy to a particular family of decision policies, which allow for a highly eﬀective polynomial time
dynamic programming heuristic search algorithm. Finally, we have lifted the monotonicity assumption and
developed an eﬃcient iterative search algorithm that is guaranteed to ﬁnd locally optimal decision policies
also in polynomial time.

Our work opens up many interesting avenues for future work. For example, in our experiments, we
have found that our dynamic programming algorithm often ﬁnds outcome monotonic binary policies with
optimal utility. However, we are lacking theoretical guarantees, e.g., approximation guarantees, supporting
this good empirical performance. Throughout the paper, we have assumed that features take discrete values.
It would be very interesting to extend our work to real valued features. Moreover, we have considered
policies that maximize utility. A natural step would be considering utility maximization under fairness
constraints [14, 36, 37]. Similarly as in previous work in strategic classiﬁcation, the individuals have white-box
access to the decision policy, however, in practice, they may only have access to explanations of speciﬁc
outcomes. Finally, there are reasons to believe that causal features should be more robust to strategic
behavior [22]. It would be interesting to investigate the use of causally aware feature selection methods [25]
in strategic settings.

References

[1] Tal Alon, Magdalen Dobson, Ariel D Procaccia, Inbal Talgam-Cohen, and Jamie Tucker-Foltz. Multiagent

evaluation mechanisms. In AAAI, pages 1774–1781, 2020.

[2] Yahav Bechavod, Katrina Ligett, Zhiwei Steven Wu, and Juba Ziani. Causal feature discovery through

strategic modiﬁcation. arXiv preprint arXiv:2002.07024, 2020.

[3] Mark Braverman and Sumegha Garg. The role of randomness and noise in strategic classiﬁcation. arXiv

preprint arXiv:2005.08377, 2020.

[4] M. Br¨uckner and T. Scheﬀer. Stackelberg games for adversarial prediction problems. In KDD, 2011.

[5] M. Br¨uckner, C. Kanzow, and T. Scheﬀer. Static prediction games for adversarial learning problems.

JMLR, 2012.

[6] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad
Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas,
Arnaud Joly, Brian Holt, and Ga¨el Varoquaux. API design for machine learning software: experiences
from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine
Learning, 2013.

[7] S. Coate and G. Loury. Will aﬃrmative-action policies eliminate negative stereotypes? The American

Economic Review, 1993.

[8] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision

making and the cost of fairness. KDD, 2017.

[9] N. Dalvi, P. Domingos, S. Sanghai, and D. Verma. Adversarial classiﬁcation. In KDD, 2004.

14

[10] J. Dong, A. Roth, Z. Schutzman, B. Waggoner, and Z. Wu. Strategic classiﬁcation from revealed

preferences. In EC, 2018.

[11] Alex Frankel and Navin Kartik.

Improving information from manipulable data. arXiv preprint

arXiv:1908.10330, 2019.

[12] R. Fryer and G. Loury. Valuing diversity. Journal of Political Economy, 2013.

[13] Nika Haghtalab, Nicole Immorlica, Brendan Lucier, and Jack Wang. Maximizing welfare with incentive-

aware evaluation mechanisms. In IJCAI, 2020.

[14] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. In NeurIPS, 2016.

[15] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classiﬁcation.

In ITCS, 2016.

[16] L. Hu and Y. Chen. A short-term intervention for long-term fairness in the labor market. In WWW,

2018.

[17] Lily Hu, Nicole Immorlica, and Jennifer Wortman Vaughan. The disparate eﬀects of strategic manipula-

tion. In FAT∗, 2019.

[18] Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer computations,

pages 85–103. Springer, 1972.

[19] Niki Kilbertus, Manuel Gomez-Rodriguez, Bernhard Sch¨olkopf, Krikamol Muandet, and Isabel Valera.

Fair decisions despite imperfect predictions. In AISTATS, 2020.

[20] J. Kleinberg and M. Raghavan. How do classiﬁers induce agents to invest eﬀort strategically? In EC,

2019.

[21] L. Liu, S. Dean, E. Rolf, M. Simchowitz, and M. Hardt. Delayed impact of fair machine learning. In

NeurIPS, 2018.

[22] John Miller, Smitha Milli, and Moritz Hardt. Strategic classiﬁcation is causal modeling in disguise. In

ICML, 2020.

[23] Smitha Milli, John Miller, Anca D Dragan, and Moritz Hardt. The social cost of strategic classiﬁcation.

In FAT∗, 2019.

[24] H. Mouzannar, M. Ohannessian, and N. Srebro. From fair decision making to social equality. In FAT∗,

2019.

[25] M. Rojas-Carulla, B. Sch¨olkopf, R. Turner, and J. Peters. Invariant models for causal transfer learning.

JMLR, 2018.

[26] T. Schnabel, P. Bennett, S. Dumais, and T. Joachims. Short-term satisfaction and long-term coverage:

Understanding how users tolerate algorithmic exploration. In WSDM, 2018.

[27] Yonadav Shavit, B Edelman, and Brian Axelrod. Causal strategic linear regression. In Proceedings of

the 37th International Conference on Machine Learning, 2020.

[28] A. Sinha, D. Gleich, and K. Ramani. Deconvolving feedback loops in recommender systems. In NeurIPS,

2016.

[29] B. Tabibian, V. Gomez, A. De, B. Schoelkopf, and M. Gomez-Rodriguez. Consequential ranking

algorithms and long-term welfare. In UAI, 2020.

15

[30] Stratis Tsirtsis and Manuel Gomez-Rodriguez. Decisions, counterfactual explanations and strategic

behavior. arXiv preprint arXiv:2002.04333, 2020.

[31] Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classiﬁcation. In FAT∗,

2019.

[32] Isabel Valera, Adish Singla, and Manuel Gomez-Rodriguez. Enhancing the accuracy and fairness of

human decision making. In NeurIPS, 2018.

[33] C´edric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.

[34] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the

black box: Automated decisions and the gpdr. Harv. JL and Tech., 2017.

[35] I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy
of probability of default of credit card clients. Expert Systems with Applications, 36(2):2473–2480, 2009.

[36] B. Zafar, I. Valera, M. Gomez-Rodriguez, and K. Gummadi. Fairness beyond disparate treatment &

disparate impact: Learning classiﬁcation without disparate mistreatment. In WWW, 2017.

[37] B. Zafar, I. Valera, M. Gomez-Rodriguez, and K. Gummadi. Fairness constraints: Mechanisms for fair

classiﬁcation. JMLR, 2019.

16

A Proofs

A.1 Proof of Theorem 1

We will reduce any given instance of the SAT problem [18], which is known to be NP-complete, to a particular
instance of our problem. In a SAT problem, the goal is ﬁnding the value of a set of boolean variables
, that satisfy s number of OR clauses, which we
¯v1, ¯v2, . . . , ¯vl
v1, v2, . . . , vl
{
{
label as

, and their logical complements

k1, k2, . . . , ks

}

}

.

First, we start by representing our problem, as deﬁned in Eq. 7, using a directed weighted bipartite graph,
whose nodes can be divided into two disjoint sets U and V . In each of these sets, there are m nodes with
. We characterize each node xi in U with P (xi) and each node xj in V with π(xj) and
labels
x1, . . . , xm
}
{
u(xj) = P (y = 1
γ. Then, we connect each node xi in U to each node xj in V and characterize each
xj)
|
edge with a weight w(xi, xj) = b(xj)

c(xi, xj) and a utility

c(xi, xj) = π(xj)

−

{

}

−

−

u(xi, xj) = π(xj)u(xj)P (xi)I(xj = argmax

xk

w(xi, xk)),

which, for each node xi in U , is only nonzero for the edge with maximum weight (solving ties at random).
Under this representation, the problem reduces to ﬁnding the values of π(xj) such that the sum of the utilities
of all edges in the graph is maximized.

Next, given an instance of the SAT problem with

, we use the above
}
{
representation to build the following instance of our problem. More speciﬁcally, consider U and V have
m = 7l + s nodes each with labels

k1, k2, . . . , ks
{

v1, v2, . . . , vl

and

}

y1, . . . , yl, ¯y1, . . . , ¯yl, a1, . . . , al, b1, . . . , bl, z11, . . . , z1l, z21, . . . , z2l, z31, . . . , z3l, k1, k2, . . . , ks

{

}

For the set U , characterize each node u with P (u), where

P (z1i) =

3(s + 1)
3l + 3(s + 1)l

, P (z2i) = P (z3i) =

1
3l + 3(s + 1)l

, P (kj) =

1
3l + 3(s + 1)l

, and

P (yi) = P (¯yi) = P (ai) = P (bi) = 0,

for all i = 1, . . . , l and j = 1, . . . , s. For the set V , characterize each node v with π(v) and u(v), where

u(yi) = u(¯y1) =

1
2l + 4(s + 1)l

, u(ai) = u(bi) =

2(s + 1)
2l + 4(s + 1)l

, u(z1i) = u(z2i) = u(z3i) = 0, and u(kj) = 0,

for all i = 1, . . . , l and j = 1, . . . , s. Then, connect each node u in U to each node v in V and set each edge
weights to w(u, v) = π(v)

c(u, v), where:

(i) c(z1i, yj) = c(z1i, ¯yj) = 0 and c(z2i, yj) = c(z3i, yj) = c(kq, yj) = c(z2i, ¯yj) = c(z3i, ¯yj) = c(kq, ¯yj) = 2 for

−

each i, j = 1, . . . , l and q = 1, . . . , s.

(ii) c(z2i, yj) = 0, c(z2i, aj) = 1

(cid:15) and c(z1i, yj) = c(z3i, yj) = c(kq, yj) = c(z1i, aj) = c(z3i, aj) =

c(kq, aj) = 2 for each i, j = 1, . . . , l and q = 1, . . . , s.

(iii) c(z3i, ¯yj) = 0, c(z3i, bj) = 1

(cid:15) and c(z1i, ¯yj) = c(z2i, ¯yj) = c(kq, ¯yj) = c(z1i, bj) = c(z2i, bj) = c(kq, bj) =

−

−

2 for each i, j = 1, . . . , l and q = 1, . . . , s.

(iv) c(ki, yj) = 0 if the clause ki contains yj, c(ki, ¯yj) = 0 if the clause ki contains ¯yj, and c(ki, aj) =

c(ki, bj) = 2 for all i = 1, . . . , s and j = 1, . . . , l.

.

,
·

) =
·

(v) All remaining edge weights, set c(
Now, note that, in this particular instance, ﬁnding the optimal values of π(v) such that the sum of the utilities
of all edges in the graph is maximized reduces to ﬁrst solving l independent problems, one per pair yj and ¯yj,
since whenever c(u, v) = 2, the edge will never be active, and each optimal π value will be always either zero
or one. Moreover, the maximum utility due to the nodes kz will be always smaller than the utility due to yj
and ¯yj and we can exclude them by the moment. In the following, we ﬁx j and compute the sum of utilities
for all possible values of yj and ¯yj:

∞

17

•

•

For π(yj) = π(¯yj) = 0, the maximum sum of utilities is
1.
For π(yj) = π(¯yj) = 1, the sum of utilities is
For π(yj) = 1

π(¯yj), the maximum sum of utilities is

(3l+3(s+1)l)·(2l+4(s+1)l) whenever π(aj) = π(bj) =

4(s+1)

3(s+1)+2

(3l+3(s+1)l)·(2l+4(s+1)l) for any value of π(aj) and π(bj).

5(s+1)
(3l+3(s+1)l)·(2l+4(s+1)l) .

•

−
π(¯yj) for all
Therefore, the maximum sum of utilities
j = 1, . . . , l and the solution that maximizes the overall utility, including the utility due to the nodes kz, gives
us the solution of the SAT problem. This concludes the proof.

(3+3(s+1))·(2l+4(s+1)l) occurs whenever π(yj) = 1

5(s+1)

−

A.2 Proof of Proposition 2

This proposition can be easily proven by contradiction. More speciﬁcally, assume that all outcome monotonic
policies π are suboptimal, i.e., u(π, γ) < u(π∗, γ), where π∗ is an optimal policy that maximizes utility, and
π∗(xln). Here,
sort the values of the optimal policy in decreasing order, i.e., 1 = π∗(xl1 )
= k, otherwise, the policy π∗ would be outcome monotonic. Now,
note that there is a state xk such that lk
deﬁne the index r = argmink lk
1 < l < lr and
π(cid:48)(xl) = π∗(xl) otherwise. Then, it is easy to see that the policy π(cid:48) has greater or equal utility than π∗ and
lr. If the policy π(cid:48) satisﬁes
it holds that π(cid:48)(xl)
outcome monotonicity, we are done. Otherwise, we repeat the procedure starting from π(cid:48) and continue
building increasingly better policies until we eventually build one that satisﬁes outcome monotonicity. By
construction, this last policy will achieve equal or greater utility than the policy π∗, leading to a contradiction.

= k and build a policy π(cid:48) such that π(cid:48)(xl) = π∗(xlr ) for all r

xt) for all xl, xt such that l, t

π∗(xl2)

π(cid:48)(xt)

P (y

P (y

xl)

⇔

...

≥

≤

≥

≥

≥

≥

−

|

|

A.3 Proof of Theorem 3

We prove this theorem by contradiction. More speciﬁcally, assume that all outcome monotonic binary policies
π are suboptimal, i.e., u(π, γ) < u(π∗, γ), where π∗ is an optimal policy. According to Proposition 2, under
outcome monotonic costs, there is always an optimal outcome monotonic policy. Now, assume there is an
optimal outcome monotonic policy π∗ such that π∗(xi−1) > π∗(xi) > π∗(xi−1)
π∗(xi) <
π∗(xi−1)
c(xi, xi−1) for some i > 1. Moreover, if there are more than one i, consider the one with the
−
highest outcome P (y

xi). Then, we analyze each case separately.

c(xi, xi−1)

−

∨

If π∗(xi−1) > π∗(xi) > π∗(xi−1)

c(xi, xi−1), we can show that the policy π(cid:48) with π(cid:48)(xj) = π∗(xj)

= i
and π(cid:48)(xi) = π∗(xi−1) has greater or equal utility than π∗. More speciﬁcally, consider an individual with
initial feature values xk. Then, it is easy to see that, if k < i, the best response under π∗ and π(cid:48) will be the
i, the best response will be either the same or change to xi under π(cid:48). In the latter case,
same and, if k
xj), where xj is the best response under π∗, otherwise, we would have a
it also holds that P (y
contradiction. Therefore, we can conclude that π(cid:48) provides higher utility than π∗.

xi) > P (y

j
∀

≥

−

|

|

|

−

If π∗(xi) < π∗(xi−1)

c(xi, xi−1), we can show that the policy π(cid:48) with π(cid:48)(xj) = π∗(xj)

= i and
π(cid:48)(xi) = π∗(xi−1)
c(xi, xi−1) has greater or equal utility than π∗. More speciﬁcally, consider an individual
with initial feature values xk and denote the individual’s best response under π∗ as xj. Then, it is easy to
see that the individual’s best response is the same under π∗ and π(cid:48), however, if xj = xi, the term in the
utility corresponding to the individual does increase under π(cid:48). Therefore, we can conclude that π(cid:48) provides
higher utility than π∗.

j
∀

−

In both cases, if the policy π(cid:48) is an outcome monotonic binary policy, we are done, otherwise, we repeat
the procedure starting from the corresponding π(cid:48) and continue building increasingly better policies until
we eventually build one that is an outcome monotonic binary policy. By construction, this last policy will
achieve equal or greater utility than the policy π∗, leading to a contradiction.

A.4 Proof of Proposition 4

Consider an individual with initial features xi such that P (y
xi) > γ. As argued just after Proposition 2,
given an individual with a set of features xi, any outcome monotonic policy always induces a best response
xi), that means, l < i. Then, we just need to prove that the best response
xl such that P (y

P (y

xl)

|

|

≥

|

18

(cid:54)
(cid:54)
(cid:54)
(cid:54)
xl cannot satisfy that P (y
max
∨
case j = i the main idea of the proof is the same.

xl) > P (y
|
π(xk) = π(xk−1)

i, π(xk) = 1

k
{

≤

k

|

|

xj) nor satisfy that P (y

xi), where j =
. Without loss of generality, we assume that j < i, however, in
}

xj) > P (y

P (y

xl)

≥

|

|

|

|

|

xl) > P (y

c(xi, xj) > π(xj−1)

First, assume that P (y

xj). Then, using the additivity and outcome monotonicity of the cost
c(xi, xj) =
and the fact that the policy is an outcome monotonic binary policy, it should hold that π(xj)
π(xj−1)
c(xi, xl). This implies
that xj is a strictly better response for the individual than xl, which is a contradiction. Now, assume that
xi). Then, using the additivity of the cost, the deﬁnition of xj and the fact that
P (y
c(xi, xl) =
xl is the best-response, it should hold that π(xj)
π(xj)

c(xi, xl) = π(xj)
c(xi, xj), which is clearly a contradiction. Therefore, xj is a best-response.

c(xi, xj) < π(xl)

c(xi, xj−1)

c(xi, xj−2)

xj) > P (y

c(xl, xj)

π(xj−2)

≥ · · · ≥

π(xl)

P (y

xl)

−

−

−

≥

−

−

−

−

−

−

≥

|

|

|

Now, consider an individual with initial features xi such that P (y
xl) > P (y
xi). Then π(xl) = π(xj)

xi)
xj) is a contradiction remains as is. Assume that P (y
c(xl, xj) or π(xl) = 0, meaning that π(xj)

|

≤

γ and π(xj)

c(xi, xj). The
xj) >
c(xl, xj) > π(xl)
c(xi, xj) <
c(xi, xj), which is clearly a contradiction.

−

≥

−

|

|

P (y

argument for proving that P (y
P (y
xl)
≥
since π(xj)
π(xl)
As a result, xj is a best-response.

|
c(xl, xj) > π(xj)
π(xj)

−
c(xi, xl)

−

≤

−

|

0. Therefore, it should hold that π(xj)

−
c(xl, xj)

≥

c(xi, xl) = π(xj)

|
−
c(xi, xj)

−

−

−

Now, consider an individual with initial features xi such that P (y
xl) > P (y

|
xj) is a contradiction remains as is. For all xl such that P (y

γ and π(xj) < c(xi, xj). The

xj)

xi)

≤

≥
|
c(xi, xj) < 0
c(xi, xl) < 0. In both cases, because π(xi) = 0, we get that xi is a

c(xl, xj) meaning that π(xl)

c(xi, xl) = π(xj)

−

−

−

xl) > P (y

argument for proving that P (y
P (y
|
or π(xl) = 0 meaning that π(xl)
best-response.

|

|
|
xi) we have π(xl) = π(xj)

−

A.5 Proof of Proposition 6

We prove that ¯u is a denominator of π(xj)
after each step in the iterative algorithm. We
prove this claim by induction. The induction basis is obvious as we initialize the values of π(xj) = 0 for all
xj. For the induction step, suppose that we are going to update π(xj) in our iterative algorithm. According
to the induction hypothesis we know that π(xk)
. Then, it can be shown that the new
}
value of π(xj) will be chosen among the elements of the following set (these are the thresholds that might
change the transfer of masses):

¯u ∈ Z ∀

1, . . . , m

1, . . . , m

∈ {

∈ {

xk

xj

∀

}

πnew(xj)

0
} ∪ {
∈ {

1
} ∪ {

maxk(π(xk)

c(xi, xk)) + c(xi, xj)

xi

0, . . . , m

−
In the above, it is clear that all these possible values are divisible by ¯u, so the new value of π(xj) will be
¯u possible
for all xj
divisible by ¯u too. Then, since 0
∈ {
≤
values for each π(xj), i.e., 0, ¯u, 2¯u, . . . , 1. As a result, there are m1+ 1
¯u diﬀerent decision policies π. Finally,
since the total utility increases after each step, the decision policy π at each step must be diﬀerent. As a
result, the algorithm will terminate after at most m1+ 1

, there are 1 + 1

1 and π(xj )

¯u ∈ Z

1, . . . , m

1 steps.

π(xj)

∈ {

}}

≤

}

|

¯u

−

19

Table 1: Dataset details

Dataset # of samples
credit

30000

Classiﬁer
Logistic Regression

k
100

Accuracy
80.4%

m
3200

γ
0.85

B Additional details on the experiments on real data

B.1 Raw features

Each credit card holder has a label which indicates whether they will default during the next month (y = 0)
or not (y = 1) and the features x are:

Marital status: whether the person is married or single.

Age group: group depending on the person’s age (< 25, 25

39, 40

−

−

59, > 60).

Education level: the level of education the individual has acquired (1-4).

Maximum bill amount over last 6 months

Maximum payment amount over last 6 Months

Months with zero balance over last 6 Months

Months with low spending over last 6 Months

Months with high spending over last 6 Months

Most recent bill amount

Most recent payment amount

Total overdue counts

Total months overdue

•

•

•

•

•

•

•

•

•

•

•

•

We consider all features except marital status, age group and education level to be actionable and, among
the actionable features, we assume that total overdue counts and total months overdue can only increase.

B.2 Further details on the experimental setup for the credit card dataset

Table B.2 summarizes the experimental setup for the credit card dataset, i.e., number of samples, the pair of
classiﬁer - number of clusters k picked through cross-validation, the accuracy achieved by the corresponding
classiﬁer, the resulting number of feature values m and the parameter γ.

20

