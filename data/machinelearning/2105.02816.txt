Semideﬁnite Programming for Community
Detection with Side Information

Mohammad Esmaeili, Hussein Metwaly Saad, Member, IEEE, and Aria Nosratinia, Fellow, IEEE

1

1
2
0
2

y
a
M
6

]
L
M

.
t
a
t
s
[

1
v
6
1
8
2
0
.
5
0
1
2
:
v
i
X
r
a

Abstract—This paper produces an efﬁcient semideﬁnite pro-
gramming (SDP) solution for community detection that incor-
porates non-graph data, which in this context is known as side
information. SDP is an efﬁcient solution for standard community
detection on graphs. We formulate a semi-deﬁnite relaxation for
the maximum likelihood estimation of node labels, subject to
observing both graph and non-graph data. This formulation is
distinct from the SDP solution of standard community detection,
but maintains its desirable properties. We calculate the exact
recovery threshold for three types of non-graph information,
which in this paper are called side information: partially revealed
labels, noisy labels, as well as multiple observations (features)
per node with arbitrary but ﬁnite cardinality. We ﬁnd that SDP
has the same exact recovery threshold in the presence of side
information as maximum likelihood with side information. Thus,
the methods developed herein are computationally efﬁcient as
well as asymptotically accurate for the solution of community
detection in the presence of side information. Simulations show
that the asymptotic results of this paper can also shed light on
the performance of SDP for graphs of modest size.

Index Terms—Community Detection, SDP, Stochastic Block

Model, Censored Block Model, Side Information

I. INTRODUCTION

Detecting communities (or clusters) in graphs is a funda-
mental problem that has many applications, such as ﬁnding
like-minded people in social networks [1], and improving rec-
ommendation systems [2]. Community detection is afﬁliated
with various problems in network science such as network
structure reconstruction [3], networks with dynamic interac-
tions [4], and complex networks [5]. Random graph mod-
els [6], [7] are frequently used in the analysis of community
detection, prominent examples of which include the stochastic
block model [7]–[9] and the censored block model [10], [11].
In the context of these models, community detection recovers
latent node labels (communities) by observing the edges of a
graph.

Community detection utilizes several metrics for residual
error as the size of the graph grows: correlated recovery [12]–
[15] (recovering the hidden community better than random
guessing), weak recovery [16]–[18] (the fraction of misclassi-
ﬁed labels in the graph vanishes with probability converging to
one), and exact recovery [9], [19], [20] (all nodes are classiﬁed
correctly with high probability). Recovery techniques include

M. Esmaeili and A. Nosratinia are with the Department of Electrical
and Computer Engineering, The University of Texas at Dallas, Richardson,
TX 75083-0688, USA, Email: esmaeili@utdallas.edu, aria@utdallas.edu. H.
Metwaly Saad is with the Department of Electrical and Computer Engineering,
Virginia Tech, Blacksburg, VA 24061, USA. Email: husseinm19@vt.edu

This work was supported in part by the grant 2008684 from the National

Science Foundation.

spectral methods [9], [21], belief propagation [22], and SDP
relaxation [23].

Semideﬁnite programming is a computationally efﬁcient
convex optimization technique that has shown its utility in
solving signal processing problems [24], [25]. In the context
of community detection, SDP was introduced in [26], where it
was used for solving a minimum bisection problem, obtaining
a sufﬁcient condition that is not optimal. In [27], a SDP
relaxation was considered for a maximum bisection problem.
For the binary symmetric stochastic block model, [28] showed
that the SDP relaxation of maximum likelihood can achieve the
optimal exact recovery threshold with high probability. These
results were later extended to more general models in [29].
Also, [30] showed the power of SDP for solving a community
detection problem in graphs with a secondary latent variable
for each node.

Community detection on graphs has been widely studied in
part because the graph structure is amenable to analysis and
admits efﬁcient algorithms. In practice, however, the available
information for inference is often not purely graphical. For
instance, in a citation network, beside the names of authors,
there are some additional non-graph information such as
keywords and abstract
that can be used and improve the
performance of community detection algorithms. For illustra-
tion, consider public-domain libraries such as Citeseer and
Pubmed. Citation networks in these libraries have been the
subject of several community detection studies, which can be
augmented by incorporating individual (non-graph) attributes
of the documents that affect the likelihood of community
memberships.

The non-graph data assisting in the solution of graph
problems is called side information. In [31], [32], the effect of
side information on the phase transition of the exact recovery
was studied for the binary symmetric stochastic block model.
In [33]–[35], the effect of side information was studied on the
phase transition of the weak and exact recovery as well as the
phase transition of belief propagation in the single community
stochastic block model. The impact of side information on the
performance of belief propagation was further studied in [34],
[36].

The contribution of this paper is the analysis of the impact of
side information on SDP solutions for community detection.
More speciﬁcally, we study the behavior of the SDP detec-
tion threshold under the exact recovery metric. We consider
graphs following the binary censored block model and the
binary symmetric stochastic block model. We begin with the
development of SDP for partially-revealed labels and noisy
labels, which are easier to grasp and visualize. This builds

 
 
 
 
 
 
intuition for the more general setting, in which we study side
information with multiple features per node, each of which
is a random variable with arbitrary but ﬁnite cardinality. The
former results also facilitate the understanding and interpre-
tation of the latter. Most categories of side information give
rise to a complete quadratic form in the likelihood function,
which presents challenges in the analysis of their semideﬁnite
programming relaxation. Overcoming these challenges is one
of the main technical contributions of the present work.

Simulation results show that the thresholds calculated in this
paper can also shed light on the understanding of the behavior
of SDP in graphs of modest size.

Notation: Matrices and vectors are denoted by capital letters,
and their elements with small letters. I is the identity matrix
and J the all-one matrix. S
0 indicates a positive semideﬁ-
nite matrix and S
||
is the spectral norm, λ2(S) the second smallest eigenvalue
is the inner product. We
(for a symmetric matrix), and
abbreviate [n] ,
. Probabilities are denoted by P(
)
·
and random variables with Bernoulli and Binomial distribution
are indicated by Bern(p) and Binom(n, p), respectively.

0 a matrix with non-negative entries.

· · ·

, n

1,

(cid:23)

≥

·i

h·

S

||

}

{

,

II. SYSTEM MODEL

This paper analyzes community detection in the presence
of a graph observation as well as individual node attributes.
The graphs in this paper follow the binary stochastic block
model and the censored block model, and side information is
in the form of either partially revealed labels, noisy labels, or
an alphabet other than the labels.

This paper considers a fully connected regime, guaranteeing
that exact recovery is possible. Throughout this paper, the
graph adjacency matrix is denoted by G. Node labels are
independent and identically distributed across n, with labels
1. The vector of node labels is denoted by X, and
+1 and
a corresponding vector of side information is denoted by Y .
The log-likelihood of the graph and side information is
X) = log P(G
|

|
i.e., G and Y are independent given X.

X) + log P(Y

log P(G, Y

X),

−

|

A. Binary Censored Block Model

The model consists of an Erd˝os-R´enyi graph with n nodes
and edge probability p = a log n
for a ﬁxed a > 0. The nodes
n
belong to two communities represented by the binary node
labels, which are latent. The entries Gij
of the
weighted adjacency matrix of the graph have a distribution
that depends on the community labels xi and xj as follows:

1, 0, 1

∈ {−

}

Gij

−
−

∼ (

p(1
p(1

ξ)δ+1 + pξδ
ξ)δ

1 + (1
−
1 + pξδ+1 + (1

p)δ0 when xi = xj
= xj
p)δ0 when xi

−
−
−
where δ is Dirac delta function and ξ
2 ] is a constant.
Further, Gii = 0 and Gij = Gji. For all j > i, the edges Gij
are mutually independent conditioned on the node labels. The
log-likelihood of G is

[0, 1

∈

log P(G
|

X) =

1
4

T1X T GX + C1,

(1)

where T1 , log

ξ

1

−
ξ

and C1 is a deterministic scalar.

(cid:16)

(cid:17)

2

B. Binary Symmetric Stochastic Block Model

In this model, if nodes i, j belong to the same community,

Gi,j

∼

Bern(p), otherwise Gij

p = a

log n
n

,

Bern(q) with

∼

q = b

log n
n

,

and a

≥

b > 0. Then the log-likelihood of G is

log P(G
|

X) =

1
4

T1X T GX + C2,

(2)

where T1 , log

p(1
q(1

q)
p)

−
−

and C2 is a deterministic scalar.

(cid:17)
C. Side Information: Partially Revealed Labels

(cid:16)

Partially-revealed side information vector Y consists of
ǫ are equal to the true label

elements that with probability 1
and with probability ǫ take value 0, i.e., are erased.

−

Conditioned on each node label, the corresponding side
information is assumed independent from other labels and
from the graph edges. Thus, the log-likelihood of Y is

log P(Y

|

X) = Y T Y log

1

ǫ

−
ǫ

(cid:18)

(cid:19)

+ n log(ǫ).

(3)

D. Side Information: Noisy Labels

Noisy-label side information vector Y consists of elements
α agree with the true label (yi = x∗i )

that with probability 1
and with probability α are erroneous (yi =
(0, 0.5). Then the log-likelihood of Y is

x∗i ), where α

−

∈

−

log P(Y

X) =

|

1
2

T2X T Y + T2

n
2

+ n log α,

(4)

where T2 , log

(cid:0)

1

α

−
α

.

(cid:1)

E. Side Information: Multiple Variables & Larger Alphabets

In this model, we disengage the cardinality of side in-
formation alphabet from the node latent variable, and also
allow for more side information random variables per node.
This is motivated by practical conditions where the available
non-graph information may be different from the node latent
variable, and there may be multiple types of side information
with varying utility for the inference.

Formally, yi,k is the random variable representing feature k
at node i. Each feature has cardinality Mk that is ﬁnite and
ﬁxed across the graph. We group these variables into a vector
yi of dimension K, representing side information for node i,
and group the vectors into a matrix Y representing all side
information for the graph.1

Without loss of generality, the alphabet of each feature k is
. The posterior probability of

1, . . . , Mk

the set of integers
the features are denoted by

{

}

αk
αk
−

+,mk

,mk

, P(yi,k = mk
, P(yi,k = mk

|

|

xi = 1),
xi =

1),

−

1If vectors yi have unequal dimension, matrix Y will accommodate the

largest vector, producing vacant entries that are defaulted to zero.

6
where mk indexes the alphabet of feature k. Then the log-
likelihood of Y is

Theorem 1. Under the binary censored block model and
partially revealed labels, if

3

a(

1

ξ

ξ)2 + β > 1,

−
then the SDP estimator is asymptotically optimal, i.e., P(
Z ∗)

o(1).

p

p

−

1

≥

−

Z =

b

,mk (cid:19)

Proof. See Appendix A.

1yi,k=mk log(αk

+,mk

αk
−

,mk ),

Theorem 2. Under the binary censored block model and
partially revealed labels, if

log P(Y

X) =

|

n

log P(yi

i=1
X
K

Mk

xi)

|

xi

n

k=1
X
K

mk=1
X
Mk

1yi,k=mk log

(cid:18)

+,mk

αk
αk
−

n

=

1
2

i=1
X
1
2

+

k=1
X
where 1 is the indicator function. Deﬁne

mk=1
X

i=1
X

K

Mk

˜yi ,

k=1
X

mk=1
X

1

yi,k=mk}

{

log

(cid:18)

αk
αk
−

+,mk

,
,mk (cid:19)

and ˜Y , [˜y1, ˜y2, . . . , ˜yn]T . Then the log-likelihood of Y is

log P(Y

X) =

|

1
2

X T ˜Y + C3,

for some constant C3. In the remainder of this paper, side
information thus deﬁned is referred to as general side infor-
mation.

III. DETECTION VIA SDP

For organizational convenience, the main results of the

paper are concentrated in this section.

For the formulation of SDP, we utilize the additional
T .

variables Z , XX T and W , Y Y T . Also, let Z ∗ , X ∗X ∗

A. Censored Block Model with Partially Revealed Labels

Combining (1) and (3), the maximum likelihood detector is

ˆX =arg max

X T GX

X

subject to xi

,

1

i
X T Y = Y T Y,

∈ {±

}

[n]

∈

where the constraint X T Y = Y T Y ensures that detected val-
ues agree with available side information. This is a non-convex
problem, therefore we consider a convex relaxation [19], [37].
Y T Y with
Replacing xi
Z, W
h

with Zii = 1, and X T Y =

1
∈ {±
= (Y T Y )2,

±

}

i

Z =arg max

Z

Z, G
i
h

subject to Z = XX T

b

Zii = 1,
Z, W
h
By relaxing the rank-one constraint introduced via Z, we
obtain the following SDP relaxation:

∈
= (Y T Y )2.

[n]

(7)

i

i

Z =arg max

Z

Z, G
i
h

subject to Z

0

b

(cid:23)
Zii = 1,
Z, W
h

i

i

[n]

∈
= (Y T Y )2.

Let β , limn

→∞ −

log ǫ
log n , where β

0.

≥

a(

1

ξ

−

−

ξ)2 + β < 1,

then for any sequence of estimators
n

p

p

.
→ ∞

Proof. See Appendix B.

Zn, P(

Zn = Z ∗)

0 as

→

b

b

(5)

B. Censored Block Model with Noisy Labels

Combining (1) and (4), the maximum likelihood detector is

ˆX =arg max

X

T1X T GX + 2T2X T Y

subject to xi

∈ {±

1

,

}

[n].

i

∈

(9)

Then (9) is equivalent to

Z =arg max

Z,X

G, Z

T1h
subject to Z = XX T

i

b

+ 2T2X T Y

Relaxing the rank-one constraint, using

Zii = 1,

[n].

i

∈

(10)

XX T

Z

−

0

(cid:23)

⇔

(cid:20)
yields the SDP relaxation of (10):

1 X T
X Z

0,

(cid:23)

(cid:21)

(6)

Z =arg max

Z,X

T1h

+ 2T2X T Y

b

subject to

G, Z

i
1 X T
X Z
(cid:20)
Zii = 1,

0

[n].

(cid:23)
(cid:21)
i

∈

(11)

Let β , limn
deﬁne

→∞

T2
log n , where β

≥

0. Also, for convenience

η(a, β) , a

γ
T1

+

β
2T1

−

log

(1

−
ξ(γ

ξ)(γ + β)

,
β) !

−

where γ ,

β2 + 4ξ(1

ξ)a2T 2
1 .

−

Theorem 3. Under the binary censored block model and noisy
labels, if

p

η(a, β) > 1 when 0
when β
β > 1

(

≤
≥

β < aT1(1
aT1(1

−
2ξ)

−

2ξ)

then the SDP estimator is asymptotically optimal, i.e., P(
Z ∗)

o(1).

1

Z =

(8)

≥

−

Proof. See Appendix C.

b

 
4

D. Stochastic Block Model with Partially Revealed Labels

Similar to the binary censored block model with partially
revealed labels, by combining (2) and (3), the SDP relaxation
is

Theorem 4. Under the binary censored block model and noisy
labels, if

η(a, β) < 1 when 0
when β
β < 1

≤
≥
then for any sequence of estimators
n

(

.
→ ∞

Proof. See Appendix D.

β < aT1(1
aT1(1

−
2ξ)

2ξ)

−
Zn, P(
Zn = Z ∗)

C. Censored Block Model with General Side Information

Combining (1) and (5), the SDP relaxation is

0 as

→

Z =arg max

Z

Z, G
i
h

b

b

b

subject to Z

0

(cid:23)
Zii = 1,
J, Z
i
h
Z, W
h

i

[n]

i

∈

= 0
= (Y T Y )2,

(13)

Z =arg max

Z,X

T1h

b

subject to

+ 2X T ˜Y

G, Z

i
1 X T
X Z
(cid:20)
Zii = 1,

0

[n].

(cid:23)
(cid:21)
i

∈

where the constraint
communities.

J, Z
h

i

= 0 arises from two equal-sized

Theorem 7. Under the binary symmetric stochastic block
model and partially revealed labels, if

(12)

The log-likelihoods and the log-likelihood-ratio of side infor-
mation, combined over all features, are as follows:

2

√b

+ 2β > 2,

√a

−

K

f1(n) ,

log

αk
αk
−

+,mk

,

,mk

k=1
X
K

k=1
X
K

f2(n) ,

f3(n) ,

k=1
X

(cid:1)
then the SDP estimator is asymptotically optimal, i.e., P(
Z ∗)

o(1).

1

(cid:0)

Z =

≥

−

b

log αk

+,mk ,

log αk
−

,mk .

Proof. See Appendix G.

Remark 1. The converse is given by [31, Theorem 3].

Two exponential orders will feature prominently in the follow-
ing results and proofs:

E. Stochastic Block Model with Noisy Labels

Similar to the binary censored block model with noisy

labels, by combining (2) and (4), the SDP relaxation is

β1 , lim
n
→∞

β , lim
→∞ −
n

,

f1(n)
log n
max(f2(n), f3(n))
log n

.

Z =arg max

Z,X

T1h

+ 2T2X T Y

b

subject to

G, Z

i
1 X T
X Z
(cid:20)
Zii = 1,
J, Z
h

i

(cid:23)
(cid:21)
i
= 0.

∈

0

[n]

(14)

Although the deﬁnition of β varies in the context of different
models, its role remains the same. In each case, β is a param-
eter representing the asymptotic quality of side information.2

Theorem 5. Under the binary censored block model and
general side information, if
β1|
|
+ β > 1

) + β > 1 when
when

aT1(1
> aT1(1

2ξ)
2ξ)

β1| ≤
β1|

|
|

η(a,
β1|

|

(

−
−

then the SDP estimator is asymptotically optimal, i.e., P(
Z ∗)

o(1).

1

Z =

≥

−

Proof. See Appendix E.

For convenience let

η(a, b, β) , a + b
2

+

β
2 −

γ
T1

+

β
2T1

log

γ + β
β
γ

−

,
(cid:19)

(cid:18)

where γ ,

β2 + abT 2
1 .

b

Theorem 8. Under the binary symmetric stochastic block
model and noisy label side information, if

p

Theorem 6. Under the binary censored block model and
general side information, if
β1|
|
+ β < 1

) + β < 1 when
when

aT1(1
> aT1(1

|
|
then for any sequence of estimators

−
−
Zn = Z ∗)

β1| ≤
β1|
Zn, P(

η(a,
β1|

2ξ)
2ξ)

(

|

0.

→

Proof. See Appendix F.

b

b

2In each case, β is proportional to the exponential order of the likelihood

function.

η(a, b, β) > 1 when 0
when β
β > 1

(

≤
≥

β < T1
T1
2 (a

2 (a
b)

−

b)

−

then the SDP estimator is asymptotically optimal, i.e., P(
Z ∗)

o(1).

1

Z =

≥

−

Proof. See Appendix H.

Remark 2. The converse is given by [31, Theorem 2].

b

F. Stochastic Block Model with General Side Information

Z,X

G, Z

+ 2X T ˜Y

Z =arg max

Similar to the binary censored block model with general side
information, by combining (2) and (5), the SDP relaxation is
T1h

i
1 X T
X Z
(cid:20)
Zii = 1,
J, Z
h
Theorem 9. Under the binary symmetric stochastic block
model and general side information, if

(cid:23)
(cid:21)
i
= 0.

subject to

(15)

[n]

∈

b

0

i

β1|
+ β > 1

) + β > 1 when
when

|

η(a, b,
β1|

|

(

T1
> T1

β1| ≤
β1|

|
|

(a

(a

b)

b)

−
2

−
2

then the SDP estimator is asymptotically optimal, i.e., P(
Z ∗)

o(1).

1

Z =

≥

−

Proof. See Appendix I.

Remark 3. The converse is given by [31, Theorem 5].

IV. NUMERICAL RESULTS

This section produces numerical simulations that shed light
on the domain of applicability of the asymptotic results
obtained earlier in the paper3.

Table I shows the misclassiﬁcation error probability of
the SDP estimators (8) and (13) with partially revealed side
information. Under the binary stochastic block model with
a = 3 and b = 1, when the side information β = 0.8,
error probability diminishes with n as predicted by earlier
asymptotic results. For these parameters, η = 1.1 > 1, and
exact recovery is possible based on the theoretical results.
When β = 0.2, then η = 0.5 < 1 which does not fall in the
asymptotic perfect recovery regime, the misclassiﬁcation error
probability is much higher. Under the binary censored block
model with a = 1 and ξ = 0.2, when the side information
β = 1, error probability diminishes with n. For these values,
η = 1.2 > 1, and exact recovery is possible based on the
theoretical results. When β = 0.3, the misclassiﬁcation error
probability is much higher. For this value of β, η = 0.5 < 1
which means exact recovery is not asymptotically possible.

Table II shows the misclassiﬁcation error probability of
the SDP estimators (11) and (14) with noisy labels side
information. Under the stochastic block model with a = 4 and
b = 1, when the side information β = 1, then η = 1.1 > 1
and the error probability diminishes with n as predicted by
earlier theoretical results. When β = 0.2, then η = 0.6 < 1
which does not fall in the asymptotic perfect recovery regime.
For this case the misclassiﬁcation error is much higher. Under
the censored block model with a = 4 and ξ = 0.25, when
then η = 1.2 > 1 and
the side information β = 1.1,
the error probability diminishes with n. When β = 0.1,
then η = 0.6 < 1 which means that exact recovery is not
possible asymptotically. For this value of β and a ﬁnite n, the
misclassiﬁcation error is not negligible.

5

For comparison, Table III shows the misclassiﬁcation error
probability of the SDP estimator without side information, i.e.,
β = 0. Under the binary stochastic block model, when a = 3
(a = 4) and b = 1, it is seen that the error probability increases
in comparison with the corresponding error probability in
Table I (Table II) where side information is available. Also,
under the binary censored block model, when a = 1 and
ξ = 0.2 (a = 4 and ξ = 0.25), it is seen that the error
probability increases in comparison with the corresponding
error probability in Table I (Table II) where side information
is available.

Using standard arguments form numerical linear algebra,
the computational complexity of the algorithms in this paper
are on the order O(mn3 + m2n2), where n is the number
of nodes in the graph, and m is a small constant, typically
between 2 to 4, indicating assumptions of the problem that
manifest as constraints in the optimization.

b

V. CONCLUSION

This paper calculated the exact recovery threshold for
community detection under SDP with several types of side
information. Among other insights, our results indicate that in
the presence of side information, the exact recovery threshold
for SDP and for maximum likelihood detection remain identi-
cal. We anticipate that models and methods of this paper may
be further extended to better match the statistics of real-world
graph data.

APPENDIX A
PROOF OF THEOREM 1

We begin by stating sufﬁcient conditions for the optimum

solution of (8) matching the true labels X ∗.

Lemma 1. For the optimization problem (8), consider the
Lagrange multipliers

µ∗, D∗ = diag(d∗i ), S∗.

If we have

G,

−

S∗ = D∗ + µ∗W
S∗
0,
λ2(S∗) > 0,
S∗X ∗ = 0,

(cid:23)

then (µ∗, D∗, S∗) is the dual optimal solution and
T is the unique primal optimal solution of (8).
X ∗X ∗

Z =

Proof. The Lagrangian of (8) is given by

b

3The code is available online at https://github.com/mohammadesmaeili/Community-Detection-by-SDP

−

L(Z, S, D, µ) =

G, Z
h

+

S, Z
h
i
W, Z
µ(
h

i −

D, Z
i − h
(Y T Y )2),

I
i

−

6

TABLE I
SDP WITH PARTIALLY REVEALED LABELS.

TABLE II
SDP WITH NOISY LABELS

TABLE III
SDP WITHOUT SIDE INFORMATION.

a

3
3
3
3
3
3
3
3
3
3
1
1
1
1
1
1
1
1
1
1

b

1
1
1
1
1
1
1
1
1
1
-
-
-
-
-
-
-
-
-
-

ξ

-
-
-
-
-
-
-
-
-
-
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2

β

0.2
0.2
0.2
0.2
0.2
0.8
0.8
0.8
0.8
0.8
0.3
0.3
0.3
0.3
0.3
1
1
1
1
1

n

100
200
300
400
500
100
200
300
400
500
100
200
300
400
500
100
200
300
400
500

Error Probability
4.1 × 10−2
3.1 × 10−2
2.5 × 10−2
2.2 × 10−2
1.9 × 10−2
5.0 × 10−4
3.2 × 10−4
1.6 × 10−4
1.2 × 10−4
9.3 × 10−5
4.1 × 10−2
2.9 × 10−2
2.2 × 10−2
1.9 × 10−2
1.7 × 10−2
1.1 × 10−3
4.2 × 10−4
2.7 × 10−4
2.1 × 10−4
1.5 × 10−4

a

4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4

b

1
1
1
1
1
1
1
1
1
1
-
-
-
-
-
-
-
-
-
-

ξ

-
-
-
-
-
-
-
-
-
-
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25

β

0.2
0.2
0.2
0.2
0.2
1
1
1
1
1
0.1
0.1
0.1
0.1
0.1
1.1
1.1
1.1
1.1
1.1

n

100
200
300
400
500
100
200
300
400
500
100
200
300
400
500
100
200
300
400
500

Error Probability
2.0 × 10−2
1.5 × 10−2
1.3 × 10−2
1.1 × 10−2
1.0 × 10−2
1.1 × 10−3
7.4 × 10−4
3.0 × 10−5
2.7 × 10−5
2.2 × 10−5
2.9 × 10−2
1.8 × 10−2
1.4 × 10−2
1.2 × 10−2
1.0 × 10−2
2.7 × 10−3
1.0 × 10−3
6.2 × 10−4
4.1 × 10−4
3.3 × 10−4

a

3
3
3
3
3
4
4
4
4
4
1
1
1
1
1
4
4
4
4
4

b

1
1
1
1
1
1
1
1
1
1
-
-
-
-
-
-
-
-
-
-

ξ

-
-
-
-
-
-
-
-
-
-
0.2
0.2
0.2
0.2
0.2
0.25
0.25
0.25
0.25
0.25

n

100
200
300
400
500
100
200
300
400
500
100
200
300
400
500
100
200
300
400
500

Error Probability
1.4 × 10−1
1.2 × 10−1
1.1 × 10−1
9.8 × 10−2
9.1 × 10−2
2.3 × 10−2
1.7 × 10−2
1.6 × 10−2
1.3 × 10−2
1.2 × 10−2
2.9 × 10−1
2.5 × 10−1
2.2 × 10−1
2.1 × 10−1
1.9 × 10−1
3.0 × 10−2
1.9 × 10−2
1.5 × 10−2
1.2 × 10−2
1.1 × 10−2

where S
multipliers. For any Z that satisﬁes the constraints in (8),

0, D = diag(di), and µ

(cid:23)

∈

R are Lagrange

G, Z
h

i

L(Z, S∗, D∗, µ∗)
D∗, I
i
h

+ µ∗(Y T Y )2

D∗, Z ∗
h
G + S∗
h

i

−

+ µ∗(Y T Y )2
µ∗W, Z ∗

i

(a)

≤
=
(b)
=

=
(c)
=

+ µ∗(Y T Y )2

S∗, Z
h

G, Z ∗
h

∈
T S∗X ∗ = 0 and

,
i
0, (b) holds because
where (a) holds because
Zii = 1 for all i
=
= (Y T Y )2. Therefore, Z ∗ is a
X ∗
primal optimal solution. Now, we will establish the uniqueness
of the optimal solution. Assume ˜Z is another primal optimal
solution. Then
S∗, ˜Z
h

i ≥
[n], and (c) holds because
W, Z ∗
h

S∗, Z ∗
h

=

i

i

i

D∗
−
h
D∗, ˜Z
h

D∗, Z ∗
h
D∗
−
h
S∗, Z ∗
h

i

=
(a)
=

=

=

i
+ µ∗

G + µ∗W, ˜Z
G, ˜Z

i − h

i
G, Z ∗
i
G + µ∗W, Z ∗

i − h

i

= 0,

W, ˜Z
h
+ µ∗

i
W, Z ∗
h

i

=

W, Z ∗
h

=
i
, and Z ∗ii = ˜Zii = 1 for all i
G, ˜Z
i
h
0 and S∗

= (Y T Y )2,
where (a) holds because
[n].
G, Z ∗
h
i
Since ˜Z
0 while its second smallest eigenvalue
(cid:23)
λ2(S∗) is positive, ˜Z must be a multiple of Z ∗. Also, since
˜Zii = Z ∗ii = 1 for all i

[n], we have ˜Z = Z ∗.

W, ˜Z
h

(cid:23)

∈

i

∈

We now show that S∗ = D∗ + µ∗W
conditions in Lemma 1 with probability 1

G satisﬁes other
o(1). Let

n

d∗i =

Gijx∗j x∗i −

µ∗

yiyjx∗j x∗i .

(16)

j=1
X
µ∗W X ∗ and based on the deﬁnition
Then D∗X ∗ = GX ∗
of S∗ in Lemma 1, S∗ satisﬁes the condition S∗X ∗ = 0. It

j=1
X

−

−
−

n

remains to show that S∗
1

(cid:23)
o(1). In other words, we need to show that

0 and λ2(S∗) > 0 with probability

−

P

V
(cid:18)

inf
X ∗,
V
k

⊥

k

V T S∗V > 0

=1

(cid:19)

1

−

≥

o(1),

(17)

where V is a vector of length n. Since for the binary censored
block model

E[G] = p(1

2ξ)(X ∗X ∗

T

I),

−

−

it follows that for any V such that V T X ∗ = 0 and

V

(18)

= 1,

k
k
E[G])V

V T S∗V =V T D∗V + µ∗V T W V

+ p(1

2ξ).

−

V T (G

−

−

Lemma 2.
c′ > 0 such that for any n
probability at least 1
n−

≥
c.

[29, Thoerem 9] For any c > 0, there exists
c′√log n with

E[G]

1,

G

k

−

k ≤

−

Lemma 3.

[38, Lemma 3]

P

V T W V

log n

≥

ǫ
1
−
√log n

≤

= n−

1

2 +o(1).

(cid:16)
Since V T D∗V

[n] d∗i and V T (G
−
, applying Lemmas 2 and 3 implies that with probability

E[G])V

≤ k

G

≥

−

p
mini
∈

(cid:17)

≥

min
[n]
i
∈

d∗i + (µ∗

′

c

)

−

log n + p(1

2ξ).

(19)

−

Lemma 4. Consider a sequence of i.i.d. random variables
p)δ0.
S1, . . . , Sm
1+(1
{
−
−
ǫ), µ∗ < 0, and δ = log n
log logn . Then
Let U

with distribution p(1

}
Binom(n

ξ)δ+1+pξδ

1, 1

−

−

−

p

E[G]
1

k
o(1),

−
V T S∗V

∼

P

P

1

n

−

i=1
X
n
1
−

Si

δ

≤

! ≤

n−

a(√1

−

ξ

−

√ξ)2+o(1),

Si

µ∗U

δ + µ∗

! ≤
Proof. It follows from Chernoff bound.

i=1
X

−

≤

ǫn[log ǫ+o(1)].

 
 
It can be shown that

n
j=1 Gij x∗i x∗j in (16) is equal in

distribution to

P

δ) =P

P(d∗i ≤

1

n
i=1 Si in Lemma 4. Then
−
n

P

Gij x∗i x∗j ≤

δ

ǫ

!

n

j=1
X

+ P

j=1
X
ξ

a(√1

−

−

ǫn−

Gij x∗i x∗j −

µ∗Zi

≤

δ + µ∗

(1

!

ǫ)

−

√ξ)2+o(1) + (1
√ξ)2+o(1)

ξ

ǫ)ǫn
(cid:0)

−
log n,

log ǫ+o(1)

(cid:1)

log ǫ
log n −

a(√1

−

−

≤
=e

(cid:0)
Binom(n

1, 1

ǫ) and (1
−
. Recall that β , limn

−

(cid:1)

→ ∞

ǫ)ǫn(log ǫ+o(1))
log ǫ
log n , where

−
→∞ −

β

n−

−

a(√1

−

ξ

−

√ξ)2+o(1).

where Zi
∼
vanishes as n
β
0. Then

≥

P(d∗i ≤

δ)

≤
Using the union bound,

log n
log log n

(cid:19)

P

(cid:18)

d∗i ≥
min
[n]
i
∈
When β + a(√1
−
with probability 1
β + a(√1

ξ

−

−
log n
log log n

V T S∗V

≥

n1

−

β

−

a(√1

−

ξ

−

√ξ)2+o(1).

1

−

≥

ξ

log n
√ξ)2 > 1, mini
log log n holds
−
∈
o(1). Combining this result with (19), if

[n] d∗i ≥

−

√ξ)2 > 1, then with probability 1

+ (µ∗

′

c

)

−

log n + p(1

−

o(1),

−

2ξ) > 0,

which concludes Theorem 1.

p

APPENDIX B
PROOF OF THEOREM 2
Since the prior distribution of X ∗ is uniform, among all
estimators, the maximum likelihood estimator minimizes the
average error probability. Therefore, it sufﬁces to show that
with high probability the maximum likelihood estimator fails.
Let

n

F ,

i
∈

min
[n],yi=0

Gij x∗j x∗i ≤ −

.
(cid:27)
P(F ). If we show that P(F)

(cid:26)
Then P(ML Fails)
1, the
maximum likelihood estimator fails. Let H denote the set of
nodes and e(i, H) denote the number of edges
ﬁrst
between node i and nodes in the set H. Then

n
log2 n ⌋

j=1
X

→

≥

1

⌊

min
[n],yi=0

i
∈

n

j=1
X

Gij x∗j x∗i ≤

min
H,yi=0

i
∈

min
H,yi=0

≤

i
∈

Deﬁne the events

Hc
j
X
∈

n

Gij x∗j x∗i

j=1
X

H,yi=0

i
∈

E1 ,

E2 ,

max
H,yi=0

i
∈

(cid:26)

e(i, H)

min
H,yi=0

i
∈

(
E1 ∩

Hc
j
X
∈

δ

1

≤

−

,
(cid:27)
Gijx∗j x∗i ≤ −

δ

.

)

Notice that F
likelihood estimator fails, it sufﬁces to show that P(E1)
and P(E2)

E2. Hence, to show that the maximum
1

→

1.

⊃

→

7

Binom(n, p), for any

∼

Lemma 5.
1, P(S
r

[39, Lemma 5] When S
e
r

rnp)

np.

e−

rnp

≥
Since e(i, H)

≥

≤

Lemma 5 that

∼

Binom
(cid:1)
(cid:0)

|
(cid:0)

, a log n
n

H

|

,

it

follows from

(cid:1)

P

e(i, H)

δ

1, yi = 0

1

ǫ

(cid:0)
≤

−
≥
log2 n
ae log log n −
Using the union bound, P(E1)
P(E1)

log n
(cid:1)
ae

1.

(cid:19)

(cid:18)

−

→

log n
log log n

a
log n

e−

≤

ǫn−

2+o(1).

1

−

≥

ǫn−

1+o(1). Thus,

Lemma 6. [29, Lemma 8] Consider a sequence of i.i.d. ran-
dom variables
ξ)δ+1 +
log log n .
pξδ
1 + (1
−
Then

with distribution p(1
n = o(n). Let f (n) = log n

S1, . . . , Sm
}
{
p)δ0, where m
−

−

−

P

m

i=1
X

Si

≤ −

f (n)

! ≥

n−

a(√1

−

ξ

−

√ξ)2+o(1).

Using Lemma 6 and since

tually independent,

Hc Gij x∗j x∗i }

i
∈

j

∈

{

H are mu-

P

P(E2) = 1

−

1
H "

P

−

Gij x∗j x∗i ≤ −

δ, yi = 0

i
Y
∈
1

−

1

≥

−

j

(cid:18) X
Hc

∈
ξ

−

a(√1

ǫn−

√ξ)2+o(1)

−

H

|

|.

h
Since β = limn
→∞ −

log ǫ
log n , it follows from (20) that

i

(cid:19)#

(20)

β

n−

−

a(√1

−

ξ

−

√ξ)2+o(1)

H

|

|

P(E2)

≥

≥

1

−

1

1

−
h
exp

n1

−

β

−

a(√1

i
√ξ)2+o(1)

ξ

−

,

(21)

−

−

−
ex. From (21), if a(√1

(cid:16)

≤
1. Therefore, P(F )

ξ

−

−

→

using 1+x
P(E2)

→

(cid:17)
√ξ)2 +β < 1, then

1 and Theorem 2 follows.

APPENDIX C
PROOF OF THEOREM 3

We begin by deriving sufﬁcient conditions for the SDP

estimator to produce the true labels X ∗.

Lemma 7. For the optimization problem (11), consider the
Lagrange multipliers

D∗ = diag(d∗i ),

S∗ ,

T
S∗A S∗
B
S∗B

S∗C (cid:21)

.

(cid:20)

T2Y,

S∗A = T2Y T X ∗,
S∗B =
−
S∗C = D∗
0,
S∗
λ2(S∗) > 0,
S∗[1, X ∗

T ]T = 0

−

(cid:23)

T1G,

then (D∗, S∗) is the dual optimal solution and
is the unique primal optimal solution of (11).

Z = X ∗X ∗

T

b

Gij x∗j x∗i + max

e(i, H),

If we have

 
 
 
Proof. Deﬁne

where the last inequality holds because

8

H ,

1 X T
X Z

(cid:20)

.

(cid:21)

The Lagrangian of (11) is given by

U T (G

G, Z

L(Z, X, S, D) = T1h
where S
any Z that satisﬁes the constraints in (11),

I
,
i
0 and D = diag(di) are Lagrange multipliers. For

+2T2h

S, H
h

D, Z

Y, X

i−h

+

(cid:23)

−

i

i

G, Z

T1h

i

+ 2T2h

Y, X

i

L(Z, X, S∗, D∗)
D∗, I
i
h

+ S∗A

(a)

≤
=
(b)
=

i

2

(d)

i − h

=
(c)

G, Z ∗

S∗B, X ∗

i
S∗B, X ∗

D∗, Z ∗
h
S∗C + T1G, Z ∗
h
= T1h
= T1h
S∗, H
i ≥
h
[n] and S∗A =

i − h
S∗B, X ∗
i −
h
+ 2T2h
i
0, (b) holds because
T
B X ∗, (c) holds
−
T2Y .
T is a primal optimal solution. Now,

i
Y, X ∗

G, Z ∗

S∗

,
i

−

S∗CX ∗, and (d) holds because S∗B =

where (a) holds because
Zii = 1 for all i
because S∗B =
Therefore, Z ∗ = X ∗X ∗
assume ˜Z is another optimal solution.

−

∈

S∗B, ˜X
h

= S∗A + 2

S∗, ˜H
i
h
(a)
S∗B, X ∗
= S∗A + 2
h
S∗, H ∗
= 0
=
i
h

i

i
+

+

D∗
h
−
D∗, Z ∗
h

T1G, ˜Z
T1h

i −

i
G, Z ∗

i

∈

where (a) holds because
=
i
S∗B, ˜X
S∗B, X ∗
[n], and
all i
(cid:23)
h
h
0 while its second smallest eigenvalue λ2(S∗) is positive, ˜H
must be a multiple of H ∗. Also, since ˜Zii = Z ∗ii = 1 for all
i

, Z ∗ii = ˜Zii = 1 for
G, ˜Z
h
i
. Since ˜H
i

[n], we have ˜H = H ∗.

G, Z ∗
h
=
i

∈
We now show that S∗ deﬁned by S∗A, S∗B, and S∗C satisﬁes
o(1). Let

other conditions in Lemma 7 with probability 1

0 and S∗

(cid:23)

n

−

d∗i = T1

Gij x∗j x∗i + T2yix∗i .

(22)

j=1
X

Then D∗X ∗ = T1GX ∗ + T2Y and based on the deﬁnitions
of S∗A, S∗B, and S∗C in Lemma 7, S∗ satisﬁes the condition
T ]T = 0. It remains to show that S∗
S∗[1, X ∗
0 and
o(1). In other words, we
λ2(S∗) > 0 with probability 1
need to show that

(cid:23)

−

P

inf

[1,X ∗T ]T ,

V

k

k

V
(cid:18)

⊥

V T S∗V > 0

=1

(cid:19)

1

−

≥

o(1),

(23)

where V is a vector of length n + 1. Let V , [v, U T ]T , where
v is a scalar and U , [u1, u2,
, un]T . For any V such that
V T [1, X ∗
T ]T = 0 and
V T S∗V = v2S∗A −
v2)
d∗i −
(1
(cid:20)

2T2vU T Y + U T D∗U

= 1, we have

T1U T GU

+ T1p(1

T1k

E[G]

2ξ)

· · ·

G

−

≥

−

−

V

(cid:21)

k

k

min
[n]
i
∈
T2Y T X ∗

+ v2

(cid:20)

k
v2)

−
n(1
−
v
|

|

2T2

−

p

U T D∗U

(1

−

≥

E[G])U

(1

−
vU T Y

≤
v

d∗i ,

v2) min
[n]
i
∈
v2)
k
−
v2).

−
n(1

G

E[G]

,

k

|
p
Lemma 8. Under the noisy label side information with noise
parameter α,

≤ |

−

n

P

i=1
X

x∗i yi

≤

√n log n

e

! ≤

n

log

2√α(1

−

α)

+o(1)

(cid:16)

(cid:0)

(cid:1)

.

(cid:17)

Proof. It follows from Chernoff bound.

Using Lemma 8, it can be shown that with probability

converging to one,

n
i=1 x∗i yi

√n log n. Thus,

≥

v2

(cid:20)

T2√n log n

P
2T2

n(1
−
v
|
. Applying Lemma 2,

p

−

|

as n

→ ∞

v2)

T1p(1

−

−

2ξ)

(cid:21)

0,

≥

V T S∗V

(1

−

≥

v2)

(cid:16)

min
[n]
i
∈

d∗i −

′

T1c

log n + T1p(1

p

−

2ξ)

.

(cid:17)
(25)

−

{

1 + (1

S1, . . . , Sm

−
the distribution depend on n via p1 = ρ1

Lemma 9. Consider a sequence f (n), and for each n a
sequence of i.i.d. random variables
with distri-
p2)δ0, where the parameters
bution p1δ+1 + p2δ
p1 −
log n
n , and
of
for some positive constants ρ1, ρ2. We assume
p2 = ρ2
n = o(n), where in the sequel the dependence of m
m(n)
f (n)
on n is implicit. Deﬁne ω , limn
log n . For sufﬁciently
large n, when ω < ρ1 −

log n
n

ρ2,

→∞

−

}

P

Si

≤

f (n)

! ≤

∗

η

n−

+o(1),

(26)

i=1
X
and when ω > ρ1 −

ρ2,

m

m

P

Si

≥

f (n)

!

i=1
X

= n−

∗

η

+o(1),

(27)

where η∗ = ρ1 + ρ2 −

ω2 + 4ρ1ρ2.

γ∗ + ω

2 log

∗
ρ2(γ
ρ1(γ∗

+ω)
ω)

−

(cid:17)

(cid:16)

and γ∗ =

Proof. Inequality (26) is derived by applying Chernoff bound.
p
Equality (27) is obtained by a sandwich argument on the
probability: an upper bound derived via Chernoff bound, and
a lower bound from [31, Lemma 15].

It follows from (22) that

P(d∗i ≤

δ) =P

n

j=1
X

Gijx∗i x∗j ≤

δ

T2
−
T1 !

(1

α)

−

n

+ P

j=1
X

Gij x∗i x∗j ≤

δ + T2

T1 !

α,

T1p(1

−

−

2ξ)

, (24)

(cid:21)

n
j=1 Gij x∗i x∗j is equal in distribution to

where
Lemma 9 with p1 = p(1

ξ) and p2 = pξ.

P

−

n
1
i=1 Si in
−

P

 
 
 
 
 
Recall that β , limn

T2
log n , where β

bound mini
∈
It follows from Lemma 9 that

→∞
[n] d∗i under the condition 0

≤

0. First, we
2ξ).

β < aT1(1

≥

−

9

Also, it can be shown that at β∗, γ∗ = β
1
−
β∗ = (1
which implies that η(a, β) > 1 when β > 1.

2ξ)aT1. Substituting in (29) leads to η(a, β)

2ξ . This implies that
0,

≥

−

−

β

∗

P

P

n

j=1
X
n

j=1
X

Gijx∗i x∗j ≤

Gijx∗i x∗j ≤

δ

T2
−
T1 ! ≤

δ + T2

T1 ! ≤

n−

η(a,β)+o(1),

n−

η(a,β)+β+o(1).

Then

P(d∗i ≤

δ)

n−

≤
= n−

η(a,β)+o(1)(1
η(a,β)+o(1).

−

α) + n−

η(a,β)+β+o(1)α

Using the union bound,

P

(cid:18)

min
[n]
i
∈

d∗i ≥

log n
log log n

1

−

≥

(cid:19)

n1

−

η(a,β)+o(1).

When η(a, β) > 1, it follows mini
∈
probability 1
can replace min d∗i
o(1):
probability 1

log n
log log n with
o(1). Thus, as long as η(a, β) > 1, we
log n
log log n and obtain, with

[n] d∗i ≥

in (25) with

−

−

When β > 1, using Lemma 10, it follows mini
∈
log n
log log n with probability 1
1, with probability 1

[n] d∗i ≥
o(1). Substituting in (25), if β >

o(1) we obtain:

−

−

V T S∗V

(1

≥
>0,

−

v2)

(cid:16)

log n

log log n −

T1c′

log n + T1p(1

p

−

2ξ)

(cid:17)

which concludes the second part of Theorem 3.

APPENDIX D
PROOF OF THEOREM 4

Since the prior distribution of X ∗ is uniform, among all
estimators, the maximum likelihood estimator minimizes the
average error probability. Therefore, we only need to show
that with high probability the maximum likelihood estimator
fails. Let

−

2ξ)

(cid:19)

F ,

min
[n]
i
∈

(

n

T1

Gij x∗j x∗i + T2x∗i yi

≤ −

T1

.

)

(cid:18)

Then P(ML Fails)

j=1
X
P(F ). Let H denote the set of ﬁrst
nodes and e(i, H) denote the number of edges be-
[n]. It can be shown

n
log2 n ⌋
⌊
tween node i and nodes in the set H
that

≥

⊂

(cid:19)

V T S∗V

v2)

(cid:18)

−

(1

≥
>0,

log n

log log n −

T1c′

log n + T1p(1

p

which concludes the ﬁrst part of Theorem 3.

We now bound mini
∈
2ξ). It follows from Lemma 9 that

[n] d∗i under the condition β >

aT1(1

−

P

P

n

j=1
X
n

j=1
X

Gij x∗i x∗j ≤

δ

T2
−
T1 ! ≤

n−

η(a,β)+o(1),

Gij x∗i x∗j ≤

δ + T2

T1 ! ≤

1.

Then

P(d∗i ≤

δ)

≤

n−

η(a,β)+o(1) + n−

β+o(1),

where α = n−

β+o(1). Using the union bound,

P

min
[n]
i
∈

d∗i ≥

δ

1

≥

−

(cid:16)

(cid:16)

(cid:17)
Lemma 10. If β > 1, then η(a, β) > 1.
Proof. Deﬁne ψ(a, β) , η(a, β)
β. It can be shown
that ψ(a, β) is a convex function in β. At the optimal β∗,
log

= 2T1. Then

−

(cid:17)

(1

)

∗

ξ)(γ
−
ξ(γ∗

∗
+β
β∗)

−

(cid:16)

(cid:17)

η(a, β)

β

a

−

≥

−

γ∗
T1

.

It can be shown that at the optimal β∗,

ξ)a2T 2
1
β∗)2
Then γ∗ = β∗ + 2ξaT1 and (28) is written as

γ∗ + β∗
β∗
γ∗ −

4ξ(1
−
(γ∗ −

−
ξ

=

=

1

ξ

η(a, β)

β

a

−

≥

−

2ξa

−

β∗
T1

.

.

(28)

(29)

n1

−

η(a,β)+o(1) + n1

−

β+o(1)

.

Deﬁne

min
[n]
i
∈

T1

(cid:18)

min
H
i
∈

≤

min
H
i
∈

≤

(cid:18)

(cid:18)

Gij x∗j x∗i + T2x∗i yi

Xj
[n]
∈
T1

(cid:19)

Gij x∗j x∗i + T2x∗i yi

(cid:19)

[n]
Xj
∈

T1

Gij x∗j x∗i + T2x∗i yi

+ max
H
i
∈

(cid:19)

e(i, H).

Hc
j
X
∈

e(i, H)

E1 ,

E2 ,

max
H
i
∈

min
H
i
∈

(cid:26)

(cid:26)

T1

(cid:18)

≤

δ

−

T1

,
(cid:27)
Gij x∗j x∗i + T2x∗i yi

≤ −

δ

.
(cid:27)

Hc
j
X
∈
E2 and it sufﬁces to show P(E1)

(cid:19)

E1 ∩
1 to prove that

Notice that F
⊃
and P(E2)
→
estimator fails. Since e(i, H)
Lemma 5,

∼

1
the maximum likelihood
Binom(
n ), from
|

, a log n

→

H

|

P(e(i, H)

δ

T1)

≥
−
log2 n
ae log log n −

T1 log n
ae

≤

(cid:18)

(cid:19)
Using the union bound, P(E1)

T1−

log n
log log n

a
log n

e−

≤

n−

2+o(1).

n−

1+o(1).

1

−

≥

 
 
 
 
Let

E ,

T1

(cid:26)

Eα ,

Gij x∗j x∗i + T2x∗i yi

≤ −

δ

,
(cid:27)

Hc
j
X
∈
Gijx∗j x∗i ≤

−

j

(cid:26) X
Hc

∈

δ + T2
,
T1 (cid:27)
T2
.
−
T1 (cid:27)

δ

α ,

E1

−

Then

j

(cid:26) X
Hc

∈

Gijx∗j x∗i ≤

−

[1

P(E)]

(a)
= 1

P(E2) = 1

= 1

−

−

H
i
Y
∈
[1

−

−
αP(Eα)

[1

P(E)]|

H

|

−
−
α)P(E1
−
−
Hc Gij x∗j x∗i + T2x∗i yi

α)]|

|,

H

(1

−

where (a) holds because
{
are mutually independent.
First, we bound P(E2) under the condition 0

T1

P

∈

j

2ξ). Using Lemma 9, P(Eα)

η(a,β)+o(1). It follows from (30) that

β <
≤
η(a,β)+β+o(1) and

n−

≥

(30)

H

i
∈

}

aT1(1
P(E1
−

−
α)

n−

≥
P(E2)

(a)

≥
(b)

1

−

1

−

h
exp

1

η(a,β)+o(1)

|

H

|

n−

i

η(a,β)+o(1)

,

n1

−

−

≥
where (a) holds because α = n−
1 + x
the ﬁrst part of Theorem 4 follows.

(cid:16)
ex. Therefore, if η(a, β) < 1, then P(E2)

(cid:17)
β+o(1) and (b) is due to
1 and

→

≤

−

We now bound P(E2) under the condition β

aT1(1

2ξ).

−

≥

Reorganizing (30),

10

Lemma 11. The sufﬁcient conditions of Lemma 7 apply to the
general side information SDP (12) by replacing S∗A = ˜Y T X ∗
and S∗B =
Proof. The proof is similar to the proof of Lemma 7.

˜Y .

−

It sufﬁces to show that S∗, deﬁned via its components S∗A,
S∗B, and S∗C, satisﬁes other conditions in Lemma 11 with
probability 1

o(1). Let

−

n

d∗i = T1

Gij x∗j x∗i + ˜yix∗i .

(32)

j=1
X

Then D∗X ∗ = T1GX ∗ + ˜Y and based on the deﬁnitions
of S∗A, S∗B, and S∗C in Lemma 11, S∗ satisﬁes the condition
T ]T = 0. It remains to show that (23) holds, i.e.,
S∗[1, X ∗
S∗

0 and λ2(S∗) > 0 with probability 1

o(1). Let

(cid:23)

−

+,mk

log

ymax , K max
k,mk (cid:12)
(cid:12)
(cid:12)
and mk
(cid:12)
T ]T = 0 and
k
2vU T ˜Y + U T D∗U

αk
αk
−
1, 2,
∈ {
V
k

(cid:18)

}

· · ·

1, 2,

where k
, K
∈ {
V such that V T [1, X ∗
V T S∗V = v2S∗A −
v2)
min
[n]
i
(cid:20)
∈
˜Y T X ∗

+ v2

(1

≥

−

d∗i −

2ymax

−

E[G]

k

v2)

−

G

T1k
n(1
−
v
|

p

,

(33)

. For any

,mk (cid:19)(cid:12)
(cid:12)
(cid:12)
, MK
(cid:12)
= 1, we have

· · ·

}

T1U T GU

−
+ T1p(1

−

2ξ)

(cid:21)

2ξ)

,

(34)

T1p(1

−

−

(cid:20)

|
where the last inequality holds in a manner similar to (24)
with the difference that in the present case

(cid:21)

P(E2) = 1

[(1

α)P(Ec
1
−

−

−

α) + αP(Ec

α)]|

H

|,

(31)

where

vU T ˜Y

v

ymax

n(1

v2).

|
Lemma 12. For feature k of general side information,

≤ |

p

−

P(Ec

α) = P

P(Ec
1
−

α) = P

j

(cid:18) X
Hc

∈

j

(cid:18) X
Hc

∈

Gijx∗j x∗i ≥

−

Gijx∗j x∗i ≥

−

δ + T2
,
T1 (cid:19)
T2
.
−
T1 (cid:19)

δ

Hc Gij x∗i x∗j is equal in distribution to

j

Also,
in Lemma 9, where p1 = p(1
η(a,β)+β+o(1) and P(Ec
n−
1
−

P

∈

−
α)

≤

|−
ξ) and p2 = pξ. Then P(Ec
α)

≤
1. It follows from (31) that

P

Hc
|
i=1

1

P

n

i=1
X

x∗i zi,k

≥

√n log n

o(1),

1

−

! ≥

Mk

zi,k ,

mk=1
X

1

yi,k=mk}

{

log

(cid:18)

αk
αk
−

+,mk

.
,mk (cid:19)

where

Si

Proof. For feature k, let

η(a,β)+β+o(1)

|

H

|

n−

β+o(1) + n−

i
η(a,β)+o(1)

H

|

|

P(E2)

α) + αn−

(1

−

1

≥
(a)
= 1

−

h
−

(b)

1

1

h
e−

−
n1−β+o(1)

≥

−
where (a) holds because α = n−
1 + x < ex. Therefore, since β
P(E2)

(cid:16)

≤

→

1 and the second part of Theorem 4 follows.

−η(a,β)+β+o(1)

n

1

−

i
,

(cid:17)

β+o(1) and (b) is due to
η(a, β), if β < 1, then

where j

1,

, Mk

}

· · ·

∈ {
n

P

x∗i zi,k

δ′

≤

! ≤

j=1
X
Applying Chernoff bound yields

i=1
X

Mk
P

P

x∗i zi,k

δ′

.
!

≤

Aj
i
X
∈

δ′ , √n log n,
ρj , 1

i

n |{

∈
and

[n] : yi,k = j

,

}|

j ρj = 1. Then

APPENDIX E
PROOF OF THEOREM 5

We begin by deriving sufﬁcient conditions for the SDP

where

P

Aj
i
X
∈

x∗i zi,k

δ′

≤

! ≤

en(ψk,j +o(1)),

estimator to produce the true labels X ∗.

ψk,j , ρj log

2
(cid:18)

+,jαk
αk
−

q

P(x∗i = 1)P(x∗i =

,j

−

1)

.
(cid:19)

 
 
 
 
Since ψk,j < 0 for any values of αk

+,j and αk
−

,j, we have

P

n

i=1
X

x∗i zi,k

δ′

≥

! ≥

1

−

Therefore, with probability 1
and Lemma 12 follows.

Using Lemmas 2 and 12,

j=1
X
o(1),

−

Mk

en(ψk,j +o(1)) = 1

o(1).

−

n
i=1 x∗i zi,k

≥

√n log n

P

V T S∗V

(1

≥

−

v2)

(cid:16)

min
[n]
i
∈

d∗i −

′

T1c

log n + T1p(1

p

−

2ξ)

.

(cid:17)
(35)

It can be shown that
distribution to
and p2 = pξ. Then
P

n
j=1 Gij x∗i x∗j

in (32) is equal

1

n
i=1 Si in Lemma 9, where p1 = p(1
−

P

in
ξ)

−

M1

M2

MK

...

P (m1, ..., mK),

(36)

P(d∗i ≤

δ) =

where

m1=1
X

m2=1
X

mK =1
X

P (m1, ..., mK) , P(xi = 1)ef2(n)P

1

n

−

Si

δ

−

f1(n)
T1

!

.

≤

i=1
X
δ + f1(n)
T1

β1| ≤

|

+ P(xi =

1)ef3(n)P

−

1

n

−

Si

≤

i=1
X
[n] d∗i under the condition

!

First, we bound mini
∈
2ξ). It follows from Lemma 9 that

aT1(1

−

1

n

−

i=1
X
n
1
−

P

P

i=1
X

Notice that

Si

≤

δ

−

f1(n)
T1

Si

≤

δ + f1(n)
T1

! ≤

! ≤

n−

η(a,β1)+o(1),

n−

η(a,β1)

β1+o(1).

−

β , lim
→∞ −
n

max(f2(n), f3(n))
log n

.

11

β1| ≥
|
β and
−

which concludes the ﬁrst part of Theorem 5.

We now bound mini
∈

[n] d∗i under the condition

limn

→∞

f2(n)
log n =

aT1(1
limn

−

→∞

0,
β. Then

2ξ). When β1 ≥
f3(n)
log n =
−
P(d∗i ≤

β1 −
δ)

n−

≤

β+o(1) + n−

β

β1+o(1).

−

f3(n)
log n =

β and limn

→∞

−

f2(n)
log n =

→∞

When β1 < 0, limn
β. Then
β1 −

P(d∗i ≤

δ)

≤
Using the union bound,

n−

β+β1+o(1) + n−

β+o(1).

o(1),

−
log n

log log n −

P

log n
log log n

d∗i ≥

min
[n]
i
∈
+ β > 1, with probability 1

≥

−

(cid:19)

1

−|

n1

log n
log log n . Substituting into (35), if

−

(cid:18)
β1|
When
|
[n] d∗i ≥
mini
∈
then with probability 1

β1|−

β+o(1).

o(1), we have
+ β > 1,

β1|

|

V T S∗V

v2)

(cid:18)

−

(1

≥
>0,

T1c′

log n + T1p(1

p

−

2ξ)

(cid:19)

which concludes the second part of Theorem 5.

APPENDIX F
PROOF OF THEOREM 6

Similar to the proof of Theorem 4, let

n

F ,

min
i
∈

[n]  

(

T1

Gij x∗j x∗i + x∗i ˜yi

)
j=1
X
P(F ) and if we show that P(F)

! ≤ −

T1

.

≥

Then P(ML Fails)
1,
the maximum likelihood estimator fails. Let H be the set of
nodes and e(i, H) denote the number of edges
ﬁrst
between node i and other nodes in the set H. It can be shown
that

n
log2 n ⌋

→

⌊

T1

min
i
∈

[n]  

Gij x∗j x∗i + x∗i ˜yi

!

[n]
Xj
∈

When β1 ≥
β1 −
−

β. Then

0, limn

f2(n)
log n =

−

β and limn

→∞

f3(n)
log n =

→∞

δ)

n−

η(a,β1)

β+o(1).

−

P(d∗i ≤
When β1 < 0, limn
β. Then
β1 −

→∞

≤
f3(n)
log n =

β and limn

→∞

−

f2(n)
log n =

Let

T1

min
i
∈

H  

T1

min
i
∈

H  

≤

≤

Gij x∗j x∗i + x∗i ˜yi

!

Gij x∗j x∗i + x∗i ˜yi

+ max
H
i
∈

!

e(i, H).

[n]
Xj
∈

Hc
j
X
∈

P(d∗i ≤

δ)

≤

n−

η(a,β1)+β1−

Using the union bound,

β+o(1) = n−

η(a,

)

β1|

|

−

β+o(1).

E1 ,

max
H
i
∈

(

e(i, H)

δ

T1

,

)

−

≤

log n
log log n

P

(cid:18)

d∗i ≥

min
[n]
i
∈
When η(a,
β1|
holds with probability 1
η(a,

|

−

1

≥

−

n1

−

η(a,

)

β1|

|

−

β+o(1).

(cid:19)
)+β > 1, it follows that mini
∈
o(1). Substituting into (35),

log n
log log n
if

) + β > 1, then with probability 1

[n] d∗i ≥
o(1),

−

log n

log log n −

v2)

(cid:18)

−

(1

≥
>0,

T1c′

log n + T1p(1

p

−

2ξ)

(cid:19)

β1|
|
V T S∗V

E2 ,

T1

min
i
∈

H  

(

Gij x∗j x∗i + x∗i ˜yi

! ≤ −

δ

.
)

Hc
j
X
∈
E2. Then the maximum likelihood
1.

1 and P(E2)

→

n ), from Lemma 5,

→

⊃

E1 ∩
Notice that F
estimator fails if we show that P(E1)
Since e(i, H)
Binom(
|
P(e(i, H)

, a log n

H

∼

T1)

δ

|

≥
−
log2 n
ae log log n −

≤

(cid:18)

T1 log n
ae

T1−

log n
log log n

(cid:19)

a
log n

e−

≤

n−

2+o(1).

 
 
 
 
 
Using the union bound, P(E1)

Let

1

−

≥

n−

1+o(1).

E ,

T1

(

Gij x∗j x∗i + x∗i ˜yi

≤ −

δ

,

)

Hc
j
X
∈
Gij x∗j x∗i ≤

δ

−

f1(n)

−
T1

E+ ,

(

Hc
j
X
∈

,

.

)

)

,

E

−

(

Hc
j
X
∈

Gij x∗j x∗i ≤

−

δ + f1(n)
T1

Deﬁne

P (m1, ..., mK) ,P(x∗i = 1)ef2(n)P(E+)

Then

P(E2) = 1

−

H
i
Y
∈

+ P(x∗i =

1)ef3(n)P(E
−

).

−

P(E)]

(a)
= 1

[1

−

[1

−

−

P(E)]|

H

|

M1

MK

= 1

1
− "

−

· · ·

P (m1, ..., mK)

m1=1
X
T1

{

mK =1
X
Hc Gij x∗j x∗i + x∗i ˜yi

j

∈

H are

i
∈

}

where (a) holds because
mutually independent.

First, we bound P(E2) under the condition

P

H

|

|

,

#

aT1(1
β1| ≤
−
|
η(a,β1)+o(1) and P(E
)
≥
−
f2(n)
β and
log n =

n−
0, limn

→∞

−

2ξ). Using Lemma 9, P(E+)
n−
limn

≥
η(a,β1)+β1+o(1). When β1 ≥
β. Then
−
P(E2) = 1

f3(n)
log n =

β1 −

n−

→∞

1

−

−
h
exp

1

η(a,β1)

β+o(1)

|

H

|

−

n1

−

η(a,β1)

i
β+o(1)

−

,

−

−

≥
(cid:16)
ex. When β1 < 0, limn
β. Then

(cid:17)
f3(n)
log n =

β and

−

→∞

using 1 + x
limn

→∞

≤
f2(n)
log n = β1 −
P(E2) = 1

1

−
h
exp

−

1

n−

η(a,β1)+β1−

β+o(1)

|

n1

−

η(a,

)

β1|

|

−

i
β+o(1)

H

|

,

−

≥
−
ex and η(a, β1)
−
) + β < 1, then P(E2)

≤

(cid:16)

using 1 + x
if η(a,
β1|
Theorem 6 follows.

|

→

We now bound P(E2) under the condition

β1 = η(a,

(cid:17)
). Therefore,
1 and the ﬁrst part of

β1|

|

|

aT1(1
β1| ≥
−
f3(n)
log n =
η(a,β1)+o(1) and

→∞

β and limn
n−

≥

n−

β1−

β+o(1)

|

H

|

n1

−

β1−

i
β+o(1)

,

f2(n)
log n =
−
β. Using Lemma 9, P(E+)

0, limn

→∞

o(1). Then

2ξ). When β1 ≥
β1 −
−
P(E
)
≥
−
P(E2)

−
1

1

η(a,β1)

−

β+o(1)

−

≥

−
exp

1

1

h

n−

−
n1

−

η(a,β1)

β+o(1)

−

−

−
(cid:16)
ex. When β1 < 0, limn

−
≥
using 1 + x
limn
→∞
and P(E
≥
−
P(E2) = 1

≤
f2(n)
β. Using Lemma 9, P(E+)
log n = β1 −
)+o(1). Then
β1|
n−
)

→∞

η(a,

|

≥

f3(n)
log n =

(cid:17)
β and
−
o(1)
1

−

1

h

nβ1−

−
n1+β1−

β+o(1)

−

β+o(1)

−
exp

1

−

≥

−

(cid:16)

n−

η(a,

)

β1|

|

−

β+o(1)

|

H

|

−

n1

−

η(a,

)

β1|

|

−

i
β+o(1)

,

(cid:17)

12

using 1 + x
β < 1, then P(E2)
follows.

≤

→

ex. Therefore, since

+
1 and the second part of Theorem 6

β1| ≤

β1|

β1|

η(a,

), if

|

|

|

APPENDIX G
PROOF OF THEOREM 7

We begin by deriving sufﬁcient conditions for the solution

of SDP (13) to match the true labels.

Lemma 13. For the optimization problem (13), consider the
Lagrange multipliers

λ∗

, µ∗

, D∗ = diag(d∗i ), S∗.

If we have

G,

−

S∗ = D∗ + λ∗J + µ∗W
S∗
0,
λ2(S∗) > 0,
S∗X ∗ = 0,

(cid:23)

then (λ∗, µ∗, D∗, S∗) is the dual optimal solution and
T is the unique primal optimal solution of (13).
X ∗X ∗

Z =

Proof. The proof is similar to the proof of Lemma 1. The
Lagrangian of (13) is given by

b

L(Z, S, D, λ, µ) =

G, Z
h

+
i
J, Z
λ
h

−

S, Z
h

i −

i − h

W, Z
µ(
h

D, Z

i −

I
i

−
(Y T Y )2),

(cid:23)
J, Z
h

0, D = diag(di), λ, µ are Lagrange multipliers.
where S
= 0, for any Z that satisﬁes the constraints
Since
G, Z
in (13), it can be shown that
. Also, similar
i
h
to the proof of Lemma 1, it can be shown that the optimum
solution is unique.

G, Z ∗

i ≤ h

i

n

d∗i =

It sufﬁces to show that S∗ = D∗ + λ∗J + µ∗W
other conditions in Lemma 13 with probability 1

G satisﬁes
o(1). Let

−
−

n

Gijx∗j x∗i −

µ∗

yiyjx∗j x∗i .

(37)

j=1
X

j=1
X
Then D∗X ∗ = GX ∗
µ∗W X ∗ and based on the deﬁnition
of S∗ in Lemma 13, S∗ satisﬁes the condition S∗X ∗ = 0. It
remains to show that (17) holds, i.e., S∗
0 and λ2(S∗) > 0
(cid:23)
o(1). Under the binary stochastic block
with probability 1
model,

−

−

E[G] =

p

q

−
2

X ∗X ∗

T +

p + q
2

J

−

pI.

It follows that for any V such that V T X ∗ = 0 and

V

V T S∗V =V T D∗V + (λ∗

k
)V ∗JV + p

p + q
2

−

V T (G

E[G])V + µ∗V T W V.

(38)

= 1,

k

mini
∈

≥

[n] d∗i and V T (G

−

Let λ∗
E[G])V

−

−
p+q
2 . Since V T D∗V
G
k
d∗i + p

E[G]

−

,

≥
≤ k
V T S∗V

min
[n]
i
∈

G

E[G]

+ µ∗V T W V.

− k

≥
[28, Thoerem 5] For any c > 0, there exists
√log n with

E[G]

1,

G

−

k

c

′

k

−

k ≤

Lemma 14.
′
c
probability at least 1

> 0 such that for any n
n−

≥
c.

−

−

Also, it can be shown that Lemma 3 holds here. Choose
µ∗ < 0, then in view of Lemmas 14 and 3, with probability
1

o(1),

V T S∗V

≥

min
[n]
i
∈

d∗i + p + (µ∗

log n.

(39)

′

c

)

−

p

Lemma 15. When δ = log n

log logn , then

P(d∗i ≤

δ)

≤
Proof. It follows from Chernoff bound.

ǫn−

1
2 (√a

√b)2+o(1) + (1

−

ǫ)ǫn.

−

Recall that β , limn

from Lemma 15 that
P(d∗i ≤
Then using the union bound,

δ)

≤

n−

log ǫ
log n , where β

≥

0. It follows

→∞ −

1
2 (√a

−

√b)2

β+o(1).

−

P

min
[n]
i
∈

d∗i ≥

log n
log log n

1

n1

−

1
2 (√a

−

√b)2

β+o(1).

−

−

≥

(cid:19)

(cid:18)
When (√a
log n
log log n holds with probability 1
with (39), if (√a
1

o(1),

−

−

√b)2 + 2β > 2, it follows that mini
∈

[n] d∗i ≥
o(1). Combining this result
√b)2 + 2β > 2, then with probability

−

−

V T S∗V

log n
log log n
which completes the proof of Theorem 7.

+ p + (µ∗

−

≥

c

′

)

log n > 0,

p

APPENDIX H
PROOF OF THEOREM 8

We begin by deriving sufﬁcient conditions for the solution

of SDP (14) to match the true labels.

Lemma 16. For the optimization problem (14), consider the
Lagrange multipliers

λ∗

, D∗ = diag(d∗i ), S∗ ,

If we have

T
S∗A S∗
B
S∗B

S∗C (cid:21)

.

(cid:20)

T2Y,

S∗A = T2Y T X ∗,
S∗B =
−
S∗C = D∗ + λ∗J
0,
S∗
λ2(S∗) > 0,
S∗[1, X ∗

(cid:23)

T ]T = 0,

T1G,

−

then (λ∗, D∗, S∗) is the dual optimal solution and
T is the unique primal optimal solution of (14).
X ∗X ∗

Z =

Proof. The proof is similar to the proof of Lemma 7. The
Lagrangian of (14) is given by

b

+ T2h
L(Z, X, S, D, λ) =T1h
I
i −
−
− h
0, D = diag(di), and λ

G, Z
i
D, Z

Y, X
+
i
J, Z
λ
h

S, H
h
,
i

i

(cid:23)

where S
J, Z ∗
multipliers. Since
h
straints in (14), it can be shown that T1h

R are Lagrange
∈
= 0, for any Z that satisﬁes the con-
G, Z

+ T2h

Y, X

i ≤

i

i

G, Z ∗

+ T2h
T1h
solution is proved similarly.

Y, X ∗

i

. Also, the uniqueness of optimum
i

13

We now show that S∗ deﬁned by S∗A, S∗B, and S∗C satisﬁes

the remaining conditions in Lemma 16 with probability 1
o(1). Let

−

n

d∗i = T1

Gij x∗j x∗i + T2yix∗i .

(40)

j=1
X

Then D∗X ∗ = T1GX ∗ + T2Y and based on the deﬁnitions
of S∗A, S∗B, and S∗C in Lemma 16, S∗ satisﬁes the condition
T ]T = 0. It remains to show that (23) holds, i.e.,
S∗[1, X ∗
S∗

0 and λ2(S∗) > 0 with probability 1

(cid:23)
For any V such that V T [1, X ∗

T ]T = 0 and

−

o(1).
V

k

= 1, we

k

have

2vT2U T Y + U T D∗U

T1U T GU

(1

V T S∗V = v2S∗A −
v2)
min
[n]
i
(cid:20)
∈
Y T X ∗

+ v2

≥

−

(cid:20)

−
+ T1p

d∗i −

G

T1k

−

E[G]

k

2T2

−

n(1
−
v
|

|

p

v2)

p

T1

−

q

(cid:21)

−
2

(cid:21)

,

(41)

where the last inequality holds in a manner similar to (24).
Using Lemma 8,

′

v2)

V T S∗V

(cid:18)

≥

−

(1

T1c

min
[n]
i
∈

log n + T1p

d∗i −
Lemma 17. Consider a sequence f (n), and for each n, let
2 , q), where p =
S1 ∼
a log n
b > 0. Deﬁne ω ,
limn

1, p) and S2 ∼
for some a
log n . For sufﬁciently large n, when ω < a

n , and q = b log n

Binom( n

Binom( n

.
(cid:19)

b
2 ,
−

2 −

(42)

f (n)

p

≥

n

→∞

P

S1 −
(cid:0)
where η∗ = a+b
γ∗
2 −
√ω2 + ab.

S2 ≤

f (n)

ω
2 log

(cid:1)
a
b

−

(cid:0)

(cid:1)

∗

η

+o(1),

n−

≤
+ ω

2 log

∗
γ
γ∗

+ω
ω

−

(cid:16)

(cid:17)

and γ∗ =

Proof. It follows from Chernoff bound.

It follows from (40) that

P(d∗i ≤

δ) =P

n

j=1
X

Gijx∗i x∗j ≤

δ

T2
−
T1 !

(1

α)

−

n

+ P

j=1
X

Gij x∗i x∗j ≤

δ + T2

T1 !

α,

n

where
Lemma 17.

j=1 Gij x∗i x∗j is equal in distribution to S1 −

S2 in

P

Recall that β , limn

T2
log n , where β

bound mini
∈
follows from Lemma 17 that

→∞
[n] d∗i under the condition 0

≤

≥
β < T1

0. First, we
b). It
2 (a

−

n

j=1
X
n

j=1
X

P

P

Then

Gij x∗i x∗j ≤

Gij x∗i x∗j ≤

δ

T2
−
T1 ! ≤

δ + T2

T1 ! ≤

n−

η(a,b,β)+o(1),

n−

η(a,b,β)+β+o(1).

P(d∗i ≤

δ)

≤

n−

η(a,b,β)+o(1).

 
 
 
 
Using the union bound,

P

(cid:18)

min
[n]
i
∈

d∗i ≥

log n
log log n

1

−

≥

(cid:19)

n1

−

η(a,b,β)+o(1).

When η(a, b, β) > 1, it follows that mini
∈
o(1). Substituting into (42),
holds with probability 1
o(1),
η(a, b, β) > 1, then with probability 1

[n] d∗i ≥

−

log n
log log n
if

−

(cid:23)

S∗
such that V T [1, X ∗
V T S∗V =v2S∗A −
v2)
min
[n]
i
(cid:20)
∈
˜Y T X ∗

+ v2

(1

≥

−

(a)

14

0 and λ2(S∗) > 0 with probability 1

o(1). For any V

T ]T = 0 and

V

k

k

−
= 1, we have

2vT2U T Y + U T D∗U

T1U T GU

−
E[G]

k
v2)

−
n(1
−
v
|
E[G]

|

d∗i −

T1k

G

2ymax

−

p
G
T1k

+ T1p

(cid:21)
p

q

−
2

(cid:21)

T1

−

(cid:20)
v2)
(cid:20)

(b)
= (1

−

d∗i −
where (a) holds in a manner similar to (24) and (41), and (b)
holds by applying Lemma 12. Then using Lemma 14,

min
[n]
i
∈

+ T1p

,
(cid:21)

−

k

V T S∗V

(1

≥

−

v2)

(cid:18)
It can be shown that

min
[n]
i
∈

T1c

log n + T1p

d∗i −
n
j=1 Gijx∗i x∗j in (43) is equal in

.
(cid:19)

(44)

p

′

distribution to S1 −
P(d∗i ≤

δ) =

where

S2 in Lemma 17. Then
M1

P
M2

MK

...

P (m1, ..., mK),

m2=1
X

mK =1
X

m1=1
X

P (m1, ..., mK) , P(x∗i = 1)ef2(n)P

δ

−

f1(n)
T1

(cid:19)

S2 ≤
S1 −
(cid:18)
δ + f1(n)
S2 ≤
T1

−

1)ef3(n)P

+ P(x∗i =

S1 −
(cid:18)
[n] d∗i under the condition
First, we bound mini
∈
b). It follows from Lemma 17 that

(cid:19)

.

T1
2 (a

β1| ≤

|

−

P

P

S1 −
(cid:18)
S1 −
(cid:18)

S2 ≤

S2 ≤

δ

−

f1(n)
T1
δ + f1(n)
T1

n−

η(a,b,β1)+o(1),

n−

η(a,b,β1)+β1+o(1).

≤

≤

(cid:19)

(cid:19)

β , lim
→∞ −
n

max(f2(n), f3(n))
log n

.

0, limn

f2(n)
log n =

−

β and limn

→∞

f3(n)
log n =

→∞

η(a,b,β1)

β+o(1).

−

n−

≤
f3(n)
log n =

β and limn

→∞

−

f2(n)
log n =

η(a,b,β1)+β1−

β+o(1) = n−

η(a,b,

)

β1|

|

−

β+o(1).

When β1 ≥
β1 −
−

β. Then

δ)

P(d∗i ≤
When β1 < 0, limn
β1 −
β. Then
P(d∗i ≤
≤
Using the union bound,

n−

δ)

→∞

log n
log log n

P

1

min
[n]
i
∈

d∗i ≥
(cid:18)
β1|
When η(a, b,
log n
log log n holds with probability 1
) + β > 1, then with probability 1
if η(a, b,

) + β > 1, it follows that mini
∈

[n] d∗i ≥
o(1). Substituting into (44),

−

≥

−

(cid:19)

−

|

|

β1|

)

β+o(1).

−

η(a,b,

n1

o(1),

−

v2)

−

log n

log log n −

T1c′

log n + T1p

> 0,

(cid:18)
which concludes the ﬁrst part of Theorem 9.

p

(cid:19)

|

β1|
(1

≥

V T S∗V

V T S∗V

(1

≥

−

v2)

log n

log log n −

T1c′

log n + T1p

> 0,

(cid:18)
which concludes the ﬁrst part of Theorem 8.

p
[n] d∗i under the condition β > T1

We now bound mini
∈

b). It follows from Lemma 17 that

(cid:19)

2 (a

−

n

P

P

(cid:18)

j=1
X
n

(cid:18)

j=1
X

Gij x∗i x∗j ≤

Gij x∗i x∗j ≤

δ

T2
−
T1 (cid:19)

δ + T2

≤

T1 (cid:19)

≤

n−

η(a,b,β)+o(1),

1.

Then

P(d∗i ≤

δ)

n−

≤
= n−

η(a,b,β)+o(1)(1
−
η(a,b,β)+o(1) + n−

α) + α
β+o(1),

where α = n−

β+o(1). Using the union bound,

P

min
[n]
i
∈

d∗i ≥

δ

(cid:19)

(cid:18)

1

−

≥

n1

−

η(a,b,β)+o(1)

n1

−

β+o(1).

−

Lemma 18. [31, Lemma 8] When β > 1, then η(a, b, β) > 1.
[n] d∗i ≥
o(1). Substituting into (42),

When β > 1, using Lemma 18, it follows that mini
∈
log n
log log n holds with probability 1
−
if β > 1, then with probability 1

o(1),

−

APPENDIX I
PROOF OF THEOREM 9

We begin by deriving sufﬁcient conditions for the solution

of SDP (14) to match the true labels.

Lemma 19. The sufﬁcient conditions of Lemma 16 apply to
the general side information SDP (15) by replacing S∗A =
˜Y T X ∗ and S∗B =
Proof. The proof is similar to the proof of Lemma 16.

˜Y .

−

It sufﬁces to show that S∗ deﬁned by S∗A, S∗B, and S∗C

satisﬁes other conditions in Lemma 19 with probability 1
o(1). Let

−

n

d∗i = T1

Gij x∗j x∗i + ˜yix∗i .

(43)

j=1
X

Then D∗X ∗ = T1GX ∗ + ˜Y and based on the deﬁnitions
of S∗A, S∗B, and S∗C in Lemma 19, S∗ satisﬁes the condition
T ]T = 0. It remains to show that (23) holds, i.e.,
S∗[1, X ∗

V T S∗V

(1

−

≥

v2)

(cid:18)

log n

log log n −

T1c′

log n + T1p

> 0,

p

(cid:19)

Notice that

which concludes the second part of Theorem 8.

−

We now bound mini
∈
T1
2 (a
limn

b). When β1 > 0,
f3(n)
β1 −
log n =
−
P(d∗i ≤
δ)

n−

→∞

≤

β. Then

When β1 < 0, limn
β. Then
β1 −

P(d∗i ≤

δ)

≤
Using the union bound,

[n] d∗i under the condition

limn

→∞

f2(n)
log n =

β1| ≥
|
β and
−

β+o(1) + n−

β

β1+o(1).

−

f3(n)
log n =

−

β and limn

→∞

f2(n)
log n =

→∞

n−

β+β1+o(1) + n−

β+o(1).

log n
log log n

P

1

d∗i ≥

min
[n]
i
∈
+β > 1, it follows that mini
∈

[n] d∗i ≥
o(1). Substituting into (44), if

≥

−

(cid:19)

−|

(cid:18)
β1|
When
with probability 1
then with probability 1

−

|

n1

β1|−

β+o(1).

log n
log log n holds
+β > 1,
β1|

|

V T S∗V

(1

−

≥

v2)

(cid:18)

T1c′

log n + T1p

> 0,

p

(cid:19)

which concludes the second part of Theorem 9.

o(1),

−

log n

log log n −

REFERENCES

[1] M. Girvan and M. E. J. Newman, “Community structure in social and
biological networks,” Proceedings of the National Academy of Sciences,
vol. 99, no. 12, pp. 7821–7826, 2002.

[2] J. Xu, R. Wu, K. Zhu, B. Hajek, R. Srikant, and L. Ying, “Jointly
clustering rows and columns of binary matrices: Algorithms and trade-
offs,” SIGMETRICS Perform. Eval. Rev., vol. 42, no. 1, pp. 29–41, Jun.
2014.

[3] K. Huang, W. Deng, Y. Zhang,

“Sparse
bayesian learning for network structure reconstruction based on
evolutionary game data,” Physica A: Statistical Mechanics and
its Applications, vol. 541, p. 123605, 2020.
[Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0378437119320102

and H. Zhu,

[4] Z. Wang, C. Wang, X. Li, C. Gao, X. Li, and J. Zhu, “Evolutionary
markov dynamics for network community detection,” IEEE Transactions
on Knowledge & Data Engineering, no. 01, pp. 1–1, may 5555.
[5] K. Huang, Z. Wang, and M. Jusup, “Incorporating latent constraints to
enhance inference of network structure,” IEEE Transactions on Network
Science and Engineering, vol. 7, no. 1, pp. 466–475, 2020.

[6] S. Fortunato, “Community detection in graphs,” Physics Reports, vol.

486, no. 3, pp. 75 – 174, 2010.

[7] M. Esmaeili and A. Nosratinia, “Community detection: Exact recovery

in weighted graphs,” arXiv preprint arXiv:2102.04439, 2021.

[8] P. W. Holland, K. B. Laskey, and S. Leinhardt, “Stochastic blockmodels:
First steps,” Social Networks, vol. 5, no. 2, pp. 109 – 137, 1983.
[9] E. Abbe and C. Sandon, “Community detection in general stochastic
block models: Fundamental limits and efﬁcient algorithms for recovery,”
in 2015 IEEE 56th Annual Symposium on Foundations of Computer
Science, Oct 2015, pp. 670–688.

[10] A. Saade, M. Lelarge, F. Krzakala, and L. Zdeborov´a, “Spectral detection
in the censored block model,” in 2015 IEEE International Symposium
on Information Theory (ISIT).

IEEE, 2015, pp. 1184–1188.

[11] B. Hajek, Y. Wu, and J. Xu, “Exact recovery threshold in the binary
censored block model,” in 2015 IEEE Information Theory Workshop-
Fall (ITW).

IEEE, 2015, pp. 99–103.

[12] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov´a, “Inference and
phase transitions in the detection of modules in sparse networks,” Phys.
Rev. Lett., vol. 107, p. 065701, Aug 2011.

[13] E. Mossel, J. Neeman, and A. Sly, “Reconstruction and estimation in
the planted partition model,” Probability Theory and Related Fields, vol.
162, no. 3, pp. 431–461, Aug 2015.

[14] L. Massouli´e, “Community detection thresholds and the weak ramanujan
property,” in Proceedings of the forty-sixth annual ACM symposium on
Theory of computing, 2014, pp. 694–703.

[15] E. Mossel, J. Neeman, and A. Sly, “A proof of the block model threshold
conjecture,” Combinatorica, vol. 38, no. 3, pp. 665–708, 2018.

15

[16] S.-Y. Yun and A. Proutiere, “Community detection via random and
adaptive sampling,” in Conference on learning theory, 2014, pp. 138–
175.

[17] E. Mossel and J. Xu, “Density evolution in the degree-correlated
stochastic block model,” in Conference on Learning Theory, 2016, pp.
1319–1356.

[18] H. Saad, A. Abotabl, and A. Nosratinia, “Exit analysis for belief
propagation in degree-correlated stochastic block models,” in 2016 IEEE
International Symposium on Information Theory (ISIT), July 2016, pp.
775–779.

[19] E. Abbe, A. S. Bandeira, and G. Hall, “Exact recovery in the stochastic
block model,” IEEE Transactions on Information Theory, vol. 62, no. 1,
pp. 471–487, Jan 2016.

[21] Y. Chen and J. Xu, “Statistical-computational

[20] E. Mossel, J. Neeman, and A. Sly, “Consistency thresholds for the
planted bisection model,” Electron. J. Probab., vol. 21, p. 24 pp., 2016.
tradeoffs in planted
problems and submatrix localization with a growing number of clusters
and submatrices,” J. Mach. Learn. Res., vol. 17, no. 1, pp. 882–938,
Jan. 2016.

[22] E. Mossel, J. Neeman, and A. Sly, “Belief propagation, robust recon-
struction and optimal recovery of block models,” in Conference on
Learning Theory, 2014, pp. 356–370.

[23] A. A. Amini, E. Levina et al., “On semideﬁnite relaxations for the block
model,” The Annals of Statistics, vol. 46, no. 1, pp. 149–179, 2018.
[24] Z.-Q. Luo and W. Yu, “An introduction to convex optimization for
communications and signal processing,” IEEE Journal on Selected Areas
in Communications, vol. 24, no. 8, pp. 1426–1438, Aug 2006.

[25] J. Mattingley and S. Boyd, “Real-time convex optimization in signal
processing,” IEEE Signal Processing Magazine, vol. 27, no. 3, pp. 50–
61, May 2010.

[26] U. Feige and J. Kilian, “Heuristics for semirandom graph problems,”
Journal of Computer and System Sciences, vol. 63, no. 4, pp. 639 –
671, 2001.

[27] A. Frieze and M. Jerrum, “Improved approximation algorithms for
maxk-cut and max bisection,” Algorithmica, vol. 18, no. 1, pp. 67–81,
May 1997.

[28] B. Hajek, Y. Wu, and J. Xu, “Achieving exact cluster recovery thresh-
old via semideﬁnite programming,” IEEE Transactions on Information
Theory, vol. 62, no. 5, pp. 2788–2797, May 2016.

[29] ——, “Achieving exact cluster recovery threshold via semideﬁnite
programming: Extensions,” IEEE Transactions on Information Theory,
vol. 62, no. 10, pp. 5918–5937, Oct 2016.

[30] M. Esmaeili and A. Nosratinia, “Community detection with secondary
latent variables,” in 2020 IEEE International Symposium on Information
Theory (ISIT).

IEEE, 2020, pp. 1355–1360.

[31] H. Saad and A. Nosratinia, “Community detection with side information:
Exact recovery under the stochastic block model,” IEEE Journal of
Selected Topics in Signal Processing, vol. 12, no. 5, pp. 944–958, 2018.
[32] H. Saad, A. Abotabl, and A. Nosratinia, “Exact recovery in the binary
stochastic block model with binary side information,” in Allerton Con-
ference on Communication, Control, and Computing, Oct. 2017, pp.
822–829.

[33] H. Saad and A. Nosratinia, “Recovering a single community with side
information,” IEEE Transactions on Information Theory, vol. 66, no. 12,
pp. 7939–7966, 2020.

[34] H. Saad and A. Nosratinia, “Belief propagation with side information for
recovering a single community,” in 2018 IEEE International Symposium
on Information Theory (ISIT).

IEEE, 2018, pp. 1271–1275.

[35] ——, “Side information in recovering a single community: Information
theoretic limits,” in 2018 IEEE International Symposium on Information
Theory (ISIT).

IEEE, 2018, pp. 2107–2111.

[36] E. Mossel and J. Xu, “Local algorithms for block models with side in-
formation,” in Proceedings of the 2016 ACM Conference on Innovations
in Theoretical Computer Science, ser. ITCS ’16, 2016, pp. 71–80.
[37] M. X. Goemans and D. P. Williamson, “Improved approximation algo-
rithms for maximum cut and satisﬁability problems using semideﬁnite
programming,” J. ACM, vol. 42, no. 6, pp. 1115–1145, Nov. 1995.
[38] M. Esmaeili, H. Saad, and A. Nosratinia, “Exact recovery by semidef-
inite programming in the binary stochastic block model with partially
revealed side information,” in IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), May 2019, pp. 3477–3481.
[39] ——, “Community detection with side information via semideﬁnite
programming,” in 2019 IEEE International Symposium on Information
Theory (ISIT).

IEEE, 2019, pp. 420–424.

