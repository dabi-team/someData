1
2
0
2
c
e
D
6
1

]
I

A
.
s
c
[

1
v
6
3
7
8
0
.
2
1
1
2
:
v
i
X
r
a

Learning to Minimize Cost-to-Serve for Multi-Node Multi-Product
Order Fulﬁlment in Electronic Commerce

Pranavi Pathakota*1, Kunwar Zaid*1, Anulekha Dhara1, Hardik Meisheri1, Shaun D’Souza2,
Dheeraj Shah2, Harshad Khadilkar1
1TCS Research
2Tata Consultancy Services Limited
India
(p.pranavi,kunwar.zaid)@tcs.com

Abstract

We describe a novel decision-making problem developed in
response to the demands of retail electronic commerce (e-
commerce). While working with logistics and retail industry
business collaborators, we found that the cost of delivery of
products from the most opportune node in the supply chain
(a quantity called the cost-to-serve or CTS) is a key chal-
lenge. The large scale, high stochasticity, and large geograph-
ical spread of e-commerce supply chains make this setting
ideal for a carefully designed data-driven decision-making al-
gorithm. In this preliminary work, we focus on the speciﬁc
subproblem of delivering multiple products in arbitrary quan-
tities from any warehouse to multiple customers in each time
period. We compare the relative performance and computa-
tional efﬁciency of several baselines, including heuristics and
mixed-integer linear programming. We show that a reinforce-
ment learning based algorithm is competitive with these poli-
cies, with the potential of efﬁcient scale-up in the real world.

Introduction
Supply chains are complex networks involving the move-
ment of people, activities, information and resources in or-
der to supply a product or service to diverse customers. The
management and operation of supply chains has been a prob-
lem of interest for decades (Holt, Modigliani, and Simon
1955; Haley and Higgins 1973; Lambert and Cooper 2000).
The earliest supply chains were focused on movement of
material from resource extraction to the manufacturer. More
recent studies considered the ‘retail’ supply chain consist-
ing of material movement from the manufacturer through
a distribution network (in the form of a hierarchy of ware-
houses) to retail stores. In the last 20 years, a new paradigm
of e-commerce has emerged: that of delivery of goods and
products from warehouses (distribution centers) directly to
customer doorsteps. This is the version of the supply chain
that is the most challenging because of the complex ﬂow of
products, and is of interest in the current paper.

The speciﬁc portions of the supply chain corresponding to
the e-commerce business involve several sub-problems such
as inventory replenishment, job-shop scheduling, bin pack-
ing, and vehicle routing. These individual problems have
been of interest to the operations research community for a

*These authors contributed equally.

long time (Ivanov et al. 2018) and exhibit unique character-
istics, posing challenges to smooth and proﬁtable operation
(Golicic et al. 2002; Lu and Liu 2015). Approaches for solv-
ing these problems include heuristics (Cooper 1964; Giusti,
Manerba, and Tadei 2020; Rabbani, Mokhtarzadeh, and
Manavizadeh 2021; Ben-Khedher and Yano 1994; Robin-
son, Narayanan, and Gao 2007; Ohta and Nakatani 2006;
Harrabi, Driss, and Ghedira 2021; Penna, Subramanian, and
Ochi 2013; Pan, Zhang, and Lim 2021) and reinforcement
learning (Golowich, Narasimhan, and Parkes 2018; Allen,
Pearn, and Monks 2021; Meisheri et al. 2020; Asadi and
Pinkley 2021; Baer et al. 2019; Chen et al. 2020; Nazari et al.
2018; Sultana et al. 2021).

Prior work: We have been unable to ﬁnd research stud-
ies that formally deﬁne and address cost-to-serve (CTS). The
problem is typically approached from a process management
point of view (Wilding 2020) to recognize that different
products and different channels to market having different
cost drivers within the supply chain. The term cost-to-serve
was used to describe customer-service costs in (Cooper and
Kaplan 1997; Braithwaite and Samakh 1998; Kaplan and
Narayanan 2001). In a case review of a Swedish manu-
facturer of heating systems (Kaplan 1989) presented the so
called whale curve. The analysis of accumulated proﬁtabil-
ity per customer demonstrated that 20% of the customers
generated 2.25 times the net company proﬁt, 70% of cus-
tomers were on the balance point, and 10% generated a loss
of 1.25 times of net proﬁts. A broader survey of CTS from
an operations management perspective is available in (Guer-
reiro, Bio, and Merschmann 2008). Recently, work has been
done towards applying CTS in customer selection (Li 2018)
and in designing strategies as regards emerging marketing
channels (Mej´ıa Argueta and Salazar 2015). CTS has been
primarily considered as a management problem and solved
using statistics or regression models.

Real-world impact: A signiﬁcant impact of CTS on
the overall supply chain proﬁtability have been studied in
(O’Byrne 2021; Easymetrics 2021). A carefully designed
decision-making system should be able to improve CTS and
reduce this number, through avenues such as efﬁcient distri-
bution, deferred service, dynamic/personalized pricing, and
(in extreme cases) order cancellation. This is a challenging
task because of the sequential nature of the problem. Each

 
 
 
 
 
 
x, wj

. Denote the jth warehouse location by wj = (wj

pmax
y).
ij
At time step t, let p(t)
ij ∈ Z+ denote the quantity available for
the ith product at jth warehouse, and let d(t)
ik ∈ Z+ denote
the demand for the ith product by customer k in that time
step (where Z+ is the set of non-negative integers). Let the
location of the customer k at time t be c(t)
k,y),
and the distance from this customer to warehouse j be
d(wj, c(t)
k ). Further assume that all products in all ware-
houses are stocked to their maximum levels pmax
after every
T time steps.

k = (c(t)

k,x, c(t)

ij

At time step t, a warehouse j is chosen to supply the re-
quested quantity d(t)
ik of each product i for customer k. The
order is then packed in cartons based on a quantization num-
ber qi for each product, which is the number of items of
product i that can be packed into a single carton. Each prod-
uct can have a different quantization number. The total num-
ber of cartons/boxes for the ith product of the kth customer
to be delivered from the jth warehouse is given by,

b(t)
ijk =

(cid:40)

ik /qi)(cid:101)

(cid:100)(d(t)
0

if i delivered from j
otherwise.

We assume that different products are not combined into
the same cartons. With this background, we can deﬁne the
cost components of CTS for the present study as follows:
1. Transportation cost, proportional to the distance between
the various chosen warehouses and the customer loca-
tion. This component is independent of the number of
cartons, and only counts ‘trips’ made by vehicles.

2. Carton cost, proportional to number of cartons required.
3. Warehouse cost: This is a ﬁxed cost applied for each
warehouse that sources at least one product in a given
order, and can vary from one warehouse to another.

Methodology

Baselines
We could not ﬁnd any explicitly deﬁned heuristics for CTS
in literature, and we therefore chose to adapt heuristics de-
ﬁned for the location allocation problem (see literature re-
view). We use the following exact optimization and heuris-
tics as baselines:
1. An exact mixed-integer linear program with the objective
of minimizing the distance, carton and warehouse costs
while fulﬁlling as many products as possible.

2. A 1-step greedy policy π1 is a heuristic that chooses the

nearest feasible warehouse to fulﬁll the orders.

3. A more sophisticated 2-step greedy policy π2. This
heuristic attempts to fulﬁl the entire order from the
second-nearest warehouse if the nearest one cannot do
so; this reduces the ‘warehouse cost’ term deﬁned earlier.
In the present deﬁnition of the problem, π2 is optimal in
the single-time-step sense provided the inventory levels
are same. It only leaves room for improvement if another
policy has the foresight to accept suboptimal decisions
for the short term in order to retain better inventory lev-
els for the longer term.

Figure 1: Illustration of e-commerce supply chain. We focus
on the portion from local warehouses to the customers.

sourcing, distribution, and pricing decision affects the in-
ventory availability, vehicle capacity, and customer satisfac-
tion, implying that locally greedy policies are unlikely to be
optimal. Furthermore, retailers can have different and time-
varying objectives based on business goals. All these con-
siderations build a strong case for the use of learning-based
techniques in place of the state-of-the-art heuristics used in
current real world solutions.

In this paper, cost-to-serve (CTS) is deﬁned as the cost
of delivering multiple products in arbitrary quantities from a
warehouse (or multiple warehouses) to customers who have
placed orders through the electronic channel (website). We
focus speciﬁcally on the right half of Figure 1, leaving out
of scope the task of replenishing local warehouses through
the upstream supply chain. CTS is used to understand the
true cost of distribution, by analysing all activities to reveal
the total cost of servicing each individual customer with a
speciﬁc product. The major contributions of our paper are,
(i) a formal deﬁnition of cost-to-serve (CTS) as a decision-
making problem, (ii) a reinforcement learning (RL) based
formulation of a preliminary sub-problem demonstrating the
potential for future expansion, (iii) validation and bench-
marking of our RL approach against state-of-the-art heuris-
tic policies, and (iv) deﬁnition of speciﬁc future directions
for this work, which will deliver business impact.

Problem formulation
We consider only two types of entities in the present work:
the distribution centers (warehouses) and customers. At a
given time-step, multiple customer orders may be generated
for ﬁxed number of products to be delivered from ﬁxed num-
ber of warehouses. The goal of the algorithm is to fulﬁl order
quantities as much as possible by choosing one warehouse to
supply each product in this order. Different products can be
supplied from different warehouses if necessary, but the full
quantity of a given product must be supplied from a single
warehouse. Each warehouse is resupplied (inventory replen-
ished) to its maximum capacity periodically, independently
of dynamic demand. Each customer order must be fulﬁlled
in the current time step if sufﬁcient inventory is available
(deferred fulﬁlment is something we are currently working
on). Any orders that are not available in the warehouses are
considered to have been dropped.

Assume that the distribution network consists of N ware-
houses with each warehouse j = {1, 2, . . . , N } stocking
i = {1, 2, . . . , M } product types up to a maximum level

Feature
Distance

Local Avail-
ability

Demand

Positional En-
coding

Global Info

d(wj, c(t))

Description
Distance
from each warehouse
Quantity p(t)
ij of current
product available at each
warehouse
Customer demand d(t)
i
for current product
One-hot
of
encoding
product, with 1 in the ith
position
Binary matrix A, entries
ai,j = 1 iff p(t)

ij ≥ d(t)

i

Size
N

N

1

M

N ×M

Figure 2: RL agent architecture for each product.

Table 1: Features in state representation for each product i.

4. Enhanced versions πP 1 and πP 2 of policies π1 and π2 are
also considered by prioritising the customers arriving in
the same time-step based on the smallest distance to the
warehouses. In experiments, we observe that these poli-
cies always outperform the original policies. Moreover,
for single customer case (1-customer arriving at a time-
step), πP 1 and πP 2 coincide with π1 and π2 respectively.

Reinforcement learning solution
Our RL approach is based on the Deep Q network
(DQN) (Mnih et al. 2015). The decision variable (action)
in our problem is to select which warehouse each prod-
uct should be shipped from for each customer. For a single
customer at timestep t, If we consider this as a concurrent
decision in the traditional RL framework, the action space
quickly becomes intractable (N × M sized). Furthermore,
such an action space would vary with the number of prod-
ucts, which is undesirable in a business where new products
are introduced (and old ones discontinued) very frequently.
To mitigate this issue, we have broken down this prob-
lem into decisions for a single product, which are then con-
catenated to complete the decision-making step. For each
product, we use the network depicted in Figure 2 to generate
decisions. The forward pass through this network is done in
parallel (time distributed layers) with weights being shared
across the products. During backpropagation, gradients from
all these decisions are averaged out to update weights. Note
that the weight sharing and parallel execution allows us to
seamlessly change the number and variety of products or-
dered by each customer. However, the combined decisions
drive the total cost (based on the number of activated ware-
houses and other quantities), thus requiring some amount
of coordination among the products. This is handled by the
state-space representation.

We deﬁne a state input that contains both local and global
information relative to the current product. Table 2 shows
the features used to represent state consisting of both local
as well as global information to ensure decision at product
level has sufﬁcient context. The global availability matrix
provides each product with an indication of which ware-

houses can supply all the other products, as does the dis-
tance vector from all warehouses to the current customer.
These inputs help the algorithm reduce the number of acti-
vated warehouses and to choose ones that are close to the
customer location. The local availability vector helps the al-
gorithm anticipate rewards for future orders of the current
product.

Rewards for the individual products are based directly on
the costs previously deﬁned, with the sole difference being
that the transportation cost used for training is only from the
selected warehouse for product i. The remaining terms are
given in full to each product (as negative rewards). For k
customers, we repeat the above inference process after de-
ducting the demand quantities that we fulﬁlled from previ-
ously served customers. We assume the priority customers
to be given, or generated on basis of heuristic.

During training, we observed that DQN quickly learns to
fulﬁl all the orders, but is unable to learn nuances such as
tradeoffs between order splitting and greedy warehouse se-
lection. In order to provide this ability, we borrow an idea
from the safe RL domain, where agents are made to respect
safety constraints during training as well as deployment. To
accelerate learning and avoid catastrophic scenario (not able
to fulﬁl the customer order), we mask the actions which are
infeasible, that is having demand which cannot be fulﬁlled
by any warehouse. This has been shown to improve RL per-
formance in safe RL and real-world applications (Garcıa and
Fern´andez 2015). We use this mask during the (cid:15)−greedy
training as well, because random exploration does not pro-
vide any guarantees of safety (Leike et al. 2017; Ecoffet et al.
2019) and might lead to biased samples in experience replay.

We have used 4 hidden layers ([40, 20, 20, 20] neurons)
neural network for DQN, with tanh activation in hidden lay-
ers and linear activation in the output layer. The training was
done using adam optimizer with a learning rate of 0.001 and
128 batch size.

AvailabilityacrosswarehousesDistancesfromwarehousesPositionalencodingGlobal infoCurrentproductquantitydemandedFullyconnectedlayersFigure 3: Training Plot for DQN.

Results

Experimentation
Throughout our experiments, we have considered a 2D grid
environment with N = 4 warehouses, M = 10 products at
each time step t. Warehouse location were ﬁxed at the centre
of each quadrant. Customer locations in each time step were
chosen with uniform random probability over the x and y
axes. The underlying demand distribution for every product
was drawn randomly from the uniform, gamma, exponential,
normal and poison distributions and then used to generate
demand for the entire episode. Replenishment of all products
and warehouses happened after T = 50 time steps, to their
maximum capacity values (which are randomly generated
for each product).

We show experiments with up to k-customers arriving in
a single time step, where k = 1, 4, 10, 40, 80, 100. The per-
unit weightage for each of the cost components is as fol-
lows: the distance cost is 100, the carton cost is 5 and the
warehouse cost is 40. The main objective of the baselines is
to fulﬁl as many products as possible in the customer order
while minimizing the total cost at that time-step. However,
an unfulﬁlment penalty of 100 towards each product which
is not fulﬁlled is considered in the overall cost while training
the RL policy. We train the RL agent purely on a k = 1 or
single-customer environment, and test the trained policy on
separate data sets created for k = 1, 4, 10, 40, 80, 100.

Figure 3 shows the training of the RL in comparison to
the performance of the baselines for the 1-customer case.
The unfulﬁlment penalty is added to the total cost of the
baselines for fair cost comparison.

Comparison with baselines on test data
Figure 4 shows total cost comparison (without the unfulﬁl-
ment penalty) of the RL algorithm with different baselines
namely MILP, π1 (worst performing policy) and πP 2 (best
performing policy) on all 6 test data sets. We observe that
RL with DQN is able to outperform πP 2 when maximum
number of customers k is less than or equal to 40. RL mod-
els are only trained for 1-customer case and trained models
are used for inference for k > 1. RL seems to struggle when
k values are increasing as this corresponds to out of distri-
bution states for the model. For 100-customer case, πP 2 and
RL perform better than the MILP with respect to the total
cost but the number of unfulﬁlled products for MILP is far
less than the other baselines as presented in Table 2.

Figure 4: Cost comparison between MILP, π1, πP 2 and RL.

Average
1-custtt
1-custup
4-custtt
4-custup
10-custtt
10-custup
40-custtt
40-custup
80-custtt
80-custup
100-custtt
100-custup

MILP
0.0985
1.273
0.1387
1.479
0.3424
5.535
0.7815
25.514
2.2257
55.272
4.0623
72.426

π1
0.0001
1.277
0.0002
1.479
0.0004
5.586
0.0011
25.827
0.0025
63.818
0.0033
104.64

π2
0.0003
1.282
0.0004
1.486
0.0010
5.595
0.0027
25.835
0.0075
63.821
0.0077
104.641

πP 1
0.0002
1.277
0.0004
1.484
0.0016
5.565
0.0144
25.779
0.0651
63.601
0.1148
104.399

πP 2
0.0003
1.282
0.0007
1.493
0.0030
5.571
0.0325
25.777
0.1460
63.7
0.2325
104.524

RL
0.0051
1.2774
0.0120
1.494
0.0264
5.5572
0.0949
25.757
0.2010
63.680
0.2479
104.4674

Table 2: Average time-taken (tt) in sec and number of un-
fulﬁlled products (up) for 6 test data sets with increasing
number of customers able to arrive in a single time step.

We note from Table 2 that as k increases, RL and πP 2
(the two best-performing policies apart from MILP) have a
signiﬁcantly higher average count of unfulﬁlled orders. On
the other hand, they have a much larger advantage (propor-
tionally speaking) in terms of computation time over MILP.
For a system that is designed for real-time decisions with
very high volume, this consideration is likely to be the key
constraint in the problem. Furthermore, while πP 2 is com-
petitive in this relatively simple environment, the real-world
system with much higher complexity will not be trivial to re-
solve using hand-written rules. Therefore, we believe RL to
be the most promising approach for the real-world deploy-
ment with all its scale and complexity.

Conclusion
In this preliminary study, we solve a simpliﬁed version of
CTS with point decisions for warehouse selection, for mul-
tiple customers using RL and several baslines. For now
we have focused on transportation and packaging cost and
on maximum order fulﬁlment at every time step. We are
working on incorporating several realistic extensions to the
present work. The idea is strongly motivated by recent real-
world developments, especially the need for real-time deci-
sions on stocking, order handling and transportation to drive
efﬁciency in a low-margin business. A dynamic decision-
making algorithm will not only help in computing the real-
time CTS for a customer order, but will also provide a range
of alternatives balancing cost and timeliness, with the view
of maximising satisfaction and minimising attrition.

References
Allen, M.; Pearn, K.; and Monks, T. 2021. Developing an
OpenAI Gym-compatible framework and simulation envi-
ronment for testing Deep Reinforcement Learning agents
solving the Ambulance Location Problem.
Asadi, A.; and Pinkley, S. N. 2021. A stochastic scheduling,
allocation, and inventory replenishment problem for battery
swap stations. Transportation Research Part E: Logistics
and Transportation Review, 146: 102212.
Baer, S.; Bakakeu, J.; Meyes, R.; and Meisen, T. 2019.
Multi-Agent Reinforcement Learning for Job Shop Schedul-
ing in Flexible Manufacturing Systems. In 2019 Second In-
ternational Conference on Artiﬁcial Intelligence for Indus-
tries (AI4I), 22–25.
Ben-Khedher, N.; and Yano, C. A. 1994. The Multi-Item
Joint Replenishment Problem with Transportation and Con-
tainer Effects. Transportation Science, 28(1): 37–54.
Braithwaite, A.; and Samakh, E. 1998. The Cost-to-Serve
International Journal of Logistics Management,
Method.
The, 9: 69–84.
Chen, R.; Yang, B.; Li, S.; and Wang, S. 2020. A self-
learning genetic algorithm based on reinforcement learning
for ﬂexible job-shop scheduling problem. Computers & In-
dustrial Engineering, 149: 106778.
Cooper, L. 1964. Heuristic Methods for Location-Allocation
Problems. SIAM Review, 6(1): 37–53.
Cooper, R.; and Kaplan, R. 1997. Cost & effect: using inte-
grated cost systems to drive proﬁtability and performance.
Easymetrics. 2021. What is Cost to Serve. https://www.
easymetrics.com/what-is-cost-to-serve/. Accessed: 2021-
06-10.
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and
Clune, J. 2019. Go-explore: a new approach for hard-
exploration problems. arXiv preprint arXiv:1901.10995.
Garcıa, J.; and Fern´andez, F. 2015. A comprehensive survey
on safe reinforcement learning. Journal of Machine Learn-
ing Research, 16(1): 1437–1480.
Giusti, R.; Manerba, D.; and Tadei, R. 2020. Multiperiod
transshipment location–allocation problem with ﬂow syn-
chronization under stochastic handling operations. Net-
works.
Golicic, S. L.; Davis, D. F.; McCarthy, T. M.; and Mentzer,
J. T. 2002. The impact of e-commerce on supply chain rela-
tionships. International Journal of Physical Distribution &
Logistics Management.
Golowich, N.; Narasimhan, H.; and Parkes, D. C. 2018.
Deep Learning for Multi-Facility Location Mechanism De-
In Proceedings of the Twenty-Seventh International
sign.
Joint Conference on Artiﬁcial Intelligence, IJCAI-18, 261–
267.
Guerreiro, R.; Bio, S.; and Merschmann, E. 2008. Cost-to-
serve measurement and customer proﬁtability analysis. In-
ternational Journal of Logistics Management, The, 19: 389–
407.

Haley, C. W.; and Higgins, R. C. 1973. Inventory policy and
trade credit ﬁnancing. Management science, 20(4-part-i):
464–471.
Harrabi, M.; Driss, O. B.; and Ghedira, K. 2021. A hybrid
evolutionary approach to job-shop scheduling with generic
time lags. Journal of Scheduling.
Holt, C. C.; Modigliani, F.; and Simon, H. A. 1955. A lin-
ear decision rule for production and employment schedul-
ing. Management Science, 2(1): 1–30.
Ivanov, D.; Sethi, S.; Dolgui, A.; and Sokolov, B. 2018. A
survey on control theory applications to operational systems,
supply chain management, and Industry 4.0. Annual Re-
views in Control, 46: 134–147.
Kaplan, R.; and Narayanan, V. 2001. Measuring and man-
aging customer proﬁtability. 15.
Kaplan, R. S. 1989. Kanthal (A). Technical report, Harvard
Business School.
Lambert, D. M.; and Cooper, M. C. 2000.
Issues in sup-
ply chain management. Industrial marketing management,
29(1): 65–83.
Leike, J.; Martic, M.; Krakovna, V.; Ortega, P. A.; Everitt,
T.; Lefrancq, A.; Orseau, L.; and Legg, S. 2017. AI safety
gridworlds. arXiv preprint arXiv:1711.09883.
Li, W. S. 2018. Cost to Serve and Customer Selection, 57–
74. ISBN 978-981-10-5728-1.
Lu, Q.; and Liu, N. 2015. Effects of e-commerce channel en-
try in a two-echelon supply chain: A comparative analysis of
single-and dual-channel distribution systems. International
Journal of Production Economics, 165: 100–111.
Meisheri, H.; Baniwal, V.; Sultana, N. N.; Khadilkar, H.; and
Ravindran, B. 2020. Using Reinforcement Learning for a
Large Variable-Dimensional Inventory Management Prob-
In Adaptive Learning Agents Work-shop, ALA-2020.
lem.
AAMAS.
Mej´ıa Argueta, C.; and Salazar, C. 2015. Cost to serve as a
strategic decision variable in the design of strategies as re-
gards emerging marketing channels. Estudios Gerenciales,
31: 50–61.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-
ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje-
land, A. K.; Ostrovski, G.; et al. 2015. Human-level control
through deep reinforcement learning. nature, 518(7540):
529–533.
Nazari, M.; Oroojlooy, A.; Tak´aˇc, M.; and Snyder, L. V.
2018. Reinforcement Learning for Solving the Vehicle
Routing Problem. NIPS’18, 9861–9871. Red Hook, NY,
USA: Curran Associates Inc.
Ohta, H.; and Nakatani, T. 2006. A heuristic job-shop
scheduling algorithm to minimize the total holding cost of
completed and in-process products subject to no tardy jobs.
International Journal of Production Economics, 101(1): 19–
29. Integrated Enterprise and Supply Chain Management.
O’Byrne, R. 2021.
Way
https://www.logisticsbureau.com/cost-to-serve-a-smarter-

Cost To Serve – A Smarter
Proﬁtability.
Supply

Improved

Chain

to

Accessed:

way-to-improved-supply-chain-proﬁtability/.
2021-06-10.
Pan, B.; Zhang, Z.; and Lim, A. 2021. A hybrid algorithm
for time-dependent vehicle routing problem with time win-
dows. Computers & Operations Research, 128: 105193.
Penna, P. H. V.; Subramanian, A.; and Ochi, L. S. 2013. An
Iterated Local Search heuristic for the Heterogeneous Fleet
Vehicle Routing Problem. Journal of Heuristics, 19: 201–
232.
Rabbani, M.; Mokhtarzadeh, M.; and Manavizadeh, N.
2021. A constraint programming approach and a hybrid of
genetic and K-means algorithms to solve the p-hub location-
allocation problems. International Journal of Management
Science and Engineering Management, 16(2): 123–133.
Robinson, E. P.; Narayanan, A.; and Gao, L.-L. 2007. Ef-
fective heuristics for the dynamic demand joint replenish-
ment problem. Journal of the Operational Research Society,
58(6): 808–815.
Sultana, N.; Baniwal, V.; Basumatary, A.; Mittal, P.; Ghosh,
S.; and Khadilkar, H. 2021. Fast Approximate Solutions us-
ing Reinforcement Learning for Dynamic Capacitated Ve-
In Adaptive Learning
hicle Routing with Time Windows.
Agents Work-shop, ALA-2021. AAMAS.
Wilding, R. 2020.
drivers.
ﬁnance-and-cost-to-serve.html.

Understanding Supply Chain cost
In https://www.richardwilding.info/supply-chain-

