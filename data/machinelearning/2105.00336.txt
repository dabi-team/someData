2
2
0
2

r
a

M
8
1

]

G
L
.
s
c
[

3
v
6
3
3
0
0
.
5
0
1
2
:
v
i
X
r
a

Comprehensive Review On Twin Support
Vector Machines

M. Tanveer1*, T. Rajani1, R. Rastogi2, Y.H. Shao3 and M. A.
Ganaie1

1Department of Mathematics, Indian Institute of Technology
Indore, Simrol, Indore 453552, India.
2Department of Computer Science, South Asian University, New
Delhi, India.
3School of Management, Hainan University, Haikou, China,
570228.

*Corresponding author(s). E-mail(s): mtanveer@iiti.ac.in;
Contributing authors: taanirajani@gmail.com;
reshma.khemchandani@sau.ac.in; shaoyuanhai21@163.com;
phd1901141006@iiti.ac.in;

Abstract

Twin support vector machine (TWSVM) and twin support vector regres-
sion (TSVR) are newly emerging eﬃcient machine learning techniques
which oﬀer promising solutions for classiﬁcation and regression chal-
lenges respectively. TWSVM is based upon the idea to identify two
nonparallel hyperplanes which classify the data points to their respec-
tive classes. It requires to solve two small sized quadratic programming
problems (QPPs) in lieu of solving single large size QPP in support vec-
tor machine (SVM) while TSVR is formulated on the lines of TWSVM
and requires to solve two SVM kind problems. Although there has been
good research progress on these techniques; there is limited literature
on the comparison of diﬀerent variants of TSVR. Thus, this review
presents a rigorous analysis of recent research in TWSVM and TSVR
simultaneously mentioning their limitations and advantages. To begin
with, we ﬁrst introduce the basic theory of support vector machine,
TWSVM and then focus on the various improvements and applications
of TWSVM, and then we introduce TSVR and its various enhance-
ments. Finally, we suggest future research and development prospects.

1

 
 
 
 
 
 
2

M. Tanveer et al.

Keywords: Machine learning, Twin support vector machines (SVM), twin
SVM, survey of twin SVM, review of twin SVM, Classiﬁcation, Regression,
Clustering.

1 Introduction

SVM [1] is a prominent classiﬁcation technique widely used since its inception.
It was ﬁrst introduced by Cortes and Vapnik [1] in 1995 for binary classiﬁ-
cation problems. SVM seeks to ﬁnd decision hyperplanes that determine the
decision boundary which can classify the data points into two classes. These
decision planes are called support hyperplanes and the distance between them
is maximized by solving a quadratic programming problem (QPP). SVM is
computationally powerful even in non-linearly separable cases by using kernel
trick [2]. SVM has remarkable advantages as it utilizes the idea of structural
risk minimization (SRM) principle which provides better generalization as well
as reduces error in the training phase. As a result of its superior performance
even in non-linear classiﬁcation problems, it has been implemented in a diverse
spectrum of research ﬁelds, ranging from text classiﬁcation, face recognition,
ﬁnancial application, brain-computer interface, bio-medicine, human action
recognition, horse race odds prediction and multiple instance learning [3–16].
SVM has also been used in feature selection methods [17]. Robust SVM penal-
ized all the samples via a new loss function for better generalization in presence
of noise [18]. Although SVM has outperformed most other systems, it still has
many limitations in dealing with complex data due to its high computational
cost of solving QPPs and its performance highly depends upon the choice of
kernel functions and its parameters. Many improvements have been made in
the last decade to enhance the accuracy of SVM [19, 20]. One such critical
enhancement was generalized eigenvalue proximal SVM (GEPSVM) [21–23]
that led the foundation of TWSVM [24, 25]. The idea behind GEPSVM is to
determine two nonparallel hyperplanes via solving two generalized eigenvalue
problems. TWSVM [24, 25] enhanced the generalization ability of GEPSVM.
TWSVM also determines two nonparallel hyperplanes and needs to solve a pair
of small QPPs in lieu of solving one complex QPP in SVM. Computational cost
of TWSVM is approximately one-fourth of the SVM. TWSVM is faster and
has better generalization performance than SVM and GEPSVM. Authors also
extended TWSVM for nonlinear classiﬁcation problems by introducing kernel.
But, in nonlinear cases when a kernel is used, the formulation requires to solve
inverse of matrices. Also, its performance highly depends upon the selection
of kernel functions and its parameters for nonlinear classiﬁcation. Although
TWSVM is still in its rudimentary stage, many improvements and variants
have been proposed by researchers due to its favorable performance especially
in-case of handling large datasets which is not possible with the conventional
SVM. Huang et al. [26] reviewed the research progress of TWSVM until 2017
for classiﬁcation problems.

M. Tanveer et al.

3

Support Vector Machine has also been used eﬀectively for regression and
is called support vector regression (SVR) [27]. SVR is diﬀerent than SVM in
some aspects as it sets a margin of tolerance (cid:15) and ﬁnd the optimal hyper-
plane such that the error is minimized. Thus, it ﬁnds a function such that error
can be maximum of (cid:15) distance, thus any error within (cid:15) deviation is accept-
able. Learning speed of SVR is low as it needs to solve a large sized QPP.
Researchers have proposed many variants of SVR to improve its performance in
terms of reducing computational complexity and improving accuracy [28, 29].
Some other variants of SVR includes (cid:15)–support vector regression ((cid:15)–SVR) [30],
Fuzzy SVM [31], ν–SVR [32], robust and sparse SVR [33] and some other algo-
rithms [34, 35]. (cid:15)–SVR has high computational cost and in order to remediate
this, an eﬃcient twin support vector regression (TSVR) [36] determined two
nonparallel hyperplanes that clusters the data points in two classes by opti-
mizing the regression function. TSVR uses (cid:15)–insensitive up and down-bound
functions to optimize the ﬁnal regression function. TSVR formulation is com-
putationally less complex as it needs to solve a pair of smaller QPPs in lieu of
solving one large QPP in SVR, thus it’s speed is much more than SVR. How-
ever, in TSVR, the spirit of TWSVM was missing, improved TSVR (TWSVR)
[37] which works on the similar lines of TWSVM was proposed. To further
boost the performance of TSVR, many algorithms have been introduced by
researchers which are discussed in section 6.

In the past years, twin support vector classiﬁcation and regression algo-
rithms have been developed rapidly and are implemented to solve some real-life
challenges. However, there is limited literature on twin support vector regres-
sion as it is a relatively new theory and needs further study and improvement.
Thus, the objective of this paper is to present an compendium of recent devel-
opments in TWSVM and TSVR, identify limitations and advantages of these
techniques and promote future developments.

The framework of this paper is as follows, section 2 presents brief about
SVM and TWSVM, section 3 and 4 include the recent advancements and appli-
cations of TWSVM respectively, section 5 presents a brief about TSVR, section
6 and 7 include the recent advancements and applications of TSVR respec-
tively and at last section 8 provides synopsis, future research and development
prospects.

2 Related Work

Suppose a binary classiﬁcation problem with dataset X ∈ Rl×n in which l1
data points are in class +1 (termed as positive class) and l2 data points are
in class −1 (termed as negative class) and these are represented by matrix A
and B respectively. For classiﬁcation problems, the data label Y ∈ {−1, 1} and
for regression Y ∈ R. Accordingly, these matrices will be of size (l1 × n) and
(l2 × n) where n is feature space dimension and l = l1 + l2.

4

M. Tanveer et al.

2.1 Support Vector Machine [1]

The optimization problem of support vector machine is given as follows:

min
u,ξ

1
2

uT u + c

l
(cid:88)

k=1

ξk

s.t. yk(uT φ(xk) + b) ≥ 1 − ξk, k = 1, . . . , l

ξk ≥ 0, k = 1, . . . , l,

(1)

where, c > 0 is the regularisation parameter, and φ(x) represents the non-linear
mapping of the input x and ξ is the vector of slack variables.

Form the above given optimization problem, one can see that all the data
samples appear in the constraints of the optimization problem. Hence, the
complexity of the model is high as it involves solving a single large quadratic
programming problem. Thus, the complexity of SVM is O(l3).

2.2 Twin Support Vector Machines [24]

Standard twin SVM is a binary classiﬁcation model. To categorize data samples
which cannot be separated by linear functions, TWSVM uses kernel functions
to convert the higher dimensional data space to the required form. The two
kernel generated surfaces are given as below:

K(xT , DT )u+ + b+ = 0 and K(xT , DT )u− + b− = 0,

(2)

here D = [A; B]; and K is a kernel function. The formulation for nonlinear
TWSVM classiﬁcation is deﬁned as below:

min
u+, b+, ξ1

1
2

(cid:107)K(A, DT )u+ + e1b+(cid:107)2 + c1eT

2 ξ1

s.t. − (K(B, DT )u+ + e2b+) + ξ1 ≥ e2, ξ1 ≥ 0

(3)

and

min
u−, b−, ξ2

1
2

(cid:107)K(B, DT )u− + e2b−(cid:107)2 + c2eT

1 ξ2

s.t. K(A, DT )u− + e1b− + ξ2 ≥ e1, ξ2 ≥ 0,

(4)

here c1, c2 ≥ 0, e1, e2 are vectors of ones of suitable dimensions and ξ1, ξ2 are
called slack variables. After using the Lagrange multipliers α ≥ 0, β ≥ 0 and
the Karush-Kuhn-Tucker (K.K.T.) [38] conditions, the duals of the QPPs in
(3) and (4) are deﬁned as below:

max
α

eT
2 α −

1
2

αT Q(P T P )−1QT α

M. Tanveer et al.

5

s.t. 0 ≤ α ≤ c1

and

1
2

eT
1 β −

max
β
s.t. 0 ≤ β ≤ c2,

βT P (QT Q)−1P T β

(5)

(6)

where P = [K(A, DT ) e1] and Q = [K(B, DT ) e2]. After solving (5) and (6),
the proximal hyperplanes are given as follows:

(cid:21)

(cid:20)u+
b+

= −(P T P + δI)−1QT α,

(cid:21)

(cid:20)u−
b−

= (QT Q + δI)−1P T β,

(7)

(8)

where δI is a regularization term and δ > 0.

A new sample x ∈ Rn is allocated to either class on the basis of the

proximity of kernel generated surface to x, i.e.,

class(x) = sign

(cid:16) K(xT , DT )u+ + b+
(cid:107)u+(cid:107)

+

K(xT , DT )u− + b−
(cid:107)u−(cid:107)

(cid:17)

,

(9)

where sign(·) is signum function.

From optimization problems (3) and (4), one can see that the constraints
of only one class appear in the optimization problem while generating the
hyperplane for the other class. Thus, the size of the constraints in QPPs of
TWSVM is approximately half compared to the SVM formulation (assuming
the data is balanced between the classes). TWSVM is approximately four times
faster than the usual SVM. The complexity of TWSVM is 2 ∗ O((l/2)3). In
terms of generalization, TWSVM and SVM have comparable generalization
performance [24].

3 Research Progress on Twin Support Vector

Machines

In this section, we discuss the progress of TWSVM based models in classiﬁca-
tion problems. The variants of TWSVM are

3.1 Least Squares Twin Support Vector Machines

To reduce TWSVM training time, Kumar and Gopal [39] formulated least

6

M. Tanveer et al.

squares TWSVM (LS-TWSVM) algorithm. The major advantage of LS-
TWSVM over TWSVM is that it only deals with linear equations in place
of QPPs in TWSVM which reduces the computational complexity of the
model. LS-TWSVM has comparable accuracy but low computational time
than TWSVM. Although the computational time of LS-TWSVM is less than
TWSVM, its generalization performance is poor as same penalties are allotted
to the samples either being positive or negative. LS-TWSVM apply the empir-
ical risk minimization (ERM) principle which aﬀects accuracy and also causes
over-ﬁtting problem. Also, it doesn’t take into account the eﬀects of samples
having diﬀerent locations. To avoid this limitation, weighted LS-TWSVM [40]
gives diﬀerent weights to the samples depending upon their locations. Exper-
imental results have shown that weighted LS-TWSVM yields better testing
accuracy than TWSVM and LS-TWSVM but its computationally time is more
than these algorithms.

To incorporate expert’s knowledge in LS-TWSVM classiﬁer, knowledge-
based LS-TWSVM (KBLS-TWSVM) [41] included polyhedral knowledge sets
in the formulation of LS-TWSVM. Experimental results have shown that
KBLS-TWSVM is simple and more apt classiﬁer compared to LS-TWSVM. In
order to improve accuracy of LS-TWSVM, Wang et al. [42] incorporated the
manifold geometric structure of data of each class. It requires to solve a set of
linear equations.

Although, LS-TWSVM performed well with large datasets compared to
TWSVM, Gao et al. [43] designed l1-norm LS-TWSVM (NLS-TWSVM) to
automatically select the relevant features in order to strengthen the algo-
rithm in dealing with high-dimensional datasets. NLS-TWSVM is based on
LS-TWSVM and includes a Tikhonov regularization term. To outdo LS-
TWSVM in accuracy and avoid over-ﬁtting problem, Improved LS-TWSVM
(ILS-TWSVM) [44]
improved the classiﬁcation accuracy by implementing
structural risk minimization principle. ILS-TWSVM is faster and yields com-
parable generalization and computational time to LS-TWSVM. Since L2 norm
magniﬁes the outlier eﬀect in least squares TWSVM models, hence, capped
L2,p norm based least squares TWSVM [45] was formulated to reduce the
eﬀect of outliers and noise.

To take advantage of the correlation between some data points and reduce
the eﬀect of noise, Ye et al. [46] used density of the sample to allot weights for
each sample (DWLSC). Experimental results demonstrate better classiﬁcation
accuracy of the DWLSC than TWSVM and LS-TWSVM.

There are several real-life problems which are essentially multicategory
problems. The extensions of TWSVM to multicategory is also an active
research domain where several research papers have evolved addressing the
same. However, class imbalance problem is common in multicategory classi-
ﬁcation as all binary SVMs are trained with all patterns. To deal with these
issues Nasiri et al. [47] formulated energy-based LS-TWSVM (ELS-TWSVM)
in which the constraints of LS-TWSVM are converted to an energy model to

M. Tanveer et al.

7

decrease the impact of noise and outliers. TWSVM, LS-TWSVM, and ELS-
TWSVM satisfy the empirical risk minimization (ERM) principle and also in
these techniques the matrices are positive semi-deﬁnite. Thus, to remediate
this problem, Tanveer et al. [48] embodied the SRM principle in ELS-TWSVM
and proposed robust ELS-TWSVM (RELS-TSVM) which makes the matrices
positive deﬁnite. Moreover, RELS-TSVM uses an energy parameter to handle
the noise and thus make the algorithm more robust. Results have shown the
promising generalization performance of RELS-TSVM with less computational
time when compared with the baseline algorithms. In the recent comprehen-
sive evaluation [49] of 187 classiﬁers on 90 datasets, RELS-TSVM [48] emerged
as the best classiﬁer. Khemchandani and Sharma [50] proposed robust least
squares TWSVM (RLS-TWSVM) which is fast and yields better generalization
performance and is also robust to handle the noise. Further, the application in
the ﬁeld of human activity recognition is explored in the paper. The authors
in [50] also proposed Incremental RLS-TWSVM to increase the training speed
of RLS-TWSVM and Incremental RLS-TWSVM also deals with noise and
outliers eﬀectively.

Tomar and Agarwal [51] proposed feature selection based LS-TWSVM to
diagnose heart diseases using F-score to choose the most relevant features
which enhances the accuracy of the model. To handle imbalanced datasets, a
novel weighted LS-TWSVM [52] for imbalanced data which has better accuracy
and geometric mean than SVM and TWSVM. To further enhance the accuracy
on imbalanced datasets, Hybrid Feature Selection Based WLS-TWSVM (HFS
based WLS-TWSVM) [53] approach was proposed for diagnosing diseases like
Breast Cancer, Diabetes and Hepatitis which can tackle data imbalance issues.
Xu et al. [54] included data distribution information into the classiﬁer struc-
tural LS-TWSVM (SLS-TWSVM). It is based on structural TWSVM and
performs clustering before classiﬁcation and embody the structural informa-
tion into the model. SLS-TWSVM has a better generalization performance
than ν-SVM [55], ν-TWSVM [56], STWSVM [57] and LS-TWSVM [39]. It
also improves the noise insensitivity of LS-TWSVM.

Mei and Xu [58] proposed a novel multi-task LS-TWSVM algorithm which
is build on directed multi-task TWSVM (DMTWSVM) and LS-TWSVM. It
focuses on multitask learning instead of commonly applied single task learning
in TWSVM and LS-TWSVM. This algorithm is computationally eﬀective as
it only requires to solve linear equations in lieu of QPPs in DMTWSVM.
Least square twin support vector hypersphere (LSTSVH) [53]

is an
enhancement of twin support vector hypersphere (TSVH) [59]. TSVH is dif-
ferent from TWSVM as it obtains two hyperspheres through solving two small
SVM type problems. Experimental results demonstrate that LSTSVH has
almost same accuracy as SVM, LS-SVM, TWSVM, LS-TWSVM, and TSVH
but has high computational time than LS-SVM and LS-TWSVM and less than
SVM and TWSVM. In 2020, Tanveer et al. [60] proposed eﬃcient large scale

8

M. Tanveer et al.

Fig. 1 Variants of TWSVM

least squares twin support vector machines (LS-LSTWSVM) which uses dif-
ferent Lagrangian functions to eliminate the need for calculating the inverse
matrices and thus enhance the performance of TWSVM on large scale datasets.

Table 1 Classiﬁcation accuracy on non-linear kernel [48]

Dataset (Train size, Test size) TWSVM [24] TBSVM [61]
Ripley (250×2, 1000×2)
Heart-c (177×13, 120×13)
Heart-stat (200×13, 70×13)
Ionosphere (246×33, 105×33)
Bupa-liver (241×6, 104×6)
Votes (306×16, 129×16)
WPBC (137×34, 57×34)
Pima-Indian (537×8, 231×8)

83.50
70.00
84.28
87.46
70.19
96.90
80.19
76.09

88.00
60.81
82.85
92.38
63.46
96.90
80.70
77.48

LS-TWSVM [39]
86.40
61.67
85.71
87.61
55.77
96.12
78.94
64.07

ELS-TWSVM [47] RELS-TSVM [48]

83.50
61.67
78.57
92.38
66.35
95.35
75.44
78.35

88.30
67.50
85.71
96.19
69.23
96.90
80.70
80.52

3.2 Projection Twin Support Vector Machine

Projection TWSVM (P-TWSVM) [62]
is a multiple-surface classiﬁcation
(MSC) technique based on the multi-weight vector projection SVM (MVSVM)
and TWSVM. In 2011, Chen et al. [62] introduced P-TWSVM with minimiz-
ing within class variance principle. Unlike ﬁnding hyperplanes in TWSVM,
P-TWSVM ﬁnds projection axes to distinctly separate samples. To enhance
performance of P-TWSVM, authors proposed a recursive algorithm in which
for each class multiple projection axes are considered. P-TWSVM is a better
classiﬁer as hyperplanes are more ﬁtted for single plane based samples while for
datasets having complex sample distribution like XOR problems, projection
directions can generate better accuracy. Experimental results have shown that

M. Tanveer et al.

9

P-TWSVM has better accuracy than GEPSVM, TWSVM, LS-TWSVM, and
MVSVM. However, it is computationally more complex than TWSVM. Thus,
Shao et al. [63] proposed improved P-TWSVM and used the idea in LS-SVM
[64] and LS-TWSVM [41] and added a regularization term into P-TWSVM
simultaneously maintaining the optimization problem to be positive deﬁnite.
It is simple, fast and has similar classiﬁcation accuracy but less computational
time than P-TWSVM.

Although P-TWSVM is an eﬃcient classiﬁer, it only implements empiri-
cal risk minimization principle which can incur possible singularity problems
which need to be dealt with principal component analysis (PCA) and linear dis-
criminant analysis (LDA). Shao et al. [65] formulated a new P-TWSVM variant
by introducing a maximum margin regularization term called RP-TWSVM in
which empirical risk is replaced by regularized risk. Authors further proposed
a successive over-relaxation (SOR) technique and a genetic algorithm (GA) for
parameter selection to optimally solve the primal problems. RP-TWSVM not
only has better accuracy but also has a better generalization than P-TWSVM.
To implement least square P-TWSVM to non-linear problems, Ding and
Hua [66] introduced kernel to LSP-TWSVM [63] which also deals with lin-
ear equations similar to LSP-TWSVM and opposed to P-TWSVM. Authors
introduced nonlinear recursive algorithm in order to improve its perfor-
mance. Experimental results have shown good classiﬁcation accuracy than
LSP-TWSVM and P-TWSVM.

Guo et al. [67] proposed a feature selection approach for LSP-TWSVM
which ﬁnds two projection directions and has comparable prediction accu-
racy to that of TWSVM, LS-TWSVM and LSP-TWSVM and a similar
generalization ability to TWSVM and LS-TWSVM.

LSP-TWSVM enhances the performance of P-TWSVM but fails to include
underlying data information to enhance classiﬁcation accuracy. To overcome
this limitation, weighted LSP-TWSVM (LIWLSP-TWSVM) [68] exploited
local information into the algorithm. LIWLSP-TWSVM is more eﬀective than
LSP-TWSVM as it uses weighted mean rather than standard mean in LSP-
TWSVM. It has better generalization ability but its computational cost is high
when solving multi classiﬁcation problems.

Hua et al. [69] formulated a novel projection TWSVM (NP-TWSVM). NP-
TWSVM has many advantages over P-TWSVM. It is faster than P-TWSVM
as calculation of inverse matrices is not avoided in NP-TWSVM. It deter-
mines the projection axes so that margin across the sample and class is greater
than or equal to zero. Experimental results have shown that it obtains better
generalization and accuracy when compared with other baseline algorithms.
An eﬃcient nonparallel sparse projection SVM (NPrSVM) [70] ﬁnds opti-
mal projection for each class such that the projection values of within class
instance are clustered as much as possible within an insensitive tube while
those of other class instance are kept away. Extensive experiments show that
NPrSVM is superior than PTWSVM in terms of generalization performance,
training time, sparsity and robustness to outliers. ν− projection twin SVM

10

M. Tanveer et al.

[71] seeks projection axis for each class in a manner that ν controls the fraction
of support vectors and error margin, and also avoids the matrix inversions.
Robust rescaled hinge loss based projection TWSVM model [72] use diﬀer-
ent parameters to control the eﬀect of outliers and the model results in better
performance.

Table 2 Classiﬁcation accuracy on non-linear kernel [68]

Dataset
Cleve (296×13)
Bupa-liver (345×6)
Heart (270×13)
Monks2 (432×6)
Australian (690×14)
Wpbc (198×33)
Parkinsons (195×22)
Vertebral (310×6)
Spect (267×23)
Spectf (267×44)

TWSVM[24] WLTSVM[73]

85.12
74.13
84.44
68.77
86.97
79.36
87.22
85.81
84.42
83.43

84.90
73.58
84.82
66.21
86.70
80.35
85.00
86.13
85.19
82.96

P-TWSVM[62]
85.93
72.18
84.44
68.71
86.99
78.03
86.36
84.52
84.42
82.49

LSP-TWSVM[63]
83.80
72.47
84.44
67.84
87.27
78.56
86.62
84.84
84.80
83.26

LIWLSP-TWSVM[68]
85.30
73.39
84.82
68.95
88.13
81.87
87.48
87.10
85.11
84.25

3.3 Twin Parametric Margin Support Vector Machine

Motivated by the idea of TWSVM and Par-ν-SVM [74], in 2011 Peng et
al. [75] formulated twin parametric-margin SVM (TPMSVM). Like par-ν-
SVM, TPMSVM seeks to determine two parametric margin hyperplanes which
deﬁnes the positive and negative margin respectively. It’s an indirect classiﬁer
even suitable when noise is heteroscedastic as it automatically adjusts a ﬂexi-
ble margin. Experimental results have shown that TPMSVM has comparable
generalization ability to SVM, Par-ν-SVM, and TWSVM. Also, TPMSVM has
much lower training time than Par-ν-SVM as it solves two small sized QPPs
like TWSVM. TPMSVM classiﬁer is not sparse due to the formulations of
its parametric- margin hyperplanes. TPMSVM has good generalization but it
is computationally complex as it solves two QPPs. In 2013, Wang et al. [76]
added a quadratic function to TPMSVM in primal space to get better training
speed. The proposed algorithm is called smooth twin parametric-margin SVM
(STPMSVM). Also, the authors proposed the use of genetic algorithm (GA)
in STPMSVM to overcome the drawback of regularizing at least four parame-
ters in TPMSVM. To obtain sparsity and feature noise insensitivity, truncated
pinball loss TPMSVM [77] was proposed. The optimal separating hyperplanes
are obtained via concave–convex procedure (CCCP).

Shao et al. [78] in 2013 proposed least squares TPMSVM (LSTPMSVM)
to decrease the training cost of TPMSVM as LSTPMSVM solves two primal
problems rather than dual problems. This change makes it less complex and
increases training speed. It has comparable or better classiﬁcation accuracy
but with remarkably less computational time than TPMSVM. For model selec-
tion, the authors proposed a particle swarm optimizer (PSO) which eﬀectively
optimizes the four parameters deﬁned in LSTPMSVM. In terms of generaliza-
tion, LSTPMSVM performs better than TPMSVM and LS-TWSVM. In order

M. Tanveer et al.

11

to consider prior structural data information, Peng et al. [79] in 2013 proposed
structural twin parametric-margin SVM (STPMSVM). Based on cluster gran-
ularity, the class data structures are included in the problem. STPMSVM not
only obtained good generalization ability but also showed fast learning speed,
and better performance than TPMSVM. TPMSVM is an eﬃcient classiﬁer but
it losses the sparsity due to the weight vectors of the hyperplanes. To solve
this issue, centroid-based TPSVM (CTPSVM) [80] was introduced which uses
the projection of the centroid points and leads to a sparse optimal hyperplane
by optimizing the centroid projection values.

To incorporate structural information present in data, in 2017, Rastogi et
al. [81] proposed robust parametric TWSVM for pattern classiﬁcation (RP-
TWSVM) which seek to ﬁnd two parametric margin hyperplanes that has the
capability to adjust margin to capture heteroscedastic noise data information.
Rastogi et al. [82], formulated angle-based TPSVM (ATPSVM) is proposed,
which can eﬃciently handle heteroscedastic noise. Other improvements which
maximizes angle between the twin hyperplanes are proposed by Rastogi et
al. in [83]. Taking motivation from TPMSVM, ATPSVM determines a pair of
hyperplanes so that the angle between their normals is maximized. Recently,
Richhariya and Tanveer proposed a novel angle based universum LS-TWSVM
(AULSTWSVM) for pattern classiﬁcation. In contrast to ATPSVM [83], the
AULSTWSVM minimizes the angle between universum hyperplane and classi-
fying hyperplane. To incorporate the prior information about the distribution
of the data using the universum, AULSTWSVM used linear loss [84] in the
formulation. Numerical experiments show the promising generalization per-
formance with very less computational time. Further, the application in the
diagnosis of Alzheimer’s disease is explored.

Moreover, the proposed AULSTWSVM includes linear loss [84] in the
optimization problem, while incorporating prior information about data distri-
bution using the universum. Moreover, the solution of AULSTWSVM involves
system of linear equations leading to less computation time [39]

3.4 Robust and Sparse Twin Support Vector Machine

TWSVM achieves better accuracy and is faster when compared to conventional
SVM but it loses sparsity as it considers l2 norm of distances in the objective
function. Thus, in order to make TWSVM sparse, Peng [85] in 2011 proposed
TWSVM in primal space which provides a sparse hyperplane with better gen-
eralization ability. To improve the robustness, a regularization term is also
added. It has comparable generalization and a rapid learning speed to TWSVM
and LS-TWSVM but the computational cost is high. In 2013, Peng and Xu [86]
proposed robust minimum class variance TWSVM (RMCV-TWSVM) classi-
ﬁer to enhance generalization and robustness of TWSVM. This algorithm has
an extra regularization term which makes its learning speed comparable to
TWSVM but obtains better generalization ability than TWSVM.

12

M. Tanveer et al.

Qi et al. [87] in 2013 proposed robust TWSVM using second-order cone pro-
gramming formulation. It is eﬀective with noise-corrupted data. This algorithm
overcomes the limitation of TWSVM and computes the inverse of matrices
which are not suitable for large datasets, it takes only the inner products
about samples by which kernel trick can be applied directly and does not
need to solve the extra inverse of matrices. It is superior in computational
time and accuracy than other TWSVMs. In 2014, Tian et al. [88] proposed
sparse nonparallel support vector machine (SN-SVM). While TWSVM losses
the sparseness, SN-SVM has the inherent sparseness as it uses the two loss
functions instead of one in existing TWSVM. Only the empirical risk is consid-
ered in TWSVM, SN-SVM introduces the regularization term by maximizing
the margin. TWSVM minimizes the loss function based on l1 or l2-norm. Thus,
Zhang et al. [89] proposed lp-norm-LS-TWSVM which is an adaptive as p can
be automatically chosen by data. Robust non-parallel SVM via second order
cone programming [90] is robust to outliers and noise, and constructs the two
separating hyperplanes via maximisation of probabilistic framework.

In order to intensify the robustness and sparsity in the original formulation
of TWSVM, Tanveer [91] in 2015, incorporated regularization technique and
formulated a linear programming l1-norm TWSVM (NLP-TWSVM) which
needs to solve linear equations rather than solving QPPs in TWSVM which
makes it fast, robust, sparse and a simple algorithm. Experimental results
demonstrate that NLP-TWSVM’s generalization ability is better and compu-
tational time is less than GEPSVM, SVM, and TWSVM. Tanveer [92, 93]
proposed smoothing approaches for 1-norm linear programming TWSVM
(SLPTWSVM). The solution of SLPTWSVM reduces to solving two systems
of linear equations which makes the algorithm extremely simple and com-
putationally eﬃcient. Tanveer et al. [94] proposed sparse pinball TWSVM
(SP-TWSVM) by introducing (cid:15) insensitive zone pinball loss function in the
orginal TWSVM formulation. SP-TWSVM is noise insensitive, retain spar-
sity and more stable for re-sampling. Robust capped l1 norm TWSVM [95]
reduced the eﬀect of outliers which resulted in better performance. To reduce
the overﬁtting issues, capped l1 norm twin bounded support vector machines
[96] was proposed. Eﬃcient and robust l1 norm TWSVM [97] used l1 norm
to maximise the ratio of interclass scatter to intraclass scatter. The authors
used iterative procedure to get the optimal separating hyperplanes. Adaptive
capped Lθ,(cid:15)-loss based TWSVM [98] is a generalized TWSVM model wherein
θ and (cid:15) parameters are optimized to meet the objectives of robust to noise and
outliers.

3.5 Pinball Loss Twin Support Vector Machine

In 2016, Xu et al. [99] proposed a novel TPMSVM with the pinball loss (Pin-
TPMSVM). The authors introduced pinball loss function that is based on
maximizing quantile distances between the two classes instead of hinge loss
function which maximizes the distance between the closest samples of the

M. Tanveer et al.

13

two classes. This leads to noise insensitivity and as well as re-sampling sta-
bility. Pin-TWSVM has excellent capability to handle noise which makes the
model a more robust classiﬁer but little attention is given on sparsity due
to which the testing time is high and there is instability in re-sampling. Xu
et al. [100] also formulated a maximum margin and minimum volume hyper-
spheres with pinball loss (Pin-M 3HM). Authors proposed this algorithm to
enhance the generalization of twin hypersphere SVM (TH-SVM) for noise
present in datasets. The algorithm classiﬁes samples of two classes by a pair
of hyper-spheres, each containing either majority or the minority class sam-
ples simultaneously maintaining maximum margin between the hyperplanes.
Pin-M 3HM is fast and has better accuracy than TWSVM. In 2018, Sharma
and Rastogi [101] proposed two models called (cid:15)-Pin-TWSVM and Flex-Pin-
TWSVM. (cid:15)-Pin-TWSVM introduces (cid:15) parameter to reduce the impact of noise
and to attain a sparse solution while Flex-Pin-TWSVM uses a self-optimized
framework which makes this algorithm ﬂexible to estimate the size of insen-
sitive zone. It adapts to the structure of data. Pin-TPMSVM has impressive
ability to handle noise but to reduce parameter tuning time, Yang et al. [102]
in 2018 introduced a new solution approach for the TPMSVM in which one
instance is considered at a time and it is solved analytically without solving
optimization problem. It is fast, simple and ﬂexible. Since pinball loss func-
tion is not diﬀerentiable at zero, hence, smooth pinball loss TWSVM [103]
used smooth approximation function and solved the objective functions via
Newton-Armijo method.

For imbalanced data classiﬁcation in 2018, Xu et al. [104] proposed a
maximum margin twin spheres which uses pinball loss. This algorithm ﬁnds
homocentric spheres so that the smaller one captures positive class samples
and the larger one repel negative samples. It requires to solve a QPP and a
Linear programming (LP) problem. This algorithm has good generalization
performance and noise insensitivity, however, suﬀers due to large complexity.
To reduce the complexity, bound estimation-based safe acceleration for maxi-
mum margin of twin spheres machine with pinball loss [105] was proposed. In
2019, Sharma et al. [106] proposed a Stochastic Quasi-Netwon method based
TPSVM using Pinball Loss Function (SQN-PTWSVM) which can scale the
training process to handle millions of data points while simultaneously deals
with noise and re-sampling data issues. Twin neural networks [107] uses fea-
ture maps which allows better discrimination among classes. The twin neural
network is also extended to multiclass problems wherein the number of neural
networks trained is proportional to number of classes.

The aforementioned pinball

loss TWSVM algorithms are based on
TPMSVM and not on the original TWSVM algorithms. In 2019, Tanveer
et al. [108] introduced pinball loss to the original TWSVM, termed as gen-
eral TWSVM with pinball loss (Pin-GTSVM). Pin-GTSVM [108, 109] is less
sensitive to the outliers and more stable algorithm for re-sampling as com-
pared to the original TWSVM. To retain the sparsity of original TWSVM
and Pin-GTSVM, Tanveer et al. [94] proposed a novel sparse pinball TWSVM

14

M. Tanveer et al.

(SP-TWSVM) which uses (cid:15)-insensitive zone pinball loss function. SP-TWSVM
has better classiﬁcation accuracy than TWSVM and Sparse Pin SVM and
it is insensitive to outliers, retains sparseness and suitable for re-sampling.
Recently, Tanveer et al. [110] proposed improved sparse pinball TWSVM
(ISPTWSVM) by adding a regularization term to the objective function of
SP-TWSVM. ISPTWSVM implements SRM principle and also the matrices
appear in the dual formulation are positive deﬁnite which makes the proposed
algorithm computationally less complex and it also achieves better accuracy
than other baseline algorithms. Most of the twin SVM based models involve
matrix inversion operations which limits their applicability to large scale data.
Hence, large scale pinball TWSVM [111] uses pinball loss function to reduce
the issues of feature noise and reformulated the Lagrangian in a manner that
matrix inversions are no longer involved in the optimization problems. This
scaled the model to large scale data.

3.6 Twin Support Vector Machine for Universum Data

and Imbalanced Datasets

In 2012, Qi et al. [112] formulated TWSVM for universum data classiﬁca-
tion (U-TWSVM). Universum data is deﬁned as the data samples which does
not belong to any given class. This algorithm utilizes the universum data to
enhance TWSVM classiﬁcation accuracy. U-TWSVM employs new data points
to the either class based on its proximity to the hyperplanes. Experimental
results have shown that U-TWSVM has better accuracy than TWSVM. In
order to construct a robust classiﬁer by including the prior information embed-
ded in the universum samples in 2014, Qi et al. [113] formulated nonparallel
SVM (U-NSVM) which maximizes the two margins related to the two nearest
adjacent classes.

As many real life problems consists of datasets which are imbalanced in
nature i.e. classes do not contain same number of data samples, due to which
many machine learning algorithms cannot be implemented. Thus, to enhance
the performance of TWSVM while dealing with imbalanced datasets, in 2014
Shao et al. [114] proposed weighted Lagrangian TWSVM for imbalanced data
classiﬁcation. Authors introduced a quadratic loss function to the formula-
tion of TWSVM which enabled faster training of data points. Also, this is
more robust to outliers and has better computational speed and accuracy
than TWSVM. In 2014, Tomar et al. [52] proposed weighted LS-TWSVM for
imbalanced datasets by adjusting the classiﬁer and assigning distinct weight
to training samples. The results of the experiment shows that its accuracy is
more than SVM, TWSVM, and LS-TWSVM. Furthermore, in 2015, Xu et al.
[115] formulated least squares TWSVM (ULS-TWSVM) which exploits the
universum data. It introduces a regularization term thus implements SRM
principle and is computationally less complex as it requires to solve linear
equations in place of QPPs in TWSVM. Cao and Shen [116] proposed com-
bining re-sampling with TWSVM for imbalanced data classiﬁcation. In this

M. Tanveer et al.

15

algorithm, authors presented a hybrid re-sampling technique which utilizes the
one side selection (OSS) algorithm and synthetic minority oversampling tech-
nique (SMOTE) algorithm to balance the training data. This is combined with
TWSVM for classiﬁcation purpose. However, SMOTE algorithm based on K-
nearest neighbors can often result in over-ﬁtting. Robust rescaled hinge loss
TWSVM [117] and density weighted TWSVM [118] for imbalanced datasets
resulted in improved performance.

Encouraged by the performance of ν-TWSVM [56] and U -TWSVM [112],
Xu et al. [119] in 2016 proposed ν-TWSVM for universum data classiﬁcation
(U ν-TWSVM). It is more ﬂexible in using the prior knowledge from universum
data to improve the generalization ability. It uses two hinge loss functions
so that data can remain in a nonparallel insensitive loss tube. Experimental
results have shown that U ν-TWSVM has better accuracy and also costs lower
running time than other baseline algorithms.

In 2017, Xu [120] proposed maximum margin twin spheres SVM algorithm
(MMTSSVM) to overcome many limitations of TWSVM while dealing with
imbalanced data. This algorithm seeks to determine two homocentric spheres
so that smaller one captures as many positive class samples and the larger
one repel negative samples simultaneously increases the margin between the
spheres.

In 2018, Richhariya et al. [121] added a regularization term into the opti-
mization problem of universum TWSVM to make matrices non-singular and
improve the generalization performance. The proposed algorithm is called
improved universum TWSVM. It has better generalization and training time
than USVM and UTWSVM. In 2019, Richhariya and Tanveer [122] proposed
a fuzzy universum SVM (FUSVM) based on information entropy. Universum
support vector machines are more eﬃcient than other SVM methods as it
includes prior information of samples. But, it is not eﬀective in case of noise-
corrupted datasets. FUSVM introduces weights such that it gives fewer weights
to the outlier universum points. Further, the authors proposed a fuzzy univer-
sum TWSVM (FUTWSVM). Both the algorithms have better generalization
performance than SVM, USVM, TWSVM, and UTWSVM. Recently, Rich-
hariya and Tanveer [123] proposed a reduced universum twin support vector
machine for class imbalance learning (RUTWSVM-CIL) with the idea of prior
information about the data distribution. RUTWSVM-CIL used reduced kernel
matrix, and thus applicable for the large sized imbalanced datasets. Numerical
experiments on several imbalanced benchmark datasets showed the applicabil-
ity of RUTWSVM-CIL. In 2020, Richhariya and Tanveer [124] also proposed
a novel parametric model for universum based twin support vector machine
and extended its application for the classiﬁcation of Alzheimer’s disease data.
Fuzzy universum least squares TWSVM [125, 126] and fuzzy least squares
TWSVM [127, 128] solved a linear system of equations instead of solving the
QPPs, hence, the model is fast and requires no external toolbox for solving the
optimization problems. Robust TBSVM [129] is proposed to make the model
more robust to noise in imbalance datasets.

16

M. Tanveer et al.

3.7 Fuzzy Twin Support Vector Machines

In 2017, Chen and Wu [130] proposed a novel fuzzy TWSVM (NFTWSVM)
which takes care of the problem of dealing with one class playing major
role in classiﬁcation, by assigning fuzzy membership to diﬀerent samples.
The proposed NFTWSVM includes fuzzy neural networks and provides more
generalized results. In 2019, Richhariya and Tanveer [122] proposed a fuzzy
universum SVM (FUSVM) based on information entropy. This algorithm is
also discussed in Section 3.6. Richhariya and Tanveer [131] also proposed a
robust fuzzy LS-TWSVM (RFLS-TWSVM-CIL) to boost the performance of
TWSVM on imbalanced datasets. The optimization problem in this algorithm
is strictly convex as it uses 2-norm of the slack variables. Authors further
proposed a fuzzy membership function to deal with imbalanced problems as
it provides weights to samples and includes data imbalanced ratio. RFLS-
TWSVM-CIL obtains better generalization and is computationally more
eﬃcient as it requires to solve two linear equations. However, RFLSTWSVM-
CIL algorithm implements empirical risk minimization principle which requires
inverse of matrices to be positive semi-deﬁnite. In order to improvise, Ganaie
et al. [132] introduced a regularization term to the primal formulation of
RFLSTWSVM-CIL. The proposed algorithm is regularized robust fuzzy least
squares (RRFLSTWSVM) which doesn’t require the extra assumption of
inverse of matrices to be positive semi-deﬁnite and performs better than
RFLSTWSVM-CIL.

In 2018, Khemchandani et al. [133] formulated fuzzy LS version of TSVC
in which each data point has a fuzzy membership value and is allotted to dif-
ferent clusters. This algorithm is also discussed in Section 3.11. In 2020, Chen
et al. [134] proposed entropy-based fuzzy least squares TWSVM which con-
siders the fuzzy membership for each data point basis entropy. Rezvani and
Wang [135] formulated intuitionistic fuzzy TWSVM (IFTWSVM) which is
modiﬁed version of fuzzy TWSVM as it considers the position of input data in
the feature space and calculate adequate fuzzy membership and also reduces
the eﬀect of noise. Zhang and Li [136] proposed fuzzy TWSVM which assigns
fuzzy membership based on intra-class hyperplane and sample aﬃnity. Exper-
imental results showed that this algorithm has better accuracy than TWSVM
and classic fuzzy TWSVM. In 2019, Gupta et al. [137] proposed entropy-based
fuzzy twin support vector machine (EFTWSVM-CIL) which is an eﬃcient
algorithm for imbalanced datasets as it assigns fuzzy membership values based
on entropy of the samples. In 2020, Chen at el. [134] proposed entropy-based
fuzzy least squares twin support vector machine (EFLSTWSVM) which is
an improvised version of EFTWSVM-CIL by formulating the QPPs in least
squares sense and retains the superior characteristics of LSTWSVM. Experi-
mental results shown that this algorithm performed better than other baseline
fuzzy TWSVM algorithms.

M. Tanveer et al.

17

Table 3 Classiﬁcation accuracy on non-linear kernel [134]

Dataset (Train size, Test size) TWSVM [24]
Australian (690×14)
Bupa-Liver (345×6)
House-Votes (435×16)
Heart-c (303×13)
Heart-Statlog (270×13)
Ionosphere (351×34)
Musk (476×166)
PimaIndian (768×8)
Sonar (208×60)
Spect (267×44)
Wpbc (198×34)

87.83
75.07
96.32
85.14
85.56
94.32
95.79
78.52
88.95
82.42
83.86

LSTWSVM [39]
87.54
75.94
96.55
84.50
85.19
94.30
94.96
78.30
91.39
83.89
83.36

EFSVM [138]
87.10
73.91
95.86
84.84
85.19
95.73
94.54
77.74
90.88
82.78
82.83

FTWSVM [139]
87.39
74.78
96.32
83.83
85.86
94.60
96.43
78.13
89.94
83.15
83.33

EFTWSVM-CIL [137]
87.68
74.49
96.09
86.13
86.30
94.60
95.58
78.64
91.86
83.52
84.36

EFLSTWSVM [134]
87.97
75.65
96.55
85.82
85.93
96.58
96.63
79.16
91.81
84.65
84.88

3.8 Some other improvements of Twin Support Vector

Machines

Kumar and Gopal [140] in 2008 enhanced TWSVM using smoothing tech-
niques. Authors proposed to transform the primal problems of TWSVM
into smooth unconstrained minimization problems and used Newton−Armijo
algorithm for optimization. Smooth TWSVM (STWSVM) has comparable
generalization to TWSVM but is signiﬁcantly a faster algorithm. Jayadeva
et al. [141] in 2009 proposed iterative alternating algorithm to make kernels
learn eﬃciently. In 2009, Zhang [142] proposed boosting based TWSVM for
clustered microcalciﬁcations detection. Results have shown that this method
improved detection accuracy. Bagging algorithm was combined with boosting
to solve the unstable problem of TWSVM. This method is called BB-TWSVM.
Twin support vector machine in linear programs and robust TWSVM [143]
have also been proposed for the classiﬁcation problems [144].

In 2010, Shao et al. [145] proposed a bi-level programming method to mul-
tiple instance classiﬁcation, called MI-TWSVM. Multiple instance learning
(MIL) is a supervised technique wherein the training data points consist of
labeled bags and these bags include multiple unlabeled instances. For binary
classiﬁcation, a bag is a negative sample if all the instances in it are negative
and it is labeled a positive sample if at least one instance is positive. Similar to
TWSVM, the proposed algorithm seeks two hyperplanes thereby the positive
hyperplane is closest to the positive instances and as distant as possible from
the negative instances and vice-versa. In 2010, Ghorai et al. [146] proposed
a unity norm TWSVM classiﬁer (UNTWSVM) which includes unity norm
equality constraints and a quadratic loss function in the objective function of
TWSVM. This algorithm can be solved by sequential quadratic optimization
method. UNTWSVM has more computational cost than TWSVM especially
for large datasets because of the nonlinear formulation. Generalized TWSVM
[147] proposed two models, in the ﬁrst model the authors used the l1 and l∞
norm in the optimization problems and in second model the convex, piecewise
quadratic objective function is solved via generalized Newton method.

In order to improve TWSVM’s generalization ability and to decrease sup-
port vectors (SVs), in 2010 Peng [56] introduced a variable p and parameter ν
in TWSVM and proposed ν-TWSVM. An improved ν-TWSVM [148] has also
been proposed for the classiﬁcation problems. The parameter ν controls the
trade-oﬀ between the SV and marginal error while adaptive p overcomes the

18

M. Tanveer et al.

limitations of constraints being over-restricted in TWSVM and thus reduces
the number of support vectors. But, it gives the same penalties to each misclas-
siﬁed sample and results in over-ﬁtting. Thus, Xu et al. [149] in 2012 introduced
the rough set theory into ν-TWSVM to remediate this limitation. This algo-
rithm is more eﬀective in avoiding over-ﬁtting as it gives diﬀerent penalties
to diﬀerent points depending upon location. It also has better generalization
than ν-TWSVM.

Although rough ν-TWSVM [149] performs better than ν-TWSVM,
it
provides diﬀerent weights to negative samples and same weights to all the
positive samples. Thus, Xu et al. [150] formulated K nearest neighbor (KNN)
weighted rough ν-TWSVM which gives diﬀerent penalties to the samples of
both the classes. It has better accuracy and lower computational cost than ν-
TWSVM and Rough ν-TWSVM. In another attempt to enhance the results of
ν-TWSVM, Khemchandani et al. [151] formulated two models for binary classi-
ﬁcation: IνTWSVM and IνTWSVM Fast. Both the algorithms are faster than
ν-TWSVM as IνTWSVM solves one small QPP and an unconstrained mini-
mization problem (UMP) while IνTWSVM Fast solves one unimodal function
and one UMP. To overcome the disadvantage of TWSVM being sensitive to
outliers, Xie and Sun [152] implemented class centroid and proposed multitask
centroid TWSVM for multitask problems.

For handling large scale datasets, Shao et al. [84] introduce weighted linear
loss in TWSVM and proposed weighted linear loss TWSVM (WL-TWSVM)
to classify large scale datasets. The proposed algorithm only deals with linear
equations which increases the training speed and also has better generaliza-
tion than TWSVM. To further fasten up the training process, Sharma et al.
[153] and Wang et al. [154], recently proposed stochastic conjugate gradient
method based twin support vector machine (SCH-TWSVM). The resulting
model showed eﬀective performance on binary activity classiﬁcation problem.
Shao et al. [61] in 2011 introduced the regularization term in TWSVM to
embody SRM principle and formulated a new algorithm called twin bounded
support vector machines (TBSVM). To further speed up training, SOR tech-
nique is used. Numerical results on various datasets show that TBSVM is
faster and has better generalization ability than TWSVM. In 2010, Ye et al.
[155] formulated localized TWSVM via convex minimization (LC-TWSVM)
which eﬀectively constructed two nearest-neighbor graphs in the original input
space to reduce the space complexity of TWSVM. Shao and Deng [156] in
2012 considered the unity norm constraints and added a regularization term
to minimize structural risk in TWSVM and proposed margin-based TWSVM
with unity norm hyperplanes (UNHMTWSVM). The proposed algorithm is
fast, has better generalization and accurate compared to TWSVM. Lagrangian
TBSVM with L2 norm [157] replaced L1 norm of the slack variables with L2
norm to improve the performance.

To include statistical data information in TWSVM, Peng and Xu [158]
formulated twin Mahalanobis distance-based SVM (TMSVM) that uses each
classes covariance to determine hyperplanes. It has better training speed and

M. Tanveer et al.

19

generalization than TWSVM. Shao et al. [159] in 2012 introduced a probability
based TWSVM model (P-TWSVM) which is more accurate than TWSVM.
In 2012, Shao and Deng [160] formulated a coordinate descent based TWSVM
to increase the eﬃciency of TWSVM by introducing a regularization term.
Further, to solve the dual problems, the authors proposed a coordinate descent
method which reduces the training time even in case of large datasets as it
deals with single data point at once.

To include the underlying correlation information between data points in
TWSVM, in 2012, Ye et al. [73] formulated a novel nonparallel plane classiﬁer,
called Weighted TWSVM with Local Information (WL-TWSVM). A major
limitation of this algorithm is that it doesn’t work for large-scale problems as it
ﬁnds the K-nearest neighbors for all the samples. Based on TWSVM, in 2013,
Peng and Xu [59] formulated a twin-hypersphere SVM (TH-SVM). Unlike
TWSVM, it determines two hyperspheres and avoids the matrix inversions in
its dual formulations.

To incorporate the structural data information in TWSVM, Qi et al. [57]
formulated structural-TWSVM which is superior in training time as well as
accuracy to that of TWSVM. However, it still ignores the importance of dif-
ferent samples within each cluster. To overcome this drawback, in 2015, Pan
et al. [161] uses K-nearest neighbor based structural TWSVM which provides
diﬀerent penalties to the samples of diﬀerent classes. However, it suﬀers from
overﬁtting due to empirical risk minimisation. To reduce the overﬁtting issues,
eﬃcient KNN weighted TWSVM [162] introduced the regularisation term to
avoid the issues of overﬁtting. This also ensured the minimization of structural
risk which results in better generalization.

In 2014, Shao et al. [163] formulated a diﬀerent nonparallel hyperplane
SVM in which hyperplanes are determined by clustering the samples based on
similarity between the two classes. This algorithm has better or comparable
accuracy with low computational time. To optimally select the parameters in
TWSVM, in 2016 Ding et al. [164] proposed TWSVM build on Fruit Fly Opti-
mization Algorithm (FOA-TWSVM). It can optimally select the parameters
in less time and with better accuracy. In 2017, Pan et al. [165] proposed safe
screening rules to make TWSVM eﬃcient for large-scale classiﬁcation prob-
lems. The safe screening rules reduce the scale by eliminating training samples
and giving same solution as the original problem. Experimental results show
that safe screening rules can greatly reduce the computational time while giv-
ing the same solutions as original ones. Yang and Xu [166] proposed safe sample
screening rule (SSSR) for Laplacian twin parametric-margin SVM (LTPSVM)
to address the problems while handling large-scale problems. In 2018, Pang
and Xu [167] proposed safe screening rule for Weighted TWSVM with local
information (WLTWSVM) to implement the algorithm for large-scale datasets.
Experimental results demonstrate the eﬀectiveness of SSSR for WLTWSVM
as it performes better than SVM and TWSVM with signiﬁcantly less com-
putational time. Recently, Zhao et al. [168] proposed an eﬃcient non-parallel

20

M. Tanveer et al.

hyperplane Universum support vector machine (U-NHSVM) for classiﬁcation
problems. U-NHSVM is ﬂexible to exploit the prior information in universum.
Tanveer [92] proposed unconstrained minimization problem (UMP) formu-
lation of Linear programming TWSVM to enhance robustness and sparsity in
TWSVM. Tanveer [169] also proposed an implicit Lagrangian TWSVM which
is solved by using ﬁnite Newton method. Tanveer and Shubham [170] in 2017
proposed smooth TWSVM via UMP which increases the generalization ability
and training speed of TWSVM.

Multi-label learning deals with data having multiple labels and has gained
a lot of attention recently. Chen et al. [171] in 2016 proposed multi-label
TWSVM (MLTWSVM) that exploits multi-label information from instances.
In 2020, Azad–Manjiri et al. [172] proposed structural twin support vec-
tor machine for multi-label learning (ML-STWSVM) which embeds the prior
structural information of data into the optimization function of MLTWSVM
based on the same clustering technology of S-TWSVM. This algorithm
achieved better performance compared to other baseline multi-label learning
algorithms.

In 2018, Rastogi et al.

[173] proposed a new loss function termed as
((cid:15)1, (cid:15)2)-insensitive zone pinball loss function which generalizes other existing
loss functions e.g. pinball loss, hinge loss. The resulting model takes care of
noise insensitivity, instability of re-sampling and scatterdness present in the
datasets. In 2019, Tanveer et. al [49] presented rigorous comparison of 187
classiﬁers which includes 8 variants of TWSVM and exhaustive evaluation of
these classiﬁers was performed on 90 UCI benchmark datasets. Results have
shown that RELS-TSVM achieved highest performance than all other classi-
ﬁers for binary classiﬁcation task. Thus, RELS-TSVM is the best TWSVM
classiﬁer. For more details, one can refer to [49]. In 2020, Ganaie and Tan-
veer [174] proposed a novel classiﬁcation approach using pre-trained functional
link to enhance the feature space. Authors performed the classiﬁcation task
by LSTWSVM on the enhanced features and validated the performance on
various datasets. Some other recent research on TWSVM include [175] where
authors proposed a novel way for generating oblique decision trees. The classi-
ﬁcation of training samples is done based on the Bhattachrayya distance with
randomly selected feature subset and then hyperplanes are generated using
TBSVM. The major advantage of the proposed model is that there is no need
for any extra regularization as matrices are positive deﬁnite. The ensemble
[176] based models of twin SVM based models was proposed in [177].

3.9 Twin Support Vector Machine for Multi-class

Classiﬁcation

Earlier, TWSVM was only implemented to solve binary class problems,
however, the majority of problems in the real-world applications are gener-
ally based on multicategory classiﬁcation. Thus, Xu et al. [178] formulated
Twin-KSVC. It implements “1-versus-1-rest” form to provide ternary output

M. Tanveer et al.

21

(−1, 0, 1). This algorithm requires to solve two smaller-sized QPPs. It has bet-
ter accuracy than 1-versus-rest TWSVM but losses sparsity. Yang et al. [179]
in 2013, proposed multiple birth SVM (MBSVM). It is computationally better
than TWSVM even when number of classes are large.

In 2013, Xie et al. [180] extended TWSVM application for multi-class prob-
lems and proposed one-versus-all TWSVM (OVA-TWSVM) which solve k-
category problem using one-versus-all (OVA) approach to develop k TWSVM.
To strengthen the performance of multi-TWSVM, Shao et al. [181] formu-
lated a separating decision tree TWSVM (DTTWSVM). The basic idea of
DTTWSVM is to embody the best separating principle rule to create a binary
tree and then built binary TWSVM model on each node. Experimental results
have shown that DTTWSVM has low computational complexity and better
generalization.

Xu and Guo [182] in 2014 formulated twin hyper-sphere multi-class SVM
(THKSVM) which employs the “rest-versus-1” structure instead of “1-versus-
rest” structure in TWSVM. In order to ﬁnd hyperspheres, it constructs k (no.
of classes) classiﬁers and ﬁnds k centers and k radiuses for each hypersphere.
Each hypersphere covers maximum points of K1 classes and is as distant as
possible from the rest classes. It has fast computational speed as compared
to Twin-KSVC and also inverse operation of matrices are not required while
solving dual QPPs of THKSVM. Thus, it performs better on large datasets
and has better accuracy than“1-versus-rest” TWSVM but lower than Twin-
KSVC. To enhance the performance of Twin-KSVC, in 2015, Nasiri et al. [183]
formulated LS version of Twin-KSVC that works similar to Twin-KSVC but
solves linear system of equations rather than pair of QPPs in Twin-KSVC.
The proposed algorithm is fast, simple and has better accuracy and lower
training time than Twin-KSVC. Another approach to enhance the performance
of Twin-KSVC and to include local information of samples, in 2016, Xu [184]
proposed K-nearest neighbor-based weighted multi-class TWSVM which uses
information from within class and applies a weight in the objective function.
This algorithm has low computational cost and better accuracy than Twin-
KSVC.

Based on the multi-class extension of the binary LS-TWSVM, weighted
multi-class LSTWSVM algorithm [185] and regularized least squares twin SVM
[186] have been proposed for multi-class imbalanced data. To control sensitivity
of classiﬁer, weight setting is employed in loss function for determining each
hyperplane. Experimental results have shown the superiority and feasibility
of the proposed algorithm for multi-class imbalanced problems. The authors
[187] extended LS-TWSVM for multi-class classiﬁcation and compared various
concepts of a multi-class classiﬁer like “One-versus-All”, “One-versus-One”,
“All-versus-One” and “Directed Acyclic Graph (DAG)”. DAG MLS-TWSVM
performance is superior and has high computational eﬃciency.

In 2016, Yang et al. [188] formulated least squares recursive projection
TWSVM. For each class, it determines k projection axes and needs to solve

22

M. Tanveer et al.

[189]

linear equations. It has similar performance as MP-TWSVM. Based on P-
TWSVM, Li et al.
in 2016, proposed multiple recursive projection
TWSVM (Multi-P-TWSVM) which solves k QPPs in order to determine k-
projection axes (for k classes). Authors introduced regularization term and
recursive procedure which increases the generalization but this algorithm is
complex when more orthogonal projection axes are generated.

In 2017, Ding et al. [190] review various multi-class algorithms as per their
structures: “one-versus-rest”, “one-versus-one”, “binary tree”, “one-versus-
one-versus-rest”, “all-versus-one” and “direct acyclic graph” based multi-class
TWSVM. All these multi-class TWSVMs have some advantages and disad-
vantages. In general, one-versus-one TWSVMs have higher performance. Ai
et al. [191] in 2018 proposed a multi-class classiﬁcation weighted least squares
TSVH using local density information in order to improve the performance of
LSTSVH. Authors introduced local density information into LSTSVH to pro-
vide weight for each data point in order to avoid noise sensitivity. In 2018,
Pang et al. [192] proposed K-nearest neighbor-based weighted multi-class
TWSVM (KMTWSVM) that incorporated ”1-versus-1-versus-rest” strategy
for multi-class classiﬁcation and also takes into account the distribution of
all instances. However, it is computationally extensive especially for large-
scale problems. Thus, authors in [192] also proposed safe instance reduction
rule (SIR-KMTWSVM) to reduce its computational time. Lima et al. [193]
proposed improvements on least squares twin multi-class classiﬁcation SVM
which is “one-versus-one-versus-rest” strategy and generated ternary output.
The proposed algorithm only needs to deal with linear equations. Numerical
results demonstrate that it achieves better classiﬁcation accuracy than Twin-
KSVC [178] and LSTKSVC [183]. Qiang et al. [194] proposed improvement
on LSTKSVC by proposing robust weighted linear loss twin multi-class sup-
port vector machine (WLT-KSVC) which takes care of the two drawbacks
of LSTKSVC; sensitive to outliers and misclassifying some rest class samples
due to the use of quadratic loss. Experiments on the UCI and NDC datasets
showed promising results of this algorithm but its training accuracy signiﬁ-
cantly decreases as the number of classes increases. Li et al. [20] proposed a
nonparallel support vector machine (NSVM) for multiclass classiﬁcation prob-
lem. Numerical experiments on several benchmark datasets clearly show the
advantage of NSVM. In 2020, Tanveer et al. [195] proposed a fast and improved
version of KWMTWSVM [196] called least square K-nearest neighbor weighted
multi-class TWSVM (LS-KWMTWSVM). Numerical experiments on various
KEEL imbalance datasets showed high accuracy and low computational time
for the proposed LS-KWMTWSVM as compared to other baseline algorithms.
A nulticlass nonparallel parametric margin SVM [197] has also been proposed
for multiclass classiﬁcation. Kernel free least squares TWSVM [198] via special
fourth order polynomial surface resulted in improved performance in multiclass
problems.

M. Tanveer et al.

23

3.10 Twin Support Vector Machine for Semi-Supervised

Learning

Semi-supervised learning (SSL) techniques have achieved extensive attention
from many researchers in the last few years due to its promising applications in
machine learning and data mining. In many real-world challenges, labeled data
is not easily available and thus it deteriorates the performance of supervised
learning algorithms due to insuﬃcient supervised information. SSL overcomes
this limitation and uses both unlabeled and labeled data.

In 2012, Qi et al. [199] formulated a novel Laplacian –TWSVM for the semi-
supervised classiﬁcation problem. This algorithm assumes that data points lie
in low dimensional space and uses the geometric information of the unlabeled
data points. Similar to TWSVM it solves a pair of QPPs with inverse matrix
operations. Results have shown that Lap-TWSVM has better ﬂexibility, pre-
diction accuracy and generalization performance than conventional TWSVM.
This algorithm has excellent performance for semi-supervised classiﬁcation
problems but due to QPPs and inverse matrix operations, its computational
cost is high. Thus, to increase the training speed of Lap-TWSVM, Chen et
al. [200] in 2014 formulated LS version of Lap-TWSVM (Lap-LS-TWSVM).
Unlike Lap-TWSVM, it needs to deal with linear equations and is done using
conjugate gradient algorithm. It has less training time than Lap-TWSVM.
Another algorithm was formulated by Chen et al. [201] in 2014 to improve
the performance of Lap-TWSVM by transforming the QPPs to UMPs in
primal space and smoothing method is used which is eﬀectively solved by
Newton-Armijo algorithm. It achieved comparable accuracy as Lap-TWSVM
with less computational time. In 2016, Khemchandani and Pal [202] extended
this to multi-class classiﬁcation and formulated Lap-LS-TWSVM which evalu-
ates training samples to “1-versus-1-versus-rest” and provides ternary output
(−1, 0, +1). It has better accuracy and less training time than Lap-TWSVM.
Based on the similar idea of Lap-TWSVM, in 2016, Yang and Xu [203]
proposed an extension of traditional TPMSVM, which makes use of graph
Laplacian and creates a connection graph of the training data points whose
solution can be obtained by solving two SVM-type QPPs. Experiments reveal
that LTPMSVM has higher classiﬁcation accuracy than SVM, TPMSVM, Lap-
TWSVM, and TWSVM.

Khemchandani and Pal [204], in 2017, blended the Laplacian–TWSVM
and Decision Tree –TWSVM classiﬁer and formulated a tree based classiﬁer
for semi-supervised multi-class classiﬁcation. Extensive experiments on color
images shows the feasibility of the model. Another interesting approach in this
direction in 2019, has been suggested by Rastogi and Sharma [205] called as
Fast Laplacian TWSVM for Active Learning (F Lap − T W SV MAL) where the
authors proposed to identify the most informative and representative training
points whose labels are queried for domain experts for annotations. Once the
corresponding labels are acquired, this limited labeled and unlabeled data are
used to train a fast model that involves solving a QPP and an unconstrained
minimization problem to seek the classiﬁcation hyperplanes.

24

M. Tanveer et al.

3.11 Twin Support Vector Machine for Clustering

Wang et al. [206] formulated twin support vector clustering algorithm (TSVC)
build upon TWSVM. It divides the data samples into k clusters such that the
data samples are around k cluster center points. It exploits the information
from clusters (both between and within clusters) and the center planes are
determined by solving a series of QPP. Authors in [206] also proposed a nearest
neighbor graph (NNG)-based initialization to make the model more stable and
eﬃcient. Improved TSVC [207] decomposed the multiclass clustering problem
into multiple two cluster problems.

In 2018, Khemchandani et al. [133] formulated fuzzy least squares version
of TSVC in which each data point has a fuzzy membership value and is allotted
to diﬀerent clusters. The proposed algorithm solves primal problems instead
of dual problems in TSVC. Experimental results shown that the proposed
algorithm obtains comparable clustering accuracy to that of TSVC but has
less training time.

Khemchandani and Pal [208] replaced the hinge loss function of TSVC
with weighted linear loss and introduced a regularization term in TSVC which
needs to solve linear equations and also implements the structural risk com-
ponent. The proposed algorithm called weighted linear loss TSVC achieves
higher accuracy than TSVC. The loss function proposed in [208] is not contin-
uous and therefore, authors in [209] modiﬁed the weighted linear loss function
to form a continuous loss function and implemented TSVC and WLL-TSVC
in semi-supervised framework and also introduced a fuzzy extension of semi-
supervised WLL-TSVC to make a robust algorithm which is less sensitive to
noise. Experimental results have demonstrated that the proposed algorithm
achieved better clustering accuracy and less computational time than TSVC
and WLL-TSVC. Tree-based localized fuzzy twin support vector clustering
(Tree-TSVC) was proposed by [210]. Tree-TSVC is a novel clustering algo-
rithm that builds the cluster that represents a node on a binary tree, where
each node comprises of proposed TWSVM based classiﬁer. Due to the tree
structure and the formulation that leads to solving a series of systems of linear
equations, Tree-TSVC model is eﬃcient in terms of training time and accuracy.
TSVC uses squared L2-norm distance which leads to noise sensitivity. Also,
each cluster plane learning iteration requires to solve a QPP which makes it
computationally complex. To address these issues in TSVC, Ye et al. [211] in
2018 used L1 norm distance and proposed L1-norm distance minimization-
based robust TSVC which only deals with linear equations instead of series of
QPPs. Authors in [211] further proposed RTSVC and Fast RTSVC to speed
up the computation of TSVC in nonlinear case. Numerical experiments shown
that this model has higher accuracy as compared to other k-clustering meth-
ods and has less computational time than TSVC. Wang et al. [212] proposed
ramp-based TSVC by introducing the ramp cost function in TSVC. The solu-
tion of the proposed algorithm is obtained by solving non-convex programming
problem using an alternating iteration algorithm. In 2019, Bai et al. [213] intro-
duced regularization in clustering and proposed large margin TSVC. Authors

M. Tanveer et al.

25

Fig. 2 Applications of Twin Support Vector Classiﬁcation

in [213] proposed a fast least squares TSVC with uniform coding output. The
algorithm achieves better performance than TSVC and other plane-clustering
methods. Wang et al. [214] in 2019 proposed a general model for plane-based
clustering which includes various extensions of k-plane clustering (kPC). It
optimizes the problem by minimizing the total loss which is derived from both
within-cluster and between-cluster. It can capture data distribution accurately.
In 2020, Richhariya and Tanveer [215] proposed projection based least square
TSVC (LSPTSVC) and employed the concave-convex procedure (CCCP) to
solve the optimization problem. LSPTSVC only needs to solve a set of linear
equations and thus leads to signiﬁcantly less computational time. TSVC uses
hinge loss function, hence suﬀers from the issues of noise and has low sampling
stability. To overcome these issues, pinball loss twin support vector clustering
[216] used pinball loss to penalize the samples. However, it suﬀers from the
issues of overﬁtting as it minimises the empirical risk. Hence, pinball loss twin
bounded support vector clustering [217] introduced the regularisation term to
minimise the structural risk and avoids the issues of overﬁtting. Introduction
of pinball loss function leads to loss of sparsity in the model. Hence, sparse
pinball loss TSVC [218, 219] used (cid:15)-insensitive pinball loss function to make
the model sparse. It implements the empirical risk minimisation principle and
thus suﬀers due to overﬁtting. To minimise the overﬁtting issues, sparse pin-
ball twin bounded support vector clustering [220] used the regularisation term
to minimise the structural risk and hence, avoid the issues of overﬁtting.

26

M. Tanveer et al.

4 Applications of Twin Support Vector

Classiﬁcation

TWSVM has been implemented to solve many real-life classiﬁcation challenges
and has shown promising results in some applications.

4.1 Biomedical applications

TWSVM has been applied to detect breast cancer in early stages by detecting a
mass in digital mammograms [221]. A hybrid feature selection based approach
has also been implemented for detecting breast cancer, Hepatitis, and Diabetes
[53]. Wang et al. [222] proposed TWSVM in combination with dual-tree com-
plex wavelet transform (DTCWT) method for Pathological Brain Detection.
TWSVM is also implemented for detecting cardiac diseases, one such applica-
tion was proposed by Houssein et al. in [223], heartbeats were detected using
Swarm-TWSVM and this algorithm achieved better accuracy than TWSVM.
Refahi et al. [224] used LSTWSVM and DAG LS-TWSVM classiﬁers for pre-
dicting arrhythmia heart disease. Chandra and Bedi [225] proposed linear
norm fuzzy based TWSVM for color based classiﬁcation of human skin which
achieved better accuracy than other conventional methods. Xu et al. [226] pro-
posed semi-supervised TWSVM for detection of Acute-on-chronic liver failure
(ACLF). Also, many researchers have applied TWSVM to detect Alzheimer’s
disease in its early stages [51, 222, 227–234].

4.2 Alzheimer’s Disease Prediction

In 2020, Richhariya and Tanveer [235] proposed an angle based universum
least squares TWSVM (AULSTWSVM) which performed with 95% accuracy
for detecting Alzheimer’s disease (AD). Authors [236] also proposed universum
support vector machine based recursive feature elimination (USVM-RFE) for
detecting AD. Khan et al. [237] proposed an approach to improve the classiﬁca-
tion accuracy in mild cognitive impairment (MCI), normal control (NC), and
AD subjects using structural magnetic resonance imaging (sMRI). Authors
used FreeSurfer to process MRI data and derive cortical features which are
used in TWSVM, LSTWSVM and RELS-TSVM to detect AD. Sharma et al.
[238] proposed fuzzy LS-TWSVM based deep learning network for prognosis
of the Alzheimer’s disease.

4.3 Speaker Recognition

Cong et al.
[239] formulated multi-class TWSVM for speaker recognition
with feature extraction based on gaussian mixture models (GMMs). It gives
better results than traditional SVM. Zhendong and Yang [240] proposed multi-
TWSVM which ﬁnd hyperplane for every class and takes constraints from
other classes separately on the QPP. This algorithm performed better than
many other algorithms for speaker recognition.

M. Tanveer et al.

27

4.4 Text Categorization

Kumar and Gopal [39] formulated a least squares TWSVM and experiments
have shown the validity of the model for text applications. Francis and Sreenath
[241] proposed manifold regularized TWSVM for text recognition and the pro-
posed method achieved highest accuracy among SVM, LSTWSVM and other
methods. Non-parallel SVM [242] and eﬃcient pinball TWSVM [243] has also
demonstrated better performance in text categorization.

4.5 Intrusion Detection

It is a system which monitors or protects the network against any malicious
activity. It has been a critical component for network security. Researchers
[244–247] applied TWSVM for intrusion detection and results have shown that
it achieves better accuracy than other intrusion detection algorithms.

4.6 Human activity recognition

Khemchandani and Sharma [50] proposed least square TWSVM for human
activity recognition which gives promising results even with the outliers. Nasiri
et al. [47] formulated energy-based LS-TWSVM algorithm. Khemchandani and
Sharma [248] also proposed robust parametric twin support vector machine
which can eﬀectively deal with the noise. Mozafari et al. [249] used the Har-
ris detector algorithm and applied LS-TWSVM for action recognition and
achieved the highest accuracy than other state-of-the-art methods. Kumar and
Rajagopal [250] proposed Multi-class TWSVM for detecting human face hap-
piness combined with Constrained Local Model. Authors [251] also proposed
semi-supervised multi TWSVM to predict human facial emotions with 13 min-
imal features that can detect six basic human emotions. Algorithm achieved
highest accuracy and least computation time with minimal feature vectors.

4.7 Image Denoising

Yang et al. [252] proposed edge/texture-preserving image denoising based on
TWSVM which is very eﬀective to preserve the informative features such as
edges and textures and better than other image denoising methods available.
Shahdoosti and Hazavei [253] proposed a ripplet formulation of the total vari-
ation method for denoising images. This algorithm attains promising results
in improving the image quality in terms of both subjective and objective
inspections.

4.8 Electroencephalogram (EEG)

Classiﬁcation of electroencephalogram (EEG) for diﬀerent mental activities
has been an active research topic. Richhariya and Tanveer [254] proposed uni-
versum TWSVM which is insensitive to outliers as it selects universum from
the EEG datasets itself to generate universum data points which remove the
eﬀect of outliers. Soman and Jayadeva [255] used the classiﬁable metric to

28

M. Tanveer et al.

Table 4 Optimization framework for TWSVM algorithms

Method

GEPSVM [270]

TWSVM [24]

TBSVM [61]

LSTWSVM [39]

Pin-TWSVM
[99]

Pin-GTSVM
[108]

NPSVM [88]

Par-TWSVM
[75]



Regularization
term
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)









wy

by







wy

by







(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

0

(cid:107)wy(cid:107)2

(cid:107)wy(cid:107)2

(cid:107)wy(cid:107)2

(cid:107)wy(cid:107)2

D(gy(Xy), 0)

D(gy(Xj ), gy(Xy)) D(gy(x)) Distance

(cid:107)gy(Xy)(cid:107)2

(cid:107)gy(Xj )(cid:107)2

|gy (x)|
(cid:107)wy(cid:107)

(cid:107)·(cid:107)

(cid:107)gy(Xy)(cid:107)2

eT (cid:107)gy(Xj ) − e(cid:107)+

|gy(x)|

(cid:107)·(cid:107)/(.)+

(cid:107)gy(Xy)(cid:107)2

eT (cid:107)gy(Xj ) − e(cid:107)+

|gy (x)|
(cid:107)wy(cid:107)

(cid:107)·(cid:107)/(.)+

(cid:107)gy(Xy)(cid:107)2

(cid:107)gy(Xj ) − e(cid:107)2

|gy(x)|

(cid:107)·(cid:107)

Lτ (Xy, y, gy, (Xy))

gy(Xj )1

(cid:107)gy(Xy)(cid:107)2

Lτ (Xy, y, gy, (Xy))

|gy (x)|
(cid:107)wy(cid:107)

|gy (x)|
(cid:107)wy(cid:107)

(cid:107)·(cid:107)/LT (.)/(cid:107)·(cid:107)1

(cid:107)·(cid:107)/LT (.)/(cid:107)·(cid:107)1

(cid:107)gy(Xy) − (cid:15)(cid:107)1

eT (gy(Xj ) − e)+

|gy(x)|

(cid:107)·(cid:107)/(cid:107)·(cid:107)1/(.)+

eT gy(Xj )

(cid:107)gy(Xy)(cid:107)+

|gy(x)|

(cid:107)·(cid:107)

choose discriminative frequency bands and used the TWSVM to learn imbal-
anced datasets. Tanveer et al. [256] proposed entropy based features in Flexible
analytic wavelet transform (FAWT) framework and RELS-TSVM [48] for clas-
siﬁcation to detect epileptic seizure EEG. Tanveer et al. [257] used FAWT
framework for classiﬁcation of seizure and seizure-free EEG signals with Hjorth
parameters as features and implemented TWSVM, LSTWSVM and RELS-
TSVM [48] for classifying signals. Li et al. [258] proposed LSTWSVM with
a frequency band selection common spatial pattern algorithm for detecting
motor imagery electroencephalography. This algorithm achieved faster training
time compared to other SVM baseline algorithms. Some more researchers have
applied TWSVM for EEG classiﬁcation, biometric identiﬁcation and other leak
detection challenges [258–264]

4.9 Other Applications

Cao et al. [265] proposed improved TWSVM with multi-objective cuckoo
search to predict software bugs. Authors employed TWSVM to predict
defected modules and used cuckoo search to optimize TWSVM and this
proposed method achieved better accuracy than other software defect pre-
diction methods. Chu et al. [266] proposed Multi-information TWSVM for
detecting steel surface defects. The TWSVM models have also been used in
image recognition or face recognition [87, 130, 158, 267] and facial expression
recognition [268] and privacy preservation [269].

Table 5 Properties of diﬀerent TWSVM Algorithms

Models \ Characteristics

SRM Sparsity Matrix Inversion

Insensitive to noise

M. Tanveer et al.

29

TWSVM [24]

TBSVM [61]

LSTWSVM [39]

RELS-TSVM [48]

Projection TWSVM [69]

TPMSVM [75]

Pin-GTSVM [108]

SP-TWSVM [94]

Pin-TPMSVM [99]

ISPTWSVM [110]

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

The pinball loss function is deﬁned as follows:

Lτ (Xy, y, gy, (Xy)) =






eT (0 − ygy(Xy)),

(0 − ygy(Xy)) ≥ 0,

(10)

−τ eT (ygy(Xy) − 0), (0 − ygy(Xy)) < 0.

where gy(x) = g(x; wy, by) = wT

y x + by = 0, D(gy(A, 0)) denotes the
intraclass distance which represents objective function while D(gy(A), gy(B))
is interclass distance which corresponds to constraints, D(gy(x)) is the
perpendicular distance of point x from the hyperplane gy(x) = 0.

Optimization framework of various TWSVM algorithms are discussed in

Table 4 [20].

Table 5 shows the diﬀerences in major TWSVM methods based on the

SRM principle, sparsity, matrix inversion and noise insensitivity.

5 Basic theory of Twin Support Vector

Regression

Peng [36] in 2010 proposed an eﬃcient twin support vector regression (TSVR)
algorithm in line with TWSVM, called twin support vector regresson (TSVR).
Like TWSVM, it also requires to solve two QPPs. It ﬁnds an end regressor

30

M. Tanveer et al.

that is the mean of ε-insensitive up and bound functions. TSVR has less com-
putational time than a standard SVR and has better generalization ability.
The down- and up-bound functions for linear case is given below:
For any x ∈ Rn, the two hyperplanes are deﬁned as follows:

f1(x) = uT

1 x + b1

and f2(x) = uT

2 x + b2,

(11)

The two QPPs in linear case are deﬁned as below:

min
(u1,b1,ζ1)∈Rn+1+m

s.t.

and

(cid:107)y − eε1 − (Au1 + b1e)(cid:107)2 + C1eT ζ1

1
2
y − (Au1 + b1e) ≥ eε1 − ζ1,

ζ1 ≥ 0

(12)

min
(u2,b2,ζ2)∈Rn+1+m

s.t.

(cid:107)y + eε2 − (Au2 + b2e)(cid:107)2 + C2eT ζ2

1
2
(Au2 + b2e) − y ≥ eε2 − ζ2, ζ2 ≥ 0,

(13)

here, C1, C2 > 0; ε1, ε2 > 0 and ζ1, ζ2 are slack variables. The ﬁnal regressor
is the mean of up and down regressors in (11), which is given as follows

f (x) =

1
2

(f1(x) + f2(x))

for all x ∈ Rn.

(14)

For nonlinear case, kernel surfaces are used rather than hyperplanes which

are given below:

f1(x) = k(xT , AT )u1 + b1

and f2(x) = k(xT , AT )u2 + b2.

(15)

The two QPPs in non-linear case are deﬁned as below:

min
(u1,b1,ζ1)∈Rm+1+m

s.t.

and

(cid:13)y − eε1 − (k(A, AT )u1 + b1e)(cid:13)
(cid:13)
2
(cid:13)

1
2
y − (k(A, AT )u1 + b1e) ≥ eε1 − ζ1, ζ1 ≥ 0

+ C1eT ζ1

min
(u2,b2,ζ2)∈Rm+1+m

s.t.

(cid:13)y + eε2 − (k(A, At)u2 + b2e)(cid:13)
(cid:13)
(cid:13)

1
2
(k(A, AT )u2 + b2e) − y ≥ eε2 − ζ2, ζ2 ≥ 0.

+ C2eT ζ2

2

(16)

(17)

For more details, one can refer to [36].

M. Tanveer et al.

31

Although taking motivation from TWSVM formulation, Peng [36]
attempted to propose TSVR where the regressor is obtained via solving a pair
of quadratic programming problems (QPPs). However, later authors in [37]
argued that TSVR formulation is not in the true spirit of TWSVM. Further,
taking the motivation from Bi and Bennett [271], they proposed an alterna-
tive approach to ﬁnd a formulation for TSVR which is in the true spirit of
TWSVM. They have shown that their proposed TSVR formulation can be
derived from TWSVM for an appropriately constructed classiﬁcation problem.

6 Research progress on Twin Support Vector

Regression

In this section, we discuss the progress of twin SVM based models for the
regression problems.

6.1 Weighted Twin Support Vector Regression

Xu and Wang [272] introduced weighted TSVR which gives diﬀerent weights
to samples and have diﬀerent inﬂuence over bound functions. Computational
results have demonstrated that this algorithm avoids over-ﬁtting and also
yields good generalization ability. The authors [196] also proposed K nearest
neighbor based weighted TSVR (KNNWTSVR) which gives diﬀerent penal-
ties to the samples based on their local information on number of K-nearest
neighbors. The weights are assigns based on number of K-nearest neigh-
bors. KNNWTSVR has better accuracy but similar computational complexity
as its optimization is similar to TSVR. However, the above algorithm only
implements empirical risk minimization and suﬀer from the inverse of posi-
tive semi-deﬁnite matrices. To overcome these limitations, Tanveer et al. [273]
introduced an eﬃcient regularized KNNWTSVR (RKNNWTSVR) algorithm
to make it strongly convex. RKNNWTSVR leads to better generalization
and robust solution. Computational cost is also reduced as it only deals with
linear equations. To overcome noise sensitivity in TSVR, Ye et al. [274] intro-
duced weighted matrix in Lagrangian (cid:15) twin support vector regression. It uses
quadratic loss functions and provides diﬀerent weights to samples through
weighted matrix. It obtains better generalization and also has less training
time than TSVR and (cid:15)-TSVR models.

6.2 Projection Twin Support Vector Regression

TSVR [36] and twin parametric insensitive SVR [275] have obtained better
generalization performance than classical SVR but both these algorithms only
implement empirical risk minimization and do not include any prior informa-
tion about the data samples which can lead to noise sensitivity. Thus, Peng et
al. [276] proposed an eﬃcient twin projection SVR (TPSVR) algorithm which
exploits the prior structural information of data into the learning process. It
seeks two projection axes such that projected points have as small as possible

32

M. Tanveer et al.

empirical variance values on the down-and up-bound functions. This algorithm
has better generalization and requires small number of support vectors. The
aforementioned models, use uniform weighting approach and assumes that all
the samples are equally important. However, this assumption may be wrong
due to outliers and noise. Hence, wavelet weighted projection TWSVM for
regression [277, 278] used wavelet based weights to reduce the eﬀect of outliers.

6.3 Robust and Sparse Twin Support Vector Regression

Although TSVR has proven to be an eﬀective classiﬁer with good generaliza-
tion ability, it is less robust due to the square of the 2-norm in the QPP of
TSVR. In 2012, Zhong et al. [279] improved TSVR by using 1-norm rather
than 2-norm distance in TSVR’s QPP. It has less training time and better
generalization. Chen et al. [280] formulated smooth TSVR (STSVR) using
smooth function in order to make the QPP of TSVR positive deﬁnite to obtain
a unique global solution. Authors converted the QPPs to unconstrained mini-
mization problems (UMPs) and applied Newton method to solve it eﬀectively.
All these algorithms still have high computational time due to quadratic or lin-
ear programming problems. To avoid this shortcoming, a least squares version
of TSVR (TLSSVR) was formulated by Zhao et al. [281] which leads to faster
computational speed as it only deals with set of linear equations. Authors also
proposed sparse TLSSVR.

In 2014, Chen et al. [282] introduced the regularization into the formula-
tion of TSVR and implemented l1-norm loss function to make it robust and
sparse simultaneously. Huang et al. [283] formulated a sparse version of least
square TSVR by introducing a regularization term to make it strongly convex
and also converted the primal problems to linear programming problems. This
leads to a sparse solution with signiﬁcantly less computational time. In 2017,
Tanveer [284] formulated 1-norm TSVR to improve robustness and sparsity
in original TSVR. 1-norm TSVR has better accuracy, generalization, and less
computational time than TSVR. In 2020, Singla et al. [285] proposed a novel
TSVR (Res-TSVR) which is robust and not sensitive to noise in data. The
optimization problem is non-convex with smooth l2 regularizer and thus, to
solve it eﬃciently, the authors converted it to a convex optimization problem.
Res-TSVR performed best as compared to other robust TSVR algorithms in
terms of robustness to noise and better generalization. Gu et al. [286] also
proposed a TSVR variant suitable to handle noise called fast clustering-based
weighted TSVR (FCWTSVR) which classify the samples into diﬀerent cat-
egories based on their similarities and provides diﬀerent weights to samples
located at diﬀerent positions. The proposed algorithm performed better than
TSVR, ε-TSVR, KNNWTSVR and WL-ε-TSVR. (cid:15)-non parallel support vec-
tor regression [287] uses two (cid:15)-tubes for better alignment of hyperplanes and
to get the more robust upbound and down bound regressor. Robust huber
loss based twin SVR [288] which penalizes the large deviation samples linearly
and small error samples are squared. This results in robustness to noise and
outliers.

M. Tanveer et al.

33

6.4 Other improvements on Twin Support Vector

Regression

TSVR has proven to provide better generalization results but it needs to solve
two QPPs which increases the learning time for TSVR. Thus, Peng [289] for-
mulated a primal TSVR (P-TSVR), which only deals with linear equations.
This improves the learning speed of TSVR and shows comparable generaliza-
tion. Further, to increase the sparsity of TSVR, the author introduced the
back-ﬁtting strategy for optimizing the unconstrained QPP. TSVR requires
two set of constraints one with each QPP which increases the computational
time for large datasets. To overcome this disadvantage, Singh et al. [290] intro-
duced rectangular kernels in the formulation of TSVR and proposed reduced
TSVR which resulted in the signiﬁcant saving of computational time and thus
promoting its application for large datasets. To further reduce computational
time, a LS version of TSVR (TLSSVR) was formulated by Zhao et al. [281]
which only deals with a set of linear equations. Authors also proposed sparse
TLSSVR.

Huang and Ding [291] further attempted to reduce the computational time
by proposing LS-TWSVM in primal space (TLSSVR) rather than dual space
(PLSTSVR). This also requires to ﬁnd solution of two linear equations and
has comparable accuracy to TSVR. To make TSVR suitable to handle het-
eroscedastic noise structure, Peng [275] proposed twin parametric insensitive
SVR (TPISVR) which determines a set of nonparallel parametric-insensitive
up and down-bound functions. It also works eﬀectively when noise depends
upon the input value. It requires to solve two SVM-type problems, which
increases its learning speed. Computational results showed that it also has
good generalization ability. Shao et al. [30] implemented the SRM principle
in TSVR primal space and proposed a new regressor (cid:15)–TSVR which seeks to
ﬁnd a pair of (cid:15)-insensitive proximal functions. Further, to reduce complex-
ity, the successive over-relaxation (SOR) technique is employed. Experimental
results show that (cid:15)−TSVR has better generalization and fast training speed
than TSVR.

Balasundaram and Tanveer [292] proposed linearly convergent Lagrangian
TSVR (LTSVR) algorithm. Experimental results have exhibited its suitabil-
ity and applicability due to the better generalization and less computational
time than TSVR. Inspired by this algorithm, Balasundaram and Gupta [293]
proposed Lagrangian dual of the 2-norm TSVR. Results have demonstrated
an increase in learning speed with better accuracy when compared to TSVR.
Tanveer et al. [273] introduced the regularization term to the objective func-
tion of TSVR and formulated regularized Lagrangian TSVR (IRLTSVR). This
algorithm implements the SRM principle and requires to solve linear system
of equations in place of QPP in TSVR. Optimization problems are positive
deﬁnite and avoid the singularity in the solution. It has better accuracy and
speed than conventional TSVR.

34

M. Tanveer et al.

Balasundaram and Tanveer [294] proposed smooth Newton method for
LTSVR which needs to solve linear equations in each iteration using Newton-
Armijo algorithm. It has comparable generalization ability but it is at least
two times faster than TSVR. Khemchandani et al. [295] proposed TSVR for
simultaneous learning. This algorithm is more accurate, computationally less
complex and more robust as it uses l1 norm error. Lagrangian twin parametric
insensitive twin SVR [296, 297] employed l2 norm of the square variables, also
it is faster as it uses linearly convergent iterative scheme for obtaining the
end regressor. Asymmetric possibility and necessity regression by twin-support
vector networks [298] and reularization based twin SVR with huber loss [299]
improved the generalization performance of the end regressor.

Peng et al. [300] implemented the use of interval data to handle interval
input-output data (ITSVR). Rastogi et al. [301, 302] provided an extension
of ν-SVR i.e ν-TWSVR and large margin distribution machine based regres-
sion that it is in the true spirit of TWSVM. Balasundaram and Meena [303]
proposed unconstrained TSVR formulation in the primal space (UP-TSVR)
which is speed and obtains better generalization than TSVR. This model is
solved by a gradient based iterative approach.

Parastalooi et al. [304] proposed a modiﬁed version of TSVR by includ-
ing the structural information from data and its distribution. Clustering is
done based on the mean and covariance matrix of the data which increases
accuracy. Furthermore, to increase the training speed, authors applied SOR
technique and also optimized parameter selection by implementing PSO algo-
rithm. Rastogi et al. [305] proposed an improved version of ν-TSVR which
can automatically adjust the values of upper and lower bound parameters to
attain better accuracy. Experimental results have shown the superiority of the
proposed algorithm over (cid:15)-TSVR.

TSVR gives same weights to all the samples but in fact, diﬀerent positions
will inﬂuence diﬀerently on the regressor which are ignored in TSVR. Thus,
Xu et al. [306] proposed asymmetric ν-TSVR based on pinball loss function.
Pinball loss function gives diﬀerent penalties to the points lying above and
below the bounds. It is insensitive to noise and also has better generalization
ability. Tanveer and Shubham [307] added regularization term in TSVR in the
primal form which yields better accuracy and more robust solution.

7 Applications of Twin Support Vector

Regression

Ye et al. [310] implemented L1 − (cid:15)- TSVR for forecasting inﬂation. This algo-
rithm proved to be excellent for feature ranking and determined important
features for inﬂation in China. Experimental results showed that its accuracy is
better than the ordinary least square (OLS) models. Ye et al. [311] also imple-
mented (cid:15)-wavelet TSVR for inﬂation forecast. Authors employed the wavelet
kernel that can be used for any curve in quadratic continuous integral space.
This algorithm derives lower root mean squared error (RMSE) and thus, is an

Table 6 Performance of various non-linear twin support vector regression (TSVR) based
algorithms. [273]

M. Tanveer et al.

35

Datasets

Models

IBM

SVR [308]

WSVR [309]

TSVR [36]

RMSE

0.1283

0.0459

0.0765

KNNWTSVR [196]

0.0330

RKNNWTSVR [273]

0.0330

Intel

SVR [308]

WSVR [309]

TSVR [36]

0.0500

0.0384

0.0405

KNNWTSVR [196]

0.0971

RKNNWTSVR [273]

0.0382

SNP-500

SVR [308]

WSVR [309]

TSVR [36]

0.0311

0.0296

0.0288

KNNWTSVR [196]

0.0296

RKNNWTSVR [273]

0.0273

MAE

0.0920

0.0379

0.0551

0.0243

0.0243

0.0406

0.0295

0.0314

0.0791

0.0285

0.0253

0.0222

0.0212

0.0219

0.0193

SSE/SST

SSR/SST

0.2872

0.0416

0.1125

0.0217

0.0217

0.0552

0.0330

0.0368

0.1922

0.0328

0.0192

0.0174

0.0166

0.0174

0.0148

0.4055

0.8365

1.3937

1.0549

1.0549

0.8174

1.0524

0.8208

0.5566

0.8390

0.8825

1.064

0.9702

0.9904

0.9907

eﬃcient method for inﬂation forecast. Ding et al. [312] predicted stock prices
using polynomial smooth twin support vector regression. Numerical exper-
iments reveal that this algorithm can obtain better regression performance
compared with SVR and TSVR. Le et al. [313] proposed a novel genetic algo-
rithm (GA) based TSVR to improve the precision of indoor positioning. It
performs better than K- nearest neighbor and neural network but comparable
to SVR with signiﬁcantly less computational time. Houssein [314] proposed
particle swarm optimization (PSO) based TSVR for forecasting wind speed.
The computational results proved that it achieves better forecasting accuracy
and outperformed genetic algorithm with TSVR and TSVR using radial basis
kernel function. In 2018, Wang and Xu [315] proposed safe screening rule
(SSR) based on variational inequality (VI) to make TSVR eﬃcient for large-
scale problems as SSR reduces computational time signiﬁcantly. Authors also
implemented dual coordinate descent method (DCDM) to further increase the
computational speed of TSVR. Improved twin SVR [316, 317] was formulated
for brain age prediction. Twin SVR models have been benchmarked for the
prediction of brain age in Alzheimer’s disease [318].

Table 3 shows the diﬀerences in major TSVR methods based on the SRM

principle, Sparsity, Matrix Inversion and Noise Insensitivity.

36

M. Tanveer et al.

Table 7 Performance of various non-linear twin support vector regression (TSVR) based
algorithms [307]

Datasets

Models

Gas furnace

SVR [308]

IBM

Intel

TSVR [36]

LTSVR [292]

RLTSVR [307]

SVR [308]

TSVR [36]

LTSVR [292]

RLTSVR [307]

SVR [308]

TSVR [36]

LTSVR [292]

RLTSVR [307]

RMSE

0.0700

0.0578

0.0634

0.0636

0.1283

0.0765

0.0328

0.0330

0.0500

0.0405

0.0343

0.0336

MAE

0.0459

0.0389

0.0427

0.0439

0.0920

0.0551

0.0241

0.0245

0.0406

0.0314

0.0257

0.0251

SSE/SST

SSR/SST

0.088

0.0599

0.0718

0.0718

0.2872

0.1125

0.0214

0.0217

0.0552

0.0368

0.0266

0.0255

0.8719

0.8744

0.8689

0.8623

0.4055

1.3937

1.0567

1.0604

0.8174

0.8208

0.9180

0.9421

Table 8 Properties of diﬀerent TSVR Algorithms

Models \ Characteristics

SRM Sparsity Matrix Inversion

Insensitive to Noise

TSVR [36]

TWSVR [37]

(cid:15)−TSVR [30]

WTSVR [272]

LTSVR [292]

RLTSVR [307]

PTSVR [289]

KNNWTSVR [196]

RKNNWTSVR [273]

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

8 Future research and development prospects

TWSVM classiﬁcation algorithms have high training speed and accuracy than
conventional SVM but it’s still in the primitive stage of development and lacks
practical application background. TWSVM has low generalization ability and
also lacks in sparsity. Therefore, TWSVM needs further research and improve-
ments to eﬀectively apply to real-life challenges. Future research prospects for
TWSVM can be as follows :

M. Tanveer et al.

37

• An interesting aspect can be coupling other machine learning algorithms
with the TWSVM. For example, a deep convolutional neural network can
extract features which can be classiﬁed using TWSVM with better accuracy.
• There is limited research on TWSVM applications for large-scale classiﬁca-
tion. Thus, how to develop TWSVM algorithms for big data classiﬁcation
problems eﬀectively is worthwhile.

• For non-linear classiﬁcation problems, TWSVM performance highly depends
upon kernel function selection and there is not enough research on this to
guide researchers to choose kernels as per diﬀerent applications to get desired
accuracy and performance of the TWSVM algorithm. Thus, kernel selection
and optimal parameters selection need further study and improvement.
• Currently, only few TWSVM algorithms have been implemented for multi-
class classiﬁcation but it leads to class imbalance problem and often losses
sparsity. Thus, further study is required for TWSVM implementation for
multi-class classiﬁcation.

• The main concept of GEPSVM/TWSVM is based on linear discriminant
analysis (LDA). A well cross study on TWSVM and LDA is worthy of future
work.

• TWSVM applications to health care is currently limited. Thus, how to
implement TWSVM eﬀectively for early diagnosis of human diseases like
Alzheimer, Epilepsy, Breast cancer etc is worthy of study.

• Clustering, which aims at dividing the data samples into diﬀerent clusters,
is major fundamental problem in classiﬁcation. Clustering based TWSVM
approach is less studied currently and needs further study and development.
• There is limited research on TWSVM applications for remote-sensing. Thus,

how to build eﬃcient classiﬁers for remote-sensing can be explored.

TSVR is much faster than conventional SVR and also has better gener-
alization ability. But, it suﬀers from a lack of model complexity control and
results in over-ﬁtting. It also losses sparsity similar to TWSVM and is sensitive
to outliers. Further research on TSVR can be on the following:
• A signiﬁcant limitation of TSVR is high computational time as it loses the
sparsity. More work is required to ﬁnd an eﬃcient sparse TSVR algorithm.
• Data cleaning, transforming and pre-processing is an important issue for
every machine learning technique and can tremendously improve results
and even help in identifying novel interactions within data. As TSVR is
a relatively new technique, various data handling, cleaning, pre-processing
techniques like missing value imputation can be explored for improving the
performance of TSVR.

• TSVR is evaluated only on few types of continuous variable problems. Appli-
cation of TSVR can be explored on a wide range of problems in diﬀerent
domains.

• The current TSVR requires oﬀ-setting of multiple hyperparameters and
hence optimal parameter selection is an issue. Thus, further research in the
direction of identifying and choosing parameters should be done.

38

M. Tanveer et al.

Acknowledgments. This work is supported by Science and Engineering
Research Board (SERB) funded Research Projects, Government of India
under Early Career Research Award Scheme, Grant No. ECR/2017/000053
and Ramanujan Fellowship Scheme, Grant No. SB/S2/RJN-001/2016, and
Council of Scientiﬁc & Industrial Research (CSIR), New Delhi, INDIA under
Extra Mural Research (EMR) Scheme grant no. 22(0751)/17/EMR-II. We
gratefully acknowledge the Indian Institute of Technology Indore for provid-
ing facilities and support. Yuan-Hai Shao acknowledges the ﬁnancial support
by the National Natural Science Foundation of China (11871183, 61866010,
11926349).

References

[1] Cortes, C., Vapnik, V.: Support vector networks. Machine Learning

20(3), 273–297 (1995)

[2] Cristianini, N., Shawe-Taylor, J.: An introduction to support vector

machines and other kernel-based learning methods (2000)

[3] Tong, S., Koller, D.: Support vector machine active learning with appli-
cations to text classiﬁcation. Journal of Machine Learning Research
2(Nov), 45–66 (2001)

[4] Agarwal, S., Tomar, D., Siddhant: Prediction of software defects using
twin support vector machine. In: 2014 International Conference on Infor-
mation Systems and Computer Networks (ISCON), pp. 128–132 (2014).
IEEE

[5] Sch¨olkopf, B., Tsuda, K., Vert, J.-P.: Support vector machine applica-

tions in computational biology (2004)

[6] Tay, F.E., Cao, L.: Application of support vector machines in ﬁnancial

time series forecasting. omega 29(4), 309–317 (2001)

[7] Gupta, D., Pratama, M., Ma, Z., Li, J., Prasad, M.: Financial time series
forecasting using twin support vector regression. PloS one 14(3), 0211402
(2019)

[8] Noble, W.S.: Support vector machine applications in computational

biology. Kernel Methods in Computational Biology 71, 92 (2004)

[9] Osuna, E., Freund, R., Girosi, F.: Training support vector machines: an

application to face detection. In: CVPR, p. 130 (1997). IEEE

[10] Hua, S., Sun, Z.: Support vector machine approach for protein subcellular

localization prediction. Bioinformatics 17(8), 721–728 (2001)

M. Tanveer et al.

39

[11] Byvatov, E., Schneider, G.: Support vector machine applications in

bioinformatics. Applied Bioinformatics 2(2), 67–77 (2003)

[12] Morra, J.H., Tu, Z., Apostolova, L.G., Green, A.E., Toga, A.W., Thomp-
son, P.M.: Comparison of adaboost and support vector machines for
detecting Alzheimer disease through automated hippocampal segmenta-
tion. IEEE Transactions on Medical Imaging 29(1), 30–43 (2010)

[13] Tanveer, M., Sharma, S., Rastogi, R., Anand, P.: Sparse support vector
machine with pinball loss. Wiley Transactions on Emerging Telecom-
munications Technologies (ETT) https://doi.org/10.1002/ett.3820
(2020)

[14] Vapnik, V.: The nature of statistical learning theory (2013)

[15] Edelman, D.: Adapting support vector machine methods for horserace
odds prediction. Annals of Operations Research 151(1), 325–336 (2007)

[16] Poursaeidi, M.H., Kundakcioglu, O.E.: Robust support vector machines
for multiple instance learning. Annals of Operations Research 216(1),
205–227 (2014)

[17] Le Thi, H.A., Nguyen, M.C.: DCA based algorithms for feature selection
in multi-class support vector machine. Annals of Operations Research
249(1-2), 273–300 (2017)

[18] Wang, S., Jiang, W., Tsui, K.-L.: Adjusted support vector machines
based on a new loss function. Annals of Operations Research 174(1),
83–101 (2010)

[19] Li, C.-N., Ren, P.-W., Shao, Y.-H., Ye, Y.-F., Guo, Y.-R.: General-
ized elastic net lp-norm nonparallel support vector machine. Engineering
Applications of Artiﬁcial Intelligence 88, 103397 (2020)

[20] Li, C.-N., Shao, Y.-H., Wang, H., Zhao, Y.-T., Huang, L.-W., Xiu, N.,
Deng, N.-Y.: Single versus union: Non-parallel support vector machine
frameworks. arXiv preprint arXiv:1910.09734 (2019)

[21] Mangasarian, O.L., Wild, E.W.: Proximal support vector machine classi-
ﬁers. In: Proceedings KDD-2001: Knowledge Discovery and Data Mining.
(2001). Citeseer

[22] Khemchandani, R., Chandra, S., et al.: Generalized eigenvalue proximal
support vector machines. In: Twin Support Vector Machines, pp. 25–42.
Springer, ??? (2017)

[23] Shao, Y.-H., Deng, N.-Y., Chen, W.-J., Wang, Z.: Improved generalized

40

M. Tanveer et al.

eigenvalue proximal support vector machine. IEEE Signal Processing
Letters 20(3), 213–216 (2013)

[24] Jayadeva, Khemchandani, R., Chandra, S.: Twin support vector
machines for pattern classiﬁcation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 29(5), 905–910 (2007)

[25] Jayadeva, R.K., Chandra, S.: Twin support vector machines: Models,

extension and applications 659 (2016)

[26] Huang, H., Wei, X., Zhou, Y.: Twin support vector machines: A survey.

Neurocomputing 300, 34–43 (2018)

[27] Basak, D., Pal, S., Patranabis, D.C.: Support vector regression. Neural
Information Processing-Letters and Reviews 11(10), 203–224 (2007)

[28] Drucker, H., Burges, C.J., Kaufman, L., Smola, A.J., Vapnik, V.: Sup-
port vector regression machines. In: Advances in Neural Information
Processing Systems, pp. 155–161 (1997)

[29] Smola, A.J., Sch¨olkopf, B.: A tutorial on support vector regression.

Statistics and Computing 14(3), 199–222 (2004)

[30] Shao, Y.-H., Zhang, C.-H., Yang, Z.-M., Jing, L., Deng, N.-Y.: An
ε-twin support vector machine for regression. Neural Computing and
Applications 23(1), 175–185 (2013)

[31] Chuang, C.-C.: Fuzzy weighted support vector regression with a fuzzy
partition. IEEE Transactions on Systems, Man, and Cybernetics, Part
B (Cybernetics) 37(3), 630–640 (2007)

[32] Chang, C.-C., Lin, C.-J.: Training ν-support vector regression: theory

and algorithms. Neural Computation 14(8), 1959–1977 (2002)

[33] Tanveer, M., Mangal, M., Ahmad, I., Shao, Y.-H.: One norm linear pro-
gramming support vector regression. Neurocomputing 173, 1508–1518
(2016)

[34] Yang, H., Huang, K., King, I., Lyu, M.R.: Localized support vec-
tor regression for time series prediction. Neurocomputing 72(10-12),
2659–2669 (2009)

[35] Elattar, E.E., Goulermas, J., Wu, Q.H.: Electric load forecasting based
on locally weighted support vector regression. IEEE Transactions on Sys-
tems, Man, and Cybernetics, Part C (Applications and Reviews) 40(4),
438–447 (2010)

[36] Peng, X.: TSVR: an eﬃcient twin support vector machine for regression.

M. Tanveer et al.

41

Neural Networks 23(3), 365–372 (2010)

[37] Khemchandani, R., Goyal, K., Chandra, S.: TWSVR: regression via twin

support vector machine. Neural Networks 74, 14–21 (2016)

[38] Kuhn, H.W., Tucker, A.W.: Nonlinear programming. Proceedings of 2nd
Berkeley Symposium. Berkeley: University of California Press, 481–492
(1951)

[39] Kumar, M.A., Gopal, M.: Least squares twin support vector machines
for pattern classiﬁcation. Expert Systems with Applications 36(4), 7535–
7543 (2009)

[40] Xu, Y., Lv, X., Wang, Z., Wang, L.: A weighted least squares twin
support vector machine. J. Inf. Sci. Eng. 30(6), 1773–1787 (2014)

[41] Kumar, M.A., Khemchandani, R., Gopal, M., Chandra, S.: Knowledge
based least squares twin support vector machines. Information Sciences
180(23), 4606–4618 (2010)

[42] Wang, D., Ye, Q., Ye, N.: Localized multi-plane TWSVM classiﬁer via
manifold regularization. In: 2010 Second International Conference on
Intelligent Human-Machine Systems and Cybernetics, vol. 2, pp. 70–73
(2010). IEEE

[43] Gao, S., Ye, Q., Ye, N.: 1-norm least squares twin support vector

machines. Neurocomputing 74(17), 3590–3597 (2011)

[44] Xu, Y., Xi, W., Lv, X., Guo, R.: An improved least squares twin support
vector machine. Journal of Information and Computational Science 9(4),
1063–1071 (2012)

[45] Yuan, C., Yang, L.: Capped l2,p-norm metric based robust least squares
twin support vector machine for pattern classiﬁcation. Neural Networks
142, 457–478 (2021)

[46] Ye, Q., Ye, N., Gao, S.: Density-based weighting multi-surface least
squares classiﬁcation with its applications. Knowledge and Information
Systems 33(2), 289–308 (2012)

[47] Nasiri, J.A., Charkari, N.M., Mozafari, K.: Energy-based model of least
squares twin support vector machines for human action recognition.
Signal Processing 104, 248–257 (2014)

[48] Tanveer, M., Khan, M.A., Ho, S.-S.: Robust energy-based least squares
twin support vector machines. Applied Intelligence 45(1), 174–186
(2016)

42

M. Tanveer et al.

[49] Tanveer, M., Gautam, C., Suganthan, P.N.: Comprehensive evaluation
of twin svm based classiﬁers on UCI datasets. Applied Soft Computing
83, 105617 (2019)

[50] Khemchandani, R., Sharma, S.: Robust least squares twin support vector
machine for human activity recognition. Applied Soft Computing 47,
33–46 (2016)

[51] Tomar, D., Agarwal, S.: Feature selection based least square twin support
vector machine for diagnosis of heart disease. International Journal of
Bio-Science and Bio-Technology 6(2), 69–82 (2014)

[52] Tomar, D., Singhal, S., Agarwal, S.: Weighted least square twin sup-
port vector machine for imbalanced dataset. International Journal of
Database Theory and Application 7(2), 25–36 (2014)

[53] Tomar, D., Agarwal, S.: Hybrid feature selection based weighted least
squares twin support vector machine approach for diagnosing breast can-
cer, hepatitis, and diabetes. Advances in Artiﬁcial Neural Systems, 1
(2015)

[54] Xu, Y., Pan, X., Zhou, Z., Yang, Z., Zhang, Y.: Structural least square
twin support vector machine for classiﬁcation. Applied Intelligence
42(3), 527–536 (2015)

[55] Chen, P.-H., Lin, C.-J., Sch¨olkopf, B.: A tutorial on ν-support vector
machines. Applied Stochastic Models in Business and Industry 21(2),
111–136 (2005)

[56] Peng, X.: A ν-twin support vector machine (ν-TSVM) classiﬁer and its
geometric algorithms. Information Sciences 180(20), 3863–3875 (2010)

[57] Qi, Z., Tian, Y., Shi, Y.: Structural twin support vector machine for

classiﬁcation. Knowledge-Based Systems 43, 74–81 (2013)

[58] Mei, B., Xu, Y.: Multi-task least squares twin support vector machine

for classiﬁcation. Neurocomputing (2019)

[59] Peng, X., Xu, D.: A twin-hypersphere support vector machine classiﬁer
and the fast learning algorithm. Information Sciences 221, 12–27 (2013)

[60] Tanveer, M., Sharma, S., Muhammad, K.: Large scale least squares twin
svms. ACM Transactions on Internet Technology 21 (2020). https://doi.
org/10.1145/3398379

[61] Shao, Y.-H., Zhang, C.-H., Wang, X.-B., Deng, N.-Y.: Improvements on
twin support vector machines. IEEE Transactions on Neural Networks

M. Tanveer et al.

43

22(6), 962–968 (2011)

[62] Chen, X., Yang, J., Ye, Q., Liang, J.: Recursive projection twin sup-
port vector machine via within-class variance minimization. Pattern
Recognition 44(10-11), 2643–2655 (2011)

[63] Shao, Y.-H., Deng, N.-Y., Yang, Z.-M.: Least squares recursive projec-
tion twin support vector machine for classiﬁcation. Pattern Recognition
45(6), 2299–2307 (2012)

[64] Suykens, J., Lukas, L., Van Dooren, P., De Moor, B., Vandewalle, J.:
Least squares support vector machine classiﬁers: a large scale algorithm.
In: European Conference on Circuit Theory and Design, ECCTD, vol.
99, pp. 839–842 (1999). Citeseer

[65] Shao, Y.-H., Wang, Z., Chen, W.-J., Deng, N.-Y.: A regularization for
the projection twin support vector machine. Knowledge-Based Systems
37, 203–210 (2013)

[66] Ding, S., Hua, X.: Recursive least squares projection twin support vector
machines for nonlinear classiﬁcation. Neurocomputing 130, 3–9 (2014)

[67] Guo, J., Yi, P., Wang, R., Ye, Q., Zhao, C.: Feature selection for least
squares projection twin support vector machine. Neurocomputing 144,
174–183 (2014)

[68] Hua, X., Ding, S.: Weighted least squares projection twin support vector
machines with local information. Neurocomputing 160, 228–237 (2015)

[69] Hua, X., Xu, S., Gao, J.: A novel projection twin support vector
machine for pattern recognition. In: Smart Cities Conference (ISC2),
2017 International (2017). IEEE

[70] Chen, W.-J., Shao, Y.-H., Li, C.-N., Wang, Y.-Q., Liu, M.-Z., Wang,
Z.: NPrSVM: nonparallel sparse projection support vector machine with
eﬃcient algorithm. Applied Soft Computing 90, 106142 (2020)

[71] Chen, W.-J., Shao, Y.-H., Li, C.-N., Liu, M.-Z., Wang, Z., Deng, N.-
Y.: ν-projection twin support vector machine for pattern classiﬁcation.
Neurocomputing 376, 10–24 (2020)

[72] Ren, Q., Yang, L.: A robust projection twin support vector machine with

a generalized correntropy-based loss. Applied Intelligence, 1–17 (2021)

[73] Ye, Q., Zhao, C., Gao, S., Zheng, H.: Weighted twin support vector
machines with local information and its application. Neural Networks
35, 31–39 (2012)

44

M. Tanveer et al.

[74] Hao, P.-Y.: New support vector algorithms with parametric insensitive/-

margin model. Neural Networks 23(1), 60–73 (2010)

[75] Peng, X.: TPMSVM: a novel twin parametric-margin support vector
machine for pattern recognition. Pattern Recognition 44(10-11), 2678–
2692 (2011)

[76] Wang, Z., Shao, Y.-H., Wu, T.-R.: A GA-based model selection for
smooth twin parametric-margin support vector machine. Pattern Recog-
nition 46(8), 2267–2277 (2013)

[77] Wang, H., Xu, Y., Zhou, Z.: Twin-parametric margin support vector
machine with truncated pinball loss. Neural Computing and Applications
33(8), 3781–3798 (2021)

[78] Shao, Y.-H., Wang, Z., Chen, W.-J., Deng, N.-Y.: Least squares twin
parametric-margin support vector machine for classiﬁcation. Applied
Intelligence 39(3), 451–464 (2013)

[79] Peng, X., Wang, Y., Xu, D.: Structural twin parametric-margin support
vector machine for binary classiﬁcation. Knowledge-Based Systems 49,
63–72 (2013)

[80] Peng, X., Kong, L., Chen, D.: Improvements on twin parametric-margin

support vector machine. Neurocomputing 151, 857–863 (2015)

[81] Rastogi, R., Sharma, S., Chandra, S.: Robust parametric twin sup-
port vector machine for pattern classiﬁcation. Neural Processing Letters
47(1), 293–323 (2018)

[82] Khemchandani, R., Saigal, P., Chandra, S.: Angle-based twin support

vector machines. Annals of OR 269, 387–417 (2018)

[83] Rastogi, R., Saigal, P., Chandra, S.: Angle-based twin parametric-margin
support vector machine for pattern classiﬁcation. Knowledge-Based
Systems 139, 64–77 (2018)

[84] Shao, Y.-H., Chen, W.-J., Wang, Z., Li, C.-N., Deng, N.-Y.: Weighted
linear loss twin support vector machine for large-scale classiﬁcation.
Knowledge-Based Systems 73, 276–288 (2015)

[85] Peng, X.: Building sparse twin support vector machine classiﬁers in

primal space. Information Sciences 181(18), 3967–3980 (2011)

[86] Peng, X., Xu, D.: Robust minimum class variance twin support vector
machine classiﬁer. Neural Computing and Applications 22(5), 999–1011
(2013)

M. Tanveer et al.

45

[87] Qi, Z., Tian, Y., Shi, Y.: Robust twin support vector machine for pattern

classiﬁcation. Pattern Recognition 46(1), 305–316 (2013)

[88] Tian, Y., Ju, X., Qi, Z.: Eﬃcient sparse nonparallel support vector
machines for classiﬁcation. Neural Computing and Applications 24(5),
1089–1099 (2014)

[89] Zhang, Z., Zhen, L., Deng, N., Tan, J.: Sparse least square twin support
vector machine with adaptive norm. Applied Intelligence 41(4), 1097–
1107 (2014)

[90] L´opez, J., Maldonado, S., Carrasco, M.: Robust nonparallel support vec-
tor machines via second-order cone programming. Neurocomputing 364,
227–238 (2019)

[91] Tanveer, M.: Robust and sparse linear programming twin support vector

machines. Cognitive Computation 7(1), 137–149 (2015)

[92] Tanveer, M.: Application of smoothing techniques for linear program-
ming twin support vector machines. Knowledge and Information Systems
45(1), 191–214 (2015)

[93] Tanveer, M.: Smoothing technique on linear programming twin sup-
port vector machines. International Journal of Machine Learning and
Computing 3(2), 240 (2013)

[94] Tanveer, M., Tiwari, A., Choudhary, R., Jalan, S.: Sparse pinball twin

support vector machines. Applied Soft Computing 78, 164–175 (2019)

[95] Wang, C., Ye, Q., Luo, P., Ye, N., Fu, L.: Robust capped l1-norm twin

support vector machine. Neural Networks 114, 47–59 (2019)

[96] Ma, J., Yang, L., Sun, Q.: Capped l1-norm distance metric-based fast
robust twin bounded support vector machine. Neurocomputing 412,
295–311 (2020)

[97] Yan, H., Ye, Q.-L., Yu, D.-J.: Eﬃcient and robust TWSVM classiﬁca-
tion via a minimum l1-norm distance metric criterion. Machine Learning
108(6), 993–1018 (2019)

[98] Ma, J., Yang, L., Sun, Q.: Adaptive robust learning framework for twin
support vector machine classiﬁcation. Knowledge-Based Systems 211,
106536 (2021)

[99] Xu, Y., Yang, Z., Pan, X.: A novel twin support-vector machine with pin-
ball loss. IEEE Transactions on Neural Networks and Learning Systems
28(2), 359–370 (2016)

46

M. Tanveer et al.

[100] Xu, Y., Yang, Z., Zhang, Y., Pan, X., Wang, L.: A maximum margin and
minimum volume hyper-spheres machine with pinball loss for imbalanced
data classiﬁcation. Knowledge-Based Systems 95, 75–85 (2016)

[101] Sharma, S., Rastogi, R.: Insensitive zone based pinball loss twin sup-
port vector machine for pattern classiﬁcation. In: 2018 IEEE Symposium
Series on Computational Intelligence (SSCI), pp. 2238–2245 (2018).
IEEE

[102] Yang, Z., Pan, X., Xu, Y.: Piecewise linear solution path for pinball twin
support vector machine. Knowledge-Based Systems 160, 311–324 (2018)

[103] Li, K., Lv, Z.: Smooth twin bounded support vector machine with pinball

loss. Applied Intelligence, 1–17 (2021)

[104] Xu, Y., Wang, Q., Pang, X., Tian, Y.: Maximum margin of twin spheres
machine with pinball loss for imbalanced data classiﬁcation. Applied
Intelligence 48(1), 23–34 (2018)

[105] Yuan, M., Xu, Y.: Bound estimation-based safe acceleration for max-
loss. Pattern

imum margin of twin spheres machine with pinball
Recognition 114, 107860 (2021)

[106] Sharma, S., Rastogi, R., Chandra, S.: Large-scale twin parametric sup-
port vector machine using pinball loss function. IEEE Transactions on
Systems, Man, and Cybernetics: Systems (2019)

[107] Pant, H., Sharma, M., Soman, S.: Twin neural networks for the clas-
siﬁcation of large unbalanced datasets. Neurocomputing 343, 34–49
(2019)

[108] Tanveer, M., Sharma, A., Suganthan, P.: General twin support vector
machine with pinball loss function. Information Sciences 494, 311–327
(2019)

[109] Ganaie, M., Tanveer, M.: Robust general twin support vector machine
with pinball loss function. Machine Learning for Intelligent Multimedia
Analytics, 103–125 (2021)

[110] Tanveer, M., Rajani, T., Ganaie, M.: Improved sparse pinball twin
SVM. In: 2019 IEEE International Conference on Systems, Man, and
Cybernetics (SMC) (2019). IEEE

[111] Tanveer, M., Tiwari, A., Choudhary, R., Ganaie, M.: Large-scale pinball

twin support vector machines. Machine Learning, 1–24 (2021)

[112] Qi, Z., Tian, Y., Shi, Y.: Twin support vector machine with universum

M. Tanveer et al.

47

data. Neural Networks 36, 112–119 (2012)

[113] Qi, Z., Tian, Y., Shi, Y.: A nonparallel support vector machine for a clas-
siﬁcation problem with universum learning. Journal of Computational
and Applied Mathematics 263, 288–298 (2014)

[114] Shao, Y.-H., Chen, W.-J., Zhang, J.-J., Wang, Z., Deng, N.-Y.: An eﬃ-
cient weighted Lagrangian twin support vector machine for imbalanced
data classiﬁcation. Pattern Recognition 47(9), 3158–3167 (2014)

[115] Xu, Y., Chen, M., Li, G.: Least squares twin support vector machine
with universum data for classiﬁcation. International Journal of Systems
Science 47(15), 3637–3645 (2016)

[116] Cao, L., Shen, H.: Combining re-sampling with twin support vector
machine for imbalanced data classiﬁcation. In: 2016 17th International
Conference on Parallel and Distributed Computing, Applications and
Technologies (PDCAT), pp. 325–329 (2016). IEEE

[117] Huang, L.-W., Shao, Y.-H., Zhang, J., Zhao, Y.-T., Teng, J.-Y.: Robust
rescaled hinge loss twin support vector machine for imbalanced noisy
classiﬁcation. IEEE Access 7, 65390–65404 (2019)

[118] Hazarika, B.B., Gupta, D.: Density weighted twin support vector
machines for binary class imbalance learning. Neural Processing Letters,
1–40 (2021)

[119] Xu, Y., Chen, M., Yang, Z., Li, G.: ν-twin support vector machine with
universum data for classiﬁcation. Applied Intelligence 44(4), 956–968
(2016)

[120] Xu, Y.: Maximum margin of twin spheres support vector machine for
imbalanced data classiﬁcation. IEEE Transactions on Cybernetics 47(6),
1540–1550 (2017)

[121] Richhariya, B., Sharma, A., Tanveer, M.: Improved universum twin sup-
port vector machine. In: 2018 IEEE Symposium Series on Computational
Intelligence (SSCI), pp. 2045–2052 (2018). IEEE

[122] Richhariya, B., Tanveer, M.: A fuzzy universum support vector machine
based on information entropy. Machine Intelligence and Signal Analysis,
569–582 (2019)

[123] Richhariya, B., Tanveer, M.: A reduced universum twin support vector
machine for class imbalance learning. Pattern Recognition 102, 107150
(2020)

48

M. Tanveer et al.

[124] Richhariya, B., Tanveer, M.: Universum least squares twin parametric
margin support vector machine. 2020 International Joint Conference on
Neural Networks (IJCNN), 1–8 (2020)

[125] Richhariya, B., Tanveer, M.: A fuzzy universum least squares twin
support vector machine (FULSTSVM). Neural Computing and Applica-
tions, 1–12 (2021)

[126] Borah, P., Gupta, D., Prasad, M.: Improved 2-norm based fuzzy least
squares twin support vector machine. In: 2018 IEEE Symposium Series
on Computational Intelligence (SSCI), pp. 412–419 (2018). IEEE

[127] Ganaie, M., Tanveer, M., Initiative, A.D.N.: Fuzzy least squares projec-
tion twin support vector machines for class imbalance learning. Applied
Soft Computing, 107933 (2021)

[128] Gupta, D., Richhariya, B.: Entropy based fuzzy least squares twin sup-
port vector machine for class imbalance learning. Applied Intelligence
48(11), 4212–4231 (2018)

[129] Borah, P., Gupta, D.: Robust twin bounded support vector machines for

outliers and imbalanced data. Applied Intelligence, 1–30 (2021)

[130] Chen, S.-G., Wu, X.-J.: A new fuzzy twin support vector machine for
pattern classiﬁcation. International Journal of Machine Learning and
Cybernetics 9(9), 1553–1564 (2018)

[131] Richhariya, B., Tanveer, M.: A robust fuzzy least squares twin support
vector machine for class imbalance learning. Applied Soft Computing
71, 418–432 (2018)

[132] Ganaie, M.A., Tanveer, M., Suganthan, P.N.: Regularized robust fuzzy
least squares twin support vector machine for class imbalance learning.
In: 2020 International Joint Conference on Neural Networks, IJCNN, pp.
1–8 (2020). IEEE

[133] Khemchandani, R., Pal, A., Chandra, S.: Fuzzy least squares twin
support vector clustering. Neural Computing and Applications 29(2),
553–563 (2018)

[134] Chen, S., Cao, J., Chen, F., Liu, B.: Entropy-based fuzzy least squares
twin support vector machine for pattern classiﬁcation. Neural Processing
Letters 51(1), 41–66 (2020)

[135] Rezvani, S., Wang, X., Pourpanah, F.: Intuitionistic fuzzy twin support
vector machines. IEEE Transactions on Fuzzy Systems 27(11), 2140–
2151 (2019)

M. Tanveer et al.

49

[136] Zhang, H., Li, H.: Fuzzy twin support vector machine based on intra-
class hyperplane. In: Journal of Physics: Conference Series, vol. 1302, p.
032016 (2019). IOP Publishing

[137] Gupta, D., Richhariya, B., Borah, P.: A fuzzy twin support vector
machine based on information entropy for class imbalance learning.
Neural Computing and Applications 31(11), 7153–7164 (2019)

[138] Fan, Q., Wang, Z., Li, D., Gao, D., Zha, H.: Entropy-based fuzzy support
vector machine for imbalanced datasets. Knowledge-Based Systems 115,
87–99 (2017)

[139] Li, K., Ma, H.: A fuzzy twin support vector machine algorithm.
International Journal of Application or Innovation in Engineering and
Management (IJAIEM) 2(3), 459–465 (2013)

[140] Kumar, M.A., Gopal, M.: Application of smoothing technique on twin
support vector machines. Pattern Recognition Letters 29(13), 1842–1848
(2008)

[141] Khemchandani, R., Jayadeva, Chandra, S.: Optimal kernel selection in
twin support vector machines. Optimization Letters 3(1), 77–88 (2009)

[142] Zhang, X.: Boosting twin support vector machine approach for MCs
detection. In: 2009 Asia-Paciﬁc Conference on Information Processing,
vol. 1, pp. 149–152 (2009). IEEE

[143] Qi, Z., Tian, Y., Shi, Y.: Robust twin support vector machine for pattern

classiﬁcation. Pattern Recognition 46(1), 305–316 (2013)

[144] Li, D., Tian, Y.: Twin support vector machine in linear programs.

Procedia Computer Science 29, 1770–1778 (2014)

[145] Shao, Y.-H., Yang, Z.-X., Wang, X.-B., Deng, N.-Y.: Multiple instance
twin support vector machines. Lect Note Oper Res 12, 433–442 (2010)

[146] Ghorai, S., Hossian, S.J., Mukherjee, A., Dutta, P.K.: Unity norm
twin support vector machine classiﬁer. In: 2010 Annual IEEE India
Conference (INDICON), pp. 1–4 (2010). IEEE

[147] Moosaei, H., Ketabchi, S., Razzaghi, M., Tanveer, M.: Generalized twin
support vector machines. Neural Processing Letters 53(2), 1545–1564
(2021)

[148] Xu, Y., Guo, R.: An improved ν-twin support vector machine. Applied

intelligence 41(1), 42–54 (2014)

[149] Xu, Y., Wang, L., Zhong, P.: A rough margin-based ν-twin support

50

M. Tanveer et al.

vector machine. Neural Computing and Applications 21(6), 1307–1317
(2012)

[150] Xu, Y., Yu, J., Zhang, Y.: KNN-based weighted rough ν-twin support

vector machine. Knowledge-Based Systems 71, 303–313 (2014)

[151] Khemchandani, R., Saigal, P., Chandra, S.: Improvements on ν-twin

support vector machine. Neural Networks 79, 97–107 (2016)

[152] Xie, X., Sun, S.: Multitask centroid twin support vector machines.

Neurocomputing 149, 1085–1091 (2015)

[153] Sharma, S., Rastogi, R.: Stochastic conjugate gradient descent twin sup-
port vector machine for large scale pattern classiﬁcation, pp. 590–602
(2018). Springer

[154] Wang, Z., Shao, Y.-H., Bai, L., Li, C.-N., Liu, L.-M., Deng, N.-Y.: Insen-
sitive stochastic gradient twin support vector machines for large scale
problems. Information Sciences 462, 114–131 (2018)

[155] Ye, Q., Zhao, C., Ye, N., Chen, X.: Localized twin SVM via convex

minimization. Neurocomputing 74(4), 580–587 (2011)

[156] Shao, Y.-H., Deng, N.-Y.: A novel margin-based twin support vec-
tor machine with unity norm hyperplanes. Neural Computing and
Applications 22(7-8), 1627–1635 (2013)

[157] Gupta, U., Gupta, D.: Lagrangian twin-bounded support vector machine
based on l2-norm. In: Recent Developments in Machine Learning and
Data Analytics, pp. 431–444. Springer, ??? (2019)

[158] Peng, X., Xu, D.: Twin mahalanobis distance-based support vector
machines for pattern recognition. Information Sciences 200, 22–37
(2012)

[159] Shao, Y.-H., Deng, N.-Y., Yang, Z.-M., Chen, W.-J., Wang, Z.: Prob-
abilistic outputs for twin support vector machines. Knowledge-Based
Systems 33, 145–151 (2012)

[160] Shao, Y.-H., Deng, N.-Y.: A coordinate descent margin based-twin sup-
port vector machine for classiﬁcation. Neural Networks 25, 114–121
(2012)

[161] Pan, X., Luo, Y., Xu, Y.: K-nearest neighbor based structural twin

support vector machine. Knowledge-Based Systems 88, 34–44 (2015)

[162] Xie, F., Xu, Y.: An eﬃcient regularized k-nearest neighbor structural
twin support vector machine. Applied Intelligence 49(12), 4258–4275

M. Tanveer et al.

51

(2019)

[163] Shao, Y.-H., Chen, W.-J., Deng, N.-Y.: Nonparallel hyperplane support
vector machine for binary classiﬁcation problems. Information Sciences
263, 22–35 (2014)

[164] Ding, S., Zhang, X., Yu, J.: Twin support vector machines based on fruit
ﬂy optimization algorithm. International Journal of Machine Learning
and Cybernetics 7(2), 193–203 (2016)

[165] Pan, X., Yang, Z., Xu, Y., Wang, L.: Safe screening rules for accelerating
twin support vector machine classiﬁcation. IEEE Transactions on Neural
Networks and Learning Systems 29(5), 1876–1887 (2017)

[166] Yang, Z., Xu, Y.: A safe sample screening rule for laplacian twin
parametric-margin support vector machine. Pattern Recognition 84,
1–12 (2018)

[167] Pang, X., Xu, Y.: A safe screening rule for accelerating weighted twin
support vector machine. Soft Computing 23(17), 7725–7739 (2019)

[168] Zhao, J., Xu, Y., Fujita, H.: An improved non-parallel universum sup-
port vector machine and its safe sample screening rule. Knowledge-Based
Systems 170, 79–88 (2019)

[169] Tanveer, M.: Newton method for implicit Lagrangian twin support vector
machines. International Journal of Machine Learning and Cybernetics
6(6), 1029–1040 (2015)

[170] Tanveer, M., Shubham, K.: Smooth twin support vector machines via

unconstrained convex minimization. Filomat 31(8), 2195–2210 (2017)

[171] Chen, W.-J., Shao, Y.-H., Li, C.-N., Deng, N.-Y.: MLTSVM: a novel twin
support vector machine to multi-label learning. Pattern Recognition 52,
61–74 (2016)

[172] Azad-Manjiri, M., Amiri, A., Sedghpour, A.S.: ML-SLSTSVM: a new
least square twin support vector machine for multi-label

structural
learning. Pattern Analysis and Applications 23(1), 295–308 (2020)

[173] Rastogi, R., Pal, A., Chandra, S.: Generalized pinball

loss SVMs.

Neurocomputing 322, 151–165 (2018)

[174] Ganaie, M., Tanveer, M.: LSTSVM classiﬁer with enhanced features from
pre-trained functional link network. Applied Soft Computing 93, 106305
(2020)

[175] Ganaie, M.A., Tanveer, M., Suganthan, P.N.: Oblique decision tree

52

M. Tanveer et al.

ensemble via twin bounded SVM. Expert Systems with Applications
143, 113072 (2020)

[176] Ganaie, M., Hu, M., Malik, A.K., Tanveer, M., Suganthan, P.N.:
Ensemble deep learning: A review. arXiv preprint arXiv:2104.02395
(2021)

[177] Tanveer, M., Ganaie, M., Suganthan, P.: Ensemble of classiﬁcation mod-
els with weighted functional link network. Applied Soft Computing 107,
107322 (2021)

[178] Xu, Y., Guo, R., Wang, L.: A twin multi-class classiﬁcation support

vector machine. Cognitive Computation 5(4), 580–588 (2013)

[179] Yang, Z.-X., Shao, Y.-H., Zhang, X.-S.: Multiple birth support vector
machine for multi-class classiﬁcation. Neural Computing and Applica-
tions 22(1), 153–161 (2013)

[180] Xie, J., Hone, K., Xie, W., Gao, X., Shi, Y., Liu, X.: Extending twin sup-
port vector machine classiﬁer for multi-category classiﬁcation problems.
Intelligent Data Analysis 17(4), 649–664 (2013)

[181] Shao, Y.-H., Chen, W.-J., Huang, W.-B., Yang, Z.-M., Deng, N.-Y.: The
best separating decision tree twin support vector machine for multi-class
classiﬁcation. Procedia Computer Science 17, 1032–1038 (2013)

[182] Xu, Y., Guo, R.: A twin hyper-sphere multi-class classiﬁcation support
vector machine. Journal of Intelligent & Fuzzy Systems 27(4), 1783–1790
(2014)

[183] Nasiri, J.A., Charkari, N.M., Jalili, S.: Least squares twin multi-class
classiﬁcation support vector machine. Pattern Recognition 48(3), 984–
992 (2015)

[184] Xu, Y.: K-nearest neighbor-based weighted multi-class twin support

vector machine. Neurocomputing 205, 430–438 (2016)

[185] Tomar, D., Agarwal, S.: An eﬀective weighted multi-class least squares
twin support vector machine for imbalanced data classiﬁcation. Inter-
national Journal of Computational Intelligence Systems 8(4), 761–778
(2015)

[186] Ali, J., Aldhaifallah, M., Nisar, K.S., Aljabr, A., Tanveer, M.: Regu-
larized least squares twin svm for multiclass classiﬁcation. Big Data
Research 27, 100295 (2022)

M. Tanveer et al.

53

[187] Tomar, D., Agarwal, S.: A comparison on multi-class classiﬁcation meth-
ods based on least squares twin support vector machine. Knowledge-
Based Systems 81, 131–147 (2015)

[188] Yang, Z.-M., Wu, H.-J., Li, C.-N., Shao, Y.-H.: Least squares recur-
sive projection twin support vector machine for multi-class classiﬁcation.
International Journal of Machine Learning and Cybernetics 7(3), 411–
426 (2016)

[189] Li, C.-N., Huang, Y.-F., Wu, H.-J., Shao, Y.-H., Yang, Z.-M.: Multiple
recursive projection twin support vector machine for multi-class classiﬁ-
cation. International Journal of Machine Learning and Cybernetics 7(5),
729–740 (2016)

[190] Ding, S., Zhao, X., Zhang, J., Zhang, X., Xue, Y.: A review on multi-class

TWSVM. Artiﬁcial Intelligence Review, 1–27 (2017)

[191] Ai, Q., Wang, A., Zhang, A., Wang, Y., Sun, H.: A multi-class classiﬁca-
tion weighted least squares twin support vector hypersphere using local
density information. IEEE Access 6, 17284–17291 (2018)

[192] Pang, X., Xu, C., Xu, Y.: Scaling KNN multi-class twin support vector
machine via safe instance reduction. Knowledge-Based Systems 148, 17–
30 (2018)

[193] de Lima, M.D., Costa, N.L., Barbosa, R.: Improvements on least squares
twin multi-class classiﬁcation support vector machine. Neurocomputing
313, 196–205 (2018)

[194] Qiang, W., Zhang, J., Zhen, L., Jing, L.: Robust weighted linear loss
twin multi-class support vector regression for large-scale classiﬁcation.
Signal Processing 170, 107449 (2020)

[195] Tanveer, M., Sharma, A., Suganthan, P.N.: Least squares KNN-based

weighted multiclass twin SVM. Neurocomputing 459, 454–464 (2021)

[196] Xu, Y., Wang, L.: K-nearest neighbor-based weighted twin support

vector regression. Applied Intelligence 41(1), 299–309 (2014)

[197] Du, S.-W., Zhang, M.-C., Chen, P., Sun, H.-F., Chen, W.-J., Shao, Y.-
H.: A multiclass nonparallel parametric-margin support vector machine.
Information 12(12), 515 (2021)

[198] Gao, Z., Fang, S.-C., Gao, X., Luo, J., Medhin, N.: A novel kernel-
free least squares twin support vector machine for fast and accurate
multi-class classiﬁcation. Knowledge-Based Systems 226, 107123 (2021)

54

M. Tanveer et al.

[199] Qi, Z., Tian, Y., Shi, Y.: Laplacian twin support vector machine for
semi-supervised classiﬁcation. Neural Networks 35, 46–53 (2012)

[200] Chen, W.-J., Shao, Y.-H., Deng, N.-Y., Feng, Z.-L.: Laplacian least
squares twin support vector machine for semi-supervised classiﬁcation.
Neurocomputing 145, 465–476 (2014)

[201] Chen, W.-J., Shao, Y.-H., Hong, N.: Laplacian smooth twin support
vector machine for semi-supervised classiﬁcation. International Journal
of Machine Learning and Cybernetics 5(3), 459–468 (2014)

[202] Khemchandani, R., Pal, A.: Multi-category Laplacian least squares twin
support vector machine. Applied Intelligence 45(2), 458–474 (2016)

[203] Yang, Z., Xu, Y.: Laplacian twin parametric-margin support vec-
tor machine for semi-supervised classiﬁcation. Neurocomputing 171,
325–334 (2016)

[204] Khemchandani, R., Pal, A.: Tree based multi-category Laplacian
TWSVM for content based image retrieval. International Journal of
Machine Learning and Cybernetics 8(4), 1197–1210 (2017)

[205] Rastogi, R., Sharma, S.: Fast Laplacian twin support vector machine
with active learning for pattern classiﬁcation. Applied Soft Computing
74, 424–439 (2019)

[206] Wang, Z., Shao, Y.-H., Bai, L., Deng, N.-Y.: Twin support vector
machine for clustering. IEEE Transactions on Neural Networks and
Learning Systems 26(10), 2583–2588 (2015)

[207] Moezzi, S., Jalali, M., Forghani, Y.: TWSVC+: Improved twin support
vector machine-based clustering. Ing´enierie des Syst`emes d’Information,
24 (5): 463-471 (2019)

[208] Khemchandani, R., Pal, A.: Weighted linear loss twin support vector
clustering. In: Proceedings of the 3rd IKDD Conference on Data Science,
2016, p. 18 (2016). ACM

[209] Rastogi, R., Pal, A.: Fuzzy semi-supervised weighted linear loss twin
support vector clustering. Knowledge-Based Systems 165, 132–148
(2019)

[210] Rastogi, R., Saigal, P.: Tree-based localized fuzzy twin support vector
clustering with square loss function. Appl. Intelligence 47(1), 96–113
(2017)

[211] Ye, Q., Zhao, H., Li, Z., Yang, X., Gao, S., Yin, T., Ye, N.: L1-norm

M. Tanveer et al.

55

distance minimization-based fast robust twin support vector k-plane
clustering. IEEE Transactions on Neural Networks and Learning Systems
29(9), 4494–4503 (2018)

[212] Zhen Wang, Y.-H.S. Xu Chen, Li, C.-N.: Ramp-based twin
clustering. Neural Computing and Applications

support vector
https://doi.org/10.1007/s00521-019-04511-3 (2019)

[213] Bai, L., Shao, Y.-H., Wang, Z., Li, C.-N.: Clustering by twin support vec-
tor machine and least square twin support vector classiﬁer with uniform
output coding. Knowledge-Based Systems 163, 227–240 (2019)

[214] Wang, Z., Shao, Y.-H., Bai, L., Li, C.-N., Liu, L.-M.: A general
model for plane-based clustering with loss function. arXiv preprint
arXiv:1901.09178 (2019)

[215] Richhariya, B., Tanveer, M., Initiative, A.D.N.: Least squares projection
twin support vector clustering (LSPTSVC). Information Sciences 533,
1–23 (2020)

[216] Tanveer, M., Gupta, T., Shah, M., Initiative, A.D.N.: Pinball loss twin
support vector clustering. ACM Transactions on Multimedia Computing,
Communications, and Applications (TOMM) 17(2s), 1–23 (2021)

[217] Tanveer, M., Tabish, M., Jangir, J.: Pinball twin bounded support vector
clustering. In: 2021 IEEE EMBS International Conference on Biomedical
and Health Informatics (BHI), pp. 1–4 (2021). IEEE

[218] Tanveer, M., Gupta, T., Shah, M., Richhariya, B.: Sparse twin support
vector clustering using pinball loss. IEEE Journal of Biomedical and
Health Informatics (2021). https://doi.org/10.1109/JBHI.2021.3059910

[219] Tanveer, M., Tabish, M., Jangir, J.: Sparse pinball twin bounded support
vector clustering. IEEE Transactions on Computational Social Systems
PP (2021). https://doi.org/10.1109/TCSS.2021.3122828

[220] Tanveer, M., Tabish, M., Jangir, J.: Pinball twin bounded support vector
clustering. In: 2021 IEEE EMBS International Conference on Biomedical
and Health Informatics (BHI), pp. 1–4 (2021). IEEE

[221] Si, X., Jing, L.: Mass detection in digital mammograms using twin
support vector machine-based CAD system. In: 2009 WASE Interna-
tional Conference on Information Engineering, vol. 1, pp. 240–243 (2009).
IEEE

[222] Wang, S., Lu, S., Dong, Z., Yang, J., Yang, M., Zhang, Y.: Dual-tree com-
plex wavelet transform and twin support vector machine for pathological

56

M. Tanveer et al.

brain detection. Applied Sciences 6(6), 169 (2016)

[223] Houssein, E.H., Ewees, A.A., ElAziz, M.A.: Improving twin support
vector machine based on hybrid swarm optimizer for heartbeat clas-
siﬁcation. Pattern Recognition and Image Analysis 28(2), 243–253
(2018)

[224] Refahi, M.S., Nasiri, J.A., Ahadi, S.: ECG arrhythmia classiﬁcation using
least squares twin support vector machines. In: Electrical Engineering
(ICEE), Iranian Conference On, pp. 1619–1623 (2018). IEEE

[225] Chandra, M.A., Bedi, S.: A twin support vector machine based approach
to classifying human skin. In: 2018 4th International Conference on
Computing Communication and Automation (ICCCA), pp. 1–5 (2018).
IEEE

[226] Xu, Y., Zhang, Y., Yang, Z., Pan, X., Li, G.: Imbalanced and semi-
supervised classiﬁcation for prognosis of aclf. Journal of Intelligent &
Fuzzy Systems 28(2), 737–745 (2015)

[227] Tanveer, M., Richhariya, B., Khan, R.U., Rashid, A.H., Khanna, P.,
Prasad, M., Lin, C.-T.: Machine learning techniques for the diagno-
sis of Alzheimer’s disease: A review. ACM Transactions on Multimedia
Computing, Communications, and Applications 16(1s), 1–35 (2020)

[228] Zhang, Y., Wang, S.: Detection of Alzheimer disease by displacement

ﬁeld and machine learning. PeerJ 3, 1251 (2015)

[229] Wang, S., Zhang, Y., Liu, G., Phillips, P., Yuan, T.-F.: Detection of
Alzheimer disease by three-dimensional displacement ﬁeld estimation in
structural magnetic resonance imaging. Journal of Alzheimer Disease
50(1), 233–248 (2016)

[230] Alam, S., Kwon, G.-R., Kim, J.-I., Park, C.-S.: Twin SVM-based classi-
ﬁcation of Alzheimer disease using complex dual-tree wavelet principal
coeﬃcients and LDA. Journal of Healthcare Engineering (2017)

[231] Tomar, D., Prasad, B.R., Agarwal, S.: An eﬃcient Parkinson disease
diagnosis system based on least squares twin support vector machine
and particle swarm optimization. In: 2014 9th International Conference
on Industrial and Information Systems (ICIIS), pp. 1–6 (2014). IEEE

[232] Wang, S., Chen, M., Li, Y., Shao, Y., Zhang, Y., Du, S., Wu, J.: Morpho-
logical analysis of dendrites and spines by hybridization of ridge detection
with twin support vector machine. PeerJ 4, 2207 (2016)

[233] Tomar, D., Ojha, D., Agarwal, S.: An emotion detection system based on

M. Tanveer et al.

57

multi least squares twin support vector machine. Advances in Artiﬁcial
Intelligence, 8 (2014)

[234] Wang, S., Zhang, Y., Yang, X., Sun, P., Dong, Z., Liu, A., Yuan, T.-F.:
Pathological brain detection by a novel image feature fractional fourier
entropy. Entropy 17(12), 8278–8296 (2015)

[235] Richhariya, B., Tanveer, M.: An eﬃcient angle-based universum least
squares twin support vector machine for classiﬁcation. ACM Trans.
Internet Technol. 21(3) (2021). https://doi.org/10.1145/3387131

[236] Richhariya, B., Tanveer, M., Rashid, A.H., Initiative, A.D.N.: Diag-
nosis of Alzheimer’s disease using universum support vector machine
based recursive feature elimination (USVM-RFE). Biomedical Signal
Processing and Control 59, 101903 (2020)

[237] Khan, R.U., Tanveer, M., Pachori, R.B., (ADNI), A.D.N.I.: A novel
method for the classiﬁcation of Alzheimer’s disease from normal controls
using magnetic resonance imaging. Expert Systems 38(1), 12566 (2021)

[238] Sharma, R., Goel, T., Tanveer, M., Murugan, R.: FDN-ADNet:
Fuzzy LS-TWSVM based deep learning network for prognosis of the
Alzheimer’s disease using the sagittal plane of MRI scans. Applied Soft
Computing 115, 108099 (2022)

[239] Cong, H., Yang, C., Pu, X.: Eﬃcient speaker recognition based on
multi-class twin support vector machines and GMMs. In: 2008 IEEE
Conference on Robotics, Automation and Mechatronics, pp. 348–352
(2008). IEEE

[240] Yang, C., Wu, Z.: Study to multi-twin support vector machines and its
applications in speaker recognition. In: 2009 International Conference on
Computational Intelligence and Software Engineering, pp. 1–4 (2009).
IEEE

[241] Francis, L.M., Sreenath, N.: Robust scene text recognition: Using man-
ifold regularized twin-support vector machine. Journal of King Saud
University-Computer and Information Sciences (2019)

[242] Tian, Y., Qi, Z., Ju, X., Shi, Y., Liu, X.: Nonparallel support vector
machines for pattern classiﬁcation. IEEE Transactions on Cybernetics
44(7), 1067–1079 (2014)

[243] Rastogi, R., Pal, A.: Eﬃcient learning of pinball twsvm using privileged
information and its applications. arXiv preprint arXiv:2107.06744 (2021)

[244] He, J., Zheng, S.-h.: Intrusion detection model with twin support vector

58

M. Tanveer et al.

machines. Journal of Shanghai Jiaotong University (Science) 19(4), 448–
454 (2014)

[245] Ding, X., Zhang, G., Ke, Y., Ma, B., Li, Z.: High eﬃcient intrusion
detection methodology with twin support vector machines. In: 2008
International Symposium on Information Science and Engineering, vol.
1, pp. 560–564 (2008). IEEE

[246] Mousavi, A., Ghidary, S.S., Karimi, Z.: Semi-supervised intrusion detec-
tion via online Laplacian twin support vector machine. In: 2015 Signal
Processing and Intelligent Systems Conference (SPIS), pp. 138–142
(2015). IEEE

[247] Nie, P., Zang, L., Liu, L.: Application of multi-class classiﬁcation algo-
rithm based on twin support vector machine in intrusion detection.
Jisuanji Yingyong/ Journal of Computer Applications 33(2), 426–429
(2013)

[248] Khemchandani, R., Sharma, S.: Robust parametric twin support vector
machine and its application in human activity recognition. In: Pro-
ceedings of International Conference on Computer Vision and Image
Processing, pp. 193–203 (2017). Springer

[249] Mozafari, K., Nasiri, J.A., Charkari, N.M., Jalili, S.: Action recognition
by local space-time features and least square Twin SVM (LS-TSVM). In:
2011 First International Conference on Informatics and Computational
Intelligence, pp. 287–292 (2011). IEEE

[250] Kumar, M.P., Rajagopal, M.K.: Detecting happiness in human face
using unsupervised twin-support vector machines. International Journal
of Intelligent Systems and Applications 10(8), 85 (2018)

[251] Kumar, M.P., Rajagopal, M.K.: Detecting facial emotions using normal-
ized minimal feature vectors and semi-supervised twin support vector
machines classiﬁer. Applied Intelligence, 1–25 (2019)

[252] Yang, H.-Y., Wang, X.-Y., Niu, P.-P., Liu, Y.-C.: Image denoising using
nonsubsampled shearlet transform and twin support vector machines.
Neural Networks 57, 152–165 (2014)

[253] Shahdoosti, H.R., Hazavei, S.M.: Combined ripplet and total varia-
tion image denoising methods using twin support vector machines.
Multimedia Tools and Applications 77(6), 7013–7031 (2018)

[254] Richhariya, B., Tanveer, M.: EEG signal classiﬁcation using universum
support vector machine. Expert Systems with Applications 106, 169–182
(2018)

M. Tanveer et al.

59

[255] Soman, S.: High performance EEG signal classiﬁcation using classi-
ﬁability and the twin SVM. Applied Soft Computing 30, 305–318
(2015)

[256] Tanveer, M., Pachori, R.B., Angami, N.V.: Entropy based features in
FAWT framework for automated detection of epileptic seizure EEG sig-
nals. In: 2018 IEEE Symposium Series on Computational Intelligence
(SSCI), pp. 1946–1952 (2018). IEEE

[257] Tanveer, M., Pachori, R.B., Angami, N.V.: Classiﬁcation of seizure and
seizure-free EEG signals using hjorth parameters. In: 2018 IEEE Sympo-
sium Series on Computational Intelligence (SSCI), pp. 2180–2185 (2018).
IEEE

[258] Li, D., Zhang, H., Khan, M.S., Mi, F.: A self-adaptive frequency selection
common spatial pattern and least squares twin support vector machine
for motor imagery electroencephalography recognition. Biomedical Sig-
nal Processing and Control 41, 222–232 (2018)

[259] Kumar, B., Gupta, D.: Universum based Lagrangian twin bounded sup-
port vector machine to classify EEG signals. Computer Methods and
Programs in Biomedicine 208, 106244 (2021)

[260] Gupta, D., Borah, P., Sharma, U.M., Prasad, M.: Data-driven mech-
anism based on fuzzy Lagrangian twin parametric-margin support
vector machine for biomedical data analysis. Neural Computing and
Applications, 1–11 (2021)

[261] She, Q., Ma, Y., Meng, M., Luo, Z.: Multiclass posterior probability twin
svm for motor imagery EEG classiﬁcation. Computational Intelligence
and Neuroscience, 95 (2015)

[262] Kost´ılek, M., ˇSt’astn`y, J.: EEG biometric identiﬁcation: repeatabil-
ity and inﬂuence of movement-related EEG. In: 2012 International
Conference on Applied Electronics, pp. 147–150 (2012). IEEE

[263] Lang, X., Li, P., Hu, Z., Ren, H., Li, Y.: Leak detection and location of
pipelines based on LMD and least squares twin support vector machine.
IEEE Access 5, 8659–8668 (2017)

[264] Dalal, M., Tanveer, M., Pachori, R.B.: Automated identiﬁcation system
for focal EEG signals using fractal dimension of FAWT-Based sub-bands
signals, 583–596 (2019)

[265] Cao, Y., Ding, Z., Xue, F., Rong, X.: An improved twin support vec-
tor machine based on multi-objective cuckoo search for software defect
prediction. International Journal of Bio-Inspired Computation 11(4),

60

M. Tanveer et al.

282–291 (2018)

[266] Chu, M., Liu, X., Gong, R., Liu, L.: Multi-class classiﬁcation method
using twin support vector machines with multi-information for steel sur-
face defects. Chemometrics and Intelligent Laboratory Systems 176,
108–118 (2018)

[267] Peng, X., Xu, D.: Bi-density twin support vector machines for pattern

recognition. Neurocomputing 99, 134–143 (2013)

[268] Richhariya, B., Gupta, D.: Facial expression recognition using iterative
universum twin support vector machine. Applied Soft Computing 76,
53–67 (2019)

[269] Anand, P., Pandey, J.P., Rastogi, R., Chandra, S.: A privacy-preserving
twin support vector machine classiﬁer for vertical partitioned data.
In: Computational Intelligence: Theories, Applications and Future
Directions-Volume I, pp. 539–552. Springer, ??? (2019)

[270] Mangasarian, O.L., Wild, E.W.: Multisurface proximal support vector
machine classiﬁcation via generalized eigenvalues. IEEE Transactions on
Pattern Analysis and Machine Intelligence 28(1), 69–74 (2006)

[271] Bi, J., Bennett, K.P.: A geometric approach to support vector regression.

Neurocomputing 55, 79–108 (2003)

[272] Xu, Y., Wang, L.: A weighted twin support vector regression. Knowledge-

Based Systems 33, 92–101 (2012)

[273] Tanveer, M., Shubham, K., Aldhaifallah, M., Nisar, K.: An eﬃcient
implicit regularized Lagrangian twin support vector regression. Applied
Intelligence 44(4), 831–848 (2016)

[274] Ye, Y.-F., Bai, L., Hua, X.-Y., Shao, Y.-H., Wang, Z., Deng, N.-Y.:
Weighted lagrange ε-twin support vector regression. Neurocomputing
197, 53–68 (2016)

[275] Peng, X.: Eﬃcient twin parametric insensitive support vector regression

model. Neurocomputing 79, 26–38 (2012)

[276] Peng, X., Xu, D., Shen, J.: A twin projection support vector machine

for data regression. Neurocomputing 138, 131–141 (2014)

[277] Wang, L., Gao, C., Zhao, N., Chen, X.: A projection wavelet weighted
twin support vector regression and its primal solution. Applied Intelli-
gence 49(8), 3061–3081 (2019)

M. Tanveer et al.

61

[278] Wang, L., Ma, Y., Chang, X., Gao, C., Qu, Q., Chen, X.: Projec-
tion wavelet weighted twin support vector regression for OFDM system
channel estimation. Artiﬁcial Intelligence Review 54(1), 469–489 (2021)

[279] Zhong, P., Xu, Y., Zhao, Y.: Training twin support vector regression via
linear programming. Neural Computing and Applications 21(2), 399–407
(2012)

[280] Chen, X., Yang, J., Liang, J., Ye, Q.: Smooth twin support vector

regression. Neural Computing and Applications 21(3), 505–513 (2012)

[281] Zhao, Y.-P., Zhao, J., Zhao, M.: Twin least squares support vector

regression. Neurocomputing 118, 225–236 (2013)

[282] Chen, X., Yang, J., Chen, L.: An improved robust and sparse twin sup-
port vector regression via linear programming. Soft Computing 18(12),
2335–2348 (2014)

[283] Huang, H., Wei, X., Zhou, Y.: A sparse method for least squares twin
support vector regression. Neurocomputing 211, 150–158 (2016)

[284] Tanveer, M.: Linear programming twin support vector regression. Filo-

mat 31(7), 2123–2142 (2017)

[285] Singla, M., Ghosh, D., Shukla, K., Pedrycz, W.: Robust twin support vec-
tor regression based on rescaled hinge loss. Pattern Recognition, 107395
(2020)

[286] Gu, B., Fang, J., Pan, F., Bai, Z.: Fast clustering-based weighted twin

support vector regression. Soft Computing, 1–17 (2020)

[287] Carrasco, M., L´opez, J., Maldonado, S.: Epsilon-nonparallel support
vector regression. Applied Intelligence 49(12), 4223–4236 (2019)

[288] Balasundaram, S., Prasad, S.C.: Robust twin support vector regres-
sion based on huber loss function. Neural Computing and Applications
32(15), 11285–11309 (2020)

[289] Peng, X.: Primal twin support vector regression and its sparse approxi-

mation. Neurocomputing 73(16-18), 2846–2858 (2010)

[290] Singh, M., Chadha, J., Ahuja, P., Jayadeva, Chandra, S.: Reduced twin

support vector regression. Neurocomputing 74(9), 1474–1477 (2011)

[291] Huang, H.-j., Ding, S.: Primal least squares twin support vector regres-
sion. Journal of Zhejiang University SCIENCE C 14(9), 722–732 (2013)

[292] Balasundaram, S., Tanveer, M.: On Lagrangian twin support vector

62

M. Tanveer et al.

regression. Neural Computing and Applications 22(1), 257–267 (2013)

[293] Balasundaram, S., Gupta, D.: Training Lagrangian twin support vec-
tor regression via unconstrained convex minimization. Knowledge-Based
Systems 59, 85–96 (2014)

[294] Balasundaram, S., Tanveer, M.: Smooth newton method for implicit
Lagrangian twin support vector regression. International Journal of
Knowledge-based and Intelligent Engineering Systems 17(4), 267–278
(2013)

[295] Khemchandani, R., Karpatne, A., Chandra, S.: Twin support vector
regression for the simultaneous learning of a function and its derivatives.
International Journal of Machine Learning and Cybernetics 4(1), 51–63
(2013)

[296] Gupta, D., Acharjee, K., Richhariya, B.: Lagrangian twin parametric
insensitive support vector regression (LTPISVR). Neural Computing and
Applications 32(10), 5989–6007 (2020)

[297] Gupta, D., Richhariya, B.: Eﬃcient implicit lagrangian twin paramet-
ric insensitive support vector regression via unconstrained minimization
problems. Annals of Mathematics and Artiﬁcial Intelligence 89(3),
301–332 (2021)

[298] Hao, P.-Y.: Asymmetric possibility and necessity regression by twin

support vector networks. IEEE Transactions on Fuzzy Systems (2020)

[299] Gupta, U., Gupta, D.: On regularization based twin support vector
regression with huber loss. Neural Processing Letters 53(1), 459–515
(2021)

[300] Peng, X., Chen, D., Kong, L., Xu, D.: Interval twin support vector regres-
sion algorithm for interval input-output data. International Journal of
Machine Learning and Cybernetics 6(5), 719–732 (2015)

[301] Rastogi, R., Anand, Pritam, C.S.: Large-margin distribution machine-

based regression. Neural Computing and Applications 3, 1–16 (2018)

[302] Rastogi, R., Anand, Pritam, C.S.: ν-norm twin support vector machine-

based regression. Optimization 66(11), 1895–1911 (2017)

[303] Balasundaram, S., Meena, Y.: Training primal twin support vector
regression via unconstrained convex minimization. Applied Intelligence
44(4), 931–955 (2016)

[304] Parastalooi, N., Amiri, A., Aliheidari, P.: Modiﬁed twin support vector

M. Tanveer et al.

63

regression. Neurocomputing 211, 84–97 (2016)

[305] Rastogi, R., Anand, P., Chandra, S.: A ν-twin support vector machine
based regression with automatic accuracy control. Applied Intelligence
46(3), 670–683 (2017)

[306] Xu, Y., Li, X., Pan, X., Yang, Z.: Asymmetric ν-twin support vec-
tor regression. Neural Computing and Applications 30(12), 3799–3814
(2018)

[307] Tanveer, M., Shubham, K.: A regularization on Lagrangian twin sup-
port vector regression. International Journal of Machine Learning and
Cybernetics 8(3), 807–821 (2017)

[308] Vapnik, V., Vapnik, V.: Statistical learning theory wiley. New York 1

(1998)

[309] Han, X., Clemmensen, L.: On weighted support vector regression.
Quality and Reliability Engineering International 30(6), 891–903 (2014)

[310] Ye, Y.-F., Cao, H., Bai, L., Wang, Z., Shao, Y.-H.: Exploring determi-
nants of inﬂation in china based on l1 − (cid:15)-twin support vector regression.
Procedia Computer Science 17, 514–522 (2013)

[311] Ye, Y., Shao, Y., Chen, W.: Comparing inﬂation forecasts using an
ε -wavelet twin support vector regression. Journal of Information &
Computational Science 10(7), 2041–2049 (2013)

[312] Ding, S., Huang, H., Nie, R.: Forecasting method of stock price based
on polynomial smooth twin support vector regression. In: International
Conference on Intelligent Computing, pp. 96–105 (2013). Springer

[313] Le, W., Wang, Z., Wang, J., Zhao, G., Miao, H.: A novel wiﬁ indoor
positioning method based on genetic algorithm and twin support vector
regression. In: The 26th Chinese Control and Decision Conference (2014
CCDC), pp. 4859–4862 (2014). IEEE

[314] Houssein, E.H.: Particle swarm optimization-enhanced twin support vec-
tor regression for wind speed forecasting. Journal of Intelligent Systems
(2017)

[315] Wang, H., Xu, Y.: Scaling up twin support vector regression with safe

screening rule. Information Sciences 465, 174–190 (2018)

[316] Ganaie, M., Tanveer, M., Beheshti, I.: Brain age prediction using
improved twin SVR. Neural Computing and Applications (2021). https:
//doi.org/10.1007/s00521-021-06518-1

64

M. Tanveer et al.

[317] Ganaie, M., Beheshti, I., Tanveer, M.: Brain age prediction with
improved least squares twin SVR. IEEE Journal of Biomedical and
Health Informatics (2022). https://doi.org/10.1109/JBHI.2022.3147524

[318] Beheshti, I., Ganaie, M., Paliwal, V., Rastogi, A., Razzak, I., Tanveer,
M.: Predicting brain age using machine learning algorithms: A compre-
hensive evaluation. IEEE Journal of Biomedical and Health Informatics
(2021)

