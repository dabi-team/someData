9
1
0
2

p
e
S
1
2

]
L
C
.
s
c
[

1
v
9
7
7
9
0
.
9
0
9
1
:
v
i
X
r
a

SELF-ATTENTION BASED END-TO-END HINDI-ENGLISH
NEURAL MACHINE TRANSLATION

A PREPRINT

Siddhant Srivastava
Soft Computing Laboratory
ABV-IIITM Gwalior
siddhant.srivastava11@gmail.com

Ritu Tiwari
Soft Computing Laboratory
ABV-IIITM Gwalior
tiwariritu2@gmail.com

September 24, 2019

ABSTRACT

Machine Translation (MT) is a zone of concentrate in Natural Language processing which manages
the programmed interpretation of human language, starting with one language then onto the next by
the PC. Having a rich research history spreading over about three decades, Machine interpretation is a
standout amongst the most looked for after region of research in the computational linguistics network.
As a piece of this current ace’s proposal, the fundamental center is examine the Deep-learning based
strategies that have gained critical ground as of late and turning into the de facto strategy in MT.
We would like to point out the recent advances that have been put forward in the ﬁeld of Neural
Translation models, different domains under which NMT has replaced conventional SMT models
and would also like to mention future avenues in the ﬁeld. Consequently, we propose an end-to-end
self-attention transformer network for Neural Machine Translation, trained on Hindi-English parallel
corpus and compare the model’s efﬁciency with other state of art models like encoder-decoder and
attention based encoder-decoder neural models on the basis of BLEU. We conclude this paper with a
comparitive analysis of the three proposed models.

1

Introduction

Machine Translation, which is a ﬁeld of concentrate under common language preparing, focuses at deciphering normal
language naturally utilizing machines. Information driven machine interpretation has turned into the overwhelming
ﬁeld of concentrate because of the availability of substantial parallel corpora. The primary target of information driven
machine interpretation is to decipher concealed source language, given that the frameworks take in interpretation
learning from sentence adjusted bi-lingual preparing information.

Statistical Machine Translation (SMT) is an information driven methodology which utilizes probabilistic models to
catch the interpretation procedure. Early models in SMT depended on generative models accepting a word as the
fundamental element [1], greatest entropy based discriminative models utilizing highlights gained from sentences
[2], straightforward and various leveled phrases [3, 4]. These strategies have been extraordinarily utilized since 2002
regardless of the way that discriminative models looked with the test of information sparsity. Discrete word based
portrayals made SMT vulnerable to learning poor gauge on the record of low check occasions. Additionally, structuring
highlights for SMT physically is a troublesome errand and require area language, which is hard remembering the
assortment and intricacy of various common dialects.

Later years have seen the extraordinary accomplishment of deep learning applications in machine interpretation. Deep
learning approaches have surpassed factual strategies in practically all sub-ﬁelds of MT and have turned into the de
facto technique in both scholarly world just as in the business. as a major aspect of this theory, we will talk about the
two spaces where deep learning has been signiﬁcantly utilized in MT. We will quickly examine Component or Domain
based deep learning strategies for machine translation [5] which utilizes deep learning models to improve the viability
of various parts utilized in SMT including language models, transition models, and re-organizing models. Our primary
spotlight in on end-to-end deep learning models for machine translation [6, 7] that utilizes neural systems to separate

 
 
 
 
 
 
A PREPRINT - SEPTEMBER 24, 2019

correspondence between a source and target language straightforwardly in an all encompassing way without utilizing
any hand-created highlights. These models are currently perceived as Neural Machine translation (NMT).

Let x signify the source language and y mean the objective language, given a lot of model parameters θ , the point of
any machine interpretation calculation is to discover the interpretation having greatest likelihood ˆy:

The decision rule is re-written using Bayes’ rule as [1]:

ˆy = arg max

P (y|x; θ).

y

ˆy = arg max

y

P (y; θlm)P (x|y; θtm)
P (x)

.

ˆy = arg max

y

P (y; θlm)P (x|y; θtm).

(1)

(2)

(3)

Where P (y; θlm) is called as language model, and P (x|y; θtm) is called as transition model. The interpretation model
likewise, is characterized as generative model, which is crumbled by means of dormant structures.

P (x|y; θtm) =

(cid:88)

z

P (x, y|z; θtm).

(4)

Where, z signiﬁes the idle structures like word arrangement between source language and target language.

2 End-to-End Deep Learning for Machine translation

Start to ﬁnish Machine Translation models [6, 7] likewise named as Neural Machine Translation (NMT), intends to
discover a correspondence among source and target normal dialects with the assistance of deep neural systems. The
fundamental distinction among NMT and customary Statistical Machine Translation (SMT) [1, 8, 3, 2] based method-
ologies is that Neural model are ﬁt for learning complex connections among characteristic dialects straightforwardly
from the information, without turning to manual hand highlights, which are difﬁcult to plan.

The standard issue in Machine Translation continues as before, given an arrangement of words in source language
sentence X = x1, ....xj, ....xJ and target language sentence Y = y1, ....yi, ....yI , NMT endeavors to factor sentence
level interpretation likelihood into setting dependant sub-word interpretation probabilities.

P (y|x; θ) =

I
(cid:89)

i=1

P (yi|x, y<i; θ)

(5)

Here y<i is alluded to as fractional interpretation. There can be sparsity among setting among source and target sentence
when the sentences become excessively long, to tackle this issue, [6] proposed an encoder-decoder arrange which
could speak to variable length sentence to a ﬁxed length vector portrayal and utilize this conveyed vector to decipher
sentences.

2.1 Encoder Decoder Framework for Machine Translation

Neural Machine Translation models stick to an Encoder-Decoder engineering, the job of encoder is to speak to subjective
length sentences to a ﬁxed length genuine vector which is named as setting vector. This setting vector contains all
the fundamental highlights which can be construed from the source sentence itself. The decoder arrange accepts this
vector as contribution to yield target sentence word by word. The perfect decoder is relied upon to yield sentence which
contains the full setting of source language sentence. Figure1 shows the overall architecture of the encoder-decoder
neural network for machine translation.

Since source and target sentences are ordinarily of various lengths, Initially [6] proposed Recurrent Neural Network for
both encoder and decoder systems, To address the issue of evaporating angle and detonating slopes happening because
of conditions among word sets, Long Short Term Memory (LSTM) [9] and Gated Recurrent Unit (GRU) [10] were
proposed rather than Vanilla RNN cell.

Training in NMT is done by maximising log-likelihood as the objective function:

ˆθ = arg max

L(θ)

θ

2

(6)

A PREPRINT - SEPTEMBER 24, 2019

Figure 1: Encoder-Decoder model for Machine Translation, Crimson boxes portray the concealed expressed of encoder,
Blue boxes indicates "End of Sentence" EOS and Green boxes show shrouded condition of the decoder. credits (Neural
Machine Translation - Tutorial ACL 2016)

Where L(θ) is deﬁned as:

L(θ) =

I
(cid:88)

i=1

logP (y(i)|x(i); θ)

After training, learned parameters ˆθ is used for translation as:

ˆy = arg max

P (y|x; ˆθ)

y

2.2 Attention Mechanism in Neural Machine Translation

(7)

(8)

The Encoder organize proposed by [6] spoke to source language sentence into a ﬁxed length vector which was in this
way used by the Decoder arrange, through observational testing, it was seen that the nature of interpretation incredibly
relied upon the span of source sentence and diminished essentially by expanding the sentence measure.

To address this issue, [7] proposed to coordinate an Attention system inside the Encoder arrange and demonstrated
this could progressively choose signiﬁcant parts of setting in source sentence to deliver target sentence. They utilized
Bi-directional RNN (BRNN’s) to catch worldwide settings:

−→
hs = f (x(s),

−−→
hs−1, θ)

←−
hs = f (x(s),

←−−
hs−1, θ)

The forward hidden state

−→
hs and backward hidden state

←−
hs are concatenated to capture sentence level context.
−−→
hs−1;

←−−
hs−1]

hs = [

(9)

(10)

(11)

The basic Ideology behind computing attention is to seek portions of interest in source text in order to generate target
words in text, this is performed by computing attention weights ﬁrst.

αj,i =

exp(a(tj−1, hi, θ))
i(cid:48)=1 exp(a(tj−1, hi(cid:48), θ))

(cid:80)I+1

(12)

Where a(tj−1, hi, θ) is the alignment function which evaluates how well inputs are aligned with respect to position i
and output at position i. Context vector cj is computed as a weighted sum of hidden states of the source.

αj,ihi

cj =

I+1
(cid:88)

i=1

3

(13)

A PREPRINT - SEPTEMBER 24, 2019

Figure 2: Consideration based Encoder-Decoder Architecture for Machine Translation. The majority of the Architecture
is like fundamental Encoder-Decoder with the expansion of Context Vector Computed utilizing consideration loads for
each word token, Attention vector is determined utilizing Context vector and concealed condition of encoder. credits
(Attention-based Neural Machine Translation with Keras, blog by Sigrid Keydana)

And target hidden state is computed as follows.

tj = f (yj−1, sj−1, cj, θ)

(14)

In Figure 2, we have attention mechanism at the encoder level, the context vector is then used by the decoder layer
for language translation. The distinction between consideration based NMT [7] from unique encoder-decoder based
engineering [6] is the way source setting is registered, in unique encoder-decoder, the source’s shrouded state is utilized
to introduce target’s underlying concealed state while in consideration instrument, a weighted aggregate of concealed
state is utilized which ensures that the signiﬁcance of every single source word in the sentence is very much protected
in the speciﬁc circumstance. This incredibly improves the execution of interpretation and hence this has turned into the
state of art model in neural machine interpretation.

3 Neural Architectures for NMT

The majority of the encoder-decoder based NMT models have used RNN and It’s variations LSTM [9] and GRU [10].
As of late, Convolution systems (CNN) [11] and self consideration systems [12] have been examined and have delivered
promising outcomes.

The issue with utilizing Recurrent systems in NMT is that it works by sequential calculation and necessities to keep up
it’s concealed advance at each progression of preparing. This makes the preparation deeply wasteful and tedious. [11]
proposed that convolution systems can, interestingly, become familiar with the ﬁxed length shrouded states utilizing
convolution task. The principle preferred standpoint of this methodology being that convolution task doesn’t rely upon
recently ﬁgured qualities and can be parallelized for multi-center preparing. Additionally Convolution systems can be
stacked in a steady progression to learn further setting settling on it a perfect decision for both the encoder and decoder.

Intermittent systems process reliance among words in a sentence in O(n) while Convolution system can accomplish the
equivalent in O(logkn) where k is the extent of convolution part.

[12] proposed a model which could register the reliance among each word pair in a sentence utilizing just the Attention
layer stacked in a steady progression in both the encoder and decoder, he named this as self-attention, the overall
architecture is shown as Figure 3. In their model, concealed state is ﬁgured utilizing self-consideration and feed forward
system, they utilize positional encoding to present the element dependent on the area of word in the sentence and their
self-consideration layer named as multi-head attention is very parallelizable. This model has appeared to be exceedingly
parallelizable due to before referenced reason and fundamentally accelerates NMT preparing, likewise bringing about
preferable outcomes over the benchmark Recurrent system based models.

As of now, there is no clear decision regarding which neural architecture is the best and different architectures give
different results depending on the problem in hand. Neural architecture is still considered to be the hottest and most
active research ﬁeld in Neural Machine Translation.

4

A PREPRINT - SEPTEMBER 24, 2019

Figure 3: Self-Attention Encoder-Decoder Transformer model. Encoder and Decoder both consists positional encoding
and stacked layers of multi-head attention and feedforward network with the Decoder containing an additional Masked
multi head attention. Transition Probabilities are calculated using linear layer followed by softmax. credits (Vaswani et
al. 2017)

4 Research gaps and open problems

deep learning strategies have altered the ﬁeld of Machine Translation, with early endeavors concentrating on improving
the key segments of Statistical Machine Translation like word arrangement [13] , interpretation model [3, 14], and
expression reordering [15, 16] and language model [17]. Since 2010, a large portion of the exploration has been
moved towards creating start to ﬁnish neural models that could relieve the need of broad component designing [6, 7].
Neural models have effectively supplanted Statistical models since their commencement in all scholarly and modern
application.

Albeit Deep learning has quickened look into in Machine Translation people group yet regardless, Current NMT models
are not free from blemishes and has certain constraints. In this segment, we will depict some current research issues
in NMT, our point is to control specialists and researchers working in this ﬁeld to get to know these issues and work
towards it for considerably quicker improvement in the ﬁeld.

4.1 Neural models motivated by semantic approaches

Start to ﬁnish models have been named as the de facto model in Machine Translation, yet it is difﬁcult to decipher the
inner calculation of neural systems which is frequently essentially said to be the "Black Box" approach. One conceivable
zone of research is to grow etymologically propelled neural models for better interpretability. It is difﬁcult to perceive
learning from concealed condition of current neural systems and thus it is similarly hard to join earlier information
which is emblematic in nature into consistent portrayal of these states [18].

4.2 Light weight neural models for learning through inadequate data

Another real disadvantage for NMT is information shortage, It is surely known that NMT models are information hungry
and requires a great many preparing cases for giving best outcomes. The issue emerges when there isn’t sufﬁcient
parallel corpora present for the majority of the language matches on the planet. In this way fabricating models that can
adapt better than average portrayal utilizing generally littler informational collection is an effectively inquired about

5

A PREPRINT - SEPTEMBER 24, 2019

Table 1: Statistics of Dataset used

Dataset
IITB-CFILT Hi-En
Train
Validation-Test

Sentence Pairs
1,495,847
1,492,827
3,020

issue today. One comparative issue is to create one-to-numerous and many-to-numerous language models rather than
balanced models. Analysts don’t know how to normal information utilizing neural system from an etymological point
of view, as this learning will help create multi-lingual interpretation models rather than balanced models utilized today.

4.3 Multi-modular Neural Architectures for present data

One more issue is to create multi-modular language interpretation models. Practically all the work done has been
founded on printed information. Research on creating nonstop portrayal combining content, discourse and visual
information to create multi-model frameworks is going all out. Additionally since there is constrained or no multi-model
parallel corpora present, advancement of such databases is likewise a fascinating ﬁeld to investigate and can likewise
proﬁt multi-modular neural designs.

4.4 Parallel and conveyed calculations for preparing neural models

At long last, current neural designs depend intensely broad calculation control for giving skillful outcomes [19, 20, 21].
In spite of the fact that there is no ﬁgure and capacity lack in current situation, yet it would be increasingly proﬁcient to
thought of light neural models of language interpretation. Additionally Recurrent models [6, 7] can’t be parallelized
because of which it is difﬁcult to create conveyed frameworks for model preparing. Luckily, late advancements, with
the rise of Convolution systems and self-consideration Networks can be parallelized and therefore disseminated among
various frameworks. But since they contain a great many related parameters, it makes it difﬁcult to circulate them
among inexactly coupled frameworks. Along these lines growing light neural designs intended to be circulated can be
new likely wilderness of NMT.

5 Methodology

The proposed methodology can be broken down to several atomic objectives. The ﬁrst step is the Acquisition of parallel
corpora, the next step is to pre-process the data acquired. Various neural models is to be implemented and trained on
the pre-processed data. The last part of our study is to compare the results obtained by the models and do a comparative
study.

5.1 Data Acquisition and preparation

For this study, we intend to work with is the English-Hindi parallel corpus, curated and made publically available by the
Center of Indian Language Technologies (CFILT), Indian Institute of Technology, Bombay [22]. Table 1 shows the
number of parallel sentences in the train and test data. This parallel datasets contains more than 1.5 million parallel
sentences for training and testing purpose, to the best our knowledge, there is no literature present till date indicating
any comparative study done based upon the Neural models on this dataset.

After getting our data in an unzipped form, the next part in our pipeline is to decompose rare words in our corpora
using subword byte pair encoding [23]. Byte pair encoding is a useful approach when we have an extremely large
vocabulary which hinders model training and thus we can decompose those rare words into common subwords and build
the vocabulary accordingly.To encode the training corpora using BPE, we need to generate BPE operations ﬁrst. The
following command will create a ﬁle named bpe32k, which contains 32k BPE operations.It also outputs two dictionaries
named vocab.de and vocab.en. Similar methodology is applied for Hindi-english data as well.

6 Model components

For this study, sequence-to-sequence LSTM network and Attention based encoder-decoder using GRU cell have been
implemented. Self attention Transformer network has been implemented and all the models are tested side by side to
create a clear superiority distinction among them. The basic theory of model components used is given below.

6

A PREPRINT - SEPTEMBER 24, 2019

6.1 RNN Cell

The basic neural cell present in Neural network works well for several problems but fails miserably when the order of
data matters, as a result these models fails to generalize and solve problems which deals with temporal or sequential
data. To reason behind this failure being that the basic neural cell doesn’t take into account the previous or backward
information for it’s computation and using the same philosophy the basic RNN cell was developed. Recurrent Neural
Network (RNN) are such network having recurrent cells which are capable of incorporating past information with
current information in terms of value computation and as a result these models have seen huge success in problems
dealing with sequential input like problems coming under the domain of Natural Language Processing, weather forecast
and other such problems.

The basic mathematical equations underlying the RNN cell are Described below:

ht = fW (ht−1, xt)

ht = tanh(Whhht−1, Wxhxt)

yt = Whyht

(15)

(16)

(17)

Here xt and yt are the input and output at the tth time step, Whh, Wxh and Why are connection weights respectively.

6.2 GRU Cell

Although the RNN cell has outperformed non-sequential neural networks but they fail to generalize to problems having
longer sequence length, the problem arises due to not able to capture long term dependencies among the sequential
units and this phenomena is termed as the vanishing gradient problem, To solve this problem [10] proposed a Gated
approach to explicitly caputure long term memory using different cells, this cell was termed as Gated Recurrent Unit
(GRU). The schematic diagram of GRU cell is given in Figure 4.

The difference between GRU cell and RNN cell lies at the computation of Hidden cell values, GRU uses two gates
update (z) and reset (r) to capture long term dependancies. The mathematical equations behind the computation are
given below.

zt = σg(Wzxt + Uzht−1 + bz)

rt = σg(Wrxt + Urht−1 + br)

ht = zt (cid:12) ht−1 + (1 − zt) (cid:12) σh(Whxt + Uh(rt (cid:12) xt) + bh)

(18)

(19)

(20)

6.3 LSTM Cell

Short for Long Short Term Memory, Given by [9] is another approach to overcome the vanishing gradient problem in
RNN, like GRU, LSTM uses gating mechanism but it uses three gates instead of two cells in GRU to capture long Term
Dependencies. The schematic diagram of LSTM cell is given in Figure 4.

LSTM cell uses the input (i), output (o) and forget (f ) gates for computation of hidden states respectively, the equations
are similar to that of GRU cell, LSTM like GRU, uses sigmoid activation for adding non-linearity to the function.

ft = σg(Wf xt + Uf ht−1 + bf )

it = σg(Wixt + Uiht−1 + bi)

ot = σg(Woxt + Uoht−1 + bo)

ct = ft (cid:12) ct−1 + it (cid:12) σc(Wcxt + Ucht−1 + bc)

ht = ot (cid:12) σc(ct)

7

(21)

(22)

(23)

(24)

(25)

A PREPRINT - SEPTEMBER 24, 2019

(a) GRU cell

(b) LSTM

Figure 4: Cell Structure

6.4 Attention Mechanism

Attention mechanism was ﬁrst developed by [7], in their paper “Neural Machine Translation by Jointly Learning to
Align and Translate” which takes in as a natural extension of their previous work on the sequence to sequence Encoder-
Decoder model. Attention is proposed as a solution to mitigate the limitation of the Encoder-Decoder architecture
which encodes the input sequence to one ﬁxed length vector from which the output is decoded at each time step. This
problem seems to be more of a issue when decoding long sequences. Attention is proposed as a singular method to both
align and translate. Alignment is the problem in machine translation that seeks to ﬁnd which parts of the input sequence
are relevant to each word in the output, whereas translation is the process of using the relevant information to select the
appropriate output.

6.5 Transformer Network

j = xj.
j = bl−1
bl

We use transformer self-attention encoder for our study. The transformer model [12] is made up of M consecutive
blocks. Each block of the transformer, denoted by transf ormerl, contains two separate components, multi-head
attention and a feed forward network. The output of each token j of block l is connected to it’s input in a residual
connection. The input to the ﬁrst block is b0

j + T ransf ormerl(bl−1

j

)

(26)

Multi-head attention applies self-attention over the same inputs multiple times by using separately normalized parameters
(attention heads) and ﬁnally concatenates the results of each head, multi-head attention mechansim is considered as a
better alternative to applying one pass of attention with more parameters as the former process can be easily parallelized.
Furthermore, computing the attention with multiple heads make it easier for the model to learn and attend to different
types of relevant information with each head. The self-attention updates input bl−1
by computing a weighted sum over
all tokens in the sequence, weighted by their importance for modeling token j.

j

Each input, inside the multi-head attention is projected to query, key and value (q, k, v) respectively. q, k and v are all
of dimensions ∈ Rd/H , where H is the number of heads and d is the dimension of embedding. the attention weights
amnh for head h between token m and n is given by scaled dot product between

amnh = σ(

qT
mhknh√
d

)

omh =

(cid:88)

j

vjh (cid:12) amjh

(27)

(28)

Finally the output of each head in multi-head attention in concatenated serially.

8

A PREPRINT - SEPTEMBER 24, 2019

Table 2: Number of Trainable parameters in each model

Model
Sequence-to-Sequence
Attention Encoder-Decoder
Self-Attention Transformer

Trainable parameters
38,678,595
38,804,547
122,699,776

Table 3: Model performance in terms of BLEU score on the task of Hindi-English and English-Hindi translation task

Model
Sequence-to-Sequence
Attention Encoder-Decoder
Self-Attention Transformer

Hindi-English English-Hindi
9.40
11.59
13.96

8.38
10.13
13.47

7 Experiments and results

7.1 Neural models

For this study, self attention based transformer network is implemented and compared using Sequence-to-sequence
and attention based encoder decoder neural architectures. All the implementation and coding part is done using the
above mentioned programming framework. We train all the three models in an end to end manner using CFILT
Hindi-English parallel corpora and the results from all the three models are compared keeping in mind the usage of
similar hyper-parameter values for ease of comparison.

For the sequence-to-sequence model, we are using LSTM cell for computation since there is no attention mechanism
involved and it is desirable for the model to capture long term dependencies. Since LSTM works far better than GRU
cell in terms of capturing long term dependencies we chose to go with it. The embedding layer, hidden layer is taken to
be of 512 dimension, both encoder and the decoder part contains 2 as the number of hidden layers. For regularization,
we are using dropout and set the value to be 20 percent. Batch size of 128 is taken.

For the attention based RNN search, we are using GRU cell for computation since attention mechanism is already
employed and will capture long term dependencies explicitly using attention value. GRU cell is computationally
efﬁcient in terms of computation as compared with LSTM cell. Like in our sequence-to-sequence model, the embedding
layer, hidden layer is taken to be of 512 dimension, both encoder and the decoder part contains 2 as the number of
hidden layers. For regularization, we are using dropout and set the value to be 20 percent. Batch size of 128 is taken.
For the self attention Transformer network, we are using hidden and embedding layer to be of size 512, for each encoder
and decoder, we ﬁx the number of layers of self-attention to be 4. In each layer, we assign 8 parallel attention heads and
the hidden size of feed forward neural network is taken to be 1024 in each cell. attention dropout and residual dropout
is taken to be 10 percent. Table 2: shows the number of trainable parameters in each of our three models.

The optimizer used for our study is the Adam optimizer, taking learning_rate decay value of 0.001, β1 value of 0.9 and
β2 value of 0.98 for ﬁrst order and second order gradient moments respectively. The accuracy metric taken is log-loss
error between the predicted and actual sentence word. We are trying to optimize the log-loss error between the predicted
and target words of the sentence. All the models are trained on 100000 steps, where is each step, a batch size of 128 is
taken for calculating the loss function. The primary objective of our training is to minimize the log-loss error between
the source and target sentences and simultaneously maximize the metric which is chosen to be the BLEU score [24].

8 Results

Table 3 shows the BLEU score of all three models based on English-Hindi, Hindi-English on CFILT’s test dataset
respectively. From the results which we get, it is evident that the transformer model achieves higher BLEU score
than both Attention encoder-decoder and sequence-sequence model. Attention encoder-decoder achieves better BLEU
score and sequence-sequence model performs the worst out of the three which further consolidates the point that if
we are dealing with long source and target sentences then attention mechanism is very much required to capture long
term dependencies and we can solely rely on the attention mechanism, overthrowing recurrent cells completely for the
machine translation task.

Figure 5 shows the word-word association heat map for selected translated and target sentences when transformer
model is trained on English-Hindi translation task and similarly Figure 6 shows the word-word association heat map for
selected translated and target sentences when transformer model is trained on Hindi-English translation task.

9

A PREPRINT - SEPTEMBER 24, 2019

(a) Sample 1

(b) Sample 2

Figure 5: Hindi-English Heatmap

(a) Sample 1

(b) Sample 2

Figure 6: English-Hindi Heatmap sample

9 Conclusion

In this paper, we initially discussed about Machine translation. We started our discussion from a brief discussion
on basic Machine translation objective and terminologies along with early Statistical approaches (SMT). We then
discussed the role of deep learning models in improving different components of SMT, Then we shifted our discussion
on end-to-end neural machine translation (NMT). Our discussion was largely based on the basic encoder-decoder based
NMT, attention based model. We ﬁnally listed the challenges in Neural Translation models and mentioned future ﬁelds
of study and open ended problems. Later we proposed a self-attention transformer network for Hindi-English language
translation and compare this model with other neural machine translation models on the basis of BLEU. We concluded
our study by delineating the advantages and disadvantages of all the three models.

10

A PREPRINT - SEPTEMBER 24, 2019

References

[1] Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. The mathematics of statistical

machine translation: Parameter estimation. Computational linguistics, 19(2):263–311, 1993.

[2] Franz Josef Och and Hermann Ney. Discriminative training and maximum entropy models for statistical machine
In Proceedings of the 40th annual meeting on association for computational linguistics, pages

translation.
295–302. Association for Computational Linguistics, 2002.

[3] Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based translation. In Proceedings of the
2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology-Volume 1, pages 48–54. Association for Computational Linguistics, 2003.

[4] David Chiang. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd
Annual Meeting on Association for Computational Linguistics, pages 263–270. Association for Computational
Linguistics, 2005.

[5] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. Fast and
robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1370–1380, 2014.

[6] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances

in neural information processing systems, pages 3104–3112, 2014.

[7] Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align

and translate. Transactions in Association of Computational Linguistics, pages 154–161, 2014.

[8] Stephan Vogel, Hermann Ney, and Christoph Tillmann. c. In Proceedings of the 16th conference on Computational

linguistics-Volume 2, pages 836–841. Association for Computational Linguistics, 1996.

[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, pages 1735–1780,

1997.

[10] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder–decoder for statistical machine translation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1724–1734, 2014.

[11] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to

sequence learning. In International Conference on Machine Learning, pages 1243–1252, 2017.

[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages
5998–6008, 2017.

[13] Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. Word alignment modeling with context dependent
deep neural network. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 166–175, 2013.

[14] Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. Learning continuous phrase representations for translation
modeling. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 699–709, 2014.

[15] Peng Li, Yang Liu, and Maosong Sun. Recursive autoencoders for itg-based translation. In Proceedings of the

2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, 2013.

[16] Peng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, and Dakun Zhang. A neural reordering model for phrase-based
translation. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics:
Technical Papers, pages 1897–1907, 2014.

[17] Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. Decoding with large-scale neural language
models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1387–1392, 2013.

[18] Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. Visualizing and understanding neural machine
translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 1150–1159, 2017.

[19] Gilvil˙e Stankeviˇci¯ut˙e, Ramun˙e Kasperaviˇcien˙e, and Jolita Horbaˇcauskien˙e. Issues in machine translation. Interna-

tional Journal on Language, Literature and Culture in Education, 4(1):75–88, 2017.

11

A PREPRINT - SEPTEMBER 24, 2019

[20] Sheila Castilho, Joss Moorkens, Federico Gaspari, Rico Sennrich, Andy Way, and Panayota Georgakopoulou.

Evaluating mt for massive open online courses. Machine Translation, pages 1–24, 2018.

[21] Alina Karakanta, Jon Dehdari, and Josef van Genabith. Neural machine translation for low-resource languages

without parallel corpora. Machine Translation, pages 1–23, 2018.

[22] Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. The iit bombay english-hindi parallel corpus.

arXiv preprint arXiv:1710.02855, 2017.

[23] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolin-
gual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 86–96, 2016.

[24] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics,
pages 311–318. Association for Computational Linguistics, 2002.

12

