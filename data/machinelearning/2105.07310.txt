Regret Analysis of Distributed Online LQR Control
for Unknown LTI Systems

Ting-Jui Chang and Shahin Shahrampour, Senior Member, IEEE

2
2
0
2

b
e
F
6

]

C
O
.
h
t
a
m

[

2
v
0
1
3
7
0
.
5
0
1
2
:
v
i
X
r
a

Abstract—Online optimization has recently opened avenues to
study optimal control for time-varying cost functions that are
unknown in advance. Inspired by this line of research, we study
the distributed online linear quadratic regulator (LQR) problem
for linear time-invariant (LTI) systems with unknown dynamics.
Consider a multi-agent network where each agent is modeled as a
LTI system. The network has a global time-varying quadratic cost,
which may evolve adversarially and is only partially observed by
each agent sequentially. The goal of the network is to collectively
(i) estimate the unknown dynamics and (ii) compute local control
sequences competitive to the best centralized policy in hindsight,
which minimizes the sum of network costs over time. This
problem is formulated as a regret minimization. We propose a
distributed variant of the online LQR algorithm, where agents
compute their system estimates during an exploration stage. Each
agent then applies distributed online gradient descent on a semi-
deﬁnite programming (SDP) whose feasible set is based on the
agent system estimate. We prove that with high probability the
regret bound of our proposed algorithm scales as O(T 2/3 log T ),
implying the consensus of all agents over time. We also provide
simulation results verifying our theoretical guarantee.

I. INTRODUCTION

In recent years, there has been a signiﬁcant interest on prob-
lems arising at the interface of control and machine learning.
Among classical control problems, LQR control [1]–[3] is
a prominent point in case. LQR control centers around LTI
systems, where the control-state pairs introduce a quadratic
cost with time-invariant parameters. When the dynamics of
the LTI system is known, for ﬁnite-horizon and inﬁnite-horizon
problems, the optimal controllers have closed-form solutions,
which can be derived by solving the corresponding Riccati
equations.

Despite the excellent insights on the LQR problem provided
by the classical control theory, in practical problems we might
encounter two challenges. (I) The environment could change
in an unpredictable way, which makes the cost parameters
time-varying and unknown in advance (e.g., in variable-supply
electricity production and building climate control with time-
varying energy costs [4]). (II) Furthermore,
the dynamics
of the LTI system may be unknown. The former challenge
has motivated research at the interface of online optimization
and control, where online LQR problem is cast as a regret
minimization and the performance of an online algorithm is
compared to that of the best ﬁxed control policy in hindsight.
The regret metric is particularly meaningful in the online
setting, where the cost parameters are unknown in advance.
The focus of online LQR is on the ﬁnite-time performance

T.J. Chang and S. Shahrampour are with the Department of Mechanical
and Industrial Engineering, Northeastern University, Boston, MA 02115, USA.
email:{chang.tin,s.shahrampour}@northeastern.edu.

This work is supported in part by NSF ECCS-2136206 Award.

from a learning-theory perspective (see details of this literature
in item 4 of Subsection I-A). The latter challenge is addressed
via adaptive control in general. In this case, the learner must
strike a balance between exploration (estimating the system
dynamics while preventing the cumulative cost from going
unbounded) and exploitation (using the estimates to compete
with the performance of the optimal controller) [5]–[8].

In this work, we consider the distributed online LQR prob-
lem for a network of LTI systems with unknown dynamics.
Each system is represented by an agent in the network that
has a global time-varying quadratic cost. The cost sequence
may evolve adversarially and is only partially observed by
each agent sequentially. The goal of each agent is to generate
a control sequence that is competitive to that of the best
centralized policy in hindsight, formulated by regret. This
setting can be applied for modeling the energy consumption in
mobile sensor networks as described in Example 1. To address
the problem, we propose a decentralized algorithm with two
phases. In the exploration phase, each agent computes system
estimates using the EXTRA algorithm [9], which is an iter-
ative decentralized optimization method. In the exploitation
phase, agents perform distributed online gradient descent on
a SDP (whose feasible set is constructed by local system
estimates) and extract the control policies accordingly. We
prove that if every agent maintains a good balance between
system identiﬁcation (exploration) and online control (ex-
ploitation), the regret is bounded by O(T 2/3 log T ), where T
is the total number of iterations. This implies that the agents
reach consensus and collectively compete with the best ﬁxed
controller in hindsight. Besides the exploration-exploitation
trade-off, the main technical challenge is that the decentralized
system identiﬁcation step results in different SDPs across
agents. This implies that the feasible set of SDP varies from
one agent to another, and we cannot directly use distributed
online optimization results on a common feasible set. We
draw upon techniques from alternating projections to tackle
this problem. Our technical proof is provided in the Appendix
(Section VI). We also provide simulation results verifying our
regret bound.

A. Related Literature

(1) Distributed LQR Control: Distributed LQR has been
widely studied in the control literature. A number of works
focus on multi-agent systems with known, identical decoupled
dynamics. In [10], a distributed control design is proposed
by solving a single LQR problem whose size scales with
the maximum degree of the graph capturing the network.
The authors of [11] derive the necessary condition for an
optimal distributed controller design, resulting in a non-convex

 
 
 
 
 
 
optimization problem. The work of [12] addresses a multi-
agent network, where the dynamics of each agent is a single
integrator. The authors of [12] show that the computation of
the optimal controller requires the knowledge of the graph
and the initial information of all agents. Given the difﬁculty
of precisely solving the optimal distributed controller, Jiao
et al. [13] provide the sufﬁcient conditions to obtain sub-
optimal controllers. All of the aforementioned works need
global information such as network topology to compute the
controllers. On the other hand, Jiao et al. [14] propose a
decentralized method to compute the controllers and show that
the system will reach consensus. For the case of unknown dy-
namics, Alemzadeh et al. [15] propose a distributed Q-learning
algorithm for dynamically decoupled systems. There are other
works focusing on distributed control without assuming identi-
cal decoupled sub-systems. Fattahi et al. [16] study distributed
controllers for unknown and sparse LTI systems. Furieri et al.
[17] address model-free methods for distributed LQR problems
and provide sample-complexity bounds for problems with lo-
cal gradient dominance property (e.g., quadratically-invariant
problems). The work of [18] investigates the convergence of
distributed controllers to a global minimum for quadratically
invariant problems with ﬁrst-order methods.

(2) System Identiﬁcation of LTI Systems: For solving LQR
problems with unknown dynamics, we ﬁrst need to learn
the underlying system. To provide performance guarantee
for the controller, it is important to explicitly quantify the
uncertainty of the model estimate. The classical theory of
system identiﬁcation for LTI systems (e.g., [19]–[22]) char-
acterizes the asymptotic properties of the estimators. On the
contrary, recent results in statistical learning focus on ﬁnite-
time guarantees. In [23], it is shown that for fully observable
systems, a least-squares estimator can learn the underlying
dynamics from multiple trajectories. These results are later
extended to the estimation using a single trajectory [24], [25].
For partially observable systems, estimators with polynomial
sample complexities are provided in the literature (e.g., [26]–
[29]), and the work of [30] improves the sample complexity
to be poly-logarithmic.

(3) Online LQR with Unknown Dynamics and Time-
Invariant Costs: There is a recent line of research dealing
with LQR control problems with unknown dynamics. Several
techniques are proposed using (i) gradient estimation (e.g.,
[31]–[34]), (ii) the estimation of dynamics matrices and deriva-
tion of the controller by considering the estimation uncertainty
(e.g., [7], [8], [23], [35]–[37]), and (iii) wave-ﬁltering [38],
[39].

(4) Online Control with Time-Varying Costs: Recently,
there has been a signiﬁcant interest in studying linear dynam-
ical systems with time-varying cost functions, where online
learning techniques are applied. This literature investigates two
scenarios: I) Known Systems: Cohen et al. [4] study the SDP
relaxation for online LQR control and establish a regret bound
T ) for known LTI systems with time-varying quadratic
of O(
costs. Agarwal et al. [40] propose the disturbance-action policy
parameterization and reduce the online control problem to

√

√

online convex optimization with memory. They show that
for adversarial disturbances and arbitrary time-varying convex
T ). Agarwal et al. [41] consider
functions, the regret is O(
the case of time-varying strongly-convex functions and im-
prove the regret bound to O(poly(logT )). Simchowitz et al.
[42] further extend the O(poly(logT )) regret bound to partially
observable systems with semi-adversarial disturbances. Yu et
al. [43] incorporate the idea of model predictive control into
online LQR control with time-invariant cost function and
correct noise predictions. Zhang et al. [44] extend this idea
to the setup where costs are time-varying and accurate dis-
turbance predictions are not accessible. Both of them provide
the dynamic regret bound with a term shrinking exponentially
with the prediction window. Our previous work [45] studies
the distributed online LQR control with known dynamics and
T ). II) Unknown Systems:
provides the regret bound of O(
For fully observable systems, Hazan et al. [46] derive the regret
of O(T 2/3) for time-varying convex functions with adversarial
noises. For partially observable systems, the work of [42]
addresses the cases of (i) convex functions with adversarial
noises and (ii) strongly-convex functions with semi-adversarial
noises, and provide regret bounds of O(T 2/3) and O(
T ),
respectively. Lale et al. [47] establish an O(poly(logT )) regret
bound for the case of stochastic perturbations, time-varying
strongly-convex functions, and partially observed states.

√

√

Our work lies precisely at the interface of distributed LQR,
online LQR and adaptive control, addressing distributed online
LQR with unknown dynamics.

II. PRELIMINARIES AND PROBLEM FORMULATION

A. Notation

The set {1, 2, . . . , n} for any integer n
The trace operator
Euclidean (spectral) norm of a vector (matrix)
Frobenius norm of a matrix
The expectation operator
The operator for the projection to set S
The entry in the i-th row and j-th column of A
The j-th column of A

[n]
Tr(·)
(cid:107)·(cid:107)
(cid:107)·(cid:107)F
E[·]
ΠS [·]
[A]ij
[A]:,j
A • B Tr(A(cid:62)B)
A (cid:23) B (A − B) is positive semi-deﬁnite
The vector of all ones
The i-th basis vector
Vectorized version of the matrix A

1
ei
vec(A)

B. Distributed Online LQR Control with Unknown Dynamics

We consider a multi-agent network of m LTI systems, where

the dynamics of agent i is given as,

xi,t+1 = Axi,t + Bui,t + wi,t,

i ∈ [m]

and xi,t ∈ Rd and ui,t ∈ Rk represent agent i state and control
(or action) at time t, respectively. Furthermore, A ∈ Rd×d,
B ∈ Rd×k, and wi,t is a Gaussian noise with zero mean
and covariance W (cid:23) σ2I. The system parameters (A, B) are
unknown to all agents and need to be estimated. The noise
sequence {wi,t} is independent over time and agents. We
also assume that (cid:107)[A B](cid:107)F ≤ ϑ and let n := d + k for the
presentation simplicity.

i,tQi,txi,t + u(cid:62)

Departing from the classical LQR control, we consider the
online distributed LQR problem, where the cost functions are
unknown in advance. At round t, agent i receives the state
xi,t and applies the action ui,t. Then, positive semi-deﬁnite
cost matrices Qi,t and Ri,t are revealed, and the agent incurs
the cost x(cid:62)
i,tRi,tui,t. Throughout this paper,
we assume that Tr(Qi,t), Tr(Ri,t) ≤ C for all i, t and some
C > 0. Agent i follows a policy that selects the control
ui,t based on the observed cost matrices Qi,1, . . . , Qi,t−1 and
Ri,1, . . . , Ri,t−1, as well as the information received from
its local neighborhood. This policy is not driven based on
individual costs. On the contrary, agents follow a team goal
through minimizing a cost collectively as we describe next.

Centralized Benchmark: In order to gauge the performance
of a distributed online LQR algorithm, we require a centralized
benchmark. In this paper, we focus on the ﬁnite-horizon
problem, where for a centralized policy π, the cost after T
steps is given as

JT (π) = E

(cid:34) T

(cid:88)

t=1

xπ
t

(cid:62)Qtxπ

t + uπ
t

(cid:62)Rtuπ
t

,

(1)

(cid:35)

t and xπ

t for a matrix K ∈ Rk×d.

where Qt = (cid:80)m
i=1 Qi,t and Rt = (cid:80)m
i=1 Ri,t, and the
expectation is over the possible randomness of the policy as
well as the noise. The superscript π in uπ
t alludes that
the state-control pairs are chosen by the policy π, given full
access to cost matrices of all agents. Notice that in the inﬁnite-
horizon version of the problem with time-invariant cost matri-
ces (Q, R), where the goal is to minimize limT →∞ JT (π)/T ,
it is well-known that for a controllable LTI system (A, B), the
optimal policy is given by the constant linear feedback, i.e.,
t = Kxπ
uπ
Regret Deﬁnition: The goal of a distributed online LQR
algorithm A is to mimic the performance of an ideal cen-
tralized algorithm that solves (1). The main two challenges
are (i) the online nature of the problem, where cost matrices
become available sequentially, and (ii) the distributed setup,
where agent i only receives information about the sequence
{Qi,t, Ri,t} while the cost is based on {Qt, Rt}. In this
setting, each agent j locally generates the control sequence
{uj,t}T
t=1, that is competitive to the best policy among a
benchmark policy class Π. This can be formulated as mini-
mizing the individual regret, which is deﬁned as follows

Network Structure: The underlying network topology is
represented by a symmetric doubly stochastic matrix P,
i.e., all elements of P are non-negative and (cid:80)m
i=1[P]ji =
(cid:80)m
j=1[P]ji = 1. If [P]ji > 0, agents i and j are neighbors;
otherwise [P]ji = 0. The network is assumed to be connected,
i.e., for any two agents i, j ∈ [m], there is a (potentially
multi-hop) path from i to j. We also assume P has a positive
diagonal. Then, there exists a geometric mixing bound for
P [48], such that (cid:80)m
mβk, i ∈ [m],
j=1
where β is the second largest singular value of P. Agents
cannot share their observed cost functions with each other, but
they can exchange a local parameter used for constructing the
controllers. The communication is consistent with the structure
of P. We elaborate on this in the algorithm description.

(cid:12)[Pk]ji − 1/m(cid:12)
(cid:12)

(cid:12) ≤

√

Example 1. Our framework can be used for minimizing the
energy consumption in mobile sensor networks (MSNs) [49]
in time-varying settings. Consider a MSN where at time t
the total mobility cost (or budget) of sensors is modeled
by matrices (Qt, Rt). Each agent i has a local budget of
(Qi,t, Ri,t), but
is to design actions that
minimize the global network cost over time. Then, actions of
this MSN should be guided to minimize the global cost in (1),
though each sensor only has local information.

the team goal

C. Strong Stability and Sequential Strong Stability

We consider the set of strongly stable linear (i.e., u = Kx)
controllers as the benchmark policy class. Following [4], we
deﬁne the notion of strong stability as follows.

Deﬁnition 1. (Strong Stability) A linear policy K is (κ, γ)-
strongly stable (for κ > 0 and 0 < γ ≤ 1) for the LTI
system (A, B), if (cid:107)K(cid:107) ≤ κ, and there exist matrices L and
H such that A + BK = HLH−1, with (cid:107)L(cid:107) ≤ 1 − γ and
(cid:107)H(cid:107)(cid:107)H−1(cid:107) ≤ κ.

Strong stability is a quantitative version of stability, in the
sense that any stable policy is strongly stable for some κ and γ,
and vice versa [4]. A strongly stable policy ensures fast mixing
and exponential convergence to a steady-state distribution. In
particular, for the LTI system xt+1 = Axt + But + wt, if a
(κ, γ)-strongly stable policy K is applied (ut = Kxt), (cid:98)Xt (the
state covariance matrix of xt) converges to X (the steady-state
covariance matrix) with the following exponential rate

Regretj

T (A) := J j

T (A) − min
π∈Π

JT (π),

(2)

(cid:107) (cid:98)Xt − X(cid:107) ≤ κ2e−2γt(cid:107) (cid:98)X0 − X(cid:107).

for agent j ∈ [m], where

J j
T (A) = E

(cid:34) T

(cid:88)

t=1

(cid:62)

xA
j,t

QtxA

j,t + uA
j,t

(cid:62)

(cid:35)

RtuA
j,t

.

(3)

A successful distributed algorithm is one that keeps the regret
sublinear with respect to T . Of course, this also depends on
the choice of the benchmark policy class Π, which is assumed
to be the set of strongly stable policies (to be deﬁned precisely
in Section II-C). Since the underlying dynamics is unknown,
agents have to ﬁnd a good trade-off between exploration
(estimating the system parameters) and exploitation (keeping
the regret sublinear).

See Lemma 3.2 in [4] for details. The sequential nature of
online LQR control requires another notion of strong stability,
called sequential strong stability [4], deﬁned as follows.

Deﬁnition 2. (Sequential Strong Stability) A sequence of lin-
ear policies {Kt}T
t=1 is (κ, γ)-strongly stable if there exist ma-
t=1 such that A+BKt = HtLtH−1
trices {Ht}T
t=1 and {Lt}T
for all t with the following properties,
1) (cid:107)Lt(cid:107) ≤ 1 − γ and (cid:107)Kt(cid:107) ≤ κ.
2) (cid:107)Ht(cid:107) ≤ β(cid:48) and (cid:107)H−1
3) (cid:107)H−1
t+1Ht(cid:107) ≤ 1 + γ/2.

t (cid:107) ≤ 1/α(cid:48) with κ = β(cid:48)/α(cid:48).

t

Sequential strong stability generalizes strong stability to the

time-varying scenario, where a sequence of policies {Kt}T

t=1

is used. The convergence of steady-state covariance matrices
induced by {Kt}T

t=1 is characterized as follows.

Lemma 1. (Lemma 3.5 in [4]) Suppose a time-varying policy
(ut = Ktxt) is applied. Denote the steady-state covariance
matrix of Kt as Xt. If {Kt} are (κ, γ)-sequentially strongly
stable and (cid:107)Xt − Xt−1(cid:107) ≤ η, (cid:98)Xt (the state covariance matrix
of xt) converges to Xt as follows

(cid:107) (cid:98)Xt+1 − Xt+1(cid:107) ≤ κ2e−γt(cid:107) (cid:98)X1 − X1(cid:107) +

2ηκ2
γ

.

D. SDP Relaxation for LQR Control

For the following dynamical system

xt+1 = Axt + But + wt, wt ∼ N (0, W),

version

inﬁnite-horizon

the
i.e.,
minimize limT →∞ JT (π)/T, with ﬁxed cost matrices Q
and R can be relaxed via a SDP when the steady-state
distribution exists. For ν > 0,
the SDP relaxation is
formulated as [4]

(1),

of

minimize J(Σ) =

(cid:19)

(cid:18)Q 0
0 R

• Σ

subject to Σxx = [A B]Σ[A B](cid:62) + W,

(4)

where

Σ (cid:23) 0, Tr(Σ) ≤ ν,

Σ =

(cid:18)Σxx Σxu
Σux Σuu

(cid:19)

.

Recall that in the online LQR problem, we deal with time-
varying cost matrices (Qt, Rt), and for any t ∈ [T ], the above
SDP yields different solutions. In fact, for any feasible solution
xuΣ−1
Σ of the above SDP, a strongly stable controller K = Σ(cid:62)
xx
can be extracted. The steady-state covariance matrix induced
by this controller is also feasible for the SDP and its cost is
at most that of Σ (see Theorem 4.2 in [4]). Moreover, for any
(slowly-varying) sequence of feasible solutions to the SDP, the
induced controller sequence is sequentially strongly-stable.

E. Challenges of Distributed Online LQR for Unknown Dy-
namical Systems

The works of [4] and [45] tackle the centralized and decen-
tralized online LQR, respectively. To keep the regret sublinear,
the key idea in online LQR is to construct sequentially strongly
stable controllers using online gradient descent (projected to
the feasible set of SDP in (4)). However, in our work, given
that system parameters (A, B) are unknown, the agents must
perform a system identiﬁcation ﬁrst. The system identiﬁcation
step results in two challenges: (i) an exploration-exploitation
trade-off to keep the regret sublinear, and (ii) different SDPs
across agents as a result of decentralized estimation. The
latter is particularly challenging, because as we can see in
(4), each agent only has a local estimate of (A, B), so the
SDPs will have different feasible sets across agents, and we
cannot directly apply distributed online optimization results
on a common feasible set (e.g., [50], [51]). In this work, we
propose an algorithm (in the next section) for which we prove
that an extracted controller based on a precise enough system
estimates ( (cid:98)A, (cid:98)B) is strongly stable w.r.t. the system (A, B).

III. ALGORITHM AND THEORETICAL RESULTS

We now develop the distributed online LQR algorithm for

unknown systems and study its theoretical regret bound.

A. Algorithm

Our proposed method is outlined in Algorithm 1. In the
ﬁrst T0 + 1 iterations, we need to collect data for the system
identiﬁcation. Suppose that each agent has access to a con-
troller K0, which is (κ0, γ0)-strongly stable w.r.t. the system
(A, B). This controller can be different across agents, but
for the presentation simplicity, we assume that agents use
the same controller K0. The knowledge of such controller
is a common assumption in centralized LQR (see e.g., [7],
[8]). In this period, agent i at time t applies the control
ui,t ∼ N (K0xi,t, 2σ2κ2
0 · I), which prevents the state xi,t
from going unbounded (lines 2-7). For the next T1 iterations,
all agents perform the system identiﬁcation step by solving a
distributed least-squares (LS) problem. In this step, the global
LS problem is formed using the data collected by all agents,
where each agent local cost is only based on its own collected
data. Here, we can use any iterative distributed optimization
algorithm to get precise enough system estimates. We employ
the EXTRA algorithm [9] since it achieves a geometric rate
for strongly convex problems, and it can be implemented in a
decentralized fashion (lines 8-16). After T0 +T1 +1 iterations,
each agent i at time t runs a distributed online gradient descent
on the SDP (4), where the local cost is deﬁned w.r.t. matrices
Qi,t and Ri,t, and the feasible set is deﬁned w.r.t. system
estimates ( (cid:98)Ai,t, (cid:98)Bi,t). A control matrix Ki,t is then extracted
from the update of Σi,t and is used to determine the action.
In particular, ui,t is sampled from a Gaussian distribution
N (Ki,txi,t, Vi,t), which entails E[ui,t|Ft] = Ki,txi,t, where
Ft is the smallest σ-ﬁeld containing the information about all
agents up to time t (lines 17-26). The choice of Vi,t is due
to a technical reason. It ensures the fast convergence of the
covariance matrix of xi,t to the steady-state covariance matrix,
when applying Ki,t to the underlying system (A, B).

B. Theoretical Result: Regret Bound

In this section, we present our main theoretical result. By
applying Algorithm 1, we show that for a multi-agent network
of unknown LTI systems (with a connected communication
graph), the individual regret of an arbitrary agent is upper-
bounded by O(T 2/3 log T ), which implies that
the agents
collectively perform as well as the best ﬁxed controller in
hindsight for large enough T .

is

that

the network

Theorem 2. Assume
connected,
(cid:107)[A B](cid:107)F ≤ ϑ, Tr(Qi,t), Tr(Ri,t) ≤ C, Tr(W) ≤ λ2 and
W (cid:23) σ2I. Given κ ≥ 1 and 0 ≤ γ < 1, set ν = 2κ4λ2/γ
and step size η = T −1/3. If we run Algorithm 1 with
T0 = T 2/3 log(T /δ) and T1 = O(log(T 1/3)),
then with
probability (1−δ), the individual regret of agent j with respect
to any (κ, γ)-strongly stable controller Ks is bounded as
follows

Regretj

T (A) = J j

T (A) − JT (Ks) = O(T 2/3 log T ),

Algorithm 1 Online Distributed LQR Control with Unknown
Dynamics

1: Require: number of agents m, doubly stochastic matrix
P ∈ Rm×m, parameter ν, step size η, a (κ0, γ0)-strongly
stable controller K0 w.r.t. system matrices (A, B), covari-
ance parameter of the noise σ, parameter ϑ.
Initialize: xi,1 = 0, ∀i ∈ [m].

2: for t = 1, 2, . . . , T0 + 1 do
3:

for i = 1, 2, . . . , m do

Receive xi,t
Perform action ui,t ∼ N (K0xi,t, 2σ2κ2

4:
5:
6:
7: end for
8: After the ﬁrst (T0 + 1) iterations, each agent i uses the

end for

0 · I)

collected data to form the local function

fi(A, B) :=

T0(cid:88)

(cid:107)[AB]zi,t −xi,t+1(cid:107)2 +

σ2ϑ−2
m

(cid:107)[AB](cid:107)2
F ,

t=1
where zi,t = [x(cid:62)

i,t u(cid:62)

i,t](cid:62).

9: Choose the step size α following the result in [9] and
denote ˜P := I+P
the agent i vector-
2 . Denote by (cid:98)Di
ized system estimate [ (cid:98)Ai (cid:98)Bi]. Apply EXTRA to solve
the global LS problem (cid:80)m
i=1 fi(A, B) in a distributed
fashion.

i for all i ∈ [m].
j − α∇fi( (cid:98)D0
i ).

10: Randomly generate (cid:98)D0
i = (cid:80)m
11: ∀i, (cid:98)D1
j=1[P]ji (cid:98)D0
12: for k = 0, 1, . . . , T1 − 1 do
13:
14:

for i = 1, 2, . . . , m do

= (cid:80)m

(cid:98)Dk+2
i
α[∇fi( (cid:98)Dk+1

i

) − ∇fi( (cid:98)Dk

j
i )].

j=1 2[ ˜P]ji (cid:98)Dk+1

− (cid:80)m

j=1[ ˜P]ji (cid:98)Dk

j −

end for

15:
16: end for
17: For all i ∈ [m], transform the vectorized (cid:98)DT1+1
matrix form [ (cid:98)Ai,t (cid:98)Bi,t] for all t ≥ (T0 + 1) + T1.

i

18: Let Ts := (T0 + T1 + 2).
19: Initialize Σi,Ts = ΣTs for any i ∈ [m].
20: for t = Ts, . . . , T do
21:
22:
23:

for i = 1, 2, . . . , m do

Receive xi,t
Compute Ki,t = (Σi,t)ux(Σi,t)−1
(Σi,t)uu − Ki,t(Σi,t)xxK(cid:62)
i,t
Perform ui,t ∼ N (Ki,txi,t, Vi,t) and observe
Qi,t, Ri,t

xx and Vi,t =

Σi,t+1 = ΠS i

t+1

[P]jiΣj,t − η

(cid:34) m
(cid:80)
j=1

(cid:18)Qi,t

(cid:19)(cid:35)

0

0 Ri,t

24:

25:

where

(cid:26)

S i

t+1 :=

Σ ∈ Rn×n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Σ (cid:23) 0, Tr(Σ) ≤ ν,
Σxx = (cid:98)Ci,t+1Σ (cid:98)C(cid:62)

i,t+1 + W

and (cid:98)Ci,t+1 = [ (cid:98)Ai,t+1 (cid:98)Bi,t+1].

end for

26:
27: end for

for T large enough.

,

(cid:27)

,

The exact expression of the lower bound for T is given in

(44), and it depends on m, n, κ, γ, β, δ. The details of the proof
are provided in the Appendix, and Section III-C highlights the
key technical challenges.

√

Remark 1. For online LQR control with known dynamics, [4]
T ) for centralized and
and [45] prove regret bounds of O(
distributed cases, respectively. However, in this work, since the
system is unknown, agents need to compute system estimates
ﬁrst. This brings forward an exploration cost that increases
the order of regret. In other words, agents objective is still to
minimize the regret, but if they do not collect enough data,
the estimation error propagates into the exploitation phase,
yielding a larger regret (in orders of magnitude).

Remark 2. For online control with unknown dynamics, both
[46] and [42] consider the setup where costs are time-varying
convex functions with adversarial noises, and they derive
the regret bound of O(T 2/3) for fully observable systems
and partially observable systems, respectively. In this work,
we consider the distributed variant of online LQR control
with stochastic noises and unknown dynamics. Our regret
bound of O(T 2/3 log T ) is consistent with previous results on
centralized problems in the convex setting (disregarding the
log factor).

Remark 3. The exact expression of regret (given in (43))
shows its dependence to (1 − β)−1, where β is the second
largest singular value of P. This implies that when the network
is well-connected (i.e., β is smaller), the resulting bound is
tighter. A smaller β allows the Markov chain P to mix faster,
which intuitively results in faster information propagation over
the network of agents.

The regret can be decomposed into three terms, where each
term must be small enough to bound the regret. In [4], two of
these terms are bounded using the properties of strong stability
and sequential strong stability, and one term is bounded using
the standard regret bound for online gradient descent. In our
setup, since (A, B) is unknown, the agents cannot work with
the ideal feasible set in (4), and as evident from line 25
of the algorithm, S i
t+1 is constructed only based on agent i
system estimate. This brings forward two challenges. (i) Agent
i constructs the controllers based on iterates Σi,t+1 that are not
necessarily in the feasible set of (4), so we need to establish
the stability properties of these controllers. (ii) The feasible
sets are different across agents (i.e., S i
t+1 for i (cid:54)= j),
so we cannot directly apply distributed online optimization
results on a common feasible set.

t+1 (cid:54)= S j

To tackle the ﬁrst challenge, we ﬁrst derive the bound on the
precision of each agent system estimate based on the EXTRA
algorithm (see Lemma 6) and combine that with statistical
properties of centralized LS estimation using results of [8].
We then establish that if each agent system estimate is close
enough to the true system, a strongly stable policy w.r.t. the
system estimate is also strongly stable w.r.t. the true system
(see Lemma 3 and Lemma 4). To address the second challenge,
we use alternating projections to prove that a point in the

back to

C. Key Technical Challenges in the Proof

(a) The plot of individual regrets of all agents over
time.

(b) The temporal average of regret converges to
zero for all agents.

(c) The averaged regrets over time for different net-
works: more connectivity results in smaller regret.

Fig. 1: The individual regrets of all agents are shown to be sublinear.

feasible set of one agent is close enough to its projection to
the feasible set of another agent, when estimates of these two
agents are close. Then, in Theorem 9, we show the contribution
of distributed online optimization to the regret. We ﬁnally put
together these results to prove our regret bound.

IV. NUMERICAL EXPERIMENTS

We now provide numerical simulations verifying the theo-

retical guarantee of our algorithm.

Experiment Setup: We ﬁrst consider a network of m = 20
agents, captured by a cyclic graph, where each agent has a
self-weight of 0.6 and assigns the weight 0.2 to each of its
two neighbors. The (hyper)-parameters are set as follows: d =
k = 3, κ = 1.5, γ = 0.4, C = 300. We let matrices A =
(1 − 2γ)I and B = (γ/κ)I to ensure the existence of a (κ, γ)-
strongly stable controller. For time-varying cost matrices, we
set Qi,t (respectively, Ri,t) as a diagonal matrix where each
diagonal term is sampled from the uniform distribution over
[0, C/d] (respectively, [0, C/k]), so that Tr(Qi,t), Tr(Ri,t) ≤
C. The disturbance wi,t is sampled from a standard Gaussian
distribution, and thus λ2 = d = 3 and σ2 = 1.

for T

simulate Algorithm 1

=
Simulation: We
the benchmark, we set Ks
1K, 2K, 3K, . . . , 60K. For
as (−κ)10−2I which is (κ, γ)-strongly stable w.r.t. A, B,
and the resulting cumulative cost
is small enough to be
the benchmark. For the projection on the feasible set, we
apply Dykstra’s projection algorithm. Due to ﬂoating-point
computations, Vi,t for action-sampling may not be positive
semi-deﬁnite (PSD). Therefore, we address it by adding to
Vi,t a small term ((1e − 15)I) to keep it PSD. The entire
process is repeated for 50 Monte-Carlo simulations, and in
the ﬁgures we present the averaged plots.

Iterations
Averaged Regret
Standard Error

20K
1.424
0.0087

30K
1.34
0.0097

40K
1.248
0.0052

50K
1.201
0.0032

60K
1.168
0.0037

TABLE I: The mean and standard error of the averaged regret over
time and agents (×10−4).

Performance: I) Sublinearity of Regret: To verify the result
of Theorem 2, in Fig. 1b, we present the averaged regret over
time (i.e., individual regret divided by T ), which is clearly

decreasing over time. In Table I, we tabulate the averaged
regrets (over time and agents) as well as their standard errors
computed from 50 trials for T = 20K, 30K, 40K, 50K, 60K.
We can see that 50 trials is enough to obtain a small standard
error. II) Impact of Network Topology: To study the impact of
network topology, we use three different networks: a cyclic
graph with 2 neighbors (Net A), a cyclic graph with 6
neighbors (Net B) and a complete graph (Net C) where every
entry of P is 1
20 . From Fig. 1c, we can see that the regret
increases when β is smaller (Net A > Net B > Net C). This
result is consistent with Remark 3 on the impact of β.

V. CONCLUSION

In this paper, we considered the distributed online
LQR problem with unknown LTI systems and time-varying
quadratic cost functions. We developed a fully decentralized
algorithm to estimate the unknown system and minimize the
ﬁnite-horizon cost, which can be cast as a regret minimization.
We proved that the individual regret, which is the performance
of the control sequence of any agent compared to the best
(linear and strongly stable) controller in hindsight, is upper
bounded by O(T 2/3 log T ). Future directions include analyz-
ing the dynamic regret deﬁned w.r.t. the optimal (instanta-
neous) control policy in hindsight, investigating coupled time-
varying cost functions, and analyzing the adversarial noise
setup.

VI. APPENDIX

Let us start with an outline of the appendix as follows.
1) In Section VI-A, we discuss the connection of strong
stability for two close enough LTI systems. We further
discuss the relation between the steady-state covariance
matrices of these systems.

2) In Section VI-B, based on the results in [8] and the
EXTRA algorithm [9], we quantify the precision of the
system estimate of each agent.

3) As mentioned in the proof sketch, there is a term in
the regret decomposition, related to distributed online
optimization with different feasible sets across agents.
In Section VI-C, we provide auxiliary lemmas to bound
this term in Theorem 9.

123456Time(iteration)104012345678IndividualRegretsofAllAgents123456Time(iteration)10411.21.41.61.822.22.42.6AveragedRegretsoverTime10-4123456Time(iterations)1041.11.151.21.251.31.351.41.451.51.55Averaged Regret over Time across All Agents10-42 neighbors6 neighborsAll neighbors4) In Section VI-D, we prove our main result (Theorem 2)

Denoting ∆ := X1 − X2, we get

by putting together the above results.

A. Strong Stability of Two Similar LTI Systems

Notice that we deal with a problem with unknown system
dynamics, where each agent decides its control signal based
on its own system estimates. Therefore, we should quantify
how a strongly stable controller w.r.t. one system performs on
another similar system, and how two steady-state covariance
matrices of these two systems are related to each other.

Lemma 3. A linear policy K which is (κ, γ)-strongly stable
(κ ≥ 1) for the LTI system (A1, B1), is also (κ, γ − 2κ2(cid:15))-
strongly stable for the LTI system (A2, B2) if (cid:107)[A1 B1] −
[A2 B2](cid:107)F ≤ (cid:15) and (cid:15) < γ

2κ2 .

Proof. Based on the deﬁnition of strong stability, we have
A1 + B1K = HLH−1 with (cid:107)H(cid:107)(cid:107)H−1(cid:107) ≤ κ and (cid:107)L(cid:107) ≤
1 − γ. Therefore, we have

A2 + B2K

=A1 + B1K + (A2 − A1) + (B2 − B1)K
=HLH−1 + HH−1[(A2 − A1) + (B2 − B1)K]HH−1
=H(cid:2)L + H−1[(A2 − A1) + (B2 − B1)K]H(cid:3)H−1.

For the middle term, it can be shown that

(cid:107)L + H−1[(A2 − A1) + (B2 − B1)K]H(cid:107)

≤(cid:107)L(cid:107) + (cid:107)H−1(cid:107)(cid:107)(A2 − A1) + (B2 − B1)K(cid:107)(cid:107)H(cid:107)
≤(1 − γ) + κ((cid:107)A2 − A1(cid:107) + (cid:107)(B2 − B1)K(cid:107))
≤(1 − γ) + κ((cid:15) + (cid:15)κ)
≤(1 − γ) + 2κ2(cid:15).

Since γ − 2κ2(cid:15) > 0, based on the result above, we can see
that the policy K is (κ, γ −2κ2(cid:15))-strongly stable w.r.t. the LTI
system (A2, B2) as A2 + B2K = HL2H−1 where L2 :=
L + H−1[(A2 − A1) + (B2 − B1)K]H and (cid:107)L2(cid:107) ≤ 1 − (γ −
2κ2(cid:15)).

Lemma 4. Given a linear policy K which is (κ, γ)-strongly
stable (κ ≥ 1) for the LTI system (A1, B1), if (cid:107)[A1 B1] −
[A2 B2](cid:107)F ≤ (cid:15) and (cid:15) < γ

2κ2 , we have that

X1 (cid:23) X2 − 4κ6Tr(W)

(cid:15)[1 − (γ − 2κ2(cid:15))]
(γ − 2κ2(cid:15))(1 − (1 − γ)2)

· I,

where X1(respectively, X2) is the steady-state covariance
matrix obtained by applying the controller ut = Kxt on the
system xt+1 = A1xt + B1ut + wt (respectively, xt+1 =
A2xt + B2ut + wt).

Proof. Based on Lemma 3, we know that
the policy K
is (κ, γ) and (κ, γ − 2κ2(cid:15))-strongly stable for the systems
(A1, B1) and (A2, B2), respectively. We then have

X2 + ∆

=(A1 + B1K)(X2 + ∆)(A1 + B1K)(cid:62) + W
=(A2 + B2K)X2(A2 + B2K)(cid:62)
+[(A1 + B1K) − (A2 + B2K)]X2(A2 + B2K)(cid:62)
+(A2 + B2K)X2[(A1 + B1K) − (A2 + B2K)](cid:62)
+[A1 − A2 + (B1 − B2)K]X2[A1 − A2 + (B1 − B2)K](cid:62)
+(A1 + B1K)∆(A1 + B1K)(cid:62) + W.

From the equation above, we have

∆

=[(A1 + B1K) − (A2 + B2K)]X2(A2 + B2K)(cid:62)
+(A2 + B2K)X2[(A1 + B1K) − (A2 + B2K)](cid:62)
+[A1 − A2 + (B1 − B2)K]X2[A1 − A2 + (B1 − B2)K](cid:62)
+(A1 + B1K)∆(A1 + B1K)(cid:62)
(cid:23)[(A1 + B1K) − (A2 + B2K)]X2(A2 + B2K)(cid:62)
+(A2 + B2K)X2[(A1 + B1K) − (A2 + B2K)](cid:62)
+(A1 + B1K)∆(A1 + B1K)(cid:62).

Denoting

Ψ := [(A1 + B1K) − (A2 + B2K)]X2(A2 + B2K)(cid:62)
+(A2 + B2K)X2[(A1 + B1K) − (A2 + B2K)](cid:62)

and applying the inequality above recursively, we get

∆ (cid:23) Ψ + (A1 + B1K)Ψ(A1 + B1K)(cid:62) + . . .

+ (A1 + B1K)nΨ((A1 + B1K)(cid:62))n
+ (A1 + B1K)n+1∆((A1 + B1K)(cid:62))n+1.

As n → ∞, the spectral norm of the right hand side is upper
bounded by κ2(cid:107)Ψ(cid:107)/[1 − (1 − γ)2], which implies

∆ (cid:23)

−κ2(cid:107)Ψ(cid:107)
1 − (1 − γ)2 I.

(5)

We also have

(cid:107)Ψ(cid:107) ≤2(cid:107)(A1 + B1K) − (A2 + B2K)(cid:107)(cid:107)X2(cid:107)(cid:107)A2 + B2K(cid:107)

≤2(2κ(cid:15))[κ(1 − (γ − 2κ2(cid:15)))](cid:107)X2(cid:107)

≤4κ2(cid:15)(1 − (γ − 2κ2(cid:15)))

κ2
γ − 2κ2(cid:15)

Tr(W),

where the second inequality is due to the strong stability of K
w.r.t. (A2, B2) and the third inequality is derived by applying
Lemma 3.3 in [4]. Substituting the above upper bound into
(5), the result is proved.

Corollary 5. Given a linear policy K which is (κ, γ)-strongly
stable (κ ≥ 1) for the LTI system (A1, B1), if (cid:107)[A1 B1] −
[A2 B2](cid:107)F ≤ (cid:15) and (cid:15) ≤ γ

4κ2 , we have the following result:

X1 = (A1 + B1K)X1(A1 + B1K)(cid:62) + W,
X2 = (A2 + B2K)X2(A2 + B2K)(cid:62) + W.

X1 (cid:23) X2 − ξ(cid:15) · I,

where ξ := Tr(W) 4κ6
γ2 .

B. Precision of the System Estimates

Since the ﬁnal regret bound depends on the precision of all
agents system estimates, in this section we show that with
a speciﬁc choice of T0 (iterations for collecting data), T1
(iterations for performing EXTRA) and T , the precision of
each agent estimate and the distance between any two agents
estimates are upper bounded by O(T −1/3).

Lemma 6. Assume (cid:107)[A B](cid:107)F ≤ ϑ and apply Algorithm 1
with T0 = T 2/3 log(T /δ) and T1 = O(log(T 1/3)) and T0 ≥
max{200(log(12n) + log( 3m
δ )), 4(cid:37) + 6m + 3d}, where (cid:37) :=
m144ϑ2κ4
0). Then, for t, t(cid:48) ∈ {Ts, . . . , T } and i, j ∈
(1 + ϑ2κ2
0
γ2
0

[m], with probability (1 − δ), we have

(cid:107)[ (cid:98)Ai,t (cid:98)Bi,t] − [A B](cid:107)F ≤ (1 +

√
38
√

2n
m

)T −1/3,

where φ is a constant. Based on (9), (10) and our choice of
T1 (T1 = (− log τ )−1 log(φT 1/3)), for t ∈ [Ts, . . . , T ] and
i, j ∈ [m], we have

(cid:107)[ (cid:98)Ai,t (cid:98)Bi,t] − [A B](cid:107)F ≤ (cid:107)[ (cid:98)Ai,t (cid:98)Bi,t] − [ (cid:98)A (cid:98)B](cid:107)F + (cid:107) (cid:98)∆(cid:107)F
√
38
√

)T −1/3,

≤(1 +

2n
m

(11)

and

(cid:107)[ (cid:98)Ai,t (cid:98)Bi,t] − [ (cid:98)Aj,t(cid:48) (cid:98)Bj,t(cid:48)](cid:107)F

≤(cid:107)[ (cid:98)Ai,t (cid:98)Bi,t] − [ (cid:98)A (cid:98)B](cid:107)F + (cid:107)[ (cid:98)Aj,t(cid:48) (cid:98)Bj,t(cid:48)] − [ (cid:98)A (cid:98)B](cid:107)F
≤2T −1/3.

(12)

and

(cid:107)[ (cid:98)Ai,t (cid:98)Bi,t] − [ (cid:98)Aj,t(cid:48) (cid:98)Bj,t(cid:48)](cid:107)F ≤ 2T −1/3.

C. Bound of the Distributed Online SDP

Proof. Let V0 := (cid:80)m
i=1
in [8], we have the following relationships:
• With probability at least (1 − δ/3),

t=1 zi,tz(cid:62)

(cid:80)T0

i,t. Based on Theorem 20

Tr(V0) ≤ (mT0)

144σ2κ4
0
γ2
0

(n + kϑ2κ2

0) log(

6mT0
δ

). (6)

• With probability at least (1 − δ/3), based on our choice

of T0, we have that

V0 (cid:23)

mT0σ2
80

I.

(7)

• By solving the following LS problem

min
[A,B]

m
(cid:88)

T0(cid:88)

i=1

t=1

(cid:107)[A B]zi,t − xi,t+1(cid:107)2 + σ2ϑ−2(cid:107)[A B](cid:107)2,

the

(cid:80)T0

=
estimates
where
((cid:80)m
i,t)V −1 and V = V0 + σ2ϑ−2I, we
i=1
have with probability at least (1 − δ/3) (based on our
choice of T0)

t=1 xi,t+1z(cid:62)

[ (cid:98)A

(cid:98)B]

Tr( (cid:98)∆V (cid:98)∆(cid:62)) ≤ 18σ2n2 log(T0/δ),

(8)

where (cid:98)∆ = [ (cid:98)A (cid:98)B] − [A B].

Combining (7) and (8), we get

(cid:107) (cid:98)∆(cid:107)F ≤

(cid:115)

38n
√
m

log(T0/δ)
T0

.

Since T0 = T 2/3 log(T /δ), we have

(cid:107) (cid:98)∆(cid:107)F ≤

T −1/3.

(9)

√
38
√

2n
m

After the ﬁrst (T0 + 1) iterations, we apply the EXTRA
algorithm [9], and based on Theorem 3.7 in [9], we have
the upper bound of the distance between ( (cid:98)Ai,t, (cid:98)Bi,t) (agent i
estimation at iteration t) and ( (cid:98)A, (cid:98)B), the solution of the global
function f (A, B) = (cid:80)m
i=1 fi(A, B) as follows. There exists
0 < τ < 1 such that

(cid:107)[ (cid:98)Ai,t (cid:98)Bi,t] − [ (cid:98)A (cid:98)B](cid:107)F ≤ φτ t−(T0+1), ∀i,

(10)

From the line 25 of Algorithm 1, we can see that the
feasible set of SDP for each agent is based on the agent system
estimate, so we cannot directly apply distributed online opti-
mization results on a common feasible set. Here, we provide
auxiliary results to bound the error due to distributed online
optimization. In Lemma 8, we use alternating projections
to prove that a point
is
close enough to its projection to the feasible set of another
agent, when estimates of these two agents are close. Then,
in Theorem 9, we show the contribution of distributed online
optimization to the regret.

in the feasible set of one agent

Lemma 7. Consider an afﬁne set H = {x|Ex = w}, where
E ∈ Rm×n is full row rank and n > m. For two points
p1, p2, we have the following relationships.
(i) (cid:107)ΠH(p1) − ΠH(p2)(cid:107) < (cid:107)p1 − p2(cid:107) if (p1 − p2) is not in
the null space of E.
(ii) (cid:107)p1 − ΠH(p1)(cid:107) = (cid:107)p2 − ΠH(p2)(cid:107) if (p1 − p2) is in the
null space of E.

Proof. The orthogonal projection to afﬁne set has the follow-
ing closed-form [52]

ΠH(p1) = [I − E(cid:62)(EE(cid:62))−1E]p1 + E(cid:62)(EE(cid:62))−1w.

(13)

The proof then follows immediately.

Lemma 8. Suppose two system estimates ( (cid:98)A1, (cid:98)B1) and
( (cid:98)A2, (cid:98)B2) such that (cid:107)[ (cid:98)A1 (cid:98)B1] − [ (cid:98)A2 (cid:98)B2](cid:107)F ≤ (cid:15)1, and
(cid:107)[ (cid:98)A1 (cid:98)B1] − [A B](cid:107)F , (cid:107)[ (cid:98)A2 (cid:98)B2] − [A B](cid:107)F ≤ (cid:15)2. Let us
denote D := {Σ|Σ (cid:23) 0, Tr(Σ) ≤ ν} and represent

H1 := {Σxx = (cid:98)C1Σ (cid:98)C(cid:62)
H2 := {Σxx = (cid:98)C2Σ (cid:98)C(cid:62)

1 + W}
2 + W}

(cid:98)C1 := [ (cid:98)A1 (cid:98)B1]
(cid:98)C2 := [ (cid:98)A2 (cid:98)B2].

Let us also deﬁne S1 := D ∩ H1 and S2 := D ∩ H2,
respectively. Then, for any point Σ1 ∈ S1, we have that
(cid:107)Σ1 − ΠS2(Σ1)(cid:107) is O((cid:15)1).

Proof. We can see that H1 can we written as

(cid:2)I 0(cid:3) Σ

(cid:21)

(cid:20)I
0

= (cid:98)C1Σ (cid:98)C(cid:62)

1 + W.

Denoting D := [I 0], we have the vectorized version of the
linear system above as

(D ⊗ D − (cid:98)C1 ⊗ (cid:98)C1)vec(Σ) = vec(W),

and we let E1 := D ⊗ D − (cid:98)C1 ⊗ (cid:98)C1. Similarly, we can write
H2 as (E2)vec(Σ) = vec(W). For the rest of the proof, we
consider Σ1 as vec(Σ1). Supposing both E1 and E2 are full
row rank and applying (13), for any point Σ1 ∈ S1 we have

1(cid:107) (cid:101), where ρ denotes the linear
Now, deﬁne k0 := (cid:100)logρ
convergence rate of alternating projections between two closed
convex sets [53]. In view of (20), we have

θ(cid:15)1
(cid:107)Σ1−Σ(cid:48)

(cid:107)ΠH2 (Σ1) − ΠH2 (yk)(cid:107) ≤ ϕ(cid:107)Σ1 − yk(cid:107), k ≤ k0,

(21)

where ϕ := max{ϕ1, ϕ2, . . . , ϕk0} < 1. Also, the linear
convergence rate along with our choice of k0 guarantees

Σ1 − ΠH2(Σ1) = E(cid:62)

2 (E2E(cid:62)

2 )−1(E2 − E1)Σ1.

(14)

(cid:107)ak0 − Σ(cid:48)

1(cid:107) = (cid:107)ΠH2 (yk0) − Σ(cid:48)

1(cid:107) ≤ ρk0(cid:107)Σ1 − Σ(cid:48)

1(cid:107) ≤ θ(cid:15)1.

We know that Σ1 ∈ S1 and [ (cid:98)A2 (cid:98)B2] has a ﬁnite norm, so
there exists a constant upper-bounding (cid:107)E(cid:62)
2 )−1(cid:107)(cid:107)Σ1(cid:107).
To show (cid:107)Σ1 − ΠH2(Σ1)(cid:107) is O((cid:15)1), it is sufﬁcient to show
(cid:107)E1 − E2(cid:107) is O((cid:15)1). Based on the expressions of E1 and E2,
we have that

2 (E2E(cid:62)

(cid:107)E2 − E1(cid:107)

=(cid:107) (cid:98)C2 ⊗ (cid:98)C2 − (cid:98)C1 ⊗ (cid:98)C1(cid:107)
≤(cid:107) (cid:98)C2 ⊗ (cid:98)C2 − (cid:98)C2 ⊗ (cid:98)C1 + (cid:98)C2 ⊗ (cid:98)C1 − (cid:98)C1 ⊗ (cid:98)C1(cid:107)F
≤((cid:107) (cid:98)C1(cid:107)F + (cid:107) (cid:98)C2(cid:107)F )(cid:107) (cid:98)C2 − (cid:98)C1(cid:107)F ,

(15)

which shows that (cid:107)E2 − E1(cid:107) is O((cid:15)1) due to the assumption
that (cid:107) (cid:98)C2 − (cid:98)C1(cid:107)F = (cid:15)1. Therefore, we conclude that there
exists a constant θ such that (cid:107)Σ1 − ΠH2 (Σ1)(cid:107) ≤ θ(cid:15)1.

S1 and S2 are both non-empty and neither is a singleton. For
any point Σ1 ∈ S1, if ΠH2 (Σ1) ∈ D, by (14) and Pythagorean
theorem, we have

(cid:107)Σ1 − ΠS2(Σ1)(cid:107) ≤ (cid:107)Σ1 − ΠH2(Σ1)(cid:107) ≤ θ(cid:15)1,

(16)

and the claim of lemma holds immediately. If ΠH2(Σ1) /∈ D,
we consider the process of applying alternating projections for
Σ1 on S2. Let y0 = Σ1 and consider the following iterates

ΠD(ak−1) = yk ΠH2(yk) = ak, k = 1, 2, 3, . . . ,

(17)

where we denote the limit point by Σ(cid:48)
sequences, based on the deﬁnition of projection we have
(cid:107)yk+1 − ak(cid:107)2 = (cid:107)yk+1 − ak+1(cid:107)2 + (cid:107)ak+1 − ak(cid:107)2,

1. For the above

which implies

(cid:107)yk+1 − ak+1(cid:107)2 = (cid:107)yk+1 − ak(cid:107)2 − (cid:107)ak+1 − ak(cid:107)2

≤ (cid:107)yk − ak(cid:107)2 − (cid:107)ak+1 − ak(cid:107)2.

(18)

Without loss of generality, we assume (cid:107)ak+1 −ak(cid:107) > 0 for all
k. If (cid:107)ak+1 −ak(cid:107) = 0 at k = ˜k, the sequence has converged in
1 for k ≥ ˜k), and
a ﬁnite number of steps (i.e., ak = yk = Σ(cid:48)
the following proof still holds. Assuming (cid:107)ak+1 − ak(cid:107) > 0,
we can see from (18) that

(cid:107)Σ1 − a0(cid:107) = (cid:107)y0 − a0(cid:107) > (cid:107)yk − ak(cid:107), k = 1, 2, . . .

(19)

If for any k > 0, (Σ1 − yk) is in the null space of E2, by
Lemma 7 we have

(cid:107)Σ1 − a0(cid:107) = (cid:107)Σ1 − ΠH2(Σ1)(cid:107) = (cid:107)yk − ΠH2(yk)(cid:107),

which contradicts (19). Therefore, we conclude that (Σ1 −yk)
is not in the null space of E2. Then, by Lemma 7, there exists
a constant 0 ≤ ϕk < 1 such that

(cid:107)ΠH2 (Σ1) − ΠH2(yk)(cid:107) ≤ ϕk(cid:107)Σ1 − yk(cid:107).

(20)

Recalling (17) and combining (21) with above, we get

(cid:107)Σ1 − ΠS2(Σ1)(cid:107) ≤ (cid:107)Σ1 − Σ(cid:48)

1(cid:107)
=(cid:107)Σ1 − ΠH2(Σ1) + ΠH2(Σ1) − ΠH2 (yk0) + ΠH2(yk0) − Σ(cid:48)
≤θ(cid:15)1 + ϕ(cid:107)Σ1 − yk0(cid:107) + θ(cid:15)1
=2θ(cid:15)1 + ϕ(cid:107)ΠD(Σ1) − ΠD(ak0−1)(cid:107)
≤2θ(cid:15)1 + ϕ(cid:107)Σ1 − ak0−1(cid:107)
=2θ(cid:15)1 + ϕ(cid:107)Σ1 − ΠH2(Σ1) + ΠH2(Σ1) − ΠH2 (yk0−1)(cid:107)
≤2θ(cid:15)1 + ϕθ(cid:15)1 + ϕ2(cid:107)Σ1 − yk0−1(cid:107).

1(cid:107)

Iteratively repeating above, we obtain

(cid:107)Σ1 − ΠS2(Σ1)(cid:107) ≤ 2θ(cid:15)1(1 +

k0(cid:88)

i=1

ϕi) + ϕk0+1(cid:107)Σ1 − y0(cid:107)

≤

2θ(cid:15)1
1 − ϕ

,

(22)

since (cid:107)Σ1 − y0(cid:107) = 0 based on the initialization of (17). From
(16) and (22), we conclude that there exists a constant ζ such
that (cid:107)Σ1 − ΠS2(Σ1)(cid:107) ≤ ζ(cid:15)1.

Theorem 9. Let Algorithm 1 run with step size η > 0 under
the conditions of Theorem 2. Deﬁne

S :=

(cid:110)

Σ (cid:23) 0

(cid:12)
(cid:12)Tr(Σ) ≤ ν Σxx = [A B]Σ[A B](cid:62) + W
(cid:12)

(cid:111)
.

Then, for any Σ ∈ S, the following quantity

T
(cid:88)

t=Ts

(cid:19)

(cid:18)Qt

0
0 Rt

• Σj,t −

T
(cid:88)

t=Ts

(cid:19)

(cid:18)Qt

0
0 Rt

• Σ,

is O(T 2/3 + T η + T 1/3

η ) for any j ∈ [m].
Proof. For the presentation simplicity let

f i
t (Σ) :=

(cid:18)Qi,t

(cid:19)

0

0 Ri,t

• Σ

ft(Σ) :=

m
(cid:88)

i=1

f i
t (Σ),

:= ∇f i

t . Observe that (cid:107)gi

and deﬁne gi
t(cid:107)F ≤ 2C since
t
Tr(Qi,t) ≤ C and Tr(Ri,t) ≤ C. For the rest of the proof,
with a slight abuse of notation, we use the vectorized versions
of matrices Σi,t, Σ, and gi
t using the same notation. We then
have

m
(cid:88)

˜Σi,t+1 =

[P]jiΣj,t − ηgi
t

j=1
Σi,t+1 = ΠS i

t+1

( ˜Σi,t+1).

(23)

t := Σi,t − ˜Σi,t. For t ∈ [Ts, . . . , T ] we can bound

Deﬁne ri
ri
t+1 as follows:

(cid:107)ri

t+1(cid:107) =(cid:107) ˜Σi,t+1 − ΠS i

t+1

( ˜Σi,t+1)(cid:107)

(cid:13)
˜Σi,t+1 −
(cid:13)
(cid:13)

≤

m
(cid:88)

j=1

[P]jiΠS i

t+1

(Σj,t)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

=

m
(cid:88)

j=1

[P]ji[Σj,t − ΠS i

t+1

(Σj,t)] − ηgi
t

(cid:13)
(cid:13)
(cid:13)

≤ζ2T −1/3 + η(cid:107)gi

t(cid:107) ≤ 2ζT −1/3 + 2ηC,

where the ﬁrst inequality is due to the properties of projection
to a convex set, and the second inequality can be derived by
applying Lemma 6 and Lemma 8 with (cid:15)1 = 2T −1/3. For the
sake of simplicity, we deﬁne the following matrices
Σt := [Σ1,t, . . . , Σm,t], ˜Σt := [ ˜Σ1,t, . . . , ˜Σm,t]
Gt := [g1

(25)

Rt := [r1
(cid:80)m

t , . . . , rm
t ].
i=1 Σi,t, we have the

:= 1
m

t , . . . , gm
t ],
Then, for the iterate Σt
following relationship

(ΣtP − ηGt + Rt+1)1

Σt+1 =

=

1
m
1
m

= Σt −

1
m

Σt+11 =
η
m
m
(cid:88)

Σt1 −

η
m

i=1

1
Rt+11
m
m
(cid:88)

ri
t+1.

Gt1 +

gi
t +

1
m

(26)

For any Σ ∈ S we have that

(cid:107)Σt+1 − Σ(cid:107)2 = (cid:107)Σt − Σ(cid:107)2 +

−

2η
m

m
(cid:88)

(cid:104)Σt − Σ, gi

t(cid:105) +

i=1

1
m2

(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

i=1

(ri

t+1 − ηgi
t)

(cid:13)
2
(cid:13)
(cid:13)

2
m

m
(cid:88)

i=1

(cid:104)Σt − Σ, ri

t+1(cid:105).

(27)

We now derive an upper bound of (27) for t ∈ [Ts, . . . , T ].
Based on (24) and the fact that (cid:107)gi

1
m2

(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

i=1

(ri

(cid:13)
2
t+1 − ηgi
(cid:13)
t)
(cid:13)

≤

1
m2

i=1

t(cid:107) ≤ 2C, we have that
(cid:35)2
(cid:34) m
(cid:88)

t+1(cid:107) + η(cid:107)gi

t(cid:107))

((cid:107)ri

≤(2ζT −1/3 + 4ηC)2.

(28)

Based on the deﬁnition of f i

t (Σ), we have that

m
(cid:88)

(cid:104)Σ−Σt, gi

t(cid:105) =

m
(cid:88)

t (Σ)−f i
f i

t (Σt) = ft(Σ)−ft(Σt). (29)

i=1

i=1
For the term (cid:104)Σt − Σ, ri
follows
(cid:104)Σt − Σ, ri

t+1(cid:105) = (cid:104)Σt − ˜Σi,t+1, ri

t+1(cid:105), we derive the upper bound as

t+1(cid:105) + (cid:104) ˜Σi,t+1 − Σ, ri
t+1(cid:105)

t+1(cid:105)

t+1

t+1

(Σ) − Σ, ri

t+1(cid:107) + (cid:104)ΠS i
(Σ), ri
t+1(cid:105)
t+1(cid:107) + (cid:104)ΠS i
t+1(cid:107)(cid:0)(cid:107)Σt − ˜Σi,t+1(cid:107) + (cid:107)ΠS i

≤ (cid:107)Σt − ˜Σi,t+1(cid:107)(cid:107)ri
+ (cid:104) ˜Σi,t+1 − ΠS i
≤ (cid:107)Σt − ˜Σi,t+1(cid:107)(cid:107)ri
≤ (cid:107)ri
≤ 2(cid:0)ζT −1/3 + ηC(cid:1)(cid:0)(cid:107)Σt − ˜Σi,t+1(cid:107) + ζ (cid:48)T −1/3(cid:1),

(Σ) − Σ, ri
(Σ) − Σ(cid:107)(cid:1)

t+1(cid:105)

t+1

t+1

t+1

(Σ), ri

where the second inequality is due to the fact that (cid:104) ˜Σi
t+1 −
ΠS i
t+1(cid:105) is non-positive based on the properties of a
projection operator, and the last inequality is based on (24)
√
2n√
as well as Lemma 8 with (cid:15)1 = (1 + 38
m )T −1/3 and ζ (cid:48) :=
ζ(1 + 38

√
2n√
m ).

Substituting (28), (29) and (30) into (27) and rearranging

(24)

it, we can get

(ζT −1/3 + 2ηC)2

(cid:0)(cid:107)Σt − Σ(cid:107)2 − (cid:107)Σt+1 − Σ(cid:107)2(cid:1)

m
η

+

ft(Σt) − ft(Σ) ≤
m
2η
2ζT −1/3 + 2ηC
η

+

m
(cid:88)

i=1

(cid:0)(cid:107)Σt − ˜Σi,t+1(cid:107) + ζ (cid:48)T −1/3(cid:1).

(31)

Adding and subtracting ft(Σj,t) on the left hand side and
observing that ft is Lipschitz continuous with the constant
2mC, we get for any j ∈ [m]

(ζT −1/3 + 2ηC)2

(cid:0)(cid:107)Σt − Σ(cid:107)2 − (cid:107)Σt+1 − Σ(cid:107)2(cid:1)

m
η

+

ft(Σj,t) − ft(Σ) ≤
m
2η
2ζT −1/3 + 2ηC
η

+

m
(cid:88)

i=1

(cid:0)(cid:107)Σt − ˜Σi,t+1(cid:107) + ζ (cid:48)T −1/3(cid:1)

Applying Lemma 10 on (32) and summing it over t ∈
[Ts, . . . , T ], we have

(32)

T
(cid:88)

t=Ts

ft(Σj,t) − ft(Σ) ≤

mT
η

(ζT −1/3 + 2ηC)2

+

2ζT −1/3 + 2ηC
η

+ 2mCT (2ζT −1/3 + 4ηC)

mT (cid:0)(2ζT −1/3 + 4ηC)
√

m
1 − β

+

m
2η

√

m
1 − β

+ ζ (cid:48)T −1/3(cid:1)

(cid:107)ΣTs − Σ(cid:107)2,

(33)

which is O(T 2/3 + T η + T 1/3
bound is O(T 2/3).

η ). If η is O(T −1/3), the above

Lemma 10. Let Algorithm 1 run with step size η > 0 and
deﬁne Σt := 1
i=1 Σi,t. Under the conditions of Theorem
m
2, we have that

(cid:80)m

(cid:107)Σt − Σi,t(cid:107) ≤ (2ζT −1/3 + 4ηC)

√

m
1 − β

,

for t ∈ [Ts + 1, . . . , T ], and

(cid:107)Σt − ˜Σi,t+1(cid:107) ≤ (2ζT −1/3 + 4ηC)

√

m
1 − β

,

i=1

+ 2mC(cid:107)Σj,t − Σt(cid:107)

(30)

for t ∈ [Ts, . . . , T ].

Proof. Using the notations deﬁned in (25), we have the
following relation Σt = Σt−1P − ηGt−1 + Rt. Unwinding
the equation, we get

Σt = ΣTsPt−Ts − η

t−Ts(cid:88)

Gt−kPk−1 +

t−Ts(cid:88)

Rt−k+1Pk−1.

k=1

k=1

(34)
Since P is doubly stochastic, we have Pk1 = 1 for all k ≥ 1.
Using (24), the geometric mixing bound of P, and the gradient
norm bound of 2C, we get

t = Ksxs

where us
t (the control sequence generated by the
benchmark controller Ks and the corresponding state se-
quence). Recalling Ts = T0 + T1 + 2, we write the regret
as

T
(cid:88)

t=Ts

+

Lt • (cid:98)Σj,t −

T
(cid:88)

t=Ts

T
(cid:88)

Lt • (cid:98)Σs

t =

Lt • ((cid:98)Σj,t − Σj,t)

T
(cid:88)

t=Ts

Lt • (Σj,t − Σs) +

t=Ts
T
(cid:88)

Lt • (Σs − (cid:98)Σs

t ),

t=Ts

(35)

(cid:107)Σt − Σi,t(cid:107) = (cid:107)Σt(

1
m

≤(cid:107)ΣTs − ΣTs[Pt−Ts]:,i(cid:107) + η

1 − ei)(cid:107)
t−Ts(cid:88)

k=1

(cid:107)Gt−k(

1
m

1 − [Pk−1]:,i)(cid:107)

where Σs is the steady-state covariance matrix induced by
Ks, and Σj,t is generated by Algorithm 1. Now, we show
how each term in (35) is bounded.

+

t−Ts(cid:88)

k=1

(cid:107)Rt−k+1(

1
m

1 − [Pk−1]:,i)(cid:107)

√

2C

mβk−1 +

t−Ts(cid:88)

(2ζT −1/3 + 2ηC)

√

mβk−1

≤η

t−Ts(cid:88)

k=1

≤(2ζT −1/3 + 4ηC)

√

k=1
m
1 − β

,

where (cid:107)ΣTs −ΣTs [Pt−Ts]:,i(cid:107) = 0 by the initialization. By the
same token,

(cid:107)Σt − ˜Σi,t+1(cid:107) = (cid:107)

1
m

Σt1 − (ΣtP − ηGt)ei(cid:107)

=(cid:107)Σt(

1
m

1 − Pei) + ηGtei(cid:107)

≤(cid:107)ΣTs − ΣTs[Pt−Ts+1]:,i(cid:107) + η

t−Ts(cid:88)

k=1

(cid:107)Gt−k(

1
m

1 − [Pk]:,i)(cid:107)

+

t−Ts(cid:88)

k=1

(cid:107)Rt−k+1(

1
m

1 − [Pk]:,i)(cid:107) + (cid:107)ηgi
t(cid:107)

≤2ηC + η

√

2C

mβk +

t−Ts(cid:88)

k=1

t−Ts(cid:88)

k=1

(2ζT −1/3 + 2ηC)

√

mβk

≤

t−Ts(cid:88)

k=0

(2ζT −1/3 + 4ηC)

√

mβk

≤(2ζT −1/3 + 4ηC)

√

m
1 − β

.

D. Bound of the Individual Regret
Proof of Theorem 2: Based on Algorithm 1, the ﬁrst (T0 +
T1 +1) iterations are used to collect data and obtain the system
estimates. The regret of this part is at most O(T0 + T1 + 1),
where T0 and T1 are speciﬁed in Lemma 6. Let us now denote

Li,t :=

(cid:18)Qi,t

(cid:19)

0

0 Ri,t

and

Lt :=

(cid:18)Qt

0
0 Rt

(cid:19)

,

where Lt = (cid:80)m

i=1 Li,t. Also let,
j,t u(cid:62)
t us(cid:62)
t

(cid:98)Σj,t := E (cid:2)[x(cid:62)
t := E (cid:2)[xs(cid:62)
(cid:98)Σs

j,t](cid:62)[x(cid:62)
](cid:62)[xs(cid:62)

j,t](cid:3)
j,t u(cid:62)
t us(cid:62)
t

](cid:3) ,

Lt • Σj,t − (cid:80)T

Lt • Σs:
(κ, γ)-strongly stable w.r.t.

(I) For the term (cid:80)T
t=Ts
We know Ks
is
and based on Lemma 3.3 in [4],
Tr(Σs) = Tr(Σs
that Σs is feasible to S. From Theorem 9, we have

(A, B),
it can be shown that
uu) ≤ 2κ4λ2/γ = ν, which ensures

xx) + Tr(Σs

t=Ts

T
(cid:88)

Lt • Σj,t −

T
(cid:88)

Lt • Σs ≤

mT
η

(ζT −1/3 + 2ηC)2

t=Ts
2ζT −1/3 + 2ηC
η

+

t=Ts
mT (cid:0)(2ζT −1/3 + 4ηC)
√

√

m
1 − β

+ ζ (cid:48)T −1/3(cid:1)

+2mCT (2ζT −1/3 + 4ηC)

m
1 − β

+

m
2η

(cid:107)ΣTs − Σ(cid:107)2.

(36)

(II) For the term (cid:80)T
Based on Lemma 6, we have for t ∈ [Ts, . . . , T ],

Lt • ((cid:98)Σj,t − Σj,t):

t=Ts

(cid:107)[ (cid:98)Aj,t (cid:98)Bj,t] − [A B](cid:107)F ≤ (cid:15) := (1 +

√
38
√

2n
m

)T −1/3,

√

ν

with probability at least 1 − δ. Let ¯κ :=
2¯κ2 .
Based on Lemma 4.3 in [4], it can be shown that Kj,t is (¯κ, ¯γ)-
strongly stable w.r.t. ( (cid:98)Aj,t, (cid:98)Bj,t), and it is (¯κ, ¯γ
2 )-strongly
stable w.r.t. (A, B) based on Lemma 3 and our choice of
T such that (cid:15) ≤ ¯γ

4¯κ2 . Based on Corollary 5, we get

σ and ¯γ := 1

X1

j,t (cid:23) X2

j,t − ξ(cid:15) · I,

j,t and X2

where X1
j,t are the steady-state covariance matri-
ces of applying Kj,t on the linear systems ( (cid:98)Aj,t, (cid:98)Bj,t) and
(A, B), respectively. ξ is Tr(W) 4¯κ6
¯γ2 . From Algorithm 1, we
have

(cid:18)(Σj,t)xx
(Σj,t)ux
(cid:18) (Σj,t)xx

Σj,t =

=

(cid:19)

(Σj,t)xu
(Σj,t)uu

(Σj,t)xxK(cid:62)
j,t

Kj,t(Σj,t)xx Kj,t(Σj,t)xxK(cid:62)
j,t

(cid:19)

+

(cid:18)0

(cid:19)

0

0 Vj,t

and

(cid:32)

(cid:98)Σj,t =

((cid:98)Σj,t)xx

((cid:98)Σj,t)xxK(cid:62)
j,t

Kj,t((cid:98)Σj,t)xx Kj,t((cid:98)Σj,t)xxK(cid:62)
j,t

(cid:33)

+

(cid:18)0

0

0 Vj,t

(cid:19)

.

Based on above we have

Li,t • ((cid:98)Σj,t − Σj,t)

(cid:1)

j,tRi,tKj,t) • (cid:0)((cid:98)Σj,t)xx − (Σj,t)xx
j,tRi,tKj,t) • (cid:0)((cid:98)Σj,t)xx − X1
j,tRi,tKj,t) • (cid:0)((cid:98)Σj,t)xx − X2
j,tRi,tKj,t)(cid:0)(cid:107)((cid:98)Σj,t)xx − X2

=(Qi,t + K(cid:62)
≤(Qi,t + K(cid:62)
≤(Qi,t + K(cid:62)
≤Tr(Qi,t + K(cid:62)
≤C(1 + ¯κ2)(cid:0)(cid:107)((cid:98)Σj,t)xx − X2

j,t(cid:107) + ξ(cid:15)(cid:1),

j,t

(cid:1)

j,t + ξ(cid:15) · I(cid:1)
j,t(cid:107) + ξ(cid:15)(cid:1)

(cid:98)Σs

t =

(37)

=

≤

i=1
m
(cid:88)

i=1

where the ﬁrst inequality can be derived based on the proof
of Theorem 4.2 in [4], and the last inequality comes from the
fact that Tr(Qi,t), Tr(Ri,t) ≤ C and (cid:107)Kj,t(cid:107) ≤ ¯κ.

Based on Lemma 10 and (24), we can derive

(cid:107)Σj,t+1 − Σj,t(cid:107) ≤

√
3
m
1 − β

(2ζT −1/3 + 4ηC).

Choose η and T to ensure (cid:107)Σj,t+1 − Σj,t(cid:107) ≤ ¯γσ2
2 ; it can then
be shown that {Kj,t}t≥(Ts) are (¯κ, ¯γ/2)-sequentially strongly
stable w.r.t. (A, B) based on the similar derivation of Lemma
4.4 in [4]. Then, we have

(cid:107)((cid:98)Σj,t)xx − X2

j,t(cid:107) ≤¯κ2e− ¯γ
2 (t−Ts)(cid:107)((cid:98)Σj,Ts )xx − X2
√
m
3
1 − β

(2ζT −1/3 + 4ηC).

4¯κ2
¯γ

+

j,Ts

(cid:107)

(38)

(cid:32)

t )xx

((cid:98)Σs
Ks((cid:98)Σs

((cid:98)Σs

t )xxKs(cid:62)

t )xx Ks((cid:98)Σs

t )xxKs(cid:62)

(cid:33)

, we have

Lt • (Σs − (cid:98)Σs
t )
m
(cid:88)

(Qi,t + KsRi,tKs(cid:62)) •

(cid:16)

Σs

xx − ((cid:98)Σs

t )xx

(cid:17)

(40)

Tr(Qi,t + KsRi,tKs(cid:62))(cid:107)Σs

xx − ((cid:98)Σs

t )xx(cid:107)

≤mC(1 + κ2)(cid:107)Σs

xx − ((cid:98)Σs
where the second inequality comes from the fact
that
Tr(Qi,t), Tr(Ri,t) ≤ C and Ks is (κ, γ)-strongly stable.
Based on Lemma 3.2 in [4], we get

t )xx(cid:107),

(cid:107)((cid:98)Σs

t )xx − Σs

xx(cid:107) ≤ κ2e−2γ(t−Ts)(cid:107)((cid:98)Σs
Ts

)xx − Σs

xx(cid:107).

(41)

Substituting (41) into (40) and summing over t ∈ [Ts, . . . , T ],
we have
T
(cid:88)

Lt • (Σs − (cid:98)Σs
t )

t=Ts

≤mC(1 + κ2)κ2(cid:107)((cid:98)Σs
Ts

)xx − Σs

xx(cid:107)

T
(cid:88)

t=Ts

e−2γ(t−Ts)

(42)

≤

mC(κ2 + κ4)(1 + 2γ)
2γ

(cid:107)((cid:98)Σs
Ts

)xx − Σs

xx(cid:107).

Substituting (36), (39) and (42) into (35), we get

T
(cid:88)

Lt • (cid:98)Σj,t −

T
(cid:88)

Lt • (cid:98)Σs

t ≤

mT
η

(ζT −1/3 + 2ηC)2

t=Ts
mT (cid:0)(2ζT −1/3 + 4ηC)
√

√

m
1 − β

+ ζ (cid:48)T −1/3(cid:1)

m
1 − β

+

m
2η

(cid:107)ΣTs − Σ(cid:107)2

+2mCT (2ζT −1/3 + 4ηC)

Substituting (38) into (37) and summing over t ∈ [Ts, . . . , T ],
we can get

t=Ts
2ζT −1/3 + 2ηC
η

+

T
(cid:88)

t=Ts

Li,t • ((cid:98)Σj,t − Σj,t)

≤C(1 + ¯κ2)¯κ2(cid:107)((cid:98)Σj,Ts)xx − X2

j,Ts

(cid:107)

+

+

T
(cid:88)

t=Ts
T
(cid:88)

t=Ts

C(1 + ¯κ2)(ξ(cid:15))

C(1 + ¯κ2)

√
3
m
1 − β

4¯κ2
¯γ

T
(cid:88)

t=Ts

e− ¯γ

2 (t−Ts)

+mC(1 + ¯κ2)(¯κ2 +

+mC(1 + ¯κ2)ξ(1 +

)(cid:107)((cid:98)Σj,Ts)xx − X2

(cid:107)

j,Ts

2¯κ2
¯γ
√
38
√

√

2n
m

)T 2/3

(2ζT −1/3 + 4ηC)

(39)

+

+mC(1 + ¯κ2)

4¯κ2
¯γ

m
3
1 − β
mC(κ2 + κ4)(1 + 2γ)
2γ

(2ζT −1/3 + 4ηC)T

(cid:107)((cid:98)Σs
Ts

)xx − Σs

xx(cid:107).

(43)

≤C(1 + ¯κ2)(¯κ2 +

2¯κ2
¯γ

)(cid:107)((cid:98)Σj,Ts )xx − X2

j,Ts

(cid:107)

+C(1 + ¯κ2)(ξ(cid:15))T
√
3
m
1 − β

+C(1 + ¯κ2)

4¯κ2
¯γ

(2ζT −1/3 + 4ηC)T,

where the second inequality comes from the fact
(cid:80)T

that
0 e−αtdt = 1/α for α > 0. Summing (39)

t=1 e−αt ≤ (cid:82) ∞

over i, the result is obtained.

(III) For the term (cid:80)T

t=Ts

By

denoting Σs

=

t ):

Lt • (Σs − (cid:98)Σs
(cid:18) Σs
xx
KsΣs

Σs

xxKs(cid:62)

xx KsΣs

xxKs(cid:62)

(cid:19)

and

By setting η = T −1/3, we can observe (43) is O(T 2/3).
Together with the linear regret in the ﬁrst (T0 + T1 + 1)
iterations, which is O(T 2/3 log T ), we conclude that the total
regret is O(T 2/3 log T ). Note that T is chosen such that the
conditions of Lemma 6 are satisﬁed; (1+ 38
4¯κ2 ;
√
1−β (2ζ + 4C)T −1/3 ≤ ¯γσ2
3
2 .

√
m )T −1/3 ≤ ¯γ
2n√

m

(cid:26) (cid:34)

T ≥ max

(1 +

√
38
√

2n
m

)

32κ8λ4
γ2σ4

(cid:35)3

√

(cid:20) 3
m
1 − β

,

(2ζ + 4C)

(cid:21)3

,

8κ4λ2
σ4γ

(cid:2)200(log(12n) + log(

3m
δ

))(cid:3)3/2

, (cid:2)4(cid:37) + 6m + 3d(cid:3)3/2(cid:27)

.

(44)

REFERENCES

[1] B. D. O. Anderson, J. B. Moore, and B. P. Molinari, “Linear optimal
control,” IEEE Transactions on Systems, Man, and Cybernetics, vol.
SMC-2, no. 4, pp. 559–559, 1972.

[2] D. P. Bertsekas, Dynamic programming and optimal control, vol. 1,

no. 2.

[3] K. Zhou, J. C. Doyle, K. Glover et al., Robust and optimal control.

Prentice hall New Jersey, 1996, vol. 40.

[4] A. Cohen, A. Hasidim, T. Koren, N. Lazic, Y. Mansour, and K. Tal-
war, “Online linear quadratic control,” in International Conference on
Machine Learning (ICML), 2018, pp. 1029–1038.

[5] Y. Abbasi-Yadkori and C. Szepesv´ari, “Regret bounds for the adaptive
control of linear quadratic systems,” in Annual Conference on Learning
Theory (COLT).
JMLR Workshop and Conference Proceedings, 2011,
pp. 1–26.

[6] M. Ibrahimi, A. Javanmard, and B. V. Roy, “Efﬁcient reinforcement
learning for high dimensional linear quadratic systems,” in Advances in
Neural Information Processing Systems (NeurIPS, 2012, pp. 2636–2644.
[7] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu, “Regret bounds for
robust adaptive control of the linear quadratic regulator,” in International
Conference on Neural Information Processing Systems (NeurIPS), 2018,
pp. 4192–4201.

[8] A. Cohen, T. Koren, and Y. Mansour, “Learning linear-quadratic regu-
T regret,” in International Conference on

lators efﬁciently with only
Machine Learning (ICML). PMLR, 2019, pp. 1300–1309.

√

[9] W. Shi, Q. Ling, G. Wu, and W. Yin, “Extra: An exact ﬁrst-order
algorithm for decentralized consensus optimization,” SIAM Journal on
Optimization, vol. 25, no. 2, pp. 944–966, 2015.

[10] F. Borrelli and T. Keviczky, “Distributed lqr design for identical dynam-
ically decoupled systems,” IEEE Transactions on Automatic Control,
vol. 53, no. 8, pp. 1901–1912, 2008.

[11] A. Mosebach and J. Lunze, “Synchronization of autonomous agents
by an optimal networked controller,” in European Control Conference
(ECC), 2014, pp. 208–213.
[12] Y. Cao and W. Ren, “Optimal

linear-consensus algorithms: An lqr
perspective,” IEEE Transactions on Systems, Man, and Cybernetics, Part
B (Cybernetics), vol. 40, no. 3, pp. 819–830, 2010.

[13] J. Jiao, H. L. Trentelman, and M. K. Camlibel, “A suboptimality
approach to distributed linear quadratic optimal control,” IEEE Trans-
actions on Automatic Control, vol. 65, no. 3, pp. 1218–1225, 2020.
[14] ——, “Distributed linear quadratic optimal control: Compute locally and
act globally,” IEEE Control Systems Letters, vol. 4, no. 1, pp. 67–72,
2020.

[15] S. Alemzadeh and M. Mesbahi, “Distributed q-learning for dynamically
decoupled systems,” in American Control Conference (ACC), 2019, pp.
772–777.

[16] S. Fattahi, N. Matni, and S. Sojoudi, “Efﬁcient learning of distributed
linear-quadratic control policies,” SIAM Journal on Control and Opti-
mization, vol. 58, no. 5, pp. 2927–2951, 2020.

[17] L. Furieri, Y. Zheng, and M. Kamgarpour, “Learning the globally
optimal distributed lq regulator,” in Learning for Dynamics and Control
(L4DC), 2020, pp. 287–297.

[18] L. Furieri and M. Kamgarpour, “First order methods for globally op-
timal distributed controllers beyond quadratic invariance,” in American
Control Conference (ACC), 2020, pp. 4588–4593.

[19] K. J. ˚Astr¨om and P. Eykhoff, “System identiﬁcation—a survey,” Auto-

matica, vol. 7, no. 2, pp. 123–162, 1971.

[20] L. Ljung, “System identiﬁcation,” Wiley encyclopedia of electrical and

electronics engineering, pp. 1–19, 1999.

[21] H.-F. Chen and L. Guo, Identiﬁcation and stochastic adaptive control.

Springer Science & Business Media, 2012.

[22] G. C. Goodwin, G. GC, and P. RL, “Dynamic system identiﬁcation.

experiment design and data analysis.” 1977.

[23] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu, “On the sample com-
plexity of the linear quadratic regulator,” Foundations of Computational
Mathematics, pp. 1–47, 2019.

[24] M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht, “Learning
without mixing: Towards a sharp analysis of linear system identiﬁca-
tion,” in Conference On Learning Theory (COLT). PMLR, 2018, pp.
439–473.

[25] T. Sarkar and A. Rakhlin, “Near optimal ﬁnite time identiﬁcation of
arbitrary linear dynamical systems,” in International Conference on
Machine Learning (ICML). PMLR, 2019, pp. 5610–5618.

[26] S. Oymak and N. Ozay, “Non-asymptotic identiﬁcation of lti systems
from a single trajectory,” in American control conference (ACC), 2019,
pp. 5655–5661.

[27] T. Sarkar, A. Rakhlin, and M. A. Dahleh, “Finite time lti system
identiﬁcation,” Journal of Machine Learning Research, vol. 22, pp. 1–
61, 2021.

[28] A. Tsiamis and G. J. Pappas, “Finite sample analysis of stochastic system
identiﬁcation,” in IEEE Conference on Decision and Control (CDC),
2019, pp. 3648–3654.

[29] M. Simchowitz, R. Boczar, and B. Recht, “Learning linear dynamical
systems with semi-parametric least squares,” in Conference on Learning
Theory (COLT). PMLR, 2019, pp. 2714–2802.

[30] S. Fattahi, “Learning partially observed linear dynamical systems from
logarithmic number of samples,” in Learning for Dynamics and Control
(L4DC). PMLR, 2021, pp. 60–72.

[31] M. Fazel, R. Ge, S. Kakade, and M. Mesbahi, “Global convergence of
policy gradient methods for the linear quadratic regulator,” in Interna-
tional Conference on Machine Learning (ICML), 2018, pp. 1467–1476.
[32] D. Malik, A. Pananjady, K. Bhatia, K. Khamaru, P. Bartlett, and
M. Wainwright, “Derivative-free methods for policy optimization: Guar-
antees for linear quadratic systems,” in International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS). PMLR, 2019, pp. 2916–
2925.

[33] H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovi´c, “On the linear
convergence of random search for discrete-time lqr,” IEEE Control
Systems Letters, vol. 5, no. 3, pp. 989–994, 2021.

[34] H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovic, “Random
search for learning the linear quadratic regulator,” in American Control
Conference (ACC), 2020, pp. 4798–4803.

[35] A. Cassel, A. Cohen, and T. Koren, “Logarithmic regret for learning
linear quadratic regulators efﬁciently,” in International Conference on
Machine Learning (ICML). PMLR, 2020, pp. 1328–1337.

[36] M. Simchowitz and D. Foster, “Naive exploration is optimal for online
lqr,” in International Conference on Machine Learning (ICML). PMLR,
2020, pp. 8937–8948.

[37] S. Lale, K. Azizzadenesheli, B. Hassibi, and A. Anandkumar, “Explore
more and improve regret in linear quadratic regulators,” arXiv preprint
arXiv:2007.12291, 2020.

[38] E. Hazan, K. Singh, and C. Zhang, “Learning linear dynamical systems
via spectral ﬁltering,” in Advances in Neural Information Processing
Systems (NeurIPS), 2017, pp. 6702–6712.

[39] S. Arora, E. Hazan, H. Lee, K. Singh, C. Zhang, and Y. Zhang, “Towards
provable control for unknown linear dynamical systems,” 2018.
[40] N. Agarwal, B. Bullins, E. Hazan, S. M. Kakade, and K. Singh, “Online
control with adversarial disturbances,” in International Conference on
Machine Learning (ICML), 2019, pp. 154–165.

[41] N. Agarwal, E. Hazan, and K. Singh, “Logarithmic regret for online con-
trol,” in Advances in Neural Information Processing Systems (NeurIPS),
2019, pp. 10 175–10 184.

[42] M. Simchowitz, K. Singh, and E. Hazan, “Improper learning for non-
stochastic control,” in Conference on Learning Theory (COLT). PMLR,
2020, pp. 3320–3436.

[43] C. Yu, G. Shi, S.-J. Chung, Y. Yue, and A. Wierman, “The power
of predictions in online control,” Advances in Neural Information
Processing Systems (NeurIPS), vol. 33, 2020.

[44] R. Zhang, Y. Li, and N. Li, “On the regret analysis of online lqr control
with predictions,” in American Control Conference (ACC), 2021, pp.
697–703.

[45] T.-J. Chang and S. Shahrampour, “Distributed online linear quadratic
control for linear time-invariant systems,” in American Control Confer-
ence (ACC), 2021, pp. 923–928.

[46] E. Hazan, S. Kakade, and K. Singh, “The nonstochastic control prob-
lem,” in Algorithmic Learning Theory (ALT), 2020, pp. 408–421.
[47] S. Lale, K. Azizzadenesheli, B. Hassibi, and A. Anandkumar, “Loga-
rithmic regret bound in partially observable linear dynamical systems,”
Advances in Neural Information Processing Systems (NeurIPS), vol. 33,
pp. 20 876–20 888, 2020.

[48] J. S. Liu, Monte Carlo strategies in scientiﬁc computing.

Springer

Science & Business Media, 2008.

[49] G. Guo, Y. Zhao, and G. Yang, “Cooperation of multiple mobile
sensors with minimum energy cost for mobility and communication,”
Information Sciences, vol. 254, pp. 69–82, 2014.

[50] F. Yan, S. Sundaram, S. Vishwanathan, and Y. Qi, “Distributed au-
tonomous online learning: Regrets and intrinsic privacy-preserving prop-
erties,” IEEE Transactions on Knowledge and Data Engineering, vol. 25,
no. 11, pp. 2483–2493, 2012.

[51] S. Shahrampour and A. Jadbabaie, “Distributed online optimization in
dynamic environments using mirror descent,” IEEE Transactions on
Automatic Control, vol. 63, no. 3, pp. 714–725, 2018.

[52] C. D. Meyer, Matrix analysis and applied linear algebra. SIAM, 2000,

vol. 71.

[53] H. H. Bauschke and J. M. Borwein, “On the convergence of von
neumann’s alternating projection algorithm for two sets,” Set-Valued
Analysis, vol. 1, no. 2, pp. 185–212, 1993.

Ting-Jui Chang received the B.S. degree in electri-
cal and computer engineering from National Chiao
Tung University, Taiwan,
in 2016, and the M.S.
degrees in computer engineering from Texas A&M
University (TAMU), USA, in 2018. He is currently
working toward the Ph.D. degree in industrial engi-
neering at Northeastern University. His research in-
terests include distributed learning and optimization,
decentralized and online control.

Shahin Shahrampour received the Ph.D. degree
the M.A.
in Electrical and Systems Engineering,
degree in Statistics (The Wharton School), and the
M.S.E. degree in Electrical Engineering, all from the
University of Pennsylvania, in 2015, 2014, and 2012,
respectively. He is currently an Assistant Professor
in the Department of Mechanical and Industrial
Engineering at Northeastern University. His research
interests include machine learning, optimization, se-
quential decision-making, and distributed learning,
with a focus on developing computationally efﬁcient

methods for data analytics. He is a Senior Member of the IEEE.

