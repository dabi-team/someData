2
2
0
2

p
e
S
6
2

]

G
L
.
s
c
[

3
v
1
7
4
2
0
.
3
0
2
2
:
v
i
X
r
a

Graph clustering with Boltzmann machines

Pierre Miasnikof ∗1,2, Mohammad Bagherbeik1, and Ali Sheikholeslami1

1The Edward S. Rogers Sr. Dept. of Electrical & Computer Engineering,
University of Toronto, Toronto, ON, Canada
2The University of Toronto Data Sciences Institute (DSI), Toronto, ON,
Canada

Abstract

Graph clustering is the process of labelling nodes so that nodes sharing
common labels form densely connected subgraphs with sparser connections
to the remaining vertices. Because of its diﬃcult formulation, we translate
the intra-cluster density maximization problem to a distance minimization
problem, through the use of a novel vertex-vertex distance that accurately re-
ﬂects density. Speciﬁcally, we extend the recent binary quadratic K-medoids
formulation to graph clustering. We also generalize a quadratic formula-
tion originally designed for partitioning complete graphs. Because binary
quadratic optimization is an NP-hard problem, we obtain numerical solu-
tions for these formulations through the use of two novel Boltzmann machine
(meta-)heuristics. For benchmarking purposes, we compare solution quality
and computational performances to those obtained using a commercial solver,
Gurobi. We also compare clustering quality to the clusters obtained using
the popular Louvain modularity maximization method. Our initial results
clearly demonstrate the superiority of our problem formulations. They also
establish the superiority of our Boltzmann machines over a traditional solver.
In the case of smaller less complex graphs, Boltzmann machines provide the
same solutions as Gurobi, but with solution times that are orders of magni-
tude lower. In the case of larger and more complex graphs, Gurobi either
fails to return meaningful results within a reasonable time frame or returns
inferior results. Finally, we also note that both our clustering formulations,
the distance minimization and K-medoids, yield clusters of superior quality
to those obtained with the Louvain algorithm.
∗corresponding author: p.miasnikof@mail.utoronto.ca

1

 
 
 
 
 
 
1 Introduction

Graph clustering, also often called network community detection, is a pivotal task
in the analysis of networks [23], data sets where the variables are represented
as vertices on a graph with edges representing their interactions.
In fact, it has
even been described as “one of the most important and challenging problems in
network analysis”, in the recent literature [56]. Graph clustering is an unsupervised
learning task which consists of assigning each vertex of a graph to a cluster of
arbitrary size. At its core, it is a combinatorial optimization problem. A successful
clustering yields clusters that form dense induced subgraphs with sparse connections
to vertices outside their respective clusters. Unfortunately, intra-cluster density
maximization is a diﬃcult problem to formulate and solve. For this reason, we
introduce two heuristic approximations to it.

The work in this article builds upon the foundational framework of Fan and Pardalos
[17, 18], Fan et al. [20] and Bauckhage et al. [8]. The contributions of this article
are a heuristic approximation of the intra-cluster density maximization problem
through the adaptation of a binary graph partitioning formulation and the extension
of the recently introduced quadratic K-medoids formulation to the case of graphs.
We tailor these formulations through the use of a vertex-vertex distance that has been
shown to accurately reﬂect density [51, 50]. Notably, both our reformulations oﬀer
a superior alternative to the leading Louvain method. To our knowledge, this article
provides the ﬁrst extension of the Fan and Pardalos graph partitioning formulations
to the general case graph clustering problem. It also introduces the ﬁrst application
of the recent quadratic formulation of K-medoids to graph clustering.

The quadratic graph partitioning formulation of Fan and Pardalos and Fan et al. was
designed for the special cases of complete graphs or graphs where all (vertex) pairs
distances were available. The K-medoids technique is a general purpose clustering
technique that was not designed for or suited to graph data sets. It, too, requires
distances between data points. To tailor these formulations to the general graph
clustering problem, we use the Jaccard distance. Naturally, the choice of distance
is of pivotal importance. For the speciﬁc purpose of clustering, it is primordially
important to use a distance which reﬂects connectivity, not shortest path geodesics
[51, 50].

We also use a novel clustering quality assessment. Instead of using the usual and
problematic modularity function, we assess quality by examining intra- and inter-
cluster densities. This novel approach has been shown to be superior [48, 49].

Another signiﬁcant contribution is the numerical solution of both problem formu-
lations using a Boltzmann machine (meta-)heuristic. For benchmarking purposes,

2

solution times and quality are compared to those obtained with a leading commer-
cial solver, Gurobi. We also conduct a comparison of the problem formulations, by
examining the clustering quality yielded by the tremendously popular Louvain mod-
ularity maximization technique [9]. In all, we compare ﬁve diﬀerent mathematical
formulation–solver/solution technique combinations:

• Quadratic distance minimization solved using a Boltzmann machine,

• Quadratic distance minimization solved using Gurobi,

• Quadratic K-medoids formulation solved using a Boltzmann machine,

• Quadratic K-medoids formulation solved using Gurobi and

• Modularity maximization solved using the Louvain algorithm.

Numerical results highlight the superiority of our two mathematical formulations as
graph clustering models. They also showcase the well-documented weaknesses of
modularity as a clustering quality function and objective function to be maximized.
Last but not least, they also outline the superiority of a Boltzmann (meta-)heuristic
over a traditional leading-edge exact solver.

The remainder of this article is organized as follows. After a brief survey of the
literature, we provide a description of the graph clustering problem, from ﬁrst
principles. We then establish the link between our two mathematical programming
clustering techniques and these deﬁning principles.

In closing, we also wish to call the readers’ attention to the fact that this work only ex-
amines clustering of undirected (weighted or unweighted) graphs without multiple
edges. Also, in our problem formulations, vertices are assigned to non-overlapping
clusters. Cluster membership is assumed to be mutually exclusive.

2 Previous work

Graph clustering, also referred to as network community detection, is a distinct
sub-ﬁeld in unsupervised learning and clustering in particular [31]. The main
distinction lies in the fact that graphs are not typically in metric space. Graphs are
typically not represented in Euclidean space and all-pairs distances are not typically
available, either. This diﬀerence makes most traditional clustering techniques, such
as K-means for example [31], inapplicable. Additionally, it should be noted that
the very deﬁnition of graph clusters and graph clustering remains a topic of debate
(e.g., [28, 38]). Nevertheless, virtually all authors agree that clusters (communities)

3

are formed by sets of densely connected vertices that have sparser connections to
the remaining vertices (e.g., [60, 21, 57, 58, 48, 49]).

A complete review of the graph clustering literature is beyond the scope of this
article. For a very broad and thorough overview of the ﬁeld, we refer the reader
to the foundational work of Schaeﬀer [60], Fortunato [21] and Fortunato and Hric
[23]. Nevertheless, we note the existence of various competing graph clustering
techniques, built on very diﬀerent mathematical foundations. The main competing
approaches to graph clustering are

• Spectral (e.g., [45]),

• Markov (e.g., [15]) and

• Mathematical programming

– Modularity maximization (e.g., [9, 2, 52, 23])

– Other objective functions (e.g., [17, 18, 41, 20, 47, 55]).

Modularity maximization is, by far, the most popular graph clustering formulation.
The Louvain method is, by far, the most popular modularity maximization technique
[9]. Its advantages are very short computation times, scalability and the fact it does
not require the number of clusters as an input parameter. Unfortunately, modularity
also suﬀers from well-documented weaknesses (e.g., [22, 1, 28, 37, 38, 48, 49]).
In contrast, Fan and Pardalos [17, 18] and Fan et al. [20] do not rely on modularity
maximization. They exploit an all pairs distance (or similarity) between vertices
and obtain clusters by minimizing (or maximizing) it. More recently, Bauckhage et
al. [8] introduce a quadratic unconstrained binary optimization (QUBO) formulation
of the K-medoids technique [35, 31]. K-medoids is not a typical graph clustering
technique, because of its reliance on all pairs distances. We adapt both the distance
minimization and K-medoid formulations to the general graph clustering problem,
through the use of Jaccard distances [33, 42, 12, 51, 50].

Given our work includes a K-medoids formulation, it is important to highlight of
the work of Ponomarenko et al. [55]. Although their objective was to detect over-
lapping clusters in graphs, these authors adapted the original Partitioning Around
Medoids algorithm of Kaufman and Rousseeuw [35] to graphs, through the use of
commute and ampliﬁed commute distances. The work in this article uses a diﬀer-
ent K-medoids formulation, the recently introduced quadratic unconstrained binary
optimization (QUBO) formulation of Bauckhage et al. [8]. We also make use of
a diﬀerent distance metric, Jaccard distance, to adapt the K-medoids technique to
graphs.

4

Of course, it is important to also compare optimization-based approaches to other
commonly used graph clustering techniques. Here, we must point out that spectral
methods come with a heavy computational cost and do not work well on larger
instances. This scale limitation was noted by Schaeﬀer [60]. Although some
authors’ more recent algorithms are described as “faster and more accurate”, they
still carry a heavy computational cost (e.g., [34]). More importantly, spectral
methods have been described as ill-suited to sparse graphs [23]. Unfortunately,
clusterable graphs, graphs whose structure can be meaningfully summarized using
clusters, are typically sparse.

Markov-based techniques revolve around simulations of random walks over the
graph. Such simulations require numerous matrix multiplications. Additionally,
Markov clustering also requires various element-wise and row operations. An
appealing feature of Markov clustering is that it does not require the number of
clusters as a parameter input. While this feature may be advantageous in cases
where a reasonable guess for the number of clusters is not known, in most cases
domain knowledge does provide clues about this number. Since it is known that
algorithms that do not require the number of clusters as an input parameter have been
found to be less accurate than those that do require it [23], this initially appealing
feature of Markov clustering may be a weakness, in many cases.

In contrast, optimization-based approaches lend themselves very well to approxi-
mate solution techniques, which carry a lower computational cost. Indeed, because
of the typical NP-hardness of most graph clustering formulations [60, 21], solving
these and other types of combinatorial optimization problems is often successfully
done via (meta-)heuristic solution techniques (e.g., [54, 11]), which explore subsets
of the solution space. In the speciﬁc case of graph clustering, many authors have
made use of (meta-)heuristic optimization techniques (e.g., [2, 52, 36]), in order
to circumvent the NP-hard nature of the problem and ﬁnd approximate solutions.
Additionally, (meta-)heuristic optimization techniques are easily parallelizable and
well suited to implementation on high performance computing platforms.

Numerous NP-hard problems have also been reformulated as Ising (QUBO) prob-
lems [24]. Such reformulations allow the implementation on massively parallel
purpose-built hardware which yield solutions using simulated annealing [44, 27,
30, 59, 4, 46]. In fact, the graph partitioning problem is one of the original prob-
lems at the intersection of Ising modeling and the study of NP-complete problems
[44].

5

(a) Well Clustered Graph

(b) Improperly Clustered Graph

Figure 1: Examples of Good and Bad Clustering

3 The graph clustering problem

As stated previously, graph clustering is a central topic in the ﬁeld of network
science [23, 56]. Unfortunately, most graph clustering formulations lead to NP-
hard problems [60, 21], which poses obvious computational challenges.

Graph clustering consists of grouping vertices considered similar. Typically, sim-
ilarity is deﬁned by shared connections. Vertices that share more connections are
deﬁned as closer, more similar, to each other than to the ones with which they share
fewer connections. Successful clustering results in vertices grouped into densely
connected induced subgraphs (e.g., [48, 49]). Figure 1 shows an example of a
successful and an unsuccessful clustering.

Assessing output quality is another fundamental challenge of unsupervised learning.
In the case of graph clustering, this challenge is greater due to the lack of a
distance measure between data points (vertices). In accordance with the universally
accepted understanding that graphs form dense subgraphs with sparser connection
to remaining vertices, we use intra- and inter-cluster densities as benchmarks for
clustering quality [48, 47, 49].

As mentioned earlier, in this work, we only consider clustering of undirected graphs
without multiple edges. We also restrict our attention to non-overlapping clusters.
In all our models, cluster membership is mutually exclusive.

6

C1	C2	C13.1 Clustering quality, intra- and inter-cluster densities

While there is much debate surrounding the exact deﬁnition of a graph cluster (net-
work community), the consensus view is that they are formed by densely connected
sets of vertices that have sparser connections to the remaining graph. In accordance
with this consensus, we use comparisons of intra-cluster, inter-cluster and overall
densities as measures of clustering quality. Such comparisons have been shown to
oﬀer far superior assessments of clustering quality than the most popular modularity
quality functions [48, 49]. These quantities are deﬁned below.

Graph (overall) density: K =

Intra-cluster density (cluster i) : K(i)

intra =

Inter-cluster density (cluster pair i, j) : K(ij)

inter =

|E|
0.5 × N (N − 1)
|eii|
0.5 × ni(ni − 1)

|eij|
ni × nj

In these deﬁnitions above, the variables used are

• |E|: the total number of edges,

• N (= |V |): the total number of vertices,

• |eii|: the total number of edges connecting vertices in cluster i,
• |eij|: the total number of edges connecting vertices in clusters i and j and
• ni: the total number of vertices in cluster i.

intra and K(ij)

To gain a graph-wide and probabilistic view, we begin by noting that a graph’s
density, K, can be interpreted as the Bernoulli probability that two arbitrary vertices
are connected by an edge. Similarly, for each cluster i or cluster pair (i, j), the
densities K(i)
inter can be interpreted as the empirical estimate of intra-cluster
and inter-cluster edge probability. These quantities are estimates of the probaility
that two vertices in cluster i and two vertices, one in cluster i the other in cluster j,
are connected by an edge. For a graph-wide view of these probabilities that is not
sensitive to cluster size, that does not suﬀer from the well known resolution limit
(e.g., [22]), we take the sample means. These means are the empirical estimates
of the intra-/inter-cluster Bernoulli edge probabilities. We denote estimates, as
opposed to actual probabilities, as ˆP .
(In the deﬁnitions below, C is the total

7

number of clusters and ci is the cluster to which vertex i has been assigned.)

K =

|E|
0.5 × N (N − 1)

= ˆP (eij)

¯Kintra =

1
C

C
(cid:88)

i

K(i)
intra = ˆP (eij|ci = cj)

¯Kinter =

1
0.5 × C(C − 1)

C
(cid:88)

C
(cid:88)

i

j=i+1

K(ij)
inter = ˆP (eij|ci (cid:54)= cj)

Through these quantities, we can describe the properties of a good clustering and
measure clustering quality. Indeed, in a well-clustered graph, we expect clusters,
on average, to form dense induced subgraphs. We also expect cluster pairs, on
average, to form sparse bi-partite graphs (when ignoring intra-cluster edges). In
summary, in the case of a good clustering, the inequalities below must hold.

¯Kinter < K < ¯Kintra

Figure 2a illustrates our density-based quality assessment with an example of good
In this example of an arguably very well-clustered graph, the intra-
clustering.
cluster density of cluster c1 (blue vertices) is K(1)
intra = 0.83. The intra-cluster
density of cluster c2 (black vertices) is K(2)
intra = 1. The mean intra-cluster density is
¯Kintra = 1
2 (0.83+1) = 0.92. The mean inter-cluster density is ¯Kinter = 1
4×3 = 0.08
(there is only one cluster pair) and the graph’s density is K = 0.43. Consequently,
the inequality ¯Kinter = 0.08 < K = 0.43 < ¯Kintra = 0.92 holds.

3 , for a ¯Kinter = 1

In contrast, Figure 2b illustrates our quality metrics with an example of bad clus-
tering. Both clusters have an intra-cluster density of 1
3 . Inter-
cluster density (inter-cluster edges are in red) is ¯Kinter = 1
2 (only one pair of
clusters here too). Of course, graph density remains the same as in the previ-
ous case, K = 0.43. Here, the inequalities observed in the previous example
are reversed ¯Kinter = 1
In the case of the poorly
clustered graph in Figure 2c, the inequalities do not hold either. In that arguably
degenerate case, all vertices are clustered in the same cluster. As a result, mean
intra-cluster density is equal to the graph’s density. Also as a result, there are no
inter-clusters edges, so mean inter-cluster density is 0.
In summary, the neces-
sary inequalities for a good clustering do not hold, instead we have the following:
¯Kinter = 0 < K = ¯Kintra = 0.43.

2 > K = 0.43 > ¯Kintra = 1
3 .

8

(a) High intra-cluster and
low inter-cluster densities

Improperly Clustered

(b)
Graph

Improperly Clustered

(c)
Graph

Figure 2: Densities of Good and Bad Clustering

4 Mathematical formulations

Good clusters form dense induced subgraphs. Correspondingly, an optimization-
based clustering formulation should consist of assigning vertices to clusters such
that intra-cluster density is maximized. This problem formulation consists of
maximizing the objective function fo shown below.

fo =

=

(cid:88)

k

(cid:88)

k

|ekk|
0.5 × nk(nk − 1)

(cid:80)
i
0.5 × ((cid:80)

(cid:80)
j>i xikxjkwij
i xik) (((cid:80)

i xik) − 1)

.

In this formulation, we use the following variables,

• ekk: the set of edges connecting two vertices in cluster k,
• nk: the number of vertices in cluster k
• and xik ∈ {0, 1}: the (binary) decision variable which takes the value of 1 if

vertex i is assigned to cluster k (0 otherwise).

Unfortunately, formulation is fractional and can only be solved by iterative algo-
rithms [47]. Fortunately, there exist alternative mathematical programming based
graph clustering formulations. In this article, we customize, implement and test
a binary quadratic distance minimization formulation [17, 18, 20] and a quadratic
K-medoids formulation [8]. We use these two models as (heuristic) approximations
to the intra-cluster density maximization problem and validate the quality of the
clustering results by examining intra- and inter-cluster densities.

9

C1C2k2 = 1k1 = 0.83K = 0.43C14.1 Binary quadratic formulation distance minimization (QP)

We begin with the binary quadratic formulation of Fan and Pardalos [17, 18] and
Fan et al. [20]. (Note: Fan and Pardalos have also done work on cut-based graph
clustering [19]. This work is not considered in this study.) Their formulation
presented both a distance minimization and an equivalent similarity maximization
problem. In the ﬁrst case (minimization), the parameter dij is a distance separating
vertices i and j. In the second case, that parameter represents similarity. In both
cases, xik is a binary variable that takes the value of 1 if vertex i is assigned to
cluster k. A constraint ensures each vertex is assigned to exactly one cluster. The
full minimization model we use in our experiments is presented below.

(cid:80)

(cid:80)
min
i
j>i
x
s.t. (cid:80)
k xik = 1

(cid:80)

k xikxjkdij

xik ∈ {0, 1}

∀i
∀i, k

As initially presented, this model cannot be directly applied to most graphs. In-
deed, in most cases, all pairs distance or similarity matrices are not available. To
generalize this model to the case of typical graphs, we need a distance or similarity
that reﬂects connectivity between vertices. We require a distance whose pairwise
minimization will lead to densely connected clusters. Shortest path distances do
not have this property. Instead, we use the Jaccard distance [33, 42, 12] between
each vertex pair for the parameters dij. These distances can be obtained directly
from the graphs’ adjacency matrices. Full details, including a discussion on the
inverse relation between Jaccard distance and intra-cluster density, can be found
in Miasnikof et al. (2021,2022) [51, 50]. A short description is also provided in
Section 4.3.

4.2 Quadratic K-medoids formulation (K-med)

K-medoids is a clustering technique that selects K exemplars (medoids) from the
data set that will form the central point of each cluster. The remaining points
are then assigned to the nearest medoid, thus forming K clusters [35, 31]. The
K-medoids problem is NP-hard and it is typically solved using iterative algorithm
heuristics [43, 35, 31]. In late 2019, Bauckhage et al. [8] presented a mathematical
programming formulation for K-medoids. Their formulation is in the form of a
quadratic unconstrained binary optimization (QUBO) problem, in order to take
advantage of the newly available purpose built architectures for solving this type of
(NP-hard) optimization problems [44, 27, 30, 59, 4, 46].

In the formulations that follow, zi = 1 if the data point i is selected as an exemplar
and zi = 0 otherwise. The vector(cid:126)1 is a vector of ones of appropriate dimension. The

10

number of data points to be clustered is given by n. The matrix ∆ is an (n×n) matrix
containing the distances separating all pairs of points. The distance separating each
pair of points i, j is denoted dij. In mathematical form, we have

(cid:126)z = [z1z2 . . . zn]T , ∀zi ∈ {0, 1},
(cid:126)1 = [11 . . . 1]T ∈ Rn and

∆
(n×n)

= [dij] .

The original problem presented by Bauckhage et al. [8] was formulated to mini-
mize the distance between each exemplar and the remaining data points (maximize
centrality), while maximizing the distance between each of these exemplars (max-
imize scattering). This trade-oﬀ optimization was achieved by the inclusions of
the non-negative trade-oﬀ parameters α and β. Because it was in QUBO form, the
objective function also included a non-negative penalty coeﬃcient γ, which was
applied to the constraint. The complete formulation can be expressed as:

min






fo = β(cid:126)zT ∆(cid:126)1
(cid:124) (cid:123)(cid:122) (cid:125)
centrality




+ γ((cid:126)zT(cid:126)1 − k)2

(cid:125)
(cid:123)(cid:122)
constraint

(cid:124)

.

− α

1
2

(cid:126)zT ∆(cid:126)z
(cid:125)
(cid:123)(cid:122)
scattering

(cid:124)

In its original presentation, this formulation was not aimed at or suited to the graph
clustering problem. We tailor it to graph clustering and Boltzmann machines, in
two ways. First, we remove the quadratic penalty constraint, which is unnecessary
with our Boltzmann heuristic. The built-in constraint handling oﬀered by the K-hot
encoding of our Boltzmann machine also reduces the burden of parameter-tuning, by
eliminating one of the three parameters in the original Bauckhage et al. formulation
[8]. Full details of the Boltzmann heuristic are presented in Section 5.

Second, as in the case of the quadratic distance minimization problem, we use the
Jaccard distances [33, 42, 12, 51, 50] as a distance metric dij. These modiﬁcations
allow the application of the K-medoids clustering technique to the general case of
graphs, where all pairs distances are not available. Finally, we also test the “robust-
iﬁcation” of distances using a Welsch’s M-estimator, as suggested by Bauckhage et
al. [8], but do not ﬁnd it useful. Through trial, we conclude the unmodiﬁed Jaccard
distance yields better results.

Our unconstrained formulation provides a completely equivalent problem (to the
original Bauckhage et al. problem). The cardinality constraint for exemplars
((cid:126)zT(cid:126)1 = K) is enforced directly by the K-hot encoding of the Boltzmann machine.

11

The ﬁnal problem can be expressed as:

min

s.t

1
2

− α

fo = β(cid:126)zT ∆(cid:126)1
(cid:124) (cid:123)(cid:122) (cid:125)
centrality





(cid:126)zT ∆(cid:126)z


(cid:125)
(cid:123)(cid:122)
scattering
(cid:126)zT(cid:126)1 = K (constraint no longer in objective)
zi ∈ {0, 1},

∀i ∈ V .

(cid:124)

Finally, we set the trade-oﬀ parameters to α = 2 and β = 1.05 × K+1
(recall K is
the number of exemplars, n is the number of points, vertices in this case). Through
trial and error, we ﬁnd these parameters provide good results. Naturally, the tuning
of these two parameters must be analyzed more closely. It should become the focus
of a future investigation.

n

4.3

Jaccard Distance

The Jaccard distance separating two vertices i and j is deﬁned as

dij = 1 −

|ci ∩ cj|
|ci ∪ cj|

∈ [0, 1] .

Here, ci (cj) represents the set of all vertices with which vertex i (j) shares an edge.
This distance measures the similarity in the respective neighborhoods of two nodes.
As stated earlier, this quantity has been shown to be inversely related to intra-cluster
cluster density [51, 50].

4.4 Louvain: modularity maximization

Because of its widespread use and its status as the “state of the art” graph clustering
technique, we compare the clustering quality obtained with our two formulations to
that obtained with the Louvain algorithm [9]. The Louvain algorithm is a greedy
iterative heuristic technique that maximizes a clustering quality function known as
modularity [53]. Unlike our two formulations presented in this article, the Louvain
technique does not require the number of clusters as an input parameter.

The Louvain method is known to be very fast, we do not include it in our study
to provide a comparison of solution times. Rather, we want to compare solution
qualities. Our goal is to assess the validity of our two mathematical formulations,
distance minimization and K-medoids, by comparing their output to that obtained
by maximizing modularity.

12

Below, we present modularity (Q), as shown in Blondel et al. [9]. Modularity is
deﬁned as

Q =

1
2m

(cid:88)

i,j

(cid:20)
Aij −

(cid:21)

kikj
2m

δ(ci, cj) .

In the equation above,

• m = |E| is the total number of edges in the graph,

• Aij is the element at the intersection of the i-th row and j-th column of the

adjacency matrix,

j Aij is the degree of vertex i,

• ki = (cid:80)
• δ(ci, cj) is the Kroenecker delta function, it is equal to one if ci = cj and

zero otherwise and

• ci is the cluster to which vertex i is assigned by the algorithm.

Modularity always lies on the interval [− 1
indicate a signiﬁcant clustering [13].

2 , 1] [10]. Values greater than 0.3 typically

A full discussion of modularity or the Louvain algorithm are beyond the scope of this
article. Results obtained with the Louvain algorithm are simply used for the purpose
of comparison. For a very thorough discussion of these topics we refer the reader to
the original authors [9], to the work of Fortunato [21] and Fortunato and Hric [23].
As highlighted earlier, the limitations of modularity (and Louvain consequently)
are also well described in the literature (e.g., [22, 1, 28, 37, 38, 48, 49]).

5 Boltzmann machines

Boltzmann machines (BM) are neural networks that have been used to heuristically
solve combinatorial optimization problems, for some time now [40, 3]. These
machines encode an optimization problem into a graph-like structure where each
decision variable is represented by a node. These nodes are logical units which
can be inactive (set to 0) or activated (set to 1). The original objective function is
then encoded as an energy function, using these logical units as decision variables
and the edge weights as coeﬃcients. Simulated annealing is used to minimize the
energy function.

To avoid confusion with the original graph being clustered, we only use the term
“graph” to refer to the original graph being clustered. The graph-like Boltzmann
encoding is referred to as the “Boltzmann network” or simply “network”. The
term “vertex” always refers to the original graph’s vertices, while the Boltzmann

13

network’s nodes are always referred to as “units”. We apply this terminological
convention throughout the remainder of this article.

The advantage of the Boltzmann encoding is that it allows large-scale parallelization
[40]. We further enhance this parallelization through the application of a parallel
tempering scheme [61, 25, 32, 16, 14]. This temperature exchange scheme allows
for a better coverage of the solution space. We use a multi-threaded implementation
of the Boltzmann machine with parallel tempering, on a 64-core machine with a
single instruction multiple data scheme (SIMD).

Two diﬀerent variations of the Boltzmann machine were created. Each structure
implicitly captures the cluster membership or the medoid cardinality constraints
of the mathematical formulations in Section 4. Unlike with digital or quantum
annealers, a QUBO objective function which encapsulates constraints is not required
[27]. However, each structure encodes the search space and objective function
diﬀerently.

5.1 One-hot encoded machine

Our ﬁrst variation of the Boltzmann machine is a one-hot encoded machine. We
call this version the integer Boltzmann machine [6]. In this architecture, each one of
the N vertices of the original graph to be clustered is represented by K Boltzmann
machine units. For a given vertex, each of the K corresponding units represents the
cluster membership to one of the K clusters. In total, we have N × K units. Under
this architecture, by design, exactly one of the K units representing a given vertex
is forced to take a value of one. Therefore, our cluster membership constraint is
always enforced structurally. We apply this variation of the Boltzmann machine to
the quadratic distance minimization formulation (QP) in Section 4.1.

The energy function (E) to be minimized is identical to the problem formulation
presented in Section 4.1:

E((cid:126)x) =

(cid:88)

(cid:88)

(cid:88)

xikxjkdij .

i

j>i

k

Each decision variable xik represents a Boltzmann network logical unit. The vector
(cid:126)x is a vector with scalar components xik. If vertex i is assigned to cluster k then
the corresponding unit xik is activated.

5.2 K-hot encoded machine

Our second variation is a K-hot encoded machine [7]. We apply this variation of the
Boltzmann machine to the K-medoids formulation (K-med). Here, each of the N

14

vertices is represented by only one unit, indicating whether or not it is an exemplar.
Once again, a structural constraint ensures exactly K out of the N units have a value
of one, at all times. Naturally, the K-hot encoding has a lower memory requirement
than the one-hot encoding machine and most solver architectures.

The energy function (E) to be minimized is identical to the problem formulation
presented in Section 4.2,





E((cid:126)x) = β

(cid:88)

xi

(cid:88)



i

j(cid:54)=i

(cid:124)

(cid:123)(cid:122)
centrality

dij



− α

(cid:88)

(cid:88)

xixjdij

.

(cid:125)

(cid:124)

i

j>i
(cid:123)(cid:122)
squattering

(cid:125)

In this case, the decision variables xi represent the BM unit corresponding to the
i-th graph vertex. The vector (cid:126)x is a vector with scalar components xi. If vertex i is
chosen as an exemplar, the unit xi is activated.

5.3 Simulated annealing and parallel tempering

Typically, Boltzmann machines are combined with simulated annealing to ﬁnd the
optimal machine state. At its core, simulated annealing is a random walk (Markov
chain) [39] through the search space. In both our variations of the Boltzmann ma-
chine, we use the Metropolis-Hastings algorithm to accept or reject solutions.

Unfortunately, simulated annealing often fails to converge to a global optimum and
remains stuck in a local one instead. This failure is mainly due to monotonically
decreasing temperatures used in the Metropolis-Hastings algorithm. Parallel tem-
pering mitigates this weakness [61, 25, 32, 16, 14]. By exchanging temperatures,
it is possible for the search to explore the feasible region further. Indeed, higher
temperatures increase the probability of accepting a non-improving move, thus of-
fering a broader coverage of the search space. This temperature exchange is known
as parallel tempering.

We use two instances of Parallel Tempering, each with 32 replicas (searches), to
utilize all 64 available cores. Each replica begins with one of 32 diﬀerent starting
temperatures {T1, T2, . . . , T32}. For convenience, these temperatures are sorted in
ascending order (i.e., T1 < T2 < . . . < T32) to form what some authors call a
“temperature ladder” (e.g., [6, 7]). While these temperatures remain constant over
the entire span of the search, they are swapped between searches according to the
pairwise exchange acceptance probability (EAP). A search Si with temperature Ti
can exchange temperatures with the search Si+1 having the temperature one step
above it on the temperature ladder, Ti+1, according to this probability: (Ei is the

15

energy function of search i)

(cid:26)

EAP = min

1, exp

(cid:18)(cid:18) 1
Ti

−

1
Ti+1

(cid:19)

(cid:19)(cid:27)

(Ei − Ei+1)

.

6 Numerical experiments and results

We test all three mathematical formulations, quadratic distance minimization (QP),
K-medoids (K-med) and Louvain, using 12 diﬀerent synthetic graphs of vary-
ing sizes and clustering diﬃculty. For the quadratic distance minimization and
K-medoids formulations, we also compare solution quality and times between a
leading commercial solver, Gurobi, and our two Boltzmann heuristics. Both Boltz-
mann machines and Gurobi were run on a 64-core/128 thread machine with SIMD
instructions, with all cores available to all three solvers. Meanwhile, we use the
single-core Louvain implementation of Aynaud [5], which we run on the same
machine.

Our test graphs are described in Section 6.2. Numerical results are presented in
Section 6.3 and Section 6.4. The easier cases of smaller planted partition graphs are
presented in Section 6.3. These results include trials with the Boltzmann heuristics,
with Gurobi and with the Louvain heuristic. We also conduct tests using larger more
complex graph structures, which are presented in Section 6.4. These results include
trials with the Boltzmann heuristics, the Louvain heuristic and only one experiment
using Gurobi. Gurobi did not return meaningful results and became unresponsive
(“crashing”) after three hours of run time in the QP formulation case.
It did,
however, return adequate results in the case of the K-medoids formulation after
roughly one hour of run time, before prematurely terminating (“crashing”) soon
after. We end our tests with an illustrative case-study using the famous United States
College Football Division IA 2000 season graph (football graph) [26]. This graph
with known cluster structure has often been used as a “ground-truth” benchmark in
graph clustering studies.

6.1 Limitation: the number of clusters

In reviewing the numerical results, it is important to consider the pivotal importance
of determining the number of clusters that best summarizes the data. As described
above, the Louvain algorithm does not require the number of clusters as an input
In contrast, both the QP and K-medoids techniques do require this
parameter.
input parameter. Naturally, this diﬀerence must be considered, as it represents a
limitation to any conclusion based on the comparisons shown in this article.

16

It is also important to note that determining the number of clusters that best suits a
data set is an open problem in the clustering literature. Indeed, several (general and
graph-speciﬁc) common clustering techniques (e.g., K-means) require the number
of clusters as an input parameter. As previously noted, it has been observed, in
the case of graph clustering, that techniques which require the number of clus-
ters as input tend to perform better than those which do not [23]. This gain in
performance is somewhat intuitive, given these techniques beneﬁt from additional
information.

6.2 Synthetic graph test scenarios

As described earlier, we conduct our experiments on 12 diﬀerent synthetic graphs,
with known cluster memberships. These graphs are generated using the NetworkX
Python library [29]. Our graphs are generated using two diﬀerent generative models,
with the use of the planted partition model (PPM) and with the stochastic block
model (SBM).

For the ﬁrst set of experiments, we generate three small PPM graphs containing
ﬁve clusters of 50 vertices each, for a total of 250 vertices. These graphs are gener-
ated using intra-/inter-cluster edge probabilities (Pintra/Pinter) of 0.9/0.1, 0.85/0.15,
0.8/0.2. These quantities also correspond to the mean intra-/inter-cluster densities of
the generative model (within a margin of 10−3). Indeed, as described in Setion 3.1,
mean intra-/inter-cluster density are empirical estimates of intra-/inter-cluster edge
probability.

The second set of experiments is conducted on larger more complex SBM graph
structures, each containing 5,266 vertices. This generative model allows for varying
cluster sizes and varying intra-/inter-cluster edge probabilities (Pintra/Pinter). While
we keep intra-/inter-cluster edge probabilities equal across culsters/cluster pairs, we
vary cluster sizes. In these experiments, we generate graphs with intra-cluster edge
probability (Pintra) of 0.8, 0.85 and 0.9. For each of those levels of intra-cluster
edge probability, we create graphs with inter-cluster edge probability (Pinter) of
0.05, 0.075, 0.1. Of course, here too, these quantities correspond to the mean intra-
/inter-cluster densities of the generative model (within a margin of 10−3). Clusters
sizes vary between 35 and 200 vertices. This large cluster size variability along
with a larger number of vertices complicate the clustering problem. Graph details
are summarized in Table 1.

In summary, all of our synthetic tests are conducted with graphs that are known to
be clusterable. Speciﬁcally, they are graphs whose cluster (community) structure
is known in advance. Reductions in intra-cluster edge probability, increases in

17

Table 1: Graph generative model details

Graph ID Pintra Pinter Num Clusters Cluster sizes Num vertices Num edges Gen Model
50
50
50
[35,200]
[35,200]
[35,200]
[35,200]
[35,200]
[35,200]
[35,200]
[35,200]
[35,200]

8,069
8,955
9,955
981,435
1,319,457
1,654,464
962,657
1,301,791
1,640,511
945,192
1,283,853
1,621,210

250
250
250
5,266
5,266
5,266
5,266
5,266
5,266
5,266
5,266
5,266

0.1
0.15
0.2
0.05
0.075
0.1
0.05
0.075
0.1
0.05
0.075
0.1

PPM
PPM
PPM
SBM
SBM
SBM
SBM
SBM
SBM
SBM
SBM
SBM

0.9
0.85
0.8
0.9
0.9
0.9
0.85
0.85
0.85
0.8
0.8
0.8

G1
G2
G3
G4
G5
G6
G7
G8
G9
G10
G11
G12

5
5
5
50
50
50
50
50
50
50
50
50

inter-cluster edge probability and variations in cluster sizes are used to introduce
noise and complicate the cluster assignment process. Similarly, increases in graph
sizes (number of vertices) are meant to increase computational challenge.

6.3 PPM results

In Tables 2- 6, graph characteristics are displayed in the ﬁrst three rows. Pintra de-
notes the intra-cluster edge probability in the generative model and inter-cluster edge
probability is denoted as Pinter. Each graph’s density is denoted as K. Clustering
results appear in the lower portion of the tables.

We immediately note that all four clustering formulation-solution technique com-
binations yield the same results, as shown in Tables 2 to 6.
In all three cases
(G1-G3), they recover the generative model exactly. In fact, the Louvain method
even recovers the exact number of clusters in the generative model.

The only distinguishing results in these experiment are the times required to ob-
tain a solution. By far, the fastest convergence was obtained with a K-medoids
formulation solved using a Boltzmann machine. The Louvain method, known to
be very fast, was the second fastest to converge. The QP formulation solved on
a Boltzmann machine was third. By far the slowest convergence was observed
in the case of K-medoids solved with Gurobi. The QP formulation solved using
the Gurobi solver was faster than in the K-medoids case, but orders of magnitude
slower than the Boltzmann machine or Louvain.

18

Table 2: Quadratic distance minimization (QP Boltzmann)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
Time to sol (s)
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G1
0.10
0.90
0.26
0.10
0.90
0.001
Y
Y
0.48

G2
0.15
0.85
0.29
0.15
0.85
0.001
Y
Y
0.38

G3
0.20
0.80
0.32
0.20
0.80
0.001
Y
Y
0.29

Table 3: Quadratic distance minimization (QP Gurobi)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
Time to sol (s)
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G1
0.1
0.9
0.26
0.10
0.90
3.64
Y
Y
0.48

G2
0.15
0.85
0.29
0.15
0.85
1.45
Y
Y
0.38

G3
0.2
0.8
0.32
0.20
0.80
1.48
Y
Y
0.29

Table 4: K-medoids (K-med Boltzmann)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
Time to sol (s)
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G1
0.1
0.9
0.26
0.10
0.90
0.000
Y
Y
0.48

G2
0.15
0.85
0.29
0.15
0.85
0.001
Y
Y
0.38

G3
0.2
0.8
0.32
0.20
0.80
0.001
Y
Y
0.29

19

Table 5: K-medoids (K-med Gurobi)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
Time to sol (s)
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G1
0.1
0.9
0.26
0.10
0.90
600
Y
Y
0.48

G2
0.15
0.85
0.29
0.15
0.85
600
Y
Y
0.38

G3
0.2
0.8
0.32
0.20
0.80
600
Y
Y
0.29

Table 6: Modularity maximization (Louvain)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
Time to sol (s)
Clusters identiﬁed
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G1
0.1
0.9
0.26
0.10
0.90
0.047
5
Y
Y
0.48

G2
0.15
0.85
0.29
0.15
0.85
0.062
5
Y
Y
0.38

G3
0.2
0.8
0.32
0.20
0.80
0.056
5
Y
Y
0.29

20

Table 7: Quadratic distance minimization (QP Boltzmann, 10 min run ime)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G4
0.05
0.9
0.07
0.06
0.76
Y
Y
0.20

G5
0.075
0.9
0.10
0.08
0.75
Y
Y
0.14

G6
0.10
0.9
0.12
0.11
0.75
Y
Y
0.11

G7
0.05
0.85
0.07
0.06
0.71
Y
Y
0.19

G8
0.075
0.85
0.09
0.08
0.71
Y
Y
0.13

G9 G10 G11 G12
0.1
0.1
0.8
0.85
0.12
0.12
0.11
0.11
0.67
0.71
Y
Y
Y
Y
0.09
0.10

0.075
0.8
0.09
0.08
0.67
Y
Y
0.12

0.05
0.8
0.07
0.06
0.67
Y
Y
0.18

Table 8: K-medoids Gurobi (K-med Gurobi, ∼ 1 hour run time)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G4
0.05
0.9
0.07
0.05
0.83
Y
Y
0.28

G5
0.075
0.9
0.10
0.08
0.58
Y
Y
0.15

G6
0.10
0.9
0.12
0.11
0.41
Y
Y
0.08

G7
0.05
0.85
0.07
0.05
0.76
Y
Y
0.26

G8
0.075
0.85
0.09
0.08
0.56
Y
Y
0.15

G9 G10 G11 G12
0.1
0.1
0.8
0.85
0.12
0.12
0.11
0.11
0.26
0.44
Y
Y
Y
Y
0.04
0.09

0.075
0.8
0.09
0.08
0.53
Y
Y
0.14

0.05
0.8
0.07
0.05
0.66
Y
Y
0.24

6.4 SBM results

Graph characteristics and numerical results are reported in Tables 7 to 10. Here
too, graph characteristics are displayed in the ﬁrst three rows. Again, Pintra denotes
the intra-cluster edge probability, inter-cluster edge probability is denoted as Pinter
and the graph’s density as K. We report Boltzmann experiment results obtained
after 10 minutes of run time, while noting convergence had not been achieved. As
mentioned previously, Gurobi failed to return meaningful results after three hours,
in the case of the QP formulation. It did, however, return adequate results in the case
of the K-medoids formulation, after roughly an hour of run time, before exiting
prematurely. The Louvain algorithm converged in less than 21.5 seconds, in all
instances.

Three notable results appear in this set of experiments. First, we note that all
three formulations, QP, K-medoids and Louvain lead to arguably good cluster-
ing, regardless of the numerical solution technique. In all cases, the inequalities

21

Table 9: K-medoids (K-med Boltzmann, 10 min run time)

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

G4
0.05
0.9
0.07
0.05
0.90
Y
Y
0.28

G5
0.075
0.9
0.10
0.08
0.84
Y
Y
0.20

G6
0.10
0.9
0.12
0.10
0.77
Y
Y
0.14

G7
0.05
0.85
0.07
0.05
0.85
Y
Y
0.27

G8
0.075
0.85
0.09
0.08
0.80
Y
Y
0.19

G9 G10 G11 G12
0.1
0.1
0.8
0.85
0.12
0.12
0.10
0.11
0.65
0.61
Y
Y
Y
Y
0.13
0.12

0.075
0.8
0.09
0.08
0.66
Y
Y
0.17

0.05
0.8
0.07
0.05
0.77
Y
Y
0.26

Graph ID

p
a
r
G

h Pinter
Pintra
K
¯Kinter
¯Kintra
Clusters identiﬁed
¯Kinter < K
K < ¯Kintra
Modularity

s
t
l
u
s
e
R

Table 10: Louvain

G4
0.05
0.9
0.07
0.05
0.56
21
Y
Y
0.27

G5
0.075
0.9
0.10
0.08
0.63
22
Y
Y
0.20

G6
0.1
0.9
0.12
0.11
0.62
18
Y
Y
0.15

G7
0.05
0.85
0.07
0.05
0.71
30
Y
Y
0.27

G8
0.075
0.85
0.09
0.08
0.59
22
Y
Y
0.19

G9
0.1
0.85
0.12
0.11
0.63
18
Y
Y
0.14

G10 G11
0.05
0.8
0.07
0.05
0.60
22
Y
Y
0.25

0.075
0.8
0.09
0.08
0.60
25
Y
Y
0.18

G12
0.1
0.8
0.12
0.11
0.51
17
Y
Y
0.13

22

¯Kinter < K < ¯Kintra hold. Second, we note that clustering quality of the dis-
tance minimization, K-medoids are roughly equivalent or superior to the clustering
quality obtained with the Louvain method, depending of the numerical solution
technique used (Gurobi or BM). Finally, we note that Louvain provides, by far, the
fastest solutions (21.5 seconds vs. 600+).

A closer examination of intra-cluster densities, shown in Table 11, highlights the
diﬀerences in solution quality. First, we immediately note that the intra-cluster
densities identiﬁed by the Louvain method remain stuck between 60 and 70%.
Regardless of the underlying graph structure, they seem to vary randomly within
that interval. In contrast, the intra-cluster densities identiﬁed by both the distance
minimization and K-medoids formulations decrease with intra-cluster edge prob-
abilities and with increases in inter-cluster edge probabilities. We also note that
while BM K-medoids oﬀers higher, more accurate, intra-cluster densities than the
(BM) QP formulation, in most cases, it also appears more sensitive to inter-cluster
edge probabilitiy.

Our results also underscore the strength of our BM solver.
Indeed, even with
the same mathematical formulation (K-med) and the beneﬁt of a much longer
run time, Gurobi yields sparser clusters than our BM. These results can be seen
in Tables 8, 9, 11. More importantly, our one-hot BM was also able to obtain
competitive results for the QP formulation in the cases of the larger more complex
SBM graphs. In those same cases, Gurobi not only failed to converge after several
hours of run time, it became unresponsive .

Finally, our results highlight the disconnection between modularity and intra-cluster
density. While the Louvain heuristic, predictably, yields the highest modularity
levels, it fails to obtain the densest clusters. Our results also show that, in many
instances, clusterings with lower modularity are in fact denser. For example, in
Table 7 we see a modularity of 0.10 and a mean intra-cluster density ¯Kintra =
0.71 obtained with the (BM) QP formulation-solver combination, in the case of
graph G9. Meanwhile, for the same graph, the Louvain technique yields a higher
modularity (0.14), but a much lower mean intra-cluster density ¯Kintra = 0.60. These
results are consistent with previous experiments comparing modularity and density
[48, 49].

6.5

Illustrative case study: the US College Football graph

Our earlier numerical tests using synthetic graphs are designed to compare the
ability of each technique and solver under study to identify densely connected
subgraphs. However, we ﬁnd it useful to also examine the ability of each formulation

23

Table 11: Side by side comparisons of ¯Kintra

Mathematical formulation

Graph characteristics

Graph ID
G4
G5
G6
G7
G8
G9
G10
G11
G12
Count Best of 3

Pinter Pintra
0.9
0.05
0.9
0.075
0.9
0.1
0.85
0.05
0.85
0.075
0.85
0.1
0.8
0.05
0.8
0.075
0.8
0.1

K
0.071
0.095
0.119
0.069
0.094
0.118
0.068
0.093
0.117

Louvain QP BM K-med Gurobi K-med BM
¯Kintra
0.56
0.63
0.62
0.71
0.59
0.63
0.60
0.60
0.51
0

¯Kintra
0.90
0.84
0.77
0.85
0.80
0.61
0.77
0.66
0.65
6

¯Kintra
0.76
0.75
0.75
0.71
0.71
0.71
0.67
0.67
0.67
3

¯Kintra
0.83
0.58
0.41
0.76
0.56
0.44
0.66
0.53
0.26
0

Table 12: US College Football Division IA 2000 season graph charateristics

|V | (vertices/teams)
|E| (edges)
K (density)
Clusters (conferences)

115
613
0.09
12

and solver to recover the cluster membership of a real-world graph’s vertices with
known cluster membership. For these comparisons, we use the famous United
States College Football Division IA 2000 season graph (football graph) [26]. This
graph is a representation of the regular season encounters between 115 college
football teams. Each team is represented by a vertex. These vertices grouped into
one of twelve conferences (clusters). Edges connect teams that faced each other
at least once during the regular season. Teams within a conference all face each
other during regular season, while they do not necessarily face teams outside their
conference during the regular season. Therefore, there are more shared connections
between teams of the same conference than between teams in diﬀerent conferences.
Graph characteristics are provided in Table 12.

While this examination reveals interesting results, we ﬁnd it important to highlight
its limitations. The objective functions of the techniques discussed in this article
are designed to yield the clusterings with densest subgraphs (QP), the most rep-
resentative exemplars (K-med) or the clusterings with the highest modularity, at
their respective optima. In accordance with the universally accepted notion that

24

a successful graph clustering yields densely connected clusters (subgraphs), we
assess clustering quality via the mean intra-cluster density of the clusters identiﬁed
through each of the techniques/solvers in this study.

Unfortunately, the ground-truth cluster membership of the vertices in a real-world
graph may not correspond to the clustering with the densest clusters. After all,
real-world graphs are instances of typically unknown latent generative models and
random noise. In many cases, it may be possible to modify the cluster assignments
of a labeled real-world graph’s vertices and obtain higher intra-cluster densities.
For this reason, results in this section should be taken in context and not understood
as a deﬁnitive ranking of the various optimization techniques and solvers. More
generally, it should be noted that ground-truth graphs are not typically the best
benchmarks for clustering quality assessments.

Another challenge posed by this examination is the measurement of clustering
accuracy with respect to the ground-truth clustering. Here, it is not suﬃcient to
compare clusters according to their labels, because these labels are arbitrary. For
example, cluster cg
1 in the ground-truth (superscript g) labeling may correspond
exactly to cluster ca
2 returned by the clustering procedure (superscript a). Simply
comparing clusters according to their labels (e.g., comparing cg
1) is not mean-
ingful. Instead, cluster constituents must be compared. Each ground-truth cluster
must be compared to each cluster identiﬁed by a clustering procedure, to assess the
similarity of cluster contents.

1 to ca

Clusters, be they ground-truth or identiﬁed by an algorithm, are disjoint sets of
vertices (empty set intersection). For this reason, we use Jaccard similarity function
(J) to compare contents of ground-truth clusters (cg
i ) and those identiﬁed by an
j ). We take the maximum Jaccard ( ˜J) similarity over all possible
algorithm (ca
ground-truth clusters as a gauge of similarity between a cluster identiﬁed by an
algorithm and its associated ground-truth benchmark:

˜J(ca

j ) = max

cg
i

(cid:40)

J(cg

i , ca

j ) =

|cg
|cg

i ∩ ca
j |
i ∪ ca
j |

(cid:41)

.

Each ˜J(ca
j ) provides a score for the similarity of a cluster as identiﬁed by clustering
algorithm (ca
j ) and its associated benchmark. A perfect match between a cluster
identiﬁed by clustering algorithm and its associated benchmark yields a value
of ˜J(ca
j ) = 0. To
obtain a graph-level view and a valid comparison, we compute the means of the
˜J over all clusters. Comparisons are presented in Table 13. These comparison

j ) = 1, while a complete mismatch yields a value of ˜J(ca

25

Table 13: Similarity to ground-truth clusters

Num exact matches ( ˜J = 1)
Mean ˜J

4
0.72

3
0.78

3
0.81

6
0.84

6
0.83

Louvain QP Gurobi QP BM K-med Gurobi K-med BM

reveal that the K-medoids formulation provides a better match to the ground-truth
clustering.

7 Conclusion

In this article, we have successfully adapted the quadratic distance minimization
(QP) and quadratic K-medoids formulations to the graph clustering problem. These
formulations provide better results than the well established “state of the art” Lou-
vain method. We also illustrate the value of a Boltzmann heuristic in solving
NP-hard combinatorial optimization problems.

Future work will focus on additional comparisons, K-medoid parameter tuning,
clustering with overlapping clusters and scalability. The Louvain method has
been applied to very large scale commercial problems. Any alternative must be
applicable to similarly large scale problems. Finally, because both our mathematical
formulations rely heavily on vertex-vertex distance, we also intend to pursue further
examinations of the topic.

Acknowledgements

The authors would like to thank Fujitsu Limited and Fujitsu Consulting (Canada)
Inc. for providing ﬁnancial support.

Availability of data and materials

Most of the data used in our experiments was generated using the NetworkX li-
brary’s planted partition and stochastic block model generators [29]. All parameters
required to reproduce our data are in Table 1.

The football graph used in Section 6.5 was obtained from Mark Newman’s web
site. http://www-personal.umich.edu/∼mejn/netdata/football.zip

26

Competing interests

The authors declare that they have no competing interests.

Funding

The authors received ﬁnancial support from Fujitsu Limited and Fujitsu Consulting
(Canada) Incorporated.

References

[1] Ackerman, M., Ben-David, S.: Measures of clustering quality: A working set
of axioms for clustering. Advances in Neural Information Processing Systems
21 - Proceedings of the 2008 Conference pp. 121–128 (2008)

[2] Aloise, D., Caporossi, G., Hansen, P., Liberti, L., Perron, S., Ruiz, M.: Mod-
ularity maximization in networks by variable neighborhood search. In: D.A.
Bader, H. Meyerhenke, P. Sanders, D. Wagner (eds.) Graph Partitioning and
Graph Clustering, 10th DIMACS Implementation Challenge Workshop, Geor-
gia Institute of Technology, Atlanta, GA, USA, February 13-14, 2012. Pro-
ceedings, pp. 113–128 (2012). URL http://www.ams.org/books/conm/
588/11705

[3] Anthony, M.: Boltzmann Machines and Combinatorial Optimization,
chap. 13, pp. 115–118 (2001). DOI 10.1137/1.9780898718539.ch13. URL
https://epubs.siam.org/doi/abs/10.1137/1.9780898718539.
ch13

[4] Aramon, M., Rosenberg, G., Valiante, E., Miyazawa, T., Tamura, H.,
Katzgraber, H.: Physics-Inspired Optimization for Quadratic Unconstrained
Problems Using a Digital Annealer. Frontiers in Physics 7, 48 (2019).
DOI 10.3389/fphy.2019.00048. URL https://www.frontiersin.org/
article/10.3389/fphy.2019.00048

[5] Aynaud, T.: python-louvain x.y: Louvain algorithm for community detection.

https://github.com/taynaud/python-louvain (2020)

[6] Bagherbeik, M., Ashtari, P., Mousavi, S.F., Kanda, K., Tamura, H., Sheik-
holeslami, A.: A permutational boltzmann machine with parallel tempering
for solving combinatorial optimization problems.
In: T. Bäck, M. Preuss,
A. Deutz, H. Wang, C. Doerr, M. Emmerich, H. Trautmann (eds.) Parallel

27

Problem Solving from Nature – PPSN XVI, pp. 317–331. Springer Interna-
tional Publishing, Cham (2020)

[7] Bagherbeik, M., Sheikholeslami, A.: Caching and vectorization schemes to
accelerate local search algorithms for assignment problems. In: 2021 IEEE
Congress on Evolutionary Computation (CEC), pp. 2549–2558 (2021). DOI
10.1109/CEC45853.2021.9504700

[8] Bauckhage, C., Piatkowski, N., Sifa, R., Hecker, D., Wrobel, S.: A QUBO
formulation of the k-medoids problem. In: R. Jäschke, M. Weidlich (eds.)
Proceedings of the Conference on "Lernen, Wissen, Daten, Analysen", Berlin,
Germany, September 30 - October 2, 2019, CEUR Workshop Proceedings,
vol. 2454, pp. 54–63. CEUR-WS.org (2019). URL http://ceur-ws.org/
Vol-2454/paper_39.pdf

[9] Blondel, V.D., Guillaume, J.L., Lambiotte, R., Lefebvre, E.: Fast unfolding
of communities in large networks. Journal of Statistical Mechanics: Theory
and Experiment 2008(10), P10008 (2008). DOI 10.1088/1742-5468/2008/
10/p10008

[10] Brandes, U., Delling, D., Gaertler, M., Gorke, R., Hoefer, M., Nikoloski,
Z., Wagner, D.: On modularity clustering.
IEEE Trans. on Knowl. and
Data Eng. 20(2), 172–188 (2008). DOI 10.1109/TKDE.2007.190689. URL
http://dx.doi.org/10.1109/TKDE.2007.190689

[11] Brownlee, J.: Clever Algorithms: Nature-Inspired Programming Recipes, 1st

edn. Lulu.com (2011)

[12] Camby, E., Caporossi, G.: The extended Jaccard distance in complex net-

works. Les Cahiers du GERAD G-2017-77 (2017)

[13] Clauset, A., Newman, M.E.J., Moore, C.: Finding community structure in
very large networks. preprint 066111 (2004). DOI 10.1103/PhysRevE.70.
066111

[14] Dabiri, K., Malekmohammadi, M., Sheikholeslami, A., Tamura, H.: Replica
exchange MCMC hardware with automatic temperature selection and parallel
trial. IEEE Transactions on Parallel and Distributed Systems 31(7), 1681–
1692 (2020). DOI 10.1109/TPDS.2020.2972359

[15] van Dongen, S.: Graph clustering by ﬂow simulation. Ph.D. thesis, Faculteit

Wiskunde en Informatica, Universiteit Utrecht (2000)

28

[16] Earl, D.J., Deem, M.W.: Parallel tempering: Theory, applications, and new
perspectives. Phys. Chem. Chem. Phys. 7, 3910–3916 (2005). DOI 10.1039/
B509983H

[17] Fan, N., Pardalos, P.M.: Linear and quadratic programming approaches for the
general graph partitioning problem. J. of Global Optimization 48(1), 57–71
(2010). DOI 10.1007/s10898-009-9520-1. URL http://dx.doi.org/10.
1007/s10898-009-9520-1

[18] Fan, N., Pardalos, P.M.: Robust optimization of graph partitioning and critical
node detection in analyzing networks. In: Proceedings of the 4th International
Conference on Combinatorial Optimization and Applications - Volume Part I,
COCOA’10, pp. 170–183. Springer-Verlag, Berlin, Heidelberg (2010). URL
http://dl.acm.org/citation.cfm?id=1940390.1940405

[19] Fan, N., Pardalos, P.M.: Multi-way clustering and biclustering by the ra-
J. Comb. Optim. 23(2), 224–251
tio cut and normalized cut in graphs.
(2012). DOI 10.1007/s10878-010-9351-5. URL https://doi.org/10.
1007/s10878-010-9351-5

[20] Fan, N., Zheng, Q.P., Pardalos, P.M.: Robust optimization of graph partition-
ing involving interval uncertainty. Theoretical Computer Science - TCS 447,
53–61 (2012). DOI 10.1016/j.tcs.2011.10.015

[21] Fortunato, S.: Community detection in graphs. Physics Reports 486, 75–
174 (2010). DOI 10.1016/j.physrep.2009.11.002. URL http://adsabs.
harvard.edu/abs/2010PhR...486...75F

[22] Fortunato, S., Barthélemy, M.: Resolution limit in community detection.
Proceedings of the National Academy of Sciences 104(1), 36–41 (2007).
DOI 10.1073/pnas.0605965104. URL http://www.pnas.org/content/
104/1/36.abstract

[23] Fortunato, S., Hric, D.: Community detection in networks: A user guide.
Physics Reports 659, 1–44 (2016). DOI 10.1016/j.physrep.2016.09.002

[24] Fu, Y., Anderson, P.W.: Application of statistical mechanics to NP-complete
problems in combinatorial optimisation. Journal of Physics A: Mathematical
and General 19(9), 1605–1620 (1986)

[25] Geyer, C.J.: Parallel tempering. In: Computing Science and Statistics Pro-
ceedings of the 23rd Symposium on the Interface, American Statistical Asso-
ciation, New York (1991)

29

[26] Girvan, M., Newman, M.E.J.: Community structure in social and biological
networks. Proceedings of the National Academy of Sciences 99(12), 7821–
7826 (2002). DOI 10.1073/pnas.122653799. URL https://www.pnas.
org/content/99/12/7821

[27] Glover, F., Kochenberger, G., Du, Y.: A Tutorial on Formulating and Using

QUBO Models. arXiv e-prints arXiv:1811.11538 (2018)

[28] Good, B.H., de Montjoye, Y.A., Clauset, A.: Performance of modularity
maximization in practical contexts. preprint 81(4), 046106 (2010). DOI
10.1103/PhysRevE.81.046106

[29] Hagberg, A., Schult, D., Swart, P.: Exploring Network Structure, Dynamics,
and Function using NetworkX. In: G. Varoquaux, T. Vaught, J. Millman (eds.)
Proceedings of the 7th Python in Science Conference, pp. 11–15. Pasadena,
CA USA (2008)

[30] Hahn, G., Djidjev, H.N.: Reducing binary quadratic forms for more scalable

quantum annealing (2018)

[31] Hastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning,
Second Edition: Data Mining, Inference, and Prediction, 2nd ed. 2009. edn.
Springer Series in Statistics. Springer (2009)

[32] Hukushima, K., Nemoto, K.: Exchange Monte Carlo method and application
to spin glass simulations. Journal of the Physical Society of Japan 65(6),
1604–1608 (1996). DOI 10.1143/JPSJ.65.1604

[33] Jaccard, P.: Étude de la distribution ﬂorale dans une portion des Alpes et du
Jura. Bulletin de la Société Vaudoise des Sciences Naturelles 37, 547–579
(1901). DOI 10.5169/seals-266450

[34] Jin, J.: FAST COMMUNITY DETECTION BY SCORE. The Annals of

Statistics 43 (2015)

[35] Kaufman, L., Rousseeuw, P.J.: Partitioning Around Medoids (Program PAM),
chap. 2, pp. 68–125. John Wiley & Sons, Ltd (1990). DOI https://doi.org/10.
1002/9780470316801.ch2. URL https://onlinelibrary.wiley.com/
doi/abs/10.1002/9780470316801.ch2

[36] Kazakovtsev, L., Antamoshkin, A.: Genetic algorithm with fast greedy
heuristic for clustering and location problems. Informatica (Slovenia) 38(3)
(2014). URL http://www.informatica.si/index.php/informatica/
article/view/704

30

[37] Kehagias, A., Pitsoulis, L.: Bad communities with high modular-
The European Physical Journal B 86(7), 330 (2013).

ity.
10.1140/epjb/e2013-40169-1. URL https://doi.org/10.1140/epjb/
e2013-40169-1

DOI

[38] Kehagias, A., Pitsoulis, L.: Graph Clustering Quality Functions. Unpublished

(2018)

[39] Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P.: Optimization by simulated anneal-
ing. Science 220(4598), 671–680 (1983). DOI 10.1126/science.220.4598.
671. URL https://www.science.org/doi/abs/10.1126/science.
220.4598.671

[40] Korst, J.H.M., Aarts, E.H.L.: Combinatorial optimization on a boltzmann
machine. Journal of Parallel and Distributed Computing 6(2), 331–357 (1989).
DOI https://doi.org/10.1016/0743-7315(89)90064-6. URL https://www.
sciencedirect.com/science/article/pii/0743731589900646

[41] Lancichinetti, A., Radicchi, F., Ramasco, J.J., Fortunato, S.: Finding Statis-
tically Signiﬁcant Communities in Networks. PLoS ONE 6, e18961 (2011).
DOI 10.1371/journal.pone.0018961

[42] Levandowsky, M., Winter, D.: Distance between Sets. Nature 234 (1971)

[43] Lloyd, S.: Least squares quantization in PCM. IEEE Transactions on Infor-
mation Theory 28(2), 129–137 (1982). DOI 10.1109/TIT.1982.1056489

[44] Lucas, A.: Ising formulations of many NP problems. Frontiers in Physics 2,

5 (2014). DOI 10.3389/fphy.2014.00005

[45] von Luxburg, U.: A Tutorial on Spectral Clustering. CoRR abs/0711.0189

(2007). URL http://arxiv.org/abs/0711.0189

[46] Matsubara, S., Takatsu, M., Miyazawa, T., Shibasaki, T., Watanabe, Y., Take-
moto, K., Tamura, H.: Digital annealer for high-speed solving of combi-
natorial optimization problems and its applications.
In: 2020 25th Asia
and South Paciﬁc Design Automation Conference (ASP-DAC), pp. 667–672
(2020). DOI 10.1109/ASP-DAC47756.2020.9045100

[47] Miasnikof, P., Pitsoulis, L., Bonner, A., Lawryshyn, Y., Pardalos, P.: Graph
clustering via intra-cluster density maximization. In: I. Bychkov, V. Kalyagin,
P. Pardalos, O. Prokopyev (eds.) Network Algorithms, Data Mining, and
Applications, pp. 37–48. Springer International Publishing (2020)

31

[48] Miasnikof, P., Shestopaloﬀ, A., Bonner, A., Lawryshyn, Y.: A Statistical
Performance Analysis of Graph Clustering Algorithms, chap. 11. Lecture
Notes in Computer Science. Springer Nature (2018)

[49] Miasnikof, P., Shestopaloﬀ, A., Bonner, A., Lawryshyn, Y., Pardalos, P.: A
density-based statistical analysis of graph clustering algorithm performance.
Journal of Complex Networks 8(3) (2020). DOI 10.1093/comnet/cnaa012.
URL https://doi.org/10.1093/comnet/cnaa012

[50] Miasnikof, P., Shestopaloﬀ, A.Y., Pitsoulis, L., Ponomarenko, A.: An empiri-
cal comparison of connectivity-based distances on a graph and their computa-
tional scalability. Journal of Complex Networks 10(1) (2022). DOI 10.1093/
comnet/cnac003. URL https://doi.org/10.1093/comnet/cnac003

[51] Miasnikof., P., Shestopaloﬀ, A.Y., Pitsoulis, L., Ponomarenko, A., Lawryshyn,
Y.: Distances on a graph. In: R.M. Benito, C. Cheriﬁ, H. Cheriﬁ, E. Moro,
L.M. Rocha, M. Sales-Pardo (eds.) Complex Networks & Their Applications
IX, pp. 189–199. Springer International Publishing, Cham (2021)

[52] Nascimento, M., Pitsoulis, L.: Community detection by modularity max-
imization using GRASP with path relinking. Computers and Operations
Research 40, 3121–3131 (2013)

[53] Newman, M.E.J., Girvan, M.: Finding and evaluating community structure
in networks. Physical review. E, Statistical, nonlinear, and soft matter physics
69, 026113 (2004)

[54] Papadimitriou, C., Steiglitz, K.: Combinatorial Optimization: Algorithms
and Complexity. Dover Books on Computer Science. Dover Publications
(1998). URL https://books.google.ca/books?id=u1RmDoJqkF4C

[55] Ponomarenko, A., Pitsoulis, L., Shamshetdinov, M.: Overlapping commu-
nity detection in networks based on link partitioning and partitioning around
medoids. PLOS ONE 16(8), 1–43 (2021). DOI 10.1371/journal.pone.
0255717. URL https://doi.org/10.1371/journal.pone.0255717

[56] Prokhorenkova, L.: Using synthetic networks for parameter tuning in com-
In: K. Avrachenkov, P. Prałat, N. Ye (eds.) Algorithms
munity detection.
and Models for the Web Graph, pp. 1–15. Springer International Publishing,
Cham (2019)

[57] Prokhorenkova, L.O., Prałat, P., Raigorodskii, A.: Modularity of complex
networks models. In: A. Bonato, F. Graham., P. Prałat (eds.) Algorithms and

32

Models for the Web Graph, pp. 115–126. Springer International Publishing,
Cham (2016)

[58] Prokhorenkova, L.O., Prałat, P., Raigorodskii, A.: Modularity in sev-
Electronic Notes in Discrete Mathemat-
eral random graph models.
ics 61, 947–953 (2017).
DOI https://doi.org/10.1016/j.endm.2017.07.
058. URL http://www.sciencedirect.com/science/article/pii/
S1571065317302238. The European Conference on Combinatorics, Graph
Theory and Applications (EUROCOMB’17)

[59] Sao, M., Watanabe, H., Musha, Y., Utsunomiya, A.: Application of Digi-
tal Annealer for Faster Combinatorial Optimization. Fujitsu Scientiﬁc and
Technical Journal 55(2), 45–51 (2019)

[60] Schaeﬀer, S.E.: Survey: Graph clustering. Comput. Sci. Rev. 1(1), 27–64
(2007). DOI 10.1016/j.cosrev.2007.05.001. URL http://dx.doi.org/
10.1016/j.cosrev.2007.05.001

[61] Swendsen, R.H., Wang, J.S.: Replica monte carlo simulation of spin-glasses.
Phys. Rev. Lett. 57, 2607–2609 (1986). DOI 10.1103/PhysRevLett.57.2607.
URL https://link.aps.org/doi/10.1103/PhysRevLett.57.2607

33

