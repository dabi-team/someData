Eﬀect or Treatment Heterogeneity?
Policy Evaluation with Aggregated and Disaggregated
Treatments

Phillip Heiler†

Michael C. Knaus‡

First version: October 04, 2021
This version: September 23, 2022

Abstract

Binary treatments are often ex-post aggregates of multiple treatments or can be
disaggregated into multiple treatment versions. Thus, eﬀects can be heterogeneous due
to either eﬀect or treatment heterogeneity. We propose a decomposition method that
uncovers masked heterogeneity, avoids spurious discoveries, and evaluates treatment
assignment quality. The estimation and inference procedure based on double/debiased
machine learning allows for high-dimensional confounding, many treatments and
extreme propensity scores. Our applications suggest that heterogeneous eﬀects
of smoking on birthweight are partially due to diﬀerent smoking intensities and
that gender gaps in Job Corps eﬀectiveness are largely explained by diﬀerences in
vocational training.

Keywords: causal inference, causal machine learning, double machine learning,
heterogeneous treatment eﬀects, overlap, treatment versions

JEL classiﬁcation: C14, C21

2
2
0
2

p
e
S
2
2

]

M
E
.
n
o
c
e
[

2
v
7
2
4
1
0
.
0
1
1
2
:
v
i
X
r
a

∗Michael Knaus gratefully acknowledges ﬁnancial support from the Swiss National Science Foundation
(SNSF) (grant number SNSF 407740_187301). The paper was circulated and presented previously under
diﬀerent titles. We would like to thank Martin Huber, Michael Lechner, Jana Mareckova, Julian Schüssler,
Anthony Strittmatter, and the participants of sessions at IAAE2021, ESEM2021, COMPIE2021, and the
FFHKT Econometrics Seminar for valuable comments and discussions. All remaining errors are ours.

†Aarhus University, Department of Economics and Business Economics, CREATES, TrygFonden’s

Centre for Child Research, Fuglesangs Allé 4, 8210 Aarhus V, Denmark, pheiler@econ.au.dk.

‡University of Tübingen, Mohlstraße. 36, 72074 Tübingen, Germany. Michael C. Knaus is also aﬃliated

with IZA, Bonn, michael.knaus@uni-tuebingen.de.

1

 
 
 
 
 
 
1 Introduction

The analysis of causal eﬀects is at the heart of empirical research in economics, political

science, the biomedical sciences, and beyond. To evaluate and design policies, interventions,

or programs for observational units with diﬀerent background characteristics, it is crucial

to develop a thorough understanding of the heterogeneity present in causal relationships.

There is now a large literature that develops and applies identiﬁcation and estimation

strategies for causal or treatment parameters that explicitly take into account such

heterogeneity, see Athey and Imbens (2017) or Abadie and Cattaneo (2018) for recent

overviews.

Most attention is on eﬀect heterogeneity of binary treatments, while less is given

to treatment heterogeneity. However, many binary treatments in applications can be

conceived as heterogeneous in the sense that they summarize (many) underlying eﬀective

treatments that impact the outcome of interest. In such cases it is not clear whether eﬀect

heterogeneity as deﬁned in the canonical binary treatment setting reﬂects heterogeneous

eﬀects or heterogeneity in the eﬀective treatments. This paper proposes new estimands

and estimators to disentangle these sources of heterogeneity in a general setting where the

analyzed binary indicator does not coincide with the eﬀective treatment. The distinction

between sources of heterogeneity is crucial for evaluating and improving assignment

mechanisms. Consider the following two scenarios:1

Scenario 1 (binarized treatments): Multiple or continuous treatments are ex-post

subsumed into a binary indicator (e.g. diﬀerent smoking intensities become “smoking

yes/no”). Such aggregations are often motivated by simplicity or data availability, but can

have unintentional consequences: First, discovered eﬀect heterogeneity can be a spurious

byproduct of aggregation and thus falsely be attributed to unit background characteristics.

Second, actual eﬀect heterogeneity could be masked as a consequence of the aggregation.

Scenario 2 (multiple treatment versions): A binary treatment takes diﬀerent versions

after assignment, e.g. access to a training program with diﬀerent specializations. Here,

eﬀect heterogeneity could result from better version targeting and not from diﬀerent

eﬀectiveness of the versions themselves. Understanding this diﬀerence is crucial for policy

1See also Supplementary Appendix B.1 for a motivating toy example.

2

makers to assess the quality of the version assignment mechanism.

In this paper we propose a novel method for decomposing eﬀect heterogeneity in

a more general scenario with observed confounders. We decompose canonical eﬀect

heterogeneity into new estimands that are representative of (i) heterogeneous eﬀects and

(ii) heterogeneity stemming from diﬀerent underlying treatment compositions. These

decomposition parameters serve as summary measures to evaluate the consequences of

(dis)aggregating treatment variables for the causal analysis. Furthermore they provide a

simple framework for comparing the quality of treatment version assignment rules and

their heterogeneity across units or groups.

We develop a simple but ﬂexible nonparametric method for estimation and statistical

inference for the decomposition parameters. Our framework allows for the use of machine

learning techniques such as random forests, deep neural networks, high-dimensional sparse

regression models in the estimation of the nuisance parameters. We provide explicit

high-level conditions regarding the required rates for machine learners, their interaction

with the nonparametric decomposition step, and the number of eﬀective treatments J. We

also provide suﬃcient conditions for explicit examples.

The decomposition can be used to conduct simple joint hypothesis tests for global or

conditional decomposition parameters that consider all eﬀective treatments simultaneously.

This allows to test necessary conditions for diﬀerent types of selection and eﬀect heterogene-

ity without the need for multiple testing procedures. It compares favorably to conventional

multi-valued treatment eﬀect analysis under many eﬀective treatments J → ∞, expanding

sets of nuisance parameters, and extreme propensity scores. In particular, regular inference

is still achievable even if propensity scores are arbitrarily close to zero (limited overlap).

This result is obtained by leveraging supereﬃciency properties of (conditional) probability

estimators.

The large sample theory also extends beyond the decomposition parameters considered

in this paper. In particular, we provide conditions under which unbiased signals with

machine learning inputs for causal parameters can be combined with weighting schemes

that are themselves estimated to obtain asymptotically valid analytical conﬁdence intervals.

Our Monte Carlo simulations suggest that the proposed intervals have coverage rates close

3

to the nominal level in ﬁnite samples.

We provide two applications of our decomposition method, one for each leading scenario:

First, we show that parts of the ﬁnding that the detrimental eﬀect of smoking on birth

weight is largest for white mothers can be explained by white mothers smoking more

heavily conditional on being smokers. Similarly, diﬀerent eﬀects for diﬀerent age groups

are partly due to teenage mothers smoking less intensively compared to older mothers.

Second, we investigate the lower eﬀectiveness of access to the Job Corps training program

for women compared to men. We ﬁnd evidence that the gender gap is largely explained by

the vocational training curriculum, which focuses more on lower paying service jobs for

women and more on higher paying craft jobs for men. Imposing the same mix of vocational

training as part of our decomposition removes 73% of the total gender diﬀerences in the

eﬀect on earnings.

The paper is structured as follows: Section 2 discusses the related literature. Section 3

outlines the decomposition of the causal eﬀect parameters and discusses their identiﬁcation.

Section 4 contains the estimation and inference method. Section 5 introduces the technical

assumptions and discusses the large sample properties. Section 6 provides the Monte

Carlo study. Section 7 contains the application. Section 8 concludes. We also provide an

implementation in R and replication notebooks.2

2 Related Literature

The proposed decomposition complements the literature that considers (dis)aggregated

binary treatments. Lechner (2002) discusses how to aggregate average eﬀects of multiple

treatments into composite treatment eﬀects. Hotz, Imbens, and Mortimer (2005) and

Hotz, Imbens, and Klerman (2006) discuss summarizing diﬀerent training components in

one binary indicator and emphasize the potential lack of external validity under latent

treatment heterogeneity. Similarly, a recent stream of papers formalizes structural causal

models and interpretations of compound treatments (Cole & Frangakis, 2009; VanderWeele,

2009; Hernán & VanderWeele, 2011; Petersen, 2011). VanderWeele and Hernan (2013)

2For Section 7.1 see mcknaus.github.io/assets/code/Replication_NB_smoking.nb.html and for Section
7.2 mcknaus.github.io/assets/code/Replication_NB_JC.nb.html on GitHub.

4

note that non-homogeneous treatments violate the second component of the “Stable Unit

Treatment Value Assumption” (SUTVA, Rubin, 1980): no-multiple-versions-of-treatment,

which requires a homogeneous treatment or at least the treatment variation irrelevance

assumption of VanderWeele (2009). VanderWeele and Hernan (2013) formalize a setting

where this assumption is violated and provide several new identiﬁcation results and

estimands. Aggregating heterogeneous treatments has also been discussed in the context

of instrumental variables (Angrist & Imbens, 1995; Marshall, 2016; Andresen & Huber,

2021), regression discontinuity designs (Cattaneo, Keele, Titiunik, & Vazquez-Bare, 2016),

and models with spillovers and interactions (Vazquez-Bare, 2022). These papers mostly

discuss the consequences of (dis)aggregation of treatments on unconditional estimands

and their connection to (weighted) causal eﬀects. Our paper focuses on the consequences

of (dis)aggregation on eﬀect heterogeneity.

The focus on eﬀect heterogeneity is motivated by the surging literature that develops

(e.g. Athey & Imbens, 2016; Wager & Athey, 2018; Athey, Tibshirani, & Wager, 2019;

Künzel, Sekhon, Bickel, & Yu, 2019; Knaus, Lechner, & Strittmatter, 2021; Nie & Wager,

2021) and applies (e.g. Davis & Heller, 2020; Knaus, Lechner, & Strittmatter, 2022; Buhl-

Wiggers, Kerwin, Muñoz, Smith, & Thornton, 2022) ﬂexible machine learning methods to

the estimation of heterogeneous causal eﬀects. We build on the double/debiased machine

learning framework by Chernozhukov et al. (2018). They use Neyman-orthogonal score

functions and sample splitting in conjunction with machine learning methods for estimation

of low-dimensional parameters that depend on nuisance quantities.

Regarding heterogeneity analysis, there is now a series of papers that obtain (functional)

parameters by localizing these score functions using (nonparametric) regression or machine

learning methods (Lee, Okui, & Whang, 2017; Zimmert & Lechner, 2019; Fan, Hsu, Lieli,

& Zhang, 2022; Semenova & Chernozhukov, 2021; Kennedy, 2020; Curth & van der

Schaar, 2021; Knaus, 2022; Heiler, 2022). Our theoretical contribution builds on the

structural function approach by Semenova and Chernozhukov (2021) with least squares

series estimation (Newey, 1997; Belloni, Chernozhukov, Chetverikov, & Kato, 2015;

Cattaneo, Farrell, & Feng, 2020). We extend some of the inferential results by Semenova

and Chernozhukov (2021) to settings where pseudo-outcomes are constructed as a weighted

5

average of Neyman-orthogonal scores with (estimated) weights and potentially many

treatments.

The paper is also related to the literature regarding inference on eﬀect parameters under

extreme propensity scores or “limited overlap” (Khan & Tamer, 2010; Rothe, 2017; Ma

& Wang, 2020; Hong, Leung, & Li, 2020; Heiler & Kazak, 2021). Limited overlap occurs

by construction when allowing for “many treatments” J → ∞. In this case, the set of

nuisance parameters is expanding and classic multi-valued treatment eﬀect parameters (e.g.

Cattaneo, 2010) are irregularly identiﬁed which complicates inference. The decomposition

method, however, always yields three aggregate (functional) parameters independently

of J. As a consequence, regular estimation and inference regarding heterogeneity is still

feasible as long as J does not grow too fast. In ﬁnite samples, determining what constitutes

a many treatments setup is diﬃcult as J is always a ﬁnite number and a small lower bound

for propensities are hard to distinguish from a zero lower bound (Rothe, 2017). Thus, a

method that is robust to a potentially large number of treatments provides safeguard for

empirical practice.

Many studies document the detrimental eﬀect of smoking during pregnancy on birth

weight (e.g. Almond, Chay, & Lee, 2005; Abrevaya, 2006; Cattaneo, 2010; Almond &

Currie, 2011). Our methodology allows us to understand how much of the heterogeneous

eﬀects of this binarized treatment is spuriously attributed to subgroup characteristics

instead of to diﬀerent intensities of smoking across these subgroups.

Previous studies evaluate the US training program Job Corps from diﬀerent angles

based on a large scale experiment (e.g. Schochet, Burghardt, & Glazerman, 2001; Schochet,

Burghardt, & McConnell, 2008; Flores, Flores-Lagunes, Gonzalez, & Neumann, 2012;

Eren & Ozbelik, 2014; Strittmatter, 2019). While most of them document heterogeneous

eﬀects, we explicitly disentangle how much of the eﬀects and their heterogeneity is driven

by selection into diﬀerent curricula. This provides a complementary perspective on the

quality of the existing assignment mechanism.

6

3 Decomposition and Identiﬁcation

3.1 The Setting

Assume we observe independent data (Yi, Di, Ti, Xi) for i = 1, . . . , n. Yi denotes the

outcome of interest, Di ∈ {0, 1} is the analyzed binary indicator, Ti ∈ T = {0, 1, . . . , J}

indicates the eﬀective treatment3, and Xi contains confounding variables. We consider

settings that are characterized by two features: (i) Not Di, but the eﬀective treatment Ti has

a direct inﬂuence on the outcome creating potential outcomes Yi(t) for each t ∈ T . Thus,
we assume SUTVA with respect to the eﬀective treatment such that Yi = (cid:80)

1(Ti = t)Yi(t).

t

(ii) Conditional on Ti, the binary indicator Di is deterministic, i.e. it perfectly separates

the support T . We use directed acyclic graphs (DAGs) (see e.g. Pearl, 1995) to outline

our main scenarios in this setting.

Figure 1: Analyzed indicator is ex-post aggregate of confounded multiple treatment:

Binarized Treatment Indicator D

Treatment T

Outcome Y

Confounders X

Figure 1 outlines the causal structure of Scenario 1 where the binary indicator variable

Di is the result of an ex-post aggregation and not structurally related to the outcome. In

practice, this aggregation is often conducted after the outcome realizes, which makes it

unlikely for Di to aﬀect Yi directly. This is indicated by a missing arrow from Di to Yi.

However, as Ti is ancestor of both binarized indicator Di and outcome Yi, they are not

statistically independent of each other even conditional on Xi. For example, a statistical

relationship between birth weight (Yi) and smoking (Di) as an aggregate of the consumed

dose of cigarettes (Ti) can be derived from observational data. Conditional on the number

of cigarettes, however, smoking is deterministic and no association remains.

The DAG in Figure 2 depicts the causal structure of Scenario 2 where a randomized

binary treatment Di precedes the confounded allocation of treatment versions Ti. Here,

3In the following the term “treatment” refers to eﬀective treatment if not stated diﬀerently.

7

Figure 2: Randomized binary treatment precedes confounded treatment versions:

Binary Treatment D

Treatment Version T

Outcome Y

Confounders X

Di is not an ex-post variable with regards to Yi. Yi and Di are associated as the latter

determines which treatment versions are available, but has no direct eﬀect beyond that. Its

eﬀect is completely mediated through the treatment versions Ti. For example, this implies

that any association between Di being access to a training program (yes/no) on earnings

Yi would disappear if we would condition on all training types including no training access

(Ti).

It is important to note that, while conceptually diﬀerent in terms of the causal

interpretation, the DAGs in Figures 1 and 2 imply the same conditional independence

relationships regarding Di, Ti, and potential outcomes Yi(t). In standard Neyman-Rubin

notation for multi-valued treatments (Rubin, 1974; Imbens, 2000; Lechner, 2001; Cattaneo,

2010), we have that

Yi(0), Yi(1) . . . , Yi(J) ⊥⊥ Di | Ti

Yi(0), Yi(1) . . . , Yi(J) ⊥⊥ Ti | Xi

(1)

(2)

where (1) is a consequence of our setting that Di is a constant given Ti and (2) follows

from the causal structure encoded in the DAG (see Appendix B.2.1). Thus, from a

statistical point of view, we treat both scenarios as being equivalent in the following. Note

that conditions (1) and (2) and everything that follows can be extended to apply to causal

graphs where additional observed confounders can aﬀect Di, Ti, and Yi simultaneously

(see Appendix B.2.2).

We use Dt,i = 1(Ti = t) to indicate that unit i is observed in treatment t and let

et(x) = P (Dt,i = 1|Xi = x) denote the corresponding propensity scores. Without loss of

generality, we assume throughout that Ti = 0 denotes a homogeneous control condition.
Thus, the binary indicator is deﬁned as Di = (cid:80)

t(cid:54)=0 Dt,i and D0,i = 1 − Di in what follows.

8

3.2 Heterogeneous eﬀects if treatment heterogeneity is ignored

We are interested in the case where the causal structure is accurately described by Section

3.1, but the analyst considers only the binary indicator Di, which is deterministic in

Ti. Then, the main quantities of interest are often conditional average treatment eﬀects

(CAT E) τ (x) or aggregations thereof like the average treatment eﬀect (AT E = E[τ (Xi)]).

However, the potential outcome under the binary indicator being one is not uniquely

deﬁned in our setting unless J = 1. The question is then, what does the quantity

τ (x) = E[Yi|Di = 1, Xi = x] − E[Yi|Di = 0, Xi = x] commonly deployed under strong

ignorability assumptions for Di (Imbens & Rubin, 2015) actually identify? Given the

setting outlined in Section 3.1, we can backwards engineer the actually identiﬁed estimand

in terms of potential outcomes of the eﬀective treatment:

τ (x) = E[Yi|Di = 1, Xi = x] − E[Yi|Di = 0, Xi = x]

=

=

=

=

(cid:88)

t(cid:54)=0
(cid:88)

t(cid:54)=0

(cid:88)

t(cid:54)=0

(cid:88)

t(cid:54)=0

E [Dt,iYi(t)|Di = 1, Xi = x] − E[Yi(0)|Xi = x]

E[Yi(t)|Dt,i = 1, Di = 1, Xi = x]P (Dt,i = 1|Xi, Di = 1) − E[Yi(0)|Xi = x]

E[Yi(t)|Dt,i = 1, Xi = x]

(cid:80)

− E[Yi(0)|Xi]

et(x)
t(cid:54)=0 et(x)
et(x)
t(cid:54)=0 et(x)

E[Yi(t) − Yi(0)|Xi = x]
(cid:124)
(cid:125)
(cid:123)(cid:122)
t−speciﬁc CATE

(cid:80)

(cid:88)

+

t(cid:54)=0

{E[Yi(t)|Dt,i = 1, Xi = x] − E[Yi(t)|Xi = x]}
(cid:125)
(cid:123)(cid:122)
(cid:124)
selection eﬀect

et(x)
t(cid:54)=0 et(x)

(cid:80)

(3)

Equation (3) shows that the estimand consists of two components: First, a weighted

average of CAT Es of the eﬀective treatments, τt(x) = E[Yi(t) − Yi(0)|Xi = x], with

weights depending on the conditional probability of being treated in the respective eﬀective

treatment. Second, a weighted average of eﬀective treatment speciﬁc selection eﬀects. The

selection eﬀects are positive if those with characteristics x who are actually observed in

treatment t show higher potential outcomes than the general population described by x, or

negative if vice versa. This term is relevant if there is selection into the eﬀective treatment

even after conditioning on the observed confounders. This can e.g. occur in the case of a

9

randomized binary treatment in Scenario 2 where the selected Xi might not include all

confounders for the treatment versions.

The decomposition in (3) highlights that the interpretation of the underlying estimand

becomes more nuanced in the presence of heterogeneous treatments. What is supposed to

be an easily interpretable CAT E depends now on the potentially unknown distribution of

eﬀective treatments and selection into those treatments. Thus, without further assumptions,

heterogeneous eﬀects attributed to the binary indicator can be driven by diﬀerent CAT Es,

diﬀerent compositions of the eﬀective treatments, diﬀerent selection eﬀects of the eﬀective

treatments, or combinations thereof.

To be able to meaningfully decompose estimand (3) below, we impose a strong ignora-

bility assumption at the eﬀective treatment level:

Assumption 1 (strong ignorability of eﬀective treatment)

(a) Unconfoundedness: Yi(t) ⊥⊥ Dt,i|Xi = x, ∀ t ∈ T and x ∈ X .

(b) Common support: 0 < P [Dt,i = 1|Xi = x] ≡ et(x), ∀ t ∈ T and x ∈ X .

Assumption 1 is a standard assumption in the multiple treatments setting (Imbens,

2000; Lechner, 2001). It imposes that (a) the set of conditioning variables is rich enough

such that after conditioning all residual variation in potential outcomes is independent

of the allocated eﬀective treatment and (b) there are comparable units across eﬀective

treatments in terms of their confounders. Under this assumption, the selection eﬀects in

(3) disappear and the underlying estimand becomes

τ (x) =

(cid:88)

t(cid:54)=0

et(x)
t(cid:54)=0 et(x)

(cid:80)

τt(x) ≡ nAT E(x).

(4)

We call this estimand the natural conditional average treatment eﬀect nAT E(x) because

it is the result of the actual or "natural" eﬀective treatment composition. It is important to

note that, even under Assumption 1, the diﬀerences between units characterized by x and

x(cid:48) can result from diﬀerent treatment shares, diﬀerent treatment CAT Es, or both. We thus

could detect seemingly heterogeneous eﬀects, even if the treatment CAT Es are constant

within treatments but not homogeneous between treatments, i.e. τt(x) = τt ∀ t ∈ T , x ∈ X

but τt (cid:54)= const. ∀ t ∈ T , as long as the probabilities to be observed in the diﬀerent

10

eﬀective treatments are heterogeneous. In this case any diﬀerence is driven by treatment

heterogeneity:

nAT E(x) − nAT E(x(cid:48)) =

(cid:34)

(cid:88)

t(cid:54)=0

et(x)
t(cid:54)=0 et(x)

(cid:80)

−

(cid:80)

et(x(cid:48))
t(cid:54)=0 et(x(cid:48))

(cid:35)

τt

(5)

This fundamentally aﬀects the interpretation of heterogeneous eﬀects even if the

underlying eﬀective treatments are not observable. If they are observable, however, we

can further decompose heterogeneous eﬀects of the binary indicator in what follows.

3.3 The Decomposition

In this section we demonstrate how to disentangle actual eﬀect heterogeneity and het-

erogeneity driven by selection into eﬀective treatments. We propose to decompose the

nAT E(x) in two parts:

(cid:88)

t(cid:54)=0
(cid:124)

(cid:80)

et(x)
t(cid:54)=0 et(x)
(cid:123)(cid:122)
nAT E(x)

τt(x)

=

(cid:125)

(cid:88)

t(cid:54)=0
(cid:124)

(cid:80)

πt
t(cid:54)=0 πt
(cid:123)(cid:122)
rAT E(x)

τt(x)

+

(cid:125)

(cid:32)

(cid:88)

t(cid:54)=0
(cid:124)

et(x)
t(cid:54)=0 et(x)

(cid:80)

−

(cid:80)

πt
t(cid:54)=0 πt

(cid:33)

τt(x)

(6)

(cid:123)(cid:122)
∆(x)

(cid:125)

where πt = E[Dt,i] are the unconditional treatment probabilities. The ﬁrst component

on the right hand side ﬁxes the composition of the eﬀective treatments at the population

value. It resembles a situation where eﬀective treatments are randomly allocated using

the population level selection probabilities. Thus, we refer to it as the random conditional

average treatment eﬀect rAT E(x). All heterogeneity in rAT E(x) is driven by “real" eﬀect

heterogeneity within treatments, τt(x) (cid:54)= τt(x(cid:48)) for some x, x(cid:48) ∈ X , as the underlying

treatment composition is held ﬁxed. In other words, diﬀerences in rAT E(x) describe

eﬀect heterogeneity compositionis paribus. Thus, we can exploit potential heterogeneity in

rAT E(x) to test necessary conditions for classic (or “within”) eﬀect heterogeneity.

The second component of the decomposition ∆(x) is the part of nAT E(x) stemming

from the interaction of non-constant eﬀective treatment probabilities and diﬀerent eﬀective

treatments having diﬀerent eﬀects (“between” treatment eﬀect heterogeneity). Thus,

the decomposition is redundant, i.e. ∆(x) = 0 ∀ x ∈ X , under (i) eﬀective treatment

11

composition homogeneity

(cid:80)

et(x)

t(cid:54)=0 et(x) − πt

(cid:80)

t(cid:54)=0 πt

= 0 ∀ t ∈ T and x ∈ X , (ii) treatment variation

irrelevance E[Yi(t)|Xi = x] = E[Yi(t(cid:48))|Xi = x] ∀ x ∈ X , t, t(cid:48) ∈ T of VanderWeele (2009)4,

or (iii) if positive and negative components net out to zero. Hence, ∆(x) (cid:54)= 0 is a necessary

condition for unequal treatment probabilities and between treatment eﬀect heterogeneity.

Furthermore, heterogeneity in ∆(x) is a necessary condition for heterogeneous assignment

probabilities, within treatment eﬀect heterogeneity, or both. Thus, the decomposition

can be used to address a variety of relevant policy questions, see also Remark 2 below.

Moreover, the focus on such necessary conditions oﬀers statistical advantages over testing

similar conditions in the standard multi-valued treatment eﬀect setup when there are

many eﬀective treatments. We return to this point in Remark 3 below and in Section 5.

Under Assumption 1, the conditional average potential outcome of treatment t is

identiﬁed as µt(Xi) ≡ E[Yi(t)|Xi] = E[Yi(t)|Dt,i = 1, Xi] = E[Yi|Dt,i = 1, Xi] and

accordingly the decomposition terms are identiﬁed as:

(µt(x) − µ0(x))

(µt(x) − µ0(x))

et(x)
t(cid:54)=0 et(x)
πt
t(cid:54)=0 πt

nAT E(x) =

rAT E(x) =

∆(x) =

(cid:88)

t(cid:54)=0
(cid:88)

t(cid:54)=0

(cid:88)

t(cid:54)=0

(cid:80)

(cid:80)

(cid:32)

et(x)
t(cid:54)=0 et(x)

(cid:80)

(cid:33)

−

πt
t(cid:54)=0 πt

(cid:80)

(µt(x) − µ0(x))

(7)

Aggregations or projections of the three estimands are thus also identiﬁed. In particular,

let Zi denote a (low dimensional) subset of confounders supported on Z ⊂ X and deﬁne

nAT E(z) = E[nAT E(Xi)|Zi = z]

rAT E(z) = E[rAT E(Xi)|Zi = z]

∆(z) = E[∆(Xi)|Zi = z].

(8)

These parameters provide more concise, predictive summaries of heterogeneity or allocation

diﬀerences for speciﬁc subgroups deﬁned by Zi = z. The unconditional decomposition

4In this case τt(x) = τ (x) and consequently ∆(x) = τ (x)

(cid:32)

(cid:88)

t(cid:54)=0
(cid:124)

(cid:80)

et(x)
t(cid:54)=0 et(x)
(cid:123)(cid:122)
=0

−

πt
t(cid:54)=0 πt

(cid:80)

(cid:33)

(cid:125)

= 0, ∀ x ∈ X .

12

terms nAT E = E[nAT E(Xi)], rAT E = E[rAT E(Xi)], and ∆ = E[∆(Xi)] are special

cases thereof. We propose an estimation and inference method for these parameters in

Section 4.

Remark 1 : In principle, an analogous decomposition could also be constructed with

alternative weights for the eﬀective treatments, e.g. equal weighting 1/J. However, using

the unconditional eﬀective treatment probabilities ensures that nAT E(x) = rAT E(x) in

the case of completely randomized eﬀective treatments.

Remark 2 : The interpretation of ∆(x) depends on the scenario:

• Scenario 1: ∆(x) and its aggregates have a descriptive interpretation. It describes

how much of nAT E(x) is driven by an underlying eﬀective treatment mix that devi-

ates from the population mix. A non-constant ∆(x) indicates that the binarization

has consequences for the detected heterogeneous eﬀects. Thus, it helps to understand

heterogeneity resulting from the binarization.

• Scenario 2: ∆(x) and its aggregates provide information for assignment evaluation.

Positive values indicate that the assignment of treatment versions is better than

random. Negative values indicate worse than random version assignment assuming

that individuals act equivalently under the hypothetical random assignment compared

to the observational assignment (Heckman, 2020). A non-constant ∆(x) indicates

that the selection quality of versions varies across diﬀerent groups. Thus, the

estimand provides an evaluation of the actual assignment mechanism.

Remark 3: In principle, one could be tempted to test eﬀect heterogeneity within the

classic multi-valued treatment framework by comparing up to J(J − 1)/2 conditional

average treatment eﬀects. This requires correction for multiple testing. More importantly,

however, the required strong overlap assumption for valid regular inference is increasingly

diﬃcult to justify when J is large. Our decomposition approach, on the other hand, is

robust to too many treatments and limited overlap. In particular, even when J → ∞,

strong overlap assumptions for accurate inference based on asymptotic normality are not

required. This comes at the cost of testing somewhat weaker conditions regarding eﬀect

heterogeneity: In the multi-valued treatment eﬀect setting we can test suﬃcient conditions

13

for eﬀect heterogeneity between and within all treatments, while the decomposition allows

for testing necessary conditions for within and between treatment eﬀect heterogeneity.

Remark 4 : The comparison of nAT E(x) and rAT E(x) shows resemblance to the

relationship between the ATE and the average treatment eﬀect on the treated (ATET ) in

the canonical setting with a homogeneous binary treatment. The AT ET gives the average

eﬀect of those treated under the actual treatment assignment, while the AT E gives the

average eﬀect under the hypothetical random assignment of treatment.

Remark 5: The unconditional rAT E is a special case of the composite treatment eﬀects

described in Lechner (2002). Furthermore, if we allow for J → ∞, it can approximate

the integrated dose-response function of a continuous treatment as deﬁned in Kennedy,

Ma, McHugh, and Small (2017), see Section B.3 for more details. The unconditional ∆ is

also similar in spirit to the Population Average Prescriptive Eﬀect deﬁned by Imai and Li

(2021) in the context of policy learning.

4 Estimation and Inference

In this section, we outline a ﬂexible estimation approach for the (conditional) decomposition

terms and propose a method for conducting valid statistical inference. The method

accommodates the use of modern machine learning and other non- or semiparametric

methods in the estimation of the required nuisance parameters.

We propose to approximate the conditional expectations of the decomposition terms

g(z) by a linear combination of transformations b(z) of heterogeneity variables z, i.e.

g(z) = b(z)(cid:48)β0 + rg(z)

(9)

where β0 is the parameter vector of the best linear predictor given as solution to equation

E[b(Zi)(g(Zi) − b(Zi)(cid:48)β0)] = 0. rg(z) is the approximation error and b(z) can be basis

transformations of the regressors of interest such as polynomials, splines, wavelets, or

other functions. The number of components in b(·) is allowed to grow with the sample

size which allows us to be agnostic about the shape of the true g-function.

Let in the following η = η(x) = (µ0(x), . . . , µJ (x), e0(x), . . . , eJ (x))(cid:48) denote the vector

14

of nuisance quantities and write η = ηi = η(Xi) with subscript and argument suppressed

whenever it does not cause confusion. Also deﬁne the unconditional selection probability

vector π = (π0, . . . , πJ ).

Table 1: Score Functions of the Decomposition Parameters

Parameter Score function ψi(η, π) = ψ[P arameter]

i

(η, π)

nAT E

Ψi(η) − ψ[0]

i (η)

rAT E

(cid:80)

t(cid:54)=0 πtψ[t]
(cid:80)
t(cid:54)=0 πt

i (η)

− ψ[0]

i (η)

∆

Ψi(η) −

(cid:80)

t(cid:54)=0 πtψ[t]
(cid:80)
t(cid:54)=0 πt

i (η)

The scores ψ[t]
respectively.

i (η) and Ψi(η) are deﬁned in equations (10) and (11),

We follow the general idea of Semenova and Chernozhukov (2021) to construct “Neyman-

orthogonal” scores ψi(η, π) such that g(z) = E[ψi(η, π)|Zi = z]. These scores are deﬁned by

having an (approximate) zero Gateaux derivative with respect to the underlying nuisance

parameters at the true parameter vector (Chernozhukov et al., 2018). The robust scores

for the three decomposition parameters considered here are weighted combinations of the

well-known Neyman-orthogonal scores for average potential outcomes (Robins & Rotnitzky,

1995), also known as augmented inverse probability weighting (AIPW) scores:

ψ[t]

i (η) = µt(Xi) +

Dt,i(Yi − µt(Xi))
et(Xi)

Ψi(η) = E[Yi|Di = 1, Xi] +

Di(Yi − E[Yi|Di = 1, Xi])
P (Di = 1|Xi)
(cid:20)
(cid:80)

Di

Yi −

t(cid:54)=0 µt(Xi)et(Xi)
(cid:80)
t(cid:54)=0 et(Xi)

(cid:80)

t(cid:54)=0 et(Xi)

(cid:80)

=

t(cid:54)=0 µt(Xi)et(Xi)
(cid:80)
t(cid:54)=0 et(Xi)

+

(10)

(11)

(cid:21)

where ψ[t]

i (η) is the score of the treatment t speciﬁc average potential outcome and
Ψi(η) is the score for the group described by the binary indicator. Table 1 shows how to

combine these scores to form unbiased signals of the decomposition parameters. These

combinations retain Neyman-orthogonality with respect to η, see Appendix B.4, but

inference has to be adjusted for uncertainty in the estimation of π, see Section 5.

15

Consider now the regression of the score functions onto the space spanned by the

k-dimensional transformation of Zi, b(Zi). This yields the estimator

(cid:18) n

(cid:88)

ˆβ =

i=1

b(Zi)b(Zi)(cid:48)

(cid:19)−1 n

(cid:88)

i=1

b(Zi)ψi(ˆη, ˆπ)

(12)

where the score of a decomposition term with estimated nuisance quantities ψi(ˆη, ˆπ)

serves as pseudo-outcome in the corresponding least squares regression on b(Zi). For
ˆπ we use simple sample averages, i.e. ˆπt = n−1 (cid:80)n

i=1 Dt,i. Estimation of ˆη can be done
via modern machine learning methods such as random forests, deep neural networks,

high-dimensional sparse likelihood and regression models or other non- and semiparametric

estimation methods with good approximation qualities for the functions at hand. For

details regarding the technical assumptions, consider Section 5. We require that all

components in ˆη are obtained via K-fold cross-ﬁtting:

Deﬁnition 4.1 K-fold cross-ﬁtting (see Deﬁnition 3.1 in Chernozhukov et al. (2018))

Take a K-fold random partition (If )K

f =1 of observation indices [K] = {1, . . . , n} with each

fold size nf = n/K. For each f ∈ [K] = {1, . . . , K}, deﬁne I c

f := {1, . . . , n}\If . Then for

each f ∈ [K], the machine learning estimator of the nuisance function are given by

ˆηf = ˆη((Yi, Xi, Ti)i∈I c

f

).

Thus for any observation i ∈ If the estimated score only uses the model for η learned from

the complementary folds ψi(ˆη, ˆπ) = ψi(ˆηf , ˆπ).

Cross-ﬁtting controls the potential bias arising from overﬁtting using ﬂexible machine

learning methods without the need to evaluate entropy conditions for the function class

that contains true and estimated nuisance quantities. If ﬁnite parametric models such as

linear or logistic are assumed for the nuisance quantities, the proposed methodology can

be applied without the need for cross-ﬁtting.

Under suitable assumptions, the predictions using estimator (12) are consistent for

g(z). Moreover, it is possible to conduct asymptotically valid inference around the best

linear predictor, i.e. for any z0 = z0,n we can construct (1 − α)% conﬁdence intervals for

16

the true decomposition function as

CI1−α(g(z0)) =

(cid:20)

b(z0)(cid:48) ˆβ ± q1−α/2

(cid:115)

(cid:21)

b(z0)(cid:48) ˆΩb(z0)
n

(13)

where q1−α/2 denotes the (1 − α/2)-quantile of the standard normal distribution and ˆΩ is a

consistent sample estimator of the asymptotic variance Ω (see Section 5 and Appendix B.5).

The estimator explicitly takes into account the additional uncertainty from estimating

the unconditional treatment probabilities in the decomposition terms. The interval in

(13) is also valid for the best linear predictor b(z0)(cid:48)β0 under moderate misspeciﬁcation if

the approximation error is not too large. It provides asymptotically accurate conﬁdence

intervals around the true g-function if the approximation error vanishes at a suitable rate

as the number of basis functions or transformations increases. For the technical details

consider Section 5.

5 Large Sample Properties

5.1 Assumptions and Main Results

In this section, we present and discuss the large sample properties of the proposed

decomposition method. First, we introduce the relevant deﬁnitions. We then discuss the

assumptions required for (i) all decomposition parameters, (ii) nAT E, and (iii) rAT E/∆

speciﬁcally and their connections to the literature. We contrast (ii) and (iii) as the nAT E

tends to require less restrictive conditions compared to rAT E/∆. We then present the

main Theorem and outline potential extensions.

In the following, quantities like ψi() or rg() are used in their generic sense, i.e. for a

given choice of decomposition parameter nAT E, rAT E or ∆. a (cid:46) b means a/b = O(1) and

a (cid:46)P b means a/b = Op(1). For a general matrix M denote its largest (smallest) eigenvalue

by λmax(M ) (λmin(M )). The realization sets of the estimated nuisance quantities are given

by Hn = En × Mn with En = E0,n × E1,n × · · · × EJ,n and Mn = M0,n × M1,n × · · · × MJ,n,

where Et,n and Mt,n are the realization sets for et and µt respectively. For estimators

17

ˆet(Xi) and ˆµt(Xi) deﬁne the Lq error rates

st,n,q = sup
ˆet∈Et,n

E[(ˆet(Xi) − et(Xi))q]1/q, mt,n,q = sup
ˆµt∈Mt,n

E[(ˆµt(Xi) − µt(Xi))q]1/q

and the slowest Lq rates over all treatments sn,q = supt(cid:54)=0 st,n,q and mn,q = supt(cid:54)=0 mt,n,q.
Note that ψ[t,0]
i (η). By deﬁnition εi = ψi(η, π) − E[ψi(η, π)|Zi] where
ψi(η, π) corresponds to the moment function of the decomposition parameter of choice

i (η) − ψ[0]

(η) = ψ[t]

i

from Table 1. Denote g(z) = E[ψi(η, π)|Zi = z] where g ∈ G with G = Gn being a function

class potentially depending on n. Thus, g(z) = b(z)(cid:48)β0 + rg(z) where β0 is the parameter

of the best linear predictor deﬁned as the root of equation E[bi(g(Zi) − b(cid:48)

iβ0)] = 0 where
bi = b(Zi). Also deﬁne the potential outcome mean error εi(t) = Yi(t) − E[Yi(t)|Xi]

and its conditional variance σ2

t (Xi) = E[εi(t)2|Xi]. For the nAT E machine learning bias

components, we deﬁne

B[nAT E]

n

:=

√

n sup
ˆη∈Hn

||E[bi(ψ[nAT E]
i

(ˆη, π) − ψ[nAT E]

i

(η, π))]||

Λ[nAT E]

n

:= sup
ˆη∈Hn

(E[||bi(ψ[nAT E]

i

(ˆη, π) − ψ[nAT E]

i

(η, π))||2])1/2

and equivalently for rAT E/∆ with moment functions according to Table 1. Remainder

terms Rn are deﬁned in Appendix A. Let γt = E[biψ[t,0]

i

(η)] = E[biτt(Xi)], γ = (γ1 . . . γJ ),

and deﬁne ai = (a[1]
i

. . . a[J]

i )(cid:48) with a[t]

i = (1 − π0)−2(Dt,i(1 − π0) + D0,iπt − πt). Now let

Q = E[bib(cid:48)

i] and deﬁne

Ω = Q−1E[(bi(εi + ri) + γai)(bi(εi + ri) + γai)(cid:48)]Q−1

Ω1 = Q−1E[bib(cid:48)

i(εi + ri)2]Q−1

Ω2 = Q−1E[γaia(cid:48)

iγ(cid:48)]Q−1.

We now present the assumptions required for all decomposition parameters. They are

meant to hold uniformly over n if not stated otherwise:

Decomposition Assumptions:

A.1) (Identiﬁcation) Q has eigenvalues bounded above and away from zero.

18

A.2) (Conditional means) The potential outcomes have bounded conditional means

sup
t

sup
x∈X

µt(x) (cid:46) 1

A.3) (Control overlap and limited treatment overlap) The control propensities are bounded

away from zero, i.e. for some c ∈ (0, 1/2)

c < inf
x∈X

e0(x) ≤ sup
x∈X

e0(x) < 1 − c

and the re-scaled inverse treatment propensity scores are proportional to the number
of diﬀerent treatments

sup
t(cid:54)=0

sup
x∈X

πt
et(x)

(cid:46) 1 and Jπt (cid:46) 1

for all t = 1, . . . , J with J = o(n).

A.4) (Bounded relative prediction error) On the realization set with probability 1 − un, the
worst relative prediction error for the cross-ﬁtted treatment propensities are bounded

sup
t(cid:54)=0

sup
ˆet∈Et,n

sup
x∈X

et(x)
ˆet(x)

(cid:46) 1

A.1 rules out multicollinearity of the basis functions used for the nonparametric

heterogeneity analysis in the last stage. A.2 is a mild heterogeneity restriction on the

potential outcomes. A.3 is crucial: It is concerned with the degree of overlap for a

general number of treatments J = Jn. In particular, it assumes that there is strong

overlap for the control group and the aggregate treatment, i.e. control and aggregate

treatment propensities are uniformly bounded away from zero. However, the propensities

for treatments t = 1, . . . , J are allowed to be arbitrarily close to zero as long as they

vanish at most at a rate proportional to their respective unconditional treatment selection

probability πt. This allows for limited overlap at each treatment level which is necessary

when their number is allowed to increase with the sample size, i.e. J → ∞. We suggest to

assess Assumption A.3 empirically by analyzing the (estimated) distribution of et(x)/πt

for all t: If these re-scaled scores have suﬃcient density bounded away from zero by the

same standard used to assess conventional propensity score distributions (Heiler & Kazak,

2021), then the assumption is likely to hold, see Appendix B.8 and B.9 for examples

based on the empirical applications from Section 7. Assuming a homogeneous 1/J-rate

for all πt is without loss of generality: If the product converges to zero for some t, it

vanishes from relevant ﬁrst-order approximations and estimation properties are eventually

19

determined by the treatments that obey Assumption A.3. Moreover, if A.3 only applies

to a smaller ﬁnite subset of treatments, it eﬀectively corresponds to strong overlap for

these particular t and thus estimators behave analogously to standard AIPW for a control

potential outcome. Note that the growth of J is restricted to rate o(n) such that consistent

estimation of unconditional multi-valued treatment eﬀects is still possible, albeit at a

slower rate compared to the strong overlap case similar to Hong et al. (2020). If there are

many control conditions aggregated into Di = 0, then strong overlap for controls could

also be relaxed analogously to the re-scaled treatment propensities.

A.4 says that the worst relative prediction for the cross-ﬁtted propensities is bounded

on the realization set. This is a non-standard assumption, in particular when J → ∞.

It is likely to hold for frequency based methods, i.e. estimators that use some form of

(weighted) average within the cells deﬁned by Dt,i for t = 0, . . . , J to construct propensities

including advanced machine learning methods. A suﬃcient, but by no means necessary,

condition is uniform consistency of ˆe(x) over X at rate o(et(x)−1). This can be shown

to hold for single-index models (Ma, Sasaki, & Wang, 2022) and nonparametric kernel

regression (Heiler & Taylor, 2022) under weak conditions. The key point is that these

estimators inherit a local supereﬃciency property from ˆπt, i.e. faster convergence rate

|ˆπt − πt| (cid:46)P (nJ)−1 in regimes with many treatments/vanishing unconditional selection

probabilities. A.4 then requires the estimators to have a consistency rate increased by a

factor of

√

J compared to the ﬁnite J case. For parametric estimators this holds as long as

J = o(n) while for kernel regression, for example, under the usual smoothness assumptions
with X of dimension d and a bandwidth h, it requires that (cid:112)log(n)nhd/J = o(1) (Heiler

& Taylor, 2022). We provide some more intuition about Assumption A.4 and the links

between large J and supereﬃcient nuisance parameter estimation in Section 5.2.

We now present the assumptions required for nAT E followed by rAT E/∆:

nAT E Assumptions:

For some m > 2 , we have that:

B.1) (Conditional Moments) The potential outcomes have at least m conditional moments

for the treated: supt supz∈Z E[εi(t)m|Zi = z, Dt,i = 1] (cid:46) 1.

B.2) (Approximation) For each n and k, there are ﬁnite constants ck and lk such that for

20

each g ∈ G

(cid:115)(cid:90)

||rg||P,2 :=

r2
g(z)dP (z) ≤ ck,

z∈Z
|rg(z)| ≤ lkck.

||rg||P,∞ := sup
z∈Z

B.3) (Machine Learning Bias) For some h1, h2 > 0 with 1/h1 + 1/h2 = 1 we have that

√

B[nAT E]

n

(cid:46)

(cid:18)

(cid:19)

nks0,n,2h2

Jsn,2h1 + mn,2h1

= o(1),

Λ[nAT E]

n

(cid:46) ξk

(cid:18)

s0,n,2 +

√

Jsn,2 + J −1mn,2

(cid:19)

= o(1).

B.4) (Basis and Linearization Error) The k basis functions are chosen such that

||Rn,Q|| (cid:46)P

(cid:114)

ξ2
k log k
n

(cid:18)

1 + k1/2lkck

(cid:19)

= o(1).

B.5) (Basis and Lindeberg Condition) Let

√

n/ξk − lkck → ∞ such that

J

√

[

n/ξk − lkck]2

+

(cid:18) (lkck) 2

m J 1
n/ξk − lkck]

√

m

[

(cid:19)m

= o(1).

B.1 imposes some regularity on the tails of the conditional potential outcomes. B.2

deﬁnes the L2 and uniform approximation rates using the basis functions for function

class G. If the basis is suﬃciently rich to span G, we say it is correctly speciﬁed and

ck → 0 as k → ∞. However, our results allow for the case of misspeciﬁcation, i.e. ck (cid:54)→ 0.

This is a standard characterization in the literature on nonparametric series methods, see

e.g. Belloni et al. (2015) for more details and examples.

B.3 is crucial: It requires high-quality approximation capabilities of the ﬁrst-stage

machine learning methods for the nuisance quantities. In the case of a ﬁnite-dimensional,

bounded basis supz∈Z ||b(z)||∞ < C and ﬁnite J, the conditions can be simpliﬁed to
√
nks0,n,2(sn,2 + mn,2) = o(1). This means that the products of the nuisance quantities for

the conditional control propensity and treatment propensities/potential outcome means

have to converge at least at rate o((nk)−1/2) identical to conditional ATE estimation in

Semenova and Chernozhukov (2021). This ﬂexible rate requirement is a consequence of the

Neyman-orthogonality of the moment function with regards to the nuisance parameters

η. In the simple case of a ﬁxed basis (e.g. a constant univariate basis as in unconditional

21

binary ATE estimation) it reduces to the well-known requirement in the double/de-

biased machine learning literature that the nuisance functions have to be of rate o(n−1/4)

(Chernozhukov et al., 2018). For many treatments J → ∞, ﬂexible k, and/or machine

learning estimators, the convergence requirements can be more demanding. We discuss

these cases and corresponding rate requirements in Section 5.2.

B.4 controls the approximation error from linearization of the estimator taking into

account the unknown design matrix Q of increasing dimension. The condition is equivalent

to the one required for linearization in conventional least squares series estimation (Belloni

et al., 2015). This suggests that, for more speciﬁc series methods such splines (Huang,

2003) and local partitioning estimators (Cattaneo et al., 2020), the rate can be improved

to (cid:112)ξ2

k log k/n(1 +

√

log klkck), see also Belloni et al. (2015), Section 4 and Cattaneo et

al. (2020), Remark SA-4 (page 12) of their supplemental appendix. Note that this rate

does not depend on J as, for linearization, the treatment dimension enters only through

estimation of the expanding set of nuisance parameters. Once the diﬀerence between true

and estimated nuisance parameters is controlled for via B.3, there is no diﬀerence to the

standard series estimation/binary ATE case with no or known nuisances.

B.5 controls the rate of the basis function relative to approximation error such that the

Lindeberg condition for asymptotic normality holds. Note that this rate is required to be

faster by a factor of J relative to conventional series estimation. This is due to the fact,

that the tails of the summands that determine the ﬁrst-order asymptotic distribution are

selected from a combination of J diﬀerent potential outcome errors εi(t) for t = 1, . . . , J.

Thus, the conditions for the many treatments case are somewhat stronger then the ones

expected for series estimation or (conditional) ATE estimation under a moment assumption

such as B.1 and ﬁnite J.

rAT E/∆ Assumptions

For some m > 2 , we have that:

C.1) (Conditional Moments) The potential outcomes have at least m conditional moments

for the selected: supt supz∈Z E[εi(t)m|Zi = z, Dt,i = 1] (cid:46) 1.

C.2) (Approximation) For each n and k, there are ﬁnite constants ck and lk such that for

22

each g ∈ G

(cid:115)(cid:90)

||rg||P,2 :=

r2
g(z)dP (z) ≤ ck,

z∈Z
|rg(z)| ≤ lkck.

||rg||P,∞ := sup
z∈Z

C.3) (Machine Learning Bias) For some h1, h2 > 0 with 1/h1 + 1/h2 = 1 we have that

n

B[rAT E]
Λ[rAT E]

n

√

nkJsn,2h1mn,2h2

(cid:46)
(cid:46) ξk(mn,2 + Jsn,2 + (cid:112)Jsn,2mn,2h1mn,2h2)

= o(1),

= o(1).

C.4) (Basis and Linearization Error) The k basis functions are chosen such that

√

||Rn,π|| (cid:46)P J
(cid:114)

||Rn,Q|| (cid:46)P

k(n−1/2 + mn,2 + Jsn,2)
ξ2
k log k
n

1 + k1/2(J 1/4 + lkck)

(cid:18)

(cid:19)

= o(1),

= o(1).

C.5) (Basis and Lindeberg Condition) Let n/kJ 2 → ∞,

√

n/ξk − lkck → ∞ such that

√

J 4
n/ξk − lkck]2

[

+

√

[

(cid:18) (lkck) 2

m J

n/ξk − lkck]

(cid:19)m

= o(1).

C.6) (Eigenvalues) λmin(Ω2) > 0 and λmax(Ω1)/λmin(Ω) (cid:46) 1.

We discuss and contrast Assumptions C.1–C.6 with B.1–B.5: C.1 and C.2 are equivalent

to B.1 and B.2 with potentially diﬀerent m, ck, and lk. C.3 controls for the estimation

of nuisance parameters. In the case of a bounded basis, the condition for Bn reduces to
√

nkJsn,2mn,2 = o(1) which is expected to be equivalent to the nAT E machine learning

bias rate when J is ﬁnite. However, in the large J case, estimating the potential outcome

means at rate mn,2 is generally slower than estimating the control propensities at s0,n,2. In
the parametric case, for example, we have that mn,2 = (cid:112)J/n =
An equivalent argument holds for Λn also leading to an additional

Js0,n,2, see Section 5.2.

J factor compared

√

√

to the nAT E case (here the third term in Λn is dominated by the ﬁrst two and can

be ignored). Thus, the product rates have to be faster by a factor of

√

J in this case,

i.e. rAT E/∆ generally require somewhat higher quality ﬁrst-stage learners in comparison

to the nAT E.

C.4 provides the error from the linearization. First note that there is an additional

term Rn,π due to the moment functions not being Neyman-orthogonal with respect to the

23

unconditional weights π used for rAT E/∆. It puts an additional restriction on the growth
of the number of treatments. For example, the ﬁrst condition reduces to (cid:112)kJ 3/n = o(1)

in case of parametric nuisance quantities. Rn,Q corresponds to the nAT E case plus an
additional term of order (cid:112)ξk log k/nk1/2J 1/4. This is a result of the interaction between
estimation error from estimating the unconditional weights with design matrix Q. Again,

for speciﬁc series such as splines or local partitioning, we conjecture that a faster rate of
(cid:112)ξk log k/n
and J 1/4 compared to the nAT E. This is due to the supereﬃciency of the unconditional

log kJ 1/4 is attainable. Note that the additional factors are only of order J 1/2

√

probability estimates ˆπt in the J → ∞ case.

C.5 is similar to B.5 but requires more stringent conditions on J and ξk compared to

the nAT E. The Lindeberg condition for nAT E/∆ is driven by the tails of a weighted

combination of moment functions from many treatment groups which can have high

variance when J is large. It is more restrictive compared to the nAT E, as, for the latter,

the weight for each t-speciﬁc moment function ψ[t,0]

i

(η) is the actual treatment propensity

et(Xi), see Table 1. Thus, the inverse propensity score weights disappear leading to a

lower variance for the nAT E5 explaining the additional J-dependent factors between B.5

and C.5.

The ﬁrst condition in C.6 rules out the degenerate case where nAT E = rAT E.

Naturally, if this is true, Assumptions B.1–B.5 apply instead. The second condition

excludes the hypothetical case where the sum of noise plus approximation error is perfectly

negatively correlated with the (γ-weighted) error from estimating the unconditional weights

π. Both restrictions are expected to always hold in practice and can also be assessed by

looking at the empirical analogues of Ω, Ω1, and Ω2.

For the estimation of the asymptotic variance, we also assume that A.V holds. The

corresponding details and discussion can be found in Appendix B.6.

A.V) (Asymptotic Variance) The assumptions in Appendix B.6 hold for nAT E and rAT E

or ∆ respectively, i.e. || ˆΩ − Ω|| = op(1).

5This is somewhat related to Li, Morgan, and Zaslavsky (2018) who show that propensity score weighted
average treatment eﬀects can be estimated with smallest variance within a class of eﬀect parameters.
In their case the weights are e(Xi)(1 − e(Xi)) and they only consider the binary treatment case, but a
similar intuition applies in the framework of this paper.

24

A.V can require somewhat stronger moment and growth conditions for the basis and/or

number of treatments. For example, for the nAT E, they reduce to the same rates required

by Semenova and Chernozhukov (2021), Theorem 3.3, condition (ii) with factor n1/m

replaced by (nJ)1/m. Under ﬁnite J, they are again equivalent. We obtain the following

Theorem:

Theorem 5.1 Let Φ(·) denote the Gaussian cumulative distribution function. Suppose

Assumptions A.1 – A.4, A.V, and B.1 – B.5 (C.1 – C.6) hold for nAT E (rAT E/∆) and

ˆβ and ˆΩ are estimated according to (12) and (18) respectively. Then, for any z0 = z0,n,

lim
n→∞

sup
t∈R

(cid:18)√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

n

b(z0)(cid:48)( ˆβ − β0)
(cid:113)
b(z0)(cid:48) ˆΩb(z0)

(cid:19)

≤ t

− Φ(t)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0.

Moreover if the approximation error is small,

√

nrg(z0)/(cid:112)b(z0)(cid:48)Ωb(z0) → 0, then

lim
n→∞

sup
t∈R

(cid:18)√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

n

b(z0)(cid:48) ˆβ − g(z0)
(cid:113)
b(z0)(cid:48) ˆΩb(z0)

(cid:19)

≤ t

(cid:12)
(cid:12)
− Φ(t)
(cid:12)
(cid:12)

= 0.

Theorem 5.1 demonstrates the asymptotic validity of the conﬁdence intervals proposed

in (13). The result accommodates the case of misspeciﬁcation often present in applied

econometric research.

It is most useful under the additional slight undersmoothing

condition that makes any misspeciﬁcation bias vanish suﬃciently fast. In particular, when

G is in a s-dimensional ball on Z of ﬁnite diameter (a Hölder class of smoothness order

s) then the condition simpliﬁes to n1/2k−( 1

2 + s

d ) log(k) → 0, see also Belloni et al. (2015),

Comment 4.3 for additional details. Note that such undersmoothing does in general not

admit IMSE optimal k choices. Alternatively, bias-correction methods could be employed

(Cattaneo et al., 2020).

Theorem 5.1 extends readily to alternative combinations of Neyman-orthogonal scores

other than rAT E or ∆. In particular, the results for the rAT E can directly be applied to

any convex combination of conditional average treatment eﬀects as long as the weights are

either (i) deterministic sequences (relative to J) or (ii) can be estimated at the same rate

as πt. This can be useful when comparing heterogeneity of a given selection mechanism

to alternative, hypothetical (estimated or true) allocation policies diﬀerent from random

25

selection as considered in this paper even when there are potentially many diﬀerent

treatments available.

5.2 Convergence Rates when J is large: Examples

In this section we provide some basic examples and intuition about the properties of

probability and nuisance function estimation when J is large and how this relates to the

machine learning bias Assumptions B.3/C.3. We ﬁrst discuss the necessity of Assumption

A.3 and the consequences for the unconditional probability estimates. We then show

how these properties translate into diﬀerent convergence rates for propensity scores and

potential outcome means under simpliﬁed parametric assumptions. We then discuss the

explicit requirements for the machine learning bias Assumptions B.3/C.3 for the ﬂexible

high-dimensional nuisance parameter case using Lasso methodology under approximate

sparsity and many treatments.

5.2.1 Large J and Assumption A.3

Consider the second part of Assumption A.3: If J is large, then Jπt (cid:46) 1 is a necessary

requirement. Because if the product diverges, Jπt > 1 causes a contradiction with the
summability constraint (cid:80)

t(cid:54)=0 πt = 1 − π0. In principle, one could allow for some t (cid:54)= 0
such that Jπt = o(1). However, restricting A.3 to hold for all t (cid:54)= 0 is without loss of

generality as otherwise it would be asymptotically equivalent to a regime where only a

smaller subset of treatments J (cid:48) < J obey A.3. Thus, all further assumptions and rates

would be equivalent with J being replaced by the new number of asymptotically relevant

treatments J (cid:48).

5.2.2 Supereﬃciency of Unconditional Probability Estimators

(Local) supereﬃciency of frequency estimators ˆπt = 1
n

i=1 Dt,i and other nuisance param-
eters is not a new discovery and has been exploited and discussed in diﬀerent places in the

(cid:80)n

literature, see e.g. Stoye (2009). For general P (Dt,i = 1) = πt, note that V [Dt,i] = πt(1−πt).
n(ˆπt−πt) d→ N (0, πt(1−πt)) which implies that |ˆπt−πt| (cid:46)P (n/πt)−1/2 (cid:46) (nJ)−1/2

Hence,

√

26

due to Assumption A.3. Thus under the many treatments regime J → ∞, the frequency

estimator is supereﬃcient, i.e. converges at a quicker rate than n−1/2.

5.2.3 Supereﬃciency of Parametric Propensity Scores

Supereﬃciency of ˆπt spills over to frequency-based/parametric estimators of propensity

scores. For example, consider the case where X is discrete (ﬁnite-dimensional) with

fx := P (Xi = x) > 0 for all x ∈ X . Consider a simple frequency-based estimator for the

treatment propensity t as

ˆet(x) =

(cid:20) 1
n

n
(cid:88)

i=1

1(Xi = x) + 1

(cid:18) n

(cid:88)

i=1

1(Xi = x) = 0

(cid:19)(cid:21)−1 1
n

n
(cid:88)

i=1

1(Xi = x)Dt,i.

where the additional indicator in the denominator assures existence. By standard argu-

ments, it follows that, for each x ∈ X ,

√

n(ˆet(x) − et(x)) d→ N (0, et(x)(1 − et(x)/fx))

which implies that |ˆet(x) − et(x)| (cid:46)P (n/et(x))−1/2 (cid:46) (nJ)−1/2 due to Assumption A.3.

Thus, the frequency-based ﬁnite-dimensional/parametric propensity score has the same

supereﬃciency property as the unconditional frequency estimator.

5.2.4 Slower convergence of Parametric Mean Functions

Parametric estimators of potential outcome means, however, are not supereﬃcient. On

the contrary, convergence rates are generally slower under the many treatments regime.

For example, consider a parametric frequency-based estimator similar to the one for the

propensity score:

ˆµt(x) =

(cid:20) 1
n

n
(cid:88)

i=1

1(Xi = x)Dt,i + 1

(cid:18) n

(cid:88)

i=1

1(Xi = x)Dt,i = 0

(cid:19)(cid:21)−1 1
n

n
(cid:88)

i=1

1(Xi = x)Dt,iYi

where X is again assumed to be discrete (ﬁnite-dimensional) with fx := P (Xi = x) > 0

for all x ∈ X . Without loss of generality, assume that E[(Yi(t) − µt(Xi))2|Xi = x] =

σ2 (Assumption B.1/C.1 would suﬃce as well). Again, by standard arguments, we

have that, for all x ∈ X ,

√

n(ˆµt(x) − µt(x)) d→ N (0, σ2/et(x)f (x)) which implies that

|ˆµt(x) − µt(x)| (cid:46)P (net(x))−1/2 (cid:46) (n/J)−1/2 due to Assumption A.3. Thus, the estimator

converges at a slower than parametric rate.

In fact, we have that, for any t (cid:54)= 0,

27

sn,t,2 = Jmt,n,2 =

√

Js0,n,2. Thus, the corresponding components in the machine learning

bias assumptions B.3/C.3 will be of equal rate in the parametric case.

5.2.5 Convergence for High-dimensional Sparse Nuisance Functions

Here we provide some intuition regarding the use of nuisance function estimation using

(frequency-based) Lasso in high-dimensional approximately sparse models. In particular,

we say potential outcome means are generated by µt(x) = x(cid:48)θ0 +rµt(x) where x ∈ X ⊆ Rpµt

(and equivalently for et(x) with logistic link). pµt

denotes the number of available regressors

that is allowed to be high-dimensional and grow with n. Assume that the typical regularity

conditions for Lasso hold as in Semenova and Chernozhukov (2021), Lemma B.1. Denote s∗
et

and s∗
µt

as the corresponding sparsity indices that obey these assumptions. For simplicity,

let the number of available regressors and sparsity indices coincide for propensity score

and potential outcome estimation, i.e. s∗

et = s∗

µt ≡ s∗ and pet = pµt ≡ p. For the machine

learning bias for the nAT E in Assumption B.3 we then conjecture that

B[nAT E]

n

(cid:46)

√

nk

(cid:114)

= s∗

(cid:114)

s∗ log(p)
n

(cid:18)

(cid:114)

J

s∗ log(p)
nJ

+

(cid:114)

(cid:19)

s log(p)
nJ

kJ log(p)2
n

based on the same argument as for the parametric frequency-based estimation above. Thus

B[nAT E]

n

= o(1) requires that sparsity indices have to obey

s∗ = o

(cid:18)(cid:114) n

kJ log(p)2

(cid:19)
.

For rAT E/∆, it is similarly required that

s∗ = o

(cid:18)(cid:114)

n
kJ 2 log(p)2

(cid:19)
.

Thus, the sparsity conditions for rAT E/∆ are stronger than for the nAT E in the many

treatments regime. This again reﬂects the increased variability due to the diﬀerent

weighting schemes between the decomposition parameters as the nAT E weights minimize

28

variance as discussed in Section 5.1. Comparing the rate requirement for the nAT E to

the one in Semenova and Chernozhukov (2021), we ﬁnd that s∗ here must be slower by

a factor of

√

J compared to their Lemma B.1. This is the price that has to be paid for

the expanding set of nuisance parameters when estimating treatment propensities and

potential outcome means for each treatment level separately instead of imposing the binary

treatment structure to begin with. Moreover, the nonparametric heterogeneity analysis

adds an additional factor of

√

k to the sparsity requirements compared to standard double

machine learning estimation of the unconditional binary ATE in Chernozhukov et al.

(2018) that only requires s = o((cid:112)n/ log(p)2) under equivalent assumptions. An analogous

derivation can be conducted for Λn as well. Note that the given sparsity assumption

here is for each treatment selection probability separately. In practice, we might want to

impose some (group-based) sparsity across treatments to improve estimation when many

treatment are available. In this case, convergence rates can be improved depending on the

degree of total complexity of the propensity scores (Farrell, 2015). We leave an extension

along these lines for future work.

6 Monte Carlo Study

In this section, we analyze the ﬁnite sample performance of the analytical conﬁdence

bounds proposed in Section 4. In particular, we evaluate the empirical coverage rates of

the corresponding conﬁdence intervals in a setup with heterogeneous eﬀective treatment

probabilities for all the decomposition parameters. We consider the case of three eﬀective

treatment levels and a univariate linear model for the heterogeneity analysis using diﬀerent

sample sizes and total number of confounding variables. In particular, in the ﬁnal step, we

regress the estimated pseudo outcomes on a single confounder and evaluate the coverage

rates for the parameters of this linear predictor. We consider two ways to estimate the

nuisance parameters (i) correctly speciﬁed parametric models and (ii) double machine

learning estimators. For the latter we apply 2-fold cross-ﬁtting using (cid:96)1-regularized linear

regression for the outcome models as well as (cid:96)1-regularized multinomial logistic regression

for the propensity scores. Tuning parameter selection is done via 5-fold cross-validation.

29

The true models satisfy the necessary sparsity assumptions required for high-quality

approximation of the machine learning methods (Belloni & Chernozhukov, 2013; Farrell,

2015; Belloni, Chernozhukov, & Wei, 2016). For more details on the designs please consider

Appendix B.7.

Table 2 contains the coverage rates of the conﬁdence intervals based on (13) using

correctly speciﬁed parametric models at a signiﬁcance level of 5%. All results are very

close to their nominal coverage rate. For the smallest n = 1000 and k = 100, there

is undercoverage for β of 5 percentage points for the rAT E which largely vanishes for

n = 5000. For the other parameters, there are no relevant size distortions.

Table 2: Monte Carlo Simulation: Results (Parametric Model)

(a) n = 1000

(b) n = 5000

k = 10

α 0.9460
β 0.9444
k = 100 α 0.9468
β 0.8974

rAT E nAT E
0.9456
0.9488
0.9488
0.9434

∆
0.9524
0.9470
0.9518
0.9474

k = 10

α 0.9486
β 0.9448
k = 100 α 0.9506
β 0.9380

rAT E nAT E
0.9480
0.9528
0.9508
0.9464

∆
0.9482
0.9570
0.9508
0.9458

The table entries contain the coverage rates under the null hypothesis for the parameters (α, β) of the
linear predictor for diﬀerent number of regressors (k), sample sizes (n) and decomposition parameters
rAT E, nAT E and ∆. The nominal coverage rate is 95%. All results are based on 5000 simulations.

Table 3 contains the coverage rates of the conﬁdence intervals based on (13) using

double machine learning at a signiﬁcance level of 5%. For rAT E(x) and nAT E(x) all

results are very close to the nominal coverage rate. For ∆(x), there is some undercoverage

for the intercept α by 1.7 to 9.6 percentage points which increases in the number of

parameters and decreases with the sample size. The slope parameter β is accurate for any

sample or regressor set size. Overall the inference based on the asymptotic approximation

in (13) seems to be reliable in ﬁnite samples.

7 Applications

7.1 Smoking and Birth Weight (Scenario 1)

The detrimental eﬀect of smoking on birth weight and its economic costs are well doc-

umented (see e.g. Almond et al., 2005; Abrevaya, 2006; Almond & Currie, 2011, and

30

Table 3: Monte Carlo Simulation: Results (Double Machine Learning)

(a) n = 1000

(b) n = 5000

k = 10

α 0.9472
β 0.9456
k = 100 α 0.9438
β 0.9486

rAT E nAT E
0.9494
0.9452
0.9420
0.9476

∆
0.8964
0.9430
0.8538
0.9542

k = 10

α 0.9516
β 0.9498
k = 100 α 0.9512
β 0.9534

rAT E nAT E
0.9500
0.9506
0.9508
0.9520

∆
0.9326
0.9478
0.9204
0.9534

The table entries contain the coverage rates under the null hypothesis for the parameters (α, β) of the
linear predictor for diﬀerent number of regressors (k), sample sizes (n) and decomposition parameters
rAT E, nAT E and ∆. The nominal coverage rate is 95%. Results are based on 5000 simulations.

references therein). Beyond the standard average eﬀects it is important to understand

the heterogeneous eﬀects to e.g. identify for which subgroups interventions to reduce

smoking during pregnancy would be most beneﬁcial. Abrevaya (2006) documents that

the negative eﬀect of smoking is less pronounced for black compared to white mothers

in a standard subgroup analysis. A variety of papers analyze heterogeneous eﬀects of

smoking as a function of mother’s age (Abrevaya, Hsu, & Lieli, 2015; Lee et al., 2017;

Zimmert & Lechner, 2019; Fan et al., 2022). They all document increasingly negative

eﬀects with higher age. The aforementioned studies consider "smoking yes/no" as the

binary treatment. Cattaneo (2010) notes that smoking is not a homogeneous treatment,

but that the negative eﬀects become more extreme for higher intensities of smoking. Thus,

the binary indicator "smoking" represents only an aggregation of smoking intensities which

directly aﬀect birth weight. This corresponds to Scenario 1. We investigate whether the

heterogeneous eﬀects documented in the literature can be at least partly explained by

diﬀerent smoking intensities of diﬀerent groups.

We analyze the dataset of Almond et al. (2005) used by Cattaneo (2010) with ﬁve

intensities of smoked cigarettes per day as the eﬀective treatment Ti ∈ T = {0, 1 − 5, 6 −

10, 11 − 15, 16 − 20, > 20}, the binary indicator deﬁned as Di = 1(Ti > 0), the outcome Yi

being birth weight in gram, and the confounders Xi including age, education, ethnicity,

and marital status of mother and father as well as health indicators and pregnancy history

of the mother.6 The dataset comprises 511,940 observations after removing the 0.1%

of the observations with missing values in relevant variables and 52 confounders. The

6We thank Matias Cattaneo for sharing the full data. A random subsample is available on his GitHub
repository.

31

Figure 3: Heterogeneous eﬀects and decomposition by ethnicity

(a) Subgroup eﬀects for ethnicity

(b) Eﬀect heterogeneity with white as benchmark

Note: Point estimates of the decomposition parameters with 95%-conﬁdence interval.

nuisance parameters are estimated with 2-fold cross-ﬁtting using an ensemble learner

of the unconditional mean, Random Forests, Lasso and Ridge regression with 2-fold

cross-validated weights. For the propensity scores, we use logistic Lasso and Ridge.

Smoking behavior diﬀers along the heterogeneity variables ethnicity and age showing

that white and older smoking mothers smoke more heavily.7 Combined with the result of

Cattaneo (2010) that diﬀerent smoking intensities have diﬀerent eﬀects, this suggests that

at least part of the heterogeneity could be explained by diﬀerent smoking intensities.

Figure 3 contains the result of the decomposition for the heterogeneity variable "eth-

nicity". The upper panel shows the decomposition for each subgroup. It is obtained

by running an OLS regression of the estimand speciﬁc pseudo-outcome on a set of four

7Appendix B.8 and in particular Figure B.4 provides the smoking distributions by heterogeneity variables.

32

nATErATEDelta−200−150−100−500−200−150−100−500−200−150−100−500WhiteOtherHispanicBlackEffectnATErATEDelta050100150050100150050100150OtherHispanicBlackDifference to Whitedummy variables indicating ethnicity of the mother without a constant. The standard

errors are then adjusted as described in Section 4. The nAT E in the left part corresponds

to standard subgroup analysis. Like previous studies, we ﬁnd that smoking reduces the

birth weight of newborns more for white women than for Blacks, Hispanics and others.

Given that smoking is a binarized treatment, it is not clear how much is really eﬀect

heterogeneity and how much is driven by the fact that subgroups diﬀer in their smoking

intensity. The decomposition term rAT E ﬁxes the intensity of smoking for all subgroups

at the population level. It provides the subgroup speciﬁc eﬀect of smoking if all groups

had the same smoking intensity. Under this harmonized smoking intensity the negative

average eﬀect of smoking is smaller for white women and larger for the others. ∆ in

the right graph quantiﬁes the diﬀerence between nAT E and rAT E. It shows relatively

small diﬀerences suggesting that diﬀerent smoking intensities are not the main driver of

the diﬀerences between white mothers and the other groups. However, they are also not

negligible as the lower panel of Figure 3 shows. It quantiﬁes the heterogeneous eﬀects by

subtracting the eﬀects for white mothers from the other three groups. We observe that a

signiﬁcant portion of the diﬀerence between black/hispanic mothers and white mothers

is driven by diﬀerent smoking intensities. For black vs. white mothers the diﬀerence in

the nAT E is 69 gram of which 12% are due to diﬀerent smoking intensities (∆ = 8). For

hispanic vs. white mothers it explains around 17% (∆ = 14).

Figure 4 depicts the heterogeneity analysis along age. We use B-splines as basis

functions of age. We select the nodes and order via leave-one-out cross-validation for each

parameter and apply the most ﬂexible/low-bias model for all parameters to ensure that the

rAT E and ∆ curves add up to the nAT E curve. The left panel of Figure 4 replicates the

well-established ﬁndings of previous papers that the nAT E is much smaller for younger

mothers than for older mothers.

In the extreme case where diﬀerent smoking intensities would fully explain the het-

erogeneous nAT E, we would see a ﬂat rAT E curve in the middle graph. However, we

only observe that the eﬀect of teenage mothers would be more negative if we harmonize

smoking intensity over all age groups.

Overall, only a relatively small part of the heterogeneous eﬀects of the binarized

33

Figure 4: Eﬀect Heterogeneity by Age

Notes: B-spline estimated decomposition parameters with 95%-conﬁdence interval.

smoking indicator can be attributed to diﬀerent smoking intensities and the larger part

seems to be driven by diﬀerent age groups actually being aﬀected diﬀerently.

7.2 Job Corps (Scenario 2)

We illustrate Scenario 2 of Figure 2 with an evaluation of the Job Corps (JC) program.

JC operates since 1964 and is the largest training program for disadvantaged youth aged

16-24 in the US (see Schochet et al., 2001, 2008, for a detailed description). The roughly

50,000 participants per year receive an intensive treatment as a combination of diﬀerent

components like academic education, vocational training, and job placement assistance.

Participants plan their educational and vocational curricula together with counselors. This

means that although the variable “access to JC” is a binary indicator, diﬀerent versions of

JC participation are conceivable. Heterogeneous eﬀects might thus be driven by diﬀerent

eﬀectiveness of JC for diﬀerent groups, by diﬀerent tailoring of the curriculum, or a

combination thereof.

We investigate this based on data from an experiment in 1994-1996 (Schochet, Burghardt,

34

& McConnell, 2019).8 This experiment is basis of a variety of studies looking at diﬀerent

aspects of JC. Many of them report gender diﬀerences in the eﬀectiveness of the programs

with women beneﬁting less than men from access to JC (e.g. Schochet et al., 2001, 2008;

Flores et al., 2012; Eren & Ozbelik, 2014; Strittmatter, 2019). One potential explanation

for this ﬁnding is that men and women focus on average on diﬀerent vocational training

within JC. In particular men receive more often training for higher paying craft jobs, while

women focus more often on training for the service sector (Quadagno & Fobes, 1995; Inanc,

Needels, & Berk, 2017).9 We apply our decomposition method to investigate this potential

explanation of the gender gap in program eﬀectiveness.

We analyze the intention to treat eﬀect (ITT) of the binary variable indicating random

access to JC (Di) on weekly earnings four years after random assignment (Yi). We

consider 11 versions of the eﬀective treatment (Ti): (i) No JC if eligible individuals did

not participate (non-compliers), (ii) JC without vocational training if eligible individuals

entered JC but did not receive vocational training, (iii-ix) training for jobs in the clerical,

health, auto mechanics, welding, electrical/electronics, construction, or food sector, (x)

other vocational training, (xi) training for multiple sectors.

Nuisance parameters are estimated with the same ensemble as in Section 7.1 using 5-fold

cross-ﬁtting. We control for 55 covariates that include pre-treatment information about

labor market history, socio-economic characteristics, education, health, crime, and JC

related variables. These control variables overlap mostly with those of Flores et al. (2012)

who also employ an unconfoundedness strategy. Considering second-order interactions

results in a total of 1428 variables after screening for nearly empty cells (less than 1%

observations) and nearly perfectly correlated variables (correlation higher than 0.99). In

total we work with a sample of 9,708 observations.

The unconditional nAT E, corresponding to the ITT of eligibility for JC on monthly

earnings, is estimated at $14.2 (S.E. 3.8), which is an increase of 7% in line with previous

studies. The unconditional rAT E is larger ($17.4, S.E. 4.1) suggesting that hypothetical

random allocation of the curricula would yield higher average outcomes compared to

the actual assignment. However, the unconditional diﬀerence ∆ is insigniﬁcant ($ − 3.1,

8The data is available as public use ﬁle via https://doi.org/10.3886/E113269V1.
9Appendix B.9 and in particular Figure B.7 provides the distribution of trainings by gender.

35

Figure 5: Eﬀect heterogeneity and decomposition by gender

Notes: The numbers in the bar show the point estimate and the p-value in parentheses.

S.E. 1.8). This suggests that, on average, the selection of versions is not statistically

distinguishable from random allocation.

Figure 5 depicts the decomposition of the gender speciﬁc eﬀects. We observe that

the eﬀect for women with the actual composition of vocational training (nAT E) is not

signiﬁcant at α = 0.05, but under the hypothetical treatment composition of the population

would show a clear positive eﬀect (rAT E = $16.0). The gender gap in eﬀectiveness basically

disappears when both groups receive the same hypothetical mix of vocational training.

The right part of Figure 5 suggests that 73% of the gender gap in the eﬀectiveness of JC

is due to diﬀerent training curricula. This means that the worse than average performance

of the assignment mechanism seen in the unconditional parameters is mostly driven by

women. While the assignment to vocational training for men is as well targeted as random

assignment, for women it is even worse. This indicates that there is room for improvement

to target vocational training in general and for women in particular. Our results suggest

that removing the worse than random targeting of vocational training for women could

decrease the gender gap in the eﬀectiveness of access to JC.

36

 9.4−6.716.0(0.081)(0.053)(0.007)17.8−0.518.3(0.001)(0.790)(0.001)−8.5−6.2−2.3(0.262)(0.116)(0.777)FemaleMaleDifference Female − MalenATEDrATEnATEDrATEnATEDrATE01020EstimandEffect8 Concluding Remarks

The method proposed in this paper provides a practical way of decomposing eﬀect het-

erogeneity obtained from analyzing a binary treatment indicator that does not coincide

with the eﬀective multi-valued treatment. The approach likely extends to other causal

parameters and identiﬁcation strategies such as continuous eﬀective treatments, selection

on unobservables/instrumental variables, or mediation analysis. It would also be interesting

to see whether the ideas could be further developed to ﬁnd the most relevant dimensions

of eﬀective treatments for cases with multiple treatment versions instead of requiring the

researcher to manually specify them.

The conceptual and empirical results highlight that potential treatment heterogeneity

underlying the analyzed binary indicator should be taken more seriously and explicitly

discussed in applications, especially when interpreting heterogeneous eﬀects. The de-

composition provides one principled way to do this. It requires to observe the eﬀective

treatment. Thus, data collection can anticipate the goal of better understanding treatment

heterogeneity by recording eﬀective treatment information beyond a binary indicator.

Furthermore, the decomposition shows that reducing the analysis to such binary indicators,

while facilitating the analysis, comes at the cost of a more intricate interpretation of

empirical results.

References

Abadie, A., & Cattaneo, M. D. (2018). Econometric methods for program evaluation.

Annual Review of Economics, 10 , 465–503.

Abrevaya, J. (2006). Estimating the eﬀect of smoking on birth outcomes using a matched

panel data approach. Journal of Applied Econometrics, 21 (4), 489–519.

Abrevaya, J., Hsu, Y.-C., & Lieli, R. P. (2015). Estimating conditional average treatment

eﬀects. Journal of Business & Economic Statistics, 33 (4), 485–505.

Almond, D., Chay, K. Y., & Lee, D. S. (2005). The costs of lower birth weight. The

Quarterly Journal of Economics, 120 (3), 1031–1083.

37

Almond, D., & Currie, J. (2011). Human capital development before age ﬁve. In Handbook

of labor economics (Vol. 4, pp. 1315–1486). Elsevier.

Andresen, M. E., & Huber, M. (2021).

Instrument-based estimation with binarised

treatments: issues and tests for the exclusion restriction. The Econometrics Journal ,

24 (3), 536–558.

Angrist, J. D., & Imbens, G. W. (1995). Two-stage least squares estimation of average

causal eﬀects in models with variable treatment intensity. Journal of the American

Statistical Association, 90 (430), 431–442.

Athey, S., & Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal eﬀects.

Proceedings of the National Academy of Sciences, 113 (27), 7353–7360.

Athey, S., & Imbens, G. W. (2017). The state of applied econometrics: causality and

policy evaluation. Journal of Economic Perspectives, 31 (2), 3–32.

Athey, S., Tibshirani, J., & Wager, S. (2019). Generalized random forests. Annals of

Statistics, 47 (2), 1148 - 1178.

Belloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-

dimensional sparse models. Bernoulli, 19 (2), 521–547.

Belloni, A., Chernozhukov, V., Chetverikov, D., & Kato, K. (2015). Some new asymp-

totic theory for least squares series: Pointwise and uniform results. Journal of

Econometrics, 186 (2), 345–366.

Belloni, A., Chernozhukov, V., & Wei, Y. (2016). Post-selection inference for generalized

linear models with many controls. Journal of Business & Economic Statistics, 34 (4),

606–619.

Buhl-Wiggers, J., Kerwin, J., Muñoz, J. S., Smith, J., & Thornton, R. (2022). Some

children left behind: variation in the eﬀects of an educational intervention. Journal

of Econometrics. doi: 10.1016/j.jeconom.2021.12.010

Cattaneo, M. D. (2010). Eﬃcient semiparametric estimation of multi-valued treatment

eﬀects under ignorability. Journal of Econometrics, 155 (2), 138–154.

Cattaneo, M. D., Farrell, M. H., & Feng, Y. (2020). Large sample properties of partitioning-

based series estimators. Annals of Statistics, 48 (3), 1718–1741.

Cattaneo, M. D., Keele, L., Titiunik, R., & Vazquez-Bare, G.

(2016).

Interpreting

38

regression discontinuity designs with multiple cutoﬀs. Journal of Politics, 78 (4),

1229–1248.

Chernozhukov, V., Chetverikov, D., Demirer, M., Duﬂo, E., Hansen, C., Newey, W., &

Robins, J. (2018). Double/Debiased machine learning for treatment and structural

parameters. The Econometrics Journal , 21 (1), C1-C68.

Cole, S. R., & Frangakis, C. E. (2009). The consistency statement in causal inference.

Epidemiology, 20 (1), 3–5.

Curth, A., & van der Schaar, M. (2021). Nonparametric estimation of heterogeneous

treatment eﬀects: From theory to learning algorithms. In Proceedings of the 24th

international conference on artiﬁcial intelligence and statistics (Vol. 130, pp. 1810–

1818). PMLR.

Davis, J. M. V., & Heller, S. B. (2020). Rethinking the beneﬁts of youth employment

programs: The heterogeneous eﬀects of summer jobs. The Review of Economics and

Statistics, 102 (4), 664–677.

Eren, O., & Ozbelik, S. (2014). Who beneﬁts from Job Corps? A distributional ananlysis of

an active labor market program. Journal of Applied Econometrics, 29 (4), 586–611.

Fan, Q., Hsu, Y.-C., Lieli, R. P., & Zhang, Y. (2022). Estimation of conditional average

treatment eﬀects with high-dimensional data. Journal of Business & Economic

Statistics, 40 (1), 313–327.

Farrell, M. H. (2015). Robust inference on average treatment eﬀects with possibly more

covariates than observations. Journal of Econometrics, 189 (1), 1–23.

Flores, C. A., Flores-Lagunes, A., Gonzalez, A., & Neumann, T. C. (2012). Estimating

the eﬀects of length of exposure to instruction in a training program: The case of

job corps. Review of Economics and Statistics, 94 (1), 153–171.

Heckman, J. J. (2020). Epilogue: Randomization and social policy evaluation revisited.

In F. Bédécarrats, I. Guérin, & F. Roubaud (Eds.), Randomized control trials in the

ﬁeld of development: A critical perspective (pp. 304–330). Oxford University Press.

Heiler, P. (2022). Estimating Heterogeneous Bounds for Treatment Eﬀects under Sample

Selection and Non-response. arXiv:2209.04329 . Retrieved from http://arxiv.org/

abs/2209.04329

39

Heiler, P., & Kazak, E. (2021). Valid inference for treatment eﬀect parameters under

irregular identiﬁcation and many extreme propensity scores. Journal of Econometrics,

222 (2), 1083–1108.

Heiler, P., & Taylor, L. (2022). Nonparametric estimation for categorical responses with

small frequencies. Working Paper .

Hernán, M. A., & VanderWeele, T. J. (2011). Compound treatments and transportability

of causal inference. Epidemiology, 22 (3), 368–377.

Hong, H., Leung, M. P., & Li, J. (2020). Inference on ﬁnite-population treatment eﬀects

under limited overlap. The Econometrics Journal , 23 (1), 32–47.

Hotz, V. J., Imbens, G. W., & Klerman, J. A. (2006). Evaluating the diﬀerential dﬀects

of alternative welfare-to-work training aomponents: A reanalysis of the California

GAIN program. Journal of Labor Economics, 24 (3), 521–566.

Hotz, V. J., Imbens, G. W., & Mortimer, J. H. (2005). Predicting the eﬃcacy of future

training programs using past experiences at other locations. Journal of Econometrics,

125 (1-2), 241–270.

Huang, J. Z. (2003). Local asymptotics for polynomial spline regression. The Annals of

Statistics, 31 (5), 1600–1635.

Imai, K., & Li, M. L.

(2021). Experimental evaluation of individualized treatment

rules. Journal of the American Statistical Association. doi: 10.1080/01621459.2021

.1923511

Imbens, G. W. (2000). The role of the propensity score in estimating dose-response

functions. Biometrika, 87 (3), 706–710.

Imbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical

sciences. Cambridge University Press.

Inanc, H., Needels, K., & Berk, J. (2017). Gender segregation in training programs and

the wage gap (Tech. Rep. Nos. Cambridge, NJ: Mathematica Policy Research).

Kennedy, E. H. (2020). Optimal doubly robust estimation of heterogeneous causal eﬀects.

arXiv:2004.14497 . Retrieved from http://arxiv.org/abs/2004.14497

Kennedy, E. H., Ma, Z., McHugh, M. D., & Small, D. S. (2017). Non-parametric methods

for doubly robust estimation of continuous treatment eﬀects. Journal of the Royal

40

Statistical Society: Series B (Statistical Methodology), 79 , 1229–1245.

Khan, S., & Tamer, E. (2010). Irregular identiﬁcation, support conditions, and inverse

weight estimation. Econometrica, 78 (6), 2021–2042.

Knaus, M. C. (2022). Double machine learning based program evaluation under uncon-

foundedness. The Econometrics Journal , 25 (3), 602–627.

Knaus, M. C., Lechner, M., & Strittmatter, A. (2021). Machine Learning Estimation of

Heterogeneous Causal Eﬀects: Empirical Monte Carlo Evidence. The Econometrics

Journal , 24 (1), 134–161.

Knaus, M. C., Lechner, M., & Strittmatter, A.

(2022). Heterogeneous employment

eﬀects of job search programmes: A machine learning approach. Journal of Human

Resources, 57 (2), 597–636.

Künzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating

heterogeneous treatment eﬀects using machine learning. Proceedings of the National

Academy of Sciences, 116 (10), 4156–4165.

Lechner, M. (2001). Identiﬁcation and estimation of causal eﬀects of multiple treatments

under the conditional independence assumption. In M. Lechner & E. Pfeiﬀer (Eds.),

Econometric evaluation of labour market policies (pp. 43–58). Heidelberg: Physica.

Lechner, M. (2002). Program heterogeneity and propensity score matching: an application

to the evaluation of active labor market policies. The Review of Economics and

Statictics, 84 , 205–220.

Lee, S., Okui, R., & Whang, Y.-J. (2017). Doubly robust uniform conﬁdence band for

the conditional average treatment eﬀect function. Journal of Applied Econometrics,

32 (7), 1207–1225.

Li, F., Morgan, K. L., & Zaslavsky, A. M. (2018). Balancing Covariates via Propensity

Score Weighting. Journal of the American Statistical Association, 113 (521), 390–400.

Ma, X., Sasaki, Y., & Wang, Y. (2022). Testing limited overlap.

Ma, X., & Wang, J. (2020). Robust Inference Using Inverse Probability Weighting. Journal

of the American Statistical Association, 115 (532), 1851–1860.

Marshall, J. (2016). Coarsening bias: How coarse treatment measurement upwardly biases

instrumental Variable Estimates. Political Analysis, 24 (2), 157–171.

41

Newey, W. K. (1997). Convergence rates and asymptotic normality for series estimators.

Journal of Econometrics, 79 (1), 147–168.

Nie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment eﬀects.

Biometrika, 108 (2), 299–319.

Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82 (4), 669–688.

Petersen, M. L. (2011). Compound treatments, transportability, and the structural causal

model: The power and simplicity of causal graphs. Epidemiology, 22 (3), 378–381.

Quadagno, J., & Fobes, C. (1995). The welfare state and the cultural reproduction of

gender: Making good girls and boys in the Job Corps. Social Problems, 42 (2),

171–190.

Richardson, T., & Robins, J. M. (2013). Single World Intervention Graphs (SWIGs):

Unifying the Counterfactual and Graphical Approaches to Causality – Presentation.

Center for the Statistics and the Social Sciences, University of Washington Series.

Working Paper , 128 .

Robins, J. M., & Rotnitzky, A. (1995). Semiparametric eﬃciency in multivariate regression

models with missing data. Journal of the American Statistical Association, 90 (429),

122–129.

Rothe, C. (2017). Robust Conﬁdence Intervals for Average Treatment Eﬀects Under

Limited Overlap. Econometrica, 85 (2), 645–660.

Rubin, D. B. (1974). Estimating causal eﬀects of treatments in randomized and nonran-

domized studies. Journal of Educational Psychology, 66 (5), 688–701.

Rubin, D. B. (1980). Randomization Analysis of Experimental Data: The Fisher Random-

ization Test Comment. Journal of the American Statistical Association, 75 (371),

591.

Rudelson, M. (1999). Random vectors in the isotropic position. Journal of Functional

Analysis, 164 (1), 60–72.

Schochet, P. Z., Burghardt, J., & Glazerman, S. (2001). National job corps study: The

impacts of job jorps on participants’ employment and related outcomes (Tech. Rep.).

Princeton, NJ: Mathematica Policy Research Inc.

Schochet, P. Z., Burghardt, J., & McConnell, S. (2008). Does job corps work? Impact

42

ﬁndings from the national job corps study. American Economic Review , 98 (5),

1864–1886.

Schochet, P. Z., Burghardt, J., & McConnell, S. (2019). Replication data for: Does job

corps work? Impact ﬁndings from the national job corps study. Inter-university

Consortium for Political and Social Research (ICPSR).

Semenova, V., & Chernozhukov, V. (2021). Debiased machine learning of conditional

average treatment eﬀects and other causal functions. The Econometrics Journal ,

24 (2), 264–289.

Stoye, J. (2009). More on conﬁdence intervals for partially identiﬁed parameters. Econo-

metrica, 77 (4), 1299–1315.

Strittmatter, A. (2019). Heterogeneous earnings eﬀects of the job corps by gender: A

translated quantile approach. Labour Economics, 10760 .

VanderWeele, T. J. (2009). Concerning the consistency assumption in causal inference.

Epidemiology, 20 (6), 880–883.

VanderWeele, T. J., & Hernan, M. A. (2013). Causal inference under multiple versions of

treatment. Journal of Causal Inference, 1 (1), 1–20.

Vazquez-Bare, G. (2022). Identiﬁcation and estimation of spillover eﬀects in randomized

experiments. Journal of Econometrics. doi: 10.1016/j.jeconom.2021.10.014

Wager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment eﬀects

using random forests. Journal of the American Statistical Association, 113 (523),

1228–1242.

Zimmert, M., & Lechner, M. (2019). Nonparametric estimation of causal heterogeneity

under high-dimensional confounding. arXiv:1908.08779 . Retrieved from http://

arxiv.org/abs/1908.08779

43

Appendices

A Proof of Theorem 5.1

A.1 Preliminaries

The proof is structured as follows: First we provide some auxiliary results. Then we derive

the asymptotically linear representation of the best linear predictor and show its asymptotic

normality using the true variance covariance matrix. The necessary derivations to replace

the true variance with the estimated sample counterpart are provided in Supplementary

Appendix B.6. For reference, we refer with BCCK to Belloni et al. (2015) and with SC to

Semenova and Chernozhukov (2021). For the following, we use empirical process notation

En[Xi] :=

1
n

n
(cid:88)

i=1

Xi, Gn[Xi] :=

1
√
n

n
(cid:88)

(Xi − E[Xi]).

i=1

Recall that Di = (cid:80)

t(cid:54)=0 Dt,i and DiYi = (cid:80)

t(cid:54)=0 Dt,iYi. Conditional independence implies

that

E[YiDi|Xi] =

(cid:88)

t(cid:54)=0

et(Xi)µt(Xi), E[µt(Xi)Di|Xi] =

(cid:88)

t(cid:54)=0

et(Xi)µt(Xi).

Recall that Dt,iDs,i = 0 for s (cid:54)= t.

A.2 Machine Learning Bias

In the following, we verify the small bias Assumption 3.5 in SC for our moment functions

evaluated at the true π. Let un = o(1) such that with probability of at least 1 − un, for all

f ∈ [K], the cross-ﬁtted ˆηf belongs to a shrinking neighborhood Hn around η. We show

that, uniformly over Hn, the moment functions for nAT E, rAT E, and ∆ satisfy

√

Bn =

n sup
ˆη∈Hn

||E[b(Zi)(ψ(ˆη, π) − ψ(η, π))]|| = o(1)

Λn = sup
ˆη∈Hn

E[||b(Zi)(ψ(ˆη, π) − ψ(η, π))||2]1/2 = o(1).

44

For the proof the followings expectations are all used omitting preﬁx supˆη∈Hn
does not cause confusion. We will make use of a general decomposition of the AIPW-type

when it

moment functions: For general binary Di, any Yi, and η = (µi, ei) we deﬁne

ψi(η) =

Di(Yi − µi)
ei

+ µi.

Decomposing the function evaluated at two points yields

ψi(ˆη) − ψi(η) = (ˆµi − µi)

1 −

(cid:18)

(cid:19)

Di
ei

− (ˆei − ei)(Yi − µi)

Di
eiˆei

+ (ˆµi − µi)(ˆei − ei)

Di
ˆeiei

≡ (a.1) − (a.2) + (a.3).

For the following we will ignore the "control" part ψ[0]

i (Wi, η) in the moment functions as
this is covered by the standard potential outcome case in SC. The rates in the following

are not aﬀected. For the following it is essential to note that, due to cross-ﬁtting, ˆet(Xi)

and ˆµt(Xi) only depend on i through Xi. Thus, evaluating expectations depending on ˆη,

we omit the explicit conditioning set on the cross-ﬁtted fold for convenience as in SC. The

unconditional convergence then follows from Chernozhukov et al. (2018), Lemma 6.1. Note

also that, conditional on Xi, any measurable function of Zi is known. We ﬁrst provide

some auxiliary results in what follows.

A.2.1 Auxiliary results

(H.1): ||ˆγt − γt|| bound

||ˆγt − γt|| ≤ ||En[bi(ψ[t]

i (ˆη) − ψ[t]

i (η))]|| − ||En[biψ[t]

i (η) − E[biψ[t]

i (η)]]||

45

Using Markov’s inequality and Cauchy-Schwarz together with the deﬁnition of the moment

function yields for the ﬁrst term

||En[bi(ψ[t]

i (ˆη) − ψ[t]

i (η))]|| (cid:46)P E[||bi(ψ[t]

i (ˆη) − ψ[t]

i (η))||]

(cid:46)P E[||bi||2]1/2E[(ψ[t]

i (ˆη) − ψ[t]

i (η))2]1/2

√

≤

kE[(ψ[t]
√

i (ˆη) − ψ[t]

i (η))2]1/2

kJ(mt,n,2 + Jst,n,2)

(cid:46)P

as

E[(ψ[t]

i (η))2]

i (ˆη) − ψ[t]
(cid:20)
(ˆµt(Xi) − µt(Xi))2(1 − Dt,i/et(Xi))2 + (ˆet(Xi) − et(Xi))2εi(t)2

≤ 4E

Dt,i
et(Xi)2ˆet(Xi)2

+ (ˆµt(Xi) − µt(Xi))2(ˆet(Xi) − et(Xi))2

(cid:21)

Dt,i
et(Xi)2ˆet(Xi)2

(cid:46)P sup
x∈X

πt
et(x)

π−1
t

(cid:18)

E[(ˆµt(Xi) − µt(Xi))2] + χ2

t,nπ−2

t E[(ˆet(Xi) − et(Xi))2]

(cid:19)

(cid:46)P J(m2

t,n,2 + J 2s2

t,n,2)

For the second term we have

||En[biψ[t]

i (η) − E[biψ[t]

i (η)]]|| (cid:46)P E[||En[biψ[t]

i (η)] − E[biψ[t]

i (η)]||]

≤ E[||En[biψ[t]

i (η)]||2]1/2

= (E[ψ[t]

i (η)2b(cid:48)

ibi]/n)1/2

= (E[E[ψ[t]
(cid:20)(cid:18)σ2

(cid:18)

=

E

i (η)2|Xi]b(cid:48)

ibi/n])1/2
(cid:19)

+ µt(Xi)2

b(cid:48)
ibi

t (Xi)
et(Xi)
πt
et(x)

(cid:18)(cid:18)

sup
x∈X

(cid:19)

π−1
t + 1

E[b(cid:48)

ibi]/n

(cid:21)

(cid:19)1/2

/n

(cid:19)1/2

(cid:46)

(cid:46)

(cid:114)

Jk
n

46

by conditional independence and bounded second moments for the potential outcomes.

Overall we obtain

(H.2): ||γt|| rate

||ˆγt − γt|| (cid:46)P

√

kJ

(cid:18)

n−1/2 + mt,n,2 + Jst,n,2

(cid:19)

||γt|| = ||E[biψ[t]

i (η)]|| = ||E[biµt(Xi)]|| (cid:46) sup

x∈X

|µt(x)|E[||bi||] (cid:46)

√

k

(H.3): ||En[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))]|| rate

||En[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))]|| = ||

(cid:88)

t(cid:54)=0

En[biψ[t]

i (ˆη)]

(cid:18) ˆπt

1 − ˆπ0

−

πt
1 − π0

(cid:19)

||

(||γt|| + ||ˆγt − γt||)|ˆπt − πt|(1 + n−1/2)

(cid:46)P J sup
t(cid:54)=0
√

(cid:46)P J
(cid:114)

=

Jk
n

(cid:114) 1
nJ

k

(H.4) Restatement of Lemma 6.2 from BCCK of the Rudelson (1999) LLN

for Matrices Let Q1, . . . , Qn be a sequence of independent symmetric non-negative k × k-

matrix valued random variables with k ≥ 2 such that Q = En[E[Qi]] and ||Qi|| ≤ M a.s.,
then for ˆQ = En[Qi]

E[|| ˆQ − Q||] (cid:46) M log k

n

(cid:114)

+

M ||Q|| log k
n

.

(H.5) γ, ai, and aia(cid:48)

i rates Deﬁne An = En[aia(cid:48)

i] and ˆAn = En[ˆaiˆa(cid:48)

i]. Recall the

deﬁnitions:

γ = (γ1, . . . , γJ ), γt = E[biτt(Xi)],

ai = (a[1]
i

, . . . , a[J]
i )

47

Thus

||γ|| (cid:46)

√

J sup
t(cid:54)=0

||γt|| (cid:46)

√

Jk

||ˆγ − γ|| (cid:46)P

√

J sup
t(cid:54)=0

||ˆγt − γt|| (cid:46)P

√

kJ 2(n−1/2 + mn,2 + Jsn,2)

E[||ai||] ≤

E[||ˆai − ai||] ≤

√

J sup
t(cid:54)=0

√

J sup
t(cid:54)=0

E[(a[t]

i )2]1/2 (cid:46)P

E[(ˆa[t]

i − a[t]

i )2]1/2 (cid:46)

√

πt (cid:46) 1

√

J sup
t(cid:54)=0
√

(cid:114) πt
n

(cid:46) n−1/2

J sup
t(cid:54)=0

as ˆa[t]

i − a[t]

i

(cid:46)P πt|ˆπ0 − π0| + π0|ˆπt − πt|. Equivalently, we have

||An|| ≤ En[||aia(cid:48)

i||] (cid:46)P E[||ai||2] ≤ J sup

πt (cid:46) 1

|| ˆAn − An|| (cid:46) En[||( ˆai − ai)a(cid:48)

i||] (cid:46)P

√

J sup
t(cid:54)=0

We further have that

t(cid:54)=0
(cid:18)

πt(ˆπ0 − π0)2 + (ˆπt − πt)2

(cid:19)1/2

(cid:46)P n−1/2

max
1≤i≤n

||aia(cid:48)

i|| ≤

√

J max
1≤i≤n

||aia(cid:48)

i||1

J max
1≤i≤n

(cid:88)

t(cid:54)=0

(Dt,i(1 − π0) − πt + D0,iπt)2

J(1 + J −2 + J −1)

√

√

(cid:46)P

(cid:46)P

√

J

(cid:46)

and for the expectation

||E[aia(cid:48)

i]|| ≤

≤

≤

√

√

√

J||E[aia(cid:48)

i]||1

(E[(a[t]

i )2] +

J sup
t(cid:54)=0

E[a[t]

i a[s]
i ])

(cid:88)

s(cid:54)=t,0

J sup
t(cid:54)=0

(πt +

(cid:88)

s(cid:54)=t,0

πtπs)

(cid:46) J −1/2

Now note that aia(cid:48)
i

are symmetric, non-negative iid matrix valued random variables. Thus

48

using Rudelson’s LLN (H.4) we obtain

E[||En[aia(cid:48)

i] − E[aia(cid:48)

i]||] (cid:46) J 1/2 log J

n

(cid:114)

+

J 1/2||E[aia(cid:48)
n

i]|| log J

(cid:114)

(cid:46)

log J
n

as J = o(n).

(H.6) Linearization of the unconditional weights:

ˆπt
t(cid:54)=0 ˆπt

(cid:80)

−

(cid:80)

πt
t(cid:54)=0 πt

=

=

=

and thus

−

πt
1 − π0

ˆπt
1 − ˆπ0
ˆπt(1 − π0) − (1 − π0)πt + (1 − π0)πt − πt(1 − ˆπ0)
(1 − π0)(1 − ˆπ0)

1 − π0
1 − ˆπ0

1

(1 − π0)2 En[([Dt,i − πt](1 − π0) + [D0,i − π0]πt)]

√

(cid:18) ˆπt
n

1 − ˆπ0

−

πt
1 − π0

(cid:19)

=

1 − π0
1 − ˆπ0

Gn[a[t]
i ]

where a[t]

i = (1 − π0)−2(Dt,i(1 − π0) + D0,iπt − πt).

(H.7) rAT E Conditional mean error variance: Recall that εi = ψi(η, π) −

E[ψi(η, π)|Zi]. The conditional mean error for the rAT E has ﬁnite second conditional

moment:

E[ε2

i |Zi] = (1 − π0)−2 (cid:88)

(cid:88)

πtπ(cid:48)

tE[(ψ[t]

i (η) − E[ψ[t]

i (η)|Zi])(ψ[t(cid:48)]

i (η) − E[ψ[t(cid:48)]

i (η)|Zi])|Zi]

t(cid:48)(cid:54)=0
t(cid:54)=0
(cid:20)σ2
t (Xi)
et(Xi)

+ (µt(Xi) − E[µt(Xi)|Zi])2

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Zi

πtπt(cid:48)E[(µt(Xi) − E[µt(Xi)|Zi])(µt(cid:48)(Xi) − E[µt(cid:48)(Xi)|Zi])|Zi]

(cid:88)

(cid:46)P

π2
t E

t(cid:54)=0
(cid:88)

(cid:88)

t(cid:54)=0

t(cid:48)(cid:54)=0

+

(cid:46) J sup
t(cid:54)=0

πt sup
x∈X

πt
et(x)

+ J 2 sup
t(cid:54)=0

π2
t

(cid:46) 1

As supt,x∈X σ2

t (x) + µt(x) is uniformly bounded by Assumptions A.2 and B.1/C.1.
(H.8) Eﬀect of estimating π for decomposition term ∆ = nAT E −rAT E: Note

49

that due to the multiplicative structure of the decomposition we have that, for any η,

ψ[∆]
i

(η, ˆπ) − ψ[∆]

i

(η, π)

= [ψ[nAT E]
i

(η, ˆπ) − ψ[rAT E]

i

=

(cid:88)

t(cid:54)=0

ψ[t,0]
i

(η)

(cid:20)

πt
t(cid:54)=0 πt

(cid:80)

−

(η, ˆπ)] − [ψ[nAT E]
(cid:21)

i

ˆπt
t(cid:54)=0 ˆπt

(cid:80)

(η, π) − ψ[rAT E]

i

(η, π)]

= −(ψ[rAT E]
i

(η, ˆπ) − ψ[rAT E]

i

(η, π))

Thus we can obtain an analogous asymptotically linear representation for the estimator of

∆ in what follows using the rAT E results with a simple sign ﬂip which does not aﬀect the

rates of the approximation bounds in the following. Moreover, the leading term used for

the normality approximation and its asymptotic variance will also be identical up to the

sign ﬂip.

(H.9) Error and error tail bounds For the rAT E, the regression error by deﬁnition

is a convex combination of centered moment functions

εi =

and thus

(cid:80)

t(cid:54)=0 πt(ψ[t]

i (η) − E[ψ[t]
(cid:80)
t(cid:54)=0 πt

i (η)|Zi])

|εi| ≤ sup
t(cid:54)=0

|ψ[t]

i (η) − E[ψ[t]

i (η)|Zi]| (cid:46) sup

t(cid:54)=0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

εi(t)Dt,i
et(Xi)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 1

almost surely due to the bounded conditional means. For the nAT E, however, the

propensity score weights yield

(cid:80)

εi =

t(cid:54)=0 Dt,iεi(t) + et(Xi)µt(Xi) − E[Dt,iεi(t) + et(Xi)µt(Xi)|Zi]
t(cid:54)=0 et(Xi)

(cid:80)

and thus

almost surely.

|εi| (cid:46) sup
t(cid:54)=0

|εi(t)| + 1

50

A.2.2 Machine Learning Bias: nAT E

For the nAT E the ψi(η)-function has components

(cid:80)

µi =

t(cid:54)=0 et(Xi)µt(Xi)
(cid:80)
t(cid:54)=0 et(Xi)

, ei =

(cid:88)

t(cid:54)=0

et(Xi), Di =

(cid:88)

t(cid:54)=0

Dt,i, Yi =

(cid:88)

t

Dt,iYi(t)

and equivalently for ψ(ˆη). We now verify the rates in SC, Assumption 3.5: For Bn, we

have that

E[bi(a.1)] = E[bi[ˆµi − µi](1 − E[Di|Xi]/ei)]

= 0

E[bi(a.2)] = E[bi[ˆei − ei](E[YiDi|Xi] − µiE[Di|Xi])/(eiˆei)]

= 0

||E[bi(a.3)]|| (cid:46) sup
x∈X
√

(1 − e0(x))−1E[||bi||2]1/2E[(ˆei − ei)2(ˆµi − µi)2]1/2

(cid:46)

kE[(ˆei − ei)2h2]

1

2h2 E[(ˆµi − µi)2h1]

1
2h1

√

√

(cid:46)

(cid:46)

kJ s0,n,2h2

(cid:18)

sup
t(cid:54)=0

(cid:18)

st,n,2h1 + sup

t(cid:54)=0,x∈X
(cid:19)

ks0,n,2h2

Jsn,2h1 + mn,2h1

(cid:19)

et(x)mt,n,2h1

for some 1/h1 + 1/h2 = 1 by Hölder’s inequality. The aggregate ˆµi rate follows from

Assumption A.3 together with expanding

ˆet(x)ˆµt(x) − et(x)µt(x) = et(x)(ˆµt(x) − µt(x)) + µt(x)(ˆet(x) − et(x))

+ (ˆet(x) − et(x))(ˆµt(x) − µt(x))

for all t. As potential outcome means are uniformly bounded by A.2, this yields that

E[(ˆµi − µi)c]1/c (cid:46) J

(cid:18)

sup
t(cid:54)=0

st,n,c + sup

et(x)mt,n,c

t(cid:54)=0,x∈X

(cid:19)

51

for any c ≥ 2. Overall we have that

B[nAT E]

n

=

√

n sup
ˆη∈Hn

||E[bi(ψnAT E

i

(ˆη) − ψnAT E

i

(η))]||

√

nks0,n,2h2

(cid:46)

(cid:18)

(cid:19)

Jsn,2h1 + mn,2h1

For Λn note that:

E[||bi(ψ(ˆη) − ψi(η))||2] (cid:46) ξ2

kE[(ψi(ˆη) − ψi(η))2].

Decomposing the second term on the right hand side exploiting Assumption A.3 together

with independence of the nuisance models and the conditional independence of the potential

outcomes yields:

E[(a.1)2] = E[(ˆµi − µi)2(1 − Di/ei)2]

= E[(ˆµi − µi)2(1 − 2 + 1/ei)]

(cid:46) E[(ˆµi − µi)2]

E[(a.2)2] = E[(ˆei − ei)2(Yi − µi)2Di/(e2

i ˆe2

i )]

= E[(ˆei − ei)2E[(Yi − µi)2|Xi, Di = 1]/(eiˆe2

i )]

(cid:46) E[(ˆei − ei)2]

E[(a.3)2] = E[(ˆµi − µi)2(ˆei − ei)2Di/(e2

i ˆe2

i )]

= E[(ˆµi − µi)2(ˆei − ei)2/(eiˆe2

i )]

(cid:46) E[(ˆµi − µi)2]

where the last inequality uses a simple constant bound on the control propensities. Now

note that here ei denotes the aggregate treatment propensity with uniformly bounded

inverse due to A.3. Thus, using the convergence rate for the aggregate mean ˆµi above, we

52

obtain that

Λ[nAT E]

n

= E[||bi(ψ(ˆη) − ψi(η))||2]1/2
√

(cid:18)

(cid:18)

(cid:46) ξk

s0,n,2 +

J

st,n,2 + sup

sup
t(cid:54)=0

t(cid:54)=0,x∈X
(cid:19)

Jsn,2 + J −1mn,2

(cid:19)(cid:19)

mt,n,2et(x)

(cid:18)

s0,n,2 +

√

(cid:46) ξk

by Assumption A.3.

A.2.3 Machine Learning Bias: rAT E and ∆

First note that, conditional on the event un

max
1≤i≤n

πt
ˆet(Xi)

= max
1≤i≤n

et(Xi)
ˆet(Xi)

πt
et(Xi)

πt
et(x)

sup
x∈X

≡ χt,n

(cid:46)P sup
x∈X

sup
ˆet∈Et,n

(cid:46)P

sup
ˆet∈Et,n,x∈X

et(x)
ˆet(x)
et(x)
ˆet(x)

(cid:46)P 1

by deﬁnition of the supremum and Assumptions A.3 and A.4. Note that this also

implies that supt(cid:54)=0 χt,n (cid:46)P 1. Now, for the rAT E, we exploit the same moment function
decomposition as for the nAT E but for each potential outcome moment function within

the weighted sum. Omitting the control part ψ[0]

i (η) again, we have that

ψi(ˆη, π) − ψi(η, π) =

(cid:20) (cid:88)

(cid:21)−1

πt

t(cid:54)=0

(cid:88)

t(cid:54)=0

(cid:20)

(cid:18)

πt

(ˆµt(Xi) − µt(Xi))

1 −

(cid:19)

Dt,i
et(Xi)

− (ˆet(Xi) − et(Xi))(Yi − µt(Xi))

+ (ˆµt(Xi) − µt(Xi))(ˆet(Xi) − et(Xi))

(cid:20) (cid:88)

(cid:21)−1

πt

=

t(cid:54)=0

(cid:88)

t(cid:54)=0

(cid:18)

πt

At,1 − At,2 + At,3

.

Dt,i
et(Xi)ˆet(Xi)
Dt,i
ˆet(Xi)et(Xi)
(cid:19)

(cid:21)

53

We now consider B[rAT E]

n

. Equivalently to the nAT E we have that

E[biAt,1] = E[biAt,2] = 0.

for all t = 1, . . . , J. The remaining term can be bounded as

(cid:88)

||E[bi

πtAt,3]||

t(cid:54)=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[bi

(cid:88)

t(cid:54)=0

πt(ˆµt(Xi) − µt(Xi))(ˆet(Xi) − et(Xi))

Dt,i
ˆet(Xi)et(Xi)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤P

(cid:88)

t(cid:54)=0

πt max
1≤i≤n

√

(cid:46)P J sup
t(cid:54)=0

χt,n

et(Xi)−1E[||bi(ˆµt(Xi) − µt(Xi))(ˆet(Xi) − et(Xi))||]

kE[(ˆµt(Xi) − µt(Xi))2(ˆet(Xi) − et(Xi))2]1/2

√

kJsn,2h1mn,2h2

(cid:46)

by Hölder’s inequality with 1/h1 + 1/h2 = 1. Thus, overall we have that

√

B[rAT E]

n

(cid:46)

nkJsn,2h1mn,2h2.

Now consider Λn. First note that

E[(ψi(ˆη, π) − ψ(η, π))2]
(cid:20) (cid:88)

(cid:88)

(cid:21)−2

=

πt

(cid:88)

(cid:20)(cid:18)

(cid:19)(cid:18)

(cid:19)(cid:21)

πtπsE

At,1 − At,2 + At,3

As,1 − As,2 + As,3

t(cid:54)=0

t(cid:54)=0

s(cid:54)=0

Now consider the summands for s = t:

t E[(At,1 − At,2 + At,3)2] (cid:46) 4π2
π2

t E[A2

t,1 + A2

t,2 + A2

t,3]

54

Bounding each term separately yields

t E[A2
π2

t E[(ˆµt(Xi) − µt(Xi))2(1 − 2 + 1/et(Xi))]
t,1] = π2
(cid:19)
(cid:18)

+ πt

E[(ˆµt(Xi) − µt(Xi))2]

(cid:46) πt

(cid:12)
(cid:12)
(cid:12)
(cid:12)

πt
et(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
x∈X

(cid:46) J −1m2

t,n,2

t E[A2
π2

t,2] = π2

t E[(ˆet(Xi) − et(Xi))2E[(Yi(t) − µt(Xi))2|Xi]/(et(Xi)ˆet(Xi)2)]

π3
t
et(x)3 χ2

t,nE[(ˆet(Xi) − et(Xi))2]

(cid:46)P π−1

t

sup
x∈X

≤ Js2

t,n,2

t E[A2
π2

t,3] = π2

t E[(ˆet(Xi) − et(Xi))2(ˆµt(Xi) − µt(Xi))2/(et(Xi)ˆet(Xi)2)]

≤ Js2

t,n,2

by Assumptions A.3 and A.4. Now consider the summands with s (cid:54)= t. Recall that

Dt,iDs,i = 0. As a preliminary, note that (conditional on the cross-ﬁtted model):

(cid:20)(cid:18)

E

1 −

(cid:20)(cid:18)

E

(cid:20)(cid:18)

E

1 −

1 −

Dt,i
et(Xi)

(cid:19)

(cid:19)(cid:18)

1 −

Dt,i
et(Xi)
Dt,i
et(Xi)
(cid:19)

Ds,i
es(Xi)
Ds,i
es(Xi)ˆes(Xi)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ds,i(Yi(s) − µs(Xi))
(cid:12)
(cid:12)

Xi

Xi

(cid:21)

(cid:21)

(cid:21)

= −1

=

1
ˆes(Xi)

Xi

= 0

Thus, summands simplify to

(cid:20)

πtπsE

− (ˆµt(Xi) − µt(Xi))(ˆµs(Xi) − µs(Xi))

+(ˆµt(Xi) − µt(Xi))(ˆµs(Xi) − µs(Xi))

(cid:20)

(cid:46)

(cid:21)

(cid:20)(cid:18)

(cid:18) ˆet(Xi) − et(Xi)
ˆet(Xi)

(cid:19)(cid:21)

+

ˆes(Xi) − es(Xi)
ˆes(Xi)

(cid:21)

(cid:19)

πtπsmt,n,2ms,n,2

+

πsst,n,2χt,n + πtss,n,2χs,n

mt,n,2h1ms,n,2h2

55

by repeated application of Hölder’s inequality with 1/h1 + 1/h2 = 1. Note that we have J

variances and J(J − 1) times the covariance term. Thus, we obtain

Λ[rAT E]

n

(cid:46) ξkE[(ψi(ˆη, π) − ψ(η, π))2]1/2

(cid:18)

(cid:46)p ξk

m2

n,2 + J 2s2

n,2 + J(J − 1)(J −2m2

n,2) + J(J − 1)(J −1sn,2mn,2h1mn,2h2)

(cid:19)1/2

(cid:46) ξk(mn,2 + Jsn,2 + (cid:112)Jsn,2mn,2h1mn,2h2)

A.3 Asymptotic Linearization and Normality

Without loss of generality, we set Q = I but assume a random design, i.e. Q is unknown

as in BCCK. Derivations are split by nAT E and rAT E/∆ and thus quantities depending

on ψi(·), ri, εi, lk, ck, β0 are decomposition parameter speciﬁc if not stated otherwise.

A.3.1 Linearization

We ﬁrst expand the ˆQ-weighted estimator around the best linear predictor. Then, we

control for the machine learning bias and the additional uncertainty from estimating π. In

the end, we combine these rates with the rates from estimating ˆQ to obtain an asymptotic

linearization of the series estimator.

rAT E/∆: Take a sequence of basis function b = bn such that ||b|| = 1. Decompose

√

nb(cid:48)(En[biψi(ˆη, ˆπ)] − ˆQβ0) =

√

nb(cid:48)En[bi(ψi(η, π) − b(cid:48)
√

iβ0)]

nb(cid:48)En[bi(ψi(ˆη, π) − ψi(η, π))]

√

nb(cid:48)En[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))]

+

+

The ﬁrst term will be part of the ﬁrst order asymptotics used for the normality results

later. Now by the derivation in Section A.2.3 and Chebyshev’s inequality we have that

√

||

nb(cid:48)En[bi(ψi(ˆη, π) − ψi(η, π))]|| (cid:46)P B[rAT E]

n

+ Λ[rAT E]
n

56

The third term can further be decomposed exploiting the multiplicative structure and

(H.6):

√

nEn[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))]

√

nEn

(cid:20)
bi

(cid:88)

t(cid:54)=0

ψ[t]

i (ˆη)

(cid:18) ˆπt

1 − ˆπ0

(cid:19)(cid:21)

−

πt
1 − π0

En[biψ[t]

i (ˆη)]

√

(cid:18) ˆπt

1 − ˆπ0

n

−

(cid:88)

t(cid:54)=0

(cid:19)

πt
1 − π0
(cid:19)

(ˆγt − γt + γt)

− 1 + 1

Gn[a[t]
i ]

(cid:18) 1 − π0
1 − ˆπ0

=

=

=

(cid:88)

t(cid:54)=0

= (ˆγ − γ)Gn[ai]

(cid:18) 1 − π0
1 − ˆπ0

(cid:19)

− 1 + 1

+

(cid:18) 1 − π0
1 − ˆπ0

(cid:19)

− 1

γGn[ai] + γGn[ai]

where γt = E[biψ[t,0]

i

(η)] = E[biτt(Xi)], γ = (γ1 . . . γJ ), and ai = (a[1]
i

. . . a[J]

i )(cid:48). Now note

that, by the iid assumption, Chebyshev’s inequality yields

||Gn[ai]|| (cid:46)P J sup
t(cid:54)=0

Thus, by (H.5), we obtain that

E[(a[t]

i )2] (cid:46) J sup

t(cid:54)=0

πt (cid:46) 1

√

nb(cid:48)En[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))] = b(cid:48)γGn[ai] + Rn,π

where

||Rn,π|| (cid:46)P ||ˆγ − γ|| ||Gn[ai]|| (1 + n−1/2) + n−1/2||γ|| ||Gn[ai]||

√

k(n−1/2 + mn,2 + Jsn,2) + n−1/2

√

Jk

(cid:46)P J
√

(cid:46) J

k(n−1/2 + mn,2 + Jsn,2)

as || 1−π0
1−ˆπ0

− 1|| (cid:46)P n−1/2 as 1 − π0 is bounded away from zero. Recall that ψi(η, π) − b(cid:48)

iβ0 =
εi + ri. Now what is left is the remainder due to estimation using ˆQ. We make use of

57

(H.4) for bounding || ˆQ − I||. Conditional on the data, observe that

V [b(cid:48)( ˆQ−1 − I)Gn[biεi]|Z1, . . . , Zn] (cid:46) b(cid:48)( ˆQ−1 − I) ˆQ( ˆQ−1 − I)b

(cid:46)P

ξ2
k log k
n

Moreover,

|b(cid:48)( ˆQ−1 − I)Gn[biri]| (cid:46)P

(cid:114)

ξ2
k log k
n

√

k

lkck

as in BCCK, Proof of Lemma 4.1 and using (H.7). For the ﬁnal term note that supz∈Z

||E[aia(cid:48)

i|Zi = z]|| (cid:46) J −1/2 as in (H.5). This yields

V [b(cid:48)( ˆQ−1 − I)Gn[γai]|Z1, . . . , Zn] = b(cid:48)( ˆQ−1 − I)γE[aia(cid:48)

i|Zi]γ(cid:48)( ˆQ−1 − I)b

(cid:46)P || ˆQ−1||2 || ˆQ−1 − I||2|| ||γ||2J −1/2
(cid:46) ξ2

√

k

J

k log k
n

Thus, decomposing and centering the linear predictor yields:

√

nb(cid:48)( ˆβ − β0) = b(cid:48)Gn[bi(εi + ri) + γai] + Rn,Q + Rn,π + Rn,η

where

||Rn,Q|| (cid:46)P

(cid:114)

(cid:18)

ξ2
k log k
n

1 + k1/2(J 1/4 + lkck)

(cid:19)

||Rn,π|| (cid:46)P J

√

k(n−1/2 + mn,2 + Jsn,2)

||Rn,η|| (cid:46)P B[rAT E]

n

+ Λ[rAT E]
n

nAT E: For the nAT E there is no Gn[ai] term as there are no unconditional probability

terms π to be estimated. Moreover, the machine-learning approximation rates are diﬀerent

as shown above. All other derivations follow along the same lines. Thus, we obtain for the

58

nAT E

where

√

nb(cid:48)( ˆβ − β0) = b(cid:48)Gn[bi(εi + ri)] + Rn,Q + Rn,η

||Rn,Q|| (cid:46)P

(cid:114)

ξ2
k log k
n

(cid:18)

1 + k1/2lkck

(cid:19)

||Rn,η|| (cid:46)P B[nAT E]

n

+ Λ[nAT E]
n

where here εi = ψ[nAT E]

i

(η) − E[ψ[nAT E]

i

(η)|Zi] and ri = E[ψ[nAT E]

i

(η)|Zi] − b(cid:48)

iβ0.

A.3.2 Asymptotic Normality

rAT E/∆: Again let Q = I without loss of generality. Under Assumption C.4 all

remainders from the previous subsection are op(1) and we are left with leading term

b(cid:48)Gn[bi(εi + ri) + γai]
||b(cid:48)Ω1/2||

=

n
(cid:88)

i=1

b(cid:48)[bi(εi + ri) + γai]
n||b(cid:48)Ω1/2||

√

We now verify the Lindeberg condition for asymptotic normality. First note that the term

above has expectation zero. By independence and the binomial formula we obtain for any

δ > 0

n
(cid:88)

E

i

(cid:20)(b(cid:48)(bi(εi + ri) + γai))2
nb(cid:48)Ωb
(cid:20)(b(cid:48)bi(εi + ri))2
nb(cid:48)Ωb

≤ 4nE

(cid:18) |b(cid:48)(bi(εi + ri) + γai)|
1

√

n||b(cid:48)Ω1/2||

(cid:19)(cid:21)

> δ

(cid:18) |b(cid:48)bi(εi + ri)|
1
n||b(cid:48)Ω1/2||

√

(cid:19)(cid:21)

>

δ
2

+ 4nE

(cid:20) (b(cid:48)γai)2
nb(cid:48)Ωb

(cid:18) |b(cid:48)γai|
1

√

n||b(cid:48)Ω1/2||

(cid:19)(cid:21)

>

δ
2

≡ (an.1) + (an.2)

Now denote

wni :=

b(cid:48)bi
||b(cid:48)Ω1/2
1

||

⇒ |wni| (cid:46) ξk√
n

, nE[|wni|2] (cid:46) 1

59

analogously to BCCK Proof of Theorem 4.2 using the conditional moment bound (H.7).

Now note that, by the eigenvalue assumption C.6, (an.1) is bounded by:

(an.1) (cid:46) nb(cid:48)Ω1b
nb(cid:48)Ωb

E

(cid:20)(b(cid:48)bi(εi + ri))2
nb(cid:48)Ω1b

(cid:18) |b(cid:48)bi(εi + ri)|
1
n||b(cid:48)Ω1/2
||

√

1

(cid:19)(cid:21)

>

δ
2

||b(cid:48)Ω1/2||
||b(cid:48)Ω1/2
1

||

(cid:46) 2nE[|wni|2ε2
i

1(|εi| + |ri| > δ/|wni|)] + 2nE[|wni|2 sup
z∈Z

|r(z)|21(|εi| + |ri| > δ/|wni|)]

≡ (an.1.i) + (an.1.ii)

Using C.2 supz |r(z)| ≤ lkck and (H.9), we obtain that

(an.1.i) (cid:46) nE[|wni|2E[ε2
i

1(|εi| > (δ

√

(cid:46) sup
z∈Z

E

(cid:20)

sup
t(cid:54)=0

(cid:46) sup
z∈Z

sup
t(cid:54)=0

sup
x∈X

εi(t)2Dt,i
et(Xi)2
π2
t
et(x)2 π−2

t E[sup
t(cid:54)=0

n/(2cξk) − lkck))|Zi]]

(cid:18) |εi(t)|Dt,i
1
et(Xi)

>

√

n
δ
2ξk

− lkck

(cid:21)

Zi = z

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

εi(t)2Dt,i1(|εi(t)|Dt,i > cn/J)|Zi = z]

where cn =

(cid:18)

√

δ
n
2ξk

− lkck

(cid:19)

. Now by the integrated tail formula we have that

E[ sup
t(cid:54)=0

εi(t)2Dt,i1(|εi(t)|Dt,i > cn/J|Zi = z)]

=

≤

(cid:90) ∞

0
(cid:88)

t(cid:54)=0

P (sup
t(cid:54)=0

εi(t)2Dt,i1(|εi(t)|Dt,i > cn/J) > w|Zi = z)dw

P (Dt,i = 1|Zi = z)

(cid:90) ∞

0

P (εi(t)21(|εi(t)| > cn/J) > w|Zi = z, Dt,i = 1)dw

(cid:46) J sup
t(cid:54)=0

πt

(cid:90) ∞

0

P (εi(t)21(|εi(t)| > cn/J) > w|Zi = z, Dt,i = 1)dw

(cid:46) sup
t(cid:54)=0

E[εi(t)21(|εi(t)| > cn/J)|Zi = z, Dt,i = 1]

Note that, by Markov’s inequality and the conditional m-th moment bound in C.1, we

have that

P (|εi(t)| > cn/J|Zi = z, Dt,i = 1) (cid:46) (cn/J)−m

60

Hölder’s inequality then yields for the equation above

E[εi(t)21(|εi(t)| > cn/J)|Zi = z, Dt,i = 1] ≤

(cid:18)

E[|εi(t)|m|Zi = z, Dt,i = 1](cn/J)−m

(cid:19)2/m

(cid:46)

(cid:20)(cid:18)δ

√

n
2cξk

− lkck

(cid:21)−2

(cid:19) 1
J

for any t (cid:54)= 0. Plugging this back into (an.1.i) and using A.3 supt(cid:54)=0 πtJ (cid:46) 1 yields

(an.1.i) (cid:46)

(cid:20)(cid:18)δ

√

n
2cξk

− lkck

(cid:21)−2

J 2

(cid:19) 1
J

kJ 4
(cid:46) ξ2
n

As δ

√

n/ξk − lkck → ∞. For (an.1.ii) it follows equivalently to BCCK, Proof of Theorem

4.2, that

(an.1.ii) (cid:46) l2

kc2

k sup
z∈Z

P (|εi| > cδ

√

n/ξk − lkck|Zi = z)

Analogously to the derivations for (an.1.i) using (H.9) we obtain that

P (|εi| > cn|Zi = z) (cid:46) (cid:88)

P (|εi(t)| > cn/J|Zi = z, Dt,i = 1)P (Dt,i = 1|Zi = z)

t(cid:54)=0

(cid:46) sup
t(cid:54)=0

P (|εi(t)| > cn/J|Zi = z, Dt,i = 1)

(cid:46) sup
t(cid:54)=0

E[|εi(t)|m|Zi = z, Dt,i = 1]
(cn/J)m

(cid:46) (cn/J)−m

where the last two steps follow from Markov’s inequality and the conditional moment

bound. Plugging this back into (an.1.ii) yields

(an.1.ii) (cid:46)

(cid:18)

√

(lkck) 2
m J
n/ξk − lkck]

[δ

(cid:19)m

61

Now consider (an.2). Note that by (H.5), we have that

(an.2) ≤

||γ||2
b(cid:48)Ωb

E[||ai||21(||ai|| > (δ/2)||b(cid:48)Ω1/2||/||γ||)]

(cid:46) ||γ||2E[||ai||21(||ai|| > Cδ(cid:112)n/kJ)]

(cid:46) kJ

(cid:88)

t(cid:54)=0

E[(a[t]

i )21(||ai|| > Cδ(cid:112)n/kJ)]

(cid:46) kJ 2P (||ai|| > Cδ(cid:112)n/kJ)

(cid:46) kJ 2P (sup
t(cid:54)=0

(a[t]
i )

√

J > Cδ(cid:112)n/kJ)

P (|a[t]

i | > Cδ(cid:112)n/kJ 2)

(cid:46) kJ 3 sup
t(cid:54)=0

= o(1)

if (cid:112)n/kJ 2 → ∞ as a[t]

i

is uniformly bounded for all t. Thus using Assumption C.5 yields

kJ 4
(an.1) + (an.2) (cid:46) ξ2
n

(cid:18)

+

√

(lkck) 2
m J
n/ξk − lkck]

[δ

(cid:19)m

= o(1)

nAT E: First note that there is no (an.2) term for the nAT E. For (an.1) most derivations

follow analogously. However, due to the propensity score weighting we can use (H.9) and

improve on some of the rates in (an.1.i)

E[ε2
i

1(|εi| > cn)|Zi = z] (cid:46) E[sup
t(cid:54)=0

εi(t)21(|εi(t)| > cn)|Zi = z]

E[εi(t)21(|εi(t)| > cn)|Zi = z]

(cid:46)P J sup
z∈Z

sup
t(cid:54)=0

(cid:46) Jc−2
n

Similarly for (an.1.ii), we have that

P (|εi| > cn) ≤ J sup
t(cid:54)=0

P (|εi(t)| > cn)

(cid:46) Jc−m

n

62

Plugging both into (an.1) then with cn as above yields

(an.1) (cid:46) J

[δ

√

n/ξk − lkck]2

+ J

kc2
l2
k
n/ξk − lkck]m

√

[δ

= o(1)

under the assumption B.5. Note that convergence is faster than (an.1) for the rAT E.

Asymptotic normality then follows from the suﬃciency of the Lindeberg condition. (cid:3)

B Supplementary Appendix

B.1 Toy example

Consider a setting with a binary heterogeneity variable Xi ∈ {0, 1} and three eﬀective treat-

ments Ti ∈ {0, 1, 2}. We impose deterministic potential outcomes that are homogeneous

within treatment status, but heterogeneous between treatments:

Yi(0) Yi(1) Yi(2)

Xi = 0

Xi = 1

0

0

-1

-1

1

1

Both groups deﬁned by Xi have the same potential outcomes under the diﬀerent

treatments. This means there can be no real eﬀect heterogeneity. However, consider now

that the probability to receive the eﬀective treatments varies with Xi:

P (Ti = 0|Xi) P (Ti = 1|Xi) P (Ti = 2|Xi)

Xi = 0

Xi = 1

0.5

0.5

1/8

3/8

3/8

1/8

Collapsing treatments one and two into a binary treatment Di = 1(Ti > 0) and running

a subgroup analysis for the "treatment" Di results in the following conditional average

treatment eﬀects (CAT E):

CAT E(Xi) = 1 − 4 · P (Ti = 1|Xi) =






0.5

if Xi = 0

−0.5

if Xi = 1.

63

Thus, the aggregation into the binary indicator leads us to "ﬁnd" a positive eﬀect

for one group and a negative eﬀect for another group although the eﬀective treatments

actually do not create heterogeneous eﬀects. Everything is just driven by them receiving a

diﬀerent mix of eﬀective treatments.

B.2

Identiﬁcation

B.2.1 Conditional independencies

We can read oﬀ the conditional independencies with respect to potential, not observed,

outcomes encoded in DAGs (1) and (2) from single-world intervention graphs (SWIG)

of Richardson and Robins (2013). We intervene on Ti to read oﬀ the independencies we

require for identiﬁcation of our decompositions.

Scenario 1:

Figure B.1: SWIG with intervention on Ti

D(t)

T |t

Y (t)

X

Scenario 2:

Figure B.2: SWIG with intervention on Ti

D

T |t

Y (t)

X

Both SWIGs (B.1) and (B.2) imply the conditional independence shown in Equation

(2). This links the observed eﬀective treatment to the unobserved potential outcomes and

justiﬁes our assumption 1a required for identifying our decomposition terms.

64

Figure B.3: DAG: Confounded binary treatment precedes confounded treatment version:

Confounders ˜X

Binary Treatment D

Treatment Version T

Outcome Y

Confounders X

B.2.2

Identiﬁcation in Scenario 2 with confounded binary treatment

Figure B.3 considers the case of a binary treatment that is potentially confounded with

both treatment version selection and outcome due to the backdoor through ˜X. This could

occur e.g. in the case of the evaluation of Job Corps access on earnings when access was

not allocated randomly but is based on observables. In this case, the derived conditional

independence assumptions change to

Yi(0), Yi(1) . . . , Yi(J) ⊥⊥ Di | Ti, ˜Xi

Yi(0), Yi(1) . . . , Yi(J) ⊥⊥ Ti | Xi, ˜Xi

(14)

(15)

Thus, the adjustment set required for identiﬁcation and entering estimation of the nui-

sance parameters of the decomposition terms needs to incorporate the additional set of

confounders ˜Xi, but all results hold equivalently.

B.3 Connection to Continuous Treatments

The unconditional rAT E parameter can be seen as a discrete approximation to the

integrated continuous eﬀect curve under the normalization that µ0 = 0. In particular,

assume that the treatment now is continuous, i.e. t ∈ J ∗ ⊂ R, with J ∗ being a compact

subset of the real line, e.g. [0, 1]. Kennedy et al. (2017), Equation (2) denotes the integrated

eﬀect curve of a continuous treatment as

(cid:90)

(cid:90)

J ∗

X

µ(t, x)π(t)dF (x)dt =

(cid:90)

J ∗

µ(t)dt

(16)

65

where µ(t, x) = E[Yi(t)|Xi = x] and π(t) being the marginal treatment density. Now

consider a discretization of the support J ∗ using J steps that deﬁne J multi-valued

treatments, e.g {1/J, 2/J, . . . , 1}. Let µt and πt be the corresponding discrete multi-valued

potential outcomes and cell probability/ propensity score. If µ(t) is continuous, then it is

straightforward to show that

lim
J→∞

rAT E = lim
J→∞

(cid:80)J

j=1 πtµt
(cid:80)J
j=1 πt

(cid:90)

=

J ∗

µ(t)dt

(17)

Thus, when we allow for J → ∞, the rAT E can arbitrarily well approximate/nest the

integrated continuous eﬀect curve. The explicit choice of discretization corresponds to the

bandwidth selection that has to be made when estimating this quantity (Kennedy et al.,

2017).

B.4 Neyman-orthogonality

The key insight required here is that the nAT E and rAT E scores are Neyman-orthogonal

with known unconditional probabilities πt, t = 1, . . . , J. We show how the additional

estimation error can be incorporated in Appendix A. Here we are concerned with the

Gateaux derivative of the nAT E and rAT E scores with respect to the vector of inﬁnite-

dimensional nuisance parameters η = (µ(x), p(x)) = (µ0(x), . . . , µJ (x), e0(x), . . . , eJ (x))(cid:48).

As π is assumed to be known, we suppress dependence ψ(η, π) = ψ(η) out of convenience

for now. Suppressing also the dependencies of the nuisance parameters on x, we write the

path-wise derivative of the conditional expectation of a score with respect to the vector of

nuisance parameters as

∂ηE[ψi(η)|Xi = x] = ∂rE[ψi(. . . , µt + r(˜µt − µt), . . . , et + r(˜et − et), . . . )|Xi = x]|r=0

66

First, we revisit Neyman-orthogonality of the doubly robust score:

∂rE[ψ[t]

i (η + r(˜η − η))|Xi = x]|r=0

(cid:20)

= ∂rE

(µt + r(˜µt − µt)) +

Dt,iYi
et + r(˜et − et)

−

Dt,i(µt + r(˜µt − µt))
et + r(˜et − et)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi = x

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)r=0

= (˜µt − µt) −

etµt(˜et − et)
e2
t

−

e2
t (˜µt − µt) − etµt(˜et − et)
e2
t

= 0

where we use that E[Dt,iYi|Xi = x] = E[Dt,i

(cid:80)

t Dt,iYi(t)|Xi = x] = E[Dt,iYi(t)|Xi =

x] = etµt by the observational rule and Assumption 1.

B.4.1

rATE

As the rAT E score is a linear combination of doubly robust scores, it inherits the Neyman-

orthogonality of its components:

∂rE[ψ[rAT E]
i
(cid:88)

=

πt
1 − π0

t(cid:54)=0

(η + r(˜η − η))|Xi = x]|r=0

∂rE[ψ[t]

i (η + r(˜η − η))|Xi = x]|r=0

− ∂rE[ψ[0]

i (η + r(˜η − η))|Xi = x]|r=0

= 0

67

B.4.2 nATE

The nAT E score diﬀers from the standard doubly robust scores but can still be shown to

be Neyman-orthogonal:

∂rE[ψ[nAT E]
i

(η + r(˜η − η))|Xi = x]|r=0
(cid:34)(cid:80)

= ∂rE

t(cid:54)=0[(µt + r(˜µt − µt))(et + r(˜et − et))]
t(cid:54)=0(et + r(˜et − et))

(cid:80)

+

(cid:80)

(cid:80)

Di

−

t(cid:54)=0[(µt + r(˜µt − µt))(et + r(˜et − et))]
t(cid:54)=0(et + r(˜et − et))]2

[(cid:80)

DiYi
t(cid:54)=0(et + r(˜et − et))
(cid:35) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)r=0
(cid:12)

Xi = x

− ∂rE[ψ[0]
(cid:80)

i (η + r(˜η − η))|Xi = x]|r=0

t(cid:54)=0[µt(˜et − et) + et(˜µt − µt)] (cid:80)

t(cid:54)=0 et

[(cid:80)
t(cid:54)=0 et]2
t(cid:54)=0(˜et − et)

(cid:80)

(cid:80)

t(cid:54)=0 etµt
[(cid:80)
t(cid:54)=0 µtet
[(cid:80)

(cid:80)

t(cid:54)=0 et]2
(cid:80)

t(cid:54)=0 et]2

t(cid:54)=0(˜et − et)

=

−

+ 2

= 0

(cid:80)

−

t(cid:54)=0 µtet
[(cid:80)

(cid:80)

t(cid:54)=0(˜et − et)

t(cid:54)=0 et]2

t(cid:54)=0[µt(˜et − et) + et(˜µt − µt)]

[(cid:80)

t(cid:54)=0 et]2

(cid:80)

t(cid:54)=0 et

−

(cid:80)

− ∂rE[ψ[0]

i (η + r(˜η − η))|Xi = x]|r=0

where we use that E[DiYi|Xi = x] = E[Di

t(cid:54)=0 etµt by the
observational rule and Assumption 1. Consequently, the diﬀerence between the nAT E

t Dt,iYi(t)|Xi = x] = (cid:80)

(cid:80)

and rAT E score that forms the ∆ score is Neyman-orthogonal as well:

∂ηE[ψ[nAT E]
i

(η) − ψ[rAT E]
i

(η)|Xi = x] = 0

B.5 Estimation of Asymptotic Variance

Let En[Xi] = 1
n

(cid:80)n

i=1 Xi. Deﬁne

ˆQ = En[b(Zi)b(Zi)(cid:48)]

ˆΩ = ˆQ−1 ˆΣ ˆQ−1

(18)

68

For the rAT E we use

(cid:20)

ˆΣ = En

(b(Zi)ei + ˆai − ¯ˆai)(b(Zi)ei + ˆai − ¯ˆai)(cid:48)

(cid:21)

with

ei = ψ[rAT E]
i

(ˆη, ˆπ) − b(Zi)(cid:48) ˆβ

ˆai =

(cid:88)

t(cid:54)=0

En[b(Zi)(ψ[t]

i (ˆη) − ψ[0]

i (ˆη))](Dt,i(1 − ˆπ0) + D0,iˆπt)
(1 − ˆπ0)2

ˆ¯ai = En[ˆai]

ˆπt = En[Dt,i]

For ∆, the ψ[rAT E]

i

(ˆη, ˆπ) has to be replaced by the corresponding score function and ˆΣ

changes to

(cid:20)

ˆΣ = En

(b(Zi)ei − ˆai + ¯ˆai)(b(Zi)ei − ˆai + ¯ˆai)(cid:48)

(cid:21)
.

For the nAT E we use only ˆΣ = En[b(Zi)b(Zi)(cid:48)e2

i ] as there are no estimated unconditional

weights.

B.6 Asymptotic Variance Estimation Theory

In the following, we use some of the Lemmas in BCCK. To do so, we impose the following

additional assumption. We require that ξ2m/(m−2)

k

log k/n (cid:46) 1, log ξk (cid:46) log k and Lipschitz

constant

ξL
k :=

sup
x,x(cid:48)∈X ,x(cid:54)=x(cid:48)

||b(x) − b(x(cid:48))||
||x − x(cid:48)||

obeys Condition (A.5) from BCCK, i.e. log ξL
k
(cid:113) ξ2

√

√

(cid:18)

(cid:19)

k log k
n

(nJ)1/m√

log k +

klkck

(cid:46) log k. Moreover, cklk (cid:46)

√

log k and

(cid:46)

log k as in Theorem 4.6 by BCCK. Moreover, we

assume Mn,1 = o(1) for the nAT E or Mn,2 = o(1) for rAT E/∆ respectively where Mn,1

and Mn,2 are deﬁned at the end of the section in equations (20) and (19) respectively. We

69

now provide ﬁrst some auxiliary results and then the derivations for rAT E/∆. The rates

for nAT E follow directly by simpliﬁcation.

B.6.1 Auxiliary Results

(MA.1) ψ[t]

i bounds First note that, for any t (cid:54)= 0,

|ψ[t]

i (η)| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Dt,iεi(t)
et(Xi)
πt
et(x)

≤ sup
x∈X

(cid:12)
(cid:12)
+ µt(Xi)
(cid:12)
(cid:12)

π−1
t

|Dt,i||εi(t)| + |µt(Xi)|

Thus

max
1≤i≤n

|ψ[t]

i (η)| (cid:46)P sup

x∈X

πt
et(x)

π−1
t max
1≤i≤n

|εi(t)| + sup
x∈X

|µt(x)|

(cid:46)P Jn1/m

by the Assumption A.2 and B.1/C.1. Moreover

|ψ[t]

i (η)ψ[t(cid:48)]

i (η)| ≤ |ψ[t]

i (η)|2 + |ψ[t(cid:48)]

i (η)|2

≤ 2 sup
t(cid:54)=0

|ψ[t]

i (η)|2

and similarly

|ψ[t]

i (ˆη) − ψ[t]

i (η)||ψ[t(cid:48)]

i (ˆη) − ψ[t(cid:48)]

i (η)| ≤ 2 sup
t(cid:54)=0

|ψ[t]

i (ˆη) − ψ[t]

i (η)|2

Now consider the product of the moment functions for any η. For the square note that

|ψ[t]

i (η)|2 =

=

2

(cid:12)
(cid:12)
+ µt(Xi)
(cid:12)
(cid:12)

(cid:12)
Dt,iεi(t)
(cid:12)
(cid:12)
et(Xi)
(cid:12)
Dt,iεi(t)2
et(Xi)2 + 2µt(Xi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

πt
et(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

≤ sup
x∈X

Dt,iεi(t)
et(Xi)

+ µt(Xi)2

t εi(t)2Dt,i + 2 sup
π−2
x∈X

µt(x) sup
x∈X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

πt
et(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

π−1
t

|εi(t)|Dt,i + sup
x∈X

µt(x)2

(cid:46) π−2

t εi(t)2Dt,i + π−1

t

|εi(t)|Dt,i + 1

70

Looking at the max then yields

sup
t(cid:54)=0

max
1≤i≤n

|ψ[t]

i (η)|2

(cid:46)P sup
t(cid:54)=0

π−2
t max
1≤i≤n

|εi(t)|2 max
1≤i≤n

Dt,i + sup
t(cid:54)=0

π−1
t max
1≤i≤n

|εi(t)| max
1≤i≤n

Dt,i + 1

(cid:46)P J 2n2/m + Jn1/m + 1

(cid:46) J 2n2/m

Equivalently we obtain

sup
t(cid:54)=0

max
1≤i≤n

|ψ[t]

i (ˆη)| ≤ sup
ˆη∈Hn

sup
t(cid:54)=0

max
1≤i≤n

|ψ[t]

i (ˆη)|

(cid:46)P sup
t(cid:54)=0

χt,nπ−1

t max
1≤i≤n

|εi(t)| max
1≤i≤n

Dt,i

(cid:46)P Jn1/m

and

sup
t(cid:54)=0

max
1≤i≤n

|ψ[t]

i (ˆη)|2 ≤ sup
ˆη∈Hn

sup
t(cid:54)=0

max
1≤i≤n

|ψ[t]

i (ˆη)|2

(cid:46)P sup
t(cid:54)=0

χt,nπ−2

t max
1≤i≤n

|εi(t)|2 max
1≤i≤n

Dt,i

(cid:46)P J 2n2/m

(MA.2) κn,1 and κn,2 rates

E[ max
1≤i≤n

|ψi(ˆη, π) − ψi(η, π)|] (cid:46)P

(cid:88)

t(cid:54)=0

πtE[ max
1≤i≤n

|ψ[t]

i (ˆη) − ψ[t]

i (η)|]

(cid:46)P J sup
t(cid:54)=0

πtE[ max
1≤i≤n

|ψ[t]

i (ˆη) − ψ[t]

i (η)|]

|ψ[t]

i (ˆη) − ψ[t]

i (η)|]

≤ sup
t(cid:54)=0

E[ max
1≤i≤n

≤ κn,1

71

E[ max
1≤i≤n

|ψi(ˆη, π) − ψi(η, π)|2] (cid:46)P

(cid:88)

(cid:88)

t(cid:54)=0

t(cid:48)(cid:54)=0

πtπt(cid:48)E[ max
1≤i≤n

|ψ[t]

i (ˆη) − ψ[t]

i (η)||ψ[t(cid:48)]

i (ˆη) − ψ[t(cid:48)]

i (η)|]

(cid:46)P J sup
t(cid:54)=0

πtE[ max
1≤i≤n

|ψ[t]

i (ˆη) − ψ[t]

i (η)|2]

|ψ[t]

i (ˆη) − ψ[t]

i (η)|2]

≤ sup
t(cid:54)=0

E[ max
1≤i≤n

≤ κn,2

(MA.3) vi decomposition For arbitrary η and π deﬁne

ˆβ(η, π) = ˆQ−1En[biψi(η, π)]

Thus ˆβ = ˆβ(ˆη, ˆπ). Rewriting the residual using estimated nuisances then yields

vi = ψi(ˆη, ˆπ) − b(cid:48)
i

ˆβ(ˆη, ˆπ)

= ψi(ˆη, π) − b(cid:48)
i

ˆβ(ˆη, π) + ψi(ˆη, ˆπ) − ψi(ˆη, π) + b(cid:48)

i( ˆβ(ˆη, π) − ˆβ(ˆη, ˆπ))

(MA.4) Unconditional probability rates For any t (cid:54)= 0 the estimation error of the

probability weights is given by

ˆπt
1 − ˆπ0

−

πt
1 − π0

=

ˆπt(1 − π0) − πt(1 − ˆπ0)
(1 − ˆπ0)(1 − π0)

(cid:18)

(cid:46)P

πt|ˆπ0 − π0| + π0|ˆπt − πt|

(1 + Op(|ˆπ0 − π0|))

(cid:19)

(cid:46)P πtn−1/2 + (n/πt)−1/2

(cid:46) (Jn)−1/2

by Chebyshev’s inequality as

E[ˆπt − πt] = 0

E[(ˆπt − πt)2] =

V [Dit]
n

= πt(1 − πt)/n (cid:46) πt/n

and |ˆπ0 − π0| (cid:46)P n−1/2 as control propensities are bounded away from 0 and 1.

72

(MA.5) Maximal impact of nuisances on predictions

max
1≤j≤n

j( ˆβ(ˆη, ˆπ) − ˆβ(ˆη, π))| = max
|b(cid:48)

1≤j≤n

|b(cid:48)
j

ˆQ−1En[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))]|

||b(z)|| || ˆQ−1|| ||En[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))]||

(cid:46)P sup
z∈Z
(cid:114)

(cid:46)P ξk

Jk
n

where the second to last line comes from H.1 - H.3. Moreover,

max
1≤j≤n

j( ˆβ(ˆη, π) − ˆβ(η, π))| = max
|b(cid:48)

1≤j≤n

|b(cid:48)
j

ˆQ−1En[bi(ψi(ˆη, π) − ψi(η, π))]|

(cid:46)P ξk|| ˆQ−1|| ||En[bi(ψi(ˆη, π) − ψi(η, π))]||

(cid:46)P ξkn−1/2(B[rAT E]

n

+ Λ[rAT E]
n

)

(cid:46)P ξkn−1/2

due to Markov’s inequality. The deviation from the best linear predictor follows from

BCCK, Theorem 4.3, under the assumptions stated in the beginning of this section:

max
1≤j≤n

j( ˆβ(η, π) − β0)| (cid:46)P ξk
|b(cid:48)

(cid:114)

log(k)
n

(MA.6) Error term tail bounds The marginal tail bound C.1 imply the following

tail bound for rAT E

max
1≤i≤n

|εi| = max
1≤i≤n

|ψ[rAT E]
i

(η, π) − E[ψ[rAT E]

i

(η, π)|Zi]|

(cid:46)P max
1≤i≤n

(cid:88)

t(cid:54)=0

πtDt,i|εi(t)|
et(Xi)

(cid:46)P max
1≤i≤n

sup
t(cid:54)=0

sup
x∈X

πt
et(x)

|εi(t)|

(cid:88)

t(cid:54)=0

Dt,i

(cid:46)P max
1≤i≤n

sup
t(cid:54)=0

|εi(t)|

73

and equivalently by B.1 for the nAT E with supx∈X

πt
et(x)

replaced by 1. Note that

E[ max
1≤i≤n

sup
t(cid:54)=0

|εi(t)|] =

(cid:90) (nJ)1/m

−∞
(cid:90) ∞

+

P ( max
1≤i≤n

sup
t(cid:54)=0

|εi(t)| > w)dw

P ( max
1≤i≤n

sup
t(cid:54)=0

|εi(t)| > w)dw

(nJ)1/m

≤ (Jn)1/m +

(cid:90) ∞

(nJ)1/m

≤ (Jn)1/m + (Jn)1/m

(cid:88)

n

t(cid:54)=0
(cid:90) ∞

P (|εi(t)| > w)dw

w1−1/mP (|εi(t)| > w)dw

(nJ)1/m

(cid:46) (Jn)1/m(1 + o(1))

(cid:46) (Jn)1/m

where the second term is convergent due to the conditional moment bound B.1/C.1 for

εi(t). Overall, this implies that max1≤i≤n |εi| (cid:46)P (Jn)1/m by Markov’s inequality.

B.6.2 Deﬁnitions and Decomposition

Deﬁne

Σ = E[(bi(εi + ri) − γai)(bi(εi + ri) − γai)(cid:48)]

Σn = En[(bi(εi + ri) − γai)(bi(εi + ri) − γai)(cid:48)]

ˆΣn = En[(bivi − ˆγˆai)(bivi − ˆγˆai)(cid:48)]

with vi = ψi(ˆη, ˆπ) − b(cid:48)
i

ˆβ and ˆai obtained by replacing the true probabilities π in ai with

the sample estimates ˆπ:

ˆai = (ˆa[1]
i

, . . . , ˆa[J]
i )
Dt,i(1 − ˆπ0) + D0,iˆπt − ˆπt
(1 − ˆπ0)2

ˆa[t]
i =

In the following we proof that || ˆΣn − Σ|| = op(1). The remaining rates for convergence of
ˆΩ = ˆQ−1 ˆΣn
ˆQ−1 to Ω can then be obtained directly from BCCK, Proof of Theorem 4.6.

74

First note the general decomposition

|| ˆΣn − Σ|| ≤ || ˆΣn − Σn|| + ||Σn − Σ||

B.6.3 || ˆΣn − Σn||

Consider the decomposition:

|| ˆΣn − Σn|| ≤ ||En[bib(cid:48)

i(v2

i − (εi + ri)2)]||

+ ||En[ˆγˆaiˆa(cid:48)

iˆγ(cid:48) − γaia(cid:48)

iγ(cid:48)]||

+ ||En[bi(vi(ˆγˆai)(cid:48) − (εi + ri)(γai))]||

We bound the three components separately.

Part 1

||En[bib(cid:48)

i(v2

i − (εi + ri)2)]||

≤ ||En[bib(cid:48)

i((ψi(ˆη, π) − b(cid:48)
i

ˆβ(ˆη, π) + ψi(ˆη, ˆπ) − ψi(ˆη, π) + b(cid:48)

i( ˆβ(ˆη, π) − ˆβ(ˆη, ˆπ)))2 − (εi + ri)2)]||

≤ ||En[bib(cid:48)

i(ψi(ˆη, π) − b(cid:48)
i

ˆβ(ˆη, π) − (εi + ri)2)]|| + ||En[bib(cid:48)

i(ψi(ˆη, ˆπ) − ψi(ˆη, π))2]||

+ ||En[bib(cid:48)

i(b(cid:48)

i( ˆβ(ˆη, π) − ˆβ(ˆη, ˆπ)))2]||

≡ (v.1) + (v.2) + (v.3)

For (v.1) we can use the same proof as in SC, Theorem 3.3 for the nAT E and obtain.

(v.1) (cid:46)P

(cid:18)

(Jn)1/m + sup
z∈Z

(cid:19)

||r(z)||

κn,1 + κn,2

75

due to (MA.6). Note that for nAT E, rAT E, and ∆, the tail bounds and approximation

errors are allowed to diﬀer. For the second term we have

(v.2) =

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

bib(cid:48)
i

En

(cid:18) (cid:88)

ψ[t]

i (ˆη)

(cid:18) ˆπt

1 − ˆπ0

−

πt
1 − π0

(cid:19)(cid:19)2(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t(cid:54)=0
(cid:20)

(cid:88)

(cid:88)

t(cid:54)=0

t(cid:48)(cid:54)=0

En

bib(cid:48)

iψ[t]

i (ˆη)ψ[t(cid:48)]

i (ˆη)

−

πt
1 − π0

(cid:19)(cid:18) ˆπt(cid:48)

1 − ˆπ0

−

πt(cid:48)
1 − π0

(cid:19)(cid:19)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) ˆπt

1 − ˆπ0
(cid:112)πtπ(cid:48)
n

t

(cid:46)P J 2 sup
t,t(cid:48)(cid:54)=0

|ψ[t]

i (ˆη)ψ[t(cid:48)]

i (ˆη)||En[bib(cid:48)

i]||

(cid:46)P J 2 sup
t(cid:54)=0

π−2
t max
1≤i≤n

|εi(t)|2|| ˆQ||

πt
n

(cid:46)P J 3n−(1−2/m)

For the third term we have

(v.3) = ||En[bibi(b(cid:48)
i

ˆQ−1En[b(cid:48)

i(ψ(ˆη, ˆπ) − ψi(ˆη, π))])2]||

(cid:46)P max
1≤j≤n

|b(cid:48)
j

ˆQ−1En[b(cid:48)

i(ψ(ˆη, ˆπ) − ψi(ˆη, π))]|2|| ˆQ||

(cid:46)P sup
z∈Z

||b(z)||2|| ˆQ−1||2||En[b(cid:48)

i(ψ(ˆη, ˆπ) − ψi(ˆη, π))]||2|| ˆQ||

(cid:46)P ξ2
k

(cid:18)(cid:114)

(cid:19)2

kJ
n

=

ξ2
kkJ
n

where the second to last line comes from (H.3).

76

Part 2 Now we use (H.5) to bound the second variance term. Let An = En[aia(cid:48)
ˆAn = En[ˆaiˆa(cid:48)

i]. We have that

i] and

||En[ˆγˆaiˆa(cid:48)

iˆγ(cid:48) − γaia(cid:48)

iγ(cid:48)]||

= ||ˆγ ˆAnˆγ(cid:48) − γAnγ(cid:48)||

≤ ||ˆγ − γ||2|| ˆAn − An|| + ||ˆγ − γ||2||An|| + ||ˆγ − γ|| || ˆAn − An|| ||γ||

+ ||ˆγ − γ|| ||An|| ||γ|| + || ˆAn − An|| ||γ||2

(cid:46)P ||ˆγ − γ|| ||An|| ||γ|| + || ˆAn − An|| ||γ||2

√

(cid:46)P

kJ 2(n−1/2 + mn,2 + Jsn,2)

√

kJ + n−1/2kJ

√

(cid:46)

kJ 2(n−1/2 + mn,2 + Jsn,2)

The covariance term is bounded by

||En[bi(vi(ˆγˆai)(cid:48) − (εi + ri)(γai))]||

≤ ||En[bi(vi − (εi + ri))(γai)(cid:48)]|| + ||En[bi(εi + ri)(ˆγˆai − γai)(cid:48)]||

+ ||En[bi(vi − (εi + ri))(ˆγˆai − γai)(cid:48)]||

≡ (c.1) + (c.2) + (c.3)

Now we use (H.5) and (MA.5) to bound (c.1)

||En[bi(vi − (ε + ri))(γai)(cid:48)]||

(cid:46)P ξk max
1≤i≤n

i( ˆβ(ˆη, ˆπ) − β0)|En[||γai||]
|b(cid:48)

+ ||En[bi(ψi(ˆη, ˆπ) − ψi(ˆη, π))(γai)(cid:48)]|| + ||En[bi(ψi(ˆη, π) − ψi(η, π))(γai)(cid:48)]||

≡ (c.1.1) + (c.1.2) + (c.1.3)

77

with decomposing the BLP error using the rates in (H.5) and (MA.5)

(c.1.1) ≤ ξkJ||γ||En[||ai||] max
1≤i≤n

i( ˆβ(ˆη, ˆπ) − β0)|
|b(cid:48)

(cid:46)P ξk

√

(cid:18)

kJ

ξk

(cid:112)Jk/n + ξkn−1/2 + ξk

(cid:19)
(cid:112)log(k)/n

(cid:46) ξ2
kkJ
√
n

For (c.1.2) note that

(c.1.2) = ||

(cid:88)

(cid:18) ˆπt

t(cid:54)=0

(cid:46)P J sup
t(cid:54)=0

1 − ˆπ0
(cid:114)πt
n

max
1≤i≤n

−

πt
1 − π0

(cid:19)

En[biψ[t]

i (ˆη)a[t]

i ]γ(cid:48)
t||

|ψ[t]

i (ˆη)|En[||bia[t]

i ||]||γt||

(cid:46)P J 3/2n−( 1

2 − 1

m )k

by (MA.1) and (H.5) in conjunction with Markov’s inequality. For (c.1.3), we have that

(c.1.3) = ||

(cid:88)

t(cid:54)=0

πt
1 − π0

En[bi(ψ[t]

i (ˆη) − ψ[t]

i (η))a[t]

i ]γ(cid:48)
t||

|ψ[t]

j (ˆη) − ψ[t]

j (η)||a[t]

i |En[||bi||]||γt||

≤ J sup
t(cid:54)=0

πt max
1≤j≤n

(cid:46)P κ1,nk

also by (H.5) and Markov’s inequality. For (c.2) note that

(c.2) ≤ ξk max
1≤j≤n

|εj + rj|En[||ˆγˆai − γai||]

(cid:46)P ξk((Jn)1/m + sup
z∈Z

(cid:46)P ξk((Jn)1/m + sup
z∈Z

||r(z)||)En[||ˆγˆai − γai||]

√

||r(z)||)

kJ 2(n−1/2 + mn,2 + Jsn,2)

78

as

En[||ˆγˆai − γai||] = En[||ˆγ − γ|| ||ai||] + En[||γ|| ||ˆai − ai||] + En[||ˆγ − γ|| ||ˆai − ai||]

(cid:46)P ||ˆγ − γ||En[||ai||] + ||γ||En[||ˆai − ai||]
kJ 2(n−1/2 + mn,2 + Jsn,2) + (cid:112)Jk/n

(cid:46)P

√

√

(cid:46)

kJ 2(n−1/2 + mn,2 + Jsn,2)

by (H.5). Now note that (c.3) is at most of rate (c.1) + (c.2) which completes the covariance

part.

B.6.4 ||Σn − Σ||

Now consider the remaining diﬀerence

||Σn − Σ|| ≤ ||En[bib(cid:48)

i(εi + ri)] − E[bib(cid:48)

i(εi + ri)]|| + ||En[γaia(cid:48)

iγ(cid:48)] − E[γaia(cid:48)

iγ(cid:48)]||

+ ||En[bi(εi + ri)(γai)(cid:48) + γai(εi + ri)b(cid:48)

i] − E[bi(εi + ri)(γai)(cid:48) + γai(εi + ri)b(cid:48)

i]||

= (d.1) + (d.2) + (d.3)

(d.1) is bounded in BCCK, Proof of Theorem 4.6, with adapted tail rates using (MA.6)

(d.1) (cid:46)P

(cid:18)

(Jn)1/m + sup
z∈Z

(cid:19)(cid:114)

||r(z)||

ξ2
k log k
n

For (d.2) note that

(d.2) ≤ ||γ||2||En[aia(cid:48)

i] − E[aia(cid:48)

i]||

(cid:46)P kJ

(cid:114)

log J
n

due to (H.5). For (d.3) ﬁrst note that

max
1≤i≤n

|a(cid:48)

iγ(cid:48)bi| (cid:46)P ξk||γ|| max

1≤i≤n

||ai|| (cid:46)P ξk

En[||bia(cid:48)

iγ(cid:48)||] ≤ ξk||γ||En[||ai||] (cid:46)P ξk

79

√

kJ

√

kJ

due to (H.5). Now we use the Symmetrization Lemma to bound (d.3). Let wi for

i = 1, . . . , n denote independent Rademacher random variables independent of the data.

Denote Ew[·] the expectation operator with respect to the measure of w. We have that

(d.3) (cid:46) E

(cid:20)

(cid:20)

Ew

||En[wi(εi + ri)(biaiγ(cid:48) + γaib(cid:48)

i)]||

(cid:21)(cid:21)

(cid:114)

(cid:46)

(cid:20)

log k
n

E

||En[(εi + ri)2(bia(cid:48)

iγ(cid:48)bia(cid:48)

iγ(cid:48) + bia(cid:48)

iγ(cid:48)γaib(cid:48)

i + γaib(cid:48)

ibia(cid:48)

iγ(cid:48) + γaib(cid:48)

iγaib(cid:48)

i)]||1/2

(cid:21)

(cid:46)P

(cid:114)

log k
n

(cid:18)

max
1≤i≤n

|εi + ri|

2 max
1≤i≤n

|aiγbi|1/2E[En[||bia(cid:48)

iγ(cid:48)||]]1/2

+ max
1≤i≤n
(cid:114)

log k
n

(cid:46)P

|a(cid:48)

iγ(cid:48)γai|1/2E[En[||bib(cid:48)

i||]]1/2 + max
1≤i≤n

|b(cid:48)

ibi|1/2E[En[||γaia(cid:48)

iγ(cid:48)||]]1/2

(cid:18)

max
1≤i≤n

|εi + ri|

(ξk

√

kJ)1/2(ξk

√

kJ)1/2 + ||γ|| max
1≤i≤n

√

||ai||

k + ξk||γ||

(cid:19)

(cid:19)

(cid:114)

(cid:46)

log k
n

(cid:18)

(nJ)1/m + sup
z∈Z

(cid:19)(cid:18)

||r(z)||

√

J 3/4ξk

k + kJ

(cid:19)

where the second line is due to Khinchin’s inequality as (biaiγ(cid:48) + γaib(cid:48)

i) are iid symmetric.
The remaining steps follow from (H.5) and (MA.6) and repeated application of Markov’s

inequality.

B.6.5 Full variance

Collecting all the rates, we obtain that

|| ˆΣn − Σ|| (cid:46)P

(cid:18)

(Jn)1/m + sup
z∈Z

(cid:19)

||r(z)||

κn,1 + κn,2 + J 3n−(1−2/m) +

ξ2
kkJ
n

+

ξ2
kkJ
√
n

+ J 3/2n−( 1

2 − 1

m )k + κ1,nk

+ ξk((Jn)1/m + sup
z∈Z

||r(z)||)

√

kJ 2(n−1/2 + mn,2 + Jsn,2)

(cid:18)

(Jn)1/m + sup
z∈Z

(cid:19)(cid:114)

||r(z)||

(cid:114)

log k
n

(cid:18)

(nJ)1/m + sup
z∈Z

+

+

ξ2
k log k
n
(cid:19)(cid:18)

||r(z)||

J 3/4ξk

√

k + kJ

(cid:19)

+ kJ

(cid:114)

log J
n

≡ Mn,2

(19)

80

For the nAT E the derivations are analogue, but there are no γai terms. Thus the solution

simpliﬁes to

|| ˆΣn − Σ|| (cid:46)P

(cid:18)

(Jn)1/m + sup
z∈Z

(cid:19)(cid:18)

||r(z)||

κn,1 +

(cid:114)

(cid:19)

ξ2
k log k
n

+ κn,2 = Mn,1

(20)

Thus assuming Mn,1 = o(1) for the nAT E or Mn,2 = o(1) for rAT E/∆ respectively

corresponds to Assumption A.V.

B.7 Supplementary Material for Section 6

We simulate n observations of (Yi, Xi, Ti). Let Xi be a k-dimensional vector of uniform

random variables Xi,j ∼ U[−1, 1] for j = 1, . . . , p and εi ∼ N (0, 1). We let Yi(t) = ui for

t (cid:54)= 1 and Yi(1) = τ + ui. Treatment probabilities P (Ti = t|Xi) = et(Xi) for t = 0, 1 . . . , J

(with t = 0 denoting control) are generated under independence of irrelevant alternatives

as

e0(x) =

et(x) =

1

1 + (cid:80)

j(cid:54)=0 exp(x1βj)

exp(x1βt)

1 + (cid:80)

j(cid:54)=0 exp(x1βj)

with β1 = 1 and βt = 0 for all t (cid:54)= 1. Thus, conditional treatment eﬀects are given

by τ1(x) = τ = 10 and τt(x) = 0 for all t (cid:54)= 1. This implies the following conditional

decomposition terms (II):

E[rAT E(Xi)|Xi,1 = x1] = τ

E[nAT E(Xi)|Xi,1 = x1] = τ

E[∆(Xi)|Xi,1 = x1] = τ

(cid:21)

(cid:20) π1
1 − π0
(cid:20) e1(x1)
1 − e0(x1)
(cid:20) e1(x1)
1 − e0(x1)

(cid:21)

(cid:21)

−

π1
1 − π0

81

Note that E[Xi,1] = 0 and V [Xi,1] = 1/3. Thus the best linear approximation of

E[∆(Xi)|Xi,1] has population parameters (α, β) with

α = τ E

= τ E

(cid:20) e1(Xi,1)
1 − e0(Xi,1)
(cid:20) e1(Xi,1)
1 − e0(Xi,1)

−

−

π1
1 − π0
π1
1 − π0

β =

τ
V [Xi,1]

E

(cid:20) e1(Xi,1)
1 − e0(Xi,1)
(cid:21)

(cid:20) e1(Xi,1)
1 − e0(Xi,1)

Xi,1

= 3τ E

− βE[Xi,1]

(cid:21)

(cid:21)

Xi,1 −

(cid:21)

Xi,1

π1
1 − π0

and equivalently for the rAT E and nAT E. Evaluating the expectation yields the following

parameterization:

Table B.1: Monte Carlo Study: Parameterization

rAT E nAT E
5.000
5.127
2.383
0.000

∆
-.127
2.383

α
β

B.8 Supplementary Material for Section 7.1

The distribution of smoking intensities is shown in Figure B.4 and Table B.2. The majority

of mothers do not smoke during pregnancy ranging from 76% for Black mothers to 96% in

the category "Other". However, the right panel of Figure B.4 shows that conditional on

smoking white mothers and older mothers smoke more heavily.

Table B.2: Distribution of smoking intensities by ethnicity (in percent)

> 20 cigs
16-20 cigs
11-15 cigs
6-10 cigs
1-5 cigs
None

Black Hispanic Other White All
1.1
5.1
1.2
7.8
3.6
81.2

0.7
4.4
0.7
11.5
6.7
76.1

0.2
0.9
0.2
1.5
1.2
96.0

0.5
2.7
0.5
5.4
4.0
87.0

1.2
5.5
1.3
7.4
3.0
81.6

Figure B.5 replicates the solid line of Figure 1 in Cattaneo (2010) with Double Machine

Learning as a byproduct. Our results are very similar and show that average potential

outcomes become smaller the higher the intensity of smoking.

82

Figure B.4: Distribution of smoking intensities along heterogeneity variables

Figure B.6 contains the re-scaled propensity scores et(x)/πt for all treatment versions.

B.9 Supplementary Material for Section 7.2

The distribution of versions is shown in Figure B.7 and Table B.3. We observe that women

are overrepresented in clerical, health and food training, while men are more likely to be

observed in automechanics, welding, electrical and construction training.

As a byproduct of the decomposition estimation, we create the AIPW scores for

every treatment version. This allows us to inspect their often noisily estimated average

potential outcomes in Figure B.8. We observe a clear pattern. The point estimates of the

predominantly male trainings are all larger than the predominantly female ones.

Figure B.9 contains the re-scaled propensity scores et(x)/πt for all treatment versions.

83

0.000.250.500.751.00BlackHispanicOtherWhiteFractionSmoking> 20 cigs16−20 cigs11−15 cigs6−10 cigs1−5 cigsNone0.000.250.500.751.00BlackHispanicOtherWhiteFraction conditional on smoking0.000.250.500.751.001020304050AgeFractionSmoking> 20 cigs16−20 cigs11−15 cigs6−10 cigs1−5 cigsNone0.000.250.500.751.0010203040AgeFraction conditional on smokingFigure B.5: Average potential outcomes of smoking intensities

Note: Average potential outcomes estimated with Double Machine Learning using an ensemble of
Ridge, Lasso and Random Forest regression. Point estimates and 95%-conﬁdence interval.

Figure B.6: Re-scaled propensity scores

84

310032003300None1−5 cigs6−10 cigs11−15 cigs16−20 cigs> 20 cigsAverage potential outcome16−20 cigs> 20 cigs1−5 cigs6−10 cigs11−15 cigs0123450123450123450.00.30.60.90.00.30.60.9Scaled p−scoredensityFigure B.7: Distribution of treatment versions by gender

Table B.3: Share of observations in treatment versions (in percent)

Control
No JC
JC without voc
Clerical
Health
Auto
Welding
Electrical
Construction
Food
Other
Multiple

Female Male All
40.2
16.1
9.8
6.1
4.1
1.6
1.6
0.8
5.2
2.4
5.1
7.0

43.3
13.5
10.3
2.4
1.3
2.5
2.4
1.1
8.0
2.1
6.2
6.8

36.1
19.6
9.3
11.1
7.8
0.3
0.6
0.3
1.4
2.7
3.6
7.2

85

0.000.250.500.751.00FemaleMaleFractionVersionMultipleOtherFoodConstructionElectricalWeldingAutoHealthClericalJC without vocNo JCControl0.000.250.500.751.00FemaleMaleFraction conditional on vocational trainingFigure B.8: Average potential outcomes of treatment versions

Note: Average potential outcomes estimated with Double Machine Learning using an ensemble of
Ridge, Lasso and Random Forest regression. Point estimates and 95%-conﬁdence interval.

Figure B.9: Re-scaled propensity scores

86

175200225250275ControlNo JCJC ithout vocClericalHealthAutoWeldingElectricalConstructionFoodOtherMultipleAverage potential outcomeOtherMultipleElectricalConstructionFoodHealthAutoWeldingNo JCJC without vocClerical012301230123024024024024Scaled p−scoredensity