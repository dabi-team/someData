Program Analysis of
Probabilistic Programs

Maria I. Gorinova

2
2
0
2

r
p
A
4
1

]
L
P
.
s
c
[

1
v
8
6
8
6
0
.
4
0
2
2
:
v
i
X
r
a

Doctor of Philosophy

— Centre for Doctoral Training in Data Science —

School of Informatics

University of Edinburgh

2022

THEUNIVERSITYOFEDINBURGH 
 
 
 
 
 
Abstract

Probabilistic programming is a growing area that strives to make statistical analysis more

accessible, by separating probabilistic modelling from probabilistic inference. In practice

this decoupling is diﬃcult. The performance of inference methods is sensitive to both the

underlying model and the observed data. Diﬀerent inference techniques are applicable

to diﬀerent classes of models, have diﬀerent advantages and shortcomings, and require

diﬀerent optimisation and diagnostics techniques to ensure robustness and reliability.

No single inference algorithm can be used as a probabilistic programming back-end that

is simultaneously reliable, eﬃcient, black-box, and general. Probabilistic programming

languages often choose a single algorithm to apply to a given problem, thus inheriting

its limitations. While substantial work has been done both to formalise probabilistic

programming and to improve eﬃciency of inference, there has been little work that makes

use of the available program structure, by formally analysing it, to better utilise the

underlying inference algorithm. My thesis is that it is possible to improve probabilistic

programming using program analysis, and I present three novel techniques (both static

and dynamic), which analyse a probabilistic program and adapt it to make inference more

eﬃcient, sometimes in a way that would have been tedious or impossible to do by hand.

Part I of the thesis focuses on static analysis and gives the ﬁrst formal treatment of the

popular probabilistic programming language Stan. While eﬃcient, Stan constrains the

space of programs expressible in the language. Programs must be written according to

Stan’s block syntax, which reduces compositionality. In addition, Stan does not support

the explicit use of discrete parameters. Part I introduces the probabilistic programming

language SlicStan: a compositional, self-optimising version of Stan, which supports both

discrete and continuous parameters. SlicStan uses information ﬂow analysis and type

inference to capture conditional independence relationships in the program and transform

it for inference in Stan. The result can be seen as a hybrid inference algorithm, where

diﬀerent parameters are inferred according to diﬀerent inference algorithms for eﬃciency.

Part II shows an example of dynamic analysis. The performance of inference algorithms

can be dramatically aﬀected by the parameterisation used to express a model. It is diﬃcult

to know in advance what parameterisation is suitable, as it depends on the properties of

the observed data. This part demonstrates that reparameterisation can be automated

by combining eﬀect handlers in the probabilistic programming language Edward2, with

variational inference preprocessing that searches over a space of possible parameterisations.

iii

Lay summary

Anna and Ben are about to play a game. Anna ﬂips two coins without showing Ben the

result. Ben guesses whether each of the coins is heads or tails. If he guesses both correctly,

he wins. Otherwise, Anna wins. But they both agree this game is a little unfair: after

all, there is only 25% chance that Ben guesses both coins correctly! Anna ﬂips the coins.

Attempting to make the game more fair, she gives Ben some additional information before

he makes his guess: one of the coins is tails. What are Ben’s chances of guessing the other

coin correctly? Is the game fair now?

Human intuition is notoriously misleading when it comes to such problems. And yet, we

are faced with problems of this kind often and across disciplines. From a doctor deciding

on a treatment based on their patient’s symptoms and test results, to studying the universe

based only on its projection on the night sky, we need ways to make inferences about

unobservable phenomena given only some incomplete related data.

Thankfully, we can express such questions about the uncertainty of phenomena using

probabilistic programming languages. Such languages aim to facilitate reasoning about

probabilities and making inferences. However, statistical inference is not an easy task, and

so probabilistic programming has been only partially successful in practice.

This dissertation shows that we can exploit the rich structure provided by a probabilistic

program to improve inference and to make probabilistic programming more accessible.

Program analysis techniques have long been used to improve conventional programming by

automatically detecting errors, optimising the program so it makes better use of available

resources, making it faster or more robust, or proving properties such as correctness and

safety. My thesis is that program analysis techniques can also be adapted to probabilistic

programming. I show three ways in which program analysis techniques improve automatic

inference: one automates an optimisation task that was previously the responsibility of the

user; one analyses dependencies between variables in the program to generate an eﬃcient

hybrid inference algorithm; and one performs a program re-write that improves inference

robustness and eﬃciency, and which would have been impossible to do by hand.

As for Ben, he is in luck. Anna accidentally swung the odds in his favour, and he has

more than 66% chance of winning if he guesses the remaining coin is heads.

iv

Acknowledgements

Many people helped along the way of completing this work, by teaching me, mentoring me,

inspiring me, collaborating with me, and supporting me both academically and socially.

Andy Gordon has been an invaluable inﬂuence during the past years. He supported this work

every step of the way and was a bottomless source of new ideas. I am grateful for everything I

learned from Andy, for his patience when I was stubborn, and for letting me do things my own

way. Thank you, to Charles Sutton, for always being excited about this work. Charles has

been the integral machine learning wisdom source during the past years and helped assess and

ﬁlter a lot of ideas. I am grateful for his continuing support, both technical and personal. Thank

you, Andy and Charles, for supervising me during the PhD, for your guidance, for the insightful

discussions, and for shaping me as a researcher.

I’m thankful to Bob Carpenter, my co-author Matthijs V´ak´ar, and the Stan team for

teaching me a lot about Stan, for being incredibly kind, and for welcoming me to the ﬁeld. Thank

you to Michael Betancourt for his excellent lectures and tutorials on statistics, and for being

a healthy source of scepticism during this journey. Thank you to my co-authors Dave Moore

and Matt Hoﬀman, and to the Bayesﬂow team, for fruitful discussions and for the excitement

about probabilistic programming. I learned a lot about approximate inference from them, as well

as about the design of deep probabilistic programming languages. Iain Murray helped shape

the course of the PhD project during annual reviews and oﬀered regular feedback and advice. I

loved having chats with Ohad Kammar, who oﬀered both technical and professional advice,

and with Daniel Hillerstr¨om, who was an incredible source of knowledge about eﬀect handlers.

Ohad and Daniel both read parts of this thesis and oﬀered helpful feedback. Thank you to Alex

Lew for the discussions about probabilistic programming and Gen, to Kai Xu for helping with

questions about Turing, and to Feras Saad for his suggestions around a better discussion of

symbolic languages, which all improved this dissertation.

A special thank you to George Papamakarios, who I learned so much from. I’m grateful for

the time he spent oﬀering expertise; we’ve had many discussions, some of which inspired parts of

this work. But most of all, I’m grateful to him for his continuous support and encouragement,

for being an example of integrity and fairness, and for always inspiring assertiveness.

Thank you to my family and friends, for always believing in me, for being there in good and

bad, keeping my spirit high through blizzards, winter storms, toilet roll shortages, and a global

pandemic. I could not have made it without Janie’s sarcastic memes, Christine’s supermassive

black hole cakes, Alex’s sass and cynicism, Deena’s gorgeous cocktails, Nelly’s weather readings,

Flic’s cats, Marc’s gif-based career advice, Conor’s machine learning rants, Ivana’s coﬀee

breaks, and James’s piano. Thank you for being there through the worst of it all.

v

This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science,

funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1)

and the University of Edinburgh. In addition, part of the work was done while interning at

Google. Thank you for making it possible for me to pursue a PhD by funding this research.

vi

Declaration

I declare that this thesis was composed by myself, that the work contained herein is my

own except where explicitly stated otherwise in the text, and that this work has not been

submitted for any other degree or professional qualiﬁcation except as speciﬁed.

(Maria I. Gorinova)

vii

Надедо.viiiTable of Contents

1 Introduction

2 Probabilistic programming

2.1 A very simple probabilistic program . . . . . . . . . . . . . . . . . . . . . .

2.2 Bayesian inference

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1 Deriving a solution analytically . . . . . . . . . . . . . . . . . . . .

2.2.2 Rejection sampling and the curse of dimensionality . . . . . . . . .

2.2.3 Markov chain Monte Carlo inference

. . . . . . . . . . . . . . . . .

1

5

5

7

7

7

9

2.2.4 Variational inference . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2.2.5

Inference on graphical models . . . . . . . . . . . . . . . . . . . . . 16

2.2.6 Other inference strategies

. . . . . . . . . . . . . . . . . . . . . . . 20

2.3 Probabilistic programming languages . . . . . . . . . . . . . . . . . . . . . 22

2.3.1 Deﬁnition and classiﬁcation of PPLs

. . . . . . . . . . . . . . . . . 23

2.3.2 Density-based (or explicit) PPLs

. . . . . . . . . . . . . . . . . . . 25

2.3.3

Sample-based (or implicit) PPLs

. . . . . . . . . . . . . . . . . . . 29

2.3.4 Eﬀect-handling-based PPLs

. . . . . . . . . . . . . . . . . . . . . . 32

2.3.5 Other PPLs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

I Static optimisation for probabilistic programming

3 Formal treatment of programming languages

35

37

3.1 Formal syntax of programming languages . . . . . . . . . . . . . . . . . . . 37

3.2 Rules of inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

3.3 Semantics of programming languages . . . . . . . . . . . . . . . . . . . . . 40

3.4 Type checking and type inference . . . . . . . . . . . . . . . . . . . . . . . 41

3.5

Information ﬂow analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4 SlicStan

45

ix

4.1 The paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

4.1.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.1.1.1 Background: Probabilistic Programming Languages and Stan . . . . .
4.1.1.2 Goals and Key Insight . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
4.1.1.3 The Insight by Example
. . . . . . . . . . . . . . . . . .

4.1.1.4 Core Contributions and Outline

47

46

47

48

4.1.2 Core Stan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

4.1.2.2 Operational Semantics of Stan Statements

4.1.2.1 Syntax of Core Stan Expressions and Statements

. . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.2.3 Syntax of Stan
4.1.2.4 Density-Based Semantics of Stan . . . . . . . . . . . . . . . . . .
4.1.2.5 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

49

50

51

53

4.1.3

SlicStan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.3.1 Syntax
4.1.3.2 Typing of SlicStan . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.3.3 Elaboration of SlicStan . . . . . . . . . . . . . . . . . . . . . . .
4.1.3.4 Semantics of SlicStan . . . . . . . . . . . . . . . . . . . . . . .
4.1.3.5 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
4.1.3.6 Diﬃculty of Specifying Direct Semantics Without Elaboration

54

55

58

60

60

62

4.1.4 Translation of SlicStan to Stan . . . . . . . . . . . . . . . . . . . . 62

4.1.4.1 Shredding . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.4.2 Transformation . . . . . . . . . . . . . . . . . . . . . . . . . .

62

64

4.1.5 Examples and Discussion . . . . . . . . . . . . . . . . . . . . . . . . 66

. . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.5.1 Type Inference
4.1.5.2 Locality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
4.1.5.3 Code Refactoring
4.1.5.4 Code Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . .

66

67

68

69

4.1.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

4.1.6.1 Formalisation of Probabilistic Programming Languages . . . . . . . .
4.1.6.2 Static Analysis for Probabilistic Programming Languages . . . . . . .
4.1.6.3 Usability of Probabilistic Programming Languages . . . . . . . . . .

71

71

72

4.1.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

4.1.8 Appendix A: Deﬁnitions and Proofs . . . . . . . . . . . . . . . . . . 76

4.1.8.1 Deﬁnitions

4.1.8.2 Proof of Semantic Preservation of Shredding

. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .

76

77

4.1.9 Appendix B:Further discussion on semantics . . . . . . . . . . . . . 82

4.1.9.1 Semantics of Generated Quantities . . . . . . . . . . . . . . . . .
. .
4.1.9.2 Relation of Density-based Semantics to Sampling-based Semantics

82

83

4.1.10 Appendix C: Elaborating and shredding if or for statements

. . . . 84

4.1.11 Appendix D: Non-centred Reparameterisation . . . . . . . . . . . . 85

4.1.12 Appendix E: Examples . . . . . . . . . . . . . . . . . . . . . . . . . 86

x

. . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.12.1 Neal’s Funnel
4.1.12.2 Cockroaches . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.12.3 Seeds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

86
90
92

4.2 Clarifying the contributions

. . . . . . . . . . . . . . . . . . . . . . . . . . 94

4.3

Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

5 Conditional independence by typing

97

5.1 The paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

5.1.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

5.1.2

SlicStan: Extended syntax and semantics . . . . . . . . . . . . . . . 100
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
5.1.2.1 Syntax
5.1.2.2 Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.1.2.3 Operational Semantics of SlicStan Statements . . . . . . . . . . . . 104
5.1.2.4 Density Semantics . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.1.2.5 Shredding and Translation to Stan . . . . . . . . . . . . . . . . . 108
5.1.2.6 Density Factorisation . . . . . . . . . . . . . . . . . . . . . . . 110

5.1.3 Theory: Conditional independence by typing . . . . . . . . . . . . . 110
(cid:96)2 Type System . . . . . . . . . . . . . . . . . . . . . . . . 113
. . . . 116
. . . . . . . . . . . . 116

5.1.3.1 The
5.1.3.2 Conditional Independence Result for
5.1.3.3 Scope of the Conditional Independence Result

(cid:96)2-Well-Typed Programs

5.1.4 Application: Discrete parameters support through a semantics-

preserving transformation . . . . . . . . . . . . . . . . . . . . . . . 117
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.1.4.1 Goal
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.1.4.2 Key Insight
5.1.4.3 Variable Elimination . . . . . . . . . . . . . . . . . . . . . . . . 120
. . . . . 120
5.1.4.4 Conditional Independence and Inferring the Markov Blanket
. . . . . . . . . . . . . . . . . 121
5.1.4.5 Sampling the Discrete Parameters
5.1.4.6 A Semantics-Preserving Transformation Rule . . . . . . . . . . . . 122
5.1.4.7 Marginalising Multiple Variables: An example . . . . . . . . . . . . 124
5.1.4.8 Relating to Variable Elimination and Complexity Analysis . . . . . . 128
. . . 128
5.1.4.9 Semantic Preservation of the Discrete Variable Transformation
5.1.4.10 Scope and limitations of Elim Gen . . . . . . . . . . . . . . . . 129

5.1.5

Implementation and empirical evaluation . . . . . . . . . . . . . . . 131
5.1.5.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.1.5.2 Empirical evaluation . . . . . . . . . . . . . . . . . . . . . . . . 131
5.1.5.3 Analysis and discussion . . . . . . . . . . . . . . . . . . . . . . 135

5.1.6 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

5.1.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

5.1.8 Appendix A: Deﬁnitions and Proofs . . . . . . . . . . . . . . . . . . 140
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

5.1.8.1 Deﬁnitions
5.1.8.2 Proofs

5.1.9 Appendix B: Examples . . . . . . . . . . . . . . . . . . . . . . . . . 149

xi

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.1.9.1 Sprinkler
. . . . . . . . . . . . . . . . . . . . . . . . 149
5.1.9.2 Soft-K-means model
5.1.9.3 A causal inference example . . . . . . . . . . . . . . . . . . . . . 150

5.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153

II Dynamic optimisation for probabilistic programming

155

6 Automatic reparameterisation

157

6.1 Eﬀect-handling based probabilistic programming . . . . . . . . . . . . . . . 157

6.1.1 Eﬀects and eﬀect handling . . . . . . . . . . . . . . . . . . . . . . . 158

6.1.2 Composing eﬀect handlers . . . . . . . . . . . . . . . . . . . . . . . 160

6.1.3 Eﬀect handling in probabilistic programming . . . . . . . . . . . . . 160

6.2 The paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162

6.2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

6.2.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165

6.2.3 Understanding the Eﬀect of Reparameterisation . . . . . . . . . . . 165

6.2.4 Reparameterising Probabilistic Programs . . . . . . . . . . . . . . . 166
. . . . . . . . . . 166
. . . . . . . . . . 167

6.2.4.1 Eﬀect Handling-based Probabilistic Programming
6.2.4.2 Model Reparameterisation Using Eﬀect Handlers

6.2.5 Automatic Model Reparameterisation . . . . . . . . . . . . . . . . . 168
6.2.5.1 Interleaved Hamiltonian Monte Carlo . . . . . . . . . . . . . . . . 168
6.2.5.2 Variationally Inferred Parameterisation . . . . . . . . . . . . . . . 168

6.2.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
6.2.6.1 Models and Datasets . . . . . . . . . . . . . . . . . . . . . . . . 170
6.2.6.2 Algorithms and Experimental Details . . . . . . . . . . . . . . . . 170
6.2.6.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170

6.2.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171

6.2.8 Appendix A: Derivation of the Condition Number of the Posterior

. . . . . . . . . . . . . . . . . . . . . . . . . . . 174
for a Simple Model
6.2.8.1 Deriving VCP and VN CP : Centred Parameterisation . . . . . . . . . 174
6.2.8.2 Deriving VCP and VN CP : Non-centred Parameterisation . . . . . . . 174
. . . . . . . . . . . . . . . . . 175
6.2.8.3 The Best Diagonal Preconditioner
. . . . . . . . . . . . . . 175
6.2.8.4 The Condition Numbers κCP and κN CP

6.2.9 Appendix B: Interceptors . . . . . . . . . . . . . . . . . . . . . . . . 175
6.2.9.1 Make log joint . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
6.2.9.2 Non-centred Parameterisation Interceptor . . . . . . . . . . . . . . 175
6.2.9.3 VIP Interceptor . . . . . . . . . . . . . . . . . . . . . . . . . . 176
. . . . . . . . . . . . . . 176
6.2.9.4 Mean-ﬁeld Variational Model Interceptor

6.2.10 Appendix C: Details of the Experiments

. . . . . . . . . . . . . . . 176

6.2.11 Appendix D: Additional Analysis . . . . . . . . . . . . . . . . . . . 177

6.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179

xii

6.3.1

Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179

6.3.2

Scope and extensions . . . . . . . . . . . . . . . . . . . . . . . . . . 179

7 Challenges ahead

Bibliography

181

182

xiii

Introduction

CHAPTER 1

Bayesian statistics provides a set of rigorous techniques that allow us to analyse and

interpret data, and to make predictions while carefully quantifying uncertainty. It has been

an invaluable tool in many areas where it is important to incorporate domain knowledge in

the modelling of the data, or where data is scarce. In Bayesian terms, we deﬁne a statistical

model that encodes our assumptions about an underlying process based on some unknown

variables and some real-world data that we observe. Through Bayesian inference (which

I will also refer to as just inference in this dissertation) we can obtain the conditional

distribution of the unknown quantities given the data, and compute expectations under it,

while rigorously keeping track of uncertainties.

But inference is not an easy task. Performing inference correctly, reliably and eﬃciently,

requires substantial knowledge of probability and statistics, it is an active area of research,

and it can be time-consuming even for experts. Thus, it can be diﬃcult for non-statistical

ﬁelds to adopt Bayesian statistics, despite the beneﬁts it has to oﬀer.

Probabilistic programming aims to decouple modelling from inference. It promises to allow

modellers to work close to their domain of expertise, without having to implement inference

algorithms from scratch. The idea of probabilistic programming languages is to specify

a model in the form of a program, which typically (but not always) encodes the way in

which the data is believed to have been generated. This program is then automatically or

semi-automatically compiled to an inference algorithm, which can compute expectations

of interest without users having to implement computational details.

While probabilistic programming has signiﬁcantly grown as an area in the past decade,

its dream to democratise Bayesian statistics is still some way ahead. Diﬀerent inference

algorithms come with diﬀerent constraints, advantages, and disadvantages. General

algorithms, which work with a wide range of models, typically take longer to give a reliable

result. Tailoring an algorithm to a speciﬁc problem can result in a considerably more

1

2

Chapter 1.

Introduction

eﬃcient inference, but using the same algorithm for a diﬀerent model might not be possible.

Probabilistic programming languages face this same problem and are often forced into a

trade-oﬀ between generality of the modelling language and eﬃciency of inference.

The focus of this dissertation is on program analysis techniques for probabilistic program-

ming. Many probabilistic programming languages leverage the formal foundations on

which they have been built to compile to a particular algorithm or perform symbolic

inference. But whether the underlying program structure can be utilised to improve

inference in a program-speciﬁc manner has been largely unexplored. The main question of

this dissertation is: Can program analysis improve inference in probabilistic programming?

I claim the answer to this question is yes and I show three ways in which program analysis

of probabilistic programs can support automatic inference.

Contributions and outline of the dissertation

The dissertation is in two parts, which focus on static and dynamic analysis respectively.

Chapter 2 provides background and motivation relevant to both parts. It gives a short

introduction to Bayesian inference, covering diﬀerent approaches and discussing their

advantages and disadvantages. It also gives an overview of probabilistic programming

languages, dividing them in three categories, and listing some of their strengths and

constraints. The brief Chapter 3, on the other hand, gives background on formal treatment

of programming languages that is relevant to Part I only.

Chapters 4–6 each focus on one of the three contributions of this dissertation:

(1) Chapter 4 describes the ﬁrst formal treatment of the probabilistic programming

language Stan, and a semantics-preserving procedure that allows for a more compo-

sitional, self-optimising version of Stan, called SlicStan. Chapter 4 is based on the

following publication:

Maria I Gorinova, Andrew D Gordon, and Charles Sutton. Probabilistic
programming with densities in SlicStan: Eﬃcient, ﬂexible, and determinis-
tic. Proceedings of the ACM on Programming Languages 3, Issue POPL,
Article 35 (January 2019).

(2) Chapter 5 extends Stan/SlicStan with explicit support for discrete parameters,

through a semantics-preserving transformation that ﬁrstly marginalises out discrete

variables from the program and then re-draws them. This can be seen as an automatic

program-speciﬁc composition of two inference algorithms. Chapter 5 is based on the

following publication:

3

Maria I Gorinova, Andrew D Gordon, Charles Sutton, and Matthijs V´ak´ar.
Conditional independence by typing. ACM Transactions on Programming
Languages and Systems 44, Issue 1, Article 4 (March 2022).

(3) Chapter 6 gives a way to automatically reparameterise probabilistic models through

eﬀect-handlers and shows how the process of ﬁnding a suitable parameterisation of a

probabilistic model can be automated through a variational inference pre-processing.

The chapter is based on the following publication:

Maria I Gorinova, Dave Moore, and Matthew D Hoﬀman. Automatic
Reparameterisation of Probabilistic Programs. Proceedings of the 37th
International Conference on Machine Learning (July 2020).

CHAPTER 2

Probabilistic programming

Inference and languages

What is probabilistic programming and why is it diﬃcult? This chapter gives a brief

overview of the topic of probabilistic programming, motivating the work of this thesis and

introducing some preliminaries. It gives a gentle introduction to the idea of probabilistic

programming (§ 2.1), and discusses the process and challenges of Bayesian inference and of
probabilistic modelling itself (§ 2.2). Finally, the chapter gives background on some of the
most well-known current probabilistic programming languages, analysing their advantages,

shortcomings, and constraints (§ 2.3).

2.1 A very simple probabilistic program

Suppose Anna ﬂips two fair coins, observes the result, and tells Ben that they are not

both heads. How does Ben’s belief about the ﬂip result of each coin changes based on this

new information?

Anna’s ﬂips generate one of four possible scenarios: both coins are heads, both are tails,

the ﬁrst is heads and the second is tails, or the ﬁrst is tails and the second is heads. But

one of these scenarios becomes impossible given the extra information Anna gives us. Each

of the two coins is heads in only one of the remaining three equally likely scenarios. Thus,

the probability that, say, the ﬁrst coin is heads given they are not both heads is 1/3.

We can express such questions about the uncertainty over phenomena given observations

using probabilistic programming languages (PPLs). In general, such languages are charac-

terised by two facilities that are not usually present in conventional programming. These

are the ability to specify random variables and the ability to observe data.

5

6

Chapter 2. Probabilistic programming

For example, we can express Anna and Ben’s problem as a probabilistic program:

1

2

3

4

c1

c2

∼

∼

bernoulli(0.5)

bernoulli(0.5)

bothHeads = c1 and c2

observe(not bothHeads)

Each coin ﬂip is a Bernoulli random variable, which is either 1 (for heads) or 0 (for tails)

with probability 0.5. In the program above, lines 1 and 2 deﬁne these two variables,

naming them c1 and c2. Line 3 introduces a third random variable, bothHeads, which is
the logical ‘and’ between c1 and c2: it is 1 only when both coin ﬂips result in heads, and 0
otherwise. Finally, line 4 observes the data: the coin ﬂips did not both result in heads. A

PPL that can interpret this program will give us the conditional probability distribution

of the unobserved variables in the program given the data. In our case, that is:

p(c1, c2

|

not bothHeads) =

0

if c1 = c2 = 1 or c1, c2 /

2

0, 1
}

∈ {




1/3 otherwise.

Notation and terminology More concretely, a probabilistic program deﬁnes a joint



density over some random variables. We typically divide these variables into parameters θ

and data

and write p(θ,

) for their joint distribution, which is also the product of the

D

D

prior on the parameters p(θ) and the likelihood of the data p(

θ). We are interested in

D |

the posterior distribution

p(θ

) =

| D

p(θ)p(
p(

D |
)
D

θ)

.

(2.1)

Speciﬁcally, we are interested in extracting information from this distribution, in order to

answer diﬀerent queries. This usually means computing expectations under this posterior
distribution, for example the expectation of a function f (θ):1

Ep(θ|D) (f (θ)) =

f (θ)p(θ

)dθ.

(2.2)

| D

(cid:90)
The process of deriving or approximating the posterior distribution, in a form that allows
us to compute expectations under it, is what we call Bayesian inference.2

But why do we need a dedicated language to do any of this? Can we not simply use

Equations 2.1 and 2.2 to compute any expectation we like? The next sections describe

some of the challenges of performing Bayesian inference in practice and, in turn, the

challenges faced by PPLs: the languages that aim to democratise this diﬃcult process.

1This expression assumes continuous parameters θ. For mixed discrete and continuous parameters, we
can replace the integration with an appropriate combination of integration and summation. Generally,
integration is used as a shorthand for such integration-summation combination throughout this thesis.

2This is only one way of deﬁning “inference”. Statistical inference can more generally be understood as
deriving properties (expectations, but also conﬁdence / credible intervals) of some underlying distribution.

2.2. Bayesian inference

7

2.2 Bayesian inference

2.2.1 Deriving a solution analytically

The ﬁrst question we might ask is can we not perform inference analytically. Indeed,
(cid:82) p(θ,D)dθ .
But the normalising constant (also referred to as the marginal likelihood or evidence)

), Equation 2.1 gives us the posterior p(θ

p(D) = p(θ,D)

given the joint p(θ,

) = p(θ,D)

| D

D

Z(

) =

p(θ,

)dθ is not always available in a closed form and it is infeasible to

D

D

(cid:82)

compute in the general case. Suppose, for example, that we are working with N Bernoulli

parameters θ = (θ1, . . . , θN ). Then Z(

) =
). The complexity of computing this expression is O(2N ), which becomes

ˆθN =0 p(θ1 = ˆθ1, . . . , θN = ˆθN ,

ˆθ1=0 · · ·

) =

D

D

1

1

θ∈{0,1}N p(θ,

(cid:80)

(cid:80)

D

infeasible for larger N .
(cid:80)

What is worse is that even if we assume an oracle that gives us Z(

), computing expecta-

tions under p(θ

) is still infeasible: Ep(θ|D) (f (θ)) =

)f (θ). In practice,

such expectations are typically approximated. One way to approximate an expectation

| D

D
θ∈{0,1}N p(θ,

D

(cid:80)

under some p(x) is through Monte Carlo estimation (Metropolis and Ulam, 1949):

Ep(x) (f (x))

1
I

≈

I

i=1
(cid:88)

f (ˆx(i))

where ˆx(i)

p(x)

∼

(2.3)

In other words, if we can obtain a set of samples ˆx(i) from the distribution of interest p(x),

then we can use this set to estimate the expectation of any function under p(x).

2.2.2 Rejection sampling and the curse of dimensionality

(i)

This leads us to a second idea of how to perform inference: can we not simply sample
(ˆθ
(i) does not match the
actual observed data, and use the remaining samples for Monte Carlo estimation?

), discard any samples where ˆ
D

(i)) from the joint p(θ,

, ˆ
D

D

bernoulli(0.5), c(i)

For instance, in the coins example, we can generate S =
where c(i)
all samples such that bothHeads(i) = 1, leaving us with S(cid:48) =
bothHeads(i) = 0

1 ∼

2 ∼

bernoulli(0.5), and bothHeads(i) = c(i)

|
. These remaining samples S(cid:48) are samples from the posterior distribution
}
bothHeads = 0).

∈

{

1 , c(i)

(c(i)
{

2 , bothHeads(i))
I
i=1,
}
c(i)
2 . Rejecting
2 , bothHeads(i))

1 ∧

S

(c(i)

1 , c(i)

p(c1, c2

|

This is a particular instantiation of the rejection sampling algorithm, and is a valid inference

algorithm, as (after scaling) p(θ

) is entirely contained by p(θ,

):

D

| D

p(ˆθ,

D

) = p(

D

)p(ˆθ

)

| D

≥

Cp(θ

)

| D

for all ˆθ and C a constant

(2.4)

8

Chapter 2. Probabilistic programming

Figure 2.1: Illustration of the curse of dimensionality: the ratio between some volume

of interest and a volume that contains it, diminishes as dimensionality increases. The

ﬁgure is taken from Michael Betancourt’s tutorial on Probabilistic Computation https:

//betanalpha.github.io/assets/case_studies/probabilistic_computation.html.

While simple, general, and exact, this algorithm does not scale. To see why, consider

continuous parameters θ and suppose that the prior over θ is an N -dimensional uniform

distribution between 0 and 1. Suppose also, that the data is such that the posterior of

θ is an N -dimensional uniform distribution between 1/3 and 2/3. The prior forms an

N-dimensional hypercube of side 1, and the posterior is an N -dimensional hypercube

of side 1/3 centred at the middle of the prior hypercube. Figure 2.1 shows this for

N

. Our rejection sampling algorithm would choose at random a point inside
1, 2, 3
}

of the big hypercube and only accept it if it is also inside the smaller hypercube. The

∈ {

acceptance rate R is proportionate to the ratio between volumes of the two hypercubes:

R =

(1/3)N

1N =

1
3N

(2.5)

Even if we are to sample from a prior closer to the posterior, say uniform between 1/3

(cid:15)

and 2/3 + (cid:15), for any small (cid:15), the ratio between volumes, and thus the acceptance rate, is:

−

R(cid:15) =

(1/3)N
(1/3 + 2(cid:15))N =

1
(1 + 6(cid:15))N

(2.6)

In either case, as the dimensionality increases, the volume outside of the inner hypercube

increases exponentially, and the acceptance rate approaches 0. This is a particular

instantiation of the curse of dimensionality (Bellman, 1961; Bishop, 2006, section 1.4).

The curse of dimensionality is why many numerical and sampling methods, such as

numerical integration, rejection sampling, and importance sampling, are impractical for

problems with more than few dimensions. It also motivates the search for more eﬀective

inference solutions, which exploit various properties of probability distributions, such as

smoothness or conditional independencies.

2.2. Bayesian inference

9

2.2.3 Markov chain Monte Carlo inference

Markov chain Monte Carlo (MCMC) is a class of sampling algorithms, introduced by

Metropolis et al. (1953) and reviewed by Neal (1993) and Murray (2007), which perform

sampling by transitioning according to a Markov chain. This chain is carefully designed,

so that its stationary distribution is the target probability distribution (for example the

posterior distribution in the case of inference). Drawing samples according to the Markov

chain eventually converges to drawing samples from the target, which makes MCMC

methods asymptotically exact: they are exact in the presence of inﬁnite amount of samples.

Consider the problem of sampling from some p(θ). Let π(θ

θ(cid:48)) be the transition
probabilities (also referred to as kernel ) of a homogeneous Markov chain. Given some initial
sample θ(0), we can generate a sequence of samples θ(1)
θ(1)),
∼
θ(N −1)). This sequence converges towards samples from p(θ), provided:
. . . , θ(N )

θ(0)), θ(2)

π(θ(N )

π(θ(1)

π(θ(2)

∼

|

|

|

∼

|

(1) the chain is ergodic, meaning it is aperiodic and it is possible to “reach” any state θ(cid:48)
from any other state θ following a ﬁnite sequence steps of non-zero probability. Then,
θ(0)) reaches a unique stationary
no matter the value of the initial θ(0), π(θ(N )
distribution (also called equilibrium distribution ) π∞(θ(N )) in the limit of N
:
→ ∞

|

lim
N →∞

π(θ(N )

|

θ(0)) = π∞(θ(N ))

π∞(θ) =

π(θ

(cid:90)

θ(cid:48))π∞(θ(cid:48))dθ(cid:48)

|

(2.7)

(2.8)

(2) p(θ) is this stationary distribution: π∞(θ) = p(θ).

θ(n−1))
As long as these conditions are met, drawing samples, one by one, from π(θ(n)
is a valid sampling algorithm. When used to sample from a posterior distribution, it is

|

also a valid inference strategy. While MCMC guarantees exact inference in the presence of

inﬁnitely many samples, the quality of inference in practice is hugely inﬂuenced by the

shape of the underlying distribution and the choice of kernel π(θ

θ(cid:48)).

|

The MCMC family of sampling algorithms is very large, and a full review is out of scope

for this thesis, but is studied in detail by many probabilistic reasoning textbooks (MacKay

2003, Chapters 29 and 30; Bishop 2006, Chapter 11; Barber 2012, Chapter 27; Murphy

2012, Chapter 24). In the rest of this section, I will focus on one particular MCMC

algorithm, which is closely related to the work of this thesis: Hamiltonian Monte Carlo.

10

Chapter 2. Probabilistic programming

Figure 2.2: An illustration of a single HMC iteration in a 2-dimensional parameter space.

The current sample is given an initial momentum at random; we take 10 leapfrog steps to

simulate the particle trajectory and obtain a candidate sample.

Hamiltonian Monte Carlo

Hamiltonian Monte Carlo (HMC) is an MCMC algorithm introduced by Duane et al.

(1987) and popularised by Neal et al. (2011). It makes use of the gradient of the target

density function as a way to eﬃciently explore the underlying probability distribution.

HMC is typically applied to problems where the number of parameters in the model is

known and ﬁxed, although extensions that work for nonparametric models have also been

recently introduced (Mak et al., 2021).

In brief, HMC works by translating the problem of sampling from a probability distribution

to exploring the dynamics of a particle in a Hamiltonian system. Suppose that the target

density p(θ) is diﬀerentiable almost everywhere. Suppose also that we are given a function
p∗(θ) that is proportionate to the p(θ); that is p(θ) = p∗(θ)
Z
Z =

for the normalising constant
log p∗(θ). A
particle on that surface can be described by its position θ and momentum p. Its potential

p∗(θ)dθ. Now consider the multidimensional surface given by

−

(cid:82)

energy E(θ) is given by the height of the surface at θ and its kinetic energy K(p) is given

by the magnitude of the momentum p and the mass m of the particle:

E(θ) =

log p(θ)

log Z

−

−

K(p) =

1
2m(cid:107)

p

2
(cid:107)

CurrentsampleProposedsampleLeapfrogstep2.2. Bayesian inference

11

In a system with a single particle and no friction, no energy is lost and the Hamiltonian,

the sum of potential and kinetic energy, stays constant:

H(θ, p) = E(θ) + K(p)

The change of the position and momentum over time t is described by a system of

diﬀerential equations:

dθ
dt

=

∇

K(p) =

1
m

p

dp
dt

=

E(θ)

−∇

Here

∇

E(θ) =

∂E(θ)
∂θ1

. . .

∂E(θ)
∂θN

and

ents of E(θ) and K(p) respectively.
(cid:17)

(cid:16)

K(p) =

∇

(cid:16)

∂K(p)
∂p1

. . .

∂K(p)
∂pN

denote the gradi-

(cid:17)

Equipped with these equations, we can simulate (run forwards in time) the “physical”

system for any initial position θ and momentum p. Now consider the MCMC transition
θ(cid:48), p(cid:48), t(cid:48)), which is 1 if simulating the system between time t(cid:48) and t, starting
kernel π(θ, p, t
at an initial position θ(cid:48) and initial momentum p(cid:48), results in position θ and momentum
p, and it is 0 otherwise. As the system is deterministic, we can also recover the initial

|

position and momentum by simulating back in time starting from the ﬁnal position and
momentum.3That is:

π(θ, p, t

|

θ(cid:48), p(cid:48), t(cid:48)) = π(θ(cid:48), p(cid:48), t(cid:48)

θ, p, t)

|

(2.9)

Consider the joint density pH(θ, p) = exp(−H(θ,p))
Z
pH(θ, p) = pH(θ(cid:48), p(cid:48)), giving us:

exp(

−

ZH

(cid:82)

K(p))dp is a normalising constant. As the Hamiltonian stays constant, we have

, where ZH =

exp(

H(θ, p))dθdp =

−

(cid:82)

pH(θ, p)π(θ, p, t

|

θ(cid:48), p(cid:48), t(cid:48)) = pH(θ(cid:48), p(cid:48))π(θ(cid:48), p(cid:48), t(cid:48)

θ, p, t)

|

(2.10)

Equation 2.10 is known as detailed balance and it is a suﬃcient condition for showing

that pH(θ, p) is the stationary distribution of π (Murphy, 2012, Theorem 17.2.3). Thus,
simulating the sliding particle physical system produces samples from the joint density

pH(θ, p). Marginalising over p gives us:

+∞

−∞

(cid:90)

pH(θ, p)dp =

+∞

1
ZH

exp(

−

E(θ)) exp(

−

K(p))dp

−∞
(cid:90)
1
Z

=

exp(

E(θ))

−

+∞

−∞

(cid:90)

Z
ZH

exp(

−

K(p))dp

= p(θ)

3This is equivalent to simulating forwards in time starting from the ﬁnal position θ(cid:48) and the reversed

ﬁnal momentum

p(cid:48).

−

12

Chapter 2. Probabilistic programming

In other words, we can obtain samples from p(θ) by augmenting the parameter space with

the momentum variables p, running a simulation of the Hamiltonian system described

above, and discarding the samples obtained for p. In practice, we need to discretise time

in order to run the simulation, which in HMC is usually done using leapfrog integration.

The error introduced by discretising the simulation is corrected with an accept/reject step

similar to the Metropolis-Hastings algorithm (Hastings, 1970).

A single iteration of HMC is visually described in Figure 2.2. In brief, obtaining a new

sample θn+1 given a current sample θn is done as follows:

(1) Sample an initial momentum p

(0, mI), where I is the identity matrix, and

compute the Hamiltonian of the system H = E(θn) + K(p).

∼ N

(2) Simulate the system starting at position θn and momentum p. This step requires
E(θ) to make

E(θ) to be diﬀerentiable with respect to θ and uses the gradient

∇

steps (some small (cid:15) in size) in the direction of steepest descent. After L leapfrog
steps the particle is at position θ∗ and has momentum p∗. This ﬁnal position of the
particle, θ∗, is the proposed sample.

(3) Decide whether to accept (θn+1 = θ∗) or reject (θn+1 = θn) the candidate sample.
This is done based on the diﬀerence between the re-evaluated Hamilton H ∗ =
E(θ∗) + K(p∗) and the initial Hamilton H. The bigger this diﬀerence, the less
accurate was the discretised simulation at approximating the behaviour of physical

system, and the less likely it is to accept the candidate sample. When the diﬀerence

is very big, we call the sample step divergent. Divergences can indicate problems with

inference and are an important diagnostics tool, as shown next and in Figure 2.3.

Advantages and shortcomings

MCMC methods are asymptotically exact: for an inﬁnite number of samples, sampling

from the transition distribution corresponds to sampling from p(θ) exactly. In practice,

this means that to obtain a reliable approximation of a distribution, we need to sample

for a long time, until the Markov chain converges. Unfortunately, there is no way to

calculate in advance for how long we should run a given chain to achieve some desired

accuracy (Tierney, 1994; Brooks and Gelman, 1998). In particular, the geometry of the

target distribution has a dramatic eﬀect on the performance of MCMC. Well-conditioned

distributions that have a small ratio between the variable with highest variance and the

variable with the lowest variance, will typically be easier to sample from than distributions

where this ratio is large. This leads to some pathological cases, where no number of samples

smaller than inﬁnity provides a correct approximation of the distribution of interest.

2.2. Bayesian inference

13

(a) Neal’s funnel density.

(b) 10000 samples drawn with HMC.

Figure 2.3: Neal’s funnel.

For example, consider Neal’s funnel, which is a model chosen by Neal (2003) to demonstrate

the diﬃculties Metropolis–Hastings (Hastings, 1970) runs into when sampling from a

distribution with strong non-linear dependencies. The model deﬁnes a density over

variables x and y:

y

∼ N

(0, 3)

xi

∼ N

(0, exp(y/2))

for i = 1, . . . , 9

The density has the form of a 10-dimensional funnel (thus the name “Neal’s funnel”), with

a very sharp neck, as shown in Figure 2.3a. MCMC methods, including HMC, have trouble

obtaining samples from the neck of the funnel, because there exists a strong non-linear

dependency between x and y, and the posterior geometry is diﬃcult for the sampler to

explore well. Figure 2.3b shows 10000 samples drawn from Neal’s funnel using HMC.

Despite half of the probability mass being inside of the funnel, HMC is not able to explore

it fully due to its high curvature. Samples where a divergence occurred are shown in

yellow, concentrating around the region that HMC’s leapfrog integrator ﬁnds problematic.

Neal’s funnel is a typical example of the dependencies that can occur in hierarchical models

in practice, and therefore it highlights the importance of MCMC diagnostic techniques that

can detect possible inference problems. Many such techniques exist, including examining

HMC divergences, comparing the within and between variance of several diﬀerent MCMC

chains, prior predictive and posterior predictive checks. In addition, there are various ways

to address inference problems, one of which, model reparameterisation, I discuss in detail

in Chapter 6. For an excellent introduction to various diagnostic techniques and ways to

address computational problems, refer to Gelman et al. (2020).

3210123x012108642024y14

Chapter 2. Probabilistic programming

2.2.4 Variational inference

In practice, sampling methods can be very computationally demanding, as they sometimes

require a huge number of iterations to obtain a reasonable approximation for a target

distribution. Variational inference (VI) methods (MacKay 2003, Chapter 33; Bishop 2006,

Chapter 10; Murphy 2012, Chapter 21; Blei et al. 2017) take a diﬀerent approach that

trades asymptotic guarantees for eﬃciency. The main idea of variational inference is to use

optimisation instead of sampling, to obtain an analytical approximation to a distribution

of interest. In brief, given a tractable family of distributions, VI tries to ﬁnd a distribution

from that family that best approximates the target distribution.

Consider some family Q of distributions over the parameters θ. We call variational
Q, such that q∗(θ) is as close as
inference the process of ﬁnding some optimal q∗(θ)
possible to the distribution of interest, for example, the posterior p(θ

∈

):

| D

q∗(θ) = arg min
q(θ)∈Q

d(p(θ

| D

), q(θ))

(2.11)

), q(θ))) encodes the dissimilarity between p and q. Diﬀerent dissimilarity

Here, d((p(θ

| D

metrics can be used, but the one that is most common in variational inference is the

Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951) from q to p:

DKL(q(θ)

p(θ

||

| D

)) = Eq(θ) (log q(θ)

log p(θ

))

| D

−

(2.12)

However, we cannot directly use this metric in an objective, as we do not know p(θ

).

Usually, the KL divergence is used to derive a lower bound for the evidence p(

| D
). Note

D

that DKL(q(θ)
is knows as Gibbs’ inequality (MacKay, 2003, Section 2.6). We derive:

0, and only equals 0 when p and q are identical. This results

p(θ

| D

≥

))

||

DKL(q(θ)

p(θ

||

| D

)) = Eq(θ) (log q(θ)

log p(θ

−

))

| D
log

p(θ,
p(

D
)

)

(cid:19)

Eq(θ)

(cid:18)
Eq(θ) (log p(θ,

D

Eq(θ) (log p(θ,

)) + Eq(θ) (log p(

D

))

)) + log p(

D

)

D

D

= Eq(θ) (log q(θ))

= Eq(θ) (log q(θ))

= Eq(θ) (log q(θ))

−

−

−

0

≥

And deﬁne the evidence lower bound (ELBO) as:

ELBO = Eq(θ) (log p(θ,

))

D

−

Eq(θ) (log q(θ))

log p(

D

)

≤

(2.13)

) is a constant with respect to θ, minimising the KL-divergence (2.12) is

As log p(

D

equivalent to maximising the ELBO (2.13). Assuming that we are given p(θ,

) (or, in

D

2.2. Bayesian inference

15

| D

the more general case, any function proportionate to p(θ

)), and that we can eﬃciently

estimate expectations under q(θ), we can re-deﬁne our variational objective as maximising

the expectation lower bound:

q∗(θ) = arg max
q(θ)∈Q

Eq(θ) (log p(θ,

))

D

−

Eq(θ) (log q(θ))

(2.14)

(cid:2)

(cid:3)

Figure 2.4: An example of the variational family of choice not capturing the complexity of

the target distribution. The target banana-shaped distribution is given by x

(0, 1)

2x2, 1). The dashed white lines show the mean-ﬁeld variational posterior.

∼ N

and y

(

∼ N

−

There are diﬀerent ways to choose a variational family Q, so that expectations under

distributions in the family are easy to compute (Bishop 2006, Chapter 10, Murphy 2012,

Chapter 21). One way that is increasingly used, and especially in the context of probabilistic

programming, is to parameterise the distribution using some set of variational parameters

φ: a distribution q(θ; φ)

Q is entirely determined by its variational parameters φ. In

addition, the family Q is chosen so that distributions belonging to it are easy to sample

∈

from. This allows us to approximate the expectations in the ELBO by Monte Carlo

averaging. Equation 2.14 becomes:

φ∗ = arg max

φ

log p(ˆθ,

)

D

−

log q(ˆθ; φ)
(cid:105)

ˆθ∼q(θ;φ) (cid:104)
(cid:88)

(2.15)

3210123x1210864202y16

Chapter 2. Probabilistic programming

Many modern diﬀerentiable programming frameworks, such as TensorFlow (Abadi et al.,

2015) and PyTorch (Paszke et al., 2019), can automatically diﬀerentiate through such

expressions. Thus, we can simply specify the loss to be the (negative) ELBO and

readily use a diﬀerentiable programming framework to perform optimisation. This is

usually referred to as black-box variational inference (Ranganath et al., 2014), automatic

diﬀerentiation variational inference (ADVI) (Kucukelbir et al., 2016), or (whenever we

work with minibatches of the data) stochastic variational inference (SVI) (Hoﬀman et al.,

2013).

Mean-ﬁeld variational inference

One simple choice of a variational family is the mean-ﬁeld variational family Q =
µ, σT I)
the variational family consists of all independent multivariable Gaussian distributions. For

|
, where N is the size of the parameter space. In other words,

RN
+ }

RN , σ

{N

(θ

µ

∈

∈

|

example, consider the model from Figure 2.4:

x

∼ N

(0, 1)

y

2x2, 1)

(
−

∼ N

The mean-ﬁeld variational family for this model is given by all distributions q(x, y; µx,y, σx,y) =

(x

|

N

µx, σx)

N

(y

|

µy, σy) for µx,y

R and σx,y

R+.

∈

∈

Advantages and shortcomings

Variational inference scales: it works for very large amounts of data, and can utilise modern

frameworks and architectures to achieve fast inference. However, it trades accuracy to be

able to do so. The variational family Q is typically restricted to be potentially much simpler

than the target distribution p, and we cannot, in general, determine how far oﬀ would even

the optimal q be from this target. Figure 2.4 shows an example of a poor approximation

resulting from variational inference. The simple mean-ﬁeld approximation is not able to

capture the tails of the banana-shaped distribution, which results in a poor estimation of

expectations under the inferred posterior: for example, the true expectation of y is

2,

−

and its standard deviation is 3, but under the variational posterior, the expectation of y is

approximately

−

0.4 and its standard deviation is 1.

2.2.5 Inference on graphical models

MCMC and VI both require a tractable density (or mass) function p∗(θ), which is
proportionate to the distribution of interest with respect to the parameters θ. However, it

is not typical for these algorithms to make use of further information about the structure

2.2. Bayesian inference

17

Figure 2.5: Factor graph corresponding to the factorisation of a simple HMM (Eq. 2.17).

of the function p∗(θ), such as its factorisation. Information about the factorisation of the
target distribution allows us to reason about conditional independencies between variables

and develop algorithms that actively exploit the structure of a probabilistic model to make

inference more eﬃcient.

(cid:81)

We express the factorisation of a distribution p(θ) =

i φi(θ(i)) in terms of graphical
models (Bishop 2006, Section 8.4; Barber 2012, Part I; Murphy 2012, Chapter 20). Here,
each φi is some function on a subset of the parameters θ(i)
θ is a separate vertex,
this factorisation through a graph, where each parameter θ
and dependencies between parameters appearing in the same subset θ(i) are expressed
by connections between those vertices corresponding to θ(i). In particular, factor graphs
θ and a factor vertex
(Frey, 2002) are bipartite graphs, with a variable vertex for each θ
for each factor φi(θ(i)) of p(θ). An edge exists between a variable node θ and a factor
φi(θ(i)) if and only if θ

θ. A graphical model encodes

θ(i).

⊆

∈

∈

∈

For example, consider a Hidden Markov Model (HMM) with N parameters z = (z1, . . . , zN ),
N observed variables y = (y1, . . . , yN ), and some given constant arrays α and β:

z1

zn

yn

∼

∼

∼

Bernoulli(α1)

Bernoulli(αzn

1)

−

Bernoulli(βzn)

for n = 2,. . . , N

for n = 1,. . . , N

π) as a shorthand for the probability mass function of a Bernoulli(π) variable

Using b(x

|

x, the factorisation of the joint distribution over z and y is:

p(z, y) = b(z1

α1)b(y1

βz1)

|

|

N

n=2
(cid:89)

b(zn

αzn

1)b(yn

−

βzn)

|

|

(2.16)

More concretely, let’s say N = 3. Then the joint is:

p(z1, z2, z3, y1, y2, y3) = b(z1

α1)b(z2

αz1)b(z3

αz2)b(y1

βz1)b(y2

βz2)b(y3

|

|

|

|

|

|

βz3)
(2.17)

z1b(z1|α1)b(y1|βz1)y1b(z2|αz1)z2b(y2|βz2)y2b(z3|αz2)z3b(y3|βz3)y318

Chapter 2. Probabilistic programming

Figure 2.5 shows this expressed in terms of a factor graph: round vertices are variable

nodes and square vertices are the factor nodes, b(z1
variable nodes (y1, y2 and y3) denote observed variables.

|

α1), b(y1

βz1) and so on. Shaded

|

Suppose we want to compute the posterior p(z

z p(z,y) . Calculating this in the
general case requires N nested sums over the parameters z, meaning its complexity is
exponential: O(2N ) But knowing the factorisation of the joint p(z, y) allows us to rewrite

|

y) = p(z,y)
(cid:80)

the expression for computing the normalising constant Z =

z p(z, y):

Z =

p(z, y)

(cid:80)

z
(cid:88)

=

=

z3
z2 (cid:88)
z1 (cid:88)
(cid:88)
b(y3

z3
(cid:88)

b(z1

|

α1)b(z2

|

αz1)b(z3

|

αz2)b(y1

βz1)b(y2

βz2)b(y3

βz3)

|

|

|

βz3)

|

b(z3

|

αz2)b(y2

βz2)

|

(cid:34)

z1
(cid:88)

(cid:34)

z2
(cid:88)

b(z1

|

α1)b(z2

|

αz1)b(y1

|

βz1)

(cid:35)(cid:35)

The inner-most sum over z1 only depends on z2, and thus we can compute it independently
of z3 as a function of z2. This generalises beyond the particular value of N we chose:

Z =

zN
(cid:88)

b(yN

βzN )

|





zN
1
(cid:88)
−

b(zN

αzN

|

1)b(yN −1

−

βzN

1) . . .

−

|

b(z3

|

αz2)b(y2

βz2)

|

(cid:34)

z1
(cid:88)

(cid:34)

z2
(cid:88)

b(z1

|

α1)b(z2

|

αz1)b(y1

|

βz1)

(cid:35)(cid:35)

. . .

(cid:35)

Starting from the inner-most expression

z1
each bracket to a function of a single variable zn+1, while summing out another variable,
(cid:1)
zn. In other words, it takes O(22) time to compute each of N
1 intermediate expressions,
thus the overall complexity is O(N ). A huge improvement over the O(2N ) from before!

(cid:0)(cid:80)

−

|

|

|

b(z1

α1)b(z2

αz1)b(y1

βz1)

, we reduce

Inference algorithms based on graphical models can utilise the structure of a given model

to automatically derive an eﬃcient model-speciﬁc inference strategy, as we did by hand

above. Some algorithms, such as the sum-product algorithm (Bishop, 2006, Section 8.4.4)

are exact, but restricted to tree-structured graphical models in which computing exact

marginals is tractable. Others, like variational message-passing (Winn et al., 2005) and

expectation propagation (Minka, 2001), as used by Infer.NET (Minka et al., 2014), are

applicable to a large class of graphical models, but only provide an approximate solution.

Here, we discuss in more detail one algorithm for exact inference that relates to the work

described in Chapter 5: variable elimination.

2.2. Bayesian inference

19

(a) Remove z1 and its neighbouring factors

(red). Create a new factor f1 by summing

out z1 from the product of these factors.

(b) Connect f1 (in green) to the former

neighbours of z1. The remaining factor

graph deﬁnes the marginal p(z2, z3, y).

(c) Remove z2 and its neighbouring factors

(d) Connect f2 (in green) to the former

(red). Create a new factor f2, by summing

neighbours of z2. The remaining factor

out z2 from the product of these factors.

graph deﬁnes the marginal p(z3, y).

Figure 2.6: Step by step example of variable elimination.

Variable elimination

Variable elimination (VE) (Zhang and Poole, 1994; Koller and Friedman, 2009) is an exact

inference algorithm eﬃcient in models with sparse structure. It is applicable to general

graphs, but it requires the model to be such that marginals are tractable. In particular, it

applies to any model that contains only discrete parameters of ﬁnite support.

The idea is to eliminate (marginalise out) variables one by one. To eliminate a variable z,

we multiply all of the factors connected to z to form a single expression, then sum over all

possible values for z to create a new factor, remove z from the graph, and ﬁnally connect

the new factor to all former neighbours of z. Here, “neighbours” refers to the variables

which are connected to a factor that connects to z.

Figure 2.6 shows the VE algorithm step-by-step applied to the HMM of length 3 from

before. We eliminate z1 to get the marginal on z2 and z3 (2.6a and 2.6b), then eliminate
z2 to get the marginal on z3 (2.6c and 2.6d). Finally, having obtained p(z3, y), we can sum
y) = p(z,y)
Z .
out z3 to obtain the normalising constant Z, which gives us the posterior p(z

|

While the procedure shown in Figure 2.6 is optimal for this model, this would not have

necessarily been the case if we had eliminated the variables in a diﬀerent order (for example,

starting from z2 would be less eﬃcient). VE provides an optimal exact marginalisation
solution, but only with respect to a particular elimination ordering. Finding the optimal

z1b(z1|α1)b(y1|βz1)y1b(z2|αz1)z2b(y2|βz2)y2b(z3|αz2)z3b(y3|βz3)y3f1(z2,y1)=Pz1[b(z1|α1)×b(y1|βz1)×b(z2|αz1)(cid:3)z2b(y2|βz2)y2y1b(z3|αz2)z3b(y3|βz3)y3f1(z2,y1)Pz1[b(z1|α1)z2b(y2|βz2)y2y1b(z3|αz2)z3b(y3|βz3)y3f2(z3,y2,y1)=Pz2[f1(z2,y1)×b(y2|βz2)×b(z3|αz2)(cid:3)z3b(y3|βz3)y3y2y120

Chapter 2. Probabilistic programming

ordering is an NP-complete problem (Yannakakis, 1981; Arnborg et al., 1987), however

many good heuristics exist (for example minimal degree ordering (George and Liu, 1989)).

Advantages and shortcomings

In the cases when a model has a sparse structure, consisting of factors that allow for ana-

lytical marginalisation, variable elimination provides an eﬃcient exact solution. However,

in the general case, the complexity of VE is exponential in the number of nodes.

More generally, algorithms that operate on graphical models include a large number

of approximate inference techniques. These include loopy belief propagation (Frey and

MacKay, 1998), variational message-passing (Winn et al., 2005), expectation propagation

(Minka, 2001), and other message-passing algorithms. While more general and able to

scale, these methods suﬀer from the same problem we saw in §§ 2.2.4: the approximated
solution might be very far oﬀ the true distribution of interest.

2.2.6 Other inference strategies

The list of algorithms presented so far is by no means exhaustive. The number of available

Bayesian inference strategies, each coming with its own set of constraints, advantages and

limitations, is too vast for it to be practical to include a full review in this dissertation.

Here, I brieﬂy discuss a few other strategies that are less relevant to the contributions of

this work, but nevertheless provide a useful background when contemplating the challenges

of probabilistic programming.

Sequential Monte Carlo

Sequential Monte Carlo (SMC) (Gordon et al., 1993; Doucet et al., 2001; Rainforth,

2017, Chapter 6), also referred to as particle ﬁltering, is a Monte Carlo method that is

particularly suitable for dynamic models that involve stochastic control ﬂow and recursion.

Like MCMC, SMC is a Monte Carlo algorithm that approximates a distribution through

a collection of samples. While MCMC obtains such a collection by repeatedly mutating

a sample from the distribution, the core idea behind SMC is to independently sample

from some proposal distribution, and compute a weight associated with each such sample,

based on how likely it is that the sample is also from the target distribution. Expectations

under the target can then be computed as a weighted average of the obtained samples.

What makes SMC diﬀerent to simple importance sampling (Barber 2012, Section 27.6;

Bishop 2006, Section 11.1.4), is that this sample-and-weight process is divided into stages.

The algorithm sequentially samples each random variable given the already sampled

2.2. Bayesian inference

21

variables and updates the weight. This has the advantage that at every intermediate

step we can augment the population of samples by resampling them based on the current

weights. This ensures that more samples are propagated in areas where we expect the

probability mass of the target to be higher, thus making better use of available resources.

SMC can also be used to construct a proposal distribution for MCMC methods. This

combination between SMC and MCMC inference is referred to as Particle Markov chain

Monte Carlo (Andrieu et al., 2009).

SMC has been a popular algorithm in the ﬁeld of probabilistic programming, due to its

generality (see § 2.3). The structure made available by the presence of a probabilistic
program is particularly useful for automatically devising a program-speciﬁc SMC inference

algorithm, which works in the presence of stochastic control ﬂow and is highly parallelisable.

However, similar to other inference algorithms, SMC also comes with its disadvantages. In

particular, the algorithm is sensitive to the choice of proposal distribution. A proposal

distribution that is close to the true target would result in eﬃcient inference, while a poorly-

suited proposal would result in low eﬀective sample size, especially in high dimensions (due

to the curse of dimensionality, §§ 2.2.2). This has resulted in research studying various
techniques for improving the proposal, including inference compilation: learning a suitable

proposal through variational inference (Le et al., 2017). SMC typically requires a number

amount of samples, usually much larger than gradient-based methods like HMC, but it

can be better suited for multi-modal distributions and is more general.

For an excellent review of sequential Monte Carlo, refer to Rainforth (2017, Chapter 6).

Likelihood-free inference methods

Most of the methods discussed so far are likelihood-based. That is, they explicitly evaluate

the likelihood p(

θ) to perform inference. However, in some cases, the likelihood

is unavailable or intractable. Likelihood-free inference methods can perform Bayesian

D |

inference based solely on simulation from the generative model. Likelihood-based and

likelihood-free are umbrella terms that include many inference strategies, and thus lack

canonical references, but have been recently surveyed, for example, by Papamakarios

(2019) and Cranmer et al. (2020).

We already saw one likelihood-free inference method: rejection sampling (§§ 2.2.2). Rejec-
tion sampling is perhaps the simplest likelihood-free inference algorithm, where we simulate
parameters ˆθ and data ˆ
D
ˆ
D
not evaluate the likelihood function at any point. Approximate Bayesian computation

from the model and throw away (or reject) any simulated data

that does not exactly match the observed data

. This is likelihood-free, as we do

D

22

Chapter 2. Probabilistic programming

(ABC) (Sisson et al., 2018; Beaumont, 2010) methods are based on a similar idea, but

accept simulated data that is suﬃciently close, some small distance (cid:15), from the true data.
That is, a sample ˆθ is accepted whenever
ˆ
D − D|
|
Carlo methods can be augmented to a likelihood-free alternative, by approximating the

< (cid:15). Many likelihood-based Monte

likelihood this way, which gives rise to algorithms such as MCMC-ABC (Marjoram et al.,

2003) and SMC-ABC (Sisson et al., 2007). These algorithms are more general, as they

do not require a tractable likelihood, but are less eﬃcient than their likelihood-based

counterparts.

Recent work in the area also includes likelihood-free inference through neural density

estimation (Durkan et al., 2018; Papamakarios et al., 2019, 2021; Gon¸calves et al., 2020).

Density estimation is the problem of estimating the value of the probability density function

of some random variable at an arbitrary point, given only a set of independently generated

samples of that variable; neural density estimation tackles this by using neural networks.

This relates closely to the problem of likelihood-free inference, where we can generate such

a set of samples, but do not know the density function of the data. Sequential neural

methods for likelihood-free inference (Durkan et al., 2018; Gon¸calves et al., 2020) perform

inference by either approximating the likelihood function using neural density estimation

and using it in combination with likelihood-based methods, or by directly estimating the

posterior density.

For an excellent review of likelihood-free inference, refer to Papamakarios (2019, Part II).

2.3 Probabilistic programming languages

Probabilistic programming (Gordon et al., 2014; van de Meent et al., 2018) aims to

democratise the diﬃcult process of Bayesian inference by decoupling the process of writing

a model from the actual inference algorithm. However, we saw in § 2.2 that inference is not
an easy task. What algorithm is best suited for inference in a given model depends hugely

on the properties of the model itself. Does the model have a ﬁxed number of random

variables? Does the model contain only discrete variables, only continuous variables, or

both? Is the model diﬀerentiable? There does not exist a single inference algorithm that

provides an eﬃcient solution for all possible models.

Probabilistic programming languages (PPLs) inherit this diﬃculty. Usually, languages

trade oﬀ generality for eﬃciency of inference or automation. Some PPLs would restrict

the space of supported models in order to use an eﬃcient algorithm such as HMC. Others

support a more general class of models, but are then forced to use a general inference

2.3. Probabilistic programming languages

23

algorithm, which can be less eﬃcient than less general alternatives. Some PPLs have

adopted the concept of programmable inference, which allows for the best of both worlds,

but requires signiﬁcantly more eﬀort from the user, who now needs to choose and adapt

the inference algorithm themselves.

This leads to the development of many diﬀerent PPLs, each with its own set of advantages

and constraints. This section aims to provide a brief overview of the topic of probabilistic

programming languages. The area has signiﬁcantly expanded in recent years, and thus

covering all existing PPLs in this dissertation will not be practical, but I focus on a few of

the more popular languages, as a basis for the discussion on program analysis.

2.3.1 Deﬁnition and classiﬁcation of PPLs

Recall the simple probabilistic program from § 2.1:

c1

c2

∼

∼

bernoulli(0.5)

bernoulli(0.5)

bothHeads = c1 and c2

observe(not b)

There are two things that make this program diﬀerent to a conventional program: the

∼

probabilistic assignment (or sample),

, and the observe statement, observe. This follows

perhaps the most established and most often used deﬁnition of probabilistic programs,

which is given by Gordon et al. (2014):

Probabilistic programs are “usual” programs (...) with two added constructs:
(1) the ability to draw values at random from distributions, and (2) the ability
to condition values of variables in a program via observe statements.

While all PPLs ﬁt this deﬁnition, the details can vary signiﬁcantly. Both sampling and

observation are understood and implemented in diﬀerent ways across languages, yielding

vastly diﬀerent syntaxes and workﬂows. How do we classify probabilistic programming

languages so that we can easily compare and contrast them? In the following sections,

I give one way to distinguish and talk about PPLs in a structured way. However, it is

important to note that not all PPLs will fall in one of the described categories, and the

described classiﬁcation should not be seen as a formal taxonomy.

A fundamental way in which PPLs diﬀer is their treatment of probabilistic assignment (

).

There are (at least) two ways in which we can interpret a statement such as y

dist(x):

(1) as stating that the underlying joint probability density contains the factor dist(y

x),

or (2) as a random draw from the distribution dist(x) the result of which is bound to y.

∼

∼

|

24

Chapter 2. Probabilistic programming

Importantly, these two are not equivalent treatments. For example, consider the simple

normal(0, 1). The unnormalised joint density under (1) is

program x
over a single variable, x: p∗(x) =

normal(0, 1); x

∼

∼

(x

0, 1)

(x

0, 1). But in the case of (2), we sample,

N
one by one, two separate variables x, say we denote them x1 and x2. The corresponding
unnormalised density of the program is p∗(x1, x2) =
0, 1). As these
variables are bound to the same variable name x, the ﬁrst result is overwritten. We are

0, 1)

(x2

(x1

N

N

N

|

|

|

|

left with the result of only the second sample statement, thus, we can also see the program

as corresponding to the density p(x2) =

p(x1, x2)dx =

(x2

|

N

0, 1).

Programs written in languages that adopt the former, density-based interpretation, ex-

(cid:82)

plicitly deﬁne an (unnormalised) joint density function. The (unnormalised) likelihood of

the model is usually available, it can be evaluated and used to perform likelihood-based

inference. However, sampling data from the model is not always possible. Such models

are called explicit or prescribed (Diggle and Gratton, 1984).

PPLs that adopt the latter, sample-based interpretation yield programs that are eﬀectively

simulators.

If ran ignoring all observe statements, such programs have the eﬀect of

simulating data from the model. Models that are deﬁned in terms of a sampling procedure

are called implicit (Diggle and Gratton, 1984), as we cannot, in general, evaluate the

likelihood function of these models: it is implicitly deﬁned.

While there are many categories in which we can divide PPLs, including based on pro-

gramming style (declarative, imperative, or functional) or type of inference used (Monte

Carlo, symbolic, variational, etc.), here I propose a way of classifying them based on their

∼

interpretation of

. This gives rise to three categories: density-based (or explicit) PPLs

(§§ 2.3.2) that yield explicit models, sample-based (or implicit) PPLs that yield implicit
models (§§ 2.3.3), and eﬀect-handling-based PPLs (§§ 2.3.4), which can interpret
either
explicitly or implicitly based on context. This classiﬁcation is useful for several reasons:

∼

(1) It relates PPLs and the models expressible in those PPLs to the established and

useful distinction between explicit and implicit probabilistic models (Diggle and

Gratton, 1984). It is important to note that these two model categories are not

mutually exclusive. It is possible to simulate data from some density-based programs,

and it is also possible to evaluate the likelihood for some sampling-based programs.

(2) It directly points to a suitable way of formalising the semantics of the language.

Typically the semantics of a PPL can be formalised in several equivalent ways, how-

ever, the distinction between explicit and implicit PPLs is helpful when the aim is to

develop semantics that is closely tied to implementation. Explicit PPLs can be given

2.3. Probabilistic programming languages

25

straightforward density-based semantics, which is the focus of Chapter 4. Implicit

PPLs are better formalised through trace-based (or sampling-based ) semantics (for

example as described by Hur et al. (2015), Staton et al. (2016), Heunen et al. (2017)).

(3) It hints at the class of algorithms suitable for each language. Being able to evaluate an

(unnormalised) density, deﬁned on a ﬁxed parameter space, is essential for gradient-

based algorithms such as HMC or VI, making these algorithms easily applicable to

explicit models. On the other hand, an implicit model will sometimes require a more

general inference algorithm, such as SMC, or in some cases, even a likelihood-free

algorithm (Cranmer et al. (2020, Section 3) give details on when likelihood-based

methods are applicable to a probabilistic program).

The following sections present one or two example probabilistic programming languages

for each of the three PPL types. In particular, I give an overview of Stan (Carpenter et al.,

2017) and Edward2 (Tran et al., 2018) (among other PPLs), which are PPLs central to

Part I and Part II of this dissertation respectively.

2.3.2 Density-based (or explicit) PPLs

Density-based PPLs, which I am also going to refer to as explicit PPLs, explicitly deﬁne an

unnormalised joint probability density. Examples of density-based languages include Stan

(Carpenter et al., 2017), BUGS (Gilks et al., 1994), and JAGS (Plummer et al., 2003), as

well as languages that operate on graphical models, including Infer.NET (Minka et al.,

2014), Fun (Borgstr¨om et al., 2011), and Tabular (Gordon et al., 2015). Turing (Ge et al.,

2018), a PPL that allows for compositional inference with diﬀerent inference techniques

being used for diﬀerent sub-parts of the model, is also density-based.

In these languages, statements such as y

dist(x) are interpreted as information that

the joint density given by the model contains the factor dist(y

x). The parameter

∼

space is ﬁxed: it consists of some parameters of interest θ, such that the dimension of

θ does not change during inference. A model in a density-based PPL usually describes
a tractable unnormalised density p∗(θ,
evaluate p∗(θ,
D
observations (that is observe(f (

) for diﬀerent parameter values θ. Languages that support deterministic

) for some non-identity deterministic function f ,

); in other words, the model can be used to

D

as opposed to observe(

)), such as Infer.NET, are an exception to this tractable

) = ˆ
D

D

= ˆ
D

D

|

unnormalised density rule.

This constrained interpretation of

leads to several advantages of density-based PPLs. As

the parameter space is ﬁxed, and the (unnormalised) joint density is usually available in

∼

26

Chapter 2. Probabilistic programming

closed form and can be evaluated, explicit PPLs are, in general, able to use more eﬃcient

algorithms compared to sample-based PPLs. In particular, we can apply gradient-based

algorithms such as HMC or VI. It is also intuitive to write both directed and undirected

models, as probabilistic statements are understood simply as adding a multiplicative factor

to the target density. However, it is not always possible to eﬃciently simulate data from the

model. Explicit models are also more constrained in terms of recursion and conditionals,

as the number of random variables they need is ﬁxed.

Below, I brieﬂy discuss two density-based PPLs: Stan and Infer.NET.

Stan

Stan (Carpenter et al., 2017) is a popular probabilistic programming language. It has been

adopted and used in numerous ﬁelds, including cosmology (Lieu et al., 2017), microbiology

(Sankaran and Holmes, 2018), epidemiology (Flaxman et al., 2020; Grinsztajn et al.,

2020), sociology (Pierson et al., 2020), psycho-linguistics (J¨ager et al., 2017), and business

forecasting (Taylor and Letham, 2018). Stan’s syntax is inspired by and similar to that

of BUGS (Gilks et al., 1994), and is close to the model speciﬁcation conventions used

in publications in the statistics community. Stan is imperative and compiles to C++

code. It consists of program blocks, which contain sequences of variable declarations and

statements. For example, consider the Eight schools model, deﬁned as:

µ

θn

∼ N

∼ N

(0, 5)

(µ, τ )

τ

∼
yn

∼ N

HalfCauchy(0, 5)

(θn, σn)

for n = 1, . . . , 8

where N, yn, σn for n

1, ..., N

∈ {

}

are given as data.

Program 2.1 gives the Stan program for Eight schools. It declares the observed data N, y,

and σ in the data block and, separately, the parameters µ, τ, and θ in the parameters block.
As τ comes from a half-Cauchy distribution, 4 it takes only positive values: the way to write

this in Stan is through a constrained type, in this case, real<lower=0>. Finally, the model

block deﬁnes the unnormalised joint density of the model. Formally, that is the model

deﬁnes p(µ, τ, θ, σ, y)

(µ

0, 5)HalfCauchy(0, 5)

inference task is to ﬁnd p(µ, τ, θ

∝ N

|

σ, y).

|

(cid:81)

N
n=1 N

(θn

|

µ, τ )

N

(yn

|

θnσn). The

In general, a Stan program can contain up to seven blocks, with all blocks discussed in

detail in Chapter 4. Most notably, the data block declares the data

and the parameters

block declares the parameters θ. Executing these blocks in order by treating the code as

D

4The half-Cauchy distribution has probability density function proportionate to the probability density

function of the Cauchy distribution, but the support of half-Cauchy is constrained to R+.

2.3. Probabilistic programming languages

a conventional imperative code results in evaluating the log density log p(

observed data

D

and at some value of the parameters θ.

27

, θ) for the

D

Program 2.1: Eight schools in Stan

data {

int N;

real y[N];

real sigma[N];

}

parameters {

real mu;

real<lower=0> tau;

real theta[N];

}

model {

mu

tau

∼

∼

normal(0, 5);

cauchy(0, 5);

for (n in 1:N) {

normal(mu, tau);

∼
normal(theta[n], sigma[n]);

theta[n]

∼

y[n]

}

}

Inference. Stan uses HMC, and more specif-

ically, an enhanced version of the No-U-Turn

Sampler (NUTS) (Hoﬀman and Gelman, 2014;

Betancourt, 2017), which is an adaptive path

lengths extension to HMC. As described above,

a Stan program essentially deﬁnes the body of

a function that takes as an argument the pa-

rameters θ and evaluates the joint log density

at that point θ. As p(θ

)

p(θ,

), we can

| D

∝

D

use this function for MCMC inference. Stan

performs inference by running the code impera-

tively for diﬀerent parameter values while using

automatic diﬀerentiation to compute gradients

with respect to the parameters.

Strengths. Stan is ﬂexible, in that it gives

the user full control over the deﬁnition of the

target joint density. Compilation of Stan code

goes through several stages of optimisation and

static transformations, which increases runtime

performance and stability of inference.

Inference is fully automatic, including HMC

hyperparameter tuning and automatic diﬀerentiation, as well as very eﬃcient. Stan also

automatically performs numerous diagnostic checks upon inference and issues warnings

and suggestions for improvement.

Constraints. Being density-based, Stan requires a ﬁxed number of parameters during

inference. It is also not generally possible to simulate data from the model. In addition,

the block syntax makes it diﬃcult to compose diﬀerent Stan programs, or to have ﬂexible

user-deﬁned functions (which I address in Chapter 4). HMC requires the joint density to

be piece-wise diﬀerentiable. This means having discrete data in Stan is possible, but it

is not possible to explicitly encode discrete parameters (which I address in Chapter 5).

While the Stan compiler performs some static optimisations, statistical optimisations, such

as reparameterisation and marginalisation, are mostly the responsibility of the user.

28

Infer.NET

Chapter 2. Probabilistic programming

Infer.NET (Minka et al., 2014) is a probabilistic programming framework, which can be

used within the .NET ecosystem, and it is well-known for its use in skill rating systems

(Herbrich et al., 2006), recommendation systems (Christakopoulou et al., 2016), and other

applications that require real-time responsiveness. Infer.NET constructs an explicit factor

graph corresponding to the speciﬁed program. Every program variable is a variable node

in that graph, and every statement, probabilistic or deterministic, is a factor in that graph.

Program 2.2 shows the Eight schools example written in Infer.NET.

Program 2.2: Eight schools in Infer.NET

var N = Variable.New<int>();

var NRange = new Range(N);

var sigma = Variable.Array<double>(NRange);

var mu = Variable.GaussianFromMeanAndPrecision(0, 5);

var tau sq = HalfCauchySquared(Variable.Constant(12.5));

var theta = Variable.Array<double>(NRange);

theta[NRange] = Variable.GaussianFromMeanAndPrecision(mu, tau sq).ForEach(NRange);

var y = Variable.Array<double>(NRange);

y[NRange] = Variable.GaussianFromMeanAndPrecision(theta[NRange], sigma[NRange]);

Inference.

Infer.NET programs compile to eﬃcient message-passing code. The language

supports both expectation propagation (Minka, 2001) and variational message-passing

(Winn et al., 2005), both of which provide an approximate solution. In models whose

factor graph is a tree of discrete variables, Infer.NET’s message-passing algorithm reduces

to belief propagation (Frey and MacKay, 1998), which is equivalent to variable elimination

(§ 2.2) and gives an exact solution. For some models, Infer.NET is able to exploit the
conditional independence structure available through the explicit factor graph, and use

blocked Gibbs sampling (Geman and Geman, 1984).

Strengths: One of the biggest advantages of Infer.NET is its speed of inference: the

language has been employed in real-life systems that work with large amounts of data and

require very fast inference results. In addition, inference is also fully black-box.

Constraints:

In addition to the constraints of a density-based PPL, Infer.NET also

supports only certain operators. For example, division and multiplication between variables

is not always possible, and the compiler can sometimes throw an error requesting the model

to be reformulated. Unlike other PPLs, Infer.NET also does not provide results about the

joint posterior distribution of parameters, but can only infer the marginal distributions.

2.3. Probabilistic programming languages

29

2.3.3 Sample-based (or implicit) PPLs

Sample-based PPLs, or also implicit PPLs, deﬁne the unnormalised joint density only

implicitly, by encoding a way to sample from that density. Examples of such languages

include Anglican (Wood et al., 2014; Tolpin et al., 2016), Church (Goodman et al., 2008),

Gen (Cusumano-Towner et al., 2019), WebPPL (Goodman and Stuhlm¨uller, 2014), and

PyProb (Baydin et al., 2019). In addition, likelihood-free inference PPLs, such as ELFI

(Lintusaari et al., 2018), SBI (Tejero-Cantero et al., 2020), and Omega (Tavares et al.,

2019) are also sample-based. That is because they deﬁne the joint density only implicitly

through simulation, and support models where the likelihood is not available by deﬁnition.

In these languages, statements such as y

dist(x) are understood as drawing a sample

from the distribution dist(x) at random and binding it to y. As a consequence, models

∼

in sample-based PPLs are essentially simulators: they describe the process of generating

data. When ran forward, these models can usually give us samples from the joint density

deﬁned by the model. However, evaluating this joint density on a ﬁxed-size subset of the

parameters is not always possible. For example, suppose we are interested in parameters

θ that are of ﬁxed support and that the model deﬁnes θ and some set of latent variables z
that have variable support. Then the unnormalised density p∗(θ,
D
not always tractable. It is in this sense that we say that the density is implicit: p∗(θ,
not always tractable, even though an explicit density on traces p(θ, z,

(cid:82)
) may still exist.

p(θ, z,

)dz is

) is

∝

D

D

)

D

Sample-based PPLs often have cleaner, more interpretable syntax compared to density-

based PPLs, but need to adopt less eﬃcient, more general-purpose inference algorithms.

Sample-based PPLs support a larger class of models, including models with complicated

control-ﬂow, recursion, and unbounded number of random variables. While undirected

models are also often supported, they are, in general, less intuitive to write than in

density-based PPLs, due to lack of generative interpretation for undirected factors.

Below, I once again use the Eight schools example to give brief overview of two sample-based

PPLs: Anglican and Gen.

Anglican

Anglican (Wood et al., 2014; Tolpin et al., 2016) is a general-purpose PPL integrated in

Clojure. It supports a wide set of probabilistic programs, including programs with discrete

and continuous parameters, unbounded recursion and control ﬂow. The language has been

extensively formalised and has strong theoretical foundations (Staton et al., 2016).

30

Chapter 2. Probabilistic programming

Program 2.3 shows Eight schools written in Anglican. 5 The model is a defquery: a function

that takes as input the observed data and returns the parameters of interest. Internally, this

query is transformed to continuation passing style (CPS) (similarly to WebPPL (Goodman

and Stuhlm¨uller, 2014)), where continuations are explicitly maintained at two checkpoints:

sample and observe. In brief, an Anglican program is a standard Clojure program, with

the exception of sample and observe, which are executed diﬀerently, as deﬁned by the

inference algorithm. Each inference algorithm deﬁnes its own implementation for the two

checkpoints and is able to alter the behaviour of the code to produce samples from the

posterior. Rainforth (2017, Chapter 7) discusses inference algorithm implementation in

Anglican in detail, while below I discuses the inference strategies readily available as part

of the language.

Program 2.3: Eight schools in Anglican

(defquery schools [N, y, sigma]

(let [mu (sample (normal 0 5))

tau (sample (gamma 1 1))]

(loop [n 0]

(if (< n N)

(let [y n (nth y n)

sigma n (nth sigma n)

theta n (sample (normal mu tau))]

(observe (normal theta n sigma n) y n)

(recur (+ n 1)))))

[mu, tau]))

Inference: Anglican supports a range

of inference techniques. To allow for

inference in any program expressible

in the language, Anglican was the

ﬁrst PPL to adopt particle-based in-

ference (Rainforth, 2017). In addition

to simple importance sampling, stan-

dard SMC (Gordon et al., 1993; Doucet

et al., 2001) and PMCMC (Andrieu

et al., 2009), the language also intro-

duces several new and PPL-focused

particle-based techniques, such as in-

teracting particle MCMC (Rainforth

et al., 2016) and particle Gibbs with ancestor sampling (van de Meent et al., 2015).

Strengths: Anglian provides clean functional syntax for general probabilistic programming.

The semantics of the language is well-studied and give it a strong theoretical background.

Anglican supports higher-order programs with varying support, recursion and stochastic

control-ﬂow. It supports a large number of fully automated inference algorithms, and

is perhaps the most sophisticated PPL in terms of particle-based inference. Composing

models in Anglican is straightforward.

5This Anglican version of Eight schools has been given slightly diﬀerent priors compared to the

previously introduced model, due to availability of built-in distributions of the language.

2.3. Probabilistic programming languages

31

Constraints: Anglican inherits some of the limitations of the inference algorithms it

adopts. In order to be well-suited for SMC inference, an Anglican program needs to

interleave sample and observe statements as much as possible. This is in direct contrast

to other languages, where in general it does not matter at what point observations are

made. To ensure high-quality inference, particle-based methods require a large number of

samples, and thus are typically more computationally-demanding than alternatives.

Gen

Gen (Cusumano-Towner et al., 2019) is a Julia-based PPL centred around the idea of

programmable inference (Mansinghka et al., 2018). It consists of components that allow

the user to build their own inference algorithms, for example by building custom MCMC

kernels, or combining several inference strategies. Like Anglican, Gen is sample-based and

supports programs with complex control ﬂow and recursion.

Program 2.4: Eight schools in Gen

Program 2.4 gives the Eight school

model written in Gen. The model

@gen function schools(N::Int, σ::Vector{Float64})

is a Julia function deﬁned using the

µ = @trace(normal(0, 5), :mu)

τ = @trace(gamma(1, 1), :tau)

for (n, σ n) in enumerate(σ)

θ n = @trace(normal(µ, τ ), (:theta, n))

y n = @trace(normal(θ n, σ n), (:y, n))

end

return nothing

end;

annotation @gen. The function takes

as arguments the unmodelled data

(data we do not assign probability to)

N and σ, and deﬁnes all modelled

variables using a @trace expression

and giving them unique addresses. A

@gen function can be used to gener-

ate traces:

instances of a trace ab-

stract data type, which contains random choices made in the program and provides various

operators such as evaluating the log probability of the trace, or computing the gradient of

the log joint density with respect to diﬀerent parameters.

Inference: While Gen supports various built-in inference algorithms, such as variational

inference, HMC and SMC, the biggest advantage of the language is the ﬂexibility it

provides in combining inference operators to build custom inference algorithms. Gen’s API

is particularly well-suited for exploring diﬀerent SMC strategies and for deﬁning custom

MCMC kernels.

Strengths: Like other sample-based PPLs, Gen provides a very ﬂexible modelling language

that allows for an unbounded number of random variables. Composing probabilistic models

32

Chapter 2. Probabilistic programming

in Gen is straightforward, and in addition, the language also provides generative function

combinators that act as higher-order functions, such as map, and can compactly combine

Gen models. Perhaps the biggest strength of Gen is that it provides components for

building custom inference algorithms, while still automating lower-level details.

Constraints: While its biggest strength, programmable inference in Gen can be challenging

for beginners. Using inference algorithms automatically is less straightforward than with

other languages (for example, there is no fully automated version of SMC). In addition,

Gen’s building blocks are constructed with speciﬁc focus on sampling-based and variational

inference, and there is no support for algorithms such as variational message-passing, loopy

belief propagation, and expectation propagation. Gen supports transformations on traces,

but it is not clear how to apply transformations that need program context, such as, for

example, the one described in Chapter 6.

2.3.4 Eﬀect-handling-based PPLs

Both density-based and sample-based PPLs assign a particular (and diﬀering) meaning

to the statement y

dist(x). In contrast, in eﬀect-handling-based PPLs, this meaning is

not ﬁxed and can change depending on context. Such languages include Pyro (Bingham

∼

et al., 2019), NumPyro (Phan et al., 2019), Edward2 (Tran et al., 2018), and Eﬀ-Bayes

(Goldstein, 2019), and they use algebraic eﬀect handlers to specify the behaviour of a

probabilistic program.

Algebraic eﬀects and handlers (Plotkin and Pretnar, 2009; Plotkin and Power, 2003)

have emerged as a convenient, modular abstraction for controlling computational eﬀects.

Eﬀect-handing-based PPLs tread the probabilistic assignment operation

as an eﬀectful

operation, the behaviours of which is speciﬁed by a separate eﬀect handler. Such an

∼

eﬀect handler can specify behaviour matching that of a density-based PPL, or that of

sampling-based PPL, but it can also give a completely diﬀerent meaning to the

operator.

∼

Deﬁning and working with handlers is part of the PPL itself, and they can be nested to

produce complex composable transformation (Moore and Gorinova, 2018). In contrast,

other languages that treat

as a special checkpoint where program behaviour can

be altered, such as Anglican and WebPPL, can implement diﬀerent algorithm-speciﬁc

∼

behaviour for sampling, but this is seen as part of the inference algorithm implementation

rather than as part of the modelling language.

I give more background on eﬀect-handling-based PPLs, including examples of deﬁning and

composing handlers, in § 6.1. Here, I give a brief overview of one such PPL: Edward2.

2.3. Probabilistic programming languages

33

Edward2

Edward2 (Tran et al., 2018) is a deep probabilistic programming language embedded

into Python and built on top of TensorFlow (Abadi et al., 2015). It has found several

applications in ﬁelds that require processing large amounts of data, such as intelligent

transportation systems (Abodo et al., 2019), recommendation systems (Mladenov et al.,

2020), and biological sequence models (Weinstein and Marks, 2021). Edward2 uses eﬀect

handlers (referred to in the language as interceptors or tracers) to change the meaning of

probabilistic assignment at runtime. Handlers can be deﬁned by the programmer, and can

also be nested to produce complex model transformations.

Program 2.5: Eight schools in Edward2

def schools(N, sigma):

mu = ed.Normal(loc=0., scale=5., name="mu")

tau = ed.HalfCauchy(loc=0., scale=5., name="tau")

theta = ed.Normal(loc=mu * tf.ones(N), scale=tau * tf.ones(N), name="theta")

y = ed.Normal(loc=theta, scale=sigma, name="y")

return y

with ed.interception(log prob):

schools(N=8, sigma=data["sigma"])

Program 2.5 shows the Eight schools model in Edward2. A model is simply a generative

function, that creates some random variables. When run forward, it samples random

variables using random number generation, meaning we can see Edward2 as a sampling-

based PPL. However, when running the model in the context of a log prob handler (last two

lines in Program 2.5), probabilistic assignment statements are instead treated as adding a

term to the model’s log joint density (similarly to Stan). Thus, in this case, we can think

of Edward2 as a density-based PPL.

Inference: Edward2 focuses on gradient-based inference methods, and has automatic

support for various algorithms such as variational inference and HMC. However, the

ﬂexibility provided by adopting eﬀect handlers in the language makes it possible for the

user to perform transformations on their model, compose inference strategies, or derive

their own. I describe one such way in which eﬀect handlers can be utilised to improve

inference in Chapter 6.

34

Chapter 2. Probabilistic programming

Strengths: Like other deep PPLs, Edward2’s inference is particularly well-suited for many

dimensional problems and large amounts of data. Composing models in the language

is straightforward, and eﬀect handlers provide a ﬂexible way to perform programmable

inference. In addition, handlers can be used as a tool for lightweight composable program

transformation, which allows for model reparameterisation, marginalisation and automatic

generation of variational families for VI (Moore and Gorinova, 2018).

Constraints: Edward2 requires some verbose and repeated syntax (for example, when

naming variables). While providing ﬂexibility, eﬀect handlers can be challenging for

inexperienced users and the automated inference support that Edward2 provides is not

always as straightforward as with some alternatives. Caution is required when working

with certain models; for example, models with discrete parameters need to be transformed

to allow for gradient-based inference. Edward2’s handlers are restricted to handlers where

the program continuation is implicit (see Chapter 6 for details) and thus only a limited

number of model transformations are possible.

2.3.5 Other PPLs

Classifying any complex subject into clean and well-deﬁned categories is always hard, and

this is also the case with the PPL classiﬁcation suggested in this dissertation. There are

languages that do not neatly ﬁt in either of the categories we discussed.

One major group of such languages is symbolic PPLs, where the inference result is an

exact symbolic expression. For example, Hakaru (Narayanan et al., 2016) and PSI (Gehr

et al., 2016) transform probabilistic programs into symbolic representations, simplify those,

and deliver an exact expression for the posterior, which may or may not be tractable.

Dice (Holtzen et al., 2020) compiles programs with discrete random variables into boolean

decision diagrams. SPPL (Saad et al., 2021) is applicable to both discrete and continuous

distributions and compiles programs to sum-product expressions. The reason why these

languages are diﬃcult to categorise is that it is unclear whether we can consider the

likelihood available or not. On the one hand, an expression for the likelihood might exist,

and the language may be able to manipulate it to produce the ﬁnal result. But on the

other hand, the evaluation of the likelihood may not tractable, which is often the case

when deterministic observations or unbounded number of random variables is allowed.

Part I

Static optimisation for probabilistic

programming

35

36

Introduction to Part I

Static analysis refers to the class of techniques that can analyse programs without executing

them — all such analysis happens at compile time. Techniques include static type checking

and type inference, data ﬂow and information ﬂow analysis, abstract interpretation, and

symbolic execution. Static analysis can be extremely useful for automatically error checking

code, as well as performing code optimisation.

This part focuses on static analysis for probabilistic programming. It starts with a short

overview of the notation and methods used (Chapter 3). It then introduces the probabilistic

programming language SlicStan — a more compositional version of Stan — and presents

two novel techniques (Chapters 4 and 5) that automatically reﬁne inference by transforming

a SlicStan program at compile time while preserving its semantics.

Formal treatment of programming

CHAPTER 3

languages

Background and intuition

This short chapter contains a description of formal treatment of programming languages

and introduces the notation used in Part I. It is predominantly meant as a simple reference

for some standard notation in programming languages research.

3.1 Formal syntax of programming languages

Usually, programming languages are studied through an idealised calculus deﬁned through

a grammar of terminal and non-terminal symbols. As an example, let’s consider a very

simple arithmetic language deﬁned by the following grammar, where c ranges over numbers

and x ranges over strings:

Simple Arithmetic Language

E ::=

c

x

E1 + E2
E2
E1

−

expression

constant

variable

addition

subtraction

A program in this language is an expression E, which can be a constant, a variable, a sum

of two expressions, or the diﬀerence between two expressions. For example, x + 2 and

3.0

−

2.1 + z + w are valid programs, while 2 ++ 3 and 2

are not.

−

37

38

Chapter 3. Formal treatment of programming languages

3.2 Rules of inference

Aspects of programming languages, such as semantics or type systems, are often formalised

through inductive deﬁnitions. An inductive deﬁnition for some relation is the least relation

(according to an underlying order), which is closed under some set of rules (for an example,

refer to Gordon (1995)). One common way of making an inductive deﬁnition is through

a set of inference rules: rules that consist of one or more premises Pn separated via a
horizontal line from a conclusion Q:

P1

. . . PN

Q

The above rule states that if we know that P1, . . . , PN all hold then we can conclude Q.
The rule is often read in the other direction, from the bottom to the top: we can conclude

Q if we can separately conclude each Pn for n = 1, . . . , N .

For example, consider a simpliﬁed propositional logic, where a logic formula P consists of

only conjunctions and disjunctions of other formulæ:

Simpliﬁed Propositional Logic:

P ::=

T

F

P1
P1

P2
P2

∧

∨

formula

true

false

conjunction

disjunction

We can inductively deﬁne a relation

set of inference rules:

(cid:96)

P , read as “P is provable”, through the following

Rules of the Simpliﬁed Propositional Logic:

(True)

(And)

(Or1)

(Or2)

T

(cid:96)

(cid:96)

P1

P1

(cid:96)

∧

P2

(cid:96)
P2

(cid:96)

P1

(cid:96)
P1

P2

∨

(cid:96)

P2

(cid:96)
P1

P2

∨

The rule (True) is an axiom: the formula T is provable without any premise.

On the other hand, (And) states that if P1 is provable and P2 is provable, than we can
conclude that P1
P2 appears as a
conclusion only in rule (And), we know that to show P1
P2 is provable we must show

P2 is provable. Alternatively, as the term

P1

∧

∧

(cid:96)

∧

3.2. Rules of inference

39

that P1 is provable and that P2 is provable. This is in contrast to disjunction, where to
P2 we must either show that P1 is provable (by applying (Or1)) or that P2 is
show
provable (by applying (Or2)).

P1

∨

(cid:96)

We can use these rules to check if a formula is provable by deriving a proof tree. For

example:

(cid:96)

(True)

T

(Or1)

(cid:96)
T
∨
(cid:96)
F)
(T
∨
∧
(T
F)
∨
∨

F
(T
∨
F)]

F)

(Or2)

(And)

(True)

T

T

∧

(cid:96)

(cid:96)
[(T
∧

A proof tree of a formula that is not provable does not exist. For example, it is not possible
T. If it was possible, it would be through the rule (And),

to derive a proof tree of

F

(cid:96)

∧

meaning we would need to derive a proof tree for

T and

F. Since there is no rule that

can show

(cid:96)

F, it is also not possible to show a proof for

(cid:96)

(cid:96)
F

(cid:96)

T.

∧

Inference rules allow for a concise formal deﬁnition of a calculus and can be used to derive

proofs of various properties. This is often either by structural induction, where we assume

a property holds for the sub-terms building up an expression in the calculus, or by rule

induction, which is induction on the height of the proof tree for a property. Structural

induction and rule induction are used in various proofs throughout Chapters 4 and 5.

As an example of rule induction, let us prove a simple property of the simpliﬁed logic of

this section: if a formula is provable then it must contain T as a subterm.

To state this formally, we introduce a new relation t(P ), read “P contains T”, deﬁned as:

(T True)

(T And1)

(T And2)

(T Or1)

(T Or2)

t(P1)

t(P2)

t(P1)

t(P2)

t(T)

t(P1

P2)

∧

t(P1

P2)

∧

t(P1

P2)

∨

t(P1

P2)

∨

Proposition 1 (Provable formulæ contain the subterm T).

For any formula P ,

P implies t(P ).

(cid:96)

Proof. We prove by rule induction on the derivation of

P .

(cid:96)

We start by the base case when the proof tree for
whole proof tree must be the axiom (True):

(cid:96)

P is of height 1. In other words, the

• Case (True). P = T. Thus t(P ) by (T True).

This concludes the induction’s base case. Next, we assume the induction hypothesis that

40

Chapter 3. Formal treatment of programming languages

for any formula P (cid:48) such that
(cid:96)
or equal to n, we have t(P (cid:48)). Consider a formula P with

P (cid:48), and the height of the proof tree of

(cid:96)

P proof tree of height n + 1.

P (cid:48) is smaller than

Then, it must be the case that we derived

P using one or more proof tree of height

in combination with one of the following rules:

(cid:96)

n

≤

(cid:96)

• Case (And). P = P1

P2, for some P1 and P2. According to (And), it must be the
∧
P2. But the proof tree of each of those is of height at most n,
case that
thus, by the induction hypothesis, t(P1) and t(P2). By (T And1) (or (T And2)), it
follows that t(P ).

P1 and

(cid:96)

(cid:96)

• Case (Or1). P = P1

∨

P2, for some P1 and P2. According to (Or1), it must be
P1 is of height at most n, thus, by the

the case that
induction hypothesis, t(P1). By (T Or1) it follows that t(P ).

P1. But the proof tree for

(cid:96)

(cid:96)

• Case (Or2). P = P1

∨

P2, for some P1 and P2. According to (Or2), it must be
P2 is of height at most n, thus, by the

the case that
induction hypothesis, t(P2). By (T Or2) it follows that t(P ).

P2. But the proof tree for

(cid:96)

(cid:96)

This concludes the induction step and hence the proof.

3.3 Semantics of programming languages

The semantics of a programming language is a formal speciﬁcation of its meaning. This

can be done by formalising the execution of a program (operational semantics, Plotkin

(1981); Nielson and Nielson (1992, Chapter 2)) or by attaching a mathematical meaning to

the terms of the language (denotational semantics, Nielson and Nielson (1992, Chapter 5)).

As an example, let’s give the operational semantics of our arithmetic language.

Suppose σ is a mapping from variable names xn to concrete values Vn: σ =
V1, . . . xN
which is such that an expression E, where variables are substituted with values according

{
. The semantics of the language is given by the relation (σ, E)
}

(cid:55)→
V ,

VN

(cid:55)→

x1

⇓

to σ, computes a value V . We can read (σ, E)

V as “In the context of σ expression E

evaluates to V .” The operational semantics can be speciﬁed recursively for diﬀerent terms

⇓

of the calculus as follows:

Operational Semantics of the Arithmetic Language:

(Eval Cst)

(Eval Var)

(Eval Add)

(Eval Sub)

(σ, c)

c

⇓

x

∈
(σ, x)

σ(x)

⇓

dom(σ)

(σ, E1)

V1

(σ, E2)

V2

(σ, E1)

V1

(σ, E2)

⇓
(σ, E1 + E2)

⇓
V1 + V2

⇓

⇓

(σ, E1

E2)

V1

⇓

−

−

V2

⇓
V2

3.4. Type checking and type inference

41

(Eval Cst) simply states that a constant c always evaluates to the same constant c,
regardless of the store σ. On the other hand, as per (Eval Var) a variable x evaluates to
the value of x given by σ, as long as x is in the domain of σ. (Eval Add) states that to

evaluate E1 + E2 we must evaluate E1 and E2 separately and add the result. Here, the
usage of + in V1 + V2 is understood as standard addition between values and not a symbol
of the calculus. Similarly, (Eval Sub) requires evaluating the subexpressions E1 and E2
to evaluate E1

E2.

−

3.4 Type checking and type inference

Type checking (Pierce, 2002; Nielson et al., 2004, Chapter 5) is a form of program analysis

where we verify the correct usage of types, such as integer, boolean, array or functional

types. Static type checking analyses the parse tree of a program without running it, to

ﬂag any type discrepancies. This is useful, as it allows us to detect possible mistakes in

the code at compile time.

To type-check programs in some language, we need to formalise its type system, which

includes rules for building types and typed expressions in the language. For example, let’s

deﬁne the type system of our arithmetic language as follows:

Types of the Arithmetic Language:

Typing Environment:

τ ::= real

int

|

Γ ::=

x1
{

(cid:55)→

τ1, . . . , xn

τn

}

(cid:55)→

Judgement of the Type System:

E : τ

Γ

(cid:96)

expression E has type τ according to Γ

A type in the language is denoted by τ , which is either real — a ﬂoating point number

— or int — an integer. The typing environment holds information about the types of

variables in the form of a mapping from variable names to types. The typing judgement

Γ

E : τ is a relation on Γ, E and τ such that an expression E is of type τ if the types of

variables in E are given by Γ. The typing rules specify what programs in the language are

(cid:96)

well-typed : what is needed for the relation Γ

E : τ to hold.

(cid:96)

42

Chapter 3. Formal treatment of programming languages

Typing Rules of the Arithmetic Language:

(Cst)

τ = ty(c)

c : τ

Γ

(cid:96)

(Var)

Γ(x) = τ

x : τ

Γ

(cid:96)

(Add)

(Sub)

Γ

E1 : τ Γ

E2 : τ

Γ

E1 : τ Γ

E2 : τ

(cid:96)

Γ

(cid:96)

(cid:96)

E1 + E2 : τ

(cid:96)

Γ

E1

(cid:96)

−

(cid:96)
E2 : τ

(Cst) simply states that the program type of a constant c corresponds to its numerical
Z). (Var) states
type (we assume ty(c) = int if c
that the type of a variable is determined by the typing environment Γ. (Add) and (Sub)

Z and ty(c) = real if c

R and c /
∈

∈

∈

deﬁne well-typedness of expressions of the form E1
E2 to be
well-typed of type τ in Γ, it needs to be the case that both E1 and E2 are well-typed of
, but it is not
type τ in Γ. This means that 1.0 + (x

2.5) is well-typed in Γ =

in order for E1

E2:

real

±

±

−

x
{

(cid:55)→

}

well-typed in Γ =

x

{

.
int
}

(cid:55)→

Some typed languages require all variable types to be explicitly speciﬁed as part of the

program and perform type checking as a form of a correctness check. It is also possible to

work with only partially speciﬁed types and infer any missing types through type inference

(Pierce, 2002, Chapter 22). For example, by analysing the program 1.0+(x

2.5) according

to the typing rules of our arithmetic language, we can deduce that x must be of type real

−

in order for the program to be well-typed.

3.5

Information ﬂow analysis

One program analysis technique central to the contributions of Part I is information ﬂow

analysis (Volpano et al., 1996; Abadi et al., 1999). Information ﬂow is the transfer of

information between two variables in a computation. In the program y = x + 1 information

ﬂows from x to y.

Analysing the ﬂow of information can be especially useful in detecting program vulner-

abilities. Static analysis techniques concerning the ﬂow of information are popular in

the security community, where the goal is to prove that systems do not leak information.

Secure information ﬂow analysis, summarised by Sabelfeld and Myers (2003), and Smith

(2007), concerns systems where variables have one of several security levels.

For example, suppose we have two security levels, low and high. Variables of level low
are low security; they hold public data. In contrast, variables of level high are of high

security; they hold secret data. In such system, we want to disallow the ﬂow of secret

information to a public variable, but allow other ﬂows of information. That is, if L is

3.5.

Information ﬂow analysis

43

a variable of security level low, H is a variable of security level high, and f is some

function, we want to forbid statements such as L = f (H), but allow:

• L = f (L)

• H = f (H)

• H = f (L)

But there are ways other than simple data ﬂow, in which information can leak to a lower

security level. For example through conditional statements. Suppose L and H from above

are boolean variables. We want to disallow statements such as if (H) then L = true, as

this leaks information about the higher security level H to the lower security level L.

In the general case, we may be interested in having more than two information levels.

Formally, those levels form a lattice — a partially ordered set (L; <), where every two

elements of L have a unique least upper bound and a unique greatest lower bound. Secure

information ﬂow analysis is used to ensure that information ﬂows only upwards with
respect to that lattice. In the case with low and high, information ﬂows only from
low to high and never in the other direction. This is also known as the noninterference

property — changes to conﬁdential inputs lead to no changes in public outputs of a system

(Goguen and Meseguer, 1982).

One way to ensure noninterference is by formalising a type system such that each variable

has a level type and noninterference holds for any well-typed program. This is also the

approach used in this dissertation, as discussed in Chapters 4 and 5.

CHAPTER 4

SlicStan

Optimising pre-and post-processing code for inference

This chapter describes SlicStan: a version of Stan that contains no program blocks and is

more compositional. The main function of Stan’s program blocks is to explicitly separate

the program into a pre-processing part (executed only once), post-processing part (executed

after each sample is drawn), and the core of the model, which is executed to generate a

single sample and requires the most computational resources. SlicStan demonstrates that

such separation need not be done manually by the programmer, but can be automated

using information-ﬂow analysis. We proceed with the main contribution of the chapter,

the paper Probabilistic Programming with Densities in SlicStan: Eﬃcient, Flexible, and

Deterministic (§ 4.1), clarifying the contributions with respect to previous work (§ 4.2),
and discussing the impact of SlicStan (§ 4.3).

4.1 The paper

This section presents the work Probabilistic Programming with Densities in SlicStan:

Eﬃcient, Flexible, and Deterministic. The paper gives the ﬁrst formal semantics of Stan

and introduces SlicStan: a more compositional version of Stan. It describes a semantic-

preserving procedure for translating SlicStan to Stan. By combining information-ﬂow

analysis and type inference, SlicStan performs automatic program optimisation, which

frees users from the need to explicitly encode the parts of the program that correspond to

pre-processing, post-processing and the computationally heavy core of the model.

The paper was accepted for presentation at the 46th ACM SIGPLAN Symposium on

Principles of Programming Languages (POPL 2019) and included in the Proceedings of the

ACM on Programming Languages, Volume 3, Issue POPL. Out of 267 papers submitted

in total, 77 papers were accepted.

45

35ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministicMARIAI.GORINOVA,UniversityofEdinburgh,UKANDREWD.GORDON,MicrosoftResearchCambridge,UKandUniversityofEdinburgh,UKCHARLESSUTTON,GoogleBrain,USandUniversityofEdinburgh,UKStanisaprobabilisticprogramminglanguagethathasbeenincreasinglyusedforreal-worldscalableprojects.However,tomakepracticalinferencepossible,thelanguagesacrificessomeofitsusabilitybyadoptingablocksyntax,whichlackscompositionalityandflexibleuser-definedfunctions.Moreover,thesemanticsofthelanguagehasbeenmainlygivenintermsofintuitionaboutimplementation,andhasnotbeenformalised.ThispaperprovidesaformaltreatmentoftheStanlanguage,andintroducestheprobabilisticprogramminglanguageSlicStan—acompositional,self-optimisingversionofStan.Ourmaincontributionsare(1)theformalisationofacoresubsetofStanthroughanoperationaldensity-basedsemantics;(2)thedesignandsemanticsoftheStan-likelanguageSlicStan,whichfacilitiesbettercodereuseandabstractionthroughitscompositionalsyntax,moreflexiblefunctions,andinformation-flowtypesystem;and(3)aformal,semantic-preservingprocedurefortranslatingSlicStantoStan.CCSConcepts:•Theoryofcomputation→Operationalsemantics;Programanalysis;Probabilisticcomputation;•Mathematicsofcomputing→Statisticalsoftware;Markov-chainMonteCarlomethods;AdditionalKeyWordsandPhrases:probabilisticprogramming,informationflowanalysisACMReferenceFormat:MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton.2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic.Proc.ACMProgram.Lang.3,POPL,Article35(January2019),48pages.https://doi.org/10.1145/32903481INTRODUCTION1.1Background:ProbabilisticProgrammingLanguagesandStanProbabilisticprogramminglanguages[Gordonetal.2014b]areaconcisenotationforspecifyingprobabilisticmodels,whileabstractingtheunderlyinginferencealgorithm.Therearemanysuchlanguages,includingBUGS[Gilksetal.1994],JAGS[Plummeretal.2003],Anglican[Woodetal.2014],Church[Goodmanetal.2012],Infer.NET[Minkaetal.2014],Venture[Mansinghkaetal.2014],Edward[Tranetal.2016]andmanyothers.Stan[Carpenteretal.2017],withnearly300,000downloadsofitsRinterface[StanDevelopmentTeam2018a],isperhapsthemostwidelyusedprobabilisticprogramminglanguage.Stan’ssyntaxisdesignedtoenableautomaticcompilationtoanefficientHamiltonianMonteCarlo(HMC)inferencealgorithm[Nealetal.2011],whichallowsprogramstoscaletoreal-wordprojectsinstatisticsanddatascience.(Forexample,theforecastingtoolProphet[TaylorandLetham2017]usesStan.)Thisefficiencycomesataprice:Stan’ssyntaxlacksthecompositionalityofothersimilarsystems,suchasEdward[Tranetal.2016]andPyMC3[Salvatieretal.2016].ThedesignofStanassumesthattheprogrammerneedstoorganisetheirmodelintoseparateblocks,whichcorrespondtodifferentstagesoftheinferencealgorithm(preprocessing,sampling,postprocessing).ThiscompartmentalisedPermissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.Forallotheruses,contacttheowner/author(s).©2019Copyrightheldbytheowner/author(s).2475-1421/2019/1-ART35https://doi.org/10.1145/3290348Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:2MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonsyntaxaffectstheusabilityofStan:relatedstatementsmaybeseparatedinthesourcecode,andfunctionsarerestrictedtoonlyactingwithinasinglecompartment.ItisdifficulttowritecomplexStanprogramsandencapsulatedistributionsandsub-modelstructuresintore-usablelibraries.1.2GoalsandKeyInsightOurgoalsare(1)toexaminetheprinciplesandassumptionsbehindtheprobabilisticprogramminglanguageStanthathelpitbridgethegapbetweenprobabilisticmodellingandblack-boxinference;and(2)todesignasuitableabstractionthatcapturesthestatisticalmeaningofStan’scompartments,butallowsforcompositionalandmoreflexibleprobabilisticprogramminglanguagesyntax.OurkeyinsightisthattheessenceofaprobabilisticprograminStanisinfactadeterministicimperativeprogram,thatcanbeautomaticallyslicedintothedifferentcompartmentsusedinthecurrentsyntaxforStan.Itmaycomeasasurprisethataprobabilisticprogramisdeterministic,butwhenperformingBayesianinferencebysamplingparameters,theprobabilisticprogramservestocomputeadeterministicscoreataspecificpointoftheparameterspace.AnimplicationofthisinsightisthatstandardformsofproceduralabstractionareeasilyadaptedtoStan.1.3TheInsightbyExampleAsdemonstration,andasacandidateforafuturere-designofStan,wepresentSlicStan1—acompositional,Stan-likelanguage,whichsupportsfirst-orderfunctions.Below,weshowanexampleofaStanprogram(right),andthesameprogramwritteninSlicStan(left).Inbothcases,thegoalistoobtainsamplesfromthejointdistributionofthevariablesy∼N(0,3)andx∼N(0,exp(y/2)),usingauxiliarystandardnormalvariablesforperformance.Workingwithsuchauxiliaryvariables,insteadofdefiningthemodelintermsofxandydirectly,canfacilitateinferenceandisastandardtechnique.Wegivemoredetailsin§§5.4andAppendixD.SlicStanrealmy_normal(realm,reals){realstd∼normal(0,1);returns*std+m;}realy=my_normal(0,3);realx=my_normal(0,exp(y/2));.Stanparameters{realy_std;realx_std;}transformedparameters{realy=3*y_std;realx=exp(y/2)*x_std;}model{y_std∼normal(0,1);x_std∼normal(0,1);}Inbothprograms,theaimistoobtainsamplesfortherandomvariablesxandy,whicharedefinedbyscalingandshiftingstandardnormalvariables.InSlicStanwedosobycallingthefunctionmy_normaltwice,whichdefinesalocalparameterstdandencapsulatesthetransformationofeachvariable.Stan,ontheotherhand,doesnotsup-portfunctionsthatdeclarenewparameters,becauseallparametersmustbedeclaredinsidetheparametersblock.Weneedtowriteouteachtransformationexplicitly,alsoexplicitlydeclaringeachauxiliaryparameter(x_stdandy_std).1SlicStan(pronouncedslick-Stan)standsfor“SlightlyLessIntenselyConstrainedStan”.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:3TheSlicStancodeisaconventionaldeterministicimperativeprogram,wherethemodelstatementstd∼normal(0,1)isaderivedformofanassignmenttoareservedvariablethatholdsthescoreataparticularpointoftheparameterspace.Duetotheabsenceofblocks,SlicStan’ssyntaxiscompositionalandmorecompact.StatementsthatwouldbelongtodifferentblocksofaStanprogramcanbeinterleaved,andnounderstandingabouttheperformanceimplicationsofdifferentcompartmentsisrequired.Viaaninformation-flowanalysis,weautomaticallytranslatetheprogramonthelefttotheoneontheright.1.4CoreContributionsandOutlineInshort,thispapermakesthefollowingcontributions:•WeformalisethesyntaxandsemanticsofacoresubsetofStan(§2).Tothebestofourknowledge,thisisthefirstformaltreatmentofStan,despitethepopularityofthelanguage.•WedesignSlicStan—acompositionalStan-likelanguagewithfirst-orderfunctions.Weformaliseaninformation-flowtypesystemthatcapturestheessenceofStan’scompartmen-talisedsyntax,givetheformalsemanticsofSlicStan,andprovestandardresultsforthecalculus,includingnoninterferenceandtype-preservationproperties(§3).•WegiveaformalprocedurefortranslatingSlicStantoStan,andprovethatitissemanticspreserving(§4).•WeexaminetheusabilityofSlicStancomparedtothatofStan,usingexamples(§5).Thispaperalsoincludesanappendix,whichprovidesadditionaldetails,discussionandexamples.2CORESTANStan[Carpenteretal.2017]isaprobabilisticprogramminglanguage,whosesyntaxissimilartothatofBUGS[Gilksetal.1994;Lunnetal.2013],andaimstobeclosetothemodelspecificationconventionsusedinthestatisticscommunity.Thissectiongivesthesyntax(§§2.1;§§2.3)andsemantics(§§2.2;§§2.4)ofCoreStan,acoresubsetofStan.Thesubsetomits,forexample,constraintdatatypes,whileloops,randomnumbergenerators,recursivefunctions,andlocalvariables.Tothebestofourknowledge,ourworkisthefirsttogiveaformalsemanticstothecoreofStan.Afulldescriptivelanguagespecificationcanbefoundintheofficialreferencemanual[StanDevelopmentTeam2017].2.1SyntaxofCoreStanExpressionsandStatementsThebuildingblocksofaStanstatementareexpressions.InCoreStan,expressionscovermostofwhattheStanmanualspecifies,includingvariables,constants,arraysandarrayelements,andfunctioncalls(ofbuiltinfunctions).Weletxrangeovervariables.L-valuesareexpressionslimitedtoarrayelementsx[E1]...[En],wherethecasen=0correspondstoavariablex.StatementscoverthecorefunctionalityofStan,withtheexceptionofwhilestatements,whichweomittotomakeshreddingofSlicStanpossible(see§§3.1and§§4.1).CoreStanSyntaxofExpressions:E::=expressionxvariablecconstant[E1,...,En]arrayE1[E2]arrayelementf(E1,...,En)functioncall2L::=x[E1]...[En]n≥0L-valueCoreStanSyntaxofStatements:S::=statementL=EassignmentS1;S2sequencefor(xinE1:E2)Sforloopif(E)S1elseS2ifstatementskipskipProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:4MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonWeassumeasetofbuiltinfunctions,rangedoverbyf.Wealsoassumeasetofstandardbuiltincontinuousordiscretedistributions,rangedoverbyd.Eachcontinuousdistributiondhasacorrespondingbuiltinfunctiond_lpdf,whichdefinesitslogprobabilitydensityfunction.Inthispaper,weomitdiscreterandomvariablesforsimplicity.Definedlikethis,thesyntaxofStanstatementsisoneofastandardimperativelanguage.Whatmakesthelanguageprobabilisticisthereservedvariabletarget,whichholdsthelogarithm3oftheprobabilitydensityfunctiondefinedbytheprogram(uptoanadditiveconstant),evaluatedatthepointspecifiedbythecurrentvaluesoftheprogramvariables.Forexample,todefineaStanmodelwithrandomvariables,µandx,whereweassumethevariablesarenormallydistributedandµ∼N(0,1)andx∼N(µ,1),wewrite:target=normal_lpdf(mu,0,1)+normal_lpdf(x,mu,1);4Here,normal_lpdfisthelogdensityofthenormaldistribution:logN(x|µ,σ)=−(x−µ)22σ2−12log2πσ2.Thevalueoftargetisequaltothelogarithmofthejointdensityoverµandx,logp(µ,x),evaluatedatthecurrentvaluesoftheprogramvariablesmuandx.Supposexissomeknowndata,andµisanunknownparameterofthemodel.Weareinterestedincomputingtheposteriordistributionofµgivenx,p(µ|x)∝p(µ,x)=N(x|µ,1)N(µ|0,1).Standirectlyencodesafunctionthatcalculatesthevalueofthelogposteriordensity(uptoanadditiveconstant),andstoresitintarget.Thus,inadditiontoStan’scorestatementsyntax,wehaveaderivedformformodellingstatements:DerivedFormforModelStatements:E∼d(E1,...En),target=target+d_lpdf(E,E1,...En)modelstatementInStan,“∼”isnotconsideredtomean“drawasamplefrom”,butrather“modifythejointdistributionoverparametersanddata.”Thisisalsoreflectedbythesemanticsgivenin§§2.4.2.2OperationalSemanticsofStanStatementsNext,wedefineastandardbig-stepoperationalsemanticsforStanexpressionsandstatements:Big-stepRelation(s,E)⇓Vexpressionevaluation(s,S)⇓s′statementevaluationHere,sands′arestates,andvaluesVaretheexpressionsconformingtothefollowinggrammar:ValuesandStates:V::=valuecconstant[V1,...,Vn]arrays::=x17→V1,...,xn7→Vnxidistinctstate(finitemapfromvariablestovalues)Therelation⇓isdeterministicbutpartial,aswedonotexplicitlyhandleerrorstates.Thepurposeoftheoperationalsemanticsistodefineadensityfunctionin§§2.4,andanyerrorsleadtothedensitybeingundefined.Intherestofthepaper,weusethenotationforstatess=x17→V1,...,xn7→Vn:2Iffisabinaryoperator,e.g.“+”,wewriteitininfix.3Stanevaluatestheunnormaliseddensityinthelogdomaintoensurenumericalstabilityandtosimplifyinternalcomputa-tions.Wefollowthisstylethroughoutthepaper,anddefinethesemanticsintermsoflogp∗,insteadofp∗.4Wetreattargetasamutableprogramvariableforsimplicity.ThisslightlydiffersfromtheactualimplementationofStan,wheretargetdoesnotallowforgenerallookupandupdate,butitisaspecialbitofstatethatcanonlybeincremented.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:5•s[x7→V]isthestates,butwherethevalueofxisupdatedtoVifx∈dom(s),ortheelementx7→Visaddedtosifx<dom(s).•s[−x]isthestates,butwherexisremovedfromthedomainofs(ifitwerepresent).Wealsodefinelookupandupdateoperationsonvalues:•IfUisann-dimensionalarrayvalueforn≥0andc1,...,cnaresuitableindexesintoU,thenthelookupU[c1]...[cn]isthevalueinUindexedbyc1,...,cn.•IfUisann-dimensionalarrayvalueforn≥0andc1,...,cnaresuitableindexesintoU,thentheupdateU[c1]...[cn]:=VisthearraythatisthesameasUexceptthatthevalueindexedbyc1,...,cnisV.OperationalSemanticsofExpressions:(EvalConst)(s,c)⇓c(EvalVar)V=s(x)x∈dom(s)(s,x)⇓V(EvalArr)(s,Ei)⇓Vi∀i∈1..n(s,[E1,...,En])⇓[V1,...,Vn](EvalArrEl)(s,E1⇓V)(s,E2⇓c)(s,E1[E2])⇓V[c](EvalPrimCall)(s,Ei)⇓Vi∀i∈1...nV=f(V1,...,Vn)5(s,f(E1,...,En))⇓VOperationalSemanticsofStatements:(EvalAssign)(whereL=x[E1]...[En])(s,Ei)⇓Vi∀i∈1..n(s,E)⇓VU=s(x)U′=(U[V1]...[Vn]:=V)(s,L=E)⇓(s[x7→U′])(EvalSeq)(s,S1)⇓s′(s′,S2)⇓s′′(s,S1;S2)⇓s′′(EvalIfTrue)(s,E)⇓true(s,S1)⇓s′(s,if(E)S1elseS2)⇓s′(EvalIfFalse)(s,E)⇓false(s,S2)⇓s′(s,if(E)S1elseS2)⇓s′(EvalForTrue)6(s,E1)⇓c1(s,E2)⇓c2c1≤c2(s[x7→c1],S)⇓s′(s′[−x],for(xin(c1+1):c2)S)⇓s′′(s,for(xinE1:E2)S)⇓s′′(EvalForFalse)(s,E1)⇓c1(s,E2)⇓c2c1>c2(s,for(xinE1:E2)S)⇓s(EvalSkip)(s,skip)⇓s2.3SyntaxofStanAfullStanprogramconsistsofsixprogramblocks,eachofwhichisoptional.Blocksappearinorder.Eachblockhasadifferentpurposeandcanreferencevariablesdeclaredinitselforpreviousblocks.Formally,wedefineaStanprogramasasequenceofsixblocks,eachcontainingvariabledeclarationsorStanstatements,asshownnext.WealsopresentanexampleStanprogramthatcontainsallsixblocksin§§5.2.5f(V1,...,Vn)meansapplyingthebuiltinfunctionfonthevaluesV1,...,Vn.6TomakeshreddingtoStanpossible,CoreStanonlysupportsfor-loopswheretheloopboundsdonotchangeduringexecution:E2doesnotcontainanyvariablesthatSwritesto.ThisdiffersfromthemoreflexibleloopsimplementedinStan.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:6MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonStanprogram:P::=StanProgramdata{Γd}transformeddata{Γtd,Std}parameters{Γp}transformedparameters{Γtp,Stp}model{Sm}generatedquantities{Γдq,Sдq}ArraysinStanaresized,butwedonotincludeanystaticchecksonarraysizesinthispaper.StanTypesandTypeEnvironment:Γ::=x1:τ1,...,xn:τn∀i∈1...nxidistinctdeclarationsτ::=real|int|bool|τ[n]typensizeThesizeofanarray,n,canbeanumberoravariable.Forsimplicity,wetreatnasdecorativeanddonotincludechecksonthesizesinthetypesystemofStan.However,thesystemcanbeextendedtoalightweightdependenttypesystem,similarlytoTabularasextendedbySzymczak[2018].EachprogramblockinStanhasadifferentpurposeasfollows:•data:declarationsoftheinputdata.•transformeddata:definitionofknownconstantsandpreprocessingofthedata.•parameters:declarationsoftheparametersofthemodel.•transformedparameters:declarationsandstatementsdefiningtransformationsofthedataandparameters.•model:statementsdefiningthedistributionsofrandomvariablesinthemodel.•generatedquantities:declarationsandstatementsthatdonotaffectinference,usedforpostprocessing,orpredictionsforunseendata.WedefineaconformancerelationonstatessandtypingenvironmentsΓ.AstatesconformstoanenvironmentΓ,wheneversprovidesvaluesofthecorrecttypesforthevariablesgiveninΓ:ConformanceRelation:s|=ΓstatesconformstoenvironmentΓRulefortheConformanceRelation:(StanState)Vi|=τi∀i∈I(xi7→Vi)i∈I|=(xi:τi)i∈IHere,V|=τdenotesthatthevalueVisoftypeτ,andhasthefollowingdefinition:•c|=int,ifc∈Z,c|=real,ifc∈R,andc|=boolifc∈{true,false}.•[V1,...,Vn]|=τ[m],if∀i∈1...n.Vi|=τ.Wedonotincludeanychecksonarraysizesinthispaper,thuswedonotassumenandmarethesameinthisdefinition.Theevaluationrelationisnotdefinedoninitialstatesthatleadtoarrayout-of-boundserrors.2.4Density-BasedSemanticsofStanFinally,wegivethesemanticsofStanintermsofthebig-steprelationfrom§§2.2.AstheStanDevelopmentTeam[2017]explain:Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:7AStanprogramdefinesastatisticalmodelthroughaconditionalprobabilityfunctionp(θ|y,x),whereθisasequenceofmodeledunknownvalues(e.g.,modelparameters,latentvariables,...),yisasequenceofmodeledknownvalues,andxisasequenceofunmodeledpredictorsandconstants(e.g.,sizes,hyperparameters).(p.22)Morespecifically,aStanprogramisexecutedtoevaluateafunctiononthedataandparameterslogp∗(θ|D),forsomegiven(andfixed)valuesofDandθ.Thisfunctionencodesthelogjointdensityofthedataandparameterslogp(θ,D)uptoanadditiveconstant,andalsoequalsthelogdensityoftheposteriorlogp(θ|D)uptoanadditiveconstant:logp(θ|D)=logp(θ,D)−logp(D)∝logp(θ,D)∝logp∗(θ|D)Thereturnvalueoflogp∗(θ|D)isstoredinthereservedvariabletarget.WegivethesemanticsofCoreStanthroughthisunnormalisedlogposteriordensityfunction.ConsideraCoreStanprogramPdefinedaspreviously,andthestatementS=Std;Stp;Sm;Sдq.ThesemanticsofPistheunnormalisedlogposteriordensityfunctionlogp∗onparametersθgivendataD(whereθ|=ΓpandD|=Γd):logp∗(θ|D),s′[target]ifthereiss′suchthat((D,θ,target7→0),S)⇓s′Ifthereisnosuchs′thenthelogdensityisundefined.Observealsothatifsuchans′exists,itisunique,becausetheoperationalsemanticsisdeterministic.Forexample,supposethatPspecifiesasimplemodelforthedataarrayy:data{intN;real[N]y;}parameters{realmu;realsigma;}model{mu∼normal(0,1);sigma∼normal(0,1);for(iin1:N){y[i]∼normal(mu,sigma);}}Supposealsothatθ=(mu7→µ,sigma7→σ)andD=(N7→n,y7→y),forsomeµ,σ,n,andavectoryoflengthn.ThestatementS=Std;Stp;Sm;Sдqisthenthebodyofthemodelblockasspecifiedabove.Then((D,θ,target7→0),S)⇓s′,withs′[target]=logN(µ|0,1)+logN(σ|0,1)+˝ni=1logN(yi|µ,σ).Thisispreciselythelogjointdensityonmu,sigmaandy,whichisproportionatetotheposteriorofmuandsigmagiveny.Thefunctionlogp∗(θ|D)isnota(log)density,butratheritencodesthelogarithmofthedensityoftheposterioruptoanadditiveconstant.Suchunnormalisedlogdensityuniquelydefinesthelogdensitylogp(θ|D):logp(θ|D)=logp∗(θ|D)−logZ(D)whereZ(D)=∫p∗(θ|D)dθThevalueZ(D)iscalledthenormalisingconstant(itisaconstantwithrespecttothevariablesθthatthedensityisdefinedon).ComputingZ(D)isinmostcasesintractable.Thus,manyinferencealgorithms(includingthoseofStan)aredesignedtosuccessfullyapproximatetheposterior,relyingonlyonbeingabletoevaluateafunctionproportionaltoit:anunnormaliseddensityfunction,suchaslogp∗(θ|D)above.ThegoalofthispaperistoformalisethestatisticalmeaningofaStanprogram,asgivenbythequotationfromthereferencemanualabove.Thissemanticsconcentratesondefiningtheunnor-malisedlogposteriorofparametersgivendata,butomitsthefactthatthevaluesoftransformedparametersandgeneratedquantitiesblocksarealsopartoftheobservablestate.TransformedProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:8MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonparametersandgeneratedquantitiescanbeseenasvariablesthataregeneratedusingthefunctionд(θ,D),s′[dom(Γtp∪Γдq)]fors′definedaspreviously.AppendixB.1discussesgeneratedquan-titiesinmoredetail,andweleavetheirfulltreatmentforfuturework.Moreover,AppendixB.2discusseshowthisdensity-basedsemanticsrelatestootherimperativeprobabilisticlanguagessemantics,suchasthesampling-basedsemanticsofProb[Huretal.2015].2.5InferenceExecutingaStanprogramconsistsofgeneratingsamplesfromtheposteriordistributionp(θ|D),asawayofperformingBayesianinference.TheprimaryalgorithmthatStanusesistheasymptoticallyexactalgorithmHamiltonianMonteCarlo(HMC)[Nealetal.2011],andmorespecifically,anenhancedversionoftheNo-U-TurnSampler(NUTS)[Betancourt2017;HoffmanandGelman2014],whichisanadaptivepathlengthsextensiontoHMC.HMCisaMarkovChainMonteCarlo(MCMC)method(seeMurray[2007]forareviewonMCMC).SimilarlytootherMCMCmethods,itobtainssamples{θi}∞i=1fromthetargetdistribution,byusingthelatestsampleθnandacarefullydesignedtransitionfunctionδtogenerateanewsampleθn+1=δ(θn).Whensamplingfromtheposteriorp(θ|D),HMCevaluatestheunnormaliseddensityp∗(θ|D)atseveralpointsintheparameterspaceateachstepn.Toimproveperformance,HMCalsousesthegradientoflogp∗(θ|D)withrespecttoθ.3SLICSTANThissectionoutlinesthesecondkeycontributionofthiswork—thedesignandsemanticsofSlicStan.SlicStanisaprobabilisticprogramminglanguage,whichaimstoprovideamorecompositionalalternativetoStan,whileretainingStan’sefficiencyandstatementsyntaxnaturaltothestatisticscommunity.Thus,wedesignthelanguagesothat:(1)SlicStanstatementsareasupersetoftheCoreStanstatementsgivenin§2,(2)SlicStanprogramscontainnoprogramblocks,andallowtheinterleavingofstatementsthatwouldbelongtodifferentprogramblocksiftheprogramwaswritteninStan,and(3)SlicStansupportsfirst-ordernon-recursivefunctions.Thisresultsinaflexiblesyntax,thatallowsforbetterencapsulationandcodereuse,similarlytoEdward[Tranetal.2016]andPyMC3[Salvatieretal.2016].ThekeyideabehindSlicStanistouseinformationflowanalysistooptimiseandtransformtheprogramtoStancode.Secureinformationflowanalysishasalonghistory,summarisedbySabelfeldandMyers[2003],andSmith[2007].Itconcernssystemswherevariableshaveoneofseveralsecuritylevels,andtheaimistodisallowtheflowofhigh-levelinformationtoalow-levelvariable,butallowotherflowsofinformation.Forexample,considertwosecuritylevels,publicandsecret.Wewanttoforbidpublicinformationtodependonsecretinformation.Formally,thelevelsformalattice({public,secret},<)withpublic<secret.Secureinformationflowanalysisisusedtoensurethatinformationflowsonlyupwardsinthelattice.Thisisformalizedasthenoninterferenceproperty[GoguenandMeseguer1982]:confidentialdatamaynotinterferewithpublicdata.LookingbacktothedescriptionofStan’sprogramblocksin§§2.3,aswellastheStanManual,weidentifythreeinformationlevelsinStan:data,model,andgenqant.Weassignoneoftheselevelstoeachprogramblock,assummarisedbyTable1.‘Chain’,‘sample’and‘leapfrog’refertostagesoftheHamiltonianMonteCarlosamplingalgorithm.Usually,Stanrunsseveralchainstoperforminference,wheretherearemanysamplesperchain,andmanyleapfrogspersample.EventhoughourinsightaboutthethreeinformationlevelscomesfromStan,theyarenottiedtoStan’speculiarities.Variablesatleveldataaretheknownquantitiesinthestatisticalinferenceproblem,thatis,thedata.Computationsatthislevelcanbeseenasaformofpreprocessing.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:9Table1.ProgramblocksinStan.AdaptedfromBetancourt[2014].BlockExecutionLeveldata—datatransformeddataperchaindataparameters—modeltransformedparametersperleapfrogmodelmodelperleapfrogmodelgeneratedquantitiespersamplegenqantVariablesatlevelmodelareunknown—theyarethequantitieswewishtoinfer.Changingthemodelvariablesorthedependenciesbetweenthemchangesthestatisticalmodelweareworkingwith,whichcanhaveahugeeffectonthequalityofinference.Finally,generatedquantitiesarevariablesthatdataandmodelvariablesdonotdependon,andcomputingthemcanbeseenasaformofpostprocessing.AllthreearefundamentalconceptsofstatisticalinferenceandarenotspecifictoStan.TherestofthissectiondefinestheSlicStanlanguage.ThesyntaxofSlicStanstatements(§§3.1)extendsthatoftheCoreStanstatementsfrom§2,anditstypesystem(§§3.2)assumesleveltypesdata,modelandgenqantonvariables.Thetypingrulesarethenimplementedsothatinwell-typedSlicStanprograms,informationflowsfromleveldata,throughmodeltogenqant.EveryCoreStanprogramcanbeturnedintoanequivalentSlicStanprogrambyconcatenatingthestatementsanddeclarationsinitscompartments.Next,wegivethesemanticsofaSlicStanprogram,muchaswedidforCoreStan,asanunnor-malisedlogdensityfunctiononparametersanddata(§§3.4),andshowsomeexamples(§§3.5).Todoso,weelaborateSlicStan’sstatementstoCoreStanstatementsbystaticallyunrollinguser-definedfunctioncallsandbringingallvariabledeclarationstothetoplevel(§§3.3).Themainpurposeofelaborationistoidentifyallparametersstaticallysoastogivethesemanticsasafunctionontheparameters.ElaborationalsoservesasafirststepintranslatingSlicStantoStan(§4).3.1SyntaxASlicStanprogramisasequenceoffunctiondefinitionsFi,followedbytop-levelstatementS.SyntaxofaSlicStanProgramF1,...,Fn,Sn≥0SlicStanprogramSlicStan’suser-definedfunctionsarenotrecursive(acalltoFicanonlyoccurinthebodyofFjifi<j).FunctionsarespecifiedbyareturntypeT,argumentswiththeirtypesai:Ti,andabodyS.Thereisareservedvariableret_gassociatedwitheachfunction,toholdthereturnvalue.SyntaxofFunctionDefinitionsF::=Tд(T1a1,...,Tnan)Sfunctiondefinition(signatureд:T1,...,Tn→T)SlicStan’sexpressionsandstatementsextendthoseofStan,byuser-definedfunctioncallsд(E1,...,En)andvariabledeclarationsTx;S(initalic).InbothdeclarationsTx;Sandloopsfor(xinE1:E2)S,thevariablexislocallyboundwithscopeS.Weidentifystatementsuptoconsistentrenamingofboundvariables.NotethatoccurrencesofvariablesinL-valuesarefree.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:10MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonSlicStanSyntaxofExpressions:E::=expressionxvariablecconstant[E1,...,En]arrayE1[E2]arrayelementf(E1,...,En)builtinfunctioncallg(E1,...,En)user-definedfun.callL::=L-valuexvariablex[E1]...[En]arrayelementSlicStanSyntaxofStatements:S::=statementL=EassignmentS1;S2sequencefor(xinE1:E2)Sforloopif(E)S1elseS2ifstatementskipskipTx;SdeclarationWeconstrainthelanguagetoonlysupportforloops,disallowingthevalueoftheloopguardtodependonthebodyoftheloop.Asdescribedinlatersubsections,inordertogivethesemanticsofaSlicStanprogram,aswellastotranslateittoStan,weneedtoelaboratethestatementstoCoreStanstatements(§§3.3),staticallyunrollinguser-definedfunctionsandextractingvariabledeclarationstothetop-level.Extendingthelanguagetosupportwhileloops(orrecursivefunctions)meansriskofnon-terminatingelaborationstep,andapotentiallyinefficientresultingStanprogram.ThisdesignchoiceisasmallrestrictionontheusabilityandrangeofexpressiblemodelscomparedtoStan:modelsinStancanonlyhaveafixednumberofparameters.Asaresult,anoverwhelmingnumberofexamplesintheStanofficialrepositoryuseforloopsonly.Wedefinederivedformsfordatadeclarations,modellingstatements,andreturnstatements.Anyuser-definedfunctionD_lpdfcanbeusedasthelogdensityfunctionofauser-defineddistributionDonthefirstargumentofD_lpdf.Forthesakeofsimplicity,weassumethebodyofauser-definedfunctiongcontainsatmostonereturnstatement,attheend,andwetreatitasanassignmenttothereturnvariableret_g.DerivedFormsdataτx;S,(τ,data)x;SdatadeclarationE∼d(E1,...En),target=target+d_lpdf(E,E1,...En)model,builtindistributionE∼D(E1,...En),target=target+D_lpdf(E,E1,...En)model,user-defineddistributionreturnE,ret_g=Ereturn3.2TypingofSlicStanNext,wepresentSlicStan’stypesystem.Wedefinealattice({data,model,genqant},<)ofleveltypes,wheredata<model<genqant.TypesTinSlicStanrangeoverpairs(τ,ℓ)ofabasetypeτ,andaleveltypeℓ—oneofdata,model,orgenqant.Arraysaresized,withn≥0.Eachbuiltinfunctionfhasafamilyofsignaturesf:(τ1,ℓ),...,(τn,ℓ)→(τ,ℓ),oneforeachlevelℓ.Types,andTypingEnvironment:ℓ::=leveltypedatadata,transformeddatamodelparameters,transformedparametersgenqantgeneratedquantitiesnsizeτ::=real|int|bool|τ[n]basetypeT::=(τ,ℓ)type:basetypeandlevelΓ::=x1:T1,...,xn:TnxidistincttypingenvironmentProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:11(Whilebuiltinfunctionsofourformalsystemarelevelpolymorphic,user-definedfunctionsaremonomorphic.Thisdesignchoicewasmadetokeepthesystemsimple,andweseenochallengestopolymorphismthatareuniquetoSlicStan.)Weassumethetypeofthereservedtargetvariabletobe(real,model):thisvariablecanonlybeaccessedwithinthemodelblockinStan,thusitslevelismodel.Eachfunctionдisassociatedwithasinglereturnvariableret_gmatchingthereturntypeofthefunction.Reservedvariablestarget:(real,model)logjointprobabilitydensityfunctionret_g:TreturnvalueofafunctionTд(T1a1,...,Tnan)SWepresentthefullsetofdeclarativetypingrules,inspiredbythoseofthesecureinformationflowcalculusdefinedbyVolpanoetal.[1996],andmoreprecisely,itssummarybyAbadietal.[1999].Theinformationflowconstraintsareenforcedbythesubsumptionrules(ESub)and(SSub),whichtogetherallowinformationtoonlyflowupwardsthedata<model<genqantlattice.Intuitively,weneedtoassociateeachexpressionEwithaleveltypetopreventlower-levelvariablestodirectlydependonhigher-levelvariables,suchasinthecased=m+1,fordofleveldataandmoflevelmodel.WealsoneedtoassociateeachstatementSwithaleveltypetopreventlower-levelvariablestoindirectlydependonhigher-levelvariables,suchasinthecaseif(m>0)d=2.JudgmentsoftheTypeSystem:Γ⊢E:(τ,ℓ)expressionEhastype(τ,ℓ)andreadsonlylevelℓandbelowΓ⊢S:ℓstatementSassignsonlytolevelℓandabove⊢FfunctiondefinitionFiswell-typedThefunctionty(c)mapsconstantstotheirtypes(forexamplety(5.5)=real).TypingRulesforExpressions:(ESub)Γ⊢E:(τ,ℓ)ℓ≤ℓ′Γ⊢E:(τ,ℓ′)(Var)Γ,x:T⊢x:T(Const)ty(c)=τΓ⊢c:(τ,data)(Arr)Γ⊢Ei:(τ,ℓ)∀i∈1..nΓ⊢[E1,...,En]:(τ[n],ℓ)(ArrEl)Γ⊢E1:(τ[n],ℓ)Γ⊢E2:(int,ℓ)Γ⊢E1[E2]:(τ,ℓ)(PrimCall)(f:T1,...,Tn→T)Γ⊢Ei:Ti∀i∈1..nΓ⊢f(E1,...,En):T(FCall)(д:T1,...,Tn→T)Γ⊢Ei:Ti∀i∈1..nΓ⊢д(E1,...,En):THereandthroughout,wemakeuseofseveralfunctionsonthelanguagebuildingblocks:•W(S)(DefinitionA.1)isthesetofvariablesthatareassignedtoinS:W(x=2∗y)={x}.•R(S)(DefinitionA.2)isthesetofvariablesreadbyS:R(x=2∗y)={y}.•Γ(L)(DefinitionA.3)isthetypeoftheL-valueLinthecontextΓ:Γ(x[0])=(real,data)forx:(real[],data)∈Γ.Therule(Decl)foravariabledeclaration(τ,ℓ)x;Shasaside-condition(x<dom(Γ)),whereΓisthelocaltypingenvironment,thatenforcesthatthevariablexisgloballyunique,thatis,thereisnootherdeclarationofxintheprogram.Theconditionx<W(S)in(For)enforcesthattheloopindexxisimmutableinsidethebodyoftheloop.In(Seq),wemakesurethatthesequenceS1;S2isshreddable,throughthepredicateS(S1,S2)(Definition4.7).ThisimposesarestrictionontherangeProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:12MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonofwell-typedprograms,whichisneededbothtoallowtranslationtoStan(see§§4.1),andtoallowinterpretingoftheprogramintermsofpreprocessing,inferenceandpostprocessing.Usingtherulesforexpressionsandstatements,wecanalsoobtainrulesforthederivedstatements.TypingRulesforStatements:(SSub)Γ⊢S:ℓ′ℓ≤ℓ′Γ⊢S:ℓ(Assign)Γ(L)=(τ,ℓ)Γ⊢E:(τ,ℓ)Γ⊢(L=E):ℓ(Decl)Γ,x:(τ,ℓ)⊢S:ℓ′x<dom(Γ)Γ⊢(τ,ℓ)x;S:ℓ′(If)Γ⊢E:(bool,ℓ)Γ⊢S1:ℓΓ⊢S2:ℓΓ⊢if(E)S1elseS2:ℓ(Seq)Γ⊢S1:ℓΓ⊢S2:ℓS(S1,S2)Γ⊢(S1;S2):ℓ(Skip)Γ⊢skip:ℓ(For)Γ⊢E1:(int,ℓ)Γ⊢E2:(int,ℓ)Γ,x:(int,ℓ)⊢S:ℓx<dom(Γ)x<W(S)Γ⊢for(xinE1:E2)S:ℓDerivedTypingRules(DataDecl)Γ,x:(τ,data)⊢S:ℓx<dom(Γ)Γ⊢dataτx;S:ℓ(PrimModel)(d_lpdf:T,T1,...,Tn→(real,model))Γ⊢E:TΓ⊢Ei:Ti∀i∈1..nΓ⊢E∼d(E1,...En):model(Return)Γ⊢ret_g:(τ,ℓ)Γ⊢E:(τ,ℓ)Γ⊢returnE:ℓ(FModel)(D:T,T1,...,Tn→(real,model))Γ⊢E:TΓ⊢Ei:Ti∀i∈1..nΓ⊢E∼D_dist(E1,...En):modelFinally,wecompletethethreejudgmentsofthetypesystemwiththerule(FDef)forcheckingthewell-formednessofafunctiondefinition.Theconditionℓi≤ℓensuresthattheleveloftheresultofafunctioncallisnosmallerthanthelevelofitsarguments.TypingRuleforFunctionDefinitions:(FDef)a1:T1,...,an:Tn,ret_g:(τ,ℓ)⊢S:ℓTi=(τi,ℓi)ℓi≤ℓ∀i∈1..n⊢(τ,ℓ)д(T1a1,...,Tnan)SInourformaldevelopment,weimplicitlyassumeafixedprogramwithwell-typedfunctions⊢F1,...,⊢Fn.Moreprecisely,weassumeagivenwell-formedprogramdefinedasfollows.Well-FormedSlicStanProgram:AprogramF1,...FN,Siswell-formediff⊢F1,...,⊢Fn,and∅⊢S:data.SlicStanstatementsare,bydesign,asupersetofCoreStanstatements.Thus,wecantreatanyCoreStanstatementasaSlicStanstatementwithbig-stepoperationalsemanticsdefinedasin§2.Byextendingtheconformancerelations|=ΓtocorrespondtoaSlicStantypingenvironment,wecanprovetypepreservationoftheoperationalsemantics,withrespecttoSlicStan’stypesystem.RuleoftheConformanceRelation:(State)Vi|=τi∀i∈I(xi7→Vi)i∈I|=(xi:(τi,ℓi))i∈IProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:13Theorem3.1(TypePreservationfor⇓).ForaCoreStanstatementSandaCoreStanexpressionE:(1)Ifs|=ΓandΓ⊢E:(τ,ℓ)and(s,E)⇓VthenV|=τ.(2)Ifs|=ΓandΓ⊢S:ℓand(s,S)⇓s′thens′|=Γ.Proof.Byinductionsonthesizeofthederivationsofthejudgments(s,E)⇓Vand(s,S)⇓s′.(cid:3)Finally,westateatermination-insensitivenoninterferenceresult.Intuitively,theresultmeansthat(observed)datacannotdependonthemodelparameters,andthatgeneratedquantitiesdonotaffectthelogdensitydistributiondefinedbythemodel.Definition3.2(ℓ-equalstates).GivenatypingenvironmentΓ,statess1|=Γands2|=Γareℓ-equalforsomelevelℓ(writtens1≈ℓs2),iftheydifferonlyforvariablesofalevelstrictlyhigherthanℓ:s1≈ℓs2,∀x:(τ,ℓ′)∈Γ.(ℓ′≤ℓ=⇒s1(x)=s2(x))Theorem3.3(Noninterference).Supposes1|=Γ,s2|=Γ,ands1≈ℓs2forsomeℓ.ThenforCoreStanstatementsSandCoreStanexpressionsE:(1)IfΓ⊢E:(τ,ℓ)and(s1,E)⇓V1and(s2,E)⇓V2thenV1=V2.(2)IfΓ⊢S:ℓand(s1,S)⇓s′1and(s2,S)⇓s′2thens′1≈ℓs′2.Proof.(1)followsbyruleinductiononthederivationΓ⊢E:(τ,ℓ),andusingthatifΓ⊢E:(τ,ℓ),x∈R(E)andΓ(x)=(τ′,ℓ′),thenℓ′≤ℓ.(2)followsbyruleinductiononthederivationΓ⊢S:ℓandusing(1).(cid:3)3.3ElaborationofSlicStanSimilarlytoStan,aSlicStanprogramdefinesaprobabilisticmodel,throughanunnormalisedlogdensityfunctiononthemodelparametersanddata.Thatis,thesemanticsofSlicStanisintermsofafixed(ordata-dependent)numberofvariables.Therefore,inordertobeabletoformallygivethesemantics,weneedtostaticallyunrollcallstouser-definedfunctions,andpullallvariabledeclarationstothetoplevel.(WediscussthedifficultiesofdirectlyspecifyingthesemanticsofSlicStanwithoutelaborationin§§3.6.)Wecallthisstaticunrollingstepelaboration,andweformaliseitthroughtheelaborationrelation⇓Γ.Intuitively,toelaborateaprogramF1,...FN,S,weelaborateitsmainbodySbyunrollinganycallstoF1,...,FN(asspecifiedby(ElabFCall)),andmoveallvariabledeclarationstothetoplevel(asspecifiedby(ElabDecl)).TheresultisanelaboratedSlicStanstatementS′andalistofvariabledeclarationsΓ.Asmentionedpreviously,toavoidnotationalclutter,weassumeatop-levelSlicStanprogramF1,...,FN,S.SincethesyntaxofaSlicStanstatementdiffersfromthatofaCoreStanstatementonlybythepresenceofuser-definedfunctioncallsandvariabledeclarations,anelaboratedSlicStanstatementisalsoawell-formedCoreStanstatement.ElaborationRelationP⇓∅⟨Γ,S′⟩programelaborationS⇓Γ⟨Γ,S′⟩statementelaborationE⇓Γ⟨Γ,S.E′⟩expressionelaborationF⇓Γ⟨r:T,A,Γ,S⟩fun.definitionelaborationElaborationRuleforaSlicStanProgram(ElabSlicStan)S⇓∅⟨Γ,S′⟩F1,...FN,S⇓∅⟨Γ,S′⟩Theunrollingrule(ElabFCall)assumesacalltoauser-definedfunctionдwithdefinitionF=Tд(T1a1,...,Tnan)S,whichelaboratesasdescribedby(ElabFDef).Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:14MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonElaborationRulesforExpressions:(ElabVar)x⇓Γ⟨∅,skip.x⟩(ElabConst)c⇓Γ⟨∅,skip.c⟩(ElabArrEl)E1⇓Γ⟨Γ1,S1.E′1⟩E2⇓Γ⟨Γ2,S2.E′2⟩Γ1∩Γ2=∅E1[E2]⇓Γ⟨Γ1∪Γ2,S1;S2.(E′1[E′2])⟩(ElabArr)Ei⇓Γ⟨Γi,Si.E′i⟩∀i∈1..n(cid:209)ni=1Γi=∅[E1,...,En]⇓Γ⟨—ni=1Γi,S1;...;Sn.([E′1,...,E′n])⟩(ElabPrimCall)Ei⇓Γ⟨Γi,Si.E′i⟩∀i∈1..n(cid:209)ni=1Γi=∅f(E1,...,En)⇓Γ⟨—ni=1Γi,S1;...;Sn.f(E′1,...,E′n)⟩(ElabFCall)(whereFisthedefinitionforfunctionд)Ei⇓Γ⟨ΓEi,SEi.E′i⟩∀i∈1..nF⇓Γ⟨rF:TF,AF,ΓF,SF⟩AF={ai:Ti|i∈1..n}{rF:TF}∩AF∩((cid:209)ni=1Γi)∩ΓF=∅д(E1,...,En)⇓Γ⟨{rF:TF}∪AF∪(—ni=1Γi)∪ΓF,(SE1;a1=E′1;...;SEn;an=E′n;SF.rF)⟩ElaborationRuleforFunctionDefinitions:(ElabFDef)S⇓{r:T}∪ΓA∪Γ⟨Γ′,S′⟩ΓA={a1:T1,...,an:Tn}{r}∩dom(ΓA)∩dom(Γ′)=∅Tд(T1a1,...,Tnan)S⇓Γ⟨r:T,ΓA,Γ′,S′⟩Asweidentifystatementsuptoα-conversion,Tx;x=1elaboratesto⟨{x1:T},x1=1⟩,butalsoto⟨{x2:T},x2=1⟩,andsoon.The(ElabDecl)rulesimplyextractsavariabledeclarationtothetoplevel.Otherthanrecursivelyapplying⇓Γtosub-partsofthestatement,the(ElabIf)and(ElabFor)rulestransformtheguardsoftherespectivecompoundstatementtobethefreshvariablesдorд1,д2respectively(asopposedtounrestrictedexpressions).ThisisanecessarypreparationstepneededfortheprogramtobecorrectlytranslatedtoStanlater(see§§4.1andAppendixC).ElaborationRulesforStatements:(ElabDecl)S⇓{x:T}∪Γ⟨Γ′,S′⟩x<dom(Γ′)Tx;S⇓Γ⟨{x:T}∪Γ′,S′⟩(ElabSkip)skip⇓Γ⟨∅,skip⟩(ElabAssign)L⇓Γ⟨ΓL,SL.L′⟩E⇓Γ⟨ΓE,SE.E′⟩ΓL∩ΓE=∅L=E⇓Γ⟨ΓL∪ΓE,SL;SE;L′=E′⟩(ElabSeq)S1⇓Γ⟨Γ1,S′1⟩S2⇓Γ⟨Γ2,S′2⟩Γ1∩Γ2=∅S1;S2⇓Γ⟨Γ1∪Γ2,S′1;S′2⟩(ElabIf)(whereΓ⊢E:T)E⇓Γ⟨ΓE,SE.E′⟩S1⇓Γ⟨Γ1,S′1⟩S2⇓Γ⟨Γ2,S′2⟩{д:T}∩ΓE∩Γ1∩Γ2=∅if(E)S1elseS2⇓Γ⟨{д:T}∪ΓE∪Γ1∪Γ2,(SE;д=E′;if(д)S′1elseS′2)⟩(ElabFor)(whereΓ⊢E1:T1andΓ⊢E2:T2)E1⇓Γ⟨Γ1,S1.E′1⟩E2⇓Γ⟨Γ2,S2.E′2⟩S⇓Γ∪{x:(int,data)}⟨ΓS,S′⟩ΓV=vΓ(ΓS,n){д1:T1,д2:T2,n:(int,data)}∩Γ1∩Γ2∩ΓV=∅for(xinE1:E2)S⇓Γ⟨{д1:T1,д2:T2,n:(int,data)}∪Γ1∪Γ2∪ΓV,S1;S2;д1=E′1;д2=E′2;n=д2−д1+1;for(xinд1:д2)vS(x,ΓV,S′)⟩Insomecaseswhenelaboratingaforloop,ΓSwillnotbeempty(inotherwords,thebodyoftheloopwilldeclarenewvariables).Thus,as(ElabFor)shows,variablesinΓSareupgradedtoanProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:15array,andthenaccessedbytheindexoftheloop.WeusethefunctionvS(DefinitionA.4)whichtakesavariablex,atypingenvironmentΓ,andastatementS,andreturnsastatementS′,whereanymentionofavariablex′∈dom(Γ)issubstitutedwithx′[x].Forexample,considerthestatementfor(iin1:N){realmodelx∼normal(0,1);y[i]∼normal(x,1);}andanenvironmentΓ,suchthatΓ⊢N:(int,data).Thebodyoftheloopdeclaresanewvariablex,thusitelaboratesto⟨ΓS,S′⟩,whereΓS={x:(real,model)},andS′={x∼normal(0,1);y[i]∼normal(x,1);}.By(ElabFor),S⇓Γ⟨{g1:(int,data),g2:(int,data)}∪ΓV,for(iing1:g2){S′′}⟩where:ΓV=vΓ(ΓS,N)={x:(real[N],model)}S′′=vS(i,ΓV,S′)=x[i]∼normal(0,1);y[i]∼normal(x[i],1);Next,westateandprovetypepreservationoftheelaborationrelation.Theorem3.4(Typepreservationof⇓Γ).ForSlicStanstatementsS,SlicStanexpressionsE,andSlicStanfunctiondefinitionsF:(1)IfΓ⊢E:(τ,ℓ)andE⇓Γ⟨Γ′,S′.E′⟩thenΓ,Γ′⊢S′:dataandΓ,Γ′⊢E′:(τ,ℓ).(2)IfΓ⊢S:ℓandS⇓Γ⟨Γ′,S′⟩thenΓ,Γ′⊢S′:ℓ(3)IfF⇓Γ⟨Γ′,S′.ret⟩thenΓ,Γ′⊢S′:dataProof.ByinductionsonthesizeofthederivationsofthejudgmentsE⇓Γ⟨Γ′,S′.E′⟩,S⇓Γ⟨Γ′,S′⟩,andF⇓Γ⟨Γ′,S′.ret⟩.(cid:3)3.4SemanticsofSlicStanWenowshowhowSlicStan’stypesystemallowsustospecifythesemanticsoftheprobabilisticprogramasanunnormalisedposteriordensityfunction.ThisshowshowthesemanticsofSlicStanconnectstothatofStan,anddemonstratesthatexplicitlyencodingtherolesofprogramvariablesintotheblocksyntaxofthelanguageisnotneeded.Wespecifythesemantics—theunnormaliseddensitylogp∗F1,...,Fn,S(θ|D)—intwosteps.3.4.1Semanticsof(Elaborated)SlicStanStatements.ConsideranelaboratedSlicStanstatementSsuchthatΓ⊢S:data.ThesemanticsofSisthefunctionlogp∗Γ⊢S,suchthatforanystates|=Γ:logp∗Γ⊢S(s),s′[target]ifthereiss′suchthat((s,target7→0),S)⇓s′3.4.2SemanticsofSlicStanPrograms.Considerawell-formedSlicStanprogramF1,...,Fn,SandsupposethatS⇓∅⟨Γ′,S′⟩.(ObservethatΓ′andS′areuniquelydeterminedbyF1,...,Fn,S.)Supposealsothat:•ΓDcorrespondstodatavariables,ΓD={x:ℓ∈Γ′|ℓ=data∧x<W(S′)},and•Γθcorrespondstomodelparameters,Γθ={x:ℓ∈Γ′|ℓ=model∧x<W(S′)}.SimilarlytoStan(§§2.4),thesemanticsofaSlicStanprogramSistheunnormalisedlogposteriordensityfunctionlogp∗F1,...,Fn,SonparametersθgivendataD(withθ|=ΓθandD|=ΓD):logp∗F1,...,Fn,S(θ|D),logp∗Γ′⊢S′(θ,D)(1)3.5ExamplesNext,wegivetwoexamplesofSlicStanprograms,theirelaboratedversions,andtheirsemanticsintheformofanunnormalisedlogdensityfunction.Here,wespecifythelevelsofvariablesinSlicStanprogramsexplicitly.In§5wedescribehowtypeinferencecanbeimplementedtoinferoptimallevelsforprogramvariables,thusmakingexplicitdeclarationoflevelsunnecessary.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:16MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton3.5.1SimpleExample.ConsideraSlicStanprogram∅,S(∅denotesnofunctiondefinitions),wherewesimplymodelthedistributionofadataarrayy:S=realmodelmu∼normal(0,1);realmodelsigma∼normal(0,1);intdataN;realdatay[N];for(iin1:N){y[i]∼normal(mu,sigma);}WedefinethesemanticsofSinthreesteps:(1)Elaboration:S⇓∅⟨Γ′,S′⟩,where:Γ′=mu:(real,model),sigma:(real,model),y:(real[N],model),N:(int,data)S′=mu∼normal(0,1);sigma∼normal(0,1);for(iin1:N){y[i]∼normal(mu,sigma);}(2)SemanticsofS′:Foranystates|=Γ′,logp∗Γ′⊢S′(s)=s′[target],where(s,S′)⇓s′.Thus:logp∗Γ′⊢S′(s)=logN(µ,0,1)+logN(σ,0,1)+˝Ni=1logN(yi,µ,σ)(3)SemanticsofS:WederiveΓD={x:ℓ∈Γ′|ℓ=data∧x<W(S′)}={N,y},andΓθ={x:ℓ∈Γ′|ℓ=model∧x<W(S′)}={µ,σ}.Therefore,thesemanticsofSistheunnormaliseddensityontheparametersµandσ,givendataNandy:logp∗S(µ,σ|y,N)=logN(µ,0,1)+logN(σ,0,1)+˝Ni=1logN(yi,µ,σ)3.5.2User-definedFunctionsExample.Next,welookatanexamplethatincludesauser-definedfunction.Here,thefunctionmy_normalisareparameterisingfunction(§§5.4),thatdefinesaGaussianrandomvariable,byscalingandshiftingastandardGaussianvariable:S=realmodelmy_normal(realmodelm,realmodels){realmodelx_std∼normal(0,1);returnm+x_std*s;}realmodelmu∼normal(0,1);realmodelsigma∼normal(0,1);intdataN;realgenqantx[N];for(iin1:N){x[i]=my_normal(mu,sigma);}(1)Elaboration:S⇓∅⟨Γ′,S′⟩,where:Γ′=mu:(real,model),sigma:(real,model),m:(real,model),s:(real,model),x_std:(real[N],model),x:(real[N],genqant),N:(int,data)S′=mu∼normal(0,1);sigma∼normal(0,1);for(iin1:N){m=mu;s=sigma;x_std[i]∼normal(0,1);x[i]=m+x_std[i]*s;}(2)SemanticsofS′:Consideranys|=Γ′.Then:logp∗Γ′⊢S′(s)=logN(µ,0,1)+logN(σ,0,1)+˝Ni=1logN(xstdi,0,1)(3)SemanticsofS:WederiveΓD={N},andΓθ={µ,σ,xstd}.ThesemanticsoftheprogramSistheunnormaliseddensityontheparametersµ,σ,andxstd,givendataN:logp∗S(µ,σ,xstd|N)=logN(µ,0,1)+logN(σ,0,1)+˝Ni=1logN(xstdi,0,1)Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:173.6DifficultyofSpecifyingDirectSemanticsWithoutElaborationSpecifyingthedirectsemanticslogp∗∅⊢S(s),withoutanelaborationstep,isnotsimple.SlicStan’suser-definedfunctionsareflexibleenoughtoallownewmodelparameterstobedeclaredinsideofthebodyofafunction.Havingsomeoftheparametersdeclaredthiswaymeansthatitisnotobviouswhatthecompletesetofparametersis,unlessweelaboratetheprogram.Considertheprogramfrom§§§3.5.2.Itssemanticsislogp∗S(µ,σ,xstd|N)=logN(µ,0,1)+logN(σ,0,1)+˝Ni=1logN(xstdi,0,1).Thisdiffersfromlogp∗S(µ,σ,xstd|N))=logN(µ,0,1)+logN(σ,0,1)+N×logN(xstd,0,1),whichwouldbetheaccumulatedlogdensityincasewedonotunrollthemy_normalcall,andinsteadimplementdirectsemantics.Inonecase,themodelhasN+2parameters:µ,σ,xstd1,...,xstdN.Intheother,themodelhasonly3parameters:µ,σ,xstd.4TRANSLATIONOFSLICSTANTOSTANTranslatingSlicStantoStanhappensintwosteps:shredding(§§4.1)andtransformation(§§4.2).Inthissection,weformalisethesestepsandshowthatthesemantics,seenasanunnormalisedlogposteriordensityfunctiononparametersgivendata,ispreservedinthetranslation.4.1ShreddingThefirststepintranslatinganelaboratedSlicStanprogramtoStanistheideaofshredding(orslicing)bylevel.SlicStanallowsstatementsthatassigntovariablesofdifferentlevelstobeinterleaved.Stan,ontheotherhand,requiresalldatalevelstatementstocomefirst(inthedataandtransformeddatablocks),thenallmodellevelstatements(intheparameters,transformedparametersandmodelblocks),andfinally,thegenqantlevelstatements(inthegeneratedquantitiesblock).Therefore,wedefinetheshreddingrelation⇕ΓonanelaboratedSlicStanstatementSandtriplesofsingle-levelstatements(SD,SM,SQ)(Definition4.1).Thatis,⇕ΓshredsastatementintothreeelaboratedSlicStanstatementsSD,SMandSQ,whereSDonlyassignstovariablesofleveldata,SMonlyassignstovariablesoflevelmodel,andSQonlyassignstovariablesoflevelgenqant.WeformallystateandprovethisresultinLemma4.2.ShreddingRelationS⇕Γ(SD,SM,SQ)statementshreddingCurrently,Stancanonlyassigntodatavariablesinsidethetransformeddatablock,tomodelvariablesinsidethetransformedparametersblock,andtogeneratedquantitiesinsidethegeneratedquantitiesblock.Therefore,inStanitisnotpossibletowriteanifstatementoraforloopwhichassignstovariablesofdifferentlevelsinsideitsbody.The(ShredIf)and(ShredFor)rulesresolvethisbycopyingtheentirebodyoftheifstatementorforlooponeachofthethreelevels.Noticethatwerestricttheifandforguardstobevariables(asopposedtoanyexpression),whichwehaveensuredisthecaseaftertheelaborationstep((ElabIf)and(ElabFor)).Forexample,considertheSlicStanprogramS,asdefinedbelow.ItelaboratestoS′andΓ′,anditisthenshreddedtothesingle-levelstatements(SD,SM,SQ):S=realdatad;realmodelm;if(d>0){d=1;m=2;}Γ′={d:(real,data),m:(real,model),g:(bool,data)}S′=g=(d>0);if(g){d=1;m=2;}SD=g=(d>0);if(g){d=1;}SM=if(g){m=2;}SQ=skip;Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:18MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonShreddingRulesforStatements:(ShredDataAssign)Γ(L)=(_,data)L=E⇕Γ(L=E,skip,skip)(ShredModelAssign)Γ(L)=(_,model)L=E⇕Γskip,L=E,skip(ShredGenQuantAssign)Γ(L)=(_,genqant)L=E⇕Γskip,skip,L=E(ShredSeq)S1⇕ΓSD1,SM1,SQ1S2⇕ΓSD2,SM2,SQ2S1;S2⇕Γ(SD1;SD2),(SM1;SM2),(SQ1;SQ2)(ShredSkip)skip⇕Γ(skip,skip,skip)(ShredIf)S1⇕Γ(SD1,SM1,SQ1)S2⇕Γ(SD2,SM2,SQ2)if(д)S1elseS2⇕Γ(if(д)SD1elseSD2),(if(д)SM1elseSM2),(if(д)SQ1elseSQ2)(ShredFor)S⇕Γ(SD,SM,SQ)for(xinд1:д2)S⇕Γ(for(xinд1:д2)SD),(for(xinд1:д2)SM),(for(xinд1:д2)SQ)Intherestofthissection,weshowthatshreddingaSlicStanprogrampreservesitssemantics(Theorem4.9),inthesensethatanelaboratedprogramShasthesamemeaningasthesequenceofitsshreddedpartsSD;SM;SQ.Wedosoby:(1)Provingthatshreddingproducessingle-levelstatements(Definition4.1andLemma4.2).(2)Defininganotionofstatementequivalence(Definition4.3)andspecifyingwhatconditionsneedtoholdtochangetheorderoftwostatements(Lemma4.4).(3)ShowinghowtoextendthetypesystemofSlicStaninorderforthelanguagetofulfilthecriteriafrom(2)(Definition4.7,Lemma4.8).Intuitively,asingle-levelstatementoflevelℓisonethatupdatesonlyvariablesoflevelℓ.Definition4.1(Single-levelStatementΓ⊢ℓ(S)).Sisasingle-levelstatementoflevelℓwithrespecttoΓ(writtenΓ⊢ℓ(S))ifandonlyif,Γ⊢S:ℓand∀x∈W(S)thereissomeτ,s.t.x:(τ,ℓ)∈Γ.Lemma4.2(Shreddingproducessingle-levelstatements).S⇕Γ(SD,SM,SQ)=⇒Γ⊢data(SD)∧Γ⊢model(SM)∧Γ⊢genquant(SQ)ThecoreofprovingTheorem4.9isthatifwetakeastatementSthatiswell-typedinΓ,andreorderitsbuildingblocksaccordingto⇕Γ,theresultingstatementS′willbeequivalenttoS.Definition4.3(Statementequivalence).S≃S′,(∀s,s′.(s,S)⇓s′⇐⇒(s,S′)⇓s′)Inthegeneralcase,toswaptheorderofexecutingS1andS2,itisenoughforeachstatementnottoassigntoavariablethattheotherstatementreadsorassignsto:Lemma4.4(StatementReordering).ForstatementsS1andS2thatarewell-typedinΓ,ifR(S1)∩W(S2)=∅,W(S1)∩R(S2)=∅,andW(S1)∩W(S2)=∅thenS1;S2≃S2;S1.Shreddingproducessingle-levelstatements,thereforeweonlyencounterreorderingsingle-levelstatementsofdistinctlevels.Thus,twooftheconditionsneededforreorderingalreadyhold.Lemma4.5.IfΓ⊢ℓ1(S1),Γ⊢ℓ2(S2)andℓ1<ℓ2thenR(S1)∩W(S2)=∅andW(S1)∩W(S2)=∅.ToreorderthesequenceS2;S1accordingtoLemma4.4,weneedtosatisfyonemorecondition,whichisR(S2)∩W(S1)=∅.WeachievethisthroughthepredicateSinthe(Seq)typingrule.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:19OnewaytodefineS(S2,S1)issothatitdirectlyreflectsthiscondition:S(S2,S1)=R(S2)∩W(S1).Thiscorrespondstoaformofasingle-assignmentsystem,wherevariablesbecomeimmutableoncetheyareread.Weadoptamoreflexiblestrategy,whereweenforcevariablesoflevelℓtobecomeimmutableonlyoncetheyhavebeenreadatalevelhigherthanℓ.Wedefine:•RΓ⊢ℓ(S):thesetofvariablesxthatarereadatlevelℓinS.Forexample,ifyisoflevelℓ,thenx∈RΓ⊢ℓ(y=x).(DefinitionA.5).•WΓ⊢ℓ(S):thesetofvariablesxoflevelℓthathavebeenassignedtoinS(DefinitionA.6).Importantly,ifΓ⊢ℓ(S),thenthesetsRΓ⊢ℓ(S)andWΓ⊢ℓ(S)arethesameasR(S)andW(S):Lemma4.6.IfΓ⊢ℓ(S),thenRΓ⊢ℓ(S)=R(S)andWΓ⊢ℓ(S)=W(S).Finally,wegivetheformaldefinitionofS:Definition4.7(Shreddablesequence).S(S1,S2),∀ℓ1,ℓ2.(ℓ2<ℓ1)=⇒RΓ⊢ℓ1(S1)∩WΓ⊢ℓ2(S2)=∅Lemma4.8(Commutativityofseqencingsingle-levelstatements).IfΓ⊢ℓ1(S1),Γ⊢ℓ2(S2),Γ⊢S2;S1:dataandℓ1<ℓ2thenS2;S1;≃S1;S2;Theorem4.9(SemanticPreservationof⇕Γ).IfΓ⊢S:dataandS⇕Γ(SD,SM,SQ)thenlogp∗Γ⊢S(s)=logp∗Γ⊢(SD;SM;SQ)(s),foralls|=Γ.Proof.NotethatifS≃S′thenlogp∗Γ⊢S(s)=logp∗Γ⊢S′(s)forallstatess|=Γ.Semanticpreser-vationthenfollowsfromprovingthestrongerresultΓ⊢S:data∧S⇕Γ(SD,SM,SQ)=⇒S≃(SD;SM;SQ)bystructuralinductiononthestructureofS.Wegivethefullproof,togetherwithproofsforLemma4.2,4.4,4.5and4.6,inA.2.(cid:3)4.2TransformationThelaststepoftranslatingSlicStantoStanistransformation.WeformalisehowashreddedSlicStanprogram⟨Γ,(SD,SM,SQ)⟩transformstoaStanprogramP,throughthetransformationrelations:TransformationRelationsΓ⇓(t)SPvariabledeclarationstransformationS⇓(d)PdatastatementtransformationS⇓(m)PmodelstatementtransformationS⇓(q)Pgenqantstatementtransformation⟨Γ,S⟩⇓(t)Ptop-leveltransformationIntuitively,ashreddedprogram⟨Γ,(SD,SM,SQ)⟩transformstoStaninfoursteps:(1)ThedeclarationsΓaresplitintoblocks,dependingonthelevelofvariablesandwhetherornottheyhavebeenassignedtoinsideofSD,SMorSQ.(2)Thedata-levelledstatementSDbecomesthebodyofthetransformeddatablock.(3)Themodel-levelledstatementSMissplitintothetransformedparametersandmodelblock,dependingonwhetherornotsubstatementsassigntothetargetvariableornot.(4)Thegenqant-levelledstatementSQbecomesthebodyofthegeneratedquantitiesblock.Thisisformalisedbythe(TransProg)rulebelow.TheStanprogramP1;P2istheStanprogramsP1andP2mergedbycomposingtogetherthestatementsineachprogramblock(DefinitionA.7).Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:20MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonTop-levelTransformationRule(TransProg)S⇕Γ(SD,SM,SQ)Γ⇓(t)(SD;SM;SQ)PSD⇓(d)PDSM⇓(m)PMSQ⇓(q)PQ⟨Γ,S⟩⇓(t)P;PD;PM;PQTransformationRulesforDeclarations:(TransData)Γ⇓(t)SPx<W(S)Γ,x:(τ,data)⇓(t)Sdata{x:τ};P(TransTrData)Γ⇓(t)SPx∈W(S)Γ,x:(τ,data)⇓(t)Stransformeddata{x:τ};P(TransParam)Γ⇓(t)SPx<W(S)Γ,x:(τ,model)⇓(t)Sparameters{x:τ};P(TransTrParam)Γ⇓(t)SPx∈W(S)Γ,x:(τ,model)⇓(t)Stransformedparameters{x:τ};P(TransGenQuant)Γ⇓(t)SPΓ,x:(τ,genqant)⇓(t)Sgeneratedquantities{x:τ};P(TransEmpty)∅⇓(t)∅Transf.RuleforDataStatements:(TransData)SD⇓(d)transformeddata{SD}Transf.RuleforGenQuantStatements:(TransGenQuant)SQ⇓(d)generatedquantities{SQ}Therules(TransParamIf),(TransModelIf),(TransParamFor),and(TransModelFor)mightproduceaStanprogramthatdoesnotcompileinthecurrentversionofStan.ThisisbecauseStanrestrictsthetransformedparametersblocktoonlyassigntotransformedparameters,andthemodelblocktoonlyassigntothetargetvariable.However,aforloop,forexample,canassigntobothkindsofvariablesinitsbody:for(iin1:N){sigma[i]=pow(tau[i],-0.5);y[i]∼normal(0,sigma[i]);}Tothebestofourknowledge,thislimitationisanimplementationalparticularityofthecurrentversionoftheStancompiler,anddoesnothaveaneffectonthesemanticsofthelanguage.7Therefore,weassumeCoreStantobeaslightlymoreexpressiveversionofStan,thatallowstransformedparameterstobeassignedinthemodelblock.TransformationRulesforModelStatements:(TransParamAssign)L,targetL=E⇓(m)transformedparameters{L=E}(TransModel)target=E⇓(m)model{target=E}(TransParamSeq)S1⇓(m)P1S2⇓(t)P2S1;S2⇓(m)P1;P27Moreover,thereisanongoingdiscussionamongstStandeveloperstomergetheparameters,transformedparametersandmodelblocksinfutureversionsofStanhttp://andrewgelman.com/2018/02/01/stan-feature-declare-distribute/.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:21(TransParamIf)target<W(S1)∪W(S2)if(E)S1elseS2⇓(m)transformedparameters{if(E)S1elseS2}(TransModelIf)target∈W(S1)∪W(S2)if(E)S1elseS2⇓(m)model{if(E)S1elseS2}(TransParamFor)target<W(S)for(xinE1:E2)S⇓(m)transformedparameters{for(xinE1:E2)S}(TransParamSkip)skip⇓(m)∅(TransModelFor)target∈W(S)for(xinE1:E2)S⇓(m)model{for(xinE1:E2)S}Theorem4.10(SemanticPreservationof⇓(t)).Considerawell-formedSlicStanprogramF1,...,Fn,S,suchthatS⇓∅⟨Γ′,S′⟩.ConsideralsoaCoreStanprogramP,suchthat⟨Γ′,S′⟩⇓(t)P.Thenforanyθ|={(x:(τ,data))∈Γ′|x<W(S′)}andD|={(x:(τ,model))∈Γ′|x<W(S′)}:logp∗F1,...,Fn,S(θ|D)=logp∗P(θ|D)Proof.Byruleinductiononthederivationof⟨Γ′,S′⟩⇓(t)P,andequation1from§§§3.4.2.(cid:3)5EXAMPLESANDDISCUSSIONInthissection,wedemonstrateanddiscussthefunctionalityofSlicStan.WecompareseveralStancodeexamples,fromStan’sReferenceManual[StanDevelopmentTeam2017]andStan’sGitHubrepositories[StanDevelopmentTeam2018b],withtheirequivalentwritteninSlicStan,andanalysethedifferences.AllexamplespresentedinthissectionhavebeentestedusingapreliminaryimplementationofSlicStan,developedbyGorinovaetal.[2018a,b],althoughinthispaperweuseforloopswheretheworkmakesuseofavectorisednotation.Firstly,weassumeatypeinferencestrategyforleveltypes,whichallowsustoremovetheexplicitspecificationoflevelsfromthelanguage(§§5.1).Next,weshowthatSlicStanallowstheusertobetterfollowtheprincipleoflocality—relatedconceptscanbekeptclosertogether(§§5.2).Secondly,wedemonstratetheadvantagesofthemorecompositionalsyntax,whencoderefactoringisneeded(§§5.3).Thelastcomparisonpointshowstheusageofmoreflexibleuser-definedfunctions,andpointsoutafewlimitationsofSlicStan(§§5.4).MoreexamplesandafurtherdiscussionontheusabilityofthelanguagesispresentedinAppendixE.5.1TypeInferenceGoingbackto§§2.3,andTable1,weidentifythatdifferentStanblocksareexecutedadifferentnumberoftimes,whichgivesusanotherorderingontheleveltypes:aperformanceordering.Codeassociatedwithvariablesofleveldataisexecutedonlyonce,asapreprocessingstepbeforeinference.Codeassociatedwithvariablesoflevelgenqantisexecutedoncepersample,rightafterinferencehascompleted,asthesequantitiescanbegeneratedfromthealreadyobtainedsamplesofthemodelparameters(inotherwords,thisisapostprocessingstep).Finally,codeassociatedwithmodelvariablesisneededateachstepoftheinferencealgorithmitself.InthecaseofHMC,thismeanssuchcodeisexecutedonceperleapfrogstep(manytimespersample).Thus,thereisaperformanceorderingofleveltypes:data≤genqant≤model:itischeaperforavariabletobedatathantobegenqant,andischeaperforittobegenqantthantobemodel.Wecanimplementtypeinferencefollowingtherulesfrom§§3.2,toinfertheleveltypeofeachvariableinaSlicStanprogram,sothat:Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:22MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton•thehardconstraintontheinformationflowdirectiondata<model<genqantisenforced•thechoiceoflevelsisoptimisedwithrespecttotheorderingdata≤genqant≤model.WehaveimplementedtypeinferenceforapreliminaryversionofSlicStan.Intherestofthissection,weassumethatnoleveltypeannotationsarenecessaryinSlicStan,exceptforwhatthedataoftheprobabilisticmodelis(specifiedusingthederivedformdataτx;S),andthattheoptimalleveltypeofeachvariableisinferredaspartofthetranslationprocess.5.2LocalityWiththefirstexample,wedemonstratethatSlicStan’sblocklesssyntaxmakesiteasiertofollowgoodsoftwaredevelopmentpractices,suchasdeclaringvariablesclosetowheretheyareused,andforwritingoutmodelsthatfollowagenerativestory.Itabstractsawaysomeofthespecificsoftheunderlyinginferencealgorithm,andthuswritingoptimisedprogramsrequireslessmentaleffort.Consideranexampleadaptedfrom[StanDevelopmentTeam2017,p.101].Weareinterestedininferringthemeanµyandvarianceσ2yoftheindependentandidenticallydistributedvariablesy∼N(µy,σy).Themodelparametersareµy(themeanofy),andτy=1/σ2y(theprecisionofy).Below,weshowthisexamplewritteninSlicStan(left)andStan(right).SlicStan1realalpha=0.1;2realbeta=0.1;3realtau_y∼gamma(alpha,beta);45datarealmu_mu;6datarealsigma_mu;7realmu_y∼normal(mu_mu,sigma_mu);89realsigma_y=pow(tau_y,−0.5);10realvariance_y=pow(sigma_y,2);1112dataintN;13datareal[N]y;14for(iin1:N){y[i]∼normal(mu_y,sigma_y);}Stan1data{2realmu_mu;3realsigma_mu;4intN;5realy[N];6}7transformeddata{8realalpha=0.1;9realbeta=0.1;10}11parameters{12realmu_y;13realtau_y;14}15transformedparameters{16realsigma_y=pow(tau_y,−0.5);17}18model{19tau_y∼gamma(alpha,beta);20mu_y∼normal(mu_mu,sigma_mu);21for(iin1:N){y[i]∼normal(mu_y,sigma_y);}22}23generatedquantities{24realvariance_y=pow(sigma_y,2);25}ThelackofblocksinSlicStanmakesitmoreflexibleintermsoforderofstatements.ThecodehereiswrittentofollowmorecloselythanStanthegenerativestory:wefirstlydefinethepriordistributionoverparameters,andthenspecifyhowwebelievedatawasgeneratedfromthem.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:23Wealsokeepdeclarationsofvariablesclosetowheretheyhavebeenused:forexample,sigma_yisdefinedrightbeforeitisusedinthedefinitionofvariance_y.ThismodelcanbeexpressedinSlicStanbyusinganyorderofthestatements,providedthatvariablesarenotusedbeforetheyaredeclared.InStanthisisnotalwayspossibleandmayresultincloselyrelatedstatementsbeinglocatedfarawayfromeachother.WithSlicStanthereisnoneedtounderstandwhendifferentstatementsareexecutedinordertoperforminference.TheSlicStancodeistranslatedtothehand-optimisedStancode,asspecifiedbythemanual,withoutanyannotationsfromtheuser,apartfromwhattheinputdatatothemodelis.InStan,however,aninexperiencedStanprogrammermighthaveattemptedtodefinethetransformeddatavariablesalphaandbetainthedatablock,whichwouldresultinasyntacticerror.Evenmoresubtly,theycouldhavedefinedalpha,betaandvariance_yallinthetransformedparametersblock,inwhichcasetheprogramwillcompiletoalessefficient,semanticallyequivalentmodel.5.3CodeRefactoringThenextexampleisadaptedfrom[StanDevelopmentTeam2017,p.202],andshowshowtheabsenceofprogramblockscanleadtoeasiertorefactorcode.Westartfromasimplemodel,standardlinearregression,andshowwhatchangesneedtobemadeinbothSlicStanandStan,inordertochangethemodeltoaccountformeasurementerror.TheinitialmodelisasimpleBayesianlinearregressionwithNpredictorpointsx,andNoutcomesy.Ithas3parameters—theinterceptα,theslopeβ,andtheamountofnoiseσ.Inotherwords,y∼N(α1+βx,σI).Ifwewanttoaccountformeasurementnoise,weneedtointroduceanothervectorofvariablesxmeas,whichrepresentsthemeasuredpredictors(asopposedtothetruepredictorsx).Wepostulatethatthevaluesofxmeasarenoisy(withstandarddeviationτ)versionsofx:xmeas∼N(x,τI).ThenextpageshowsthesetwomodelswritteninSlicStan(left)andStan(right).Ignoringallthelines/correctionsinredgivesustheinitialregressionmodel,theonenotaccountingformeasurementerrors.Theentirecode,includingtheredcorrections,givesusthesecondregressionmodel,theonethatdoesaccountformeasurementerrors.Transitioningfrommodelonetomodeltworequiresthefollowingcorrections:•InSlicStan:–Deletethedatakeywordforx(line2).–Introduceanywhereintheprogramstatementsdeclaringthemeasurementsxmeas,theirdeviationτ,thenowparameterx,anditshyperparametersµx,σx(lines11–17).•InStan:–Movex’sdeclarationfromdatatoparameters(line5andline9).–Declarexmeasandτindata(lines3–4).–Declarex’shyperparametersµxandσxinparameters(lines10–11).–Addstatementsmodellingxandxmeasinmodel(lines18–19).PerformingthecoderefactoringrequiresthesameamountofcodeinSlicStanandStan.However,inSlicStanthechangesinterferemuchlesswiththecodealreadywritten.Wecanaddstatementsextendingthemodelanywhere(aslongvariablesaredeclaredbeforetheyareused).InStan,ontheotherhand,weneedtomodifyeachblockseparately.ThisexampledemonstratesasuccessfulsteptowardsouraimofmakingStanmorecompositional—composingprogramsiseasierinSlicStan.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:24MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonRegressioninSlicStan1dataintN;2datareal[N]x;3realmu_x;4realsigma_x;5datareal[N]x_meas;6datarealtau;78realalpha∼normal(0,10);9realbeta∼normal(0,10);10realsigma∼cauchy(0,5);11datareal[N]y;1213for(iin1:N){14x[i]∼normal(mu_x,sigma_x);15x_mean[i]∼normal(x[i],tau);16y[i]∼normal(alpha+beta∗x[i],sigma);17}RegressioninStan1data{2intN;3real[N]x_meas;4realtau;5real[N]x;6real[N]y;7}8parameters{9real[N]x;10realmu_x;11realsigma_x;12realalpha;13realbeta;14realsigma;15}16model{17alpha∼normal(0,10);18beta∼normal(0,10);19sigma∼cauchy(0,5);20for(iin1:N){21x[i]∼normal(mu_x,sigma_x);22x_mean[i]∼normal(x[i],tau);23y[i]∼normal(alpha+beta∗x[i],sigma);}24}5.4CodeReuseFinally,wedemonstratetheusageofmoreflexiblefunctionsinSlicStan,whichallowforbettercodereuse,andthereforecanleadtoshorter,morereadablecode.Intheintroductionofthispaper,wepresentedatransformationthatiscommonlyusedwhenspecifyinghierarchicalmodel—thenon-centredparametrisationofanormalvariable.Inbrief,anMCMCsamplermayhavedifficultiesinexploringaposteriordensitywell,ifthereexiststrongnon-lineardependenciesbetweenvariables.Insuchcases,wecanreparameterisethemodel:wecanexpressitintermsofdifferentparameters,sothattheoriginalparameterscanberecovered.Inthecaseofanormalvariablex∼N(µ,σ),wedefineitasx=µ+σx′,wherex′∼N(0,1).Weexplaininmoredetailtheusageofthenon-centeredparametrisationinAppendixD.Inthissection,weshowthe“EightSchools”example[Gelmanetal.2013,p.119],whichalsousesnon-centredparametrisationinordertoimproveperformance.EightschoolsstudytheeffectsoftheirSAT-Vcoachingprogram.Theinputdataistheestimatedeffectsyoftheprogramforeachoftheeightschools,andtheirsharedstandarddeviationσ.Thetaskistospecifyamodelthataccountsforerrors,byconsideringtheobservedeffectstobenoisyestimatesofthetrueeffectsθ.AssumingaGaussianmodelfortheeffectsandthenoise,wehavey∼N(θ,σI)andθ∼N(µ1,τI).BelowisthismodelwritteninSlicStan(left)andStan(right,adaptedfromStan’sGitHubrepository[StanDevelopmentTeam2018b]).Inbothcases,weusenon-centredreparameterisationtoimproveperformance:inStan,thecoachingeffectfortheithschool,theta[i],isdeclaredasatransformedparameterobtainedfromthestandardnormalvariableeta[i];inSlicStan,wecanonceagainmakeuseofthenon-centredreparameterisationfunctionmy_normal.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:25“EightSchools”inSlicStan1realmy_normal(realm,realv){2realstd∼normal(0,1);3returnv∗std+m;4}56datareal[8]y;7datareal[8]sigma;8real[8]theta;910realmu;11realtau;1213for(iin1:8){14theta[i]=my_normal(mu,tau);15y[i]∼normal(theta[i],sigma[i]);16}“EightSchools”inStan1data{2realy[8];3realsigma[8];4}5parameters{6realmu;7realtau;8realtheta_std[8];9}10transformedparameters{11realtheta[8];12for(jin1:8){theta[j]=mu+tau∗theta_std[i];}13}14model{15for(jin1:8){16y[i]∼normal(theta[i],sigma[i]);817theta_std[i]∼normal(0,1);18}19}OneadvantageoftheoriginalStancodecomparedtoSlicStanistheflexibilitytheuserhastonameallmodelparameters.InStan,theauxiliarystandardnormalvariablestheta_stdarenamedbytheuser,whileinSlicStan,thenamesofparametersdefinedinsideofafunctionareautomaticallygenerated,andmightnotcorrespondtothenamesoftransformedparametersofinterest.Allparameternamesareimportant,astheyarepartoftheoutputofthesamplingalgorithm,whichisshowntotheuser.Eventhoughinthiscasetheauxiliaryparameterswereintroducedsolelyforperformancereasons,inspectingtheirvaluesinStan’soutputcanbeusefulfordebuggingpurposes.6RELATEDWORKThereexistsarangeofprobabilisticprogramminglanguagesandsystems.Stan’ssyntaxisinspiredbythatofBUGS[Gilksetal.1994],whichusesGibbssamplingtoperforminference.OtherlanguagesincludeAnglican[Woodetal.2014],Church[Goodmanetal.2012]andVenture[Mansinghkaetal.2014],whichfocusonexpressivenessofthelanguageandrangeofsupportedmodels.Theyprovidecleansyntaxandformalisedsemantics,butuselessefficient,moregeneral-purposeinferencealgorithms.TheInfer.NETframework[Minkaetal.2014]usesanefficientinferencealgorithmcalledexpectationpropagation,butsupportsalimitedrangeofmodels.Turing[Geetal.2018]allowsdifferentinferencetechniquestobeusedfordifferentsub-partsofthemodel,butrequirestheusertoexplicitlyspecifywhichinferencealgorithmstouseaswellastheirhyperparametersMorerecently,therehasbeentheintroductionofdeepprobabilisticprogramming,intheformofEdward[Tranetal.2018,2016]andPyro[UberAILabs2017],whichfocusonusingdeeplearningtechniquesforprobabilisticprogramming.EdwardandPyroarebuiltontopofthedeeplearninglibrariesTensorFlow[Abadietal.2016]andPyTorch[Paszkeetal.2017]respectively,andsupportarangeofefficientinferencealgorithms.However,theylacktheconcisenessandformalismofsomeoftheothersystems,anditmanycasesrequiresophisticatedunderstandingofinference.8InthefullversionofStanthesestatementscanbe“vectorised”forefficiency,e.g.y∼normal(theta,sigma);Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:26MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonOtherlanguagesandsystemsincludeHakaru[Narayananetal.2016],Figaro[Pfeffer2009],Fun[Borgströmetal.2011],Greta[Goldingetal.2018]andmanyothers.Therestofthissectionaddressesrelatedworkdonemostlywithintheprogramminglanguagescommunity,whichfocusesonthesemantics(§§6.1),staticanalysis(§§6.2),andusability(§§6.3)ofprobabilisticprogramminglanguages.AmoreextensiveoverviewoftheconnectionbetweenprobabilisticprogrammingandprogramminglanguageresearchisgivenbyGordonetal.[2014b].6.1FormalisationofProbabilisticProgrammingLanguagesTherehasbeenextensiveworkontheformalisationofprobabilisticprogramminglanguagessyntaxandsemantics.AwidelyaccepteddenotationalsemanticsformalisationisthatofKozen[1981].Otherworkincludesadomain-theoreticsemantics[JonesandPlotkin1989],measure-theoreticsemantics[Borgströmetal.2011;Ścibioretal.2015;Torontoetal.2015],operationalsemantics[Borgströmetal.2016a;DalLagoandZorzi2012;Statonetal.2016],andmorerecently,categoricalformalisationforhigher-orderprobabilisticprograms[Heunenetal.2017].Mostpreviousworkspecifieseitherameasure-theoreticdenotationalsemantics,orasampling-basedoperationalsemantics.Somework[HuangandMorrisett2016;Huretal.2015;Statonetal.2016]givesbothdenotationalandoperationalsemantics,andshowsacorrespondencebetweenthetwo.Thedensity-basedsemanticswespecifyforStanandSlicStanisinspiredbytheworkofHuretal.[2015],whogiveanoperationalsampling-basedsemanticstotheimperativelanguageProb.Intuitively,thedifferencebetweenthetwostylesofoperationalsemanticsis:•Operationaldensity-basedsemanticsspecifieshowaprogramSisexecutedtoevaluatethe(unnormalised)posteriordensityp∗(θ|D)atsomespecificpointθoftheparameterspace.•Operationalsampling-basedsemanticsspecifieshowaprogramSisexecutedtoevaluatethe(unnormalised)probabilityp∗(t)oftheprogramgeneratingsomespecifictraceofsamplest.RefertoAppendixB.2forexamplesandfurtherdiscussionofthedifferencesbetweendensity-basedandsampling-basedsemantics.6.2StaticAnalysisforProbabilisticProgrammingLanguagesWorkonstaticanalysisforprobabilisticprogramsincludesseveralpapersthatfocusonimprovingefficiencyofinference.R2[Norietal.2014]appliesasemantics-preservingtransformationtotheprobabilisticprogram,andthenusesamodifiedversionoftheMetropolis–Hastingsalgorithmthatexploitsthestructureofthemodel.Thisresultsinmoreefficientsampling,whichcanbefurtherimprovedbyslicingtheprogramtoonlycontainpartsrelevanttoestimatingatargetprobabilitydistribution[Huretal.2014].Claretetal.[2013]presentanewinferencealgorithmthatisbasedondata-flowanalysis.Hakaru[Narayananetal.2016]isarelativelynewprobabilisticprogramminglanguageembeddedinHaskell,whichperformsautomaticandsemantic-preservingtransformationsontheprogram,inordertocalculateconditionaldistributionsandperformexactinferencebycomputeralgebra.ThePSIsystem[Gehretal.2016]analysesprobabilisticprogramsusingasymbolicdomain,andoutputsasimplifiedexpressionrepresentingtheposteriordistribution.TheJulia-embeddedlanguageGen[Cusumano-TownerandMansinghka2018]usestypeinferencetoautomaticallygenerateinferencetacticsfordifferentsub-partsofthemodel.SimilarlytoTuring,theuserthencombinesthegeneratedtacticstobuildamodel-specificinferencealgorithm.Withtheexceptionoftheworkonslicing[Huretal.2014],whichisshowntoworkwithChurchandInfer.NET,eachoftheabovesystemseitherusesitsownprobabilisticlanguageorthemethodisapplicableonlytoarestrictedtypeofmodels(forexamplebooleanprobabilisticprograms).SlicStanisdifferentinthatitusesinformationflowanalysisandtypeinferenceinordertoself-optimisetoStan—ascalableprobabilisticprogramminglanguagewithalargeuser-base.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:276.3UsabilityofProbabilisticProgrammingLanguagesThispaperalsorelatestothelineofworkonusabilityofprobabilisticprogramminglanguages.Gordonetal.[2014a]implementaschema-drivenlanguage,Tabular,whichallowsprobabilisticprogramstobewrittenasannotatedrelationalschemas.Fabular[Borgströmetal.2016b]extendsthisideabyincorporatingsyntaxforhierarchicallinearregressioninspiredbythelme4package[Batesetal.2014].BayesDB[Mansinghkaetal.2015]introducesBQL(BayesianQueryLanguage),whichcanbeusedtoanswerstatisticalquestionsaboutdata,throughSQL-likequeries.Otherworkincludesvisualisationofprobabilisticprograms,intheformofgraphicalmodels[Bishopetal.2002;Gilksetal.1994;Gorinovaetal.2016],andmoredata-drivenapproaches,suchassynthesisingprogramsfromrelationaldatasets[ChasinsandPhothilimthana2017;Norietal.2015].7CONCLUSIONProbabilisticinferenceisachallengingtask.Asaconsequence,existingprobabilisticlanguagesareforcedtotradeoffefficiencyofinferenceforrangeofsupportedmodelsandusability.Forexample,Stan,anincreasinglypopularprobabilisticprogramminglanguage,makesefficientscalableautomaticinferencepossible,butsacrificescompositionalityofthelanguage.ThispaperformalisesthesyntaxofacoresubsetofStanandgivesitsoperationaldensity-basedsemantics;itintroducesanew,compositionalprobabilisticprogramminglanguage,SlicStan;anditgivesasemantic-preservingprocedurefortranslatingSlicStantoStan.SlicStanadoptsaninformation-flowtypesystem,thatcapturesthetaxonomyclassesofvariablesoftheprobabilisticmodel.Theclassescanbeinferredtoautomaticallyoptimisetheprogramforprobabilisticinference.Tothebestofourknowledge,thisworkisthefirstformaltreatmentoftheStanlanguage.Weshowthattheuseofstaticanalysisandformallanguagetreatmentcanfacilitateefficientblack-boxprobabilisticinference,andimproveusability.Lookingforward,itwouldbeinterestingtoformalisetheusageofpseudo-randomgeneratorsinsideofStan.Variablesinthegeneratedquantitiesblockcanbegeneratedusingpseudo-randomnumbergenerators.Inotherwords,theusercanexplicitlycomposeHamiltonianMonteCarlowithforward(ancestral)samplingtoimproveinferenceperformance.SlicStancanbeextendedtoautomaticallydeterminewhatthemostefficientwaytosampleavariableis,whichcouldsignificantlyimproveusability.Anotherinterestingfuturedirectionwouldbetoadaptthesampling-basedsemanticsofHuretal.[2015]toSlicStanandestablishhowthedensity-basedsemanticsofthispapercorrespondstoit.ACKNOWLEDGMENTSWethankBobCarpenterandtheStanteamforinsightfuldiscussions,andtheanonymousreviewersandGeorgePapamakariosforusefulcomments.MariaGorinovawassupportedbytheEPSRCCentreforDoctoralTraininginDataScience,fundedbytheUKEngineeringandPhysicalSciencesResearchCouncil(grantEP/L016427/1)andtheUniversityofEdinburgh.REFERENCESMartínAbadi,AnindyaBanerjee,NevinHeintze,andJonGRiecke.1999.Acorecalculusofdependency.InProceedingsofthe26thACMSIGPLAN-SIGACTsymposiumonPrinciplesofProgrammingLanguages(POPL).ACM,147–160.MartínAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,JeffreyDean,MatthieuDevin,SanjayGhemawat,GeoffreyIrving,MichaelIsard,etal.2016.TensorFlow:ASystemforLarge-ScaleMachineLearning..InOSDI,Vol.16.265–283.DouglasBates,MartinMaechler,BenBolker,StevenWalker,etal.2014.lme4:Linearmixed-effectsmodelsusingEigenandS4.Rpackageversion1,7(2014),1–23.MichaelBetancourt.2014.HamiltonianMonteCarloandStan.MachineLearningSummerSchool(MLSS)lecturenotes(2014).MichaelBetancourt.2017.AconceptualintroductiontoHamiltonianMonteCarlo.arXivpreprintarXiv:1701.02434(2017).Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:28MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonChristopherMBishop,DavidSpiegelhalter,andJohnWinn.2002.VIBES:AvariationalinferenceengineforBayesiannetworks.InAdvancesinNeuralInformationProcessingSystems.777–784.JohannesBorgström,UgoDalLago,AndrewD.Gordon,andMarcinSzymczak.2016a.ALambda-calculusFoundationforUniversalProbabilisticProgramming.InProceedingsofthe21stACMSIGPLANInternationalConferenceonFunctionalProgramming(ICFP2016).ACM,NewYork,NY,USA,33–46.https://doi.org/10.1145/2951913.2951942JohannesBorgström,AndrewD.Gordon,MichaelGreenberg,JamesMargetson,andJurgenVanGael.2011.MeasureTransformerSemanticsforBayesianMachineLearning.InProceedingsofthe20thEuropeanConferenceonProgrammingLanguagesandSystems:PartoftheJointEuropeanConferencesonTheoryandPracticeofSoftware(ESOP’11/ETAPS’11).Springer-Verlag,Berlin,Heidelberg,77–96.http://dl.acm.org/citation.cfm?id=1987211.1987216JohannesBorgström,AndrewD.Gordon,LongOuyang,ClaudioRusso,AdamŚcibior,andMarcinSzymczak.2016b.Fabular:RegressionFormulasAsProbabilisticProgramming.InProceedingsofthe43rdAnnualACMSIGPLAN-SIGACTSymposiumonPrinciplesofProgrammingLanguages(POPL’16).ACM,NewYork,NY,USA,271–283.https://doi.org/10.1145/2837614.2837653BobCarpenter,AndrewGelman,MatthewHoffman,DanielLee,BenGoodrich,MichaelBetancourt,MarcusBrubaker,JiqiangGuo,PeterLi,andAllenRiddell.2017.Stan:AProbabilisticProgrammingLanguage.JournalofStatisticalSoftware,Articles76,1(2017),1–32.https://doi.org/10.18637/jss.v076.i01SarahChasinsandPhitchayaMangpoPhothilimthana.2017.Data-DrivenSynthesisofFullProbabilisticPrograms.InInternationalConferenceonComputerAidedVerification.Springer,279–304.GuillaumeClaret,SriramKRajamani,AdityaVNori,AndrewDGordon,andJohannesBorgström.2013.Bayesianinferenceusingdataflowanalysis.InProceedingsofthe20139thJointMeetingonFoundationsofSoftwareEngineering.ACM,92–102.MarcoCusumano-TownerandVikashK.Mansinghka.2018.ADesignProposalforGen:ProbabilisticProgrammingwithFastCustomInferenceviaCodeGeneration.WorkshoponMachineLearningandProgrammingLanguages(MAPL)(2018).UgoDalLagoandMargheritaZorzi.2012.Probabilisticoperationalsemanticsforthelambdacalculus.RAIRO-TheoreticalInformaticsandApplications46,3(2012),413–450.HongGe,KaiXu,andZoubinGhahramani.2018.Turing:ALanguageforFlexibleProbabilisticInference.InProceedingsoftheTwenty-FirstInternationalConferenceonArtificialIntelligenceandStatistics(AISTATS)(ProceedingsofMachineLearningResearch),AmosStorkeyandFernandoPerez-Cruz(Eds.),Vol.84.PMLR,PlayaBlanca,Lanzarote,CanaryIslands,1682–1690.http://proceedings.mlr.press/v84/ge18b.htmlTimonGehr,SasaMisailovic,andMartinVechev.2016.PSI:Exactsymbolicinferenceforprobabilisticprograms.InInternationalConferenceonComputerAidedVerification.Springer,62–83.AndrewGelman,JohnBCarlin,HalSStern,DavidBDunson,AkiVehtari,andDonaldBRubin.2013.BayesianDataAnalysis.Chapman&Hall/CRCPress,London,thirdedition.AndrewGelmanandJenniferHill.2007.DataanalysisusingRegressionandMultilevel/HierarchicalModels.CambridgeUniversityPress.625pages.WRGilks,AThomas,andD.J.Spiegelhalter.1994.AlanguageandprogramforcomplexBayesianmodelling.TheStatistician43(1994),169–178.JosephAGoguenandJoséMeseguer.1982.Securitypoliciesandsecuritymodels.InSecurityandPrivacy,1982IEEESymposiumon.IEEE,11–11.NickGolding,TymoteuszWołodźko,andBenMarwick.2018.Greta:SimpleandScalableStatisticalModellinginR.2018.https://greta-dev.github.io/greta/index.htmlNoahGoodman,VikashMansinghka,DanielRoy,KeithBonawitz,andDanielTarlow.2012.Church:alanguageforgenerativemodels.arXivpreprintarXiv:1206.3255(2012).AndrewDGordon,ThoreGraepel,NicolasRolland,ClaudioRusso,JohannesBorgstrom,andJohnGuiver.2014a.Tabular:aschema-drivenprobabilisticprogramminglanguage.InACMSIGPLANNotices,Vol.49.ACM,321–334.AndrewD.Gordon,ThomasA.Henzinger,AdityaV.Nori,andSriramK.Rajamani.2014b.Probabilisticprogramming.InFutureofSoftwareEngineering(FOSE2014).167–181.MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton.2018a.SlicStan:ABlocklessStan-likeLanguage.StanCon.https://doi.org/10.5281/zenodo.1284348MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton.2018b.SlicStan:ImprovingProbabilisticProgrammingusingInformationFlowAnalysis.ProbabilisticProgrammingLanguages,Semantics,andSystems(PPS2018)(2018).MariaI.Gorinova,AdvaitSarkar,AlanF.Blackwell,andDonSyme.2016.ALive,Multiple-RepresentationProbabilisticProgrammingEnvironmentforNovices.InProceedingsofthe2016CHIConferenceonHumanFactorsinComputingSystems(CHI’16).ACM,NewYork,NY,USA,2533–2537.https://doi.org/10.1145/2858036.2858221ChrisHeunen,OhadKammar,SamStaton,andHongseokYang.2017.AConvenientCategoryforHigher-OrderProbabilityTheory.arXivpreprintarXiv:1701.02547(2017).Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:29MatthewDHoffmanandAndrewGelman.2014.TheNo-U-turnsampler:adaptivelysettingpathlengthsinHamiltonianMonteCarlo.JournalofMachineLearningResearch15,1(2014),1593–1623.DanielHuangandGregMorrisett.2016.Anapplicationofcomputabledistributionstothesemanticsofprobabilisticprogramminglanguages.InEuropeanSymposiumonProgrammingLanguagesandSystems.Springer,337–363.Chung-KilHur,AdityaV.Nori,SriramK.Rajamani,andSelvaSamuel.2015.AProvablyCorrectSamplerforProbabilisticPrograms.In35thIARCSAnnualConferenceonFoundationofSoftwareTechnologyandTheoreticalComputerScience,FSTTCS2015,December16-18,2015,Bangalore,India(LIPIcs),PrahladhHarshaandG.Ramalingam(Eds.),Vol.45.SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,475–488.https://doi.org/10.4230/LIPIcs.FSTTCS.2015.475Chung-KilHur,AdityaVNori,SriramKRajamani,andSelvaSamuel.2014.Slicingprobabilisticprograms.InACMSIGPLANNotices,Vol.49.ACM,133–144.ClaireJonesandGordonDPlotkin.1989.AProbabilisticPowerdomainofEvaluations.InLogicinComputerScience,1989.LICS’89,Proceedings.,FourthAnnualSymposiumon.IEEE,186–195.DexterKozen.1981.SemanticsofProbabilisticPrograms.Journalofcomputerandsystemsciences22,3(1981),328–350.DavidLunn,ChristopherJackson,NickyBest,AndrewThomas,andDavidSpieghalter.2013.TheBUGSBook.CRCPress.VikashMansinghka,DanielSelsam,andYuraPerov.2014.Venture:ahigher-orderprobabilisticprogrammingplatformwithprogrammableinference.arXivpreprintarXiv:1404.0099(2014).VikashMansinghka,RichardTibbetts,JayBaxter,PatShafto,andBaxterEaves.2015.BayesDB:Aprobabilisticprogrammingsystemforqueryingtheprobableimplicationsofdata.arXivpreprintarXiv:1512.05006(2015).T.Minka,J.M.Winn,J.P.Guiver,S.Webster,Y.Zaykov,B.Yangel,A.Spengler,andJ.Bronskill.2014.Infer.NET2.6.MicrosoftResearchCambridge.http://research.microsoft.com/infernet.IainMurray.2007.AdvancesinMarkovChainMonteCarlomethods.UniversityofLondon,UniversityCollegeLondon(UnitedKingdom).PraveenNarayanan,JacquesCarette,WrenRomano,Chung-chiehShan,andRobertZinkov.2016.ProbabilisticInferencebyProgramTransformationinHakaru(SystemDescription).InFunctionalandLogicProgramming,OlegKiselyovandAndyKing(Eds.).SpringerInternationalPublishing,Cham,62–79.RadfordMNeal.2003.Slicesampling.Annalsofstatistics(2003),705–741.RadfordMNealetal.2011.MCMCusingHamiltoniandynamics.HandbookofMarkovChainMonteCarlo2,11(2011).AdityaVNori,Chung-KilHur,SriramKRajamani,andSelvaSamuel.2014.R2:AnEfficientMCMCSamplerforProbabilisticPrograms..InAAAI.2476–2482.AdityaVNori,SherjilOzair,SriramKRajamani,andDeepakVijaykeerthy.2015.Efficientsynthesisofprobabilisticprograms.InACMSIGPLANNotices,Vol.50.ACM,208–217.OmirosPapaspiliopoulos,GarethORoberts,andMartinSköld.2007.Ageneralframeworkfortheparametrizationofhierarchicalmodels.Statist.Sci.(2007),59–73.AdamPaszke,SamGross,SoumithChintala,GregoryChanan,EdwardYang,ZacharyDeVito,ZemingLin,AlbanDesmaison,LucaAntiga,andAdamLerer.2017.AutomaticdifferentiationinPyTorch.(2017).AviPfeffer.2009.Figaro:Anobject-orientedprobabilisticprogramminglanguage.CharlesRiverAnalyticsTechnicalReport137(2009),96.MartynPlummeretal.2003.JAGS:AprogramforanalysisofBayesiangraphicalmodelsusingGibbssampling.InProceedingsofthe3rdinternationalworkshopondistributedstatisticalcomputing,Vol.124.Vienna,Austria.AndreiSabelfeldandAndrewCMyers.2003.Language-basedinformation-flowsecurity.IEEEJournalonselectedareasincommunications21,1(2003),5–19.JohnSalvatier,ThomasV.Wiecki,andChristopherFonnesbeck.2016.ProbabilisticprogramminginPythonusingPyMC3.PeerJComputerScience2(April2016),e55.https://doi.org/10.7717/peerj-cs.55AdamŚcibior,ZoubinGhahramani,andAndrewDGordon.2015.Practicalprobabilisticprogrammingwithmonads.InACMSIGPLANNotices,Vol.50.ACM,165–176.GeoffreySmith.2007.Principlesofsecureinformationflowanalysis.MalwareDetection(2007),291–307.StanDevelopmentTeam.2017.StanModelingLanguage:User’sGuideandReferenceManual.http://mc-stan.org.StanDevelopmentTeam.2018a.RStan:theRinterfacetoStan.http://mc-stan.org/Rpackageversion2.17.3.StanDevelopmentTeam.2018b.StanGitHubRepositories.https://github.com/stan-devSamStaton,HongseokYang,FrankWood,ChrisHeunen,andOhadKammar.2016.Semanticsforprobabilisticprogramming:higher-orderfunctions,continuousdistributions,andsoftconstraints.InProceedingsofthe31stAnnualACM/IEEESymposiumonLogicinComputerScience.ACM,525–534.MarcinSzymczak.2018.ProgramminglanguagesemanticsasafoundationforBayesianinference.Ph.D.Dissertation.TheUniversityofEdinburgh.SeanJTaylorandBenjaminLetham.2017.ForecastingatScale.https://facebookincubator.github.io/prophet/.NeilToronto,JayMcCarthy,andDavidVanHorn.2015.Runningprobabilisticprogramsbackwards.InEuropeanSymposiumonProgrammingLanguagesandSystems.Springer,53–79.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:30MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonDustinTran,MatthewD.Hoffman,SrinivasVasudevan,ChristopherSuter,DaveMoore,AlexeyRadul,MatthewJohnson,andRifA.Saurous.2018.Edward2:Simple,Distributed,Accelerated.(2018).https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/edward2ToappearinAdvancesinNeuralInformationProcessingSystems.DustinTran,AlpKucukelbir,AdjiB.Dieng,MajaRudolph,DawenLiang,andDavidM.Blei.2016.Edward:Alibraryforprobabilisticmodeling,inference,andcriticism.arXivpreprintarXiv:1610.09787(2016).UberAILabs.2017.Pyro:Adeepprobabilisticprogramminglanguage.http://pyro.ai/.DennisVolpano,CynthiaIrvine,andGeoffreySmith.1996.Asoundtypesystemforsecureflowanalysis.Journalofcomputersecurity4,2-3(1996),167–187.FrankWood,JanWillemvandeMeent,andVikashMansinghka.2014.ANewApproachtoProbabilisticProgrammingInference.InProceedingsofthe17thInternationalconferenceonArtificialIntelligenceandStatistics.1024–1032.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:31ADEFINITIONSANDPROOFSA.1DefinitionsDefinitionA.1(Assigns-tosetW(S)).W(S)isthesetthatcontainsthenamesofglobalvariablesthathavebeenassignedtowithinthestatementS.Itisdefinedrecursivelyasfollows:W(x[E1]...[En]=E)={x}W({Tx;S})=W(S)\{x}W(S1;S2)=W(S1)∪W(S2)W(skip)=∅W(if(E)S1elseS2)=W(S1)∪W(S2)W(for(xinE1:E2)S)=W(S)\{x}DefinitionA.2(ReadssetR(S)).R(S)isthesetthatcontainsthenamesofglobalvariablesthathavebeenreadwithinthestatementS.Itisdefinedrecursivelyasfollows:R(x)={x}R(c)=∅R([E1,...,En])=—ni=1R(Ei)R(E1[E2])=R(E1)∪R(E2)R(f(E1,...,En))=—ni=1R(Ei)R(F(E1,...,En))=—ni=1R(Ei)R(x[E1]...[En]=E)=—ni=1R(Ei)∪R(E)R({Tx;S})=R(S)\{x}R(S1;S2)=R(S1)∪R(S2)R(skip)=∅R(if(E)S1elseS2)=R(E)∪R(S1)∪R(S2)R(for(xinE1:E2)S)=R(E1)∪R(E2)∪R(S)\{x}DefinitionA.3(TypeofexpressionEinΓ).Γ(E)isthetypeoftheexpressionEwithrespecttoΓ:Γ(x)=(τ,ℓ)forx:(τ,ℓ)∈ΓΓ(c)=(ty(c),data)Γ([E1,...,En])=(τ[],ℓ1⊔···⊔ℓn)ifΓ(Ei)=(τ,ℓi)fori∈1..n,andℓ′⊔ℓ′′denotingtheleastupperboundofℓ′andℓ′′,,anditisundefinedotherwise.Γ(E1[E2])=(τ,ℓ⊔ℓ′)ifΓ(E1)=(τ[],ℓ)andΓ(E2)=(int,ℓ′),anditisundefinedotherwise.Γ(f(E1,...,En))=(τ,ℓ)ifΓ(Ei)=(τi,ℓi)fori∈1..n,andf:(τ1,ℓ1),...,(τn,ℓn)→(τ,ℓ).TheelaborationrelationtransformsSlicStanstatementsandexpressionstoCoreStanstatementsandexpressions.Thus,throughoutthisdocument,weusetheterms“elaboratedstatement”and“elaboratedexpression”tomeanaCoreStanstatementandaCoreStanexpressionrespectively.DefinitionA.4(VectorisingfunctionsvΓ,vE,vS).(1)vΓ(Γ,n),{x:(τ[n],ℓ)}x:(τ,ℓ)∈Γ,foranytypingenvironmentΓ.(2)vE(x,Γ,E)isdefinedforavariablex,typingenvironmentΓ,andanelaboratedexpressionE:vE(x,Γ,x′)=(x′[x]ifx′∈dom(Γ)x′ifx′<dom(Γ)vE(x,Γ,c)=cvE(x,Γ,[E1,...,En])=[vE(E1),...,vE(En)]vE(x,Γ,E1[E2])=vE(E1)[vE(E2)]vE(x,Γ,f(E1,...,En))=f(vE(E1),...,vE(En))(3)vS(x,Γ,S)isdefinedforavariablex,typingenvironmentΓ,andanelaboratedstatementS:vS(x,Γ,L=E)=(vE(L)=vE(E))vS(x,Γ,S1;S2)=vS(x,Γ,S1);vS(x,Γ,S2)vS(x,Γ,if(E)S1elseS2)=if(vE(E))vS(S1)elsevS(S2)vS(x,Γ,for(x′inE1:E2)S′)=for(x′invE(E1):vE(E2))vS(S′))vS(skip)=skipDefinitionA.5(RΓ⊢ℓ(S)).RΓ⊢ℓ(S)isthesetthatcontainsthenamesofglobalvariablesthathavebeenreadatlevelℓwithrespecttoΓwithinthestatementS.Itisdefinedrecursivelyasfollows:RΓ⊢ℓ(x)=∅RΓ⊢ℓ(x[E1]...[En])=(—ni=1R(Ei)ifΓ(x)=ℓ∅otherwiseRΓ⊢ℓ(L=E)=(RΓ⊢ℓ(L)∪R(E)ifΓ(L)=ℓ∅otherwiseRΓ⊢ℓ(S1;S2)=RΓ⊢ℓ(S1)∪RΓ⊢ℓ(S2)Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:32MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonRΓ⊢ℓ(skip)=∅ForS=if(E)S1elseS2,andA=RΓ⊢ℓ(S1)∪RΓ⊢ℓ(S2):RΓ⊢ℓ(S)=(R(E)∪AifA,∅∅otherwiseForS=for(xinE1:E2)S′,andA=RΓ⊢ℓ(S′)RΓ⊢ℓ(S)=(R(E1)∪R(E2)∪AifA,∅∅otherwiseDefinitionA.6(WΓ⊢ℓ(S)).WΓ⊢ℓ(S),{x∈W(S)|Γ(x)=(τ,ℓ)forsomeτ}DefinitionA.7.MergingStanprogramsP1;P2LetP1andP2betwoStanprograms,suchthatfori=1,2:Pi=data{Γ(i)d}transformeddata{Γ(i)′dS(i)′d}parameters{Γ(i)m}transformedparameters{Γ(i)′mS(i)′m}model{Γ(i)′′mS(i)′′m}generatedquantities{Γ(i)qS(i)q}ThesequenceofP1andP2,writtenP1;P2isthendefinedas:Pi=data{Γ(1)d,Γ(2)d}transformeddata{Γ(1)′d,Γ(2)′dS(1)′d;S(2)′d}parameters{Γ(1)m,Γ(2)m}transformedparameters{Γ(1)′m,Γ(2)′mS(1)′m;S(2)′m}model{Γ(1)′′m,Γ(2)′′mS(1)′′m;S(2)′′m}generatedquantities{Γ(1)q,Γ(2)qS(1)q;S(2)q}A.2ProofofSemanticPreservationofShreddingLemmaA.8.Ifs|=Γthendom(s)=dom(Γ).Proof.Byinspectionofthedefinitionofs|=Γ.(cid:3)LemmaA.9.IfSiswell-typedinsomeenvironmentΓ,x∈dom(s)and(s,S)⇓s′andx<W(S)thens(x)=s′(x).Proof.Byinductiononthederivation(s,S)⇓s′.(cid:3)LemmaA.10.If(s1,S)⇓s′1and(s2,S)⇓s′2forsomes1,s′1,s2,s′2,ands1(x)=s2(x)forallx∈A,whereA⊇R(S),thens′1(y)=s′2(y)forally∈A∪W(S).Proof.ByinductiononthestructureofS.(cid:3)RestatementofLemma4.2(Shreddingproducessingle-levelstatements)S⇕Γ(SD,SM,SQ)=⇒Γ⊢data(SD)∧Γ⊢model(SM)∧Γ⊢genquant(SQ)Proof.ByruleinductiononthederivationofS⇕Γ(SD,SM,SQ).(cid:3)RestatementofLemma4.4(StatementReordering)ForstatementsS1andS2thatarewell-typedinΓ,ifR(S1)∩W(S2)=∅,W(S1)∩R(S2)=∅,andW(S1)∩W(S2)=∅thenS1;S2≃S2;S1.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:33Proof.LetRi=R(Si)andWi=W(Si)fori=1,2.Takeanystatesandassumethats|=Γ.Supposethat(s,S1)⇓s1,(s,S2)⇓s2,(s1,S2)⇓s12,and(s2,S1)⇓s21.Wewanttoprovethats12=s21.ByTheorem3.1andLemmaA.8,wehavedom(Γ)=dom(s)=dom(s1)=dom(s2)=dom(s12)=dom(s21).Now,asS1writesonlytoW1,byLemmaA.9,wehavethatforallvariablesx∈dom(Γ):x<W1=⇒s(x)=s1(x)∧s2(x)=s21(x)(2)ButR2andW1aredisjoint,andW2andW1aredisjoint,thereforex<W1forallx∈R2∪W2,andhencebyLemmaA.9:x∈R2∪W2=⇒s(x)=s1(x)∧s2(x)=s21(x)(3)IftwostatesareequaluptoallvariablesinR(S2),thenS2hasthesameeffectonthem(LemmaA.10).Combiningthiswith(3)givesus:x∈R2∪W2=⇒s2(x)=s12(x)(4)Next,combining(3)and(4),givesus:x∈R2∪W2=⇒s2(x)=s12(x)=s21(x)(5)Applyingthesamereasoning,butstartingfromS2,wealsoobtain:x<W2=⇒s(x)=s2(x)∧s1(x)=s12(x)(6)x∈R1∪W1=⇒s1(x)=s21(x)=s12(x)(7)Finally,wehave:•∀x∈R1∩W2.s12(x)=s21(x),asR1∩W2=∅;•∀x∈W1∩R2.s12(x)=s21(x),asW1∩R2=∅;•∀x∈W1∩W2.s12(x)=s21(x),asW1∩W2=∅;•∀x<W1∪W2.s12(x)=s21(x),bycombining(2)with(6);•∀x∈R2∪W2.s12(x)=s21(x),by(5);•∀x∈R1∪W1.s12(x)=s21(x),by(7);Thiscoversallpossiblecasesforx∈dom(Γ),therefore∀x∈dom(Γ).s12(x)=s21(x).Butdom(Γ)=dom(s12)=dom(s21),thuss12=s21.(cid:3)LemmaA.11.IfΓ(x)=(τ,ℓ),Γ⊢E:(τ′,ℓ′),andx∈R(E)thenℓ≤ℓ′.Proof.ByinductiononthestructureofthederivationΓ⊢E:(τ′,ℓ′).(cid:3)LemmaA.12.IfΓ(E)=(τ,ℓ),Γ⊢ℓ′(S),andEoccursinS,thenℓ≤ℓ′.Proof.ByinductiononthestructureofS.(cid:3)RestatementofLemma4.5IfΓ⊢ℓ1(S1),Γ⊢ℓ2(S2)andℓ1<ℓ2thenR(S1)∩W(S2)=∅andW(S1)∩W(S2)=∅.Proof.SupposeΓ⊢ℓ1(S1),Γ⊢ℓ2(S2)andℓ1<ℓ2.Then,directlyfromDefinition4.1,wehavethatW(S1)∩W(S2)=∅.Next,weprovebycontradictionthatR(S1)∩W(S2)=∅.Supposethatforsomex,x∈R(S1)andx∈W(S2).Fromx∈W(S2)andΓ⊢ℓ2(S2),wehaveΓ(x)=(τ,ℓ2)forsomeτ.FromthedefinitionofR(S1)andx∈R(S1),theremustbeanexpressionEinS1,suchthatx∈R(E).SupposeΓ(E)=(τE,ℓE).Now,Γ(x)=(τ,ℓ2),x∈R(E),andΓ⊢E:(τE,ℓE),thereforeℓ2≤ℓE(LemmaA.11).ButΓ⊢ℓ1(S1),andEoccursinS1,thereforeℓE≤ℓ1(LemmaA.12).Wehaveℓ2≤ℓE≤ℓ1.Butℓ1<ℓ2,whichisacontradiction.(cid:3)Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:34MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonRestatementofLemma4.6IfΓ⊢ℓ(S),thenRΓ⊢ℓ(S)=R(S)andWΓ⊢ℓ(S)=W(S).Proof.SupposeΓ⊢ℓ(S).Thefirstequality,RΓ⊢ℓ(S)=R(S),followsbystructuralinductiononRΓ⊢ℓ(S).Furthermore,bydefinitionofsingle-levelstatements,ifx∈W(S),thenΓ(x)=(τ,ℓ)forsomeτ.Thus,bydefinitionofWΓ⊢ℓ(S),wehavethatWΓ⊢ℓ(S)=W(S).(cid:3)LemmaA.13(Associativityof≃).S1;(S2;S3)≃(S1;S2);S3foranyS1,S2,andS3.Proof.Byexpandingthedefinitionofstatementequivalence(Definition4.3).(cid:3)LemmaA.14(Congruenceof≃).IfS1≃S2thenS1;S≃S2;SandS;S1≃S;S2foranyS.Proof.Byexpandingthedefinitionofstatementequivalence(Definition4.3).(cid:3)LemmaA.15.Foranytwostatess,s′,andastatementfor(xinд1:д2)S,supposen1=s(д1),n2=s(д2)andn1≤n2.Then(s,for(xinд1:д2)S)⇓s′ifandonlyifthereexistssx,suchthat(s,x=n1;S;x=(n1+1);S...x=n2;S)⇓sxands′=sx[−x].Proof.Byinductiononn=n2−n1.(cid:3)LemmaA.16.Ifx<W(S1)then(x=n;S1;S2)≃(x=n;S1;x=n;S2).Proof.Byexpandingthedefinitionofstatementequivalence(Definition4.3).(cid:3)LemmaA.17.IfΓ⊢ℓ1(S1),Γ⊢ℓ2(S2),ℓ1<ℓ2,Γ⊢S2;S1:data,andx<W(S2;S1)thenx=i;S2;x=j;S1≃x=j;S1;x=i;S2;x=jforallintegersiandj.Proof.Byexpandingthedefinitionofstatementequivalence(Definition4.3),andusingLemmaA.14andLemmaA.16.(cid:3)RestatementofLemma4.8(Commutativityofseqencingsingle-levelstatements)IfΓ⊢ℓ1(S1),Γ⊢ℓ2(S2),Γ⊢S2;S1:dataandℓ1<ℓ2thenS2;S1;≃S1;S2;Proof.SinceΓ⊢S2;S1:data,anddata≤ℓ1<ℓ2,itmustbethatRΓ⊢ℓ2(S2)∩WΓ⊢ℓ1(S1)=∅.ByLemma4.6,RΓ⊢ℓ2(S2)=R(S2)andWΓ⊢ℓ1(S1)=W(S1),asS1andS2aresingle-leveloflevelℓ1andℓ2respectively.Therefore,R(S2)∩W(S1)=∅.FromΓ⊢ℓ1(S1),Γ⊢ℓ2(S2),ℓ1<ℓ2andbyLemma4.5,wehaveR(S1)∩W(S2)=∅andW(S1)∩W(S2)=∅.Therefore,byLemma4.4,S2;S1≃S1;S2.(cid:3)RestatementofTheorem4.9(SemanticPreservationof⇕Γ)IfΓ⊢S:dataandS⇕Γ(SD,SM,SQ)thenlogp∗Γ⊢S(s)=logp∗Γ⊢(SD;SM;SQ)(s),foralls|=Γ.Proof.NotethatifS≃S′thenlogp∗Γ⊢S(s)=logp∗Γ⊢S′(s)forallstatess|=Γ.SemanticpreservationthenfollowsfromprovingthestrongerresultS⇕Γ(SD,SM,SQ)=⇒Γ⊢S:data=⇒S≃(SD;SM;SQ)byruleinductiononS⇕Γ(SD,SM,SQ).LetΦ(S,SD,SM,SQ),S⇕Γ(SD,SM,SQ)=⇒Γ⊢S:data=⇒S≃SD;SM;SQTakeanyS,SD,SM,SQ,andassumeS⇕Γ(SD,SM,SQ)andΓ⊢S:data.(ShredSkip)skip⇕Γ(skip,skip,skip)Foralls,(s,skip)⇓s,andalso(s,skip;skip;skip)⇓s.Thusskip≃skip;skip;skip.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:35(ShredDataAssign)Γ(L)=(_,data)L=E⇕Γ(L=E,skip,skip)Foranystates,if(s,L=E)⇓s′,then(s,L=E;skip;skip)⇓s′,andviceversa.Thus,Φ(L=E,L=E,skip,skip)holds.(ShredModelAssign)Γ(L)=(_,model)L=E⇕Γ(skip,L=E,skip)Foranystates,if(s,L=E)⇓s′,then(s,skip;L=E;skip)⇓s′,andviceversa.Thus,Φ(L=E,skip,L=E,skip)holds.(ShredGenQuantAssign)Γ(L)=(_,genqant)L=E⇕Γ(skip,skip,L=E)Foranystates,if(s,L=E)⇓s′,then(s,skip;skip;L=E)⇓s′,andviceversa.Thus,Φ(L=E,skip,skip,L=E)holds.(ShredSeq)S=(S1;S2).SupposethatΦ(Γ,S1)andΦ(Γ,S2),andassumeS1⇕Γ(SD1,SM1,SQ1),S2⇕Γ(SD2,SM2,SQ2).Thus,S1≃SD1;SM1;SQ1andS2≃SD2;SM2;SQ2.Now:≃(SD1;SM1;SQ1);(SD2;SM2;SQ2)fromΦ(Γ,S1)andΦ(Γ,S2)≃(SD1;SM1);SD2;SQ1;(SM2;SQ2)byLemmas4.2,4.8,A.13andA.14≃(SD1);SD2;SM1;(SQ1;SM2;SQ2)byLemmas4.2,4.8,A.13andA.14≃(SD1;SD2;SM1);SM2;SQ1;(SQ2)byLemmas4.2,4.8,A.13andA.14≃(SD1;SD2);(SM1;SM2);(SQ1;SQ2)Asby(ShredSeq),S⇕Γ(SD1;SD2),(SM1;SM2),(SQ1;SQ2),itfollowsthatΦ(Γ,S1;S2).(ShredIf)S=(if(д)S1elseS2).SupposethatΦ(Γ,S1)andΦ(Γ,S2),andassumeS1⇕Γ(SD1,SM1,SQ1),S2⇕Γ(SD2,SM2,SQ2).Thus,by(ShredIf):if(д)S1elseS2⇕Γ(if(д)SD1elseSD2),(if(д)SM1elseSM2),(if(д)SQ1elseSQ2)Nowtakeanytwostatessands′,suchthats|=Γand(s,S)⇓s′.GiventhatΓ⊢S:data,Γ(д)=(bool,_)by(If).Therefores(д)=trueors(д)=false.(1)Ifs(д)=true,itmustbethat(s,S1)⇓s′.Then:(s,if(д)S1elseS2)⇓s′by(EvalIfTrue)(s,(SD1;SM1;SQ1))⇓s′fromΦ(Γ,S1)(s,(if(д)SD1elseSD2;if(д)SM1elseSM2;if(д)SQ1elseSQ2))⇓s′3×(EvalIfTrue)(2)Ifs(д)=false,itmustbethat(s,S2)⇓s′.Then:(s,if(д)S1elseS2)⇓s′by(EvalIfFalse)(s,(SD2;SM2;SQ2))⇓s′fromΦ(Γ,S2)(s,(if(д)SD1elseSD2;if(д)SM1elseSM2;if(д)SQ1elseSQ2))⇓s′3×(EvalIfFalse)Thus,(s,if(д)S1elseS2))⇓s′=⇒(s,(if(д)SD1elseSD2;if(д)SM1elseSM2;if(д)SQ1elseSQ2))⇓s′.Fortheimplicationintheoppositedirection:Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:36MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton(1)Ifs(д)=true,takeanys′suchthat(s,(if(д)SD1elseSD2;if(д)SM1elseSM2;if(д)SQ1elseSQ2))⇓s′.Then:(s,(SD1;SM1;SQ1))⇓s′by3×(EvalIfTrue)(s,S1)⇓s′fromΦ(Γ,S1)(s,if(д)S1elseS2)⇓s′by(EvalIfTrue)(2)Ifs(д)=false,takeanys′suchthat(s,(if(д)SD1elseSD2;if(д)SM1elseSM2;if(д)SQ1elseSQ2))⇓s′.Then:(s,(SD2;SM2;SQ2))⇓s′by3×(EvalIfFalse)(s,S2)⇓s′fromΦ(Γ,S2)(s,if(д)S1elseS2)⇓s′by(EvalIfFalse)Thus,(s,(if(д)SD1elseSD2;if(д)SM1elseSM2;if(д)SQ1elseSQ2))⇓s′=⇒(s,if(д)S1elseS2))⇓s′.Therefore,if(д)S1elseS2≃(if(д)SD1elseSD2);(if(д)SM1elseSM2);(if(д)SQ1elseSQ2),andΦ(Γ,if(д)S1elseS2).(ShredFor)SupposeS=(for(xinд1:д2)S′)=LHS.Then:S′⇕Γ(S′D,S′M,S′Q)LHS⇕Γ(for(xinд1:д2)S′D),(for(xinд1:д2)S′M),(for(xinд1:д2)S′Q)TakeRHS=(for(xinд1:д2)S′D);(for(xinд1:д2)S′M);(for(xinд1:д2)S′Q)WemustshowΦ(S′,S′D,S′M,S′Q)=⇒LHS≃RHS.AssumeΦ(S′,S′D,S′M,S′Q),andconsiders,s′,suchthat(s,LHS)⇓s′toshow(s,RHS)⇓s′.Supposen1=s(д1)andn2=s(д2).Theneithern1≤n2orn1>n2:(1)Casen1≤n2.UsingLemmaA.15,wehavethatthereexistssx,suchthat(s,x=n1;S′;x=(n1+1);S′...x=n2;S′)⇓sxands′=sx[−x].AsΦ(S′,S′D,S′M,S′Q),S′⇕Γ(S′D,S′M,S′Q)andΓ⊢S′:data(by(For)),wehavethatS′≃S′D;S′M;S′Q.CombinedwiththeresultfromaboveandusingLemmaA.14,thisgivesus(s,x=n1;S′D;S′M;S′Q;x=(n1+1);S′D;S′M;S′Q...x=n2;S′D;S′M;S′Q)⇓sxands′=sx[−x].ByLemmaA.16,wethenhave(s,x=n1;S′D;x=n1;S′M;x=n1;S′Q;...x=n2;S′D;x=n2;S′M;x=n2;S′Q)⇓sxands′=sx[−x].ByLemma4.2Γ⊢data(S′D),Γ⊢model(S′M),andΓ⊢genqant(S′Q).Thus,weap-plyLemmaA.17toget(s,x=n1;S′D;...x=n2;S′D;x=n1;S′M;...x=n2;S′M;x=n1;S′Q;...x=n2;S′Q)⇓sxands′=sx[−x].Byapplying(EvalSeq),wesplitthisinto:–(s,x=n1;S′D;...x=n2;S′D)⇓sxd–(sxd,x=n1;S′M;...x=n2;S′M)⇓sxm–(sxm,x=n1;S′Q;...x=n2;S′Q)⇓sxForsomesxdandsxm.Bytakingsd=sxd[−x]andsm=sxm[−x],andapplyingA.15,weget:–(s,for(xinд1:д2)S′D)⇓sd–(sxd,for(xinд1:д2)S′M)⇓sm–(sxm,for(xinд1:д2)S′Q)⇓s′Asx<R(for(xinд1:д2)SM),x<R(for(xinд1:д2)S′Q),sd=sxd[−x]andsm=sxm[−x],wealsohave:–(s,for(xinд1:д2)S′D)⇓sdProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:37–(sd,for(xinд1:д2)S′M)⇓sm–(sm,for(xinд1:д2)S′Q)⇓s′Therefore,by(EvalSeq):(s,(for(xinд1:д2)S′D);(for(xinд1:д2)S′M);(for(xinд1:д2)S′Q))⇓s′,so(s,LHS)⇓s′=⇒(s,RHS)⇓s′.(2)Casen1>n2.By(EvalForFalse)s′=s.Also,(s,for(xinд1:д2)S′D)⇓s,(s,for(xinд1:д2)S′M)⇓sand(s,for(xinд1:д2)S′Q)⇓s.Soby(EvalSeq),(s,RHS)⇓s,andthus(s,LHS)⇓s′=⇒(s,RHS)⇓s′.Byassuminginsteadsands′,suchthat(s,RHS)⇓s′,andreversingthisreasoning,wealsoobtain(s,RHS)⇓s′=⇒(s,LHS)⇓s′.Therefore(s,LHS)⇓s′⇐⇒(s,RHS)⇓s′,soLHS≃RHS.(cid:3)BFURTHERDISCUSSIONONSEMANTICSB.1SemanticsofGeneratedQuantitiesInadditiontodefiningrandomvariablestobesampledusingHMC,Stanalsosupportssamplingusingpseudo-randomnumbergenerators.Forexample,astandardnormalparameterxcanbesampledintwoways:(1)Bydeclaringxtobeaparameterofthemodel,andgivingitaprior:parameters{realx;}model{x∼normal(0,1);}(2)Treatingxasageneratedquantityandusingapseudo-randomnumbergenerator:generatedquantities{x=normal_rng(0,1);}Option(1)willsamplexusingHMC,whichisnotneededinthiscase.Option(2)isamuchmoreefficientsolution.Thus,aStanusercanexplicitlyoptimisetheirprogrambyspecifyinghowHMCshouldbecomposedwithforward(ancestral)sampling.Inthedensity-basedsemanticspresentedinthispaper,wedonotformalisethisusageofpseudo-randomnumbergenerators.Wetreatthefunctionnormal_rng(mu,sigma)asanyotherfunction,ignoringitsrandomnature.WedefinethesemanticsofaStanprogramtobetheunnormalisedlogposterioroverparametersonly—logp∗(θ|D).However,thissemanticscanbeextendedtocoverthegeneratedquantitiesgaswell:logp∗(θ,g|D).Theeasiestwaytodothatistosimplytreatnormal_rngasanotherderivedform:L=d_rng(E1,...En),L∼d(E1,...En)randomnumbergenerationHowever,thiscausesadiscrepancywiththecurrentinformation-flowtypesystem.Perhapsamoresuitabletreatmentisasanassignmenttoanotherreservedvariable,whichholdsadifferentdensitytothatoftarget:L=d_rng(E1,...En),gen=gen+d_lpdf(L,E1,...En)randomnumbergenerationThedensity-basedsemanticsofaStanprogramcanthenbedefinedas:logp∗(θ,g|D)=logp∗(θ|D)+logp(g|θ,D)wherelogp∗(θ|D),asbefore,isgivenbythetargetvariable,andlogp(g|θ,D)isgivenbygen.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:38MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonAninterestingdirectionforfutureworkistoextendthesemanticsandtypesystemofSlicStan,sothatmodellingstatements,suchasx∼normal(0,1)canbetreatedeitherasmodifyingthetargetdensity,orrandomnumbergeneration,dependingonthelevelofthevariablex.ThiscanallowSlicStanprogramstobeoptimisedbyautomaticallydeterminingthemostefficientwaytocomposeHMCandforwardsampling,basedontheconcretemodel.B.2RelationofDensity-basedSemanticstoSampling-basedSemanticsThedensity-basedsemanticsofStanandSlicStangiveninthispaperisinspiredbythesampling-basedsemanticsthatHuretal.[2015]givetotheimperativelanguageProb.Thissectionoutlinesthedifferencesbetweenthetwosemantics.B.2.1Operationalsemanticsrelations.Theintuitionbehindthedensity-basedsemanticsofStanisthattherelation(s,S)⇓s′specifieswhatthevalueofthe(unnormalised)posteriorisataspecificpointintheparameterspace.Forθ⊂sandD⊂s,p∗(θ|D)=s′(target).TheintuitionbehindtheoperationalsemanticsrelationbyHuretal.[2015],(s,S)⇓t(s′,p),isthatthereisprobabilitypfortheprogramS,startedininitialstates,tosampleatracetandterminateinstates′.Forprogramswithnoobservedvalues,andsingleprobabilisticassignment(x∼d1(...);x∼d2(...)isnotallowed),weguessthatif(s,S)⇓t(s′,p),then((s∪t),S)⇓s′[target7→p],andθ=t.B.2.2Differenceinthemeaningof∼.InStan,amodelstatementsuchasx∼normal(0,1),doesnotdenotesampling,butachangetothetargetdensity.Thevalueofxremainsthesame;weonlycomputethestandardnormaldensityatthecurrentvalueofx.ThisisalsosimilartothescoreoperatorofStatonetal.[2016].OperationalDensity-basedSemanticsofModelStatements(DerivedRule)(EvalModel)(s,E)⇓V(s,Ei)⇓Vi∀i∈1..nV′=s(target)+d_lpdf(V,V1,...,Vn)(s,E∼d(E1,...,En))⇓s[target7→V′]Inthesampling-basedsemanticsofHuretal.[2015],ontheotherhand,x∼normal(0,1)isunderstoodas“wesampleastandardnormalvariableandassignitsvaluetox.”OperationalSampling-basedSemanticsofModelStatements[Huretal.2015](SamplingModel)v∈Valp=Dist(s(θ))(v)(s,x∼Dist(θ))⇓x7→[v](s[x7→v],p)Inthissampling-basedsemantics,variablescanbesampledmorethanonce,andwekeeptrackoftheentiretraceofsamples.InStan’sdensity-basedsemantics,modellingavariablemorethanoncewouldmeanmodifyingthetargetdensitymorethanonce.Forexample,considertheprogram:x∼normal(-5,1);x∼normal(5,1);Thedifferencebetweenthedensity-basedandsampling-basedsemanticsisthenasfollows:•Density-based:theprogramdenotestheunnormaliseddensityp∗(x)=N(x|−5,1)N(x|5,1).Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:39•Sampling-based:theprogramdenotestheunnormaliseddensityp∗(x(1),x(2))=N(x(1)|−5,1)N(x(2)|5,1),withx(1)andx(2)beingvariablesdenotingthevalueofxwesampledthefirstandsecondtimeintheprogramrespectively.B.2.3Differenceinthemeaningofobserve.Asmentionedpreviously,wepresumethatforsingleprobabilisticassignmentprogramsthatcontainonlyunobservedparameters,outdensity-basedsemanticsisequivalenttothesampling-basedsemanticsofHuretal.[2015].Howeverthetwosemanticstreatobservationsdifferently.ConsiderthefollowingSlicStanprogram,whereyisanobservedvariable:realmu∼normal(0,1);datarealy∼normal(mu,1);Thedensity-basedsemanticsofthisprogramisafunctionofµ:p∗(µ|y=v)=N(µ|0,1)N(v|µ,1)wherevissomeconcretevalueforthedatay,whichissuppliedexternally.ThecorrespondingProbprogramis:doublemu∼normal(0,1);doubley∼normal(mu,1);observe(y=v);Thedatavisencodedintheprogram,andthesampling-basedsemanticsisafunctionofµandy:p∗(µ,y)=(N(µ|0,1)N(y|µ,1),ify=v0,otherwiseIntuitively,theoperationalsampling-basedsemanticsdefineshowtosamplethevariablesµandy,andthenrejecttherunify,v.Thisintroducesazero-probabilityconditioningproblemwhenworkingwithcontinuousvariables,andfailsinpractise.Theoperationaldensity-basedsemanticsofthispaperputsSlicStan’sobservationsclosertotheideaofsoftconstraints.UsingthescoreoperatorofStatonetal.[2016],wecanwrite:letx∼normal(0,1)inscore(density_normal(y,(x,1));Onceagain,yhassomeconcretevaluev,andthescoreoperatorcalculatesthedensityofyatv.Statonetal.[2016]makethescoreoperatorpartoftheirmetalanguage,whilewebuilditintothedensity-basedsemanticsitself.CELABORATINGANDSHREDDINGIFORFORSTATEMENTSThissectiondemonstrateswithanexampletheelaborationandshreddingofifandforstatements.ConsiderthefollowingSlicStanprogram:datarealx;realdatad;realmodelm;if(x>0){d=2*x;m∼normal(d,1);}Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:40MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttonThebodyoftheifstatementcontainsanassignmenttoadatavariable(d=2*x),andamodelstatement(m∼normal(d,1)).TheformerbelongstothetransformeddatablockofaStanprogram,whilethelatterbelongstothemodelblock.Weneedtoshredtheentirebodyoftheifstatement,intoseveralifstatements,eachofwhichcontainsstatementsofasinglelevelonly.Firstly,theelaborationstepensuresthattheguardofeachifstatement(andtheboundsofeachforloop)isafreshbooleanvariable,g,whichisnotmodifiedanywhereintheprogram:g=(x>0)if(g){d=2*x;m∼normal(d,1);}Thentheshreddingstepcancopytheifstatementateachlevel,withoutchangingthemeaningoftheoriginalprogram:SD=g=(x>0)if(g){d=2*x;}SM=if(g){m∼normal(d,1);}SQ=skipFinally,thistranslatestotheStanprogram:data{realx;}transformeddata{reald;boolg=(x>0);if(g){d=2*x;}}parameters{realm;}model{if(g){m∼normal(d,1);}}DNON-CENTREDREPARAMETERISATIONReparameterisingamodelmeansexpressingitintermsofdifferentparameters,sothattheoriginalparameterscanberecoveredfromthenewones.ReparametrisationplaysakeyroleinoptimisingsomemodelsforMCMCinference,asitcouldtransformaposteriordistributionthatisdifficulttosamplefrominpractice,intoaflatter,easiertosamplefromdistribution.Toshowtheimportanceofonesuchreparameterisationtechnique,thenon-centredreparame-terisation,considerthepathologicalNeal’sFunnelexample,whichwaschosenby[Neal2003]todemonstratethedifficultiesMetropolis–HastingsrunsintowhensamplingfromadistributionwithProc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:41strongnon-lineardependencies.Themodeldefinesadensityovervariablesxandy,9suchthat:y∼N(0,3)x∼N(0,ey2)Thedensityhastheformofafunnel(thusthename“Neal’sFunnel”),withaverysharpneck,asshowninFigure1c.Wecaneasilyimplementthemodelinastraightforwardway(centredparameterisation),asontheleftbelow.CentredparameterisationinStanparameters{realy;realx;}model{y∼normal(0,3);x∼normal(0,exp(y/2));}Non-centredparameterisationinStanparameters{realy_std;realx_std;}transformedparameters{realy=3.0∗y_std;realx=exp(y/2)∗x_std;}model{y_std∼normal(0,1);//impliesy∼N(0,3)x_std∼normal(0,1);//impliesx∼N(0,e(y/2))}However,inthatcase,Stan’ssamplerhastroubleobtainingsamplesfromtheneckofthefunnel,becausethereexistsastrongnon-lineardependencybetweenxandy,andtheposteriorgeometryisdifficultforthesamplertoexplorewell(seeFigure1a).Alternatively,wecanreparameterisethemodel,sothatthemodelparametersarechangedfromxandytothestandardnormalparametersx(std)andy(std),andtheoriginalparametersarerecoveredusingshiftingandscaling:y(std)∼N(0,1)x(std)∼N(0,1)y=y(std)×3x=x(std)×ey2Thisreparameterisation,callednon-centredparametrisation,isaspecialcaseofamoregeneraltransformintroducedbyPapaspiliopoulosetal.[2007].Asshownontherightabove,thenon-centredmodelislongerandlessreadable.However,itperformsmuchbetterthanthecentredone.Figure1bshowsthatbyreparameterisingthemodel,weareabletoexplorethetailsofdensitybetterthanifweusethestraightforwardimplementation.Neal’sFunnelisatypicalexampleofthedependenciesthatpriorsinhierarchicalmodelscouldhave.Theexampledemonstratesthatinsomecases,especiallywhenthereislittledataavailable,usingnon-centredparameterisationcouldbevitaltotheperformanceoftheinferencealgorithm.Thecentredtonon-centredparameterisationtransformationisthereforecommontoStanmodels,andisextensivelydescribedbytheStanDevelopmentTeam[2017]asausefultechnique.EEXAMPLESE.1Neal’sFunnelWecontinuewiththeNeal’sfunnelexamplefromAppendixD,todemonstratetheusageofuser-definedfunctionsinSlicStan.Reparameterisingamodel,whichinvolvesa(centred)Gaussianvariablex∼N(µ,σ)involvesintroducinganewparameterx(std).Therefore,anon-centredreparameterisationfunctioncannotbedefinedinStan,asStanuser-definedfunctionscannotdeclarenewparameters.InSlicStan,on9Forsimplicity,weconsidera2-dimensionalversionofthefunnel,asopposedtotheoriginal10-dimensionalversion.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:42MariaI.Gorinova,AndrewD.Gordon,andCharlesSuttontheotherhand,reparameterisingthemodeltoanon-centredparameterisationcanbeimplementedbysimplycallingthefunctionmy_normal.BelowistheNeal’sfunnelinSlicStan(left)andStan(right).(a)24,000samplesobtainedusingStan(defaultset-tings)forthenon-efficientformofNeal’sFunnel.(b)24,000samplesobtainedusingStan(defaultset-tings)fortheefficientformofNeal’sFunnel.(c)TheactuallogdensityofNeal’sFunnel.Darkregionsareofhighdensity(logdensitygreaterthan−8).Fig.1Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:43“Neal’sFunnel”inSlicStan1realmy_normal(realm,reals){2realstd∼normal(0,1);3returns∗std+m;4}5realy=my_normal(0,3);6realx=my_normal(0,exp(y/2));“Neal’sFunnel”inStan1parameters{2realy_std;3realx_std;4}5transformedparameters{6realy=3.0∗y_std;7realx=exp(y/2)∗x_std;8}9model{10y_std∼normal(0,1);11x_std∼normal(0,1);12}Thenon-centredSlicStanprogram(left)isonlylongerthanitscentredversion,duetothepresenceofthedefinitionofthefunction.Incomparison,Stanrequiresdefiningthenewparametersx_stdandy_std(lines2,3),movingthedeclarationsofxandytothetransformedparametersblock(lines6,7),definingthemintermsoftheparameters(lines8,9),andchangingthedefinitionofthejointdensityaccordingly(lines12,13).Wealsopresentthe“translated”Neal’sFunnelmodel,asitwouldbeoutputtedbyanimplementedcompiler.WenoticeamajordifferencebetweenthetwoStanprograms—inonecasethevariablesofinterestxandyaredefinedinthetransformedparametersblock,whileintheothertheyaredefinedinthegeneratedquantitiesblock.Inanintuitive,centredparameterisationofthismodel,xandyareinfacttheparameters.Therefore,itismuchmorenaturaltothinkofthosevariablesastransformedparameterswhenusinganon-centredparameterisation.However,asshowninTable1,variablesdeclaredinthetransformedparametersblockarere-evaluatedateveryleapfrog,whilethosedeclaredinthegeneratedquantitiesblockarere-evaluatedateverysample.Thismeansthateventhoughitismoreintuitivetothinkofxandyastransformedparameters(originalStanprogram),declaringthemasgeneratedquantitieswherepossibleresultsinabetteroptimisedinferencealgorithminthegeneralcase.TherearesomeadvantagesoftheoriginalStancodethatthetranslatedfromSlicStanStancodedoesnothave.Theoriginalversionisconsiderablyshorterthanthetranslatedone.Thisisduetothelackoftheadditionalvariablesm,mp,s,sp,ret,andretp,whichareaconsequenceofstaticallyunrollingthefunctioncallsintheelaborationstep.WhenusingSlicStan,theproducedStanprogramactsasanintermediaterepresentationoftheprobabilisticprogram,meaningthatthereducedreadabilityofthetranslationisnotnecessarilyproblematic.However,thepresenceoftheadditionalvariablescanalso,insomecases,leadtoslowerinference.Thisproblemcanbetackledbyintroducingstandardoptimisingcompilerstechniques,suchasvariableandcommonsubexpressionelimination.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:44MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton“Neal’sFunnel”translatedtoStan1transformeddata{2realm;3realmp;4m=0;5mp=0;6}7parameters{8realx_std;9realx_stdp;10}11model{12x_std∼normal(0,1);13xr_stdp∼normal(0,1);14}15generatedquantities{16reals;17realsp;18realret;19realretp;20realx;21realy;22s=3;23ret=s*x_std+m;24y=ret;25sp=exp(y*0.5);26retp=(sp*x_stdp+mp);27x=retp;28}Moreover,wenoticethenamesofthenewparametersinthetranslatedcode:x_stdandx_stdp.Thenamesareimportant,astheyarepartoftheoutputofthesamplingalgorithm.UnlikeStan,withtheuser-defined-functionversionofNeal’sfunnel,inSlicStantheprogrammerdoesnothavecontrolonthenamesofthenewlyintroducedparameters.Onecanarguethattheuserwasnotinterestedinthoseparametersinthefirstplace(astheyaresolelyusedtoreparameterisethemodelformoreefficientinference),soitdoesnotmatterthattheirnamesarenotdescriptive.However,iftheuserwantstodebugtheirmodel,theoutputfromtheoriginalStanmodelwouldbemoreusefulthanthatofthetranslatedone.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:45E.2CockroachesThe“Cockroaches”exampleisdescribedbyGelmanandHill[2007,p.161],anditconcernsmeasur-ingtheeffectsofintegratedpestmanagementonreducingcockroachnumbersinapartmentblocks.TheyusePoissonregressiontomodelthenumberofcaughtcockroachesyiinasingleapartmenti,withexposureui(thenumberofdaysthattheapartmenthadcockroachtrapsinit),andregressionpredictors:•thepre-treatmentcockroachlevelri;•whethertheapartmentisinaseniorbuilding(restrictedtotheelderly),si;and•thetreatmentindicatorti.Inotherwords,withβ0,β1,β2,β3beingtheregressionparameters,wehave:yi∼Poisson(uiexp(β0+β1ri+β2si+β3ti))Afterspecifyingtheirmodelthisway,GelmanandHillsimulateareplicateddatasetyrep,andcompareittotheactualdataytofindthatthevarianceofthesimulateddatasetismuchlowerthanthatoftherealdataset.Instatistics,thisiscalledoverdispersion,andisoftenencounteredwhenfittingmodelsbasedonasingleparameterdistributions,10suchasthePoissondistribution.Abettermodelforthisdatawouldbeonethatincludesanoverdispersionparameterλthatcanaccountforthegreatervarianceinthedata.Thenextpageshowsthe“Cockroach”examplebefore(ignoringtheredlines)andafter(as-sumingtheredlines)addingtheoverdispersionparameter,inbothSlicStan(left)andStan(right).Similarlytobefore,SlicStangivesusmoreflexibilityastowherethestatementsaccountingforoverdispersioncanbeadded.Stan,ontheotherhand,introducesanentirelynewtothisprogramblock—transformedparameters.10Inadistributionspecifiedbyasingleparameterα,themeanandvariancebothdependonα,andarethusnotindependent.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:46MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton“Cockroaches”inSlicStan1dataintN;2datareal[N]exposure2;3datareal[N]roach1;4datareal[N]senior;5datareal[N]treatment;67real[N]log_expo=log(exposure2);89real[4]beta;1011realtau∼gamma(0.001,0.001);12realsigma=1.0/sqrt(tau);13real[N]lambda∼normal(0,sigma);1415dataint[N]y16∼poisson_log11(log_expo+beta[1]17+beta[2]*roach118+beta[3]*treatment19+beta[4]*senior20+lambda);“Cockroaches”inStan1data{2intN;3real[N]exposure2;4real[N]roach1;5real[N]senior;6real[N]treatment;7inty[N];8}9transformeddata{10real[N]log_expo=log(exposure2);11}12parameters{13real[4]beta;14real[N]lambda;15realtau;16}17transformedparameters{18realsigma=1.0/sqrt(tau);19}20model{21tau∼gamma(0.001,0.001);22lambda∼normal(0,sigma);23y∼poisson_log11(log_expo+beta[1]24+beta[2]*roach125+beta[3]*treatment26+beta[4]*senior27+lambda);28}Exampleadaptedfromhttps://github.com/stan-dev/example-models/.11Stan’spoisson_logisanumericallystablewaytomodelaPoissonvariablewheretheeventrateiseαforsomeα.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.ProbabilisticProgrammingwithDensitiesinSlicStan:Efficient,Flexible,andDeterministic35:47E.3SeedsNext,wetakethe“Seeds”exampleintroducedbyLunnetal.[2013,p.300]in“TheBUGSBook”.Inthisexample,wehaveIplates,withplateihavingatotalofNiseedsonit,niofwhichhavegerminated.Moreover,eachplateihasoneof2typesofseedsx(i)1,andoneof2typesofrootextractx(i)2.Weareinterestedinmodellingthenumberofgerminatedseedsbasedonthetypeofseedandrootextract,whichwedointwosteps.Firstly,wemodelthenumberofgerminatedseedswithaBinomialdistribution,whosesuccessprobabilityistheprobabilityofasingleseedgerminating:ni∼Binomial(N,pi)Wemodeltheprobabilityofasingleseedonplateigerminatingastheoutputofalogisticregressionwithinputvariablesthetypeofseedandrootextract:pi=σ(α0+α1x(i)1+α2x(i)2+α12x(i)1x(i)2+β(i))Intheabove,α0,α1,α2,α12andβ(i)areparametersofthemodel,withβ(i)allowingforover-dispersion(see§§E.2).Thenextpageshowsthe“Seeds”modelwritteninSlicStan(left)andinStan(right).TheStancodewasadaptedfromtheexamplemodelslistedonStan’sGitHubpage.Asbefore,weseethatSlicStan’scodeisshorterthanthatofStan.Italsoallowsformoreflexibilityintheorderofdeclarationsanddefinitions,makingitpossibletokeeprelatedstatementstogether(e.g.lines14and15oftheexamplewritteninSlicStan).Onceagain,SlicStanprovidesmoreabstraction,astheprogrammerdoesnothavetospecifyhoweachvariableofthemodelshouldbetreatedbytheunderlyinginferencealgorithm.InsteaditautomaticallydeterminesthiswhenittranslatestheprogramtoStan.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.35:48MariaI.Gorinova,AndrewD.Gordon,andCharlesSutton“Seeds”inSlicStan1dataintI;2dataint[I]n;3dataint[I]N;4datareal[I]x1;5datareal[I]x2;67real[I]x1x2=x1.*x2;89realalpha0∼normal(0.0,1000);10realalpha1∼normal(0.0,1000);11realalpha2∼normal(0.0,1000);12realalpha12∼normal(0.0,1000);1314realtau∼gamma(0.001,0.001);15realsigma=1.0/sqrt(tau);1617real[I]b∼normal(0.0,sigma);18n∼binomial_logit12(N,alpha019+alpha1*x120+alpha2*x221+alpha12*x1x222+b);“Seeds”inStan1data{2intI;3intn[I];4intN[I];5real[I]x1;6real[I]x2;7}89transformeddata{10real[I]x1x2=x1.*x2;11}12parameters{13realalpha0;14realalpha1;15realalpha12;16realalpha2;17realtau;18real[I]b;19}20transformedparameters{21realsigma=1.0/sqrt(tau);22}23model{24alpha0∼normal(0.0,1000);25alpha1∼normal(0.0,1000);26alpha2∼normal(0.0,1000);27alpha12∼normal(0.0,1000);28tau∼gamma(0.001,0.001);2930b∼normal(0.0,sigma);31n∼binomial_logit12(N,alpha032+alpha1*x133+alpha2*x234+alpha12*x1x235+b);36}Exampleadaptedfromhttps://github.com/stan-dev/example-models/.12Stan’sbinomial_logitdistributionisanumericallystablewaytousealogisticsigmoidincombinationwithaBinomialdistribution.Proc.ACMProgram.Lang.,Vol.3,No.POPL,Article35.Publicationdate:January2019.94

Chapter 4. SlicStan

4.2 Clarifying the contributions

Author contributions. The authors of the paper are myself, Andy Gordon and Charles

Sutton. Andy Gordon conceived the idea about using information ﬂow analysis to

slice a SlicStan program into Stan blocks and suggested using operational semantics to

formalise Stan and SlicStan. Both Andy Gordon and Charles Sutton supervised the

project throughout, gave comments and suggestions, and helped write and revise the

paper. As a leading author, my contribution included working out the details of the

Stan/SlicStan semantics and type system, implementing SlicStan, deriving the proofs in

the paper, preparing code examples, and co-writing the paper.

Similarities to previous work. The SlicStan project started before the onset of this

PhD program. Part of the work presented in the paper, and thus in this chapter, was

also presented in the Probabilistic Programming with SlicStan MSc dissertation (Gorinova,

2017). In particular, the MSc dissertation included an initial version of SlicStan’s syntax

and information-ﬂow type system (

3.1 and

3.2 of the paper), as well as an elaboration

§§
and transformation procedures for this reduced version of SlicStan (

§§

3.3 and

4.2 of

§§

§§

the paper). While similar in spirit to the ﬁnal paper, these parts of the MSc dissertation

concerned only a subset of SlicStan’s ﬁnal syntax and were signiﬁcantly developed, as

clariﬁed below. Finally, the comparison between Stan and SlicStan (
§

from the paper) was largely adapted from the MSc dissertation.

5 and Appendix E

Contributions of this chapter. The SlicStan paper, and thus this chapter, contain

the following changes and contributions that are new compared to the MSc dissertation:

• The syntax of SlicStan was extended to include conditional statements and for-loops.
The new syntax also omits having separate symbol for distributions, making the

calculus more concise. It treats terms such as E

d(E1, . . . , En) as derived forms
rather than as part of the core calculus. (Although, the last change was reversed in

∼

Chapter 5.)

• The type system presented in the MSc dissertation contained an oversight, which

made it diﬃcult to correctly slice SlicStan programs such as:

real x = 0;

normal(x, 1);

real y

x = 1;

∼

In the initial, MSc dissertation version of SlicStan, this program would type-check

4.2. Clarifying the contributions

95

when x is of level data and y is of level model. However, such type level assignment

would result in the following Stan program:

transformed data {

real x = 0;

x = 1;

}

parameters { real y; }

model { y

∼

normal(x, 1); }

As blocks in Stan are executed in order, the meaning of the program has changed

in the translation: the statement x = 1 must appear after y

normal(x, 1), but

appears before it. This is not a problem of the translation procedure, but a type
problem. It is not possible for x to be of level data and for the statements to be in
the correct order, as any transformation of a variable of level data must appear in the
transformed parameters block, which appears strictly before model-level statements

∼

such as y

∼

normal(x, 1).

This chapter corrects the above problem by ensuring that such programs fail to

type-check. It introduces the concept of shreddable sequence (Deﬁnition 4.7 of the

paper), so that variables become immutable once they are read at a higher level

than their own. In the paper, the program above fails to type-check if x is of level
data and y is of level model, but type-checks for x and y both of level model.

• Extending SlicStan with conditionals and loops required a more careful look at the
rules for translating SlicStan to Stan. For example, conditional statements that

contain sub-statements at several levels, need to be sliced by copying the guard of

the if-statement per sub-statement level:

SlicStan

data bool g;

data real x;

model real y;

if (g) {

x = 1;

Stan

data { bool g; }

transformed data {

real x;

if (g) { x = 1; }

else { x = -1; }

normal(x, 1);

}

y

∼

}

else {

x = -1;

normal(x, 2);

y

∼

}

parameters { real y; }

model {

if (g) { y

else { y

∼

normal(x, 1); }

∼
normal(x, 2); }

}

96

Chapter 4. SlicStan

Such slicing was formalised through the new to the paper shredding relation (

§§

4.1).

• The biggest contribution of the paper presented in this chapter that is new compared
to the MSc thesis is the formalised density-based semantics of Stan and SlicStan.

This includes most of

2 and

§

§

3.4, which are entirely new to the paper.

• Additionally, the paper utilises this formal semantics by stating and proving several
key results, most notably the semantic preservation of the transformation that

translates SlicStan programs to Stan programs (Theorem 4.10 of the paper).

4.3

Impact

The idea of a “blockless Stan” and using information-ﬂow to achieve it has inﬂuenced

several lines of work. Baudart et al. (2019) describe a new probabilistic programming

language, Yaps, which is inspired by SlicStan to allow for a concise Python-based frontend

for Stan. In addition, the density-based semantics of Stan that the paper presents has been

used as a basis to formalise a procedure for translating Stan to generative (implicit) PPLs

(Baudart et al., 2021). AQUA (Huang et al., 2021) is a PPL that uses symbolic inference

and quantization of the probability density, whose semantics has also been inspired by

that of SlicStan. The SlicStan paper is also one of the ﬁrst to utilise static analysis for

probabilistic programming to improve inference, as summarised by Bernstein (2019).

While the paper presented here treats the program slicing as a way to divide the program

into pre-inference, post-inference and inference, there is an alternative interpretation of the

three slices. In Stan, the post-processing code, which is placed in the generated quantities

block, can include calls to pseudo-random number generation functions. The program as a

whole can then be seen as inferring some variables (those deﬁned in the parameters block)

via HMC, and others (those drawn with pseudo-random number generators) via ancestral

sampling. This idea is further discussed and developed in the next chapter.

Notably, SlicStan has been a point of discussion when it comes to the future of Stan. The

Stan++/Stan3 Preliminary Design discussion (Stan Development Team, 2018) features Slic-

Stan and suggests adapting the information-ﬂow approach to allow for more compositional

feature version of Stan. However, this plan has not been made oﬃcial or realised.

Errata

The example on page 17 of the paper is a valid SlicStan program that compiles. However,

running it would result in a runtime error, as the variable d is used before it is deﬁned.

The program on page 24 has both variables x mean and x meas. Read both as x mean.

CHAPTER 5

Conditional independence by typing

Variable elimination for inference with discrete parameters

Stan has often been criticised for its lack of explicit support for discrete parameters, which

is a consequence of using gradient-based algorithms for inference. However, a workaround

is possible, where the user can manually sum out any discrete parameters, thus encoding

them in an implicit way. This chapter builds upon the information-ﬂow analysis ideas

of Chapter 4 and presents a type system for conditional independence analysis, which

can be used to automate the process of discrete variable marginalisation in SlicStan. The

main contribution of the chapter is the paper Conditional Independence by Typing (§ 5.1).
While focused on conditional independence and variable elimination, this work can be seen

as an instance of a more general framework for static analysis of probabilistic programs,

which I brieﬂy discuss in § 5.2.

5.1 The paper

This section presents the work Conditional Independence by Typing. The paper builds upon

the original SlicStan work and presents an information-ﬂow type system that captures

conditional independence relationships in probabilistic programs. The paper shows how

certain conditional independence relationships can be automatically deduced using type-

inference. It presents one practical application of the type system: a semantic-preserving

transformation, which can transform SlicStan programs with discrete parameters in a way

that allows for eﬃcient gradient-based inference.

The paper was accepted for publication at the ACM Transactions on Programming

Languages and Systems, Volume 44, Issue 1 (TOPLAS 2022).

97

98

Chapter 5. Conditional independence by typing

Author contributions. The paper is co-authored by me, Andy Gordon, Charles Sutton

and Matthijs V´ak´ar. As a leading author, my contributions included conceiving the idea of

using information-ﬂow for conditional independence analysis, working out the details of the

conditional independence type system and the program transformation for marginalising

discrete variables, implementing the analysis within SlicStan, deriving some of the proofs

in the paper, preparing and performing the experiments, and co-writing the paper. Both

Andy Gordon and Charles Sutton supervised the project throughout, gave comments and

suggestions, and co-wrote and revised the paper. Matthijs V´ak´ar contributed to adapting

SlicStan’s semantics and typing rules to cover generated quantities, helped derive some of

the deﬁnitions and proofs, and co-wrote parts of the paper.

4ConditionalindependencebytypingMARIAI.GORINOVA,UniversityofEdinburghANDREWD.GORDON,MicrosoftResearchandUniversityofEdinburghCHARLESSUTTON,UniversityofEdinburghMATTHIJSVÁKÁR,UtrechtUniversityAcentralgoalofprobabilisticprogramminglanguages(PPLs)istoseparatemodellingfrominference.However,thisgoalishardtoachieveinpractice.Usersareoftenforcedtore-writetheirmodelsinordertoimproveefficiencyofinferenceormeetrestrictionsimposedbythePPL.Conditionalindependence(CI)relationshipsamongparametersareacrucialaspectofprobabilisticmodelsthatcaptureaqualitativesummaryofthespecifiedmodelandcanfacilitatemoreefficientinference.Wepresentaninformationflowtypesystemforprobabilisticprogrammingthatcapturesconditionalindependence(CI)relationships,andshowthat,forawell-typedprograminoursystem,thedistributionitimplementsisguaranteedtohavecertainCI-relationships.Further,byusingtypeinference,wecanstaticallydeducewhichCI-propertiesarepresentinaspecifiedmodel.Asapracticalapplication,weconsidertheproblemofhowtoperforminferenceonmodelswithmixeddiscreteandcontinuousparameters.InferenceonsuchmodelsischallenginginmanyexistingPPLs,butcanbeimprovedthroughaworkaround,wherethediscreteparametersareusedimplicitly,attheexpenseofmanualmodelre-writing.Wepresentasource-to-sourcesemantics-preservingtransformation,whichusesourCI-typesystemtoautomatethisworkaroundbyeliminatingthediscreteparametersfromaprobabilisticprogram.Theresultingprogramcanbeseenasahybridinferencealgorithmontheoriginalprogram,wherecontinuousparameterscanbedrawnusingefficientgradient-basedinferencemethods,whilethediscreteparametersareinferredusingvariableelimination.WeimplementourCI-typesystemanditsexampleapplicationinSlicStan:acompositionalvariantofStan.1ACMReferenceFormat:MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár.2021.Conditionalindependencebytyping.ACMTrans.Program.Lang.Syst.44,1,Article4(December2021),54pages.https://doi.org/10.1145/34904211INTRODUCTIONThenumberofprobabilisticprogramminglanguages(PPLs)hasgrownfarandwide,andsohastherangeofinferencetechniquestheysupport.Somefocusonproblemsthatcanbesolvedanalytically,andprovideasymbolicsolution[Gehretal.2016],othersareveryflexibleinthemodelstheycanexpressandusegeneral-purposeinferencealgorithms[Woodetal.2014].Someusegradient-basedmethods[Carpenteretal.2017],ormessage-passingmethods[Minkaetal.2014]toprovideanefficientsolutionatthecostofrestrictingtherangeofexpressibleprograms.Eachoptionpresentsitsownchallenges,whetherintermsofspeed,accuracyorinferenceconstraints,whichiswhyPPLusersoftenarerequiredtolearnasetofmodelre-writingtechniques:tobeabletochangetheprogramuntilitcanbefeasiblyusedwithinthebackendinferencealgorithm.TakeforexampleStan[Carpenteretal.2017],whichisusedbypractitionersinawiderangeofsciencesandindustriestoanalysetheirdatausingBayesianinference.Whileefficientinferencealgorithmsexistforcontinuous-onlyandforsomediscrete-onlymodels,itismuchlessclearwhatalgorithmtouseforarbitrarymodelswithlargenumbersofbothdiscreteandcontinuous1Theimplementationisavailableathttps://github.com/mgorinova/SlicStan.2021.Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Notforredistribution.ThedefinitiveVersionofRecordwaspublishedinACMTransactionsonProgrammingLanguagesandSystems,https://doi.org/10.1145/3490421.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:2MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár(latent,i.e.,unobserved)parameters.Stanhasmadeaconsciouschoicenottosupportprobabilisticmodelswithdiscreteparameters,soastoperforminferenceusing(dynamic)HamiltonianMonteCarlo(HMC)[BetancourtandGirolami2015;HoffmanandGelman2014;Nealetal.2011]),whichprovidesefficient,gradient-basedinferencefordifferentiablemodels.Asaresult,Stanhasoftenbeencriticised[Gelmanetal.2015]foritslackofsupportfordiscreteparameters.Whatisusuallyoverlookedisthatmanymodelswithdiscreteparameterscan,infact,beaccommodatedinStan,bymanuallymarginalising(summing)outthediscreteparametersanddrawingthemconditionallyonthecontinuousparameters[StanDevelopmentTeam2019b,Chapter7].Oneofthecoremodelrewritingtechniquesismarginalisation:summingoverallpossiblevaluesthatarandomvariablecantaketoobtainamarginaldensityfunctionthatdoesnotinvolvethatvariable.Marginalisingefficientlyisnotalwaysanobviousprocedure,asitrequiresexploitingconditionalindependencerelationshipsamongthevariablesinthemodel.Forprobabilisticgraphicalmodels,therearewell-knownalgorithmsforenumeratingalloftheconditionalindependenceassumptionsimpliedbyamodel.Butprobabilisticprogramsaremuchmoregeneral,includingcontrolflowandassignment.Forthismoregeneralcase,itismuchlessclearhowtodetermineconditionalindependencerelationshipsautomatically,anddoingsorequirescombiningideasfromtraditionalprogramanalysisandfromprobabilisticgraphicalmodelling.Inthispaper,weintroduceaninformationflowtypesystemthatcandeduceconditionalinde-pendencerelationshipsbetweenparametersinaprobabilisticprogram.Findingsuchrelationshipscanbeusefulinmanyscenarios.Asanexampleapplication,weimplementasemantics-preservingsource-to-sourcetransformationthatautomaticallymarginalisesdiscreteparameters.WeworkinSlicStan[Gorinovaetal.2019],aformofStanwithamorecompositionalsyntaxthantheoriginallanguage.OursystemextendsSlicStantosupportdiscreteparametersinthecasewhenthediscreteparameterspaceisbounded.Thistransformcorrespondstothevariableeliminationalgorithm[KollerandFriedman2009;ZhangandPoole1994]:anexactinferencealgorithm,efficientinmodelswithsparsestructure.Combiningthistransformationwithanefficientalgorithmforcontinuousparameters,likeHMC,givesusamodel-specific,automaticallyderivedinferencestrategy,whichisacompositionofvariableeliminationandthealgorithmofchoice.Whileweonlyfocusononeapplicationinthispaper,ourtypesystemforconditionalindependenceisapplicabletoprogramtransformationsofprobabilisticprogramsmoregenerally,andwebelieveitcanenableothercomposed-inferencestrategies.Inshort,wemakethefollowingcontributions:(1)FactorisedsemanticsforSlicStan:Asabasisforprovingcorrectnessofourtransformation,weextendSlicStan’stypesystem,sothatshredding(whichslicesaSlicStanprogramintoStanforexecution)correctlyseparateswell-typedprogramsintodatapreprocessing,mainmodel,andpurelygenerativecode(Theorem1).(2)Maintheoreticalresult:Weshowhowaverysimple,relativelystandardinformationflowtypesystemcanbeusedtocaptureaconditionalindependenceinprobabilisticprograms(§3)andestablishacorrespondencebetweenwell-typedprogramsandconditionalindependencepropertiesoftheprobabilitydistributionitimplements(Theorem2,Theorem3).(3)Mainpracticalresult:Wedescribeandimplement(inSlicStan)asource-to-sourcetransfor-mationthatrepeatedlyusestheresultfrom(2)toefficientlymarginaliseoutthediscreteparametersoftheprogram,andwegiveagenerativeprocedurefordrawingtheseparameters(§4),thusautomatinginferenceformixeddiscrete-continuousmodels.Weprovethatourtransformationissemantics-preserving(Theorem4).ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:3//Stantargetprogramdata{realx;}parameters{real𝜇;}model{x∼normal(𝜇,1);}generatedquantities{realx_pred=normal_rng(𝜇,1);}//SlicStanfromPOPL’19real𝜇;datarealx∼normal(𝜇,1);realx_pred=normal_rng(𝜇,1);//ExtendedSlicStanreal𝜇;datarealx∼normal(𝜇,1);realx_pred∼normal(𝜇,1);Fig.1.ExampleofdifferencetopreviousversionofSlicStan2SLICSTAN:EXTENDEDSYNTAXANDSEMANTICSSlicStan[Gorinovaetal.2019]isaStan-likeprobabilisticprogramminglanguage.ComparedtoStan,itprovidesextracompositionalitybydroppingtherequirementthatprogramsbeblock-structured.SlicStanusestypeinferenceinaninformation-flowtypesystem[Abadietal.1999;Gordonetal.2015;Volpanoetal.1996]toautomaticallyrearrangetheprogramintopartsroughlycorrespondingtotheblockstructureofStan:pre-processing(data),model,andpost-processing(generatedquantities).Originally,thisshreddingwasdevelopedtocompileSlicStantoStan.Inthispaper,weshowthatitcanbeused,moregenerally,toautomaticallycompiletoanefficientprogram-specificinferencescheme.LikeStan,SlicStanisimperativeandallowsfordeterministicassignment,for-loops,if-statements,probabilisticassignment,andfactor-statements.OnecontributionofthisworkisthatwepresentanupdatedversionofSlicStan.AkeydifferencetotheoriginalversionofSlicStanisthetreatmentofsampling(∼)statements.IntheoriginalSlicStanpaper[Gorinovaetal.2019],astatementsuchas𝑥∼N(0,1)wasunderstoodsimplyasasyntacticsugarforfactor(N(𝑥|0,1)):addingafactortotheunderlyingdensityofthemodel,ratherthanperformingactualsampling.InourupdatedversionofSlicStan,samplingstatementsarepartofthecoresyntax.Thesemanticsof𝑥∼N(0,1)remainsequivalenttothatoffactor(N(𝑥|0,1))intermsofdensitysemantics,howeveritcouldbeimplementeddifferentlydependingonthecontext.Inparticular,𝑥∼N(0,1)couldbeimplementedasasimplecalltoarandomnumbergeneratorinStan,𝑥=N𝑟𝑛𝑔(0,1),likeintheexampleinFigure1.Thiswayoftreating∼statementsdifferentlyisuseful,asitallowsforanincreaseofthefunc-tionalityoftheSlicStan’sinformation-flowanalysis.Consider,forexampletheSlicStanprogramontherightofFigure1.Usingtheoriginaltypesystem,both𝜇and𝑥predwillbeoflevelmodel,astheyarebothinvolvedina∼statement.Thus,whentranslatedtoStan,both𝜇and𝑥predmustbeinferredwithHMC(oranothersimilaralgorithm),whichisexpensive.However,theupdatedtypesystemofthispaperallowsfor𝑥predtobeoflevelgenqant,whichispreferable:inthecontextofStan,thismeansonly𝜇needstobeinferredwithHMC,while𝑥predcanbesimplydrawnusingarandomnumbergenerator.Moregenerally,theupdatedSlicStantypesystemallowsforfactorisingthedensitydefinedbytheprogram:fordataD,parameters𝜽andgeneratedquantities𝑄,aprogramdefiningadensity𝑝(D,𝜽,𝑄)canbeslicedintotwoprogramswithdensities𝑝(D,𝜽)and𝑝(𝑄|D,𝜽)respectively(Theorem1).Theparameters𝜽areinferredusingHMC(oranothergeneral-purposeinferencealgorithm)accordingto𝑝(D,𝜽),whilethequantities𝑄aredirectlygeneratedaccordingto𝑝(𝑄|D,𝜽).Treating∼statementsdifferentlybasedoncontextisverysimilarinspirittoexistingeffect-handlingbasedPPLs[MooreandGorinova2018]likeEdward2andPyro,where∼canbehandledindifferentways.However,inourcase,thisdifferenceintreatmentisdeterminedstatically,automatically,andonlyinthetranslationtoStanoranotherbackend.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:4MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárAnotherdifferencebetweenGorinovaetal.[2019]’sSlicStanandourupdatedversionisthetarget(𝑆)expression,whichweusetocapturelocallythedensitydefinedbystatements.Thesechangesareasmallbutusefulcontributionofthecurrentwork:theyarekeytoallowingustodecomposetheprogramandcomposedifferentinferencestrategiesforefficiency.Intherestofthissection,wegivetheupdatedformalsyntax,typingandsemanticsofSlicStananddescribeshredding—theprocedurekeytothetranslationofStan/inferencecomposition.2.1SyntaxSlicStanhasthefollowingtypes,programs,L-values,statements,andexpressions.Wehighlightthedifferencewith[Gorinovaetal.2019]withboxes.SlicStanTypes:ℓ::=data|model|genqantleveltype𝑛∈Nsize𝜏::=real|int|int⟨𝑛⟩|𝜏[]basetype𝑇::=(𝜏,ℓ)typeSlicStanProgram:𝑃::=Γ,𝑆programSlicStanL-Values:𝐿::=𝑥[𝐸1]···[𝐸𝑛]L-valueSlicStanTypingEnvironments:Γ::={𝑥1↦→𝑇1,...,𝑥𝑛↦→𝑇𝑛}typingenvironmentSlicStanStatements:𝑆::=statement𝐿=𝐸assignment𝑆1;𝑆2sequencefor(𝑥in𝐸1:𝐸2)𝑆forloopif(𝐸)𝑆1else𝑆2ifstatementskipskipfactor(E)factorstatement𝐿∼𝑑(𝐸1,...,𝐸𝑛)samplestatementSlicStanExpressions:𝐸::=expression𝑥variable𝑐constant[𝐸1,...,𝐸𝑛]array𝐸1[𝐸2]arrayelement𝑓(𝐸1,...,𝐸𝑛)functioncall[𝐸|𝑥in𝐸1:𝐸2]arraycomprehensiontarget(S)evaluatingadensitySlicStanprogramsconsistofapairΓ,𝑆ofatypingenvironmentΓ(afinitemapthatassignsglobalvariables𝑥totheirtypes𝑇)andastatement𝑆.FollowingtheusualstyleofdeclaringvariablesinC-likelanguages,weinformallypresentprogramsΓ,𝑆inexamplesbysprinklingthetypedeclarationsofΓthroughoutthestatement𝑆.Forexample,wewritedatareal𝑥∼normal(0,1)fortheprogram{𝑥↦→(real,data)},𝑥∼normal(0,1).Sometimes,wewillleaveouttypesorwriteincompletetypesinourexamples.Inthiscase,weintendforthemissingtypestobedeterminedusingtypeinference.Aswediscussindetailin§§2.3,afactor(𝐸)statementcanbereadasmultiplyingthecurrentweight(contributiontothemodel’sjointdensity)oftheprogramtracebythevalueof𝐸.Conversely,atarget(𝑆)expressioninitialisestheweightto1andreturnstheweightthatisaccumulatedafterevaluating𝑆.Forexample,if:𝑆=x∼normal(0,1);y=2*x;z∼normal(y,1);=factor(normal_pdf(x|0,1));y=2*x;factor(normal_pdf(z|y,1));Thentarget(𝑆)issemanticallyequivalenttonormal_pdf(x|0,1)*normal_pdf(z|2*x,1).Weextendthebasetypesofthelanguageof[Gorinovaetal.2019]withint⟨𝑛⟩,whichdenotesapositiveintegerconstrainedfromabovebyaninteger𝑛.Forexampleif𝑥isoftypeint⟨2⟩,thenACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:5𝑥canonlybe1or2.Thesetypesallowustospecifythesupportofdiscretevariables,andtheycaneasilybeextendedtoincludebothupperandlowerbounds.Forthepurposeofourtypingrules,wetreatint⟨𝑛⟩identicallytoint.Weonlydifferentiatebetweenthesetypesin§4,whereourtransformationusesthesizeannotationtoeliminateadiscretevariable.2.2TypingTypes𝑇inSlicStanrangeoverpairs(𝜏,ℓ)ofabasetype𝜏,andaleveltypeℓ.Theleveltypesℓformalattice({data,model,genqant},≤),wheredata≤model≤genqant.Wewriteˆ𝑛𝑖=1ℓ𝑖fortheleastupperboundofthelevelsℓ1,...,ℓ𝑛.Wecallvariablesofleveldatadata(variables),oflevelmodelmodelparameters,andoflevelgenqantgeneratedquantities.Werefertovariablesthatareeitheroflevelmodelorgenqantsimplyasparameters.GivenatypingenvironmentΓ,wecanconsiderthewell-typednessofexpressionsandstatements,giventhetypesassignedtovariablesbyΓ.ThejudgmentΓ⊢𝐸:(𝜏,ℓ)meansthatexpression𝐸hastype𝜏andreadsonlylevelℓandbelow.ThejudgmentΓ⊢𝑆:ℓmeansthatstatement𝑆assignsonlytolevelℓandabove.WewriteΓ⊢𝑆asashorthandforΓ⊢𝑆:data.Thetypingrulesforexpressionsarethoseof[Gorinovaetal.2019]withaddedrulesforthetwoconstructsofarraycomprehensionsandtarget(𝑆)-expressions.Thetypingrulesforstatementsareasin[Gorinovaetal.2019],withthreedifferences(highlightedinboxes).(Factor)and(Sample)addtypingrulesforthenownewlanguageconstructsfactor(𝐸)and𝐿∼𝑑(𝐸1,...,𝐸𝑛).Thelan-guagesupportsafinitenumberofbuilt-infunctions𝑓withtype𝜏1,...,𝜏𝑛→𝜏and(conditional)distributions𝑑∈Dist(𝜏1,...,𝜏𝑛;𝜏)over𝜏givenvaluesoftypes𝜏1,...,𝜏𝑛.TypingRulesforExpressions:(ESub)Γ⊢𝐸:(𝜏,ℓ)ℓ≤ℓ′Γ⊢𝐸:(𝜏,ℓ′)(Var)Γ,𝑥:𝑇⊢𝑥:𝑇(Const)ty(𝑐)=𝜏Γ⊢𝑐:(𝜏,data)(PrimCall)(𝑓:𝜏1,...,𝜏𝑛→𝜏)Γ⊢𝐸𝑖:(𝜏𝑖,ℓ𝑖)∀𝑖∈1..𝑛Γ⊢𝑓(𝐸1,...,𝐸𝑛):(𝜏,ˆ𝑛𝑖=1ℓ𝑖)(ArrEl)Γ⊢𝐸1:(𝜏[],ℓ)Γ⊢𝐸2:(int,ℓ)Γ⊢𝐸1[𝐸2]:(𝜏,ℓ)(Target)Γ⊢𝑆:ℓ′′∀ℓ′>ℓ.𝑅Γ⊢ℓ′(𝑆)=∅2Γ⊢target(𝑆):(real,ℓ)(Arr)Γ⊢𝐸𝑖:(𝜏,ℓ)∀𝑖∈1..𝑛Γ⊢[𝐸1,...,𝐸𝑛]:(𝜏[],ℓ)(ArrComp)Γ⊢𝐸1:(int,ℓ)Γ⊢𝐸2:(int,ℓ)Γ,𝑥:(int,ℓ)⊢𝐸:(𝜏,ℓ)𝑥∉dom(Γ)Γ⊢[𝐸|𝑥in𝐸1:𝐸2]:(𝜏[],ℓ)TypingRulesforStatements:(SSub)Γ⊢𝑆:ℓ′ℓ≤ℓ′Γ⊢𝑆:ℓ(Assign)3Γ(𝐿)=(𝜏,ℓ)Γ⊢𝐸:(𝜏,ℓ)Γ⊢(𝐿=𝐸):ℓ(If)Γ⊢𝐸:(real,ℓ)Γ⊢𝑆1:ℓΓ⊢𝑆2:ℓΓ⊢if(𝐸)𝑆1else𝑆2:ℓ(Seq)Γ⊢𝑆1:ℓΓ⊢𝑆2:ℓS(𝑆1,𝑆2)∧G(𝑆1,𝑆2)Γ⊢(𝑆1;𝑆2):ℓ(Factor)Γ⊢𝐸:(real,model)Γ⊢factor(𝐸):model(Skip)Γ⊢skip:ℓ2Weuseℓ′>ℓasashorthandforℓ≤ℓ′∧¬ℓ′≤ℓACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:6MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár(Sample)3(𝑑∈Dist(𝜏1,...,𝜏𝑛;𝜏))Γ(𝐿)=(𝜏,ℓ′)Γ⊢𝐸𝑖:(𝜏𝑖,ℓ),∀𝑖∈1..𝑛ℓ=ℓ′⊔modelΓ⊢𝐿∼𝑑(𝐸1,...,𝐸𝑛):ℓ(For)Γ⊢𝐸1:(int,ℓ)Γ⊢𝐸2:(int,ℓ)Γ,𝑥:(int,ℓ)⊢𝑆:ℓ𝑥∉dom(Γ)𝑥∉𝑊(𝑆)Γ⊢for(𝑥in𝐸1:𝐸2)𝑆:ℓIntheserules,wemakeuseofthefollowingnotation(seeAppendixAforprecisedefinitions).•𝑊(𝑆):thesetofvariables𝑥thathavebeenassignedtoin𝑆.•𝑅Γ⊢ℓ(𝑆):thesetofvariables𝑥thatarereadatlevelℓin𝑆.•𝑊Γ⊢ℓ(𝑆):thesetofvariables𝑥oflevelℓthathavebeenassignedtoin𝑆.•e𝑊Γ⊢ℓ(𝑆):thesetofvariables𝑥oflevelℓthathavebeen∼-edin𝑆.•𝑊e𝑊Γ⊢ℓ(𝑆)=𝑊Γ⊢ℓ(𝑆)∪e𝑊Γ⊢ℓ(𝑆)TheintentioninSlicStanisthatstatementsoflevelℓareexecutedbeforethoseofℓ′ifℓ<ℓ′.Inordertofollowthatimplementationstrategywithoutreorderingpossiblynon-commutativepairsofstatements,weimposetheconditionS(𝑆1,𝑆2)whenwesequence𝑆1and𝑆2in(Seq).Definition1(Shreddableseq).S(𝑆1,𝑆2)≜∀ℓ1,ℓ2.(ℓ2<ℓ1)=⇒𝑅Γ⊢ℓ1(𝑆1)∩𝑊Γ⊢ℓ2(𝑆2)=∅.Forexample,thisexcludesthefollowingproblematicprogram:datarealsigma=1;modelrealmu∼normal(0,sigma);sigma=2;Above,sigmaandthestatementssigma=1andsigma=2areofleveldata,whichmeanstheyshouldbeexecutedbeforethestatementmu∼normal(0,sigma),whichisoflevelmodel.However,thiswouldchangetheintendedsemanticsoftheprogram,givingmuaN(0,2)priorinsteadoftheintendedN(0,1)prior.ThisproblematicprogramfailstotypecheckinSlicStan,asitisnotshreddable:¬S(mu∼normal(0,sigma),sigma=2).Definition2(Generativeseq).G(𝑆1,𝑆2)≜∀ℓ≠model.e𝑊Γ⊢ℓ(𝑆1)∩𝑊e𝑊Γ⊢ℓ(𝑆2)=∅∧𝑊e𝑊Γ⊢ℓ(𝑆1)∩e𝑊Γ⊢ℓ(𝑆2)=∅Tobeabletoread𝑥∼N(0,1)atlevelgenqant,dependingonthecontext,eitherasaprob-abilisticassignmentto𝑥orasadensitycontribution,weimposetheconditionG(𝑆1,𝑆2)whenwesequence𝑆1and𝑆2.Thisexcludesproblematicprogramslikethefollowing,inwhichthemultipleassignmentstoycreateadiscrepancybetweenthedensitysemanticsoftheprogram𝑝(𝑦)=N(𝑦|0,1)N(𝑦|0,1)andthesampling-basedsemanticsoftheprogramy=5.genquantrealy∼normal(0,1);y∼normal(0,1);y=5;ThisproblematicprogramfailstotypecheckinSlicStanowingtotheGconstraint:¬G(y∼normal(0,1),y∼normal(0,1)),andalso¬G(y∼normal(0,1),y=5).3HereweuseΓ(𝐿)tolookupthetypeoftheL-value𝐿inΓ.Sometimeswewilluseanoverloadedmeaningofthisnotation(Definition14)tolook-uptheleveltypeofageneralexpression.WhichΓ(.)werefertowillbeclearfromcontext.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:72.3OperationalSemanticsofSlicStanStatementsInthispaper,weuseamodifiedversionofthesemanticsgiveninGorinovaetal.[2019].Weextendthecall-by-valueoperationalsemanticsgiveninthatpaper,andderiveamoreequationalformthatalsoincludesthegeneratedquantities.Wedefineastandardbig-stepoperationalsemanticsforSlicStanexpressionsandstatements:Big-stepRelation(𝑠,𝐸)⇓𝑉expressionevaluation(𝑠,𝑆)⇓(𝑠′,𝑤)statementevaluationHere,𝑠and𝑠′arestates,𝑉isavalueand𝑤∈R>0isaweight.Ourstatementscanreadandwritethestatewitharbitrarydestructiveupdates.Theweightcanbethoughtofasanelementofstatethatstoresapositiverealvaluewhichonlygetsaccessedbymultiplyingitwiththevalueofanexpression𝐸,throughtheuseoffactor(𝐸)-statements.Itcanonlybereadthroughatarget(𝑆)-statementwhichinitialisestheweightto1,evaluatesthestatement𝑆andreturnsthefinalweight.Formally,statesandvaluesaredefinedasfollows.ValuesandStates:𝑉::=value𝑐constant[𝑉1,...,𝑉𝑛]array𝑠::=𝑥1↦→𝑉1,...,𝑥𝑛↦→𝑉𝑛𝑥𝑖distinctstate(finitemapfromvariablestovalues)Intherestofthepaper,weusethenotationforstates𝑠=𝑥1↦→𝑉1,...,𝑥𝑛↦→𝑉𝑛:•𝑠[𝑥↦→𝑉]isthestate𝑠,butwherethevalueof𝑥isupdatedto𝑉if𝑥∈dom(𝑠),ortheelement𝑥↦→𝑉isaddedto𝑠if𝑥∉dom(𝑠).•𝑠[−𝑥]isthestates,butwhere𝑥isremovedfromthedomainof𝑠(ifitwerepresent).Wealsodefinelookupandupdateoperationsonvalues:•If𝑈isan𝑛-dimensionalarrayvaluefor𝑛≥0and𝑐1,...,𝑐𝑛aresuitableindexesinto𝑈,thenthelookup𝑈[𝑐1]...[𝑐𝑛]isthevaluein𝑈indexedby𝑐1,...,𝑐𝑛.•If𝑈isan𝑛-dimensionalarrayvaluefor𝑛≥0and𝑐1,...,𝑐𝑛aresuitableindexesinto𝑈,thenthe(functional)update𝑈[𝑐1]...[𝑐𝑛]:=𝑉isthearraythatisthesameas𝑈exceptthatthevalueindexedby𝑐1,...,𝑐𝑛is𝑉.Therelation⇓isdeterministicbutpartial,aswedonotexplicitlyhandleerrorstates.Thepurposeoftheoperationalsemanticsistodefineadensityfunctionin§§2.4,andanyerrorsleadtothedensitybeingundefined.Thebig-stepsemanticsisdefinedasfollows.OperationalSemanticsofExpressions:(EvalConst)(𝑠,𝑐)⇓𝑐(EvalVar)𝑉=𝑠(𝑥)𝑥∈dom(𝑠)(𝑠,𝑥)⇓𝑉(EvalArr)(𝑠,𝐸𝑖)⇓𝑉𝑖∀𝑖∈1..𝑛(𝑠,[𝐸1,...,𝐸𝑛])⇓[𝑉1,...,𝑉𝑛](EvalArrEl)(𝑠,𝐸1⇓𝑉)(𝑠,𝐸2⇓𝑐)(𝑠,𝐸1[𝐸2])⇓𝑉[𝑐](EvalPrimCall)4(𝑠,𝐸𝑖)⇓𝑉𝑖∀𝑖∈1...𝑛𝑉=𝑓(𝑉1,...,𝑉𝑛)(𝑠,𝑓(𝐸1,...,𝐸𝑛))⇓𝑉ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:8MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár(EvalArrComp)5(𝑠,𝐸1)⇓𝑛(𝑠,𝐸2)⇓𝑚(𝑠,𝐸[𝑖/𝑥])⇓𝑉𝑖,∀𝑛≤𝑖≤𝑚(𝑠,[𝐸|𝑥in𝐸1:𝐸2])⇓[𝑉𝑛,...,𝑉𝑚](EvalTarget)(𝑠,𝑆)⇓(𝑠′,𝑤)(𝑠,target(𝑆))⇓𝑤OperationalSemanticsofStatements:(EvalAssign)(where𝐿=𝑥[𝐸1]...[𝐸𝑛])(𝑠,𝐸𝑖)⇓𝑉𝑖∀𝑖∈1..𝑛(𝑠,𝐸)⇓𝑉𝑈=𝑠(𝑥)𝑈′=(𝑈[𝑉1]...[𝑉𝑛]:=𝑉)(𝑠,𝐿=𝐸)⇓(𝑠[𝑥↦→𝑈′],1)(EvalSkip)(𝑠,skip)⇓(𝑠,1)(EvalSeq)(𝑠,𝑆1)⇓(𝑠′,𝑤)(𝑠′,𝑆2)⇓(𝑠′′,𝑤′)(𝑠,𝑆1;𝑆2)⇓(𝑠′′,𝑤∗𝑤′)(EvalForFalse)(𝑠,𝐸1)⇓𝑐1(𝑠,𝐸2)⇓𝑐2𝑐1>𝑐2(𝑠,for(𝑥in𝐸1:𝐸2)𝑆)⇓(𝑠,1)(EvalForTrue){(𝑠,𝐸𝑖)⇓𝑐𝑖}𝑖=1,2𝑐1≤𝑐2(𝑠[𝑥↦→𝑐1],𝑆)⇓(𝑠′,𝑤)(𝑠′[−𝑥],for(𝑥in(𝑐1+1):𝑐2)𝑆)⇓(𝑠′′,𝑤′)(𝑠,for(𝑥in𝐸1:𝐸2)𝑆)⇓(𝑠′′,𝑤∗𝑤′)(EvalIfTrue)(𝑠,𝐸)⇓𝑐≠0.0(𝑠,𝑆1)⇓(𝑠′,𝑤)(𝑠,if(𝐸)𝑆1else𝑆2)⇓(𝑠′,𝑤)(EvalIfFalse)(𝑠,𝐸)⇓0.0(𝑠,𝑆2)⇓(𝑠′,𝑤)(𝑠,if(𝐸)𝑆1else𝑆2)⇓(𝑠′,𝑤)(EvalFactor)(𝑠,𝐸)⇓𝑉(𝑠,factor(𝐸))⇓(𝑠,𝑉)(EvalSample)6(𝑠,𝐿)⇓𝑉(𝑠,𝐸𝑖)⇓𝑉𝑖,∀1≤𝑖≤𝑛𝑉′=𝑑(𝑉|𝑉1,...,𝑉𝑛)(𝑠,𝐿∼𝑑(𝐸1,...,𝐸𝑛))⇓(𝑠,𝑉′)Mostrulesofthebig-stepoperationalsemanticsarestandard,withtheexceptionof(EvalFactor)and(EvalSample),whichcorrespondtothePPL-specificlanguageconstructsfactorand𝐿∼𝑑(𝐸1,...,𝐸𝑛).Whilewerefertothelatterconstructasprobabilisticassignment,itsformalsemanticsisnotthatofanassignmentstatement:boththeleftandtherighthand-sideofthe“assignment”areevaluatedtoavalue,inorderforthedensitycontribution𝑑(𝑉|𝑉1,...,𝑉𝑛)tobeevaluatedandfactoredintotheweightofthecurrentexecutiontrace.Contraryto(EvalAssign),thereisnobindingofaresulttoavariablein(EvalSample).Ofcourse,asiscommoninprobabilisticprogramming,itmight,attimes7,bebeneficialtoexecutethesestatementsasactualprobabilisticassignments.Ourtreatmentofthesestatementsisagnosticofsuchimplementationdetails,however.Thedesignofthetypesystemensuresthatinformationcanflowfromalevelℓtoahigheroneℓ′≥ℓ,butnotaloweroneℓ′<ℓ:anoninterferenceresult.Tostatethisformally,weintroducethenotionsofconformancebetweenastate𝑠andatypingenvironmentΓandℓ-equalityofstates.Wedefineaconformancerelationonstates𝑠andtypingenvironmentsΓ.Astate𝑠conformstoanenvironmentΓ,whenever𝑠providesvaluesofthecorrecttypesforthevariablesusedinΓ:ConformanceRelation:𝑠|=Γstate𝑠conformstoenvironmentΓRulefortheConformanceRelation:4𝑓(𝑉1,...,𝑉𝑛)meansapplyingthebuilt-infunction𝑓onthevalues𝑉1,...,𝑉𝑛.5Here,wewrite𝐸[𝐸′/𝑥]fortheusualcaptureavoidingsubstitutionof𝐸′for𝑥in𝐸.6By𝑑(𝑉|𝑉1,...,𝑉𝑛),wemeantheresultofevaluatingtheintendedbuilt-inconditionaldistribution𝑑on𝑉,𝑉1,...,𝑉𝑛.7Forexample,inourStanbackendforSlicStan,ifsuchastatementisoflevelmodel,itwillbeexecutedasdensitycontribution,whileifitisoflevelgenqant,itwillbeexecutedasaprobabilisticassignment.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:9(StanState)𝑉𝑖|=𝜏𝑖∀𝑖∈𝐼(𝑥𝑖↦→𝑉𝑖)𝑖∈𝐼|=(𝑥𝑖:𝜏𝑖)𝑖∈𝐼Here,𝑉|=𝜏denotesthatthevalue𝑉isoftype𝜏,andithasthefollowingdefinition:•𝑐|=int,if𝑐∈Z,and𝑐|=real,if𝑐∈R.•[𝑉1,...,𝑉𝑛]|=𝜏[𝑛],if∀𝑖∈1...𝑛.𝑉𝑖|=𝜏.Definition3(ℓ-eqalstates).GivenatypingenvironmentΓ,states𝑠1|=Γand𝑠2|=Γareℓ-equalforlevelℓ(written𝑠1≈ℓ𝑠2),iftheydifferonlyforvariablesofalevelstrictlyhigherthanℓ:𝑠1≈ℓ𝑠2≜∀𝑥:(𝜏,ℓ′)∈Γ.(ℓ′≤ℓ=⇒𝑠1(𝑥)=𝑠2(𝑥))Lemma1(Noninterferenceof⊢).Suppose𝑠1|=Γ,𝑠2|=Γ,and𝑠1≈ℓ𝑠2forsomeℓ.ThenforSlicStanstatement𝑆andexpression𝐸:(1)IfΓ⊢𝐸:(𝜏,ℓ)and(𝑠1,𝐸)⇓𝑉1and(𝑠2,𝐸)⇓𝑉2then𝑉1=𝑉2.(2)IfΓ⊢𝑆:ℓand(𝑠1,𝑆)⇓𝑠′1,𝑤1and(𝑠2,𝑆)⇓𝑠′2,𝑤2then𝑠′1≈ℓ𝑠′2.Proof.(1)followsbyruleinductiononthederivationΓ⊢𝐸:(𝜏,ℓ),andusingthatifΓ⊢𝐸:(𝜏,ℓ),𝐸reads𝑥andΓ(𝑥)=(𝜏′,ℓ′),thenℓ′≤ℓ.(2)followsbyruleinductiononthederivationΓ⊢𝑆:ℓandusing(1).WepresentmoredetailsoftheproofinAppendixA.□2.4DensitySemanticsThesemanticaspectofaSlicStanprogramΓ,𝑆thatwearethemostinterestedinisthefinalweight𝑤obtainedafterevaluatingtheprogram𝑆.Thisisthevaluetheprogramcomputesfortheunnormalisedjointdensity𝑝∗(x)=𝑝∗(D,𝜽,𝑄)overthedataD,themodelparameters𝜽,andgeneratedquantities𝑄oftheprogram(see§§2.6).GivenaprogramΓ,𝑆,weseparatethetypingenvironmentΓintodisjointparts:Γ𝜎andΓx,suchthatΓ𝜎containspreciselythevariablesthataredeterministicallyassignedin𝑆andΓxcontainsthosewhichnevergetdeterministicallyassigned;thatisthevariablesxwithrespecttowhichwedefinethetargetunnormaliseddensity𝑝∗(x):Γ𝜎={(𝑥:𝑇)∈Γ|𝑥∈𝑊(𝑆)}Γx=Γ\Γ𝜎.Similarly,anyconformingstate𝑠|=Γseparatesas𝜎⊎xwith𝜎={(𝑥↦→𝑉)∈𝑠|𝑥∈𝑊(𝑆)}x=𝑠\𝜎.Then,𝜎|=Γ𝜎andx|=Γx.ThesemanticsofaSlicStanprogramΓ𝜎,Γx,𝑆isafunctionJ𝑆Konstates𝜎|=Γ𝜎andx|=Γxthatyieldsapairofastate𝜎′andaweight𝑤,suchthat:J𝑆K(𝜎)(x)=𝜎′,𝑤,where𝜎⊎x,𝑆⇓𝜎′⊎x,𝑤.Wewillsometimesreferonlytooneofthetwoelementsofthepair𝜎,𝑤.Inthosecasesweusethenotation:J𝑆K𝑠(𝜎)(x),J𝑆K𝑝(𝜎)(x)=J𝑆K(𝜎)(x).WecallJ𝑆K𝑠thestatesemanticsandJ𝑆K𝑝thedensitysemanticsofΓ,𝑆.Wewillbeparticularlyinterestedinthedensitysemantics.ThefunctionJ𝑆K𝑝(𝜎)issomepositivefunction𝜙(x)ofx.Ifx1,x2isapartitioningofxand∫𝜙(x)dx1isfinite,wesay𝜙(x)isanunnormaliseddensitycorrespondingtothenormaliseddensity𝑝(x1|x2)=𝜙(x)/∫𝜙(x)dx1overx1andwewriteJ𝑆K𝑝(𝜎)(x)∝𝑝(x1|x2).Sometimes,when𝜎isclearfromcontext,wewillleaveitimplicitandsimplywrite𝑝(x)for𝑝(x;𝜎).Next,weobservehowthestateanddensitysemanticscompose.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:10MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárLemma2(Semanticscomposes).Thestateanddensitysemanticscomposeasfollows:J𝑆1;𝑆2K𝑠(𝜎)(x)=J𝑆2K𝑠(J𝑆2K𝑠(𝜎)(x))(x)J𝑆1;𝑆2K𝑝(𝜎)(x)=J𝑆1K𝑝(𝜎)(x)×J𝑆2K𝑝(J𝑆1K𝑠(𝜎)(x))(x)Throughoutthepaperweusethefollowingnotationtoseparatethestoreinaconciseway.Definition4(Γℓ(𝑠)or𝑠ℓ).ForatypingenvironmentΓandastore𝑠|=Γ,letΓℓ(𝑠)={(𝑥↦→𝑉)∈𝑠|Γ(𝑥)=(_,ℓ)}.Whenitisclearwhichtypingenvironmentthenotationrefersto,wewritesimply𝑠ℓinsteadofΓℓ(𝑠).Usingthisdefinition,were-statethenoninterferenceresultinthefollowingconvenientform.Lemma3(Noninterferenceof⊢reformulated).LetΓ𝜎,Γx⊢𝑆beawell-typedSlicStanprogram.Foralllevelsℓ∈{data,model,genquant},thereexistuniquefunctions𝑓ℓ,suchthatforall𝜎|=Γ𝜎,x|=Γxand𝜎′suchthatJ𝑆K𝑠(𝜎)(x)=𝜎′,𝜎′ℓ=𝑓ℓ({𝜎ℓ′,xℓ′|ℓ′≤ℓ}).2.5ShreddingandTranslationtoStanAkeyaimofSlicStanistorearrangetheinputprogramintothreephasesofexecution,correspondingtothelevelsofthetypesystem:datapreprocessing,coremodelcodetorunMCMCoranotherinferencealgorithmon,andgenqant,orgeneratedquantities,whichamounttosamplepost-processingafterinferenceisperformed.Themotivationforthesephasesisthattheyallnaturallyappearintheworkflowofprobabilisticprogramming.TheblocksoftheStanarebuiltaroundthisphasedistinction,andcompilationofSlicStantoStanandcomparableback-endsrequiresit.Thephasesimposedifferentrestrictionsonthecodeandmakeitincurdifferingcomputationalcosts.Themodelphaseisbyfarthemostexpensivetoevaluate:codeinthisphasetendstobeexecutedrepeatedlywithintheinnerloopofaninferencealgorithmlikeanMCMCmethod.Further,ittendstobeautomaticallydifferentiated[GriewankandWalther2008]incasegradient-basedinferencealgorithmsareused,whichrestrictstheavailableprogrammingfeaturesandincreasesthespaceandtimecomplexityofevaluation.TypeinferenceinSlicStancombinedwithshreddingallowstheusertowritetheircodewithoutworryingabouttheperformanceofdifferentphases,ascodewillbeshreddedintoitsoptimalphaseofexecution.Theshreddingrelationisinthecoreofthisrearrangement.ShreddingtakesaSlicStanstatement𝑆andsplitsitintothreesingle-levelstatements(Definition5).Thatis,𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄meanswesplit𝑆intosub-statements𝑆𝐷,𝑆𝑀,𝑆𝑄,were𝑆𝐷mentionsonlydatavariables,𝑆𝑀mentionsdataandmodelvariables,and𝑆𝑄istherestoftheprogram,andsuchthatthecomposition𝑆𝐷;𝑆𝑀;𝑆𝑄behavesthesameastheoriginalprogram𝑆.Whencombinedwithtypeinference,shreddingautomaticallydeterminesoptimalstatementplacement,suchthatonlynecessaryworkisexecutedinthe‘heavy-weight’modelpartofinference.Weadapttheshreddingfrom[Gorinovaetal.2019],sothatthefollowingholdsforthethreesub-statementsofashreddedwell-typedSlicStanprogramΓ⊢𝑆:•𝑆𝐷implementsdeterministicdatapreprocessing:nocontributionstothedensityareallowed.•𝑆𝑀istheinferencecore:itistheleastrestrictiveofthethreeslices—eitherorbothof𝑆𝐷and𝑆𝑄canbemergedinto𝑆𝑀.Itcaninvolvecontributionstothedensitywhichrequireadvancedinferenceforsampling.Therefore,thisisthepartoftheprogramwhichrequiresthemostcomputationduringinference(inStan,whatisruninsideHMC);•𝑆𝑄representssamplepost-processing:anycontributionstothedensityaregenerative.Thatis,theycanimmediatelybeimplementedusingdrawsfromrandomnumbergenerators.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:11Intermsofinference,wecanrun𝑆𝐷onceasapre-processingstep.Thenuseasuitableinferencealgorithmfor𝑆𝑀(inthecaseofStan,that’sHMC,butwecanuseotherMCMCorVIalgorithms),and,finally,weuseancestralsamplingfor𝑆𝑄.8ShreddingRelation𝑆⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)statementshreddingShreddingRulesforStatements:(ShredAssign)Γ(𝐿)=(_,data)→𝑆𝐷=𝐿=𝐸,𝑆𝑀=𝑆𝑄=skipΓ(𝐿)=(_,model)→𝑆𝑀=𝐿=𝐸,𝑆𝐷=𝑆𝑄=skipΓ(𝐿)=(_,genqant)→𝑆𝑄=𝐿=𝐸,𝑆𝐷=𝑆𝑀=skip𝐿=𝐸⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)(ShredSeq)𝑆1⇕Γ𝑆𝐷1,𝑆𝑀1,𝑆𝑄1𝑆2⇕Γ𝑆𝐷2,𝑆𝑀2,𝑆𝑄2𝑆1;𝑆2⇕Γ(𝑆𝐷1;𝑆𝐷2),(𝑆𝑀1;𝑆𝑀2),(𝑆𝑄1;𝑆𝑄2)(ShredFactor)Γ(𝐸)=data→𝑆𝐷=factor(𝐸),𝑆𝑀=𝑆𝑄=skipΓ(𝐸)=model→𝑆𝑀=factor(𝐸),𝑆𝐷=𝑆𝑄=skipΓ(𝐸)=genqant→𝑆𝑄=factor(𝐸),𝑆𝐷=𝑆𝑀=skipfactor(𝐸)⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)(ShredSkip)skip⇕Γ(skip,skip,skip)(ShredSample)Γ(𝐿,𝐸1,...,𝐸𝑛)=data→𝑆𝐷=𝐿∼𝑑(𝐸1,...,𝐸𝑛),𝑆𝑀=𝑆𝑄=skip)Γ(𝐿,𝐸1,...,𝐸𝑛)=model→𝑆𝑀=𝐿∼𝑑(𝐸1,...,𝐸𝑛),𝑆𝐷=𝑆𝑄=skip)Γ(𝐿,𝐸1,...,𝐸𝑛)=genqant→𝑆𝑄=𝐿∼𝑑(𝐸1,...,𝐸𝑛),𝑆𝐷=𝑆𝑀=skip)𝐿∼𝑑(𝐸1,...,𝐸𝑛)⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)(ShredIf)𝑆1⇕Γ(𝑆𝐷1,𝑆𝑀1,𝑆𝑄1)𝑆2⇕Γ(𝑆𝐷2,𝑆𝑀2,𝑆𝑄2)if(𝑔)𝑆1else𝑆2⇕Γ(if(𝑔)𝑆𝐷1else𝑆𝐷2),(if(𝑔)𝑆𝑀1else𝑆𝑀2),(if(𝑔)𝑆𝑄1else𝑆𝑄2)(ShredFor)𝑆⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)for(𝑥in𝑔1:𝑔2)𝑆⇕Γ(for(𝑥in𝑔1:𝑔2)𝑆𝐷),(for(𝑥in𝑔1:𝑔2)𝑆𝑀),(for(𝑥in𝑔1:𝑔2)𝑆𝑄)Here,Γ(𝐸)(Definition14)givestheprincipaltypeofanexpression𝐸,whileΓ(𝐸1,...,𝐸𝑛)(Definition15)givestheleastupperboundoftheprincipaltypesof𝐸1,...,𝐸𝑛.The(ShredIf)and(ShredFor)rulesmakesuretoshredifandforstatementssothattheyareseparatedintopartswhichcanbecomputedindependentlyateachofthethreelevels.Notethattheusageofifandforguardsissimplified,toavoidstatingrulesforwhentheguard(s)areofdifferentlevels.Forexample,ifwehaveastatementif(𝐸)𝑆1else𝑆2,where𝐸isoflevelmodel,wecannotaccess𝐸atleveldata,thustheactualshreddingrulewewoulduseis:(ShredIfModelLevel)𝑆1⇕Γ(𝑆𝐷1,𝑆𝑀1,𝑆𝑄1)𝑆2⇕Γ(𝑆𝐷2,𝑆𝑀2,𝑆𝑄2)if(𝑔)𝑆1else𝑆2⇕Γskip,(if(𝑔)𝑆𝐷1;𝑆𝑀1else𝑆𝐷2;𝑆𝑀2),(if(𝑔)𝑆𝑄1else𝑆𝑄2)8Ancestral(orforward)samplingreferstothemethodofsamplingfromajointdistributionbyindividuallysamplingvariablesfromthefactorsconstitutingthejointdistribution.Forexample,wecansamplefrom𝑝(𝑥,𝑦)=𝑝(𝑥)𝑝(𝑦|𝑥)byrandomlygeneratingˆ𝑥accordingto𝑝(𝑥),andthenrandomlygeneratingˆ𝑦accordingto𝑝(𝑦|𝑥=ˆ𝑥).ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:12MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárTheseshreddingrulesfollowverycloselythosegivenbyGorinovaetal.[2019].Themaindifferenceisthatsamplestatements(𝐿∼𝑑(𝐸1,...,𝐸𝑛))areallowedtobeofgenqantlevelandcanbeincludedinthelast,generativesliceoftheprogram(seerule(ShredSample)).Inotherwords,suchgenqantsamplestatementsarethosestatementsthatcanbeinterpretedasprobabilisticassignment(usingrandomnumbergeneratorfunctions)todirectlysamplefromtheposteriordistributionaccordingtoancestralsampling.WeprovideproofsforthefollowingkeyresultsinAppendixA:shreddingproducessingle-levelstatements(Definition5andLemma4)andshreddingissemanticspreserving(Lemma6).Intuitively,asingle-levelstatementoflevelℓisonethatupdatesonlyvariablesoflevelℓ.Definition5(Single-levelStatementΓ⊢ℓ(𝑆)).Wedefinesingle-levelstatements𝑆oflevelℓwithrespecttoΓ(writtenΓ⊢ℓ(𝑆)),byinduction:SingleLevelStatements:(AssignSingle)Γ(𝑥)=(_,ℓ)Γ⊢ℓ(𝑥[𝐸1]···[𝐸𝑛]=𝐸)(SeqSingle)Γ⊢ℓ(𝑆)Γ⊢ℓ(𝑆′)Γ⊢ℓ(𝑆;𝑆′)(ForSingle)Γ,𝑥:(int,ℓ)⊢ℓ(𝑆)Γ⊢ℓ(for(𝑥in𝐸1:𝐸2)𝑆)(IfSingle)Γ⊢ℓ(𝑆1)Γ⊢ℓ(𝑆2)Γ⊢ℓ(if(𝐸)𝑆1else𝑆2)(SkipSingle)Γ⊢ℓ(skip)(FactorSingle)Γ⊢𝐸:ℓ∀ℓ′<ℓ.Γ⊬𝐸:ℓ′Γ⊢ℓ(factor(𝐸))(SampleSingle)Γ⊢𝐿∼𝑑(𝐸1,...,𝐸𝑛):ℓ∀ℓ′<ℓ.Γ⊬𝐿∼𝑑(𝐸1,...,𝐸𝑛):ℓ′Γ⊢ℓ(𝐿∼𝑑(𝐸1,...,𝐸𝑛))Lemma4(Shreddingproducessingle-levelstatements).Γ⊢𝑆∧𝑆⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)=⇒Γ⊢data(𝑆𝐷)∧Γ⊢model(𝑆𝑀)∧Γ⊢genquant(𝑆𝑄)Weprovearesultabouttheeffectofsingle-levelstatementsonthestoreandweightofwell-typedprograms(Lemma5).Intuitively,thisresultshowsthatasingle-levelstatementoflevelℓactsonthestateandweightinawaythatisindependentoflevelsgreaterthanℓ.Lemma5(Propertyofsingle-levelstatements).LetΓ𝜎,Γx,𝑆beaSlicStanprogram,suchthat𝑆isasingle-levelstatementoflevelℓ,Γ⊢ℓ(𝑆).Thenthereexistuniquefunctions𝑓and𝜙,suchthatforany𝜎,x|=Γ𝜎,Γx:J𝑆K(𝜎)(𝑥)=𝑓(𝜎≤ℓ,x≤ℓ)∪𝜎>ℓ,𝜙(𝜎≤ℓ)(x≤ℓ),wherewewrite𝜎≤ℓ={(𝑥↦→𝑉)∈𝜎|Γ𝜎(𝑥)=(_,ℓ)}and𝜎>ℓ=𝜎\𝜎≤ℓ.Lemma6(SemanticPreservationof⇕Γ).IfΓ⊢𝑆:dataand𝑆⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)thenJ𝑆K=J𝑆𝐷;𝑆𝑀;𝑆𝑄K.2.6DensityFactorisationAsanextensionof[Gorinovaetal.2019],weshowthatshreddinginducesanaturalfactorizationofthedensityimplementedbytheprogram:𝑝(D,𝜽,𝑄)=𝑝(𝜽,D)𝑝(𝑄|𝜽,D).9Thismeansthatwecanseparatetheprogramintoadeterministicpreprocessingpart,apartthatusesa‘heavy-weight’inferencealgorithm,suchasHMC,andapartthatusessimpleancestralsampling.9Here,𝑝(𝑄|𝜽,D)denotestheconditionalprobabilitydensityof𝑄,giventhevaluesof𝜽andD.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:13Theorem1(Shreddinginducesafactorisationofthedensity).SupposeΓ⊢𝑆:dataand𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄andΓ=Γ𝜎,ΓD,Γ𝜽,Γ𝑄.Forall𝜎,D,𝜽,and𝑄:if𝜎,D,𝜽,𝑄|=Γ𝜎,ΓD,Γ𝜽,Γ𝑄,andJ𝑆K𝑝(𝜎)(D,𝜽,𝑄)∝𝑝(D,𝜽,𝑄)ande𝑊(𝑆𝑄)=dom(Γ𝑄)then:(1)J𝑆𝑀K𝑝(𝜎𝐷)(D,𝜽,𝑄)∝𝑝(𝜽,D)(2)J𝑆𝑄K𝑝(𝜎𝑀)(D,𝜽,𝑄)=𝑝(𝑄|𝜽,D)where𝜎𝐷=J𝑆𝐷K𝑠(𝜎)(D,𝜽,𝑄),𝜎𝑀=J𝑆𝑀K𝑠(𝜎𝐷)(D,𝜽,𝑄),and𝑝(D,𝜽,𝑄)=𝑝(D,𝜽)𝑝(𝑄|D,𝜽).Proof.Thisfollowsbyprovingamoregeneralresultusinginductiononthestructureof𝑆,Lemma6,Lemma2andLemma4.SeeAppendixAforfullproof.□ThegivenSlicStanprogram𝑆definesajointdensity𝑝(D,𝜽,𝑄).Byshreddingweobtainamodelblock𝑆𝑀thatdefines𝑝(𝜽,D)andagenqantblock𝑆𝑄thatdefines𝑝(𝑄|𝜽,D).Hence,inferenceinStanusingtheseblocksrecoversthesemantics𝑝(D,𝜽,𝑄)oftheSlicStanprogram.3THEORY:CONDITIONALINDEPENDENCEBYTYPINGThissectionpresentsthemaintheoreticalcontributionofthepaper:aninformationflowtypesystemforconditionalindependence.Wepresentatypesystemandshowthatawell-typedprograminthatsystemisguaranteedtohavecertainconditionalindependenciesinitsdensitysemantics.Asareminder,determiningtheconditionalindependencerelationshipsbetweenvariablesisimportant,assuchrelationshipscaptureaqualitativesummaryofthespecifiedmodelandcanfacilitatemoreefficientinference.Forexample,in§4wepresentanapplicationthatusesourtypesystem:asemantic-preservingtransformationthatallowsfordiscreteparameterstobeintroducedinSlicStan,whichwaspreviouslynotpossibleduetoefficiencyconstraints.Ouraimistooptimiseprobabilisticprogramsbytransformingabstractsyntaxtreesorinterme-diaterepresentations(asintheStancompiler)thatareclosetoabstractsyntax.Hence,weseekawaytocomputeconditionaldependenciesbyatype-basedsourceanalysis,ratherthanbyexplicitlyconstructingaseparategraphicalrepresentationoftheprobabilsiticmodel.Giventhreedisjointsetsofrandomvariables(RVs)𝐴,𝐵and𝐶,wesaythat𝐴isconditionallyindependentof𝐵given𝐶,written𝐴⊥⊥𝐵|𝐶,ifandonlyiftheirdensitiesfactoriseas𝑝(𝐴,𝐵|𝐶)=𝑝(𝐴|𝐶)𝑝(𝐵|𝐶).(Analternativeformulationstatesthat𝐴⊥⊥𝐵|𝐶ifandonlyif𝑝(𝐴,𝐵,𝐶)=𝜙1(𝐴,𝐶)𝜙2(𝐵,𝐶)forsomefunctions𝜙1and𝜙2.)Derivingconditionalindependenciesinthepresenceofagraphicalmodel(suchasafactorgraph10)isstraightforward,whichiswhysomePPLsfocusonbuildingandperforminginferenceongraphs(forexample,Infer.NET[Minkaetal.2014]).However,buildingandmanipulatingafactorgraphingenerativePPLs(e.g.Gen[Cusumano-Towneretal.2019],Pyro[UberAILabs2017],Edward2[Tranetal.2018],PyMC3[Salvatieretal.2016])orimperativedensity-basedPPLs(SlicStan,Stan)isnotstraightforward.Dependenciesbetweenmodelledvariablesmightbeseparatedbyvariousdeterministictransformations,makingithardertotracktheinformationflow,and–moreimportantly–moredifficulttoisolatepartsofthemodelneededfortransformationssuchasvariableelimination.InthecaseofSlicStan,eachprogramcanstillbethoughtofasspecifyingafactorgraphimplicitly.Inthispaper,wefocusontheproblemofhowtoworkwithconditionalindependenceinformationimplicitlyencodedinaprobabilisticprogram,withouthavingaccesstoanexplicitfactorgraph.Forexample,considerProgramA:10Afactorgraphisabipartitegraphthatshowsthefactorisationofamultivariablefunction.Variablesarecircularnodes,andeachfactorofthefunctionisasquarenode.Anedgeexistsbetweenavariablenode𝑥andafactornode𝜙ifandonlyif𝜙isafunctionof𝑥.SeeProgramAanditscorrespondingfactorgraphasanexample,or[KollerandFriedman2009]fordetails.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:14MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárA.SimpleHiddenMarkovModel(HMM)int<2>z1∼bern(𝜃0);real𝜃1=foo(𝜃0,z1);int<2>z2∼bern(𝜃1);real𝜙1=foo(1,z1);real𝜙2=foo(1,z2);int<2>y1∼bern(𝜙1);int<2>y2∼bern(𝜙2);𝑧1𝑧1∼𝑏(𝜃0)𝑦1∼𝑏(foo(1,𝑧1))𝑦1𝑧2∼𝑏(foo(𝜃0,𝑧1))𝑧2𝑦2∼𝑏(foo(1,𝑧2))𝑦2Thefactorgraphaboverepresentsthefactorisationofthejointdensityfunctionovertheparame-tersoftheprogram:𝑝(𝑧1,𝑧2,𝑦1,𝑦2)=𝑏(𝑧1|𝜃0)𝑏(𝑦1|foo(1,𝑧1))𝑏(𝑧2|foo(𝜃0,𝑧1))𝑏(𝑦2|foo(1,𝑧2)).Eachofthefourfactorsisrepresentedbyasquarenodeinthegraph,anditconnectstothevariables(circlenodes)thatthefactordependson.Thisrepresentationisusefulforthinkingaboutconditionalindependencies.Forexample,itisimmediatelyevidentfromthegraphthatvariableswhichconnecttothesamesquarenodecannotbeconditionallyindependentastheyshareafactor.Moregenerally,ifthereisan(uninterruptedbyobservedvariables)directpathbetweentwovariables,thenthesetwovariablesarenotconditionallyindependent[Frey2002].Whenlookingatthefactorgraph,itisstraightforwardtoseethat𝑧1and𝑧2arenotconditionallyindependent,andneitherare𝑧1and𝑦1nor𝑧2and𝑦2,asthereisadirectpathbetweeneachofthesepairs.Whenlookingattheprogram,however,weneedtoreasonabouttheinformationflowthroughthedeterministicvariables𝜃1,𝜙1and𝜙2toreachthesameconclusion.Moreover,manipulationoftheprogrambasedonconditionaldependenciescanalsobemoredifficultwithoutafactorgraph.Asanexample,considertheproblemofvariableelimination(whichwediscussinmoredetailsin§§4.3).Ifwearetoeliminate𝑧1inthefactorgraph,usingvariableelimination,wewouldsimplymergethefactorsdirectlyconnectedto𝑧1,sumover𝑧1,andattachthenewfactorstoallformerneighboursof𝑧1(inthiscase𝑦1and𝑧2,butnot𝑦2).However,inthecaseofanimperativeprogram,weneedtoisolateallthestatementsthatdependon𝑧1,andgroupthemtogetherwithoutchangingthemeaningoftheprogrambeyondtheelimination:B.HMMwith𝑧1marginalisedoutfactor(sum([target(z1∼bern(𝜃0);real𝜃1=foo(𝜃0,z1);z2∼bern(𝜃1);real𝜙1=foo(1,z1);y1∼bern(𝜙1);)|z1in1:2]));real𝜙2=foo(1,z2);int<2>y2∼bern(𝜙2);.˝𝑧1[𝑏(𝑧1|𝜃0)×𝑏(𝑦1|foo(1,𝑧1))×𝑏(𝑧2|foo(𝜃0,𝑧1))]𝑧2𝑦2∼𝑏(foo(1,𝑧2))𝑦2𝑦1Weneedawaytoanalysetheinformationflowtodetermineconditionalindependenciesbetweenvariables.Intheexampleabove,wecanleave𝑦2outoftheeliminationof𝑧1,because𝑧1and𝑦2areconditionallyindependentgiven𝑧2,written𝑧1⊥⊥𝑦2|𝑧2.Toanalysetheinformationflow,weintroduceanoveltypesystem,whichwerefertoviatherelation⊢2.Itworkswithalowersemi-lattice({l1,l2,l3},≤)oflevels,wherel1≤l2andl1≤l3andl2andl3areunrelated.(Recallthatalowersemi-latticeisapartialorderinwhichanytwoelementsℓ1,ℓ2haveagreatestlowerboundℓ1⊓ℓ2butdonotalwayshaveanupperbound.)Awell-typedprograminducesaconditionalindependencerelationshipforthe(random)variables(RVs)intheprogram:l2-RVs⊥⊥l3-RVs|l1-RVs.Intheexampleabove,thisresultallowsustoeliminatel2-variables(𝑧1),whileonlyconsideringl1-variables(𝑦1and𝑧2)andknowingl3-variables(𝑦2)areunaffectedbytheelimination.WecanuseACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:15xl1J𝑆1K𝑝J𝑆2K𝑝xl2J𝑆3K𝑝xl3(a)Factorgraphofvariablesofdifferentlevels.xl1xl2xl3(b)Informationflowbetweenlevels.Fig.2.Intuitionforthesemi-latticecasel1<l2andl1<l3,wherexℓisoflevelℓ.Wegetxl2⊥⊥xl3|xl1.ashreddingrelationalmostidenticaltothatof§§2.5toslicetheprograminasemantics-preservingway,andisolatethesub-statementsneededforelimination.Here,𝜃1and𝜙1mustbeoflevell2fortheprogramtobewell-typed.Thus,allstatementsinvolving𝑧1,𝜃1or𝜙1areoflevell2,andtheshreddingrelationgroupsthemtogetherinsideoftheeliminationloopfor𝑧1.Figure2showstherelationshipbetweenthelevelsl1,l2,l3andtheshreddingrelation.Informa-tionflowsfroml1tol2andl3,butthereisnoflowofinformationbetweenl2andl3(Figure2b).A⊢2-well-typedprogram𝑆isshreddedby⇕Γinto𝑆1,𝑆2and𝑆3,where𝑆1onlymentionsl1vari-ables,𝑆2onlymentionsl1andl2variables,and𝑆3onlymentionsl1andl3variables.Thiscanbeunderstoodasanewfactorgraphformulationoftheoriginalprogram𝑆,whereeachofthesubstatements𝑆1,𝑆2,𝑆3definesafactorconnectedtoanyinvolvedvariables(Figure2a).Ourapproachreliesondeterminingthel1,l2,l3leveltypesbytypeinference,astheyarenotintrinsictothevariablesorprograminanyway,butaredesignedsolelytodetermineconditionalindependencerelationships.Thesetypesarenotaccessiblebytheprobabilisticprogramminguser.Ourtypesystemmakesitpossibletoanswervariousquestionsaboutconditionalindependenceinaprogram.Assumingaprogramdefiningajointdensity𝑝(x),wecanusethetypesystemto:(1)Checkifx2⊥⊥x3|x1forsomepartitioningx=x1,x2,x3.(2)Findanoptimalvariablepartitioning.Givenavariable𝑥∈x,findapartitioningx=x1,x2,x3,suchthat𝑥∈x2,x2⊥⊥x3|x1,andx1andx2areassmallaspossible.(3)AskquestionsabouttheMarkovboundaryofavariable.Giventwovariables𝑥and𝑥′,findthepartitioningx=𝑥,x1,x2,suchthat𝑥⊥⊥x1|x2andx2isassmallaspossible.Is𝑥′inx2?Inotherwords,is𝑥′intheMarkovboundaryof𝑥?Intherestof§3,wegivethe⊢2typesystem(§§3.1),stateanoninterferenceresult(Lemma7,Lemma8)andshowthatsemanticsispreservedwhenshredding⊢2-well-typedprograms(Lemma10).Wepresentthetypesystemandtransformationrulesinadeclarativestyle.Theimplementationreliesontypeinference,whichwediscussin§§4.4.Wederivearesultaboutthewayshreddingfactorisesthedensitydefinedbytheprogram(Theorem2).Weproveaconditionalindependenceresult(§§3.2,Theorem3)anddiscussthescopeofourapproachwithexamples(§§3.3).3.1The⊢2TypeSystemWeintroduceamodifiedversionofSlicStan’stypesystem.Onceagain,types𝑇rangeoverpairs(𝜏,ℓ)ofabasetype𝜏,andaleveltypeℓ,butlevelsℓareoneofl1,l2,orl3,whichformalowersemi-lattice({l1,l2,l3},≤),wherel1≤l2andl1≤l3.Thismeans,forexample,thatanl2variablecandependonanl1variable,butanl3variablecannotdependonanl2variable,asleveltypesl2andl3areincomparable.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:16MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárThetypesystemisastandardinformationflowtypesystem,verysimilartothe⊢systemintroducedin§§2.2.Wemarktheonlynon-standardrules,(Sample2),(Factor2),and(Seq2),whichalsodifferfromthoseof⊢.(Sample2)and(Factor2)bothhavethesameeffectasanassignmenttoanimplicitweightvariablethatcanbeofanyofthethreelevels.(Seq2)isalessrestrictiveversionof(Seq)andexactlyasin[Gorinovaetal.2019],anditmakessuretheprogramcanbeslicedlater.Notealsothatthenon-interferencebetweenl2andl3reliesonthe(PrimCall2)rulenotbeingderivablewhentheleastupperboundˆ𝑛𝑖=1ℓ𝑖doesnotexist.TypingRulesforExpressions:(ESub2)Γ⊢2𝐸:(𝜏,ℓ)ℓ≤ℓ′Γ⊢2𝐸:(𝜏,ℓ′)(Var2)Γ,𝑥:𝑇⊢2𝑥:𝑇(Const2)ty(𝑐)=𝜏Γ⊢2𝑐:(𝜏,l1)(Arr2)Γ⊢2𝐸𝑖:(𝜏,ℓ)∀𝑖∈1..𝑛Γ⊢2[𝐸1,...,𝐸𝑛]:(𝜏[𝑛],ℓ)(ArrEl2)Γ⊢2𝐸1:(𝜏[𝑛],ℓ)Γ⊢𝐸2:(int,ℓ)Γ⊢2𝐸1[𝐸2]:(𝜏,ℓ)(PrimCall2)(𝑓:𝜏1,...,𝜏𝑛→𝜏)Γ⊢2𝐸𝑖:(𝜏𝑖,ℓ𝑖)∀𝑖∈1..𝑛Γ⊢2𝑓(𝐸1,...,𝐸𝑛):(𝜏,ˆ𝑛𝑖=1ℓ𝑖)(ArrComp2)∀𝑖=1,2.Γ⊢2𝐸𝑖:(int,ℓ)Γ,𝑥:(int,ℓ)⊢𝐸:(𝜏,ℓ)𝑥∉dom(Γ)Γ⊢2[𝐸|𝑥in𝐸1:𝐸2]:(𝜏[𝑛],ℓ)(Target2)Γ⊢2𝑆:ℓ′′∀ℓ′>ℓ.𝑅Γ⊢ℓ′(𝑆)=∅Γ⊢2target(𝑆):(real,ℓ)TypingRulesforStatements:(SSub2)Γ⊢2𝑆:ℓ′ℓ≤ℓ′Γ⊢2𝑆:ℓ(Assign2)Γ(𝐿)=(𝜏,ℓ)Γ⊢2𝐸:(𝜏,ℓ)Γ⊢2(𝐿=𝐸):ℓ(Sample2)Γ⊢2factor(D(𝐿|𝐸1,...,𝐸𝑛)):ℓΓ⊢2𝐿∼Ddist(𝐸1,...𝐸𝑛):ℓ(Factor2)Γ⊢2𝐸:(real,ℓ)Γ⊢2factor(𝐸):ℓ(Seq2)Γ⊢2𝑆1:ℓΓ⊢2𝑆2:ℓS(𝑆1,𝑆2)Γ⊢2(𝑆1;𝑆2):ℓ(If2)Γ⊢2𝐸:(bool,ℓ)Γ⊢2𝑆1:ℓΓ⊢2𝑆2:ℓΓ⊢2if(𝐸)𝑆1else𝑆2:ℓ(Skip2)Γ⊢2skip:ℓ(For2)Γ⊢2𝐸1:(int,ℓ)Γ⊢2𝐸2:(int,ℓ)Γ,𝑥:(int,ℓ)⊢2𝑆:ℓ𝑥∉dom(Γ)𝑥∉𝑊(𝑆)Γ⊢2for(𝑥in𝐸1:𝐸2)𝑆:ℓWestateandproveanoninterferenceresultfor⊢2,whichfollowssimilarlytotheresultfor⊢.Lemma7(Noninterferenceof⊢2).Suppose𝑠1|=Γ,𝑠2|=Γ,and𝑠1≈ℓ𝑠2forsomeℓ.ThenforaSlicStanstatement𝑆andexpression𝐸:(1)IfΓ⊢2𝐸:(𝜏,ℓ)and(𝑠1,𝐸)⇓𝑉1and(𝑠2,𝐸)⇓𝑉2then𝑉1=𝑉2.(2)IfΓ⊢2𝑆:ℓand(𝑠1,𝑆)⇓𝑠′1,𝑤1and(𝑠2,𝑆)⇓𝑠′2,𝑤2then𝑠′1≈ℓ𝑠′2.Proof.(1)followsbyruleinductiononthederivationΓ⊢2𝐸:(𝜏,ℓ),andusingthatifΓ⊢2𝐸:(𝜏,ℓ),𝑥∈𝑅(𝐸)andΓ(𝑥)=(𝜏′,ℓ′),thenℓ′≤ℓ.(2)followsbyruleinductiononthederivationΓ⊢2𝑆:ℓandusing(1).□ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:17Onceagainwederiveamoreconvenientformofthenoninterferenceresult.Becausetheleveltypesl2andl3arenotcomparableintheorder≤,changesinthestoreatl2donotaffectthestoreatl3andviceversa.Lemma8(Noninterferenceof⊢2-well-typedprograms).LetΓ𝜎,Γx,𝑆beaSlicStanprogram,andΓ⊢2𝑆:l1.Thereexistuniquefunctions𝑓,𝑔andℎ,suchthatforall𝜎|=Γ𝜎,x|=Γxand𝜎′suchthatJ𝑆K𝑠(𝜎)(x)=𝜎′:𝜎′l1=𝑓(𝜎l1,xl1),𝜎′l2=𝑔(𝜎l1,𝜎l2,xl1,xl2),𝜎′l3=ℎ(𝜎l1,𝜎l3,xl1,xl3)Proof.Followsfromnoninterference(Lemma7).□Next,weextendtheshreddingrelationfrom§§2.5,andtheconceptofsingle-levelstatements,toSlicStanprogramsthatarewell-typedwithrespectto⊢2.Thisisdonebysimplytreatingl1asdata,l2asmodel,andl3asgenqantforthepurposeofshredding.Weincludethefulldefinitionofshreddingwithrespectto⊢2forcompletenessbelow.Weusethesamenotation⇕Γ,andwegenerallytreatthestandardshreddingrelationfrom2.5andtheconditionalindependenceshreddingrelationpresentedhere,asthesamerelation,asthereisnodifferencebetweenthetwo,otherthanthenamingoflevels.ShreddingRulesforStatements:(Shred2Assign)Γ(𝐿)=l1→𝑆1=𝐿=𝐸,𝑆2=𝑆3=skipΓ(𝐿)=l2→𝑆2=𝐿=𝐸,𝑆1=𝑆3=skipΓ(𝐿)=l3→𝑆3=𝐿=𝐸,𝑆1=𝑆2=skip𝐿=𝐸⇕Γ(𝑆1,𝑆2,𝑆3)(Shred2Seq)𝑆1⇕Γ𝑆(1)1,𝑆(1)2,𝑆(1)3𝑆2⇕Γ𝑆(2)1,𝑆(2)2,𝑆(2)3𝑆1;𝑆2⇕Γ(𝑆(1)1;𝑆(2)1),(𝑆(1)2;𝑆(2)2),(𝑆(1)3;𝑆(2)3)(Shred2Factor)Γ(𝐸)=l1→𝑆1=factor(𝐸),𝑆2=𝑆3=skipΓ(𝐸)=l2→𝑆2=factor(𝐸),𝑆1=𝑆3=skipΓ(𝐸)=l3→𝑆3=factor(𝐸),𝑆1=𝑆2=skipfactor(𝐸)⇕Γ(𝑆1,𝑆2,𝑆3)(Shred2Skip)skip⇕Γ(skip,skip,skip)(Shred2Sample)Γ(𝐿,𝐸1,...,𝐸𝑛)=l1→𝑆1=𝐿∼𝑑(𝐸1,...,𝐸𝑛),𝑆2=𝑆3=skipΓ(𝐿,𝐸1,...,𝐸𝑛)=l2→𝑆2=𝐿∼𝑑(𝐸1,...,𝐸𝑛),𝑆1=𝑆3=skipΓ(𝐿,𝐸1,...,𝐸𝑛)=l3→𝑆3=𝐿∼𝑑(𝐸1,...,𝐸𝑛),𝑆1=𝑆2=skip𝐿∼𝑑(𝐸1,...,𝐸𝑛)⇕Γ(𝑆1,𝑆2,𝑆3)(Shred2If)𝑆1⇕Γ𝑆(1)1,𝑆(1)2,𝑆(1)3𝑆2⇕Γ𝑆(2)1,𝑆(2)2,𝑆(2)3if(𝑔)𝑆1else𝑆2⇕Γ(if(𝑔)𝑆(1)1else𝑆(2)1),(if(𝑔)𝑆(1)2else𝑆(2)2),(if(𝑔)𝑆(1)3else𝑆(2)3)(Shred2For)𝑆⇕Γ𝑆1,𝑆2,𝑆3for(𝑥in𝑔1:𝑔2)𝑆⇕Γ(for(𝑥in𝑔1:𝑔2)𝑆1),(for(𝑥in𝑔1:𝑔2)𝑆2),(for(𝑥in𝑔1:𝑔2)𝑆3)Asbefore,shreddingproducessingle-levelstatements,andshreddingpreservessemanticswithrespectto⊢2-well-typedprograms.Lemma9(Shreddingproducessingle-levelstatements,⊢2).bIf𝑆⇕Γ𝑆1,𝑆2,𝑆3thenΓ⊢l1(𝑆1),Γ⊢l2(𝑆2),andΓ⊢l3(𝑆3).ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:18MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárLemma10(Semanticpreservationof⇕Γ,⊢2).IfΓ⊢2𝑆:l1and𝑆⇕Γ𝑆1,𝑆2,𝑆3thenJ𝑆K=J𝑆1;𝑆2;𝑆3K.Inaddition,wederivearesultabouttheeffectofsingle-levelstatementsonthestoreandweightof⊢2-well-typedprograms.Lemma11(Propertyof⊢2single-levelstatements).LetΓ𝜎,Γx,𝑆beaSlicStanprogram,andΓ⊢2𝑆:l1,and𝑆besingle-levelstatementoflevelℓ,Γ⊢2ℓ(𝑆).Thenthereexistuniquefunctions𝑓and𝜙,suchthatforany𝜎,x|=Γ𝜎,Γx:(1)Ifℓ=l1,thenJ𝑆K(𝜎)(𝑥)=(cid:0)𝑓(𝜎l1,xl1),𝜎l2,𝜎l3(cid:1),𝜙(𝜎l1)(xl1)(2)Ifℓ=l2,thenJ𝑆K(𝜎)(𝑥)=(cid:0)𝜎l1,𝑓(𝜎l1,𝜎l2,xl1,xl2),𝜎l3(cid:1),𝜙(𝜎l1,𝜎l2)(xl1,xl2)(3)Ifℓ=l3,thenJ𝑆K(𝜎)(𝑥)=(cid:0)𝜎l1,𝜎l2,𝑓(𝜎l1,𝜎l3,xl1,xl3)(cid:1),𝜙(𝜎l1,𝜎l3)(xl1,xl3)WegiveproofsforLemma9,10,and11inAppendixA.Theseresultsallowsustoderivethesecondkeytheoremofthispaper,Theorem2,which,similarlytoTheorem1,givesusaresultonthewayshreddingfactorisesthedensitydefinedbytheprogram.Here,andthroughoutthepaper,weusesubscriptstorefertospecificsubsetsofΓ.Forexample,Γl1standsforthesubsetoftheparametersΓx,suchthat𝑥:(𝜏,ℓ)∈Γl1ifandonlyif𝑥:(𝜏,ℓ)∈Γxandℓ=l1.Theorem2(Shreddinginducesafactorisationofthedensity(2)).SupposeΓ⊢2𝑆:l1withΓ=Γ𝜎,Γl1,Γl2,Γl3,𝑆⇕Γ𝑆1,𝑆2,𝑆3.Thenfor𝜎,𝜽1,𝜽2,𝜽3|=Γ𝜎,Γ1,Γ2,Γ3,and𝜎′,𝜎′′suchthatJ𝑆1K(𝜎)(𝜽1,𝜽2,𝜽3)=𝜎′,andJ𝑆2K(𝜎′)(𝜽1,𝜽2,𝜽3)=𝜎′′wehave:(1)J𝑆1K𝑝(𝜎)(𝜽1,𝜽2,𝜽3)=𝜙1(𝜽1)(2)J𝑆2K𝑝(𝜎′)(𝜽1,𝜽2,𝜽3)=𝜙2(𝜽1,𝜽2)(3)J𝑆3K𝑝(𝜎′′)(𝜽1,𝜽2,𝜽3)=𝜙3(𝜽1,𝜽3)Proof.ByapplyingLemma11toeachof𝑆1,𝑆2,𝑆3,whicharesingle-levelstatements(Lemma9).□3.2ConditionalIndependenceResultfor⊢2-Well-TypedProgramsTheorem3statesthekeytheoreticalresultofthispaper:thetypinginprogramswell-typedwithrespectto⊢2correspondstoaconditionalindependencerelationship.Inourproofs,weusethefactorisationcharacterisationofconditionalindependencestatedbyDefinition6.Thisisawell-knownresultintheliterature(e.g.[Murphy2012,Theorem2.2.1.]).Definition6(Characterisationofconditionalindependenceasfactorisation).Forvariables𝑥,𝑦,𝑧andadensity𝑝(𝑥,𝑦,𝑧),𝑥isconditionallyindependentof𝑦given𝑧withrespectto𝑝,written𝑥⊥⊥𝑝𝑦|𝑧,ifandonlyif∃𝜙1,𝜙2suchthat𝑝(𝑥,𝑦,𝑧)=𝜙1(𝑥,𝑧)𝜙2(𝑦,𝑧).Anequivalentformulationis𝑝(𝑥,𝑦|𝑧)=𝑝(𝑥|𝑧)𝑝(𝑦|𝑧).Weextendthenotionofconditionalindependencetoapplytoageneralfunction𝜙(𝑥,𝑦,𝑧),usingthenotation𝑥⊥𝜙𝑦|𝑧tomean∃𝜙1,𝜙2suchthat𝜙(𝑥,𝑦,𝑧)=𝜙1(𝑥,𝑧)𝜙2(𝑦,𝑧).Theorem3(⊢2-well-typedprogramsinduceaconditionalindependencerelationship).ForaSlicStanprogramΓ,𝑆suchthatΓ⊢2𝑆:l1,Γ=Γ𝜎,Γl1,Γl2,Γl3,andfor𝜎,𝜽1,𝜽2,𝜽3|=Γ𝜎,Γl1,Γl2,Γl3,wehave𝜽2⊥𝜙𝜽3|𝜽1.WhenJ𝑆K𝑝(𝜎)(𝜽1,𝜽2,𝜽3)∝𝑝(𝜽1,𝜽2,𝜽3),wehave𝜽2⊥⊥𝑝𝜽3|𝜽1.Proof.Let𝜽=𝜽1,𝜽2,𝜽3,𝑆⇕Γ𝑆1,𝑆2,𝑆3,andlet𝜎′and𝜎′′besuchthat𝜎′=J𝑆1K𝑠(𝜎)(𝜽),and𝜎′′=J𝑆2K𝑠(𝜎′)(𝜽).Then,bysemanticpreservationofshredding(Lemma10),wehaveJ𝑆K𝑝(𝜎)(𝜽)=J𝑆1;𝑆2;𝑆3K𝑝(𝜎)(𝜽)byLemma10ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:19C.CrossModelrealx1∼normal(0,1)realx2∼normal(0,1)realx3∼normal(x1+x2,1)realx4∼normal(x3,1)realx5∼normal(x3,1)(a)Asimple‘cross’model.𝑥3𝑥1𝑥2𝑥4𝑥5(b)Graphicalmodel.𝑥1⊥⊥𝑥2𝑥1⊥⊥𝑥4|{𝑥3}∪𝐴,∀𝐴⊆{𝑥2,𝑥5}𝑥1⊥⊥𝑥5|{𝑥3}∪𝐴,∀𝐴⊆{𝑥2,𝑥4}𝑥2⊥⊥𝑥4|{𝑥3}∪𝐴,∀𝐴⊆{𝑥1,𝑥5}𝑥2⊥⊥𝑥5|{𝑥3}∪𝐴,∀𝐴⊆{𝑥1,𝑥4}𝑥4⊥⊥𝑥5|{𝑥3}∪𝐴,∀𝐴⊆{𝑥1,𝑥2}(c)CIrelationships.Fig.3.Thecrossmodel,aswritteninSlicStan(a)withitsDAG(b)andCIrelationships(c).=J𝑆1K𝑝(𝜎)(𝜽)×J𝑆2K𝑝(𝜎′)(𝜽)×J𝑆3K𝑝(𝜎′′)(𝜽)byLemma2=𝜙1(𝜽1)×𝜙2(𝜽1,𝜽2)×𝜙3(𝜽1,𝜽3)byTheorem2=𝜙′(𝜽1,𝜽2)×𝜙3(𝜽1,𝜽3)forsome𝜙1,𝜙2,and𝜙3,𝜙′(𝜽1,𝜽2)=𝜙1(𝜽1)×𝜙2(𝜽1,𝜽2).Thus𝜽2⊥𝜙𝜽3|𝜽1bydefinitionof⊥𝜙.Suppose𝜙(𝜽1,𝜽2,𝜽3)∝𝑝(𝜽1,𝜽2,𝜽3).Then𝑝(𝜽1,𝜽2,𝜽3)=𝜙(𝜽1,𝜽2,𝜽3)×𝑍=𝜙′(𝜽1,𝜽2)×𝜙3(𝜽1,𝜽3)×𝑍=𝜙′(𝜽1,𝜽2)×𝜙′′(𝜽1,𝜽3),where𝑍isaconstantand𝜙′′(𝜽1,𝜽3)=𝜙3(𝜽1,𝜽3)×𝑍.Therefore,𝜽2⊥⊥𝑝𝜽3|𝜽1.□3.3ScopeoftheConditionalIndependenceResultWehaveshownthat⊢2-well-typedprogramsexhibitaconditionalindependencerelationshipintheirdensitysemantics.However,itisnotthecasethateveryconditionalindependencerelationshipcanbederivedfromthetypesystem.Inparticular,wecanonlyderiveresultsoftheform𝜽2⊥⊥𝜽3|𝜽1,where𝜽1,𝜽2,𝜽3isapartitioningof𝜽|=ΓxforaSlicStanprogramΓ𝜎,Γx,𝑆.Thatis,therelationshipincludesallparametersintheprogram.Wediscussthescopeofourapproachusinganexampleandshowasituationwheretryingtoderiveaconditionalindependenceresultthatdoesnotholdresultsinafailuretotypecheck.3.3.1Exampleof⊢2-well-typedprogram→conditionalindependence.ConsidertheCrossModelinFigure3,itsSlicStanprogram(a),itsdirectedgraphicalmodel(b)andtheconditionalindependence(CI)relationshipsthatholdforthatmodel(c).Outofthemanyrelationshipsabove,wecanderiveallrelationshipsthatinvolveallthevariables.Thatis,wecanuseourtypesystemtoderiveallconditionalindependencerelationshipsthatholdandareoftheform𝐴⊥⊥𝐵|𝐶,where𝐴,𝐵,𝐶issomepartitioningof{𝑥1,...,𝑥5}.However,notethefollowingpropertiesofconditionalindependence:𝐴⊥⊥𝐵|𝐶⇐⇒𝐵⊥⊥𝐴|𝐶and𝐴⊥⊥𝐵1,𝐵2|𝐶⇐⇒𝐴⊥⊥𝐵1|𝐶and𝐴⊥⊥𝐵2|𝐶Someoftherelationshipsabovecanbecombinedandwritteninotherways,e.g.𝑥1⊥⊥𝑥4|𝑥2,𝑥3and𝑥1⊥⊥𝑥5|𝑥2,𝑥3canbewrittenasasinglerelationship𝑥1⊥⊥𝑥4,𝑥5|𝑥2,𝑥3,thusexpressingthemasasinglerelationshipthatincludesallvariablesintheprogram.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:20MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárExploringdifferentmappingsbetweentheparameters𝑥1,...,𝑥5andthetypelevelsl1,l2,l3,forwhichtheaboveprogramtypechecks,wecanderiveallCIrelationshipsthatholdforthismodel,exceptforone:𝑥1⊥⊥𝑥2,whichwecannotderivewithourapproach.3.3.2Conditionalindependencerelationshipdoesnothold→typeerror.Supposethatwetrytoderivetheresult𝑥1⊥⊥𝑥2|𝑥3,𝑥4,𝑥5.ThisdoesnotholdforProgramC.ByTheorem3,wehavethataprogrambeing⊢2-well-typedimpliesthatl2⊥⊥l3|l1.So,wecanderive𝑥1⊥⊥𝑥2|𝑥3,𝑥4,𝑥5usingTheorem3ifweshowthatΓ⊢2𝑆:l1,forΓ={𝑥1:l2,𝑥2:l3,𝑥3:l1,𝑥4:l1,𝑥5:l1}and𝑆beingProgramC.TotypecheckΓ⊢2𝑆:l1,weneedtotypecheck𝑥3∼normal(𝑥1+𝑥2,1)atsomelevelℓ.Thus,by(Sample2)and(PrimCall2),𝑥1,𝑥2and𝑥3needtotypecheckatℓ.Thetypesof𝑥1,𝑥2and𝑥3arel2,l3andl1,respectively.So,using(ESub2),itmustbethecasethatl2≤ℓ,andl3≤ℓ,andl1≤ℓ.However,nosuchlevelexistsinourlowersemi-lattice,asl2andl3havenoupperbound.Therefore,typecheckingfailsandwecannotderive𝑥1⊥⊥𝑥2|𝑥3,𝑥4,𝑥5.4APPLICATION:DISCRETEPARAMETERSSUPPORTTHROUGHASEMANTICS-PRESERVINGTRANSFORMATIONThissectionpresentsthemainpracticalcontributionofourwork:asemantics-preservingprocedurefortransformingaprobabilisticprogramtoenablecombinedinferenceofdiscreteandcontinuousmodelparameters,whichwehaveimplementedforSlicStan.Theprocedurecorrespondstovariableelimination(VE)fordiscreteparametersimplementedintheprobabilisticprogramitself,whichcanbecombinedwithgradient-basedmethods,suchasHMC,toperforminferenceonallparameters.PPLsthathavegradient-basedmethodsinthecoreoftheirinferencestrategydonot,ingeneral,supportdirectlyworkingwithdiscreteparameters.Standisallowsdiscretemodelparametersaltogether,whilePyro[UberAILabs2017]andEdward2[Tranetal.2018]throwaruntimeerrorwheneverdiscreteparametersareusedwithinagradient-basedmethod.However,workingwithdiscreteparametersintheselanguagesisstillpossible,albeitinanimplicitway.Inmanycases,discreteparameterscanbemarginalisedoutmanually,andthendrawnconditionallyonthecontinuousparameters.Stan’suserguideshowsmanyexamplesofthisapproach[StanDevelopmentTeam2019a,Chapter7].Pyroprovidesanon-requestmarginalisationfunctionality,whichautomatesthisimplicittreatmentforplatedfactorgraphs[Obermeyeretal.2019].Thekeyideaoftheworkaroundistomarginaliseoutthediscreteparametersbyhand,sothattheresultingprogramcorrespondstoadensityfunctionthatdoesnotdependonanydiscreteparameters.Thatis,theuserwritesaprogramthatcomputes˝𝜽𝑑𝑝(𝜽𝑑,𝜽𝑐)=𝑝(𝜽𝑐),wherethedensitysemanticsoftheoriginalprogramwas𝑝(𝜽𝑑,𝜽𝑐)fordiscreteparameters𝜽𝑑andcontinuousparameters𝜽𝑐.ThisallowsforcontinuousparametersoftheprogramtobesampledwithHMC,orothergradient-basedinferencealgorithms,whereasthatwouldhavenotbeenpossiblefortheprogramwithbothdiscreteandcontinuouslatentvariables.BecauseaSlicStanprogramcomputesadensitydirectly,itiseasytomodifyittomarginaliseavariable.ForaSlicStanprogramΓ,𝑆,withparametersx|=Γx,andadiscreteparameter𝑧oftypeint⟨𝐾⟩,theprogramelim(int⟨𝐾⟩𝑧)𝑆≜factor(sum([target(𝑆)|𝑧in1:𝐾])11marginalises𝑧:Jfactor(sum([target(𝑆)|𝑧in1:𝐾]))K𝑝(𝜎)(x)=𝐾(cid:213)𝑧=1J𝑆K𝑝(𝜎)(x)∝𝐾(cid:213)𝑧=1𝑝(x)=𝑝(x\{𝑧})Inotherwords,wecaneasilymarginaliseoutalldiscretevariablesinaprobabilisticprogram,byencapsulatingtheentireprograminnestedloops(nestedarraycomprehensionexpressionsinourexamples).However,thisapproachbecomesinfeasibleformorethanafewvariables.Variable11Here,weassumethefunctionsumisavailableinthelanguage.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:21D.AHiddenMarkovModel(HMM)...int<2>z1∼bernoulli(theta[1]);int<2>z2∼bernoulli(theta[z1]);int<2>z3∼bernoulli(theta[z2]);datarealy1∼normal(phi[z1],1);datarealy2∼normal(phi[z2],1);datarealy3∼normal(phi[z3],1);E.Inefficientmarginalisation...factor(sum[target(factor(sum[target(factor(sum[target(z1∼bernoulli(theta[1]);z2∼bernoulli(theta[z1]);z3∼bernoulli(theta[z2]);y1∼normal(phi[z1],1);y2∼normal(phi[z2],1);y3∼normal(phi[z3],1);)|z1in1:2]);|z2in1:2]);|z3in1:2]);F.Efficientmarginalisation...real[2]f1=//newfactoronz2[sum([target(z1∼bernoulli(theta[1]);z2∼bernoulli(theta[z1]);y1∼normal(phi[z1],1);)|z1in1:2])|z2in1:2]real[2]f2=//newfactoronz3[sum([target(factor(f1[z2]);y2∼normal(phi[z2],1);z3∼bernoulli(theta[z2]);)|z2in1:2])|z3in1:2]factor(sum[target(factor(f2[z3]);y3∼normal(phi[z3],1);)|z3in1:2]);elimination[KollerandFriedman2009;ZhangandPoole1994]exploitsthestructureofamodeltodoaslittleworkaspossible.ConsidertheHMMsnippet(ProgramD)withthreediscrete(binary)hiddenvariables𝑧1,𝑧2and𝑧3,andobservedoutcomes𝑦1,𝑦2and𝑦3.Naivelymarginalisingoutthehiddenvariablesresultsinnestedloopsaroundtheoriginalprogram(ProgramE).Inthegeneralcaseof𝑁hiddenvariables,theresultingprogramisofcomplexity𝑂(2𝑁).However,thisiswasteful:expressionslike𝑧3∼bernoulli(𝜃[𝑧2])donotdependon𝑧1,andsodonotneedtobeinsideofthe𝑧1-eliminationloop.Variableelimination(VE)avoidsthisproblembypre-computingsomeofthework.ProgramFimplementsVEforthismodel:wheneliminatingavariable,say𝑧1,wepre-computestatementsthatinvolve𝑧1foreachpossiblevalueof𝑧1andstoretheresultingdensitycontributionsinanewfactor,𝑓1.Thisnewfactordependsonthevariablesinvolvedinthosestatements—theneighboursof𝑧1—inthiscasethatissolely𝑧2.Wethenrepeattheprocedurefortheothervariables,re-usingthealreadycomputedfactorswherepossible.InthespecialcaseofanHMM,andgivenasuitableeliminationorder,variableeliminationrecoversthecelebratedforwardalgorithm[Rabiner1989],whichhastimecomplexity𝑂(𝑁).OurgoalistoautomaticallytranslatethesourcecodeofProgramDtoProgramF,exploitingstaticallydetectableindependencepropertiesinthemodel.4.1GoalOurultimategoalistotransformaprogram𝑆withcontinuousparameters𝜽𝑐,discreteparameters𝜽𝑑,dataDanddensitysemanticsJ𝑆K𝑝(𝜎)(𝜽𝑑,𝜽𝑐,D)∝𝑝(𝜽𝑑,𝜽𝑐|D),intotwosubprograms:𝑆hmcand𝑆gen,suchthat:•Thedensitydefinedby𝑆hmcisthemarginal𝑝(𝜽𝑐|D),withthediscreteparameters𝜽𝑑marginalisedout.Thisfirststatement,𝑆hmc,representsthemarginalisationpartoftheprogramACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:22MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár(see§§4.3)andallowsforHamiltonianMonteCarlo(HMC)samplingof𝜽𝑐,asitdoesnotinvolveanydiscreteparameters.•Thedensitydefinedby𝑆genistheconditional𝑝(𝜽𝑑|𝜽𝑐,D).Thissecondstatement,𝑆gen,representsthegenerativepartoftheprogram(§§4.5)anditencodesawaytodraw𝜽𝑑generatively,withoutusingHMCoranotherheavy-weightinferencealgorithm.SimilarlytotheextendedSlicStanslicingbasedoninformation-flowtypeinference,herewealsowanttotransformandsliceintosub-programs,eachfocusingonasubsetoftheparameters,andpreservingtheoverallmeaning:J𝑆K𝑝∝𝑝(𝜽𝑑,𝜽𝑐|D)=𝑝(𝜽𝑐|D)×𝑝(𝜽𝑑|𝜽𝑐,D)∝J𝑆hmcK𝑝×J𝑆genK𝑝=J𝑆hmc;𝑆genK𝑝12Ourapproachperformsasemantics-preservingtransformation,guidedbyinformation-flowandtypeinference,whichcreatesanefficientprogram-specificinferencealgorithmautomatically,combiningHMCwithvariableelimination.4.2KeyInsightThekeypracticalinsightofthisworkistousetheadaptationofSlicStan’sleveltypesof§3anditsinformationflowtypesystemtorearrangetheprograminasemantics-preservingway,sothatdiscreteparameterscanbeforward-sampled,insteadofsampledusingaheavy-weightinferencealgorithm.Weachievethisbyaprogramtransformationforeachofthediscretevariables.Assumingthatweareapplyingthetransformationwithrespecttoavariable𝑧,weuse:•Thetop-levelinformationflowtypesystemΓ⊢𝑆:datafrom§§2.2,whichinvolvestheleveltypesdata≤model≤genqant.ThispartitionsthemodelledvariablesxintodataD,modelparameters𝜽andgeneratedquantities𝑄.Whenweusetypeinferencefor⊢inconjunctionwithshredding𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄(§§2.5),weslicethestatement𝑆intoadatapart𝑆𝐷(involvingonlyvariablesinD),anon-generativepart𝑆𝑀(involvingDand𝜽)andagenerativepart𝑆𝑄(involvingD,𝜽and𝑄).•Theconditionalindependenceinformationflowtypesystem,Γ⊢2𝑆:l1from§3,whichusesalowersemi-latticeofleveltypesl1≤l2,l1≤l3.A⊢2-well-typedprograminducesaconditionalindependencerelationship:l2-variablesareconditionallyindependentofl3-variablesgivenl1-variables:xl2⊥⊥xl3|xl1,wherex=xl1,xl2,xl3=𝜽,D.Whenweusetypeinferencefor⊢2inconjunctionwithshredding𝑆⇕Γ𝑆1,𝑆2,𝑆3(§§2.5),weisolate𝑆2:apartoftheprogramthatdoesnotinterferewith𝑆3.Wecanmarginaliseoutl2-variablesinthatsub-statementonly,keepingtherestoftheprogramunchanged.•ThediscretevariabletransformationrelationΓ,𝑆𝑧−→Γ′,𝑆′(definedin§§§4.6.2),whichtakesaSlicStanprogramΓ,𝑆thathasdiscretemodelparameter𝑧,andtransformsittoaSlicStanpro-gramΓ′,𝑆′,where𝑧isnolongeramodel-levelparameterbutinsteadoneoflevelgenqant.Wedefinetherelationintermsof⊢and⊢2asperthe(ElimGen)rule.4.3VariableEliminationVariableelimination(VE)[KollerandFriedman2009;ZhangandPoole1994]isanexactinferencealgorithmoftenphrasedintermsoffactorgraphs.Itcanbeusedtocomputepriororposteriormarginaldistributionsbyeliminating,onebyone,variablesthatareirrelevanttothedistributionofinterest.VEusesdynamicprogrammingcombinedwithacleveruseofthedistributivelawofmultiplicationoveradditiontoefficientlycomputeanestedsumofaproductofexpressions.12Thisexpressionissimplifiedforreadability.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:23𝑧1𝑏(𝑧1|𝜃1)𝑏(𝑦1|𝜙𝑧1)𝑦1𝑏(𝑧2|𝜃𝑧1)𝑧2𝑏(𝑦2|𝜙𝑧2)𝑦2𝑏(𝑧3|𝜃𝑧2)𝑧3𝑏(𝑦3|𝜙𝑧3)𝑦3(a)Toeliminate𝑧1,weremove𝑧1andallitsneigh-bouringfactors(inred).Createanewfactor𝑓1,bysummingout𝑧1fromtheproductofthesefactors.𝑓1(𝑧2,𝑦1)=˝𝑧1[𝑏(𝑧1|𝜃1)×𝑏(𝑦1|𝜙𝑧1)×𝑏(𝑧2|𝜃𝑧1)(cid:3)𝑧2𝑏(𝑦2|𝜙𝑧2)𝑦2𝑦1𝑏(𝑧3|𝜃𝑧2)𝑧3𝑏(𝑦3|𝜙𝑧3)𝑦3(b)Connect𝑓1(ingreen)totheformerneighboursof𝑧1.Theremainingfactorgraphdefinesthemar-ginal𝑝(𝑧2,𝑧3|y).𝑓1(𝑧2,𝑦1)𝑧2𝑏(𝑦2|𝜙𝑧2)𝑦2𝑦1𝑏(𝑧3|𝜃𝑧2)𝑧3𝑏(𝑦3|𝜙𝑧3)𝑦3(c)Toeliminate𝑧2,weremove𝑧2andallitsneigh-bouringfactors(inred).Createanewfactor𝑓2,bysummingout𝑧2fromtheproductofthesefactors.𝑓2(𝑧3,𝑦2,𝑦1)=˝𝑧2[𝑓1(𝑧2,𝑦1)×𝑏(𝑦2|𝜙𝑧2)×𝑏(𝑧3|𝜃𝑧2)(cid:3)𝑧3𝑏(𝑦3|𝜙𝑧3)𝑦3𝑦2𝑦1(d)Connect𝑓2(ingreen)totheformerneighboursof𝑧2.Theremainingfactorgraphdefinesthemar-ginal𝑝(𝑧3|y).Fig.4.Stepbystepexampleofvariableelimination.Wealreadysawanexampleofvariableeliminationin§3(ProgramsAandB).Theideaistoeliminate(marginaliseout)variablesonebyone.Toeliminateavariable𝑧,wemultiplyallofthefactorsconnectedto𝑧toformasingleexpression,thensumoverallpossiblevaluesfor𝑧tocreateanewfactor,remove𝑧fromthegraph,andfinallyconnectthenewfactortoallformerneighbours13of𝑧.RecallProgramD,withlatentvariables𝑧1,𝑧2,𝑧3andobserveddatay=𝑦1,𝑦2,𝑦3.Figure4showstheVEalgorithmstep-by-stepappliedtothisprogram.Weeliminate𝑧1togetthemarginalon𝑧2and𝑧3(4aand4b),theneliminate𝑧2togetthemarginalon𝑧3(4cand4d).4.4ConditionalIndependenceRelationshipsandInferringtheMarkovBlanketThekeypropertywearelookingfor,inordertobeabletomarginaliseoutavariableindependentlyofanother,isconditionalindependencegivenneighbouringvariables.Ifweshreda⊢2-well-typedprograminto𝑆1,𝑆2and𝑆3,andthinkofJ𝑆1K𝑝,J𝑆2K𝑝andJ𝑆3K𝑝asfactors,itiseasytovisualisethefactorgraphcorrespondingtotheprogram:itisasinFigure5a.Eliminatingallxl2variables,endsuponlymodifyingtheJ𝑆2K𝑝factor(Figure5b).WhenusingVEtomarginaliseoutaparameter𝑧,wewanttofindthesmallestsetofotherparameters𝐴,suchthat𝑧⊥⊥𝐵|𝐴,where𝐵istherestoftheparameters.Theset𝐴isalsocalled𝑧’sminimalMarkovblanketorMarkovboundary.Onceweknowthisset,wecanensurethatweinvolvethesmallestpossiblenumberofvariablesin𝑧’selimination,whichisimportanttoachieveaperformantalgorithm.Forexample,whenweeliminate𝑧1inProgramD,both𝑧2and𝑦1needtobeinvolved,as𝑧1sharesafactorwiththem.Bycontrast,thereisnoneedtoinclude𝑦2,𝑧3,𝑦3andthestatementsassociatedwiththem,astheyareunaffectedby𝑧1,given𝑧2.Thevariables𝑦1and𝑧2form𝑧1’sMarkovblanket:giventhesevariables,𝑧1isconditionallyindependentofallothervariables.Thatis,𝑧1⊥⊥𝑧3,𝑦2,𝑦3|𝑧2,𝑦1.Thetypesystemwepresentin§3cantellusiftheconditionalindependencerelationshipxl2⊥⊥xl3|xl1holdsforaconcretepartitioningofthemodelledvariablesx=xl1,xl2,xl3.But13‘Neighbours’referstothevariableswhichareconnectedtoafactorwhichconnectsto𝑧.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:24MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárxl1J𝑆1K𝑝J𝑆2K𝑝xl2J𝑆3K𝑝xl3(a)A⊢2-well-typedprogramwithparametersx.xl1J𝑆1K𝑝˝xl2J𝑆2K𝑝J𝑆3K𝑝xl3(b)Eliminatingxl2consistsofmodifyingonlyJ𝑆2K𝑝.Fig.5.ThefactorgraphandVEinducedbytheshedding𝑆⇕Γ𝑆1,𝑆2,𝑆3accordingtothesemi-latticel1≤l2,l3.tofindtheMarkovblanketofavariable𝑧wewanttoeliminate,werelyontypeinference.Wedefineaperformanceorderingbetweentheleveltypesl3≺l1≺l2,whereourfirstpreferenceisforvariablestobeoflevell3,levell1isoursecondpreference,andl2isourlastresort.Inourimplementation,weusebidirectionaltype-checking[PierceandTurner2000]tosynthesisehardconstraintsimposedbythetypesystem,andresolvethem,whileoptimisingforthesoftconstraintsgivenbythe≺ordering.Thismaximisesthenumberofvariablesthatareconditionallyindependentof𝑧givenitsblanket(l3)andminimisesthenumberofvariablesformingtheblanket(l1).Fixing𝑧tobeofl2level,andl2beingtheleastpreferredoption,ensuresthatonly𝑧andvariablesdependenton𝑧throughdeterministicassignmentareofthatlevel.4.5SamplingtheDiscreteParametersVariableeliminationgivesawaytoefficientlymarginaliseoutavariable𝑧fromamodeldefiningdensity𝑝(x),toobtainanewdensity𝑝(x\{𝑧}).InthecontextofSlicStan,thismeanswehavethetoolstoeliminatealldiscreteparameters𝜽𝑑,fromadensity𝑝(D,𝜽𝑐,𝜽𝑑)ondataD,continuousparameters𝜽𝑐anddiscreteparameters𝜽𝑑.Theresultingmarginal˝𝜽𝑑𝑝(D,𝜽𝑐,𝜽𝑑)=𝑝(D,𝜽𝑐)doesnotinvolvediscreteparameters,andthereforewecanusegradient-basedmethodstoinfer𝜽𝑐.However,themethodsofardoesnotgiveusawaytoinferthediscreteparameters𝜽𝑑.Toinferthese,weobservethat𝑝(x)=𝑝(x\{𝑧})𝑝(𝑧|x\{𝑧}),whichmeansthatwecanpreservethesemanticsoftheoriginalmodel(whichdefines𝑝(x)),byfindinganexpressionfortheconditional𝑝(𝑧|x\{𝑧}).Ifx1,x2isapartitioningofx\{𝑧}suchthat𝑧⊥⊥x2|x1,then(fromDefinition6)𝑝(x)=𝜙1(𝑧,x1)𝜙2(x1,x2)forsomefunctions𝜙1and𝜙2.Thus,𝑝(𝑧|x\{𝑧})=𝜙1(𝑧,x1)·(𝜙2(x1,x2)/𝑝(x\{𝑧}))∝𝜙1(𝑧,x1).Inthecasewhen𝑧isadiscretevariableoffinitesupport,wecancalculatetheconditionalprobabilityexactly:𝑝(𝑧|x\{𝑧})=𝜙1(𝑧,x1)˝𝑧𝜙1(𝑧,x1).WecanapplythiscalculationtothefactorisationofaprogramΓ⊢2𝑆thatisinducedbyshredding(Theorem2).Inthatcase,xl2,xl1,J𝑆2K𝑝playtherolesof𝑧,x1,and𝜙1,respectively.Consequently,weobtainaformulafordrawingxl2conditionalontheotherparameters:xl2∼categorical(cid:16)hJ𝑆2K𝑝(xl2,xl1)˝xl2J𝑆2K𝑝(xl2,xl1)|xl2∈supp(xl2)i(cid:17).4.6ASemantics-PreservingTransformationRuleInthissectionwedefineasource-to-sourcetransformationthatimplementsasinglestepofvariableelimination.Thetransformationre-writesaSlicStanprogramΓ,𝑆withadiscretemodel-levelparameter𝑧,toaSlicStanprogram,where𝑧isagenqant-levelparameter.Combiningtherulewiththeshreddingpresentedin§2resultsinsupportforefficientinference(see§§4.8fordiscussionoflimitations)ofbothdiscreteandcontinuousrandomvariables,wherecontinuousvariablescanbeinferredusinggradient-basedmethods,suchasHMCorvariationalinference,whilediscretevariablesaregeneratedusingancestralsampling.ThetransformationallowsforSlicStanprogramsACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:25withexplicituseofdiscreteparameterstobetranslatedtoStan.Weshowastep-by-stepexampleofourdiscreteparametertransformationin§§4.7.4.6.1The𝜙,elimandgenderivedforms.Weintroducethreederivedformsthatallowustostatetheruleconcisely.VariableEliminationDerivedFormselim(int⟨𝐾⟩𝑧)𝑆≜factor(sum([target(𝑆)|𝑧in1:𝐾]))𝜙(int⟨𝐾1⟩𝑧1,...,int⟨𝐾𝑁⟩𝑧𝑁)𝑆≜[...[target(𝑆)|𝑧1in1:𝐾1]|···|𝑧𝑁in1:𝐾𝑁]gen(int⟨𝐾⟩𝑧)𝑆≜𝑧∼categorical([target(𝑆)|𝑧in1:𝐾])Theeliminationexpressionelim(int⟨𝐾⟩𝑧)𝑆addsanewfactorthatisequivalenttomarginal-ising𝑧in𝑆.Inotherwords,Jelim(int⟨𝐾⟩𝑧)𝑆K𝑝(𝜎)(x)=˝𝐾𝑧=1J𝑆K𝑝(𝜎)(x)(seeLemma14).A𝜙-expression𝜙(int⟨𝐾1⟩𝑧1,...,int⟨𝐾𝑁⟩𝑧𝑁)𝑆simplycomputesthedensityofthestatement𝑆inamultidimensionalarrayforallpossiblevaluesofthevariables𝑧1,...𝑧𝑁.Inotherwords,J(𝑓=𝜙(int⟨𝐾1⟩𝑧1,...,int⟨𝐾𝑁⟩𝑧𝑁)𝑆);factor(𝑓[𝑧1]...[𝑧𝑁])K𝑝(𝜎)(x)=J𝑆K𝑝(𝜎)(x)(Lemma14).The𝜙-expressionallowsustopre-computealltheworkthatwemayneedtodowhenmarginalis-ingotherdiscretevariables,whichresultsinefficientnesting.Finally,thegenerationexpressioncomputestheconditionalofavariable𝑧giventherestoftheparameters,asin§§4.5(seeLemma15).4.6.2Eliminatingasinglevariable𝑧.The(ElimGen)rulebelowspecifiesasemantics-preservingtransformationthattakesaSlicStanprogramwithadiscretemodel-levelparameter𝑧,andtrans-formsittoonewhere𝑧isgenqant-levelparameter.Inpractice,weapplythisruleonceperdiscretemodel-levelparameter,whicheliminatesthoseparametersone-by-one,similarlytothevariableeliminationalgorithm.AndlikeinVE,theorderinginwhichweeliminatethosevariablescanimpactperformance.The(ElimGen)rulemakesuseoftwoauxiliarydefinitionsthatwedefinenext.Firstly,theneighboursof𝑧,Γne,aredefinedbytherelationne(Γ,Γ′,𝑧)(Definition7),whichlooksfornon-dataandnon-continuousl1-variablesinΓ′.Definition7(Neighboursof𝑧,ne(Γ,Γ′,𝑧)).Fora⊢typingenvironmentΓ,a⊢2typingenviron-mentΓ′=Γ′𝜎,Γ′xandavariable𝑧∈dom(Γ′x),theneighboursof𝑧aredefinedas:ne(Γ,Γ′,𝑧)≜{𝑥:(𝜏,ℓ)∈Γ′x|ℓ=l1andΓ(𝑥)=(int⟨𝐾⟩,model)forsome𝐾}Secondly,st(𝑆2)(Definition8)isastatementthathasthesamestoresemanticsas𝑆2,butdensitysemanticsof1:Jst(𝑆2)K𝑠=J𝑆2K𝑠,butJst(𝑆2)K𝑝=1.Thisensuresthatthetransformationpreservesboththedensitysemanticsandthestoresemanticsof𝑆andisneededbecausegen(𝑧)𝑆2discardsanystorecomputedby𝑆2,thusonlycontributingtotheweight.Definition8.Givenastatement𝑆,wedefinethestatementst(𝑆)byreplacingallfactor(𝐸)-and𝐿∼𝑑(𝐸1,...,𝐸𝑛)-substatementsin𝑆byskip(seeAppendixAfortheprecisedefinition).Theelim-genrule:(ElimGen)Γ(𝑧)=(int⟨𝐾⟩,model)Γne=ne(Γ,Γ𝑀,𝑧)𝑆′=𝑆𝐷;𝑆′𝑀;𝑆𝑄𝑆′𝑀=𝑆1;𝑓=𝜙(Γne){elim(int⟨𝐾⟩𝑧)𝑆2};factor(𝑓[dom(Γne)]);𝑆3;gen(𝑧)𝑆2;st(𝑆2)Γ⊢𝑆:data𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄Γ𝑧−→Γ𝑀Γ𝑀⊢2𝑆𝑀:l1𝑆𝑀⇕Γ𝑀𝑆1,𝑆2,𝑆3Γ′⊢𝑆′:dataΓ,𝑆𝑧−→Γ′,𝑆′ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:26MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárWecanusethe(ElimGen)ruletotransformsaSlicStanprogram,withrespecttoaparameter𝑧,asdescribedbyAlgorithm1.Thisinvolvesthreemainsteps:(1)Separateout𝑆𝑀—themodel-levelsub-partof𝑆—usingthetop-leveltypesystem⊢(line1ofAlgorithm1).(2)Separateout𝑆2—thepartof𝑆𝑀thatinvolvesthediscreteparameter𝑧—usingtheconditionalindependencetypesystem⊢2(lines2–8).(3)PerformasingleVEstepbymarginalisingout𝑧in𝑆2andsample𝑧fromtheconditionalprobabilityspecifiedby𝑆2(lines10–11).Algorithm1.Singlestepofapplying(ElimGen)Arguments:(Γ,𝑆),𝑧//Aprogram(Γ,𝑆);thevariable𝑧toeliminateRequires:Γ⊢𝑆:data//(Γ,𝑆)iswell-typedReturns:(Γ′,𝑆′)//Thetransformedprogram1:Slice(Γ,𝑆)into𝑆𝐷,𝑆𝑀,𝑆𝑄accordingto𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄.2:DeriveincompleteΓ𝑀fromΓbasedonΓ𝑧−→Γ𝑀.//dataofΓisoflevell1inΓ𝑀.3://Continuousmodelvar.ofΓarel1inΓ𝑀.4://𝑧isoflevell2inΓ𝑀.5://Allothermodelvariablesaregiven6://atypelevelplaceholderinΓ𝑀.7:InfermissingtypesofΓ𝑀accordingtoΓ𝑀⊢2𝑆𝑀:l1.8:Slice(Γ𝑀,𝑆𝑀)into𝑆1,𝑆2,𝑆3accordingto𝑆𝑀⇕Γ𝑀𝑆1,𝑆2,𝑆3.9:10:Γne=ne(Γ,Γ𝑀,𝑧)//Determinethediscreteneighboursof𝑧.11://Eliminate𝑧andre-generate𝑧.𝑆′𝑀=(𝑆1;𝑓=𝜙(Γne){elim(int⟨𝐾⟩𝑧)𝑆2};factor(𝑓[dom(Γne)]);𝑆3;gen(𝑧)𝑆2;st(𝑆2))12:𝑆′=𝑆𝐷;𝑆′𝑀;𝑆𝑄13:InferanoptimalΓ′accordingtoΓ′⊢𝑆′:data14:return(Γ′,𝑆′)Allothersub-statementsoftheprogram,𝑆𝐷,𝑆1,𝑆3and𝑆𝑄,staythesameduringthetransfor-mation.Byisolating𝑆2andtransformingonlythispartoftheprogram,wemakesurewedonotintroducemoreworkthannecessarywhenperformingvariableelimination.Toefficientlymarginaliseout𝑧,wewanttofindtheMarkovboundaryof𝑧givenalldataandcontinuousmodelparameters:thedataisgiven,andmarginalisationhappensinsidethecontinuousparametersinferenceloop,sowecanseecontinuousparametersasgivenforthepurposeofdiscreteparametersmarginalisation.Thuswearelookingfortherelationship:𝑧⊥⊥𝜽𝑑2|D,𝜽𝑐,𝜽𝑑1,whereDisthedata,𝜽𝑐arethecontinuousmodel-levelparameters,𝜽𝑑1isasubsetofthediscretemodel-levelparametersthatisassmallaspossible(theMarkovblanket),and𝜽𝑑2istherestofthediscretemodel-levelparameters.Wecanfindanoptimalpartitioningofthediscreteparameters𝜽𝑑1,𝜽𝑑2thatrespectsthisrelationshipofinterestusingthetypesystemfrom§3togetherwithtypeinference.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:27ThejudgementΓ𝑀⊢2𝑆𝑀:l1inducesaconditionalindependencerelationshipoftheformxl2⊥⊥xl3|xl1,wherex|=Γx(Theorem3).TherelationΓ𝑧−→Γ𝑀(Definition9)constrainstheformofΓ𝑀basedonΓ.Thisisneededtomakesureweareworkingwitharelationshipoftheformweareinterestedin—𝑧⊥⊥𝜽𝑑2|D,𝜽𝑐,𝜽𝑑1—andthatbasetypes𝜏arethesamebetweenΓandΓ𝑀.Inparticular,Γ𝑧−→Γ𝑀constrains𝑧tobetheonlyl2parameterinΓ𝑀andalldataandcontinuousmodel-levelparametersofΓarel1inΓ𝑀.Note,dom(Γ𝑀)⊆dom(Γ)andΓ𝑀onlycontainsvariablesthatareoflevelmodelandbelowinΓ.VariablesthatareoflevelgenqantinΓarenotinΓ𝑀.Definition9(Γ𝑧−→Γ′).Fora⊢typingenvironmentΓanda⊢2typingenvironmentΓ′,avariable𝑧andastatement𝑆,wehave:Γ𝑧−→Γ′=Γ(𝑧)=(𝜏,model)andΓ′x,l2={𝑧:𝜏,l2}forsome𝜏𝑥:(𝜏,ℓ)∈Γsuchthatℓ≤model⇐⇒𝑥:(𝜏,ℓ′)∈Γ′forsomeℓ′∈{l1,l2,l3}𝑥:(𝜏,data)∈Γ→𝑥:(𝜏,l1)∈Γ′𝑥:(𝜏,model)∈Γxand𝜏=realor𝜏=real[]...[]→𝑥:(𝜏,l1)∈Γ′Followingconventionfromearlierinthepaper,weuselevelsubscriptstorefertospecificsubsetsofΓ:intheabovedefinition,Γ′x,l2referstothesubsetofparametersxinΓ′,whichareoflevell2.4.7MarginalisingMultipleVariables:AnexampleToeliminatemorethanonediscreteparameter,weapplythe(ElimGen)rulerepeatedly.Here,weworkthroughafullexample,showingthedifferentstepsofthisrepeated(ElimGen)transformation.ConsideranextendedversionoftheHMMmodelfromthebeginningofthissection(ProgramD),reformulatedtoincludetransformedparameters:G.AnextendedHMM𝑆=real[2]phi∼beta(1,1);real[2]theta∼beta(1,1);realtheta0=theta[0];int<2>z1∼bernoulli(theta0);realtheta1=theta[z1];int<2>z2∼bernoulli(theta1);realtheta2=theta[z2];int<2>z3∼bernoulli(theta2);realphi1=phi[z1];realphi2=phi[z2];realphi3=phi[z3];datarealy1∼normal(phi1,1);datarealy2∼normal(phi2,1);datarealy3∼normal(phi3,1);realtheta3=theta[z3];intgenz∼bernoulli(theta4);ThetypingenvironmentΓ={𝑦1,2,3:(real,data),𝜙:(real[2],model),𝜃:(real[2],model),𝜃0,1,2:(real,model),𝜙1,2,3:(real,model),𝑧1,2,3:(int<2>,model),𝜃3:(real,genqant),𝑔𝑒𝑛𝑧:(int<2>,genqant)}Thevariablesweareinterestedintransformingare𝑧1,𝑧2and𝑧3:thesearethemodel-leveldiscreteparametersofProgramG.Thevariablegenzisalreadyatgenqantlevel,sowecansamplethiswithancestralsampling(noneedforautomaticmarginalisation).Weeliminate𝑧1,𝑧2and𝑧3onebyone,inthatorder.Theorderofeliminationgenerallyhasasignificantimpactonthecomplexityoftheresultingprogram(seealso§§4.8),butwedonotfocusonhowtochooseanorderinghere.Theproblemoffindinganoptimalorderingiswell-studied[Amir2010;Arnborgetal.1987;Kjærulff1990]andisorthogonaltothefocusofourwork.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:28MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár4.7.1Eliminating𝑧1.Eliminatingasinglevariablehappensinthreesteps,asshowninFigure6:standardshreddinginto𝑆𝐷,𝑆𝑀and𝑆𝑄,conditionalindependenceshreddingof𝑆𝑀into𝑆1,𝑆2and𝑆3,andcombiningeverythingbasedon(ElimGen).(1)Standardshredding:𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄.Firstly,weseparateoutthepartsoftheprogramthatdependondiscreteparametersgeneratively.Thatisanypartoftheprogramthatwouldbeingeneratedquantitieswithrespecttotheoriginalprogram.Inourcase,thisincludesthelasttwolinesin𝑆.Thiswouldalsoincludethegenpartsofthetransformprogram,thatdrawdiscreteparametersasgeneratedquantities.Thus,𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄,where𝑆𝐷isempty,(1)Standardshreddingof𝑆1𝑆𝐷=skip;2𝑆𝑀=phi∼beta(1,1);3theta∼beta(1,1);4theta0=theta[0];5z1∼bernoulli(theta0);6theta1=theta[z1];7z2∼bernoulli(theta1);8theta2=theta[z2];9z3∼bernoulli(theta2);10phi1=phi[z1];11phi2=phi[z2];12phi3=phi[z3];13y1∼normal(phi1,1);14y2∼normal(phi2,1);15y3∼normal(phi3,1);16𝑆𝑄=theta3=theta[z3];17genz∼bernoulli(theta3);(2)CIshreddingof𝑆𝑀1𝑆1=phi∼beta(1,1);2theta∼beta(1,1);3theta0=theta[0];4𝑆2=z1∼bernoulli(theta0);5theta1=theta[z1];6z2∼bernoulli(theta1);7phi1=phi[z1];8y1∼normal(phi1,1);9𝑆3=theta2=theta[z2];10z3∼bernoulli(theta2);11phi2=phi[z2];12phi3=phi[z3];13y2∼normal(phi2,1);14y3∼normal(phi3,1);(3)Applying(ElimGen):ProgramG-11phi∼beta(1,1);2theta∼beta(1,1);3theta0=theta[0];45f1=𝜙([int<2>z2]){6elim(int<2>z1){7z1∼bernoulli(theta0);8theta1=theta[z1];9z2∼bernoulli(theta1);10phi1=phi[z1];11y1∼normal(phi1,1);12}}13factor(f1[z2]);1415theta2=theta[z2];16z3∼bernoulli(theta2);17phi2=phi[z2];18phi3=phi[z3];19y2∼normal(phi2,1);20y3∼normal(phi3,1);2122gen(intz1){23z1∼bernoulli(theta0);24theta1=theta[z1]25z2∼bernoulli(theta1);26phi1=phi[z1];27y1∼normal(phi1,1);28}29theta1=theta[z1];30phi1=phi[z1];3132theta3=theta[z3];33genz∼bernoulli(theta3);Fig.6.Step-by-stepeliminationof𝑧1inProgramG.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:29𝑆𝑄=(theta3=theta[z3];genz∼bernoulli(theta3)),and𝑆𝑀istherestoftheprogram(seeFigure6,(1)).(2)Conditionalindependenceshredding:𝑆𝑀⇕Γ𝑀𝑆1,𝑆2,𝑆3.Inthenextstep,wewanttoestablishaconditionalindependencerelationship𝑧1|𝐴⊥⊥y,𝜙0,𝜃0,𝐵,where𝑧1issomediscreteparameterand𝐴,𝐵isapartitioningoftherestofthediscreteparametersinthemodel:{𝑧2,𝑧3}.Wederiveanew,⊢2typingenvironmentΓ𝑀,usingΓ𝑧−→Γ𝑀:Γ𝑀={𝑦1,2,3:(real,l1),𝜙:(real[2],l1),𝜃:(real[2],l1),𝑧1:(int<2>,l2),𝑧2:(int<2>,?),𝑧3:(int<2>,?)𝜃0:(real,l1),𝜃1,2:(real,?),𝜙1,2,3:(real,?)}Here,weusethenotation?foratypeplaceholder,whichwillbeinferredusingtypeinference.TheoptimalΓ𝑀underthetypeinferencesoftconstraintl3≺l1≺l2suchthatΓ𝑀⊢2𝑆𝑀:l1issuchthatthelevelsof𝜃1and𝜙1arel2,𝑧2isl1and𝜃2,𝜙2and𝜙3arel3.Shreddingthengivesus𝑆𝑀⇕Γ𝑀𝑆1,𝑆2,𝑆3,asinFigure6,(2).(3)Combiningbasedon(ElimGen).Havingrearrangedtheprogramintosuitablesub-statements,weuse(ElimGen)togetProgramG-1(Figure6,(3))and:Γ′={𝑦1,2,3:(real,data),𝜙:(real,model),𝜃1:(real,genqant),𝜃0,2:(real,model),𝜙1:(real,genqant),𝜙2,3:(real,model),𝑧1:(int,genqant),𝑧2,3:(int<2>,model),𝜃3:(real,genqant),𝑔𝑒𝑛𝑧:(int<2>,genqant)}Eliminating𝑧2.Weapplythesameproceduretoeliminatethenextvariable,𝑧2,fromtheupdatedProgramG-1.Thevariable𝑧1isnolongeramodel-levelparameter,thustheonlyneighbouringparameterof𝑧2is𝑧3.Notealsothatthecomputationofthefactor𝑓1doesnotincludeanyfreediscreteparameters(both𝑧1and𝑧2arelocaltothecomputationduetoelimand𝜙).Thus,wedonotneedtoincludethecomputationofthisfactoranywhereelseintheprogram(itdoesnotgetnestedintoothercomputations).Weobtainanewprogram,ProgramG-2:ProgramG-21phi∼beta(1,1);2theta∼beta(1,1);3theta0=theta[0];45f1=𝜙([int<2>z2]){elim(int<2>z1){6z1∼bernoulli(theta0);7theta1=theta[z1];8z2∼bernoulli(theta1);9phi1=phi[z1];10y1∼normal(phi1,1);11}}12f2=𝜙([int<2>z3]){elim(int<2>z2){13factor(f1[z2]);14theta2=theta[z2];15z3∼bernoulli(theta2);16phi2=phi[z2];17y2∼normal(phi2,1);18}}1920factor(f2[z3]);21phi3=phi[z3];22y3∼normal(phi3,1);2324gen(intz2){25factor(f1[z2]);26theta2=theta[z2];27z3∼bernoulli(theta2);28phi2=phi[z2];29y2∼normal(phi2,1);30}31theta2=theta[z2];32phi2=phi[z2];33ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:30MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár34gen(intz1){35z1∼bernoulli(theta0);36theta1=theta[z1];37z2∼bernoulli(theta1);38phi1=phi[z1];39y1∼normal(phi1,1);40}41theta1=theta[z1];42phi1=phi[z1];4344theta3=theta0[z3];45genz∼bernoulli(theta3);.Eliminating𝑧3.Finally,weeliminate𝑧3,whichistheonlydiscretemodel-levelparameterleftintheprogram.Thus,𝑧3hasnoneighboursand𝑓3isofarity0:itisarealnumberinsteadofavector.ThefinalprogramgeneratedbyourimplementationisProgramG-3:ProgramG-31phi0∼beta(1,1);2theta0∼beta(1,1);34f1=𝜙(int<2>z2){elim(int<2>z1){5z1∼bernoulli(theta0);6theta1=theta[z1];7z2∼bernoulli(theta1);8phi1=phi[z1];9y1∼normal(phi1,1);10}}1112f2=𝜙(int<2>z3){elim(int<2>z2){13factor(f1[z2]);14theta2=theta[z2];15z3∼bernoulli(theta2);16phi2=phi[z2];17y2∼normal(phi2,1);18}}1920f3=𝜙(){elim(int<2>z3){21factor(f2[z3]);22phi3=phi[z3];23y3∼normal(phi3,1);24}}2526factor(f3);27gen(intz3){28factor(f2[z3]);29phi3=phi[z3];;30y3∼normal(phi3,1);31}32phi3=phi[z3];33gen(intz2){34factor(f1[z2]);35theta2=theta[z2];;36z3∼bernoulli(theta2);37phi2=phi[z2];;38y2∼normal(phi2,1);39}40theta2=theta[z2];41phi2=phi[z2];42gen(intz1){43z1∼bernoulli(theta0);44theta1=theta[z1];45z2∼bernoulli(theta1);46phi1=phi[z1];47y1∼normal(phi1,1);48}49theta1=theta[z1];50phi1=phi[z1];5152gen3=theta[z3];53genz∼bernoulli(theta3);4.8RelatingtoVariableEliminationandComplexityAnalysisAssumeD,𝜽𝑑,and𝜽𝑐arethedata,discretemodel-levelparameters,andcontinuousmodel-levelparameters,respectively.As𝑆2isasingle-levelstatementoflevell2,thedensitysemanticsof𝑆2isoftheform𝜓(xl1,xl2)=𝜓(D,𝜽𝑐,𝜽𝑑,l1,𝑧)(Lemma11).Aselim(int⟨𝐾⟩𝑧)_bindsthevariable𝑧and𝜙(Γne){_}bindsthevariablesindom(Γne),theex-pression𝜙(Γne){elim(int⟨𝐾⟩𝑧)𝑆2dependsonlyoncontinuousparametersanddata,anditcontainsnofreementionsofanydiscretevariables.Thismeansthattheexpressionwillbeoflevell1andACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:31shreddedinto𝑆1duringthemarginalisationofanysubsequentdiscretevariable𝑧′.Thesubstate-ment𝑆2willalwaysbesomesub-statementoftheoriginalprogram(priortoanytransformations),uptopotentiallyseveralconstantfactorsoftheformfactor(𝑓[dom(Γne)]).Thisobservationmakesiteasytoreasonabouthowrepeatedapplicationofthe(ElimGen)transformchangesthecomplexityoftheprogram.IfthecomplexityofaSlicStanprogramwith𝑁discreteparametersofsupport1,...,𝐾,isO(𝑆),thenthecomplexityofaprogramwherewenaivelymarginalisedoutthediscretevariables(ProgramE)willbeO(𝑆×𝐾𝑁).Incontrast,transformingwith(ElimGen)givesusaprogramofcomplexityatmostO(𝑁×𝑆×𝐾𝑀+1)where𝑀isthelargestnumberofdirectneighboursinthefactorgraphinducedbytheprogram.Further,thecomplexitycouldbesmallerdependingontheeliminationorderingofchoice.Thisresultisnotsurprising,asweconjecturethatrepeatedapplicationof(ElimGen)isequivalenttovariableelimination(thoughwedonotformallyprovethisequivalence),whichisofthesamecomplexity.ItisclearfromthiscomplexityobservationthatVEisnotalwaysefficient.Whenthedependencygraphisdense,𝑀willbecloseto𝑁,thusinferencewillbeinfeasibleforlarge𝑁.Fortunately,inmanypracticalcases(suchasthosediscussedin§5),thisgraphissparse(𝑀≪𝑁)andourapproachissuitableandefficient.Wenotethatthisisagenerallimitationofexactinferenceofdiscreteparameters,anditisnotalimitationofourapproachinparticular.4.9SemanticPreservationoftheDiscreteVariableTransformationTheresultweareinterestedinisthesemanticpreservationofthetransformationrule𝑧−→.Theorem4(Semanticpreservationof𝑧−→).ForSlicStanprogramsΓ,𝑆andΓ′,𝑆′,andadiscreteparameter𝑧:Γ,𝑆𝑧−→Γ′,𝑆′impliesJ𝑆K=J𝑆′K.Proof.Notethatshreddingpreservessemanticswithrespecttoboth⊢and⊢2(Lemma6and10),examinethemeaningofderivedforms(Lemma14and15),notepropertiesofsingle-levelstatements(Lemma11),andapplytheresultsonfactorisationofshredding(Theorem1)andconditionalindependence(Theorem3).WepresentthefullproofinAppendixA.□Inaddition,wealsoshowthatitisalwayspossibletofindaprogramderivablewith(ElimGen),suchthatamodel-levelvariable𝑧istransformedtoagenqant-levelvariable.Lemma12(Existenceofmodeltogenqanttransformation).ForanySlicStanprogramΓ,𝑆suchthatΓ⊢𝑆:l1,andavariable𝑧∈dom(Γ)suchthatΓ(𝑧)=(int⟨𝐾⟩,model),thereexistsaSlicStanprogramΓ′,𝑆′,suchthat:Γ,𝑆𝑧−→Γ′,𝑆′andΓ′(𝑧)=(int⟨𝐾⟩,genquant)Proof.Byinspectingtheleveltypesofvariablesineachpartofaprogramderivableusing(ElimGen).WeincludethefullproofinAppendixA.□ThepracticalusefulnessofTheorem4stemsfromthefactthatitallowsustoseparateinferencefordiscreteandcontinuousparameters.Afterapplying(ElimGen)toeachdiscretemodel-levelparameter,weareleftwithaprogramthatonlyhasgenqant-leveldiscreteparameters(Lemma12).Wecanthenslicetheprograminto𝑆hmcand𝑆genandinfercontinuousparametersbyusingHMC(orotheralgorithms)on𝑆hmcand,next,drawthediscreteparametersusingancestralsamplingbyrunningforward𝑆gen.Theorem4tellsusthatthisisacorrectinferencestrategy.Whenusedinthecontextofamodelwithonlydiscreteparameters,ourapproachcorrespondstoexactinferencethroughVE.Inthepresenceofdiscreteandcontinuousparameters,ourtransfor-mationgivesananalyticalsub-solutionforthediscreteparametersinthemodel.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:32MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárAlimitationofourmethodisthat,duetoitsdensity-basednature,itcanonlybeappliedtomodelsoffixedsize.Itcannot,initscurrentform,supportmodelswherethenumberofrandomvariableschangesduringinference,suchasDirichletProcesses.However,thisisatypicalconstraintadoptedinBayesianinferenceforefficiency.Anotherlimitationisthatdiscretevariablesneedtohavefinite(andfixed)support.Forexample,themethodcannotbeappliedtotransformaPoisson-distributedvariable.Insomebutnotallapplications,truncatingunboundeddiscreteparametersatarealisticupperboundwouldsufficetomakethemethodapplicable.Anadvantageofourmethodisthatitcanbecombinedwithanyinferencealgorithmthatrequiresafunctionproportionaltothejointdensityofvariables.Thisincludesgradient-basedalgorithms,suchasHMCandvariationalinference,butitcanalsobeusedwithmethodsthatallowfor(e.g.unbounded)discretevariablesasananalyticalsub-solutionthatcanoptimiseinference.Forexample,consideraPoissonvariable𝑛∼Poisson(𝜆)andaBinomialvariable𝑘∼Binomial(𝑛,𝑝).While𝑛isofinfinitesupport,andwecannotdirectlysumoverallofitspossiblevalues,analyticallymarginalisingout𝑛givesus𝑘∼Poisson(𝜆𝑝).Futureworkcanutilisesuchanalyticalresultsinplaceofexplicitsummationwherepossible.4.10Scopeandlimitationsof(ElimGen)ProgramHdataintK;real[K][K]phi;real[K]mu;int<K>z1∼categorical(phi[0]);int<K>z2;int<K>z3;if(z1>K/2){z2∼categorical(phi[z1]);z3∼categorical(phi[z2]);}else{z2∼categorical(phi[z1]);z3∼categorical(phi[z1]);}datarealy1∼normal(mu[z1],1);datarealy2∼normal(mu[z2],1);datarealy3∼normal(mu[z3],1);ProgramH-A:Optimaltransformation...factor(elim(int<2>z1){z1∼categorical(phi[0]);y1∼normal(mu[z1],1));if(z1>K/2){elim(int<2>z2){elim(int<2>z3){z2∼categorical(phi[z1]);z3∼categorical(phi[z2]);y2∼normal(mu[z2],1);y3∼normal(mu[z3],1);}}}else{elim(int<2>z2){z2∼categorical(phi[z1]));y2∼normal(mu[z2],1);}elim(int<2>z3){z3∼categorical(phi[z1]));y3∼normal(mu[z3],1);}}});ProgramH-B:Ourtransformation...f1=𝜙(int<2>z2,int<2>z3){elim(int<2>z1){z1∼categorical(phi[0]);if(z1>K/2){z2∼categorical(phi[z1]);z3∼categorical(phi[z2]);}else{z2∼categorical(phi[z1]);z3∼categorical(phi[z1]);}y1∼normal(mu[z1],1);y2∼normal(mu[z2],1);y3∼normal(mu[z3],1);}}f2=𝜙(int<2>z3){elim(int<2>z2)factor(f1[z2,z3]);}f3=𝜙(){elim(int<2>z3)factor(f2[z3]);}factor(f3);Fig.7.Aprogramwithdifferentconditionaldependenciesdependingoncontrolflow.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:33Previously,wediscussedthescopeoftheconditionalindependenceresultofthepaper(§§3.3).Similarly,herewedemonstratewithanexample,asituationwhereourapproachofeliminatingvariablesone-by-oneusing(ElimGen)isnotoptimal.Considerthesimplecontrol-flowProgramHbelow.Inthisexample𝑧2and𝑧3arenotconditionallyindependentgiven𝑧1=1,buttheyareconditionallyindependentgiven𝑧1>𝐾/2.Thisindepen-denceisalsoreferredtoascontext-specificindependence[Boutilieretal.1996;MinkaandWinn2009].Wecanusedifferenteliminationstrategydependingonwhichif-branchoftheprogramwefindourselves.ProgramH-Ademonstratesthis:itscomplexityisO(𝐾2×𝐾2+𝐾2×2×𝐾)=O(12𝐾3+𝐾2).Thetypingrelation⊢2canonlydetectoverall(in)dependencies,wheresetsofvariablesareconditionallyindependentgivensome𝑋,regardlessofwhatvalue𝑋takes.Thus,ourstaticanalysisisnotabletodetectthat𝑧2⊥⊥𝑧3|𝑧1=0.ThisresultsinProgramH-B,whichhascomplexityO(𝐾3+𝐾2+𝐾):thesamecomplexityastheoptimalProgramH-A,butwithabiggerconstant.Evenifweextendourapproachtodetectthat𝑧2and𝑧3areindependentinonebranch,itisunclearhowtoincorporatethisnewinformation.Ourstrategyisbasedoncomputingintermediatefactorsthatallowre-usingalreadycomputedinformation:eliminating𝑧1requirescomputinganewfactor𝑓1thatnolongerdependson𝑧1.Werepresented𝑓1withamultidimensionalarrayindexedby𝑧2and𝑧3,andweneedtodefineeachelementofthatarray,thuswecannotdecouplethemforparticularvaluesof𝑧1.Runtimesystemsthatcomputeintermediatefactorsinasimilarway,suchasPyro[UberAILabs2017],facethatsamelimitation.Birch[MurrayandSchön2018],ontheotherhand,willbeabletodetecttheconditionalindependenceinthecase𝑧1>𝐾/2,butitwillnotmarginalise𝑧1,asitcannot(analytically)marginaliseoverbranches.Instead,itusesSequentialMonteCarlo(SMC)torepeatedlysample𝑧1andproceedaccordingtoitsvalue.5IMPLEMENTATIONANDEMPIRICALEVALUATIONThetransformationweintroducecanbeusefulforvarietyofmodels,anditcanbeadaptedtoPPLstoincreaseefficiencyofinferenceandusability.Mostnotably,itcanbeusedtoextendStantoallowfordirecttreatmentofdiscretevariables,wherepreviouslythatwasnotpossible.Inthissection,wepresentabriefoverviewofsuchadiscreteparameterextensionforSlicStan(§§5.1).Toevaluatethepracticalityof(ElimGen),webuildapartialNumPyro[Phanetal.2019]backendforSlicStan,andcompareourstaticapproachtovariableeliminationfordiscreteparameterstothedynamicapproachofNumPyro(§§5.2).Wefindthatourstatictransformationstrategyspeedsupinferencecomparedtothedynamicapproach,butthatformodelswithalargenumberofdiscreteparametersperformancegainscouldbediminishedbytheexponentiallygrowingcompilationtime(§§5.3).Inadditiontodemonstratingthepracticalityofourcontributionthroughempiricalevaluation,wealsodiscusstheusefulnessofourcontributionthroughexamples,inAppendixB.5.1ImplementationWeupdatetheoriginalSlicStan14accordingtothemodificationdescribedin§2,andextendittosupportautomaticvariableeliminationthroughtheschemeoutlinedin§4.AswiththefirstversionofSlicStan,thetransformationproducesanewSlicStanprogramthatisthentranslatedtoStan.Thevariableeliminationtransformationprocedureworksbyapplying(ElimGen)iteratively,onceforeachdiscretevariable,asweshowin§§4.7.Theleveltypesl1,l2andl3arenotexposedtotheuser,andareinferredautomatically.Usingbidirectionaltype-checking,weareabletosynthesiseasetofhardconstraintsthatthelevelsmustsatisfy.Thesehardconstraintswilltypicallybesatisfied14Availableathttps://github.com/mgorinova/SlicStan.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:34MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárbymorethanoneassignmentofvariablestolevels.Wesearchfortheoptimaltypeswithrespecttothesoftconstraintsl3≺l1≺l2,usingthetheoremproverZ3[DeMouraandBjørner2008].5.2EmpiricalevaluationToevaluatethepracticalityofourapproach,wecomparetothepriorworkmostcloselyrelatedtoours:thatofObermeyeretal.[2019],whoimplementefficientvariable-eliminationforplatedfactorgraphsinPyro[UberAILabs2017].Theirapproachuseseffect-handlersanddynamicallymarginalisesdiscretevariables,sothatgradient-basedinferenceschemescanbeusedforthecontinuousparameters.ThisVEstrategyhasalsobeenimplementedinNumPyro[Phanetal.2019].AsbothoursandPyro’sstrategiescorrespondtoVE,wedonotexpecttoseedifferencesincomplexityoftheresultingprograms.However,asinourcasetheVEalgorithmisdeterminedandsetupatcompiletime,whileinthecaseofPyro/NumPyro,thisisdoneatruntime.Themainquestionweaimtoaddressiswhethersettingupthevariableeliminationlogisticsatcompiletimeresultsinapracticalruntimespeed-up.Toallowforthiscomparison,webuiltapartialNumPyrobackendforSlicStan.Foreachmodelwechoose,wecomparetheruntimeperformanceofthreeNumPyroprograms:(1)TheNumPyroprogramobtainedbytranslatingaSlicStanprogramwithdiscreteparameterstoNumPyrodirectly(labelled‘NumPyro’).Thisisthebaseline:weleavethediscreteparametereliminationtoNumPyro.(2)TheNumPyroprogramobtainedbytranslatingatransformedSlicStanprogram,wherealldiscreteparametershavebeeneliminatedaccordingto(ElimGen)(labelled‘SlicStan’).Thevariableeliminationset-upisdoneatcompiletime;NumPyrodoesnotdoanymarginalisation.(3)Ahand-optimisedNumPyroprogram,whichusestheplateandmarkovprogramconstructstospecifysomeoftheconditionalindependenciesintheprogram(labelled‘NumPyro-Opt’).Ineachcase,wemeasurethetime(inseconds)forsamplingasinglechainconsistingof2500warm-upsamples,and10000samplesusingNUTS[HoffmanandGelman2014].Inaddition,wereportthreecompilationtimes:(1)ThecompilationtimeoftheNumPyroprogramobtainedbytranslatingaSlicStanprogramwithdiscreteparameterstoNumPyrodirectly(labelled‘NumPyro’).(2)ThecompilationtimeoftheNumPyroprogramobtainedbytranslatingatransformedSlicStanprogram,wherealldiscreteparametershavebeeneliminated(labelled‘SlicStan’).(3)ThetimetakenfortheoriginalSlicStanprogramtobetransformedusing(ElimGen)andtranslatedtoNumPyrocode(labelled‘SlicStan-to-NumPyro’).Weconsiderdifferentnumbersofdiscreteparametersforeachmodel,upto25discreteparameters.Wedonotconsidermorethan25parametersduetoconstraintsoftheNumPyrobaseline,whichwediscussinmoredetailin§§5.3.Werunexperimentsontwoclassesofmodeloftenseeninpractice:hiddenMarkovmodels(§§§5.2.1)andmixturemodels(§§§5.2.2).Toensureafaircomparison,thesameeliminationorderingwasusedacrossexperiments.Experimentswererunonadual-core2.30GHzIntelXeonCPUandaTeslaT4GPU(whenapplicable).AllSlicStanmodelsusedintheexperimentsareavailableattheSlicStanrepo.5.2.1HiddenMarkovmodels.WeshowedseveralexamplesofsimpleHMMsthroughoutthepaper(ProgramA,ProgramD,ProgramG)andworkedthroughacompleteexampleofVEinanHMM(4.7).Weevaluateourapproachonboththesimplefirst-orderHMMseenpreviously,andontwoadditionalones:second-orderHMMandfactorialHMM.First-orderHMM.Thefirst-orderHMMisasimplechainof𝑁discretevariables,eachtakingavaluefrom1to𝐾accordingtoacategoricaldistribution.TheeventprobabilitiesforthedistributionACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:35of𝑧𝑛aregivenby𝜽𝑧𝑛−1,where𝜽issomegiven𝐾×𝐾matrix.EachdatapointyismodelledascomingfromaGaussiandistributionwithmean𝜇𝑧𝑛andstandarddeviation1,where𝝁isa𝐾−dimensionalcontinuousparameterofthemodel.𝜇𝑘∼N(0,1)for𝑘∈1,...,𝐾𝑧1∼categorical(𝜽1)𝑧𝑛∼categorical(𝜽𝑧𝑛−1)for𝑛∈2,...,𝑁𝑦𝑛∼N(𝜇𝑧𝑛,1)for𝑛∈1,...,𝑁Wemeasurethecompilationtimeandthetimetakentosample1chainwitheachofthe3NumPyroprogramscorrespondingtothismodel.Weuse𝐾=3anddifferentvaluesfor𝑁,rangingfrom𝑁=3to𝑁=25.Figure8showsasummaryoftheresults.WeseethatbothonCPUandGPU,theprogramtransformedusingSlicStanoutperformstheautomaticallygeneratedNumPyroandalsothemanuallyoptimisedNumPyro-Opt.Eachofthethreeprogramshascompilationtimeexponentiallyincreasingwiththenumberofvariables,howeverSlicStan’scompilationtimeincreasesthefastest.Wediscussthisdrawbackinmoredetailin§§5.3,highlightingtheimportanceofanextendedloop-levelanalysisbeingconsideredinfuturework.Second-orderHMM.Thesecond-orderHMMisverysimilartothefirst-orderHMM,butthediscretevariablesdependontheprevious2variables,inthiscasetakingthemaximumofthetwo.𝜇𝑘∼N(0,1)for𝑘∈1,...,𝐾𝑧1∼categorical(𝜃1),𝑧2∼categorical(𝜃𝑧1)𝑧𝑛∼categorical(𝜃max(𝑧𝑛−2,𝑧𝑛−1))for𝑛∈3,...,𝑁𝑦𝑛∼N(𝜇𝑧𝑛,1)for𝑛∈1,...,𝑁Similarlytobefore,weruntheexperimentfor𝐾=3anddifferentvaluesfor𝑁,rangingfrom𝑁=3to𝑁=25.WeshowtheresultsinFigure9,whichonceagainshowsSlicStanoutperformingNumPyroandNumPyro-Optintermsofruntime,buthavingslowercompilationtimeforalargernumberofdiscreteparameters.FactorialHMM.InafactorialHMM,eachdatapoint𝑦𝑛isgeneratedusingtwoindependenthiddenstates𝑧𝑛andℎ𝑛,eachdependingontheprevioushiddenstates𝑧𝑛−1andℎ𝑛−1.𝜇𝑘∼N(0,1)for𝑘∈1,...,𝐾2𝑧1∼categorical(𝜃1),ℎ1∼categorical(𝜃1)𝑧𝑛∼categorical(𝜃𝑧𝑛−1),ℎ𝑛∼categorical(𝜃ℎ𝑛−1)for𝑛∈2,...,𝑁𝑦𝑛∼N(𝜇𝑧𝑛∗ℎ𝑛,1)for𝑛∈1,...,𝑁Weruntheexperimentfor𝐾=3anddifferentlengthofthechain𝑁,rangingfrom𝑁=1(2discreteparameters)to𝑁=12(24discreteparameters).WeshowtheresultsinFigure10:similarlytobefore,SlicStanoutperformsbothNumPyroandNumPyro-Optintermsofruntime.Wealsoobservethat,inthecaseofSlicStan,thetimetakentosampleasinglechainincreasesmoreslowlyasweincreasethenumberofdiscretevariables.5.2.2Mixturemodels.Anotherusefulapplicationofmixeddiscreteandcontinuousvariablemodelsisfoundinmixturemodels.Werunexperimentsontwomodels:soft𝐾-meansclusteringandlinearregressionwithoutlierdetection.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:36MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár510152025Number of discrete parameters18.519.019.520.020.521.021.522.022.5Time (sec) per chainFirst-order HMM: Runtime, CPUSlicStanNumPyroNumPyro-Opt510152025Number of discrete parameters14.014.515.015.516.016.5Time (sec) per chainFirst-order HMM: Runtime, GPUSlicStanNumPyroNumPyro-Opt35812151825Number of discrete parameters020406080100Compilation time (sec)First-order HMM: Compilation timesSlicStanNumPyroSlicStan-to-NumPyroFig.8.HMMresults510152025Number of discrete parameters181920212223242526Time (sec) per chainSecond-order HMM: Runtime, CPUSlicStanNumPyroNumPyro-Opt510152025Number of discrete parameters1415161718192021Time (sec) per chainSecond-order HMM: Runtime, GPUSlicStanNumPyroNumPyro-Opt35812151825Number of discrete parameters020406080100120Compilation time (sec)Second-order HMM: Compilation timesSlicStanNumPyroSlicStan-to-NumPyroFig.9.Second-orderHMMresults510152025Number of discrete parameters20222426283032Time (sec) per chainFactorial HMM: Runtime, CPUSlicStanNumPyroNumPyro-Opt510152025Number of discrete parameters141618202224Time (sec) per chainFactorial HMM: Runtime, GPUSlicStanNumPyroNumPyro-Opt261014182224Number of discrete parameters0102030405060Compilation time (sec)Factorial HMM: Compilation timesSlicStanNumPyroSlicStan-to-NumPyroFig.10.FactorialHMMresults510152025Number of discrete parameters304050607080Time (sec) per chainSoft K-means: Runtime, CPUSlicStanNumPyroNumPyro-Opt510152025Number of discrete parameters20253035404550Time (sec) per chainSoft K-means: Runtime, GPUSlicStanNumPyroNumPyro-Opt35812151825Number of discrete parameters050100150200250300350400Compilation timeSoft K-means: Compilation timesSlicStanNumPyroSlicStan-to-NumPyroFig.11.SoftK-meansresults510152025Number of discrete parameters19202122232425Time (sec) per chainOutlier detection: Runtime, CPUSlicStanNumPyroNumPyro-Opt510152025Number of discrete parameters1415161718Time (sec) per chainOutlier detection: Runtime, GPUSlicStanNumPyroNumPyro-Opt35812151825Number of discrete parameters020406080Compilation time (sec)Outlier detection: Compilation timesSlicStanNumPyroSlicStan-to-NumPyroFig.12.OutliersresultsACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:37SoftK-means.TheGaussianmixturemodelunderlinesthecelebratedsoft𝐾-meansalgorithm.Here,weareinterestedinmodellingsome𝐷-dimensionaldatathatbelongstooneof𝐾(unknown)Gaussianclusters.Eachcluster𝑘isspecifiedbya𝐷-dimensionalmean𝜇.,𝑘.Eachdatapoint𝑦.,𝑛isassociatedwithacluster𝑧𝑛.𝜇𝑑,𝑘∼N(0,1)for𝑑∈1,...,𝐷and𝑘∈1,...,𝐾𝑧𝑛∼categorical(𝜋)for𝑛∈1,...,𝑁𝑦𝑑,𝑛∼N(𝜇𝑑,𝑧𝑛,1)for𝑑∈1,...,𝐷and𝑛∈1,...,𝑁Weruntheexperimentsfor𝐾=3,𝐷=10,and𝑁=3,...,25andshowtheresultsinFigure11.WeobserveaclearlineartrendoftheruntimegrowingwithN,withSlicStanperformingbetteranditsruntimegrowingmoreslowlythanthatofNumPyro.WhiletheSlicStan-translatedcoderunsfasterthanNumPyro-Optfor𝑁≤25,weobservethattheSlicStanruntimegrowsfasterthanthatofthemanuallyoptimisedNumPyro-Optcode.Outlierdetection.ThefinalmodelweconsiderisaBayesianlinearregressionthatallowsforoutlierdetection.Themodelconsidersdatapoints(𝑥𝑛,𝑦𝑛),where𝑦liesontheline𝛼𝑥+𝛽withsomeaddednoise.Thenoise𝜎𝑧𝑛dependsonaBernoulliparameter𝑧𝑛,whichcorrespondstowhetherornotthepoint(𝑥𝑛,𝑦𝑛)isanoutlierornot.Thenoiseforoutliers(𝜎1)andthenoisefornon-outliers(𝜎2)aregivenashyperparameters.𝛼∼N(0,10),𝛽∼N(0,10)𝜋(𝑟𝑎𝑤)1∼N(0,1),𝜋(𝑟𝑎𝑤)2∼N(0,1),𝜋=exp𝜋(𝑟𝑎𝑤)1exp𝜋(𝑟𝑎𝑤)1+exp𝜋(𝑟𝑎𝑤)2𝑧𝑛∼bernoulli(𝜋)for𝑛∈1,...,𝑁𝑦𝑛∼N(𝛼𝑥𝑛+𝛽,𝜎𝑧𝑛)for𝑛∈1,...,𝑁SimilarlytotheearlierHMMmodels,SlicStanhasthesmallestruntimeperchain,butattheexpenseoffastgrowingcompiletime(Figure12).5.3AnalysisanddiscussionOurmethodcanbeappliedtogeneralmodelscontainingafixedandknownnumberoffinite-supportdiscreteparameters,whichsignificantlyreducestheamountofmanualeffortthatwaspreviouslyrequiredforsuchmodelsinlanguageslikeStan[Damianoetal.2018].Inaddition,asshowninFigures8–12,SlicStanoutperformsboththeNumPyrobaselineandthehand-optimisedNumPyro-Opt,intermsofruntime.Thissuggeststhatastatic-timediscretevariableoptimisation,liketheoneintroducedinthispaper,isindeedbeneficialandspeedsupinference.Onelimitationofourexperimentalanalysisistherelativelysmallnumberofdiscreteparametersweconsider.DuetothearraydimensionlimitimposedbyPyTorch/NumPy,Pyrocannothavemorethan25discretevariables(64forCPU)unlessthedependencebetweenthemisspecifiedusingmarkovorplate(aswithNumPyro-Opt).ForNumPyrothishardcodedlimitis32.Thus,itwouldnotbepossibletocomparetotheNumPyrobaselineforalargernumberofvariables,thoughcomparingtothehand-optimisedNumPyro-Optwouldstillbepossible.PerhapsthebiggestlimitationofthediscreteparametersversionofSlicStanistheexponentiallygrowingcompilationtime.Usingasemi-latticeinsteadofalatticeinthe⊢2leveltypeanalysisbreakstherequirementofthebidirectionaltypesystemthatensuresefficiencyoftypeinference.TheconstraintsgeneratedbythetypesystemcannolongerberesolvedbySlicStan’soriginallinear-timealgorithm.Whilepolynomial-timeconstraint-solvingstrategymaystillexist,wechooseACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:38MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákártoemployZ3toautomaticallyresolvethetypeinferenceconstraints,andleavetheconsiderationformoreefficienttypeinferencealgorithmforfuturework.ThisalsohighlightstheimportanceofafutureSlicStanversionthatconsidersarraysofdiscreteparameters.Ouralgorithmcurrentlysupportsonlyindividualdiscreteparameters.Inthecaseswherethesizeofanarrayofdiscreteparametersisstaticallyknown,the(ElimGen)procedurecanbeappliedtoaprogramwheresucharrayshavebeen‘flattened’intoacollectionofindividualdiscretevariables,whichisthestrategyweadoptfortheexperimentsinthissection.Buttobeapplicablemorewidely,the(ElimGen)ruleneedstobegeneralisedbasedonarrayelementleveldependenceanalysis,forexamplebyincorporatingideasfromthepolyhedralmodel[Feautrier1992].Asthearrayleveldependenceanalysisthatwouldberequiredinmostpracticaluse-casesisverystraightforward,webelievethiswouldbeausefulandfeasibleappliedextensionofourwork.Inaddition,thiswouldsignificantlydecreasethenumberofprogramvariablesforwhichweneedtoinferaleveltypeduringthe(ElimGen)transformation,thusmakingcompilationpracticalforlargerorarbitrarynumbersofdiscreteparameters.6RELATEDWORKThispaperprovidesatypesystemthatinducesconditionalindependencerelationships,anditdiscussesonepracticalapplicationofsuchtypesystem:anautomaticmarginalisationprocedurefordiscreteparametersoffinitesupport.Conditionalindependence.ThetheoreticalaimofourpaperissimilartothatofBartheetal.[2019],whodiscussaseparationlogicforreasoningaboutindependence,andthefollow-upworkofBaoetal.[2021],whoextendthelogictocaptureconditionalindependence.Oneadvantageofourmethodisthattheverificationofconditionalindependenceisautomatedbytypeinference,whileitwouldrelyonmanualreasoningintheworksofBartheetal.[2019]andBaoetal.[2021].Ontheotherhand,thelogicapproachcanbeappliedtoawidervarietyofverificationtasks.AmtoftandBanerjee[2020]showacorrespondencebetweenvariableindependenceandslicingadiscrete-variables-onlyprobabilisticprogram.Thebiggestdifferencetoourworkisthattheirworkconsidersonlyconditionalindependenceofvariablesgiventheobserveddata:thatisCIrelationshipsoftheformx1⊥⊥x2|Dforsomesubsetsofvariablesx1andx2anddataD.ThelanguageofAmtoftandBanerjee[2020]requiresobserveddatatobespecifiedsyntacticallyusinganobservestatement.Conditionalindependenciesaredeterminedonlygiventhisobserveddata,andthemethodfordetermininghowtosliceaprogramistiedtotheobservestatements.FromtheAmtoftandBanerjee[2020]paper:“Abasicintuitionbehindourapproachisthatanobservestatementcanberemovedifitdoesnotdependonsomethingonwhichthereturnedvariable𝑥alsodepends.”Incontrast,weareabletofindCIrelationshipsgivenanyvariablesweareinterestedin(x1⊥⊥x2|x3forsomex1,x2,andx3),andtypeinferenceconstitutesofastraightforwardalgorithmforfindingsuchrelationships.Ontheotherhand,AmtoftandBanerjee[2020]permitunboundednumberofvariables(e.g.while(y>0)y∼bernoulli(0.2)),whileitisnotclearhowtoextendSlicStan/Stantosupportthis.Whilenotinaprobabilistprogrammingsetting,Lobo-Vesgaetal.[2020]usetaintanalysistofindindependenciesbetweenvariablesinaprogram,inordertofacilitateeasytradeoffbetweenprivacyandaccuracyindifferentialprivacycontext.Automaticmarginalisation.Themostcloselyrelatedpreviouswork,intermsoftheautomaticmarginalisationprocedure,isthatofObermeyeretal.[2019]andthatofMurrayetal.[2018].Ober-meyeretal.[2019]implementefficientvariable-eliminationforplatedfactorgraphsinPyro[UberAILabs2017].Theirapproachuseseffect-handlersandcanbeimplementedinothereffect-handlingbasedPPLs,suchasEdward2[Tranetal.2018].Murrayetal.[2018]introducea‘delayedsampling’procedureinBirch[MurrayandSchön2018],whichoptimisestheprogramviapartialanalyticalACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:39solutionstosub-programs.Theirmethodcorrespondstoautomaticvariableeliminationand,moregenerally,automaticRao–Blackwellization.Whilewefocusondiscretevariableeliminationonly,ourconditionalindependencetypesystemcanbedirectlyusedformoregeneralanalysis.Themethodfrom§4canbeextendedtomarginaliseoutandsamplecontinuousvariableswhenevertheyarepartofananalytically-tractablesub-program,similarlytodelayedsamplinginBirch.Onekeydifferenceofourapproachisthattheprogramre-writesareguidedbythetypesystemandhappenatcompiletime,beforeinferenceisrun.Incontrast,bothPyroandBirchmaintainadynamicgraphthatguidestheanalysisatruntime.Symbolicinference.Whereafullanalyticalsolutionispossible,severalprobabilisticprogramminglanguagescanderiveitviasymbolicmanipulation,includingHakaru[Narayananetal.2016]andPSI[Gehretal.2016,2020],whileDice[Holtzenetal.2020]performsexactinferenceformodelswithdiscreteparametersonly,byanalysingtheprogramstructure.Incontrast,wefocusonre-writingtheprogram,anddecomposingitintopartstobeusedwithfastandmoregeneralasymptoticallyexactorapproximateinferencealgorithms,likeHMC,variationalinferenceorothers.ExtendingHMCtosupportdiscreteparameters.TheideaofmodifyingHMCtohandlediscretevariablesanddiscontinuitieshasbeenpreviouslyexplored[Nishimuraetal.2017;PakmanandPaninski2013;Zhangetal.2012;Zhou2020].Morerecently,Zhouetal.[2019]introducedtheprobabilisticprogramminglanguageLF-PPL,whichisdesignedspecificallytobeusedwiththeDiscontinuousHamiltonianMonteCarlo(DHMC)algorithm[Nishimuraetal.2017].Thealgorithm,andtheirframeworkcanalsobeextendedtosupportdiscreteparameters.LF-PPLprovidessupportforanHMCversionthatitselfworkswithdiscontinuities.OurapproachistostaticallyrewritetheprogramtomatchtheconstraintsofStan,vanillaHMC,anditsseveralwell-optimisedextensions,suchasNUTS[HoffmanandGelman2014].Composableandprogrammableinference.Recentyearshaveseenagrowingnumberoftechniquesthatallowfortailored-to-the-programcompilationtoaninferencealgorithm.Forexample,Gen[Cusumano-Towneretal.2019]canstaticallyanalysethemodelstructuretocompiletoamoreefficientinferencestrategy.Inaddition,languageslikeGenandTuring[Geetal.2018]facilitatecomposableandprogrammableinference[Mansinghkaetal.2018],wheretheuserisprovidedwithinferencebuildingblockstoimplementtheirownmodel-specificalgorithm.Ourmethodcanbeunderstoodasanautomaticcompositionbetweentwoinferencealgorithms:variableeliminationandHMCoranyotherinferencealgorithmthatcanbeusedtosamplecontinuousvariables.7CONCLUSIONThispaperintroducesaninformationflowtypesystemthatcanbeusedtocheckandinfercondi-tionalindependencerelationshipsinaprobabilisticprograms,throughtypecheckingandinference,respectively.Wepresentapracticalapplicationofthistypesystem:asemantics-preservingtransfor-mationthatmakesitpossibletouse,andtoefficientlyandautomaticallyinferdiscreteparametersinSlicStan,Stan,andotherdensity-basedprobabilisticprogramminglanguages.Thetransformedprogramcanbeseenasahybridinferencealgorithmontheoriginalprogram,wherecontinuousparameterscanbedrawnusingefficientgradient-basedinferencemethods,likeHMC,whilethediscreteparametersaredrawnusingvariableelimination.Whilethevariableeliminationtransformationusesresultsonconditionalindependenceofdiscreteparameters,ourtypesystemisnotrestrictedtothisusage.Conditionalindependencerelationshipscanbeofinterestinmanycontextinprobabilisticmodelling,includingmoregeneraluseofvariableelimination,message-passingalgorithms,Rao-Blackwellization,andfactorisingaACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:40MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárprogramforacomposed-inferenceapproach.Webelieveconditionalindependencebytypingcanenableinterestingfutureworkthatautomatestheimplementationofsuchmethods.ACKNOWLEDGMENTSWethankVikashMansinghkaforsuggestingtheoutlierdetectionexample,whichweusedforevaluation,aswellasLawrenceMurryforclarifyingthebehaviourofBirch,andanonymousreview-erswhosehelpfulsuggestionsimprovedthepaper.MariaGorinovawassupportedbytheEPSRCCentreforDoctoralTraininginDataScience,fundedbytheUKEngineeringandPhysicalSciencesResearchCouncil(grantEP/L016427/1)andtheUniversityofEdinburgh.MatthijsVákárwasfundedbytheEuropeanUnion’sHorizon2020researchandinnovationprogrammeundertheMarieSkłodowska-CuriegrantagreementNo.895827.REFERENCESMartínAbadi,AnindyaBanerjee,NevinHeintze,andJonG.Riecke.1999.ACoreCalculusofDependency.InPOPL.ACM,147–160.EyalAmir.2010.Approximationalgorithmsfortreewidth.Algorithmica56,4(2010),448–479.TorbenAmtoftandAnindyaBanerjee.2020.ATheoryofSlicingforImperativeProbabilisticPrograms.ACMTransactionsonProgrammingLanguagesandSystems(TOPLAS)42,2(2020),1–71.StefanArnborg,DerekGCorneil,andAndrzejProskurowski.1987.Complexityoffindingembeddingsinak-tree.SIAMJournalonAlgebraicDiscreteMethods8,2(1987),277–284.JialuBao,SimonDocherty,JustinHsu,andAlexandraSilva.2021.ABunchedLogicforConditionalIndependence.In202136thAnnualACM/IEEESymposiumonLogicinComputerScience(LICS).IEEE,1–14.GillesBarthe,JustinHsu,andKevinLiao.2019.Aprobabilisticseparationlogic.ProceedingsoftheACMonProgrammingLanguages4,POPL(2019),1–30.MichaelBetancourtandMarkGirolami.2015.HamiltonianMonteCarloforhierarchicalmodels.CurrenttrendsinBayesianmethodologywithapplications79(2015),30.CraigBoutilier,NirFriedman,MoisesGoldszmidt,andDaphneKoller.1996.Context-SpecificIndependenceinBayesianNetworks.InProceedingsoftheTwelfthInternationalConferenceonUncertaintyinArtificialIntelligence(UAI’96).115–123.BobCarpenter,AndrewGelman,MatthewHoffman,DanielLee,BenGoodrich,MichaelBetancourt,MarcusBrubaker,JiqiangGuo,PeterLi,andAllenRiddell.2017.Stan:AProbabilisticProgrammingLanguage.JournalofStatisticalSoftware,Articles76,1(2017),1–32.https://doi.org/10.18637/jss.v076.i01MarcoF.Cusumano-Towner,FerasA.Saad,AlexanderK.Lew,andVikashK.Mansinghka.2019.Gen:AGeneral-PurposeProbabilisticProgrammingSystemwithProgrammableInference.InProceedingsofthe40thACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation(Phoenix,AZ,USA)(PLDI2019).AssociationforComputingMachinery,NewYork,NY,USA,221–236.https://doi.org/10.1145/3314221.3314642LuisDamiano,BrianPeterson,andMichaelWeylandt.2018.ATutorialonHiddenMarkovModelsusingStan.StanCon(2018).https://doi.org/10.5281/zenodo.1284341.LeonardoDeMouraandNikolajBjørner.2008.Z3:AnefficientSMTsolver.InInternationalconferenceonToolsandAlgorithmsfortheConstructionandAnalysisofSystems(TACAS).Springer,337–340.PaulFeautrier.1992.Someefficientsolutionstotheaffineschedulingproblem.PartII.Multidimensionaltime.Internationaljournalofparallelprogramming21,6(1992),389–420.BrendanJ.Frey.2002.ExtendingFactorGraphssoastoUnifyDirectedandUndirectedGraphicalModels.InProceedingsoftheNineteenthConferenceonUncertaintyinArtificialIntelligence(Acapulco,Mexico)(UAI’03).MorganKaufmannPublishersInc.,SanFrancisco,CA,USA,257–264.HongGe,KaiXu,andZoubinGhahramani.2018.Turing:alanguageforflexibleprobabilisticinference.InInternationalConferenceonArtificialIntelligenceandStatistics(AISTATS).1682–1690.http://proceedings.mlr.press/v84/ge18b.htmlTimonGehr,SasaMisailovic,andMartinVechev.2016.PSI:Exactsymbolicinferenceforprobabilisticprograms.InInternationalConferenceonComputerAidedVerification.Springer,62–83.TimonGehr,SamuelSteffen,andMartinVechev.2020.𝜆PSI:Exactinferenceforhigher-orderprobabilisticprograms.InProceedingsofthe41stACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation.883–897.AndrewGelman,DanielLee,andJiqiangGuo.2015.Stan:AprobabilisticprogramminglanguageforBayesianinferenceandoptimization.JournalofEducationalandBehavioralStatistics40,5(2015),530–543.AndrewD.Gordon,ClaudioV.Russo,MarcinSzymczak,JohannesBorgström,NicolasRolland,ThoreGraepel,andDanielTarlow.2015.ProbabilisticProgramsasSpreadsheetQueries.InESOP(LectureNotesinComputerScience,Vol.9032).Springer,1–25.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:41MariaIGorinova,AndrewDGordon,andCharlesSutton.2019.ProbabilisticprogrammingwithdensitiesinSlicStan:efficient,flexible,anddeterministic.ProceedingsoftheACMonProgrammingLanguages3,POPL(2019),35.AndreasGriewankandAndreaWalther.2008.Evaluatingderivatives:principlesandtechniquesofalgorithmicdifferentiation.SIAM.MatthewDHoffmanandAndrewGelman.2014.TheNo-U-turnsampler:AdaptivelysettingpathlengthsinHamiltonianMonteCarlo.JournalofMachineLearningResearch15,1(2014),1593–1623.StevenHoltzen,GuyVandenBroeck,andToddMillstein.2020.Dice:CompilingDiscreteProbabilisticProgramsforScalableInference.arXivpreprintarXiv:2005.09089(2020).UffeKjærulff.1990.Triangulationofgraphs–algorithmsgivingsmalltotalstatespace.(1990).DaphneKollerandNirFriedman.2009.Probabilisticgraphicalmodels:principlesandtechniques.MITpress.ElisabetLobo-Vesga,AlejandroRusso,andMarcoGaboardi.2020.Aprogrammingframeworkfordifferentialprivacywithaccuracyconcentrationbounds.In2020IEEESymposiumonSecurityandPrivacy(SP).IEEE,411–428.VikashK.Mansinghka,UlrichSchaechtle,ShivamHanda,AlexeyRadul,YutianChen,andMartinRinard.2018.ProbabilisticProgrammingwithProgrammableInference.InProceedingsofthe39thACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation(Philadelphia,PA,USA)(PLDI2018).ACM,NewYork,NY,USA,603–616.https://doi.org/10.1145/3192366.3192409TomMinkaandJohnWinn.2009.Gates.InAdvancesinNeuralInformationProcessingSystems,D.Koller,D.Schuur-mans,Y.Bengio,andL.Bottou(Eds.),Vol.21.CurranAssociates,Inc.https://proceedings.neurips.cc/paper/2008/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdfT.Minka,J.M.Winn,J.P.Guiver,S.Webster,Y.Zaykov,B.Yangel,A.Spengler,andJ.Bronskill.2014.Infer.NET2.6.MicrosoftResearchCambridge.http://research.microsoft.com/infernet.DaveMooreandMariaI.Gorinova.2018.EffectHandlingforComposableProgramTransformationsinEdward2.InternationalConferenceonProbabilisticProgramming(2018).https://arxiv.org/abs/1811.06150KevinPMurphy.2012.Machinelearning:aprobabilisticperspective.MITpress.LawrenceM.Murray,DanielLundén,JanKudlicka,DavidBroman,andThomasB.Schön.2018.DelayedSamplingandAutomaticRao-BlackwellizationofProbabilisticPrograms.ProceedingsofMachineLearningResearch,Twenty-FirstInternationalConferenceonArtificialIntelligenceandStatistics(AISTATS)84(2018),1037–1046.http://proceedings.mlr.press/v84/murray18a.htmlLawrenceMMurrayandThomasBSchön.2018.Automatedlearningwithaprobabilisticprogramminglanguage:Birch.AnnualReviewsinControl46(2018),29–43.PraveenNarayanan,JacquesCarette,WrenRomano,Chung-chiehShan,andRobertZinkov.2016.ProbabilisticInferencebyProgramTransformationinHakaru(SystemDescription).InFunctionalandLogicProgramming,OlegKiselyovandAndyKing(Eds.).SpringerInternationalPublishing,Cham,62–79.RadfordMNealetal.2011.MCMCusingHamiltoniandynamics.HandbookofMarkovChainMonteCarlo2,11(2011).AkihikoNishimura,DavidDunson,andJianfengLu.2017.DiscontinuousHamiltonianMonteCarloforsamplingdiscreteparameters.arXivpreprintarXiv:1705.08510(2017).FritzObermeyer,EliBingham,MartinJankowiak,JustinChiu,NeerajPradhan,AlexanderRush,andNoahGoodman.2019.TensorVariableEliminationforPlatedFactorGraphs.InternationalConferenceonMachineLearning(2019).APakmanandLPaninski.2013.Auxiliary-variableexactHamiltonianMonteCarlosamplersforbinarydistributions.AdvancesinNeuralInformationProcessingSystems(2013).DuPhan,NeerajPradhan,andMartinJankowiak.2019.ComposableEffectsforFlexibleandAcceleratedProbabilisticProgramminginNumPyro.arXivpreprintarXiv:1912.11554(2019).BenjaminCPierceandDavidNTurner.2000.Localtypeinference.ACMTransactionsonProgrammingLanguagesandSystems(TOPLAS)22,1(2000),1–44.LawrenceRRabiner.1989.AtutorialonhiddenMarkovmodelsandselectedapplicationsinspeechrecognition.Proc.IEEE77,2(1989),257–286.JohnSalvatier,ThomasVWiecki,andChristopherFonnesbeck.2016.ProbabilisticprogramminginPythonusingPyMC3.PeerJComputerScience2(2016),e55.StanDevelopmentTeam.2019a.Stanlanguagereferencemanual.Version2.26.0.http://mc-stan.org.StanDevelopmentTeam.2019b.Stanuser’sguide.Version2.26.0.http://mc-stan.org.DustinTran,MatthewD.Hoffman,SrinivasVasudevan,ChristopherSuter,DaveMoore,AlexeyRadul,MatthewJohnson,andRifA.Saurous.2018.Edward2:Simple,Distributed,Accelerated.(2018).https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/edward2AdvancesinNeuralInformationProcessingSystems(NeurIPS).UberAILabs.2017.Pyro:Adeepprobabilisticprogramminglanguage.http://pyro.ai/.DennisM.Volpano,CynthiaE.Irvine,andGeoffreySmith.1996.ASoundTypeSystemforSecureFlowAnalysis.J.Comput.Secur.4,2/3(1996),167–188.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:42MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárJohnWinn.2012.Causalitywithgates.InInternationalConferenceonArtificialIntelligenceandStatistics(AISTATS).1314–1322.http://proceedings.mlr.press/v22/winn12.htmlFrankWood,JanWillemvandeMeent,andVikashMansinghka.2014.ANewApproachtoProbabilisticProgrammingInference.InProceedingsofthe17thInternationalconferenceonArtificialIntelligenceandStatistics.1024–1032.NevinLianwenZhangandDavidPoole.1994.AsimpleapproachtoBayesiannetworkcomputations.InProceedingsoftheBiennialConference-CanadianSocietyforComputationalStudiesofIntelligence.CanadianInformationProcessingSociety,171–178.YichuanZhang,ZoubinGhahramani,AmosJStorkey,andCharlesA.Sutton.2012.ContinuousrelaxationsfordiscreteHamiltonianMonteCarlo.InAdvancesinNeuralInformationProcessingSystems,F.Pereira,C.J.C.Burges,L.Bottou,andK.Q.Weinberger(Eds.).CurranAssociates,Inc.,3194–3202.http://papers.nips.cc/paper/4652-continuous-relaxations-for-discrete-hamiltonian-monte-carlo.pdfGuangyaoZhou.2020.MixedHamiltonianMonteCarloforMixedDiscreteandContinuousVariables.InAdvancesinNeuralInformationProcessingSystems(NeurIPS).YuanZhou,BradleyJ.Gram-Hansen,TobiasKohn,TomRainforth,HongseokYang,andFrankWood.2019.LF-PPL:ALow-LevelFirstOrderProbabilisticProgrammingLanguageforNon-DifferentiableModels.InInternationalConferenceonArtificialIntelligenceandStatistics(AISTATS).http://proceedings.mlr.press/v89/zhou19b.htmlACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:43ADEFINITIONSANDPROOFSA.1DefinitionsDefinition10(Assigns-toset𝑊(𝑆)).W(S)isthesetthatcontainsthenamesofglobalvariablesthathavebeenassignedtowithinthestatementS.Itisdefinedrecursivelyasfollows:𝑊(𝑥[𝐸1]...[𝐸𝑛]=𝐸)={𝑥}𝑊(𝑆1;𝑆2)=𝑊(𝑆1)∪𝑊(𝑆2)𝑊(if(𝐸)𝑆1else𝑆2)=𝑊(𝑆1)∪𝑊(𝑆2)𝑊(for(𝑥in𝐸1:𝐸2)𝑆)=𝑊(𝑆)\{𝑥}𝑊(skip)=∅𝑊(factor(𝐸))=∅𝑊(𝐿∼𝑑(𝐸1,...,𝐸𝑛))=∅Definition11(Readsset𝑅(𝑆)).R(S)isthesetthatcontainsthenamesofglobalvariablesthathavebeenreadwithinthestatementS.Itisdefinedrecursivelyasfollows:𝑅(𝑥)={𝑥}𝑅(𝑐)=∅𝑅([𝐸1,...,𝐸𝑛])=—𝑛𝑖=1𝑅(𝐸𝑖)𝑅(𝐸1[𝐸2])=𝑅(𝐸1)∪𝑅(𝐸2)𝑅(𝑓(𝐸1,...,𝐸𝑛))=—𝑛𝑖=1𝑅(𝐸𝑖)𝑅([𝐸|𝑥in𝐸1:𝐸2])=𝑅(𝐸)∪𝑅(𝐸1)∪𝑅(𝐸2)𝑅(target(𝑆))=𝑅(𝑆)𝑅(𝑥[𝐸1]...[𝐸𝑛]=𝐸)=—𝑛𝑖=1𝑅(𝐸𝑖)∪𝑅(𝐸)𝑅(𝑆1;𝑆2)=𝑅(𝑆1)∪𝑅(𝑆2)𝑅(if(𝐸)𝑆1else𝑆2)=𝑅(𝐸)∪𝑅(𝑆1)∪𝑅(𝑆2)𝑅(for(𝑥in𝐸1:𝐸2)𝑆)=𝑅(𝐸1)∪𝑅(𝐸2)∪𝑅(𝑆)\{𝑥}𝑅(skip)=∅𝑅(factor(𝐸))=𝑅(𝐸)𝑅(𝐿∼𝑑(𝐸1,...,𝐸𝑛))=𝑅(𝐿)∪𝑅(𝐸1)∪···∪𝑅(𝐸𝑛)Definition12(Samples-tosete𝑊(𝑆)).e𝑊(𝑆)isthesetthatcontainsthenamesofglobalvariablesthathavebeensampledwithinthestatementS.Itisdefinedrecursivelyasfollows:e𝑊(𝐿=𝐸)=∅e𝑊(𝑆1;𝑆2)=e𝑊(𝑆1)∪e𝑊(𝑆2)e𝑊(if(𝐸)𝑆1else𝑆2)=e𝑊(𝑆1)∪e𝑊(𝑆2)e𝑊(for(𝑥in𝐸1:𝐸2)𝑆)=e𝑊(𝑆)\{𝑥}e𝑊(skip)=∅e𝑊(factor(𝐸))=∅e𝑊(𝑥[𝐸1]...[𝐸𝑛]∼𝑑(𝐸1,...,𝐸𝑛))={𝑥}Definition13(Freevariables𝐹𝑉(𝑆)).𝐹𝑉(𝑆)isthesetthatcontainsthefreevariablesthatareusedinastatement𝑆.Itisrecursivelydefinedasfollows:𝐹𝑉(𝑥)={𝑥}𝐹𝑉(𝑐)=∅𝐹𝑉([𝐸1,...,𝐸𝑛])=—𝑛𝑖=1𝐹𝑉(𝐸𝑖)𝐹𝑉(𝐸1[𝐸2])=𝐹𝑉(𝐸1)∪𝐹𝑉(𝐸2)𝐹𝑉(𝑓(𝐸1,...,𝐸𝑛))=—𝑛𝑖=1𝐹𝑉(𝐸𝑖)𝐹𝑉([𝐸|𝑥in𝐸1:𝐸2])=𝐹𝑉(𝐸)∪𝐹𝑉(𝐸1)∪𝐹𝑉(𝐸2)𝐹𝑉(target(𝑆))=𝐹𝑉(𝑆)𝐹𝑉(𝑥[𝐸1]...[𝐸𝑛]=𝐸)=—𝑛𝑖=1𝐹𝑉(𝐸𝑖)∪𝐹𝑉(𝐸)𝐹𝑉(𝑆1;𝑆2)=𝐹𝑉(𝑆1)∪𝐹𝑉(𝑆2)𝐹𝑉(if(𝐸)𝑆1else𝑆2)=𝐹𝑉(𝐸)∪𝐹𝑉(𝑆1)∪𝐹𝑉(𝑆2)𝐹𝑉(for(𝑥in𝐸1:𝐸2)𝑆)=𝐹𝑉(𝐸1)∪𝐹𝑉(𝐸2)∪𝐹𝑉(𝑆)\{𝑥}𝐹𝑉(skip)=∅𝐹𝑉(factor(𝐸))=𝐹𝑉(𝐸)𝐹𝑉(𝐿∼𝑑(𝐸1,...,𝐸𝑛))=𝐹𝑉(𝐿)∪𝐹𝑉(𝐸1)∪···∪𝐹𝑉(𝐸𝑛)Definition14.WeoverloadthenotationΓ(𝐿)thatlooksupthetypeofanL-valueinΓ.Whenappliedtoamoregeneralexpression𝐸,Γ(𝐸)looksupthetypelevelof𝐸inΓ:Γ(𝑥)=ℓ,whereℓisthelevelof𝑥inΓΓ(𝑐)=dataΓ([𝐸1,...,𝐸𝑛])=ˆ𝑛𝑖=1Γ(𝐸𝑖)Γ(𝐸1[𝐸2])=Γ(𝐸1)⊔Γ(𝐸2)Γ(𝑓(𝐸1,...,𝐸𝑛))=ˆ𝑛𝑖=1Γ(𝐸𝑖)Γ([𝐸|𝑥in𝐸1:𝐸2])=Γ(𝐸)⊔Γ(𝐸1)⊔Γ(𝐸2)Definition15.Γ(𝐸1,...,𝐸𝑛)≡Γ(𝐸1)⊔···⊔Γ(𝐸𝑛).ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:44MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárDefinition16(𝑅Γ⊢ℓ(𝑆)).𝑅Γ⊢ℓ(𝑆)isthesetthatcontainsthenamesofglobalvariablesthathavebeenreadatlevelℓwithinthestatement𝑆.Itisdefinedrecursivelyasfollows:𝑅Γ⊢ℓ(𝑥[𝐸1]...[𝐸𝑛]=𝐸)=(cid:26)—𝑛𝑖=1𝑅(𝐸𝑖)∪𝑅(𝐸)ifΓ(𝑥)=(_,ℓ)∅otherwise𝑅Γ⊢ℓ(𝑆1;𝑆2)=𝑅Γ⊢ℓ(𝑆1)∪𝑅Γ⊢ℓ(𝑆2)𝑅Γ⊢ℓ(if(𝐸)𝑆1else𝑆2)=𝑅Γ⊢ℓ(𝐸)∪𝑅Γ⊢ℓ(𝑆1)∪𝑅Γ⊢ℓ(𝑆2)𝑅Γ⊢ℓ(for(𝑥in𝐸1:𝐸2)𝑆)=𝑅Γ⊢ℓ(𝐸1)∪𝑅Γ⊢ℓ(𝐸2)∪𝑅Γ⊢ℓ(𝑆)\{𝑥}𝑅Γ⊢ℓ(skip)=∅𝑅Γ⊢ℓ(factor(𝐸))=(cid:26)𝑅(𝐸)ifℓ=model∅else𝑅Γ⊢ℓ(𝐿∼𝑑(𝐸1,...,𝐸𝑛))=𝑅(𝐿∼𝑑(𝐸1,...,𝐸𝑛))ifℓ=ˆ{ℓ′|∃𝑥∈𝐹𝑉(𝐿∼𝑑(𝐸1,...,𝐸𝑛)∃𝜏.Γ(𝑥)=(𝜏,ℓ′)}∅otherwise.Definition17(𝑊Γ⊢ℓ(𝑆)).𝑊Γ⊢ℓ(𝑆)≜{𝑥∈𝑊(𝑆)|Γ(𝑥)=(𝜏,ℓ)forsome𝜏}Definition18(e𝑊Γ⊢ℓ(𝑆)).e𝑊Γ⊢ℓ(𝑆)≜{𝑥∈e𝑊(𝑆)|Γ(𝑥)=(𝜏,ℓ)forsome𝜏}Definition19.Givenastatement𝑆,wedefinethestatementst(𝑆)bystructuralinductionon𝑆:st(𝑥[𝐸1]...[𝐸𝑛]=𝐸)=𝑥[𝐸1]...[𝐸𝑛]=𝐸st(𝑆1;𝑆2)=st(𝑆1);st(𝑆2)st(if(𝐸)𝑆1else𝑆2)=if(𝐸)st(𝑆1)elsest(𝑆2))st(for(𝑥in𝐸1:𝐸2)𝑆)=for(𝑥in𝐸1:𝐸2)st(𝑆)st(skip)=skipst(factor(𝐸))=skipst(𝐿∼𝑑(𝐸1,...,𝐸𝑛))=skipDefinition20(Neighboursof𝑧,ne(Γ,Γ′,𝑧)).Fora⊢typingenvironmentΓ,a⊢2typingenvironmentΓ′=Γ′𝜎,Γ′xandavariable𝑧∈dom(Γ′x),theneighboursof𝑧aredefinedas:ne(Γ,Γ′,𝑧)≜{𝑥:(𝜏,ℓ)∈Γ′x|ℓ=l1andΓ(𝑥)=(int⟨𝐾⟩,model)forsome𝐾}A.2ProofsRestatementofLemma1(Noninterferenceof⊢)Suppose𝑠1|=Γ,𝑠2|=Γ,and𝑠1≈ℓ𝑠2forsomeℓ.ThenforSlicStanstatement𝑆andexpression𝐸:(1)IfΓ⊢𝐸:(𝜏,ℓ)and(𝑠1,𝐸)⇓𝑉1and(𝑠2,𝐸)⇓𝑉2then𝑉1=𝑉2.(2)IfΓ⊢𝑆:ℓand(𝑠1,𝑆)⇓𝑠′1,𝑤1and(𝑠2,𝑆)⇓𝑠′2,𝑤2then𝑠′1≈ℓ𝑠′2.Proof.(1)followsbyruleinductiononthederivationΓ⊢𝐸:(𝜏,ℓ),andusingthatifΓ⊢𝐸:(𝜏,ℓ),𝑥∈𝑅(𝐸)andΓ(𝑥)=(𝜏′,ℓ′),thenℓ′≤ℓ.(2)followsbyruleinductiononthederivationΓ⊢𝑆:ℓandusing(1).Mostcasesfollowtriviallyfromtheinductivehypothesis.Anexceptionisthe(Target)case,whichweshowbelow.(Target)Weusethepremise∀ℓ′>ℓ.𝑅Γ⊢ℓ′(𝑆)=∅,togetherwithalemmathatfor𝑆,𝑠1and𝑠2suchthat𝑠1,𝑆⇓𝑠′1,𝑤1,and𝑠2,𝑆⇓𝑠′2,𝑤2,and∀𝑥∈𝑅(𝑆).𝑠1(𝑥)=𝑠2(𝑥),wehavethat𝑤1=𝑤2.(Thislemmafollowsbystructuralinductionon𝑆.)Inthecaseof(Target),𝑠1,target(𝑆)⇓𝑤1,and𝑠2,target(𝑆)⇓𝑤2and𝑅(𝑆)=—ℓ′𝑅Γ⊢ℓ′(𝑆)=(cid:0)—ℓ′≤ℓ𝑅Γ⊢ℓ′(𝑆)(cid:1)∪(—ℓ′>ℓ𝑅Γ⊢ℓ′(𝑆))=—ℓ′≤ℓ𝑅Γ⊢ℓ′(𝑆).Then,forany𝑥∈𝑅(𝑆),𝑥∈𝑅Γ⊢ℓ′(𝑆)forsomeℓ′≤ℓ,soΓ(𝑥)=(𝜏,ℓ𝑥)suchthatℓ𝑥≤ℓ′≤ℓ.Andthus,bydefinitionof≈ℓ,𝑠1(𝑥)=𝑠2(𝑥)forany𝑥∈𝑅(𝑆).Byapplyingthelemmaabove,wethenget𝑤1=𝑤2,asrequired.□ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:45RestatementofLemma4(Shreddingproducessingle-levelstatements)𝑆⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)=⇒Γ⊢data(𝑆𝐷)∧Γ⊢model(𝑆𝑀)∧Γ⊢genquant(𝑆𝑄)Proof.Byruleinductiononthederivationof𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄.□RestatementofLemma5(Propertyofsingle-levelstatements)LetΓ𝜎,Γx⊢𝑆beSlicStanprogram,suchthat𝑆issingle-levelstatementoflevelℓ,Γ⊢ℓ(𝑆).Thenthereexistuniquefunctions𝑓and𝜙,suchthatforany𝜎,x|=Γ𝜎,Γx:J𝑆K(𝜎)(𝑥)=𝑓(𝜎≤ℓ,x≤ℓ)∪𝜎>ℓ,𝜙(𝜎≤ℓ)(x≤ℓ),wherewewrite𝜎≤ℓ={(𝑥↦→𝑉)∈𝜎|Γ𝜎(𝑥)=(_,ℓ)}and𝜎>ℓ=𝜎\𝜎≤ℓ.Proof.Thispropertyfollowsfromnoninterference(Lemma1),ifweunderstandfactorandsamplestatementsasassignmentstoareservedweightvariablesofdifferentlevels.LetΓ,𝑆beaSlicStanprogramandsupposeweobtain𝑆′by:•Substitutingeveryfactor(𝐸)statementwith𝑤ℓ=𝑤ℓ∗𝐸,whereΓ(𝐸)=real,ℓand𝑤data,𝑤modeland𝑤qenqantarewrite-only,distinctandreservedvariablesintheprogram.•Substitutingevery𝐿∼𝑑(𝐸1,...,𝐸𝑛)statementwith𝑤ℓ=𝑤ℓ∗𝑑pdf(𝐿|𝐸1,...,𝐸𝑛),whereΓ(𝑑pdf(𝐿|𝐸1,...,𝐸𝑛))=real,ℓ.Thenforall𝜎,x|=Γ,wehaveJ𝑆K𝑝(𝜎)(x)=˛ℓ𝜎′(𝑤ℓ),where𝜎′=J𝑆′K𝑠(𝜎,∀ℓ.𝑤ℓ↦→1)(x).Bynon-interference(Lemma1),foranylevelℓandstore𝜎2≈ℓ𝜎,if𝜎′2=J𝑆′K𝑠(𝜎2,∀ℓ.𝑤ℓ↦→1)(x),then𝜎′2≈ℓ𝜎′.Thus𝜎′2(𝑤ℓ′)=𝜎2(𝑤ℓ′)forℓ′≤ℓ,andtherefore,when𝑆isasingle-levelstatementoflevelℓ,J𝑆′K𝑠(𝜎,∀ℓ.𝑤ℓ↦→1)(x)=𝑓(𝜎≤ℓ,x≤ℓ),𝜎>ℓ,𝑤≤ℓ↦→𝜙(𝜎≤ℓ,x≤ℓ),𝑤>ℓ↦→1,forsomefunctions𝑓and𝜙.Finally,thisgivesusJ𝑆K𝑠(𝜎,x)=(𝑓(𝜎≤ℓ,x≤ℓ),𝜎>ℓ),J𝑆K𝑝(𝜎,x)=𝜙(𝜎≤ℓ,x≤ℓ).□RestatementofLemma6(SemanticPreservationof⇕Γ)IfΓ⊢𝑆:dataand𝑆⇕Γ(𝑆𝐷,𝑆𝑀,𝑆𝑄)thenJ𝑆K=J𝑆𝐷;𝑆𝑀;𝑆𝑄K.Proof.Followsbyadaptingprooffrom[Gorinovaetal.2019].□RestatementofLemma10(SemanticPreservationof⇕Γ2)IfΓ⊢2𝑆:l1and𝑆⇕Γ𝑆1,𝑆2,𝑆3thenJ𝑆K=J𝑆1;𝑆2;𝑆3K.Proof.Followsbyadaptingprooffrom[Gorinovaetal.2019].□Lemma13.ForaSlicStanexpression𝐸andafunction𝜙(𝑥,𝑦)=𝑉,where𝑉isavaluesuchthat(𝜎,𝑥,𝑦),𝐸⇓𝑉forevery𝑥and𝑦andsome𝜎,if𝑥∉𝑅(𝐸),then:∃𝜙′suchthat𝜙(𝑥,𝑦)=𝜙′(𝑦)forall𝑥,𝑦Proof.Byinductiononthestructureof𝐸.□RestatementofTheorem1(Shreddinginducesafactorisationofthedensity).SupposeΓ⊢𝑆:dataand𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄andΓ=Γ𝜎∪ΓD∪Γ𝜽∪Γ𝑄.Forall𝜎,D,𝜽,and𝑄:if𝜎,D,𝜽,𝑄|=Γ𝜎,ΓD,Γ𝜽,Γ𝑄,andJ𝑆K𝑝(𝜎)(D,𝜽,𝑄)∝𝑝(D,𝜽,𝑄)ande𝑊(𝑆𝑄)=dom(Γ𝑄)then:(1)J𝑆𝑀K𝑝(𝜎𝐷)(D,𝜽,𝑄)∝𝑝(𝜽,D)(2)J𝑆𝑄K𝑝(𝜎𝑀)(D,𝜽,𝑄)=𝑝(𝑄|𝜽,D)where𝜎𝐷=J𝑆𝐷K𝑠(𝜎)(D,𝜽,𝑄)and𝜎𝑀=J𝑆𝑀K𝑠(𝜎𝐷)(D,𝜽,𝑄).Proof.Weprovethisbyestablishingamoregeneralresult:For𝜎,D,𝜽,𝑄|=Γ𝜎,ΓD,Γ𝜽,Γ𝑄,𝐴=e𝑊(𝑆𝑄)⊆𝑄andsome𝐵⊆𝑄\𝐴,ifJ𝑆K𝑝(𝜎)(D,𝜽,𝑄)∝𝑝(D,𝜽,𝐴|𝐵)then:(1)J𝑆𝐷K𝑝(𝜎)(D,𝜽,𝑄)=1ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:46MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár(2)J𝑆𝑀K𝑝(𝜎𝐷)(D,𝜽,𝑄)=𝑝(𝜽,D)(3)J𝑆𝑄K𝑝(𝜎𝑀)(D,𝜽,𝑄)=𝑝(𝐴|𝜽,D,𝐵)Notethatinthecasewheree𝑊(𝑆𝑄)=𝑄,wehave𝐴=𝑄and𝐵=∅,andtheoriginalstatementofthetheorem,J𝑆𝑄K𝑝(𝜎𝑀)(D,𝜽,𝑄)=𝑝(𝑄|𝜽,D),holds.Weprovetheextendedformulationabovebyinductiononthestructureof𝑆anduseofLemma2,Lemma4andLemma5,Lemma6.Takeany𝜎,D,𝜽,𝑄|=Γ𝜎,ΓD,Γ𝜽,Γ𝑄andletΦ(𝑆,𝑆𝐷,𝑆𝑀,𝑆𝑄)≜Γ⊢𝑆:data∧𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄∧𝐴=e𝑊(𝑆𝑄)=⇒∃𝐵⊆𝑄\𝐴.∀𝜎𝐷,𝜎𝑀.(J𝑆K𝑝(𝜎)(D,𝜽,𝑄)∝𝑝(D,𝜽,𝐴|𝐵)∧J𝑆𝐷K(𝜎)(D,𝜽,𝑄)=𝜎𝐷∧J𝑆𝑀K(𝜎𝐷)(D,𝜽,𝑄)=𝜎𝑀=⇒J𝑆𝐷K𝑝(𝜎)(D)=1∧J𝑆𝑀K𝑝(𝜎𝐷)(D,𝜽)=𝑝(𝜽,D)∧∃𝐵⊆𝑄\e𝑊(𝑆𝑄).J𝑆𝑄K𝑝(𝜎𝑀)(D,𝜃,𝑄)=𝑝(𝐴|𝜽,D,𝐵)(cid:17)TakeanyΓ,𝑆,𝑆𝐷,𝑆𝑀,𝑆𝑄suchthat𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄,𝐴=e𝑊(𝑆𝑄),andtakeany𝜎,D,𝜽,𝑄|=Γ𝜎,ΓD,Γ𝜽,Γ𝑄,anunnormaliseddensity𝑝and𝐵⊆𝑄\𝐴,suchthatJ𝑆K𝑝(𝜎)(D,𝜽,𝑄)∝𝑝(D,𝜽,𝐴|𝐵).Weprovebyruleinductiononthederivationof𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄thatΦ(𝑆,𝑆𝐷,𝑆𝑀,𝑆𝑄).(ShredSeq)Let𝑆=𝑆1;𝑆2and𝑆1⇕Γ𝑆1𝐷,𝑆1𝑀,𝑆1𝑄and𝑆2⇕Γ𝑆2𝐷,𝑆2𝑀,𝑆2𝑄.Thus𝑆⇕Γ(𝑆1𝐷;𝑆2𝐷),(𝑆1𝑀;𝑆2𝑀),(𝑆1𝑄;𝑆2𝑄).AssumeΦ(𝑆1,𝑆1𝐷,𝑆1𝑀,𝑆1𝑄)andΦ(𝑆2,𝑆2𝐷,𝑆2𝑀,𝑆2𝑄).Let:•𝐴1=e𝑊(𝑆1𝑄)and𝐵1⊆𝑄\𝐴1issuchthatJ𝑆1𝑄K𝑝(𝜎𝑀)(D,𝜽,𝑄)=𝑝1(𝐴1|D,𝜽,𝐵1).•J𝑆1K(𝜎)(D,𝜽,𝑄)=𝜎′.•J𝑆1K𝑝(𝜎)(D,𝜽,𝑄)∝𝑝1(D,𝜽,𝐴1|𝐵1).•𝐴2=e𝑊(𝑆2𝑄)and𝐵2⊆𝑄\𝐴2issuchthatJ𝑆2𝑄K𝑝(𝜎𝑀)(D,𝜽,𝑄)=𝑝2(𝐴2|D,𝜽,𝐵2).•J𝑆2K𝑝(𝜎′)(D,𝜽,𝑄)∝𝑝2(D,𝜽,𝐴2|𝐵2).Thus,byLemma2,J𝑆K𝑝=J𝑆1;𝑆2K𝑝=J𝑆1K𝑝×J𝑆2K𝑝,so𝑝(D,𝜽,𝐴|𝐵)∝𝑝1(D,𝜽,𝐴1|𝐵1)𝑝2(D,𝜽,𝐴2|𝐵2).For(1),wehave∀𝜎|=Γ𝜎.J𝑆1𝐷K𝑝(𝜎)(D,𝜽,𝑄)=J𝑆2𝐷K𝑝(𝜎)(D,𝜽,𝑄)=1.Thus,byLemma2,J𝑆1𝐷;𝑆2𝐷K𝑝=J𝑆1𝐷K𝑝×J𝑆2𝐷K𝑝=1.FromΦ(𝑆1,𝑆1𝐷,𝑆1𝑀,𝑆1𝑄)andΦ(𝑆2,𝑆2𝐷,𝑆2𝑀,𝑆2𝑄)wealsohave:•J𝑆1𝑄K𝑝(𝜎𝑀)(D,𝜽,𝑄)=𝑝(𝐴1|𝜽,D,𝐵1)•J𝑆2𝑄K𝑝(𝜎′𝑀)(D,𝜽,𝑄)=𝑝(𝐴2|𝜽,D,𝐵2)𝐴=e𝑊(𝑆𝑄)=e𝑊(𝑆1𝑄;𝑆2𝑄)=e𝑊(𝑆1𝑄)∪e𝑊(𝑆2𝑄)=𝐴1∪𝐴2From𝑆welltyped,itmustbethecasethat𝐴1∩𝐴2=∅.Thus,wewrite𝐴=𝐴1,𝐴2.Wewillprovethatthepropertyholdsfor𝐵=𝐵1∪𝐵2\𝐴1\𝐴2.Bysemanticpreservationof⇕Γ(Lemma6),J𝑆1K𝑝=J𝑆1𝐷;𝑆1𝑀;𝑆1𝑄K𝑝=J𝑆1𝐷K𝑝×J𝑆1𝑀K𝑝×J𝑆1𝑄K𝑝∝1×𝑝1(𝜽,D)×𝑝1(𝐴1|𝜽,D,𝐵1).Similarly,J𝑆2K𝑝∝1×𝑝2(𝜽,D)×𝑝2(𝐴2|𝜽,D,𝐵2)=𝑝2(𝜽,D)𝑝2(𝐴2|𝜽,D,𝐴1,𝐵1).But𝑝(𝜽,D,𝐴|𝐵)∝𝑝1(𝜽,D,𝐴1|𝐵1)𝑝2(𝜽,D,𝐴2|𝐵2),so:𝑝(𝜽,D,𝐴|𝐵)∝𝑝1(𝜽,D)𝑝1(𝐴1|𝜽,D,𝐵1)𝑝2(𝜽,D)𝑝2(𝐴2|𝜽,D,𝐴1,𝐵1)ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:47So,𝑝(𝜽,D)=∫𝑝(D,𝜽,𝐴|𝐵)𝑝(𝐵)𝑑𝐴𝑑𝐵∝∫𝑝1(𝜽,D)𝑝1(𝐴1|𝜽,D,𝐵1)𝑝2(𝜽,D)𝑝2(𝐴2|𝜽,D,𝐴1,𝐵1)𝑝(𝐵)𝑑𝐴1𝑑𝐴2𝑑𝐵∝𝑝1(𝜽,D)𝑝2(𝜽,D)∫𝑝(𝐵)𝑝1(𝐴1|𝜽,D,𝐵1)𝑝2(𝐴2|𝜽,D,𝐴1,𝐵1)𝑑𝐴1𝑑𝐴2𝑑𝐵=𝑝1(𝜽,D)𝑝2(𝜽,D)∫𝑝(𝐵)(cid:18)∫𝑝1(𝐴1|𝜽,D,𝐵1)(cid:18)∫𝑝2(𝐴2|𝜽,D,𝐴1,𝐵1)𝑑𝐴2(cid:19)𝑑𝐴1(cid:19)𝑑𝐵=𝑝1(𝜽,D)𝑝2(𝜽,D)∝𝑝1(𝜽,D)𝑝2(𝜽,D)ThusJ𝑆𝑀K𝑝=J𝑆1𝑀;𝑆2𝑀K𝑝∝𝑝1(𝜽,D)𝑝2(𝜽,D)∝𝑝(𝜽,D)Finally,forlastpropertyon𝑆,weusethechainruleofprobability,semanticspropertyofsequencing,andtheresultfromabovetoget:𝑝(𝐴|D,𝜽,𝐵)=𝑝(D,𝜽,𝐴|𝐵)𝑝(D,𝜽|𝐵)∝𝑝1(D,𝜽)𝑝2(D,𝜽)𝑝1(𝐴1|D,𝜽,𝐵1)𝑝2(𝐴2|D,𝜽,𝐵2)𝑝(D,𝜽)×𝑝(𝐵)𝑝(𝐵|D,𝜽)∝𝑝1(𝐴1|D,𝜽,𝐵1)𝑝2(𝐴2|D,𝜽,𝐵2)=J𝑆1𝑄K𝑝J𝑆2𝑄K𝑝=J𝑆𝑄K𝑝Thus:𝑝(𝐴|D,𝜽,𝐵)=𝑝1(𝐴1|D,𝜽,𝐵1)𝑝2(𝐴2|D,𝜽,𝐵2)𝑍Where:𝑍=∫𝑝1(𝐴1|D,𝜽,𝐵1)𝑝2(𝐴2|D,𝜽,𝐵2)𝑑𝐴=∫𝑝1(𝐴1|D,𝜽,𝐵1)(cid:18)∫𝑝2(𝐴2|D,𝜽,𝐵2)𝑑𝐴2(cid:19)𝑑𝐴1=1So𝑍=1,and𝑝(𝐴|D,𝜽,𝐵)=𝑝1(𝐴1|D,𝜽,𝐵1)𝑝2(𝐴2|D,𝜽,𝐵2)=J𝑆𝑄K𝑝.Thus:•J𝑆𝐷K𝑝=J𝑆1𝐷;𝑆2𝐷K𝑝=1•J𝑆𝑀K𝑝=J𝑆1𝑀;𝑆2𝑀K𝑝∝𝑝1(𝜽,D)𝑝2(𝜽,D)=𝑝(𝜽,D)•J𝑆𝑄K𝑝=J𝑆1𝑄;𝑆2𝑄K𝑝=𝑝1(𝐴1|𝜽,D,𝐵1)𝑝2(𝐴2|𝜽,D,𝐴1,𝐵1)=𝑝(𝐴1,𝐴2|𝜽,D,𝐵)Φ((𝑆1;𝑆2),(𝑆1𝐷;𝑆2𝐷),(𝑆1𝑀;𝑆2𝑀),(𝑆1𝑄;𝑆2𝑄))fromhere.□RestatementofLemma9(Shreddingproducessingle-levelstatements2)𝑆⇕Γ𝑆1,𝑆2,𝑆3=⇒Γ⊢l1(𝑆1)∧Γ⊢l2(𝑆2)∧Γ⊢l3(𝑆3)Proof.Byruleinductiononthederivationof𝑆⇕Γ𝑆1,𝑆2,𝑆3.□RestatementofLemma10(Semanticpreservationof⇕Γ,⊢2)IfΓ⊢2𝑆:l1and𝑆⇕Γ𝑆1,𝑆2,𝑆3thenJ𝑆K=J𝑆1;𝑆2;𝑆3K.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:48MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárProof.□RestatementofLemma11(Propertyofsingle-levelstatements2)LetΓ𝜎,Γx,𝑆beaSlicStanprogram,andΓ⊢2𝑆:l1,and𝑆issingle-levelstatementoflevelℓ,Γ⊢2ℓ(𝑆).Thenthereexistuniquefunctions𝑓and𝜙,suchthatforany𝜎,x|=Γ𝜎,Γx:(1)Ifℓ=l1,thenJ𝑆K(𝜎)(𝑥)=(cid:0)𝑓(𝜎l1,xl1),𝜎l2,𝜎l3(cid:1),𝜙(𝜎l1)(xl1)(2)Ifℓ=l2,thenJ𝑆K(𝜎)(𝑥)=(cid:0)𝜎l1,𝑓(𝜎l1,𝜎l2,xl1,xl2),𝜎l3(cid:1),𝜙(𝜎l1,𝜎l2)(xl1,xl2)(3)Ifℓ=l3,thenJ𝑆K(𝜎)(𝑥)=(cid:0)𝜎l1,𝜎l2,𝑓(𝜎l1,𝜎l3,xl1,xl3)(cid:1),𝜙(𝜎l1,𝜎l3)(xl1,xl3)Proof.Byunderstandingfactorandsamplestatementsasassignmenttoareservedweightvariablesofdifferentlevels(similarlytoLemma5)andnoninterference(Lemma7).□RestatementofLemma12(Existenceofmodeltogenqanttransformation)ForanySlic-StanprogramΓ,𝑆suchthatΓ⊢𝑆:l1,andavariable𝑧∈dom(Γ)suchthatΓ(𝑧)=(int⟨𝐾⟩,model),thereexistsaSlicStanprogramΓ′,𝑆′,suchthat,Γ,𝑆𝑧−→Γ′,𝑆′andΓ′(𝑧)=(int⟨𝐾⟩,genquant)Proof.TakeaSlicStanprogramΓ,𝑆,atypingenvironmentΓ𝑀,avariable𝑧,andstatements𝑆𝐷,𝑆𝑀and𝑆𝑄,suchthat:Γ(𝑧)=(int⟨𝐾⟩,model)Γ⊢𝑆:dataΓ𝑧−→Γ𝑀𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄Γ𝑀⊢2𝑆𝑀:l1Takealsostatements𝑆1,𝑆2,𝑆3,and𝑆′𝑀,andatypingenvironmentΓnesuchthat𝑆𝑀⇕Γ𝑀𝑆1,𝑆2,𝑆3Γne=ne(Γ,Γ𝑀,𝑧)𝑆′𝑀=𝑆1;𝑓=𝜙(Γne){elim(int⟨𝐾⟩𝑧)𝑆2};factor(𝑓[dom(Γne)]);𝑆3;gen(𝑧)𝑆2;st(𝑆2)LetΓ′issuchthatdom(Γ′)=dom(Γ)∪{𝑓}andforall𝑥:𝜏,ℓ∈Γ:Γ′(𝑥)=(𝜏,ℓ)ifℓ≠model(𝜏,ℓ)ifℓ=modelandΓ𝑀(𝑥)≠(𝜏,l2)(𝜏,genqant)ifℓ=modelandΓ𝑀(𝑥)=(𝜏,l2)Bysemanticpreservationofshredding(Lemma6,Lemma10)andtypepreservationoftheoperationalsemantics([Gorinovaetal.2019]),Γ⊢𝑆𝐷;𝑆1;𝑆2;𝑆3;𝑆𝑄:data,andthus,by(Seq),Γ⊢𝑆𝐷:data,Γ⊢𝑆1:data,...,Γ⊢𝑆𝑄:data.BydefinitionofΓ′,Γ′data⊂Γdata.𝑆𝐷issingle-levelofleveldataandΓ⊢𝑆𝐷:data,soΓdata⊢𝑆𝐷:dataandthusΓ′⊢𝑆𝐷:data.Similarly,Γ⊢𝑆1:DandΓ⊢𝑆3:D.Γ⊢𝑆2:data,sousing(Phi),(Elim)and(Factor),andnotingthatbydefinitiondom(Γne)⊂dom(Γ𝑀,l1),soΓne⊂Γ,wecanderive:Γ′⊢𝑓=𝜙(Γne){elim(int⟨𝐾⟩𝑧)𝑆2};factor(𝑓[dom(Γne)]):dataByΓ⊢𝑆2:dataandthedefinitionofΓ′,andusing(Gen)anddefinitionofst,wealsoderive:Γ′⊢gen(𝑧)𝑆2;st(𝑆2):genqantFinally,𝑆𝑄isasingle-levelstatementoflevelgenqantandforall𝑥:𝜏,ℓ∈Γ,𝑥:𝜏,ℓ′∈Γ,whereℓ≤ℓ′.Therefore,Γ⊢𝑆𝑄:dataimpliesΓ′⊢𝑆𝑄:data.Altogether,thisgivesusΓ′⊢𝑆𝐷;𝑆′𝑀;𝑆𝑄,andsoby(ElimGen),Γ,𝑆𝑧−→Γ′,𝑆𝐷;𝑆′𝑀,𝑆𝑄.□ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:49Lemma14.LetΓ,𝑆beaSlicStanprogram,suchthat𝜎,x|=Γ,J𝑆K𝑠(𝜎)(x)=𝜎′andJ𝑆K𝑝(𝜎)(x)=𝜓(x)forsomefunction𝜓.If𝑓∉dom(Γ)isafreshvariable,𝑧,𝑧1,...𝑧𝑛∈dom(Γx)arediscretevariablesofbasetypesint⟨𝐾⟩,int⟨𝐾1⟩,...,int⟨𝐾𝑛⟩respectively,and𝑆′isastatementsuchthat𝑆′=𝑓=𝜙(int⟨𝐾1⟩𝑧1,...int⟨𝐾𝑛⟩𝑧𝑛){elim(int⟨𝐾⟩𝑧)𝑆};factor(𝑓[𝑧1,...,𝑧𝑛]);thenJ𝑆′K𝑠(𝜎)(x)=𝜎′′with𝜎′′[−𝑓]=𝜎′andJ𝑆′K𝑝(𝜎)(x)=˝𝐾𝑧=1𝜓(x).Proof.Byexaminingtheoperationalsemanticsofassignment,factor,andthederivedformselimand𝜙.□Lemma15.LetΓ,𝑆beaSlicStanprogram,suchthat𝜎,x|=Γ,J𝑆K𝑠(𝜎)(x)=𝜎′andJ𝑆K𝑝(𝜎)(x)=𝜓(x)forsomefunction𝜓.If𝑧∈dom(Γx)isadiscretevariableofbasetypeint⟨𝐾⟩,and𝑆′isastatementsuchthat𝑆′=gen(𝑧)𝑆;st(𝑆);thenJ𝑆′K𝑠(𝜎)(x)=𝜎′,𝜓(x)isnormalisablewithrespectto𝑧with𝜓(x)∝𝑝(𝑧|x\{𝑧}),andJ𝑆′K𝑝(𝜎)(x)=𝑝(𝑧|x\{𝑧}).Proof.Byexaminingtheoperationalsemanticsof∼andtarget,andbyinductiononthestructureof𝑆toproveJst(𝑆)K𝑠=J𝑆K𝑠andJst(𝑆)K𝑝=1.□TypingRulesforDerivedForms:(Elim)Γ′⊢𝑆:data𝑅Γ⊢genqant(𝑆)=∅Γ′=Γ[𝑧↦→int⟨𝐾⟩,model]Γ⊢elim(int⟨𝐾⟩𝑧)𝑆:model(Gen)Γ(𝑧)=(int,genqant)Γ⊢𝑆:dataΓ⊢gen(int⟨𝐾⟩𝑧)𝑆:genqant(Phi)Γ′⊢𝑆:data∀ℓ′>ℓ.𝑅Γ⊢ℓ′(𝑆)=∅Γ′=Γ[𝑧1↦→(int⟨𝐾1⟩,ℓ),...,𝑧𝑁↦→(int⟨𝐾𝑁⟩,ℓ)]Γ⊢𝜙(int⟨𝐾1⟩𝑧1,...,int⟨𝐾𝑁⟩𝑧𝑁)𝑆:real,ℓRestatementofTheorem4(Semanticpreservationof𝑧−→)ForSlicStanprogramsΓ,𝑆andΓ′,𝑆′,andadiscreteparameter𝑧:Γ,𝑆𝑧−→Γ′,𝑆′→J𝑆K=J𝑆′K.Proof.LetΓ,𝑆andΓ′,𝑆′beSlicStanprograms,and𝑧beadiscreteparameter,suchthatΓ,𝑆𝑧−→Γ′,𝑆′.Let𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄,𝑆⇕Γ′𝑆′𝐷,𝑆′𝑀,𝑆′𝑄,and𝑆𝑀⇕Γ′′𝑆1,𝑆2,𝑆3forΓ′′suchthatΓ𝑧−→Γ′′andΓ′′⊢2𝑆𝑀:l1.LetΓ=Γ𝜎,Γdata,Γmodel,Γgenqant,Γ′=Γ′𝜎,Γ′data,Γ′model,Γ′genqantandΓ′′=Γ′′𝜎,Γ′′l1,Γ′′l2,Γ′′l3betheusualpartitioningofeachofthetypingenvironments.Let𝑧beastoresuchthat𝑧|={𝑧:Γ(𝑧)}.LetD,𝜽and𝑄bestoressuchthatD|=Γdata,𝑧,𝜽|=Γmodel,and𝑄|=Γgenqant.Let𝜽1,𝜽2and𝜽3beapartitioningof𝜽,suchthatD,𝜽1|=Γ′′l1,𝑧,𝜽2|=Γ′′l2,and𝜽3|=Γ′′l3.Then,bydefinitionofΓ𝑧−→Γ′′,𝜽2=𝑧.ByTheorem1:•J𝑆𝐷K𝑝(𝜎)(D,𝑧,𝜽,𝑄)=1•J𝑆𝑀K𝑝(𝜎𝐷)(D,𝑧,𝜽,𝑄)∝𝑝(𝑧,𝜽,D)ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:50MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákár•J𝑆𝑄K𝑝(𝜎𝑀)(D,𝑧,𝜽,𝑄)=𝑝(𝑄|𝑧,𝜽,D)Γ,𝑆𝑑−→Γ′,𝑆′,thus𝑆′mustbeoftheform𝑆′=𝑆𝐷;𝑆1;𝑓=𝜙(Γl1′′){elim(int⟨𝐾⟩𝑧)𝑆2};factor(𝑓[dom(Γ′′l1)]);𝑆3;gen(𝑧)𝑆2;st(𝑆2);𝑆𝑄whereΓ⊢𝑆:data,𝑆⇕Γ𝑆𝐷,𝑆𝑀,𝑆𝑄,Γ𝑧−→Γ′′,Γ⊢2𝑆𝑀:l1,and𝑆𝑀⇕Γ′′𝑆1,𝑆2,𝑆3.Therelation⇕Γissemantics-preservingforwell-typedprogramswithrespecttoboth⊢and⊢2(Lemma6andLemma10).ThusJ𝑆K=J𝑆𝐷;𝑆1;𝑆2;𝑆3;𝑆𝑄K.Wepresentadiagrammaticderivationofthechangeonstoreanddensitythateachsub-partintheoriginalandtransformedprogrammakesinFigure13.Combiningalloftheseresultsgivesthat:J𝑆′K𝑠(𝜎)(D,𝜽,𝑄)=𝜎′′=𝜎′[𝑓↦→𝑣]=J𝑆K𝑠(𝜎)((D,𝜽,𝑄))[𝑓↦→𝑣]Inotherwords,thetransformation𝑧−→preservesstoresemantics(uptocreatingofonenewfreshvariablef).Forthedensity,weget:J𝑆′K𝑝(𝜎)(D,𝜽,𝑄)=𝜙1(D,𝜽1)"(cid:213)𝑧𝜙2(D,𝜽1,𝑧)#𝜙3(D,𝜽1,𝜽3)𝑝(𝑧|D,𝜽1)𝑝(𝑄|D,𝜽)fromFigure13="(cid:213)𝑧𝜙1(D,𝜽1)𝜙2(D,𝜽1,𝑧)𝜙3(D,𝜽1,𝜽3)#𝑝(𝑧|D,𝜽1)𝑝(𝑄|D,𝜽)bythedistributivelaw∝"(cid:213)𝑧𝑝(D,𝜽1,𝑧,𝜽2)#𝑝(𝑧|D,𝜽1)𝑝(𝑄|D,𝜽)byTheorem1andLemma10=𝑝(D,𝜽1,𝜽2)𝑝(𝑧|D,𝜽1)𝑝(𝑄|D,𝜽)marginalisationof𝑧=𝑝(D,𝜽1,𝜽2)𝑝(𝑧|D,𝜽1,𝜽3)𝑝(𝑄|D,𝜽)by𝑧⊥⊥𝜽3|𝜽1(Theorem3)=𝑝(D,𝜽,𝑄)bythechainruleforprobability∝J𝑆K𝑝(𝜎)(D,𝜽,𝑄)Together,thisgivesusJ𝑆K=J𝑆′K(upto𝑆′creatingonenewfreshvariablef).□ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:51𝜎𝜎(𝐷),1𝜎(𝐷1),𝜙1(𝜎(𝐷1)l1)(D,𝜽1)(cid:16)𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2),𝜎(𝐷1)l3(cid:17),𝜙2(𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2))(D,𝜽1,𝑧)(cid:16)𝜎(𝐷1)l1,𝜎(𝐷1)l2,𝑓↦→𝑣,𝜎(𝐷1)l3(cid:17),˝𝑧𝜙2(𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2))(D,𝜽1,𝑧)(cid:16)𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2),𝑓3(𝜎(𝐷1)l1,l3)(cid:17),𝜙3(𝜎(𝐷1)l1,𝑓3(𝜎(𝐷1)l1,l3))(D,𝜽1,𝜽3)(cid:16)𝜎(𝐷1)l1,𝜎(𝐷1)l2,𝑓↦→𝑣,𝑓3(𝜎(𝐷1)l1,l3)(cid:17),𝜙3(𝜎(𝐷1)l1,𝑓3(𝜎(𝐷1)l1,l3))(D,𝜽1,𝜽3)(cid:16)𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2),𝑓↦→𝑣,𝑓3(𝜎(𝐷1)l1,l3)(cid:17),𝑝(𝑧|D,𝜽1)𝑓𝑔(cid:16)𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2),𝑓3(𝜎(𝐷1)l1,l3)(cid:17),𝑓↦→𝑣,𝜙𝑔(𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2),𝑓3(𝜎(𝐷1)l1,l3))(D,𝜽,𝑄)=𝑝(𝑄|D,𝜽)𝑓𝑔(cid:16)𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2),𝑓3(𝜎(𝐷1)l1,l3)(cid:17),𝜙𝑔(𝜎(𝐷1)l1,𝑓2(𝜎(𝐷1)l1,l2),𝑓3(𝜎(𝐷1)l1,l3))(D,𝜽,𝑄)=𝑝(𝑄|D,𝜽)𝑆𝐷𝑆1𝑆2byLemma11𝑆′2byLemma14𝑆3byLemma11𝑆3byLemma11and𝑓freshgen(𝑧)𝑆2byLemma15𝑆𝑄byTheorem1𝑆𝑄byTheorem1and𝑓freshFig.13.Diagrammaticproofofsemanticpreservationof𝑧−→BEXAMPLESB.1SprinklerOften,beginnersareintroducedtoprobabilisticmodellingthroughsimple,discretevariableexam-ples,astheyaremoreintuitivetoreasonabout,andoftenhaveanalyticalsolutions.Unfortunately,ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:52MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárGraphicalmodelcloudysprin-klerrainwetSlicStan+discreteparameterssupport1datareal[2]p_rain,p_sprinkler;2datareal[2][2]p_wet;3realp∼beta(1,1);4int<2>cloudy∼bern(p);5int<2>sprinkler∼bern(p_sprinkler[cloudy]);6int<2>rain∼bern(p_rain[cloudy]);7int<2>wet∼bern(p_wet[sprinkler][rain]);SlicStan1...2f1=𝜙(int<2>rain,int<2>sprinkler){3elim(int<2>cloudy){4cloudy∼bern(p);5sprinkler∼bern(p_sprinkler[cloudy]);6rain∼bern(p_rain[cloudy]);}}7f2=𝜙(int<2>rain,int<2>wet){8elim(int<2>sprinkler){9factor(f1[rain,sprinkler]);10wet∼bern(p_wet[sprinkler,rain]);}}11f3=𝜙(int<2>wet){elim(int<2>rain){12factor(f2[rain,wet]);}}13f4=𝜙(){elim(int<2>wet){14factor(f3[wet]);}}15factor(f4);16...Fig.14.The‘Sprinkler’example.onecannotexpresssuchexamplesdirectlyinPPLsthatdonotsupportdiscreteparameters.Onewell-knowndiscretevariableexample,oftenusedintutorialsonprobabilisticmodelling,isthe‘Sprinkler’example.Itmodelstherelationshipbetweencloudyweather,whetheritrains,whetherthegardensprinklerison,andthewetnessofthegrass.InFigure14,weshowaversionofthesprinklermodelwritteninSlicStanwithdiscreteparameters(left)andthemarginalisationpartofitscorrespondingtransformedversion(right).Ascloudy⊥⊥wet|sprinkler,rain,wedonotneedtoincludewetintheeliminationofcloudy,andthenewfactoriscomputedfordifferentvaluesofonlysprinklerandrain(lines2–6).Therestofthevariablesareeliminatedone-by-one,involvingallremainingvariables(lines7–15).ThesnippetoftheSlicStancodegeneratedbyourtransformationisanexactimplementationofthevariableeliminationalgorithmforthismodel.Thisnotonlyfacilitatesaplatformforlearningprobabilisticprogrammingusingstandardintroductorymodels,butitcanalsobeausefultoolforlearningconceptssuchasmarginalisation,conditionalindependence,andexactinferencemethods.B.2Soft-K-meansmodelInFigure15,wepresentthestandardsoft-k-meansclusteringmodelasitiswritteninSlicStanwithsupportfordiscretemodelparameters(left).Therightcolumnshowstheresultingcodethatourprogramtransformationgenerates.ThiscodeconsistsofplainSlicStancodeandnosupportfordiscretemodelparametersisneededtoperforminferenceonit.Themodelcanbeusedfor(softly)dividing𝑁datapointsyin𝐷-dimensionalEuclideanspaceinto𝐾clusterswhichhavemeans𝝁andprobability𝝅.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.Conditionalindependencebytyping4:53SlicStan+discretedataintD;dataintK;datareal[K]pi;datarealN=3;datareal[D][N]y;real[D][K]mu;for(din1:D){for(kin1:K){mu[d][k]∼normal(0,1);}}int<K>z1∼categorical(pi);int<K>z2∼categorical(pi);int<K>z3∼categorical(pi);for(din1:D){y[d][1]∼normal(mu[d][z1],1);y[d][2]∼normal(mu[d][z2],1);y[d][3]∼normal(mu[d][z3],1);}SlicStan...for(din1:D){for(kin1:K){mu[d,k]∼normal(0,1);}}factor(elim(int<K>z1){z1∼categorical(pi);for(dataintdin1:D){y[d,1]∼normal(mu[d,z1],1);}});factor(elim(int<K>z2){z2∼categorical(pi);for(dataintdin1:D){y[d,2]∼normal(mu[d,z2],1);}});factor(elim(int<K>z3){z3∼categorical(pi);for(dataintdin1:D){y[d,3]∼normal(mu[d,z3],1);}});gen(intz3){z3∼categorical(pi);for(dataintdin1:D){y[d,3]∼normal(mu[d,z3],1);}}gen(intz2){z2∼categorical(pi);for(dataintdin1:D){y[d,2]∼normal(mu[d,z2],1);}}gen(intz1){z1∼categorical(pi);for(dataintdin1:D){y[d,1]∼normal(mu[d,z1],1);}}Fig.15.Soft𝐾-means.B.3AcausalinferenceexampleThequestionofhowtoadaptPPLstocausalqueries,hasbeenrecentlygainingpopularity.Onewaytoexpressinterventionsandreasonaboutcausality,istoassumeadiscretevariablespecifyingthedirection(orabsenceof)causalrelationship,andspecifydifferentbehaviourforeachcaseusingifstatements[Winn2012].Weshowasimplecausalinferenceexample(Figure16)writteninSlicStanwithdirectsupportfordiscreteparameters(left)andthecodethatourtransformationgenerates(right)onwhichwecanperforminferenceusingacombinationofe.g.HMCandancestralsampling.Thismodelcanbereadasfollows.Assumethatweareinasituationwherewewanttoansweracausalquestion.Wewanttoanswerthisquestionbasedon𝑁pairedobservationsof𝐴and𝐵,insomeofwhichwemighthaveintervened(doB).Ourmodelproceedsbydrawinga(prior)ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.4:54MariaI.Gorinova,AndrewD.Gordon,CharlesSutton,andMatthijsVákárSlicStan+discretedatarealq;dataintN;dataint[N]A,B,doB;datarealprob_intervention;realpAcausesB∼beta(1,1);int<2>AcausesB∼bernoulli(pAcausesB);for(nin1:N)if(doB[n]>0)B[n]∼bernoulli(prob_intervention);if(AcausesB>1){for(nin1:N){A[n]∼bernoulli(0.5);if(doB[n]<1){if(A[n]>0){B[n]∼bernoulli(q);}else{B[n]∼bernoulli(1-q);}}}}else{for(nin1:N){if(doB[n]<1){B[n]∼bernoulli(0.5);}if(B[n]>0){A[n]∼bernoulli(q);}else{A[n]∼bernoulli(1-q);}}}SlicStandatarealq;dataintN;dataint[N]A,B,doB;datarealprob_intervention;realpAcausesB∼beta(1,1);for(dataintnin1:N)if(doB[n]>0)B[n]∼bernoulli(prob_intervention);factor(elim(int<2>AcausesB){AcausesB∼bernoulli(pAcausesB);if(AcausesB>1){for(dataintnin1:N){A[n]∼bernoulli(0.5);if(doB[n]<1){if(A[n]>0){B[n]∼bernoulli(q);}else{B[n]∼bernoulli(1-q);}}}}else{for(dataintnin1:N){if(doB[n]<1){B[n]∼bernoulli(0.5);}if(B[n]>0){A[n]∼bernoulli(q);}else{A[n]∼bernoulli(1-q);}}}});Fig.16.Acausalinferenceexample.probabilitythat𝐴causes𝐵fromabetadistribution,andthenspecifying𝐴and𝐵fordifferentscenarios(intervention,𝐴causes𝐵andnointervention,𝐵causes𝐴andnointervention)usingconditionalstatements.ACMTrans.Program.Lang.Syst.,Vol.44,No.1,Article4.Publicationdate:December2021.5.2. Discussion

153

5.2 Discussion

The paper presents two type systems:

(1) Generative subprogram type system

. The ﬁrst type system is close to the

(cid:96)

original SlicStan (Chapter 4), where code is split into pre-processing, core, and

post-processing parts. The improvement implemented in the current chapter allows

us to treat sampling statements as either contributing to the evaluation of the target

density function (density-based semantics) or as literal sampling (sampling-based

semantics) based on context. This allows for a SlicStan program to be sliced into

a pre-processing part, core part, where inference is done using Hamiltonian Monte

Carlo or another resource-consuming inference algorithm, and purely generative part,

where samples can be generated through ancestral sampling. That is, the sliced

program corresponds to HMC-AS hybrid inference.

(2) Conditional independence type system

2. The second type system is the
main contribution of the paper and allows for exploring conditional independence

(cid:96)

relationships. One way in which this system can be used is to transform a program

with discrete parameters by marginalising out variables eﬃciently. The transformed

program can be seen as a hybrid inference algorithm on the original program, where

continuous parameters are drawn using Hamiltonian Monte Carlo or another gradient-

based algorithm, while the discrete parameters are drawn using variable elimination.

That is, the transformed program corresponds to HMC-VE hybrid inference.

While diﬀerent, the two type systems, and the program slicing associated with each, share

a lot of elements. For example, noninterference holds for both systems (Lemma 1 and

Lemma 7). The two shredding relations associated with each of

2 are identical, up
to the naming of level types. Shredding a SlicStan statement with respect to either type

and

(cid:96)

(cid:96)

system produces single level statements (Lemma 4 and Lemma 9), preserves semantics

(Lemma 6 and 10), and induces a (type system dependent) factorisation of the density

(Theorem 1 and Theorem 2).

This hints at the possibility of generalising the approach. In particular, future work may

build a general framework for static analysis of probabilistic programs, where the choice

of partially ordered set and the design of typing rules can be motivated by a particular

factorisation of interest. If such a framework can be built in a modular way, it could serve

as a basis for a PPL that supports composable and programmable inference.

154

Errata

Chapter 5. Conditional independence by typing

The sentence “Sometimes, when σ is clear from context, we will leave it implicit and

simply write p(x) for p(x; σ).” on page 4:9 of the paper is an artefact left from previous

version of the notation and is redundant.

Part II

Dynamic optimisation for

probabilistic programming

155

156

Introduction to Part II

In the previous part, we saw examples of static analysis for probabilistic programming,

that is analysis that is performed at compile-time, before the program is executed. Such

analysis can be incredibly useful for exploiting conditional independencies and performing

semantic-preserving rewrites, which improve the performance of the underlying inference

algorithm. However, inference performance is often aﬀected not only by the structure of

the probabilistic model in question, but also by the nature of the observed data.

Dynamic analysis refers to the class of program analysis techniques that are performed

during the execution of a program. This part focuses on dynamic analysis for probabilistic

programs and presents one such dynamic method in Chapter 6. The method transforms

a program at run-time, taking into account both the model and the data, in order to

improve the quality of inference results.

CHAPTER 6

Automatic reparameterisation

Variational inference pre-processing for eﬃcient inference

This chapter presents an instance of dynamic program analysis for probabilistic program-

ming: an automatic model reparameterisation procedure. The parameterisation of a model

is the particular way in which the model is expressed in terms of parameters. What param-

eterisation we choose to express our model plays a vital role in the way the geometry of

the posterior of interest looks like, and hence (as we saw in § 2.2), has a direct eﬀect on the
quality of inference. While changing the parameterisation of a model, or reparameterising

it, can be done in many ways (for example through a simple static transformation), this

chapter looks at reparameterisation through the lens of eﬀect-handling based PPLs. Such

PPLs provide a dynamic way to interpret random assignment statements and I give further

background on the topic in § 6.1. The main contribution of the chapter is the paper
Automatic Reparameterisation of Probabilistic Programs (§ 6.2), which presents a way to
reparameterise Edward2 programs using eﬀect-handlers and based on both the deﬁned

model and the observed data. Finally, the chapter discusses the impact and limitations of

the proposed approach (§ 6.3).

6.1 Eﬀect-handling based probabilistic programming

Chapter 2 proposed classifying PPLs in three categories — explicit (§§ 2.3.2), implicit
(§§ 2.3.3), and eﬀect-handling based (§§ 2.3.4) — and brieﬂy introduced each category. In
this section, I give further background on the mechanisms behind the last, eﬀect-handling

based type of probabilistic programming languages. This section is partially based on

Moore and Gorinova (2018).

157

158

Chapter 6. Automatic reparameterisation

6.1.1 Eﬀects and eﬀect handling

When working with complex code, or when we want to rigorously reason about the

behaviour of a program, it is often useful to write pure functions. That is, deterministic

functions that do not mutate a global state, or any of the input arguments, do not throw

exceptions and do not perform I/O operations. They do not interact with the external

world, or code outside of the function, in any way. Such functions have no side eﬀects.

While it is easier to reason about pure functions, it can also be very constraining in

real-world situations.

Algebraic eﬀects and their handlers have emerged as a convenient way to control impure

behaviour. They are built upon a strong mathematical foundation (Plotkin and Pretnar,

2009; Plotkin and Power, 2003), their semantics can be precisely deﬁned (Kammar et al.,

2013), and in some cases they provide an improvement in runtime complexity compared

to pure languages (Hillerstr¨om et al., 2020). While providing a useful formal framework

for reasoning about side eﬀects, algebraic eﬀects and handlers have also proved to be an

increasingly convenient modular abstraction that has been adopted across many disciplines.

Some examples include concurrent programming (Dolan et al., 2017), meta programming

(Yallop, 2017), and probabilistic programming (Bingham et al., 2019).

The premise behind algebraic eﬀects and handlers is that impure behaviour arises from a

set of eﬀectful operations: for example, read, print, set, raise, and so on. Such operations

interact with some handling code in order to execute. For example, consider a process that

wants to read from a ﬁle. It sends a request to the OS kernel and suspends execution. The

kernel checks the request, executes it, and responds with the result of the read operation.

The process then resumes execution. This idea of eﬀect handlers as operating systems in

further explored by Hillerstr¨om (2021).

While in the case of accessing a ﬁle, the handling code is external to the program, in some

cases it could also be internal: for example, when an exception is risen, it can be handled

inside the same program. Algebraic eﬀects and handlers extend this idea of handling

programmatically exceptions, to any eﬀectful operation. The concrete implementation of

operations is given and managed by code written by the user.

To demonstrate the workings of eﬀect handlers in this section, Consider a simple example

of a program that reverses the order of print statements given by Pretnar (2015):

let abc = (print("a"); print("b"); print("c"))

let reverse = handler { print(s; x. k)

with reverse handle abc

k(); print(s) }

→

6.1. Eﬀect-handling based probabilistic programming

159

Here, print is an operation, while abc is a computation that prints out the letters ‘a’, ‘b’

and ‘c’, in this order, using three separate calls to print. Key to working with algebraic

eﬀects is the presence of continuations: functions that specify how the program “continues”

after a certain point; they express “what to do next” (Appel, 1992). Thus, the program

print("a"); print("b"); print("c") is a shorthand for the continuation-passing style program

print("a", lambda x. print("b", lambda y. print("c", lambda z. ())))

where print(s,k) is a continuation-passing style version of print, where we print to standard

output and then explicitly call the continuation k: print(s,k) = print(s);k(), though this

behaviour may change based on an eﬀect handler.

To simplify the lambda notation, consider the notation print(s;x.k), which explicitly

says that the result of print(s) is bound to x, where the continuation k depends on x:

print(x;s.k) is equivalent to print(s, lambda x. k(x)). The example above is then:

print("a"; x. print("b"; y. print("c"; z. return1 ())))

As print does not return a result, we may also simply write print(s. k):

print("a". print("b". print("c". return ())))

The handler reverse reverses the order in which print operations are executed. Every

print(s.k) operation call inside of the context of reverse is executed based on the body of

the handler: the handler ﬁrstly resumes the continuation k, and only then performs the

operation itself. The computation with reverse handle abc is the result of executing abc,

while handling operations with reverse. This prints out ‘c’, ‘b’ and ‘a’ in this order.

Types of eﬀect handlers. There exist diﬀerent types of handlers, depending on the

implementation, described in detail by Kammar et al. (2013) and Hillerstr¨om (2016).

For example, open handlers forward unhandled operations to other handlers, but closed

handlers do not. Eﬀects are handled only once with shallow handlers, but they are

propagated to any nested handlers and can be handled several times when using deep

handlers. If the continuation k must be invoked exactly once, then the handlers are linear,

while if it could be invoked more than once, they are multi-shot. Exception handlers do

not invoke the continuation.

Throughout this section, I assume open deep handlers. Edward2 uses open linear handlers,

which are shallow, but allow for explicit eﬀect forwarding, which mimics deep handlers.

1In this chapter, return x can be thought of as an operation, whose default behaviour is to discard
any subsequent continuation and ﬁnish the computation with x (similarly to exceptions behaviour). A
more in-depth discussion around the meaning of return is beyond the scope of this dissertation, but it is
examined in detail in (Hillerstr¨om, 2021, Section 1.2).

160

Chapter 6. Automatic reparameterisation

6.1.2 Composing eﬀect handlers

One very useful feature of open eﬀect handlers is that they can be nested or composed to

combine the way they interpret the computation. For example, consider a handler join,

which concatenates all strings appearing in a print statement. One way to deﬁne such a

handler is, as given by Pretnar (2015):

let join =

handler {

return v

print(s. k)

→

→

(v, "")
let (v, acc) = k() in return (v, s + acc)2 }

The computation with join handle abc evaluates to the tuple ((),"abc" ). Using reverse

in the context of join can be written as with join handle with reverse handle abc, which

evaluates to ((),"cba" ).

A more imperative-style way of writing handlers that pass around a “state” (in this case

the accumulated string acc) is with a mutable variable:

set(acc, "");

let join = handler { print(s. k)

with join handle with reverse handle abc;

set(acc, get(acc) + s); k() 2 }

→

return acc

Here, acc is a mutable variable initialised to "" that we concatenate to at every print

statement. The ﬁnal value of acc holds the result: "cba" .

This more imperative style is also closer to the Edward2 implementation of this chapter.

6.1.3 Eﬀect handling in probabilistic programming

Recently, eﬀect handlers have been adopted by some PPLs as a useful, modular way of

implementing transformations of probabilistic programs for inference (Scibior and Kammar,

2015; Bingham et al., 2019; Tran et al., 2018; Moore and Gorinova, 2018; Goldstein, 2019).

The insight is to treat sampling statements as operations that can be handled by a

separately deﬁned handler. This enables a range of useful program transformations,

including (though not limited to): conditioning, reparameterisation, tracing, density

function derivation, variational family generation. When implemented in a diﬀerentiable

programming framework, such as TensorFlow (Abadi et al., 2015), PyTorch (Paszke et al.,
2019), or JAX (Bradbury et al., 2018), the resulting code is also diﬀerentiable,3 making

2Here, + is used to mean string concatenation.
3These frameworks do not typically provide native support for eﬀects and handlers. The PPL eﬀect-
handling code is usually implemented in the diﬀerentiable language itself, meaning it is itself diﬀerentiable.

6.1. Eﬀect-handling based probabilistic programming

161

models easy to use in combination with gradient-based inference algorithms.

Consider the operation sample(dist; x. k), whose default behaviour is to sample at random

from the distribution dist, bind the result to the variable x and invoke the continuation

k. Consider also the operation observed((value, dist); x. k), whose default behaviour

simply binds x to the value value and invokes k. It is then straightforward to write a

probabilistic model using this sample operation. For example, the following code describes

a Beta-Binomial model of n trials:

let beta binomial(n) =

sample(beta(1., 1.); z);

sample(binomial(z, n); x);

return x

To condition on some data, say x = 7, we can use an eﬀect handler, which changes every

sample operation to an observed operation:

let condition(rv, value) =

handler { sample(dist; rv. k)

let conditioned(n) = with condition(x,4 7) handle beta binomial(n)

observed((value, dist); rv); k(value) }

→

This conditioning, however, is not enough for us to obtain a posterior of z. As we saw in

Chapter 2, one way to perform inference is by repeatedly evaluating the joint probability

density function of the model. Once again, we can use an eﬀect handler to obtain the joint

density function:

let joint(vals) =

set(density, 1);

let h = handler {

sample(dist; rv. k)

set(density, get(density) * pdf(dist,vals[rv]));

→

k(vals[rv])

observed((value,dist); rv.k)

→

set(density, get(density) * pdf(dist,value));

k(value)

}

with h handle conditioned(n);

return density

4Here, x is not a variable, but rather a name of a variable. This is an unsatisfactory solution for the
well-known problem of distinguishing between instances of the same eﬀect (Biernacki et al., 2019). How to
refer to variables from outside of the model in a principled way, is a well-known problem also in probabilistic
programming, where it leads to repeated syntax, such as Edward2’s x = ed.Normal(0, 1, name="x").

162

Chapter 6. Automatic reparameterisation

Here joint is a function on a dictionary that maps unobserved random variable names

to a particular value in their domain. In the case above, we would evaluate joint for

vals = dict(z

z val) for diﬀerent values of z val. Previously, we discussed handlers that

explicitly handle only a single eﬀect. Here, h handles two eﬀects, sample and observed,

(cid:55)→

meaning instances of either of these eﬀects will be handled in the context of h. The code

also assumes that a function pdf is available, such that for any distribution dist and a value

v in the domain of dist, pdf(dist, v) evaluates the probability density function of dist at v.

Eﬀect-handling based probabilistic programming languages have several advantages to

density- and sampling-based PPLs. Conceptually, eﬀect-handling based PPLs treat sam-

pling abstractly and separate its use in deﬁning the model from its actual implementation.

Thus, models can be interpreted as either sampling-based, density-based, or in a completely

diﬀerent way, depending on usage. In addition, working with eﬀects and handlers, and more

speciﬁcally being able to compose them, allows for sophisticated program transformations,

which do not need to be integrated into a separate compiler, but can be user-speciﬁed in the

PPL itself. This highlights the potential of eﬀect handlers as a basis for a programmable

inference framework (Mansinghka et al., 2018), although explicit work has not yet been

done in this direction.

The rest of this chapter shows several uses of eﬀect handlers in the probabilistic program-

ming language Edward2, presenting handlers that perform model reparameterisation and

automatic variational guide synthesis.

6.2 The paper

The main contribution of this chapter is the paper Automatic Reparameterisation of

Probabilistic Programs. It considers the problem of reparameterisation of probabilistic

programs: changing the way the model is expressed in terms of parameters, in order to

improve the quality of inference. The paper discusses the eﬀects of parameterisation on the

geometry of the posterior through examples, highlighting the practical challenges of ﬁnding

a suitable reparameterisation. It presents two techniques for automatically reparameterising

probabilistic programs using eﬀect handlers. The ﬁrst is a simple interleaved sampling from

two diﬀerent model parameterisations. The second provides a novel continuous relaxation

of the question of what parameterisation to use, and uses variational inference to ﬁnd a

suitable such parameterisation.

The paper was accepted for presentation at the Thirty-seventh International Conference

on Machine Learning (ICML 2020) and included in the Proceedings of Machine Learning

6.2. The paper

163

Research, Volume 119. Out of 4990 submissions in total, 1088 papers were accepted. An

earlier version of the paper also appeared at the non-archival symposium Advances in

Approximate Bayesian Inference (AABI 2018).

Author contributions. The paper is co-authored by me, Dave Moore and Matt Hoﬀ-

man. My contributions included modifying Edward2 to enable composing eﬀect handlers,

performing the initial analytical analysis, developing the automatic reparameterisation

framework, running preliminary experiments, and co-writing parts of the paper. Dave

Moore mentored the project throughout, implemented improvements that allowed for

using TensorFlow Probability’s MCMC kernels more eﬃciently, ran the ﬁnal experiments,

did the additional analysis presented in Appendix D, and co-wrote parts of the paper.

Matt Hoﬀman conceived the idea of the project, mentored it throughout, co-wrote parts

of the paper, and oﬀered feedback, comments, and suggestions.

AutomaticReparameterisationofProbabilisticProgramsMariaI.Gorinova*1DaveMoore2MatthewD.Hoffman2AbstractProbabilisticprogramminghasemergedasapow-erfulparadigminstatistics,appliedscience,andmachinelearning:bydecouplingmodellingfrominference,itpromisestoallowmodellerstodi-rectlyreasonabouttheprocessesgeneratingdata.However,theperformanceofinferencealgo-rithmscanbedramaticallyaffectedbytheparame-terisationusedtoexpressamodel,requiringuserstotransformtheirprogramsinnon-intuitiveways.Weargueforautomatingthesetransformations,anddemonstratethatmechanismsavailableinre-centmodellingframeworkscanimplementnon-centringandrelatedreparameterisations.Thisenablesnewinferencealgorithms,andwepro-posetwo:asimpleapproachusinginterleavedsamplingandanovelvariationalformulationthatsearchesoveracontinuousspaceofparameteri-sations.Weshowthattheseapproachesenablerobustinferenceacrossarangeofmodels,andcanyieldmoreefﬁcientsamplersthanthebestﬁxedparameterisation.1.IntroductionReparameterisingaprobabilisticmodelmeansexpressingitintermsofnewvariablesdeﬁnedbyabijectivetransfor-mationoftheoriginalvariablesofinterest.Thereparam-eterisedmodelexpressesthesamestatisticalassumptionsastheoriginal,butcanhavedrasticallydifferentposteriorgeometry,withsigniﬁcantimplicationsforbothvariationalandsampling-basedinferencealgorithms.Non-centringisaparticularlycommonformofreparam-eterisationinBayesianhierarchicalmodels.Considerarandomvariablez∼N(µ,σ);wesaythisisincentred*WorkdonewhileinterningatGoogle.1UniversityofEdin-burgh,Edinburgh,UK2Google,SanFrancisco,CA,USA.Corre-spondenceto:MariaI.Gorinova<m.gorinova@ed.ac.uk>,DaveMoore<davmre@google.com>,MatthewD.Hoffman<mhoff-man@google.com>.Proceedingsofthe37thInternationalConferenceonMachineLearning,Vienna,Austria,PMLR119,2020.Copyright2020bytheauthor(s).parameterisation(CP).Ifweinsteadworkwithanauxil-iary,standardnormalvariable˜z∼N(0,1),andobtainzbyapplyingthetransformationz=µ+σ˜z,wesaythevariable˜zisinitsnon-centredparameterisation(NCP).Al-thoughthecentredparameterisationisoftenmoreintuitive,non-centringcandramaticallyimprovetheperformanceofinference(Betancourt&Girolami,2015).Neal’sfunnel(Figure1a)providesasimpleexample:mostMarkovchainMonteCarlo(MCMC)algorithmshavetroublesamplingfromthefunnelduetothestrongnon-lineardependencebe-tweenlatentvariables.Non-centringthemodelremovesthisdependence,convertingthefunnelintoasphericalGaussiandistribution.Bayesianpractitionersareoftenadvisedtomanuallynon-centretheirmodels(StanDevelopmentTeametal.,2016);however,thisbreakstheseparationbetweenmodellingandinferenceandrequiresexpressingthemodelinapotentiallylessintuitiveform.Moreover,itrequirestheusertoun-derstandtheconceptofnon-centringandtoknowaprioriwhereinthemodelitmightbeappropriate.Becausethebestparameterisationforagivenmodelmayvaryacrossdatasets,evenexpertsmayneedtoﬁndtheoptimalparameterisationbytrialanderror,burdeningmodellersandslowingdownthemodeldevelopmentloop(Blei,2014).Weproposethatnon-centringandsimilarreparameterisa-tionsbehandledautomaticallybyprobabilisticprogram-mingsystems.Wedemonstratehowsuchprogramtrans-formationsmaybeimplementedusingtheeffecthandlingmechanismspresentinseveralmoderndeepprobabilisticprogrammingframeworks,andconsidertwoinferencealgo-rithmsenabledbyautomaticreparameterisation:interleavedHamiltonianMonteCarlo(iHMC),whichalternatesHMCstepsbetweencentredandnon-centredparameterisations,andanovelalgorithmwecallVariationallyInferredParame-terisation(VIP),whichsearchesoveracontinuousspaceofreparameterisationsthatincludesnon-centringasaspecialcase.1Wecomparethesestrategiestoaﬁxedcentredandnon-centredparameterisationacrossarangeofwell-knownhierarchicalmodels.OurresultssuggestthatbothVIPandiHMCcanenableformoreautomatedrobustinference,of-tenperformingatleastaswellasthebestﬁxedparame-1Codeforthesealgorithmsandexperimentsisavailableathttps://github.com/mgorinova/autoreparam.AutomaticReparameterisationofProbabilisticPrograms(a)Centred(left)andnon-centred(right)parame-terisation.NealsFunnel(z,x):z∼N(0,3)x∼N(0,exp(z/2))(b)Modelthatgeneratesvariableszandx.z=0lpz=logpN(z|0,3)x=0lpx=logpN(x|0,exp(z/2))(c)Themodelinthecontextoflog_prob_at_0.Figure1.Neal’sfunnel(Neal,2003):z∼N(0,3);x∼N(0,ez/2).terisationandsometimesbetter,withoutrequiringaprioriknowledgeoftheoptimalparameterisation.Bothstrategieshavethepotentialtofreemodellersfromthinkingaboutmanualreparameterisation,acceleratethemodellingcycle,andimprovetherobustnessofinferenceinnext-generationmodellingframeworks.2.RelatedWorkThevalueofnon-centringiswell-knowntoMCMCprac-titionersandresearchers(StanDevelopmentTeametal.,2016;Betancourt&Girolami,2015),andcanalsoleadtobettervariationalﬁtsinhierarchicalmodels(Yaoetal.,2018).However,theliteraturelargelytreatsthisasamod-ellingchoice;Yaoetal.(2018)proposethat“thereisnogeneralruletodeterminewhethernon-centredparameterisa-tionisbetterthanthecentredone.”Wearenotawareofpriorworkthattreatsnon-centringdirectlyasacomputationalphenomenontobeexploitedbyinferencesystems.Non-centredparameterisationofprobabilisticmodelscanbeseenasanalogoustothereparameterisationtrickinstochas-ticoptimisation(Kingma&Welling,2013);bothinvolveexpressingavariableintermsofadiffeomorphictransfor-mationfroma"standardised"variable.Inthecontextofprobabilisticinference,thesearecomplementarytools:thereparameterisationtrickyieldslow-variancestochasticgradi-entsofvariationalobjectives,whereasnon-centringchangesthegeometryoftheposterioritself,leadingtoqualitativelydifferentvariationalﬁtsandMCMCtrajectories.InthecontextofGibbssampling,Papaspiliopoulosetal.(2007)introduceafamilyofpartiallynon-centredparame-terisationssimilartothoseweuseinVIP(describedbelow)andshowthatitimprovesmixinginaspatialGLMM.Ourcurrentworkcanbeviewedasangeneral-purposeexten-sionofthisworkthatmechanicallyreparameterisesuser-providedmodelsandautomatesthechoiceofparameter-isation.Similarly,Yu&Meng(2011)proposedaGibbssamplingschemethatinterleavesstepsincentredandnon-centredparameterisations;ourinterleavedHMCalgorithmcanbeviewedasanautomated,gradient-baseddescendentoftheirscheme.Recently,therehasbeenworkonacceleratingMCMCinfer-encethroughlearnedreparameterisation:Parno&Marzouk(2018)andHoffmanetal.(2019)runsamplersintheimageofabijectivemapﬁttedtotransformthetargetdistribu-tionapproximatelytoanisotropicGaussian.Thesemaybeviewedas‘black-box’methodsthatrelyonlearningthetar-getgeometry,potentiallyusinghighlyexpressiveneuralvari-ationalmodels,whileweuseprobabilistic-programtransfor-mationstoapply‘white-box’reparameterisationssimilartothoseamodellercouldinprincipleimplementthemselves.Becausetheyexploitmodelstructure,white-boxapproachescancorrectpathologiessuchasthoseofNeal’sfunnel(Fig-ure1a)directly,reliably,andatmuchlowercost(inparame-tersandinferenceoverhead)thanblack-boxmodels.White-andblack-boxreparameterisationsarenotmutuallyexclu-sive,andmayhavecomplementaryadvantages;combiningthemisalikelyfruitfuldirectionforimprovinginferenceinstructuredmodels.Previousworkinprobabilisticprogramminghasbeenex-ploringother‘white-box’approachestoperformoroptimiseinference.Forexample,Hakaru(Narayananetal.,2016;Zinkov&Shan,2017)andPSI(Gehretal.,2016;2020)useprogramtransformationstoperformsymbolicinference,whileGen(Cusumano-Towneretal.,2019)andSlicStan(Gorinovaetal.,2019)canstaticallyanalysethemodelstructuretocompiletoamoreefﬁcientinferencestrategy.Tothebestofourknowledge,theapproachpresentedinthispaperistheﬁrsttoapplyvariationalinferenceasadynamicpre-processingstep,whichoptimisestheprogrambasedonboththeprogramstructureandobserveddata.3.UnderstandingtheEffectofReparameterisationNon-centringreparameterisationisnotalwaysoptimal;itsusefulnessdependsonpropertiesofboththemodelandtheobserveddata.Inthissection,wedevelopintuitionbyworkingwithasimplehierarchicalmodelforwhichwecanderivetheposterioranalytically.ConsiderasimpleAutomaticReparameterisationofProbabilisticProgramsrealisationofamodeldiscussedbyBetancourt&Girolami(2015,(2)),whereforavectorofNdatapointsy,andsomegivenconstantsσandσµ,wehave:θ∼N(0,1)µ∼N(θ,σµ)yn∼N(µ,σ)foralln∈1...NInthenon-centredmodel,yisdeﬁnedintermsof˜µandθ,where˜µisastandardGaussianvariable:θ∼N(0,1)˜µ∼N(0,1)yn∼N(θ+σµ˜µ,σ)foralln∈1...NFigure2aandFigure2bshowthegraphicalmodelsforthetwoparameterisations.Inthenon-centredcase,thedirectdependencybetweenθandµissubstitutedbyaconditionaldependencygiventhedatay,whichcreatesan“explainingaway”effect.Intuitively,thismeansthatthestrongertheevidenceyis(largeN,andsmallvariance),thestrongerthedependencybetweenθand˜µbecomes,creatingapoorly-conditionedposteriorthatmayslowinference.AstheGaussiandistributionisself-conjugate,theposteriorineachcase(centredornon-centred)isalsoaGaussiandistribution,andwecananalyticallyinspectitscovariancematrixV.Toquantifythequalityoftheparameterisationineachcase,weinvestigatetheconditionnumberκoftheposteriorcovariancematrixundertheoptimaldiagonalpreconditioner.Thismodelsthecommonpractice(imple-mentedintoolssuchasPyMC3andStanandfollowedinourexperiments)ofsamplingusingaﬁtteddiagonalprecon-ditioner.Figure2cshowstheconditionnumbersκCPandκNCPforeachparameterisationasafunctionofq=N/σ2;thefullderivationisinAppendixA.Thisﬁgureconﬁrmstheintu-itionthatthenon-centredparameterisationisbettersuitedforsituationwhentheevidenceisweak,whilestrongevi-dencecallsforcentredparameterisation.Inthisexamplewecanexactlydeterminetheoptimalparameterisation,sincethemodelhasonlyonevariablethatcanbereparameterisedandtheposteriorhasaclosedform.Inmorerealisticset-tings,evenexpertscannotpredicttheoptimalparameterisa-tionforhierarchicalmodelswithmanyvariablesandgroupsofdata,andthewrongchoicecanleadtopoorconditioning,heavytailsorotherpathologicalgeometry.4.ReparameterisingProbabilisticProgramsAnadvantageofprobabilisticprogrammingisthatthepro-gramitselfprovidesastructuredmodelrepresentation,andwecanexploremodelreparameterisationthroughthelensofprogramtransformations.Inthispaper,wefocusontransforminggenerativeprobabilisticprogramswheretheprogramrepresentsasamplingprocessdescribinghowthedatawasgeneratedfromsomeunknownlatentvariables.Mostprobabilisticprogramminglanguages(PPLs)providesomemechanismfortransformingagenerativeprocessintoaninferenceprogram;ourautomaticreparameterisationapproachisapplicabletoPPLsthattransformgenerativeprogramsusingeffecthandling.ThisincludesmoderndeepPPLssuchasPyro(UberAILabs,2017)andEdward2(Tranetal.,2018).4.1.EffectHandling-basedProbabilisticProgrammingConsideragenerativeprogram,whererunningtheprogramforwardgeneratessamplesfromtheprioroverlatentvari-ablesanddata.Effecthandling-basedPPLstreatgeneratingarandomvariablewithinsuchamodelasaneffectfulopera-tion(anoperationthatisunderstoodashavingsideeffects)andprovidewaysforresolvingthisoperationintheformofeffecthandlers,toallowforinference.Forexample,weoftenneedtotransformastatementthatgeneratesarandomvariabletoastatementthatevaluatessome(log)densityormassfunction.Wecanimplementthisusinganeffecthandler:log_prob_at_0=handler{v∼D(a1,...,aN)7→v=0;lpv=logpD(v|a1,...,aN)}2Thehandlerlog_prob_at_0handlesstatementsoftheformv∼D(a1,...,aN).Suchstatementsnormallymean“samplearandomvariablefromthedistributionD(a1,...,aN)andrecorditsvalueinv”.However,whenexecutedinthecontextoflog_prob_at_0(wewritewithlog_prob_at_0handlemodel),statementsthatcontainrandom-variableconstructionsarehandledbyset-tingthevalueofthevariablevto0,thenevaluatingthelogdensity(ormass)functionofD(a1,...,aN)atv=0andrecordingitsvalueinanew(program)variablelpv.Forexample,considerthefunctionimplementingNeal’sfun-nelinFigure1b.Whenexecutedwithoutanycontext,thisfunctiongeneratestworandomvariables,zandx.Whenexecutedinthecontextofthelog_prob_at_0handler,itdoesnotgeneraterandomvariables,butitinsteadevaluateslogpN(z|0,3)andlogpN(x|0,exp(z/2))(Figure1c).Thisapproachcanbeextendedtoproduceafunctionthatcorrespondstothelogjointdensity(ormass)func-tionofthelatentvariablesofthemodel.In§§B.1,wegivethepseudo-codeimplementationofafunctionmake_log_joint,whichtakesamodelM(z|x)—thatgenerateslatentvariableszandgeneratesandobservesdatax—andreturnsthefunctionf(z)=logp(z,x).Thisis2Algebraiceffectsandhandlerstypicallyinvolvepassingacontinuationwithinthehandler.WemakethecontinuationimplicittostayclosetoEdward2’simplementation.AutomaticReparameterisationofProbabilisticProgramsθµynn=1,...,N(a)Centred.θ˜µynn=1,...,N(b)Non-centred.(c)Theconditionnumberasafunctionofthedata’sstrength.Figure2.Effectsofreparameterisingasimplemodelwithknownposterior.acoreoperation,asittransformsagenerativemodelintoafunctionproportionaltotheposteriordistribution,whichcanberepeatedlyevaluatedandautomaticallydifferentiatedtoperforminference.Moregenerally,effectfuloperationsareoperationsthatcanhavesideeffects,e.g.writingtoaﬁle.Theprogram-minglanguagesliteratureformalisescaseswhereimpurebehaviourarisesfromasetofeffectfuloperationsintermsofalgebraiceffectsandtheirhandlers(Plotkin&Power,2001;Plotkin&Pretnar,2009;Pretnar,2015).Aconcreteimplementationforaneffectfuloperationisgivenintheformofeffecthandlers,which(similarlytoexceptionhan-dlers)areresponsibleforresolvingtheoperation.Effecthandlerscanbeusedasapowerfulabstractioninprobabilis-ticprogramming,andhavebeenincorporatedintorecentframeworkssuchasPyroandEdward2(Moore&Gorinova,2018).4.2.ModelReparameterisationUsingEffectHandlersOnceequippedwithaneffecthandling-basedPPL,wecaneasilyconstructhandlerstoperformmanymodeltransfor-mations,includingmodelreparameterisation.Non-centringHandler.ncp=handler{v∼N(µ,σ),v/∈data7→˜v∼N(0,1);v=µ+σ˜v}Anon-centringhandlercanbeusedtonon-centreallstan-dardisable3latentvariablesinamodel.Thehandlersimplyappliestostatementsoftheformv∼N(µ,σ),wherevisnotadatavariable,andtransformsthemto˜v∼N(0,1),v=µ+σ˜v.Whennestedwithinalog_probhandler(liketheonefrom§§4.1),log_probhandlesthetrans-formedstandardnormalstatement˜v∼N(0,1).Thus,make_log_jointappliedtoamodelinthencpcontext3WefocusonGaussianvariables,butnon-centringisbroadlyapplicable,e.g.tothelocation-scalefamilyandrandomvariablesthatcanbeexpressedasabijectivetransformationz=fθ(˜z)ofa“standardised”variable˜z.returnsthelogjointfunctionofthetransformedvariables˜zratherthantheoriginalvariablesz.Forexample,make_log_joint(NealsFunnel(z,x))gives:logp(z,x)=logN(z|0,3)+logN(x|0,exp(z/2))make_log_joint(withncphandleNealsFunnel(z,x))correspondstothefunction:logp(˜z,˜x)=logN(˜z|0,1)+logN(˜x|0,1)wherez=3˜zandx=exp(z/2)˜x.Thisapproachcaneasilybeextendedtootherparameter-isations,includingpartiallycentredparameterisations(asshownlaterin§§5.2),non-centringandwhiteningmulti-variateGaussians,andtransformingconstrainedvariablestohaveunboundedsupport.Edward2Implementation.Weimplementreparameter-isationhandlersinEdward2,adeepPPLembeddedinPythonandTensorFlow(Tranetal.,2018).AmodelinEdward2isaPythonfunctionthatgeneratesrandomvariables.InthecoreofEdward2isaspecialcaseofeffecthandlingcalledinterception.Toobtainthejointdensityofamodel,thelanguageprovidesthefunctionmake_log_joint_fn(model)4,whichusesalog_probinterceptor(handler)aspreviouslydescribed.Weextendtheusageofinterceptiontotreatsamplestate-mentsinoneparameterisationassamplestatementsinan-otherparameterisation(similarlytothencphandlerabove):defnoncentre(rv_fn,**d):#Assumesalocation-scalefamily.rv_fn=ed.interceptable5(rv_fn)rv_std=rv_fn(loc=0,scale=1)returnd["loc"]+d["scale"]*rv_std4Correspondstomake_log_joint(model)inourexample.5Wrappingtheconstructorwithed.interceptableen-suresthatwecannestthisinterceptorinthecontextofotherinterceptors.AutomaticReparameterisationofProbabilisticProgramsWeusetheinterceptorbyexecutingamodelofinterestwithintheinterceptor’scontext(usingPython’scontextman-agers).Thisoverrideseachrandomvariable’sconstructortoconstructavariablewithlocation0andscale1,andscaleandshiftthatvariableappropriately:withed.interception(noncentre):neals_funnel()WepresentandexplaininmoredetailallinterceptorsusedforthisworkinAppendixB.5.AutomaticModelReparameterisationWeintroducetwoinferencestrategiesthatexploitautomaticreparameterisation:interleavedHamiltonianMonteCarlo(iHMC),andtheVariationallyInferredParameterisation(VIP).5.1.InterleavedHamiltonianMonteCarloAutomaticreparameterisationopensupthepossibilityofal-gorithmsthatexploitmultipleparameterisationsofasinglemodel.WeconsiderinterleavedHamiltonianMonteCarlo(iHMC),whichusestwoHMCstepstoproduceeachsamplefromthetargetdistribution:theﬁrststepismadeinCP,us-ingtheoriginalmodellatentvariables,whilethesecondstepismadeinNCP,usingtheauxiliarystandardisedvariables.InterleavingMCMCkernelsacrossparameterisationshasbeenexploredinpreviousworkonGibbssampling(Yu&Meng,2011;Kastner&Frühwirth-Schnatter,2014),whichdemonstratedthatCPandNCPstepscanbecombinedtoachievemorerobustandperformantsamplers.Ourcon-tributionistomaketheinterleavingautomaticandmodel-agnostic:insteadofrequiringtheusertowritemultipleversionsoftheirmodelandacustominferencealgorithm,weimplementiHMCasablack-boxinferencealgorithmforcentredEdward2models.Algorithm1outlinesiHMC.IttakesasinglecentredmodelMcp(z|x)thatdeﬁneslatentvariableszandgeneratesdatax.Itusesthefunctionmake_ncptoautomaticallyobtainanon-centredversionofthemodel,Mncp(˜z|x),whichdeﬁnesauxiliaryvariables˜zandfunctionf,suchthatz=f(˜z).5.2.VariationallyInferredParameterisationThebestparameterisationforagivenmodelmaymixcen-tredandnon-centredrepresentationsfordifferentvariables.Toefﬁcientlysearchthespaceofreparameterisations,weproposethevariationallyinferredparameterisation(VIP)al-gorithm,whichselectsaparameterisationbygradient-basedoptimisationofadifferentiablevariationalobjective.VIPcanbeusedasapre-processingsteptoanotherinferencealgorithm;asitonlychangestheparameterisationofthemodel,MCMCmethodsappliedtothelearnedparameteri-sationmaintaintheirasymptoticguarantees.Consideramodelwithlatentvariablesz.Weintroduceparameterisationparametersλ=(λi)∈[0,1]foreachvariablezi,andtransformzi∼N(zi|µi,σi)bydeﬁning˜zi∼N(λiµi,σλii)andzi=µi+σ1−λii(˜zi−λiµi).ThisdeﬁnesacontinuousrelaxationthatincludesNCPasthespecialcaseλ=0andCPasλ=1.Moregenerally,itsupportsacombinatoriallylargeclassofper-variableandpartialcentrings.Example.RecalltheexamplemodelfromSection3,whichdeﬁnesthejointdensityp(θ,µ,y)=N(θ|0,1)×N(µ|θ,σµ)×N(y|µ,σ).Usingtheparameterisationabovetoreparameteriseµ,weget:p(θ,ˆµ,y)=N(θ|0,1)×N(ˆµ|λθ,σλµ)×N(y|θ+σ1−λµ(ˆµ−λθ),σ)Similarlytobefore,weanalyticallyderiveanexpressionfortheposteriorunderdifferentvaluesofλ.Figure4showstheconditionnumberκ(λ)ofthediagonallypreconditionedposterior,fordifferentvaluesofq=N/σ2withﬁxedpriorscaleσµ=1.Asexpected,whenthedataisweak(q=0.01),settingtheparameterisationparameterλtobecloseto0(NCP),resultsinabetterconditionedposteriorthansettingitcloseto1(CP),andconverselyforstrongdata(q=100).Moreinterestingly,inintermediatecases(q=1)theoptimalvalueforλistrulybetween0and1,yieldingamodestbutrealimprovementovertheextremepoints.Optimisation.Forageneralmodelwithlatentvariableszanddatax,weaimtochoosetheparameterisationλunderwhichtheposteriorp(˜z|x;λ)is“mostlike”anindepen-dentnormaldistribution.AnaturalobjectivetominimiseisKL(q(˜z;θ)||p(˜z|x;λ)),whereq(˜z;θ)=N(˜z|µ,diag(σ))isanindependentnormalmodelwithvaria-tionalparametersθ=(µ,σ).Minimisingthisdivergencecorrespondstomaximisingavariationallowerbound,theELBO(Bishop,2006):L(θ,λ)=Eq(˜z;θ)(logp(x,˜z;λ)−logq(˜z;θ))Notethattheauxiliaryparametersλarenotstatisticallyidentiﬁable:themarginallikelihoodlogp(x;λ)=logp(x)isconstantwithrespecttoλ.However,thecomputationalpropertiesofthereparameterisedmodelsdiffer,andthevariationalboundwillprefermodelsforwhichthepos-terioriscloseinKLtoadiagonalnormal.Ourkeyhy-pothesis(whichtheresultsinFigure6seemtosupport)isthatdiagonal-normalapproximabilityisagoodproxyforMCMCsamplingefﬁciency.Tosearchforagoodmodelreparameterisation,weoptimiseL(θ,λ)usingstochasticgradientstosimultaneouslyﬁttheAutomaticReparameterisationofProbabilisticProgramsAlgorithm1:InterleavedHamiltonianMonteCarloArguments:datax;acentredmodelMcp(z|x)Returns:Ssamplesz(1),...z(S)fromp(z|x)1:Mncp(˜z|x),f=make_ncp(Mcp(z|x))2:logpcp=make_log_joint(Mcp(z|x))3:logpncp=make_log_joint(Mncp(˜z|x))4:5:z0=init()6:fors∈[1,...,S]do7:z0=hmc_step(logpcp,z(s−1))8:z00=hmc_step(logpncp,f−1(z0))9:z(s)=f(z00)10:returnz(1),...,z(S)Algorithm2:VariationallyInferredParameterisationArguments:datax;acentredmodelMcp(z|x)Returns:Ssamplesz(1),...z(S)fromp(z|x)1:Mvip(˜z|x;λ),f=make_vip(Mcp(z|x))2:logp(x,˜z)=make_log_joint(Mvip(˜z|x;λ))3:4:Q(˜z;θ)=make_variational(Mvip(˜z|x;λ))5:logq(˜z;θ)=make_log_joint(q(˜z;θ))6:7:L(θ,λ)=Eq(logp(x,˜z;λ))−Eq(logq(˜z;θ))8:θ∗,λ∗=argmaxL(θ,λ)9:logp(x,˜z)=make_log_joint(Mvip(˜z|x;λ∗))10:z(1),...,z(S)=hmc(logp)11:returnf(z(1)),...,f(z(S))(a)Differentparameterisationsλofthefunnel,withmean-ﬁeldnormalvariationalﬁtq(˜z)(overlayedinwhite).(b)Alternativeviewasimplicitvariationaldistributionsq∗λ(z)(overlayedinwhite)ontheoriginalspace.Figure3.Neal’sfunnel:z∼N(0,3);x∼N(0,ez/2),withmean-ﬁeldnormalvariationalﬁtoverlayed.Figure4.Theconditionnumberκ(λ)forvaryingq=N/σ2andσµ=1inthesimplemodelfromSection3.variationaldistributionqtotheposteriorpandoptimisetheshapeofthatposterior.Figure3aprovidesavisualex-ample:anindependentnormalvariationaldistributionisapoorﬁttothepathologicalgeometryofacentredNeal’sfunnel,butnon-centringleadstoawell-conditionedposte-rior,wherethevariationaldistributionisaperfectﬁt.Ingeneralsettingswherethereparameterisedmodelisnotex-actlyGaussian,sampling-basedinferencecanbeusedtoreﬁnetheposterior;weapplyVIPasapreprocessingstepforHMC(summarisedinAlgorithm2).Boththereparame-terisationandtheconstructionofthevariationalmodelqareimplementedasautomaticprogramtransformationsusingEdward2’sinterceptors.AnalternateinterpretationofVIPisthatitexpandsavari-ationalfamilytoamoreexpressivefamilycapableofrep-AutomaticReparameterisationofProbabilisticProgramsresentingpriordependence.Lettingz=fλ(˜z)representthepartialcentringtransformation,anindependentnormalfamilyq(˜z)onthetransformedmodelcorrespondstoanimplicitposteriorq∗λ(z)=q(cid:0)˜z=f−1λ(z)(cid:1)|detJf−1λ(z)|ontheoriginalmodelvariables.Underthisinterpretation,λarevariationalparametersthatservetoaddfreedomtothevariationalfamily,allowingittointerpolatefromindepen-dentnormal(atλi=1,Figure3bleft)toarepresentationthatcapturestheexactpriordependencestructureofthemodel(atλi=0,Figure3bright).6.ExperimentsWeevaluatetheusefulnessofourapproachasarobustandfullyautomaticalternativetomanualreparameterisation.WecompareourmethodstoHMCranonfullycentredorfullynon-centredmodels,oneofwhichoftengivescatas-trophicallybadresults.OurresultsshownotonlythatVIPimprovesrobustnessbyavoidingcatastrophicreparame-terisations,butalsothatitsometimesﬁndsaparameteri-sationthatisbetterthanboththefullycentredandfullynon-centredalternatives.6.1.ModelsandDatasetsWeevaluateourproposedapproachesbyusingHamiltonianMonteCarlotosamplefromtheposteriorofhierarchicalBayesianmodelsonseveraldatasets:Eightschools(Rubin,1981):estimatingthetreatmentef-fectsθiofacoursetaughtateachofi=1...8schools,giventestscoresyiandstandarderrorsσi:µ∼N(0,5)logτ∼N(0,5)θi∼N(µ,τ)yi∼N(θi,σi)Radon(Gelman&Hill,2006):hierarchicallinearregres-sion,inwhichtheradonlevelriinahomeiincountycismodelledasafunctionofthe(unobserved)county-levelef-fectmc,thecountyuraniumreadinguc,andxi,thenumberofﬂoorsinthehome:µ,a,b∼N(0,1)mc∼N(µ+auc,1)logri∼N(mc[i]+bxi,σ)Germancredit(Dua&Graff,2017):logisticregression;hierarchicalprioroncoefﬁcientscales:logτ0∼N(0,10)logτi∼N(logτ0,1)βi∼N(0,τi)y∼Bernoulli(σ(βXT))Election’88(Gelman&Hill,2006):logisticmodelof1988USpresidentialelectionoutcomesbycounty,givendemographiccovariatesxiandstate-leveleffectsαs:βd∼N(0,100)µ∼N(0,100)logτ∼N(0,10)αs∼N(µ,τ)yi∼Bernoulli(σ(αs[i]+βTxi))ElectricCompany(Gelman&Hill,2006):pairedcausalanalysisoftheeffectofviewinganeducationalTVshowoneachof192classformsoverG=4grades.TheclassroomsweredividedintoP=96pairs,andoneclassineachpairwastreated(xi=1)atrandom:µg∼N(0,1)ap∼N(µg[p],1)bg∼N(0,100)logσg∼N(0,1)yi∼N(ap[i]+bg[i]xi,σg[i])6.2.AlgorithmsandExperimentalDetailsForeachmodelanddataset,wecompareourmethods,in-terleavedHMC(iHMC)andVIP-HMC,withbaselinesofrunningHMConeitherfullycentred(CP-HMC)orfullynon-centred(NCP-HMC)models.WeinitialiseeachHMCchainwithsamplesfromanindependentGaussianvaria-tionalposterior,andusetheposteriorscalesasadiagonalpreconditioner;forVIP-HMCthisvariationaloptimisationalsoincludestheparameterisationparametersλ.Allvaria-tionaloptimisationswererunforthesamenumberofsteps,sotheywereaﬁxedcostacrossallmethodsexceptiHMC(whichdependsonpreconditionersforboththecentredandnon-centredtransitionkernels).TheHMCstepsizeandnumberofleapfrogstepsweretunedfollowingtheproceduresdescribedinAppendixC,whichalsocontainsadditionaldetailsoftheexperimentalsetup.Wereporttheaverageeffectivesamplesizeper1000gra-dientevaluations(ESS/∇),withstandarderrorscomputedfrom200chains.Weusegradientevaluations,ratherthanwallclocktime,astheyarethedominantoperationinbothHMCandVIandareeasiertomeasurereliably;inpractice,thewallclocktimesweobservedpergradientevaluationdidnotdiffersigniﬁcantlybetweenmethods.Thisisnotsurpris-ing,sincethe(minimal)overheadofinterceptionisincurredonlyonceatgraph-buildingtime.Thismetricisadirectevaluationofthesampler;wedonotcountthegradientstepstakenduringtheinitialvariationaloptimization.Inadditiontoeffectivesamplesize,wealsodirectlyexam-inedtheconvergenceofposteriormomentsforeachmethod.Thisyieldedsimilarqualitativeconclusionstotheresultswereporthere;moreanalysiscanbefoundinAppendixD.6.3.ResultsFigures5and6showtheresultsoftheexperiments.Inmostcases,eitherthecentredornon-centredparameterisationworkswell,whiletheotherdoesnot.AnexceptionistheGermancreditdataset,wherebothCP-HMCandNCP-HMCgiveasmallESS:1.2±0.2or1.3±0.2ESS/∇respectively.iHMC.Acrossthedatasetsinbothﬁgures,weseethatiHMCisarobustalternativetoCP-HMCandNCP-HMC.ItsperformanceisalwayswithinafactoroftwoofthebestofCP-HMCandNCP-HMC,andsometimesbetter.InAutomaticReparameterisationofProbabilisticProgramsFigure5.Effectivesamplesizeand95%conﬁdenceintervalsfortheradonmodelacrossUSstates.Figure6.Effectivesamplesize(w/95%intervals)andtheoptimisedELBOacrossseveralmodels.additiontobeingrobust,iHMCcansometimesnavigatetheposteriormoreefﬁcientlythaneitherofCP-HMCandNCP-HMCcan:inthecaseofGermancredit,itperformsbetterthanboth(3.0±0.2ESS/∇).VIP.PerformanceofVIP-HMCistypicallyasgoodasthebetterofCP-HMCandNCP-HMC,andsometimesbetter.OntheGermancreditdataset,itachieves5.6±0.6ESS/∇,morethanthreetimestherateofCP-HMCandNCP-HMC,andsigniﬁcantlybetterthaniHMC.Figure6showsthecor-respondencebetweentheoptimisedmean-ﬁeldELBOandtheeffectivesamplingrate.WeseethatparameterizationswithhigherELBOstendtoyieldbettersamplers,whichsup-portstheELBOasareasonablepredictoroftheconditioningofamodel.WeshowsomeoftheparameterisationsthatVIPﬁndsinFigure7.VIP’sbehaviourappearsreasonable:formostdatasetswelookedat,VIPﬁndsthe“correct”globalpa-rameterisation:mostparameterisationparametersaresettoeither0or1(Figure7,left).Inthecaseswhereaglobalparameterisationisnotoptimal(e.g.radonMO,radonPAand,mostnotably,Germancredit),VIPﬁndsamixedpa-rameterisation,combiningcentred,non-centred,andpar-tiallycentredvariables(Figure7,centreandright).Theseexamplesdemonstratethesigniﬁcanceoftheeffectthatautomaticreparameterisationcanhaveonthequalityofin-ference:manuallyﬁndinganadequateparameterisationintheGermancreditcasewould,atbest,requireunreasonableamountofhandtuning,whileVIPﬁndssuchparameterisa-tionautomatically.Itisinterestingtoexaminetheshapeoftheposteriorland-scapeunderdifferentparameterisations.Figure8showstypicalmarginalsoftheGermancreditmodel.Inthecen-tredcase,thegeometryisfunnel-likebothintheprior(ingrey)andtheposterior(inred).Inthenon-centredcase,thepriorisanindependentGaussian,buttheposteriorsstillpossesssigniﬁcantcurvature.Thepartiallycentredparame-terisationschosenbyVIPappeartoyieldmorefavourableposteriorgeometry,wherethechangeincurvatureissmallerthanthatpresentintheCPandNCPcases.ApracticallessonfromourexperimentsisthatwhiletheELBOappearstocorrelatewithsamplerquality,theyarenotnecessarilyequallysensitive.Avariationalmodelthatgiveszeromasstohalfoftheposteriorisonlylog2awayfromperfectintheELBO,butthecorrespondingsamplermaybequitebad.WefoundithelpfultoestimatetheELBOwitharelativelylargenumber(tenstohundreds,weused256)ofMonteCarlosamples.Aswithmostvariationalmethods,theVIPoptimisationisnonconvexingeneral,andlocaloptimaarealsoaconcern.Weoccasionallyencounteredlocaloptimaduringdevelopment,thoughwefoundVIPtobegenerallywell-behavedonmodelsforwhichsimplerop-timisationsarewell-behaved.Inapracticalimplementation,onemightdetectoptimizationfailurebycomparingtheVIPELBOtothoseobtainedfromﬁxedparameterizations;formodest-sizedmodels,adeepPPLcanoftenrunmultiplesuchoptimizationsinparallelatminimalcost.7.DiscussionOurresultsdemonstratethatautomatedreparameterisationofprobabilisticmodelsispractical,andenablesinferencealgorithmsthatcaninsomecasesﬁndparameterisationsAutomaticReparameterisationofProbabilisticProgramsFigure7.AheatmapofVIPparameterisations.Eachsquarerep-resentstheobtainedusingVIPparameterisationparameterλas-sociatedwithadifferentlatentvariableinthemodels(s)(e.g.topleftcornerofGermancreditcorrespondstoλlogτ1).LightregionscorrespondtoCPanddarkregionstoNCP.Figure8.SelectedpriorandposteriormarginalsunderdifferentparameterisationsoftheGermancreditmodel.evenbetterthanthoseahumancouldrealisticallyexpress.Thesetechniquesallowmodellerstofocusonexpressingstatisticalassumptions,leavingcomputationtothecomputer.Weviewthemethodsinthispaperasexcitingproofsofconcept,andhopethattheywillinspireadditionalworkinthisspace.Likeallvariationalmethods,VIPassumestheposteriorcanbeapproximatedbyaparticularfunctionalform;inthiscase,independentGaussians‘pulledback’throughthenon-centringtransform.Ifthisfamilyofposteriorsdoesnotcon-tainareasonableapproximationofthetrueposterior,thenVIPwillnotbeeffectiveatwhiteningtheposteriorgeometry.Somecaseswherethismighthappenincludemodelswheredifﬁcultgeometryarisesfromheavy-tailedcomponents(forexample,x∼Cauchy(0,1);y∼Cauchy(x,1)),orwhenthetrueposteriorhasstructureddependenciesthatarenotwellcapturedbypartialcentring(forexample,manytime-series).Suchcasescanlikelybehandledbyoptimisingoveraugmentedfamiliesofreparameterisations,anddesigningsuchfamiliesisaninterestingtopicforfuturework.Whilewefocusonreparameterisinghierarchicalmodelsnat-urallywrittenincentredform,theinversetransformation—detectingandexploitingimplicithierarchicalstructureinmodelsexpressedasalgebraicequations—isanimportantareaoffuturework.ThismaybecompatiblewithrecenttrendsexploringtheuseofsymbolicalgebrasystemsinPPLruntimes(Narayananetal.,2016;Hoffmanetal.,2018).Wealsoseepromiseinautomatingreparameterisationsofheavy-tailedandmultivariatedistributions,andindesigningnewinferencealgorithmstoexploitthesecapabilities.AcknowledgementsWethanktheanonymousreviewersfortheirusefulcom-mentsandsuggestions.MariaGorinovawassupportedinpartbytheEPSRCCentreforDoctoralTraininginDataSci-ence,fundedbytheUKEngineeringandPhysicalSciencesResearchCouncil(grantEP/L016427/1)andtheUniversityofEdinburgh.REFERENCESAndrieu,C.andThoms,J.AtutorialonadaptiveMCMC.Statisticsandcomputing,18(4):343–373,2008.Betancourt,M.andGirolami,M.HamiltonianMonteCarloforhierarchicalmodels.CurrenttrendsinBayesianmethodologywithapplications,79:30,2015.Bishop,C.M.Patternrecognitionandmachinelearning.Springer,2006.Blei,D.M.Build,compute,critique,repeat:Dataanalysiswithlatentvariablemodels.AnnualReviewofStatisticsandItsApplication,1:203–232,2014.Cusumano-Towner,M.F.,Saad,F.A.,Lew,A.K.,andMansinghka,V.K.Gen:Ageneral-purposeprob-abilisticprogrammingsystemwithprogrammablein-ference.InProceedingsofthe40thACMSIGPLANConferenceonProgrammingLanguageDesignandIm-plementation,PLDI2019,pp.221–236,2019.doi:10.1145/3314221.3314642.Dua,D.andGraff,C.UCImachinelearningrepository,2017.URLhttp://archive.ics.uci.edu/ml.Gehr,T.,Misailovic,S.,andVechev,M.PSI:Exactsym-bolicinferenceforprobabilisticprograms.InInterna-tionalConferenceonComputerAidedVeriﬁcation,pp.62–83.Springer,2016.Gehr,T.,Steffen,S.,andVechev,M.λPSI:Exactinfer-enceforhigher-orderprobabilisticprograms.InProceed-ingsofthe41stACMSIGPLANConferenceonProgram-mingLanguageDesignandImplementation,pp.883–897,2020.Gelman,A.andHill,J.Dataanalysisusingregressionandmultilevel/hierarchicalmodels.Cambridgeuniversitypress,2006.AutomaticReparameterisationofProbabilisticProgramsGorinova,M.I.,Gordon,A.D.,andSutton,C.Probabilis-ticprogrammingwithdensitiesinSlicStan:Efﬁcient,ﬂexible,anddeterministic.ProceedingsoftheACMonProgrammingLanguages,3(POPL):35,2019.Hoffman,M.,Sountsov,P.,Dillon,J.V.,Langmore,I.,Tran,D.,andVasudevan,S.NeuTra-lizingbadgeometryinHamiltonianMonteCarlousingneuraltransport.arXivpreprintarXiv:1903.03704,2019.Hoffman,M.D.,Johnson,M.,andTran,D.Autoconj:Recognizingandexploitingconjugacywithoutadomain-speciﬁclanguage.InNeuralInformationProcessingSystems,2018.Kastner,G.andFrühwirth-Schnatter,S.Ancillarity-sufﬁciencyinterweavingstrategy(ASIS)forboostingMCMCestimationofstochasticvolatilitymodels.Com-putationalStatistics&DataAnalysis,76:408–423,2014.Kingma,D.P.andBa,J.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,2014.Kingma,D.P.andWelling,M.Auto-encodingvariationalBayes.arXivpreprintarXiv:1312.6114,2013.Moore,D.andGorinova,M.I.Effecthandlingforcompos-ableprogramtransformationsinEdward2.InternationalConferenceonProbabilisticProgramming,2018.URLhttps://arxiv.org/abs/1811.06150.Narayanan,P.,Carette,J.,Romano,W.,Shan,C.,andZinkov,R.Probabilisticinferencebyprogramtransfor-mationinHakaru(systemdescription).InInternationalSymposiumonFunctionalandLogicProgramming-13thInternationalSymposium,FLOPS2016,Kochi,Japan,March4-6,2016,Proceedings,pp.62–79.Springer,2016.doi:10.1007/978-3-319-29604-3_5.URLhttp://dx.doi.org/10.1007/978-3-319-29604-3_5.Neal,R.M.Slicesampling.TheAnnalsofStatistics,31(3):705–741,2003.ISSN00905364.URLhttp://www.jstor.org/stable/3448413.Papaspiliopoulos,O.,Roberts,G.O.,andSköld,M.Ageneralframeworkfortheparametrizationofhierarchicalmodels.StatisticalScience,pp.59–73,2007.Parno,M.D.andMarzouk,Y.M.Transportmapaccel-eratedMarkovchainMonteCarlo.SIAM/ASAJournalonUncertaintyQuantiﬁcation,6(2):645–682,2018.doi:10.1137/17M1134640.URLhttps://doi.org/10.1137/17M1134640.Plotkin,G.andPower,J.Adequacyforalgebraiceffects.InHonsell,F.andMiculan,M.(eds.),FoundationsofSoftwareScienceandComputationStructures,pp.1–24,Berlin,Heidelberg,2001.SpringerBerlinHeidelberg.ISBN978-3-540-45315-4.Plotkin,G.andPretnar,M.Handlersofalgebraiceffects.InCastagna,G.(ed.),ProgrammingLanguagesandSystems,pp.80–94,Berlin,Heidelberg,2009.SpringerBerlinHeidelberg.ISBN978-3-642-00590-9.Pretnar,M.Anintroductiontoalgebraiceffectsandhandlers.Invitedtutorialpaper.ElectronicNotesinTheoreticalComputerScience,319:19–35,2015.ISSN1571-0661.The31stConferenceontheMathematicalFoundationsofProgrammingSemantics(MFPSXXXI).Rubin,D.B.Estimationinparallelrandomizedexperiments.JournalofEducationalStatistics,6(4):377–401,1981.ISSN03629791.URLhttp://www.jstor.org/stable/1164617.StanDevelopmentTeametal.Stanmodellinglan-guageusersguideandreferencemanual.Technicalre-port,2016.https://mc-stan.org/docs/2_19/stan-users-guide/.Tran,D.,Hoffman,M.D.,Vasudevan,S.,Suter,C.,Moore,D.,Radul,A.,Johnson,M.,andSaurous,R.A.Sim-ple,distributed,andacceleratedprobabilisticprogram-ming.2018.URLhttps://arxiv.org/abs/1811.02091.AdvancesinNeuralInformationPro-cessingSystems.UberAILabs.Pyro:Adeepprobabilisticprogramminglanguage,2017.http://pyro.ai/.Yao,Y.,Vehtari,A.,Simpson,D.,andGelman,A.Yes,butdiditwork?:Evaluatingvariationalinference.arXivpreprintarXiv:1802.02538,2018.Yu,Y.andMeng,X.-L.Tocenterornottocenter:Thatisnotthequestion—anancillarity–sufﬁciencyinterweavingstrategy(ASIS)forboostingMCMCefﬁciency.JournalofComputationalandGraphicalStatistics,20(3):531–570,2011.Zinkov,R.andShan,C.Composinginferencealgorithmsasprogramtransformations.InElidan,G.,Kersting,K.,andIhler,A.T.(eds.),ProceedingsoftheThirty-ThirdConferenceonUncertaintyinArtiﬁcialIntelligence,UAI2017,Sydney,Australia,August11-15,2017.AUAIPress,2017.AutomaticReparameterisationofProbabilisticProgramsA.DerivationoftheConditionNumberofthePosteriorforaSimpleModelCentredParameterisationθ∼N(0,1)µ∼N(θ,σµ)yn∼N(µ,σ)foralln∈1...NNon-centredParameterisationθ∼N(0,1)˜µ∼N(0,1)yn∼N(θ+σµ˜µ,σ)foralln∈1...NAstheGaussiandistributionisself-conjugate,theposteriordistribution(givenx)ineachcase(centredornon-centred)isalsoaGaussiandistribution,whoseshapeisentirelyspec-iﬁedbyacovariancematrixV.Toquantifythequalityofeachparameterisation,weinvestigatetheconditionnumberκoftheposteriorcovariancematrixineachcaseunderthebestdiagonalpreconditioner.Wedothisinthreesteps:1.WederivethecovariancematricesVCPandVNCP,suchthatp(µ,θ|y)=N(µ,θ|mCP,VCP)andp(˜µ,θ|y)=N(˜µ,θ|mNCP,VNCP)(Equation1andEqua-tion2).2.WeﬁndthebestdiagonalpreconditionersD∗CPandD∗NCP:forP=CP,NCP,thatisD∗P=argminD(λ(2)P/λ(1)P),whereλ(1)Pandλ(2)ParetheeigenvaluesofU=DTVPD(Equation3andEqua-tion4).3.Wecomparetheconditionnumbersκcp(q)=λ(2)cp/λ(1)cpandκncp(q)=λ(2)ncp/λ(1)ncp,whereλ(i)(n)cparetheeigenvaluesofU∗=(D∗)TVD∗A.1.DerivingVCPandVNCP:CentredParameterisationp(µ,θ|y)∝p(µ,θ,y)∝N(µ|θ,σµ)N(θ|0,1)NYn=1N(yn|µ,σ)∝exp −12 (µ−θ)2σ2µ+θ2+NXn=1(yn−µ)2σ2!!∝exp(cid:18)−12(cid:18)µ2(cid:18)1σ2µ+Nσ2(cid:19)+θ2(cid:18)1σ2µ+1(cid:19)−2µθ(cid:18)1σ2µ(cid:19)+µ −2σ2NXn=1yn!!!Atthesametime,forA=V−1NCP,wehave:N(µ,θ|mCP,VCP)∝exp −12(cid:18)(cid:18)µθ(cid:19)−m(cid:19)TA(cid:18)(cid:18)µθ(cid:19)−m(cid:19)!∝exp(cid:18)−12(cid:0)µ2A11+θ2A22+µθ(2A12)+µ(−2A11m1−2A12m2)+µ2A11θ(−2A22m2−2A12m1)(cid:1)(cid:19)Thus,forq=N/σ2,weget:A= 1σ2µ+q−1σ2µ−1σ2µ1σ2µ+1!Andtherefore:VCP=1σ2µq+q+1(cid:18)1+σ2µ11qσ2µ+1(cid:19)(1)A.2.DerivingVCPandVNCP:Non-centredParameterisationLikeintheprevioussubsection,wehave:p((cid:15),θ|y)∝p((cid:15),θ,y)∝N((cid:15)|0,1)N(θ|0,1)NYn=1N(yn|σµ(cid:15)+θ,σ)∝exp −12 ((cid:15)2+θ2+NXn=1(yn−σµ(cid:15)−θ)2σ2!!∝exp −12 (cid:15)2 1+Nσ2µσ2!+θ2(cid:18)1+Nσ2(cid:19)+(cid:15)θ(cid:18)2Nσµσ2(cid:19)+(cid:15)(cid:18)−2σµPynσ2(cid:19)+θ(cid:18)−2Pynσ2(cid:19)(cid:19)(cid:19)Similarlytobefore,wederiveA=(cid:18)σ2µq+1σµqσµqq+1(cid:19),andtherefore:VNCP=1σ2µq+q+1(cid:18)q+1−σµq−σµqσ2µq+1(cid:19)(2)AutomaticReparameterisationofProbabilisticProgramsA.3.ThebestdiagonalpreconditionerConsideradiagonalpreconditionerD=(cid:18)d001(cid:19).ThebestdiagonalpreconditionerD∗ofVissuchthat:D∗=argminD(λ2/λ1)whereλ1,λ2aretheeigenvaluesofU=DTVDFirstly,intermsofthecovariancematrixinthecentredcase,wehave:U=DTVCPD=(cid:18)d001(cid:19)(cid:18)1σ2µq+q+1(cid:18)1+σ2µ11qσ2µ+1(cid:19)(cid:19)(cid:18)d001(cid:19)=1σ2µq+q+1(cid:18)(1+σ2µ)d2ddqσ2µ+1(cid:19)Thesolutionsofdet(U−λI)=0arethesolutionsof:((1+σ2µ)d2−λ(σ2µq+q+1))(qσ2µ+1−λ(σ2µq+q+1))=d2which,aftersimpliﬁcation,becomes:(σ2µq+q+1)λ2−(σ2µq+1+d2(σ2µ+1))λ+d2σ2µ=0Wewanttoﬁnddthatminimisesλ2/λ1.Letu=d2.Wearelookingforu,suchthat∂∂uλ2λ1=0,inordertoﬁndd∗CP=argmind(λ2/λ1).Byexpandingandsimplifyingweget:2∂∂u(σ2µq+1+u(σ2µ+1))=(σ2µq+1+u(σ2µ+1))/uAndthus:d∗CP=√u=sσ2µq+1σ2µ+1(3)WeobtainthebestdiagonalpreconditionerD∗NCP=(cid:18)d∗NCP001(cid:19)inasimilarmanner,ﬁnallygetting:d∗NCP=√u=sσ2µq+1q+1(4)A.4.TheConditionNumbersκCPandκNCPFinally,wesubstituted∗CPandd∗NCPintherespectiveeigen-valueequationstoderivetheconditionnumberineachcase:κCP=λ(CP)2/λ(CP)1=σ2µq+1+√(σ2µq+1)2−σ2µ(σ2µq+q+1)(σ2µq+1)/(v+1)σ2µq+1−√(σ2µq+1)2−σ2µ(σ2µq+q+1)(σ2µq+1)/(v+1)(5)κNCP=λ(NCP)2/λ(NCP)1=σ2µq+1+√(σ2µq+1)2−σ2µ(σ2µq+q+1)(σ2µq+1)/(q+1)σ2µq+1−√(σ2µq+1)2−σ2µ(σ2µq+q+1)(σ2µq+1)/(q+1)(6)B.InterceptorsInterceptorscanbeusedasapowerfulabstractionsinaprobabilisticprogrammingsystems,asdiscussedpreviouslybyMoore&Gorinova(2018),andshownbybothPyroandEdward2.Inparticular,wecanuseinterceptorstoautomaticallyreparameteriseamodel,aswellastospecifyvariationalfamilies.Inthissection,weshowEdward2pseudo-codefortheinterceptorsusedtoimplementiHMCandVIP-HMC.B.1.MakelogjointThefollowingcodeisanoutlineofEdward2’simpllementa-tionofafunctionthatevaluatesthelogdensitylogp(x)atsomegivenx:defmake_log_joint_fn(model):deflog_joint_fn(**kwargs):log_prob=0deflog_prob_interceptor(rv_constructor,**rv_kwargs):#Overridesarandomvariable’svalue#andaccumulatesitslogprob.rv_name=rv_kwargs.get("name")rv_kwargs["value"]=kwargs.get(rv_name)rv=rv_constructor(**rv_kwargs)log_prob=log_prob+\rv.distribution.log_prob(rv.value)returnrvwithed.interception(log_prob_interceptor):model()returnlog_probreturnlog_joint_fnByexecutingthemodelfunctioninthecontextoflprob_interceptor,weoverrideeachsamplestatement(acalltoarandomvariableconstructorrv_constructor),togenerateavariablethattakesonthevalueprovidedintheargumentsoflog_joint_fn.Asasideeffect,wealsoaccumulatetheresultofevaluatingeachvariable’spriorden-sityattheprovidedvalue,which,bythechainrule,givesusthelogjointdensity.B.2.Non-centredParameterisationInterceptorByinterceptingeveryconstructionofanormalvariable(or,moregenerally,oflocation-scalefamilyvariables),wecanAutomaticReparameterisationofProbabilisticProgramscreateastandardnormalvariableinstead,andscaleandshiftappropriately.defncp_interceptor(rv_constr,**rv_kwargs):#Assumesrv_constrisinthe#location-scalefamilyname=rv_kwargs["name"]+"_std"rv_std=\ed.interceptable6(rv_constr)(loc=0,scale=1)returnrv_kwargs["loc"]+\rv_kwargs["scale"]*rv_stdRunningamodelthatdeclarestherandomvariablesθinthecontextofncp_interceptorwilldeclareanewsetofstandardnormalrandomvariablesθ(std).Nestingthisinthecontextofthelog_prob_interceptorfrom??willthenevaluatethelogjointdensitylogp(θ(std)).Forexample,goingbacktoNeal’sfunnel,runningwithed.interception(log_prob_interceptor):neals_funnel()correspondstoevaluatinglogp(z,x)=logN(z|0,3)+logN(x|0,ez/2),whilerunningwithed.interception(log_prob_interceptor):withed.interception(ncp_interceptor):neals_funnel()correspondstoevaluatinglogp(z(std),x(std))=logN(z(std)|0,1)+logN(x(std)|0,1).B.3.VIPInterceptorTheVIPinterceptorissimilartotheNCPinterceptor.ThenotabledifferenceisthatitcreatesnewlearnableTensor-ﬂowvariables,whichcorrespondtotheparameterisationparametersλ:defvip_interceptor(rv_constr,**rv_kwargs):name=rv_kwargs["name"]+"_vip"rv_loc=rv_kwargs["loc"]rv_scale=rv_kwargs["scale"]a=tf.nn.sigmoid(tf.get_variable(name+"_a_unconstrained",initializer=tf.zeros_like(rv_loc))rv_vip=ed.interceptable(rv_constr)(loc=a*rv_loc,scale=rv_scale**a)returnrv_loc+\rv_scale**(1-a)*(rv_vip-a*rv_loc)6Wrappingtheconstructorinwithed.interceptableen-suresthatwecannestthisinterceptorinthecontextofotherinterceptors.B.4.Mean-ﬁeldVariationalModelInterceptorFinally,weshowamean-ﬁeldvariationalfamiliyintercep-tor,whichweusebothtotunethestepsizesforHMC(seeAppendixC),andtomakeuseofVIPautomatically.Themfvi_interceptorsimplysubstituteseachsamplestatementwithsamplingfromanormaldistributionwithparametersspeciﬁedbysomefreshvariationalparametersµandσ:defvip_interceptor(rv_constructor,**rv_kwargs):name=rv_kwargs["name"]+"_q"mu=tf.get_variable(name+"_mu")sigma=tf.nn.softmax(tf.get_variable(name+"_sigma"))rv_q=ed.interceptable(ed.Normal)(loc=mu,scale=sigma,name=name)returnrv_qC.DetailsoftheExperimentsAlgorithms.•CP-HMC:HMCrunonafullycentredmodel.•NCP-HMC:HMCrunonafullynon-centredmodel.•iHMC:interleavedHMC.•VIP-HMC:HMCrunontheamodelreparameterisedasgivenbyVIP.EachrunconsistsofVIpre-processingandHMCinference.VariationalInferencePre-processing.Weuseauto-maticdifferentiationtocomputestochasticgradientsoftheELBOwithrespecttoλ,θandperformtheoptimi-sationusingAdam(Kingma&Ba,2014).Weimplementtheconstraintλi∈[0,1]usingasigmoidtransformation;λi=1/(cid:16)1+exp(−˜λi)(cid:17)for˜λi∈R.PriortorunningHMC,wealsorunVItoapproximateper-variableinitialstepsizes(equivalently,adiagonalprecon-ditioningmatrix),andtoinitialisethechains.ForeachofCP-HMCandNCP-HMCthisisjustmean-ﬁeldVI,andforVIP-HMCtheVIprocedureisVIP.EachVImethodisrunfor3000optimisationsteps,andtheELBOisapproximatedusing256MonteCarlosamples.WeusetheAdamoptimiserwithinitiallearningrateα∈[0.02,0.05,0.1,0.2,0.4],decayedtoα/5after1000stepsandα/20after2000steps,andreturnedtheresultwiththehighestELBO.AutomaticReparameterisationofProbabilisticProgramsHamiltonianMonteCarloInference.Ineachcasewerun200chainsforawarm-upperiodof2000steps,fol-lowedby10000stepseach,andreporttheaverageeffectivesamplesize(ESS)per1000gradientevaluations(ESS/∇).SinceESSisnaturallyestimatedfromscalartraces,weﬁrstestimateper-variableeffectivesamplesizesforeachmodelvariable,andtaketheoverallESStobetheminimumacrossallvariables.TheHMCstepsizestwasadaptedtotargetanacceptanceprobabilityof0.75,followingasimpleupdaterulelogst+1=logst+0.02·(I[αt−0.75]−I[0.75−αt])whereαtistheacceptanceprobabilityoftheproposedstateatstept(Andrieu&Thoms,2008).Theadaptationrunsdur-ingtheﬁrst1500stepsofthewarm-upperiod,afterwhichweallowthechaintomixtowardsastationarydistribution.Thenumberofleapfrogstepsischosenusing‘oracle’tuning:eachsamplerisrunwithlogarithmicallyincreasingnumberofleapfrogstepsin{1,2,4,...,128},andwereporttheresultthatmaximisesESS/∇.Thisisintendedtodecoupletheproblemoftuningthenumberofleapfrogstepsfromtheissuesofparameterisationconsiderinthispaper,andensurethateachmethodisreasonablytuned.ForiHMC,wetuneasinglenumberofleapfrogstepsthatissharedacrossboththeCPandNCPsubsteps.D.AdditionalAnalysisInadditiontotheestimatedeffectivesamplesizes,wedi-rectlyexaminedposteriormomentsestimatedfromeachmethod.TheoryimpliesthattheestimatedposteriormeanandstandarddeviationshouldconvergetotheirtruevaluesatarateofO(√N),whereNisthenumberofeffectivesamples,sowewouldexpecttheseresultstobebroadlyconsistentwiththeestimatedeffectivesamplessizesin§6.Foreachmodel,wecomputeda‘goldstandard’estimatedposteriormeanandstandarddeviation.Weﬁrstusedeachmethodtoestimateempiricalmeansandstandarddeviationsforthelatentvariables,usingthefullsetof200×10000samplesproducedacrossallchains.Wethentooktheme-dianofeachstatisticacrossthefourmethods,i.e.,themeanofthetwocentralvalues,forourﬁnalestimate.Thisisrobusttothecasewhereoneofthefourmethodstotallyfailstomix—generallybecauseafullycentredorfullynoncentredparameterisationisnotappropriate—aslongastheothermethodsproducereasonablesamples.Byinspec-tion,atleastthreeofthefourmethodsagreedcloselyinallcases,whichprovidessomecomfortthatourgoldstandardestimatesarereasonable.Table1showsnormalizedexpectederrorintheposteriorstatisticsestimatedbyasinglechainofeachmethod,asafunctionofthenumberofgradientstepstaken.Foreachlatentvariablezi,wecomputetheexpectedabsoluteerrorinthemean,usingtherunningmeanˆµ(:t)i,kestimatedfromtheﬁrsttgradientstepsofthekthchain,andthegoldstandardmeanµicomputedasabove,tober(:t)i=1KXk(cid:12)(cid:12)(cid:12)ˆµ(:t)i,k−µi,(cid:12)(cid:12)(cid:12)whereKisthenumberofchains.Weanalogouslycomputeexpectedabsoluteerrorinstandarddeviationbys(:t)i=1KXk(cid:12)(cid:12)(cid:12)ˆσ(:t)i,k−σi(cid:12)(cid:12)(cid:12).Eachmodelhasmanylatentvariables,whoseposteriorshavedifferentscales.Tosummariseinferenceacrossallvariablesinamodel,wereportthemeanoftheerrorsnormalizedbythestandarddeviation(whichwetreatasareasonablerepresentativeoftheposteriorscaleforeachvariable)acrossallNlatentvariables,¯r(:t)=1NXir(:t)i/σi¯s(:t)=1NXis(:t)i/σi.ThesequantitiesareplottedinTable1.Notethatthexaxisisthenumberofgradientstepstaken;thismaycorrespondtodifferentnumbersofactualsamplesfromeachmethod,dependingonthenumberofleapfrogstepsused.Asdis-cussedin§6,thenumberofgradientstepsistheappropriatemetrichere,asit’sareasonableproxyforwallclocktime.Therelativeperformanceofthemethodsgenerallycorre-spondstoandsupportstheeffectivesamplesizeestimatesreportedin§6.VIPconvergesnotablyfasterthanothermethodsintheGermancreditmodel,andotherwiseaboutasquicklyasthebetterofCPandNCP,eachofwhichsometimesfailsquitebadlyonitsown.InterleavedHMCisgenerallybetweenCPandNCP,withtheexceptionoftheestimatedstandarddeviationsontheelectriccompanydataset,whereitnotablybeatsoutalloftheothermethods.Wealsoprovideautocorrelationsforeachmethod,plottingautocorrelationsforeachlatentvariableacrossthreechainsalongwiththemeanautocorrelation(plottedinbold).Givenanautocorrelationsequencer1,...,rN−1onNsamples,theeffectivesamplesizeisdeﬁnedasESS(N)=N1+2PN−1i=1N−iNri;thesearethevaluesreportedin§6.AutomaticReparameterisationofProbabilisticProgramsErrorinmean(¯r(:t))Errorinstddev(¯s(:t))AutocorrelationsRadon,MAEightschoolsElectionElectricGermancreditTable1.Additionalinferencediagnostics.6.3. Discussion

179

6.3 Discussion

6.3.1 Impact

The work has been well received within other eﬀect-handling based PPLs: both Pyro

(Bingham et al., 2019) and NumPyro (Phan et al., 2019) have implemented the variationally
inferred parameterisation (VIP) handler to allow for automatic reparameterisation.5, 6

6.3.2 Scope and extensions

This chapter focused on one particular family of reparameterisations (the family of partial

non-centred reparameterisations), applicable to one particular family of distributions (the

location-scale family). However, it is possible to use eﬀect handlers to perform model

reparameterisation more generally. It is also possible to extend and adapt VIP to be

applicable to other families of distributions and reparameterisations.

Multivariate distributions. VIP can be used for a broad range of distributions: any

univariate location-scale distribution (such as Gaussian, Logistic and Cauchy) are directly

covered by the work of this paper. Adapting VIP to multivariate location-scale distributions

is also possible. One way to do so for an N -dimensional variable z

∼ N
interpolate the eigenvalues of the covariance matrix Σ.
If the eigendecomposition of
Σ is Σ = Q diag(λ) Q−1 (which also equals Q diag(λ) QT , as Σ is positive deﬁnite and
symmetric, thus Q is orthogonal), VIP can be implemented by interpolating ˜λ between
for
1 and λ through the parameterisation parameters φ

[0, 1]N : we set ˜λi = λφi

(µ, Σ) is to

i

∈

i = 1, . . . , N . The reparameterisation is then:

˜z

∼ N

( ˜µ, ˜Σ)

z = µ + Q diag(

λ

(cid:112)

˜λ) QT (˜z

˜µ)

−

(cid:11)

(cid:12)

µ, ˜Σ = Q diag(˜λ) QT ,

where ˜µ = ˜λ
and

(cid:12)
denotes elementwise division ((a

b)i = aibi),
b)i = ai/bi). As before, this results in interpolating
between non-centred and centred parameterisation: when φ = 0, we have ˜µ = 0 and
˜Σ = Q I QT = I; when φ = 1, we have ˜µ = µ and ˜Σ = Q diag(λ) QT = Σ.

denotes elementwise multiplication ((a

(cid:12)

(cid:11)

(cid:11)

Other reparameterisations. Eﬀect handlers can be used to implement reparameteri-

sations more generally. In particular, if sampling a variable z from a distribution d1 is

equivalent to sampling ˜z from a distribution d2 and obtaining z from ˜z via a transformation

f , then we can deﬁne the following handler to reparameterise out models:

5 https://docs.pyro.ai/en/latest/infer.reparam.html#module-pyro.infer.reparam.loc_scale
6 http://num.pyro.ai/en/latest/reparam.html#numpyro.infer.reparam.LocScaleReparam

180

Chapter 6. Automatic reparameterisation

let reparam = handler {

sample(rv; d1. k)

→
sample(z tilde; d2);

let z = f(z tilde);

k(z)

}

For example, for a variable z coming from any distribution d and provided we know F −1

d —
the inverse cumulative distribution function (inverse CDF) of d — we can reparameterise

z using an inverse CDF transform:

let icdf reparam = handler {

sample(rv; d. k)

→

sample(z tilde; uniform(0, 1));
let z = F −1
d (z tilde);
k(z)

}

However, in order to adapt VIP to any new family of reparameterisations, we need to

design a continuous relaxation between the reparameterised/non-reparameterised model,

the way VIP does so for non-centred/centred parameterisation. In other words, VIP is not

immediately applicable to other reparameterisations, which is one of its biggest limitations.

Challenges ahead

CHAPTER 7

Many challenges lie ahead of making Bayesian inference accessible. This thesis showed

that probabilistic programming languages can utilise the underlying structure of a model

to optimise inference. However, a lot of work remains before we can tailor a model-speciﬁc

algorithm to a problem automatically.

Previous chapters already highlighted some of the future directions that remain unexplored.

Chapter 5 emphasised the need for a general framework for static analysis of probabilistic

programs, which can slice a program according to a particular factorisation of interest,

and perhaps facilitate programmable inference. Chapter 6 demonstrated how to automate

one speciﬁc model reparameterisation, but leaves the open questions of how to generalise

to other reparameterisation families.

Some other challenges of probabilistic programming were not mentioned here, but could be

addressed by program analysis in the future. Perhaps most signiﬁcantly, this dissertation

did not consider problems where the number of model parameters is unbounded, nor

inference algorithms that can be applied to such problems. Similarly, we assumed that the

likelihood of the model is available in a closed form. Future work may look into factorising

a model, so that challenging sub-parts of it, for example those containing an unbounded

number of variables, or deterministic observations, are treated separately to automatically

synthesise an eﬃcient model-speciﬁc strategy.

181

Bibliography

Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.

Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,

Geoﬀrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh

Levenberg, Dandelion Man´e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,

Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay

Vasudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,

and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.

https://www.tensorflow.org/

Mart´ın Abadi, Anindya Banerjee, Nevin Heintze, and Jon G. Riecke. 1999. A Core Calculus of Dependency.

In POPL. ACM, 147–160.

Franklin Abodo, Andrew Berthaume, Stephen Zitzow-Childs, and Leonardo Bobadilla. 2019. Strengthening

the Case for a Bayesian Approach to Car-following Model Calibration and Validation using Probabilistic

Programming. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE, 4360–4367.

Christophe Andrieu, Gareth O Roberts, et al. 2009. The pseudo-marginal approach for eﬃcient Monte

Carlo computations. The Annals of Statistics 37, 2 (2009), 697–725.

Andrew W. Appel. 1992. Compiling with Continuations. Cambridge University Press.

Stefan Arnborg, Derek G Corneil, and Andrzej Proskurowski. 1987. Complexity of ﬁnding embeddings in

a k-tree. SIAM Journal on Algebraic Discrete Methods 8, 2 (1987), 277–284.

David Barber. 2012. Bayesian reasoning and machine learning. Cambridge University Press.

Guillaume Baudart, Javier Burroni, Martin Hirzel, Louis Mandel, and Avraham Shinnar. 2021. Compiling

Stan to generative probabilistic languages and extension to deep probabilistic programming. Proceed-

ings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and

Implementation (PLDI) (2021).

Guillaume Baudart, Martin Hirzel, Kiran Kate, Louis Mandel, and Avraham Shinnar. 2019. Machine

Learning in Python with No Strings Attached. In Proceedings of the 3rd ACM SIGPLAN International

Workshop on Machine Learning and Programming Languages (MAPL 2019). Association for Computing

Machinery, 1–9. https://doi.org/10.1145/3315508.3329972

Atılım G¨une¸s Baydin, Lukas Heinrich, Wahid Bhimji, Lei Shao, Saeid Naderiparizi, Andreas Munk, Jialin

Liu, Bradley Gram-Hansen, Gilles Louppe, Lawrence Meadows, Philip Torr, Victor Lee, Prabhat, Kyle

182

Bibliography

183

Cranmer, and Frank Wood. 2019. Eﬃcient Probabilistic Inference in the Quest for Physics Beyond the

Standard Model. In Advances in Neural Information Processing Systems (NeurIPS’19).

Mark A. Beaumont. 2010. Approximate Bayesian Computation in Evolution and Ecology. Annual

Review of Ecology, Evolution, and Systematics 41, 1 (2010), 379–406. https://doi.org/10.1146/

annurev-ecolsys-102209-144621 arXiv:https://doi.org/10.1146/annurev-ecolsys-102209-144621

Richard E Bellman. 1961. Adaptive Control Processes: A Guided Tour. Princeton university press.

Ryan Bernstein. 2019. Static Analysis for Probabilistic Programs. arXiv preprint arXiv:1909.05076

(2019).

Michael Betancourt. 2017. A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint

arXiv:1701.02434 (2017).

Dariusz Biernacki, Maciej Pir´og, Piotr Polesiuk, and Filip Sieczkowski. 2019. Binders by day, labels

by night: eﬀect instances via lexically scoped handlers. Proceedings of the ACM on Programming

Languages 4, POPL (2019), 1–29.

Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Kar-

aletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D Goodman. 2019. Pyro: Deep universal

probabilistic programming. The Journal of Machine Learning Research 20, 1 (2019), 973–978.

Christopher M Bishop. 2006. Pattern recognition and machine learning. Springer, New York, NY.

David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. 2017. Variational inference: A review for statisticians.

Journal of the American statistical Association 112, 518 (2017), 859–877.

Johannes Borgstr¨om, Andrew D Gordon, Michael Greenberg, James Margetson, and Jurgen Van Gael.

2011. Measure transformer semantics for Bayesian machine learning. In European Symposium on

Programming. Springer, 77–96.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,

George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX:

Composable transformations of Python+NumPy programs. http://github.com/google/jax

Stephen P Brooks and Andrew Gelman. 1998. General methods for monitoring convergence of iterative

simulations. Journal of computational and graphical statistics 7, 4 (1998), 434–455.

Bob Carpenter, Andrew Gelman, Matthew Hoﬀman, Daniel Lee, Ben Goodrich, Michael Betancourt,

Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. Stan: A Probabilistic Programming

Language. Journal of Statistical Software, Articles 76, 1 (2017), 1–32. https://doi.org/10.18637/

jss.v076.i01

Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards Conversational

Recommender Systems. In 22nd ACM SigKDD Conference on Knowledge Discovery and Data Min-

ing. ACM - Association for Computing Machinery. https://www.microsoft.com/en-us/research/

publication/towards-conversational-recommender-systems/

Kyle Cranmer, Johann Brehmer, and Gilles Louppe. 2020. The frontier of simulation-based inference.

184

Bibliography

Proceedings of the National Academy of Sciences 117, 48 (2020), 30055–30062. https://doi.org/10.

1073/pnas.1912789117 arXiv:https://www.pnas.org/content/117/48/30055.full.pdf

Marco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and Vikash K. Mansinghka. 2019. Gen:

A General-Purpose Probabilistic Programming System with Programmable Inference. (2019), 221–236.

https://doi.org/10.1145/3314221.3314642

Peter J. Diggle and Richard J. Gratton. 1984. Monte Carlo Methods of Inference for Implicit Statistical

Models. Journal of the Royal Statistical Society. Series B (Methodological) 46, 2 (1984), 193–227.

http://www.jstor.org/stable/2345504

Stephen Dolan, Spiros Eliopoulos, Daniel Hillerstr¨om, Anil Madhavapeddy, KC Sivaramakrishnan, and

Leo White. 2017. Concurrent system programming with eﬀect handlers. In International Symposium

on Trends in Functional Programming. Springer, 98–117.

Arnaud Doucet, Nando De Freitas, and Neil Gordon. 2001. An introduction to sequential Monte Carlo

methods. In Sequential Monte Carlo methods in practice. Springer, 3–14.

Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. 1987. Hybrid Monte Carlo.

Physics letters B 195, 2 (1987), 216–222.

Conor Durkan, George Papamakarios, and Iain Murray. 2018. Sequential neural methods for likelihood-free

inference. arXiv preprint arXiv:1811.08723 (2018).

Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Thomas A Mellan, Helen Coupland,

Charles Whittaker, Harrison Zhu, Tresnia Berah, Jeﬀrey W Eaton, et al. 2020. Estimating the eﬀects

of non-pharmaceutical interventions on COVID-19 in Europe. Nature 584, 7820 (2020), 257–261.

Brendan J. Frey. 2002. Extending Factor Graphs so as to Unify Directed and Undirected Graphical

Models. In Proceedings of the Nineteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI’03).

257–264.

Brendan J Frey and David JC MacKay. 1998. A revolution: Belief propagation in graphs with cycles.

Advances in Neural Information Processing Systems (1998), 479–485.

Hong Ge, Kai Xu, and Zoubin Ghahramani. 2018. Turing: A language for ﬂexible probabilistic inference.

In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS). 1682–1690. http:

//proceedings.mlr.press/v84/ge18b.html

Timon Gehr, Sasa Misailovic, and Martin Vechev. 2016. PSI: Exact symbolic inference for probabilistic

programs. In International Conference on Computer Aided Veriﬁcation. Springer, 62–83.

Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao,

Lauren Kennedy, Jonah Gabry, Paul-Christian B¨urkner, and Martin Modr´ak. 2020. Bayesian Workﬂow.

arXiv:2011.01808 [stat.ME]

Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian

restoration of images. IEEE Transactions on pattern analysis and machine intelligence 6 (1984),

721–741.

Bibliography

185

Alan George and Joseph WH Liu. 1989. The evolution of the minimum degree ordering algorithm. Siam

review 31, 1 (1989), 1–19.

Wally R Gilks, Andrew Thomas, and David J Spiegelhalter. 1994. A language and program for complex

Bayesian modelling. Journal of the Royal Statistical Society: Series D (The Statistician) 43, 1 (1994),

169–177.

Joseph A Goguen and Jos´e Meseguer. 1982. Security policies and security models. In 1982 IEEE Symposium

on Security and Privacy. IEEE, 11–11.

Oliver Goldstein. 2019. Modular probabilistic programming with algebraic eﬀects. Master’s thesis. University

of Edinburgh.

Pedro J Gon¸calves, Jan-Matthis Lueckmann, Michael Deistler, Marcel Nonnenmacher, Kaan ¨Ocal, Giacomo
Bassetto, Chaitanya Chintaluri, William F Podlaski, Sara A Haddad, Tim P Vogels, David S Greenberg,

and Jakob H Macke. 2020. Training deep neural density estimators to identify mechanistic models of

neural dynamics. eLife 9 (sep 2020), e56261. https://doi.org/10.7554/eLife.56261

Noah D. Goodman, Vikash K. Mansinghka, Daniel Roy, Keith Bonawitz, and Joshua B. Tenenbaum.

2008. Church: A Language for Generative Models. In Proceedings of the Twenty-Fourth Conference on

Uncertainty in Artiﬁcial Intelligence (Helsinki, Finland) (UAI’08). AUAI Press, 220–229.

Noah D Goodman and Andreas Stuhlm¨uller. 2014. The Design and Implementation of Probabilistic

Programming Languages. http://dippl.org. Accessed: 2021-4-8.

Andrew D Gordon. 1995. A tutorial on co-induction and functional programming. Functional Programming,

Glasgow 1994 (1995), 78–95.

Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sriram K. Rajamani. 2014. Probabilistic

programming. In FOSE. ACM, 167–181.

Andrew D. Gordon, Claudio V. Russo, Marcin Szymczak, Johannes Borgstr¨om, Nicolas Rolland, Thore

Graepel, and Daniel Tarlow. 2015. Probabilistic Programs as Spreadsheet Queries. In ESOP (Lecture

Notes in Computer Science, Vol. 9032). Springer, 1–25.

Neil J Gordon, David J Salmond, and Adrian FM Smith. 1993. Novel approach to nonlinear/non-Gaussian

Bayesian state estimation. In IEE proceedings F (radar and signal processing), Vol. 140. IET, 107–113.

Maria I Gorinova. 2017. Probabilistic Programming with SlicStan. Master’s thesis. University of Edinburgh.

L´eo Grinsztajn, Elizaveta Semenova, Charles C Margossian, and Julien Riou. 2020. Bayesian workﬂow for

disease transmission modeling in Stan. arXiv preprint arXiv:2006.02985 (2020).

W Keith Hastings. 1970. Monte Carlo sampling methods using Markov chains and their applications.

Biometrika 57, 1 (1970), 97–109.

Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkill: A Bayesian skill rating system. In

Proceedings of the 19th international conference on neural information processing systems. 569–576.

Chris Heunen, Ohad Kammar, Sam Staton, and Hongseok Yang. 2017. A convenient category for higher-

order probability theory. In 2017 32nd Annual ACM/IEEE Symposium on Logic in Computer Science

(LICS). IEEE, 1–12.

186

Bibliography

Daniel Hillerstr¨om. 2021. Foundations for Programming and Implementing Eﬀect Handlers. Ph.D.

Dissertation. The University of Edinburgh, Scotland, UK. Pending examination.

Daniel Hillerstr¨om, Sam Lindley, and John Longley. 2020. Eﬀects for eﬃciency: Asymptotic speedup with

ﬁrst-class control. Proceedings of the ACM on Programming Languages 4, ICFP (Aug 2020), 1–29.

https://doi.org/10.1145/3408982

Daniel Hillerstr¨om. 2016. Compilation of eﬀect handlers and their applications in concurrency. MSc (R)

thesis, The University of Edinburgh, Scotland (2016).

Matthew D. Hoﬀman, David M. Blei, Chong Wang, and John Paisley. 2013. Stochastic Variational

Inference. Journal of Machine Learning Research 14, 4 (2013), 1303–1347.

Matthew D Hoﬀman and Andrew Gelman. 2014. The No-U-turn sampler: Adaptively setting path lengths

in Hamiltonian Monte Carlo. Journal of Machine Learning Research 15, 1 (2014), 1593–1623.

Steven Holtzen, Guy Van den Broeck, and Todd Millstein. 2020. Dice: Compiling Discrete Probabilistic

Programs for Scalable Inference. arXiv preprint arXiv:2005.09089 (2020).

Zixin Huang, Saikat Dutta, and Sasa Misailovic. 2021. AQUA: Automated Quantized Inference for

Probabilistic Programs.

Chung-Kil Hur, Aditya V Nori, Sriram K Rajamani, and Selva Samuel. 2015. A provably correct sampler

for probabilistic programs. In 35th IARCS Annual Conference on Foundations of Software Technology

and Theoretical Computer Science (FSTTCS 2015). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.

Lena A. J¨ager, Felix Engelmann, and Shravan Vasishth. 2017. Similarity-based interference in sentence

comprehension: Literature review and Bayesian meta-analysis. Journal of Memory and Language 94

(2017), 316–339. https://doi.org/10.1016/j.jml.2017.01.004

Ohad Kammar, Sam Lindley, and Nicolas Oury. 2013. Handlers in action. ACM SIGPLAN Notices 48, 9

(2013), 145–158.

Daphne Koller and Nir Friedman. 2009. Probabilistic graphical models: principles and techniques. MIT

press.

Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. 2016. Automatic

diﬀerentiation variational inference. arXiv preprint arXiv:1603.00788 (2016).

Solomon Kullback and Richard A Leibler. 1951. On information and suﬃciency. The annals of mathematical

statistics 22, 1 (1951), 79–86.

Tuan Anh Le, Atilim Gunes Baydin, and Frank Wood. 2017.

Inference compilation and universal

probabilistic programming. In Artiﬁcial Intelligence and Statistics (AISTATS’17). PMLR, 1338–1348.

Maggie Lieu, Will M Farr, Michael Betancourt, Graham P Smith, Mauro Sereno, and Ian G McCarthy.

2017. Hierarchical inference of the relationship between concentration and mass in galaxy groups and

clusters. Monthly Notices of the Royal Astronomical Society 468, 4 (2017), 4872–4886.

Jarno Lintusaari, Henri Vuollekoski, Antti Kangasr¨a¨asi¨o, Kusti Skyt´en, Marko J¨arvenp¨a¨a, Pekka Marttinen,

Michael U. Gutmann, Aki Vehtari, Jukka Corander, and Samuel Kaski. 2018. ELFI: Engine for

Bibliography

187

Likelihood-Free Inference. Journal of Machine Learning Research 19, 16 (2018), 1–7. http://jmlr.

org/papers/v19/17-374.html

David JC MacKay. 2003. Information theory, inference and learning algorithms. Cambridge University

Press.

Carol Mak, Fabian Zaiser, and Luke Ong. 2021. Nonparametric Hamiltonian Monte Carlo. In International

Conference on Machine Learning. PMLR, 7336–7347.

Vikash K. Mansinghka, Ulrich Schaechtle, Shivam Handa, Alexey Radul, Yutian Chen, and Martin Rinard.

2018. Probabilistic Programming with Programmable Inference. In Proceedings of the 39th ACM

SIGPLAN Conference on Programming Language Design and Implementation (Philadelphia, PA, USA)

(PLDI 2018). ACM, New York, NY, USA, 603–616. https://doi.org/10.1145/3192366.3192409

Paul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavar´e. 2003. Markov chain Monte Carlo

without likelihoods. Proceedings of the National Academy of Sciences 100, 26 (2003), 15324–15328.

Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward

Teller. 1953. Equation of state calculations by fast computing machines. The journal of chemical

physics 21, 6 (1953), 1087–1092.

Nicholas Metropolis and S. Ulam. 1949. The Monte Carlo Method. J. Amer. Statist. Assoc. 44, 247

(1949), 335–341. http://www.jstor.org/stable/2280232

T. Minka, J.M. Winn, J.P. Guiver, S. Webster, Y. Zaykov, B. Yangel, A. Spengler, and J. Bronskill. 2014.

Infer.NET 2.6. Microsoft Research Cambridge. http://research.microsoft.com/infernet.

Thomas P. Minka. 2001. Expectation Propagation for Approximate Bayesian Inference. In Proceedings of

the Seventeenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI’01). 362–369.

Martin Mladenov, Chih-wei Hsu, Vihan Jain, Eugene Ie, Christopher Colby, Nicolas Mayoraz, Hubert Pham,

Dustin Tran, Ivan Vendrov, and Craig Boutilier. 2020. Demonstrating Principled Uncertainty Modeling

for Recommender Ecosystems with RecSim NG. In Fourteenth ACM Conference on Recommender

Systems (RecSys’20). Association for Computing Machinery, 591–593. https://doi.org/10.1145/

3383313.3411527

Dave Moore and Maria I. Gorinova. 2018. Eﬀect Handling for Composable Program Transformations in

Edward2. International Conference on Probabilistic Programming (2018). https://arxiv.org/abs/

1811.06150.

Kevin P Murphy. 2012. Machine learning: A probabilistic perspective. MIT press.

Iain Murray. 2007. Advances in Markov Chain Monte Carlo methods. University College London.

Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. 2016.

Probabilistic Inference by Program Transformation in Hakaru (System Description). In Functional and

Logic Programming. 62–79.

R Neal. 1993. Probabilistic inference using MCMC methods. University of Toronto (1993).

Radford M Neal. 2003. Slice sampling. Annals of statistics (2003), 705–741.

188

Bibliography

Radford M Neal et al. 2011. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte

Carlo 2, 11 (2011).

Flemming Nielson, Hanne R Nielson, and Chris Hankin. 2004. Principles of program analysis. Springer

Science & Business Media.

Hanne Riis Nielson and Flemming Nielson. 1992. Semantics with applications. Vol. 104. Springer.

George Papamakarios. 2019. Neural density estimation and likelihood-free inference. Ph.D. Dissertation.

University of Edinburgh.

George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-

narayanan. 2021. Normalizing Flows for Probabilistic Modeling and Inference. Journal of Machine

Learning Research 22, 57 (2021), 1–64. http://jmlr.org/papers/v22/19-1028.html

George Papamakarios, David Sterratt, and Iain Murray. 2019. Sequential neural likelihood: Fast likelihood-

free inference with autoregressive ﬂows. In Artiﬁcial Intelligence and Statistics (AISTATS’19). PMLR,

837–848.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor

Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,

Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie

Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning

Library. In Advances in Neural Information Processing Systems, Vol. 32.

Du Phan, Neeraj Pradhan, and Martin Jankowiak. 2019. Composable eﬀects for ﬂexible and accelerated

probabilistic programming in NumPyro. arXiv preprint arXiv:1912.11554 (2019).

Benjamin C Pierce. 2002. Types and programming languages. MIT press.

Emma Pierson, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker,

Vignesh Ramachandran, Phoebe Barghouty, Cheryl Phillips, Ravi Shroﬀ, et al. 2020. A large-scale

analysis of racial disparities in police stops across the United States. Nature human behaviour 4, 7

(2020), 736–745.

Gordon Plotkin and John Power. 2003. Algebraic operations and generic eﬀects. Applied categorical

structures 11, 1 (2003), 69–94.

Gordon Plotkin and Matija Pretnar. 2009. Handlers of Algebraic Eﬀects. In Programming Languages and

Systems, Giuseppe Castagna (Ed.). Springer Berlin Heidelberg, 80–94.

Gordon D Plotkin. 1981. A structural approach to operational semantics. Aarhus university.

Martyn Plummer et al. 2003. JAGS: A program for analysis of Bayesian graphical models using Gibbs

sampling. In Proceedings of the 3rd international workshop on distributed statistical computing, Vol. 124.

Vienna, Austria.

Matija Pretnar. 2015. An Introduction to Algebraic Eﬀects and Handlers. Invited tutorial paper. Electronic

Notes in Theoretical Computer Science 319 (2015), 19–35. The 31st Conference on the Mathematical

Foundations of Programming Semantics (MFPS XXXI).

Bibliography

189

Tom Rainforth, Christian Naesseth, Fredrik Lindsten, Brooks Paige, Jan-Willem van de Meent, Arnaud

Doucet, and Frank Wood. 2016. Interacting particle Markov chain Monte Carlo. In International

Conference on Machine Learning. PMLR, 2616–2625.

Thomas William Gamlen Rainforth. 2017. Automating inference, learning, and design using probabilistic

programming. Ph.D. Dissertation. University of Oxford.

Rajesh Ranganath, Sean Gerrish, and David Blei. 2014. Black box variational inference. In Artiﬁcial

intelligence and statistics. PMLR, 814–822.

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka. 2021. SPPL: Probabilistic Programming

with Fast Exact Symbolic Inference. Association for Computing Machinery, 804–819. https://doi.

org/10.1145/3453483.3454078

Andrei Sabelfeld and Andrew C. Myers. 2003. Language-based information-ﬂow security. IEEE J. Sel.

Areas Commun. 21, 1 (2003), 5–19.

Kris Sankaran and Susan P Holmes. 2018.

Latent variable modeling for the microbiome.

Biostatistics 20, 4 (06 2018), 599–614.

https://doi.org/10.1093/biostatistics/kxy018

arXiv:https://academic.oup.com/biostatistics/article-pdf/20/4/599/30160977/kxy018.pdf

Adam Scibior and Ohad Kammar. 2015. Eﬀects in Bayesian inference. In Workshop on Higher-Order

Programming with Eﬀects (HOPE).

Scott A Sisson, Yanan Fan, and Mark Beaumont. 2018. Handbook of approximate Bayesian computation.

CRC Press.

Scott A Sisson, Yanan Fan, and Mark M Tanaka. 2007. Sequential Monte Carlo without likelihoods.

Proceedings of the National Academy of Sciences 104, 6 (2007), 1760–1765.

Geoﬀrey Smith. 2007. Principles of secure information ﬂow analysis. In Malware Detection. Springer,

291–307.

Stan Development Team. 2018. Stan++/Stan3 Preliminary Design. https://discourse.mc-stan.org/

t/stan-stan3-preliminary-design/4147. In the Stan Forums.

Sam Staton, Frank Wood, Hongseok Yang, Chris Heunen, and Ohad Kammar. 2016. Semantics for

probabilistic programming: Higher-order functions, continuous distributions, and soft constraints. In

2016 31st annual ACM/IEEE symposium on logic in computer science (LICS). IEEE, 1–10.

Zenna Tavares, Xin Zhang, Edgar Minaysan, Javier Burroni, Rajesh Ranganath, and Armando So-

lar Lezama. 2019. The Random Conditional Distribution for Higher-Order Probabilistic Inference.

arXiv:1903.10556 [cs.PL]

Sean J Taylor and Benjamin Letham. 2018. Forecasting at scale. The American Statistician 72, 1 (2018),

37–45.

Alvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis Lueckmann, Conor Durkan, Pedro J

Gon¸calves, David S Greenberg, and Jakob H Macke. 2020. sbi : a toolkit for simulation-based inference.

Journal of Open Source Software 5, 52 (2020), 2505.

190

Bibliography

Luke Tierney. 1994. Markov chains for exploring posterior distributions. the Annals of Statistics (1994),

1701–1728.

David Tolpin, Jan-Willem van de Meent, Hongseok Yang, and Frank Wood. 2016. Design and Implemen-

tation of Probabilistic Programming Language Anglican. In Proceedings of the 28th Symposium on the

Implementation and Application of Functional Programming Languages (IFL 2016). Association for

Computing Machinery, Article 6, 12 pages. https://doi.org/10.1145/3064899.3064910

Dustin Tran, Matthew D. Hoﬀman, Dave Moore, Christopher Suter, Srinivas Vasudevan, Alexey Radul,

Matthew Johnson, and Rif A. Saurous. 2018. Simple, Distributed, and Accelerated Probabilistic

Programming. In Advances in Neural Information Processing Systems (NeurIPS’18).

Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. 2018. An introduction to

probabilistic programming. arXiv preprint arXiv:1809.10756 (2018).

Jan-Willem van de Meent, Hongseok Yang, Vikash Mansinghka, and Frank Wood. 2015. Particle Gibbs

with ancestor sampling for probabilistic programs. In Artiﬁcial Intelligence and Statistics. PMLR,

986–994.

Dennis M. Volpano, Cynthia E. Irvine, and Geoﬀrey Smith. 1996. A Sound Type System for Secure Flow

Analysis. J. Comput. Secur. 4, 2/3 (1996), 167–188.

Eli N. Weinstein and Debora S. Marks. 2021. A structured observation distribution for generative biological

sequence prediction and forecasting. bioRxiv (2021). https://doi.org/10.1101/2020.07.31.231381

arXiv:https://www.biorxiv.org/content/early/2021/02/24/2020.07.31.231381.full.pdf

John Winn, Christopher M Bishop, and Tommi Jaakkola. 2005. Variational message passing. Journal of

Machine Learning Research 6, 4 (2005).

Frank Wood, Jan Willem van de Meent, and Vikash Mansinghka. 2014. A New Approach to Probabilistic

Programming Inference. In Artiﬁcial Intelligence and Statistics (AISTATS’14). 1024–1032.

Jeremy Yallop. 2017. Staged generic programming. Proceedings of the ACM on Programming Languages

1, ICFP (2017), 1–29.

Mihalis Yannakakis. 1981. Computing the minimum ﬁll-in is NP-complete. SIAM Journal on Algebraic

Discrete Methods 2, 1 (1981), 77–79.

Nevin Lianwen Zhang and David Poole. 1994. A simple approach to Bayesian network computations. In

Proceedings of the Biennial Conference-Canadian Society for Computational Studies of Intelligence.

Canadian Information Processing Society, 171–178.

