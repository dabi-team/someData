©2022 IEEE; THIS WORK HAS BEEN SUBMITTED TO THE IEEE FOR POSSIBLE PUBLICATION. COPYRIGHT MAY BE TRANSFERRED WITHOUT NOTICE.

1

Intrusion Prevention through Optimal Stopping
Kim Hammar †‡ and Rolf Stadler†‡
† Division of Network and Systems Engineering, KTH Royal Institute of Technology, Sweden
‡ KTH Center for Cyber Defense and Information Security, Sweden
kimham, stadler
}
April 4, 2022

@kth.se

Email:

{

2
2
0
2

r
p
A
1

]

G
L
.
s
c
[

7
v
9
8
2
0
0
.
1
1
1
2
:
v
i
X
r
a

Abstract—We study automated intrusion prevention using
reinforcement learning. Following a novel approach, we formulate
the problem of intrusion prevention as an (optimal) multiple
stopping problem. This formulation gives us insight into the
structure of optimal policies, which we show to have threshold
properties. For most practical cases, it is not feasible to obtain
an optimal defender policy using dynamic programming. We
therefore develop a reinforcement learning approach to approx-
imate an optimal threshold policy. We introduce T-SPSA, an
efﬁcient reinforcement learning algorithm that learns threshold
policies through stochastic approximation. We show that T-
SPSA outperforms state-of-the-art algorithms for our use case.
Our overall method for learning and validating policies includes
two systems: a simulation system where defender policies are
incrementally learned and an emulation system where statistics
are produced that drive simulation runs and where learned
policies are evaluated. We show that this approach can produce
effective defender policies for a practical IT infrastructure.

Index Terms—Network security, automation, optimal stopping,
reinforcement learning, Markov Decision Process, MDP, POMDP.

Attacker

Clients
. . .

alerts

Gateway

1

1

IDS

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

25

26

19

20

21

22

23

24

27

28

29

30

31

Fig. 1: The IT infrastructure and the actors in the use case.

Defender

I. INTRODUCTION

An organization’s security strategy has traditionally been
deﬁned, implemented, and updated by domain experts [1].
Although this approach can provide basic security for an
organization’s communication and computing infrastructure,
a growing concern is that infrastructure update cycles become
shorter and attacks increase in sophistication [2], [3]. Conse-
quently, the security requirements become increasingly difﬁ-
cult to meet. To address this challenge, signiﬁcant efforts have
started to automate security frameworks and the process of
obtaining effective security policies. Examples of this research
include: automated creation of threat models [4]; computation
of defender policies using dynamic programming and control
theory [5], [6]; computation of exploits and corresponding
defenses through evolutionary methods [7]; identiﬁcation of
infrastructure vulnerabilities through attack simulations and
threat intelligence [8], [9]; computation of defender policies
through game-theoretic methods [10], [11]; and use of machine
learning techniques to estimate model parameters and policies
[12], [13].

In this paper, we present a novel approach to automatically
learn defender policies. We apply this approach to an intrusion
prevention use case. Here, we use the term ”intrusion preven-
tion” as suggested in the literature, e.g. in [1]. It means that
a defender prevents an attacker from reaching its goal, rather
than preventing it from accessing any part of the infrastructure.

Our use case involves the IT infrastructure of an organi-
zation (see Fig. 1). The operator of this infrastructure, which
we call the defender, takes measures to protect it against a
possible attacker while, at the same time, providing a service
to a client population. The infrastructure includes a public
gateway through which the clients access the service and
which also is open to a possible attacker. The attacker decides
when to start an intrusion and then executes a sequence of
actions that includes reconnaissance and exploits. Conversely,
the defender aims at preventing intrusions and maintaining
service to its clients. It monitors the infrastructure and can
defend it by taking defensive actions, which can prevent a
possible attacker but also incur costs. What makes the task of
the defender difﬁcult is the fact that it lacks direct knowledge
of the attacker’s actions and must infer that an intrusion occurs
from monitoring data.

We study the use case within the framework of discrete-time
dynamical systems. Speciﬁcally, we formulate the problem of
ﬁnding an optimal defender policy as an (optimal) multiple
stopping problem. In this formulation, the defender can take a
ﬁnite number of stops. Each stop is associated with a defensive
action and the objective is to decide the optimal times when
to stop. This approach gives us insight
into the structure
of optimal defender policies through the theory of dynamic
programming and optimal stopping [14], [15]. In particular,
we show that an optimal multi-threshold policy exists that can

 
 
 
 
 
 
be efﬁciently computed and implemented.

The structure of optimal policies in dynamical systems
is a well studied area [16], [17]. However, it has not been
considered in prior research on automated intrusion prevention
[12], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27],
[28], [29]. Further, although the optimal stopping problem
frequently is used to formulate problems in the ﬁelds of ﬁnance
and communication systems [30], [31], [32], [33], [34], [35],
[36], [37], [38], [13], to the best of our knowledge, formulating
intrusion prevention as a multiple stopping problem is a novel
approach.

Since the defender can access only a set of infrastructure
metrics and does not directly observe the attacker, we use
a Partially Observed Markov Decision Process (POMDP)
to model the multiple stopping problem. An optimal policy
for a POMDP can be obtained through two main methods:
dynamic programming and reinforcement
learning. In our
case, dynamic programming is not feasible due to the size of
the POMDP [39]. Therefore, we use a reinforcement learning
approach to obtain the defender policy. We simulate a long
series of POMDP episodes whereby the defender continuously
updates its policy based on outcomes of previous episodes.
To update the policy, we introduce T-SPSA, a reinforcement
learning algorithm that exploits the threshold structure of
optimal policies. We show that T-SPSA efﬁciently learns a
near-optimal policy despite the high complexity of computing
optimal policies for general POMDPs [39].

Our method for learning and validating policies includes
building two systems (see Fig. 2). First, we develop an
emulation system where key functional components of the
target infrastructure are replicated. In this system, we run
attack scenarios and defender responses. These runs produce
system metrics and logs that we use to estimate empirical
distributions of infrastructure metrics, which are needed to
simulate POMDP episodes. Second, we develop a simulation
system where POMDP episodes are executed and policies are
incrementally learned. Finally, the policies are extracted and
evaluated in the emulation system and possibly implemented
in the target infrastructure (see Fig. 2). In short, the emulation
system is used to provide the statistics needed to simulate
the POMDP and to evaluate policies, whereas the simulation
system is used to learn policies.

We make three contributions with this paper. First, we
formulate intrusion prevention as a problem of multiple stop-
ping. This novel formulation allows us a) to derive properties
of an optimal defender policy using results from dynamic
programming and optimal stopping; and b) to approximate an
optimal policy for a non-trivial infrastructure conﬁguration.
Second, we present a reinforcement learning approach to ob-
tain policies in an emulated infrastructure. With this approach,
we narrow the gap between the evaluation environment and
a scenario playing out in a real system. We also address a
limitation of many related works, which rely on simulations
solely to evaluate policies [12], [7], [18], [20], [21], [40],
[19]. Third, we present T-SPSA, an efﬁcient reinforcement
learning algorithm that exploits the threshold structure of
optimal policies and outperforms state-of-the-art algorithms
for our use case.

SIMULATION SYSTEM

s1,1

s2,1

...

s1,2

s2,2

...

s1,3

s2,3

...

Policy Mapping
π

. . .

. . .

...

s1,n

s2,n

...

Model
Estimation

POMDP model &
Reinforcement Learning

EMULATION SYSTEM

Policy evaluation &
Model estimation

Policy
Implementation π

Selective
Replication

TARGET
INFRASTRUCTURE

Automated
Intrusion Prevention

Fig. 2: Our approach for ﬁnding and evaluating intrusion
prevention policies.

We conclude this section with remarks about the context
of this research and the practical relevance of the results in
this paper. The objective of our line of research is to construct
a mathematical and conceptual framework, validated by an
experimental environment, that produces defender policies for
realistic scenarios through self-learning. We are engaged in a
program with high potential reward that will need many years
of investigation. This paper provides an important result and
milestone in this program.

From a practical point of view, the main question the paper
answers is this: at which points in time should a defender
take defensive actions given periodic but limited observational
data? The paper proposes a fundamental framework to study
this question. We show theoretically and experimentally that
the optimal action times can be obtained through thresholds
that
the framework predicts and which can be efﬁciently
implemented in a real system.

II. THE INTRUSION PREVENTION USE CASE

We consider an intrusion prevention use case that involves
the IT infrastructure of an organization. The operator of this
infrastructure, which we call the defender, takes measures to
protect it against an attacker while, at the same time, providing
a service to a client population (Fig. 1). The infrastructure
includes a set of servers that run the service and an intrusion
detection system (IDS) that logs events in real-time. Clients
access the service through a public gateway, which also is
open to the attacker.

We assume that the attacker intrudes into the infrastructure
through the gateway, performs reconnaissance, and exploits
found vulnerabilities, while the defender continuously moni-
tors the infrastructure through accessing and analyzing IDS
statistics and login attempts at
the servers. The defender
can take a ﬁxed number of defensive actions to prevent
the attacker. A defensive action is for example to revoke
user certiﬁcates in the infrastructure, which will recover user
accounts compromised by the attacker. It is assumed that the
defender takes the defensive actions in a predetermined order.
The ﬁnal action that the defender can take is to block all

2

external access to the gateway. As a consequence of this action,
the service as well as any ongoing intrusion are disrupted.

In deciding when to take defensive actions, the defender has
two objectives: (i) maintain service to its clients; and (ii), keep
a possible attacker out of the infrastructure. The optimal policy
for the defender is to monitor the infrastructure and maintain
service until the moment when the attacker enters through
the gateway, at which time the attacker must be prevented by
taking defensive actions. The challenge for the defender is to
identify the precise time when this moment occurs.

In this work, we model the attacker as an agent that starts the
intrusion at a random point in time and then takes a predeﬁned
sequence of actions, which includes reconnaissance to explore
the infrastructure and exploits to compromise servers.

We study the use case from the defender’s perspective.
The evolution of the system state and the actions by the
defender are modeled with a discrete-time Partially Observed
Markov Decision Process (POMDP). The reward function of
this process encodes the beneﬁt of maintaining service and
the loss of being intruded. Finding an optimal defender policy
thus means maximizing the expected reward.

III. THEORETICAL BACKGROUND

This section covers the preliminaries on Markov decision

processes, reinforcement learning, and optimal stopping.

A. Markov Decision Processes

,

(cid:105)

P

=

A

M

,
A

at
st,st+1

[14], [16].

A Markov Decision Process (MDP) models the control of
a discrete-time dynamical system and is deﬁned by a seven-
at
at
tuple
st,st+1, γ, ρ1, T
st,st+1,
R
(cid:104)S
denotes the set of states and

S
denotes the set of actions.
refers to the probability of transitioning from state
P
st to state st+1 when taking action at (Eq. 1), which has the
Markov property P [st+1
s1, . . . , st]. Similarly,
|
R is the expected reward when taking action at
R
and transitioning from state st to state st+1 (Eq. 2), which
R. If
is bounded, i.e.
are independent of the time-step t, the
are ﬁnite, the
and
S
(0, 1] is the discount
∈
[0, 1] is the initial state distribution, and T is

|R
at
st,st+1
P
MDP is said to be stationary and if
MDP is said to be ﬁnite. Finally, γ
factor, ρ1 :
S →
the time horizon.

st] = P [st+1
|

at
st,st+1| ≤

at
st,st+1 ∈

for some M

at
st,st+1

M <

and

∞

R

A

∈

at
st,st+1
at
st,st+1

P

R

= P [st+1
|
= E [rt+1

st, at]
at, st, st+1]

|

(1)

(2)

The system evolves in discrete time-steps from t = 1 to t = T ,
which constitute one episode of the system.

[17].

In contrast

is an extension of an MDP [41],
an MDP,
servable. A POMDP is deﬁned by a nine-tuple
at
st,st+1, γ, ρ1, T,

A Partially Observed Markov Decision Process (POMDP)
to
in a POMDP the states are not directly ob-
P =
. The ﬁrst seven ele-
denotes the set of observations
st+1, at] is the observation
|
,
∈ S

at
st,st+1,
(cid:104)S
ments deﬁne an MDP.
and
function, where ot+1
and

P
(ot+1, st+1, at) = P[ot+1
, st+1

are ﬁnite, the POMDP is said to be ﬁnite.

, and at

,
A

∈ A

∈ O

. If

M

Z(cid:105)

R

O

O

O

Z

S

,

,

,

A

∈ S

∈ B

The belief state bt

is deﬁned as bt(s) = P[st = s
ht]
|
for all s
. bt is a sufﬁcient statistic of the state st based
on the history ht of the initial state distribution, the actions,
and the observations: ht = (ρ1, a1, o1, . . . , at−1, ot)
. The
∈ H
= ∆(
1)-simplex [42],
belief space
B
[43], where ∆(
) denotes the set of probability distributions
. By deﬁning the state at time t to be the belief state
over
bt, a POMDP can be formulated as a continuous-state MDP:

) is the unit (

|S| −

S

S

S

M

,

=
The belief state can be computed recursively as follows [17]:

, γ, ρ1, T

,
A

(cid:104)B

R

P

(cid:105)

,

.

at
bt,bt+1

at
bt,bt+1

bt+1(st+1) = C

Z

(ot+1, st+1, at)

(cid:88)

at
st,st+1bt(st)

(3)

P
(ot+1, st+1, at) (cid:80)

st∈S

where C = 1/ (cid:80)
at
st,st+1bt(st)
is a normalizing factor independent of st+1 to make bt+1
sum to 1.

st+1∈S Z

st∈S P

B. The Reinforcement Learning Problem

Reinforcement learning deals with the problem of choosing
a sequence of actions for a sequentially observed state variable
to maximize a reward function [44], [45]. This problem can
be modeled with an MDP if the state space is observable, or
with a POMDP if the state space is not fully observable.

∆(

} × S →

), where ∆(

1, . . . , T
{

In the context of an MDP, a policy is deﬁned as a function
) denotes the set of
π :
. In the case of a POMDP,
probability distributions over
), or,
a policy is deﬁned as a function π :
alternatively, as a function π :
). In
1, . . . , T
{
both cases, a policy is called stationary if it is independent of
the time-step t given the current state or belief state.

H →
} × B →

∆(
A
∆(
A

A

A

A

An optimal policy π∗ is a policy that maximizes the
expected discounted cumulative reward over the time horizon:

π∗

arg max
π∈Π

∈

Eπ

(cid:35)

γt−1rt

(cid:34) T

(cid:88)

t=1

(4)

where Π is the policy space, γ is the discount factor, rt is the
reward at time t, and Eπ denotes the expectation under π.

Optimal deterministic policies exist for ﬁnite MDPs and
POMDPs with bounded rewards and either ﬁnite horizons or
(0, 1) [16], [17]. If the MDPs
inﬁnite horizons with γ
or POMDPs also are stationary and the horizons are either
(0, 1), optimal stationary policies
random or inﬁnite with γ
exist [16], [17].

∈

∈

The Bellman equations relate any optimal policy π∗ to the
R, where

two value functions V ∗ :

R and Q∗ :

and

S

S →

S × A →

A

E(cid:2)rt+1 + γV ∗(st+1)

are state and action spaces of an MDP [46]:
(cid:3)
V ∗(st) = max
st, at
|
at∈A
Q∗(st, at) = E(cid:2)rt+1 + γV ∗(st+1)
(cid:3)
st, at
|
arg max
at∈A

Q∗(st, at)

π∗(st)

∈

(5)

(6)

(7)

V ∗(st) and Q∗(st, at) denote the expected cumulative dis-
counted reward under π∗ for each state and state-action pair,
respectively. Solving Eqs. 5-6 means computing the value
functions from which an optimal policy can be obtained (Eq.
7). In the case of a POMDP, the Bellman equations contain
bt instead of st and V ∗(bt) is convex [47].

3

Two principal methods are used for ﬁnding an optimal
policy in a ﬁnite MDP or POMDP: dynamic programming
and reinforcement learning.

First, the dynamic programming method (e.g. value iteration
[46], [48], [16]) assumes complete knowledge of the seven-
tuple MDP or the nine-tuple POMDP and obtains an optimal
policy by solving the Bellman equations iteratively (Eq. 7),
with polynomial time-complexity per iteration for MDPs and
PSPACE-complete time-complexity for POMDPs [39].

Second, the reinforcement learning method computes or
approximates an optimal policy without requiring complete
knowledge of the transition probabilities or observation proba-
bilities of the MDP or POMDP. Three classes of reinforcement
learning algorithms exist: value-based algorithms, which ap-
proximate solutions to the Bellman equations (e.g. Q-learning
[49]); policy-based algorithms, which directly search through
policy space using gradient-based methods (e.g. Proximal
Policy Optimization (PPO) [50]); and model-based algorithms,
which learn the transition or observation probabilities of
the MDP or POMDP (e.g. Dyna-Q [45]). The three algo-
rithm types can also be combined, e.g. through actor-critic
algorithms, which are mixtures of value-based and policy-
based algorithms [45]. In contrast to dynamic programming
algorithms, reinforcement learning algorithms generally have
no guarantees to converge to an optimal policy except for the
tabular case [51], [52].

C. The Markovian Optimal Stopping Problem

Optimal stopping is a classical problem domain with a
well-developed theory [15], [53], [54], [55], [48], [56], [57],
[16], [58]. Example use cases for this theory include: asset
selling [48], change detection [37], machine replacement [17],
hypothesis testing [15], gambling [55], selling decisions [35],
queue management [36], advertisement scheduling [33], in-
dustrial control [59], and the secretary problem [16], [32].

Many variants of the optimal stopping problem have been
studied. For example, discrete-time and continuous-time prob-
lems, ﬁnite horizon and inﬁnite horizon problems, problems
with fully observed and partially observed state spaces, prob-
lems with ﬁnite and inﬁnite state spaces, Markovian and non-
Markovian problems, and single-stop and multi-stop problems.
Consequently, different solution methods for these variants
have been developed. The most commonly used methods are
the martingale approach [54], [55], [60] and the Markovian
approach [53], [48], [16], [56], [57].

In this paper, we investigate the multiple stopping problem
with L stops, a ﬁnite time horizon T , discrete-time progres-
sion, bounded rewards, a ﬁnite state space, and the Markov
property. We use the Markovian solution approach and model
the problem as a POMDP, where the system state evolves
as a discrete-time Markov process (st,l)T
t=1 that is partially
observed and depends on the number of stops remaining
l

∈ {
At each time-step t of the decision process, two actions are
available: “stop” (S) and “continue” (C). The stop action with
and if only one
l stops remaining yields a reward
of the L stops remain, the process terminates. In the case of

S
st,st+1,lt
R

1, . . . , L

.
}

Episode

time-step t = 1

Intrusion event

Intrusion ongoing

Early stopping times

t

t = T

Stopping times that
affect the intrusion

Fig. 3: Optimal multiple stopping formulation of intrusion
prevention; the horizontal axis represents time; T is the time
1; the dashed line shows
horizon; the episode length is T
the intrusion start time; the optimal policy is to prevent the
attacker at the time of intrusion.

−

a continue action or a non-ﬁnal stop action at, the decision
process transitions to the next state according to the transition
probabilities

and yields a reward

.

The stopping time with l stops remaining is a random
variable τl that is dependent on s1, . . . , sτl and independent
of sτl+1, . . . sT [54]:

at
st,st+1,lt

R

at
st,st+1,lt

P

τl = inf

t : t > τl+1, at = S
{

,
}

l

∈

1, .., L, τL+1 = 0 (8)

The objective is to ﬁnd a stopping policy π∗

}
that depends on l and maximizes the expected discounted
cumulative reward of the stopping times τL, τL−1, . . . , τ1:

→ {

l (st)

S, C

π∗
l ∈

arg max
πl

Eπl

(cid:34) τL−1
(cid:88)

t=1

γt−1

+ . . . +

τ1−1
(cid:88)

t=τ2+1

γt−1

C

st,st+1,L + γτL−1

R

,sτL+1,L

S
sτL
R

(cid:35)

C

st,st+1,1 + γτ1−1

R

S
sτ1 ,sτ1+1,1

R

(9)

Due to the Markov property, any policy that satisﬁes Eq. 9 also
satisﬁes the Bellman equation (Eq. 7), which in the partially
observed case is:

π∗
l (b)

arg max
{S,C}

∈

(cid:34)

(cid:104)

El
(cid:124)

S
b,bo
R

S ,l + γV ∗
(cid:123)(cid:122)
stop (S)

(cid:105)
l−1(bo
S)

(cid:104)

, El
(cid:125)
(cid:124)

R

C
b,bo

C ,l + γV ∗
(cid:123)(cid:122)
continue (C)

(cid:105)
l (bo
C)

(cid:125)

(10)

(cid:35)

b
∀

∈ B

where πl is the stopping policy with l stops remaining, El
denotes the expectation with l stops remaining, b is the belief
state, V ∗
S and
l
bo
C can be computed using Eq. 3, and
a,l is the expected
reward of action a
in belief state bt when observing
o with l stops remaining.

is the value function with l stops remaining, bo

a
b,bo
R

S, C

∈ {

}

IV. FORMALIZING THE INTRUSION PREVENTION USE
CASE AND OUR REINFORCEMENT LEARNING APPROACH

We ﬁrst present a formal model of the use case described
in Section II and then we introduce our solution method.
Speciﬁcally, we ﬁrst deﬁne a POMDP model of the intrusion
prevention use case. Then, we apply the theory of dynamic
programming and optimal stopping to obtain structural results
of an optimal defender policy. Lastly, we describe our rein-
forcement learning approach to approximate an optimal policy.

4

1
t
≥
lt > 0

0

intrusion starts
Qt = 1

2
t
≥
lt > 0

1

ﬁnal stop
lt = 0

intrusion prevented
lt = 0

∅

Fig. 5: State transition diagram of the POMDP: each circle
represents a state; an arrow represents a state transition; a label
indicates the event that triggers the state transition; an episode
starts in state s1 = 0 with l1 = L.

∅

Eq. 11 deﬁnes the transition probabilities to the terminal
. The terminal state is reached when the ﬁnal (lt = 1)
state
stop action S (at = 1) is taken. If Eq. 11 is not applicable,
i.e., if the system does not reach the terminal state, then the
transition probabilities when taking action S (at = 1) or C
(at = 0) are deﬁned by Eqs. 12-14.

Eq. 12 captures the case where no intrusion occurs and
st+1 = st = 0; Eq. 13 speciﬁes the case when the intrusion
starts where st = 0 and st+1 = 1; and Eq. 14 describes the
case where an intrusion is in progress and st+1 = st = 1.

With this deﬁnition of the transition probabilities, the evolu-
tion of the system can be understood using the state transition
diagram in Fig. 5.

5) Observation Function

(ot+1, st+1, at): We assume
Z
that the number of IDS alerts and login attempts generated
during one time-step are discrete random variables X
fX ,
fZ that depend on the state. Consequently, the
Y
probability that ∆x severe alerts, ∆y warning alerts, and ∆z
login attempts occur during time-step t can be expressed as
fXY Z(∆x, ∆y, ∆z

fY , Z

st).

∼

∼

∼

Z

We deﬁne the time-homogeneous observation function
(ot+1, st+1, at) = P[ot+1
(cid:0)(lt, ∆x, ∆y, ∆z), st,
(cid:1) = 1
(cid:0)
·

st+1, at] as follows:
|
(cid:1) = fXY Z(∆x, ∆y, ∆z

st)
|

(16)

(15)

,
∅

,
∅

Z

Z

·

|

at
st,lt

R

6) Reward Function

: The objective of the intrusion
prevention use case is to maintain service on the infrastructure
while, at
the same time, preventing a possible intrusion.
Therefore, we deﬁne the reward function to give the maximal
reward if the defender maintains service until the intrusion
starts and then prevents the intrusion by taking L stop actions.
is parameterized by the
R
reward that the defender receives for stopping an intrusion
(Rst = 50), the reward for maintaining service (Rsla = 1),
and the loss of being intruded (Rint =

The reward per time-step

at
st,lt

10):

−

·
∅,0 = 0
R
S
st,lt
R
C
st,lt

R

= stRst/4lt
= Rsla + stRint/L

st

st

0, 1
}
0, 1
}

∈ {

∈ {

(17)

(18)

(19)

Fig. 4: The cumulative distribution function (CDF) of the
intrusion start time It.

A. A POMDP Model of the Intrusion Prevention Use Case

We formulate the intrusion prevention use case as a multiple
stopping problem where an intrusion starts at a random time
and each stop is associated with a defensive action (Fig. 3).
We model this problem as a POMDP.

A

1) Actions

: The defender has two actions: “stop” (S)
.
and “continue” (C). The action space is thus
}
We encode S with 1 and C with 0 to simplify the formal
description below.

S, C

A

=

{

The number of stops that the defender must execute to
1, which is a predeﬁned parameter

prevent an intrusion is L
of our use case.
2) States

≥

.

,

S

=

=

∅}

∈ {

∅ ∈ S

S
0, 1
}

and Initial State Distribution ρ1: The system
is zero if no intrusion is occurring and st = 1
state st
if an intrusion is ongoing. In the initial state, no intrusion is
occurring and s1 = 0. Hence, the initial state distribution is
the degenerate distribution ρ1(0) = 1. Further, we introduce
a terminal state
, which is reached after the defender
takes the ﬁnal stop action or after an intrusion is prevented
(see below). The state space is thus

0, 1,
{
: The defender has a partial view
3) Observations
O
the defender observes ot =
of the system. If st
∅
(lt, ∆xt, ∆yt, ∆zt), where lt
is the number of
∈ {
stops remaining and (∆xt, ∆yt, ∆zt) are bounded counters
that denote the number of severe IDS alerts, warning IDS
alerts, and login attempts generated during time-step t, re-
spectively. If the system is in the terminal state, the defender
=
observes oT =
the observation space is
0, . . . , ∆zmax
0, . . . , ∆xmax
.
{
4) Transition Probabilities
: We model the start
t=1, where Qt

of an intrusion by a Bernoulli process (Qt)T
∼
Ber(p = 0.01) is a Bernoulli random variable. The time of
the ﬁrst occurrence of Qt = 1 is the start time of the intrusion
Ge(p =
It, which thus is geometrically distributed, i.e., It
0.01) (Fig. 4).

0, . . . , ∆ymax
} × {
at
st,st+1,lt

. Hence,
∅
} × {

O
} ∪ ∅

1, 2, . . . , L

∼

P

}

We deﬁne the time-homogeneous transition probabilities
at
st, at] as follows:
st,st+1,lt
|

= Plt [st+1

P

] = 1

·

,
∅|∅
p

−

P1 [
∅|·
Plt [0
Plt [1
Plt [1

, 1] = Plt [
0, at] = 1
|
0, at] = p
|
1, at] = 1
|

if lt
if lt
if lt

−

−

−

at > 0
at > 0
at > 0

(11)

(12)

(13)

(14)

where Plt denotes the probability with lt stops remaining. All
other state transitions occur with probability 0.

5

50100150200250300intrusionstarttimet0.00.51.0CDFIt(t)It∼Ge(p=0.01)(cid:54)
Eq. 17 states that the reward in the terminal state is zero. Eq.
18 indicates that each stop incurs a cost by interrupting service
and possibly a reward if it affects an ongoing intrusion. Lastly,
Eq. 19 states that the defender receives a positive reward for
maintaining service and a loss for each time-step that it is
under intrusion. (Remark: the reward function can equivalently
be stated to give a cumulative reward upon transitioning to the
terminal state and zero reward otherwise [16].)

7) Time Horizon T∅: The time horizon T∅ is a random
variable that indicates the time t when the terminal state
is
reached. Since the expected time of intrusion E[It] is ﬁnite, it
follows that Eπl [T∅] <
for any policy πl that is guaranteed
to use L stops as t
. Further, as the continue reward is
negative when t > It, the optimal stopping times τ1, . . . , τL
exist. (Remark: it is also possible to deﬁne T =
be an inﬁnitely absorbing state.)

∞
→ ∞

and let

∞

∅

∅

8) Policy Space Πl and Objective Function J: As the
POMDP is stationary and the time horizon T∅ is not predeter-
mined, it is sufﬁcient to consider stationary policies. Further,
since the POMDP is ﬁnite, an optimal deterministic policy
exists [16], [17]. Despite this, we consider stochastic policies
to enable smooth optimization. Speciﬁcally, we consider the
Πl is a
space of stationary stochastic policies Πl where πl
policy πl :
.
}

), which depends on l

∈ {
Πl maximizes the expected dis-

B →
An optimal policy π∗

∈
1, . . . , L

l ∈
counted cumulative reward over the horizon T∅:

∆(

A

SL
...

S2

S1

0

α∗
L . . .

α∗
2

α∗
1

b(1)

1

Fig. 6: Illustration of Theorem 1: there exist L thresholds α∗
α∗
L ∈ B
≥
satisﬁes Eqs. 22-24.

and an optimal threshold policy π∗
l

2, . . . ,

α∗

1 ≥
that

belief states where it is optimal to continue. The number of
stops remaining, l, ranges from 1 to L.

Applying the theory developed in [17], [34], [33], we obtain

the following structural result for an optimal policy.

Theorem 1. Given the POMDP in Section IV-A, let L denote
the number of stop actions, fXY Z|s the conditional distribu-
tion of the observations, b(1) the belief state, Sl the stopping
set, and Cl the continuation set. The following holds:
(A)

Sl−1

Sl

⊆

(B) If L = 1, there exists a value α∗

policy π∗

L that satisﬁes:

l

1, . . . L
}

∈ {

(22)

[0, 1] and an optimal

∈

J(πl) = Eπl

(cid:34) T

∅(cid:88)

γt−1

π∗
l ∈

arg max
πl∈Πl

t=1
J(πl)

(cid:35)

at
st,lt

R

We set the discount factor to be γ = 1. (The objective in
Eq. 20 is upper bounded when γ = 1 since Eπl [T∅] is ﬁnite
Πl that is guaranteed to use L stops as
for any policy πl
, which is true for any optimal policy (see Lemma 1
t
in Appendix A).)

→ ∞

∈

Eq. 20 deﬁnes an optimization problem which reﬂects the
objective of our use case. In the following section, we state
structural properties of an optimal policy that solves this
problem.

B. Threshold Properties of an Optimal Policy

A policy that solves the multiple stopping problem is a
solution to Eqs. 20-21. We know from the theory of dynamic
programming that this policy satisﬁes the Bellman equation
formulated in terms of the belief state (Eq. 10) [17], [42].

{

=

0, 1,

ht] (see Sec-

1. Further, since bt(0) = 1

The belief state bt is deﬁned as bt(st) = P[st
|
tion III-A). As the state space of the POMDP is
∅}
S
(see Fig. 5), bt is a probability vector with two components:
bt(0) = P[st = 0
ht] and bt(1) = P[st = 1
ht], where
|
|
bt(1), the belief
t = 1, . . . T∅ −
state is determined by bt(1) and the belief space
can be
described by the unit interval, i.e.

= [0, 1].
into two sets—the stopping set Sl =
We partition
(cid:0)b(1)(cid:1) = S
b(1)
, which contains the belief
{
states where it is optimal to stop, and the continuation set
Cl =
, which contains the

}
(cid:0)b(1)(cid:1) = C

[0, 1] : π∗
l

[0, 1] : π∗
l

−

∈

B

B

B

b(1)
{

∈

}

(20)

(21)

(C) If L

π∗
L(b(1)) = S

b(1)

α∗

≥

⇐⇒

(23)

≥

1 and fXY Z|s is totally positive of order 2 (i.e.,
[0, 1]
1 ≥

TP2), there exist L values α∗
and an optimal policy π∗
l

2 ≥
that satisﬁes:

L ∈

α∗

α∗

. . .

≥

π∗
l (b(1)) = S

⇐⇒

b(1)

≥

α∗
l

l

1, . . . , L

∈ {

(24)

}

Proof. See Appendix A.

Theorem 1.A states that the stopping sets have a nested
structure. This means that if it is optimal to stop when b(1) has
1 stops remain, then it is also optimal
a certain value while l
to stop for the same value when l or more stops remain.

−

Theorem 1.B and Theorem 1.C state that there exist an
1,
optimal policy with threshold properties (see Fig. 6). If L
the probability matrix of
an additional condition applies:
fXY Z|s must be TP2 (all second order minors must be non-
negative) [17, Deﬁnition 10.2.1, pp. 223][61]. This condition
is satisﬁed for example if fXY Z|s is stochastically monotone
in s.

≥

Knowing that

there exists optimal policies with special
structure has two beneﬁts. First, insight into the structure of
optimal policies often leads to a concise formulation and efﬁ-
cient implementation of the policies [16], [11]. This is obvious
in the case of threshold policies. Second, the complexity of
computing or learning an optimal policy can be reduced by
exploiting structural properties [17], [36]. In the following
section, we describe a reinforcement learning algorithm that
exploits the structural result in Theorem 1.

6

C. Our Reinforcement Learning Algorithm: T-SPSA

Client

Functions

Application servers

(26)

and

≥

≥

α∗

α∗

1 ≥

Theorem 1 states that under given assumptions and given
1 stop actions, there exists an optimal policy which
L
uses L thresholds α∗
[0, 1]. We present
2, . . . ,
an algorithm, which we call T-SPSA, that computes these
thresholds through reinforcement learning.
RL. The component
We parameterize π with a vector θ
∈
1, . . . L
θl of θ relates to the threshold with l
stops
}
remaining. T-SPSA updates θ through stochastic gradient
ascent with the following gradient [62]:
(cid:34) T

L ∈

∈ {

(cid:35)

∇

θ log πθ,l(at

θJ(θ) = Eπθ,l

∅(cid:88)
t=1 ∇
To ensure differentiability, we deﬁne πθ,l
stochastic policy that approximates a threshold policy:
(cid:19)−20(cid:33)−1

bt(1))
|

(cid:32)

aτ
bτ ,lt

T
∅(cid:88)
τ =t R
to be a smooth

(25)

πθ,l

(cid:0)S

b(1)(cid:1) =
|

1 +

(cid:18) b(1)(1
−
σ(θl)(1
−
sigmoid

σ(θl))
b(1))

the

σ(
where
σ(θ1), σ(θ2), . . . , σ(θL)

)
·

is

function
[0, 1] are the L thresholds.

∈

∈

∈ {

outcomes ˆJ(θ(n) + cn∆n) and ˆJ(θ(n) −

∈
We learn the threshold vector θ through simulation of the
RL randomly.
POMDP as follows. First, we initialize θ(1) ∈
1, 2, . . .
of T-SPSA, we per-
Second, for each iteration n
}
R
cn∆n, where cn
turb θ(n) to obtain θ(n)+cn∆n and θ(n)−
RL. Then, we run two POMDP episodes where
and ∆n
the defender takes actions according to the two perturbed
threshold vectors (Eq. 26). We then use the obtained episode
cn∆n) to estimate
the gradient in Eq. 25 using the Simultaneous Perturbation
Stochastic Approximation (SPSA) gradient estimator [63],
[64]:
(cid:16) ˆ
∇
where i
cn = c
eters.

is the component index of the gradient,
nλ is the perturbation size and c and λ are hyperparam-

(cid:17)
θ(n)J(θ(n))

ˆJ(θ(n) + cn∆n)

−
2cn(∆n)i

ˆJ((n)−

1, . . . , L

cn∆n)

(27)

∈ {

=

}

i

The perturbation vector ∆n is deﬁned as:

(∆n)i =






+1

1

−

with probability

with probability

1
2
1
2

(28)

Next, we use the estimated gradient and the stochastic approx-
imation algorithm [52] to update the vector of thresholds to
maximize J(θ) (Eq. 20):

θ(n+1) = θ(n) + an ˆ
∇

θ(n) J(θ(n))

(29)

is the step size and A and (cid:15) are

where an =
hyperparameters.

a
(n+A)(cid:15)

This process of running two episodes and updating the
threshold vector continues until it has sufﬁciently converged.
The described algorithm, T-SPSA, converges to a local max-
imum of J(θ) with probability one under standard conditions
[63]. For this reason, we run the algorithm several times with
different initial conditions. We list the pseudocode of T-SPSA
in Appendix D and give its hyperparameters in Appendix B.
Our Python implementation of T-SPSA is available at: [65].

7

1
2
3

HTTP, SSH, SNMP, ICMP N2, N3, N10, N12
IRC, PostgreSQL, SNMP
FTP, DNS, Telnet

N31, N13, N14, N15, N16
N10, N22, N4

TABLE 1: Emulated client population; each client interacts
with application servers using a set of network functions.

L − lt

Action

Command in the Emulation

0
1
2

Revoke user certiﬁcate
Blacklist IPs
Block gateway

openssl ca -revoke <certificate>
iptables -A INPUT -s <ip> -j DROP
iptables -A INPUT -i eth0 -j DROP

TABLE 2: Defender stop commands in the emulation.

V. EMULATING THE TARGET INFRASTRUCTURE TO
INSTANTIATE THE SIMULATION AND TO EVALUATE THE
LEARNED POLICIES

To simulate episodes of the POMDP and to compute the
belief state we must know the distributions of alerts and login
attempts conditioned on the system state. We estimate these
distributions using measurements from the emulation system
shown in Fig. 2. Moreover, to evaluate the performance of
policies learned in the simulation system, we run episodes
in the emulation system by executing actions of an emulated
attacker and having the defender execute stop actions at times
given by the learned policies.

A. Emulating the Target Infrastructure

The emulation system executes on a cluster of machines that
runs a virtualization layer provided by Docker [66] containers
and virtual links. It implements network isolation and trafﬁc
shaping on the containers using network namespaces and the
NetEm module in the Linux kernel [67]. Resource constraints
of the containers, e.g. CPU and memory constraints, are
enforced using cgroups.

The conﬁguration of the emulated infrastructure is given by
the topology in Fig. 1 and the conﬁguration in Appendix C.
The system emulates the clients, the attacker, the defender, as
well as 31 physical components of the target infrastructure
(e.g application servers and the gateway). Physical entities
are emulated and software functions are executed in Docker
containers of the emulation system. The software functions
replicate important components of the target infrastructure,
such as, web servers, databases, and an IDS.

We emulate internal connections between servers in the
infrastructure as full-duplex loss-less connections with bit ca-
pacities of 1000 Mbit/s in both directions and emulate external
connections between the gateway and the client population and
the attacker as full-duplex connections with bit capacities of
100 Mbit/s with 0.1% packet loss in normal operation and
random bursts of 1% packet loss.

The client population is emulated by three Docker contain-
ers that interact with the application servers through functions
and protocols listed in Table 1.

The emulation evolves in time-steps. During each step, the
defender and the attacker can perform one action each. The

Time-steps t

NOVICEATTACKER

EXPERIENCEDATTACKER

EXPERTATTACKER

1-It ∼ Ge(0.01)
It + 1-It + 6

It + 7-It + 10

It + 11-It + 14

It + 15-It + 16
It + 17-It + 19

(Intrusion has not started)
RECON1, brute-force attacks (SSH,Telnet,FTP)
on N2, N4, N10, login(N2, N4, N10),
backdoor(N2, N4, N10)
RECON1, CVE-2014-6271 on N17,
login(N17), backdoor(N17)
SSH brute-force attack on N12, login(N12)
CVE-2010-0426 exploit on N12, RECON1

(Intrusion has not started)
RECON2, CVE-2017-7494 exploit on N4,
brute-force attack (SSH) on N2, login(N2, N4),
backdoor(N2, N4), RECON2
CVE-2014-6271 on N17, login(N17)
backdoor(N17), SSH brute-force attack on N12
login(N12), CVE-2010-0426 exploit on N12,
RECON2, SQL Injection on N18
login(N18), backdoor(N18)
RECON2, CVE-2015-1427 on N25, login(N25)

(Intrusion has not started)
RECON3, CVE-2017-7494 exploit on N4,
login(N4), backdoor(N4)
RECON3, SQL Injection on N18
login(N18), backdoor(N18),
RECON3, CVE-2015-1427 on N25
login(N25), backdoor(N25),
RECON3, CVE-2017-7494 exploit on N27
login(N27), backdoor(N27)

TABLE 3: Attacker actions in the emulation.

Attacker

NOVICEATTACKER
EXPERIENCEDATTACKER
EXPERTATTACKER

L

1
2
3

Reconnaissance

TCP/UDP scan
ICMP ping scan
ICMP ping scan

TABLE 4: Number of stops required to prevent the attacker
L and reconnaissance commands of the attacker proﬁles.

defender executes either a continue action or a stop action.
The continue action has no effect on the progression of the
emulation but the stop action has. We have implemented L = 3
stop actions which are listed in Table 2. The ﬁrst stop revokes
all user certiﬁcates and recovers user accounts compromised
by the attacker. The second and third stops update the ﬁrewall
conﬁguration of the gateway. Speciﬁcally, the second stop
adds a rule to the ﬁrewall that drops incoming trafﬁc from
IP addresses that have been ﬂagged by the IDS and the third
stop blocks all incoming trafﬁc.

We have implemented three attacker proﬁles: NOVICEAT-
TACKER, EXPERIENCEDATTACKER, and EXPERTATTACKER,
all of which execute the sequence of actions listed in Table 3,
where It is the start time of the intrusion. The actions consist
of reconnaissance commands and exploits. During each time-
step, one action is executed. The three attackers differ in the
reconnaissance command that they use and the number of
stops L required to prevent the attack (see Table 4).

NOVICEATTACKER uses brute-force attacks to exploit pass-
word vulnerabilities (e.g. SSH dictionary attacks) and uses
a TCP/UDP port scan for reconnaissance. The attack is pre-
vented if the defender takes a stop action and revokes the user
certiﬁcates.

EXPERIENCEDATTACKER uses a ping scan for reconnais-
sance and performs both brute-force attacks and more sophis-
ticated attacks, such as a command injection attack (e.g. CVE-
2014-6271). The attack is prevented if the defender takes two
stop actions and blacklists IP addresses that have been ﬂagged
by the IDS in addition to revoking the user certiﬁcates.

Lastly, EXPERTATTACKER only targets vulnerabilities that
can be exploited without brute-force methods and thus gen-
erates less network trafﬁc, for example remote execution
vulnerabilities, such as, CVE-2017-7494. The attacker uses a
ping scan for reconnaissance like EXPERIENCEDATTACKER.
The attack is prevented if the defender executes three stop
actions and blocks the gateway.

Since the ping-scan generates fewer IDS alerts than the

Fig. 7: Empirical distributions of severe IDS alerts ∆x (top
row), warning IDS alerts ∆y (middle row), and login attempts
∆z (bottom row) generated during time-steps of intrusions
by different attackers as well as during time-steps when no
intrusion occurs.

TCP/UDP port scan, the reconnaissance actions of EXPERI-
ENCEDATTACKER and EXPERTATTACKER are harder to detect
than those of NOVICEATTACKER.

B. Estimating the Distributions of Alerts and Login Attempts

In this section, we describe how we collect data from the
emulation system and estimate the distributions of alerts and
login attempts.

1) At the end of every time-step, the emulation system
collects the metrics ∆x, ∆y, ∆z, which contain the alerts
and login attempts that occurred during the time-step. For the
evaluation reported in this paper we collected measurements
from 21000 time-steps of 30 seconds each.

2) From the collected measurements, we compute the em-
pirical distribution ˆfXY Z as estimate of the corresponding
distribution fXY Z in the target infrastructure. For each state
st, we obtain the conditional distribution ˆfXY Z|st.

Fig. 7 shows some of the empirical distributions. The
distributions related to EXPERIENCEDATTACKER are omitted
for better readability. The estimated distributions from EXPER-
TATTACKER and EXPERIENCEDATTACKER mostly overlap
with the distributions obtained when no intrusion occurs.
However, a clear difference between the distributions obtained

8

010020030040050060010−3ˆfXYZ(∆x)#SevereIDSAlerts∆x05010015020025030010−310−1ˆfXYZ(∆y)#WarningIDSAlerts∆y02040608010010−2ˆfXYZ(∆z)#LoginAttempts∆zNoviceExpertNointrusionduring an intrusion of NOVICEATTACKER and the distribu-
tions when no intrusion occurs can be observed. From these
empirical distributions, we note that the assumption that the
observation distribution is TP2 in Theorem 1.C is reasonable.

simulation system to learn the policies; and (2), evaluation
of the learned policies through running POMDP episodes in
the emulation system. This section describes our evaluation
results.

C. Simulating an Episode of the POMDP

During a simulation of the POMDP, the system state evolves
according to the dynamics described in Section IV, and the
observations evolve according to the estimated distribution
ˆfXY Z. In the initial state, no intrusion occurs. During an
episode, an intrusion normally occurs at a random start time.
It is also possible that the defender performs L stops before
the intrusion would start, in which case no intrusion starts.

·|

∼

−

πθ,l(

A simulated episode evolves as follows. The episode starts
in state s1 = 0 and l1 = L. During each time-step, the
simulation system samples an action from the defender policy
bt). If the action is stop (at = 1) and lt = 1,
at
the episode ends. Otherwise, the number of remaining stop
actions is updated: lt+1 = lt
at. Further, if an intrusion
is in progress, the system executes an attacker action fol-
lowing Table 3. It then updates the state st
st+1 and
samples ∆xt+1, ∆yt+1, ∆zt+1 from the empirical distribution
ˆfXY Z|st+1. (The activities of the clients are not simulated but
are captured by ˆfXY Z.) The simulation then computes the
belief bt+1 using Eq. 3 and computes the defender reward rt+1
using Eqs. 17-19. (Note that the exact reward can be computed
during training and evaluation of policies but not when the
policies are deployed in the target infrastructure as it depends
on the hidden state.) The sequence of time-steps continues
until the defender performs the ﬁnal stop, after which the
episode ends. If the attacker sequence in Table 3 is completed
before the defender performs the ﬁnal stop, the sequence is
restarted.

→

D. Emulating an Episode of the POMDP

Just like a simulated episode, an emulated episode starts
with the same initial conditions, evolves in discrete time-
steps, and experiences an intrusion event at a random time.
However, an episode in the emulation system differs from
an episode in the simulation system in the following ways.
First, attacker and defender actions in the emulation system
include computing and networking functions with real side-
effects in the emulation environment (see Table 2 and Table
3). Further, the defender observations in the emulation system
are not sampled but are obtained through reading log ﬁles and
metrics of the emulated infrastructure. Lastly, the emulated
client population performs requests to the emulated application
servers just like on a real infrastructure (see Section V-A).
Due to these differences, running an episode in the emulation
system takes much longer time than running a similar episode
in the simulation system.

VI. LEARNING INTRUSION PREVENTION POLICIES FOR
THE TARGET INFRASTRUCTURE

Our approach for ﬁnding effective defender policies in-
cludes (1) extensive simulation of POMDP episodes in the

9

The environment for training policies and running simu-
lations is a Tesla P100 GPU. The hyperparameters for the
training algorithm are listed in Appendix B. The emulated
infrastructure is deployed on a server with a 24-core Intel
Xeon Gold 2.10GHz CPU and 768 GB RAM. We have
made available the code of our simulation system, as well
as the measurement traces used to estimate the observation
distributions of the POMDP, which can be used by others to
extend and validate our results [65].

A. Evaluation Process

We train three defender policies against NOVICEAT-
TACKER, EXPERIENCEDATTACKER and EXPERTATTACKER
until convergence. For each attacker we run 10(cid:48)000 training
episodes to estimate an optimal defender policy using the
method described in Section IV-C. After each episode we
evaluate the current defender policy.

To evaluate a defender policy, we run evaluation episodes
and compute various performance metrics. Speciﬁcally, we
run 500 evaluation episodes in the simulation system and 5
evaluation episodes in the emulation system.

The 10(cid:48)000 training episodes and the evaluation described
above constitute one training run. We run ﬁve training runs
with different random seeds. A single training run takes about
4 hours of processing time on a P100 GPU to perform the
simulations and the policy-training, as well as around 12 hours
for evaluating the policies in the emulation system.

≥

We compare the policies learned through T-SPSA with
three baseline policies. The ﬁrst baseline prescribes the stop
action whenever an IDS alert occurs, i.e., whenever (∆x +
1. The second baseline is obtained by conﬁguring
∆y)
the Snort IDS as an Intrusion Prevention System (IPS) which
drops network trafﬁc following its internal recommendation
system (see Appendix C for the Snort conﬁguration). To
calculate the reward, we deﬁne 100 dropped IP packets of
the Snort IPS to be a stop action of the defender. Lastly, the
third baseline is an ideal policy which presumes knowledge
of the exact intrusion time and performs all stop actions at
exactly that time.

We evaluate our algorithm, T-SPSA, by comparing it
with three baseline algorithms: Proximal Policy Optimization
(PPO) [50], Heuristic Search Value Iteration (HSVI) [68],
and Shiryaev’s algorithm [69]. PPO is a state-of-the-art re-
learning algorithm, HSVI is a state-of-the-art
inforcement
dynamic programming algorithm for POMDPs, and Shiryaev’s
algorithm is an optimal algorithm for change detection. The
main difference between T-SPSA and the ﬁrst two baselines
(PPO and HSVI) is that T-SPSA exploits the threshold
structure expressed in Theorem 1 and the main difference
in comparison with Shiryaev’s algorithm is that T-SPSA
learns L thresholds whereas Shiryaev’s algorithm uses a single
predeﬁned threshold. We set this threshold to 0.75 based on a
hyperparameter search (see Appendix B).

Fig. 8: Learning curves obtained during training of T-SPSA; red curves show simulation results and blue curves show
emulation results; the purple, orange, and black curves relate to baseline policies; the rows from top to bottom relate to:
NOVICEATTACKER, EXPERIENCEDATTACKER, and EXPERTATTACKER; the columns from left to right show performance
metrics: episodic reward, episode length, empirical prevention probability, empirical early stopping probability, and the time
between the start of intrusion and the Lth stop action; the curves show the mean and 95% conﬁdence interval for ﬁve training
runs with different random seeds.

Fig. 9: Comparison between T-SPSA and three baseline algorithms; all curves show simulation results; red curves relate to
T-SPSA; blue curves relate to PPO; orange curves relate to HSVI; purple curves relate to Shiryaev’s algorithm with threshold
α = 0.75; the columns from left to right relate to: NOVICEATTACKER, EXPERIENCEDATTACKER, and EXPERTATTACKER;
all curves show the mean and 95% conﬁdence interval for ﬁve training runs with different random seeds.

B. Learning Intrusion Prevention Policies

that the learned policies have converged as well.

Fig. 8 shows the performance of the learned policies against
the three attacker types. The red curves represent the results
from the simulation system and the blue curves show the
results from the emulation system. The purple and orange
curves give the performance of the Snort IPS baseline and
the baseline policy that mandates a stop action whenever an
IDS alert occurs, respectively. The dashed black curves give
the performance of the baseline policy that assumes knowledge
of the exact intrusion time.

An analysis of the graphs in Fig. 8 leads us to the following
conclusions. We observe that the learning curves converge
quickly to constant mean values for all attackers and across
all investigated performance metrics. From this we conclude

Second, we observe that the converged values of the learn-
ing curves are close to the dashed black curves, which give an
upper bound to any optimal policy. In addition, we see that the
empirical probability of preventing an intrusion of the learned
policies is close to 1 (middle column of Fig. 8) and that the
empirical probability of stopping before the intrusion starts is
close to 0 (second rightmost column of Fig. 8). This suggests
that the learned policies are close to optimal. We also observe
that all learned policies do signiﬁcantly better than the Snort
IPS baseline and the baseline that stops whenever an IDS alert
occurs (leftmost column in Fig. 8).

Third, although the learned policies, as expected, perform
better in the simulation system than in the emulation system,
we are encouraged by the fact that the curves of the emulation

10

050100NoviceRewardperepisode050100Episodelength(steps)0.00.51.0P[intrusioninterrupted]0.00.51.01.5P[earlystopping]510Durationofintrusion−50050100experienced0501001500.00.51.00.00.51.01.505100204060trainingtime(min)−50050100expert0204060trainingtime(min)0501001500204060trainingtime(min)0.00.51.00204060trainingtime(min)0.00.51.01.52.00204060trainingtime(min)05101520πθ,lsimulationπθ,lemulation(∆x+∆y)≥1baselineSnortIPSupperbound0102030405060trainingtime(min)050100RewardperepisodeagainstNovice0102030405060trainingtime(min)RewardperepisodeagainstExperienced0102030405060trainingtime(min)RewardperepisodeagainstExpertPPOT-SPSAShiryaev’sAlgorithm(α=0.75)HSVIupperboundsystem are close to those of the simulation system.

We also note from Fig. 8 that

the learned policies do
best against NOVICEATTACKER and less well against EX-
PERIENCEDATTACKER and EXPERTATTACKER. For instance,
the learned policies against EXPERIENCEDATTACKER and
EXPERTATTACKER are more likely to stop before an intrusion
has started (second rightmost column of Fig. 8). This indicates
that NOVICEATTACKER is easier to detect for the defender
as its actions create more IDS alerts than those of the other
attackers, as pointed out in Section V-A.

Lastly, Fig. 9 shows a comparison between our reinforce-
ment learning algorithm (T-SPSA) and the three baseline
algorithms in the simulation system. We observe in Fig. 9 that
both T-SPSA and PPO converge to close approximations of an
optimal policy within an hour of training whereas HSVI does
not converge within the measured time. The slow convergence
of HSVI manifests the intractability of using dynamic pro-
gramming to compute policies in large POMDPs [39]. We also
see in Fig. 9 that T-SPSA converges signiﬁcantly faster than
PPO. This is expected since T-SPSA considers a smaller space
of policies than PPO. Finally, we also note in Fig. 9 that T-
SPSA outperforms Shiryaev’s algorithm, which demonstrates
the beneﬁt of using L thresholds instead of a single threshold.

VII. RELATED WORK

Traditional approaches to intrusion prevention use packet
inspection and static rules for detection of intrusions and selec-
tion of response actions [70], [71], [1]. Their main drawback
lies in the need for domain experts to conﬁgure the rule sets.
As a consequence, much effort has been devoted to developing
methods for ﬁnding security policies in an automatic way. This
research uses concepts and methods from various areas, most
notably from anomaly detection (see example [72]), change-
point detection (see example [37]), statistical learning (see
examples [73], [74], [75]), control theory (see survey [6]),
game theory (see textbooks [10], [76], [77], [78]), artiﬁcial
intelligence (see survey [79]), dynamic programming (see
example [5]), reinforcement learning (see surveys [80], [81]),
evolutionary methods (see example [7]), and attack graphs (see
example [82]).

While the research reported in this paper is informed
by all the above works, we limit the following discussion
to prior work that centers around ﬁnding security policies
through reinforcement learning, a topic area that has grown
considerably in recent years. Three seminal papers: [83], [84],
and [85], published in 2000, 2005, and 2008, respectively,
analyze intrusion prevention use cases and evaluate traditional
reinforcement learning algorithms for this task. These papers
have inspired much follow-up research, e.g. on studying deep
reinforcement
learning algorithms for intrusion prevention
[12], [13], [25] and studying new use cases, such as defense
against jamming attacks [86], mitigation of denial of service
attacks [87], [88], defense against advanced persistent threats
[89], placement of honeypots [90], botnet detection [91], [92],
detection of ﬂip attacks [93], detection of network trafﬁc
anomalies [94], greybox fuzzing [95], and defense against
topology attacks [96].

Among the recent works that use reinforcement learning
to ﬁnd security policies, many focus on intrusion prevention
use cases similar to the one we discuss in this paper [12],
[13], [18], [19], [20], [38], [21], [22], [40], [24], [25], [26],
[23], [27], [28], [29]. These works use a variety of models,
including MDPs [20], [22], [24], [25], [29], Markov games
[18], [12], [23], and POMDPs [13], [26], [38], as well as var-
ious reinforcement learning algorithms, including Q-learning
[18], [20], [22], SARSA [38], PPO [12], [13], hierarchical
reinforcement learning [24], DQN [25], Thompson sampling
[26], MuZero [23], NFQ [27], DDQN [29], and DDPG [28].
This paper differs from the works referenced above in
two main ways. First, we formulate the intrusion prevention
problem as a multiple stopping problem. The other works
formulate the problem as solving a general MDP, POMDP,
or Markov game. The advantage of our approach is that we
obtain structural properties of optimal policies, which have
practical beneﬁts (see Section IV-B).

Problem formulations based on optimal stopping theory can
be found in prior research on change detection [69], [97],
[38], [37], [93], [13]. Compared to these papers, our approach
is more general by allowing multiple stop actions within
an episode. Another difference is that we model intrusion
prevention rather than intrusion detection. Further, compared
with traditional change detection algorithms, e.g. CUSUM [97]
and Shiryaev’s algorithm [69], our algorithm learns thresholds
and does not assume them to be preconﬁgured.

Second, our solution method to ﬁnd effective policies for
intrusion prevention includes using an emulation system in
addition to a simulation system. The advantage of our method
compared to the simulation-only approaches [12], [13], [18],
[19], [20], [38], [21], [22], [40], [24], [25], [26] is that the
parameters of our simulation system are determined by mea-
surements from an emulation system instead of being chosen
by a human expert. Further, the learned policies are evaluated
in the emulation system, not in the simulation system. As a
consequence, the evaluation results give higher conﬁdence of
the obtained policies’ performance in the target infrastructure
than what simulation results would provide.

Some prior works on reinforcement learning for intrusion
prevention that make use of emulation are: [23], [27], [28],
and [29]. They emulate software-deﬁned networks based on
Mininet [98]. The main differences between these efforts and
the work described in this paper are: (1) we develop our
own emulation system which allows for experiments with a
large variety of exploits; (2) we focus on a different intrusion
prevention use case; (3) we do not assume that the defender
has perfect observability; and (4), we use an underlying theo-
retical framework to formalize the use case, derive structural
properties of optimal policies, and test these properties in an
emulation system.

Finally, [99] and [100] describe ongoing efforts in building
emulation platforms for reinforcement learning, which resem-
ble our emulation system. In contrast to these papers, our
emulation system has been built to investigate the speciﬁc use
case of intrusion prevention and forms an integral part of our
general solution method (see Fig. 2).

11

VIII. CONCLUSION AND FUTURE WORK

In this paper, we proposed a novel formulation of the
intrusion prevention problem based on the theory of optimal
stopping. This formulation allowed us to derive that a threshold
policy based on infrastructure metrics is optimal, which has
several practical beneﬁts.

To ﬁnd and evaluate policies, we used a reinforcement
learning method that includes a simulation system and an
emulation system. In contrast to a simulation-only approach,
our method produces policies that can be executed in a target
infrastructure.

Through extensive evaluations, we showed that our ap-
proach can produce effective defender policies for a practical
conﬁguration of an IT infrastructure (Figs. 8-9). We also
demonstrated that our reinforcement learning algorithm (T-
SPSA), which takes advantage of the threshold structure
(Theorem 1), outperforms state-of-the-art algorithms on our
use case.

We make assumptions in this paper that limit the practical
applicability of the results: the attacker follows a static policy,
and the defender learns only the times of taking defensive
actions but not the types of actions. Therefore, the question
arises whether our approach can be extended so that (1) the
attacker can pursue a wide range of realistic policies and
(2) the defender learns optimal policies that express not only
when defensive actions needs to be taken but also the speciﬁc
measure to be executed.

Addressing these points is part of our research agenda.
The dynamic attacker can be studied using a game-theoretic
extension of the introduced framework. The theory tells us that
an optimal solution can be found through self-play in a similar
manner as described in the paper, but further work is needed
to show that such a solution is feasible in practice. Scenarios
involving several attackers can also be studied in this context.
We also plan to extend the defender model to include the
selection of defensive actions. One possible approach is to
learn two orthogonal policies: a policy that decides when to
take a defensive action and another policy that decides which
action to take.

IX. ACKNOWLEDGMENTS

This research has been supported in part by the Swedish
armed forces and was conducted at KTH Center for Cyber
Defense and Information Security (CDIS). The authors would
like to thank Pontus Johnson for his useful
to this
research and Vikram Krishnamurthy for helpful discussions.
The authors are also grateful to Forough Shahab Samani and
Xiaoxuan Wang for their constructive comments on a draft of
this paper.

input

APPENDIX A
PROOF OF THEOREM 1.

Given the POMDP introduced in Section IV-A, let L denote
the number of stop actions, fXY Z the observation distribution,
= [0, 1] the belief space (see Section IV-B), b(1) the belief

B
state, Sl the stopping set, and Cl the continuation set.

12

l and π∗

We use the value iteration algorithm to establish structural
properties of V ∗
l [16], [17]. Let V k
l , de-
note the value function, the stopping set, and the continuation
set at iteration k of the value iteration algorithm, respectively.
Let V 0
[0, 1] and l
. Then,
l
}
∈ {
l = Cl
l = Sl, and limk→∞ C k
limk→∞ V k
[16], [17].

(cid:0)b(1)(cid:1) = 0 for b(1)

l , limk→∞ S k

l , and C k

l = V ∗

l , S k

1, . . . , L

∈

The main idea behind the proof of Theorem 1 is to show

that the stopping sets Sl have the form Sl = [α∗
and that α∗
α∗
state the following four lemmas.

l+1 for l

1, . . . , L

l ≥

∈ {

⊆ B
. Towards this goal, we

l , 1]

}

Lemma 1. During a POMDP episode, an optimal policy π∗
L
prescribes L stop actions.

Proof. The proof follows directly from the deﬁnition of the
transition probabilities (see Eqs. 11-14) and the reward func-
tion (see Eqs. 17-19).

Lemma 2. S1 is a convex subset of

.

B

Proof. The proof can be found in [13, pp. 10, Lemma 3] and
in [17, pp. 258, Theorem 12.2.1].

Lemma 3.
ing in b(1) for lt

P

at
st,st+1,lt

is TP2 and
1, . . . , L

.
}
Proof. The transition probabilities (see Section IV-A) are
given by the following two row-stochastic matrices:

∈ {

S
b(1),lt −R
R

C
b(1),lt

is increas-





0

1

∅

0
0.99
0
0

1
0.01
1
0



,

∅
0
0
1





0

1

∅

0
0
0
0

1
0
0
0



,

∅
1
1
1

(30)

P

(cid:1)2
2

The left matrix corresponds to the transition probabilities
when at = C, or, when at = S and lt > 1. The right
matrix represents the transition probabilities when at = S
at
and lt = 1. To show that
is TP2, it is sufﬁcient
st,st+1,lt
to show that all (cid:0)3
second order minors of both matrices
are non-negative. The second-order minors of the ﬁrst matrix
are M1,2 = M1,3 = M2,3 = M3,1 = M3,2 = 0, M1,1 = 1,
M2,1 = 0.01, M2,2 = M3,3 = 0.99, where Mi,j denotes the
determinant of the submatrix formed by deleting the ith row
and jth column. For the second matrix all second order minors
are zero. Hence,
is TP2.
P
C
b(1),lt

is expanded to:

at
st,st+1,lt

S
b(1),lt − R

R

S
b(1),lt − R

R

C
b(1),lt

= b(1)

(cid:18) 50
4lt

(cid:19)

+ 10/L

1

−

(31)

which is increasing in b(1).

Lemma 4. Given two beliefs b(cid:48)(1)
vations o
following holds for any a

at
st,st+1,lt

¯o, if

≥

P

≥

b(1) and two obser-
and fXY Z|s are TP2, then the
:
, k
}

1, . . . , L

, and lt

∈ O

∈ {

∈ A

a (1)

1) b(cid:48),o
bo
a(1)
≥
2) P[o
b(cid:48), a]
k
≥
|
3) bo
b¯o
a(1)
a(1)
a (1) and bo
where b(cid:48),o
after taking action a

≥

≥

P[o

k

b, a]
|

≥

a(1) denote the beliefs updated with Eq. 3

and observing o

.

∈ O

∈ A

Proof. The proof is published in [17, Theorem 10.3.1, pp.
225,238]. (Remark: in the referenced proof, the monotone
in our case
likelihood ratio (MLR) order is considered;
= 2, hence the MLR order reduces to the natural order
b(1).)

|S \ ∅|
b(cid:48)(1)

≥

Proof of Theorem 1.A. The proof has originally been pub-
lished in [34, Propositions 4.5-4.8, pp. 437-441]. It is also
available in a more accessible form in [33, Theorem 1.C,
Theorem 8, pp. 389-397]. We give our own version of the
proof since the referenced proofs assume zero reward for the
continue action and assume that rewards are independent of l.
Sl−1, the Bellman equation and the fact that
(see Eq.

If b(1)

and o

b(1) for all a

=

∈

P[o
b] = Po
a, b] = P[o
|
|
15) implies that:

∈ A

∅

S
b(1),l−1 − R
R
(cid:88)
V ∗
l−2

C
b(1),l−1+
(cid:0)bo(1)(cid:1)

Po

b(1)

(cid:16)

o∈O

(32)

(cid:0)bo(1)(cid:1)(cid:17)

V ∗
l−1

−

0

≥

∈

V k
l

(cid:0)b(1)(cid:1)

l (b(1)) is non-decreasing in l for all k

We show that b(1)
(cid:0)b(1)(cid:1) =
Let W k
l
To show that b(1)
show that W k
proceed to show this statement by mathematical induction.

Sl follows from the above inequality.
∈
(cid:0)b(1)(cid:1).
b(1),l +V k
C
S
l−1
b(1),l −R
R
−
Sl, it is sufﬁcient to
Sl−1 =
b(1)
⇒
∈
0. We

For iteration k = 0 of value iteration, W 0
l
(cid:0)b(1)(cid:1)

≥
(cid:0)b(1)(cid:1) =
(cid:0)b(1)(cid:1) = 0, which is trivially non-decreasing
V 0
l
(cid:0)b(1)(cid:1) is non-decreasing
in l. Assume by induction that W k−1
(cid:0)b(1)(cid:1)
in l for iterations k
is non-decreasing in l also for iteration k, we show that
(cid:0)b(1)(cid:1)
W k
l
There are four cases to consider:
1) If b(1)

2, . . . , 1. To show that W k
l

−
(cid:0)b(1)(cid:1)

V 0
l−1

W k

1, k

S k

l−1

−

−

−

≥

0.

l−2, then:

l

S k
l ∩
(cid:0)b(1)(cid:1)
(cid:88)
Po

∈
W k
l

=

−

b(1)

S k
(cid:0)b(1)(cid:1)
(cid:0)bo(1)(cid:1)

l−1 ∩
W k
l−1
(cid:16)
W k−1
l−1

o∈O

(33)

W k−1
l−2

(cid:0)bo(1)(cid:1)(cid:17)

−

2) If b(1)

S k

l ∩

which is non-negative by the induction assumption.
C k
(cid:0)b(1)(cid:1) =
(cid:0)bo(1)(cid:1)

S
b(1),l−1 − R
R
(cid:0)bo(1)(cid:1)(cid:17)
V k−1
l−1

S k
∈
(cid:0)b(1)(cid:1)
(cid:88)
Po

l−1
V k−1
l−2

W k
−
(cid:16)

l−2, then:

l−1 ∩

W k
l

b(1)

+

C
b(1),l−1 (34)

−

o∈O

3) If b(1)

which is non-negative because b(1)
by Eq. 10).

l−2, then:

S k
∈
(cid:0)b(1)(cid:1)
(cid:88)
Po

b(1)

W k
l

+

o∈O

C k

l ∩

l−1 ∩

W k
−
(cid:16)

l−1
V k−1
l−1

C k
(cid:0)b(1)(cid:1) =
(cid:0)bo(1)(cid:1)

−

C
b(1),l−1 − R
R
(cid:0)bo(1)(cid:1)(cid:17)
V k−1
l−2

S
b(1),l−1 (35)

which is non-negative because b(1)
by Eq. 10).

C k

l−1 (it is implied

∈

4) If b(1)

C k

C k
l−1 ∩
l ∩
∈
(cid:0)b(1)(cid:1)
W k
W k
l
l−1
−
(cid:16)
(cid:88)
Po
W k−1
l

C k
l−2, then:
(cid:0)b(1)(cid:1) =
(cid:0)bo(1)(cid:1)

b(1)

o∈O

W k−1
l−1

−

(cid:0)bo(1)(cid:1)(cid:17)

(36)

(37)

13

which is non-negative by the induction assumption.

The other cases, e.g. b(1)
discarded due to the induction assumption. Hence, W k
is non-decreasing in l for all k

l−2, can be
l (b(1))

≥
Since the left-hand side of Eq. 32 is non-decreasing in l it

l−1 ∩

l ∩

0.

∈

C k

C k

S k

follows that if Eq. 32 holds, i.e. if b(1)
Sl.

∈

Sl−1, then b(1)

∈

Proof of Theorem 1.B. The proof follows the chain of reason-
ing in [17, Corollary 12.2.2, pp. 258].

Using Lemma 2, we know that the stopping set S1 is a
= [0, 1]. That is, it has the form [α∗, β∗]

convex subset of
where 0

α∗

B
β∗

≤

≤

≤

If b(1) = 1, the Bellman equation (Eq. 10) states that:

1. We show that β∗ = 1.

(cid:34)

π∗
1(1)

arg max
{S,C}

∈

50 + V ∗
)
0 (
,
∅
(cid:123)(cid:122)
(cid:125)
(cid:124)
a=S

9 +

(cid:88)

Z

o∈O

(o, 1, C)V ∗
1

(cid:0)bo

C(1)(cid:1)

−
(cid:124)

(cid:35)

(38)

(cid:125)

(cid:123)(cid:122)
a=C
As L = 1, it follows from Lemma 1 that an optimal policy
prescribes one stop action during a POMDP episode and that
the intrusion is prevented after the ﬁrst stop. Hence, V ∗
) =
0 (
∅
·
∅,0 = 0. Moreover, since s = 1 is an absorbing state until the
R
stop, it follows from the deﬁnition of bo
C(1) = 1
50 (see Eqs. 17-19),
1 (1)
for all o
we get:

C (Eq. 3) that bo

. Thus, since V ∗

∈ O \ ∅

≤

(cid:34)

(cid:35)

π∗
1(1)

arg max
{S,C}

∈

,

50
(cid:124)(cid:123)(cid:122)(cid:125)
a=S

9 + V ∗
(cid:123)(cid:122)
a=C

1 (1)
(cid:125)

−
(cid:124)

= S

(39)

This means that β∗ = 1 and therefore S1 = [α∗, 1].

Corollary 1. If
set Sl is connected, l

P

at
st,st+1,lt

and fXY Z|s are TP2, the stopping
1, . . . , L

∈ {

.
}

Proof We adapt the proof from [33, Theorem 1.B, pp. 389-
397] to our model. In contrast to the referenced proof, our
model includes non-zero rewards for the continue action and

= 2.

Sl,

If b(1)

|S \ ∅|
P[o
a, b] = P[o
Eq. 15) implies that:

∈

b] = Po
|

|

the Bellman equation and the fact

b(1) for all a

and o

=

∈ A

that
(see

∅
(cid:0)bo(1)(cid:1)(cid:17)

0

≥
(40)
Sl for

≥
B

We show that the above inequality implies that b(cid:48)(1)
∈
b(1), which means that Sl is connected.
any b(cid:48)(1)
= [0, 1], the beliefs are totally ordered according
Since
to the standard ordering. Further, since fXY Z|s is TP2 by
is TP2 by Lemma 3, bo(1) is
assumption and
P
weakly increasing in both b(1) and o
b(cid:48), a]
k
|
Sl−1
it is sufﬁcient to show that
V ∗
l
this by mathematical induction.

≥
P[o
b, a] for any k
(Lemma 4). Thus, since
≥
Sl (Theorem 1.A) and S1 = [α∗
1, 1] (Theorem 1.B),
b(1),l + V ∗
C
l−1

−
(cid:0)b(1)(cid:1) is weakly increasing in b(1). We proceed to show

S
b(1),l − R
R

. Further, P[o

at
st,st+1,lt

(cid:0)b(1)(cid:1)

∈ O

∈ O

≥

⊆

k

|

S k

l−1 (it is implied

∈

S
b(1),l − R
R

C
b(1),l +

(cid:88)

o∈O

(cid:16)

Po

b(1)

V ∗
l−1

(cid:0)bo(1)(cid:1)

V ∗
l

−

(cid:54)
(cid:54)
−

V 0
l

(cid:0)bo(1)(cid:1)

C
S
b(1),l +
For iteration k = 0 of value iteration,
b(1),l − R
R
(cid:0)bo(1)(cid:1) =
C
S
V 0
b(1),l which is weakly
l−1
b(1),l − R
R
increasing in b(1) by Lemma 3. Assume by induction that
the expression is weakly increasing in b(1) for iterations k
1, k
assumption holds also for iteration k.

−
2, . . . , 1. We show that this implies that the induction

−

Since Sl−1

Sl (Theorem 1.A) and S1 = [α∗

1, 1]

(Theorem 1.B), there are three cases to consider

⊆

1) If b(1)

∈

Sl

Sl−1, then:

S
b(1),l − R
C
b(1),l−1 +

R

R

∩
C
b(1),l + V k
l−1
(cid:16)
(cid:88)
Po

b(1)

o∈O

(cid:0)b(1)(cid:1)
V k−1
l−2

V k
l
−
(cid:0)bo(1)(cid:1)

(cid:0)b(1)(cid:1) =
V k−1
l−1

S
b(1),l−1−
R
(cid:0)bo(1)(cid:1)(cid:17)

−

(41)

which is weakly increasing in b(1) by the induction
assumption.
Sl

Cl−1, then:

2) If b(1)

∈

∩

S
b(1),l − R
R
(cid:16)
(cid:88)
Po

b(1)

b(1),l + V k
C
l−1
V k−1
l−1

(cid:0)bo(1)(cid:1)

(cid:0)b(1)(cid:1)

V k
l
−
(cid:0)bo(1)(cid:1)(cid:17)
V k−1
l−1

= 0

(cid:0)b(1)(cid:1) = (42)

−

o∈O

which is trivially weakly increasing in b(1).

3) If b(1)

∈

Cl

S
b(1),l − R
R
C
b(1),l +

(cid:88)

R

o∈O

Cl−1, then:

∩
b(1),l + V k
C
l−1
V k−1
l−1

Po

b(1)

(cid:16)

(cid:0)b(1)(cid:1)

−
(cid:0)bo(1)(cid:1)

V k
l

−

(cid:0)b(1)(cid:1) =
V k−1
l

S
b(1),l−
R
(cid:0)bo(1)(cid:1)(cid:17)

(43)

Hyperparameters for the POMDP
γ, ∆xmax, ∆ymax, ∆zmax

Values
1, 6 · 102, 3 · 102, 102

Hyperparameters for T-SPSA
c, γ, (cid:15), A, a

Values
1, 0.101, 0.602, 100, 1

Hyperparameters for PPO
lr α, batch, # layers, # neurons, clip (cid:15)
GAE λ, ent-coef, activation

Values
10−
0.95, 10−

4, 4 · 103t, 2, 32, 0.2
4, ReLU

Hyperparameters for HSVI
(cid:15)

Hyperparameters for Shiryaev’s algorithm
α

Values
0.01

Values
0.75

TABLE 5: Hyperparameters of the POMDP and the algorithms
used for evaluation.

ID (s)

OS:Services:Exploitable Vulnerabilities

N1
N2
N4
N10
N12
N17
N18
N22
N23
N24
N25
N27
N3,N11,N5-N9
N13

16,N19

−

−

21,N26,N28

31

−

Ubuntu20:Snort(community ruleset v2.9.17.1),SSH:-
Ubuntu20:SSH,HTTP Erl-Pengine,DNS:SSH-pw
Ubuntu20:HTTP Flask,Telnet,SSH:Telnet-pw
Ubuntu20:FTP,MongoDB,SMTP,Tomcat,TS3,SSH:FTP-pw
Jessie:TS3,Tomcat,SSH:CVE-2010-0426,SSH-pw
Wheezy:Apache2,SNMP,SSH:CVE-2014-6271
Deb9.2:IRC,Apache2,SSH:SQL Injection
Jessie:PROFTPD,SSH,Apache2,SNMP:CVE-2015-3306
Jessie:Apache2,SMTP,SSH:CVE-2016-10033
Jessie:SSH:CVE-2015-5602,SSH-pw
Jessie: Elasticsearch,Apache2,SSH,SNMP:CVE-2015-1427
Jessie:Samba,NTP,SSH:CVE-2017-7494
Ubuntu20:SSH,SNMP,PostgreSQL,NTP:-
Ubuntu20:NTP, IRC, SNMP, SSH, PostgreSQL:-

which is weakly increasing in b(1) by the induction
assumption.

TABLE 6: Conﬁguration of the target infrastructure (Fig. 1).

P

S
b(1),l − R
R

Proof of Theorem 1.C. Since
= [0, 1] (see Section IV-B),
B
at
is TP2 by Lemma
fXY Z|s is TP2 by assumption,
st,st+1,lt
C
b(1),l is increasing in b(1) (Lemma 3), it
3, and
follows from Corollary 1 that Sl is a connected subset of [0, 1]
. Further, from Theorem 1.B we know that
for l
}
∈ {
Sl+1 for l
S1 = [α∗
1
}
α∗
l ≥

(Theorem 1.A), we conclude that Sl = [α∗

1, . . . , L
1, 1]. Then, because Sl

1, . . . , L
∈ {
−
l , 1] and that

l+1 for l

1, . . . , L

∈ {

.
}

α∗

⊆

−

1

APPENDIX B
HYPERPARAMETERS

The hyperparameters used for the evaluation are listed in

Table 5 and were obtained through grid search.

APPENDIX C
CONFIGURATION OF THE INFRASTRUCTURE IN FIG. 1

The conﬁguration of the target infrastructure (Fig. 1) is

available in Table 6.

5:
6:
7:
8:

9:

10:
11:
12:

APPENDIX D
THE T-SPSA ALGORITHM

Algorithm 1 contains the pseudocode of T-SPSA.

14

Algorithm 1 T-SPSA

Input

RL: the POMDP, initial L thresholds

P , θ(1) ∈

M
N : number of iterations
a, c, λ, A, (cid:15): scalar coefﬁcients
Output
θ(N +1): learned threshold vector

1: procedure T-SPSA(
M
1, . . . , N
2:
}
a
(n+A)(cid:15) , cn
3:
1, . . . , L

for n
∈ {
an
←
for i

4:

P , θ(1), N , a, c, λ, A, (cid:15))
do

c
nλ

←
}
{−

do
1, 1
}

(

)

∈ {
(∆n)i

end for
Rhigh
for i

∼
(cid:16) ˆ
∈ {
∇
end for
θ(n+1) ←
end for
return θ(N +1)

∼ U
ˆJ(θ(n) +cn∆n), Rlow
1, . . . , L
do
(cid:17)
}
θ(n)J(θ(n))

i ←

Rhigh−Rlow
2cn(∆n)i

θ(n) + an ˆ
∇

θ(n)J(θ(n))

13:
14: end procedure

ˆJ(θ(n) −

∼

cn∆n)

REFERENCES

[1] A. Fuchsberger, “Intrusion detection systems and intrusion prevention
systems,” Inf. Secur. Tech. Rep., vol. 10, no. 3, p. 134–139, Jan. 2005.
[2] K.-K. R. Choo, “The cyber threat landscape: Challenges and future
research directions,” Comput. Secur., vol. 30, no. 8, p. 719–731, 2011.
[3] E. Zouave et al., “Artiﬁcially intelligent cyberattacks,” 2020, ,Swedish

Defence Research Agency.

[4] P. Johnson, R. Lagerstr¨om, and M. Ekstedt, “A meta language for
threat modeling and attack simulations,” in Proceedings of the 13th
International Conference on Availability, Reliability and Security, ser.
ARES 2018, New York, NY, USA, 2018.

[5] M. Rasouli, E. Miehling, and D. Teneketzis, “A supervisory control
approach to dynamic cyber-security,” in Decision and Game Theory
for Security. Cham: Springer International Publishing, 2014, pp. 99–
117.

[6] E. Miehling, M. Rasouli, and D. Teneketzis, Control-Theoretic Ap-

proaches to Cyber-Security, 2019.

[7] R. Bronfman-Nadas, N. Zincir-Heywood, and J. T. Jacobs, “An artiﬁcial
improve mobile malware detectors?” in 2018
arms race: Could it
Network Trafﬁc Measurement and Analysis Conference (TMA), 2018.
[8] N. Wagner et al., “Towards automated cyber decision support: A case
study on network segmentation for security,” in IEEE Symposium Series
on Computational Intelligence (SSCI), 2016, pp. 1–10.

[9] C. Wagner et al., “Misp: The design and implementation of a collabo-
rative threat intelligence sharing platform,” in Proceedings of the 2016
ACM on Workshop on Information Sharing and Collaborative Security.
[10] T. Alpcan and T. Basar, Network Security: A Decision and Game-
Theoretic Approach, 1st ed. USA: Cambridge University Press, 2010.
[11] S. Sarıtas¸, E. Shereen, H. Sandberg, and G. D´an, “Adversarial attacks
on continuous authentication security: A dynamic game approach,” in
Decision and Game Theory for Security, Cham, 2019, pp. 439–458.

[12] K. Hammar and R. Stadler, “Finding effective security strategies
through reinforcement learning and Self-Play,” in International Con-
ference on Network and Service Management (CNSM 2020), Izmir,
Turkey, 2020.

[13] ——, “Learning intrusion prevention policies through optimal stop-
ping,” in International Conference on Network and Service Manage-
ment (CNSM 2021), Izmir, Turkey, 2021, https://arxiv.org/pdf/2106.
07160.pdf.

[14] R. Bellman, “A markovian decision process,” Journal of Mathematics

and Mechanics, vol. 6, no. 5, pp. 679–684, 1957.

[15] A. Wald, Sequential Analysis. Wiley and Sons, New York, 1947.
[16] M. L. Puterman, Markov Decision Processes: Discrete Stochastic
Dynamic Programming, 1st ed. USA: John Wiley and Sons, Inc.,
1994.

[17] V. Krishnamurthy, Partially Observed Markov Decision Processes:
From Filtering to Controlled Sensing. Cambridge University Press,
2016.

[18] R. Elderman et al., “Adversarial reinforcement learning in a cyber

security simulation,” in ICAART, 2017.

[19] J. Schwartz, H. Kurniawati, and E. El-Mahassni, “Pomdp +
information-decay: Incorporating defender’s behaviour in autonomous
penetration testing,” Proceedings of the International Conference on
Automated Planning and Scheduling, vol. 30, no. 1, pp. 235–243, Jun.
2020.

[20] F. M. Zennaro and L. Erdodi, “Modeling penetration testing with
reinforcement learning using capture-the-ﬂag challenges and tabular
q-learning,” CoRR, 2020, https://arxiv.org/abs/2005.12632.

[21] W. Blum, “Gamifying machine learning for stronger security and
ai models,” 2021, https://www.microsoft.com/security/blog/2021/04/
08/gamifying-machine-learning-for-stronger-security-and-ai-models/.
[22] A. Ridley, “Machine learning for autonomous cyber defense,” 2018,

the Next Wave, Vol 22, No.1 2018.

[23] J. Gabirondo-L´opez, J. Ega˜na, J. Miguel-Alonso, and R. Orduna Urru-
tia, “Towards autonomous defense of sdn networks using muzero based
intelligent agents,” IEEE Access, vol. 9, pp. 107 184–107 199, 2021.

[24] K. Tran et al., “Deep hierarchical reinforcement agents for automated

penetration testing,” 2021, https://arxiv.org/abs/2109.06449.

[25] R. Gangupantulu et al., “Using cyber terrain in reinforcement learning
for penetration testing,” 2021, https://arxiv.org/abs/2108.07124.
[26] Z. Hu, M. Zhu, and P. Liu, “Adaptive cyber defense against multi-stage
attacks using learning-based pomdp,” ACM Trans. Priv. Secur., 2020.
[27] I. Akbari, E. Tahoun, M. A. Salahuddin, N. Limam, and R. Boutaba,
“Atmos: Autonomous threat mitigation in sdn using reinforcement
learning,” in NOMS IEEE/IFIP Network Operations and Management
Symposium, 2020, pp. 1–9.

[28] Y. Liu et al., “Deep reinforcement learning based smart mitigation
of ddos ﬂooding in software-deﬁned networks,” in 2018 IEEE 23rd
International Workshop on Computer Aided Modeling and Design of
Communication Links and Networks (CAMAD), 2018, pp. 1–6.
[29] T. V. Phan and T. Bauschert, “Deepair: Deep reinforcement learning
for adaptive intrusion response in software-deﬁned networks,” IEEE
Transactions on Network and Service Management, pp. 1–1, 2022.

[30] G. Sofronov, J. Keith, and D. Kroese, “An optimal sequential procedure
for a buying-selling problem with independent observations,” Journal
of Applied Probability, vol. 43, no. 2, pp. 454–462, 2006.

[31] R. Carmona and N. Touzi, “Optimal multiple stopping and valuation
of swing options,” Mathematical Finance, vol. 18, pp. 239–268, Apr.
2008.

[32] R. Kleinberg, “A multiple-choice secretary algorithm with applications
to online auctions,” in Proceedings of the Sixteenth Annual ACM-SIAM
Symposium on Discrete Algorithms, ser. SODA ’05, 2005.

[33] V. Krishnamurthy, A. Aprem, and S. Bhatt, “Multiple stopping time
pomdps: Structural results & application in interactive advertising on
social media,” Automatica, vol. 95, pp. 385–398, 2018.

[34] T. Nakai, “The problem of optimal stopping in a partially observable
markov chain,” Journal of Optimization Theory and Applications,
vol. 45, no. 3, pp. 425–442, Mar 1985.

[35] J. du Toit and G. Peskir, “Selling a stock at the ultimate maximum,”

The Annals of Applied Probability, vol. 19, no. 3, Jun 2009.

[36] A. Roy, V. S. Borkar, A. Karandikar, and P. Chaporkar, “Online rein-
forcement learning of optimal threshold policies for markov decision
processes,” CoRR, 2019, http://arxiv.org/abs/1912.10325.

[37] A. G. Tartakovsky et al., “Detection of intrusions in information
systems by sequential change-point methods,” Statistical Methodology,
2006.

[38] M. N. Kurt, O. othersOgundijo, C. Li, and X. Wang, “Online cyber-
attack detection in smart grid: A reinforcement learning approach,”
IEEE Transactions on Smart Grid, vol. 10, no. 5, pp. 5174–5185, 2019.
[39] C. H. Papadimitriou and J. N. Tsitsiklis, “The complexity of markov
decision processes,” Math. Oper. Res., vol. 12, p. 441–450, Aug. 1987.
[40] M. Zhu, Z. Hu, and P. Liu, “Reinforcement learning algorithms for
adaptive cyber defense against heartbleed,” in Proceedings of the First
ACM Workshop on Moving Target Defense, ser. MTD ’14. New York,
NY, USA: Association for Computing Machinery, 2014, p. 51–58.
[41] R. A. Howard, Dynamic Programming and Markov Processes. Cam-

bridge, MA: MIT Press, 1960.

[42] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and
acting in partially observable stochastic domains,” USA, 1996.

[43] K.

˚Astr¨om, “Optimal control of markov processes with incomplete
state information,” Journal of Mathematical Analysis and Applications,
vol. 10, no. 1, pp. 174–205, 1965.

[44] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-dynamic programming.

Belmont, MA: Athena Scientiﬁc, 1996.

[45] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning,

1st ed. Cambridge, MA, USA: MIT Press, 1998.

[46] R. Bellman, Dynamic Programming. Dover Publications, 1957.
[47] E. J. Sondik, “The optimal control of partially observable markov
processes over the inﬁnite horizon: Discounted costs,” Operations
Research, vol. 26, no. 2, pp. 282–304, 1978.

[48] D. P. Bertsekas, Dynamic Programming and Optimal Control, 3rd ed.

Belmont, MA, USA: Athena Scientiﬁc, 2005, vol. I.

[49] C. Watkins, “Learning from delayed rewards,” Ph.D. dissertation, 1989.
[50] J. Schulman et al., “Proximal policy optimization algorithms,” 2017,

http://arxiv.org/abs/1707.06347.

[51] T. Jaakkola, M. Jordan, and S. Singh, “Convergence of stochastic
iterative dynamic programming algorithms,” in Advances in Neural
Information Processing Systems, vol. 6, 1994.

[52] H. Robbins and S. Monro, “A Stochastic Approximation Method,” The
Annals of Mathematical Statistics, vol. 22, no. 3, pp. 400 – 407, 1951.
[53] A. N. Shirayev, Optimal Stopping Rules. Springer-Verlag Berlin, 2007,

reprint of russian edition from 1969.

[54] G. Peskir and A. Shiryaev, Optimal stopping and free-boundary prob-

lems, ser. Lectures in mathematics (ETH Z¨urich). Springer, 2006.

[55] Y. Chow, H. Robbins, and D. Siegmund, “Great expectations: The

theory of optimal stopping,” 1971.

[56] S. M. Ross, Introduction to Stochastic Dynamic Programming: Prob-
ability and Mathematical. USA: Academic Press, Inc., 1983.
[57] J. Bather, Decision Theory: An Introduction to Dynamic Programming

and Sequential Decisions. USA: John Wiley and Sons, Inc., 2000.

[58] H. V. Poor and O. Hadjiliadis, Quickest Detection.

Cambridge

University Press, 2008.

15

[88] K. A. Simpson, S. Rogers, and D. P. Pezaros, “Per-host ddos mitiga-
tion by direct-control reinforcement learning,” IEEE Transactions on
Network and Service Management, vol. 17, no. 1, pp. 103–117, 2020.
[89] B. Ning and L. Xiao, “Defense against advanced persistent threats in
smart grids: A reinforcement learning approach,” in 2021 40th Chinese
Control Conference (CCC), 2021, pp. 8598–8603.

[90] L. Huang and Q. Zhu, “Adaptive honeypot engagement through rein-
forcement learning of semi-markov decision processes,” in Decision
and Game Theory for Security, 2019, pp. 196–216.

[91] M. Alauthman et al., “An efﬁcient reinforcement learning-based botnet

detection approach,” J. Netw. Comput. Appl., vol. 150, 2020.

[92] G. Apruzzese et al., “Deep reinforcement adversarial learning against
botnet evasion attacks,” IEEE Transactions on Network and Service
Management, vol. 17, no. 4, pp. 1975–1987, 2020.

[93] H. Liu, Y. Li, J. M˚artensson, L. Xie, and K. H. Johansson, “Reinforce-
ment learning based approach for ﬂip attack detection,” in 2020 59th
IEEE Conference on Decision and Control (CDC), 2020.

[94] S. Dong, Y. Xia, and T. Peng, “Network abnormal trafﬁc detection
model based on semi-supervised deep reinforcement learning,” IEEE
Transactions on Network and Service Management, vol. 18, 2021.
[95] J. Wang, C. Song, and H. Yin, “Reinforcement learning-based hierar-

chical seed scheduling for greybox fuzzing,” in NDSS, 2021.

[96] J. Yan, H. He, X. Zhong, and Y. Tang, “Q-learning-based vulnerability
topology attacks,” IEEE

analysis of smart grid against sequential
Transactions on Information Forensics and Security, 2017.

[97] E. S. Page, “Continuous inspection schemes,” Biometrika, vol. 41, no.

1/2, pp. 100–115, 1954.

[98] B. Lantz, B. Heller, and N. McKeown, “A network in a laptop: Rapid
prototyping for software-deﬁned networks,” in Proceedings of the 9th
ACM SIGCOMM Workshop on Hot Topics in Networks, 2010.
[99] M. Standen et al., “Cyborg: A gym for the development of autonomous

cyber agents,” CoRR, https://arxiv.org/abs/2108.09118.

[100] A. Molina-Markham et al., “Network environment design for au-
tonomous cyberdefense,” 2021, https://arxiv.org/abs/2103.07583.

[59] M. Rabi and K. H. Johansson, “Event-triggered strategies for industrial
control over wireless networks,” in Proceedings of the 4th Annual
International Conference on Wireless Internet, ser. WICON ’08, 2008.
[60] J. L. Snell, “Applications of martingale system theorems,” Transactions

of the American Mathematical Society, vol. 73, pp. 293–312, 1952.

[61] S. Karlin, “Total positivity, absorption probabilities and applications,”

Transactions of the American Mathematical Society, vol. 111, 1964.

[62] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient
methods for reinforcement learning with function approximation,” in
Advances in Neural Information Processing Systems, 1999.

[63] J. C. Spall, “Multivariate stochastic approximation using a simultane-
ous perturbation gradient approximation,” IEEE TRANSACTIONS ON
AUTOMATIC CONTROL, vol. 37, no. 3, pp. 332–341, 1992.

[64] J. Spall, “Implementation of the simultaneous perturbation algorithm
for stochastic optimization,” IEEE Transactions on Aerospace and
Electronic Systems, vol. 34, no. 3, pp. 817–823, 1998.

[65] K. Hammar and R. Stadler, “gym-optimal-intrusion-response,” 2021,

https://github.com/Limmen/gym-optimal-intrusion-response.

[66] D. Merkel, “Docker: lightweight linux containers for consistent de-
velopment and deployment,” Linux journal, vol. 2014, no. 239, p. 2,
2014.

[67] S. Hemminger, “Network emulation with netem,” Linux Conf, 2005.
[68] T. Smith and R. Simmons, “Heuristic search value iteration for
pomdps,” in Proceedings of the 20th Conference on Uncertainty in
Artiﬁcial Intelligence, ser. UAI ’04, Arlington, Virginia, USA, 2004.

[69] A. N. Shiryaev, “On optimum methods in quickest detection problems,”

Theory Probab. Appl., vol. 8, no. 1, pp. 22–46, 1963.

[70] M. Roesch, “Snort - lightweight intrusion detection for networks,” in
Proceedings of the 13th USENIX Conference on System Administration,
ser. LISA ’99. USA: USENIX Association, 1999, p. 229–238.
[71] Khraisat et al., “Survey of intrusion detection systems: techniques,
datasets and challenges,” Cybersecurity, vol. 2, no. 1, p. 20, 2019.
[72] J. Dromard, G. Roudi`ere, and P. Owezarski, “Online and scalable
unsupervised network anomaly detection method,” IEEE Transactions
on Network and Service Management, vol. 14, no. 1, pp. 34–47, 2017.
[73] C. J. Fung, J. Zhang, and R. Boutaba, “Effective acquaintance man-
agement based on bayesian learning for distributed intrusion detection
networks,” IEEE Transactions on Network and Service Management,
vol. 9, no. 3, pp. 320–332, 2012.

[74] C. J. Fung, J. Zhang, I. Aib, and R. Boutaba, “Dirichlet-based trust
management for effective collaborative intrusion detection networks,”
IEEE Transactions on Network and Service Management, 2011.
[75] S. Huang et al., “Hitanomaly: Hierarchical transformers for anomaly
detection in system log,” IEEE Transactions on Network and Service
Management, vol. 17, no. 4, pp. 2064–2076, 2020.

[76] M. Tambe, Security and Game Theory: Algorithms, Deployed Systems,

Lessons Learned, 1st ed. USA: Cambridge University Press, 2011.

[77] C. J. Fung and R. Boutaba, Intrusion Detection Networks - A Key to

Collaborative Security. CRC Press, 2013.

[78] L. Buttyan and J.-P. Hubaux, Security and Cooperation in Wireless
Networks: Thwarting Malicious and Selﬁsh Behavior in the Age of
Ubiquitous Computing. USA: Cambridge University Press, 2007.
[79] N. Dhir, H. Hoeltgebaum, N. Adams, M. Briers, A. Burke, and P. Jones,
“Prospective artiﬁcial intelligence approaches for active cyber defence,”
CoRR, vol. abs/2104.09981, 2021, https://arxiv.org/abs/2104.09981.

[80] T. T. Nguyen and V. J. Reddi, “Deep reinforcement learning for cyber

security,” CoRR, 2019, http://arxiv.org/abs/1906.05799.
[81] Y. Huang, L. Huang, and Q. Zhu, “Reinforcement

learning for
feedback-enabled cyber resilience,” Annual Reviews in Control, 2022.
[82] E. Miehling, M. Rasouli, and D. Teneketzis, “A pomdp approach to the
dynamic defense of large-scale cyber networks,” IEEE Transactions on
Information Forensics and Security, vol. 13, no. 10, 2018.

[83] J. C. Georgia, “Next generation intrusion detection: Autonomous
reinforcement learning of network attacks,” in In Proceedings of the
23rd National Information Systems Secuity Conference, 2000, pp. 1–12.
[84] X. Xu and T. Xie, “A reinforcement learning approach for host-based
intrusion detection using sequences of system calls,” in Advances in
Intelligent Computing, 2005.

[85] A. Servin and D. Kudenko, “Multi-agent reinforcement learning for
intrusion detection,” in Adaptive Agents and Multi-Agent Systems III.
Adaptation and Multi-Agent Learning, 2008.

[86] N. Mowla et al., “Afrl: Adaptive federated reinforcement learning for
intelligent jamming defense in fanet,” Journal of Communications and
Networks, vol. 22, no. 3, 2020.

[87] K. Malialis and D. Kudenko, “Multiagent router throttling: Decentral-
ized coordinated response against ddos attacks,” in IAAI, 2013.

16

