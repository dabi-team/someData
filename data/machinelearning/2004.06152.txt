Sparse Regression at Scale: Branch-and-Bound rooted in
First-Order Optimization

Hussein Hazimeh∗, Rahul Mazumder†, and Ali Saab‡

Massachusetts Institute of Technology

April, 2021

Abstract

We consider the least squares regression problem, penalized with a combination of the (cid:96)0 and
squared (cid:96)2 penalty functions (a.k.a. (cid:96)0(cid:96)2 regularization). Recent work shows that the resulting
estimators are of key importance in many high-dimensional statistical settings. However, exact
computation of these estimators remains a major challenge. Indeed, modern exact methods,
based on mixed integer programming (MIP), face diﬃculties when the number of features p ∼
104. In this work, we present a new exact MIP framework for (cid:96)0(cid:96)2-regularized regression that
can scale to p ∼ 107, achieving speedups of at least 5000x, compared to state-of-the-art exact
methods. Unlike recent work, which relies on modern commercial MIP solvers, we design a
specialized nonlinear branch-and-bound (BnB) framework, by critically exploiting the problem
structure. A key distinguishing component in our framework lies in eﬃciently solving the node
relaxations using a specialized ﬁrst-order method, based on coordinate descent (CD). Our CD-
based method eﬀectively leverages information across the BnB nodes, through using warm starts,
active sets, and gradient screening. In addition, we design a novel method for obtaining dual
bounds from primal CD solutions, which certiﬁably works in high dimensions. Experiments
on synthetic and real high-dimensional datasets demonstrate that our framework is not only
signiﬁcantly faster than the state of the art, but can also deliver certiﬁably optimal solutions
to statistically challenging instances that cannot be handled with existing methods. We open
source the implementation through our toolkit L0BnB.

1
2
0
2

r
p
A
4
1

]

O
C

.
t
a
t
s
[

2
v
2
5
1
6
0
.
4
0
0
2
:
v
i
X
r
a

∗MIT Operations Research Center. Email: hazimeh@mit.edu
†MIT Sloan School of Management, Operations Research Center and MIT Center for Statistics. Email:

rahulmaz@mit.edu

‡AJO. Email: saab@mit.edu

1

 
 
 
 
 
 
1

Introduction

We consider the sparse linear regression problem with additional (cid:96)2 regularization [13, 32]. A
natural way to impose sparsity in this context is through controlling the (cid:96)0 (pseudo) norm of the
estimator, which counts the number of nonzero entries. More concretely, let X ∈ Rn×p be the data
matrix, with n samples and p features, and y ∈ Rn be the response vector. We focus on the least
squares problem with a combination of (cid:96)0 and (cid:96)2 regularization:

min
β∈Rp

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ0(cid:107)β(cid:107)0 + λ2(cid:107)β(cid:107)2
2,

(1)

where (cid:107)β(cid:107)0 is deﬁned as the number of nonzero entries in the regression coeﬃcients β ∈ Rp, and (cid:107)β(cid:107)2
2
is the squared (cid:96)2-norm of β (also referred to as ridge regularization). The regularization parameters
λ0 and λ2 are assumed to be speciﬁed by the practitioner1. We note that the presence of ridge
regularization (i.e., λ2 > 0) can be important from a statistical viewpoint—see for example [31, 39,
32] for further discussions on this matter. Statistical properties of (cid:96)0-based estimators have been
extensively studied in the statistics literature [26, 46, 54, 19, 52, 39]. Speciﬁcally, under suitable
assumptions on the underlying data and appropriate choices of tuning parameters, global solutions
of (1) have optimal support recovery properties [52, 23]; and optimal prediction error bounds that do
not depend upon X [46]. Appealing prediction error bounds available from optimal solutions of (1)
for the low-signal regime are discussed in [39]. Such strong guarantees are generally not satisﬁed
by heuristic solutions to (1)—see [54] for theoretical support and [32] for numerical evidence. From
a practical perspective, the ability to certify the optimality of solutions (e.g., via dual bounds) is
important and can engender trust in mission critical applications such as healthcare.

Despite its appeal, Problem (1) is labeled as NP-Hard [42] and poses computational challenges.
Recently, there has been exciting work in developing Mixed Integer Programming (MIP)-based
approaches to solve (1), e.g., [17, 10, 41, 21, 13, 32, 12, 53, 6]. Speciﬁcally, [10] demonstrated that
oﬀ-the-shelf MIP solvers can handle problem instances for p up to a thousand. Larger instances
can be handled when λ2 is suﬃciently large and the feature correlations are low—for example, see
the approach of [13]; and the method in [20] for the classiﬁcation variant of (1). The approaches
of [13, 20] rely on commercial MIP solvers such as Gurobi. While these state-of-the-art global
optimization approaches show very promising results, they are still relatively slow for practical
usage [31, 32]. For example, our experiments show that these methods cannot terminate in two
hours for typical instances with p ∼ 104. On the other hand, the fast Lasso solvers, e.g., glmnet [25],
and local optimization methods for (1), such as L0Learn [32], can handle much larger instances,
and they typically terminate in the order of milliseconds to seconds.

Our goal in this paper is to advance the computational methodology for the global optimization
of Problem (1). In particular, we aim to (i) reduce the run time for solving problem instances with
p ∼ 104 from hours to seconds, and (ii) scale to larger problem instances with p ∼ 107 in reasonable
times (order of minutes to hours). To this end, we propose a specialized nonlinear branch-and-
bound (BnB) framework that does not rely on commercial MIP solvers. We employ a ﬁrst-order
method, which carefully exploits the problem structure, to solve the node relaxations. This makes
our approach quite diﬀerent from prior work on global optimization for Problem (1), which rely
on commercial MIP solvers, e.g., Gurobi and CPLEX. These MIP solvers are also based on a BnB

1The choice of (λ0, λ2) depends upon the particular application and/or dataset. We aim to compute solutions

to (1) for a family of (λ0, λ2)-values.

2

framework, but they are equipped with general-purpose relaxation solvers and heuristics that do
not take into account the speciﬁc structure in Problem (1).

Our BnB solves a mixed integer second order cone program (MISOCP) that is based on a
perspective reformulation [24, 2, 27] of Problem (1). The algorithm exploits the sparsity structure
in the problem during diﬀerent stages: when solving node relaxations, branching, and obtaining
upper bounds. The continuous node relaxations that appear in our BnB have not been studied
at depth in earlier work. A main contribution of our work is to show that these relaxations,
which involve seemingly complicated linear and conic constraints, can be eﬃciently handled using
a primal coordinate descent (CD)-based algorithm. Indeed, this represents a radical change from
the primal-dual relaxation solvers commonly used in state-of-the-art MIP solvers [8]. Our choice
of CD is motivated by its ability to eﬀectively share information across the BnB nodes (such as
warm starts), and more generally by its high scalability in the context of sparse learning, e.g.,
see [25, 38, 32]. Along with CD, we propose additional strategies, namely, active set updates and
gradient screening, which reduce the coordinate update complexity by exploiting the information
shared across the BnB tree.

Although our CD-based algorithm for solving BnB node relaxations is highly scalable, it only
generates primal solutions. However, dual bounds are required for search space pruning in BnB.
Thus, we propose a novel method to eﬃciently generate dual bounds from the primal solutions. We
analyze these dual bounds and prove that their tightness is not aﬀected by the number of features
p, but rather by the number of nonzeros in the primal solution. This result serves as a theoretical
justiﬁcation for why our CD-based algorithm can lead to tight dual bounds in high dimensions.

Contributions and Structure: We summarize our key contributions below.

• We formulate Problem (1) as a MISOCP, based on a perspective formulation. We provide
a new analysis of the relaxation tightness, which identiﬁes parameter ranges for which the
perspective formulation can outperform popular formulations (see Section 2).

• To solve the MISOCP, we design a specialized nonlinear BnB, with the following main con-

tributions (see Section 3):

– We show that the node relaxations, which involve linear and conic constraints, can be
reformulated as a least squares problem with a non-diﬀerentiable but separable penalty.
To solve the latter reformulation, we develop a primal CD algorithm, along with active
set updates and gradient screening that use information shared across the BnB tree to
reduce the coordinate update cost.

– We develop a new eﬃcient method for obtaining dual bounds from the primal solutions.
We analyze these dual bounds and show that their tightness depends on the sparsity
level rather than p.

– We introduce eﬃcient methods that exploit sparsity when selecting branching variables

and obtaining incumbents2.

• We perform a series of experiments on high-dimensional synthetic and real datasets, with p up
to 8.3 × 106. We study the eﬀect of the regularization parameters and dataset characteristics
on the run time, and perform ablation studies. The results indicate that our approach can
be 5000x faster than the state of the art in some settings, and is capable of handling diﬃcult

2An incumbent, in the context of BnB, refers to the best integral solution found so far.

3

statistical instances which were virtually unsolvable before (See Section 4). We open source
the implementation through our toolkit L0BnB:

https://github.com/alisaab/L0BnB

Related Work: As mentioned earlier, an impressive line of recent work considers solving Problem
(1), or its cardinality-constrained variant, to optimality. [10] used Gurobi on a Big-M formulation,
which can handle n ∼ p ∼ 103 in the order of minutes to hours. [13] scale the problem even further
by applying outer-approximation (using Gurobi) on a boolean reformulation [45] of the problem.
Their approach can handle p ∼ 105 in the order of minutes when n and λ2 are suﬃciently large,
and the feature correlations are suﬃciently small. This outer-approximation approach has also
been generalized to sparse classiﬁcation in [11].
[53] consider solving a perspective formulation
[24] of the problem directly using Gurobi and reported timings that compare well with [13]—the
largest problem instances they consider have p ∼ 103. [20] show that a variant of Problem (1) for
classiﬁcation can be solved through a sequence of MIPs (solved using Gurobi), each having a small
number of binary variables, as opposed to p binary variables in the common approaches. Their
approach can handle n = 103 and p = 50, 000 in minutes if the feature correlations are suﬃciently
small. Our specialized BnB, on the other hand, can solve all the instances mentioned above with
speed-ups that exceed 5000x, and can scale to problems with p ∼ 107. Moreover, our numerical
experiments show that, unlike prior work, our BnB can handle diﬃcult problems with relatively
small λ2 and/or high feature correlations.

In addition to the global optimization approaches discussed above, there is an interesting body
of work in the broader optimization community on improved relaxations for sparse ridge regression,
e.g., [45, 21, 5, 53], and algorithms that locally optimize an (cid:96)0-based objective [14, 7, 32].

There is also a rich literature on solving mixed integer nonlinear programs (MINLPs) using BnB,
e.g., see [36, 8]. Our approach is based on the nonlinear BnB framework [18], where a nonlinear
subproblem is solved at every node of the search tree. Interior point methods are a popular choice
for these nonlinear subproblems, especially for MISOCPs [36], e.g., they are used in MOSEK [3]
and are also one of the supported options in CPLEX [47]. Generally, interior point based nonlinear
solvers are not as eﬀective in exploiting warm starts and sparsity as linear programming solvers
[8], which led to an alternative approach known as outer-approximation (OA) [22].
In OA, a
sequence of relaxations, consisting of mixed integer linear programs, are solved until converging to
a solution of the MINLP. State-of-the-art solvers such as BARON [48] and Gurobi [28] apply OA
on extended formulations—[48] laid the ground work for this approach. There is also a line of work
on specialized OA reformulations and algorithms for mixed integer conic programs (which include
MISOCPs), e.g., see [50, 37, 51] and the references therein. In this paper, we pursue a diﬀerent
approach: making use of problem-speciﬁc structure, we show that the relaxation of the MISOCP
can be eﬀectively handled by our proposed CD-based algorithm.

Notation and Supplementary Material: We denote the set {1, 2, . . . , p} by [p]. For any
set A, the complement is denoted by Ac. We let (cid:107) · (cid:107)q denote the standard (cid:96)q norm with q ∈
{0, 1, 2, ∞}. For any vector v ∈ Rk, sign(v) ∈ Rk refers to the vector whose ith component is given
by sign(vi) = vi/|vi| if vi (cid:54)= 0 and sign(vi) ∈ [−1, 1] if vi = 0. We denote the support of β ∈ Rp
by Supp(β) = {i : βi (cid:54)= 0, i ∈ [p]}. For a set S ⊆ [p], use βS ∈ R|S| to denote the subvector of β
with indices in S. Similarly, XS refers to the submatrix of X whose columns correspond to S. For
a scalar a, we denote [a]+ = max{a, 0}. Given a set of real numbers {ai}N
i=1 and a scalar c, we

4

use {ai}N
appendix.

i=1 · c to denote {cai}N

i=1. The proofs of all propositions, lemmas, and theorems are in the

2 MIP Formulations and Relaxations

In this section, we present MIP formulations for Problem (1) and study their corresponding relax-
ations.

2.1 MIP Formulations

The Big-M Formulation: We assume that there is a ﬁnite scalar M > 0 (a-priori speciﬁed) such
that an optimal solution of Problem (1), say β∗, satisﬁes: (cid:107)β∗(cid:107)∞ ≤ M . This allows for modeling
(1) as a mixed integer quadratic program (MIQP) using the following Big-M formulation:

B(M ) : min
β,z

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ0

(cid:88)

i∈[p]

zi + λ2(cid:107)β(cid:107)2
2

s.t. − M zi ≤ βi ≤ M zi, i ∈ [p]
zi ∈ {0, 1}, i ∈ [p],

(2)

where each binary variable zi controls whether βi is zero or not via the ﬁrst constraint in (2)—i.e.,
if zi = 0 then βi = 0. Such Big-M formulations are widely used in mixed integer programming
and have been recently explored in multiple works on (cid:96)0 regularization, e.g., [10, 39, 32, 53]. See
[10, 53] for a discussion on how to estimate M in practice.

The Perspective Formulation:
In MIP problems where bounded continuous variables are
activated by indicator variables, perspective reformulations [24, 2, 27] can lead to stronger MIP
relaxations and thus improve the run time of BnB algorithms. Here, we apply a perspective
reformulation to the ridge term (cid:107)β(cid:107)2
2 in Problem (2). Speciﬁcally, we introduce the auxiliary
continuous variables si ≥ 0, i ∈ [p] and rotated second order cone constraints β2
i ≤ sizi, i ∈ [p]. We
i∈[p] si. Thus, each si takes the place of β2
then replace the term (cid:107)β(cid:107)2
i . This leads to the
following reformulation of (2):

2 with (cid:80)

PR(M ) : min
β,z,s

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ0

(cid:88)

i∈[p]

zi + λ2

(cid:88)

si

i∈[p]

s.t.

β2
i ≤ sizi, i ∈ [p]
− M zi ≤ βi ≤ M zi, i ∈ [p]
zi ∈ {0, 1}, si ≥ 0, i ∈ [p].

(3)

Problem (3) can be expressed as a MISOCP. Similar to (2), formulation (3) is equivalent to Problem
(1) (as long as M is suitably chosen). Algorithms for formulation (3) will be the main focus of our
paper.
If we set M = ∞ in (3), then the constraints βi ∈ [−M zi, M zi], i ∈ [p] can be dropped, which makes
PR(∞) independent of a Big-M parameter. If λ2 > 0, then PR(∞) is equivalent to (1)—this holds
since β2
i ≤ sizi enforces zi = 0 =⇒ βi = 0. We note that [21] have studied Problem (3) and focused
on the special case of PR(∞). [21] shows that PR(∞) is equivalent to the pure binary formulations
considered in [45, 13]. We also note that [53] have considered a similar perspective formulation for

5

the cardinality constrained variant of Problem (1). In Proposition 1 below, we present new bounds
that quantify the relaxation strengths of PR(M ), PR(∞), and B(M ). Moreover, we are the ﬁrst
to present a tailored BnB procedure for formulation (3).

2.2 Relaxation of the Perspective Formulation (3)

In this section, we study the interval relaxation of Problem (3), which is obtained by relaxing all
binary zi’s to the interval [0, 1]. Speciﬁcally, we present a new compact reformulation of the interval
relaxation that leads to useful insights and facilitates our algorithm development. We also discuss
how this reformulation compares with the interval relaxations of B(M ) and PR(∞).

Theorem 1 shows that the interval relaxation of (3) can be reformulated purely in the β space.
This leads to a regularized least squares criterion, where the regularizer involves the reverse Hu-
ber [44] penalty—a hybrid between the (cid:96)1 and (cid:96)2 (squared) penalties. The reverse Huber penalty
B : R → R, is given by:

B(t) =

(cid:40)

|t|
(t2 + 1)/2

|t| ≤ 1

|t| ≥ 1.

(4)

Theorem 1. (Reduced Relaxation) Let us deﬁne the functions ψ, ψ1, ψ2 as

ψ(βi; λ0, λ2, M ) =

(cid:40)

ψ1(βi; λ0, λ2) := 2λ0B(βi
ψ2(βi; λ0, λ2, M ) := (λ0/M + λ2M )|βi|

(cid:112)λ2/λ0)

if (cid:112)λ0/λ2 ≤ M
if (cid:112)λ0/λ2 > M.

The interval relaxation of (3) is equivalent to:

min
β∈Rp

F (β) :=

1
2

(cid:107)y − Xβ(cid:107)2

2 +

(cid:88)

i∈[p]

ψ(βi; λ0, λ2, M ) s.t. (cid:107)β(cid:107)∞ ≤ M,

(5)

and we let VPR(M ) denote the optimal objective value of (5).

The reduced formulation (5) has an important role in both the subsequent analysis and the
algorithmic development in Section 3. Theorem 1 shows that the conic and Big-M constraints in the
relaxation of (3) can be completely eliminated, at the expense of introducing (in the objective) the
non-diﬀerentiable penalty function (cid:80)
i ψ(βi; λ0, λ2, M ) which is separable across the p coordinates
βi, i ∈ [p]. Depending on the value of (cid:112)λ0/λ2 compared to M , the penalty ψ(βi; λ0, λ2, M ) is either
the reverse Huber penalty (i.e., ψ1) or the (cid:96)1 penalty (i.e., ψ2), both of which are sparsity-inducing.
In Figure 1 (left panel), we plot ψ1(β; λ0, λ2) for λ2 = 1 at diﬀerent values of λ0. In Figure 1 (right
panel), we plot ψ(β; λ0, λ2, M ) at λ0 = λ2 = 1 and for diﬀerent values of M . The appearance of a
pure (cid:96)1 penalty in the objective is interesting in this case since the original formulation in (3) has
a ridge term in the objective. Informally speaking, when (cid:112)λ0/λ2 > M , the constraint |βi| ≤ M zi
becomes active at any optimal solution, which turns the ridge term into an (cid:96)1 penalty—for further
discussion on this matter, see the proof of Theorem 1.

Next, we analyze the tightness of the perspective relaxation in (5).

Tightness of the Perspective Relaxation (5): [21] has shown that the interval relaxation of
PR(∞) can be written as:

VPR(∞) = min
β∈Rp

G(β) :=

1
2

(cid:107)y − Xβ(cid:107)2

2 +

(cid:88)

i∈[p]

ψ1(βi; λ0, λ2),

(6)

6

Figure 1:
diﬀerent values of M .

[Left]: Plot of ψ1(β; λ0, 1) for diﬀerent values of λ0.

[Right]: Plot of ψ(β; 1, 1, M ) for

where ψ1(βi; λ0, λ2) is deﬁned in Theorem 1. Note that (6) can also be obtained from Theorem
1 (with M = ∞). For (cid:112)λ0/λ2 ≤ M , the relaxation of PR(M ) in Theorem 1 matches that in
(6), but with the additional box constraint (cid:107)β(cid:107)∞ ≤ M . When M is large, this box constraint
becomes inactive, making the interval relaxations of PR(M ) and PR(∞) equivalent. However,
when (cid:112)λ0/λ2 > M , the interval relaxation of PR(M ) can have a (strictly) larger objective than
that of both B(M ) and PR(∞), as we show in Proposition 1.

Proposition 1. Let VB(M ) denote the optimal objective of the interval relaxation of B(M ). Let
β∗ be an optimal solution to (5), and deﬁne the function h(λ0, λ2, M ) = λ0/M + λ2M − 2
λ0λ2.
Then, the following holds for (cid:112)λ0/λ2 > M :

√

VPR(M ) ≥ VB(M ) + λ2(M (cid:107)β∗(cid:107)1 − (cid:107)β∗(cid:107)2
2)
VPR(M ) ≥ VPR(∞) + h(λ0, λ2, M )(cid:107)β∗(cid:107)1.

(7)

(8)

If there is an i ∈ [p] with 0 < |β∗

We make a couple of remarks. For bound (7), we always have (M (cid:107)β∗(cid:107)1 − (cid:107)β∗(cid:107)2
i | < M , then (M (cid:107)β∗(cid:107)1 − (cid:107)β∗(cid:107)2

2) ≥ 0 since
(cid:107)β∗(cid:107)∞ ≤ M .
2) > 0, and
consequently VPR(M ) > VB(M ) (as long as λ2 > 0). In bound (8), for (cid:112)λ0/λ2 > M , h(λ0, λ2, M ) is
strictly positive and monotonically decreasing in M , which implies that VPR(M ) > VPR(∞) (as long
as β∗ (cid:54)= 0). In Section 4.2.1, our experiments empirically validate Proposition 1: using PR(M ) with
a suﬃciently tight (but valid) M can speed up the same BnB solver by more than 90x compared
to PR(∞).

Note that Proposition 1 does not directly compare VPR(∞) with VB(M ). In Proposition 2, we
establish a new result which compares VPR(∞) with VB(M ). Before we present Proposition 2, we
introduce some notation. Let S(λ2) be the set of optimal solutions (in β) of the interval relaxation
of PR(∞); and deﬁne

L(M ) := {λ2 > 0 | ∃β ∈ S(λ2) s.t. (cid:107)β(cid:107)∞ ≤ M }.

Proposition 2. Let L(M ) be as deﬁned in (9). Then, the following holds:

VB(M ) ≥ VPR(∞), if M ≤ 1
2
VB(M ) ≤ VPR(∞), if M ≥ (cid:112)λ0/λ2 and λ2 ∈ L(M ).

(cid:112)λ0/λ2

7

(9)

(10)

(11)

1.000.750.500.250.000.250.500.751.000.00.20.40.60.81.01.21.41(;0,2)0=0.010=0.10=0.20=0.30=0.52.01.51.00.50.00.51.01.52.001234567(;0,2,M)122M0/2M=0.50/2M=0.30/2Proposition 2 implies that if λ2 is relatively small (with other parameters remaining ﬁxed),
then the relaxation of the Big-M formulation (with objective VB(M )) will have a higher objective
than the relaxation of PR(∞). On the other hand, if λ2 is suﬃciently large, then the relaxation of
PR(∞) will have a higher objective.

We note that the result of Proposition 2 applies for any M ≥ 0, even if it is mis-speciﬁed.
However, in the case of mis-speciﬁcation, VB(M ) (and also VPR(M )) may no longer correspond to a
valid relaxation of Problem (1). In contrast, VPR(∞) is always a valid relaxation of (1).

3 A Specialized Branch-and-Bound (BnB) Framework

In this section, we develop a specialized nonlinear BnB framework for solving the perspective
formulation in (3). First, we brieﬂy recall the high-level mechanism behind nonlinear BnB, in the
context of our problem.

Nonlinear BnB at a Glance: The algorithm starts by solving the (nonlinear) interval relaxation
of (3), i.e., the root node. Then, it selects a branching variable, say variable j ∈ [p], and creates
two new nodes (optimization subproblems): one node with zj = 0 and another with zj = 1, where
all the other zi’s are relaxed to the interval [0, 1]. For every unvisited node, the algorithm proceeds
recursively, i.e., by solving an optimization subproblem at the current node and then branching on
a new variable to create two new nodes. This leads to a search tree with nodes corresponding to
optimization subproblems and edges representing branching decisions.

To reduce the size of the search tree, BnB prunes a node (i.e., does not branch on it) in either
one of the following situations: (i) an optimal solution to the relaxation at the current node has
an integral z or (ii) the objective of the current relaxation exceeds the best available upper bound
on (3). In case (ii), the relaxation need not be solved exactly:
lower bounds on the relaxation’s
objective (a.k.a. dual bounds) can be used for pruning. However, in case (i), if the dual bound
does not exceed the best upper bound, the node should be solved to optimality in order to ensure
correctness of the pruning decision3.

As BnB explores more nodes, its (global) lower bound is guaranteed to converge to the optimal
objective of (3). In practice, we can terminate the algorithm early, if the gap between the best
lower and upper bounds is below a pre-speciﬁed user-deﬁned threshold.

Overview of our Strategies: The discussion above outlines how nonlinear BnB operates in
general. The speciﬁc strategies used such as solving the relaxations, passing information across
the nodes, and selecting branching variables, can have a key impact on scalability. In the rest of
this section, we will give a detailed account of the strategies used in our BnB. We ﬁrst provide an
overview of these strategies:

• A Primal Relaxation Solver: Unlike state-of-the-art approaches for nonlinear BnB, which
employ primal-dual interior point solvers for node relaxations [8], we rely solely on a primal
method which consists of a highly scalable CD-based algorithm. The algorithm solves the node
relaxations of (3) in the β-space as opposed to the extended (β, s, z) space—these relaxations
are variants of the reduced relaxation introduced in (5). The algorithm heavily shares and
exploits warm starts, active sets, and information on the gradients, across the BnB tree. This
will be developed in Section 3.1.

3In practice, we solve the relaxation problem at the node to optimality by ensuring that the relative diﬀerence

between the primal and dual bounds is less than a small user-deﬁned numerical tolerance.

8

• Dual Bounds: Dual bounds on the objective of node relaxations are required by BnB
for search space pruning, yet our relaxation solver works in the primal space for scalability
considerations. We develop a new eﬃcient method for obtaining dual bounds from the primal
solutions. We provide an analysis of this method and show that the tightness of the dual
bounds depends on the sparsity level and not on the number of features p. See Section 3.2.

• Branching and Incumbents: We present an eﬃcient variant of strong branching, which
leverages the solutions and active sets of previous node relaxations to make optimization
tractable. Moreover, we employ several eﬃcient heuristics to obtain incumbents. See Section
3.3.

For simplicity of exposition, in the remainder of Section 3, we assume that the columns of X

and y have unit (cid:96)2 norm.

3.1 Primal Relaxation Solver: Active-set Coordinate Descent

To simplify the presentation, we will focus on solving the root relaxation. To this end, we solve
the reduced formulation in the β-space (5). We operate on the reduced formulation compared to
the interval relaxation of (3) in the extended (β, s, z) space due to computational reasons. After
solving (5), we use the resulting solution, say β∗, to construct a corresponding solution (β∗, s∗, z∗)
to the interval relaxation of (3)—see the proof of Theorem 1 for how to obtain (β∗, s∗, z∗) from β∗.
We then use z∗ for branching. The rest of the nodes in BnB involve ﬁxing some binary variables in
(3) to 0 or 1, so their subproblems can be obtained by minor modiﬁcations to the root relaxation.
For completeness, in Appendix B.2, we discuss how to formulate and solve these node subproblems.
Problem (5) is of the composite form [43]: the objective is the sum of a smooth loss function
and a non-smooth but separable penalty. In addition, the feasible set, consisting of the constraints
|βi| ≤ M , i ∈ [p] is separable across the coordinates. This makes Problem (5) amenable to cyclic
CD [49]. To our knowledge, the use of cyclic CD for Problem (5) is novel. We also emphasize that
a direct application of cyclic CD to Problem (5) will face scalability issues, and more importantly,
it does not readily deliver dual bounds. The additional strategies we develop later in this section
are essential for both achieving scalability and obtaining (provably) high quality dual bounds.

Cyclic CD visits the coordinates according to a ﬁxed ordering, updating one coordinate at a

time, as detailed in Algorithm 1.

Algorithm 1: Cyclic CD for Relaxation (5)

• Input: Initialization ˆβ

• While not converged:

– For i ∈ [p] :

ˆβi ← arg min

βi∈R

F ( ˆβ1, . . . , βi, . . . , ˆβp)

s.t.

|βi| ≤ M.

(12)

Every limit point of Algorithm 1 is a global minimizer of (5) [49], and the algorithm has a sublinear
rate of convergence [34]. We will show that the solution of (12) can be computed in closed-form.
To this end, since the columns of X have unit (cid:96)2-norm, we note that (12) is equivalent to:

min
βi∈R

1
2

(βi − ˜βi)2 + ψ(βi; λ0, λ2, M )

s.t.

|βi| ≤ M,

(13)

9

where ˜βi := (cid:104)y − (cid:80)
boxed soft-thresholding operator T : R → R as

j(cid:54)=i Xj ˆβj, Xi(cid:105). Given non-negative scalar parameters a and m, we deﬁne the

T (t; a, m) :=

(|t| − a) sign(t)


0



m sign(t)

if |t| ≤ a

if a < |t| ≤ a + m

otherwise.

For (cid:112)λ0/λ2 ≤ M , the solution of (13) is given by:
(cid:40)

√

ˆβi =

λ0λ2, M )

T ( ˜βi; 2
T ( ˜βi(1 + 2λ2)−1; 0, M )

√

if | ˜βi| ≤ 2
otherwise

λ0λ2 + (cid:112)λ0/λ2

and for (cid:112)λ0/λ2 > M , the solution of (13) is:

ˆβi = T ( ˜βi; λ0/M + λ2M, M ).

(14)

(15)

Thus, update (12) in Algorithm 1 can be computed in closed-form using expressions (14) and (15).
See Appendix B.1 for a derivation of the update rules.

Interior-point methods used in state-of-the-art nonlinear BnB solvers cannot
Warm Starts:
easily use warm starts. On the other hand, cyclic CD has proven to be very eﬀective in exploiting
warm starts, e.g., see [25, 32]. For every node in BnB, we use its parent’s primal solution as a warm
start. Recall that the parent and children nodes have the same relaxations, except that a single
binary variable (used for branching) is ﬁxed to zero or one in the children4. Intuitively, this can
make the warm start close to the optimal solution. Moreover, the supports of the optimal solutions
of the parent and children nodes are typically very close. We exploit this observation by sharing
information about the supports across the BnB tree, as we describe next in Section 3.1.1.

3.1.1 Active Sets

Update (12) in Algorithm 1 requires O(n) operations, so every full cycle (across all p coordinates)
of the algorithm has cost of O(np). This becomes a major bottleneck for large n or p, since CD
can require many cycles before convergence. In practice, the majority of the variables stay zero
during the course of Algorithm 1 (assuming that the regularization parameters are chosen so that
the optimal solution of the relaxation is sparse, and good warm starts are used). Motivated by this
observation, we run Algorithm 1 restricted to a small subset of the variables A ⊆ [p], which we
refer to as the active set, i.e., we solve the following restricted problem:

ˆβ ∈ arg min

β∈Rp

F (β)

s.t.

(cid:107)β(cid:107)∞ ≤ M, βAc = 0.

(16)

After solving (16), we augment the active set with any variable i ∈ Supp( ˆβ)c that violates the
following optimality condition:

0 = arg min

βi∈R

F ( ˆβ1, . . . , βi, . . . , ˆβp)

s.t.

|βi| ≤ M.

4Suppose the parent node branches on zi. After reformulating the node relaxations in the β space (as discussed
in Appendix B.2), the relaxation of the child with zi = 1 will be the same as the parent except that the penalty of
coordinate i in the parent, ψ(βi; λ0, λ2, M ), will be replaced with λ0 + λ2β2
i . For the child with zi = 0, the relaxation
will be similar to the parent but with βi ﬁxed to 0.

10

We repeat this procedure of solving the restricted problem (16) and then augmenting A with
violating variables, until there are no more violations. The algorithm is summarized below.

Algorithm 2: The Active-set Algorithm5

• Input: Initial solution ˆβ and initial active set A.

• Repeat:

– Step 1: Solve (16) using Algorithm 1 to get a solution ˆβ.
– Step 2: V ← {i ∈ Supp( ˆβ)c | 0 (cid:54)= arg min|βi|≤M F ( ˆβ1, . . . , βi, . . . , ˆβp)}.
– Step 3: If V is empty terminate, otherwise, A ← A ∪ V.

The quality of the initial active set aﬀects the number of iterations in Algorithm 2. For example,
if A is a superset of the support of an optimal solution to (5), then V in the ﬁrst iteration of
Algorithm 2 will be empty, and the algorithm will terminate in a single iteration. For every node
in BnB, we choose the initial active set to be the same as the ﬁnal active set of the parent node.
This works well in practice because parent and children nodes typically have similar supports. For
the root relaxation, we obtain the initial active set by choosing a small subset of features which
have the highest (absolute) correlation with y.

In Section 3.2, we present a novel method that makes use of the updates in Algorithm 2 to obtain
provably high-quality dual bounds. We note that our approach goes beyond standard active-set
methods [25]. In particular, (i) we use active sets in the context of a BnB tree, percolating informa-
tion from parents to children (as opposed to warm-start continuation across a grid of regularization
parameters); and (ii) we exploit the active sets to deliver dual bounds. In the next remark, we
discuss how Step 1 in Algorithm 2 can be performed inexactly.

Remark 1. In practice, we solve the restricted optimization problem in Step 1 of Algorithm 2
inexactly. Speciﬁcally, in Step 1, we terminate Algorithm 1 when the relative change in the objective
values is below a small numerical tolerance6. For Step 3, we ensure that V is (exactly) empty before
terminating the algorithm, which guarantees that there are no optimality violations outside the
support. Having no optimality violations outside the support will be essential to obtain the tight
dual bounds discussed in Section 3.2.

3.1.2 Gradient Screening

Algorithm 2 eﬀectively reduces the number of coordinate updates, through restricting optimization
to the active set. However, checking the optimality conditions outside the active set, i.e., performing
Step 2, requires O(np) operations. This is a bottleneck when p is large, even if a small number of
such checks (or passes) are performed. To mitigate this, we present a gradient screening method
which reduces the time complexity of these optimality checks. Our method is inspired by the
gradient screening technique proposed in [33] for a diﬀerent problem: learning sparse hierarchical
interactions via a convex optimization formulation. In the current paper, the optimality checks

5Algorithm 2 solves the root relaxation. For other nodes, the objective function F (β) will need to be modiﬁed to
account for the ﬁxed variables (as detailed in Appendix B.2), and all the variables ﬁxed to zero at the current node
should be excluded from the set V in Step 2.

6If upon termination, an integral z is obtained, we solve the relaxation to optimality, as discussed in the introduc-

tion of Section 3.

11

in Step 2 of Algorithm 2 essentially require computing a gradient of the least squares loss, in
order to construct V. Loosely speaking, gradient screening is designed to avoid computing the
“non-essential” parts of this gradient by using previously computed quantities in the BnB tree.

In the following proposition, we give an explicit way to construct the set V in Step 2 of Algorithm

2.

Proposition 3. Let ˆβ and V be as deﬁned in Algorithm 2, and deﬁne ˆr = y − X ˆβ. Then, the set
V can be equivalently written as follows:

V = {i ∈ Supp( ˆβ)c | |(cid:104)ˆr, Xi(cid:105)| > c(λ0, λ2, M )},

(17)

where Xi denotes the i-th column of X; and c(λ0, λ2, M ) = 2
c(λ0, λ2, M ) = (λ0/M + λ2M ) if (cid:112)λ0/λ2 > M .

√

λ0λ2 if (cid:112)λ0/λ2 ≤ M , and

By Proposition 3, constructing V directly costs O(n(p − (cid:107) ˆβ(cid:107)0)). Next, we will discuss how to
compute V with a lower cost, by making use of previously computed quantities. Suppose we have
access to a set ˆV ⊆ [p] such that V ⊆ ˆV. Then, we can construct V by restricting the checks in (17)
to ˆV instead of Supp( ˆβ)c, i.e., the following holds:

V = {i ∈ ˆV | |(cid:104)ˆr, Xi(cid:105)| > c(λ0, λ2, M )}.

(18)

Assuming ˆV is available, the cost of computing V in (18) is O(n| ˆV|). This cost can be signiﬁcantly
smaller than that of (17), if | ˆV| is suﬃciently small. Next, we present a method that can obtain a
relatively small ˆV in practice, thereby speeding up the computation of V.
Computation of ˆV: Proposition 4 presents a method to construct a set ˆV which satisﬁes V ⊆ ˆV
(as discussed above), using information from a warm start β0 (e.g., the solution of the relaxation
from the parent node in BnB).

Proposition 4. Let ˆβ and V be as deﬁned in Algorithm 2. Let β0 be an arbitrary vector in Rp.
Deﬁne ˆr = y − X ˆβ, r0 = y − Xβ0, and (cid:15) = (cid:107)Xβ0 − X ˆβ(cid:107)2. Then, the following holds:

V ⊆ ˆV :=

(cid:111)
(cid:110)
i ∈ Supp( ˆβ)c | |(cid:104)r0, Xi(cid:105)| > c(λ0, λ2, M ) − (cid:15)
.

(19)

The set ˆV deﬁned in (19) depends solely on (cid:15) and the quantities |(cid:104)r0, Xi(cid:105)|, i ∈ Supp( ˆβ)c, which
we will assume to be sorted and available along with the warm start β0. This is in contrast
to a direct evaluation of V in (17), which requires the costly computation of the terms |(cid:104)ˆr, Xi(cid:105)|,
i ∈ Supp( ˆβ)c. Given the sorted values |(cid:104)r0, Xi(cid:105)|, i ∈ Supp( ˆβ)c, we can identify ˆV in (19) with
O(log p) operations, using a variant of binary search. Thus, the overall cost of computing V using
(18) is O(n| ˆV| + log p). We also note that in practice the inclusion V ⊆ ˆV in (19) is strict.

Proposition 4 tells us that the closer Xβ0 is to X ˆβ, the smaller ˆV is, i.e., the lower is the cost
of computing V in (18). Thus, for the method to be successful, we need a good warm start β0. In
practice, we obtain β0 for the current node in BnB from its parent, and we update β0 as necessary
during the course of Algorithm 2 (as detailed below). Let (cid:15)gs ∈ (0, 1) be a pre-speciﬁed parameter
for the gradient screening procedure. The procedure, which replaces Step 2 of Algorithm 2, is
deﬁned as follows:

Gradient Screening

12

1. If this is the root node and the ﬁrst iteration of Algorithm 2, then: update β0 ← ˆβ, r0 =
y − Xβ0; compute and sort the terms |(cid:104)r0, Xi(cid:105)|, i ∈ [p]; compute V using (17); and skip all
the steps below.

2. If this is the ﬁrst iteration of Algorithm 2, get β0 from the parent node in the BnB tree.

3. Compute ˆV using (19) and then V using (18)7.

4. If | ˆV| > (cid:15)gsp, update β0 ← ˆβ, r0 = y − Xβ0; re-compute and sort the terms |(cid:104)r0, Xi(cid:105)|, i ∈ [p].

If Xβ0 is not a good estimate of X ˆβ, then ˆV might become large. To avoid this issue, we update
β0 in Step 4 above, every time the set ˆV becomes relatively large (| ˆV| > (cid:15)gsp). The parameter (cid:15)gs
controls how often β0 is updated. In our implementation, we use (cid:15)gs = 0.05 by default, but we note
that the parameter can be generally tuned in order to improve the running time. The updated β0
will be passed to the children in the BnB tree. While Step 4 above can be costly, it is not performed
often in practice as the solutions of the relaxations in the parent and children nodes are typically
close.

Based on our current implementation and experiments, we see notable beneﬁts from gradient
screening when p ≥ 104 (e.g., more than 2x speedup for p = 106). For smaller values of p, we
typically observe a small additional overhead from gradient screening. Moreover, we note that
gradient screening will increase memory consumption, because each of the open nodes in the tree
will need to store a p-dimensional vector (consisting of the quantities |(cid:104)r0, Xi(cid:105)|, i ∈ [p], which are
maintained by gradient screening).

3.2 Dual Bounds

In practice, we use Algorithm 2 to obtain inexact primal solutions for relaxation (5), as discussed
in Remark 1. However, dual bounds are needed to perform search space pruning in BnB. Here,
we present a new eﬃcient method to obtain dual bounds from the primal solutions. Moreover, we
prove that our method can obtain dual bounds whose tightness depends on the sparsity level of the
relaxation rather than p. We start by introducing a Lagrangian dual of relaxation (5) in Theorem
2.
Theorem 2. For (cid:112)λ0/λ2 ≤ M , a dual of Problem (5) is given by:

max
α∈Rn,γ∈Rp

h1(α, γ) := −

1
2

(cid:107)α(cid:107)2

2 − αT y −

(cid:88)

i∈[p]

v(α, γi),

where v : Rn+1 → R is deﬁned as follows:

v(α, γi) :=

(cid:104) (αT Xi − γi)2
4λ2

− λ0

(cid:105)

+

+ M |γi|.

Otherwise, if (cid:112)λ0/λ2 > M , a dual of Problem (5) is given by

max
ρ∈Rn,µ∈Rp

h2(ρ, µ) := −

1
2

(cid:107)ρ(cid:107)2

2 − ρT y − M (cid:107)µ(cid:107)1

s.t.

|ρT Xi| − µi ≤ λ0/M + λ2M, i ∈ [p].

(20)

(21)

(22)

7Each variable i ﬁxed to zero by BnB at the current node should be excluded when computing V and ˆV.

13

Let β∗ be an optimal solution to Problem (5) and deﬁne r∗ = y − Xβ∗. The optimal dual variables
for (20) are given by:

α∗ = −r∗ and γ∗

i = 1[|β∗

i |=M ](α∗T Xi − 2M λ2 sign(α∗T Xi)), i ∈ [p].

Moreover, the optimal dual variables for (22) are:

ρ∗ = −r∗

and µ∗

i = 1[|β∗

i |=M ](|ρ∗T Xi| − λ0/M − λ2M ), i ∈ [p].

(23)

(24)

Note that strong duality holds for relaxation (5) since it satisﬁes Slater’s condition [9], so the

optimal objective of the dual in Theorem 2 matches that of (5).
Dual Feasible Solutions: Let ˆβ be an inexact primal solution obtained using Algorithm 2 and
deﬁne ˆr = y − X ˆβ. We discuss next how to construct a dual feasible solution using ˆβ, i.e., without
solving the dual in Theorem 2.
Case of (cid:112)λ0/λ2 ≤ M : Here, we construct a dual solution (ˆα, ˆγ) for Problem (20) as follows:

ˆα = −ˆr and ˆγ ∈ arg max

γ∈Rp

h1(ˆα, γ).

(25)

The choice ˆα = −ˆr is motivated by the optimality conditions in Theorem 2, while ˆγ maximizes
the dual objective (with α ﬁxed to ˆα). Note that the constructed solution is (trivially) feasible
since the dual in (20) is unconstrained. Since (20) is separable across the γi’s, ˆγi is equivalently
the solution of minγi∈R v(ˆα, γi), whose corresponding solution is given by:

ˆγi = T (ˆαT Xi; 2M λ2, ∞).

(26)

√

Thus, ˆγ can be obtained in closed-form using (26). Since ˆβ is the output of Algorithm 2, we
know that the corresponding set V in Algorithm 2 is empty. Thus, by Proposition 3, we have
λ0λ2 for any i ∈ Supp( ˆβ)c. Using ˆr = −ˆα and (cid:112)λ0/λ2 ≤ M in the previous in
|ˆrT Xi| ≤ 2
inequality, we get |ˆαT Xi| ≤ 2M λ2. Using the latter bound in (26), we get that ˆγi = 0 for any
i ∈ Supp( ˆβ)c. However, for i ∈ Supp( ˆβ), ˆγi can be potentially nonzero.
Case of (cid:112)λ0/λ2 > M : We construct a dual feasible solution (ˆρ, ˆµ) for (22) as follows:

ˆρ = −ˆr and ˆµ ∈ arg max

µ∈Rp

h2(ˆρ, µ)

s.t.

(ˆρ, µ) is feasible for (22).

(27)

Similar to (27), the choice ˆρ = −ˆr is motivated by the optimality conditions in Theorem 2, whereas
the choice ˆµ maximizes the dual objective under the condition that ρ = −ˆr (while ensuring feasi-
bility). It can be readily seen that ˆµi is given in closed form by:

ˆµi =

(cid:104)
|ˆρT Xi| − λ0/M − λ2M

(cid:105)

+

.

(28)

Note that for any i ∈ Supp( ˆβ)c, we have |ˆρT Xi| = |ˆrT Xi| ≤ λ0/M + λ2M (by Proposition 3), which
implies that ˆµi = 0. However, for i ∈ Supp( ˆβ), ˆµi can be potentially nonzero.
Quality of the Dual Bounds: In Theorem 3, we quantify the tightness of the dual bounds
obtained from the dual feasible solutions (25) and (27).

14

Theorem 3. Let α∗, γ∗, ρ∗, and µ∗ be the optimal dual variables deﬁned in Theorem 2. Let β∗
be an optimal solution to (5), and ˆβ be an inexact solution obtained using Algorithm 2. Deﬁne the
primal gap (cid:15) = (cid:107)X(β∗ − ˆβ)(cid:107)2. Let k = (cid:107) ˆβ(cid:107)0 denote the number of nonzeros in the inexact solution
to (5). For a ﬁxed (M, λ2), the following holds for the dual solution (ˆα, ˆγ) deﬁned in (25):

h1(ˆα, ˆγ) ≥ h1(α∗, γ∗) − kO((cid:15)) − kO((cid:15)2),

and for the dual solution (ˆρ, ˆµ) deﬁned in (27), we have:

h2(ˆρ, ˆµ) ≥ h2(ρ∗, µ∗) − kO((cid:15)) − O((cid:15)2).

(29)

(30)

Interestingly, the bounds established in Theorem 3 do not depend on p, but rather on the support
size k. Speciﬁcally, the constants in O((cid:15)) and O((cid:15)2) only involve M and λ2 (these constants are
made explicit in the proof). In practice, we seek highly sparse solutions, i.e., k (cid:28) p—suggesting that
the quality of the dual bounds deteriorates with k and not p. The main driver behind these tight
bounds is Algorithm 2, which performs optimality checks on the coordinates outside the support.
If vanilla CD was used instead of Algorithm 2, then the term k appearing in the bounds (29) and
(30) will be replaced by p, making the bounds loose8.
A direct computation of the dual bound
Eﬃcient Computation of the Dual Bounds:
h1(ˆα, ˆγ) or h2(ˆρ, ˆµ) costs O(np) operations. Interestingly, we show that this cost can be reduced to
O(n + n(cid:107) ˆβ(cid:107)0) (where we recall that ˆβ is a solution from Algorithm 2). First, we consider the case
of (cid:112)λ0/λ2 ≤ M , where the goal is to compute h1(ˆα, ˆγ). By Lemma 2 (in the appendix), we have
v(ˆα, ˆγi) = 0 for every i ∈ Supp( ˆβ)c. Thus, h1(ˆα, ˆγ) can be simpliﬁed to:

h1(ˆα, ˆγ) = −

1
2

(cid:107)ˆα(cid:107)2

2 − ˆαT y −

(cid:88)

v(ˆα, ˆγi).

i∈Supp( ˆβ)

(31)

Now, we consider the case of (cid:112)λ0/λ2 > M , where the goal is to evaluate h2(ˆρ, ˆµ). By construction,
the solution (27) is dual feasible, which means that the constraints in (22) need not be checked
when computing the bound. Moreover, ˆµi = 0 for every i ∈ Supp( ˆβ)c (see the discussion after
(28)). Thus, the dual bound can be expressed as follows:

h2(ˆρ, ˆµ) = −

1
2

(cid:107)ˆρ(cid:107)2

2 − ˆρT y − M

(cid:88)

|ˆµi|.

i∈Supp( ˆβ)

(32)

The expressions in (31) and (32) can be computed in O(n + n(cid:107) ˆβ(cid:107)0) operations.

3.3 Branching and Incumbents

Many branching strategies for BnB have been explored in the literature, e.g., random branching,
strong branching, and pseudo-cost branching [8, 15, 4, 1]. Among these strategies, strong branching
has proven to be very eﬀective in minimizing the size of the search tree [4, 15, 8]. Strong branching
selects the variable which leads to the maximum increase in the lower bounds of the children
nodes. To select such a variable, two temporary node subproblems should be solved for every
non-integral variable in the current relaxation. This can become a computational bottleneck, as
each temporary subproblem involves solving a nonlinear optimization problem similar to (5). To

8This holds by using the argument in the proof of Theorem 3 for Algorithm 1 instead of Algorithm 2.

15

address this challenge, we use a fast (approximate) version of strong branching, in which we restrict
the optimization in these temporary subproblems to the active set of the current node (instead of
optimizing over all p variables). In practice, this often leads to very similar search trees compared
to exact strong branching, since the active set of the parent is typically close to the support of the
children.

We obtain the initial upper bound using L0Learn [32], which uses CD and eﬃcient local search
algorithms to obtain good quality feasible solutions for Problem (3). Moreover, at every node in
the BnB tree, we attempt to improve the incumbent by making use of the support of a solution
to the node relaxation. Speciﬁcally, we solve the following (cid:96)2 regularized least squares problem
restricted to the relaxation’s support S: minβS ∈R|S|
2. Since S is typically
small, this problem can be solved eﬃciently by inverting a small |S| × |S| matrix. If S is similar
to the support of the parent node, then a solution for the current node can be computed via a
low-rank update [29].

1
2 (cid:107)y − XSβS(cid:107)2

2 + λ2(cid:107)βS(cid:107)2

4 Experiments

We perform a series of high-dimensional experiments to study the run time of our BnB, understand
its sensitivity to parameter and algorithm choices, and compare to state-of-the-art approaches.
While our dataset and parameter choices are motivated by statistical considerations, our focus here
is not to study the statistical properties of (cid:96)0 estimators. We refer the reader to [32] for empirical
studies on statistical properties.

4.1 Experimental Setup

Synthetic Data Generation: We generate a multivariate Gaussian data matrix with samples
drawn from MVN(0, Σp×p), a sparse coeﬃcient vector β† ∈ Rp with k† equi-spaced nonzero entries
iid∼ N (0, σ2). We denote the support of the true regression
all set to 1, and a noise vector (cid:15)i
coeﬃcients β† by S†. The response is then obtained from the linear model y = Xβ† + (cid:15). We deﬁne
the signal-to-noise ratio (SNR) as follows: SNR= Var(Xβ†)/σ2. Unless otherwise speciﬁed, we
set σ2 to achieve SNR= 5—this is a relatively diﬃcult setting which still allows for full support
recovery, under suitable choices of n, p, and Σ (see [32, 40] for a discussion on appropriate levels
of SNR). We perform mean-centering followed by normalization (to have unit (cid:96)2 norm) on y and
each column of X. To help in exposition, we denote the resulting processed dataset by (˜y, ˜X) and
the (scaled) regression coeﬃcients by ˜β†.
Warm Starts, λ0, λ2, and M : The parameters λ2 and M can aﬀect the run time signiﬁcantly, so
we study the sensitivity to these choices in our experiments. We consider choices that are relevant
from a statistical perspective, as we discuss next. For a ﬁxed λ2, let β(λ2) be the solution of ridge
regression restricted to the support of the true solution ˜β†:

β(λ2) ∈ arg min

β∈Rp

1
2

(cid:107)˜y − ˜Xβ(cid:107)2

2 + λ2(cid:107)β(cid:107)2
2

s.t.

β(S†)c = 0,

We deﬁne λ∗

2 as the λ2 which minimizes the (cid:96)2 estimation error of β(λ2), i.e.,

λ∗
2 ∈ arg min

λ2≥0

(cid:107) ˜β† − β(λ2)(cid:107)2.

16

(33)

(34)

In the experiments, we report our λ2 choices as a fraction or multiple of λ∗

We estimate λ∗
2 using grid search, with 50 points equi-spaced on a logarithmic scale in the range
[10−4, 104].
2 (e.g.,
λ2 = 0.1λ∗
2). Moreover, for each λ2, we deﬁne M ∗(λ2) = (cid:107)β(λ2)(cid:107)∞ and report our choices of the
Big-M in terms of M ∗(λ2)—we also use the notation M ∗ to refer to M ∗(λ2) when λ2 is clear from
context. Note that if Problem (1) has a unique solution, and this solution has support S†, then
M ∗(λ2) is the smallest valid choice of M in formulation (3). Unless otherwise speciﬁed, for each
λ2 considered, we ﬁx λ0 to a value λ∗
0 (which is a function of λ2) that leads to the true support
size k†. We obtain λ∗
0 might not exist in general, but in all the
experiments we considered, L0Learn was able to such a value. Moreover, Section 4.2.3, presents an
experiment where λ0 is varied over a range that is independent of L0Learn.
We obtain warm starts from L0Learn10 and use them for all the MIP solvers considered.
Solvers and Settings: Our solver, L0BnB11, is written in Python with critical code sections
optimized using Numba [35]. We compare L0BnB with Gurobi (GRB), MOSEK (MSK), and
BARON (B) on formulation (3). We also compare with [13] who solve the cardinality-constrained
variant of (1) with the number of nonzeros set to k†. In all solvers (including L0BnB), we use the
following settings:

0 using L0Learn9 [32]. Note that λ∗

• Relative Optimality Gap: Given an upper bound UB and a lower bound LB, the relative

optimality gap is deﬁned as (UB − LB)/UB. We set this to 1%.

• Integer Feasibility Tolerance ((cid:15)if): This tolerance is used to determine whether a variable
obtained from a node relaxation will be declared as integral (for the purpose of branching).
Speciﬁcally, in our context, if a variable zi is within (cid:15)if from 0 or 1, then it is declared as
integral. We set (cid:15)if = 10−4.

• Primal-Dual Optimality Gap ((cid:15)pd): This is the relative gap between the primal and dual
bounds at a given node, which is used as termination criterion for the subproblem solver.
We set (cid:15)pd = 10−5. In L0BnB, this gap is satisﬁed when an integral solution to a node’s
relaxation is encountered.

Gradient Screening in L0BnB: As discussed in Section 3.1.2, our gradient screening implementation
leads to noticeable speed-ups for large p (≥ 104 in our experience). For smaller values of p, we do
not observe run time improvements. In the experiment of Section 4.2.1 (Table 6), where we vary
p, we report the running time with and without gradient screening. In the other experiments, we
do not use gradient screening.

Reproducibility and Additional Details: In the spirit of reproducibility, the function used to
generate and process the synthetic datasets is publicly available on L0BnB’s Github page. In all
experiments, we report the values of M , λ0, and λ2 (either in the main text or in Appendix C).
Moreover, for all approaches, we report the number of BnB nodes explored in Appendix C.

9L0Learn computes a data-dependent grid of λ0 values. When CD is used to optimize (1), each λ0 in the grid

leads to a diﬀerent support size. See Section 4.1 of [32] for more details.

10We use the default CD-based algorithm in L0Learn, which does not depend on any Big-M parameter. We remind

the reader that L0Learn is a local optimization method that does not provide certiﬁcates of global optimality.

11https://github.com/alisaab/l0bnb

17

4.2 Comparison with State-of-the-art solvers

4.2.1 Varying Number of Features

In this section, we study the scalability of the diﬀerent solvers in terms of the number of features p.
We generate synthetic datasets12 with n = 103, p ∈ {103, 104, 105, 106}, and k† = 10. We consider a
constant correlation setting, where Σij = 0.1 ∀i (cid:54)= j and 1 otherwise. The parameters λ2 and M can
have a signiﬁcant eﬀect on the run time. Thus, we report the timings for diﬀerent choices of these
parameters. In particular, in Table 1 (top panel), we report the timings for λ2 ∈ {0.1λ∗
2},
where for each λ2, we ﬁx M = 1.5M ∗(λ2) (where λ∗
2 and M ∗ are deﬁned in Section 4.1). In Table
1 (bottom panel), we ﬁx λ2 = λ∗
2 and report the timings for M ∈ {M ∗(λ2), 2M ∗(λ2), 4M ∗(λ2), ∞}.
Note that the results for L0BnB in Table 1 are without gradient screening; in Table 6 in Appendix
C.1, we report the timings with gradient screening enabled. Moreover, in Appendix C.1, we report
the values of λ∗

2, along with the number of BnB nodes explored.

0 and λ∗

2, 10λ∗

2, λ∗

Table 1:
(Sensitivity to λ2 and M ) Running time in seconds for solving (1) (via formulation (3)) by
our proposal (L0BnB), Gurobi (GRB), MOSEK (MSK), and BARON (B). The method [13] solves the
cardinality-constrained variant of (1). For methods that do not terminate in 4 hours: the optimality gap is
shown in parenthesis, and a dash (-) is used in the special case of a 100% gap. [Top panel] For each λ2, we
use M = 1.5M ∗(λ2).
2, and four diﬀerent values of M . The Big-M values
are based on: M ∗(λ∗
2) = 0.243. The true solution satisﬁes:
(cid:107) ˜β†(cid:107)∞ = 0.208. In the bottom panel, we found that PR(M ∗) and PR(∞) have the same optimal solution.

[Bottom panel] We use λ2 = λ∗

2) = 0.164, and M ∗(10λ∗

2) = 0.232, M ∗(0.1λ∗

λ2 = λ∗
2

λ2 = 0.1λ∗
2

λ2 = 10λ∗
2

p L0BnB GRB MSK B
103 0.7
70
104
3
105
34
106 1112

92 (4%) (34%)
2
12
(78%)
-
-
(86%) 545
-

[13] L0BnB GRB MSK B [13] L0BnB GRB MSK B [13]
28 (5%) 0.08
-
-
-
-
-
-
-

0.01
148
0.06 (11%) 314
-
0.5
-
8

154 -
(12%) 3872 -
-
-

(15%) 1697

5
8
46

(23%)

57

-
-

-
-

-
-

-
-

-
-

-

M = M ∗

M = 2M ∗

M = 4M ∗

M = ∞

27

p L0BnB GRB MSK B L0BnB GRB MSK B L0BnB GRB MSK B L0BnB GRB MSK B
103 0.2
0.8
128 -
-
104 0.9 10571 665
5
9025 -
(24%) 3260 -
105
-
-
-
81
106 121
-
-
- 10986

0.8 3974 91
1219 137 -
-
6
(50%) 3289 -
-
-
81
-
- 11010

57 (2%) 0.8
5
-
70
-
9265
-

112

-
-

-
-

-
-

-
-

-
-

-
-

8

The results in the top panel of Table 1 indicate signiﬁcant speed-ups, reaching over 200, 000x
compared to Gurobi, 28, 000x compared to MOSEK, and 20, 000x compared to [13]. At λ2 = λ∗
2,
L0BnB is the only solver that can handle p ≥ 105, and can, in fact, handle p = 106 in less than
20 minutes. Recall that λ∗
2 minimizes the (cid:96)2 estimation error and leads to an estimator that is the
closest to the ground truth. When the ridge parameter is weak i.e., for λ2 = 0.1λ∗
2, L0BnB is again
the only solver that can handle p ≥ 105. When the ridge parameter is large, i.e., for λ2 = 10λ∗
2, the

12To ensure that the same estimates of λ∗

2 and M ∗ are used for the diﬀerent choices of p, we generate a single data
matrix with 106 features. For each p, we take a submatrix that consists of all the rows and a subset of p columns
(which includes the true support).

18

optimization problem seems to become easier: L0BnB can be more than 100x faster compared to
λ∗
2, and [13] can handle up to p ≈ 106. The speed-ups for λ2 = 10λ∗
2 can be attributed to the fact
that a larger λ2 adds a large amount of regularization to the objective (via the perspective term)—
improving the performance of the relaxation solvers13. It is also worth emphasizing that L0BnB
is prototyped in Python, as opposed to the highly eﬃcient BnB routines available in commercial
solvers such as Gurobi and MOSEK.

Ideally, we desire a solver that can solve Problem (1) over a range of λ2 values, which includes
values in the neighborhood of λ∗
2. However, the results in Table 1 suggest that the state-of-the-art
methods (except L0BnB) seem to only work for quite large values of λ2 (which, in this case, do
not correspond to solutions that are interesting from a statistical viewpoint). On the other hand,
L0BnB seems to be the only method that can scale to p ∼ 106 while being relatively robust to the
choice of λ2.

In the bottom panel of Table 1, the results also indicate that L0BnB signiﬁcantly outperforms
Gurobi, MOSEK, and BARON for diﬀerent choices of M . For all the solvers, the run time increases
with M , and the longest run times are for M = ∞. This empirically validates our result in
Proposition 1, where we show that for a suﬃciently small (but valid) value of M , PR(M ) can be
better than PR(∞), in terms of the relaxation quality. However, even with M = ∞, L0BnB can
solve p = 106 in around 3 hours, whereas all other solvers have a 100% gap after 4 hours. We also
note that Table 1 considers both the two settings in Theorem 1: (cid:112)λ0/λ2 > M and (cid:112)λ0/λ2 ≤ M .
Speciﬁcally, we have (cid:112)λ∗
2), 1.5M ∗(λ∗
2 ≤ M for
M ∈ {4M ∗(λ∗
2), ∞}. L0BnB achieves notable improvements over other solvers for both of these
settings.

2 > M for M ∈ {M ∗(λ∗

2)}, and (cid:112)λ∗

2), 2M ∗(λ∗

0/λ∗

0/λ∗

4.2.2 Varying Signal-to-Noise Ratio (SNR)

Here we investigate the eﬀect of SNR on the running time of the diﬀerent solvers. To this end, we
generate synthetic datasets with n = 103, p = 103, k† = 10, under a constant correlation setting,
where Σij = 0.1 ∀i (cid:54)= j and 1 otherwise. We vary SNR in {0.5, 1, 2, 3, 4, 5}. We set λ0 = λ∗
0, λ2 = λ∗
2
2 depend on SNR), and M = 1.5M ∗(λ2). We report the running time versus SNR
(where λ∗
0 and λ∗
in Table 2. The corresponding values of λ∗
2, and the number of BnB nodes are reported
in Appendix C.2. The results indicate that L0BnB is much faster, and less sensitive to changes
in SNR, compared to the other solvers. For example, L0BnB can handle SNR=0.5 in less than 4
minutes, whereas the fastest competing solver (MOSEK) takes around 46 minutes.

0 and λ∗

4.2.3 Varying λ0 and λ2

In the experiment of Section 4.2.1, we studied the running time for λ0 = λ∗
0 so that the model
recovers the true support size. In this experiment, we study the running time over a grid of λ0 and
λ2 values, which includes various support sizes. We consider synthetic instances with p ∈ {103, 104},
n = 103, k† = 10, under a constant correlation setting, where Σij = 0.1 ∀i (cid:54)= j and 1 otherwise.
We vary λ2 ∈ {0.1, 0.5, 1, 2, 10} · λ∗
2 and λ0 ∈ {0.5, 0.1, 0.01} · λm
0 is a value of λ0 which
sets all coeﬃcients to zero. Following [32]14, we use λm
∞. Next, we describe,

0 = (2 + 4λ2)−1(cid:107)X T y(cid:107)2

0 , where λm

13Gurobi is the only exception to this observation. We investigated this: Gurobi generates additional cuts only for

the case of 10λ∗

2, which seem to slow down the relaxation solver.

14[32] shows that λm

0 is the smallest choice of λ0 for which β = 0 is a coordinate-wise minimizer for Problem (1).

In this experiment, we veriﬁed numerically that β = 0 is a global minimizer at λm
0 .

19

Table 2: Running time (seconds) for solving (3) on a synthetic dataset with n = p = 103, at diﬀerent SNR
levels. The parameter λ2 is set to the optimal choice λ∗
2, which depends on the current SNR level. For
methods that do not terminate in 2 hours: the optimality gap is shown in parenthesis, and a dash (-) is used
in the special case of a 100% gap.

SNR M L0BnB GRB MSK B

0.5 0.269 231 (2%) 2757 -

1

2

3

4

5

0.307 1.7

1213 1046 -

0.333 1.4

0.346 1.3

0.347 1.0

0.348 0.8

119

102

77

77

288 -

173 -

113 -

92

-

how given a pair (λ0, λ2) in the grid, we estimate the corresponding Big-M value M (λ0, λ2). Let
β(λ0, λ2) be the solution of Problem (1) restricted to the true support:

β(λ0, λ2) ∈ arg min

β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ0(cid:107)β(cid:107)0 + λ2(cid:107)β(cid:107)2

2 s.t. β(S†)c = 0.

We compute β(λ0, λ2) exactly using formulation (3) with M = ∞. We then compute an estimate of
the Big-M value: ˜M (λ0, λ2) := (cid:107)β(λ0, λ2)(cid:107)∞. For every (λ0, λ2) in the grid, we solve Problem (3)
for M (λ0, λ2) = 1.5 ˜M (λ0, λ2). In Table 3, we report the running time for p = 103 (top panel) and
p = 104 (bottom panel). The values of λ∗
2 and λ0, along with the number of BnB nodes explored,
are reported in Appendix C.3.

In Table 3, for all values of λ2 considered and λ0 ∈ {0.5, 0.1} · λm

0 , L0BnB is faster than the
competing methods, with speed-ups exceeding 10,000x compared to both Gurobi and MOSEK.
However, for λ0 = 0.01λm
0 , none of the solvers are able to solve the problem in 1 hour, and the gaps
seem to be comparable. We also note that for p = 104, L0BnB is able to solve (to optimality) 13
out of the 20 instances in the 1 hour limit. In contrast, Gurobi could not solve any of the instances
and MOSEK could only solve 6.

4.3 Sensitivity to Data Parameters

2 and M = 1.5M ∗(λ∗

We study how L0BnB’s running time is aﬀected by the following data speciﬁc parameters: number
of samples (n), feature correlations (Σ), and number of nonzero coeﬃcients (k†). In the experiments
below, we ﬁx λ2 = λ∗
2). For all the experiments in this section, the values of
λ0, λ2, and M used are reported in Appendix C.4.
Number of Samples: We ﬁx k† = 10, p = 104, and consider a constant correlation setting
Σij = 0.1 for i (cid:54)= j and 1 otherwise15. The timings for n ∈ {102, 103, 104, 105} are in Table 4. The
results indicate that the problem can be solved in reasonable times (order of seconds to minutes)
for n ≥ 103. The problems can be solved the fastest when n is close to p, and the extreme cases
n = 102 and n = 105 are the slowest. We contend that for large n, the CD updates (which cost
O(n) each) become a bottleneck. For n = 102, the CD updates are cheaper, however, the size of the

15We choose p = 104 to ensure that the matrix ﬁts into memory when n is large.

20

Table 3: Running time for solving (3) over a grid of λ0 and λ2 values. “NNZ” is the number of nonzeros
in the solution to (3) (or the best incumbent if all solvers cannot terminate in 1 hour). For methods that
do not terminate in 1 hour: the optimality gap is shown in parenthesis, and a dash (-) is used in the special
case of a 100% gap. The true solution satisﬁes: (cid:107) ˜β†(cid:107)∞ = 0.208.

p = 103

λ2

10λ∗
2

2λ∗
2

λ∗
2

0.5λ∗
2

0.1λ∗
2

λ2

10λ∗
2

2λ∗
2

λ∗
2

0.5λ∗
2

0.1λ∗
2

λ0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0

λ0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0
0.5λm
0
0.1λm
0
0.01λm
0

NNZ M L0BnB GRB
(3%)
(5%)
(17%)
651
68
(5%)
2648
109
(11%)
965
63
(16%)
697
58
(32%)

1
0.01
(12%)
3
0.1
(3%)
20
0.3
(10%)
59
1
(19%)
128
1
(47%)

0.293
0.245
0.245
0.491
0.332
0.332
0.524
0.348
0.348
0.542
0.356
0.356
0.557
0.364
0.364

5
10
36
4
10
15
3
10
10
3
10
10
3
10
10

MSK B
-
158
-
5
-
(16%)
-
2737
-
106
-
(4%)
-
1278
-
65
-
(6%)
-
(6%)
-
172
-
(12%)
-
(8%)
-
303
-
(55%)

p = 104

NNZ M L0BnB GRB MSK B
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

1617
62
(7%)
(13%)
2459
(15%)
(18%)
3338
(24%)
(26%)
3483
(36%)
(19%)
(46%)
(77%)

2
0.1
(3%)
263
0.7
(15%)
1920
2
(29%)
(3%)
5
(42%)
(8%)
24
(73%)

-
-
-
-
-
-
-
-
-
(29%)
-
-
-
-
-

0.293
0.245
0.245
0.491
0.332
0.332
0.524
0.348
0.348
0.542
0.356
0.356
0.557
0.364
0.364

5
10
45
8
10
17
3
10
14
3
10
14
3
10
13

21

search tree is signiﬁcantly larger—suggesting that a small value of n can lead to loose relaxations
and require more branching. Note also that the underlying statistical problem is the most diﬃcult
for n = 102 compared to other larger values of n.
Feature Correlations: We generate synthetic datasets with p ∈ {104, 105}, n = 103, k† = 10,
and Σ = Diag(Σ(1), Σ(2), . . . , Σ(k†)) is block-diagonal. Such block structures are commonly used
in the sparse learning literature [55]. Each Σ(l) is a correlation matrix with the same dimension.
Given a correlation parameter ρ ∈ (0, 1), for each l ∈ [k†], we assume that Σ(l)
ij = ρ|i−j| for any
i, j ∈ [p/k†], i.e., the correlation is exponentially decaying in each block. We report the timings for
diﬀerent values of the parameter ρ in Table 4. The results indicate that higher correlations lead
to an increase in the run time, but even the high correlation instances can be solved in reasonable
time. For example, when p = 104, ρ = 0.9 can be solved in less than 10 minutes. When p = 105,
ρ = 0.8 can be solved in around 13 minutes, and ρ = 0.9 can be solved to a 13% gap in 2 hours.
Sparsity of the Regression Coeﬃcients: We consider datasets with n = 103, p = 104, and
the same correlation setting as the experiment for the number of samples. The results in Table 4
show that problems with 15 nonzeros can be handled in 166 seconds, and for 20 and 25 nonzeros,
decent gaps (≤ 10%) can be obtained in 2 hours. We also note that for larger λ2 or tighter M
choices, larger values of k† can be handled.

Table 4: Run time in seconds (denoted by t) for L0BnB. If L0BnB does not solve the problem in a 2 hour
time limit, the optimality gap is shown in parenthesis.

Varying n

n 102 103 104 105

t (42%) 11 30 1701

Varying correlation coeﬃcient ρ

ρ

0.1 0.3 0.5 0.7 0.8 0.9

t(p = 104) 4 4 5 16 55

530

t(p = 105) 35 39 60 231 808 (13%)

Varying k†

k† 10 15

20

25

t 4 166 (10%) (6%)

4.4 Real Data and Ablation Studies (Algorithm Settings)

High-dimensional Real Data: Here we investigate the run time of L0BnB on the Riboﬂavin
dataset [16]—a genetics dataset used for predicting Vitamin B2 production levels. The original
dataset has p = 4088 and n = 71. We augment the dataset with pairwise feature interactions
to get p = 8, 362, 004. We mean center and normalize the response and the columns of the data
matrix. We then run 5-fold cross-validation in L0Learn (with default parameters) to ﬁnd the
optimal regularization parameters ˆλ0 and ˆλ2. We set M to be 10 times the (cid:96)∞ norm of the solution
obtained from cross-validation. In L0BnB, we solve the problem for λ2 ∈ {0.1ˆλ2, ˆλ2} and vary λ0
to generate solutions of diﬀerent support sizes. The run time, for each λ2, as a function of the
support size is reported in Figure 2. Interestingly, at ˆλ2, all the support sizes obtained (up to 15
nonzeros) can be handled in less than a minute. Moreover, the increase in time is relatively slow
as the number of nonzeros increases. When λ2 becomes smaller (i.e., λ2 = 0.1ˆλ2), the run times
increase (though they are reasonable) as the problem becomes more diﬃcult. When an optimal
solution has six nonzeros, L0BnB for λ2 = 0.1ˆλ2 is approximately 20x slower than λ2 = ˆλ2.
Ablation Study: We perform an ablation study to measure the eﬀect of key choices in our BnB
on the run time. Particularly, we consider the following changes: (i) replacing our relaxation solver

22

with MOSEK, (ii) turning oﬀ warm starts in our solver, and (iii) turning oﬀ active sets in our
solver. We measure the run time before and after these changes on synthetic data with n = 103,
k† = 10, Σij = 0.1 for all i (cid:54)= j and 1 otherwise. We use the parameters λ0 = λ∗
2, and
M = 1.5M ∗(λ∗
2) (with the same values as those used in the experiment of Section 4.2.1). The
run times for diﬀerent choices of p are reported in Table 5. The results show that replacing our
relaxation solver with MOSEK will slow down the BnB by more than 1200x at p = 104. The results
for MOSEK are likely to be even slower for p = 106 as it still had a 100% gap after 2 hours. We
note that MOSEK employs a state-of-the-art conic optimization solver (based on an interior point
method). The signiﬁcant speed-ups here can be attributed to our CD-based algorithm, which is
designed to eﬀectively exploit the sparsity structure in the problem. The results also indicate that
warm starts and active sets are important for run time, e.g., removing warm starts and active sets
at p = 105 can slow down the algorithm by more than 16x and 67x, respectively.

0, λ2 = λ∗

Table 5: Time (seconds) after the following changes
to our relaxation solver: (i) replacing it with MOSEK,
(ii) removing warm starts, and (iii) removing active
sets. Gap is shown in parenthesis if the method does
not terminate in 2 hours.

p

104

L0BnB

3

105

34

106

1112

(i)

(ii)

(iii)

3802 (13%) (100%)

25

66

560

(21%)

2291

(23%)

5 Conclusion

Figure 2: L0BnB run time on a real genomics dataset
(Riboﬂavin) with n = 71 and p ≈ 8.3 × 106.

We considered the exact computation of estimators from the (cid:96)0(cid:96)2-regularized least squares problem.
While current approaches for this problem rely on commercial MIP-solvers, we propose a highly
specialized nonlinear BnB framework for solving the problem. A key workhorse in our approach is
a fast coordinate descent procedure to solve the node relaxations along with active set updates and
gradient screening, which exploit information across the search tree for computational eﬃciency.
Moreover, we proposed a new method for obtaining dual bounds from our primal coordinate descent
solutions and showed that the quality of these bounds depend on the sparsity level, rather than the
number of features p. Our experiments on both real and synthetic data indicate that our method
exhibits over 5000x speedups compared to the fastest solvers, handling high-dimensional instances
with p = 8.3 × 106 in the order of seconds to few minutes. Our method appears to be more robust
to the choices of the regularization parameters and can handle diﬃcult statistical problems (e.g.,
relatively high correlations or small number of samples).

Our work demonstrates for the ﬁrst time that carefully designed ﬁrst-order methods can be
highly eﬀective within a BnB framework; and can perhaps, be applied to more general mixed
integer programs involving sparsity. There are multiple directions for future work.
[6] recently
showed that safe screening rules, which eliminate variables from the optimization problem, can be

23

a very eﬀective preprocessing step for (cid:96)0(cid:96)2 regularized regression. One promising direction is to
extend such rules to dynamically eliminate variables during the course of BnB. Another important
direction is to develop specialized methods that can dynamically infer and tighten the Big-M values
used in our formulation.

Acknowledgements

Hussein Hazimeh acknowledges research support from the Oﬃce of Naval Research ONR-N000141812298.
Rahul Mazumder acknowledges research funding from the Oﬃce of Naval Research ONR-N000141812298
(Young Investigator Award), the National Science Foundation (NSF-IIS-1718258) and IBM. The
authors would like to thank the referees for their constructive comments, which led to an improve-
ment in the paper.

References

[1] Achterberg, T., Koch, T., Martin, A.: Branching rules revisited. Operations Research Letters

33(1), 42–54 (2005)

[2] Akt¨urk, M.S., Atamt¨urk, A., G¨urel, S.: A strong conic quadratic reformulation for machine-job
assignment with controllable processing times. Operations Research Letters 37(3), 187–191
(2009)

[3] Andersen, E.D., Roos, C., Terlaky, T.: On implementing a primal-dual interior-point method

for conic quadratic optimization. Mathematical Programming 95(2), 249–277 (2003)

[4] Applegate, D., Bixby, R., Cook, W., Chv´atal, V.: On the solution of traveling salesman

problems (1998)

[5] Atamturk, A., Gomez, A.: Rank-one convexiﬁcation for sparse regression. arXiv preprint

arXiv:1901.10334 (2019)

[6] Atamturk, A., Gomez, A.: Safe screening rules for l0-regression from perspective relaxations.
In: H.D. III, A. Singh (eds.) Proceedings of the 37th International Conference on Machine
Learning, Proceedings of Machine Learning Research, vol. 119, pp. 421–430. PMLR (2020).
URL http://proceedings.mlr.press/v119/atamturk20a.html

[7] Beck, A., Eldar, Y.C.: Sparsity constrained nonlinear optimization: Optimality conditions and
algorithms. SIAM Journal on Optimization 23(3), 1480–1509 (2013). DOI 10.1137/120869778.
URL https://doi.org/10.1137/120869778

[8] Belotti, P., Kirches, C., Leyﬀer, S., Linderoth, J., Luedtke, J., Mahajan, A.: Mixed-integer

nonlinear optimization. Acta Numerica 22, 1–131 (2013)

[9] Bertsekas, D.: Nonlinear Programming. Athena scientiﬁc optimization and computation series.

Athena Scientiﬁc (2016). URL https://books.google.com/books?id=TwOujgEACAAJ

[10] Bertsimas, D., King, A., Mazumder, R.: Best subset selection via a modern optimization lens.

The Annals of Statistics 44(2), 813–852 (2016)

24

[11] Bertsimas, D., Pauphilet, J., Van Parys, B.: Sparse classiﬁcation: a scalable discrete optimiza-

tion perspective. arXiv preprint arXiv:1710.01352 (2017)

[12] Bertsimas, D., Pauphilet, J., Van Parys, B.: Sparse regression: Scalable algorithms and em-

pirical performance. arXiv preprint arXiv:1902.06547 (2019)

[13] Bertsimas, D., Van Parys, B., et al.: Sparse high-dimensional regression: Exact scalable algo-

rithms and phase transitions. The Annals of Statistics 48(1), 300–323 (2020)

[14] Blumensath, T., Davies, M.E.: Iterative hard thresholding for compressed sensing. Applied

and computational harmonic analysis 27(3), 265–274 (2009)

[15] Bonami, P., Lee, J., Leyﬀer, S., W¨achter, A.: More branch-and-bound experiments in convex
nonlinear integer programming. Preprint ANL/MCS-P1949-0911, Argonne National Labora-
tory, Mathematics and Computer Science Division (2011)

[16] B¨uhlmann, P., Kalisch, M., Meier, L.: High-dimensional statistics with a view toward appli-

cations in biology (2014)

[17] Cozad, A., Sahinidis, N.V., Miller, D.C.: Learning surrogate models for simulation-based
optimization. AIChE Journal 60(6), 2211–2227 (2014). DOI https://doi.org/10.1002/aic.
14418. URL https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.14418

[18] Dakin, R.J.: A tree-search algorithm for mixed integer programming problems. The computer

journal 8(3), 250–255 (1965)

[19] David, G., Ilias, Z.: High dimensional regression with binary coeﬃcients. estimating squared

error and a phase transtition. In: Conference on Learning Theory, pp. 948–953 (2017)

[20] Dedieu, A., Hazimeh, H., Mazumder, R.: Learning sparse classiﬁers: Continuous and mixed

integer optimization perspectives. arXiv preprint arXiv:2001.06471 (2020)

[21] Dong, H., Chen, K., Linderoth, J.: Regularization vs. Relaxation: A conic optimization per-

spective of statistical variable selection. ArXiv e-prints (2015)

[22] Duran, M.A., Grossmann, I.E.: An outer-approximation algorithm for a class of mixed-integer

nonlinear programs. Mathematical programming 36(3), 307–339 (1986)

[23] Fletcher, A.K., Rangan, S., Goyal, V.K.: Necessary and suﬃcient conditions for sparsity
pattern recovery. IEEE Transactions on Information Theory 55(12), 5758–5772 (2009)

[24] Frangioni, A., Gentile, C.: Perspective cuts for a class of convex 0–1 mixed integer programs.

Mathematical Programming 106(2), 225–236 (2006)

[25] Friedman, J., Hastie, T., Tibshirani, R.: Regularization paths for generalized linear models
via coordinate descent. Journal of Statistical Software 33(1), 1–22 (2010). URL http://www.
jstatsoft.org/v33/i01/

[26] Greenshtein, E., et al.: Best subset selection, persistence in high-dimensional statistical learn-

ing and optimization under l1 constraint. The Annals of Statistics 34(5), 2367–2386 (2006)

25

[27] G¨unl¨uk, O., Linderoth, J.: Perspective reformulations of mixed integer nonlinear programs

with indicator variables. Mathematical programming 124(1-2), 183–205 (2010)

[28] Gurobi Optimization, I.: Gurobi optimizer reference manual. URL http://www. gurobi. com

(2020)

[29] Hammarling, S., Lucas, C.: Updating the qr factorization and the least squares problem.

Manchester Institute for Mathematical Sciences, University of Manchester (2008)

[30] Hart, W.E., Watson, J.P., Woodruﬀ, D.L.: Pyomo: modeling and solving mathematical pro-

grams in python. Mathematical Programming Computation 3(3), 219–260 (2011)

[31] Hastie, T., Tibshirani, R., Tibshirani, R.J.: Extended comparisons of best subset selection,

forward stepwise selection, and the lasso. arXiv preprint arXiv:1707.08692 (2017)

[32] Hazimeh, H., Mazumder, R.: Fast best subset selection: Coordinate descent and local com-
binatorial optimization algorithms. Operations Research 68(5), 1517–1537 (2020). DOI
10.1287/opre.2019.1919. URL https://doi.org/10.1287/opre.2019.1919

[33] Hazimeh, H., Mazumder, R.: Learning hierarchical interactions at scale: A convex optimization
approach. In: International Conference on Artiﬁcial Intelligence and Statistics, pp. 1833–1843.
PMLR (2020)

[34] Hong, M., Wang, X., Razaviyayn, M., Luo, Z.Q.: Iteration complexity analysis of block coor-

dinate descent methods. Mathematical Programming 163(1-2), 85–114 (2017)

[35] Lam, S.K., Pitrou, A., Seibert, S.: Numba: A llvm-based python jit compiler. In: Proceedings
of the Second Workshop on the LLVM Compiler Infrastructure in HPC, pp. 1–6 (2015)

[36] Lee, J., Leyﬀer, S.: Mixed integer nonlinear programming, vol. 154. Springer Science &

Business Media (2011)

[37] Lubin, M., Yamangil, E., Bent, R., Vielma, J.P.: Extended formulations in mixed-integer con-
vex programming. In: International Conference on Integer Programming and Combinatorial
Optimization, pp. 102–113. Springer (2016)

[38] Mazumder, R., Friedman, J.H., Hastie, T.: Sparsenet: Coordinate descent with nonconvex
penalties. Journal of the American Statistical Association 106(495), 1125–1138 (2011). DOI
10.1198/jasa.2011.tm09738. URL https://doi.org/10.1198/jasa.2011.tm09738. PMID:
25580042

[39] Mazumder, R., Radchenko, P., Dedieu, A.: Subset selection with shrinkage: Sparse linear

modeling when the snr is low. arXiv preprint arXiv:1708.03288 (2017)

[40] Mazumder, R., et al.: Discussion of “best subset, forward stepwise or lasso? analysis and
recommendations based on extensive comparisons”. Statistical Science 35(4), 602–608 (2020)

[41] Miyashiro, R., Takano, Y.: Subset selection by mallows’ cp: A mixed integer programming

approach. Expert Systems with Applications 42(1), 325–331 (2015)

[42] Natarajan, B.K.: Sparse approximate solutions to linear systems. SIAM journal on computing

24(2), 227–234 (1995)

26

[43] Nesterov, Y.: Gradient methods for minimizing composite functions. Mathematical Program-

ming 140(1), 125–161 (2013)

[44] Owen, A.B.: A robust hybrid of lasso and ridge regression. Contemporary Mathematics 443(7),

59–72 (2007)

[45] Pilanci, M., Wainwright, M.J., El Ghaoui, L.: Sparse learning via boolean relaxations. Math-

ematical Programming 151(1), 63–87 (2015)

[46] Raskutti, G., Wainwright, M.J., Yu, B.: Minimax rates of estimation for high-dimensional
IEEE transactions on information theory 57(10), 6976–6994

linear regression over lq-balls.
(2011)

[47] Studio-CPLEX, I.I.C.O.: Users manual-version 12 release 6.

IBM ILOG CPLEX Division:

Incline Village, NV, USA (2013)

[48] Tawarmalani, M., Sahinidis, N.V.: A polyhedral branch-and-cut approach to global optimiza-

tion. Mathematical programming 103(2), 225–249 (2005)

[49] Tseng, P.: Convergence of a block coordinate descent method for nondiﬀerentiable mini-
mization. Journal of Optimization Theory and Applications 109(3), 475–494 (2001). DOI
10.1023/A:1017501703105. URL http://dx.doi.org/10.1023/A:1017501703105

[50] Vielma, J.P., Ahmed, S., Nemhauser, G.L.: A lifted linear programming branch-and-bound
INFORMS Journal on Computing

algorithm for mixed-integer conic quadratic programs.
20(3), 438–450 (2008)

[51] Vielma, J.P., Dunning, I., Huchette, J., Lubin, M.: Extended formulations in mixed integer
conic quadratic programming. Mathematical Programming Computation 9(3), 369–418 (2017)

[52] Wainwright, M.J.: Information-theoretic limits on sparsity recovery in the high-dimensional
and noisy setting. IEEE Transactions on Information Theory 55(12), 5728–5741 (2009)

[53] Xie, W., Deng, X.: Scalable algorithms for the sparse ridge regression. SIAM Journal on

Optimization 30(4), 3359–3386 (2020)

[54] Zhang, Y., Wainwright, M.J., Jordan, M.I.: Lower bounds on the performance of polynomial-
time algorithms for sparse linear regression. In: Conference on Learning Theory, pp. 921–948.
PMLR (2014)

[55] Zhao, P., Yu, B.: On model selection consistency of lasso. Journal of Machine learning research

7(Nov), 2541–2563 (2006)

27

A Proofs of Technical Results

Proof of Theorem 1: The interval relaxation of (3) can be expressed as:

min
β

(cid:110) 1
2

(cid:107)y − Xβ(cid:107)2

2 +

(cid:88)

i∈[p]

min
si,zi

(λ0zi + λ2si)

(cid:111)

β2
i ≤ sizi, i ∈ [p]
− M zi ≤ βi ≤ M zi, i ∈ [p]
zi ∈ [0, 1], si ≥ 0, i ∈ [p]

(35a)

(35b)

(35c)

(35d)

Let ω(βi; λ0, λ2, M ) := minsi,zi(λ0zi + λ2si) s.t. (35b), (35c), (35d). Next we obtain an expression
for ω(βi; λ0, λ2, M ).
Let (βi, zi, si) be a feasible solution for (35). Note that ˆzi := max{ β2
M } is the smallest possible
i
si
value of zi, which satisﬁes constraints (35b) and (35c) (for the case of βi = si = 0, we deﬁne
β2
i /si = 0). Thus, the objective value corresponding to (βi, ˆzi, si) is less than or equal to that of
a feasible solution (βi, zi, si). This implies that we can replace constraints (35b) and (35c) with
the constraint zi = max{ β2
M } without changing the optimal objective of the problem. This
i
si
replacement leads to:

, |βi|

, |βi|

ω(βi; λ0, λ2, M ) = min
si,zi

(λ0zi + λ2si)

s.t.

zi = max

(cid:110) β2
i
si

,

|βi|
M

(cid:111)

, zi ∈ [0, 1], si ≥ 0.

In the above, we can eliminate the variable zi, leading to:

ω(βi; λ0, λ2, M ) = min

si

(cid:110)

max

λ0

(cid:124)

β2
i
si

+ λ2si

(cid:123)(cid:122)
Case I

, λ0
(cid:124)

(cid:125)

(cid:111)

|βi|
M

+ λ2si
(cid:123)(cid:122)
(cid:125)
Case II

s.t. si ≥ β2

i , |βi| ≤ M.

(36)

β2
i
si

Suppose that Case I attains the maximum in (36). This holds if si ≤ |βi|M . The function
λ0
+ λ2si is convex in si, and the optimality conditions of this function imply that the optimal
if (cid:112)λ0/λ2 ≤ |βi| ≤ M . Plugging
solution is s∗
s∗
i into (36) leads to ω(βi; λ0, λ2, M ) = 2λ0B(βi

i = |βi|(cid:112)λ0/λ2 if |βi| ≤ (cid:112)λ0/λ2 ≤ M and s∗

(cid:112)λ2/λ0), assuming (cid:112)λ0/λ2 ≤ M .

i = β2
i

Now suppose Case II attains the maximum in (36). This holds if si ≥ |βi|M . The function
|βi|
M + λ2si is monotonically increasing in si, where si is lower bounded by |βi|M and β2
i . But we
i (since |βi| ≤ M ), which implies that the optimal solution is s∗
i = |βi|M .
into (36) we get ω(βi; λ0, λ2, M ) = (λ0/M + λ2M )|βi|, and this holds as long as

λ0
always have |βi|M ≥ β2
Substituting s∗
i
(cid:112)λ0/λ2 > M .

Proof of Proposition 1: First, we show (7). Note that VB(M ) can be simpliﬁed to:

VB(M ) = min

(cid:107)β(cid:107)∞≤M

H(β) :=

1
2

(cid:107)y − Xβ(cid:107)2

2 +

(cid:16) λ0
M

(cid:88)

i∈[p]

|βi| + λ2β2
i

(cid:17)

.

(37)

Recalling Theorem 1 and the deﬁnition of F (β) in (5), note that for (cid:112)λ0/λ2 > M :

VPR(M ) − VB(M ) ≥ F (β∗) − H(β∗) =

(cid:110)

ψ2(β∗

i ; λ0, λ2, M ) −

|β∗

i | − λ2(β∗

i )2(cid:111)

λ0
M

(M |β∗

i | − (β∗

i )2)

(cid:88)

i∈[p]
(cid:88)

=λ2

i∈[p]

28

where the inequality holds since β∗, an optimal solution to (5), is feasible for (37). This establishes
(7).

We now show (8). Since |β∗

λ0λ2
(this corresponds to the ﬁrst case in (4)). Also recall that VPR(∞) = minβ G(β), where G(β) is
deﬁned in (6) (this follows from applying Theorem 1 with M = ∞). The following then holds:

i | ≤ M and (cid:112)λ0/λ2 > M , we must have ψ1(β∗

i ; λ0, λ2) = 2|β∗
i |

√

VPR(M ) − VPR(∞) ≥ F (β∗) − G(β∗) =

(cid:88)

i∈[p]

(cid:110)

ψ2(β∗

i ; λ0, λ2, M ) − ψ1(β∗

i ; λ0, λ2)

(cid:111)

where the inequality holds since β∗, an optimal solution to (5), is feasible for (6).

=h(λ0, λ2, M )(cid:107)β∗(cid:107)1,

(38)

Proof of Proposition 2:
in (6), and that VB(M ) is deﬁned in (37). Deﬁne a function t : R → R as follows

First, we recall that VPR(∞) = minβ G(β), where G(β) is deﬁned

t(βi) := 2λ0B(βi

(cid:112)λ2/λ0) −

λ0
M

|βi| − λ2β2
i .

Next, we prove (10).
Proof of (10): Suppose that M ≤ 1
2
and the deﬁnition of B, we have B(βi

(cid:112)λ0/λ2 and |βi| ≤ M . Using the fact that |βi| ≤ (cid:112)λ0/λ2
(cid:112)λ2/λ0) = (cid:112)λ2/λ0|βi|. This leads to:
(cid:16)

(cid:17)

t(βi) =

(cid:112)

2

λ0λ2 − λ0
M

|βi| − λ2β2

i ≤ 0,

(39)

where the inequality follows since 2
optimal solution to (37). Then, the following holds:

λ0λ2 − λ0/M ≤ 0 for M ≤ 1
2

√

(cid:112)λ0/λ2. Now, let β† be an

VB(M ) − VPR(∞) ≥ H(β†) − G(β†) = −

(cid:88)

t(β†

i ) ≥ 0,

i∈[p]

(40)

where the ﬁrst inequality follows from VB(M ) = H(β†) and VPR(∞) ≤ G(β†). The second inequality
in (40) follows from (39). This establishes (10).
To show (11), we will need the following lemma.
Lemma 1. Let M ≥ (cid:112)λ0/λ2, then for any βi ∈ [−M, M ], we have t(βi) ≥ 0.
Proof of Lemma 1. Suppose that M ≥ (cid:112)λ0/λ2 and |βi| ≤ M . There are two cases to consider
here: Case (i): |βi| ≤ (cid:112)λ0/λ2 and Case (ii): |βi| ≥ (cid:112)λ0/λ2.
For Case (i), from the deﬁnition of B, we have 2λ0B(βi

λ0λ2|βi|. Therefore,

√

t(βi) =

(cid:16)

2

(cid:112)

λ0λ2 −

λ0
M

(cid:112)λ2/λ0) = 2
(cid:17)

|βi| − λ2β2
i .

In the above, it is easy to check that for M ≥ (cid:112)λ0/λ2 and |βi| ≤ (cid:112)λ0/λ2, we have t(βi) ≥ 0. Now
, which is
for Case (ii), we have 2λ0B(βi
non-negative since we assume that |βi| ≤ M . This establishes Lemma 1.

(cid:112)λ2/λ0) = λ0 + λ2β2

i —this leads to t(βi) = λ0

1 − |βi|
M

(cid:16)

(cid:17)

Proof of (11): Deﬁne v∗(M ) = min(cid:107)β(cid:107)∞≤M G(β) (recall, G(β) is deﬁned in (6)) and let β∗ be an
optimal solution i.e., v∗(M ) = G(β∗). Suppose that M ≥ (cid:112)λ0/λ2. Then, the following holds:

v∗(M ) − VB(M ) ≥ G(β∗) − H(β∗) =

t(β∗

i ) ≥ 0

(cid:88)

i∈[p]

(41)

29

where the ﬁrst inequality holds since β∗ is a feasible solution to (37) so VB(M ) ≤ H(β∗), and the
last inequality is due to Lemma 1.

Now suppose that λ2 ∈ L(M ) (deﬁned in (9)) and let ˆβ ∈ S(λ2) (i.e., ˆβ is optimal for (6))
be such that it satisﬁes (cid:107) ˆβ(cid:107)∞ ≤ M . Since VPR(∞) ≤ v∗(M ) and ˆβ is feasible for the problem
corresponding to v∗(M ), then v∗(M ) = G( ˆβ). But by (41) we have v∗(M ) ≥ VB(M ), which com-
bined with v∗(M ) = G( ˆβ), leads to G( ˆβ) ≥ VB(M ). This establishes (11) (since by deﬁnition
G( ˆβ) = VPR(∞)); and completes the proof of Proposition 2.

Fix some i ∈ Supp( ˆβ)c. Recall that the one-dimensional problem:
Proof of Proposition 3:
min|βi|≤M F ( ˆβ1, . . . , βi, . . . , ˆβp) is equivalent to Problem (13), where ˜βi = (cid:104)y − (cid:80)
j(cid:54)=i Xj ˆβj, Xi(cid:105).
Since ˆβi = 0, we have ˜βi = (cid:104)ˆr, Xi(cid:105) where, ˆr = y − X ˆβ. For (cid:112)λ0/λ2 ≤ M , (14) implies that the
λ0λ2. Similarly, for (cid:112)λ0/λ2 > M , by (15), the solution of
solution of (13) is nonzero iﬀ | ˜βi| > 2
(13) is nonzero iﬀ | ˜βi| > λ0/M + λ2M . Using these observations in the deﬁnition of V in Algorithm
2, leads to the result of the proposition.

√

Proof of Proposition 4: Fix some i ∈ V. Note that

|(cid:104)ˆr, Xi(cid:105) − (cid:104)r0, Xi(cid:105)| = |(cid:104)Xβ0 − X ˆβ, Xi(cid:105)| ≤ (cid:107)Xβ0 − X ˆβ(cid:107)2(cid:107)Xi(cid:107)2 ≤ (cid:15).

Using the triangle inequality and the bound above, we get:

|(cid:104)ˆr, Xi(cid:105)| ≤ |(cid:104)r0, Xi(cid:105)| + |(cid:104)ˆr, Xi(cid:105) − (cid:104)r0, Xi(cid:105)| ≤ |(cid:104)r0, Xi(cid:105)| + (cid:15).

Therefore, if i ∈ V, i.e., |(cid:104)ˆr, Xi(cid:105)| > c(λ0, λ2, M ), then |(cid:104)r0, Xi(cid:105)| + (cid:15) > c(λ0, λ2, M ), implying that
i ∈ ˆV, as deﬁned in (19).

Proof of Theorem 2: Problem (5) can be written equivalently as:

min
β∈Rp,r∈Rn

1
2

(cid:107)r(cid:107)2

2 +

(cid:88)

i∈[p]

ψ(βi; λ0, λ2, M ) s.t. r = y − Xβ,

|βi| ≤ M, ∀i ∈ [p].

(42)

Case of (cid:112)λ0/λ2 ≤ M : We ﬁrst consider the case when (cid:112)λ0/λ2 ≤ M . We dualize all the

constraints in (42), leading to the following Lagrangian dual:

max
α∈Rn,η∈Rp

≥0

min
β∈Rp,r∈Rn

L(β, r, α, η)

(43)

where L(β, r, α, η) is the Lagrangian function deﬁned as follows:

L(β, r, α, η) :=

1
2

(cid:107)r(cid:107)2

2 +

(cid:88)

i∈[p]

ψ1(βi; λ0, λ2) + αT (r − y + Xβ) +

(cid:88)

i∈[p]

ηi(|βi| − M ).

Next, we discuss how to solve the inner minimization problem in (43). Let us write:

min
β,r

L(β, r, α, η) = min

r

(cid:110) 1
2

(cid:107)r(cid:107)2

2 + αT r

(cid:111)

+

(cid:88)

i∈[p]

(cid:110)

Di(βi)

(cid:111)

− αT y −

min
βi

(cid:88)

i∈[p]

M ηi

(44)

where Di(βi) := ψ1(βi; λ0, λ2)+αT Xiβi +ηi|βi|. Note that the minimizer wrt r is given by ˜r∗ = −α.
In what follows, we consider some i ∈ [p] and derive an optimal solution ˜β∗
i of minβi Di(βi). There
are two cases to consider here.

30

Case 1: |βi| ≤ (cid:112)λ0/λ2. Here, ψ1(βi; λ0, λ2) = 2

√

λ0λ2|βi|. Note that

˜β∗
i = arg min

βi

Di(βi) =

(cid:40)
0
−(cid:112)λ0/λ2 sign(αT Xi)

√

if

2

λ0λ2 + ηi − |αT Xi| ≥ 0

otherwise

and as we will see, the second case above is a special case of Case 2 below.
Case 2: |βi| ≥ (cid:112)λ0/λ2. In this case, ψ1(βi; λ0, λ2) = λ2β2
i + λ0. Note that
i + αT Xiβi + ηi|βi| = T (−αT Xi; ηi, ∞)/(2λ2)

˜β∗
i = arg min

Di(βi) = arg min

λ2β2

βi

βi

(45)

(46)

i | ≥ (cid:112)λ0/λ2 in
where, T above is the soft-thresholding operator [25]. But ˜β∗
this case. Using the deﬁnition of T (.), the latter condition can be written as: (|αT Xi| − ηi)/(2λ2) ≥
(cid:112)λ0/λ2, which simpliﬁes to |αT Xi| − ηi ≥ 2

i in (46) should satisfy | ˜β∗

i ) can be written as:

√

λ0λ2. In this case, Di( ˜β∗
(cid:17)2

+ λ0

if

|αT Xi| − ηi ≥ 2

(cid:112)

λ0λ2.

(47)

Di( ˜β∗

i ) = −

(cid:16)

1
4λ2

|αT Xi| − ηi

Combining (45) and (47), we have: Di( ˜β∗
of Di( ˜β∗

i ) along with ˜r∗ = −α in (43), we get the following dual problem:

i ) = −

(cid:104) (|αT Xi|−ηi)2
4λ2

− λ0

(cid:105)

+

. Plugging the above expression

max
α∈Rn,η∈Rp

≥0

−

1
2

(cid:107)α(cid:107)2

2 − αT y −

p
(cid:88)

i=1

(cid:104) (|αT Xi| − ηi)2
4λ2

− λ0

(cid:105)

−

+

p
(cid:88)

i=1

M ηi.

(48)

Note we can drop the non-negativity constraint ηi ≥ 0 above. Using a new variable γ in place

of η (note that ηi = |γi| for all i), Problem (48) can be reformulated as:

max
α∈Rn,γ∈Rp

−

1
2

(cid:107)α(cid:107)2

2 − αT y −

p
(cid:88)

i=1

(cid:104) (αT Xi − γi)2
4λ2

− λ0

(cid:105)

+

−

p
(cid:88)

i=1

M |γi|

(49)

which is the formulation in (20).

i | < M then η∗

i = 0 and consequently γ∗

We now show (23). Let (α∗, η∗) be an optimal solution of (48). First, it is easy to see r∗ =
arg minr L(β∗, r, α∗, η∗) = −α∗. For the second part of (23), note by complementary slackness: if
|β∗
i | = M > 0.
We ﬁrst consider the case of β∗
i = M , α = α∗, and
i sign(−α∗T Xi))/(2λ2), where sign(α∗T Xi) = −1. Using this along
η = η∗, we get M = (−α∗T Xi −η∗
with the fact that γ∗
i = α∗T Xi+2λ2M = α∗T Xi−2λ2M sign(α∗T Xi).
Using this identity along with a similar argument for βi = −M , we arrive at the expression in (23).
Case of (cid:112)λ0/λ2 > M : The proof for this case follows along the lines similar to what was

i = 0. Next, we will derive γ∗
i = M . Using (46) with the identiﬁcations: ˜β∗

i sign(α∗T Xi), we obtain γ∗

i for the case of |β∗

i = η∗

shown above. We omit the proof.

The following lemma is useful for proving Theorem 3.
Lemma 2. Suppose (cid:112)λ0/λ2 ≤ M . Let ˆβ be a solution from Algorithm 2 and (ˆα, ˆγ) be the corre-
sponding dual solution deﬁned in (25). Let β∗ and r∗ be as deﬁned in Theorem 2, and deﬁne the
primal gap (cid:15) = (cid:107)X(β∗ − ˆβ)(cid:107)2. Then, for every i ∈ Supp( ˆβ)c, we have v(ˆα, ˆγi) = 0 (see (21) for
deﬁnition of v); and for every i ∈ Supp( ˆβ), we have

v(ˆα, ˆγi) ≤ ci(cid:15) + (4λ2)−1(cid:15)2 + v(α∗, γ∗

i ),

(50)

where ci = (2λ2)−1 if |β∗

i | < M , and ci = M if |β∗

i | = M .

31

Proof of Lemma 2. Fix some i ∈ Supp( ˆβ)c. Since ˆβ is the output of Algorithm 2, the set V in
Algorithm 2 must be empty. Thus, using Proposition 3, we have: |ˆrT Xi| ≤ 2
λ0λ2. As ˆr = −ˆα,
and consequently using the deﬁnition of ˆγ in (26), we have ˆγi = 0. Thus,

√

(ˆαT Xi − ˆγi)2/(4λ2) = (ˆrT Xi)2/(4λ2) ≤ λ0,

which implies that v(ˆα, ˆγi) = 0.

Now ﬁx some i ∈ Supp( ˆβ), and let a := (ˆα − α∗)T Xi and b := α∗T Xi − γ∗

deﬁnition of h1 in (20), we have ˆγi = arg minγi v(ˆα, γi), which leads to v(ˆα, ˆγi) ≤ v(ˆα, γ∗
upper bound on v(ˆα, ˆγi) can be then obtained as follows:

i . By (25) and the
i ). An

v(ˆα, ˆγi) ≤ v(ˆα, γ∗

i ) =

=

=

≤

≤

(cid:104) (ˆαT Xi − γ∗

i )2

4λ2

− λ0

(cid:105)

+

+ M |γ∗
i |

(cid:104) ((ˆα − α∗)T Xi + α∗T Xi − γ∗

i )2

4λ2

(cid:104) a2 + 2ab + b2
4λ2

− λ0

(cid:105)

+

+ M |γ∗
i |

− λ0

(cid:105)

+

+ M |γ∗
i |

a2 + 2|a||b|
4λ2
a2 + 2|a||b|
4λ2

+

(cid:104) b2
4λ2

− λ0

(cid:105)

+

+ M |γ∗
i |

+ v(α∗, γ∗

i ).

(51)

Next, we obtain upper bounds on |a| and |b|. By the Cauchy-Schwarz inequality, we have |a| ≤
(cid:107)X(β∗ − ˆβ)(cid:107)2(cid:107)Xi(cid:107)2 = (cid:15)(cid:107)Xi(cid:107)2. Since the columns of X are normalized, the bound simpliﬁes to
|a| ≤ (cid:15). Since, (cid:107)y(cid:107)2 = 1 and α∗ = −r∗ (see Theorem 2), we have:

1
2

(cid:107)α∗(cid:107)2

2 =

1
2

(cid:107)r∗(cid:107)2

2 ≤ F (β∗) ≤ F (0) =

1
2

(cid:107)y(cid:107)2

2 =

1
2

,

implying that (cid:107)α∗(cid:107)2 ≤ 1.
We now present a bound on |b| = |α∗T Xi − γ∗

i | by considering two cases:

Case 1: (|β∗
|b| = |α∗T Xi| ≤ (cid:107)α∗(cid:107)2(cid:107)Xi(cid:107)2 ≤ 1.

i | < M ): From the deﬁnition of γ∗
i

in (23), we have γ∗

i = 0, which leads to

Case 2: (|β∗

i | = M ): By (23), γ∗

i = α∗T Xi − 2M λ2 sign(α∗T Xi), which leads to |b| ≤ 2M λ2.

Plugging |a| ≤ (cid:15) and the bounds on |b| (above) into (51), we arrive to (50).
Proof of Theorem 3: We consider two cases based on (cid:112)λ0/λ2 ≤ M or (cid:112)λ0/λ2 > M .
Showing bound (29): First, we consider the case of (cid:112)λ0/λ2 ≤ M , i.e., we will establish the
bound in (29). Note that the following holds:

1
2

(cid:107)ˆα(cid:107)2

2 + ˆαT y =

=

≤

1
2
1
2
1
2

(cid:107)ˆα − α∗ + α∗(cid:107)2

2 + (ˆα − α∗ + α∗)T y

(cid:107)ˆα − α∗(cid:107)2

2 + (ˆα − α∗)T α∗ +

2 + (ˆα − α∗)T y + α∗T y

(cid:15)2 + (cid:15)(cid:107)α∗(cid:107)2 +

1
2

(cid:107)α∗(cid:107)2

(cid:107)α∗(cid:107)2

1
2
2 + (cid:15)(cid:107)y(cid:107)2 + α∗T y,

(52)

32

where the inequality above follows by applying Cauchy-Schwarz and noting that (cid:107)ˆα − α∗(cid:107)2 =
(cid:107)X(β∗ − ˆβ)(cid:107)2 = (cid:15). Using (cid:107)y(cid:107)2 = 1 and (cid:107)α∗(cid:107)2 ≤ 1 (this was established in the proof of Lemma 2)
in (52), we get:

1
2

(cid:107)ˆα(cid:107)2

2 + ˆαT y ≤

(cid:15)2 + 2(cid:15) +

1
2

1
2

(cid:107)α∗(cid:107)2

2 + α∗T y.

Plugging (53) into the objective function in (20) (with α = ˆα and γ = ˆγ):

h1(ˆα, ˆγ) ≥ −

1
2

(cid:107)α∗(cid:107)2

2 − α∗T y − 2(cid:15) −

(cid:15)2 −

1
2

(cid:88)

i∈[p]

v(ˆα, ˆγi).

(53)

(54)

By Lemma 2, for every i ∈ Supp( ˆβ)c, we have v(ˆα, ˆγi) = 0, which implies v(ˆα, ˆγi) ≤ v(α∗, γ∗
v is a non-negative function).

i ) (since

Using the inequality v(ˆα, ˆγi) ≤ v(α∗, γ∗

i ) for every i ∈ Supp( ˆβ)c; and inequality (50) for every

i ∈ Supp( ˆβ), in (54), we get:

h1(ˆα, ˆγ) ≥ −

1
2

(cid:107)α∗(cid:107)2

2 − α∗T y −

(cid:88)

i∈[p]

v(α∗, γ∗

i ) − 2(cid:15) −

(cid:15)2 −

1
2

(cid:88)

(cid:16)

ci(cid:15) + (4λ2)−1(cid:15)2(cid:17)

i∈Supp( ˆβ)

= h1(α∗, γ∗) − 2(cid:15) −

(cid:15)2 −

1
2

(cid:88)

(cid:16)

ci(cid:15) + (4λ2)−1(cid:15)2(cid:17)

.

i∈Supp( ˆβ)

(55)

Let

k1 = |{i ∈ Supp( ˆβ) | | ˆβi| < M }| and k2 = |{i ∈ Supp( ˆβ) | | ˆβi| = M }|.

Using the expressions for cis (from Lemma 2) in (55), we get:

h1(ˆα, ˆγ) ≥ h1(α∗, γ∗) − 2(cid:15) −

1
2

(cid:15)2 − k1

(cid:16)

(2λ2)−1(cid:15) + (4λ2)−1(cid:15)2(cid:17)

(cid:16)

M (cid:15) + (4λ2)−1(cid:15)2(cid:17)

− k2

Rearranging the terms in the above and using the fact that k = k1 + k2, we get:

h1(ˆα, ˆγ) ≥ h1(α∗, γ∗) − (cid:15)(2 + k1(2λ2)−1 + k2M ) − (cid:15)2(cid:16) 1
2

+

(cid:17)

,

k
4λ2

which leads to (29).
Showing bound (30): Now, we consider the case of (cid:112)λ0/λ2 > M , where we will establish the
bound in (30). By the same argument used in deriving (54), we have:

h2(ˆρ, ˆµ) ≥ −

1
2

(cid:107)ρ∗(cid:107)2

2 − ρ∗T y − 2(cid:15) −

1
2

(cid:15)2 − M (cid:107)ˆµ(cid:107)1.

Next, we ﬁx some i ∈ Supp( ˆβ) and we upper bound ˆµi as follows:

|ˆµi| =

(cid:104)
|ˆρT Xi| − λ0/M − λ2M

(cid:105)

+

(cid:104)
|(ˆρ − ρ∗)T Xi| + |ρ∗T Xi| − λ0/M − λ2M
≤

(cid:105)

≤|(ˆρ − ρ∗)T Xi| +

(cid:104)
|ρ∗T Xi| − λ0/M − λ2M

(cid:105)

+

+

≤(cid:15) + |µ∗

i |,

33

(56)

(57)

where in the last step above we use Cauchy-Schwarz for the ﬁrst term (noting that ρ∗ = −r∗); and
for the second term, we use Theorem 2 along with the following:

|ρ∗T Xi| ≤ λ0/M + λ2M if

|β∗

i | < M and |ρ∗T Xi| ≥ λ0/M + λ2M if

|β∗

i | = M.

(58)

i | < M , (24) implies
i = 0, which leads to |ρ∗T Xi| ≤ λ0/M + λ2M from the feasibility condition in (22). When
i | = M , we will establish (58) via contradiction: If |ρ∗T Xi| < λ0/M + λ2M holds true, then (24)

Conditions in (58) follow from the optimality of (ρ∗, µ∗) for (22). (For |β∗
that µ∗
|β∗
implies µ∗

i < 0, which would violate optimality for (22).)

For i ∈ Supp( ˆβ)c, we have ˆµi = 0 (see the discussion after (28)), which implies |ˆµi| ≤ |µ∗

i |. Using
i |, i ∈ Supp( ˆβ)c and (57) (for all i ∈ Supp( ˆβ)) in (56), and simplifying, we

the inequalities |ˆµi| ≤ |µ∗
get:

h2(ˆρ, ˆµ) ≥ h2(ρ∗, µ∗) − (cid:15)(2 + M k) − (cid:15)2/2,

(59)

which leads to (30).

B Additional Technical Details

B.1 Derivation of solutions to Problem (13)
First, we consider the setting of (cid:112)λ0/λ2 ≤ M . From Theorem 1, the penalty ψ(βi; λ0, λ2, M ) =
(cid:112)λ2/λ0). Using the deﬁnition of B in (4), we have:
ψ1(βi; λ0, λ2) = 2λ0B(βi

ψ1(βi; λ0, λ2) =

√

(cid:40)
2
λ2β2

λ0λ2|βi|
i + λ0

if |βi| ≤ (cid:112)λ0/λ2
if |βi| ≥ (cid:112)λ0/λ2.

√

√

√

|β∗

i = T ( ˜βi; 2

Case of |βi| ≤ (cid:112)λ0/λ2: The penalty in this case is 2

λ0λ2|βi|, and the ﬁrst-order optimality
conditions imply that the solution of Problem (13) is given by a capped version (due to the box
constraint on βi) of the soft thresholding operator [25]: β∗
λ0λ2, M ); and this is optimal
√
λ0λ2, M )| ≤ (cid:112)λ0/λ2. Therefore, if | ˜βi| ≤ (cid:112)λ0/λ2 +
i | ≤ (cid:112)λ0/λ2, i.e., |T ( ˜βi; 2
as long as:
√
λ0λ2, the solution is given by T ( ˜βi; 2
2
λ0λ2, M ).
Case of |βi| ≥ (cid:112)λ0/λ2: Here the penalty is λ2β2

i + λ0, and the solution is given by T ( ˜βi(1 +
2λ2)−1; 0, M ). The latter solution is optimal as long as |T ( ˜βi(1 + 2λ2)−1; 0, M )| ≥ (cid:112)λ0/λ2, which
can be simpliﬁed to | ˜βi| ≥ (cid:112)λ0/λ2 + 2
This completes the derivation of (14).
Finally, we consider the setting of (cid:112)λ0/λ2 > M . By Theorem 1, the penalty is ψ(βi; λ0, λ2, M ) =
(λ0/M + λ2M )|βi|. Thus, using arguments similar to the above, the solution is T ( ˜βi; λ0/M +
λ2M, M ). This completes the derivation of (15).

λ0λ2.

√

B.2 Node Subproblems

In Theorem 1, we presented a reformulation of the root relaxation in the β space. Here we discuss
how to similarly reformulate and solve the subproblem at an arbitrary node in the search tree.
Recall that at any node, some zis can be ﬁxed to 0 or 1 (this depends on the branching decisions
made until reaching the node). At a given node, let Z and N be the sets of indices of the zis that

34

are ﬁxed to 0 and 1, respectively. Then, the following convex subproblem needs to be solved at the
node:

(cid:107)y − Xβ(cid:107)2

2 + λ0

(cid:88)

zi + λ2

(cid:88)

si

i∈[p]

i∈[p]

1
2

min
β,z,s

s.t.

β2
i ≤ sizi, i ∈ [p]
− M zi ≤ βi ≤ M zi, i ∈ [p]
si ≥ 0, i ∈ [p]
zi = 0, i ∈ Z, zi = 1, i ∈ N , zi ∈ [0, 1], i ∈ [p] \ (Z ∪ N ).

(60)

Deﬁne the penalty ˜ψ(βi; λ0, λ2) := λ0 + λ2β2
of Theorem 1, Problem (60) can be equivalently expressed in the β space as:

i . Then, using an argument similar to that in the proof

˜F (β) :=

min
β∈Rp

1
2

(cid:107)y − Xβ(cid:107)2

2 +

(cid:88)

i∈N c

ψ(βi; λ0, λ2, M ) +

˜ψ(βi; λ0, λ2)

(cid:88)

i∈N

(61)

s.t. (cid:107)β(cid:107)∞ ≤ M, βi = 0, i ∈ Z.

Note that Problem (61) is similar to Problem (5), except that (i) the variables corresponding to
N use the penalty ˜ψ(βi; λ0, λ2) instead of ψ(βi; λ0, λ2, M ), and (ii) the variables corresponding
to Z are ﬁxed to zero. To optimize Problem (61) using cyclic CD, the coordinates in (N ∪ Z)c
are updated exactly as described in Section 3.1. Next, we discuss how cyclic CD updates the
coordinates in N . For any i ∈ N , the algorithm updates coordinate i by solving the problem:

˜F ( ˆβ1, . . . , βi, . . . , ˆβp)

s.t.

|βi| ≤ M.

min
βi∈R

Assuming the columns of X have unit (cid:96)2 norm, the problem above is equivalent to (13) but with
ψ(βi; λ0, λ2, M ) replaced with ˜ψ(βi; λ0, λ2); and the corresponding solution is given by: T ( ˜βi(1 +
2λ2)−1; 0, M ).

C Additional Experimental Results and Details

C.1 Experiment of Section 4.2.1

In Table 6, we report the running time of L0BnB, with and without gradient screening. The number
of nodes explored by the diﬀerent solvers is reported in Table 7. In this experiment: λ∗
2 = 0.0409;
and λ∗

0 is equal to 0.012, 0.0115, and 0.0132, for λ2 set to λ∗

2, respectively.

2, and 10λ∗

2, 0.1λ∗

C.2 Experiment of Section 4.2.2

The parameters λ0 and λ2, and the number of BnB nodes explored are reported in Table 8.

C.3 Experiment of Section 4.2.3

The parameter λ∗
in Table 9.

2 = 0.0409. The parameter λ0 and the number of BnB nodes explored are reported

35

Table 6: Running time in seconds for L0BnB, with and without gradient screening (GS). This experiment
is based on the same data and parameters as Table 1. Both approaches explore exactly the same BnB tree.

λ2 = λ∗
2

λ2 = 0.1λ∗
2

λ2 = 10λ∗
2

p No GS With GS No GS With GS No GS With GS
103 0.7
104
3.3
105
34
106 1112

1.6
11.5
545
(23%)

2.1
11.3
459
(8%)

0.011
0.06
0.5
8.1

0.011
0.05
0.5
7.3

0.9
2.9
19
414

M = M ∗

M = 2M ∗

M = 4M ∗

M = ∞

p No GS With GS No GS With GS No GS With GS No GS With GS
103 0.16
104
0.9
105
7.5
106
121

0.8
5.4
81
10986

0.8
5.5
81
11010

1.1
4.8
50
5639

1.1
4.8
50
5612

0.98
4.3
43
4640

0.8
4.9
70
9265

0.25
0.6
3.9
52

Table 7: Number of BnB nodes explored in the experiment of Section 4.2.1. For [13], we report the number
of cuts used by the cutting-plane algorithm. A dash indicates that the method could not solve more than 1
node in 4 hours.

λ2 = λ∗
2

λ2 = 0.1λ∗
2

λ2 = 10λ∗
2

159

532 161 2 9895

p L0BnB GRB MSK B [13] L0BnB GRB MSK B [13] L0BnB GRB MSK B [13]
103
104
105
385
106 1087

506 382 - 4769

357 1143 - 9

374 347 - 9

- 35563

- 5212

10104

- 10

- 11

4337

329

531

225

31

13

13

3

3

3

3

3

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

M = M ∗

M = 2M ∗

M = 4M ∗

M = ∞

49

65

48 3

p L0BnB GRB MSK B L0BnB GRB MSK B L0BnB GRB MSK B L0BnB GRB MSK B
103
104
105
106

2120 261 -

714 258 -

658 273 -

- 10059

- 8919

- 9805

1403 -

852 -

867 -

119

193

185

287

128

193

305

305

761

693

761

61

75

67

1

2

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

36

Table 8: The parameters λ0 and λ2, and the number of nodes explored by the diﬀerent solvers in the
experiment of Section 4.2.2. The parameter M is reported in the main text.

SNR λ∗
0

λ∗
2 L0BnB GRB MSK B
0.5 0.00402 0.126 34155 85402 7784 1

1

2

3

4

5

0.0057 0.0869

0.0097 0.0596

0.0113 0.0409

0.0121 0.0409

0.0120 0.0409

223

217

221

189

159

4020 2840 1

868

461

428

532

736 1

400 1

227 1

161 1

C.4 Experiment of Section 4.3

The number of nodes for all experiments of Section 4.3, and the parameters for varying n and k
are presented in Table 10. For varying correlation coeﬃcient: for p = 104, we have λ∗
0 = 0.0326,
λ∗
0 = 0.0298, λ∗
2 = 0.0091, and M = 0.465 (for all values of ρ). For p = 105, we have λ∗
2 = 0.00202,
and M = 0.449 for all ρ, except for ρ = 0.9 which has λ∗

0 = 0.0304.

C.5 Computing Setup and Software

We used four machines in our experiments, each using an Intel(R) Xeon(R) Platinum 8252C CPU
and 48GB of RAM. We ran Gurobi and MOSEK using their Python APIs, BARON using the
Pyomo Python API [30], and [13]’s OA approach using the SubsetSelectionCIO toolkit [11] in
Julia. Relevant software versions: Ubuntu 18.04.3, Python 3.7.10, R 3.6.3, Julia 0.6.4, Gurobi
9.0.1, MOSEK 9.2.3, BARON 2021.1.13, Pyomo 5.7.1, and L0Learn 2.0. For Julia, exceptionally,
we used Gurobi 8.0.1 due to compatibility issues with Gurobi 9.

37

Table 9: The parameter λ0 and the number of BnB nodes explored in the experiment of Section 4.2.3.

λ2

10λ∗
2

2λ∗
2

λ∗
2

0.5λ∗
2

0.1λ∗
2

λ2

10λ∗
2

2λ∗
2

λ∗
2

0.5λ∗
2

0.1λ∗
2

λ0
0.02692
0.00538
0.00054
0.04207
0.00841
0.00084
0.04526
0.00905
0.00091
0.04704
0.00941
0.00094
0.04856
0.00971
0.00097

λ0
0.02692
0.00538
0.00054
0.04207
0.00841
0.00084
0.04526
0.00905
0.00091
0.04704
0.00941
0.00094
0.04856
0.00971
0.00097

p = 103
L0BnB GRB MSK B
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

343
1
270993
1051
27
187743
2825
71
175876
6417
113
66769
11811
175
22096

60303
27103
2056
55882
354
2045
26721
343
27259
18821
242
18817
12518
218
19983

376
1
10271
1764
89
9977
7564
237
9819
10959
394
9884
13643
802
10228

p = 104
L0BnB GRB MSK B
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
353
1
1
1
1
1
1
1
1
1
1

343
1
56504
10097
37
49358
43745
111
32142
75893
191
18253
96091
507
10433

378
1
826
901
470
820
886
801
824
915
906
822
857
1056
828

38

Table 10: Problem parameters and number of nodes for the experiment of Section 4.3.

n

102

Varying n
103

104

105

k†

10

15

20

25

Varying k†

λ0
λ2
M 0.410

0.0104 0.0121 0.0152 0.0181

0.0119 0.00615 0.00415 0.00275

0.0409 0.00139 0.00295 0.0001

0.0409 0.0409 0.0596 0.126

λ0
λ2
M 0.348 0.275

0.337

0.329 0.316

0.224

0.186

Nodes 1386125

383

259

421

Nodes 225

5261 218667 139353

Varying correlation coeﬃcient ρ

ρ

0.1 0.3 0.5 0.7

0.8

0.9

Nodes (p = 104) 597 613 845 2295 7913 86083
Nodes (p = 105) 589 633 943 3145 11873 127267

39

