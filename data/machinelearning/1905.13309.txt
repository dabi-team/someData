Machine Learning Methods for Shark Detection

Jordan Felicien Masakuna (jordan@aims.ac.za)
African Institute for Mathematical Sciences (AIMS)

Supervised by: Dr. Simukai Utete and Prof. Jeﬀ Sanders
AIMS, South Africa

Submitted in partial fulﬁllment of a structured masters degree at AIMS South Africa

21 May 2015

9
1
0
2

y
a
M
1
2

]

V
C
.
s
c
[

1
v
9
0
3
3
1
.
5
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
Abstract

This essay reviews human observer-based methods employed in shark spotting in Muizenberg Beach. It
investigates Machine Learning methods for automated shark detection with the aim of enhancing human
observation. A questionnaire and interview were used to collect information about shark spotting, the
motivation of the actual Shark Spotter program and its limitations. We have deﬁned a list of desirable
properties for our model and chosen the adequate mathematical techniques. The preliminary results
of the research show that we can expect to extract useful information from shark images despite the
geometric transformations that sharks perform, its features do not change. To conclude, we have
partially implemented our model; the remaining implementation requires dataset.

Key words: Feature Extraction, Classiﬁcation, Fusion, Support Vector Machine, k−Nearest Neighbours,
Neural Network, Dempster-Shafer Fusion, Fourier Descriptor, Legendre Moment, Complex Moment.

Declaration

I, the undersigned, hereby declare that the work contained in this research project is my original work, and
that any work done by others or by myself previously has been acknowledged and referenced accordingly.

Jordan Felicien Masakuna, 21 May 2015

i

Contents

Abstract

1 Introduction

2 Model Properties
2.1 Properties
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Mathematical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

i

1

3
3
5
6

3 Pretreatments and Transformations

8
8
Image Preprocessing (IP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1
3.2 Feature Extraction (FE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

4 Classiﬁcation and Fusion Techniques

14
4.1 Artiﬁcial Neural Network (ANN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2 k-Nearest Neighbours (KNN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.3 Support Vector Machine (SVM)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4.4 Dempster-Shafer Fusion (DSF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

5 Results and Discussion

24
5.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
5.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

6 Conclusion

References

26

28

ii

1.

Introduction

Many Cape Town people spend their holidays on beaches either swimming or surﬁng. There is danger
in the water with the presence of sharks which can cause death. Nevertheless, people come and enjoy
themselves at the beaches as shown in Figure 1.1a. Great white shark is the type of shark most spotted
in Cape Town and it can reach 6.1 meters of length. Naturally in Summer, sharks can frequent beaches
(http://sharkspotters.org.za/information).

(a)
http://www.surfers-corner.co.za

Muizenberg

Beach

(b)
http://sharkspotters.org.za

shark

spotter

Figure 1.1: shark spotter

In 2004 a program named Sharks Spotters (http://sharkspotters.org.za/) was formed with the objective
of spotting sharks in the ocean while people are surﬁng. In Muizenberg for example, the spotters (see
Figure 1.1b) are positioned on the mountain and scan a speciﬁc area of the water through binoculars.
The spotters on the mountain are in communication with other spotters on the beach. There are
four types of ﬂags (see Figure 1.2) that spotters use to communicate with other spotters, surfers and
swimmers. The four ﬂags transmit the following messages: (1) Green ﬂag : the conditions are good for
spotters to work. (2) Black ﬂag : the conditions are poor for spotters to perform well. (3) Red ﬂag : be
warned a shark is in the water. (4) Finally, the White ﬂag signals surfers and swimmers to leave the
water immediately in order to avoid the risk of death.

Figure 1.2: Flags (http://sharkspotters.org.za/)

When we interviewed the spotters they mentioned that poor conditions such as rain, murky or rough
weather, cloudy and windy conditions make it diﬃcult to spot sharks. Consequently, spotters run the
two following risks: (1) False Positive: spotters could ask surfers to evacuate under the false belief
that a shark has been spotted. If this occurs frequently, surfers will no longer trust spotters. This is the
ﬁrst danger. (2) False Negative: spotters might not see a shark while there is one in the water. This
is the second danger. The purpose of this work is thus to propose an automatised system supporting
decision-making when a suspect object is spotted. An image is taken of a suspect object and both
humans and an automated system classify the object whether it is a shark or not, compare the results
and make a decision. We need to reduce the risks of False Positive and False Negative by converting

1

Page 2

the actual manual shark spotter program to a semi-automated shark spotter.

Shark spotting involves three main steps: (1) detecting a suspect shape in the water; (2) deciding
whether or not it is a shark; (3) delivering a message to surfers when there is a shark. The automated
shark spotter that we propose involves the three steps above stated as well. We aim to design shark
detection that is able to: (1) detect sharks despite the geometric transformations they can perform; (2)
detect sharks despite poor weather conditions.

In order to fulﬁl those aims, we consider shark spotting as a multi-class classiﬁcation task in Machine
Learning. We need to classify four objects, namely a mature shark, a school of sharks, a baby shark and
other objects in the water. A classiﬁcation assigns a new image to one of the four classes. After spotting
a school of sharks, the spotter has to communicate with the nearest beach and the surrounding beaches
as well. The risk of baby sharks is less considerable than the risk of mature sharks. Since the actual
program gives good results apart from when poor weather conditions prevail, we need to design a model
that can support the existing system. Bagheri et al. (2013) considered how to fuse diﬀerent classiﬁcation
techniques in order to improve their outputs. We also consider diﬀerent classiﬁcation techniques and
fuse their outputs.

Before classifying images, we prepare them by reducing noise and improving image contrast caused by
poor weather, detect diﬀerent shapes and extract useful information. We assume that data is not linearly
separable, meaning it is not possible to separate the four classes linearly in space. And the image to be
classiﬁed contains objects in the ocean such as ﬁsh, people and water.

To extract useful information from an image we focus on obtaining space-based and frequency-based
features. We combine space-based and frequency-based outputs to diversify the features. We use a
Generic Fourier Descriptor during frequency-based feature extraction. We use a Complex Moment and
a Legendre Moment for space-based feature extraction. The choice of the two space-based features is
motivated by the fact that one constructs an orthogonal set of features while the other does not (Shu
et al., 2007).

To classify an image, we ﬁrst use an Artiﬁcial Neural Network (ANN) which trains a classiﬁer from
training data so that it can assign a new image to the class using its knowledge. ANN evaluates the
training error based on inputs and minimizes the error (Bishop, 1995). Then k-Nearest Neighbours
which evaluates the closeness between the query image and every image in data set, looks for k images
that are very close to the query image and proceeds by voting. A query image xq is assigned to class
ck having more images among k images chosen (Suguna and Thanushkodi, 2010). Then a Support
Vector Machine (SVM) reduces the classiﬁcation error. SVM transforms a non-linear problem to a
linear problem by using the kernel function that converges to a global minimal since the problem is
converted to a convex problem (Crammer and Singer, 2001). We use Dempster-Shafer Fusion (DSF)
to fuse classiﬁcation technique outputs. DSF considers the uncertainties of data and combines diﬀerent
sources of evidence (Benmokhtar and Huet, 2006).

The rest of the essay has the following organization: Chapter 2 will present a list of model properties
in order to get a good output and decide how to choose the mathematical techniques to be used.
Chapter 3 will show the diﬀerent preliminary tasks applied to an image in order to reduce noise, to
improve its contrast, to segment an image and to extract useful information. Chapter 4 will present the
three classiﬁcation techniques to be used and how they are combined in order to improve their outputs.
Chapter 5 presents the results obtained and a discussion thereof.
In Chapter 6, we will conclude and
present further work.

2. Model Properties

We are concerned with the classiﬁcation task in Machine Learning that assigns a query image to one
of the four classes. The problem that can arise is that two classiﬁers can classify the same query image
diﬀerently even though they are using the same training data. We mitigate this classiﬁcation error by
deﬁning the desired properties that make a good classiﬁer. The purpose of a classiﬁer is to ﬁnd a
function F (xi) = F (X , xi) which behaves like the exact unknown function F(xi). The function F(xi)
is deﬁned as follows:

F : X → Ω ,

(2.0.1)

where X and xi are the set of training data and its ith image and Ω = {c1, · · · , ck} is the set of classes.
In the next section we deﬁne a list of properties that the system should have in order to ensure that
accurate results are obtained from the shark detection model, i.e. to prove the accuracy of the model.

2.1 Properties

A good model has to cover the following properties that represent the relationship between the input
X and the output F(xi). This list of properties is not exhaustive, but it is enough to design a model
that satisﬁes the requirements of shark detection. The notation to be used throughout this work is as
follows. Let

• X ∈ Rn×m and Y ∈ Ωn be the sequences of training data and outputs respectively;

• X ◦ and X c ⊂ X be the permutation of two rows (ith and jth) of X and the features set

respectively;

• xi ∈ X , yi ∈ Y and xq be the ith image in X , its output and the query image respectively;

• R(X ) and N (X ) be the number of images in X and the number of pixels in xi respectively;

• M = {M1, · · · , Ml} be the set of classiﬁer models;

• Γ(f, h) be the error between two functions f and h;

• g(x, y) be the grayscale intensity where (x, y) represents its location;
• w(xq) ∈ [0, 1]l×k be the decision proﬁle matrix such that (cid:80)k

j=1 wj

i (xq) = 1.

Property 1. (Optimality) The optimal function of F (xi) is the exact unknown function F(xi). It is
easy to get the exact function, i.e. F (xi) = F(xi) when we deal with linear separable data, but it is
diﬃcult for non-linear-separable data in which we seek to minimize error between the two functions.

Property 2. (Computational Cost) Let X , X c, N and Γ be as deﬁned above.

N (X ) ≥ N (X c) =⇒ Γ[F(X c, xq) − F (X c, xq)] ≤ Γ[F(X , xq) − F (X , xq)] .

(2.1.1)

Property 3. (Graph) The set X is a disconnected graph containing k connected components. Images
are assigned to each component based on their similarity (closeness).

Property 4. (Misclassiﬁcation) Let F(xi) = cj. A misclassiﬁcation is a case where an image of class
cj is assigned to class ck with j (cid:54)= k. F is a misclassiﬁer if and only if F (xi) (cid:54)= F(xi) for some xi.

3

Section 2.1. Properties

Page 4

Property 5. (Convergence) The classiﬁer F (xi) ﬁts F(xi), i.e. it behaves like F(xi) for all images in
domain X . The error has to be minimized such that:

Γ[F(xi) − F (xi)] ≈ 0 .

(2.1.2)

Property 6. (Uniqueness) The function classiﬁer F (xi) is not unique and does not have an explicit
algebraic expression because its form depends on the technique used. The reliability of a technique in
comparison with another is not absolute, i.e. it depends on the data. Let X be as deﬁned above, F1(xi),
F2(xi) be two classiﬁers. Then

F1 is better than F2 in X ⇐⇒ Γ[F(x) − F1(x)] ≤ Γ[F(x) − F2(x)] ∀x ∈ X .

(2.1.3)

Property 7. (Surjectivity) The function classiﬁer F(xi) is surjective, i.e. ∀ cj ∈ Ω ∃xi ∈ X such
that F (xi) = cj. In other words nj ≥ sj and (cid:80)k
j=1 nj = n where the class cj contains nj images and
sj ≥ 1 is the required number of images belonging to cj.

Property 8. (Row Dependence) The X and Y rows are completely dependent in order to preserve
the exactness. Let X ◦ and Y ◦ be deﬁned above. Then

X ◦ ⇐⇒ Y ◦ .

(2.1.4)

Property 9. (Useful Information) Useful information aids in decision making. Redundant information
or data which is not useful either causes misclassiﬁcation or does not inﬂuence the result. Let p =
N (X c) < N (X ) = m and d = m − p.

F (X c, xi) = yi ∀xi =⇒ X d is redundant information, i.e. X c is a good relevant features subset,
F (X , xi) = yi ∀xi =⇒ X d is useful information,
F (X c, xi) (cid:54)= yi or F (X , xi) (cid:54)= yi for some xi =⇒ we can not decide about X d.

Property 10. (Transformations Invariance) It is desirable that the classiﬁer F (xi) be invariant under
rotation, translation and scaling. For scaling, one can deﬁne a degree of scaling in order to avoid
misclassiﬁcation since a too scaled shark can be seen as a simple ﬁsh. Let the function t(xi) denote
any transformation of xi. Using the function classiﬁer we expect:

F (t(xi)) = F(xi) .

(2.1.5)

Property 11. (Classiﬁcation) Let µi(xq) be the probability of xq to belong to ith class where µi(xq) ∈
[0, 1] and (cid:80)
i µi(xq) = 1. The classiﬁcation is based on the largest probability µj(xq) to belong to the
exact class.

F (xq) = cj such that µj(xq) =

{µi(xq)} .

(2.1.6)

k
max
i=1

Therefore, if there exist µi(xq) and µj(xq) such that µi(xq) ≈ µj(xq), then we run the risks of False
Negative and False Positive.

Property 12. (Robustness) The classiﬁer is robust to noise caused by the weather, wind, camera
status and spotter position. Let x◦

i = xi + n(xi) where n(xi) denotes the function of noise. Then

F (x◦

i ) = F(xi) .

(2.1.7)

Property 13. (Size of Data) There is no requirement on how m = N (X ) and n = R(X ) are related.

(n, m) ∈ N2 .

(2.1.8)

Section 2.2. Mathematical Analysis

Page 5

Property 14. (Binarization) A good classiﬁer has to detect the pixels forming the foreground G and
the background B. In other words, it should detect boundaries and shapes of the query image xq. With
the threshold θ, the intensity function g(x, y) is replaced by the binary function h(x, y) deﬁned:

h(x, y) =

(cid:40)
0
1

if g(x, y) ≤ θ, i.e. g(x, y) ∈ B ,
otherwise, i.e. g(x, y) ∈ G .

(2.1.9)

Property 15. (Segmentation) The classiﬁer F (xi) has to detect and extract diﬀerent shapes in a
given image. Let xq be a query image and xj

q be the jth shape in xq. Then

Γ[F(xj

q) − F (xj

q)] ≤ Γ[F(xq) − F (xq))] ∀xj

q ∈ xq .

(2.1.10)

(cid:88)

j

Property 16. (Decision Fusion) Let Mi be a classiﬁer model, δi be the training error due in Mi and
δ be the error due to the combination of all Mi for i = {1, 2, · · · , k} then

δ ≤

k
min
i=1

{δi} .

(2.1.11)

This list of properties we consider to be desirable in modelling of the classiﬁer.
In what follows, we
will focus on the choices of relevant mathematical techniques relative to shark detection and develop a
system to satisfy these properties.

2.2 Mathematical Analysis

Our analysis based on the characteristics of sharks, the types of cameras used, the climate, wind and so
on as above mentioned, and also the techniques that will be used. Before we perform classiﬁcation, we
need to pass through two preliminary steps namely, Image Preprocessing (IP) and Feature Extraction
(FE) as shown in Figure 2.1. Let us look at the role of each step and deﬁne the properties required
according to our work that is going to lead us to an optimal choice of mathematical technique.

In the ﬁrst step, Image Preprocessing, we have three tasks:

Filtering. An image is ﬁltered in order to remove noise and improve contrast.
spotting, the noise and bad contrast are due to poor weather conditions

In the case of shark

Binarization. An image is binarized in order to detect distinct boundaries. We assume that there are
two diﬀerent sets of information in an image (boundaries and shapes) and we need to assign each pixel
to its corresponding set. A good binarization technique has to consider the overall pixel variance in the
image. The idea is to look for a threshold θ and compare each pixel to θ in order to decide which pixel
belongs to the ﬁrst set and which belongs to the second.

Segmentation. Segmentation is the process of detecting the diﬀerent shapes in an image. When we use
cameras to spot sharks, we can observe more than one shape. Humans can distinguish diﬀerent shapes
more easily than an automated system can.
It will be less expensive computationally to recognize an
image having one shape. We need a technique which can segment an image by performing for example,
a linear combination between pixels based on their intensities and contrasts.

In the second step, Feature Extraction, we need to consider only useful information from an image.
Nevertheless, with regards to sharks in the ocean, diﬀerent feature extractors might focus on diﬀerent

Section 2.3. Architecture

Page 6

types of data, for example space-based or frequency-based data. Shark space-based features due to
its movements in space, would be diﬀerent to its frequency-based features (named F in Figure 2.1)
due to waves produced by its movements. Fortunately, we aim to combine feature extractors to get
adversity of features for use in the classiﬁcation task (see Figure 2.1). Again a good feature has to be
invariant under geometric transformations motivated by previous research in Machine Learning (Flusser,
2000). Then observing diﬀerent transformations that can be performed on the shark image, we deal
with translation, rotation and scaling (TRS). For space-based features, the moment is most used and
represents the projection of the image onto the monomial formed by the basis set {x, y} (Shu et al.,
2007). It is desirable to get a feature extractor that promotes the reconstruction of the image from its
set of features. Concerning feature space-based, there are two diﬀerent moments: one promotes the
image reconstruction since its basis is orthogonal (Orthogonal Moment named OM in Figure 2.1) and
the other does not (Geometric Moment named GM).

Figure 2.1: Analysis Result

To assign a query image xc
q to one of the classes cj (j = 1, · · · , k), there are several ways to deal
with classiﬁcation: one can evaluate the similarity (CS) between the query image and the rest of the
training data followed by votes. Another way would be to reduce the classiﬁcation error (CM). It deﬁnes
classiﬁcation error on the results and seeks to minimize it. Another way would be to train the classiﬁer
(CT) from training data so that it can assign a new image to a class using its knowledge. It evaluates
training error based on inputs and minimize error. The three approaches are completely diﬀerent and
can produce diﬀerent results. We need to combine them in order to preserve their individual strengths,
i.e. get more accurate output since practically, it is not obvious how to choose the best approach. The
technique that will be used to combine the classiﬁer outputs (which is called FU in Figure 2.1) should
consider, not only to the outputs obtained by the classiﬁers, but also to their weaknesses (reliability)
and robustness. It should not trust training data at all (uncertainty), i.e. it must be a good judge.

2.3 Architecture

Based on the mathematical analysis and classiﬁer properties described in the previous sections, the
choices of mathematical techniques are given as follows:

Filtering. We choose Median Filter (Jassim and Altaani, 2013) because it replaces each pixel by its
nearest neighbours that constructs a more representative set of pixels unlike Average Filter, Highpass
Filter or Gaussian Filter. Consequently, the property of Robustness is covered.

Binarization. We use the procedure proposed by Otsu because it is searching the threshold by consid-
ering the pixel variances unlike Mean Threshold. This technique covers the property of Binarization.

Section 2.3. Architecture

Page 7

Segmentation. The Random Walker Technique (Grady, 2006) is used instead of Edge detection because
it maps an image to a graph whereby the combination of vertices and edges constructs a linear system
which is easy to solve. Consequently the property of Segmentation is covered.

Feature Extraction. Considering existing Feature Extractors such as Zernike Moment and Moment.
The Complex Moment is chosen as GM, Legendre Moment as OM and Fourier Transform as F relative
to Figure 2.1. Consequently FE covers the properties of Computational Cost, Useful Information and
Transformations Invariance.

Classiﬁcation. From the analysis made for classiﬁcation and referring to Figure 2.1, k-Nearest Neigh-
bours is strongly motivated as classiﬁer based on similarity (CS) because of its combination with Genetic
Approach. Neural Network is chosen as the classiﬁer based on training (CT) where the error is evaluated
by partial derivative (gradient). As a classiﬁer based on misclassiﬁcation (CM), Support Vector Ma-
chine is encouraged to be used since it transforms a classiﬁer to an optimization problem thus ensuring
a globally optimal solution. The Classiﬁer covers the properties of Optimality, Graph, Misclassiﬁcation,
Convergence, Uniqueness, Surjectivity, Rows Dependence, Decision and Size of Data.

Fusion. Dempster-Shafer Fusion is chosen over Voting and Bayesian Inference because it combines the
classiﬁer outputs based on uncertainty and lack of information. It covers the property on Fusion.

(a) Global Architecture

(b) Detailed Architecture

Figure 2.2: Shark Detection Architecture

The architecture 2.2b of our model was built up through a Bottom-Up approach, i.e. we have ﬁrst got an
idea of how the architecture had to look and then looked for mathematical techniques that could satisfy
the model properties. Figure 2.2b shows the diﬀerent detailed steps to classify an image and Figure
2.2a shows the global steps by describing the input and output of each step. A detailed explanation of
the architecture will be given in the coming chapters. The choice of each technique is motivated also
by its computational descriptions.

3. Pretreatments and Transformations

This chapter will give us a brief detail of Image Preprocessing (IP) and Feature Extraction (FE). IP
contains mathematical methods for how a shark image is preprocessed in order to reduce noise caused
by wind or weather in the ocean. FE contains mathematical techniques for how a shark image is
transformed in order to get relevant features promoting an accurate classiﬁcation. We start with the
raw data X and the query image xq as inputs and obtain the improved data X c and xc
q as outputs (see
Figure 2.2).

3.1 Image Preprocessing (IP)

Let us give the diﬀerent mathematical techniques chosen for IP and see how the image is transformed
in this step.

3.1.1 Dimensionality and Filtering. The ﬁrst step of IP in our work is ﬁltering, but before that we
would like to reduce image size by changing its mode.
In the default image mode, each pixel of an
image has ﬁve-dimensionality (x, y, i, j, k) where (x, y) is the pixel position and (i, j, k) represents code
function colour intensities for Red, Green and Blue (RGB) respectively. The new mode (grayscale mode)
we need to deﬁne has three-dimensionality (x, y, f ) where f is the intensity function. We need to map
from basis (x, y, i, j, k) to basis (x, y, f ) where the function f (x, y) is given as follows:

f (x, y) =

αi(x, y) + βj(x, y) + γk(x, y)
µ

,

(3.1.1)

where α, β, γ ∈ [0, 255], µ ∈ {1, 255} and i, j, k ∈ [0, 255]. The coeﬃcients α, β, γ are deﬁned such
that α + β + γ = 1. Hence f (x, y) ∈ [0, 1] if µ = 255 and f (x, y) ∈ [0, 255] if µ = 1. Any colour can
be obtained from the basis (i, j, k).

Median Filter. After reducing the dimensionality, we now need to remove noise by ﬁltering the image.
In Median Filter (Jassim and Altaani, 2013), each pixel is replaced by the median g(x, y) of its nearest
neighbour pixels, i.e. each pixel is replaced by a representative pixel g(x, y). The number of neighbours
to consider is ﬁxed by the user and it is recommended to use an odd number so that the median is
exactly one of the existing pixels. As we notice, after applying this ﬁltering, all the nearest neighbours
of a pixel have the same grayscale intensity function g(x, y).

3.1.2 Binarization and Segmentation. The second step after reducing noise is to ﬁnd diﬀerent shapes
in an image (segmentation). We would ﬁrst like, according to the requirement of the segmentation
technique to be used, to get two diﬀerent sets G (foreground set) and B (background set) based on the
grayscale intensities g(x, y) of the pixels. In order to get B and G, some researchers (Jassim and Altaani,
2013) used to compare each pixel to the mean pixel θ considered as a threshold. The mathematical
expression of binarization is given by:

h(x, y) =

(cid:40)
0
1

if g(x, y) ≤ θ, i.e. g(x, y) ∈ B ,
otherwise, i.e. g(x, y) ∈ G ,

(3.1.2)

where h(x, y) is a binarized function. Unfortunately, this approach does not consider variances in image.
Therefore, an optimal approach to ﬁnd the threshold θ which considers the pixel variance is given by Otsu
(Jassim and Altaani, 2013). Otsu searches for the threshold θ that minimizes the intraclass variance

8

Section 3.1. Image Preprocessing (IP)

Page 9

σ2
in(t) and maximizes the interclass variance σ2
tmin = min(xq), tmax = max(xq), wb(t) = (cid:80)t−1
p=tmin

out(t) in image where t is any threshold candidate.

If

π(p) and wa(t) = (cid:80)tmax

p=t π(p) , then

in(t) = wb(t)σ2
σ2

b (t) + wa(t)σ2

a(t) and σ2

out(t) = wa(t)[µb(t) − µ]2 + wa(t)[µa(t) − µ]2 ,

(3.1.3)

b (t) and σ2

where σ2
a(t) are the variances of the pixels in B and G respectively, µb(t) and µa(t) are the
means of the pixels in B and G respectively, µ is the mean of image pixels and π(p) the probability of
each pixel p in image xq.

Otsu’s Algorithm (OA) (Jassim and Altaani, 2013). The OA is given as follows: (1) Compute
probabilities π(p) ∀p ∈ xq. (2) Compute σ2
in(t) and σ2
out(t) for all thresholds t. (3) The optimal
threshold θ corresponds to t which has the maximum of σ2
out(t) and the minimum of σ2
in(t). In case of
more than one optimal threshold, rather their average is considered.

Segmentation. After binarizing the image by detecting the two sets B and G using OA as described
above, we now need to segment the image xq, i.e. to look for diﬀerent shapes in xq. In image segmen-
q of xq, i.e. ﬁnd the diﬀerent shapes xj
tation, each pixel pi ∈ xq is assigned exactly to one shape xj
q in
G based on probability γj
q). The algorithm which is going to be used
is based on graph theory (Grady, 2006) and computes the probability that each vertex of the graph
belongs to the diﬀerent shapes. The image xq is represented by the graph G = (P, H) where pi ∈ P
(set of pixels) and hij = (pi, pj) ∈ H (set of edges).

i (probability for pi to belong to xj

The Random Walker Algorithm (RWA) makes the two following assumptions: (1) G is a connected and
undirected graph. (2) The neighbours of a pixel are chosen randomly. The motivation for considering
RWA as a segmentation technique in our work is due to the fact that it maps randomly an image xq to
a graph G. Let us see brieﬂy how the graph is weighted and how the segments xj

q of xq are found.

The weights of graph G are deﬁned by the Gaussian function ηij (Grady, 2006)

(cid:18)

ηij = exp

−

[g(xi, yi) − g(xj, yj)]2
σ

(cid:19)

,

(3.1.4)

where g(xi, yi) is the image intensity at node pi and σ is the standard deviation in xq. The RWA
optimizes the following energy (Grady, 2006):

Q(γ) = ΥT M Υ ,

where the semi-positive Laplacian matrix M formed by the graph G is given by:

M = (µi,j)n×n, µi,j =




d(pi)
−ηij

0

if i = j ,
if i (cid:54)= j and pi is adjacent to pj ,
otherwise ,

(3.1.5)

(3.1.6)

where γi is the sequence of probabilities of vertex pi with respect to all shapes xj
is the degree of vertex pi. The matrix of probabilities Υ is given by Υ = (cid:2)γ1 γ2

q found in xq and d(pi)
(cid:3)T .

· · · γn

Random Walker Algorithm (RWA). The RWA is given as follows: (1) Apply Binarization to get the
sets B and G. (2) Map the graph (image) by using Equation (3.1.4). (3) Solve Equation (3.1.5). (4)
The vertex pi is assigned to the shape xk

i }j≥1. Note that (cid:80)

q such that γk

i = max{γj

i = 1.

j γj

This leads us to a new step as shown in Figure 2.2 whereby a relevant features set of an image, invariant
under geometric transformations (scaling, translation and rotation) will be selected.

Section 3.2. Feature Extraction (FE)

Page 10

3.2 Feature Extraction (FE)

As stated above, the aim of a feature extractor is to extract relevant information from an image.
Consequently, it reduces image size by considering only useful information. If we are given an image xi
of size m = r × q, then we deﬁne the new size p of the image feature such that p < m. FE receives
the improved training data X and image query xq in its input and returns the set of relevant features
X c of X and the relevant feature xc

q of the image xq.

Generally, feature extractors do not provide features which are invariant under all geometric transfor-
mations. Therefore, it will be required to normalize feature extractors in order to get features invariant
under geometric transformations. Throughout this section, Cartesian coordinates will be used in proving
scaling and translation invariance and polar coordinates in proving of rotation invariance. Let us ﬁrst
give a short background of concepts to use in FE as described in Mathematical Analysis.

3.2.1 Moments and Fourier Spectra. If the image t(xq) denotes at least one of the geometric
transformations (Translation, Rotation and Scaling TRS) of the image xq, then we aim to look for a set
of moment-based features M and a set of frequency-based features S such that M(t(xq)) = M(xq)
and S(t(xq)) = S(xq). The general formulas of geometric moment (Flusser, 2000) and Fourier Spectra
(Zhang and Lu, 2000) are given respectively as follows:
(cid:90) ∞

(cid:90) ∞

(cid:90) ∞

(cid:90) ∞

µab =

xaybg(x, y)dxdy and S(u, v) =

exp[−2πj(ux + vy)]g(x, y)dxdy , (3.2.1)

−∞

−∞

−∞

−∞

where a and b are moment orders on x−axis and y−axis respectively and u and v are the spatial
frequencies in vertical and horizontal directions respectively. The centroid (xc, yc) for moment on x
and y axes of image is given by xc = µ10
and yc = µ01
. The centroid (xc, yc) for frequency is given
µ00
µ00
by the averages (¯x, ¯y) on x and y axes. It follows, by setting new translated variables x(cid:48) = x + κ and
y(cid:48) = y + β, that

c = xc + κ and y(cid:48)
x(cid:48)

c = yc + β .

(3.2.2)

Using (3.2.2), we get x(cid:48) − x(cid:48)
c = y − yc. This leads us to this following theorem.
3.2.2 Theorem. The central moment and the central Fourier Spectral extract sets of features M and
S respectively are invariant under translation.

c = x − xc and y(cid:48) − y(cid:48)

From Equations (3.2.1), we can reconstruct the image using their inverse forms. Hence, the pixel
intensities obtained from Moment and Fourier Spectra are given respectively by:

g(x, y) =

(cid:90) ∞

(cid:90) ∞

−∞

−∞

xaybµabdadb and g(x, y) =

(cid:90) ∞

(cid:90) ∞

−∞

−∞

exp[−2πj(ux + vy)]S(u, v)dudv .

(3.2.3)

Let us give a useful remark (Flusser, 2000) that is going to be used in diﬀerent proofs in this section.
3.2.3 Remark. If the intensity function g(x, y) has g(cid:48)(x(cid:48), y(cid:48)) as its transformed version then g(cid:48)(x(cid:48), y(cid:48)) =
g(x, y) .

We now introduce the ﬁrst feature extraction technique based on moments.

3.2.4 Complex Moment Invariant (CMI). The CMI is the ﬁrst technique chose. It is an extension of
the Moment deﬁned in Equation (3.2.1) by including the complex term j. The CMI Cab of order a + b
in Cartesian and polar coordinates is deﬁned as follows (Flusser, 2000):

Cab =

(cid:90) ∞

(cid:90) ∞

−∞

−∞

(x + jy)a(x − jy)bg(x, y)dxdy and Cab =

(cid:90) ∞

(cid:90) 2π

0

0

ra+b+1ej(a−b)θg(r, θ)drdθ ,

(3.2.4)

Section 3.2. Feature Extraction (FE)

Page 11

where r = (cid:112)x2 + y2, θ = arctan
extracts a set of features invariant under TRS.

y
x

, y = r sin θ and x = r cos θ. We now need to show how CMI

Firstly, uniform invariance under scaling can be proven by setting x(cid:48) = αx and y(cid:48) = αy where α is the
scaling factor. Then the new CMI C(cid:48)

ab is:

(cid:90) ∞

(cid:90) ∞

(x(cid:48) + jy(cid:48))a(x(cid:48) − jy(cid:48))bg(cid:48)(x(cid:48), y(cid:48))dx(cid:48)dy(cid:48)

C(cid:48)

ab =

−∞

−∞
(cid:90) ∞

= αa+b+2

−∞

−∞

(cid:90) ∞

(x + jy)a(x − jy)bg(x, y)dxdy = αa+b+2Cab .

ab = αa+b+2Cab is not invariant under scaling, we need to get another moment by normalizing

Since C(cid:48)
Cab. In particular C(cid:48)

C(cid:48)
ab
00)w =
(C(cid:48)
Setting 2w = a + b + 2, we get C(cid:48)
a + b + 2.

00 = α2C00, hence the new moment is deﬁned as follows:
αa+b+2
α2w Cab .

αa+b+2Cab
(α2C00)w =

Cab
(C00)w =

αa+b+2
α2w

C(cid:48)
ab =

ab = Cab, i.e. the moment Cab is invariant under scaling with 2w =

Secondly, translation invariance is achieved straightforward by the use of Theorem 3.2.2. Finally, the
invariance under rotation can be proven by setting r(cid:48) = r and θ(cid:48) = θ + ψ with ψ the angle of rotation.
ab on g(cid:48)(r(cid:48), θ(cid:48)) in polar coordinates is
Then the new complex moment C(cid:48)

C(cid:48)

ab =

=

(cid:90) ∞

(cid:90) 2π

0
(cid:90) ∞

0
(cid:90) 2π

0

0

r(cid:48)a+b+1ej(a−b)θ(cid:48)

g(cid:48)(r(cid:48), θ(cid:48))dr(cid:48)dθ(cid:48) =

(cid:90) ∞

(cid:90) 2π

0

0

ra+b+1ej(a−b)(θ+ψ)g(cid:48)(r, θ + ψ)drdθ

ra+b+1ej(a−b)(θ+ψ)g(r, θ)drdθ = ej(a−b)ψCab .

Since Cab is not invariant under rotation, we need to normalize it. The expression of C(cid:48)
following lemma stated by Flusser (2000):
3.2.5 Lemma. Let g(cid:48) be a rotated function of g such that g(r, θ) = g(cid:48)(r, θ + ψ) where ψ is the angle
of rotation. Then

ab leads to the

C(cid:48)

ab = ej(a−b)ψCab .

(3.2.5)

The expression in (3.2.5) shows that the moment Cab is invariant under rotation if a = b which is
weak since many features such that a (cid:54)= b will be lost. We are looking how to normalize this so that
the moment is invariant even though a (cid:54)= b. Flusser (2000) used a phase cancellation approach by
deﬁning k diﬀerent elements of a, b and c such that (cid:80)k
i=1 ejψci(ai−bi) =
ejψ (cid:80)k
. This leads us
to modify a theorem (rotation invariance) stated in (Flusser, 2000) and get the new theorem (TRS
invariance) stated as follows:
3.2.6 Theorem. Let k ≥ 1, ai, bi and ci(i = 1, · · · , k) be non-negative integers such that (cid:80)k
bi) = 0. If

i=1 ci(ai−bi), then the normalized moment can be deﬁned as M = (cid:81)k

j=1 cj(aj − bj) = 0. Since (cid:81)k

i=1 Cci
aibi

i=1 ci(ai −

Cab =

(cid:90) ∞

(cid:90) ∞

−∞

−∞

(cid:20)
(x − xc) + j(y − yc)

(cid:21)a(cid:20)

(x − xc) − j(y − yc)

g(x, y)dxdy ,

(3.2.6)

(cid:21)b

then the moment

k
(cid:89)

i=1

Mci

ai,bi

where Mci

ai,bi

=

Cci
aibi
Cwi
00

and wi = ci

ai + bi + 2
2

is invariant under TRS .

(3.2.7)

Section 3.2. Feature Extraction (FE)

Page 12

The new CMI deﬁned in (3.2.7) extracts the features set M invariant under TRS. The next step is to
construct a set of features by combining the diﬀerent orders ai, bi and ci of moments. For instance the
set of feature M = {M1,2
2,1 } is invariant under TRS. Let us move to
the second feature technique based on frequency.

1,2 M1,5/2

2,0, M1,5/2

2,0, M2,5

0,2M1,2

1,2M1,2

3.2.7 Generic Fourier Descriptor (GFD). The GFD is based on spatial frequency by transforming the
image in the frequency plane using Fourier Transform as stated in Equation (3.2.1). The expression of
Fourier F (u, v) in polar coordinates (Zhang and Lu, 2000):

S(ρ, ψ) =

(cid:90) ∞

(cid:90) 2π

0

0

rg(r, θ)e−2πjrρ cos(θ+ψ)drdθ ,

(3.2.8)

where v = ρ sin ψ, u = ρ cos ψ, θ = arctan

, r = (cid:112)x2 + y2, y = r sin θ and x = r cos θ. And (r, θ)
are polar coordinates in the image plane and (ρ, ψ) are polar coordinates in frequency space. The bad
news is that to normalize the Fourier Transform deﬁned in (3.2.8) in order to extract features invariant
under TRS is not possible (Zhang and Lu, 2000). One of the ways to deal with that, as suggested by
Zhang and Lu (2000), is to keep the Fourier Cartesian form S(u, v) in polar form S(ρ, ψ). Therefore,
the result in polar coordinates is given as follows:

y
x

S(ρ, ψ) =

(cid:90) ∞

(cid:90) 2π

0

0

g(r, θ)e−j2π(rρ+θψ)drdθ ,

(3.2.9)

where ρ and ψ are respectively radial frequency and angular frequency.
To ﬁrst achieve invariance under uniform scaling, we set the new polar coordinates r(cid:48) = (cid:112)x(cid:48)2 + y(cid:48)2 =
αy
(cid:112)α2x2 + α2y2 = αr and θ(cid:48) = arctan
αx

= θ. The new Fourier Transform is given by:

y(cid:48)
x(cid:48) = arctan

(cid:90) ∞

(cid:90) 2π

S(cid:48)(ρ, ψ) =

g(cid:48)(r(cid:48), θ(cid:48))e−j2π(r(cid:48)ρ+θ(cid:48)ψ)dr(cid:48)dθ(cid:48) =

0
0
(cid:90) 2π
(cid:90) ∞

= α

0

0

g(r, θ)e−j2π(αrρ+θψ)drdθ .

(cid:90) ∞

(cid:90) 2π

0

0

g(r, θ)e−j2π(αrρ+θψ)αdrdθ

S(ρ, ψ) is not invariant under scaling. In order to cancel α in e−j2π(αrρ+θψ) of S(cid:48)(ρ, ψ), we divide the
radius r by its maximum value R = max
x,y

{r}. Therefore Equation (3.2.9) becomes:

S(ρ, ψ) =

(cid:90) ∞

(cid:90) 2π

0

0

g(r, θ)e−j2π( r

R ρ+θψ)drdθ .

(3.2.10)

In order to cancel the factor α in S(cid:48)(ρ, ψ), since from Equation (3.2.10) S(cid:48)(ρ, ψ) = αS(ρ, ψ) and
S(cid:48)(0, 0) = αS(0, 0), we divide S(ρ, ψ) by S(0, 0). Therefore the new Fourier Transform is:

S(ρ, ψ) =

S(ρ, ψ)
S(0, 0)

,

(3.2.11)

where S(0, 0) (cid:54)= 0. It follows by Theorem 3.2.2 that the central Fourier Transform S(ρ, ψ) is invariant
under translation. It is straightforward that the equation deﬁned in (3.2.11) is invariant under rotation
by setting r(cid:48) = r and θ(cid:48) = θ + φ. This leads us to the following theorem:
y − yc
3.2.8 Theorem. Let r = (cid:112)(x − xc)2 + (y − yc)2, θ = arctan
x − xc
S(ρ, ψ) be deﬁned in Equation (3.2.10). Then the normalized Fourier Transform

and the Fourier Transform

S(ρ, ψ) =

S(ρ, ψ)
S(0, 0)

is invariant under TRS .

(3.2.12)

Section 3.2. Feature Extraction (FE)

Page 13

We do not need to construct systematically as done in CMI the basis which is one of the advantage of
Generic Fourier Descriptor. Its set of features looks like:

S = {S(0, 1), · · · , S(1, 0), S(1, 1), · · · } ∈ Rp .

(3.2.13)

Let us jump to the last Feature Extraction technique based on Legendre moments.

3.2.9 Exact Legendre Moment. The Legendre Moment of order c = (a + b) with the ath and bth
order Legendre polynomials La(x) and Lb(x) is deﬁned as follows (Chong et al., 2004):

Mab = Kab

(cid:90) 1

(cid:90) 1

−1

−1

La(x)Lb(y)g(x, y)dxdy ,

(3.2.14)

where Kab =

(2a + 1)(2b + 1)
4

. According to Chong et al. (2004), Rodrigues deﬁned La(x) by

La(x) =

1
2aa!

da
dxa (x2 − 1)a a = 0, 1, 2, · · · .

(3.2.15)

It follows from Equation (3.2.15) that L0(x) = 1 and L1(x) = x.

Since for x, y ∈ [−1, 1] the set formed by the Legendre Polynomials is completely orthogonal, we need
to map the true variables x and y to normalized pixel coordinates xi and yj respectively. If the image
has size n × m, the normalized pixel coordinates are given by xi = −1 + iδx and yj = −1 + jδy where
δx = 2

m . It can be proven that δxi = δx and δyj = δy.
The Legendre Polynomial deﬁned in Equation (3.2.15) has recurrence relation deﬁned by

n and δy = 2

La+1(x) = −

aLa−1(x) − (2a + 1)xLa(x)
(a + 1)

.

(3.2.16)

According to Chong et al. (2004), because of the computational cost of Legendre moment deﬁned in
Equation (3.2.14), its accurate approximation (ELM) is deﬁned as follows:

Mab =

n
(cid:88)

m
(cid:88)

i=1

j=1

Ia(xi − x0)Ib(yj − y0)g(xi, yj) for a, b ≥ 1 ,

(3.2.17)

(cid:21)vj+1

where Ia(xi) =

(cid:20)

2a + 1
2a + 2

(cid:21)ui+1

xiLa(xi) − La−1(xi)

and Ib(yj) =

ui

(cid:20)

2b + 1
2b + 2

yjLb(yj) − Lb−1(yj)

.

vj

This ELM deﬁned in Equation (3.2.17) is invariant under translation and scaling (Chong et al., 2004)
where xi, yj ∈ [−1, 1], (xo, yo) is the scaled centroid, and the upper and lower limits are deﬁned as
follows:

ui+1 = xi +

δxi
2

, ui = xi −

δxi
2

, vj+1 = yj +

δyj
2

and vj = yj −

δyj
2

.

(3.2.18)

So, the set of features obtained from ELM invariant under translation and scaling looks like:

M = {M11, M12, · · · , M21, M22, · · · } ∈ Rp .
To conclude this chapter, Image Preprocessing as a preliminary step has reduced dimensionality of the
image, improved the image contrast, reduced noise and detected diﬀerent shapes in an image. Likely,
Feature Extractors extracted features invariant under TRS. As a result, we have got a reﬁned set of
training data X c and image query xc
q. The X c can be either test data used to compute the result
accuracy or validation data used to select the best performed model approach.

(3.2.19)

We introduce the next and last block of techniques which is besides the main block of this work. In this
next chapter, three diﬀerent techniques will be used as classiﬁers and another technique will be used to
fuse classiﬁer outputs in order to improve their outputs.

4. Classiﬁcation and Fusion Techniques

As explained in Chapter 2, a classiﬁer has an ability to select a new image (query image) xc
q from one of
the existing classes based on similarity. In this chapter, we will give our understanding of three diﬀerent
classiﬁcation approaches chosen and the technique used to combine their outputs.

Each of the classiﬁer models Mi has to assign the image xc
ωj
i (xc
decision proﬁle matrix ωj
given in Chapter 2. Let us start with the ﬁrst classiﬁcation technique.

q) ∈ [0, 1]. As inputs we have the training data X c and query image xc
i (xc

q in all of the classes in terms of probabilities
q. As outputs we have the
q)] as stated in the notation

q) where ωi(xc

q) ∈ ωi(xc

q), · · · , ωk

q) = [ω1

i (xc

i (xc

4.1 Artiﬁcial Neural Network (ANN)

ANN is inspired by the human nervous system. The human nervous system is based on neurons which
process information. A neuron receives information sent by another neuron through its dendrites and

Figure 4.1: Neural Network Architecture

sends the information to other neurons using its axon. The biological brain has over 10 billions neurons
and each neuron is connected to the others through 10000 synapses (Bishop, 1995). ANN guarantees
us good learning through its diﬀerent techniques. The presence of diﬀerential equations (gradients),
random choice and updates of weights are the keys to ANN success.

Our Neural Network architecture is explained as follows:
it contains three layers as shown in Figure
4.1. The input layer contains neurons that activate the network, and the only task this layer has is to
transmit information. The hidden layer receives information from the input layer and transmits it to
the other layers (hidden layers or output layer). The output layer can only receive information from
the hidden layer and returns Neural Network output. A network contains exactly one input layer, one
output layer and, the number of hidden layers depends on the recognition pattern problem; but it is
recommended to use one hidden layer (Nielsen, Accessed in April 2015). The ANN architecture can be
performed using the One Against All procedure (OAA) as proposed in Ou and Murphey (2007). There

14

Section 4.1. Artiﬁcial Neural Network (ANN)

Page 15

are other procedures such as the P Against Q (PAQ) and the One Against One (OAO). These three
procedures show us how layers are connected to one another.

In accordance with the architecture shown in Figure 4.1, the goal of ANN is to ﬁnd the set of weights w
that will map accurately to the exact output. Let us now focus on a mathematical description of ANN.

4.1.1 MultiLayer Perceptron (MLP). According to Nielsen (Accessed in April 2015), it is recom-
mended to use MLP when we deal with non-linear separable data. MLP architecture contains at least
one hidden layer as shown in Figure 4.1.

Each neuron, except those of the input layer performs the sum of the information received and actives
the sum, i.e. it scales the sum to a real value in [0, 1]. So, our model is deﬁned such that every neuron
has the same activation function g(.) and has its own bias b. A bias as shown in Figure 4.1, helps ANN
to shift its output from 0 to 1 or vice-versa.

ij be the weight of the dendrite from the ith neuron of layer l − 1 to the jth neuron of layer l; zl
Let wl
i
be the ith neuron of layer l; bl
i be the bias of the ith neuron in the lth layer; and ol
i be the output of the
ith neuron in layer l. The idea is to initialize all the weights, compute the output oL
j in the output layer
based on the Feedfoward (FF) approach, i.e. the layer l uses the output of layer l − 1 as its input. At
the end of FF, the network output oL
j (xi) of neuron j is computed where xi represents a single image
in training data X c. If the error between the ANN output oL
j and the exact output yj is still signiﬁcant,
then update the weights based on Backpropagation (BP) and apply again FF until the error becomes
small enough.

The updates of weights and biases can be developed using two diﬀerent approaches: online and batch.
In this work we choose batch simply because the minimization of the overall error ensures us a good
expected result. The epoch time is the length of time the classiﬁer learns, i.e. the time limit the
BP technique is applied. The desired output yj is a k- dimension vector since we have k classes,
i.e. yj = (0, 0, · · · , 1) if we want to depict the kth class. Let us see how to compute the overall output
error which is going to lead us to the description of BP technique.

4.1.2 Cost Function. The cross entropy function (Nielsen, Accessed in April 2015) is deﬁned by:

Ec(w, b) = −

1
n

(cid:88)

(cid:88)

(cid:20)

i

j

yj(xi) ln oL

j (xi) + (1 − yj(xi)) ln(1 − oL

(cid:21)
j (xi))

,

(4.1.1)

where oj(xi) = g(zj) = g

(cid:18)

(cid:80)

k wjkxj + bj

(cid:19)

[1 + exp(−z)]−1. It follows that:

and g(.) the sigmoid activation function given by g(z) =

g(cid:48)(z) = g(z)[1 − g(z)] .

(4.1.2)

The entropy has the advantage that as it gets close to zero when the neuron’s output becomes close
to the exact output and the larger the error, the faster the classiﬁer (unlike quadratic cost function for
instance). The changes of w and b are determined by using the Backpropagation (BP) approach. BP
introduces the intermediate quantity δl
i representing the error in the ith neuron of layer l. Let us now
perform mathematical analysis of BP by developing the diﬀerential equations of the cost error Ec(w, b)
deﬁned in 4.1.1 with respect to weights and biases.

4.1.3 Backpropagation Technique (BP). The BP gives the procedure to compute δl
and the output ol

j are given by:

i. The soma zl
j

zl
j =

(cid:88)

k

wl

jkol−1

k + bl

j and ol

j = g(zl

j) .

(4.1.3)

Section 4.1. Artiﬁcial Neural Network (ANN)

Page 16

Let us perform the partial derivative of Ec with respect to weights wL
the ANN output layer. For ergonomic reason, the outputs yj(xi) and oL
and oL

k respectively. Hence,

jk and bias bL

j where L represents
k (xi) will be expressed by yj

∂Ec
∂wL
jk

= −

= −

1
n

1
n

(cid:88)

(cid:18)

yj

i

(cid:18)

(cid:88)

oL−1
k

g(cid:48)(zL
j )
g(zL
j )
yj − g(zL
j )
j )(1 − g(zL

g(zL

i

j ))

− (1 − yj)

oL−1
g(cid:48)(zL
j )
k
1 − g(zL
j )

(cid:19)

,

(cid:19)

oL−1
k

g(cid:48)(zL

j ) .

Using Equations (4.1.2) and (4.1.3),

∂Ec
∂wL
jk

=

1
n

(cid:18)

(cid:88)

oL
j − yj

(cid:19)

oL−1
k

.

i

∂Ec
∂bL
j

= −

= −

1
n

1
n

− (1 − yj)

(cid:88)

(cid:18)

yj

i

(cid:18)

(cid:88)

g(cid:48)(zL
j )
g(zL
j )
yj − g(zL
j )
j )(1 − g(zL

g(zL

i

j ))

(cid:19)

g(cid:48)(zL

j ) .

g(cid:48)(zL
j )
1 − g(zL
j )

(cid:19)

,

Using Equations (4.1.2) and (4.1.3),

∂Ec
∂bL
j

=

1
n

(cid:88)

i

(cid:18)

oL
j − yj

(cid:19)

.

(4.1.4)

(4.1.5)

Let us notice that in each output neuron, there is an error due to computation, i.e. during the computa-
tion of the ANN output ol
j).
One can say that this is good news, since the error improves the way the network learns. Nielsen
(Accessed in April 2015) deﬁned the error δl

j, the neuron instead of performing ol

j), it performs yl

j = g(zl

j = g(zl

j + ∆zl

j such as:

∂Ec
∂wl
jk

=

∂Ec
∂zl
j

∂zl
j
∂wl
jk

where δl

j =

∂Ec
∂zl
j

.

(4.1.6)

Using the deﬁnition of zl

j stated in Equation (4.1.3),

∂zl
j
∂wl

jk

= ol−1
k

. Let us now compute the neuron

error δL
j

in the output layer L.

δL
j =

∂Ec
∂zl
j

= −

1
n

(cid:18)

(cid:88)

i

yj − g(zL
j )
j )(1 − g(zL

j ))

g(zL

(cid:19)

g(cid:48)(zL

j ) =

1
n

(cid:18)

(cid:88)

i

g(zL

j ) − yj

(cid:19)

=

1
n

(cid:88)

i

(cid:18)

oL
j − yj

(cid:19)

. (4.1.7)

So, the expressions in Equations (4.1.4) and (4.1.5) can be expressed in terms of δL

j by:

∂Ec
∂wL
jk

= δL

j oL−1
k

and

∂Ec
∂bL
j

= δL
j .

(4.1.8)

∂Ec
∂wL−1
ij

=

=

Finally,

k
∂oL−1
j
∂wL−1
ij

i

(cid:88)

k

∂Ec
∂wL−1
ij

Section 4.1. Artiﬁcial Neural Network (ANN)

Page 17

Since we have δL
start with the layer L − 1 which is close to the output and then generalize it for any hidden layer l.

j , we need to see if we can express the errors δl

j in the hidden layers.

It is better to

∂Ec
∂wL−1
ij

=

1
n

(cid:88)

(cid:88)

(cid:18)

k

i

g(zL

k ) − yk

(cid:19) ∂zL
k
∂wL−1
ij

=

1
n

(cid:88)

(cid:88)

k

i

Using Equation (4.1.3) in the above equation, we have

(cid:18)

g(zL

k ) − yk

(cid:19) ∂zL
k
∂oL−1
j

∂oL−1
j
∂wL−1
ij

.

(cid:88)

(cid:88)

(cid:18)

g(zL

k ) − yk

(cid:19)

wL
jk

1
n

wL

jkδL

k = g(cid:48)(zL−1

j

=

∂oL−1
j
∂wL−1
ij
∂zL−1
j
∂wL−1
ij

)

∂oL−1
j
∂wL−1
ij

(cid:20) (cid:88)

k

wL
jk

(cid:18) 1
n

(cid:18)

(cid:88)

i

g(zL

k ) − yk

(cid:19)(cid:19)(cid:21)

,

(cid:88)

k

wL

jkδL

k = g(zL−1

j

)[1 − g(zL−1

j

)]oL−2
j

wL

jkδL
k .

(cid:88)

k

= oL−2
j

δL−1
j

where δL−1

j

= g(zL−1
j

)(1 − g(zL−1

j

))

(cid:88)

wL

jkδL
k .

(4.1.9)

Since we expressed δL−1
of layer l + 1. More generally the error in layer l is given by:

i , we can thus write the error δl

in terms of δL

j

j = ol+1
δl
j

[1 − ol+1

j

]

ji δl+1
wl+1
i

.

(cid:88)

i

k
j in terms of δl+1

i

for all neuron i

(4.1.10)

As we have seen how the weights and biases change throughout the process using BP, the next step is
to show how the weights and biases are updated.

4.1.4 Update of weights and biases. Stochastic Gradient Descent (SGD) is used to update the weights
and biases. SGD is chosen because of the random choice of weights and biases which avoids converging
with the local minimal of Ec and the consideration of the gradients. Hence, the new weights and biases
are given respectively by Bottou (2012):

wl

ij → wl

ij − β

(cid:88)

ol−1
j

j

j and bl
δl

j → bl

j − β

(cid:88)

δl
j ,

j

(4.1.11)

with the learning rate β chosen adequately. The expressions ol−1
of Ec(w, b) over wl
ij and bl
of the weights and biases.

j and δl
δl
j are respectively the gradients
j. It is shown in (Bottou, 2012) that SGD provides an optimal convergence

j

4.1.5 ANN Algorithm. In summary, the number of hidden layers, the length of epoch T and the
learning rate β deﬁne the ANN architecture. Thus there is no exact theoretical way to design ANN
architecture, except by experiment. By experience and through some papers read (Nielsen, Accessed
in April 2015), one hidden layer is recommended since more than one hidden layer improves the risk of
convergence to a local minimal. So, the ANN can be summarised as follows:

(1) Initialize randomly the weights and biases in ANN in order to avoid a local minimum. (2) Apply
the FF approach to get Neural output using Equation (4.1.3). (3) Compute the output error δL
j using
Equation (4.1.7) and apply BP to compute the output error in the rest of the hidden layers using
Equation (4.1.10). (4) Apply SGD to update the weights and biases using Equation (4.1.11). Let us
now give a toy example in order to improve our understanding of ANN.

Section 4.2. k-Nearest Neighbours (KNN)

Page 18

4.1.6 Example. We are given two inputs neurons with an image x = [1, 0.5], one output neuron with
y = 0 the exact output and the learning rate β = 0.1. (1) We initialized the weights to w1 = 0.5 and
w2 = 0.4, and the bias to b = 0.7. (2) z = (cid:80)
i wixi+b = 0.5×1+0.4×0.5+0.7 = 1.4. The ANN output
is o = g(z) = 0.802. (3) The error in the output is δ = 0.802 − 0 = 0.802. (4) The new weights are
w1 = w1−βx1δ = 0.5−0.1×1×0.802 = 0.4198 and w2 = w2−βx2δ = 0.4−0.1×0.5×0.802 = 0.3599,
the new bias is b = b − βδ = 0.7 − 0.1 × 0.802 = 0.6198. For this ﬁrst step, the entropy error is
Ec(w, b) = 1.619488. Apply FF again until the error Ec becomes close to zero.

Let us jump to the second classiﬁcation technique chosen. This new technique is completely diﬀerent
to ANN but encouraged through its diﬀerent approaches that we will see very soon.

4.2 k-Nearest Neighbours (KNN)

The KNN is the second classiﬁcation technique chosen in this work. KNN looks for k images that
are very close to the query image based on similarity (ﬁtness). A query image xq is assigned to class
ck having more images among k images chosen (Suguna and Thanushkodi, 2010). That means we
look for k images of set Xk which dominate the rest of the images in X \ Xk.
If xk ∈ Xk then
xk (cid:31) xi ∀xi ∈ X \ Xk where (cid:31) expresses the closeness between query image xq and any image in X .

Dealing with a query image xq and training data X , the KNN ﬁrst computes the ﬁtness fxq (j) between
xq and all the images xj ∈ X which is given by the Mahalanobis distance (Weinberger and Saul, 2009):

fxq (j) =

(cid:113)

(xq − xj)tS−1(xq − xj) .

(4.2.1)

The Mahalanobis distance is preferred in this case because of the consideration of covariances S into
account which removes all linear statistical dependencies from data X . To compute the ﬁtness of all
images in training data shows the weakness of KNN. Therefore, the following subsections show how
KNN is improved by combining it with Genetic Approach (GA). One says that GA improves KNN since
images in training data are no longer all considered.

4.2.1 GA Terminology. GA initializes the population P0 ∈ X and in each step, it improves the
population based on genes without increasing its size. Each image xi ∈ X has a genetic representation
called chromosome κi. An image xi is a sequence of real numbers as described in Chapter 2. Then the
25th image in X looks like [x1
25] = [0.1, 0.253, · · · , 0.634]; so its corresponding chromosome
is κ25 = 00 · · · 25
where p is the number of genes in chromosome κi. GA is based on two main
(cid:124) (cid:123)(cid:122) (cid:125)
p

25, · · · , xp

25, x2

operators: crossover and mutation. Crossover Λ creates a new chromosome (oﬀspring (cid:37)) from an
existing chromosome (parent ρ); the oﬀspring obtained from two parents has to inherit their good genes
(Λ(ρ1, ρ2) =⇒ (cid:37)). Mutation λ changes the characteristics of the chromosomes a little such that, if
κnew = λ(κi) then κnew ≈ κi. So far we have a general understanding of GA; let us show how each of
the operators stated above works.

4.2.2 Genetic Operators (Alabsi and Naoum, 2012). Let us assume that each chromosome has
length r in binary representation.

Crossover Λ. In population Pt, we select two parents ρ1 and ρ2, generate a crossover point v randomly
and create two binary masks m1 = 2r − 2v and m2 = 2r − 1. The two oﬀspring are computed as follows:
(cid:37)1 = (ρ1 ∧ m1) ∨ (ρ2 ∧ m2) and (cid:37)2 = (ρ2 ∧ m1) ∨ (ρ1 ∧ m2). All the oﬀspring generated (cid:37)i
belong to the oﬀspring set Qt.

Section 4.3. Support Vector Machine (SVM)

Page 19

In population Tt = Qt ∪ Pt, we select a chromosome κi, generate a gene v randomly
Mutation λ.
and create a binary mask m = 2v. The new chromosome κnew = κi
XOR m. The selection
of chromosomes of new generation Pt+1 is based completely on their ﬁtness function fxq (i) deﬁned
in Equation (4.2.1). Roulette Wheel Selection (Alabsi and Naoum, 2012) is one of the reproduction
techniques used to generate Pt+1. Since mutation and crossover operators are random, then the k
images selected by GA are dominant.

4.2.3 GKNN Algorithm. In summary, the GKNN is given as follows: (1) Generate randomly the ﬁrst
population P0 of k images and evaluate their ﬁtness. (2) Apply the crossover operator to generate
oﬀspring set Qt. (3) Change the characteristics of each element (mutation) κi ∈ Tt = Pt ∪ Qt and
generate new set Nt. (4) Select k images from Nt = Tt ∪ Pt based on their ﬁtness and assign them
to the new population Pt+1. The stopping criterion occurs when the highest ﬁtness in the current
population is less than or equal to that of the previous population. (5) The class of query image xq
corresponds to the class of image having more images among k images of Pt+1 (voting). The vote can
involve probabilities (Suguna and Thanushkodi, 2010). Hence, the class of query image xq corresponds
to the class of image having the largest probability.
4.2.4 Example. We are given the training data X = {(1, 1), (0, 1), (2, 3), (2, 2), (1, 1), (4, 2)}, the
sequence of outputs Y = [1, 1, 2, 2, 1, 2] and the set of chromosomes κ = {001, 010, 011, 100, 101, 110}.
We need to classify the query object x = (1, 0). (1) We initialize P0 = {x1, x2}. Using the ﬁtness
function deﬁned in Equation (4.2.1), we have fx(1) = 1.53 and fx(2) = 2.198. (2) Apply crossover
in P0 with v = 2 and r = 3. Then m1 = 100 and m2 = 111. The oﬀspring are (cid:37)1 = (001 ∧
100) ∨ (010 ∧ 111) = 010 and (cid:37)2 = (001 ∧ 111) ∨ (010 ∧ 100) = 001. Therefore,
T0 = {(cid:37)1, (cid:37)2}. (3) Mutate T0 elements based on their genes. So, with v = 2 we have m = 100.
Therefore, κnew
2 = 010 XOR 100 = 110 = κ6 =⇒ x6.
So the set N0 = {x1, x2, x5, x6}. (4) For generating the new generation P1, we need the ﬁtness of
x5 and x6 which are fx(5) = 1.53 and fx(6) = 2.614. Since fx(1) = fx(5) < fx(2) < fx(6), then
P1 = {x1, x5}. As we can notice that P0 (cid:54)= P1, but after applying the same procedure in P1, we will
get P2 = {x1, x5} = P1. Therefore, we vote for x ∈ c1 since x1 and x5 ∈ c1.

1 = 001 XOR 100 = 101 = κ5 =⇒ x5, κnew

With GKNN the computational cost is reduced and the k images selected are more similar because of
the type of ﬁtness function chosen and the random change of genes during the selection process. Let
us now introduce the last classiﬁcation technique.

4.3 Support Vector Machine (SVM)

SVM is one of the most used classiﬁcation techniques.
Its functionality extends that of ANN. SVM
transforms a non-linear problem in the original space to a linear problem in the feature space by using
the kernel function. SVM classiﬁes data by increasing the margin between data classes while keeping
data separable (Bishop, 1995) as shown in Figure 4.2a. Following in Crammer and Singer (2001), we
call support vectors (SV), the points nearest to the separating hyperplane h.

It follows that, dealing with linear separable data, SVM easily achieves the classiﬁcation task. The
hyperplane is of the form:

h = {x| < w, x > +b = 0} .

(4.3.1)

Let us now describe a binary classiﬁcation with linear separable data in the following subsection in order
to get a good understanding of SVM.

Section 4.3. Support Vector Machine (SVM)

Page 20

(a) Binary classiﬁcation

(b) NL separable

(c) Toy example

Figure 4.2: SVM

4.3.1 Binary classiﬁcation (BC). We call BC any mathematical problem which has two diﬀerent
outcomes as shown in Figure 4.2a. The points in the direction of w belong to class c1 and in the opposite
direction belong to class c2. The variable b is an oﬀset allowing the displacement of the hyperplane and
the normal vector w determines the orientation of the hyperplane. From a mathematical point of view,
w is the weighted sum of the support vectors (SV). SVM can detect more than one hyperplane and
decides which one is the best. The classiﬁer is given by

F (x) = w · xT + b .

(4.3.2)

The decision is taken such that for all xi ∈ c1, F (xi) ≥ 1 and for all xi ∈ c2, F (xi) ≤ −1. Since we
know how BC works, let us introduce a simple toy example in order to give more understanding of BC.
4.3.2 Example. From Figure 4.2c, the hyperplane is well placed between the two points x1 = (1, 1)
and x2 = (2, 2). Clearly the support vector is w = (2, 2) − (1, 1) = (1, 1). More generally, we
can write w = (a, a). Using Equation (4.3.2), we have F (x1) = −1 =⇒ 2a + b = −1 and
F (x2) = 1 =⇒ 4a + b = 1. The solution of these two equations is given by a = 1 and b = −3.
Hence, F (x) = x1 + x2 − 3. The decision can be shown as follows, with x = (1, 0). For instance,
F (x) = −2 ≤ −1 =⇒ (1, 0) ∈ c1. In this example, the SV is directly determined but generally that is
not the case. Therefore, a classiﬁcation task is converted to an optimization problem whereby we seek
to minimize the length of w.

Based on the hypothesis saying that data is non-linear (NL) separable, it is diﬃcult to get a boundary
between classes in original space. The idea is to apply Kernel function in order to transform this problem
to a linear separable one in feature space using inner product. Let us jump to the multiclassiﬁcation
problem with non linear separable data which is our case.

4.3.3 Multiclassiﬁcation. Let X , xi and yi be as deﬁned above. According to Crammer and Singer
(2001), the classiﬁer on X is deﬁned as follows:

FS(xq) = arg

k
max
j=1

{(cid:104)Scj , xq(cid:105)} with S =








and Scj ∈ Rp .

(4.3.3)








Sc1
Sc2
...
Sck

The inner product (cid:104)Scj , xq(cid:105) represents the conﬁdence for image xq to belong to the jth class. Based on
the property of Misclassiﬁcation deﬁned in Chapter 2, the empirical error (Crammer and Singer, 2001)

Section 4.3. Support Vector Machine (SVM)

is deﬁned:

ξX (S) =

1
n

n
(cid:88)

i=1

(cid:18)

(cid:19)

Π

FS(xi) (cid:54)= yi

where Π(λ) =

(cid:40)

1 if λ is true ,
0 otherwise .

Page 21

(4.3.4)

In Crammer and Singer (2001), the error ξX (S) of a single example (xi, yi) written in Equation (4.3.4)
can be rewritten and bounded as:

(cid:18)

(cid:19)

Π

FS(xi) (cid:54)= yi

≤

k
max
j=1

(cid:26)

(cid:27)

Scj xi + 1 − δyicj

− Syixi where

δyicj =

(cid:40)

1 if yi = cj ,
0 otherwise .

It follows from Equations (4.3.4) and (4.3.5) that:

ξX (S) ≤

1
n

n
(cid:88)

i=1

(cid:26)

(cid:20) k
max
j=1

Scj xi + 1 − δyicj

− Syixi

.

(cid:27)

(cid:21)

(4.3.5)

(4.3.6)

It is shown in Crammer and Singer (2001) that

k
max
j=1

{Scj xi + 1 − δyicj } − Syixi = εi ∀i if X is not

linear separable with the slack variable εi ≥ 0. It follows that:

k
max
j=1

{Scj xi + 1 − δyicj } − Syixi = εi =⇒ Scj xi + 1 − δyicj − Syixi ≤ εi .

(4.3.7)

We need to look for a matrix S such that ξX (S) is minimized as much as possible. So, we can transform
this matter to an optimization problem and use Karush-Kuhn-Tucker (KKT) conditions (Luo and Yu,
2006) to solve it.

4.3.4 Optimization problem. We need to apply KKT to solve this optimization problem, thus we need
to deal with a convex problem which is always solvable. Minimizing the matrix S subject to constraint
deﬁned in Equation (4.3.7) can be formulated as follows where A is the regularization constant:

minimize
S,ε

A(cid:107)S(cid:107)2

2 +

n
(cid:88)

i=1

εi ,

subject to: Scj xi − Syixi − δyicj − εi + 1 ≤ 0 ∀i, j .

(4.3.8)

The next step is to get the dual form of equation 4.3.8 in order to consider high space dimension, use
the Quadratic Programming Technique and use the Kernel function of the form κ(x1, x2) = (cid:104)x1, x2(cid:105).
The KKT theorem deﬁned in Luo and Yu (2006) leads to the Lagrangian L(S, ε, λ) where λ is the dual
variable, i.e. min L(S, ε, λ) =⇒ max D(λ). So the primal problem deﬁned in Equation (4.3.8) can
be written in Lagrangian form as follows:

L(S, ε, λ) = A

k
(cid:88)

j=1

(cid:107)Scj (cid:107)2

2+

n
(cid:88)

εi+

n
(cid:88)

k
(cid:88)

(cid:20)
Scj xi−Syixi−δyicj −εi+1

(cid:21)

λicj

subject to λicj ≥ 0 ∀i, j .

i=1

i=1

j=1

Since the variables S and ε are primal, the minimality of primal problem L(S, ε, λ) involves that

and

∂L
∂εi

∂L
∂εi

= 0. Therefore (Crammer and Singer, 2001),

= 0 =⇒

k
(cid:88)

j

λicj = 1 and

∂L
∂Scj

= 0 =⇒ Scj = A−1

(cid:18)

(cid:19)

(cid:21)

δyicj − λicj

· xi

.

(4.3.10)

(cid:20) n
(cid:88)

i=1

(4.3.9)

∂L
∂Scj

= 0

Section 4.4. Dempster-Shafer Fusion (DSF)

Page 22

It is proven (Crammer and Singer, 2001) that the dual max D(λ) of the primal min L(S, ε, λ) which
is given by

maximize
λ

A

n
(cid:88)

i=1

ηi · δyi −

n
(cid:88)

k
(cid:88)

i=1

j=1

κ(xi, xj)(ηi · ηj) ,

subject to: ηi ≤ δyi and ηiδ = 0 ∀i ,

(4.3.11)

its objective function D(λ) is concave. Hence, it admits an unique solution where ηi = δyi − λi =
[ηic1, · · · , ηick ] = [δyic1 − λic1, · · · , δyick − λick ], λi = [λic1, · · · , λick ], δyi = [δyic1, · · · , λyick ], δ =
{1}k = [1, · · · , 1] and the kernel κ(xi, xj) = xi · xj. Using the new notation above, the conﬁdence
vector of class cj deﬁned in Equation (4.3.10) can be rewritten as follows:

Scj = A−1

n
(cid:88)

i=1

ηicj · xi .

(4.3.12)

This leads us to the new formulation of the classiﬁer deﬁned in Equation (4.3.3). The new classiﬁer
(Crammer and Singer, 2001) is given by:

FS(xq) = arg

k
max
j=1

i=1

(cid:26) n
(cid:88)

(cid:27)

ηicj · κ(xi, xq)

.

(4.3.13)

4.3.5 SVM Algorithm. In summary, the SVM is given as follows: (1) Solve the optimization prob-
lem stated in (4.3.11) using the Karush-Kruhn Tucker theorem.
i=1 ηicj · κ(xi, xq)
for j = 1, · · · , k.
(3) Finally the class of the query image xq corresponds to cl such that ηicl =
{(cid:80)n

(2) Compute (cid:80)n

arg

i=1 ηicj · κ(xi, xq)}.

k
max
j=1

It is shown in Figure 2.2 that three diﬀerent feature extraction techniques (FE) are used. Applying each
of the three classiﬁcation techniques on every FE, we obtain a model with nine classiﬁers Mi. Each
model Mi at this stage returns the ith row of decision matrix proﬁle w(xc
q). Therefore, this following
technique combines the diﬀerent classiﬁer outputs in order to obtain an unique improved output.

4.4 Dempster-Shafer Fusion (DSF)

The main and only motivation of combining classiﬁcation outputs is to reduce error in order to solve
property Decision Fusion. The choice of DSF is because of the uncertainties in decision proﬁle matrix
described above is solved. DSF is based on Dempster-Shafer Theory which is a theory of evidence
combining diﬀerent sources of evidence (Benmokhtar and Huet, 2006). Let us give a brief mathematical
description of DSF.

DSF gets the decision proﬁle matrix w(xq) as inputs and this matrix (Bagheri et al., 2013) is deﬁned
as follows:

w(xq) =








ω1(xq)
ω2(xq)
...
ωl(xq)










=






ω1
1(xq) ω2
2(xq) ω2
ω1
...
l (xq) ω2
ω1

1(xq)
2(xq)
...
l (xq)



· · · ωk
· · · ωk
. . .
· · · ωk

1 (xq)
2 (xq)
...
l (xq)






,

(4.4.1)

where ωi ∈ [0, 1]k and ωj
using the classiﬁer model Mi. Take note that (cid:80)k

i (xq) ∈ [0, 1] is a proﬁle degree for the query image xq to belong to class cj
i (xq) = 1. The ﬁnal decision (Bagheri et al.,

j=1 ωj

Section 4.4. Dempster-Shafer Fusion (DSF)

Page 23

2013) is given by computing the value of class support βj(xq) for each class cj. The class corresponding
to the largest βj(xq) will be considered as the ﬁnal class to be aﬀected to the input xq. The computation
is given as follows:

βj(xq) = Aj

l
(cid:89)

i=1

πj[ωi(xq)] ,

(4.4.2)

where the coeﬃcient Aj is chosen in order to ensure the expression (cid:80)l
πj[ωi(xq)] of the class cj obtained from Mi is needed in Equation (4.4.2). Its expression is given by:

j(xq) = 1. The belief degree

i=1 βi

πj[ωi(xq)] =

r(cid:54)=j

j(xq) (cid:81)
λi
(cid:20)
1 − (cid:81)

j(xq)

1 − λi

(cid:18)

(cid:19)

1 − λi

r(xq)

(cid:18)

1 − λi

r(xq)

r(cid:54)=j

(cid:19)(cid:21) .

(4.4.3)

The proximity λi
xq is given by:

j(xq) between the decision template Λj of class cj and the output ωi(xq) of query image

(cid:18)

1 + (cid:107)Λi

j − ωi(xq)(cid:107)

(cid:19)−1

(cid:19)−1 with Λj =

λi
j(xq) =

(cid:80)k

r=1

(cid:18)

1 + (cid:107)Λi

r − ωi(xq)(cid:107)

1
nj

(cid:88)

z∈cj

w(z) ,

(4.4.4)

where Λi
j represents the ith row of Λj and nj the total input data belonging to class cj. As shown in
Figure 2.2, DSF is applied twice in order to get an accurate output. Let us give an example to see how
a decision can be taken through DSF.
4.4.1 Example. We are given respectively the decision proﬁle matrix and the decision templates Λj
in which each row i represents outputs of the ith classiﬁer and each column j represents jth class, for
j = 1, 2 and i = 1, · · · , 4:

w(xq) =







0.7
0.3
0.75 0.25
0.47 0.53
0.5
0.5







, Λ1 =







0.3
0.7
0.9
0.1
0.89 0.11
0.2
0.8







and Λ2 =







0.3 0.7
0.4 0.6
0.3 0.7
0.2 0.8







.

(4.4.5)

From the above decision matrix w(xq), the query image belongs to class 1 according to classiﬁers 1 and
2, it belongs to class 2 according to the classiﬁer 3 while we can not decide for the last classiﬁer. This
is bad news since the output of our query image depends on the classiﬁer. Fortunately DSF solves this
problem. Using Equation (4.4.2), the class support of each class is given by β(xq) = [0.011, 0.006].
Since the support on the ﬁrst class 0.011 is greater than the support of the second, the query image xq
belongs to the ﬁrst class.

To conclude this chapter, we have seen how an image can be classiﬁed using three diﬀerent classiﬁcation
techniques. The ﬁrst technique sought to minimize the error of training; the second technique assigned
a query image to a class based on similarity (closeness); the last technique transformed the problem to
an optimization problem and applied KKT theorem. Since the aim of this work is to help human in
decision-making as stated in our general introduction, we need to get an improved output compared to
those obtained from classiﬁers. DSF is used for this purpose. Let us now present our results.

5. Results and Discussion

According to the objectives assigned to this work, we should present the results on image preprocessing,
feature extraction, classiﬁcation and fusion. Due to the lack of data to train our classiﬁers, we are able
to present only the results on feature extraction. We show how the geometric transformations of an
image do not inﬂuence the extraction of its features. Figure 5.1 presents the sample of shark images
used to validate our model.

Figure 5.1: A shark, its geometric transformations and a ﬁsh (http://www.mid-day.com/)

5.1 Results

sharks
shark
translated shark
scaled rotated shark
ﬁsh

M02M20 M2

9.0868
9.0868
9.0868
2.0727

12M20 M12M21 M2
1.1138
4.153
1.1138
4.153
1.1137
4.1528
1.1178
8.155

21M02 M3
4.153
4.153
4.1528
8.155

13M3
1.4449
1.4449
1.4448
1.4374

42 M3

21M02 M2
5.2694
5.2694
5.269
1.2551

32M2
23
7.3248
7.3248
7.3241
1.3135

Table 5.1: CMI numerical results

(a) GFD Result

(b) ELM Result

Figure 5.2: Feature Extraction Results

Table 5.1 presents the results on Complex Moment. Its ﬁrst row presents the diﬀerent moments used to
get features set; its second row presents the features obtained from the shark; its third and fourth rows

24

Section 5.2. Discussion

Page 25

present the translated and scaled rotated shark features respectively; the last row presents the features
obtained from a normal ﬁsh. Figure 5.2a presents features obtained from the Generic Fourier Descriptor
of shark and its geometric transformations as well. The same for Figure 5.2b that presents features
extracted from Exact Legendre Moment.

5.2 Discussion

We have presented in the previous section the results obtained through our programming on feature
extraction. Presently, we are going to give more explanation on the results, show how they are related
to the mathematical descriptions which have been developed in the previous chapters.

Concerning Feature Extraction techniques, Table 5.1 shows us that the shark and its diﬀerent geometric
transformations give approximatively the same features set in thousandth.
It shows again how shark
features are diﬀerent compared to another object (ﬁsh). This result has conﬁrmed the mathematical
descriptions of Complex Moment made in Chapter 3. The set of features obtained from CMI are invariant
under TRS and two diﬀerent images do not have the same set of features.

Figure 5.2a shows that the three curves of shark and its geometric transformations lie one another apart
from the ﬁsh features presented by a dotted line. Again this result has conﬁrmed the mathematical
descriptions of Generic Fourier Transform made in Chapter 3. The set of features obtained from GFD
are invariant under TRS and two diﬀerent images do not have the same set of features.

As we notice from the result of Exact Legendre Moment shown in Figure 5.2b, the translated shark
gives a feature curve diﬀerent to a normal shark curve while the scaled shark curve lies on the shark
curve. Unfortunately, Chong et al. (2004) conﬁrmed that Exact Legendre Moment is invariant under
translation. This statement is contradicted by our result. The scaled variable given by xi = −1 + iδx
deﬁned in the second section of Chapter 3 (Exact Legendre Moment) where δx = 2
n , requires that the
two images (normal and translated) have the same size n, i.e. they must have the same variation δx
which is not the case. This is the reason of the diﬀerence of the two curves (shark and its translated
form) shown in 5.2b. Unfortunately, the same translated image worked with other feature extraction
techniques such as Complex Moment as shown in Table 5.1. We conclude by saying, the approximation
(3.2.17) of the Legendre Moment is not invariant under translation, unless there is a speciﬁc image with
which it works.

Concerning Image Preprocessing (IP), we did not implement its mathematical techniques. Fortunately,
we have found an image package in Python called Skimage that contains IP techniques implemented
(e.g. Median Filter, Otsu algorithm and Random Walker algorithm), but their results are not presented
in this present work due to lack of time.

After discussing on the results, we are now willing to give the overall conclusion in this coming chapter.

6. Conclusion

We were asked throughout this work to propose an automated system supporting the actual manual
shark spotter program in decision making. The motivation of this work was mainly concerned with poor
weather conditions for spotters to work. We investigated that the spotter ﬁrst detects a suspect object,
both manual and automated systems classify whether it is a shark or not and ﬁnally a decision is taken.

Our experiment was about achieving both theoretically and practically, the speciﬁcation of the model
used: to deﬁne its properties and its architecture. The architecture was built through a Bottom-Up ap-
proach, i.e. we have ﬁrst got an idea of how the architecture had to look, motivated by previous research
in Machine Learning (Benmokhtar and Huet, 2006), and then looked for mathematical techniques that
could accurately cover the model properties. Theoretically we have achieved what was expected, but
an implementation lacked dataset.

Concerning the shark detection architecture that we designed (Figure 2.2), its ﬁrst two steps are speciﬁc
to the shark problem. The motivation of image preprocessing and feature extraction techniques used, are
based on shark properties as described in Chapter 2. The last two steps are generic, i.e. any classiﬁcation
problem can use them.

The interesting parts of this work were the idea of deﬁning the relevant properties of our model and
the extraction of features whose results were presented in the previous chapter. The results have
conﬁrmed the mathematical descriptions made on feature extraction techniques chieﬂy on Complex
Moment and Generic Fourier Descriptor while the property on translation invariance was not satisﬁed
on Exact Legendre Moment. Regarding its weakness, the automated system we proposed depends on
electric power and on human attention since the images are captured by human beings (spotter).

As further work, we suggest:

1. Instead of using captured images to recognize sharks as stated in our general introduction, re-

searchers can think about how to classify sharks based on tracking images.

2. It is desirable that researchers study how feature extraction functions look in terms of its properties
and form. Therefore, they can focus on how to come up with their own functions based on
properties they should cover depending on the problem to be treated.

3. How would hand-oﬀ be operated between beaches in the presence of shark when surfaces covered
by two spotters either overlap or not: to see how the diﬀerent beaches can construct a complete
graph supporting each other on detection of sharks. What are the relevant spotters places in order
to promote communication? In presence of a shark or school of sharks in one area, a spotter will
watch their trajectory. If a shark leaves one area for another or a shark is in a surface where two
beaches overlap, then the ﬁrst spotter who detects a shark has to communicate with neighbouring
spotters.

4. In our general introduction we saw that, with the ﬁrst two ﬂags, spotters absolve their respon-
sibilities due to poor visibility. The present work was mainly motivated by the prevalence of the
last two ﬂags. The new idea is, in order to widen the conditions of use in the case of the ﬁrst two
ﬂags, to place the cameras in diﬀerent strategic positions where an object under water can easily
be spotted (even attached to a hot-air balloon above the beach for example). Such positions can
be chosen to decrease the frequency of either of the ﬁrst two ﬂags being deployed, and hence to
improve the system as a whole.

26

Acknowledgements

Writing a thesis not only requires motivation, but also a lot of time. Firstly, I would like to thank my
God, the Lord ALMIGHTY, the master of time and circumstances. It is through Him that I could ﬁnish
this work.

Secondly, a great thanks to my supervisors, Dr. Simukai Utete and Prof. Jeﬀ Sanders, for giving
me the means to write this work in complete serenity through their fruitful exchanges, sharp scientiﬁc
opinions, always accompanied with friendly encouragement. Thanks to my tutor Martha for reading
and commenting on the improvement of my work. Thanks to all AIMS staﬀ and administration for
this opportunity which has transformed my life.
I would also like to thank AIMS students for their
inducement. Thanks to Prof. Manya Ndjadi and Prof. Kafunda Katalay for telling me about AIMS and
for recommending it to me.

I would also like to thank my parents Augustin Masakuna and Bea Indua through whom God created
I will always be proud to have you as parents. Likewise to all my family and friends for their
me.
I would like to thank Mr. Medard Ilunga, Mr. Nico Nzau, Mr. Georges Matand, Mr.
inducement.
Victor Molisho, Mr. Patiano Mukishi and the rest of the ACGT for giving me this wonderful training
opportunity.

27

References

F. Alabsi and R. Naoum. Comparison of selection methods and crossover operations using steady state
genetic based intrusion detection system. Journal of Emerging Trends in Computing and Information
Sciences, 3:1053–1058, 2012.

M. A. Bagheri, Q. Gao, and S. Escalera. Logo recognition based on the dempster-shafer fusion of

multiple classiﬁers, 2013. in 26th Canadian conf. on Artiﬁcial Intelligence, Regina, Canada.

R. Benmokhtar and B. Huet. Neural network combining classiﬁer based on dempster-shafer theory. In
Proceedings of the International Multiconference on Computer Science and Information Technology,
pages 3–10, Sophia Antipolis, France, 2006. Institut Eurecom.

C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.

L. Bottou. Stochastic gradient descent tricks. Neural networks:Tricks of the trade, 2:421–436, 2012.

C. W. Chong, P. Raveendran, and R. Mukundan. Translation and scale invariants of legendre moments.

The Journal of Pattern Recognition Society, 37:119–129, 2004.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector ma-

chines. Journal of Machine Learning Research, 2:265–292, 2001.

J. Flusser. On the independence of rotation moment invariants. The Journal of Pattern Recognition

Society, 33:1405–1410, 2000.

L. Grady. Random walks for image segmentation. IEEE Transcations on Pattern Analysis and Machine

Intelligence, 28:1–10, 2006.

F. A. Jassim and F. H. Altaani. Hybridization of otsu method and median ﬁlter for color image.

International Journal of Soft Computing and Engineering, 2:1–6, 2013.

Z.-Q. Luo and W. Yu. An introduction to convex optimization for communications and signal processing.

IEEE Journal on Selected Areas in Communication, 24:1426–1438, 2006.

M. Nielsen.

Neural networks and deep learning, Accessed in April 2015.

URL http://

neuralnetworksanddeeplearning.com/index.html.

G. Ou and Y. L. Murphey. Multi-class pattern classiﬁcation using neural networks. The Journal of

Pattern Recognition Society, 40:4–18, 2007.

H. Shu, L. Luo, and J.-L. Coatrieux. Moment-based approaches in imaging. part 1, basic features. IEEE

Engineering in Medicine and Biology Magazine, 26:1–11, 2007.

N. Suguna and D. K. Thanushkodi. An improved k-nearest neighbor classiﬁcation using genetic algo-

rithm. International Journal of Computer Science Issues, 7:18–21, 2010.

K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classiﬁca-

tion. Journal of Machine Learning Research, 10:207–244, 2009.

D. Zhang and G. Lu. Shape based image retrieval using generic fourier descriptors. The Journal of

Pattern Recognition Society, 33:1405–1410, 2000.

28

