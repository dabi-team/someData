0
2
0
2

b
e
F
7
2

]

G
L
.
s
c
[

2
v
6
6
8
4
0
.
9
0
9
1
:
v
i
X
r
a

PREPRINT

1

Deep Declarative Networks: A New Hope

Stephen Gould, Richard Hartley, Dylan Campbell

Abstract—We explore a new class of end-to-end learnable models wherein data processing nodes (or network layers) are deﬁned in
terms of desired behavior rather than an explicit forward function. Speciﬁcally, the forward function is implicitly deﬁned as the solution to
a mathematical optimization problem. Consistent with nomenclature in the programming languages community, we name these models
deep declarative networks. Importantly, we show that the class of deep declarative networks subsumes current deep learning models.
Moreover, invoking the implicit function theorem, we show how gradients can be back-propagated through many declaratively deﬁned
data processing nodes thereby enabling end-to-end learning. We show how these declarative processing nodes can be implemented in
the popular PyTorch deep learning software library allowing declarative and imperative nodes to co-exist within the same network. We
also provide numerous insights and illustrative examples of declarative nodes and demonstrate their application for image and point
cloud classiﬁcation tasks.

Index Terms—Deep Learning, Implicit Differentiation, Declarative Networks

✦

1 INTRODUCTION

M ODERN deep learning models are composed of

parametrized processing nodes (or layers) organized
in a directed graph. There is an entire zoo of different
model architectures categorized primarily by graph struc-
ture and mechanisms for parameter sharing [33]. In all cases
the function for transforming data from the input to the
output of a processing node is explicitly deﬁned. End-to-
end learning is then achieved by back-propagating an error
signal along edges in the graph and adjusting parameters
so as to minimize the error. Almost universally, the error
signal is encoded as the gradient of some global objective (or
regularized loss) function and stochastic gradient descent
based methods are used to iteratively update parameters.
Automatic differentiation (or hand-derived gradients) is
used to compute the derivative of the output of a processing
node with respect to its input, which is then combined
with the chain rule of differentiation proceeding backwards
through the graph. The error gradient can thus be calculated
efﬁciently for all parameters of the model as the signal is
passed backwards through each processing node.

In this paper we advocate for a new class of end-to-
end learnable models, which we call deep declarative net-
works (DDNs), and which collects several recent works
on differentiable optimization [1, 2, 3, 4, 28] and related
ideas [11, 45, 47] into a single framework. Instead of ex-
plicitly deﬁning the forward processing function, nodes in a
DDN are deﬁned implicitly by specifying behavior. That is,
the input–output relationship for a node is deﬁned in terms
of an objective and constraints in a mathematical optimiza-
tion problem, the output being the solution of the problem
conditioned on the input (and parameters). Importantly,
as we will show, we can still perform back-propagation
through a DDN by making use of implicit differentiation.
Moreover, the gradient calculation does not require knowl-

•

S. Gould and D. Campbell are with the Australian Centre for Robotic
Vision and the Research School of Computer Science at the Australian
National University. R. Hartley is with the the Australian Centre for
Robotic Vision and the Research School of Engineering at the Australian
National University. E-mail: stephen.gould@anu.edu.au

edge of the method used to solve the optimization problem,
only the form of the objective and constraints, thereby
allowing any state-of-the-art solver to be used during the
forward pass.

DDNs subsume conventional deep learning models in
that any explicitly deﬁned forward processing function
can also be deﬁned as a node in a DDN. Furthermore,
declaratively deﬁned nodes and explicitly deﬁned nodes
can coexist within the same end-to-end learnable model. To
make the distinction clear, when both types of nodes appear
in the same model we refer to the former as declarative
nodes and the latter as imperative nodes. To this end, we
have developed a reference DDN implementation within
PyTorch [39], a popular software library for deep learning,
supporting both declarative and imperative nodes.

We present some theoretical results that show the condi-
tions under which exact gradients can be computed and the
form of such gradients. We also discuss scenarios in which
the exact gradient cannot be computed (such as non-smooth
objectives) but a descent direction can still be found, allow-
ing stochastic optimization of model parameters to proceed.
These ideas are explored through a series of illustrative ex-
amples and tested experimentally on the problems of image
and point cloud classiﬁcation using modiﬁed ResNet [30]
and PointNet [40] architectures, respectively.

The decisive advantage of the declarative view of deep
neural networks is that it enables the use of classic con-
strained and unconstrained optimization algorithms as a
modular component within a larger, end-to-end learnable
network. This extends the concept of a neural network
layer to include, for example, geometric model ﬁtting,
such as relative or absolute pose solvers or bundle ad-
justment, model-predictive control algorithms, expectation-
maximization, matching, optimal transport, and structured
prediction solvers to name a few. Moreover, the change
in perspective can help us envisage variations of standard
neural network operations with more desirable properties,
such as robust feature pooling instead of standard average
pooling. There is also the potential to reduce opacity and
redundancy in networks by incorporating local model ﬁt-

 
 
 
 
 
 
PREPRINT

2

ting as a component within larger models. For example, we
can directly use the (often nonlinear) underlying physical
and mathematical models rather than having to re-learn
these within the network. Importantly, this allows us to
provide guarantees and enforce hard constraints on rep-
resentations within a model (for example, that a rotation
within a geometric model is valid or that a permutation
matrix is normalized correctly). Furthermore, the approach
is still applicable when no closed form solution exists,
allowing sophisticated approaches with non-differentiable
steps (such as RANSAC [26]) to be used internally. Global
end-to-end learning of network parameters is still possible
even in this case.

Since this is a new approach, there are still several
challenges to be addressed. We discuss some of these in
this paper but leave many for future work by us and the
community. Some preliminary ideas relating to declarative
networks appear in previous works (discussed below) but,
to the best of our knowledge, this paper is the ﬁrst to present
a general coherent framework for describing these models.

2 BACKGROUND AND RELATED WORK

The ability to differentiate through declarative nodes relies
on the implicit function theorem, which has a long history
whose roots can be traced back to works by Descartes,
Leibniz, Bernoulli and Euler [44]. It was Cauchy who ﬁrst
placed the theorem on rigorous mathematical grounds and
Dini who ﬁrst presented the theorem in its modern multi-
variate form [21, 32]. Roughly speaking, the theorem states
conditions under which the derivative of a variable y with
respect to another variable x exists for implicitly deﬁned
functions, f (x, y) = 0, and provides a means for computing
the derivative of y with respect to x when it does exist.

In the context of deep learning,

it is the derivative
of the output of a (declarative) node with respect to its
input that facilitates end-to-end parameter learning by back-
propagation [33, 41]. In this sense the learning problem is
formulated as an optimization problem on a given error
metric or regularized loss function. When declarative nodes
appear in the network, computing the network output and
hence the loss function itself requires solving an inner op-
timization problem. Formally, we can think of the learning
problem as an upper optimization problem and the network
output as being obtained from a lower optimization prob-
lem within a bi-level optimization framework [7, 46].

Bi-level optimization (and the need for implicit differ-
entiation) has appeared in various settings in the machine
learning literature, most notably for the problem of meta-
learning. For example, Do et al. [19] consider the problem of
determining regularization parameters for log-linear models
formulated as a bi-level optimization problem. Domke [20]
addresses the problem of learning parameters of continuous
energy-based models wherein inference (equivalent to the
forward pass in a neural network) requires ﬁnding the min-
imizer of a so-called energy function. The resulting learning
problem is then necessarily bi-level. Last, Klatzer and Pock
[31] propose a bi-level optimization framework for choosing
hyper-parameter settings for support vector machines that
avoids the need for cross-validation.

In computer vision and image processing, bi-level op-
timization has been used to formulate solutions to pixel-
labeling problems. Samuel and Tappen [42] propose learn-
ing parameters of a continuous Markov random ﬁeld with
bi-level optimization and apply their technique to image
denoising and in-painting. The approach is a special case of
energy-based model learning [20]. Ochs et al. [37] extend bi-
level optimization to handle non-smooth lower-level prob-
lems and apply their method to image segmentation tasks.
Recent works have started to consider the question of
placing speciﬁc optimization problems within deep learning
models [4, 11, 28, 45, 47]. These approaches can be thought
of as establishing the groundwork for DDNs by developing
speciﬁc declarative components. Gould et al. [28] summa-
rize some general results for differentiating through un-
constrained, linearly constrained and inequality constrained
optimization problems. In the case of unconstrained and
equality constrained problems the results are exact whereas
for inequality constrained problems they make use of an
interior-point approximation. We extend these results to the
case of exact differentiation for problems with nonlinear
equality and inequality constraints.

Amos and Kolter [4] also show how to differentiate
through an optimization problem, for the speciﬁc case of
quadratic programs (QPs). A full account of the work, in-
cluding discussion of more general cone programs, appears
in Amos [3]. Along the same lines Agrawal et al. [2] report
results for efﬁciently differentiating through cone programs
with millions of parameters. In both cases (quadratic pro-
grams and cone programs) the problems are convex and
efﬁcient algorithms exist for ﬁnding the minimum. We make
no restriction on convexity for declarative nodes but still
assume that an algorithm exists for evaluating them in the
forward pass. Moreover, our paper establishes a uniﬁed
framework for viewing these works in an end-to-end learn-
able model—the deep declarative network (DDN).

Other works consider the problem of differentiating
through discrete submodular problems [18, 45]. These prob-
lems have the nice property that minimizing a convex relax-
ation still results in the optimal solution to the submodular
minimization problem, which allows the derivative to be
computed [18]. For submodular maximization there exist
polynomial time algorithms to ﬁnd approximate solutions.
Smoothing of these solutions results in a differentiable
model as demonstrated in Tschiatschek et al. [45].

Close in spirit to the idea of a deep declarative network is
the recently proposed SATNet [47]. Here MAXSAT problems
are approximated by solving a semi-deﬁnite program (SDP),
which is differentiable. A fast solver allows the problem to
be evaluated efﬁciently in the forward pass and thanks to
implicit differentiation there is no need to explicitly unroll
the optimization procedure, and hence store Jacobians, mak-
ing it memory and time efﬁcient in the backward pass. The
method is applied to solving Sudoku problems presented
as images, which requires that the learned network encode
logical constraints.

Another interesting class of models that ﬁt into the deep
declarative networks framework are deep equilibrium mod-
els recently proposed by Bai et al. [6]. Here the model exe-
cutes a sequence of ﬁxed-point iterations y(t) = f (x, y(t−1))
until convergence in the forward pass. Bai et al. [6] show

PREPRINT

3

that rather than back-propagating through the unbounded
sequence of ﬁxed-point iterations, the derivative of the solu-
tion y with respect to input x can be computed directly via
the implicit function theorem by observing that y satisﬁes
implicit function f (x, y) − y = 0.

Chen et al. [11] show how to differentiate through or-
dinary differential equation (ODE) initial value problems at
some time T using the adjoint sensitivity method, and view
residual networks [30] as a discretization of such problems.
This approach can be interpreted as solving a feasibility
problem with an integral constraint function, and hence
an exotic type of declarative node. The requisite gradients
in the backward pass are computed elegantly by solving
a second augmented ODE. While such problems can be
included within our declarative framework, in this paper
we focus on results for twice-differentiable objective and
constraints functions that can be expressed in closed form.

There have also been several works that have proposed
differentiable components based on optimization problems
for addressing speciﬁc tasks. In the context of video clas-
siﬁcation, Fernando and Gould [23, 24] show how to dif-
ferentiate through a rank-pooling operator [25] within a
deep learning model, which involves solving a support
vector regression problem. The approach was subsequently
generalized to allow for a subspace representation of the
video and learning on a manifold [12].

Santa Cruz et al. [43] propose a deep learning model for
learning permutation matrices for visual attribute ranking
and comparison. Two variants are proposed both relax-
ing the permutation matrix representation to a doubly-
stochastic matrix representation. The ﬁrst variant involves
iteratively normalizing rows and columns to approximately
project a positive matrix onto the set of doubly-stochastic
matrices. The second variant involves formulating the pro-
jection as a quadratic problem and solving exactly.

Lee et al. [34] consider the problem of few-shot learning
for visual recognition. They embed a differentiable QP [4]
into a deep learning model that allows linear classiﬁers to be
trained as a basis for generalizing to new visual categories.
Promising results are achieved on standard benchmarks and
they report low training overhead in that solving the QP
takes about the same time as image feature extraction.

In the context of planning and control, Amos et al. [5]
propose a differentiable model predictive controller in a
reinforcement learning setting. They are able to demonstrate
superior performance on classic pendulum and cartpole
problems. De Avila Belbute-Peres et al. [14] show how to
differentiate through physical models, speciﬁcally, the op-
timal solution of a linear complementarity problem, which
allows a simulated physics environment to be placed within
an end-to-end learnable system.

Last, imposing hard constraints on the output of a deep
neural network using a Krylov subspace method was inves-
tigated in M´arquez-Neila et al. [35]. The approach is applied
to the task of human pose estimation and demonstrates the
feasibility of training a very high-dimensional model that
enforces hard constraints, albeit without improving over a
softly constrained baseline.

3 NOTATION

Our results require differentiating vector-valued functions
with respect to vector arguments. To assist presentation we
clarify our notation here. Consider the function f (x, y, z)
where both y and z are themselves functions of x. We have

d
dx

f =

∂f
∂x

dx
dx

+

∂f
∂y

dy
dx

+

∂f
∂z

dz
dx

(1)

by the chain rule of differentiation. For functions taking
vector arguments, f : Rn → R, we write the derivative
vector as

Df =

∂f
∂x2
For vector-valued functions f : R → Rm we write

∂f
∂xn (cid:21)

∈ R1×n.

∂f
∂x1

, . . . ,

(cid:20)

,

(2)

Df =

dfm
dx
More generally, we deﬁne the derivative Df of f : Rn →

∈ Rm×1.

df1
dx

, . . . ,

(3)

(cid:21)

(cid:20)

T

Rm as an m × n matrix with entries
∂fi
∂xj

Df (x)

=

ij

(x).

(cid:16)

(cid:17)

Then the chain rule for h(x) = g(f (x)) is simply

Dh(x) = Dg(f (x))Df (x)

(4)

(5)

where the matrices automatically have the right dimensions
and standard matrix-vector multiplication applies. When
taking partial derivatives we use a subscript to denote
the formal variable over which the derivative is computed
(the remaining variables ﬁxed), for example DX f (x, y). For
brevity we use the shorthand D2

XY f to mean DX (DY f )T

.

When no subscript is given for multi-variate functions
we take D to mean the total derivative with respect to
the independent variables. So, with x as the independent
variable, the vector version of Equation 1 becomes

Df = DX f + DY f Dy + DZf Dz.

(6)

4 DEEP DECLARATIVE NETWORKS

We are concerned with data processing nodes in deep neural
networks whose output y ∈ Rm is deﬁned as the solution
to a constrained optimization problem parametrized by the
input x ∈ Rn. In the most general setting we have

y ∈ arg min

f (x, u)

u∈C

(7)

where f : Rn × Rm → R is an arbitrary parametrized
objective function and C ⊆ Rm is an arbitrary constraint
set (that may also be parametrized by x).1

To distinguish such processing nodes from processing
nodes (or layers) found in conventional deep learning mod-
els, which specify an explicit mapping from input to output,
we refer to the former as declarative nodes and the latter as
imperative nodes, which we formally deﬁne as follows.

Deﬁnition 4.1: (Imperative Node). An imperative node is
one in which the implementation of the forward processing

1. In practical deep learning systems inputs and outputs are generally
one or more multi-dimensional tensors. For clarity of exposition we
consider inputs and outputs as single vectors without loss of generality.

PREPRINT

x

DJ(x)

x

DJ(x)

θ DJ(θ)

y = ˜f (x; θ)

θ DJ(θ)

y ∈ arg min

f (x, u; θ)

u∈C

4

x

y ∈ arg min

f (x, u)

u∈C

y

J(x, y)

y

DJ(x)

(∗)

DY J(x, y)

DJ(y)

y

DJ(y)

DX J(x, y)

Fig.
2: Bi-level optimization problem showing back-
propagation of gradients through a deep declarative node.
The quantity (∗) is DY J(x, y)Dy(x) which when added to
DX J(x, y) gives DJ(x, y(x)). The bypass connections (topmost
and bottommost paths) do not exist when the upper-level
objective J only depends on x through y. Moreover, if f appears
in J as the only term involving y then DY J(x, y) is zero and the
backward edge (∗) is not required. That is, DJ(x) = DX J(x, y).

Fig. 1: Parametrized data processing nodes in an end-to-end
learnable model with global objective function J. During the
forward evaluation pass of an imperative node (top) the in-
put x is transformed into output y based on some explicit
parametrized function ˜f (·; θ). During the forward evaluation
pass of a declarative node (bottom) the output y is computed
as the minimizer of some parametrized objective function
f (x, ·; θ). During the backward parameter update pass for
either node type, the gradient of the global objective function
with respect to the output DJ(y) is propagated backwards via
the chain rule to produce gradients with respect to the input
DJ(x) and parameters DJ(θ).

function ˜f is explicitly deﬁned. The output is then deﬁned
mathematically as

y = ˜f (x; θ)

where θ are the parameters of the node (possibly empty).

Deﬁnition 4.2: (Declarative Node). A declarative node is
one in which the exact implementation of the forward
processing function is not deﬁned; rather the input–output
relationship (x 7→ y) is deﬁned in terms of behavior speci-
ﬁed as the solution to an optimization problem

y ∈ arg min

f (x, u; θ)

u∈C

is discrete). Nevertheless, under certain conditions on the
objective and constraints, covering a wide range of problems
or problem approximations (such as shown by Wang et al.
[47] for MAXSAT), the declarative node is differentiable and
can be placed within an end-to-end learnable model as we
discuss below.

4.1 Learning

We make no assumption about how the optimal solution y is
obtained in a declarative node, only that there exists some
algorithm for computing it. The consequence of this as-
sumption is that, when performing back-propagation, we do
not need to propagate gradients through a complicated algo-
rithmic procedure from output to input. Rather we rely on
implicit differentiation to directly compute the gradient of
the node’s output with respect to its input (or parameters).
Within the context of end-to-end learning with respect to
some global objective (i.e., loss function) this becomes a bi-
level (multi-level) optimization problem [7, 15], which was
ﬁrst studied in the context of two-player leader–follower
games by Stackelberg in the 1930s [46]. Formally, we can
write

minimize J(x, y)
subject to y ∈ arg minu∈C f (x, u)

(8)

where f is the (parametrized) objective function, θ are the
node parameters (possibly empty), and C is the set of
feasible solutions, that is, constraints.

Figure 1 is a schematic illustration of the different types
of nodes. Since nodes form part of an end-to-end learnable
model we are interested in propagating gradients back-
wards through them. In the sequel we make no distinction
between the inputs and the parameters of a node as these
are treated the same for the purpose of computing gradients.
The deﬁnition of declarative nodes (Deﬁnition 4.2) is
very general and encompasses many subproblems that we
would like embedded within a network—robust ﬁtting,
projection onto a constraint set, matching, optimal transport,
physical simulations, etc. Some declarative nodes may not
be efﬁcient to evaluate (that is, solve the optimization prob-
lem specifying its behavior), and in some cases the gradient
may not exist (e.g., when the feasible set or output space

where the minimization is over all tunable parameters in the
network. Here J(x, y) may depend on y through additional
layers of processing. Note also that y is itself a function of
x. As such minimizing the objective via gradient descent
requires the following computation

DJ(x, y) = DX J(x, y) + DY J(x, y)Dy(x),

(9)

The key challenge for declarative nodes then is the calcu-
lation of Dy(x), which we will discuss in Section 4.3. A
schematic illustration of a bi-level optimization problem and
associated gradient calculations is shown in Figure 2.

The higher-level objective J is often deﬁned as the sum
of various terms (including loss terms and regularization
terms) and decomposes over individual examples from a
training dataset. In such cases the gradient of J with respect
to model parameters also decomposes as the sum of gradi-
ents for losses on each training example (and regularizers).

PREPRINT

5

A simpler form of the gradient arises when the lower-
level objective function f appears in the upper-level objec-
tive J as the only term involving y. Formally, if J(x, y) =
g(x, f (x, y)) and the lower-level problem is unconstrained
(that is, C = Rm), then

DJ(x, y) = DX g(x, f ) + DF g(x, f ) (Df + DYf Dy)

= DX g(x, f ) + DF g(x, f )Df

(10)

since DY f (x, y) = 0 by virtue of y being an unconstrained
minimum of f (x, ·). For example, if J(x, y) = f (x, y),
then DJ(x, y) = DX f (x, y). This amounts to a variant of
block coordinate descent on x and y, where y is optimized
fully during each step, and in such cases we do not need
to propagate gradients backwards through the declarative
node. The remainder of this paper is focused on the more
general case where y appears in other terms in the loss
function (possibly through composition with other nodes)
and hence calculation of Dy is required.

4.2 Common sub-classes of declarative nodes

It will be useful to consider three very common cases of the
general setting presented in Equation 7, in increasing levels
of sophistication. The ﬁrst is the unconstrained case,

4.3 Back-propagation through declarative nodes

Our results for computing gradients are based on im-
plicit differentiation. We begin with the unconstrained case,
which has appeared in different guises in several previous
works [28]. The result is a straightforward application of
Dini’s implicit function theorem [21, p19] applied to the
ﬁrst-order optimality condition DY f (x, y) = 0.

Proposition 4.4 (Unconstrained): Consider a function f :
Rn × Rm → R. Let

y(x) ∈ arg min

u∈Rm

f (x, u).

Assume y(x) exists and that f is second-order differen-
tiable in the neighborhood of the point (x, y(x)). Set H =
XY f (x, y(x)) ∈ Rm×n.
D2
Then for H non-singular the derivative of y with respect to
x is

Y Y f (x, y(x)) ∈ Rm×m and B = D2

Dy(x) = −H −1B.

Proof. For any optimal y, the ﬁrst-order optimality condition
requires DY f (x, y) = 01×m. The result then follows from
the implicit function theorem: transposing and differentiat-
ing both sides with respect to x we have

0m×n = D (DY f (x, y))

T

= D2

XY f (x, y) + D2

Y Y f (x, y)Dy(x)

(14)

y ∈ arg min

u∈Rm

f (x, u)

(11)

which can be rearranged to give

where the objective function f is minimized over the en-
tire m-dimensional output space. The second is when the
feasible set is deﬁned by p possibly nonlinear equality
constraints,

y ∈ arg minu∈Rm f (x, u)

subject to

hi(x, u) = 0,

i = 1, . . . , p.

(12)

The third is when the feasible set is deﬁned by p equality
and q inequality constraints,

y ∈ arg minu∈Rm f (x, u)

subject to

hi(x, u) = 0,
gi(x, u) ≤ 0,

i = 1, . . . , p
i = 1, . . . , q.

(13)

Note that in all cases we can think of y as an implicit
function of x, and will often write y(x) to make the input–
output relationship clear. We now show that, under mild
conditions, the gradient of y with respect to x can be
computed for these and other special cases without having
to know anything about the algorithmic procedure used to
solve for y in the ﬁrst place.

In the sequel it will be useful to characterize solutions y

using the concept of regularity due to Bertsekas [9, §3.3.1].

Deﬁnition 4.3: (Regular Point). A feasible point u is said
to be regular if the equality constraint gradients DU hi and
the active inequality constraint gradients DU gi are linearly
independent, or there are no equality constraints and the
inequality constraints are all inactive at u.2

Dy(x) = −

D2

Y Y f (x, y)

−1

D2

XY f (x, y)

(15)

when D2

(cid:0)
Y Y f (x, y) is non-singular.

(cid:1)

In fact, the result above holds for any stationary point
of f (x, ·) not just minima. However, for declarative nodes it
is the minima that are of interest. Our next result gives the
gradient for equality constrained declarative nodes, making
use of the fact that the optimal solution of a constrained
optimization problem is a stationary point of the Lagrangian
associated with the problem.

Proposition 4.5 (Equality Constrained): Consider functions
f : Rn × Rm → R and h : Rn × Rm → Rp. Let

y(x) ∈ arg minu∈Rm f (x, u)

subject to

hi(x, u) = 0, i = 1, . . . , p.

Assume that y(x) exists, that f and h = [h1, . . . , hp]T
are
second-order differentiable in the neighborhood of (x, y(x)),
and that rank(DY h(x, y)) = p. Then for H non-singular

T
Dy(x) = H −1A

T
AH −1A
(cid:16)

−1

(cid:17)

(cid:0)

AH −1B − C

− H −1B

(cid:1)

where

A = DY h(x, y) ∈ Rp×m

p

B = D2

XY f (x, y) −

λiD2

XY hi(x, y) ∈ Rm×n

i=1
X
C = DX h(x, y) ∈ Rp×n

p

H = D2

Y Y f (x, y) −

λiD2

Y Y hi(x, y) ∈ Rm×m

2. An inequality constraint gi is active for a feasible point u if

gi(x, u) = 0 and inactive if gi(x, u) < 0.

and λ ∈ Rp satisﬁes λTA = DY f (x, y).

i=1
X

PREPRINT

Proof. By the method of Lagrange multipliers [8] we can
form the Lagrangian

L(x, y, λ) = f (x, y) −

λihi(x, y).

(16)

p

i=1
X

Assume y is optimal for a ﬁxed input x. Since DY h(x, y) is
full rank we have that y is a regular point. Then there exists
a λ such that the Lagrangian is stationary at the point (y, λ).
Here both y and λ are understood to be functions of x. Thus

(DY f (x, y) −

(cid:20)

p
i=1 λiDY hi(x, y))

h(x, y)
P

T

(cid:21)

= 0m+p

(17)

where the ﬁrst m rows are from differentiating L with
respect to y (that is, DY LT
) and the last p rows are from
differentiating L with respect to λ (that is, DΛLT

).

Now observe that at the optimal point y we have that
either DY f (x, y) = 01×m, that is, the optimal point of
the unconstrained problem automatically satisﬁes the con-
straints, or DY f (x, y) is non-zero and orthogonal to the
constraint surface deﬁned by h(x, y) = 0. In the ﬁrst case
we can simply set λ = 0p. In the second case we have (from
the ﬁrst row in Equation 17)

DY f (x, y) =

for A deﬁned above.

p

i=1
X

T
A
λiDY hi(x, y) = λ

(18)

Now, differentiating the gradient of the Lagrangian with

respect to x we have

6
computed explicitly solving λTA = DY f (x, y), which has
unique analytic solution

λ = (AA

T

T
)−1A (DY f )

.

(25)

Thus given optimal y we can ﬁnd λ.

Although the result looks expensive to compute, much
of the computation is shared since H −1AT
is independent
of the coordinate of x for which the gradient is being com-
puted. Moreover, H need only be factored once and then
reused in computing H −1B for each input dimension, that
is, column of B. And for many problems H has structure
that can be further exploited to speed calculation of Dy.

Inequality constrained problems encompass a richer
class of deep declarative nodes. Gould et al. [28] addressed
the question of computing gradients for these problems by
approximating the inequality constraints with a logarithmic
barrier [10] and leveraging the result for unconstrained
problems. However,
it is also possible to calculate the
gradient without resorting to approximation by leveraging
the KKT optimality conditions [9] as has been shown in
previous work for the special case of convex optimization
problems [2, 4]. Here we present a result for the more
general non-convex case.

We ﬁrst observe that for a smooth objective function
f the optimal solution y(x) to the inequality constrained
problem in Equation 13 is also a local (possibly global)
minimum of the problem with any or all inactive inequality
constraints removed. Moreover, the derivative Dy(x) is the
same for both problems. As such, without loss of generality,
we can assume that all inequality constraints are active.

(DY f (x, y))T −

p

i=1 λi(DY hi(x, y))T

D

h(x, y)
P
For the ﬁrst row this gives

(cid:20)

= 0

(19)

(cid:21)

Proposition 4.6 (Inequality Constrained): Consider func-
tions f : Rn × Rm → R, h : Rn × Rm → Rp and
g : Rn × Rm → Rq. Let

D2

XY f + D2
Y Y f Dy − DY h
p

T

Dλ

−

λi

D2

XY hi + D2

Y Y hiDy

= 0m×n

(20)

subject to

hi(x, u) = 0, i = 1, . . . , p
gi(x, u) ≤ 0, i = 1, . . . , q.

y(x) ∈ arg minu∈Rm f (x, u)

i=1
X
and for the second row we have

(cid:0)

(cid:1)

DX h + DY hDy = 0p×n.

(21)

Therefore

D2

Y Y f −

(cid:20)

+

p
i=1 λiD2

Y Y hi −DY hT

DY h
P
D2
XY f −

p
i=1 λiD2

0p×p (cid:21) (cid:20)
XY hi

(cid:21)
= 0(m+p)×n

(cid:20)

DX h
P

(cid:21)

Dy
Dλ

where all functions are evaluated at (x, y). We can now solve
by variable elimination [10] to get

Assume that y(x) exists, that f , h and g are second-
order differentiable in the neighborhood of (x, y(x)), and
that all inequality constraints are active at y(x). Let ˜h =
[h1, . . . , hp, g1, . . . , gq] and assume rank(DY ˜h(x, y)) = p+q.
Then for H non-singular

T
Dy(x) = H −1A

T
AH −1A
(cid:16)

−1

(cid:17)

(cid:0)

AH −1B − C

− H −1B

(cid:1)

where

(22)

A = DY ˜h(x, y) ∈ R(p+q)×m

p+q

B = D2

XY f (x, y) −

λiD2

XY

˜hi(x, y) ∈ Rm×n

Dλ(x) =

T
AH −1A
(cid:16)

−1

AH −1B − C

(23)

C = DX ˜h(x, y) ∈ R(p+q)×n

i=1
X

T
Dy(x) = H −1A

(cid:17)
(cid:0)
T
AH −1A

−1

AH −1B − C

(cid:1)

− H −1B

(cid:16)

(cid:17)

(cid:0)

(cid:1)

(24)

with A, B, C, and H as deﬁned above.

The Lagrange multipliers λ in Proposition 4.5 are often
made available by the method used to solve for y(x), e.g.,
in the case of primal-dual methods for convex optimization
problems. Where it is not provided by the solver it can be

H = D2

Y Y f (x, y) −

λiD2

Y Y

˜hi(x, y) ∈ Rm×m

p+q

i=1
X

and λ ∈ Rp+q satisﬁes λTA = DY f (x, y) with λi ≤ 0 for i =
p + 1, . . . , p + q. The gradient Dy(x) is one-sided whenever
there exists an i > p such that λi = 0.

Proof. The existence of λ and non-negativity of λi
for
i = p + 1, . . . , p + q comes from the fact that y(x) is a regular

PREPRINT

g(y) < 0

y1

y2

Assume that y(x) exists, that f (x, u) and h(u) are second-
order differentiable in the neighborhood of (x, y(x)) and
y(x), respectively, and that DY h(y(x)) 6= 0. Then

7

Dy(x) =

H −1aaTH −1
aTH −1a

− H −1

B

!

y3

where

Fig. 3: Illustration of different scenarios for the solution to
inequality constrained deep declarative nodes. In the ﬁrst sce-
nario (y1) the solution is a local minimum strictly satisfying the
constraints. In the second scenario (y2) the solution is on the
boundary of the constraint set with the negative gradient of the
objective pointing outside of the set. In the third scenario (y3)
the solution is on the boundary of the constraint set and is also
a local minimum.

point (because DY ˜h(x, y) is full rank). The remainder of
the proof follows Proposition 4.5. One-sidedness of Dy(x)
results from the fact that λi(x), for i = p + 1, . . . , p + q, is
not differentiable at zero.

The main difference between the result for inequality
constrained problems (Proposition 4.6) and the simpler
equality constrained case (Proposition 4.5) is that the gra-
dient is discontinuous at points where any of the Lagrange
multipliers for (active) inequality constraints are zero. We
illustrate this by analysing a deep declarative node with a
single inequality constraint,

y ∈ arg minu∈Rm f (x, u)

subject to

g(x, u) ≤ 0

(26)

where both f and g are smooth.

Consider three scenarios (see Figure 3). First,

if the
constraint is inactive at solution y, that is, g(x, y) < 0,
then we must have DY f (x, y) = 0 and can take Dy(x)
to be the same as for the unconstrained case. Second, if
the constraint is active at the solution, that is, g(x, y) = 0
but DY f (x, y) 6= 0 then in must be that λ 6= 0, and from
Proposition 4.6, the gradient Dy(x) is the same as if we
had replaced the inequality constraint with an equality. Last,
if the constraint is active at y and DY f (x, y) = 0 then
Dy(x) is undeﬁned. In this last scenario we can choose
either the unconstrained or constrained gradient in order
to obtain an update direction for back-propagation similar
to how functions such as rectiﬁed linear units are treated in
standard deep learning models.

4.4 Simpler equality constraints

Often there is only a single ﬁxed equality constraint that
does not depend on the inputs—we consider such problems
in Section 5.2—or the equality constraints are all linear. The
above result can be specialized for these cases.
Corollary 4.7: Consider functions f : Rn × Rm → R and
h : Rm → R. Let

y(x) ∈ arg minu∈Rm f (x, u)

subject to

h(u) = 0.

∈ Rm,

T
a = (DY h(y))
B = D2
H = D2

XY f (x, y) ∈ Rm×n,
Y Y f (x, y) − λD2
∂h
∂yi

−1 ∂f
∂yi

(y)

(cid:19)

λ =

(cid:18)

Y Y h(y) ∈ Rm×m, and
∂h
∂yi

(x, y) ∈ R for any

(y) 6= 0.

Proof. Follows from Proposition 4.5 with p = 1, DX h ≡
01×n and D2

XY h ≡ 0m×n.

Observation 4.8: When the constraint function is indepen-
dent of x we have Dy(x) ⊥ DY h(y), from Equation 21 with
DX h = 0. In other words, y can only change (as a function
of x) in directions that maintain the constraint h(y) = 0.

Given this simpler form of the gradient for declara-
tive nodes with a single equality constraint, one might
be tempted to naively combine multiple equality con-
straints into a single constraint, for example ˜h(x, u) ,
i (x, u) = 0 (or any other function of the hi’s that is
identically zero if any only if the hi’s are all zero). However,
P
this will not work as it violates the assumptions of the
method of Lagrange multipliers, namely that DY ˜h(x, y) 6= 0
at the optimal point.

p
i=1 h2

The following result is for the common case of multiple
ﬁxed linear equality constraints that do not depend on the
inputs (which has also been reported previously [28]).
Corollary 4.9: Consider a function f : Rn × Rm → R and
let A ∈ Rp×m and d ∈ Rp with rank(A) = p deﬁne a set of
p under-constrained linear equations Au = d. Let

y(x) ∈ arg minu∈Rm f (x, u)
subject to Au = d.

Assume that y(x) exists and that f (x, u) is second-order
differentiable in the neighborhood of (x, y(x)). Then

Dy(x) =

T
H −1A
(cid:16)
Y Y f (x, y) and B = D2
where H = D2

T
(AH −1A

XY f (x, y).

)−1AH −1 − H −1

B

(cid:17)

Proof. Follows from Proposition 4.5 with h(x, u) , Au−d so
XY h ≡ 0m×n and D2
that DX h ≡ 0p×n, D2

Y Y h ≡ 0m×m.

4.5 Geometric interpretation

Consider, for simplicity, a declarative node with scalar input
x (n = 1) and a single equality constraint (p = 1). An
alternative form for the gradient from Proposition 4.5, which
lends itself to geometric interpretation as shown in Figure 4,
is given by

Dy(x) = H − 1

2

˜b −

ˆa

(cid:18)

(cid:16)

ˆa +

T˜b
(cid:17)

c
k˜ak

ˆa

(cid:19)

(27)

 
PREPRINT

8

(ˆa · ˜b)ˆa

ˆa

˜b = H

1

2 Dyunc(x)

H

1

2 Dy(x)

˜b − (ˆa · ˜b)ˆa

O

− c

k˜ak ˆa

Fig. 4: Geometry of the gradient for an equality constrained
optimization problem. The unconstrained gradient Dyunc(x) is
corrected to ensure that the solution remains on the constraint
surface after gradient descent with an inﬁnitesimal step size.

2 a, ˜b = −H − 1

where ˜a = −H − 1
2 b and ˆa = k˜ak−1˜a. First,
consider the case when the constraints do not depend on
the input (c = 0). The unconstrained derivative Dyunc(x) =
H − 1
2 ˜b (equality when λ = 0) is corrected by removing the
component perpendicular to the constraint surface. That is,
the unconstrained gradient is projected to the tangent plane
of the constraint surface, as shown in Figure 4. The gradi-
ent is therefore parallel to the constraint surface, ensuring
that gradient descent with an inﬁnitesimal step size does
not push the solution away from the constraint surface.
Thus, the equation encodes the following procedure: (i) pre-
1
multiply the unconstrained gradient vector Dyunc(x) by H
2 ;
(ii) project onto the linearly transformed (by H − 1
2 ) tangent
plane of the constraint surface; and (iv) pre-multiply by
H − 1
2 . Observe that the equation compensates for the cur-
vature H before the projection. The same intuition applies
to the case where the constraints depend on the inputs
(c 6= 0), with an additional bias term accounting for how
the constraint surface changes with the input x.

4.6 Relationship to conventional neural networks

subsume traditional

feed-
Deep declarative networks
forward and recurrent neural networks in the sense that
any layer in the network that explicitly deﬁnes its outputs in
terms of a differentiable function of its inputs can be reduced
to a declarative processing node. A similar observation was
made by Amos and Kolter [4], and the notion is formalized
in the following proposition.
Proposition 4.10: Let ˜f : Rn → Rm be an explicitly deﬁned
differentiable forward processing function for a neural net-
work layer. Then we can alternatively deﬁne the behavior of
the layer declaratively as

y = arg min

u∈Rm

1
2

ku − ˜f (x)k2.

2 ku − ˜f (x)k2 is
Proof. The objective function f (x, u) = 1
strongly convex. Differentiating it with respect to u and
setting to zero we get y = ˜f (x). By Proposition 4.4 we have
H = I and B = −DX ˜f (x), giving Dy(x) = D ˜f (x).

Of course, despite the appeal of having a single mathe-
matical framework for describing processing nodes, there
is no reason to do this in practice since imperative and
declarative nodes can co-exist in a network.

Composability. One of the key design principles in
deep learning is that models should deﬁne their output
as the composition of many simple functions (such as
y = ˜f (˜g(x))). Just like conventional neural networks deep
declarative networks can also be composed of many op-
timization problems arranged in levels as the following
example shows:

y ∈ arg minu f (z, u)

(28)

subject to z ∈ arg minv g(x, v).
Here the gradients combine by the chain rule of differ-

entiation as one might expect:

Dz(x) = −H −1
Dy(z) = −H −1
Dy(x) = Dy(z)Dz(x)

g Bg(x, z)
f Bf (z, y)

(29)

where Hg = D2
ZZg(x, z), etc. Here, during back propagation
in computing DJ(x) from DJ(y) for training objective J , it
may be more efﬁcient to ﬁrst compute DJ(z) as

DJ(x) = (DJ(y)Dy(z))

Dz(x),

(30)

DJ(z)

which we will discuss further in Section 6.1.

{z

}

|

4.7 Extensions and discussion

4.7.1 Feasibility problems
Feasibility problems, where we simply seek a solution y(x)
that satisﬁes a set of constraints (themselves functions of x),
can be formulated as optimization problems [10], and hence
declarative nodes. Here the objective is constant (or inﬁnite
for an infeasible point) but the problem is more naturally
written as

u

ﬁnd
subject to hi(x, u) = 0, i = 1, . . . , p
gi(x, u) ≤ 0, i = 1, . . . , q.

(31)

The gradient can be derived by removing inactive inequality
constraints and maintaining the invariant ˜h(x, y(x)) = 0
where ˜h = [h1, . . . , hp, g1, . . . , gq], i.e., following the con-
straint surface. Implicit differentiation gives

ADy(x) + C = 0
(32)
where A = DY ˜h(x, y) and C = DX ˜h(x, y) as deﬁned in
Proposition 4.6. This is the same set of linear equations as
the second row of the Lagrangian in our proof of Proposi-
tion 4.5, and has a unique solution whenever rank(A) = m.3

4.7.2 Non-smooth objective and constraints

Propositions 4.5 and 4.6 showed how to compute deriva-
tives of the solution to parametrized equality constrained
optimization problems with second-order differentiable ob-
jective and constraint functions (in the neighborhood of
the solution). However, we can also deﬁne declarative
nodes where the objective and constraints are non-smooth
(that is, non-differentiable at some points). To enable back-
propagation learning with such declarative nodes all we

3. In Proposition 4.5 we assumed that rank(A) = p ≤ m. Typically
we have p < m and so ADy(x) = −C is under-determined, and the
curvature of the objective resolves Dy(x).

PREPRINT

9

require is a local descent direction (or Clarke generalized
gradient [13]). This is akin to the approach taken in standard
deep learning models with non-smooth activation functions
such as the rectiﬁed linear unit, y = max{x, 0}, which is
non-differentiable at x = 0.4 We explore this in practice
when we consider robust pooling and projecting onto the
boundary of Lp-balls in our illustrative examples and ex-
periments below.

4.7.3 Non-regular solutions
The existence of the Lagrange multipliers λ (and their
uniqueness) in our results above was guaranteed by our
assumption that y(x) is a regular point. Moreover, the fact
that A = DY h is full rank (and H non-singular) ensures
that Dy(x) can be computed. Under other constraint qual-
iﬁcations, such as Slater’s condition [10], the λ are also
guaranteed to exist but may not be unique. Moreover, we
may have rank(A) 6= p + q. If p + q > m but A is full
rank, which can happen if multiple constraints are active
at y(x), then we can directly solve the overdetermined
system of equations ADy(x) = −C. On the other hand, if
A is rank deﬁcient then we have redundant constraints and
must reduce A to a full rank matrix by removing linearly
dependent rows (and ﬁxing the corresponding λi to zero) or
resort to using a pseudo-inverse to approximate Dy(x). This
can happen if two or more (nonlinear inequality) constraints
are active at y(x) and tangent to each other at that point.

4.7.4 Non-unique solutions

In the development of deep declarative nodes we have
made no assumption about the uniqueness of the solution to
the optimization problem (i.e., output of the node). Indeed,
many solutions may exist and our gradients are still valid.
Needless to say, it is essential to use the same value of y
in the backward pass as was computed during the forward
pass (which is automatically handled in modern deep learn-
ing frameworks by caching the forward computations). But
there are other considerations too.

Consider for now the unconstrained case (Equation 11).
If D2
Y Y f (x, y) ≻ 0 then f is strongly convex in the neigh-
borhood of y. Hence y is an isolated minimizer (that is, a
unique minimizer within its neighborhood), and the gra-
dient derived in Proposition 4.4 holds for all such points.
Nevertheless, when performing parameter learning in a
deep declarative network it is important to be consistent
in solving optimization problems with multiple minima to
avoid oscillating between solutions as we demonstrate with
the following pathological example. Consider

will point in opposite directions, which is problematic for
end-to-end learning.

Singular Hessians. Now, if D2

Y Y f (x, y) is singular then
y may not be an isolated minimizer. This can occur, for
example, when y is an over-parametrized descriptor of some
physical property such as an unnormalized quaternion rep-
resentation of a 3D rotation. In such cases we cannot use
Proposition 4.4 to ﬁnd the gradient. In fact the gradient is
undeﬁned. There are three strategies we can consider for
such points. First, reformulate the problem to use a minimal
parametrization or introduce constraints to remove degrees
of freedom thereby making the solution unique. Second,
make the objective function strongly convex around the
solution, for example by adding the proximal term δ
2 ku−yk2
for some small δ > 0. Third, use a pseudo-inverse to solve
the linear equation D2
XY (x, y) for ∆y
and take ∆y as a descent direction. Supposing H † is a
pseudo-inverse of D2
Y Y f (x, y), we can compute an entire
family of solutions as

Y Y f (x, y)∆y = −D2

∆y ∈ {−H †B + (I − H †H)Z | Z ∈ Rm×n}

(35)

where H = D2

Y Y f (x, y) and B = D2

XY f (x, y).

As a toy example, motivated by the quaternion represen-
tation, consider the following problem that aligns the output
vector with an input vector x 6= 0 in R4,

y ∈ arg min

u∈R4

f (x, u) with f (x, u) , −xTu
kuk2

(36)

which has solution y = αx for arbitrary α > 0. Here we
Y Y f (x, y) = α−2kxk−1
have D2
, which is
2
singular (by the matrix inversion lemma [27]). Fixing one
degree of freedom resolves the problem, which in this case
is easily done by forcing the output vector to be normalized:

I − kxk−2

2 xxT

(cid:1)

(cid:0)

y ∈ arg minu∈R4 −xTu

subject to

kuk2 = 1.

(37)

Alternatively we can compute the Moore–Penrose pseudo-
inverse [36] of D2

Y Y f (x, y) to obtain

Dy = α

I −

(cid:18)

1
kxk2
2

T

xx

(cid:19)

(38)

which is the same gradient as the constrained case where
the solution is ﬁxed to have α = 1/kxk2 for this problem.

y ∈ arg minu∈R 0
subject to

(y − 1)2 − x2 = 0

(33)

5 ILLUSTRATIVE EXAMPLES

which has solution y = 1 ± x and, by inspection,

Dy =

−1,
+1,

(

for y = 1 − x
for y = 1 + x

(34)

Depending on the choice of optimal solution the gradient
of the loss being propagated backwards through the node

4. Incidentally, the (elementwise) rectiﬁed linear unit can be deﬁned

declaratively as y = arg minu∈Rn

+

1
2 ku − xk2
2.

In this section we provide examples of unconstrained and
constrained declarative nodes that illustrate the theory de-
veloped above. For each example we begin with a stan-
dard operation in deep learning models, which is usually
implemented as an imperative node. We show how the
operations can be equivalently implemented in a declarative
framework, and then generalize the operations to situations
where an explicit implementation is not possible, but where
the operation results in more desirable model behavior.

PREPRINT

5.1 Robust pooling

Average (or mean) pooling is a standard operation in deep
learning models where multi-dimensional feature maps are
averaged over one or more dimensions to produce a sum-
mary statistic of the input data. For the one-dimensional
case, x ∈ Rn 7→ y ∈ R, we have
n

y =

1
n

xi

i=1
X

(39)

(40)

(41)

and

Dy =

∂y
∂x1

(cid:20)

, . . . ,

∂y
∂xn (cid:21)

=

T

.

1

1
n

As a declarative node the mean pooling operator is

y ∈ arg min

u∈R

1
2

n

i=1
X

(u − xi)2

While the solution, and hence gradients, can be ex-
pressed in closed form, it is well-known that as a summary
statistic the mean is very sensitive to outliers. We can make
the statistic more robust by replacing the quadratic penalty
function φquad(z) = 1
2 z2 with one that is less sensitive to
outliers. The pooling operation can then be generalized as

y ∈ arg min

u∈R

n

φ(u − xi; α)

(42)

i=1
X
where φ is a penalty function taking scalar argument and
controlled by parameter α.

For example, the Huber penalty function deﬁned as

φhuber(z; α) =

(

1

2 z2
α(|z| − 1

for |z| ≤ α
2 α) otherwise.

(43)

is more robust to outliers. The Huber penalty is convex
and hence the pooled value can be computed efﬁciently
via Newton’s method or gradient descent [10]. However, no
closed-form solution exists. Moreover, the solution set may
be an interval of points. The pseudo-Huber penalty [29],

φpseudo(z; α) = α2

1 +

 r

z
α

(cid:16)

2

− 1

(cid:17)

!

(44)

has similar behaviour to the Huber but is strongly convex
so the optimal solution (pooled value) is unique.

The Welsch penalty function [17] is even more robust to

outliers ﬂattening out to a ﬁxed cost as |z| → ∞,

φwelsch(z; α) = 1 − exp

−

(cid:18)

z2
2α2

(cid:19)

(45)

However it is non-convex and so obtaining the solution
y(x) is non-trivial. Nevertheless, given a solution we can use
Proposition 4.4 to compute a gradient for back-propagation
parameter learning.

Going one step further we can deﬁned the truncated

quadratic penalty function,

φtrunc(z; α) =

1
2 z2
for |z| ≤ α
1
2 α2 otherwise.

(

(46)

In addition to being non-convex, the function is also non-
smooth. The solution amounts to ﬁnding the maximal

10

set of values all within some ﬁxed distance α from the
n
i=1 φtrunc(u − xi; α) is not
mean. The objective f (x, u) =
differentiable at points where there exists an i such that
|y(x) − xi| = α, in the same way that the rectiﬁed linear
unit is non-differentiable at zero. Nevertheless, we can still
compute a gradient almost everywhere (and take a one-
sided gradient at non-differentiable points).

P

The various penalty functions are depicted in Table 2.
When used to deﬁne a robust pooling operation there is no
closed-form solution for all but the quadratic penalty. Yet the
gradient of the solution with respect to the input data for all
robust penalties can be calculated using Proposition 4.4 as
summarized in the table.

5.2 Lp-sphere or Lp-ball projection
Euclidean projection onto an L2-sphere, equivalent to L2
normalization, is another standard operation in deep learn-
ing models. For x ∈ Rn 7→ y ∈ Rn, we have

y =

1
kxk2

x

and

Dy(x) =

1
kxk2 (cid:18)

I −

1
kxk2
2

T

xx

.

(cid:19)

(47)

(48)

As a declarative node the L2-sphere projection operator is

y ∈ arg minu∈Rn
subject to

1
2 ku − xk2
2
kuk2 = 1.

(49)

While the solution, and hence gradients, can be ex-
pressed in closed-form, it may be desirable to use other
Lp-spheres or balls, for regularization, sparsiﬁcation, or to
improve generalization performance [38], as well as for
adversarial robustness. The projection operation onto an Lp-
sphere can then be generalized as

yp ∈ arg minu∈Rn
subject to

1
2 ku − xk2
2
kukp = 1

(50)

where k · kp is the Lp-norm.

For example, projecting onto the L1-sphere has no
closed-form solution, however Duchi et al. [22] provide
an efﬁcient O(n) solver. Similarly, projecting onto the L∞-
sphere has an efﬁcient (trivial) solver. Given a solution from
one of these solvers, we can use Proposition 4.5 to compute a
gradient. For both cases, the constraint functions h are non-
smooth and so are not differentiable whenever the optimal
projection y lies on an (n − k)-face for 2 ≤ k ≤ n. For
example, when y lies on a vertex, changes in x may not have
any effect on y. While the gradient obtained from Propo-
sition 4.5 provides a valid local descent direction (Clarke
generalized gradient [13]), it can be modiﬁed to prefer a
zero gradient at these plateau dimensions by masking the
gradient, and thereby becoming identical to the gradient
obtained by numerical differentiation methods.

The various constraint functions and gradients are given
in Table 1. When used to deﬁne a projection operation, only
the L2 case has a closed-form solution. Nonetheless, the
gradient of the solution with respect to the input data for all
constraint functions can be calculated using Proposition 4.5
as summarized in the table.

PREPRINT

11

L2

L1

L∞

h(u)

kuk2 − 1 =

n
i=1 u2

i − 1

kuk1 − 1 =

n
i=1|ui| − 1

kuk∞ − 1 = max

{|ui|} − 1

i

qP

x

y2

closed-form, smooth,
unique† solution

y

I − yyT

1 − kxk2

1
kxk2

1
kxk2

I − yyT

(cid:16)

I − yyT

(cid:16)

(cid:17)

(cid:17)

DY h(y)

D2

Y Y h(y)

λ

Dy(x)

Dzy(x)

P

y1

x

x

y∞

non-smooth (C0),
isolated solutions

vec {sign (yi)}T

non-smooth (C0),
isolated solutions

vec {[[i ∈ I ⋆]] sign (yi)}T
I ⋆ = {i | |yi| ≥ |yj| ∀j}

0n×n

0n×n

sign (yi) (yi − xi) ∀i

[[i ∈ I ⋆]] sign (yi) (yi − xi) ∀i ∈ I ⋆

I −

DY h(y)T DY h(y)
DY h(y)DY h(y)T

diag (|DY h(y)|) −

DY h(y)T DY h(y)
DY h(y)DY h(y)T

I −

DY h(y)T DY h(y)
DY h(y)DY h(y)T

I − diag (|DY h(y)|)

TABLE 1: The gradient of the Euclidean projection onto various Lp-spheres with constraint functions h, when it exists. The
projection is found as y(x) = arg minu∈Rn f (x, u) subject to h(u) = 0, with f (x, u) = 1
XY f (x, y) = −I,
Y Y f (x, y) = I. The plots show the Euclidean projection of an example point x ∈ R2 onto Lp-spheres for p = 1, 2 and ∞.
and D2
Despite not being able to solve the problem in closed form for L1 and L∞ constraints, given a solution the gradient Dy(x) can
still be calculated. While Dy(x) provides a valid local descent direction in all cases, the gradient can be altered to prefer a zero
gradient along plateau dimensions by zeroing the rows and columns corresponding to the zero (L1) or non-zero (L∞) elements
of DY h(y). This gradient Dzy(x) is obtained by masking Dy(x) with ppT for L1 or ¯p¯pT for L∞, where p = |DY h(y)| is treated as
a Boolean vector. †Except for x = 0 where the solution is non-isolated (the entire sphere). For L1 and L∞ the solution is unique
when x is outside the ball but can be non-unique for x inside the ball.

2. In all cases, B = D2

2 ku−xk2

φ(z; α)

D2

Y Y f (x, y)

D2

XY f (x, y)

Dy(x)

QUADRATIC

PSEUDO-HUBER

HUBER

1
2 z2

φ

f

z

α2

(cid:18)q

1 +

z
α

(cid:1)

(cid:0)
φ

2 − 1

(cid:19)

z

f

z2

1
2
α(|z| − 1
2

(

for |z| ≤ α
α) otherwise.

φ

f

z

WELSCH

1 − exp

− z2
2α2

(cid:17)

z

(cid:16)
φ

f

TRUNC. QUAD.

1
2
1
2

(

z2
for |z| ≤ α
α2 otherwise.

φ

f

z

u

u

u

u

u

closed-form, convex,
smooth, unique solution

convex, smooth,
unique solution

convex, non-smooth (C1),
non-isolated solutions

non-convex, smooth,
isolated solutions

non-convex, non-smooth (C0),
isolated solutions

n

−1T
n

1

n 1T

n

n
i=1

1 +

(cid:18)

(cid:16)

y−xi
α

P

vec

−

1 +

(

(cid:18)

(cid:16)

y−xi
α

−3/2

2

(cid:19)

−3/2

T

(cid:17)

2

)

(cid:19)

(cid:17)

T

n
i=1 [[|y − xi| ≤ α]]

n
i=1

α2−(y−xi)2
α4

exp

− (y−xi)2
2α2

n
i=1 [[|y − xi| ≤ α]]

P

vec {−[[|y − xi| ≤ α]]}

P

vec

n

(cid:16)

(y−xi)2−α2
α4

exp

− (y−xi)2
2α2

(cid:17)

T

(cid:17)o

P

vec {−[[|y − xi| ≤ α]]}T

vec

wi
j=1 wj

Pn

(cid:26)
wi = α2−(y−xi)2

α4

(cid:16)

T

(cid:27)

where

exp

− (y−xi)2
2α2

(cid:16)

(cid:17)

[[|y−xi|≤α]]
j=1 [[|y−xj |≤α]]

Pn

vec

(cid:26)

T

(cid:27)

wi
j=1 wj

Pn

vec

(cid:26)

(cid:27)

wi =

1 +

(cid:18)

(cid:16)

y−xi
α

where

−3/2

2

(cid:19)

(cid:17)

[[|y−xi|≤α]]
j=1 [[|y−xj |≤α]]

Pn

vec

(cid:26)

T

(cid:27)

TABLE 2: The gradient of the estimate for a robust mean over a vector of values for various penalty functions φ(z; α) when it exists. The robust estimate is found as y(x) =
i=1 φ(u − xi; α). The ﬁrst plot (row 2) shows the penalty function, the second plot (row 3) shows an example f (x, u) for x = (−2α, 2α) ∈ R2.
arg minu∈R f (x, u) with f (x, u) = Pn
Plots have been drawn at different scales to enhance visualization of shape differences between the penalty functions. Despite not being able to solve the problem in closed form
for most penalty functions, given a solution the gradient Dy(x) can still be calculated. Notice the similarity in gradient forms due to the objective f being composed of a sum of
independent penalties, each penalty symmetric in x and y. Moreover, the Huber and truncated quadratic have exactly the same gradient form (when it exists) even though their
solutions may be different. Under some conditions Huber and Welsch may result in a zero D2
Y Y f and, hence, an undeﬁned gradient. However, we did not see this in practice.
More importantly, computation of the Welsch gradient is sensitive to numerical underﬂow and care should be taken to appropriately scale the wi’s before dividing by their sum.

P
R
E
P
R
N
T

I

1
2

PREPRINT

13

We can also consider a declarative node that projects

onto the unit Lp-ball with output deﬁned as
1
2 ku − xk2
2
kukp ≤ 1

y◦
p ∈ arg minu∈Rn
subject to

(51)

where we now have an inequality constrained convex op-
timization problem (for p ≥ 1). Here we take the gradient
Dy◦
p to be zero if kxkp < 1 and Dyp otherwise. In words,
we set the gradient to zero if the input already lies inside
the unit ball. Otherwise we use the gradient obtained from
projecting onto the Lp-sphere.

forward and backward pass. However, many other interest-
ing nodes will require specialized solvers for their forward
functions (e.g.,
the coordinate-based SDP solver proposed
by Wang et al. [47]). Even so, the gradient calculation in
the backward pass can be implemented using generic auto-
matic differentiation techniques whenever the objective and
constraint functions are twice continuously differentiable
(in the neighborhood of the solution). For example, the
following Python code makes use of the autograd pack-
age to compute the gradient of an arbitrary unconstrained
deep declarative node with twice differentiable, ﬁrst-order
objective f at minimum y given input x.

6 IMPLEMENTATION CONSIDERATIONS

In this section we provide some practical considerations
relating to the implementation of deep declarative nodes.

6.1 Vector–Jacobian product

import autograd . numpy as np

1
2 from autograd import grad ,
3
4 def g r a d i e n t ( f , x , y ) :
fY = grad ( f , 1 )
5
fYY = j a c o b i a n ( fY , 1 )
fXY = j a c o b i a n ( fY , 0 )

6

7

Python Code

j a c o b i a n

As discussed above, the key challenge for declarative nodes
is in computing Dy(x) for which we have provided linear
algebraic expressions in terms of ﬁrst- and second-order
derivatives of the objective and constraint functions. In
computing the gradient of the loss function, J , for the end-
to-end model we compute

8

9

DJ(x) = DJ(y)Dy(x)

(52)

and the order in which we evaluate this expression can have
a dramatic effect on computational efﬁciency.

For simplicity consider the unconstrained case (Proposi-

tion 4.4) and let vT = DJ(y) ∈ R1×m. We have

DJ(x) = −v

T

H −1B

(53)

where H ∈ Rm×m and B ∈ Rm×n. The expression can be
evaluated in two distinct ways—either as (vTH −1)B or as
vT(H −1B) where parentheses have been used to highlight
calculation order. Assuming that H −1 has been factored
(and contains no special structure), the cost of evaluating
DJ(x) ∈ R1×n is O(m2 + mn) for the former and O(m2n)
for the latter, and thus there is a computational advantage
to evaluating Equation 53 from left to right.

Moreover, it is often the case that n ≫ m in deep
learning models, such as for robust pooling. Here, rather
than pre-computing B, which would require O(mn) bytes
of precious GPU memory to store, it is much more space
efﬁcient to compute the elements of DJ(x) iteratively as

T
DJ(x)i = ˜v

bi

(54)

where ˜v = −H −1v is cached and bi is the i-th column of
B computed on the ﬂy and therefore only requiring O(m)
bytes.

6.2 Automatic differentiation

The forward processing function for some deep declarative
nodes (such as those deﬁned by convex optimization prob-
lems) can be implemented using generic solvers. Indeed,
Agrawal et al. [1] provide a general framework for speci-
fying (convex) deep declarative nodes through disciplined
convex optimization and providing methods for both the

r et u r n −1.0 ∗ np . l i n a l g . s o l v e ( fYY ( x , y ) ,

fXY ( x , y ) )

If called repeatedly (such as during learning) the par-
tial derivative functions fY, fYY and fXY can be pre-
processed and cached. And of course the gradient can
instead be manually coded for special cases and when
the programmer chooses to introduce memory and speed
efﬁciencies. The implementation for equality and inequal-
ity constrained problems follows in a straightforward way
and is omitted here for brevity. We provide full Python
and PyTorch reference implementations and examples at
http://deepdeclarativenetworks.com.

7 EXPERIMENTS

We conduct experiments on standard image and point
cloud classiﬁcation tasks to assess the viability of apply-
ing declarative networks to challenging computer vision
problems. Our goal is not to obtain state-of-the-art results.
Rather we aim to validate the theory presented above and
demonstrate how declarative nodes can easily be integrated
into non-trivial deep learning pipelines. To this end, we
implement the pooling and projection operations from Sec-
tion 5. For efﬁciency, we use the symbolic derivatives ob-
tained in Tables 2 and 1, and never compute the Jacobian
directly,
instead computing the vector–Jacobian product
to reduce the memory overhead. All code is available at
http://deepdeclarativenetworks.com.

For the point cloud classiﬁcation experiments with ro-
bust pooling, we use the ModelNet40 CAD dataset [48],
with 2048 points sampled per object, normalized into a unit
ball [40]. Each model is trained using stochastic gradient
descent for 60 epochs with an initial learning rate of 0.01,
decaying by a factor of 2 every 20 epochs, momentum (0.9),
and a batch size of 24. All models, variations of PointNet [40]
implemented in PyTorch, have 0.8M parameters. Impor-
tantly, pooling layers do not add additional parameters.

The results are shown in Tables 3 and 4, for a varying
fraction of outliers seen during training and testing (Table 3)
or only during testing, that is trained without outliers (Ta-
ble 4). We report top-1 accuracy and mean Average Precision

PREPRINT

(mAP) on the test set. Outlier points are sampled uniformly
from the unit ball, randomly replacing existing inlier points,
following the same protocol as Qi et al. [40]. The robust
pooling layer replaces the max pooling layer in the model,
with the quadratic penalty function being denoted by Q,
pseudo-Huber by PH, Huber by H, Welsch by W, and
truncated quadratic by TQ. The optimizer for the last two
(non-convex) functions is initialized to both the mean and
the median, and then the lowest local minimum selected
as the solution. It is very likely that RANSAC [26] search
would produce better results, but this was not tested. For all
experiments, the robustness parameter α (loosely, the inlier
threshold) is set to one.

We observe that the robust pooling layers perform sig-
niﬁcantly better than max pooling and quadratic (average)
pooling when outliers are present, especially when not
trained with the same outlier rate. There is a clear trend
that, as the outlier rate increases, the most accurate model
tends to be more robust model (further towards the right).

For

the image classiﬁcation experiments with Lp-
sphere and Lp-ball projection, we use the ImageNet 2012
dataset [16], with the standard single central 224 × 224 crop
protocol. Each model is trained using stochastic gradient
descent for 90 epochs with an initial learning rate of 0.1,
decaying by a factor of 10 every 30 epochs, weight decay
(1e-4), momentum (0.9), and a batch size of 256. All mod-
els, variations of ResNet-18 [30] implemented in PyTorch,
have 11.7M parameters. As with the robust pooling layers,
projection layers do not add additional parameters.

The results are shown in Table 5, with top-1 accuracy,
top-5 accuracy, and mean Average Precision (mAP) reported
on the validation set. The projection layer, prepended by
a batch normalization layer with no learnable parameters,
is inserted before the ﬁnal fully-connected output layer of
the network. The features are pre-scaled according to the
formula 2
3 mediani kfikp where fi are the batch-normalized
training set features of the penultimate layer of the pre-
trained ResNet model. This corresponds to scaling factors
of 250, 15, and 3 for p = 1, 2, and ∞ respectively, and can be
thought of as varying the radius of the Lp-ball. The chosen
scale ensures that the projection affects most features, but is
not too aggressive.

The results indicate that feature projection improves the
mAP signiﬁcantly, with a more modest increase in top-1 and
top-5 accuracy. This suggests that projection encourages a
more appropriate level of conﬁdence about the predictions,
improving the calibration of the model.

8 CONCLUSION

In the preceding sections we have presented some theory
and practice for deep declarative networks. On the one
hand we developed nothing new—argmin can simply be
viewed as yet another function and all that is needed is
to work out the technical details of how to compute its
derivative. On the other hand deep declarative networks
offers a new way of thinking about network design with
expressive processing functions and constraints but without
having to worry about implementation details (or having to
back-propagate gradients through complicated algorithmic
procedures).

14

TABLE 3: The effect of robust pooling layers on point cloud
classiﬁcation results for the ModelNet40 dataset [48], with
varying percentages of outliers (O) and the same rate of outliers
seen during training and testing. Outliers points are uniformly
sampled from the unit ball. PointNet [40] is compared to
our variants that replace max pooling with robust pooling:
quadratic (Q), pseudo-Huber (PH), Huber (H), Welsch (W),
and truncated quadratic (TQ), all trained from scratch. Top-1
accuracy and mean average precision are reported.

Top-1 Accuracy %

Mean Average Precision ×100
O
% [40] Q PH H W TQ [40] Q PH H W TQ
0 88.4 84.7 84.7 86.3 86.1 85.4 95.6 93.8 95.0 95.4 95.0 93.8
10 79.4 84.3 85.6 85.5 86.6 85.5 89.4 94.3 94.6 95.1 94.6 94.7
20 76.2 84.8 84.8 85.2 86.3 85.5 87.8 94.8 95.0 95.0 94.8 95.0
50 72.0 84.0 83.1 83.9 84.3 83.9 83.3 93.8 93.5 94.3 94.8 94.8
90 29.7 61.7 63.4 63.1 65.3 61.8 38.9 76.8 78.7 78.5 79.1 76.6

TABLE 4: The effect of robust pooling layers on point cloud
classiﬁcation results for the ModelNet40 dataset [48], with
varying percentages of outliers (O) and no outliers seen during
training. During testing, outlier points are uniformly sampled
from the unit ball. Models are identical to those in Table 3. Top-1
accuracy and mean average precision are reported.

Top-1 Accuracy %

Mean Average Precision ×100
O
% [40] Q PH H W TQ [40] Q PH H W TQ
0 88.4 84.7 84.7 86.3 86.1 85.4 95.6 93.8 95.0 95.4 95.0 93.8
1 32.6 84.9 84.7 86.4 86.2 85.3 48.6 93.8 95.1 95.3 95.1 93.0
10 6.47 83.9 84.6 85.3 86.0 85.9 8.20 93.4 94.8 94.4 94.9 93.9
20 5.95 79.6 82.8 81.1 84.7 84.9 7.73 91.9 93.4 92.7 94.2 94.6
30 5.55 70.9 74.2 72.2 77.6 83.2 6.00 87.8 89.5 85.1 90.9 92.8
40 5.35 55.3 59.1 55.4 63.1 75.6 6.41 77.6 80.2 72.7 83.2 90.6
50 4.86 32.9 36.0 34.6 44.1 57.9 5.68 62.3 60.2 60.1 66.4 85.3
60 4.42 14.5 16.2 18.1 27.1 30.6 5.08 39.1 36.3 38.5 42.7 68.5
70 4.25 5.03 6.33 7.95 14.1 11.9 4.66 22.5 19.3 18.4 25.7 47.9
80 3.11 4.10 4.51 5.64 8.88 5.11 4.21 10.8 8.91 8.98 14.9 26.7
90 3.72 4.06 4.06 4.30 5.68 4.22 4.49 8.20 5.98 5.80 8.37 9.78

TABLE 5: The effect of projection layers on image classiﬁcation
results for the ImageNet 2012 dataset [16]. Top-1 and top-5
accuracy, and mean Average Precision (mAP) are reported. All
models are trained from scratch, with the exception of ResNet-
18-pt, which uses the PyTorch pre-trained weights.

Model

Top-1 Acc. % Top-5 Acc. %

ResNet-18
ResNet-18-pt
ResNet-18-L1Sphere
ResNet-18-L2Sphere
ResNet-18-L∞Sphere
ResNet-18-L1Ball
ResNet-18-L2Ball
ResNet-18-L∞Ball

69.80
69.76
69.92
70.66
70.03
70.17
70.59
70.06

89.26
89.08
89.38
89.60
89.22
89.47
89.70
89.29

mAP ×100
58.97
53.76
62.72
71.97
63.98
61.26
72.43
63.33

By facilitating the inclusion of declarative nodes into
end-to-end learnable models, network capacity can be di-
rected towards identifying features and patterns in the data
rather than having to re-learn an approximation for theory
that is already well-established (for example, physical sys-
tem models) or enforce constraints that we know must hold
(for example, that the output must lie on a manifold). As
such, with deep declarative networks, we have the potential

PREPRINT

15

to create more robust models that can generalize better
from less training data (and with fewer parameters) or,
indeed, approach problems that could not be tackled by
deep learning previously as demonstrated in recent works
such as Amos and Kolter [4] and Wang et al. [47].

As with any new approach there are some shortcomings
and much still to do. For example, it is possible for problems
with parametrized constraints to become infeasible during
learning, so care should be taken to avoid parameterizations
that may result in an empty feasible set. More generally,
optimization problems can be expensive to solve and those
with multiple solutions can create difﬁculties for end-to-end
learning. Also more theory needs to be developed around
non-smooth objective functions. We have presented some
techniques for dealing with problems with inequality con-
straints (when we may only have a single-sided gradient) or
when the KKT optimality conditions cannot be guaranteed
but there are many other avenues to explore. Along these
lines it would also be interesting to consider the trade-off in
ﬁnding general descent directions rather than exact gradi-
ents and efﬁcient approximations to the forward processing
function, especially in the early stages of learning.

The work of Chen et al. [11] that shows how to differen-
tiate through an ordinary differential equation also presents
interesting directions for future work. Viewing such models
as declarative nodes with differential constraints suggests
many extensions including coupling ordinary differential
equations with constrained optimization problems, which
may be useful for physical simulations or modeling dynam-
ical systems.

Finally, the fact that deep declarative nodes provide
some guarantees on their output suggests that a principled
approach to analyzing the end-to-end behavior of a deep
declarative network might be possible. This would be es-
pecially useful in deploying deep learned models in safety
critical applications such as autonomous driving or complex
control systems. And while there are many challenges yet to
overcome, the hope is that deep declarative networks will
deliver solutions to problems faced by existing deep neural
networks leading to better robustness and interpretability.

ACKNOWLEDGMENTS

We give warm thanks to Bob Williamson and John Lloyd for
helpful discussions, and anonymous reviewers for insight-
ful suggestions. We thank Itzik Ben-Shabat for recommend-
ing experiments on point cloud classiﬁcation.

REFERENCES

[1] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond,
and Z. Kolter. Differentiable convex optimization lay-
ers. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.

[2] A. Agrawal, S. Barratt, S. Boyd, E. Busseti, and W. M.
Moursi. Differentiating through a cone program. Tech-
nical report, Stanford University (arXiv:1904.09043),
April 2019.

[4] B. Amos and J. Z. Kolter. OptNet: Differentiable op-
In Proc. of
timization as a layer in neural networks.
the International Conference on Machine Learning (ICML),
2017.

[5] B. Amos, I. D. J. Rodriguez, J. Sacks, B. Boots, and J. Z.
Kolter. Differentiable MPC for end-to-end planning
and control. In Advances in Neural Information Processing
Systems (NeurIPS), 2018.

[6] S. Bai, Z. Kolter, and V. Koltun. Deep equilibrium
In Advances in Neural Information Processing

models.
Systems (NeurIPS), 2019.
J. F. Bard. Practical Bilevel Optimization: Algorithms and
Applications. Kluwer Academic Press, 1998.

[7]

[8] D. P. Bertsekas. Constrained Optimization and Lagrange

Multiplier Methods. Academic Press, 1982.

[9] D. P. Bertsekas. Nonlinear Programming. Athena Scien-

tiﬁc, 2004.

[10] S. P. Boyd and L. Vandenberghe. Convex Optimization.

Cambridge, 2004.

[11] T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K.
Duvenaud. Neural ordinary differential equations.
In Advances in Neural Information Processing Systems
(NeurIPS), 2018.

[12] A. Cherian, B. Fernando, M. Harandi, and S. Gould.
In
Generalized rank pooling for action recognition.
Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[13] F. H. Clarke. Generalized gradients and applications.
Transactions of the American Mathematical Society, 205:
247–262, 1975.

[14] F. de Avila Belbute-Peres, K. Smith, K. Allen, J. Tenen-
baum, and J. Z. Kolter.
End-to-end differentiable
physics for learning and control. In Advances in Neural
Information Processing Systems (NeurIPS). 2018.

[15] S. Dempe and S. Franke. On the solution of convex
bilevel optimization problems. Computational Optimiza-
tion and Applications, pages 1–19, 2015.

[16] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. ImageNet: A large-scale hierarchical image
In Proc. of the IEEE Conference on Computer
database.
Vision and Pattern Recognition (CVPR), 2009.

[17] J. E. Dennis Jr and R. E. Welsch. Techniques for nonlin-
ear least squares and robust regression. Communications
in Statistics - Simulation and Computation, 7(4):345–359,
1978.

[18] J. Djolonga and A. Krause. Differentiable learning of
submodular models. In Advances in Neural Information
Processing Systems (NeurIPS), 2017.

[19] C. B. Do, C.-S. Foo, and A. Y. Ng. Efﬁcient mul-
tiple hyperparameter learning for log-linear models.
In Advances in Neural Information Processing Systems
(NeurIPS), 2007.

[20] J. Domke. Generic methods for optimization-based
In Proc. of the International Conference on

modeling.
Artiﬁcial Intelligence and Statistics (AISTATS), 2012.
[21] A. L. Dontchev and R. T. Rockafellar. Implicit Functions
and Solution Mappings: A View from Variational Analysis.
Springer-Verlag, 2nd edition, 2014.

[3] B. Amos. Differentiable Optimization-Based Modeling for
Machine Learning. PhD thesis, Carnegie Mellon Univer-
sity, 2019.

[22] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
Efﬁcient projections onto the l1-ball for learning in high
In Proc. of the International Conference on
dimensions.

PREPRINT

16

Machine Learning (ICML), pages 272–279, 2008.

and Pattern Recognition (CVPR), 2016.

[41] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
Learning representations by back-propagating errors.
Nature, 323:533–536, 1986.

[42] K. G. G. Samuel and M. F. Tappen. Learning optimized
MAP estimates in continuously-valued MRF models.
In Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2009.

[43] R. Santa Cruz, B. Fernando, A. Cherian, and S. Gould.
IEEE Trans. on Pattern

Visual permutation learning.
Analysis and Machine Intelligence (PAMI), 2018.

[44] G. M. Scarpello and D. Ritelli. Historical outline of the
theorem of implicit functions. Divulgaciones Matematica,
10:171–180, 2002.

[45] S. Tschiatschek, A. Sahin, and A. Krause. Differentiable
submodular maximization. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), 2018.

[46] H. von Stackelberg, D. Bazin, L. Urch, and R. R. Hill.

Market structure and equilibrium. Springer, 2011.

[47] P.-W. Wang, P. L. Donti, B. Wilder, and Z. Kolter.
SATNet: Bridging deep learning and logical reasoning
In Proc. of
using a differentiable satisﬁability solver.
the International Conference on Machine Learning (ICML),
2019.

[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang,
and J. Xiao. 3D ShapeNets: A deep representation for
In Proc. of the IEEE Conference on
volumetric shapes.
Computer Vision and Pattern Recognition (CVPR), pages
1912–1920, 2015.

[23] B. Fernando and S. Gould. Learning end-to-end video
classiﬁcation with rank-pooling. In Proc. of the Interna-
tional Conference on Machine Learning (ICML), 2016.
[24] B. Fernando and S. Gould. Discriminatively learned
In International
hierarchical rank pooling networks.
Journal of Computer Vision (IJCV), volume 124, pages
335–355, Sep 2017.

[25] B. Fernando, E. Gavves, J. M. Oramas, A. Ghodrati,
and T. Tuytelaars. Modeling video evolution for action
recognition. In Proc. of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2015.

[26] M. A. Fischler and R. C. Bolles. Random sample
consensus: A paradigm for model ﬁtting with appli-
cations to image analysis and automated cartography.
Commun. ACM, 24(6):381–395, 1981.

[27] G. H. Golub and C. F. van Loan. Matrix Computations.
Johns Hopkins University Press, 3rd edition, 1996.
[28] S. Gould, B. Fernando, A. Cherian, P. Anderson,
R. Santa Cruz, and E. Guo. On differentiating parame-
terized argmin and argmax problems with application
to bi-level optimization. Technical report, Australian
National University (arXiv:1607.05447), July 2016.
[29] R. I. Hartley and A. Zisserman. Multiple View Geometry
in Computer Vision. Cambridge University Press, 2004.
[30] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual
In Proc. of the IEEE
learning for image recognition.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.

[31] T. Klatzer and T. Pock. Continuous hyper-parameter
In Computer

learning for support vector machines.
Vision Winter Workshop (CVWW), 2015.

[32] S. G. Krantz and H. R. Parks. The Implicit Function
Springer,

Theorem: History, Theory, and Applications.
2013.

[33] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning.

Nature, 521(7553):436–444, 2015.

[34] K. Lee, S. Maji, A. Ravichandran, and S. Soatto. Meta-
learning with differentiable convex optimization.
In
Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019.

[35] P. M´arquez-Neila, M. Salzmann, and P. Fua.

Impos-
ing hard constraints on deep networks: Promises and
In CVPR Workshop on Negative Results in
limitations.
Computer Vision, 2017.

[36] C. D. Meyer. Generalized inversion of modiﬁed matri-
ces. SIAM Journal on Applied Math., 24(3):315–323, 1973.
[37] P. Ochs, R. Ranftl, T. Brox, and T. Pock. Bilevel op-
timization with nonsmooth lower level problems.
In
International Conference on Scale Space and Variational
Methods in Computer Vision (SSVM), 2015.

[38] S. Oymak. Learning compact neural networks with
In Proc. of the International Conference

regularization.
on Machine Learning (ICML), pages 3963–3972, 2018.
[39] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer. Automatic differentiation in PyTorch.
In
NeurIPS Autodiff Workshop, 2017.

[40] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3d classiﬁcation and segmen-
tation. In Proc. of the IEEE Conference on Computer Vision

