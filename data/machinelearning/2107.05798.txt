Cautious Policy Programming: Exploiting KL Regularization
for Monotonic Policy Improvement in Reinforcement Learning

Lingwei Zhu∗, Toshinori Kitamura, Takamitsu Matsubara

Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma, Nara, Japan

2
2
0
2

n
a
J

6
1

]

G
L
.
s
c
[

3
v
8
9
7
5
0
.
7
0
1
2
:
v
i
X
r
a

Abstract

In this paper, we propose cautious policy programming (CPP), a novel value-based reinforcement learning (RL) algorithm that
can ensure monotonic policy improvement during learning. Based on the nature of entropy-regularized RL, we derive a new
entropy-regularization-aware lower bound of policy improvement that depends on the expected policy advantage function but not
on state-action-space-wise maximization as in prior work. CPP leverages this lower bound as a criterion for adjusting the degree of
a policy update for alleviating policy oscillation. Diﬀerent from similar algorithms that are mostly theory-oriented, we also propose
a novel interpolation scheme that makes CPP better scale in high dimensional control problems. We demonstrate that the proposed
algorithm can trade oﬀ performance and stability in both didactic classic control problems and challenging high-dimensional Atari
games.

Keywords: Reinforcement Learning, Monotonic Improvement, Entropy Regularization

1. Introduction

Reinforcement learning (RL) has recently achieved impres-
sive successes in ﬁelds such as robotic manipulation [1] and
video game playing [2]. However, compared with supervised
learning that has a wide range of practical applications, RL ap-
plications have primarily been limited to game playing or lab
robotics. A crucial reason for such limitation is the lack of guar-
antee that the performance of RL policies will improve mono-
tonically; they often oscillate during policy updates. As such,
deploying such updated policies without examining their relia-
bility might bring severe consequences in real-world scenarios,
e.g., crashing a self-driving car.

Dynamic programming (DP) [3] oﬀers a well-studied frame-
work under which strict policy improvement is possible: with a
known state transition model, reward function, and exact com-
putation, monotonic improvement is ensured and convergence
is guaranteed within a ﬁnite number of iterations [4]. How-
ever, in practice an accurate model of the environment is rarely
available. In situations where either model knowledge is ab-
sent or the DP value functions cannot be explicitly computed,
approximate DP and corresponding RL methods are to be con-
sidered. However, approximation introduces unavoidable up-
date and Monte-Carlo sampling errors, and possibly restricts
the policy space in which the policy is updated, leading to the
policy oscillation phenomenon [5, 6], whereby the updated pol-
icy performs worse than pre-update policies during intermedi-
ate stages of learning. Inferior updated policies resulting from
policy oscillation could pose a physical threat to real-world

∗Corresponding author
Email address: lingwei.andrew.zhu@gmail.com (Lingwei Zhu)

RL applications. Further, as value-based methods are widely
employed in the state-of-the-art RL algorithms [7], addressing
the problem of policy oscillation becomes important in its own
right.

Previous studies [8, 9] attempt to address this issue by op-
timizing lower bounds of policy improvement: the classic con-
servative policy iteration (CPI) [8] algorithm states that, if the
new policy is linearly interpolated by the greedy policy and the
baseline policy, non-negative lower bound on the policy im-
provement can be deﬁned. Since this lower bound is a nega-
tive quadratic function in the interpolation coeﬃcient, one can
solve for the maximizing coeﬃcient to obtain maximum im-
provement at every update. CPI opened the door of mono-
tonic improvement algorithms and the concept of linear inter-
polation can be regarded as performing regularization in the
stochastic policy space to reduce greediness. Such regulariza-
tion is theoretically sound as it has been proved to converge to
global optimum [10, 11]. For the last two decades, CPI has
inspired many studies on ensuring monotonic policy improve-
ment. However, those studies (including CPI itself) are mostly
theory-oriented and hardly applicable to practical scenarios, in
that maximizing the lowerbound requires solving several state-
action-space-wise maximization problems, e.g. estimating the
maximum distance between two arbitrary policies. One sig-
niﬁcant factor causing the complexity might be its excessive
generality [8, 9]; these bounds do not focus on any particular
class of value-based RL algorithms, and hence without further
assumptions the problem cannot be simpliﬁed.

Another recent trend of developing algorithms robust to the
oscillation is by introducing regularizers into the reward func-
tion. For example, by maximizing reward as well as Shannon
entropy of policy [12], the optimal policy becomes a multi-

Preprint submitted to Neural Networks

January 19, 2022

 
 
 
 
 
 
modal Boltzmann softmax distribution which avoids putting unit
probability mass on the greedy but potentially sub-optimal ac-
tions corrupted by noise or error, signiﬁcantly enhancing the
robustness since optimal actions always have nonzero probabil-
ities of being chosen. On the other hand, the introduction of
Kullback-Leibler (KL) divergence [13] has recently been iden-
tiﬁed to yield policies that average over all past value functions
and errors, which enjoys state-of-the-art error dependency the-
oretically [14]. Though entropy-regularized algorithms have
superior ﬁnite-time bounds and enjoy strong empirical perfor-
mance, they do not guarantee to reduce policy oscillation since
degradation during learning can still persist [15].

It is hence natural to raise the question of whether the prac-
tically intractable lowerbounds from the monotonic improve-
ment literature can beneﬁt from entropy regularization if we
restrict ourselves to the entropy-regularized policy class. By
noticing that the policy interpolation and entropy regulariza-
tion actually perform regularization in diﬀerent aspects, i.e. in
the stochastic policy space and reward function, we answer
this question by aﬃrmative. We show focusing on the class of
entropy-regularizede policies signiﬁcantly simpliﬁes the prob-
lem as a very recent result indicates a sequence of entropy-
regularized policies has bounded KL divergence [16]. This re-
sult sheds light on approximating the intractable lowerbounds
from the monotonic improvement algorithms since many quan-
tities are related to the maximum distance between two arbitrary
policies.

In this paper, we aim to tackle the policy oscillation prob-
lem by ensuring monotonic improvement via optimizing a more
tractable lowerbound. This novel entropy regularization aware
lower bound of policy improvement depends only the expected
policy advantage function. We call the resultant algorithm cau-
tious policy programming (CPP). CPP leverages this lower bo-
und as a criterion for adjusting the degree of a policy update for
alleviating policy oscillation. By introducing heuristic designs
suitable for nonlinear approximators, CPP can be extended to
working with deep networks. The extensions are compared
with the state-of-the-art algorithm [17] on monotonic policy im-
provement. We demonstrate that our approach can trade oﬀ per-
formance and stability in both didactic classic control problems
and challenging Atari games.

The contribution of this paper can be succinctly summa-

rized as follows:

• we develop an easy-to-use lowerbound for ensuring mono-

tonic policy improvement in RL.

• we propose a novel scalable algorithm CPP which opti-

mizes the lowerbound.

• CPP is validated to reduce policy oscillation on high-
dimensional problems which are intractable for prior meth-
ods.

Here, the ﬁrst and second points are presented in Sec. 4, after a
brief review on related work in Sec. 2 and preliminary in Sec.
3. The third point is inspected in Sec. 5 which presents the
results. CPP has touched upon many related problems, and we

provide in-depth discussion in Sec. 6. The paper is concluded
in Sec. 7. To not interrupt the ﬂow of the paper, we defer all
proofs until the Appendix.

2. Related Work

The policy oscillation phenomenon, also termed overshoot-
ing by [6] and referred to as degraded performance of updated
policies, frequently arises in approximate policy iteration algo-
rithms [5] and can occur even under asymptotically converged
value functions [6]. It has been shown that aggressive updates
with sampling and update errors, together with restricted policy
spaces, are the main reasons for policy oscillation [9]. In mod-
ern applications of RL, policy oscillation becomes an important
issue when learning with deep networks when various sources
of errors have to been taken in to account. It has been investi-
gated by [18, 19] that those errors are the main cause for typical
oscillating performance with deep RL implementations.

To attenuate policy oscillation, the seminal algorithm con-
servative policy iteration (CPI) [8] propose to perform regu-
larization in the stochastic policy space, whereby the greed-
ily updated policy is interpolated with the current policy to
achieve less aggressive updates. CPI has inspired numerous
conservative algorithms that enjoy strong theoretical guaran-
tees [9, 20, 21, 22] to improve upon CPI by proposing new
lower bounds for policy improvement. However, since their
focus is on general Markov decision processes (MDPs), deriv-
ing practical algorithms based on the lower bounds is nontrivial
and the proposed lower bounds are mostly of theoretical value.
Indeed, as admitted by the authors of [23] that a large gap be-
tween theory and practice exists, as manifested by the their ex-
perimental results that even for a simple Cartpole environment,
state-of-the-art algorithm failed to deliver attenuated oscillation
and convergence speed comparable with heuristic optimization
scheme such as Adam [24]. This might explain why adaptive
coeﬃcients must be introduced in [17] to extend CPI to be com-
patible with deep neural networks. To remove this limitation,
our focus on entropy-regularized MDPs allows for a straightfor-
ward algorithm based on a novel, signiﬁcantly simpliﬁed lower
bound.

Another line of research toward alleviating policy oscilla-
tion is to incorporate regularization as a penalty into the reward
function, leading to the recently booming literature on entropy-
regularized MDPs [25, 26, 16, 27, 14, 28]. Instead of interpolat-
ing greedy policies, the reward is augmented with entropy of the
policy, such as Shannon entropy for more diverse behavior and
smooth optimization landscape [29], or Kullback-Leibler (KL)
divergence for enforcing policy similarity between policy up-
dates and hence achieving superior sample eﬃciency [30, 31].
The Shannon entropy renders the optimal policy of the regular-
ized MDP stochastic and multi-modal and hence robust against
errors and noises in contrast to the deterministic policy that
puts all probability mass on a single action [7]. On the other
hand, augmenting with KL divergence shapes the optimal pol-
icy an average of all past value functions, which is signiﬁcantly
more robust than a single point estimate. Compared to the CPI-
based algorithms, entropy-regularized algorithms do not have

2

guarantee on per-update improvement. But they have demon-
strated state-of-the-art empirical successes on a wide range of
challenging tasks [32, 33, 34, 35]. To the best of the authors’
knowledge, unifying those two regularization schemes has not
been considered in published literature before.

It is worth noting that, inspired by [8], the concept of mono-
tonic improvement has been exploited also in policy search sce-
narios [36, 37, 38, 39, 23]. However, there is a large gap be-
tween theory and practice in those policy gradient methods. On
one hand, though [36, 40] demonstrated good empirical per-
formance, their relaxed trust region is often too optimistic and
easily corrupted by noises and errors that arise frequently in
the deep RL setting: as pointed out by [41], the trust region
technique itself alone fails to explain the eﬃciency of the algo-
rithms and lots of code-level tricks are necessary. On the other
hand, exactly following the guidance of monotonic improving
gradient does not lead to tempered oscillation and better perfor-
mance even for simple problems [42, 23]. Another shortcoming
of policy gradient methods is they focus on local optimal pol-
icy with strong dependency on initial parameters. On the other
hand, we focus on value-based RL that searches for global op-
timal policies.

3. Preliminary

3.1. Reinforcement Learning

RL problems can be formulated by MDPs expressed by the
quintuple (S, A, T , R, γ), where S denotes the state space, A
denotes the ﬁnite action space, and T denotes transition dy-
:= T (s(cid:48)|s, a) represents the transition
namics such that T a
ss(cid:48)
from state s to s(cid:48) with action a taken. R = r a
ss(cid:48) is the im-
mediate reward associated with that transition. In this paper,
we consider r a
ss(cid:48) as bounded in the interval [−1, 1]. γ ∈ (0, 1)
is the discount factor. For simplicity, we consider the inﬁnite
horizon discounted setting with a ﬁxed starting state s0. A pol-
icy π is a probability distribution over actions given some state.
We also deﬁne the stationary state distribution induced by π as
dπ(s) = (1 − γ) (cid:80)∞

t=0 γtT (st = s|s0, π).

RL algorithms search for an optimal policy π∗ that maxi-

mizes the state value function for all states s:

π∗ := arg max

V π(s) = arg max

E

π

π


∞(cid:88)


t=0


(cid:12)(cid:12)(cid:12)s0 = s
 ,

γtrt

Lemma 1. [8] For any stationary policies π(cid:48) and π, the follow-
ing equation holds:

∆Jπ(cid:48)

π,dπ(cid:48) := Jπ(cid:48)

d − Jπ

d

(cid:88)

=

dπ(cid:48)

(s)

(cid:88)

s

a

π(cid:48)(a|s)Aπ(s, a),

where Jπ(cid:48)

d := Es0,a0,...

(cid:20)
(1 − γ)

∞(cid:88)

t=0

(cid:21)

=

(cid:88)

γtrt

dπ(cid:48)

(s)

(cid:88)

π(cid:48)(a|s)ra

ss(cid:48) ,

s

a

(1)

J is the discounted cumulative reward, and Aπ(s, a) :=
Qπ(s, a) − Vπ(s) is the advantage function. Though Lemma 1
relates policy improvement to the expected advantage function,
pursuing policy improvement by directly exploiting Lemma 1
is intractable as it requires comparing π(cid:48) and π point-wise for
inﬁnitely many new policies. Many existing works [8, 9, 36]
instead focus on ﬁnding a π(cid:48) such that the right-hand side of Eq.
(1) is lower bounded. To alleviate policy oscillation brought
by the greedily updated policy ˜π, [8] proposes adopting partial
update:

π(cid:48) = ζ ˜π + (1 − ζ)π.

(2)

Eq. (2) corresponds to performing regularization in the stochas-
tic policy space by interpolating the greedy policy and the cur-
rent policy to achieve conservative updates.

The concept of linearly interpolating policies has inspired
many algorithms that enjoy strong theoretical guarantees [9, 22,
37]. However, those algorithms are mostly of theoretical value
and have only been applied to small problems due to intractable
optimization or estimation when the state-action space is high-
dimensional/continuous. Indeed, as admitted by the authors of
[23], there is a large gap between theory and practice when us-
ing algorithms based on policy regularization Eq. (2): even on
a simple CartPole problem, a state-of-the-art algorithm fail to
compete with heuristic optimization technique. Like our pro-
posal in this paper, a very recent work [17] attempts to bridge
this gap by proposing heuristic coeﬃcient design for learning
with deep networks. We discuss the relationship between it and
the CPP in Section 4.4.

In the next section, we detail the derivation of the proposed
lower bound by exploiting entropy regularization. This novel
lower bound allows us to signiﬁcantly simplify the intractable
optimization and estimation in prior work and provide a scal-
able implementation.

where the expectation is with respect to the transition dynamics
T and policy π. The state-action value function Qπ is more
frequently used in the control context:

4. Proposed Method

Qπ∗

(s, a) = max

π

E


∞(cid:88)


t=0


(cid:12)(cid:12)(cid:12)s0 = s, a0 = a
 .

γtrt

This section features the proposed novel lower bound on
which we base a novel algorithm for ensuring monotonic policy
improvement.

3.2. Lower Bounds on Policy Improvement

To frame the monotonic improvement problem, we intro-
duce the following lemma that formally deﬁnes the criterion of
policy improvement of some policy π(cid:48) over π:

4.1. Entropy-regularized RL

In the following discussion, we provide a general formula-
tion for entropy-regularized algorithms [25, 7, 16]. At iteration
K, the entropy of current policy πK and the Kullback-Leibler

3

(KL) divergence between πK and some baseline policy ¯π are
added to the value function:
(cid:20)

(cid:21)

(cid:88)

π(a|s)

T a
ss(cid:48)

(cid:0)ra

ss(cid:48) + γV πK

¯π (s(cid:48))(cid:1) − IπK

¯π

,

V πK
¯π (s) :=

a∈A
s(cid:48)∈S

IπK
¯π

= −τ log πK(a|s) − σ log

πK(a|s)
¯π(a|s)

,

(3)

where τ controls the weight of the entropy bonus and σ weights
the eﬀect of KL regularization. The baseline policy ¯π is of-
ten taken as the policy from previous iteration πK−1. Based on
[43, 15], we know the state value function V πK
¯π deﬁned in Eq.
(3) and state-action value function QπK
¯π also satisfy the Bellman
recursion:

QπK

¯π (s, a) := ra

ss(cid:48) + γ

(cid:88)

s(cid:48)

T a

ss(cid:48) V πK

¯π (s(cid:48)).

For notational convenience, in the remainder of this paper, we
use the following deﬁnition:

α :=

τ
τ + σ

,

β := 1

τ + σ

.

(4)

An intuitive explanation to Eq. (3) is that the entropy term en-
dows the optimal policy with multi-modal policy behavior [27]
by placing nonzero probability mass on every action candidate,
hence is robust against error and noise in function approxima-
tion that can easily corrupt the conventional deterministic op-
timal policy [44]. On the other hand, KL divergence provides
smooth policy updates by limiting the size of the update step
[25, 16, 36]. Indeed, it has been recently shown that augment-
ing the reward with KL divergence renders the optimal policy
an exponential smoothing of all past value functions [14]. Lim-
iting the update step plays a crucial role in the recent successful
algorithms since it prevents the aggressive updates that could
easily be corrupted by errors [18, 19]. It is worth noting that
when the optimal policy is attained, the KL regularization term
becomes zero. Hence in Eq. (3), the optimal policy maximizes
the cumulative reward while keeping the entropy high.

4.2. Entropy-regularization-aware Lower Bound

Recall in Eq. (2) performing regularization in the stochastic
policy space for the greedily updated policy ˜π requires prepar-
ing an reference policy π. This policy could be from expert
knowledge or previous policies. The resultant π(cid:48), has guaran-
teed monotonic improvement which we formulate as the fol-
lowing lemma:

Lemma 2 ([9]). Provided that policy π(cid:48) is generated by partial
update Eq. (2), ζ is chosen properly, and A˜π
π,dπ ≥ 0, then the

following policy improvement is guaranteed:
(cid:1)2

(cid:0)(1 − γ)A˜π
π,dπ
2γδ∆A˜π
π

,

∆Jπ(cid:48)

π,dπ(cid:48) ≥
with ζ = min (1, ζ∗),
(1 − γ)2A˜π
γδ∆A˜π
π
(cid:88)

where ζ∗ =

δ = max

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

s

a∈A
= max
s,s(cid:48)

∆A˜π
π
s dπ(s) (cid:80)

π,dπ

,

(5)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:0)˜π(a|s) − π(a|s)(cid:1)

,

|A˜π

π(s) − A˜π

π(s(cid:48))|,

where A˜π

π,dπ := (cid:80)

a (˜π(a|s) − π(a|s)) Qπ(s, a).

Proof. See Section Appendix A.1.1 for the proof.

The interpolated policy π(cid:48) optimizes the bound and the pol-
icy improvement is a negative quadratic function in ζ. However,
this optimization problem is highly non-trivial as δ and ∆A˜π
π re-
quire searching the entire state-action space. This challenge
explains why CPI-inspired methods have only been applied to
small problems with low-dimensional state-action spaces [9,
22, 23].

When the expert knowledge is not available, we can sim-
ply choose previous policies. Speciﬁcally, at any iteration K,
we want to ensure monotonic policy improvement given policy
πK. We propose constructing a new monotonically improving
policy as:

˜πK+1 = ζπK+1 + (1 − ζ)πK.

(6)

It is now clear by comparing Eq. (2) with Eq. (6) that our pro-
posal takes π(cid:48), ˜π, π as ˜πK+1, πK+1, πK, respectively. It is worth
noting that πK+1 is the updated policy that has not been de-
ployed.

However, the intractable quantities δ and ∆A˜π

π in Lemma 2
are still an obstacle to deriving a scalable algorithm. Speciﬁ-
cally, by writing the component A˜π
π(s) =
A˜π

π as
(cid:0)˜π(a|s) − π(a|s)(cid:1)Qπ(s, a),

π(s) of ∆A˜π

(cid:88)

a

we see that both δ and ∆A˜π
π require accurately estimating the
total variation between two policies. This could be diﬃcult
without enforcing constraints such as gradual change of poli-
cies. Fortunately, by noticing that the consecutive entropy-
regularized policies πK+1, πK have bounded total variation, we
can leverage the boundedness to bypass the intracatable estima-
tion.

Lemma 3 ([16]). For any policies πK and πK+1 generated by
taking the maximizer of Eq. (3), the following bound holds for
their maximum total variation:

max
s

DT V (πK+1(·|s) || πK(·|s)) ≤

min
where BK = 1 − γK
1 − γ

(cid:110) √

1 − e−4BK −2CK ,

(cid:112)

8BK + 4CK

(cid:111)

,

(cid:15)β, CK = βrmax

K−1(cid:88)

k=0

αkγK−k−1,

(7)

4

K denotes the current iteration index and 0 ≤ k ≤ K − 1 is the
loop index. (cid:15) is the uniform upper bound of error.

Proof. See Section Appendix A.1.2 for the proof.

improvement of policy iteration when computation is exact. To
handle the negative case caused by error or approximate com-
putations, we can simply stack more samples to reduce the vari-
ance, as will be detailed in Sec. 4.5.

Lemma 3 states that, entropy-regularized policies have boun-
ded total variation (and hence bounded KL divergence by Pinsker’s
and Kozuno’s inequality [16]). This bound allows us to bypass
the intractable estimation in Lemma 2 and approximate ˜πK+1
that optimizes the lowerbound. We formally state this result in
the Theorem 4 below.

For convenience, we assume there is no error, i.e. BK = 0.
Setting BK = 0 is only for the ease of notation of our latter
derivation. Our results still hold by simply replacing all appear-
ance of CK to BK + CK. On the other hand, in implementa-
tion it requires a sensible choice of upper bound of error which
is typically diﬃcult especially for high dimensional problems
and with nonlinear function approximators. Fortunately, by the
virtue of KL regularization in Eq.
(3), it has been shown in
[25, 45] that if the sequence of errors is a martingale diﬀer-
ence under the natural ﬁltration, then the summation of errors
asymptotically cancels out. Hence it might be safe to simply set
BK = 0 if we assume the martingale diﬀerence condition.

Theorem 4. Provided that partial update Eq. (6) is adopted,
AπK+1
πK ,dπK ≥ 0, and ζ is chosen properly as speciﬁed below, then
any maximizer policy of Eq. (3) guarantees the following im-
provement that depends only on α, β, γ and AπK+1
πK ,dπK after any
policy update:

∆J ˜πK+1

πK ,d ˜πK+1

≥

(cid:0)1 − γ)3(AπK+1

πK ,dπK )2

4γ

(cid:40)

max

1
1 − e−2CK

,

1
4CK

(cid:41)

,

with ζ = min (1, ζ∗), CK = β

K−1(cid:88)

k=0

αkγK−k−1,

(8)

where ζ∗ =

(1 − γ)3AπK+1
2γ

πK ,dπK

(cid:40)

max

1
1 − e−2CK

,

1
4CK

(cid:41)

,

α, β are deﬁned in Eq. (4) and

(cid:88)

AπK+1
πK ,dπK :=

dπK (s) AπK+1

πK

(s),

AπK+1
πK

(s) :=

s
(cid:88)

a

(cid:0)πK+1(a|s) − πK(a|s)(cid:1)QπK (s, a)

(9)

(10)

are the expected policy advantage, and the policy advantage
function, respectively.

Proof. See Section Appendix A.1.3 for the proof.

While theoretically we need to compare 1 − e−2CK and 4CK
when computing ζ∗, in implementation the exponential function
e−2CK might be sometimes close to 1 and hence causing numer-
ical instability. Hence in the rest of the paper we shall stick to
using the constant CK rather than the exponential function.

In the lower bound Eq.

πK ,dπK needs to be es-
timated. It is worth noting that ∀s, AπK+1
(s) ≥ 0 is a straight-
πK
forward criterion that is naturally satisﬁed by the greedy policy

(8), only AπK+1

4.3. The CPP Policy Iteratiion

We now detail the structure of our proposed algorithm based
on Theorem 4. Speciﬁcally, value update, policy update, and
stationary distribution estimation are introduced, followed by
discussion on a subtlety in practice and two possible solutions.
Following [46], CPP can be written in the following suc-

cinct policy iteration style:



(cid:111)

(11)

CPP =

πK+1 ← GQπK
¯π
QπK+1 ← (TπK+1 )mQπK
(cid:110)
(4CK)−1CγAπK+1
ζ = min
˜πK+1 ← ζπK+1 + (1 − ζ)πK,



where Cγ := (1−γ)3
is the horizon constant. Note that for numer-
ical stability we stick to using (4CK)−1 as the entropy-bounding
1
1−e−2CK .
constant rather than using

πK ,dπK , 1

Like CPI, CPP can obtain global optimal policy rather than
just achieving monotonic improvement (which might still con-
verge to a local optimum) by the argument of [10]. The ﬁrst step
corresponds to the greedy step of policy iteration, the second
step policy estimation step, third step computing interpolation
coeﬃcient ζ and the last step interpolating the policy.

2γ

4.3.1. Policy Improvement and Policy Evaluation

The ﬁrst two steps are standard update and estimation steps
indi-

of policy iteration algorithms [47]. The subscript of QπK
¯π
cates it is entropy-regularized as introduced in Eq. (3).

The policy improvement step consists of evaluating GQπK
¯π ,
¯π . By the Fenchel
¯π has a

which is the greedy operator acting on QπK
conjugacy of Shannon entropy and KL divergence, GQπK
closed-form solution [16, 48]:

GQπK

¯π (a|s) =

(cid:80)

¯π(a|s)α exp

b ¯π(b|s)α exp

(cid:16)
(cid:17)
βQπK
¯π (s, a)
(cid:16)
βQπK

¯π (s, b)

(cid:17) ,

where α, β were deﬁned in Eq. (4).

The policy evaluation step estimates the value of current
policy πK+1 by repeatedly applying the Bellman operator TπK+1 :

(TπK+1 )mQπK := TπK+1 . . . TπK+1
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)

QπK ,

TπK+1 QπK := ra

ss(cid:48) + γ

(cid:88)

s(cid:48)

T a
ss(cid:48)

a

m times

(cid:88)

πK+1(a|s(cid:48))QπK

¯π (s(cid:48), a).

(12)

Note that m = 1, ∞ correspond to the value iteration and policy
iteration, respectively [49]. Other interger-valued m ∈ [2, ∞)
correspond to the approximate modiﬁed policy iteration [46].
πK ,dπK in Theorem 4, both AπK+1
πK
and dπK need to be estimated from samples. Estimating AπK+1
(s)
πK
is straightforward by its deﬁnition in Eq.
(10). We can ﬁrst
compute QπK (s, a) − VπK (s), ∀s, a for the current policy, and

Now in order to estimate AπK+1

5

then update the policy to obtain πK+1(a|s). On the other hand,
sampling with respect to dπK results in an on-policy algorithm,
which is expensive. We provide both on- and oﬀ-policy imple-
mentations of CPP in the following sections, but in principle
oﬀ-policy learning algorithms can be applied to estimate dπK
by exploiting techniques such as importance sampling (IS) ra-
tio [50].

4.3.2. Leveraging Policy Interpolation

Computing ζ in Eq. (8) involves the horizon constant Cγ :=
(1−γ)3
and policy diﬀerence bound constant CK. The horizon
2γ
constant is eﬀective in DP scenarios where the total number of
timesteps is typically small, but might not be suitable for learn-
ing with deep networks that feature large number of timesteps:
a vanishingly small Cγ will signiﬁcantly hinder learning, hence
it should be removed in deep RL implementations. We detail
this consideration in Section 4.5.

The updated policy πK+1 in Eq. (11) cannot be directly de-
ployed since it has not been veriﬁed to improve upon πK. We
interpolate between πK+1 and πK with coeﬃcient ζ such that
the resultant policy ˜πK+1 by ﬁnding the maximizer of a negative
quadratic function in ζ. The maximizer ζ∗ optimizes the lower-
bound ∆J ˜πK+1
πK ,d ˜πK+1 . Here, ζ is optimally tuned and dynamically
changing in every update. It reﬂects the cautiousness against
policy oscillation, i.e., how much we trust the updated policy
πK+1. Generally, at the early stage of learning, ζ should be small
in order to explore conservatively.

However, a major concern is that Lemma 3 holds only for
Boltzmann policies, while the interpolated policies are gener-
ally no longer Boltzmann. In practice, we have two options for
handling this problem:

1. we use the interpolated policy only for collecting samples
(i.e. behavior policy) but not for computing next policy;
2. we perform an additional projection step to project the
interpolated policy back to the Boltzmann class as the
next policy.

The ﬁrst solution might be suitable for relatively simple prob-
lems where the safe exploration is required: the behavior pol-
icy is conservative in exploring when ζ ≈ 0. But learning can
still proceed even with such small ζ. Hence this scheme suits
problems where interaction with the environment is crucial but
progress is desired. On the other hand, the second scheme is
more natural since the oﬀ-policyness caused by the mismatch
between the behavior and learning policy might be compounded
by high dimensionality. The increased mismatch might be per-
turbing to performance. In the following section, we introduce
CPP using linear function approximation for the ﬁrst scheme
and deep CPP for the second scheme.

For the second scheme, manipulating the interpolated pol-
icy is inconvenient since we will have to remember all previ-
ous weights and more importantly, the theoretical properties of
Boltzmann policies do not hold any longer. To solve this is-
sue, heuristically an information projection step is performed
for every interpolated policy to obtain a Boltzmann policy. In
practice, this policy is found by solving minπ DKL(π||ζ ¯πK+1 +

(1 − ζ)πK). Though the information projection step can only ap-
proximately guarantee that the CVI bound continues to apply
since the replay buﬀer capacity is ﬁnite, it has been commonly
used in practice [7, 17]. In our implementation of deep CPP,
the projection problem is solved eﬃciently using autodiﬀeren-
tiation (Line 7 of Algorithm 2).

4.4. Approximate Interpolation Coeﬃcient

The lowerbound of policy improvement depends on AπK+1
πK ,dπK .
Though it is general diﬃcult to compute exactly, very recently
[17] propose to estimate it using batch samples. We hence de-
ﬁne several quantities following [17]: let Bt denote a batch ran-
domly sampled from the replay buﬀer B and deﬁne ˆAK(s) :=
π(s), ˆAK := Es∼B[ ˆAK(s)]
maxa Q(s, a) − V(s) as an estimate of A˜π
π,dπ , and ˆAK,min := mins∼B ˆAK(s) as the mini-
as an estimate of A˜π
mum of the batch. When we use linear function approximation
with on-policy buﬀer BK, we simply change the minibatch B in
the above notations to the on-policy buﬀer BK.

Given the notations deﬁned above, we can compare the ex-

isting interpolation coeﬃcients as the following:

CPI: the classic CPI algorithm proposes to use the coeﬃ-

cient:

ζCPI = (1 − γ) ˆAK
4rmax

,

(13)

where rmax is the largest possible reward. When the knowledge
of the largest reward is not available, approximation based on
batches or buﬀer will have to be employed.

Exact SPI: SPI proposes to extend CPI by using the follow-

ing coeﬃcient:

ζE-SPI = (1 − γ)2 ˆAK
γδ∆AπK+1
πK
where δ, ∆AπK+1
πK were speciﬁed in Lemma 2. When δ, ∆AπK+1
πK
cannot be exactly computed, sample-based approximation will
have to employed.

(14)

,

Approximate SPI: as suggested by [9, Remark 1], approx-

imate ζ can be derived if we na¨ıvely leverage δ∆AπK+1

πK < 4

1−γ :

ζA-SPI = (1 − γ)3 ˆAK

4γ

.

(15)

Linear CPP: if policies are entropy-regularized as indi-
πK by using Lemma

cated in Eq. (3), we can upper bound δ∆AπK+1
3:

ζCPP = (1 − γ)3 ˆAK

8γCK

.

(16)

By the deﬁnition of CK in Eq. (7), ζCPP can take on a wider
range of values than ζA-SPI.

Deep CPI: for better working with deep networks, the fol-
lowing adaptive coeﬃcient was proposed in deep CPI (DCPI)
[17]:

ζDCPI = ˆζ0

mK
MK

,


mK = ρ1mK−1 + (1 − ρ1) ˆAK


MK = min(ρ2MK−1, ˆAK,min),

(17)

6

Algorithm 1: Linear Cautious Policy Programming

Algorithm 2: Deep Cautious Policy Programming

Input: α, β, γ CPP parameters, I the total number of

Input: α, β, γ CPP parameters, T the total number of

iterations, T the number of steps for each
iteration
1 initialize θ, ˜π0 at random;
2 empty on-policy buﬀer BK = {};
3 for K = 1, . . . , I do
4

for t = 1, . . . , T do

5

6

7

8

9

10

Interact using policy ˜πK−1;
t , aK
Collect (sK

t , sK

t , rK

t+1) into buﬀer BK ;

compute basis matrix ΦK using BK;
update θ by normal equations Eq. (19);
compute ˆζ0 = 1
and ˆζ = ˆζ0
CK
empty on-policy buﬀer BK;

mK
MK

using Eq. (17);

where ρ1, ρ2 ∈ (0, 1) are learning rates, and ˆζ0 = 1
CPI [8].

4 same with

Deep CPP: we follow the DCPI coeﬃcient design for mak-
ing ζCPP suitable for deep RL. Speciﬁcally, we modify DCPP
by deﬁning ˆζ0 = 1
CK

:

steps, F the interaction period, C the update
period
1 initialize θ at random;
2 set θ− = θ, K = 0 and buﬀer B to be empty;
3 for K = 1, . . . , T do
4

interact with the environment using policy π(cid:15);
collect a transition tuple (s, a, r, s(cid:48)) into buﬀer B ;
if K mod F == 0 then

sample a minibatch Bt from B and compute the
loss Lvalue and Lpolicy using Eqs. (20), (21);
do one step of gradient descent on the loss
Ltrain = Lvalue + Lpolicy;
compute ˆAK, ˆAK and moving average mK, MK
using Eq. (17);
if K mod C == 0 then

θ− ← θ ;
compute ˆζ0 = 1
CK
(17);

and ζCPP = ˆζ0

mK
MK

using Eq.

5

6

7

8

9

10

11

12

ζDCPP = clip

(cid:40)

1
CK

mK
MK

(cid:41)

, 0, 1

,

(18)

where ε is a small constant preventing singular matrix inversion
and TπK+1 QπK is the empirical Bellman operator deﬁned by

where mK, MK are same as Eq. (17).
Based on Eqs. (16), (18), we detail the linear and deep imple-
mentations of CPP in the next section.

4.5. Approximate CPP

We introduce the linear implementation of CPP following
[51, 25] and deep CPP inspired by [17] in Algs. 1 and 2, re-
spectively. It is worth noting that in linear CPP we assume the
interpolated policy ˜π is used only for collecting samples (line
5 of Alg. 1) hence no projection is necessary as it does not
interfere with computing next policy.

Linear CPP. We adopt linear function approximation (LFA)
to approximate the Q-function by Q(s, a) = φ(s, a)T θ, where
φ(x) = [ϕ1(x), . . . , ϕM(x)]T , x = [s, a]T , ϕ(x) is the basis func-
tion and θ corresponds to the weight vector. One typical choice
of basis function is the radial basis function:

ϕi(x) = exp (cid:0) −

||x − ci||2
σ2

(cid:1),

where ci is the center and σ is the width. We construct basis
matrix Φ = [φ1(x1), . . . , φM(xN)] ∈ RT ×M, where T is the num-
ber of timesteps. Speciﬁcally, at K-th iteration, we maintain an
on-policy buﬀer BK. For every timestep t ∈ [1, T ], we collect
t+1) into the buﬀer and compute the basis matrix at
t , sK
(sK
the end of every iteration.

t , aK

t , rK

To obtain the best-ﬁt θK+1 for the K +1-th iteration, we solve

the least-squares problem ||TπK+1 QπK − ΦθK||2:

TπK+1 QπK (sK

t , aK

t ) := r(sK

t , aK

t ) + γ

(cid:88)

a

πK+1(a|sK

t+1)QK(sK

t+1, a).

Since the buﬀer is on-policy, we empty it at the end of every
iteration (line 10).

Deep CPP. Though CPP is an on-policy algorithm, by fol-
lowing [17] oﬀ-policy data can also be leveraged with the hope
that random sampling from the replay buﬀer covers areas likely
to be visited by the policy in the long term. Oﬀ-policy learning
greatly expands CPP’s coverage, since on-policy algorithms re-
quire expensively large number of samples to converge, while
oﬀ-policy algorithms are more competitive in terms of sample
complexity in deep RL scenarios.

We implement CPP based on the DQN architecture, where
the Q-function is parameterized as Qθ, where θ denotes the
weights of an online network, as can be seen from Line 2. Line
3 begins the learning loop. For every step we interact with
the environment using policy π(cid:15), where (cid:15) denotes the epsilon-
greedy policy threshold. As a result, a tuple of experience is
collected to the buﬀer.

Line 6 of Alg. 2 begins the update loop. We sample a mini-
batch from the buﬀer and compute the loss Lvalue, Lpolicy de-
ﬁned in Eqs. (20), (21), respectively. Since our implementation
is based on DQN, we do not include additional policy network
as done in [17]. Instead, we denote the policy as πθ to indicate
that the policy is a function of Qθ as shown in Eq. (22). The
base policy is hence denoted by π−
θ to indicate it is computed by

θK+1 = (cid:0)ΦT Φ + εI(cid:1)−1ΦT TπK+1 QπK ,

(19)

7

the target network of θ−. We deﬁne the regression target as:

ˆQ(st, at, rt, st+1) =(rt + γ

πθ(a|st+1)(cid:0)Q−(st+1, a)+

(cid:88)

a∈A

τ log πθ(a|st+1) + σ log

πθ(a|st+1)
π−
θ (a|st+1)

(cid:1).

Hence, the loss for θ is deﬁned by:

Lvalue(θ) = E(st,at,... )∼B

(cid:20)(cid:16)

Qθ(st, at) − ˆQ(st, at, rt, st+1)

(cid:17)2(cid:21)

.

(20)

It should be noted that the interpolated policy cannot be di-
rectly used as it is generally no longer Boltzmann. To tackle
this problem, we further incorporate the following minimiza-
tion problem to project the interpolated policy back to the Boltz-
mann policy class:

Lpolicy(θ) =
(cid:104)
E(st,at,... )∼B

(cid:16)

DKL

πθ(at|st)

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12) ζGQθ + (1 − ζ)π−

θ (at|st)

(cid:17)(cid:105)

,

(21)

where GQθ takes the maximizer of the action value function.
The reason why we can express the policy π and GQθ with the
subscript θ is because the policy is a function of action value
function, which has a closed-form solution (see [16] for de-
tails):

GQθ(a|s) =

π−
θ (a|s)α exp (βQθ(s, a))
a(cid:48)∈A π−

θ (a(cid:48)|s)α exp (βQθ(s, a(cid:48)))

,

(cid:80)

(22)

(cid:16)(cid:80)

which by simple induction can be written completely in terms
of Qθ as GQθ(a|s) ∝ exp
[14]. Line 8 performs
one step of gradient descent on the the compound loss and line
9 computes the approximate expected advantage function for
computing ζ.

(cid:17)
j=0 Qθ j(s, a)

There is one subtlety in that the deﬁnition of K is unclear
in the deep RL context: there is no clear notion of iteration. If
we na¨ıvely deﬁne K as the the number of steps or the number
of updates, then by deﬁnition CK in Eq.
(18) could quickly
converge to 0 or explode, rendering CPP losing the ability of
controlling update. Hence in our implementation, we increment
K by one every time we update the target network (every C
steps), which results in a suitable magnitude of K.

5. Experimental Results

The proposed CPP algorithm can be applied to a variety
of entropy-regularized algorithms.
In this section, we utilize
conservative value iteration (CVI) as the base algorithm in [16]
for our experiments. In our implementation, for the K + 1-th
update, the baseline policy ¯π in Eq. (3) is πK.

For didactic purposes, we ﬁrst examine all algorithms (spec-
iﬁed below) in a safety gridworld and the classic control prob-
lem pendulum swing-up. The tabular gridworld allows for ex-
act computation to inspect the eﬀect of algorithms. On the other
hand, pendulum swing-up leverages linear function approxima-
tion detailed in Alg. 1. We then apply the algorithms on a set
of Atari games to demonstrate the eﬀectiveness of our proposed

8

method. It is worth noting that even state-of-the-art monotonic
improving methods failed in complicated Atari games [23]. The
gridworld, pendulum swing-up and Atari games manifest the
growth of complexity and allow for comparison on how the al-
gorithms trade oﬀ stability and scalability.

For the gridworld and pendulum experiments, we compare
Linear CPP using coeﬃcient Eq.
(16) against safe policy it-
eration (SPI) [9] which is the closest to our work. We employ
Exact-SPI (E-SPI) coeﬃcient in Eq. (14) on the gridworld since
in small state spaces where the quantities δ, ∆AπK+1
can be ac-
πK
curately estimated. As a result, SPI performance should upper
bound that of CPP since CPP was derived by further loosen-
ing on SPI. For problems with larger state-action spaces, SPI
performance may become poor as a result of insuﬃcient sam-
ples for estimating those quantities, hence Approximate-SPI
(A-SPI) Eq. (15) should be used. However, leveraging A-SPI
coeﬃcient often results in vanishingly small ζ values.

For Atari games, we compare Deep CPP leveraging Eq.
(18) against on- and oﬀ-policy state-of-the-art algorithms, see
Section 5.3 for a detailed list. Speciﬁcally, we implement deep
CPP using oﬀ-policy data to show it is capable of leveraging
oﬀ-policy samples, hence greatly expanding its coverage since
on-policy algorithms typically have expensive sample require-
ment.

5.1. Gridworld with Danger
5.1.1. Experimental Setup

The agent in the 5 × 5 grid world starts from a ﬁxed position
at the upper left corner and can move to any of its neighbor-
ing states with success probability p or to a random diﬀerent
direction with probability 1 − p. Its objective is to travel to a
ﬁxed destination located at the lower right corner and receives
a +1 reward upon arrival. Stepping into two danger grids lo-
cated at the center of the gridworld incurs a cost of −1. Every
step costs −0.1. We maintain tables for value functions to in-
spect the case when there is no approximation error. Parameters
are tuned to yield empirically best performance. For testing the
sample eﬃciency, every iteration terminates after 20 steps or
upon reaching the goal, and only 30 iterations are allowed for
training. For statistical signiﬁcance, the results are averaged
over 100 independent trials.

5.1.2. Results

Figure (1a) shows the performance of SPI, CPP, and CVI,
respectively. Recall that SPI used the exact coeﬃcient Eq. (14).
The black, blue, and red lines indicate their respective cumula-
tive reward (y-axis) along the number of iterations (x-axis). The
shaded area shows ±1 standard deviation. CVI learned policies
that visited danger regions more often and result in delayed con-
vergence compared to CPP. Figure (1b) compares the average
policy oscillation deﬁned in Eq. (23).

The slightly worse oscillation value of CPP than SPI with
ζE-SPI is expected as CPP exploited a lower bound that is looser
than that of SPI. However, as will be shown in the following ex-
amples when both linear and nonlinear function approximation
are adopted, SPI failed to learn meaningful behaviors due to the
inability to accurately estimate the complicated lower bound.

(a)

(b)

Figure 1: Comparison between SPI, CPP, and CVI on the safety grid
world. The black line shows the mean SPI cumulative reward, the blue
line CPP, and the red line CVI in Figure (1a), with the shaded area
indicating ±1 standard deviation. Figure (1b) compares the respective
policy oscillation value deﬁned in Eq. (23).

5.2. Pendulum Swing Up

Since the state space is continuous in the pendulum swing
up, E-SPI can no longer expect to accurately estimate δ∆AπK+1
,
πK
so we employ A-SPI in Eq. (15) and compare both E-SPI and
A-SPI against Linear CPP Eq. (16).

5.2.1. Experimental Setup

A pendulum of length 1.5 meters has a ball of mass 1kg at
its end starting from the ﬁxed initial state [0, −π]. The pendu-
lum attempts to reach the goal [0, π] and stay there for as long as
possible. The state space is two-dimensional s = [θ, ˙θ], where
θ denotes the vertical angle and ˙θ the angular velocity. Action
is one-dimensional torque [−2, 0, 2] applied to the pendulum.
The reward is the negative addition of two quadratic functions
quadratic in angle and angular velocity, respectively:

R = −

1
z

(aθ2 − b˙θ2),

where 1
angular velocity. We set z = 10, a = 1, b = 0.01.

z normalizes the rewards and a large b penalizes high

To demonstrate that the proposed algorithm can ensure mono-

tonic improvement even with a small number of samples, we
allow 80 iterations of learning; each iteration comprises 500
steps. For statistical evidence, all ﬁgures show results averaged
over 100 independent experiments.

5.2.2. Results

We compare CPP with CVI and both E-SPI and A-SPI in
Figure 2. In this simple setup, all algorithms showed similar
trend. But CPP managed to converge to the optimal solution in
all seeds, as can be seen from the variance plot. On the other
hand, both SPI versions exhibited lower mean scores and large
variance, which indicate that for many seeds they failed to learn
the optimal policy. In Figure (2a), both E-SPI and CVI exhib-
ited wild oscillations, resulting in large average oscillaton val-

(a)

(b)

(c)

Figure 2: Comparison of SPI, CPP, and CVI on the pendulum swing
up task. Figure (2a) illustrates the policy oscillation value deﬁned in
Eq. (23). Figure (2b) shows the cumulative reward with ±1 standard
deviation. Figure (2c) shows the ζ values.

ues, in which the oscillation criterion is deﬁned as:

∀K, s.t. RK+1 − RK < 0,
||OJ||∞ = max

|RK+1 − RK|,

K
(cid:115)
(cid:0) (cid:88)

(RK+1 − RK)2(cid:1),

(23)

||OJ||2 =

K

where RK+1 refers to the cumulative reward at the K +1-th itera-
tion. It is worth noting that the diﬀerence RK+1 − RK is obtained
by ˜πK+1, ˜πK, which is the lower bound of that by ˜πK+1, πK. In-
tuitively, ||OJ||∞ and ||OJ||2 measure maximum and average os-
cillation in cumulative reward. The stars between CPP and CVI
represent statistical signiﬁcance at level p = 0.05.

The reason for SPI’s drastic behavior can be observed in
Figure (2c) (truncated to 30 iterations for better view); in E-SPI,
insuﬃcient samples led to very large ζ. The aggressive choice
of ζ led to a large oscillation value. On the other hand, A-SPI
went to the other extreme of producing vanishingly small ζ due
to the loose choice of ζ for ensuring improvement of ∆Jπ(cid:48)
π,dπ(cid:48) ≥
(1−γ)3(A˜π
, as can be seen from the almost horizontal lines in
8γ
the same ﬁgure; A-SPI had average value ∆J ˜πK+1
= 2.39 ×
10−9 and ζ = 1.69 × 10−6. CPP converged with much lower
oscillation thanks to the smooth growth of the ζ values; CPP
was cautious in the beginning (ζ ≈ 0) and gradually became
conﬁdent in the updates when it was close to the optimal policy
(ζ ≈ 1).

πK ,d ˜πK+1

π)2

However, it might happen that ζ values are large but proba-
bility changes are actually small and vice versa. To certify CPP
did not produce such pathological mixture policy and indeed

9

**||OJ||||OJ||25152535Average Oscillation ValueSPI-CVIMI-CVICVISPICPP||OJ||||OJ||22060100140Average Oscillation Value******MI-CVIA-SPI-CVIE-SPI-CVICVICPPA-SPIE-SPI102030Iteration00.250.50.751Value of MI-CVIA SPI-CVIE SPI-CVICPPA-SPIE-SPIdemonstrate that CPP is capable of achieving superior balance
between learning speed and oscillation values.

For on-policy algorithms, we include the celebrated prox-
imal policy gradient (PPO) [40], a representative trust-region
method. We also compare with Advantage Actor-Critic (A2C)
[53] which is a standard on-policy actor-critic algorithm: our
intention is to conﬁrm the expensive sample requirement of on-
policy algorithms typically render them underperformant when
the number of timesteps is not suﬃciently large.

For the oﬀ-policy algorithms, we decide to include several
state-of-the-art DQN variants: Munchausen DQN (MDQN) [54]
features the implicit KL regularization brought by the Mun-
it was shown that MDQN was the
chausen log-policy term:
only non-distributional RL method outperforming distributional
ones. We also include another state-of-the-art variant: Momen-
tum DQN (MoDQN) [45] that avoids estimating the intractable
base policy in KL-regularized RL by constructing momentum.
MoDQN has been shown to obtain superior performance on a
wide range of Atari games. Finally, as an ablation study, we
are interested in the case ζ = 1, which translates to conserva-
tive value iteration (CVI) [16] based on the framework Eq. (3).
CVI has not seen deep RL implementation to the best of our
knowledge. Hence a performant deep CVI implementation is
of independent interest.

All algorithms are implemented using library Stable Base-
lines 3 [55], and tuned using the library Optuna [56]. Further,
all on- and oﬀ-policy algorithms share the same network ar-
chitectures for their group (i.e. MDQN and CPP share the same
architecture and PPO and A2C share another same architecture)
for fair comparison. The experiments are evaluated over 3 ran-
dom seeds. Details are provided in Appendix A.2. We expect
that on simple tasks PPO and A2C might be stable due to the
on-policy nature, but too slow to learn meaningful behaviors.
However, PPO is known to take drastic updates and heavily
needs code-level optimization to correct the drasticity [41]. On
the other hand, for complicated tasks, too drastic policy updates
might be corrupted by noises and errors, leading to divergent
learning. By contrast, CPP should balance between learning
speed and oscillation value, leading to gradual but smooth im-
provement.

5.3.2. Results

Final Scores.

As is visible from Figure 4, Deep CPP
achieved either the ﬁrst or second place in terms of ﬁnal scores
on all environments, with the only competitive algorithm being
MDQN which is the state-of-the-art DQN variant, and occa-
sionally CVI which is the case of ζ = 1. However, MDQN suf-
fered from numerical stability on the environment Seaquest as
can be seen from the ﬂat line at the end of learning.

CVI performed well on the simple environment MsPacman,
which can be interpreted as that learning on simple environ-
ments is not likely to oscillate, and hence the policy regular-
ization imposed by ζ is not really necessary, setting ζ = 1 is
the best approach for obtaining high return. However, in gen-
eral it is better to have adjustable update: on the environment
BeamRider the beneﬁt of adjusting the degree of updates was
signiﬁcant: CPP learning curve quickly rised at the beginning

(a) CPP interpolated policy of swinging right ˜π(aright|st).

(b) E-SPI interpolated policy of swinging right ˜π(aright|st).

Figure 3: CPP and E-SPI interpolated policies of pendulum swing-
ing right ˜π(aright|st) (z-axis) for timesteps t = 1, . . . , 500 (x-axis) from
the ﬁrst to last iteration (y-axis). E-SPI interpolated policy performed
might much more aggressive than the CPP policy caused by the large
ζ values shown in Figure (2c).

cautiously learned, we plot in Figure 3 the interpolated policies
of CPP and E-SPI yielding action probability of the pendulum
swinging right ˜π(aright|st). The probability change is plotted in
z-axis, timesteps t = 1, . . . , 500 of all iterations are drawn on
x, y axes. For both cases, ˜π(aright|s) ≈ 0.33 which is uniform
at the beginning of learning. However, E-SPI policy ˜π(aright|s)
gradually peaked from around 10th iteration, which led to very
aggressive behavior policy. Such aggressive behavior was con-
sistent with the overly large ζ values shown in Figure (2c). On
the other hand, CPP policy ˜π(aright|s) was more tempered and
showed a gradual change conforming to its ζ change. The prob-
ability plots together with ζ values in Figure (2c) indicate that
the CPP interpolation was indeed eﬀective in producing non-
trivial diverse mixture policies.

5.3. Atari Games

5.3.1. Experimental Setup

We applied the algorithms to a set of challenging Atari games:

MsPacmann, SpaceInvaders, Beamrider, Assault and Sea-
quest [52] using the adaptive ζ introduced in Eq. (18). We
compare deep CPP with both on- and oﬀ-policy algorithms to

10

Figure 4: Comparison on Atari games averaged over 3 random seeds. CPP, MoDQN, MDQN and CVI are implemented as variants of DQN and
hence are oﬀ-policy. PPO and A2C are on-policy. Correspondence between algorithms and colors is shown in the lower right corner. Overall,
CPP achieved the best balance between ﬁnal scores, learning speed and oscillation values.

Criterion

Algorithm Assault

||OJ||2

||OJ||∞

CPP
MDQN
MoDQN
CVI
PPO
A2C
CPP
MDQN
MoDQN
CVI
PPO
A2C

151
129
162
77
74
218
59
51
111
6
16
52

Seaquest
622
2149
813
449
68
98
561
2141
716
361
9
15

SpaceInvaders
89
77
91
83
72
48
42
16
36
51
7
8

MsPacman
249
202
288
292
280
395
26
52
124
98
36
249

BeamRider
460
220
718
220
74
87
292
149
665
105
33
34

Table 1: The oscillation values of algorithms listed in Sec. 5.3.1 measured in ||OJ||2 and ||OJ||∞ deﬁned by Eq. (23). CPP achieved the best
balance between ﬁnal score, learning speed and oscillation values. Note that CPP was implemented to leverage oﬀ-policy data. Algorithms of
small oscillation values, such as PPO, failed to compete with CPP in terms of ﬁnal scores and convergence speed.

of learning, showing a signiﬁcant large gap with all other algo-
rithms. Further, while CVI occasionally performed well, it suf-
fered also from numerical stability: on the environment Assault,
CVI and MoDQN achieved around 1000 ﬁnal scores but ran
into numerical issues as visible from the end of learning. This
problem has been pointed out in [45].

On the other hand, on all environments on-policy algorithms
A2C and PPO failed to learn meaningful behaviors. On some
environment such as Assault A2C showed divergent learning
behavior at around 4 × 106 and PPO did not learn meaningful
behavior until the end. This observation suggests that the sam-
ple complexity of on-policy algorithms is high and generally
not favorable compared to oﬀ-policy algorithms.

Oscillation. The averaged oscillation values of all algo-
rithms are listed in Table 1. While MDQN showed competi-
tive performance against CPP, it exhibited wild oscillation on
the diﬃcult environment Seaquest [57] and ﬁnally ran into

numerical issue as indicated by the ﬂatline near the end. The
oscillation value reached to around 2100. Since MDQN is the
state-of-the-art regularized value iteration algorithm featuring
implicit regularization, this result illustrates that on diﬃcult en-
vironments, only reward regularization might not be suﬃcient
to maintain stable learning. On the other hand, CPP achieved a
balance between stable learning and small oscillation, with os-
cillation value around 600, attaining ﬁnal score slightly lower
than MDQN and higher than MoDQN and CVI.

The oscillation values and ﬁnal scores should be combined
together for evaluating how algorithms perform. CVI, MoDQN
sometimes showed similar performance to CPP, but in general
the ﬁnal scores are lower than CPP, with higher oscillation val-
ues. On the other hand, MDQN showed competitive ﬁnal scores,
but sometimes it exhibited wild oscillation and ran into numer-
ical issues, implying that on some environments where low os-
cillation is desired, CPP might be more desirable than MDQN.

11

and fast convergence can be proved. However, realistic appli-
cations are beyond the scope for their analysis and no scalable
implementation has been provided. On the other hand, value-
based methods have readily applicable error propagation analy-
sis [60, 46, 61] for the function approximation setting, but they
seldom focus on monotonic improvement guarantees such as
JK+1 − JK ≥ 0. In this paper, we started from the value-based
perspective to derive monotonic improvement formulation and
provide scalable implementation suitable for learning with deep
networks.

We veriﬁed that CPP can approximately ensure monotonic
improvement in low-dimensional problems and achieved supe-
rior tradeoﬀ between learning speed and stabilized learning in
high-dimensional Atari games. This tradeoﬀ is best seen from
the value of ζ: in the beginning of learning the agent prefers to
be cautious, resulting in small ζ values as can be seen from Fig-
ure (2c). In relatively simple scenarios where exact computa-
tion or linear function approximation suﬃces, ζ < 1 might slow
down convergence rate in favor of more stable learning. On the
other hand, in challenging problems this cautiousness might in
turn accelerate learning in the later stages, as can be seen from
the CVI curves in Figure 4 that correspond to drastically setting
ζ = 1: except in the environment MsPacman, in all other en-
vironments CVI performed worse than CPP. This might be due
to that learning with deep networks involve heavy approxima-
tion error and noises. Smoothly changing of the interpolation
coeﬃcient becomes necessary under these errors and noises,
which is a core factor of CPP. We found that CPP was espe-
cially useful in challenging tasks where both learning progress
and cautiousness are required. We believe CPP bridges the gap
between theory and practice that long exists in the monotonic
improvement RL literature: previous algorithms have only been
tested on simple environments yet failed to deliver guaranteed
stability.

CPP made a step towards practical monotonic improving
RL by leveraging entropy-regularized RL. However, there is
still room for improvement. Since the entropy-regularized poli-
cies are Boltzmann, generally the policy interpolation step does
not yield another Boltzmann by adding two Boltzmann policies.
Hence an information projection step should be performed to
project the resultant policy back to the Boltzmann class to re-
trieve Boltzmann properties. While this projection step can
be made perfect in the ideal case, in practice there is an un-
avoidable projection error. This error if well controlled, could
be damaging and signiﬁcantly degrade the performance. How
to remove this error is an interesting future direction.

Another sublety of CPP is on the use of Lemma 3. Lemma
3 states that the maximum KL divergence of a sequence of CVI
policies is bounded. However, since we performed interpola-
tion on top of CVI policies, it is hence not clear whether this
guarantee continues to hold for the interpolated policy, which
renders our use of Lemma 3 heurisitic. As demonstrated by the
experimental results, we found such heuristic worked well for
the problems studied. We leave the theoretical justiﬁcation of
Lemma 3 on interpolated policies to future work.

We believe the application of CPP, i.e., the combination of
policy interpolation and entropy-regularization to other state-

Figure 5: Learning curves of DCPI on Seaquest with four coeﬃcient
designs. All designs achieved the ﬁnal score of 50, while CPP achieved
around 3000 in Figure 4.

On-policy algorithms even showed low oscillation values, but
their ﬁnal scores are considered unacceptable.

5.4. Ablation Study

We are interested in comparing the performance of DCPI
with CPP to see the role played by ζDCPP. It is also enlightening
by inspecting the result of ﬁxing ζ as a constant value. In this
subsection, we perform ablation study by comparing the the
following four designs:

• DCPI with ﬁxed ζ = 0.01: this is to inspect the result of

constantly low interpolation coeﬃcient.

• DCPI with ﬁxed ζ = 0.5: this is to examine the perfor-

mance of equally weighting all policies.

• CPI: this uses the coeﬃcient from Eq. (13).

• SPI: this uses the DCPI architecture, but we compute

ζA-SPI by using Eq. (15).

We examine those four designs on the challenging environment
Seaquest. Other experimental settings are held same with Sec.
5.3.

As can be seen from Figure 5, all designs showed a simi-
lar trend of converging to some sub-optimal policy. The ﬁnal
scores were around 50, which was signiﬁcantly lower than CPP
in Figure 4. This result is not surprising since for ζ = 0.01,
almost no update was performed. For ζ = 0.5, the algorithm
weights contribution of all policies equally without caring about
their quality. On this environment, ζA-SPI is vanishingly small
similar with that shown in Figure (2c). Lastly, for CPI the num-
ber of learning steps is not suﬃcient for learning meaningful
behavior.

6. Discussion

Leveraging the entropy-regularized formulation for mono-
tonic improvement has been recently analyzed in the policy gra-
dient literature for tabular MDP [39, 58, 59].
In the tabular
MDP setting with exact computation, monotonic improvement

12

of-the-art methods is feasible at least within the value iteration
scenario. Indeed, CPP performs two regularization: one in the
stochastic policy space and the other in the reward function.
There are many algorithms share the reward function regular-
ization idea with CPP, which implies the possibility of adding
another layer of regularization on top of it. On the other hand,
distributional RL methods may also beneﬁt from the interpo-
lation since they output distribution of rewards which renders
interpolation straightforward. We leave them to future investi-
gation.

Another interesting future direction is to extend CPP to the
actor-critic setting that can handle continuous action spaces.
Though both CPI-based and entropy-regularized concepts have
been respectively applied in actor-critic algorithms, there has
not seen published results showing featuring this combination.
We expect that the combination could greatly alleviate the pol-
icy oscillation phenomenon in complicated continuous action
control domain such as Mujoco environments.

7. Conclusion

In this paper we proposed a novel RL algorithm: cautious
policy programming that leveraged a novel entropy regulariza-
tion aware lower bound for monotonic policy improvement.
The key ingredients of the CPP is the seminal policy interpola-
tion and entropy-regularized policies. Based on this combina-
tion, we proposed a genre of novel RL algorithms that can eﬀec-
tively trade oﬀ learning speed and stability, especially inhibiting
the policy oscillation problem that arises frequently in RL appli-
cations. We demonstrated the eﬀectiveness of CPP against ex-
isting state-of-the-art algorithms on simple to challenging envi-
ronments, in which CPP achieved performance consistent with
the theory.

8. Acknowledgement

This research is funded by JSPS KAKENHI Grant Number

21H03522 and 21J15633.

Appendix A. Appendix

In the ﬁrst part of the Appendix, we detail the proofs of
the theorems and lemmas that appear in our paper. We provide
implementation details in the latter half.

Appendix A.1. Proof of Theorem 4

In order to prove Theorem 4, we introduce the following
two lemmas. The ﬁrst concerns monotonic policy improve-
ment and the second provides a tool for connecting it with the
entropy-regularization-aware lower bound.

13

Appendix A.1.1. Monotonic Policy Improvement Lemma

In this section we provide the proof of Lemma 2. The proof
was borrowed from [9] but for the ease of reading we rephrase
it here.

Lemma 2. Provided that policy π(cid:48) is generated by partial
π,dπ ≥ 0, then the

update Eq. (2), ζ is chosen properly, and A˜π
following improvement is guaranteed:

(cid:0)(1 − γ)A˜π
π,dπ
2γδ∆A˜π
π

(cid:1)2

,

∆Jπ(cid:48)

π,dπ(cid:48) ≥
with ζ = min (1, ζ∗),
(1 − γ)2A˜π
γδ∆A˜π
π
(cid:88)

where ζ∗ =

δ = max

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

s

a∈A
= max
s,s(cid:48)

∆A˜π
π

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:0)˜π(a|s) − π(a|s)(cid:1)

,

|A˜π

π(s) − A˜π

π(s(cid:48))|.

π,dπ

,

(A.1)

Proof. The proof follows the similar derivation in the classic
CPI [8] and similar results appeared many times in e.g. [9, 22].
We also show that the role of ζ and (1 − ζ) in Eq. (2) can be
exchanged by solving a similar problem. To begin, we leverage
Theorem 3.5 of [9] that:

∆Jπ(cid:48)

π,dπ(cid:48) ≥ Aπ(cid:48)

π,dπ −

γ∆Aπ(cid:48)
π
2(1 − γ)2 max

s

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)

a∈A

(cid:0)π(cid:48)(a|s) − π(a|s)(cid:1)

.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Substituting in π(cid:48) = ζ ˜π + (1 − ζ)π, we have:
(cid:88)

(cid:88)

dπ(s)

π(cid:48)(a|s)Aπ(s, a)

Aπ(cid:48)
π,dπ =

s
(cid:88)

=

dπ(s)

a
(cid:88)

(cid:0)ζ ˜π(a|s) + (1 − ζ)π(a|s)(cid:1)Aπ(s, a)

s
(cid:88)

= ζ

dπ(s)

a
(cid:88)

s

a

˜π(a|s)Aπ(s, a) = ζA˜π

π,dπ,

∆Aπ(cid:48)
π

= max
s,s(cid:48)
= max
s,s(cid:48)

δ = max

s

= max
s

|Aπ(cid:48)

π (s(cid:48))|

π (s) − Aπ(cid:48)
π(s) − ζA˜π

π(s(cid:48))|,

|ζA˜π

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)

a∈A

(cid:88)

a∈A

(cid:0)π(cid:48)(a|s) − π(a|s)(cid:1)

,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:0)ζ ˜π(a|s) − ζπ(a|s)(cid:1)

.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(A.2)

(A.3)

(A.4)

Hence, Eq. (A.2) is transformed into:

∆Jπ(cid:48)

π,dπ(cid:48) ≥ ζA˜π

π,dπ −

γζ2∆A˜π
π
2(1 − γ)2 max

s

(cid:12)(cid:12)(cid:12)

(cid:88)

a∈A

(cid:0)˜π(a|s) − π(a|s)(cid:1)(cid:12)(cid:12)(cid:12).

(A.5)

The right hand side (r.h.s.) is a quadratic function in ζ and has
its maximum at

leverage the very recent result [16, Propsition 3], which states
that:

ζ∗ =

γ∆A˜π

π maxs

(1 − γ)2A˜π
(cid:12)(cid:12)(cid:12)
(cid:80)

a∈A

π,dπ

(cid:0)˜π(a|s) − π(a|s)(cid:1)(cid:12)(cid:12)(cid:12)

By substituting ζ∗ back to Eq. (A.5), we obtain:

.

(A.6)

DKL (πK+1(·|s) || πK(·|s)) ≤ 4BK + 2CK,

max
s

where BK = 1 − γK
1 − γ

(cid:15)β, CK = βrmax

K−1(cid:88)

k=0

αkγK−k−1,

(A.9)

∆Jπ(cid:48)

π,dπ(cid:48) ≥

(cid:0)(1 − γ)A˜π
π,dπ
2γδ∆A˜π
π

(cid:1)2

.

(A.7)

When ζ∗ > 1, we clip it using min(1, ζ∗).

Note that, if we exchange the roles of ζ and (1 − ζ), the
coeﬃcients in Eq.
(A.3) should be (1 − ζ). Equation (A.5)
would become a quadratic function in (1 − ζ); hence the r.h.s.
of Eq. (A.7) would be the maximum of (1 − ζ∗). This concludes
the proof.

Remark. By noting that ˜π(a|s) − π(s, a) appears in both
π, we see that the policy improvement ∆Jπ(cid:48)
δ and ∆A˜π
π,dπ is gov-
erned by the maximum total variation of policies. While one
can exploit Lemma 2 for a value-based RL algorithm, it can be
seen that it could only apply to problems with small-state ac-
tion spaces. In general, without further assumptions on π(cid:48), ˜π, π,
lower-bounding policy improvement is intractable, as maximiza-
tion δ and ∆A˜π
π in a large state space require exponentially many
samples for accurate estimation.

Appendix A.1.2. Entropy-regularization Lemma

To optimize the lowerbound in Lemma 2, it is required to
know δ [9], which is intractable for large state spaces without
further speciﬁcation on the considered policy class.

By considering the class of entropy-regularized MDPs, Lem-
ma 2 can be signiﬁcantly simpliﬁed, of which the following
lemma plays a crucial role.

Lemma 3. For any policies πK and πK+1 generated by tak-
(3), the following bound holds for

ing the maximizer of Eq.
their maximum total variation:

max
s

DT V (πK+1(·|s) || πK(·|s)) ≤
(cid:110) √

min
where BK = 1 − γK
1 − γ

(cid:15)β, CK = βrmax

1 − e−4BK −2CK ,

(cid:112)

8BK + 4CK

(cid:111)

,

K−1(cid:88)

k=0

αkγK−k−1,

(A.8)

K denotes the current iteration index and 0 ≤ k ≤ K − 1 is the
loop index. (cid:15) is the uniform upper bound of error.

Proof. By the Fenchel conjugacy of the Shannon entropy and
KL divergence [62], it is clear that the maximizing policies for
the regularized MDP are Boltzmann softmax [63] as shown in
Section 4.3.1. The relationship between Boltzmann softmax
policies has recently been actively investigated [25, 64]. We

14

where (cid:15) is the uniform upper bound of errors.
(cid:112)
While Pinsker’s inequality DT V (p||q) ≤

2DKL(p||q), where
p, q are distributions can be used to directly exploit Eq. (A.9),
there is a gap between the total variation and KL divergence
since DT V ≤ 1 and DKL is potentially unbounded. Leveraging
Pinsker’s inequality on Eq. (A.9) and then on Eqs. (A.3,A.4)
will result in large errors when DKL ≥

√
2
2 .

To tackle this problem, we introduce the following bound

due to [65] that has more benign behavior1:

DT V (p||q) ≤

√

1 − e−DKL(p||q).

(A.10)

A similar bound appears also in [67] but is a slightly looser.
More relevant inequalities of such kind can be found in [66].
Both [65] and [67] feature the component e−DKL(p||q) that en-
sures the total variation bound is well-deﬁned: the upperbound
√
1 − e−DKL(p||q) is guaranteed to be no large than 1. Hence we
can combine Eq. (A.10) with Eq. (A.9) by taking the maxi-
mization on both sides, yielding the following relationship:

max
s

DT V (πK+1(·|s)||πK(·|s)) ≤

≤

√

√

1 − e− maxs DKL(πK+1(·|s)||πK (·|s)

1 − e−4BK −2CK .

(A.11)

Now by applying Pinsker’s inequality on Eq. (A.9), we have
the following relationship:

max
s

DT V (πK+1(·|s) || πK(·|s)) ≤

(cid:112)

8BK + 4CK,

(A.12)

taking the minimum of Eqs. (A.11, A.12) yields the promised
result.

Now back to Eq.

(A.9), since the reward is bounded in
[−1, 1], rmax can be conveniently dropped. Also, note that for
simplicity we assume there is no update error, i.e., BK = 0.
However, it can be straightforwardly extended to cases where
errors present by simply choosing an upper-bound (cid:15) for errors.
It is worth noting that in deep RL setting the magnitude of
(cid:15) might be non-trivial and has to be considered in parameter
tuning. Intuitively, Lemma 3 ensures that an updated entropy-
regularized policy will not deviate much from the previous pol-
icy.

1Eq. (A.10 appears in other places in diﬀerent forms such as in [66, Eq.
(4)]). It is worth mentioning they are the same in essence and diﬀer only in
notations.

Appendix A.1.3. Proof of Theorem 4

Now, given Lemma 2 and Lemma 3, we are ready to prove

Theorem 4. We ﬁrst restate it for ease of reading.

Theorem 4. Provided that partial update Eq. (6) is adopted,
AπK+1
πK ,dπK ≥ 0, and ζ is chosen properly, then any maximizer pol-
icy of Eq. (3) guarantees the following improvement that de-
pends only on α, β, γ and AπK+1

πK ,dπK after any policy update:

∆J ˜πK+1

πK ,d ˜πK+1

≥

(cid:0)1 − γ)3(AπK+1

πK ,dπK )2

4γ

(cid:40)

max

1
1 − e−2CK

,

1
4CK

(cid:41)

,

with ζ = min (1, ζ∗), CK = β

K−1(cid:88)

k=0

αkγK−k−1,

where ζ∗ =

(1 − γ)3AπK+1
2γ

πK ,dπK

(cid:40)

max

1
1 − e−2CK

,

1
4CK

(cid:41)

.

Proof. The proof follows similarly to the proof of Lemma 2
and hence [9]. We prove Theorem 4 by noticing the following
inequalities hold for δ and ∆A˜π

π of Eq. (5), respectively:

∆A˜π
π

= max
s,s(cid:48)
|A˜π

≤ 2 max

(cid:12)(cid:12)(cid:12)
a
(cid:88)

= 2 max

s

≤ 2 max

s

s

(cid:12)(cid:12)(cid:12)Qπ
(cid:12)(cid:12)(cid:12)
≤ 2
√

|A˜π

π(s) − A˜π

π(s)| = 2 max
(cid:88)

s

π(s(cid:48))|
(cid:12)(cid:12)(cid:12)

(cid:88)

a

(cid:0)˜π(a|s)Qπ(s, a) − π(a|s)Qπ(s, a)(cid:1)(cid:12)(cid:12)(cid:12)

˜π(a|s)(cid:0)Qπ(s, a) − Vπ(s)(cid:1)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)∞ max
(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)
(cid:0)˜π(a|s) − π(a|s)(cid:1)Qπ(s, a)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)˜π(a|s) − π(a|s)

(cid:88)

a

≤ 2

2Vmax max

s

s

a
(cid:113)

DKL

(cid:0)˜π(·|s)||π(·|s)(cid:1),

(A.13)

1−γ rmax is the maximum possible value function.
1−γ .

where Vmax := 1
Since we assume reward is upper bounded by 1, Vmax = 1
The second inequality makes use of the triangle inequality:
(cid:12)(cid:12)(cid:12)

(cid:0)˜π(a|s) − π(a|s)(cid:1)(cid:12)(cid:12)(cid:12),

δ ≤ max

(A.14)

(cid:88)

s

a∈A

+
= 1, with p set to 1 and q set to ∞. The last inequality is

and the third inequality makes use of H¨older’s inequality 1
p
1
q
because of Pinsker’s inequality:
(cid:12)(cid:12)(cid:12)˜π(a|s) − π(a|s)

(cid:12)(cid:12)(cid:12) ≤ max

(cid:88)

(cid:112)

2DKL(˜π(·|s)||π(·|s)), (A.15)

max
s

s

a∈A

Following [9], by incorporating Eqs. (A.14, A.15) and Eqs.

and the fact that ||Qπ||∞ ≤ Vmax = 1

1−γ .

(A.11, A.12) into ∆Jπ(cid:48)

π,dπ(cid:48) ≥

(cid:1)2

(cid:0)
(1−γ)A˜π

π,dπ
2γδ∆A˜π
π

, in Eq. (A.1) we have:

15

(cid:16)

(cid:16)

(1 − γ)A˜π
π,dπ
2γδ∆A˜π
π
(1 − γ)A˜π

π,dπ

(cid:17)2

(cid:17)2

2γ

1
DT V
max
s
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)

δ, Eq.(A.15)

1

2Vmax max
DT V
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
π, Eq.(A.13)

∆A˜π

s

∆Jπ(cid:48)

π,dπ(cid:48) ≥

≥

=

(1 − γ)3(A˜π

π,dπ)2

1
2 maxs D2
T V

2γ
(1 − γ)3(A˜π

π,dπ)2

4γ
(1 − γ)3(A˜π

π,dπ)2

1
4CK

,

1
1 − e−2CK

,

4γ

(A.9)
≥

(A.11)
≥

or

(A.16)

by taking the maximum of the two possible outcomes, the result
becomes:

∆Jπ(cid:48)

π,dπ(cid:48) ≥

(1 − γ)3
4γ

· (A˜π

π,dπ)2 · max

(cid:40)

1
1 − e−2CK

,

1
4CK

(cid:41)

.

The way of choosing ζ is same as Eq. (A.7) solving the equation
that is negative quadratic in ζ.

Appendix A.2. Implementation Details

In Algorithm 2 we followed [17] for computing the station-
ary weighted advantage function that empirically shows good
performance. It should be noted that accurately estimating sta-
tionary distribution is still nontrivial [68] and we leave the im-
provement to CPP in this regard to our future work.

Deep CPP, MDQN, MoDQN and CVI in the experimental
section share the same network architecture and hyperparame-
ters as speciﬁed by the following table:

Hyperparameters
Number of convolutional layers
Convolutional layer channels
Convolutional layer kernel size
Convolutional layer stride
Number of fully connected layers
Batch size
Replay buﬀer size
Discount rate
Steps per update
Learning rate
Optimizer
Loss
T the total number of steps
F the interaction period
C the update period

Values
3
(32, 64, 64)
(8, 4, 3)
(4, 2, 1)
1, with 512 hidden units
64
106
0.99
4
1 × 10−4
Adam [24]
Mean squared error
5 × 106
4
8000

By comparing our results on Deep CPP and Deep CPI [17]
we see there is diﬀerence on the horizon. We ran all algorithms

for 5 × 106 steps while Deep CPI was ran for 5 × 107 steps.
However, we can still make a comparison by the scores up to
5 × 106 steps. By comparing on the environments that appeared
in both papers we have:

Environment
MsPacman
SpaceInvaders
Seaquest

DCPP DCPI
2200
2000
800
800
2000
3000

Hence we see on relatively simple environments like MsPacman
and SpaceInvaders DCPP and DCPI performed similarly. On
the other hand, on the challenging environment Seaquest [57],
DCPP achieved around 30% higher scores at the end of 5 × 106
environment steps.

We also report the tuned hyperparameters unique to each

algorithm in Figure 3 using Optuna [56]:

Algorithm

CPP

MDQN

MoDQN

CVI

Parameters
Entropy τ: 0.0124
KL regularization σ: 0.001
Entropy τ: 0.03
Munchausen term σ: 0.9

Same with [45]
Gap coeﬃcient α: 0.00024
Temperature β: 0.000225

The hyperparameters were obtained by running on the environ-
ment SpaceInvaders for 300 Optuna trials [56]. Each trial
consists of 105 steps and the resultant 300 sets of parameters
were ranked. For the on-policy algorithms, PPO and A2C are
built-in with Stable Baselines 3 library [55] and the parameters
were already ﬁne-tuned. We evaluated them without changing
their default hyperparameters.

References

[1] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. Mc-
Grew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schnei-
der, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba,
L. Zhang, Solving rubik’s cube with a robot hand (2019).
arXiv:
arXivpreprint,arXiv:1910.07113.
URL https://arxiv.org/pdf/1910.07113.pdf

[2] V. Mnih, K. Kavukcuoglu, D. Silver, Others, Human-level control through

deep reinforcement learning, Nature 518 (7540) (2015) 529–533.

[3] D. P. Bertsekas, Dynamic Programming and Optimal Control, 2005.

arXiv:arXiv:1011.1669v3.

[4] Y. Ye, The simplex and policy-iteration methods are strongly polynomial
for the markov decision problem with a ﬁxed discount rate, Mathematics
of Operations Research 36 (2011) 593–603.

[5] D. Bertsekas, Approximate policy iteration: A survey and some new
methods, Journal of Control Theory and Applications 9 (2011) 310–335.
[6] P. Wagner, A reinterpretation of the policy oscillation phenomenon in ap-
proximate policy iteration, in: Advances in Neural Information Process-
ing Systems 24, 2011, pp. 2573–2581.

[7] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Oﬀ-policy
maximum entropy deep reinforcement learning with a stochastic actor, in:
Proceedings of the 35th International Conference on Machine Learning,
Vol. 80, 2018, pp. 1861–1870.

[8] S. Kakade, J. Langford, Approximately optimal approximate reinforce-
ment learning, in: 19th International Conference on Machine Learning
(ICML), 2002, pp. 267–274.

16

[9] M. Pirotta, M. Restelli, A. Pecorino, D. Calandriello, Safe policy itera-
tion, in: Proceedings of the 30th International Conference on Machine
Learning, Vol. 28, 2013, pp. 307–315.

[10] B. Scherrer, M. Geist, Local policy search in a convex space and conser-
vative policy iteration as boosted policy search, in: Machine Learning and
Knowledge Discovery in Databases, 2014, pp. 35–50.

[11] G. Neu, A. Jonsson, V. G´omez, A uniﬁed view of entropy-regularized

markov decision processes, arXiv:1705.07798 (2017).
URL http://arxiv.org/abs/1705.07798

[12] B. D. Ziebart, Modeling purposeful adaptive behavior with the principle
of maximum causal entropy, Ph.D. thesis, Carnegie Mellon University
(2010).

[13] E. Todorov, Linearly-solvable Markov decision problems, in: Advances
in Neural Information Processing Systems (NIPS), 2006, pp. 1369–1376.
[14] N. Vieillard, T. Kozuno, B. Scherrer, O. Pietquin, R. Munos, M. Geist,
Leverage the average: an analysis of regularization in rl, in: Advances in
Neural Information Processing Systems 33, 2020, pp. 1–12.

[15] O. Nachum, M. Norouzi, K. Xu, D. Schuurmans, Trust-pcl: An oﬀ-policy
trust region method for continuous control, in: International Conference
on Learning Representations, 2018, pp. 1–11.

[16] T. Kozuno, E. Uchibe, K. Doya, Theoretical analysis of eﬃciency and ro-
bustness of softmax and gap-increasing operators in reinforcement learn-
ing, in: Proceedings of the Twenty-Second International Conference on
Artiﬁcial Intelligence and Statistics, Vol. 89, 2019, pp. 2995–3003.
[17] N. Vieillard, O. Pietquin, M. Geist, Deep conservative policy iteration, in:
The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI’20,
AAAI Press, 2020, pp. 6070–6077.

[18] S. Fujimoto, H. van Hoof, D. Meger, Addressing function approximation
error in actor-critic methods, in: Proceedings of the 35th International
Conference on Machine Learning, Vol. 80, 2018, pp. 1587–1596.
[19] J. Fu, A. Kumar, M. Soh, S. Levine, Diagnosing bottlenecks in deep
q-learning algorithms, Vol. 97 of International Conference on Machine
Learning, 2019, pp. 2021–2030.

[20] M. Pirotta, M. Restelli, L. Bascetta, Adaptive step-size for policy gradi-
ent methods, in: Advances in Neural Information Processing Systems,
Vol. 26, Curran Associates, Inc., 2013, pp. 1–9.

[21] Y. Abbasi-Yadkori, P. L. Bartlett, S. J. Wright, A fast and reliable policy
improvement algorithm, in: Proceedings of the 19th International Con-
ference on Artiﬁcial Intelligence and Statistics, Vol. 51 of Proceedings of
Machine Learning Research, PMLR, 2016, pp. 1338–1346.

[22] A. M. Metelli, M. Mutti, M. Restelli, Conﬁgurable Markov decision pro-
cesses, in: Proceedings of the 35th International Conference on Machine
Learning, Vol. 80 of Proceedings of Machine Learning Research, 2018,
pp. 3491–3500.

[23] M. Papini, A. Battistello, M. Restelli, Balancing learning speed and sta-
bility in policy gradient via adaptive exploration, in: Proceedings of
the Twenty Third International Conference on Artiﬁcial Intelligence and
Statistics, Vol. 108 of Proceedings of Machine Learning Research, 2020,
pp. 1188–1199.

[24] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: 3rd
International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
[25] M. G. Azar, V. G´omez, H. J. Kappen, Dynamic policy programming,
Journal of Machine Learning Research 13 (1) (2012) 3207–3245.
[26] R. Fox, A. Pakman, N. Tishby, Taming the noise in reinforcement learning
via soft updates, in: Proceedings of the Thirty-Second Conference on
Uncertainty in Artiﬁcial Intelligence, 2016, pp. 202–211.

[27] T. Haarnoja, H. Tang, P. Abbeel, S. Levine, Reinforcement learning with
deep energy-based policies, in: Proceedings of the 34th International
Conference on Machine Learning, Vol. 70, 2017, pp. 1352–1361.
[28] J. Mei, C. Xiao, R. Huang, D. Schuurmans, M. M¨uller, On principled en-
tropy exploration in policy optimization, in: Proceedings of the Twenty-
Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI-
19, 2019, pp. 3130–3136.

[29] Z. Ahmed, N. Le Roux, M. Norouzi, D. Schuurmans, Understanding the
impact of entropy on policy optimization, in: Proceedings of 36th Inter-
national Conference on Machine Learning, Vol. 97, 2019, pp. 151–160.

[30] E. Uchibe, Model-free deep inverse reinforcement learning by logistic

regression, Neural Processing Letters 47 (3) (2018) 891–905.

[31] E. Uchibe, K. Doya, Forward and inverse reinforcement learning sharing
network weights and hyperparameters, Neural Networks 144 (2021) 138–

mann, Stable-baselines3: Reliable reinforcement learning implementa-
tions, Journal of Machine Learning Research 22 (268) (2021) 1–8.
[56] T. Akiba, S. Sano, T. Yanase, T. Ohta, M. Koyama, Optuna: A next-
generation hyperparameter optimization framework, in: Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Dis-
covery Data Mining, KDD ’19, Association for Computing Machinery,
2019, p. 2623–2631.

[57] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves,
V. Mnih, R. Munos, D. Hassabis, O. Pietquin, C. Blundell, S. Legg, Noisy
networks for exploration, in: Proceedings of the International Conference
on Representation Learning (ICLR 2018), Vancouver (Canada), 2018.

[58] A. Agarwal, S. M. Kakade, J. D. Lee, G. Mahajan, Optimality and ap-
proximation with policy gradient methods in markov decision processes,
in: Proceedings of Thirty Third Conference on Learning Theory, Vol. 125
of Proceedings of Machine Learning Research, 2020, pp. 64–66.

[59] S. Cen, C. Cheng, Y. Chen, Y. Wei, Y. Chi, Fast global convergence of nat-
ural policy gradient methods with entropy regularization, arXiv preprint,
arXiv:2007.06558 (2020).

[60] R. Munos, Error bounds for approximate value iteration, in: Proceedings
of the 20th National Conference on Artiﬁcial Intelligence - Volume 2,
AAAI’05, AAAI Press, 2005, pp. 1006–1011.

[61] A. Lazaric, M. Ghavamzadeh, R. Munos, Analysis of classiﬁcation-
based policy iteration algorithms, Journal of Machine Learning Research
17 (19) (2016) 1–30.

[62] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University

Press, USA, 2004.

[63] M. Geist, B. Scherrer, O. Pietquin, A theory of regularized Markov deci-
sion processes, in: Proceedings of the 36th International Conference on
Machine Learning, Vol. 97, 2019, pp. 2160–2169.

[64] K. Asadi, M. L. Littman, An alternative softmax operator for reinforce-
ment learning, in: Proceedings of the 34th International Conference on
Machine Learning (ICML), Vol. 70 of Proceedings of Machine Learning
Research, PMLR, International Convention Centre, Sydney, Australia,
2017, pp. 243–252.

[65] J. Bretagnolle, C. Huber, Estimation des densit´es :

risque minimax,

S´eminaire de probabilit´es de Strasbourg 12 (1978) 342–363.

[66] I. Sason, S. Verd´u, f-divergence inequalities, IEEE Transactions on Infor-

mation Theory 62 (2016) 5973–6006.

[67] A. B. Tsybakov, Introduction to Nonparametric Estimation, 1st Edition,

Springer Publishing Company, Incorporated, 2008.

[68] J. Wen, B. Dai, L. Li, D. Schuurmans, Batch stationary distribution esti-
mation, in: Proceedings of the 37th International Conference on Machine
Learning, Vol. 119, 2020, pp. 10203–10213.

153.

[32] Y. Cui, T. Matsubara, K. Sugimoto, Kernel Dynamic Policy Program-
ming: Applicable Reinforcement Learning to Robot Systems with High
Dimensional States, Neural networks 94 (2017) 13–23.

[33] Y. Tsurumine, Y. Cui, E. Uchibe, T. Matsubara, Deep reinforcement
learning with smooth policy update: Application to robotic cloth manip-
ulation, Robotics and Autonomous Systems 112 (2019) 72 – 83.

[34] L. Zhu, Y. Cui, G. Takami, H. Kanokogi, T. Matsubara, Scalable rein-
forcement learning for plant-wide control of vinyl acetate monomer pro-
cess, Control Engineering Practice 97 (2020) 104331–104340.

[35] L. Zhu, G. Takami, M. Kawahara, H. Kanokogi, T. Matsubara, Alleviating
parameter-tuning burden in reinforcement learning for large-scale process
control, Computers & Chemical Engineering 158 (2022) 107658.
[36] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region
policy optimization, in: Proceedings of the 32nd International Conference
on Machine Learning, Vol. 37, 2015, pp. 1889–1897.

[37] R. Akrour, A. Abdolmaleki, H. Abdulsamad, J. Peters, G. Neumann,
Model-free trajectory-based policy optimization with monotonic im-
provement, Journal of Machine Learning Research 19 (14) (2018) 1–25.
[38] L. Shani, Y. Efroni, S. Mannor, Adaptive trust region policy optimiza-
tion: Global convergence and faster rates for regularized mdps, CoRR
abs/1909.02769 (2019).
URL http://arxiv.org/abs/1909.02769

[39] J. Mei, C. Xiao, C. Szepesvari, D. Schuurmans, On the global conver-
gence rates of softmax policy gradient methods, in: Proceedings of the
37th International Conference on Machine Learning, Vol. 119, 2020, pp.
6820–6829.

[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal

policy optimization algorithms, arXiv:1707.06347 (2017).
URL https://arxiv.org/pdf/1707.06347.pdf

[41] L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph,
A. Madry, Implementation matters in deep policy gradients: A case study
on ppo and trpo, in: International Conference on Learning Representa-
tions (ICLR), 2019, pp. 1–12.

[42] M. Papini, M. Pirotta, M. Restelli, Adaptive batch size for safe policy gra-
dients, in: Advances in Neural Information Processing Systems, Vol. 30,
2017, pp. 1–10.

[43] O. Nachum, M. Norouzi, K. Xu, D. Schuurmans, Bridging the gap be-
tween value and policy based reinforcement learning, in: Advances in
Neural Information Processing Systems 30, 2017, pp. 2775–2785.
[44] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dy-
namic Programming, 1st Edition, John Wiley & Sons, Inc., New York,
NY, USA, 1994.

[45] N. Vieillard, B. Scherrer, O. Pietquin, M. Geist, Momentum in reinforce-
ment learning, in: Proceedings of the Twenty Third International Confer-
ence on Artiﬁcial Intelligence and Statistics, Vol. 108, 2020, pp. 2529–
2538.

[46] B. Scherrer, M. Ghavamzadeh, V. Gabillon, B. Lesner, M. Geist, Approx-
imate modiﬁed policy iteration and its application to the game of tetris,
Journal of Machine Learning Research 16 (1) (2015) 1629–1676.
[47] R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction, A

Bradford Book, Cambridge, MA, USA, 2018.

[48] A. Beck, First-Order Methods in Optimization, Society for Industrial and

Applied Mathematics, Philadelphia, PA, 2017.

[49] D. P. Bertsekas, J. N. Tsitsiklis, Neuro-Dynamic Programming, 1st Edi-

tion, Athena Scientiﬁc, 1996.

[50] D. Precup, Eligibility traces for oﬀ-policy policy evaluation, Computer

Science Department Faculty Publication Series (2000) 80.

[51] M. G. Lagoudakis, R. Parr, Least-squares policy iteration, The Journal of

Machine Learning Research 4 (44) (2003) 1107–1149.

[52] M. G. Bellemare, Y. Naddaf, J. Veness, M. Bowling, The arcade learn-
ing environment: An evaluation platform for general agents, Journal of
Artiﬁcial Intelligence Research 47 (1) (2013) 253–279.

[53] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, K. Kavukcuoglu, Asynchronous methods for deep reinforce-
ment learning, in: Proceedings of The 33rd International Conference on
Machine Learning, Vol. 48, 2016, pp. 1928–1937.

[54] N. Vieillard, O. Pietquin, M. Geist, Munchausen reinforcement learning,
in: Advances in Neural Information Processing Systems 33, 2020, pp.
1–11.

[55] A. Raﬃn, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, N. Dor-

17

