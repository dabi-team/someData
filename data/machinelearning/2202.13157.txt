2
2
0
2

g
u
A
8
1

]
L
M

.
t
a
t
s
[

2
v
7
5
1
3
1
.
2
0
2
2
:
v
i
X
r
a

High Dimensional Statistical Estimation under Uniformly
Dithered One-bit Quantization∗

Junren Chen1,∗, Cheng-Long Wang2,†, Michael K. Ng1,‡, Di Wang2,3,‡

August 19, 2022

Abstract

In this paper, we propose a uniformly dithered one-bit quantization scheme for high-
dimensional statistical estimation. The scheme contains truncation, dithering, and quan-
tization as typical steps. As canonical examples, the quantization scheme is applied to
three estimation problems: sparse covariance matrix estimation, sparse linear regression,
and matrix completion. We study both sub-Gaussian and heavy-tailed regimes, with the
underlying distribution of heavy-tailed data assumed to possess bounded second or fourth
moment. For each model we propose new estimators based on one-bit quantized data.
In sub-Gaussian regime, our estimators achieve optimal minimax rates up to logarithmic
factors, which indicates that our quantization scheme nearly introduces no additional
cost. In heavy-tailed regime, while the rates of our estimators become essentially slower,
these results are either the ﬁrst ones in such one-bit quantized and heavy-tailed setting,
or exhibit signiﬁcant improvements over existing comparable results. Moreover, we con-
tribute considerably to the problems of one-bit compressed sensing and one-bit matrix
completion. Speciﬁcally, we extend one-bit compressed sensing to sub-Gaussian or even
heavy-tailed sensing vectors via convex programming. For one-bit matrix completion,
our method is essentially diﬀerent from the standard likelihood approach and can han-
dle pre-quantization random noise with unknown distribution. Experimental results on
synthetic data are presented to support our theoretical analysis.

∗Junren Chen and Michael K. Ng were supported in part by Hong Kong Research Grant Council GRF
12300218, 12300519, 17201020, 17300021, C1013-21GF, C7004-21GF and Joint NSFC-RGC N-HKU76921. Di
Wang and Cheng-Long Wang were supported in part by the baseline funding BAS/1/1689-01-01, funding
from the CRG grand URF/1/4663-01-01, FCC/1/1976-49-01 from CBRC and funding from the AI Initiative
REI/1/4811-10-01 of King Abdullah University of Science and Technology (KAUST).

1Department of Mathematics, The University of Hong Kong, Pokfulam, Hong Kong.
2Division of CEMSE, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Ara-

bia.

3Computational Bioscience Research Center (CBRC), King Abdullah University of Science and Technology

(KAUST), Thuwal, Saudi Arabia.

∗Email: chenjr58@connect.hku.hk
†Email: chenglong.wang@kaust.edu.sa
‡Corresponding author. Email: mng@maths.hku.hk, di.wang@kaust.edu.sa

1

 
 
 
 
 
 
1

Introduction

One-bit quantization of signals or data recently has received much attention in both signal
processing and machine learning communities. In some signal processing problems, power con-
sumption, manufacturing cost and chip area of analog-to-digital devices grow exponentially
with their resolution [54], thus, it is impractical and infeasible to use high-precision data or
signals. Alternatively, it was proposed to use low-resolution, speciﬁcally one-bit quantization,
see for instance [4, 31, 34, 53, 64, 77]. In a more general sense, the quantization process that
maps an analog signal into digital representation of a ﬁnite dictionary is a critical process
in signal processing. Besides, in many distributed machine learning or federated learning
scenarios, multiple parties transmit information among themselves. The communication cost
can be prohibitive for distributed algorithms where each party only has a low-power and low-
bandwidth device such as a mobile device [60]. To address the bottleneck of communication
cost, recent works have studied how to send a small number or even one bit per entry for
such distributed machine learning applications [3, 8, 79, 86]. However, theoretical results on
statistical estimation under data quantization are extremely rare in literature, hence the be-
haviour of a learning process based on quantized data remains unclear. Recently, Dirksen et
al. [36] proposed a one-bit estimator for an unstructured covariance matrix. Although their
estimator can achieve a near minimax rate, their method is only for low dimensional case and
cannot be extended to the high dimensional (approximately) sparse or low rank setting, which
is commonly encountered in statistics and signal processing.

We further ﬁll the severe theoretical vacancy in this paper. More speciﬁcally, we study
three fundamental high-dimensional statistical estimation problems based on data that are
quantized to one bit. The quantization scheme include the typical steps of truncation, dither-
ing, and quantization (note that truncation is for heavy-tailed data only, which shrinks outliers
and enhances robustness), see Section 1.2 for detailed discussions. We present extensive the-
oretical results on sparse covariance matrix estimation, sparse linear regression, and low-rank
matrix completion, under both sub-Gaussian data and heavy-tailed data. Here, the underly-
ing distribution of heavy-tailed data is assumed to have bounded second or fourth moment.
Our estimators in sub-Gaussian regime have remarkable statistical properties, i.e., they could
achieve near minimax rates (up to some logarithmic factors). In the heavy-tailed regime, our
estimators can still deliver a faithful estimation under a high-dimensional scaling, while due
to a bias-and-variance trade-oﬀ, the error rates are essentially slower than the minimax ones
in the classical settings. However, to our best knowledge, these are the ﬁrst high-dimensional
statistical results under such two-fold predicament, i.e., heavy-tailed distribution that breaks
the robustness, and one-bit quantization that loses data information. Here we summarize our
key results and contributions as follows (For simplicity we only consider parameters n, d, s
(or r), q and omit the others).

• In Section 2, for some zero-mean d-dimensional random vector X, we study the problem
of estimating its covariance matrix Σ∗ = E
= [σ∗ij], where Σ∗ has the approx-
q < 1
s for some 0
imate column-wise sparsity structure, i.e., supj
(cid:0)
and s > 0. Denote the full data that are i.i.d. copies of X by X1, ..., Xn. For sub-
Gaussian X, we i.i.d. sample the dithering noise vector
that are
{
γ, γ]d, and then dither and quantize each Xk to binary data
uniformly distributed on [
sign(Xk + Γk1), sign(Xk + Γk2). Based on these binary data, we propose a thresholding

Γk1, Γk2 : k

d
i=1 |

(cid:1)
P

[n]
}

σ∗ij|

XX T

≤

−

≤

∈

[d]

∈

q

2

estimator
show a near optimal minimax rate

Σ, see (2.4) and (2.18). Although only two bits are collected per entry, we

b

Σ − Σ∗

k

kop . s log n
(cid:16)

log d
n

q)/2

(1

−

.

For heavy-tailed X assumed to have bounded fourth moment, we ﬁrst element-wisely
truncate the full sample Xk to be
Xk := sign(Xk) min
(element-wise operation).
Xk by dithering and quantization. Our
Then similar to sub-Gaussian data, we deal with
e
estimator possesses an estimation error bound for operator norm error

, η

{|

b

}

(cid:17)
Xk|

k
• In Section 3, we study sparse linear regression Yk = X T

(cid:16)

(cid:17)

b

Σ − Σ∗

kop . s

e
log d
n

q)/4

(1

−

.

q

{

∈

∈

∈

≤

≤

θ∗i |

(Xk, Yk) : k

s for some 0

Rd satisﬁes

k Θ∗ + ǫk, k

[n] where the
d
q < 1 and s > 0,
desired signal Θ∗ = [θ∗i ]
i=1 |
the covariate Xk and the additive noise ǫk can be either sub-Gaussian or heavy-tailed.
P
, we ﬁrst study a novel setting where both Xk
[n]
Given the full data
}
and Yk are quantized to binary data. The covariate Xk is quantized by exactly the same
method as Section 2. For sub-Gaussian Xk and ǫk, the response Yk is quantized to be
γ, γ]. When Xk and ǫk are heavy-
sign(Yk + Λk) with Λk uniformly distributed on [
tailed (with bounded fourth moment), we truncate Yk to be
Yk and then similarly apply
Yk. The estimation relies on the one-bit sparse covariance
the dithered quantization to
Σ developed in Section 2. To deal with the lack of positive semi-
matrix estimator
deﬁniteness, we assume ΣXX = EXkX T
k has column-wise sparsity, which accommodates
the conventional isotropic condition (i.e., ΣXX = Id) used in compressed sensing. We
b
formulate the recovery as a convex programming problem with objective function com-
bining a generalized quadratic loss and an ℓ1 regularizer, see (3.17). In sub-Gaussian
case, we show our estimator

Θ could achieve a near optimal minimax rate of

−

e

e

b
−
In heavy-tailed case, our estimator possesses the error rate

k2 . √s

log n

Θ∗

r

Θ

(cid:16)

(cid:17)

k

b

log d
n

1

−

q/2

.

Θ

k

−

Θ∗

k2 . √s

log d
n

q/2)/4

(1

−

.

(cid:16)
Besides the ﬁrst results for this new setting, we also revisit the canonical one-bit com-
pressed sensing problem where we quantize Yk in a same manner but have full knowledge
of Xk. We estimate Θ∗ via analogous convex programming problems, see (3.24) and
(3.28). In sub-Gaussian regime, our estimator achieves a near optimal minimax rate

(cid:17)

b

Θ

k

−

Θ∗

k2 . √s

log d log n
n

q/2

1

−

.

(cid:16)r
In heavy-tailed regime, our estimator can still handle the high-dimensional sparse setting
which satisﬁes

(cid:17)

b

Θ

k

−

Θ∗

k2 . s

2
3

log d
n

(1

q/2)/3

−

.

(cid:16)
As it turns out, these two results exhibit signiﬁcant improvements upon existing worsk
(e.g., more general sensing vector, recovery from convex programming, or faster error
rate), see a detailed comparison in Appendix D.

(cid:17)

b

3

d

×

d matrix Θ∗ with singular values σ1(Θ∗)

• In Section 4, we study the problem of low-rank matrix completion Yk =
h
...

+ ǫk,
i
σd(Θ∗) is
where the desired d
≥
k=1 σk(Θ∗)q
q < 1 and r > 0. The
(approximately) low-rank
≤
covariate Xk is uniformly distributed on
where ei is the i-th column
of the Id, ǫk is sub-Gaussian or heavy-tailed noise. Given the full data
, we
quantize Yk to one bit by the same process as one-bit compressed sensing in Section
3. Our estimator
Θ is given by minimizing an objective functions constituted of a
generalized quadratic loss and a nuclear norm penalty, see (4.6). If ǫk is sub-Gaussian,
we show that

r for some 0
[d]
}

Θ achieves a near optimal minimax rate

≤
eieT
j
{

(Xk, Yk)

Xk, Θ∗

: i, j

P

≥

∈

b

}

{

If ǫk is heavy-tailed with bounded second moment, we show the recovery guarantee

b

Θ − Θ∗
d2

k

2
F

k

. rd−

q

log n

(cid:16)

d log d
n

q/2

1

−

,

(cid:17)

b

b

Θ − Θ∗
d2

k

2
F

k

. rd−

q

d log d
n

1/2

−

q/4

.

(cid:16)

(cid:17)

We emphasize that our approach is totally diﬀerent from the only existing method for
one-bit matrix completion, i.e., based on maximizing a likelihood function. Our essential
advantage is that our method can handle unknown pre-quantization random noise that
can even be heavy-tailed. See more discussions in Appendix D.

The rest of the paper is given as follows. In Section 5 we provide an overview of the proofs
and the main techniques; In Section 6, we present experimental results to corroborate our
theories; We ﬁnally give some concluding remarks in Section 7. The complete proofs, the
comparisons with existing results and details of numerical experiments are deferred to the
Appendix.

1.1 Notations and Preliminaries

∗

We ﬁrst introduce diﬀerent vector or matrix norms. Let [N] =
Rd, the ℓ1 norm, ℓ2 norm and max norm are given by

X = [xi]
∈
d
2)1/2,
xi|
(
i=1 |
number of non-zero entries in X. For a matrix X = [xij]
P
norm and max norm are deﬁned as

As general principles, lowercase letters (e.g., s, r) represent scalars, capital letters (e.g., X, Y )
represent vectors, and capital bold letters (e.g., X, Θ) represent matrices. Some exceptions are
that we use capital letter Y, Yk to denote the responses, Λ, Λk to denote the dithering noise for
Y, Yk, and Xk,i for the i-th entry of Xk. Notations marked by
denote the desired underlying
signals, e.g., Σ∗, Θ∗, Θ∗, while those with a hat denote our estimators, e.g.,
Σ,
Θ,
1, 2, ..., N
. For a vector
}
{
d
X
k2 =
k1 =
,
b
b
i=1 |
k
k
X
k0 to denote the
, respectively. Note that we also use
P
k
d, the operator norm, Frobenius
d
d
ij)1/2,
j=1 x2
kF = (
k2,
i=1
σd(X),
≥
d
i=1 σi(X) serves as the counterpart of the ℓ1 norm of vectors.
) to vectorize A in a column-wise manner, i.e.,
d,

xij|
kmax = max1
d |
k
≤
X
then the nuclear norm
k
d, we use vec(
Given A = [α1, ..., αd]
∈
d ]T , while the inverse of vec(
vec(A) = [αT
2 , ..., αT
1 , αT
·
then the inner product in Rd
d is deﬁned by
A, B
×
Throughout the paper, we use n to denote the number of samples in data, d to denote
the dimension of the feature vector of each sample. Expectation and probability are denoted

X
k
. Assume the singular values are σ1(X)
knu =
Rd

). Assume B
= Tr(AT B) = vec(A)T vec(B).

Rd
×
∈
XV
2=1 k
k

kmax = maxi

) is denoted by mat(

kop = sup

xi|
X

σ2(X)
P

[d] |
∈

xi|
b

...
P

Θ.

Rd

P

X

X

X

≥

≥

∈

k

k

i,j

i

h

≤

×

×

·

·

V

k

4

), P(

·

·

}

}

D1, D2, D3, ...

) respectively. For a speciﬁc event E, 1(E) stands for the corresponding indi-
by E(
cator function, i.e., 1(E) = 1 if E happens, 1(E) = 0 otherwise. We work with quite a
lot of parameters arising in several signal processing steps. To avoid confusion of constants,
we use
to denote constants whose values may vary from line to line, while
{
C1, C2, C3, ...
would only be used once to set a speciﬁc parameter, see (2.6), (2.9) for example.
{
We adopt standard asymptotic notations that omits absolute constants. Speciﬁcally, we
use B1 . B2 or B1 = O(B2) to abbreviate the fact that B1 ≤
CB2 for some absolute constant
C. Similarly, we write B1 & B2 or alternatively B1 = Ω(B2) if B1 ≥
CB2 for some C > 0. If
both B1 = O(B2) and B1 = Ω(B2) hold, i.e., B1 equals B2 up to constants, we write B1 ≍
B2.
0,
≥
sign(x) =
Tζ(x) =
x
x1(
|
To broaden the range of our readers, we give some preliminaries on sub-Gaussian random

1 if x < 0. Hard thresholding operator with threshold ζ is deﬁned by

) extracts the sign of a real number x, i.e., sign(x) = 1 if x

) operate on vectors or matrices element-wisely.

The function sign(

ζ). Both sign(

) and

Tζ(

| ≥

−

·

·

·

variable or concentration inequality as follows.

Deﬁnition 1. Given a real random variable X
exponential norm

X

k

kψ1 are deﬁned as
2

t > 0 : E exp

X 2
t2

X

kψ2 = inf

k

n
(cid:16)
X is said to be sub-Gaussian if

≤

(cid:17)
o
X
kψ2 ≤ ∞

k

R, its sub-Gaussian norm

∈

X

kψ2, sub-

k

,

X

kψ1 = inf

k

n

.

t > 0 : E exp

|

X
t

|

(cid:16)

≤

(cid:17)

2

.

(1.1)

o

Deﬁnition 2. Given a real random vector X
V T X

X

∈

kψ2 = sup
For X, Y

2=1 k
k
R we note a useful relation (see Lemma 2.7.7 in [87] for instance)

kψ2. X is said to be sub-Gaussian if

X

k

.

Rd, the sub-Gaussian norm is deﬁned to be
kψ2 ≤ ∞

k

V

k

∈

XY

kψ2 ≤ k

X

kψ1k

Y

kψ1.

k

(1.2)

Sub-Gaussian variable X share similar properties with Gaussian random variable, such as
light probability tail and bounded moment constraint. Here we only introduce the properties
used in this work, and interesting readers shall refer to [87].

Proposition 1. (Proposition 2.5.2, [87] Assume random variable X is sub-Gaussian, then for
absolute constants D1, D2 we have:
(a) For any t > 0, P

2 exp

X

t

.

(b) For any p

1,

≥

(cid:0)
E|

X

|

D2k

≤

|

≤

| ≥
p

1/p
(cid:1)

D1t2
2
X
ψ2
k

k

−
(cid:0)
X
kψ2√p.

(cid:1)

Proposition 2. (Proposition 2.6.1, [87]) Let X1, ..., XN be independent, zero-mean, sub-
Gaussian random variables, then for some absolute constant D1 we have
D1

2
ψ2 ≤

N
k=1 Xk

(cid:0)

(cid:1)

N
k=1 k

Xkk

2
ψ2.

(cid:13)
(cid:13) P

(cid:13)
(cid:13)

For concentration results, we only introduce Hoeﬀding’s inequality and Bernstein’s in-
P
equality. Several other concentration inequalities (e.g., Matrix Bernstein’s inequality) would
be properly referred to the sources when they are invoked in the proof.

Proposition 3. (Hoeﬀding’s inequality, e.g., Theorem 1.9, [76]) Let X1, ..., Xn be independent,
bounded random variables satisfying Xi ∈
1
n

[ai, bi], then for any t > 0 it holds that

(Xk − EXk)

2 exp

(1.3)

≥

≤

−

t

n

n

.

P

ai)2

2n2t2
i=1(bi −

(cid:17)

Xk=1

(cid:16)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

5

P

Proposition 4. (Bernstein’s inequality, e.g., Theorem 2.8.1, [87]) Let X1, ..., XN be indepen-
dent random variables, then for any t > 0 and for some absolute constant D1 we have

N

P

(Xk − EXk)

t

≥

≤

2 exp

−

D1 min

t2
Xkk

N
k=1 k

,

2
ψ1

t

maxk

[N ] k
∈

Xkkψ1

.

(1.4)

Xk=1

(cid:12)
(cid:12)

(cid:8)

(cid:16)

(cid:17)

(cid:16)(cid:12)
(cid:12)
Although sub-Gaussian data has exciting statistical properties like similar tail bounds as
Gaussian distribution, data in some real problems may have much heavier tail, to name a few,
data in economics and ﬁnance [49], biomedical data [13, 90], noise in signal processing [82, 89],
and even signal itself [2,61,63]. Therefore, it is necessary to study one-bit statistical estimation
in heavy-tailed regime, which is also considered in this work. We use bounded moment of
N+. Note that
certain order to capture the heavy-tailedness, i.e., E|
this is a widely used strategy [42, 43, 48, 52, 81, 89, 92].

M for some l

(cid:9)(cid:17)

P

X

≤

∈

|

l

1.2 One-bit Quantization Scheme

Truncation, dithering and quantization are three typical signal processing steps in our work.
Here we summarize our one-bit quantization scheme brieﬂy:

1. Truncation. The truncation step will only be used to heavy-tailed data. Speciﬁcally,
we ﬁrst specify a threshold η > 0, then the truncation step shrinks a scalar x to be
, and hence x with magnitude smaller than η would remain unchanged
sign(x) min
in truncation. Vectors are truncated element-wisely, and notations marked by tilde are
used exclusively to denote truncated data, for example,

x
|

, η

Xk and

Yk.

{|

}

⊂

Rm, we use X

2. Dithering. The dithering step is applied to all the data that we plan to quantize to
one bit. For E
uni(E) to state that X obeys uniform distribution
on E. In sub-Gaussian case we dither the covariate Xk and response Yk by uniformly
distributed noise. Speciﬁcally, as we need to sample two bits per entry for Xk, we draw
γ, γ]d) and dither Xk to be Xk + Γk1, Xk + Γk2. We only need 1-bit
Γk1, Γk2 ∼
γ, γ]) and obtain the dithered
information for each response Yk, so we sample Λk ∼
response Yk + Λk. In heavy-tailed case Xk and Yk are substituted by the truncated data
Xk and

uni([

uni([

Yk.

∼

−

−

e

e

3. Quantization. In quantization step we simply apply sign(

e

e
notations marked by a dot (e.g., ˙Yk,
data. More precisely, we have ˙Yk = sign(Yk + Λk),
Gaussian Xk, Yk, and ˙Yk = sign(
X and Y .

Yk + Λk),

˙Xk1,

) to the dithered data, and
˙Xk2) exclusively represent the one-bit quantized
˙Xkj = sign(Xk + Γkj), j = 1, 2 for sub-
Xk + Γkj), j = 1, 2 for heavy-tailed

˙Xkj = sign(

·

e
Intuition and Heuristic

1.3

e

In this subsection we illustrate the main idea of our approaches before proceeding to details.
Speciﬁcally, we will provide the intuition of our one-bit quantization scheme, and then heuris-
tically analyse a multi-bits matrix completion setting to illustrate the the reason why our
estimators could achieve near optimal minimax rates in sub-Gaussian regime.
In fact, the
idea of the whole paper stems from two simple observations, which are given in the following
two lemmas. We mention that Corollary 1 motivates [36] to estimate E(XY ) and hence an

6

unstructured covariance matrix via binary data, while Lemma 1 is its more general form and
enlightens the estimators in our work. For instance, while full observations are not available,
our loss function in matrix completion is constructed by substituting the full data Yk in the
˙Yk (see (4.6)). This idea comes from Lemma 1.
empirical ℓ2 loss with the one-bit surrogate γ

·

Lemma 1. Let X, Λ be two independent random variables satisfying
where γ

B, then we have E

sign(X + Λ)

= EX.

γ

X

|

| ≤

B, Λ

∼

uni

γ, γ]

[
−

≥

·

Corollary 1. (Lemma 16, [36]) Let X, Y be bounded random variables satisfying
Y
|
of X, Y. Then we have E

B, Λ1, Λ2 are i.i.d. uniformly distributed on [
γ2

−
sign(Y + Λ2)

sign(X + Λ1)

≥
= EXY .

γ, γ], γ

| ≤

(cid:2)

(cid:3)

·

·

(cid:1)
B,
B, and Λ1, Λ2 are independent

(cid:0)
X

| ≤

|

Next, by informal arguments, we heuristically compare full-data-based matrix comple-
tion and quantized matrix completion where one can sample ﬁnitely many bits from each Yk
(namely multi-bits matrix completion). This comparison can provide some insights of why
our estimators can achieve a near optimal minimax rate in sub-Gaussian regime.

(cid:2)

(cid:3)

We consider a full-data sample of size n from matrix completion (4.1) and denote it by

(X1, Y1), ..., (Xn, Yn)

.

Dfull =

n

o
Λkj : j

Now, for some positive integer f (n) we i.i.d. draw
from uni
and sample f (n) bits from each Yk by the proposed dithered quantization, that is,
(cid:0)
sign(Yk + Λkj) : j
∈
binary observations

γ, γ]
[
,
−
˙Ykj :=
(cid:1)
{
f (n)
. This quantization process yields the sample containing n

[f (n)]

[f (n)]

∈

}

}

{

·

Interestingly, from

Dmult =

(Xk, ˙Ykj) : k
n
Dmult one can build a dataset with size n as

[n], j

∈

∈

[f (n)]

.

o

(Xk, Yk,appr) : Yk,appr =

Dappr =




1
f (n)

˙Ykj, k

γ

·

[n]

∈

.




[f (n)]
Xj
∈



For simplicity we assume
Ω(1)) we have maxk |
O(n−

We aim to reveal that the above three samples are comparably informative for the estimation.
Ykkψ2 = O(1), then with probability at least
ǫkkψ2 = O(1),
k
k
= O(√log n) (see, e.g., Theorem 1.14 in [76]). Thus, we can
Yk|
1
−
choose γ = Poly(log n)1 to guarantee γ > maxk |
Yk|
with high probability, and we proceed the
Yk, equivalently we can write
analysis under this event. Deﬁne ǫk,appr := Yk,appr −



Yk,appr = Yk + ǫk,appr =

Xk, Θ∗

+ ǫk + ǫk,appr.

(1.5)

For ǫk,appr, Lemma 1 gives EΛkj (γ
on Yk, ǫk,appr is the mean of f (n) zero-mean, independent random variables lying in [
Yk, γ

Yk]. Thus, Proposition 2 and Hoeﬀding’s Lemma (see Lemma 1.8, [76]) give

˙Ykj) = Yk and hence Eǫk,appr = 0. Moreover, conditioned

(cid:11)

(cid:10)

·

−

γ

−

−

(1.6)

ǫk,apprkψ2 = O

k

γ

f (n)

.

(cid:17)

(cid:16)

Therefore,
suﬃce due to γ = Poly(log n). In conclusion,

as long as f (n) dominates γ2, while f (n) = Poly(log n) would
Poly(log n) binary data can

ǫk,apprkψ2 = O

k

1

p

(cid:0)

(cid:1)

Dmult containing n

·

1Here Poly(log n) denotes any term T satisfying T = O([log n]m) for some positive integer m.

7

k

ǫkkψ2 =

k
Dappr, and hence

generate the sample
from (1.5). Moreover, since
original model (4.1). This reveals
comparable to
degradation of recovery error after one-bit quantization.

Dappr of size n, where each Yk,appr can be viewed as a full observation
ǫk,apprkψ2 = O(1), (1.5) is nearly equivalent to the
Poly(log n) binary data, are
Dappr with n
Dfull with n full data. Furthermore, this indicates the inessential logarithmic
Note that similar heuristics can be found in sub-Gaussian regime of (sparse) covariance
matrix estimation and sparse linear regression. Of course, such multi-bits heuristic deviates
from the one-bit setting where we collect only one bit from each Yk (see the following graphical
illustration). But since f (n) := Poly(log n) is negligible compared with n, one may tend to
˙Yk := sign(Yk + Λk) : k
believe
are comparable. From this
perspective, the near-optimal rates in sub-Gaussian regime are matter of courses.

Dmult and

D1bit =

f (n)]

[n

∈

{

}

·

·

Y1

...
...

Yn

Y1,appr

˙Y11
˙Y12
...
˙Y1f (n)
˙Yn1
˙Yn2
...
˙Ynf (n)
Dmult)
(A heuristic multi-bits setting)

(

(

...
...

Yn,appr

Dappr)

(

Dfull)

Y1

Y2
...

Yf (n)

Y(n

Y(n

−

−

1)f (n)+1

1)f (n)+2

...

Ynf (n)

˙Y1
˙Y2
...
˙Yf (n)

1)f (n)+1

−

1)f (n)+2

−

...
˙Ynf (n)
D1bit)

(

˙Y(n
˙Y(n

(The one-bit setting)

However, in heavy-tailed regime the story becomes totally diﬀerent. Speciﬁcally, γ =
Poly(log n) will no longer guarantee γ > maxk |
with high probability. When this vital
condition fails, the dithering becomes invalid for responses with absolute value larger than
γ.
Indeed, for these measurements the proposed dithered quantization reduces to a direct
collection of the sign, while under such direct quantization we even lose the well-posedness of
the problem (e.g., matrix completion, see [32]) or the possibility of full signal reconstruction
(e.g., one-bit compressed sensing, see [70]).

Yk|

To resolve the issue, we truncate the heavy-tailed data according to some threshold η,
which produces data bounded by η. Then we can treat the truncated data as sub-Gaussian
with γ > η. It is not hard to see that
data and use dithering noise drawn from uni
[
−
η represents the data bias introduced in truncation, more precisely, smaller η corresponds
(cid:0)
to larger bias. Enlightened by (1.6) and Hoeﬀding’s Lemma, γ is positively related to data
variance. Deﬁnitely, for estimation or signal recovery we prefer data with small bias (i.e.,
big η) and small variance (i.e., small γ). But, note that we also need γ > η to enforce the
eﬀectiveness of dithering. Thus, a trade-oﬀ between bias and variance is needed, and making
an optimal balance between bias and variance leads to our error rates in heavy-tailed regime.
See Example 1 and Example 2 in Section 5.

γ, γ]

(cid:1)

8

2 Sparse Covariance Matrix Estimation

Rd be a random
We start from the problem of estimating a sparse covariance matrix. Let X
˙Xk2), and
vector with zero mean, the i.i.d. realizations Xk are quantized to one-bit data ( ˙Xk1,
we aim to estimate the underlying covariance matrix Σ∗ = EXX T based on the quantized
data.

∈

We ﬁrst ideally assume the underlying d-dimensional random vector X has entries bounded
= EXX T = Σ∗ is just the desired covari-
by γ, then Corollary 1 delivers that E
ance matrix. Besides, the concentration of γ2
k2 should be fast due to boundedness, see
·
Hoeﬀding’s inequality in Proposition 3. Combining these observations, [36] proposed a covari-
ance matrix estimator as an empirical version of E
, followed by symmetrization:

˙Xk1 ˙X T
k2
˙Xk1 ˙X T
(cid:3)

γ2

γ2

(cid:2)

·

˙Xk1 ˙X T
k2

·

˘Σ =

γ2
2n

n

˙Xk1 ˙X T

(cid:2)
k2 + ˙Xk2 ˙X T

k1

Xk=1 h

(cid:3)
.

i

(2.1)

For sub-Gaussian Xk, this estimator achieves a near minimax rate (compared with full data
setting in [19])

kop . log n
Here, we point out that sampling two bits (rather than one bit) per entry is merely for
estimating the diagonal entries of Σ∗, since the one-bit version of (2.1),

(2.2)

r

−

k

.

˘Σ

Σ∗

d log d
n

˘Σ1bit =

γ2
n

n

˙Xk1 ˙X T
k1,

Xk=1
always gives γ2 in the diagonal and hence fails to recover the diagonal of the covariance matrix.
It is evident that (2.2) requires at least n & d to provide a non-trivial error bound. Actu-
k /n has extremely
ally it has been reported that even the sample covariance matrix
n [51], not to mention (2.1).
poor performance under high dimensional scaling where d
On the other hand, high-dimensional databases are undoubtedly becoming ubiquitous in ge-
nomics, biomedical, imaging, tomography, ﬁnance and so forth, while covariance matrix plays
a fundamental role in analysis of these databases.

k XkX T

P

≥

To address the high-dimensional issue, extra structures are necessary to reduce the intrinsic
problem dimensionality. For covariance matrix we usually have sparsity as prior knowledge,
especially in the situation where dependencies among diﬀerent features are weak, for instance,
the Genomics data [39], functional data drawn from underlying curves [74]. A precise formu-
lation of the sparse prior is provided in Assumption 1.

Assumption 1. (Approximate column-wise sparsity) For a speciﬁc 0
≤
of covariance matrix Σ∗ = [σ∗ij] are approximately sparse in the sense that

q < 1, the columns

sup
[d]
j

∈

d

i=1
X

q

σ∗ij|

|

≤

s

(2.3)

In literature there are two mainstreams to incorporate sparsity into covariance matrix
estimation, namely penalized likelihood method [12, 78] and a thresholding method [11, 18,

9

20, 22, 40]. Thresholding method refers to the direct regularizer that element-wisely hard
k /n), which promotes sparsity
thresholding the sample covariance matrix, i.e.,
n
k=1 XkX T
intuitively. With suitable threshold ζ, Cai and Zhou [21] showed
k /n) could
achieve minimax rate under operator norm over the class of column-wisely sparse covariance
matrices (Assumption 1). Motivated by previous work, we propose to hard thresholding ˘Σ in
Σ = [
(2.1) to obtain a high-dimensional estimator
Tζ ˘Σ.
Σ =
b
b

n
k=1 XkX T

σij] given by

Tζ(

Tζ(

(2.4)

P

P

The statistical rates of
follows.

Σ under both max norm and operator norm are established in what

b

2.1 Sub-Gaussian Data

b

Rd with

(2.5)

Assume Xk = [Xk,1, Xk,2, ..., Xk,d] are i.i.d.
zero-mean sub-Gaussian components. In particular, we assume

sampled from a random vector X

∈

EXk = 0,
From (2.4), ˘Σ = [˘σij] serves as an intermediate estimator to construct
an element-wise error bound of ˘Σ in Theorem 1.

Xk,ikψ2 ≤

i
∀

[d].

σ,

∈

k

Σ, hence we ﬁrst provide

Theorem 1. Assume (2.5) holds. For speciﬁc δ
suﬃciently large constant C1 we set the dithering scale γ as

≥

1 we assume n > 2δ log d. For some

b

and assume γ > σ. Then for ˘Σ = [˘σij] we have

γ = C1σ

log

r

n
2δ log d

(cid:16)

(cid:17)

σ∗ij|
−
[d]. Moreover, we have the error bound for max norm

˘σij −

|
(cid:16)

. σ2 log n

r

≥

(cid:17)

1

P

δ log d
n

δ

2d−

for i, j

∈

˘Σ − Σ∗

kmax . σ2 log n

r

P

k
(cid:16)

δ log d
n

(cid:17)

1

−

≥

2d2
−

δ.

(2.6)

(2.7)

(2.8)

Recall that our estimator is obtained by hard thresholding ˘Σ. The next Theorem shows
that with suitable threshold ζ, the hard thresholding even brings a tighter statistical bound
for element-wise error.

Theorem 2. Assume (2.5) holds, δ
is given as (2.6) with some C1. Then we choose the threshold ζ by

≥

1 is the same as Theorem 1, and the dithering scale γ

ζ = C2σ2 log n

r

δ log d
n

,

where C2 is a suﬃciently large constant. Then for any i, j

[d] we have

σij −

σ∗ij|

. min

P

|
(cid:16)

σ∗ij|

|

n

, σ2 log n

r

b

10

∈
δ log d
n

o(cid:17)

1

−

≥

2d−

δ.

(2.9)

(2.10)

n )(1

s((log n)2 log d

By combining (2.10) and Assumption 1, we are in a position to establish the rate of
Σ under operator norm. Speciﬁcally, we prove that our one-bit estimator achieves a rate
O
, which almost matches the minimax rate O
proved in
b
Theorem 2 of [21]. Note that the estimator based on full data in [21] achieves the minimax
rate. From this perspective, the one-bit quantization only introduces minor information loss
to the learning process, i.e., a logarithmic factor. Thus, by using our method, one can embrace
the privileges of one-bit data and covariance matrix of comparable accuracy simultaneously.

log d
n

q)/2

q)/2

s

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(1

−

−

Theorem 3. Assume Assumption 1, (2.5) hold, δ is the same as Theorem 1, 2 (set δ
4),
and the dithering scale γ, the threshold ζ are respectively given by (2.6), (2.9) with some
C1, C2. Besides, assume δ log d(log n)2/n is suﬃciently small. Let p = δ/4, we have

≥

Σ − Σ∗

1/p

p
op

k

Ek

. s

σ2 log n

δ log d
n

r

q

1

−

.

(2.11)

(cid:17)
Moreover, the probability tail of operator norm error is bounded as

(cid:17)

(cid:16)

(cid:16)

b

Σ − Σ∗
k

P

(cid:16)

b

kop . s

σ2 log n

h

r

δ log d
n

q

1

−

i

(cid:17)

1

−

≥

exp(

δ).

−

(2.12)

Remark 1. We point out that the proof of Theorem 3 may be of independent technical interest,
especially the probabilistic inequality (2.12) that seems quite new in the literature. In fact, only
the upper bound for the second moment (i.e., p=2 and δ = 8 in (2.11)) is obtained in literature
1
(e.g., Theorem 3 in [21]), and by Markov inequality this can only give a probability term 1
δ1−q
−
δ). The key
in (2.12). Here, by contrast, we derive a much better probabilistic term 1
idea is to adaptively bound the Ω(δ)-th moment rather than a speciﬁc second moment, which
gives (2.11). It is straightfoward to apply this method to the traditional full-data thrsholding
estimator and gain some improvements of existing results.

exp(

−

−

Σ is

Σ)vivT

To guarantee positive semi-deﬁniteness, we introduce a trick developed in literature. As-
d
i=1 λi(
i , we remove the components corre-
Σ), 0)vivT
i .
Σ − Σ∗
b
Σ+ retains the operator
P
Σ. However, removing the negative components may destroy the element-wise

sume the eigenvalue decomposition of
sponding to negative eigenvalues and obtain the positive part
It is not hard to show that
norm rate of
error or the sparse pattern of
b

Besides, it is worthy noting that we present Theorem 3 under operator norm by convention,
, see an

Σ+ =
kop. Thus,
b

P
Σ+ − Σ∗
b
kop ≤

but both (2.11) and (2.12) are applicable to the larger norm
b
initial step in the proof (A.6).

d
i=1 max(λi(

b
Σ, see [73].

= supj

xij|

k1,

X

i |

b

b

b

k

k

k

2

∞

P

2.2 Heavy-tailed Data

Let Xk = [Xk,1, ..., Xk,d]T be i.i.d. drawn from the random vector X
consider zero-mean, heavy-tailed X assumed to have bounded 4-th moment

∈

Rd, in this part we

EXk = 0, E|

Xk,i|

4

≤

M,

i

∈

∀

[d]

(2.13)

11

Note that this oﬀers great relaxation compared with sub-Gaussian random variable (e.g.,
Proposition 1(b)) and encompasses more distributions such as t-distribution, log-normal dis-
tribution.

Compared with the light tail in Proposition 1(a), X satisfying (2.13) can have a much
heavier tail, and so overlarge data appear more frequently. To illustrate why this is problematic
in our quantization method, we mention that our dithering noise has ﬁnite scale γ, hence the
dithering is invalid for data with magnitude larger than γ. More precisely, we have

sign(Xk,i + Γkj,i) = sign(Xk,i),

if

Xk,i|

|

> γ.

Therefore, for those entries larger than γ, our signal processing reduces to a direct quantization
without dithering noise, which is known to introduce great loss of information.

To deal with the issue, we ﬁrst truncate the data larger than a speciﬁed threshold η and
Xk bounded by η, which is of the spirit to introduce some biases for
obtain the truncated data
variance reduction. Now that the truncated data are bounded, we similarly dither them by
uniform noise, and then quantize to ˙Xkj = sign(
γ, γ]d).
Motivated by Corollary 1, we propose an intermediate estimator

uni([

−

e

Xkj +Γkj), j = 1, 2, where Γkj ∼
e
k2 + ˙Xk2 ˙X T
˙Xk1 ˙X T

k1

,

Xk=1 h

i

n

˘Σ =

γ2
2n

which extends (2.1) to heavy-tailed data. Element-wise error for ˘Σ is given in Theorem 4.

Theorem 4. Assume (2.13) holds. For some ﬁxed δ
truncation parameter η and the dithering scale γ by

≥

1 and C3, C4 (C4 > C3), we set the

η = C3M 1/4




γ = C4M 1/4

Then for ˘Σ = [˘σij] given in (2.14), we have



n
δ log d
n
δ log d

(cid:16)

(cid:16)

1/8

1/8

(cid:17)

,

(cid:17)

1/4

|
(cid:16)
Moreover, we have the error bound under max norm

h

P

i

(cid:17)

˘σij −

σ∗ij|

. √M

δ log d
n

1

−

≥

2dδ.

˘Σ − Σ∗

kmax . √M

P

k
(cid:16)

δ log d
n

h

1/4

i

(cid:17)

1

−

≥

2d2
−

δ.

(2.14)

(2.15)

(2.16)

(2.17)

Parallel to the sub-Gaussian regime, we use an additional hard thresholding step to promote
sparsity. That is, based on the intermediate estimator ˘Σ in (2.14), we choose some suitable
thresholding parameter ζ and deﬁne the estimator
Tζ ˘Σ.

(2.18)

Σ =

We show the element-wise and operator norm statistical rates in Theorem 5, Theorem 6.

b

12

Theorem 5. Assume (2.13) holds, δ is the same as Theorem 4, and the truncation threshold
η and the dithering scale γ are set as (2.15) with some C3, C4. Then we set the threshold ζ in
(2.18) by

ζ = C5√M

1/4

δ log d
n

(cid:16)

(cid:17)

where C5 is a suﬃciently large constant. Then for any i, j

[d] we have

σij −

σ∗ij|

. min

P

|
(cid:16)

, √M

σ∗ij|

|

n

δ log d
n

h

b

∈
1/4

i

o(cid:17)

1

−

≥

2d−

δ.

(2.19)

(2.20)

Theorem 6. Assume Assumption 1, (2.13) hold, δ is ﬁxed and the same as Theorem 4, 5
(set δ
4), the truncation threshold η, the dithering scale γ, the threshold ζ are set as (2.15),
(2.19) for some speciﬁed C3, C4, C5. Besides, assume that δ log d/n is suﬃciently small. Let
p = δ/4, then we have the bound for the moment of order p

≥

Σ − Σ∗

p
op

k

Ek

(cid:16)

(cid:17)

1/p

. sM (1
−

q)/2

δ log d
n

q)/4

(1

−

.

i

(2.21)

h

Moreover, we bound the probability tail of operator norm error

b

Σ − Σ∗
k

P

(cid:16)

b

kop . sM (1

−

q)/2

δ log d
n

h

q)/4

(1

−

i

(cid:17)

1

−

≥

exp(

δ).

−

(2.22)

3 Sparse Linear Regression

We intend to establish our results for sparse linear regression (Section 3) and low-rank ma-
trix completion (Section 4) under the uniﬁed framework of trace regression, which should be
established ﬁrst. Trace regression with Θ∗
Rd

d as desired signal is formulated as

×

∈
Xk, Θ∗

Yk =

+ ǫk,

(3.1)

where Xk
assumed to be (approximately) low-rank (e.g., [43, 65, 66])

d is covariate, ǫk is additive noise. To handle high-dimensional scaling, Θ∗ is

Rd

∈

×

(cid:11)

(cid:10)

d

σk(Θ∗)

q

|

≤

|

r, for some 0

q < 1,

≤

(3.2)

σd(Θ∗) are the singular values of Θ∗. For this low-rank trace
where σ1(Θ∗)
regression problem, a standard approach to estimate or reconstruct Θ∗ is via the M-estimator
(e.g., [67])

σ2(Θ∗)

≥

≥

≥

∈S
(Θ) is a loss function that requires

b

L

where
that promotes low-rankness.
framework to obtain convergence rate for trace regression when

knu is the penalty
{
In [65] Negahban and Wainwright ﬁrst established a general
(Θ) is a quadratic loss, and

(Xk, Yk)

Θ

k

}

,

b

Θ

arg min
Θ

∈

L

Θ

(Θ) + λ

knu,
Θ to ﬁt the data

k

(3.3)

L

13

Xk=1
...

then many subsequent papers developed and extended the theoretical framework, to name
a few, negative log-likelihood loss function [41], other estimation problems such as matrix
completion with sparse corruption [57] and sparse high-dimensional time series [6], extension
(Θ), a standard quadratic loss (i.e., ℓ2 loss)
to quaternion ﬁeld [28]. For data ﬁtting term
based on full data is

L

n

(Θ) =

L

1
2n

Yk − h

|

1
2

i |

Xk, Θ

2 =

vec(Θ)T ΣXXvec(Θ)

ΣY X, Θ
i

− h

+ constant,

Xk=1
n
k=1 vec(Xk)vec(Xk)T /n, ΣY X =

n
k=1 YkXk/n. However, this standard
where ΣXX =
quadratic loss does not directly apply to our setting where full data are not available. In order
to introduce some ﬂexibility, we consider a generalized quadratic loss

P

P

(Θ) =

L

1
2

vec(Θ)T Qvec(Θ)

B, Θ
i

,

− h

(3.4)

Rd

Rd2

d2

×

∈

is symmetric, B

d. We present a framework for trace regression in
where Q
Lemma 2. Note that Theorem 1 in [43] is only for Q, B in (3.4) being the (truncated) sample
covariance, hence Lemma 2 can be viewed as its extension to more general Q, B that suﬃces
for our needs. Besides, our version is reﬁned to be more technically amenable since a useful
relation (3.6) is established even without the restricted strong convexity (3.7). One shall see
that (3.6) can signiﬁcantly simplify the proofs for Theorem 9, 10, 11, 12.

∈

×

Lemma 2. Consider trace regression (3.1) with (approximate) low-rankness (3.2), the es-
timator is given by (3.3) where the loss function is a generalized quadratic loss (3.4). Let
∆ =

Θ − Θ∗. If Q is positive semi-deﬁnite, and λ satisﬁes

b

b

then it holds that

λ

2

k

≥

mat(Q

·

vec(Θ∗))

B

kop,

−

∆

knu ≤

k

1
2−q

10r

2−2q
2−q
F

.

∆

k

k

(3.5)

(3.6)

Moreover, if the restricted strong convexity (RSC) holds, i.e., there exists κ > 0 such that

b
∆)T Qvec(

vec(

b

∆)

κ
k

∆
k

2
F,

≥

then we have the convergence rate for Frobenius norm and nuclear norm

b

b

b

∆

kF ≤

k

30√r

b

q/2

1

−

λ
κ

(cid:16)

(cid:17)

and

∆

k

knu ≤

300r

b

q

1

−

.

λ
κ

(cid:16)

(cid:17)

With the preliminary of trace regression we now go into sparse linear regression

Yk = X T

k Θ∗ + ǫk,

where Θ∗
independent of Xk. In addition, Θ∗ is approximately sparse.

Rd is the desired signal, Xk is the covariate (or sensing vector), ǫk is noise

∈

14

(3.7)

(3.8)

(3.9)

Assumption 2. (Approximate sparsity on vector) For a speciﬁc 0
Θ∗ = [θ∗1, ..., θ∗d]T satisﬁes

d

q < 1, the desired signal

≤

q

θ∗i |

|

≤

s.

i=1
X

(3.10)

It is not hard to see that (3.9), (3.10) are encompassed by (3.1), (3.2) if Xk, Θ∗ are diagonal,
i.e., Xk = diag(Xk), Θ∗ = diag(Θ∗), so we consider analogue of (3.3) as the estimator. The
ﬁrst issue is the choice of loss function since the existing methods are invalid: we can neither
use the quadratic loss as [43,65] without full data, nor the negative log-likelihood as [41] due to
the noise ǫk with unknown distribution. Instead, we resort to a generalized quadratic loss given
BT Θ
in (3.4) to proceed. For sparse linear regression, particularly, we let
where Q

L
Rd. Thus, our estimator is given by

d is symmetric, B

2ΘT QΘ

(Θ) = 1

Rd

−

×

∈

∈

Θ

∈

arg min
Rd

Θ

∈

1
2

ΘT QΘ

BT Θ + λ

Θ

k1.

k

−

(3.11)

Lemma 2 implies the following Corollary.

b

Corollary 2. Consider linear regression (3.9) with (approximate) sparsity (3.10), the esti-
If Q is positive semi-deﬁnite, λ satisﬁes
Θ∗.
mator

Θ is given by (3.11). Let

∆ =

Θ

−

b

then it holds that

λ
b

2
b
k

≥

QΘ∗

B

kmax,

−

∆

k1 ≤

k

1
2−q

10s

2−2q
2−q
2

.

∆

k

k

Moreover, if for some κ > 0 we have the restricted strong convexity

b

∆T Q

∆

κ
k

≥

b
∆
k

2
2,

then we have the error bound for ℓ2 and ℓ1 norm

b

b

b

∆

k2 ≤

k

30√s

b

λ
κ

(cid:16)

q/2

1

−

(cid:17)

and

∆

k

k1 ≤

300s

b

q

1

−

λ
κ

(cid:16)

(cid:17)

(3.12)

(3.13)

(3.14)

(3.15)

It remains to properly specify Q, B in (3.11). Note that the expected quadratic risk is

given by

|

E|

X T

k Θ

2 = ΘT

Yk −

E(XkX T

k )Θ
thus a general guideline to choose Q, B is implied, that is, Q should be close to the covariance
matrix of Xk, and B should well approximate the covariance E(YkXk). Naturally, based on
Σ in (2.4) or ˘Σ in (2.1) as Q. Nevertheless, the issue is that they
one-bit data we can still use
may not be positive semi-deﬁnite, while the positive semi-deﬁniteness of Q is an indispensable
condition in Corollary 2.

(E(YkXk))T Θ + constant,

−

b

To close the gap, we assume ΣXX = EXkX T

k is column-wisely sparse.We mention that
such sparsity is quite common in high-dimensional regime, and it deﬁnitely accommodates
isotropic sensing vectors that is widely adopted in compressed sensing (See Remark 2).

15

Assumption 3. X1, ..., Xn are i.i.d. drawn from a zero-mean random vector with covariance
matrix ΣXX = EXkX T
k = [σij] satisfying Assumption 1 under parameter (0, s0), i.e., the
number of non-zero elements in each column is less than s0. Besides, ΣXX is positive deﬁnite,
and for some absolute constant κ0 > 0 it satisﬁes λmin(ΣXX)

2κ0.

≥

Under Assumption 3, our estimator

heavy-tailed data, is positive deﬁnite with high probability. Thus, we set Q =
b
Note that E(YkXk) is also covariance, enlightened by Corollary 1, we similarly set
b

n

Σ deﬁned in (2.4) for sub-Gaussian data, or (2.18) for
Σ in (3.11).

ΣY X =

Now we have speciﬁed our estimator as

b

1
n

γ2

·

Xk=1

˙Yk ˙Xk1.

Θ

∈

arg min
Rd

Θ

∈

1
2

ΘT

ΣΘ

ΣT

Y X Θ + λ

Θ

k1.

k

−

3.1 Sub-Gaussian Data

b

b

b

(3.16)

(3.17)

σ1, σ2}
max

k
(cid:8)

We assume the sub-Gaussian covariate and zero-mean sub-Gaussian noise satisfy
ǫkkψ2 ≤
σ2, and
k Θ∗
k
Xkkψ2 +
Θ∗
k2k
k
assume for some σ > 0

σ1,
ǫkkψ2 ≤
Ykkψ2 ≤ k
). To lighten notations without losing generality, we

Θ∗
k2 ≤
k
ǫkkψ2 = O(max
{

R = O(1). In this setting, we have

Xkkψ2 ≤
k

k
kψ2 +

X T

k

k

Xkkψ2,

Ykkψ2

k

≤

σ

(3.18)

−

and use the uniform noise with the same dithering scale γ to dither Xk and Yk before one-
γ, γ]d), Λk ∼
bit quantization. More precisely, we choose dithering noise Γk1, Γk2 ∼
−
˙Xk2,
γ, γ]) with γ in (2.6), then we obtain the one-bit data ( ˙Xk1,
uni([
Ykkψ2
Xkkψ2,
k
Xkkψ2 and
k
uni([γY , γY ]). In our numerical simula-

˙Yk).
We mention that our result directly extends to more general setting where
may vary a lot. Indeed, we can adaptively choose dithering scale according to
Ykkψ2, for instance, Γk1, Γk2 ∼
k
tions, we also applied diﬀerent dithering scales to Xk and Yk to improve the recovery.

γX , γX]d), Λk ∼
−

uni([

uni([

(cid:9)

k

k

−

ΣΘ∗

In Theorem 7 we will give the near minimax statistical rate for the estimator

Θ. The idea is
to invoke Corollary 2, and this requires (3.12) and (3.14). To properly set λ to conﬁrm (3.12),
ΣY Xkmax from above. Combining Assumption 3 and results in
it suﬃces to bound
Section 2, we can show (3.14) holds with high probability.
b

R for some absolute constant
Theorem 7. Assume (3.9), Assumption 2, (3.18) hold,
R, and we have Assumption 3 for the covariate Xk. Before the quantization we dither the data
with γ in (2.6). We consider
Σ,
ΣY X are respectively set as (2.4),
(3.16), and ζ is given by (2.9). Moreover, we choose λ by
b
δ log d
n

Θ given by (3.17) where

λ = C6 log n

k2 ≤

(3.19)

Θ∗

b

b

b

b

k

r

Θ∗. When (log n)2 log d/n is suﬃciently small, with

with suﬃciently large C6. Let
δ)
probability at least 1

exp(

−

−

−
δ, we have

q/2

1

−

(cid:17)
1
−

q

.

(3.20)

∆ =

Θ
2d2
−
−
b
b
k2 . √s
∆
k1 . s
∆
b

(cid:16)

k

k





b

σ2 log n

(cid:16)
σ2 log n

r

δ log d
n
δ log d
n

(cid:17)

r

16

k Xk/n, the proposed one-bit covari-
Remark 2. Compared with the sample covariance
ance matrix estimator
Σ lacks positive semi-deﬁniteness. We address the issue by assuming
column-wise sparsity of ΣXX , which together with λmin(ΣXX) = Ω(1) can provide positive def-
initeness under high-dimensional scaling. This assumption is also used in [91] to resolve the
same issue. As an example, this accommodates isotropic sensing vectors that is conventionally
adopted in compressed sensing literature, see [25, 38, 72] for instance.

P

b

n
k=1 X T

3.2 Heavy-tailed Data

4

∈

V T Xk|

V
k2 ≤
k
R4M1 + M2

We then switch to the heavy-tailed case where Xk and ǫk are only assumed to possess bounded
R = O(1). Moreover,
4-th moment. We consider the scaling of the desired signal as
Θ∗
k2 ≤
k
Rd,
4
M2. Then we have the
ǫk|
M1 for any V
1, and E|
we assume E|
≤
≤
fourth moment of Yk is also bounded by O
. To lighten the notation without
losing generality, we assume the same upper bound M for covariate and response:

(cid:0)
V T Xk|
which allows us to use the same truncation parameter η and dithering scale γ for Xk and Yk.
Similar to the same comment for sub-Gaussian case, if the fourth moment of Xk and Yk
have diﬀerent scales, our method still works under diﬀerent truncation parameters (ηX, ηY )
and dithering parameters (γX, γY ). Moreover, it is straightforward to adapt our method to the
mixing case studied in [43] where Xk is sub-Gaussian but ǫk (and hence Yk) is heavy-tailed. In
this mixing setting, only the responses are treated as heavy-tailed data and truncated before
dithering.

4, E|

sup
V

(3.21)

Yk|

max

M,

E|

(cid:1)
4

≤

2
k

(cid:8)

(cid:9)

≤

1

k

R for some absolute constant
Theorem 8. Assume (3.9), Assumption 2, (3.21) hold,
R, and we have Assumption 3 for the covariate Xk. By setting η, γ as (2.15) such that γ > η,
Yk) with parameter η, then dither the truncated
we ﬁrst truncate (Xk, Yk) element-wisely to (
˙Xk2, ˙Yk) ﬁnally. We consider
data with uniform noise on [
ΣY X is given in (3.16). Moreover,
Θ in (3.17) where
we choose
b

γ, γ] and quantize the data to ( ˙Xk1,
e

Σ is given in (2.18) with ζ set as (2.19),

k2 ≤

Xk,

Θ∗

−

1/4

e

b

b

k

λ = C7√M

(3.22)

with suﬃciently large C7. Let
exp(
at least 1

2d2
−

∆ =
Θ
−
d, we have

δ)

−

−

−

Θ∗. When log d/n is suﬃciently small, with probability

δ log d
n

(cid:16)

(cid:17)

b
b
k2 . √sM 1/2
∆
k1 . sM (1
∆
b

−

q/4

−

q)/2

δ log d
n
δ log d
(cid:16)
n

k


k


1/4

−

q/8

q)/4

(1
(cid:17)

−

.

(3.23)

b

(cid:16)
We emphasize that our method does not rely on the full knowledge of ΣXX , instead, our
method applies as long as ΣXX satisﬁes Assumption 3. In a variant setting where ΣXX is
known as a priori, we can directly set Q = ΣXX in (3.11), and the same error rates can be
obtained by the same techniques. Instead of giving the details, we only note two diﬀerences:
Σ);
Firstly, the sparsity in Assumption 3 can be removed (as we do not need the estimator
ΣY X does not involve ˙Xk2, see (3.16).
Secondly, only one bit per entry for Xk is suﬃcient since

(cid:17)

17

b

b

3.3 One-bit Compressed Sensing

1, 1

We just studied sparse linear regression based on the one-bit quantized covariates and responses
( ˙Xk1,
˙Xk2, ˙Yk), while the only related problem studied in existing works is one-bit compressed
sensing (1-bit CS). In 1-bit CS, one considers the same linear model (3.9) and wants to estimate
the sparse underlying signal Θ∗ based on (Xk, ˙Yk), where Xk denotes the full covariate, and
˙Yk ∈ {−
is the one-bit quantized version of the response Yk. In particular, earlier works
mainly studied a direct quantization with ˙Yk = sign(X T Θ∗) (see, e.g., [15, 50, 69, 70]), while
recent works (e.g., [5, 37, 38, 58, 83]) began to consider dithered quantization which are more
˙Yk = sign(X T Θ∗ + Λ) for some dithering noise Λ. Speciﬁcally, by the
related to our work, i.e.,
additional dithering step, these works overcome several limitations and present better results.
For instance, full reconstruction with norm [58], exponentially-decaying error rate [5], and
extension to non-Gaussian sensing vectors [37, 38, 83].

}

Since one still has full knowledge on Xk in 1-bit CS, our problem setting is novel and
evidently more tricky. From a practical viewpoint, due to the binary covariate, the storage
and communication costs are further lowered in our method. Technically, the key element that
allows quantization of covariate is the new 1-bit sparse covariance matrix estimator developed
in Section 2. To facilitate presentation and future study, we term this new setting as one-bit
quantized-covariate compressed sensing (1-bit QC-CS) to distinguish with the canonical 1-bit
CS. Thus, it is unfair to compare our Theorem 7, 8 with existing results for 1-bit CS.

Σ in (3.17) by the sample covariance matrix

To see the contributions of this paper more explicitly, we analogously establish results
for 1-bit CS where full-precision Xk are available, under both sub-Gaussian and heavy-tailed
regimes. Similar to (3.17), we formulate the estimation as a convex programming problem,
k /n for sub-
but substitute
Gaussian Xk, or the truncated sample covariance matrix
k /n for heavy-
tailed Xk. Here, for heavy-tailed case, we truncate Xk element-wisely, but we distinguish the
truncation threshold of Xk, Yk by diﬀerent notations ηX , ηY and they are set to be diﬀerent
values. More precisely, the i-th entry of
Xk is given by
, while
before the dithered quantization, Yk is truncated to be

n
k=1 XkX T
X T
Xk

Σ ˜X ˜X =
b

, ηX}
.

ΣXX =

n
k=1
P

P

b

b

e

Although the results are similarly established by the framework of trace regression, we feel
obliged to note some diﬀerences. Let us consider the sub-Gaussian regime for illustration.
Firstly, the column-wise sparsity of ΣXX in Assumption 3, whose main aim is to guarantee
positive semi-deﬁniteness of the one-bit covariance matrix estimator
Σ, can now be removed as
ΣXX is automatically positive semi-deﬁnite. But on the other hand, without this assumption,
ΣXX kop, hence the proof cannot
we no longer have a dimension-free upper bound on
b
ΣXXkmax,
ΣXX −
proceed to (B.11). Indeed, one only has dimension-free upper bound on
k1 = O(1) to close the gap (This is also assumed
and we need to impose a stronger scaling
in heavy-tailed case of sparse linear regression in [43], see Lemma 1(b) therein). Beyond that,
b
(B.9) is also missing, so we need to establish the restricted strong convexity (3.7) in Lemma
2 via some additional technicalities.

ΣXX −
b

Θ∗

b

k

k

k

e
Xk,i = sign(Xk,i) min
Yk = sign(Yk) min
e
e

{|

e
Xk,i|
, ηY }

{|
Yk|

In the next two theorems we present our results on 1-bit CS, which are directly compa-
rable to the vast literature of 1-bit CS. To facilitate the ﬂow of our presentation, a detailed
comparison is postponed to Appendix D. One shall see that, even without mentioning our ﬁrst
study of 1-bit QC-CS, the following two results concerning 1-bit CS already gain signiﬁcant
improvements upon existing works.

Theorem 9. (1-bit CS with sub-Gaussian data) Assume (3.9), Assumption 2, (3.18) hold (σ

18

R for some absolute constant R. For the zero-mean
in (3.18) is an absolute constant),
covariate Xk, deﬁne the covariance matrix ΣXX = EXkX T
2κ0
for some absolute constant κ0 > 0. We quantize Yk to be ˙Yk = sign(Yk + Λk) with Λk uniformly
γ, γ], and γ is set as (2.6) for speciﬁc δ > 0. The estimation is formulated
distributed on [
−
as a convex programming problem

k and we assume λmin(ΣXX)

k1 ≤

Θ∗

≥

k

(3.24)

(3.25)

Θ

∈

arg min
Rd

Θ

∈

1
2

ΘT

ΣXX Θ

ΣT

Y XΘ + λ

Θ

k1.

k

−

Moreover, we set

ΣXX,

b
ΣT
Y X and λ in (3.24) as

b

b

ΣXX =

b
XkX T
k ,

ΣY X =

1
n

n

b

Xk=1

1
n

n

Xk=1

γ

·

˙YKXk, λ = C8

δ log d log n
n

r

b

with some suﬃciently large C8. Let
with probability at least 1

7d2
−

−

b

Θ
∆ =
δ, we have
b
b
k2 . √s
∆
k1 . s
∆
b

(cid:16)r

(cid:16)r

k


k


Θ∗, then when s

−

δ log d
n

(cid:0)

(cid:1)

1

−

q/2 is suﬃciently small,

δ log d log n
n
δ log d log n
n

q/2

1

−

(cid:17)
1
−

q

.

(3.26)

By taking advantage of the full covariate and new technical tools, in heavy-tailed regime
, which is faster than the

s2/3

1/3

q/6

−

b

of 1-bit CS we show the ℓ2 norm error rate O
corresponding rate for 1-bit QC-CS in Theorem 8.
(cid:0)

(cid:0)

(cid:1)

(cid:1)

(cid:17)

log d
n

Theorem 10. (1-bit CS with heavy-tailed data) Assume (3.9), Assumption 2, (3.21) hold
R for some absolute constant R. For the zero-
(M in (3.21) is an absolute constant),
Θ∗
mean covariate Xk we let ΣXX = EXkX T
2κ0 for some absolute
Xk with threshold ηX , and truncate
constant κ0 > 0. We element-wisely truncate Xk to be
Yk to be
Yk + Λk)
γ, γ]. For speciﬁc δ > 0, we set these signal processing
with Λk uniformly distributed on [
parameters as

Yk is dithered and quantized to be ˙Yk = sign(

k1 ≤
k and assume λmin(ΣXX)

Yk with threshold ηY . Then,

−

≥

e

k

e

e

e

ηX = C9

n
δ log d

1
4

, ηY = C10

n
δ log d

1
6

, γ = C11

n
δ log d

1
6

,

(3.27)

(cid:16)
where C11 > C10 to give γ > ηY . The estimation is formulated as a convex programming
problem

(cid:16)

(cid:17)

(cid:17)

(cid:16)

(cid:17)

Θ

∈

arg min
Rd

Θ

∈

1
2

ΘT

Σ ˜X ˜XΘ

ΣT

Y XΘ + λ

Θ

k1.

k

−

Moreover, we set

Σ ˜X ˜X,

b
ΣY X and λ in (3.28) as

b

b

b
Σ ˜X ˜X =

b

1
n

n
b

Xk=1

Xk

X T
k ,

ΣY X =

e

e

b

1
n

n

Xk=1

19

˙Yk

γ

·

Xk, λ = C12

e

1/3

δ log d
n

(cid:17)

(cid:16)

(3.28)

(3.29)

with some suﬃciently large C12. Let
small, with probability at least 1

−
√δ), we have

∆ =

Θ

Θ∗, then when s2

δ log d
n

1

−

q/2 is suﬃciently

(cid:0)

(cid:1)

O(d2
−
b

−

k2 . s
∆
k1 . s
∆
b

k

k




2
3

4
3



b

b

δ log d
n
δ log d
n

(cid:16)

(cid:16)

1−q/2
3

1−q
3

(cid:17)

(cid:17)

.

(3.30)

To conclude this section, we brieﬂy state how to adjust Theorem 9, 10 to the situation
ΣXX in (3.24),
Σ ˜X ˜X in (3.28). Because some error terms vanish, and the restricted strong convexity holds

where ΣXX is known as a priori. Likewise, we use the known ΣXX to replace
or
automatically, the technical proofs of the error rates can be greatly shortened.

b

b

4 Low-rank Matrix Completion

Matrix completion refers to the problem of recovering a low-rank matrix with incomplete ob-
servations of the entries, which is motivated by recommendation system, system identiﬁcation,
quantum state tomography, image inpainting, and many others, see [7, 28–30, 33, 44, 47] for
instance. The literature can be roughly divided into two lines, exact recovery and approximate
recovery (i.e., statistical estimation). To establish exact recovery guarantee, the underlying
matrix is required to satisfy a quite stringent incoherence condition proposed and developed
in [24, 26, 27, 75]. By contrast, it was shown that matrix with low spikiness could be well
approximated (or estimated) under much more relaxed condition [55, 59, 66]. This Section is
intended to study the estimation problem of matrix completion via the binary data produced
by our one-bit quantization scheme. For simplicity we consider square matrix and formulate
the model as

Yk =
where Θ∗ is the underlying low-rank data matrix of interest, Xk distributed on
[d]
}
ǫk independent of Xk. We consider a random, uniform sampling scheme

∈
is the sampler that extracts one entry of Θ∗, Yk is the k-th observation corrupted by noise

j : i, j

(4.1)

eieT

+ ǫk

{

i

h

Xk, Θ∗

X1, ..., Xn are i.i.d. uniformly distributed on

eieT

j : i

{

[d], j

,

[d]
}

∈

∈

(4.2)

but we mention that the results can be directly adapted to non-uniform sampling scheme,
see [55]. To embrace more real applications, Θ∗ is assumed to be approximately low-rank [66].

Assumption 4. (Approximate low-rankness on matrix) Let σ1(Θ∗)
values of Θ∗, 0

q < 1. For some r > 0 it holds that

≤

d

Xk=1

σk(Θ∗)q

r.

≤

...

≥

≥

σd(Θ∗) be singular

(4.3)

Since Xk only has d2 values, we can use

bits to encode Xk without losing any
2 log2 d
⌈
information. Therefore, we only quantize Yk to binary data ˙Yk and study the estimation via
(Xk, ˙Yk). Based on previous experience in 1-bit QC-CS and 1-bit CS, we use a generalized

⌉

20

quadratic loss (3.4) with Q, B speciﬁed to be

Q =

1
n

n

Xk=1

vec(Xk)vec(Xk)T , B =

1
n

n

Xk=1

˙YkXk.

γ

·

(4.4)

max

Θ∗
The spikiness of Θ∗ is deﬁned to be d
in [66], note that completing a matrix with
k
k
Θ∗
k
k
high spikiness (close to d) with incomplete observations could be an ill-posed problem per
se [33], hence [66] assumed Θ∗ to have bounded spikiness. Besides the spikiness, a similar
but more straightforward assumption is a max-norm constraint (e.g., [23, 28, 32, 55]), which
intuitively excludes the appearance of overlarge entry and hence some pathological cases such
as Θ∗ = ei0eT
[d]. Here, we adopt this more direct condition and
assume

j0 for some (i0, j0)

[d]

×

∈

F

kmax ≤
Substitute (4.4), (3.4) into (3.3), together with the max-norm constraint (4.5), we now deﬁne
our estimator via the following convex programming problem

k

(4.5)

Θ∗

α∗.

Θ

∈

arg min
α∗
Θ

max

k

k

≤

= arg min
α∗

max

Θ

k

k

≤

1
2n

n

Xk=1

1
2

vec(Θ)T Qvec(Θ)

B, Θ
i

− h

+ λ

Θ

knu

k

Xk, Θ

γ

·

−

˙Yk

2 + λ

Θ

knu

k

(4.6)

(cid:0)(cid:10)
Compared with the program (3.17) involving
Σ used in sparse linear regression, (4.6) is more
intuitive since we simply replace the full observation Yk in a standard quadratic loss with its
one-bit surrogate γ
b

˙Yk. We draw this inspiration from Lemma 1.

(cid:11)

(cid:1)

Applying Lemma 2 to the problem set-up of low-rank matrix completion directly gives

Corollary 3, hence its proof is omitted.

Corollary 3. Consider (4.1) under random sampling (4.2), Θ∗ satisﬁes Assumption 4 and
(4.5). Consider

Θ in (4.6). Let

Θ − Θ∗. If

∆ =

b

·

b

then it holds that

λ

2

≥

n

b

h

Xk=1

(cid:2)

1
b
n

(cid:13)
(cid:13)

Xk, Θ∗

γ

·

i −

˙Yk

Xk

(cid:3)

op,
(cid:13)
(cid:13)

∆

knu ≤

k

1
2−q

10r

2−2q
2−q
F

.

∆

k

k

Moreover, if the RSC holds, i.e., for some κ > 0

then we have

∆

kF ≤

k

30√r

b

λ
κ

(cid:16)

(cid:17)

b
n

1
n

|

Xk=1

∆

b

|
(cid:11)

Xk,

(cid:10)

q/2

1

−

b

2

κ
k

∆
k

2
F,

≥

b

λ
κ

(cid:16)

q

1

−

.

(cid:17)

and

∆

knu ≤

k

300r

b

21

(4.7)

(4.8)

(4.9)

(4.10)

4.1 Sub-Gaussian noise

We ﬁrst cope with sub-Gaussian noise ǫk satisfying

Eǫk = 0,

(4.11)
k
Recall our dithered quantization scheme is formulated as ˙Yk = sign(Yk +Λk) with Λk uniformly
γ, γ]. To invoke Corollary 3 and obtain the statistical rate, we need to choose
distributed on [
suitable λ that guarantees (4.7) with high probability, and this requires an upper bound for
the right hand side of (4.7). For clarity, we present this part as the following Lemma.

ǫkkψ2 ≤

σ.

−

Lemma 3. Consider (4.1) under sampling scheme (4.2), max-norm constraint (4.5), and
sub-Gaussian noise assumption (4.11). For a speciﬁc δ > 1, we choose the dithering noise
scale γ by

with some suﬃciently large C13 such that γ
have

≥

γ = C13 max

α∗, σ

{

}

log

n
δd log(2d)

r
(cid:16)
α∗, σ
2 max
{

}

(cid:17)
. If δd log d
n

(4.12)

is suﬃciently small, we

n

1
n

δ log d
nd

Xk, Θ∗

˙Yk

Xk

·

h

(cid:3)

(cid:2)

γ

F

δ.

−

(cid:13)
(cid:13)

i −

Xk=1

log n

α∗, σ

}r

2d1
−

∆
k

(4.13)

Xk, Θ

n
k=1 |

X (Θ) = n−

with probability higher than 1

op . max
{
(cid:13)
(cid:13)
It remains to consider (4.9). To lighten the notation we use X = (X1, ..., Xn) to denote
1

the observed positions and deﬁne
≥
2
κ
F may not always hold under high-dimensional scaling and the special covariate (4.2).
P
k
In this case, one often needs to establish (4.9) with a relaxed term (i.e., tolerance function),
see Deﬁnition 2 in [67] for instance.

b
Based on this idea, in Theorem 1 of [66], Negahban and Wainwright ﬁrst established such
relaxed RSC over a constraint set. Later, in Lemma 12 of [55], Klopp considered a diﬀerent
constraint set and provided a reﬁned proof, but only for the exact low-rank setting, i.e., q = 0
in Assumption 4. More recently, in Lemma 5 of [28], Chen and Ng considered a constraint set
depending on q
[0, 1) and extended the proof in [55] to approximate low-rank regime. As a
consequence, a simpler and much shorter proof for the error bound in [66] could be obtained,
see more discussions in [28]. Here we show the relaxed RSC over the constraint set deﬁned
in [28], see

2. It is known that

(ψ) in (4.14).

X (Θ)

|
(cid:11)

F

∈

(cid:10)

C

Lemma 4. For a speciﬁc δ and suﬃciently large ψ, we consider the constraint set

(ψ) =

Θ

C

Rd

×

d :
k

∈

(cid:8)

k

Θ

kmax ≤
2
F ≥

Θ
k

2α∗,

k

Θ

(α∗d)2

r

10r

knu ≤
ψδ log(2d)
n

1
2−q

Θ
k

k

2−2q
2−q
F

,

.

(cid:9)
(0, 1), such that with probability at least 1

Then there exists some absolute constant κ
it holds that

∈

where the relaxation term T0 is given by

X (Θ)

F

2

κd−

Θ
k

2
F −

k

T0,

Θ

∀

∈ C

(ψ),

≥

T0 =

r
q)dq

−

(2

240α∗

(cid:16)

d log(2d)
n

r

q

2

−

.

(cid:17)

22

(4.14)

d−

δ,

−

(4.15)

(4.16)

Θ∗. The
We are now ready to derive the statistical bound of the estimation error
(ψ).
main idea is parallel to previous works [28, 55, 66], i.e., to discuss whether
Note that this only hinges on the third constraint in (4.14), since the ﬁrst two constraints are
automatically satisﬁed by
Theorem 11. Under the setting of Lemma 3, assume Θ∗ satisﬁes Assumption 4, we consider
the estimator

Θ
∆ =
∆ belongs to
b

Θ deﬁned in (4.6). Moreover, we set λ by

∆, see (4.5) and (4.8).

b
b

−

b

C

b

λ = C14 max

α∗, σ

{

log n

}r

δ log d
nd

(4.17)

is suﬃciently small, r & dq, n . d2 log(2d), then

with suﬃciently large C14, assume δd log d
with probability higher than 1

3d1
−

n

δ, we have

−
F/d2 . rd−
2

q

(α∗)2, σ2

max

∆
k
knu/d . rd−

k


k

Remark 3. Under a speciﬁc scaling
2
for the mean square error d−
F is equivalent to

α∗, σ

max

Θ∗

∆
b

(cid:16)

(cid:16)

b

{

k

{

2

q

}r

∆
k

k

q/2

1

−

log n

}

log n

δd log d
n
δd log d
n

(cid:17)

(cid:17)
1
−

q

.

(4.18)

kF = 1, Xk = d

eieT

j adopted in [43, 66], our bound

·

∗
α(Θ

)2, σ2

2
F . r
b

{

k

max

max

∆
k
Θ∗
where α(Θ∗) = d
[1, d] is the spikiness of the desired Θ∗. Compared with the full-data-
b
k
k
Θ∗
F ∈
k
k
based estimator in [66] that achieves near minimax rate, our one-bit estimator only degrades
by a minor factor log n, hence is also near minimax. It is quite striking that the underlying
matrix can be recovered fairly well based on the one-bit measurements produced by our uniformly
dithered quantization.

(cid:17)

(cid:16)

}

,

d log d log n
n

1

q/2

−

4.2 Heavy-tailed noise

The heavy-tailed noise is assumed to have bounded second moment in this part, i.e.,

2

M.

Eǫk = 0, E|

ǫk|
(4.19)
Although the same notation ˙Yk as sub-Gaussian case is used, note that the one-bit response
here is obtained with the truncation step before the dithered quantization. Speciﬁcally, Yk is
, η
Yk = sign(Yk) min
ﬁrst truncated to be
, then the dithered quantization is applied to
obtain ˙Yk = sign(
γ, γ]). The restricted strong convexity is already
Yk + Λk) where Λk ∼
established by Lemma 4, so it remains to upper bound the right hand side of (4.7) before
using Corollary 3.
e

Yk|
{|
uni([
−

≤

e

}

Lemma 5. Consider (4.1) under sampling scheme (4.2), max-norm constraint (4.5), and
heavy-tailed noise assumption (4.19). For a speciﬁc δ > 1, we set the truncation threshold η,
dithering scale γ as

η = C15 max

γ = C16 max






α∗, √M

{

α∗, √M

{

23

}

(cid:16)
}

(cid:16)

n
δd log d
n
δd log d

1/4

1/4

(cid:17)

,

(cid:17)

(4.20)

where C16 > C15, γ > 2 max
{

α∗, √M

. If δd log d

n

}

is suﬃciently small, we have

n

1
n

Xk=1

Xk, Θ∗
h

γ

·

i −

˙Yk

Xk

op . max

{

α∗, √M

1/4

δ log d
nd3

(cid:17)

}

(cid:16)

(4.21)

(cid:13)
(cid:13)
with probability higher than 1

(cid:2)

(cid:3)

(cid:13)
(cid:13)

2d1
−

δ.

−

Parallel to proof of Theorem 11, a discussion on whether

∆

(ψ) unfolds some key

relations that further lead to the desired error bounds. The result is given in Theorem 12.

Theorem 12. Under the setting of Lemma 5, assume Θ∗ satisﬁes Assumption 4, we consider
the estimator

Θ deﬁned in (4.6). Moreover, we set λ as

b

∈ C

b

λ = C17 max

α∗, √M

{

}

δ log d
nd3

h

i

1/4

(4.22)

is suﬃciently small, r & dq, then with probability at

with suﬃciently large C17. Assume δd log d
least 1

δ, we have

3d1
−

n

−

F/d2 . rd−
2

∆
k
knu/d . rd−
∆
b

b

(cid:0)

k

k






q

max

(α∗)2, M

{
α∗, √M

(cid:16)

q

max
{

δd log d
n
}r
δd log d
n

1/4
(cid:17)

q/2

1

−

}

h

i

(cid:1)

.

q

1

−

(4.23)

Again we can change the bound for mean square error

and Xk = d

·

eieT

j , then it reads

F/d2 to the scaling
2

∆
k

k

Θ∗

k

kF = 1

∆

kF ≤

k

√r

max

α∗, √M

{

}

(cid:16)

b
1/4

q/2

1

−

.

δd log d
n

h

i

(cid:17)

This is also consistent with previous two estimation problems where the error rates become
essentially slower.Such essential degradation is also due to a trade-oﬀ between bias and vari-
ance.

b

To close this section, we point out that our method of one-bit matrix completion is new
and essentially diﬀerent from the existing likelihood approach (see, e.g., [23, 32]). Notably,
our method can deal with unknown pre-quantization noise ǫk that can be sub-Gaussian or
heavy-tailed, while such unknown noise precludes the standard likelihood approach. A review
of previous works and more detailed comparison can be found in Appendix D.

5 An Overview of the Proofs

We give an overview of the proofs so that readers can quickly grasp our proof strategies. For
illustration we will focus on sub-Gaussian regime. While for heavy-tailed case, we will use
two examples to show the same technicalities, together with an optimal trade-oﬀ between
truncation threshold and dithering scale, can yield the presented results. Finally, we techni-
cally compare our work with [43], showing that their techniques cannot apply to our one-bit,
heavy-tailed setting.

24

In this section, we will consider the parameters (n, d, s, r, q) and treated other terms as

constants.

For sparse covariance matrix estimation, the element-wise error rate of ˘Σ in Theorem 1 is
= σ∗ij, E˘σij = σ∗ij may
> γ. Thus, we ﬁrst divide the element-wise error into

a fundamental element. Unlike the full data case where E
not hold due to the possibility of
a concentration term R1 and a bias term R2

Xk,iXk,j

Xk,i|

(cid:1)

(cid:0)

|

˘σij −

|

σ∗ij| ≤ |

˘σij − E˘σij|

+

|E˘σij −

σ∗ij|

:= R1 + R2.

Since the quantized data is bounded, a fast concentration rate for R1 is guaranteed by Ho-
eﬀding’s inequality, while R2 can be controlled by standard arguments. We strike a bal-
ance between R1, R2 by setting γ = O
, then the concentration term R1 =
log

n
log d

O

log d(log n)2
n

(cid:1)(cid:1)
dominates the error, hence the error bound only degrades by a factor log n

(cid:0)

(cid:0)q

compared with O

(cid:0)q
Recall that our estimator

log d
n

(cid:1)

for the full-data sample covariance matrix.

b

(cid:0)q

(cid:1)
procedures to show operator norm error rate of
the full-data-based hard thresholding estimator
σij −

Σ is deﬁned by element-wisely hard thresholding ˘Σ, and the
Σ are parallel to corresponding results for
In brief, some
Tζ
b
σ∗ij|

k /n
,
σ∗ij|
discussions unfold the element-wise rate
, which is
(cid:1)
{|
tighter than the bound for
. This tighter rate, together with the sparsity, can yield
a dimension-free bound for the dominating term of operator norm error. Despite a similar
proof strategy, we need more involved analyses to deal with some new challenges from the
data quantization. These additional eﬀorts, for example, can be seen in the treatment of R2
(A.8).

n
k=1 XkX T
min

log d(log n)2
n

˘σij −

in [20].

σ∗ij|

= O

(cid:0) P

q

b

}

(cid:1)

(cid:0)

|

|

For sparse linear regression (including 1-bit QC-CS, 1-bit CS) and matrix completion, we
derive the error rates for each problem based on Lemma 2, a framework of trace regression.
Compared with the key lemma (Theorem 1) in [43] , we present Lemma 2 in a more general
form that accommodates generalized quadratic loss (3.4), and the purpose is that more ﬂexible
Q, B constructed from the binary data can be used. The advantage of using such framework
is a rather clear proof roadmap constituted by two steps:

• Step 1. Bound

kop from above and choose λ that guarantee (3.5);
• Step 2. Establish the restricted strong convexity (3.7), and invoke (3.8) to obtain the

mat(Q

vec(Θ∗))

B

−

k

·

error rate.

We ﬁrst discuss Step 1.

some Q, B approximating ΣXX = EXkX T
ΣXX Θ∗ = ΣY X it can be divided as two approximation error terms

In sparse linear regression we need λ

kmax with
k , ΣY X = EYkXk, respectively. Thus, by noting

≥

−

B

QΘ∗

k

2

QΘ∗

k

B

−

kmax ≤ k

(Q

−

ΣXX )Θ∗

+

kmax

approximation term I

B

ΣY Xkmax

k
approximation term II

−

.

{z
One possibility to control the approximation error term is via existing results. For instance,
in 1-bit QC-CS we set Q to be the proposed sparse covariance matrix estimator
Σ. Thus,
the bound of term I follows from results in Section 2 (see, e.g., (B.11)). On the other hand,

{z

}

}

|

|

25

b

we can also adopt a standard strategy of bounding the concentration error and the deviation
(i.e., bias). For example, we can divide term II into (see, e.g., R2, R3 in (B.15))

B

k

ΣY X kmax ≤ k

B

− EB

kmax

−

+

B

−

kE

YkXk

.

kmax

concentration term II.1

(cid:0)

bias term II.2
(cid:1)

|

}

{z

|

{z

}

For matrix completion the methodology is similar, see (C.1) for example. We apply various
concentration inequalities to bound the concentration terms, to name a few, the basic concen-
tration of a sub-Gaussian variable (B.17), Bernstein’s inequality (B.16), matrix Bernstein’s
inequality (C.2).
In contrast, more standard tools like Cauchy-Schwarz, Markov’s inequal-
ity can upper bound bias terms. Let λfull denote the optimal choice of λ in the full-data
settings (see, e.g., [28, 43, 65, 66]). As it comes out, in the sub-Gaussian regime of our one-
bit setting, one can always strike an almost perfect balance among all the terms such that
mat(Q
λ = Poly(log n)

kop (3.5).
≥
Step 2 concerns the restricted strong convexity of Q with regard to

∆. Note that this
mainly hinges on the covariate. Thus, in 1-bit CS and matrix completion where full covariate
is available, we can directly borrow existing results from the full-data settings with no quantiza-
tion [28,43]. For 1-bit QC-CS with quantized Xk, the desired RSC property straightforwardly
follows from ΣXX’s sparsity and the resulting dimension-free bound of
ΣXX kop. Finally,
we apply (3.8) to obtain the error rates. Since λ = Poly(log n)
λfull suﬃces for Step 1, un-
der the dithered one-bit quantization scheme, the error rate at worst degrades by logarithmic
factor.

λfull can guarantee λ

vec(Θ∗))

B

Σ

−

−

b

b

k

k

2

·

·

·

In heavy-tailed regime we introduce truncation parameter η and require γ > η. The
strategies and technical tools for the proofs are almost the same as sub-Gaussian regime,
while the diﬀerence is that we can no longer strike a perfect balance among all terms.We
brieﬂy give two examples to demonstrate the proofs.

Example 1. (Theorem 4.) Similar to sub-Gaussian regime, the element-wise error in Theorem
4 is the basic of Theorem 5, 6. The error can be ﬁrst divided into two terms as

˘σij −

|

σ∗ij| ≤ |

˘σij − E˘σij|

+

|E(Xk,iXk,j −

Xk,i

Xk,j)

|

:= R1 + R2.

For the concentration term Hoeﬀding’s inequality gives the bound R1 = O
Cauchy-Schwarz inequality and the assumption of bounded 4-th moment, we show R2 = O
(cid:1)
for the bias term. Intuitively, larger γ brings larger data variance and hence larger R1, and
(cid:1)
smaller η corresponds to more biases, and hence larger R2. Recall that we need γ > η
to ensure the eﬀectiveness of dithering. Hence, we strike a balance between η, γ by setting

. By
1
η2

q

(cid:0)

(cid:0)

γ2

log d
n

e

e

η, γ

≍

n
log d

(cid:0)

(cid:1)

1/8, which leads to an overall error O

4

log d
n

.

q

(cid:0)

(cid:1)

Example 2. (Theorem 10.) To our best knowledge, Theorem 10 presents the ﬁrst com-
putationally eﬃcient method for 1-bit CS with heavy-tailed sensing vectors, and the rate

O

3

s2 log d
n

(cid:0)

q

sub-Gaussian regime. Let us start from Step 1 and ﬁrst decompose

(for s-sparse Θ∗) is still faster than the convex approach in [38], which is only for
ΣY X kmax into
b

Σ ˜X ˜XΘ∗

−

b

k

(cid:1)

26

△

four terms (see notations given in (3.29))

k

Σ ˜X ˜X Θ∗
(

ΣY Xkmax ≤ k
−
X T
Xk
Σ ˜X ˜X − E
k )Θ∗
b
b

≤
b

(

Σ ˜X ˜X −
+
max
b

ΣXX )Θ∗
XkX T

E

(cid:13)
(cid:13)
|

concentration term I.1
e
{z

(cid:13)
(cid:0)
(cid:13)
ΣY X − E(γ
}
(cid:13)
(cid:13)b
|
For two concentration terms, Bernstein’s inequality gives I.1 = O

·
(cid:13)
concentration term II.1
(cid:13)

(cid:13)
(cid:13)
γ
·

Xk)

(cid:13)
(cid:13)
|

(cid:13)
(cid:13)
|

˙Yk

{z

+

e

e

(cid:0)

}

ΣY X −
k
X T
Θ∗
k
b
(cid:1)

kmax +
Xk
k −
bias term I.2
e
e
+
{z
max

E

max

ΣY Xkmax := I + II

YkXk

˙Yk
}
bias term II.2

Xk −
e
{z
log d
n + η2

X

.

max

(cid:1)(cid:13)
(cid:13)
log d
n

}
and II.1 =

(cid:0)q

·

3

(cid:0)

(cid:0)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:0)

2

k

γ

O

≍

≥

≍

−

−

log d

log d

q

q

d−

1
η2
Y

1
η2
X

ηX ·
n

n + γ

n
log d

log d
(cid:1)
n

log d
(cid:0)
n

(cid:0)
q

Σ ˜X ˜XΘ∗

Σ ˜X ˜XΘ∗

. Hence, λ

1/4, ηY , γ

and II.2 = O

suﬃces for λ

(cid:1)
+ 1
η2
X

(cid:1)
(cid:0)
n
log d
3

with probability 1

Ω(1). For two bias terms, some probability
arguments and bounded 4-th moment can yield I.2 = O
. Recall
the heavy-tailed Yk would be quantized to one-bit, we require γ > ηY . To achieve an optimal
1/6, which gives an overall upper
trade-oﬀ among ηX , ηY , γ, we set ηX ≍
bound
−
k
ΣY Xkmax.
b
Σ ˜X ˜X also serves as a plug-in
b
estimator for sparse linear regression in [43], we can borrow their Lemma 2(b) and obtain an
Xk as data bounded by ηX and deal
error bound. It should be pointed out that if we treat
with I.1, II.1 via Hoeﬀding’s inequality, we can only establish an essentially slower error rate.
By contrast, Bernstein’s inequality enables us to make full use of Xk’s bounded 4-th moment
and yield tighter bound.

For Step 2, since the truncated sample covariance matrix

ΣY Xkmax = O
b

△
To close this section, we compare the heavy-tailed, full-data setting in [43] and our heavy-
tailed, one-bit quantized regime from a technical perspective. Although the techniques in [43]
can yield (near) minimax rates in sparse linear regression and matrix completion, we point
out that they are ineﬀective in our heavy-tailed regime. Indeed, their key technical ingredients
to avoid essential degradation heavily hinge on the moment constraint of the truncated data,
which is inherited from the original data. More precisely, the truncated response satisﬁes
l for any l > 0. In contrast, (except for Xk in Theorem 10) heavy-tailed data
E|
≤ E|
are further quantized to one-bit in our work, and the quantization evidently ruins the moment
l = γl. As a consequent, the technical elements in [43] become
constraint since E|
ineﬀective in our setting. We give sparse linear regression as a concrete example.

Yk|
e

˙Yk|

Yk|

e

b

b

γ

·

l

Example 3. (Comparing sparse linear regression in [43] and this work.) In the proof of Lemma
Xk −
1 in [43], Bernstein’s inequality is used to deal with the concentration term
e

Xkkmax. Thanks to the moment constraints of
e

Yk, they can show

n
k=1

Xk,

P

Yk

Yk

1
n

e

e

k

n

E

(5.1)

Yk

Xk

max = O

log d
e
n

+

ηX ηY log d
n

1
n

(cid:13)
(cid:13)

Yk

e

Xk − E
e

Xk=1

e

e

(cid:13)
(cid:13)

e
r
(cid:0)

with high probability. By contrast, in our Theorem 10 for 1-bit CS, the corresponding term is
˙Yk fails to inherit the moment constraint
the concentration term II.1 in Example 2. Since γ
from Yk, the same Bernstein’s inequality only delivers (see (B.21), (B.22))

·

(cid:1)

n

Xk=1

1
n

(cid:13)
(cid:13)

˙Yk

γ

·

Xk − Eγ
e

·

˙Yk

Xk

max = O

γ

(cid:13)
(cid:13)

e

27

(cid:16)

(cid:0)

log d
n

+

ηX log d
n

r

(cid:1)(cid:17)

(5.2)

d−

Ω(1), which is worse since γ becomes a common factor. Furthermore, in
with probability 1
˙Yk ˙Xk −Eγ2
our Theorem 8 for 1-bit QC-CS the corresponding concentration term is
·
˙Yk ˙Xkkmax. Note that both covariate and response are quantized and hence lose the moment
constraint, we directly invoke Hoeﬀding’s inequality and obtain (see (B.14), (A.12))

n
k=1

γ2
n

P

−

k

γ2
n

n

Xk=1

˙Yk ˙Xk − Eγ2

·

˙Yk ˙Xk

(cid:13)
(cid:13)

max = O
(cid:13)
(cid:13)

γ2

(cid:0)

r

log d
n

(cid:1)

(5.3)

with high probability. This is worse than both (5.1), (5.2), since γ2 appears as a leading
It shall be clear that γ or γ2 appearing as a multiplicative factor of
multiplicative factor.

log d
n

lead to essential degradation.

q

△
In matrix completion, similar issue arises in the application of matrix Bernstein’s inequality,
hence making the techniques developed in [43] useless in our one-bit quantized setting. It is
an open question whether our error rates in heavy-tailed regime can be improved.

6 Experimental Results

In this section we present experimental results on synthetic data that can corroborate and
demonstrate our theories. To facilitate the presentation ﬂow, the simulation details (e.g., the
underlying parameters, covariate) and the algorithms for the convex programming problems
are collected in Appendix E.

6.1 Sparse Covariance Matrix Estimation
In our simulation Σ∗ has exactly s-sparse columns (see Appendix E).

In sub-Gaussian regime, with high probability Theorem 3 delivers the error bound

Σ

k

−

Σ∗

kop . s

r

(log n)2 log d
n

.

(6.1)

b

Thus, the operator norm error is expected to only logarithmically depends on the ambient
dimension d, while essentially depend on the the sparsity s (that can be viewed as the intrinsic
dimension of the problem). We draw Xk from multivariate Gaussian distribution to verify the
theory. Speciﬁcally, we try (d, s) = (2500, 3), (2700, 3), (2900, 3), (2700, 9), and test the sample
size n = 900, 1200, 1500, 1800, 2100, 2400, 2700 for each (d, s). The (log-log) error curves

for all (d, s) are plotted on the left of Figure 1, with the theoretical curve O
also
provided for comparison of the error rate. Clearly, the curves with diﬀerent dimension d but
the same sparsity s are almost coincident, which conﬁrms the inessential dependency on d for
the error. On the other hand, the estimation error depends on s non-trivially since the curve
of s = 9 is obviously higher, which is consistent with (6.1). Moreover, the experimental curves
are roughly parallel to the red one, hence our estimator based on binary data exhibits a near
optimal minimax rate.

(cid:0)q

(cid:1)

(log n)2
n

In heavy-tailed regime, for Σ∗ with s-sparse columns Theorem 6 guarantees

Σ

k

−

Σ∗

kop . s

b

28

1/4

.

log d
n

(cid:17)

(cid:16)

(6.2)

The relation between estimation error and parameters s, d are similar to (6.1), while the con-
vergence rate becomes slower. In our simulations, heavy-tailed data are drawn from Student’s
t distribution. We test (d, s) = (2200, 3), (2400, 3), (2600, 3), (2400, 9) under sample size

n = 900, 1200, 1500, 1800, 2100, 2400.

(6.3)

We report the results in the right ﬁgure of Figure 1. Consistent with the error bound, three
curves with same s but diﬀerent d are fairly close, while larger s (s = 9) leads to essentially
larger error. Although our theoretical rate O
does not match the optimal rate in
the classical setting, these curves are well aligned with the theoretical curve, hence the rate
is experimentally veriﬁed. Furthermore, we test (d, s) = (2400, 9) with the truncation step
removed and then show the error curve with legend ”no truncation”. One shall see the
estimation error becomes worse without truncation. Therefore, truncation is not merely of
technical importance for proving an error bound, but can indeed improve the estimation in
heavy-tailed regime.

n−

1/4

(cid:1)

(cid:0)

-1.2

-1.4

-1.6

-1.8

-2

-0.6

-0.8

-1

-1.2

-1.4

7

7.5

8

6.8

7

7.2

7.4

7.6

7.8

Figure 1: Sparse covariance matrix estimation. Left: Sub-Gaussian; Right: Heavy-tailed.

6.2 Sparse Linear Regression

One-bit quantized-covariate compressed sensing (1-bit QC-CS). In 1-bit QC-CS, both
covariate Xk and response Yk are quantized to one-bit. Note that we use exactly sparse Θ∗,
hence in sub-Gaussian regime Theorem 6 delivers the guarantee

while for heavy-tailed regime the error bound in Theorem 7 reads as
b

Θ

k

−

Θ∗

k2 . log n

r

s log d
n

,

(6.4)

Θ

Θ∗

k2 . √s
With simulation details given in Appendix E, we try Θ∗ with (d, s) = (2400, 3), (2200, 6),
(2400, 6), (2600, 6) under the same size (6.3). The experimental results in sub-Gaussian regime,
heavy-tailed regime are shown as (log-log) curves on the left, the right of Figure 2, respectively.
We also plot the theoretical rates for comparison. To show the eﬃcacy of truncation in

(6.5)

−

(cid:16)

(cid:17)

b

k

.

1/4

log d
n

29

heavy-tailed regime, keeping other parameters unchanged, we test (d, s) = (2400, 3) without
truncation step. The errors are accordingly plot as a curve with legend ”no quantization”.

The results corroborate the theory from diﬀerent perspectives. Firstly, the curves with
the same s but diﬀerent d are extremely close, while the errors under s = 6 are signiﬁcantly
larger than s = 3. This veriﬁes (6.5) and (6.8) that exhibit non-trivial dependence on the
sparsity s but only logarithmic dependence on the ambient dimension d. Secondly, since the
experimental curves are fairly aligned with the red one, the theoretical convergence rates can
be veriﬁed. Moreover, comparing the curve of (d, s) = (2400, 3) and ”no quantization” on the
right of Figure 2, shrinking heavy-tailed data indeed brings better estimation of Θ∗.

One-bit compressed sensing (1-bit CS). Diﬀerent from the novel setting of 1-bit QC-CS,
in 1-bit CS one has full covariate Xk and only quantize Yk to one-bit. Under s-sparse Θ∗,
Theorem 9 gives the near minimax error bound for sub-Gaussian regime

Θ

k

−

Θ∗

k2 .

r

s log d log n
n

,

(6.6)

while Theorem 12 shows

b

1
3

s2 log d
n

k

b

(cid:16)

Θ

−

Θ∗

k2 .
for heavy-tailed regime. To verify the obtained error bounds, under sample size (6.3), in sub-
Gaussian regime we try (d, s) = (2400, 3), (2200, 9), (2400, 9), (2600, 9), while in heavy-tailed
regime we test (d, s) = (2400, 3), (2200, 6), (2400, 6), (2600, 6). For (d, s) = (2400, 3) with
heavy-tailed data, we also conduct an independent simulation with the truncation of Xk, Yk
removed but other conditions unchanged. The (log-log) error curves and the theoretical rates
are plotted in Figure 3. Similar to those illustrated in 1-bit QC-CS, several key implications
of Figure 3, such as dependence on s, d, can support and demonstrate our theoretical error
bounds (6.6), (6.7).

(6.7)

(cid:17)

-0.4

-0.6

-0.8

-1

-1.2

-0.2

-0.3

-0.4

-0.5

-0.6

-0.7

-0.8

6.8

7

7.2

7.4

7.6

7.8

6.8

7

7.2

7.4

7.6

7.8

Figure 2: 1-bit QC-CS. Left: Sub-Gaussian; Right: Heavy-tailed.

6.3 Low-rank Matrix Completion

While the error bounds in Theorem 11, 12 are stated with regard to k
, we ﬁrst adapt
them to our simulation details stated in Appendix E, i.e., the underlying (exactly) low-rank

b∆
2
F
k
d2

30

-0.2

-0.4

-0.6

-0.8

-1

-1.2

-0.6

-0.7

-0.8

-0.9

-1

-1.1

-1.2

-1.3

6.8

7

7.2

7.4

7.6

7.8

6.8

7

7.2

7.4

7.6

7.8

Figure 3: 1-bit CS. Left: Sub-Gaussian; Right: Heavy-tailed.

matrices have comparable spikiness α(Θ∗) and unit Frobenius norm, and the noise is moderate
compared with the signal. Thus, under sub-Gaussian ǫk, we translates MSE error bound in
(4.18) into the following via some calculations

and for heavy-tailed noise (4.23) delivers

b

Θ

k

−

Θ∗

kF .

rd log d log n
n

,

r

Θ

k

−

Θ∗

kF .

r2d log d
n

1/4

.

(6.8)

(6.9)

(cid:16)
To corroborate the theoretical error rates, we simulate the proposed one-bit matrix completion
method using Θ∗ with (d, r) = (100, 1), (100, 2), (120, 2), under the sample size n = 6000,
7000, 8000, 9000, 10000. In heavy-tailed regime, we also try (d, r) = (120, 2) with the response
truncation step removed. The experimental results are plotted as (log-log) error curves in
Figure 4.

(cid:17)

b

Clearly, in both sub-Gaussian regime (left ﬁgure) and heavy-tailed regime (right ﬁgure),
the errors signiﬁcantly increase when either r or d becomes larger. This corroborates the
implications of (6.8), (6.9) that the estimation error essentially hinges on r and d. Moreover,
the experimental curves are well aligned with the theoretical curve, hence the theoretical error
rates are observed. Comparing two black curves of (d, r) = (120, 2) and ”no quantization” in
the right ﬁgure, the truncation step seems do not bring notable improvement to the recovery of
Θ∗. This is perhaps because the the moderate noise
t(ν = 3) is used in the simulation,
thus making the bias-and-variance trade-oﬀ less important. On the other hand, we believe a
more signiﬁcant advantage of using the truncation step can be observed under severer noise.

1
250√3 ·

7 Concluding Remarks

In this paper we propose a uniformly dithered one-bit quantization scheme and apply it to three
high-dimensional statistical estimation problems, namely sparse covariance matrix estimation,
sparse linear regression, and matrix completion. Typical steps for the scheme are truncation,
dithering, and quantization. Here, truncation is only applied to heavy-tailed data for robust-
ness, and in dithering a uniformly distributed dithering noise is adopted. From the one-bit

31

-0.1

-0.2

-0.3

-0.4

-0.5

-0.6

-0.7

-0.1

-0.2

-0.3

-0.4

-0.5

-0.6

-0.7

8.7

8.8

8.9

9

9.1

9.2

8.7

8.8

8.9

9

9.1

9.2

Figure 4: Low-rank matrix completion. Left: Sub-Gaussian; Right: Heavy-tailed.

data, we propose new estimators that are computationally eﬃcient. Under high-dimensional
scaling, even only one or two bits per entry are collected, our estimators can recover the un-
derlying parameters fairly well. In sub-Gaussian regime, the proposed estimators achieve near
minimax rates, so the quantization costs very little to the recovery. In heavy-tailed regime,
the error rates do no match the minimax ones. This degradation is due to a trade-oﬀ between
bias and variance, see the demonstrations in Subsection 1.3, or the examples in Section 5.
However, these results either show essential advantages over comparable papers, or are the
ﬁrst results in such one-bit, heavy-tailed setting. It would be an interesting open question
whether the rates in heavy-tailed regime could be faster.

Our work makes considerable contributions to each of the three problems. Compared
with [36] that proposed a one-bit covariance matrix estimator ˘Σ, the results presented in
Section 2 can be viewed as a two-fold extension, that is, extension to high-dimensional scaling
(n < d) and to heavy-tailed distribution. We also highlight the technical contribution that
bounding higher order moment in (2.11) yields exponentially decaying probability tail in (2.12),
see Remark 1. In Section 3, we ﬁrst propose and study one-bit quantized-covariate compressed
sensing (1-bit QC-CS), see Theorem 7, 8. Corresponding results for one-bit compressed sensing
(1-bit CS) are also obtained, see Theorem 9, 10. Compared with the 1-bit CS literature,
our results exhibit signiﬁcant improvements from diﬀerent respects. Most prominently, the
sensing vector is not restricted to standard Gaussian, but instead it can be sub-Gaussian or
even heavy-tailed with unknown covariance matrix. Besides, some other beneﬁts include faster
rate (speciﬁcally, even the rate in Theorem 10 is faster than many existing results in 1-bit CS
with dithered quantization) and computational feasibility, see the comparison in Appendix D.
In Section 4, while all existing papers for one-bit matrix completion (1-bit MC) are in essence
based on maximum likelihood estimation, we provide a novel approach that uses a generalized
quadratic loss for data ﬁtting. Notably, our new method can handle unknown pre-quantization
random noise that precludes the likelihood approach. See a comparison in Appendix D.

Besides the possible improvement of the rates in heavy-tailed regime, there are some other
interesting questions for future research. While most assumptions in our work is quite general
(e.g., sensing vectors, noise distribution), the dithering noise is restricted to be uniformly
distributed. Thus, one may consider the extension to non-uniform dithering noise. This
extension seems a bit challenging since the foundational element Lemma 1 heavily relies on a
uniformly distributed Λ. Besides, we only consider pre-quantization noise ǫk in sparse linear
regression and matrix completion. Naturally, one can explore the robustness of our estimators

32

under other types of corruption like post-quantization noise (i.e., sign-ﬂipping). Finally, we
point out a more practical aspect. We report that our method may not yield satisfactory
estimation in 1-bit MC when entries of Θ∗ vary a lot in magnitude, which is probably because
a common γ can not well preserve the information. It would also be a good question how to
make the proposed method more practical for real applications.

References

[1] Albert Ai, Alex Lapanowski, Yaniv Plan, and Roman Vershynin. One-bit compressed
sensing with non-gaussian measurements. Linear Algebra and its Applications, 441:222–
239, 2014.

[2] Tuncer C Aysal and Kenneth E Barner. Second-order heavy-tailed distributions and tail

analysis. IEEE transactions on signal processing, 54(7):2827–2832, 2006.

[3] Youhui Bai, Cheng Li, Quan Zhou, Jun Yi, Ping Gong, Feng Yan, Ruichuan Chen, and
Yinlong Xu. Gradient compression supercharged high-performance data parallel dnn
In Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems
training.
Principles, pages 359–375, 2021.

[4] Ofer Bar-Shalom and Anthony J Weiss. Doa estimation using one-bit quantized measure-
ments. IEEE Transactions on Aerospace and Electronic Systems, 38(3):868–884, 2002.

[5] Richard G Baraniuk, Simon Foucart, Deanna Needell, Yaniv Plan, and Mary Wootters.
Exponential decay of reconstruction error from binary measurements of sparse signals.
IEEE Transactions on Information Theory, 63(6):3368–3385, 2017.

[6] Sumanta Basu and George Michailidis. Regularized estimation in sparse high-dimensional

time series models. The Annals of Statistics, 43(4):1535–1567, 2015.

[7] James Bennett, Stan Lanning, et al. The netﬂix prize. In Proceedings of KDD cup and

workshop, volume 2007, page 35. New York, NY, USA., 2007.

[8] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anand-
In International

signsgd: Compressed optimisation for non-convex problems.

kumar.
Conference on Machine Learning, pages 560–569. PMLR, 2018.

[9] Sonia A Bhaskar. Probabilistic low-rank matrix recovery from quantized measurements:
Application to image denoising. In 2015 49th Asilomar Conference on Signals, Systems
and Computers, pages 541–545. IEEE, 2015.

[10] Sonia A Bhaskar. Probabilistic low-rank matrix completion from quantized measure-

ments. The Journal of Machine Learning Research, 17(1):2131–2164, 2016.

[11] Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The

Annals of Statistics, 36(6):2577–2604, 2008.

[12] Jacob Bien and Robert J Tibshirani.

Sparse estimation of a covariance matrix.

Biometrika, 98(4):807–820, 2011.

33

[13] Atanu Biswas, Sujay Datta, Jason P Fine, and Mark R Segal. Statistical advances in the
biomedical sciences: clinical trials, epidemiology, survival analysis, and bioinformatics,
volume 630. John Wiley & Sons, 2007.

[14] St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration inequalities: A

nonasymptotic theory of independence. Oxford university press, 2013.

[15] Petros T Boufounos and Richard G Baraniuk. 1-bit compressive sensing. In 2008 42nd
Annual Conference on Information Sciences and Systems, pages 16–21. IEEE, 2008.

[16] Petros T Boufounos, Laurent Jacques, Felix Krahmer, and Rayan Saab. Quantization
In Compressed sensing and its applications, pages 193–237.

and compressive sensing.
Springer, 2015.

[17] Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learn-

ing via the alternating direction method of multipliers. Now Publishers Inc, 2011.

[18] T Tony Cai, Zhao Ren, and Harrison H Zhou. Optimal rates of convergence for estimating
toeplitz covariance matrices. Probability Theory and Related Fields, 156(1-2):101–143,
2013.

[19] T Tony Cai, Cun-Hui Zhang, and Harrison H Zhou. Optimal rates of convergence for

covariance matrix estimation. The Annals of Statistics, 38(4):2118–2144, 2010.

[20] T Tony Cai and Harrison H Zhou. Minimax estimation of large covariance matrices under

ℓ1-norm. Statistica Sinica, pages 1319–1349, 2012.

[21] T Tony Cai and Harrison H Zhou. Optimal rates of convergence for sparse covariance

matrix estimation. The Annals of Statistics, 40(5):2389–2420, 2012.

[22] Tony Cai and Weidong Liu. Adaptive thresholding for sparse covariance matrix estima-

tion. Journal of the American Statistical Association, 106(494):672–684, 2011.

[23] Tony Cai and Wen-Xin Zhou. A max-norm constrained minimization approach to 1-bit

matrix completion. J. Mach. Learn. Res., 14(1):3619–3647, 2013.

[24] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the

IEEE, 98(6):925–936, 2010.

[25] Emmanuel J Candes and Yaniv Plan. A probabilistic and ripless theory of compressed

sensing. IEEE transactions on information theory, 57(11):7235–7254, 2011.

[26] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimiza-

tion. Foundations of Computational mathematics, 9(6):717–772, 2009.

[27] Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal

matrix completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.

[28] Junren Chen and Michael K Ng. Color image inpainting via robust pure quaternion
matrix completion: Error bound and weighted loss. SIAM Journal on Imaging Sciences,
to appear.

34

[29] Yongyong Chen, Xiaolin Xiao, and Yicong Zhou. Low-rank quaternion approximation for
color image processing. IEEE Transactions on Image Processing, 29:1426–1439, 2019.

[30] Yudong Chen and Yuejie Chi. Harnessing structures in big data via guaranteed low-
rank matrix estimation: Recent theory and fast algorithms via convex and nonconvex
optimization. IEEE Signal Processing Magazine, 35(4):14–31, 2018.

[31] Junil Choi, Jianhua Mo, and Robert W Heath. Near maximum-likelihood detector and
channel estimator for uplink multiuser massive mimo systems with one-bit adcs. IEEE
Transactions on Communications, 64(5):2005–2018, 2016.

[32] Mark A Davenport, Yaniv Plan, Ewout Van Den Berg, and Mary Wootters. 1-bit matrix
completion. Information and Inference: A Journal of the IMA, 3(3):189–223, 2014.

[33] Mark A Davenport and Justin Romberg. An overview of low-rank matrix recovery from
incomplete observations. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–
622, 2016.

[34] Oliver De Candido, Hela Jedda, Amine Mezghani, A Lee Swindlehurst, and Josef A
Nossek. Reconsidering linear transmit signal processing in 1-bit quantized multi-user
miso systems. IEEE Transactions on Wireless Communications, 18(1):254–267, 2018.

[35] Sjoerd Dirksen. Quantized compressed sensing: a survey. In Compressed Sensing and Its

Applications, pages 67–95. Springer, 2019.

[36] Sjoerd Dirksen, Johannes Maly, and Holger Rauhut. Covariance estimation under one-bit

quantization. arXiv preprint arXiv:2104.01280, 2021.

[37] Sjoerd Dirksen and Shahar Mendelson. Robust one-bit compressed sensing with partial

circulant matrices. arXiv preprint arXiv:1812.06719, 2018.

[38] Sjoerd Dirksen and Shahar Mendelson. Non-gaussian hyperplane tessellations and robust
one-bit compressed sensing. Journal of the European Mathematical Society, 23(9):2913–
2947, 2021.

[39] Bradley Efron. Large-scale inference: empirical Bayes methods for estimation, testing,

and prediction, volume 1. Cambridge University Press, 2012.

[40] Noureddine El Karoui. Operator norm consistent estimation of large-dimensional sparse

covariance matrices. The Annals of Statistics, 36(6):2717–2756, 2008.

[41] Jianqing Fan, Wenyan Gong, and Ziwei Zhu. Generalized high-dimensional trace regres-
sion via nuclear norm regularization. Journal of econometrics, 212(1):177–202, 2019.

[42] Jianqing Fan, Kaizheng Wang, Yiqiao Zhong, and Ziwei Zhu. Robust high-dimensional
Statistical Science,

factor models with applications to statistical machine learning.
36(2):303–327, 2021.

[43] Jianqing Fan, Weichen Wang, and Ziwei Zhu. A shrinkage principle for heavy-tailed data:
High-dimensional robust low-rank matrix recovery. Annals of statistics, 49(3):1239, 2021.

35

[44] Maryam Fazel, Haitham Hindi, and Stephen P Boyd. Log-det heuristic for matrix rank
minimization with applications to hankel and euclidean distance matrices. In Proceedings
of the 2003 American Control Conference, 2003., volume 3, pages 2156–2162. IEEE, 2003.

[45] Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear
variational problems via ﬁnite element approximation. Computers & mathematics with
applications, 2(1):17–40, 1976.

[46] Pengzhi Gao, Ren Wang, Meng Wang, and Joe H Chow. Low-rank matrix recovery from
noisy, quantized, and erroneous measurements. IEEE Transactions on Signal Processing,
66(11):2918–2932, 2018.

[47] David Gross, Yi-Kai Liu, Steven T Flammia, Stephen Becker, and Jens Eisert. Quantum
state tomography via compressed sensing. Physical review letters, 105(15):150401, 2010.

[48] Lijie Hu, Shuo Ni, Hanshen Xiao, and Di Wang. High dimensional diﬀerentially private
stochastic optimization with heavy-tailed data. In Proceedings of the 41st ACM SIGMOD-
SIGACT-SIGAI Symposium on Principles of Database Systems, pages 227–236, 2022.

[49] Marat Ibragimov, Rustam Ibragimov, and Johan Walden. Heavy-tailed distributions and

robustness in economics and ﬁnance, volume 214. Springer, 2015.

[50] Laurent Jacques, Jason N Laska, Petros T Boufounos, and Richard G Baraniuk. Robust 1-
bit compressive sensing via binary stable embeddings of sparse vectors. IEEE transactions
on information theory, 59(4):2082–2102, 2013.

[51] Iain M Johnstone. On the distribution of the largest eigenvalue in principal components

analysis. Annals of statistics, pages 295–327, 2001.

[52] Yuan Ke, Stanislav Minsker, Zhao Ren, Qiang Sun, and Wen-Xin Zhou. User-friendly
covariance estimation for heavy-tailed distributions. Statistical Science, 34(3):454–471,
2019.

[53] Shahin Khobahi, Naveed Naimipour, Mojtaba Soltanalian, and Yonina C Eldar. Deep
signal recovery with one-bit quantization. In ICASSP 2019-2019 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP), pages 2987–2991. IEEE,
2019.

[54] Alon Kipnis, Yonina C Eldar, and Andrea J Goldsmith. Fundamental distortion limits of
analog-to-digital compression. IEEE Transactions on Information Theory, 64(9):6013–
6033, 2018.

[55] Olga Klopp. Noisy low-rank matrix completion with general sampling distribution.

Bernoulli, 20(1):282–303, 2014.

[56] Olga Klopp, Jean Lafond, ´Eric Moulines, and Joseph Salmon. Adaptive multinomial

matrix completion. Electronic Journal of Statistics, 9(2):2950–2975, 2015.

[57] Olga Klopp, Karim Lounici, and Alexandre B Tsybakov. Robust matrix completion.

Probability Theory and Related Fields, 169(1):523–564, 2017.

36

[58] Karin Knudson, Rayan Saab, and Rachel Ward. One-bit compressive sensing with norm

estimation. IEEE Transactions on Information Theory, 62(5):2748–2758, 2016.

[59] Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penal-
ization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics,
39(5):2302–2329, 2011.

[60] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
eﬃciency. arXiv preprint arXiv:1610.05492, 2016.

[61] Piotr Kruczek, Rados law Zimroz, and Agnieszka Wy loma´nska. How to detect the cyclo-
stationarity in heavy-tailed distributed signals. Signal Processing, 172:107514, 2020.

[62] Jean Lafond, Olga Klopp, Eric Moulines, and Joseph Salmon. Probabilistic low-rank ma-
trix completion on ﬁnite alphabets. Advances in Neural Information Processing Systems,
27, 2014.

[63] Ming Li, Wei Zhao, and Biao Chen. Heavy-tailed prediction error: A diﬃculty in pre-
dicting biomedical signals of noise type. Computational and Mathematical Methods in
Medicine, 2012, 2012.

[64] Jianhua Mo and Robert W Heath. Limited feedback in single and multi-user mimo sys-
tems with ﬁnite-bit adcs. IEEE Transactions on Wireless Communications, 17(5):3284–
3297, 2018.

[65] Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with
noise and high-dimensional scaling. The Annals of Statistics, pages 1069–1097, 2011.

[66] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted
matrix completion: Optimal bounds with noise. The Journal of Machine Learning Re-
search, 13(1):1665–1697, 2012.

[67] Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A uniﬁed
framework for high-dimensional analysis of m-estimators with decomposable regularizers.
Statistical science, 27(4):538–557, 2012.

[68] Renkun Ni and Quanquan Gu. Optimal statistical and computational rates for one bit
matrix completion. In Artiﬁcial Intelligence and Statistics, pages 426–434. PMLR, 2016.

[69] Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic
regression: A convex programming approach. IEEE Transactions on Information Theory,
59(1):482–494, 2012.

[70] Yaniv Plan and Roman Vershynin. One-bit compressed sensing by linear programming.

Communications on Pure and Applied Mathematics, 66(8):1275–1297, 2013.

[71] Yaniv Plan and Roman Vershynin. The generalized lasso with non-linear observations.

IEEE Transactions on information theory, 62(3):1528–1537, 2016.

[72] Yaniv Plan, Roman Vershynin, and Elena Yudovina. High-dimensional estimation with

37

geometric constraints. Information and Inference: A Journal of the IMA, 6(1):1–40, 2017.

[73] Mohsen Pourahmadi. High-dimensional covariance estimation: with high-dimensional

data, volume 882. John Wiley & Sons, 2013.

[74] Jim O Ramsey and Bernard W Silverman. Functional data analysis. Springer Series in

Statistics, New York: Springer Verlag, 2005.

[75] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning

Research, 12(12), 2011.

[76] Phillippe Rigollet and Jan-Christian H¨utter. High dimensional statistics. Lecture notes

for course 18S997, 813:814, 2015.

[77] Kilian Roth, Jawad Munir, Amine Mezghani, and Josef A Nossek. Covariance based
In 2015 IEEE International

signal parameter estimation of coarse quantized signals.
Conference on Digital Signal Processing (DSP), pages 19–23. IEEE, 2015.

[78] Philipp R¨utimann and Peter B¨uhlmann. High dimensional sparse covariance estimation

via directed acyclic graphs. Electronic Journal of Statistics, 3:1133–1160, 2009.

[79] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradi-
ent descent and its application to data-parallel distributed training of speech dnns. In
Fifteenth Annual Conference of the International Speech Communication Association.
Citeseer, 2014.

[80] Jie Shen, Pranjal Awasthi, and Ping Li. Robust matrix completion from quantized ob-
servations. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics,
pages 397–407. PMLR, 2019.

[81] Qiang Sun, Wen-Xin Zhou, and Jianqing Fan. Adaptive huber regression. Journal of the

American Statistical Association, 115(529):254–265, 2020.

[82] Ananthram Swami and Brian M Sadler. On some detection and estimation problems in

heavy-tailed noise. Signal Processing, 82(12):1829–1846, 2002.

[83] Christos Thrampoulidis and Ankit Singh Rawat. The generalized lasso for sub-gaussian
measurements with dithered quantization. IEEE Transactions on Information Theory,
66(4):2487–2500, 2020.

[84] Joel A Tropp. An introduction to matrix concentration inequalities. arXiv preprint

arXiv:1501.01571, 2015.

[85] Sara A Van de Geer. Estimation and testing under sparsity. Springer, 2016.

[86] Shay Vargaftik, Ran Ben-Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak, and
Michael Mitzenmacher. Drive: One-bit distributed mean estimation. Advances in Neural
Information Processing Systems, 34, 2021.

[87] Roman Vershynin. High-dimensional probability: An introduction with applications in

data science, volume 47. Cambridge university press, 2018.

38

[88] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, vol-

ume 48. Cambridge University Press, 2019.

[89] Di Wang and Ruey S Tsay. Robust estimation of high-dimensional vector autoregressive

models. arXiv preprint arXiv:2107.11002, 2021.

[90] Robert F Woolson and William R Clarke. Statistical methods for the analysis of biomedical

data, volume 371. John Wiley & Sons, 2011.

[91] Eunho Yang, Aur´elie C Lozano, and Pradeep K Ravikumar. Closed-form estimators for
high-dimensional generalized linear models. Advances in Neural Information Processing
Systems, 28, 2015.

[92] Ziwei Zhu and Wenjing Zhou. Taming heavy-tailed features by shrinkage. In International
Conference on Artiﬁcial Intelligence and Statistics, pages 3268–3276. PMLR, 2021.

A Proofs: Sparse Covariance Matrix Estimation

Proof of Lemma 1. Since X and Λ are independent, we have

γ

E

sign(X + Λ)

= EX EΛ

γ

sign(X + Λ)

= EX

γ

·

h
= EX

γ

h

·

(cid:16)

i
γ + X
2γ −

γ

X

−
2γ

h

(cid:17)i

·

= EX.

note that the third equal sign relies on γ

B.

≥

i

h

· P(Λ

X) + (

≥ −

γ)

−

· P(Λ <

X)

−

i

(cid:3)

Proof of Corollary 1. Since Λ1 and Λ2 are i.i.d. uniformly distributed on [
independent of X, Y , then by using Lemma 1 we have

γ, γ] and

−

γ2

E

sign(X + Λ1)sign(Y + Λ2)

= EX,Y EΛ1 EΛ2

γ2

sign(X + Λ1)sign(Y + Λ2)

·

·

h
= EX,Y

EΛ1

γ

the result follows.

(cid:16)

h

sign(X + Λ1)

·

i
EΛ2

γ

i

h

·

sign(Y + Λ2)

h

= EXY,

i(cid:17)

i

(cid:3)

A.1 Sub-Gaussian Data

Proof of Theorem 1. For ﬁxed i, j, triangle inequality yields

˘σij −

|

σ∗ij| ≤ |

˘σij − E˘σij|

+

|E˘σij −

σ∗ij|

:= R1 + R2.

(A.1)

It suﬃces to bound R1, R2 from above.
Bound of R1. We introduce the element-wise notation of the quantized data as
[ ˙Xkj,1,
Since

[2], then by (2.1) ˘σij = 1
n
γ2, Hoeﬀding’s inequality (Proposition 3) yields

˙Xk1,i ˙Xk2,j + ˙Xk2,i ˙Xk1,j

˙Xkj =
˙Xk1,i ˙Xk2,j+ ˙Xk2,i ˙Xk1,j

˙Xkj,d]T ,

˙Xkj,2, ...,
γ2
2

[n], j

n
k=1

γ2
2

∈

∈

∀

k

(cid:2)

P

.

(cid:3)

(cid:2)

(cid:12)
(cid:12)
(cid:12)

P(

|

(cid:3)(cid:12)
(cid:12)
˘σij − E˘σij| ≥
(cid:12)

≤

2 exp(

t)

≤

nt2/2γ4),

−

t > 0.

∀

39

We set t = γ2

2δ log d
n

and obtain

q

P

γ2

R1 ≥

r

2δ log d
n

(cid:16)

(cid:17)

2d−

δ.

≤

(A.2)

Bound of R2. By Corollary 1 and some algebra, we have

Xk,iXk,j

E

˙Xk1,i ˙Xk2,j −

R2 =

=

γ2
·
E[γ2 ˙Xk1,i ˙Xk2,j −
(cid:12)
(cid:12)
(cid:12)
Xk,i|
Xk,iXk,j|1(
(cid:12)
≤ E|
(cid:12)

(cid:0)

|

Xk,iXk,j][1(
> γ) + E|

Xk,i| ≥
γ
(cid:1)(cid:12)
(cid:12)
{|
Xk,iXk,j|1(

} ∪ {|
Xk,j|

|

)]

> γ

Xk,j|
}
> γ) := R21 + R22.

(cid:12)
(cid:12)
(cid:12)

Note that R21, R22 can be bounded likewise, thus we only show the upper bound of R21. We
use Cauchy-Schwarz inequality, and then Proposition 1, it yields

Xk,iXk,j|

2

P(

Xk,i|

|

> γ)

R21 ≤

E|

q

. √σ4

exp

· r

·
q
D1γ2
σ2

(cid:1)

−

(cid:0)

σ2 exp

≤

≤ r
D1γ2
2σ2

1
2 E(

.

(cid:1)

−

(cid:0)

4 +

Xk,i|

|

Xk,j|

|

4)

·

P(

Xk,i|

|

> γ)

q

We further plug in (2.6) and assume C1 is suﬃciently large such that D1C 2
R21 . σ2

. Therefore, we conclude that

1 ≥

2δ log d
n

q

R2 =

|E˘σij −

σ∗ij|

. σ2

2δ log d
n

.

r

1, it delivers

(A.3)

Combining (A.2) and (A.3) we derive P

2dδ. With no loss
˘σij −
|
of generality, we can assume 2δ log d > e, then γ2 . σ2 log n, then (2.7) follows. It is not hard
(cid:16)
(cid:3)
to see that (2.8) follows from (2.7) via a union bound.

σ∗ij|

. γ2

q

≥

−

(cid:17)

1

δ log d
n

Proof of Theorem 2. Since γ has been speciﬁed with some C1, from Theorem 1 we know
there exists an absolute constant D1 such that

˘σij −

|

σ∗ij| ≤

P

(cid:16)

D1σ2 log n

r

δ log d
n

(cid:17)

1

−

≥

2d−

δ.

(A.4)

δ probability and

Assume C2 is suﬃciently large such that C2 > D1. We ﬁrst rule out 2d−
assume
Case 1.

n . Recall that

D1σ2 log n

σij =

δ log d

˘σij −
|
˘σij|
|

σ∗ij| ≤
< ζ, then by deﬁnition we have
σ∗ij −

q
σ∗ij| ≤ |

|

by triangle inequality we have
have

σij = 0, hence
b
˘σij| ≤
˘σij|
b

+

|

Tζ(˘σij), we analyse two cases.
=
|

σ∗ij|
(D1 + C2)σ2 log n

σ∗ij| ≤ |

σij −

. Besides,

σ∗ij|
n , hence we

δ log d

|

b

σij −

|

σ∗ij| ≤

Case 2.

˘σij| ≥

|

ζ, then we have

b

Moreover, since C2 > D1, we have
b

σ∗ij|

(D1 + C2 + 1) min

|
n
σij −
σij = ˘σij, hence
|
˘σij −
˘σij| − |
σ∗ij| ≥ |
b

|

, σ2 log n

r
˘σij −
|
(C2 −

=

σ∗ij|
σ∗ij| ≥

q

.

δ log d
n

o
σ∗ij| ≤
D1)σ2 log n

D1σ2 log n

δ log d
n .

δ log d

q

n , which

q

40

implies that σ2 log n
pieces together we obtain

q

δ log d

n ≤

1

C2

D1 |

−

σ∗ij|

, hence we have

σij −

|

σ∗ij| ≤

D1

C2

D1 |

−

σ∗ij|

. By putting

σij −

|

σ∗ij| ≤

D1 +

r
Combining two cases leads to (2.10), hence the proof is concluded.

(cid:16)

(cid:17)

b

D1
C2 −

D1

min

σ∗ij|

|
n

b

, σ2 log n

δ log d
n

.

o

(cid:3)

Proof of Theorem 3. Since γ and ζ are properly set with some C1, C2, by Theorem 2, (2.10)
holds with some absolute constant D1 hidden behind ”.”. For convenience we deﬁne

Aij =

σ∗ij|
{|
|
n
ij be its complement, then we have P(A c
ij)
supj

Let A c
[α1, ..., αn] with columns αj, we have

σ∗ij| ≤

σij −

D1 min

A

b

k

kop ≤

Σ − Σ∗

k

Ek

p
op ≤ E

sup
[d]
j

b
2p

≤

E sup
[d]
j

∈

d

|

h

i=1
X

b

d

σij −

|

σ∗ij|

p

i

≤ E sup

[d]

j

∈

p

+ 2p

i

E sup
[d]
j

∈

h

∈

h
σij −

i=1
X
b
σ∗ij|1(Aij)

h
d

i=1
X
b
σij −

|

i=1
X

b

Bound of R1. Let us ﬁrst bound R1. By (2.3) and (A.5) we have

, σ2 log n

r

δ log d

n }

o

.

(A.5)

≤
[d] k
∈
d

δ. For d
2d−
αjk1. Thus, some algebra gives

d symmetric matrix A =

×

σij −

|

σ∗ij|1(Aij) +

d

i=1
X

σij −

|

σ∗ij|1(A c
ij)

p

i

σ∗ij|1(A c
ij)

p

i

b

:= R1 + R2,

d

i=1
X
d

≤

i=1
X

σij −

|

σ∗ij|1(Aij)

≤

b
D1|

q

σ∗ij|

σ2 log n

(cid:16)

r

d

D1 min

σ∗ij|

|

n

, σ2 log n

r

q

1

−

(cid:17)

≤

D1s

σ2 log n

(cid:16)

r

δ log d
n

o
δ log d
n

(cid:17)

i=1
X
δ log d
n

This further gives

Bound of R2. Recall

σij =

2D1s

R1 ≤
(cid:16)
h
Tζ ˘σij, let T1 =

σ2 log n

.

q

1

p

−

δ log d
n
r
(cid:17)
i
σ∗ij|1(A c
σij −
ij)

d
i=1 |

p, we have

d

T1 ≤

h

i=1
X
1

(3d)p

−

≤

d

h

i=1
X

b
σ∗ij|1(A c
ij)1(

|

˘σij|

|

< ζ) +

p

1(A c

ij)1(

σ∗ij|

|

< ζ) +

˘σij|

|

d

|

i=1
X

(cid:2) P
b
˘σij − E˘σij|1(A c

ij) +

d

(cid:3)
|E˘σij −

p

σ∗ij|1(A c
ij)

d

i=1
X

˘σij − E˘σij|

|

p

i=1
X
1(A c

ij) +

i
σ∗ij|

p

d

i=1
X

|E˘σij −

Combining with the form of R2 yields

d

d

2p

E

R2 ≤

j=1 h
i=1
X
X
p
b
˘σij − E˘σij|

σij −

|

σ∗ij|1(A c
ij)

p

i

≤

1(A c

ij) + E

|E˘σij −

i,j
X

+ E

|

i,j
X

6pdp

−

1

E

σ∗ij|

|

p

1(A c

ij)1(

< ζ)

˘σij|

|

i,j
X
1(A c
ij)

(cid:16)
p
σ∗ij|

41

:= 2pdp

−

1

R21 + R22 + R23

.

(cid:17)

(cid:0)

(cid:1)

1(A c
ij)

.

i

(A.8)

(A.6)

(A.7)

q

1

−

.

Let us deal with R21, R22, R23 separately.
Bound of R21. Suppose the event A c
σij −

(A.5) we know

σ∗ij|

σ∗ij|

ij ∩ {|
> D1 min

=

˘σij|
{|

|

< ζ
σ∗ij|

}
, σ2 log n

σ∗ij| ≤

|E˘σij −
assume D1, C2 are suﬃciently large and satisfy D1 ≥
Combining with (2.9) in Theorem 2, we have

q

δ log d
n

|
D1,0σ2
b

holds, then

σij = 0, combining with

δ log d

n }

. Recall (A.3), we assume
b

max
{

3C2, 3

, C2 ≥

}

max

D1,0, 10C 2
1 }

.

{

q
for some constant D1,0. To avoid technical complication, we simply

σ∗ij| ≥

|

D1σ2 log n

r

δ log d

n ≥

3ζ > 3

˘σij| ≥

|

3

σ∗ij| −

|

3

σ∗ij −

|

,

˘σij|

which implies

Moreover, we have

σ∗ij −

|

˘σij| ≥

2
3|

σ∗ij|

and ζ

1
3 |

.

σ∗ij|

≤

(A.9)

δ log d

D1,0σ2

|E˘σij −
σ∗ij −

σ∗ij| ≤
˘σij| ≥

|

ζ

n ≤

σ∗ij|
r
σ∗ij|
, we use triangle inequality and obtain

≤

.

1
3|

Besides

3ζ, based on

σ∗ij| ≥
|
2
3|

2
3|

+

σ∗ij| ≤ |

σ∗ij −

˘σij| ≤ |

˘σij − E˘σij|

|E˘σij −

σ∗ij| ≤ |

˘σij − E˘σij|

+

1
3 |

,

σ∗ij|

(A.10)

which implies

˘σij − E˘σij| ≥

|

1
3|

σ∗ij|

. Therefore, we draw the conclusion that

A c

˘σij − E˘σij| ≥
Now we can invoke Hoeﬀding’s inequality (Proposition 3) and obtain

ij ∩ {|

˘σij|

σ∗ij|

} ∩ {|

⇒ {|

> 3ζ

< ζ

=

}

1
3 |

.

σ∗ij|}

p

σ∗ij|

E

1(A c

ij ∩ {|

˘σij|

< ζ

)

}

|

R21 =

≤

i,j
X

i,j
X
σ∗ij|

|

p

1(

(cid:2)
σ∗ij|

|

> 3ζ)P

|
(cid:16)

σ∗ij|

|

p

1(

σ∗ij|

|

2

≤

i,j
X

> 3ζ) exp

1
3 |

σ∗ij|

(cid:17)

(cid:3)

˘σij − E˘σij| ≥
n
σ∗ij|
|
18γ4

−

2

(cid:16)

(cid:17)

Moreover, some calculus can verify supy

0 y

≥

y/36)

(D2)p(√p)p. Thus, we proceed as

≤

R21 ≤

2

p

σ∗ij|

|

1(

> 3ζ) exp

|

σ∗ij|
σ∗ij|
n
|
γ4

i,j
X
p

γ2
√n

γ2
√n

= 2

(cid:16)

2

≤

(cid:16)

i,j (cid:16)h
X

(cid:17)

p

p

2 exp

y

sup
0
y

≥

(cid:17)

(cid:16)

2

p
2

i

−

h

exp

y
36

p

2 exp(

−

−
σ∗ij|
n
|
18γ4
n
σ∗ij|
|
36γ4

2

2

(cid:17)

1(

i(cid:17)(cid:16)

(cid:16)

−

h

nζ 2
4γ4

,

i(cid:17)

d2 exp

i(cid:17)(cid:16)

−

h

42

> 3ζ) exp

σ∗ij|

|

2

n
σ∗ij|
|
36γ4

i(cid:17)

−

h

Recall (2.6), (2.9) and that we assume C2 ≥
d2
n , which delivers d2 exp(
10γ2
−

nζ 2
4γ4 )

δ log d

−

≤

q

10C 2

1 , we have ζ

10C 2

1 σ2 log n

≥
25δ. We now put pieces together and obtain

q

n ≥

δ log d

R21 ≤

25δ

d2
−

2D2γ2

(cid:16)

p

p
n

(cid:17)

r

≤

25δ

d2
−

D3σ2 log n
(cid:16)

r

p

.

δ
n

(cid:17)

Bound of R22. By Cauchy-Schwarz inequality we have

R22 ≤

˘σij − E˘σij|

E|

2p

P(A c
ij)

δ
2

d−

q

≤

i,j
X

2E|

˘σij − E˘σij|

2p.

i,j q
X
γ2
2

n
k=1

˙Xk1,i ˙Xk2,j + ˙Xk2,i ˙Xk1,j

n , γ2
γ2
Recall ˘σij = 1
n ],
n
so by Hoeﬀding’s Lemma (e.g., Lemma 1.8 in [76]), ˘σij − E˘σij is the sum of n independent
(cid:3)
random variable, and each variable has sub-Gaussian norm scaling O
. Thus, Proposition
2 gives

. Now we invoke Proposition 1(b) to obtain

with each summand lying between [

γ2
n

P

−

(cid:2)

˘σij − E˘σijkψ2 = O

k

γ2
√n

(cid:0)

(cid:1)

(cid:0)
R22 . d2
−

(cid:1)

δ
2

D4,0γ2

p

p
n

(cid:17)

r

d2
−

δ
2

≤

D4σ2 log n
(cid:16)

r

p

.

δ
n

(cid:17)

is constant, hence we use (A.3) and obtain

Bound of R23. Note that

(cid:16)
|E˘σij −

σ∗ij|

R23 ≤ |E˘σij −

σ∗ij|

p

δ

2d−

≤

i,j
X

2d2
−

δ

D5σ2

(cid:16)

δ log d
n

p

.

(cid:17)

r

Now we are in a position to put everything together. By combining the upper bounds for
p. Substitute it into (A.8),

D6σ2 log n

d2
−

δ
2

δ log d
n

R2i, i = 1, 2, 3, we have R21 + R22 + R23 ≤
recall p = δ
≥

4, we obtain

4 and δ

(cid:0)

q

(cid:1)

d1
−

δ
4

6D6σ2 log n

R2 ≤

(cid:0)

δ log d
n

p

≤

r

(cid:1)

(cid:0)

6D6σ2 log n

r

δ log d
n

p

.

(cid:1)

This bound is dominated by the bound of R1 when δ log d(log n)2/n is suﬃciently small (note
that conventionally one assumes s = Ω(1)). Thus, there exists absolute constant D7 such that

Σ − Σ∗

p
op ≤

k

Ek

D7s

σ2 log n

r
which gives (2.11). We further invoke Markov inequality:

(cid:16)

b

h

δ log d
n

q

1

−

p

,

i

(cid:17)

Σ − Σ∗

Σ − Σ∗
b

kop ≥

p
op ≥

k

e4D7s

σ2 log n

h
e4D7s

r
σ2 log n

1

q

−

δ log d
n
i
δ log d
n

r

(cid:16)

h

P

=P

k
(cid:16)

k
(cid:16)

(2.12) follows. Now the proof is concluded.

b

1

(cid:17)
q
−

p

i

(cid:17)

(cid:17)

exp(

≤

4p) = exp(

−

δ),

−

(cid:3)

43

, by using Corollary 1 we can ”expect out” the

A.2 Heavy-tailed Data

Proof of Theorem 4. Since γ > η
≥ |
independent dithering noises Γk1,i, Γk2,j,

Xk,i|
e
Xk,j + Γk2,j)
Xk,i + Γk1,i)sign(

E˘σij = E

γ2

sign(

·

=E eXk,i

eXk,j

(cid:0)
EΓk1,i

γ

Thus, by triangle inequality we have

e

(cid:16)

(cid:2)

sign(
e

·

Xk,i + Γk1,i)

EΓk2,j

e
(cid:3)(cid:17)(cid:16)

γ

(cid:1)
·

(cid:2)

sign(

Xk,j + Γk2,j)

= E

Xk,i

Xk,j.

e

(cid:3)(cid:17)

e

e

˘σij −

|

σ∗ij| ≤ |

˘σij − E˘σij|

+

|E(Xk,iXk,j −

Xk,i

Xk,j)

|

:= R1 + R2.

(A.11)

n
Bound of R1. From ˘σij =
k=1
dent random variables lying in [
plug in the value of γ (2.15), we have

P

γ2
2n ( ˙Xk1,i ˙Xk2,j + ˙Xk1,j ˙Xk2,i) we know ˘σij is mean of n indepen-
e
γ2, γ2], then by Hoeﬀding’s inequality (Proposition 3) and
−

e

= 2 exp

−

(cid:16)
4 √M

nt2
2γ4

(cid:17)
R1 ≥
Xk,i 6
(cid:0)
e
{|
+ E

(1(

|
h

i

P(R1 ≥

t)

≤

2 exp

−

(cid:16)

t2√nδ log d
2C 4
4 M

t > 0.

∀

,

(cid:17)

(A.12)

Setting t = √2MC 2
4
Bound of R2. Since the truncated version
(cid:0)

yields P

δ log d
n

(cid:1)

1/4

1/4

δ log d
n

√2C 2
= Xk,i only when
(cid:2)

(cid:3)

2d−

δ.
> η, so

≤
Xk,i|
(cid:1)
|

R2 ≤ E
= E

Xk,iXk,j −

|
h
Xk,iXk,j|1(

Xk,i|
e

|

Xk,i

Xk,j|
> η)
e

> η

Xk,i|
} ∪ {|
Xk,iXk,j|1(

Xk,j|
Xk,j|

|

> η

))

}

i

> η)

:= R21 + R22.

|
h

i
2
Xk,iXk,j|

2

By Cauchy-Schwarz inequality, we bound R21 by R21 ≤
Xk,i|
we have E|
≤ E(
|
> η)
yields that P(
≤
η2 = √M
η, we have R21 ≤
C2
3

4 +
Xk,j|
|
Xk,i|
η4 = M

Xk,iXk,j|
Xk,i|
|

δ log d
n

1/4

E|

M

4

4)/2

> η), moreover,
P(
M. A direct application of Markov inequality
η4 . Plug in the above two inequalities and the value of

Xk,i|

E|

p

≤

|

. Since R22 can be bounded likewise, it holds that

R2 =

(cid:1)
(cid:0)
|E(Xk,iXk,j −

Xk,i

Xk,j)

2
C 2
3

√M

| ≤

δ log d
n

(cid:16)

(cid:17)

1
4

.

(A.13)

e
Now we can put things together and obtain (2.16). Moreover, (2.17) follows from a union
(cid:3)
bound, hence the proof is concluded.

e

Proof of Theorem 5. The proof is parallel to that of Theorem 2. For some speciﬁed C3, C4,
by Theorem 4 there exists an absolute constant D1 such that

˘σij −

σ∗ij| ≤

D1√M

P

|
(cid:16)
We assume C5 > D1 and ﬁrst rule out probability 2d−
σ∗ij| ≤
upon the event
cases.

˘σij −

D1√M

δ log d
n

1/4

h

|

(cid:2)

(cid:3)

1
4

δ log d
n

1

≥

−

2d−

δ.

(A.14)

(cid:17)

i
δ in (A.14), so we can proceed the proof
. According to the threshold ζ we discuss two

44

Case 1.

|
inequality gives

˘σij|

=

|

|

σ∗ij|
σij −
< ζ, then we have
(D1 + C5)√M
σ∗ij −
˘σij|
+
b
b
(D1 + C5 + 1) min
σ∗ij| ≤

σij = 0, thus,
˘σij| ≤

σ∗ij| ≤ |
σij −
> ζ, then we have
1/4

σ∗ij|

|
n

b

|

|

. Let us show it can also be bounded by
b

h
σij = ˘σij, which leads to
σ∗ij|

|

|
δ log d
n

(cid:0)
, √M

. Moreover, triangle

σ∗ij| ≤ |

σ∗ij|
4 , so we have

1

1
4

δ log d
(cid:1)
n
i
σij −
σ∗ij| ≤
|
. A reverse triangle inequal-

˘σij −

o
σ∗ij|

=

|

b

˘σij| − |

˘σij −

σ∗

|

> ζ

˘σij −

− |

σ∗

| ≥

(C5 −

D1)√M

δ log d
n

1/4,

. Now we can draw the conclusion that

(cid:2)

(cid:3)

h
Combining two cases leads to (2.20), so we complete the proof.

D1
C5 −

D1

) min

, √M

σ∗ij|

|
n

δ log d
n

1
4

.

i

o

(cid:3)

1/4

≤
σ∗ij| ≤

1

C5

D1 |

−

σ∗ij|

(D1 +

(cid:2)
|

(cid:3)
σij −

b

˘σij|

|
δ log d
n

Case 2.
D1√M
ity gives
(cid:2)

|

(cid:3)
σ∗ij| ≥ |

so we obtain √M

δ log d
n

Proof of Theorem 6. Since η, γ, ζ are speciﬁed with some C3, C4, C5, by Theorem 5 there
exists absolute constant D1 such that (2.20) holds. We deﬁne the event

Aij =

σ∗ij| ≤

σij −
|
n
δ (Here, A c

D1 min

σ∗ij|

{|

, √M

δ log d
n

1
4

}

,

(A.15)

ij denotes the complementary event). Now we can divide

(cid:2)

o

(cid:3)

then we have P(A c
2d−
ij)
the operator norm error according to Aij and A c

≤

b

d

p

Σ − Σ∗

p
op ≤ E

k

sup
[d]
j

Ek

σij −

|

σ∗ij|

≤ E sup

[d]

j

σij −

|

d

∈

b
2p

i=1
i=1
X
X
b
b
σ∗ij|1(Aij)
σij −
Bound of R1. By the sparsity (2.3) and (A.15), for any j

h
σij −

E sup
[d]
j

E sup
[d]
j

+ 2p

i=1
X

i=1
X

h
d

≤

h

i

i

h

∈

∈

∈

|

|

p

b

b

σ∗ij|1(Aij) +

d

i=1
X

σij −

|

σ∗ij|1(A c
ij)

p

i

b

:= R1 + R2.

σ∗ij|1(A c
ij)

p

i

[d] we have

∈

ij, it gives
d

d

i=1
X
d

≤

i=1
X

This leads to

σij −

|

σ∗ij|1(Aij)

≤

d

D1 min

i=1
X

, √M

σ∗ij|

|

n

b
D1|

q

σ∗ij|

√M

(cid:16)

h

δ log d
n

1
4

q

1

−

≤

i

(cid:17)

D1s

√M

(cid:16)

h

δ log d
n

h
δ log d
n

1
4

i

1
4

o
1

−

q

.

i

(cid:17)

Bound of R2. Let T1 =
˘σij|
the problem into
{|
σ∗ij|1(A c

T1 ≤

|

d

h

i=1
X
1

(3d)p

−

≤

d

h

i=1
X

R1 ≤

d
i=1 |
< ζ
(cid:2) P
}

(cid:16)
σij −
{|

and
b
< ζ) +

p

.

−

(1

q)/2

q)/4

δ log d
n
p. Recall that
(cid:0)

2D1sM (1
−
σ∗ij|1(A c
ij)
ζ
˘σij| ≥
, then triangle inequality yields
}
˘σij − E˘σij|1(A c

|E˘σij −

(cid:17)
σij =

ij) +

b
d

(cid:1)

(cid:3)

|

d

ij)1(

˘σij|

|

i=1
X

p

1(A c

ij)1(

σ∗ij|

|

< ζ) +

˘σij|

|

d

i=1
X

˘σij − E˘σij|

|

p

45

Tζ ˘σij, under A c

ij we divide

i=1
X
1(A c

ij) +

p

σ∗ij|1(A c
ij)

i
σ∗ij|

p

d

i=1
X

|E˘σij −

1(A c
ij)

.

i

Now we put it into the expression of R2 and obtain

p

i

≤

i,j
X

d

d

2p

E

R2 ≤

+ E

|

i,j
X

σij −

|

σ∗ij|1(A c
ij)

6pdp

−

1

E

σ∗ij|

|

p

1(A c

ij)1(

< ζ)

˘σij|

|

j=1 h
i=1
X
X
1(A c
p
b
˘σij − E˘σij|

ij) + E

i,j
X
1(A c
ij)

(cid:16)
p
σ∗ij|

|E˘σij −

σij −
max
{

Bound of R21. Suppose the event A c
ij ∩ {|
, √M
σ∗ij|
σ∗ij|
that
{|
|
|
2/C 2
, and C5 ≥
D1 ≥
max
{
}
δ log d
D1√M
n

σ∗ij|
=
3C5, 3

> D1 min

1/4

b

σ∗ij| ≥
|
σ∗ij −
|
Xk,i

which implies
E(Xk,iXk,j −

(cid:3)
and ζ
Xk,j). Combining (A.13), (2.19) gives

˘σij| ≥

(cid:2)
σ∗ij|

1
3|

2
3|

≤

:= 6pdp

−

1(R21 + R22 + R23).

(cid:17)

˘σij|

holds, then

(A.16)
< ζ
σij = 0, thus, (A.15) delivers
δ log d
. With no loss of generality, we assume
n
3 , 4C 2
. Combining with (2.19), Theorem 5, we have
4}
(cid:2)

}
1/4

b

}

(cid:3)

3ζ > 3

˘σij| ≥

3

σ∗ij| −

3

σ∗ij −

,

˘σij|

≥
σ∗ij|

|

|

|
. Since η < γ, it always holds that E˘σij −

σ∗ij =

e
e
|E˘σij −

σ∗ij| ≤

2
C 2
3

√M

δ log d
n

1/4

C5√M

≤

h

i
by triangle inequality and have

h

δ log d
n

ζ

≤

≤

1
3 |

.

σ∗ij|

1/4

i

We upper bound

σ∗ij −

|

˘σij|

2
3 |

σ∗ij −
σ∗ij| ≤ |
˘σij − E˘σij| ≥

˘σij| ≤ |
σ∗ij|

1
3|

|

+

˘σij − E˘σij|
. Therefore,

which implies

|E˘σij −

σ∗ij| ≤ |

˘σij − E˘σij|

+

1
3 |

,

σ∗ij|

A c

ij ∩ {|

< ζ

˘σij|

}

=

⇒ {|

σ∗ij|

> 3ζ

} ∩ {|

˘σij − E˘σij| ≥

1
3 |

,

σ∗ij|}

so we can bound R21 via

σ∗ij|

|

p

1(

|

ij ∩ {|

p

E

1(A c
h
σ∗ij|

> 3ζ)P

˘σij|

< ζ

)

}

i

R21 =

|

i,j
X
σ∗ij|
γ2
√n

p

(cid:17)

p

γ2
√n

≤

i,j
X

= 2

(cid:16)

2

≤

(cid:16)

i,j (cid:16)h
X
y
sup
0
y

≥

(cid:17)

(cid:16)

|
(cid:16)

p
2

˘σij − E˘σij| ≥
n
σ∗ij|
|
36γ4

exp

−

i

h

2

n
|

σ∗ij|
γ4

p

2 exp

−

h

y
36

d2 exp

i(cid:17)(cid:16)

−

h

1
3 |

σ∗ij|

2

2

≤

(cid:17)

i,j
X

σ∗ij|

|

p

1(

|

1(

σ∗ij|

|

> 3ζ) exp

i(cid:17)(cid:16)

nζ 2
4γ4

γ2
√n

h

p

(cid:17)

(cid:16)

2

≤

(cid:16)

i(cid:17)

> 3ζ) exp

σ∗ij|
n
σ∗ij|
|
36γ4

−

2

i(cid:17)

p

2 exp

y

sup
0
y

≥

−

h

2

n
σ∗ij|
|
18γ4

(cid:17)

−

(cid:16)

y
36

d2
−

4δ,

i(cid:17)

where the second inequality is from Hoeﬀding’s inequality (Proposition 3), while we plug in
2pp/2.
γ, ζ and use C5 ≥
4 in the last line. Some calculus show supy
≤
Then we plug in the above inequality and the value of γ (2.15), for some D3 we have

0 yp/2 exp(

y/36)

4C 2

Dp

−

≥

R21 ≤

4δ

d2
−

2D2γ2

(cid:16)

p

p
n

(cid:17)

r

≤

46

4δ

d2
−

D3√M

(cid:16)

h

δ
n log d

1
4

p

i

(cid:17)

Bound of R22. This is the same as the corresponding part in the proof of Theorem 3. In brief,
we can show an upper bound of the same form, but with diﬀerent value of γ2 (given in (2.15)):

d2
−

δ
2

D4,0γ2

R22 ≤

p

p
n

≤

r

(cid:16)
Bound of R23. Note that
|E˘σij −
bounded in the proof of Theorem 4. In particular, (A.13) gives
By combining with P(A c
2d−
ij)

δ, we bound R23 via

(cid:17)
|E(

σ∗ij|

Xk,i

=

e

h

Xk,iXk,j)

≤

δ
2

d2
−

D4√M
(cid:16)
Xk,j −
e

δ
n log d

1
4

p

i

(cid:17)

is constant which has been
1/4

√M

2
C2
3

δ log d
n

.

|
|E˘σij −

σ∗ij| ≤

(cid:2)

(cid:3)

R23 ≤ |E˘σij −

σ∗ij|

p

δ

2d−

δ

d2
−

≤

D5√M
(cid:16)

h

δ log d
n

1
4

p

,

i

(cid:17)

i,j
X

Now we are in a position to put things together. By combining the upper bounds for R2i, i =
p/4. We further substitute it into (A.16),
1, 2, 3, we have R21 + R22 + R23 ≤
and recall p = δ
4, δ
4, we obtain

2 M p/2

δ log d
n

6d2
−

Dp

δ

≥

R2 ≤

d1
−

δ
4

6D6√M

(cid:3)

(cid:2)

1
4

p

≤

δ log d
n

6D6√M

δ log d
n

1
4

p

.

i
When δ log d/n is small enough, this upper bound for R2 is smaller than the obtained bound
for R1. Thus, we know there exists absolute constant D7 such that

(cid:17)

(cid:16)

(cid:17)

(cid:16)

h

i

h

Σ − Σ∗

p
op ≤

k

Ek

D7sM (1
−

q)/2

(cid:16)

(2.21) follows. We further use Markov inequality:

b

δ log d
n

h

q)/4

p

(1

−

,

i

(cid:17)

∗

Σ − Σ

Σ − Σ∗
b

kop ≥
p
op ≥

k

P

=P

k
(cid:16)

k
(cid:16)

e4D7sM (1
−

q)/2

e4D7sM (1
−
h

(1

q)/4

−

δ log d
n
i
δ log d
n

h
q)/2

(cid:17)
q)/4

p

(1

−

h
this displays (2.22) and concludes the proof.

b

i

i

(cid:17)

exp(

≤

4p) = exp(

−

δ),

−

(cid:3)

B Proofs: Sparse Linear Regression

Proof of Lemma 2. The proof is obtained by modifying and combining Lemma 1 in [67]
and Theorem 1 in [43].
I. From the detinition of

Θ (3.3), we have

(

Θ)

b

L

− L

(Θ∗)

Θ∗

λ

k

≤

knu −

λ

Θ

knu.

k

By (3.4), some algebra delivers that

b

b

(

Θ)

L

b

−L
1
2

=

(Θ∗) =

∆)T Qvec(

vec(

1
2
∆)T Qvec(
∆) +
b

vec(

b

b

∆)

−

mat(Q
b

·

D

47

B,

∆

+ vec(Θ∗)T Qvec(

∆)

E
D
vec(Θ∗))
b

−

B,

∆

.

b

E

b

(B.1)

(B.2)

Since Q is positive semi-deﬁnite, combining with

A1, A2

(

Θ)

L

− L

(Θ∗)

≥ − k

mat(Q

·

h
vec(Θ∗))

II. Consider the SVD Θ∗ = U ΣV T =

b

U1 U2

Rd

(d

V2
(cid:2)
we consider two linear subspaces of Rd
×

z) is a partition of singular vectors, z
(cid:3)

∈

−

×

(cid:20)
∈ {
d deﬁned as

A1

A2

kopk

i ≤ k

knu, (3.5)
knu.
∆

λ
2 k

B

−

∆

kopk
0

Σ11

0 Σ22

0, 1, ..., d
=

M

{

b

knu ≥ −
V T
1
V T
2 (cid:21)
(cid:21) (cid:20)
will be speciﬁed later. If z
}
U1A1V ∗

b
, where U1, V1

1 : A1

and

Rd

Rz

∈

×

×

z

∈

}

(B.3)

z, U2,

1

≥

=

U1 U2

M

A1 A2
A3
0

V T
1
V T
2 (cid:21)

(cid:21) (cid:20)

(cid:20)

n (cid:2)
and

(cid:3)
denote the projection onto

PM
PM
d, assume that ∆ =
×

Rd

: A1

∈

Rz

z, A2

×

∈

Rz

×

(d

−

z), A3

∈

R(d

−

z

z)

×

,

o

M
∆11 ∆12
∆21 ∆22

and

M
V T
1
V T
2 (cid:21)

(cid:21) (cid:20)

respectively. Given a matrix

, then

and

PM

PM

have the

U1 U2

(cid:2)

(cid:20)

(cid:3)

then let

∆

∈

explicit form

∆ = U1∆11V T

1 and

PM

∆ =

U1 U2

PM

⊥∆ = ∆

Besides, let
− PM
decomposable [67] with respect to the pair of subspaces (
it holds that

− PM

PM

PM

∆,

(cid:2)
⊥∆ = ∆

∆1 +

kPM

By using
PM
PM
with (B.1), we obtain

and

, we have

⊥∆2

knu =
knu ≤ kPM

PM
∆

k

∆1

kPM
∆

knu +

M

M
knu +

kPM

.

0

(cid:20)

∆11 ∆12
∆21

V T
1
V T
2 (cid:21)
(cid:3)
∆. Note that the nuclear norm is
d,

) since for any ∆1, ∆2

(cid:21) (cid:20)

Rd

,

×

∈

kPM
∆

⊥

⊥∆2

knu.

(B.4)

knu, plug in (B.3) and combine

b
Θ∗

Θ

knu − k

k

b

b

∆

knu +

⊥

∆

knu

kPM

1
2

knu ≤
=

kPM
h
PM

i
= 0, it can be easily veriﬁed that (B.4), (B.5)

b

.

(B.5)

In the special case z = 0, we just let
b
and what follow still hold.
III. In this part we derive (3.6). We calculate that

PM

b

Θ

k

knu − k
Θ∗

≥ kPM
b
=

kPM

⊥

Θ∗ +

knu =

Θ∗
knu +
∆
knu −

⊥

kPM
∆
⊥Θ∗
b

kPM

kPM
2

PM
knu − kPM

knu − kPM

⊥Θ∗ +
⊥Θ∗

∆

PM
∆

∆ +

PM
knu − kPM
b
knu,

b

⊥

∆

knu − kPM
Θ∗

knu − kPM

b

Θ∗ +

PM
knu − kPM

⊥Θ∗
⊥Θ∗

knu
knu

(B.6)

note that we use decomposability (B.4) and triangle inequality in the third line. By combining
b
(B.5), (B.6) we obtain

⊥Θ∗

∆

∆

b

3

⊥

∆

kPM

kPM
∆
b
knu ≤
knu ≤ kPM
Assume the singular values of Θ∗ are σ1(Θ∗)
...
we choose a threshold τ > 0 and then let z = max
rank(

2z, we have

kPM

√2z

knu ≤
knu +

∆)

∆
b

∆

∆

≥

b

b

b

k

⊥

knu + 4
4(

PM

≤

kPM

knu ≤
z

k

b

zτ q

σk(Θ∗)q

kPM

knu, it holds that
∗
knu).
⊥Θ

kPM

∆

kPM

knu +

(B.7)
σd(Θ∗). Instead of choosing z directly
[d] : σw(Θ∗)
. Since
w
0
∈
kF. Moreover, by (3.2) we have

b
} ∪ {

{
(cid:8)

≥

≥

(cid:9)

}

τ

b
≤

Xk=1

b
≤

d

Xk=1

σk(Θ∗)q

r,

≤

48

rτ −
which implies z
we can bound the last term in (B.7) by

q. Therefore, we have

≤

∆

knu ≤

kPM

√2rτ −

q/2

∆

kF. By simple algebra

k

⊥Θ∗

knu =

kPM

d

Xk=z+1

σk(Θ∗) =

d

b
σk(Θ∗)qσk(Θ∗)1

−

Xk=z+1

b

q

≤

rτ 1
−

q.

By putting pieces together, we obtain

∆

knu ≤

k

4

√2rτ −

q
2

(cid:16)

k

b

kF + rτ 1
∆
b∆
k
k
√r

b

F

2/(2

q

,

−

τ > 0.

∀

(cid:17)
q)
, then we obtain (3.6).

−

∆

We only consider
IV. Assume we have RSC (3.7), we derive the convergence rate. With RSC, from (B.2) we
(cid:16)
have tighter estimation than (B.3):

= 0, then we choose τ =

(cid:17)

b

(

Θ)

L

− L

(Θ∗)

1
2

κ
k

∆
k

2
F −

λ
2 k

∆

knu.

≥
(Θ∗)

On the other hand we have
obtain
Again plug it into (3.6) we obtain the bound for nuclear norm.

knu from (B.1). By combining them we
2
F. Then plug in (3.6), the bound for Frobenius norm in (3.8) follows.
(cid:3)

knu ≥

κ
3λk

∆
k

− L

Θ)

∆

≤

b

b

λ

k

b
∆
k

b
(
L

b

b

b

Θ

tr = diag(Θ∗). Consider the convex set

(3.9) can be recast as a trace regression Yk =
=

Proof of Corollary 2.
where Xk,tr = diag(Xk), Θ∗
diag(Θ),
kmax ≤
constituted of the rows and columns with numbering in
and the rows and columns not in
semi-deﬁnite. It is not hard to see that
matrix

Xk,tr, Θ∗
+ ǫk,
tri
h
d : Θ =
Θ
×
{
d2 is the matrix whose submatrix
d,
{
are all zero. Obviously, Qtr is positive
Θ deﬁned by (3.11) is equivalent to ﬁnding the diagonal

×
1, d + 2, 2d + 3, ..., d2

, let Btr = diag(B), and Qtr

1, d + 2, 2d + 3, ..., d2

Θtr via

is Q

Rd2

Rd

Rd

R

∈

∈

∈

S

{

k

}

}

}

×

b

Θtr

∈

b
arg min
Θ

∈S
(Θ) = 1

knu,

(Θ) + λ

Θ

L
k
2 vec(Θ)T Qtrvec(Θ)

where the loss function is given by
be the main diagonal of

Btr, Θ
i
Θtr. Then all the results follow by using Lemma 2.

− h

L

b

, and then let

Θ
(cid:3)
b

B.1 Sub-Gaussian Data

b

Proof of Theorem 7. To use Corollary 2 we only need to establish (3.12), (3.14).
I. We ﬁrst show that when (log n)2 log d/n is suﬃciently small,
probability. By Assumption 3 and Theorem 3 we have

Σ is positive deﬁnite with high

Σ − ΣXX

kop ≤

D1σ2 log n

P

k
(cid:16)

δ log d
n

r

(cid:17)
Σ − ΣXX

≥

b
Under suﬃciently small (log n)2 log d/n we have
δ). Use λmin(
than 1
Combining with λmin(ΣXX)

κ0 with probability higher
kop ≤
) to denote the smallest eigenvalue for a symmetric matrix.
·
2κ0 in Assumption 3, we obtain

exp(

−

−

b

k

b

1

−

exp(

δ).

−

(B.8)

≥
λmin(
Σ)

b

49

b

λmin(ΣXX )

Σ

ΣXXkop ≥

−

− k

κ0,

≥

(B.9)

6
which implies that
II. It remains to bound
−
ΣY X = EYkXk and ﬁrst note that

ΣΘ∗

b

k

Σ is positive deﬁnite, and (3.14) holds.

b
ΣY X = E(YkXk) = E(XkX T

ΣY Xkmax and show (3.12) holds with high probability. Let
b

k Θ∗ + ǫkXk) = E(XkX T

k )Θ∗ = ΣXXΘ∗.

By repeating the proof of Theorem 1, we have the element-wise error for

ΣY X

P

k

(cid:16)

ΣY X −
b

ΣY Xkmax ≤

D2σ2 log n

r

δ log d
n

(cid:17)

2d1
−

b
δ.

1

−

≥

(B.10)

We now combine (B.8) and (B.10), it holds with probability higher than 1
that

2d1
−

δ

exp(

δ)

−

−

−

ΣΘ∗

k

Σ
b

≤k

−

ΣY Xkmax ≤ k
−
k2 +
Θ∗
ΣXX kopk
b
b
k

−
ΣY X −

ΣΘ∗

ΣXX Θ∗

kmax +
ΣY Xkmax ≤
b

ΣY X kmax
ΣY X −
k
(D1R + D2)σ2 log n
b

(B.11)

δ log d
n

.

r

b

2(D1R + D2), then (3.12)
Thus, we can choose suﬃciently large C6 in (3.19) such that C6 ≥
holds with high probability. Now that (3.12) and (3.14) have been veriﬁed, Corollary 2 gives
(cid:3)
(3.15). We further substitute (3.19) into (3.15) and conclude the proof.

B.2 Heavy-tailed Data

Proof of Theorem 8. The proof is parallel to Theorem 7. By Assumption 3 and Theorem
6, we have the probability tail for operator norm deviation

Σ

ΣXXkop ≤

−

D1√M

P

k
(cid:16)

1/4

δ log d
n

h

b
when log d/n is suﬃciently small, we can assume
than 1
λmin(
Σ)
≥
nite and (3.14) holds.

Σ
δ). This, together with λmin(ΣXX )
b

exp(

−

−

k

κ0 under the same probability. Thus, with high probability

1

exp(

≥

−

δ),

−

(B.12)

i

−

(cid:17)
ΣXX kop ≤
≥

κ0 with probability higher
2κ0 given in Assumption 3, gives
Σ is positive deﬁ-

It remains to establish (3.12) and apply Corollary 2. By repeating the proof of Theorem

b

b

4, we can show the max-norm error for

ΣY X to approximate ΣY X = EYkXk as

P

k
(cid:16)

ΣY X −
b

b
ΣY X kmax ≤

D2√M

1

−

≥

2d1
−

δ.

(B.13)

1/4

δ log d
n

h

i

(cid:17)

Now we combine (B.12) and (B.13), with probability higher than 1

exp(

δ)

−

−

2d1
−

δ it yields

k

−

Σ
b

≤k

ΣΘ∗

ΣΘ∗

ΣXXΘ∗

−
ΣY X −

ΣY X kmax ≤ k
−
k2 +
Θ∗
ΣXX kopk
b
b
k
b
Thus, in (3.22) we can choose suﬃciently large C7 such that C7 ≥
λ
ΣΘ∗
≥
error bounds follow.

kmax +
ΣY Xkmax ≤
b

2(D2R + D1), then we verify
ΣY X kmax. Now we can use (3.15) in Corollary 2 and plug in (3.22), the desired
(cid:3)
b

ΣY X −
k
(D1R + D2)√M

1/4

.

(B.14)

δ log d
n

−

b

k

2

i

h

−
ΣY X kmax
b

50

B.3 One-bit Compressed Sensing

Proof of Theorem 9. We prove the error bound based on Corollary 2. Evidently, we need

δ log d log n
n

to show setting λ = C8
ΣY Xkmax. Note that ΣXX Θ∗ = EXk(X T
obtain
b

q

with suﬃciently large C8 can guarantee λ

2

ΣXXΘ∗

−
k Θ∗) = E(YkXk), we ﬁrst invoke triangle inequality to

≥

k

b

ΣXXΘ∗
n

k

1
n

+
b

k

−

γ

ΣXX −
ΣY X kmax ≤ k
(cid:0)
˙YkXk − E
˙YkXk
γ
b
b
·
·

Θ∗

ΣXX

kmax +
(cid:0)
kmax := R1 + R2 + R3.

kE

(cid:1)

YkXk −

Xk=1

(cid:0)

(cid:1)

˙YkXk

γ

·

kmax

(cid:1)

(B.15)

Bound of R1. We ﬁrst give a standard upper bound of
write the i-th entry of Xk as Xk,i, then we have the (i, j)-th entry of
by 1
k=1 Xk,iXk,j − EXk,iXk,j. Note that
Xk,ikψ2k
n
Bernstein’s inequality given in Proposition 4 we have

ΣXX −
b
Xk,iXk,jkψ1 ≤ k

k

k

n

ΣXX −
Xk,jkψ2 ≤
b

ΣXX kmax. For (i, j)

∈

×

[d]
[d],
ΣXX is given
σ2, thus by

P

P

(cid:16)(cid:12)
(cid:12)

1
n

n

Xk=1

Xk,iXk,j − EXk,iXk,j

≥

(cid:12)
(cid:12)

A union bound further gives

2 exp

t

(cid:17)

≤

−

(cid:16)

D1 min

(cid:8)

nt2
σ4 ,

nt
σ2

,
(cid:9)(cid:17)

∀

t > 0.

(B.16)

P

k

ΣXX kmax ≥

ΣXX −
b
1
,
Set t = max
D1
holds with probability higher than 1
gives

1
√D1

δ log d

σ2

q

(cid:8)

(cid:9)

(cid:0)

n , when n
2d2
−

−

≤

t

(cid:1)

2d2 exp

D1 min

−

(cid:16)

(cid:8)
δ log d we obtain

nt2
σ4 ,

nt
σ2

,

t > 0.

∀

(cid:9)(cid:17)
ΣXX kmax . σ2
ΣXX −
Θ∗
b

k

δ log d
n
q
k1 ≤

R

≥
k
δ. With the same probability, combining

δ log d
ΣXX −
n
˙Yk
Bound of R2. By Lemma 1 when
b
Cauchy-Schwarz inequality, we can ﬁrst bound R2 from above as

k1 . σ2
γ we have EΛk

ΣXX kmaxk

R1 ≤ k

Yk| ≤

r
γ

Θ∗

|

·

.

= Yk. Use this fact and

R2 =

E

γ ˙Yk

Yk −

Xk

max =
(cid:13)
Yk|
(cid:13)

> γ)

E

Yk −
(cid:0)
max
≤
[d]
j
∈

(cid:1)
YkXk,j|1(

|

Yk|

4 + E|

Xk,j|

4

E|

Yk|

|

(cid:13)
max
(cid:13)
[d]
j
∈

≤

(cid:0)

E

≤

max
[d] r
j
∈

|
(cid:0)
1
2

(cid:0)

(cid:1)
> γ

Yk|
|
Xk,j|

2

(cid:0)
Xk 1

γ ˙Yk

(cid:1)

(cid:0)
2
Yk|
|

E

q

|
(cid:2)
> γ) . σ2 exp

max

(cid:1)(cid:13)
P(
(cid:13)
(cid:3)p
D2γ2
σ2

−

> γ)

Yk|

|

,

(cid:1)

(cid:13)
(cid:13)
(cid:1)
P(

(cid:1)

where the last inequality follows from (3.18) and Proposition 1.
Bound of R3. For j
Proposition 2 further gives

[d], by (1.1) it is evident that
˙YkXk,jkψ2 ≤

n
k=1 γ

∈

1
n

k

·

γ
γ
k
D3
√n γσ. Thus, Proposition 1(a) yields

Xk,jkψ2 ≤

k

·

γσ, hence

(cid:0)
˙YkXk,jkψ2 ≤

1
n

n

Xk=1

P

(cid:16)(cid:12)
(cid:12)
(cid:12)

P
˙YkXk,j − E
γ

·

γ

·

˙YkXk,j

(cid:0)

(cid:1)(cid:12)
(cid:12)
(cid:12)

2 exp

≥

t

(cid:17)

≤

−

(cid:16)

D4nt2
γ2σ2

,

(cid:17)

∀

t > 0.

(B.17)

51

Furthermore, a union bound over j

∈
δ log d
D4n and obtain

[d] gives P

R3 ≥
(cid:0)

t

(cid:1)

2d

·

≤

exp

D4nt2
γ2σ2

,

∀

(cid:1)

−

(cid:0)

t > 0. We

further set t = γσ

q

γσ

P

R3 ≤
(cid:16)

r

δ log d
D4n

(cid:17)

1

−

≥

2d1
−

δ.

By (2.6) we can assume σ < γ . σ√log n. Thus, by (B.15) and the upper bounds for
4d2
R1, R2, R3, with probability higher than 1
−

δ we have

−

ΣXXΘ∗

k

r
Since in this Theorem we assume σ is an absolute constant, we can choose suﬃciently large
C8 in (3.25) to guarantee λ
ΣY Xkmax holds with high probability. By Corollary
2, it already leads to (3.13), a relation that facilitates the following discussions.
is
b

Now that (3.12) has been veriﬁed, we turn to consider the RSC (3.14). When δ log d

ΣXXΘ∗

r

−

≥

b

b

k

n

−

ΣY X kmax . σγ
b
2

δ log d
n

. σ2

δ log n log d
n

.

suﬃciently small, combining with λmin(ΣXX )

κ0k
(cid:16)
This event, together with (3.13), implies
b

ΣXX

∆

≥

b

b

b

∆
k

P

∆T

2
2 −

≥
D5δ log d
n

1

−

≥

3d1
−

δ.

2
1

∆
k

k

b

(cid:17)

2κ0, Lemma 2(a) in [43] gives

∆T

ΣXX

∆

κ0k

∆
k

2
2 −

D6 ·

≥

δ log d
n

·

2
2−q

s

∆

k

k

4−4q
2−q
2

.

(B.18)

We proceed the proof upon the condition (B.18) and divide it into the following two cases.

b

b

b

b

2
2−q

s

δ log d
n

Case 1. If D6 ·
2 . Thus,
we can invoke (3.15) in Corollary 2 and then plug in the value of λ in (3.25). This displays
the desired error bounds.
Case 2. Otherwise, it holds that

2, (B.18) gives the RSC (3.14) with κ = κ0
2

κ0
2 k

∆
k

∆
k

b

b

k

·

b
4−4q
2−q
2 ≤

D6 ·

δ log d
n

·

2
2−q

s

4−4q
2−q
2

∆
k

k

κ0
2 k

2
2.

∆
k

≥

With no loss of generality, we assume

∆

= 0. Under the scaling that √s

b

b

(B.19)

δ log d
n

1

−

q/2 is

suﬃciently small we have q

(0, 1) (Since when q = 0, D6 ·

b

∈

(B.19) gives

∆ = 0). Again use suﬃciently small √s

2

s

δ log d
2−q < κ0
n
q/2, (B.19) delivers

−

·

1

(cid:0)q
2 together with
(cid:1)

δ log d
n

b

∆

k2 .

k

√s

h

(cid:16)r

δ log d
n

q
2

1

−

2
q

(cid:17)

i

This, together with (3.13), gives the upper bound for

b

(cid:1)
δ log d
n

q
2

1

−

.

(cid:17)

(cid:0)q
√s

≤

(cid:16)r
k1 as
∆

k

Thus, we conclude the proof.

b

k1 . s
∆

k

δ log d
n

q

1
b
−

.

(cid:17)

(cid:16)r

52

(cid:3)

6
Proof of Theorem 10. We follow similar ideas used in the proof of Theorem 9 and intend
to invoke Corollary 2. First let us verify the crucial relation λ
ΣY X kmax. Note
that ΣXXΘ∗ = EYkXk, by triangle inequality we can divide it into three terms Ri, 1
3
≤
≤
b

Σ ˜X ˜XΘ∗

≥

−

k

2

i

Σ ˜X ˜X −
(cid:13)
(cid:0)
˙Yk
Xk
γ
b
(cid:13)
·

ΣXX

Θ∗

max +
(cid:13)
(cid:13)

(cid:1)

max := R1 + R2 + R3.

(cid:0)

(cid:13)
(cid:13)

E

˙Yk

γ

·

YkXk

b
Xk −
e

max

(cid:1)(cid:13)
(cid:13)

(B.20)

k

Σ ˜X ˜XΘ∗
n
1
n

+
b

−

γ

Xk=1

ΣY X kmax ≤
˙Yk
Xk − E
b
·
e

(cid:0)

k

(cid:13)
(cid:13)

ΣXXkmax ≤

Bound of R1. We ﬁrst decompose

(cid:1)(cid:13)
e
(cid:13)
Σ ˜X ˜X −
k
X T
Xk
Σ ˜X ˜X − E
b
k
(cid:13)
Let us deal with them element-wisely. For R11 and any (i, j)
(cid:13) b
e
Xk,i| ≤
truncated covariate satisﬁes
e
k,iX 2
EX 2

ΣXX kmax as
max +

Σ ˜X ˜X −
b

k,i + EX 4
k,j

XkX T

EX 4

k −

Xk,j

Xk,i

(cid:1)(cid:13)
(cid:0)
[d]
e
(cid:13)
×
ηX , combining with (3.21) it gives

e
∈

nM

Xk

(cid:13)
(cid:13)

(cid:13)
(cid:13)

e

n

n

n

|

2

E

E

≤

(cid:0)
e
Xk,i
E(

(cid:1)
e
Xk,j)q
+ ≤

e

e

≤

Xk=1
n

Xk=1

k,j ≤

q

Xk,j|
e

Xk=1

≤

Xk,i

E|

e

Xk=1
n

Xk=1






(cid:0)
X )q

(η2

n

2

−

(cid:1)
Xk,j)2

E(

Xk,i

nM

·

≤

(η2

X)q

2,

−

q
∀

≥

3

.

Xk=1

e

e

Thus, by the version of Bernstein’s inequality given in Theorem 2.10 in [14], we obtain

X T
k

max := R11 + R12.

[d], recall that the

n

Xk,i

Xk=1

1
n

P

(cid:16)(cid:12)
(cid:12)

Xk,j − E
e

Xk,i

Xk,j

>

2Mt
n

+

η2
Xt
n

r

exp(

t),

−

∀

≤

t > 0.

(cid:17)

e
Moreover, we can use an union bound and get

e

e

1
2

(cid:12)
(cid:12)

P

R11 >

(cid:16)

2Mt
n

+

r

η2
Xt
n

(cid:17)
n
log d

d2

exp(

t),

−

∀

·

t > 0.

≤

Thus, we set t = δ log d and plug in ηX ≍
we have R11 .

δ log d

n . We now turn to R12 and have the (i, j)-th entry bounded by

(cid:1)

(cid:0)

1/4

, then with probability at least 1

2d2
−

δ

−

E

q
Xk,iXk,i −
(cid:0)

Xk,i

Xk,j

Xk,iXk,j|

1(

Xk,i|

|

≤ E|

> ηX) + 1(

Xk,j|

|

> ηX)

.

(cid:12)
The two terms can be bounded likewise, so we only deal with one of them by Cauchy-Schwarz
(cid:12)
inequality and (3.21):

(cid:1)(cid:12)
(cid:12)

e

e

(cid:1)

(cid:0)

Xk,i|

|

> ηX )

≤

E|

Xk,iXk,j|1(
1
2

≤ r
Therefore, with high probability we have

E|

(cid:0)

Xk,i|

4 + E|

Xk,j|

Xk,iXk,j|
Xk,i|
η4
X

≤

4

2

P(

q
M
η2
X

.

Xk,i|

|

> ηX)

δ log d
n

.

r

E|

q

E|

4

s

(cid:1)

R1 ≤ k

Σ ˜X ˜X −
b

ΣXX kmaxk

Θ∗

k1 . (R11 + R12) .

δ log d
n

.

r

53

Bound of R2. We consider the j-th entry. Note that γ > ηY , Lemma 1 gives

˙Yk

·

YkXk,j)

Yk|
Xk,j −
|E(γ
By Cauchy-Schwarz inequality, (3.21) and the value of ηY , we obtain
e

Xk,j −
e

YkXk,j|1(

YkXk,j)

|E(

| ≤ E

Yk

=

e

(cid:0)

|

|

|

> ηY ) + 1(

Xk,j|

|

> ηX )

.

(cid:1)

YkXk,j|1(

|

Yk|

E|

> ηY )

≤

E|

2

YkXk,j|

· P(

|

Yk|

> ηY )

M
η2
Y

≤

.

δ log d
n

1
3

.

(cid:17)

(cid:16)

q
YkXk,j|1(
1/3.

|

Similarly, it holds that E|
[d], we obtain R2 .
j
∈

δ log d
n
Bound of R3. We consider the j-th entry ﬁrst. Recall that
(cid:1)
√M , thus we have

X 2

Xk,j > ηX)

≤

(cid:0)

2

E

M
η2
X

.

δ log d

n . Since this is valid for any

q

|

Xk,j| ≤
e

ηX , and by (3.21) we know

Xk,j|

≤

k,j ≤ E|
n

e

γ

E

(cid:0)
E(γ

·

·

˙Yk

Xk,j

2 = γ2

˙Yk

(cid:1)
e
Xk,j)q
+ ≤

γq

e

n

Xk=1
n

Xk=1

e
Xk,j|
E|
e

Xk=1
n

Xk=1






X 2

E

k,j ≤

n√M γ2

(B.21)

n√M γ2(γ

q

≤

ηX)q

2,

−

·

q

∀

≥

3.

Now, we can invoke the Bernstein’s inequality given in Theorem 2.10 in [14] and obtain

n

1
n

Xk=1

˙Yk

γ

·

P

(cid:16)(cid:12)
(cid:12)

Xk,j − Eγ
e

(cid:12)
(cid:12)

˙Yk

Xk,j

> γ

·

2√M t
n

s

+

γ

·

ηX t
n

e
Thus, for some absolute constant hidden behind ”&”, a union bound gives

exp(

t),

−

∀

≤

t > 0.

(cid:17)

R3 & γ

P

t
n

γ

·

+

t

ηX ·
n

r

(cid:17)
(cid:16)
We set t = δ log d and plug in our choices ηX ≍
R3 .
holds with probability at least 1
Now combining the upper bounds for Ri, 1

δ log d
n

1/3

d

·

≤

exp(

t),

−

∀

t > 0.

(B.22)

n
δ log d
d1
δ.
(cid:1)
−
i

(cid:0)
−
≤

1/4

and γ

≍

1/6

n
δ log d

, it yields that

(cid:0)

(cid:1)
3 and (B.20), we can choose λ =
ΣY Xkmax. Note that
k1 ≤
b

∆

b

k

k

(cid:0)
δ log d
n

(cid:1)
≤
1/3 with suﬃciently large C12 to guarantee λ

C12
ΣXX Θ∗ = EYkXk. By Corollary 2 under the same probability we have (3.13), i.e.,

Σ ˜X ˜X Θ∗

≥

−

2

10s

(cid:0)
1
2−q

(cid:1)
∆
k

k

2−2q
2−q
2

.

To invoke Corollary 2 we still need to establish the RSC (3.14). Note that our choice of the
truncation parameter ηX is the same as [43], so we can use Lemma 2(b) therein2. Combining
b
with λmin(ΣXX)

2κ0 and (3.13), it gives

b

≥

(cid:16)

∆T

Σ ˜X ˜X

∆

P

2κ0k

∆
k

2
2 −

≥

D1

r

δ log d
n

2
2−q

s

∆
k

k

4−4q
2−q
2

d2
−

√δ.

1

−

≥

(cid:17)

We assume the above event holds, and divide the discussion into two cases.

b

b

b

b

b

2We mention that this result is presented with the probability term reversed in both Arxiv and Journal

54

Cases 1. If D1

κ0k
the RSC (3.14). Therefore, we can use (3.15) in Corollary 2 and plug in λ

κ0k

Σ ˜X ˜X

∆
k

∆
k

∆
k

2
2, we have

∆T

q

δ log d
n s

∆

≥

≤

k

2
2, thus conﬁrming
1/3 to
δ log d
n

b

b

b

b

≍

2
2−q

4−4q
2−q
2

yield the error bound
Cases 2. Otherwise, we assume

k

b
k2 . √s
∆

δ log d
n

(cid:16)

(cid:17)

b
(1

−

q
2 )/3

:= B1.

b

(cid:0)

(cid:1)

(B.23)

δ log d
n

2
2−q

s

∆
k

k

4−4q
2−q
2

> κ0k

∆
k

2
2.

D1

r

With no loss of generality we assume

∆

b
= 0. If q = 0, under the scaling that s

b

δ log d
n

q/2

1

−

is suﬃciently small, (B.23) can imply
b
generality, then (B.23) gives

∆ = 0. Thus, we assume q

∈

(0, 1) without losing

(cid:0)q

(cid:1)

k2 .
∆

k

s

δ log d
n

b
1

−

q/2

1/q

1
q

= s

δ log d
n

1
2q −

1
4

:= B2.

Therefore, we obtain

b

suﬃciently small we estimate B2
B1

(cid:16)r
h
k2 . max
∆
{
as

k

(cid:17)
i
B1, B2}

b

B2
B1

1
6

= s

s

h

r
(cid:0)

δ log d
n

Therefore, we arrive at the desired upper bound

(3.13), the bound for

k1 follows.
∆

k

(cid:16)
= B1 max

(cid:17)
1, B2
B1

. When s

δ log d
n

1

−

q/2 is

(cid:8)

(cid:9)

(cid:0)q

(cid:1)

q
2

1

−

1
q −

2
3

i
k2 . s
∆

(cid:1)

k

b

1

6 .

s

δ log d
n

≤

2
3

q/2)/3

(1

−

. Combining with

(cid:16)

(cid:17)

(cid:3)

C Proofs: Low-rank Matrix Completion

b

C.1 Sub-Gaussian Data

Proof of Lemma 3. I. We ﬁrst prove several facts that would be frequently used later.
Fact 1: EX T
Since Xk and X T
we can assume Xk = ek(i)eT

k follow the same distribution, we only calculate EX T

[d]). Then we calculate that

k Xk = EXkX T

k(j) where (k(i), k(j))

k = Id/d.

k Xk. Equivalent to (4.2)

uni([d]

×

∼
k(i)ek(i)eT

k(j) = Ek(j)ek(j)eT

k(j)

EX T

k Xk = Ek(i),k(j)ek(j)eT
d

=

d−

1ek(j)eT

k(j) = Id/d.

Xk(j)=1
Rd

Fact 2: Given random matrix A
B
1

, by using

d, then

×
U T BV , we have

∈

kEA

}

k

kop = supU,V
kEA

kop = sup

U,V

∈S

E[U T AV ]

≤ E[ sup

U,V

∈S

U T AV ] = Ek

A

kop

∈S

kop ≤ Ek

A

kop. Let

=

x

{

∈

S

Rd :

x
k2 =

k

versions of [43], but the proof therein is ﬁne and can yield what we need here.

55

6
d, then
Fact 3: Given random matrix A
kE(A
kop.
We only show the ﬁrst inequality, the second follows likewise. By calculation we have

Rd
kop ≤ kEAAT

− EA)T (A

− EA)(A

− EA)T

kop ≤ kEAT A

− EA)

kE(A

∈

×

kop,

kE(A

kEAT A
− EA)T (A
where we use the positive semi-deﬁniteness of EAT
II. We now start the proof. We ﬁrst note that

− EA)

kop =

EA

− EAT
EA and EAT A

kop ≤ kEAT A
− EAT

kop,
EA.

ΣY X = E(YkXk) = E(

Xk, Θ∗

h

i

Xk + ǫkXk) = E(

Xk, Θ∗
h

i

Xk),

so by using triangle inequality we obtain

n

1
n

E

(cid:13)
(cid:13)
(cid:13)

Xk=1
(γ

(cid:2)
˙Yk −

·

(cid:2)

Xk, Θ∗

h

γ

·

i −

˙Yk

Xk

Yk)Xk

op ≤

(cid:13)
(cid:13)
n
(cid:13)

h

Xk=1

(cid:2)

(cid:3)

+

1
n

(cid:13)
(cid:13)
(cid:13)

op

(cid:3)(cid:13)
(cid:13)
(cid:13)

n

1
n

Xk=1
(cid:13)
(cid:13)
(cid:13)
Xk, Θ∗

˙YkXk

γ

·

− E(γ

·

˙YkXk)

+

op

(cid:2)
Xk

i

− E(

Xk, Θ∗
h

i

Xk)

(cid:3)(cid:13)
(cid:13)
(cid:13)
op

(cid:3)(cid:13)
(cid:13)
(cid:13)

(C.1)
Bound of R1. We intend to use matrix Bernstein inequality (See Theorem 6.1.1 in [84]) to
Sk :=
bound R1. Consider a ﬁnite seqnence of independent, zero-mean random matrices
γ

, and by Fact 2 we have

˙YkXk) : k

˙YkXk

[n]

(cid:8)

:= R1 + R2 + R3.

(cid:13)
(cid:13)
(cid:13)

·

·

− E(γ
Sk

k

kop ≤ k

Then we bound max
{k
kESkST
k kop ≤ kEγ2
·
Thus, we have

γ

∈
˙YkXk
·
· ESkST
n
XkX T

(cid:9)
kop +
k kop,
k kop ≤

kE(γ
n

·
· EST

˙YkXk)

kop ≤

γ + Ek

γ

·

˙YkXk

2γ.

kop ≤

k Sk

k
γ2/d, and similarly it holds that

. By using Fact 1 and Fact 3. we have
kEST
γ2/d.

kop ≤

k Sk

kop}

n

ν

Sk

:= max

Xk=1

(cid:0)

(cid:1)

n

· ESkST

k kop,

n

k

· EST

k Sk

{k

kop} ≤

nγ2
d

.

By using matrix Bernstein inequality, for any t > 0 we have

P(R1 ≥

t)

≤

2d exp

nt2
2γ[γ/d + 2t/3]

.

(cid:17)

−

(cid:16)

(C.2)

We let t = 2γ

δ log(2d)
nd

, when δd log(2d)

n

< 9/16 it holds that

q

P

R1 ≥
(cid:16)

≤

2γ

r

2d exp

δ log(2d)
nd

ndt2
4γ2
(cid:17)
˙Yk −
Yk)Xk]
kmax. Let Xk,ij denotes
[d], then the distribution of Xk,ij is
2, otherwise Xk,ij = 0. Also, we let Θ∗ij be the (i,j)-th entry of Θ∗.

(cid:16)
kE[(γ
·
[d]
×
∈

(2d)1
−

(C.3)

≤

−

δ.

(cid:17)

= Yk. Furthermore, it holds that

Bound of R2. We ﬁrst bound the max norm error
the (i, j)-th entry of Xk. Consider speciﬁc (i, j)
given by P(Xk,ij = 1) = d−
When

γ

˙Yk

Yk|
|
|E(γ
·
2
= d−

< γ by Lemma 1 we have EΛk
˙Yk −
Yk)Xk,ij| ≤ E|
Yk| ≥
Xk,ij 1(
Yk|
|
|
(cid:2)

Yk|
γ)

(cid:0)
Xk,ij 1(
|
Xk,ij = 1

E

·
(cid:1)
γ) = E
Yk| ≥
2

= d−

E

|
(cid:2)

(cid:12)
(cid:12)

(cid:3)
56

Xk,ij 1(

E

Yk|
|
Θ∗ij + ǫk|1(
(cid:0)
(cid:2)

|

|

Yk| ≥
γ)
Θ∗ij + ǫk| ≥

Xk,ij
.
γ)
(cid:12)
(cid:12)

(cid:3)(cid:1)

(cid:3)

By (4.5) we have

Θ∗ij| ≤

|

α∗, recall that γ

2α∗, so

|

≥

|

Θ∗ij + ǫk| − |
ǫk| ≥ |
α∗. Moreover, we obtain

Θ∗ij| ≥

which implies

ǫk| ≥

|

Θ∗ij + ǫk| ≥
γ
2

α∗

−

≥

γ

γ implies

,

Θ∗ij + ǫk| ≤ |

Θ∗ij|

|

+

ǫk| ≤

|

α∗ +

ǫk| ≤

|

2

.

ǫk|

|

Thus, we apply Cauchy-Schwarz inequality, (4.12), Proposition 1, it gives

ǫk| ≥

|

γ
2

)

(cid:3)

|E(γ

·

˙Yk −

Yk)Xk,ij| ≤

2

d−

E

|

≤

2

2d−

Eǫ2
k

P

q

r

ǫk| ≥

|

γ
2

Θ∗ij + ǫk|1(
2σ exp

(cid:2)
. d−

|

Θ∗ij + ǫk| ≥
D1γ2
σ2

−

γ)

≤

(cid:3)
. d−

1σ

2

2d−

E

ǫk|1(
|
(cid:2)
δ log(2d)
nd

,

r

(cid:16)
where the last ”.” follows from γ given in (4.12) with suﬃciently large C13. Since the esti-

(cid:17)

(cid:0)

(cid:1)

mation holds for any (i, j), we have
between

.
kop and

k

.
kmax, it further gives

k

kE[(γ ˙Yk −

Yk)Xk]

kmax ≤

d−

1σ

δ log(2d)
nd

. By the relation

q

R2 ≤

d

· kE[(γ

·

˙Yk −

Yk)Xk]

kmax . σ

δ log(2d)
nd

.

r

Bound of R3. Similar to R1 we use matrix Bernstein inequality. We consider the ﬁnite inde-
pendent, zero-mean random matrix sequence

Wk :=

Xk, Θ∗

Xk

h
α∗, so

i
Xk, Θ∗

− E(
Xk

Xk, Θ∗
h

i

Xk) : k

[n]

.

∈

Note that

Xk, Θ∗

| h

(cid:8)
i | ≤

Wk

k

kop ≤ k h

k h
Xk, Θ∗

Xk

i

By using Fact 1 and Fact 3, we obtain
kEW T
Likewise we have

kop ≤

(α∗)2
d

k Wk

α∗, by Fact 2 we have

(cid:9)

kop ≤

Xk, Θ∗

i
kop +
kE h
kEWkW T
, so we derive the bound

kop ≤
k kop ≤ kE hXk, Θ∗i2 XkX T

Xk

2α∗

2γ.

≤

i

(α∗)2
d

.

k kop ≤

n

ν

Wk

Xk=1

(cid:0)

(cid:1)

:= max

{k

n

· EWkW T

k kop,

n

k

· EW T

k Wk

kop} ≤

n(α∗)2

d ≤

nγ2
d

.

Parallel to R1, by using Matrix Bernstein inequality and set t = 2γ

δ log(2d)
nd

,

q

R3 ≥

2γ

r

δ log(2d)
nd

(cid:17)

P

(cid:16)

(2d)1
−

δ.

≤

(C.4)

We combine the obtained upper bounds for R1, R2, R3 and draw the conclusion that with
probability higher than 1

δ, we have

2d1
−

−
n

h

Xk=1

(cid:2)

1
n

(cid:13)
(cid:13)
(cid:13)

Xk, Θ∗

γ

·

i −

˙Yk

Xk

(cid:3)

57

δ log d
nd

.

. γ

r

op

(cid:13)
(cid:13)
(cid:13)

Now we can use γ

C13 max

α∗, σ

{

}

≤

√log n to conclude the proof.

(cid:3)

Proof of Lemma 4. I. We ﬁrst decompose the complementary event of (4.15) which can
be stated as B =
X (Θ) =
2
d−

Θ0
F, so B implies the following event

. Note that EF

(ψ), s.t.

X (Θ0)

2
F −

T0}

κd−

∈ C

Θ0

{∃

≤

F

k

k

2

2

Θ
k

k

Θ0

(ψ), s.t.

X (Θ0)

X (Θ0)

(1

κ)d−

2

Θ0

{∃

∈ C

| ≥
Let D0 = (α∗d)2(ψδ log(2d)/n)1/2, then by (4.14) we have
2
F ≥
(that will be selected later), there exists positive integer l such that
(ψ)
We further consider

1D0, βlD0)

(ψ, l) =

−
Θ0

− EF

k
D0, so by a speciﬁc β > 1
1D0, βlD0).
[βl
−

2
F ∈
, and deﬁne a term

Θ :

Θ0

[βl

|F

k

k

k

k

k

−

2

F + T0}

.

(C.5)

C

∩ {

C
X (l) = sup

Z

(ψ,l) |F

Θ

∈C

k

2
Θ
F ∈
k
X (Θ)

}
,

− EF

X (Θ)

|

then we know the event deﬁned in (C.5) implies the event

Bl =

{Z

X (l)

(1

κ)d−

2βl

−

≥

−
N∗ we obtain P(B)

1D0 + T0}

.

(C.6)

By taking the union bound over l
II. It suﬃces to bound P(Bl). We ﬁrst bound the deviation
P
X = (

X (l)
|
X1, X2, ..., Xn) where only the ﬁrst component may be diﬀerent from X

∞l=1 P(Bl).
X (l)
|Z

− EZ

≤

∈

. We consider

f
sup
X ,

f
fX |Z

X (l)

fX (l)

− Z

|

= sup
fX
X ,

sup

(ψ,l) |F

X (Θ)

− EF

X (Θ)

| −

sup

Θ

∈C

(ψ,l) |F

fX (Θ)

fX (Θ)

− EF

sup
fX
X ,

≤

sup

(ψ,l) |F

Θ

∈C

X (Θ)

− F

(cid:12)
(cid:12)
(cid:12)

= sup
fX1
X1,

sup

Θ

∈C

(ψ,l)

1
n

X1, Θ

X1, Θ

2

2

− |

|
(cid:11)

(cid:10)

f

|
(cid:11)

(cid:12)
(cid:12)
(cid:12)

(cid:10)

(cid:12)
(cid:12)
(cid:12)

Note that n components of X are symmetrical, by bounded diﬀerent inequality (e.g., Corollary
2.21, [88]), for any t > 0 we have

|

(cid:12)
(cid:12)
(cid:12)
.

4(α∗)2
n

≤

Θ

∈C

(cid:12)
(cid:12)
(cid:12)
fX (Θ)

|

(cid:12)
(cid:12)
(cid:12)

X (l)

− EZ

X (l)

t

≥

P

Z

≤

exp

nt2
8(α∗)4

−

.

(C.7)

(cid:16)
X (l). Let E = (ε1, ..., εn) be i.i.d. Rademacher random variables
It remains to bound EZ
satisfying P(εk = 1) = P(εk =
1) = 1/2, then by symmetrization of expectations (e.g.,
Theorem 16.1, [85]), Talagrand’s inequality (e.g., Theorem 16.2, [85]), the second constraint
in (4.14), it yields that

−

(cid:17)

(cid:16)

(cid:17)

X (l) = E sup

EZ

(ψ,l) |F

Θ

∈C

X (Θ)

− EF

X (Θ)

|

= E sup
Θ

(ψ,l)

∈C

n

εk

Xk, Θ

2

1
n

2EX EE

sup

Θ

(ψ,l)

∈C
n

Xk=1

(cid:12)
(cid:12)
(cid:12)
εkXk

(cid:10)
sup

16α∗E

≤

(cid:11)
Θ

(cid:12)
(cid:12)
(cid:12)
knu ≤

16α∗E sup

Θ

∈C

(ψ,l)

160α∗r

1
2−q

{

≤

≤

1
n

(cid:13)
(cid:13)
(cid:13)

Xk=1

op

Θ

∈C

(ψ,l) k

(cid:13)
(cid:13)
(cid:13)

1
n

(cid:12)
(cid:12)
(cid:12)

n

Xk=1
1
n

Xk=1

1−q
2−q

(cid:12)
(cid:10)
(cid:12)
(cid:12)
βlD0}

Xk, Θ

2

− E

Xk, Θ

2

(cid:11)

(cid:9)(cid:12)
(cid:12)
(cid:12)

.

(cid:8)(cid:10)
n

(cid:11)
εkXk, Θ

(cid:10)

n

(cid:11)(cid:12)
(cid:12)
(cid:12)
εkXk

1
n

E

(cid:13)
(cid:13)
(cid:13)

Xk=1

op

(cid:13)
(cid:13)
(cid:13)

(C.8)
Assume d log(2d)/n < 1/16, by matrix bernstein inequality (Theorem 6.1.1, [84]) it holds that

58

E

1
n

n
k=1 εkXk

(cid:13)
(cid:13)
(cid:13)

P

op ≤

(cid:13)
(cid:13)
(cid:13)
EZ

3
2

log(2d)
nd

q

. We then plug it in (C.8), some algebra yields

X (l)

(2

−

q)T0

≤

1
2−q

d−

2βlD0

1−q
2−q

(cid:8)

(cid:9)

(cid:8)

1
2

−
−

≤

q
q

βlD0
d2 + T0

By combining with (C.6), (C.7) and let κ1 = 1
κ
−
β −
can choose κ suﬃciently close to 0, β suﬃciently close to 1), we have

q (here we assume κ1 ∈

q

(cid:9)
1
−
2
−

(C.9)

(0, 1) since we

P(Bl)

≤ P

X (l)

Z

(cid:16)

X (l)

− EZ

βlD0
d2

κ1

≥

exp

−

(cid:16)

≤

(cid:17)

nκ2
1β2lD2
0
8(α∗d)4

.

(cid:17)

(C.10)

We further plug in D0 and use β2l

2l log β, it yields that

≥

P(B)

≤

∞

P(Bl)

≤

∞

ψδκ2
1 log β
4

l

(2d)−

Xl=1
(cid:3)
the last inequality holds since we can let ψ be large such that ψ

Xl=1

(cid:2)

d−

δ,

≤

4(κ2

1 log β)−

1.

≥

(cid:3)

Proof of Theorem 11. I. By Lemma 3 we can choose suﬃciently large C14 in (4.17) to ensure
δ, then (4.8) holds with high probability. From
2d1
(4.7) holds with probability higher than 1
−
δ to ensure (4.15) holds.
Lemma 4 we can further rule out probability d−
kmax +

2α∗. Thus, the estimation
(ψ),
(ψ) holds only depends on the third constraint, and let us discuss as

kmax ≤
(ψ). Since (4.8) displays the second constraint in

kmax ≤ k
C

Θ∗

Θ

−

b

C

k

k

∆
By (4.5) and (4.6) we have
error satisﬁes the ﬁrst constraint of
b
(ψ)
whether
follows:
Case 1.

∈ C

∈ C

∆

(ψ). Note that it can only violate the third constraint of

(ψ), so we know

. Under the assumption r & dq, n . d2 log(2d), it holds that

C

∈ C

∆ /
b
2
F ≤
k
b

(α∗d)2

ψδ log(2d)
n

q

that

∆

k

b

2
F

. (α∗)2

δ log(2d)
n

r

. (α∗)2 δd log d

n

. rd−

q

(α∗)2 δd log d

q/2

1

−

.

(C.11)

n

(cid:16)
∆

k

k

2
F −

(cid:17)
T0. If T0 ≥

1
2κd−

2

∆
k

k

2
F, then

b

(C.12)

Case 2.
∈ C
we plug in T0 and obtain

(ψ). By (4.15) we know

X (

∆)

F

≥

2

κd−

2
F

. rd−

q

b
b
(α∗)2 δd log d
(cid:16)

n

(cid:17)

q/2

1

−

.

k

∆
k
d2
b
2
F, then we have

If T0 ≤
≥
(4.9), so we now use Corollary 3 and obtain

∆)

X (

∆

F

k

k

1
2κd−

2

1
2κd−

2

∆
k

k

2
F. Note that this displays the RSC in

b

k

2
F

b

b

. rd−

q

max

(cid:16)

(α∗)2, σ2

{

}

log d log n

δd
n

∆
k
d2
b

q/2

1

−

.

(C.13)

Now we can see that in all cases considered above, the bound of
a direct application of (4.8) delivers the bound of
with probability higher than 1

F/d2 in (4.18) holds. Then
2
knu/d in (4.18). To conclude, (4.18) holds
(cid:3)

3d1
−

∆

δ.

b

k

k

(cid:17)
∆
k

−

b

59

k

∆
k
d2
b
∆

b

C.2 Heavy-tailed Data

Proof of Lemma 5. From (4.1) we have E(YkXk) = E(
˙Yk
Lemma 1 we know EΛk

Yk, hence we have

=

γ

·

Xk, Θ∗
h

i

Xk), and since η < γ by

n

Xk, Θ∗
h

(cid:0)
γ

i −

·

(cid:1)
Xk

˙Yk

e

n

γ

1
n

op ≤

Xk=1
(cid:13)
(cid:2)
(cid:13)
(cid:13)
∗
Xk
Xk, Θ

i

n

i
1
n

(cid:13)
(cid:13)
(cid:13)
h

Xk=1

(cid:2)

˙YkXk

·

− E(γ

·

˙YkXk)

− E(

∗
Xk, Θ
h

i

Xk)

+

op

(cid:3)(cid:13)
(cid:13)
(cid:13)
:= R1 + R2 + R3.

op

(cid:3)(cid:13)
(cid:13)
(cid:13)

Yk)Xk

+

op

(cid:3)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Bound of R1, R3. We use matrix Bernstein inequality (Theorem 6.1.1, [84]), and the arguments
are exactly the same as the corresponding parts in the proof of Lemma 3. As a result, one can
still invoke Matrix Bernstein to show (C.3) and (C.4), but only with diﬀerent value of γ. To
obtain the explicit form of the bounds, we further plug in γ in (4.20), with probability higher
than 1

δ it gives

2d1
−

1
n

E

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Xk=1 h
Yk −
(
e

(cid:2)

−

max

R1, R3}

{

. max
{

α∗, √M

δ log d
nd3

1/4

.

(cid:19)

}

(cid:18)

Bound of R2. Let Xk,ij be the (i, j)-th entry of Xk, where (i, j)
|E(
bound the element-wise error
gives

Yk)Xk,ij|

[d] is ﬁxed, we ﬁrst
[d]
. Recall the deﬁnition of truncation, Lemma 1

×

∈

(

=

|E

Yk)Xk,ij

Yk −
e

Yk|
|
| ≤ E|
(cid:3)
2. Let Θ∗ij be the (i, j)-th entry of
Note that Xk,ij can only be 1 or 0, and P(Xk,ij = 1) = d−
Θ∗, we further compute it via law of total expectation, then use Cauchy-Schwarz inequality
and Marcov’s inequality, ﬁnally plug in η ﬁnally. These steps deliver

Yk)Xk,ij 1(

Xk,ij 1(

> η).

Yk|

Yk|

> η)

|
(cid:3)

|E

(cid:2)

(cid:2)

|

Yk −
e
Yk −
(
e

Yk|
E|
= d−

2

Xk,ij 1(

E

E

(cid:16)
> η)

> η) = E

Yk|
Xk,ij 1(

Yk|
|
|
h
Xk,ij = 1
Yk|
|
(cid:12)
h
(cid:12)
Θ∗ij + ǫk|
Θ∗ij + ǫk| ≥
(cid:12)
E|
|
q
1[(Θ∗ij)2 + Eǫ2
2η−
4(dC15)−
k]

Yk|
P(

η)

|
2

2

d−

2d−

Xk,ij 1(

> η)

Yk|
|
= d−

2

E

1

2η−

|
h
E|
α∗, √M

d−

i
≤
1 max
{

≤

≤

≤

Xk,ij
(cid:12)
i(cid:17)
Θ∗ij + ǫk|1(
(cid:12)
(cid:12)
Θ∗ij + ǫk|
δ log d
nd3

}

2

(cid:16)

(cid:17)

Θ∗ij + ǫk| ≥

|

η)

i

1/4

Since the above analysis works for all (i, j)
[d]
norm, which delivers a bound for operator norm

∈

×

[d], this is also an upper bound for the max

R2 ≤

d

(

E

Yk)Xk

. max

{

max

α∗, √M

(cid:2)
The result follows from the upper bounds for R1, R2, R3.

(cid:3)(cid:13)
(cid:13)
(cid:13)

Yk −
e

(cid:13)
(cid:13)
(cid:13)

δ log d
nd3

1/4

.

(cid:17)

}

(cid:16)

(C.14)

(cid:3)

Proof of Theorem 12. By Lemma 5 we can choose suﬃciently large C17 in (4.22) to ensure
δ, then it further implies (4.8), meaning that
2d1
(4.7) holds with probability higher than 1
−
(ψ). From Lemma 4 we can further rule out probability
∆ satisﬁes the second constraint of

−

C

b

60

(ψ), then by exactly the same analysis in proof of Theorem 11, we obtain
b

k

δ,

−

∆

∆ /

3d1
−

∆ satisﬁes the ﬁrst two constraints of

δ so that (4.15) holds. Evidently we have

d−
than 1
conditions we further discuss as follows:
Case 1.
(C.11).
Case 2.
holds. Otherwise, we have the restricted strong convexity
apply Corollary 3 and plug in λ, it holds that

kmax ≤
C

(ψ), then we have

2
F −

∆
k

∈ C

κd−

∈ C

∆)

X (

b
∆

≥

F

b

b

b

b

k

2

2α∗. Thus, with probability higher
(ψ), and (4.15) holds. Based on these

T0. If T0 ≥
∆)
X (
F

2

1
2κd−
k
1
2 κd−

≥

2
F, then (C.12)
∆
k
2
2
∆
F. We then
k
k
b

b
δd log d
n

q/2

1

−

.

b

(C.15)

F/d2 . rd−
2

q

max

∆
k

k

(α∗)2, M

{

}r

(cid:0)

(cid:1)

b
It is not hard to see that the right hand side of (C.15) dominates the bound in (C.11) and
knu/d follows from a
(C.12), so the bound for
k
(cid:3)
direct application of (4.8).

F/d2 in (4.23) holds. The bound for
2

∆

∆

k

k

b

b

D Comparisons with Related Work

D.1

1-bit Compressed Sensing

In this part we compare our Theorem 9, 10 with existing results of 1-bit CS. Among improve-
ments of diﬀerent perspectives, we would emphasize our contributions of extending 1-bit CS to
non-Gaussian sensing vectors, and the ﬁrst tractable recovery convex programming recovery
method for heavy-tailed sensing vectors.

The traditional setting of 1-bit CS, where one needs to recovery a sparse d-dimensional
signal Θ∗ based on measurement ˙Yk = sign(X T
k Θ∗) with some Xk, was ﬁrst introduced in [15]
and widely studied in subsequent works (e.g., [50,69,70]). By projection-based method [72] or
K-Lasso [71], similar results have been obtained for a model with more general observation and
signal assumption. Nevertheless, all these results are restricted to Gaussian sensing vectors
that can be unrealistic in practice3. There does exist one work, [1], presents result for Xk with
i.i.d. sub-Gaussian entries. However, the result in [1] is overly restrictive and impractical, see
the discussions in [38] for more details.

To overcome the Gaussian restriction (and also some other limitations), recent works show
introducing dithering noise can go a long way. With dithering noise Λk, the measurement
now becomes ˙Yk = sign(X T
k Θ∗ + Λk). In this setting, speciﬁcally, we can recover the signal
with norm information [58], achieve exponentially-decaying error rate (This requires adaptive
dithering) [5], and perhaps more prominently, accommodate non-Gaussian Xk [37, 38, 83]. In
what follows, we will focus on comparing Theorem 9, 10 with the most related works [38, 83]
that adopt uniform dithering noise (by contrast, [5, 58] use Gaussian dithering noise). The
comparisons will be conducted on exactly sparse Θ∗ since [38,83] do not adopt the formulation
(0, 1) for approximately sparse Θ∗. For other developments on 1-bit CS (or
more generally, quantized compressed sensing), we refer readers to the survey papers [16, 35].
P
Dirksen and Mendelson [38] ﬁrst essentially extend 1-bit CS to non-Gaussian Xk. Their
methodology is aligned with classic works like [70], that is, to start from the viewpoint of

n
k=1 |

θ∗k|

s, q

≤

∈

q

3More precisely, [71] handles Xk

Xk

∼ N

(0, Id).

∼ N

(0, Σ) with unknown Σ while other several papers above assume

61

random hyperplane tessellation. Speciﬁcally, under sub-Gaussian or even heavy-tailed Xk
with uniform dithering noise, they show a relatively small number of random hyperplanes
(that depends on the complexity of Θ∗) leads to ρ-uniform tessellation on the signal set of
interest. Moreover, they apply the new hyperplane tessellation results to 1-bit CS and propose
two reconstruction optimization problems

4

n

1(sign(X T

k Θ + Λk)

= Yk),

s.t.

Θ

k

k0 ≤

s,

Θ

k

k2 ≤

1

.

(D.1)

YkX T

k Θ,

s.t.

Θ

k

k1 ≤

s,

Θ

k

k2 ≤

1

(a) :

Θ

(b) :

b
Θ

∈

∈

arg min
Rd

Θ

∈

arg min
Rd

Θ

∈

b






Xk=1
1
2λ k

Θ

2
2 −

k

1
2n

n

Xk=1

Although (a) is shown to possess uniform recovery guarantee with fast rate in both sub-
Gaussian and heavy-tailed Xk, it is essentially intractable due to the ℓ0 constraint and the
0-1 objective function. Also, a secondary drawback is that, the information of Λk is needed
in problem (a), which induces undesired memory or transmission costs. For these reasons, (a)
is mainly of theoretical interest. To address the issue, (b) is proposed as a convex relaxation
of (a). Under sub-Gaussian Xk and ǫk, the error rate of (b) for s-sparse Θ∗ was shown to
be ˜O
delivered by
our Theorem 9. Indeed, their error rate for sub-Gaussian data is even worse than our rate
for heavy-tailed data, i.e., ˜O
heavy-tailed Xk has not yet been established.

, and this is signiﬁcantly inferior to the near optimal rate ˜O

given in Theorem 10, while the guarantee of (b) under

(cid:0)p

p

s2
n

s
n

s
n

(cid:1)

(cid:0)

(cid:1)

3

4

Besides the error rate, all results in [38] assume Xk is isotropic, i.e., ΣXX = Id. By
contrast, our theory can handle unknown ΣXX satisfying λmin(ΣXX) = Ω(1). Actually, since
Assumption 3 nicely encompasses isotropic Xk, we can even carry out the more tricky 1-bit
QC-CS under their conditions, see Theorem 7, 8 for the setting with known ΣXX.

q

(cid:0)

(cid:1)

k

Θ

We further give two side remarks to end the comparison with [38]. Firstly, it is necessary to
k1 to invoke (b), while this is avoided in our unconstrained recovery
have pre-estimation of
program. Secondly, their results are only for symmetric Xk, while we only assume zero mean.
On the other hand, the advantages of [38] due to Dirksen and Mendelson may be more general
signal set, lower moment requirement in heavy-tailed Xk, and a partial extension to structured
random measurement matrix in the companion work [37]. These are left as future research
directions of our theories.

A result directly comparable to our Theorem 9 is due to Thrampoulidis and Rawat [83].

Under almost the same setting they assume Θ∗

T and consider a constrained Lasso

∈

Θ

arg min
T
Θ

∈

n

X T

k Θ

1
2n

γ

−

·

˙Yk

2

.

(D.2)

∈

b

Xk=1
This is analogous to our convex programming problem (3.24): Up to constant, the objective
of (D.2) equals the loss function (i.e., the ﬁrst two terms) in (3.24); And the only diﬀerence
is that, the structure of Θ∗, speciﬁcally sparsity, is incorporated into (D.2) via the constraint,
but appears in (3.24) as a regularizer. The recovery guarantee is given in Theorem IV.1 in [83].
Interestingly, when restricted to exactly s-sparse Θ∗, they choose T =
k1}
and show ℓ2 norm error rate ˜O
that coincides with Theorem 9. Despite these similarities,

k1 ≤ k

Θ :

Θ∗

Θ

k

{

(cid:1)

(cid:0)

s
n

4In [38], heavy-tailed Xk is assumed to satisfy E

(cid:0)p

(cid:1)

vT Xk

|
(cid:0)

vT Xk

2

|

(cid:1)

L

≤

E|
(cid:0)

2

|

(cid:1)

for any v

Rd.

∈

62

6
v

k

2=1 E|
k

vT Xk|

our Theorem 9 exhibits several obvious improvements. Firstly, we consider pre-quantization
noise ǫk while they only study noiseless case. Secondly, we assume zero-mean Xk satisﬁes
λmin(ΣXX) = Ω(1), but [83] requires symmetric Xk to satisfy a nondegeneracy condition
formulated as inf
= Ω(1), which is more restrictive. Thirdly, their guarantee is
δ) is ﬁner. In addition,
O(d2
valid with probability at least 0.99, while our probability term 1
−
k1 is needed to specify T and invoke (D.2), while
we comment that a pre-estimation of
our unconstrained program (3.24) is free of this issue and hence is more practically appealing.
Thus, even without mentioning our result for 1-bit CS in heavy-tailed regime, Theorem 9 can
represent the all-round improvement of Theorem IV.1 in [83]. Indeed, their only advantage
T , while we believe our result straightforwardly
seems to be the more general assumption Θ∗
extends to other interesting signal structures such as (approximate) low-rankness.

Θ∗

−

k

∈
In [5, 58] the authors study ˙Yk = sign(X T

k Θ∗ + Λk) where Λk is Gaussian dithering noise.
Convex programming problems are also proposed in both papers to recover s-sparse Θ∗, but
Xk is restricted to standard Gaussian sensing vector. Speciﬁcally, the theoretical rate in
Theorem 4 of [58] reads ˜O
. This is
obviously slower than the rates presented in our Theorem 9 (sub-Gaussian Xk), Theorem 10
(heavy-tailed Xk).

, and Theorem 2 of [5] gives the guarantee ˜O

p

p

s
n

s
n

(cid:1)

(cid:0)

(cid:1)

(cid:0)

5

4

In a nutshell, this work gains signiﬁcant improvements on current results of 1-bit CS, in

terms of generality of sensing vectors, faster rate and computational feasibility.

D.2

1-bit Matrix Completion

In this part, we compare our Theorem 11, 12 with existing results on 1-bit matrix completion
(1-bit MC), a problem ﬁrst proposed and studied in [23, 32]. We shall see that, our work
presents the ﬁrst result in 1-bit MC that can handle pre-quantization random noise with
unknown distribution.

˙Yk = sign(X T

Unlike 1-bit CS where one can still recover the direction Θ∗/

k2 from the direct quan-
k Θ∗), due to the nature of the covariate in matrix com-
tized measurement
pletion (i.e., Xk = ei(k)eT
j(k)), 1-bit MC could be extremely ill-posed if we only observe
, even when Θ∗ is rank-1, see the discussion in [32]. Thus, to proceed
˙Yk = sign
the study of 1-bit MC, dithering noise (denoted by Λk) is indispensable for the well-posedness,
hence the measurement (i.e., observation) becomes

Xk, Θ∗

Θ∗

(cid:11)(cid:1)

(cid:0)(cid:10)

k

˙Yk = sign

Xk, Θ∗

+ Λk

.

Existing works consider dithering noise with rather general distribution, but mainly emphasize
Logistic model and Probit model5.

(cid:11)

(cid:1)

(cid:0)(cid:10)

Let us give a brief review of existing results. In the ﬁrst study of 1-bit MC [32], Davenport

et al. proposed to recover Θ∗ via negative log-likelihood minimization (put d1 = d2 = d)

Θ

∈

arg min
Θ

Rd×d LNLL(Θ),

∈

s.t.

Θ

kmax ≤

k

α∗,

Θ

knu ≤

k

α∗d√r .

(D.3)

In (D.3), the ﬁrst constraint is commonly used in matrix completion (see the interpretation
at the beginning of Section 4), the second constraint relaxes rank(Θ)

r via the relation

b

knu ≤
5The Probit model corresponds to Gaussian dithering noise Λk

kF ≤

k

k

k

p

kmax

rank(Θ)

Θ

d

Θ

Θ

≤
rank(Θ),

(0, σ2).
p

∼ N

63

and the loss function

LNLL(Θ) is
1
n

−

n

Xk=1 h

LNLL(Θ) =

1( ˙Yk = 1) log P(

Xk, Θ

+ Λk ≥

0)

(D.4)

+ 1( ˙Yk =

(cid:11)
(cid:10)
1) log P(

−

Xk, Θ

+ Λk < 0)

.

(cid:10)

(cid:11)

i

Some developments can be seen in subsequent works, to name a few, [23] used another sur-
rogate of matrix rank rather than the nuclear norm6, [10, 56, 62] extended 1-bit MC to ﬁnite
alphabets, [9,10,68] imposed (exactly) low-rank constraint without relaxation, [56,62] adopted
nuclear norm penalty to avoid the pre-estimation of

Θ∗

We stress that all above works are restricted to a noiseless setting, by saying this, we do not
regard Λk as a detrimental noise since the dithering is indeed beneﬁcial to the recovery. When
it comes to noise that may corrupt the recovery, we are aware of only two recent papers [46,80].
In [46], Gao et al. considered a deterministic sparse pattern S∗ mixing with the desired low-
rank structure Θ∗. Speciﬁcally, this more general ”low-rank plus sparse” model (in one-bit
setting) can be formulated as

knu needed in (D.3).

k

˙Yk = sign

Xk, Θ∗ + S∗

+ Λk

, where

vec(S∗)

k

s.

k0 ≤

(D.5)

In [80], Shen et al. studied 1-bit MC with post-quantization noise in a form of sign ﬂipping,
which can be described by

(cid:0)(cid:10)

(cid:11)

(cid:1)

˙Yk = δk ·

sign

Xk, Θ∗

+ Λk

, where P(δk =

1) = τ0, P(δk = 1) = 1

−

τ0.

−

(D.6)

(cid:0)(cid:10)

(cid:11)

(cid:1)

Evidently, for (D.5) or (D.6), as done in [46, 80], the recovery can still be based on negative
log-likelihood minimization. However, if we consider pre-quantization noise ǫk with unknown
distribution (This is a natural and well-studied situation in other statistical estimation mod-
els), i.e.,

˙Yk = sign

Xk, Θ∗

(D.7)
recovery based on likelihood no longer works due to lack of knowledge of
LNLL(Θ). Therefore,
before our work, it was an open question whether 1-bit MC under unknown pre-quantization
random noise is possible.

+ Λk + ǫk

(cid:0)(cid:10)

(cid:11)

(cid:1)

,

Our Theorem 11, 12 aﬃrmatively answer this open question. Particularly, under uniformly
distributed Λk, sub-Gaussian or even heavy-tailed ǫk, we formulate 1-bit CS as a convex
programming problem and establish theoretical guarantee. This is due to a novel loss function
deviating from existing papers, that is, we now use a generalized quadratic loss (see (4.6))

(Θ) =

L

1
2n

n

Xk, Θ

˙Yk

2.

γ

·

−

(D.8)

Xk=1

(cid:0)(cid:10)

(cid:11)

(cid:1)

uni

To see the essential diﬀerence, one may compare (D.8) and the explicit form of (D.4) when
. For the core idea behind, while maximum likelihood estimation is a quite
Λk ∼
˙Yk can
standard estimation strategy, the inspiration of (D.8) is drawn from Lemma 1, i.e., γ
serve as a surrogate of the full observation Yk =

Xk, Θ∗

+ ǫk.

γ, γ]

[
−

(cid:0)

(cid:1)

·

6This rank surrogate is called max-norm but totally diﬀerent from
(cid:11)

(cid:10)

max in our work. To avoid confusion,

.

k

k

64

At ﬁrst glance, one may feel that (D.8) is a bit coarse compared with negative log-likelihood,
but it is striking that under sub-Gaussian noise our estimator achieves near minimax rate
(Theorem 11). For comparison, we go back to the noiseless (i.e., ǫk = 0 in (D.7)) and exactly
low-rank (i.e., q = 0 in (4.3)) case. In this case, Theorem 11 gives a bound ˜O
for
mean squared error. This is faster than ˜O
and similar to the more recent paper [62].
(cid:0)

q
To sum up, we present the ﬁrst result for 1-bit MC with unknown pre-quantization ran-
In addition, by some extra
dom noise, which can either be sub-Gaussian or heavy-tailed.
technicalities, we believe our method can be extended to both deterministic sparse corruption
in [46] and sign ﬂipping noise in [80]. On the other hand, our restriction is that Λk should be
uniformly distributed, and it would be an interesting open question whether our method can
be extended to other dithers.

obtained in two pioneering works [23,32],

(α∗)2 rd
n

(α∗)2

rd
n

(cid:1)

(cid:0)

(cid:1)

E Details and Algorithms in Experiments

E.1 Sparse Covariance Matrix Estimation

Simulation Details. To generate the d
Assumption 1 with q = 0 and sparsity s, we ﬁrst construct

×

d underlying covariance matrix Σ∗ that satisﬁes

Σ∗

0 =

Σ∗
1
0

0
Id−3s

,

(cid:19)

(cid:18)
3s, and Σ∗
s are deﬁned as σ∗2,ii = 1 for
0.03, σ∗2,ij = 0.03 for all other entries. By normalizing

2 = [σ∗2,ij]

Rs

∈

×

where Σ∗
1 = diag(Σ∗
[s], σ∗2,12 = σ∗2,21 = 0.99
i
the operator norm, we set

2, Σ∗

∈

2, Σ∗
2)

−

∈
(s

R3s
×
2)

−

·

Σ∗ =

Σ∗
0
Σ∗

k

0kop

.

(0, Σ∗), and draw heavy-tailed Xk from Student’s t
We i.i.d. draw sub-Gaussian Xk ∼ N
)” with ν = 6. Then, we apply the one-bit
distribution via the Matlab function ”mvtrnd(
quantization scheme with parameters slightly tuned to be well-functioning, to obtain the
. Now, we can directly construct the 1-bit estimator
binary data
In our results, each
Σ deﬁned in (2.4), (2.18), and track the experimental recovery error.
experiment is obtained as the mean value of 15 independent runs.
b
E.2 Sparse Linear Regression

[n], j = 1, 2

˙Xkj : k

∈

(cid:9)

(cid:8)

·

Simulation Details. We conduct numerical experiments of 1-bit QC-CS (Theorem 7, 8) and
1-bit CS (Theorem 9, 10). We consider isotropic covariate (i.e., EXkX T
k = Id) that is kind
of convention in compressed sensing (e.g., [25, 72]), which admits Assumption 3 required for
1-bit QC-CS. For the covariate, speciﬁcally, sub-Gaussian Xk are generated from Gaussian
distribution, while entries of heavy-tailed Xk are i.i.d. drawn from
t(ν = 6). Here,

t(ν = 6) represents Student’s t distribution with 6 degrees of freedom, and

2
3 ·

q

2
3 aims to

q

we refer readers to [23] for the details.

65

[n]
}

∈

{
˙Xk2, ˙Yk) : k

normalize the variance. We set the ﬁrst s entries of Θ∗ to be 1
√s, while other entries are 0,
hence Θ∗ is (exactly) s-sparse. Sub-Gaussian and heavy-tailed noise ǫk are respectively drawn
t(ν = 6). All these parameters specify the model, so we can generate
from
N
the full data
q

3
5 ) and 0.3
·
(Xk, Yk) : k

for a speciﬁc (n, d, s).

(0,

∈

[n]
}

(Xk, Yk) : k

(Xk, ˙Yk) : k

Then we apply the one-bit quantization scheme to quantize
in 1-bit QC-CS, or

to
( ˙Xk1,
in 1-bit CS. All parameters
{
{
are properly set, and we stress that the truncation and dithering parameters for Xk, Yk are dif-
ferent. For instance, in sub-Gaussian 1-bit QC-CS we use dithering noise Λk ∼
,
= γY . After the data quantization, we can solve the pro-
Γkj ∼
(cid:1)
Θ. We track the ℓ2 norm error
posed convex programming problems to obtain the estimator
(cid:0)
k2 and report the mean value of 15 independent runs.

k
Algorithm. Note that the convex programming problems (3.17), (3.26) and (3.30) share a
common formulation

with γX 6

γX , γX]d

[n]
}

[n]
}

γY , γY

[
−

[
−

uni

uni

Θ∗

−

Θ

∈

∈

b

b

{

(cid:0)

(cid:1)

Θ

arg min
Rd

Θ

∈

1
2

ΘT

Σ1Θ

ΣT

2 Θ + λ

Θ

k1,

k

−

(E.1)

∈
Σ1 is positive semi-deﬁnite,

b

Rd. Here, we use alternating direction method of
where
multipliers (ADMM) to solve (E.1), and the convergence of our algorithm is guaranteed since
the variable is divided into two blocks [45]. For more details of ADMM, we refer readers to the
survey paper [17]. We now invoke the framework of ADMM and show the iterative formula.
Divide Θ

Rd, (3.17) is equivalent to

Σ2 ∈
b

Rd into M, Z

b

b

b

∈

∈

1
2

arg min
Rd
M,Z

∈

M T

Σ1M

ΣT

2 M + λ

−

Z

k1, s.t. M = Z.

k

By introducing the multiplier Υ

∈

b
Rd, the augmented Lagrangian function reads

b

1
2

M T

Σ1M

ΣT

2 M + λ

Z

k1 + ΥT (M

k

−

−

Z) +

ρ
2 k

M

Z

2
2.

k

−

Minimizing (M, Z) alternatively and updating Υ via gradient ascent give the iteration formulas

b

b

Mt+1 = (
Zt+1 =
Υt+1 = Υt + ρ

1(
Id)−
Σ1 + ρ
·
1
Sλ/ρ(Mt+1 + ρ−
b
(Mt+1 −
·

Σ2 + ρ
Υt)
·
b
Zt+1)




Zt −

·

Υt)

(E.2)


R, and then let

that updates (Mt, Zt, Υt) to (Mt+1, Zt+1, Υt+1). In (E.2), we deﬁne
β
thresholding operator.

Sβ(

if x

∈

}

·

|−
) element-wisely operate on vectors. This is known as the soft

{

|

Sβ(x) = sign(x) max

0,

x

E.3 Low-rank Matrix Completion

×

d rank r underlying matrix Θ∗ is generated by the formulation Θ∗ = ΘlΘr
ΘlΘr
k

Simulation Details. We simulate low-rank matrix completion with exactly low-rank matrix
Θ∗. The d
,
(0, 1). Furthermore, Θ∗
Rr
where entries of Θl
Θ∗
with diﬀerent (d, r) are controlled to possess comparable spikiness α(Θ∗) = d
. While
k
k
Θ∗
k
k
the covariate is speciﬁed to be Xk = ek(i)eT
[d]) (4.2), we test

d are i.i.d. drawn from

k(j) with (k(i), k(j))

r and Θr

uni([d]

Rd

N

∈

∈

max

×

×

k

F

F

∼

×

66

N

1
√3

(0, 1

1
250 ·

400) or

t(ν = 3)

both sub-Gaussian noise and heavy-tailed noise. Speciﬁcally, sub-Gaussian or heavy-tailed
t(ν = 3) is the
ǫk are i.i.d. copies of
Student’s t distribution with 3 degrees of freedom and variance rescaled to 1. Following these
+ǫk.
parameters, the full data
The responses are processed by the one-bit quantization scheme and quantized to one-bit ˙Yk,
Θ. In (4.6), we set
then solving the convex programming problem (4.6) gives the estimator
α∗ =
kmax and properly tune λ so that it balances the data ﬁdelity and low-rank structure.
kF and report the mean value of 15 independent
We track the Frobenius norm error
trials.

are obtained from the model Yk =

, respectively. Here,

(Xk, Yk) : k

Xk, Θ∗

(cid:0)
[n]
}

Θ∗

Θ∗

1
√3

Θ

−

∈

b

k

{

k

(cid:10)

(cid:11)

(cid:1)

Algorithm. We similarly apply ADMM to solve (4.6), and ﬁrst separate variable Θ to be two
blocks M , Z
(E) to be the indicator function widely used in optimization,
i.e., 1
otherwise. Then, we can move the max-norm
constraint to objective and obtain the equivalent program

×
(E) = 0 if E happens, 1

d. Deﬁne 1

(E) =

Rd

∞

∈

′

′

′

b

arg min
Rd×d
M ,Z

∈

1
2n

n

Xk, M

Xk=1

(cid:0)(cid:10)

˙Yk

′

2 + 1

(

γ

·

−

M

k

kmax ≤

α∗) + λ

Z

knu, s.t. M = Z.

k

(cid:11)

(cid:1)

Let Υ

Rd

×

∈

d be the multiplier, we have the augmented Lagrangian function

1
2n

n

Xk, M

Xk=1

(cid:0)(cid:10)

˙Yk

2

+ 1

′

(

γ

·

−

(cid:11)

(cid:1)

M

k

kmax ≤

α∗) + λ

Z

knu +

k

Υ, M

Z

+

−

(cid:10)

(cid:11)

Some additional notations are necessary before presenting the algorithms. Let
Xk = eieT

, then we deﬁne J1 = [J1(i, j)], J2 = [J2(i, j)]

d as

Rd

×

ρ
2 k

M

Z

2
F.

k

−

Iij =

k

{

∈

[n] :

j }

∈

J1(i, j) =

γ

·

Xk
∈Iij

˙Yk , J2(i, j) =

1 =

.

|Iij|

Xk
∈Iij

Rd

·

PΩ(

) to be the projection onto Ω

d under Frobenius norm. Let 1 be the
We deﬁne
⊂
all-ones matrix with self-evident size, and ./ represents the element-wise division between two
matrices of the same size. Furthermore, we introduce the soft thresholding operator
) for
a matrix A that admits singular value decomposition A = U ΣV ∗, where the singular values
of A are arranged in the diagonal matrix Σ. Based on
for
x
) element-wisely operates on the diagonal
matrix Σ. Now, one can derives the ADMM iteration formulas as

Sβ(x) = sign(x) max
{

Sβ(Σ)V ∗ and let

Sβ(A) = U

R, we deﬁne

Sβ(
β

Sβ(

| −

0,

∈

x

}

×

|

·

·

Zt + J1

α∗[(nρ
·
≤
Υt + Mt+1)

nΥt)./(nρ

1 + J2)]

·

−

.

(E.3)

M

Mt+1 =
Pk
Zt+1 =
Sλ/ρ(ρ−
Υt+1 = Υt + ρ

k

max
1

·
(Mt+1

·






Zt+1)

−

67

