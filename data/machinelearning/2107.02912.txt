Supervised Bayesian Speciﬁcation
Inference from Demonstrations

Journal Title
XX(X):1–13
©The Author(s) 2019
Reprints and permission:
sagepub.co.uk/journalsPermissions.nav
DOI: 10.1177/ToBeAssigned
www.sagepub.com/

SAGE

Ankit Shah1, Pritish Kamath1, Shen Li1, Patrick Craven2, Kevin Landers2, Kevin Oden2 and
Julie Shah1

1
2
0
2

l
u
J

6

]
I

A
.
s
c
[

1
v
2
1
9
2
0
.
7
0
1
2
:
v
i
X
r
a

Abstract
When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly
long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD)
has failed to capture this notion of the acceptability of a task’s execution; meanwhile, temporal logics provide a ﬂexible
language for expressing task speciﬁcations. Inspired by this, we present Bayesian speciﬁcation inference, a probabilistic
model for inferring task speciﬁcation as a temporal logic formula. We incorporate methods from probabilistic programming
to deﬁne our priors, along with a domain-independent likelihood function to enable sampling-based inference. We
demonstrate the efﬁcacy of our model for inferring speciﬁcations, with over 90% similarity observed between the inferred
speciﬁcation and the ground truth – both within a synthetic domain and during a real-world table setting task.

Keywords
Speciﬁcation Inference, Learning from Demonstrations, Probabilistic models

Introduction

Imagine showing a friend how to play your favorite quest-
based video game. A mission within such a game might be
composed of multiple sub-quests that must be completed in
order to ﬁnish that level. In this scenario, it is likely that
your friend would comprehend what needs to be done in
order to complete the mission well before he or she was
actually able to play the game effectively. While learning
from demonstrations, human apprentices can identify whether
a task is executed correctly long before gaining expertise in
that task. In the context of learning from demonstrations for
robotic tasks, a system that can evaluate the acceptability of an
execution before learning to execute a task can lead to more-
focused exploration of execution strategies. Further, a system
that can express its speciﬁcations would be more transparent
with regard to its objectives than a system that simply
imitates the demonstrator. Such characteristics are highly
desirable in applications such as manufacturing or disaster
response, where the cost of a mistake can be especially high.
Finally, a robotic system with a correct understanding of
the acceptability of executions can explore more-creative
execution traces not present in the demonstrated set.

Most current approaches to learning from demonstration
frame this problem as one of learning a reward function
or policy within the setting of a Markov decision process;
however, user speciﬁcation of acceptable behaviors through
reward functions and policies remains an open problem
Arnold et al. (2017). Temporal
logics have been used
in prior research as a language for expressing desirable
system behaviors, and can improve the interpretability
of speciﬁcations if expressed as compositions of simpler
templates (akin to those described by Dwyer et al. (1999)).
In this work, we propose a probabilistic model for inferring

Prepared using sagej.cls [Version: 2017/01/17 v1.20]

a task’s temporal structure as a linear temporal logic (LTL)
speciﬁcation.

A speciﬁcation inferred from demonstrations is valuable
in conjunction with synthesis algorithms for veriﬁable
controllers (Kress-Gazit et al. (2009) and Raman et al. (2015)),
as a reward signal during reinforcement learning (Li et al.
(2017) and Littman et al. (2017)), and as a system model for
execution monitoring. In our work, we frame speciﬁcation
learning as a Bayesian inference problem.

The ﬂexibility of LTL for specifying behaviors also
represents a key challenge with regard to inference due to
a large hypothesis space. We deﬁne prior and likelihood
distributions over a smaller but relevant part of the LTL
formulas, using templates based on work by Dwyer et al.
(1999). Ideas from universal probabilistic programming
languages formalized by Freer et al. (2014) and Goodman
et al. (2012); Goodman and Stuhlm¨uller (2014) are key to
our modeling approach; indeed, probabilistic programming
languages enabled Ellis et al. (2017, 2015) to perform
inference over complex, recursively deﬁned hypothesis spaces
of graphics programs and pronunciation rules.

We evaluate our model’s performance within three domains.
First, we incorporate a synthetic domain and a real-world
task involving setting a dinner table, both of which are
representative of candidate tasks for robots to learn from
demonstrations. For both these domains, the ground-truth
the capability
speciﬁcations are known, and we report

1CSAIL, MIT
2Lockheed Martin Corporation

Corresponding author:
Ankit Shah Computer Science and Artiﬁcial Intelligence Laboratory,
Massachusetts Institute of Technology, Cambrdige, MA 02139, USA.
Email: ajshah@mit.edu

 
 
 
 
 
 
2

Journal Title XX(X)

of our model
to achieve greater than 90% similarity
between the inferred and ground-truth speciﬁcations. We also
demonstrate the capability of our model to infer mission
objective speciﬁcations for evaluating large-force combat
ﬂying exercises involving multiple friendly and hostile
aircrafts. The LFE domain is particularly challenging, as it
incorporates multiple decision-making participants, some of
which act cooperatively and some in an adversarial fashion.
We demonstrate that our model makes predictions that are
well-aligned with those of an expert acting as the commander
for an example LFE mission. Further, we demonstrate that
our method of using template compositions allows for an
interpretable decision boundary for the classiﬁer inferred by
our model.

Bayesian speciﬁcation inference was ﬁrst introduced in
work by Shah et al. (2018); in this paper, we present an
advancement of that work and apply the model to newer
evaluation domains. First, we extend the probabilistic model
to be capable of learning both inductively (from positive
examples only) and from positive and negative examples.
We also extend the evaluation presented by Shah et al. (2018)
to include the large-force exercise domain.

as input to the model. Lemieux et al. (2015) introduced
Texada, a general-speciﬁcation mining tool for software
logs. Texada outputs all possible satisﬁed instances of a
particular formula template; however, it treats each time step
as a string, with all unique strings within the log treated as
unique propositions. Texada would treat a system with n
propositions as a system with 2n distinct propositions; thus,
interpreting a mined formula is non-trivial. Kong et al. (2014),
Kong et al. (2017), and Yoo and Belta (2017) mined PSTL
speciﬁcations for given demonstrations while simultaneously
inferring signal propositions akin to our own user-deﬁned
atomic propositions by conducting breadth-ﬁrst search over a
directed acyclic graph (DAG formed by candidate formulas.
Our prior speciﬁcations allow for better connectivity between
different formulas, while using MCMC-based approximate
inference enables ﬁxed runtimes.

We adopt a fully Bayesian approach to model
the
inference problem, allowing our model to maintain a posterior
distribution over candidate formulas. This distribution
provides a measure of conﬁdence when predicting the
acceptability of a new demonstration that the aforementioned
approaches do not.

Related Work

One common approach discussed in prior research frames
learning from demonstration as an inverse reinforcement
learning (IRL) problem. Ng and Russell (2000) and Abbeel
and Ng (2004) ﬁrst formalized the problem of inverse
reinforcement learning as one of optimization in order to
identify the reward function that best explains observed
demonstrations. Ziebart et al. (2008) introduced algorithms
to compute optimal policy for imitation using the maximum
entropy criterion. Konidaris et al. (2012) and Niekum et al.
(2015) framed IRL in a semi-Markov setting, allowing for
implicit representation of the temporal structure of the task.
Surveys by Argall et al. (2009) and Chernova and Thomaz
(2014) provided a comprehensive review of techniques built
upon these works as applied to robotics. However, according
to Arnold et al. (2017), one drawback of inverse reinforcement
learning is the non-triviality of extracting task speciﬁcations
from a learned reward function or policy. Our method bridges
this gap by directly learning the speciﬁcations for acceptable
execution of a given task.

Temporal logics, introduced by Pnueli (1977), are an
expressive grammar used to describe the desirable temporal
properties of task execution. Temporal logics have previously
served as a language for goal deﬁnitions in reinforcement
learning algorithms (Li et al. (2017) and Littman et al. (2017)),
reactive controller synthesis (Kress-Gazit et al. (2009)) and
Raman et al. (2015)), and domain-independent planning (Kim
et al. (2017)).

Kasenberg and Scheutz (2017) explored mining globally
persistent speciﬁcations from optimal traces of a ﬁnite-state
Markov decision process (MDP). Jin et al. (2015) proposed
algorithms for mining temporal speciﬁcations similar to rise
and setting times for closed-loop control systems. Works
by Kong et al. (2014), Kong et al. (2017), Yoo and Belta
(2017), and Lemieux et al. (2015) are most closely related
to our own, as our work incorporates only the observed
state variable (and not the actions of the demonstrators)

Linear Temporal Logic

Linear temporal logic (LTL), introduced by Pnueli (1977),
provides an expressive grammar for describing temporal
behaviors. A LTL formula is composed of atomic propositions
(discrete time sequences of Boolean literals) and both logical
and temporal operators, and is interpreted over traces [α] of
the set of propositions, α. The notation [α], t |= ϕ indicates
that ϕ holds at time t. The trace [α] satisﬁes ϕ (denoted as
[α] |= ϕ) iff [α], 0 |= ϕ. The minimal syntax of LTL can be
described as follows:

ϕ ::= p | ¬ϕ1 | ϕ1 ∨ ϕ2 | Xϕ1 | ϕ1Uϕ2

(1)

p is an atomic proposition; ϕ1 and ϕ2 are valid LTL
formulas. The operator X is read as ‘next’ and Xϕ1 evaluates
as true at time t if ϕ1 evaluates to true at t + 1. The operator
U is read as ‘until’ and the formula ϕ1Uϕ2 evaluates as true
at time t1 if ϕ2 evaluates as true at some time t2 > t1 and
ϕ1 evaluates as true for all time steps t such that t1 ≤ t ≤ t2.
In addition to the minimal syntax, we also use the additional
ﬁrst- order logic operators ∧ (and) and (cid:55)→ (implies), as well as
other higher-order temporal operators, F (eventually) and G
(globally). Fϕ1 evaluates to true at t1 if ϕ1 evaluates as true
for some t ≥ t1. Gϕ1 evaluates to true at t1 if ϕ1 evaluates
as true for all t ≥ t1.

Bayesian Speciﬁcation Inference

A large number of tasks comprised of multiple subtasks can
be represented by a combination of three temporal behaviors
among those deﬁned by Dwyer et al. (1999) — namely,
global satisfaction of a proposition, eventual completion of
a subtask, and temporal ordering between subtasks. With
ϕglobal, ϕeventual, and ϕorder representing LTL formulas for
these behaviors, the task speciﬁcation is written as follows:
ϕ = ϕglobal ∧ ϕeventual ∧ ϕorder

(2)

We represent the task demonstrations as an observed
sequence of state variables, [x]. Let αin{0, 1}n represent a

Prepared using sagej.cls

Shah et al.

3

vector of ﬁnite dimension formed by n Boolean propositions.
The propositions are related to the state variables through a
labeling function, α = f (x), which is known a priori.

The inference model is provided a label, y, to indicate
whether an execution is acceptable or not, along with
the training set D =
the actual demonstrations. Thus,
i ∈ {1, 2, . . . , ndemo}} consists of ndemo
{([α]i, yi) ;
demonstrations along with the label. The output, again, is
a probability distribution P (ϕ|D).

Formula Template

Global satisfaction: Let T be the set of candidate
propositions to be globally satisﬁed, and let τ ⊆ T be the
actual subset of satisﬁed propositions. The LTL formula that
speciﬁes this behavior is written as follows:

ϕglobal =

(cid:33)

(G(τ ))

(cid:32)

(cid:94)

τ ∈τ

(3)

Such formulas are useful for specifying that some
constraints must always be met – for example, a robot must
avoid collisions while in motion, or an aircraft must avoid
no-ﬂy zones.

Eventual completion: Let Ω be the set of all candidate
subtasks, and let W1 ⊆ Ω be the set of subtasks that must
be completed if the conditions represented by πw; w ∈ W1
are met. ωw are propositions representing the completion of
a subtask. The LTL formula that speciﬁes this behavior is
written as follows:

ϕeventual =

(cid:32)

(cid:94)

w∈W1

(cid:33)

(πw → Fωw)

(4)

Temporal ordering: Every set of feasible ordering
constraints over a set of subtasks is mapped to a DAG over
nodes representing these subtasks. Each edge in the DAG
corresponds to a binary precedence constraint. Let W2 be the
set of binary temporal orders deﬁned by W2 = {(w1, w2) :
w1 ∈ V , w2 ∈ Descendants(w1)}, where V is the set of all
nodes within the task graph. Thus, the ordering constraints
include an enumeration of not just the edges in the task-graph,
but all descendants of a given node. For subtasks w1 and w2,
the ordering constraint is written as follows:

ϕorder =





(cid:94)

(w1,w2)∈W2



(πw1 → (¬ωw2Uωw1))

 (5)

This formula states that if conditions for the execution of
w1 i.e. πw1 are satisﬁed, w2 must not be completed until w1
has been completed.

For the purposes of this paper, we assume that all required
propositions α = [τ , π, ω]T and labeling functions f (x) are
known, along with the sets T and Ω and the mapping of
the condition propositions πw to their subtasks. Given these
assumptions, the problem of inferring the correct formula for
a task is equivalent to identifying the correct subsets τ , W1,
and W2, that explain the observed demonstrations well.

Prepared using sagej.cls

Speciﬁcation Learning as Bayesian Inference

The Bayes theorem is fundamental
inference, and is stated as follows:

to the problem of

P (h | D) =

P (h)P (D | h)
h∈H P (h)P (D | h)

(cid:80)

(6)

P (h) is the prior distribution over the hypothesis space, and
P (D | h) is the likelihood of observing the data given a
hypothesis. Our hypothesis space is deﬁned by H = ϕ, where
ϕ is the set of all formulas that can be generated by the
production rule deﬁned by the template in Equation 2. The
observed data comprises the set of demonstrations provided
to the system by expert demonstrators (note that we assume
all these demonstrations are acceptable). D is the training
dataset.

Prior speciﬁcation While sampling candidate formulas as per
the template depicted in Equation 2, we treat the sub-formulas
in Equations 3, 4, and 5 as independent to each other. As
generating the actual formula, given the selected subsets, is
deterministic, sampling ϕglobal and ϕeventual is equivalent to
selecting a subset of a given ﬁnite universal set. Given a set A,
we deﬁne SampleSubset(A,p) as the process of applying a
Bernoulli trial with a success probability of p to each element
of A and returning the subset of elements for which the
trial was successful. Thus, sampling ϕglobal and ϕeventual is
accomplished by performing SampleSubset(T , pG) and
SampleSubset(Ω, pE). Sampling ϕorder is equivalent to
sampling a DAG, with the nodes of the graph representing
subtasks. Based on domain knowledge, appropriately
constraining the DAG topologies would result in better
inference with fewer demonstrations. Here, we present
three possible methods of sampling a DAG, with different
restrictions on the graph topology.

i ← 1; Ci ← []
P ← random permutation(Ω)
for a ∈ P do

Algorithm 1 SampleSetsOfLinearChains
1: function SAMPLESETSOFLINEARCHAIN(Ω,ppart)
2:
3:
4:
5:
6:
7:
8:
9:

Ci.append(a)
k ← Bernoulli(ppart)
if k = 1 then

i = i + 1; Ci ← []

return Cj ∀ j

Linear chains: A linear chain is a DAG such that all
subtasks must occur within a single, unique sequence out
of all permutations. Sampling a linear chain is equivalent
to selecting a permutation from a uniform distribution, and
is achieved via the following probabilistic program: for a
set of size n, sample n − 1 elements from that set without
replacement, with uniform probability.

Sets of linear chains: This graph topology includes graphs
formed by a set of disjoint sub-graphs, each of which is
either a linear chain or a solitary node. The execution of
subtasks within a particular linear chain must be completed
in the speciﬁed order; however, no temporal constraints
exist between the chains. Algorithm 1 depicts a probabilistic
program for constructing these sets of chains. In line 2, the
ﬁrst active linear chain is initialized as an empty sequence.
In line 3, a random permutation of the nodes is produced.
For each element a ∈ P , line 5 adds the element to the last

4

Journal Title XX(X)

Table 1. Prior deﬁnitions and hyperparameters.

Prior

ϕOrder

Hyperparameters

Prior 1
Prior 2
Prior 3

RandomPermutation(Ω)
SampleSetsOfLinearChains(Ω, ppart)
SampleForestofSubTasks(Ω, Nnew)

pG, pE
pG, pE , ppart
pG, pE , Nnew

active chain. Lines 6 and 8 ensure that after each element,
either a new active chain is initiated (with a probability of
ppart) or the old active chain continues (with a probability of
1 − ppart).

Forest of sub-tasks: This graph topology includes forests
(i.e., sets of disjoint trees). A given node has no temporal
constraints with respect to its siblings, but must precede all its
descendants. Algorithm 2 depicts a probabilistic program for
sampling a forest. Line 2 creates P , a random permutation
of the subtasks. Line 3 initializes an empty forest. In order
to support a recursive sampling algorithm, the data structure
representing forests is deﬁned as an array of trees, F . The
ith tree has two attributes: a root node, F [i].root, and a
‘descendant forest,’ F [i].descendant, in which the root node
of each tree is a child of the root node deﬁned as the ﬁrst
attribute. The length of the forest, F .length, is the number
of trees included in that forest. The size of a tree, F [i].size,
is the number of nodes within the tree (i.e., the root node
and all of its descendants). For each subtask in the random
permutation P , line 5 inserts the given subtask into the forest
as per the recursive function InsertIntoForest deﬁned
in lines 7 through 13. In line 8, an integer i is sampled from
a categorical distribution, with {1, 2, . . . , F .length + 1} as
the possible outcomes. The probability of each outcome is
proportional to the size of the trees in the forest, while the
probability of F .length + 1 being the outcome is proportional
to Nnew, a user-deﬁned parameter. This sampling process is
similar in spirit to the Chinese restaurant process (Aldous
(1985)). If the outcome of the draw is F .length + 1, then a
new tree with root node a is created in line 10; otherwise,
InsertIntoForest is called recursively to add a to the
forest F [i].descendants, as per line 12.

F =InsertIntoForest(F ,a)

P ← random permutation(Ω)
F ← []
for a ∈ P do

Algorithm 2 SampleForestofSubtasks
1: function SAMPLEFORESTOFSUBTASKS(Ω,Nnew)
2:
3:
4:
5:
6:
7: function INSERTINTOFOREST(F , a)
8:
9:
10:
11:
12:
13:

Create new tree F [F .length + 1].root = a

return F

return F

else

F [i].descendants = InsertIntoForest(F [i].descendants, a)

i ← Categorical([F [1].size, F [2].size, . . . , F [F .length].size, Nnew])
if i = F .length + 1 then

Three prior distributions based on the four probabilistic
programs are described in Table 1. In all the priors, ϕglobal
and ϕeventual are sampled using SampleSubset(T , pG)
and SampleSubset(Ω, pE), respectively.

Likelihood function The likelihood distribution, P ({[α]i} |
ϕ, {yi}), is the probability of observing the trajectories within
the dataset given the candidate speciﬁcation. It is reasonable
to assume that the demonstrations are independent of each
other; thus, the total likelihood can be factored as follows:

Prepared using sagej.cls

P ({[α]i} | ϕ, {yi}) = (cid:81)

i∈{1,2,...,ndemo} P (ϕ)P ([α]i |

ϕ, yi).

The probability of observing a given trajectory demon-
stration is dependent upon the underlying dynamics of the
domain and the characteristics of the agents producing the
demonstrations. In the absence of this knowledge, our aim
is to develop an informative, domain-independent proxy for
the true likelihood function based only on the properties of
the candidate formula; we call this the ‘complexity-based’
(CB) likelihood function. Our approach is founded upon the
classical interpretation of probability championed by Laplace
(1951), which involves computing probabilities in terms of a
set of equally likely outcomes. Let there be Nconj conjunctive
clauses in ϕ; there are then 2Nconj possible outcomes in terms
of the truth values of the conjunctive clauses. In the absence
of any additional information, we assign equal probabilities
to each of the potential outcomes. Then, according to the
classical interpretation of probability, for candidate formula
ϕ1 (deﬁned by subsets τ1, W11 , and W211) and ϕ2 (deﬁned
by subsets τ2, W12 , and W22) the likelihood odds ratio if
yi = 1 is deﬁned as follows:

|

|
|

=

|+|W21
|+|W22
|+|W21

(cid:40) 2Nconj1
2Nconj2
2Nconj1
(cid:15)

= 2|τ1|+|W11
2|τ2|+|W12
= 2|τ1|+|W11
(cid:15)

P ([α]i | ϕ1)
P ([α] | ϕ2)

, [α] |= ϕ2
, [α] (cid:50) ϕ2
(7)
Here, a ﬁnite probability proportional to (cid:15) is assigned to
a demonstration that does not satisfy the given candidate
formula. With this likelihood distribution, a more-restrictive
formula with a low prior probability can gain favor over
a simpler formula with higher prior probability given a
large number of observations that would satisfy it. However,
if the candidate formula is not the true speciﬁcation, a
larger set of demonstrations is more likely to include non-
satisfying examples, thereby substantially decreasing the
posterior probability of the candidate formula. The design
of this likelihood function is inspired by the size principle
described by Tenenbaum (2000).

A second choice for a likelihood function, inspired by
Shepard (1987), is deﬁned as the SIM model by Tenenbaum
this the ‘complexity-independent’ (CI)
(2000); we call
likelihood function, and it is deﬁned as follows:

P ([α] | ϕ) =

(cid:40)

1 − (cid:15),
(cid:15),

if [α] |= ϕ
Otherwise

(8)

We must deﬁne likelihood functions for both acceptable
and unacceptable demonstrations. Note that the likelihood
function deﬁned by Equation 7 produces a relatively larger
likelihood value if the candidate formula correctly classiﬁes
the demonstration, and a very small likelihood value if it does
not. Following the classical probability argument as before,
with 2Nconj conjunctive clauses in a candidate formula, there
are 2Nconj possible evaluations of each of the individual

Shah et al.

5

clauses that would result in the given demonstration not
satisfying the candidate formula. Thus, the likelihood function
for yi = 0 is deﬁned as follows:

P ([α]i | ϕ1)
P ([α]i | ϕ2)

=






2Nconj1 (2Nconj2 −1)
2Nconj2 (2Nconj1 −1)

2Nconj1
(2Nconj1 −1)(cid:15)

, [α] (cid:50) ϕ2

, [α] |= ϕ2

(9)

generating examples of LFE executions, and used our model
to infer speciﬁcations for successful completion of mission
objectives. In this domain, the true speciﬁcations are not
known, and we only have annotations of the demonstrated
scenario from a subject matter expert (in this case, the
mission commander who designs the scenario and debriefs
participating pilots).

An equivalent SIM likelihood function for examples with

Metrics

yi = 0 is deﬁned as follows:

P ([α] | ϕ) =

(cid:40)

1 − (cid:15),
(cid:15),

if [α] (cid:50) ϕ
Otherwise

(10)

Note that for larger values of Nconj1 and Nconj2 and a
negative label yi = 1, the difference between the CI and the
CB likelihood function is very small.

in
Inference We implemented our probabilistic model
webppl (Goodman and Stuhlm¨uller (2014)), a Turing-
complete probabilistic programming language. The hyper-
parameters, including those deﬁned in Table 1 and (cid:15), were set
as follows: pE, pG = 0.8; ppart = 0.3; Nnew = 5; (cid:15) = 4 ×
log(2) × (|T + |Ω| + 0.5|Ω|(|Ω| − 1)). These values were
held constant for all evaluation scenarios. The equation
for (cid:15) was deﬁned such that evidence of a single non-
satisfying demonstration would negate the contribution of
four satisfying demonstrations to the posterior probability.
The posterior distribution of candidate formulas is constructed
using webppl’s Markov chain Monte Carlo (MCMC) sam-
pling algorithm from 10,000 samples, with 100 samples
serving as burn-in. The posterior distribution is stored as
a categorical distribution, with each possibility representing a
unique formula. The maximum a posteriori (MAP) candidate
represents the best estimate for the speciﬁcation as per the
model. We ran the inference on a desktop with an Intel i7-
7700 processor.

Evaluations

We evaluated the performance of our model across three
different domains. We developed a synthetic domain with
a low dimensional state-space where we could easily vary
the complexity of the ground-truth speciﬁcations. We also
applied our model to a real-world task of setting a dinner
table – a task often incorporated into studies of learning from
demonstrations (Toris et al. (2015)). This task has a large raw
state-space incorporating the poses of the objects included
in the domain. This domain demonstrates the beneﬁts of
using propositions to represent task speciﬁcations, as the
complexity of the problem depends upon the number of
Boolean propositions and not the dimensionality of the raw
state-space. (Note that the ground-truth speciﬁcations are
known in both of these domains, and it is easy to measure the
quality of the solution by comparing it to the ground-truth
speciﬁcation.)

the quality of the
The evaluation metrics used to test
inferred speciﬁcations depend upon whether the ground-truth
speciﬁcations are known. For domains in which it is known
(the synthetic and dinner-table domains), the ground-truth
speciﬁcation is deﬁned using subsets τ ∗, W ∗
2 (as
per Equations 3, 4, and 5), and a candidate formula ϕ is
deﬁned by subsets τ , W1, and W2. In such cases, we deﬁne
the degree of similarity using the Jaccard index (Paul (1912))
as follows:

1 , and W ∗

L(ϕ) =

| {τ ∗ ∪ W ∗
| {τ ∗ ∪ W ∗

1 ∪ W ∗
1 ∪ W ∗

2 } ∩ {τ ∪ W1 ∪ W2} |
2 } ∪ {τ ∪ W1 ∪ W2} |

(11)

The maximum possible value of L(ϕ) is one such that both
formulas are equivalent. One key beneﬁt of our approach
is that we compute a posterior distribution over candidate
formulas; thus, we report the expected value of E(L(ϕ))
as a measure of the deviation of the inferred distribution
from the ground truth. We also report the maximum value of
L(ϕ) among the top 5 candidates in the posterior distribution.
We classify the inferred orders in W2 as correct if they are
included in the ground truth, incorrect if they reverse any
constraint within the ground truth, and ‘extra’ otherwise.
(Extra orders over-constrain the problem, but do not induce
incorrect behaviors.)

For the LFE domain, where the ground-truth speciﬁcations
are unknown but SME annotations for whether the mission
objectives were accomplished are provided for the dataset,
we use the weighted F1 score for both ‘achieved’ and ‘failed’
labels. This score is evaluated on a test set that is held out
while using the remaining examples in the dataset to infer the
speciﬁcations.

Synthetic Domain

In our synthetic domain, an agent navigates within a two-
dimensional space that includes points of interest (POIs) to
visit and threats to avoid. The state of the agent x represents
the position of that agent within the task space.

Let τ = {1, 2 . . . , nthreats} represent a set of threats
positioned at xTi ∀ i ∈ τ , respectively. A proposition τi is
associated with each threat location i ∈ tau such that:

(cid:40)

τi =

(cid:107)x − xTi(cid:107) ≥ (cid:15)threat

true,
false, otherwise

(12)

Finally, we also applied our inference model to the large-
force exercise (LFE) domain. Large-force exercises are
simulated air-combat games used to train combat pilots.
We developed simulation environments using joint semi-
automated forces (JSAF), a constructive environment for

The proposition τTi holds if the agent is not within the

avoidance radius (cid:15)threat of the threat location.
Let Ω = {1, 2, . . . , nP OI } represent

the set of POIs
positioned at xPi ∀ i ∈ Ω. A proposition ωi is associated
with each POI such that:

Prepared using sagej.cls

6

Journal Title XX(X)

(a) Scenario 1

Figure 1. Example trajectories from Scenario 1. Green circles
denote the POIs; red circles denote the avoidance zones of
threats.

(cid:40)

ωi =

(cid:107)x − xPi(cid:107) ≤ (cid:15)P OI

true,
false, otherwise

(13)

(b) Scenario 1

ωi evaluates as true if the agent is within a tolerance radius

(cid:15)P OI of the POI.

Finally, propositions πi ∀ i ∈ Ω are conditions propositions
that denote the accessibility of the POI i, and are deﬁned as
follows:

(cid:40)

πi =

false, ∃ j such that (cid:13)
true,
otherwise

(cid:13)xPi − xTj

(cid:13)
(cid:13) ≤ (cid:15)threat

(14)

πi evaluates as false if the POI i is inside the avoidance

region of any of the threats.

The agent can be programmed to visit the accessible
POIs and avoid threats as per the ground-truth speciﬁcation.
The ground-truth speciﬁcations are stated by deﬁning the
following: a set T ⊆ τ that represents the subset of threats
that the agent must avoid; a set W1 ⊆ Ω that represents
the subset of POIs the agent must visit; and the ordering
constraints deﬁned by W2, a set of feasible pairwise
precedence constraints between the POIs.

Here, we demonstrate the results of applying our
inference model to three scenarios with differing ground-truth
speciﬁcations.

Scenario 1: In Scenario 1, we placed ﬁve threats in the
task-domain, and their positions were sampled from a uniform
distribution for each demonstration. There were four points
of interest, labeled 1, 2, 3, 4, and their positions were ﬁxed
across all demonstrations. The agents were required to visit
the POIs in a ﬁxed order ([1, 2, 3, 4]). Example trajectories
from this scenario are depicted in Figure 1.

The posterior distribution was computed using prior
1 (deﬁned in Table 1), with both CB (Equation 7) and
CI (Equation 8) likelihood functions. The expected and
maximum values among the top 5 a posteriori formula
candidates of L(ϕ) are depicted in Figure 2. We observed
that the CB likelihood function performed better than the
CI likelihood function at inferring the complete speciﬁcation.

Figure 2. Figure 2a depicts the results from Scenario 1, with the
dotted line representing the maximum possible value of L(ϕ).
Figure 2b shows the number of unique formulas in the posterior
distribution

Using the CI function resulted in a higher posterior probability
assigned to formulas with high prior probability that were
satisﬁed by all demonstrations. (These tended to be simple,
non-informative formulas; the CB function assigned higher
probability mass to more-complex formulas that explained
the demonstrations correctly.) Figure 2b depicts the number
of unique formulas in the posterior distributions. The CB
likelihood function resulted in posteriors being more peaky,
with fewer unique formulas as training set size increased; this
effect was not observed with the CI function.

The posterior distribution was also computed using priors
2 and 3 with the CB likelihood function. The expected
and maximum values among the top 5 a posteriori formula
candidates of L(ϕ) are depicted in Figure 3a. Prior 3 aligned
better with the ground-truth speciﬁcation with fewer training
examples. With a larger training set, prior 2 recovered the
exact speciﬁcation, while prior 3 failed to do so. Figure 3b
depicts the expected value of the correct and extra orders in
the candidate formulas included in the posterior distribution.
The a priori bias of prior 3 toward longer chains is apparent,
as it recovered more correct orders with fewer training
demonstration in comparison to prior 2. Prior 2 recovered
all correct priors with more training examples; however, prior
3 failed to do so with 30 training examples.

Scenario 2: Scenario 2 contained ﬁve POIs 1, 2, 3, 4, 5
and ﬁve threats. Like Scenario 1, the threat positions were
sampled uniformly for each demonstration. All the POIs, if
accessible, had to be visited. A partial ordering constraint
was imposed such that POIs [1, 3, 5] had to be visited in that
speciﬁc order, while POIs {2, 4} could be visited in any order.
Some demonstrations generated for Scenario 2 are depicted
in Figure 4.

Prepared using sagej.cls

05101520253035Number of Training Demonstrations00.20.40.60.811.20102030Number of Demonstrations0100200300400Number of FormulasCBCIShah et al.

7

(a) Scenario 1

(a) Scenario 2 L(ϕ)

(b) Scenario 1

(b) Scenario 2 orders

Figure 3. Figure 3a depicts the results from Scenario 1 using
priors 2 and 3, with the dotted line representing the maximum
possible value of L(ϕ). Figure 3b depicts the expected value of
the number of correct and extra orders in the posterior
distribution.

Figure 5. Figure 5a indicates the L(ϕ) values for Scenario 2,
and Figure 5b depicts the correct and extra orderings inferred in
Scenario 2. The dotted lines represent the number of orderings
in the true speciﬁcation.

but both priors converged to the correct speciﬁcation given
enough training examples.

Scenario 3: Scenario 3 included ﬁve threats and ﬁve POIs
labeled {1, 2, 3, 4, 5}, respectively. The threat positions were
uniformly sampled for each scenario. Each of the POIs,
if accessible, had to be visited; however, there were no
constraints placed on the order in which they were visited.
Figure 6 depicts some of the example demonstrations.

Again, the posterior distribution was computed using priors
2 and 3. The expected and maximum values among the top
5 formula candidates of L(ϕ) are depicted in Figure 7a.
In this scenario, both priors performed equally well with
regard to recovering the ground-truth speciﬁcation. With 10
or more demonstrations, both priors returned the ground-
truth speciﬁcation as the maximum a posteriori estimate.
The expected value of the extra orders contained in the
posterior distributions is depicted in Figure 7b. Once again,
the tendency of prior 3 to return longer chains is apparent, as
more formulas in the posterior distribution returned a greater
number of extra ordering constraints as compared with prior
2.

The runtime for MCMC inference is a function of the
number of samples generated, the number of demonstrations
in the training set, and demonstration length. Scenarios 1
and 2 required an average runtime of 10 and 90 minutes for
training set sizes of 5 and 50, respectively.

TempLogIn (Kong et al. (2017)) required 33 minutes to
terminate with three PSTL clauses. For all the scenarios,
the mined formulas did not capture any of the temporal
behaviors in Section , indicating that additional PSTL clauses
were required. However, with ﬁve and 10 PSTL clauses, the
algorithm did not terminate within the 24-hour runtime cutoff.

Figure 4. Example trajectories from Scenario 2. Green circles
denote the POIs; red circles denote the avoidance zones of
threats.

For Scenario 2, the posterior distribution was computed
using priors 2 and 3, as the ground-truth speciﬁcation did
not lie in support of prior 1. The expected and maximum
values among the top 5 formula candidates of L(ϕ) are
depicted in Figure 5a. Given a sufﬁcient number of training
examples, both priors were able to infer the complete formula;
with 10 or more training examples, both priors returned the
ground-truth formula among the top 5 candidates with regard
to posterior probabilities. Figure 5b depicts the correct and
extra orders inferred in Scenario 2. Prior 3 assigned a larger
prior probability to longer task chains compared with prior 2,

Prepared using sagej.cls

05101520253035Number of Training Demonstrations00.20.40.60.811.205101520253035Number of Demonstrations02468NumberE[#Correct Orders]: Prior 2E[#Extra Orders]: Prior 2E[#Correct Orders]: Prior 3E[#Extra Orders]: Prior 301020304050Number of Training Demonstrations00.20.40.60.811.201020304050Number of Demonstrations0123456NumberE[#Correct Orders]: Prior 2E[#Extra Orders]: Prior 2E[#Correct Orders]: Prior 3E[#Extra Orders]: Prior 38

Journal Title XX(X)

conﬁgurations of the dining set pieces, depending upon the
type of food served. The pieces placed on the table were
varied for each of the eight conﬁgurations; however, the
positions of the pieces remained constant across all ﬁnal
conﬁgurations. A total of 71 demonstrations were collected,
with six participants providing multiple demonstrations for
each of the four conﬁgurations.

The eight dinner set pieces included a large dinner plate,
a smaller appetizer plate, a bowl, a fork, a knife, a spoon, a
water glass, and a mug; the set of pieces is represented by Ω.
Each piece was tracked with a motion-capture system over
the course of the demonstration, with the pose of an object
i ∈ Ω in the world frame represented by T O
i . In addition,
the pose of the wrists of the demonstrators T O
h1 and T O
h2
were also tracked throughout the demonstration. We deﬁned
propositions that tracked whether an object was in its correct
position or whether a demonstrator’s wrist was too close to
the centerpiece using task-space region (TSR) constraints
proposed by Berenson et al. (2011).

Figure 6. Example trajectories from Scenario 3. The green
circles denote the POIs; the red circles denote the threat
avoidance zones.

The origin for each TSR constraint is located at the desired
ﬁnal position of each object. The pose T O
wi represents the
transform between the origin frame and the TSR frame for
the object, i. The bounds for Bi represent the translation and
rotational tolerances of the constraint. Finally, Pi represents
the set of poses in the TSR frame that fall within the tolerance
bounds. The pose of object i with respect to the TSR frame is
given by T wi
i . A proposition ωi is associated
with object i as follows:

i = (T O
wi

)−1T O

(a) Scenario 2 L(ϕ)

(cid:40)

ωi =

T Wi
true,
i ∈ Pi
false. otherwise

(15)

Thus, the proposition ωi evaluates as true if the pose of

object i satisﬁes the TSR constraints, and false otherwise.

A TSR constraint is also associated with the centerpiece,
where T O
represents the pose of the centerpiece with respect
c
to the world frame, and the bounds of the constraint are
deﬁned by Bc, with Pc representing the set of poses that fall
within the tolerances. The poses of the demonstrator’s wrists
with respect to this TSR frame are given by T c
for i ∈ {1, 2}.
hi
A proposition τc is associated with the centerpiece, and is
deﬁned as follows:
(cid:40)

τc =

false, T c
true,

h1 ∈ Pc ∨ T c
otherwise

h2 ∈ Pc

(16)

τc evaluates as false if either of the wrist poses falls within

the TSR bounds, and evaluates as true otherwise.

Finally, condition propositions πi ∀ i ∈ Ω encode whether
the object i must be placed. Their values are set prior to
the demonstration and held constant for its duration. These
propositions encode the fact that serving certain courses
during a meal requires speciﬁc placement of certain dinner
pieces.

table,

the dinner

Based on the propositions deﬁned above and the
conﬁgurations of
the ground-truth
speciﬁcations of this task are as follows: the demonstrator’s
wrists should never enter the centerpiece’s TSR region (global
satisfaction); if πi is true, then the corresponding dinner
piece must be placed on the table (eventual completion);
and the large plate must be placed before the smaller plate,
which in turn must be placed before the bowl (ordering).

(b) Scenario 2 orders

Figure 7. Figure 7a indicates the L(ϕ) values for Scenario 3,
and Figure 7b depicts the correct and extra orderings inferred in
Scenario 3. The dotted lines represent the number of orderings
in the true speciﬁcation.

Scaling TempLogIn to larger formula lengths is difﬁcult, as
the size of the search graph increases exponentially with the
number of PSTL clauses, and the algorithm must evaluate all
formula candidates of length n before candidates of length
n + 1.

Dinner Table Domain

We also tested our model on a real-world task: setting a
dinner table. This task featured eight dining set pieces that
had to be organized on a table while the demonstrator avoided
contact with a centerpiece. Figure 8a depicts each of the ﬁnal

Prepared using sagej.cls

01020304050Number of Training Demonstrations00.20.40.60.811.201020304050Number of Demonstrations-0.500.511.522.53NumberE[#Correct Orders]: Prior 2E[#Extra Orders]: Prior 3E[#Correct Orders]: Prior 2E[#Extra Orders]: Prior 3Shah et al.

9

(a)

(b)

Figure 8. Figure 8a depicts all the ﬁnal conﬁgurations. Figure 8b depicts the demonstration setup. (Photographed by the authors in
April 2017.)

Evaluating Large Force Exercises

Large-force exercises (LFE) are combat ﬂight
training
exercises that involve multiple aircraft groups, with each
group playing a designated role in the completion of the
mission. Evaluating a LFE execution is a challenging task for
the mission commander. The raw state-space of the domain
includes the navigation data for each aircraft involved in the
scenario (up to 36 aircrafts were included in the scenarios
we simulated), along with conﬁguration settings for each of
those aircrafts (weapon stores, weapon deployments, etc.)
and outcomes of combat engagements that occur throughout
the scenario. The mission commander must distill this time-
series and evaluate the mission based on multiple output
modalities. He or she must ﬁrst identify the transition points
between predetermined scenario phases, then evaluate the
overall success of the mission’s execution in terms of a
ﬁnite number of predetermined objectives. Evaluation of the
mission objectives depends not only upon the ﬁnal state
of the scenario, but also on the behavior of the aircrafts
throughout the mission, thus making LTL a suitable grammar
for representing mission objective speciﬁcations.

We evaluated the capabilities of our model to infer LTL
speciﬁcations that match a mission commander’s evaluations
of mission objective completion. In this section, we begin
by describing the nature of the offensive counter air (OCA)
mission that serves as the subject of our study. Next, we
describe how these missions are evaluated by experts, and
how the stated mission objectives are well-suited for use with
the temporal behavioral templates we use in our candidate
formulas. Finally, we describe the results obtained when
applying our model to the LFE domain dataset.

LFE Scenario description Each LFE for the OCA mission
we modeled consists of 18 friendly aircrafts and a variable
number of enemy aircrafts and ground-based threats. Among
the friendly aircrafts, there are eight escort aircrafts that are
capable air-to-air ﬁghters, eight SEAD (suppression of enemy
air defenses) aircrafts capable of attacking ground-based
threats, and two strike aircrafts that carry the ammunition
that must be deployed in order to attack a designated ground
target within a time-on-target (TOT) window. The aircrafts’
starting positions during a typical scenario are depicted in
Figure 10.The role of the mission commander is to debrief
the participants once a LFE scenario execution is completed.
During debrieﬁng, the LFE-OCA scenario is segmented into
four phases by design as follows:

• Escort Push

(a) Dinner table L(ϕ)

(b) Dinner Table orders

Figure 9. Figure 9a depicts the L(ϕ) values for the dinner table
domain, with the dotted line representing the maximum possible
value. Figure 9b depicts the correct and extra orderings inferred
within this domain; the dotted lines represent the number of
orderings in the true speciﬁcation.

We constructed the posterior distributions over candidate
speciﬁcation using priors 2 and 3 by incorporating subsets of
the training demonstrations of varying sizes, and evaluated the
similarity between the inferred speciﬁcations and the ground
truth using the expected and maximum values among the top
5 a posteriori candidates of the metric L(ϕ).

With prior 2, our model correctly identiﬁed the ground
truth as one of the top 5 a posteriori formula candidates in all
cases. With prior 3, the inferred formula contained additional
ordering constraints compared with the ground truth. Using
all 71 demonstrations, the MAP candidate had one additional
ordering constraint: that the fork be placed prior to the spoon.
Upon review, it was observed that this condition was not
satisﬁed in only four of the 71 demonstrations.

Prepared using sagej.cls

3040506070Number of Training Demonstrations00.20.40.60.811.23040506070Number of Demonstrations0123456NumberE[#Correct Orders]: Prior 2E[#Extra Orders]: Prior 2E[#Correct Orders]: Prior 3E[#Extra Orders]: Prior 310

Journal Title XX(X)

• Strikers Push
• Time-On-Target (TOT)
• Egress

The mission commander must
identify the times that
correspond to the transitions between these mission phases,
and also provide an assessment of whether the following three
mission objectives were achieved:

• MO1: Gain and maintain air superiority.
• MO2: Destroy an assigned target within the TOT

window.

• MO3: Friendly attrition should not exceed 25%.

Each of the mission objectives is a Boolean-valued function
of the raw state-space of the LFE scenario, and the mapping
between them is not explicitly known. Inputs from subject
matter experts (SMEs) were also utilized to represent the
mission execution in terms of certain Boolean propositions
over which we can apply our probabilistic model. The
propositions were deﬁned as follows:

1. Enemy aircraft attrition (50%, 75%, 100%) (three

propositions).

2. Either strike aircraft ﬁred upon.

3. Either strike aircraft shot down.

4. Last munition released by strikers.

5. Strike aircrafts ﬂying in on-target ﬂight phase.

6. Assigned target hit.

7. Friendly aircraft attrition (25%, 50%, 75%) (three
propositions, each turn false if the corresponding
attrition is reached).

In order to generate realistic demonstrations of how the
different executions unfold, the scenarios were deﬁned in Joint
Semi-Automated Forces (JSAF) – a constructive environment
capable of simulating realistic aircraft behavior. The data
collected for each demonstration included the position, speed,
attitude, and rates of each of the aircrafts (both friendly
and hostile); the individual mission phase of each aircraft
(a discrete set of phases by which the aircraft speciﬁc mission
timeline can be labeled); and the ﬁring times, designated
targets, detonation times, and outcomes of each weapon
deployment over the course of the scenario. The mapping
from the collected data to the Boolean propositions stated
above is well deﬁned.

In order to apply our probabilistic model to the LFE domain,
we deﬁned the sets τ and Ω. The propositions 7, 2, and 3 were
included in the set τ as candidates for global satisfaction. The
propositions 1, 4, 5, and 6 were included in Ω as candidates
for eventual completion.

Data collection A total of 24 instances of LFEs were
simulated and included in the dataset. Each instance had
a different outcome with respect to the mission objectives,
based on the different outcomes of engagements between
friendly and hostile forces. Each scenario was evaluated by
an SME acting as a mission commander performing a manual
debrief. The primary annotation task was to evaluate whether

Prepared using sagej.cls

Figure 10. The starting conﬁguration of a large-force exercise
scenario. The red aircrafts are the hostile forces, and the blue are
friendly forces.

each of the objectives was successfully achieved upon mission
completion. The secondary annotation task was to determine
the segmentation points among the four scenario phases on
the mission timeline. The segmentation task is not directly
relevant to speciﬁcation inference, but we used the labels
to simultaneously train a secondary classiﬁer in one of the
baselines.

Benchmarks The training data for evaluations of LFEs
consists of both acceptable and unacceptable demonstrations,
along with the label for that demonstration; thus, it can be
viewed as a supervised learning problem. We decided to
compare the classiﬁcation accuracy of our model against
a classiﬁer trained with a recurrent neural network as the
underlying architecture.

1. Stand-alone: Here, the recurrent neural network is
trained to jointly optimize the binary cross-entropy for
classiﬁcation of each of the three mission objectives.
The loss functions for all the mission objectives are
equally weighted. The recurrent neural networks are
composed of long and short-term memory (LSTM)
modules (Hochreiter and Schmidhuber (1997)), along
with their bidirectional variants (Graves et al. (2005)).
Such models have shown state-of-the-art performance
during time-series classiﬁcation tasks (Ord´o˜nez and
Roggen (2016)). These models – henceforth referred to
as ‘LSTM’ and ‘Bi-LSTM,’ respectively – were trained
using only the time-series of the propositions as inputs.

2. Coupled: In prior research, performance improve-
ments on a primary task have been observed due to
simultaneous training on a secondary related task (Sohn
et al. (2015)). We hypothesized that simultaneously
training the classiﬁer on the secondary task of iden-
tifying scenario phases might improve classiﬁcation
accuracy compared with a standalone RNN. The loss
functions used were binary cross-entropy for each of
the mission objectives and categorical cross-entropy
for the scenario phase identiﬁcation. The overall loss
function was an equally weighted sum of the individual
cost functions. These models were also composed of
LSTM modules and their bidirectional counterparts,
and are referred to as ‘LSTM Coupled’ and ‘Bi-LSTM
Coupled,’ respectively. These models were trained
using the propositions and collected ﬂight phase data.

Evaluations The classiﬁcation models were evaluated
through a four-fold cross-validation wherein the training

Shah et al.

11

Table 2. Weighted F1 scores for both scenario outcomes for
each of the classiﬁers.

Classiﬁer

MO1 MO2 MO3

LSTM
Bi-LSTM
LSTM Coupled
Bi-LSTM Coupled
BSI (Prior 2)
BSI (Prior 3)

0.533
0.533
0.533
0.533
0.674
0.674

0.533
0.533
0.533
0.533
0.712
0.676

0.481
0.481
0.481
0.481
0.877
0.877

dataset was divided into four equal partitions, with three of
the partitions used for training (18 scenarios) and testing
performed on the remaining partition (6 scenarios); this
was repeated across all partitions. The predictions of the
model for each of the scenarios were assimilated at the end.
We also applied our model to the entire dataset in order
to analyze which of the propositions were included in the
maximum a posteriori estimate of the speciﬁcations. The
overall accuracy of the classiﬁers was evaluated using the F1
score on all the predictions for both the possible outcomes
of the mission objectives (’Achieved’ and ’Failed’) for each
mission objective.

Results As presented in Table 2, our model outperformed
RNN-based supervised learning models. With a four-fold split
of training and test data, prior 2 seemed to outperform prior
3; one possible explanation would be that the bias of prior 3
toward longer task chains might result in a higher rate of false
negatives.

We also noticed the tendency of RNN models to collapse
to predicting the most commonly occurring outcome in the
training set for all values of inputs. Thus, the model was
unable to achieve high accuracies even on the training set,
suggesting that it is not only the small size of the dataset that
results in poor performance. This might indicate that either
greater model capacity or a different model architecture may
be required.

Next, we analyzed the maximum a posteriori formula
returned by our model using prior 2, and the F1 scores
obtained were 0.959, 0.918, and 0.959 for the three mission
objectives, respectively. The compositional structure of the
model allowed us to examine the propositions included in
the formulas and interpret the decision boundaries of the
classiﬁers; the results were as follows:

1. MO1 (Gain and maintain air-superiority) The
propositions included in ϕglobal were 7, 3, and 2;
these correspond to a maximum allowable friendly
attrition rate of less than 25%, and enforcing the
condition that the strikers were never ﬁred upon or
shot down, respectively. (This is consistent with the
deﬁnition of air superiority.) The propositions included
in ϕeventual were 4, 1, and 5; these correspond to
strikers eventually releasing their weapons, the friendly
forces shooting down 75% of the enemy ﬁghters, and
strike aircrafts eventually reaching their on-target ﬂight
phase, respectively. (Again, the included propositions
indicate that gaining air superiority allowed strikers to
operate freely.) Finally, ϕorder enforced that friendly
forces shot down 50% of the hostile air threats before
strikers released their weapons.

Prepared using sagej.cls

2. MO2: (Destroy assigned target) The propositions
included in ϕglobal were 7 and 3; these represent a
maximum friendly attrition of 50%, and only enforcing
that the strikers were never shot down, respectively.
(Note that this does not enforce the condition that
strikers were never ﬁred upon.) ϕeventual included 1,
4, 5, and 6; these represent eventually shooting down
all hostile aircrafts (which would seem unnecessary),
strikers entering their on-target ﬂight phase, eventually
releasing their weapons — and, most importantly,
attacking the assigned target. ϕorder enforced the
condition that the friendly aircrafts had to shoot down
all hostiles before the close of the TOT window.

3. MO3: No more than 25% friendly losses: The
propositions in ϕglobal included 7, 2, and 3; these
correctly enforced that no more than 25% friendly
aircrafts could be shot down, and also that the strikers
were never shot down or ﬁred upon. ϕeventual included
1, 4, and 5, representing 75% hostile force attrition, and
enforced that the strikers had to eventually enter their
on-target phase and deploy their weapons. No orders
were included in the formula. The propositions that
enforced weapon deployment by strikers and requisite
hostile attrition were not required for this objective to be
fulﬁlled; however, they were included by the model due
to their frequent occurrence with objective completion.
The compositional nature of the model allows the user
to identify constraints that will be easily enforced.

Conclusion

In this work, we presented a probabilistic model to infer task
speciﬁcations in terms of three behaviors encoded as LTL
templates. We presented three prior distributions that allow for
efﬁcient sampling of candidate formulas as per the templates.
We also presented a likelihood function that depends only
upon the number of conjunctive clauses in the candidate
formula, and is transferable across domains as it requires no
information about the domain itself. Finally, we demonstrated
our model on three distinct evaluation domains. On the
domains where the ground-truth speciﬁcations were known,
we demonstrated the capability of our model to identify the
ground-truth speciﬁcation with up to 90% similarity , in both
a low-dimensional synthetic domain and a real-world dinner
table domain. In the large-force exercise domain, where the
ground-truth speciﬁcations are not known, we showed the
ability of our model to align its predictions with those of an
expert to a greater extent than supervised learning techniques.
We also demonstrated our model’s ability to explain its
decision boundaries due to the compositional nature of the
formula template.

Acknowledgements

This work has been funded by the Lockheed Martin
corporation and the Air Force Research Laboratory. We would
like to acknowledge Dr. Kevin Gluck, and Dr. Donald Duckro
at the Airman Systems Directorate and Zachary ’Zen’ Wallace
at the Rickard Consulting Group for their expertise in mission
design and analysis that were instrumental in creating the LFE
scenario. We would also like to thank David Macannuco at the

12

Journal Title XX(X)

Lockheed Martin corporation for his expertise in modeling
and simulation.

References

Abbeel P and Ng AY (2004) Apprenticeship learning via inverse
In: Proceedings of the twenty-ﬁrst

reinforcement learning.
international conference on Machine learning. ACM, p. 1.
Aldous DJ (1985) Exchangeability and related topics. In: Hennequin
PL (ed.) ´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII —
1983. Berlin, Heidelberg: Springer Berlin Heidelberg. ISBN
978-3-540-39316-0, p. 92.

Argall BD, Chernova S, Veloso M and Browning B (2009) A survey
of robot learning from demonstration. Robotics and autonomous
systems 57(5): 469–483.

Arnold T, Kasenberg D and Scheutz M (2017) Value alignment
or misalignment–what will keep systems accountable. In: 3rd
International Workshop on AI, Ethics, and Society.

Berenson D, Srinivasa S and Kuffner J (2011) Task space regions:
A framework for pose-constrained manipulation planning. The
International Journal of Robotics Research 30(12): 1435–1460.
Chernova S and Thomaz AL (2014) Robot learning from human
teachers. Synthesis Lectures on Artiﬁcial Intelligence and
Machine Learning 8(3): 1–121.

Dwyer MB, Avrunin GS and Corbett JC (1999) Patterns in property
speciﬁcations for ﬁnite-state veriﬁcation. In: Proceedings of the
21st international conference on Software engineering. ACM,
pp. 411–420.

Ellis K, Ritchie D, Solar-Lezama A and Tenenbaum JB (2017)
Learning to infer graphics programs from hand-drawn images.
arXiv preprint arXiv:1707.09627 .

Ellis K, Solar-Lezama A and Tenenbaum J (2015) Unsupervised
In: Advances in neural

learning by program synthesis.
information processing systems. pp. 973–981.

Freer CE, Roy DM and Tenenbaum JB (2014) Towards common-
sense reasoning via conditional simulation: legacies of turing
in artiﬁcial intelligence. In: Downey R (ed.) Turing’s Legacy,
Lecture Notes in Logic, volume 42. Cambridge University Press.
ISBN 9781107338579, pp. 195–252.

Goodman N, Mansinghka V, Roy DM, Bonawitz K and Tenenbaum
JB (2012) Church: a language for generative models. arXiv
preprint arXiv:1206.3255 .

Goodman ND and Stuhlm¨uller A (2014) The Design and
Implementation of Probabilistic Programming Languages.
http://dippl.org. Accessed: 2018-4-9.

Graves A, Fern´andez S and Schmidhuber J (2005) Bidirectional lstm
networks for improved phoneme classiﬁcation and recognition.
In: International Conference on Artiﬁcial Neural Networks.
Springer, pp. 799–804.

Hochreiter S and Schmidhuber J (1997) Long short-term memory 9:

1735–80.

Jin X, Donz´e A, Deshmukh JV and Seshia SA (2015) Mining
IEEE
requirements from closed-loop control models.
Transactions on Computer-Aided Design of Integrated Circuits
and Systems 34(11): 1704–1717.

Kasenberg D and Scheutz M (2017) Interpretable apprenticship
learning with temporal logic speciﬁcations. arXiv preprint
arXiv:1710.10532 .

Kim J, Banks CJ and Shah JA (2017) Collaborative planning with
encoding of users’ high-level strategies. In: AAAI. pp. 955–962.

Prepared using sagej.cls

Kong Z, Jones A and Belta C (2017) Temporal logics for learning
and detection of anomalous behavior. IEEE Transactions on
Automatic Control 62(3): 1210–1222.

Kong Z, Jones A, Medina Ayala A, Aydin Gol E and Belta C (2014)
Temporal logic inference for classiﬁcation and prediction from
data. In: Proceedings of the 17th international conference on
Hybrid systems: computation and control. ACM, pp. 273–282.
Konidaris G, Kuindersma S, Grupen R and Barto A (2012) Robot
learning from demonstration by constructing skill trees. The
International Journal of Robotics Research 31(3): 360–375.
Kress-Gazit H, Fainekos GE and Pappas GJ (2009) Temporal-logic-
based reactive mission and motion planning. IEEE transactions
on robotics 25(6): 1370–1381.

Laplace PS (1951) A philosophical essay on probabilities, translated
from the 6th french edition by frederick wilson truscott and
frederick lincoln emory.

Lemieux C, Park D and Beschastnikh I (2015) General

ltl
speciﬁcation mining (t). In: Automated Software Engineering
(ASE), 2015 30th IEEE/ACM International Conference on.
IEEE, pp. 81–92.

Li X, Vasile CI and Belta C (2017) Reinforcement learning with
In: Intelligent Robots and Systems
temporal logic rewards.
(IROS), 2017 IEEE/RSJ International Conference on. IEEE, pp.
3834–3839.

Littman ML, Topcu U, Fu J, Isbell C, Wen M and MacGlashan
J (2017) Environment-independent task speciﬁcations via gltl.
arXiv preprint arXiv:1704.04341 .

Ng AY and Russell SJ (2000) Algorithms for inverse reinforcement
In: Proceedings of the Seventeenth International
learning.
Conference on Machine Learning, ICML ’00. San Francisco,
ISBN 1-
CA, USA: Morgan Kaufmann Publishers Inc.
55860-707-2, pp. 663–670. URL http://dl.acm.org/
citation.cfm?id=645529.657801.

Niekum S, Osentoski S, Konidaris G, Chitta S, Marthi B and Barto
AG (2015) Learning grounded ﬁnite-state representations from
unstructured demonstrations. The International Journal of
Robotics Research 34(2): 131–157.

Ord´o˜nez FJ and Roggen D (2016) Deep convolutional and lstm
recurrent neural networks for multimodal wearable activity
recognition. Sensors 16(1). DOI:10.3390/s16010115. URL
http://www.mdpi.com/1424-8220/16/1/115.
Paul J (1912) The distribution of the ﬂora in the alpine zone.1.
New Phytologist 11(2): 37–50. DOI:10.1111/j.1469-8137.1912.
tb05611.x.

Pnueli A (1977) The temporal logic of programs. In: Foundations
of Computer Science, 1977., 18th Annual Symposium on. IEEE,
pp. 46–57.

Raman V, Donz´e A, Sadigh D, Murray RM and Seshia SA (2015)
Reactive synthesis from signal temporal logic speciﬁcations. In:
Proceedings of the 18th International Conference on Hybrid
Systems: Computation and Control. ACM, pp. 239–248.
Shah A, Kamath P, Li S and Shah J (2018) Bayesian inference of
temporal task speciﬁcations from demonstrations. In: NIPS (to
appear).

Shepard R (1987) Toward a universal law of generalization for
psychological science. Science 237(4820): 1317–1323. DOI:
10.1126/science.3629243.

Sohn K, Lee H and Yan X (2015) Learning structured output
representation using deep conditional generative models. In:
Cortes C, Lawrence ND, Lee DD, Sugiyama M and Garnett R

Shah et al.

13

(eds.) Advances in Neural Information Processing Systems 28.
Curran Associates, Inc., pp. 3483–3491.

Tenenbaum JB (2000) Rules and similarity in concept learning. In:
Advances in neural information processing systems. pp. 59–65.
Toris R, Kent D and Chernova S (2015) Unsupervised learning
of multi-hypothesized pick-and-place task templates via
In: Robotics and Automation (ICRA), 2015
crowdsourcing.
IEEE International Conference on. IEEE, pp. 4504–4510.
Yoo C and Belta C (2017) Rich time series classiﬁcation using

temporal logic. In: Robotics: Science and Systems.

Ziebart BD, Maas AL, Bagnell JA and Dey AK (2008) Maximum
entropy inverse reinforcement learning. In: AAAI, volume 8.
Chicago, IL, USA, pp. 1433–1438.

Prepared using sagej.cls

