0
2
0
2

b
e
F
1

]
L
P
.
s
c
[

4
v
3
2
5
4
0
.
1
1
9
1
:
v
i
X
r
a

A Simple Diﬀerentiable Programming Language∗

MARTÍN ABADI, Google Research, United States
GORDON D. PLOTKIN, Google Research, United States

Automatic diﬀerentiation plays a prominent role in scientiﬁc computing and in modern machine learning,
often in the context of powerful programming systems. The relation of the various embodiments of automatic
diﬀerentiation to the mathematical notion of derivative is not always entirely clear—discrepancies can arise,
sometimes inadvertently. In order to study automatic diﬀerentiation in such programming contexts, we deﬁne
a small but expressive programming language that includes a construct for reverse-mode diﬀerentiation. We
give operational and denotational semantics for this language. The operational semantics employs popular
implementation techniques, while the denotational semantics employs notions of diﬀerentiation familiar from
real analysis. We establish that these semantics coincide.

CCS Concepts: • Theory of computation → Denotational semantics; Operational semantics; • Soft-
ware and its engineering → Domain speciﬁc languages; • Computing methodologies → Machine
learning.

Additional Key Words and Phrases: automatic diﬀerentiation, diﬀerentiable programming.

ACM Reference Format:
Martín Abadi and Gordon D. Plotkin. 2020. A Simple Diﬀerentiable Programming Language. Proc. ACM Pro-
gram. Lang. 4, POPL (January 2020), 29 pages. https://doi.org/10.1145/3371106

1 INTRODUCTION
Automatic diﬀerentiation is a set of techniques for calculating the derivatives of functions de-
scribed by computer programs (e.g., [Baydin et al. 2018; Griewank 2000; Hascoët and Pascual 2013;
Pearlmutter and Siskind 2008]). These techniques are not required to produce symbolic represen-
tations for derivatives as in classic symbolic diﬀerentiation; on the other hand, neither do they em-
ploy ﬁnite-diﬀerence approximation methods common in numerical diﬀerentiation. Instead, they
rely on the chain rule from calculus to obtain the desired derivatives from those of the programs’s
basic operations. Thus, automatic diﬀerentiation is at the intersection of calculus and program-
ming. However, the programs of interest are more than chains of operations: they may include
control-ﬂow constructs, data structures, and computational eﬀects (e.g., side-eﬀects or exceptions).
Calculus does not provide an immediate justiﬁcation for the treatment of such programming-
language features.

In the present work we help bridge the gap between rules for automatic diﬀerentiation in ex-
pressive programming languages and their mathematical justiﬁcation in terms of denotational

∗This version of the paper is the POPL publication version, but with some minor corrections.

Authors’ addresses: Martín Abadi, Google Research, United States, abadi@google.com; Gordon D. Plotkin, Google Research,
United States, plotkin@google.com.

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and
the full citation on the ﬁrst page. Copyrights for third-party components of this work must be honored. For all other uses,
contact the owner/author(s).
© 2020 Copyright held by the owner/author(s).
2475-1421/2020/1-ART
https://doi.org/10.1145/3371106

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

 
 
 
 
 
 
2

Martín Abadi and Gordon D. Plotkin

semantics. Speciﬁcally, we consider automatic diﬀerentiation from a programming-language per-
spective by deﬁning and studying a small but powerful functional ﬁrst-order language. The lan-
guage has conditionals and recursively deﬁned functions (from which loops can be constructed),
but only rudimentary data structures. Additionally, it contains a construct for reverse-mode diﬀer-
entiation, explained in detail below. Our language is thus inspired by modern systems for machine
learning, which include standard programming constructs and support reverse-mode diﬀerenti-
ation. Reverse-mode diﬀerentiation permits the computation of gradients, forward-mode deriva-
tives, and more. Indeed as our diﬀerentiation construct is a language primitive, diﬀerentiations
can be nested within diﬀerentiations, allowing the computation of higher-order derivatives.

In the setting of a language such as ours, we can consider some common approaches to imple-

menting diﬀerentiation:

• One approach relies on code transformation, whether on source code or intermediate repre-
sentations. For example, for the derivative of a conditional expression if B then M1 else M2,
it would output if B then N1 else N2, where N1 and N2 are the derivatives of M1 and M2
respectively. This approach is employed, for instance, in Theano [Bergstra et al. 2010], Ten-
sorFlow 1.0 [Abadi et al. 2016a; Yu et al. 2018], and Tangent [van Merrienboer et al. 2018].
• Another approach relies on tracing, typically eliminating control structures to produce a
simpler form of code, which we call an execution trace, that can more easily be diﬀerentiated.
For example, to produce the derivative of if B then M1 else M2, tracing would evaluate the
conditional and produce a trace of the branch taken. Execution traces correspond to graphs
of basic operations, and can be taken to be sequences of elementary assignments or else
functional programs in A-normal form. Their derivatives can be calculated by applying the
chain rule to those basic operations, perhaps via a code transformation (but now of a much
simpler kind). Tracing may also record some intermediate values in an evaluation trace, to
reduce, or eliminate, the need for recomputation.1
This approach thereby conveniently avoids the problem of deﬁning code transformations
for conditionals and many other language constructs. It can also be implemented eﬃciently,
sometimes in part with JIT compilation. For these reasons, trace-based diﬀerentiation is of
growing importance. It is employed, for instance, in Autograd [Maclaurin et al. 2015], Ten-
sorFlow Eager Mode [Agrawal et al. 2019], Chainer [Tokui et al. 2015], PyTorch [Paszke et al.
2019], and JAX [Frostig et al. 2018]2.

We therefore focus on trace-based diﬀerentiation, and give our language an operational se-
mantics using the trace-based approach. To do so, we deﬁne a sublanguage of execution trace
terms (called simply trace terms below). These have no conditionals, function deﬁnitions or calls,
or reverse-mode diﬀerentiations. They do have local deﬁnitions, corresponding to fanout in the
graphs, but may not be in A-normal form. Tracing is modeled by a new kind of evaluation, called
symbolic evaluation. This uses an environment for the free variables of a term to remove condi-
tionals and function calls. Function derivatives at a given value are evaluated in three stages: ﬁrst,
the function is traced at that value; next, the resulting trace term is symbolically diﬀerentiated
(largely just using the chain rule), resulting in another such trace term; and, ﬁnally, that term is
evaluated.

1Terminology in the automatic diﬀerentiation literature varies. Here we follow [Griewank 2000] for evaluation traces. Our
execution traces are, perhaps in somewhat diﬀerent manifestations, variously termed Wengert lists or tapes or evaluation
traces [Baydin et al. 2018; Pearlmutter and Siskind 2008]. They can also be seen as combinations of the operation and index
traces of [Griewank 2000].
2See https://www.sysml.cc/doc/146.pdf.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

3

We do not account for some of the optimizations used in practice. Doing so would have been a
more complicated enterprise, possibly with more arbitrary choices tied to implementation details,
and we wished to get a more straightforward formalization working ﬁrst.

From a mathematical perspective, both approaches to implementing diﬀerentiation pose cor-
rectness problems. In particular, functions deﬁned using conditionals need not be continuous, let
alone diﬀerentiable. Consider, for example, the following deﬁnition

f (x : real) : real = if (x < 0) then 0 else x

of the popular ReLU function [Goodfellow et al. 2016]. This function is not diﬀerentiable at 0. Fur-
ther, changing the function body to if (x < 0) then 0 else 1 yields a non-continuous function.
What is more, both approaches can produce wrong answers even for diﬀerentiable functions! Con-
sider, for example, the following deﬁnition of the identity function on the reals:

д(x : real) : real = if (x = 0) then 0 else x
The derivative of this function at x = 0 is 1. However, diﬀerentiation “by branches” (whether by
code transformation or tracing) would produce the wrong answer, 0.

In order to capture the mathematical perspective, in addition to its operational semantics we
give our language a denotational semantics. This semantics is based on classical notions of diﬀer-
entiation from real analysis (see, for example, [Trench 2003]). That theory concerns multivariate
functions on the reals deﬁned on open domains, i.e., partial such functions with open domains
of deﬁnition. In our semantics, we make use of those that are smooth (that is, those that can be
partially diﬀerentiated any number of times). A particularly pleasing aspect of this mathematical
development is how well domain theory (needed to account for recursion) interacts with diﬀeren-
tiation.

Partiality is necessary, as for any language with general recursion, but it also gives us useful
ﬂexibility in connection with diﬀerentiation. For example, let Û< be the approximation to < which
is equal to it except on the diagonal (i.e., where both arguments are equal) where it is undeﬁned.
Then

Ûf (x : real) : real = if (x Û< 0) then 0 else x
deﬁnes an approximation to ReLU which is undeﬁned at 0. The approximation to < is (unlike <)
continuous (i.e., the pre-images of true and false are open sets), and the approximation to ReLU
is diﬀerentiable wherever it is deﬁned. Therefore, we design the semantics of our language so that
it forbids functions such as f but allows related approximations such as Ûf . An interesting question
is how satisfactory an idealization this is of programming practice (which in any case works with
approximate reals). We return to this point in the ﬁnal section.

Proceeding in this way, we obtain adequacy theorems (i.e., operational soundness and complete-
ness theorems) connecting the operational semantics of our language with a denotational seman-
tics based on the classical theory of diﬀerentiation of partially deﬁned multivariate real functions.
Our theorems apply not only to conditional expressions but to the full language.

In sum, the main contributions of this paper are: (1) a ﬁrst-order language with conditionals,
recursive function deﬁnitions, and a reverse-mode diﬀerentiation construct; (2) an operational se-
mantics that models one form of trace-based diﬀerentiation; (3) a denotational semantics based on
standard mathematical notions from real analysis and domain theory; and (4) theorems that show
that the two semantics coincide, i.e., the derivatives computed by the operational semantics are
indeed the correct derivatives in a mathematical sense. Beyond the speciﬁcs of these results, this
paper aims to give some evidence of the relevance of ideas and techniques from the programming-
languages literature for programming systems that include automatic diﬀerentiation, such as cur-
rent systems for machine learning.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

4

Martín Abadi and Gordon D. Plotkin

Additional context. While traditionally associated with scientiﬁc computing, automatic diﬀerenti-
ation is now a central component of many modern machine learning systems, and those for deep
learning in particular [Goodfellow et al. 2016]. These systems often employ automatic diﬀeren-
tiation to compute the gradients of “loss functions” with respect to parameters, such as neural
network weights. Loss functions measure the error resulting from particular values for the param-
eters. For example, when a machine-learning model is trained with a dataset consisting of pairs
(x0, y0), . . . , (xn−1, yn−1), aiming to learn a function that maps the xi ’s to the yi ’s, the loss function
may be the distance between the yi ’s and the values the model predicts when presented with the
xi ’s. By applying gradient descent to adjust the parameters, this error can be reduced, until conver-
gence or (more commonly) until the error is tolerable enough. This simple approach has proven
remarkably eﬀective: it is at the core of many recent successes of machine learning in a variety of
domains.

Whereas gradients are for functions of type realn → real, for n ≥ 0, treating the more gen-
eral functions of type realn → realm, for n, m ≥ 0, works better with function composition, and
with the composite structures such as tensors of reals used in deep learning. The literature contains
two basic “modes” for diﬀerentiating such functions. Forward-mode extends the computation of
the function, step by step, with the computation of derivatives; it can be seen as evaluating the
function on dual numbers of the form v + Ûvε where ε is nilpotent. In contrast, reverse-mode prop-
agates derivatives backwards from each output, typically after the computation of the function.
Reverse-mode diﬀerentiation is often preferred because of its superior eﬃciency for functions of
type realn → realm with m << n. In particular, systems for machine learning, which often deal
with loss functions for which m = 1, generally rely on reverse-mode diﬀerentiation. We refer the
reader to the useful recent survey [Baydin et al. 2018] for additional background on these two
modes of diﬀerentiation; it also discusses the use of higher-order diﬀerentiation.

Applications to machine learning are our main motivation. Accordingly, our language is loosely
inspired by systems for machine learning, and the implementation strategies that we consider are
ones of current interest there. We also de-emphasize some concerns (e.g., numerical stability) that,
at present, seem to play a larger role in scientiﬁc computing than in machine learning. As noted
in [Baydin et al. 2016], the machine learning community has developed a mindset and a body of
techniques distinct from those traditional in automatic diﬀerentiation.

The literature on scientiﬁc computing has addressed the correctness problem for condition-
als [Beck and Fischer 1994; Fischer 2001], although not in the context of a formally deﬁned pro-
gramming language. In [Mayero 2002] a formal proof of correctness for an algorithm for the au-
tomatic diﬀerentiation of straight-line sequences of Fortran assignments was given using the Coq
theorem prover [Bertot and Castéran 2013]. Closer to machine learning, [Selsam et al. 2017] con-
sider a stochastic graphical formalism where the nodes are random variables, and use the Lean
theorem prover [de Moura et al. 2015] to establish the correctness of stochastic backpropagation.
However, overall, the literature does not seem to contain semantics and theorems for a language
of the kind we consider here.

Our work is also related to important papers by Ehrhard, Regnier, et al. [Ehrhard and Regnier
2003], and by Di Gianantonio and Edalat [Di Gianantonio and Edalat 2013]. Ehrhard and Regnier
introduce the diﬀerential λ-calculus; this is a simply-typed higher-order λ-calculus with a forward-
mode diﬀerentiation construct which can be applied to functions of any type. It can be modeled us-
ing the category of convenient vector spaces and smooth functions between them (see [Blute et al.
2010; Kriegl and Michor 1997]). Ehrhard and Regnier do not give an operational semantics but
they do give rules for symbolic diﬀerentiation and it should not be too diﬃcult to use them to
give an operational semantics. However their language with its convenient vector space seman-
tics only supports total functions. It therefore cannot be extended to include recursive function

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

5

deﬁnitions or conditionals (even with total predicates, as continuous functions from Rn to the
booleans are constant). Di Gianantonio and Edalat prove adequacy theorems for their language,
as do we, but their work diﬀers from ours in several respects. In particular, their language has
ﬁrst-order forward-mode but no reverse-mode diﬀerentiation: our language eﬀectively supports
both, and at all orders. On the other hand, their language allows recursively-deﬁned higher-order
functions and accommodates functions, such as the ReLU function, which are diﬀerentiable in
only a weaker sense. As far as we know, no other work on diﬀerentiable programming languages
(e.g., [Elliott 2018; Manzyuk 2012; Pearlmutter and Siskind 2008; Shaikhha et al. 2018; Wang et al.
2018]) gives operational and denotational semantics and proves adequacy theorems. Further aﬁeld,
there is a good deal of work in the categorical literature on categories equipped with diﬀerential
structure, for example [Blute et al. 2009; Bucciarelli et al. 2010].
Outline. Section 2 deﬁnes our language. Section 3 gives it an operational semantics with rules for
symbolically evaluating general terms to trace terms, and for symbolically diﬀerentiating these
terms. Sections 4 and 5 cover the needed mathematical material and the denotational semantics.
Sections 6 establishes the correspondence between operational and denotational semantics. Sec-
tion 7 concludes with discussion and some suggestions for future work.

2 A SIMPLE LANGUAGE
The types S,T , U , . . . of our language are given by the grammar:
::= real | unit | T × U

T

We will make use of iterated products T0 ×. . . ×Tn−1, deﬁned to be unit when n = 0, T0 when n = 1,
and, recursively, (T0 ×. . . ×Tn−1)×Tn, when n > 1; we write realn for the n-fold iterated product of
real. Note that this type system includes the types of tensors (multidimensional arrays) of a given
shape: the type of tensors of shape (d0, . . . , dn−1) is the iterated product reald0 ×. . .×realdn−1 . The
terms L, M, N , P, . . . and boolean terms B of the language are built from operation symbols op ∈ Op
and predicate symbols pred ∈ Pred. An example operation symbol could be DProdn for dot product
of vectors of dimension n (for n ∈ N); an example predicate symbol could be Û<.

The terms are given by the following grammar, where x and f range over disjoint countably
inﬁnite alphabets of ordinary and function variables, respectively. We assume available a standard
ordering of the function variables.

M ::= x | r (r ∈ R) | M + N | op(M) |
let x : T = M in N |
∗ | hM, N iT ,U | fstT ,U (M) | sndT ,U (M) |
if B then M else N |
letrec f (x : T ) : U = M in N |
f (M) |
M.rdL(x : T . N )

B

::= pred(M) | true | false

These constructs are all fairly standard, except for rd, which is for reverse-mode diﬀerentiation,
and which we explain below. We treat addition separately from the operations to underline the fact
that the commutative monoid it forms, together with zero, is basic for diﬀerentiation. For example,
the rules for symbolic diﬀerentiation given below make essential use of this structure, but do not
need any more of the available vector space structure.

Note the type subscripts on pairing and projection terms. Below, we rely on these subscripts for

symbolic diﬀerentiation. In practice they could, if needed, be added when type-checking.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

6

Martín Abadi and Gordon D. Plotkin

The sets FV(M) and FFV(M) of free ordinary variables and free function variables of a term M
are understood as usual (and similarly for boolean terms). As is also usual, we do not distinguish
α-equivalent terms (or boolean terms).

The useful abbreviation

let hx0 :T0, . . . , xn−1 :Tn−1i = M in N

provides an elimination construct for iterated products. When n = 0 this is

let x : unit = M in N
where x < FV(N ); when n = 1 it is the above let construct; otherwise, it is deﬁned recursively by:

let hx0 :T0, . . . , xn :Tni = M in N =

(where z is chosen not free in N ).

let z : (T0 × . . . × Tn) = M in
let hx0 :T0, . . . , xn−1 :Tn−1i = fstT0×...×Tn−1,Tn (z) in
let xn :Tn = sndT0×...×Tn−1,Tn (z) in N

We have zero and addition only at type real. At other types we proceed inductively:
0unit = ∗

0T ×U = h0T , 0U i

0real = 0

and:

M +unit N =

let x : unit = M in
let y : unit = N in ∗

M +T ×U N =

let hx1 :T , x2 :U i = M in
let hy1 :T , y2 :U i = N in
hx1 +T y1, x2 +U y2i

Skating over the diﬀerence between terms and their denotations, M.rdL(x :T . N ) is the reverse-
mode derivative at L : T , evaluated at M : U , of the function f such that f (x : T ) : U = N .
Reverse-mode diﬀerentiation includes gradients as a special case. When T = realn and U = real,
the gradient of f at L is given by:

gradL(x : realn . N ) = 1.rdL(x : realn. N )
For deﬁnitions of gradients, Jacobians, and derivatives see Section 4.1 below, particularly equa-
tions (1), (2), (3), and (4). More generally, for an introduction to real analysis including vector-
valued functions of several variables and their diﬀerentials and Jacobians, see, for example, [Trench
2003].

As in [Christianson 2012], and as validated by equation (6) below, forward-mode diﬀerentiation

can be deﬁned using nested reverse-mode diﬀerentiation. We can set:

M.fdU , L(x :T . N ) = M.rd0U (y :U . y.rdL(x :T . N ))

So our language eﬀectively also has forward-mode diﬀerentiation.

Function deﬁnitions can be recursive. Indeed a function can even be deﬁned in terms of its own
derivative: in a recursive function deﬁnition letrec f (x :T ) :U = M in N , the language allows
occurrences of f within the term N ′ in a sub-term M ′.rdL′(y : T ′. N ′) of M. This generality may
be useful—examples have arisen in the context of Autograd [Maclaurin et al. 2015]3. Pleasantly,
both our operational and denotational semantics accommodate it without special complications.
When we deﬁne a function without recursion, we may abbreviate letrec to let.

3See https://dougalmaclaurin.com/talk.pdf

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

7

Turning to typing, operation and predicate symbols have given arities, written op :T → U and
pred :T ; we write OpT ,U for the set of operation symbols of arity T → U . For example, we would
have DProdn : realn × realn → real and Û< : real2. Figure 1 gives typing rules for sequents

where (type) environments Γ have the form

Φ | Γ ⊢ M :T

Φ | Γ ⊢ B

x0 :T0, . . . , xn−1 :Tn−1

(xi all diﬀerent) and where function (type) environments Φ have the form

f0 :T0 →U0, . . . , fn−1 :Tn−1 →Un−1
(fi all diﬀerent). We adopt the usual overwriting notations Γ[Γ ′] and Φ[Φ′] for type environments.

Φ | Γ ⊢ x :T (x :T ∈ Γ)

Φ | Γ ⊢ r : real (r ∈ R)

Φ|Γ⊢M : real

Φ|Γ⊢N : real

Φ|Γ⊢M+N : real

Φ|Γ⊢M : T
Φ|Γ⊢op(M) : U

(op : T → U )

Φ|Γ⊢M : T

Φ|Γ[x : T ]⊢N : U

Φ|Γ⊢let x : T = M in N : U

Φ | Γ ⊢ ∗ : unit

Φ|Γ⊢M : T

Φ|Γ⊢N : U

Φ|Γ⊢hM, N iT ,U : T ×U

Φ|Γ⊢M : T ×U
Φ|Γ⊢fstT ,U (M) : T

Φ|Γ⊢M : T ×U
Φ|Γ⊢sndT ,U (M) : U

Φ|Γ⊢B

Φ|Γ⊢M : T

Φ|Γ⊢N : T

Φ|Γ⊢if B then M else N : T

Φ[f : T →U ]|x : T ⊢M : U

Φ[f : T →U ]|Γ⊢N : S

Φ|Γ⊢ letrec f (x : T ) : U = M in N : S

Φ|Γ⊢M : T
Φ|Γ⊢f (M) : U

(f

: T → U ∈ Φ)

Φ|Γ[x : T ]⊢N : U

Φ|Γ⊢L : T

Φ|Γ⊢M .rdL (x : T . N ) : T

Φ|Γ⊢M : U

Φ | Γ ⊢ true

Φ | Γ ⊢ false

Φ|Γ⊢M : T
Φ|Γ⊢pred(M)

(pred : T )

Fig. 1. Typing rules

The typing rule for function deﬁnitions forbids any global variable occurrences (i.e., free vari-
ables in function deﬁnitions). This restriction involves no loss in expressiveness: as in lambda
lifting, one can just add any global variables to a function’s parameters, and then apply the func-
tion to the global variables wherever it is called. The restriction enabled us to prove part (2) of
Theorem 6.2 (below), but we conjecture it is not needed.

Our various abbreviations have natural admissible typing rules:

Φ | Γ ⊢ M :T0 × . . . , ×Tn−1

Φ | Γ[x0 :T0, . . . , xn−1 :Tn−1] ⊢ N : U

Φ | Γ ⊢ let hx0 :T0, . . . , xn−1 :Tn−1i = M in N :U

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

8

Martín Abadi and Gordon D. Plotkin

Φ | Γ ⊢ N : realn

Φ | Γ[x : realn] ⊢ M : real

Φ | Γ ⊢ gradN (x : realn. M) : realn

Φ | Γ[x :T ] ⊢ N :U

Φ | Γ ⊢ L :T

Φ | Γ ⊢ M :T

Φ | Γ ⊢ M.fdU , L(x :T . N ) :U

Φ | Γ ⊢ 0T :T

Φ | Γ ⊢ M :T

Φ | Γ ⊢ N :T

Φ | Γ ⊢ M +T N :T

We may write Γ ⊢ M : T (or ⊢ M : T ) instead of Φ | Γ ⊢ M : T if M has no free ordinary (or
function) variables (and similarly for boolean terms). Typing is unique: for any Γ, Φ, and M there
is at most one type T such that Φ | Γ ⊢ M : T holds.

As an example, we use our language to program a miniature version of an algorithm for train-
ing a machine learning model by gradient descent, loosely based on [Goodfellow et al. 2016, Al-
gorithm 8.1]. In such training, one often starts with an untrained model, which is a function from
inputs (for example, images) and parameter values to output “predictions” (for example, image
labels). Relying on a dataset of input/output pairs, one then picks values of the parameters by
gradient descent, as indicated in the Introduction. In our miniature version, we treat inputs, pa-
rameter values, and outputs as reals, and we assume that the training data consists of only one
ﬁxed input/output pair (a, b). We also assume that we have real constants w0 (for the initial value
for gradient descent), rate (for the learning rate, which is ﬁxed) and maxLoss (for the desired max-
imum loss on the dataset), and the inﬁx predicate symbol Û<. We can then deﬁne the trained model
from the untrained model and a loss function as follows:
let myTrainedModel(x : real) : real =
let currentLoss(w : real) : real = myLoss(hb, myUntrainedModel(ha, wi)i) in
let gradLoss(w : real) : real = gradw (w ′ : real.currentLoss(w ′)) in
letrec descend(w : real) : real =

if currentLoss(w) Û< c then w else descend(w − rate ∗ gradLoss(w)) in

myUntrainedModel(hx, descend(w0)i)
in . . .

The example above is typical of what can be expressed in our language, and many variants
of machine learning techniques that rely on gradient descent (e.g., as in [Goodfellow et al. 2016],
and commonly used in systems like TensorFlow) are in scope as well. For instance, there is no
diﬃculty in expressing optimization with momentum, or diﬀerentially private stochastic gradient
descent (e.g., [Abadi et al. 2016b; Song et al. 2013]). Probabilistic choice may be treated via random
number generators, as is done in practice. Architectures that rely on convolutions or RNN cells
can be expressed, even conveniently, with a suitable choice of primitives.

3 OPERATIONAL SEMANTICS
We give a big-step operational semantics, speciﬁed with Felleisen and Friedman’s method using
evaluation contexts and redexes [Felleisen and Friedman 1987]. Other styles of operational seman-
tics accommodating diﬀerentiation are surely also possible.

Terms and boolean terms are (ordinarily) evaluated to closed values and (necessarily) closed
boolean values. The most original aspect of our operational semantics concerns the evaluation of
diﬀerential terms; this is based on the trace-based approach outlined in the Introduction, and uses
a second mode of evaluation: symbolic evaluation.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

9

The core idea is that to evaluate a diﬀerential term

M.rdL(x :T . N )

one ﬁrst evaluates L and M, and then performs diﬀerentiation before evaluating further. There
are two diﬀerentiation stages. First, using the closed value V of L for the diﬀerentiation variable
x, N is symbolically evaluated to a trace term C, thereby removing all control constructs from
N , but possibly keeping the variable x free in C, as the derivative may well depend on it. For
example, when N is if x Û< 0 then 0 else x, the value V allows the guard of the conditional
to be evaluated, but the occurrence of x in the else branch is not replaced by V . Second, C is
symbolically diﬀerentiated at V with respect to x.

However, this idea is not enough by itself as the diﬀerential term may occur inside yet another
diﬀerential term. One therefore also needs to be able to symbolically evaluate the diﬀerential term.
That is done much as in the case of ordinary evaluation, but now symbolically evaluating redexes
in L and M until one is left with the problem of symbolically evaluating a term of the form

W .rdV (x :T . N )

where V and W are values that may contain free variables. One then proceeds as above, sym-
bolically evaluating N (now using the closed value V ′ of V ) and then performing symbolic dif-
ferentiation. As there is some duplication between these two symbolic and ordinary evaluation
processes, our rule for ordinarily evaluating a diﬀerential term is designed, when executed, to ﬁrst
symbolically evaluate the term, and then ordinarily evaluate the resulting trace term.

The need to keep track of diﬀerentiation variables and their values for symbolic evaluation leads
us to use value environments for ordinary variables. It is convenient to also use them for ordinary
evaluation and to use function environments for function variables for both modes of evaluation.

Values V ,W , X , . . . are terms given by the grammar:

V ::= x | r (r ∈ R) | ∗ | hV ,W iT ,U

Note that, as indicated above, values may have free variables for the purposes of diﬀerentiation.
Boolean values Vbool are boolean terms given by:

Vbool ::= true | false

Closed values have unique types ⊢ V : TV ; the set of closed values of type T is ValT ; and the
set of boolean values is Valbool. We assume available operation and predicate symbol evaluation
functions

ev : OpT ,U × ValT ⇀ ValU

bev : PredT × ValT ⇀ Valbool

We also assume that for every operator op :T → U there is an operator opr :T × U → T . The idea
is that opr (hL, Mi) is the reverse-mode derivative of op at L evaluated at M. We write M.opr (L) for
opr (hL, Mi). For example, for DProd2 we would have:

ev(DProd2, hha, bi, ha′, b ′ii) = aa′ + bb ′

and

ev((DProd2)r, hhha, bi, ha′, b ′ii, ci) = hhca′, cb ′i, hca, cbii
We next deﬁne (value) environments ρ, function environments φ, and (recursive function) closures

Cl, the last two mutually recursively:

- Value environments are ﬁnite functions

ρ = {x0 7→ V0, . . . , xn−1 7→ Vn−1}

from ordinary variables to closed values.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

10

Martín Abadi and Gordon D. Plotkin

- Every ﬁnite function

φ = { f0 7→ Cl0, . . . , fn−1 7→ Cln−1}

from function variables to closures is a function environment.

- If FV(M) ⊆ {x } and FFV(M)\{ f } ⊆ Dom(φ) then hφ, f , x,T , U , Mi is a closure, written

cloφ (f (x : T ) : U . M).

For any V and ρ with FV(V ) ⊆ Dom(ρ), ρ(V ) is the closed value obtained by substituting ρ(x) for
all free occurrences of x in V .

Trace terms C, D, . . ., are deﬁned as follows:

C ::= x | r (r ∈ R) | C + D | op(C) |
let x :T = C in D |
∗ | hC, DiT ,U | fstT ,U (C) | sndT ,U (C)

They are the terms with no conditionals, function deﬁnitions or applications, or diﬀerentiations.

We will deﬁne two ordinary evaluation relations, and one symbolic one:

• For all φ and ρ we deﬁne evaluation relations between terms and closed values and between

boolean terms and closed boolean values via rules establishing sequents of the forms:

φ | ρ ⊢ M ⇒ V

φ | ρ ⊢ B ⇒ Vbool

• For all φ and ρ we deﬁne a symbolic evaluation relation between terms and trace terms via

rules establishing sequents of the form:

φ | ρ ⊢ M { C
Evaluation contexts (boolean evaluation contexts), ranged over by E (resp. Ebool ), are terms with

a unique hole [ ] :

E

::= [ ] | E + N | V + E | op(E) |
let x :T = E in N |
hE, N iT ,U | hV , EiT ,U | fstT ,U (E) | sndT ,U (E) |
if Ebool then M else N |
f (E) |
M.rdE (x : T . N ) | E.rdV (x : T . N )

Ebool ::= pred(E)

We write E[M] for the term obtained by replacing the hole [ ] in E by the term M and Ebool [M]
similarly; a context E is trivial if it is [ ]; and FV and FFV are extended to contexts. We have
FV(E[M]) = FV(E) ∪ FV(M) and FFV(E[M]) = FFV(E) ∪ FFV(M) and similarly for boolean contexts.

Redexes, ranged over by R, and boolean redexes, ranged over by Rbool , are given by:

R

::= V + W | op(V ) |

let x :T = V in N |
fstT ,U (V ) | sndT ,U (V ) |
if Vbool then M else N |
letrec f (x :T ) :U = M in N | f (V ) |
W .rdV (x :T . N )

Rbool ::= pred(V )

Note that boolean expressions are useful here in that they enable separate conditional and pred-
icate redexes, and so evaluating predicates and making choices are distinct in the operational
semantics.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

11

The next lemma is the basis of a division into cases that supports operational semantics using

evaluation contexts in the style of Felleisen and Friedman.

Lemma 3.1 (Evaluation context analysis).

(1) Every term M, other than a value, has exactly one of the following two forms:

• E[R] for a unique evaluation context and redex, or
• E[Rbool] for a unique, and non-trivial, evaluation context and boolean redex.

(2) Every boolean term B, other than a boolean value, has exactly one of the following two forms:

• Ebool [R] for a unique, and non-trivial, boolean evaluation context and redex, or
• Ebool [Rbool] for a unique boolean evaluation context and boolean redex.

The next lemma is useful to track types when proving theorems about the operational semantics.

Lemma 3.2 (Evaluation context polymorphism). Suppose that Φ | Γ ⊢ E[M] :T . Then, for some

type U we have Φ | Γ ⊢ M :U and, whenever Φ | Γ ⊢ N :U , we have Φ | Γ ⊢ E[N ] :T .

Analogous results hold for typings of any of the forms Φ | Γ ⊢ E[B] : T or Φ | Γ ⊢ Ebool[M] or

Φ | Γ ⊢ Ebool[B].

By the uniqueness of types, the types whose existence is claimed in the above lemma are unique.

φ | ρ ⊢ V ⇒ ρ (V )

φ |ρ ⊢V ⇒r

φ |ρ ⊢W ⇒s

φ |ρ ⊢V +W ⇒t
(where t = r + s)

φ |ρ ⊢V ⇒V ′
φ |ρ ⊢op(V )⇒W

(where ev(op, V ′) ≃ W )

φ |ρ ⊢V ⇒V ′

φ |ρ [V ′/x ]⊢N ⇒W

φ |ρ ⊢let x:T = V in N ⇒W

φ |ρ ⊢V ⇒hW1,W2 iT ,U
φ |ρ ⊢fstT ,U (V )⇒W1

φ |ρ ⊢V ⇒hW1,W2 iT ,U
φ |ρ ⊢sndT ,U (V )⇒W2

φ |ρ ⊢M⇒V
φ |ρ ⊢if true then M else N ⇒V

φ |ρ ⊢N ⇒V
φ |ρ ⊢if false then M else N ⇒V

φ[cloφ (f (x :T ):U . M)/f ]|ρ ⊢N ⇒V
φ |ρ ⊢ letrec f (x :T ):U = M in N ⇒V

φ |ρ ⊢V ⇒V ′

φ′[φ(f )/f ]| {x 7→V ′ }⊢M⇒W
φ |ρ ⊢f (V )⇒W
(where φ(f ) = cloφ′ (f (x : T ) : U . M))

φ |ρ ⊢W .rdV (x :T . N ){C

φ |ρ ⊢C ⇒X

φ |ρ ⊢W .rdV (x :T . N )⇒X

φ |ρ ⊢V ⇒V ′
φ |ρ ⊢pred(V )⇒Wbool

(where bev(pred, V ′) ≃ Wbool)

Fig. 2. Ordinary operational semantics: values and redexes

The rules for ordinary evaluation are given in Figures 2 and 3; those for symbolic evaluation are
given in Figures 4 and 5. The deﬁnitions are mutually recursive. They make use of the symbolic
diﬀerentiation of trace terms: given a trace term C, and values V and W (not necessarily closed),
we deﬁne a trace term

W .RV (x :T . C)
intended to denote the reverse-mode derivative of the function x :T 7→ C, at V , evaluated at W . A
deﬁnition is given in Figure 6; in the deﬁnition we assume that x < FV(V ,W ), and, as is common,
that all binding variables are diﬀerent.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

12

Martín Abadi and Gordon D. Plotkin

φ |ρ ⊢R⇒V

φ |ρ [V /x ]⊢E[x ]⇒W

φ |ρ ⊢E[R]⇒W

(E nontrivial, x < Dom(ρ ))

φ |ρ ⊢Rbool⇒Vbool

φ |ρ ⊢E[Vbool]⇒W

φ |ρ ⊢E[Rbool ]⇒W

φ |ρ ⊢R⇒V

φ |ρ [V /x ]⊢Ebool[x ]⇒Wbool

φ |ρ ⊢Ebool[R]⇒Wbool

(x < Dom(ρ ))

φ |ρ ⊢Rbool⇒Vbool

φ |ρ ⊢Ebool[Vbool]⇒Wbool

φ |ρ ⊢Ebool[Rbool]⇒Wbool

Fig. 3. Ordinary operational semantics: contexts

φ | ρ ⊢ V { V

φ | ρ ⊢ V + W { V + W

φ | ρ ⊢ op(V ) { op(V )

φ |ρ ⊢M {C
φ |ρ ⊢if true then M else N {C

φ |ρ ⊢N {C
φ |ρ ⊢if false then M else N {C

φ[cloφ (f (x :T ):U . M)/f ]|ρ ⊢N {C
φ |ρ ⊢ letrec f (x :T ):U = M in N {C

φ |ρ ⊢V ⇒V ′

φ |ρ [V ′/x ]⊢N {C

φ |ρ ⊢let x :T = V in N {let x :T = V in C

φ | ρ ⊢ fstT ,U (V ) { fstT ,U (V )

φ | ρ ⊢ sndT ,U (V ) { sndT ,U (V )

φ |ρ ⊢V ⇒V ′

φ′[φ(f )/f ]| {x 7→V ′ }⊢M {C

φ |ρ ⊢f (V ){let x:T = V in C
(where φ(f ) = cloφ′ (f (x : T ) : U . M))

φ |ρ ⊢V ⇒V ′

φ |ρ [V ′/x ]⊢N {C

φ |ρ ⊢W .rdV (x :T . N ){W . RV (x :T . C )

Fig. 4. Symbolic operational semantics: values and redexes

φ |ρ ⊢R{C

φ |ρ ⊢C ⇒V

φ |ρ [V /x ]⊢E[x ]{D

φ |ρ ⊢E[R]{let x :TV = C in D

(E nontrivial and x < Dom(ρ ))

φ |ρ ⊢Rbool⇒Vbool

φ |ρ ⊢E[Vbool]{C

φ |ρ ⊢E[Rbool]{C

Fig. 5. Symbolic operational semantics: contexts

Proposition 3.3. The following typing rule is admissible:

Γ[x :T ] ⊢ C :U Γ ⊢ V :T

Γ ⊢ W :U

Γ ⊢ W .RV (x :T . C) :T
In large part because of the restrictions on trace terms, their symbolic diﬀerentiation is just
a systematic, formal application of the chain rule. In our setting, this application requires a fair
amount of attention to detail, for instance the use of the type decorations when giving derivatives
of pairing and projection terms.

The reader may wish to try the following two evaluation examples with nested diﬀerentiation:
1.rd1(x : real. x × 1.rd1(y : real. x + y)) ⇒ 1

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

13

W .RV (x :T.y)

W .RV (x :T.r )

=

(cid:26)
= 0T

W (y = x)
(y , x)
0T
(r ∈ R)

W .RV (x :T.D +E)

= W .RV (x :T.D)+T W .RV (x :T.E)

W .RV (x :T.op(D))

=

let x :T = V in
let y :S = W .opr (D) in y.RV (x :T.D)

(y < FV(V ), op :S → U )

W .RV (x :T.let y :S = D in E) =

let x :T = V in
let y :S = D in
W .RV (x :T.E) +T
(let y :S = W .Ry (y :S.E) in y.RV (x :T.D))

(y < FV(W ), y, y < FV(V , D))

W .RV (x :T.∗)

= 0T

W .RV (x :T . hD, EiU , S )

W .RV (x :T.fstU , S (D))

W .RV (x :T.sndU , S (D))

=

=

=

let y :U , z :S = W in
y.RV (x :T . D) +T z.RV (x :T . E)
(y, z < FV(V , D, E))

let x :T = V in
let y :U × S = D in hW , 0S i.RV (x :T.D)

(y < FV(V ,W , D))

let x :T = V in
let y :U × S = D in h0U ,W i.RV (x :T.D)

(y < FV(V ,W , D))

Fig. 6. Definition of W .RV (x :T . C)

and

letrec f (x : real) : real = 1.rd1(y : real. x + y) in 1.rd1(x : real.x + f (x)) ⇒ 1
Examples of this kind can be used to illustrate perturbation confusion in forward diﬀerentiation,
e.g., [Siskind and Pearlmutter 2005, 2008].

We need some basic results on our evaluation relations. Two are standard: determinacy and
type safety, and are used implicitly throughout the rest of the paper. The third connects symbolic
and ordinary evaluation: one can interpolate symbolic evaluation within ordinary evaluation. It is
principally helpful to reduce the completeness part of symbolic evaluation to the completeness of
ordinary evaluation (see Theorem 6.7).

Proposition 3.4 (Determinacy of evaluation). The following hold:
(1) For any φ, ρ, and M, there is at most one value V s.t. φ | ρ ⊢ M ⇒ V .
(2) For any φ, ρ, and M, there is at most one trace term C s.t. φ | ρ ⊢ M { C.

The following interpolation proposition establishes a certain consistency between the ordinary

and symbolic evaluation relations.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

14

Martín Abadi and Gordon D. Plotkin

Proposition 3.5 (Operational interpolation). For all φ, ρ, and closed values V , the following

are equivalent:

(1) φ | ρ ⊢ M ⇒ V ,
(2) φ | ρ ⊢ M { C and φ | ρ ⊢ C ⇒ V , for some C.
For a type safety theorem, we need typing judgments ρ : Γ, φ : Φ, and Cl :T → U for environments,
function environments, and closures (implicitly extending the notion of type). These are deﬁned
inductively by the following rules:

⊢ Vi :Ti

(i = 0, n − 1)

⊢ {x0 7→ V0, . . . , xn−1 7→ Vn−1} : x0 : T0, . . . , xn−1 : Tn−1

⊢ Cli :Ti → Ui

(i = 0, n − 1)

⊢ { f0 7→ Cl0, . . . , fn−1 7→ Cln−1} : f0 : T0 →U0, . . . , fn−1 : Tn−1 →Un−1

⊢ φ : Φ Φ[f :T → U ] | x : T ⊢ M :U
⊢ cloφ (f (x : T ) : U . M) : T → U
Note that a closure cloφ (f (x : T ) : U . M) can only have type T → U . So in the third rule Φ is
determined up to the ordering of its function type declarations. Whether the conclusion of the
rule follows does not depend on the choice of this ordering. We write Φφ for the choice of Φ with
declarations ordered using the standard function variable ordering.

Proposition 3.6 (Type safety). Suppose Φ | Γ ⊢ M :T , ⊢ φ : Φ and ⊢ ρ : Γ. Then we have:

and

φ | ρ ⊢ M ⇒ V =⇒ ⊢ V :T

φ | ρ ⊢ M { C =⇒ Γ ⊢ C :T

4 MATHEMATICAL PRELIMINARIES

We now turn to the mathematical facts needed for the denotational semantics of our language.
These concern the two modes of diﬀerentiation and their interaction with domain theory. We
follow [Abramsky and Jung 1994] for domain theory, but write dcppo for pointed dcpo, and say a
partial order is coherent iﬀ every compatible subset has a lub. (A subset is compatible if any two of
its elements have an upper bound.) Every coherent partial order is a dcppo.

The collection of partial functions f :X ⇀ Y with open domain between two topological spaces

forms a partial order under graph inclusion:

equivalently, using the Kleene order 4:

f ≤ д ⇐⇒ f ⊆ д

f ≤ д ⇐⇒ ∀x ∈ X . f x (cid:22) дx
This partial order is a coherent dcppo with ⊥ the everywhere undeﬁned function and compatible
sups given by unions. A partial function f : X ⇀ Y is continuous if f −1(B) is open whenever B
is; the subcollection of continuous partial functions forms a coherent subdcppo. This holds as, for
any open set B ⊆ Y and compatible collection of partial functions fi (i ∈ I ) :X ⇀ Y , we have:
fi )−1(B) =

(B)

(

f −1
i

Üi ∈I
We write C[X , Y ] for the dcppo of partial continuous functions from X to Y .

Øi ∈I

4We write e (cid:22) e′ for two mathematical expressions e and e′ to mean that if e is deﬁned so is e′, and they are then equal.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

15

It is convenient to use a variation on cartesian product when working with powers of R. We set:
Rm+n

Rm Û× Rn =

(m, n ≥ 0)

def

This version of product is associative. Vector concatenation then serves as tupling; however, for
clarity, we may use the usual notation (x0, . . . , xk−1) instead of x0 . . . xk−1. There are evident deﬁ-
nitions of the projections π m0, ...,mk −1

: Rm0 Û× . . . Û× Rmk −1 → Rmi , and of the tupling

i

hf0, . . . , fk−1i : Rn ⇀ Rm0 Û× . . . Û× Rmk −1
of fi : Rn ⇀ Rmi . We may ignore the superscripts on the projections when they can be understood
from the context.

4.1 Continuity, Diﬀerentiability, and Smoothness
Standard multivariate analysis of vector-valued real functions from Rn to Rm (with n, m > 0)
considers functions f deﬁned on an open domain of Rn, see, e.g., [Trench 2003]. These are precisely
the partial functions:

with open domain.

When m = 1, such a function f has partial derivatives

f : Rn ⇀ Rm (n, m > 0)

where

∂j (f ) : Rn ⇀ R

(j = 0, n − 1)

∂j (f )(x0, . . . , xn−1) ≃def

∂ f
∂xj

viewing f as a function of x0, . . . , xn−1. Taken together, these partial derivatives form its gradient
∇(f ) : Rn ⇀ Rn

where

We write ∇x(f ) for ∇(f )(x).

∇(f )(x) ≃ h∂0(f )(x), . . . , ∂n−1(f )(x)i

(1)

We say f is continuously diﬀerentiable if all the ∂j (f ) are continuous with domain that of f ,
equivalently if ∇(f ) is continuous with domain that of f . As an example, removing 0 from the
domain of deﬁnition of the non-diﬀerentiable ReLU function f (x) = max(x, 0), we obtain a contin-
uously diﬀerentiable partial function with domain R\0; its derivative also has domain R\0, with
value 0, if x < 0, and 1, if x > 0.

We now turn to the general case where f : Rn ⇀ Rm. The Jacobian Jx(f ) of f at x ∈ Rn is the

m by n matrix:

Jx(f )i, j ≃ ∂j (πi ◦ f )(x)
(2)
where the matrix is undeﬁned if any of the ∂j (πi ◦ f )(x) are. These Jacobians form a partial function:
J(f ) : Rn ⇀ Mat(m, n)
where Mat(m, n) is the collection of m by n matrices. Viewing Mat(m, n) as Rm×n, we say f is
continuously diﬀerentiable if J(f ) is continuous and has the same domain as f (equivalently if each
component πi ◦ f of f is continuously diﬀerentiable).

The diﬀerential 5

d(f ) : Rn Û× Rn ⇀ Rm

5Diﬀerentials are discussed in [Trench 2003] see: p325 for diﬀerentials of functions of several variables; p348 for higher-
order diﬀerentials; p381 for diﬀerentials of vector-valued functions of several variables; and p388 for the chain rule in
diﬀerential terms.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

16

of f is deﬁned by:

Martín Abadi and Gordon D. Plotkin

d(f )(x, y) ≃ Jx(f ) · y
and f is continuously diﬀerentiable iﬀ d(f ) has domain Dom(f ) × Rn and is continuous there.

(3)

We write dx(f ) for the partial function d(f )(x, −); it is either ⊥ or everywhere deﬁned and linear,

the latter occurring precisely when Jx(f ) is deﬁned. If f is linear then dx f = f , for all x ∈ Rn.

In the automatic diﬀerentiation literature, dx(f ) is called the forward-mode derivative of f at x.

For the reverse-mode derivative we deﬁne:

dr (f ) : Rn Û× Rm ⇀ Rn

by:

(4)
x(f ) for the partial function dr (f )(x, −); f is continuously diﬀerentiable iﬀ dr (f ) has

dr (f )(x, y) ≃ Jx(f )t · y

and write dr
domain Dom(f ) × Rm and is continuous there.

In terms of the diﬀerentials, the two modes are related by:

x(f ) = dx(f )†
dr
x(f ) = dx(f )† = f †.
The semantic content of the deﬁnition of forward-mode from reverse-mode given in Section 2

(setting ⊥†=⊥). So, if f is linear, then dr

(5)

is the following equality:

0 (dR

This holds as: dR

(6)
x f is linear if ,⊥).
The continuously diﬀerentiable functions are closed under composition. If h : Rn ⇀ Rl is the
composition of two such functions f : Rn ⇀ Rm and д : Rm ⇀ Rl , then the chain rule expresses
the derivative of h in terms of those of f and д. In terms of Jacobians, the chain rule is:

x f )
x f )† = ((dx f )†)† = dx f (the ﬁrst equality holds as dR

x f ) = (dR

dx f = dR

0 (dR

Jx(h) ≃ Jf (x)(д) · Jx(f )

(x ∈ Dom(h))

(Note that x ∈ Dom(h) iﬀ x ∈ Dom(f ) and f (x) ∈ Dom(д).) In terms of forward-mode derivatives
the chain rule is:

dx(h) = df (x)(д) ◦ dx(f )

(x ∈ Dom(h))

(7)

and in terms of reverse-mode derivatives it is:

f (x)(д)
Derivatives with respect to two variables can be reduced to derivatives in each separately. Specif-

(x ∈ Dom(h))

(8)

x(h) = dr
dr

x(f ) ◦ dr

ically, suppose f : Rn+n′ ⇀ Rm. Then, for x ∈ Rn and y ∈ Rn′

, we have:

dhx, yi(f )(u, v) ≃ dx(f (−, y))(u) + dy(f (x, −))(v)

(u ∈ Rn, v ∈ Rn′

)

and

hx, yi(f ) = hdR
dR

x (f (−, y)), dR

y (f (x, −))i

(9)

(10)

These equations are useful for dealing with fan-in.

Our programming language has all ﬁnite product types of the reals. We will therefore need to
work with partial functions with open domain f : Rn ⇀ Rm where n or m is zero. To this end we
regard R0 as having as sole element the empty vector, the trivial topology, and the trivial vector
space structure. Every such total function is linear, and this determines its adjoint.

We take such a function f : Rn ⇀ Rm, where n or m is zero, to be continuously diﬀerentiable if

it is continuous, and we deﬁne d(f ) : Rn Û× Rn ⇀ Rm and dr (f ) : Rn Û× Rm ⇀ Rn by:

d(f )(x, y) ≃

(f (x) ↓)
0
↑ (otherwise)

(cid:26)

dr (f )(x, y) ≃

(f (x) ↓)
0
↑ (otherwise)

(cid:26)

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

17

The derivative d(f ) has domain Dom(f )×Rn and is continuous (and so continuously diﬀerentiable)
iﬀ f is, and a similar remark applies to dr (f ). We understand dx(f ) and dr
x(f ) similarly to before.
In case f is linear (i.e., total) we have dx(f ) = f and dr
x(f ) = f † as before and equation (5) relating
the two modes continues to hold, as does equation (6) and also equations (10) and (9) concerning
derivatives with respect to two variables. In particular for t : Rn → R0 we have:

Regarding compositions we note a useful fact:

(dxt)x′ = ∗

(dr

xt)∗ = 0

Fact 1. If h : Rn ⇀ Rl (l, n ≥ 0) is constant on its domain, then it is continuously diﬀerentiable iﬀ

it is continuous and then, for any x in its domain, dx(h) is the constantly 0 function, as is dr

x(h).

It follows that the continuously diﬀerentiable functions, if taken in our wider sense, remain
closed under composition and pointwise addition, and that the chain rule for forward derivatives
continues to hold, as does that for reverse derivatives. From now on whenever we consider partial
functions from an Rn to an Rm, we include the cases where n or m is 0.

The projections π m0, ...,mk −1

i

: Rm0 Û× . . . Û× Rmk −1 → Rmi are total linear functions, so we have:

i

dxπ m0, ...,mk −1
xπ m0, ...,mk −1
(dr

i

= π m0, ...,mk −1

i

)y = (0, . . . , 0, y, 0, . . . , 0)

Regarding the tupling hf0, . . . , fk−1i : Rn ⇀ Rm0 Û× . . . Û× Rmk −1 of fi : Rn ⇀ Rmi we have:

dxhf0, . . . , fk−1i
(dr

xhf0, . . . , fk−1i)(y0, . . . , yk−1) ≃ (dr

= hdx f0, . . . , dx fk−1i
x f0)y0 + . . . + (dr

x fk−1)yk−1

For the semantics of our language we work with inﬁnitely diﬀerentiable functions, i.e., smooth
ones. First we deﬁne smoothness classes Ck . We say that a partial function f : Rn ⇀ Rm is C0 if
it is continuous, and, inductively, is Ck+1 if d(f ) has domain Dom(f ) × Rn and is Ck . This deﬁnes
a decreasing sequence of classes of functions, and we say that f is smooth or C∞ if it is Ck for
all k. The C1 functions are precisely the continuously diﬀerentiable ones. Using the chain rule
for diﬀerentials one shows that the Ck functions, and so too the smooth ones, are closed under
composition. The projections are smooth, as are all linear functions and the Ck functions, and so
too the smooth ones, are closed under tupling.

4.2 Cppos of Diﬀerentiable Functions
The subgraph partial order on partial functions between powers of R interacts well with the dif-
ferential structure:

Proposition 4.1.
(1) For any f ≤ д : Rn ⇀ Rm with open domain we have:
df = (dд) ↾ (Dom(f ) × Rn)

dr f = (drд) ↾ (Dom(f ) × Rm)

(2) For any compatible family of functions with open domain fi : Rn ⇀ Rm (i ∈ I ), we have:

d

fi =

dfi

Üi ∈I

Üi ∈I

dr

fi =

dr fi

Üi ∈I

Üi ∈I

Proof. For the ﬁrst part, if m or n is 0, the conclusion is immediate using Fact 1. Otherwise, as

f and д agree on an open set including x we have:

Jx(f ) ≃ Jx(д)

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

18

Martín Abadi and Gordon D. Plotkin

and the conclusion follows. For the second part, set f =
using the ﬁrst part we calculate:

Ô

fi . For the forward-mode derivative,

df = df ↾ (Dom(f ) × Rn)

= df ↾

i ∈I (Dom(fi ) × Rn)

=

i ∈I df ↾ (Dom(fi ) × Rn) =

i ∈I dfi
Ð

Ô
The proof for the reverse-mode derivative is similar.

Ô

(cid:3)

Proposition 4.2.
(1) Let f ≤ д : Rn ⇀ Rm be partial functions with open domain. Then f is smooth if д is.
(2) Let fi : Rn ⇀ Rm be a compatible family of partial functions with open domain, and with sup

f . Then f is smooth if all the fi are.

Proof. For the ﬁrst part, we prove by induction that if д is Ck then so is f . For k = 0 we note that
for any open V ⊆ Rm, f −1(V ) = д−1(V ) ∩ Dom(f ). For k + 1, as д is Ck+1, Dom(dд) = Dom(д) × Rn
and dд is Ck . From part (1) of Proposition 4.1 we have df ≤ dд and Dom(df ) = Dom(f ) × Rn. So
df and dд have open domain, df ≤ dд, and dд is Ck . It follows from the induction hypothesis that
df is Ck . So f is Ck+1, as required.

For the second part we prove, by induction on k that if all the fi are Ck , then so is f . For k = 0
this is clear. For k + 1, we have, for all i, that Dom(dfi ) = Dom(fi ) × Rn and fi is Ck . From part (2)
of Proposition 4.1 we have df =

i dfi . So, ﬁrst,

Ô
Dom(df ) = Dom(
=

i dfi )

=
i Dom(dfi )
i Dom(fi ) × Rn = Dom(f ) × Rn

Ð

Ô

and second, also using the induction hypothesis, we have that df is Ck . So f is Ck+1, as required.
(cid:3)

Ð

We write S[Rn, Rm] for the coherent dcppo of smooth partial functions between Rn and Rm.

4.3 Conditionals and Recursion
Diﬀerentiation and conditionals interact well. The conditional combinator Condn,m is deﬁned for
p :Rn ⇀ T and f , д :Rn ⇀ Rm by:

Condn,m(p, f , д)(x) ≃


where T = {tt, ff }. The conditional combinator is continuous. For diﬀerentiability, with T a discrete


topological space, we have:

f (x)
д(x)
↑

(p(x) = tt)
(p(x) = ff )
(otherwise)

Proposition 4.3. Suppose p is continuous (equivalently: both p−1(tt) and p−1(ff ) are open). Then:
d(Condn,m(p, f , д)) = Cond(n+n),m(p ◦ π1, df , dд)

and

dr (Condn,m(p, f , д)) = Cond(n+m),n(p ◦ π1, dr f , drд)

Further, if f , д are smooth so is Condn,m(p, f , д).
Proof. Assume that p is continuous. Set h = Condn,m(p, f , д). The domain of h is open, indeed

Dom(h) = (p−1(tt) ∩ Dom(f )) ∪ (p−1(ff ) ∩ Dom(д)) so dh is deﬁned.

To prove the equality, choose x ∈ Rn. There are three cases. First, if p(x) ↑ then h(x) ↑ and so
(dh)(x) ↑; the equality therefore holds at x. Second if p(x) = tt then, as h ↾ p−1(tt) = f ↾ p−1(tt) and

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

19

p−1(tt) is open, we see that, by part (1) of Proposition 4.1, dxh = dxh ↾ p−1(tt) = dx f ↾ p−1(tt) = dx f ,
and so the equality again holds at x. The third case is similar to the second.

Suppose further that f , д are smooth. We have h ↾ p−1(tt) ≤ f , and so, by part (1) of Proposi-
tion 4.2, h ↾ p−1(tt) is smooth as f is and h ↾ p−1(tt) has open domain. Similarly h ↾ p−1(ff ) is
smooth. But then, by part (2) of Proposition 4.2, h is smooth as h = h ↾ p−1(true) ∨h ↾ p−1(ff ). (cid:3)

In less formal terms than the proof, equality holds because if, say, the condition p holds at
x ∈ Rn, it holds in a neighborhood O of x. So f and the conditional are equal throughout O, and
therefore have the same derivative there. The equality justiﬁes the approaches to the diﬀerentiation
of conditionals described in the Introduction.

Diﬀerentiation and recursion also interact well. For any continuous f : P × Q → Q (P a dcpo,
Q a dcppo) we write µy : Q.f (x, y) for the least ﬁxed-point (l.f.p.) of f (x, −). It is the sup of the
iterates µny :Q.f (x, y), deﬁned inductively by:

µn+1y :Q.f (x, y) = f (x, µny :Q.f (x, y))
starting from ⊥Q . As functions of P, the l.f.p. and the iterates are continuous. When f : Q → Q,
we write µy : Q.f (y), etc.

Proposition 4.4.
(1) Set Q = S[Rm, Rl ] and R = S[Rm Û× Rm, Rl ]. Then if F : Rn × Q → Q and G : Rn × Q × R → R

are such that

we have:

dF (x, f ) = G(x, f , df )

(x ∈ Rn, f ∈ Q)

d(µ f :Q.F (x, f )) = µ f ′:R. G(x, µ f :Q.F (x, f ), f ′)

(2) Set Q = S[Rm, Rl ] and R = S[Rm Û× Rl , Rm]. Then if F : Rn × Q → Q and

G : Rn × Q × R → R are such that

dr F (x, f ) = G(x, f , dr f )

(x ∈ Rn, f ∈ Q)

we have:

dr (µ f :Q.F (x, f )) = µ f ′:R. G(x, µ f :Q.F (x, f ), f ′)

Proof. We only consider the forward-mode case as the reverse-mode case is similar. In one

direction we prove by induction on n that

This is evident for n = 0. For n + 1 we calculate (missing out types):

d(µn f :Q.F (x, f )) ≤ µ f ′:R. G(x, µ f :Q.F (x, f ), f ′)

d(µn+1 f .F (x, f )) = d(F (x, µn f .F (x, f )))

= G(x, µn f .F (x, f ), dµn f .F (x, f ))
≤ G(x, µ f .F (x, f ), µ f ′. G(x, µ f .F (x, f ), f ′))
= µ f ′. G(x, µ f .F (x, f ), f ′)

In the other direction we prove by induction on n that

µn f ′:R. G(x, µ f :Q.F (x, f ), f ′) ≤ d(µ f :Q.F (x, f ))

This is evident for n = 0. For n + 1 we calculate:

µn+1 f ′. G(x, µ f .F (x, f ), f ′) = G(x, µ f .F (x, f ), µn f ′. G(x, µ f .F (x, f ), f ′))

≤ G(x, µ f .F (x, f ), dµ f .F (x, f ))
= dF (x, µ f .F (x, f ))
= d(µ f .F (x, f ))

The conclusion then follows using the continuity of d (part (2) of Proposition 4.1).

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

20

Martín Abadi and Gordon D. Plotkin

Although we do not do so here, this proposition can be used to justify code transformations of

recursive function deﬁnitions.

5 DENOTATIONAL SEMANTICS
We begin with a denotational semantics of types as powers of the reals:

(cid:3)

= R
[[real]]
= R0
[[unit]]
[[T × U ]] = [[T ]] Û× [[U ]]

Note that [[T ]] = R|T |, where, |real| = 1, |unit| = 0 and, recursively, |T ×U | = |T | + |U |. Then, for
the semantics of environments Γ = x0 :T0, . . . , xn−1 :Tn−1 we set
[[Γ]] = [[T0]] Û× . . . Û× [[Tn−1]]

and for that of function environments Φ = f0 :T0 →Un, . . . , fn−1 :Tn−1 →Un−1 we set

[[Φ]] = S[[[T0]], [[U0]]] × . . . × S[[[Tn−1]], [[Un−1]]]
We use γ , δ to range over [[Γ]] and ϕ over [[Φ]]. The vectors γ ∈ [[Γ]] correspond to semantic
environment functions ρ on Dom(Γ) with ρ(xi ) = πi (γ ), and this correspondence is 1-1. We take
advantage of it to write γ [a/x], when a ∈ [[T ]], for an element of [[Γ[x : T ]]] (assuming Γ and T
available from the context). We understand ϕ[α/f ] similarly, for ϕ ∈ [[Φ]] and α ∈ S[[[T ]], [[U ]]].

The denotational semantics of a term Φ | Γ ⊢ M :T will be a continuous function:

and that of a boolean term Φ | Γ ⊢ B will be continuous functions:

[[Φ]]

[[Φ|Γ⊢M:T ]]
−−−−−−−−→ S[[[Γ]], [[T ]]]

[[Φ]]

[[Φ|Γ⊢B]]
−−−−−−→ C[[[Γ]], T]

When the environments and types are understood from the context, we may just write [[M]] or
[[B]].

For the semantics of operation and predicate symbols, for every op :T → U we assume available
a smooth function [[op]] : [[T ]] ⇀ [[U ]] and for every pred :T we assume available a a continuous
function [[pred]] : [[T ]] ⇀ T, such that, for every closed value V :T :

and, for every closed value V :T :

[[op]]([[V ]]) ≃ [[ev(op, V )]]

[[pred]]([[V ]]) ≃ [[bev(pred, V )]]

(11)

(12)

The denotational semantics is given in Figure 7. Note that it uses the no-free-variable assump-
tion in the clause for recursive functions. Apart from the semantics of reverse diﬀerentiation,
which uses the reverse-mode derivative dr , it is quite standard. However, the facts that the deno-
tations of terms carry smooth functions to smooth functions, and that the denotations of boolean
terms carry smooth functions to continuous ones, use the mathematics developed in the previous
section, particularly: the chain rule, e.g., for function application and let constructs; the preserva-
tion of smooth functions by the conditional combinator; the remarks on products; and, for recur-
sive function deﬁnitions, the fact that the lub of an increasing sequence of smooth functions is
smooth.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

21

[[x]]ϕ γ

[[r ]]ϕ γ

[[M + N ]]ϕ γ

[[op(M)]]ϕ γ

≃ γ (x)

≃ r

(r ∈ R)

≃ [[M]]ϕ γ + [[N ]]ϕ γ

≃ [[op]]([[M]]ϕ γ )

[[let x :T = M inN ]]ϕ γ

≃ [[N ]]ϕ (γ [[[M]]ϕ γ /x])

[[∗]]ϕ γ

[[hM, N iT ,U ]]ϕ γ

[[fstT ,U (M)]]ϕ γ

[[sndT ,U (M)]]ϕ γ

≃ ∗

≃ h[[M]]ϕ γ , [[N ]]ϕ γ i

≃ [[π0]]([[M]]ϕ γ )

≃ [[π1]]([[M]]ϕ γ )

[[ifB thenM elseN ]]ϕ γ

≃

[[letrec f (x :T ) :U = M in N ]]ϕ γ ≃

[[M]]ϕ γ
[[N ]]ϕ γ
↑

([[B]]ϕ γ ≃ tt)
([[B]]ϕ γ ≃ ff )

(otherwise)
[[N ]](ϕ[µα : S[[[T ]],[[U ]]].



λa : [[T ]]. [[M]](ϕ[α/f ])a/f ])γ

[[f (M)]]ϕ γ

≃ ϕ(f )([[M]]ϕ γ )

[[M.rdL(x :T . N )]]ϕ γ

≃ dr

[[L]]ϕ γ

(a ∈ [[T ]] 7→ [[N ]]ϕ (γ [a/x]))([[M]]ϕ γ )

[[true]]ϕ γ

[[false]]ϕ γ

≃ tt

≃ ff

[[pred(M)]]ϕ γ

≃ [[pred]]([[M]]ϕ γ )

Fig. 7. Denotational semantics

If a term M contains no function variables (or variables), [[M]]ϕ γ is independent of ϕ (and γ ),
and we write [[M]]γ (resp., [[M]]) for it. Trace terms C have no function variables, and closed values
V have no function variables or variables.

Using the semantics of terms, we can deﬁne the denotational semantics of value environments,

function environments, and closures, the latter two by a mutual structural induction.
• For every ρ : Γ, where Γ = x0 :T0, . . . , xn−1 :Tn−1, we deﬁne [[ρ : Γ]] ∈ [[Γ]] by:

[[ρ : Γ]] = ([[ρ(x0)]], . . . , [[ρ(xn−1)]])
• For every φ : Φ, where Φ = f0 :T0 →Un, . . . , fn−1 :Tn−1 →Un−1, we deﬁne [[φ : Φ]] ∈ [[Φ]] by:
[[φ : Φ]] = ([[φ(f0)]], . . . , [[φ(fn−1)]])

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

22

Martín Abadi and Gordon D. Plotkin

• For every Cl = cloφ (f (x :T ) :U . M) :T →U we deﬁne [[Cl]] ∈ S[[[T ]], [[U ]]] by:

[[Cl]] = µα : S[[[T ]], [[U ]]]. λa : [[T ]].[[M]]([[φ : Φφ]][α/f ])a

We omit Γ and Φ from [[ρ : Γ]] and [[φ : Φ]] when they can be understood from the context.

Lemma 5.1. For any type T we have:

(1) The denotation [[V ]] of any value V : T exists.
(2) For any v ∈ [[T ]] there is a unique value V : T such that v = [[V ]].

The following two fairly standard results about evaluation contexts are needed to show ade-

quacy.

Lemma 5.2 (Evaluation context compositionality). Suppose Φ | Γ ⊢ M : U , Φ | Γ ⊢ E[M] : T ,

and Φ | Γ ⊢ N :U . Then:

[[M]]ϕ γ ≃ [[N ]]ϕ γ =⇒ [[E[M]]]ϕ γ ≃ [[E[N ]]]ϕ γ

(ϕ ∈ [[Φ]], γ ∈ [[Γ]])

Analogous results hold for: boolean terms in evaluation contexts, E[B]; terms in boolean evaluation

contexts Ebool[M]; and boolean terms in boolean evaluation contexts Ebool[B].

Lemma 5.3 (Evaluation context strictness).
(1) Suppose that Φ | Γ ⊢ E[M] :T and Φ | Γ ⊢ M :U . Then, for any x < FV(E), we have:

[[E[M]]]ϕ γ ↓ =⇒ [[M]]ϕ γ ↓

(ϕ ∈ [[Φ]], γ ∈ [[Γ]])

and so:

[[E[M]]]ϕ γ ≃ [[E[x]]]ϕ (γ [[[M]]ϕ γ /x])

(ϕ ∈ [[Φ]], γ ∈ [[Γ]])

The analogous result holds for terms in boolean contexts Ebool[M].

(2) Suppose that Φ | Γ ⊢ E[B] :T and Φ | Γ ⊢ B. Then:

[[E[B]]]ϕ γ ↓ =⇒ [[B]]ϕ γ ↓

(ϕ ∈ [[Φ]], γ ∈ [[Γ]])

The analogous result holds for boolean terms in boolean contexts Ebool[B].

6 ADEQUACY
We present our main results, on the correspondence between operational and denotational seman-
tics. Taken together these are our adequacy theorems. As well as the usual correctness and com-
pleteness theorems, there are results peculiar to diﬀerentiation, both of interest in themselves and
also necessary for the others. Theorem 6.1 shows the correctness of our source code transforma-
tion of trace terms, in that formal diﬀerentiation corresponds to actual diﬀerentiation. Theorem 6.2
has two parts. The ﬁrst is the usual statement of correctness for the ordinary evaluation relation.
The second is a statement of correctness of symbolic evaluation. This states that the trace term
resulting from symbolic evaluation of a term has the same denotation not only at the environment
used for the symbolic evaluation but on a whole open set including it. This, and Theorem 6.1, are
needed to prove the ordinary correctness of the ordinary evaluation relation in the case of dif-
ferentiation as in that case ordinary evaluation proceeds by ﬁrst symbolically diﬀerentiating and
then applying our source code transformation. Finally Theorem 6.7 is the expected completeness
theorem, that if the semantics of a term is deﬁned, then both its ordinary and symbolic evaluation
terminate. Its proof makes use of Proposition 3.5 which interpolates a symbolic evaluation inside
any ordinary (non-boolean) evaluation.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

23

6.1 Operational Correctness

Theorem 6.1 (Reverse-mode differentiation). Suppose that Γ[x : T ] ⊢ C : U , Γ ⊢ V : T and

Γ ⊢ W :U (and so Γ ⊢ W .rdV (x :T . C) :T ). Then, for any γ ∈ [[Γ]], we have:

[[W .RV (x :T . C)]]γ ≃ [[W .rdV (x :T . C)]]γ

Proof. The proof is by structural induction on C. We give a representative case. Suppose C
is let y : U = D in E. Set γa = γ [a/x] ∈ [[Γ[x : T ]]], for any a ∈ [[T ]], γV = γ[[V ]]γ , and
γ ′ = γV [[[D]]γV /y] ∈ [[Γ[x :T ][y :U ]]]. Then

[[W .RV (x :T.C)]]γ ≃ [[W .RV (x :T.E) +T (let y :S = W .Ry (y :S.E) in y.RV (x :T.D))]]γ ′

with x < FV(V ,W ), y < FV(W ), and y, y < FV(V , D). We calculate:

(a ∈ [[T ]] 7→ [[let y :U = D in E]]γa)[[W ]]γ
dr
[[V ]]γ
dr
(a ∈ [[T ]] 7→ [[E]]γa[[[D]]γa/y])[[W ]]γ
≃
[[V ]]γ
dr
((c ∈ [[T × U ]] 7→ [[E]]γπ0(c)[π1(c)/y]) ◦ ha ∈ [[T ]] 7→ a, a ∈ [[T ]] 7→ [[D]]γai)[[W ]]γ
≃
[[V ]]γ
dr
(ha ∈ [[T ]] 7→ a, a ∈ [[T ]] 7→ [[D]]γai)[
≃
[[V ]]γ

(dr

h[[V ]]γ , [[D]]γV i(c ∈ [[T × U ]] 7→ [[E]]γπ0(c)[π1(c)/y]))[[W ]]γ ]

dr
[[V ]]γ

(ha ∈ [[T ]] 7→ a, a ∈ [[T ]] 7→ [[D]]γai)[

hdr

[[V ]]γ

(a ∈ [[T ]] 7→ [[E]]γa[[[D]]γV /y]), dr

[[D]]γV

(ha ∈ [[T ]] 7→ a, a ∈ [[T ]] 7→ [[D]]γai)[

(a ∈ [[T ]] 7→ [[E]]γa[[[D]]γV /y])[[W ]]γ , dr

[[D]]γV

dr
[[V ]]γ
hdr
[[V ]]γ ′(a ∈ [[T ]] 7→ a)[dr
dr

[[V ]]γ

[[V ]]γ

+ dr

[[V ]]γ ′(a ∈ [[T ]] 7→ [[D]]γ ′)[dr
dr
[[V ]]γ (a ∈ [[T ]] 7→ [[E]]γa[[[D]]γV /y])[[W ]]γ
[[V ]]γ ′(a ∈ [[T ]] 7→ [[D]]γ ′)[dr

+ dr

[[D]]γV

[[D]]γV

≃

≃

≃

≃

≃

[[W .RV (x :T.E) +T (let y :S = W .Ry (y :S.E) in y.RV (x :T.D))]]γ ′

(b ∈ [[U ]] 7→ [[E]]γ[[V ]]γ [b/y])[[W ]]γ ]

(a ∈ [[T ]] 7→ [[E]]γa[[[D]]γV /y])[[W ]]γ ]

(b ∈ [[U ]] 7→ [[E]]γ[[V ]]γ [b/y])[[W ]]γ ]

(b ∈ [[U ]] 7→ [[E]]γ[[V ]]γ [b/y])i[[W ]]γ ]

(b ∈ [[U ]] 7→ [[E]]γ[[V ]]γ [b/y])[[W ]]γ i]

Note the use of Equation (10) in the the fourth step.

(cid:3)

Theorem 6.2 (Operational correctness). Suppose that Φ | Γ ⊢ M:T , ⊢ φ : Φ, and ⊢ ρ : Γ. Then:
(1) Operational semantics.

φ | ρ ⊢ M ⇒ V =⇒ [[M]][[φ]][[ρ]] = [[V ]]

(and similarly for boolean terms).
(2) Symbolic operational semantics.

φ | ρ ⊢ M { C =⇒ ∃ O ⊆open [[Γ]]. [[ρ]] ∈ O ∧ ∀γ ∈ O. [[M]][[φ]]γ ≃ [[C]]γ

Proof. The two parts are proved by mutual induction on the size of the proofs that establish
the given operational relations, and by cases on the form of M. As an example case of the second
part, suppose M has the form let x : T = V in L. Then for some D and V ′ we have a smaller
proof of φ | ρ[V ′/x] ⊢ L { D, where V ′ = ρ(V ), and C has the form let x : T = V in D.

By the induction hypothesis there is an open set O such that [[ρ[V ′/x]]] ∈ O and, for all δ ∈ O,
def θ −1(O) is

we have [[L]][[φ]]δ ≃ [[D]]δ . Set θ = γ ∈ [[Γ]] 7→ γ [[[V ]]γ /x]. As θ is continuous, O ′ =
open. We show it is the required open set.

- First, [[ρ]] ∈ O ′ as we have: θ ([[ρ]]) = [[ρ]][[[V ]][[ρ]]/x] = [[ρ]][[[V ′]]/x] = [[ρ[V ′/x]]] ∈ O.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

24

Martín Abadi and Gordon D. Plotkin

- Second, for any γ ∈ O ′ we have:

[[let x : T = V in L]][[φ]]γ ≃ [[L]][[φ]]γ [[[V ]]γ /x]

≃ [[D]]γ [[[V ]]γ /x]
≃ [[let x : T = V in D]]γ

(as θ (γ ) ∈ O)

(cid:3)

The following corollary shows that our strategy of ﬁrst symbolically reducing to produce a trace

term, then symbolically diﬀerentiating, is correct.

Corollary 6.3. Suppose that Φ | Γ[x :T ] ⊢ M :U , Γ ⊢ V :T , and Γ ⊢ W :U . Then, for any ⊢ φ : Φ

and ⊢ ρ : Γ we have:

φ | ρ[V ρ/x] ⊢ M { C =⇒ [[W .rdV (x : T . M)]][[φ]][[ρ]] ≃ [[W .rdV (x : T . C)]][[φ]][[ρ]]

6.2 Operational Completeness
We turn to proving operational completeness, that the evaluation, or symbolic evaluation, of a
term terminates when it should, i.e., when its denotation is deﬁned. For terms M write φ | ρ ⊢ M ⇓
when, for some V , φ | ρ ⊢ M ⇒ V and, assuming φ and ρ known from the context, say that M
terminates; we adopt similar terminology for boolean terms.

To prove operational completeness we use a standard strategy: ﬁrst proving operational com-
pleteness for an auxiliary “approximation language” in which recursive deﬁnitions are replaced by
approximations to them, which we call limited recursive deﬁnitions, and then lifting that result to
the main language. Speciﬁcally we replace the syntactic form for recursive deﬁnitions of the main
language by the family of syntactic forms:

letrecn f (x : T ) : U = M in N

(n ∈ N)

with the evident typing rule, and make analogous consequential changes in the various deﬁnitions.

In the deﬁnition of function environments and closures, the clause for closures becomes:

- If FFV(M)\{ f } ⊆ Dom(φ), FV(M) ⊆ {x }, and n ∈ N, then

hn, φ, f , x,T , U , Mi

is a closure, written as: clon,φ (f (x : T ) : U . M).

The limited recursive deﬁnition redexes are letrecn f (x :T ) :U = M in N ; their evaluation rules
are in Figure 8. There, and below, we write ⊢l for limited recursion language judgements.

φ[clon, φ (f (x :T ):U . M)/f ]|ρ ⊢l N ⇒V ′
φ |ρ ⊢l letrecn f (x :T ):U = M in N ⇒V ′

φ |ρ ⊢l V ⇒V ′ φ′[clon, φ′ (f (x :T ):U . M)/f ]| {x 7→V ′ }⊢l M⇒W
φ |ρ ⊢l f (V )⇒W

where φ(f ) = clon+1, φ′ (f (x : T ) : U . M)

φ[clon, φ (f (x :T ):U . M)/f ]|ρ ⊢l N {C
φ |ρ ⊢l letrecn f (x :T ):U = M in N {C

φ |ρ ⊢l V ⇒V ′ φ′[clon, φ′ (f (x :T ):U . M)/f ]| {x 7→V ′ }⊢l M {C
φ |ρ ⊢l f (V ){let x:T = V in C

where φ(f ) = clon+1, φ′ (f (x : T ) : U . M)

Fig. 8. Operational semantics of bounded recursion

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

25

For the closure typing judgement Cl :T → U we substitute:

⊢l φ : Φ Φ[f :T → U ] | x :T ⊢l M :U
⊢l clon,φ (f (x : T ) : U . M) : T → U

As regards the denotational semantics, the clause for limited recursive deﬁnitions is:

[[ letrecn f (x :T ) :U = M in N ]](ϕ)(γ ) ≃

[[N ]](ϕ[(µnα : S[[[T ]], [[U ]]]. λa : [[T ]]. [[M]](ϕ[α/f ])a)/f ]) (γ )

The results for the operational and denotational semantics carry over to the restricted setting, and
we refer to them in the same way as we do to the unrestricted versions.

Relating the two languages, the n-th approximant M (n) of a language term M is obtained by
replacing every recursive deﬁnition in M by an n-limited one (similarly for boolean terms) and
n-th approximants of closures and function environments are deﬁned by structural recursion:

cloφ (f (x :T ) :U . M)(n) = clon,φ(n) (f (x :T ) :U . M (n))

{. . . , fi 7→ Cli , . . .}(n) = {. . . , fi 7→ Cl(n)

i

, . . .}

The terms M (n) can be deﬁned by structural recursion; we just give one clause of the deﬁnition:
( letrec f (x : T ) : U = M in N )(n) = letrecn f (x : T ) : U = M (n) in N (n)

Approximation preserves typing judgments.
Termination is proved for the approximation language by structural induction via a suitable

notion of computability.
- A closure

is computable iﬀ n = 0 or n > 0 and, for all ⊢l V : T we have:

⊢l clon,φ (f (x : T ) : U . M) : T → U

[[M]][[(φ[clon−1,φ (f (x :T ) :U . M)/f ]) : Φφ [f :T → U ]]][[V ]]↓ =⇒

φ[clon−1,φ (f (x :T ) :U . M)/f ] | {x 7→V } ⊢l M ⇓
- A function environment ⊢l φ : Φ is computable iﬀ ⊢l φ(f ) : Φ(f ) is a computable closure, for

every f ∈ Dom(φ).

- A term Φ | Γ ⊢l M : T is computable iﬀ for every computable ⊢l φ : Φ and every ⊢ ρ : Γ

[[M]][[φ]][[ρ]] ↓ =⇒ φ | ρ ⊢l M ⇓

(and similarly for boolean terms).

Strictly speaking, in the above we should say that it is the sequent Φ | Γ ⊢l M : T that is computable
and similarly for closures and function environments.

Lemma 6.4.
(1) Every closure ⊢l clon,φ (f (x : T ) : U . M) : T → U is computable.
(2) Every function environment ⊢l φ : Φ is computable.
(3) Every term Φ | Γ ⊢l M : T is computable.
(4) Every boolean term Φ | Γ ⊢l B is computable.

The next two lemmas enable us to lift completeness from the approximation language to the
main one. The ﬁrst lets us pass from semantic existence in the main language to semantic exis-
tence in the approximation language; the second allows us to pass in the opposite direction from
termination in the approximation language to termination in the main one.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

26

Martín Abadi and Gordon D. Plotkin

Lemma 6.5. For any well-typed term M of the main language we have:

[[M]] =

[[M (n)]]

Ün ∈N

and similarly for boolean terms, closures, and function environments.

Lemma 6.6. For any term M of the main language we have:

and

φ(n) | ρ ⊢l M (n) ⇒ V =⇒ φ | ρ ⊢ M ⇒ V

φ(n) | ρ ⊢l M (n) { C =⇒ φ | ρ ⊢ M { C

and similarly for boolean terms.

Operational completeness follows straightforwardly from these three lemmas:
Theorem 6.7 (Operational completeness). Suppose that Φ | Γ ⊢ M : T , ⊢ φ : Φ, and ⊢ ρ : Γ.

Then:

(1) Operational semantics.

[[M]][[φ]][[ρ]] ↓ =⇒ φ | ρ ⊢ M ⇓

(and similarly for boolean terms).
(2) Symbolic operational semantics.

[[M]][[φ]][[ρ]] ↓ =⇒ ∃C. φ | ρ ⊢ M { C

Proof.
(1) Suppose [[M]][[φ]][[ρ]] ↓. By Lemma 6.5 [[M]][[φ]] =

for some n. By Lemma 6.4, both M (n) and φ(n) are computable. So φ(n)
Lemma 6.6, we then have φ | ρ ⊢ M ⇓, as required. The proof for boolean terms is similar.

Ô

n[[M (n)]][[φ(n)]]. So [[M (n)]][[φ(n)]][[ρ]] ↓
| ρ ⊢l M (n) ⇓. By

(2) This follows from the ﬁrst part and Proposition 3.5.

(cid:3)

7 DISCUSSION
There is much more to do on the theory of diﬀerentiable programming languages, even at a basic
level; we brieﬂy suggest some possibilities. Trace-based automatic diﬀerentiation systems gener-
ally work with A-normal forms, or equivalent structures. They also employ optimizations. For
example, as mentioned in the Introduction, they may record auxiliary information in evaluation
traces to reduce recomputation. Other automatic diﬀerentiation systems rely on code transfor-
mations for diﬀerentiation. It would be interesting to deﬁne and study such optimizations and
alternative approaches, perhaps in the setting of our language.

Another interesting possibility would be to work with non-diﬀerentiable functions like ReLU
or with non-smooth functions. For the former, one might use Clarke sub-gradients [Clarke 1990],
following [Di Gianantonio and Edalat 2013] (the Clarke sub-gradient of ReLU at 0 is the interval
[0, 1]); for the latter, one may use Ck functions. Yet another possibility would be to work with
approximate reals, rather than reals, and to seek numerical accuracy theorems; one might employ
a domain-theoretic notion of sub-diﬀerentiation of functions over Scott’s interval domain, gener-
alizing the Clarke sub-gradient (see [Edalat and Lieutier 2004; Edalat and Maleki 2018]).

One would like results for richer languages, with a wider range of types or with computational
eﬀects. The problem is then how these additional features interact with diﬀerentiation. An ex-
tension to side-eﬀects would make contact with the literature on the automatic diﬀerentiation of

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

27

imperative languages, such as Fortran. An extension to probability, in some form, would make con-
tact with stochastic optimization and, further, with probabilistic languages for statistical learning.
For higher-order types, there may be a domain-theoretic analogue of convenient vector spaces
that additionally supports recursion. Further, one might, as suggested in [Vákár et al. 2018], seek a
domain-theoretic analogue of diﬀeological spaces (see [Iglesias-Zemmour 2013]); that would also
accommodate sum and recursive types. One might also wish to program with Riemannian mani-
folds, to accommodate natural gradient descent [Amari 1996]; these too should ﬁt into a diﬀeolog-
ical framework. In another direction, the work on categories with diﬀerential structure may yield
an axiomatic version of adequacy theorems for programming languages with diﬀerentiation con-
structs; such categories further equipped with structure to model partiality [Cockett et al. 2011]
are of particular interest.

Finally, if perhaps orthogonally, it is important to add explicit tensor (multi-dimensional array)
types, with accompanying shape analysis. There is a long history of programming-language design
in this area; a salient example is the design of Remora [Slepak et al. 2014].

REFERENCES
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat,
Geoﬀrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit
Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016a. TensorFlow:
a system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation
(OSDI 16). USENIX Association, 265–283.

Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016b. Deep
learning with diﬀerential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security (CCS ’16). ACM, 308–318. https://doi.org/10.1145/2976749.2978318

Samson Abramsky and Achim Jung. 1994. Domain theory.

In Handbook of Logic in Computer Science (Vol. 3), Samson

Abramsky, Dov M. Gabbay, and T. S. E. Maibaum (Eds.). Oxford University Press, Inc., 1–168.

Akshay Agrawal, Akshay Naresh Modi, Alexandre Passos, Allen Lavoie, Ashish Agarwal, Asim Shankar, Igor Ganichev,
Josh Levenberg, Mingsheng Hong, Rajat Monga, et al. 2019. TensorFlow Eager: A multi-stage, Python-embedded DSL
for machine learning. arXiv preprint arXiv:1903.01855 (2019).

Shun-ichi Amari. 1996. Neural learning in structured parameter spaces — natural Riemannian gradient. In Advances in

Neural Information Processing Systems 9, NIPS, M. Mozer, M. I. Jordan, and T. Petsche (Eds.). MIT Press, 127–133.

Atilim Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeﬀrey Mark Siskind. 2018. Automatic diﬀer-

entiation in machine learning: a survey. Journal of Machine Learning Research 18, 153 (2018), 1–43.

Atilim Günes Baydin, Barak A. Pearlmutter, and Jeﬀrey Mark Siskind. 2016. Tricks from deep learning. CoRR abs/1611.03777

(2016).

Thomas Beck and Herbert Fischer. 1994. The if-problem in automatic diﬀerentiation. J. Comput. Appl. Math. 50, 1-3 (May

1994), 119–131. https://doi.org/10.1016/0377-0427(94)90294-1

James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian,
David Warde-Farley, and Yoshua Bengio. 2010. Theano: A CPU and GPU math expression compiler. In Proceedings of
the Python for scientiﬁc computing conference (SciPy), Vol. 4.

Yves Bertot and Pierre Castéran. 2013. Interactive theorem proving and program development: CoqâĂŹArt: the calculus of

inductive constructions. Springer Science & Business Media.

Richard Blute, Thomas Ehrhard, and Christine Tasson. 2010. A convenient diﬀerential category.

arXiv preprint

arXiv:1006.3140 (2010).

Richard F Blute, J Robin B Cockett, and Robert AG Seely. 2009. Cartesian diﬀerential categories. Theory and Applications

of Categories 22, 23 (2009), 622–672.

Antonio Bucciarelli, Thomas Ehrhard, and Giulio Manzonetto. 2010. Categorical models for simply typed resource calculi.

Electronic Notes in Theoretical Computer Science 265 (2010), 213–230.

Bruce Christianson. 2012. A Leibniz notation for automatic diﬀerentiation. In Recent Advances in Algorithmic Diﬀerentiation,
Shaun Forth, Paul Hovland, Eric Phipps, Jean Utke, and Andrea Walther (Eds.). Lecture Notes in Computational Science
and Engineering, Vol. 87. Springer, 1–9.

Frank H. Clarke. 1990. Optimization and nonsmooth analysis. Classics in Applied Mathematics, Vol. 5. SIAM.
J Robin B Cockett, Geoﬀ SH Cruttwell, and Jonathan D Gallagher. 2011. Diﬀerential restriction categories. Theory and

Applications of Categories 25, 21 (2011), 537–613.

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

28

Martín Abadi and Gordon D. Plotkin

Leonardo Mendonça de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von Raumer. 2015. The Lean
Theorem Prover (System Description). In Automated Deduction - CADE-25 - 25th International Conference on Automated
Deduction, Berlin, Germany, August 1-7, 2015, Proceedings (Lecture Notes in Computer Science), Amy P. Felty and Aart
Middeldorp (Eds.), Vol. 9195. Springer, 378–388. https://doi.org/10.1007/978-3-319-21401-6_26

Pietro Di Gianantonio and Abbas Edalat. 2013. A language for diﬀerentiable functions. In Foundations of Software Science

and Computation Structures, Frank Pfenning (Ed.). Springer, 337–352.

Abbas Edalat and André Lieutier. 2004. Domain theory and diﬀerential calculus (functions of one variable). Mathematical

Structures in Computer Science 14, 6 (2004), 771–802. https://doi.org/10.1017/S0960129504004359

Abbas Edalat and Mehrdad Maleki. 2018. Diﬀerential calculus with imprecise input and its logical framework. In
Foundations of Software Science and Computation Structures - 21st International Conference, FOSSACS 2018. 459–475.
https://doi.org/10.1007/978-3-319-89366-2_25

Thomas Ehrhard and Laurent Regnier. 2003. The diﬀerential lambda-calculus. Theo. Comp. Sci. 309, 1-3 (2003), 1–41.
Conal Elliott. 2018. The simple essence of automatic diﬀerentiation. In Proceedings of the ACM on Programming Languages

(ICFP).

Matthias Felleisen and Daniel P. Friedman. 1987. Control operators, the SECD-machine and the λ-calculus.

In Formal

Description of Programming Concepts III, M. Wirsing (Ed.). Elsevier, 193–217.
H. Fischer. 2001. Automatic diﬀerentiation: root problem and branch problem.
Floudas and P. M. Pardalos (Eds.). Vol. I. Kluwer Academic Publishers, 118–122.

In Encyclopedia of Optimization, C. A.

Roy Frostig, Matthew James Johnson, and Chris Leary. 2018. Compiling machine learning programs via high-level tracing.

Presented at SysML 2018.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.
Andreas Griewank. 2000. Evaluating derivatives - principles and techniques of algorithmic diﬀerentiation. Frontiers in applied

mathematics, Vol. 19. SIAM.

L. Hascoët and V. Pascual. 2013. The Tapenade automatic diﬀerentiation tool: principles, model, and speciﬁcation. ACM

Transactions On Mathematical Software 39, 3 (2013).

P. Iglesias-Zemmour. 2013. Diﬀeology. American Mathematical Society.
Andreas Kriegl and Peter W Michor. 1997. The convenient setting of global analysis. Vol. 53. American Mathematical Soc.
Dougal Maclaurin, David Duvenaud, and Ryan P Adams. 2015. Autograd: eﬀortless gradients in Numpy. In ICML 2015

AutoML Workshop, Vol. 238.

Oleksandr Manzyuk. 2012. A simply typed λ-calculus of forward automatic diﬀerentiation. Electronic Notes in Theoretical
Computer Science 286 (2012), 257 – 272. https://doi.org/10.1016/j.entcs.2012.08.017 Proceedings of the 28th Conference
on the Mathematical Foundations of Programming Semantics (MFPS XXVIII).

Micaela Mayero. 2002. Using theorem proving for numerical analysis (correctness proof of an automatic diﬀerentiation
algorithm). In Theorem Proving in Higher Order Logics, 15th International Conference, TPHOLs 2002, Hampton, VA, USA,
August 20-23, 2002, Proceedings (Lecture Notes in Computer Science), Victor Carreño, César A. Muñoz, and Soﬁène Tahar
(Eds.), Vol. 2410. Springer, 246–262. https://doi.org/10.1007/3-540-45685-6_17

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,
Natalia Gimelshein, Luca Antiga, et al. 2019. PyTorch: An imperative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems. 8024–8035.

Barak A. Pearlmutter and Jeﬀrey Mark Siskind. 2008. Reverse-mode AD in a functional framework: lambda the ultimate

backpropagator. ACM Trans. Program. Lang. Syst. 30, 2 (2008), 7:1–7:36. https://doi.org/10.1145/1330017.1330018

Daniel Selsam, Percy Liang, and David L. Dill. 2017. Developing bug-free machine learning systems with formal math-
ematics. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017 (Proceedings of Machine Learning Research), Doina Precup and Yee Whye Teh (Eds.), Vol. 70. PMLR,
3047–3056.

Amir Shaikhha, Andrew Fitzgibbon, Dimitrios Vytiniotis, Simon Peyton Jones, and Christoph Koch. 2018. Eﬃcient diﬀer-

entiable programming in a functional array-processing language. CoRR abs/1806.02136 (2018). arXiv:1806.02136

Jeﬀrey Mark Siskind and Barak A. Pearlmutter. 2005. Perturbation confusion and referential transparency: correct func-
tional implementation of forward-mode AD. In Implementation and Application of Functional Languages—17th Interna-
tional Workshop, IFL’05, A. Butterﬁeld (Ed.). 1–9. Trinity College Dublin, Computer Science Department Technical
Report TCD-CS-2005-60.

Jeﬀrey Mark Siskind and Barak A. Pearlmutter. 2008. Nesting forward-mode AD in a functional framework. Higher-Order

and Symbolic Computation 21, 4 (2008), 361–376. https://doi.org/10.1007/s10990-008-9037-1

Justin Slepak, Olin Shivers, and Panagiotis Manolios. 2014. An array-oriented language with static rank polymorphism.
In Proceedings of the 23rd European Symposium on Programming Languages and Systems - Volume 8410. Springer-Verlag,
27–46. https://doi.org/10.1007/978-3-642-54833-8_3

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

A Simple Diﬀerentiable Programming Language

29

Shuang Song, Kamalika Chaudhuri, and Anand D. Sarwate. 2013. Stochastic gradient descent with diﬀerentially private
updates. In IEEE Global Conference on Signal and Information Processing, GlobalSIP 2013, Austin, TX, USA, December 3-5,
2013. IEEE, 245–248. https://doi.org/10.1109/GlobalSIP.2013.6736861

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a next-generation open source framework for
deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-Ninth Conference
on Neural Information Processing Systems (NIPS), Vol. 5. 1–6.

W.F. Trench. 2003. Introduction to Real Analysis. Prentice Hall/Pearson Education.
M. Vákár, O. Kammar, and S. Staton. 2018. Diﬀeological spaces and semantics for diﬀerential programming. Presented at

Domains XIII Workshop.

Bart van Merrienboer, Dan Moldovan, and Alexander B. Wiltschko. 2018. Tangent: automatic diﬀerentiation using source-
code transformation for dynamically typed array programming. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada.,
S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.). 6259–6268.

Fei Wang, Xilun Wu, Grégory M. Essertel, James M. Decker, and Tiark Rompf. 2018. Demystifying diﬀerentiable program-

ming: shift/reset the penultimate backpropagator. CoRR abs/1803.10228 (2018). arXiv:1803.10228

Yuan Yu, Martín Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeﬀ Dean, Sanjay Ghemawat, Tim Harley,
Peter Hawkins, Michael Isard, Manjunath Kudlur, Rajat Monga, Derek Murray, and Xiaoqiang Zheng. 2018. Dynamic
control ﬂow in large-scale machine learning. In Proceedings of the Thirteenth EuroSys Conference (EuroSys ’18). ACM,
Article 18, 15 pages. https://doi.org/10.1145/3190508.3190551

Proc. ACM Program. Lang., Vol. 4, No. POPL, Article . Publication date: January 2020.

