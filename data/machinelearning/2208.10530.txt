Smoothness Analysis for Probabilistic Programs
with Application to Optimised Variational Inference

2
2
0
2

g
u
A
2
2

]
L
P
.
s
c
[

1
v
0
3
5
0
1
.
8
0
2
2
:
v
i
X
r
a

WONYEOL LEE, Stanford University, USA
XAVIER RIVAL, INRIA Paris, ENS, and CNRS/PSL University, France
HONGSEOK YANG, KAIST, South Korea and Institute for Basic Science (IBS), South Korea

We present a static analysis for discovering differentiable or more generally smooth parts of a given probabilistic
program, and show how the analysis can be used to improve the pathwise gradient estimator, one of the most
popular methods for posterior inference and model learning. Our improvement increases the scope of the
estimator from differentiable models to non-differentiable ones without requiring manual intervention of
the user; the improved estimator automatically identifies differentiable parts of a given probabilistic program
using our static analysis, and applies the pathwise gradient estimator to the identified parts while using a
more general but less efficient estimator, called score estimator, for the rest of the program. Our analysis has a
surprisingly subtle soundness argument, partly due to the misbehaviours of some target smoothness properties
when viewed from the perspective of program analysis designers. For instance, some smoothness properties,
such as partial differentiability and partial continuity, are not preserved by function composition, and this makes
it difficult to analyse sequential composition soundly without heavily sacrificing precision. We formulate five
assumptions on a target smoothness property, prove the soundness of our analysis under those assumptions,
and show that our leading examples satisfy these assumptions. We also show that by using information from our
analysis instantiated for differentiability, our improved gradient estimator satisfies an important differentiability
requirement and thus, under a mild regularity condition, computes the correct estimate on average, i.e., it returns
an unbiased estimate. Our experiments with representative probabilistic programs in the Pyro language show
that our static analysis is capable of identifying smooth parts of those programs accurately, and making our
improved pathwise gradient estimator exploit all the opportunities for high performance in those programs.

Additional Key Words and Phrases: smoothness, static analysis, probabilistic programming, variational inference

1 INTRODUCTION
Probabilistic programs define models from machine learning and statistics, and are used to analyse
datasets from a wide range of applications [2, 4, 13–16, 26–28, 36, 38–41, 48]. These programs are
written in languages with special runtimes, called inference engines, which can be used to answer
probabilistic queries, such as posterior inference and marginal likelihood estimation, or to learn
model parameters in those programs, such as weights of neural networks. Whether a probabilistic
program is useful for, for instance, discovering a hidden pattern in a given dataset or making an
accurate prediction largely lies in these inference engines. These engines should compute accurate
approximations or good model parameters within a fixed time budget. It is, thus, not surprising
that substantial research efforts have been made to develop efficient inference algorithms and their
implementations (as inference engines) [5, 19, 22, 26, 30, 34, 37, 43, 45–47, 50].

We are concerned with smoothness1 properties of probabilistic programs, which have been ex-
ploited by performant posterior-inference and model-learning algorithms and engines. For instance,

1In mathematics, “smoothness” typically refers to the specific property of functions: being infinitely differentiable. In this
paper, we override the term to denote a set of properties of functions describing well-behavedness (e.g., differentiability).

Authors’ addresses: Wonyeol Lee, Computer Science, Stanford University, USA, wonyeol@cs.stanford.edu; Xavier Rival,
INRIA Paris, ENS, and CNRS/PSL University, France, rival@di.ens.fr; Hongseok Yang, School of Computing and Kim Jaechul
Graduate School of AI, KAIST, South Korea, hongseok.yang@kaist.ac.kr, Discrete Mathematics Group, Institute for Basic
Science (IBS), South Korea.

2022. 2475-1421/2022/8-ART0 $15.00
https://doi.org/

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

 
 
 
 
 
 
0:2

Wonyeol Lee, Xavier Rival, and Hongseok Yang

when probabilistic programs are differentiable (in the sense that they define differentiable unnor-
malised densities), their posteriors can be inferred by Hamiltonian Monte Carlo [29], one of the best
performing MCMC algorithms. Also, in that case, their posteriors and model parameters can be
inferred or learnt using the pathwise gradient estimator [21, 33], a popular technique for estimating
the gradient of a function using samples. We also point out that the need for smoothness arises in a
broader context of machine learning and computer science; Lipschitz continuity is one of the desired
or at least recommended properties for neural networks [1, 20], and also differentiability commonly
features as a requirement for pieces of code inside simulation software and cyber physical systems,
where differential equations are used to specify the environment [31].

We present a static analysis that enables optimised posterior inference and model learning for
probabilistic programs. We develop a static analysis that discovers differentiable or more generally
smooth parts of given probabilistic programs, and show how the analysis can be used to improve the
pathwise gradient estimator. Our improvement increases the scope of the estimator from differen-
tiable to non-differentiable models, without requiring any intervention from the user; the improved
estimator automatically identifies differentiable parts of probabilistic programs using our static
analysis, and applies the pathwise gradient estimator to the identified parts while using a more
general but less efficient estimator, called score estimator [32, 44], for the rest of the programs.

Our static analysis for smoothness has a surprisingly subtle soundness argument, partly due to the
misbehaviours of some target smoothness properties when viewed from the perspective of program
analysis designers. For instance, some smoothness properties, such as partial differentiability and
partial continuity, are not preserved by function composition, and this makes it difficult to analyse
sequential composition soundly without heavily sacrificing precision. In fact, overlooking such
misbehaviours has been a source of errors in published static analyses for continuity [6, 7].2 We
formulate five assumptions that clearly identify what a smoothness property should satisfy in order
to avoid unsound analysis. Interestingly, these assumptions also determine what the property is
allowed to violate. For instance, they reveal that the smoothness property does not have to be closed
under the limits of chains of smooth (partial) functions, although the closure under such limits, called
admissibility, has often been used to justify proof rules about or static analysis of loops. Dispensing
with the admissibility requirement broadens the scope of our program analysis non-trivially; some
useful smoothness properties from mathematics fail to meet the requirement.

Our variant of the pathwise gradient estimator works by program transformation and non-standard
execution. It first transforms given probabilistic programs based on the results of our static analysis,
and then executes the transformed (and original) programs according to a standard (and non-standard)
semantics. During execution, our estimator computes a quantity involving differentiation, which
becomes the estimate of the target gradient. We prove that our estimator satisfies an important differ-
entiability requirement and thus is correct (under a mild regularity condition): the computed estimate
is unbiased, i.e., it is the target gradient when averaged over random choices made during execution.
Our static analysis and variant of the pathwise gradient estimator have been implemented for a
subset of the Pyro probabilistic programming language [2]. They have been successfully applied to
the 13 representative Pyro examples, which include advanced models with deep neural networks,
such as attend-infer-repeat [12] and single-cell annotation using variational inference [49]. For each
of these examples, Pyro provides a default, selective use of the pathwise gradient estimator but

2 The analysis in [6] infers the continuity property for multivariate programs, but it incorrectly joins two input-variable sets if
a program is continuous with respect to each set jointly. Such a rule would hold if separate per-input-variable continuity were
considered, but it does not hold for multivariate joint continuity. Conversely, the analysis in [7] considers a per-input-variable
definition of continuity, but incorrectly assumes that this per-input-variable continuity is preserved by function composition.
See §A.1 for more details. We do not claim that these unsoundness issues are hard to fix. Instead, our point is that a similar
issue may be introduced easily and remain undetected due to the subtlety in the soundness of a static analysis.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:3

𝑐𝑚 =

𝑥1 := sam("z1", distN (0, 5), 𝜆𝑦.𝑦);
𝑥2 := sam("z2", distN (𝑥1, 3), 𝜆𝑦.𝑦);
if (𝑥2 > 0) {obs(distN (1, 1), 0)}
{obs(distN (−2, 1), 0)}
else

(cid:170)
(cid:174)
(cid:174)
(cid:174)
(cid:174)
(cid:172)

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:173)
(cid:171)

,

𝑐𝑔 =

(cid:18) 𝑥1 := sam("z1", distN (𝜃1, 1), 𝜆𝑦.𝑦);
𝑥2 := sam("z2", distN (𝜃2, 1), 𝜆𝑦.𝑦)

(cid:19)

.

Fig. 1. A model 𝑐𝑚 and a guide 𝑐𝑔 in a PPL. Here distN (𝑎, 𝑏) is the distribution expression, and denotes the
normal distribution with mean 𝑎 and variance 𝑏.

𝑐 ′
𝑔 =

(cid:32) 𝑥1 := sam("z1", distN (0, 1) , 𝜆𝑦.𝑦 + 𝜃1 );
𝑥2 := sam("z2", distN (0, 1) , 𝜆𝑦.𝑦 + 𝜃2 )

(cid:33)

,

𝑐 ′′
𝑔 =

(cid:32) 𝑥1 := sam("z1", distN (0, 1) , 𝜆𝑦.𝑦 + 𝜃1 );
𝑥2 := sam("z2", distN (𝜃2, 1), 𝜆𝑦.𝑦)
𝑔 (or 𝑐 ′′
𝑔 ).

(cid:33)

.

Fig. 2. A fully (or selectively) reparameterised guide 𝑐 ′

without any correctness guarantee. Our analysis and improved estimator automatically reproduced
those uses, and proved their correctness (assuming a mild regularity condition), more specifically,
the unbiasedness of the estimator in those use cases.

2 INFORMAL DESCRIPTION OF BASIC CONCEPTS AND OUR APPROACH
We start by describing informally basic concepts and the goal of our approach, which we hope helps
the reader to see the big picture of our technical contributions. To simplify presentation, we use toy
examples in the section. But we emphasise that our approach has been applied to representative
Pyro programs that describe advanced machine-learning models with deep neural networks.

Probabilistic programming and variational inference. In a probabilistic programming lan-
guage (PPL), a program expresses a probabilistic model. As an example, consider the program 𝑐𝑚
in Fig. 1, which describes a probabilistic model of the random variables 𝑧1 and 𝑧2 in R by specifying
their unnormalised density

𝑝𝑐𝑚 (𝑧1, 𝑧2) = N (𝑧1; 0, 5) · N (𝑧2; 𝑧1, 3) · (1[𝑧2>0] · N (0; 1, 1) + 1[𝑧2 ≤0] · N (0; −2, 1)),
where N (𝑥; 𝑎, 𝑏) is the probability density of a normal distribution with mean 𝑎 and variance 𝑏, and
1[𝜑 ] is the indicator function that returns 1 if 𝜑 holds and 0 otherwise. The first two N factors in
the equation come from the sample commands (sam) in 𝑐𝑚. They are called prior distributions, and
describe prior knowledge on two random variables named 𝑧1 and 𝑧2. The last factor comes from the
if and observe commands (if and obs), which express that an unnamed random variable is sampled
and observed to have 0 and its distribution is distN(1, 1) or distN(−2, 1) depending on whether 𝑧2
is positive or not. This factor is called likelihood, and it states information about 𝑧1 and 𝑧2 that comes
from an observed data point 0. Ignore the third arguments of the sample commands of 𝑐𝑚 for now,
which have no effect on 𝑝𝑐𝑚 ; they will be explained later.

The purpose of writing 𝑐𝑚 in a PPL, called model, is to infer its normalised probability density

𝑝𝑐𝑚 (𝑧1, 𝑧2) ≜ 𝑝𝑐𝑚 (𝑧1, 𝑧2)/

∫

𝑝𝑐𝑚 (𝑧1, 𝑧2) 𝑑𝑧1𝑑𝑧2,

also called normalised posterior density. Intuitively, this normalised density brings together two
types of information about 𝑧1 and 𝑧2, the first from their prior distributions (expressed in the first
and second lines of 𝑐𝑚), and the second from the observed data point 0 that depends on 𝑧1 and 𝑧2
(the third and fourth lines of 𝑐𝑚). This inference task is called posterior inference problem. Among
a wide range of approaches to the problem, we focus on the approach called variational inference,
which forms the core of the recent combination of PPLs and deep learning.

In variational inference, we posit another program 𝑐𝑔, called guide, that is simpler than 𝑐𝑚 and
parameterised by 𝜃 . Then, we approximate the normalised density of 𝑐𝑚 by 𝑐𝑔 with an optimal

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:4

Wonyeol Lee, Xavier Rival, and Hongseok Yang

choice of 𝜃 . For instance, consider the program 𝑐𝑔 in Fig. 1. The program specifies the following
already-normalised probability density

𝑝𝑐𝑔,𝜃 (𝑧1, 𝑧2) = N (𝑧1; 𝜃1, 1) · N (𝑧2; 𝜃2, 1).

It can serve as a guide program for 𝑝𝑐𝑚
by 𝑝𝑐𝑔,𝜃 , variational inference aims
. To best approximate 𝑝𝑐𝑚
at finding 𝜃 that minimises some notion of the discrepancy (called KL divergence) between 𝑝𝑐𝑔,𝜃 and
, or equivalently that maximises the objective function L (called evidence lower bound):
𝑝𝑐𝑚

arg max

𝜃

L (𝜃 )

for L (𝜃 ) ≜ E𝑝𝑐𝑔,𝜃 (𝑧1,𝑧2) [𝑓𝜃 (𝑧1, 𝑧2)] with 𝑓𝜃 (𝑧1, 𝑧2) ≜ log(𝑝𝑐𝑚 (𝑧1, 𝑧2)/𝑝𝑐𝑔,𝜃 (𝑧1, 𝑧2)).

A standard way to solve this optimisation problem is to apply the gradient-ascent algorithm: starting
from an initial value 𝜃 (0) of 𝜃 , compute 𝜃 (𝑡 ) iteratively by 𝜃 (𝑡 +1) ≜ 𝜃 (𝑡 ) + 𝜂 · ∇𝜃 L (𝜃 (𝑡 ) ), and return
𝜃 (𝑇 ) for a sufficiently large 𝑇 ∈ N. Here 𝜂 ∈ R>0 denotes a learning rate.

A challenging part in the algorithm is to compute ∇𝜃 L (𝜃 ). An exact computation of the gradient
is mostly intractable due to the expectation inside L, which hinders the gradient from having a
closed-form formula. Hence, in practice, we rather estimate (not exactly compute) the gradient via a
Monte Carlo method: draw a random sample ( ˆ𝑧1, ˆ𝑧2) from some distribution 𝑞𝜃 , apply some function
𝑔𝜃 to the sample, and use the result as an estimate to the gradient, i.e.,

𝑔𝜃 ( ˆ𝑧1, ˆ𝑧2) ≈ ∇𝜃 L (𝜃 )

for a sample ( ˆ𝑧1, ˆ𝑧2) drawn from 𝑞𝜃 ( ˆ𝑧1, ˆ𝑧2).

(1)

An important desired property of such a gradient estimator is unbiasedness, which states that the
estimate is accurate in expectation: E𝑞𝜃 (𝑧1,𝑧2) [𝑔𝜃 (𝑧1, 𝑧2)] = ∇𝜃 L (𝜃 ). This property is desired so as
to ensure that the algorithm converges to a local optimum.

Gradient estimators for variational inference: SCE, PGE, and SPGE. A standard estimator for
∇𝜃 L (𝜃 ) is the score estimator (SCE) [32, 44], which is unbiased under mild conditions. It estimates
∇𝜃 L (𝜃 ) by following the recipe in Eq. (1) with 𝑞𝜃 (𝑧1, 𝑧2) = 𝑝𝑐𝑔,𝜃 (𝑧1, 𝑧2) and

𝑔𝜃 (𝑧1, 𝑧2) = 𝑓𝜃 (𝑧1, 𝑧2) · ∇𝜃 log 𝑞𝜃 (𝑧1, 𝑧2).

That is, the estimator draws a sample from the guide distribution 𝑝𝑐𝑔,𝜃 and applies the above 𝑔𝜃 to
obtain a gradient estimate. It is applicable to a wide range of model-guide pairs while remaining
unbiased, but it is known to have a large approximation error (i.e., have a large variance).

The pathwise gradient estimator (PGE) [21, 33] is another standard gradient estimator, which is
known to have a smaller approximation error than the SCE and thus has been a preferred option
𝑔 that is a 𝜃 -independent reparameter-
against the SCE. The PGE requires an additional program 𝑐 ′
isation of the guide 𝑐𝑔. A program 𝑐 ′ is said to be 𝜃 -independent if the probability densities of the
sampled random variables in 𝑐 ′ are 𝜃 -independent. It is called a reparameterisation of 𝑐 if 𝑐 and 𝑐 ′
sample the same set of random variables and they have the same semantics on those variables in
the following sense: when there are 𝑛 random variables, for any measurable ℎ : R𝑛 → R, we have
E𝑝𝑐 (𝑧) [ℎ(𝑣𝑐 (𝑧))] = E𝑝𝑐′ (𝑧) [ℎ(𝑣𝑐′ (𝑧))], where 𝑝𝑐 : R𝑛 → R is the probability density of all 𝑛 random
variables in𝑐, and 𝑣𝑐 : R𝑛 → R𝑛 is the so called value function of𝑐, which applies the lambda functions
in the third arguments of 𝑐’s sample commands to the corresponding random variables. For example,
𝑔 in Fig. 2 is a 𝜃 -independent reparameterisation of 𝑐𝑔 for 𝜃 = (𝜃1, 𝜃2). It has the following probability
𝑐 ′
density 𝑝𝑐′
𝑔,𝜃 (𝑧1, 𝑧2) = (𝑧1 + 𝜃1, 𝑧2 + 𝜃2).
Note that 𝑝𝑐′
does not depend on 𝜃 , as required by the 𝜃 -independence of 𝑐𝑔. We can show this 𝑐 ′
𝑔
is a reparameterisation of 𝑐𝑔 in Fig. 1 by using the fact that 𝑣𝑐𝑔 is the identity function and 𝑦 = 𝑥 + 𝑎
for 𝑥 drawn from N (𝑥; 0, 1) follows the distribution N (𝑦; 𝑎, 1).

𝑔 (𝑧1, 𝑧2) = N (𝑧1; 0, 1) · N (𝑧2; 0, 1), and the value function 𝑣𝑐′

𝑔

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:5

Given a reparameterised guide 𝑐 ′
Eq. (1) this time with 𝑞′(𝑧1, 𝑧2) = 𝑝𝑐′

𝑔, the PGE estimates ∇𝜃 L (𝜃 ) by again following the recipe in
𝑔 (𝑧1, 𝑧2) and
1, 𝑧 ′
2)

𝑔,𝜃 (𝑧1, 𝑧2).

2) = 𝑣𝑐′

for (𝑧 ′

1, 𝑧 ′

𝜃 (𝑧1, 𝑧2) = ∇𝜃 𝑓𝜃 (𝑧 ′
𝑔′

𝑔

1, 𝑧 ′

𝑔,𝜃 .3

This estimator differs from the SCE in two aspects. First, a random sample is drawn from a reparam-
eterised-guide distribution 𝑝𝑐′
computes the deriv-
2) (not with respect to 𝜃 only), since the argument of
ative of 𝑓𝜃 (𝑧 ′
1, 𝑧 ′
depends on 𝜃 via 𝑣𝑐′
𝑓𝜃 (−) in 𝑔′
𝜃

, not from 𝑝𝑐𝑔,𝜃 . Next, the estimation function 𝑔′
𝜃

2) with respect to 𝜃 and (𝑧 ′

While having a small approximation error, to ensure the unbiasedness, the PGE requires more
than the SCE. An important additional requirement for the PGE is that (i) 𝑝𝑐𝑚 (𝑧1, 𝑧2) and 𝑝𝑐𝑔,𝜃 (𝑧1, 𝑧2)
𝑔,𝜃 (𝑧1, 𝑧2) be differentiable in 𝜃 for all 𝑧1, 𝑧2. The
should be differentiable in 𝜃 and 𝑧1, 𝑧2 and (ii) 𝑣𝑐′
requirement is imposed partly to ensure that no differentiation error arises in computing 𝑔′
. This
𝜃
differentiability requirement, however, can be easily violated if a model or a guide starts to use
branches or loops. For instance, it is violated by our example in Figs. 1 and 2 as 𝑝𝑐𝑚 (𝑧1, 𝑧2) is not
differentiable in 𝑧2. This violation makes the PGE biased for the example, i.e.,

E𝑞′

𝜃 (𝑧1,𝑧2) [𝑔′

𝜃 (𝑧1, 𝑧2)] = (· · · , 1

3 (𝜃1 − 𝜃2)) ≠ (· · · , 1

3 (𝜃1 − 𝜃2) + 3

2 N (−𝜃2; 0, 1)) = ∇𝜃 L (𝜃 ),

and thus causes the gradient-ascent algorithm to converge to a suboptimal 𝜃 : applying the PGE to the
example produces a suboptimal solution 𝜃 = (0, 0), whereas the optimal solution is 𝜃 ≈ (0.95, 1.52).
The selective pathwise gradient estimator (SPGE) [37] combines the two previous gradient estima-
tors to alleviate their limitations: one has a large approximation error, and the other imposes a strong
𝑔 that is a reparameter-
requirement for unbiasedness. The SPGE requires an additional program 𝑐 ′′
isation of the guide 𝑐𝑔 but needs not be 𝜃 -independent (unlike the PGE). An instance of 𝑐 ′′
𝑔 for our
example is given in Fig. 2, which changes the sample command for 𝑧1 in 𝑐𝑔 but keeps the one for 𝑧2.
Note that the changed sample command for 𝑧1 in 𝑐 ′′
𝑔 uses a 𝜃 -independent probability distribution.
Typically, 𝑐 ′′
𝑔 is obtained by selecting a subset of the random variables in 𝑐𝑔 and changing the sample
commands for the selected variables such that their probability distributions become 𝜃 -independent;
𝑔 , the SPGE estimates ∇𝜃 L (𝜃 )
the sample commands for the unselected remain as they are. Given 𝑐 ′′
by following the recipe in Eq. (1) with 𝑞′′

𝜃 (𝑧1, 𝑧2) = 𝑝𝑐′′
for (𝑧 ′′
1 , 𝑧 ′′
𝜃 (𝑧1, 𝑧2) = ∇𝜃 𝑓𝜃 (𝑧 ′′
𝑔′′
consists of two terms, which come from that of the PGE and
Note that the estimation function 𝑔′′
𝜃
the SCE. The second term adjusts the PGE to correctly account for unchanged random variables (e.g.,
𝑧2 in the example of Fig. 2).

𝑔,𝜃 (𝑧1, 𝑧2) and
𝜃 (𝑧1, 𝑧2)

2 ) · ∇𝜃 log 𝑞′′

2 ) + 𝑓𝜃 (𝑧 ′′

𝑔,𝜃 (𝑧1, 𝑧2).

2 ) = 𝑣𝑐′′

1 , 𝑧 ′′

1 , 𝑧 ′′

(2)

By allowing a guide that makes only some selected (not all) random variables 𝜃 -independent, the
SPGE offers two advantages at the same time: it achieves a smaller approximation error than the SCE,
and imposes a weaker requirement for unbiasedness than the PGE. In particular, the differentiability
requirement for the SPGE is weaker than that for the PGE, which is as follows for our example in Figs. 1
and 2: (i) 𝑝𝑐𝑚 (𝑧1, 𝑧2) and 𝑝𝑐𝑔,𝜃 (𝑧1, 𝑧2) be differentiable in 𝜃 and 𝑧1 (but they may be non-differentiable
𝑔,𝜃 (𝑧1, 𝑧2) be differentiable in 𝜃 for all 𝑧1, 𝑧2. This requirement holds,
in 𝑧2); and (ii) 𝑣𝑐′′
and as a result, the SPGE with this 𝑐 ′′
𝑔 is biased as
seen before).

𝑔 is unbiased (whereas the PGE with the given 𝑐 ′

𝑔,𝜃 (𝑧1, 𝑧2) and 𝑝𝑐′′

Variable-selection problem for SPGE. To maximize the advantages offered by the SPGE, we
consider the following algorithmic problem:

3By the chain rule,
𝜕𝑓𝜃 (𝑧′
1, 𝑧′
2)
𝜕𝜃1

𝜕𝑓𝜃 (𝑥1, 𝑥2)
𝜕𝜃1

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(𝑥1,𝑥2,𝜃 )
1,𝑧′
=(𝑧′
2,𝜃 )

(cid:42)(cid:32) 𝜕𝑓𝜃 (𝑥1, 𝑥2)
𝜕𝑥1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

,

(𝑥1,𝑥2,𝜃 )
1,𝑧′
=(𝑧′
2,𝜃 )

𝜕𝑓𝜃 (𝑥1, 𝑥2)
𝜕𝑥2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(𝑥1,𝑥2,𝜃 )
1,𝑧′
=(𝑧′
2,𝜃 )

(cid:33)

,

(cid:32) 𝜕𝑣𝑐′

𝑔,𝜃 (𝑦1, 𝑦2)
𝜕𝜃1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(𝑦1,𝑦2,𝜃 )
=(𝑧1,𝑧2,𝜃 )

(cid:33)(cid:43)

.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:6

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Definition 2.1 (SPGE Variable-Selection Problem; Informal). Assume that we are given a model 𝑐𝑚,
a guide 𝑐𝑔, and a reparameterisation plan 𝜋, i.e., a map from sample commands to sample commands.
𝜋,𝑆 be the result
Then, find automatically a large subset 𝑆 of random variables such that if we let 𝑐𝑔
𝜋,𝑆 is a
of 𝜋-transforming every sample command in 𝑐𝑔 that defines a random variable in 𝑆, then 𝑐𝑔
𝜋,𝑆 ) satisfies the differentiability requirement for the SPGE. □
reparameterisation of 𝑐 and (𝑐𝑚, 𝑐𝑔, 𝑐𝑔

An instantiation of the problem for our example is that 𝑐𝑚 and 𝑐𝑔 are programs in Fig. 1 and 𝜋
transforms commands of the form 𝑦 := sam(𝑛, distN(𝑒 ′, 1), 𝜆𝑦.𝑦) to 𝑦 := sam(𝑛, distN (0, 1), 𝜆𝑦.𝑦 +𝑒 ′),
while leaving all the other sample commands as they are. In this instantiation, the condition in the
problem is met by 𝑆 = ∅ and 𝑆 = {𝑧1}, and the latter option is preferred due to its size. Note that the
solution 𝑆 = {𝑧1} yields the guide 𝑐 ′′

𝑔 in Fig. 2, that is, 𝑐𝑔

𝜋,𝑆 = 𝑐 ′′
𝑔 .

Existing PPLs, when applying the SPGE, choose an 𝑆 without checking the differentiability require-
ment, and this can make the requirement easily violated. For instance, given a model-guide pair, in
one of its standard settings, Pyro automatically applies the SPGE with 𝑆 being the set of all continuous
random variables in the guide. This choice of 𝑆, however, does not guarantee the requirement is met.
For our example in Fig. 1, Pyro chooses 𝑆 = {𝑧1, 𝑧2}, but this 𝑆 violates the requirement; due to this,
the SPGE becomes biased and Pyro returns a suboptimal 𝜃 = (0, 0).

In the rest of the paper, we will present our solution to the SPGE variable-selection problem. A
core component of our solution is a general static analysis framework for smoothness properties (§5),
such as differentiability, which our solution uses to discharge the differentiability requirement for
the SPGE correctly and automatically. As we briefly mentioned in the introduction, automatically
analysing the smoothness properties of a program in a sound manner is surprisingly subtle. Our
analysis framework identifies five assumptions for smoothness properties, and prove that the analysis
is sound if a target smoothness property satisfies these assumptions.

Our solution for the SPGE variable-selection problem (§6) runs the static analysis on given 𝑐𝑚
and 𝑐𝑔, and computes a maximal set 𝑆 ′ of random variables in which 𝑝𝑐𝑚 and 𝑝𝑐𝑔,𝜃 are differentiable.
𝜋,𝑆′ satisfies the
Then, it heuristically searches for a subset of 𝑆 ′ starting from 𝑆 ′ itself such that 𝑐𝑔
differentiability requirement. For instance, for our example in Fig. 1, our differentiability analysis
infers that 𝑝𝑐𝑚 and 𝑝𝑐𝑔,𝜃 are differentiable in {𝑧1} and {𝑧1, 𝑧2}, respectively. From this, we set 𝑆 ′ = {𝑧1},
run our analysis again on 𝑐𝑔
and
are differentiable in 𝜃 . Thus, this 𝑆 ′ becomes the final result. In fact, this first-round success
𝑣𝑐𝑔
appeared in our experiments; our implementation shows on all tested examples that the initial choice
of 𝑆 ′ is indeed valid in the above sense so that no subset search is necessary (§7).

𝜋,𝑆′ meets the requirement, i.e., 𝑝𝑐𝑔

𝜋,𝑆′, and are confirmed that 𝑐𝑔

𝜋,𝑆′,𝜃

𝜋,𝑆′,𝜃

We point out that to develop and analyse mathematically our solution for the SPGE variable-
selection problem, we formalise the SPGE in the PPL setting and formally derive a sufficient condition
(which includes the differentiability requirement) for its unbiasedness (§4).

3 SETUP
We use a simple imperative probabilistic programming language, which models the core of popular
imperative PPLs, such as Pyro. Programs in the language describe densities, which are sometimes
unnormalised (i.e., they do not integrate to 1). In this section, we describe the syntax and semantics
of the language, and also variational inference for the language.

Syntax of a simple imperative PPL. Let PVar be a finite set of program variables, Str be a finite
set of strings, and Fn be a set of function symbols that represent measurable maps of type R𝑘 → R.
The language has the following syntax:

Real Expr. 𝑒 ::= 𝑥 | 𝑟 | op(𝑒1, . . . , 𝑒𝑘 )

Boolean Expr. 𝑏 ::= true | 𝑒1 < 𝑒2 | 𝑏1 ∧ 𝑏2 | ¬𝑏

Name Expr. 𝑛 ::= name(𝛼, 𝑒)

Distribution Expr. 𝑑 ::= distN(𝑒, 𝑒 ′)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:7

Command 𝑐 ::= skip | 𝑥 := 𝑒 | 𝑐; 𝑐 ′ | if 𝑏 {𝑐} else {𝑐 ′} | while 𝑏 {𝑐} | 𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒) | obs(𝑑, 𝑟 )
Here 𝑥, 𝑟 , op, and 𝛼 stand for a program variable in PVar, a real number, a function symbol in Fn, and
a string in Str, respectively.

The language has four kinds of expressions, which denote maps from states to values of appropriate
types. All the real and boolean expressions are standard. The name expressions𝑛 denote the identifiers
of drawn samples (i.e., random variables). They are built by appending an integer (obtained by the
floor of a real) to a string in Str; e.g., name("z", 3.2) denotes the name ("z", 3). The distribution
expression distN(𝑒, 𝑒 ′) denotes the normal distribution with mean 𝑒 and variance 𝑒 ′. The language
supports standard commands for imperative computation, and additionally has sample and observe
for probabilistic programming. The sample command 𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒) creates a random variable
named 𝑛 by drawing a sample 𝑟 from 𝑑; then, it transforms 𝑟 to 𝑒 [𝑟 /𝑦] and stores the result in the
program variable 𝑥. In the programs written by the user of the language, only the identity function
𝜆𝑦.𝑦 appears as the third argument of the sample commands. But as we explain later, when a program is
constructed from another by a gradient estimator, such as the SPGE, it may contain sample commands
with non-identity function arguments. The observe command obs(𝑑, 𝑟 ) describes that an unnamed
random variable is drawn from 𝑑 and is immediately observed to have the value 𝑟 . Computationally,
obs(𝑑, 𝑟 ) calculates the probability density of 𝑑 at 𝑟 and updates a variable that tracks the product of
these probabilities from all the observations, by multiplying the variable with the calculated density.

Density semantics of the PPL. We use a semantics of our language where commands are inter-
preted as calculators for densities, which may be unnormalised. Commands transform states, but
in so doing, they compute densities of sampled random variables. More precisely, in the semantics,
a command starts with an initial state that fixes not just the values of program variables but also
those of all the random variables that are to be sampled during execution. When the command runs,
it calculates the densities of those random variables at their given initial values, and also computes
the probability density of all the observations, called likelihood. The product of all these densities
and the likelihood becomes the so called unnormalised posterior density.

Let N be the set of natural numbers. Fix 𝑁 ∈ N with 𝑁 ≥ 1. Formally, the semantics uses the states

of the following form:

𝜇 ∈ Name ≜ {(𝛼, 𝑖) | 𝛼 ∈ Str, 𝑖 ∈ N ∩ [0, 𝑁 )},
𝑎 ∈ AVar ≜ {like} ∪ {pr 𝜇, val𝜇, cnt𝜇 | 𝜇 ∈ Name},
𝑢, 𝑣 ∈ Var ≜ Name ⊎ PVar ⊎ AVar,

𝜎 ∈ St ≜ [Var → R],

St[𝐾] ≜ [𝐾 → R] for 𝐾 ⊆ Var.

Here 𝜎 (𝜇) for 𝜇 ∈ Name is the initial value of the random variable 𝜇, which is used by the sample
command and does not change during execution. For technical simplicity, the set Name has the
restriction that the integer part of a name must be in [0, 𝑁 ).4 The set AVar consists of four types of
auxiliary variables. The auxiliary variable like stores the likelihood (i.e., the probability density of
all the observations), and its value is initialised to 1 and changes whenever the observe command
obs(𝑑, 𝑟 ) runs; the new value becomes the old times the density of the probability distribution 𝑑 at
𝑟 . The other auxiliary variables pr 𝜇, val𝜇, and cnt𝜇 are associated with a random variable 𝜇. They
are initialised with N (𝜎 (𝜇); 0, 1) (i.e., the density of the standard normal distribution at 𝜎 (𝜇)), 𝜎 (𝜇),
and 0, respectively, and get updated by the sample command 𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒) where 𝑛 denotes

4This restriction is usually respected by probabilistic programs in practice, since they normally sample random variables
whose number is uniformly bounded over all traces. The uniform bound 𝑁 can often be found by a simple static analysis. This
restriction along with the finiteness of PVar and Str implies the finiteness of Var, and this makes our technical development
simpler since 𝜎 ∈ St becomes a function on a finite-dimensional space; defining differentiability for functions in [R∞ → R]
requires more technical materials (e.g., Frechet derivative and Hilbert space).

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:8

Wonyeol Lee, Xavier Rival, and Hongseok Yang

𝜇. The command increases cnt𝜇 by 1, so as to record the occurrence of a sampling event for 𝜇. Then,
it looks up the given value 𝜎 (𝜇) of the random variable 𝜇, transforms the value to 𝑒 [𝜎 (𝜇)/𝑦], and
stores the result in 𝑥 and val𝜇. Finally, the command computes the density of the distribution 𝑑 at
the looked-up value 𝜎 (𝜇), and updates pr 𝜇 with this density. The unnormalised posterior density
(i.e., the joint density of all the random variables and observations) is then obtained by multiplying
at the end of program execution the values of like and pr 𝜇 for all 𝜇 ∈ Name.

The formal semantics of expressions is standard, and has the following types:

: St → R,

𝑒
(cid:74)

(cid:75)

: St → B,

𝑏
(cid:74)

(cid:75)

: St → Name,

𝑛
(cid:74)

(cid:75)

: St → D.

𝑑
(cid:74)

(cid:75)

Here B is the set of booleans, i.e., true and false, and D is the set of positive probability-density func-
tions on R, i.e., a subset of [R → (0, ∞)] whose elements are measurable functions that integrate to 1.
The semantics is defined for a minor extension of the set of expressions where non-program variables
are allowed to appear, such as (𝜇 + 𝑥). The interpretation of expressions is mostly standard. We
𝜎),
show only the case for the name expressions 𝑛 ≡ name(𝛼, 𝑒):
(cid:75)
where create_name is an operator that takes a string-real pair (𝛼, 𝑟 ) ∈ Str × R and creates a name
(𝛼, 𝑖) in Name. We assume that create_name(𝛼, 𝑖) = (𝛼, 𝑖) for 𝑖 ∈ N ∩ [0, 𝑁 ), but other than this
assumption, we leave the definition of create_name open.

𝜎 ≜ create_name(𝛼,
(cid:75)

name(𝛼, 𝑒)
(cid:74)

𝑒
(cid:74)

Note that according to the semantics, the evaluation of an expression always produces a value
of the right type. In particular, it never generates an error. For instance, when an argument of an
operator op or a distribution constructor distN is outside its intended domain as in log(−3) and
distN (0, −2), or when the integer part of a name expression is outside [0, 𝑁 ) as in name("z", −1), our
semantics does not generate an error. Instead, it returns some pre-chosen default value of the right
type. This slightly unusual way of handling errors is also adopted in our semantics of commands to
be presented shortly, and it lets us avoid the complexity caused by error handling when we formalise
variational inference and develop our program analysis for smoothness properties.

The formal semantics of commands is also mostly standard with the handling of errors via default
values, although its interpretation of sample and observe commands deserves special attention.
Let ⊥ be an element not in St, and define St⊥ to be the usual lifting of St with ⊥. That is, St⊥ is a
partially-ordered set St ⊎ {⊥} with the following order: for 𝜉, 𝜉 ′ ∈ St⊥, we have 𝜉 ⊑ 𝜉 ′ if and only
if 𝜉 = ⊥ or 𝜉 = 𝜉 ′. We write the standard lifting of a function 𝑓 : St → St⊥ by 𝑓 † : St⊥ → St⊥ (i.e.,
𝑓 †(𝜉) ≜ if (𝜉 = ⊥) then 𝜉 else 𝑓 (𝜉)). The semantics of a command 𝑐 is a map
: St → St⊥, and is
defined inductively as shown below:

𝑐
(cid:74)

(cid:75)

𝑐
(cid:74)

𝜎 else
(cid:75)

skip
(cid:74)
𝑥 := 𝑒
(cid:74)
𝑐; 𝑐 ′
(cid:74)
if 𝑏 {𝑐} else {𝑐 ′}
(cid:74)
while 𝑏 {𝑐}
(cid:74)
𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒 ′)
(cid:74)

𝜎 ≜ 𝜎,
(cid:75)
𝜎 ≜ 𝜎 [𝑥 ↦→
𝜎],
𝑒
(cid:75)
(cid:74)
(cid:75)
†(
𝑐 ′
𝜎 ≜
𝜎),
𝑐
(cid:75)
(cid:74)
(cid:75)
(cid:75)
(cid:74)
𝑐 ′
𝜎 ≜ if (
𝜎 = true) then
𝜎,
𝑏
(cid:74)
(cid:75)
(cid:75)
(cid:75)
(cid:74)
𝜎 = true) then 𝑓 †(
𝜎 ≜ (fix 𝐹 )(𝜎) where 𝐹 (𝑓 )(𝜎) ≜ if (
𝑏
(cid:74)
(cid:75)
(cid:75)
𝜎 ≜ 𝜎 [𝑥 ↦→ 𝑟, val𝜇 ↦→ 𝑟, pr 𝜇 ↦→
𝜎 (𝜎 (𝜇)), cnt𝜇 ↦→ 𝜎 (cnt𝜇) + 1]
𝑑
(cid:75)
(cid:74)
(cid:75)
𝜎 and 𝑟 ≜
𝑛
(cid:75)
(cid:74)
𝜎 ≜ 𝜎 [like ↦→ 𝜎 (like) ·
𝜎 (𝑟 )].
𝑑
(cid:75)
(cid:75)
(cid:74)
The interpretation uses the least fixed-point operator fix for continuous maps 𝐹 on the function
space [St → St⊥], where the function space is ordered pointwise and continuity means the one
with respect to this order. According to this interpretation, 𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒 ′) increments the cnt𝜇
variable for the name 𝑛 = 𝜇 so that the variable, which has 0 initially, records the number of times
that the random variable with the same name 𝑛 is sampled during execution.

𝜎) else 𝜎,
(cid:75)

𝑒 ′[𝜇/𝑦]
(cid:74)

where 𝜇 ≜

obs(𝑑, 𝑟 )

𝜎,
(cid:75)

𝑐
(cid:74)

(cid:74)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:9

Having some cnt𝜇 variable increased by 2 or larger at some point of execution is not an intended
behaviour of a command 𝑐. That is, if 𝑐 is a well-designed command, every random variable with
a fixed name should be sampled at most once during the execution of 𝑐. This intended behaviour of
commands plays an important role in our results, and we refer to it using the following terminology.

Definition 3.1. An always-terminating command 𝑐 does not have a double-sampling error if for any
□

𝜎 ∈ St, we have

𝑐
(cid:74)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) ≤ 1 for all 𝜇 ∈ Name.
(cid:75)

Variational inference. We consider the most common form of variational inference for Pyro-like
probabilistic programming languages where we are asked to learn a good approximation of the
posterior of a given model, i.e., the conditional distribution of the model given a dataset. Typically,
a parameterised approximate posterior is given in variational inference, and learning corresponds to
finding good values of those parameters. A popular approach is to measure the quality of parameter
values by the so called evidence lower bound (ELBO), and to optimise ELBO.

To translate what we have described so far to our context, we need to explain a general recipe for
generating a density 𝑝𝑐 for a command 𝑐, which is in general unnormalised (i.e., does not integrate to
1). The recipe specifies 𝑝𝑐 as follows:5 for each 𝜎𝜃 ∈ St[𝜃 ], 𝑝𝑐,𝜎𝜃 : St[Name] → [0, ∞) is defined by

𝑝𝑐,𝜎𝜃 (𝜎𝑛) ≜

(cid:40)

𝑐
(cid:74)
0

𝜎 (like) · (cid:206)𝜇 ∈Name
(cid:75)

𝑐
(cid:74)

𝜎 (pr 𝜇)
(cid:75)

𝑐
(cid:74)

if
𝜎 ∈ St and
(cid:75)
otherwise

𝑐
(cid:74)

𝜎 (cnt𝜇) ≤ 1 for all 𝜇
(cid:75)

(3)

where 𝜎 = 𝜎0 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛 ∈ St, and the ⊕ operator combines two real-valued maps with disjoint
domains in the standard way. Also, 𝜎0 ∈ St[(PVar \ 𝜃 ) ⊎ AVar] maps like to 1, pr 𝜇 to N (𝜎𝑛 (𝜇); 0, 1)
and val𝜇 to 𝜎𝑛 (𝜇) for every 𝜇 ∈ Name, and all other variables to 0. Here St[Name] is understood as
a measurable space constructed by taking the product of the |Name| copies of the measurable space
R and the integral is taken over the uniform measure on St[Name] (i.e., the product of the |Name|
copies of the Lebesgue measure on R).

In variational inference in our PPL context, we are given two commands 𝑐𝑚 and 𝑐𝑔, called model
and guide. We assume that (i) these commands always terminate and do not have a double-sampling
error, (ii) some variables 𝜃 = {𝜃1, . . . , 𝜃𝑘 } ⊆ PVar that only appear in 𝑐𝑔, not in 𝑐𝑚, are identified as
parameters to be optimised, and (iii) the density 𝑝𝑐𝑔,𝜎𝜃 of the guide 𝑐𝑔 integrates to 1 and defines a
probability distribution.6 Given the model-guide pair (𝑐𝑚, 𝑐𝑔), a popular approach for variational
inference is to solve the following optimisation problem approximately,
(cid:2)log(𝑝𝑐𝑚 (𝜎𝑛)/𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛))(cid:3),

(4)

E𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛)

argmax
𝜎𝜃

when the expectation is well-defined for all 𝜎𝜃 . The objective of this optimisation is the ELBO that
we mentioned earlier. Here 𝑝𝑐𝑚 means 𝑝𝑐𝑚,𝜎′
does not matter since
𝑐𝑚 does not access the parameters 𝜃 and so 𝑝𝑐𝑚,𝜎′

for some/any 𝜎 ′
𝜃
for all 𝜎 ′
𝜃 = 𝑝𝑐𝑚,𝜎′′
𝜃

; the choice of 𝜎 ′
𝜃
𝜃 ∈ St[𝜃 ].
, 𝜎 ′′

Often variational inference is applied when the model 𝑐𝑚 is parameterised as well. In those cases,
it asks for finding good parameters of the model 𝑐𝑚 as well as those of the guide 𝑐𝑔. So, an algorithm
for variational inference this time simultaneously learns a good model for given observations and
a good approximate posterior for the learnt model. This more general form of variational inference

𝜃

𝜃

5To simplify presentation, we use the fact that the language in our formalism uses the normal distributions only and
the densities of these normal distributions are always greater than 0. Our formalism can be generalised to support any
distributions, and our implementation is based on such generalisation.
6In practice, one more assumption is required: the set of random variables sampled from the model should be the same as
that from the guide. This assumption can be checked automatically, e.g., by [24, 25]. In this work, however, this assumption
is always satisfied: all random variables in Name are sampled by a sample command or at the beginning (via initialisation).

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:10

Wonyeol Lee, Xavier Rival, and Hongseok Yang

can be easily accommodated in our setup. We just need to drop the condition that the parameters
may not appear in 𝑐𝑚, and to use 𝑝𝑐𝑚,𝜎𝜃 instead of 𝑝𝑐𝑚 in the optimisation objective in Eq. (4):

argmax
𝜎𝜃

L (𝜎𝜃 ) for L (𝜎𝜃 ) ≜ E𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛)

(cid:2)log(𝑝𝑐𝑚,𝜎𝜃 (𝜎𝑛)/𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛))(cid:3) .

(5)

The rest of the paper focuses on this general form of variational inference.

4 SELECTIVE PATHWISE GRADIENT ESTIMATOR
We consider a gradient-based algorithm for the optimisation problem in Eq. (5). The algorithm finds
a good 𝜎𝜃 by repeatedly estimating the gradient of the optimisation objective at the current 𝜎𝜃 ,

grad_est(𝜎𝜃 ) ≈ ∇𝜃 E𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛)

(cid:2)log(𝑝𝑐𝑚,𝜎𝜃 (𝜎𝑛)/𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛))(cid:3),

and updating 𝜎𝜃 with the estimate under a learning rate 𝜂 > 0, that is, 𝜎𝜃 ← 𝜎𝜃 + 𝜂 · grad_est(𝜎𝜃 ).
Note that the core of the algorithm lies in the computation of grad_est(𝜎𝜃 ).

In this section, we describe a particular algorithm for the gradient computation, called selective
pathwise gradient estimator (SPGE), which is often regarded as the algorithm of choice and corresponds
to the inference algorithm developed for stochastic computation graphs [37] and implemented for
Pyro. Our description of the SPGE takes the often-ignored aspect of customising the SPGE algorithm
for PPLs seriously, and it is accompanied with a novel formal analysis of the customisation. Our
analysis clearly identifies information about probabilistic programs that is useful for this customised
SPGE algorithm, and prepares the stage for our program analysis for smoothness properties in §5.

4.1 Program transformation
We start by describing a program transformation that changes some sample commands in a given
probabilistic program. This transformation is used crucially by the SPGE.

The key component of the transformation is a partial function 𝜋 called reparameterisation plan,
which has the type NameEx × DistEx × LamEx ⇀ DistEx × LamEx. Here NameEx, DistEx, and
LamEx denote the sets of name expressions, distribution expressions, and lambda expressions of
the form 𝜆𝑦.𝑒, respectively. The plan 𝜋 specifies how we transform sample commands. Concretely,
assume that we are given 𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒). We check whether 𝜋 (𝑛, 𝑑, 𝜆𝑦.𝑒) is defined or not. If
not, we keep the original sample command. Otherwise, so that 𝜋 (𝑛, 𝑑, 𝜆𝑦.𝑒) is (𝑑 ′, 𝜆𝑦 ′.𝑒 ′), we replace
the command with 𝑥 := sam(𝑛, 𝑑 ′, 𝜆𝑦 ′.𝑒 ′).

A natural extension of this intended transformation of 𝜋 leads to the following program trans-

formation for a general command 𝑐, denoted by 𝑐𝜋 :

𝜋
≜ skip,
skip
𝑥 := 𝑒𝜋 ≜ 𝑥 := 𝑒,
𝜋,
𝜋 ≜ 𝑐𝜋 ; 𝑐 ′
𝜋

𝑐; 𝑐 ′
if 𝑏 {𝑐} else {𝑐 ′}
while 𝑏 {𝑐}

𝑥 := sam(𝑛, 𝑑, 𝑙)

obs(𝑑, 𝑟 )

𝜋

},

≜ if 𝑏 {𝑐𝜋 } else {𝑐 ′
≜ while 𝑏 {𝑐𝜋 },

≜

(cid:26)𝑥 := sam(𝑛, 𝑑 ′, 𝑙 ′)
𝑥 := sam(𝑛, 𝑑, 𝑙)

≜ obs(𝑑, 𝑟 ).

𝜋

𝜋

𝜋

if ∃(𝑑 ′, 𝑙 ′). 𝜋 (𝑛, 𝑑, 𝑙) = (𝑑 ′, 𝑙 ′)
otherwise,

The transformation recursively traverses 𝑐, and applies 𝜋 to all the sample commands in 𝑐. Note that
for any 𝜋, there exists a total function 𝜋 ′ such that 𝑐𝜋 = 𝑐𝜋 ′ for all 𝑐; the 𝜋 ′ coincides with 𝜋 in the
domain of 𝜋, and outside of this domain, it is the identity function. But such 𝜋 ′ loses information
about the domain of 𝜋, which plays a crucial role in our formalisation of the SPGE.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:11

We are primarily interested in semantics-preserving instances of · 𝜋 . The next definition helps

us to identify such instances.

Definition 4.1. A reparameterisation plan 𝜋 is valid if for all 𝑛 ∈ NameEx, 𝑑, 𝑑 ′ ∈ DistEx, and
(𝜆𝑦.𝑒), (𝜆𝑦 ′.𝑒 ′) ∈ LamEx such that 𝜋 (𝑛, 𝑑, 𝜆𝑦.𝑒) = (𝑑 ′, 𝜆𝑦 ′.𝑒 ′), the following condition holds: for all
states 𝜎 ∈ St and measurable subsets 𝐴 ⊆ R,

∫

(𝜎 [𝑦↦→𝑟 ]) ∈𝐴] ·

1[

𝑒
(cid:74)

(cid:75)

𝑑
(cid:74)

𝜎 (𝑟 ) 𝑑𝑟 =
(cid:75)

∫

(𝜎 [𝑦′↦→𝑟 ]) ∈𝐴] ·

1[

𝑒′
(cid:74)

(cid:75)

𝑑 ′
(cid:74)

𝜎 (𝑟 ) 𝑑𝑟 .
(cid:75)

□ (6)

√

The condition says that the distribution obtained by sampling from 𝑑 and applying 𝜆𝑦.𝑒 is the
same as that obtained by sampling from 𝑑 ′ and applying 𝜆𝑦 ′.𝑒 ′. An example of a widely-used valid
reparameterisation plan maps its input as follows, whenever defined: 𝜋0(𝑛, distN (𝑒1, 𝑒2), 𝜆𝑦.𝑒3) =
𝑒2+𝑒1)/𝑦]), where we assume𝑦 does not appear in𝑒1 and𝑒2, the substitution
(distN(0, 1), 𝜆𝑦.𝑒3 [(𝑦×
− denotes a square-
in 𝜋0 expresses the composition of two functions 𝜆𝑦.𝑒3 and 𝜆𝑦.(𝑦 ×
root operator that handles non-positive arguments in the same way as distN (𝑒, −) does: if
𝜎 ≤ 0
𝑒2
(cid:75)
(cid:74)
𝑟2. The above plan satis-
𝜎 = 𝜆𝑟 . N (𝑟 ;
and
𝜎, 𝑟2) for some 𝑟2 > 0, then
𝜎 =
(cid:75)
(cid:75)
(cid:75)
𝑟2 +𝑟1 with a sample𝑦 from N (0, 1) is distributed by N (𝑟1, 𝑟2).
fies the condition in Eq. (6), because𝑦 ×
We now show that · 𝜋 with a valid 𝜋 preserves semantics. For a command 𝑐 and 𝜎𝜃 ∈ St[𝜃 ], define

distN (𝑒1, 𝑒2)

𝑒2 +𝑒1), and

𝑒1
(cid:74)

𝑒2

√

√

√

√

√

(cid:74)

(cid:74)

the value function 𝑣𝑐,𝜎𝜃 : St[Name] → St[Name] as follows:
𝑐
(cid:74)

𝑣𝑐,𝜎𝜃 (𝜎𝑛)(𝜇) ≜ let 𝜎 ≜ 𝜎0 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛 in

𝜎 (val𝜇)
(cid:75)

(cid:40)

𝑐
(cid:74)
0

𝜎 ∈ St and
if
(cid:75)
otherwise

𝑐
(cid:74)

𝜎 (cnt𝜇′) ≤ 1 for all 𝜇 ′
(cid:75)

where 𝜎0 ∈ St[(PVar \ 𝜃 ) ⊎ AVar] maps like to 1, and pr 𝜇 to N (𝜎𝑛 (𝜇); 0, 1) and val𝜇 to 𝜎𝑛 (𝜇) for
every 𝜇 ∈ Name, and it also maps all the other variables to 0. The value function basically applies the
lambda functions in 𝑐’s sample commands to the corresponding random variables. The next theorem
proves that if 𝜋 is valid, the program transformation · 𝜋 preserves the semantics in the sense that
the integral of a function ℎ remains the same under 𝑐 and 𝑐𝜋 for any 𝑐. Note that the two integrals
in the theorem are connected via the value functions of 𝑐 and 𝑐𝜋 .

Theorem 4.2. Let 𝜋 be a valid reparameterisation plan, and 𝑐 be a command. Then, for all 𝜎𝜃 ∈ St[𝜃 ]

and all measurable ℎ : St[Name] → R, we have
(cid:17)
(cid:16)

∫

𝑑𝜎𝑛

𝑝𝑐,𝜎𝜃 (𝜎𝑛) · ℎ(𝑣𝑐,𝜎𝜃 (𝜎𝑛))

∫

=

(cid:16)

𝑑𝜎𝑛

𝑝𝑐𝜋 ,𝜎𝜃 (𝜎𝑛) · ℎ(𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛))

(cid:17)

where the left integral is defined if and only if the right integral is defined.
Remark 1. One immediate yet important consequence of the theorem is that if 𝑝𝑐,𝜎𝜃 is a probability
□
density, so is 𝑝𝑐𝜋 ,𝜎𝜃

. This consequence will be used in §4.2 and the proof of Theorem 4.4 later.

4.2 Gradient estimator via program transformation
Let 𝑐 be a command that always terminates and does not have a double-sampling error, and let
𝜎𝜃 ∈ St[𝜃 ]. We define the partial density function 𝑝 ⟨𝑆 ⟩
𝑐,𝜎𝜃 of 𝑐 over a subset 𝑆 ⊆ Name as
𝑐,𝜎𝜃 (𝜎𝑛) ≜ (cid:214)
𝑝 ⟨𝑆 ⟩
: St[Name] → (0, ∞),
𝑐
𝜇 ∈𝑆 (cid:74)

(𝜎0 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛)(pr 𝜇),

𝑝 ⟨𝑆 ⟩
𝑐,𝜎𝜃

where 𝜎0 is set as in the definition of 𝑝𝑐,𝜎𝜃 in Eq. (3). The partial density 𝑝 ⟨𝑆 ⟩
is essentially the full
𝑐,𝜎𝜃
density 𝑝𝑐,𝜎𝜃 in Eq. (3) with the omission of the factors not mentioned in 𝑆. Intuitively, it computes
the density of the random variables in 𝑆 conditioned on the random variables outside of 𝑆.

(7)

(cid:75)

The SPGE computes an approximate gradient of the objective L in Eq. (5) using the program
transformation in the previous subsection. Its inputs are a model 𝑐𝑚, a guide 𝑐𝑔, parameters 𝜃 to
𝜋 always terminate and do not
optimise, and a reparameterisation plan 𝜋, where (i) 𝑐𝑚, 𝑐𝑔, and 𝑐𝑔

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:12

Wonyeol Lee, Xavier Rival, and Hongseok Yang

have a double-sampling error, and (ii) 𝑐𝑔 defines the normalised probability density 𝑝𝑐𝑔,𝜎𝜃 for all
𝜎𝜃 ∈ St[𝜃 ]. Given these inputs, the SPGE computes an approximate gradient in three steps. First,
it defines the set rv(𝜋) ⊆ Name of random variables to be reparameterised:

(8)

𝜋 ,𝜎𝜃

𝜋 , and draws a sample ˆ𝜎𝑛 from 𝑝𝑐𝑔

.7 Drawing a sample ˆ𝜎𝑛 makes sense here since 𝑝𝑐𝑔

rv(𝜋) ≜ {(𝛼, 𝑖) ∈ Name | (name(𝛼, _), _, _) ∈ dom(𝜋)}
where _ means some existentially quantified (meta) variable. Second, the SPGE transforms the guide
𝑐𝑔 to 𝑐𝑔
𝜋 ,𝜎𝜃
is a probability density (i.e., it normalises to 1) by Remark 1. Another important point is that drawing
𝜋 in the standard sampling semantics (not in our density
ˆ𝜎𝑛 can be done simply by executing 𝑐𝑔
semantics), where each sample command is interpreted as a random draw, not as a density calculator.
Third, the SPGE computes the following approximation of ∇𝜃 L (𝜎𝜃 ) and returns it as a result:
𝑛)(cid:1) · log(𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
(𝜎 ′
𝑛) + ∇𝜃 log 𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′

grad_est(𝜎𝜃, ˆ𝜎𝑛) ≜ (cid:0)∇𝜃 log 𝑝 ⟨Name\rv (𝜋 ) ⟩
(𝜎 ′

𝑐𝑔,𝜎𝜃
Recall that if a command 𝑐 always terminates, both the partial density 𝑝 ⟨𝑆 ⟩
𝑐,𝜎𝜃 (𝜎𝑛) and the full density
𝑝𝑐,𝜎𝜃 (𝜎𝑛) can be computed simply by executing 𝑐 in our semantics and calculating the defining
formulas of both densities from the final state of the execution. Thus, all the terms in grad_est can
be computed by executing 𝑐𝑔 and 𝑐𝑚 according to our density semantics or differentiating the results
of these executions via, for instance, automatic differentiation as done in Pyro. Note that grad_est
applies two non-trivial optimisations, when compared with the (naive) SPGE explained in Eq. (2):
𝜋 , and its second term
its first term involves a partial density of 𝑐𝑔 instead of the full density of 𝑐𝑔
involves again a partial density of 𝑐𝑔 this time instead of the full density of 𝑐𝑔.

𝑐𝑔,𝜎𝜃
− ∇𝜃 log 𝑝 ⟨rv (𝜋 ) ⟩

𝑛)/𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛),

𝑛))
for 𝜎 ′

𝜋 ,𝜎𝜃 ( ˆ𝜎𝑛).

𝑛 ≜ 𝑣𝑐𝑔

(9)

Is the SPGE correct in any sense? The answer depends on its inputs. If the inputs satisfy the
requirements that we will explain soon, the result of the SPGE is precisely ∇𝜃 L (𝜎𝜃 ) on average, that
is, ∇𝜃 L (𝜎𝜃 ) = E[grad_est(𝜎𝜃, ˆ𝜎𝑛)], where the expectation is taken over the sample ˆ𝜎𝑛 used by the
SPGE. This property is called unbiasedness, and it plays the crucial role for ensuring that parameters
updated iteratively with estimated gradients converge to a local optimum.

Let us now spell out the requirements on the inputs of the SPGE. To do so, we need to introduce

one further concept for the reparameterisation plans 𝜋.

Definition 4.3. A reparameterisation plan 𝜋 is simple if for all (𝑛, 𝑑, 𝜆𝑦.𝑒) and (𝑛′, 𝑑 ′, 𝜆𝑦 ′.𝑒 ′) in
NameEx × DistEx × LamEx such that 𝑛 and 𝑛′ have the same string part, we have (𝑛, 𝑑, 𝜆𝑦.𝑒) ∈
□
dom(𝜋) ⇐⇒ (𝑛′, 𝑑 ′, 𝜆𝑦 ′.𝑒 ′) ∈ dom(𝜋).

The simplicity is one of the requirements that the SPGE imposes on 𝜋. It ensures that the set rv(𝜋)
computed by the SPGE describes the sample commands in 𝑐𝑔 to be transformed by · 𝜋 . Specifically, it
forbids 𝜋 from using any syntax-specific information of the arguments of a sample command when
it decides whether to transform the command or not. All the requirements of the SPGE, including
the simplicity just explained, are summarised in the next theorem.

Theorem 4.4. Let 𝑐𝑚, 𝑐𝑔, and 𝜋 be the inputs to the SPGE (which satisfy the assumptions (i) and
(ii) described above in this subsection). Suppose that L (𝜎𝜃 ) and ∇𝜃 L (𝜎𝜃 ) are well-defined for every
𝜎𝜃 ∈ St[𝜃 ]. Further, assume that every sample command in 𝑐𝑔 has 𝜆𝑦.𝑦 as its third argument, and 𝑐𝑔
does not have observe commands. Then, for all 𝜎𝜃 ∈ St[𝜃 ],

∇𝜃 L (𝜎𝜃 ) = E𝑝𝑐𝑔 𝜋 ,𝜎𝜃 ( ˆ𝜎𝑛) [grad_est(𝜎𝜃, ˆ𝜎𝑛)]

(10)

if 𝜋 satisfies the following requirements:

7In practice, the SPGE often draws a fixed number of independent samples ˆ𝜎 (1)
1
𝑀

𝑖=1 grad_est(𝜎𝜃 , ˆ𝜎 (𝑖 )
(cid:205)𝑀

𝑛 ) as an estimate of ∇𝜃 L (𝜎𝜃 ). The presented results hold for this more general case as well.

𝑛 , . . . , ˆ𝜎 (𝑀 )

from 𝑝𝑐𝑔

𝜋 ,𝜎𝜃

𝑛

and computes

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:13

(R1) 𝜋 is valid and simple.
(R2) The below functions from St[𝜃 ] × St[Name] to (0, ∞) are differentiable in 𝜃 ∪ rv(𝜋) jointly:

(𝜎𝜃, 𝜎𝑛) ↦−→ 𝑝𝑐𝑚,𝜎𝜃 (𝜎𝑛),

(𝜎𝜃, 𝜎𝑛) ↦−→ 𝑝 ⟨rv (𝜋 ) ⟩
(R3) For all 𝜎𝑛 ∈ St[Name], the below functions on St[𝜃 ] are differentiable in 𝜃 jointly:
𝜎𝜃 ↦−→ 𝑝 ⟨rv (𝜋 ) ⟩
𝜋 ,𝜎𝜃

(𝜎𝜃, 𝜎𝑛) ↦−→ 𝑝 ⟨Name\rv (𝜋 ) ⟩

𝜎𝜃 ↦−→ 𝑣𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛),

(𝜎𝑛),

(𝜎𝑛),

𝑐𝑔,𝜎𝜃

𝑐𝑔,𝜎𝜃

𝜋 ,𝜎𝜃

𝑐𝑔

(𝜎𝑛).

(𝜎𝑛).

𝜎𝜃 ↦−→ 𝑝 ⟨Name\rv (𝜋 ) ⟩
𝑐𝑔
(𝜎𝑛) = 0.

(R4) For all 𝜎𝜃 ∈ St[𝜃 ] and 𝜎𝑛 ∈ St[Name], we have ∇𝜃 𝑝 ⟨rv (𝜋 ) ⟩
𝜋 ,𝜎𝜃
(R5) The below equations hold for all 𝜎𝜃 ∈ St[𝜃 ]:

𝑐𝑔

∫

∇𝜃

(cid:16)

𝑑𝜎𝑛

∫

∇𝜃

(cid:16)

𝑑𝜎𝑛

𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛) · log

𝜋 ,𝜎𝜃 (𝜎𝑛)
𝑝𝑐𝑔
𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)
𝑛 for 𝑣𝑐𝑔

∫

∫

(cid:17)

(cid:17)

=

=

(cid:16)

𝑑𝜎𝑛

∇𝜃 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)

(cid:17)

,

(cid:16)

𝑑𝜎𝑛 ∇𝜃

𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛) · log

𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)

(cid:17)

.

In the second equation, we write 𝜎 ′

𝜋 ,𝜎𝜃 (𝜎𝑛).
To be clear, 𝑓 : St[𝐾] → R𝑛 for 𝐾 ⊆ Var is said to be differentiable in 𝐾 ′ ⊆ 𝐾 jointly if for any
𝜏 ∈ St[𝐾 \ 𝐾 ′], 𝑓 | [𝜏 ] : St[𝐾 ′] → R𝑛 is (jointly) differentiable, where 𝑓 | [𝜏 ] (𝜎) ≜ 𝑓 (𝜎 ⊕ 𝜏).

𝜋 , and the value function of 𝑐𝑔

In this work, we focus on the requirements (R2) and (R3) on smoothness. They require that five
𝜋 be differentiable in certain variables.
density functions of 𝑐𝑚, 𝑐𝑔, and 𝑐𝑔
We will develop a program-analysis framework to check these differentiability requirements soundly
and automatically (§5), and will describe an algorithm to the SPGE variable-selection problem, using
the developed analysis framework (§6). The remaining requirements (R1), (R4) and (R5) are of less
interest in this work. (R1) and (R4) can be guaranteed by simple syntactic checks and our way of
constructing 𝜋 (Lemma D.1). (R5) follows from (R2) and (R3) and a few more regularity conditions
on densities which are usually satisfied in practice (Theorem D.2).

We point out that Pyro uses the SPGE in their inference engine, but without checking the above
requirements. In particular, its default option simply uses the 𝜋 that transforms all the continuous
random variables in a guide, and this can easily violate the requirements and make the SPGE biased.

4.3 Local Lipschitzness for relaxed requirements
In Theorem 4.4, we considered the requirements (R2) and (R3) about the differentiability of density
and value functions, as a sufficient condition for the unbiasedness of the SPGE. They are, however,
sometimes too strong to hold in practice due to the use of popular non-differentiable functions
(e.g., ReLU). As we will see in §7, the requirements are indeed violated by some representative Pyro
programs even though the conclusion of Theorem 4.4 holds for those programs (i.e., the estimated
gradients by the SPGE for those programs are unbiased).

To validate the unbiasedness of the SPGE for more examples in practice, we consider the following
relaxation of the requirements (R2) and (R3), which changes differentiability to local Lipschitzness:

(R2’) The functions in (R2) are locally Lipschitz in 𝜃 ∪ rv(𝜋) jointly.
(R3’) For every 𝜎𝑛 ∈ St[Name], the functions in (R3) are locally Lipschitz in 𝜃 jointly.

Here 𝑓 : R𝑛 → R𝑚 is Lipschitz if there is 𝐶 > 0 such that ∥𝑓 (𝑥) − 𝑓 (𝑥 ′)∥2 ≤ 𝐶 ∥𝑥 − 𝑥 ′∥2 for all
𝑥, 𝑥 ′ ∈ R𝑛; and 𝑓 is locally Lipschitz if for all 𝑥 ∈ R𝑛, there is an open neighborhood 𝑈 ⊆ R𝑛 of 𝑥 such
that 𝑓 |𝑈 : 𝑈 → R𝑚 is Lipschitz. Further, 𝑓 : St[𝐾] → R𝑚 for 𝐾 ⊆ Var is locally Lipschitz in 𝐾 ′ ⊆ 𝐾
jointly if for any 𝜏 ∈ St[𝐾 \ 𝐾 ′], 𝑓 | [𝜏 ] : St[𝐾 ′] → R𝑚 is locally Lipschitz, where 𝑓 | [𝜏 ] (𝜎) ≜ 𝑓 (𝜎 ⊕ 𝜏).
Although differentiability does not imply local Lipschitzness, continuous differentiability does. Since
most differentiable functions used in practice are continuously differentiable, asking for (R2’) and (R3’)
amounts to relaxing the requirements of (R2) and (R3) in practice.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:14

Wonyeol Lee, Xavier Rival, and Hongseok Yang

We choose local Lipschitzness as an alternative to differentiability in (R2) and (R3) for two main
reasons. First, local Lipschitzness is satisfied by most functions used in practice, which can even be
non-differentiable (e.g., ReLU). This would allow us to validate the unbiasedness of the SPGE for more
programs, even when they use non-differentiable functions. Second, using local Lipschitzness in (R2)
and (R3) does not break the results given in §4.2, even though local Lipschitzness is more practically
permissive than differentiability, as we explained above. To give the reader an intuition on why, we
point out that there are three places where differentiability in (R2) and (R3) has been used crucially
in the results: when we ensure that the gradients in the RHS of Eq. (10) are well-defined, Eq. (10)
holds, and (R5) holds under mild conditions. In these three places, we have used the next properties of
differentiability, respectively: differentiable functions (i) have well-definedness gradients, (ii) satisfy
the chain rule, and (iii) allow the interchange between integration and differentiation under mild
conditions (Theorem D.2). What we prove or observe is that local Lipschitzness satisfies a weaker
version of the three properties that are still strong enough to prove the results in §4.2: locally-Lipschitz
functions (i’) have well-defined gradients almost everywhere, (ii’) satisfy the chain rule almost
everywhere in restricted settings (Lemma E.1), and (iii’) allow the interchange between integration
and differentiation under the same conditions considered in (iii) (Theorem E.2). Based on these results,
we prove that the results in §4.2 still hold even when (R2) and (R3) are replaced by (R2’) and (R3’).
See Theorem E.3 for the precise statement that uses local Lipschitzness instead of differentiability.
As local Lipschitzness property has wider coverage than differentiability in practice while ensuring
that the results in §4.2 remain valid, our implementation and experiments consider the option of
using (R2’) and (R3’) as well as that of using (R2) and (R3); see §7 for details. But until then, we will
consider just the latter option for the sake of brevity.

5 PROGRAM ANALYSIS FOR SMOOTHNESS
Recall that our goal is to develop an algorithm for the SPGE variable-selection problem in Defini-
tion 2.1, which asks for finding a large set 𝑆 of random variables with a certain property when given
a model 𝑐𝑚, a guide 𝑐𝑔, and a reparameterisation plan 𝜋. When rephrased using the terminologies
that we covered so far, finding such an 𝑆 amounts to finding a restriction 𝜋0 of the given 𝜋 such
that (𝑐𝑚, 𝑐𝑔, 𝜋0) satisfies the requirements in Theorem 4.4. Thus, the key for developing a desired
algorithm for the problem lies in constructing an automatic method for proving that the requirements
in Theorem 4.4, in particular, the smoothness requirements (R2) and (R3) are met. In this section,
we propose a program analysis for smoothness properties, which can help find 𝜋0 that meets the
requirements (R2) and (R3), and which, together with the optimiser in the next section, leads to an
algorithm for solving the SPGE variable-selection problem.

We first define a parametric abstraction for smoothness properties (§5.1). We then describe a
program analysis based on this abstraction and prove the soundness of the analysis (§5.2). We finally
instantiate the analysis to differentiability and local Lipschitzness (§5.3). The results in this section
are not limited to PPLs, but are applicable to general imperative programming languages.

5.1 Parametric abstraction for smoothness properties
At a high level, our parametric abstraction for smoothness properties is built out of two components.
The first is a predicate over commands that expresses a target smoothness property but in a condi-
tional form. The predicate is parameterised by two sets of variables, 𝐾 for the input variables and
𝐿 for the output variables. Intuitively, the predicate holds for a command if conditioning the input
variables outside of 𝐾 to any fixed values and varying only the ones in 𝐾 makes the command a
smooth function on the output variables in 𝐿. Our program analysis tracks a conditional smoothness
property formalised by this predicate, and in so doing, it identifies a smooth part of a given command,
even when the command fails to be so with respect to some variables.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:15

The second component is also a predicate over commands, but it deals with dependency, instead of
smoothness. It is again parameterised by 𝐾 and 𝐿, and expresses that to compute the output variables
in 𝐿, a command accesses only the input variables in 𝐾. Our program analysis tracks dependency
formalised by this predicate, so as to achieve high precision, especially when handling sequential
composition. To see this, imagine that we want to check the differentiability of a sequence 𝑐; 𝑐 ′. A
natural approach is to use the chain rule. If the dependency-tracking part of our analysis is missing,
in order to establish that the output on a variable 𝑣 by the sequence is differentiable in an input
variable 𝑢, the analysis should show the output on 𝑣 by the second command 𝑐 ′ is differentiable
in all variables, and the output on any variable by the first command 𝑐 is differentiable in 𝑢. This
requirement on 𝑐 and 𝑐 ′ is too strong. Often, 𝑐 ′ uses only a small number of variables to compute
𝑣, and it is sufficient to require that just on those used variables, 𝑐 should be differentiable in 𝑢.
Similarly, 𝑐 commonly updates only a small number of variables using 𝑢, and it is enough to require
that just in those 𝑢-dependent variables, the second command 𝑐 ′ is differentiable when computing
the output 𝑣. The dependency-tracking part lets our analysis carry out such reasoning and achieve
better precision. Formally, this means our analysis uses a version of reduced product [11] between
dependency analysis and the analysis that tracks the target smoothness property.

We now formally describe each of these components as well as their combination.

Family of smoothness predicates. Our program analysis assumes that a target smoothness
5.1.1
property is specified in terms of a family of predicates, 𝜙 = (𝜙𝐾,𝐿 : 𝐾, 𝐿 ⊆ Var), where 𝜙𝐾,𝐿 is a set
of partial functions from St[𝐾] to St[𝐿] (i.e., 𝜙𝐾,𝐿 ⊆ [St[𝐾] ⇀ St[𝐿]]).

Example 5.1 (Differentiability). In the instantiation of our program analysis for differentiability,
we use the family 𝜙 (𝑑) where for all 𝐾, 𝐿 ⊆ Var, a partial function 𝑓 : St[𝐾] → St[𝐿] belongs to 𝜙 (𝑑)
𝐾,𝐿
□
if and only if (i) dom(𝑓 ) is open and (ii) 𝑓 is (jointly) differentiable in its domain.

At first, one may wonder why we use a family of 𝜙𝐾,𝐿 predicates instead of a single predicate 𝜙0
over [St → St⊥]. The reason is that, as mentioned above, the analysis aims at a conditional variant of
the traditional notion of smoothness. For instance, instead of checking that a function 𝑓 : St → St⊥ is
differentiable on St\𝑓 −1({⊥}), the analysis proves differentiability conditioned on certain variables be-
ing fixed: if we fix the input variables in Var\𝐾 and vary just those in 𝐾 in the initial state, and look at the
output variables in 𝐿 only, then the function 𝑓 becomes differentiable, although it might not be so when
all input/output variables are considered. To express this, we need the whole family of 𝜙𝐾,𝐿 predicates.
This notion of conditional differentiability is similar to, but not the same as, so called partial
differentiability. Partial differentiability in 𝐾 says that, for every 𝑣 ∈ 𝐾, if we fix all the input vari-
ables except 𝑣, including those in 𝐾 \ {𝑣 }, and consider the output variables in 𝐿 only, 𝑓 becomes
differentiable. As we will show in Remark 4, the set of partially-differentiable functions is not closed
under a certain operator, but we need the closure to ensure that our program analysis is sound. Our
conditional differentiability does not suffer from this issue.

Smoothness abstraction. Based on the family 𝜙, we build a predicate Φ that captures the
5.1.2
smoothness of commands. The Φ constrains functions from St to St⊥, unlike 𝜙𝐾,𝐿. For 𝐾, 𝐿 ⊆ Var
with 𝐾 ⊇ 𝐿, define 𝜋𝐾,𝐿 to be the projection from St[𝐾] to St[𝐿].

Definition 5.2. The smoothness abstraction Φ is the predicate over a function 𝑓 ∈ [St → St⊥] and
variable sets 𝐾, 𝐿 ⊆ Var. It is satisfied by (𝑓 , 𝐾, 𝐿) if for all 𝜏 ∈ St[Var \ 𝐾], the predicate 𝜙𝐾,𝐿 holds
for the following partial function 𝑔 : St[𝐾] ⇀ St[𝐿]: dom(𝑔) ≜ {𝜎 ∈ St[𝐾] | 𝑓 (𝜎 ⊕ 𝜏) ∈ St} and
𝑔(𝜎) ≜ (𝜋Var,𝐿 ◦ 𝑓 )(𝜎 ⊕ 𝜏) for 𝜎 ∈ dom(𝑔). We denote the satisfaction of Φ by |= Φ(𝑓 , 𝐾, 𝐿).
□
Note that the function 𝑔 is constructed from 𝑓 by fixing the Var \ 𝐾 part of the input state to 𝜏, and
looking at only the 𝐿 part of the output. This construction is precisely the one used in the informal

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:16

Wonyeol Lee, Xavier Rival, and Hongseok Yang

definition of conditional differentiability described above, and its use reflects the fact that our program
analysis attempts to prove a conditional smoothness property.

5.1.3 Dependency abstraction. Our abstract domain has a component for tracking dependency
between input-output variables. Dependency here means that a given input variable is used for
computing a given output variable. We define a predicate Δ that has a similar format as Φ. Intuitively,
Δ(𝑓 , 𝐾, 𝐿) holds if and only if the 𝐿 part of the output of 𝑓 depends at most on the 𝐾 part of the input
to 𝑓 . To define Δ formally, for 𝐾 ⊆ Var, let ∼𝐾 be the following equivalence relation over states:
𝜎 ∼𝐾 𝜎 ′ if and only if 𝜎 (𝑣) = 𝜎 ′(𝑣) for all 𝑣 ∈ 𝐾.

Definition 5.3. The dependency abstraction Δ is the predicate on 𝑓 ∈ [St → St⊥] and 𝐾, 𝐿 ⊆ Var
that holds if for all 𝜎, 𝜎 ′ ∈ St with 𝜎 ∼𝐾 𝜎 ′, we have (𝑓 (𝜎) ∈ St ⇐⇒ 𝑓 (𝜎 ′) ∈ St) and (𝑓 (𝜎) ∈
St =⇒ 𝑓 (𝜎) ∼𝐿 𝑓 (𝜎 ′)). We denote the satisfaction of Δ by |= Δ(𝑓 , 𝐾, 𝐿).
□
5.1.4 Combined abstraction. We bring together the two abstractions that we just defined, and
construct the final abstract domain D♯ used by our program analysis.

Intuitively, each element of D♯ is a predicate on a function 𝑓 ∈ [St → St⊥] expressed by the
conjunction of the following form: (cid:211)𝑚
𝑖=1 Φ(𝑓 , 𝐾𝑖, 𝐿𝑖 ) ∧ (cid:211)𝑛
𝑗 ). A direct but naive way of
implementing this intuition is to let D♯ be the collection of all the constraints of this form, but it
permits too many constraints and leads to a costly program analysis. We take a more economical
alternative that further restricts the allowed form of the constraints. The alternative requires that the
conjunction from above should be constructed out of two mappings 𝑝 and 𝑑 from output variables
to input variable sets, and a set 𝑉 of input variables. The 𝑝 describes smoothness, and the 𝑑 and 𝑉
dependency. They together encode the constraint
(cid:219)

𝑗=1 Δ(𝑓 , 𝐾 ′

𝑗, 𝐿′

(cid:219)

Φ(𝑓 , 𝑝 (𝑣), {𝑣 }) ∧

Δ(𝑓 , 𝑑 (𝑢), {𝑢}) ∧ Δ(𝑓 , 𝑉 , ∅).

𝑣 ∈Var

𝑢 ∈Var

Thus, a function 𝑓 ∈ [St → St⊥] satisfies the constraint encoded by 𝑝, 𝑑, and 𝑉 if (i) for every output
variable 𝑣, when we fix the values of all the input variables outside of 𝑝 (𝑣), the (partial) function
𝜎 ↦−→ 𝑓 (𝜎)(𝑣) is smooth (e.g., differentiable); (ii) for every output variable 𝑢, the (partial) function
𝜎 ↦−→ 𝑓 (𝜎)(𝑢) does not access any variable outside of 𝑑 (𝑢) to compute the value of 𝑢; and (iii) the
values of input variables in 𝑉 determine whether 𝑓 returns ⊥ or not.

Definition 5.4. The abstract domain D♯ consists of triples (𝑝, 𝑑, 𝑉 ) ∈ [Var → P (Var)]2 × P (Var),
called abstract state, such that 𝑝 (𝑣) ⊇ 𝑑 (𝑣)𝑐 and 𝑑 (𝑣) ⊇ 𝑉 for all 𝑣 ∈ Var, where −𝑐 is the standard
operation for set complement. That is,

D♯ ≜ {(𝑝, 𝑑, 𝑉 ) ∈ [Var → P (Var)]2 × P (Var) | 𝑝 (𝑣) ⊇ 𝑑 (𝑣)𝑐 and 𝑑 (𝑣) ⊇ 𝑉 for all 𝑣 ∈ Var}.
We order abstract states as follows: (𝑝, 𝑑, 𝑉 ) ⊑ (𝑝 ′, 𝑑 ′, 𝑉 ′) if and only if 𝑉 ⊆ 𝑉 ′ and for all 𝑣 ∈ Var,
𝑝 (𝑣) ⊇ 𝑝 ′(𝑣) and 𝑑 (𝑣) ⊆ 𝑑 ′(𝑣). These abstract states are concretised by 𝛾 : D♯ → P ([St → St⊥]):
𝑓 ∈ 𝛾 (𝑝, 𝑑, 𝑉 ) ⇐⇒ |= Δ(𝑓 , 𝑉 , ∅), |= Φ(𝑓 , 𝑝 (𝑣), {𝑣 }), and |= Δ(𝑓 , 𝑑 (𝑣), {𝑣 }) for all 𝑣 ∈ Var. □ (11)
Note that the definition of D♯ contains two conditions. The first condition 𝑝 (𝑣) ⊇ 𝑑 (𝑣)𝑐 comes
from our assumption that if a function does not depend on a variable 𝑢, it is smooth in 𝑢. This and
other assumptions of the analysis will be explained shortly in §5.2.2. The other condition 𝑑 (𝑣) ⊇ 𝑉
originates from the relationship that if Δ(𝑓 , 𝐾, {𝑣 }) holds, so does Δ(𝑓 , 𝐾, ∅).

Example 5.5 (Differentiability). Consider the setup of Example 5.1 and the program 𝑐 ≡ (𝑦 :=
𝑥 ∗ 𝑥; if (𝑥 ≥ 0) {𝑠 := 1} else {𝑠 := −1}). Let (𝑝, 𝑑, 𝑉 ) be the smallest abstract state that describes
the program. In this program, 𝑠 is not differentiable in 𝑥, but 𝑦 is. So, 𝑝 (𝑠) = Var \ {𝑥 } ⊇ {𝑦, 𝑠} and
𝑝 (𝑦) = Var ⊇ {𝑥, 𝑦, 𝑠}. Note that 𝑝 (𝑠) contains the input variables 𝑠 and 𝑦 because by not depending

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:17

on those input variables, the output 𝑠 is differentiable in those variables. For the dependency part,
□
we have 𝑑 (𝑠) = 𝑑 (𝑦) = {𝑥 } and 𝑉 = ∅.

5.2 Parametric static program analysis
Our program analysis is based on abstract interpretation [10], and computes an approximation of
of a given command 𝑐 using the abstract domain D♯. We formalise this
the concrete semantics
♯ ∈ D♯ by induction on the structure
computation by the abstract semantics of 𝑐, which defines
of commands, and over-approximates

in the sense of the concretization 𝛾 in Eq. (11).

𝑐
(cid:74)

𝑐
(cid:74)

(cid:75)

(cid:75)

𝑐
(cid:74)

(cid:75)

𝑐
(cid:74)

♯. The overall structure of the
5.2.1 Analysis definition. Fig. 3 shows the abstract semantics of
semantics follows the standard compositional semantics of an imperative language. For instance, the
abstract semantics of sequential composition is defined in terms of those of constituent commands,
and the semantics of a loop is the least fixed point of a monotone operator over D♯. However, the
specifics of the semantics include non-standard details, and we spell them out by going through the
♯.
defining clauses of
𝑐
(cid:74)
(cid:75)
The definition of
skip
(cid:75)
(cid:74)

♯ formalises the effect of skip on smoothness and dependency. The def-
inition says that skip computes each output variable 𝑣 in a smooth manner in all input variables,
and in so doing, it creates the dependency between the variable 𝑣 to itself at the input state. The 𝑉
part of

♯ is the empty set since skip always terminates.

(cid:75)

The next case is 𝑥 := 𝑒. Its abstract semantics records the smoothness and dependency information
of the updated variable 𝑥 by analysing the expression 𝑒. For the smoothness part, the semantics
♯ that computes an under-approximation of the set of variables in which
invokes the subroutine
𝑒
♯ computes the set of all the free variables
(cid:77)
(cid:76)
the expression 𝑒 is smooth. For the dependency part,
of 𝑒 so as to get an over-approximation of all variables that may affect the value of 𝑒. For variables
other than 𝑥,

♯ behaves like

𝑥 := 𝑒
(cid:74)

♯.

(cid:75)

skip
(cid:74)

(cid:75)

(cid:75)

(cid:75)

𝑐 ′
(cid:74)

♯ and 𝑑 ′ from

The abstract semantics of a sequence 𝑐; 𝑐 ′ composes those of the sub-commands 𝑐 and 𝑐 ′. It uses
the liftings 𝑓∪, 𝑓∩ : P (Var) → P (Var) of functions 𝑓 of type Var → P (Var), which are defined
♯ constructs
as follows: 𝑓∪(𝑉 ) ≜ (cid:208)𝑣 ∈𝑉 𝑓 (𝑣) and 𝑓∩ (𝑉 ) ≜ (cid:209)𝑣 ∈𝑉 𝑓 (𝑣). The abstract semantics
𝑐; 𝑐 ′
♯ after lifting the former. Note
(cid:74)
the dependency part 𝑑 ′′ by composing 𝑑 from
𝑐
(cid:75)
(cid:74)
the inclusion of the set 𝑉 in the definition of 𝑑 ′′. This is to account for the case that 𝑑 ′(𝑣) in the
definition is the empty set; in that case, 𝑑∪ (𝑑 ′(𝑣)) is empty as well and does not have any information
♯ is more involved, and implements the intuition
about termination. The smoothness part 𝑝 ′′ of
𝑐; 𝑐 ′
(cid:74)
described briefly in §5.1. In order to conclude that input variables in 𝑉0 together smoothly affect an
output variable 𝑣 in the computation of 𝑐; 𝑐 ′, the 𝑝 ′′ considers the intermediate state after the first
command 𝑐, and forms two groups of variables at that intermediate state: 𝑑 ′(𝑣) and 𝑝 ′(𝑣)𝑐 . Note that
the desired smoothness property for the input variables in 𝑉0 and the output variable 𝑣 may fail if
the first command 𝑐 uses some variable 𝑢0 ∈ 𝑉0 non-smoothly to update a variable 𝑢 ′
0 in 𝑑 ′(𝑣), or it
1 ∈ 𝑝 ′(𝑣)𝑐 . In the former case, the
uses some variable 𝑢1 ∈ 𝑉0 to compute the value of a variable 𝑢 ′
non-smoothness of 𝑐 causes an issue, and in the latter case, the non-smoothness of 𝑐 ′ causes an issue.
The 𝑝 ′′ collects the input variables that avoid these two failure modes and also do not influence the
termination of the sequence. As we show in our soundness theorem, doing so is sufficient because
it amounts to using a version of chain rule for the target smoothness property.

(cid:75)

𝑥 := 𝑒
(cid:74)

(cid:75)

skip
(cid:75)

(cid:74)

The abstract semantics of an if command conservatively assumes that any variable in its condition
𝑏 may affect the value of any output variable (by influencing whether the true or false branch of the
command gets executed) and this influence is potentially non-smooth. For every output variable
𝑣, the smooth set 𝑝 ′′(𝑣) for the if command implements this assumption by excluding free variables
in 𝑏, and the computed dependency set does the same but this time by including free variables in 𝑏.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:18

Wonyeol Lee, Xavier Rival, and Hongseok Yang

♯ ≜ (𝜆𝑣.Var, 𝜆𝑣.{𝑣 }, ∅),
♯ ≜ ((𝜆𝑣. 𝑣 ≡ 𝑥 ?
𝑒
(cid:76)
♯ ≜ let (𝑝, 𝑑, 𝑉 ) ≜

skip
(cid:75)
(cid:74)
𝑥 := 𝑒
(cid:74)
𝑐; 𝑐 ′
(cid:74)

(cid:75)

♯ and (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜
where 𝑝 ′′(𝑣) ≜ (𝑉 ∪ 𝑝∩ (𝑑 ′(𝑣))𝑐 ∪ 𝑑∪(𝑝 ′(𝑣)𝑐 ))𝑐,

(cid:75)
(cid:75)
(cid:75)
𝑑 ′′(𝑣) ≜ 𝑉 ∪ 𝑑∪ (𝑑 ′(𝑣)), and 𝑉 ′′ ≜ 𝑉 ∪ 𝑑∪ (𝑉 ′),
𝑐 ′
𝑐
(cid:74)
(cid:74)

♯ ≜ let (𝑝, 𝑑, 𝑉 ) ≜
where 𝑝 ′′(𝑣) ≜ fv(𝑏)𝑐 ∩ 𝑝 (𝑣) ∩ 𝑝 ′(𝑣),

♯ and (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

♯ : Var), (𝜆𝑣. 𝑣 ≡ 𝑥 ? fv(𝑒) : {𝑣 }), ∅),
(cid:77)
𝑐
(cid:74)

𝑐 ′
(cid:74)

♯ in (𝑝 ′′, 𝑑 ′′, 𝑉 ′′)

♯ in (𝑝 ′′, 𝑑 ′′, 𝑉 ′′)

(cid:75)

(cid:75)
𝑑 ′′(𝑣) ≜ fv(𝑏) ∪ 𝑑 (𝑣) ∪ 𝑑 ′(𝑣), and 𝑉 ′′ ≜ fv(𝑏) ∪ 𝑉 ∪ 𝑉 ′,
♯ in fix 𝐹 ♯

while 𝑏 {𝑐}

♯ ≜ let (𝑝, 𝑑, 𝑉 ) ≜
where 𝐹 ♯ (𝑝0, 𝑑0, 𝑉0) ≜ (𝑝 ′, 𝑑 ′, 𝑉 ′),
(cid:75)
𝑝 ′(𝑣) ≜ fv(𝑏)𝑐 ∩ (𝑉 ∪ 𝑝∩ (𝑑0(𝑣))𝑐 ∪ 𝑑∪(𝑝0(𝑣)𝑐 ))𝑐,
𝑑 ′(𝑣) ≜ fv(𝑏) ∪ (𝑉 ∪ 𝑑∪ (𝑑0(𝑣))) ∪ {𝑣 }, and 𝑉 ′ ≜ fv(𝑏) ∪ (𝑉 ∪ 𝑑∪(𝑉0)),

𝑐
(cid:74)

(cid:75)

(cid:75)

(cid:74)

if 𝑏 {𝑐} else {𝑐 ′}

(cid:74)

𝑥 := sam(𝑛, distN (𝑒1, 𝑒2), 𝜆𝑦.𝑒 ′)
(cid:74)

♯≜ (𝑝, 𝑑, ∅)

for 𝑛 = name(𝛼, 𝑟 ) with 𝑟 ∈ R

(cid:75)

♯

♯

♯

𝑝 (𝑣) ≜

where 𝜇 ≜ create_name(𝛼, 𝑟 ),







𝑒 ′[𝜇/𝑦]
(cid:76)
(cid:77)
pdfN (𝜇; 𝑒1, 𝑒2)
(cid:76)
cnt𝜇 + 1
(cid:76)
(cid:77)
Var
fv(𝑒 ′[𝜇/𝑦])
{𝜇} ∪ fv(𝑒1) ∪ fv(𝑒2)
{𝑣 }
♯ ≜ (𝑝, 𝑑, ∅)

and 𝑑 (𝑣) ≜

(cid:77)

if 𝑣 ∈ {𝑥, val𝜇 }
if 𝑣 ≡ pr 𝜇
if 𝑣 ≡ cnt𝜇
otherwise,

if 𝑣 ∈ {𝑥, val𝜇 }
if 𝑣 ≡ pr 𝜇
otherwise,

for 𝑛 = name(𝛼, 𝑒) with 𝑒 ∉ R
♯

♯

♯

♯

(cid:77)

(cid:77)



where 𝑝 (𝑣) ≜

fv(𝑒)𝑐 ∩ (cid:209)𝜇=(𝛼,_) ∈Name
𝑒 ′[𝜇/𝑦]

fv(𝑒)𝑐 ∩
(cid:76)
𝑒 ′[𝜇/𝑦]
fv(𝑒)𝑐 ∩
(cid:76)
(cid:77)
pdfN (𝜇; 𝑒1, 𝑒2)
fv(𝑒)𝑐 ∩
(cid:76)

cnt𝜇 + 1
(cid:77)
(cid:76)
Var
fv(𝑒) ∪ (cid:208)𝜇=(𝛼,_) ∈Name fv(𝑒 ′[𝜇/𝑦])

fv(𝑒) ∪ {val𝜇 } ∪ fv(𝑒 ′[𝜇/𝑦])
fv(𝑒) ∪ {pr 𝜇, 𝜇} ∪ fv(𝑒1) ∪ fv(𝑒2)

fv(𝑒) ∪ {cnt𝜇 }
{𝑣 }
♯ ≜ (𝑝, 𝑑, ∅)
where 𝑝 (𝑣) ≜ (𝑣 ≡ like) ?

and 𝑑 (𝑣) ≜



(cid:75)

♯ : Var,
and 𝑑 (𝑣) ≜ (𝑣 ≡ like) ? {like} ∪ fv(𝑒1) ∪ fv(𝑒2) : {𝑣 }.

like × pdfN (𝑟 ; 𝑒1, 𝑒2)
(cid:76)

(cid:77)

if 𝑣 ≡ 𝑥
if 𝑣 ≡ val𝜇 for 𝜇 = (𝛼, _)
if 𝑣 ≡ pr 𝜇 for 𝜇 = (𝛼, _)
if 𝑣 ≡ cnt𝜇 for 𝜇 = (𝛼, _)
otherwise,
if 𝑣 ≡ 𝑥
if 𝑣 ≡ val𝜇 for 𝜇 = (𝛼, _)
if 𝑣 ≡ pr 𝜇 for 𝜇 = (𝛼, _)
if 𝑣 ≡ cnt𝜇 for 𝜇 = (𝛼, _)
otherwise,

obs(distN (𝑒1, 𝑒2), 𝑟 )

(cid:74)

Fig. 3. Abstract semantics of commands defining

♯ ∈ D♯.

𝑐
(cid:74)

(cid:75)

The abstract semantics of a loop computes the least fixed point of a monotone operator 𝐹 ♯ : D♯ →
D♯ using the standard Kleene iteration. The operator 𝐹 ♯ describes the effect of one iteration of the
loop, and it is derived from the standard loop unrolling and our abstract semantics of sequencing
and the if command.

The abstract semantics of an observe command obs(distN(𝑒1, 𝑒2), 𝑟 ) uses the fact that the com-
mand has the same concrete semantics as the assignment like := like × pdfN(𝑟 ; 𝑒1, 𝑒2), where pdfN

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

𝑥 := sam(𝑛, distN (𝑒1, 𝑒2), 𝜆𝑦.𝑒 ′)
(cid:74)

(cid:75)

Smoothness Analysis and Selective Reparameterisation

0:19

is the density function of the normal distribution. The semantics computes (𝑝, 𝑑, 𝑉 ) according to
that of the assignment, which we explained earlier.

The final case is the abstract semantics of a sample command. The semantics performs a case
analysis on the first argument of sam. If it is a constant expression not involving any variables, then
the abstract semantics constructs the name 𝜇 of the sampled random variable, and updates 𝑝, 𝑑, and
𝑉 according to the concrete semantics of the command. Otherwise, the abstract semantics acknowl-
edges that the precise name 𝜇 of the random variable cannot be known statically, and performs so
called weak update by joining two pieces of information before and after the update of the command
in the concrete semantics. Note that the abstract semantics does not require the third argument
of sam should be the identity function. The ability of dealing with a general function in the third
argument is needed since our analysis is intended to be applied to programs after the transformation
of the SPGE, which may introduce such an argument.

The abstract semantics is well-defined under the following relatively weak assumption:

Assumption 1 (Expression analysis and free variables).

♯ ⊇ fv(𝑒)𝑐 for all expressions 𝑒.
This assumption is satisfied by the instantiations of the semantics with differentiability and local
Lipschitzness, which are used in our implementation. It will be assumed in the rest of the paper.

𝑒
(cid:76)

(cid:77)

♯

𝑐
(cid:74)

(cid:75)

Theorem 5.6. If Assumption 1 holds, then for all commands 𝑐, we have

let (𝑝, 𝑑, 𝑉 ) ≜

, we have 𝑝 (𝑣) ⊇ 𝑑 (𝑣)𝑐 and 𝑑 (𝑣) ⊇ 𝑉 for all variables 𝑣 ∈ Var.

(cid:75)

𝑐
(cid:74)

♯ ∈ D♯, that is, when we

Example 5.7 (Differentiability). Consider the differentiability property and the example program of
Example 5.5. Let (𝑝1, 𝑑1, 𝑉1) and (𝑝2, 𝑑2, 𝑉2) be the results of analysing the first assignment command
𝑦 := 𝑥 ∗ 𝑥 and the following if command of the program. Then,
(𝑝1, 𝑑1, 𝑉1) = (𝜆𝑣.Var, 𝜆𝑣. (𝑣≡𝑦) ? {𝑥 } : {𝑣 }, ∅), (𝑝2, 𝑑2, 𝑉2) = (𝜆𝑣.{𝑥 }𝑐, 𝜆𝑣. (𝑣≡𝑠) ? {𝑥 } : {𝑥, 𝑣 }, {𝑥 }).
Let (𝑝, 𝑑, 𝑉 ) be the analysis result for the entire program. Then,

𝑝 (𝑣) = (cid:0)𝑉1 ∪ (𝑝1)∩ (𝑑2(𝑣))𝑐 ∪ (𝑑1)∪ (𝑝2(𝑣)𝑐 )(cid:1)𝑐

= 𝑑1(𝑥)𝑐 = {𝑥 }𝑐,

𝑉 = 𝑉1 ∪ (𝑑1)∪(𝑉2) = {𝑥 }.

Also, 𝑑 (𝑣) = 𝑉1 ∪ (𝑑1)∪ (𝑑2(𝑣)) = (if (𝑣 ≡ 𝑠) then {𝑥 } else {𝑥, 𝑣 }). As shown in Fig. 3, the variable 𝑥
that may affect the condition expression of the if command is removed from the smoothness sets,
and 𝑝 (𝑠) = 𝑝 (𝑦) = {𝑥 }𝑐 . Note that this result is conservative with respect to 𝑦.
□
5.2.2 Analysis soundness and assumptions. The soundness of our analysis states that for every
command 𝑐, its abstract semantics
via 𝛾:
♯). The soundness is conditioned on Assumption 1 and six new assumptions. Most of
𝑐
(cid:74)
these new assumptions are concerned with the predicate family for the target smoothness property
𝜙 = (𝜙𝐾,𝐿 : 𝐾, 𝐿 ⊆ Var), and say that certain canonical operators are smooth according to 𝜙 so that
using them in the abstract semantics should not cause an issue. In this subsection, we present the
six assumptions one by one, and sketch how those assumptions are related to the soundness.

♯ ∈ D♯ over-approximates the concrete semantics

∈ 𝛾 (

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

We start with the assumption that the analysis of each expression

set of variables in which the evaluation of 𝑒 is smooth.

♯ under-approximates the

𝑒
(cid:76)

(cid:77)

Assumption 2 (Expression analysis soundness). For all expressions 𝑒, variables 𝑥, subsets
and 𝐿 = {𝑥 }, if we let 𝑔 : St[𝐾] → St[𝐿]

𝐾, 𝐿 ⊆ Var, and states 𝜏 ∈ St[Var \ 𝐾] such that 𝐾 =
be the function defined by 𝑔(𝜎) ≜ [𝑥 ↦→
This assumption is used in our soundness argument whenever the abstract semantics uses
computing smoothness information about an expression 𝑒.

(𝜎 ⊕ 𝜏)], the function 𝑔 satisfies 𝜙𝐾,𝐿 (i.e., 𝑔 ∈ 𝜙𝐾,𝐿).
𝑒
(cid:76)

♯ for

𝑒
(cid:74)

𝑒
(cid:76)

(cid:75)

(cid:77)

(cid:77)

♯

The next two assumptions assert the smoothness of the standard operators on the product spaces.

Assumption 3 (Projection). For all 𝐾, 𝐿 ⊆ Var with 𝐾 ⊇ 𝐿, the projection 𝜋𝐾,𝐿 satisfies 𝜙𝐾,𝐿.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:20

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Assumption 4 (Pairing). For all 𝐾, 𝐿, 𝑀 ⊆ Var with 𝐿 ∩ 𝑀 = ∅, if 𝑓 ∈ 𝜙𝐾,𝐿 and 𝑔 ∈ 𝜙𝐾,𝑀 ,
we have ⟨𝑓 , 𝑔⟩ ∈ 𝜙𝐾,𝐿∪𝑀 , where ⟨𝑓 , 𝑔⟩ is the pairing of two partial functions: ⟨𝑓 , 𝑔⟩(𝜎) ≜ if (𝜎 ∈
dom(𝑓 ) ∩ dom(𝑔)) then 𝑓 (𝜎) ⊕ 𝑔(𝜎) else undefined.
Note that St[𝐿 ∪ 𝑀] is isomorphic to St[𝐿] × St[𝑀], the product space that we referred to above.
The assumptions say that the projection is smooth, and the pairing of smooth functions is smooth.
Our analysis uses Assumption 3 to deal with variables not modified by a command. For instance,
when analysing an assignment 𝑥 := 𝑒, the analysis uses Assumption 3 and concludes that on every
output variable 𝑣 other than 𝑥, the assignment is smooth in all the input variables. Assumption 4 is
used to justify the handling of a sequence 𝑐; 𝑐 ′ by our analysis, in particular, the part that the analysis
combines smoothness information over multiple output variables after the first command 𝑐.

The projection and pairing assumptions are about how shrinking and expanding output variables
affect the target smoothness property. The next restriction assumption is about shrinking the input
|= Φ(𝑓 , 𝐾, 𝐿), and is used in the abstract
variables. It validates the weakening of the 𝐾 part of
semantics of 𝑐; 𝑐 ′ (and other composite commands).

Assumption 5 (Restriction). For all 𝐾, 𝐾 ′, 𝐿 ⊆ Var with 𝐾 ⊇ 𝐾 ′, and 𝜏 ∈ St[𝐾 \ 𝐾 ′], if 𝑓 ∈ 𝜙𝐾,𝐿,

then we have 𝑔 ∈ 𝜙𝐾 ′,𝐿, where 𝑔(𝜎) ≜ if (𝜎 ⊕ 𝜏 ∈ dom(𝑓 )) then 𝑓 (𝜎 ⊕ 𝜏) else undefined.

The following assumption says that the function composition preserves smoothness. It is related

to the chain rule for differentiation, and used to justify the abstract semantics of a sequence 𝑐; 𝑐 ′.

Assumption 6 (Composition). For all 𝐾, 𝐿, 𝑀 ⊆ Var, if 𝑓 ∈ 𝜙𝐾,𝐿 and𝑔 ∈ 𝜙𝐿,𝑀 , we have𝑔◦𝑓 ∈ 𝜙𝐾,𝑀 ,
where 𝑔 ◦ 𝑓 is the standard composition of two partial functions: (𝑔 ◦ 𝑓 )(𝜎) ≜ if (𝜎 ∈ dom(𝑓 ) ∧ 𝑓 (𝜎) ∈
dom(𝑔)) then 𝑔(𝑓 (𝜎)) else undefined.

The final assumption lets the analysis infer smoothness information about the completely-

undefined function. It is used to justify the handling of loops by our analysis.

Assumption 7 (Strictness). For all 𝐾, 𝐿 ⊆ Var, we have (𝜆𝜎 ∈ St[𝐾].undefined) ∈ 𝜙𝐾,𝐿.
Theorem 5.8 (Soundness). If Assumptions 1–7 hold, the analysis computes the sound abstraction

of the concrete semantics of commands in the following sense: for all commands 𝑐,

♯).

(cid:75)

𝑐
(cid:74)

𝑐
(cid:74)

∈ 𝛾 (
Remark 2. A standard method for proving a property of a loop or more generally a recursively
defined function is so called Scott induction. In this method, we view a property as a set T of state
transformers and a loop as the least fixed point of a continuous function 𝐹 on state transformers.
Then, we prove the three conditions: (i) T contains the least state transformer, (ii) it is closed under
the least upper bound of any increasing sequence of state transformers, and (iii) T is preserved by 𝐹 .
The first and second conditions are called strictness and admissibility, respectively, and these three
conditions imply that the least fixed point of 𝐹 belongs to T .

(cid:75)

Our soundness proof for the loop case deviates slightly from this standard method. If it followed the
method instead, we would need, in addition to the strictness assumption, the following assumption,
which corresponds to the second admissibility condition:

Assumption 8 (Admissibility). Let 𝐾, 𝐿 ⊆ Var, and order partial functions in [St[𝐾] ⇀ St[𝐿]]
by the inclusion of the graphs of partial functions. Then, for every increasing sequence {𝑓𝑛 : St[𝐾] ⇀
St[𝐿]}𝑛 ∈N (i.e., the graph of 𝑓𝑛+1 includes that of 𝑓𝑛 for all 𝑛 ∈ N), if every 𝑓𝑛 satisfies 𝜙𝐾,𝐿, so does the
least upper bound 𝑓∞ of the sequence (defined by its graph being the union of the graphs of all 𝑓𝑛’s).
The inclusion of this admissibility assumption would, then, limit the applicability of our program
analysis, since some well-known smoothness properties, such as Lipschitz continuity and local

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:21

boundedness, do not satisfy the assumption, although they satisfy our five assumptions (Assump-
tions 3–7). On the plus side, the inclusion of the admissibility assumption could enable our analysis
to handle loops more accurately, possibly by tracking the impact of the boolean condition of each
loop on smoothness more precisely. Our soundness proof avoids the admissibility assumption by
exploiting the conservative handling of loop conditions by our analysis and showing intuitively that
□
it is sufficient to consider chains that converge in a finite number of steps.

5.3 Instantiations
Our program analysis requires that the family of smoothness predicates should satisfy Assump-
tions 3–7. Although these assumptions are violated by some smoothness properties, such as partial
differentiability and partial continuity, they are met by our leading example 𝜙 (𝑑) for differentiability
(Example 5.1), and also by the predicate family 𝜙 (𝑙) for local Lipschitzness, which is used in our
implementation. Recall the definitions of the predicate families 𝜙 (𝑑) and 𝜙 (𝑙) : for all 𝐾, 𝐿 ⊆ Var,
𝜙 (𝑑)
𝐾,𝐿 ≜ {𝑓 : St[𝐾] ⇀ St[𝐿] | dom(𝑓 ) is open and 𝑓 is (jointly) differentiable in its domain},
𝜙 (𝑙)
𝐾,𝐿 ≜ {𝑓 : St[𝐾] ⇀ St[𝐿] | dom(𝑓 ) is open, and for all 𝜎 ∈ dom(𝑓 ), there are 𝐶 > 0 and

an open 𝑂 ⊆ dom(𝑓 ) s.t. 𝜎 ∈ 𝑂 and ∥ 𝑓 (𝜎0) − 𝑓 (𝜎1)∥2 ≤ 𝐶 ∥𝜎0 − 𝜎1∥2 for all 𝜎0, 𝜎1 ∈ 𝑂 }.

Theorem 5.9. Both 𝜙 (𝑑) and 𝜙 (𝑙) satisfy Assumptions 3–7.

Remark 3. The requirement of open domain in 𝜙 (𝑑) is sometimes too constraining and hurts the
accuracy of the analysis. It can, however, be relaxed, and we can generalise 𝜙 (𝑑) to the following
predicate family 𝜙 (𝑑′) , which corresponds to the standard definition of differentiability on a manifold
with boundary in differential geometry [23, Chapter 2]:
𝜙 (𝑑′)
𝐾,𝐿 ≜ {𝑓 : St[𝐾] ⇀ St[𝐿] | for all 𝜎 ∈ dom(𝑓 ), there exist an open 𝑈 ⊆ St[𝐾] and 𝑔 : 𝑈 → St[𝐿]

such that 𝜎 ∈ 𝑈 , 𝑓 = 𝑔 on 𝑈 ∩ dom(𝑓 ), and 𝑔 is (jointly) differentiable}.

Note the weakening of open-domain requirement in 𝜙 (𝑑′) : the open domain 𝑈 in the above definition
does not have to be included in dom(𝑓 ). The family 𝜙 (𝑑′) satisfies Assumptions 3–7, and can lead to
a more permissive instantiation of our program analysis than the family 𝜙 (𝑑) , especially in handling
□
atomic commands, such as assignment, sample, and observe.

Remark 4. At this point, the reader might feel that Assumptions 3–7 are satisfied by nearly all
smoothness properties. This impression is not accurate. For instance, the composition assumption
does not hold for the notions of differentiability of partial functions formalised by the following
𝜙 (𝑑′′) and 𝜙 (pd) , nor for the partial continuity formalised by 𝜙 (pc) :

𝜙 (𝑑′′)
𝐾,𝐿 ≜ {𝑓 : St[𝐾] ⇀ St[𝐿] | 𝑓 is (jointly) differentiable in the interior of its domain},
𝜙 (pd)
𝐾,𝐿 ≜ {𝑓 : St[𝐾] ⇀ St[𝐿] | dom(𝑓 ) is open, and for all 𝑣 ∈ 𝐾, 𝑓 is partially differentiable in 𝑣 },
𝜙 (pc)
𝐾,𝐿 ≜ {𝑓 : St[𝐾] ⇀ St[𝐿] | dom(𝑓 ) is open, and for all 𝑣 ∈ 𝐾, 𝑓 is partially continuous in 𝑣 }.
Table 1 contains counterexamples that show the failure of the composition assumption for these
predicate families. In fact, when instantiated with these families, our program analysis is not sound.
□
The same table shows example programs and incorrect conclusions derived by our analysis.

6 ALGORITHM FOR THE SPGE VARIABLE-SELECTION PROBLEM
We now put together the results from §4 and §5 to formally define and soundly (yet approximately)
solve the SPGE variable-selection problem. We start with the formal definition of the problem:

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:22

Wonyeol Lee, Xavier Rival, and Hongseok Yang

𝜙
𝜙 (𝑑′′)
𝐾,𝐿
𝜙 (pd)
𝐾,𝐿

, 𝜙 (pc)
𝐾,𝐿

𝑓
𝑓 (𝑥) = 𝑥 2 defined on R
𝑓 (𝑥) = (𝑥, 𝑥) defined on R

𝑔
𝑔(𝑥) = 1[𝑥 >0] defined on [0, 1]
𝑔(𝑥, 𝑦) =

(cid:110) 𝑥𝑦/(𝑥 2 + 𝑦2) if (𝑥, 𝑦) ≠ (0, 0)
0

otherwise

defined on R2

Table 1. Failure cases of the composition assumption. For each given 𝜙, we have 𝑓 , 𝑔 ∈ 𝜙 but 𝑔 ◦ 𝑓 ∉ 𝜙, where
𝑓 and 𝑔 are interpreted as (total or partial) functions from St[𝐾] to St[𝐿]. Let 𝑐1 ≡ (𝑦 = 𝑥 2; 𝑧 = 𝑔(𝑦)) and
𝑐2 ≡ (𝑦 = 𝑥; 𝑧 = 𝑔(𝑥, 𝑦)). Then, for each 𝑖-th 𝜙,
incorrectly concludes that 𝑧 is smooth with respect to 𝑥.

♯

𝑐𝑖
(cid:74)

(cid:75)

Definition 6.1 (SPGE Variable-Selection Problem; Formal). Assume we are given a model 𝑐𝑚, a guide
𝜋 always terminate and
𝑐𝑔, and a (initial) simple reparameterisation plan 𝜋0 such that 𝑐𝑚, 𝑐𝑔, and 𝑐𝑔
have no double-sampling errors for all 𝜋 ⊑ 𝜋0. Given these 𝑐𝑚, 𝑐𝑔, and 𝜋0, find a reparameterisation
plan 𝜋 ⊑ 𝜋0 such that (i) 𝜋 is simple and satisfies (R2) and (R3) in §4.2, and (ii) |rv(𝜋)| is maximised.
We say that 𝜋 is a sound solution if it satisfies (i), and an optimal solution if it satisfies (i) and (ii). □

Here we write 𝜋 ⊑ 𝜋 ′ if the graph of 𝜋 is included in that of 𝜋 ′.

The input 𝜋0 in the problem is a newcomer. It fixes a semantics-preserving transformation for all
the sample commands. Typically, 𝜋0 is defined on the entire NameEx × DistEx × LamEx, and remains
fixed across all input model-guide pairs (𝑐𝑚, 𝑐𝑔). More importantly, it is valid so that the change of
any sample command by 𝜋0 preserves the semantics of the command when we take into account
both the second distribution argument and the third lambda argument of the sample command. The
validity of 𝜋0 is inherited by any sound solution 𝜋 of the SPGE variable-selection problem since
validity as a property on reparameterisation plans is down-closed with respect to the ⊑ order. In our
setup, 𝜋0 is fixed to be the following reparameterisation plan from §4.1:
√

𝜋0(𝑛, distN (𝑒1, 𝑒2), 𝜆𝑦.𝑒3) ≜ (distN (0, 1), 𝜆𝑦.𝑒3 [(𝑦 ×

𝑒2 + 𝑒1)/𝑦])

(12)

for all 𝑛 ∈ NameEx and expressions 𝑒1, 𝑒2, and 𝑒3.

As an example of the SPGE variable-selection problem, consider the problem for the 𝜋0 in
Eq. (12) and the model-guide pair (𝑐𝑚, 𝑐𝑔) given in Fig. 1, where "zi" in the figure is interpreted
as name("zi", 0). Then, as discussed in §2, the problem has the following optimal solution: 𝜋 ≜
𝜋0|𝑆×DistEx×LamEx for 𝑆 ≜ {name(𝛼, 𝑒) ∈ NameEx | 𝛼 (cid:46) "z2"}.

We present an algorithm for computing a sound (yet possibly suboptimal) solution to the problem.

(1) By running our program analysis instantiated with differentiability (described in §5.2 and §5.3),
♯, where we use p, d, and V for the

compute (p𝑚, d𝑚, V𝑚) ≜
output of the analysis to distinguish them from densities 𝑝 and distributions 𝑑.

♯ and (p𝑔, d𝑔, V𝑔) ≜

𝑐𝑚
(cid:74)

𝑐𝑔
(cid:74)

(cid:75)

(cid:75)

(2) Using p𝑚 and p𝑔, check

𝜃 ⊆ 𝐾, where 𝐾 ≜ p𝑚 (like) ∩

(cid:217)

𝜇 ∈Name

p𝑚 (pr 𝜇) ∩

(cid:217)

𝜇 ∈Name

p𝑔 (pr 𝜇).

(13)

If the check fails, return an error message that our program analysis cannot discharge (R2) for
any 𝜋, since the analysis concludes that the density function of𝑐𝑚 or𝑐𝑔 can be non-differentiable
in 𝜃 (even when rv(𝜋) = ∅). If the check passes, initialise the set of reparameterised random
variables by

𝑆 ≜ {(𝛼, 𝑖) ∈ Name | for all 𝑖 ′ ∈ N, (𝛼, 𝑖 ′) ∈ Name =⇒ (𝛼, 𝑖 ′) ∈ 𝐾 }.

(3) Using 𝑆 and 𝜋0, construct a reparameterisation plan 𝜋 ⊑ 𝜋0 by 𝜋 ≜ 𝜋0 [𝑆], where 𝜋0 [𝑆] (𝑛, 𝑑, 𝑙)
is 𝜋0(𝑛, 𝑑, 𝑙) if (𝑛, 𝑑, 𝑙) ∈ dom(𝜋0), 𝑛 = name(𝛼, _), and (𝛼, _) ∈ 𝑆; otherwise, it is undefined.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

(4) By running the differentiability analysis on 𝑐𝑔

𝜋 , compute (p𝑔, d𝑔, V𝑔) ≜

(cid:217)

𝜃 ⊆

𝜇 ∈Name

p𝑔 (pr 𝜇) ∩

(cid:217)

𝜇 ∈Name

p𝑔 (val𝜇).

𝑐𝑔
(cid:74)

𝜋

♯ and check

(cid:75)

0:23

(14)

If the check passes, return 𝜋 as the output of the algorithm. If not, update𝑆 by𝑆 \{(𝛼, 𝑖) ∈ Name}
after choosing some (𝛼, _) ∈ 𝑆, and then repeat the above procedure (from the step (3), the
point where we construct 𝜋 using 𝑆) until 𝑆 becomes empty.

Our algorithm computes a sound solution, because of the soundness of our program analysis:

Theorem 6.2. Let 𝑐𝑚, 𝑐𝑔, and 𝜋0 be the inputs to the SPGE variable-selection problem. If the above

algorithm returns 𝜋 for (𝑐𝑚, 𝑐𝑔, 𝜋0), then 𝜋 is a sound solution for the problem.

Our algorithm solves the problem only approximately: there is no formal guarantee that it al-
ways computes an optimal solution. The suboptimality may arise due to two approximations: the
overapproximation of our program analysis when it computes differentiability information, and the
heuristic choices made by our algorithm when the algorithm computes the random-variable set 𝑆.
We demonstrate, however, that our algorithm finds optimal solutions for all the benchmarks in §7.
Our algorithm calls our program analysis at most |{𝛼 ∈ Str | (𝛼, _) ∈ 𝑆0}| + 2 times, where 𝑆0
is the initial value of 𝑆 (i.e., the set of random variables whose sample commands are to be trans-
formed) in the algorithm. However, for all the benchmarks in §7, our algorithm terminated with the
initial set 𝑆0 and thus called our smoothness analysis only 3 times (on the model, the guide, and the
reparameterised guide according to 𝑆0).

7 EXPERIMENTAL EVALUATION AND RELATED WORKS
In our experiments, we consider two research questions. First, can the analysis proposed in §5 be
instantiated and implemented so that it can produce meaningful smoothness results on real-world
probabilistic programs? Second, can the algorithm proposed in §6 find near-optimal solutions to the
SPGE variable-selection problem on real-world probabilistic programs? To assess the two questions,
we have implemented a static smoothness analyser for Pyro programs based on §5, and a variable
selector based on §6 which (approximately) solves the variable-selection problem. Our analyser
and variable selector are implemented in OCaml, and support a subset of the Pyro PPL and two
smoothness properties: differentiability and local Lipschitzness.

Implementation. Although the analysis described in §5 may look simple when considering a basic
PPL, real-world PPLs such as Pyro are of a much higher degree of complexity. First, they provide
a large panel of continuous/discrete probability distributions for sample and observe commands, and
library functions for tensors and neural networks. Second, programs in real-world PPLs may fail to be
smooth for reasons other than if-else and while commands. In particular, values sampled from discrete
distributions, and arguments to operators and distribution constructors that are well-defined only on
a strict subset of values, may induce non-smoothness. A straightforward treatment of these will result
in an overly conservative analysis, treating far too many variables as potentially non-smooth. Third,
Pyro programs typically rely on tensors (of large, statically unknown size) to deal with large datasets,
and it is generally infeasible to reason about each (real-valued) element of tensors individually. In
the following, we discuss how our static analyser addresses these issues and provides sound, useful
information about smoothness of Pyro programs.
Distributions and library functions. Our analyser supports 17 distributions (continuous or discrete).
Each distribution is characterized by a pair (𝑏, 𝑎) for a boolean 𝑏 and an array of booleans 𝑎, where
𝑏 (or 𝑎𝑖 ) denotes whether its probability density is differentiable or locally Lipschitz with respect
to the sampled value (or the 𝑖-th argument) of the distribution. For example, a normal distribution

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Wonyeol Lee, Xavier Rival, and Hongseok Yang

0:24

Name

Probabilistic model
Splitting normal example in Fig. 1

Deep Markov model
Hidden Markov models

spnor
sgdef Deep exponential family
dmm
mhmm
scanvi Single-cell annotation using variational inference
air
cvae

Attend-infer-repeat
Conditional variational autoencoder

LoC while sam obs param
2
12
13
12
21
16
15

16
105
112
137
147
174
205

2
12
2
5
7
6
2

1
1
1
5
2
1
1

0
0
3
1
0
2
0

Table 2. Subset of Pyro examples used in experiments and their key features (see §I for the rest). The last
five columns show the total number of code lines (excluding comments), loops, sample commands, observe
commands, and learnable parameters (declared explicitly by pyro.param or implicitly by a neural network
module). Each number is the sum of the counts in the model and guide.

is described by (true,[true,true]) (assuming that the second argument is positive) and a Poisson
distribution by (false,[true]). Similarly, the analyser supports a large number of PyTorch/Pyro
library functions for tensors and neural networks, and assumes the correct smoothness information
about them. For instance, the ReLU function is considered locally Lipschitz but not differentiable.
Refining smoothness information based on safety pre-analysis. Although the expression x/y is generally
non-smooth with respect to y (even if it is well-defined for y=0), if more information is available, for
instance that y always lies in range [1, 10], we can safely consider it smooth with respect to both x and
y. Likewise, the density of a normal distribution is generally non-smooth with respect to the standard
deviation argument 𝜎 (even if it is well-defined for 𝜎 ≤ 0), so more precise smoothness information
can be produced when 𝜎 is known to be always positive. Thus, establishing precise smoothness
information requires to first establish safety properties related to program operations. To achieve
this, our tool actually performs two analyses in sequence: (i) a safety pre-analysis infers ranges over
all numerical variables and marks each argument to an operator or a distribution constructor as
either “safe” or “potentially unsafe”; (ii) the smoothness analysis formalised in §5 utilises information
computed in the first phase to produce precise smoothness information. The first phase boils down
to a forward abstract interpretation based on basic abstract domains like intervals and signs [10].
It logs safety information for each program statement just like static analyses for runtime errors and
undefined behaviors [3]. As formalised in §5.2, the second analysis is compositional. Due to their
different nature, the two analyses need to be done in sequence.
Tensors. Pyro programs commonly use nested loops and indexed tensors. As the number of dimensions
of such tensors is often statically unknown, enumerating all dimensions is not feasible; so we rely on
a conservative summarisation of tensor dimensions. Intuitively, this means that when density might
not be smooth with respect to one dimension of a tensor, the analysis conservatively concludes that
it might not be smooth with respect to any dimension. In our experiments, this abstraction does not
result in any precision loss.

Evaluation. We evaluated our analyser and variable selector on 13 representative Pyro examples
from the Pyro webpage [42] that use standard SVI engines and contain explicitly written model-
guide pairs (without AutoGuide). They include advanced models with deep neural networks such
as attend-infer-repeat [12] and single-cell annotation using variational inference [49]. Additionally,
we included the example in Fig. 1, for which Pyro offers an unsound reparameterisation plan. Table 2
lists half of these 14 Pyro examples with their code size and conceptual complexity (see §I for the
rest). Experiments were performed on a Macbook Pro with 2.3GHz Core i9 and 32GB RAM.
Smoothness analyser. We assess our smoothness analyser on the 14 Pyro examples for differentiability
and local Lipschitzness (§5.3), and show a subset of results in Table 3 (see §I for the rest). The results

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:25

Differentiable

Locally Lipschitz

Name

spnor-m
spnor-g
sgdef-m
sgdef-g
dmm-m
dmm-g
mhmm-m
mhmm-g
scanvi-m
scanvi-g
air-m
air-g
cvae-m
cvae-g

Manual Ours Time Manual Ours Time
0.009
0.008
0.006
0.015
0.016
0.020
0.075
0.008
0.032
0.058
0.105
0.072
0.027
0.023

0.006
0.007
0.003
0.016
0.014
0.026
0.063
0.007
0.032
0.052
0.108
0.075
0.025
0.031

1
4
6
18
10
5
10
6
12
15
4
15
8
9

1
4
6
18
10
5
10
6
12
15
4
15
8
9

1
4
6
18
4
4
10
6
6
8
1
3
3
5

1
4
6
18
4
4
10
6
6
8
1
3
3
5

#CRP
2
4
6
18
10
5
10
6
12
15
4
16
8
9

Table 3. Results of smoothness analyses. “Manual” and “Ours” denote the number of continuous random
variables and learnable parameters in which the density of the program is smooth, computed by hand and
by our analyser. “Time” denotes the runtime of our analyser in seconds. “#CRP” denotes the total number
of continuous random variables and learnable parameters in the program. -m and -g denote model and guide.
We consider {(𝛼, 𝑖) ∈ Name} as one random variable for each 𝛼 ∈ Name.

demonstrate that our analysis can cope successfully with real-world Pyro programs. First, our analysis
is accurate. For all examples, the analysis identifies the exact ground-truth set of random variables
and parameters in which the density of the program is differentiable (or locally Lipschitz). In many of
them, information computed by the pre-analysis is required to achieve these exact results; e.g., some
examples (e.g., dpmm and air) require precise information about which distribution arguments can be
proved to be always in the proper range of values. Second, the runtime of our analysis is low. Typical
probabilistic programming applications are not of a very large size, and conceptual complexity is
generally the main issue, thus the analysis performance presents no scalability concern.

We draw two more observations from the results. First, for spnor-m and air-g, the density of each
program is not locally Lipschitz in one continuous random variable. These non-local-Lipschitznesses
arise as follows: for the former, the random variable ("z2" in Fig. 1) is used in the branch condition
of an if-else command that contains observe commands, thereby creating discontinuity; and for the
latter, the random variable ("z_where") is passed into the denominator of a division operator, thereby
causing a division-by-zero error for some value.

Second, for all the other examples, the density is locally Lipschitz in all continuous random vari-
ables and parameters, but is often non-differentiable in many parameters (and continuous random
variables too); see, for instance, scanvi and cvae. Due to this, the requirement (R2) is not satisfied for
these examples even with the empty reparameterisation plan (corresponding to the score estimator);
that is, if we use the differentiability requirements (R2) and (R3) to validate the unbiasedness of
gradient estimators, even the score estimator cannot be validated for these examples. From manual
inspection, we checked that the non-differentiabilities from these examples all arise by the use of
locally Lipschitz but non-differentiable operators (e.g., relu and grid_sample). Since many practical
models (and guides) use locally Lipschitz but non-differentiable operators, this observation strongly
suggests that a right smoothness requirement for validating gradient estimators is not differentiability
(which has been used as a standard requirement), but rather local Lipschitzness (e.g., (R2’) and (R3’)).
Variable selector. To evaluate our variable selector, we consider the SPGE variable-selection problem
with local Lipschitzness requirements, i.e., the problem that uses (R2’) and (R3’) in §4.3 instead of
(R2) and (R3) in §4.2. We do not consider the original problem (with differentiability requirements),

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:26

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Ours

Time
0.021
0.034
0.054
0.083
0.143
0.247
0.063

Sound
1
6
1
2
3
1
1

Pyro \ Ours
Sound Unsound
1
0
0
0
0
1
0

0
0
0
0
0
0
0

#CR #DR
0
0
0
1
1
2
0

2
6
1
2
3
2
1

Name

spnor
sgdef
dmm
mhmm
scanvi
air
cvae

Table 4. Results of variable selections. “Ours-Time” denote the runtime of our variable selector in seconds.
“Ours-Sound” and “Pyro \ Ours” denote the number of random variables in the example that are in 𝜋ours, and
that are in 𝜋0 but not in 𝜋ours, respectively, where 𝜋ours and 𝜋0 denote the reparameterisation plans given by
our variable selector and by Pyro. “Pyro \ Ours” is partitioned into “Sound” and “Unsound”: the latter denotes
the number of random variables that make (R2’) or (R3’) violated when added to 𝜋ours, and the former denotes
the number of the rest. “#CR” and “#DR” denote the total number of continuous and discrete random variables
in the example. We consider {(𝛼, 𝑖) ∈ Name} as one random variable for each 𝛼 ∈ Name.

since for many examples the differentiability requirements are not satisfied even by the empty
reparameterisation plan (i.e., score estimator) as observed above. For an initial reparameterisation
plan 𝜋0 for the problem, we use the plan given by Pyro’s default variable selector: it is defined for
all continuous random variables and applies standard reparameterisations (e.g., Eq. (12) for a normal
distribution). In this settings, we apply our variable selector to the problem on the 14 Pyro examples.
Table 4 displays the results (only for 7 examples; see §I for the rest) and compares them with 𝜋0.

The results demonstrate that for all examples, our variable selector finds the optimal reparameterisa-
tion plan with a small runtime. We also observe that for all cases, it terminates in the first iteration and
calls our smoothness analyser only three times, as mentioned in §6. Note that the reparameterisation
plan given by Pyro is also optimal for all but two examples. We emphasise, however, that our variable
selector not only finds a reparameterisation plan but also verifies the local Lipschitzness requirements
(R2’) and (R3’), whereas Pyro’s default variable selector does not do so. Indeed, for two examples,
Pyro’s reparameterisation plan is unsound as it violates the local Lipschitzness requirements. Hence,
these results should be interpreted as: for all but two examples, our variable selector (and smoothness
analyser) successfully validate the unbiasedness of the default gradient estimator used by Pyro.

The two examples for which Pyro becomes unsound are spnor and air. Recall that they have two
continuous random variables (one for each) in which their densities are not locally Lipschitz. The
unsoundness of Pyro on these examples stems precisely from the fact that it reparameterises the
two non-locally-Lipschitz random variables without checking any local Lipschitzness requirements.

Related Work. The high-level idea of using program transformation for improved posterior in-
ference and model learning in PPLs has been explored previously [8, 17, 30, 35, 37]. In particular,
Schulman et al. [37] proposed a method for implementing the SPGE for stochastic computation graphs
via graph transformation, and this method was adopted in the implementation of the same estimator
in Pyro and also in our work. However, the method lacks a formal analysis on the implemented
estimator especially in the context of probabilistic programs; it does not have a version of Theorem 4.4,
which formally identifies requirements for the unbiasedness of the estimator. Also, the method does
not check the required smoothness properties of given probabilistic programs. Our work fills in these
gaps. Gorinova et al. [17] proposed an automatic technique to transform models in a PPL using the
same or closely-related transformation of sample commands in the SPGE. The work is, however,
concerned with transforming models and taming their posterior distributions, while ours focuses on
transforming guides. Also, the work does not check smoothness properties of transformed models that

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:27

are required for running efficient inference algorithms, such as Hamiltonian Monte Carlo, on those
models, while our work checks those properties using our program analysis. We also point out that
program analyses or type systems for probabilistic programs have been developed to detect common
errors [24, 25] or infer important probabilistic properties, such as conditional independence [18].

The smoothness properties computed by our program analysis, such as differentiability and local
Lipschitzness, fall in the scope of hyperliveness in the hierarchy of hyperproperties [9]. Intuitively,
hyperliveness properties are those that cannot be refuted based on any finite counterexample (i.e.,
made of finitely-many finite execution traces), and counterexamples for differentiability and local
Lipschitzness should indeed require infinitely-many execution traces due to the use of limit or all
neighbouring inputs in their definitions. Not so many analyses have considered such hyperliveness
properties. Among those, the most relevant to our work are the continuity analyses of Chaudhuri
et al. [6, 7]. It uses a program abstraction that is rather similar to ours, but their analyses suffer from
soundness issues, due to the incorrect joining of continuity sets [6] and also to an unsound rule for
sequential composition [7] (see §A.1 for details). We do not claim that these issues are difficult to
fix. Our point is just that developing program analyses for smoothness properties requires special
care. Chaudhuri et al.’s work focuses on proving smoothness properties of control software, or
revealing the unexpected continuity of discrete algorithms. On the other hand, our program analysis
is designed to assist variational inference and model learning for probabilistic programs.

ACKNOWLEDGMENTS
We thank Hangyeol Yu for helping us prove an initial version of Theorems 4.2 and 4.4. Lee was
supported by Samsung Scholarship. Yang was supported by the Engineering Research Center Program
through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT
(NRF-2018R1A5A1059921) and also by the Institute for Basic Science (IBS-R029-C1).

REFERENCES
[1] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In International

Conference on Machine Learning (ICML), pages 214–223, 2017.

[2] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh,
Paul A. Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal probabilistic programming. Journal of
Machine Learning Research, 20(28):1–6, 2019.

[3] B. Blanchet, P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Miné, D. Monniaux, and X. Rival. A Static Analyzer for
Large Safety Critical Software. In Programming Languages, Design and Implementation (PLDI), pages 196–207, 2003.
[4] Bob Carpenter, Andrew Gelman, Matthew Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker,
Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal of Statistical Software,
76(1):1–32, 2017.

[5] Arun Tejasvi Chaganty, Aditya V. Nori, and Sriram K. Rajamani. Efficiently sampling probabilistic programs via program

analysis. In Artificial Intelligence and Statistics (AISTATS), pages 153–160, 2013.

[6] Swarat Chaudhuri, Sumit Gulwani, and Roberto Lublinerman. Continuity analysis of programs.

In Principles of

Programming Languages (POPL), pages 57–70, 2010.

[7] Swarat Chaudhuri, Sumit Gulwani, and Roberto Lublinerman. Continuity and robustness of programs. Commun. ACM,

55(8):107–115, 2012.

[8] Guillaume Claret, Sriram K. Rajamani, Aditya V. Nori, Andrew D. Gordon, and Johannes Borgström. Bayesian inference

using data flow analysis. In Foundations of Software Engineering (FSE), pages 92–102, 2013.

[9] M. R. Clarkson and F. B. Schneider. Hyperproperties. In Computer Security Foundations (CSF), pages 51–65, 2008.
[10] Patrick Cousot and Radhia Cousot. Abstract interpretation: A unified lattice model for static analysis of programs

by construction or approximation of fixpoints. In Principles of Programming Languages (POPL), pages 238–252, 1977.

[11] Patrick Cousot and Radhia Cousot. Systematic design of program analysis frameworks. In Principles of Programming

Languages (POPL), pages 269–282, 1979.

[12] S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, and Geoffrey E.
Hinton. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models. In Neural Information Processing
Systems (NIPS), pages 3233–3241, 2016.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:28

Wonyeol Lee, Xavier Rival, and Hongseok Yang

[13] Hong Ge, Kai Xu, and Zoubin Ghahramani. Turing: A language for flexible probabilistic inference.

In Artificial

Intelligence and Statistics (AISTATS), pages 1682–1690, 2018.

[14] Timon Gehr, Sasa Misailovic, and Martin T. Vechev. PSI: exact symbolic inference for probabilistic programs.

In

Computer Aided Verification (CAV), pages 62–83, 2016.

[15] Noah Goodman, Vikash Mansinghka, Daniel M Roy, Keith Bonawitz, and Joshua B Tenenbaum. Church: a language

for generative models. In Uncertainty in Artificial Intelligence (UAI), pages 220–229, 2008.

[16] Andrew D. Gordon, Thore Graepel, Nicolas Rolland, Claudio Russo, Johannes Borgstrom, and John Guiver. Tabular: A
schema-driven probabilistic programming language. In Principles of Programming Languages (POPL), pages 321–334, 2014.
[17] Maria I. Gorinova, Dave Moore, and Matthew D. Hoffman. Automatic reparameterisation of probabilistic programs.

In International Conference on Machine Learning (ICML), pages 3648–3657, 2020.

[18] Maria I. Gorinova, Andrew D. Gordon, Charles Sutton, and Matthijs Vákár. Conditional independence by typing. ACM

Trans. Program. Lang. Syst., 44(1):4:1–4:54, 2022.

[19] Steven Holtzen, Guy Van den Broeck, and Todd D. Millstein. Scaling exact inference for discrete probabilistic programs.

Proc. ACM Program. Lang., 4(OOPSLA):140:1–140:31, 2020.

[20] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention.

In International

Conference on Machine Learning (ICML), pages 5562–5571, 2021.

[21] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning

Representations (ICLR), 2014.

[22] Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic variational inference in stan. In

Neural Information Processing Systems (NIPS), pages 568–576, 2015.

[23] John M. Lee. Introduction to Smooth Manifolds. Graduate Texts in Mathematics. Springer, second edition, 2012.
[24] Wonyeol Lee, Hangyeol Yu, Xavier Rival, and Hongseok Yang. Towards verified stochastic variational inference for

probabilistic programs. Proc. ACM Program. Lang., 4(POPL):16:1–16:33, 2020.

[25] Alexander K. Lew, Marco F. Cusumano-Towner, Benjamin Sherman, Michael Carbin, and Vikash K. Mansinghka. Trace
types and denotational semantics for sound programmable inference in probabilistic languages. Proc. ACM Program.
Lang., 4(POPL):19:1–19:32, 2020.

[26] Vikash K. Mansinghka, Daniel Selsam, and Yura N. Perov. Venture: a higher-order probabilistic programming platform

with programmable inference. arXiv:1404.0099, 2014.

[27] T. Minka, J.M. Winn, J.P. Guiver, S. Webster, Y. Zaykov, B. Yangel, A. Spengler, and J. Bronskill. Infer.NET 2.6, 2014.

Microsoft Research Cambridge. http://research.microsoft.com/infernet.

[28] Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. Probabilistic inference by
program transformation in hakaru (system description). In Functional and Logic Programming (FLOPS), pages 62–79, 2016.

[29] Radford M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 54:113–162, 2010.
[30] Aditya V. Nori, Chung-Kil Hur, Sriram K. Rajamani, and Selva Samuel. R2: an efficient MCMC sampler for probabilistic

programs. In AAAI Conference on Artificial Intelligence (AAAI), pages 2476–2482, 2014.

[31] André Platzer. Logical Foundations of Cyber-Physical Systems. Springer, 2018. ISBN 978-3-319-63587-3.
[32] Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In Artificial Intelligence and

Statistics (AISTATS), pages 814–822, 2014.

[33] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approximate Inference

in Deep Generative Models. In International Conference on Machine Learning (ICML), 2014.

[34] Daniel Ritchie, Paul Horsfall, and Noah D. Goodman. Deep amortized inference for probabilistic programs.

arXiv:1610.05735, 2016.

[35] Daniel Ritchie, Andreas Stuhlmüller, and Noah D. Goodman. C3: lightweight incrementalized MCMC for probabilistic
programs using continuations and callsite caching. In Artificial Intelligence and Statistics (AISTATS), pages 28–37, 2016.
[36] John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. Probabilistic programming in python using pymc3.

PeerJ Comput. Sci., 2:e55, 2016.

[37] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation

graphs. In Neural Information Processing Systems (NIPS), pages 3528–3536, 2015.

[38] N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah D. Goodman, Pushmeet Kohli, Frank
Wood, and Philip Torr. Learning disentangled representations with semi-supervised deep generative models. In Neural
Information Processing Systems (NIPS), pages 5927–5937, 2017.

[39] David Tolpin, Jan-Willem van de Meent, Hongseok Yang, and Frank D. Wood. Design and implementation of probabilistic
programming language anglican. In Implementation and Application of Functional Programming Languages (IFL), pages
6:1–6:12, 2016.

[40] Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja R. Rudolph, Dawen Liang, and David M. Blei. Edward: A library for

probabilistic modeling, inference, and criticism. arXiv:1610.09787, 2016.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:29

[41] Dustin Tran, Matthew D. Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, and Alexey Radul. Simple,
distributed, and accelerated probabilistic programming. In Neural Information Processing Systems (NeurIPS), pages
7609–7620, 2018.

[42] Uber AI Labs. Pyro examples. http://pyro.ai/examples/, 2022. Version used: June 18, 2022.
[43] Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. An introduction to probabilistic programming.

arXiv:1809.10756, 2018.

[44] Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.

Machine Learning, 8(3-4):229–256, 1992.

[45] David Wingate and Theophane Weber. Automated variational inference in probabilistic programming. arXiv:1301.1299,

2013.

[46] David Wingate, Noah D. Goodman, Andreas Stuhlmüller, and Jeffrey Mark Siskind. Nonstandard interpretations of
probabilistic programs for efficient inference. In Neural Information Processing Systems (NIPS), pages 1152–1160, 2011.
[47] David Wingate, Andreas Stuhlmüller, and Noah D. Goodman. Lightweight implementations of probabilistic programming
languages via transformational compilation. In Artificial Intelligence and Statistics (AISTATS), pages 770–778, 2011.
[48] Frank Wood, Jan Willem van de Meent, and Vikash Mansinghka. A new approach to probabilistic programming

inference. In Artificial Intelligence and Statistics (AISTATS), pages 1024–1032, 2014.

[49] Chenling Xu, Romain Lopez, Edouard Mehlman, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Probabilistic
harmonization and annotation of single-cell transcriptomics data with deep generative models. Molecular systems
biology, 17(1):e9620, 2021.

[50] Yuan Zhou, Hongseok Yang, Yee Whye Teh, and Tom Rainforth. Divide, conquer, and combine: a new inference strategy
for probabilistic programs with stochastic support. In International Conference on Machine Learning (ICML), pages
11534–11545, 2020.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:30

Wonyeol Lee, Xavier Rival, and Hongseok Yang

A DEFERRED RESULTS IN §1

A.1 Unsoundness of continuity analyses in [6, 7]
The continuity analysis in [6] considers joint continuity, whereas the continuity analysis in [7]
considers partial continuity. That is, given a command 𝑐, an output variable 𝑣 of 𝑐, and some input
variables 𝑢1, . . . , 𝑢𝑚 to 𝑐, the former analyses whether 𝑣 is continuous in {𝑢1, . . . , 𝑢𝑚 } jointly, whereas
the latter analyses whether 𝑣 is continuous in 𝑢𝑖 separately for every 1 ≤ 𝑖 ≤ 𝑚.

The former analysis contains the rule called Join [6, Figure 3] and the latter analysis contains the
rule called Sequence [7, Figure 1]. The two rules can be rewritten (with some simplifications) as
follows, in terms of functions between R𝑛: for any 𝑓 , 𝑔 : R𝑛 → R𝑛 and 𝑆, 𝑆 ′,𝑇 , 𝑈 ⊆ {1, . . . , 𝑛},

For each 𝑗 ∈ 𝑇 , 𝑓𝑗 is continuous in {𝑥𝑖 | 𝑖 ∈ 𝑆 }
For each 𝑗 ∈ 𝑇 , 𝑓𝑗 is continuous in {𝑥𝑖 | 𝑖 ∈ 𝑆 ′}
For each 𝑗 ∈ 𝑇 , 𝑓𝑗 is continuous in {𝑥𝑖 | 𝑖 ∈ 𝑆 ∪ 𝑆 ′}

(Join)

For each 𝑗 ∈ 𝑇 , 𝑓𝑗 is continuous in 𝑥𝑖 for each 𝑖 ∈ 𝑆
For each 𝑘 ∈ 𝑈 , 𝑔𝑘 is continuous in 𝑦 𝑗 for each 𝑗 ∈ 𝑇
For each 𝑘 ∈ 𝑈 , (𝑔 ◦ 𝑓 )𝑘 is continuous in 𝑥𝑖 for each 𝑖 ∈ 𝑆

(Sequence)

where 𝑓 and 𝑔 are functions of variables 𝑥1, . . . , 𝑥𝑛 and 𝑦1, . . . , 𝑦𝑛, respectively, and ℎ𝑖 ≜ proj𝑖 ◦ ℎ for
ℎ : R𝑛 → R𝑛 and 𝑖 ∈ {1, . . . , 𝑛} denotes the 𝑖-th component of ℎ. As mentioned above, the Join rule
analyses joint continuity, while the Sequence rule analyses partial continuity. Further, the Join rule
says that joint continuity is preserved under the union of input variables, while the Sequence rule
says that partial continuity is preserved under the composition of functions.

The two rules, however, are unsound with the following counterexamples. Let ℎ : R2 → R2 be

the function

(cid:40)

ℎ(𝑥1, 𝑥2) ≜

1 + 𝑥 2

2), 𝑥2)

(𝑥1𝑥2/(𝑥 2
(0, 𝑥2)

if (𝑥1, 𝑥2) ≠ (0, 0)
otherwise.

Note that ℎ1 is continuous in 𝑥1 and in 𝑥2 separately, but not in {𝑥1, 𝑥2} jointly. First, for the Join rule,
consider the following 𝑓 : R2 → R2 and 𝑆, 𝑆 ′,𝑇 ⊆ {1, 2}:

𝑓 (𝑥1, 𝑥2) ≜ ℎ(𝑥1, 𝑥2),

𝑆 ≜ {1},

𝑆 ′ ≜ {2},

𝑇 ≜ {1, 2}.

Then, the premise of the Join rule holds, so the conclusion of the rule must hold. But this is not the case
since 𝑓1 = ℎ1 is not continuous in {𝑥1, 𝑥2}. Hence, the Join rule is unsound. Next, for the Sequence
rule, consider the following 𝑓 , 𝑔 : R2 → R2 and 𝑆,𝑇 , 𝑈 ⊆ {1, 2}:

𝑓 (𝑥1, 𝑥2) ≜ (𝑥1, 𝑥1),

𝑔(𝑥1, 𝑥2) ≜ ℎ(𝑥1, 𝑥2),

𝑆 ≜ 𝑇 ≜ 𝑈 ≜ {1, 2}.

Then, the premise of the Sequence rule holds, so the conclusion of the rule must hold. But this is not
the case since (𝑔 ◦ 𝑓 )1 is not continuous in 𝑥1 (due to (𝑔 ◦ 𝑓 )1(𝑥1, 𝑥2) = 1[𝑥1≠0] · 1
2 ). Hence, the Sequence
rule is unsound. These counterexamples show that joint continuity is not preserved under the union
of input variables, and partial continuity is not preserved under the composition of functions.

The two aforementioned counterexamples can be easily translated into programs: the first becomes
𝑐1 ≡ (𝑧 := ℎ1(𝑥, 𝑦)) and the second becomes𝑐2 ≡ (𝑦 := 𝑥; 𝑧 := ℎ1(𝑥, 𝑦)), where 𝑥,𝑦, and𝑧 are program
variables and ℎ1 is the binary operator defined above. The analysis in [6] deduces that in 𝑐1, 𝑧 is con-
tinuous in 𝑥 and 𝑦 (jointly), and the analysis in [7] deduces that in 𝑐2, 𝑧 is continuous in 𝑥 (separately).
Both deductions, however, are incorrect as seen above, and the two analyses are thus unsound.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:31

𝑞𝜃 (𝑧)

𝑔𝜃 (𝑧)

SCE

PGE

𝑝𝑐𝑔,𝜃 (𝑧)
𝑓𝜃 (𝑧) · ∇𝜃 log 𝑞𝜃 (𝑧) ∇𝜃 𝑓𝜃 (𝑧 ′)

𝑔 (𝑧)

𝑝𝑐′

requirements

𝑝𝑐𝑔,𝜃 (𝑧): diff. in 𝜃

for 𝑧 ′ = 𝑣𝑐′

𝑔,𝜃 (𝑧)

—
𝑝𝑐𝑚 (𝑧): diff. in 𝜃 and 𝑧
𝑝𝑐𝑔,𝜃 (𝑧): diff. in 𝜃 and 𝑧
𝑣𝑐′

𝑔,𝜃 (𝑧): diff. in 𝜃

SPGE

𝑝𝑐′′

𝑔,𝜃 (𝑧)

∇𝜃 𝑓𝜃 (𝑧 ′′) + 𝑓𝜃 (𝑧 ′′) · ∇𝜃 log 𝑞′′
𝑔,𝜃 (𝑧)

𝜃 (𝑧)

for 𝑧 ′′ = 𝑣𝑐′′
𝑔,𝜃 (𝑧): diff. in 𝜃

𝑝𝑐′′
𝑝𝑐𝑚 (𝑧): diff. in 𝜃 and changed 𝑧𝑖 ’s
𝑝𝑐𝑔,𝜃 (𝑧): diff. in 𝜃 and changed 𝑧𝑖 ’s
𝑣𝑐′′

𝑔,𝜃 (𝑧): diff. in 𝜃

Table 5. Gradient estimators for variational inference. “diff.” denotes “differentiable”.

B DEFERRED RESULTS IN §2

B.1 Table Summarising §2
Table 5 compares key aspects of the three gradient estimators (SCE, PGE, and SPGE) explained in §2.

C DEFERRED RESULTS IN §4.1

C.1 Proof of Theorem 4.2
We introduce several definitions, state lemmas, and prove Theorem 4.2 using the lemmas. We prove
the lemmas in §C.2 and §C.3.

Recall the partition Var = PVar ⊎ Name ⊎ AVar of Var. We use the following letters to denote the
values of each part: 𝜎𝑝 ∈ St[PVar], 𝜎𝑛 ∈ St[Name], and 𝜎𝑎 ∈ St[AVar]. Based on the partition, we
define the next functions:

prs(𝑐) : St[PVar] × St[Name] × St[AVar] → [0, ∞),

prs(𝑐)(𝜎𝑝, 𝜎𝑛, 𝜎𝑎) ≜





0

(𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝜎𝑎)(like)
𝑐
(cid:75)
(cid:74)
· (cid:206)𝜇 ∈Name
𝑐
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝜎𝑎)(pr 𝜇)

if noerr (𝑐, 𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝜎𝑎)

otherwise,

vals(𝑐) : St[PVar] × St[Name] × St[AVar] → St[Name],

vals(𝑐)(𝜎𝑝, 𝜎𝑛, 𝜎𝑎) ≜

(cid:40)𝜆𝜇 ∈ Name.
𝑐
(cid:74)
𝜆𝜇 ∈ Name. 0

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝜎𝑎)(val𝜇)

if noerr (𝑐, 𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝜎𝑎)
otherwise,

where noerr (𝑐, 𝜎) is a predicate for a command 𝑐 and 𝜎 ∈ St, defined by

noerr (𝑐, 𝜎) ⇐⇒

𝑐
(cid:74)

𝜎 ∈ St ∧ (cid:0)∀𝜇 ∈ Name.
(cid:75)

𝑐
(cid:74)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) ≤ 1(cid:1).
(cid:75)

The predicate noerr (𝑐, 𝜎) says that 𝑐 terminates for 𝜎 without a double-sampling error. The functions
prs and vals generalise the density function 𝑝 and the value function 𝑣, respectively; in particular,
they do not assume a particular initial state 𝜎0 used in Eq. (3). We consider the generalisation of 𝑝
and 𝑣 so as to enable inductive proofs.

Although generalising 𝑝 and 𝑣, the functions prs and vals are not sufficient to enable inductive
proofs since their inputs and outputs contain some unnecessary parts, which stops induction from
working well (especially in the sequential composition case): namely, the part of St[Name] that is
not read during execution, and the part of St[AVar] that is not updated during execution. To exclude

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:32

Wonyeol Lee, Xavier Rival, and Hongseok Yang

those unnecessary parts, we first define the set of substates of St[Name] as follows:

𝜉𝑛 ∈ St□ [Name] ≜ (cid:216)
𝐾 ⊆Name

St[𝐾].

Based on these substates, we define the next functions:

prs□(𝑐) : St[PVar] × St□ [Name] → [0, ∞),

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) ≜





0

𝑐
(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(like)
(cid:75)
(cid:74)
· (cid:206)𝜇 ∈dom(𝜉𝑛)

𝑐
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(pr 𝜇)

if ∃𝜎𝑟 . used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛)

otherwise,

vals□(𝑐) : St[PVar] × St□ [Name] → St□ [Name],

vals□ (𝑐)(𝜎𝑝, 𝜉𝑛) ≜

(cid:40)𝜆𝜇 ∈ dom(𝜉𝑛).
𝑐
(cid:74)
𝜆𝜇 ∈ dom(𝜉𝑛). 0

(cid:75)

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(val𝜇)

if ∃𝜎𝑟 . used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛)
otherwise,

pvars□(𝑐) : St[PVar] × St□ [Name] → St[PVar],

pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛) ≜

(cid:40)𝜆𝑥 ∈ PVar.
𝑐
(cid:74)
𝜆𝑥 ∈ PVar. 0

(cid:75)

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(𝑥)

if ∃𝜎𝑟 . used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛)
otherwise,

where used (𝑐, 𝜎, 𝜉𝑛) is a predicate for a command 𝑐, 𝜎 ∈ St, and 𝜉𝑛 ∈ St□ [Name], defined by

used (𝑐, 𝜎, 𝜉𝑛) ⇐⇒ noerr (𝑐, 𝜎) ∧ (cid:0)𝜎 (like) = 1(cid:1) ∧ (cid:0)𝜉𝑛 = 𝜎 |dom(𝜉𝑛)

∧ (cid:0)dom(𝜉𝑛) = {𝜇 ∈ Name |

(cid:1)
𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) = 1}(cid:1).
(cid:75)

𝑐
(cid:74)

The predicate used (𝑐, 𝜎, 𝜉𝑛) says that 𝑐 terminates for 𝜎 without a double-sampling error, like is
initialised to 1 in 𝜎, and 𝜉𝑛 is the Name part of 𝜎 that is sampled during the execution of 𝑐 from 𝜎.
By using used (−, −, −), the three functions do not take the unnecessary part of a state as an input,
and do not return the unnecessary part of a state in the output. The three functions are well-defined.

Lemma C.1. prs□, vals□, and pvars□ are well-defined, i.e., they do not depend on the choice of 𝜎𝑟 .

We now state two main lemmas for Theorem 4.2. The first lemma describes how prs and vals are
connected with prs□ and vals□. The second lemma says that a particular integral involving prs□,
vals□, and pvars□ is the same for 𝑐 and 𝑐𝜋 if a reparameterisation plan 𝜋 is valid.

Lemma C.2. Let 𝑐 be a command, and 𝑓𝑖 : R → R for 𝑖 ∈ {1, 2, 3} be measurable functions such that

𝑓1(𝑟 ) ≥ 0 for all 𝑟 ∈ R. Define 𝑓∗ : St[Name] → St[AVar] by

𝑓∗ (𝜎𝑛)(𝑎) ≜

1
𝑓1(𝜎𝑛 (𝜇))
𝑓2 (𝜎𝑛 (𝜇))
𝑓3(𝜎𝑛 (𝜇))

if 𝑎 ≡ like
if 𝑎 ≡ pr 𝜇 for 𝜇 ∈ Name
if 𝑎 ≡ val𝜇 for 𝜇 ∈ Name
if 𝑎 ≡ cnt𝜇 for 𝜇 ∈ Name.






Then, for all 𝜎𝑝 ∈ St[PVar] and all measurable ℎ : St[Name] → R,

∫

(cid:16)

𝑑𝜎𝑛

prs(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛)) · ℎ

(cid:16)

vals(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛))

(cid:17)(cid:17)

∫

=

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

vals□(𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:33

where the integral on the LHS is defined if and only if the one on the RHS is defined, and the function
𝑔 : St□ [Name] → R is defined by
(cid:16)

∫

(cid:17)

(cid:16)

(cid:16) (cid:214)

(cid:17)(cid:17)

𝑓1(𝜉 ′

𝑛 (𝜇))

· ℎ

𝑛 ⊕ 𝜆𝜇 ∈ dom(𝜉 ′
𝜉 ′′

𝑛). 𝑓2(𝜉 ′

𝑛 (𝜇))

.

𝑔(𝜉 ′′

𝑛 ) =

𝑑𝜉 ′
𝑛

1[dom(𝜉 ′′

𝑛)⊎dom(𝜉 ′

𝑛)=Name] ·

𝜇 ∈dom(𝜉 ′

𝑛)

(15)

Lemma C.3. Let 𝑐 be a command and 𝑔 : St[PVar] × St□ [Name] → R be a measurable function.

Then, for all 𝜎𝑝 ∈ St[PVar],

∫

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

∫

=

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛), vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

where the integral on the LHS is defined if and only if the one on the RHS is defined.

We now prove Theorem 4.2 using these two lemmas.

Proof of Theorem 4.2. Let 𝜋 be a valid reparameterisation plan, 𝑐 be a command, 𝜎𝜃 ∈ St[𝜃 ], and
ℎ : St[Name] → R be a measurable function. Suppose that the integral on the LHS of Theorem 4.2
is defined. Recall that for a given 𝜎𝑛 ∈ St[Name], the definitions of 𝑝 and 𝑣 (in §3 and §4.1) use the
initial state 𝜎 ≜ 𝜎𝜃 ⊕ 𝜎𝑛 ⊕ 𝜎0 ∈ St, where 𝜎0 ∈ St[(PVar \ 𝜃 ) ∪ AVar] depends on 𝜎𝑛 and has the
following definition:

𝜎0(𝑣) ≜

0
1
N (𝜎𝑛 (𝜇); 0, 1)
𝜎𝑛 (𝜇)
0

if 𝑣 ∈ PVar \ 𝜃
if 𝑣 ≡ like
if 𝑣 ≡ pr 𝜇 for 𝜇 ∈ Name
if 𝑣 ≡ val𝜇 for 𝜇 ∈ Name
if 𝑣 ≡ cnt𝜇 for 𝜇 ∈ Name.






The initial state 𝜎 can be re-expressed as

using the following 𝜎𝑝 ∈ St[PVar] and 𝑓∗ : St[Name] → St[AVar]:

𝜎 = 𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝑓∗ (𝜎𝑛)

(16)

𝜎𝑝 (𝑥) ≜

(cid:40)𝜎𝜃 (𝑥)
0

if 𝑥 ∈ 𝜃
if 𝑥 ∈ PVar \ 𝜃,

𝑓∗ (𝜎𝑛)(𝑎) ≜

1
𝑓1(𝜎𝑛 (𝜇))
𝑓2(𝜎𝑛 (𝜇))
𝑓3(𝜎𝑛 (𝜇))

if 𝑎 ≡ like
if 𝑎 ≡ pr 𝜇 for 𝜇 ∈ Name
if 𝑎 ≡ val𝜇 for 𝜇 ∈ Name
if 𝑎 ≡ cnt𝜇 for 𝜇 ∈ Name,






where 𝑓1(𝑟 ) ≜ N (𝑟 ; 0, 1), 𝑓2(𝑟 ) ≜ 𝑟 , and 𝑓3(𝑟 ) ≜ 0. Using this, we get the desired equation:

∫

(cid:16)

𝑑𝜎𝑛

𝑝𝑐,𝜎𝜃 (𝜎𝑛) · ℎ

(cid:16)
𝑣𝑐,𝜎𝜃 (𝜎𝑛)

(cid:17)(cid:17)

∫

∫

∫

=

=

=

(cid:16)

𝑑𝜎𝑛

prs(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛)) · ℎ

(cid:16)

vals(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛))

(cid:17)(cid:17)

(cid:16)

(cid:16)

𝑑𝜉𝑛

𝑑𝜉𝑛

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

vals□ (𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

prs□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

vals□(𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:34

Wonyeol Lee, Xavier Rival, and Hongseok Yang

∫

∫

=

=

(cid:16)

𝑑𝜎𝑛

prs(𝑐𝜋 )(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛)) · ℎ

(cid:16)

vals(𝑐𝜋 )(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛))

(cid:17)(cid:17)

𝑑𝜎𝑛

(cid:16)
𝑝𝑐𝜋 ,𝜎𝜃 (𝜎𝑛) · ℎ

(cid:16)
𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)

(cid:17)(cid:17)

where 𝑔 : St□ [Name] → R is defined as Eq. (15). The first equality holds by Eq. (16) and the definition
of 𝑝𝑐,𝜎𝜃 , 𝑣𝑐,𝜎𝜃 , prs, and vals. The second equality holds by Lemma C.2 (applied to 𝑐). The third equality
follows from Lemma C.3. The fourth equality holds by Lemma C.2 (applied to 𝑐𝜋 ). The fifth equality
holds by Eq. (16) and the definitions of 𝑝𝑐𝜋 ,𝜎𝜃
, prs, and vals. Note that the same equational
, 𝑣𝑐𝜋 ,𝜎𝜃
reasoning with the reverse direction can be used to prove the claimed equation of the theorem when
□
the integral on the RHS of the equation is defined.

C.2 Proofs of Lemmas C.1 and C.2

𝑟 ∈ St[Var \ (dom(𝜎𝑝 ) ∪ dom(𝜉𝑛))] such that used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎 ′

Proof of Lemma C.1. Let 𝑐 be a command, 𝜎𝑝 ∈ St[PVar], and 𝜉𝑛 ∈ St□ [Name]. Consider
𝜎𝑟 ∈ St[Var \ (dom(𝜎𝑝 ) ∪ dom(𝜉𝑛))] such that used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛). We want to show that
prs□ (𝑐)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐)(𝜎𝑝, 𝜉𝑛), and pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛) do not depend on the choice of 𝜎𝑟 . To do so,
𝑟 , 𝜉𝑛). Let 𝜎 ≜ 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟
consider 𝜎 ′
and 𝜎 ′ ≜ 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎 ′

𝑟 . Then, it suffices to show that
𝑐
(cid:74)
𝑐
(cid:74)
𝑐
(cid:74)
Since used (𝑐, 𝜎, 𝜉𝑛) and used (𝑐, 𝜎 ′, 𝜉𝑛), we have 𝜎 (like) = 1 = 𝜎 ′(like) and so 𝜎 |𝑉 = 𝜎 ′|𝑉 for
𝑉 = PVar ∪ dom(𝜉𝑛) ∪ {like}. Using this and used (𝑐, 𝜎, 𝜉𝑛), we can apply Lemma C.6-(3) and -(4) to
𝜎 ′(𝑣) for all 𝑣 ∈ PVar ∪ {like} ∪ {pr 𝜇, val𝜇 | 𝜇 ∈ dom(𝜉𝑛)}. Hence, we obtain the
get
𝑐
(cid:75)
(cid:74)
□
desired equations in Eq. (17).

𝜎 ′(like) · (cid:206)𝜇 ∈dom(𝜉𝑛)
(cid:75)
𝜎 ′(val𝜇)
(cid:75)
𝜎 ′(𝑥)
(cid:75)

𝜎 (pr 𝜇) =
(cid:75)
𝜎 (val𝜇) =
(cid:75)
𝜎 (𝑥) =
𝑐
(cid:75)
(cid:74)

𝑐
(cid:74)
for all 𝜇 ∈ dom(𝜉𝑛),

𝜎 (like) · (cid:206)𝜇 ∈dom(𝜉𝑛)
(cid:75)

for all 𝑥 ∈ PVar.

𝜎 ′(pr 𝜇),
(cid:75)

𝜎 (𝑣) =
(cid:75)

𝑐
(cid:74)
𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

(17)

Proof of Lemma C.2. Let 𝑐 be a command, ℎ : St[Name] → R be a measurable function,
𝑓∗ : St[Name] → St[AVar] be the function defined in the statement of this lemma, and 𝜎𝑝 ∈ St[PVar].

We first prove that the following equations hold for any measurable ℎ′ : St[Name] → R:

∫

(cid:16)

𝑑𝜎𝑛

1[noerr (𝑐,𝜎𝑝 ⊕𝜎𝑛 ⊕𝑓∗ (𝜎𝑛)) ] · ℎ′(𝜎𝑛)

(cid:17)

=

=

=

=

∫

(cid:16)

𝑑𝜎𝑛

1[noerr (𝑐,𝜎𝑝 ⊕𝜎𝑛 ⊕𝑓∗ (𝜎𝑛)) ] · ℎ′(𝜎𝑛) ·

∑︁

1[used (𝑐,𝜎𝑝 ⊕𝜎𝑛 ⊕𝑓∗ (𝜎𝑛),𝜎𝑛 |𝐾 ) ]

∑︁

∫

𝐾 ⊆Name
∑︁

∫

𝐾 ⊆Name

𝐾 ⊆Name
1[used (𝑐,𝜎𝑝 ⊕𝜎𝑛 ⊕𝑓∗ (𝜎𝑛),𝜎𝑛 |𝐾 ) ] · 1[noerr (𝑐,𝜎𝑝 ⊕𝜎𝑛 ⊕𝑓∗ (𝜎𝑛)) ] · ℎ′(𝜎𝑛)

(cid:16)

𝑑𝜎𝑛

∫

𝑑𝜉𝑛

[𝐾→R]

[Name\𝐾→R]

(cid:16)

𝑑𝜉 ′
𝑛

1[used (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛),𝜉𝑛) ]

(cid:17)

(cid:17)

· 1[noerr (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

∑︁

∫

𝑑𝜉𝑛

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′
∫

(cid:16) ∑︁

𝑛)) ] · ℎ′(𝜉𝑛 ⊕ 𝜉 ′
𝑛)
(cid:16)

𝐾 ⊆Name

[𝐾→R]

𝐿 ⊆Name

[𝐿→R]

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛)) ] · ℎ′(𝜉𝑛 ⊕ 𝜉 ′
𝑛)

(cid:17)

(cid:17)(cid:17)

· 1[noerr (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′
(cid:16)

∫

𝑑𝜉𝑛

𝑑𝜉 ′
𝑛

∫

=

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name] · 1[used (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛),𝜉𝑛) ]

𝑑𝜉 ′
𝑛

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name] · 1[used (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛),𝜉𝑛) ]

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:35

· 1[noerr (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛)) ] · ℎ′(𝜉𝑛 ⊕ 𝜉 ′
𝑛)

(cid:17)

.

All of these equations mean that one side of the equation is defined if and only if the other side
is defined, and when both sides are defined, they are the same. The first equality holds because
noerr (𝑐, 𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝑓∗ (𝜎𝑛)) implies that there exists a unique 𝐾 ⊆ Name with used (𝑐, 𝜎𝑝 ⊕ 𝜎𝑛 ⊕
𝑓∗(𝜎𝑛), 𝜎𝑛 |𝐾 ); here we use 𝑓∗ (𝜎𝑛)(like) = 1. The second equality holds since Name is finite. The third
equality holds because St[Name] is isomorphic to [𝐾 → R] × [Name \ 𝐾 → R]. The fourth equality
holds since 𝜉 ′
𝑛)=Name] = 0. The fifth equality
holds by the definition of St□ [Name] and its underlying measure.

𝑛 ∈ [𝐿 → R] with 𝐿 ≠ Name \ 𝐾 implies 1[dom(𝜉𝑛)⊎dom(𝜉 ′

Using this result, we obtain the desired equation:

∫

=

=

(cid:16)

∫

∫

𝑑𝜎𝑛

𝑑𝜉𝑛

(cid:16)

𝑑𝜉 ′
𝑛

(cid:16)

𝑑𝜎𝑛

prs(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛)) · ℎ

(cid:16)

vals(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛))

(cid:17)(cid:17)

1[noerr (𝑐,𝜎𝑝 ⊕𝜎𝑛 ⊕𝑓∗ (𝜎𝑛)) ] · prs(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛)) · ℎ
∫

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name] · 1[used (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛),𝜉𝑛) ]

(cid:16)

vals(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛))

(cid:17)(cid:17)

· 1[noerr (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′
· prs(𝑐)(𝜎𝑝, 𝜉𝑛 ⊕ 𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′
𝑛)) ]
𝑛, 𝑓∗ (𝜉𝑛 ⊕ 𝜉 ′

𝑛)) · ℎ

(cid:16)

vals(𝑐)(𝜎𝑝, 𝜉𝑛 ⊕ 𝜉 ′

𝑛, 𝑓∗ (𝜉𝑛 ⊕ 𝜉 ′

𝑛))

∫

=

∫

𝑑𝜉𝑛

(cid:16)

𝑑𝜉 ′
𝑛

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name] · 1[used (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛),𝜉𝑛) ]

· prs(𝑐)(𝜎𝑝, 𝜉𝑛 ⊕ 𝜉 ′

𝑛, 𝑓∗ (𝜉𝑛 ⊕ 𝜉 ′

𝑛)) · ℎ

(cid:16)

vals(𝑐)(𝜎𝑝, 𝜉𝑛 ⊕ 𝜉 ′

𝑛, 𝑓∗ (𝜉𝑛 ⊕ 𝜉 ′

𝑛))

(cid:17)(cid:17)

(cid:17)(cid:17)

∫

=

∫

𝑑𝜉𝑛

(cid:16)

𝑑𝜉 ′
𝑛

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name] · 1[used (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′
(cid:17)

(cid:16)

(cid:16) (cid:214)

· prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) ·

𝑓1(𝜉 ′

𝑛 (𝜇))

· ℎ

vals□ (𝑐)(𝜎𝑝, 𝜉𝑛) ⊕

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛),𝜉𝑛) ]

∫

=

∫

𝑑𝜉𝑛

(cid:16)

𝑑𝜉 ′
𝑛

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name]

𝜇 ∈dom(𝜉 ′

𝑛)

· prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) ·

(cid:16) (cid:214)

𝑓1 (𝜉 ′

𝑛 (𝜇))

(cid:17)

(cid:16)

· ℎ

vals□ (𝑐)(𝜎𝑝, 𝜉𝑛) ⊕

∫

=

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) ·

𝜇 ∈dom(𝜉 ′
𝑛)
∫
(cid:16)

𝑑𝜉 ′
𝑛

1[dom(vals□ (𝑐) (𝜎𝑝,𝜉𝑛))⊎dom(𝜉 ′

𝑛)=Name] ·

(cid:16)

(cid:16)

𝜆𝜇 ∈ dom(𝜉 ′

𝑛). 𝑓2(𝜉 ′

𝑛 (𝜇))

𝜆𝜇 ∈ dom(𝜉 ′

𝑛). 𝑓2(𝜉 ′

𝑛 (𝜇))

(cid:17)(cid:17)(cid:17)

(cid:17)(cid:17)(cid:17)

(cid:16) (cid:214)

𝑓1 (𝜉 ′

𝑛 (𝜇))

(cid:17)

𝜇 ∈dom(𝜉 ′

𝑛)

(cid:16)

· ℎ

vals□ (𝑐)(𝜎𝑝, 𝜉𝑛) ⊕

(cid:16)

∫

=

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

vals□ (𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

𝜆𝜇 ∈ dom(𝜉 ′

𝑛). 𝑓2(𝜉 ′

𝑛 (𝜇))

(cid:17)(cid:17)(cid:17)(cid:17)

where 𝑔 : St□ [Name] → R is defined as in the statement of this lemma, and each equation again
means that one side of it is defined if and only if the other side is defined, and when both sides are
defined, they are the same. The first and third equalities hold because prs(𝑐)(𝜎𝑝, 𝜎𝑛, 𝑓∗ (𝜎𝑛)) ≠ 0
implies 1[noerr (𝑐,𝜎𝑝 ⊕𝜎𝑛 ⊕𝑓∗ (𝜎𝑛)) ] = 1. The second equality uses the equation that we have shown in the
previous paragraph. The fourth equality holds because of the following reason: if

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name] · 1[used (𝑐,𝜎𝑝 ⊕𝜉𝑛 ⊕𝜎𝑟 ,𝜉𝑛) ] = 1

for 𝜎𝑟 ≜ 𝜉 ′

𝑛 ⊕ 𝑓∗ (𝜉𝑛 ⊕ 𝜉 ′

𝑛),

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:36

then

Wonyeol Lee, Xavier Rival, and Hongseok Yang

prs(𝑐)(𝜎𝑝, 𝜉𝑛 ⊕ 𝜉 ′

𝑛, 𝑓∗(𝜉𝑛 ⊕ 𝜉 ′

𝑛)) =
=

𝑐
(cid:74)
𝑐
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(like) · (cid:206)𝜇 ∈Name
𝑐
(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(pr 𝜇)
(cid:74)
(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(like) · (cid:206)𝜇 ∈dom(𝜉𝑛)
𝑐
(cid:75)
(cid:75)
(cid:74)
· (cid:206)𝜇 ∈Name\dom(𝜉𝑛) (𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(pr 𝜇)

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(pr 𝜇)

(cid:75)

= prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · (cid:206)𝜇 ∈Name\dom(𝜉𝑛) (𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(pr 𝜇)
= prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · (cid:206)𝜇 ∈dom(𝜉 ′

𝑛) 𝑓1(𝜉 ′

𝑛 (𝜇))

and

vals(𝑐)(𝜎𝑝, 𝜉𝑛 ⊕ 𝜉 ′

𝑛, 𝑓∗ (𝜉𝑛 ⊕ 𝜉 ′

𝑛)) = 𝜆𝜇 ∈ Name.

𝑐
(cid:75)
(cid:74)
= (cid:0)𝜆𝜇 ∈ dom(𝜉𝑛).

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(val𝜇)
𝑐
(cid:74)

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(val𝜇)(cid:1)

(cid:75)

⊕ (cid:0)𝜆𝜇 ∈ Name \ dom(𝜉𝑛). (𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(val𝜇)(cid:1)

= vals□ (𝑐)(𝜎𝑝, 𝜉𝑛) ⊕ (cid:0)𝜆𝜇 ∈ Name \ dom(𝜉 ′
= vals□ (𝑐)(𝜎𝑝, 𝜉𝑛) ⊕ (cid:0)𝜆𝜇 ∈ dom(𝜉 ′
𝑛). 𝑓2(𝜉 ′

𝑛). (𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(val𝜇)(cid:1)
𝑛 (𝜇))(cid:1).

These equalities for prs(𝑐) and vals(𝑐) themselves hold for the below reasons:

• The first equalities hold by used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛) and the definitions of prs(𝑐) and vals(𝑐).
• The second equalities hold by Lemma C.6-(2), which is applicable since used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛).
• The third equalities hold by used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛) and the definitions of prs□ (𝑐) and vals□ (𝑐).
• The fourth equalities hold by dom(𝜉𝑛) ⊎ dom(𝜉 ′

𝑛) = Name and the definition of 𝑓∗.

Returning back to the main equations, we point out that the fifth equality comes from the next fact:

(cid:16)

1[dom(𝜉𝑛)⊎dom(𝜉 ′

𝑛)=Name] · prs□ (𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)

≠ 0 =⇒ 1[used (𝑐,𝜎𝑝 ⊕ (𝜉𝑛 ⊕𝜉 ′

𝑛) ⊕𝑓∗ (𝜉𝑛 ⊕𝜉 ′

𝑛),𝜉𝑛) ] = 1.

The justification for this implication is given below:

• If the premise holds, then there exists 𝜎𝑟 such that used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛). Since 𝜎𝑟 (like) = 1 =
𝑛) coincide on PVar ∪ dom(𝜉𝑛) ∪ {like}.

𝑛) ⊕ 𝑓∗ (𝜉𝑛 ⊕ 𝜉 ′

𝑓∗ (−)(like), 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 and 𝜎𝑝 ⊕ (𝜉𝑛 ⊕ 𝜉 ′
Thus, Lemma C.7 gives the conclusion.

Again back to the main equations, we note that the sixth equality holds since

dom(𝜉𝑛) = dom(vals□ (𝑐)(𝜎𝑝, 𝜉𝑛)),

and the seventh equality follows from the definition of 𝑔. This completes the proof.

□

Lemma C.4. For all commands 𝑐 and states 𝜎 ∈ St such that

for all 𝜇 ∈ Name.

𝑐
(cid:74)

𝜎 (cnt𝜇) ≥ 𝜎 (cnt𝜇)
(cid:75)

and

𝑐
(cid:74)

𝜎 ∈ St, we have
(cid:75)

𝑐
(cid:74)
𝜎 (𝜇) = 𝜎 (𝜇)
(cid:75)

Proof. We prove the lemma by induction on the structure of 𝑐. Let 𝜎 ∈ St such that

𝑐
(cid:74)

Cases 𝑐 ≡ skip, or 𝑐 ≡ (𝑥 := 𝑒), or 𝑐 ≡ obs(𝑑, 𝑟 ). In these cases,
𝜎 (𝜇) = 𝜇 for all 𝜇. The claim of the lemma, thus, follows.
(cid:75)
Case 𝑐 ≡ (𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒 ′)). Let 𝜇 ≜

𝑛
(cid:74)

𝜎, 𝑝 ≜
(cid:75)

𝑑
(cid:74)

𝜎, and 𝑟 ≜
(cid:75)

𝑒 ′[𝜇/𝑦]
(cid:74)

𝜎. Then,
(cid:75)

𝑐
(cid:74)
Thus, the claim of the lemma follows.

𝜎 = 𝜎 [𝑥 ↦→ 𝑟, val𝜇 ↦→ 𝑟, pr 𝜇 ↦→ 𝑝 (𝑟 ), cnt𝜇 ↦→ 𝜎 (cnt𝜇) + 1].
(cid:75)

𝑐
(cid:74)
𝜎 (cnt𝜇) = 𝜎 (cnt𝜇) and
(cid:75)

𝜎 ∈ St.
(cid:75)

𝑐
(cid:74)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:37

Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). Pick 𝜇 ∈ Name. Then,
𝑐 ′′
(cid:74)

𝜎 (𝜇) =
(cid:75)

𝑐 ′; 𝑐 ′′
(cid:74)

(

𝑐 ′
(cid:74)

𝜎)(𝜇) =
(cid:75)

(cid:75)

𝑐 ′
(cid:74)

𝜎 (𝜇) = 𝜎 (𝜇).
(cid:75)

Here the second and third equalities use induction hypothesis on 𝑐 ′ and 𝑐 ′′, respectively. Also,
(cid:17)

(cid:17)

(cid:16)

(cid:16)

𝑐 ′; 𝑐 ′′
(cid:74)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) =
(cid:75)

(

𝑐 ′
(cid:74)

𝜎)(cnt𝜇) −
(cid:75)

𝑐 ′
(cid:74)

𝜎 (cnt𝜇)
(cid:75)

(cid:75)

+

𝑐 ′
(cid:74)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇)
(cid:75)

𝑐 ′′
(cid:74)
≥ 0.

The inequality here uses induction hypothesis on 𝑐 ′ and 𝑐 ′′.

Case 𝑐 ≡ (if 𝑏 {𝑐 ′} else {𝑐 ′′}). Assume that

under this assumption. The other case of
by induction hypothesis on 𝑐 ′,

𝑏
(cid:74)

(cid:75)

𝜎 = true. We will prove the claims of the lemma
(cid:75)
= false can be proved similarly. Pick 𝜇 ∈ Name. Then,

𝑏
(cid:74)

𝑐
(cid:74)

𝜎 (𝜇) =
(cid:75)

𝑐 ′
(cid:74)

𝜎 (𝜇) = 𝜎 (𝜇)
(cid:75)

and

𝑐
(cid:74)

𝜎 (cnt𝜇) =
(cid:75)

′

𝜎 (cnt𝜇) ≥ 𝜎 (cnt𝜇).
(cid:75)

(cid:74)

Case 𝑐 ≡ (while 𝑏 {𝑐 ′}). Let T be the following subset of [St → St⊥]:

𝑓 ∈ T ⇐⇒ ∀𝜎 ∈ St.

(cid:16)

𝑓 (𝜎) ≠ ⊥ =⇒ ∀𝜇 ∈ Name. 𝑓 (𝜎)(𝜇) = 𝜎 (𝜇) ∧ 𝑓 (𝜎)(cnt𝜇) ≥ 𝜎 (cnt𝜇)

(cid:17)

.

Let 𝐹 be the operator on [St → St⊥] whose least fixed point becomes the semantics of the loop 𝑐.
The desired conclusion follows if we show that T contains 𝜆𝜎.⊥ and is closed under taking the limit
of a chain in T , and 𝐹 preserves T . The least element 𝜆𝜎.⊥ belongs to T since there are no states
𝜎 with (𝜆𝜎.⊥)(𝜎) ≠ ⊥. Consider an increasing sequence 𝑓0, 𝑓1, . . . in T , and let 𝑓∞ ≜ (cid:195)𝑛 ∈N 𝑓𝑛. Pick
𝜎 such that 𝑓∞ (𝜎) ≠ ⊥. Then, 𝑓∞ (𝜎) = 𝑓𝑚 (𝜎) for some 𝑚 ∈ N. Since 𝑓𝑚 ∈ T , we have

𝑓𝑚 (𝜎)(𝜇) = 𝜎 (𝜇)

and

𝑓𝑚 (𝜎)(cnt𝜇) ≥ 𝜎 (cnt𝜇)

for all 𝜇 ∈ Name. Since 𝑓𝑚 (𝜎) = 𝑓∞ (𝜎), we also have, for every 𝜇 ∈ Name, 𝑓∞ (𝜎)(𝜇) = 𝜎 (𝜇) and
𝑓∞(𝜎)(cnt𝜇) ≥ 𝜎 (cnt𝜇), as desired. It remains to show that 𝐹 (𝑓 ) ∈ T for all 𝑓 ∈ T . Pick 𝑓 ∈ T and
𝜎 ∈ St such that 𝐹 (𝑓 )(𝜎) ∈ St. If
𝜎 = false, we have 𝐹 (𝑓 )(𝜎) = 𝜎, and the claims of the lemma
(cid:75)
𝜎). Pick 𝜇 ∈ Name. Then, by induction hypothesis on 𝑐 ′ and the
follow. Otherwise, 𝐹 (𝑓 )(𝜎) = 𝑓 (
(cid:75)
membership 𝑓 ∈ T ,
𝐹 (𝑓 )(𝜎)(𝜇) = 𝑓 (

𝑏
(cid:74)
𝑐 ′
(cid:74)

and

𝐹 (𝑓 )(𝜎)(cnt𝜇) = 𝑓 (

We have just shown that 𝐹 (𝑓 ) ∈ T , as desired.

𝜎)(𝜇) =
(cid:75)

𝑐 ′
(cid:74)
𝜎)(cnt𝜇) ≥
(cid:75)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝜎 (𝜇) = 𝜎 (𝜇),
(cid:75)
𝜎 (cnt𝜇) ≥ 𝜎 (cnt𝜇).
(cid:75)

𝑐 ′
(cid:74)

□

Definition C.5. Define used− as the predicate used but without the condition that like should be

1. That is, for all commands 𝑐, states 𝜎 ∈ St, and 𝜉𝑛 ∈ St□ [Name],

used− (𝑐, 𝜎, 𝜉𝑛) ⇐⇒

𝑐
(cid:74)

𝜎 ∈ St ∧ (cid:0)
(cid:75)

𝑐
(cid:74)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) ≤ 1 for all 𝜇 ∈ Name(cid:1)
(cid:75)

∧ 𝜉𝑛 = 𝜎 |dom(𝜉𝑛)
∧ dom(𝜉𝑛) = {𝜇 ∈ Name |
Lemma C.6. Let 𝑐 be a command, 𝜎0, 𝜎1 ∈ St, and 𝜉𝑛 ∈ St□ [Name]. Suppose that used− (𝑐, 𝜎0, 𝜉𝑛)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) = 1}.
(cid:75)

𝑐
(cid:74)

and 𝜎1|𝑉 = 𝜎0|𝑉 for 𝑉 ≜ PVar ∪ dom(𝜉𝑛). Then, the following properties hold:

(1)
(2)
(3)
(4)
(5)

𝑐
(cid:74)
𝑐
(cid:74)
𝑐
(cid:74)
𝑐
(cid:74)
𝑐
(cid:74)

𝜎1 ∈ St.
(cid:75)
𝜎1(𝑎) = 𝜎1(𝑎) for all 𝑎 ∈ {pr 𝜇, val𝜇, cnt𝜇 | 𝜇 ∈ Name \ dom(𝜉𝑛)}.
(cid:75)
𝑐
𝜎1(𝑣) =
(cid:75)
(cid:74)
𝜎1(like) =
(cid:75)
𝜎1(𝑎) − 𝜎1(𝑎) =
(cid:75)

𝜎0(𝑣) for all 𝑣 ∈ PVar ∪ {pr 𝜇, val𝜇 | 𝜇 ∈ dom(𝜉𝑛)}.
(cid:75)
𝑐
(cid:74)

𝜎0(𝑎) − 𝜎0(𝑎) for all 𝑎 ∈ {cnt𝜇 | 𝜇 ∈ Name}.
(cid:75)

𝜎0(like), if 𝜎0(like) = 𝜎1(like).
(cid:75)

𝑐
(cid:74)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:38

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Proof. For 𝜎 ′

0, 𝜎 ′

1 ∈ St and 𝜉 ′

𝑛 ∈ St□ [Name], write
𝜎 ′
1

𝜎 ′
0 ∼𝜉 ′
1|𝑉 for 𝑉 ≜ PVar ∪ dom(𝜉 ′

𝑛

to mean that 𝜎 ′
0|𝑉 = 𝜎 ′
conditions of the lemma as follows:

𝑛). Note that using this notation, we can write the

used− (𝑐, 𝜎0, 𝜉𝑛) ∧ 𝜎0 ∼𝜉𝑛 𝜎1.
We will prove, by induction on the structure of 𝑐, that these conditions imply the five properties
claimed by the lemma. Our proof will sometimes use a simple observation that the five properties
claimed by the lemma and the relationship 𝜎0 ∼𝜉𝑛 𝜎1 imply used− (𝑐, 𝜎1, 𝜉𝑛). One consequence of the
observation is that if our lemma holds, its five properties also hold with 𝜎0 and 𝜎1 swapped. We will
often use this consequence.

𝑐
(cid:74)

𝜎0 = 𝜎0 and
(cid:75)

Case 𝑐 ≡ skip. In this case,

𝜎1 = 𝜎1. From these equalities, the claimed prop-
𝑐
(cid:75)
(cid:74)
erties (1), (2), (4) and (5) follow. For the remaining property (3), we note that dom(𝜉𝑛) = ∅ and the
property, thus, follows from 𝜎0 ∼𝜉𝑛 𝜎1.
Case 𝑐 ≡ (𝑥 := 𝑒). In this case,

𝜎1]. The results
(cid:75)
are not ⊥, and they are identical to the pre-states 𝜎0 and 𝜎1 as far as auxiliary variables in AVar are
concerned. Also, expressions in commands do not depend on variables other than program variables,
𝜎1(𝑥) for all 𝑥 ∈ PVar. From all of these
so that 𝜎0 ∼𝜉𝑛 𝜎1 gives
𝜎0(𝑥) =
𝜎0 =
𝑒
(cid:75)
(cid:75)
(cid:75)
(cid:74)
observations, the claimed properties (1)–(5) follow.

𝜎0 = 𝜎0 [𝑥 ↦→
(cid:75)

𝜎1 = 𝜎1 [𝑥 ↦→
(cid:75)

𝜎0] and
(cid:75)

𝜎1 and
(cid:75)

𝑒
(cid:74)

𝑒
(cid:74)

𝑒
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

Case 𝑐 ≡ (𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒 ′)). Since 𝜎0(𝑥) = 𝜎1(𝑥) for all 𝑥 ∈ PVar, we have

and
𝜎0 =
(cid:75)
commands, we have

𝜎1. Let 𝜇 ≜
(cid:75)

𝑑
(cid:74)

𝑑
(cid:74)

𝑛
(cid:74)

𝜎0, 𝑝 ≜
(cid:75)

𝑑
(cid:74)

𝜎0, and 𝑟 ≜
(cid:75)

𝑒 ′[𝜇/𝑦]
(cid:74)

𝜎1
(cid:75)
𝜎0. By the semantics of the sample
(cid:75)

𝜎0 =
(cid:75)

𝑛
(cid:74)

𝑛
(cid:74)

𝜎0 = 𝜎0 [𝑥 ↦→ 𝑟, val𝜇 ↦→ 𝑟, pr 𝜇 ↦→ 𝑝 (𝜎0(𝜇)), cnt𝜇 ↦→ 𝜎0(cnt𝜇) + 1].
(cid:75)
Since used−(𝑐, 𝜎0, 𝜉𝑛) holds, we have 𝜉𝑛 = 𝜎0| {𝜇 }, which in turn implies 𝜎0(𝜇) = 𝜎1(𝜇) because
𝜎0 ∼𝜉𝑛 𝜎1. Thus,

𝑐
(cid:74)

𝑒 ′[𝜇/𝑦]
(cid:74)

𝜎0 = 𝑟 , and
𝑒 ′[𝜇/𝑦]
(cid:74)
(cid:75)
𝑒 ′[𝜇/𝑦]
𝜎1, val𝜇 ↦→
(cid:74)
(cid:75)
= 𝜎1 [𝑥 ↦→ 𝑟, val𝜇 ↦→ 𝑟, pr 𝜇 ↦→ 𝑝 (𝜎0(𝜇)), cnt𝜇 ↦→ 𝜎1(cnt𝜇) + 1].

𝜎1 =
(cid:75)
𝑒 ′[𝜇/𝑦]
(cid:74)

𝜎1 = 𝜎1 [𝑥 ↦→
(cid:75)

𝜎1, pr 𝜇 ↦→ 𝑝 (𝜎1(𝜇)), cnt𝜇 ↦→ 𝜎1(cnt𝜇) + 1]
(cid:75)

𝑐
(cid:74)

The RHS of the last equality implies that the five properties claimed by the lemma hold.

Case 𝑐 ≡ obs(𝑑, 𝑟 ). We have

Then,

𝑑
(cid:74)

𝜎0 =
(cid:75)

𝑑
(cid:74)

𝜎1 since 𝜎0(𝑥) = 𝜎1(𝑥) for all 𝑥 ∈ PVar. Let 𝑝 ≜
(cid:75)

𝑑
(cid:74)

𝜎0.
(cid:75)

𝑐
(cid:74)

𝜎0 = 𝜎0 [like ↦→ 𝜎0(like) · 𝑝 (𝑟 )]
(cid:75)

and
Also, dom(𝜉𝑛) = ∅ since used− (𝑐, 𝜎0, 𝜉𝑛) holds. From what we have proved and also the agreement
of 𝜎0 and 𝜎1 on program variables, the five properties claimed by the lemma follow.

𝜎1 = 𝜎1 [like ↦→ 𝜎1(like) · 𝑝 (𝑟 )].
(cid:75)

𝑐
(cid:74)

Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). Since
𝜎0 =
𝑐
𝑐 ′′
(cid:75)
(cid:74)
(cid:74)
𝜎 ′
𝑐 ′
0 ≜
𝜎0,
(cid:74)
(cid:75)
𝑁0 ≜ {𝜇 ∈ Name |
𝑁 ′

(cid:75)

† (

𝑐 ′′
(cid:74)
0 ≜ {𝜇 ∈ Name | 𝜎 ′
0(cnt𝜇) − 𝜎0(cnt𝜇) = 1}.

𝜎 ′
0(cnt𝜇) − 𝜎0(cnt𝜇) = 1},
(cid:75)

𝑐 ′
(cid:74)

𝜎0) ∈ St, we have
(cid:75)

𝑐 ′
(cid:74)

𝜎0 ∈ St. Let
(cid:75)

Then, 𝑁0 = dom(𝜉𝑛) because used− (𝑐 ′; 𝑐 ′′, 𝜎0, 𝜉𝑛) holds. We will prove the following facts:

0 ⊆ 𝑁0.

(1) 𝑁 ′
(2) Let 𝜉 ′
(3)
𝑐 ′
(cid:74)

𝑛 ≜ 𝜉𝑛 |𝑁 ′
𝜎1 ∈ St.
(cid:75)

0

, and 𝜉 ′′

𝑛 ≜ 𝜉𝑛 | (𝑁0\𝑁 ′

0) . Then, used− (𝑐 ′, 𝜎0, 𝜉 ′

𝑛) and used−(𝑐 ′′, 𝜎 ′

0, 𝜉 ′′

𝑛 ) hold.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:39

(4) Let 𝜎 ′

𝜎1. Then, 𝜎 ′
(cid:75)

1 ≜

𝑐 ′
(cid:74)
𝜎1 since dom(𝜉 ′

0 ∼𝜉 ′′

𝑛

1.
𝜎 ′

𝑛

𝑛) = 𝑁 ′

These four facts imply the five properties claimed by the lemma. Here is the reason. Note that
0 ⊆ 𝑁0 = dom(𝜉𝑛). This relationship between 𝜎0 and 𝜎1 and the second
𝜎0 ∼𝜉 ′
𝑛, 𝜎0, 𝜎1). Also, the second and fourth facts allow us to
fact let us use induction hypothesis on (𝑐 ′, 𝜉 ′
use induction hypothesis on (𝑐 ′′, 𝜉 ′′
1). We can derive the five properties from what we get from
these two applications of induction hypothesis:
0, 𝜎 ′

𝑛 , 𝜎 ′

𝑛 , 𝜎 ′

0, 𝜎 ′

(1) By induction hypothesis on (𝑐 ′′, 𝜉 ′′
1), we have
(2) For all 𝑎 ∈ {pr 𝜇, val𝜇, cnt𝜇 | 𝜇 ∈ Name \ dom(𝜉𝑛)},
1(𝑎) =

𝑐 ′; 𝑐 ′′
(cid:74)

𝜎1(𝑎) =
(cid:75)

1(𝑎) = 𝜎 ′
𝜎 ′
(cid:75)
The second equality comes from induction hypothesis on (𝑐 ′′, 𝜉 ′′
dom(𝜉𝑛), and the fourth equality from induction hypothesis on (𝑐 ′, 𝜉 ′
dom(𝜉𝑛).

𝜎1(𝑎) = 𝜎1(𝑎).
(cid:75)
𝑛 , 𝜎 ′

𝑐 ′′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′; 𝑐 ′′
(cid:74)

𝜎1 =
(cid:75)

𝑐 ′′
(cid:74)

1 ∈ St.
𝜎 ′
(cid:75)

0, 𝜎 ′

1) and dom(𝜉 ′′
𝑛, 𝜎0, 𝜎1) and dom(𝜉 ′

𝑛 ) ⊆
𝑛) ⊆

(3) For all 𝑣 ∈ PVar ∪ {pr 𝜇, val𝜇 | 𝜇 ∈ dom(𝜉 ′′
𝜎 ′
𝑐 ′; 𝑐 ′′
1 (𝑣) =
(cid:75)
(cid:74)

𝜎1(𝑣) =
(cid:75)

𝑐 ′′
(cid:74)

Also, for all 𝑎 ∈ {pr 𝜇, val𝜇 | 𝜇 ∈ dom(𝜉 ′
and we can calculate:

𝑛 )}, by induction hypothesis on (𝑐 ′′, 𝜉 ′′
𝑐 ′; 𝑐 ′′
(cid:74)

𝜎0(𝑣).
(cid:75)
𝑛)}, we have 𝑎 ∈ {pr 𝜇, val𝜇 | 𝜇 ∈ Name \ dom(𝜉 ′′

𝜎 ′
0 (𝑣) =
(cid:75)

𝑐 ′′
(cid:74)

1),
0, 𝜎 ′

𝑛 , 𝜎 ′

𝑛 )},

𝑐 ′; 𝑐 ′′
(cid:74)

𝑐 ′′
1(𝑎) = 𝜎 ′
𝜎 ′
𝜎1(𝑎) =
1(𝑎)
(cid:74)
(cid:75)
(cid:75)
𝑐 ′
𝑐 ′
𝜎1 (𝑎) =
𝜎0(𝑎)
=
(cid:74)
(cid:74)
(cid:75)
(cid:75)
= 𝜎 ′
𝜎 ′
𝑐 ′′
𝑐 ′; 𝑐 ′′
0(𝑎) =
0(𝑎) =
(cid:75)
(cid:74)
(cid:74)
The second equality uses induction hypothesis on (𝑐 ′′, 𝜉 ′′
𝑛 , 𝜎 ′
induction hypothesis on (𝑐 ′, 𝜉 ′
applied to (𝑐 ′′, 𝜉 ′′

𝑛 , 𝜎 ′

0, 𝜎 ′

1) and again to the same tuple but with 𝜎 ′
𝑛, 𝜎0, 𝜎1),

(4) If 𝜎0(like) = 𝜎1(like), by induction hypothesis on (𝑐 ′, 𝜉 ′
𝜎 ′
0(like) =

𝑐 ′
(cid:74)

𝜎0(like) =
(cid:75)

𝑐 ′
(cid:74)

𝜎1(like) = 𝜎 ′
(cid:75)
𝑛 , 𝜎 ′

which in turn implies, by induction hypothesis on (𝑐 ′′, 𝜉 ′′

𝜎0(𝑎).
(cid:75)
1), and the fourth comes from the
0, 𝜎 ′
𝑛, 𝜎0, 𝜎1). The sixth equality follows from induction hypothesis

0 and 𝜎 ′

1 swapped.

𝜎 ′
0(like) =
(cid:75)

𝑐 ′′
(cid:74)

𝜎 ′
1 (like) =
(cid:75)

𝜎1(like).
(cid:75)

1(like),
1),
0, 𝜎 ′
𝑐 ′; 𝑐 ′′
(cid:74)

𝑐 ′; 𝑐 ′′
(cid:74)

𝜎0(like) =
(cid:75)

𝑐 ′′
(cid:74)
(5) For all 𝑎 ∈ {cnt𝜇 | 𝜇 ∈ Name},
𝑐 ′; 𝑐 ′′
(cid:74)

𝜎1(𝑎) − 𝜎1(𝑎) =
(cid:75)
=

𝑐 ′; 𝑐 ′′
𝑐 ′
𝜎1(𝑎) −
𝜎1(𝑎) +
(cid:75)
(cid:74)
(cid:74)
(cid:75)
𝜎 ′
1(𝑎) − 𝜎 ′
𝑐 ′′
𝑐 ′
1(𝑎) +
(cid:75)
(cid:74)
(cid:74)
𝑐 ′
0(𝑎) − 𝜎 ′
𝜎 ′
𝑐 ′′
0(𝑎) +
(cid:74)
(cid:74)
(cid:75)
𝑐 ′; 𝑐 ′′
𝜎0(𝑎) − 𝜎0(𝑎).
(cid:75)
(cid:74)
The only non-trivial inequality is the third one, and it follows from induction hypothesis on
(𝑐 ′, 𝜉 ′

𝜎1(𝑎) − 𝜎1(𝑎)
(cid:75)
𝜎1(𝑎) − 𝜎1(𝑎)
(cid:75)
𝜎0(𝑎) − 𝜎0(𝑎)
(cid:75)

𝑐 ′
(cid:74)

=

=

1).
𝑛, 𝜎0, 𝜎1) and (𝑐 ′′, 𝜉 ′′
0, 𝜎 ′
We prove the four facts as follows:
(1) Let 𝜇 ∈ 𝑁 ′

𝑛 , 𝜎 ′

0. Since used− (𝑐 ′; 𝑐 ′′, 𝜎0, 𝜉𝑛), we have
𝜎 ′
0(𝜇) − 𝜎0(𝜇) =
(cid:75)

𝑐 ′; 𝑐 ′′
𝑐 ′′
(cid:74)
(cid:74)
0,
Also, by Lemma C.4 and the definition of 𝑁 ′

𝜎0 − 𝜎0(𝜇) ≤ 1.
(cid:75)

0(𝜇) − 𝜎0(𝜇) ≥ 𝜎 ′
𝜎 ′
(cid:75)
0(𝜇) − 𝜎0(𝜇) = 1, which implies that 𝜇 ∈ 𝑁0, as desired.
𝜎 ′
(cid:75)

0(𝜇) − 𝜎0(𝜇) = 1.

𝑐 ′′
(cid:74)

Thus,

𝑐 ′′
(cid:74)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:40

Wonyeol Lee, Xavier Rival, and Hongseok Yang

(2) We should show that used− (𝑐 ′, 𝜎0, 𝜉 ′

𝑛 ) hold. The conjuncts in the defini-
0, 𝜉 ′′
𝑛) except the second follow immediately from used− (𝑐 ′; 𝑐 ′′, 𝜎0, 𝜉𝑛) and the
tion of used− (𝑐 ′, 𝜎0, 𝜉 ′
definition of 𝜉𝑛. For the remaining second conjunct, we use Lemma C.4 and used− (𝑐 ′; 𝑐 ′′, 𝜎0, 𝜉𝑛),
and prove the conjunct as shown below: for all 𝜇 ∈ Name,

𝑛) and used− (𝑐 ′′, 𝜎 ′

𝑐 ′
(cid:74)

𝜎0(cnt𝜇) − 𝜎0 (cnt𝜇) ≤
(cid:75)
For used− (𝑐 ′′, 𝜎 ′
0, 𝜉 ′′
consequences of used− (𝑐 ′; 𝑐 ′′, 𝜎0, 𝜉𝑛) and the definition of 𝜉 ′′
in the definition as follows: for all 𝜇 ∈ Name,

𝑛 ), we first note that the first and third conjuncts in its definition are direct
𝑛 . We prove the second conjunct

𝜎0)(cnt𝜇) − 𝜎0(cnt𝜇) ≤ 1.
(cid:75)

𝑐 ′′
(cid:74)

𝑐 ′
(cid:74)

(cid:75)

(

𝑐 ′′
(cid:74)

0(cnt𝜇) − 𝜎 ′
𝜎 ′
(cid:75)

𝑐 ′
𝑐 ′; 𝑐 ′′
𝜎0(cnt𝜇) −
0(cnt𝜇) =
(cid:74)
(cid:75)
(cid:74)
𝑐 ′; 𝑐 ′′
𝜎0 (cnt𝜇) − cnt0(cnt𝜇)
≤
(cid:75)
(cid:74)
≤ 1.

cnt0(cnt𝜇)
(cid:75)

The first inequality uses Lemma C.4, and the second comes from used−(𝑐 ′; 𝑐 ′′, 𝜎0, 𝜉𝑛). It remains
𝑛 ), which we do below: for all
to show the fourth conjunct in the definition of used− (𝑐 ′′, 𝜎 ′
𝜇 ∈ Name,

0, 𝜉 ′′

𝑐 ′′
(cid:74)

𝑐 ′′
(cid:74)

0(cnt𝜇) = 1 ∧ 𝜎 ′

0(cnt𝜇) − 𝜎 ′
𝜎 ′
0(cnt𝜇) = 1
(cid:75)
𝜎 ′
0(cnt𝜇) − 𝜎 ′
⇐⇒
(cid:75)
⇐⇒ 𝜇 ∈ 𝑁0 ∧ 𝜇 ∉ 𝑁 ′
0
⇐⇒ 𝜇 ∈ dom(𝜉 ′′
The first equivalence comes from Lemma C.4 and
0(cnt𝜇) − 𝜎0(cnt𝜇) ≤ 1, which holds be-
𝜎 ′
(cid:75)
0.
cause of used− (𝑐 ′; 𝑐 ′′, 𝜎0, 𝜉𝑛). The second equivalence follows from the definitions of 𝑁0 and 𝑁 ′
𝑛), we can apply induction

𝑐 ′′
(cid:74)
𝜎1 and we have used− (𝑐 ′, 𝜎0, 𝜉 ′

0(cnt𝜇) − 𝜎0(cnt𝜇) = 0

(3) Since 𝜎0 ∼𝜉𝑛 𝜎1 implies 𝜎0 ∼𝜉 ′

hypothesis to (𝑐 ′, 𝜉 ′

𝜎1 ∈ St.
(cid:75)
(4) We continue our reasoning in the previous item, and derive from induction hypothesis on

𝑛, 𝜎0, 𝜎1), and get

𝑐 ′
(cid:74)

𝑛 ).

𝑛

(𝑐 ′, 𝜉 ′

𝑛, 𝜎0, 𝜎1) the fact that for all 𝑥 ∈ PVar,
𝜎 ′
𝜎0(𝑥) =
0(𝑥) =
(cid:75)

𝑐 ′
(cid:74)

Also, for all 𝜇 ∈ dom(𝜉 ′′

𝑛 ),

𝑐 ′
(cid:74)

𝜎1(𝑥) = 𝜎 ′
(cid:75)

1(𝑥).

0(𝜇) = 𝜎0(𝜇) = 𝜎1(𝜇) = 𝜎 ′
𝜎 ′

1(𝜇),

𝑏
(cid:74)

where the first and third equalities come from Lemma C.4, and the second equality follows
from the assumption that 𝜎0 ∼𝜉𝑛 𝜎1.
Case 𝑐 ≡ (if 𝑏 {𝑐 ′} else 𝑐 ′′). Assume that

𝜎0. We prove the five
𝜎0 = true. Then,
(cid:75)
(cid:75)
properties claimed by the lemma under this assumption. The proof for the other possibility, namely,
𝜎0 = false is similar. Since 𝜎0 ∼𝜉𝑛 𝜎1, the states 𝜎0 and 𝜎1 coincide for the values of program
𝑏
(cid:74)
(cid:75)
𝜎0 as well, it suffices to show
𝜎1. Since
variables. Thus,
(cid:75)
(cid:75)
the five properties claimed by the lemma for (𝑐 ′, 𝜎0, 𝜎1, 𝜉𝑛). This sufficient condition follows from
𝜎0 = true imply used−(𝑐 ′, 𝜎0, 𝜉𝑛).
induction hypothesis on (𝑐 ′, 𝜎0, 𝜎1, 𝜉𝑛), since used− (𝑐, 𝜎0, 𝜉𝑛) and
(cid:75)
Case 𝑐 ≡ (while 𝑏 {𝑐 ′}). Consider the version of used− where the first parameter can be a state
transformer 𝑓 : St → St⊥, instead of a command. Similarly, consider the version of the five properties
claimed by the lemma where we use a state transformer 𝑓 : St → St⊥, again instead of a command.
We denote both versions by used−(𝑓 , 𝜎 ′′
1 ). Let T be the subset of St → St⊥
𝑛 ) and 𝜑 (𝑓 , 𝜉 ′′
defined by

𝜎1 = true, and
(cid:75)

𝑐 ′
(cid:74)
𝑏
(cid:74)

𝜎0 =
(cid:75)

𝜎1 =
(cid:75)

𝜎0 =
(cid:75)

0 , 𝜎 ′′

𝑛 , 𝜎 ′′

0 , 𝜉 ′′

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑏
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑓 ∈ T ⇐⇒ ∀𝜎 ′′

0 , 𝜎 ′′

1 ∈ St. ∀𝜉 ′′

𝑛 ∈ St□.

(cid:16)(cid:16)

used−(𝑓 , 𝜎 ′′

0 , 𝜉 ′′

𝑛 ) ∧ 𝜎 ′′

0 ∼𝜉 ′′

𝑛

(cid:17)

𝜎 ′′
1

=⇒ 𝜑 (𝑓 , 𝜉 ′′

𝑛 , 𝜎 ′′

0 , 𝜎 ′′
1 )

(cid:17)

,

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

and 𝐹 : [St → St⊥] → [St → St⊥] be the following operator used in the semantics of the loop

0:41

𝑐
(cid:74)

:
(cid:75)

𝐹 (𝑓 )(𝜎) ≜ if (

𝑏
(cid:74)

𝜎 = true) then 𝑓 † (
(cid:75)

𝑐 ′
(cid:74)

𝜎) else 𝜎.
(cid:75)

We will show that T contains 𝜆𝜎. ⊥ and is closed under taking the least upper bound of an increasing
chain in [St → St⊥], and the operator 𝐹 preserves T . These three conditions imply that
is in T ,
which in turn gives the five properties claimed by the lemma.
The first condition holds simply because used− ((𝜆𝜎.⊥), 𝜎 ′′

𝑐
(cid:74)
𝑛 . To prove
0 and 𝜉 ′′
the closure under the least upper bound of a chain, consider an increasing sequence 𝑓0, 𝑓1, . . . in T .
Let 𝑓∞ ≜ (cid:195)𝑛 ∈N 𝑓𝑛. Consider 𝜎 ′′
𝑛 ).
0 , 𝜉 ′′
1 ) holds. By the definition of 𝑓∞, there exists 𝑚 ∈ N such that
We should show that 𝜑 (𝑓∞, 𝜉 ′′
𝑓∞(𝜎0) = 𝑓𝑚 (𝜎0). Then, the assumption used−(𝑓∞, 𝜎 ′′
𝑛 ). This in turn
𝑛 ) implies used− (𝑓𝑚, 𝜎 ′′
gives 𝜑 (𝑓𝑚, 𝜉 ′′
1 ) because 𝑓𝑚 ∈ T . By what we have proved and the definition of 𝑓∞, we have

1 ∈ St and 𝜉 ′′
0 , 𝜎 ′′

1 and used− (𝑓∞, 𝜎 ′′
𝜎 ′′

𝑛 ∈ St□ such that 𝜎 ′′

𝑛 ) is false for all 𝜎 ′′

0 , 𝜎 ′′
𝑛 , 𝜎 ′′

0 , 𝜎 ′′

0 ∼𝜉 ′′

𝑛 , 𝜎 ′′

0 , 𝜉 ′′

0 , 𝜉 ′′

0 , 𝜉 ′′

(cid:75)

𝑛

𝑓𝑚 (𝜎 ′′

0 ) = 𝑓∞ (𝜎 ′′

0 ) ∈ St

and

𝑓𝑚 (𝜎 ′′

1 ) = 𝑓∞ (𝜎 ′′

1 ) ∈ St.

𝑛 , 𝜎 ′′

0 , 𝜎 ′′

1 ) entails 𝜑 (𝑓∞, 𝜉 ′′

Thus, 𝜑 (𝑓𝑚, 𝜉 ′′
1 ), as desired. It remains to show that 𝐹 (𝑓 ) ∈ T for
all 𝑓 ∈ T . Pick 𝑓 ∈ T . We first replay our proof for the sequential-composition case after view-
as the sequential composition of 𝑐 ′ and 𝑓 . This replay, then, gives the membership
ing 𝑓 † ◦
𝑐 ′
(cid:74)
as the true
∈ T . Next, we replay our proof for the if case on 𝐹 (𝑓 ) after viewing 𝑓 † ◦
𝑐 ′
𝑓 † ◦
(cid:74)
(cid:75)
□
branch and 𝜆𝜎. 𝜎 =

as the false branch. This replay implies the required 𝐹 (𝑓 ) ∈ T .

0 , 𝜎 ′′

𝑛 , 𝜎 ′′

𝑐 ′
(cid:74)

(cid:75)

(cid:75)

skip
(cid:75)

(cid:74)

Lemma C.7. Let 𝑐 be a command, 𝜎, 𝜎 ′ ∈ St, and 𝜉𝑛 ∈ St□ [Name].
• If 𝜎 |𝑉 = 𝜎 ′|𝑉 for 𝑉 ≜ PVar ∪ dom(𝜉𝑛) ∪ {like}, then used (𝑐, 𝜎, 𝜉𝑛) implies used (𝑐, 𝜎 ′, 𝜉𝑛).
• If 𝜎 |𝑈 = 𝜎 ′|𝑈 for 𝑈 ≜ PVar ∪ dom(𝜉𝑛), then used− (𝑐, 𝜎, 𝜉𝑛) implies used− (𝑐, 𝜎 ′, 𝜉𝑛).

Proof. Assume the settings in the statement of this lemma. For the first claim, assume used (𝑐, 𝜎, 𝜉𝑛).

Then, by the definition of used and noerr,

𝜎 ∈ St ∧ (cid:0)∀𝜇 ∈ Name.
𝑐
(cid:74)
(cid:75)
∧ (cid:0)𝜉𝑛 = 𝜎 |dom(𝜉𝑛)

(cid:1) ∧ (cid:0)dom(𝜉𝑛) = {𝜇 ∈ Name |

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) ≤ 1(cid:1) ∧ (cid:0)𝜎 (like) = 1(cid:1)
(cid:75)
𝑐
(cid:74)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) = 1}(cid:1).
(cid:75)

𝑐
(cid:74)

From this and Lemma C.6 (which is applicable since used (𝑐, 𝜎, 𝜉𝑛) and 𝜎 |𝑉 = 𝜎 ′|𝑉 ), we obtain

𝜎 ′ ∈ St ∧ (cid:0)∀𝜇 ∈ Name.
𝑐
(cid:74)
(cid:75)
∧ (cid:0)𝜉𝑛 = 𝜎 ′|dom(𝜉𝑛)

(cid:1) ∧ (cid:0)dom(𝜉𝑛) = {𝜇 ∈ Name |

𝜎 ′(cnt𝜇) − 𝜎 ′(cnt𝜇) ≤ 1(cid:1) ∧ (cid:0)𝜎 ′(like) = 1(cid:1)
(cid:75)
𝑐
(cid:74)

𝜎 ′(cnt𝜇) − 𝜎 ′(cnt𝜇) = 1}(cid:1).
(cid:75)

𝑐
(cid:74)

Note that we have the first clause by Lemma C.6-(1), the second and fifth clauses by Lemma C.6–(5),
and the third and fourth clauses by 𝜎 |𝑉 = 𝜎 ′|𝑉 . Hence, used (𝑐, 𝜎 ′, 𝜉𝑛) holds. The proof of the second
claim is exactly the same except that we apply Lemma C.6 to 𝜎 |𝑈 = 𝜎 ′|𝑈 to prove only the four
□
clauses of used that exclude 𝜎 ′(like) = 1.

C.3 Proof of Lemma C.3

Proof of Lemma C.3. We prove this lemma by induction on the structure of 𝑐. Let 𝑔 : St[PVar] ×
St□ [Name] → R be a measurable function and 𝜎𝑝 ∈ St[PVar]. In this proof, each equation involving
integrals means (otherwise noted) that one side of the equation is defined if and only if the other
side is defined, and when both sides are defined, they are the same.

Case 𝑐 ≡ skip, 𝑐 ≡ (𝑥 := 𝑒), or 𝑐 ≡ obs(𝑑, 𝑟 ). In this case, 𝑐𝜋 ≡ 𝑐 so the desired equation holds.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:42

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Case 𝑐 ≡ (𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒)). If (𝑛, 𝑑, 𝜆𝑦.𝑒) ∉ dom(𝜋), then 𝑐𝜋 ≡ 𝑐 and thus the desired
equation holds. So assume that 𝜋 (𝑛, 𝑑, 𝜆𝑦.𝑒) = (𝑑 ′, 𝜆𝑦 ′.𝑒 ′) for some 𝑑 ′ and 𝜆𝑦 ′.𝑒 ′. Then, 𝑐𝜋 ≡ (𝑥 :=
sam(𝑛, 𝑑 ′, 𝜆𝑦 ′.𝑒 ′)).

First, by the validity of 𝜋, for all states 𝜎 ∈ St and measurable subsets 𝐴 ⊆ R,

∫

1[

𝑒 [𝑟 /𝑦 ]
(cid:74)

𝜎 ∈𝐴] ·
(cid:75)

𝑑
(cid:74)

𝜎 (𝑟 ) 𝑑𝑟 =
(cid:75)

∫

1[

𝑒′ [𝑟 /𝑦′ ]
(cid:74)

𝜎 ∈𝐴] ·
(cid:75)

𝑑 ′
(cid:74)

𝜎 (𝑟 ) 𝑑𝑟,
(cid:75)

where both sides are always defined. Using this and the monotone convergence theorem, we can
show that for all measurable 𝑓 : R → R,

∫

𝑓 (

𝑒 [𝑟 /𝑦]
(cid:74)

𝜎) ·
(cid:75)

𝑑
(cid:74)

𝜎 (𝑟 ) 𝑑𝑟 =
(cid:75)

∫

𝑓 (

𝑒 ′[𝑟 /𝑦 ′]
(cid:74)

𝜎) ·
(cid:75)

𝑑 ′
(cid:74)

𝜎 (𝑟 ) 𝑑𝑟 .
(cid:75)

(18)

Next, choose any 𝜎𝑟0 ∈ St[Var \ PVar]. Since fv(𝑛) ⊆ PVar, there exists 𝜇 ∈ Name such that

Using this and fv(𝑒), fv(𝑑) ⊆ PVar, we obtain the following: for any 𝜎𝑟 ∈ St[Var \ PVar],

(𝜎𝑝 ⊕ 𝜎𝑟 ) = 𝜇

for all 𝜎𝑟 ∈ St[Var \ PVar].

𝑛
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑟 )(𝜎𝑟 (𝜇)) =

𝑐
(cid:74)

(cid:75)
𝑐
(cid:74)
𝑐
(cid:74)
𝑐
(cid:74)

(𝜎𝑝 ⊕ 𝜎𝑟 )(like) = 1,
𝑑
(𝜎𝑝 ⊕ 𝜎𝑟 )(pr 𝜇) =
(cid:74)
(cid:75)
(cid:75)
𝑒 [𝜎𝑟 (𝜇)/𝑦]
(𝜎𝑝 ⊕ 𝜎𝑟 )(val𝜇) =
(cid:74)
(𝜎𝑝 ⊕ 𝜎𝑟 )(cnt𝜇) = 1,
(cid:75)
𝑐
(cid:74)

𝑐
(cid:74)
(𝜎𝑝 ⊕ 𝜎𝑟 )|PVar = 𝜎𝑝 [𝑥 ↦→

(cid:75)

𝑑
(cid:74)
(𝜎𝑝 ⊕ 𝜎𝑟 ) =
(𝜎𝑝 ⊕ 𝜎𝑟 )(cnt𝜇′) = 0
(cid:75)
𝑒 [𝜎𝑟 (𝜇)/𝑦]
(cid:74)

(cid:75)

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑟0)].
This implies that for any 𝜉𝑛 ∈ St□ [Name], if prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) ≠ 0, then

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑟0)(𝜎𝑟 (𝜇)),
(cid:75)
𝑒 [𝜎𝑟 (𝜇)/𝑦]
(cid:74)

(cid:75)
for 𝜇 ′ (cid:46) 𝜇,

(𝜎𝑝 ⊕ 𝜎𝑟0),

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) = 1 ·

dom(𝜉𝑛) = {𝜇},
𝑑
(cid:74)

pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛) = 𝜎𝑝 [𝑥 ↦→
vals□ (𝑐)(𝜎𝑝, 𝜉𝑛) = [𝜇 ↦→

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑟0)(𝜉𝑛 (𝜇)),
𝑒 [𝜉𝑛 (𝜇)/𝑦]
(cid:74)

(𝜎𝑝 ⊕ 𝜎𝑟0)],

(cid:75)
(𝜎𝑝 ⊕ 𝜎𝑟0)].

𝑒 [𝜉𝑛 (𝜇)/𝑦]
(cid:74)
Note that the same equations hold for 𝑐𝜋 , except that we replace 𝑑, 𝑒, and 𝑦 in the RHS of the above
equations by 𝑑 ′, 𝑒 ′, and 𝑦 ′. Using these, we obtain:

(cid:75)

∫

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

𝑒 [𝜉𝑛 (𝜇)/𝑦]
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑟0)

(cid:17)(cid:17)

∫

∫

[ {𝜇 }→R]
(cid:16)

𝑑𝑟

(cid:16)

𝑑𝑟

R
∫

R
∫

∫

[ {𝜇 }→R]
(cid:16)

𝑑𝜉𝑛

𝑑
(cid:74)
(cid:75)
𝑑 ′
(cid:74)

(cid:16)

𝑑𝜉𝑛

𝑑
(cid:74)

(cid:16)
𝑔
(𝜎𝑝 ⊕ 𝜎𝑟0 )(𝜉𝑛 (𝜇)) · (cid:98)
(cid:75)
(cid:16)
𝑔
(𝜎𝑝 ⊕ 𝜎𝑟0 )(𝑟 ) · (cid:98)

𝑒 [𝑟 /𝑦]
(cid:74)
(cid:16)
𝑔
(𝜎𝑝 ⊕ 𝜎𝑟0)(𝑟 ) · (cid:98)

(cid:75)
𝑒 ′[𝑟 /𝑦 ′]
(cid:75)
(cid:74)
(cid:16)
𝑔
(𝜎𝑝 ⊕ 𝜎𝑟0)(𝜉𝑛 (𝜇)) · (cid:98)
(cid:16)

𝑑 ′
(cid:74)

(cid:75)
prs□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:75)
𝑑𝜉𝑛

(cid:16)

=

=

=

=

=

(𝜎𝑝 ⊕ 𝜎𝑟0)

(cid:17)(cid:17)

(𝜎𝑝 ⊕ 𝜎𝑟0)

(cid:17)(cid:17)

𝑒 ′[𝜉𝑛 (𝜇)/𝑦 ′]
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑟0)

(cid:17)(cid:17)

pvars□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛), vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

𝑔 : R → R is defined as
(cid:98)

where
𝑔(𝑟 ) = 𝑔(𝜎𝑝 [𝑥 ↦→ 𝑟 ], [𝜇 ↦→ 𝑟 ]). Here the first and fifth equalities use
(cid:98)
the equations proven above, the second and fourth equalities use that [{𝜇} → R] is isomorphic to
R, and the third equality uses Eq. (18). This proves the desired equation.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

Case 𝑐 ≡ (if 𝑏 {𝑐 ′} else 𝑐 ′′). In this case, since 𝑓 𝑣 (𝑏) ⊆ PVar and 𝑐𝜋 ≡ if 𝑏 {𝑐 ′

have only two subcases:

0:43

𝜋

} else 𝑐 ′′

𝜋 , we

• For all 𝜎𝑟 ∈ St[Var \ PVar],
𝑐
(cid:74)
• For all 𝜎𝑟 ∈ St[Var \ PVar],
𝑐
(cid:74)
If the first subcase holds, we have

(𝜎𝑝 ⊕ 𝜎𝑟 ) =
(cid:75)
(𝜎𝑝 ⊕ 𝜎𝑟 ) =
(cid:75)

𝑐 ′
(cid:75)
(cid:74)
𝑐 ′′
(cid:75)
(cid:74)

(𝜎𝑝 ⊕ 𝜎𝑟 ) and
(𝜎𝑝 ⊕ 𝜎𝑟 ) and

𝑐𝜋
(cid:74)
𝑐𝜋
(cid:74)

(𝜎𝑝 ⊕ 𝜎𝑟 ) =
(cid:75)
(𝜎𝑝 ⊕ 𝜎𝑟 ) =
(cid:75)

𝜋

𝜋

𝑐 ′
(cid:74)
𝑐 ′′
(cid:74)

(𝜎𝑝 ⊕ 𝜎𝑟 ).
(𝜎𝑝 ⊕ 𝜎𝑟 ).

(cid:75)
(cid:75)

∫

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□(𝑐)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

∫

∫

∫

=

=

=

(cid:16)

(cid:16)

(cid:16)

𝑑𝜉𝑛

𝑑𝜉𝑛

𝑑𝜉𝑛

prs□ (𝑐 ′)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□ (𝑐 ′)(𝜎𝑝, 𝜉𝑛), vals□(𝑐 ′)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

𝜋

prs□ (𝑐 ′

)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□(𝑐 ′

𝜋

)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐 ′

𝜋

)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

prs□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛), vals□(𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

where the second equality is by IH on 𝑐 ′. If the second subcase holds, we obtain a similar equation
by IH on 𝑐 ′′. Hence, the desired equation holds in all subcases.

Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). In this case, we obtain the following equation:

∫

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛) · 𝑔
∫

prs□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ·

(cid:16)

(cid:16)

pvars□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

(cid:16)

𝑑𝜉 ′′
𝑛

prs□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1) · 1[dom(𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅]

pvars□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
(cid:16)

𝑛) · 𝑔′ (cid:16)
prs□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
∫

(cid:16)

𝑛), 𝜉 ′′
𝑛

(cid:1), vals□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ vals□ (𝑐 ′′) (cid:0)pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1)(cid:17)(cid:17)(cid:17)

pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), vals□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
𝑛)

(cid:17)(cid:17)

where 𝑔′( (cid:98)𝜎 ′

𝑝, (cid:98)𝜉 ′

𝑛) ≜

𝑑𝜉 ′′
𝑛

prs□ (𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

𝑛 ) · 1[dom((cid:99)𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅]

(cid:16)

· 𝑔

pvars□ (𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

𝑛 ), (cid:98)𝜉 ′

𝑛 ⊕ vals□ (𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′
𝑛 )

(cid:17)(cid:17)

(cid:16)

𝑑𝜉 ′
𝑛

prs□ (𝑐 ′

𝜋

)(𝜎𝑝, 𝜉 ′

𝑛) · 𝑔′ (cid:16)

pvars□ (𝑐 ′

𝜋

)(𝜎𝑝, 𝜉 ′

𝑛), vals□ (𝑐 ′

𝜋

)(𝜎𝑝, 𝜉 ′
𝑛)

(cid:17)(cid:17)

· · ·

(∗)

𝑑𝜉 ′
𝑛

(cid:16)

· 𝑔

𝑑𝜉 ′
𝑛

∫

=

∫

=

∫

=

where the first equality is from Lemma C.9, the second equality uses dom(𝜉 ′
and the third equality is by IH on 𝑐 ′. We now analyse 𝑔′( (cid:98)𝜎 ′
𝑔′( (cid:98)𝜎 ′
𝑝, (cid:98)𝜉 ′
𝑛)
∫

𝑛) as follows:

𝑝, (cid:98)𝜉 ′

(cid:16)

(cid:16)

=

𝑑𝜉 ′′
𝑛

prs□(𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

∫

=

(cid:16)

𝑑𝜉 ′′
𝑛

prs□(𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

pvars□(𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

𝑛 ), vals□ (𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′
𝑛 )

(cid:17)(cid:17)

𝑛 ) · 1[dom((cid:99)𝜉 ′
𝑛 ) · 𝑔′′ (cid:16)

𝑛)∩dom(𝜉 ′′

𝑛)=∅] · 𝑔

pvars□ (𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

𝑛 ), (cid:98)𝜉 ′

𝑛 ⊕ vals□ (𝑐 ′′)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′
𝑛 )

(cid:17)(cid:17)

𝑛) = dom(vals□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛)),

𝑝 , (cid:99)𝜉 ′′

where 𝑔′′( (cid:99)𝜎 ′′
(cid:16)

𝑑𝜉 ′′
𝑛

prs□(𝑐 ′′

𝑛 ) ≜ 1[dom((cid:99)𝜉 ′
𝑛 ) · 𝑔′′ (cid:16)
𝑝, 𝜉 ′′
)( (cid:98)𝜎 ′

∫

∫

=

=

𝜋

𝜋

(cid:16)

𝑑𝜉 ′′
𝑛

prs□(𝑐 ′′

𝑛)∩dom((cid:99)𝜉 ′′

𝑛)=∅] · 𝑔

(cid:16)
𝑝 , (cid:98)𝜉 ′
(cid:99)𝜎 ′′

𝑛 ⊕ (cid:99)𝜉 ′′
𝑛

(cid:17)

pvars□ (𝑐 ′′

𝜋

)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

𝑛 ), vals□ (𝑐 ′′

𝜋

)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′
𝑛 )

(cid:17)(cid:17)

)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

𝑛 ) · 1[dom((cid:99)𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅] · 𝑔

(cid:16)

pvars□ (𝑐 ′′

𝜋

)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′

𝑛 ), (cid:98)𝜉 ′

𝑛 ⊕ vals□ (𝑐 ′′

𝜋

)( (cid:98)𝜎 ′

𝑝, 𝜉 ′′
𝑛 )

(cid:17)(cid:17)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:44

Wonyeol Lee, Xavier Rival, and Hongseok Yang

where the second and fourth equalities use dom(𝜉 ′′
ity is by IH on 𝑐 ′′. Using this, we obtain the following equation for the main quantity (∗):

𝑛 ) = dom(vals□ (𝑐 ′′)( (cid:98)𝜎 ′

𝑛 )), and the third equal-

𝑝, 𝜉 ′′

∫

(∗) =

(cid:16)

𝑑𝜉 ′
𝑛

𝜋

prs□(𝑐 ′

)(𝜎𝑝, 𝜉 ′

𝑛) ·

∫

(cid:16)

𝑑𝜉 ′′
𝑛

prs□ (𝑐 ′′

𝜋

) (cid:0)pvars□ (𝑐 ′

𝜋

)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1) · 1[dom(𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅]

(cid:16)

· 𝑔

pvars□ (𝑐 ′′
𝜋

vals□ (𝑐 ′

𝜋

) (cid:0)pvars□(𝑐 ′

𝜋

)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1),

)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ vals□ (𝑐 ′′

𝜋

) (cid:0)pvars□(𝑐 ′

𝜋

)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1)(cid:17)(cid:17)(cid:17)

∫

=

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐 ′

𝜋 ; 𝑐 ′′

𝜋

)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□(𝑐 ′

𝜋 ; 𝑐 ′′

𝜋

)(𝜎𝑝, 𝜉𝑛), vals□(𝑐 ′

𝜋 ; 𝑐 ′′

𝜋

)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

where the first equality uses dom(𝜉 ′
𝑛) = dom(vals□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
𝜋
Lemma C.9, as we did above. By 𝑐 ′; 𝑐 ′′

𝜋 , we get the desired equation.

𝜋 ; 𝑐 ′′

𝑛)), and the second equality is by

≡ 𝑐 ′

Case 𝑐 ≡ (while 𝑏 {𝑐 ′}). In this case, 𝑐𝜋 ≡ (while 𝑏 {𝑐 ′

}). Without loss of generality, assume
that 𝑔 is a nonnegative function; we can prove the general case of 𝑔 directly from the nonnegative
case of 𝑔, by considering the nonnegative part and the negative part of 𝑔 separately.

𝜋

Consider the version of prs□ (−), pvars□(−), and vals□ (−), where the parameter can be a state
transformer 𝑓 : St → St⊥, instead of a command. We denote the versions by prs□ (𝑓 ), pvars□ (𝑓 ), and
vals□ (𝑓 ). Define T ⊆ [St → St⊥]2 and 𝑇 : [St → St⊥]2 → [St → St⊥]2 by

(𝑓 , 𝑓 ) ∈ T ⇐⇒

∫

𝑑𝜉𝑛 𝐺𝑔′,𝜎′

𝑝 (𝑓 )(𝜉𝑛) =

∫

𝑑𝜉𝑛 𝐺𝑔′,𝜎′

𝑝 (𝑓 )(𝜉𝑛)

for all measurable 𝑔′ : St[PVar] × St□ [Name] → R≥0 and 𝜎 ′

𝑝 ∈ St[PVar],

𝑇 (𝑓 , 𝑓 ) ≜ (𝐹 (𝑓 ), 𝐹 (𝑓 )),

where 𝐺𝑔′,𝜎′

𝑝 (𝑓 ) ∈ St□ [Name] → R≥0 and 𝐹, 𝐹 : [St → St⊥] → [St → St⊥] are defined by

𝐺𝑔′,𝜎′

𝐹 (𝑓 )(𝜎) ≜ if (

pvars□(𝑓 )(𝜎 ′

𝑝, 𝜉𝑛), vals□ (𝑓 )(𝜎 ′

𝑝 (𝑓 )(𝜉𝑛) ≜ prs□(𝑓 )(𝜎 ′
𝐹 (𝑓 )(𝜎) ≜ if (

𝑝, 𝜉𝑛) · 𝑔′ (cid:16)
𝜎 = true) then (𝑓 † ◦
(cid:75)
𝜎 = true) then (𝑓 † ◦
(cid:75)
, respectively. We
Note that 𝐹 and 𝐹 are the operators used in the semantics of the loops
will show that T contains (𝜆𝜎. ⊥, 𝜆𝜎. ⊥), the operator 𝑇 preserves T , and T is closed under taking
the least upper bound of an increasing chain in [St → St⊥]2, where the order on [St → St⊥]2 is
defined as: (𝑓0, 𝑓0) ⊑ (𝑓1, 𝑓1) ⇐⇒ 𝑓0 ⊑ 𝑓1 ∧ 𝑓0 ⊑ 𝑓1. These three conditions imply (
) ∈ T ,
(cid:75)
which in turn proves the desired equation:

)(𝜎) else 𝜎,
(cid:75)
𝜋

)(𝜎) else 𝜎.

𝑐 ′
(cid:74)
𝑐 ′
(cid:74)

𝑏
(cid:74)
𝑏
(cid:74)

𝑐𝜋
(cid:74)

𝑐𝜋
(cid:74)

𝑝, 𝜉𝑛)

𝑐
(cid:74)

𝑐
(cid:74)

and

,
(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:17)

,

(cid:16)

𝑑𝜉𝑛

prs□(𝑐)(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

∫

=

=

∫

∫

∫

𝑑𝜉𝑛 𝐺𝑔,𝜎𝑝 (

𝑑𝜉𝑛 𝐺𝑔,𝜎𝑝 (

)(𝜉𝑛)
𝑐
(cid:74)
(cid:75)
𝑐𝜋
(cid:74)

)(𝜉𝑛)

(cid:75)
prs□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) · 𝑔

(cid:16)

(cid:16)

=

𝑑𝜉𝑛

where the second equality follows from (

pvars□(𝑐𝜋 )(𝜎𝑝, 𝜉𝑛), vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)
𝑐𝜋
) ∈ T .
(cid:75)
(cid:74)
𝑝 , and 𝜉𝑛. To show the
𝑝 (𝜆𝜎. ⊥)(𝜉𝑛) = 0 for all 𝑔′, 𝜎 ′
The first condition holds simply because 𝐺𝑔′,𝜎′
second condition, pick (𝑓 , 𝑓 ) ∈ T . Our goal is to show 𝑇 (𝑓 , 𝑓 ) ∈ T . We first replay our proof for

𝑐
(cid:74)

,
(cid:75)

,

(cid:17)(cid:17)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:45

𝜋

𝜋

𝜋

(cid:75)

(cid:75)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

the sequential-composition case on (𝑓 † ◦
the sequential composition of 𝑐 ′ and 𝑓 , and of 𝑐 ′
membership (𝑓 † ◦
, 𝑓 † ◦
𝑐 ′
(cid:75)
(cid:74)
after viewing 𝑓 † ◦
and 𝑓 † ◦
(cid:75)
This replay implies the required 𝑇 (𝑓 , 𝑓 ) = (𝐹 (𝑓 ), 𝐹 (𝑓 )) ∈ T .

as
𝜋 and 𝑓 , respectively. This replay, then, gives the
) ∈ T . Next, we replay our proof for the if case on (𝐹 (𝑓 ), 𝐹 (𝑓 )),
𝜋
(cid:75)
as the false branch.
𝑐 ′
(cid:74)

as the true branches, and 𝜆𝜎. 𝜎 =

), after viewing 𝑓 † ◦

, 𝑓 † ◦
(cid:75)

skip
(cid:75)

𝑐 ′
(cid:74)
𝑐 ′
(cid:74)

and 𝑓 † ◦

≜ (cid:195)𝑘 ∈N 𝑓𝑘 . Consider a measurable 𝑔′ : St[PVar] × St□ [Name] → R≥0 and 𝜎 ′

To show the third condition, consider an increasing sequence {(𝑓𝑘, 𝑓𝑘 )}𝑘 ∈N in T . Let 𝑓∞ ≜ (cid:195)𝑘 ∈N 𝑓𝑘
𝑝 ∈ St[PVar].
𝑝 (𝑓∞)(𝜉𝑛). Since {𝑓𝑘 }𝑘 ∈N is increasing, for
𝑝 (𝑓𝑘 )}𝑘 ∈N is a

and 𝑓
𝑝 (𝑓∞)(𝜉𝑛) = ∫ 𝑑𝜉𝑛 𝐺𝑔′,𝜎′
We should show that ∫ 𝑑𝜉𝑛 𝐺𝑔′,𝜎′
any 𝜎 ∈ St, 𝑓𝑘 (𝜎) ∈ St implies that 𝑓𝑘′ (𝜎) = 𝑓𝑘 (𝜎) ∈ St for all 𝑘 ′ ≥ 𝑘. Hence, {𝐺𝑔′,𝜎′
pointwise increasing sequence: for all 𝜉𝑛 ∈ St□ [Name],
0 ≤ 𝐺𝑔′,𝜎′

for all 𝑘 ∈ N.

𝑝 (𝑓𝑘 )(𝜉𝑛) ≤ 𝐺𝑔′,𝜎′

𝑝 (𝑓𝑘+1)(𝜉𝑛)

∞

(cid:75)

(cid:74)

(cid:75)

Also, by the definition of 𝑓∞, for any 𝜎 ∈ St, there exists 𝐾 ∈ N such that 𝑓∞ (𝜎) = 𝑓𝐾 (𝜎); thus,
𝐺𝑔′,𝜎′

𝑝 (𝑓∞) is the pointwise limit of {𝐺𝑔′,𝜎′

𝑝 (𝑓𝑘 )}𝑘 ∈N: for all 𝜉𝑛 ∈ St□ [Name],

𝑝 (𝑓∞)(𝜉𝑛) = lim
𝑘→∞
Note that the corresponding results hold for 𝑓∞ and 𝑓𝑘 . Using these results, we finally obtain the
following as desired:

𝑝 (𝑓𝑘 )(𝜉𝑛).

𝐺𝑔′,𝜎′

𝐺𝑔′,𝜎′

∫

𝑑𝜉𝑛 𝐺𝑔′,𝜎′

𝑝 (𝑓∞)(𝜉𝑛) = lim
𝑘→∞

= lim
𝑘→∞

∫

∫

𝑑𝜉𝑛 𝐺𝑔′,𝜎′

𝑝 (𝑓𝑘 )(𝜉𝑛)

𝑑𝜉𝑛 𝐺𝑔′,𝜎′

𝑝 (𝑓𝑘 )(𝜉𝑛) =

∫

𝑑𝜉𝑛 𝐺𝑔′,𝜎′

𝑝 (𝑓∞)(𝜉𝑛).

The first and third equalities follow from the monotone convergence theorem, applied to the above
results. The second equality holds since (𝑓𝑘, 𝑓𝑘 ) ∈ T . This completes the proof of the while case. □

Lemma C.8. Let 𝑐 be a command, 𝜎0, 𝜎1 ∈ St, and 𝑟0 ∈ R. Suppose that 𝜎1(like) = 𝜎0(like) · 𝑟0 and

𝜎1|𝑉 = 𝜎0|𝑉 for 𝑉 ≜ Var \ {like}. If
𝜎1 ∈ St,
(cid:75)

𝜎0 ∈ St, then
(cid:75)
𝑐
𝜎1(like) =
(cid:74)
(cid:75)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝜎0(like) · 𝑟0,
(cid:75)

(

𝑐
(cid:74)

𝜎1)|𝑉 = (
(cid:75)

𝑐
(cid:74)

𝜎0)|𝑉 .
(cid:75)

Proof. Let 𝑉 ≜ Var \ {like}. Pick an arbitrary command 𝑐. We prove the lemma by induction
𝜎0 ∈ St, 𝜎1(like) = 𝜎0(like) · 𝑟0, and
𝑐
(cid:75)
(cid:74)
𝜎0|𝑉 .
𝜎0(like) · 𝑟0, and
𝑐
𝑐
(cid:75)
(cid:74)
(cid:75)
(cid:74)

on the structure of 𝑐. Let 𝜎0, 𝜎1 ∈ St and 𝑟0 ∈ R such that
𝜎1|𝑉 = 𝜎0|𝑉 . We should show that
𝜎1(like) =
(cid:75)

Case 𝑐 ≡ skip. In this case, what we need to prove is identical to the assumption on (𝜎0, 𝜎1, 𝑟0).
Case 𝑐 ≡ (𝑥 := 𝑒). By the semantics of the assignments, we have

𝜎1 ∈ St,
(cid:75)

𝜎1|𝑉 =
(cid:75)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

The last requirement also holds since

𝑒
(cid:74)
Case 𝑐 ≡ (𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒 ′). By the semantics of the sample commands, we have

𝜎1 and 𝜎0|𝑉 = 𝜎1|𝑉 .
(cid:75)

𝑐
(cid:74)

𝜎1(like) = 𝜎1(like) = 𝜎0(like) · 𝑟0 =
(cid:75)
𝜎0 =
(cid:75)

𝑒
(cid:74)

𝑐
(cid:74)

𝑑
(cid:74)

𝑑
(cid:74)
𝑑
(cid:74)

𝜎1. Let 𝜇 ≜
𝜎0 =
(cid:75)
(cid:75)
𝜎0 and 𝑟 ≜
𝑒 ′[𝜇/𝑦]
(cid:74)
(cid:75)

Also, the assignments do not change the value of like, so that
𝜎0(like) · 𝑟0. It remains to show that
𝑐
(cid:74)
(cid:75)
and
𝑓 ≜

𝑐
(cid:74)
𝜎0. Then, by the same reason,
(cid:75)

𝜎1|𝑉 . Since 𝜎0|𝑉 = 𝜎1|𝑉 , we have
(cid:75)
𝑒 ′[𝜇/𝑦]
(cid:74)
𝜎0. We prove the required equality as follows:
(cid:75)
𝜎0|𝑉 = 𝜎0 [𝑥 ↦→ 𝑟, val𝜇 ↦→ 𝑟, pr 𝜇 ↦→ 𝑓 (𝜎0(𝜇)), cnt𝜇 ↦→ 𝜎0(cnt𝜇) + 1]|𝑉
(cid:75)
= 𝜎1 [𝑥 ↦→ 𝑟, val𝜇 ↦→ 𝑟, pr 𝜇 ↦→ 𝑓 (𝜎0(𝜇)), cnt𝜇 ↦→ 𝜎0(cnt𝜇) + 1]|𝑉
= 𝜎1 [𝑥 ↦→ 𝑟, val𝜇 ↦→ 𝑟, pr 𝜇 ↦→ 𝑓 (𝜎1(𝜇)), cnt𝜇 ↦→ 𝜎1(cnt𝜇) + 1]|𝑉

𝜎0|𝑉 =
(cid:75)

𝜎0 =
(cid:75)

𝑛
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝜎1 ∈ St.
𝑐
(cid:74)
(cid:75)
𝜎1 (like) = 𝜎1(like) = 𝜎0(like) · 𝑟0 =
(cid:75)
𝑛
𝑛
𝜎1
𝜎0 =
(cid:75)
(cid:75)
(cid:74)
(cid:74)
𝜎1. Let
𝑒 ′[𝜇/𝑦]
(cid:75)
(cid:74)

𝑐
(cid:74)

𝜎1 ∈ St, and
(cid:75)
𝜎0(like) · 𝑟0.
(cid:75)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:46

Wonyeol Lee, Xavier Rival, and Hongseok Yang

=

𝑐
(cid:74)

𝜎1|𝑉 .
(cid:75)

Case 𝑐 ≡ (obs(𝑑, 𝑟 )). By the semantics of the observe commands, we have

𝑐
(cid:74)

𝜎0|𝑉 = 𝜎0|𝑉 = 𝜎1|𝑉 =
(cid:75)

𝑐
(cid:74)

𝜎1 ∈ St. Also, the
(cid:75)
𝜎1|𝑉 . The
(cid:75)

𝑐
(cid:74)

observe commands do not change any variable except like. So,
remaining requirement for like can be proved as follows:
𝑐
(cid:74)

𝜎1(𝑟 ) = 𝜎0(like) · 𝑟0 ·
(cid:75)

𝜎1(like) = 𝜎1(like) ·
(cid:75)
Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). We have

𝑑
(cid:74)

(

first to (𝑐 ′, 𝜎0, 𝜎1, 𝑟0), and then to (𝑐 ′′,

𝑐 ′
(cid:74)

𝜎0 ∈ St and
(cid:75)
𝑐 ′
𝜎0,
(cid:74)
(cid:75)

𝑐 ′
(cid:74)

𝜎1 (𝑟 ) = 𝜎0(like) · 𝑟0 ·
𝑑
(cid:75)
(cid:74)
𝜎0) ∈ St. We apply induction hypothesis
𝑐 ′′
(cid:74)
(cid:75)
𝜎1, 𝑟0). Then, we get the requirements of the lemma.
(cid:75)

𝜎0(like) · 𝑟0.
(cid:75)

𝜎0(𝑟 ) =
(cid:75)

𝑐 ′
(cid:74)

𝑑
(cid:74)

𝑐
(cid:74)

(cid:75)

Case 𝑐 ≡ (if 𝑏 {𝑐 ′} else {𝑐 ′′}). We deal with the case that
𝜎0 = true, we have
𝜎0 = false can be proved similarly. Since
(cid:75)
(cid:75)

𝑏
(cid:74)
apply induction hypothesis to 𝑐 ′. If we do so, we get

𝑏
(cid:74)

𝜎0 = true. The other case of
𝑏
(cid:75)
(cid:74)
𝜎0 ∈ St. Thus, we can
𝜎0 =
𝑐 ′
(cid:75)
(cid:75)
(cid:74)

𝑐
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝜎1 ∈ St,
(cid:75)

𝜎0(like) · 𝑟0,
(cid:75)
This gives the desired conclusion because
𝑏
𝜎1 =
𝑏
(cid:74)
(cid:75)
(cid:74)
𝑐
(cid:74)

𝜎0 =
(cid:75)
Case 𝑐 ≡ (while 𝑏 {𝑐 ′}). Let 𝐹 be the operator on [St → St⊥] such that

𝑐 ′
(cid:74)
𝜎0 = true and so
(cid:75)

𝜎1(like) =
(cid:75)

𝜎1|𝑉 =
(cid:75)

𝜎0.
(cid:75)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

and

is the least fixed point
of 𝐹 . Define a subset T of [St → St⊥] as follows: a function 𝑓 ∈ [St → St⊥] is in T if and only if
for all 𝜎 ′
1(like) = 𝜎 ′

𝑐
(cid:74)
0(like) · 𝑟0, we have

1 ∈ St such that 𝜎 ′

0|𝑉 and 𝜎 ′

1|𝑉 = 𝜎 ′

0, 𝜎 ′

(cid:75)

𝑐 ′
(cid:74)

𝑐
(cid:74)

𝜎0|𝑉 .
(cid:75)
𝜎1 =
(cid:75)

𝑐 ′
(cid:74)

𝜎1, and
(cid:75)

𝑓 (𝜎 ′

0) ≠ ⊥ =⇒

(cid:16)

𝑓 (𝜎 ′

1) ≠ ⊥ ∧ 𝑓 (𝜎 ′

1)|𝑉 = 𝑓 (𝜎 ′

0)|𝑉 ∧ 𝑓 (𝜎 ′

1)(like) = 𝑓 (𝜎 ′

0)(like) · 𝑟0

(cid:17)

.

The set T contains the least function 𝜆𝜎.⊥, and is closed under the least upper bound of any chain
in [St → St⊥]. It is also closed under 𝐹 . This 𝐹 -closure follows essentially from our arguments for
sequential composition, if command, and skip, and induction hypothesis on 𝑐 ′. What we have shown
for T implies that T contains the least fixed point of 𝐹 , which gives the desired property for 𝑐. □

Lemma C.9. Let 𝑐 ′, 𝑐 ′′ be commands and 𝑔 : St[PVar] × St□ [Name] → R be a measurable function.

Then, for any 𝜎𝑝 ∈ St[PVar],
∫

(cid:16)

𝑑𝜉𝑛

prs□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛) · 𝑔
∫

prs□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ·

(cid:16)

𝑑𝜉 ′
𝑛

(cid:16)

pvars□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

(cid:16)

𝑑𝜉 ′′
𝑛

prs□ (𝑐 ′′)(pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′

𝑛 ) · 1[dom(𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅]

∫

=

(cid:16)

· 𝑔

pvars□ (𝑐 ′′)(pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′

𝑛 ), vals□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ vals□ (𝑐 ′′)(pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛 )

(cid:17)(cid:17)(cid:17)

.

Proof. Let 𝑐 ′, 𝑐 ′′ be commands, 𝑔 : St[PVar] × St□ [Name] → R a measurable function, and
𝜎𝑝 ∈ St[PVar]. In this proof, each equation involving integrals means (otherwise noted) that one
side of the equation is defined if and only if the other side is defined, and when both sides are defined,
they are the same.

First, to convert a single-integral on 𝜉𝑛 to a double-integral on 𝜉 ′

𝑛 as in the desired equation,
we show the following claim: for any measurable 𝑓 : St□ [Name] → St and ℎ : St□ [Name] → R
such that 𝑓 (𝜉𝑛)|dom(𝜉𝑛) = 𝜉𝑛 for all 𝜉𝑛 ∈ St□ [Name], we have
∫

𝑛 and 𝜉 ′′

(cid:16)

𝑑𝜉𝑛

1[used (𝑐′;𝑐′′,𝑓 (𝜉𝑛),𝜉𝑛) ] · ℎ(𝜉𝑛)

(cid:17)

=

∑︁

∫

𝐾 ⊆Name

[𝐾→R]

(cid:16)

𝑑𝜉𝑛

1[used (𝑐′;𝑐′′,𝑓 (𝜉𝑛),𝜉𝑛) ] · ℎ(𝜉𝑛)

(cid:17)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

∑︁

∫

𝐾 ⊆Name
∑︁

[𝐾→R]
∫

∑︁

(cid:16)

𝑑𝜉𝑛

1[used (𝑐′;𝑐′′,𝑓 (𝜉𝑛),𝜉𝑛) ] · ℎ(𝜉𝑛) ·

∑︁

𝐿 ⊆𝐾

1[used (𝑐′,𝑓 (𝜉𝑛),𝜉𝑛 |𝐿) ]

(cid:16)

𝑑𝜉𝑛

1[used (𝑐′,𝑓 (𝜉𝑛),𝜉𝑛 |𝐿) ] · 1[used (𝑐′;𝑐′′,𝑓 (𝜉𝑛),𝜉𝑛) ] · ℎ(𝜉𝑛)

(cid:17)

(cid:17)

=

=

=

=

=

∫

𝑑𝜉 ′
𝑛
[𝐿→R]
∫

𝑑𝜉 ′′
𝑛
[𝐾\𝐿→R]
∫

𝑑𝜉 ′
𝑛
[𝐿′→R]

𝑑𝜉 ′′
𝑛
[𝑀′→R]

[𝐾→R]

𝐾 ⊆Name
∑︁

𝐿 ⊆𝐾
∑︁

∫

𝐾 ⊆Name
∑︁

𝐿′ ⊆Name

∑︁

𝐿 ⊆𝐾

∑︁

𝑀′ ⊆Name
𝐿′∩𝑀′=∅
∫

𝐿′ ⊆Name

[𝐿′→R]

𝑑𝜉 ′
𝑛

(cid:16) ∑︁

∫

𝑀′ ⊆Name

[𝑀′→R]

(cid:16)

𝑑𝜉 ′′
𝑛

1[dom(𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅] · 1[used (𝑐′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛) ]

· 1[used (𝑐′;𝑐′′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛) ] · ℎ(𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 )

(cid:17)(cid:17)

(cid:16)

1[used (𝑐′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛) ] · 1[used (𝑐′;𝑐′′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛) ] · ℎ(𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 )

(cid:16)

1[used (𝑐′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛) ] · 1[used (𝑐′;𝑐′′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛) ] · ℎ(𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 )

(cid:17)

0:47

(cid:17)

∫

=

∫

𝑑𝜉 ′
𝑛

(cid:16)

𝑑𝜉 ′′
𝑛

1[dom(𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅] · 1[used (𝑐′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛) ] · 1[used (𝑐′;𝑐′′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛) ] · ℎ(𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 )

(cid:17)

.

The first equality uses the definition of St□ [Name] and its measure. The second equality uses that
used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉𝑛) implies a unique existence of 𝐿 ⊆ dom(𝜉𝑛) such that used (𝑐 ′, 𝜎, 𝜉𝑛 |𝐿); we already
showed the existence of such 𝐿 in the proof of Lemma C.6 (for the sequential composition case), and
the uniqueness follows from the definition of used. The third equality uses that 𝐾 is finite, and the
fourth equality uses that [𝐾 → R] is isomorphic to [𝐿 → R] × [𝐾 \ 𝐿 → R] for any 𝐿 ⊆ 𝐾. The fifth
equality holds uses that {(𝐿, 𝐾 \ 𝐿) | 𝐾 ⊆ Name, 𝐿 ⊆ 𝐾 } = {(𝐿′, 𝑀 ′) | 𝐿′, 𝑀 ′ ⊆ Name, 𝐿′ ∩ 𝑀 ′ = ∅}.
The sixth equality uses that Name is finite, dom(𝜉 ′
𝑛 ) = 𝑀 ′. The seventh equality
uses the definition of St□ [Name] and its measure.

𝑛) = 𝐿′, and dom(𝜉 ′′

Second, to decompose prs□(𝑐 ′; 𝑐 ′′), pvars□(𝑐 ′; 𝑐 ′′), and vals□(𝑐 ′; 𝑐 ′′) as in the desired equation, we
𝑛 ) = ∅

𝑛 ∈ St□ [Name] with dom(𝜉 ′

𝑛) ∩dom(𝜉 ′′

𝑛, 𝜉 ′′

show the following claim. Suppose that 𝜎 ∈ St and 𝜉 ′
satisfy used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′
𝑛 ⊕ 𝜉 ′′
𝑛 ⊕ 𝜉 ′′
prs□ (𝑐 ′; 𝑐 ′′)(𝜎 |PVar, 𝜉 ′

𝑛 ) and used (𝑐 ′, 𝜎, 𝜉 ′

𝑛). Then, we first get
𝑐 ′; 𝑐 ′′
(cid:74)

𝑛 ⊕𝜉 ′′
𝑛)

𝑛 ) =
=

𝜎 (pr 𝜇)
(cid:75)

(cid:75)

𝜎 (like) · (cid:206)𝜇 ∈dom(𝜉 ′
𝑐 ′; 𝑐 ′′
(cid:75)
(cid:74)
𝑐 ′
𝑐 ′′
(
(cid:74)
(cid:74)
· (cid:206)𝜇 ∈dom(𝜉 ′
𝑛)
𝜎 (like) ·
𝑐 ′
(cid:74)
(cid:75)
· (cid:206)𝜇 ∈dom(𝜉 ′
𝑛)

𝜎)(like)
(cid:75)
𝑐 ′′
(cid:74)
𝑐 ′′
(cid:74)

=

𝑐 ′
(
(cid:75)
(cid:74)
(cid:0)(
(cid:75)
𝜎) [like ↦→ 1](cid:1) (pr 𝜇)
𝜎 (pr 𝜇) · (cid:206)𝜇 ∈dom(𝜉 ′′
𝑐 ′
𝑛)
(cid:74)
(cid:75)
(cid:75)
(cid:1)
𝑛) · prs□ (𝑐 ′′) (cid:0)(
𝜎) [like ↦→ 1]|PVar, 𝜉 ′′
𝑐 ′
= prs□(𝑐 ′)(𝜎 |PVar, 𝜉 ′
𝑛
(cid:75)
(cid:74)
(cid:1).
𝑛) · prs□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎 |PVar, 𝜉 ′
𝑛), 𝜉 ′′
= prs□(𝑐 ′)(𝜎 |PVar, 𝜉 ′
𝑛

𝜎)(pr 𝜇) · (cid:206)𝜇 ∈dom(𝜉 ′′
𝑛)
(cid:75)
𝜎) [like ↦→ 1](cid:1) (like)
𝑐 ′
(cid:75)
(cid:74)
(cid:0)(
𝑐 ′′
(cid:74)

𝜎)(pr 𝜇)
(cid:75)

𝑐 ′′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

(cid:75)

(cid:75)

(

Here is the proof of each equality.

• The first equality uses used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′
• The second equality uses noerr (𝑐 ′; 𝑐 ′′, 𝜎), which comes from used (𝑐 ′; 𝑐 ′′, 𝜎, −).
• The third equality comes from Lemma C.8, Lemma C.6-(2), and Lemma C.6-(3). The two appli-
𝑛 ) = ∅, where
𝑛) by the claim in the

𝑛 ) and dom(𝜉 ′
𝑛 ) and used (𝑐 ′, 𝜎, 𝜉 ′

𝑛) ∩ dom(𝜉 ′′

𝑛 ⊕ 𝜉 ′′

𝑛 ).

cations of Lemma C.6 are valid since used− (𝑐 ′′,
𝜎, 𝜉 ′′
𝑐 ′
(cid:74)
(cid:75)
the first predicate follows from used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′
𝑛 ⊕ 𝜉 ′′
proof of Lemma C.6 (for the sequential composition case).
• The fourth equality uses that used (𝑐 ′, 𝜎, 𝜉 ′
𝑛) and used (𝑐 ′′, (
second predicate follows from used− (𝑐 ′′,
𝑐 ′
(cid:74)

𝜎, 𝜉 ′′
(cid:75)

𝑐 ′
(cid:74)

𝜎) [like ↦→ 1], 𝜉 ′′
(cid:75)

𝑛 ) and Lemma C.7.

𝑛 ), where the

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:48

Wonyeol Lee, Xavier Rival, and Hongseok Yang

• The fifth equality uses (

𝜎) [like ↦→ 1]|PVar = (
(cid:75)
the second part of the equation comes from used (𝑐 ′, 𝜎, 𝜉 ′

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝜎)|PVar = pvars□ (𝑐 ′)(𝜎 |PVar, 𝜉 ′
(cid:75)

𝑛).

𝑛), where

By the same argument so far (except that pr 𝜇 and × are replaced by val𝜇 and ⊕), we next get

vals□(𝑐 ′; 𝑐 ′′)(𝜎 |PVar, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ) = vals□ (𝑐 ′)(𝜎 |PVar, 𝜉 ′

𝑛) ⊕ vals□(𝑐 ′′) (cid:0)pvars□(𝑐 ′)(𝜎 |PVar, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1).

By a similar argument, we lastly get

pvars□(𝑐 ′; 𝑐 ′′)(𝜎 |PVar, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ) =
=

(cid:75)

𝑐 ′; 𝑐 ′′
(cid:74)
𝑐 ′′
(cid:74)
𝑐 ′′
(cid:74)

𝜎 |PVar
(cid:75)
𝑐 ′
𝜎)|PVar
(
(cid:74)
(cid:75)
(cid:0)(
𝜎) [like ↦→ 1](cid:1) |PVar
𝑐 ′
=
(cid:74)
(cid:75)
(cid:75)
= pvars□(𝑐 ′′) (cid:0)(
𝜎) [like ↦→ 1]|PVar, 𝜉 ′′
𝑐 ′
𝑛
(cid:74)
(cid:75)
(cid:1).
= pvars□(𝑐 ′′) (cid:0)pvars□(𝑐 ′)(𝜎 |PVar, 𝜉 ′
𝑛), 𝜉 ′′
𝑛

(cid:1)

Here is the proof of each equality.

𝑛 ).

𝑛 ⊕ 𝜉 ′′

• The first equality uses used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′
• The second equality uses noerr (𝑐 ′; 𝑐 ′′, 𝜎) (shown above).
• The third equality uses used− (𝑐 ′′,
𝑐 ′
𝜎, 𝜉 ′′
(cid:75)
(cid:74)
𝜎) [like ↦→ 1], 𝜉 ′′
• The fourth equality uses used (𝑐 ′′, (
𝑐 ′
(cid:75)
(cid:74)
𝜎) [like ↦→ 1]|PVar = pvars□ (𝑐 ′)(𝜎 |PVar, 𝜉 ′
• The fifth equality uses (
(cid:75)
Third, to remove some indicator terms that will appear in our derivation, we show the next
𝑛) and

𝑛 ) (shown above) and Lemma C.6-(3).

𝑛 ) = ∅, used− (𝑐 ′, 𝜎, 𝜉 ′

𝑛 ) (shown above).

𝑛) (shown above).

𝑛) ∩ dom(𝜉 ′′

𝑐 ′
(cid:74)
𝑛 ∈ St□ [Name] with dom(𝜉 ′
𝑛, 𝜉 ′′
𝑛 ) imply used− (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′
𝑛 ⊕ 𝜉 ′′
𝜎 ∈ St ∧ 𝜉 ′
(cid:75)

𝑐 ′
(cid:74)

claim: for any 𝜎 ∈ St and 𝜉 ′
used−(𝑐 ′′,

𝑐 ′
(cid:74)

𝜎, 𝜉 ′′
(cid:75)

𝑛 ). Assume the premise. Then, we have

𝑛 = 𝜎 |dom(𝜉 ′
𝑛)
∧ (cid:0)∀𝜇 ∈ Name.
∧ dom(𝜉 ′
𝑛 = (

𝑐 ′
(cid:74)
𝑛) = {𝜇 ∈ Name |
𝑐 ′
𝜎)|dom(𝜉 ′′
𝑛)
(cid:74)
(cid:75)
∧ (cid:0)∀𝜇 ∈ Name.
𝑐 ′′
(
(cid:74)
∧ dom(𝜉 ′′
𝑛 ) = {𝜇 ∈ Name |

𝑐 ′
(cid:74)

(cid:75)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) ≤ 1(cid:1)
(cid:75)

𝑐 ′
(cid:74)

𝜎 ′(cnt𝜇) − 𝜎 (cnt𝜇) = 1},
(cid:75)

𝜎)(cnt𝜇) −
(cid:75)
𝑐 ′
𝑐 ′′
(cid:74)
(cid:74)

(cid:75)

(

𝜎 (cnt𝜇) ≤ 1(cid:1)
(cid:75)

𝑐 ′
(cid:74)
𝜎)(cnt𝜇) −
(cid:75)

𝑐 ′
(cid:74)

𝜎 (cnt𝜇) = 1}.
(cid:75)

𝑐 ′′
(cid:74)

(cid:75)

(

𝑐 ′
(cid:74)

𝜎) ∈ St ∧ 𝜉 ′′
(cid:75)

We should show

𝑐 ′; 𝑐 ′′
(cid:74)

𝜎 ∈ St ∧ 𝜉 ′
(cid:75)

𝑛 = 𝜎 |dom(𝜉 ′

𝑛 ⊕ 𝜉 ′′
∧ (cid:0)∀𝜇 ∈ Name.
𝑛 ⊕ 𝜉 ′′
∧ dom(𝜉 ′

𝑐 ′; 𝑐 ′′
(cid:74)

𝑛 ⊕𝜉 ′′
𝑛)
𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) ≤ 1(cid:1)
(cid:75)
𝑐 ′; 𝑐 ′′
𝑛 ) = {𝜇 ∈ Name |
(cid:74)

𝜎 ′(cnt𝜇) − 𝜎 (cnt𝜇) = 1}.
(cid:75)

We obtain the four clauses as follows. The first clause follows from
𝑛) and 𝜉 ′′
clause comes from 𝜉 ′
𝑛) = 𝜎 |dom(𝜉 ′′
from Lemma C.4. The third and fourth clauses hold by the following:

𝜎)|dom(𝜉 ′′
(cid:75)

𝑛 = 𝜎 |dom(𝜉 ′

𝑛 = (

𝑐 ′
(cid:74)

(

𝑐 ′′
(cid:74)

𝜎) ∈ St. The second
(cid:75)
𝑛) , where the last equality comes

𝑐 ′
(cid:74)

(cid:75)

𝜎 (cnt𝜇) − 𝜎 (cnt𝜇)
(cid:75)
(

𝑐 ′
𝜎)(cnt𝜇) − 𝜎 (cnt𝜇)
(cid:74)
(cid:75)
(cid:75)
𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) = 1
𝑐 ′
(cid:75)
(cid:74)
𝑐 ′
𝜎)(cnt𝜇) −
𝑐 ′′
(
(cid:74)
(cid:74)
(cid:75)
(cid:75)
𝜎 (cnt𝜇) − 𝜎 (cnt𝜇) = 0

𝑐 ′
(cid:74)

𝑐 ′; 𝑐 ′′
(cid:74)
𝑐 ′′
=
(cid:74)




=

𝜎 (cnt𝜇) = 1
(cid:75)

if 𝜇 ∈ dom(𝜉 ′
𝑛)
if 𝜇 ∈ dom(𝜉 ′′
𝑛 )
if 𝜇 ∈ Name \ (dom(𝜉 ′

𝑛) ∪ dom(𝜉 ′′

𝑛 )).

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:49

• The first case uses Lemma C.6-(2) (applied to used− (𝑐 ′′,

and the fourth clause of used− (𝑐 ′, 𝜎, 𝜉 ′

𝑛).

𝑐 ′
(cid:74)

𝑛 ) and dom(𝜉 ′

𝑛) ∩ dom(𝜉 ′′

𝑛 ) = ∅)

𝜎, 𝜉 ′′
(cid:75)

• The second case uses Lemma C.6-(2) (applied to used− (𝑐 ′, 𝜎, 𝜉 ′

𝑛) and dom(𝜉 ′

𝑛) ∩ dom(𝜉 ′′

𝑛 ) = ∅)

and the fourth clause of used− (𝑐 ′′,

𝑐 ′
(cid:74)

𝑛 ).

𝜎, 𝜉 ′′
(cid:75)

• The third case uses Lemma C.6-(2) (applied to used− (𝑐 ′′,
Finally, we put the three results together. Define 𝑓 : St□ [Name] → St as 𝑓 (𝜉𝑛) ≜ 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ (𝜆𝑣 ∈

𝑛 ) and used−(𝑐 ′, 𝜎, 𝜉 ′

𝜎, 𝜉 ′′
(cid:75)

𝑐 ′
(cid:74)

𝑛)).

Var \ (PVar ∪ dom(𝜉𝑛)). 1). Then, we obtain the desired equation as follows:
∫

prs□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛) · 𝑔

pvars□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛)

𝑑𝜉𝑛

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:16)

pvars□(𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛), vals□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛)

(cid:17)(cid:17)

(cid:16)

∫

∫

=

=

𝑑𝜉𝑛

𝑑𝜉 ′
𝑛

(cid:16)

𝑑𝜉 ′′
𝑛

1[used (𝑐′;𝑐′′,𝑓 (𝜉𝑛),𝜉𝑛) ] · prs□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛) · 𝑔
∫

1[dom(𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅] · 1[used (𝑐′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛) ] · 1[used (𝑐′;𝑐′′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛) ]

𝑛 ⊕ 𝜉 ′′

𝑛 ) · 𝑔

(cid:16)

pvars□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ), vals□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 )

(cid:17)(cid:17)

· prs□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉 ′
(cid:16)

∫

𝑑𝜉 ′′
𝑛

1[dom(𝜉 ′

𝑑𝜉 ′
𝑛

· prs□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

∫

=

∫

=

𝑛)∩dom(𝜉 ′′

𝑛)=∅] · 1[used (𝑐′,𝑓 (𝜉 ′

𝑛 ⊕𝜉 ′′
𝑛) · prs□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛),𝜉 ′

𝑛) ] · 1[used (𝑐′;𝑐′′,𝑓 (𝜉 ′
(cid:1)

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛 ⊕𝜉 ′′

𝑛) ]

𝑛), 𝜉 ′′
𝑛
(cid:1), vals□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ vals□ (𝑐 ′′) (cid:0)pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:16)

pvars□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
· 𝑔
∫

𝑛), 𝜉 ′′
𝑛

𝑑𝜉 ′
𝑛

(cid:16)

𝑑𝜉 ′′
𝑛

1[dom(𝜉 ′

𝑛)∩dom(𝜉 ′′

𝑛)=∅] · prs□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) · prs□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1)

(cid:16)

· 𝑔

pvars□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1), vals□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ vals□ (𝑐 ′′) (cid:0)pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛

(cid:1)(cid:17)(cid:17)

(cid:1)(cid:17)(cid:17)

.

The first equality uses that prs□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛) ≠ 0 implies 1[used (𝑐′;𝑐′′,𝑓 (𝜉𝑛),𝜉𝑛) ] = 1:

• Since prs□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉𝑛) ≠ 0, there is𝜎𝑟 ∈ St[Var\(PVar∪dom(𝜉𝑛))] such that used (𝑐 ′; 𝑐 ′′, 𝜎𝑝 ⊕
𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛). Note (𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )|𝑉 = 𝑓 (𝜉𝑛)|𝑉 for 𝑉 = PVar ∪ dom(𝜉𝑛) ∪ {like}. From these
and Lemma C.7, we get used (𝑐 ′; 𝑐 ′′, 𝑓 (𝜉𝑛), 𝜉𝑛).

The second and third equalities use the first and second results we proved above, respectively.
(cid:1) ≠ 0 and
The fourth equality uses the next claim: prs□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
𝑛), 𝜉 ′′
𝑛
dom(𝜉 ′
𝑛) ] = 1. We prove the
𝑛 ) = ∅ imply 1[used (𝑐′,𝑓 (𝜉 ′
claim using the third result we proved above:

𝑛) · prs□(𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
𝑛 ⊕𝜉 ′′

𝑛) ] · 1[used (𝑐′;𝑐′′,𝑓 (𝜉 ′

𝑛) ∩ dom(𝜉 ′′

𝑛 ⊕𝜉 ′′

𝑛 ⊕𝜉 ′′

𝑛),𝜉 ′

𝑛),𝜉 ′

• Assume the premise. From prs□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

such that used (𝑐 ′, 𝜎𝑝 ⊕ 𝜉 ′
PVar ∪ dom(𝜉 ′

𝑟 ∈ St[Var \ (PVar ∪ dom(𝜉 ′
𝑛). Note (𝜎𝑝 ⊕ 𝜉 ′
𝑛 ⊕ 𝜉 ′′
𝑟 )|𝑉 ′ = 𝑓 (𝜉 ′
𝑛) ∪ {like}. From these and Lemma C.7, we get used (𝑐 ′, 𝑓 (𝜉 ′
𝑛 ⊕ 𝜉 ′′

𝑛))]
𝑛 )|𝑉 ′ for 𝑉 ′ =
𝑛) as desired.
𝑛 ), 𝜉 ′
𝑟 ∈ St[Var\ (PVar∪dom(𝜉 ′′
𝑛 ))] such that

𝑛) ≠ 0, there is 𝜎 ′
𝑛 ⊕ 𝜎 ′

(cid:1) ≠ 0, there is 𝜎 ′′

𝑛 ⊕ 𝜎 ′

𝑛), 𝜉 ′′
𝑛

𝑟 , 𝜉 ′

• From prs□ (𝑐 ′′) (cid:0)pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

used (𝑐 ′′, pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ 𝜉 ′′

𝑛 ⊕ 𝜎 ′′

𝑟 , 𝜉 ′′

𝑛 ).

Since used (𝑐 ′, 𝑓 (𝜉 ′
pvars□ (𝑐 ′)(𝑓 (𝜉 ′
𝜉 ′′
𝑛 ))|dom(𝜉 ′′

𝑛) . Thus,

𝑛 ⊕ 𝜉 ′′
𝑛 ), 𝜉 ′
𝑛 )|PVar, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛), we have
𝑐 ′
𝑛) =
(cid:74)

(cid:75)

𝑐 ′
(cid:74)
(cid:75)
𝑛 ⊕ 𝜉 ′′
(𝑓 (𝜉 ′

𝑛 ⊕ 𝜉 ′′

(𝑓 (𝜉 ′
𝑛 ))|PVar; also, by Lemma C.4, 𝜉 ′′

𝑛 )) ∈ St and pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛 =

(𝑓 (𝜉 ′

𝑛) =
𝑛 ⊕

𝑐 ′
(cid:74)

(cid:75)

(pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ 𝜉 ′′

𝑛 ⊕ 𝜎 ′′

𝑟 )|𝑉 ′′ =

(𝑓 (𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ))|𝑉 ′′

𝑐
(cid:74)

(cid:75)

for 𝑉 ′′ = PVar ∪ dom(𝜉 ′′

𝑛 ). From these and Lemma C.7, we get used−(𝑐 ′′,

(𝑓 (𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 )), 𝜉 ′′

𝑛 ).

𝑐 ′
(cid:74)

(cid:75)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

□

(19)

(20)

(21)

0:50

Wonyeol Lee, Xavier Rival, and Hongseok Yang

• From dom(𝜉 ′

𝑛) ∩ dom(𝜉 ′′

𝑛), and used− (𝑐 ′′,
𝑛 ) = ∅, used− (𝑐 ′, 𝑓 (𝜉 ′
we can apply the third result proved above, and get used− (𝑐 ′; 𝑐 ′′, 𝑓 (𝜉 ′
𝑓 (𝜉 ′

𝑛 )(like) = 1, we get used (𝑐 ′; 𝑐 ′′, 𝑓 (𝜉 ′

(𝑓 (𝜉 ′
𝑐 ′
(cid:75)
(cid:74)
𝑛 ), 𝜉 ′
𝑛 ⊕ 𝜉 ′′
𝑛 ) as desired.

𝑛 ⊕ 𝜉 ′′

𝑛 ⊕ 𝜉 ′′

𝑛 ⊕ 𝜉 ′′

𝑛 ), 𝜉 ′

𝑛 ), 𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 ⊕ 𝜉 ′′

𝑛 ),
𝑛 )), 𝜉 ′′
𝑛 ). Since

𝑛 ⊕ 𝜉 ′′
This completes the proof.

D DEFERRED RESULTS IN §4.2

D.1 Deferred Statements and Their Proofs

Lemma D.1. Let 𝑐 be a command and 𝜋 be a simple reparameterisation plan. Suppose that for all

𝑛 ∈ NameEx, 𝑑, 𝑑 ′ ∈ DistEx, and (𝜆𝑦.𝑒) ∈ LamEx such that 𝜋 (𝑛, 𝑑, 𝜆𝑦.𝑒) = (𝑑 ′, _), we have
1, 𝑟 ′
2)

𝑑 ′ = distN (𝑟 ′

for some 𝑟 ′

2 ∈ R.

1, 𝑟 ′

Further, assume that for all 𝜎𝑛 ∈ St[Name], the function

𝜎𝜃 ∈ St[𝜃 ] ↦−→ 𝑝 ⟨rv (𝜋 ) ⟩
𝑐𝜋 ,𝜎𝜃
is continuous. Then, for all 𝜎𝜃 ∈ St[𝜃 ] and 𝜎𝑛 ∈ St[Name],
(𝜎𝑛) = 0.

∇𝜃 𝑝 ⟨rv (𝜋 ) ⟩
𝑐𝜋 ,𝜎𝜃

(𝜎𝑛)

Proof. Consider 𝑐 and 𝜋 that satisfies the given conditions. Fix 𝜎𝑛 ∈ St[Name]. Let 𝑓 : St[𝜃 ] → R
be the function in Eq. (20). Suppose that Eq. (21) does not hold. Then, 𝑓 is not a constant function.

On the one hand, since 𝑓 is continuous (by assumption) and not constant, the image of 𝑓 over its
domain (i.e., 𝑓 (St[𝜃 ]) ⊆ R) is an uncountable set. This can be shown as follows: since the image of
a connected set over a continuous function is connected, 𝑓 (St[𝜃 ]) is a connected set in R; since 𝑓
is not constant, 𝑓 (St[𝜃 ]) contains at least two points; since 𝑓 (St[𝜃 ]) is connected, it should contain
a non-empty interval, so it should be an uncountable set.

On the other hand, since 𝜋 is simple and satisfies Eq. (19), and since 𝑐 has only finitely many sample
commands, 𝑓 (St[𝜃 ]) is a finite set. So this contradicts to that 𝑓 (St[𝜃 ]) is an uncountable set. Hence,
□
𝑓 should satisfy Eq. (21).

Theorem D.2. Let 𝑓 : R × R𝑛 → R be a measurable function that satisfies the next conditions:
• For all 𝑥 ∈ R𝑛, 𝑓 (−, 𝑥) : R → R is differentiable.
∫
• For all 𝜃 ∈ R,
R𝑛 𝑓 (𝜃, 𝑥) 𝑑𝑥 is well-defined.
• For all 𝜃 ∈ R, there is an open 𝑈 ⊆ R such that 𝜃 ∈ 𝑈 and

∫
R𝑛 Lip(cid:0)𝑓 (−, 𝑥)|𝑈 (cid:1)𝑑𝑥 is well-defined.

Here Lip(𝑔) for a function 𝑔 : 𝑉 → R with 𝑉 ⊆ R denotes the smallest Lipschitz constant:

Lip(𝑔) ≜

sup
𝑟,𝑟 ′ ∈𝑉 , 𝑟 ≠𝑟 ′

|𝑔(𝑟 ′) − 𝑔(𝑟 )|
|𝑟 ′ − 𝑟 |

.

Then, for all 𝜃 ∈ R, both sides of the following are well-defined and equal:
∫

∫

∇𝜃

𝑓 (𝜃, 𝑥) 𝑑𝑥 =

∇𝜃 𝑓 (𝜃, 𝑥) 𝑑𝑥

R𝑛

R𝑛

where ∇𝜃 denotes the partial differentiation operator with respect to 𝜃 .

D.2 Proof of Theorem 4.4
The proof of Theorem 4.4 relies on the following two lemmas, which are proven in §D.3. The first
lemma states that if a command contains no observe commands, then its (full) density function can
be decomposed into its partial density functions over 𝑆 and Name \ 𝑆 for any 𝑆 ⊆ Name. The second
lemma states that if 𝜋 is simple and 𝑐 uses only 𝜆𝑦.𝑦 as the third argument of its sample commands,
then the partial density function of 𝑐𝜋 over non-transformed random variables (i.e., variables in
Name \ rv(𝜋)) is connected to that of 𝑐 via the value function of 𝑐𝜋 .

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:51

Lemma D.3. Let 𝑐 be a command. If 𝑐 does not contain observe commands, then, for all 𝑆 ⊆ Name,

𝜎𝜃 ∈ St[𝜃 ], and 𝜎𝑛 ∈ St[Name],

𝑝𝑐,𝜎𝜃 (𝜎𝑛) = 𝑝 ⟨𝑆 ⟩

𝑐,𝜎𝜃 (𝜎𝑛) · 𝑝 ⟨Name\𝑆 ⟩

𝑐,𝜎𝜃

(𝜎𝑛).

Lemma D.4. Let 𝑐 be a command and 𝜋 be a reparameterisation plan. Suppose that every sample com-
mand in 𝑐 has 𝜆𝑦.𝑦 as its third argument. Then, for all 𝜎𝜃 ∈ St[𝜃 ] and 𝜎𝑛 ∈ St[Name], if 𝑝𝑐𝜋 ,𝜎𝜃 (𝜎𝑛) > 0,
then

𝑝 ⟨Name\rv (𝜋 ) ⟩
𝑐𝜋 ,𝜎𝜃

(𝜎𝑛) = 𝑝 ⟨Name\rv (𝜋 ) ⟩
𝑐,𝜎𝜃

(𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)).

We now prove Theorem 4.4 using the two lemmas.

Proof of Theorem 4.4. Let 𝑆 = rv(𝜋). Before starting the main derivation of the selective gradi-
ent estimator, we show the differentiability of several functions which are to be used in the derivation.
From (R2) and (R3), the next functions over St[𝜃 ] are differentiable for all 𝜎𝑛 by the preservation of
differentiability under function composition:
𝜎𝜃 ↦−→ 𝑝 ⟨𝑆 ⟩
𝜎𝜃 ↦−→ 𝑝 ⟨𝑆 ⟩
𝑐𝑔

𝜎𝜃 ↦−→ 𝑝 ⟨Name\𝑆 ⟩
𝜎𝜃 ↦−→ 𝑝 ⟨Name\𝑆 ⟩
𝑐𝑔

𝜎𝜃 ↦−→ 𝑝𝑐𝑚,𝜎𝜃 (𝑣𝑐𝑔

(𝑣𝑐𝑔
(𝜎𝑛).

𝜋 ,𝜎𝜃 (𝜎𝑛)),

𝜋 ,𝜎𝜃 (𝜎𝑛)),

𝜋 ,𝜎𝜃 (𝜎𝑛)),

𝑐𝑔,𝜎𝜃 (𝑣𝑐𝑔

(𝜎𝑛),

𝑐𝑔,𝜎𝜃

𝜋 ,𝜎𝜃

𝜋 ,𝜎𝜃

From this, the next functions over St[𝜃 ] are also differentiable for all 𝜎𝑛 by Lemma D.3 with 𝑐𝑔 and
𝑐𝑔

𝜋 and by the fact that the multiplication of differentiable functions is differentiable:

𝜎𝜃 ↦−→ 𝑝𝑐𝑔,𝜎𝜃 (𝑣𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)),

𝜎𝜃 ↦−→ 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛).

These differentiability results are required in the below proof to apply several gradients rules (e.g.,
∇𝜃 (𝑓 (𝜃 ) + 𝑔(𝜃 )) = ∇𝜃 𝑓 (𝜃 ) + ∇𝜃𝑔(𝜃 )) which may fail for non-differentiable functions.

Fix 𝜎𝜃 ∈ St[𝜃 ]. Using the above differentiability results, we derive the selective gradient estimator

as follows, where we write 𝜎 ′

𝑛 for 𝑣𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛):

∇𝜃 L𝜃

= ∇𝜃

= ∇𝜃

∫

∫

∫

∫

∫

∫

=

=

=

=

∫

=

(cid:18)
𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛) · log

𝑑𝜎𝑛

(cid:19)

𝑝𝑐𝑚,𝜎𝜃 (𝜎𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎𝑛)

(cid:32)

𝑑𝜎𝑛

𝑝

𝑐𝑔

𝜋 ,𝜎𝜃

(𝜎𝑛) · log

𝑑𝜎𝑛 ∇𝜃

(cid:18)
𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛) · log

(cid:18)

𝑑𝜎𝑛

∇𝜃 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛) · log

(cid:33)

𝑛 )

𝑝𝑐𝑚,𝜎𝜃 ( 𝜎 ′
𝑝𝑐𝑔,𝜎𝜃 ( 𝜎 ′
𝑛 )
(cid:19)
𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)

+ 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛) · ∇𝜃 log

(cid:19)

𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)

𝑑𝜎𝑛 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)

(cid:18)

∇𝜃 log 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛) · log

𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)

− ∇𝜃 log 𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′

𝑛) + ∇𝜃 log 𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)

(cid:19)

𝑑𝜎𝑛 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)

(cid:20) (cid:16)

∇𝜃 log 𝑝 ⟨𝑆 ⟩
𝑐𝑔

𝜋 ,𝜎𝜃

(𝜎𝑛) + ∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩

𝑐𝑔

𝜋 ,𝜎𝜃

(cid:17)

(𝜎𝑛)

· log

𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)

(cid:16)

−

∇𝜃 log 𝑝 ⟨𝑆 ⟩

𝑐𝑔,𝜎𝜃 (𝜎 ′

𝑛) + ∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩

𝑐𝑔,𝜎𝜃

(cid:17)

(𝜎 ′
𝑛)

+ ∇𝜃 log 𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)

(cid:21)

𝑑𝜎𝑛 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)

(cid:20)

(cid:0) 0 + ∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩

𝑐𝑔

𝜋 ,𝜎𝜃

(𝜎𝑛)(cid:1) · log

𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)

− (cid:0)∇𝜃 log 𝑝 ⟨𝑆 ⟩

𝑐𝑔,𝜎𝜃 (𝜎 ′

𝑛) + 0 (cid:1) + ∇𝜃 log 𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)

(cid:21)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:52

∫

=

𝑑𝜎𝑛 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)

(cid:18)

∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩

𝑐𝑔 ,𝜎𝜃

( 𝜎 ′

𝑛 ) · log

𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)
𝑝𝑐𝑔,𝜎𝜃 (𝜎 ′
𝑛)

Wonyeol Lee, Xavier Rival, and Hongseok Yang

− ∇𝜃 log 𝑝 ⟨𝑆 ⟩

𝑐𝑔,𝜎𝜃 (𝜎 ′

𝑛) + ∇𝜃 log 𝑝𝑐𝑚,𝜎𝜃 (𝜎 ′
𝑛)

(cid:19)

.

We justify key steps of the above derivation below.

• The second equality comes from Theorem 4.2 and the fact that 𝑣𝑐𝑔,𝜎𝜃 is the identity function

(since the third argument of every sample command in 𝑐𝑔 is the identity function 𝜆𝑦.𝑦).
• The third equality holds because differentiation there commutes with integration by (R5).
• The fourth comes from the product rule for differentiation: ∇𝜃 (𝑓 (𝜃 ) · 𝑔(𝜃 )) = ∇𝜃 𝑓 (𝜃 ) · 𝑔(𝜃 ) +
𝑓 (𝜃 )·∇𝜃𝑔(𝜃 ) for all differentiable 𝑓 and𝑔. Here 𝑓 and𝑔 in the original equation are differentiable
because differentiability is preserved under division and log for positive-valued functions.
• The fifth equality holds because ∇𝜃 𝑓 (𝜃 ) = 𝑓 (𝜃 ) · ∇𝜃 log 𝑓 (𝜃 ) for all differentiable and positive-

valued 𝑓 .

• The sixth equality follows from Lemma D.3 applied to 𝑐𝑔 and 𝑐𝑔

𝜋 (both of which do not
contain observe commands), and from the linearity of differentiation: ∇𝜃 (𝑓 (𝜃 ) + 𝑔(𝜃 )) =
∇𝜃 𝑓 (𝜃 ) + ∇𝜃𝑔(𝜃 ) for all differentiable 𝑓 and 𝑔. Here 𝑓 and 𝑔 in the original equation are
differentiable because differentiability is preserved under log for positive-valued functions.

• The seventh equality follows from (R4) and

(cid:2)∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩
The proof of Eq. (22) will be given after we complete this justification of the derivation.

E𝑝𝑐𝑔 𝜋 ,𝜎𝜃 (𝜎𝑛)

𝑛)(cid:3) = 0.

(𝜎 ′

𝑐𝑔,𝜎𝜃

(22)

• The last equality comes from Lemma D.4 applied to 𝑐𝑔.
The only remaining part is to prove Eq. (22). We derive the equation as follows:

𝜋 ,𝜎𝜃 (𝜎𝑛) · ∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩

𝑐𝑔,𝜎𝜃

(𝜎 ′

𝑛)(cid:1)

𝑑𝜎𝑛 (cid:0)𝑝𝑐𝑔
∫

𝑑𝜎𝑛 (cid:0)𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛) · ∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩

( 𝜎𝑛 )(cid:1)

𝜋 ,𝜎𝜃 (𝜎𝑛) ·

(cid:16)

𝜋 ,𝜎𝜃

𝑐𝑔
∇𝜃 log 𝑝 ⟨Name\𝑆 ⟩
𝑐𝑔

𝜋 ,𝜎𝜃

𝜋 ,𝜎𝜃 (𝜎𝑛) · ∇𝜃 log 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)

(cid:17)

𝑑𝜎𝑛

𝑑𝜎𝑛

(cid:16)
𝑝𝑐𝑔

(cid:16)
𝑝𝑐𝑔

∫

=

=

=

=

∫

∫

∫

𝜋 ,𝜎𝜃 (𝜎𝑛)

𝑑𝜎𝑛 ∇𝜃 𝑝𝑐𝑔
∫

= ∇𝜃

𝑑𝜎𝑛 𝑝𝑐𝑔

𝜋 ,𝜎𝜃 (𝜎𝑛)

(𝜎𝑛) + ∇𝜃 log 𝑝 ⟨𝑆 ⟩
𝑐𝑔

𝜋 ,𝜎𝜃

(𝜎𝑛)

(cid:17)(cid:17)

= ∇𝜃 1 = 0.

Here is the justification of the above derivation:

• The first equality comes from Lemma D.4 applied to 𝑐𝑔.
• The second equality follows from (R4).
• The third equality holds because of Lemma D.3 applied to 𝑐𝑔

𝜋 (which does not contain ob-
serve commands), and the linearity of differentiation: ∇𝜃 (𝑓 (𝜃 ) + 𝑔(𝜃 )) = ∇𝜃 𝑓 (𝜃 ) + ∇𝜃𝑔(𝜃 )
for all differentiable 𝑓 and 𝑔. Here 𝑓 and 𝑔 in the original equation are differentiable because
differentiability is preserved under log for positive-valued functions.

• The fourth equality holds because ∇𝜃 𝑓 (𝜃 ) = 𝑓 (𝜃 ) · ∇𝜃 log 𝑓 (𝜃 ) for all differentiable and

positive-valued 𝑓 .

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:53

• The fifth equality uses (R5), which states the commutativity between differentiation and

integration in the equality.

• The six equality comes from that 𝑝𝑐𝑔

𝜋 ,𝜎𝜃

is a probability density by Remark 1.

This completes the proof.

□

D.3 Proofs of Lemmas D.3 and D.4
We define the partial density version of prs ⟨𝑆 ⟩

□ (𝑐) for 𝑆 ⊆ Name:

prs ⟨𝑆 ⟩

□ (𝑐) : St[PVar] × St□ [Name] → [0, ∞),

prs ⟨𝑆 ⟩

□ (𝑐)(𝜎𝑝, 𝜉𝑛) ≜

(cid:40)(cid:206)𝜇 ∈dom(𝜉𝑛)∩𝑆
0

𝑐
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 )(pr 𝜇)

if ∃𝜎𝑟 . used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛)
otherwise.

□ (𝑐) enjoys many of the properties that prs□(𝑐) has. For instance, prs ⟨𝑆 ⟩

prs ⟨𝑆 ⟩
□ (𝑐) is a well-defined
function (i.e., its value does not depend on the choice of 𝜎𝑟 ), as prs□ (𝑐) does. Since the proof of those
properties of prs ⟨𝑆 ⟩
□ (𝑐) would be almost identical to that of prs□(𝑐), we will use them in the following
proofs without explicitly (re)proving them.

Proof of Lemma D.3. Let 𝑐 be a command that has no observe commands. Let 𝑆 ⊆ Name,
𝜎𝜃 ∈ St[𝜃 ], and 𝜎𝑛 ∈ St[Name]. We set 𝜎0 as in the definition of 𝑝𝑐,𝜎𝜃 in Eq. (3) (as a function
of 𝜎𝑛). Let 𝜎 = 𝜎𝜃 ⊕ 𝜎𝑛 ⊕ 𝜎0. If noerr (𝑐, 𝜎) does not hold, then the LHS and RHS of the desired equation
become zero, so the equation holds. If noerr (𝑐, 𝜎) holds, we get the desired equation as follows:

𝑐,𝜎𝜃 (𝜎𝑛) · 𝑝 ⟨Name\𝑆 ⟩
𝑝 ⟨𝑆 ⟩

𝑐,𝜎𝜃

𝜎 (pr 𝜇)(cid:1) · (cid:0)
𝑐
𝑐
(cid:74)
(cid:75)
(cid:74)
· (cid:206)𝜇 ∈Name
𝜎 (pr 𝜇)
𝑐
(cid:75)
(cid:74)

𝜎 (like) · (cid:206)𝜇 ∈𝑆
(cid:75)
𝜎 (like)(cid:1) 2
(cid:75)
𝜎 (like) · 𝑝𝑐,𝜎𝜃 (𝜎𝑛)
(cid:75)

(𝜎𝑛) = (cid:0)
𝑐
(cid:74)
= (cid:0)
𝑐
(cid:74)
𝑐
=
(cid:74)
= 𝜎 (like) · 𝑝𝑐,𝜎𝜃 (𝜎𝑛)
= 𝑝𝑐,𝜎𝜃 (𝜎𝑛).

𝜎 (like) · (cid:206)𝜇 ∈Name\𝑆
(cid:75)

𝑐
(cid:74)

𝜎 (pr 𝜇)(cid:1)
(cid:75)

The second last equality uses Lemma D.5, and the last equality uses 𝜎 (like) = 1 (which holds by the
□
definition of 𝜎0). This completes the proof.

Proof of Lemma D.4. Consider a command 𝑐, a reparameterisation plan 𝜋, 𝜎𝜃 ∈ St[𝜃 ], and
𝜎𝑛 ∈ St[Name]. Assume that all the sample commands of 𝑐 have 𝜆𝑦.𝑦 as their third arguments, and
𝑝𝑐𝜋 ,𝜎𝜃 (𝜎𝑛) > 0.

We first define several objects and make observations on them. Let 𝑆 ≜ Name \ rv(𝜋). Define

𝑓∗ : St[Name] → St[AVar] to be the function for constructing an initial state:



if 𝑣 ≡ pr 𝜇 for some 𝜇
if 𝑣 ≡ val𝜇 for some 𝜇
if 𝑣 ≡ cnt𝜇 for some 𝜇
if 𝑣 ≡ like,

𝑓pr (𝜎𝑛 (𝜇))
𝑓val (𝜎𝑛 (𝜇))
𝑓cnt (𝜎𝑛 (𝜇))
1

𝑓∗ (𝜎𝑛)(𝑣) ≜



where 𝑓val (𝑟 ) ≜ 𝑟 , 𝑓pr (𝑟 ) ≜ N (𝑟 ; 0, 1), and 𝑓cnt (𝑟 ) ≜ 0. Define initial states 𝜎, 𝜎 ∈ St for 𝑝𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)
and 𝑝𝑐,𝜎𝜃 (𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)), respectively, as

𝜎 ≜ 𝜎𝑝 ⊕ 𝜎𝑛 ⊕ 𝑓∗ (𝜎𝑛),

𝜎 ≜ 𝜎𝑝 ⊕ 𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛) ⊕ 𝑓∗ (𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)),

where 𝜎𝑝 ≜ 𝜎𝜃 ⊕ (𝜆𝑣 ∈ PVar \ 𝜃 . 0) ∈ St[PVar]. Then, the assumption 𝑝𝑐𝜋 ,𝜎𝜃 (𝜎𝑛) > 0 implies
noerr (𝑐𝜋, 𝜎) by the definition of 𝑝. From this, 𝜎 (like) = 1, and the definition of used, there exists

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:54

Wonyeol Lee, Xavier Rival, and Hongseok Yang

𝜉𝑛 ∈ St□ [Name] such that used (𝑐𝜋, 𝜎, 𝜉𝑛). From this, we have

𝜎 = 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 ,

used (𝑐𝜋, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛),

for some 𝜎𝑟 . Next, let

𝜉𝑛 ≜ vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛).
We can apply Lemma D.6 to used (𝑐𝜋, 𝜎, 𝜉𝑛), since all the sample commands of 𝑐 have 𝜆𝑦.𝑦 in their
third arguments (by assumption). The application of the lemma gives:

∀𝜎 ′

𝑟 ∈ St[Var \ (PVar ∪ dom(𝜉𝑛))]. 𝜎 ′

𝑟 (like) = 1 =⇒ used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎 ′

𝑟 , 𝜉𝑛),

prs ⟨𝑆 ⟩

□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = prs ⟨𝑆 ⟩

□ (𝑐)(𝜎𝑝, 𝜉𝑛),

where the for-all part comes from Lemma C.7.

We now show two claims. The first claims is: there exists 𝜎𝑟 such that

𝜎 = 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 ,

used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛).

By the definition of 𝜎, it suffices to show that 𝜉𝑛 = (cid:0)𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)(cid:1) |dom(𝜉𝑛) . This indeed holds as fol-
lows: for any 𝜇 ∈ dom(𝜉𝑛), 𝜉𝑛 (𝜇) = vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)(𝜇) =
𝜎 (val𝜇) = 𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)(𝜇), where the
(cid:75)
second equality uses used (𝑐𝜋, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛). The second claim is: for all 𝜇 ∈ 𝑆 \ dom(𝜉𝑛),

𝑐𝜋
(cid:74)

𝜎 (pr 𝜇) = 𝜎 (pr 𝜇).

Here is the proof of the claim: 𝜎 (pr 𝜇) = 𝑓pr (𝜎 (𝜇)) = 𝑓pr (𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)(𝜇)) = 𝑓pr (
𝜎 (val𝜇)) =
(cid:75)
𝑓pr (𝜎 (val𝜇)) = 𝑓pr (𝜎 (𝜇)); and 𝜎 (pr 𝜇) = 𝑓pr (𝜎 (𝜇)); here the second last equality in the first equation
uses Lemma C.6-(2) with 𝜇 ∉ dom(𝜉𝑛) and used (𝑐𝜋, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛), and the last equality in the first
equation uses 𝑓val (𝑟 ) = 𝑟 .

Based on the observations made so far, we show the desired equation as follows:

𝑐𝜋
(cid:74)

𝑝 ⟨𝑆 ⟩
𝑐𝜋 ,𝜎𝜃

(𝜎𝑛) =

(cid:214)
𝜇 ∈𝑆∩dom(𝜉𝑛) (cid:74)

𝑐𝜋

𝜎 (pr 𝜇) ·
(cid:75)

= prs ⟨𝑆 ⟩

□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) ·

𝑐𝜋

(cid:214)
𝜇 ∈𝑆\dom(𝜉𝑛) (cid:74)
(cid:214)
𝜎 (pr 𝜇)

𝜎 (pr 𝜇)
(cid:75)

= prs ⟨𝑆 ⟩

□ (𝑐)(𝜎𝑝, 𝜉𝑛) ·

𝜇 ∈𝑆\dom(𝜉𝑛)

(cid:214)

𝜎 (pr 𝜇)

𝜇 ∈𝑆\dom(𝜉𝑛)

=

(cid:214)
𝑐
𝜇 ∈𝑆∩dom(𝜉𝑛) (cid:74)
𝑐,𝜎𝜃 (𝑣𝑐𝜋 ,𝜎𝜃 (𝜎𝑛)).

= 𝑝 ⟨𝑆 ⟩

𝜎 (pr 𝜇) ·
(cid:75)

(cid:214)
𝑐
𝜇 ∈𝑆\dom(𝜉𝑛) (cid:74)

𝜎 (pr 𝜇)
(cid:75)

The first and last equalities are by the definition of 𝑝. The second equality uses used (𝑐𝜋, 𝜎𝑝 ⊕𝜉𝑛 ⊕𝜎𝑟 , 𝜉𝑛)
and Lemma C.6-(2) with 𝜇 ∉ dom(𝜉𝑛). The third equality uses dom(𝜉𝑛) = dom(𝜉𝑛), the observation
made in the first paragraph, and the second claim in the above. The fourth equality uses the first
□
claim in the above, and Lemma C.6-(2) with 𝜇 ∉ dom(𝜉𝑛).

Lemma D.5. Let 𝑐 be a command and 𝜎 ∈ St. If 𝑐 has no observe commands and
𝑐
(cid:74)

𝜎 (like) = 𝜎 (like).
(cid:75)

𝑐
(cid:74)

𝜎 ∈ St, then
(cid:75)

Proof. Let 𝑐 be a command that does not contain an observe command. We show the claim of
𝜎 ∈ St. We will show that
(cid:75)

the lemma by induction on the structure of 𝑐. Pick 𝜎 ∈ St such that
𝑐
(cid:74)
Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

𝜎 (like) = 𝜎 (like).
(cid:75)

𝑐
(cid:74)

Smoothness Analysis and Selective Reparameterisation

0:55

𝑐
(cid:74)

𝜎 =
(cid:75)

𝑐 ′
(cid:74)

𝜎.
(cid:75)

𝜎 (like) = 𝜎 (like) by the definition of the semantics.
𝑐
(cid:75)
(cid:74)
𝜎 (like) = 𝜎 (like) by the definition of the semantics.
(cid:75)

Case 𝑐 ≡ skip. In this case,
Case 𝑐 ≡ (𝑥 := 𝑒). Again,
𝑐
(cid:74)
Case𝑐 ≡ (𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒 ′). Once more,
Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). We have
𝜎 ∈ St and
(cid:75)
𝑐 ′
(cid:74)

to (𝑐 ′, 𝜎), and again to (𝑐 ′,
𝑐 ′; 𝑐 ′′
𝜎 (like) =
(cid:75)
(cid:74)

𝑐 ′
(cid:74)

𝜎 (like) = 𝜎 (like) by the definition of the semantics.
𝑐
(cid:74)
(cid:75)
𝜎) ∈ St. We apply induction hypothesis first
𝑐 ′′
𝑐 ′
(cid:74)
(cid:75)
(cid:74)
𝜎 (like) = 𝜎 (like), and the second
𝜎). The first application gives
(cid:75)
(cid:75)
𝜎 (like). The desired conclusion follows from these two equalities.
(cid:75)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

(cid:75)

(

Case 𝑐 ≡ (if 𝑏 {𝑐 ′} else {𝑐 ′′}). We deal with the case that
𝜎 = true, we have
𝜎 = false can be proved similarly. Since
(cid:75)
(cid:75)
𝑐 ′
(cid:74)

𝑏
(cid:74)
apply induction hypothesis to 𝑐 ′. If we do so, we get
conclusion because

𝜎 = true. The other case of
𝑏
(cid:75)
(cid:74)
𝜎 ∈ St. Thus, we can
𝜎 =
𝑐 ′
(cid:75)
(cid:75)
(cid:74)
𝜎 (like) = 𝜎 (like). This gives the desired
(cid:75)

𝑏
(cid:74)

𝑐
(cid:74)

Case 𝑐 ≡ (while 𝑏 {𝑐 ′}). Let 𝐹 be the operator on [St → St⊥] such that

of 𝐹 . Define a subset T of [St → St⊥] as follows:

is the least fixed point

𝑐
(cid:74)

(cid:75)

𝑓 ∈ T ⇐⇒

(cid:16)

∀𝜎 ′ ∈ St. 𝑓 (𝜎 ′) ∈ St =⇒ 𝑓 (𝜎 ′)(like) = 𝜎 ′(like).

(cid:17)

The set T contains the least function 𝜆𝜎.⊥, and is closed under the least upper bound of any chain
in [St → St⊥]. It is also closed under 𝐹 . This 𝐹 -closure follows essentially from our arguments for
sequential composition, if command, and skip, and induction hypothesis on 𝑐 ′. What we have shown
for T implies that T contains the least fixed point of 𝐹 , which gives the desired property for 𝑐. □

Lemma D.6. Let 𝑐 be a command and 𝜋 be a reparameterisation plan. Suppose that every sample
command in 𝑐 has 𝜆𝑦.𝑦 as its third argument. Then, for all 𝜎𝑝 ∈ St[PVar] and 𝜉𝑛 ∈ St□ [Name], if
used (𝑐𝜋, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛) for some 𝜎𝑟 , then

∃𝜎𝑟 . used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛),
pvars□(𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛),
(𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = prs ⟨Name\rv (𝜋 ) ⟩

□

prs ⟨Name\rv (𝜋 ) ⟩

□

(𝑐)(𝜎𝑝, 𝜉𝑛),

where

𝜉𝑛 ≜ vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛).
Proof. Fix a reparameterisation plan 𝜋. The proof proceeds by induction on the structure of 𝑐. Let
𝜎𝑝 ∈ St[PVar], and 𝜉𝑛 ∈ St□ [Name]. Assume that 𝑐 uses only 𝜆𝑦.𝑦 in the third argument of its sample
commands, and used (𝑐𝜋, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛) for some 𝜎𝑟 . Let 𝜎 ≜ 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 and 𝑆 ≜ Name \ rv(𝜋).
Then, we simply have used (𝑐𝜋, 𝜎, 𝜉𝑛).

Cases 𝑐 ≡ skip, 𝑐 ≡ (𝑥 := 𝑒), or 𝑐 ≡ obs(𝑑, 𝑟 ). In this case,

𝜎 (cnt𝜇) = 𝜎 (cnt𝜇) for all 𝜎 ∈ St
(cid:75)
and 𝜇 ∈ Name. So dom(𝜉𝑛) = dom(𝜉𝑛) = ∅ and thus 𝜉𝑛 = 𝜉𝑛. We also know 𝑐𝜋 ≡ 𝑐. From these, all
of the three conclusions follow immediately.

𝑐
(cid:74)

Case 𝑐 ≡ (𝑥 := sam(𝑛, 𝑑, 𝜆𝑦.𝑒)). Since fv(𝑛) ⊆ PVar, there exists 𝜇 ∈ Name such that

𝜎𝑟 ) = 𝜇 for all 𝜎𝑟 ∈ St[Var \ PVar]. So, for all 𝜎𝑟 ∈ St[Var \ PVar] and 𝜇 ′ ∈ Name \ {𝜇},
(cid:40)

(𝜎𝑝 ⊕ 𝜎𝑟 )(cnt𝜇′) =

𝑐
(cid:74)

(cid:75)

(𝜎𝑝 ⊕ 𝜎𝑟 )(cnt𝜇′) + 1
(𝜎𝑝 ⊕ 𝜎𝑟 )(cnt𝜇′)

if 𝜇 ′ = 𝜇
otherwise.

(𝜎𝑝 ⊕

𝑛
(cid:74)

(cid:75)

(23)

From this, we get dom(𝜉𝑛) = dom(𝜉𝑛) = {𝜇}. Further, by assumption, we get 𝑒 ≡ 𝑦. We now prove
the three conclusions based on these observations and case analysis on (𝑛, 𝑑, 𝜆𝑦.𝑒).

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:56

Wonyeol Lee, Xavier Rival, and Hongseok Yang

First, assume (𝑛, 𝑑, 𝜆𝑦.𝑒) ∉ dom(𝜋). Then, 𝑐𝜋 ≡ 𝑐 and

𝜉𝑛 = vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = [𝜇 ↦→

𝑐𝜋
(cid:74)

𝜎 (val𝜇)] = [𝜇 ↦→
(cid:75)

𝑒 [𝜎 (𝜇)/𝑦]
(cid:74)

𝜎] = [𝜇 ↦→ 𝜉𝑛 (𝜇)] = 𝜉𝑛,
(cid:75)

where the second last equality uses 𝑒 ≡ 𝑦. Hence, the three conclusions clearly hold.

Next, assume (𝑛, 𝑑, 𝜆𝑦.𝑒) ∈ dom(𝜋). Suppose that 𝜋 (𝑛, 𝑑, 𝜆𝑦.𝑒) = (𝑑, 𝜆𝑦.𝑒). Then, 𝑐𝜋 ≡ (𝑥 :=

sam(𝑛, 𝑑, 𝜆𝑦.𝑒)) and

𝜉𝑛 = vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = [𝜇 ↦→

Since Eq. (23) holds also for

𝑐𝜋
(cid:74)

(cid:75)

𝑐𝜋
(cid:74)

𝜎 (val𝜇)] = [𝜇 ↦→
(cid:75)

𝑒 [𝜎 (𝜇)/𝑦]
(cid:74)

𝜎] = [𝜇 ↦→
(cid:75)

, and since dom(𝜉𝑛) = {𝜇}, we get the first conclusion:

𝑒 [𝜉𝑛 (𝜇)/𝑦]
(cid:74)

𝜎].
(cid:75)

used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛).

To prove the second conclusion, let 𝜎 = 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 . Then, for all 𝑣 ∈ PVar,

pvars□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)(𝑣) =

𝑐𝜋
(cid:74)

𝜎 (𝑣) =
(cid:75)

(cid:40)

𝜎 =
𝑒 [𝜎 (𝜇)/𝑦]
(cid:74)
(cid:75)
𝜎 (𝑣) = 𝜎𝑝 (𝑣)

𝑒 [𝜉𝑛 (𝜇)/𝑦]
(cid:74)

𝜎
(cid:75)

if 𝑣 ≡ 𝑥
otherwise,

(cid:40)

𝑐
(cid:74)

𝜎 (𝑣) =
(cid:75)

pvars□ (𝑐)(𝜎𝑝, 𝜉𝑛)(𝑣) =

𝜎 = 𝜉𝑛 (𝜇) =
𝑒 [𝜎 (𝜇)/𝑦]
(cid:74)
(cid:75)
𝜎 (𝑣) = 𝜎𝑝 (𝑣),
where the second equation uses 𝑒 ≡ 𝑦. Hence, the second conclusion holds. For the third conclusion,
let 𝑛 = name(𝛼, _). Then, 𝜇 = (𝛼, _) ∈ {(𝛼, 𝑖) ∈ Name | 𝑖 ∈ N} ⊆ rv(𝜋). Thus, dom(𝜉𝑛) ∩ 𝑆 =
dom(𝜉𝑛) ∩ 𝑆 = {𝜇} ∩ 𝑆 = {𝜇} ∩ (Name \ rv(𝜋)) = ∅. From this, we get the third conclusion:
□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = 1 = prs ⟨𝑆 ⟩

𝑒 [𝜉𝑛 (𝜇)/𝑦]
(cid:74)

if 𝑣 ≡ 𝑥
otherwise,

□ (𝑐)(𝜎𝑝, 𝜉𝑛).

prs ⟨𝑆 ⟩

𝜎
(cid:75)

Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). First, we make several observations necessary to prove the conclusion. By
𝜋
𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

used (𝑐𝜋, 𝜎, 𝜉𝑛), we have
𝜎 ′ ≜

𝜎) ∈ St. Let
(cid:75)
𝜎 ′
𝑝 ≜ 𝜎 ′|PVar,

𝜎 ′′
𝑝 ≜ 𝜎 ′′|PVar.

(cid:75)

(

𝜋

𝜋

𝜋

𝜎 ∈ St and
𝑐 ′′
(cid:74)
(cid:75)
𝜋
𝜎 ′′ ≜
𝑐 ′′
(cid:74)

𝜎 ′,
(cid:75)

Then, by used (𝑐𝜋, 𝜎, 𝜉𝑛) and the claim in the proof of Lemma C.6 (for the sequential composition
case), there exist 𝜉 ′

𝑐 ′
(cid:74)

𝜎,
(cid:75)

𝑛 and 𝜉 ′′
𝜉𝑛 = 𝜉 ′

𝑛 such that
𝑛 ,
𝑛 ⊕ 𝜉 ′′

used (𝑐 ′

𝜋, 𝜎, 𝜉 ′

𝑛),

used− (𝑐 ′′

𝜋, 𝜎 ′, 𝜉 ′′
𝑛 ).

By the latter two, we can apply induction to (𝑐 ′, 𝜎𝑝, 𝜉 ′

𝑛) and (𝑐 ′′, 𝜎 ′

𝑝, 𝜉 ′′

𝑛 ), and IH gives the following:

pvars□(𝑐 ′)(𝜎𝑝, 𝜉 ′

pvars□ (𝑐 ′′)(𝜎 ′

𝑝, 𝜉 ′′

𝜋

𝜋

𝜋

(cid:75)

= (

= (cid:0)

𝑛) = pvars□ (𝑐 ′
𝑐 ′
(cid:74)
𝑐 ′
(cid:74)
𝑛 ) = pvars□ (𝑐 ′′
𝑐 ′′
(cid:74)
𝑐 ′′
(cid:74)

)(𝜎𝑝, 𝜉 ′
𝑛)
(𝜎 [like ↦→ 1])(cid:1) |PVar
𝜎)|PVar = 𝜎 ′|PVar = 𝜎 ′
𝑝,
(cid:75)
𝑝, 𝜉 ′′
𝑛 )
(𝜎 ′[like ↦→ 1])(cid:1) |PVar
𝜎 ′)|PVar = 𝜎 ′′|PVar = 𝜎 ′′
𝑝 ,
(cid:75)

)(𝜎 ′

= (cid:0)

= (

(cid:75)

𝜋

𝜋

𝜋

where

[By IH on 𝑐 ′]
𝜋, 𝜎, 𝜉 ′

𝑛)]
[By used (𝑐 ′
[By Lemma C.6-(3)]

[By IH on 𝑐 ′′]
𝜋, 𝜎 ′, 𝜉 ′′

[By used− (𝑐 ′′

𝑛 )]
[By Lemma C.6-(3)]

𝜋

𝜉 ′
𝑛 ≜ vals□(𝑐 ′
By the former equation, we get
𝜉𝑛 = vals□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛)

)(𝜎𝑝, 𝜉 ′

𝑛),

𝜉 ′′
𝑛 ≜ vals□ (𝑐 ′′

𝜋

)(𝜎 ′

𝑝, 𝜉 ′′

𝑛 ).

= vals□ (𝑐 ′

𝜋 ; 𝑐 ′′

𝜋

)(𝜎𝑝, 𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 )

[By 𝜉𝑛 = 𝜉 ′

𝑛 ]
𝑛 ⊕ 𝜉 ′′

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:57

)(𝜎𝑝, 𝜉 ′

)(𝜎𝑝, 𝜉 ′

𝑛) ⊕ vals□ (𝑐 ′′
𝑛) ⊕ vals□ (𝑐 ′′

𝜋

)(𝜎𝑝, 𝜉 ′

𝑛), 𝜉 ′′
𝑛 )

𝜋

𝜋

)(pvars□ (𝑐 ′
)(𝜎 ′
𝑝, 𝜉 ′′
𝑛 )

[By the former equation]

𝜋

𝜋

= vals□ (𝑐 ′
= vals□ (𝑐 ′
𝑛 ⊕ 𝜉 ′′
= 𝜉 ′
𝑛

where the third equality uses used (𝑐 ′
proof of Lemma C.9.

𝜋, 𝜎, 𝜉 ′

𝑛), used (𝑐 ′

𝜋 ; 𝑐 ′′

𝜋, 𝜎, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ), and the second claim in the

We now show the first conclusion. By IH on (𝑐 ′, 𝜎𝑝, 𝜉 ′

∃𝜎 ′

𝑟 . used (𝑐 ′, 𝜎𝑝 ⊕ 𝜉 ′

𝑛 ⊕ 𝜎 ′

𝑟 , 𝜉 ′

𝑛),

𝑝, 𝜉 ′′

𝑛) and (𝑐 ′′, 𝜎 ′
𝑟 . used (𝑐 ′′, 𝜎 ′

𝑛 ), we get
𝑛 ⊕ 𝜎 ′′
𝑝 ⊕ 𝜉 ′′

∃𝜎 ′′

𝑟 , 𝜉 ′′

𝑛 ).

Let

Then,

𝑐 ′
(cid:74)

𝜎 ∈ St by used (𝑐 ′, 𝜎𝑝 ⊕ 𝜉 ′
(cid:75)
𝜎 |PVar = 𝜎𝑝,

(

𝑛 ⊕ 𝜉 ′′
𝜎 ≜ 𝜎𝑝 ⊕ (𝜉 ′
𝑟 , 𝜉 ′
𝑛 ⊕ 𝜎 ′
𝜎)|PVar = (
(cid:75)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

(cid:75)

𝑛 ) ⊕ 𝜎 ′

𝑟 |dom(𝜎′

𝑛) .
𝑟 )\dom(𝜉 ′′

𝑛) and Lemma C.6-(1), and we have

(𝜎𝑝 ⊕ 𝜉 ′

𝑛 ⊕ 𝜎 ′

𝑟 ))|PVar
𝑛) = 𝜎 ′
𝑝,

(

𝑐 ′
𝜎)|dom(𝜉 ′′
(cid:75)
(cid:74)
𝑟 , 𝜉 ′
𝑛 ⊕ 𝜎 ′
used (𝑐 ′, 𝜎, 𝜉 ′

𝑛), used (𝑐 ′′, 𝜎 ′
𝑛),

𝑛) = 𝜎 |dom(𝜉 ′′
𝑝 ⊕ 𝜉 ′′

= pvars(𝑐 ′)(𝜎𝑝, 𝜉 ′
𝑛) = 𝜉 ′′
𝑛 .
𝑟 , 𝜉 ′′
𝑛 ⊕ 𝜎 ′′
used− (𝑐 ′′,

𝜎 |dom(𝜉 ′

𝑛) = 𝜉 ′
𝑛,

By these, used (𝑐 ′, 𝜎𝑝 ⊕ 𝜉 ′

By these and the third claim in the proof of Lemma C.9, we get

𝑛 ), and Lemma C.7, we get
𝜎, 𝜉 ′′
(cid:75)

𝑐 ′
(cid:74)

𝑛 ).

[By Lemma C.6-(3)]

[By the above]

[By Lemma C.4]

used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ).

By this and Lemma C.7, we get the following as desired, since 𝜉𝑛 = 𝜉 ′
𝜎 = 𝜎𝑝 ⊕ (𝜉 ′

𝑛 ) ⊕ 𝜎𝑟 for some 𝜎𝑟 :

𝑛 ⊕ 𝜉 ′′

𝑛 ⊕ 𝜉 ′′

𝑛 (shown in the above) and

Next, we show the second conclusion as follows:

used (𝑐, 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 , 𝜉𝑛).

pvars□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = (
𝜎)(cid:1) |PVar
= (cid:0)
(cid:75)
(cid:75)
= 𝜎 ′′|PVar = 𝜎 ′′
𝑝 ,

𝜎)|PVar
𝜋
𝜋
(cid:75)
𝑐 ′
(
(cid:74)

𝑐𝜋
(cid:74)
𝑐 ′′
(cid:74)

[By used (𝑐𝜋, 𝜎, 𝜉𝑛)]

pvars□(𝑐)(𝜎𝑝, 𝜉𝑛) = pvars□ (𝑐 ′; 𝑐 ′′)(𝜎𝑝, 𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 )

[By 𝜉𝑛 = 𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 ]

= pvars□(𝑐 ′′)(pvars□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
= pvars□(𝑐 ′′)(𝜎 ′

𝑛 ) = 𝜎 ′′
𝑝 ,

𝑛), 𝜉 ′′
𝑛 )

𝑝, 𝜉 ′′
where the second last equality uses used (𝑐 ′, 𝜎, 𝜉 ′
the proof of Lemma C.9.

[By the above]

𝑛), used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ), and the second claim in

prs ⟨𝑆 ⟩

𝜋

𝜋

)(𝜎𝑝, 𝜉 ′

Lastly, we show the third conclusion as follows:
□ (𝑐𝜋 )(𝜎𝑝, 𝜉𝑛) = prs ⟨𝑆 ⟩
= prs ⟨𝑆 ⟩
= prs ⟨𝑆 ⟩
= prs ⟨𝑆 ⟩
= prs ⟨𝑆 ⟩
= prs ⟨𝑆 ⟩

□ (𝑐 ′; 𝑐 ′′
)(𝜎𝑝, 𝜉 ′
□ (𝑐 ′
)(𝜎𝑝, 𝜉 ′
□ (𝑐 ′
□ (𝑐 ′)(𝜎𝑝, 𝜉 ′
□ (𝑐)(𝜎𝑝, 𝜉 ′
□ (𝑐)(𝜎𝑝, 𝜉𝑛).

𝑛 ⊕ 𝜉 ′′
𝑛 )
𝑛) · prs ⟨𝑆 ⟩
□ (𝑐 ′′
𝑛) · prs ⟨𝑆 ⟩
□ (𝑐 ′′
𝑛) · prs ⟨𝑆 ⟩
□ (𝑐 ′′)(pvars(𝑐 ′)(𝜎𝑝, 𝜉 ′
𝑛 ⊕ 𝜉 ′′
𝑛 )

)(pvars(𝑐 ′
)(𝜎𝑝, 𝜉 ′
)(pvars(𝑐 ′)(𝜎𝑝, 𝜉 ′

𝜋

𝜋

𝜋

𝜋

𝑛), 𝜉 ′′
𝑛 )
𝑛), 𝜉 ′′
𝑛 )
𝑛), 𝜉 ′′
𝑛 )

[By 𝜉𝑛 = 𝜉 ′

𝑛 ]
𝑛 ⊕ 𝜉 ′′

[By IH on 𝑐 ′]
[By IH on 𝑐 ′ and 𝑐 ′′]

[By 𝜉𝑛 = 𝜉 ′

𝑛 ⊕ 𝜉 ′′
𝑛 ]

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:58

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Here the second and fifth equalities use the second claim in the proof of Lemma C.9 with the following:
used (𝑐 ′

𝑛), and used (𝑐 ′; 𝑐 ′′, 𝜎, 𝜉 ′

𝑛 ), used (𝑐 ′, 𝜎, 𝜉 ′

𝑛), used (𝑐 ′

𝜋, 𝜎, 𝜉 ′

𝜋, 𝜎, 𝜉 ′

𝑛 ⊕ 𝜉 ′′

𝑛 ⊕ 𝜉 ′′

𝜋 ; 𝑐 ′′

𝑛 ).

Case 𝑐 ≡ (if 𝑏 {𝑐 ′} else {𝑐 ′′}). In this case, 𝑐𝜋 ≡ (if 𝑏 {𝑐 ′
(𝜎𝑝 ⊕ 𝜎𝑟 ) is constant for all 𝜎𝑟 ∈ St[Var \ PVar]. Without loss of generality, assume
𝑐𝜋
(cid:74)

𝑏
(cid:74)
true. Then,
(cid:75)
Hence, by IH on (𝑐 ′, 𝜎𝑝, 𝜉𝑛), we get the three conclusions directly.

}). Since fv(𝑏) ⊆ PVar,
(𝜎𝑝 ⊕ 𝜎𝑟 ) =
(𝜎𝑝 ⊕𝜎𝑟 ) for all 𝜎𝑟 ∈ St[Var\PVar].

(𝜎𝑝 ⊕𝜎𝑟 ) and

(𝜎𝑝 ⊕𝜎𝑟 ) =

(𝜎𝑝 ⊕𝜎𝑟 ) =

} else {𝑐 ′′

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑏
(cid:74)

𝑐
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

𝜋

𝜋

𝜋

𝜋

(cid:75)

(cid:75)

(cid:75)

𝜋 ′

and

𝑐
(cid:74)

𝑐 ′
(cid:74)

𝑐𝜋
(cid:74)

)(𝜎) else 𝜎.

(𝑓 )(𝜎) ≜ if (

𝜎 = true) then (𝑓 † ◦
(cid:75)

Case 𝑐 ≡ (while 𝑏 {𝑐 ′}). In this case, 𝑐𝜋 ≡ (while 𝑏 {𝑐 ′

}). Consider the version of prs□ (−)
where the parameter can be a state transformer 𝑓 : St → St⊥, instead of a command. Similarly,
consider the version of the three conclusions where we use two state transformers 𝑓 , 𝑓 : St → St⊥,
again instead of a command. We denote the versions by prs□ (𝑓 ) and 𝜑 (𝑓 , 𝑓 , 𝜎𝑝, 𝜉𝑛). We write 𝑓 ∼ 𝑓
if prs□ (𝑓 )(𝜎𝑝, 𝜉𝑛) > 0 implies 𝜑 (𝑓 , 𝑓 , 𝜎𝑝, 𝜉𝑛) for all 𝜎𝑝 ∈ St[PVar] and 𝜉𝑛 ∈ St□ [Name]. Further, we
define 𝐹 𝜋 ′ : [St → St⊥] → [St → St⊥] as
𝐹 𝜋 ′

𝑏
(cid:74)
Note that 𝐹 𝜋 and 𝐹 𝜋0 are the operators used in the semantics of the loops
, respectively,
where 𝜋0 denotes the empty reparameterisation plan. We will show three claims: 𝜆𝜎.⊥ ∼ 𝜆𝜎.⊥; if
𝑓 ∼ 𝑓 , then 𝐹 𝜋 (𝑓 ) ∼ 𝐹 𝜋0 (𝑓 ); and if increasing sequences {𝑓𝑘 }𝑘 ∈N and {𝑓𝑘 }𝑘 ∈N satisfy 𝑓𝑘 ∼ 𝑓𝑘 for all
𝑘 ∈ N, then 𝑓∞ ∼ 𝑓∞ holds for 𝑓∞ = (cid:195)𝑘 ∈N 𝑓𝑘 and 𝑓∞ = (cid:195)𝑘 ∈N 𝑓𝑘 . These three claims imply
,
𝑐
(cid:74)
which in turn proves the desired three conclusions.

as the sequential composition of 𝑐 ′
𝑐 ′
(cid:74)
(cid:75)
and 𝑓 † ◦
(cid:75)

(cid:75)
The first claim holds simply because prs□ (𝜆𝜎.⊥)(−, −) is always 0. To show the second claim,
consider 𝑓 , 𝑓 : St → St⊥ such that 𝑓 ∼ 𝑓 . We first replay our proof for the sequential-composition
𝜋 and 𝑓 , and of
case on (𝑓 , 𝑓 ) after viewing 𝑓 † ◦
𝑐 ′
𝑐 ′
(cid:74)
(cid:74)
𝑐 ′ and 𝑓 , respectively. This replay, then, gives the relationship 𝑓 † ◦
. Next, we replay
𝑐 ′
𝜋
(cid:74)
our proof for the if case on (𝐹 𝜋 (𝑓 ), 𝐹 𝜋0 (𝑓 )), after viewing 𝑓 † ◦
as the true branches,
𝑐 ′
(cid:75)
(cid:74)
as the false branch. This replay implies the required relationship 𝐹 𝜋 (𝑓 ) ∼ 𝐹 𝜋0 (𝑓 ).
and 𝜆𝜎. 𝜎 =
To show the third condition, consider increasing sequences {𝑓𝑘 }𝑘 ∈N and {𝑓𝑘 }𝑘 ∈N such that
𝑓𝑘 ∼ 𝑓𝑘 for all 𝑘 ∈ N. Let 𝑓∞ = (cid:195)𝑘 ∈N 𝑓𝑘 and 𝑓∞ = (cid:195)𝑘 ∈N 𝑓𝑘 . Consider any 𝜎𝑝 and 𝜉𝑛 such that
prs□ (𝑓∞)(𝜎𝑝, 𝜉𝑛) > 0. We should show 𝜑 (𝑓∞, 𝑓∞, 𝜎𝑝, 𝜉𝑛). Pick any 𝜎𝑟 ∈ St[Var \ (PVar ∪ dom(𝜉𝑛))]
with 𝜎𝑟 (like) = 1. Let 𝜎 = 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 and 𝜎 = 𝜎𝑝 ⊕ 𝜉𝑛 ⊕ 𝜎𝑟 . Note that the value of each term in
𝜑 (· · · ) (i.e., used (· · · ), pvars□ (· · · ), and prs ⟨𝑆 ⟩
□ (· · · )) is independent of the choice of 𝜎𝑟 by Lemma C.7
and the well-definedness of pvars□ and prs ⟨𝑆 ⟩
□ . Since the two given sequences are increasing, there
exists 𝐾 ∈ N such that 𝑓∞ (𝜎) = 𝑓𝐾 (𝜎) and 𝑓∞ (𝜎) = 𝑓𝐾 (𝜎). From this and prs□(𝑓∞)(𝜎𝑝, 𝜉𝑛) > 0,
we have prs□(𝑓𝐾 )(𝜎𝑝, 𝜉𝑛) > 0. This in turn gives 𝜑 (𝑓𝐾, 𝑓𝐾, 𝜎𝑝, 𝜉𝑛) since 𝑓𝐾 ∼ 𝑓𝐾 . Lastly, again by
□
𝑓∞(𝜎) = 𝑓𝐾 (𝜎) and 𝑓∞ (𝜎) = 𝑓𝐾 (𝜎), we obtain 𝜑 (𝑓∞, 𝑓∞, 𝜎𝑝, 𝜉𝑛) as desired.

∼ 𝑓 † ◦
𝑐 ′
(cid:74)

skip
(cid:75)

and 𝑓 † ◦

𝑐𝜋
(cid:74)

∼

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:74)

𝜋

𝜋

E DEFERRED RESULTS IN §4.3

E.1 Deferred Statements and Their Proofs

Lemma E.1. Let 𝑓 , 𝑔 : R𝑛 → R be locally Lipschitz functions. Then, the following differentiation rules

hold for almost every 𝑥 ∈ R𝑛:

∇(𝑓 + 𝑔)(𝑥) = ∇𝑓 (𝑥) + ∇𝑔(𝑥),
∇(𝑓 · 𝑔)(𝑥) = ∇𝑓 (𝑥) · 𝑔(𝑥) + 𝑓 (𝑥) · ∇𝑔(𝑥),
∇ log 𝑓 (𝑥) = 1/𝑓 (𝑥) · ∇𝑓 (𝑥),

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:59

where for the third rule we assume 𝑓 (𝑦) > 0 for all 𝑦 ∈ R𝑛.

Proof. Note that the three functions + : R2 → R, · : R2 → R, and log : R>0 → R are all
□

differentiable. Hence, applying Lemma E.4 produces the claim.

Theorem E.2. Let 𝑓 : R × R𝑛 → R be a measurable function that satisfies the next conditions:
• For all 𝑥 ∈ R𝑛, 𝑓 (−, 𝑥) : R → R is locally Lipschitz.
∫
• For all 𝜃 ∈ R,
R𝑛 𝑓 (𝜃, 𝑥) 𝑑𝑥 is well-defined.
• For all 𝜃 ∈ R, ∇𝜃 𝑓 (𝜃, 𝑥) is well-defined for almost all 𝑥 ∈ R.
∫
R𝑛 Lip(cid:0)𝑓 (−, 𝑥)|𝑈 (cid:1)𝑑𝑥 is well-defined.
• For all 𝜃 ∈ R, there is an open 𝑈 ⊆ R such that 𝜃 ∈ 𝑈 and
Here “almost all” is with respect to the Lebesgue measure. Then, for all 𝜃 ∈ R, both sides of the following
are well-defined and equal:

∇𝜃

∫

R𝑛

𝑓 (𝜃, 𝑥) 𝑑𝑥 =

∫

R𝑛

∇𝜃 𝑓 (𝜃, 𝑥) 𝑑𝑥 .

Theorem E.3. Let 𝑐𝑚, 𝑐𝑔, and 𝜋 be the inputs to the SPGE (which satisfy the assumptions (i) and (ii)
described in §4.2). Suppose that L (𝜎𝜃 ) and ∇𝜃 L (𝜎𝜃 ) are well-defined for every 𝜎𝜃 ∈ St[𝜃 ]. Further,
assume that every sample command in 𝑐𝑔 has 𝜆𝑦.𝑦 as its third argument, and 𝑐𝑔 does not have observe
commands. Then, for all 𝜎𝜃 ∈ St[𝜃 ],

if 𝜋 satisfies the requirements (R1), (R2’), (R3’), (R4), and (R5).

∇𝜃 L (𝜎𝜃 ) = E𝑝𝑐𝑔 𝜋 ,𝜎𝜃 ( ˆ𝜎𝑛) [grad_est(𝜎𝜃, ˆ𝜎𝑛)]

Proof. The proof is essentially the same as the proof of Theorem 4.4, except that we invoke the
following properties of local Lipschitzness (instead of differentiability): the composition of locally
Lipschitz functions is again locally Lipschitz, and the differentiation rules for +, ×, and 𝑙𝑜𝑔 hold
□
almost everywhere for locally Lipschitz functions (Lemma E.1).

Lemma E.4. Let 𝑓 : 𝑋1 → 𝑋2 and 𝑔 : 𝑋2 → 𝑋3 for some open sets 𝑋𝑖 ⊆ R𝑛𝑖 . Suppose that 𝑓 is locally
Lipschitz and 𝑔 is differentiable. Then, 𝑔 ◦ 𝑓 : 𝑋1 → 𝑋3 is differentiable almost everywhere and the
chain rule for 𝑔 ◦ 𝑓 holds almost everywhere, i.e.,

𝐷 (𝑔 ◦ 𝑓 )(𝑥) = 𝐷 (𝑔)(𝑓 (𝑥)) · 𝐷 (𝑓 )(𝑥)

for almost every 𝑥 ∈ 𝑋1. Here we use the Lebesgue measure as an underlying measure.

Proof. Since local Lipschitzness is preserved under a function composition, 𝑔 ◦ 𝑓 is locally Lip-
schitz and thus differentiable almost everywhere. Since 𝑓 is also differentiable almost everywhere
and 𝑔 is differentiable everywhere, the set

𝑈 = 𝑋1 \ {𝑥 ∈ 𝑋1 | (𝑔 ◦ 𝑓 is differentiable at 𝑥)

∧ (𝑔 is differentiable at 𝑓 (𝑥))
∧ (𝑓 is differentiable at 𝑥)}

has Lebesgue measure zero. Note that the differentiability of 𝑔 is importantly used here; if 𝑔 is non-
differentiable even at a point, 𝑈 can have positive measure. The chain rule for 𝑔 ◦ 𝑓 holds for each
□
𝑥 ∈ 𝑈 and this concludes the proof.

F DEFERRED RESULTS IN §5.2

F.1 Proof of Theorem 5.6

Proof of Theorem 5.6. We prove the theorem by induction on the structure of 𝑐. Let (𝑝, 𝑑, 𝑉 ) ≜
♯, and pick 𝑣 ∈ Var. We have to show that 𝑝 (𝑣) ⊇ 𝑑 (𝑣)𝑐 and 𝑑 (𝑣) ⊇ 𝑉 . We call these two

𝑐
(cid:74)
requirements as conditions (i) and (ii).

(cid:75)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:60

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Case 𝑐 ≡ skip. In this case, 𝑝 (𝑣) = Var and 𝑉 = ∅, from which the conditions (i) and (ii) follow.

Case 𝑐 ≡ 𝑥 := 𝑒. In this case, 𝑉 = ∅. So, the condition (ii) holds. For the proof of the condition (i),
♯ and 𝑑 (𝑣) = fv(𝑒). Since
♯ ⊇ fv(𝑒)𝑐 , the condition holds. If 𝑣 is different from 𝑥, then 𝑝 (𝑣) is Var, and so it includes 𝑑 (𝑣)𝑐 .

we do case analysis on 𝑣. If 𝑣 is the updated variable 𝑥, we have 𝑝 (𝑣) =
𝑒
(cid:76)

(cid:77)
Case 𝑐 ≡ obs(distN (𝑒1, 𝑒2), 𝑟 ). The proof of this case is similar to the one for the assignments.

𝑒
(cid:76)

(cid:77)

Since 𝑉 = ∅, the condition (ii) holds. If 𝑣 is the variable like, then

𝑝 (𝑣) =

like × pdfN(𝑟 ; 𝑒1, 𝑒2)
(cid:76)

(cid:77)

♯ ⊇ fv(like × pdfN(𝑟 ; 𝑒1, 𝑒2))𝑐 =

(cid:16)

{like} ∪ fv(𝑒1) ∪ fv(𝑒2)

(cid:17)𝑐

= 𝑑 (𝑣)𝑐 .

So, the condition (i) holds in this case. If 𝑣 is not the variable like, then 𝑝 (𝑣) = Var, from which the
condition (i) follows.

Case 𝑐 ≡ 𝑥 := sam(name(𝛼, 𝑒), distN (𝑒1, 𝑒2), 𝜆𝑦.𝑒 ′). In this case, 𝑉 = ∅, from which the condition
(ii) follows. We do case analysis on whether 𝑒 is a real constant 𝑟 or not. During the case analysis,
we use the assumption that fv(𝑒)𝑐 ⊆

♯ for all 𝑒, without mentioning it explicitly.

First, we deal with the case that𝑒 ≡ 𝑟 . Let 𝜇 ≜ create_name(𝛼, 𝑟 ). If 𝑣 is none of 𝑥, val𝜇, pr 𝜇, and cnt𝜇,
we have 𝑝 (𝑣) = Var, which gives the condition (i). If 𝑣 ∈ {𝑥, val𝜇 }, we prove the condition (i) as follows:

𝑒
(cid:76)

(cid:77)

𝑑 (𝑣)𝑐 = fv(𝑒 ′[𝜇/𝑦])𝑐 ⊆

♯

𝑒 ′[𝜇/𝑦]
(cid:76)

(cid:77)

= 𝑝 (𝑣).

If 𝑣 ≡ pr 𝜇, we calculate the condition (i) as follows:

𝑑 (pr 𝜇)𝑐 = ({𝜇} ∪ fv(𝑒1) ∪ fv(𝑒2))𝑐 = fv(pdfN (𝜇; 𝑒1, 𝑒2))𝑐 ⊆

pdfN (𝜇; 𝑒1, 𝑒2)
(cid:76)

(cid:77)

♯

= 𝑝 (pr 𝜇).

If 𝑣 ≡ cnt𝜇, we derive the condition (i) as follows:

𝑑 (cnt𝜇)𝑐 = {cnt𝜇 }𝑐 = fv(cnt𝜇 + 1)𝑐 ⊆

♯

cnt𝜇 + 1
(cid:76)

(cid:77)

= 𝑝 (cnt𝜇).

Next, we handle the case that 𝑒 is not a real constant. If 𝑣 is none of 𝑥, val𝜇, pr 𝜇, and cnt𝜇 for some
𝜇 = (𝛼, _), we have 𝑝 (𝑣) = Var, which implies the condition (i). If 𝑣 ≡ 𝑥, we show the condition (i)
as follows:

𝑝 (𝑣) =

(cid:16)

fv(𝑒)𝑐 ∩

(cid:217)
𝜇=(𝛼,_) ∈Name (cid:76)

𝑒 ′[𝜇/𝑦]

(cid:77)

♯(cid:17)

(cid:16)

⊇

fv(𝑒) ∪

(cid:216)

fv(𝑒 ′[𝜇/𝑦])

(cid:17)𝑐

= 𝑑 (𝑣)𝑐 .

𝜇=(𝛼,_) ∈Name

If 𝑣 ≡ val𝜇 for some 𝜇 = (𝛼, _), we calculate the condition (i) as follows:

𝑝 (𝑣) =

(cid:16)

fv(𝑒)𝑐 ∩

♯(cid:17)

(cid:16)

⊇

𝑒 ′[𝜇/𝑦]
(cid:76)

(cid:77)

fv(𝑒) ∪ {val𝜇 } ∪ fv(𝑒 ′[𝜇/𝑦])

(cid:17)𝑐

= 𝑑 (𝑣)𝑐 .

If 𝑣 ≡ pr 𝜇 for some 𝜇 = (𝛼, _), we prove the condition (i) as follows:

𝑝 (𝑣) = fv(𝑒)𝑐 ∩

pdfN(𝜇; 𝑒1, 𝑒2)

(cid:76)

(cid:77)

♯ ⊇ fv(𝑒)𝑐 ∩ fv(pdfN (𝜇; 𝑒1, 𝑒2))𝑐

(cid:16)

⊇

fv(𝑒) ∪ {𝜇, pr 𝜇 } ∪ fv(𝑒1) ∪ fv(𝑒2)

(cid:17)𝑐

= 𝑝 (pr 𝜇).

Finally, if 𝑣 ≡ cnt𝜇 for some 𝜇 = (𝛼, _), we show the condition (i) as follows:

𝑝 (𝑣) = fv(𝑒)𝑐 ∩

cnt𝜇 + 1
(cid:77)
(cid:76)

♯ ⊇ fv(𝑒)𝑐 ∩ fv(cnt𝜇 + 1)𝑐 = (fv(𝑒) ∪ {cnt𝜇 })𝑐 = 𝑑 (𝑣)𝑐 .

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:61

Case 𝑐 ≡ 𝑐 ′; 𝑐 ′′. Let (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

𝑐 ′
(cid:74)

(cid:75)

♯ and (𝑝 ′′, 𝑑 ′′, 𝑉 ′′) ≜
∪(𝑑 ′′(𝑣)) ⊇ 𝑉 ′ ∪ 𝑑 ′

𝑐 ′′
(cid:74)

(cid:75)
∪(𝑉 ′′) = 𝑉 .

𝑑 (𝑣) = 𝑉 ′ ∪ 𝑑 ′

♯. The condition (ii) holds since

For the condition (ii), we prove the required subset relationship as follows:

𝑑 (𝑣)𝑐 = (𝑉 ′ ∪ 𝑑 ′

∪(𝑑 ′′(𝑣)))𝑐 = (𝑉 ′)𝑐 ∩

(cid:217)

𝑑 ′(𝑤)𝑐

𝑤 ∈𝑑′′ (𝑣)
(cid:217)

⊆ (𝑉 ′)𝑐 ∩

𝑝 ′(𝑤) ∩

(cid:217)

𝑑 ′(𝑤)𝑐

(cid:16)
𝑉 ′ ∪ 𝑝 ′

𝑤 ∈𝑑′′ (𝑣)
∩(𝑑 ′′(𝑣))𝑐 ∪ 𝑑 ′

𝑤 ∈𝑝′′ (𝑣)𝑐
(cid:17)𝑐
∪(𝑝 ′′(𝑣)𝑐 )

=

= 𝑝 (𝑣).

The subset relationship in the above derivation holds because 𝑑 ′(𝑤)𝑐 ⊆ 𝑝 ′(𝑤) and 𝑑 ′′(𝑣) ⊇ 𝑝 ′′(𝑣)𝑐
by induction hypothesis.

Case 𝑐 ≡ if 𝑏 {𝑐 ′} else {𝑐 ′′}. Let (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

tion hypothesis,

♯ and (𝑝 ′′, 𝑑 ′′, 𝑉 ′′) ≜

𝑐 ′
(cid:74)

(cid:75)

𝑐 ′′
(cid:74)

(cid:75)

♯. Then, by induc-

(cid:16)

𝑉 =

fv(𝑏) ∪ 𝑉 ′ ∪ 𝑉 ′′(cid:17)
which implies the condition (ii). Also, by induction hypothesis again,
fv(𝑏)𝑐 ∩ 𝑑 ′(𝑣)𝑐 ∩ 𝑑 ′′(𝑣)𝑐 (cid:17)

fv(𝑏) ∪ 𝑑 ′(𝑣) ∪ 𝑑 ′′(𝑣)

𝑑 (𝑣)𝑐 =

⊆

⊆

(cid:16)

(cid:16)

(cid:16)

fv(𝑏)𝑐 ∩ 𝑝 ′(𝑣) ∩ 𝑝 ′′(𝑣)

(cid:17)

= 𝑑 (𝑣),

(cid:17)

= 𝑝 (𝑣),

which shows the condition (ii).

(cid:75)

𝑐 ′
(cid:74)

Case 𝑐 ≡ while𝑏 {𝑐 ′}. Let (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

♯, and 𝐹 ♯ be the operator in the abstract semantics of 𝑐.
Note that the abstract domain D♯ contains (𝑝⊥, 𝑑⊥, 𝑉⊥) = ((𝜆𝑣.Var), (𝜆𝑣.∅), ∅). Thus, it is sufficient
to show that 𝐹 ♯ is a well-defined monotone function on D♯, because then the least fixed point of
𝐹 ♯ is also in D♯ and satisfies the conditions (i) and (ii). The monotonicity of 𝐹 ♯ holds because when
(𝑝1, 𝑑1, 𝑉1) ≜ 𝐹 ♯ (𝑝0, 𝑑0, 𝑉0), the inputs 𝑝0, 𝑑0, and 𝑉0 are used in the right polarity in the definitions
of 𝑝1, 𝑑1, and 𝑉1; for instance, 𝑝0 is used only in the positive position (with respect to the subset
order) when it is used to define 𝑝1. To prove well-definedness of 𝐹 ♯, assume that 𝑝0(𝑣0) ⊇ 𝑑0 (𝑣0)𝑐
and 𝑑0(𝑣0) ⊇ 𝑉0 for all 𝑣0 ∈ Var, and pick a variable 𝑣1 ∈ Var. Then, since 𝑉0 ⊆ 𝑑0(𝑣1),
(cid:16)

(cid:17)

(cid:16)

𝑉1 =

fv(𝑏) ∪ 𝑑 ′

∪ (𝑉0) ∪ 𝑉 ′(cid:17)

⊆

fv(𝑏) ∪ 𝑑 ′

∪ (𝑑0(𝑣1)) ∪ 𝑉 ′ ∪ {𝑣1}

= 𝑑1(𝑣1).

Also, by the induction hypothesis on the loop body 𝑐 ′ and the relationship 𝑑0(𝑣1) ⊇ 𝑝0(𝑣1)𝑐 ,

𝑑1 (𝑣1)𝑐 = fv(𝑏)𝑐 ∩ (𝑉 ′)𝑐 ∩

(cid:217)

𝑑 ′(𝑤)𝑐 ∩ {𝑣1}𝑐

⊆ fv(𝑏)𝑐 ∩ (𝑉 ′)𝑐 ∩

𝑤 ∈𝑑0 (𝑣1)
(cid:217)

𝑝 ′(𝑤) ∩

(cid:217)

𝑑 ′(𝑤)𝑐

= fv(𝑏)𝑐 ∩

Thus, (𝑝1, 𝑑1, 𝑉1) is also in D♯.

(cid:16)
𝑉 ′ ∪ 𝑝 ′

𝑤 ∈𝑑0 (𝑣1)
∩ (𝑑0(𝑣1))𝑐 ∪ 𝑑 ′

𝑤 ∈𝑝0 (𝑣1)𝑐
(cid:17)𝑐
∪(𝑝0(𝑣1)𝑐 )

= 𝑝1(𝑣1).

□

F.2 Proof of Theorem 5.8
Our program analysis consists of two parts, one for tracking the dependency information and the
other for tracking the smoothness information. The first part does not depend on the second, although
it is used crucially by the second part. We exploit this one-way relationship between the two parts

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:62

Wonyeol Lee, Xavier Rival, and Hongseok Yang

of our analysis, and prove the soundness of the dependency-tracking part first and then that of the
other smoothness-tracking part. Consider a command 𝑐, and let (𝑝, 𝑑, 𝑉 ) ≜

♯. Then, we have:

Theorem F.1. For all 𝑣 ∈ Var, we have |= Δ(
Theorem F.2. For all 𝑣 ∈ Var, we have |= Φ(

, 𝑑 (𝑣), {𝑣 }). Also,
(cid:75)
, 𝑝 (𝑣), {𝑣 }).
(cid:75)
We prove the two soundness results in §F.3 and §F.4. From these, we immediately obtain the main
soundness theorem:

𝑐
(cid:74)
𝑐
(cid:74)

|= Δ(

𝑐
(cid:74)

𝑐
(cid:75)
(cid:74)
, 𝑉 , ∅).
(cid:75)

Proof of Theorem 5.8. Let 𝑐 be a command and (𝑝, 𝑑, 𝑉 ) =
|= Δ(

we have |= Δ(
, 𝑑 (𝑣), {𝑣 }),
𝑐
(cid:74)
(cid:75)
definition of 𝛾 (i.e., Eq. (11)), we have

𝑐
♯) as desired.
(cid:74)

∈ 𝛾 (

𝑐
(cid:74)

𝑐
(cid:74)

♯. Then, by Theorems F.1 and F.2,
, 𝑝 (𝑣), {𝑣 }) for all 𝑣 ∈ Var. Hence, by the
(cid:75)
□

(cid:75)

, 𝑉 , ∅), and |= Φ(
(cid:75)
𝑐
𝑐
(cid:74)
(cid:74)

(cid:75)

(cid:75)

F.3 Proof of Theorem F.1

Proof of Theorem F.1. We prove the theorem by induction on the structure of 𝑐. Let (𝑝, 𝑑, 𝑉 ) ≜
♯. Pick a variable 𝑣 ∈ Var and states 𝜎, 𝜎 ′, 𝜎0, 𝜎 ′

0 ∈ St such that

𝑐
(cid:74)

(cid:75)

𝜎 ∼𝑑 (𝑣) 𝜎 ′ and 𝜎0 ∼𝑉 𝜎 ′
0.

We will show that (i) if
𝑐
𝑐
(cid:74)
(cid:74)
We refer to these two properties as conditions (i) and (ii) in the rest of the proof.

𝜎 ′ ∈ St, then
𝑐
(cid:75)
(cid:74)
𝜎 ′(𝑣). Since 𝑉 ⊆ 𝑑 (𝑣), these two imply the claim of the theorem.
(cid:75)

𝜎0 ∈ St, then
𝑐
(cid:74)
(cid:75)
𝜎 (𝑣) =
(cid:75)

0 ∈ St, and (ii) if
𝜎 ′
(cid:75)

𝜎 ∈ St and
(cid:75)

𝜎 ′, i.e.,
(cid:75)

𝜎 ∼{𝑣 }
(cid:75)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

Case 𝑐 ≡ skip. In this case, 𝑑 (𝑣) = {𝑣 } and 𝑉 = ∅. The condition (i) holds since skip always ter-
𝜎 ′′ = 𝜎 ′′ for all 𝜎 ′′, and the relation ∼𝑑 (𝑣) coincides
(cid:75)

minates. The condition (ii) also holds because
with ∼{𝑣 }.

𝑐
(cid:74)

Case 𝑐 ≡ (𝑥 := 𝑒). In this case, 𝑉 = ∅, and the condition (i) holds since the assignments always

terminate. For the condition (ii), we do case analysis on the variable 𝑣.
𝑒
(cid:74)

𝑐
(cid:74)
• Case 𝑣 (cid:46) 𝑥. In this case, 𝑑 (𝑣) = {𝑣 }, and so 𝜎 (𝑣) = 𝜎 ′(𝑣). This implies that

• Case 𝑣 ≡ 𝑥. In this case, 𝑑 (𝑣) = fv(𝑒). This implies
𝑒
(cid:74)
𝜎 ′(𝑥). This implies the desired
𝑐
𝜎 ∼{𝑥 }
(cid:75)
(cid:74)
(cid:75)
𝜎 ′(𝑣), which gives the desired relationship.
(cid:75)

𝜎 ′ =
𝑒
(cid:74)
(cid:75)
𝜎 ′(𝑣) =

𝜎 ′. Thus,
(cid:75)

𝜎 =
(cid:75)
𝜎 ′.
(cid:75)

𝑐
(cid:74)
𝑐
(cid:74)

𝑐
(cid:74)

𝜎 =
(cid:75)

𝜎 (𝑥) =
(cid:75)
𝑐
(cid:74)

𝑒
(cid:74)
𝜎 (𝑣) = 𝜎 (𝑣) =
(cid:75)

Case 𝑐 ≡ obs(distN (𝑒1, 𝑒2), 𝑟 ). The observe commands always terminate. Thus, the condition (i)
holds. We prove the condition (ii) by case analysis on the variable 𝑣. If 𝑣 is not like, then 𝑑 (𝑣) = {𝑣 },
𝜎 (𝑣) = 𝜎 (𝑣), and
𝜎 (𝑣) =
𝑐
𝜎 ′(𝑣), as desired. If 𝑣 is like, then 𝑑 (𝑣) = fv(𝑒1) ∪ fv(𝑒2) ∪ {like}, and for some function 𝑔 : R4 → R,
(cid:75)
(cid:74)
(cid:75)
𝑐
(cid:75)
(cid:74)
𝑐
(cid:74)

𝜎 ′(𝑣) = 𝜎 ′(𝑣). Thus, in this case, the assumption 𝜎 ∼𝑑 (𝑣) 𝜎 ′ implies
(cid:75)

𝜎 (𝑣) = 𝑔(𝜎 (like), 𝑟,
(cid:75)

𝜎) and
(cid:75)

𝜎,
(cid:75)

𝑒1
(cid:74)

𝑒2
(cid:74)

𝑒2
(cid:74)

𝑒1
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

Therefore, from the assumption 𝜎 ∼𝑑 (𝑣) 𝜎 ′, it follows that

𝜎 ′(𝑣) = 𝑔(𝜎 ′(like), 𝑟,
(cid:75)
𝜎 (𝑣) =
(cid:75)

𝜎 ′).
(cid:75)
𝜎 ′(𝑣), as desired.
(cid:75)

𝜎 ′,
(cid:75)

𝑐
(cid:74)

𝑐
(cid:74)

Case 𝑐 ≡ (𝑥 := sam(𝑛, distN (𝑒1, 𝑒2), 𝜆𝑦.𝑒 ′)). The sample commands always terminate. So, the

condition (i) holds. We prove the condition (ii) by case analysis on 𝑛.

The first case is that𝑛 is a constant expression, i.e., it is an expression of the form name(𝛼, 𝑟 ) for some
𝛼 ∈ Str and real number𝑟 . Let 𝜇 ≜ create_name(𝛼, 𝑟 ). If𝑣 is not one of𝑥, val𝜇, and pr 𝜇, then𝑑 (𝑣) = {𝑣 },
𝜎 (𝑣) = 𝑔(𝜎 (𝑣)) and
and
𝜎 ′(𝑣) = 𝑔(𝜎 ′(𝑣)) for some function 𝑔 : R → R, so that the assumption
𝑐
𝑐
(cid:74)
(cid:75)
(cid:74)
(cid:75)
𝜎 ′(𝑣), as desired. If 𝑣 is 𝑥 or val𝜇, then 𝑑 (𝑣) = fv(𝑒 ′[𝜇/𝑦]),
𝜎 ∼𝑑 (𝑣) 𝜎 ′ implies that
𝑐
𝜎 (𝑣) =
𝑐
(cid:75)
(cid:75)
(cid:74)
(cid:74)
𝜎 ′(𝑣) holds.
𝜎 ′, so that the required
𝜎, and
𝑒 ′[𝜇/𝑦]
𝜎 ′(𝑣) =
𝑐
𝜎 (𝑣) =
𝑐
(cid:74)
(cid:75)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:75)
(cid:75)
Finally, if 𝑣 = pr 𝜇, then 𝑑 (𝑣) = {𝜇} ∪ fv(𝑒1) ∪ fv(𝑒2), and so, the assumption 𝜎 ∼𝑑 (𝑣) 𝜎 ′ implies that

𝑒 ′[𝜇/𝑦]
(cid:74)

𝜎 (𝑣) =
(cid:75)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝜎 (pr 𝜇) =
(cid:75)

(cid:74)

pdfN (𝜇; 𝑒1, 𝑒2)

𝜎 =
(cid:75)

pdfN (𝜇; 𝑒1, 𝑒2)
(cid:74)

𝜎 ′ =
(cid:75)

𝑐
(cid:74)

𝜎 ′(pr 𝜇),
(cid:75)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:63

which is precisely the equality that we want.

The next case is that 𝑛 is not a constant expression. Let name(𝛼, 𝑒) be the form of 𝑛. If 𝑣 is not
𝜎 (𝑣) = 𝜎 (𝑣), and
one of 𝑥, val𝜇, pr 𝜇, and cnt𝜇 for some 𝜇 of the form (𝛼, _), then 𝑑 (𝑣) = {𝑣 },
𝑐
(cid:75)
(cid:74)
𝜎 ′(𝑣). Assume
𝜎 ′(𝑣) = 𝜎 ′(𝑣), so that the assumption 𝜎 ∼𝑑 (𝑣) 𝜎 ′ implies the desired
𝑐
𝜎 (𝑣) =
𝑐
(cid:75)
(cid:74)
(cid:74)
(cid:75)
(cid:75)
0 ≜
that 𝑣 is one of 𝑥, val𝜇, pr 𝜇, and cnt𝜇 for some 𝜇 with 𝜇 = (𝛼, _). Let 𝜇0 ≜
𝜎 ′.
𝜎 and 𝜇 ′
𝑛
(cid:75)
(cid:75)
(cid:74)
0. If 𝑣 is 𝑥, then
Since 𝑑 (𝑣) ⊇ fv(𝑛) in this case, the assumption 𝜎 ∼𝑑 (𝑣) 𝜎 ′ ensures that 𝜇0 = 𝜇 ′
fv(𝑒 ′[𝜇0/𝑦]) ⊆ 𝑑 (𝑣), so that the assumption 𝜎 ∼𝑑 (𝑣) 𝜎 ′ gives the desired

𝑛
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

0/𝑦]

𝜎 =
(cid:75)

𝑒 ′[𝜇 ′
(cid:74)

𝜎 (𝑣) =
(cid:75)

𝑒 ′[𝜇0/𝑦]
(cid:74)

𝜎 ′(𝑣).
𝑐
(cid:75)
(cid:74)
𝜎 (cnt𝜇) = 𝑔(𝜎 (cnt𝜇)) and
If 𝑣 is cnt𝜇 for some 𝜇 of the form (𝛼, _), then cnt𝜇 ∈ 𝑑 (𝑣), and
𝑐
(cid:74)
(cid:75)
𝜎 ′(𝑣), as desired. If 𝑣 is
𝜎 ′(cnt𝜇) = 𝑔(𝜎 ′(cnt𝜇)) for some function 𝑔 : R → R, so that
𝑐
𝜎 (𝑣) =
𝑐
(cid:74)
(cid:75)
(cid:75)
(cid:75)
(cid:74)
𝜎, 𝜎 (val𝜇)) and
val𝜇 for 𝜇 = (𝛼, _), then 𝑑 (𝑣) ⊇ {val𝜇 } ∪ fv(𝑒 ′[𝜇/𝑦]), and
𝜎 (val𝜇) = ℎ(
𝑒 ′[𝜇/𝑦]
(cid:74)
(cid:75)
(cid:75)
𝜎 ′(val𝜇) as
𝜎 ′, 𝜎 ′(val𝜇)) for some ℎ : R × R → R, so that
𝜎 (val𝜇) =
𝑐
𝜎 ′(val𝜇) = ℎ(
𝑐
(cid:74)
(cid:75)
(cid:75)
(cid:74)
(cid:75)
(cid:75)
desired. Finally, if 𝑣 is pr 𝜇 for some 𝜇 of the form (𝛼, _), then 𝑑 (𝑣) ⊇ {pr 𝜇, 𝜇} ∪ fv(𝑒1) ∪ fv(𝑒2), and
for some 𝑘 : R4 → R,

𝑒 ′[𝜇/𝑦]
(cid:74)

𝜎 ′ =
(cid:75)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝜎 (𝑣) = 𝑘 (𝜎 (pr 𝜇), 𝜎 (𝜇),
(cid:75)

𝑐
(cid:74)
so that the assumption 𝜎 ∼𝑑 (𝑣) 𝜎 ′ guarantees that

𝜎) and
(cid:75)

𝜎,
(cid:75)

𝑒1
(cid:74)

𝑒2
(cid:74)

Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). Let (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

𝑐 ′
(cid:74)
𝑑 (𝑣) = 𝑉 ′ ∪ (𝑑 ′)∪ (𝑑 ′′(𝑣)) = 𝑉 ′ ∪

𝑒1
(cid:74)

𝜎 ′,
(cid:75)

𝑒2
(cid:74)

𝜎 ′),
(cid:75)

𝜎 ′(𝑣) = 𝑘 (𝜎 ′(pr 𝜇), 𝜎 ′(𝜇),
(cid:75)
𝑐
(cid:74)

𝜎 ′(𝑣), as desired.
𝑐
𝜎 (𝑣) =
(cid:75)
(cid:74)
(cid:75)
♯ and (𝑝 ′′, 𝑑 ′′, 𝑉 ′′) ≜

♯. Recall that

𝑐 ′′
(cid:74)

(cid:75)
(cid:216)

(cid:75)
{𝑑 ′(𝑣 ′′) | 𝑣 ′′ ∈ 𝑑 ′′(𝑣)} and 𝑉 = 𝑉 ′ ∪ (𝑑 ′)∪ (𝑉 ′′).

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝜎0 ∼𝑉 ′′
(cid:75)
𝜎 and
(cid:75)

𝑐 ′; 𝑐 ′′
(cid:74)
𝑐 ′
(cid:74)

Let us handle the condition (i) first. Since

0 are ∼𝑉 -related and 𝑉 includes 𝑉 ′. Thus,

𝜎0 ∈ St, we have
(cid:75)
𝑐 ′
(cid:74)

𝜎0 ∈ St. But 𝜎0 ∼𝑉 ′ 𝜎 ′
0,
(cid:75)
because 𝜎0 and 𝜎 ′
0 ∈ St as well by induction hypoth-
𝜎 ′
(cid:75)
esis, and it is sufficient to show
0. Note that for every 𝑣 ′′ ∈ 𝑉 ′′, by the definition
𝜎0 ∼𝑉 ′′
𝜎 ′
(cid:75)
(cid:75)
0, which implies, by induction hypothesis, that
of 𝑉 , we have 𝑉 ⊇ 𝑑 ′(𝑣 ′′), and so 𝜎0 ∼𝑑′ (𝑣′′) 𝜎 ′
0. As a result, we have the desired
0.
𝜎0 ∼{𝑣′′ }
𝜎 ′
𝑐 ′
𝑐 ′
𝜎 ′
𝑐 ′
(cid:75)
(cid:74)
(cid:75)
(cid:74)
(cid:74)
(cid:75)
Next, we deal with the condition (ii). Since
𝜎 ′ are both in St, there exist
𝑐 ′; 𝑐 ′′
𝑐 ′; 𝑐 ′′
(cid:74)
(cid:75)
(cid:74)
𝜎 = 𝜎1 and
1 such that
states 𝜎1, 𝜎 ′
1. We apply the induction hypothesis to 𝑐 ′ and get
𝜎 ′ = 𝜎 ′
𝑐 ′
𝑐 ′
(cid:74)
(cid:75)
(cid:74)
(cid:75)
1 are in St, we apply the induction hypothesis again but this
𝜎1 and
1. Since
𝑐 ′′
𝑐 ′′
𝜎1 ∼𝑑′′ (𝑣) 𝜎 ′
𝜎 ′
(cid:74)
(cid:74)
(cid:75)
(cid:75)
1, and obtain
time to 𝑐 ′′, 𝜎1, and 𝜎 ′
𝜎1 ∼{𝑣 }
𝑐 ′′
(cid:74)
(cid:75)
𝜎 (𝑣) =
(cid:75)

1, which implies the desired
𝜎 ′
𝑐 ′′
(cid:74)
(cid:75)
𝜎1(𝑣) =
(cid:75)

𝑐 ′′
𝑐
(cid:74)
(cid:74)
Case 𝑐 ≡ (if 𝑏 {𝑐 ′} else {𝑐 ′′}). Let (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

fv(𝑏) ∪ 𝑑 ′(𝑣) ∪ 𝑑 ′′(𝑣) and 𝑉 = fv(𝑏) ∪ 𝑉 ′ ∪ 𝑉 ′′.

applies to the other case that
Furthermore, since 𝑉 ′ ⊆ 𝑉 and so 𝜎0 ∼𝑉 ′ 𝜎 ′

(cid:75)
We prove the condition (𝑖) under the assumption that

𝑐 ′′
(cid:74)
𝜎0 = true. Essentially the same proof
𝑏
(cid:74)
(cid:75)
0 = true.
𝜎0 = false. Since 𝑉 includes fv(𝑏), we also have
𝑏
𝜎 ′
𝑏
(cid:75)
(cid:74)
(cid:75)
(cid:74)
0 ∈ St.
0 by the induction hypothesis, we get that
𝜎 ′
𝑐 ′
(cid:75)
(cid:74)
Next we show the condition (𝑖𝑖) under the assumption that
𝑏
(cid:74)
𝑐
(cid:74)

𝜎 = true. As before, the proof of
(cid:75)
𝜎 = false is essentially the same. Since 𝑑 (𝑣) includes fv(𝑏) and 𝑑 ′(𝑣), we have
(cid:75)
𝜎 and
𝑐 ′
𝜎 ′
𝜎 ′ =
(cid:75)
(cid:75)
(cid:75)
(cid:74)
𝜎 ′, which implies that
𝑐 ′
(cid:75)
(cid:74)
𝜎 ′(𝑣) =
𝑐 ′
(cid:74)
(cid:75)

𝜎 ′ = true and 𝜎 ∼𝑑′ (𝑣) 𝜎 ′. Also, because
(cid:75)
𝑐 ′
(cid:74)
𝑐 ′
(cid:74)

the other case
𝑏
(cid:74)
are in St. Thus, by induction hypothesis,

𝜎 ′
1(𝑣) =
(cid:75)
♯ and (𝑝 ′′, 𝑑 ′′, 𝑉 ′′) ≜
𝑐 ′
(cid:74)

𝜎 =
𝑐
(cid:75)
(cid:74)
𝜎 ∼{𝑣 }
(cid:75)
𝜎 (𝑣) =
(cid:75)

♯. Then, 𝑑 (𝑣) =

𝜎 ′, both
(cid:75)

𝜎 (𝑣) =
(cid:75)

𝜎 ′(𝑣).
(cid:75)

𝜎 ′(𝑣),
(cid:75)

𝜎 and
(cid:75)

𝑐 ′′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑏
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

𝑐
(cid:74)

(cid:75)

as desired.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:64

Wonyeol Lee, Xavier Rival, and Hongseok Yang

𝑐
(cid:74)

(cid:75)

Case 𝑐 ≡ (while 𝑏 {𝑐0}). Let (𝑑0, 𝑝0, 𝑉0) ≜

♯, and 𝐹 ♯ be the operator in the abstract semantics
(cid:75)
of 𝑐 such that (𝑝, 𝑑, 𝑉 ) is the least fixed point of 𝐹 ♯. Also, let 𝐹 be the operator in the concrete semantics
of 𝑐 such that

is the least fixed point of 𝐹 . Now define

𝑐0
(cid:74)

𝑇 ≜ {𝑓 ∈ [St → St⊥] | for all 𝑣 ∈ Var,
≜ 𝜆𝜎. undefined, (ii) it is closed under
We will show that (i) 𝑇 contains the empty function ⊥St→St⊥
the least upper bounds of increasing chains, and (iii) the function 𝐹 maps functions in 𝑇 to some
functions in the same set. These three imply that the least fixed point of 𝐹 , namely,
, is in 𝑇 , which
𝑐
(cid:75)
(cid:74)
gives the desired conclusion.
|= Δ(⊥St→St⊥
, 𝑈 , 𝑈 ′) for all

|= Δ(𝑓 , 𝑑 (𝑣), {𝑣 }) and |= Δ(𝑓 , 𝑉 , ∅)}.

The membership of ⊥St→St⊥

to 𝑇 is immediate, since we have

𝑈 , 𝑈 ′ ⊆ Var.

0, 𝜎, and 𝜎 ′ such that

To prove the next requirement, namely, the closure under the least upper bounds of increasing
chains, consider a chain (𝑓𝑛)𝑛 ∈N in 𝑇 , i.e., a sequence such that 𝑓𝑛 (𝜎) = 𝑓𝑛+1(𝜎) for all 𝑛 ∈ N and
𝜎 with 𝑓𝑛 (𝜎) ∈ St. Let 𝑓∞ be the least upper bound of the 𝑓𝑛’s (i.e., 𝑓∞ (𝜎) = 𝑓𝑛 (𝜎) if 𝑓𝑛 (𝜎) ∈ St and
𝑓∞(𝜎) = ⊥ if 𝑓𝑛 (𝜎) = ⊥ for all 𝑛 ∈ N). As in all the other cases so far, we pick an arbitrary variable
𝑣 ∈ Var and arbitrary states 𝜎0, 𝜎 ′
𝜎0 ∼𝑉 𝜎 ′
0,
0) ∈ St and 𝑓∞ (𝜎) ∼{𝑣 } 𝑓∞ (𝜎 ′), which correspond to what we have called
We will show that 𝑓∞ (𝜎 ′
conditions (i) and (ii) in the previous cases. Since 𝑓∞ (𝜎0) ∈ St, there exists 𝑛 ∈ N such that 𝑓𝑛 (𝜎0) ∈ St.
Because |= Δ(𝑓𝑛, 𝑉 , ∅) and 𝜎0 ∼𝑉 𝜎 ′
0) ∈ St,
as desired. Our proof of the condition (ii) has a similar form. Since both 𝑓∞(𝜎) and 𝑓∞(𝜎 ′) are in St,
there exists 𝑛 ∈ N such that 𝑓∞ (𝜎) = 𝑓𝑛 (𝜎) and 𝑓∞ (𝜎 ′) = 𝑓𝑛 (𝜎 ′). By assumption, 𝜎 ∼𝑑 (𝑣) 𝜎 ′, and
𝑓𝑛 ∈ 𝑇 . Thus, 𝑓𝑛 (𝜎) ∼{𝑣 } 𝑓𝑛 (𝜎 ′), which gives the desired 𝑓∞ (𝜎) ∼{𝑣 } 𝑓∞ (𝜎 ′).

0) ∈ St, which implies that 𝑓∞ (𝜎 ′

𝑓∞ (𝜎), 𝑓∞ (𝜎 ′) ∈ St.

0, we have 𝑓𝑛 (𝜎 ′

𝑓∞ (𝜎0) ∈ St,

𝜎 ∼𝑑 (𝑣) 𝜎 ′,

0) = 𝑓𝑛 (𝜎 ′

It remains to show the last requirement, i.e., the closure under 𝐹 . Pick an arbitrary 𝑓 ∈ 𝑇 . Consider

a variable 𝑣 ∈ Var and states 𝜎0, 𝜎 ′
𝜎0 ∼𝑉 𝜎 ′
0,
We will show that 𝐹 (𝑓 )(𝜎 ′
properties as conditions (i) and (ii), as we have done before.

0, 𝜎, and 𝜎 ′ such that

𝜎 ∼𝑑 (𝑣) 𝜎 ′,

𝐹 (𝑓 )(𝜎0) ∈ St,
0) ∈ St and 𝐹 (𝑓 )(𝜎) ∼{𝑣 } 𝐹 (𝑓 )(𝜎 ′), while referring to these two desired

𝐹 (𝑓 )(𝜎), 𝐹 (𝑓 )(𝜎 ′) ∈ St.

𝑏
(cid:74)

𝑏
(cid:74)

Let us handle the condition (i) first. If

𝜎0) ∈ St, we have 𝑓 (
(cid:75)

𝑐0
(cid:74)
Next, we prove the condition (ii). If

0 = false, because 𝜎0 ∼𝑉 𝜎 ′
𝜎 ′
(cid:75)
𝜎0 = true, then
(cid:75)

𝜎0 = false, we have
0 and
𝑏
(cid:75)
(cid:74)
fv(𝑏) ⊆ 𝑉 . Thus, in this case, 𝐹 (𝑓 )(𝜎 ′
0 is also true. Furthermore,
0 ∈ St. If
𝜎 ′
𝑏
0) = 𝜎 ′
(cid:75)
(cid:74)
0 ∈ St since 𝑉 ⊇ 𝑉0, 𝜎0 ∼𝑉 𝜎 ′
in this case, by induction hypothesis,
𝜎0 ∈ St. Also, by
0, and
𝑐0
𝜎 ′
𝑐0
(cid:75)
(cid:75)
(cid:74)
(cid:74)
induction hypothesis again,
0. Since 𝑓 ∈ 𝑇 and
0, since 𝑉 ⊇ (𝑑0)∪ (𝑉 ) and 𝜎0 ∼𝑉 𝜎 ′
𝑐0
𝜎 ′
𝑐0
𝜎0 ∼𝑉
(cid:75)
(cid:74)
(cid:75)
(cid:74)
0) ∈ St, which implies that 𝐹 (𝑓 )(𝜎 ′
𝜎 ′
𝑐0
𝑓 (
(cid:75)
(cid:74)
𝜎 ′ = false since fv(𝑏) ⊆ 𝑑 (𝑣) and
𝜎 = false, we have
𝑏
(cid:75)
(cid:75)
(cid:74)
𝜎 ∼𝑑 (𝑣) 𝜎 ′. Thus, in this case, 𝐹 (𝑓 )(𝜎) = 𝜎 and 𝐹 (𝑓 )(𝜎 ′) = 𝜎 ′. Also, {𝑣 } ⊆ 𝑑 (𝑣), and so, 𝜎 ∼𝑑 (𝑣) 𝜎 ′
𝜎 = true. Then,
implies that 𝐹 (𝑓 )(𝜎) = 𝜎 ∼{𝑣 } 𝜎 ′ = 𝐹 (𝑓 )(𝜎 ′), as desired. Now assume that
𝑏
(cid:74)
(cid:75)
𝜎 ′ are in St, so
𝜎 and
𝑐0
𝑏
(cid:74)
(cid:75)
(cid:74)
(cid:75)
𝜎 ′). Furthermore, since 𝑑 (𝑣) ⊇ (𝑑0)∪(𝑑 (𝑣)) and
𝜎) and 𝐹 (𝑓 )(𝜎 ′) = 𝑓 (
that 𝐹 (𝑓 )(𝜎) = 𝑓 (
𝑐0
(cid:75)
(cid:75)
(cid:74)
𝜎 ′) ∈ St,
𝜎 ′. We then use the fact that 𝑓 ∈ 𝑇 and 𝑓 (
𝜎 ∼𝑑 (𝑣) 𝜎 ′, we have
𝜎 ∼𝑑 (𝑣)
𝑐0
𝜎), 𝑓 (
𝑐0
(cid:75)
(cid:75)
(cid:75)
(cid:74)
(cid:75)
(cid:74)
□
𝜎 ′), which gives the desired 𝐹 (𝑓 )(𝜎) ∼{𝑣 } 𝐹 (𝑓 )(𝜎 ′).
and conclude that 𝑓 (
𝑐0
𝜎) ∼{𝑣 } 𝑓 (
𝑐0
(cid:75)
(cid:74)
(cid:75)
(cid:74)

𝜎 ′ = true by the reason that fv(𝑏) ⊆ 𝑑 (𝑣) and 𝜎 ∼𝑑 (𝑣) 𝜎 ′. Also,
(cid:75)

0) ∈ St, as desired.

𝑐0
(cid:74)

𝑐0
(cid:74)

𝑐0
(cid:74)

𝑐0
(cid:74)

𝑏
(cid:74)

F.4 Proof of Theorem F.2
Let seq be the following operator, which models sequential composition:

seq : [St → St⊥] × [St → St⊥] → [St → St⊥]

seq(𝑓 , 𝑔) ≜ 𝑔† ◦ 𝑓 .

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:65

Also, define an operator cond for modelling if commands as follows:

cond : [St → B] × [St → St⊥] × [St → St⊥] → [St → St⊥]

cond (ℎ, 𝑓 , 𝑔)(𝜎) ≜

(cid:40)𝑓 (𝜎)
𝑔(𝜎)

if ℎ(𝜎) = true,
if ℎ(𝜎) = false.

Proof of Theorem F.2. We prove the theorem by induction on the structure of𝑐. Let (𝑝, 𝑑, 𝑉 ) ≜

Case 𝑐 ≡ skip. In this case,

𝑐
(cid:74)

conclusion, consider 𝑣 ∈ Var and 𝜏 ∈ St[𝑝 (𝑣)𝑐 ] = St[∅]. We should show 𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }, where

(cid:75)

(cid:75)
(𝜎) = 𝜎 for all 𝜎 ∈ St, and 𝑝 (𝑣) = Var for all 𝑣 ∈ Var. To prove the

♯.

𝑐
(cid:74)

𝑔(𝜎) =

(cid:40)

(𝜋Var,{𝑣 } ◦
undefined

𝑐
(cid:74)

(cid:75)

)(𝜎 ⊕ 𝜏)

(𝜎 ⊕ 𝜏) ∈ St

𝑐
(cid:74)

if
(cid:75)
otherwise.

Since

𝑐
(cid:74)

(cid:75)

(𝜎 ⊕ 𝜏) =

𝑐
(cid:74)

(cid:75)

(𝜎) = 𝜎 ∈ St for all 𝜎 ∈ St, we have 𝑔 = 𝜋Var,{𝑣 }. Thus, Assumption 3 implies

𝑔 = 𝜋Var,{𝑣 } ∈ 𝜙Var,{𝑣 } = 𝜙𝑝 (𝑣),{𝑣 }.

Case 𝑐 ≡ (𝑥 := 𝑒). In this case,

♯ if 𝑣 ≡ 𝑥. Consider 𝑣 ∈ Var and 𝜏 ∈ St[𝑝 (𝑣)𝑐 ]. We should show 𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }, where

(cid:75)

𝑒
(cid:74)

𝜎] for all 𝜎 ∈ St. Also, 𝑝 (𝑣) = Var if 𝑣 (cid:46) 𝑥,
(cid:75)

(𝜎) = 𝜎 [𝑥 ↦→

𝑐
(cid:74)

and

𝑒
(cid:76)

(cid:77)

𝑔(𝜎) =

(cid:40)

(𝜋Var,{𝑣 } ◦
undefined

𝑐
(cid:74)

(cid:75)

)(𝜎 ⊕ 𝜏)

(𝜎 ⊕ 𝜏) ∈ St

𝑐
(cid:74)

if
(cid:75)
otherwise.

If 𝑣 (cid:46) 𝑥, then 𝑔(𝜎) = 𝜋Var,{𝑣 } (
(𝜎)) = 𝜋Var,{𝑣 } (𝜎 [𝑥 ↦→
the first equality uses 𝑝 (𝑣) = Var, and the last uses 𝑣 (cid:46) 𝑥. Hence, Assumption 3 implies

𝜎]) = 𝜋Var,{𝑣 } (𝜎) for all 𝜎 ∈ St, where
(cid:75)

𝑒
(cid:74)

𝑐
(cid:74)

(cid:75)

If 𝑣 ≡ 𝑥, then 𝑔(𝜎) = (𝜋Var,{𝑥 } ◦
for all 𝜎 ∈ St. Since 𝜏 ∈ St[(
𝑒
(cid:76)

Case 𝑐 ≡ (𝑐 ′; 𝑐 ′′). Let (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

(cid:75)

𝑔 = 𝜋Var,{𝑣 } ∈ 𝜙Var,{𝑣 } = 𝜙𝑝 (𝑣),{𝑣 }.
)(𝜎 ⊕ 𝜏) = 𝜋Var,{𝑥 } ((𝜎 ⊕ 𝜏) [𝑥 ↦→
𝑐
♯)𝑐 ] and 𝑝 (𝑣) =
(cid:74)
𝑒
(cid:76)
(cid:77)
(𝜎 ⊕ 𝜏)] ∈ 𝜙
(cid:75)
♯ and (𝑝 ′′, 𝑑 ′′, 𝑉 ′′) ≜

𝑒
(cid:76)

(cid:77)

𝑒
(cid:74)

♯, Assumption 2 implies

(cid:75)

♯,{𝑥 } = 𝜙𝑝 (𝑣),{𝑣 }.
𝑐 ′′
(cid:74)
(cid:17)𝑐

(cid:75)

♯. Then,

𝑒
(cid:74)
𝑐 ′
(cid:74)

(cid:75)

(cid:77)
𝑔 = 𝜆𝜎. [𝑥 ↦→

𝑝 (𝑣) =

=

(cid:16)
𝑉 ′ ∪ (𝑝 ′)∩(𝑑 ′′(𝑣))𝑐 ∪ (𝑑 ′)∪ (𝑝 ′′(𝑣)𝑐 )
(cid:16)
(cid:16)
𝑉 ′ ∪ (𝑑 ′)∪ (𝑝 ′′(𝑣)𝑐 )

(𝑝 ′)∩(𝑑 ′′(𝑣))

\

(cid:17)

(cid:17) for all 𝑣 ∈ Var.

(𝜎 ⊕ 𝜏)]) = [𝑥 ↦→

(𝜎 ⊕ 𝜏)]

𝑒
(cid:74)

(cid:75)

,
(cid:75)

𝑐 ′
(cid:74)

𝑐 ′′
(cid:74)

). To prove the conclusion, let 𝑣 ∈ Var. It suffices to apply Lemma F.7
, 𝐾 = 𝑝 (𝑣), 𝐿 = 𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣), 𝐿′ = 𝑑 ′′(𝑣), and 𝑀 = {𝑣 }. What remains is to
(cid:75)

(cid:75)

𝑐
(cid:74)
, 𝑔 =

Also, we have
= seq(
(cid:75)
to 𝑓 =
𝑐 ′′
(cid:74)
show the preconditions of the lemma:
, 𝑝 (𝑣), 𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣)).
𝑐 ′
(cid:74)
(cid:75)
, 𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣), {𝑣 }).
𝑐 ′′
, 𝑝 (𝑣)𝑐, 𝑑 ′′(𝑣) \ (𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣))).
(cid:75)
(cid:74)
𝑐 ′
(cid:75)
(cid:74)
, 𝑑 ′′(𝑣), {𝑣 }).
𝑐 ′′
(cid:75)
(cid:74)

𝑐 ′
(cid:74)
(cid:75)
|= Φ(
|= Φ(
|= Δ(
|= Δ(

(a)
(b)
(c)
(d)

We obtain (b) as follows: by induction hypothesis on 𝑐 ′′, we have |= Φ(
, 𝑝 ′′(𝑣), {𝑣 }), and by the
(cid:75)
weakening lemma for Φ (Lemma F.4), we have (b). We obtain (d) directly by Theorem F.1 on 𝑐 ′′. For (a),
consider induction hypothesis on 𝑐 ′, which says that |= Φ(
, 𝑝 ′(𝑤), {𝑤 }) for all 𝑤 ∈ Var. By the
(cid:75)
merging lemma for Φ (Lemma F.6), we have |= Φ(
, (𝑝 ′)∩ (𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣)), 𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣)). Since
(cid:75)

𝑐 ′
(cid:74)
𝑝 (𝑣) ⊆ (𝑝 ′)∩(𝑑 ′′(𝑣)) ⊆ (𝑝 ′)∩(𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣)),

𝑐 ′′
(cid:74)

𝑐 ′
(cid:74)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:66

Wonyeol Lee, Xavier Rival, and Hongseok Yang

we obtain (a) by the weakening lemma for Φ (Lemma F.4). For (c), observe that

𝑝 (𝑣)𝑐 ⊇ 𝑉 ′ ∪ (𝑑 ′)∪ (𝑝 ′′(𝑣)𝑐 ) and 𝑑 ′′(𝑣) \ (𝑑 ′′(𝑣) ∩ 𝑝 ′′(𝑣)) = 𝑝 ′′(𝑣)𝑐

(24)

𝑐 ′
(cid:74)

, 𝑉 ′, ∅) and |= Δ(
(cid:75)

where the second equality follows from 𝑝 ′′(𝑣) ⊇ 𝑑 ′′(𝑣)𝑐 . By Theorem F.1 on 𝑐 ′, we have
|=
, 𝑉 ′, 𝑝 ′′(𝑣)𝑐 )
Δ(
holds, and if 𝑝 ′′(𝑣)𝑐 ≠ ∅, then |= Δ(
, (𝑑 ′)∪(𝑝 ′′(𝑣)𝑐 ), 𝑝 ′′(𝑣)𝑐 ) holds by the merging lemma for Δ
(cid:75)
(cid:75)
(Lemma F.5). By Eq. (24) and the weakening lemma for Δ (Lemma F.3), we obtain (c) for both cases.
Note that we crucially used 𝑝 (𝑣)𝑐 ⊇ 𝑉 ′ to handle the case 𝑝 ′′(𝑣)𝑐 = ∅.

, 𝑑 ′(𝑤), {𝑤 }) for all 𝑤 ∈ Var. If 𝑝 ′′(𝑣)𝑐 = ∅, then |= Δ(
(cid:75)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

𝑐 ′
(cid:74)

(cid:75)

𝑐 ′′
(cid:74)

(cid:75)

Case 𝑐 ≡ (if 𝑏 {𝑐 ′}, else {𝑐 ′′}). Let (𝑝 ′, 𝑑 ′, 𝑉 ′) ≜

♯ and (𝑝 ′′, 𝑑 ′′, 𝑉 ′′) ≜

♯. Then,

Also,
𝑓 =

𝑝 (𝑣) = fv(𝑏)𝑐 ∩ 𝑝 ′(𝑣) ∩ 𝑝 ′′(𝑣) for all 𝑣 ∈ Var.
). To prove the conclusion, let 𝑣 ∈ Var. It suffices to apply Lemma F.8 to
𝑐
𝑏
𝑐 ′′
𝑐 ′
= cond (
(cid:74)
(cid:74)
(cid:74)
(cid:74)
(cid:75)
, 𝐾 = 𝑝 (𝑣), 𝐿 = {𝑣 }, and 𝑏. What remains is to show the preconditions of the lemma:
, 𝑔 =
𝑐 ′
𝑐 ′′
(cid:74)
(cid:75)
(cid:74)
(cid:75)
|= Φ(
(a)
, 𝑝 (𝑣), {𝑣 }).
𝑐 ′
(cid:75)
(cid:74)
|= Φ(
, 𝑝 (𝑣), {𝑣 }).
(b)
𝑐 ′′
(c) 𝑝 (𝑣)𝑐 ⊇ fv(𝑏).
(cid:74)
(cid:75)

(25)

,
(cid:75)

,
(cid:75)

(cid:75)

We obtain (a) and (b) as follows: by induction hypothesis on 𝑐 ′ and 𝑐 ′′, we have |= Φ(
and |= Φ(
(a) and (b). We obtain (c) directly by Eq. (25).

, 𝑝 ′(𝑣), {𝑣 })
(cid:75)
, 𝑝 ′′(𝑣), {𝑣 }), and by Eq. (25) and the weakening lemma for Φ (Lemma F.4), we have
(cid:75)

𝑐 ′′
(cid:74)

𝑐 ′
(cid:74)

Case 𝑐 ≡ (while 𝑏 {𝑐0}). The proof starts by decomposing

and

𝑐
(cid:74)

(cid:75)

𝑐
(cid:74)

(cid:75)

♯ into smaller pieces. Let

(𝑝0, 𝑑0, 𝑉0) ≜

♯.

𝑐0
(cid:74)

(cid:75)

Define 𝐹 : [St → St⊥] → [St → St⊥] and 𝐹 ♯ : D♯ → D♯ as in §3 and Fig. 3:

if
if

𝑐0
(cid:74)

𝜎)
(cid:75)

𝑏
(cid:74)
𝑏
(cid:74)

Define 𝑡 ′

𝐹 (𝑡)(𝜎) ≜

𝜎 = false
(cid:75)
𝜎 = true,
(cid:75)

(cid:40)𝜎
𝑡 † (
𝜆𝑣. fv(𝑏)𝑐 ∩ (𝑉0 ∪ (𝑝0)∩(𝑑 (𝑣))𝑐 ∪ (𝑑0)∪(𝑝 (𝑣)𝑐 ))𝑐,
𝐹 ♯ (𝑝, 𝑑, 𝑉 ) ≜ (cid:169)
𝜆𝑣. fv(𝑏) ∪ 𝑉0 ∪ (𝑑0)∪(𝑑 (𝑣)) ∪ {𝑣 },
(cid:173)
fv(𝑏) ∪ (𝑑0)∪ (𝑉 ) ∪ 𝑉0
(cid:171)
𝑛 ∈ [St → St⊥] and (𝑝 ′
𝑛, 𝑉 ′
𝑛, 𝑑 ′
(cid:40)𝜆𝜎. 𝐹 𝑛 (𝑡⊥)(𝜎)
(𝐹 ♯)𝑛 (𝑝⊥, 𝑑⊥, 𝑉⊥),
if 𝑛 ∈ N
(cid:195)𝑖 ∈N(𝑝 ′
(cid:195)𝑖 ∈N 𝑡 ′
if 𝑛 = ∞,
𝑖 , 𝑑 ′
𝑖
where 𝑡⊥ = 𝜆𝜎. ⊥ and (𝑝⊥, 𝑑⊥, 𝑉⊥) = (𝜆𝑣. Var, 𝜆𝑣. ∅, ∅). Then, we have
= 𝑡 ′

∞, 𝑉 ′
(cid:75)
The proof is organized as follows. Define 𝑇 ,𝑇 ′ ⊆ [St → St⊥] as

𝑛 ) ∈ D♯ for 𝑛 ∈ N ∪ {∞} as

∞ and

𝑖 , 𝑉 ′
𝑖 )

= (𝑝 ′

∞, 𝑑 ′

𝑛, 𝑉 ′

𝑛 ) ≜

𝑡 ′
𝑛 ≜

𝑛, 𝑑 ′

∞).

𝑐
(cid:74)

𝑐
(cid:74)

(𝑝 ′

(cid:40)

(cid:75)

♯

.

(cid:170)
(cid:174)
(cid:172)

if 𝑛 ∈ N
if 𝑛 = ∞,

𝑇 ≜ {𝑓 ∈ [St → St⊥] | ∀𝑣 ∈ Var.
𝑇 ′ ≜ {𝑓 ∈ [St → St⊥] | ∀𝑣 ∈ Var.

In Theorem F.1, we proved

|= Δ(𝑓 , 𝑑 ′
|= Φ(𝑓 , 𝑝 ′

∞ (𝑣), {𝑣 }) ∧ |= Δ(𝑓 , 𝑉 ′
∞ (𝑣), {𝑣 })}.

∞, ∅)},

In this theorem, our goal is to show 𝑡 ′

𝑡 ′
𝑛 ∈ 𝑇 for all 𝑛 ∈ N ∪ {∞}.
∞ ∈ 𝑇 ′. To do so, we prove the next three statements:

(26)

0 ∈ 𝑇 ′.

(a) 𝑡 ′
(b) If 𝑡 ′ ∈ 𝑇 ′ ∩ 𝑇 , then 𝐹 (𝑡 ′) ∈ 𝑇 ′.
𝑛 ∈ 𝑇 ′ for all 𝑛 ∈ N, then 𝑡 ′
(c) If 𝑡 ′

∞ ∈ 𝑇 ′.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:67

𝑛 ∈ 𝑇 ′ for all 𝑛 ∈ N, and this and

∞ (𝑣), {𝑣 }) for all 𝑣 ∈ Var.

It suffices to prove the three because (a), (b), and Eq. (26) imply 𝑡 ′
∞ ∈ 𝑇 ′. We now prove (a), (b), and (c) as follows.
(c) imply 𝑡 ′

First, (a) follows directly from Lemma F.9.
Next, we prove (b). Consider 𝑡 ′ ∈ 𝑇 ′ ∩𝑇 . Our goal is to show |= Φ(𝐹 (𝑡 ′), 𝑝 ′

Observe that

, 𝑡 ′),
(cid:75)
By Theorem F.1 and induction hypothesis on 𝑐0, we have

𝐹 (𝑡 ′) = cond (

, seq(
(cid:75)

𝑐0
(cid:74)

𝑏
(cid:74)

).

skip
(cid:75)

(cid:74)

and by assumption, we have

∈ 𝛾 (𝑝0, 𝑑0, 𝑉0),

𝑐0
(cid:74)

(cid:75)

𝑡 ′ ∈ 𝛾 (𝑝 ′

∞, 𝑑 ′

∞, 𝑉 ′

∞) = 𝑇 ′ ∩ 𝑇 .

By applying to these the proofs of skip, sequential composition, and conditional cases, we have

|= Φ(𝐹 (𝑡 ′), 𝑝 ′′(𝑣), {𝑣 }) for all 𝑣 ∈ Var

where

𝑝 ′′(𝑣) = fv(𝑏)𝑐 ∩

(cid:16)
𝑉0 ∪ (𝑝0)∩(𝑑 ′

∞ (𝑣)𝑐 )

(cid:17)𝑐

∩ Var.

∞ (𝑣))𝑐 ∪ (𝑑0)∪ (𝑝 ′
∞, 𝑑 ′

Since 𝑝 ′′ is the 𝑝 part of 𝐹 ♯ (𝑝 ′
Hence, we obtain |= Φ(𝐹 (𝑡 ′), 𝑝 ′

∞, 𝑉 ′

∞) and (𝑝 ′

∞, 𝑑 ′
∞ (𝑣), {𝑣 }) for all 𝑣 ∈ Var. This completes the proof of (b).

∞) is a fixed point of 𝐹 ♯, we have 𝑝 ′′ = 𝑝 ′
∞.

∞, 𝑉 ′

Finally, we prove (c). Suppose that 𝑡 ′

𝑛 ∈ 𝑇 ′ for all 𝑛 ∈ N, and let 𝑣 ∈ Var. Our goal is to show

Observe that Lemma F.10 implies the goal when applied to 𝐾 = 𝑝 ′
{𝑡 ′

𝑛 }𝑛 ∈N. Hence, it suffices to show the three preconditions of the lemma:

∞ (𝑣), 𝐿 = {𝑣 }, and {𝑓𝑛 }𝑛 ∈N =

|= Φ(𝑡 ′

∞, 𝑝 ′

∞ (𝑣), {𝑣 }).

𝑛 }𝑛 ∈N is an 𝜔-chain.

• {𝑡 ′
• For all 𝜏 ∈ St[𝑝 ′
• For all 𝑛 ∈ N, we have |= Φ(𝑡 ′

𝑛, 𝑝 ′

∞ (𝑣), {𝑣 }).

∞ (𝑣)𝑐 ] and 𝑛 ∈ N, the set {𝜎 ′ ∈ St[𝑝 ′

∞ (𝑣)] | 𝑡 ′

𝑛 (𝜎 ′ ⊕ 𝜏) ∈ St} is ∅ or St[𝑝 ′

∞ (𝑣)].

The first precondition was already observed in §3. The third one holds by the assumption that 𝑡 ′
for all 𝑛 ∈ N. For the second one, it is enough to show the next two statements:

𝑛 ∈ 𝑇 ′

(i) For all 𝑈 ⊆ Var with 𝑈 ⊇ 𝑉 ′

∞, and for all 𝜏 ∈ St[𝑈 ] and 𝑛 ∈ N, the next set is ∅ or St[𝑈 𝑐 ]:
{𝜎 ′ ∈ St[𝑈 𝑐 ] | 𝑡 ′

𝑛 (𝜎 ′ ⊕ 𝜏) ∈ St}.

(ii) For all 𝑣 ∈ Var,

∞ (𝑣)𝑐 ⊇ 𝑉 ′
𝑝 ′
∞.
We give the proof of the two statements below. This completes the proof of the while-loop case.

Proof of (ii). We prove a stronger statement: for all 𝑛 ∈ N and 𝑣 ∈ Var, 𝑝 ′
𝑛 (𝑣)𝑐 ⊇ (cid:208)𝑛 ∈N 𝑉 ′

ment implies (ii) because 𝑝 ′
statement by induction on 𝑛. For 𝑛 = 0, we have

∞ (𝑣)𝑐 = ((cid:209)𝑛 ∈N 𝑝 ′

𝑛 (𝑣))𝑐 = (cid:208)𝑛 ∈N 𝑝 ′

𝑛 (𝑣)𝑐 ⊇ 𝑉 ′
𝑛 = 𝑉 ′

𝑛 . This state-
∞. We prove the

𝑛 (𝑣)𝑐 = Var
𝑝 ′

𝑐 = ∅ ⊇ ∅ = 𝑉 ′

𝑛 for all 𝑣 ∈ Var.

For 𝑛 > 0, let 𝑣 ∈ Var. By induction hypothesis, 𝑝 ′

𝑛−1(𝑣)𝑐 ⊇ 𝑉 ′

𝑛 (𝑣)𝑐 = fv(𝑏) ∪ 𝑉0 ∪ (𝑝0)∩ (𝑑 ′
𝑝 ′
⊇ fv(𝑏) ∪ 𝑉0 ∪ (𝑑0)∪ (𝑉 ′

𝑛−1(𝑣))𝑐 ∪ (𝑑0)∪ (𝑝 ′
𝑛−1) = 𝑉 ′
𝑛 .

𝑛−1 holds. Using this, we have
𝑛−1(𝑣)𝑐 )

This completes the proof of (ii).

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:68

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Proof of (i). Consider 𝑈 ⊆ Var, 𝜏 ∈ St[𝑈 ], and 𝑛 ∈ N such that 𝑈 ⊇ 𝑉 ′

∞. Let Σ ≜ {𝜎 ′ ∈ St[𝑈 𝑐 ] |
𝑛 (𝜎 ′ ⊕ 𝜏) ∈ St}. If Σ = ∅, there is nothing left to prove. So assume Σ ≠ ∅. To prove Σ = St[𝑈 𝑐 ], we
𝑡 ′
need to show that 𝑡 ′
𝑛 (𝜎 ′ ⊕ 𝜏) ∈ St
using the next two claims:

𝑛 (𝜎 ′ ⊕ 𝜏) ∈ St for any 𝜎 ′ ∈ St[𝑈 𝑐 ]. Choose 𝜎 ′ ∈ St[𝑈 𝑐 ]. We show 𝑡 ′

(iii) For all 𝑛 ∈ N and 𝜎 ∈ St,

𝑡 ′
𝑛 (𝜎) =

(cid:40)

𝑐 (𝑖)
0
(cid:74)
⊥

𝜎
(cid:75)

if 𝑖 ∈ 𝐼𝑛 (𝜎)
otherwise,

where 𝑐 (𝑖)

0 ≜ (skip; 𝑐0; · · · ; 𝑐0) that has 𝑖 copies of 𝑐0, and
𝑐 (𝑖)
0
(cid:74)

𝜎 ∈ St ∧
(cid:75)

𝐼𝑛 (𝜎) ≜ {𝑖 ∈ [0, 𝑛 − 1] |

(cid:75)

(

(
(cid:75)
Note that Eq. (27) is well-defined since 𝐼𝑛 (𝜎) has at most one element.

𝜎) = · · · =
(cid:75)

𝑏
(cid:74)

∧

(cid:75)

(

𝑏
(cid:74)
𝑏
(cid:74)

𝑐 (𝑖)
𝜎) = false
0
(cid:74)
(cid:75)
𝑐 (𝑖−1)
0
(cid:74)

(27)

𝑐 (0)
0
(cid:74)

𝜎) = true}.
(cid:75)

(iv) For all 𝑛 ∈ N,

|= Δ(

𝑐 (𝑛)
0
(cid:74)

, 𝑉 ′
(cid:75)

∞, fv(𝑏)).

We give the proof of the two claims below, and for now we just assume them.

Since Σ ≠ ∅, there is some 𝜎 ′′ ∈ St[𝑈 𝑐 ] such that 𝑡 ′

𝑛 (𝜎 ′′ ⊕ 𝜏) ∈ St. Since 𝑡 ′

𝑛 (𝜎 ′′ ⊕ 𝜏) ∈ St, (iii)

implies that

𝑛 (𝜎 ′′ ⊕ 𝜏) =
𝑡 ′
for some 𝑚 ∈ 𝐼𝑛 (𝜎 ′′ ⊕ 𝜏). Since 𝜎 ′ ⊕ 𝜏 ∼𝑉 ′
𝑐 (𝑚)
𝑖 ∈ [0, 𝑚] (by
0
(cid:74)
𝑐 (𝑖)
(𝜎 ′ ⊕ 𝜏) ∈ St and
0
(cid:74)

(cid:75)
(𝜎 ′′ ⊕ 𝜏) ∈ St), (iv) implies that

𝑐 (𝑖)
0
(cid:74)

(cid:75)

(cid:75)

(cid:75)

∞

for all 𝑖 ∈ [0, 𝑚].
By combining these with 𝑚 ∈ 𝐼𝑛 (𝜎 ′′ ⊕ 𝜏), we get 𝑚 ∈ 𝐼𝑛 (𝜎 ′ ⊕ 𝜏). Hence, by (iii), we have

(𝜎 ′ ⊕ 𝜏) ∼fv (𝑏)

(𝜎 ′′ ⊕ 𝜏)

(cid:75)

𝑐 (𝑖)
0
(cid:74)

𝑐 (𝑚)
(𝜎 ′′ ⊕ 𝜏) ∈ St
0
(cid:74)
𝜎 ′′ ⊕ 𝜏 (by 𝑈 ⊇ 𝑉 ′

∞) and

𝑐 (𝑖)
0
(cid:74)

(cid:75)

(𝜎 ′′ ⊕ 𝜏) ∈ St for all

This completes the proof of (i).

𝑛 (𝜎 ′ ⊕ 𝜏) =
𝑡 ′

𝑐 (𝑚)
0
(cid:74)

(cid:75)

(𝜎 ′ ⊕ 𝜏) ∈ St.

Proof of (iii). We prove this by induction on 𝑛. For 𝑛 = 0, 𝑡 ′

𝑛 (𝜎) = ⊥ and 𝐼𝑛 (𝜎) = ∅ for all 𝜎 ∈ St.

Hence, Eq. (27) holds. For 𝑛 > 0, we have

𝑛−1)(𝜎)

𝑛 (𝜎) = 𝐹 (𝑡 ′
𝑡 ′
(cid:40)𝜎
(𝑡 ′
𝑛−1)† (

=

=

=

(

(cid:75)

𝜎
𝑐 (𝑖)
0
(cid:74)
⊥
𝑐 (0)
𝜎
0
(cid:74)
(cid:75)
𝑐 (𝑖+1)
0
(cid:74)
⊥








𝜎
(cid:75)

𝑐0
(cid:74)

𝜎)
(cid:75)

𝑐0
(cid:74)

𝜎)
(cid:75)

if
if

𝑏
(cid:74)
𝑏
(cid:74)
𝑏
(cid:74)
𝑏
(cid:74)

𝜎 = false
(cid:75)
𝜎 = true
(cid:75)
if
𝜎 = false · · · (∗1)
(cid:75)
if
𝜎 = true,
(cid:75)
otherwise

𝑐0
(cid:74)

𝜎 ∈ St and 𝑖 ∈ 𝐼𝑛−1(
(cid:75)

𝑐0
(cid:74)

𝜎) · · · (∗2)
(cid:75)

if 0 ∈ 𝐼𝑛 (𝜎) · · · (∗′
1)
if 𝑖 + 1 ∈ 𝐼𝑛 (𝜎) and 𝑖 + 1 ≥ 1 · · · (∗′
2)
otherwise.

The second equality is by the definition of 𝐹 , the third by induction hypothesis, and the last by the
following: for any 𝜎 ∈ St and 𝑗 ∈ {1, 2}, (∗𝑗 ) holds iff (∗′
𝜎 ∈ St
𝑐 (𝑖+1)
(cid:75)
implies
𝜎 ∈ St. Hence, Eq. (27) holds. This
𝜎 ∈ St implies
(
0
(cid:75)
(cid:75)
(cid:74)
(cid:75)
completes the proof of (iii).

𝑗 ) holds; and for any 𝑖 ∈ N,

𝜎, and
(cid:75)

𝑐 (𝑖+1)
0
(cid:74)

𝜎) =
(cid:75)

𝑐 (𝑖)
0
(cid:74)

𝑐0
(cid:74)

𝑐0
(cid:74)

𝑐0
(cid:74)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:69

Proof of (iv). To prove this, we prove a stronger statement: for all 𝑛 ∈ N,

This statement implies (iv) since 𝑉 ′
𝑛 ⊆ 𝑉 ′
for Δ (Lemma F.3). We prove the statement by induction on 𝑛. For 𝑛 = 0,
𝑛+1 = fv(𝑏) ∪ 𝑉0. By Theorem F.1 on skip, we have |= Δ(
𝑉 ′
by the merging lemma for Δ (Lemma F.5), we have

𝑐 (𝑛)
𝑛+1, fv(𝑏)).
0
(cid:74)
∞ for all 𝑛 ∈ N and we have the weakening lemma
𝑐 (𝑛)
and
0
(cid:74)
, {𝑣 }, {𝑣 }) for all 𝑣 ∈ Var, and then
skip
(cid:75)

, 𝑉 ′
(cid:75)
=

skip
(cid:75)

|= Δ(

(cid:75)

(cid:74)

(cid:74)

Since 𝑉 ′
for𝑛 > 0,
on 𝑐0, and induction hypothesis of the theorem (not that of the claim (iv)), we have

𝑛+1, fv(𝑏)) holds by the weakening lemma for Δ (Lemma F.3). Next,
= (𝑝0, 𝑑0, 𝑉0), Theorem F.1
𝑛+1 = fv(𝑏) ∪ 𝑉0 ∪ (𝑑0)∪(𝑉 ′

𝑛+1 ⊇ fv(𝑏),
𝑐 (𝑛)
=
0
(cid:74)

𝑐 (𝑛)
|= Δ(
0
𝑐0; 𝑐 (𝑛−1)
(cid:74)
0
(cid:74)

, 𝑉 ′
(cid:75)
and𝑉 ′

𝑛 ). By

𝑐0
(cid:74)

(cid:75)

(cid:75)

(cid:75)

♯

|= Δ(

, fv(𝑏), fv(𝑏)).
skip
(cid:75)

(cid:74)

Also, by induction hypothesis of our strengthening of the claim (iv) and the weakening lemma for
Δ (Lemma F.3),

∈ 𝛾 (𝑝0, 𝑑0, 𝑉0).

𝑐0
(cid:74)

(cid:75)

By applying to these the proof of Theorem F.1 (on the sequential composition case), we have

|= Δ(

𝑐 (𝑛−1)
0
(cid:74)

, 𝑉 ′
(cid:75)

𝑛, {𝑣 }) for all 𝑣 ∈ fv(𝑏).

|= Δ(

𝑐0; 𝑐 (𝑛−1)
0
(cid:74)

By the merging lemma for Δ (Lemma F.5),
cludes 𝑉0 ∪ (𝑑0)∪ (𝑉 ′
This completes the proof of (iv).

𝑛 ), we get |= Δ(

𝑛 ), {𝑣 }) for all 𝑣 ∈ fv(𝑏).

, 𝑉0 ∪ (𝑑0)∪ (𝑉 ′
(cid:75)
𝑐 (𝑛)
|= Δ(
𝑛+1 in-
0
𝑐 (𝑛)
(cid:74)
𝑛+1, fv(𝑏)) by the weakening lemma for Δ (Lemma F.3).
0
(cid:74)

, 𝑉0 ∪ (𝑑0)∪(𝑉 ′
(cid:75)

𝑛 ), fv(𝑏)) holds. Since 𝑉 ′

, 𝑉 ′
(cid:75)

Case 𝑐 ≡ (𝑥 := sam(name(𝛼, 𝑒), distN (𝑒1, 𝑒2), 𝜆𝑦.𝑒 ′)). To prove the conclusion, consider 𝑣 ∈ Var

and 𝜏 ∈ St[𝑝 (𝑣)𝑐 ]. We should show 𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }, where

𝑔(𝜎) = 𝜋Var,{𝑣 } (

We prove this by case analysis on 𝑣.

(𝜎 ⊕ 𝜏)) = [𝑣 ↦→

𝑐
(cid:74)

(cid:75)

(𝜎 ⊕ 𝜏)(𝑣)].

𝑐
(cid:74)

(cid:75)

First, suppose 𝑣 ∉ {𝑥 } ∪ {val𝜇, pr 𝜇, cnt𝜇 | 𝜇 ∈ Name, 𝜇 = (𝛼, _)}. Then, 𝑝 (𝑣) = Var and

𝑔(𝜎) = [𝑣 ↦→

𝑐
(cid:74)

𝜎 (𝑣)] = [𝑣 ↦→ 𝜎 (𝑣)] = 𝜋Var,{𝑣 } (𝜎)
(cid:75)

(cid:75)

𝑐
(cid:74)

for all 𝜎 ∈ St[𝑝 (𝑣)]. Here the first equality follows from 𝜏 ∈ St[∅], and the second equality holds
does not change the value of 𝑣. Hence, by Assumption 3, 𝑔 = 𝜋Var,{𝑣 } ∈ 𝜙Var,{𝑣 } = 𝜙𝑝 (𝑣),{𝑣 }.
since
Next, suppose 𝑣 ∈ {𝑥 } ∪ {val𝜇, pr 𝜇, cnt𝜇 | 𝜇 ∈ Name, 𝜇 = (𝛼, _)}. Then, we have 𝑝 (𝑣)𝑐 ⊇ fv(𝑒):
if 𝑒 is a constant, fv(𝑒) = ∅ holds, and if 𝑒 is not a constant, the definition of 𝑝 (𝑣) ensures this. Thus,
there exists 𝜇0 ∈ Name such that create_name(𝛼,
(𝜎 ⊕ 𝜏)) = 𝜇0 for all 𝜎 ∈ St[𝑝 (𝑣)]. We now do
refined case analysis on 𝑣 using 𝜇0.

𝑒
(cid:74)

(cid:75)

• Case 𝑣 ∈ {val𝜇, pr 𝜇, cnt𝜇 | 𝜇 ∈ Name, 𝜇 = (𝛼, _), 𝜇 ≠ 𝜇0}. In this case,

𝑔(𝜎) = [𝑣 ↦→ (𝜎 ⊕ 𝜏)(𝑣)] = 𝜋Var,{𝑣 } (𝜎 ⊕ 𝜏)

does not change the value of 𝑣. By
for all 𝜎 ∈ St[𝑝 (𝑣)]. Here the first equality holds since
Assumption 3, we have 𝜋Var,{𝑣 } ∈ 𝜙Var,{𝑣 }. Then, by Assumption 5, we obtain 𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }. Note
♯,
that this argument does not depend on the value of 𝑝 (𝑣) (which can be Var, fv(𝑒)𝑐 ∩
etc., depending on 𝑒 and 𝑣).

𝑣 + 1
(cid:76)

𝑐
(cid:74)

(cid:75)

(cid:77)
1 . So, there
1 ] such that 𝜏 = 𝜏1 ⊕ 𝜏2. Let ℎ : St[𝐾1] → St[{𝑣 }] be

♯. Then, 𝑝 (𝑣)𝑐 ⊇ (

𝑒 ′[𝜇0/𝑦]
(cid:76)

♯)𝑐 = 𝐾𝑐

(cid:77)

(cid:77)

𝑒 ′[𝜇0/𝑦]
(cid:76)

• Case 𝑣 ∈ {𝑥, val𝜇0 }. Define 𝐾1 ≜

exist 𝜏1 ∈ St[𝐾𝑐
a function defined by

1 ] and 𝜏2 ∈ St[𝑝 (𝑣)𝑐 \ 𝐾𝑐

ℎ(𝜎 ′) ≜ [𝑣 ↦→

𝑒 ′[𝜇0/𝑦]
(cid:74)

(cid:75)

(𝜎 ′ ⊕ 𝜏1)].

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:70

Then,

𝑔(𝜎) = (cid:2)𝑣 ↦→

Wonyeol Lee, Xavier Rival, and Hongseok Yang

(𝜎 ⊕ 𝜏)(cid:3) = ℎ(𝜎 ⊕ 𝜏2)

𝑒 ′[𝜇0/𝑦]
(cid:74)

(cid:75)

for all 𝜎 ∈ St[𝑝 (𝑣)]. By Assumption 2, we have ℎ ∈ 𝜙𝐾1,{𝑣 }, Then, by Assumption 5, we obtain
𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }.
• Case 𝑣 ≡ pr 𝜇0

♯. Since 𝜏 ∈ St[𝑝 (𝑣)𝑐 ]
and 𝑝 (𝑣)𝑐 = (𝐾 \ fv(𝑒)𝑐 ) ⊎ 𝐾𝑐 , there exist 𝜏1 ∈ St[𝐾 \ fv(𝑒)𝑐 ] and 𝜏2 ∈ St[𝐾𝑐 ] such that
𝜏 = 𝜏1 ⊕ 𝜏2. Using 𝜏1 and 𝜏2, we have

. In this case, 𝑝 (𝑣) = fv(𝑒)𝑐 ∩ 𝐾 for 𝐾 =

pdfN (𝜇0; 𝑒1, 𝑒2)
(cid:76)

(cid:77)

𝑔(𝜎) = (cid:2)𝑣 ↦→
= (cid:2)𝑣 ↦→

distN (𝑒1, 𝑒2)
(cid:75)
pdfN(𝜇0; 𝑒1, 𝑒2)

(cid:74)

(𝜎 ⊕ 𝜏) (cid:0)(𝜎 ⊕ 𝜏)(𝜇0)(cid:1)(cid:3)
(𝜎 ⊕ 𝜏)(cid:3) = ℎ(𝜎 ⊕ 𝜏1)

(cid:74)
for all 𝜎 ∈ St[𝑝 (𝑣)], where ℎ : St[𝐾] → St[{𝑣 }] is defined by

(cid:75)

ℎ(𝜎 ′) = [𝑣 ↦→

(𝜎 ′ ⊕ 𝜏2)].

pdfN (𝜇0; 𝑒1, 𝑒2)
(cid:74)
𝑐
(cid:74)

(cid:75)
Here the first equality follows from the definition of
, the second equality holds because pdfN
is the density function of a normal distribution, and the third equality comes from 𝜏 = 𝜏1 ⊕ 𝜏2.
By Assumption 2, we have ℎ ∈ 𝜙𝐾,{𝑣 }. Then, by Assumption 5, we obtain 𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }.

• Case 𝑣 ≡ cnt𝜇0. The proof is similar to the above case 𝑣 ≡ pr 𝜇0

. In this case, 𝑝 (𝑣) = fv(𝑒)𝑐 ∩ 𝐾
♯. As in the above case, there exist 𝜏1 ∈ St[𝐾 \ fv(𝑒)𝑐 ] and 𝜏2 ∈ St[𝐾𝑐 ] such

(cid:75)

for 𝐾 =
cnt𝜇0 + 1
that 𝜏 = 𝜏1 ⊕ 𝜏2, and we have

(cid:77)

(cid:76)

𝑔(𝜎) = [𝑣 ↦→ (𝜎 ⊕ 𝜏)(cnt𝜇0 ) + 1]

for all 𝜎 ∈ St[𝑝 (𝑣)], where ℎ : St[𝐾] → St[{𝑣 }] is defined by

= [𝑣 ↦→

cnt𝜇0 + 1
(cid:75)
(cid:74)

(𝜎 ⊕ 𝜏)] = ℎ(𝜎 ⊕ 𝜏1)

ℎ(𝜎 ′) = [𝑣 ↦→

cnt𝜇0 + 1

(𝜎 ′ ⊕ 𝜏2)].

(cid:74)

(cid:75)

By Assumption 2, we have ℎ ∈ 𝜙𝐾,{𝑣 }. Then, by Assumption 5, we obtain 𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }.
Case 𝑐 ≡ obs(distN(𝑒1, 𝑒2), 𝑟 ). To prove the conclusion, consider 𝑣 ∈ Var and 𝜏 ∈ St[𝑝 (𝑣)𝑐 ]. We

should show 𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }, where

𝑔(𝜎) = 𝜋Var,{𝑣 } (

We prove this by case analysis on 𝑣.

(𝜎 ⊕ 𝜏)) = [𝑣 ↦→

𝑐
(cid:74)

(cid:75)

(𝜎 ⊕ 𝜏)(𝑣)].

𝑐
(cid:74)

(cid:75)

First, suppose 𝑣 (cid:46) like. Then, 𝑝 (𝑣) = Var and

𝑔(𝜎) = [𝑣 ↦→

𝑐
(cid:74)

𝜎 (𝑣)] = [𝑣 ↦→ 𝜎 (𝑣)] = 𝜋Var,{𝑣 } (𝜎)
(cid:75)

for all 𝜎 ∈ St[𝑝 (𝑣)]. Here the first equality is by 𝜏 ∈ St[∅], and the second equality holds since
does not change the value of 𝑣. Hence, by Assumption 3, 𝑔 = 𝜋Var,{𝑣 } ∈ 𝜙Var,{𝑣 } = 𝜙𝑝 (𝑣),{𝑣 }.

𝑐
(cid:74)

(cid:75)

Next, suppose 𝑣 ≡ like. Then, 𝑝 (𝑣) =

like × pdfN(𝑟 ; 𝑒1, 𝑒2)
(cid:76)

(cid:77)

♯ and

𝑔(𝜎) = [𝑣 ↦→ (𝜎 ⊕ 𝜏)(like) ·

distN (𝑒1, 𝑒2)

(𝜎 ⊕ 𝜏)(𝑟 )]

= [𝑣 ↦→

like × pdfN(𝑟 ; 𝑒1; 𝑒2)

(cid:74)

(cid:74)

(cid:75)
(𝜎 ⊕ 𝜏)]

(cid:75)

for all 𝜎 ∈ St[𝑝 (𝑣)]. Here the first equality is by the definition of
, and the second equality holds
because pdfN is the density function of a normal distribution. Hence, by Assumption 2, we have
□
𝑔 ∈ 𝜙𝑝 (𝑣),{𝑣 }.

𝑐
(cid:74)

(cid:75)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:71

F.5 Proofs of Lemmas for Theorem F.2
Here are the lemmas used to prove Theorem F.2:

Lemma F.3 (Weakening; Δ). Let 𝑓 ∈ [St → St⊥] and 𝐾, 𝐾 ′, 𝐿, 𝐿′ ⊆ Var. Then,

|= Δ(𝑓 , 𝐾, 𝐿) ∧ (𝐾 ⊆ 𝐾 ′) ∧ (𝐿 ⊇ 𝐿′) =⇒ |= Δ(𝑓 , 𝐾 ′, 𝐿′).

Proof. Consider 𝜎, 𝜎 ′ ∈ St with 𝜎 ∼𝐾 ′ 𝜎 ′. Then, 𝜎 ∼𝐾 𝜎 ′ because 𝐾 ⊆ 𝐾 ′. Since |= Δ(𝑓 , 𝐾, 𝐿),

(𝑓 (𝜎) ∈ St ⇐⇒ 𝑓 (𝜎 ′) ∈ St) and (𝑓 (𝜎) ∈ St =⇒ 𝑓 (𝜎) ∼𝐿 𝑓 (𝜎 ′)).

Note that the conclusion of the second conjunct implies 𝑓 (𝜎) ∼𝐿′ 𝑓 (𝜎 ′) since 𝐿′ ⊆ 𝐿. From what
□
we have just shown, the desired conclusion |= Δ(𝑓 , 𝐾 ′, 𝐿′) follows.

Lemma F.4 (Weakening; Φ). Let 𝑓 ∈ [St → St⊥] and 𝐾, 𝐾 ′, 𝐿, 𝐿′ ⊆ Var. Then,

|= Φ(𝑓 , 𝐾, 𝐿) ∧ (𝐾 ⊇ 𝐾 ′) ∧ (𝐿 ⊇ 𝐿′) =⇒ |= Φ(𝑓 , 𝐾 ′, 𝐿′).

Proof. We prove the lemma using Assumptions 3, 5 and 6. Consider 𝜏 ∈ St[(𝐾 ′)𝑐 ], and let 𝑔 be

the following partial function:

𝑔 : St[𝐾 ′] ⇀ St[𝐿′],

(cid:40)

𝑔(𝜎 ′) ≜

(𝜋Var,𝐿′ ◦ 𝑓 )(𝜎 ′ ⊕ 𝜏)
undefined

if 𝑓 (𝜎 ′ ⊕ 𝜏) ∈ St
otherwise.

We should show𝑔 ∈ 𝜙𝐾 ′,𝐿′. Note that (𝐾 ′)𝑐 ⊇ 𝐾𝑐 . Thus, there exist𝜏1 ∈ St[(𝐾 ′)𝑐 \𝐾𝑐 ] and𝜏2 ∈ St[𝐾𝑐 ]
such that 𝜏 = 𝜏1 ⊕ 𝜏2. Define a partial function ℎ : St[𝐾] → St[𝐿] by
(cid:40)

ℎ(𝜎 ′′) ≜

(𝜋Var,𝐿 ◦ 𝑓 )(𝜎 ′′ ⊕ 𝜏2)
undefined

if 𝑓 (𝜎 ′′ ⊕ 𝜏2) ∈ St
otherwise.

Then, since |= Φ(𝑓 , 𝐾, 𝐿), we have ℎ ∈ 𝜙𝐾,𝐿. Note that for all 𝜎 ′ ∈ St[𝐾 ′],

𝑔(𝜎 ′) = (𝜋𝐿,𝐿′ ◦ ℎ)(𝜎 ′ ⊕ 𝜏1).

By Assumptions 3, 5 and 6, the above equation implies 𝑔 ∈ 𝜙𝐾 ′,𝐿′, as desired.

□

Lemma F.5 (Merging; Δ). Let 𝑓 ∈ [St → St⊥] and 𝐾, 𝐾 ′, 𝐿, 𝐿′ ⊆ Var. Then,

|= Δ(𝑓 , 𝐾, 𝐿) ∧ |= Δ(𝑓 , 𝐾 ′, 𝐿′) =⇒ |= Δ(𝑓 , 𝐾 ∪ 𝐾 ′, 𝐿 ∪ 𝐿′).

Proof. Consider 𝜎, 𝜎 ′ ∈ St with 𝜎 ∼𝐾∪𝐾 ′ 𝜎 ′. Then, 𝜎 ∼𝐾 𝜎 ′, and by the assumption that

|= Δ(𝑓 , 𝐾, 𝐿), we have

𝑓 (𝜎) ∈ St ⇐⇒ 𝑓 (𝜎 ′) ∈ St.
It remains to show that if 𝑓 (𝜎), 𝑓 (𝜎 ′) ∈ St, then 𝑓 (𝜎) ∼𝐿∪𝐿′ 𝑓 (𝜎 ′). Assume 𝑓 (𝜎), 𝑓 (𝜎 ′) ∈ St. Since
𝜎 ∼𝐾∪𝐾 ′ 𝜎 ′ and we have |= Δ(𝑓 , 𝐾, 𝐿) and |= Δ(𝑓 , 𝐾 ′, 𝐿′) by assumption,

𝑓 (𝜎) ∼𝐿 𝑓 (𝜎 ′) and 𝑓 (𝜎) ∼𝐿′ 𝑓 (𝜎 ′).

This implies that 𝑓 (𝜎) ∼𝐿∪𝐿′ 𝑓 (𝜎 ′), as desired.

□

Lemma F.6 (Merging; Φ). Let 𝑓 ∈ [St → St⊥] and 𝐾, 𝐾 ′, 𝐿, 𝐿′ ⊆ Var. Then,

|= Φ(𝑓 , 𝐾, 𝐿) ∧ |= Φ(𝑓 , 𝐾 ′, 𝐿′) =⇒ |= Φ(𝑓 , 𝐾 ∩ 𝐾 ′, 𝐿 ∪ 𝐿′).

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:72

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Proof. Uses the weakening lemma for Φ (Lemma F.4), we have

|= Φ(𝑓 , 𝐾 ∩ 𝐾 ′, 𝐿) and |= Φ(𝑓 , 𝐾 ∩ 𝐾 ′, 𝐿′).
This and Assumption 4 then imply the desired conclusion. Concretely, for all 𝜏 ∈ St[(𝐾 ∩ 𝐾 ′)𝑐 ], if
𝑔, 𝑔1, and 𝑔2 are the following partial functions

𝑔 : St[𝐾 ∩ 𝐾 ′] ⇀ St[𝐿 ∪ 𝐿′],

𝑔(𝜎 ′) ≜

𝑔1 : St[𝐾 ∩ 𝐾 ′] ⇀ St[𝐿],

𝑔1(𝜎 ′) ≜

𝑔2 : St[𝐾 ∩ 𝐾 ′] ⇀ St[𝐿′],

𝑔2(𝜎 ′) ≜

(cid:40)

(cid:40)

(cid:40)

(𝜋Var,𝐿∪𝐿′ ◦ 𝑓 )(𝜎 ′ ⊕ 𝜏)
undefined

if 𝑓 (𝜎 ′ ⊕ 𝜏) ∈ St
otherwise,

(𝜋Var,𝐿 ◦ 𝑓 )(𝜎 ′ ⊕ 𝜏)
undefined

if 𝑓 (𝜎 ′ ⊕ 𝜏) ∈ St
otherwise,

(𝜋Var,𝐿′ ◦ 𝑓 )(𝜎 ′ ⊕ 𝜏)
undefined

if 𝑓 (𝜎 ′ ⊕ 𝜏) ∈ St
otherwise,

then 𝑔1 ∈ 𝜙𝐾∩𝐾 ′,𝐿, 𝑔2 ∈ 𝜙𝐾∩𝐾 ′,𝐿′, and 𝑔 = ⟨𝑔1, 𝑔2⟩, so that by Assumption 4, we have 𝑔 ∈ 𝜙𝐾∩𝐾 ′,𝐿∪𝐿′
□
as desired.

Lemma F.7 (Seqence). Let 𝑓 , 𝑔 ∈ [St → St⊥] and 𝐾, 𝐿, 𝐿′, 𝑀 ⊆ Var. Then,
|= Φ(𝑓 , 𝐾, 𝐿) ∧ |= Φ(𝑔, 𝐿, 𝑀) ∧ |= Δ(𝑓 , 𝐾𝑐, 𝐿′ \ 𝐿) ∧ |= Δ(𝑔, 𝐿′, 𝑀) =⇒ |= Φ(seq(𝑓 , 𝑔), 𝐾, 𝑀).
Proof. Consider 𝑓 , 𝑔 ∈ [St → St⊥] and 𝐾, 𝐿, 𝐿′, 𝑀 ⊆ Var that satisfy the given conditions:

|= Φ(𝑓 , 𝐾, 𝐿),

|= Δ(𝑔, 𝐿′, 𝑀).
To prove the conclusion, pick an arbitrary 𝜏 ∈ St[𝐾𝑐 ]. We have to show ℎ ∈ 𝜙𝐾,𝑀 , where

|= Φ(𝑔, 𝐿, 𝑀),

|= Δ(𝑓 , 𝐾𝑐, 𝐿′ \ 𝐿),

and

ℎ(𝜎) =

(cid:40)𝜋Var,𝑀 ((𝑔† ◦ 𝑓 )(𝜎 ⊕ 𝜏))
undefined

if (𝑔† ◦ 𝑓 )(𝜎 ⊕ 𝜏) ∈ St
otherwise.

Observe that since |= Φ(𝑓 , 𝐾, 𝐿) and |= Φ(𝑔, 𝐿, 𝑀), we have ℎ1 ∈ 𝜙𝐾,𝐿 and ℎ2 ∈ 𝜙𝐿,𝑀 for any
𝜏1 ∈ St[𝐾𝑐 ] and 𝜏2 ∈ St[𝐿𝑐 ], where ℎ1 and ℎ2 are parameterised by 𝜏1 and 𝜏2, and defined by

ℎ1 : St[𝐾] ⇀ St[𝐿],

ℎ1(𝜎) ≜

ℎ2 : St[𝐿] ⇀ St[𝑀],

ℎ2(𝜎) ≜

(cid:40)𝜋Var,𝐿 (𝑓 (𝜎 ⊕ 𝜏1))
undefined
(cid:40)𝜋Var,𝑀 (𝑔(𝜎 ⊕ 𝜏2))
undefined

if 𝑓 (𝜎 ⊕ 𝜏1) ∈ St
otherwise,

if 𝑔(𝜎 ⊕ 𝜏2) ∈ St
otherwise.

Given these, it suffices to show the claim that ℎ = ℎ2 ◦ ℎ1 for some 𝜏1 and 𝜏2: if the claim holds, then
we have ℎ = ℎ2 ◦ ℎ1 ∈ 𝜙𝐾,𝑀 by Assumption 6, since ℎ1 ∈ 𝜙𝐾,𝐿 and ℎ2 ∈ 𝜙𝐿,𝑀 . We prove the claim by
case analysis on 𝑓 (− ⊕ 𝜏).

Case 𝑓 (𝜎 ⊕ 𝜏) ∉ St for all 𝜎 ∈ St[𝐾]. In this case, we set 𝜏1 ≜ 𝜏 and pick any 𝜏2 ∈ St[𝐿𝑐 ]. Then,
for all 𝜎 ∈ St[𝐾], ℎ(𝜎) and (ℎ2 ◦ ℎ1)(𝜎) are both undefined, as desired. Note that the latter term is
undefined since ℎ1(𝜎) is undefined.

Case 𝑓 (𝜎 ′ ⊕ 𝜏) ∈ St for some 𝜎 ′ ∈ St[𝐾]. In this case, we set 𝜏1 ≜ 𝜏 and 𝜏2 ≜ 𝜋Var,𝐿𝑐 (𝑓 (𝜎 ′ ⊕ 𝜏)).
To show ℎ = ℎ2 ◦ ℎ1, consider any 𝜎 ∈ St[𝐾]. If 𝑓 (𝜎 ⊕ 𝜏) ∉ St, then by the same argument for the
above case, ℎ(𝜎) and (ℎ2 ◦ ℎ1)(𝜎) are both undefined. So, assume that 𝑓 (𝜎 ⊕ 𝜏) ∈ St. Then,

ℎ(𝜎) =

(cid:40)𝜋Var,𝑀 (𝑔(𝜎1))
undefined

if 𝑔(𝜎1) ∈ St
otherwise,

(ℎ2 ◦ ℎ1)(𝜎) =

(cid:40)𝜋Var,𝑀 (𝑔(𝜎2))
undefined

if 𝑔(𝜎2) ∈ St
otherwise,

(28)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:73

where

𝜎1 = 𝑓 (𝜎 ⊕ 𝜏) ∈ St,

𝜎2 = 𝜋Var,𝐿 (𝑓 (𝜎 ⊕ 𝜏)) ⊕ 𝜋Var,𝐿𝑐 (𝑓 (𝜎 ′ ⊕ 𝜏)) ∈ St.

Our goal is to show that ℎ(𝜎) and (ℎ2 ◦ ℎ1)(𝜎) are both undefined, or they are both defined and
are the same. By |= Δ(𝑔, 𝐿′, 𝑀) and Eq. (28), it suffices to show 𝜎1 ∼𝐿′ 𝜎2. To prove this, we show
a stronger statement: 𝜎1 ∼𝐿 𝜎2 and 𝜎1 ∼𝐿′\𝐿 𝜎2. The former relation holds since 𝜋Var,𝐿 (𝜎1) =
𝜋Var,𝐿 (𝑓 (𝜎 ⊕ 𝜏)) = 𝜋Var,𝐿 (𝜎2). The latter relation is equivalent to 𝑓 (𝜎 ⊕ 𝜏) ∼𝐿′\𝐿 𝑓 (𝜎 ′ ⊕ 𝜏), and this
holds by |= Δ(𝑓 , 𝐾𝑐, 𝐿′ \ 𝐿) and 𝜎 ⊕ 𝜏 ∼𝐾𝑐 𝜎 ′ ⊕ 𝜏. Hence, ℎ = ℎ2 ◦ ℎ1 as desired.
□

Lemma F.8 (Conditional). Let 𝑓 , 𝑓 ′ ∈ [St → St⊥] and 𝐾, 𝐿 ⊆ Var. Then, for any boolean expres-

sion 𝑏,

|= Φ(𝑓 , 𝐾, 𝐿) ∧ |= Φ(𝑓 ′, 𝐾, 𝐿) ∧ (𝐾𝑐 ⊇ fv(𝑏)) =⇒ |= Φ(cond (

𝑏
(cid:74)

, 𝑓 , 𝑓 ′), 𝐾, 𝐿).
(cid:75)

Proof. Let 𝑓 , 𝑓 ′, 𝐾, 𝐿, and 𝑏 be the functions, sets and a boolean expression such that

|= Φ(𝑓 , 𝐾, 𝐿),

|= Φ(𝑓 ′, 𝐾, 𝐿),

and 𝐾𝑐 ⊇ fv(𝑏).

Consider 𝜏 ∈ St[𝐾𝑐 ]. Define 𝑓 ′′ ≜ cond (

𝑏
(cid:74)

, 𝑓 , 𝑓 ′), and also partial functions 𝑔, 𝑔′, and 𝑔′′ as follows:
(cid:75)

𝑔 : St[𝐾] ⇀ St[𝐿],

𝑔(𝜎) ≜

𝑔′ : St[𝐾] ⇀ St[𝐿],

𝑔′(𝜎) ≜

𝑔′′ : St[𝐾] ⇀ St[𝐿],

𝑔′′(𝜎) ≜

(cid:40)

(𝜋Var,𝐿 ◦ 𝑓 )(𝜎 ⊕ 𝜏)
undefined

if 𝑓 (𝜎 ⊕ 𝜏) ∈ St
otherwise,

(cid:40)

(cid:40)

(𝜋Var,𝐿 ◦ 𝑓 ′)(𝜎 ⊕ 𝜏)
undefined

if 𝑓 ′(𝜎 ⊕ 𝜏) ∈ St
otherwise,

(𝜋Var,𝐿 ◦ 𝑓 )(𝜎 ⊕ 𝜏)
undefined

if 𝑓 (𝜎 ⊕ 𝜏) ∈ St
otherwise,

We should show 𝑔′′ ∈ 𝜙𝐾,𝐿. Since 𝐾𝑐 ⊇ fv(𝑏), either
𝑏
(cid:74)
both 𝑔 and 𝑔′ are in 𝜙𝐾,𝐿, we have the desired 𝑔′′ ∈ 𝜙𝐾,𝐿 in both cases.

(𝜎 ⊕ 𝜏) = true for all 𝜎 ∈ St[𝐾] or
(𝜎 ⊕ 𝜏) = false for all 𝜎 ∈ St[𝐾]. In the former case, 𝑔′′ = 𝑔, and in the latter case, 𝑔′′ = 𝑔′. Since
□

𝑏
(cid:74)

(cid:75)

(cid:75)

Lemma F.9 (Loop; base). Let 𝐾, 𝐿 ⊆ Var. Then,

|= Φ((𝜆𝜎 ∈ St. ⊥), 𝐾, 𝐿).

Proof. Consider 𝜏 ∈ St[𝐾𝑐 ]. Define a partial function 𝑔 : St[𝐾] ⇀ St[𝐿] by

𝑔(𝜎) ≜

(cid:40)

(𝜋Var,𝐿 ◦ (𝜆𝜎 ∈ St.⊥))(𝜎)
undefined
= undefined.

if (𝜆𝜎 ∈ St.⊥)(𝜎) ∈ St
otherwise

Then, 𝑔 ∈ 𝜙𝐾,𝐿 by Assumption 7.

□

Lemma F.10 (Loop; limit). Let 𝐾, 𝐿 ⊆ Var and {𝑓𝑛 ∈ [St → St⊥]}𝑛 ∈N be an 𝜔-chain (i.e., 𝑓𝑛 ⊑ 𝑓𝑛+1
for all 𝑛 ∈ N). Here we write 𝑓 ⊑ 𝑔 if 𝑓 (𝜎) ⊑ 𝑔(𝜎) for all 𝜎 ∈ St. Suppose that for any 𝜏 ∈ St[𝐾𝑐 ] and
𝑛 ∈ N, the set {𝜎 ∈ St[𝐾] | 𝑓𝑛 (𝜎 ⊕ 𝜏) ∈ St} is either ∅ or St[𝐾]. Then,

(cid:219)

𝑛 ∈N

|= Φ(𝑓𝑛, 𝐾, 𝐿) =⇒ |= Φ(

(cid:196)

𝑓𝑛, 𝐾, 𝐿).

𝑛 ∈N

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:74

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Proof. Consider an 𝜔-chain {𝑓𝑛 ∈ [St → St⊥]}𝑛 ∈N such that

|= Φ(𝑓𝑛, 𝐾, 𝐿) for all 𝑛. Pick an
arbitrary 𝜏 ∈ St[𝐾𝑐 ]. Let 𝑓∞ ≜ (cid:195)𝑛 ∈N 𝑓𝑛, and define partial functions 𝑔∞ and 𝑔𝑛 for all 𝑛 as follows:
(cid:40)

𝑔∞ : St[𝐾] ⇀ St[𝐿],

𝑔∞(𝜎) ≜

(𝜋Var,𝐿 ◦ 𝑓∞)(𝜎 ⊕ 𝜏)
undefined

if 𝑓∞ (𝜎 ⊕ 𝜏) ∈ St
otherwise,

𝑔𝑛 : St[𝐾] ⇀ St[𝐿],

𝑔𝑛 (𝜎) ≜

(cid:40)

(𝜋Var,𝐿 ◦ 𝑓𝑛)(𝜎 ⊕ 𝜏)
undefined

if 𝑓𝑛 (𝜎 ⊕ 𝜏) ∈ St
otherwise.

Then, {𝑔𝑛 }𝑛 ∈N is an 𝜔-chain when we order 𝑔𝑛’s by graph inclusion, and 𝑔∞ is the least upper bound
of this chain for the same order. We will show that 𝑔∞ = 𝑔𝑛 for some 𝑛 ∈ N by case analysis on
Σ𝑛 ≜ {𝜎 ∈ St[𝐾] | 𝑓𝑛 (𝜎 ⊕ 𝜏) ∈ St}. Note that this implies the desired 𝑔∞ ∈ 𝜙𝐾,𝐿 because 𝑔𝑛 ∈ 𝜙𝐾,𝐿
for all 𝑛 ∈ N. If Σ𝑛 = ∅ for all 𝑛 ∈ N, then 𝑔 = 𝑔𝑛 for any 𝑛, since both 𝑔 and 𝑔𝑛 are the same empty
partial function. Otherwise, by the assumption of the lemma, Σ𝑛 = St[𝐾] for some 𝑛. This means
that 𝑔𝑛 is the total function, and so 𝑔𝑛 = 𝑔𝑚 for all 𝑚 ≥ 𝑛, which implies that 𝑔 = 𝑔𝑛, as desired. □

G DEFERRED RESULTS IN §5.3

G.1 Proof of Theorem 5.9

Proof of Theorem 5.9. We go through the assumptions, and show that they are satisfied by 𝜙 (𝑑)

and 𝜙 (𝑙) .

Case of Assumption 3. Let 𝐾, 𝐿 ⊆ Var such that 𝐿 ⊆ 𝐾. The projection 𝜋𝐾,𝐿 is total and has
an open set as its domain. Furthermore, the projection 𝜋𝐾,𝐿 is differentiable and 1-Lipschitz contin-
uous. Since Lipschitz continuity implies local Lipschitz continuity, we have both 𝜋𝐾,𝐿 ∈ 𝜙 (𝑑)
𝐾,𝐿 and
𝜋𝐾,𝐿 ∈ 𝜙 (𝑙)

𝐾,𝐿, as desired.

Case of Assumption 4. Let 𝐾, 𝐿0, 𝐿1 ⊆ Var such that 𝐿0 ∩ 𝐿1 = ∅. Consider 𝑓0, 𝑔0 ∈ [St[𝐾] ⇀

St[𝐿0]] and 𝑓1, 𝑔1 ∈ [St[𝐾] ⇀ St[𝐿1]] such that all of the following hold:

𝑓0 ∈ 𝜙 (𝑑)
𝐾,𝐿0

,

𝑓1 ∈ 𝜙 (𝑑)
𝐾,𝐿1

,

𝑔0 ∈ 𝜙 (𝑙)
𝐾,𝐿0

,

and

𝑔1 ∈ 𝜙 (𝑙)
𝐾,𝐿1

.

Let

𝐿 ≜ 𝐿0 ∪ 𝐿1;

𝑓 : St[𝐾] ⇀ St[𝐿],

𝑓 (𝜎) ≜

𝑔 : St[𝐾] ⇀ St[𝐿],

𝑔(𝜎) ≜

(cid:40)𝑓0(𝜎) ⊕ 𝑓1(𝜎)
undefined
(cid:40)𝑔0(𝜎) ⊕ 𝑔1(𝜎)
undefined

if 𝜎 ∈ dom(𝑓0) ∩ dom(𝑓1),
otherwise;

if 𝜎 ∈ dom(𝑔0) ∩ dom(𝑔1),
otherwise.

We should show that 𝑓 and 𝑔 satisfy 𝜙 (𝑑)
are the intersections of two open sets, so that they are open as required.

𝐾,𝐿 and 𝜙 (𝑙)

𝐾,𝐿, respectively. In both cases, dom(𝑓 ) and dom(𝑔)

To prove the differentiability of 𝑓 , consider 𝜎 ∈ dom(𝑓 ). Let ℎ0 and ℎ1 be the linear functions in
[St[𝐾] ⇀ St[𝐿0]] and [St[𝐾] ⇀ St[𝐿1]], respectively, such that their domains are open and contain
0, and for all 𝑖 ∈ {0, 1},

lim
𝜎′→0

∥ 𝑓𝑖 (𝜎 + 𝜎 ′) − 𝑓𝑖 (𝜎) − ℎ𝑖 (𝜎 ′)∥2
∥𝜎 ′∥2

= 0.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

= lim
𝜎′→0

= lim
𝜎′→0
√︄(cid:18)

=

Smoothness Analysis and Selective Reparameterisation

0:75

Let ℎ be the linear function in [St[𝐾] ⇀ St[𝐿]] defined by

ℎ(𝜎) ≜

(cid:40)ℎ0(𝜎) ⊕ ℎ1(𝜎)
undefined

if 𝜎 ∈ dom(ℎ0) ∩ dom(ℎ1);
otherwise.

Then, dom(ℎ) is open and contains 𝜎. Furthermore,

lim
𝜎′→0

∥𝑓 (𝜎 + 𝜎 ′) − 𝑓 (𝜎) − ℎ(𝜎 ′)∥2
∥𝜎 ′∥2

√︃

∥ 𝑓0(𝜎 + 𝜎 ′) − 𝑓0(𝜎) − ℎ0(𝜎 ′)∥2

2 + ∥ 𝑓1(𝜎 + 𝜎 ′) − 𝑓1(𝜎) − ℎ1(𝜎 ′)∥2
2
∥𝜎 ′∥2
√︄(cid:18) ∥𝑓0(𝜎 + 𝜎 ′) − 𝑓0(𝜎) − ℎ0(𝜎 ′)∥2
(cid:19) 2
∥𝜎 ′∥2

(cid:18) ∥𝑓1(𝜎 + 𝜎 ′) − 𝑓1(𝜎) − ℎ1(𝜎 ′)∥2
∥𝜎 ′∥2

+

(cid:19) 2

∥𝑓0(𝜎 + 𝜎 ′) − 𝑓0(𝜎) − ℎ0(𝜎 ′)∥2
∥𝜎 ′∥2

(cid:19) 2

(cid:18)

+

lim
𝜎′→0

∥𝑓1(𝜎 + 𝜎 ′) − 𝑓1(𝜎) − ℎ1(𝜎 ′)∥2
∥𝜎 ′∥2

(cid:19) 2

lim
𝜎′→0

= 0.

Thus, 𝑓 is differentiable at 𝜎, as desired.

It remains to prove the local Lipschitzness of 𝑔. Pick 𝜎 ∈ dom(𝑔). Then, 𝑔0 and 𝑔1 are defined at
𝜎 and they are locally Lipschitz. Thus, there exist open sets 𝑂0 ⊆ dom(𝑔0) and 𝑂1 ⊆ dom(𝑔1) and
constants 𝐵0, 𝐵1 > 0 such that 𝜎 belongs to both 𝑂0 and 𝑂1, and for all 𝜎0, 𝜎 ′
1 ∈ 𝑂1,

0 ∈ 𝑂0 and 𝜎1, 𝜎 ′

∥𝑔0 (𝜎0) − 𝑔0(𝜎 ′

0∥2
Let 𝑂 ≜ 𝑂0 ∩ 𝑂1. The set 𝑂 is open, and contains 𝜎. Furthermore, for all 𝜎 ′, 𝜎 ′′ ∈ 𝑂,

0)∥2 ≤ 𝐵0∥𝜎0 − 𝜎 ′

∥𝑔1(𝜎1) − 𝑔1(𝜎 ′

1)∥2 ≤ 𝐵1∥𝜎1 − 𝜎 ′

and

1∥2.

∥𝑔(𝜎 ′) − 𝑔(𝜎 ′′)∥2 =

√︃

∥𝑔0(𝜎 ′) − 𝑔0(𝜎 ′′)∥2

2 + ∥𝑔1(𝜎 ′) − 𝑔1(𝜎 ′′)∥2
2

≤

=

Thus, 𝑔 is Lipschitz in 𝑂, as desired.

√︃

0 ∥𝜎 ′ − 𝜎 ′′∥2
𝐵2

2 + 𝐵2

1 ∥𝜎 ′ − 𝜎 ′′∥2

2

√︃

𝐵2
0 + 𝐵2

1 · ∥𝜎 ′ − 𝜎 ′′∥2.

Case of Assumption 5. Consider 𝐾, 𝐾 ′, 𝐿 ⊆ Var with 𝐾 ⊆ 𝐾 ′, and 𝜏 ∈ St[𝐾 ′ \ 𝐾]. Let 𝑓 and 𝑔

be partial functions in [St[𝐾 ′] ⇀ St[𝐿]] such that 𝑓 ∈ 𝜙 (𝑑)

𝐾 ′,𝐿 and 𝑔 ∈ 𝜙 (𝑙)

𝐾 ′,𝐿. Let

𝑓1 : St[𝐾] ⇀ St[𝐿],

𝑓1(𝜎) ≜

𝑔1 : St[𝐾] ⇀ St[𝐿],

𝑔1(𝜎) ≜

if 𝜎 ⊕ 𝜏 ∈ dom(𝑓 )

(cid:40)𝑓 (𝜎 ⊕ 𝜏)
undefined otherwise,
(cid:40)𝑔(𝜎 ⊕ 𝜏)
undefined otherwise.

if 𝜎 ⊕ 𝜏 ∈ dom(𝑔)

We should show that 𝑓1 and 𝑔1 satisfy 𝜙 (𝑑)

𝐾,𝐿 and 𝜙 (𝑙)

dom(𝑓1) = {𝜎 | 𝜎 ⊕ 𝜏 ∈ dom(𝑓 )}

𝐾,𝐿, respectively. Note that
and dom(𝑔1) = {𝜎 | 𝜎 ⊕ 𝜏 ∈ dom(𝑔)}.

These two sets are open since dom(𝑓 ) and dom(𝑔) are open and for any open 𝑂, the slice {𝜎 ∈ St[𝐾] |
𝜎 ⊕ 𝜏 ∈ 𝑂 } is open. Let 𝜎0 ∈ dom(𝑓1) and 𝜎1 ∈ dom(𝑔1). We will show that 𝑓1 is differentiable at 𝜎0,
and 𝑔1 is Lipschitz in an open neighbourhood of 𝜎1.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:76

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Since 𝑓 is differentiable and 𝜎0 ⊕ 𝜏 ∈ dom(𝑓 ), there exists a linear map ℎ : St[𝐾 ′] ⇀ St[𝐿] such

that dom(ℎ) is open and contains 0, and

lim
𝜎′→0

∥𝑓 (𝜎0 ⊕ 𝜏 + 𝜎 ′) − 𝑓 (𝜎0 ⊕ 𝜏) − ℎ(𝜎 ′)∥2
∥𝜎 ′∥2

= 0.

Let 𝜏0 ≜ 𝜆𝑣 ∈ 𝐾 ′ \ 𝐾 . 0, and ℎ1 be the partial function from St[𝐾] to St[𝐿] defined by
(cid:40)ℎ(𝜎 ⊕ 𝜏0)
undefined otherwise.

if 𝜎 ⊕ 𝜏0 ∈ dom(ℎ),

ℎ1(𝜎) ≜

Then, ℎ1 is linear, its domain is open (since taking a slice of an open set in St[𝐾] (cid:27) R|𝐾 ′ | by fixing
some coordinate variables gives an open set), and

lim
𝜎′′→0

∥𝑓1(𝜎0 + 𝜎 ′′) − 𝑓1(𝜎0) − ℎ1(𝜎 ′′)∥2
∥𝜎 ′′∥2

= lim
𝜎′′→0

= 0.

This means that 𝑓1 is differentiable at 𝜎0.

∥𝑓 (𝜎0 ⊕ 𝜏 + 𝜎 ′′ ⊕ 𝜏0) − 𝑓 (𝜎0 ⊕ 𝜏) − ℎ(𝜎 ′′ ⊕ 𝜏0)∥2
∥𝜎 ′′ ⊕ 𝜏0∥2

Since 𝑔 is locally Lipschitz and 𝜎1 ⊕ 𝜏 ∈ dom(𝑔), there exists an open subset 𝑂 of dom(𝑔) such

that 𝑂 contains 𝜎1 ⊕ 𝜏 and 𝑔 is Lipschitz in 𝑂, that is, there exists a real number 𝐵 > 0 such that

for all 𝜎, 𝜎 ′ ∈ 𝑂. Let

∥𝑔(𝜎) − 𝑔(𝜎 ′)∥2 ≤ 𝐵 · ∥𝜎 − 𝜎 ′∥2

𝑂 ′ ≜ {𝜎 ∈ St[𝐾] | 𝜎 ⊕ 𝜏 ∈ 𝑂 }.

Then, 𝑂 ′ is open, and it contains 𝜎1. Furthermore, for all 𝜎, 𝜎 ′ ∈ 𝑂 ′,

∥𝑔1(𝜎) − 𝑔1(𝜎 ′)∥2 = ∥𝑔(𝜎 ⊕ 𝜏) − 𝑔(𝜎 ′ ⊕ 𝜏)∥2 ≤ 𝐵 · ∥𝜎 ⊕ 𝜏 − 𝜎 ′ ⊕ 𝜏 ∥2 = 𝐵 · ∥𝜎 − 𝜎 ′∥2.

Thus, 𝑔1 is Lipschitz in 𝑂 ′, as desired.

Case of Assumption 6. For the composition condition, we handle the differentiability case only.

The other case can be proved similarly. Consider

𝐾, 𝐿, 𝑀 ⊆ Var,

𝑓 ∈ [St[𝐾] ⇀ St[𝐿]],

and 𝑔 ∈ [St[𝐿] ⇀ St[𝑀]].

Assume that 𝑓 ∈ 𝜙 (𝑑)
We should show that ℎ ∈ 𝜙 (𝑑)
Note that

𝐾,𝐿 and 𝑔 ∈ 𝜙 (𝑑)

𝐿,𝑀 . Let ℎ be the standard composition of partial functions 𝑔 and 𝑓 .
𝐾,𝑀 as well, that is, dom(ℎ) is open and ℎ is differentiable on its domain.

dom(ℎ) = dom(𝑓 ) ∩ 𝑓 −1(dom(𝑔)).

𝐿,𝑀 , the set dom(𝑔) is open. Because 𝑓 ∈ 𝜙 (𝑑)

Since 𝑔 ∈ 𝜙 (𝑑)
𝐾,𝐿, dom(𝑓 ) is open and 𝑓 is continuous on its
domain. The latter implies that 𝑓 −1(dom(𝑔)) is open as well. Thus, the intersection of dom(𝑓 ) and
𝑓 −1(dom(𝑔)) is open as desired. The differentiability of ℎ on its domain holds since the restriction of
𝑓 to dom(ℎ) gives a differentiable total function from dom(ℎ) to dom(𝑔), that of 𝑔 to dom(𝑔) is also
a differentiable total function, and the composition of two differentiable functions is differentiable.

Case of Assumption 7. The empty set is open, and the empty function is jointly differentiable and
locally Lipschitz continuous. Thus, the strictness assumption holds for both predicate families. □

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:77

H DEFERRED RESULTS IN §6

H.1 Proof of Theorem 6.2

Proof of Theorem 6.2. Suppose that the algorithm returns 𝜋 (without an error message). We

should show the conclusion that 𝜋 is simple and satisfies (R2) and (R3).
We make several observations before proving the conclusion. Let

(p𝑚, d𝑚, V𝑚) ≜

♯,

(p𝑔, d𝑔, V𝑔) ≜

♯,

(p𝑔, d𝑔, V𝑔) ≜

𝑐𝑚
(cid:74)

(cid:75)

𝑐𝑔
(cid:74)

(cid:75)

𝑐𝑔
(cid:74)

𝜋

♯,

(cid:75)

and 𝐾 ⊆ Var be the set defined in Eq. (13). Also, let 𝑆𝑟 be the set of names that the algorithm uses to
construct the returned reparameterisation plan 𝜋. Then, by the algorithm, we have the two inclusions
in Eq. (13) and Eq. (14), and also 𝜋 = 𝜋0 [𝑆𝑟 ]. In addition, 𝑆𝑟 ⊆ 𝐾 since

𝑆𝑟 ⊆ {(𝛼, 𝑖) ∈ Name | for all 𝑖 ′ ∈ N, (𝛼, 𝑖 ′) ∈ Name =⇒ (𝛼, 𝑖 ′) ∈ 𝐾 } ⊆ 𝐾 .

We now prove the conclusion in three parts.

First part: We show that 𝜋 is simple. To show this, consider (𝑛, 𝑑, 𝑙), (𝑛′, 𝑑 ′, 𝑙 ′) ∈ NameEx ×
DistEx×LamEx such that𝑛 = name(𝛼, 𝑒) and𝑛′ = name(𝛼, 𝑒 ′) for some 𝛼 ∈ Str,𝑒, and𝑒 ′. Suppose that
(𝑛, 𝑑, 𝑙) ∈ dom(𝜋). We should show (𝑛′, 𝑑 ′, 𝑙 ′) ∈ dom(𝜋). Since (𝑛, 𝑑, 𝑙) ∈ dom(𝜋) = dom(𝜋0 [𝑆𝑟 ]),
we have (𝑛, 𝑑, 𝑙) ∈ dom(𝜋0) and (𝛼, _) ∈ 𝑆𝑟 . This implies that (𝑛′, 𝑑 ′, 𝑙 ′) ∈ dom(𝜋0) because 𝜋0 is
simple. Since 𝑛′ = name(𝛼, _) and (𝛼, _) ∈ 𝑆𝑟 , we have (𝑛′, 𝑑 ′, 𝑙 ′) ∈ dom(𝜋) as desired.

Second part: We show that 𝜋 satisfies (R2) in three steps.
First step: We prove 𝜃 ∪ rv(𝜋) ⊆ 𝐾. To do so, observe that the 𝑆 in the algorithm always satisfies

the following property:

(𝛼, 𝑖) ∈ 𝑆 =⇒ (𝛼, 𝑖 ′) ∈ 𝑆

for any (𝛼, 𝑖), (𝛼, 𝑖 ′) ∈ Name.

(29)

We can prove this by induction: the initial 𝑆 (i.e., 𝑆 = {(𝛼, 𝑖) ∈ Name | for all 𝑖 ′ ∈ N, (𝛼, 𝑖 ′) ∈
Name =⇒ (𝛼, 𝑖 ′) ∈ 𝐾 }) satisfies the property, and each update of 𝑆 (i.e., 𝑆 ← 𝑆 \ {(𝛼, 𝑖) ∈ Name}
for some (𝛼, _) ∈ 𝑆) preserves the property. From this, we obtain rv(𝜋) = rv(𝜋0) ∩ 𝑆:

rv(𝜋) = rv(𝜋0 [𝑆])

= {(𝛼, 𝑖) ∈ Name | ∃𝑒, 𝑑, 𝑙 . (name(𝛼, 𝑒), 𝑑, 𝑙) ∈ dom(𝜋0 [𝑆])}
= (cid:8)(𝛼, 𝑖) ∈ Name | ∃𝑒, 𝑑, 𝑙 . (cid:0)(name(𝛼, 𝑒), 𝑑, 𝑙) ∈ dom(𝜋0) ∧ ∃𝑖 ′. (𝛼, 𝑖 ′) ∈ 𝑆 (cid:1)(cid:9)
= (cid:8)(𝛼, 𝑖) ∈ Name | (cid:0)∃𝑒, 𝑑, 𝑙 . (cid:0)(name(𝛼, 𝑒), 𝑑, 𝑙) ∈ dom(𝜋0)(cid:1) ∧ (cid:0)∃𝑖 ′. (𝛼, 𝑖 ′) ∈ 𝑆 (cid:1) (cid:9)
= {(𝛼, 𝑖) ∈ Name | (name(𝛼, _), _, _) ∈ dom(𝜋0)} ∩ {(𝛼, 𝑖) ∈ Name | (𝛼, _) ∈ 𝑆 }
= rv(𝜋0) ∩ 𝑆,

where the second and third equalities use the definitions of rv(−) and 𝜋0 [𝑆], respectively, and the
last equality uses Eq. (29). Since rv(𝜋) ⊆ 𝑆 ⊆ 𝐾 and 𝜃 ⊆ 𝐾 by Eq. (13) (the inclusion of 𝜃 ), we get
𝜃 ∪ rv(𝜋) ⊆ 𝐾 as desired.

Second step: We prove that for all 𝑢 ∈ {like} ∪ {pr 𝜇 | 𝜇 ∈ Name} and 𝑣 ∈ {pr 𝜇 | 𝜇 ∈ Name}, the
following functions (which are total since 𝑐𝑚 and 𝑐𝑔 always terminate) are differentiable with respect
to the variables in 𝜃 ∪ rv(𝜋) jointly:

(𝜎𝜃, 𝜎𝑛) ∈ St[𝜃 ] × St[Name] ↦−→
(𝜎𝜃, 𝜎𝑛) ∈ St[𝜃 ] × St[Name] ↦−→

𝑐𝑚
(cid:74)
𝑐𝑔
(cid:74)
where 𝜎𝑝\𝜃 ≜ (𝜆𝑣 ∈ PVar \ 𝜃 . 0) and the function 𝑔 : St[Name] → St[AVar] takes 𝜎𝑛 and returns
𝜎𝑎 such that 𝜎𝑎 maps like to 1, pr 𝜇 to N (𝜎𝑛 (𝜇); 0, 1), val𝜇 to 𝜎𝑛 (𝜇), and all the other variables to 0.
The state 𝜎𝑝\𝜃 ⊕ 𝑔(𝜎𝑛) is the very initialisation used in Eq. (3). This step consists of two substeps.

(𝜎𝑝\𝜃 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛 ⊕ 𝑔(𝜎𝑛))(𝑢),
(cid:75)
(𝜎𝑝\𝜃 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛 ⊕ 𝑔(𝜎𝑛))(𝑣),

(30)

(cid:75)

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:78

Wonyeol Lee, Xavier Rival, and Hongseok Yang

First substep: We first show that for all 𝑢 ∈ {like} ∪ {pr 𝜇 | 𝜇 ∈ Name} and 𝑣 ∈ {pr 𝜇 | 𝜇 ∈ Name},
the functions 𝑓𝑚, 𝑓𝑔 : St[𝜃 ] × St[Name] × St[AVar] → R are differentiable with respect to the
variables in 𝜃 ∪ rv(𝜋) jointly:

𝑓𝑚 (𝜎𝜃, 𝜎𝑛, 𝜎𝑎) ≜
𝑓𝑔 (𝜎𝜃, 𝜎𝑛, 𝜎𝑎) ≜

(𝜎𝑝\𝜃 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛 ⊕ 𝜎𝑎)(𝑢),
(𝜎𝑝\𝜃 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛 ⊕ 𝜎𝑎)(𝑣).

(31)

𝑐𝑚
(cid:74)
𝑐𝑔
(cid:74)

(cid:75)

(cid:75)

𝑐𝑔
(cid:74)

Note that in 𝑓𝑚 and 𝑓𝑔, the AVar part does not depend on the Name part (unlike in Eq. (30)). For the proof,
pick arbitrary 𝑢 ∈ {like} ∪ {pr 𝜇 | 𝜇 ∈ Name} and 𝑣 ∈ {pr 𝜇 | 𝜇 ∈ Name}. Then, 𝜃 ∪ rv(𝜋) ⊆ 𝐾 ⊆
p𝑚 (𝑢) ∩ p𝑔 (𝑣), where the first inclusion is from the above result and the second from Eq. (13) (the defi-
nition of 𝐾). By the soundness of differentiability analysis (Theorem 5.8),
, p𝑚 (𝑢), {𝑢}) and
(cid:75)
, p𝑔 (𝑣), {𝑣 }). From this, and by the weakening lemma of Φ with 𝜃 ∪ rv(𝜋) ⊆ p𝑚 (𝑢) ∩ p𝑔 (𝑣)
|= Φ(
(cid:75)
(Lemma F.4), we have |= Φ(
, 𝜃 ∪ rv(𝜋), {𝑣 }). Hence, the functions
(cid:75)
in Eq. (31) are differentiable with respect to 𝜃 ∪ rv(𝜋) jointly as desired, by the definition of Φ (§5.1)
and the definition of “𝑓 : St[𝐿] → R for 𝐿 ⊆ Var is differentiable with respect to 𝐿′ ⊆ 𝐿 jointly” (§4.2).
Second substep: We now prove that the claim of the second step follows from the first substep
just proved. Pick any 𝑢 ∈ {like} ∪ {pr 𝜇 | 𝜇 ∈ Name}. We should show that the first function in
Eq. (30) is differentiable with respect to 𝜃 ∪ rv(𝜋). Note that we should also show the same for the
second function (for any 𝑣), but the proof is similar to the first function so we omit this case. To
𝑛,0 ∈ St[Name \ rv(𝜋)] and 𝜎𝑎,0 ∈ St[AVar]. Define
prove the claim for the first function, pick any 𝜉 ′
𝑓 ′ : St[𝜃 ] × St[rv(𝜋)] × St[AVar] as

, 𝜃 ∪ rv(𝜋), {𝑢}) and |= Φ(
(cid:75)

𝑐𝑚
(cid:74)

𝑐𝑚
(cid:74)

𝑐𝑔
(cid:74)

|= Φ(

Then, by Lemma C.6-(2) and Lemma C.6-(3),

𝑓 ′(𝜎𝜃, 𝜉𝑛, 𝜎𝑎) ≜ 𝑓 (𝜎𝜃, 𝜉𝑛 ⊕ 𝜉 ′

𝑛,0, 𝜎𝑎).

𝑓 ′(𝜎𝜃, 𝜉𝑛, 𝜎𝑎) ≜

(cid:40)𝑓 (𝜎𝜃, 𝜉𝑛 ⊕ 𝜉 ′
proj(𝜎𝑎)

𝑛,0, 𝜎𝑎,0)

if (𝜎𝜃, 𝜉𝑛) ∈ 𝑈
if (𝜎𝜃, 𝜉𝑛) ∉ 𝑈

for some 𝑈 ⊆ St[𝜃 ] × St[rv(𝜋)] and some projection map 𝑝𝑟𝑜 𝑗 : St[AVar] → R. Also, since 𝑓 is
differentiable with respect to 𝜃 ∪ rv(𝜋), 𝑓 ′(−, −, 𝜎𝑎) : St[𝜃 ] × St[rv(𝜋)] is differentiable and thus
continuous for all 𝜎𝑎 ∈ St[AVar]. From these, Lemma H.1 is applicable to 𝑓 ′, implying that 𝑈 should
be either ∅ or St[𝜃 ] × St[rv(𝜋)]. We now consider 𝑓 ′′ : St[𝜃 ] × St[rv(𝜋)] → R defined by

𝑓 ′′(𝜎𝜃, 𝜉𝑛) ≜ 𝑓 ′(𝜎𝜃, 𝜉𝑛, 𝑔(𝜉𝑛) ⊕ 𝑔(𝜉 ′

𝑛,0)),

where 𝑔 is extended to accept a substate in St□ [Name] and return a substate for the corresponding
auxiliary part. Then, to prove the claim, it suffices to show that 𝑓 ′′ is differentiable (since 𝜉 ′
𝑛,0 was
chosen arbitrarily). We do case analysis on 𝑈 . If 𝑈 = St[𝜃 ] × St[rv(𝜋)], then
for all 𝜎𝜃 and 𝜉𝑛;

𝑓 ′′(𝜎𝜃, 𝜉𝑛) = 𝑓 (𝜎𝜃, 𝜉𝑛 ⊕ 𝜉 ′

𝑛,0, 𝜎𝑎,0)

since 𝑓 is differentiable with respect to 𝜃 ∪ rv(𝜋), 𝑓 ′′ is differentiable. If 𝑈 = ∅, then

𝑓 ′′(𝜎𝜃, 𝜉𝑛) = proj(𝑔(𝜉𝑛) ⊕ 𝑔(𝜉 ′

𝑛,0))

for all 𝜎𝜃 and 𝜉𝑛;

since𝑔, ⊕, and proj are all differentiable (because𝑔 only uses projection and the density of the standard
normal distribution), 𝑓 ′′ is differentiable. Hence, 𝑓 ′′ is differentiable in both cases, and this shows
the claim of the second step.

Third step: We prove that 𝜋 satisfies (R2), i.e., the functions in (R2) are differentiable with respect to
𝜃 ∪ rv(𝜋) jointly. This holds because: 𝑐𝑚 and 𝑐𝑔 do not have a double-sampling error, so each function
in (R2) is a multiplication of some of the functions in Eq. (30) (for different 𝑢 and 𝑣); the functions in
Eq. (30) are differentiable with respect to 𝜃 ∪ rv(𝜋) jointly (by the above result); and multiplication
preserves differentiability.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

Smoothness Analysis and Selective Reparameterisation

0:79

Third part: We show that 𝜋 satisfies (R3), i.e., the functions in (R3) are differentiable with re-
spect to 𝜃 jointly. For this, it suffices to show the claim that for all 𝑣 ∈ {pr 𝜇, val𝜇 | 𝜇 ∈ Name} and
𝜎𝑛 ∈ St[Name], the following function is differentiable with respect to 𝜃 jointly:

𝜋

(cid:75)

𝑐𝑔
(cid:74)

𝜎𝜃 ∈ St[𝜃 ] ↦−→

(𝜎𝑝\𝜃 ⊕ 𝜎𝜃 ⊕ 𝜎𝑛 ⊕ 𝑔(𝜎𝑛))(𝑣).

(32)
𝜋 does not have a double-sampling error, so each function in (R2) is either a
This implies (R3) because:𝑐𝑔
multiplication or a pairing of the function in Eq. (32) (for different𝑣); and multiplication and pairing pre-
serve differentiability. To show the claim, consider any 𝑣 ∈ {pr 𝜇, val𝜇 | 𝜇 ∈ Name}. Then, 𝜃 ⊆ p𝑔 (𝑣)
, p𝑔 (𝑣), {𝑣 }). From
by Eq. (14). By the soundness of differentiability analysis (Theorem 5.8),
𝜋
(cid:75)
this, and by the weakening lemma of Φ with 𝜃 ⊆ p𝑔 (𝑣) (Lemma F.4), we have |= Φ(
, 𝜃, {𝑣 }).
(cid:75)
Hence, the function in Eq. (32) is differentiable with respect to 𝜃 jointly. This completes the overall
□
proof.

𝑐𝑔
(cid:74)

𝑐𝑔
(cid:74)

|= Φ(

𝜋

Lemma H.1. Let 𝑓 : R𝑛 × R𝑚 → R be a function such that

(cid:40)𝑓1(𝑥)
𝑓2(𝑦)
for some 𝑓1 : R𝑛 → R, 𝑓2 : R𝑚 → R, and 𝑈 ⊆ 𝑅𝑛. Suppose that 𝑓2(R𝑚) = R and 𝑓 (−, 𝑦) : R𝑛 → R is
continuous for all 𝑦 ∈ R𝑚. Then, 𝑈 is either ∅ or R𝑛.

if 𝑥 ∈ 𝑈
if 𝑥 ∉ 𝑈

𝑓 (𝑥, 𝑦) =

Proof. Here is a sketch of the proof. We prove the lemma by contradiction. Suppose that 𝑈 is
neither ∅ nor R𝑛. Then, the boundary of 𝑈 (i.e., bd(𝑈 ) ⊆ R𝑛) is nonempty, since the boundary of a
set is empty if and only if the set is both open and closed, and since ∅ and R𝑛 are the only subsets
of R𝑛 that are both open and closed. Let 𝑥 ∈ bd(𝑈 ) and consider two cases: 𝑥 ∈ 𝑈 or 𝑥 ∉ 𝑈 . In
each of the two cases, we can show that there exists 𝑦 ∈ R𝑚 such that 𝑓 (−, 𝑦) is not continuous at 𝑥.
When showing the discontinuity, we use the following: 𝑥 ∈ bd(𝑈 ); the specific way that 𝑓 is defined
(in terms of 𝑈 , 𝑓1, and 𝑓2); and the assumption that 𝑓2(R𝑚) = R. By assumption, 𝑓 (−, 𝑦) should be
continuous over the entire R𝑛, so we get contradiction.
□

I DEFERRED RESULTS IN §7

I.1 Deferred Experiment Details and Results

Name

Probabilistic model
Dirichlet process mixture models
Variational autoencoder (VAE)
Inference compilation
Bayesian regression
Amortised latent Dirichlet allocation

dpmm
vae
csis
br
lda
prodlda Probabilistic topic modelling
ssvae

Semi-supervised VAE

LoC while sam obs param
4
5
5
5
5
5
7

27
35
38
42
57
58
60

6
2
2
10
8
2
4

0
0
0
0
0
0
0

1
1
2
1
1
1
1

Table 6. Pyro examples used in experiments and their key features (continued from Table 2). The last five
columns show the total number of code lines (excluding comments), loops, sample commands, observe
commands, and learnable parameters (declared explicitly by pyro.param or implicitly by a neural network
module). Each number is the sum of the counts in the model and guide.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

0:80

Wonyeol Lee, Xavier Rival, and Hongseok Yang

Differentiable

Locally Lipschitz

Name

dpmm-m
dpmm-g
vae-m
vae-g
csis-m
csis-g
br-m
br-g
lda-m
lda-g
prodlda-m
prodlda-g
ssvae-m
ssvae-g

Manual Ours Time Manual Ours Time
0.002
0.003
0.003
0.002
0.001
0.004
0.002
0.004
0.002
0.007
0.007
0.006
0.003
0.009

0.002
0.003
0.002
0.002
0.001
0.004
0.002
0.004
0.002
0.007
0.008
0.007
0.004
0.007

2
6
3
4
1
2
5
10
3
7
2
5
3
6

2
6
3
4
1
2
5
10
3
7
2
5
3
6

2
6
3
4
1
6
5
10
3
7
2
5
3
6

2
6
3
4
1
6
5
10
3
7
2
5
3
6

#CRP
2
6
3
4
1
6
5
10
3
7
2
5
3
6

Table 7. Results of smoothness analyses (continued from Table 3). “Manual” and “Ours” denote the number
of continuous random variables and learnable parameters in which the density of the program is smooth,
computed by hand and by our analyser. “Time” denotes the runtime of our analyser in seconds. “#CRP” denotes
the total number of continuous random variables and learnable parameters in the program. -m and -g denote
model and guide. We consider {(𝛼, 𝑖) ∈ Name} as one random variable for each 𝛼 ∈ Name.

Name

dpmm
vae
csis
br
lda
prodlda
ssvae

Ours

Time
0.007
0.004
0.014
0.009
0.011
0.018
0.013

Sound
2
1
1
5
3
1
1

Pyro \ Ours
Sound Unsound
0
0
0
0
0
0
0

0
0
0
0
0
0
0

#CR #DR
1
0
0
0
2
0
1

2
1
1
5
3
1
1

Table 8. Results of variable selections (continued from Table 4). “Ours-Time” denote the runtime of our variable
selector in seconds. “Ours-Sound” and “Pyro \ Ours” denote the number of random variables in the example that
are in 𝜋ours, and that are in 𝜋0 but not in 𝜋ours, respectively, where 𝜋ours and 𝜋0 denote the reparameterisation
plans given by our variable selector and by Pyro. “Pyro \ Ours” is partitioned into “Sound” and “Unsound”: the
latter denotes the number of random variables that make (R2’) or (R3’) violated when added to 𝜋ours, and the
former denotes the number of the rest. “#CR” and “#DR” denote the total number of continuous and discrete
random variables in the example. We consider {(𝛼, 𝑖) ∈ Name} as one random variable for each 𝛼 ∈ Name.

Proc. ACM Program. Lang., Vol. 0, No. 0, Article 0. Publication date: August 2022.

