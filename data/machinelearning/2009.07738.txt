Neuro-symbolic Neurodegenerative Disease Modeling
as Probabilistic Programmed Deep Kernels

Alexander Lavin
Latent Sciences
lavin@latentsci.com

1
2
0
2

n
a
J

2
1

]

G
L
.
s
c
[

3
v
8
3
7
7
0
.
9
0
0
2
:
v
i
X
r
a

Abstract

We present a probabilistic programmed deep kernel learn-
ing approach to personalized, predictive modeling of neu-
rodegenerative diseases. Our analysis considers a spectrum
of neural and symbolic machine learning approaches, which
we assess for predictive performance and important medi-
cal AI properties such as interpretability, uncertainty reason-
ing, data-efﬁciency, and leveraging domain knowledge. Our
Bayesian approach combines the ﬂexibility of Gaussian pro-
cesses with the structural power of neural networks to model
biomarker progressions, without needing clinical labels for
training. We run evaluations on the problem of Alzheimer’s
disease prediction, yielding results that surpass deep learning
in both accuracy and timeliness of predicting neurodegen-
eration, and with the practical advantages of Bayesian non-
parametrics and probabilistic programming.

Introduction
Accurate prediction of disease trajectories is critical for
informing decisions, particularly in the earliest phases of
disease where action can be impactful. Machine learning
methods show promise in diagnostics, but mainstream neu-
ral network methods struggle to model longitudinal disease
biomarkers to predict disease trajectories. Even more, they
are ineffective with sparse and limited data that is common
in clinical cohorts, and the deep learning methods are difﬁ-
cult to trust due to lack of interpretable mechanisms. Proba-
bilistic generative models, on the other hand, are well-suited
to the task of disease modeling. This class of methods allows
quantiﬁcation and reasoning with uncertainties, is robust to
sparse and noisy data, yields interpretable models and pre-
dictions, and can utilize prior information and structure en-
coded by domain experts. Probabilistic programming, a rel-
atively new area at the intersection of artiﬁcial intelligence
and programming languages, provides the ideal representa-
tion of these models in software. A probabilistic model in
the form of probabilistic program code describes the data-
generating process from unknown latent variables to ob-
served data, and in general the languages enable automatic
inference over arbitrary programs. The types of models that
may be written as probabilistic programs include not only
basic Bayesian networks and graphical models, but also ones

2021 International Workshop on Health Intelligence

more expressive and ﬂexible, such as non-parametric models
and graphical models with dynamic structure.

There have been many successful uses of probabilistic
programming and neural networks in medical AI. Yet pre-
dictive modeling neurodegenerative diseases presents even
more challenges, mainly due to the complex nature and het-
erogeneity of these conditions. For instance, Alzheimer’s
disease is characterized by dozens of biomarkers ranging
from spinal ﬂuid proteins and genetic precursors to psy-
chophysical assessments and brain MRI, the nature of which
vary widely across the population of patients. Even more,
there are potentially dozens of subtypes of the disease, and
the underlying mechanisms remain relatively unknown. This
is compounded by the data regime, where measurements are
sparse, cohorts are relatively small, and clinical labels are
subjective. To approach the problem of predictive modeling
neurodegeneration, we would ideally leverage the represen-
tational learning capacity of neural networks, while making
use of symbolic methods that operate with little data and en-
able experts to encode rich domain knowledge.

We propose such a neuro-symbolic approach that inte-
grates the ﬂexibility of Bayesian non-parametrics, the fea-
ture learning of neural networks, and the expressive capabil-
ities of probabilistic programming. Our main contributions
are the following:

• We build on the work of deep kernel learning (Wilson
et al. 2016a) to develop probabilistic programmed deep
kernel learning (PP-DKL), which enables us to improve
the biomarker modeling with “kernel warping” and mono-
tonic Gaussian processes.

• We discuss and evaluate a spectrum of machine learning
approaches towards neurodegenerative disease modeling:
from data-driven neural networks to domain-engineered
graphical models and statistical methods.

• We show our PP-DKL method achieves state-of-the-art
results on predicting neurodegeneration in Alzheimer’s
disease, along with practical advantages such as inter-
pretability and uncertainty reasoning. Further, the results
imply PP-DKL can predict neurodegeneration earlier than
other methods.

 
 
 
 
 
 
Background

Probabilistic Programming

Probabilistic programming is a paradigm that equates proba-
bilistic generative models with executable programs, where
an inference backend takes programs and observed data to
generate inference results. Probabilistic programming lan-
guages (PPLs) enable one to leverage the power of program-
ming languages to create rich and complex models, and use
built–in inference algorithms that can operate on any model
written in the language.

A PPL is typically implemented as an extension of an
existing Turing complete programming language (such as
Python (Bingham et al. 2019), C/C++ (Paige and Wood
2014; Lavin and Mansinghka 2018), and Julia (Ge, Xu, and
Ghahramani 2018)) with operations for sampling and con-
ditioning of random variables. This approach yields unre-
stricted (universal) PPLs (Gordon et al. 2014) that can op-
erate on arbitrary programs (Goodman et al. 2008; Pfeffer
2009; Mansinghka, Selsam, and Perov 2014; Wood, van de
Meent, and Mansinghka 2014; Goodman and Stuhlm¨uller).
Another class of PPL is more restrictive and thus computa-
tionally efﬁcient, where constraining the set of expressible
models ensures that particular inference algorithms can be
efﬁciently applied (Lunn et al. 2009; Winn and Minka 2009;
Milch et al. 2005; Tran et al. 2016).

Core to PPL design is the decoupling of modeling and
inference; model code is generally concise and modular,
and inference algorithms are capable of operating on arbi-
trary programs. These general-purpose “inference engines”
are typically implemented as forward-simulation sampling
methods (namely Sequential Monte Carlo and particle
MCMC (Andrieu, Doucet, and Holenstein 2010)) or varia-
tional inference. Inference engines for universal probabilis-
tic programming languages use forward-simulation based
sampling methods, which is the basis of our work.

A model program speciﬁes a joint distribution p(y, x)
over data y and variables x. Inference aims to character-
ize the distribution of program execution traces: a map from
random choices to their speciﬁc values. In other words, a
generative model is compiled to a form that can be inter-
preted by an inference engine, which then outputs some
characterization of the posterior such as a series of samples.
Formally the probability of an execution trace can be deﬁned
as

p(y, x) =

N
(cid:89)

n=1

p(yn|ζgn, xn)p(xn|xn−1)

where yn is the n-th observed data point, p(yn|ζgn ) is its
normalized likelihood, ζgn (xn) is the program argument
with random procedures gn(xn), xn is the ordered set of all
random choices with p(xn|xn−1) its normalized prior prob-
ability, and x and y are the sets of all latent and observing
random procedures, respectively (Wood, van de Meent, and
Mansinghka 2014).

For more detailed derivation and further information on
probabilistic programming we refer the reader to the recent
tutorial of van de Meent et al. (2018).

Neurodegenerative Disease Modeling
Neurodegenerative disorders such as Alzheimer’s disease
are characterised by the progressive pathological alteration
of the brain’s biochemical processes and morphology, lead-
ing to irreversible impairment of cognitive functions. These
diseases are uniquely challenging to model computationally
due to heterogeneous biological pathways (Beckett et al.
2015; Hardy and Selkoe 2002), complex temporal patterns
(Jedynak et al. 2015; Donohue et al. 2014), and diverse in-
teractions (Froelich et al. 2017; Pascoal et al. 2017). The
driving aim for longitudinal modeling Alzheimer’s disease
is to provide predictions that enable early detection of at-
risk subjects and possibly reveal latent disease mechanisms.
Statistical techniques for longitudinal disease modeling
are ubiquitous, most commonly generalized linear or non-
linear mixed-effects models (Laird and Ware 1982; Lind-
strom and Bates 1990), and various survival models such as
the Cox proportional hazards (Cox 1992). These models of-
ten assume linearity and proportionality in the underlying
stochastic disease process, which makes them ill-suited to
model the complexities of neurodegenerative processes.

More promising statistical approaches apply joint mixed-
effects models to uncover longitudinal trends across patho-
logical abnormalities, notably the approach of Li et al.
(2019) that constructs a generative model for individual
biomarker progressions. More general is the continuous-
time HMM for disease progression from Liu et al. (2015),
speciﬁcally to account for irregular sampling of longitudi-
nal data and hidden state-transition times. Their approach
yields interpretable trajectories as state transition trends (as
does ours), which is a rather useful research and prognostic
tool, but the method does not provide predictive modeling.

Bayesian non-parametric methods are promising, notably
Gaussian Processes as powerful probabilistic predictors of
dementia risk (Peterson et al. 2017; Ziegler et al. 2014;
Lorenzi et al. 2017; Hyun et al. 2016; Rudovic et al. 2019).
For the most part these approaches rely heavily on cogni-
tive assessment scores and clinical status, and mainly model
population-level statistics. The result is inability to model
early phases of Alzheimer’s, nor individual-speciﬁc patholo-
gies.

Neural network approaches, as general-purpose function
approximators, may be well-suited to the complexities inher-
ent in modeling Alzheimer’s disease. Using deep convolu-
tional neural networks, state-of-the-art vision-based results
were shown by Ding et al. (2019), but with subpar perfor-
mance in early neurodegeneration, let alone the presymp-
tomatic phases. We hypothesize restricting biomarkers to
the visual modality sets a bound on how early a model can
predict neurodegeneration. Similarly, clinical measures such
as cognitive assessments are far too late; research suggests
that brain changes associated with the disease begin upwards
of 20 years before cognitive symptoms may appear (Ville-
magne et al. 2013). Match-Net (Jarrett, Yoon, and van der
Schaar 2020) is a promising deep learning approach that ag-
gregates the biomarker modalities into one deep network,
and models longitudinal dependencies with temporal convo-
lutions. Even more, to help address data sparsity issues that
are common in neurodegenerative patient populations, the

model learns correlations between missing data and disease
progression. The authors show state-of-the-art results on the
task of Alzheimer’s risk scoring, but in a fully supervised
way towards an ill-deﬁned task. That is, most existing ap-
proaches focus on modeling patients based on their clinical
status, i.e. the clinical diagnosis categorizing them into one
of the three main stages of Alzheimer’s: cognitively normal
(CN), mild cognitive impairment (MCI), and Alzheimer’s
dementia (AD). These labels are largely subjective and qual-
itative; misdiagnosis is pervasive in practice (Jobke et al.
2018). We stress that any machine learning model trained
on Alzheimer’s clinical labels is thus ill-deﬁned.

Thus, for Alzheimer’s predictive modeling, it is impera-
tive for approaches to learn unsupervised (i.e. not training
on unreliable clinical labels). Several additional properties
are highly desirable in medical AI in general: interpretabil-
ity, uncertainty reasoning, longitudinal modeling, account-
ing for inconsistent sampling and missing observations in
clinical data. These qualities tend to be drawbacks of neural
network approaches while being advantages of more sym-
bolic methods (Ghassemi et al. 2020; Stiglic et al. 2020).

Problem Formulation

We ﬁrst deﬁne the problem of predictive modeling neurode-
generation, which is different and more clinically useful than
standard survival modeling or classifying neurodegenerative
disease (as discussed above).

Given previous biomarker measurements from individ-
ual patients, we aim to predict their cognitive capabil-
ities in the future. This task avoids the unreliability of
Alzheimer’s clinical labels, while also being more useful
in practice, as this prediction capability is valuable towards
personalized prognosis and clinical trials. We aim to predict
the key cognitive decline metrics for Alzheimer’s progres-
sion (MMSE, ADAS-Cog13, and CDRSB) corresponding to
length-τ horizons into the future.

Let there be N subjects in a study, from which there are
K observation (response) variables measured at different
follow-up times. The K observation variables can be a mix
of binary, ordinal, or continuous measurements/outcomes.
We denote the measured biomarker k for individual i at time
j as xijk, where i = 1...N , k = 1...K, j = 1...qik. Note
q is used to count the number of measurements of k for
individual i, as we must allow for the sampling to be var-
ied and sparse. The outcome scores for single patient vis-
its are stored as yi = {MMSE ∈ (0, 30), ADAS-Cog13 ∈
(0, 85), CDRSB ∈ (0, 18)}, and Y = {yi}N
i=1. Furthermore,
each patient is represented by data pairs: {xi, yi}, where
xi = {x1, ..., xt} contains the input features (i.e. biomarker
values) up to visit t of T .

Proposed Method

Our approach ﬂexibly combines deep neural networks,
Gaussian processes, and simulation-based inference as prob-
abilistic programmed neurodegenerative disease models.

Monotonic Gaussian Processes
A GP is a stochastic process which is fully speciﬁed by
its mean function and covariance function such that any ﬁ-
nite set of random variables have a joint Gaussian distri-
bution (Rasmussen and Williams 2005). GPs provide a ro-
bust method for modeling non-linear functions in a Bayesian
nonparametric framework; ordinarily one considers a GP
prior over the function and combines it with a suitable like-
lihood to derive a posterior estimate for the function given
data. GPs are ﬂexible in that, unlike parametric counter-
parts, they adapt to the complexity of the data. Even more,
GPs provide a principled way to reason about uncertainties
(Ghahramani 2015).

A GP deﬁnes a distribution over functions f : Rd → R

from inputs to target values:

f (x) ∼ GP (µ(x), kφ (xi, xj))
with mean function µ(x) and covariance kernel function
kφ (xi, xj) parameterized by φ. Any collection of function
values is jointly Gaussian,

f (X) = [f (x1) , . . . , f (xn)]T ∼ N (µ, KX,X )
with mean vector and covariance matrix deﬁned by the GP,
s.t. µi = µ(xi) and (KX,X )ij = kφ (xi, xj). In practice, we
often assume that observations include i.i.d. Gaussian noise:
y(x) = f (x) + (cid:15)(x), where (cid:15) ∼ N (0, φ2
n), and the covari-
ance function becomes

Cov (y (xi) , y (xj)) = k (xi, xj) + φ2

nδij

where δij = I[i = j]. One can then compute a Gaussian
posterior distribution in closed form by conditioning on the
observed data (XL, yL) and make predictions at unlabeled
points XU .

Sparse GP methods have signiﬁcantly improved the scal-
ability of GP inference, speciﬁcally the method of induc-
ing points (Titsias 2009; Hensman, Matthews, and Ghahra-
mani 2015). This approach allows us to make a variational
approximation to the posterior process that is Gaussian and
only uses M << D inducing features (where D is the full
number of data points). This decreases the time and space
complexity from O(d3) and O(d2), respectively, to O(dm2)
and O(dm).

To prior the Gaussian process biomarker progressions to
better model increasing severity of neurodegeneration, we
implement montonicity constraints. Speciﬁcally we use the
approach of Riihim¨aki and Vehtari (2010), to include deriva-
tives information at a number of GP input locations and
force the derivative process to be positive at these locations.
In future work we would like to improve on this with mono-
tonic GP ﬂows (Ustyuzhaninov et al. 2020).

Probabilistic Programmed Deep Kernel Learning
Deep kernel learning (DKL) combines the structural proper-
ties of deep architectures with the non-parametric ﬂexibility
of kernel methods (Wilson et al. 2016a). Speciﬁcally com-
bining deep neural networks with Gaussian process yields
several useful characteristics:

Figure 1: Two-dimensional embeddings learned by a DKL model (left) and our probabilistic programmed version (right);
the colorbar shows the magnitude of the normalized outputs. In the left panel, DKL learns a poor embedding, where colors
representing different output magnitudes are intermingled. In the right panel, PP-DKL is able to learn a better representation
of the dataset; kernel warping creates a cohesive deep kernel, yielding inducing points that lie in the original data space rather
than the neural network feature space.

• The information capacity of the model grows with the
amount of available data (as a non-parametric method),
but its complexity is automatically calibrated through the
marginal likelihood of the GP.

• The non-parametric GP layer provides ﬂexibility and au-
tomatic calibration, and typically high performance (in
the face of noise). This helps reduce the need for extensive
hand tuning from the user.

• Inference and learning procedures for O(n) training and

O(1) testing time.

Given input data x ∈ X, a neural network parameterized
by w is used to extract features hw(x) ∈ R. The outputs are
modeled as

f (x) ∼ GP (µ (hw(x)) , kφ (hw (xi) , hw (xj)))

for some mean function µ(·) and covariance kernel function
kφ (·, ·) with parameters φ. Parameters θ = (w, φ) of the
deep kernel are learned jointly by minimizing the negative
log-likelihood of the labeled data: − log p (yL | XL, θ). For
Gaussian distributions, the marginal likelihood is a closed-
form, differentiable expression, allowing DKL models to be
trained via backpropagation.

A standard approach to DKL is to apply GPs to the
feature-level outputs of a NN, one GP per feature. This rep-
resentation yields GP inducing points in feature-space H.
Ma, Ganapathiraman, and Zhang (2019) show kernel “warp-
ing” to induce feature representations that respect invari-
ances that reach beyond transformation. In this context, we
would want to “warp” the GP with the neural network fea-
ture extractor to produce a deep kernel kwarped that yields
inducing points in the space of the original data X:

kwarped(x, h) = q(k(f (x), f (h)))

This can be readily implemented in probabilistic program-
ming by deﬁning the combined deep kernel within a prob-
abilistic program, which can then be simply used by the
MCMC black-box inference engine as usual. Otherwise one
must implement complex, custom inference methods for the
warped deep kernel model. We refer to this approach as
probabilistic-programmed deep kernel learning (PP-DKL).
To gain some intuition about how the warping helps the
model learn a better representation, we visualize in Fig. 1
the neural network embeddings learned by the DKL and PP-
DKL models on the Alzheimer’s dataset (ADNI, described
in the Experiments section). The left plot shows DKL learns
a rather poor embedding, where different colors represent-
ing different output magnitudes are intermingled. Yet the
warped deep kernel resulting from PP-DKL yields embed-
dings that better represent the dataset, as shown in the right
plot.

Neurodegeneration Programs

A probabilistic model in the form of probabilistic program
code describes the data-generating process from unknown
latent variables values to observed data. We posit the un-
derlying disease state is represented by a latent stochastic
process that manifests as a series of observable, longitudi-
nal biomarkers. We encode additional domain knowledge
by using monotonic GPs to model how each biomarker de-
teriorates over time, in agreement with main Alzheimer’s
progression hypotheses (Jedynak et al. 2015). The model is
trained on population level data to capture general statistics
for biomarker progressions. Individual instances of a “neu-
rodegeneration program” are fed a patient’s observed data in
the form of biomarker values, and predicts their individual
biomarker progressions.

A neurodegeneration program can be used as a simula-
tor of a patient’s latent disease process. That is, probabilis-
tic programs are likened to simulators in the sense that in

Table 1: Comparison of models on neurodegeneration prediction task. See text for details.

MAE

ICC

MMSE (0-30) A-Cog13 (0-85) CDRSB (0-18) MMSE (0-30) A-Cog13 (0-85) CDRSB (0-18)
5.76 ± 0.91
2.18 ± 0.42
1.36 ± 0.27
1.46 ± 0.36
1.82 ± 0.26
1.67 ± 0.20
4.11 ± 0.92

0.62 ± 0.06
0.79 ± 0.08
0.88 ± 0.06
0.83 ± 0.07
0.79 ± 0.08
0.80 ± 0.10
0.69 ± 0.10

1.73 ± 0.08
0.91 ± 0.11
0.60 ± 0.06
0.64 ± 0.07
0.77 ± 0.05
0.60 ± 0.05
0.91 ± 0.11

0.74±0.08
0.86 ± 0.09
0.92 ± 0.04
0.91 ± 0.05
0.88 ± 0.08
0.89 ± 0.10
0.75 ± 1.0

7.20 ± 1.01
4.99 ± 0.92
3.99 ± 0.09
4.04 ± 0.68
4.52 ± 0.58
4.71 ± 0.51
6.96 ± 0.98

0.41 ± 0.20
0.71 ± 0.10
0.85 ± 0.11
0.82 ± 0.13
0.76 ± 0.06
0.76 ± 0.07
0.50 ± 0.19

Models

JM
pGP
PP-DKL
PP-DKL’
DKL
Match-Net
RNN

simulation-based inference, the simulator itself deﬁnes the
statistical model (Cranmer, Brehmer, and Louppe 2020).
The neurodegeneration program deﬁnes a statistical model
with random samplings, from which we can generate prob-
abilistic trajectories for individual biomarkers. This is one
means by which PP-DKL is interpretable: outputs can be in-
spected as individual and aggregate biomarker predictions
over time – i.e. trajectories with conﬁdence intervals (from
uncertainty propagation) – rather than point predictions or a
single disease risk score. PP-DKL is also interpretable in the
sense that a model is encoded as a program, so we can use
the program structure and analysis to understand the proper-
ties of a model before inference (Pfeffer et al. 2018).

The GP mean and variance is propagated throughout pro-
gram execution, so quantifying the speciﬁc uncertainties
with PP-DKL is trivial; we can sample model programs for
aleatoric (system stochasticity such as observation and pro-
cess noise) and epistemic (subjective uncertainty due to lim-
ited data) uncertainty measures.

We implement PP-DKL in the “Pyro” probabilistic pro-
gramming language (which is embedded in Python and
leverages PyTorch) (Bingham et al. 2019).

Experiments

We experiment with data from the Alzheimer’s Disease
Neuroimaging Initiative (ADNI), a multicohort longitudinal
study in which volunteers diagnosed as cognitively healthy
or with various degrees of cognitive impairment have been
evaluated since 2005 (adni.loni.usc.edu). ADNI subjects
are evaluated via neuroimaging (PET), cerebrospinal ﬂuid
(CSF), and other biomarkers, as well as clinical and neu-
ropsychological assessments. As discussed earlier, the di-
agnostic labels in ADNI are ill-deﬁned and thus an unsuit-
able task (here and in clinical practice). We thus emphasize
the task of cognitive scoring predictions corresponding to
length-τ horizons into the future, rather than diagnostic clas-
siﬁcation. We downloaded the standard dataset processed for
the TADPOLE Challenge (Marinescu et al. 2018), represent-
ing 1,737 unique patients.

To evaluate performance we follow the speciﬁcations of
(Peterson et al. 2017): we ran a 10-fold patient-independent
cross-validation, and report the mean absolute error (MAE)
and intra-class correlation (ICC (3,1)) metrics. The latter
ranges from 0–1, and is used to measure the (absolute)

agreement between the model predictions and the ground-
truth for target scores. Table 1 reports the mean ±SD of the
10-folds. All the input features were z-normalized, and we
then applied principal component analysis to reduce the ef-
fects of noise in the data, preserving 95% of variance. Note
the experiments in (Peterson et al. 2017) select a subset of
patients above a certain threshold of data points. However
we do not do this, as it is important to evaluate the robust-
ness of methods to sparseness in clinical data.

Baselines: neural nets to symbolic models. We evalu-
ate a spectrum of neurodegenerative disease modeling ap-
proaches, from neural network models with massive param-
eter spaces that learn nearly any relevant function approxi-
mation, to methods that are strongly-speciﬁed with domain
and mechanistic details. At the tabula-rasa end of the spec-
trum we have deep learning approaches which aim to au-
tomatically discover good feature representations through
end-to-end optimization of neural networks. We speciﬁ-
cally evaluate Match-Net (Jarrett, Yoon, and van der Schaar
2020), the leading deep learning approach in this domain,
and a baseline recurrent neural networks (RNN). At the in-
tersection of neural networks and symbolic AI, we have
deep kernel learning. Speciﬁcally we compare several vari-
ations: DKL is the standard approach from (Wilson et al.
2016b), PP-DKL is our probabilistic programmed deep ker-
nel approach, and PP-DKL’ is without the monotonicity con-
straints. All DKL methods use the same feature-extractor
network architecture (similar to (Wilson et al. 2016a)): [d −
100 − 50 − 50 − 2]. Moving along the spectrum away from
neural networks, we run the auto-regressive personalized
GP (pGP) of (Peterson et al. 2017), and ﬁnally a conven-
tional statistical method, joint modeling (JM). Note we do
not run the purely mechanistic approach of ordinary differ-
ential equations as these have not been shown to perform
well in predictive modeling neurodegenerative diseases. All
models are implemented in PyTorch, and code will be open-
source with publication. Further details on the dataset and
models are provided in the Appendix.

Results

The results are shown in Table 1. Notice we sort the models
top-down, progressively becoming more “pure” deep neural
network, with our neuro-symbolic approach PP-DKL listed

in the middle. We ﬁnd PP-DKL in general outperforms all
other methods, across the cognitive score predictions and
metrics. Relative to the other deep kernel variations – the
standard DKL and the non-monotonic PP-DKL’ – we ob-
serve that both kernel-warping via probabilistic program-
ming and the GP monotonicity constraints on biomarker
progressions improve performance as expected; interest-
ingly, the monotonic GPs show tighter variance in general.

As expected from the results in Jarrett, Yoon, and van der
Schaar (2020), we also see Match-Net outperforming the
RNN baseline on the neurodegeneration prediction tasks. In-
terestingly, the performance difference is exacerbated in the
neurodegeneration task here, which is more challenging than
the fully supervised, risk-scoring task in their prior work.
An important component of Match-Net that behooves lon-
gitudinal disease modeling is parameterizing a window for
which to use historical measurements; comparatively, recur-
rent models may consume the entire history, and at the other
extreme a Cox model only utilizes the most recent measure-
ment. This is a prime example of building speciﬁc induc-
tive biases in neural networks as a means of encoding prior
knowledge (albeit minimal) for a speciﬁc task. Nonethe-
less, Match-Net (and deep learning approaches in general),
are not well-suited for the task of predicting neurodegen-
eration as they require vast quantities of labeled data, and
suffer from miscalibration and lack of interpretable mecha-
nisms. PP-DKL, on the other hand, is data-efﬁcient due to
implicit modeling structure, and well-calibrated because the
Bayesian formulation provides a posterior over predictions.
To evaluate the data-efﬁciency on this task, we ﬁnd PP-DKL
performs within 5% of the performances in Table 1 when we
cut the training data by 25%, but comparatively Match-Net
errors jump upwards of 20% across the tests and metrics.

Interpretability being an important feature in medical AI
(for usability and trust in practice), we suggest probabilistic
programming methods to be a more promising direction than
“black-box” deep neural networks. PPL are interpretable by
deﬁnition (Ghahramani 2015): model structure is made ex-
plicit (rather than abstracted away and learned purely from
data), and the generative formulation lends itself to model
criticisms and explanation methods, such as prior and poste-
rior predictive checks (Tran et al. 2016).

Further, these results have potentially important impli-
cations for early interventions in Alzheimer’s disease: The
known progression of neurodegenerative processes (Jedynak
et al. 2015) suggests the longitudinal biomarkers we used
as model inputs precede cognitive decline upwards of 100
months. Thus, for a model to perform well on our task, it
must be able to predict cognitive decline well into the future.
This result is imperative in Alzheimer’s disease where inter-
ventions must target the earliest, presymptomatic phases in
order to make a difference.

Conclusion
We have presented a novel probabilistic programmed deep
kernel learning (PP-DKL) approach that performs state-
of-the-art in predicting cognitive decline in Alzheimer’s
disease. Our neuro-symbolic approach has advantageous
medical-AI characteristics such as data-efﬁciency, model

and prediction interpretability, and principled uncertainty
reasoning. At a higher level, we have compared two per-
spectives on AI-based longitudinal disease modeling: data-
driven with neural networks, and domain-engineered with
probabilistic programs.

We suggest PP-DKL will perform well in other disease
areas, namely Parkinson’s, CTE, and other neurodegenera-
tive conditions, which is an important direction of our future
work. Note the preference of PP-DKL over deep learning
is most likely true for heterogeneous diseases with smaller
datasets.

Another important direction to pursue with PP-DKL is to-
wards diagnostic and prognostic algorithms. One can utilize
the “neurodegeneration programs” as personalized disease
simulators, potentially towards counterfactual reasoning and
causal inference (Richens, Lee, and Johri 2020). Even more,
the programs provide a continuous representation of each in-
dividual’s disease state, i.e. representing neurodegeneration
as a sample-continuous process. Further studies can thus de-
rive an objective, data-driven deﬁnition of Alzheimer’s dis-
ease and subtypes, allowing us to stage and diagnose indi-
viduals in a precise, pathological way.

Broader impact. Artiﬁcial
intelligence and machine
learning methods offer a lot of promise in medicine and
healthcare. Our PP-DKL method can be a powerful tool in
neurodegenerative disease diagnosis and prognostic mod-
eling;
there are over 50 million people globally with
Alzheimer’s disease, and it is the 6th leading cause of death
in the United States. The ease of experimentation with neu-
rodegeneration models as probabilistic programs can have
signiﬁcant effects in better understanding neurodegenerative
processes. We’ve made clear that Alzheimer’s diagnostics
are poor labels and clinical endpoints, but we can do even
better than cognitive scores used in this paper and in drug
development. Ideally we can utilize our approach to study a
multi-biomarker model that precisely deﬁnes disease stages;
cognitive scores may be noisy, prone to bias, and present dif-
ﬁculties with ﬂoor/ceiling effects. Any diagnosis method, be
it human or machine, in clinical practice runs the risk of er-
rors measured in human lives. It is imperative that methods
such as ours continue with thorough veriﬁcation and valida-
tion steps (importantly, with medical professionals and other
domain experts in the loop).

References
Andrieu, C.; Doucet, A.; and Holenstein, R. 2010. Particle
Markov chain Monte Carlo methods. Journal of The Royal
Statistical Society Series B-statistical Methodology 72: 269–
342.

Beckett, L.; Donohue, M.; Wang, C.; Aisen, P.; and Initia-
tive, A. D. N. 2015. The Alzheimer’s Disease Neuroimaging
Initiative phase 2: Increasing the length, breadth, and depth
of our understanding. Alzheimer’s & Dementia 11: 823–831.

Bingham, E.; Chen, J. P.; Jankowiak, M.; Obermeyer, F.;
Pradhan, N.; Karaletsos, T.; Singh, R.; Szerlip, P. A.; Hors-
fall, P.; and Goodman, N. D. 2019. Pyro: Deep Universal

Probabilistic Programming. J. Mach. Learn. Res. 20: 28:1–
28:6.
Cox, D. R. 1992. Regression models & life-tables. Springer.
ISBN 978-1-4612-4380-9.

Cranmer, K.; Brehmer, J.; and Louppe, G. 2020. The frontier
of simulation-based inference. Proceedings of the National
Academy of Sciences 117: 30055 – 30062.
Ding, Y.; Sohn, J. H.; Kawczynski, M.; Trivedi, H.; Har-
nish, R.; Jenkins, N.; Lituiev, D.; Copeland, T. P.; Aboian,
M.; Aparici, C. M.; Behr, S.; Flavell, R.; Huang, S.-Y.; Za-
locusky, K.; Nardo, L.; Seo, Y.; Hawkins, R.; Pampaloni,
M. H.; Hadley, D.; and Franc, B. 2019. A Deep Learning
Model to Predict a Diagnosis of Alzheimer Disease by Us-
ing 18F-FDG PET of the Brain. Radiology 290 2: 456–464.
Donohue, M.; Jacqmin-Gadda, H.; Goff, M. L.; Thomas, R.;
and Initiative, A. D. N. 2014. Estimating long-term mul-
tivariate progression from short-term data. Alzheimer’s &
Dementia 10: s400–s410.
Froelich, L.; Peters, O.; Lewczuk, P.; Gruber, O.; Teipel, S.;
Gertz, H. J.; Jahn, H.; Jessen, F.; Kurz, A.; Luckhaus, C.;
Huell, M.; Pantel, J.; Reischies, F.; Schr¨oder, J.; Wagner, M.;
Rienhoff, O.; Wolf, S.; Bauer, C.; Schuchhardt, J.; Heuser,
I.; Ruether, E.; Henn, F. A.; Maier, W.; Wiltfang, J.; and Ko-
rnhuber, J. 2017. Incremental value of biomarker combina-
tions to predict progression of mild cognitive impairment to
Alzheimer’s dementia. Alzheimer’s Research & Therapy 9.
Ge, H.; Xu, K.; and Ghahramani, Z. 2018. Turing: A Lan-
guage for Flexible Probabilistic Inference.

Ghahramani, Z. 2015. Probabilistic machine learning and
artiﬁcial intelligence. Nature 521: 452–459.
Ghassemi, M.; Naumann, T.; Schulam, P.; Beam, A. L.;
Chen, I.; and Ranganath, R. 2020. A Review of Challenges
and Opportunities in Machine Learning for Health. AMIA
Joint Summits on Translational Science proceedings. AMIA
Joint Summits on Translational Science 2020: 191–200.
Goodman, N. D.; Mansinghka, V. K.; Roy, D. M.; Bonawitz,
K.; and Tenenbaum, J. 2008. Church: a language for gener-
ative models. In UAI.
Goodman, N. D.; and Stuhlm¨uller, A. ???? The Design and
Implementation of Probabilistic Programming Languages.
http://dippl.org. Accessed: 2014-08-27.

Gordon, A.; Henzinger, T.; Nori, A.; and Rajamani, S. 2014.
Probabilistic programming. Future of Software Engineering
Proceedings .
Hardy, J.; and Selkoe, D. 2002. The Amyloid Hypothesis of
Alzheimer’s Disease: Progress and Problems on the Road to
Therapeutics. Science 297: 353 – 356.
Hensman, J.; Matthews, A.; and Ghahramani, Z. 2015. Scal-
In AIS-
able Variational Gaussian Process Classiﬁcation.
TATS.
Hyun, J. W.; Li, Y.; Huang, C.; Styner, M.; Lin, W.; and Zhu,
H. 2016. STGP: Spatio-temporal Gaussian process models
for longitudinal neuroimaging data. NeuroImage 134: 550–
562.

Jarrett, D.; Yoon, J.; and van der Schaar, M. 2020. Dy-
namic Prediction in Clinical Survival Analysis Using Tem-
poral Convolutional Networks. IEEE Journal of Biomedical
and Health Informatics 24: 424–436.

Jedynak, B.; Liu, B.; Lang, A.; Gel, Y.; and Initiative, A.
D. N. 2015. A computational method for computing an
Alzheimer’s disease progression score; experiments and val-
idation with the ADNI data set. Neurobiology of Aging 36:
s178–s184.

Jobke, B.; McBride, T.; Nevin, L.; Peiperl, L.; Ross, A.;
Stone, C.; and Turner, R. 2018. Setbacks in Alzheimer re-
search demand new strategies, not surrender. PLoS Medicine
15.

Laird, N.; and Ware, J. 1982. Random-effects models for
longitudinal data. Biometrics 38 4: 963–74.

Lavin, A.; and Mansinghka, V. K. 2018. Probabilistic pro-
gramming for data-efﬁcient robotics. In The International
Conference on Probabilistic Programming (PROBPROG).

Li, D.; Iddi, S.; Thompson, W.; and Donohue, M. 2019.
Bayesian latent time joint mixed effect models for multico-
hort longitudinal data. Statistical Methods in Medical Re-
search 28: 835 – 845.

Lindstrom, M.; and Bates, D. 1990. Nonlinear mixed effects
models for repeated measures data. Biometrics 46 3: 673–
87.

Liu, Y.-Y.; Li, S.; Li, F.; Song, L.; and Rehg, J. M. 2015. Ef-
ﬁcient Learning of Continuous-Time Hidden Markov Mod-
els for Disease Progression. Advances in neural information
processing systems 28: 3599–3607.

Lorenzi, M.; Filippone, M.; Alexander, D. C.; and Ourselin,
S. 2017. Disease Progression Modeling and Prediction
through Random Effect Gaussian Processes and Time Trans-
formation.

Lunn, D.; Spie´gelhalter, D.; Thomas, A.; and Best, N. 2009.
The BUGS project: Evolution, critique and future directions.
Statistics in medicine 28 25: 3049–67.

Ma, Y.; Ganapathiraman, V.; and Zhang, X. 2019. Learning
Invariant Representations with Kernel Warping. In AISTATS.

Mansinghka, V. K.; Selsam, D.; and Perov, Y. N. 2014.
Venture: a higher-order probabilistic programming platform
with programmable inference. ArXiv abs/1404.0099.

Marinescu, R.; Oxtoby, N.; Young, A.; Bron, E.; Toga, A.;
Weiner, M.; Barkhof, F.; Fox, N.; Klein, S.; Alexander, D.;
the EuroPOND Consortium; and the Alzheimer’s Disease
Neuroimaging Initiative, F. 2018. TADPOLE Challenge:
Prediction of Longitudinal Evolution in Alzheimer’s Dis-
ease. arXiv: Populations and Evolution .

Milch, B.; Marthi, B.; Russell, S.; Sontag, D.; Ong, D. L.;
and Kolobov, A. 2005. BLOG: Probabilistic Models with
Unknown Objects. In IJCAI.

Paige, B.; and Wood, F. D. 2014. A Compilation Target for
Probabilistic Programming Languages. In ICML.

Winn, J.; and Minka, T. 2009. Probabilistic Programming
with Infer.NET. URL https://www.microsoft.com/en-us/
research/publication/probabilistic-programming-infer-net/.

Wood, F.; van de Meent, J.-W.; and Mansinghka, V. K. 2014.
A New Approach to Probabilistic Programming Inference.
In AISTATS.
Ziegler, G.; Ridgway, G.; Dahnke, R.; and Gaser, C. 2014.
Individualized Gaussian process-based prediction and detec-
tion of local and global gray matter abnormalities in elderly
subjects. Neuroimage 97: 333 – 348.

Pascoal, T. A.; Mathotaarachchi, S.; Shin, M.; Benedet, A.;
Mohades, S.; Wang, S.; Beaudry, T.; Kang, M. S.; Soucy, J.-
P.; Labbe, A.; Gauthier, S.; and Rosa-Neto, P. 2017. Syner-
gistic interaction between amyloid and tau predicts the pro-
gression to dementia. Alzheimer’s & Dementia 13: 644–653.

Peterson, K.; Rudovic, O.; Guerrero, R.; and Picard,
Personalized Gaussian Processes for Fu-
R. W. 2017.
ture Prediction of Alzheimer’s Disease Progression. ArXiv
abs/1712.00181.

Pfeffer, A. 2009. Figaro : An Object-Oriented Probabilistic
Programming Language.

Pfeffer, A.; Ruttenberg, B. E.; Kretschmer, W.; and
O’Connor, A. 2018. Structured Factored Inference for Prob-
abilistic Programming. In AISTATS.

Rasmussen, C. E.; and Williams, C. K. I. 2005. Gaus-
sian Processes for Machine Learning (Adaptive Compu-
tation and Machine Learning). The MIT Press.
ISBN
026218253X.

Richens, J. G.; Lee, C. M.; and Johri, S. 2020. Improving the
accuracy of medical diagnosis with causal machine learning.
Nature Communications 11.

Riihim¨aki, J.; and Vehtari, A. 2010. Gaussian processes with
monotonicity information. In AISTATS.

Rudovic, O.; Utsumi, Y.; Guerrero, R.; Peterson, K.; Rueck-
ert, D.; and Picard, R. W. 2019. Meta-Weighted Gaussian
Process Experts for Personalized Forecasting of AD Cogni-
tive Changes. ArXiv abs/1904.09370.

Stiglic, G.; Kocbek, P.; Fijacko, N.; Zitnik, M.; Verbert, K.;
and Cilar, L. 2020.
Interpretability of machine learning
based prediction models in healthcare. Wiley Interdiscip.
Rev. Data Min. Knowl. Discov. 10.

Titsias, M. K. 2009. Variational Learning of Inducing Vari-
ables in Sparse Gaussian Processes. In AISTATS.

Tran, D.; Kucukelbir, A.; Dieng, A. B.; Rudolph, M.; Liang,
D.; and Blei, D. 2016. Edward: A library for probabilistic
modeling, inference, and criticism. ArXiv abs/1610.09787.

Ustyuzhaninov, I.; Kazlauskaite, I.; Ek, C. H.; and Camp-
bell, N. W. 2020. Monotonic Gaussian Process Flows. In
AISTATS.

van de Meent, J.-W.; Paige, B.; Yang, H.; and Wood, F.
2018. An Introduction to Probabilistic Programming. ArXiv
abs/1809.10756.

Villemagne, V.; Burnham, S.; Bourgeat, P.; Brown, B.;
for the Australian Imaging Biomarkers; and Group, L. R.
2013. Amyloid-beta deposition, neurodegeneration, and
cognitive decline in sporadic Alzheimer’s disease: a
prospective cohort study. The Lancet Neurology 12: 357–
367.

Wilson, A.; Hu, Z.; Salakhutdinov, R.; and Xing, E. 2016a.
Deep Kernel Learning. In AISTATS.

Wilson, A.; Hu, Z.; Salakhutdinov, R.; and Xing, E.
2016b. Stochastic Variational Deep Kernel Learning. ArXiv
abs/1611.00336.

• Magnetic resonance imaging (MRI) biomarkers can be
used to quantify atrophy and structural brain integrity
by measuring the volume of speciﬁc structures, and with
diffusion tensor imaging (DTI). The volumes of interest
include entorhinal, fusiform, hippocampus, intracranial,
mid-temp, ventricles, and whole brain.

• Demographics: age, gender, ethnicity, race, years of edu-
cation, and marital status. All are categorical variables but
for age and education.

• Alipoprotein E4 variant (APOE E4) gene, known as the
largest generic risk factor for Alzheimer’s disease. Geno-
typing in ADNI determines if each patient has the APOE
E4 gene present, and classiﬁes patients into one of two
genetypes (APGEN1 or APGEN2) based on their alleles.

The clinical measurements are taken at approximate 1/2-
year intervals; the average absolute deviation between orig-
inal values and ﬁnal timestamps amounts to approximately
4 days. Some studies ﬁx the measurements at exactly six
month timestamps, but we do not. Also note that some stud-
ies eliminate patients with sparse data, e.g. Peterson et al.
(2017) cut 1,737 patients to only 100 with rich, nearly com-
plete data. We do not eliminate patients for sparse data, as
overcoming sparsity is an important capability in real-world
use.

Appendix

Modeling & Training
We experimented with the following kernels:
The standard squared exponential (or radial basis function
(RBF)) kernel:

kRBF (xi, xj) = φ2

f exp

−

(cid:32)

(cid:33)

(cid:107)xi − xj(cid:107)2
2
2φ2
l

with parameters φ2
teristic length scale, respectively.

f and φ2

l for signal variance and charac-

The rational quadratic kernel, which is equivalent to
adding together many RBF kernels with different length
scales:

(cid:32)

kRQ (x, x(cid:48)) = σ2

1 +

(cid:33)−α

(x − x(cid:48))2
2α(cid:96)2

where parameter α determines the relative weighting of
large-scale and small-scale variations.

Polynomial kernels:

kpoly (xi, xj) = (cid:0)φf xT

i xj + φl

(cid:1)p

, p ∈ Z+

Periodic kernels and non-smooth kernels

(namely
Matern) would not be suitable for our biomarker progres-
sions.

Our reported results reﬂect the rational quadratic kernel,

although those results were marginally better than RBF.

Note we train these parameters in the log-domain to en-
force positivity constraints on the kernel parameters and pos-
itive deﬁniteness of the covariance. The parametric neural
networks are regularized with L2 weight decay to reduce
overﬁtting, and models are implemented and trained in Py-
Torch using the ADAM optimizer.

Dataset Details
Data used in this work was obtained from the Alzheimer’s
Disease Neuroimaging Initiative (ADNI), a multicohort lon-
gitudinal study in which volunteers diagnosed as cognitively
healthy or with various degrees of cognitive impairment
have been evaluated since 2005 (adni.loni.usc.edu). The re-
cent TADPOLE Challenge (Marinescu et al. 2018) provides
a standardized dataset combining data from the multiple
phases of ADNI and across the study sites. The multi-modal
dataset from the ADNI database we used is comprised of
biomarkers across seven different modalities:

• Three cerebrospinal ﬂuid (CSF) measurements: amyloid-

beta, tau, and phosphorylated tau levels.

ADAS-Cog13, MMSE,

• Cognitive assessments: CDR Sum of Boxes (CDRSB),
ADAS-Cog11,
RAVLT-
immediate subtype, RAVLT-learning subtype, RAVLT-
forgetting subtype, RAVLT-percent forgetting subtype,
and FAQ. We limit our experiments to only use CDRSB,
ADAS-Cog13, and MMSE, which are the most prevelant
in practice.

