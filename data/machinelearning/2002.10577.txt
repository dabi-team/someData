0
2
0
2

r
a

M
3
1

]

Y
S
.
s
s
e
e
[

2
v
7
7
5
0
1
.
2
0
0
2
:
v
i
X
r
a

Dynamic Power Allocation and Virtual Cell
Formation for Throughput-Optimal Vehicular Edge
Networks in Highway Transportation

Md Ferdous Pervej and Shih-Chun Lin
Department of Electrical and Computer Engineering
North Carolina State University, Raleigh, NC 27695, USA
Email: {mpervej, slin23}@ncsu.edu

Abstract—This paper investigates highly mobile vehicular net-
works from users’ perspectives in highway transportation. Partic-
ularly, a centralized software-deﬁned architecture is introduced
in which centralized resources can be assigned, programmed,
and controlled using the anchor nodes (ANs) of the edge servers.
Unlike the legacy networks, where a typical user is served from
only one access point (AP), in the proposed system model, a
vehicle user is served from multiple APs simultaneously. While
this increases the reliability and the spectral efﬁciency of the
assisted users, it also necessitates an accurate power allocation
in all transmission time slots. As such, a joint user association
and power allocation problem is formulated to achieve enhanced
reliability and weighted user sum rate. However, the formulated
problem is a complex combinatorial problem, remarkably hard
to solve. Therefore, ﬁne-grained machine learning algorithms
are used to efﬁciently optimize joint user associations and
power allocations of
the APs in a highly mobile vehicular
network. Furthermore, a distributed single-agent reinforcement
learning algorithm, namely SARL-MARL,
is proposed which
obtains nearly identical genie-aided optimal solutions within a
nominal number of training episodes than the baseline solution.
Simulation results validate that our solution outperforms existing
schemes and can attain genie-aided optimal performances.

I. INTRODUCTION

Vehicular networking has lately drawn signiﬁcant attention
in wireless networking, particularly for next-generation com-
munication platform, such as beyond 5G systems. The advent
of vehicular ad-hoc networking [1] has made it possible to
establish communication among vehicles on the road. This has
posed as a direct method to share important safety messages
among the vehicles using device-to-device like communica-
tion. The early development of dedicated short-range commu-
nications (DSRC) [2] were designed following IEEE 802.11
physical and medium access layers technologies [3]. However,
DSRC uses collision avoidance medium access schemes [4].
Particularly, when network load increases, this creates an extra
burden when extreme reliability has to be ensured [5]. The so-
called cellular vehicle-to-everything (C-V2X) was developed
by the 3GPP as a potential alternative [4] to these limitations
of DSRC. In C-V2X there are mainly two radio interfaces,

This work was supported by NC State 2019 FRPD, Cisco Systems, Inc.,

and North Carolina Department of Transportation (NCDOT).

namely the cellular interface (Uu interface) and the sidelink
(PC5 interface). VUs maintain their communication with the
infrastructure using the Uu interface. On the other hand, they
can communicate with each other directly using the PC5
interface. The applications of V2X aim to incorporate both
DSRC and C-V2X [6], in reality, and have signiﬁcantly drawn
the attention recently to reduce travel time, trafﬁc congestion,
and ubiquitous internet connectivity for smart cities [7].

There are several works in the literature addressing various
aspects of vehicular networking. S¸ ahin et.al. have presented
a virtual cell formation for V2X in [8] where vehicles that
are in close-proximity can receive downlink multicast data
from the transmission points. Gao et. al. have proposed a
deep neural network based resource allocation scheme in [9]
where they have used block coordinated descent method and
minimized weighted minimum mean square error. Lien et.
al. have considered a local area data network and shown a
way to achieve lower latency in the radio access part in [10].
Considering the potential of performing caching at the edge
nodes, the authors have used sophisticated tools of stochastic
optimization and reinforcement learning (RL) to achieve lower
latency using feedback less transmission. He et. al. have
proposed an integrated platform for connected vehicles in
[11]. The authors have formulated an integrated platform,
in which they have orchestrated a joint networking, caching
and computing resource allocation problem, and applied deep
reinforcement learning (DRL) techniques.

Tan et. al. have incorporated both caching and computing
in mobility aware vehicular networks in [12]. The authors
have shaped vehicle mobility using a naive contact time-based
model. Considering both VUs and RSUs have caching and
computing capabilities, they have used a DRL framework to
ﬁnd the optimal caching placement and computing resource
allocation strategy in this paper. Ye et. al. have used DRL to
allocate radio resources for vehicle-to-vehicle (V2V) commu-
nication in [13]. In particular, they have reused the vehicle-
to-infrastructure (V2I) uplink channels for V2V transmissions.
Distributed vehicular networking resource allocation problems
have also been addressed in [14] using multi-agent reinforce-
ment learning. Similar to Ye et. al., the authors have reused

 
 
 
 
 
 
uplink frequency resources for V2V communications. They
have devised a system where their agents can interact with
the environment and decide which channel to reuse and V2V
transmission power level for V2V communication.

While most of these works address various aspects of V2X
communication from the traditional network perspective, in
this work, we incorporate user-centric dynamic cell design
in a sophisticated software-deﬁned environment to serve the
users from multiple sources simultaneously. Particularly, we
contemplate a centralized environment where resources can
be programmed, controlled and distributed based on the users’
demands. As presented in Fig. 1, we consider multiple edge
servers are delicately deployed closer to the edge nodes
in order to lessen end-to-end latency and improve spectral
efﬁciency. Each of these edge servers is physically connected
to a cloud server and controlled by its corresponding ANs.
Furthermore,
low-powered relay like APs are stationed as
RSUs to guarantee ubiquitous connectivity of the VUs. Each
of these APs is mesh connected to each of the edge servers.
Moreover, we are interested in joint virtual cells design - for
each of the scheduled VUs, and optimal beamforming vectors
of the low-powered APs. Therefore, in this paper, we address
joint user scheduling and power allocation problem in a highly
mobile vehicular downlink network from the users’ prospect.
To the best of our knowledge, this is the ﬁrst work to
consider software-deﬁned joint user-centric cell formation and
power allocation, which ensures reliable communication and
maximizing weighted sum rate (WSR) in a highway vehicular
platform. Due to the complex combinatorial nature of the
formulated optimization problem, it is challenging to solve
the problem via traditional optimization methods. Thus, we
propose a distributed Q-learning based RL solution to ob-
tain optimal WSR and validate our results with genie-aided
optimal, baseline single agent RL (SARL), multi-agent RL
(MARL) and other baseline schemes. The rest of the paper
is organized as follows. Section II presents our system model
with vehicle node distributions, SDV2I communication and
problem statements. The proposed RL solution is described in
Section III followed by results and discussions in Section IV.
Finally, Section V concludes the paper.

II. SYSTEM MODEL AND PROBLEM FORMULATION

We present our software-deﬁned system model, followed
by the node distributions, communication model and dynamic
user-centric cell formation problem in this section.

A. Software-Deﬁned System Model

We consider highly mobile autonomous VUs are moving on
the road. Let us denote the VU set by U = {u1, u2, . . . , uU },
where U ∈ Z+. Various low-powered APs are deployed in the
surrounding geographic region. Let the set of APs be denoted
by A = {a1, a2, . . . , aA}, where A ∈ Z+. The VUs move on the
road and each VU establishes its connection with the network
via these APs. We assume that all of the APs are physically
connected to edge servers. Note that, in reality, the number of
such connections will be conﬁned into a small group due to

Fig. 1. User-centric vehicular edge networking.

geographic locations of these edge nodes. Furthermore, each
of the edge servers is controlled, programmed and operated
by its respective AN. Let us denote AN by bl ∈ B. In other
words, there exists |B| numbers of edge servers each of which
can be expressed by the identical notation of its AN. Each
of the edge servers has a ﬁxed and limited radio spectrum
assigned by the cloud server. Let Wl [Hz] denote the available
radio resources of the edge server bl. Considering an open-
loop communication system, we assume that the channel state
information (CSI) is perfectly known at each of the ANs. The
ANs can form and schedule the beamforming weights of the
APs for each of the virtual cells. Therefore, our system model
is based on sophisticated SofAir [15], where the ANs control,
create and assign resource slices to the APs based on user’s
demands and thus, enhances the overall spectral efﬁciency.

In order to ensure reliability and ubiquitous connectivity,
we assume a VU can associate with multiple APs that are in
its communication region. That is, in comparison to the con-
ventional network-centric approach, we consider user-centric
communication by formulating virtual cells for the scheduled
users by associating each of them to multiple APs. Some
patterns of forming virtual cells are shown by the dotted
ellipses in Fig. 1. Without any loss of generality, let us denote
a user and an AP by ui ∈ U and a j ∈ A , respectively. Due to
a user-centric approach, let Ai(t) denote the set of APs that a
VU ui can associate to, in time slot t. Furthermore, we denote
the VU-AP association by the following indicator function:

(cid:40)

a j
i (t) =

1,
0, otherwise.

if AP a j is associated with VU ui

(1)

Edge	ServerAPAPAPAPAPAnchor	NodeEdge	ServerAnchor	NodeCloud	ServerVirtual	Cell	2Virtual	Cell	4Virtual	Cell	1Virtual	Cell	3Moreover, an AP might be connected to multiple VUs at the
same time. Let us further denote the set of VUs connected to
an AP a j at time slot t by the set U j(t). We then have the
following indicator function:
(cid:40)

if VU ui is associated with AP a j

ui

j(t) =

1,
0, otherwise.

(2)

Hence, Ai(t) denotes the set of APs that VU ui is connected
to whereas, U j(t) denotes the set of VUs connected to the AP
a j in a given time slot.

B. Road Model and Node Distributions

We consider a straight three-lane one-way road structure
without any intersection as our region of interest (ROI). Specif-
ically, we consider the freeway case of 3GPP [16]. However,
our modeling can be extended to complex practical road
modeling. We are interested in establishing a communication
framework for the V2X platform where the vehicle models
are independent of road structure. The lane can be denoted
by Lm, where m ∈ {1, 2, 3} in our case. We deploy VUs and
APs following uniform distributions while maintaining a safety
distance between two VUs. The method of updating mobility
is described in Algorithm 1. Note that the Doppler effect has
not considered in this paper and will be examined in our future
work.

Algorithm 1 Vehicular Road Trafﬁc Modeling
1: Input: Total number of VUs and lanes
2: for each time step, t = 1, 2, . . . , Tn do
3:
4:
5:

for each vehicle do

for each lane do

Update vehicle’s position by adding linear displace-
(cid:46) If new position is outside the

ment based on its velocity
ROI, TERMINATE

end for

end for
Return: All vehicles’ positions

6:
7:
8:
9: end for

C. Software-Deﬁned Vehicle-to-Infrastructure Communication

We assume that the VUs are equipped with a single antenna,
whereas, each of the low-powered APs is equipped with a N j
number of antennas. Unless mentioned otherwise, we assume
an omnidirectional antenna for both VU and APs. Further-
more, with a slight abuse of notation, we represent the set of
APs in Ai(t) by the set
, where
| · | represents cardinality of set Ai(t). Moreover, |Ai(t)| ≤ A
i.e., the total number of APs assigned to the virtual cell i of
VU ui has to be less than or equal to the total number of
available APs in the network for all time slot t.

a1,i(t), a2,i(t), . . . , a|A j|,i(t)

(cid:110)

(cid:111)

We model the wireless channel, between AP and VU, as an
independent quasi-static ﬂat fading model during a basic time
block. Let us denote the channel response at a VU ui from the
(t)]T , where hi jp(t)
AP a j by hai
denotes the channel between ui and the pth antenna of AP a j

(t) = [hi j1(t), hi j2(t), . . . , hi jN j

j

at time t. Then, the channel response at the VU ui from all
APs is represented as follows:

(cid:105)T

(cid:104)
hT
ai
1

(t)

hi(t) =

(t), hT
ai
2

(t), . . . , hT
ai
A

= Di(t)ρi(t)τi(t) ∈ CN×1,
(3)
where Di(t), ρi(t) and τi(t) ∼ CN (0, IN) are large scale fading,
log-Normal shadowing and fast fading channel vectors, respec-
tively. Note that channels are modeled following appropriate
measures listed in [16]. Moreover, N = ∑A
j=1 N j denotes the
total number of antennas in all APs.

(t)

(cid:104)
wT
ai
1

(t), . . . , wT
ai
A

We assume linear downlink beamforming in our SD
V2I platform. Let us denote the beamforming vector for
(t) ∈ CN j×1
VU ui at AP a j by wai
in time t. Then,
j
to VU ui can be denoted by
the beamforming vector
(cid:105)T
wi(t) ∆=
∈ CNi×1. Furthermore, the entire
network beamforming design can be denoted by W(t) ∆=
{w1(t), . . . , wU (t)}. Now, let us also denote the unit powered
intended signal for VU ui by xi(t) ∈ C. Therefore, we have to
satisfy E[xH
i (t)xi(t)] = 1 all the time. Furthermore, applying
the beamforming vector, the transmitting signal of the AP a j
is denoted as s j(t) = ∑U
(t)xi(t). Therefore, the received
signal at the VU ui is then calculated as follows:

i=1 wai

j

(t)s j(t) + ηi(t)

hH
ai
j

yi(t) = ∑
a j∈A
i (t)wi(t)xi(t)
(cid:125)
(cid:123)(cid:122)
desired signal

= hH
(cid:124)

+ ∑

ui(cid:48) ∈U \ui
(cid:124)

hH
i (t)wi(cid:48)(t)xi(cid:48)(t)

(cid:123)(cid:122)
interference

(cid:125)

+ ηi(t)
,
(cid:124)(cid:123)(cid:122)(cid:125)
noise

(4a)

(4b)

where ηi(t) ∼ CN (0, 1) denotes the received noise which
is zero mean circularly symmetric Gaussian distributed with
variance σ 2.

D. User-Centric Dynamic Cell Formation

With our analysis and vehicular trafﬁc modeling, as pre-
sented in II-B, we now aim to derive the instantaneous
achievable rate at the VU. At ﬁrst, we calculate the signal-
to-interference-plus-noise ratio (SINR) as follows:

(cid:12)
(cid:12)hH

i (t)wi(t)(cid:12)
2
(cid:12)
(cid:12)
(cid:12)hH

γt
i (W(t)) =

(5)

σ 2 + ∑ui(cid:48) ∈U \ui
Considering time division duplexing (TDD) operated system,
we thus calculate the instantaneous achievable data rate for
VU ui as follows:

i (t)wi(cid:48)(t)(cid:12)
2
(cid:12)

Rt

i (W(t)) = (1 − κ) log2

(cid:0)1 + γt

i (W(t))(cid:1) ,

(6)

where κ represents spectral efﬁciency loss due to signaling at
the APs. Note that if a user is scheduled during the transmis-
sion time slot, the beamforming vector wi(t) is nonzero, thus
the rate is nonzero as well. On the other hand, if a user is not
scheduled, the beamforming vector for that user should be zero
leading to a zero achievable rate. Furthermore, as multiple APs
are serving each of the scheduled users, the backhaul resource
consumption by each of those users should also be carefully

calculated. As such, next, we calculate the backhaul resource
consumption [17] as follows:
(cid:13)
(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)
(cid:13)wai

i (W(t)) ,

(cid:13)
(cid:13)
(cid:13)wai

Ci(t) =

, . . . ,

(7)

(t)

(t)

Rt

(cid:13)
(cid:13)
(cid:13)2

(cid:105)(cid:13)
(cid:13)
(cid:13)0

(cid:13)
(cid:13)
(cid:13)2

A

1

where (cid:107)·(cid:107)0 represents the total number of nonzero elements
in a vector and commonly referred as l0-norm.

Now, due to resource constraints, we may not serve all users
at the same time. Therefore, there will be certain restrictions
on the number of active users in the VU set U . However,
recall that our formulated rate calculation in equation (6) can
also be used to design the user scheduling. We intend to serve
all active users in U in a transmission time slot by forming
virtual cells for each of the users and dynamically selecting the
transmission power of the APs. We aim to ﬁnd optimal user-
centric cell formation and beamforming weights calculation
for the APs in our objective function. The question that we
try to answer is - what are the optimal user associations
and APs transmission powers that maximize the throughput
in our SD controlled highly mobile vehicular network? A
naive approach would be serving a user from as many APs as
possible with the maximum transmission powers of the APs.
However, transmitting to a particular VU from the APs with
maximum transmission power will severely impact the SINR
performances of the other active VUs. Therefore, it is essential
to know the optimal transmission power of the APs for each
of the active VUs.

We present our joint optimization problem as follows:

Find:

Maximize

Subject to

(t), ∀i ∈ U , j ∈ A

j

j(t), wai
ζi(t)Ci(t)

a j
i (t), ui
∑
i∈U
1 ≤ |Ai(t)| ≤ A, ∀i ∈ U
i (W(t)) ≥ γ min
γt
, ∀i ∈ U
U
∑
i=1
a j
i (t) ∈ {0, 1}, ui

(cid:13)
(cid:13)
(cid:13)wai

≤ Pmax
j

(cid:13)
2
(cid:13)
(cid:13)
2

(t)

i

j

j(t) ∈ {0, 1},

, ∀ j ∈ A

(8a)

(8b)

(8c)

(8d)

(8e)

i

j

and Pmax

where ζi(t), γ min
are the weights of the data rate
of user ui at time t, minimum SINR requirement for reliable
communication and maximum allowable transmit power of AP
a j, respectively. Note that constraint (8b) ensures that our user-
centric virtual cell contains more than one AP to form the
cluster. Constraint (8c) ensures the obtained SINR is greater
than a minimum threshold. Moreover, constraint (8d) limits
the total transmit power of AP a j to be at maximum Pmax

.

j

Due to the l0 norm, the formulated problem is not suitable
to be solved using a gradient-based algorithm. Furthermore,
due to the combinatorial nature of the originated problem, it
is extremely hard to solve within a short period. Note that if
we know all ai
j(t)s using
equations (1-2). Therefore, for each of the AP, there are 2U −
1 possible combinations, only for the VU-AP associations,
in a single time-slot. Besides, for each of these associations,
the AP has to select the power level for each of the active

j(t)s - ∀ j ∈ A , we can ﬁgure out the ui

users. Moreover, in a centralized controlled environment, the
centralized agent needs to make a central decision for all such
AP-VU associations and power levels selection pairs. As such,
it is obvious that as the number of APs and VUs increases the
complexities increases exponentially. Therefore, we employ
an elegant machine learning approach to solve the problem in
what follows.

III. REINFORCEMENT LEARNING-BASED VEHICULAR
EDGE SLICING MECHANISM

In this section, we discuss the problem-solution from the RL
perspective. We ﬁrst clearly state the state, action, and reward
of the RL agent. Then we present our learning algorithms.

A. State

The state-space contains all channel state information Ht .
It also contains the geographic locations of the VUs and APs.
Let us denote the positions of the VUs and APs at state st by
Xt
i and Xt
j, respectively. Therefore, we denote our state space
as follows:
j, Ht (cid:9) .

st = (cid:8)Xt

i, Xt

(9)

B. Action

The action space contains the VU-AP association indicator
functions followed by the beamforming vectors for the selected
VU-AP associations.

at =

(cid:110)

ai
j(t) ∀i ∈ U , wai

j

(t) ∀ j ∈ A

(cid:111)

(10)

Notice that, in equation (10), the RL agent needs to take two
action sets i.e., VU-AP associations and beamforming weights.
However, both of these actions cannot be performed at the
same time. Simply put, for deciding the optimal beamforming
weights, the agent needs to know the VU-AP associations.
Therefore, the action set can be thought of as a twin step
process. At ﬁrst, the RL agent needs to decide the VU-AP
associations. Based on that decision, it then can allocate each
AP’s transmission power for all of the VU that are being
served by the respective AP. Therefore, we divide the action
space into twin scales as described below.

Considering an open-loop system - one-shot transmission,
to ensure reliability and increase the user data rate, we have
considered serving a scheduled VU from multiple APs. If a
user is scheduled to be served, we then design the beamform-
ing vectors of the APs for that particular user. Now, note that
we assume perfect CSI is known at the AN. As such, we model
the beamforming weights using the following equation:

(t) =

wai

j

hai

(cid:13)
(cid:13)
(cid:13)hai

j

j

(t)
(cid:13)
(cid:13)
(cid:13)2

(t)

×

(cid:113)

Pai

j

(t),

(11)

j

(t) is the wireless channel information from AP a j
where hai
to VU ui and Pai
(t) is the allocated transmission power of AP
a j to transmit to VU ui. Therefore, we rewrite the action space
as:

j

at =

(cid:110)

ai
j(t) ∀i ∈ U , Pai

j

(t) ∀i ∈ U & j ∈ A

(cid:111)

.

(12)

j

Now, instead of continuous power level, we divide the APs
transmission power level into multiple discrete levels. Particu-
(t) ∈ {5, 10, 15, 20}
(t) into 4 levels (e.g., Pai
larly, we divide Pai
dBm). In other words, each AP can select its transmission
power to serve a user from one of these power levels. There-
fore, our objective function is still the same as presented in
equation (8). We formulate the beamforming vectors using the
optimal power selection and equation (11).

j

C. Reward

Without any loss of generality, we deﬁne the weighted sum
rate as the reward function for our learning algorithm. We
also ensure that each of the users receives the minimum SINR
threshold for a chosen action; otherwise, we return a zero
reward. Accordingly, the reward function is given as follows:

rt =

(cid:40)

∑i∈U ζi(t)Ci(t),
0,

i (W(t)) > γ min

i

if γt
otherwise.

∀i ∈ U

(13)

D. Single Agent Reinforcement Learning (SARL)

Q-learning is a model-free RL framework [18] which takes
the state and action into account and solve hard optimization
problem such as equation (8) efﬁciently. In each state st the
agent takes an action at from which it gets a reward rt and
the environment transit to the next state st+1. The governing
equation of Q-learning is shown in the following:

Q(st , at )←(1−α)Q(st , at )+α

rt + γ max

Q(st+1, a)

, (14)

(cid:16)

(cid:17)

a

where α and γ are learning rate and discount factor, re-
spectively. Our learning algorithm for SARL is presented in
Algorithm 2.

Algorithm 2 Q - learning based RL Algorithm
1: Initialize: For all possible states and actions generate random

Q(st , at ) table

2: for each episode do
3:
4:

Initiate the environment
Generate st =
while not terminated do

i, Xt
Xt

(cid:110)

j, Ht (cid:111)

5:
6:
7:
8:
9:
10:
11: end for

Take action using ε-greedy policy
Calculate reward, rt , transit to next state st+1
Update Q table following equation (14)
st ← st+1

end while

(cid:46) If st is the terminal state

where j = 1, . . . , N represents the cooperating agents. The
update rule for the nth agents follows the following equation
[19]:

Qn(sn, an)←(1−α)Qn(sn, an)+α

(cid:18)
rn(sn, ¯a) + γ max Qn
b∈An

(s(cid:48)

(cid:19)
n, b)
(16)

.

F. Distributed SARL with Multi-agent Learning (SARL-MARL
Collaboration)

While SARL is the baseline RL scheme, it may suffer to
attain the best performance if the action and state space are
too large. In such a platform MARL can be used to shrink the
number of action space. However, whether MARL will achieve
the optimal performance is still questionable as each agent
takes its decision independently. Although, Liu et. al. [19]
have considered collaboration among the agents, we validate
through simulation results that this scheme fails to attain the
best performance in our speciﬁc problem. As an alternative,
we incorporate the concept of distributed learning. Since a
single agent RL framework has to evaluate all possible actions,
we consider dividing the action space into multiple agents.
We use individual Q-table and keep track of the global best
performance. Note that the number of possible states for all
agents are still the same as SARL, we are only evaluating the
performance from segmented action spaces.

Let us denote the number of agents by N. Then,
the
dimension of the Q-table for each of the agents is RS×A/N,
where S and A represents the size of the state space and action
space, respectively. Note that we selected N such a way that
A/N ∈ Z+. In each of the state st , each of these N agents takes
their action following the same methodology as the baseline
Q-learning. Let us denote Qcentral ∈ RS a centralized vector
that stores the global best action in each of the states. In each
of the time step, we update the Qcentral using the following
equation:

Qcentral[at ] ←

(cid:40)

Qcentral[at ], if any rt [at ] > rt [aold
Qcentral[aold
], otherwise.

t

t

], ∀agents ∈ N

(17)
Note that each of the agents follows and updates its action
and Q-table according to the baseline SARL. The detailed
procedure of our distributed SARL with multi-agent learning
is presented in Algorithm 3.

E. Multi-agent Reinforcement Learning (MARL)

In MARL, multiple agents can take their individual actions
and get an overall centralized reward for their joint decisions.
Liu et. al. have recently presented a MARL framework in [19]
in which each agent has its own Q-table and can take action
independently. Particularly, following the ε-greedy policy, if
the value of the random variable is greater than ε, the authors
have chosen the action of the nth agent using the following
equation:

at
n = arg maxa

(cid:32)

∑
1≤ j≤N

(cid:33)

Qt

j(st

j, a)

,

(15)

IV. PERFORMANCE EVALUATION

We consider 500 meters of a three-lane one-way freeway
as our ROI. Vehicle’s new position is generated by adding
linear displacement with VU velocity of 140 km/h as per [16].
The channel model and related parameters are also chosen as
speciﬁed in [16]. In order to keep a tractable sate space, we
consider updating it after every 100 milliseconds. Furthermore,
for the ease of simulation, we consider a full buffer network
model in which each of the APs serves all VUs simultaneously.
Note that our proposed problem solution can work in other
scheduling algorithms as well. As we are not adopting any
actual scheduling schemes, we consider full buffer scenarios

(a) Performance comparisons of different schemes

(b) User fairness

Fig. 2. Performance comparisons: AP coverage radius = 250 m, γ min

i = 10 dB.

Algorithm 3 SARL-MARL: Distributed SARL
1: Initialize: Total number of agents, N, divide the large action

space into N small sets.

2: For all possible states and action sets generate random Ql(st , at )

tables, where l ∈ N represents the agent id.

3: Generate Qcentral ∈ RS randomly
4: for each episode do
5:

Initiate the environment, generate st =
while not terminated do
for each l ∈ N do

6:
7:
8:

(cid:110)

i, Xt
Xt

j, Ht (cid:111)

Observe the environment; choose at , based on the
observation, following ε-greedy policy; receive reward rt ; update
its Q-table using equation (14)

if rt > reward using Qcentral[at ]

then
update Qcentral using equation (17)

end if

9:
10:
11:
12:
13:
14:
15: end for

end for
st ← st+1

end while

(cid:46) If st is the terminal state

with ζi(t) = 1, ∀i ∈ U & t. We consider 1 AN, 3 APs and
3 users. While the VUs are dropped uniformly in each lane,
the APs are placed 150 meters apart ﬁxed locations. We have
assumed 8 antennas per AP. For a tractable state space, we
have considered that, at a given time step, all VUs are in the
same x locations - while they have different y locations. We
i = 10 dB ∀i ∈ U & t and κ = 0.1.
also have γ min

For the SARL, MARL and SARL-MARL collaborative
algorithms, we have considered γ = 0.8. Besides, the value
of both ε and α are decayed linearly from 1 to 0.01 in
each episode. Also, we have considered 3 APs as independent
agents for the MARL algorithm of Liu et. al. [19] and we
have used N = 4 for our SARL-MARL distributed learning
algorithm. For both SARL and MARL cases, we have trained
our models for 1×105 episodes while we only train our model

/U dBm power.

in 2.5 × 104 episodes in the SARL-MARL distributed learning
case. In order to validate the results of each of these algo-
rithms, we have compared the results with the genie-aided:
optimal solution. We have also used two baseline schemes,
namely, (1) random power: each AP randomly choose its
transmitting power from {5, 5, 15} and (2) equal power: each
AP transmit to all VUs using Pmax

j
We compare the average rewards of all of these schemes
after running 250 test episodes in Fig. 2a. Our proposed
SARL-MARL, baseline SARL, and MARL algorithms deliver
average per VU reward of 29.68, 30.11, and 29.46 bits/sec/Hz.
These are nearly identical to the genie-aided optimal per VU
reward of 30.12 bits/sec/Hz. However, recall that we have
trained both SARL and MARL for 105 episodes, whereas
the proposed SARL-MARL distributed learning algorithm has
been trained on only 2.5 × 104 episodes. The performance
of the equal and random power allocation schemes are, as
expected, worse than the RL schemes. With regard to user
fairness, our RL schemes and reward function always ensure
that each of the users receives a fair data rate. Since we return
zero rewards in the case that any of the user’s SINR is below
the threshold level, we expect that our RL agents learn this
pattern quickly. This is also veriﬁed by our simulation results
in Fig. 2b.

in our optimization problem, we have set

We further consider the impact of the reliability threshold.
the
Note that,
that each of the users has to get a
reliability constraint
minimum SINR threshold to ensure reliability. This, therefore,
guarantees that regardless of the environment, our RL will
learn to allocate resources such a way that each of the VUs
receives at least γ min
SINR. Furthermore, Fig. 3 indicates that
as this threshold increases, the network should experience
difﬁculties in achieving this demand for all of the users.
Both Figs. 3a-3b conﬁrm that our proposed distributed SARL

i

Genie-AidedSARL-MARLSARLMARL [19]Equal PaijRandom PaijDifferent Schemes7075808590WSR [bits/sec/Hz]90.371189.030390.331188.3774.650274.1577Comparison of the RewardUser 1User 2User 3User Index1015202530Rate [bits/s/Hz]User FairnessOptimalSARL-MARLSARLMARL [19](a) SINR threshold vs. WSR

(b) SINR threshold vs. success probability

Fig. 3.

Impact of the SINR threshold.

multi-agent learning (SARL-MARL) algorithm reaches near-
optimal results compare to that of Liu et. al.’s MARL and
other baseline schemes. The performance gap between the
optimal and the proposed SARL-MARL results are very close.
Besides, the gap between our proposed solution and other
baseline schemes are hugely evident as the reliability threshold
increases.

Finally, we show the impact of the coverage radius of the
APs in Fig. 4. Note that a VU can only be served if its in the
coverage region of the AP. Therefore, as the coverage radius
of the AP increases, more VUs can be served by that AP.
As such, the expected sum rate of the user should increase,
if the power levels of the APs are chosen appropriately with
the increase of its coverage radius. This is also reﬂected and
validated in our result presented in Fig. 4.

V. CONCLUSION
In this paper, we have presented an efﬁcient way to dynam-
ically allocate the transmission power of the APs and virtual
cell formation for the VUs in user-centric vehicular networks.
Using well-bred machine learning algorithms, we have demon-
strated that the original hard combinatorial problem can be
solved efﬁciently. Furthermore, as the numbers of possible
states and actions increase, the traditional SARL suffers to
behave optimally due to the curse of dimensionality. While
MARL requires quite a large number of training episodes to
attain near-optimal performance, the proposed SARL-MARL
can achieve similar performance within a nominal number of
training episodes.

REFERENCES

[1] H. Hartenstein and L. P. Laberteaux, “A tutorial survey on vehicular ad
hoc networks,” IEEE Commun. Mag., vol. 46, no. 6, pp. 164–171, June
2008.

[2] D. Jiang, V. Taliwal, A. Meier, W. Holfelder, and R. Herrtwich, “Design
of 5.9 ghz dsrc-based vehicular safety communication,” IEEE Wireless
Commun., vol. 13, no. 5, pp. 36–43, Oct. 2006.

Fig. 4.

Impact of the coverage radius of the access point.

[3] H. Seo, K. Lee, S. Yasukawa, Y. Peng, and P. Sartori, “Lte evolution
for vehicle-to-everything services,” IEEE Commun. Mag., vol. 54, no. 6,
pp. 22–28, June 2016.

[4] R. Molina-Masegosa and J. Gozalvez, “Lte-v for sidelink 5g v2x
vehicular communications: A new 5g technology for short-range vehicle-
to-everything communications,” IEEE Veh. Technol. Mag., vol. 12, no. 4,
pp. 30–39, Dec. 2017.

[5] G. Araniti, C. Campolo, M. Condoluci, A. Iera, and A. Molinaro, “Lte
for vehicular networking: a survey,” IEEE Commun. Mag., vol. 51, no. 5,
pp. 148–157, May 2013.

[6] K. Z. Ghafoor, M. Guizani, L. Kong, H. S. Maghdid, and K. F. Jasim,
“Enabling efﬁcient coexistence of dsrc and c-v2x in vehicular networks,”
IEEE Wireless Commun., 2019.

[7] J. Heo, B. Kang, J. M. Yang, J. Paek, and S. Bahk, “Performance-cost
tradeoff of using mobile roadside units for v2x communication,” IEEE
Trans. Veh. Technol., vol. 68, no. 9, pp. 9049–9059, Sep. 2019.

[8] T. Sahin, M. Klugel, C. Zhou, and W. Kellerer, “Virtual cells for 5g v2x
communications,” IEEE Commun. Stand. Mag., vol. 2, no. 1, pp. 22–28,

0.02.55.07.510.012.515.017.520.0mini [dB]5560657075808590WSR [bits/sec/Hz]WSR vs. SINR ThresholdGenie-AidedSARL-MARLSARLMARL [19]Equal PowerRandom Power051090920.02.55.07.510.012.515.017.520.0mini [dB]0.550.600.650.700.750.800.850.900.95Probability of SuccessProbability of Success vs. SINR ThresholdGenie-AidedSARL-MARLSARLMARL [19]Equal PowerRandom Power05100.9000.9250.950150200250300350400AP Coverage Radius [m]405060708090100110120WSR [bits/sec/Hz]WSR vs. AP Coverage RadiusGenie-AidedSARL-MARLSARLMARL [19]Equal PowerRandom Power2502753003253509095100105March 2018.

[9] T.-H. Li, M. R. Khandaker, F. Tariq, K.-K. Wong, and R. T. Khan,
“Learning the wireless v2i channels using deep neural networks,” in
Proc. VTC2019-Fall.
IEEE, Sep. 2019, pp. 1–5.

[10] S.-Y. Lien, S.-C. Hung, D.-J. Deng, C.-L. Lai, and H.-L. Tsai, “Low
latency radio access in 3gpp local area data networks for v2x: Stochastic
optimization and learning,” IEEE Internet Things J., June 2018.
[11] Y. He, N. Zhao, and H. Yin, “Integrated networking, caching, and com-
puting for connected vehicles: A deep reinforcement learning approach,”
IEEE Trans. Veh. Technol., vol. 67, no. 1, pp. 44–55, Jan. 2018.
[12] L. T. Tan and R. Q. Hu, “Mobility-aware edge caching and computing
in vehicle networks: A deep reinforcement learning,” IEEE Trans. Veh.
Technol., vol. 67, no. 11, pp. 10 190–10 203, Nov. 2018.

[13] H. Ye, G. Y. Li, and B.-H. F. Juang, “Deep reinforcement learning based
resource allocation for v2v communications,” IEEE Trans. Veh. Technol.,
vol. 68, no. 4, pp. 3163–3173, Apr. 2019.

[14] L. Liang, H. Ye, and G. Y. Li, “Spectrum sharing in vehicular networks
based on multi-agent reinforcement learning,” IEEE J. Sel. Areas in
Commun., vol. 37, no. 10, pp. 2282–2292, Oct. 2019.

[15] S.-C. Lin, “End-to-end network slicing for 5g&b wireless software-

deﬁned systems,” in Proc. of IEEE GLOBECOM, Dec. 2018.

[16] “3rd Generation Partnership Project; Technical Speciﬁcation Group
Radio Access Network; Study on LTE-based V2X Services,” 3GPP TR
36.885 V14.0.0, Release 14, Jun. 2016.

[17] B. Dai and W. Yu, “Sparse beamforming for limited-backhaul network
mimo system via reweighted power minimization,” in Proc. IEEE
GLOBECOM, Dec. 2013.

[18] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no.

3-4, pp. 279–292, 1992.

[19] X. Liu, Y. Liu, Y. Chen, and L. Hanzo, “Trajectory design and power
control for multi-uav assisted wireless networks: A machine learning
approach,” IEEE Trans. Veh. Technol., vol. 68, no. 8, pp. 7957–7969,
Aug. 2019.

