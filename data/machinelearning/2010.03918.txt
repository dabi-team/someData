0
2
0
2

t
c
O
9

]

G
L
.
s
c
[

2
v
8
1
9
3
0
.
0
1
0
2
:
v
i
X
r
a

Near-Optimal Comparison Based Clustering

Michaël Perrot
Univ Lyon, UJM-Saint-Etienne, CNRS, IOGS,
LabHC UMR 5516, F-42023, SAINT-ETIENNE, France
michael.perrot@univ-st-etienne.fr

Pascal Mattia Esser,

Debarghya Ghoshdastidar

Department of Informatics
Technical University of Munich
{esser,ghoshdas}@in.tum.de

Abstract

The goal of clustering is to group similar objects into meaningful partitions. This
process is well understood when an explicit similarity measure between the objects
is given. However, far less is known when this information is not readily available
and, instead, one only observes ordinal comparisons such as “object i is more simi-
lar to j than to k.” In this paper, we tackle this problem using a two-step procedure:
we estimate a pairwise similarity matrix from the comparisons before using a clus-
tering method based on semi-deﬁnite programming (SDP). We theoretically show
that our approach can exactly recover a planted clustering using a near-optimal
number of passive comparisons. We empirically validate our theoretical ﬁndings
and demonstrate the good behaviour of our method on real data.

1

Introduction

In clustering, the objective is to group together objects that share the same semantic meaning, that
are similar to each other, into k disjoint partitions. This problem has been extensively studied in the
literature when a measure of similarity between the objects is readily available, for example when
the examples have a Euclidean representation or a graph structure (Shi and Malik, 2000; Arthur and
Vassilvitskii, 2007; von Luxburg, 2007). However, it has attracted less attention when the objects
are difﬁcult to represent in a standard way, for example cars or food. A recent trend to tackle this
problem is to use comparison based learning (Ukkonen, 2017; Emamjomeh-Zadeh and Kempe, 2018)
where, instead of similarities, one only observes comparisons between the examples:
Triplet comparison: Object xi is more similar to object xj than to object xk;
Quadruplet comparison: Objects xi and xj are more similar to each other than objects xk and xl.
There are two ways to obtain these comparisons. On the one hand, one can adaptively query them
from an oracle, for example a crowd. This is the active setting. On the other hand, they can be
directly given, with no way to make new queries. This is the passive setting. In this paper, we study
comparison based learning for clustering using passively obtained triplets and quadruplets.

Comparison based learning mainly stems from the psychometric and crowdsourcing literature (Shep-
ard, 1962; Young, 1987; Stewart et al., 2005) where the importance and robustness of collecting
ordinal information from human subjects has been widely discussed. In recent years, this framework
has attracted an increasing amount of attention in the machine learning community and three main
learning paradigms have emerged. The ﬁrst one consists in obtaining an Euclidean embedding of
the data that respects the comparisons as much as possible and then applying standard learning
techniques (Borg and Groenen, 2005; Agarwal et al., 2007; Jamieson and Nowak, 2011; Tamuz et al.,
2011; van der Maaten and Weinberger, 2012; Terada and von Luxburg, 2014; Zhang et al., 2015;

Preprint. Under review.

 
 
 
 
 
 
Amid and Ukkonen, 2015; Arias-Castro, 2017). The second paradigm is to directly solve a speciﬁc
task from the ordinal comparisons, such as data dimension or density estimation (Kleindessner and
von Luxburg, 2015; Ukkonen et al., 2015), classiﬁcation and regression (Haghiri et al., 2018), or
clustering (Vikram and Dasgupta, 2016; Ukkonen, 2017; Ghoshdastidar et al., 2019). Finally, the
third paradigm is an intermediate solution where the idea is to learn a similarity or distance function,
as in embedding approaches, but, instead of satisfying the comparisons, the objective is to solve one
or several standard problems such as classiﬁcation or clustering (Kleindessner and von Luxburg,
2017). In this paper, we focus on this third paradigm and propose two new similarities based on
triplet and quadruplet comparisons respectively. While these new similarities can be used to solve any
machine learning problem, we show that they are provably good for clustering under a well known
planted partitioning framework (Abbe, 2017; Yan et al., 2018; Xu et al., 2020).

O

(cid:0)n3(cid:1) different triplets and

Motivation of this work. A key bottleneck in comparison based learning is the overall number of
(cid:0)n4(cid:1) different
available comparisons: given n examples, there exist
quadruplets. In practice, it means that, in most applications, obtaining all the comparisons is not
realistic. Instead, most approaches try to use as few comparisons as possible. This problem is relatively
easy when the comparisons can be actively queried and it is known that Ω (n ln n) adaptively selected
comparisons are sufﬁcient for various learning problems (Haghiri et al., 2017; Emamjomeh-Zadeh
and Kempe, 2018; Ghoshdastidar et al., 2019). On the other hand, this problem becomes harder
when the comparisons are passively obtained. The general conclusion in most theoretical results on
(cid:0)n4(cid:1)
learning from passive ordinal comparisons is that, in the worst case, almost all the
comparisons should be observed (Jamieson and Nowak, 2011; Emamjomeh-Zadeh and Kempe, 2018).
The focus of this work is to show that, by carefully handling the passively obtained comparisons,
it is possible to design comparison based approaches that use almost as few comparisons as active
approaches for planted clustering problems.

(cid:0)n3(cid:1) or

O

O

O

Near-optimal guarantees for clustering with passive comparisons. In hierarchical clustering,
Emamjomeh-Zadeh and Kempe (2018) showed that constructing a hierarchy that satisﬁes all compar-
isons in a top-down fashion requires Ω (cid:0)n3(cid:1) passively obtained triplets in the worst case. Similarly,
Ghoshdastidar et al. (2019) considered a planted model and showed that Ω (cid:0)n3.5 ln n(cid:1) passive quadru-
plets sufﬁce to recover the true hierarchy in the data using a bottom-up approach. Since the main
difﬁculty lies in recovering the small clusters at the bottom of the tree, we believe that this latter
result also holds for standard clustering. In this paper, we consider a planted model for standard
clustering and we show that, when the number of clusters k is constant, Ω (cid:0)n(ln n)2(cid:1) passive triplets
or quadruplets are sufﬁcient for exact recovery. This result is comparable to the sufﬁcient number of
active comparisons in most problems, that is Ω (n ln n) (Haghiri et al., 2017; Emamjomeh-Zadeh and
Kempe, 2018). Furthermore, it is near-optimal as to cluster n objects it is necessary to observe all the
examples at least once and thus have access to at least Ω (n) comparisons. Finally, to obtain these
results, we study a semi-deﬁnite programming (SDP) based clustering method and our analysis could
be of signiﬁcant interest beyond the comparison based framework.

General noise model for comparison based learning. In comparison based learning, there are two
main sources of noise. First, the observed comparisons can be noisy, that is the observed triplets and
quadruplets are not in line with the underlying similarities. This noise stems, for example, from the
randomness of the answers gathered from a crowd. It is typically modelled by assuming that each
observed comparison is randomly (and independently) ﬂipped (Jain et al., 2016; Emamjomeh-Zadeh
and Kempe, 2018). This is mitigated in the active setting by repeatedly querying each comparison,
but may have a signiﬁcant impact in the passive setting where a single instance of each comparison
is often observed. Apart from the aforementioned observation errors, the underlying similarities
may also have intrinsic noise. For instance, the food data set by Wilber et al. (2014) contains triplet
comparisons in terms of which items taste more similar, and it is possible that the taste of a dessert is
closer to a main dish than to another dessert. This noise has been considered in Ghoshdastidar et al.
(2019) by assuming that every pair of items possesses a latent random similarity, which affects the
responses to comparisons. In this paper, we propose, to the best of our knowledge, the ﬁrst analysis
that considers and shows the impact of both types of noise on the number of passive comparisons.

Scalable comparison based similarity functions. Several similarity and kernel functions have been
proposed in the literature (Kleindessner and von Luxburg, 2017; Ghoshdastidar et al., 2019). However,
computing these similarities is usually expensive as they require up to
(n) passes over the set of
available comparisons. In this paper, we propose new similarity functions whose construction is much
more efﬁcient than previous kernels. Indeed, they can be obtained with a single pass over the set of

O

2

available comparisons. It means that our similarity functions can be computed in an online fashion
where the comparisons are obtained one at a time from a stream. The main drawback compared to
existing approaches is that we lose the positive semi-deﬁniteness of the similarity matrix, but our
theoretical results show that this is not an issue in the context of clustering. We also demonstrate this
empirically as our similarities obtain results that are comparable with state of the art methods.

2 Background and theoretical framework

In this section, we present the comparison based framework and our planted clustering model, under
which we later show that a small number of passive comparisons sufﬁces for learning. We consider
the following setup. There are n items, denoted by [n] =
, and we assume that, for
1, 2, . . . , n
{
every pair of distinct items i, j
[n], there is an implicit real-valued similarity wij that we cannot
directly observe. Instead, we have access to

∈

}

Triplets:

Quadruplets:

T

= (cid:8)(i, j, r)
∈
= (cid:8)(i, j, r, s)

Q

∈
(cid:0)n4(cid:1) possible quadruplets and

[n]3 : wij > wir, i, j, r distinct(cid:9) ,

[n]4 : wij > wrs, i

= j, r

= s, (i, j)

or
= (r, s)(cid:9) .

(1)

O

There are
a large number of comparisons via crowdsourcing. In practice,
of all possible comparisons. We note that if a triple i, j, r
then either (i, j, r)
or (i, r, j)
when tuples (i, j) and (r, s) are compared, we have either (i, j, r, s)

(cid:0)n3(cid:1) possible triplets, and it is expensive to collect such
only contain a small fraction
Q
[n] is observed with i as reference item,
depending on whether i is more similar to j or to r. Similarly,

or (r, s, i, j)

∈ T

∈ T

or

O

∈

T

∈ Q

.
∈ Q

Sampling and noise in comparisons. This paper focuses on passive observation of comparisons. To
model this, we assume that the comparisons are obtained via uniform sampling, and every comparison
is equally likely to be observed. Let p
(0, 1] denote a sampling rate that depends on n. We
∈
assume that every comparison (triplet or quadruplet) is independently observed with probability p. In
(cid:0)pn3(cid:1), and we can control the sampling rate p to study the
expectation,
=
|T |
or
effect of the number of observations,

, on the performance of an algorithm.

(cid:0)pn4(cid:1) and

|Q|

O

=

O
|Q|

|T |

As noted in the introduction, the observed comparisons are typically noisy due to random ﬂipping of
answers by the crowd workers and inherent noise in the similarities. To model the external (crowd)
(0, 1], we assume that any
noise we follow the work of Jain et al. (2016) and, given a parameter (cid:15)
observed comparison is correct with probability 1
(cid:15)). To
be precise, for observed triple i, j, r

2 (1 + (cid:15)) and ﬂipped with probability 1

[n] such that wij > wir,

2 (1

−

∈

P(cid:0)(i, j, r)

∈ T |

wij > wir

(cid:1) =

, whereas P(cid:0)(i, r, j)

wij > wir

∈ T |

1

(cid:1) =

(cid:15)

.

−
2

(2)

The probabilities for ﬂipping quadruplets can be similarly expressed. We model the inherent noise by
assuming wij to be random, and present a model for the similarities under planted clustering.

Planted clustering model. We now present a theoretical model for the inherent noise in the similari-
ties that reﬂects a clustered structure of the items. The following model is a variant of the popular
stochastic block model, studied in the context of graph clustering (Abbe, 2017), and is related to the
non-parametric weighted stochastic block model (Xu et al., 2020).

We assume that the item set [n] is partitioned into k clusters
k of sizes n1, . . . , nk, re-
C
k are unknown to the
spectively, but the number of clusters k as well as the clusters
C1, . . . ,
C
algorithm. Let Fin and Fout be two distributions deﬁned on R. We assume that the inherent (and
unobserved) similarities

are random and mutually independent, and

C1, . . . ,

∈
1 + (cid:15)
2

wij

C(cid:96) for some (cid:96),
We further assume that wii is undeﬁned, wji = wij, and that for w, w(cid:48) independent,

Fout

Fin

and

wij

∼

∼

∈

otherwise.

Pw,w(cid:48)∼Fin (w > w(cid:48)) = Pw,w(cid:48)∼Fout (w > w(cid:48)) = 1/2,
Pw∼Fin,w(cid:48)∼Fout(w > w(cid:48)) = (1 + δ)/2
for some δ

∈

and

(0, 1].

(3)

The ﬁrst condition in (3) requires that Fin, Fout do not have point masses, and is assumed for
analytical convenience. The second condition ensures that within cluster similarities are larger than
inter-cluster similarities—a natural requirement. Ghoshdastidar et al. (2019) used a special case

3

wij : i < j
{
if i, j

}

(cid:54)
(cid:54)
(cid:54)
of the above model, where Fin, Fout are assumed to be Gaussian with identical variances σ2, and
means satisfy µin > µout . In this case, δ = 2Φ(cid:0)(µin
1 where Φ is the cumulative
−
distribution function of the standard normal distribution.

µout)/√2σ(cid:1)

−

The goal of
this paper is to obtain bounds on the number of passively obtained
triplets/quadruplets that are sufﬁcient to recover the aforementioned planted clusters with
zero error. To this end, we propose two similarity functions respectively computed from triplet and
quadruplet comparisons, and show that a similarity based clustering approach using semi-deﬁnite
programming (SDP) can exactly recover clusters planted in the data using few passive comparisons.

3 A theoretical analysis of similarity based clustering

Before presenting our new comparison based similarity functions, we describe the SDP approach
for clustering from similarity matrices that we use throughout the paper (Yan et al., 2018; Chen and
Yang, 2020). In addition, we prove a generic theoretical guarantee for this approach that holds for
any similarity matrix and, thus, that could be of interest even beyond the comparison based setting.

Similarity based clustering is widely used in machine learning, and there exist a range of popular
approaches including spectral methods (von Luxburg, 2007), semi-deﬁnite relaxations (Yan and
Sarkar, 2016), or linkage algorithms (Dasgupta, 2016) among others. We consider the following SDP
Rn×n be a symmetric similarity matrix among n items, and
for similarity based clustering. Let S
n×k be the cluster assignment matrix that we wish to estimate. For unknown number of
Z
0, 1
∈ {
}
clusters k, it is difﬁcult to directly determine Z, and hence, we estimate the normalised clustering
matrix X
, and Xij = 0 otherwise.
Note that trace (X) = k. The following SDP was proposed and analysed by Yan et al. (2018) under
the stochastic block model for graphs, and can also be applied in the more general context of data
clustering (Chen and Yang, 2020). This SDP is agnostic to the number of clusters, but penalises large
values of trace (X) to restrict the number of estimated clusters:

|C| if i, j co-occur in estimated cluster

Rn×n such that Xij = 1

∈

∈

C

trace (SX)

λ trace (X)

max
X
s.t. X

−
0, X

0, X1 = 1.

≥
Here, λ is a tuning parameter and 1 denotes the vector of all ones. The constraints X
restricts the optimisation to non-negative, positive semi-deﬁnite matrices.

(cid:23)

(SDP-λ)

0 and X

0

(cid:23)

≥

C

×

C1, . . . ,

∈
(cid:96) and
C

We ﬁrst present a general theoretical result for SDP-λ. Assume that the data has an implicit partition
k of sizes n1, . . . , nk and with cluster assignment matrix Z, and suppose that
into k clusters
k block structure (cid:101)S = ZΣZ T .
the similarity S is close to an ideal similarity matrix (cid:101)S that has a k
Rk×k is such that Σ(cid:96)(cid:96)(cid:48) represents the ideal pairwise similarity between items
The matrix Σ
(cid:96)(cid:48). Typically, under a random planted model, (cid:101)S is the same as E[S] up to
from clusters
C
possible differences in the diagonal terms. For S = (cid:101)S and certain values of λ, the unique optimal
Rk×k is diagonal with
solution of SDP-λ is a block diagonal matrix X ∗ = ZN −1Z T , where N
entries n1, . . . , nk (see Appendix B). Thus, in the ideal case, solving the SDP provides the desired
normalised clustering matrix from which one can recover the partition
k. The following
result shows that X ∗ is also the unique optimal solution of SDP-λ if S is sufﬁciently close to (cid:101)S.
Proposition 1 (Recovery of planted clusters using SDP-λ). Let Z
for a planted k-way clustering, (cid:101)S = ZΣZ T , and X ∗ = ZN −1Z T as deﬁned above. Deﬁne
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n×k be the assignments
}

(cid:18) Σ(cid:96)(cid:96) + Σ(cid:96)(cid:48)(cid:96)(cid:48)
2

and ∆2 = max
i∈[n]

∈
C1, . . . ,

∆1 = min
(cid:96)(cid:54)=(cid:96)(cid:48)

max
(cid:96)∈[k]

Σ(cid:96)(cid:96)(cid:48)

∈ {

(cid:88)

0, 1

(cid:101)Sij

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Sij

j∈C(cid:96)

−

−

(cid:19)

(cid:17)

(cid:16)

C

1

,

.

(cid:96)

|C

|

X ∗ is the unique optimal solution of SDP-λ for any choice of λ in the interval

(cid:13)
(cid:13)
(cid:13)S

(cid:13)
(cid:13)
(cid:101)S
(cid:13)2

−

< λ < min

(cid:96)

min

n(cid:96)

·

(cid:26) ∆1
2

(cid:27)

, ∆1 −

6∆2

.

The proof of Proposition 1, given in Appendix B, is adapted from Yan et al. (2018) although
uniqueness was not proved in this previous work. The term ∆1 quantiﬁes the separation between the

4

ideal within and inter-cluster similarities, and is similar in spirit to the weak assortativity criterion for
stochastic block models (Yan et al., 2018). On the other hand, the matrix spectral norm
(cid:107)2
and the term ∆2 both quantify the deviation of the similarities S from their ideal values (cid:101)S. Note
that the number of clusters can be computed as k = trace (X) and cluster assignment Z is obtained
by clustering the rows of X ∗ using k-means or spectral clustering for example. In the experiments
(Section 5), we present a data-dependent approach to tune λ and ﬁnd k.

−

(cid:101)S

S

(cid:107)

We conclude this section by noting that most of the previous analyses of SDP clustering either
assume sub-Gaussian data (Yan and Sarkar, 2016) or consider similarity matrices with independence
assumptions (Chen and Xu, 2014; Yan et al., 2018) that might not hold in general, and do not hold
for our AddS-3 and AddS-4 similarities described in the next section. In contrast, the deterministic
criteria stated in Proposition 1 make the result applicable in more general settings.

4 Similarities from passive comparisons

We present two new similarity functions computed from passive comparisons (AddS-3 and AddS-4)
and guarantees for recovering planted clusters using SDP-λ in conjunction with these similarities.
Kleindessner and von Luxburg (2017) introduced pairwise similarities computed from triplets. A
quadruplets variant was proposed by Ghoshdastidar et al. (2019). These similarities, detailed in
Appendix A, are positive-deﬁnite kernels and have multiplicative forms. In contrast, we compute the
similarity between items i, j by simply adding binary responses to comparisons involving i and j.

Similarity from quadruplets. We construct the additive similarity for quadruplets, referred to as
= j, deﬁne
AddS-4, in the following way. Recall the deﬁnition of

(cid:88)

Sij =

(cid:0)I{(i,j,r,s)∈Q} −
where I{·} is the indicator function. The intuition is that if i, j are similar (wij is large), then for every
more
observed tuple i, j, r, s, wij > wrs is more likely to be observed. Thus, (i, j, r, s) appears in
often than (r, s, i, j), and Sij is a (possibly large) positive term. On the other hand, smaller wij leads
to a negative value of Sij. Under the aforementioned planted model with clusters of size n1, . . . , nk,
one can verify that Sij indeed reveals the planted clusters in expectation since if i, j belong to the

(AddS-4)

r(cid:54)=s

Q

Q

in Equation (1) and for every i
(cid:1),
I{(r,s,i,j)∈Q}

same planted cluster, then E[Sij] = p(cid:15)δ

(cid:88)

n(cid:96)(n

n(cid:96))

−
2

, and E[Sij] =

p(cid:15)δ

−

(cid:19)

(cid:18)n(cid:96)
2

(cid:88)

(cid:96)∈[k]

otherwise.

(cid:96)∈[k]

Thus, in expectation, the within cluster similarity exceeds the inter-cluster similarity by p(cid:15)δ(cid:0)n
2
Similarity from triplets. The additive similarity based on passive triplets AddS-3 is given by

(cid:1).

Sij =

(cid:88)

r(cid:54)=i,j

(cid:0)I{(i,j,r)∈T } −

I{(i,r,j)∈T }

(cid:1) + (cid:0)I{(j,i,r)∈T } −

I{(j,r,i)∈T }

(cid:1)

(AddS-3)

for every i
= j. The AddS-3 similarity Sij aggregates all the comparisons that involve both i and j,
with either i or j as the reference item. Similar to the case of AddS-4, Sij tends to be positive when
wij is large, and negative for small wij. One can also verify that, under a planted model, the expected
within cluster AddS-3 similarity exceeds the inter-cluster similarity by p(cid:15)δ(n

2).

T

Q

or

A signiﬁcant advantage of AddS-3 and AddS-4 over existing similarities is in terms of computational
time for constructing S. Unlike existing kernels, both similarities can be computed from a single
. In addition, the following result shows that the proposed similarities can exactly
pass over
recover planted clusters using only a few (near optimal) number of passive comparisons.
Theorem 1 (Cluster recovery using AddS-3 and AddS-4). Let X ∗ denote the normalised cluster-
ing matrix corresponding to the true partition, and nmin be the size of the smallest planted cluster.
Given the triplet or the quadruplet setting, there exist absolute constants c1, c2, c3, c4 > 0 such
n , X ∗ is the unique optimal solution of SDP-λ if δ satisﬁes
that, with probability at least 1

1

−

−

c1

√n ln n
nmin

< δ

≤

1 , and one of the following two conditions hold:

(triplet setting) S is given by AddS-3, and the number of triplets

and the parameter λ satisfy

•

> c2

|T |

n3(ln n)2
(cid:15)2δ2n2

min

and c3 max

(cid:41)

|T |
ln n
n3 , (ln n)2

< λ < c4|T |

(cid:15)δnmin
n2

;

(cid:40)(cid:114)

(cid:114)

ln n
n

,

(cid:15)

|T |

|T |

5

(cid:54)
(cid:54)
•

(quadruplet setting) S is given by AddS-4, and the number of quadruplets
(cid:40)(cid:114)

(cid:114)

(cid:41)

and λ satisfy

|Q|

and c3 max

|Q|

ln n
n

,

(cid:15)
|Q|

ln n
n3 , (ln n)2

< λ < c4|Q|

(cid:15)δnmin
n2

.

> c2

|Q|

n3(ln n)2
(cid:15)2δ2n2

min

The condition on δ and the number of comparisons ensure that the interval for λ is non-empty.

(cid:15)2δ2 n(ln n)2(cid:17)
(cid:16) k2

Theorem 1 is proved in Appendix C. This result shows that given a sufﬁcient number of comparisons,
one can exactly recover the planted clusters using SDP-λ with an appropriate choice of λ. In particular,
if there are k planted clusters of similar sizes and δ satisﬁes the stated condition, then recovery of the
planted clusters with zero error is possible with only Ω
passively obtained triplets or
quadruplets. We make a few important remarks about the sufﬁcient conditions stated in Theorem 1.
Remark 1 (Comparison with existing results). For ﬁxed k and ﬁxed (cid:15), δ
(0, 1], Theorem 1 states
that Ω (cid:0)n(ln n)2(cid:1) passive comparisons (triplets or quadruplets) sufﬁce to exactly recover the clusters.
This signiﬁcantly improves over the Ω (cid:0)n3.5 ln n(cid:1) passive quadruplets needed by Ghoshdastidar
et al. (2019) in a planted setting, and the fact that Ω (cid:0)n3(cid:1) triplets are necessary in the worst case
(Emamjomeh-Zadeh and Kempe, 2018).
Remark 2 (Dependence of the number of comparisons on the noise levels (cid:15), δ). When one can
actively obtain comparisons, Emamjomeh-Zadeh and Kempe (2018) showed that it sufﬁces to query
(cid:1) dependence in the active setting, the sufﬁcient number
Ω (cid:0)n ln (cid:0) n
of passive comparisons in Theorem 1 has a stronger dependence of 1
(cid:15)2 on the crowd noise level (cid:15).
While we do not know whether this dependence is optimal, the stronger criterion is intuitive since,
unlike the active setting, the passive setting does not provide repeated observations of the same
comparisons that can easily nullify the crowd noise. The number of comparisons also depends as 1
δ2
on the inherent noise level, which is similar to the conditions in Ghoshdastidar et al. (2019).

(cid:1)(cid:1) triplets. Compared to the ln (cid:0) 1

∈

(cid:15)

(cid:15)

(cid:17)

(cid:16)

O

(cid:29)

|T |

|Q|

√n ln n

(cid:0)(cid:112) n
ln n

√n planted clusters (Chen

Theorem 1 states that exact recovery primarily depends on two sufﬁcient conditions, one on δ and the
other on the number of passive comparisons (
). The following two remarks show that both
or
conditions are necessary, up to possible differences in logarithmic factors.
Remark 3 (Necessity of the condition on δ). The condition on δ imposes the condition of nmin =
. This requirement on nmin appears naturally in planted problems. Indeed, assuming that
Ω
(cid:1)

all k clusters are of similar sizes, the above condition is equivalent to a requirement of k =
and it is believed that polynomial time algorithms cannot recover k
and Xu, 2014, Conjecture 1).
Remark 4 (Near-optimal number of comparisons). To cluster n items, one needs to observe each
example at least once. Hence, one trivially needs at least Ω (n) comparisons (active or passive).
Similarly, existing works on actively obtained comparisons show that Ω (n ln n) comparisons are
sufﬁcient for learning in supervised or unsupervised problems (Haghiri et al., 2017; Emamjomeh-
Zadeh and Kempe, 2018; Ghoshdastidar et al., 2019). We observe that, in the setting of Remark 1,
it sufﬁces to have Ω (cid:0)n(ln n)2(cid:1) passive comparisons which matches the necessary conditions up
to logarithmic factors. However, the sufﬁcient condition on the number of comparisons becomes
Ω (cid:0)k2n(ln n)2(cid:1) if k grows with n while (cid:15) and δ are ﬁxed. It means that the worst case of k =
(cid:1), stated in Remark 3, can only be tackled using at least Ω (cid:0)n2 ln n(cid:1) passive comparisons.
O
Remark 5 (No new information beyond Ω (cid:0)n2/(cid:15)2(cid:1) comparisons). Theorem 1 shows that for large
n and Ω (cid:0)n2/(cid:15)2(cid:1) number of comparisons, the condition for exact recovery of the clusters is only
governed by the condition on δ as the interval for λ is always non empty. It means that, beyond a
quadratic number of comparisons, no new information is gained by observing more comparisons. This
explains why signiﬁcantly fewer passive comparisons sufﬁce in practice than the known worst-case
requirements of Ω (cid:0)n3(cid:1) passive triplets or Ω (cid:0)n4(cid:1) passive quadruplets.

(cid:0)(cid:112) n
ln n

We conclude our theoretical discussion with a remark about recovering planted clusters when the
pairwise similarities wij are observed. Our methods are near optimal even in this setting.
Remark 6 (Recovering planted clusters for non-parametric Fin, Fout). Theoretical studies in the
classic setting of clustering with observed pairwise similarities
typically assume that
the distributions Fin and Fout for the pairwise similarities are Bernoulli (in unweighted graphs), or

wij : i < j
{

}

6

(a) Vary the number of comparisons

(b) Vary the external noise level, (cid:15)

(c) Vary the distributions Fin, Fout

Figure 1: ARI of various methods on the planted model (higher is better). We vary: (1a) the number
of comparisons

; (1b) the crowd noise level (cid:15); (1c) the distributions Fin and Fout.

and

|T |

|Q|

take ﬁnitely many values (labelled graphs), or belong to exponential families (Chen and Xu, 2014;
Aicher et al., 2015; Yun and Proutiere, 2016). Hence, the applicability of such results are restrictive.
Recently, Xu et al. (2020) considered non-parametric distributions for Fin, Fout, and presented a
near-optimal approach based on discretisation of the similarities into ﬁnitely many bins. Our work
suggests an alternative approach: compute ordinal comparisons from the original similarities and use
clustering on AddS-3 or AddS-4. Theorem 1 then guarantees, for any non-parametric and continuous
Fin and Fout, exact recovery of the planted clusters under a near-optimal condition on δ.

5 Experiments

The goal of this section is three-fold: present a strategy to tune λ in SDP-λ; empirically validate our
theoretical ﬁndings; and demonstrate the performance of the proposed approaches on real datasets.

(cid:80)

σi(Xλ)

Choosing λ and estimating the number of clusters based on Theorem 1. Given a similarity
matrix S, the main difﬁculty involved in using SDP-λ is tuning the parameter λ. Yan et al. (2018)
proposed the algorithm SPUR to select the best λ as λ∗ = arg max0≤λ≤λmax
i≤kλ
trace(Xλ) where Xλ
is the solution of SDP-λ, kλ is the integer approximation of trace (Xλ) and an estimate of the number
of clusters, σi(Xλ) is the i-th largest eigenvalue of Xλ, and λmax is a theoretically well-founded upper
bound on λ. The maximum of the above objective is 1, achieved when Xλ has the same structure
as X ∗ in Proposition 1. In our setting, Theorem 1 gives an upper bound on λ that depends on (cid:15), δ
and nmin which are not known in practice. Furthermore, it is computationally beneﬁcial to use the
theoretical lower bound for λ instead of using λ

≥
We propose to modify SPUR based on the fact that the estimated number of clusters k monotonically
decreases with λ (details in Appendix D). Given Theorem 1, we choose λmin =
c(ln n)/n and
. The trace of the SDP-λ solution then gives two estimates of the
λmax = c/n, where c =
or
|Q|
[kλmax, kλmin] instead of searching over
number of clusters, kλmin and kλmax, and we search over k
λ—in practice, it helps to search over the values max
kλmin + 2. We select k that
{
maximises the above SPUR objective, where X is computed using a simpler SDP (Yan et al., 2018):

0 as suggested in SPUR.

2, kλmax} ≤

|T |

(cid:112)

≤

∈

k

maxX

S, X

s.t. X

0, X

0, X1 = 1,

trace (X) = k.

(SDP-k)

(cid:105)

(cid:104)

(cid:23)
Clustering with AddS-3 and AddS-4. For the proposed similarity matrices AddS-3 and AddS-4,
the above strategy provides the optimal number of clusters k and a corresponding solution Xk of
SDP-k. The partition is obtained by clustering the rows of Xk using k-means. Alternative approaches,
such as spectral clustering, lead to similar performances (see Appendix E).

≥

Evaluation function. We use the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) between
1, 1] and measures the agreement
the ground truth and the predictions. The ARI takes values in [
between two partitions: 1 implies identical partitions, whereas 0 implies that the predicted clustering
is random. In all the experiments, we report the mean and standard deviation over 10 repetitions.

−

Simulated data with planted clusters. We generate data using the planted model from Section 2 and
verify that the learned clusters are similar to the planted ones. As default parameters we use n = 1000,
(cid:0)0, σ2(cid:1)
k = 4, (cid:15) = 0.75,
with σ = 0.1 and δ = 0.5. In each experiment, we investigate the sensitivity of our method by varying
one of the parameters while keeping the others ﬁxed. We use SPUR to estimate the number of clusters.

= n(ln n)4 and Fin =

(cid:1) , σ2(cid:1) , Fout =

(cid:0)√2σΦ−1 (cid:0) 1+δ

|Q|

|T |

N

N

=

2

7

n(lnn)1n(lnn)3.5n(lnn)6|T|,|Q|0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.81.0(cid:15)0.00.20.40.60.81.0AddS-3MulK-3n(lnn)3n(lnn)4U,βn(lnn)3U,βn(lnn)4U,Nn(lnn)3U,Nn(lnn)40.00.20.40.60.81.0AddS-3MulK-3n(lnn)1n(lnn)3.5n(lnn)6|T|,|Q|0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.81.0(cid:15)0.00.20.40.60.81.0AddS-3MulK-3n(lnn)3n(lnn)4U,βn(lnn)3U,βn(lnn)4U,Nn(lnn)3U,Nn(lnn)40.00.20.40.60.81.0AddS-3MulK-3n(lnn)1n(lnn)3.5n(lnn)6|T|,|Q|0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.81.0(cid:15)0.00.20.40.60.81.0AddS-3MulK-3n(lnn)3n(lnn)4U,βn(lnn)3U,βn(lnn)4U,Nn(lnn)3U,Nn(lnn)40.00.20.40.60.81.0AddS-3MulK-3As baselines, we use SDP-k (using the number of clusters estimated by our approaches) followed by
k-means with two comparison based multiplicative kernels: MulK-3 for triplets (Kleindessner and
von Luxburg, 2017) and MulK-4 for quadruplets (Ghoshdastidar et al., 2019).

We present some signiﬁcant results in Figure 1 and defer the others to Appendix E. In Figure 1a, we
vary the number of sampled comparisons. Unsurprisingly, our approaches are able to exactly recover
the planted clusters using as few as n(ln n)3 comparisons—extra ln n factor compared to Theorem 1
accounts for (cid:15), δ and constants. MulK-3 and MulK-4 respectively need n(ln n)4.5 and n(ln n)5.5
comparisons (both values exceed n2 for n = 1000). In all our experiments, AddS-3 and AddS-4 have
comparable performance while MulK-3 is signiﬁcantly better than MulK-4. Thus we focus on triplets
in the subsequent experiments for the sake of readability. In Figure 1b, we vary the external noise
level (cid:15). Given n(ln n)4 comparisons, AddS-3 exactly recovers the planted clusters for (cid:15) as small
as 0.25 (high crowd noise) while, given the same number of comparisons, MulK-3 only recovers
the planted clusters for (cid:15) > 0.9. Figure 1c shows that AddS-3 outperforms MulK-3 even when
different distributions for Fin and Fout are considered (Uniform + Beta or Uniform + Normal; details
in Appendix E). It also shows that the distributions affect the performances, which is not evident
from Theorem 1, indicating the possibility of a reﬁned analysis under distributional assumptions.

MNIST clustering with comparisons. We consider two datasets which are subsets of the MNIST
test data (LeCun and Cortes, 2010): (i) a subset of 2163 examples containing all 1 and 7 (MNIST
1vs.7), two digits that are visually very similar, and (ii) a randomly selected subsets of 2000 examples
from all 10 classes (MNIST 10). To generate the comparisons, we use the Gaussian similarity on a
2-dimensional embedding of the entire MNIST test data constructed with t-SNE (van der Maaten,
1, 1]2. We focus on the triplet setting and
2014) and normalized so that each example lies in [
consider additional baselines. First, we use t-STE (van der Maaten and Weinberger, 2012), an ordinal
embedding approach, to embed the examples in 2 dimensions, and then cluster them using k-means
on the embedded data. Second, we directly use k-means on the normalized data obtained with t-SNE.
The latter is a baseline with access to Euclidean data instead of triplet comparisons.

−

|T |

= n(ln n)2 is sufﬁcient for AddS-3 to reach the performance of
For MNIST 1vs.7 (Figure 2a),
k-means and t-STE while MulK-3 requires n(ln n)3 triplets. Furthermore, note that AddS-3 with
known number of clusters performs similarly to AddS-3 using SPUR, indicating that SPUR estimates
= n(ln n)2, AddS-3
the number of clusters correctly. If we consider MNIST 10 (Figure 2b) and
with known k outperforms AddS-3 using SPUR, suggesting that the number of comparisons here
is not sufﬁcient to estimate the number of clusters accurately. Moreover, AddS-3 with known k
outperforms MulK-3 while being close to the performance of t-STE. Finally for n(ln n)4 triplets,
all ordinal methods converge to the baseline of k-means with access to original data. The ARI of
AddS-3 SPUR improves when the number of comparisons increases due to better estimations of the
number of clusters—estimated k increases from 3 for

= n(ln n)2 up to 9 for

= n(ln n)4.

|T |

|T |

|T |

Real comparison based data. We consider the Food dataset (Wilber et al., 2014) in Appendix F and
the Car dataset (Kleindessner and von Luxburg, 2016) here. It contains 60 examples grouped into
3 classes (SUV, city cars, sport cars) with 4 outliers, and exhibits 12112 triplet comparisons. For
this dataset, AddS-3 SPUR estimates k = 2 instead of the correct 3 clusters. Figure 2c considers all
ordinal methods with k = 2 and k = 3, and shows the pairwise agreement (ARI) between different
methods and also with the true labels. While MulK-3 with k = 3 agrees the most with the true labels,
all the clustering methods agree well for k = 2 (top-left 3
3 block). Hence, the data may have
another natural clustering with two clusters, suggesting possible discrepancies in how different people
judge the similarities between cars (for instance, color or brand instead of the speciﬁed classes).

×

6 Conclusion

It is generally believed that a large number of passive comparisons is necessary in comparison based
learning. Existing results on clustering require at least Ω (cid:0)n3(cid:1) passive comparisons in the worst-case
or under a planted framework. We show that, in fact, Ω (cid:0)n(ln n)2(cid:1) passive comparisons sufﬁce for
accurately recovering planted clusters. This number of comparisons is near-optimal, and almost
matches the number of active comparisons typically needed for learning. Our theoretical ﬁndings
are based on two simple approaches for constructing pairwise similarity matrices from passive
comparisons. While we studied the merits of AddS-3 and AddS-4 in the context of clustering, they
could be used for other problems such as semi-supervised learning, data embedding, or classiﬁcation.

8

(a) MNIST 1vs.7, n = 2163

(b) MNIST 10, n = 2000

(c) Car dataset, |T | = 12112

Figure 2: Experiments on real datasets. (2a)–(2b) ARI on MNIST; (2c) ARI similarity matrix
comparing the clusters obtained by the different methods on car (darker means more agreement).

Broader Impact

This work primarily has applications in the ﬁelds of psychophysics and crowdsourcing, and more
generally, in learning from human responses. Such data and learning problems could be affected by
implicit biases in human responses. However, this latter issue is beyond the scope of this work and,
thus, was not formally analysed.

Acknowledgments and Disclosure of Funding

The work of DG is partly supported by the Baden-Württemberg Stiftung through the BW Elitepro-
gramm for postdocs. The work of MP has been supported by the ACADEMICS grant of the
IDEXLYON, project of the Université de Lyon, PIA operated by ANR-16-IDEX-0005.

References

E. Abbe. Community detection and stochastic block models: recent developments. The Journal of

Machine Learning Research, 18(1):6446–6531, 2017.

S. Agarwal, J. Wills, L. Cayton, G. Lanckriet, D. Kriegman, and S. Belongie. Generalized non-metric
multidimensional scaling. In International Conference on Artiﬁcial Intelligence and Statistics,
pages 11–18, 2007.

C. Aicher, A. Z. Jacobs, and A. Clauset. Learning latent block structure in weighted networks.

Journal of Complex Networks, 3(2):221–248, 2015.

E. Amid and A. Ukkonen. Multiview triplet embedding: Learning attributes in multiple maps. In

International Conference on Machine Learning, pages 1472–1480, 2015.

E. Arias-Castro. Some theory for ordinal embedding. Bernoulli, 23(3):1663–1693, 2017.

D. Arthur and S. Vassilvitskii. K-means++: The advantages of careful seeding. In Proceedings of the

18th Annual ACM-SIAM Symposium on Discrete Algorithms, page 1027–1035, 2007.

I. Borg and P. Groenen. Modern multidimensional scaling: Theory and applications. Springer, 2005.

X. Chen and Y. Yang. Diffusion k-means clustering on manifolds: provable exact recovery via

semideﬁnite relaxations. Applied and Computational Harmonic Analysis, 2020.

Y. Chen and J. Xu. Statistical-computational phase transitions in planted models: The high-
dimensional setting. In International Conference on Machine Learning, pages 244–252, 2014.

S. Dasgupta. A cost function for similarity-based hierarchical clustering. In Symposium on Theory of

Computing, pages 118–127, 2016.

E. Emamjomeh-Zadeh and D. Kempe. Adaptive hierarchical clustering using ordinal queries. In

Symposium on Discrete Algorithms, pages 415–429, 2018.

9

n(lnn)2n(lnn)3n(lnn)40.00.20.40.60.8ARIk-means,k=2MulK-3,k=2AddS-3,SPURAddS-3,k=2t-STE,k=2n(lnn)2n(lnn)3n(lnn)40.00.20.40.60.8k-means,k=10MulK-3,k=10AddS-3,SPURAddS-3,k=10t-STE,k=10n(lnn)2n(lnn)3n(lnn)40.00.20.40.60.8ARIk-means,k=2MulK-3,k=2AddS-3,SPURAddS-3,k=2t-STE,k=2n(lnn)2n(lnn)3n(lnn)40.00.20.40.60.8k-means,k=10MulK-3,k=10AddS-3,SPURAddS-3,k=10t-STE,k=10AddS-3SPURtruelabelst-STEk=3AddS-3SPURMulK-3k=2t-STEk=2truelabelsAddS-3k=3MulK-3k=3t-STEk=30.40.50.60.70.80.91.0D. Ghoshdastidar, M. Perrot, and U. von Luxburg. Foundations of comparison-based hierarchical
clustering. In Advances in Neural Information Processing Systems, pages 7454–7464, 2019.

S. Haghiri, D. Ghoshdastidar, and U. von Luxburg. Comparison-based nearest neighbor search. In

International Conference on Artiﬁcial Intelligence and Statistics, pages 851–859, 2017.

S. Haghiri, D. Garreau, and U. von Luxburg. Comparison-based random forests. In International

Conference on Machine Learning, pages 1866–1875, 2018.

L. Hubert and P. Arabie. Comparing partitions. Journal of Classiﬁcation, 2(1):193–218, 1985.

L. Jain, K. G. Jamieson, and R. Nowak. Finite sample prediction and recovery bounds for ordinal
embedding. In Advances in Neural Information Processing Systems, pages 2711–2719, 2016.

K. G. Jamieson and R. D. Nowak. Low-dimensional embedding using adaptively selected ordinal data.
In Annual Allerton Conference on Communication, Control, and Computing, pages 1077–1084,
2011.

S. Janson and A. Ruci´nski. The infamous upper tail. Random Structures & Algorithms, 20(3):

317–342, 2002.

M. Kleindessner and U. von Luxburg. Dimensionality estimation without distances. In International

Conference on Artiﬁcial Intelligence and Statistics, pages 471–479, 2015.

M. Kleindessner and U. von Luxburg. Lens depth function and k-relative neighborhood graph:

versatile tools for ordinal data analysis, 2016.

M. Kleindessner and U. von Luxburg. Kernel functions based on triplet similarity comparisons. In

Advances in Neural Information Processing Systems, pages 6807–6817, 2017.

Y. LeCun and C. Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/,

2010.

B. Mason, L. Jain, and R. Nowak. Learning low-dimensional metrics.

In Advances in neural

information processing systems, pages 4139–4147, 2017.

R. N. Shepard. The analysis of proximities: Multidimensional scaling with an unknown distance

function. i. Psychometrika, 27(2):125–140, 1962.

J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis

and machine intelligence, 22(8):888–905, 2000.

N. Stewart, G. D. A. Brown, and N. Chater. Absolute identiﬁcation by relative judgment. Psychologi-

cal review, 112(4):881, 2005.

O. Tamuz, C. Liu, S. Belongie, O. Shamir, and A. T. Kalai. Adaptively learning the crowd kernel. In

International Conference on Machine Learning, pages 673–680, 2011.

Y. Terada and U. von Luxburg. Local ordinal embedding. In International Conference on Machine

Learning, pages 847–855, 2014.

J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational

mathematics, 12(4):389–434, 2012.

A. Ukkonen. Crowdsourced correlation clustering with relative distance comparisons. arXiv preprint

arXiv:1709.08459, 2017.

A. Ukkonen, B. Derakhshan, and H. Heikinheimo. Crowdsourced nonparametric density estimation
using relative distances. In AAAI Conference on Human Computation and Crowdsourcing, 2015.

L. van der Maaten and K. Weinberger. Stochastic triplet embedding. In IEEE International Workshop

on Machine Learning for Signal Processing, pages 1–6, 2012.

Laurens van der Maaten. Accelerating t-sne using tree-based algorithms. The Journal of Machine

Learning Research, 15(1):3221–3245, 2014.

10

S. Vikram and S. Dasgupta. Interactive bayesian hierarchical clustering. In International Conference

on Machine Learning, pages 2081–2090, 2016.

U. von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416, 2007.

M. Wilber, S. Kwak, and S. Belongie. Cost-effective hits for relative similarity comparisons. In

Human Computation and Crowdsourcing (HCOMP), Pittsburgh, 2014.

M. Xu, V. Jog, and P.-L. Loh. Optimal rates for community estimation in the weighted stochastic

block model. The Annals of Statistics, 48(1):183–204, 2020.

B. Yan and P. Sarkar. On robustness of kernel clustering.

In Advances in Neural Information

Processing Systems, pages 3098–3106, 2016.

B. Yan, P. Sarkar, and X. Cheng. Provable estimation of the number of blocks in block models. In

International Conference on Artiﬁcial Intelligence and Statistics, pages 1185–1194, 2018.

F. W. Young. Multidimensional scaling: History, theory, and applications. Lawrence Erlbaum

Associates, 1987.

S.-Y. Yun and A. Proutiere. Optimal cluster recovery in the labeled stochastic block model. In

Advances in Neural Information Processing Systems, pages 965–973, 2016.

L. Zhang, S. Maji, and R. Tomioka. Jointly learning multiple measures of similarities from triplet

comparisons. arXiv preprint arXiv:1503.01521, 2015.

A Existing comparison based similarities / kernel functions

The literature on ordinal embedding from triplet comparisons is extensive (Jamieson and Nowak,
2011; Arias-Castro, 2017). In contrast, the idea of directly constructing similarity or kernel matrices
from the comparisons, without embedding the data in an Euclidean space, is rather new. Such
an approach is known to be signiﬁcantly faster than embedding methods, and provides similar or
sometimes better performances in certain learning tasks. To the best of our knowledge, there are
only two works that learn kernel functions from comparisons (Kleindessner and von Luxburg, 2017;
Ghoshdastidar et al., 2019), while the works of Jain et al. (2016) and Mason et al. (2017) estimate
a Gram (or kernel) matrix from the triplets, which is then further used for data embedding. In
this section, we describe the aforementioned approaches for constructing pairwise similarities from
comparisons. Through this discussion, we illustrate the fundamental difference between the proposed
additive similarities, AddS-3 and AddS-4, and the existing kernels that are of multiplicative nature
(Kleindessner and von Luxburg, 2017; Ghoshdastidar et al., 2019).

Kernels from ordinal data were introduced by Kleindessner and von Luxburg (2017), who proposed
two kernel functions (named k1 and k2) based on observed triplets. The kernels originated from the
notion of Kendall’s τ correlation between two rankings, and k1 was empirically observed to perform
slightly better. We mention this kernel function, which we refer to as a multiplicative triplet kernel
(MulK-3). For any distinct i, j

[n], the MulK-3 similarity is computed as

∈

Sij =

(cid:80)
r<s

(cid:0)I{(i,r,s)∈T } −

I{(i,s,r)∈T }

(cid:1)(cid:0)I{(j,r,s)∈T } −

I{(j,s,r)∈T }

(cid:1)

(cid:112)

((cid:96), r, s)

∈ T

: (cid:96) = i

}|

|{

(cid:112)

((cid:96), r, s)

|{

∈ T

: (cid:96) = j

}|

(MulK-3)

T

where
is the set of observed triplets. Note that this kernel does not consider comparisons involving
wij but, instead, uses multiplicative terms indicating how i and j behave with respect to every pair
(cid:1)
n2 , the denominators in MulK-3 are approximately p(cid:0)n
r, s. For uniform sampling with rate p
for every i
= j. Hence, it sufﬁces to focus only on the numerator. Ghoshdastidar et al. (2019)
proposed a kernel similar to MulK-3 for the case of quadruplets, which is referred to as multiplicative
quadruplet kernel (MulK-4). For i

= j, it is given by

(cid:29)

ln n

2

Sij =

(cid:88)

(cid:88)

(cid:96)(cid:54)=i,j

r<s

(cid:0)I{(i,(cid:96),r,s)∈Q} −

I{(r,s,i,(cid:96))∈Q}

(cid:1)(cid:0)I{(j,(cid:96),r,s)∈Q} −

I{(r,s,j,(cid:96))∈Q}

(cid:1).

(MulK-4)

11

(cid:54)
(cid:54)
O

Ghoshdastidar et al. (2019) studied MulK-4 in the context of hierarchical clustering, and showed that
(cid:0)n3.5 ln n(cid:1) passive quadruplet comparisons to exactly recover a planted hierarchical
it requires
structure in the data. Combining their concentration results with Proposition 1 shows that the same
number of passive quadruplets sufﬁces to recover the planted clusters considered in this work. Note
that both MulK-3 and MulK-4 kernel functions have a multiplicative nature since each entry is an
aggregate of products. This is essential for their positive semi-deﬁnite property. In contrast, the
proposed AddS-3 and AddS-4 similarities simply aggregate comparisons involving the pairwise
similarity wij, and hence, are not positive semi-deﬁnite kernels.

We also mention the work on fast ordinal triplet embedding (FORTE) (Mason et al., 2017), which
learns a metric from the given triplet comparisons. One can easily adapt the formulation to that of
Rn×n from triplets. Consider the squared distance in the corresponding
learning a kernel matrix K
reproducing kernel Hilbert space (RKHS), d2
2Kij + Kjj. Assuming that the triplets
adhere to the distance relation in the RKHS, it is easy to see that when a comparison of t =
i, r, s
with i as pivot is available, then

K(i, j) = Kii

−

∈

{

}

yt := I{(i,r,s)∈T } −

I{(i,s,r)∈T } = sign (cid:0)d2
= sign (Krr

K(i, r)

−
2Kir

K(i, s)(cid:1)
d2

Kss + 2Kis) ,

−
Rn×n.
which is the sign of a linear map of K, which we can denote as sign(
Mt, K
(cid:104)
One can learn the optimal kernel matrix, that satisﬁes most triplet comparisons, by minimising the

) for some Mt
(cid:105)

−

∈

empirical loss

1

|T |

(cid:88)

t∈T

(cid:96)(yt

Mt, K
(cid:104)

) with positive deﬁniteness constraints for K, where (cid:96) is a loss
(cid:105)

function (log loss is suggested by Jain et al. (2016)).

B Proof of Proposition 1

In this section, we ﬁrst provide a proof of Proposition 1 which is split into two parts: the proof
of optimality of X ∗, and the proof of uniqueness of the optimal solution. In addition, we provide
a derivation for the claim that X ∗ is the unique optimal solution for SDP-λ when S = (cid:101)S and
0 < λ < nmin∆1. The derivation, given at the end of the section, follows from simplifying some of
the computations in the proof of Proposition 1.

B.1 Optimality of X ∗ when S is close to (cid:101)S

The proof is adapted from Yan et al. (2018). We ﬁrst state the Karush-Kahn-Tucker (KKT) conditions
for SDP-λ. Let Γ, Λ
0)
Rn be the Lagrange
and the positive semi-deﬁniteness constraint (X
parameter for the row sum constraints. The tuple (X, Λ, Γ, α) is a primal-dual optimal solution for
SDP-λ if and only if it satisﬁes the following KKT conditions:

Rn×n be the Lagrange parameters for the non-negativity constraint (X

0), respectively. Let α

(cid:23)

≥

∈

∈

Stationarity :
Primal feasibility :
Dual feasibility :
Complementary slackness :

1αT

α1T = 0

; X1 = 1

λI + Λ + Γ
S
−
0
X
≥
Λ
0
(cid:23)
Λ, X
(cid:104)

; X
; Γ
= 0

(cid:105)

−

−
0
(cid:23)
0
≥
; ΓijXij = 0

i, j

∀

(cid:104)

(cid:105)

A, B

to denote trace (AB) for symmetric matrices A, B. The above derivation is
where we use
straightforward. The term 1αT + α1T in the stationarity condition arises due to the symmetry of X,
that is, since row-sum and column-sum are identical. We construct a primal-dual witness to show
that X ∗ is the optimal solution of SDP-λ under the stated conditions on λ. We use the following
R|C| be the projection of u on the indices
[n], we let uC
notations. For any vector u
C ⊂
∈
Rn×n, ACC(cid:48) is the sub-matrix corresponding to row
contained in
. Similarly, for a matrix A
(cid:48). We also deﬁne 1m and Im the constant vector of ones and
indices in
the identity matrix of size m, respectively. We use
k to denote the planted clusters of size
n1, . . . , nk. We consider the following primal-dual construction that is similar to Yan et al. (2018),
where X = X ∗. For every j, (cid:96)

C1, . . . ,
= j, we deﬁne

C
and column indices in

Rn and

1, . . . , k

∈

∈

C

C

C

α :

αCj =

and (cid:96)
(cid:32)

∈ {
1
nj

}
SCj Cj 1nj −

λ
2nj

+

1
2n2
j

(cid:33)

1T
nj SCj Cj 1nj

1nj

(4)

12

(cid:54)



ΛCj Cj =

−

ΛCj C(cid:96) =

−
(cid:40)ΓCj Cj = 0
ΓCj C(cid:96) =

Λ :

Γ :

SCj Cj + αCj 1T
(cid:18)

nj + 1nj αT
(cid:19)

Cj + λInj
(cid:18)

Inj −

1
nj

1nj 1T
nj

SCj C(cid:96)

In(cid:96) −

(cid:19)

1
n(cid:96)

1n(cid:96)1T
n(cid:96)

SCj C(cid:96) −

−

ΛCj C(cid:96) + αCj 1T

n(cid:96) + 1nj αT

C(cid:96) .

(5)

(6)

The proof of Proposition 1 is based on verifying the KKT conditions for the above Λ, Γ, α and
1nj 1T
X = X ∗. To this end, note that X ∗
= j. Primal feasibility
is obviously satisﬁed by X ∗, and it is easy to see that the choice of ΛCj Cj and ΓCj C(cid:96) ensures that
stationarity holds. Hence, we only need to verify dual feasibility and complementary slackness.
The complementary slackness condition for Γ holds since ΓCj Cj = 0 and X ∗
verify

Cj C(cid:96) = 0 for (cid:96)

Cj C(cid:96) = 0 for j

Cj Cj = 1
nj

nj and X ∗

= 0, observe that

= (cid:96). To

ΛCj C(cid:96), X ∗
(cid:104)

Cj C(cid:96)(cid:105)

=

(cid:88)
(cid:104)

j

ΛCj Cj , X ∗

Cj Cj (cid:105)

=

=

(cid:88)

j
(cid:88)

j

1
nj

1T
nj ΛCj Cj 1nj

1
nj

−

nj SCj Cj 1nj + 21T
1T

nj αCj + λ,

Λ, X ∗
(cid:104)
Λ, X ∗

=

(cid:104)

(cid:105)

(cid:105)
(cid:88)

j,(cid:96)

where the last step follows by substituting ΛCj Cj from (5) and noting that 1T
nj
the value of αCj above shows that each term in the sum is zero, and hence,
We now verify the dual feasibility and ﬁrst prove that Γ
substitute ΛCj C(cid:96) and αCj in (6) to obtain

1nj = nj. Substituting
Λ, X ∗
(cid:104)
(cid:105)
0, in particular, ΓCj C(cid:96) ≥

= (cid:96). We

0 for j

= 0.

≥

ΓCj C(cid:96) =

SCj C(cid:96) +

−

+

+

=

1
nj

−

1nj 1T

(cid:18)
Inj −
1
nj

SCj Cj 1nj 1T

n(cid:96) −

1
n(cid:96)

1nj 1T

n(cid:96)SC(cid:96)C(cid:96) −
1
n(cid:96)
nj SCj C(cid:96)1n(cid:96)
njn(cid:96)

nj SCj C(cid:96) −
(cid:32) 1T

−

+

(cid:19)

1
nj

1nj 1T
nj

SCj C(cid:96)

1
n(cid:96)

1n(cid:96) 1T
n(cid:96)

(cid:19)

(cid:33)

1T
nj SCj Cj 1nj

1nj 1T
n(cid:96)

1T
n(cid:96)SC(cid:96)C(cid:96)1n(cid:96)

(cid:19)

1nj 1T
n(cid:96)

(cid:32)

λ
2nj
(cid:18) λ
2n(cid:96)

(cid:18)
In(cid:96) −
1
2n2
j

+

+

1
2n2
(cid:96)
1
nj

SCj C(cid:96) 1n(cid:96)1T

n(cid:96) +

SCj Cj 1nj 1T

n(cid:96) +

λ
2nj −

1T
nj SCj Cj 1nj
2n2
j

−

λ
2n(cid:96) −

1nj 1T

n(cid:96)SC(cid:96)C(cid:96)
(cid:33)

1
n(cid:96)
1T
n(cid:96)SC(cid:96)C(cid:96) 1n(cid:96)
2n2
(cid:96)

1nj 1T

n(cid:96) .

Consider i

Γir =

∈ C
1
nj

−

j and r

(cid:96). From above, we can compute Γir as

∈ C

1T
nj SCj r

SiC(cid:96)1n(cid:96) +

SiCj 1nj +

1
nj

−

1
n(cid:96)
1T
nj SCj C(cid:96)1n(cid:96)
njn(cid:96)
1
n(cid:96)

Si(cid:48)r

−

−

(cid:88)

r(cid:48)∈C(cid:96)

1T
n(cid:96)SC(cid:96)r

1
n(cid:96)
1T
n(cid:96)SC(cid:96)C(cid:96) 1n(cid:96)
2n2
(cid:96)
1
n(cid:96)

(cid:88)

Sii(cid:48) +

1T
nj SCj Cj 1nj
2n2
j
1
nj

Sir(cid:48) +

(cid:88)

i(cid:48)∈Cj

−

+

=

1
nj

−

(cid:88)

i(cid:48)∈Cj

λ
2nj −

λ
2n(cid:96)

−

Srr(cid:48)

+

1
njn(cid:96)

(cid:88)

Si(cid:48)r(cid:48)

i(cid:48)∈Cj ,r(cid:48)∈C(cid:96)

1
2n2
j

−

(cid:88)

i,i(cid:48)∈Cj

Sii(cid:48)

−

r(cid:48)∈C(cid:96)
1
2n2
(cid:96)

(cid:88)

r,r(cid:48)∈C(cid:96)

Srr(cid:48)

λ
2nj −

λ
2n(cid:96)

.

−

j, r

∈ C

Our goal is to derive a lower bound for Γir and show that, for suitable values of λ, Γir
0 for all
(cid:96). We bound each of the terms from below. For the last two terms involving λ, we
i
note that both terms are at least
, where nmin = min(cid:96) n(cid:96). For each of the other terms, we
rewrite the summations in terms of the ideal similarity matrix (cid:101)S and bound the deviation in terms of

λ
2nmin

∈ C

≥

−

13

(cid:54)
(cid:54)
(cid:54)
∆2 = max
i∈[n]

max
(cid:96)∈[k]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

Sir

1
n(cid:96)

(cid:80)
r∈C(cid:96)

(cid:101)Sir

−

(cid:17)

(cid:12)
(cid:12)
(cid:12)
. For the ﬁrst term, we have
(cid:12)
(cid:12)

1
nj

−

(cid:88)

i(cid:48)∈Cj

Si(cid:48)r =

1
nj

−

(cid:88)

(cid:101)Si(cid:48)r

1
nj

−

(cid:88)

(cid:16)

i(cid:48)∈Cj

Si(cid:48)r

−

(cid:17)

(cid:101)Si(cid:48)r

i(cid:48)∈Cj
1
nj

−

=

Σj(cid:96)

−

(cid:88)

(cid:16)

i(cid:48)∈Cj

Si(cid:48)r

−

(cid:17)

(cid:101)Si(cid:48)r

Σj(cid:96)

≥ −

∆2.

−

For the second inequality, we use the structure of (cid:101)S to note that (cid:101)Sir = Σj(cid:96) for every i
(cid:96),
∈ C
and ﬁnally the deviation term is bounded by ∆2. Similarly, one can bound the second, third and
fourth terms from below by (
∆2), respectively. For the ﬁfth
term, we write

∆2) and (Σ(cid:96)(cid:96)

∆2), (Σjj

Σj(cid:96)

∈ C

i, r

−

−

−

−

1
njn(cid:96)

(cid:88)

Si(cid:48)r(cid:48) =

i(cid:48)∈Cj ,r(cid:48)∈C(cid:96)

1
njn(cid:96)

(cid:88)

(cid:101)Si(cid:48)r(cid:48) +

i(cid:48)∈Cj ,r(cid:48)∈C(cid:96)

1
njn(cid:96)

(cid:88)

(cid:16)

Si(cid:48)r(cid:48)

i(cid:48)∈Cj ,r(cid:48)∈C(cid:96)

(cid:17)

(cid:101)Si(cid:48)r(cid:48)

−

= Σj(cid:96) +

1
nj

(cid:88)

i(cid:48)∈Cj

(cid:32)

1
n(cid:96)

(cid:88)

(cid:16)

r(cid:48)∈C(cid:96)

Si(cid:48)r

(cid:101)Si(cid:48)r

−

(cid:33)

(cid:17)

Σj(cid:96)

∆2,

−

≥

since each term in the outer summation is at least
seventh terms from below by 1
∆2) and 1
lower bounds, we have

2 (Σjj

−

−
2 (Σ(cid:96)(cid:96)

−

∆2. Similarly, one can bound the sixth and
∆2), respectively. Combining the above

λ

nmin ≥

(∆1 −

6∆2)

−

λ
nmin

,

Γir

≥

1
2

Σjj +

1
2

where we recall that ∆1 = min
(cid:96)(cid:54)=(cid:96)(cid:48)

Σ(cid:96)(cid:96)

Σj(cid:96)

−
(cid:16) Σ(cid:96)(cid:96)+Σ(cid:96)(cid:48) (cid:96)(cid:48)
2

6∆2 −
(cid:17)

Σ(cid:96)(cid:96)(cid:48)

−

−

. Hence, for λ

nmin(∆1 −

≤

6∆2), as stated in

Proposition 1, Γir

0, and more generally, Γ is non-negative.

≥

We ﬁnally derive the positive semi-deﬁniteness of Λ. Deﬁne the vectors u1, . . . , uk
(u(cid:96))i = 1 if i
verify this, we compute the

Rn such that
(cid:96) and 0 otherwise. We ﬁrst claim that u1, . . . , uk lie in the null space of Λ. To

j-th block of Λu(cid:96). For j

= (cid:96),

∈ C

∈

C

(Λu(cid:96))Cj = ΛCj C(cid:96)1n(cid:96) =

(cid:18)

Inj −

−

1
nj

1nj 1T
nj

(cid:19)

(cid:18)

SCj C(cid:96)

In(cid:96) −

1
n(cid:96)

1n(cid:96)1T
n(cid:96)

(cid:19)

1n(cid:96) = 0,

whereas for j = (cid:96), we have from (4) and (5),

(Λu(cid:96))C(cid:96) = ΛC(cid:96)C(cid:96)1n(cid:96)
=

SC(cid:96)C(cid:96)1n(cid:96) + n(cid:96)αC(cid:96) + 1n(cid:96) αT
C(cid:96)
(cid:32)

−

=

SC(cid:96)C(cid:96)1n(cid:96) + SC(cid:96)C(cid:96)1n(cid:96) −

−

2

= 0.

1n(cid:96) + λ1n(cid:96)
1T
n(cid:96)SC(cid:96)C(cid:96)1n(cid:96)
2n(cid:96)

λ
2

+

(cid:33)

1n(cid:96) + 1n(cid:96)

1T
n(cid:96)SC(cid:96)C(cid:96)1n(cid:96)
n(cid:96)

+ λ1n(cid:96)

Thus Λu(cid:96) = 0 for (cid:96) = 1, . . . , k, and to prove that Λ
u
for every (cid:96). For such a vector u, we have

≥
Rn that are orthogonal to u1, . . . , uk. In other words, we consider only u such that uT
C(cid:96)

0, it sufﬁces to show that uT Λu

(cid:23)

∈

0 for all
1n(cid:96) = 0

uT Λu =

k
(cid:88)

j,(cid:96)=1

uT
Cj ΛCj C(cid:96)uC(cid:96) =

=

k
(cid:88)

j=1

k
(cid:88)

j=1

uT
Cj ΛCj Cj uCj +

uT
Cj ΛCj C(cid:96) uC(cid:96)

(cid:88)

j(cid:54)=(cid:96)

uT
Cj (

SCj Cj + λInj )uCj −
−

uT
Cj SCj C(cid:96) uC(cid:96)

(cid:88)

j(cid:54)=(cid:96)

14

(cid:54)
=

k
(cid:88)

j=1

(cid:88)

j,(cid:96)

λuT

Cj uCj −
uT Su,

2

uT
Cj SCj C(cid:96)uC(cid:96)

u
(cid:107)
(cid:107)
is the Euclidean norm. The third equality follows from (5) and uT
C(cid:96)

1n(cid:96) = 0 for every (cid:96).
where
In addition to above, recall that (cid:101)S = ZΣZ T , where Z = [u1 . . . uk]. Hence, for u orthogonal to
u1, . . . , uk, we have uT (cid:101)Su = 0, which, combined with above, gives

u
(cid:107)
(cid:107)

= λ

−

uT Λu = λ

2
(cid:107)
2
(cid:107)

−

−
(cid:13)
(cid:13)
(cid:13)S

−

u
(cid:107)
= λ
u
(cid:107)
(cid:16)
λ

≥
> 0

uT Su
uT (cid:16)
S
(cid:13)
(cid:13)
(cid:101)S
(cid:13)2

−

(cid:17)

(cid:101)S

u

−
(cid:17)

2

u
(cid:107)
(cid:107)

for all u if λ >
range of λ, the KKT conditions are satisﬁed and X ∗ is the optimal solution for SDP-λ.

, which is the condition stated in Proposition 1. Thus, for the speciﬁed

−

(cid:13)
(cid:13)
(cid:13)S

(cid:13)
(cid:13)
(cid:101)S
(cid:13)2

B.2 Uniqueness of the optimal solution X ∗

The uniqueness of the solution can be shown by proving that any other optimal solution X (cid:48) for SDP-λ
must satisfy X (cid:48) = X ∗. This is shown in two steps. First, we show that any optimal solution X (cid:48) must
have the same block structure as X ∗ and X ∗
X (cid:48)
0. We use this fact to show that the objective
value for X ∗ is strictly greater than that for any such X (cid:48).

−

(cid:23)

Note that the previously constructed Lagrange parameters in (4)–(6) need not correspond to the
optimal solution associated with X (cid:48). However, for the previously deﬁned α, Λ, Γ, we can still use the
condition for stationarity to write

Λ + Γ, X (cid:48)
(cid:104)

(cid:105)

=

=

S + 1αT + α1T + λI, X (cid:48)
n
(cid:88)

(cid:105)
(αi + αj)X (cid:48)

S, X (cid:48)

+

(cid:104)−

−(cid:104)

(cid:105)

i,j=1

ij + λtrace (X)

trace (SX (cid:48)) + λtrace (X (cid:48)) + 2

=

−

n
(cid:88)

i=1

αi

where the simpliﬁcation happens noting that X (cid:48) is primal feasible and hence (cid:80)
optimality of X (cid:48) and X ∗, we have trace (SX (cid:48))

λtrace (X (cid:48)) = trace (SX ∗)

j Xij = 1. Due to
λtrace (X ∗), and so,

−

−

Λ + Γ, X (cid:48)
(cid:104)

(cid:105)

=

−

trace (SX ∗) + λtrace (X ∗) + 21T α

= λ +

(cid:32)

k
(cid:88)

−

j=1

1T
nj SCj Cj 1nj
nj

(cid:33)

+ 21T

nj αCj

= 0,

Γ, X (cid:48)
(cid:104)

are zero. To verify this, note that Γ and X (cid:48) are both non-negative and hence,

where the ﬁnal step follows by substituting αCj from (4). From above, we argue that both
(cid:104)
Γ, X (cid:48)
and
(cid:104)
(cid:105)
Λ, X (cid:48)
On the other hand, from the deﬁnition of Frobenius (or Hilbert-Schmidt) norm, we have
(cid:13)Λ1/2X (cid:48)1/2(cid:13)
(cid:13)
2
(cid:13)
F ≥
Since both inner products,
and
conclude that each of them equals zero.

(cid:105)
0.
=
0, where the matrices square roots exist since Λ, X (cid:48) are both positive semi-deﬁnite.
, are non-negative and yet their sum is zero, we can

Γ, X (cid:48)
(cid:104)

(cid:105) ≥
(cid:105)

Λ, X (cid:48)

(cid:104)

(cid:105)

(cid:105)

(cid:104)

Λ, X (cid:48)

(cid:105)

=

Λ, X (cid:48)
(cid:104)

(cid:13)Λ1/2X (cid:48)1/2(cid:13)
(cid:13)
2
F = 0 implies ΛX (cid:48) = 0, or the range space of X (cid:48) lies in the null
Note that
(cid:13)
(cid:107)2, the
space of Λ. Recall, from the proof of positive semi-deﬁniteness of Λ, that, for λ >
(cid:101)S
null space of Λ is exactly spanned by Z = [u1 . . . uk]. Thus, the range space of X (cid:48) is spanned by the
Rk×k that is symmetric, non-negative,
columns of Z, or in other words X (cid:48) = ZAZ T for some A
and positive semi-deﬁnite (to ensure that X (cid:48) is primal feasible), and (cid:80)
j Aijnj = 1 (to satisfy the row

−

∈

S

(cid:107)

15

sum constraint). Recall that X ∗ = ZN −1Z T where N = diag(n1, . . . , nk). Thus, X (cid:48) has the same
block structure as X ∗. However, this result does not imply that we can recover k planted clusters
from X (cid:48) since it is possible that A has less than k distinct rows.
We now argue that X ∗
note that

X (cid:48) must be positive semi-deﬁnite, a property that we use later. To see this,

X (cid:48) = ZN −1/2 (cid:16)
where ZN −1/2 is a matrix with orthonormal columns. Hence, to prove that X ∗
to show that Ik
(cid:23)
−
smaller than 1. This can veriﬁed as

N 1/2AN 1/2(cid:17)

0, it sufﬁces
0 or, equivalently, that the largest eigenvalue of N 1/2AN 1/2 is

N 1/2AN 1/2

N −1/2Z T ,

−
X ∗

X (cid:48)

Ik

−

(cid:23)

−

−

(cid:13)
(cid:13)

(cid:13)N 1/2AN 1/2(cid:13)
(cid:13)
(cid:13)2

= max

u : (cid:107)u(cid:107)=1

uT N 1/2AN 1/2u = max

u : (cid:107)u(cid:107)=1

k
(cid:88)

i,j=1

Aij√ninjuiuj.

From the AM-GM inequality, we have √ninjuiuj

(cid:0)niu2

j + nju2
i

(cid:1). Hence,

1
2

≤

(cid:13)
(cid:13)

(cid:13)N 1/2AN 1/2(cid:13)

(cid:13)
(cid:13)2 ≤

max
u : (cid:107)u(cid:107)=1

1
2

k
(cid:88)

i,j=1

Aij

(cid:0)niu2

j + nju2
i

(cid:1) =

k
(cid:88)

i=1

u2
i = 1,

where we use the fact that (cid:80)
We now claim that

j Aijnj = (cid:80)

i Aijni = 1. From this discussion, we have X ∗

X (cid:48)

0.

(cid:23)

−

(cid:12)
(cid:12)
(cid:12)trace

(cid:16)

(S

(cid:101)S)(X ∗

−

X)

(cid:17)(cid:12)
(cid:12)
(cid:12) ≤

(cid:13)
(cid:13)
(cid:13)S

(cid:13)
(cid:13)
(cid:101)S
(cid:13)2

trace (X ∗

X (cid:48)) ,

(7)

−

−

−
X (cid:48) is positive semi-deﬁnite.
−
= X ∗, with A satisfying the above mentioned conditions,

which follows from von Neumann’s trace inequality and the fact that X ∗
We now prove that for any X (cid:48) = ZAZ T
(cid:107)2 < λ < 1
and for
(cid:101)S

2 ∆1nmin,

S
(cid:107)

−

−
which shows that X ∗ is the unique optimal solution. We compute

−

trace (SX ∗)

λtrace (X ∗) > trace (SX (cid:48))

λtrace (X (cid:48)) ,

trace (SX ∗)

trace (SX (cid:48)) + λtrace (X (cid:48))

−

λtrace (X ∗)
−
= trace (S(X ∗
(cid:16)
(cid:101)S(X ∗

= trace

> trace

> trace

(cid:16)

(cid:16)

(cid:101)S(X ∗

(cid:101)S(X ∗

(cid:16)

−

X (cid:48))
(cid:101)S)(X ∗

λtrace (X ∗

X (cid:48)))
−
(cid:17)
X (cid:48))
+ trace
(S
(cid:13)
(cid:13)
(cid:17)
(cid:13)
(cid:13)
(cid:13)S
(cid:101)S
(cid:13)2
nmin∆1trace (X ∗

−
trace (X ∗

X (cid:48))

X (cid:48))

−

−

(cid:17)

−

−

(cid:17)

X (cid:48))

−

λtrace (X ∗

−

λtrace (X ∗

X (cid:48))

−
X (cid:48)) .

−

−

−

−

−

(8)

X (cid:48))

−

X (cid:48))

−

In the last step, we use

(cid:13)
(cid:13)
(cid:13)S

(cid:13)
(cid:13)
(cid:101)S
(cid:13)2

−

+ λ < 2λ < nmin∆1. We later prove that

trace

(cid:16)

(cid:101)S(X ∗

(cid:17)
X (cid:48))

−

k
(cid:88)

(cid:96)=1

≥

n(cid:96)(1

A(cid:96)(cid:96)n(cid:96))∆1 ≥

−

nmin∆1trace (X ∗

X (cid:48)) .

−

(9)

X (cid:48)) > 0 for all X (cid:48)

Using (9) in the previous derivation proves (8) or the fact that X ∗ is the unique optimal solution,
= X ∗. Hence, we need to verify the strict positivity
provided that trace (X ∗
X (cid:48)) = 0. Due to the row sum constraint for X (cid:48), we have
of the trace. Assume that trace (X ∗
(cid:80)
1. On the other hand trace (X (cid:48)) = trace (X ∗) = k
holds if (cid:80)
(cid:96) A(cid:96)(cid:96)n(cid:96) = k, which is only possible if A(cid:96)(cid:96)n(cid:96) = 1 for every (cid:96), and hence A(cid:96)j = 0
= X ∗, we have
X (cid:48)) = 0 if and only if X (cid:48) = X ∗. For every X (cid:48)
= (cid:96). Thus, trace (X ∗
for j
X (cid:48)) = (cid:80)
trace (X ∗

−
A(cid:96)(cid:96)n(cid:96)) > 0. We conclude the proof by proving (9). We compute

−
j A(cid:96)jnj = 1, which implies A(cid:96)(cid:96)n(cid:96)

≤

−

−

(cid:16)

trace

(cid:96)(1

−
(cid:101)S(X ∗

−

(cid:17)
X (cid:48))

= trace (cid:0)ZΣZ T Z(N −1
= trace (cid:0)ΣZ T Z(N −1

A)Z T (cid:1)
−
A)Z T Z(cid:1)

−

16

(cid:54)
(cid:54)
(cid:54)
(cid:54)
= trace (cid:0)ΣN (N −1

A)N (cid:1)

−

=

k
(cid:88)

(cid:96)=1

Σ(cid:96)(cid:96)n(cid:96)(1

A(cid:96)(cid:96)n(cid:96))

−

k
(cid:88)

(cid:88)

−

(cid:96)=1

j(cid:54)=(cid:96)

A(cid:96)jnjn(cid:96)Σ(cid:96)j,

where the third equality follows from the fact Z T Z = N . Recall from the deﬁnition of ∆1 that
Σ(cid:96)j

∆1. Using this, we can write

≤

1
2 (Σjj + Σ(cid:96)(cid:96))
(cid:16)

(cid:101)S(X ∗

trace

−

−

(cid:17)
X (cid:48))

k
(cid:88)

(cid:96)=1

k
(cid:88)

(cid:96)=1

k
(cid:88)

(cid:96)=1

≥

=

=

Σ(cid:96)(cid:96)n(cid:96)(1

Σ(cid:96)(cid:96)n(cid:96)(1

Σ(cid:96)(cid:96)n(cid:96)(1

−

−

−

A(cid:96)(cid:96)n(cid:96)) +

A(cid:96)(cid:96)n(cid:96)) +

k
(cid:88)

(cid:88)

(cid:96)=1

j(cid:54)=(cid:96)

k
(cid:88)

(cid:88)

(cid:96)=1

j(cid:54)=(cid:96)

A(cid:96)jnjn(cid:96)∆1 −

1
2

k
(cid:88)

(cid:88)

(cid:96)=1

j(cid:54)=(cid:96)

A(cid:96)jnjn(cid:96)(Σ(cid:96)(cid:96) + Σjj)

A(cid:96)jnjn(cid:96)∆1 −

k
(cid:88)

(cid:88)

(cid:96)=1

j(cid:54)=(cid:96)

A(cid:96)jnjn(cid:96)Σ(cid:96)(cid:96)

A(cid:96)(cid:96)n(cid:96)) +

k
(cid:88)

(cid:96)=1

n(cid:96)∆1(1

A(cid:96)(cid:96)n(cid:96))

−

k
(cid:88)

(cid:96)=1

−

n(cid:96)Σ(cid:96)(cid:96)(1

A(cid:96)(cid:96)n(cid:96)).

−

In the ﬁrst equality, we exploit the symmetry of the third summation, while the second equality uses
the row sum constraint to write (cid:80)
A(cid:96)(cid:96)n(cid:96). Cancelling ﬁrst and third terms, we get

j(cid:54)=(cid:96) A(cid:96)jnj = 1

−

trace

(cid:16)
(cid:101)S(X ∗

(cid:17)
X (cid:48))

−

∆1

≥

k
(cid:88)

(cid:96)=1

n(cid:96)(1

−

A(cid:96)(cid:96)n(cid:96))

∆1nmin

≥

k
(cid:88)

(cid:96)=1

(1

−

A(cid:96)(cid:96)n(cid:96)) = nmin∆1trace (X ∗

X (cid:48)) ,

−

which proves (9), and completes the proof.

B.3 Unique optimality of X ∗ when S = (cid:101)S

We now prove that X ∗ is the unique optimal solution when S = (cid:101)S = ZΣZ T and 0 < λ < nmin∆1.
This claim does not immediately follow from Proposition 1, but can be derived from the proof.
We ﬁrst prove the optimality of X ∗ in this case. Recall, from the proof of Proposition 1, that the
claim hinges on showing that Γ
0. From the previous proof, it sufﬁces to show that
0 and uT Λu
0 for any u that is orthogonal to the columns of Z. To show that the latter
ΓCj C(cid:96) ≥
holds, recall that

0 and Λ

(cid:23)

≥

≥

uT Λu = λ

2
u
(cid:107)
(cid:107)

−

uT Su.

Since S = (cid:101)S = ZΣZ T and Z T u = 0, we get uT Λu = λ
0
u
(cid:107)
for all λ > 0. To verify the non-negativity of ΓCj C(cid:96), we observe that, in this case, it can be computed
as

0, which in turn shows that Λ

2
(cid:107)

(cid:23)

≥

ΓCj C(cid:96) =

1
nj

−

1nj 1T

nj (cid:101)SCj C(cid:96) −
(cid:32) 1T

1
n(cid:96)
nj (cid:101)SCj C(cid:96) 1n(cid:96)
njn(cid:96)

(cid:101)SCj C(cid:96)1n(cid:96)1T

n(cid:96) +

1
nj

(cid:101)SCj Cj 1nj 1T

n(cid:96) +

λ
2nj −

−

1T
nj (cid:101)SCj Cj 1nj
2n2
j

−

λ
2n(cid:96) −

n(cid:96) (cid:101)SC(cid:96)C(cid:96)

1nj 1T

1
n(cid:96)
1T
n(cid:96) (cid:101)SC(cid:96)C(cid:96)1n(cid:96)
2n2
(cid:96)

(cid:33)

1nj 1T
n(cid:96)

2Σj(cid:96) + Σjj + Σ(cid:96)(cid:96) + Σj(cid:96)

−

(cid:19)

(cid:18) 1
nj

λ
2

+

1
n(cid:96)

−

Σjj + Σ(cid:96)(cid:96)
2

−

(cid:19)

1nj 1T
n(cid:96)

+

(cid:18)

=

≥

1nj 1T
n(cid:96)

(cid:18)

(cid:19)

λ
∆1 −
nmin
nmin∆1, ΓCj C(cid:96) ≥

So for λ
proof of optimality, we derive that X ∗ is an optimal solution in this case for 0 < λ

0, and hence Γ is non-negative. Combining this with the previous

≤

nmin∆1.

≤

17

The proof of uniqueness is similar to the more general case in Proposition 1. We use the previously
Rk×k, and
derived claim that any optimal solution X (cid:48) must be of the form X (cid:48) = ZAZ T for some A
X ∗
X (cid:48)).
X
(cid:23)
Hence, we have
(cid:101)SX ∗(cid:17)
(cid:16)

0. We have also previously shown that trace

nmin∆1trace (X ∗

λtrace (X ∗)

λ)trace (X ∗

λtrace (X (cid:48))

(cid:16)
(cid:101)S(X ∗

(cid:101)SX (cid:48)(cid:17)

X (cid:48)) ,

trace

trace

X (cid:48))

≥

−

−

−

∈

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(nmin∆1 −

≥

−

−

−

−

which is strictly positive for λ < nmin∆1, and hence, X ∗ is the unique optimal solution in this case.

C Proof of Theorem 1

We prove the result for triplets and quadruplets in separate sections. While the proof structure is the
same in both cases, the computations are quite different. Before presenting the proofs, we list the key
steps.

We ﬁrst compute the expectation of the similarity matrix S computed using AddS-3 or AddS-4, and
derive appropriate ideal matrices (cid:101)S in each case. In our proofs, (cid:101)S = E[S], except differences in the
diagonal entries since Sii = 0 for all i. From the block structure of (cid:101)S, we can compute ∆1.

Subsequently, concentration inequalities are used to derive upper bounds on
(cid:107)2 and ∆2 in terms
(cid:101)S
of the model parameters. In this context, note that though the pairwise similarities
wij : i < j
}
{
are independent, the entries of the matrix S are highly dependent since each wij appears in multiple
entries of S. Hence, to decouple such dependencies, we use a technique by Janson and Ruci´nski
(2002), which considers the dependency graphs of the random variables and ﬁnds an equitable
colouring to ﬁnd independent sets of comparable sizes. To the best of our knowledge, the present
work is the ﬁrst study which uses the equitable colouring approach of Janson and Ruci´nski (2002) to
derive spectral norm bounds. Ghoshdastidar et al. (2019) use this technique only to bound matrix
entries.

−

S

(cid:107)

Finally, we use concentration to show that for a sampling rate p large enough, the number of
comparisons (
) is close to its expected value. Hence, we can replace the sampling rate p in
the previously derived bounds by the number of comparisons, leading to differences in constants only.

|Q|

|T |

or

Notation. For the sake of simplicity, we will ignore absolute constants in the inequalities stated
below, and use the notations (cid:46) and (cid:38) to write inequalities that hold up to some multiplicative absolute
constant.

C.1 Quadruplet setting

We ﬁrst present the proof for the quadruplet setting using the aforementioned steps.

Computation of ∆1. We ﬁrst derive the expectation of the AddS-4 similarity matrix S, where for
i

= j,

E[Sij] =

(cid:88)

r(cid:54)=s

P(cid:0)(i, j, r, s)

(cid:1)
∈ Q

−

P(cid:0)(r, s, i, j)

(cid:1).

∈ Q

(10)

∈

Note that the summation in AddS-4 is a sum over all distinct pairs r, s, noting that we do not count
both (s, r) and (r, s) since they refer to the same comparison. To compute the expectation of each
term in the summation, recall that the items belong to the planted clusters
k and, for each
C
item i, use ψi
ψi. The expected
values of the terms are given in Table 1.
We only explain the derivation of P(cid:0)(i, j, r, s)
values are computed similarly. In this case,
P(cid:0)(i, j, r, s)

C1, . . . ,
∈ C
(cid:1) for the case ψi = ψj and ψr

[k] to denote the cluster index in which i belongs, that is, i

= ψs as the other

(cid:1) = P(cid:0)(i, j, r, s)
= pP(cid:0)(i, j, r, s)
P(cid:0)(i, j, r, s)

(cid:12)
(cid:12)(i, j), (r, s) compared(cid:1)P(cid:0)(i, j), (r, s) compared(cid:1)
∈ Q
(cid:12)
(cid:12)(i, j), (r, s) compared(cid:1)
∈ Q
(cid:12)
(cid:12)wij > wrs; (i, j), (r, s) compared(cid:1)P(wij > wrs)
∈ Q
+ P(cid:0)(i, j, r, s)

(cid:105)
(cid:12)
(cid:12)wij < wrs; (i, j), (r, s) compared(cid:1)P(wij < wrs)
∈ Q

(cid:104)
= p

∈ Q

∈ Q

18

(cid:54)
(cid:54)
Table 1: Value of each term in the summation in (10), assuming i
(cid:1) P(cid:0)(r, s, i, j)
p/2

Case

∈ Q

∈ Q

= j, r

= s, (i, j)
(cid:1) Difference

= (r, s).

ψi = ψj; ψr = ψs
ψi = ψj; ψr
= ψs
= ψj; ψr = ψs
ψi
= ψj; ψr
= ψs
ψi

P(cid:0)(i, j, r, s)
p/2
p(1 + (cid:15)δ)/2
(cid:15)δ)/2
p(1

−
p/2

(cid:15)δ)/2
p(1
−
p(1 + (cid:15)δ)/2
p/2

0
p(cid:15)δ
p(cid:15)δ
0

−

= p

(cid:20) (1 + (cid:15))
2

(1 + δ)
2

(1

+

−
2

(cid:15))

(1

(cid:21)

δ)

−
2

= p

(1 + (cid:15)δ)
2

,

where in each product, the term P(wij > wrs) is computed from (3), and the other term, denoting
ﬂipped answers, follows from the quadruplet variant of (2). Based on Table 1, we have for i, j such
that ψi = ψj

E[Sij] =

(cid:88)

p(cid:15)δ = p(cid:15)δ

k
(cid:88)

n(cid:96)(n

(r,s):ψr(cid:54)=ψs

(cid:96)=1

n(cid:96))

−
2

if i

= j. For i = j, obviously E[Sij] = 0. For i, j such that ψi

= ψj, we have

E[Sij] =

(cid:88)

−
(r,s):ψr=ψs

p(cid:15)δ =

p(cid:15)δ

−

k
(cid:88)

(cid:96)=1

(cid:19)
.

(cid:18)n(cid:96)
2

Hence, we deﬁne the ideal similarity matrix as (cid:101)Sij = E[Sij] for i

= j, and (cid:101)Sii = p(cid:15)δ

k
(cid:80)
(cid:96)=1

n(cid:96)(n−n(cid:96))
2

.

Observe that (cid:101)S = ZΣZ T , where Z
and Σ
∈
this case, we have

∈ {
Rk×k such that Σ(cid:96)(cid:96) = p(cid:15)δ (cid:80)
(cid:96)

n×k is the assignment matrix for the planted clusters,
0, 1
}
n(cid:96)(n−n(cid:96))
= (cid:96)(cid:48). Hence, in
2

and Σ(cid:96)(cid:96)(cid:48) =

(cid:1) for (cid:96)

p(cid:15)δ (cid:80)

(cid:0)n(cid:96)
2

(cid:96)

−

∆1 = p(cid:15)δ

(cid:19)

.

(cid:18)n
2

(11)

(cid:107)2. Deﬁne
(cid:101)S
E[S](cid:1),

]

Preliminary computations and deﬁnitions for concentration. As noted earlier, (cid:101)S and E[S] are
identical, except in the diagonal entries. Hence, we mainly have to obtain concentration of f (S
−
E[S]), where f is a non-negative scalar function. In the case of ∆2, f denotes the maximum partial
row sum, whereas f is the spectral norm in the bound for
as
the collection of random pairwise similarities. We write

wij : i < j
{

S
(cid:107)

W

−

=

}

S

E[S] = (cid:0)S

E[S

](cid:1) + (cid:0)E[S

−

−

|W
where the ﬁrst difference accounts for randomness in sampling and crowd noise, while the second
. This helps in separately concentrating both terms,
difference accounts for the inherent noise in
W
E[S])
which have different dependence structures. Formally, we perform the concentration of f (S
in the following way, assuming f satisﬁes triangle inequality (which holds in the cases that we later
consider).
P(cid:0)f (S

E[S]) > t(cid:1)

E[S]) > t(cid:1)

E[S

|W

−

−

]

≤

≤

−

P(cid:0)f (S
P(cid:0)f (S
EW

−
(cid:2)P·|W

|W

E[S
|W
(cid:0)f (S

|W

]) + f (E[S
]) > t/2(cid:1) + P(cid:0)f (E[S
E[S

−

]) > t/2(cid:1)(cid:3) + P(cid:0)f (E[S

|W

−

]

E[S]) > t/2(cid:1)
]

≤

−
where P·|W denotes the probability over sampling and crowd noise, but conditioned on
we derive an uniform upper bound on the conditional probability, irrespective of
expectation is trivially bounded. To separately deal with the randomness in
due to sampling and crowd noise, we write

|W

|W

W

−

. In fact,
W
, and hence the
and the randomness

W

E[S]) > t/2(cid:1),

−

Sij =

(cid:88)

r(cid:54)=s

(cid:0)I{(i,j,r,s)∈Q} −

I{(r,s,i,j)∈Q}

(cid:1) =

(cid:88)

r<s

ξijrs

(cid:0)I{wij >wrs} −

I{wij <wrs}

(cid:1)

(12)

19

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
denotes whether the comparison between (i, j) and (r, s) is observed
where ξijrs
(ξijrs = 0 if not observed), and whether the crowd response was correct (ξijrs = +1) or ﬂipped
(ξijrs =

1). Under our sampling and noise model,

1, 0, +1
}

∈ {−

−

P(ξijrs = 0) = 1

p,

−

P(ξijrs = 1) =

p(1 + (cid:15))
2

,

P(ξijrs =

1) =

−

p(1

(cid:15))

−
2

p. Note that the set Ξ =
ξijrs : i < j, r < s, (i, j) <
is a collection of independent random variables. Here, (i, j) < (r, s) denotes a lexicographic

and so, E[ξijrs] = p(cid:15) and Var (ξijrs)
(r, s)
}
ordering of tuples since we do not care about the ordering between (i, j) and (r, s).

≤

{

In addition, recall that Fin, Fout are continuous, and hence, with probability 1, any two pairwise
similarities are distinct. Hence, we can write I{wij >wrs} −
I{wij <wrs} = 2I{wij >wrs} −
1. It is noted
that ξijrs is independent of (2I{wij >wrs} −
1), and furthermore, the latter variable is deterministic
conditioned on

. Based on this and using the notation of ξijrs, we write

W

Sij

−

E[Sij

] =

|W

(cid:88)

r<s

Bijrs,

where

Bijrs = (ξijrs

p(cid:15))(cid:0)2I{wij >wrs} −

1(cid:1)

−

E[Sij

]

−

|W

E[Sij] =

(cid:88)

r<s

B(cid:48)

ijrs, where

ijrs = 2p(cid:15)(cid:0)I{wij >wrs} −
B(cid:48)

P(wij > wrs)(cid:1).

(13)

We make the following observations about the collection of random variables Bijrs, B(cid:48)
crucial to the subsequent concentration results. It is easy to see that
probability 1, and E[Bijrs] = E[B(cid:48)
sets

Bijrs
|
p and Var (cid:0)B(cid:48)

ijrs] = 0, Var (Bijrs)

ijrs, which are
B(cid:48)
2p(cid:15) with
ijrs| ≤
|
4p2(cid:15)2. Deﬁne the
≤

2,
| ≤
(cid:1)
ijrs

≤

,

2

B

B

B

−

and

∈ C

∈ C
(cid:1)

(cid:96), j
(cid:96), j

(cid:1) (cid:0)(cid:0)n
2

B
and

= i, r < s, (i, j)

=
B
(cid:48) =
B
i(cid:96) =
(cid:48)
i(cid:96) =

= i, r < s, (i, j)
1(cid:1) random variables. Bijrs =

,
= (r, s)
}
= (r, s)
}
= (r, s)
}
= (r, s)
}

Bijrs : i < j, r < s, (i, j)
{
B(cid:48)
ijrs : i < j, r < s, (i, j)
{
Bijrs : j
{
B(cid:48)
ijrs : j
{
(cid:48) have (cid:0)n
Each of
Bijrs is independent of all other variables in
has a maximum degree of 1. On the other hand, B(cid:48)
form B(cid:48)
than 4(cid:0)n
B
dependency graph with degree at most 1, the dependency graph of
We now use the above discussion to derive upper bounds on ∆2 and
Upper bound for ∆2. To derive a bound on ∆2, we ﬁrst note that
(cid:12)
(cid:12)
(cid:12)
E[Sij]
(cid:12)
(cid:12)
(cid:12)

i(cid:48)j(cid:48)rs, B(cid:48)
7. Similarly,

r(cid:48)s(cid:48)ij and B(cid:48)
i(cid:96),

(cid:48)
i(cid:96) have at most n(cid:96)

ijr(cid:48)s(cid:48), B(cid:48)
(cid:1)

∆2 ≤

max
(cid:96)∈[k]

max
i∈[n]

(cid:0)(cid:0)n
2

1
n(cid:96)

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Sij

j∈C(cid:96)

−

+

−

B

B

2

(14)

for every i
for every i

[n], (cid:96)

[n], (cid:96)

[k],

[k].

∈

∈

∈

∈

rsi(cid:48)j(cid:48), and so, the dependence graph for
(cid:1)

. Thus, a dependency graph on

−

Brsij, but conditioned on
, conditioned on

,
W
,
B
W
ijrs depends on all the random variables of the
(cid:48) has degree smaller
i(cid:96) has a
(cid:1)
3.

1(cid:1) random variables. While

i(cid:96) has degree at most n(cid:96) + (cid:0)n
B

−

−

B

B

2

(cid:48)

S
(cid:107)

(cid:107)2.
(cid:101)S

−

a0
nmin

where a0 = (cid:101)Sii takes into account the fact that (cid:101)S and E[S] differ only in diagonal terms. In the
subsequent steps, we bound the ﬁrst term. For any t > 0, the union bound leads to

(cid:88)

j∈C(cid:96)

Sij

−



> t



(cid:12)
(cid:12)
(cid:12)
E[Sij]
(cid:12)
(cid:12)
(cid:12)

(cid:88)

j∈C(cid:96)

Sij

−

E[Sij

|W

] + E[Sij

]

−

|W

E[Sij]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



> n(cid:96)t

 .

1
n(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)


P



P

max
i∈[n]

max
(cid:96)∈[k]

(cid:88)

(cid:88)

i∈[n]

(cid:96)∈[k]

(cid:88)

(cid:88)

i∈[n]

(cid:96)∈[k]

≤

≤

EW

P·|W

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:88)

j∈C(cid:96)

Sij

−

E[Sij

|W

(cid:12)
(cid:12)
(cid:12)
]
(cid:12)
(cid:12)
(cid:12)

>

n(cid:96)t
2

20







(cid:12)

(cid:12)
(cid:12)
 + P
(cid:12)

(cid:12)
(cid:12)

(cid:88)

j∈C(cid:96)

E[Sij

]

−

|W



> n(cid:96)t



(cid:12)
(cid:12)
(cid:12)
E[Sij]
(cid:12)
(cid:12)
(cid:12)

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:88)

(cid:88)

≤

i∈[n]

(cid:96)∈[k]





EW

P·|W



(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j∈C(cid:96)

(cid:88)

r<s

Bijrs

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)







 + P

>

n(cid:96)t
2

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:88)

(cid:88)

j∈C(cid:96)

r<s

B(cid:48)

ijrs





>

n(cid:96)t
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(15)

B

W

i(cid:96) shows that, conditioned on

, the summation involves terms in the set

For the probability conditioned on
i(cid:96) in (14). The
, the summation is a sum of independent random
discussion on
variables Bijrs whose properties are stated after (13). Hence, we can apply Bernstein’s inequality to
bound the conditional probability as
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

3 2 n(cid:96)t

n(cid:96)t
2

P·|W

Bijrs

2 exp

2pn(cid:96)

(cid:88)

(cid:88)

j∈C(cid:96)

W

r<s





(cid:32)

(cid:33)

>

≤

−

B

( n(cid:96)t
2 )2
(cid:1) + 2
(cid:0)n
2
(cid:26) n(cid:96)t2

2
(cid:27)(cid:19)

(cid:18)

(cid:46) 2 exp

min

−

pn2 , n(cid:96)t

(cid:46) 1
n3

for t (cid:38) max

(cid:115)






pn2 ln n
nmin

,

ln n
nmin






. Since the

(cid:0) 1
n3

O

(cid:1) bound on the probability holds uniformly for

W

all
, it also bounds the ﬁrst term in (15).
For the second probability in (15), note that the B(cid:48)
ijrs in the summation are not independent, and we
cannot directly apply Bernstein inequality. Hence, we apply the technique in Janson and Ruci´nski
(cid:48)
i(cid:96) into
(2002, Theorem 5) which bounds the probability by partitioning the random variables in
(cid:1)
3,
independent sets. Since the dependency graph on
−
(cid:48)
we can obtain an equitable (d + 1)-colouring, with each independent set of size
or
/(d + 1)
i(cid:96)|
(cid:98)|B
(cid:99)
(cid:48)
(cid:48)
i(cid:96),(d+1),
i(cid:96),(1), . . . ,
(cid:100)|B
and we can apply Bernstein’s inequality to bound the summation over each independent set. Hence,
we bound the second probability in (15), for every i, (cid:96), as

B
i(cid:96) has maximum degree d = n(cid:96) + (cid:0)n
2

, which are both smaller than n(cid:96). Denote the independent sets by

/(d+1)
(cid:101)

(cid:48)
i(cid:96)|

B

B

B

(cid:48)

P

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:88)

(cid:88)

r<s

j∈C(cid:96)


B(cid:48)

ijrs





>

n(cid:96)t
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

B(cid:48)∈B(cid:48)
i(cid:96),(r)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B(cid:48)

>

P

 max

r∈{1,...,d+1}

≤

d+1
(cid:88)

r=1

P

≤

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:88)

B(cid:48)∈B(cid:48)
i(cid:96),(r)


>

n(cid:96)t
2(d + 1)





B(cid:48)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





n(cid:96)t
2(d + 1)

2(d + 1) exp



−

2 (cid:80)
B(cid:48)∈B(cid:48)

i(cid:96),(r)

( n(cid:96)t
2(d+1) )2
Var (B(cid:48)) + 2

3 2p(cid:15) n(cid:96)t

2(d+1)

≤

≤

2(d + 1) exp

(cid:46) n2 exp

(cid:18)

−

min

(cid:32)

( n(cid:96)t
d+1 )2
8p2(cid:15)2n(cid:96) + 2
−
(cid:26) n(cid:96)t2

p2(cid:15)2n4 ,

(cid:33)

,

ln n
nmin

3 p(cid:15) n(cid:96)t
d+1
(cid:27)(cid:19)
n(cid:96)t
p(cid:15)n2
(cid:40)(cid:114) ln n
nmin

,

which is

O

(cid:0) 1
n3

(cid:1) for t (cid:38) p(cid:15)n2

max

·

(union bound)

(Bernstein bound)







(cid:41)

. The ﬁrst term dominates since, under the

condition on δ, we have nmin (cid:38) ln n. Thus, we conclude that, with probability 1

∆2 ≤

max
i∈[n]

max
(cid:96)∈[k]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(cid:96)

(cid:88)

j∈C(cid:96)

Sij

−

(cid:12)
(cid:12)
(cid:12)
E[Sij]
(cid:12)
(cid:12)
(cid:12)

+

a0
nmin

21

1
4n ,

−

(cid:46) max

(cid:115)






pn2 ln n
nmin

,

ln n
nmin

, p(cid:15)n2

(cid:114) ln n
nmin

,

p(cid:15)δn2
nmin






,

(16)

where the last term is obviously dominated by the third term.

]

(cid:107)

S

(cid:101)S

−

−

−

|W

E[S

E[S

S
(cid:107)

E[S]

(cid:107)2 +

(cid:107)2 +

E[S
(cid:107)

Upper bound for

(cid:107)2. Similar to the case of ∆2, we bound the spectral norm as
(cid:101)S
(cid:107)2,
]
(cid:101)S

−
S
(cid:107)2 ≤ (cid:107)
where the last term equals a0 = (cid:101)Sii since E[S]
we derive a bound conditioned on
S

|W
−
(cid:101)S is a diagonal matrix. For the ﬁrst term,
, the matrix
, which has a dependence graph with degree 1. We partition
] = A + A(cid:48), where
B
A and A(cid:48) are the symmetric matrices corresponding to each of the independent sets. We derive
a spectral norm for each of A and A(cid:48). For this, we ﬁrst claim that, conditioned on
, the event
(cid:1). To
occurs with probability 1
(cid:1) independent random variables Bijrs. By

−
into two independent sets via equitable colouring, and write S

pn2 ln n, ln n
ij) is a sum of at most (cid:0)n

. Recall from (13)–(14) that, conditioned on

W
] comprises of variables in

Aij
E
|
see this, observe that Aij (or A(cid:48)
Bernstein inequality,

(cid:9) (cid:46) max

W
− O

E[S]
(cid:107)

maxi,j

A(cid:48)
|

(cid:110)(cid:112)

(cid:0) 1
n

E[S

(cid:111)(cid:111)

|W

|W

ij|

W

,
|

−

−

=

(cid:110)

B

(cid:8)

2

P·|W (
Aij
|

|

> τ )

≤

2 exp

−

(cid:32)

(cid:33)

(cid:46) exp

(cid:18)

min

−

(cid:27)(cid:19)

(cid:26) τ 2

pn2 , τ

τ 2
(cid:1) + 4
2p(cid:0)n
3 τ
2
(cid:111)

which is

O

(cid:0) 1
n3

(cid:1) for τ (cid:38) max

(cid:110)(cid:112)

pn2 ln n, ln n

. Applying the union bound gives P(
E

c) =

(cid:0) 1
n

(cid:1).

O

and

Conditioned on
try bounded by
(cid:80)

, the matrices A, A(cid:48) have independent zero mean entries, with each en-
(cid:110)(cid:112)
E
. Furthermore, from the variance of Bijrs, we have
max
j Var (Aij) < pn3, and same for A(cid:48). Hence, by matrix Bernstein inequality (Tropp, 2012),

pn2 ln n, ln n

(cid:16)
W
O

maxi

(cid:111)(cid:17)

P·|W,E (
(cid:107)

S

−

E[S

(cid:107)2 > t)
]

≤

|W

P·|W,E (
(cid:107)


A

2n exp



−

≤

(cid:32)

(cid:107)2 > t/2) + P·|W,E (
t2/4
(cid:110)(cid:112)

pn3 + 1
3 t
(cid:40)

·

A(cid:48)
(cid:107)

(cid:107)2 > t/2)



max

pn2 ln n, ln n



(cid:111)

t2
pn3 ,

t
pn2 ln n

(cid:112)

,

t
ln n

(cid:41)(cid:33)

(cid:46) 1
n

(cid:46) n exp

min

−

(cid:110)(cid:112)

for t (cid:38)
large enough. Denote the complement of

pn2(ln n)3 , (ln n)2(cid:111)
by

pn3 ln n ,

(cid:112)

, where the second term is smaller than the ﬁrst for n

c. For t satisfying the stated condition,

E

E

E[S
(cid:107)2 > t)
]
|W
(cid:2)P·|W (
S
−
(cid:107)
(cid:2)P·|W,E (
S
(cid:107)
(cid:2)P·|W,E (
S
(cid:107)

−

−

E[S

|W

E[S

E[S

]

(cid:107)2 > t)(cid:3)
(cid:107)2 > t) P·|W (
]
E
(cid:107)2 > t) + P·|W (
]
E

|W

|W

c)(cid:3)

P (
S
(cid:107)
−
= EW
= EW
(cid:46) EW
(cid:46) 1
n

) + P·|W,E c (

S
(cid:107)

−

E[S

]

(cid:107)2 > t) P·|W (
E

|W

c)(cid:3)

as each term in the expectation is

(cid:1). Thus, we have

(cid:0) 1
n

O

S
(cid:107)

−

E[S

]

(cid:107)2 (cid:46)

|W

(cid:110)(cid:112)

pn3 ln n , (ln n)2(cid:111)
.

E[S

−
B

E[S]
(cid:107)2, we note that the entries of the matrix comprises of mutually dependent
(cid:48) deﬁned in (14). We need to partition the entries into independent sets.
(cid:48)

To bound
]
(cid:107)
|W
variables in the set
Since the dependency graph for
B
E[S] =
into d + 1 independent sets of nearly identical sizes (equitable colouring). Let E[S
−
Rn×n is a
A(1) + . . . + A(d+1) denote the corresponding partition of the matrix, where A((cid:96))
symmetric matrix, consisting of the variables in the (cid:96)-th independent set. Due to independence
of variables, we have A((cid:96))
ijrs for some r, s, and hence, we can conclude that each A((cid:96)) is a

(cid:48) has maximum degree d = 4 (cid:0)(cid:0)n

1(cid:1), we can partition

ij = B(cid:48)

|W
∈

−

B

(cid:1)

2

]

22

symmetric matrix with independent zero-mean entries, bounded by 2p(cid:15) and variance at most 4p2(cid:15)2
(follows from properties of B(cid:48)

ijrs). Thus, by matrix Bernstein inequality (Tropp, 2012), we have

(cid:16)

P

(cid:17)

A((cid:96))
(cid:107)

(cid:107)2 > τ

n exp

≤

(cid:18)

t2

−

8p2(cid:15)2n + 2

3 p(cid:15)t

(cid:19)

,

and combining with the union bound,

E[S

P (
(cid:107)

]

−

|W

E[S]

(cid:107)2 > t)

≤

P

(cid:18)

max
(cid:96)∈[d+1] (cid:107)

A((cid:96))

(cid:32)

(cid:19)

(cid:107)2 >

t
d + 1
( t
d+1 )2
8p2(cid:15)2n + 2

n(d + 1) exp

≤

(cid:46) n3 exp

(cid:18)

min

−

−
(cid:26) t2

p2(cid:15)2n5 ,

3 p(cid:15) t
d+1
(cid:27)(cid:19)
t
p(cid:15)n2

(cid:33)

,

which is

O

(cid:0) 1
n

(cid:1) for t (cid:38) p(cid:15)n2

·

(cid:110)

√n ln n, ln n

max

(cid:111)

, where the ﬁrst term obviously dominates.

Combining the above derivations, we have with probability 1

(cid:110)(cid:112)

−
pn3 ln n , (ln n)2 , p(cid:15)n2√n ln n , p(cid:15)δn2(cid:111)

−

S
(cid:107)

(cid:107)2 (cid:46) max
(cid:101)S
where the last term (arising due to a0) is dominated by the third.
Deriving interval for λ in terms of
the quadruplet setting. To this end, our main objective is to verify the conditions in Proposition 1:

. We now use (11), (16) and (17) to complete the proof for

(17)

|Q|

1
4n ,

∆1
2

< ∆1 −

6∆2, that is, ∆2 <

∆1
12

and

S
(cid:107)

(cid:107)2 <
(cid:101)S

−

nmin∆1
2

.

Using (11) and the bound in (16), we observe that ∆2 (cid:46) ∆1 if

(cid:26)

p (cid:38) max

ln n
(cid:15)2δ2n2nmin

,

ln n
(cid:15)δn2nmin

(cid:27)

and

δ (cid:38)

(cid:114) ln n
nmin

,

where the condition on δ arises due to the third term in the bound in (16). Similarly, comparing the
(cid:107)2 (cid:46) nmin∆1 if
bound in (17) to (11), we get that
(cid:101)S
(ln n)2
(cid:15)δn2nmin

S
(cid:107)
ln n
(cid:15)2δ2nn2

√n ln n
nmin

p (cid:38) max

δ (cid:38)

and

−

(cid:26)

(cid:27)

,

,

min

where the condition on δ arises from the third bound in (17). Combining the above cases, we conclude
that if

δ (cid:38)

and

√n ln n
nmin
(cid:107)2 are satisﬁed, and by Proposition 1, X ∗ is the unique optimal
(cid:101)S

p (cid:38) (ln n)2
(cid:15)2δ2nn2

(18)

min

,

then the criteria for ∆2 and
solution for SDP-λ with the range of λ given by

−

S

(cid:107)

S
(cid:107)

−

(cid:107)2 (cid:46) max
(cid:101)S

(cid:110)(cid:112)

pn3 ln n , p(cid:15)√n5 ln n , (ln n)2(cid:111)

(cid:46) λ <

p(cid:15)δnmin
2

(cid:19)

(cid:18)n
2

=

∆1
2

.

(19)

We ﬁnally show that the condition on p holds under the stated condition of

|Q|

state the above interval for λ in terms of
independently with probability p, we have that E[
it is easy to verify that for p (cid:38) ln n
with probability 1
which leads to the statement of Theorem 1 in the quadruplet setting.

. Under the assumption that each quadruplet is observed
(cid:1) = O(pn4). By Bernstein inequality,
](cid:1)
(cid:0) 1
(cid:38) ln n, we have
2 E[
n4 in (18)–(19) up to difference in constants,

|Q|
n4 or equivalently
(cid:1). Hence, we can replace p by |Q|

] = p(cid:0)(n
2)
2

|Q| ∈

2 E[

(cid:0) 1
n

− O

], 3

|Q|

|Q|

|Q|

|Q|

(cid:38) n3(ln n)2
(cid:15)2δ2n2

min

, and

23

C.2 Triplet setting

The proof structure in the triplet case is similar to that of the quadruplet setting. We derive an
appropriate ideal matrix (cid:101)S, where (cid:101)S = E[S], except for some differences in the diagonal entries since
Sii = 0 for all i. From the block structure of (cid:101)S, we can compute ∆1. Subsequently, concentration
inequalities are used to derive upper bounds on
(cid:107)2 and ∆2 in terms of the model parameters.
(cid:101)S
S
(cid:107)
As done in the analysis of AddS-4, we let
denote the collection of random pairwise similarities,
and decompose

W

−

(cid:101)S = (cid:0)S

S

−

E[S

−

|W

](cid:1) + (cid:0)E[S

]

|W

−

E[S](cid:1) + (cid:0)E[S]

(cid:101)S(cid:1).

−

The last term is easy to tackle, and we use separate concentration for the ﬁrst two terms both in the
context of ∆2 and the spectral norm. Bounds on these terms, combined with Proposition 1, provide
sufﬁcient conditions on δ and sampling rate p such that exact recovery occurs. Finally, we show that
(cid:1), and state the
for p large enough, the number of triplets
conditions in terms of

is close to its expected value pn(cid:0)n−1

|T |

2

.

Computation of ∆1. The expectation of the AddS-3 similarity Sij, for i
(cid:1)

(cid:88)

(cid:1)

P(cid:0)(i, r, j)

(cid:1) + P(cid:0)(j, i, r)

P(cid:0)(i, j, r)

E[Sij] =

= j, is given by
P(cid:0)(j, r, i)

∈ T

−

∈ T

∈ T

−

(cid:1).

(20)

∈ T

r(cid:54)=i,j

|T |

We now compute each term in the summation using the notation ψi
ψi. The
expected values of the terms are given in Table 2, where the last column represents the overall term
= i, j in (20). The derivation for these values is identical to the one in the quadruplet
for each r
setting.

[k] to indicate i

∈ C

∈

Table 2: Value of each term in the summation in (20), assuming i, j, r are distinct.

Case

ψi = ψj = ψr
= ψr
ψi = ψj
= ψj = ψr
ψi
= ψj
ψi = ψr
= ψr
= ψj
ψi

∈ T

P(cid:0)(i, j, r)
p/2
p(1 + (cid:15)δ)/2
p/2

p(1

(cid:15)δ)/2

−
p/2

(cid:1) P(cid:0)(i, r, j)
p/2

∈ T

p(1

(cid:15)δ)/2

−
p/2
p(1 + (cid:15)δ)/2
p/2

∈ T

(cid:1) P(cid:0)(j, i, r)
p/2
p(1 + (cid:15)δ)/2
(cid:15)δ)/2
p(1

−
p/2
p/2

(cid:1) P(cid:0)(j, r, i)
p/2

∈ T

p(1
(cid:15)δ)/2
−
p(1 + (cid:15)δ)/2
p/2
p/2

(cid:1) Aggregate
0
2p(cid:15)δ
p(cid:15)δ
p(cid:15)δ
0

−
−

Based on Table 2, we can infer that for i

E[Sij] =

= j such that ψi = ψj,
(cid:88)

2p(cid:15)δ = 2p(cid:15)δ(n

nψi)

−

For i, j such that ψi

= ψj, we have

r /∈Cψi

E[Sij] =

(cid:88)

(

−
r∈Cψi ,r(cid:54)=i

p(cid:15)δ) +

(cid:88)

(
−
r∈Cψj ,r(cid:54)=j

p(cid:15)δ) =

p(cid:15)δ(nψi + nψj −
−

2).

Hence, we deﬁne the ideal similarity matrix as (cid:101)Sij = ZΣZ T , where Σ(cid:96)(cid:96) = 2p(cid:15)δ(n
Σ(cid:96)(cid:96)(cid:48) =

= (cid:96)(cid:48), and we can compute

p(cid:15)δ(n(cid:96) + n(cid:96)(cid:48)

2) for (cid:96)

−

−

∆1 = p(cid:15)δ(n

2).

−

n(cid:96)) and

−

(21)

Preliminary computations and deﬁnitions for concentration. We deﬁne
wij : i < j
{
the collection of random pairwise similarities, and split the concentration of ∆2 and
terms involving S
]
part of the quadruplet setting, and here, we introduce the key random variables. We ﬁrst write

as
}
(cid:107)2 into
(cid:101)S
E[S]. The basic idea is discussed in the corresponding

] and E[S

S
(cid:107)

E[S

|W

|W

W

−

−

=

−

Sij =

=

(cid:88)

r(cid:54)=i,j
(cid:88)

r(cid:54)=i,j

(cid:0)I{(i,j,r)∈T } −

I{(i,r,j)∈T }

(cid:1) + (cid:0)I{(j,i,r)∈T } −

I{(j,r,i)∈T }

(cid:1)

ξijr

(cid:0)I{wij >wir} −

I{wij <wir}

(cid:1) + ξjir

(cid:0)I{wji>wjr} −

I{wji<wjr}

(cid:1)

(22)

24

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
denotes whether the comparison between (i, j) and (i, r) is observed
where ξijr
(ξijr = 0 if not observed), and whether the crowd response was correct (ξijr = +1) or ﬂipped
(ξijr =

1). Under our sampling and noise model,

1, 0, +1
}

∈ {−

−
P(ξijr = 0) = 1

,

−

p,

p(1 + (cid:15))
2
ξijr : j < r, i

P(ξijr =

P(ξijr = 1) =

p. The set Ξ =

−
2
and so, E[ξijr] = p(cid:15) and Var (ξijr)
denotes the collection
of such random variables, where we abuse notation by using ξijr and ξirj to refer to the same variable.
We note that the variables in Ξ are mutually independent.
We use the continuous nature of Fin, Fout to write I{wij >wir} −
further deﬁne
E[Sij
with Bijr = (ξijr
Bijr + Bjir
Sij

I{wij <wir} = 2I{wij >wir} −
p(cid:15))(cid:0)2I{wij >wir} −

1, and

= j, r

1) =

1(cid:1),

(cid:88)

p(1

] =

|W

(cid:15))

−

−

≤

−

}

{

r(cid:54)=i,j

E[Sij

]

−

|W

E[Sij] =

(cid:88)

r(cid:54)=i,j

B(cid:48)

ijr + B(cid:48)

jir with B(cid:48)

ijr = 2p(cid:15)(cid:0)I{wij >wir} −

P(wij > wir)(cid:1).

(23)

The random variables Bijr, B(cid:48)
probability 1, E[Bijr] = E[B(cid:48)

ijr have the following properties:
p and Var (cid:0)B(cid:48)
ijr] = 0, Var (Bijr)
= i, j
= i, r

Bijr
|
(cid:1)
ijr

≤

2,

2p(cid:15) with
ijr| ≤
| ≤
4p2(cid:15)2. We deﬁne the sets

B(cid:48)
|

= i, r

=
Bijr : j
B
{
(cid:48) =
B(cid:48)
ijr : j
B
{
Bijr, Bjir : j
i(cid:96) =
{
B(cid:48)
(cid:48)
jir : j
i(cid:96) =
B
{
(cid:48) have n(n

B

= i, j

≤
,
}
,
}
(cid:96), r
= i, j
(cid:96), r

}

}

B

∈

∈

−

[k],

and

and

∈ C

∈ C

[n], (cid:96)

[n], (cid:96)

= i, j

ijr, B(cid:48)
1)(n

2) random variables, whereas
Birj, but conditioned on

B
. Thus, a dependency graph on

for every i
for every i
∈
(cid:48)
Each of
i(cid:96) have at most 2n(cid:96) (n
2)
−
B
random variables. Note that Bijr =
, Bijr is independent of all other
−
variables in
, has a maximum degree of 1.
B
i(cid:96). On the other hand, B(cid:48)
(cid:48)
The same is also true for
ijr depends on the random variables that involve
B
jri, B(cid:48)
either wij or wir, that is B(cid:48)
jir, B(cid:48)
irj, B(cid:48)
ijr(cid:48) and
B(cid:48)
. Thus each B(cid:48)
(cid:48).
ijr depends on at most 5 + 12(n
}
B
(cid:48)
(cid:48)
The same holds when we restrict the set to
i(cid:96) have
i(cid:96). Thus, the dependency graph for
dependency graph with
(n) maximum degree. We now use the above deﬁned random variables
and their properties to derive upper bounds on ∆2 and
(cid:107)2.
(cid:101)S
Upper bound for ∆2. To derive a bound on ∆2, we ﬁrst note that

rji, as well as all six variants of variables B(cid:48)

(n) variables in
(cid:48) and

W
, conditioned on

B
rij, B(cid:48)

ij(cid:48)r, j(cid:48), r(cid:48) /

i, j, r

3) =

S
(cid:107)

∈ {

[k].

W

i(cid:96),

O

O

−

−

−

∈

B

B

B

B

∆2 ≤

max
i∈[n]

max
(cid:96)∈[k]

1
n(cid:96)

(cid:88)

j∈C(cid:96)

Sij

−

E[Sij

|W

+ max
i∈[n]

max
(cid:96)∈[k]

1
n(cid:96)

(cid:88)

j∈C(cid:96)

E[Sij

]

−

|W

(cid:12)
(cid:12)
(cid:12)
]
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
E[Sij]
(cid:12)
(cid:12)
(cid:12)

+ max
i∈[n]

(cid:101)Sii
nmin

.

In the subsequent steps, we bound the ﬁrst term. For any t > 0, the union bound leads to

(24)



P

max
i∈[n]

max
(cid:96)∈[k]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(cid:96)

(cid:88)

Sij

E[Sij

−

|W



> t



(cid:12)
(cid:12)
(cid:12)
]
(cid:12)
(cid:12)
(cid:12)

(cid:88)

j∈C(cid:96)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
j∈C(cid:96)


(cid:88)

(cid:88)

P

i∈[n]

(cid:96)∈[k]

≤

=

Sij

−

E[Sij

|W



> n(cid:96)t



(cid:12)
(cid:12)
(cid:12)
]
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

i∈[n]

(cid:96)∈[k]

EW

P·|W

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:88)

(cid:88)

j∈C(cid:96)

r(cid:54)=i,j

Bijr + Bjir





> n(cid:96)t





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The summation inside the conditional probability involves terms in
previous discussions show that the dependency graph of
can split the 2n(cid:96)(n
2) variables in
concentration for each of them separately using Bernstein inequality in the following way.
(cid:12)
(cid:12)
(cid:12)
]
(cid:12)
(cid:12)
(cid:12)

i(cid:96) deﬁned in (24), and the
i(cid:96) has maximum degree of 1. Hence, we
Bi(cid:96),(2), and derive

i(cid:96) into two independent sets, say

B
Bi(cid:96),(1) and

max
i∈[n]

−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
(cid:96)∈[k]

E[Sij

1
n(cid:96)

(cid:88)

> t

Sij

|W

j∈C(cid:96)







P

−

B

B

25

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:88)

(cid:88)

i∈[n]

(cid:96)∈[k]

(cid:88)

(cid:88)

≤

≤

i∈[n]

(cid:96)∈[k]
(cid:18)

(cid:46) n2 exp



EW

P·|W

(cid:20)

EW

min

−

2 exp

−
(cid:26) nmint2
pn

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:18)

(cid:88)

B∈Bi(cid:96),(1)

B

>

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(n(cid:96)t/2)2



 + [P·|W

n(cid:96)t
2

(cid:19)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:18)

+ 2 exp

(cid:88)

B∈Bi(cid:96),(2)

n(cid:96)t
2

B

>

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(n(cid:96)t/2)2









(cid:19)(cid:21)

−

2p

|Bi(cid:96),(2)|

+ 2n(cid:96)t/3

+ 2n(cid:96)t/3

2p

|Bi(cid:96),(1)|
(cid:27)(cid:19)

, nmint

,

.



> n(cid:96)t



,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the last step follows by noting that each of the two independent sets have Ω (nn(cid:96)) variables, and
(cid:41)

the bounds are independent of

. The above probability is

W

O

(cid:0) 1
n

(cid:1) for t (cid:38) max

(cid:40)(cid:114) pn ln n
nmin

ln n
nmin

For the second term in the upper bound for ∆2, we have



P

max
i∈[n]

max
(cid:96)∈[k]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n(cid:96)

(cid:88)

j∈C(cid:96)

E[Sij

]

−

|W

(cid:12)
(cid:12)
(cid:12)
E[Sij]
(cid:12)
(cid:12)
(cid:12)



> t



≤

(cid:88)

(cid:88)

i∈[n]

(cid:96)∈[k]

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

P

(cid:88)

(cid:88)

j∈C(cid:96)

r(cid:54)=i,j

B(cid:48)

ijr + B(cid:48)

jir

B

(cid:48)
i(cid:96) has maximum degree d =

where the tail bound is for the sum of all random variables in
on
(cid:48)
independent set of size
i(cid:96)|
(cid:98)|B
(cid:48)
independent sets by
i(cid:96),(1), . . . ,
each independent set. Hence, we bound the probability for every i, (cid:96), as

(cid:48)
i(cid:96). Since the dependency graph
(n), we can obtain an equitable (d + 1)-colouring with each
O
, which are smaller than n(cid:96). We denote the
/(d + 1)
(cid:99)
(cid:48)
i(cid:96),(d+1), and use Bernstein inequality to bound the summation over

/(d + 1)
(cid:101)

(cid:48)
i(cid:96)|

(cid:100)|B

or

B

B

B

P

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:88)

(cid:88)

j∈C(cid:96)

r(cid:54)=i,j

B(cid:48)

ijr + B(cid:48)

jir

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





> n(cid:96)t



P

 max

r∈{1,...,d+1}

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





>

n(cid:96)t
(d + 1)

B(cid:48)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

B(cid:48)∈B(cid:48)
i(cid:96),(r)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B(cid:48)

>



P



d+1
(cid:88)

r=1

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
B(cid:48)∈B(cid:48)
i(cid:96),(r)
(cid:32)

2(d + 1) exp

≤

≤

(cid:46) n exp

(cid:18)

min

−

8p2(cid:15)2
−
(cid:26) n(cid:96)t2

|B
p2(cid:15)2n2 ,





n(cid:96)t
(d + 1)

( n(cid:96)t
(d+1) )2
+ 4
(cid:48)
i(cid:96),(r)|
(cid:27)(cid:19)
n(cid:96)t
p(cid:15)n

,

(cid:33)

3 p(cid:15) n(cid:96)t

(d+1)

(cid:0) 1
n3

which is

(cid:1) for t (cid:38) p(cid:15)n

ln n
nmin
which arises due to the condition on δ. Combining the above discussions we claim that, with
probability 1

. The ﬁrst term dominates for nmin (cid:38) ln n,

max

O

·

,

(cid:40)(cid:114) ln n
nmin

(cid:41)

1
4n ,

−

∆2 (cid:46) max

(cid:40)(cid:114) pn ln n
nmin

,

ln n
nmin

, p(cid:15)n

(cid:114) ln n
nmin

(cid:41)

,

,

p(cid:15)δn
nmin

(25)

where the last term is obviously dominated by the third term.

(cid:107)

S

(cid:101)S

−

−

E[S

S
(cid:107)

S
(cid:107)2 ≤ (cid:107)

Upper bound for

(cid:107)2. Similar to the case of ∆2, we bound
(cid:101)S
E[S]
(cid:107)2 +
. Recall
where the last term equals maxi (cid:101)Sii. For the ﬁrst term, we derive a bound conditioned on
, which
from (23)–(24) that, conditioned on
] comprises of variables in
−
into two independent sets via equitable
has a dependence graph with degree 1. We partition
] = A + A(cid:48), where A and A(cid:48) are the matrices corresponding to each
E[S
colouring, and write S
of the independent sets. We derive a spectral norm for each of A and A(cid:48). For this, we ﬁrst claim
(cid:9) (cid:46) max (cid:8)√pn ln n, ln n(cid:9)(cid:9) occurs
that, conditioned on

|W
, the event

, the matrix S

= (cid:8)maxi,j

E[S]
(cid:107)

E[S
(cid:107)

(cid:107)2 +

(cid:107)2,
(cid:101)S

W
B

E[S

|W

|W

|W

W

−

−

−

−

B

(cid:8)

]

]

W

E

Aij
|

,
|

A(cid:48)
|

ij|

26

2) independent

(cid:27)(cid:19)

−

, τ

with probability 1
random variables Bijr, Bjir. By Bernstein inequality,

(cid:1). To see this, observe that Aij (or A(cid:48)

(cid:0) 1
n

− O

ij) is a sum of 2(n

P·|W (
|

Aij

> τ )

2 exp

|

≤

−

4p(n

(cid:18)

τ 2
2) + 4
3 τ

(cid:19)

(cid:18)

(cid:46) exp

(cid:26) τ 2
pn

min

−

−
(cid:1) for τ (cid:38) max (cid:8)√pn ln n, ln n(cid:9). Applying the union bound gives P(
E

(cid:1).
, the matrices A, A(cid:48) have independent zero mean entries, with each en-
and
(cid:0)max (cid:8)√pn ln n, ln n(cid:9)(cid:1). Furthermore, from the variance of Bijr, we have
j Var (Aij) < 2pn2, and the same holds for A(cid:48). Hence, by matrix Bernstein inequality

c) =

W
O

(cid:0) 1
n

O

E

O

which is

(cid:0) 1
n3
Conditioned on
try bounded by
(cid:80)
maxi
(Tropp, 2012),

P·|W,E (

S
(cid:107)

−

E[S

]

(cid:107)2 > t)

≤

|W

P·|W,E (
A
(cid:107)
(cid:32)

A(cid:48)
(cid:107)2 > t/2) + P·|W,E (
(cid:107)

(cid:107)2 > t/2)
(cid:33)

t2/4

2n exp

≤

−

(cid:46) n exp

(cid:18)

−

pn2 + 1
3 t
·
(cid:26) t2
pn2 ,

min

max (cid:8)√pn ln n, ln n(cid:9)
(cid:46) 1
t
n
ln n

t
√pn ln n

(cid:27)(cid:19)

,

S

−

(cid:112)

E[S

(cid:110)(cid:112)

to obtain

(cid:107)
E[S

pn2 ln n ,

pn2 ln n , (ln n)2(cid:111)

pn(ln n)3 , (ln n)2(cid:111)
for t (cid:38)
large enough. As in the quadruplet setting, we add the probability P(
(cid:110)(cid:112)

, where the second term is smaller than the ﬁrst for n
c) and take expectation over

(n), we partition

with probability 1

|W
E[S]
(cid:48) deﬁned in (24). Since the dependency graph for

(cid:107)2 (cid:46)
]
W
(cid:107)2, we note that the entries of the matrix comprises of mutually dependent
To bound
]
|W
(cid:107)
(cid:48) has maximum degree
variables in the set
(cid:48) into d + 1 independent sets of nearly identical sizes (equitable colouring).
d =
E[S] = A(1) + . . . + A(d+1) denote the corresponding partition of the matrix, where
Let E[S
|W
Rn×n is a symmetric matrix consisting of the variables in the (cid:96)-th independent set. Due to the
A((cid:96))
independence of the variables, we have A((cid:96))
ji = B(cid:48)
= i, j. Hence, each
A((cid:96)) is a symmetric matrix with independent zero-mean entries, bounded by 2p(cid:15) and variance at most
4p2(cid:15)2 (follows from properties of B(cid:48)
ijr). Thus, by matrix Bernstein inequality (Tropp, 2012), we have

jir for some r

ij = A((cid:96))

ijr or B(cid:48)

(cid:0) 1
n

− O

−
B

(cid:1).

O

−

∈

B

B

E

]

(cid:16)

P

(cid:17)

A((cid:96))
(cid:107)

(cid:107)2 > τ

n exp

≤

(cid:18)

t2

−

8p2(cid:15)2n + 2

3 p(cid:15)t

(cid:19)

,

and combining with the union bound,

E[S

P (
(cid:107)

]

−

|W

E[S]

(cid:107)2 > t)

≤

P

(cid:18)

max
(cid:96)∈[d+1] (cid:107)

A((cid:96))

(cid:32)

(cid:19)

(cid:107)2 >

t
d + 1
( t
d+1 )2
8p2(cid:15)2n + 2

(cid:33)

n(d + 1) exp

≤

(cid:46) n2 exp

(cid:18)

min

−

−
(cid:26) t2

p2(cid:15)2n3 ,

3 p(cid:15) t
d+1
(cid:27)(cid:19)
t
p(cid:15)n

,

which is

(cid:0) 1
n

(cid:1) for t (cid:38) p(cid:15)n

(cid:110)

max

√n ln n, ln n

(cid:111)

·
Combining the above derivations, we have with probability 1

O

1
4n ,

−

, where the ﬁrst term obviously dominates.

S

(cid:107)

−

(cid:107)2 (cid:46) max
(cid:101)S

(cid:110)(cid:112)

pn2 ln n , (ln n)2 , p(cid:15)n√n ln n , p(cid:15)δn

(cid:111)

(26)

where the last term (arising due to maxi (cid:101)Sii) is dominated by the third.
. We now use (21), (25) and (26) to complete the proof for
Deriving interval for λ in terms of
the triplet setting. We verify the conditions in Proposition 1 by deriving conditions under which
∆2 < 1
2 nmin∆1. Similar to the proof for the quadruplet setting, we compare
the upper bounds in (16) and (17) with ∆1 and nmin∆1, respectively. As in the previous setting, the

(cid:107)2 < 1
(cid:101)S

12 ∆1 and

S
(cid:107)

|T |

−

27

(cid:54)
ﬁrst two bounds in (16)–(17) lead to conditions on p, while the third term leads to a condition on δ.
Combining the different cases, it follows that if

δ (cid:38)

and

√n ln n
nmin
(cid:107)2 are satisﬁed, and by Proposition 1, X ∗ is the unique optimal
(cid:101)S

p (cid:38) (ln n)2
(cid:15)2δ2n2

(27)

min

,

then the criteria for ∆2 and
solution for SDP-λ with the range of λ given by

−

S

(cid:107)

S

(cid:107)

−

(cid:107)2 (cid:46) max
(cid:101)S

(cid:110)(cid:112)

pn2 ln n , p(cid:15)√n3 ln n , (ln n)2(cid:111)

(cid:46) λ <

p(cid:15)δnmin(n

2

2)

−

=

∆1
2

. (28)

We ﬁnally show that the condition on p holds under the stated condition of

|T |

state the above interval for λ in terms of
independently with probability p, we E[
easy to verify that for p (cid:38) ln n
probability 1
which leads to the statement of Theorem 1 in the triplet setting.

n3 or equivalently
(cid:1). Hence, we can replace p by |T |

. Under the assumption that each triplet is observed
|T |
(cid:0)pn3(cid:1). By Bernstein inequality, it is
] = pn(cid:0)n−1
](cid:1) with
(cid:0) 1
2 E[
n3 in (27)–(28) up to differences in constants,

(cid:1) =
(cid:38) ln n, we have

|T | ∈

2 E[

(cid:0) 1
n

− O

], 3

|T |

|T |

|T |

|T |

O

2

(cid:38) n3(ln n)2
(cid:15)2δ2n2

min

, and

D Algorithmic details

In this section, we provide details on the modiﬁed SPUR algorithm that we use to tune the parameter
λ, and to select the number of clusters.

SPUR, acronym for Semideﬁnite Program with Unknown r (r denoting the number of clusters), was
proposed by Yan et al. (2018) to tune the parameter λ of SDP-λ in the context of graph clustering
(see Algorithm 1). The underlying idea of this approach is to search for the optimal λ using a grid
search over the range 0 < λ < λmax, where λmax is derived from an exact recovery result under
stochastic block model.

Algorithm 1: Semideﬁnite Program with Unknown k (SPUR).
input
begin

: graph A, number of candidates T .

for t = 1 to T do
T ln (1 + λmax)(cid:1)
λt = exp (cid:0) t
Solve SDP-λ with λ = λt to obtain Xt.
Estimate kt = integer approximation of trace (Xt).

−

1.

(Yan et al. (2018) set λmax =

A

(cid:107)op)

(cid:107)

end

Choose ˆt = arg max

t

(cid:80)

i≤kt σi(Xt)
trace (Xt)

end
output : Number of clusters kˆt, Xˆt.

, where σi(Xt) denotes i-th largest eigenvalue of Xt.

n (for triplets) or |Q|

In the present setting, Theorem 1 shows that the planted clusters can be exactly recovered given a
sufﬁcient number of comparisons and an appropriate choice of λ. From Theorem 1, a candidate for
λmax can be chosen as |T |
n (for quadruplets), which is a loose upper bound for the
n. Thus, following Yan et al. (2018), we
theoretical interval for λ, obtained by noting that (cid:15)δnmin ≤
could use Algorithm 1 with our choice of λmax.
Unfortunately, this approach has two main drawbacks. First, it ignores the lower bound in Theorem 1
and, second, setting T , the number of λ values that should be considered in Algorithm 1, is difﬁcult. To
address the former issue, we propose to consider Theorem 1 once more and to use λmin =
c(ln n)/n
as a lower bound for λ instead of 0, as used in Yan et al. (2018). To address the latter issue, we use
the fact that the estimated number of clusters k monotonically decreases with λ as shown in the next
Lemma.

(cid:112)

28

Lemma 1 (The estimated number of clusters decreases monotonically with increasing λ). For
any λ > 0, let Xλ denote the solution of SDP-λ and kλ =
be the integer approximation
of trace (Xλ), which is an estimate of the number of clusters. Then, kλ is a non-increasing function
of λ, that is

trace (Xλ)
(cid:101)

(cid:98)

λ(cid:48)

λ

≥

⇒

kλ(cid:48)

≤

kλ.

Proof. We start this proof by noting that since kλ is the integer approximation of trace (Xλ), it
sufﬁces to show that trace (Xλ) is a non-increasing function of λ. Then, consider distinct λ(cid:48), λ and
let Xλ(cid:48), Xλ be the solutions of SDP-λ with parameters λ(cid:48), λ, respectively. We have

trace (SXλ)
trace (SXλ)

−

λtrace (Xλ)
λ(cid:48)trace (Xλ)

≥

trace (SXλ(cid:48))
trace (SXλ(cid:48))

−

−
−
Subtracting the second inequality from the ﬁrst inequality implies

≤

trace (SXλ)

λtrace (Xλ)

−

−

≥

which implies

(trace (SXλ)
trace (SXλ(cid:48))

λ(cid:48)trace (Xλ))
λtrace (Xλ(cid:48))

−

−

−

λtrace (Xλ(cid:48)) ,
λ(cid:48)trace (Xλ(cid:48)) .

trace (SXλ(cid:48)) + λ(cid:48)trace (Xλ(cid:48))

(λ(cid:48)

−

λ)trace (Xλ)

(λ(cid:48)

≥

λ)(trace (Xλ(cid:48))

trace (Xλ))

λ)trace (Xλ(cid:48))
−
0. Thus, for λ(cid:48) > λ, we can conclude that

≤
trace (Xλ), which shows that trace (Xλ) and kλ are non-increasing functions of λ.

−

−

or equivalently, (λ(cid:48)
trace (Xλ(cid:48))

≤

Following this, using λmin and λmax, we get two estimates of the number of clusters, kλmin and
[kλmax, kλmin ] instead of searching over λ—in practice, it helps
kλmax. Then, we search over k
kλmin + 2. We select k that maximises the above
to search over the values max
{
SPUR objective, where Xk is computed using the simpler SDP-k (Yan et al., 2018). This approach is
summarized in Algorithm 2.

∈
2, kλmax} ≤

≤

k

Algorithm 2: Comparison-based SPUR
input
or
begin

: n and

Q

T

or

|T |

Deﬁne c =
|Q|
Let S be obtained with AddS-3 or AddS-4
(cid:113) c(ln c)
n

Deﬁne λmin =
Xλmin, Xλmax ←
kλmin , kλmax ← (cid:98)
for k = max
2, kλmax}
{
Solve SDP-k with k to obtain Xk.

and λmax = c
n
SDP-λmin, SDP-λmax on S
trace (Xλmin)
,
(cid:101)
(cid:98)
to kλmin + 2 do

trace (Xλmax)

(cid:101)

end
Choose ˆk = argmax

k

(cid:80)

i≤k σi(Xk)
trace(Xk)

end
output : Number of clusters ˆk, Xˆk.

, where σi(Xk) denotes i-th largest eigenvalue of Xk.

E Additional results for the planted model

In this section, we provide additional experiments on our planted model. We show that changing
the clustering method used in the last step of our approach to cluster the matrix X learned by
SDP-λ or SDP-k does not affect the results. We demonstrate that, given a sufﬁcient number of
comparisons, SPUR correctly estimates the number of clusters. We give details on the distributions
used in Figure 1c. Finally, we consider several additional experiments where we vary the planted
model parameters that were ignored in Section 5 in the main paper.

29

SPUR with k-means

SPUR with spectral

known k with k-means known k with spectral

Figure 3: Comparing clustering algorithms to partition X in the last step. Using k-means or spectral
clustering does not affect the output of our approach.

E.1 Clustering method in the last step

In the last step of our approach, we use k-means to cluster the learned matrix Xk. We experimentally
demonstrate here that the partition obtained is, in fact, independent of the clustering algorithm used in
this step. Hence, in Figure 3, we compare spectral clustering with k-means. As in the main paper, we
here consider varying the number of observations,
and varying the crowd noise (cid:15) for both the
setting where k is estimated by SPUR and where we consider k to be known. There is no differences
between the ARI obtained when using k-means or spectral clustering.

|Q|

|T |

,

E.2 Compare SPUR with known k

An important question is how good is SPUR at estimating the true number of clusters. We illustrate
this in Figure 4. We start by comparing the ﬁrst two columns, showing how the ARI changes for
= n(ln n)3 we see that using
various parameters of the planted model. In the setting of
a known number of clusters outperforms SPUR, especially in parameter ranges that are harder to
= n(ln n)4, SPUR
cluster (e.g. small δ, (cid:15) or for a larger number of clusters). If we consider
correctly estimates the number of clusters and thus we omit the plots with known k.

,
|Q|

,
|Q|

|T |

|T |

E.3 Experimental details for changing Fin, Fout in the planted model

In this section, we give implementation details on the different distributions considered in Figure 1c.
In the following let φ be the normal pdf and Φ the normal cdf. Recall that, in all the experiments, we
ﬁx δ = 0.5 as the default.

Parameters for Fin and Fout normal distributions. Let Fin =
(µout, σ).
We ﬁx σ = 0.1 and µout = 0. Using δ we can compute µin. Indeed, in this case, the cumula-
tive distribution function is known and, thus, by setting it equal to Pw∼Fin,w(cid:48)∼Fout(w > w(cid:48)) =
1+δ
(0, 1] (as given in Equation (3)) we directly get the δ deﬁned in Section 2:
2
δ = 2Φ (cid:0)(µin
(cid:1).
Parameters for Fin and Fout Beta distributions. Let Fin = Beta(α, β), Fout = Beta(α(cid:48), β(cid:48)). We
set α(cid:48) = β(cid:48) = 1 such that Fout = Beta(1, 1) = Unif(0, 1). We can then compute

1. Then, assuming that µout = 0, we get µin = √2σΦ−1 (cid:0) 1+δ

∈
µout)/(√2σ)(cid:1)

(µin, σ) and Fout =

for some δ

N

N

−

−

2

Pw∼Beta(α,β),w(cid:48)∼Beta(1,1)(w > w(cid:48)) = Ew

(cid:20)(cid:90) w

(cid:21)

dw(cid:48)

0
= Ew [w]
α
α + β

=

30

n(lnn)1n(lnn)3.5n(lnn)6|T|,|Q|0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3n(lnn)1n(lnn)3.5n(lnn)6|T|,|Q|0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3n(lnn)1n(lnn)3.5n(lnn)6|T|,|Q|0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3n(lnn)1n(lnn)3.5n(lnn)6|T|,|Q|0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.81.0(cid:15)0.00.20.40.60.81.0ARIAddS-3MulK-3n(lnn)3n(lnn)40.40.60.81.0(cid:15)0.00.20.40.60.81.0ARIAddS-3MulK-3n(lnn)3n(lnn)40.40.60.81.0(cid:15)0.00.20.40.60.81.0ARIAddS-3MulK-3n(lnn)3n(lnn)40.40.60.81.0(cid:15)0.00.20.40.60.81.0ARIAddS-3MulK-3n(lnn)3n(lnn)4where the last line follows from the mean of the Beta distribution. Setting this equal to 1+δ
2

and

solving for α gives: α = β

. In our experiments, we ﬁx β = 2.

(cid:17)

(cid:16) 1+δ
1−δ

Parameters for Fin Normal and Fout Uniform. Let Fin =
we compute:

N

(µ, 0), Fout = Unif(0, 1). To set µ,

Pw∼N (µ,0),w(cid:48)∼Unif(0,1)(w > w(cid:48)) =

(cid:90) ∞

0
(cid:90) 1

φ(w

−

µ)dw

(cid:34)(cid:90) min(w,1)

(cid:35)

dw(cid:48)

0
(cid:90) ∞

−
Solving numerically for µ gives µ = 1+δ
2 .

0
=1 + φ(

=

wφ(w

µ)dw +

φ(w

µ)dw + µ (Φ(1

−

µ)

−

φ(1

−

1
µ) + (µ

−

−
1)Φ(1

µ)

−

−

µΦ(

−

µ)

µ)

Φ(

−

−

µ))

−

E.4 Inﬂuence of different planted model parameters

In this section we present additional experiments where we vary various parameters of the planted
model. Recall that we consider the following parameters as default: n = 1000, k = 4, (cid:15) = 0.75,
(cid:0)0, σ2(cid:1) with σ = 0.1 and

= n(ln n)4 and Fin =

(cid:1) , σ2(cid:1) , Fout =

(cid:0)√2σΦ−1 (cid:0) 1+δ

N

2

N

=
|T |
δ = 0.5.

|Q|

= n(ln n)3 there
Number of samples n, ﬁrst row in Figure 4. We can ﬁrst note that for
is no difference in the behaviour between SPUR and known k. Both AddS-3 and AddS-4 achieve
full recovery while MulK-3 and MulK-4 predictions are random. To learn somewhat meaningful
partitions with MulK-3, one needs to increase the number of observations to n(ln n)4. However, even
with this many comparisons, MulK-4 still learns random clusters.

,
|Q|

|T |

= n(ln n)3, we see that, for both SPUR
Intrinsic noise δ, second row in Figure 4. Using
and known k, AddS-3 and AddS-4 exactly recover the clusters even when the intrinsic noise is high,
that is δ = 0.4. MulK-3 and MulK-4 can only make random predictions in this case. When the
number of observations increases to n(ln n)4, AddS-3 and AddS-4 exactly recover the clusters even
for values of δ that are as small as 0.25. In this case, MulK-4 still predicts random clusters, while
0.6.
MulK-3 is able to recover the clusters when the intrinsic noise is sufﬁciently small, that is δ

,
|Q|

|T |

Crowd noise (cid:15), third row in Figure 4. This parameter was already analyzed in the main paper. The
plots are recalled here for the sake of completeness.

≥

Number of clusters k, fourth row in Figure 4. Finally, we vary the number of planted clusters. Here,
= n(ln n)3,
we observe the most noticeable difference between SPUR and known k. For
AddS-3 and AddS-4 with SPUR achieve perfect recovery for up to ﬁve clusters. While we notice a
similar behaviour for AddS-3 and AddS-4 with known k, the drop in ARI only starts for k > 7 and
is far less important than with SPUR. For n(ln n)4 observations AddS-3 and AddS-4 consistently
recover all the clusters. On the other hand, MulK-3 only recovers clusters up to k = 3 (here, MulK-3
uses the number of clusters estimated by AddS-3 with SPUR, that is k = 3). Once again, MulK-4
can only make random predictions.

,
|Q|

|T |

F Further results for experiments on real comparison based data

In this ﬁnal section we present supporting results for the real data experiments presented in Section 5.

F.1 Details on the Car dataset

The Car dataset (Kleindessner and von Luxburg, 2016) is a comparison based dataset that contains 60
examples grouped into 3 classes (SUV, city cars, sport cars) with 4 outliers. This dataset originally
comes with a set of 6056 comparisons of the form “xi is most central in the triple xi, xj, xk.” Each
of these comparisons corresponds to two triplets: “xj is more similar to xi than to xk” and “xk is
more similar to xi than to xj.” Hence, we have access to 12112 triplet comparisons.

31

n(ln n)3 with SPUR

n(ln n)3 with known k

n(ln n)4 with SPUR

Figure 4: Further experiments on the planted model. On the one hand, SPUR needs sufﬁciently
many comparisons to correctly estimate the number of underlying clusters. On the other hand, our
approaches are not overly sensitive to changes in the planted model parameters and are able to exactly
recover the planted clusters with n(ln n)3 comparisons even in fairly difﬁcult cases (small δ, high
k, . . . ). Furthermore, given n(ln n)4 comparisons, our approaches are able to exactly recover the
planted clusters in all the considered cases.

32

250500750100012501500n0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3250500750100012501500n0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3250500750100012501500n0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.8δ0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.8δ0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.8δ0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.81.0(cid:15)0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.81.0(cid:15)0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-30.40.60.81.0(cid:15)0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3246810k0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3246810k0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3246810k0.00.20.40.60.81.0ARIAddS-4MulK-4AddS-3MulK-3Figure 5: ARI between the clustering obtained by the different baselines. AddS-3 and AddS-4 with
SPUR both estimate that the number of cluster is k = 2. There is a high degree of agreement between
the different approaches.

F.2 Food Dataset

In addition to the Car dataset we now look at a second comparison based dataset called Food (Wilber
et al., 2014). It contains 100 food images and comes with 190376 triplet comparisons. Since there
are no ground truth labels for the food dataset, we use the number of clusters estimated by SPUR for
all methods and plot, in Figure 5, the similarity matrix between the different clustering approaches
considered. Here, there is a high degree of agreement between all the clustering methods. Thus, most
approaches predict the same clusters up to minor differences for a few data points. In Figure 7, we
plot the clusters obtained by AddS-3 with SPUR (estimated k is 2). The two clusters seem to separate
Sweet foods from Savoury foods. Intuitively, it seems indeed natural for humans to judge that two
sweet foods are more similar to each other than to a third savoury food.

F.3 MNIST

In this section, we consider additional experiments on the MNIST dataset. First, we consider a second
similarity measure to generate the triplets. Then, we illustrate the partitions obtained with AddS-3
with known k and SPUR respectively.

Gaussian similarity. In the main paper, we use the Gaussian similarity to generate the comparisons.
More precisely, we compute the similarity between two examples xi and xj as

wij = exp

(cid:33)

(cid:32)

xi
(cid:107)

2
2

(cid:107)

xj

−
γ2

with γ = 1.

Cosine similarity. Instead of the Gaussian similarity, we could consider alternatives to generate the
comparisons. For example, the Cosine similarity:

wij = (cid:104)
xi
(cid:107)

xi, xj
(cid:105)
xj
(cid:107)2

(cid:107)2 (cid:107)

.

In Figure 6, we show that using this alternative similarity affects the absolute results of the considered
approaches. However, it does not change the overall trend, that is, as the number of comparisons
increases, AddS-3 converges to the baseline of k-means with access to the original similarities.

Clustering using known k. Figure 8a shows the t-SNE embedding of 2000 MNIST samples of all
ten classes, where we see a clear separation between some classes (for example, 0 and 1) and very
close embedding between others (for example, 1 and 9). Note that the classes obtained by AddS-3
are shown up to permutations and may not reﬂect the majority label in the different clusters. Further
note that the data presented here corresponds to a single repetition out of the 10 repetitions used to
compute the mean ARI (with standard deviation) in the main paper and this appendix. In Figure 8d,
= n(ln n)2, the learned partition is not very representative of the original labels.
we see that, for
= n(ln n)3, the recovery
Figure 8c shows that, when the number of comparisons increases to
ability of AddS-3 is greatly improved. However, the obtained partitions are not entirely satisfactory.
= n(ln n)4,
Finally, Figure 8b shows that, when the number of comparisons further increases to
the clustering obtained is close to the true labeling and most clusters are correctly identiﬁed.

|T |

|T |

|T |

Clustering using SPUR. In this second set of experiments, we extend our observations from the
previous paragraph to the labeling obtained by AddS-3 using SPUR. One can note that SPUR always

33

AddS-3SPURMulK-3k=2t-STEk=2AddS-3SPURAddS-4SPURMulK-3k=2MulK-4k=2t-STEk=20.8500.8750.9000.9250.9500.9751.000(a) MNIST 1vs.7, n = 2163

(b) MNIST 10, n = 2000

Figure 6: Experiments on MNIST using the cosine similarity. The absolute ARI performances are
different from the Gaussian similarity. However, the overall trend is preserved and, given sufﬁciently
many comparisons, all the ordinal baselines reach the performance of k-means on the original data.

= n(ln n)3, the number of
underestimates the number of clusters. Hence, in Figure 9a, with
= n(ln n)4, the number of predicted clusters
predicted clusters is k = 6 while, in Figure 9b, with
is k = 8. This explain the slightly worse behaviour of SPUR compared to known k in Figure 2b in the
= n(ln n)4,
main paper. Nevertheless, the difference in average ARI is not so signiﬁcant when
suggesting that 8 clusters is, in fact, a good estimate of the number of clusters that can reliably be
distinguished by the different methods.

|T |

|T |

|T |

34

n(lnn)2n(lnn)3n(lnn)40.00.20.40.6ARIk-means,k=2MulK-3,k=2AddS-3,SPURAddS-3,k=2t-STE,k=2n(lnn)2n(lnn)3n(lnn)40.00.10.20.30.40.5k-means,k=10MulK-3,k=10AddS-3,SPURAddS-3,k=10t-STE,k=10n(lnn)2n(lnn)3n(lnn)40.00.20.40.6ARIk-means,k=2MulK-3,k=2AddS-3,SPURAddS-3,k=2t-STE,k=2n(lnn)2n(lnn)3n(lnn)40.00.10.20.30.40.5k-means,k=10MulK-3,k=10AddS-3,SPURAddS-3,k=10t-STE,k=10Class A with 36 images (Sweet?)

Class B with 64 images (Savoury?)

Figure 7: Clusters obtained by AddS-3 on the food dataset. It seems that the Sweet foods are separated
from the Savoury ones.

35

(a) MNIST embedding with true labels

(b) AddS-3 k = 10, |T | = n(ln n)2

(c) AddS-3 k = 10, |T | = n(ln n)3

(d) AddS-3 k = 10, |T | = n(ln n)4

Figure 8: t-SNE embedding of 2000 MNIST samples with (8a) true labeling and (8d)–(8b) clusters
obtained by AddS-3 with known k = 10 and varying number of observations. The classes are given
up to permutations and may not reﬂect the majority label in each cluster.

(a) AddS-3 SPUR, |T | = n(ln n)3

(b) AddS-3 SPUR, |T | = n(ln n)4

Figure 9: t-SNE embedding of 2000 MNIST samples with the clusters predicted by AddS-3 using
SPUR and varying number of comparisons. The classes are given up to permutations and may not
reﬂect the majority label in each cluster.

36

−1.0−0.50.00.5−0.75−0.50−0.250.000.250.500.75Classes0123456789−1.0−0.50.00.5−0.75−0.50−0.250.000.250.500.75Classes0123456789−1.0−0.50.00.5−0.75−0.50−0.250.000.250.500.75Classes0123456789−1.0−0.50.00.5−0.75−0.50−0.250.000.250.500.75Classes0123456789−1.0−0.50.00.5−0.75−0.50−0.250.000.250.500.75Classes012345−1.0−0.50.00.5−0.75−0.50−0.250.000.250.500.75Classes01234567