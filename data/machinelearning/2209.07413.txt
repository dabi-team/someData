2
2
0
2

t
c
O
2
1

]

G
L
.
s
c
[

2
v
3
1
4
7
0
.
9
0
2
2
:
v
i
X
r
a

EZNAS: Evolving Zero-Cost Proxies For Neural
Architecture Scoring

Yash Akhauri1

J. Pablo Muñoz2 Nilesh Jain2 Ravi Iyer2

1Cornell University

2Intel Labs

ya255@cornell.edu
{pablo.munoz,nilesh.jain,ravishanker.iyer}@intel.com

Abstract

Neural Architecture Search (NAS) has signiﬁcantly improved productivity in
the design and deployment of neural networks (NN). As NAS typically evaluates
multiple models by training them partially or completely, the improved productivity
comes at the cost of signiﬁcant carbon footprint. To alleviate this expensive training
routine, zero-shot/cost proxies analyze an NN at initialization to generate a score,
which correlates highly with its true accuracy. Zero-cost proxies are currently
designed by experts conducting multiple cycles of empirical testing on possible
algorithms, datasets, and neural architecture design spaces. This experimentation
lowers productivity and is an unsustainable approach towards zero-cost proxy
design as deep learning use-cases diversify in nature. Additionally, existing zero-
cost proxies fail to generalize across neural architecture design spaces. In this
paper, we propose a genetic programming framework to automate the discovery
of zero-cost proxies for neural architecture scoring. Our methodology efﬁciently
discovers an interpretable and generalizable zero-cost proxy that gives state of
the art score-accuracy correlation on all datasets and search spaces of NASBench-
201 and Network Design Spaces (NDS). We believe that this research indicates a
promising direction towards automatically discovering zero-cost proxies that can
work across network architecture design spaces, datasets, and tasks.

1

Introduction

The manual trial-and-error method of designing neural network architectures and assessing whether
it meets performance and accuracy requirements is not scalable to complex neural architecture design
spaces and hardware deployment scenarios. Neural Architecture Search is intended to address this
problem of iterative and expensive design by automating the search of neural network architectures.
Enabling such automation requires accurate and efﬁcient prediction of the accuracy of candidate
neural network architectures. The process of predicting the accuracy often involves partial or complete
training of many neural network architectures in the search space. Such methods are infeasible for
very large architecture design spaces, as considerable sampling and training of neural networks
would be required to sufﬁciently describe the entire design space. Popular NAS techniques often
involve generating and training neural network architectures and using this accuracy to train accuracy
predictors to serve as feedback in the search algorithm. This architecture generation and feedback
methodology can be leveraged by algorithms such as reinforcement learning (Luo et al. [2019]) and
evolutionary algorithms (Real et al. [2019]). One-shot methods (Cai et al. [2019, 2020], Xie et al.
[2020], Yang et al. [2020], Liu et al. [2019]) maintain a super-network, from which sub-networks are
sampled. The weights are shared between the sub-networks which reduces NAS cost as there are
lesser parameters to train.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
The primary challenge of Neural Architecture Search is the evaluation of candidate architectures,
which can take hours to days to estimate (Abdelfattah et al. [2021]). Further, practical deployment
of neural networks is no longer limited to accuracy-oriented neural architecture design. The need
for efﬁcient deployment has given rise to signiﬁcant literature in co-design of hardware and neural
network architectures (Choi et al. [2021], Akhauri et al. [2021], Zhang et al. [2020]). However,
hardware performance metrics in such co-design tasks are often non-differentiable in nature due to
factors such as memory access patterns and cache hierarchy. The interaction between the workload
and the hardware makes this optimization problem signiﬁcantly more complex, and being able to score
neural network architectures without the expensive training step would allow for the development of
more efﬁcient co-design algorithms.

Most zero-cost proxies utilize a single minibatch of data and a single forward/backward propagation
pass to score a neural network (Abdelfattah et al. [2021]). We refer to algorithms that score neural
network architectures without training as Zero-Cost Neural Architecture Scoring Metrics (ZC-NASM).
Design of existing ZC-NASMs are driven by human intuition or are theoretically inspired. These
algorithms attempt to quantify the trainability and expressivity of neural networks. The primary hurdle
with existing ZC-NASM design is that they are not generalizable to different neural architecture
design spaces.

In this paper, we introduce a genetic programming driven methodology to automatically discover
ZC-NASMs that are interpretable, generalizable and deliver state of the art score-accuracy correlation.
We refer to our method as EZNAS (Evolutionarily Generated Zero-Cost Neural Architecture Scoring
Metric). Our approach is:

• Interpretable: We discover ZC-NASMs as expression trees that explicitly indicate which

NN features and mathematical operations are being utilized.

• Generalizable: The ZC-NASM discovered by EZNAS delivers SoTA score-accuracy
correlation on unseen neural architecture design spaces and datasets (NASBench-201,
NDS & NATS-Bench). Our framework utilizes simple mathematical operations and an
expression tree structure that can be trivially extended to implement arbitrarily complex
ZC-NASMs

• Efﬁcient: We are able to discover our SoTA ZC-NASM on an Intel(R) Xeon(R) Gold
6242 CPU in under 24 hours. This requires 12.5× lesser CO2e than a NAS search. Our
ZC-NASM is generalizable to multiple design spaces, increasing the end-to-end efﬁciency
of NN architecture search by over two orders of magnitude.

2 Related Work

Neural Architecture Search (NAS) was proposed to reduce the human effort that goes into
manually designing complex neural network architectures. Early efforts in the ﬁeld of NAS adopted
brute force methods by training candidate architectures and using the obtained accuracy as a proxy to
discover better architectures. AmoebaNet (Real et al. [2019]) utilized evolutionary algorithms (EA)
and 3150 GPU days of compute to achieve 74.5% top-1 accuracy on the ImageNet dataset. Many EA
and Reinforcement Learning (RL) driven methods have since signiﬁcantly improved the efﬁciency of
the search process (Yang et al. [2020], Liu et al. [2018], Tan et al. [2019], Zoph et al. [2018]). These
methods often require sampling and training of several candidate architectures. One-shot (Hu et al.
[2020], Xie et al. [2020], Cai et al. [2019]) methods of NAS do not require training of candidate
architectures to completion, but train large super-networks and identify sub-networks that can give
high accuracy. Such super-networks can be generated automatically from pre-trained models (Muñoz
et al. [2021]). These improvements have signiﬁcantly reduced the cost of NAS. However, as the
search space continues to get larger with new architectural innovations, we need more efﬁcient
methods to predict the accuracy of neural networks in intractably large design spaces.

Zero-Cost Neural Architecture Scoring is a promising paradigm which explores zero-cost proxies
for estimating the true accuracy of neural networks. The majority of research in this ﬁeld introduces
novel methods of scoring neural networks at initialization along with a theoretical or intuitive
explanation of their scoring strategy. For instance, (Abdelfattah et al. [2021]) empirically studies
metrics such as synflow, SNIP and FISHER. Figure 2 depicts the expression tree representations of
these metrics. NASWOT (Mellor et al. [2021]) works by treating the output of each layer as a binary

2

Figure 1: (A): Collecting neural network statistics for the ZC-NASM. The D input tensor represents
a single mini-batch from the dataset serving as input to the neural network. N represents a randomly
initialized noise tensor. P represents an input to the neural network which is a data-sample perturbed
by noise. NN statistics are collected for each of the D, N and P inputs. (B): The ZC-NASM is
applied to each ReLU-Conv2D-BatchNorm2D (RCB) instance of the neural network. The ZC-NASM
has 22 tensors available to it as arguments in each RCB instance, generated by collecting intermediate
tensors of the neural network for the three types of input (D, N , P). The ZC-NASM depicted above
only utilizes 2 intermediate tensors (T 3 and T 4GP ) in each layer to generate a score.

indicator (zero if value is negative, one if value is positive) and using the hamming distance between
two binary codes induced by an untrained network from two inputs as a measure of dissimilarity.
The intuition here is that the more similar two inputs are, the more challenging it is for the network
to learn how to separate them. Other works such as TENAS (Chen et al. [2021]) are motivated by
creating proxies for trainability and expressivity of neural networks to rank them.

Program Synthesis: Program synthesis is the
task of automatically discovering programs that
satisfy user constraints. AutoML-Zero (Real
et al. [2020]) evolves entire machine learning al-
gorithms from scratch with little restrictions on
form and using only simple mathematical opera-
tions. The algorithms are learnt symbolically, rep-
resenting programs as a sequence of instructions.
We posit that the current endeavor of identifying
fundamental architectural properties that correlate
strongly with testing accuracy can beneﬁt from a
methodology that minimizes human bias and in-
tervention in a similar fashion. This stems from
the observation that majority of the existing ZC-
NASMs can be represented as simple programs
that utilize the neural network statistics as inputs
which can be discovered with genetic programming.

3 Evolutionary Framework

Figure 2: Expression tree representation of existing
ZC-NASMs. T 3GD represents the Conv2D weight
gradient for a mini-batch of data, T 3D represents
the weights of Conv2D. T 4GD represents the activa-
tion gradient and T 4D represents the corresponding
activation. T 3GN represents the Conv2D weight
gradient for a mini-batch of random noisy tensor,
T 3N represents the weights of Conv2D. Each pro-
gram is applied to all layers of the neural network,
and the output is aggregated to generate the ﬁnal
score. Further details of this program representation
is explained later.

EZNAS uses evolutionary search to discover programs that can score neural network architectures
such that it correlates highly with accuracy. In this section, we detail the speciﬁcs of how ZC-NASM
programs are constructed, evaluated and evolved with EZNAS.

3

3.1 Program Representation

Each individual ZC-NASM has to be evaluated on
tens of gigabytes of tensors of NN architectures
with no approximations to generate precise ZC-
NASM score-accuracy correlation to serve as the
measure of ﬁtness. To make such search compu-
tationally tractable, it is crucial to not introduce
redundant operations in the program. Our ini-
tial attempt described in the appendix resembled
that of AutoML-Zero (Real et al. [2020]) where
ZC-NASMs were represented as a sequence of
instructions and a memory space to store interme-
diate tensors. This led to discovery of ZC-NASMs
with many redundant computations due to pro-
gram length bloating and an intractably large run-
time. To reduce the complexity of search, we necessitate a expression tree structure on the ZC-NASM
program to capture the executional ordering of the program. The program output appears at a root
node, and the child (terminal) nodes are the arguments of the expression tree. These arguments are
the network statistics. The advantage of this program representation is that there is only a single root
node with dense connectivity from the root to the terminal nodes, leading to fewer redundancies.

Algorithm 1 EZNAS Search Algorithm
1: evol_space = [NN1,enas, NN2,enas,... NNN,enas]
2: population = gen_random_valid_population(n)
3: # Evaluate and assign ﬁtness over evol_space.
4: population = evaluate(population)
5: for gen=1:T do
6:
7:
8:
9:
10:
11:
12:
13:

# Variation Function
children = VarOr(population)
offspring.append(selectValid(children))
new_c = gen_random_valid_population((cid:98)n/2(cid:99))
offspring.append(new_c)
population = evaluate(offspring)

offspring = []
while len(offspring) < (cid:98)n/2(cid:99) do

3.2 Neural Network Statistics Generation

Each ZC-NASM requires a set of arguments as inputs. These arguments are the intermediate
tensors of the neural network. As depicted in Figure 1, for any sampled neural network from the
NDS or NASBench-201 spaces, we identify every ReLU-Conv2D-BatchNorm2D (RCB) instance at
initialization, and register the activations, weights and the corresponding gradients for three types
of network inputs (a mini-batch of data (D), a random noisy tensor (N ), and a mini-batch of data
perturbed by random noise (P)).

We collect T 1{D,N,P }, T 2{D,N,P }, T 3, T 4{D,N,P } (10 tensors) and T 1G{D,N,P }, T 2G{D,N,P },
T 3G{D,N,P }, T 4G{D,N,P } (12 tensors) from each RCB instance. T3 represents the Conv2D weights
and does not change for the network input type, thus giving us 22 tensors for each RCB instance.
We present an alternate formulation by identifying Conv2D-BatchNorm2D-ReLU (CBR) instance for
network statistics generation to demonstrate that the evolutionary framework is not restricted to a
RCB structure in the appendix.

3.3 Mathematical Operations

The expression tree describes the execution order of the mathematical operations available to us. It is
crucial to provide a varied set of operations to process the neural network statistics effectively. We
provide 34 unique operations in our program search space. We include basic mathematical operations
(Addition, Product) as well as some advanced operations such as Cosine Similarity and Hamming
Distance. We provide the full list of mathematical operations in the supplementary material.

3.4 Program Application

Majority of the neural network architectures available in NASBench-201 and NDS have over 100
instances of ReLU-Conv2D-BatchNorm2D. This may mean that an expression tree would have as
many as 2200 (22 × 100) possible arguments (terminal nodes), each of which can be used multiple
times. This would result in a computationally intractable expression tree. To simplify the search
problem, we generate a single expression tree with 22 possible inputs. It is not necessary that the
root node of an expression tree would give a scalar value, so we add a to_scalar operation above
the root node of the expression tree. As seen in Figure 1 the expression tree is then applied on every
ReLU-Conv2D-BatchNorm2D instance and the output is aggregated across all instances using an
aggregation_function. This serves as the ’score’ of the the sampled neural network architecture.
In EZNAS-A, the aggregation_function and to_scalar are both mean. In the appendix, we
explore L2-Norm as a to_scalar function as well and ﬁnd that we are able to discover effective
ZC-NASMs.

4

3.5 Evolutionary Algorithm

Our search algorithm discovers programs by mod-
ifying the expression tree representation of the
population by variation_functions. A ﬁt-
ness score is generated for every expression
tree in the population at each generation. The
variation_function (VarOr) is then used to
generate new offspring. We describe our search
algorithm in Algorithm 1

3.5.1 Population Initialization

We initialize a population of n programs. We do
not impose any restrictions on the operations the
nodes can use in the expression tree. Due to this,
the number of valid expression trees is several
orders of magnitude lesser than the total number
of expression trees that can be generated with our
mathematical operators and network statistics. To increase search efﬁciency, we would like to ensure
that we sample and evolve only valid expression trees. To enable this, we ensure that all individuals
in the population are valid programs by testing program execution on a small sub-set of network
statistics data. All programs that produce outputs with inf , nan or fail to execute are replaced by
new randomly initialized valid programs.

Figure 3: Variation of expression trees every gen-
eration. The x signs denote point of cross-over or
mutation.

3.5.2 Fitness Objective

As the goal of zero-cost NAS is to be able to rank neural network architectures well, we utilize the
Kendall Tau rank correlation coefﬁcient as the ﬁtness objective. The search objective is to maximize
the Kendall Tau rank correlation coefﬁcient between the scores generated by a program and the test
accuracy of the neural networks in the evol_space.

3.5.3 Variation Algorithms

In our tests, we utilize the VarOr implementation from Distributed Evolutionary Algorithms in Python
(DEAP) (Fortin et al. [2012]) framework for the variation of individual programs. We generate n
(hyper-parameter) offspring programs at each generation. (cid:98)n/2(cid:99) offspring are generated as a result
of three operations; crossover, mutation or reproduction. These variations are depicted in Figure 3.
For crossover, two individual programs are randomly selected from the population and mated. Our
mating function randomly selects a crossover point from each individual and exchanges the sub-trees
with the selected point as root between each individual. The ﬁrst child is appended to the offspring
population. For mutation, we randomly select a point in the individual program, and replace the
sub-tree at that point by a randomly generated expression tree. We repeat the variation algorithm on
the population till (cid:98)n/2(cid:99) valid individuals are generated. To encourage diversity, we also randomly
generate (cid:98)n/2(cid:99) valid individuals. We have placed static limits on the depth of all expression trees at
10.

3.6 Program Evaluation Methodology

At each generation, the ﬁtness of the entire population is invalidated and recalculated. Calculating the
ﬁtness of each program on the entire dataset (which can be approximately 1 TB) is computationally
infeasible. Further, we may want to ﬁnd generalized programs that give high ﬁtness on many different
architecture design spaces and datasets. This would translate to evaluating each individual on over 7
TB of data on the NDS and NASBench-201 search spaces.

Reducing the computation by evaluating the ﬁtness of the population on a single small ﬁxed sub-set of
neural networks from the search space causes discovered programs to trivially over-ﬁt to the sub-set
statistics in our tests. To address over-ﬁtting of programs to small datasets of network statistics while
minimizing compute resources required for evaluating on the entire dataset of network statistics,
we generate an evolution task dataset. This is generated by randomly sampling 80 neural networks

5

SPEARMAN ρ CF10 CF100 IN16-120

EZNAS-A

0.83

0.82

synﬂow
jacob_cov
FLOPs
Params
grad_norm
snip
grasp
ﬁsher

0.74
0.73
0.75
0.75
0.58
0.58
0.48
0.36

0.76
0.71
0.72
0.72
0.64
0.63
0.54
0.39

0.78

0.75
0.71
0.69
0.69
0.58
0.58
0.56
0.33

Table 1: Spearman ρ for NASBench-
201.

SPEARMAN ρ CF10 CF100 IN16-120

EZNAS-A

0.89

0.74

NASWOT

0.45

0.18

0.81

0.41

Table 2: Spearman ρ for NATS-Bench-
SSS.

KENDALL τ

CF10

CF100

IN16-120

EZNAS-A

NASWOT
AngleNAS
FLOPs
Params

0.65

0.57
0.57
0.56
0.56

0.65

0.61
0.60
0.54
0.54

0.61

0.55
0.54
0.50
0.50

Table 3: Kendall τ for NASBench-201.

KENDALL τ DARTS Amoeba ENAS PNAS NASNet

EZNAS-A

NASWOT
grad_norm
synﬂow
FLOPs
Params

0.56

0.47
0.28
0.37
0.51
0.50

0.45

0.52

0.51

0.44

0.22
-0.1
-0.06
0.26
0.26

0.37
0.38
-0.02 -0.01
0.03
0.02
0.34
0.47
0.32
0.47

0.30
-0.08
-0.03
0.20
0.21

Table 4: Kendall τ for NDS CIFAR-10.

from each available search space (NASBench-201 and NDS) and dataset (CIFAR-10, CIFAR-100,
ImageNet-16-120). We evaluate individuals by randomly choosing s search spaces, and sampling
20 neural networks from each of the chosen search spaces. We take the minimum ﬁtness achieved
by the individual program on the s spaces. We consider s as a hyper-parameter. In our tests, this is
consistently kept at 4.

3.7 Program Testing Methodology

At the end of the evolutionary search, our primary goal is to test whether the programs discovered
are able to provide high ﬁtness on previously unseen neural network architectures. We test the ﬁttest
program from our ﬁnal population as well as the two ﬁttest programs encountered through-out the
evolutionary search. At test time, we take the program and ﬁnd the score-accuracy correlation over
4000 neural network architectures sampled from the NASBench-201 and NDS design spaces.

4 Results

As described in Algorithm 1, we search for effective ZC-NASMs on every design space from NDS
CIFAR-10 and the NASBench-201 datasets. Each search takes approximately 24 hours on a Intel(R)
Xeon(R) Gold 6242 CPU with 1 terabyte of RAM.

We choose the most consistent ZC-NASM
(EZNAS-A, discovered by evolving pro-
grams on the NDS-DARTS CIFAR-10
search space) from our evolutionary search
and compare it with existing ZC-NASMs
from recent works. We report both the
Kendall τ rank correlation coefﬁcient and
the Spearman ρ rank-order correlation co-
efﬁcient to fairly compare EZNAS-A with
a broad set of ZC-NASMs from recent lit-
erature. We provide our Spearman ρ cor-
relation on the NATS-Bench SSS search
space and NDS ImageNet design spaces as
well.

NASBench-201: We report our score-
accuracy correlation by scoring all neural

Figure 4: Search speedup with EZNAS-A on CIFAR-
10 NAS-Bench-201. The average best test accuracy is
taken over 10 repeated experiments.

6

networks in the NASBench-201 design space. As seen in Table 3, we obtain the highest score-
accuracy Kendall Tau Ranking Correlation on all datasets. We also benchmark our work against the
pruning based saliency metrics formalized in (Abdelfattah et al. [2021]) and ﬁnd that our program
EZNAS-A has the highest Spearman ρ among the ZC-NASMs in Table 1.

NDS: We report our score-accuracy correlation by scoring all neural networks in the NDS design
space for the CIFAR-10 dataset and scoring 40 random neural networks over 5 seeds for the ImageNet
dataset (Table 5). We ﬁnd that EZNAS-A has the highest score accuracy Kendall Tau Ranking
Correlation on ea ch of the design spaces of NDS CIFAR-10 in Table 4.

NATS-Bench: We report our score-accuracy correlation by scoring 200 randomly sampled neural
networks in the NATS-Bench TSS (same as NB-201) and SSS (Dong et al. [2021]) design space
averaged over 5 seeds for the CIFAR-10, CIFAR-100 and ImageNet16-120 dataset in Table 2.

Neural Architecture Search Integration: We integrate our ZC-NASMs EZNAS-A with the Aging
Evolution (AE) Search algorithm from (Real et al. [2019]). Our evolutionary search discovers a
ZC-NASM that is competitive in search efﬁciency with existing ZC-NASMs in Figure 4.

5 Examination Of Best Programs

SPEARMAN ρ DARTS Amoeba PNAS ENAS NASNet

EZNAS-A

EZNAS discovers a set of ZC-NASMs at the
end of the evolutionary search.
In this sec-
tion, we analyze two of the best ZC-NASMs
discovered by our method to give a deeper un-
derstanding of the nature of programs. We
refer to these programs as EZNAS-A and EZNAS-B. For each program, we generate random input
tensors of varying sizes and average the ZC-NASM score 5000 times. This is an approximate method
to understand how the ZC-NASM responds to change in architectural parameters (kernel size, number
of channels, activation size).

Figure 5: Spearman ρ for NDS ImageNet.

0.70

0.58

0.43

0.43

0.31

EZNAS-A: The evolution task dataset for discovering this program was NDS-DARTS. The input to
the ZC-NASM in Figure 6 is the T 3GN (Weight Gradient with Random Noise Input) tensor. We ﬁnd
that the score increases as the number of channels or depth increases, we also observe that kernel
sizes of 1 and 7 give higher scores than 3 and 5 with the lowest score being assigned to kernel of size
3. It is interesting to see that the expectation value of EZNAS-A in Figure 6 translates to a weighted
form of parameter counting, with a non-linear monotonically increasing scaling of score with the
number of input/output channels and a locally parabolic relationship between the score and the kernel
size with the minimum score at kernel size of 3.

EZNAS-B: The evolution task dataset for discovering this program was NDS-ENAS. The input to the
ZC-NASM in Figure 6 is T 1GP (Difference in pre-activation gradients for a noise perturbed data
mini-batch). We ﬁnd that the activation map size is exponentially more inﬂuential to the score when
compared to the number of channels.

Through this analysis, we see that EZNAS-A & EZNAS-B generate a score which is correlated with the
activation or weight sizes. It is interesting to note that when compared to ZC-NASMs from recent
literature, this form of weighted parameter counting works more consistently. This is supported by
our ﬁnding that FLOPs and Params are more generalizable than ZC-NASMs which work on the
NASBench-201 space but fail on the NDS space.

6 Discussion and Future Work

With the EZNAS formulation, we discover effective ZC-NASMs which generalize across design
spaces and datasets, as well as give SoTA score-accuracy correlation. This is a promising new
direction, but there is much work to be done. In this section, we discuss the current limitations of
EZNAS with scope for future work.

Program Design: To simplify the search problem, we take a mean (aggregation_function)
of the scores generated across all ReLU-Conv2D-BatchNorm2D instances. This formulation
does not take layer connectivity patterns into account explicitly. Extending our evolutionary

7

Figure 6: (Left) Analysis of our best program (EZNAS-A) on the Image Classiﬁcation task. to_scalar
is not required as the output is already a scalar. (Right) Analysis of our second best program (EZNAS-B)
on the Image Classiﬁcation task.

search to take into account the global connectivity pattern of the neural networks or learning
weighted averaging techniques for the individual layer (RCB instance) scores could help us
discover better ZC-NASMs. Further, we impose structural restrictions on our program by ne-
cessitating a ﬁxed structure (ReLU-Conv2D-BatchNorm2D). We have to truncate instances of
ReLU-Conv2D-Conv2D-BatchNorm2D from the NDS space by ignoring the second convolution.
As more diverse architectures are introduced in the ﬁeld, we must re-formulate collection of network
statistics in a more robust manner. Note that NASWOT measures the similarity in the binary codes
generated from the ReLU units for two different inputs. Our program design limits us to only
using a single mini-batch of inputs and does not compare inputs directly. Thus, we would not be
able to discover the NASWOT ZC-NASM metric with our approach. Fortunately, integrating such
functionality into the program design space is feasible and is an interesting line of future work to
generate more complex metrics.

It is also important to have a robust and diverse set of mathematical operations to discover effective
ZC-NASMs. We utilize 34 mathematical operations inspired by AutoML-Zero (Real et al. [2020]) and
operations found in existing ZC-NASMs. None of our operations or network statistics generation have
any scalar hyper parameters. For example, the Power operation is ﬁxed to do an element-wise square,
and the noise generation for input tensor is ﬁxed to N (0, 1). Optimizing these values dynamically
and ensuring a sufﬁciently diverse set of operations may enable discovery of better ZC-NASMs.

Network Statistics: Due to the computational resources required to generate network statistics,
we have to pre-compute them and use the generated data as an evolutionary task dataset. We only
use network statistics with input batch-size of 1 for evolution. A single sample statistic of a neural
network is insufﬁcient to describe the architecture, as evidenced by our increase in score-accuracy
correlation with the batch size in Figure 9. Further, from Figure 9 we observe high variance in the
ﬁtness of a ZC-NASM when evaluated for multiple seeds with a batch size of 1. Testing over a large
number of seeds or increasing the batch-size would cause a linear increase in memory requirement as
well as increase in run-time of the evolutionary search.

Ranking Architectures:
It is interesting to observe that in the entire neural architecture design
space, FLOPs and Params are competitive proxies and sometimes better than many existing ZC-
NASMs. A deeper investigation reveals that ranking the top 10% of the neural networks in the design
space is a signiﬁcantly more difﬁcult task. In the top 10% of neural networks, FLOPs and Params
are extremely weak proxies. Further, (Abdelfattah et al. [2021]) ﬁnds that among their ZC-NASMs,
synflow is the only one which is able to serve as a weak correlator for performance in top 10% of the
neural networks on the NASBench-201 CIFAR-100 and ImageNet16-120 design spaces. However,
synflow is not able to rank networks effective in the top 10% of CIFAR-10 NASBench-201 and the
entire NDS design space.

8

Figure 7: (Left) Effect on score-accuracy correlation of EZNAS-A with respect to batch size. This
test was done on the NDS PNAS design space over 7 seeds for 400 Neural Networks.(Right) Effect
of seed on our score-accuracy correlation of EZNAS-A with a batch size of 1. CIFAR and ImageNet
abbreviated as CF and IN respectively. This test was done on each design space over 7 seeds for 400
Neural Networks.

EZNAS-A is able to rank neural networks robustly across both the NASBench-201 and NDS design
spaces, but does not rank the top 10% of neural networks effectively. We discover alternative
ZC-NASMs that can weakly rank the top 10% of neural networks, but such ZC-NASMs fail to
generalize across neural architecture design search spaces. This ability to identify good architectures
but inability to distinguish between the best architectures may be an inherent limitation of ZC-NASMs.
Alternatively, improvements to our evolutionary framework may result in robust ZC-NASMs that
can rank the top 10% neural networks effectively. This is an interesting research direction and may
require creation of more neural architecture design spaces like NASBench-201/NDS which exhibit
lower correlation with FLOPs/Params.

Evolutionary Search: In this paper, we introduce an intuitive program representation and appropriate
variation methodologies to enable discovery of ZC-NASMs with minimal human intervention.
However, recent advances in the ﬁeld of Neuro-Symbolic Program Synthesis (Parisotto et al. [2017])
to learn mappings from input-output examples (neural network statistics to neural architecture scores)
can motivate improvements in our evolutionary search. Further, exploring architecture connectivity
encodings (White et al. [2021]) for neural architecture search and discovering programs to rank
neural architecture encodings may enable discovery of effective connectivity patterns to enable a
deeper understanding of important features in neural architecture design beyond those contained in
activations maps and weights.

Efﬁciency: EZNAS is able to discover a set of ZC-NASMs on a single CPU in under 24 hours,
which translates to 358.6 g of CO2e (Lannelongue et al. [2020]). In comparison, a single NAS search
(assuming 8 GPU days (Zhou et al. [2020])) can take over 4.49 kg of CO2e. Recent works integrate
their zero-cost proxies with differentiable architecture search (Xiang et al. [2021]) to deliver 40×
NAS speed-up. As the architecture design spaces that ZC-NASMs need to be discovered on diversify,
the efﬁciency of our methodology must be improved to scale to larger problems. Seeding the initial
population with viable candidates can enable faster discovery of robust ZC-NASMs. The search
process can be made more efﬁcient by using lower precision numerical formats, or exploring proxy
tasks on smaller datasets (down-sampled image datasets, smaller neural network design architecture
etc.).

7 Conclusion

In this paper, we present EZNAS, a novel genetic programming driven approach to discover Zero-Cost
Neural Architecture Scoring Metrics (ZC-NASMs). The key advantage of EZNAS is that it is an
interpretable approach to discover generalizable methods to rank neural networks. Generalizability
of a ZC-NASM is crucial for its practical utility, as a robust ZC-NASM should be able to rank NNs
in previously unseen neural architecture search spaces.

With our approach, we are able to discover a ZC-NASM (EZNAS-A) which evolved only on the NDS
DARTS space, but delivers state of the art score-accuracy correlation across both the NASBench-
201 and NDS design space. We also demonstrate the generalizability of EZNAS-A by providing
extremely strong correlation results on NATS-Bench as well as NDS ImageNet. We demonstrate
competitive search efﬁciency of EZNAS-A by integrating our metric with the Aging Evolution (AE)
search algorithm from (Abdelfattah et al. [2021]). Further, we provide an in-depth analysis of the
nature of programs we have discovered, along with a detailed account of existing limitations of our
methodology to motivate future work in this direction. EZNAS has the potential to reduce human
bias in design of ZC-NASMs, and aid in discovery of programs that provide deeper insights into
properties that make a neural network perform well.

9

References

[1] Mohamed S. Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, and Nicholas D. Lane. Zero-cost

proxies for lightweight nas, 2021.

[2] Yash Akhauri, Adithya Niranjan, J. Pablo Munoz, Suvadeep Banerjee, Abhijit Davare, Pasquale
Cocchini, Anton A. Sorokin, Ravi Iyer, and Nilesh Jain. Rhnas: Realizable hardware and neural
architecture search, 2021.

[3] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target

task and hardware, 2019.

[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one

network and specialize it for efﬁcient deployment, 2020.

[5] Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in

four gpu hours: A theoretically inspired perspective, 2021.

[6] Kanghyun Choi, Deokki Hong, Hojae Yoon, Joonsang Yu, Youngsok Kim, and Jinho Lee.

Dance: Differentiable accelerator/network co-exploration, 2021.

[7] Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural

architecture search, 2020.

[8] Xuanyi Dong, Lu Liu, Katarzyna Musial, and Bogdan Gabrys. NATS-bench: Benchmarking
NAS algorithms for architecture topology and size. IEEE Transactions on Pattern Analysis
and Machine Intelligence, pages 1–1, 2021. doi: 10.1109/tpami.2021.3054824. URL https:
//doi.org/10.11092Ftpami.2021.3054824.

[9] Félix-Antoine Fortin, François-Michel De Rainville, Marc-André Gardner, Marc Parizeau, and
Christian Gagné. DEAP: Evolutionary algorithms made easy. Journal of Machine Learning
Research, 13:2171–2175, jul 2012.

[10] Shoukang Hu, Sirui Xie, Hehui Zheng, Chunxiao Liu, Jianping Shi, Xunying Liu, and Dahua

Lin. Dsnas: Direct neural architecture search without parameter retraining, 2020.

[11] Loïc Lannelongue, Jason Grealey, and Michael Inouye. Green algorithms: Quantifying the

carbon footprint of computation, 2020.

[12] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei,
Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search, 2018.

[13] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search,

2019.

[14] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimiza-

tion, 2019.

[15] Joseph Mellor, Jack Turner, Amos Storkey, and Elliot J. Crowley. Neural architecture search

without training, 2021.

[16] J. Pablo Muñoz, Nikolay Lyalyushkin, Yash Akhauri, Anastasia Senina, Alexander Kozlov, and
Nilesh Jain. Enabling NAS with automated super-network generation. CoRR, abs/2112.10878,
2021. URL https://arxiv.org/abs/2112.10878.

[17] Emilio Parisotto, Abdelrahman Mohamed, Rishabh Singh, Lihong Li, Denny Zhou, and Push-
meet Kohli. Neuro-symbolic program synthesis. In 5th International Conference on Learning
Representations (ICLR 2017), February 2017. URL https://www.microsoft.com/en-us/
research/publication/neuro-symbolic-program-synthesis-2/.

[18] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efﬁcient neural

architecture search via parameter sharing, 2018.

[19] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network

design spaces for visual recognition, 2019.

10

[20] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for

image classiﬁer architecture search, 2019.

[21] Esteban Real, Chen Liang, David R. So, and Quoc V. Le. Automl-zero: Evolving machine

learning algorithms from scratch, 2020.

[22] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and

Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile, 2019.

[23] Colin White, Willie Neiswanger, Sam Nolen, and Yash Savani. A study on encodings for neural

architecture search, 2021.

[24] Lichuan Xiang, Łukasz Dudziak, Mohamed S. Abdelfattah, Thomas Chau, Nicholas D. Lane,

and Hongkai Wen. Zero-cost proxies meet differentiable architecture search, 2021.

[25] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: Stochastic neural architecture

search, 2020.

[26] Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing Xu, Qi Tian, and

Chang Xu. Cars: Continuous evolution for efﬁcient neural architecture search, 2020.

[27] Yongan Zhang, Yonggan Fu, Weiwen Jiang, Chaojian Li, Haoran You, Meng Li, Vikas Chandra,

and Yingyan Lin. Dna: Differentiable network-accelerator co-search, 2020.

[28] Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang,
and Wanli Ouyang. Econas: Finding proxies for economical neural architecture search, 2020.

[29] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable

architectures for scalable image recognition, 2018.

11

8 Appendix

8.1 NASBench-201 and NDS

For image classiﬁcation, we utilize the NASBench-201 [7] and NDS [19] NAS search spaces for our
evolutionary search as well as testing. NASBench-201 consists of 15,625 neural networks trained
on the CIFAR-10, CIFAR-100 and ImageNet-16-120 datasets. Neural Networks in Network Design
Spaces (NDS) uses the DARTS [13] skeleton. The networks are comprised of cells sampled from
each of AmoebaNet [20], DARTS [13], ENAS [18], NASNet [29] and PNAS [12]. There exists
approximately 5000 neural network architectures in each NDS design space.

8.2 Sequential Program Representation

Our initial attempts at discovering ZC-NASMs
took a different approach to program representa-
tion. The sequential program representation de-
scribed in Figure 8 posed no structural limitations
on the program. We have 22 static memory ad-
dresses, which contained network statistics and
are referenced with integers 0-21. To store in-
termediate tensors generated by the program, we
allocate 80 dynamic memory addresses, which can
be over-written during the program execution as
well. To store intermediate scalars generated by
the program, we allocate 20 memory addresses.
As seen in Figure 8, we represent the programs
as integers, where each instruction is expressed
as 4 integers. The ﬁrst integer provides the write
address, the second integer provides the operation
ID and the third and fourth integers provide the read addresses for the operation. We initialize valid
random integer arrays and convert them to programs to evaluate and fetch the ﬁtness (score-accuracy
correlation). We allow Mate, InsertOP, RemoveOP, MutateOP & MutateOpArg as variation
functions. The Mate function takes two individuals, and takes the ﬁrst half of each individual. Then,
these components are interpolated to generate a new individual. The InsertOP function inserts an
operation at a random point in the program. The RemoveOP function removes an operation at a ran-
dom point in the program. The MutateOP changes a random operation in program without changing
read/write addresses. The MutateOpArg function simply replaces one of the read arguments of any
random instruction with another argument from the same address space (dynamic address argument
cannot be replaced by a scalar address argument).

Figure 8: Our sequential program representation.

While we are able to discover weak ZC-NASMs with this formulation, we observe that there are too
many redundancies in the programs discovered. Program length bloating as well as operations that
do not contribute to the ﬁnal output were frequently observed. Due to these issues, the evolution
time evaluation of individual ﬁtness quickly became an intractable problem. To address this, we
change our program representation to a expression tree representation in the results reported in the
paper. This representation necessitates contribution of each operation to the ﬁnal output, which means
there is no redundant compute. While the sequential program representation is valid, we believe
that signiﬁcant engineering efforts are required to ensure discovery of meaningful programs. Our
sequential program representation is directly inspired by the formulation used in AutoML-Zero [21].
AutoML-Zero makes signiﬁcant approximations in the learning task to evolutionarily discover MLPs.
While AutoML-Zero has a much larger program space to search for, approximations in computing
individual ﬁtness are not feasible in our formulation as generating exact score-accuracy correlation is
an important factor in selecting individuals with high ﬁtness.

8.3 Noise and Perturbation for Network Statistics

To generate network statistics, we use three types of input data.
single random sample from the dataset (e.g.
CIFAR-10).
tion as input = torch.randn(data_sample.shape).

is simply a
a single image or a batch of images from
To generate a noisy input, we simply use the default torch.randn func-
The third type of input we pro-

The ﬁrst

12

Figure 9: (Left) Experiment demonstrating the effect of seed on our score-accuracy correlation for
neural network initialization with a batch size of 1. (Right) Effect of seed on our score-accuracy
correlation with a batch size of 1. CIFAR and ImageNet abbreviated as CF and IN respectively. This
test was done on each design space over 7 seeds for 400 Neural Networks.

vide is a data-sample which has been perturbed by random noise (input = data_sample +
0.01**0.5*torch.randn(data_sample.shape)).

8.4 Network Initialization Seed Test

In Figure 9 (Left), we use different seeds to change the initialization and input tensors, but keep
the neural architectures being sampled ﬁxed in the respective spaces. The variance in the score
accuracy correlation is much lesser than in Figure 9 (Right) where the seed also controls the neural
architectures being sampled. This shows the true variation in our EZNAS-A ZC-NASM with respect
to network initialization.

8.5 Hardware used for evolution and testing

Our evolutionary algorithm runs on Intel(R) Xeon(R) Gold 6242 CPU with 630GB of RAM. Our
RAM utilization for evolving programs on a single Image Classiﬁcation dataset was approximately
60GB. RAM utilization can vastly vary (linearly) based on the number of neural network statistics
that are being used for the evolutionary search. Our testing to generate the statistics for the seed
experiments as well as the ﬁnal Spearman ρ and Kendall Tau Rank Correlation Coefﬁcient is done on
an NVIDIA DGX-2 server with 4 NVIDIA V-100 32GB GPUs.

8.6 Varying network statistics and to_scalar

In Figure 10, we detail 3 tests while evolving on the NDS_DARTS CIFAR-10 search space in an iden-
tical fashion to EZNAS-A (referred to as EZNAS-(R-C-B)-(Mean) here). EZNAS-(C-B-R)-(Mean)
and EZNAS-(C-B-R)-(L2) correspond to alternate to_scalar and network statistics collection
tests respectively. We demonstrate that while EZNAS-(R-C-B)-(Mean) is more effective, we are
able to discover ZC-NASMs with all three formulations.

Figure 10: Experiment demonstrating the ability to discover ZC-NASMs with an alternate net-
work statistics collection strategy and to_scalar function. Experiments are named as EZNAS-
(Statistics Collection Structure)-(to_scalar). Two statistics collection structures are tested. (R-C-B)
is a ReLU-Conv2D-BatchNorm2D structure, (C-B-R) is a Conv2D-BatchNorm2D-ReLU structure.
(to_scalar) can be Mean or L2.

13

Figure 11: Full correlation table. Each column represents the dataset evolution was performed
on. The DARTS-CIFAR10 column is the EZNAS-A NASM. Each row represents the dataset the
best discovered NASM program was tested on. Best score-accuracy KTR in bold and underlined.
Second best score-accuracy KTR in italics and underlined. These tests are done by evolving on 100
neural networks and testing on the test task dataset (1000 randomly sampled neural networks on
NASBench-201 and 200 randomly sampled neural networks on NDS). The network statistics were
generated with a batch size of 1.

8.7 Hyper-parameters for discovering EZNAS-A

num_generation: 15
population_size: 50
tournament_size: 4
MU: 25
lambda_ : 50
crossover_prob: 0.4
mutation_prob: 0.4
min_tree_depth: 2
max_tree_depth: 10

14

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes]
(c) Did you discuss any potential negative societal impacts of your work? [No] Our method
increases the efﬁciency of NAS signiﬁcantly, and is aimed at reducing the negative
societal impact of AI.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main exper-
imental results (either in the supplemental material or as a URL)? [No] We plan to
open-source our work as part of a larger framework in the future.

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes]

(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] We report error bars for the score-accuracy correlation of
our discovered metric against variation of batc hsize as well as seed.

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes]

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [No]
(c) Did you include any new assets either in the supplemental material or as a URL? [No]
(d) Did you discuss whether and how consent was obtained from people whose data
you’re using/curating? [No] Crediting the curators was sufﬁcient as speciﬁed in their
open-source license.

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

15

Op ID

Operation

Description
Output: C, Input: A, B

OP0
OP1
OP2
OP3
OP4
OP5
OP6

OP7

OP8

OP9
OP10
OP11

OP12

OP13
OP14
OP15
OP16
OP17
OP18

OP19

Element-wise Sum
Element-wise Difference
Element-wise Product
Matrix Multiplication
Lesser Than
Greater Than
Equal To

Log

AbsLog

Abs
Power
Exp

Normalize

ReLU
Sign
Heaviside
Element-wise Invert
Frobenius Norm
Determinant

LogDeterminant

OP20

SymEigRatio

OP21

EigRatio

OP22
OP23

OP24

OP25

Normalized Sum
L1 Norm

Hamming Distance

KL Divergence

OP26

Cosine Similarity

OP27
OP28
OP29
OP30
OP31
OP32
OP33

Softmax
Sigmoid
Ones Like
Zeros Like
Greater Than Zero
Less Than Zero
Number Of Elements

C = A+B
C = A-B
C = A*B
C = A@B
C = (A<B).bool()
C = (A>B).bool()
C = (A==B).bool()
A[A<=0] = 1
C = torch.log(A)
A[A==0] = 1
C = torch.log(torch.abs(A))
C = torch.abs(A)
C = torch.pow(A, 2)
C = torch.exp(A)
C = (A - Amean)/Astd
C[C!=C] = 0
C = torch.functional.F.relu(A)
C = torch.sign(A)
C = torch.heaviside(A, values=[0])
C = 1/A
C = torch.norm(A, p=’fro’)
C = torch.det(A)
C = torch.logdet(A)
C[C!=C]=0
A = A.reshape(A.shape[0], -1)
A = A@A.T
A = A + A.T
e,v = torch.symeig(A, eigenvectors=False)
C = e[-1]/e[0]
A = A.reshape(A.shape[0], -1)
A = torch.einsum(’nc,mc->nm’, [A,A])
e,v = torch.eig(A)
C = (e[-1]/e[0])[0]
C = torch.sum(A)/A.numel()
torch.sum(abs(A))/A.numel()
A = Heaviside(A)
B = Heaviside(B)
C = sum(A!=B)
C = torch.nn.KLDivLoss(’batchmean’)(A,B)
A = A.reshape(A.shape[0], -1)
B = B.reshape(B.shape[0], -1)
C = torch.nn.CosineSimilarity()(A, B)
C = torch.sum(C)
C = torch.functional.F.softmax(A)
C = torch.functional.F.sigmoid(A)
C = torch.ones_like(A)
C = torch.zeros_like(A)
C = A>0
C = A<0
C = torch.Tensor([A.numel()])

Table 5: List of operations available for the genetic program.

16

