2
2
0
2

n
u
J

9
2

]
E
N
.
s
c
[

1
v
7
7
5
3
0
.
7
0
2
2
:
v
i
X
r
a

Automatic Synthesis of Neurons for Recurrent Neural Nets

Roland Olsson
Chau Tran
Lars Magnusson
Department of Computer Science
Østfold University College
N-1757 Halden, Norway

Roland.Olsson@hiof.no
Chau.Tran@hiof.no
Lars.Magnusson@hiof.no

Abstract

We present a new class of neurons, ARNs, which give a cross entropy on test data that
is up to three times lower than the one achieved by carefully optimized LSTM neurons.
The explanations for the huge improvements that often are achieved are elaborate skip
connections through time, up to four internal memory states per neuron and a number of
novel activation functions including small quadratic forms.

The new neurons were generated using automatic programming and are formulated as

pure functional programs that easily can be transformed.

We present experimental results for eight datasets and found excellent improvements for
seven of them, but LSTM remained the best for one dataset. The results are so promising
that automatic programming to generate new neurons should become part of the standard
operating procedure for any machine learning practitioner who works on time series data
such as sensor signals.
Keywords: Recurrent neural nets, neuron synthesis, automatic programming

1. Introduction

Time series or sequence data is abundant in machine learning and range from signal pro-
cessing to stock prices and more complex data such as natural language. Indeed, the entire
life of a human being is also a time series dataset generated by the ﬁve senses.

This paper focuses on improving LSTM neurons (Hochreiter and Schmidhuber, 1997)
since they both are theoretically interesting and also one of the most popular and eﬀective
machine learning tools.

By using the ADATE automatic programming system (Olsson, 1995), we generated new
neurons for each speciﬁc dataset and found that they were up to three times better than
LSTM as measured by cross entropy on test data. We used eight datasets with between 22
and 500 timesteps and found the biggest improvement for the 500 timestep dataset.
The main novelties of the ADATE Recursive Neurons (ARNs) are as follows.

1. ARNs often contain skip connections like in Highway nets and ResNet, but through

the time dimension.

2. Up to four memory states per neuron instead of just one as in the LSTM.

3. Novel activation functions built from, for example, small quadratic forms and se-

quences of a set of three predeﬁned and well known activation functions.

©0000 Roland Olsson, Chau Tran and Lars Magnusson.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

 
 
 
 
 
 
Olsson, Tran and Magnusson

4. A given neuron may use linear combinations of the states of other neurons.

5. Diﬀerentiating between recurrent output as well as state values from “self” and “all

others”.

We will now give an overview of the paper. First, we present related work on auto-
matic programming and evolution of recurrent neurons. The next section describes how to
represent ARNs in a small and purely functional subset of the Standard Meta Language
(SML) (Milner et al., 1997) and also gives a compact and purely functional deﬁnition of the
classic LSTM as an illustration. We then provide experimental results for eight medium
size datasets that typically are found in practical sensor data analysis applications. We
ﬁnish by some conclusions and a glimpse of the almost inﬁnite possibilities for future work
that are enabled by the technology in this paper. The appendix contains seven examples of
ARNs to exhibit some of their novelties.

2. Related work

Long short-term memory (LSTM) was originally presented in Sepp Hochreiter’s masters the-
sis (Hochreiter and Schmidhuber, 1997) with J¨urgen Schmidhuber as the advisor and made
more well known by Hochreiter and Schmidhuber (Hochreiter and Schmidhuber, 1997).

It remains a quite popular and competitive technology also 25 years later, which is
remarkable in a rapidly developing ﬁeld such as machine learning. We assume that the
reader already is familiar with LSTMs and will not describe them here.

Some newer and prominent recurrent neurons that sometimes outperform LSTM include
Gated Recurrent Units (Cho et al., 2014) and Independently Recurrent Neural Nets (Li
et al., 2018).

2.1 Using evolutionary computation to synthesize recurrent neurons

Galvan and Mooney (Galv´an and Mooney, 2021) provide an extensive review of neuroevo-
lution and show that the ﬁeld primarily deals with using evolutionary methods for hyper-
parameter or architecture and topology search for entire neural nets using a predeﬁned set
of neuron types. They do not mention any work that evolves the neurons themselves.

An early attempt to evolve neurons using ADATE was made by Berg et. al. (Berg et al.,
2008), but these neurons were a special type of spiking neurons for segmentation of gray
scale images containing noisy rectangles. Although reasonable improvements were found,
the results are not practically competitive and have been superseded by modern CNN based
nets for image processing.

A key distinction between diﬀerent approaches to neuron evolution is whether the goal is
to ﬁnd a general neuron that is better for a large class of datasets or whether it is to produce
more specialized neurons that work best for an application speciﬁc class of datasets. Our
goal is to produce superior and application speciﬁc neurons whereas Bayer et. al (Bayer
et al., 2009) and Jozefowicz et. al. (Jozefowicz et al., 2015) try to produce more general
replacements for the LSTM, which may be much more diﬃcult to achieve.

Of course, the disadvantage with application speciﬁc neurons, for example neurons spe-
cialized for high frequency trading, is that it is more diﬃcult to replace the neuron itself

2

Automatic Neuron Synthesis

in the well known neural toolboxes than it is to just optimize the hyperparameters and the
architecture. A possible advantage though is that much bigger application speciﬁc improve-
ments may be found. Thus, we argue that all three of overall architecture, hyperparameters
and neuron design need to be optimized for the best results.

Bayer et. al (Bayer et al., 2009) mention another distinction, namely if a very indirect
representation such as DNA is employed to represent neurons or whether transformations
such as so-called “mutations” are performed directly on the neuron circuits. They use the
latter approach and deﬁne a number of well designed transformations on a directed acyclic
graph representation of neurons. Thus, they build an evolutionary system from scratch
expressly for evolving LSTM replacements. Our approach is a bit diﬀerent since we use the
universal ADATE automatic programming system and do not code any evolutionary search
nor any transformations speciﬁcally for neuron evolution.

Jozefowicz et. al. (Jozefowicz et al., 2015) also build their evolutionary computation
system from scratch and excel in neural architecture design and hyperparameter search. It
is obvious from the paper that they have deep expertise in these areas. In contrast to us,
they perform hyperparameter search on-the-ﬂy during the evolution. As a result of this and
other experimental choices, they evaluate a number of new neuron candidates that is about
ﬁve orders of magnitude smaller than we did with ADATE.

2.2 Automatic design of algorithms through evolution

Automatic design of algorithms through evolution (ADATE) (Olsson, 1995) was originally
developed for automatic synthesis of symbolic and recursive functional programs, but a few
years later also given simple mechanisms for optimizing ﬂoating point constants. It writes
programs in a very small subset of SML (Milner et al., 1997), which was conceived by Robin
Milner as a meta language for symbolic logic (Gordon et al., 1978). As we will see in this
paper, it is also quite suitable for meta machine learning.

Some other and fundamentally diﬀerent approaches to inductive programming, that are
more suitable for explainable machine learning than ADATE, are discussed in the JMLR
special issue edited by (Kitzelmann et al., 2006).

ADATE has the ability to invent help functions as they are needed and to perform so-
called lifting and distribution transformations on case- and let-expressions as well as many
other semantics preserving and useful program transformations (Olsson, 1995). However,
the details of these are beyond the scope of the current paper.

ADATE needs a speciﬁcation of the problem to be solved. A speciﬁcation contains the

following primary ingredients.

1. Algebraic datatypes, for examples various lists and trees.

2. A signature for the function f that is to be synthesized and optionally also an initial

deﬁnition of that function.

3. An evaluation function that tests a synthesized function on a number of training
inputs. Note that f may occur as a small part of a big program and that calculating f
typically means to run this program on the training inputs. Thus, ADATE performs
symbolic reinforcement (meta-)learning.

3

Olsson, Tran and Magnusson

Given the speciﬁcation, ADATE runs on a cluster where many programs are transformed
and evaluated in parallel. The result is a Pareto front of gradually more syntactically
complex and better programs. Syntactic complexity is the sum of the base-2 logarithm of
the occurrence probabilities of the symbols in all nodes in the syntax tree of a program. It
falls on the human operator of ADATE to select a suitable program from the Pareto front.
Below, we will make it simple and always choose the one that has the lowest mean squared
error or cross entropy on validation data.

3. How to represent ARNs as functional programs

We will ﬁrst explain how to represent neurons and then explain how a neuron is a tran-
sition function from one timestep to the next. Then, we will give a functional program
corresponding to an LSTM neuron as an example.

3.1 Datatypes

A key ingredient in neural nets is linear combination of a set of ﬂoating point values and
we will start by explaining how to represent this using algebraic datatypes. For example,
consider a linear combination of two values x1 and x2 using weights w1 and w2 and bias b.

w1x1 + w2x2 + b

In our algebraic datatype, we abstract away the weights and represent a linear combi-
nation using the SML type linComb, deﬁned below, with two constructors bias and cons.
The * is Cartesian product and the | introduces a sum type.

datatype linComb = bias | cons of real * linComb

This is essentially the type of lists that always contain at least one value, namely a
bias. The type real represents ﬂoating point numbers in spite of its SML name. The linear
combination above would correspond to the following list.

cons( x1, cons( x2, bias ) )

At this point, the reader perhaps wonder how on earth the weights will be speciﬁed. The
answer is that we only allow a limited number of weight mappings, where each mapping
is indicated by a function called lci for i = 0, 1, . . . , n for n + 1 mappings. The current
implementation has n = 4, which means that there will be ﬁve diﬀerent sets of forward,
recurrent and peep weight matrices.

The expression below will be translated by our compiler to the linear combination above

assuming that mapping 0 has weights w1 and w2 and bias b.

lc0( cons( x1, cons( x2, bias ) ) )

However, if we were to use say lc1 instead of lc0, another bias and set of weights would

be used after compilation.

4

Automatic Neuron Synthesis

3.2 Transition function

It is now time to start explaining how to represent recursive neural net neurons and their
internal and external connections.

To introduce our notation, let us ﬁrst consider the transition function f of type R2 → R
for a simple RNN neuron with input x(t) and output y(t) at time t. Thus, the inputs and
outputs of f are as follows.

An LSTM neuron has an internal state s which means that its type is R3 → R2.

f (x(t), y(t)) = y(t+1)

f (x(t), s(t), y(t)) = (s(t+1), y(t+1))

An ADATE recursive neuron (ARN) has four internal states instead of just one which

implies that it could be a function with the following signature.

f (x(t), s(t)

0 , s(t)

1 , s(t)

2 , s(t)

3 , y(t)) = (s(t+1)

0

, s(t+1)
1

, s(t+1)
2

, s(t+1)
3

, y(t+1))

However, we will diﬀerentiate between the output from the neuron itself at timestep t
and the linear combination of the outputs of the other neurons in the same layer. This
“recognition of self” turns out to be an important feature of ARNs as shown by the exper-
imental results in Section 4.4.

Experimentally, we also found that skip connections through time were invented by
ADATE and it is easy to see that this is enabled by having many internal states instead of
just one. For example, if we wish the output at timestep t to be available as an input at
timestep t + 2, we can choose s(t+1)
to y(t) and s(t+1)
1 . Many other and much more
intricate ways of curating information through time are possible and have been invented by
ADATE.

to s(t)

1

2

3.3 Functional programming deﬁnition of an ARN

In order to explain the signature of an ARN, we will switch to functional programming
notation and start by explaining the input parameters.

InputsLC : linComb. The list of inputs.

SelfPeep0 : real. State s(t)
0 .

SelfPeep1 : real. State s(t)
1 .

SelfPeep2 : real. State s(t)
2 .

SelfPeep3 : real. State s(t)
3 .

SelfOutput : real. The current output value yt.

OtherPeepsLC : linComb. The list of the s(t)

0 values of all other nodes in the layer.

5

Olsson, Tran and Magnusson

OtherOutputsLC : linComb. The list of the y(t) values of all other nodes in the layer.

Finally, we will explain how variables are bound to values of expressions in the small
subset of SML used in ADATE. The syntax is a bit unusual and relies on case-expressions
for this. For example, the case-expression below means that the variable V will represent
the value of the expression E1 and that its scope is E2.

case E1 of V => E2

Of course, these case expressions are used to construct functional programs that corre-

spond to directed acyclic graphs (DAGs) in neural circuit diagrams.

Consider the usual LSTM neuron with peepholes in Figure 1 that is a direct translation
of the LSTM circuit diagram in (Greﬀ et al., 2016). We now have everything that is needed
to understand the functional programming deﬁnition of this neuron. Figure 1 contains ﬁve
case-expressions before giving the output tuple in which only the ﬁrst and the last ﬁelds are
used and the three in the middle just contain dummy values, in this case 0.0, since they are
not needed for LSTM. Recall that the output quintuple ﬁrst contains the four next states
and then the next output as its last ﬁeld. Since all our datasets are centered and scaled,
it turns out that biases are almost totally redundant in the LSTM, which means that they
have been omitted in Figure 1.

As another example, we will explain a rather simple neuron syntesized by ADATE and

that comes from the Pareto front for Double pendulum discussed later in Section 4.6.

The program corresponding to the neuron is given in Figure 2. The auxiliary function
g has been invented by ADATE. Its purpose is to bind S0 to the tanh expression. The
return value of g is a quintuple where the last ﬁeld is a quadratic polynomial that gives the
output from the neuron. The ﬁrst ﬁeld is just S0, which will become SelfPeep0 for the
next timestep. A linear combination of these values for the other neurons than “self” will
become OtherPeepsLC.

Figure 3 shows a more traditional representation of the neuron with one copy of the
neuron for timestep t and another for t + 1 with dashed lines indicating information ﬂow
across timesteps. The ﬁgure uses linear algebra notation with bold lower case variables
being vectors, whereas bold upper case variables represent matrices. The vector of state
0 values is s0, corresponding to the scalar variable S0 in the program above. The input
weight matrices U1 and U2 correspond to the weight mappings indicated by lc1 and lc2
respectively. The recurrent weight matrix W0 is hollow, that is has zeroes along its diagonal
to exclude “self” from OtherPeepsLC. The vector a0 contains so-called auxiliary weights
and is multiplied element-wise (Hadamard product) as indicated by the operator (cid:12). It is
not really needed here but corresponds to how the cons is translated by our compiler.

The shaded boxes in the diagram implicitly contain weight vectors and matrices, which

have been omitted in order not to clutter the diagram.

The intermediate vector s0,0 represents lc2 InputsLC + SelfPeep0 and s0,1 repre-
sents lc1( cons( lc0 OtherPeepsLC, InputsLC ) ). When we introduce new interme-
diate variables, we use a second subscript as above to diﬀerentiate them

In our actual C++ code, all input weight matrices are stacked in one matrix and the
same goes for all recurrent weight matrices in order to reduce the number of matrix-vector
multiplications.

6

Automatic Neuron Synthesis

case

tanh(

lc0 InputsLC + lc0( cons( SelfOutput, OtherOutputsLC ) )
)

of Z =>
case

sigmoid(

lc1 InputsLC +
lc1( cons( SelfPeep0, cons( SelfOutput, OtherOutputsLC ) ) )
)

of I =>
case

sigmoid(

1.0 + lc2 InputsLC +
lc2( cons( SelfPeep0, cons( SelfOutput, OtherOutputsLC ) ) )
)

of F =>
case F * SelfPeep0 + I * Z
case

sigmoid(

of S0 =>

lc3 InputsLC +
lc3( cons( S0, cons( SelfOutput, OtherOutputsLC ) ) )
)

of O =>

( S0, 0.0, 0.0, 0.0, O * tanh S0 )

Figure 1: The LSTM neuron with peepholes as a functional program.

let

fun g S0 = ( S0, S0, S0, S0, S0 - S0 * S0 )

in

g(

tanh(

relu( lc2 InputsLC + SelfPeep0 ) -
lc1( cons( lc0 OtherPeepsLC, InputsLC ) )
) )

end

Figure 2: A small neuron for modeling a double pendulum.

7

Olsson, Tran and Magnusson

Figure 3: A circuit diagram for the neuron in Figure 2.

8

Automatic Neuron Synthesis

3.4 Choosing activation functions

ADATE needs to be given a set of predeﬁned functions to use for program synthesis. Obvi-
ously, we include addition, subtraction, multiplication and division. It would be possible for
ADATE to use these together with if-expressions to synthesize its own activation functions,
for example ReLU (Fukushima, 1969) or good approximations of the hyperbolic ones, but
after some preliminary experimentation we decided against using if-expressions altogether.
Instead, we chose to include three predeﬁned activation functions, which turn out to be

very useful for synthesizing other and novel activation functions.

First, we chose to include tanh but not sigmoid since ADATE easily can express it as a

scaled version of tanh.

Second, the relu function was included and after that we also chose to add a linear
approximation of tanh which we called saturated ReLU (srelu). The latter is deﬁned as
being the identity function for values between -1 and 1, just 1 for values greater than 1 and
-1 for values less than -1. It could be implemented using two ReLU functions, but we chose
to relieve ADATE of that extra burden.

Our intuition was that elimination of the more curved parts of tanh may lead to smaller
problems with vanishing gradients since relu also is linear and seems to have this advantage.
However, we still wanted a function more suitable for gating than relu and therefore deﬁned
srelu.

Thus, our set of predeﬁned activation functions is tanh, relu and srelu.

4. Experiments

In this section, we will ﬁrst explain our neural net architecture and hyperparameter op-
timization, then how to very quickly evaluate neuron candidates and then introduce the
datasets that we tested ARN on. Finally, we will present the experimental results along
with statistical analyses.

In general, we always split datasets into three subsets, with 50% for training, 25% for
validation and 25% for testing. The test set is used only once at the very end to prevent
subtle forms of information leaks and the overﬁtting that may result.

4.1 Architecture and hyperparameter optimization

Before starting an ADATE run for a new dataset, we optimized the architecture and the
hyperparameters for LSTM and used these also for the neurons discovered by ADATE.
Assume that we have a dataset with no outputs and that there are l nodes in the LSTM
layer.

We used the simple net architecture below.

1. An LSTM layer with l nodes.

2. A hyperbolic tangent layer with no nodes.

3. A linear layer with no nodes.

9

Olsson, Tran and Magnusson

We use Glorot initialization for forward weights and orthogonal initialization for recur-
rent weights with a scaling factor of 0.1 in both cases. Also, we use an initial extra bias of
1 for the forget gates as recommended by (Jozefowicz et al., 2015) and others.

The loss function was mean squared error (MSE) for regression and softmax plus cross
entropy for classiﬁcation. We tried a variety of other backends such as one or more ReLU
layers instead of the hyperbolic tangent layer, but found no signiﬁcant gains in validation
losses. Before doing any other optimization, we found the optimal number l of LSTM nodes
under the assumption that l = 2i for i = 1, 2, . . . , 7. The reason that we could use a rather
small upper bound for the number of nodes is that we did not use datasets such as those
from NLP that require a big memorization ability.

The next topic is hyperparameter optimization. Choi et. al. (Choi et al., 2019) found
that comparisons of diﬀerent weight optimizers such as ADAM (Kingma and Ba, 2014)
and NADAM (Dozat, 2016) often is skewed by inadequate hyperparameter optimization
and recommend that all parameters in the algorithms should be optimized. We used their
methodology with the initial parameter ranges that they give in Appendix D.8 for every
single dataset. As they recommend, we performed a random search within these ranges
but used many more evaluations, namely 512 random parameter sets for each dataset and
picked the best one for each dataset using the validation data. Thus, we optimized all
parameters in the ADAM algorithm and not just the learning rate.

We tried with both ADAM and NADAM, but the diﬀerence between them was negligible.

ADAM was used in all subsequent experiments.

We also used the same learning rate schedule as Choi et. al. (Choi et al., 2019), namely
the one from Shallue et. al. (Shallue et al., 2019), and optimized the initial learning rate, the
duration of the decay and the amount of decay together with the ADAM hyperparameters.
Our batch size is so small that no warm-up is needed.

The amount of training is often measured in the number of epochs, but we will instead
use the total number of training examples including duplicates, that is the number of
epochs times the training set cardinality. Each training session consisted of running 320
000 training examples with a batch size of four, that is a total of 80 000 weight updates.
After preliminary experiments, we abandoned L1 and L2 regularization and dropout but
chose to employ model checkpointing as follows. For every 20 000 training examples, we
ran the net with the current weights on the validation data and updated the best weights
found if there was a validation improvement.

4.2 Experiment design and implementation

A key challenge with ADATE as for many other evolutionary algorithms is that very many
evaluations may be required, where evaluation means to compute an evaluation value, some-
times called “ﬁtness”, for a solution candidate, which here is an ARN.

ADATE ﬁrst screens candidates using a quite limited and very quick evaluation. The
candidates that pass the screening, typically less than 1%, then move on to a second stage
evaluation that in most cases use at least 100 times more time per candidate. Finally,
less than 1% of the ones who pass the second stage are then passed to a third and ﬁnal
stage. The ﬁrst two stages use a smaller number of training steps and a smaller number of
recurrent nodes as follows.

10

Automatic Neuron Synthesis

First stage. The goal for this stage was to have an overall evaluation time for a new neuron
that is less than 100 ms. For some CPUs and datasets, we achieved 50 ms and for
other combinations it is up to 200 ms. Of course, the evaluation results will be very
bad in this stage, but still enough to ﬁlter away really bad programs. The number
of nodes was always four and we used only 5000 training examples. Additionally, we
restricted the training to run on only the ﬁve last timesteps in each time series.

Second stage. Here, we use 40 000 training examples, all timesteps and l/4 nodes.

In
comparison with the ﬁrst stage, the run time is increased by a factor given by the
following expression where nt is the number of time steps.

8 · (l/16)2 · nt/5

For example, with l = 64 and 100 time steps, the increase is 2560 times.

Third stage. This stage uses the full 320 000 training examples and l nodes. Thus, its

expected run time is 8 · 42, that is 128, times longer than the second stage.

For say l = 64 and 100 timesteps, the total speed ampliﬁcation is about 2560·128/3,
that is about 105 times, since the implementaion actually tries to spend equally long run
time on each stage.

Our compiler translates a functional program that represents a neuron into byte code,
which then is run on-the-ﬂy by a byte code interpreter implemented in C++. Since matrix
operations require almost all the execution time, it does not matter that neuron evaluation
is somewhat slower than optimized machine code.

All of the neural net is also implemented in C++ and uses the automatic diﬀerentia-
tion library AADC that kindly was made available by Matlogica Ltd, who also provided
absolutely outstanding support for how to use it in the most eﬃcient way, especially for
matrix operations. The resulting vectorized machine code, that is AVX256 or optionally
AVX512, appears to run several times faster than Tensorﬂow, when the latter is restricted
to a single CPU core. We evaluated the C interfaces of all the leading neural net packages
but did not ﬁnd anything that could compete with the speed, stability and ease of use that
characterizes AADC.

To check the correctness of our implementation, we ran the same datasets with LSTM
using both our C++ code and Tensorﬂow and compared the results. Additionally, we im-
plemented one of the new neurons in Tensorﬂow and compared with the C++ code without
ﬁnding any statistically signiﬁcant diﬀerences. All comparisons and other experiments used
64-bit ﬂoating point numbers since a 32-bit AADC version was not yet available from Mat-
logica.

An ADATE run used between 500 and 1000 CPU cores depending on what was available.
The number of evaluated candidate neurons was on the order of one billion for each dataset
and around three days of run time were typically needed to ﬁnd the best results.

4.3 Datasets

Since we need to evaluate new neurons rather quickly, we chose to focus on datasets that
can be eﬀectively run with 128 LSTM neurons or less, that is rather small nets. For this

11

Olsson, Tran and Magnusson

reason, we avoided speech and NLP datasets that typically require bigger nets as well as
attention (Bahdanau et al., 2014; Graves, 2013) to get the best results.

Many of the datasets come from the excellent online repository for time series datasets
that is curated by Bagnall et. al. (Bagnall et al., 2017) and their time series classiﬁcation
(TSC) research group. Most of the datasets are time series from various physical sensors.
The only preprocessing that we employed was either centering and scaling or one hot
encoding, depending on whether a predictor or an output was ordinal or nominal. Note
that the goal of the experiments is a reliable comparison between LSTM and new neurons
and not to get state-of-the-art results for any dataset. For this reason, we did not do any
feature engineering or data augmentation. Thus, due to not doing any more advanced
preprocessing, our results may for some datasets be far from the best that is possible.

We will ﬁrst give a brief description of each dataset and then list the number of examples,

that is time series, and the number of predictors for each one.

3W This is a predictive maintenance dataset that was donated to the UCI machine learning
repository by Petrobras (Vargas et al., 2019) and consists of actual sensor values and
events in oil wells supplemented by simulated and handcrafted values and events.
The task is to predict undesirable events. We preprocessed the dataset into non-
overlapping time windows and used only sensor readings, for example temperature
and pressure, as predictors.

Crop This dataset is from TSC but was originally collected by Tan et. al. (Tan et al.,
2017). The goal is to classify the type of crop on a patch of land from the time series
of images produced by the Sentinel-2A satellite, which has plant growth monitoring
as one of its primary missions.

Double pendulum The task of the RNN is to learn to simulate a double pendulum. In
general, RNNs can be applied to modeling of chemical and physical processes and are,
for example, used quite successfully for this purpose by the Borregaard bioreﬁnery
Inspired by this, we generated the Double Pendulum
since the beginning of 2022.
dataset from the simulation provided with the SimBody physics engine. The predictors
are the center of gravity coordinates of the ﬁrst as well as the second pendulum
at a given timestep and the response is the corresponding coordinates for the next
timestep. Each time series is generated from randomly chosen initial positions and
angular velocities. We deliberately lowered the sampling frequency so that black box
modeling from measurement data alone becomes challenging.

ECG5000 This dataset is a 20-hour long ECG and was originally published by Goldberger
et. al. (Goldberger et al., 2000), who preprocessed the signals to make each heartbeat
equally long using interpolation. We downloaded it from TSC.

FordB This dataset was originally used in a competition at WCCI 2008 (Abou-Nasr and
Feldkamp, 2008) and the task is to determine if there is something wrong with an
internal combustion engine based on recordings of the engine sound. This dataset was
also taken from TSC.

Insect wingbeat This dataset (Chen et al., 2014) contains spectrograms from the sounds
generated by the wings of insects, in this case mostly mosquitoes and ﬂies. The goal

12

Automatic Neuron Synthesis

Table 1: Number of examples, number of timesteps, number of inputs and number of out-

puts after one hot encoding.

Dataset
3W
Crop
Double pendulum
ECG5000
FordB
Insect wingbeat
LSST
WISDM

#examples #timesteps #inputs #outputs

2864
24000
10000
5000
3836
20000
4925
27540

64
46
128
140
500
22
36
40

8
1
4
1
1
200
6
3

17
24
4
5
2
10
14
6

is to classify the species of an insect based on a sound recording. We downloaded it
from TSC.

LSST This is yet another TSC dataset but originally comes from a 2018 Kaggle compe-
tition (Allam Jr et al., 2018).
It contains simulated data from the Vera C. Rubin
Observatory, also known as the Large Synoptic Survey Telescope (LSST), that are
measurements of how the brightness of an astronomical object varies with time. The
goal is to determine which kind of object that was observed.

WISDM The task is to predict the activity of a cell phone user based on the time series
of accelerometer values (Kwapisz et al., 2011). We used the most diﬃcult version
of the WISDM dataset which is version 1.1 without overlapping time windows and
downloaded it from the UCI machine learning repository.

As can be seen in Table 1, most of the datasets have a small or medium number of
examples. This is a use case which is just as important as big data since many datasets
that arise in practice have sizes similar to those in the table. Indeed, most datasets gathered
by TSC are even smaller, but we did not want to include these since the conﬁdence intervals
may become too big.

4.4 Experimental results

Table 2 compares an LSTM net with an ARN net for the test data sets. We used categorical
cross entropy (CCE) for the classiﬁcation datasets and mean squared error (MSE) for the
regression dataset. The p-value was calculated using McNemar’s test for classiﬁcation and
the Wilcoxon signed-rank test for regression with continuity correction in both cases. As
can be seen in Table 2, the p-values are better than ﬁve sigma for six of the eight datasets.
The column “Factor better” shows how many times better the ARN was compared with

LSTM with respect to CCE or MSE.

Since we carefully optimized the hyperparameters for LSTM, the big improvements
obtained for say 3W, Double pendulum, Ford B, LSST and WISDM, cannot be explained
by the LSTMs being poorly tuned. Instead, the most likely explanation is that the new
ARN neurons have a superior modeling and ﬁtting ability.

13

Olsson, Tran and Magnusson

Table 2: Test data cross entropies or MSE and accuracies for LSTM and ARN.

Dataset

LSTM

ARN

3W
Crop
Pendulum
ECG5000
FordB
Wingbeat
LSST
WISDM

CCE/MSE CCE/MSE better
1.71
1.20
1.40
1
3.64
1.09
1.35
1.69

0.517
0.796
0.154
0.188
0.510
1.120
1.598
0.137

0.303
0.661
0.110
0.188
0.140
1.029
1.183
0.081

Factor LSTM ARN
Acc
Acc
0.934
0.804
0.775
0.742

0.949
0.759
0.574
0.478
0.963

0.947
0.950
0.605
0.627
0.975

p-value

1.2 · 10−18
3.8 · 10−11
1.1 · 10−19
0.74
3.6 · 10−31
1.0 · 10−5
1.1 · 10−21
3.5 · 10−8

As can be seen in the table, the improvement that was obtained varies a lot between the
datasets. For example, for the FordB dataset, we obtained a more than three-fold reduction
of the test data CCE, whereas there was no improvement at all for the ECG5000 dataset.
One possible explanation for the huge improvement for FordB is that this dataset has as
many as 500 timesteps and that LSTM cannot eﬀectively handle so long time dependencies,
whereas ARN can do that using skip connections through time. By looking at the best
program for FordB in Appendix A.4, it is easy to see that skip connections are indeed used,
but otherwise this is a really hard neuron to analyze since it was not designed by human
beings.

Since the experiments optimized cross entropy instead of accuracy, it is likely that
somewhat higher accuracies could be obtained if ADATE were to directly optimize the
accuracy instead.

We omitted the neuron for ECG5000 since it was no improvement, but the best ones
for all the other datasets are listed in the appendices. As can be seen there, there is a
huge variation from one dataset to the next, but both skip connections and the linear
combinations of state 0 (OtherPeepsLC ) are frequently used. From the outset, we naively
guessed that reading the memories of other neurons would give rise to problems akin to
vanishing or exploding gradients, but this seems to be much less problematic than we
thought.

All of the best programs use novel activation functions that we have never seen be-
fore. For example, the best program for WISDM in Appendix A.7 uses srelu applied to
quadratic forms in several places. Many of the other neurons also contain various quadratic
expressions, but which roles they have is not clear.

There are some obvious redundancies in the coding, for example applying relu twice.

ADATE is able to remove such redundancies, but much more run time might be needed.

In general, evolution in ADATE is so eﬀective that neurons are likely to become exquisitely

adapted to the special characteristics of the dataset. This means that neurons should not
be reused from one class of datasets to the next, that is, a new ADATE run is needed
for each new type of application. One way to avoid this special adaption would be to run
on say hundreds of diﬀerent datasets simultaneously using many more cores and possibly
accelerators.

14

Automatic Neuron Synthesis

4.5 A closer look at some ARNs

In this section, we will discuss the circuit diagrams for the best LSST neuron and the best
In
FordB neuron. These were selected since they are among the simpler best neurons.
order to translate the neuron syntax from SML to diagrams, we ﬁrst printed them as C
expressions using our compiler, converted them to equations and then manually simpliﬁed
these equations before using them to draw the neurons.

Figure 4: A circuit diagram for the best neuron for the LSST dataset .

Both the diagram and the equations for LSST are shown in Figure 4. All vector-
vector multiplications are element-wise and all variables denoted by b or B are vectors or
matrices manually derived from cons expressions in the SML code that after translation to
C contains auxiliary weights, denoted by a variables. We do not show the full and lengthy
manual translation from SML to simpliﬁed equations.

It is a bit fascinating to see that ADATE actually has invented a small convolutional ﬁlter
represented by the equation for y(t)
0 . This ﬁlter combines the inputs from three consectutive
timesteps using a linear combination. However, the output from the ﬁlter is combined with
skip connections from previous outputs represented by y(t)
1 . Thus, the convolution is in
principle over inputs and some outputs and the result is fed to relu as can be seen in the
equation for y(t).

15

Olsson, Tran and Magnusson

Another interesting feature of this neuron is that it totally has abandoned tanh and
srelu as activation functions. As can be seen in Figure 4, it only contains two relu
functions. However, we are not able to theoretically understand the dynamic behaviour of
RNNs that contain such neurons and can only claim that it produces superior accuracy for
the LSST dataset.

Figure 5: A circuit diagram for the best neuron for the FordB dataset .

The best neuron for the FordB dataset is more complex and shown in Figure 5. We will
now take a look at some elaborate skip connections in this neuron that use the output from
time t − 4 to calculate the output at time t. Since the dataset contains 500 timesteps, we
can speculate that it is especially important to curate information over long time periods
and that special connections may have been evolved to handle that.

Let → mean that the left hand side is partially used to compute the right hand side.
Then, the information ﬂow from y(t−4) to y(t) is as follows as can be seen in the circuit
diagram.

y(t−4) → s(t−3)

0 → s(t−2)

2 → s(t−1)

3 → y(t)

16

Automatic Neuron Synthesis

Thus, the internal states are employed to implement information ﬂow that appears to

be custom designed to retain long term memory for the FordB dataset.

As a curiosity, we can just mention that ADATE has discovered ARNs that retain long
term memory much better than LSTM for thousands of timesteps when simulating complex
Mealy machines.

4.6 Analysis of a Pareto front

Figure 6: The Pareto front for Double pendulum.

Recall that a Pareto front contains gradually bigger and better programs so that the
smallest and worst program comes ﬁrst whereas the last program is the biggest and the
best. As an example, we will use the Pareto front for the Double pendulum dataset which
consists of 24 programs. The ﬁrst four of these are so small that they are not interesting
but the last 20 are shown in Figure 6. For this dataset, an LSTM has a validation MSE of
0.146 as indicated by the horizontal line in Figure 6, which means that all programs below
the line are better than LSTM for Double pendulum on validation data.

By using Pareto fronts, ADATE tries to both minimize MSE and program size. This
has been so successful that all programs in the front are smaller, at least according to the
ADATE syntactic complexity, than the LSTM program, which has a size of 324 bits. Thus,
ADATE has found a plethora of programs that are both smaller and better than LSTM for
Double pendulum.

We will take a closer look at the two programs that are indicated by the blue squares
in Figure 6. The smallest one, corresponding to the leftmost square, has an MSE of 0.176
and is the following simple RNN, where no internal neuron states are used. Therefore, we
have also manually edited the program to remove them

relu( lc2( cons( lc1( OtherOutputsLC ), InputsLC ) ) )

This is easy to understand and just a linear combination of the inputs and the outputs
from other neurons, albeit formulated as a linear combination of a linear combination.
Apparently, relu is more suitable than the other activation functions.

The next square square in Figure 6 is the program in Figure 2 that has an MSE of 0.139,

which is better than for the LSTM.

17

Olsson, Tran and Magnusson

Even if this neuron is quite simple, it is already becoming hard to understand exactly
why it is better, with analysis being impeded by the use of internal states from other
neurons. A partial hypothesis may be that using quadratic base functions may give more
suitable regression modeling of a double pendulum.

The best neuron for Double Pendulum, which is the green and last square in Figure 6,
is given in Appendix A.3. It is much better and has an MSE of 0.094 on validation data.
It is also much more diﬃcult to understand.

Also, note that the test data MSE for this neuron is 0.11 and that using validation
data for both hyperparameter optimization, ADATE runs, and model checkpointing leads
to some overﬁtting on validation data. In spite of this, the neuron is signiﬁcantly better
than LSTM also on test data with a p-value around 10−19.

The syntactic complexity makes a big jump with little gain in MSE from the second last
to the best program in the front. Due to Ockhams’s razor (of Ockham, 1323), overﬁtting
can be suspected when this happens.

5. Conclusions and future work

We tested the ADATE recurrent neurons (ARNs) on eight datasets. For the four best
datasets, the ARNs were a factor of 3.6, 1.7, 1.7 and 1.4 better respectively as measured by
cross entropy or mean squared error on test data. The p-values for these four were below
about 10−18, which is clearly better than ﬁve sigma.

For the other four datasets, there were signiﬁcant improvements for three but failure
for one. The reason it did not work for one dataset seems to be overﬁtting to the valida-
tion data caused by a combination of hyperparameter tuning, model check pointing and
using validation data to calculate evaluation values for programs generated by ADATE and
selecting the best one.

The results are so good that ARNs should be tried in practice whenever recurrent nets
are used. The reasons for the big improvements that were found for at least half of the
datasets are as follows.

1. ADATE is quite capable when it comes to automatically adapting the code of a neuron

to a speciﬁc class of datasets.

2. The ARNs contain several somewhat unusual ingredients such as

(a) Up to four internal states per neuron. Sometimes, these are apparently used by

ADATE to construct intricate skip connections through time.

(b) Linear combinations of the states of other neurons in addition to the usual com-

binations of outputs.

(c) Custom made activation functions that sometimes contain quadratic forms.

(d) Diﬀerentiation between “self” and “all others”.

The technology is ready to use right away in C++ with an option to manually port
to Tensorﬂow. We encourage people interested in using it to contact the corresponding
author of this paper. However, a cluster with between 500 and 1000 cores is needed to

18

Automatic Neuron Synthesis

run on a subset of data with up to about 50 000 training examples for about three days.
Thus, the number of datasets that can be processed may be somewhat limited by available
computational resources.

Here are some possibilities for future work.

1. Let an ARN layer be an ensemble of diﬀerent neuron types and use ADATE to evolve

all the diﬀerent types, either one type at a time or all types simultaneously.

2. Use many stacked ARN layers with diﬀerent neuron types in each layer.

3. Find a way to eﬃciently optimize hyperparameters as a part of the evolution instead

of doing it only once at the beginning.

4. Run on hundreds of datasets during the same evaluation in order to ﬁnd neurons that
are generally better. However, we expect that evolving neurons for speciﬁc classes of
datasets often will yield much better results.

5. Study other neural architectures than RNNs. For example, the batch normalization
and ResNet blocks in a CNN could be viewed as the function f to improve and the
time dimension in an RNN could be replaced by a layer index dimension, where say
four states per block are curated from one layer to the next by f.

6. Another possibility would be deﬁne diﬀerentiable functional programs and neural
nets that represent them. These could use diﬀerentiable datastructures and handle
recursion using a predeﬁned set of diﬀerentiable schemas that are generalizations of
say catamorphisms, anamorphisms and hylomorphisms.

It may also be possible to employ the automatic programming presented in this paper for
a variety of other current and future neural net and diﬀerentiable programming technologies.

Appendix A. The best neurons according to validation data

This appendix lists the best programs according to validation data for the seven datasets
out of eight for which improvements were found. They are presented exactly as they were
printed by the ADATE system and could typically be somewhat simpliﬁed manually. Thus,
they may look more complex than they really are.

The best neuron for WISDM has also been implemented by one of us as a custom layer in
Tensorﬂow in addition to the automatically generated C++ code that our compiler generates
based on the SML programs.

Requests for neuron synthesis and C++ or Tensorﬂow implementations for other datasets

are very welcome.

Appendix A.1. The best ARN for 3W

fun f

(

SelfPeep0,
SelfPeep1,

19

Olsson, Tran and Magnusson

SelfPeep2,
SelfPeep3,
SelfOutput,
OtherPeepsLC,
OtherOutputsLC,
InputsLC
) =

case cons( SelfOutput, cons( SelfOutput, InputsLC ) ) of

V21902689 =>

case
(

SelfOutput,
tanh(

srelu(
(

(

(

lc3( cons( lc1( V21902689 ), V21902689 ) ) *
srelu( SelfPeep3 )
) -

tanh(

lc0(

cons(

SelfPeep3,
cons( tanh( SelfOutput ), InputsLC )
)

)
) +
srelu(

relu(

tanh(

lc2(

cons(

~0.14531347527330391E~1,
cons(

SelfPeep3,
cons(

srelu( srelu( SelfPeep2 ) ),
cons(

SelfOutput,
cons( SelfPeep0, InputsLC )
)

)

)

)

20

Automatic Neuron Synthesis

)

)

)

)

)

)

)

)

)

of

( V21915A57, V21915A58 ) =>

(

V21915A58,
SelfPeep0,
srelu( relu( lc2( OtherOutputsLC ) ) ),
SelfPeep0,
srelu(
(

V21915A57 -
relu(

tanh(

tanh(

srelu(

srelu(

case ( SelfPeep1, V21915A58 ) of
( V21915A59, V21915A5A ) =>
( V21915A5A - V21915A59 )

)

)

)

)

)

)

)

)

Appendix A.2. The best ARN for Crop

fun f

(

SelfPeep0,
SelfPeep1,
SelfPeep2,
SelfPeep3,
SelfOutput,
OtherPeepsLC,

21

Olsson, Tran and Magnusson

OtherOutputsLC,
InputsLC
) =

case
(

SelfOutput,
(

( lc3( OtherPeepsLC ) * lc0( InputsLC ) ) -
(

tanh(

srelu(
lc0(

cons(

SelfPeep1,
cons(

SelfPeep3,
cons( lc4( OtherPeepsLC ), bias )
)

)

)

)
) +

lc3( InputsLC )
)

)

)

of

( VDBC64A, VDBC64B ) =>

case srelu( srelu( ( VDBC64B - SelfPeep1 ) ) ) of

V1C427005 =>

(

VDBC64B,
SelfPeep0,
lc0( cons( SelfPeep0, OtherPeepsLC ) ),
SelfPeep2,
srelu(

case ( V1C427005, V1C427005 ) of
( V1C481BD5, V1C481BD6 ) =>

let

fun g1CDF7C46 V1CDF7C47 =
( VDBC64A - V1CDF7C47 )

in

g1CDF7C46( ( V1C481BD6 * V1C481BD5 ) )

end
)

)

22

Automatic Neuron Synthesis

Appendix A.3. The best ARN for Double pendulum

fun f

(

(

(

SelfPeep0,
SelfPeep1,
SelfPeep2,
SelfPeep3,
SelfOutput,
OtherPeepsLC,
OtherOutputsLC,
InputsLC
) =

relu( relu( lc1( cons( lc1( OtherPeepsLC ), InputsLC ) ) ) ) -
relu(

lc3(

cons(

lc4( OtherPeepsLC ),
cons( 0.22174599383632232, InputsLC )
)

)

)

),

SelfPeep1,
SelfPeep2,
SelfPeep3,
(

(

(

lc4(

cons(

lc4( OtherPeepsLC ),
cons(

lc0(

cons(

SelfPeep2,
cons(

( ~0.6961519103176064 + SelfPeep1 ),
InputsLC
)

)

),

InputsLC

23

Olsson, Tran and Magnusson

)

)
) *

lc0( OtherPeepsLC )
) +

SelfPeep0
) *

(

SelfPeep1 -
(

tanh(

case

cons(

lc3(

cons(

SelfPeep2,
cons( ~0.6961519103176064, InputsLC )
)

of

),

InputsLC
)

V1662138D =>

(

srelu(
lc0(

cons(

SelfPeep0,
cons( lc0( V1662138D ), InputsLC )
)

)
) *
srelu(
lc0(

cons(

relu(

lc3(

cons(

SelfPeep0,
cons(

lc0(

cons( lc0( V1662138D ), V1662138D )
),

InputsLC
)

)

24

Automatic Neuron Synthesis

)

),

InputsLC
)

)

)

)

) +

~0.45284110969509517
)

)

)

)

Appendix A.4. The best ARN for FordB

fun f

(

SelfPeep0,
SelfPeep1,
SelfPeep2,
SelfPeep3,
SelfOutput,
OtherPeepsLC,
OtherOutputsLC,
InputsLC
) =

case srelu( srelu( relu( relu( SelfOutput ) ) ) ) of

V281AF3E7 =>

case
(

V281AF3E7,
(

SelfPeep2 -
(

tanh( lc1( OtherPeepsLC ) ) +
lc0(

cons(

SelfPeep3,
cons( V281AF3E7, cons( SelfPeep2, InputsLC ) )
)

)

)

)

)

of

25

(

(

Olsson, Tran and Magnusson

( VDBC64A, VDBC64B ) =>

(

VDBC64B,
tanh( srelu( srelu( VDBC64B ) ) ),
lc0( cons( tanh( SelfPeep0 ), InputsLC ) ),
SelfPeep2,
srelu(
(

VDBC64A -
case
case

let

fun g1C57E5DF V1C57E5E0 =

(

V1C57E5E0,
(

~0.12690104588539253E~1 -

case ( VDBC64B, SelfPeep1 ) of
( V1C57EDC3, V1C57EDC4 ) =>

srelu(
(

( V1C57EDC3 - SelfPeep0 ) +
( V1C57EDC3 - V1C57EDC4 )
)
) )

)

)

in

g1C57E5DF(

case g1C57E5DF 0.1029527350644156 of

( A, B) => A + B

)

end
( V5FAAD0B, V5FAAD0C ) =>

of

( V5FAAD0B, srelu( V5FAAD0C ) )

of

( V1D190879, V1D19087A ) =>

tanh( tanh( ( V1D19087A * V1D190879 ) ) ) )

)

)

)

Appendix A.5. The best ARN for Insect wingbeat

fun f

(

26

Automatic Neuron Synthesis

SelfPeep0,
SelfPeep1,
SelfPeep2,
SelfPeep3,
SelfOutput,
OtherPeepsLC,
OtherOutputsLC,
InputsLC
) =

case
(

SelfOutput,
(

(

srelu(

tanh(

tanh(

lc3(

cons(

0.10022791851659179E1,
cons( lc4( OtherOutputsLC ), InputsLC )
)

)

)

)
) *

srelu( tanh( lc0( InputsLC ) ) )
) -

tanh( srelu( ~0.6873893912995532E~2 ) ) +
srelu( relu( SelfOutput ) )
)

(

)

)

of

( VDBC64A, VDBC64B ) =>

case tanh( srelu( srelu( tanh( relu( VDBC64B ) ) ) ) ) of

V1C9C2AC6 =>

case ( V1C9C2AC6, V1C9C2AC6 ) of
( V1C9C2AD2, V1C9C2AD3 ) =>

(

VDBC64B,
( tanh( SelfPeep2 ) * SelfPeep3 ),
relu( tanh( relu( V1C9C2AD3 ) ) ),
SelfPeep2,
srelu( ( VDBC64A - tanh( V1C9C2AD2 ) ) )

27

Olsson, Tran and Magnusson

)

Appendix A.6. The best ARN for LSST

fun f

(

SelfPeep0,
SelfPeep1,
SelfPeep2,
SelfPeep3,
SelfOutput,
OtherPeepsLC,
OtherOutputsLC,
InputsLC
) =

(

relu( lc1( OtherOutputsLC ) ),
SelfOutput,
lc0( InputsLC ),
SelfPeep2,
(

relu(

lc1(

cons(

srelu( ~0.1267263484282487 ),
cons(

SelfPeep3,
case

cons(

SelfPeep0,
cons(

SelfPeep2,
cons(

lc2( cons( lc0( OtherOutputsLC ), InputsLC ) ),
InputsLC
)

)

)

of

V259F26F2 => cons( lc0( V259F26F2 ), V259F26F2 )

)

)

)
) +

SelfOutput
)

28

Automatic Neuron Synthesis

)

Appendix A.7. The best ARN for WISDM

fun f

(

case
(

SelfPeep0,
SelfPeep1,
SelfPeep2,
SelfPeep3,
SelfOutput,
OtherPeepsLC,
OtherOutputsLC,
InputsLC
) =

SelfOutput,
(

lc0( bias ) -
(

tanh(

srelu(

srelu(
lc1(

cons(

SelfPeep1,
cons(

SelfPeep3,
cons(
(

lc4( InputsLC ) *
lc2( cons( SelfPeep0, InputsLC ) )
),
cons(

srelu(

relu(

lc2(

cons(

lc0( OtherOutputsLC ),
OtherPeepsLC
)

)

)

),

29

Olsson, Tran and Magnusson

case

cons( lc1( OtherPeepsLC ), InputsLC )

of

V271482D0 =>

cons( lc2( V271482D0 ), V271482D0 )

)

)

)

)

)

)

)
) +
srelu(

relu( lc2( cons( lc4( OtherPeepsLC ), InputsLC ) ) )
)

)

)

)

of

( VDBC64A, VDBC64B ) =>

(

VDBC64B,
SelfPeep0,
srelu( srelu( srelu( lc0( OtherPeepsLC ) ) ) ),
SelfPeep2,
srelu(
(

VDBC64A -
(

( VDBC64B - SelfPeep1 ) *
( VDBC64B - SelfPeep1 )
)

)

)

)

References

M. Abou-Nasr and L. Feldkamp. Ford classiﬁcation challenge. Competition organized in
conjunction with the IEEE World Congress on Computational Intelligence (WCCI 2008),
2008.

T. Allam Jr, A. Bahmanyar, R. Biswas, M. Dai, L. Galbany, R. Hloˇzek, E. E. Ishida,
S. W. Jha, D. O. Jones, R. Kessler, et al. The photometric lsst astronomical time-series
classiﬁcation challenge (plasticc): Data set. arXiv preprint arXiv:1810.00001, 2018.

30

Automatic Neuron Synthesis

A. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh. The great time series classiﬁcation
bake oﬀ: a review and experimental evaluation of recent algorithmic advances. Data
Mining and Knowledge Discovery, 31:606–660, 2017.

D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to

align and translate. arXiv preprint arXiv:1409.0473, 2014.

J. Bayer, D. Wierstra, J. Togelius, and J. Schmidhuber. Evolving memory cell structures
In International conference on artiﬁcial neural networks, pages

for sequence learning.
755–764. Springer, 2009.

H. Berg, R. Olsson, T. Lindblad, and J. Chilo. Automatic design of pulse coupled neurons

for image segmentation. Neurocomputing, 71(10-12):1980–1993, 2008.

Y. Chen, A. Why, G. Batista, A. Mafra-Neto, and E. Keogh. Flying insect classiﬁcation

with inexpensive sensors. Journal of insect behavior, 27(5):657–677, 2014.

K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio. On the properties of neural
machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.

D. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E. Dahl. On empirical
comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446, 2019.

T. Dozat. Incorporating nesterov momentum into adam. Project report for CS229, Stanford

University, 2016.

K. Fukushima. Visual feature extraction by a multilayered network of analog threshold
elements. IEEE Transactions on Systems Science and Cybernetics, 5(4):322–333, 1969.

E. Galv´an and P. Mooney. Neuroevolution in deep neural networks: Current trends and

future challenges. IEEE Transactions on Artiﬁcial Intelligence, 2(6):476–493, 2021.

A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorﬀ, P. C. Ivanov, R. G. Mark,
J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. Physiobank, physiotoolkit,
and physionet: components of a new research resource for complex physiologic signals.
Circulation, 101(23):215–220, 2000.

M. Gordon, R. Milner, L. Morris, M. Newey, and C. Wadsworth. A metalanguage for
interactive proof in lcf. In Proceedings of the 5th ACM SIGACT-SIGPLAN symposium
on Principles of programming languages, pages 119–130, 1978.

A. Graves. Generating sequences with recurrent neural networks.

arXiv preprint

arXiv:1308.0850, 2013.

K. Greﬀ, R. K. Srivastava, J. Koutn´ık, B. R. Steunebrink, and J. Schmidhuber. Lstm: A
search space odyssey. IEEE transactions on neural networks and learning systems, 28
(10):2222–2232, 2016.

S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):

1735–1780, 1997.

31

Olsson, Tran and Magnusson

R. Jozefowicz, W. Zaremba, and I. Sutskever. An empirical exploration of recurrent network
architectures. In International conference on machine learning, pages 2342–2350. PMLR,
2015.

D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

E. Kitzelmann, U. Schmid, R. Olsson, and L. P. Kaelbling. Inductive synthesis of functional
programs: An explanation based generalization approach. Journal of Machine Learning
Research, 7(2), 2006.

J. R. Kwapisz, G. M. Weiss, and S. A. Moore. Activity recognition using cell phone ac-

celerometers. ACM SigKDD Explorations Newsletter, 12(2):74–82, 2011.

S. Li, W. Li, C. Cook, C. Zhu, and Y. Gao.

Independently recurrent neural network
(indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 5457–5466, 2018.

R. Milner, M. Tofte, R. Harper, and D. MacQueen. The deﬁnition of standard ML: revised.

MIT press, 1997.

W. of Ockham. Summa Totius Logicae. University of Oxford, 1323.

R. Olsson. Inductive functional programming using incremental program transformation.

Artiﬁcial intelligence, 74(1):55–81, 1995.

C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring
the eﬀects of data parallelism on neural network training. Journal of Machine Learning
Research, 20(112):1–49, 2019.

C. W. Tan, G. I. Webb, and F. Petitjean. Indexing and classifying gigabytes of time series
under time warping. In Proceedings of the 2017 SIAM international conference on data
mining, pages 282–290. SIAM, 2017.

R. E. V. Vargas, C. J. Munaro, P. M. Ciarelli, A. G. Medeiros, B. G. do Amaral, D. C.
Barrionuevo, J. C. D. de Ara´ujo, J. L. Ribeiro, and L. P. Magalhaes. A realistic and
public dataset with rare undesirable real events in oil wells. Journal of Petroleum Science
and Engineering, 181:106223, 2019.

32

