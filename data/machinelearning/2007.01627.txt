0
2
0
2

v
o
N
4

]

G
L
.
s
c
[

4
v
7
2
6
1
0
.
7
0
0
2
:
v
i
X
r
a

NeuMiss networks: differentiable programming for
supervised learning with missing values

Marine Le Morvan1,2 Julie Josse1,3 Thomas Moreau1 Erwan Scornet3 Gaël Varoquaux1, 4

1 Université Paris-Saclay, Inria, CEA, Palaiseau, 91120, France
2 Université Paris-Saclay, CNRS/IN2P3, IJCLab, 91405 Orsay, France
3 CMAP, UMR7641, Ecole Polytechnique, IP Paris, 91128 Palaiseau, France
4 Mila, McGill University, Montréal, Canada

{marine.le-morvan, julie.josse, thomas.moreau, gael.varoquaux}@inria.fr
erwan.scornet@polytechnique.edu

Abstract

The presence of missing values makes supervised learning much more challenging.
Indeed, previous work has shown that even when the response is a linear function
of the complete data, the optimal predictor is a complex function of the observed
entries and the missingness indicator. As a result, the computational or sample
complexities of consistent approaches depend on the number of missing patterns,
which can be exponential in the number of dimensions. In this work, we derive the
analytical form of the optimal predictor under a linearity assumption and various
missing data mechanisms including Missing at Random (MAR) and self-masking
(Missing Not At Random). Based on a Neumann-series approximation of the
optimal predictor, we propose a new principled architecture, named NeuMiss
networks. Their originality and strength come from the use of a new type of
non-linearity: the multiplication by the missingness indicator. We provide an
upper bound on the Bayes risk of NeuMiss networks, and show that they have
good predictive accuracy with both a number of parameters and a computational
complexity independent of the number of missing data patterns. As a result they
scale well to problems with many features, and remain statistically efﬁcient for
medium-sized samples. Moreover, we show that, contrary to procedures using EM
or imputation, they are robust to the missing data mechanism, including difﬁcult
MNAR settings such as self-masking.

1

Introduction

Increasingly complex data-collection pipelines, often assembling multiple sources of information, lead
to datasets with incomplete observations and complex missing-values mechanisms. The pervasiveness
of missing values has triggered an abundant statistical literature on the subject [14, 31]: a recent survey
reviewed more than 150 implementations to handle missing data [10]. Nevertheless, most methods
have been developed either for inferential purposes, i.e. to estimate parameters of a probabilistic
model of the fully-observed data, or for imputation, completing missing entries as well as possible
[6]. These methods often require strong assumptions on the missing-values mechanism, i.e. either
the missing at random (MAR) assumption [27] – the probability of being missing only depends on
observed values – or the more restrictive Missing Completely At Random assumption (MCAR) – the
missingness is independent of the data. In MAR or MCAR settings, good imputation is sufﬁcient to
ﬁt statistical models, or even train supervised-learning models [11]. In particular, a precise knowledge

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
of the data-generating mechanism can be used to derive an Expectation Maximization (EM) [2]
formulation with the minimum number of necessary parameters. Yet, as we will see, this is intractable
if the number of features is not small, as potentially 2d missing-value patterns must be modeled.

The last missing-value mechanism category, Missing Not At Random (MNAR), covers cases where
the probability of being missing depends on the unobserved values. This is a frequent situation in
which missingness cannot be ignored in the statistical analysis [12]. Much of the work on MNAR data
focuses on problems of identiﬁability, in both parametric and non-parametric settings [29, 20–22].
In MNAR settings, estimation strategies often require modeling the missing-values mechanism [9].
This complicates the inference task and is often limited to cases with few MNAR variables. Other
approaches need the masking matrix to be well approximated with low-rank matrices [18, 1, 7, 16, 32].

Supervised learning with missing values has different goals than probabilistic modeling [11] and
has been less studied. As the test set is also expected to have missing entries, optimality on the
fully-observed data is no longer a goal per se. Rather, the goal of minimizing an expected risk lend
itself well to non-parametric models which can compensate from some oddities introduced by missing
values. Indeed, with a powerful learner capable of learning any function, imputation by a constant
is Bayes consistent [11]. Yet, the complexity of this function that must be approximated governs
the success of this approach outside of asymptotic regimes. In the simple case of a linear regression
with missing values, the optimal predictor has a combinatorial expression: for d features, there are 2d
possible missing-values patterns requiring 2d models [13].

Le Morvan et al. [13] showed that in this setting, a multilayer perceptrons (MLP) can be consistent
even in a pattern mixture MNAR model, but assuming 2d hidden units. There have been many
adaptations of neural networks to missing values, often involving an imputation with 0’s and con-
catenating the mask (the indicator matrix coding for missing values) [23, 19, 15, 34, 4]. However
there is no theory relating the network architecture to the impact of the missing-value mechanism on
the prediction function. In particular, an important practical question is: how complex should the
architecture be to cater for a given mechanism? Overly-complex architectures require a lot of data,
but being too restrictive will introduce bias for missing values.

The present paper addresses the challenge of supervised learning with missing values. We propose
a theoretically-grounded neural-network architecture which allows to implicitly impute values as a
function of the observed data, aiming at the best prediction. More precisely,

• We derive an analytical expression of the Bayes predictor for linear regression in the presence of
missing values under various missing data mechanisms including MAR and self-masking MNAR.
• We propose a new principled architecture, named NeuMiss network, based on a Neumann se-
ries approximation of the Bayes predictors, whose originality and strength is the use of (cid:12)M
nonlinearities, i.e. the elementwise multiplication by the missingness indicator.

• We provide an upper bound on the Bayes risk of NeuMiss networks which highlights the beneﬁts

of depth and learning to approximate.

• We provide an interpretation of a classical ReLU network as a shallow NeuMiss network. We
further demonstrate empirically the crucial role of the (cid:12) nonlinearities, by showing that increasing
the capacity of NeuMiss networks improves predictions while it does not for classical networks.
• We show that NeuMiss networks are suited medium-sized datasets: they require O(d2) samples,

contrary to O(2d) for methods that do not share weights between missing data patterns.

• We demonstrate the beneﬁts of the proposed architecture over classical methods such as EM
algorithms or iterative conditional imputation [31] both in terms of computational complexity
–these methods scale in O(2dd2) [28] and O(d3) respectively–, and in the ability to be robust to
the missing data mechanism, including MNAR.

2 Optimal predictors in the presence of missing values

Notations We consider a data set Dn = {(X1, Y1), . . . , (Xn, Yn)} of independent pairs (Xi, Yi),
distributed as the generic pair (X, Y ), where X ∈ Rd and Y ∈ R. We introduce the indicator vector
M ∈ {0, 1}d which satisﬁes, for all 1 ≤ j ≤ d, Mj = 1 if and only if Xj is not observed. The random
vector M acts as a mask on X. We deﬁne the incomplete feature vector (cid:101)X ∈ (cid:101)X = (R ∪ {NA})d
(see [27], [26, appendix B]) as (cid:101)Xj = NA if Mj = 1, and (cid:101)Xj = Xj otherwise. As such, (cid:101)X is a

2

mixed categorical and continuous variable. An example of realization (lower-case letters) of the
previous random variables would be a vector x = (1.1, 2.3, −3.1, 8, 5.27) with the missing pattern
m = (0, 1, 0, 0, 1), giving (cid:101)x = (1.1, NA, − 3.1, 8, NA).
For realizations m of M , we also denote by obs(m) (resp. mis(m)) the indices of the zero entries of
m (resp. non-zero). Following classic missing-value notations, we let Xobs(M ) (resp. Xmis(M )) be
the observed (resp. missing) entries in X. Pursuing the above example, we have mis(m) = {1, 4},
obs(m) = {0, 2, 3}, xobs(m) = (1.1, −3.1, 8), xmis(m) = (2.3, 5.27). To lighten notations, when
there is no ambiguity, we remove the explicit dependence in m and write, e.g., Xobs.

2.1 Problem statement: supervised learning with missing values

We consider a linear model of the complete data, such that the response Y satisﬁes:

Y = β(cid:63)

0 + (cid:104)X, β(cid:63)(cid:105) + ε,

for some β(cid:63)

0 ∈ R, β(cid:63) ∈ Rd, and ε ∼ N (0, σ2).

(1)

Prediction with missing values departs from standard linear-model settings: the aim is to predict Y
given (cid:101)X, as the complete input X may be unavailable. The corresponding optimization problem is:

f (cid:63)
(cid:101)X

∈ argmin
f : (cid:101)X →R

E[(Y − f ( (cid:101)X))2],

(2)

where f (cid:63)
is the Bayes predictor for the squared loss, in the presence of missing values. The main
(cid:101)X
difﬁculty of this problem comes from the half-discrete nature of the input space (cid:101)X . Indeed, the Bayes
predictor f (cid:63)
(cid:101)X

( (cid:101)X) = E(cid:2)Y | (cid:101)X(cid:3) can be rewritten as:
(cid:88)

( (cid:101)X) = E (cid:2)Y | M, Xobs(M )

(cid:3) =

f (cid:63)
(cid:101)X

m∈{0,1}d

E (cid:2)Y |Xobs(m), M = m(cid:3) 1M =m,

(3)

which highlights the combinatorial issue of solving (2): one may need to optimize 2d submodels, for
the different m. In the following, we write the Bayes predictor f (cid:63) as a function of (Xobs(M ), M ):
f (cid:63)(Xobs(M ), M ) = E (cid:2)Y |Xobs(M ), M (cid:3) .

2.2 Expression of the Bayes predictor under various missing-values mechanisms

There is no general closed-form expression for the Bayes predictor, as it depends on the data
distribution and missingness mechanism. However, an exact expression can be derived for Gaussian
data with various missingness mechanisms.
Assumption 1 (Gaussian data). The distribution of X is Gaussian, that is, X ∼ N (µ, Σ).
Assumption 2 (MCAR mechanism). For all m ∈ {0, 1}d, P (M = m|X) = P (M = m).
Assumption 3 (MAR mechanism). For all m ∈ {0, 1}d, P (M = m|X) = P (M = m|Xobs(m)).

Proposition 2.1 (MAR Bayes predictor). Assume that the data are generated via the linear model
deﬁned in equation (1) and satisfy Assumption 1. Additionally, assume that either Assumption 2 or
Assumption 3 holds. Then the Bayes predictor f (cid:63) takes the form

f (cid:63)(Xobs, M ) = β(cid:63)

0 + (cid:104)β(cid:63)

obs, Xobs(cid:105) + (cid:104)β(cid:63)

mis, µmis + Σmis,obs(Σobs)−1(Xobs − µobs)(cid:105),

(4)

where we use obs (resp. mis) instead of obs(M ) (resp. mis(M )) for lighter notations.

Obtaining the Bayes predictor expression turns out to be far more complicated for general MNAR
settings but feasible for the Gaussian self-masking mechanism described below.
Assumption 4 (Gaussian self-masking). The missing data mechanism is self-masked with
P (M |X) = (cid:81)d

k=1 P (Mk|Xk) and ∀k ∈

P (Mk = 1|Xk) = Kk exp

(cid:19)

with 0 < Kk < 1.

(cid:74)
(cid:18)

,

1, d
(cid:75)
(Xk − (cid:101)µk)2
1
(cid:101)σ2
2

−

k

3

Proposition 2.2 (Bayes predictor with Gaussian self-masking). Assume that the data are generated
via the linear model deﬁned in equation (1) and satisfy Assumption 1 and Assumption 4. Let
Σmis|obs = Σmis,mis−Σmis,obsΣ−1
obsΣobs,mis, and let D be the diagonal matrix such that diag(D) =
((cid:101)σ2

1, . . . , (cid:101)σ2

f (cid:63)(Xobs, M ) = β(cid:63)

d). Then the Bayes predictor writes
obs, Xobs(cid:105) + (cid:104)β(cid:63)
0 + (cid:104)β(cid:63)
× (˜µmis + DmisΣ−1

mis, (Id + DmisΣ−1

mis|obs)−1

mis|obs(µmis + Σmis,obs (Σobs)−1 (Xobs − µobs)))(cid:105)

(5)

The proof of Propositions 2.1 and 2.2 are in the Supplementary Materials (A.3 and A.4). These are
the ﬁrst results establishing exact expressions of the Bayes predictor in a MAR and speciﬁc MNAR
mechanisms. Note that these propositions show that the Bayes predictor is linear by pattern under the
assumptions studied, i.e., each of the 2d submodels in equation 3 are linear functions of Xobs. For
non-Gaussian data, the Bayes predictor may not be linear by pattern [13, Example 3.1].

Generality of the Gaussian self-masking model For a self-masking mechanism where the prob-
ability of being missing increases (or decreases) with the value of the underlying variable, probit
or logistic functions are often used [12]. A Gaussian self-masking model is also a suitable model:
setting the mean of the Gaussian close to the extreme values gives a similar behaviour. In addition, it
covers cases where the probability of being missing is centered around a given value.

3 NeuMiss networks: learning by approximating the Bayes predictors

3.1

Insight to build a network: sharing parameters across missing-value patterns

Computing the Bayes predictors in equations (4) or (5) requires to estimate the inverse of each
submatrix Σobs(m) for each missing-data pattern m ∈ {0, 1}d, ie one linear model per missing-data
pattern. For a number of hidden units ∝ 2d, a MLP with ReLU non-linearities can ﬁt these linear
models independently from one-another, and is shown to be consistent [13]. But it is prohibitive
when d grows. Such an architecture is largely over-parametrized as it does not share information
between similar missing-data patterns. Indeed, the slopes of each of the linear regression per pattern
given by the Bayes predictor in equations (4) and (5) are linked via the inverses of Σobs.

Thus, one approach is to estimate only one vector µ and one covariance matrix Σ via an expectation
maximization (EM) algorithm [2], and then compute the inverses of Σobs. But the computational
complexity then scales linearly in the number of missing-data patterns (which is in the worst case
exponential in the dimension d), and is therefore also prohibitive when the dimension increases.

In what follows, we propose an in-between solution, modeling the relationships between the slopes
for different missing-data patterns without directly estimating the covariance matrix. Intuitively,
observations from one pattern will be used to estimate the regression parameters of other patterns.

3.2 Differentiable approximations of the inverse covariances with Neumann series

The major challenge of equations (4) and (5) is the inversion of the matrices Σobs(m) for all m ∈
{0, 1}d. Indeed, there is no simple relationship for the inverses of different submatrices in general.
As a result, the slope corresponding to a pattern m cannot be easily expressed as a function of Σ.
We therefore propose to approximate (cid:0)Σobs(m)
(cid:1)−1
for all m ∈ {0, 1}d recursively in the following
way. First, we choose as a starting point a d × d matrix S(0). S(0)
obs(m) is then deﬁned as the sub-matrix
of S(0) obtained by selecting the columns and rows that are observed (components for which m = 0)
and is our order-0 approximation of (cid:0)Σobs(m)
. Then, for all m ∈ {0, 1}d, we deﬁne the order-(cid:96)
approximation S((cid:96))

via the following iterative formula: for all (cid:96) ≥ 1,

obs(m) of (cid:0)Σobs(m)

(cid:1)−1

(cid:1)−1

obs(m) = (Id − Σobs(m)) S((cid:96)−1)
S((cid:96))

obs(m) + Id.

(6)

The iterates S((cid:96))
in fact Neumann series truncated to (cid:96) terms if S(0) = Id.

obs(m) converge linearly to (Σobs(m))−1(A.5 in the Supplementary Materials), and are

4

We now deﬁne the order-(cid:96) approximation of the Bayes predictor in MAR settings (equation (4)) as

(cid:96) (Xobs, M ) = (cid:104)β(cid:63)
f (cid:63)

obs, Xobs(cid:105) + (cid:104)β(cid:63)

mis, µmis + Σmis,obsS((cid:96))

obs(m)(Xobs − µobs)(cid:105).

(7)

The error between the Bayes predictor and its order-(cid:96) approximation is provided in Proposition 3.1.

Proposition 3.1. Let ν be the smallest eigenvalue of Σ. Assume that the data are generated via
a linear model deﬁned in equation (1) and satisfy Assumption 1. Additionally, assume that either
Assumption 2 or Assumption 3 holds and that the spectral radius of Σ is strictly smaller than one.
Then, for all (cid:96) ≥ 1,

(cid:96) (Xobs, M ) − f (cid:63)(Xobs, M )(cid:1)2(cid:21)
(cid:20)
(cid:0)f (cid:63)

E

≤

(1 − ν)2(cid:96)(cid:107)β(cid:63)(cid:107)2
2
ν

(cid:20)
(cid:13)
(cid:13)Id − S(0)

E

obs(M )Σobs(M )

(cid:21)

(cid:13)
2
(cid:13)
2

(8)

The error of the order-(cid:96) approximation decays exponentially fast with (cid:96). More importantly, if the
submatrices S(0)
obs of S(0) are good approximations of (Σobs)−1 on average, that is if we choose S(0)
which minimizes the expectation in the right-hand side in inequality (8), then our model provides a
good approximation of the Bayes predictor even with order (cid:96) = 0. This is the case for a diagonal
covariance matrix, as taking S(0) = Σ−1 has no approximation error as (Σ−1)obs = (Σobs)−1.

3.3 NeuMiss network architecture: multiplying by the mask

Network architecture We propose a neural-network architecture to approximate the Bayes predic-
tor, where the inverses (Σobs)−1 are computed using an unrolled version of the iterative algorithm.
Figure 1 gives a diagram for such neural network using an order-3 approximation corresponding to a
depth 4. x is the input, with missing values replaced by 0. µ is a trainable parameter corresponding to
the parameter µ in equation (7). To match the Bayes predictor exactly (equation (7)), weight matrices
should be simple transformations of the covariance matrix indicated in blue on Figure 1.
Following strictly Neummann iterates would call for a shared weight matrix across all W (k)
N eu. Rather,
we learn each layer independently. This choice is motivated by works on iterative algorithm unrolling
[5] where independent layers’ weights can improve a network’s approximation performance [33].
Note that [3] has also introduced a neural network architecture based on unrolling the Neumann
series. However, their goal is to solve a linear inverse problem with a learned regularization, which is
very different from ours.

Multiplying by the mask Note that the observed indices change for each sample, leading to an
implementation challenge. For a sample with missing data pattern m, the weight matrices S(0), W (1)
N eu
and W (2)
N eu of Figure 1 should be masked such that their rows and columns corresponding to the
indices mis(m) are zeroed, and the rows of WM ix corresponding to obs(m) as well as the columns of
WM ix corresponding to mis(m) are zeroed. Implementing efﬁciently a network in which the weight
matrices are masked differently for each sample can be challenging. We thus use the following trick.
Let W be a weight matrix, v a vector, and ¯m = 1 − m. Then (W (cid:12) ¯m ¯m(cid:62))v = (W (v (cid:12) ¯m)) (cid:12) ¯m,
i.e, using a masked weight matrix is equivalent to masking the input and output vector. The network
can then be seen as a classical network where the nonlinearities are multiplications by the mask.

µ (cid:12) ¯m

(cid:12) ¯m

(cid:12) ¯m

(cid:12) ¯m

µ (cid:12) m

(cid:12)m

x (cid:12) ¯m −

S(0)

W (1)
N eu
(Id − Σobs)

+

W (2)
N eu
(Id − Σobs)

+ W (3)

M ix
(Σmis,obs)

+

Wβ
β

Y

Neumann iterations

Non-linearity

Figure 1: NeuMiss network architecture with a depth of 4 — ¯m = 1 − m. Each weight matrix
W (k) corresponds to a simple transformation of the covariance matrix indicated in blue.

5

Approximation of the Gaussian self-masking Bayes predictor Although our architecture is
motivated by the expression of the Bayes predictor in MCAR and MAR settings, a similar architecture
can be used to target the prediction function (5) for self-masking data. To see why, let’s ﬁrst assume
that DmisΣ−1

mis|obs ≈ Id. Then, the self-masking Bayes predictor (5) becomes:

f (cid:63)(Xobs, M ) ≈ β(cid:63)

0 + (cid:10)β(cid:63)
+ (cid:104)β(cid:63)

mis,

obs, Xobs(cid:105)
1
2

(˜µmis + µmis) +

Σmis,obs (Σobs)−1 (Xobs − µobs)(cid:11)

(9)

1
2

2 (˜µmis + µmis) and Σmis,obs is scaled down by a factor 1

i.e., its expression is the same as for the M(C)AR Bayes predictor (4) except that µmis is replaced
by 1
2 . Thus, under this approximation, the
self-masking Bayes predictor can be modeled by our proposed architecture (just as the M(C)AR
Bayes predictor), the only difference being the targeted values for the parameters µ and Wmix
mis|obs ≈ ˆDmis where ˆD is a
of the network. A less coarse approximation also works: DmisΣ−1
diagonal matrix. In this case, the proposed architecture can perfectly model the self-masking Bayes
predictor: the parameter µ of the network should target (Id + ˆD)−1(˜µ + ˆDµ) and Wmix should
target (Id + ˆD)−1 ˆD Σ instead of simply Σ in the M(C)AR case. Consequently, our architecture can
well approximate the self-masking Bayes predictor by adjusting the values learned for the parameters
µ and Wmix if DmisΣ−1

mis|obs are close to diagonal matrices.

3.4 Link with the multilayer perceptron with ReLU activations

A common practice to handle missing values is to consider as input the data concatenated with the
mask eg in [13]. The next proposition connects this practice to Neumman networks.
Proposition 3.2 (equivalence MLP - depth-1 NeuMiss network). Let [X (cid:12) (1 − M ), M ] ∈ [0, 1]d ×
{0, 1}d be an input X imputed by 0 concatenated with the mask M .

• Let HReLU = (cid:0)W ∈ Rd×2d, ReLU (cid:1) be a hidden layer which connects [X (cid:12) (1 − M ), M ]

to d hidden units, and applies a ReLU nonlinearity to the activations.

• Let H(cid:12)M = (cid:0)W ∈ Rd×d, µ, (cid:12)M (cid:1) be a hidden layer that connects an input (X − µ) (cid:12)
(1 − M ) to d hidden units, and applies a (cid:12)M nonlinearity.

k

and h(cid:12)M

the outputs of the kth hidden unit of each layer. Then there exists
Denote by hReLU
a conﬁguration of the weights of the hidden layer HReLU such that H(cid:12)M and HReLU have
the same hidden units activated for any (Xobs, M ), and activated hidden units are such that
hReLU
k

(Xobs, M ) + ck where ck ∈ R.

(Xobs, M ) = h(cid:12)M

k

k

Proposition 3.2 states that a hidden layer HReLU can be rewritten as a H(cid:12)M layer up to a constant.
Note that, as soon as another layer is stacked after H(cid:12)M or HReLU , this additional constant can be
absorbed into the biases of this new layer. Thus the weights of HReLU can be learned so as to mimic
H(cid:12)M . In our case, this means that a MLP with ReLU activations, one hidden layer of d hidden units,
and which operates on the concatenated vector, is closely related to the 1-depth NeuMiss network
(see Figure 1), thereby providing theoretical support for the use of the latter MLP. This theoretical
link completes the results of [13], who showed experimentally that in such a MLP O(d) units were
enough to perform well on Gaussian data, but only provided theoretical results with 2d hidden units.

4 Empirical results

4.1 The (cid:12)M nonlinearity is crucial to the performance

The speciﬁcity of NeuMiss networks resides in the (cid:12)M nonlinearities, instead of more conventional
choices such as ReLU. Figure 2 shows how the choice of nonlinearity impacts the performance as a
function of the depth. We compare two networks that take as input the data imputed by 0 concatenated
with the mask: MLP Deep which has 1 to 10 hidden layers of d hidden units followed by ReLU
nonlinearities and MLP Wide which has one hidden layer whose width is increased followed by a
ReLU nonlinearity. This latter was shown to be consistent given 2d hidden units [13].

Figure 2 shows that increasing the capacity (depth) of MLP Deep fails to improve the performances,
unlike with NeuMiss networks. Similarly, it is also signiﬁcantly more effective to increase the

6

Figure 2: Performance as a func-
tion of capacity across architec-
tures — Empirical evolution of the
performance for a linear generat-
ing mechanism in MCAR settings.
Data are generated under a linear
model with Gaussian covariates in
a MCAR setting (50% missing val-
ues, n = 105, d = 20).

capacity of the NeuMiss network (depth) than to increase the capacity (width) of MLP Wide. These
results highlight the crucial role played by the (cid:12) nonlinearity. Finally, the performance of MLP Wide
with d hidden units is close to that of NeuMiss with a depth of 1, suggesting that it may rely on the
weight conﬁguration established in Proposition 3.2.

4.2 Approximation learned by the NeuMiss network

The NeuMiss architecture was designed to approximate well the Bayes predictor (4). As shown
in Figure 1, its weights can be chosen so as to express the Neumann approximation of the Bayes
predictor (7) exactly. We will call this particular instance of the network, with S(0) set to identity,
the analytic network. However, just like LISTA [5] learns improved weights compared to the ISTA
iterations, the NeuMiss network may learn improved weights compared to the Neumann iterations.
Comparing the performance of the analytic network to its learned counterpart on simulated MCAR
data, Figure 3 (left) shows that the learned network requires a much smaller depth compared to
the analytic network to reach a given performance. Moreover, the depth-1 learned network largely
outperforms the depth-1 analytic network, which means that it is able to learn a good initialization
S(0) for the iterates. Figure 3 also compares the performance of the learned network with and without
residual connections, and shows that residual connections are not needed for good performance. This
observation is another hint that the iterates learned by the network depart from the Neumann ones.

4.3 NeuMiss networks require O(d2) samples

Figure 3 (right) studies the depth for which NeuMiss networks perform well for different number of
samples n and features d. It outlines that NeuMiss networks work well in regimes with more than
10 samples available per model parameters, where the number of model parameters scales as d2. In
general, even with many samples, depth of more than 5 explore diminishing returns. Supplementary
ﬁgure 5 shows the same behavior in various MNAR settings.

MCAR

Figure 3: Left: learned versus analytic Neumann iterates — NeuMiss analytic is the NeuMiss
architecture with weights set to represent (6), supposing we have access to the ground truth parameters,
NeuMiss (resp. NeuMiss res) corresponds to the network without (resp. with) residual connections.
Right: Required capacity in various settings — Performance of NeuMiss networks varying the
depth in simulations with different number of samples n and of features d.

7

103104Number of parameters0.00−0.05−0.10R2 score - Bayes rateMLP DeepMLP WideNeuMissTest setTrain setNetworkdepth13579width1d3d10d30d50d100101102depth−0.15−0.10−0.050.00R2 score - Bayes rateNeuMiss_resNeuMissNeuMiss_analyticNeuMiss_resNeuMissNeuMiss_analytic10−210−1100# params / # samples0.0−0.1−0.2R2 score - Bayes raten=1⋅104, d=10n=2⋅104, d=10n=1⋅105, d=10n=1⋅104, d=20n=2⋅104, d=20n=1⋅105, d=20n=1⋅104, d=50n=2⋅104, d=50n=1⋅105, d=50NeuMissnetworkdepth13579MCAR

Gaussian self-masking

Probit self-masking

Figure 4: Predictive performances in various scenarios — varying missing-value mechanisms,
number of samples n, and number of features d. All experiments are repeated 20 times. For self-
masking settings, the x-xaxis is in log scale, to accommodate the large difference between methods.

4.4 Prediction performance: NeuMiss networks are robust to the missing data mechanism

We now evaluate the performance of NeuMiss networks compared to other methods under various
missing values mechanisms. The data are generated according to a multivariate Gaussian distribution,
with a covariance matrix Σ = U U (cid:62) + diag((cid:15)), U ∈ Rd× d
2 , and the entries of U drawn from a
standard normal distribution. The noise (cid:15) is a vector of entries drawn uniformly in (cid:2)10−2, 10−1(cid:3)
to make Σ full rank. The mean is drawn from a standard normal distribution. The response Y is
generated as a linear function of the complete data X as in equation 1. The noise is chosen to obtain
a signal-to-noise ratio of 10. 50% of entries on each features are missing, with various missing
data mechanisms: MCAR, MAR, Gaussian self-masking and Probit self-masking. The Gaussian
self-masking is obtained according to Assumption 4, while the Probit self-masking is a similar
setting where the probability for feature j to be missing depends on its value Xj through an inverse
probit function. We compare the performances of the following methods:

• EM: an Expectation-Maximisation algorithm [30] is run to estimate the parameters of the joint
probability distribution of X and Y –Gaussian– with missing values. Then based on this estimated
distribution, the prediction is given by taking the expectation of Y given X.

• MICE + LR: the data is ﬁrst imputed using conditional imputation as implemented in scikit-learn’s
[25] IterativeImputer, which proceeds by iterative ridge regression. It adapts the well known MICE
[31] algorithm to be able to impute a test set. A linear regression is then ﬁt on the imputed data.
• MLP: A multilayer perceptron as in [13], with one hidden layer followed by a ReLU nonlinearity,
taking as input the data imputed by 0 concatenated with the mask. The width of the hidden layer is
varied between d and 100 d hidden units, and chosen using a validation set. The MLP is trained
using ADAM and a batch size of 200. The learning rate is initialized to 10−2
and decreased by a
d
factor of 0.2 when the loss stops decreasing for 2 epochs. The training ﬁnishes when either the
learning rate goes below 5 × 10−6 or the maximum number of epochs is reached.

• NeuMiss : The NeuMiss architecture, without residual connections, choosing the depth on a
validation set. The architecture was implemented using PyTorch [24], and optimized using
stochastic gradient descent and a batch size of 10. The learning rate schedule and stopping
criterion are the same as for the MLP.

For MCAR, MAR, and Gaussian self-masking settings, the performance is given as the obtained R2
score minus the Bayes rate (the closer to 0 the better), the best achievable R2 knowing the underlying
ground truth parameters. In our experiments, an estimation of the Bayes rate is obtained using the
score of the Bayes predictor. For probit self-masking, as we lack an analytical expression for the
Bayes predictor, the performance is given with respect to the best performance achieved across all
methods. The code to reproduce the experiments is available in GitHub 1.

1https://github.com/marineLM/NeuMiss

8

−0.10−0.050.00EMMICE + LRMLPNeuMiss−0.050.00−0.10−0.050.00EMMICE + LRMLPNeuMiss−0.10−0.050.00−0.15−0.10R2 - Bayes rateEMMICE + LRMLPNeuMiss−0.15−0.10d=10n=20000n=100000d=20d=50-0.1-0.01EMMICE + LRMLPNeuMiss-0.1-0.01-0.1-0.02EMMICE + LRMLPNeuMiss-0.1-0.01-0.2-0.1R2 - Bayes rateEMMICE + LRMLPNeuMiss-0.1-0.05d=10n=20000n=100000d=20d=50-0.1-0.01EMMICE + LRMLPNeuMiss-0.1-0.01-0.1-0.01EMMICE + LRMLPNeuMiss-0.1-0.01-0.1-0.01R2 - (best R2)EMMICE + LRMLPNeuMiss-0.1-0.01d=10n=20000n=100000d=20d=50In MCAR settings, ﬁgure 4 shows that, as expected, EM gives the best results when tractable. Yet, we
could not run it for number of features d ≥ 50. NeuMiss is the best performing method behind EM,
in all cases except for n = 2 × 104, d = 50, where depth of 1 or greater overﬁt due to the low ratio of
number of parameters to number of samples. In such situation, MLP has the same expressive power
and performs slightly better. Note that for a high samples-to-parameters ratio (n = 1 × 105, d = 10),
NeuMiss reaches an almost perfect R2 score, less than 1% below the Bayes rate. The results for the
MAR setting are very similar to the MCAR results, and are given in supplementary ﬁgure 6.

For the self-masking mechanisms, the NeuMiss network signiﬁcantly improves upon the competitors,
followed by the MLP. This is even true for the probit self-masking case for which we have no
theoretical results. The gap between the two architectures widens as the number of samples increases,
with the NeuMiss network beneﬁting from a large amount of data. These results emphasize the
robustness of NeuMiss and MLP to the missing data mechanism, including MNAR settings in which
EM or conditional imputation do not enable statistical analysis.

5 Discussion and conclusion

Traditionally, statistical models are adapted to missing values using EM or imputation. However,
these require strong assumptions on the missing values. Rather, we frame the problem as a risk
minimization with a ﬂexible yet tractable function family. We propose the NeuMiss network, a
theoretically-grounded architecture that handles missing values using multiplication by the mask as
nonlinearities. It targets the Bayes predictor with differentiable approximations of the inverses of the
various covariance submatrices, thereby reducing complexity by sharing parameters across missing
data patterns. Strong connections between a shallow version of our architecture and the common
practice of inputing the mask to an MLP is established.

The NeuMiss architecture has clear practical beneﬁts. It is robust to the missing-values mechanism,
often unknown in practice. Moreover its sample and computational complexity are independent of
the number of missing-data patterns, which allows to work with datasets of higher dimensionality
and limited sample sizes. This work opens many perspectives, in particular using this network as a
building block in larger architectures, eg to tackle nonlinear problems.

Broader Impact

In our work, we proposed theoretical foundations to justify the use of a speciﬁc neural network
architecture in the presence of missing-values.

Neural networks are known for their challenging black-box nature. We believe that such theory leads
to a better understanding of the mechanisms at work in neural networks.

Our architecture is tailored for missing data. These are present in many applications, in particular in
social or health data. In these ﬁelds, it is common for under-represented groups to exhibit a higher
percentage of missing values (MNAR mechanism). Dealing with these missing values will deﬁnitely
improve prediction for these groups, thereby reducing potential bias against these exact same groups.

As any predictive algorithm, our proposal can be misused in a variety of context, including in
medical science, for which a proper assessment of the speciﬁc characteristics of the algorithm output
is required (assessing bias in prediction, prevent false conclusion resulting from misinterpreting
outputs). Yet, by improving performance and understanding of a fundamental challenge in many
applications settings, our work is not facilitating more unethical aspects of AI than ethical applications.
Rather, medical studies that suffer chronically from limited sample sizes are mostly likely to beneﬁt
from the reduced sample complexity that these advances provide.

Acknowledgments and Disclosure of Funding

This work was funded by ANR-17-CE23-0018 - DirtyData - Intégration et nettoyage de données pour
l’analyse statistique (2017) and the MissingBigData grant from DataIA.

9

References

[1] Jean-Yves Audibert, Olivier Catoni, and Others. Robust linear least squares regression. The Annals of

Statistics, 39(5):2766–2794, 2011.

[2] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the royal statistical society. Series B (methodological), pages 1–38, 1977.

[3] D. Gilton, G. Ongie, and R. Willett. Neumann networks for linear inverse problems in imaging. IEEE

Transactions on Computational Imaging, 6:328–343, 2020.

[4] Yu Gong, Hossein Hajimirsadeghi, Jiawei He, Megha Nawhal, Thibaut Durand, and Greg Mori. Variational
selective autoencoder. In Cheng Zhang, Francisco Ruiz, Thang Bui, Adji Bousso Dieng, and Dawen Liang,
editors, Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference, volume 118
of Proceedings of Machine Learning Research, pages 1–17. PMLR, 08 Dec 2020.

[5] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th
International Conference on International Conference on Machine Learning, pages 399–406, 2010.

[6] Trevor Hastie, Rahul Mazumder, Jason D. Lee, and Reza Zadeh. Matrix completion and low-rank svd via
fast alternating least squares. J. Mach. Learn. Res., 16(1):3367–3402, January 2015. ISSN 1532-4435.

[7] José Miguel Hernández-Lobato, Neil Houlsby, and Zoubin Ghahramani. Probabilistic matrix factorization
with non-random missing data. In International Conference on Machine Learning, pages 1512–1520,
2014.

[8] Suk-Geun Hwang. Cauchy’s Interlace Theorem for Eigenvalues of Hermitian Matrices. The American

Mathematical Monthly, 111(2):157, February 2004. ISSN 00029890. doi: 10.2307/4145217.

[9] Joseph G Ibrahim, Stuart R Lipsitz, and M-H Chen. Missing covariates in generalized linear models when
the missing data mechanism is non-ignorable. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 61(1):173–190, 1999.

[10] Nicholas Tierney Imke Mayer Julie Josse and Nathalie Vialaneix. R-miss-tastic: a uniﬁed platform for

missing values methods and workﬂows, 2019.

[11] Julie Josse, Nicolas Prost, Erwan Scornet, and Gaël Varoquaux. On the consistency of supervised learning

with missing values. arXiv preprint arXiv:1902.06931, 2019.

[12] J K Kim and Z Ying. Data Missing Not at Random, special issue. Statistica Sinica. Institute of Statistical

Science, Academia Sinica, 2018.

[13] Marine Le Morvan, Nicolas Prost, Julie Josse, Erwan Scornet, and Gaël Varoquaux. Linear predic-
tor on linearly-generated data with missing values: non consistency and solutions. arXiv preprint
arXiv:2002.00658, 2020.

[14] Roderick J A Little and Donald B Rubin. Statistical analysis with missing data. John Wiley & Sons, 2019.

[15] Chao Ma, Sebastian Tschiatschek, Konstantina Palla, José Miguel Hernández-Lobato, Sebastian Nowozin,
and Cheng Zhang. Eddi: Efﬁcient dynamic discovery of high-value information with partial vae. arXiv
preprint arXiv:1809.11142, 2018.

[16] Wei Ma and George H Chen. Missing not at random in matrix completion: The effectiveness of estimating
missingness probabilities under a low nuclear norm assumption. In Advances in Neural Information
Processing Systems, pages 14871–14880, 2019.

[17] Rajeshwari Majumdar and Suman Majumdar. On the conditional distribution of a multivariate normal

given a transformation–the linear case. Heliyon, 5(2):e01136, 2019.

[18] Benjamin M Marlin and Richard S Zemel. Collaborative prediction and ranking with non-random missing

data. In Proceedings of the third ACM conference on Recommender systems, pages 5–12. ACM, 2009.

[19] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: Deep generative modelling and imputation of in-
complete data sets. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
pages 4413–4423, Long Beach, California, USA, 09–15 Jun 2019. PMLR.

[20] Wang Miao, Peng Ding, and Zhi Geng.

Identiﬁability of normal and normal mixture models with
nonignorable missing data. Journal of the American Statistical Association, 111(516):1673–1683, 2016.

10

[21] K Mohan and J Pearl. Graphical Models for Processing Missing Data. Technical Report R-473-L,

Department of Computer Science, University of California, Los Angeles, CA, 2019.

[22] Razieh Nabi, Rohit Bhattacharya, and Ilya Shpitser. Full law identiﬁcation in graphical models of missing

data: Completeness results. arXiv preprint arXiv:2004.04872, 2020.

[23] Alfredo Nazabal, Pablo M Olmos, Zoubin Ghahramani, and Isabel Valera. Handling incomplete heteroge-

neous data using vaes. arXiv preprint arXiv:1807.03653, 2018.

[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems, pages 8024–8035, 2019.

[25] F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer,
R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, and E Duchesnay.
Scikit-learn: Machine Learning in Python . Journal of Machine Learning Research, 12:2825–2830, 2011.

[26] Paul R Rosenbaum and Donald B Rubin. Reducing bias in observational studies using subclassiﬁcation
on the propensity score. Journal of the American Statistical Association, 79(387):516–524, 1984. doi:
10.2307/2288398.

[27] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976.

[28] George AF Seber and Alan J Lee. Wiley series in probability and statistics. Linear Regression Analysis,

pages 36–44, 2003.

[29] Gong Tang, Roderick JA Little, and Trivellore E Raghunathan. Analysis of multivariate missing data with

nonignorable nonresponse. Biometrika, 90(4):747–764, 2003.

[30] Ported to R by Alvaro A. Novo. Original by Joseph L. Schafer <jls@stat.psu.edu>. norm: Analysis of

multivariate normal datasets with missing values, 2013. R package version 1.0-9.5.

[31] S van Buuren. Flexible Imputation of Missing Data. Chapman and Hall/CRC, Boca Raton, FL, 2018.

[32] Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. Doubly robust joint learning for recommendation on
data missing not at random. In International Conference on Machine Learning, pages 6638–6647, 2019.

[33] Bo Xin, Yizhou Wang, Wen Gao, and David Wipf. Maximal Sparsity with Deep Networks? In Advances

in Neural Information Processing Systems (NeurIPS), pages 4340–4348, 2016.

[34] Jinsung Yoon, James Jordon, and Mihaela Schaar. GAIN: Missing Data Imputation using Generative

Adversarial Nets. In International Conference on Machine Learning, pages 5675–5684, 2018.

11

Supplementary materials – NeuMiss networks: differentiable
programming for supervised learning with missing values

A Proofs

A.1 Proof of Lemma 1

Lemma 1 (General expression of the Bayes predictor). Assume that the data are generated via the linear model
deﬁned in equation (1), then the Bayes predictor takes the form
obs(M ), Xobs(M )(cid:105) + (cid:104)β(cid:63)

(10)
mis(M )) correspond to the decomposition of the regression coefﬁcients in observed and

f (cid:63)(Xobs(M ), M ) = β(cid:63)
obs(M ), β(cid:63)

mis(M ), E[Xmis(M )|M, Xobs(M )](cid:105),

0 + (cid:104)β(cid:63)

where (β(cid:63)
missing elements.

Proof of Lemma 1. By deﬁnition of the linear model, we have

f (cid:63)
(cid:101)X ( (cid:101)X) = E[Y | (cid:101)X]
= E[β(cid:63)
= β(cid:63)

0 + (cid:104)β(cid:63), X(cid:105) | M, Xobs(M )]
obs(M ), Xobs(M )(cid:105) + (cid:104)β(cid:63)

0 + (cid:104)β(cid:63)

mis(M ), E[Xmis(M ) | M, Xobs(M )](cid:105).

A.2 Proof of Lemma 2
Lemma 2 (Product of two multivariate gaussians). Let f (X) = exp (cid:0)(X − a)(cid:62)A−1(X − a)(cid:1) and g(X) =
exp (cid:0)(X − b)(cid:62)B−1(X − b)(cid:1) be two Gaussian functions, with A and B positive semideﬁnite matrices. Then
the product f (X)g(X) is another gaussian function given by:
1
2

(a − b)(cid:62)(A + B)−1(a − b))

(X − µp)(cid:62)Σ−1

f (X)g(X) = exp

p (X − µp)

exp

1
2

−

−

(cid:18)

(cid:19)

(cid:19)

(cid:18)

where µp and Σp depend on a, A, b and B.

Proof of Lemma 2. Identifying the second and ﬁrst order terms in X we get:

p = A−1 + B−1
Σ−1
p µp = A−1a + B−1b

Σ−1

By completing the square, the product can be rewritten as:
1
2

(a(cid:62)A−1a + b(cid:62)B−1b − µ(cid:62)

f (X)g(X) = exp

−

(cid:18)

p Σ−1

p µp

(cid:19)

(cid:18)

exp

−

1
2

(X − µp)(cid:62)Σ−1

p (X − µp)

(11)

(12)

(cid:19)

Let’s now simplify the scaling factor:
c = a(cid:62)A−1a + b(cid:62)B−1b − µ(cid:62)

= a(cid:62)A−1a + b(cid:62)B−1b −

(cid:16)

p µp

p Σ−1
a(cid:62)A−1(A−1 + B−1)−1 + b(cid:62)B−1(A−1 + B−1)−1(cid:17) (cid:0)A−1a + B−1b(cid:1)

= a(cid:62)(A−1 − A−1(A−1 + B−1)−1A−1)a + b(cid:62)(B−1 − B−1(A−1 + B−1)−1B−1)b

− 2a(cid:62)(A−1(A−1 + B−1)−1B−1)b

= a(cid:62)(A + B)−1a + b(cid:62)(A + B)−1b − 2a(cid:62)(A + B)−1b
= (a − b)(cid:62)(A + B)−1(a − b)

The third equality is true because A and B are symmetric. The fourth equality uses the Woodbury identity and
the fact that:

(A−1(A−1 + B−1)−1B−1) = (cid:0)B(A−1 + B−1)A(cid:1)−1

The last equality allows to conclude the proof.

= (cid:0)BA−1A + BB−1A(cid:1)−1
= (B + A)−1

12

A.3 Proof of Proposition 2.1

Proposition 2.1 (MAR Bayes predictor). Assume that the data are generated via the linear model deﬁned in
equation (1) and satisfy Assumption 1. Additionally, assume that either Assumption 2 or Assumption 3 holds.
Then the Bayes predictor f (cid:63) takes the form
f (cid:63)(Xobs, M ) = β(cid:63)

obs, Xobs(cid:105) + (cid:104)β(cid:63)
where we use obs (resp. mis) instead of obs(M ) (resp. mis(M )) for lighter notations.

mis, µmis + Σmis,obs(Σobs)−1(Xobs − µobs)(cid:105),

0 + (cid:104)β(cid:63)

(4)

Lemma 1 gives the general expression of the Bayes predictor for any data distribution and missing data
mechanism. From this expression, on can see that the crucial step to compute the Bayes predictor is computing
E[Xmis|M, Xobs], or in other words, E[Xj|M, Xobs] for all j ∈ mis. In order to compute this expectation,
we will characterize the distribution P (Xj|M, Xobs) for all j ∈ mis. Let mis(cid:48)(M, j) = mis(M ) \ {j}. For
clarity, when there is no ambiguity we will just write mis(cid:48). Using the sum and product rules of probability, we
have:

P (Xj|M, Xobs) =

=

=

P (M, Xj, Xobs)
P (M, Xobs)
(cid:82) P (M, Xj, Xobs, Xmis(cid:48) )dXmis(cid:48)
(cid:82) (cid:82) P (M, Xj, Xobs, Xmis(cid:48) )dXmis(cid:48) dXj

(cid:82) P (M |Xobs, Xj, Xmis(cid:48) )P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48)
(cid:82) (cid:82) P (M |Xobs, Xj, Xmis(cid:48) )P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48) dXj

In the MCAR case, for all m ∈ {0, 1}d, P(M = m|X) = P(M = m), thus we have
P (M ) (cid:82) P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48)
P (M ) (cid:82) (cid:82) P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48) dXj
P (Xobs, Xj)
P (Xobs)

P (Xj|M, Xobs) =

=

(13)

(14)

(15)

(16)

(17)

(18)
On the other hand, assuming MAR mechanism, that is, for all m ∈ {0, 1}d, P (M = m|X) = P (M =
m|Xobs(m)), we have, given equation (15),

= P (Xj|Xobs)

P (Xj|M, Xobs) =

=

P (M |Xobs) (cid:82) P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48)
P (M |Xobs) (cid:82) (cid:82) P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48) dXj
P (Xobs, Xj)
P (Xobs)

= P (Xj|Xobs)

(19)

(20)

(21)

Therefore, if the missing data mechanism is MCAR or MAR, we have, according to equation (18) and (21),
E[Xmis(M ) | M, Xobs(M )] = E[Xmis(M ) |Xobs(M )].

Since X is a Gaussian vector distributed as N (µ, Σ), we know that
E[Xmis(M ) |Xobs(M )] satisﬁes
(cid:12)
E (cid:2)Xmis(m)
(cid:12) Xobs(m)

(cid:3) = µmis(m) + Σmis(m),obs(m)
[see, e.g., 17]. This concludes the proof according to Lemma 1.

(cid:0)Σobs(m)

the conditional expectation

(cid:1)−1 (cid:0)Xobs(m) − µobs(m)

(cid:1) ,

(22)

A.4 Proof of Proposition 2.2

Proposition 2.2 (Bayes predictor with Gaussian self-masking). Assume that the data are generated via the
linear model deﬁned in equation (1) and satisfy Assumption 1 and Assumption 4. Let Σmis|obs = Σmis,mis −
Σmis,obsΣ−1
d). Then the Bayes
predictor writes

obsΣobs,mis, and let D be the diagonal matrix such that diag(D) = ((cid:101)σ2

1, . . . , (cid:101)σ2

f (cid:63)(Xobs, M ) = β(cid:63)

0 + (cid:104)β(cid:63)

obs, Xobs(cid:105) + (cid:104)β(cid:63)

mis, (Id + DmisΣ−1

mis|obs)−1

× (˜µmis + DmisΣ−1

mis|obs(µmis + Σmis,obs (Σobs)−1 (Xobs − µobs)))(cid:105)

(5)

In the Gaussian self-masking case, according to Assumption 4, the probability factorizes as P (M = m|X) =
P (Mmis(m) = 1|Xmis(m))P (Mobs(m) = 0|Xobs(m)). Equation 15 can thus be rewritten as:

P (Xj|M, Xobs) =

P (Mobs = 0|Xobs) (cid:82) P (Mmis = 1|Xmis)P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48)
P (Mobs = 0|Xobs) (cid:82) (cid:82) P (Mmis = 1|Xmis)P (Xobs, Xj, Xmis(cid:48) )dXmis(cid:48) dXj

=

(cid:82) P (Mmis = 1|Xmis)P (Xmis|Xobs)dXmis(cid:48)
(cid:82) (cid:82) P (Mmis = 1|Xmis)P (Xmis|Xobs)dXmis(cid:48) dXj

(23)

(24)

13

Let D be the diagonal matrix such that diag(D) = (cid:101)σ2, where (cid:101)σ is deﬁned in Assumption 4. Then the masking
probability reads:

P (Mmis = 1|Xmis) =

d
(cid:89)

k∈mis

(cid:18)

Kk exp

−

1
2

(Xmis − (cid:101)µmis)(Dmis,mis)−1(Xmis − (cid:101)µmis)

(cid:19)

Using the conditional Gaussian formula, we have P (Xmis|Xobs) = N (Xmis|µmis|obs, Σmis|obs) with

µmis|obs = µmis + Σmis,obsΣ−1
Σmis|obs = Σmis,mis − Σmis,obsΣ−1

obs,obs (Xobs − µobs)
obsΣobs,mis

(25)

(26)

(27)

Thus, according to equation (25), P (Mmis = 1|Xmis) and P (Xmis|Xobs) are Gaussian functions of Xmis.
By Lemma 2, their product is also a Gaussian function given by:

P (Mmis = 1|Xmis)P (Xmis|Xobs) = K exp

−

(cid:18)

(Xmis − aM )(cid:62) (AM )−1 (Xmis − aM )

(cid:19)

(28)

1
2

where aM and AM depend on the missingness pattern and

K =

d
(cid:89)

k∈mis

(cid:113)

Kk

(2π)|mis||Σmis|obs|

(cid:18)

exp

−

1
2

(cid:19)
((cid:101)µmis − µmis|obs)(cid:62)(Σmis|obs + Dmis,mis)−1((cid:101)µmis − µmis|obs)

(AM )−1 = D−1

mis,mis + Σ−1
mis,mis (cid:101)µmis + Σ−1

mis|obs
mis|obsµmis|obs

(AM )−1 aM = D−1

Because K does not depend on Xmis, it simpliﬁes from eq 24. As a result we get:

P (Xj|M, Xobs) =

(cid:82) N (Xmis|aM , AM )dXmis(cid:48)
(cid:82) (cid:82) N (Xmis|aM , AM )dXmis(cid:48) dXj

= N (Xj|(aM )j, (AM )j,j)

By deﬁnition of the Bayes predictor, we have
(cid:101)X ( (cid:101)X) = β(cid:63)
f (cid:63)

0 + (cid:104)β(cid:63)

obs(M ), Xobs(M )(cid:105) + (cid:104)β(cid:63)

mis(M ), E[Xmis(M )|M, Xobs(M )](cid:105),

where

E[Xmis|M, Xobs] = (aM )mis.

Combining equations (30), (31), (35), we obtain

E[Xmis|M, Xobs] =

(cid:17)−1

(cid:16)
Id + DmisΣ−1
(cid:104)

×

˜µmis + DmisΣ−1

mis|obs

mis|obs

(cid:0)µmis + Σmis,obs (Σobs)−1 (Xobs − µobs)(cid:1)(cid:105)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

A.5 Controlling the convergence of Neumann iterates

Here we establish an auxiliary result, controlling the convergence of Neumann iterates to the matrix inverse.
Proposition A.1 (Linear convergence of Neumann iterations). Assume that the spectral radius of Σ is strictly
less than 1. Therefore, for all missing data patterns m ∈ {0, 1}d, the iterates S((cid:96))
obs(m) deﬁned in equation (6)
converge linearly towards (Σobs(m))−1 and satisfy, for all (cid:96) ≥ 1,

(cid:107)Id − Σobs(m)S((cid:96))

obs(m)(cid:107)2 ≤ (1 − νobs(m))(cid:96)(cid:107)Id − Σobs(m)S(0)

obs(m)(cid:107)2 ,

where νobs(m) is the smallest eigenvalue of Σobs(m).

Note that Proposition A.1 can easily be extended to the general case by working with Σ/ρ(Σ) and multiplying
the resulting approximation by ρ(Σ), where ρ(Σ) is the spectral radius of Σ.

Proof. Since the spectral radius of Σ is strictly smaller than one, the spectral radius of each submatrix Σobs(m)
is also strictly smaller than one. This is a direct application of Cauchy Interlace Theorem [8] or it can be seen
with the deﬁnition of the eigenvalues

ρ(Σobs(m)) = max

u∈R|obs(m)|

u(cid:62)Σobs(m)u = max
x∈Rd
xmis=0

x(cid:62)Σx ≤ max
x∈Rd

x(cid:62)Σx = ρ(Σ) .

14

Note that S(cid:96)
formula

obs(m) = (cid:80)(cid:96)−1

k=0 (Id − Σobs)k + (Id − Σobs)(cid:96) S0

obs(m) can be deﬁned recursively via the iterative

obs(m) = (Id − Σobs(m))S(cid:96)−1
S(cid:96)

obs(m) + Id

(38)

The matrix (Σobs(m))−1 is a ﬁxed point of the Neumann iterations (equation (38)). It veriﬁes the following
equation

By substracting 38 to this equation, we obtain

(Σobs(m))−1 = (Id − Σobs(m))(Σobs(m))−1 + Id .

(Σobs(m))−1 − S(cid:96)

obs(m) = (Id − Σobs(m))((Σobs(m))−1 − S(cid:96)−1

obs(m)) .

Multiplying both sides by Σobs(m) yields

(Id − Σobs(m)S(cid:96)

obs(m)) = (Id − Σobs(m))(Id − Σobs(m)S(cid:96)−1

obs(m)) .

Taking the (cid:96)2-norm and using Cauchy-Schwartz inequality yields

(cid:107)Id − Σobs(m)S(cid:96)

obs(m)(cid:107)2 ≤ (cid:107)Id − Σobs(m)(cid:107)2(cid:107)Id − Σobs(m)S(cid:96)−1

obs(m)(cid:107)2 .

(39)

(40)

(41)

(42)

Let νobs(m) be the smallest eigenvalue of Σobs(m), which is positive since Σ is invertible. Since the largest
eigenvalue of Σobs(m) is upper bounded by 1, we get that (cid:107)Id − (cid:101)Σ(cid:107)2 = (1 − νobs(m)) and by recursion we
obtain

(cid:107)Id − Σobs(m)S(cid:96)

obs(m)(cid:107)2 ≤ (1 − νobs(m))(cid:96)(cid:107)Id − Σobs(m)S0

obs(m)(cid:107)2 .

(43)

A.6 Proof of Proposition 3.1

Proposition 3.1. Let ν be the smallest eigenvalue of Σ. Assume that the data are generated via a linear model
deﬁned in equation (1) and satisfy Assumption 1. Additionally, assume that either Assumption 2 or Assumption 3
holds and that the spectral radius of Σ is strictly smaller than one. Then, for all (cid:96) ≥ 1,

(cid:20)

E

(cid:0)f (cid:63)

(cid:96) (Xobs, M ) − f (cid:63)(Xobs, M )(cid:1)2

(cid:21)

≤

(1 − ν)2(cid:96)(cid:107)β(cid:63)(cid:107)2
2
ν

(cid:20)

E

(cid:13)
(cid:13)Id − S(0)

obs(M )Σobs(M )

(cid:13)
(cid:13)

2
2

(cid:21)

(8)

According to Proposition 2.1 and the deﬁnition of the approximation of order p of the Bayes predictor (see
equations (7))

(cid:101)X,(cid:96)( (cid:101)X) = (cid:104)β(cid:63)
f (cid:63)

obs, Xobs(cid:105) + (cid:104)β(cid:63)

mis, µmis + Σmis,obsS((cid:96))

obs (Xobs − µobs)(cid:105) ,

Then

(cid:101)X ( (cid:101)X))2]
mis , Σmis,obs(S(cid:96)

E[(f (cid:63)

(cid:101)X,(cid:96)( (cid:101)X) − f (cid:63)
(cid:104)(cid:10)β(cid:63)
= E
(cid:104)
= E
(cid:104)
= E

(β(cid:63)

(β(cid:63)

mis)(cid:62)Σmis,obs(S(cid:96)

obs − Σ−1

mis)(cid:62)Σmis,obs(S(cid:96)

obs − Σ−1

obs − Σ−1

obs)(Xobs − µobs)(cid:11)2(cid:105)
obs)(Xobs − µobs)(Xobs − µobs)(cid:62)(S(cid:96)

obs − Σ−1

obs)Σobs,misβ(cid:63)

mis

(cid:105)

obs) E[(Xobs − µobs)(Xobs − µobs)(cid:62)|M ]
(cid:125)

(cid:124)

(cid:123)(cid:122)
Σobs

(S(cid:96)

obs − Σ−1

obs)Σobs,misβ(cid:63)

mis

(44)

(45)

(46)

(cid:105)

1

(cid:104)
= E

= E

(β(cid:63)
(cid:104)(cid:13)
(cid:13)(Σobs)
(cid:104)(cid:13)
(cid:13)(Σobs)− 1
≤ (cid:107)Σ−1(cid:107)2(cid:107)Σ(cid:107)2

= E

≤

1
ν

(cid:107)β(cid:63)(cid:107)2

mis)(cid:62)Σmis,obs(S(cid:96)

obs − Σ−1

obs)Σobs(S(cid:96)

obs − Σ−1

2 (Σobs)−1(ΣobsS(cid:96)

obs − Idobs)Σobs,misβ(cid:63)

2 (Idobs − ΣobsS(cid:96)

(cid:105)

(cid:13)
(cid:13)

2
2

mis

obs)Σobs,misβ(cid:63)
(cid:3)

obs(cid:107)2
2

2(cid:107)β(cid:63)(cid:107)2

2E(cid:2)(cid:107)Idobs − ΣobsS(cid:96)
2E(cid:2)(1 − νobs)2(cid:96)(cid:107)Idobs − ΣobsS0

obs(cid:107)2
2

(cid:3)

obs)Σobs,misβ(cid:63)
(cid:105)
(cid:13)
2
(cid:13)
2

mis

mis

(cid:105)

(47)

(48)

(49)

(50)

(51)

(52)

An important point for going from (50) to (51) is to notice that for any missing pattern, we have

(cid:107)Σobs,mis(cid:107)2 ≤ (cid:107)Σ(cid:107)2 and (cid:107)Σ−1

obs(cid:107)2 ≤ (cid:107)Σ−1(cid:107)2 .

15

The ﬁrst inequality can be obtained by observing that computing the largest singular value of Σobs,mis reduces
to solving a constrained version of the maximization problem that deﬁnes the largest eigenvalue of Σ:
(cid:107)Σx(cid:107)2

(cid:107)Σobs,mis(cid:107)2 = max

2 = (cid:107)Σ(cid:107)2 .

(cid:107)Σobs,·x(cid:107)2 ≤ max
(cid:107)x(cid:107)2=1
xobs=0

(cid:107)Σx(cid:107)2 ≤ max
(cid:107)x(cid:107)2=1

(cid:107)xmis(cid:107)2=1

(cid:107)Σobs,misxmis(cid:107)2 ≤ max
(cid:107)x(cid:107)2=1
xobs=0
i x)2 ≤ (cid:80)d

2 = (cid:80)

where we used (cid:107)Σobs,·x(cid:107)2
A similar observation can be done for computing the smallest eigenvalue of Σ, λmin(Σ):

i x)2 = (cid:107)Σx(cid:107)2
2.

i∈obs(Σ(cid:62)

i=1(Σ(cid:62)

λmin(Σ) = min

(cid:107)x(cid:107)2=1

x(cid:62)Σx ≤ min
(cid:107)x(cid:107)2=1
xmis=0

x(cid:62)Σx = min

(cid:107)xobs(cid:107)2=1

x(cid:62)
obsΣobsxobs = λmin(Σobs) .

and we can deduce the second inequality by noting that λmin(Σ) =

1
(cid:107)Σ−1(cid:107)2
2

and λmin(Σobs) =

1
(cid:107)Σ−1
obs(cid:107)2
2

.

A.7 Proof of Proposition 3.2

Proposition 3.2 (equivalence MLP - depth-1 NeuMiss network). Let [X (cid:12) (1 − M ), M ] ∈ [0, 1]d × {0, 1}d
be an input X imputed by 0 concatenated with the mask M .

• Let HReLU = (cid:0)W ∈ Rd×2d, ReLU (cid:1) be a hidden layer which connects [X (cid:12) (1 − M ), M ] to d

hidden units, and applies a ReLU nonlinearity to the activations.

• Let H(cid:12)M = (cid:0)W ∈ Rd×d, µ, (cid:12)M (cid:1) be a hidden layer that connects an input (X − µ) (cid:12) (1 − M ) to

d hidden units, and applies a (cid:12)M nonlinearity.

k

and h(cid:12)M

Denote by hReLU
the outputs of the kth hidden unit of each layer. Then there exists a conﬁguration
of the weights of the hidden layer HReLU such that H(cid:12)M and HReLU have the same hidden units activated
for any (Xobs, M ), and activated hidden units are such that hReLU
(Xobs, M ) + ck where
ck ∈ R.

(Xobs, M ) = h(cid:12)M

k

k

k

(cid:17)

∈ Rd×2d, ReLU

Obtaining
(cid:16)(cid:104)

W (X), W (M )(cid:105)

a (cid:12)M nonlinearity

from a ReLU nonlinearity. Let HReLU
=
be a hidden layer which connects [X, M ] to d hidden units, and
applies a ReLU nonlinearity to the activations. We denote by b ∈ Rd the bias corresponding to this layer. Let
. Depending on the missing data pattern that is given as input, the kth entry can correspond to either a
k ∈
(cid:75)
missing or an observed entry. We now write the activation of the kth hidden unit depending on whether entry k
is observed or missing. The activation of the kth hidden unit is given by

1, d
(cid:74)

ak = W (X)
= W (X)

k,. X + W (M )
k,obsXobs + W (M )

k,. M + bk

k,mis1mis + bk.

(53)

(54)

Emphasizing the role of W (M )
entry is observed or missing

k,k and W (X)

k,k , we can decompose equation (54) depending on whether the kth

If k ∈ mis,

If k ∈ obs,

ak = W (X)

ak = W (X)

k,obsXobs + W (M )
k,k Xk + W (X)

k,k + W (M )

k,mis\{k}1k,mis\{k} + bk

k,obs\{k}Xobs\{k} + W (M )

k,mis1mis + bk.

Suppose that the weights W (X) as well as W (M )
of X is ﬁnite, there exists a bias b∗
k which veriﬁes:
k,k Xk + W (X)

ak = W (X)

∀X,

i,j

i.e., there exists a bias b∗
Similarly, there exists W ∗,(M )

such that:

k,k
ak = W (X)

, i (cid:54)= j are ﬁxed. Then, under the assumption that the support

k,obs\{k}Xobs\{k} + W (M )

k,mis1mis + b∗

k ≤ 0

k such that the activation of the kth hidden unit is always negative when k is observed.

∀X,

i.e., there exists a weight W ∗,(M )
k,k
missing. Note that these results hold because the weight W (M )
k is missing. Let hk = ReLU (ak). By choosing bk = b∗

k,obsXobs + W ∗,(M )
k,mis\{k}1k,mis\{k} + b∗
such that the activation of the kth hidden unit is always positive when k is
k,k only appears in the expression of ak when entry
k and W (M )

k,k + W (M )

, we have that:

k,k = W ∗,(M )

k ≥ 0

(58)

k,k

If k ∈ mis, hk = ak
If k ∈ obs, hk = 0

As a result, the output of the hidden layer HReLU can be rewritten as:

i.e., a (cid:12)M nonlinearity is applied to the activations.

hk = ak (cid:12) M

16

(59)
(60)

(61)

(55)

(56)

(57)

(66)

(67)

(68)

(69)

Equating the slopes and biases of HReLU and H(cid:12)M . Let H(cid:12)M = (cid:0)W ∈ Rd×d, µ, (cid:12)M (cid:1) be the
layer that connect (X − µ) (cid:12) (1 − M ) to d hidden units via the weight matrix W , and applies a (cid:12)M nonlinearity
to the activations. We will denote by c ∈ Rd the bias corresponding to this layer.

The activations for this layer are given by:

ak = Wk,obs(Xobs − µobs) + ck

= Wk,obsXobs − Wk,obsµobs + ck

Then by applying the non-linearity we obtain the output of the hidden layer:

If k ∈ mis, hk = ak
If k ∈ obs, hk = 0

(62)
(63)

(64)
(65)

It is straigthforward to see that with the choice of bk = b∗
for HReLU , both hidden layers
have the same output hk = 0 when entry k is observed. It remains to be shown that there exists a conﬁguration
of the weights of HReLU such that the activations ak when entry k is missing are equal to those of H(cid:12)M . To
avoid confusions, we will now denote by a(N )
the activations of HReLU .
We recall here the activations for both layers as derived in 63 and 55.

the activations of H(cid:12)M and by a(R)

k,k = W ∗,(M )

k and W (M )

k,k

k

k

(cid:40)

If k ∈ mis,

a(N )
k = Wk,obsXobs − Wk,obsµobs + ck
k,k + W (M )
k,obsXobs + W ∗,(M )
k = W (X)
a(R)

k,mis\{k}1k,mis\{k} + b∗

k

k,. = Wk,., we obtain that both activations have the same slopes with regards to X. We now turn

By setting W (X)
to the biases. We have that:
W ∗,(M )

k,k + W (M )

k,mis\{k}1k,mis\{k} + b∗

k = W (M )

k,. 1 − W (M )

k,obs1 + b∗

k

We now set:

∀j ∈ obs, W (M )

kj = Wkjµj
k. 1 + b∗

k = ck

W (M )

to obtain that both activations have the same biases. Note that 68 sets the weights Wk,j for all j (cid:54)= k (since obs
can contain any entries except k). As a consequence, equation 69 implies an equation invloving W ∗,(M )
and b∗
k
where all other parameters have already been set. Since W ∗,(M )
k are also chosen to satisfy the inequalities
57 and 58, it may not be possible to choose them so as to also satify equation 69. As a result, the functions
computed by the activated hidden units of HReLU can be equal to those computed by H(cid:12)M up to a constant.

and b∗

kk

kk

B Additional results

B.1 NeuMiss network scaling law in MNAR

B.2 NeuMiss network performances in MAR

The MAR data was generated as follows: ﬁrst, a subset of variables with no missing values is randomly selected
(10%). The remaining variables have missing values according to a logistic model with random weights, but
whose intercept is chosen so as to attain the desired proportion of missing values on those variables (50%). As
can be seen from ﬁgure 6, the trends observed for MAR are the same as those for MCAR.

17

Gaussian self-masking

Probit self-masking

Figure 5: Required capacity in various MNAR settings — Top: Gaussian self-masking, bottom:
probit self-masking. Performance of NeuMiss networks varying the depth in simulations with
different number of samples n and of features d.

MAR

Figure 6: Predictive performances in MAR scenario — varying number of samples n, and number
of features d. All experiments are repeated 20 times.

18

10−210−1100# params / # samples0.0−0.1−0.2R2 score - Bayes raten=1⋅104, d=10n=2⋅104, d=10n=1⋅105, d=10n=1⋅104, d=20n=2⋅104, d=20n=1⋅105, d=20n=1⋅104, d=50n=2⋅104, d=50n=1⋅105, d=50NeuMissnetworkdepth1357910−210−1100# params / # samples0.0−0.1−0.2R2 score - Bayes raten=1⋅104, d=10n=2⋅104, d=10n=1⋅105, d=10n=1⋅104, d=20n=2⋅104, d=20n=1⋅105, d=20n=1⋅104, d=50n=2⋅104, d=50n=1⋅105, d=50NeuMissnetworkdepth13579−0.10−0.050.00EMMICE + LRMLPNeuMiss−0.10.0−0.10−0.050.00EMMICE + LRMLPNeuMiss−0.10.0−0.15−0.10−0.05R2 - Bayes rateEMMICE + LRMLPNeuMiss−0.15−0.10−0.05d=10n=20000n=100000d=20d=50