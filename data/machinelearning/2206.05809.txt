Geometric Policy Iteration for Markov Decision Processes

Yue Wu
University of California, Davis
yvwu@ucdavis.edu

Jesús A. De Loera
University of California, Davis
deloera@math.ucdavis.edu

2
2
0
2

n
u
J

4
2

]

G
L
.
s
c
[

2
v
9
0
8
5
0
.
6
0
2
2
:
v
i
X
r
a

ABSTRACT
Recently discovered polyhedral structures of the value function for
finite discounted Markov decision processes (MDP) shed light on
understanding the success of reinforcement learning. We investi-
gate the value function polytope in greater detail and characterize
the polytope boundary using a hyperplane arrangement. We further
show that the value space is a union of finitely many cells of the
same hyperplane arrangement, and relate it to the polytope of the
classical linear programming formulation for MDPs. Inspired by
these geometric properties, we propose a new algorithm, Geometric
Policy Iteration (GPI), to solve discounted MDPs. GPI updates the
policy of a single state by switching to an action that is mapped
to the boundary of the value function polytope, followed by an
immediate update of the value function. This new update rule aims
at a faster value improvement without compromising computa-
tional efficiency. Moreover, our algorithm allows asynchronous
updates of state values which is more flexible and advantageous
compared to traditional policy iteration when the state set is large.
We prove that the complexity of GPI achieves the best known bound
(cid:17) of policy iteration and empirically demonstrate

O
the strength of GPI on MDPs of various sizes.

(cid:16) | A |
1−𝛾 log 1
1−𝛾

CCS CONCEPTS
• Computing methodologies → Markov decision processes.

KEYWORDS
Markov Decision Processes, Policy Iteration, Polytopes

ACM Reference Format:
Yue Wu and Jesús A. De Loera. 2022. Geometric Policy Iteration for Markov
Decision Processes. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’22), August 14–18, 2022,
Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/
10.1145/3534678.3539478

1 INTRODUCTION
The Markov decision process (MDP) is the mathematical foundation
of reinforcement learning (RL) which has achieved great empirical
success in sequential decision problems. Despite RL’s success, new
mathematical properties of MDPs are to be discovered to better
theoretically understand RL algorithms. In this paper, we study the
geometric properties of discounted MDPs with finite states and
actions, and propose a new value-based algorithm inspired by their
polyhedral structures.

This work is licensed under a Creative Commons Attribution
International 4.0 License.

KDD ’22, August 14–18, 2022, Washington, DC, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9385-0/22/08.
https://doi.org/10.1145/3534678.3539478

A large family of methods for solving MDPs is based on the no-
tion of (state) values. The strategy of these methods is to maximize
the values, then extract the optimal policy from the optimal values.
One classic method is value iteration [4, 15] in which values are
greedily improved to optimum using the Bellman operator. It is
also well known that the optimal values can be solved by linear
programming (LP) [22] which attracts a lot of research interest due
to its mathematical formulation. The most efficient algorithms in
practice are often variants of policy iteration [15] which facilitates
the value improvement with policy updates. The value function,
which maps policies to the value space, is central to our analysis
throughout, and it plays a key role in understanding how values
are related to policies from a geometric perspective.

Although policy iteration and its variants are very efficient in
practice, their worst-case complexity was long believed exponen-
tial [19]. The major breakthrough was made by Ye [32] where
the author proved that both policy iteration and LP with Sim-
(cid:17). The author first
plex method [9] terminate in O
proved that the Simplex method with the most-negative-reduced-
cost pivoting rule is strongly polynomial in this situation. Then, a
variant of policy iteration called simple policy iteration was shown
to be equivalent to the Simplex method. Hansen et al. [12] later im-
proved the complexity of policy iteration by a factor of |𝑆 |. The best
(cid:17) proved by
known complexity of policy iteration is O
Scherrer [24].

(cid:16) | A |
1−𝛾 log 1
1−𝛾

(cid:16) |S | | A |
1−𝛾

log |S |
1−𝛾

In the LP formulation, the state values are optimized through
the vertices of the LP feasible region which is a convex polytope.
Surprisingly, it was recently discovered that the space of the value
function is a (possibly non-convex) polytopes [8]. We call such
object the value function polytope denoted by V. As opposed
to LP, the state values are navigated through V in policy iteration.
Moreover, the line theorem [8] states that the set of policies that
only differ in one state is mapped onto the same line segment in
the value function polytope. This suggests the potential of new
algorithms based on single-state updates.

Our first contribution is on the structure of the value function
polytope V. Specifically, we show that a hyperplane arrangement
𝐻𝑀𝐷𝑃 is shared by V and the polytope of the linear programming
formulation for MDPs. We characterize these hyperplanes using
the Bellman equation of policies that are deterministic in a single
state. We prove that the boundary of the value function polytope
𝜕V is the union of finitely many (convex polyhedral) cells of 𝐻𝑀𝐷𝑃 .
Moreover, each full-dimensional cell of the value function polytope
is contained in the union of finitely many full-dimensional cells
defined by 𝐻𝑀𝐷𝑃 . We further conjecture that the cells of the ar-
rangement cannot be partial, but they have to be entirely contained
in the value function polytope.

The learning dynamic of policy iteration in the value function
polytope shows that every policy update leads to an improvement of

 
 
 
 
 
 
KDD ’22, August 14–18, 2022, Washington, DC, USA

Yue Wu and Jesús A. De Loera

state values along one line segment of V. Based on this, we propose
a new algorithm, geometric policy iteration (GPI), a variant of
the classic policy iteration with several improvements. First, policy
iteration may perform multiple updates on the same line segment.
GPI avoids this situation by always reaching an endpoint of a line
segment in the value function polytope for every policy update.
This is achieved by efficiently calculating the true state value of each
potential policy update instead of using the Bellman operator which
only guarantees a value improvement. Second, GPI updates the
values for all states immediately after each policy update for a single
state, which makes the value function monotonically increasing
with respect to every policy update. Last but not least, GPI can be
implemented in an asynchronous fashion. This makes GPI more
flexible and advantageous over policy iteration in MDPs with a
very large state set.

(cid:16) | A |
1−𝛾 log 1
1−𝛾

We prove that GPI converges in O

(cid:17) iterations, which
matches the best known bound for solving finite discounted MDPs.
Although using a more complicated strategy for policy improve-
ment, GPI maintains the same O (cid:0)|S|2|A|(cid:1) arithmetic operations in
each iteration as policy iteration. We empirically demonstrate that
GPI takes fewer iterations and policy updates to attain the optimal
value.

1.1 Related Work
One line of work related to this paper is on the complexity of the
policy iteration. For MDPs with a fixed discount factor, the com-
plexity of policy iteration has been improved significantly [12, 18,
21, 24, 32]. There are also positive results reported on stochastic
games (SG). Hansen et al. [12] proved that a two-player turn-based
SG can be solved by policy iteration in strongly polynomial time
when the discount factor is fixed. Akian and Gaubert [1] further
proved that policy iteration is strongly polynomial in mean-payoff
SG with state-dependent discount factors under some restrictions.
In terms of more general settings, the worst-case complexity can
still be exponential [11, 13, 14, 19]. Another line of related work
studies the geometric properties of MDPs and RL algorithms. The
concept of the value function polytope in this paper was first pro-
posed in Dadashi et al. [8], which was also the first recent work
studying the geometry of the value function. Later, Bellemare et al.
[2] explored the direction of using these geometric structures as
auxiliary tasks in representation learning in deep RL. Dabney et al.
[7] also aimed at improving the representation learning by shaping
the policy improvement path within the value function polytope.
The geometric perspective of RL also contributes to unsupervised
skill learning where no reward function can be accessed [10]. Very
recently, Müller and Montufar [20] analyzed the geometry of state-
action frequencies in partially observable MDPs, and formulated
the problem of finding the optimal memoryless policy as a polyno-
mial program with a linear objective and polynomial constraints.
The geometry of the value function in robust MDP is also studied
in Wang et al. [29].

2 PRELIMINARIES
An MDP has five components M = ⟨S, A, R, P, 𝛾⟩ where S and A
are finite state set and action set, P : S×A → Δ(S) is the transition
function with Δ(·) denoting the probability simplex. R : S×A → R

is the reward function and 𝛾 = [0, 1) is the discount factor that
represents the value of time.

A policy 𝜋 : S → Δ(A) is a mapping from states to distribu-
tions over actions. The goal is to find a policy that maximizes the
cumulative sum of rewards.

Define 𝑉 𝜋 ∈ R |S | as the vector of state values. 𝑉 𝜋 (𝑠) is then the
expected cumulative reward starting from a particular state 𝑠 and
acting according to 𝜋:

𝑉 𝜋 (𝑠) = E𝑃 𝜋

∞
∑︁

(cid:16)

𝑖=0

𝛾𝑖 R (𝑠𝑖, 𝑎𝑖 ) | 𝑠0 = 𝑠

(cid:17)

.

The Bellman equation [3] connects the value 𝑉 𝜋 at a state 𝑠

with the value at the subsequent states when following 𝜋:

𝑉 𝜋 (𝑠) = E𝑃 𝜋

(cid:16)

R (𝑠, 𝑎) + 𝛾𝑉 𝜋 (𝑠 ′)

(cid:17)

.

(1)

Define 𝑟 𝜋 and 𝑃 𝜋 as follows.

𝑟 𝜋 (𝑠) =

𝑃 𝜋 (𝑠 ′ | 𝑠) =

∑︁

𝑎 ∈A
∑︁

𝑎 ∈A

𝜋 (𝑎 | 𝑠)R (𝑠, 𝑎),

𝜋 (𝑎 | 𝑠)P (𝑠 ′ | 𝑠, 𝑎),

Then, the Bellman equation for a policy 𝜋 can be expressed in
matrix form as follows.

𝑉 𝜋 = 𝑟 𝜋 + 𝛾𝑃 𝜋𝑉 𝜋

= (𝐼 − 𝛾𝑃 𝜋 )−1𝑟 𝜋 .
Under this notation, we can define the Bellman operator T 𝜋 and
the optimality Bellman operator T ∗ for an arbitrary value vector
𝑉 as follows.

(2)

T 𝜋𝑉 = 𝑟 𝜋 + 𝛾𝑃 𝜋𝑉 ,
T 𝜋𝑉 .
T ∗𝑉 = max

𝜋
𝑉 is optimal if and only if 𝑉 = T ∗𝑉 . MDPs can be solved by value
iteration (VI) [3] which consists of the repeated application of the
optimality Bellman operator 𝑉 (𝑘+1) := T ∗𝑉 (𝑘) until a fixed point
has been reached.

Let P (A) S denote the space of all policies, and V denote the
space of all state values. We define the value function 𝑓𝑣 (𝜋) :
P (A) S → V as

𝑓𝑣 (𝜋) = (𝐼 − 𝛾𝑃 𝜋 )−1𝑟 𝜋 .

(3)

The value function 𝑓𝑣 is fundamental to many algorithmic solu-
tions of an MDP. Policy iteration (PI) [15] repeatedly alternates
between a policy evaluation step and a policy improvement step
until convergence. In the policy evaluation step, the state values
𝑉 𝜋 of the current policy 𝜋 is evaluated which involves solving a
linear system (Eq. (2)). In the policy improvement step, PI iterates
over all states and update the policy by taking a greedy step using
the optimality Bellman operator as follows.

(cid:40)

(cid:41)

𝜋 ′(𝑠) ∈ argmax

R (𝑠, 𝑎) + 𝛾 ∑︁
𝑠′
Simple policy iteration (SPI) is a variant of policy iteration. It
only differs from policy iteration in the policy improvement step

P (𝑠 ′ | 𝑠, 𝑎)𝑉 𝜋 (𝑠 ′)

, ∀𝑠 ∈ S.

𝑎 ∈A

Geometric Policy Iteration for Markov Decision Processes

KDD ’22, August 14–18, 2022, Washington, DC, USA

where the policy is only updated for the state-action pair with the
largest improvement over the following advantage function.

˜𝐴(𝑠, 𝑎) = R (𝑠, 𝑎) + 𝛾 ∑︁
𝑠′

P (𝑠 ′ | 𝑠, 𝑎)𝑉 𝜋 (𝑠 ′) − 𝑉 𝜋 (𝑠).

SPI selects a state-action pair from argmax𝑠,𝑎
the policy accordingly.

˜𝐴(𝑠, 𝑎) then updates

2.1 Geometry of the Value Function
While the space of policies P (A) S is the Cartesian product of
|S| probability simplices, Dadashi et al. [8] proved that the value
function space is a possibly non-convex polytope [33]. Figure 1
shows a convex and a non-convex 𝑓𝑣 polytopes of 2 MDPs in blue
regions. The proof is built upon the line theorem which is an equally
important geometric property of the value space. The line theorem
depends on the following definition of policy determinism.

Definition 2.1 (Policy Determinism). A policy 𝜋 is

• 𝑠-deterministic for 𝑠 ∈ S if it selects one concrete action for

sure in state 𝑠, i.e., 𝜋 (𝑎|𝑠) ∈ {0, 1}, ∀𝑎;

• deterministic if it is 𝑠-deterministic for all 𝑠 ∈ S.

For both Figure 1a and 1b, we plot policies that agree on one
state to illustrate the line theorem. The policy determinism decides
if policies are mapped to a vertex, onto the boundary or inside the
polytope.

3 THE CELL STRUCTURE OF THE VALUE

FUNCTION POLYTOPE

In this section, we revisit the geometry of the (non-convex) value
function polytope presented in Dadashi et al. [8]. We establish a
connection to linear programming formulations of the MDP which
then can be adapted to show a finer description of cells in the value
function polytope as unions of cells of a hyperplane arrangement.
For more on hyperplane arrangements and their structure, see
Stanley [26].

It is known since at least the 1990’s that finding the optimal
value function of an MDP can be formulated as a linear program
(see for example [6, 22]). In the primal form, the feasible constraints
are defined by {𝑉 ∈ R |S | (cid:12)
(cid:12) 𝑉 ≽ T ∗𝑉 }, where T ∗ is the optimality
Bellman operator. Concretely, the following linear program is well-
known to be equivalent to maximizing the expected total reward
in Eq. (2). We call this convex polyhedron the MDP-LP polytope
(because it is a linear programming form of the MDP problem).

∑︁

𝑠

𝛼 (𝑠)𝑉 (𝑠)

min
𝑉
s.t. 𝑉 (𝑠) ≥ R (𝑠, 𝑎) + 𝛾 ∑︁
𝑠′

P (𝑠 ′ | 𝑠, 𝑎)𝑉 (𝑠 ′), ∀𝑠 ∈ S, 𝑎 ∈ A.

where 𝛼 is a probability distribution over S.

Our main new observation is that the MDP-LP polytope and the
value polytope are actually closely related, and one can describe
the regions of the (non-convex) value function polytope in terms
of the (convex) cells of the arrangement.

Theorem 2. Consider the hyperplane arrangement 𝐻𝑀𝐷𝑃 , with

|A||S| hyperplanes, consisting of those of the MDP polytope, i.e.,

𝐻𝑀𝐷𝑃 =

(cid:40)
𝑉 (𝑠) = R (𝑠, 𝑎) + 𝛾 ∑︁
𝑠′

P (𝑠 ′ | 𝑠, 𝑎)𝑉 (𝑠 ′) | ∀𝑠 ∈ S, 𝑎 ∈ A

(cid:41)

.

Then, the boundary of the value function polytope 𝜕V is the union of
finitely (convex polyhedral) cells of the arrangement 𝐻𝑀𝐷𝑃 . Moreover,
each full-dimensional cell of the value polytope is contained in the
union of finitely many full-dimensional cells defined by 𝐻𝑀𝐷𝑃 .

Proof. Let us first consider a point 𝑉 𝜋 being on the boundary of
the value function polytope. Theorem 2 and Corollary 3 of Dadashi
et al. [8] demonstrated that the boundary of the space of value func-
tions is a (possibly proper) subset of the ensemble of value functions
of policies, where at least one state has a fixed deterministic choice
for all actions. Note that from the value function Eq. (3), then the
hyperplane

𝑉 (𝑠) = R (𝑠, 𝑎𝑠

𝑙 ) + 𝛾 ∑︁

P (𝑠 ′ | 𝑠, 𝑎𝑠

𝑙 )𝑉 (𝑠 ′)

𝑠′

includes all policies taking policy 𝑎𝑠
𝑙 = 𝜋𝑙 (𝑠) in state 𝑠. Thus the
points of the boundary of the value function polytope are con-
tained in the hyperplanes of 𝐻𝑀𝐷𝑃 . Now we can see how the
𝑘-dimensional cells of the boundary are then in the intersections
of the hyperplanes too.

(a)

(b)

Figure 1: The blue regions are the value spaces of 2 MDPs
with |S| = 2 and |A| = 2. The regions are obtained by plot-
ting 𝑓𝑣 of 50, 000 random policies. (a): Both {𝜋𝑖 } and {𝛿𝑖 } agree
on 𝑠1 but differ in 𝑠2. 𝜋1 and 𝜋3 are deterministic. 𝜋2 is 𝑠1-
deterministic. 𝛿1 and 𝛿3 are 𝑠2-deterministic. (b): {𝜋𝑖 } and {𝛿𝑖 }
agree on 𝑠1 and 𝑠2, respectively. 𝜋1, 𝜋3, and 𝛿1 are determinis-
tic while 𝜋2 and 𝛿2 are 𝑠1 and 𝑠2-deterministic, respectively.

The line theorem captures the geometric property of a set of
policies that differ in only one state. Specifically, we say two policies
𝜋1, 𝜋2 agree on states 𝑠1, .., 𝑠𝑘 ∈ S if 𝜋1 (· | 𝑠𝑖 ) = 𝜋2 (· | 𝑠𝑖 ) for each 𝑠𝑖 ,
𝑖 = 1, . . . , 𝑘. For a given policy 𝜋, we denote by 𝑌 𝜋
𝑠1,...,𝑠𝑘 ⊆ P (A) S
the set of policies that agree with 𝜋 on 𝑠1, . . . , 𝑠𝑘 ; we will also write
𝑌 𝜋
to describe the set of policies that agree with 𝜋 on all states
S\{𝑠 }
except 𝑠. When we keep the probabilities fixed at all but state 𝑠, the
functional 𝑓𝑣 draws a line segment which is oriented in the positive
orthant (that is, one end dominates the other). Furthermore, the
endpoints of this line segment are 𝑠-deterministic policies.

The line theorem is stated as follows:

Theorem 1 (Line theorem [8]). Let 𝑠 be a state and 𝜋 a policy.
S\{𝑠 }, denoted 𝜋𝑙 , 𝜋𝑢 ,

Then there are two 𝑠-deterministic policies in 𝑌 𝜋
which bracket the value of all other policies 𝜋 ′ ∈ 𝑌 𝜋

S\{𝑠 }:

𝑓𝑣 (𝜋𝑙 ) ≼ 𝑓𝑣 (𝜋 ′) ≼ 𝑓𝑣 (𝜋𝑢 ).

KDD ’22, August 14–18, 2022, Washington, DC, USA

Yue Wu and Jesús A. De Loera

The zero-dimensional cells (vertices) are clearly a subset of the
zero-dimensional cells of the arrangement 𝐻𝑀𝐷𝑃 because, by above
results, the zero-dimensional cells are precisely in the intersection of
|S| many hyperplanes from 𝐻𝑀𝐷𝑃 , which is equivalent to choosing
a fixed set of actions for all states. This corresponds to solving a
linear system consisting of the hyperplanes that bound V (same
as Eq. (2)). But more generally, if we fix the policies for only 𝑘
states, the induced space lies in a |S| − 𝑘 dimensional affine space.
Consider a policy 𝜋 and 𝑘 states 𝑠1, . . . , 𝑠𝑘 , and write 𝐶𝜋
|S |
for the columns of the matrix (𝐼 − 𝛾𝑃 𝜋 )−1 corresponding to states
other than 𝑠1, . . . , 𝑠𝑘 . Define the affine vector space 𝐻 𝜋
𝐻 𝜋
𝑠1,...,𝑠𝑘 = 𝑉 𝜋 + 𝑆𝑝𝑎𝑛(𝐶𝜋

𝑘+1, . . . , 𝐶𝜋

, . . . , 𝐶𝜋

|S |).

𝑠1,..,𝑠𝑘

𝑘+1

Now For a given policy 𝜋, we denote by 𝑌 𝜋
𝑠1,...,𝑠𝑘 ⊆ P (A) S the
set of policies which agree with 𝜋 on 𝑠1, . . . , 𝑠𝑘 ; Thus the value
functions generated by 𝑌 𝜋
are contained in the affine vector
space 𝐻 𝜋

.

: 𝑓𝑣 (𝑌 𝜋
𝑠1,...,𝑠𝑘
The points of 𝐻 𝜋

𝑠1,..,𝑠𝑘
𝑠1,..,𝑠𝑘 ) = V ∩ 𝐻 𝜋
𝑠1,...,𝑠𝑘

𝑠1,..,𝑠𝑘

in one or more of the 𝐻𝑀𝐷𝑃 planes (each
hyperplane is precisely fixing one policy action pair). This is the
intersection of 𝑘 hyperplanes given by the following equations.

(cid:40)
𝑉 (𝑠) = R (𝑠, 𝑎) + 𝛾 ∑︁
𝑠′

P (𝑠 ′ | 𝑠, 𝑎)𝑉 (𝑠 ′) | ∀𝑠 ∈ {𝑠1, . . . , 𝑠𝑘 }, 𝑎 ∈ A

(cid:41)

.

Thus we can be sure of the stated containment.

Finally, the only remaining case is when 𝑉 𝜋 is in the interior of
the value polytope. If that is the case, because 𝐻𝑀𝐷𝑃 partitions the
entire Euclidean space, it must be contained in at least one of the
□
full-dimensional cell of 𝐻𝑀𝐷𝑃 .

(a)

(b)

Figure 2: (a): 𝑓𝑣 polytope (blue) and MDP-LP polytope (green)
of an MDP with |S| = 2 and |A| = 2. (b) 𝑓𝑣 polytope over-
lapped with the hyperplane arrangement 𝐻𝑀𝐷𝑃 from Theo-
rem 2. This MDP has 3 actions so |𝐻𝑀𝐷𝑃 | = 6.

Figure 2a is an example of the value function polytope in blue,
MDP-LP polytope in green and its bounding hyperplanes (the ar-
rangement 𝐻𝑀𝐷𝑃 ) as blue and red lines. In Figure 2b we exem-
plify Theorem 2 by presenting a value function polytope with
delimited boundaries where 𝐻𝑀𝐷𝑃 hyperplanes are indicated in
different colors. The deterministic policies are those for which
𝜋 (𝑎|𝑠) ∈ {0, 1} ∀𝑎 ∈ A, 𝑠 ∈ S. In both pictures, the values of de-
terministic policies in the value space are shown as red dots. The
boundaries of the value polytope are indeed included in the set
of cells of the arrangement 𝐻𝑀𝐷𝑃 as stated by Theorem 2. These
figures of value function polytopes (blue regions) were obtained by

randomly sampling policies and plotting their corresponding state
values.

Some remarks are in order. Note how sometimes the several
adjacent cells of the MDP arrangement together form a connected
cell of the value function polytope. We also observe that for any
set of states 𝑠1, .., 𝑠𝑘 ∈ S and a policy 𝜋, 𝑉 𝜋 can be expressed as a
convex combination of value functions of {𝑠1, .., 𝑠𝑘 }-deterministic
policies. In particular, V is included in the convex hull of the value
functions of deterministic policies. It is also demonstrated clearly
in Figure 2b that the value functions of deterministic policies are
not always vertices and the vertices of the value polytope are not
always value functions of deterministic policies, but they are always
intersections of hyperplanes on 𝐻𝑀𝐷𝑃 . However, optimal values
will always include a deterministic vertex. This observation sug-
gests that it would suffice to find the optimal policy by only visiting
deterministic policies on the boundary. It is worthwhile to note
that the optimal value of our MDP would be at the unique intersec-
tion vertex of the two polytopes. We note that the blue regions in
Figure 2a are not related to the polytope of the dual formulation
of LP. Unlike the MDP polytope which can be characterized as the
intersection of finitely many half-spaces, we do not have such a
neat representation for the value function polytope. The pictures
presented here and many more experiments we have done suggest
the following stronger result is true:

Conjecture: if the value polytope intersects a cell of the ar-
rangement 𝐻𝑀𝐷𝑃 , then it contains the entire cell, thus all full-
dimensional cells of the value function polytope are equal to the
union of full-dimensional cells of the arrangement.

Proving this conjecture requires showing that the map from
policies to value functions is surjective over the cells it touches.
At the moment we can only guarantee that there are no isolated
components because the value polytope is a compact set. More
strongly Dadashi et al. [8] shown (using the line theorem) that there
is path connectivity from 𝑉 𝜋 , in any cell, to others is guaranteed
by a polygonal path. More precisely if we let 𝑉 𝜋 and 𝑉 𝜋 ′ be two
value functions. Then there exists a sequence of 𝑘 ≤ |S| policies,
𝜋1, . . . , 𝜋𝑘 , such that 𝑉 𝜋 = 𝑉 𝜋1 , 𝑉 𝜋 ′
= 𝑉 𝜋𝑘 , and for every 𝑖 ∈
1, . . . , 𝑘 − 1, the set {𝑓𝑣 (𝛼𝜋𝑖 + (1 − 𝛼)𝜋𝑖+1) | 𝛼 ∈ [0, 1]} forms a line
segment.

It was observed that algorithms for solving MDPs have different
learning behavior when visualized in the value polytope space.
For example, policy gradient methods [16, 17, 28, 30, 31] have an
improvement path inside of the value function polytope; value
iteration can go outside of the polytope which means there can
be no corresponding policy during the update process; and policy
iteration navigates exactly through deterministic policies. In the
rest of our paper we use this geometric intuition to design a new
algorithm.

4 THE METHOD OF GEOMETRIC POLICY

ITERATION

We now present geometric policy iteration (GPI) that improves over
PI based on the geometric properties of the learning dynamics.
Define an action switch to be an update of policy 𝜋 in any state
𝑠 ∈ S. The Line theorem shows that policies agreeing on all but
one state lie on a line segment. So an action switch is a move along

Geometric Policy Iteration for Markov Decision Processes

KDD ’22, August 14–18, 2022, Washington, DC, USA

Similarly, we have 𝑟𝛿 = 𝑟 𝜋 + Δ𝑟 = 𝑟 𝜋 + Δ𝑟𝑎1𝑠 . Then, we have

(a)

(b)

Figure 3: The value sequences of one iteration which in-
volves a sweep over all states looking for policy updates. (a):
In PI, we may not reach the end of a line segment for an
action switch. (b): An endpoint is always reached in GPI.

a line segment to improve the value function. In PI, we use the
optimality Bellman operator T ∗𝑉 (𝑠) = max𝜋 (𝑟 𝜋 + 𝛾𝑃 𝜋𝑉 )(𝑠) to
decide the action to switch to for state 𝑠. However, T ∗𝑉 (𝑠) does
not guarantee the largest value improvement 𝑉 ∗ (𝑠) − 𝑉 (𝑠) for 𝑠.
This phenomenon is illustrated in Figure 3 where we plot the value
sequences of PI and the proposed GPI.

We propose an alternative action-switch strategy in GPI that di-
rectly calculates the improvement of the value function for one state.
By choosing the action with the largest value improvement, we
can always reach the endpoint of a line segment which potentially
reduces the number of action switches.

This strategy requires efficient computation of the value function
because a naive calculation of the value function by Eq. (2) is very
expensive due to the matrix inversion. On the other hand, PI only
re-evaluates the value function once per iteration. Our next theorem
states that the new state-value can be efficiently computed. This is
achieved by using the fact that the policy improvement step can be
done state-by-state within a sweep over the state set, so adjacent
policies in the update sequence only differ in one state.

Theorem 3. Given Q𝜋 = (𝐼 − 𝛾𝑃 𝜋 )−1

and 𝑉 𝜋 = Q𝜋𝑟 𝜋 . If a new
policy 𝛿 only differs from 𝜋 in state 𝑠 with 𝛿 (𝑠) = 𝑎 ≠ 𝜋 (𝑠), 𝑉 𝛿 (𝑠)
can be calculated efficiently by

(cid:18)

(cid:19)⊤

1𝑠 +

𝑉 𝛿 (𝑠) =

Q𝜋 (𝑠, 𝑠)
1 − w⊤
𝑎 q𝑠
where w𝑎 = 𝛾 (P (𝑠, 𝑎) − P (𝑠, 𝜋 (𝑠))) is a |S|-d vector, Δ𝑟𝑎 = R (𝑠, 𝑎)−
R (𝑠, 𝜋 (𝑠)) is a scalar, q𝑠 is the 𝑠th column of Q𝜋 , and 1𝑠 is a vector
with entry 𝑠 being 1, others being 0.

(cid:0)𝑉 𝜋 + Δ𝑟𝑎 q𝑠 (cid:1) ,

w𝑎

(4)

Proof. We here provide a general proof that we can calculate

𝑉 𝛿 given policy 𝜋, 𝑉 𝜋 , and 𝛿 differs from 𝜋 in only one state.

𝑉 𝛿 =

(cid:16)

𝐼 − 𝛾𝑃𝛿 (cid:17)−1

𝑟𝛿 = (cid:0)𝐼 − 𝛾𝑃 𝜋 − 𝛾 Δ𝑃 (cid:1)−1 𝑟𝛿,

where Δ𝑃 = 𝑃𝛿 − 𝑃 𝜋 . Assume 𝛿 and 𝜋 differ in state 𝑠. Δ𝑃 is a
rank-1 matrix with row 𝑗 being P (𝑠, 𝜋 (𝑠)) − P (𝑠, 𝑎), and all other
rows being zero vectors.

We can then express Δ𝑃 as the outer product of two vectors

Δ𝑃 = 1𝑠 w⊤

𝑎 , where 1𝑠 is a one-hot vector

1𝑠 (𝑖) =

(cid:40)1,
0,

if 𝑖 = 𝑠,
otherwise,

(5)

and w𝑎 is defined above.

𝑉 𝛿 =

(cid:16)

𝐼 − 𝛾𝑃𝛿 (cid:17)−1

(cid:18)

(cid:1)−1 (cid:0)𝑟 𝜋 + Δ𝑟𝑎 1𝑠 (cid:1)

𝑟𝛿
= (cid:0)𝐼 − 𝛾𝑃 𝜋 − 𝛾 Δ𝑃 (cid:1)−1 (cid:0)𝑟 𝜋 + Δ𝑟 (cid:1)
= (cid:0)𝐼 − 𝛾𝑃 𝜋 − 1𝑠 w⊤
𝑎
𝑎 Q𝜋
Q𝜋 1𝑠 w⊤
(cid:0)𝑟 𝜋 + Δ𝑟𝑎 1𝑠 (cid:1)
𝑎 Q𝜋 1𝑠
1 − w⊤
(cid:16)Sherman-Morrison, Q𝜋 = (𝐼 − 𝛾𝑃 𝜋 )−1(cid:17)
q𝑠 w⊤
𝑎
1 − w⊤
𝑎 q𝑠

(cid:0)𝑉 𝜋 + Δ𝑟𝑎 q𝑠 (cid:1) .
(here, Q𝜋 1𝑠 = q𝑠, Q𝜋𝑟 𝜋 = 𝑉 𝜋 )

𝜋 +

𝐼 +

Q

=

=

(cid:19)

(cid:18)

(cid:19)

Thus, for state 𝑠, we have

𝑉 𝛿 (𝑠) =

(cid:18)

1𝑠 +

Q𝜋 (𝑠, 𝑠)
1 − w⊤
𝑎 q𝑠

(cid:19)

w𝑎

(cid:0)𝑉 𝜋 + Δ𝑟𝑎 q𝑠 (cid:1) ,

which completes the proof.

□

Theorem 3 suggests that updating the value of a single state
using Eq. (4) takes O (|S||A|) arithmetic operations which matches
the complexity of the optimality Bellman operator used in policy
iteration.

(a)

(b)

Figure 4: Two paths are shown for each PI, GPI. The green
and red paths denote one iteration with 𝜋 (𝑠1) and 𝜋 (𝑠2) up-
dated first, respectively. (a): The policy improvement path
of PI. The red path is not action-switch-monotone which
will lead to an additional iteration. (b): GPI is always action-
switch-monotone. The red path achieves the optimal values
in one action switch.

The second improvement over policy iteration comes from the
fact that the value improvement path in V may not be monotonic
with respect to action switches. Although it is well-known that the
update sequence {𝑉 𝜋 (𝑘 )
} is non-decreasing in the iteration number
𝑘, the value function could decrease in the policy improvement
step of the policy iteration. An illustration is shown as the red path
of PI in Figure 4. The possible value decrease is because when the
Bellman operator T is used to decide an action switch, 𝑉 is fixed for
the entire sweep of states. This leads us to the motivation for GPI
which is to update the value function after each action switch such
that the value function action-switch-monotone. This idea can be
seamlessly combined with Theorem 3 since the values of all states
can be updated efficiently in O (cid:0)|S|2|A|(cid:1) arithmetic operations.
Thus, the complexity of completing one iteration is the same as
policy iteration.

KDD ’22, August 14–18, 2022, Washington, DC, USA

Yue Wu and Jesús A. De Loera

Algorithm 1 Geometric Policy Iteration
Input: P, R, 𝛾
1: set iteration number 𝑘 = 0 and randomly initialize 𝜋 (𝑘)
2: Calculate Q
3: for 𝑖 = 1, . . . , |S| do
4:

0 = (𝐼 − 𝛾𝑃 (𝑘)
(𝑘)

)−1 and 𝑉 (𝑘)

(𝑘)
= Q
0

𝑟 (𝑘)
0

0

0

0

6:

5:

calculate the best action 𝑎𝑖 according to Eq. (6)
(𝑘)
update Q
according to Eq. (9)
𝑖
𝜋 (𝑘)
(𝑖) = 𝑎𝑖
𝑖
𝑉 (𝑘)
(𝑘)
= Q
7:
𝑖
𝑖
is optimal then return 𝜋 (𝑘)
8: if 𝑉 (𝑘)
|S |
|S |
, 𝑉 (𝑘+1)
(𝑘+1)
= Q
0
0
step 3

, 𝜋 (𝑘+1)
0

= 𝑉 (𝑘)
|S |

= 𝜋 (𝑘)
|S |

𝑟 (𝑘)
𝑖

(𝑘)
|S |

9: Q

, 𝑘 = 𝑘 + 1. Go to

We summarize GPI in Algorithm 1. GPI looks for action switches
for all states in one iteration, and updates the value function after
each action switch. Let superscript 𝑘 denote the iteration index,
subscript 𝑖 denote the state index in one iteration. To avoid clutter,
we use 𝑖 to denote the state 𝑠𝑖 being updated and drop superscript
𝜋 in 𝑃 𝜋 and 𝑟 𝜋 . Step 2 evaluates the initial policy 𝜋 (𝑘)
. The differ-
(𝑘)
ence here is that we store the intermediate matrix Q
for later
0
computation. From step 3 to step 7, we iterate over all states to
search for potential updates. In step 4, GPI selects the best action
by computing the new state-value of each potential action switch
by Eq. (6).

0

𝑎𝑖 ∈ argmax

𝑎 ∈A

where

(cid:32)





(𝑘)
𝑖−1 (𝑖, 𝑖)
Q
1 − w⊤
𝑎 q𝑖

(cid:33)⊤

w𝑎

1𝑖 +

(cid:16)
𝑉 (𝑘)
𝑖−1 + Δ𝑟𝑎 q𝑖

w𝑎 = 𝛾
(cid:16)

Δ𝑟𝑎 =

(cid:16)

P (𝑖, 𝑎) − P (𝑖, 𝜋 (𝑘)

(cid:17)

,

R (𝑖, 𝑎) − R (𝑖, 𝜋 (𝑘)

𝑖−1 (𝑖))
(cid:17)

𝑖−1 (𝑖))

,

,

(6)

(cid:17) 



(7)

(8)

and 1𝑖 is a vector with 𝑖th entry being 1 and others being 0.

Define q𝑖 to be the 𝑖th column of Q

(𝑘)
𝑖−1. w⊤
𝑖

(𝑘)
using the selected action 𝑎𝑖 . In step 5, we update Q
𝑖

is obtained by Eq. (7)

(𝑘)
Q
𝑖

= Q

(𝑘)
𝑖−1 +

q𝑖 w⊤
𝑖 Q
1 − w⊤

(𝑘)
𝑖−1
𝑖 q𝑖

.

as follows.

(9)

Proof. Using Bellman equation, we have

𝑉 𝜋 ′

− 𝑉 𝜋 = 𝑟 𝜋 ′
Eq. (12) can be rearranged as

+ 𝛾𝑃 𝜋 ′𝑉 𝜋 ′

− 𝑟 𝜋 − 𝛾𝑃 𝜋𝑉 𝜋 .

(12)

𝑉 𝜋 ′

− 𝑉 𝜋 = 𝑟 𝜋 ′

− 𝑟 𝜋 + 𝛾𝑃 𝜋 ′ (cid:16)

𝑉 𝜋 ′

− 𝑉 𝜋 (cid:17)

+ 𝛾

(cid:16)

𝑃 𝜋 ′

− 𝑃 𝜋 (cid:17)

𝑉 𝜋 ,

and Eq. (10) follows.

To get Eq. (11), we rearrange Eq. (12) as
− 𝑉 𝜋 (cid:17)

− 𝑟 𝜋 + 𝛾𝑃 𝜋 (cid:16)

− 𝑉 𝜋 = 𝑟 𝜋 ′

𝑉 𝜋 ′

𝑉 𝜋 ′

and Eq. (11) follows.

+ 𝛾

(cid:16)

𝑃 𝜋 ′

− 𝑃 𝜋 (cid:17)

𝑉 𝜋 ′,

□

Our first result is an immediate consequence of re-evaluating

the value function after an action switch.

Proposition 1. The value function is non-decreasing with respect
𝑖+1 ≥ 𝑉 (𝑘)

to action switches in GPI, i.e., 𝑉 (𝑘)

.

𝑖

Proof. From Eq. (10) in Lemma 1, we have

𝑉 𝜋 ′

− 𝑉 𝜋 (cid:17)

= 𝑟 𝜋 ′

+ 𝛾𝑃 𝜋 ′𝑉 𝜋 − 𝑉 𝜋 .

(cid:16)

𝐼 − 𝛾𝑃 𝜋 ′ (cid:17) (cid:16)
Since 𝑃 𝜋 ≥ 0, we have
𝑉 𝜋 ′

− 𝑉 𝜋 ≥ 𝑟 𝜋 ′

+ 𝛾𝑃 𝜋 ′𝑉 𝜋 − 𝑉 𝜋 = T 𝜋 ′𝑉 𝜋 − 𝑉 𝜋 ,

which implies that for any 𝜋 ′, 𝜋,

𝑉 𝜋 ′
𝑖+1 and 𝜋 (𝑘)
Now, consider 𝜋 (𝑘)
𝑖
GPI, for state 𝑖 we have 𝑉 (𝑘)
𝑖+1 𝑉 (𝑘)
𝑖+1 ( 𝑗) ≥ T (𝑘)
𝑉 (𝑘)
𝑖
𝑖+1 ≥ 𝑉 (𝑘)
Combined, we have 𝑉 (𝑘)

𝑖

(13)

≥ T 𝜋 ′𝑉 𝜋 .
. According to the updating rule of
(𝑖). For state 𝑗 ≠ 𝑖, we have
𝑉 (𝑘)
𝑖

𝑖+1 (𝑖) ≥ 𝑉 (𝑘)
( 𝑗) = T (𝑘)

( 𝑗) = 𝑉 (𝑘)

( 𝑗).

𝑖

𝑖

𝑖

, which completes the proof. □

We next turn to the complexity of GPI and bound the number
of iterations required to find the optimal solution. The analysis
depends on the lemma described as follows.

Lemma 2. Let 𝑉 ∗ denote the optimal value. At iteration 𝑘 of GPI,

we have the following inequality.

(cid:16)
𝑉 ∗ − 𝑉 (𝑘)

𝑖

(cid:17)

(𝑖) ≤ 𝛾𝑃 ∗ (cid:16)

𝑉 ∗ − 𝑉 (𝑘−1)
𝑖

(cid:17)

(𝑖).

Proof. From Bellman equation, we have

The policy is updated in step 6 and the value vector is updated in
step 7 where 𝑟 (𝑘)
is the reward vector under the new policy. The
algorithm is terminated when the optimal values are achieved.

𝑖

𝑉 ∗ − 𝑉 (𝑘)

𝑖

= T ∗𝑉 ∗ − 𝑉 (𝑘)
= T ∗𝑉 ∗ − T ∗𝑉 (𝑘−1)

𝑖

𝑖

+ T ∗𝑉 (𝑘−1)
𝑖

− 𝑉 (𝑘)
𝑖

.

4.1 Theoretical Guarantees
Before we present any properties of GPI, let us first prove the
following very useful lemma.

Lemma 1. Given two policies 𝜋 and 𝜋 ′, we have the following

equalities.

𝑉 𝜋 ′

𝑉 𝜋 ′

(cid:16)

𝐼 − 𝛾𝑃 𝜋 ′ (cid:17)−1 (cid:16)
− 𝑉 𝜋 =
− 𝑉 𝜋 = (cid:0)𝐼 − 𝛾𝑃 𝜋 (cid:1)−1 (cid:16)

𝑟 𝜋 ′

𝑉 𝜋 ′

+ 𝛾𝑃 𝜋 ′𝑉 𝜋 − 𝑉 𝜋 (cid:17)
− 𝑟 𝜋 − 𝛾𝑃 𝜋𝑉 𝜋 ′ (cid:17)

.

,

(10)

(11)

For state 𝑖, we have

(cid:17)

(𝑖)

(cid:16)
𝑉 ∗ − 𝑉 (𝑘)
= 𝛾𝑃 ∗ (cid:16)
≤ 𝛾𝑃 ∗ (cid:16)
≤ 𝛾𝑃 ∗ (cid:16)
= 𝛾𝑃 ∗ (cid:16)

𝑖
𝑉 ∗ − 𝑉 (𝑘−1)
𝑖
𝑉 ∗ − 𝑉 (𝑘−1)
𝑖
𝑉 ∗ − 𝑉 (𝑘−1)
𝑖
𝑉 ∗ − 𝑉 (𝑘−1)
𝑖

(cid:17)

(cid:17)

(cid:17)

(cid:17)

(𝑖) +

(𝑖) +

(𝑖) +

(𝑖).

(cid:17)

(𝑖)

− 𝑉 (𝑘)
𝑖

(cid:16)

T ∗𝑉 (𝑘−1)
𝑖
(cid:16)max
𝜋
(cid:16)
𝑉 (𝑘)
𝑖

− 𝑉 (𝑘)
𝑖
T 𝜋𝑉 (𝑘−1)
𝑖
− 𝑉 (𝑘)
𝑖

(𝑖)

(cid:17)

(14)

(15)

(cid:17)

(𝑖)

Geometric Policy Iteration for Markov Decision Processes

KDD ’22, August 14–18, 2022, Washington, DC, USA

Let 𝜋 ′ = argmax𝜋 T 𝜋𝑉 (𝑘−1)
updating rule of GPI and (13),
≥ 𝑉 𝜋 ′

𝑉 (𝑘)
𝑖

𝑖

𝑖 ≥ T 𝜋 ′𝑉 (𝑘−1)

𝑖

,

. The inequality (15) is because of the

which completes the proof.

Theorem 4. GPI finds the optimal policy in O

erations.

□

(cid:16) | A |
1−𝛾 log 1
1−𝛾

(cid:17)

it-

Proof. Define Δ∗

𝑘 ∈ R |S | with Δ∗

𝑘 (𝑠) = 𝑉 ∗ (𝑠) −𝑉 (𝑘)

𝑖

(𝑠), ∀𝑠 ∈ S.

Then, by Lemma 2, we have

.

(cid:13)
(cid:13)Δ∗
𝑘

𝑘 ≤ 𝛾𝑃 ∗Δ∗
Δ∗
𝑘−1,
(cid:13)
(cid:13)∞ ≤ 𝛾𝑘 (cid:13)
(cid:13)
(cid:13)Δ∗
(cid:13)∞
0
(cid:13)
0 ( 𝑗) = (cid:13)
Let 𝑗 be the state such that Δ∗
(cid:13)Δ∗
(cid:13)∞
0
can be obtained by Eq. (11) in Lemma 1.
(cid:13)
(cid:13)∞ ≤ 𝛾𝑘 (cid:13)
(cid:13)
(cid:13)Δ∗
(cid:13)∞
0
(cid:13)
(cid:16)
𝐼 − 𝛾𝑃 (0)
(cid:13)
≤ 𝛾𝑘
(cid:13)
(cid:13)
𝛾𝑘
1 − 𝛾

(cid:16)
𝑉 ∗ − T (0)

(cid:13)
(cid:13)Δ∗
𝑘

=

𝑗

𝑗 𝑉 ∗(cid:13)
(cid:13)
(cid:13)∞

(cid:17)−1(cid:13)
(cid:13)
𝑉 ∗ − T (0)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
𝑗 𝑉 ∗(cid:17)

( 𝑗).

, the following properties

Also from Eq. (11), we have
(cid:16)
𝑉 ∗(cid:17)
𝑉 ∗ − T (𝑘)

𝑗

It follows that

( 𝑗) ≤ Δ∗

𝑘 ( 𝑗) ≤ (cid:13)

(cid:13)Δ∗
𝑘

(cid:13)
(cid:13)∞

.

(16)

(cid:16)
𝑉 ∗ − T (𝑘)

𝑗

𝑉 ∗(cid:17)

( 𝑗) ≤

𝛾𝑘
1 − 𝛾

(cid:16)
𝑉 ∗ − T (0)

𝑗 𝑉 ∗(cid:17)

( 𝑗),

which implies when 𝛾𝑘
1−𝛾 < 1, the non-optimal action for 𝑗 in 𝜋 (0)
is switched in 𝜋 (𝑘) and will never be switched back to in future
iterations. Now we are ready to bound 𝑘. By taking the logarithm
for both sides of 𝛾𝑘

1−𝛾 < 1, we have

𝑘 log 𝛾 ≥ log(1 − 𝛾)

𝑘 >

log(1 − 𝛾)
log 𝛾

=

log 1
1−𝛾
log 1
𝛾
(cid:32)

𝑘 >

1
1 − 𝛾

log

1
1 − 𝛾

log

1
𝛾

≥

1
𝛾 − 1
1
𝛾

(cid:33)

= 1 − 𝛾

.

Each non-optimal action is eliminated after at most O
iterations, and there are O (|A|) non-optimal actions. Thus, GPI
(cid:17) iterations to reach the optimal pol-
takes at most O
□
icy.

(cid:16) | A |
1−𝛾 log 1
1−𝛾

(cid:16) 1
1−𝛾 log 1
1−𝛾

(cid:17)

4.2 Asynchronous Geometric Policy Iteration
When the state set is large it would be beneficial to perform policy
updates in an orderless way [27]. This is because iterating over the
entire state set may be prohibitive, and exactly evaluating the value
function with Eq. (2) may be too expensive. Thus, in practice, the
value function is often approximated when the state set is large.
One example is modified policy iteration [22, 23] where the policy
evaluation step is approximated with certain steps of value iteration.

Since GPI avoids the matrix inversion by updating the value
function incrementally, it has the potential to update the policy for
arbitrary states available to the agent. This property also opens
up the possibility of asynchronous (orderless) updates of policies
and the value function when the state set is large or the agent has
to update the policy for the state it encounters in real-time. The
asynchronous update strategy can also help avoid being stuck in
states that lead to minimal progress and may reach the optimal
policy without reaching a certain set of states.

Asynchronous GPI (Async-GPI) follows the action selection
mechanism of GPI, and its general framework is as follows. As-
sume the transition matrix is available to the agent, we randomly
initialize the policy 𝜋 (0) and calculate the initial Q(0) and 𝑉 (0) ac-
cordingly. In real-time settings, the sequence of states {𝑠0, 𝑠1, 𝑠2, . . .}
are collected by an agent through real-time interaction with the
environment. At time step 𝑡, we search for an action switch for state
𝑠𝑡 using Eq. (6). Then, we update the 𝜋 (𝑡 ) , Q(𝑡 ) with Eq. (9), and
𝑉 (𝑡 ) . Asynchronous value-based methods converge if each state
is visited infinitely often [5]. We later demonstrate in experiments
that Async-GPI converges well in practice.

5 EXPERIMENTS
We test GPI on random MDPs of different sizes. The baselines are
policy iteration (PI) and simple policy iteration (SPI). We compare
the number of iterations, actions switches, and wall time. Here we
denote the number of iterations as the number of sweeps over the
entire state set. Action switches are those policy updates within
each iteration. The results are shown in Figure 5. We generate MDPs
with |S| = {100, 200, 300, 500, 1000} corresponding to Figure 5 (a)-
(e). For each state size, we increase the number of actions (horizontal
axes) to observe the difference in performance. The rows from the
top to bottom are the number of iterations, action switches and
wall time (vertical axes), respectively. Since SPI only performs one
action switch per iteration, we only show its number of action
switches. The purpose of adding SPI to the baseline is to verify
if our GPI can effectively reduce the number of action switches.
Since SPI sweeps over the entire state set and updates a single
state with the largest improvement, it is supposed to have the least
number of action switches. However, SPI’s larger complexity of
performing one update should lead to higher running time. This
is supported by the experiments as Figure 5 (a) and (b) show that
SPI (green curves) takes the least number of switches and longest
time. We drop SPI in Figure 5 (c)-(e) to have a clearer comparison
between GPI and PI (especially in wall time). The proposed GPI
has a clear advantage over PI in almost all tests. The second row
of Figure 5 (a) and (b) shows that the number of action switches of
GPI is significantly fewer than PI and very close to SPI although
the complexity of a switch is cheaper by a factor of |S|. And the
reduction in the number of action switches leads to fewer iterations.
Another important observation is that the margin increases as the
action set becomes larger. This is strong empirical evidence that
demonstrates the benefits of GPI’s action selection strategy which
is to reach the endpoints of line segments in the value function
polytope. The larger the action set is, the more policies lying on
the line segments and thus the more actions being excluded in one
switch. The wall time of GPI is also very competitive compared

KDD ’22, August 14–18, 2022, Washington, DC, USA

Yue Wu and Jesús A. De Loera

(a)

(b)

(c)

(d)

(e)

Figure 5: The results of MDPs with {100, 200, 300, 500, 1000} states in (a)-(e). The horizontal axes are the number of actions for
all graphs. The vertical axes are the number of iterations, number of action switches, and wall time for the first to the third
row, respectively. The performance curves of SPI, PI, and GPI are in green, blue, and red, respectively. The SPI curves are
only presented in (a) and (b) to provide a “lower bound" on the number of action switches, and are dropped for larger MDPs
due to its higher running time. The number of switches of GPI remains low compared to PI. The proposed GPI consistently
outperforms PI in both iteration count and wall time. The advantages of GPI become more significant as the action set size
grows.

(a)

(b)

(c)

(d)

Figure 6: Comparison between asynchronous geometric policy iteration (red curve) and asynchronous value iteration (blue
curve) in 4 MDPs. |A| = 100 for all MDPs and |S| = {300, 500, 1000, 2000} for (a)-(d), respectively. The horizontal axes are the
number of updates. The vertical axes show the mean of the value function.

to PI which further demonstrates that GPI can be a very practical
algorithm for solving finite discounted MDPs.

We also test the performance of the asynchronous GPI (Async-
GPI) on MDPs with |S| = {300, 500, 1000, 2000} and |A| = 100. For

each setting, we randomly generate a sequence of states that is
larger than |S|. We compare Async-GPI with asynchronous value
iteration (Async-VI) which is classic asynchronous dynamic pro-
gramming algorithm. At time step 𝑡, Async-VI performs one step of

Geometric Policy Iteration for Markov Decision Processes

KDD ’22, August 14–18, 2022, Washington, DC, USA

the optimality Bellman operator on a single state 𝑠𝑡 that is available
to the algorithm. The results are shown in Figure 6. The mean of
the value function is plotted against the number of updates. We ob-
serve that Async-GPI took significantly fewer updates to reach the
optimal value function. The gap becomes larger when the state set
grows in size. These results are expected because Async-GPI also
has a higher complexity to perform an update and Async-VI never
really solves the real value function before reaching the optimality.

6 CONCLUSIONS AND FUTURE WORK
In this paper, we discussed the geometric properties of finite MDPs.
We characterized the hyperplane arrangement that includes the
boundary of the value function polytope, and further related it to
the MDP-LP polytope by showing that they share the same hyper-
plane arrangement. Unlike the well-defined MDP-LP polytope, it
remains unclear which bounding hyperplanes are active and which
halfspaces of them belong to the value space. Besides the conjecture
stated earlier, we would like to understand in the future which cells
of the hyperplane arrangement form the value function polytope,
and may derive a bound on the number of convex cells. It is also
plausible that the rest of the hyperplane arrangement will help us
devise new algorithms for solving MDPs.

Following the fact that policies that differ in only one state are
mapped onto a line segment in the value function polytope, and
that the only two policies on the polytope boundary are determin-
istic in that state, we proposed a new algorithm called geometric
policy iteration that guarantees to reach an endpoint of the line
segment for every action switch. We developed a mechanism that
makes the value function monotonically increase with respect to
action switches and the whole process can be computed efficiently.
Our experiments showed that our algorithm is very competitive
compared to the widely-used policy iteration and value iteration.
We believe this type of algorithm can be extended to multi-agent
settings, e.g., stochastic games [25]. It will also be interesting to
apply similar ideas to model-based reinforcement learning.

ACKNOWLEDGMENTS
This work is supported by NSF DMS award 1818969 and a seed
award from Center for Data Science and Artificial Intelligence
Research at UC Davis.

REFERENCES
[1] Marianne Akian and Stéphane Gaubert. 2013. Policy iteration for perfect infor-
mation stochastic mean payoff games with bounded first return times is strongly
polynomial. arXiv: Optimization and Control (2013).

[2] Marc G. Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel
Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. 2019. A
Geometric Perspective on Optimal Representations for Reinforcement Learning.
In Proceedings of the 33rd International Conference on Neural Information Processing
Systems. Article 392.

[3] Richard E. Bellman. 1957. Dynamic Programming. Princeton University Press.

392 pages.

[4] Dimitri P. Bertsekas. 1987. Dynamic Programming: Deterministic and Stochastic

Models. Prentice-Hall, Inc.

[5] Dimitri P. Bertsekas and John N. Tsitsiklis. 1989. Convergence Rate and Termina-
tion of Asynchronous Iterative Algorithms. In Proceedings of the 3rd International
Conference on Supercomputing. Association for Computing Machinery, 461–470.

[6] Dimitri P. Bertsekas and John N. Tsitsiklis. 1996. Neuro-dynamic programming.

Optimization and neural computation series, Vol. 3. Athena Scientific.

[7] Will Dabney, André Barreto, Mark Rowland, Robert Dadashi, John Quan, Marc
G. Bellemare, and David Silver. 2021. The Value-Improvement Path: Towards
Better Representations for Reinforcement Learning. Proceedings of the AAAI
Conference on Artificial Intelligence 35, 8, 7160–7168.

[8] Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G.
Bellemare. 2019. The Value Function Polytope in Reinforcement Learning. In
Proceedings of the 36th International Conference on Machine Learning (Proceedings
of Machine Learning Research), Vol. 97. PMLR, 1486–1495.

[9] George B. Dantzig. 1963. Linear programming and extensions. Princeton Univ.

Press.

[10] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. 2021. The Infor-
mation Geometry of Unsupervised Reinforcement Learning. CoRR abs/2110.02719
(2021).

[11] John Fearnley. 2010. Exponential Lower Bounds for Policy Iteration. In Automata,

Languages and Programming. 551–562.

[12] Thomas D. Hansen, Peter B. Miltersen, and Uri Zwick. 2013. Strategy Iteration Is
Strongly Polynomial for 2-Player Turn-Based Stochastic Games with a Constant
Discount Factor. J. ACM 60, 1, Article 1 (feb 2013).

[13] Romain Hollanders, Jean-Charles Delvenne, and Raphaël M. Jungers. 2012. The
complexity of Policy Iteration is exponential for discounted Markov Decision
Processes. In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC).
5997–6002.

[14] Romain Hollanders, Balázs Gerencsér, Jean-Charles Delvenne, and Raphaël M.
Jungers. 2016. Improved bound on the worst case complexity of Policy Iteration.
Operations Research Letters 44, 2 (2016), 267–272.

[15] Ronald A. Howard. 1960. Dynamic Programming and Markov Processes. MIT

Press.

[16] Sham M Kakade. 2002. A Natural Policy Gradient. In Proceedings of the 14th
International Conference on Neural Information Processing Systems, T. Dietterich,
S. Becker, and Z. Ghahramani (Eds.), Vol. 14. MIT Press, 1531–1538.

[17] Vijay Konda and John Tsitsiklis. 1999. Actor-Critic Algorithms. In Proceedings
of the 12th International Conference on Neural Information Processing Systems,
S. Solla, T. Leen, and K. Müller (Eds.). MIT Press, 1008–1014.

[18] Michael L. Littman, Thomas L. Dean, and Leslie Pack Kaelbling. 2013. On the
Complexity of Solving Markov Decision Problems. CoRR abs/1302.4971 (2013).
[19] Yishay Mansour and Satinder Singh. 1999. On the Complexity of Policy Iteration.
In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence.
401–408.

[20] Johannes Müller and Guido Montufar. 2022. The Geometry of Memoryless
Stochastic Policy Optimization in Infinite-Horizon POMDPs. In International
Conference on Learning Representations.

[21] Ian Post and Yinyu Ye. 2012. The simplex method is strongly polynomial for

deterministic Markov decision processes. CoRR abs/1208.5083 (2012).

[22] Martin L. Puterman. 1994. Markov Decision Processes: Discrete Stochastic Dynamic

Programming. John Wiley & Sons, Inc.

[23] Martin L. Puterman and Moon Chirl Shin. 1978. Modified Policy Iteration Algo-
rithms for Discounted Markov Decision Problems. Management Science 24, 11
(1978), 1127–1137.

[24] Bruno Scherrer. 2013. Improved and Generalized Upper Bounds on the Com-
plexity of Policy Iteration. In Proceedings of the 26th International Conference on
Neural Information Processing Systems. 386–394.

[25] Lloyd S. Shapley. 1953. Stochastic Games. Proceedings of the National Academy

of Sciences 39, 10 (1953), 1095–1100.

[26] Richard P. Stanley. 2007. An introduction to hyperplane arrangements.

In
Geometric combinatorics. IAS/Park City Math. Ser., Vol. 13. Amer. Math. Soc.,
Providence, RI, 389–496. https://doi.org/10.1090/pcms/013/08

[27] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Intro-

duction (second ed.). The MIT Press.

[28] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999.
Policy Gradient Methods for Reinforcement Learning with Function Approxi-
mation. In Proceedings of the 12th International Conference on Neural Information
Processing Systems. MIT Press, 1057–1063.

[29] Kaixin Wang, Navdeep Kumar, Kuangqi Zhou, Bryan Hooi, Jiashi Feng, and Shie
Mannor. 2022. The Geometry of Robust Value Functions. CoRR abs/2201.12929
(2022).

[30] Ronald Williams and Jing Peng. 1991. Function Optimization Using Connectionist

Reinforcement Learning Algorithms. Connection Science 3 (09 1991), 241–268.

[31] Ronald J. Williams. 1992. Simple Statistical Gradient-Following Algorithms for
Connectionist Reinforcement Learning. Mach. Learn. 8, 3–4 (may 1992), 229–256.
[32] Yinyu Ye. 2011. The Simplex and Policy-Iteration Methods Are Strongly Polyno-
mial for the Markov Decision Problem with a Fixed Discount Rate. Math. Oper.
Res. 36, 4 (nov 2011), 593–603.

[33] G.M. Ziegler. 2012. Lectures on Polytopes. Springer New York.

