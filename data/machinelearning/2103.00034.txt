Beyond Perturbation Stability: LP Recovery Guarantees for MAP
Inference on Noisy Stable Instances

1
2
0
2

b
e
F
6
2

]
L
M

.
t
a
t
s
[

1
v
4
3
0
0
0
.
3
0
1
2
:
v
i
X
r
a

Hunter Lang∗
MIT
hjl@mit.edu

Aravind Reddy∗
Northwestern University
arareddy@u.northwestern.edu

David Sontag
MIT
dsontag@mit.edu

Aravindan Vijayaraghavan
Northwestern University
aravindv@northwestern.edu

Abstract

Several works have shown that perturbation
stable instances of the MAP inference problem
in Potts models can be solved exactly using a
natural linear programming (LP) relaxation.
However, most of these works give few (or no)
guarantees for the LP solutions on instances
that do not satisfy the relatively strict pertur-
bation stability deﬁnitions. In this work, we
go beyond these stability results by showing
that the LP approximately recovers the MAP
solution of a stable instance even after the
instance is corrupted by noise. This “noisy
stable” model realistically ﬁts with practical
MAP inference problems: we design an algo-
rithm for ﬁnding “close” stable instances, and
show that several real-world instances from
computer vision have nearby instances that
are perturbation stable. These results suggest
a new theoretical explanation for the excellent
performance of this LP relaxation in practice.

1 Introduction

In this work, we study the MAP inference problem in
the ferromagnetic Potts model, which is also known as
uniform metric labeling (Kleinberg & Tardos, 2002).
Given a graph G = (V, E), this problem is:

minimize
x:V →[k]

X

c(u, x(u)) + X

w(u, v)1[x(u) 6= x(v)].

u∈V

(u,v)∈E

Here we are optimizing over labelings x : V → [k]
where [k] = {1, 2, . . . , k}. The objective is comprised
of “node costs” c : V × [k] → R, and “edge weights”
w : E → R>0; a labeling x pays the cost c(u, i) when
it labels node u with label i and pays w(u, v) on edge

∗Denotes equal contribution.

(u, v) when it labels u and v diﬀerently. This problem
is NP-hard for variable k ≥ 3 (Kleinberg & Tardos,
2002) even when the graph G is planar (Dahlhaus et al.,
1992). However, there are several eﬃcient and empiri-
cally successful approximation algorithms for the MAP
inference problem—such as TRW (Wainwright et al.,
2005) and MPLP (Globerson & Jaakkola, 2008)—that
are related in some way to the local LP relaxation,
which is also sometimes called the pairwise LP (Wain-
wright & Jordan, 2008; Chekuri et al., 2001). This
LP relaxation returns an approximate MAP solution
for most problem instances. However, when the pa-
rameters of these models are learned so as to enable
good structured prediction, often the LP relaxation
exactly or almost exactly recovers the MAP solution
(Meshi et al., 2019). The connection between the LP
relaxation and commonly used approximate MAP infer-
ence algorithms then leads to the following compelling
question, which is of great practical relevance for under-
standing the “tightness” of the LP solution (informally,
how close the LP solution is to the MAP solution).

Can we explain the exceptional performance of the lo-
cal LP relaxation in recovering the MAP solution in
practice?

Several works have studied diﬀerent conditions that im-
ply the local relaxation or related relaxations are tight
(e.g., Kolmogorov & Wainwright, 2005; Wainwright &
Jordan, 2008; Thapper et al., 2012; Weller et al., 2016;
Rowland et al., 2017). Recent work on tightness of the
local relaxation has focused on a class of several related
conditions known as perturbation stability. Intuitively,
an instance is perturbation stable if the solution x∗ to
the MAP inference problem is unique, and moreover,
x∗ is the unique solution even when the edge weights w
are multiplicatively perturbed by a certain adversarial
amount (Bilu & Linial, 2010). This structural assump-
tion about the instance (G, c, w) captures the intuition
that, on “real-world” instances, the ground-truth so-
lution is stable and does not change much when the
weights are slightly perturbed.

For constants β, γ ≥ 1, we say that w0 is a (β, γ)-

 
 
 
 
 
 
Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Figure 1: Left: prior work (Lang et al., 2018) showed that a stable instance can be exactly solved eﬃciently.
Colors indicate the label of each vertex in the MAP solution x∗. On stable instances, solving the LP relaxation
(represented by the arrow) recovers the MAP solution. However, real-world instances are not suitably stable for
this result to apply in practice (Lang et al., 2019). Right: in this work, we show that solving the LP relaxation
on a (slightly) corrupted stable instance (corruptions shown as bold edges) approximately recovers the original
MAP solution. This is true even if the corruption changes the MAP solution (as in the bottom example). In
other words, we prove that “easy” instances are still approximately easy even after small perturbations.

perturbation of the weights w if 1
β ·w(u, v) ≤ w0(u, v) ≤
γ · w(u, v) for all (u, v) ∈ E. Suppose x∗ is the unique
MAP solution to the instance (G, c, w). Then, we
say (G, c, w) is a (β, γ)-stable instance if x∗ is also
the unique MAP solution to every instance (G, c, w0)
where w0 is a (β, γ)-perturbation of w. Lang et al.
(2018) showed that when (G, c, w) is (2, 1)-stable, the
solution to the local LP relaxation is persistent i.e., the
LP solution exactly recovers the MAP solution x∗.

While theoretically interesting, (2, 1)-stability is a strict
condition that is unlikely to be satisﬁed in practice:
the solution x∗ is not allowed to change at all when the
weights are perturbed. No real-world instances have
yet been shown to be (2, 1)-stable (Lang et al., 2019).
Moreover, the LP relaxation is also not persistent on
most of those instances. However, the solution of the
local LP relaxation is still nearly persistent i.e., the
LP solution is very close to the MAP solution x∗ (see
Deﬁnition 3.1 for a formal deﬁnition). Those examples
made it clear that theory must go beyond perturbation
stability to explain this phenomenon of near-persistence
that is prevalent in practice (see e.g., Sontag, 2010;
Shekhovtsov et al., 2017).

Why is the LP relaxation nearly persistent on MAP
inference instances in practice?

There are several theoretical frameworks to explain
exactness or tightness of LP relaxations, such as total
unimodularity, submodularity (Kolmogorov & Wain-
wright, 2005), and perturbation stability (Lang et al.,
2018, 2019), as well as structural assumptions about

the graph G (Wainwright & Jordan, 2008), or combined
assumptions about G and the form of the objective
function (Weller et al., 2016; Rowland et al., 2017).
However, these frameworks can not be used to prove
near-persistence.

Figure 1 (informally) shows our main result. The left
side depicts the previous result of Lang et al. (2018): if
the instance is (2, 1)-stable (a fairly strong structural
assumption), the LP relaxation exactly recovers the full
solution x∗. This result is limited because real-world
instances have been shown to not satisfy (2, 1)-stability
(Lang et al., 2019). The right side shows our main
result:
if the instance is a slightly corrupted (2, 1)-
stable instance, the LP relaxation still approximately
recovers the solution x∗ to the stable instance.

Intuitively, we may expect a real-world instance to be
“close” to a stable instance (i.e., to be a “corrupted sta-
ble” instance, as in Figure 1) even if the instance itself
is not stable. We design an algorithm to check whether
this is the case. We ﬁnd that on several real examples,
sparse and small-norm changes to the instance make it
appropriately stable for our theorems to apply. In other
words, we certify that these real instances are close to
stable instances. For these instances, our theoretical
results explain why the LP relaxation approximately
recovers the MAP solution.

More formally, we assume that there is some latent
stable instance (G, ¯c, ¯w), and that the observed instance
(G, ˆc, ˆw) is a noisy version of (G, ¯c, ¯w) that is close to it.
Let ˆx be the solution to the local LP on the observed

LPLPLPcorrupted stable(MAP solution)LP solution (approximate recovery)LPstable instance(MAP solution)LP solution(exact recovery)labelscorruptionsLP fails to recover true labelHunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

instance (G, ˆc, ˆw), and let ¯x be the (unknown) MAP
solution on the unseen stable instance (G, ¯c, ¯w). We
prove that under certain conditions, the LP solution
ˆx is nearly persistent i.e., the Hamming error kˆx − ¯xk1
is small (see Deﬁnition 3.1). In other words, the local
LP solution to the observed instance approximately
recovers the latent integral solution ¯x.

We complement this by studying a natural generative
model that generates noisy stable instances which, with
high probability, satisfy the above conditions for near
persistency.
In other words, the observed instance
(G, ˆc, ˆw) is obtained by random perturbations to the
latent stable instance (G, ¯c, ¯w), and the LP relaxation
approximately recovers the MAP solution to the latent
instance with high probability.

Our theoretical results imply that the local LP approx-
imately recovers the MAP solution when the observed
instance is close to a stable instance. Our empirical re-
sults suggest that real-world instances are very close to
stable instances. These results together suggest a new
explanation for the near-persistency of the solution of
the local LP relaxation for MAP inference in practice.
To prove these results and derive our algorithm for
ﬁnding a “close-by” stable instance, we make several
novel technical contributions, which we outline below.

Technical contributions.

• In Section 4, we generalize the (2, 1)-stability result
of Lang et al. (2018) to work under a much weaker
assumption, which we call (2, 1)-expansion stabil-
ity. That is, we prove the local LP is tight on (2, 1)-
expansion stable instances. Additionally, given the
instance’s MAP solution, (2, 1)-expansion stabil-
ity is eﬃciently checkable. To the best of our
knowledge, most other perturbation stability as-
sumptions are not known to satisfy this desirable
property. This generalization is crucial for the eﬃ-
ciency of our algorithm for ﬁnding stable instances
that are close to a given observed instance.

• In Section 5, we give a simple extension of (2, 1)-
expansion stability called (2, 1, ψ)-expansion sta-
bility. We prove it implies a “curvature” result
around the MAP solution ¯x. On instances that
satisfy this condition, if a labeling ˆx is close in
objective value to ¯x, it must also be close in the so-
lution space. This result lets us translate between
objective gap and Hamming distance.

latent and observed instances. The proof uses a
rounding algorithm for metric labeling in a novel
way to obtain stronger guarantees. When com-
bined with our other results, this proves that when
the latent instance is (2, 1, ψ)-expansion stable, the
LP solution is nearly persistent on the observed
instance with high probability. These results sug-
gest a theoretical explanation for the phenomenon
of near-persistence of the LP solution in practice.

• We design an eﬃcient algorithm for ﬁnding
(2, 1, ψ)-expansion stable instances that are “close”
to a given instance (G, ˆc, ˆw) in Section 7. To the
best of our knowledge, this is the ﬁrst algorithm for
ﬁnding close-by stable instances, and is also an eﬃ-
cient algorithm for checking (2, 1, ψ)-expansion sta-
bility. This algorithm allows us to check whether
real-world instances can plausibly be considered
“corrupted stable” instances as shown in Figure 1.

• We run our algorithm on several real-world in-
stances of MAP inference in Section 8, and ﬁnd
that the observed instances (G, ˆc, ˆw) often admit
close-by (2, 1, ψ)-stable instances (G, ¯c, ¯w). More-
over, we ﬁnd that the local LP solution ˆx typically
has very close objective to ¯x in (G, ¯c, ¯w). Our
curvature result for (2, 1, ψ)-stable instances thus
gives an explanation for the tightness of the local
LP relaxation on (G, ˆc, ˆw).

2 Related work

Perturbation stability. Several works have given
recovery guarantees for the local LP relaxation on
perturbation stable instances of uniform metric label-
ing (Lang et al., 2018, 2019) and for similar problems
(Makarychev et al., 2014; Angelidakis et al., 2017).

Lang et al. (2019) give partial recovery guarantees
for the local LP when parts (blocks) of the observed
instance satisfy a stability-like condition, and they
showed that practical instances have blocks that satisfy
their condition. However, the required block stability
condition in turn depends on certain quantities related
to the LP dual. This is unsatisfactory since this does
not explain when and why such instances are likely to
arise in practice. For a more extensive treatment of
the subject, we refer the reader to the “Perturbation
Resilience” chapter from Roughgarden (2021).

• In Section 6, we study a natural generative model
where the observed instance is generated from an
arbitrary latent stable instance by random (sub-
Gaussian) perturbations to the costs and weights.
We prove that, with high probability, every feasi-
ble LP solution takes close objective values on the

Easy instances corrupted with noise. Our ran-
dom noise model is similar to several planted average-
case models like stochastic block models (SBMs) con-
sidered in the context of problems like community de-
tection, correlation clustering and partitioning (see e.g.,
McSherry, 2001; Abbe, 2018; Globerson et al., 2015).

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Instances generated from these models can also be
seen as the result of random noise injected into an
instance with a nice block structure that is easy to
solve. Several works give exact recovery and approxi-
mate recovery guarantees for semideﬁnite programming
(SDP) relaxations for such models in diﬀerent parame-
ter regimes (Abbe, 2018; Guédon & Vershynin, 2016).
In our model however, we start with an arbitrary sta-
ble instance as opposed to an instance with a block
structure (which is trivial to solve). Moreover, we
are unaware of such analysis in the context of linear
programs. Please see Section 6 for a more detailed
comparison. To the best of our knowledge, we are
the ﬁrst to study instances generated from random
perturbations to stable instances.

Partial optimality algorithms. Several works
have developed fast algorithms for identifying parts
of the MAP assignment. These algorithms output an
approximate solution ˆx and a set of vertices where ˆx
provably agrees with the MAP solution x∗ (e.g., Kov-
tun, 2003; Shekhovtsov, 2013; Swoboda et al., 2016;
Shekhovtsov et al., 2017). Like these works, our results
also prove that an approximate solution ˆx has small
error |ˆx − x∗|. However, these previous approaches are
more concerned with designing fast algorithms for ﬁnd-
ing such ˆx. In contrast, we focus on giving structural
conditions that explain why a particular ˆx (the solu-
tion to the local LP relaxation) often approximately
recovers x∗. Our algorithm in Section 7 is thus not
meant as an eﬃcient method for certifying that |ˆx − x∗|
is small, but rather as a method for checking whether
our structural condition (that the observed instance is
close to a stable instance) is satisﬁed in practice.

3 Preliminaries

In this section we introduce our notation, deﬁne the
local LP relaxation for MAP inference, and give more
details on perturbation stability. As in the previous sec-
tion, the MAP inference problem in the ferromagnetic
Potts model on the instance (G, c, w) can be written
in energy minimization form as:

minimize
x:V →[k]

X

c(u, x(u)) + X

w(u, v)1[x(u) 6= x(v)].

u∈V

(u,v)∈E

(1)
Here x is an assignment (or labeling) of vertices to
labels i.e. x : V → {1, 2, . . . , k}. We can identify each
labeling x with a point (xu : u ∈ V ; xuv : (u, v) ∈ E),
where each xu ∈ {0, 1}k and each xuv ∈ {0, 1}k×k.
In this work, we consider all node costs c(u, i) ∈ R
and all edge weights w(u, v) > 0. We note that this is
equivalent to the formulation where all node costs and
edge weights are non-negative (Kleinberg & Tardos,

2002). See Appendix A for a proof of this equivalence.

We encode the node costs and the edge weights in a
vector θ ∈ Rnk+mk2 where n = |V | and m = |E| s.t.
θ(u, i) = c(u, i), θ(u, v, i, j) = w(u, v)1[i 6= j]. Then
the objective can be written as hθ, xi. We set xu(i) = 1
when x(u) = i, and 0 otherwise. Similarly, we set
xuv(i, j) = 1 when x(u) = i and x(v) = j, and 0
otherwise. Where convenient, we use x to refer to this
point rather than the labeling x : V → [k]. We can
then rewrite (1) as:

min.
x

X

k
X

u∈V

i=1

c(u, i)xu(i) + X

w(u, v) X

xuv(i, j)

(u,v)∈E

i6=j

subject to:

k
X

i=1

k
X

i=1

k
X

xu(i) = 1

∀ u ∈ V

xuv(i, j) = xv(j) ∀ (u, v) ∈ E, j ∈ [k]

xuv(i, j) = xu(i) ∀ (u, v) ∈ E, i ∈ [k]

j=1
xuv(i, j) ∈ {0, 1}
xu(i) ∈ {0, 1}

∀ (u, v), (i, j)
∀ u, i.

This is equivalent to (1), and is an integer linear pro-
gram (ILP). By relaxing the integrality constraints
from {0, 1} to [0, 1], we obtain the local LP relaxation:

min.
x∈L(G)

X

k
X

u∈V

i=1

c(u, i)xu(i) + X

w(u, v) X

xuv(i, j),

(u,v)∈E

i6=j

where L(G) is the polytope deﬁned by the same con-
straints as above, with x ∈ {0, 1} replaced with
x ∈ [0, 1]. This is known as the local polytope (Wain-
wright & Jordan, 2008). The vertices of L(G) are either
integral, meaning all xu and xuv take values in {0, 1},
or fractional, when some variables take values in (0, 1).
Integral vertices of this polytope correspond to label-
ings x : V → [k], so if the LP solution is obtained at
an integral vertex, then it is also a MAP assignment.

If the solution x∗ of this relaxation on an instance
(G, c, w) is obtained at an integral vertex, we say the
LP is tight on the instance, because the LP has exactly
recovered a MAP assignment. If the LP is not tight,
takes in-
there may still be some vertices u where x∗
u
tegral values. In this case, if x∗
(i) = 1 and ¯x(u) = i,
u
i.e., the LP solution agrees with the MAP assignment
¯x at vertex u, the LP is said to be persistent at u.
(i) ∈ {0, 1} does not imply the LP is persistent at u,
x∗
u
in general. The LP solution x∗ is said to be persistent
if it agrees with ¯x at every vertex u ∈ V .

Recovery error: In practice, the local LP relaxation
is often not tight, but is nearly persistent. We will

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

measure the recovery error of our LP solution in terms
of the “Hamming error” between the LP solution and
the MAP assignment.

Deﬁnition 3.1 (Recovery error). Given an instance
(G, c, w) of (1), let ¯x be a MAP assignment, and let x∗
be a solution to the local LP relaxation. The recovery
error is given by (with some abuse of notation)

1
2 kx∗ − ¯xk1 :=
=

1
2 kx∗
1
X
2

V − ¯xV k1
X

u∈V

i∈[k]

(cid:12)
(cid:12)x∗
u

(i) − 1[¯x(u) = i](cid:12)
(cid:12).

xV ∈ Rnk denotes the portion of x restricted to the
vertex set V . If x∗ is integral, the recovery error mea-
sures the number of vertices where x∗ disagrees with
¯x. When the recovery error of x∗ is 0, the solution x∗
is persistent. We will say that the LP solution x∗ is
nearly persistent when the recovery error of solution x∗
is a small fraction of n.

In our analysis, we will consider the following subset
L∗(G) of L(G) which is easier to work with, and which
contains all points we are interested in.

Deﬁnition 3.2 (L∗(G)). We deﬁne L∗(G) ⊆ L(G) to
be the set of points x ∈ L(G) which further satisfy
the constraint that xuv(i, i) = min(xu(i), xv(i)) for all
(u, v) ∈ E and i ∈ [k].

Claim 3.3. For a given graph G, every solution x ∈
L(G) that minimizes hθ, xi for some valid objective
vector θ = (c, w) also belongs to L∗(G). Further, all
integer solutions in L(G) also belong to L∗(G).

We prove this claim in Appendix A.

Our new stability result relies on the set of expansions
of a labeling x.
Deﬁnition 3.4 (Expansion). Let x : V → [k] be a
labeling of V . For any label α ∈ [k], we say that x0 is
an α-expansion of x if x0 6= x and the following hold
for all u ∈ V :

x(u) = α =⇒ x0(u) = α,
x0(u) 6= α =⇒ x0(u) = x(u).

That is, x0 may only expand the set of points labeled
α, and cannot make other changes to x.

4 Expansion Stability

In this section, we generalize the stability result of Lang
et al. (2018) to a much broader class of instances. This
generalization allows us to eﬃciently check whether a
real-world instance could plausibly have the structure

v

1 + ε

1 + ε

u

1 + ε

w

Node
u
v
w

Costs
.5 ∞ ∞
1
0 ∞
1 ∞ 0

Figure 2: (2, 1)-expansion stable instance that is not
(2, 1)-stable. In the original instance (shown left), the
optimal solution labels each vertex with label 1, for
an objective of 2.5. This instance is not (2, 1)-stable:
consider the (2, 1)-perturbation that multiples all edge
weights by 1/2. In this perturbed instance, the original
solution still has objective 2.5, and the new optimal
solution labels (u, v, w) → (1, 2, 3). This has a node
cost of 0.5 and an edge cost of (3 + 3ε)/2, for a to-
tal of 2 + 3ε/2 < 2.5. Since the original solution is
not optimal in the perturbed instance, this instance
is not (2, 1)-perturbation stable. However, note that
the only expansions of the original solution (which
had all label 1) that have non-inﬁnite objective are
(u, v, w) → (1, 2, 1) and (u, v, w) → (1, 1, 3). These
each have objective 2.5 + ε, which is strictly greater
than the perturbed objective of the original solution.
In fact, checking this single perturbation, known as the
adversarial perturbation is enough to verify expansion
stability: this instance is (2, 1)-expansion stable. We
include the full details in Appendix B.

shown in Figure 1 (that is, whether the instance is close
to a suitably stable instance).

Consider a ﬁxed instance (G, c, w) with a unique
MAP solution ¯x.
Theorem 1 of Lang et al.
(2018) requires that for all θ0 ∈ {(c, w0) | w0 ∈
{(2, 1)-perturbations of w}}, hθ0, xi > hθ0, ¯xi for all la-
belings x 6= ¯x. That is, that result requires ¯x to be
the unique optimal solution in any (2, 1)-perturbation
of the instance. By contrast, our result only requires
¯x to have better perturbed objective than the set of
expansions of ¯x (c.f. Deﬁnition 3.4).
Deﬁnition 4.1 ((2,1)-expansion stability). Let ¯x be
the unique MAP solution for (G, c, w), and let E¯x be
the set of expansions of ¯x (see Deﬁnition 3.4). Let

Θ = {(c, w0) | w0 ∈ {(2, 1)-perturbations of w}}

be the set of all objective vectors within a (2, 1)-
perturbation of θ = (c, w). We say the instance
(G, c, w) is (2, 1)-expansion stable if the following holds
for all θ0 ∈ Θ and all x ∈ E¯x:

hθ0, xi > hθ0, ¯xi.

That is, ¯x is better than all of its expansions x 6= ¯x in
every (2, 1)-perturbation of the instance.

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Theorem 4.2 (Local LP is tight on (2, 1)-expansion
stable instances). Let ¯x and ˆx be the MAP and local LP
solutions to a (2, 1)-expansion stable instance (G, c, w),
respectively. Then ¯x = ˆx i.e. the local LP is tight on
(G, c, w).

We defer the proof of this theorem to Appendix B as
it is similar to the proof of Theorem 1 from Lang et al.
(2018). The (2, 1)-expansion stability assumption is
much weaker than (2, 1)-stability because the former
only compares ¯x to its expansions, whereas the latter
compares ¯x to all labelings. While the rest of our results
can also be adapted to the (2, 1)-stability deﬁnition,
this relaxed assumption gives better empirical results.
Figure 2 shows an example of a (2, 1)-expansion stable
instance that is not (2, 1)-stable. This shows that our
new stability condition is less restrictive.

5 Curvature around MAP solution
and near persistence of the LP
solution

In this section, we show that a condition related to
(2, 1)-expansion stability, called (2, 1, ψ)-expansion sta-
bility, implies a “curvature” result for the objective
function around the MAP solution ¯x. On instances
satisfying this condition, any point ˆx ∈ L(G) with ob-
jective close to ¯x also has small kˆx − ¯xk1, so ˆx and ¯x
are close in solution space. In other words, if the LP
solution ˆx to a “corrupted” (2, 1, ψ)-expansion stable in-
stance is near-optimal in the original (2, 1, ψ)-expansion
stable instance (whose solution is ¯x), then the result in
this section implies kˆx − ¯xk1 is small. This immediately
gives a version of the result in the right panel of Fig-
ure 1: suppose we deﬁne an instance to be close to a
(2, 1, ψ)-expansion stable (G, ¯c, ¯w) if its LP solution ˆx is
approximately optimal in (G, ¯c, ¯w). Then the curvature
result implies that the LP approximately recovers the
stable instance’s MAP solution ¯x for all close instances.
In Section 6, we give a generative model where the gen-
erated instances are “close” according to this deﬁnition
with high probability.

The (2, 1, ψ)-expansion stability condition, for ψ > 0,
says that the instance is (2, 1)-expansion stable even if
we allow all node costs c(u, i) to be additively perturbed
by up to ψ. This extra additive stability will allow us
to prove the curvature result. This is related to the
use of additive stability in Lang et al. (2019) to give
persistency guarantees.
Deﬁnition 5.1 ((2, 1, ψ)-expansion stable). For ψ > 0,
we say an instance (G, c, w) is (2, 1, ψ)-expansion stable
if (G, c0, w) is (2, 1)-expansion stable for all c0 with
c ≤ c0 ≤ c + ψ · 1 where 1 is the all-ones vector.

near persistence of the LP solution on (2, 1, ψ) expan-
sion stable instances in terms of the gap in objective
value.

Theorem 5.2. Let (G, c, w) be a (2, 1, ψ)-expansion
stable instance with MAP solution ¯x. Let θ = (c, w).
Then for any x ∈ L∗(G), the recovery error (see
Def. 3.1) satisﬁes

1
2 kx − ¯xk1 :=

1
2 kxV − ¯xV k1 ≤

1

ψ

|hθ, xi − hθ, ¯xi|. (2)

Proof (sketch). For any x ∈ L∗(G), we construct a
feasible solution ˆx which is a strict convex combination
of x and ¯x that is very close to ¯x. Then, we apply
a rounding algorithm to ˆx to get a random integer
solution h. Let ˆθ represent the worst (2, 1)-perturbation
for ¯x. This is the instance where all the edges not cut by
¯x have their weights multiplied by 1/2. We deﬁne the
objective diﬀerence using ˆθ as Ah = hˆθ, hi−hˆθ, ¯xi. First
we show an upper bound for E[Ah] using properties of
the rounding algorithm. Then we show that for any
solution h in the support of our rounding algorithm,
Ah ≥ ψ · Bh where Bh is the Hamming error of h
(when compared to ¯x). On the other hand, one can
also use the properties of the rounding algorithm to
get a lower bound on E[Bh] in terms of the recovery
error (i.e., Hamming error) of the LP solution. These
bounds together imply the required upper bound on
the recovery error of the LP solution.

We defer the complete proof and an alternate dual-
based proof to Appendix C.

Theorem 5.2 shows that on a (2, 1, ψ)-expansion stable
instance, small objective gap hθ, xi−hθ, ¯xi implies small
distance ||xV − ¯xV ||1 in solution space. Although this
holds for any x ∈ L∗(G), we will be interested in x that
are LP solutions to an observed, corrupted version of
the stable instance.

We now show that if the observed instance has a nearby
stable instance, then the LP solution for the observed in-
stance has small Hamming error. For any two instances
ˆθ = (ˆc, ˆw) and ¯θ = (¯c, ¯w) on the same graph G, the met-
ric between them d(ˆθ, ¯θ) := sup
x∈L∗(G) |hˆθ, xi − h¯θ, xi|.
Corollary 5.3 (LP solution is good if there is a nearby
stable instance). Let ˆxM AP and ˆx be the MAP and local
LP solutions to an observed instance (G, ˆc, ˆw). Also, let
¯x be the MAP solution for a latent (2, 1, ψ)-expansion
stable instance (G, ¯c, ¯w). If ˆθ = (ˆc, ˆw) and ¯θ = (¯c, ¯w),

1
2 kˆxV − ˆxM AP

V

k1 ≤

2d(ˆθ, ¯θ)
ψ

+

1
2 kˆxM AP

V

− ¯xV k1.

The following theorem shows low recovery error i.e.,

We defer the proof of this corollary to Appendix C.

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

6 Generative model for noisy stable

instances

In the previous section, we showed that if an instance
(G, ˆc, ˆw) is “close” to a (2, 1, ψ)-expansion stable in-
stance (G, ¯c, ¯w) (i.e., the LP solution ˆx to (G, ˆc, ˆw)
has good objective in (G, ¯c, ¯w)), then kˆx − ¯xk is small,
where ¯x is the MAP solution to the stable instance.
In this section, we give a natural generative model
for (G, ˆc, ˆw), based on randomly corrupting (G, ¯c, ¯w),
in which ˆx has good objective in (G, ¯c, ¯w) with high
probability. Together with the curvature result from
the previous section (Theorem 5.2), this implies that
the LP relaxation, run on the noisy instances (G, ˆc, ˆw),
approximately recovers ¯x with high probability.

We now describe our generative model for the prob-
lem instances, which starts with an arbitrary stable
instance and perturbs it with random additive pertur-
bations to the edge costs and node costs (of potentially
varying magnitudes). The random perturbations re-
ﬂect possible uncertainty in the edge costs and node
costs of the Markov random ﬁeld. We will assume
the random noise comes from any distribution that is
sub-Gaussian1. However, there is a small technicality:
the edge costs need to be positive (node costs can be
negative). For this reason we will consider truncated
sub-Gaussian random variables for the noise for the
edge weights. We deﬁne sub-Gaussian and truncated
sub-Gaussian random variables in Appendix D.

Generative Model: We start with an instance
(G, ¯c, ¯w) that is (2, 1, ψ)-expansion stable, and perturb
the edge costs and node costs independently. Given
any instance (G, ¯c, ¯w), an instance (G, ˆc, ˆw) from the
model is generated as follows:

1. For all node-label pairs (u, i), ˆc(u, i) = ¯c(u, i) +
˜c(u, i), where ˜c(u, i) is sub-Gaussian with mean 0
and parameter σu,i.

2. For all edges (u, v), ˆw(u, v) = ¯w(u, v) + ˜w(u, v),
where ˜w(u, v) is an independent r.v.
that
is (−w(u, v), γu,v)-truncated sub-Gaussian with
mean 0.

3. (G, ˆc, ˆw) is the observed instance.

By the deﬁnition of our model, the edge weights
ˆw(u, v) ≥ 0 for all (u, v) ∈ E. The parameters of
the model are the unperturbed instance (G, ¯c, ¯w), and
the noise parameters { γu,v, σu,i }u,v∈V,i∈[k]
. On the
one hand, the above model captures a natural average-
case model for the problem. For a ﬁxed ground-truth
solution x∗ : V → [k], consider the stable instance
= 2 for all u, v in the same clus-
(H, c, w) where w∗
uv

= 1 otherwise; and
ter (i.e., x∗(u) = x∗(v)) and w∗
uv
with c∗(u, i) = 1 if x∗(u) = i, and c∗(u, i) = 1 + ψ
otherwise. The above noisy stable model with stable
instance (H, c, w) generates instances that are remi-
niscent of (stochastic) block models, with additional
node costs. On the other hand, the above model is
much more general, since we can start with any stable
instance (G, c, w).

With high probability over the random corruptions
of our stable instance, the local LP on the corrupted
instance approximately recovers the MAP solution ¯x
of the stable instance. The key step in the proof of
this theorem is showing that, with high probability, the
observed instance is close to the latent stable instance
in the metric we deﬁned earlier.
Lemma 6.1 (d(ˆθ, ¯θ) is small w.h.p. ). There exists a
universal constant c < 1 such that for any instance in
the above model, with probability at least 1 − o(1),

|hˆθ, xi − h¯θ, xi| ≤ c

sup
x∈L∗(G)

√

v
u
u
t

nk

X

u,i

σ2

u,i

+ k2
4

X

uv

γ2
u,v

Proof (sketch). For any ﬁxed x ∈ L∗(G), we can show
that |hˆθ, xi − h¯θ, xi| is small w.h.p. using a standard
large deviations bound for sums of sub-Gaussian ran-
dom variables. The main technical challenge is in
showing that the supremum over all feasible solutions
is small w.h.p. The standard approach is to perform
a union bound over an ε-net of feasible LP solutions
in L∗. However, this gives a loose bound.
Instead,
we upper bound the supremum by using a rounding
algorithm for LP solutions in L∗(G), and union bound
only over the discrete solutions output by the rounding
algorithm. This gives signiﬁcant improvements over the
standard approach; for example, in a d-regular graph
with equal variance parameter γuv, this saves a factor
of

d apart from logarithmic factors in n.

√

We defer the details to Appendix D. The above proof
technique that uses a rounding algorithm to provide a
deviation bound for a continuous relaxation is similar
to the analysis of SDP relaxations for average-case
problems (see e.g., Makarychev et al., 2013; Guédon &
Vershynin, 2016). The above lemma, when combined
with Theorem 5.2 gives the following guarantee.

Theorem 6.2 (LP solution is nearly persistent). Let
ˆx be the local LP solution to the observed instance
(G, ˆc, ˆw) and ¯x be the MAP solution to the latent
(2, 1, ψ)-expansion stable instance (G, ¯c, ¯w). With high
probability over the random noise,

1All of the results that follow can also be generalized to
sub-exponential random variables; however for convenience,
we restrict our attention to sub-Gaussians.

1
2 kˆxV − ¯xV k1 ≤

2

ψ

√

· c

nk ·

sX

σ2

u,i

+ k2 X

γ2
u,v

u,i

uv

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Proof. We know that for any feasible solution x ∈
L(G), h¯θ, xi ≥ h¯θ, ¯xi. Therefore, h¯θ, ˆxi ≥ h¯θ, ¯xi. Re-
member that we deﬁned d(ˆθ, ¯θ) as sup
x∈L∗(G) |hˆθ, xi −
h¯θ, xi|. Since ˆx and ¯x are both points in L∗(G),

h¯θ, ˆxi ≤ hˆθ, ˆxi + d(ˆθ, ¯θ) ≤ hˆθ, ¯xi + d(ˆθ, ¯θ)

≤ h¯θ, ¯xi + 2d(ˆθ, ¯θ)

The ﬁrst and third inequalities follow from the deﬁni-
tion of d(ˆθ, ¯θ). The second inequality follows from
the fact that ˆx is the minimizer of h¯θ, xi over all
x ∈ L(G). Therefore,0 ≤ h¯θ, ˆxi − h¯θ, ¯xi ≤ 2d(ˆθ, ¯θ).
2 kˆx − ¯xk1 ≤ 2d(ˆθ,¯θ)
.
Using this in Theorem 5.2, we get 1
Lemma 6.1 then gives an upper bound on d(ˆθ, ¯θ) that
holds w.h.p.

ψ

For a d-regular graph in the uniform setting, we get
the following useful corollary:
Corollary 6.3 (MAP solution recovery for regular
graphs ). Suppose we have a d-regular graph G with
= σ2 for all
γ2
u,v
vertex-label pairs (u, i). Also, suppose only a fraction
ρ of the vertices and η of the edges are subject to the
noise. With high probability over the random noise,

= γ2 for all edges (u, v), and σ2
u,i

kˆxV − ¯xV k1
2n

≤

2ck

q

ρσ2 + ηdk

8 γ2

ψ

Note that when ˆx is an integer solution, the left-hand-
side is the fraction of vertices misclassiﬁed by ˆx.

7 Finding nearby stable instances

In this section, we describe an algorithm for eﬃciently
ﬁnding (2, 1, ψ)-expansion stable instances that are
“close” to an observed instance (G, ˆc, ˆw). This algorithm
allows us to check whether we can plausibly model real-
world instances as “corrupted” versions of a (2, 1, ψ)-
expansion stable instance.

In addition to an observed instance (G, ˆc, ˆw), the al-
gorithm takes as input a “target” labeling xt. For
example, xt could be a MAP solution of the observed
instance. Surprisingly, once given a target solution,
this algorithm is eﬃcient.

We want to search over costs c and weights w. The
broad goal to solve the following optimization problem:

minimize
c≥0,w≥0
subject to

f (c, w)

(3)

(G, c, w) is (2, 1, ψ)-expansion stable
with MAP solution xt,

where f (c, w) is any convex function of c and w. In
particular, we will use f1(c, w) = ||(c, w) − (ˆc, ˆw)||1 and

2

2 ||(c, w) − (ˆc, ˆw)||2

for minimizing the L1
f2(c, w) = 1
and L2 distances between to the observed instance.
The output of this optimization problem will give the
closest objective vector (¯c, ¯w) for which the instance
(G, ¯c, ¯w) is (2, 1, ψ)-expansion stable. If the optimal
objective value of this optimization is 0, the observed
instance (G, ˆc, ˆw) is (2, 1, ψ)-expansion stable.

There is always a feasible (c, w) for (3) (see Appendix E
for a proof), but it may change many weights and costs.
Next we derive an eﬃciently-solvable reformulation of
(3). In the next section, we ﬁnd that the changes to
ˆc and ˆw required to ﬁnd a (2, 1, ψ)-expansion stable
instance are relatively sparse in practice.

Theorem 7.1. The optimization problem (3) can be
expressed as a convex minimization problem over a
polytope described by poly(n, m, k) constraints. When
f (c, w) = ||(c, w) − (ˆc, ˆw)||1, (3) can be expressed as a
linear program.

The instance (G, c, w) is (2, 1, ψ)-expansion stable if
xt is better than every expansion y of xt in every
(2, 1, ψ)-perturbation of (c, w). Let E be the set of all
expansions of the target solution xt. Then for all θ0
within a (2, 1, ψ)-perturbation of θ = (c, w), we should
have that hθ0, xti ≤ miny∈E hθ0, xti.
It is enough to
check the adversarial (2, 1, ψ)-perturbation θadv. This
perturbation makes the target solution xt as bad as
possible. If xt is better than all the expansions y ∈ E
in this perturbation, it is better than all y ∈ E in every
(2, 1, ψ)-perturbation (see Appendix E for a proof). We
set θadv = (cadv, wadv) as:

cadv(u, i) =

(

c(u, i) + ψ i = xt(u),
otherwise.
c(u, i)

wadv(u, v) =

(

xt(u) 6= xt(u),

w(u, v)
2 w(u, v) otherwise.
1

The target solution xt is ﬁxed, so hθadv, xti is a linear
function of the optimization variables c and w. For
α ∈ [k], let E α be the set of α-expansions of xt. Because
E = ∪α∈[k]E α, we have hθ0, xti ≤ miny∈E hθ0, xti if and
only if hθ0, xti ≤ miny∈E α hθ0, xti for all α ∈ [k]. We
can simplify the original optimization problem to:

minimize
c≥0,w≥0
subject to

f (c, w)

hθadv, xti ≤ min
y∈E α

hθadv, yi

∀ α ∈ [k],

θadv is a linear function of c, w as deﬁned above. We
now use the structure of the sets E α to simplify the con-
straints. The optimal value of miny∈E αhθadv, yi is the
objective value of the best (w.r.t. θadv) α-expansion
of xt. The best α-expansion of a ﬁxed solution xt
can be found by solving a minimum cut problem in

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

Table 1: Results from the output of (5) on three stereo vision instances. More details in Appendix F.

Instance Costs changed Weights changed

(normalized) Recovery error bound

tsukuba
venus
cones

4.9%
22.5%
1.2%

2.3%
1.3%
2.1%

0.0518
0.0214
0.0437

||1/2n

||ˆxV − ˆxM AP
V
0.0027
0.0016
0.0022

an auxiliary graph Gxt
(α) whose weights depend on
aux
linearly on the objective θadv, and therefore depend
linearly on our optimization variables (c, w) (Boykov
et al., 2001, Section 4). Therefore, the optimization
problem miny∈E α hθadv, yi can be expressed as a mini-
mum cut problem. Because this minimum cut problem
can be written as a linear program, we can rewrite each
constraint as

hθadv, xti ≤

min
z:A(α)z=b(α),z≥0

hθadv, zi,

(4)

where {A(α)z = b(α), z ≥ 0} is the feasible region of
the standard metric LP corresponding to the minimum
cut problem in Gxt
(α). The number of vertices in
aux
Gxt
(α) and the number of constraints in A(α)z = b(α)
aux
is poly(m, n, k) for all α. We now derive an equivalent
linear formulation of (4) using a careful application of
strong duality. The dual to the LP on the RHS is:

maximize
ν

hb(α), νi, s.t. A(α)T ν ≤ θadv.

Because strong duality holds for this linear program,
we have that (4) holds if and only if there exists ν with
A(α)T ν ≤ θadv such that hθadv, xti ≤ hb(α), νi.
This is a linear constraint in (c, w, ν). By using this
dual witness trick for each label α ∈ [k], we obtain:

minimize
c≥0,w≥0,{να}
subject to

f (c, w)

(5)

hθadv, xti ≤ hb(α), ναi
A(α)T να ≤ θadv

∀ α

∀ α.

The constraints of (5) are linear in the optimization
variables (c, w) and να. The dimension of θadv is nk +
mk2, so there are k(nk + mk2 + 1) constraints and
nk + m + P
α |b(α)| = poly(m, n, k) variables. Because
minimization of the L1 distance f1(c, w) can be encoded
using a linear function and linear constraints, (5) is
a linear program in this case.
It is clear from the
derivation of (5) that it is equivalent to (3). This
proves Theorem 7.1. This formulation (5) can easily be
input into “oﬀ-the-shelf” convex programming packages
such as Gurobi (Gurobi Optimization, 2020).

8 Numerical results

Table 1 shows the results of running (5) on real-world
instances of MAP inference to ﬁnd nearby (2, 1, ψ)-

expansion stable instances. We study stereo vision mod-
els using images from the Middlebury stereo dataset
(Scharstein & Szeliski, 2002) and Potts models from
Tappen & Freeman (2003). Please see Appendix F for
more details about the models and experiments.

We ﬁnd, surprisingly, that only relatively sparse
changes are required to make the observed instances
(2, 1, ψ)-expansion stable with ψ = 1. On these in-
stances, we evaluate the recovery guarantees by our
bound from Theorem 5.2 and compare it to the actual
value of the recovery error ||ˆx − ˆxM AP ||1/2n. In all of
our experiments, we choose the target solution xt for (5)
to be equal to the MAP solution ˆxM AP of the observed
instance. Therefore, we ﬁnd a (2, 1, ψ)-expansion sta-
ble instance that has the same MAP solution as our
observed instance. The recovery error bound given by
Theorem 5.2 is then also a bound for the recovery error
between ˆx and ˆxM AP , because ˆxM AP = xt. On these
instances, the bounds from our curvature result (The-
orem 5.2) are reasonably close to the actual recovery
value. However, this bound uses the property that ˆx
has good objective in the stable instance and so it is
still a “data-dependent” bound in the sense that it uses
an empirically observed property of the LP solution ˆx.
In Appendix F, we show how to reﬁne Corollary 5.3 to
give non-vacuous recovery bounds that do not depend
on ˆx.

9 Conclusion

We studied the phenomenon of near persistence of the
local LP relaxation on instances of MAP inference in
ferromagnetic Potts model. We gave theoretical re-
sults, algorithms (for ﬁnding nearby stable instances)
and empirical results to demonstrate that even after a
(2, 1, ψ)-perturbation stable instance is corrupted with
noise, the solution to the LP relaxation is nearly persis-
tent i.e., it approximately recovers the MAP solution.
Our theoretical results imply that the local LP approx-
imately recovers the MAP solution when the observed
instance is close to a stable instance. Our empirical re-
sults suggest that real-world instances are very close to
stable instances. These results together suggest a new
explanation for the near-persistency of the solution of
the local LP relaxation for MAP inference in practice.

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Acknowledgments

We would like to thank all the reviewers for their feed-
back. This work was supported by NSF AitF awards
CCF- 1637585 and CCF-1723344. Aravind Reddy was
also supported in part by NSF CCF-1955351 and HDR
TRIPODS CCF-1934931. Aravindan Vijayaraghavan
was also supported by NSF CCF-1652491.

References

Abbe, E. Community detection and stochastic block
models. Foundations and Trends® in Communica-
tions and Information Theory, 14(1-2):1–162, 2018.
ISSN 1567-2190. doi: 10.1561/0100000067.

Angelidakis, H., Makarychev, K., and Makarychev,
Y. Algorithms for stable and perturbation-resilient
problems. In Proceedings of the 49th Annual ACM
SIGACT Symposium on Theory of Computing, pp.
438–451, 2017.

Archer, A., Fakcharoenphol, J., Harrelson, C.,
Krauthgamer, R., Talwar, K., and Tardos, É. Ap-
proximate classiﬁcation via earthmover metrics. In
Proceedings of the ﬁfteenth annual ACM-SIAM sym-
posium on Discrete algorithms, pp. 1079–1087. Soci-
ety for Industrial and Applied Mathematics, 2004.

Bilu, Y. and Linial, N. Are stable instances easy? In
Innovations in Computer Science, pp. 332–341, 2010.

Birchﬁeld, S. and Tomasi, C. A pixel dissimilarity
measure that is insensitive to image sampling. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 20(4):401–406, 1998.

Boykov, Y., Veksler, O., and Zabih, R. Fast approx-
imate energy minimization via graph cuts. IEEE
Transactions on pattern analysis and machine intel-
ligence, 23(11):1222–1239, 2001.

Chekuri, C., Khanna, S., Naor, J. S., and Zosin, L.
Approximation algorithms for the metric labeling
problem via a new linear programming formulation.
In Proceedings of the Twelfth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA ’01, pp.
109–118, USA, 2001. ISBN 0898714907.

Dahlhaus, E., Johnson, D. S., Papadimitriou, C. H.,
Seymour, P. D., and Yannakakis, M. The complexity
of multiway cuts. In Proceedings of the twenty-fourth
annual ACM symposium on Theory of computing, pp.
241–251, 1992.

Globerson, A. and Jaakkola, T. S. Fixing max-product:
Convergent message passing algorithms for map lp-
relaxations. In Advances in neural information pro-
cessing systems, pp. 553–560, 2008.

Globerson, A., Roughgarden, T., Sontag, D., and
Yildirim, C. How hard is inference for structured

prediction? volume 37 of Proceedings of Machine
Learning Research, pp. 2181–2190, Lille, France, 07–
09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/globerson15.html.

Guédon, O. and Vershynin, R. Community detection in
sparse networks via grothendieck’s inequality. Proba-
bility Theory and Related Fields, 165(3-4):1025–1049,
2016.

Gurobi Optimization, L. Gurobi optimizer reference
manual, 2020. URL http://www.gurobi.com.

Kleinberg, J. and Tardos, E. Approximation algorithms
for classiﬁcation problems with pairwise relation-
ships: Metric labeling and markov random ﬁelds.
Journal of the ACM (JACM), 49(5):616–639, 2002.

Kolmogorov, V. and Wainwright, M. J. On the optimal-
ity of tree-reweighted max-product message-passing.
In Proceedings of the Twenty-First Conference on
Uncertainty in Artiﬁcial Intelligence, pp. 316–323,
2005.

Kovtun, I. Partial optimal labeling search for a np-
hard subclass of (max,+) problems. In Joint Pattern
Recognition Symposium, pp. 402–409. Springer, 2003.

Lang, H., Sontag, D., and Vijayaraghavan, A. Opti-
mality of approximate inference algorithms on stable
instances. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1157–1166, 2018.

Lang, H., Sontag, D., and Vijayaraghavan, A. Block
stability for map inference. In The 22nd International
Conference on Artiﬁcial Intelligence and Statistics,
pp. 216–225, 2019.

Makarychev, K., Makarychev, Y., and Vijayaraghavan,
A. Sorting noisy data with partial information. In
Proceedings of the 4th conference on Innovations in
Theoretical Computer Science, pp. 515–528. ACM,
2013.

Makarychev, K., Makarychev, Y., and Vijayaragha-
van, A. Bilu–linial stable instances of max cut and
minimum multiway cut. In Proceedings of the twenty-
ﬁfth annual ACM-SIAM symposium on Discrete al-
gorithms, pp. 890–906. SIAM, 2014.

McSherry, F. Spectral partitioning of random graphs.
In Proceedings of the 42nd IEEE symposium on
Foundations of Computer Science, FOCS ’01, pp.
529–, Washington, DC, USA, 2001. IEEE Com-
puter Society.
ISBN 0-7695-1390-5. URL http:
//dl.acm.org/citation.cfm?id=874063.875554.

Meshi, O., London, B., Weller, A., and Sontag, D.
Train and test tightness of lp relaxations in struc-
tured prediction. The Journal of Machine Learning
Research, 20(1):480–513, 2019.

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

Roughgarden, T. (ed.). Beyond the Worst-Case Analy-
sis of Algorithms. Cambridge University Press, 2021.
doi: 10.1017/9781108637435.

Rowland, M., Pacchiano, A., and Weller, A. Conditions
beyond treewidth for tightness of higher-order lp
relaxations. In Artiﬁcial Intelligence and Statistics,
pp. 10–18, 2017.

Scharstein, D. and Szeliski, R. A taxonomy and evalu-
ation of dense two-frame stereo correspondence algo-
rithms. International journal of computer vision, 47
(1):7–42, 2002.

Shekhovtsov, A. Exact and partial energy minimization

in computer vision. 2013.

Shekhovtsov, A., Swoboda, P., and Savchynskyy, B.
Maximum persistency via iterative relaxed inference
in graphical models. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 40(7):1668–1682,
2017.

Sontag, D. Approximate Inference in Graphical Models
using LP Relaxations. PhD thesis, Massachusetts
Institute of Technology, Department of Electrical
Engineering and Computer Science, 2010.

Sontag, D., Globerson, A., and Jaakkola, T. Introduc-
tion to dual decomposition for inference. In Opti-
mization for Machine Learning. MIT Press, 2011.

Swoboda, P., Shekhovtsov, A., Kappes, J. H., Schnörr,
C., and Savchynskyy, B. Partial optimality by prun-
ing for map-inference with general graphical models.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(7), 2016.

Tappen and Freeman. Comparison of graph cuts with
belief propagation for stereo, using identical mrf
parameters. In Proceedings Ninth IEEE International
Conference on Computer Vision, pp. 900–906 vol.2,
2003.

Thapper, J. et al. The power of linear programming for
valued csps. In 2012 IEEE 53rd Annual Symposium
on Foundations of Computer Science, pp. 669–678.
IEEE, 2012.

Vershynin, R. High-dimensional probability: An intro-
duction with applications in data science, volume 47.
Cambridge university press, 2018.

Wainwright, M. J. and Jordan, M. I. Graphical models,
exponential families, and variational inference. Now
Publishers Inc, 2008.

Wainwright, M. J., Jaakkola, T. S., and Willsky, A. S.
Map estimation via agreement on trees: message-
passing and linear programming. IEEE transactions
on information theory, 51(11):3697–3717, 2005.

Weller, A., Rowland, M., and Sontag, D. Tightness
of lp relaxations for almost balanced models.
In
Artiﬁcial Intelligence and Statistics, pp. 47–55, 2016.

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Beyond Perturbation Stability: Supplementary Material

A Preliminaries Details

Claim A.1. For Uniform Metric Labeling, we can assume c(u, i) ≥ 0 and w(u, v) > 0 without loss of generality.

Proof. For problem instances where some node costs are strictly negative, let cmin be the minimum value
among all the node costs. Consider a new problem instance where we keep the edge costs the same, but set
c0(u, i) = c(u, i) + |cmin| for all u ∈ V and i ∈ [k]. This new problem instance has all non-negative node costs, and
the optimization problem is equivalent, because we added the same constant for all solutions. This reformulation
also does not aﬀect the (2, 1)-expansion stability or (2, 1, ψ)-expansion stability of the instance.
Likewise, for problem instances where some edge weights are 0, let E0 be the set of all edges with 0 edge weight.
Consider a new problem instance with E0 = E \ E0, with w(u, v) unchanged for (u, v) ∈ E \ E0, and identical
node costs. The MAP optimization problem remains the same, and the new instance ((V, E0), c, w) is equivalent:
it has the same MAP solution, and satisﬁes the stability deﬁnitions if and only if the original instance does as
well.

Claim 3.3. For a given graph G, every solution x ∈ L(G) that minimizes hθ, xi for some valid objective vector
θ = (c, w) also belongs to L∗(G). Further, all integer solutions in L(G) also belong to L∗(G).

Proof of Claim 3.3. Recall the local LP:

X

X

c(u, i)xu(i) + X

w(u, v) X

xuv(i, j)

(u,v)∈E

i6=j

xu(i) = 1

∀ u ∈ V

xuv(i, j) = xv(j)

∀ (u, v) ∈ E, j ∈ [k]

xuv(i, j) = xu(i)

∀ (u, v) ∈ E, i ∈ [k]

min.
x

i
u∈V
subject to: X
i
X

i
X

j

xuv(i, j) ∈ [0, 1]
xu(i) ∈ [0, 1]

∀ (u, v), (i, j)
∀ u, i.

(6)

(7)

(8)

(9)

(10)

(11)

The feasible region deﬁned by the above constraints is L(G). L∗(G) ⊆ L(G) is the set of points that satisfy
the additional constraint that xuv(i, i) = min(xu(i), xv(i)) for all (u, v) ∈ E and i ∈ [k]. For any feasible node
variable assignments {xu}, L∗(G) is not empty: a simple ﬂow argument2 implies that the constraints (8), (9),
and (10) are always satisﬁable even when we set xuv(i, i) = min(xu(i), xv(i)). For all integer feasible solutions in
L(G), notice that xuv(i, j) = 1 if xu(i) = 1 and xv(j) = 1 or 0 otherwise. Therefore, all integer solutions satisfy
this additional constraint. Consider a θ where all edge weights are strictly positive. If x minimizes hθ, xi, x must

2For an edge (u, v), consider the bipartite graph ˜G = ((U, V ), E), where |U | = |V | = k. We let xu(i) represent
the supply at node i in U , and let xv(i) represent the demand at node j in V . Because xu and xv are both feasible,
the total supply equals the total demand. E contains all edges between U and V , so we can send ﬂow from i ∈ U to
j ∈ V for any (i, j) pair. Let xuv(i, j) represent this ﬂow, and set xuv(i, i) = min(xu(i), xv(i)). For every i, this either
satisﬁes the demand at node Vi or exhausts the supply at node Ui. In each case, we can remove that satisﬁed/exhausted
node from the graph. After this choice of xu(i, i), the total remaining supply equals the total remaining demand
(P
i xv(i) − min(xu(i), xv(i))), all supplies and demands are nonnegative, and the remaining
graph ˜G0 is a complete bipartite graph (over fewer nodes). This implies that the ﬂow constraints (8), (9), (10) are still
feasible.

i xu(i) − min(xu(i), xv(i)) = P

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

v

1 + ε

1 + ε

u

1 + ε

w

Node
u
v
w

Costs
.5 ∞ ∞
1
0 ∞
1 ∞ 0

Figure 3: (2, 1)-expansion stable instance that is not (2, 1)-stable. In the original instance (shown left), the
optimal solution labels each vertex with label 1, for an objective of 2.5. The adversarial (2, 1)-perturbation for
this instance replaces all the edge weights of 1 + ε with (1 + ε)/2. In this perturbed instance, the optimal solution
labels (u, v, w) → (1, 2, 3). This has a node cost of 0.5 and an edge cost of (3 + 3ε)/2, for a total of 2 + 3ε/2 < 2.5.
Since the original solution is not optimal in the perturbed instance, this instance is not (2, 1)-perturbation stable.
However, note that the only expansions of the original solution (which had all label 1) that have non-inﬁnite
objective are (u, v, w) → (1, 2, 1) and (u, v, w) → (1, 1, 3). These each have objective 2.5 + ε, which is strictly
greater than the perturbed objective of the original solution. Therefore, this instance is (2, 1)-expansion stable.

pay the minimum edge cost consistent with its node variables xu(i). So if we ﬁx the xu(i) portion of x, we know
that the edge variables xuv of x are a solution to:
X

min
x∈L(G)

w(u, v) X

xuv(i, j).

(u,v)∈E

i6=j

Notice that since we have ﬁxed the node variables xu(i), there is no interaction between the xuv variables across
diﬀerent edges. So we can minimize this objective by minimizing each individual term w(u, v) P
i6=j xuv(i, j).
Since wuv > 0 for all edges, we need to minimize P
i6=j xuv(i, j). Notice that for every edge (u, v) ∈ E, we get
that P
j xuv(i, j) from constraint 9. Therefore
i xuv(i, i). Thus, minimizing P
P
i xuv(i, i). And
the maximizing choice for xuv(i, i) = min(xu(i), xv(i)) due to constraints 8 and 9.

j xuv(i, j) = 1 by substituting xu(i) in constraint 7 with P

i6=j xuv(i, j) is the same as maximizing P

i6=j xuv(i, j) = 1 − P

P

i

B Expansion Stability details

Claim B.1. An instance (G, w, c) is (2, 1)-expansion stable iﬀ the MAP solution ¯x is strictly better than all its
expansions in the adversarial perturbation θadv. That is, for all x ∈ E¯x, hθadv, xi > hθadv, ¯xi where θadv has the

same node costs c but has weights wadv(u, v) =

( 1

2 w(u, v)
w(u, v)

¯x(u) = ¯x(v)
¯x(u) 6= ¯x(v).

Proof. Consider θ0 = (c, w0), any valid (2, 1)-perturbation of θ = (c, w) i.e. for every edge (u, v) ∈ E, w(u,v)
2 ≤
w0(u, v) ≤ w(u, v). For any valid labeling x, let Ex represent the edges cut by x. Then, for any x which is an
expansion of ¯x i.e. x ∈ E¯x,
hθ0, xi − hθ0, ¯xi = X

c(u, x(u)) − c(u, ¯x(u)) + X

w0(u, v) −

w0(u, v)

X

u∈V
= X
u∈V

c(u, x(u)) − c(u, ¯x(u)) + X

w0(u, v) −

X

w0(u, v)

(u,v)∈Ex

(u,v)∈E¯x

(u,v)∈Ex\E¯x

(u,v)∈E¯x\Ex

= hθadv, xi − hθadv, ¯xi + X

w0(u, v) − wadv(u, v) + X

wadv − w0(u, v)

(u,v)∈Ex\E¯x

(u,v)∈E¯x\Ex

= hθadv, xi − hθadv, ¯xi + X

w0(u, v) −

(u,v)∈Ex\E¯x

w(u, v)
2

+ X

w(u, v) − w0(u, v)

(u,v)∈E¯x\Ex

Since w0 is a valid (2, 1)-perturbation, w0(u, v) ≥ w(u, v)/2 and w0(u, v) ≤ w(u, v). Therefore, for any valid
(2, 1)-perturbation θ0, we have

hθ0, xi − hθ0, ¯xi ≥ hθadv, xi − hθadv, ¯xi.

If the instance is (2, 1)-expansion stable, then certainly hθadv, xi > hθadv, ¯xi for all x ∈ E ¯x, since θadv is a valid
(2, 1)-perturbation of θ. If the instance is not (2, 1)-expansion stable, there exists a θ0 and an x ∈ E ¯x for which

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

hθ0, xi − hθ0, ¯xi ≤ 0. But the above inequality then implies that hθadv, xi − hθadv, ¯xi ≤ 0 as well. This gives both
directions.

This claim shows that to check whether an instance is (2, 1)-expansion stable, it is suﬃcient to check that the
MAP solution is strictly better than all its expansions in the adversarial perturbation θadv. We don’t need to
verify that this condition is satisﬁed in every valid (2, 1)-perturbation. Because the optimal expansion of ¯x in the
instance with objective θadv can be computed eﬃciently, this claim also implies that (2, 1)-expansion stability can
be eﬃciently checked once the MAP solution ¯x is known.
Claim B.2. (2, 1)-expansion stability is strictly weaker than (2, 1)-perturbation stability.

Proof. Figure 2 gives an instance of uniform metric labeling that is (2, 1)-expansion stable but not (2, 1)-
perturbation stable. Here, 0 < ε < 1/3.

Theorem 4.2 (Local LP is tight on (2, 1)-expansion stable instances). Let ¯x and ˆx be the MAP and local LP
solutions to a (2, 1)-expansion stable instance (G, c, w), respectively. Then ¯x = ˆx i.e. the local LP is tight on
(G, c, w).

Proof. First, we note that for any x ∈ L∗(G), the objective value of the local LP can be written in a form that
depends only on the node variables xV . The objective term corresponding to the edges

X

w(u, v) X

xuv(i, j) = X

w(u, v)

(u,v)∈E

i6=j

(u,v)∈E



X



i,j

xuv(i, j) −

X

xuv(i, i)





= X

w(u, v)

1 −

(u,v)∈E

= X

w(u, v)

1 −

(u,v)∈E

X

i

X

i

i
!

xuv(i, i)

= X

w(u, v)

1 −

(u,v)∈E

(cid:18) xu(i) + xv(i)
2

|xu(i) − xv(i)|
2

−

X

i
(cid:19)!

!

min(xu(i), xv(i))

= X

w(u, v)

(u,v)∈E

  1
2

X

i

!

|xu(i) − xv(i)|

Here we used the deﬁnition of L∗(G) and the facts that P
(u, v) ∈ E, i ∈ [k].

i xu(i) = 1 for all (u, i) and P

j xuv(i, j) = xu(i) for all

Thus, for any x ∈ L∗(G), the objective of the local LP can be written as

X

X

c(u, i)xu(i) + X

w(u, v)d(u, v)

u∈V

i

(u,v)∈E

P

where d(u, v) := 1
i |xu(i) − xv(i)|. This is the objective function of another LP relaxation for uniform metric
2
labeling called the “metric LP”, which is equivalent to the local LP (Archer et al., 2004). Note that both ¯x and ˆx
are in L∗(G) by Claim 3.3. Therefore, the objective function can be written in the above form for both of them.

In the next section, we introduce a rounding algorithm and prove some guarantees for the random solutions
h output by it. We then use these guarantees to show an upper bound on the expected cost of these random
solutions in a perturbed instance of the problem. Finally, we use this upper bound to prove that ˆx = ¯x.

B.1 ε-close rounding:

Given any feasible solution x ∈ L(G) and a valid labeling ¯x, we construct a related feasible solution x0 which is
ε-close to ¯x in the ‘∞-norm i.e. kx0 − ¯xk∞ ≤ ε:

x0 = εx + (1 − ε)¯x,

(12)

 
 
 
Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

where ε < 1/k and we have identiﬁed the labeling ¯x : V → L with its corresponding vertex of the marginal
polytope (a vector in {0, 1}nk+mk2). We consider the following rounding algorithm applied to x0, which is a
modiﬁed version of the ε-close rounding algorithm used in Lang et al. (2018):

Algorithm 1 ε-close rounding
1: Choose i ∈ {1, . . . , k} uniformly at random.
2: Choose r ∈ (0, 1/k) uniformly at random.
3: Initialize labeling h : V → [k].
4: for each u ∈ V do
5:
6:
7:
8:
end if
9:
10: end for
11: Return h

Set h(u) = ¯x(u)

Set h(u) = i.

(i) > r then

if x0
u

else

Lemma B.3 (Rounding guarantees). Given any x0 constructed using (12), the labeling h output by Algorithm 1
satisﬁes the following guarantees:

P { h(u) = i } = x0
u
P { h(u) 6= h(v) } ≤ 2d(u, v)
P { h(u) = h(v) } = (1 − d(u, v))

(i)

∀ u ∈ V, i ∈ [k]
∀ (u, v) ∈ E : ¯x(u) = ¯x(v)
∀ (u, v) ∈ E : ¯x(u) 6= ¯x(v),

where d(u, v) = 1
2

P

i |x0
u

(i) − x0
v

(i)| is the edge separation of the constructed feasible point x0.

Proof of Lemma B.3 (rounding guarantees). First, ﬁx u ∈ V and a label i 6= ¯x(u). We output h(u) = i precisely
x0
u(i)
(i) ≤ ε <
when i is chosen and 0 < r < x0
u
1/k
1/k for all i 6= ¯x(u)). Now we output h(u) = ¯x(u) with probability 1−P
(j) =
x0
u
For the second, consider an edge (u, v) not cut by ¯x, so ¯x(u) = ¯x(v). Then (u, v) is cut by h when some i 6= ¯x(u)
is chosen and r falls between x0
u

(i) (we used here that x0
u
j6=¯x(u) x0
u

= x0
u
P { h(u) = j } = 1−P

(i), which occurs with probability 1
k

(i) = 1. This proves the ﬁrst guarantee.

(i). This occurs with probability

(¯x(u)), since P

(i) and x0
v

i x0
u

j6=¯x(u)

1

k

X

max(x0
u

(i), x0
v

i6=¯x(u)

(i)) − min(x0
u

1/k

(i), x0
v

(i))

= X

i6=¯x(u)

|x0
u

(i) − x0
v

(i)| ≤ 2d(u, v).

Finally, consider an edge (u, v) cut by ¯x, so that ¯x(u) 6= ¯x(v). Here h(u) = h(v) if some i, r are chosen with
(i)) with probability min(x0
r < min(x0
. Note that this is still valid
u
if i = ¯x(u) or i = ¯x(v), since only one of those equalities can hold. So we get

(i)). We have r < min(x0
u

u(i),x0
1/k

(i), x0
v

(i), x0
v

v(i))

1

k

X

i

(i), x0
min(x0
v
u
1/k

(i))

=

1
2

X

i

x0
u

(i) + x0
v

(i) − |x0
u

(i) − x0
v

(i)|

= 1 − d(u, v),

!

where we used again that P

i x0
u

(i) = 1.

Given these rounding guarantees, we can relate the expected cost diﬀerence between h and ¯x in a perturbation of
the original instance to the cost diﬀerence between x and ¯x in the original instance. We are only interested in the
case when x ∈ L∗(G) and so the objective function f (x) = P

(u,v)∈E w(u, v)d(u, v).

i c(u, i)xu(i) + P

P

u∈V

 
Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

B.2 Using the rounding guarantees

Lemma B.4. Given an integer solution ¯x, a feasible LP solution x ∈ L∗(G), and a random output h of Algorithm
1 on an input x0 = εx + (1 − ε)¯x, deﬁne

w0(u, v) =

( 1

2 w(u, v)
w(u, v)

¯x(u) = ¯x(v)
¯x(u) 6= ¯x(v)

and let f 0(y) = P
P
costs, but using weights w0. Here d(y, u, v) = 1
2
this perturbed objective between h and ¯x. Then,

i c(u, i)yu(i) + P

u∈V

(u,v)∈E w0(u, v)d(y, u, v) be the objective in the instance with the original
i |yu(i) − yv(i)|. Let Ah := f 0(h) − f 0(¯x) be the diﬀerence in

P

E[Ah] = E [f 0(h) − f 0(¯x)] ≤ ε · (f (x) − f (¯x)) .

Proof.

E [f 0(h) − f 0(¯x)] = X

X

c(u, i)P { h(u) = i } −

X

c(u, ¯x(u)) + X

w0(u, v)P { h(u) 6= h(v) }

u∈V

i
X

−

u∈V

uv:¯x(u)=¯x(v)

w0(u, v)P { h(u) = h(v) }

uv:¯x(u)6=¯x(v)
X

c(u, i)x0
u

i
X

= X
u

−

= X
u

uv:¯x(u)6=¯x(v)
X

c(u, i)x0
u

i
= f (x0) − f (¯x).

(i) −

X

c(u, ¯x(u)) + X

2w0(u, v)d(x0, u, v)

u

uv:¯x(u)=¯x(v)

w0(u, v)(1 − d(x0, u, v))

(i) −

X

c(u, ¯x(u)) + X

w(u, v)d(x0, u, v) −

X

w(u, v)

u

uv∈E

uv:¯x(u)6=¯x(v)

where the second-to-last equality used the deﬁnition of w0 (note that w0 is identical to the worst-case perturbation
wadv for ¯x). Because f is convex (in particular, d(x, u, v) is convex in x), we have f (x0) ≤ εf (x) + (1 − ε)f (¯x).
Therefore,

E [f 0(h) − f 0(¯x)] ≤ εf (x) + (1 − ε)f (¯x) − f (¯x) = ε(f (x) − f (¯x)),

which is what we wanted.

B.3 Final proof of Theorem 4.2:

Suppose the local LP solution ˆx is not the same as the MAP solution ¯x i.e. ˆx 6= ¯x. Consider x0 = εˆx + (1 − ε)¯x
where 0 < ε < 1/k (see equation (12)). Let h be the random integer solution output by using Algorithm 1 on x0.
By Lemma B.4, we have

E [f 0(h) − f 0(¯x)] ≤ ε · (f (ˆx) − f (¯x))

We note that any solution h that we get from rounding x0 is either ¯x or an expansion move of ¯x. This is because
we pick only a single label i in step 1 of Algorithm 1 and label all vertices u either i or ¯x(u). Therefore, for the i
picked in step 1, h is an i-expansion of ¯x if h 6= ¯x.

E [f 0(h) − f 0(¯x)] = E [f 0(h) − f 0(¯x)|h 6= ¯x] P[h 6= ¯x] + E [f 0(h) − f 0(¯x)|h = ¯x] P[h = ¯x]

= E [f 0(h) − f 0(¯x)|h 6= ¯x] P[h 6= ¯x]

Since (G, c, w) is a (2, 1)-expansion stable instance, we know that f 0(h) > f 0(¯x) when h 6= ¯x since all h in the
support of the rounding (other than ¯x) are expansion moves of ¯x and we get f 0 by a valid (2, 1)-perturbation of
(G, c, w). Therefore, E [f 0(h) − f 0(¯x)|h 6= ¯x] > 0. We also have that P[h 6= ¯x] > 0 since we assumed that ˆx 6= ¯x.
Therefore, E [f 0(h) − f 0(¯x)] > 0. But we know that f (ˆx) − f (¯x) ≤ 0 since ˆx is the minimizer of f (x) among all
feasible x ∈ L(G). So Lemma B.4 implies E [f 0(h) − f 0(¯x)] ≤ 0. Thus we have a contradiction and so the local
LP solution ˆx has to be the same as the MAP solution ¯x.

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

C Stability and Curvature around MAP solution: details

Theorem 5.2. Let (G, c, w) be a (2, 1, ψ)-expansion stable instance with MAP solution ¯x. Let θ = (c, w). Then
for any x ∈ L∗(G), the recovery error (see Def. 3.1) satisﬁes

1
2 kx − ¯xk1 :=

1
2 kxV − ¯xV k1 ≤

1

ψ

|hθ, xi − hθ, ¯xi|.

(2)

Here, we provide two proofs for this theorem, one deals directly with the local LP relaxation and the other uses
the dual of the relaxation. The dual proof is more general than the primal proof as it works for all x ∈ L(G), not
just for those in L∗(G).

C.1 Primal-based proof

Proof. For any x ∈ L∗(G), consider a feasible solution x0 which is ε-close to ¯x constructed using Equation 12 i.e.
x0 = εx + (1 − ε)¯x. Let h be the random solution output by Algorithm 1 on x0.

Lemma C.1 (Bound for E [Bh]). For any h in the support of the rounding of x0 = εx + (1 − ε)¯x, let us deﬁne
Bh to be the number of vertices which it labels diﬀerently from ¯x. In other words, it is the number of vertices
which are misclassiﬁed by h i.e. Bh := P
1[h(u) 6= ¯x(u)]. Then,

u∈V

E [Bh] = ε

1
2 kxu − ¯xuk1

X

u∈V

Proof.

E[Bh] = X

E[1[h(u) 6= ¯x(u)]] = X

P { h(u) 6= ¯x(u) } = X

1 − P { h(u) = ¯x(u) }

u∈V
= X
u∈V

1 − x0
u

(¯x(u)) = X

1 − (εxu(¯x(u)) + (1 − ε)) = X

ε (1 − xu(¯x(u)))

u∈V

u∈V

u∈V

u∈V

= ε

1
2

X

u∈V


1 − xu(¯x(u)) + X


 = ε

xu(i)

i6=¯x(u)

1
2 kxu − ¯xuk1

X

u∈V

Here, we used the fact that for all u ∈ V, ¯xu(¯x(u)) = 1 and ¯xu(i) = 0 ∀ i 6= ¯x(u) and since x is a feasible solution
to the LP, it satisﬁes P

i6=¯x(u) xu(i) = 1 − xu(¯x(u)) for all u ∈ V .

Lemma C.2 (Lower bound for Ah using (2, 1, ψ)-expansion stability). If (G, w, c) is a (2, 1, ψ)-expansion stable
instance, then for any h in the support of the rounding of x0 = εx + (1 − ε)¯x,

Ah ≥ ψ · Bh

where Ah = f 0(h) − f 0(¯x) and f 0 is the objective in the instance (G, c, w0) where w0 is the worst (2, 1) perturbation
for ¯x i.e.

w0(u, v) =

( 1

2 w(u, v)
w(u, v)

¯x(u) = ¯x(v)
¯x(u) 6= ¯x(v)

Proof. Note that Ah here is the same as the one deﬁned for Lemma B.4. Since the instance (G, c, w) is (2, 1, ψ)-
expansion stable, we know that (G, c0, w) should be (2, 1)-expansion stable for all c0 such that c ≤ c0 ≤ c + ψ · 1.

Consider the worst c0 for ¯x i.e. c0(u, i) =

(

c(u, i) + ψ i = ¯x(u)
i 6= ¯x(u)
c(u, i)

. Let f 00 be the objective in the instance (G, c0, w0).

As discussed in section B.3, we know that any h 6= ¯x in the support of the rounding is an expansion move of ¯x.
Therefore, for any h 6= ¯x in the support of the rounding of x0,

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

f 00(h) − f 00(¯x) > 0 =⇒

X

c0(u, h(u)) − c0(u, ¯x(u)) + X

w0(u, v) −

X

w0(u, v) > 0

=⇒

X

c(u, h(u)) − (c(u, ¯x(u)) + ψ · 1[h(u) 6= ¯x(u)]) + X

w0(u, v) −

X

w0(u, v) > 0

u∈V

(u,v):h(u)6=h(v)

(u,v):¯x(u)6=¯x(v)

u∈V

=⇒ f 0(h) − f 0(g) > ψ ·

X

u∈V

1[h(u) 6= ¯x(u)] =⇒ Ah > ψ · Bh.

(u,v):h(u)6=h(v)

(u,v):¯x(u)6=¯x(v)

This is true for all h 6= ¯x in the support of the rounding of x0. When h = ¯x, we have Ah = Bh = 0. Therefore for
all h in the support of the rounding of x0, we have that Ah ≥ ψ · Bh.

C.2 Final proof of Theorem 5.2:

We use the Lemmas B.4(upper bound for E[Ah]), C.2(lower bound for Ah), and C.1(bound for E[Bh]) to prove
Theorem 5.2. For all h in the support of rounding of x, Ah ≥ ψ · Bh. Also,

E[Ah] ≤ ε (f (x) − f (¯x)) , E[Bh] = ε

1
2 kxu − ¯xuk1

X

u∈V

Suppose that kx − ¯xk1 > τ · (f (x) − f (¯x)). Then,

E[Ah]
E[Bh] ≤

f (x) − f (¯x)

P

1

2 kxu − ¯xuk1

u∈V

<

2

τ

But since Ah ≥ ψ · Bh for every h in the rounding of x, we get that
, we get a contradiction and thus we get,
Setting τ = 2
ψ

E[Ah]
E[Bh] ≥ ψ.

1
2 kx − ¯xk1 ≤

1

ψ

· (f (x) − f (¯x)) =

1

ψ

· (hθ, xi − hθ, ¯xi)

Corollary 5.3 (LP solution is good if there is a nearby stable instance). Let ˆxM AP and ˆx be the MAP and local
LP solutions to an observed instance (G, ˆc, ˆw). Also, let ¯x be the MAP solution for a latent (2, 1, ψ)-expansion
stable instance (G, ¯c, ¯w). If ˆθ = (ˆc, ˆw) and ¯θ = (¯c, ¯w),

1
2 kˆxV − ˆxM AP

V

k1 ≤

2d(ˆθ, ¯θ)
ψ

+

1
2 kˆxM AP

V

− ¯xV k1.

Proof of Corollary 5.3. First, we note that for the nearby stable instance, the MAP and the local LP solutions
are the same due to Theorem 4.2. Therefore, for any feasible solution x ∈ L(G), h¯θ, xi ≥ h¯θ, ¯xi. In particular,
this implies that h¯θ, ˆxi ≥ h¯θ, ¯xi and h¯θ, ˆxM AP i ≥ h¯θ, ¯xi since ˆx, ˆxM AP are also feasible solutions. Remember that
we deﬁned d(ˆθ, ¯θ) := sup

x∈L∗(G) |hˆθ, xi − h¯θ, xi|. Therefore,
h¯θ, ˆxi ≤ hˆθ, ˆxi + d(ˆθ, ¯θ) ≤ hˆθ, ¯xi + d(ˆθ, ¯θ) ≤ h¯θ, ¯xi + 2d(ˆθ, ¯θ).
The ﬁrst and third inequalities hold due to the deﬁnition of d(ˆθ, ¯θ). The second inequality follows from the fact
that ˆx is the minimizer for hˆθ, xi among x ∈ L(G). Thus, 0 ≤ h¯θ, ˆxi − h¯θ, ¯xi ≤ 2d(ˆθ, ¯θ). From Theorem 5.2, we
get 1

. Thus,

2 kˆxV − ¯xV k1 ≤ 2d(ˆθ,¯θ)

ψ

1
2 kˆxV − ˆxM AP

V

k1 ≤

≤

1
2 kˆxV − ¯xV k1 +
2d(ˆθ, ¯θ)
ψ

+

1
2 kˆxM AP

V

1
2 kˆxM AP

V

− ¯xV k1

− ¯xV k1.

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

C.3 Dual-based proof

Here we provide an alternate proof of the curvature result using the dual of the local LP relaxation. First, we show
that the curvature bound is related to the dual margin of the instance. Then we show that (2, 1, ψ)-expansion
stability implies that the dual margin is at least ψ. Throughout this section, we assume the local LP solution ˆx is
unique and integral (as guaranteed, for example, by (2, 1)-expansion stability), so ˆx = ¯x.

Relaxing the local LP’s marginalization constraints in both directions for each edge, we obtain the following
Lagrangian for the local LP:

L(δ, x) = X

X


θu(i) + X


 xu(i) + X

X

δuv(i)

u

i

v∈N (u)

uv

ij

(θuv(i, j) − δuv(i) − δvu(j)) xuv(i, j)

where each xu is constrained to be in the (k − 1)-dimensional simplex, and each xuv the k2 − 1-dimensional
simplex (i.e., the normalization constraints remain). There are no constraints on the dual variables δ. Observe
that for any δ and any primal-feasible x, L(δ, x) = hθ, xi. This gives rise to the reparametrization view: for a
ﬁxed δ, deﬁne θδ
(i, j) = θuv(i, j) − δuv(i) − δvu(j). Then L(δ, x) = hθδ, xi.
u
This will allow us to deﬁne equivalent primal problems with simpler structure than the original. L(δ, x) also gives
the dual function:

v∈N (u) δuv(i), and θδ

(i) = θu(i) + P

uv

D(δ) = min

x

L(δ, x) = X

u

min
i


θu(i) + X


 + X

δuv(i)

v∈N (u)

uv

min
i,j

(θuv(i, j) − δuv(i) − δvu(j)) .

A dual point δ is a dual solution if δ ∈ argmax
δ0 D(δ0). Theorem 4.2 implies that the local LP has a unique,
integral solution when the instance is (2, 1, ψ)-expansion stable. Sontag et al. (2011, Theorem 1.3) show that this
(i) is
implies the existence of a dual solution δ∗ that is locally decodable at all nodes u: for each u, argmin
unique, and moreover, the edge and node dual subproblems agree:

i θδ∗

u

(cid:18)

argmin
i

θδ∗
u

(i), argmin

j

(cid:19)

θδ∗
v

(j)

∈ argmin
i,j

θδ∗
uv

(i, j).

(13)

In this case, the primal solution deﬁned by “decoding” δ∗, x(u) = argmin
et al., 2011).

i θδ∗

u

(i), is the MAP solution (Sontag

For locally decodable δ∗, we deﬁne the node margin ψu(δ∗) at a node u as:

ψu(δ∗) =

min
i6=argminj θδ∗ (j)

θδ∗ (i) − min

j

θδ∗ (j).

This is the diﬀerence between the optimal reparametrized node cost at u and the next-smallest cost. Local
decodability of δ is the property that ψu(δ) > 0 for every u.
Together with (13), the following lemma implies that we need only consider locally decodable dual solutions
where the optimal primal solution pays zero edge cost.
Lemma C.3 (Dual edge “removal”). Given a locally decodable dual solution δ, we can transform it to a locally
decodable dual solution δ0 that satisﬁes mini,j θδ0
uv

(i, j) = 0 and has the same (additive) margin at every node.

Proof. Fix an edge (u, v), and consider any pair i∗, j∗ in argmin
(i∗, j∗) = θuv(i∗, j∗) + ε for
i,j θδ
uv
ε ∈ R. Now deﬁne δ0
(j) = δvu(j) − ε for all j). Because we changed
(i) = δuv(i) − ε for all i (or, equivalently, δ0
vu
(i) by a constant for each i, local decodability is preserved and the additive margin of local decodability is not
θδ
u
changed. We incurred a change of +ε in the dual objective of δ from the edge term mini,j θδ0
(i, j), and a −ε in
uv
the objective from the decrease in the node term mini θδ0
(i), so δ0 is still optimal. We can repeat this process for
u
every edge (u, v).

(i, j). Put θδ
uv

uv

Lemma C.3 implies that when (x∗, δ∗) is a pair of primal/dual optima and δ∗ is locally decodable, we can assume
that L(x∗, δ∗) = P
(i) = 1. That
is, the primal optimum pays no edge cost in the problem reparametrized by the dual opt δ∗. Finally, Lemma

), where we overload notation to deﬁne x∗
u

to be the label for which x∗
u

u θδ∗

(x∗
u

u

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

(i, j) ≥ 0.

C.3 implies that we can always assume that θδ∗
(i, j) ≥ 0 for all (u, v), (i, j). Therefore, if there is any locally
uv
decodable dual solution, and the primal LP solution is integral and unique, we may assume there exists a locally
decodable dual solution δ such that ¯x(u) = argmin
(¯x(u), ¯x(v)) = 0,
and θδ
uv
Lemma C.4 (Dual margin implies curvature around ¯x). For an instance with objective θ and MAP solu-
tion ¯x, assume there exists a locally decodable dual solution δ such that ¯x(u) = argmin
i θδ(i), (¯x(u), ¯x(v)) ∈
argmin
(i, j) ≥ 0. Additionally, let ψ(δ) = minu ψu(δ) be the smallest
node margin. Note that ψ(δ) > 0 because δ is locally decodable. Then for any x ∈ L(G),

i θδ(i), (¯x(u), ¯x(v)) ∈ argmin

(¯x(u), ¯x(v)) = 0, and θδ
uv

(i, j), θδ
uv

(i, j), θδ
uv

i,j θδ
uv

i,j θδ
uv

1
2 ||xV − ¯xV ||1 ≤

hθ, x − ¯xi
ψ(δ)

Proof. Let ∆ = hθ, x − ¯xi. Since x and ¯x are both primal-feasible, we have L(x, δ) = hθ, xi and L(¯x, δ) = hθ, ¯xi.
Therefore,

L(x, δ) = L(¯x, δ) + ∆.

(14)

Because θδ(¯x(u), ¯x(v)) = 0 for all (u, v), we have

L(¯x, δ) = X

θδ
u

(¯x(u)).

u

Additionally, because θδ
uv

(i, j) ≥ 0,

L(x, δ) = X

X

(i)xu(i) + X

θδ
u

X

θδ
uv

(i, j)xuv(i, j) ≥

X

X

θδ
u

(i)xu(i).

u

i

uv

ij

u

i

Combining the above two inequalities with (14) gives:

X

X

u

i

θδ
u

(i)xu(i) ≤

X

u

θδ
u

(¯x(u)) + ∆

(15)

Because δ is locally decodable to ¯x, and the smallest node margin is equal to ψ(δ), we have that for every u,
θδ
u

(i) for all i 6= ¯x(u). The margin condition implies:

(¯x(u)) + ψ(δ) ≤ θδ
u

X

(¯x(u))xu(¯x(u)) + X

θδ
u

X

(θδ
u

(¯x(u)) + ψ(δ))xu(i) <

u

u

i6=¯x(u)

X

X

u

i

θδ
u

(i)xu(i),

and simplifying using P

i xu(i) = 1 gives:

X

θδ
u

(¯x(u)) + ψ(δ) X

X

xu(i) <

X

X

θδ
u

(i)xu(i).

u

u

i6=¯x(u)

u

i

Plugging in to (15) gives:

X

X

xu(i) <

u

i6=¯x(u)

∆
ψ(δ) .

The left-hand-side is precisely ||xV − ¯xV ||1/2.

Now we show that (2, 1, ψ)-expansion stability implies that there exists a locally decodable dual solution δ with
dual margin ψ(δ) ≥ ψ.
Lemma C.5 ((2, 1, ψ)-expansion stability gives a lower bound on dual margin). Let (G, c, w) be a (2, 1, ψ)-
expansion stable instance with ψ > 0. Then there exists a locally decodable dual solution δ with dual margin
ψ(δ) ≥ ψ.

Proof. Deﬁne new costs cψ as

cψ(u, i) =

(

c(u, i) + ψ ¯x(u) = i
c(u, i)

otherwise.

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

By deﬁnition, the instance (G, cψ, w) is (2, 1)-expansion stable (see Deﬁnition 4.1). Theorem 4.2 implies the
pairwise LP solution is unique and integral on (G, cψ, w). This implies there exists a dual solution δ0 that is
locally decodable to ¯x. The only guarantee on the dual margin of δ0 is that ψ(δ0) > 0. But note that δ0 is also
an optimal dual solution for (G, c, w), since its objective in that instance is the same as the objective of ¯x. But in
that instance, the dual margin at every node is at least ψ, because cψ(u, ¯x(u)) − c(u, ¯x(u)) = ψ. So ψ(δ) ≥ ψ.

These two lemmas directly imply Theorem 5.2. This dual proof is slightly more general than the primal proof,
since the curvature result applies to any x ∈ L(G).

D Details for Generative model

Deﬁnition D.1 (sub-Gaussians and (b, σ)-truncated sub-Gaussians). Suppose b ∈ R, σ ∈ R+. A random variable
X with mean µ is sub-Gaussian with parameter σ if and only if E[eλ(X−µ)] ≤ exp(λ2σ2/2) for all λ ∈ R. The
random variable X is (b, σ)-truncated sub-Gaussian if and only if X is supported in (b, ∞) and X is sub-Gaussian
with parameter σ.

We remark that the above deﬁnition captures many well-studied families of bounded random variables e.g.,
Rademacher distributions, uniform distributions on an interval etc. We remark that a bounded random variable
supported on [−M, M ] is also sub-Gaussian with parameter M . However in our setting, it needs to be truncated
only on negative side, and the bound M will be much larger than the variance parameter σ; the bound is solely to
ensure non-negativity of edge costs. A canonical example to keep in mind is a truncated Gaussian distribution.We
use the following standard large deviations bound for sums of sub-Gaussian random variables (for details, refer to
Thm 2.6.2 from Vershynin (2018)). Given independent r.v.s X1, X2, . . . , Xn, with Xi drawn from a sub-Gaussian
with parameter σi we have for µ = Pn

E[Xi] and σ2 = Pn

,

i=1 σ2
i

i=1

P

h(cid:12)
(cid:12)
(cid:12)

n
X

i=1

(cid:12)
i
(cid:12)
(cid:12) ≥ t
Xi − µ

≤ 2 exp

(cid:16)

−

(cid:17)

.

t2
2σ2

(16)

Lemma 6.1 (d(ˆθ, ¯θ) is small w.h.p. ). There exists a universal constant c < 1 such that for any instance in the
above model, with probability at least 1 − o(1),

|hˆθ, xi − h¯θ, xi| ≤ c

sup
x∈L∗(G)

√

v
u
u
t

nk

X

u,i

σ2

u,i

+ k2
4

X

uv

γ2
u,v

Proof. As discussed in section B, for any x ∈ L∗(G), the objective of the local LP can be written as

X

X

c(u, i)xu(i) + X

w(u, v)d(u, v)

u∈V

i

(u,v)∈E

where d(u, v) := 1
2

P

i |xu(i) − xv(i)|. Let ˆf (x) := hˆθ, xi, ¯f (x) := h¯θ, xi. Then,

|hˆθ, xi − h¯θ, xi| = | ˆf (x) − ¯f (x)| =

(cid:12)
(cid:12)
(cid:12)

X

X

˜c(u, i)xu(i) + X

˜w(u, v)d(u, v)

u∈V

i∈L

(u,v)∈E

(cid:12)
(cid:12)
(cid:12)

For any feasible LP solution x, consider the following rounding algorithm R:

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Choose ri ∈ (0, 1) uniformly at random.
for each u ∈ V do

Algorithm 2 R rounding
1: for each i ∈ L do
2:
3:
4:
5:
6:
7:
8:
9:
10: end for

if xu(i) > ri then
R(x)u(i) = 1.

end if
end for

R(x)u(i) = 0

else

Then, we have

E[ ˆf (R(x)) − ¯f (R(x))] = X

X

˜c(u, i) E[1[R(x)u(i) = 1]] + X

˜w(u, v)
2

E[1[R(x)u(i) 6= R(x)v(i)]]

X

i

P[min (xu(i), xv(i)) ≤ ri < max (xu(i), xv(i))]

X

i

u∈V
= X
u∈V
= X
u∈V
= X
u∈V

i∈L

X

i∈L

X

i∈L
X

i∈L

˜c(u, i)P[xu(i) > ri] + X

(u,v)∈E
˜w(u, v)
2

(u,v)∈E
˜w(u, v)
2

˜c(u, i)xu(i) + X

(u,v)∈E

|xu(i) − xv(i)|

X

i

˜c(u, i)xu(i) + X

˜w(u, v)d(u, v) = ˆf (x) − ¯f (x)

(u,v)∈E

Therefore,

sup
x∈L∗(G)

| ˆf (x) − ¯f (x)| = sup

| E[ ˆf (R(x)) − ¯f (R(x))]| ≤

x∈L∗(G)

sup
ˆxV ∈{0,1}nk

| ˆf (ˆxV ) − ¯f (ˆxV )|

Note that for all x ∈ L∗(G), ˆf (x) and ¯f (x) only depend on the portion of x restricted to the vertices i.e. xV .
This is why we only need to look at ˆxv ∈ {0, 1}nk for the last inequality.
For any ﬁxed ˆxV ∈ {0, 1}nk, since ˜w(u, v), ˜c(u, i) are all mean 0 and sub-Gaussian with parameters γu,v, σu,i, we
have for any t > 0,

P

i
h
| ˆf (ˆxV ) − ¯f (ˆx)| > t

≤ 2 exp





(cid:16)P

2

u,i σ2
u,i

−t2
+ k2/4 P

uv γ2

u,v





(cid:17)

Taking t = c

√

nk

qP

u,i σ2
u,i

+ k2/4 P

uv γ2

u,v

, we get that for any ﬁxed ˆxV ∈ {0, 1}nk,

i
h
| ˆf (ˆx) − ¯f (ˆx)| > t

P

≤ 2 exp (cid:0)−c2nk(cid:1)

Taking a union bound over {0, 1}nk, we get that

"

P

sup
ˆxV ∈{0,1}nk

#
| ˆf (ˆx) − ¯f (ˆx)| > t

≤ 2 exp (cid:0)nk (cid:0)log 2 − c2(cid:1)(cid:1)

Here, c needs to be greater than

√

ln 2 ≈ 0.83 to get a high probability guarantee.

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

Corollary 6.3 (MAP solution recovery for regular graphs ). Suppose we have a d-regular graph G with γ2
for all edges (u, v), and σ2
u,i
and η of the edges are subject to the noise. With high probability over the random noise,

= γ2
= σ2 for all vertex-label pairs (u, i). Also, suppose only a fraction ρ of the vertices

u,v

kˆxV − ¯xV k1
2n

≤

2ck

q

ρσ2 + ηdk

8 γ2

ψ

Proof. From Theorem 6.2, we have that, with high probability over the random noise

1
2 kˆxV − ¯xV k1 ≤

2

ψ

√

· c

nk ·

v
u
u
t

X

u,i

σ2

u,i

+ k2
4

X

uv

γ2
u,v

In this setting, this leads to

√

· c

nk ·

2

ψ

v
u
u
t

X

u,i

σ2

u,i

+ k2
4

X

uv

γ2
u,v

=

√

· c

nk ·

2

ψ

r

ρnkσ2 + η

k2
4

nd
2 γ2 =

q

2cnk

ρσ2 + ηdk

8 γ2

ψ

since |V | = n, |L| = k, and |E| = nd
2

.

E Algorithm for ﬁnding nearby stable instances details

Let ¯x be a MAP solution, and let E ¯x be the set of expansions of ¯x. We prove that an instance is (2, 1, ψ)-expansion
stable if and only if hθadv, ¯xi ≤ hθadv, xi for all x ∈ E ¯x. In other words, it is suﬃcient to check for stability in the
adversarial perturbation for ¯x. This proves that we need not check every possible perturbation when ﬁnding a
(2, 1, ψ)-expansion stable instance.
Claim E.1. Let (G, c, w) be an instance of uniform metric labeling with MAP solution ¯x. Deﬁne:

wadv(u, v) =

( 1

2 wuv
w(u, v)

¯x(u) = ¯x(v)
¯x(u) 6= ¯x(v)

cadv(u, i) =

(

c(u, i) + ψ ¯x(u) = i
c(u, i)

otherwise.

Let θadv be the objective vector in the instance (G, cadv, wadv). Then

hθ0, ¯xi ≤ hθ0, xi

for all (2, 1, ψ)-perturbations θ0 of θ and all x ∈ E ¯x if and only if:

hθadv, ¯xi ≤ hθadv, xi

for all x ∈ E ¯x.

Proof. The proof is analogous to that of Claim B.1. If the instance is (2, 1, ψ)-expansion stable, then hθ0, xi −
hθ0, ¯xi ≥ 0 for all (2, 1, ψ)-perturbations θ0 and all expansions x of ¯x. Because θadv is a valid (2, 1, ψ)-perturbation,
this gives one direction. For the other, note that if the instance is not (2, 1, ψ)-expansion stable, there exists a θ0
and an x ∈ E ¯x for which hθ0, xi−hθ0, ¯xi < 0. A direct computation shows that hθ0, xi−hθ0, ¯xi ≥ hθadv, xi−hθadv, ¯xi
for all (2, 1, ψ)-perturbations θ0 of θ. Then we have hθadv, xi − hθadv, ¯xi < 0.

This claim justiﬁes (5), which only enforces that ¯x is at least as good as all of its expansions in θadv. The following
claim implies that there is always a feasible point of (5) that makes modiﬁcations of bounded size to c and w.

Beyond Perturbation Stability: LP Recovery Guarantees for MAP Inference on Noisy Stable Instances

Claim E.2. Consider an instance (G, c, w) with a unique MAP solution ¯x. Let w0 be deﬁned as

and let c0 be deﬁned as

w0(u, v) =

(

w(u, v)
2w(u, v)

¯x(u) 6= ¯x(v)
¯x(u) = ¯x(v),

c0(u, i) =

(

c(u, i) − ψ ¯x(u) = i
¯x(u) 6= i.
c(u, i)

Then the instance (G, c0, w0) is (2, 1, ψ)-expansion stable with MAP solution ¯x.

Proof (sketch). The original MAP solution ¯x is also the MAP solution to (G, c0, w0). Then the original instance
(G, c, w) is obtained from (G, c0, w0) by performing the adversarial (2, 1, ψ)-perturbation for ¯x (see Claim E.1).
Because ¯x was the unique MAP solution to this instance, it has better objective than all of its expansions.
Therefore, (G, c0, w0) is (2, 1, ψ)-expansion stable, by Claim E.1.

(G, c0, w0) is a “nearby” stable instance to (G, c, w), but it requires changes to quite a few edges—every edge
that is not cut by ¯x—and changes the node costs of every vertex. Surprisingly, the stable instances we found in
Section 8 were much closer than (G, c0, w0)—that is, only sparse changes were required to transform the observed
instance (G, c, w) to a (2, 1, ψ)-expansion stable instance.

F Experiment details

In this section, we give more details for the numerical examples for which we evaluate our curvature bound from
Theorem 5.2. We studied instances for stereo vision, where the input is two images taken from slightly oﬀset
locations, and the desired output is the disparity of each pixel location between the two images (this disparity is
inversely proportional to the depth of that pixel). We used the models from Tappen & Freeman (2003) on three
images from the Middlebury stereo dataset (Scharstein & Szeliski, 2002). In this model, G is a grid graph with
one node corresponding to each pixel in one of the images (say, the one taken from the left), the costs c(u, i) are
set using the Birchﬁeld-Tomasi matching costs (Birchﬁeld & Tomasi, 1998), and the edge weights w(u, v) are set
as:

(

w(u, v) =

P × s
s

|I(u) − I(v)| < T
otherwise.

Here I(u) is the intensity of one of the images (again, say the left image) at pixel location u, and we set
(P, T, s) = (2, 50, 4). This is a Potts model. The tsukuba, cones, and venus images were 120 x 150, 125 x 150,
and 383 x 434, respectively. These models had k = 7, k = 5, and k = 5, respectively.
To generate Table 1, we ran the algorithm in (5) using Gurobi (Gurobi Optimization, 2020) for the L1 distance. For
each observed instance (G, ˆc, ˆw), this output a nearby (2, 1, ψ)-stable instance (G, ¯c, ¯w). In all of our experiments,
we used ψ = 1. Additionally, we always set the target MAP solution xt in (5) to be equal to the observed MAP
solution ˆxM AP . To evaluate our recovery bound, we compared the objective of the observed LP solution ˆx to the
ˆxM AP in (G, ¯c, ¯w). That is, if ¯θ is the objective for (G, ¯c, ¯w), we computed h¯θ, ˆxi − h¯θ, ˆxM AP i = h¯θ, ˆxi − h¯θ, ¯xi,
where the second equality is because we set the target solution xt to be equal to ˆxM AP , so ˆxM AP = ¯x. Because
ψ = 1, the diﬀerence between these two objectives is precisely the value of our curvature bound. In particular,
Theorem 5.2 guarantees that

||ˆxV − ˆxM AP

V

||1 ≤

1

(cid:0)h¯θ, ˆxi − h¯θ, ˆxM AP i(cid:1) .

1
2n

n
The right-hand-side is shown for these instances in the “Recovery error bound” column of Table 1, and the true
value of 1
||1 (i.e., the true recovery error) is shown in the identically titled column of Table 1. On
(cid:0)h¯θ, ˆxi − h¯θ, ˆxM AP i(cid:1) is close to 0, so our curvature bound “explains” a large portion of ˆx’s
these instances, 1
n
recovery of ˆxM AP . These instances are close to (2, 1, ψ)-stable instances where ˆx and ˆxM AP have close objective,
and this implies by Theorem 5.2 that ˆx approximately recovers ˆxM AP .

2n ||ˆxV − ˆxM AP

V

However, this result relies on a property of the LP solution ˆx: that it has good objective in the stable instance
discovered by the procedure (5). Compare this to Corollary 5.3, which only depends on properties of the observed

Hunter Lang∗, Aravind Reddy∗, David Sontag, Aravindan Vijayaraghavan

instance ˆθ and the stable instance ¯θ (in particular, some notion of “distance” between them). Given an observed
instance ˆθ and stable instance ¯θ, we can try to compute d(¯θ, ˆθ) from Corollary 5.3 to give a bound that does not
depend on ˆx. Unfortunately, this distance can be large, leading to a bound that can be vacuous (i.e., normalized
Hamming recovery > 1). The following reﬁnement of Corollary 5.3 gives much tighter bounds.
Corollary F.1 (LP solution is good if there is a nearby stable instance, reﬁned). Let ˆxM AP and ˆx be the
MAP and local LP solutions to an observed instance (G, ˆc, ˆw). Also, let ¯x be the MAP solution for a latent
(2, 1, ψ)-expansion stable instance (G, ¯c, ¯w). If ˆθ = (ˆc, ˆw) and ¯θ = (¯c, ¯w), deﬁne

d(¯θ, ˆθ) := sup

h¯θ, xi − h¯θ, ¯xi − (hˆθ, xi − hˆθ, ¯xi).

x∈L∗(G)

Note that while we still use the name d(·, ·), evoking a metric, d is not symmetric. Then

1
2 kˆxV − ˆxM AP

V

k1 ≤

d(¯θ, ˆθ)
ψ

+

1
2 kˆxM AP

V

− ¯xV k1.

Proof. Note:

1
2 kˆxV − ˆxM AP

V

k1 ≤

1
2 kˆxV − ¯xV k1 +

1
2 kˆxM AP

V

− ¯xV k1.

By the deﬁnition of d, for any x ∈ L∗(G),

h¯θ, xi − h¯θ, ¯xi ≤ d(¯θ, ˆθ) + (hˆθ, xi − hˆθ, ¯xi).

Now if we set x = ˆx, the LP solution to the observed instance, we have hˆθ, ˆxi − hˆθ, ¯xi) ≤ 0, so

Theorem 5.2 then implies 1

2 kˆxV − ¯xV k1 ≤ d(¯θ, ˆθ)/ψ, which gives the claim.

h¯θ, ˆxi − h¯θ, ¯xi ≤ d(¯θ, ˆθ).

Given an observed instance ˆθ and a (2, 1, ψ)-expansion stable instance ¯θ output by (5) with ¯xM AP = ˆxM AP , we
can upper bound d(¯θ, ˆθ) by computing

dup(¯θ, ˆθ) := sup

h¯θ, xi − h¯θ, ¯xi − (hˆθ, xi − hˆθ, ¯xi),

x∈L(G)

which is a linear program in x because we relaxed L∗(G) to L(G). Corollary F.1 then implies that the recovery
error of ˆx is at most dup(¯θ, ˆθ)/ψ, which we can compute. Table 2 shows the results of this procedure on two of
the same instances from Table 1 in the “Unconditional bound” column. While the values of this bound are much
larger than the “Curvature bound” of Theorem 5.2, they are much more theoretically appealing, since they only
depend on the diﬀerence between ˆθ and ¯θ rather than on a property of the LP solution ˆx to ˆθ. For Table 2, we
did a grid search for ψ over {1, . . . , 10}; ψ = 4 gave the optimal unconditional bound for both instances. The
diﬀerence in ψ explains the slight diﬀerences between the other columns of Tables 1 and 2.

Table 2: Results from the output of (5) on two stereo vision instances. Curvature bound shows the bound
obtained from Theorem 5.2, which depends on the observed LP solution ˆx. Unconditional bound shows the bound
from the reﬁned version of Corollary 5.3, which depends only on the observed instance and the stable instance.
This “unconditional” bound explains a reasonably large fraction of the LP solution’s recovery for these instances:
because the instance is close to a stable instance, the LP solution approximate recovers the MAP solution.

Instance Costs changed Weights changed Curvature bound Unconditional bound

tsukuba
cones

4.9%
2.81%

2.8%
2.31%

0.0173
0.0137

0.4878
0.2819

||1/2n

||ˆxV − ˆxM AP
V
0.0027
0.0022

