1
2
0
2
c
e
D
3
2

]

B
D
.
s
c
[

1
v
8
3
6
2
1
.
2
1
1
2
:
v
i
X
r
a

RumbleML: program the lakehouse with JSONiq [Scalable Data
Science]

Ghislain Fourny
ETH Zurich
Zurich, Switzerland
ghislain.fourny@inf.ethz.ch

David Dao
ETH Zurich
Zurich, Switzerland
david.dao@inf.ethz.ch

Can Berker Cikis∗
(unaffiliated)
Zurich, Switzerland
canberkerwork@gmail.com

Ce Zhang
ETH Zurich
Zurich, Switzerland
ce.zhang@inf.ethz.ch

Gustavo Alonso
ETH Zurich
Zurich, Switzerland
alonso@inf.ethz.ch

Abstract
Lakehouse systems have reached in the past few years unprece-
dented size and heterogeneity and have been embraced by many
industry players. However, they are often difficult to use as they
lack the declarative language and optimization possibilities of re-
lational engines. This paper introduces RumbleML, a high-level,
declarative library integrated into the RumbleDB engine and with
the JSONiq language. RumbleML allows using a single platform
for data cleaning, data preparation, training, and inference, as well
as management of models and results. It does it using a purely
declarative language (JSONiq) for all these tasks and without any
performance loss over existing platforms (e.g. Spark). The key in-
sights of the design of RumbleML are that training sets, evaluation
sets, and test sets can be represented as homogeneous sequences
of flat objects; that models can be seamlessly embodied in function
items mapping input test sets into prediction-augmented result
sets; and that estimators can be seamlessly embodied in function
items mapping input training sets to models. We argue that this
makes JSONiq a viable and seamless programming language for data
lakehouses across all their features, whether database-related or
machine-learning-related. While lakehouses bring Machine Learn-
ing and Data Wrangling on the same platform, RumbleML also
brings them to the same language, JSONiq. In the paper, we present
the first prototype and compare its performance to Spark show-
ing the benefit of a huge functionality and productivity gain for
cleaning up, normalizing, validating data, feeding it into Machine
Learning pipelines, and analyzing the output, all within the same
system and language and at scale.

PVLDB Reference Format:
Ghislain Fourny, David Dao, Can Berker Cikis, Ce Zhang, and Gustavo
Alonso. RumbleML: program the lakehouse with JSONiq [Scalable Data
Science]. PVLDB, 15(1): XXX-XXX, 2022.
doi:XX.XX/XXX.XX

∗The work was performed during his studies at ETH Zurich.
This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 15, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX

PVLDB Artifact Availability:
The source code, data, and/or other artifacts have been made available at
https://github.com/RumbleDB/rumbleml-experiments.

1 Introduction
Machine learning has been enjoying ever-increasing popularity in
the past years, thanks to the growing availability of data, computing
power, and numerous theoretical breakthroughs, such as Genera-
tive Adversarial Networks (GANs) [15], deep learning [21], and so
on. Machine Learning (ML) is slowly becoming an indispensable
part of database management systems, whether they are databases,
data stores, data warehouses, or data lakes. A new paradigm called
lakehouse has recently been suggested [4] integrating structured
and unstructured data management, as well as ML under the same
platform. Unfortunately, lakehouses still lack a common language,
forcing users to resort to different languages and systems for differ-
ent tasks: data cleaning and preparation, training and inference, as
well as model and result management.

ML workflows frequently follow a conventional pattern. Firstly,
all available data is split into two overarching groups: the training
data and the test data. The training data is used to train a model,
that is, identify the important features in data and assign weights to
them. These weights serve as the basis for predictions. The trained
model’s performance is evaluated by applying it to the test data. The
goal here is to optimize the model to generate correct predictions
on any given dataset while avoiding over-fitting on the training
data. Obtaining a model with satisfactory performance may take a
couple of iterations. Then, it is ready to be applied to production
data to generate predictions.

ML still faces challenges that impede an even broader use [22][3].
First, traditional ML expects clean and highly-structured data input.
Unfortunately, data is often heterogeneous, nested, and messy in
the real world. It requires preparation, clean up and curation, as
well as normalization. These are usually not well addressed in the
design of ML software, and ML practitioners frequently report data
preparation to be their biggest challenge and most time consum-
ing [41].

 
 
 
 
 
 
declare function local:convert($input)
{
annotate(

for $l in unparsed-text-lines($input)
let $tokens := tokenize ($l, " ")
let $left := head($tokens)
let $right := tail($tokens)
let $label := if (contains($left, "indoor"))

then 0
else 1
let $features := {|

for $i at $p in $right
return { string($p) : $i }

|}
return { "label" : $label, "features" : $features },
{

"label" : "string",
"features" : {|

for $i in 1 to 4096
return { string($i) : "double" }

|}

}

)
};
let $training-data := local:convert($training-input)
let $test-data := local:convert($test-input)
let $vector-assembler := get-transformer(

"VectorAssembler",
{

"inputCols" : ["features"],
"outputCol" : "transformedFeatures"

}

)
let $linearsvc := get-estimator(

"LinearSVC",
{

"featuresCol": "transformedFeatures",
"maxIter": 5

}

)
let $pipeline := get-estimator(

"Pipeline",
{

"stages": [$vector-assembler, $linearsvc]

}

)
let $pip := $pipeline($training-data, {})
let $prediction := $pip($test-data, {})
let $total := count($prediction)
return count($prediction[$$.label eq $$.prediction])

div $total

Figure 1: An entire pipeline, from cleaning the messy text
input to computing accuracy, entirely written in JSONiq.

Second, most ML systems require the user to code at a low level
with an awareness of the physical data layout. These frameworks
are commonly built-in imperative languages and do not offer much
help with data cleaning tasks. This situation leaves the user with
two options: either working in the same programming environment
and implementing data cleaning scripts in these iterative languages
or switching to a different programming environment. Users gener-
ally opt for the former option, e.g., with Python. The scripts iterate
over the data and perform conditional formatting that resembles a
chain of if/else statements. This practice has a major drawback in
terms of user productivity: Implementing complicated value map-
pings in imperative scripts increases development and maintenance
costs as they may require advanced programming knowledge and
can be error-prone.

Third, many systems such as TensorFlow [1], Weka [19], Py-
Torch [40] require advanced knowledge of ML and are only acces-
sible to ML experts. There is awareness in the ML community for
the need for a declarative ML approach [6], both in the form of
declarative ML tasks and in the form of declarative ML algorithms.
Fourth, most tools run locally and are thus restricted in the
amount of data that they can process. Among the tools that scale,
spark.ml is often cited as the one enjoying the most popularity in
the open-source world.

As it turns out, these challenges are very well addressed by the
database community when it comes solely to data management [2]
thanks to the widely adopted SQL language [11] for tabular data,
and a newer generation of languages such as JSONiq for nested,
heterogeneous datasets [35]. In this paper, we suggest taking this
avenue further by taking a programming language, JSONiq, to a full
lakehouse system. The training, evaluation, and use of ML models
are seamlessly integrated into the database ecosystem rather than
the other way round. We argue that JSONiq is well suited for a
lakehouse, as it is capable at the same time of (i) performing well
on highly structured datasets, (ii) preparing, cleaning up, curating,
and normalizing the data, and (iii) supporting ML training and
evaluation via its higher-order function feature as shown in this
paper. An example JSONiq program that does all of this is shown
in Figure 1.

To prove that such is the case, we built a first prototype inte-
grating the functionality offered by the spark.ml library into the
distributed RumbleDB engine. RumbleDB is designed to improve
user productivity in parallel processing through the utilization of
the declarative JSONiq language rather than the iterative APIs of
Spark. RumbleML extends this notion into the ML domain by expos-
ing the capabilities of spark.ml. By delegating the computation with
as little overhead as possible, RumbleML has no loss of performance
in comparison to spark.ml. The main design goal of RumbleML is
not to be the fastest ML framework but the most productive for the
user. To implement RumbleML, we did not design a new ML frame-
work from scratch, but rather, we exposed spark.ml as faithfully
as possible. We also did not design a new query language or data
processing engine, but instead, we extended the existing RumbleDB
engine with its JSONiq language.

The successful implementation of RumbleML demonstrates the
feasibility of seamless integration of the spectrum of lakehouse fea-
tures under the same declarative, functional, and high-level query
language, hiding the underlying gluing from the user. While in

2

this paper we focus on declarative ML tasks, RumbleML is also for-
ward compatible with declarative ML algorithms, as function items
can be implemented by users as well and need not be restricted to
spark.ml functionality. Our approach opens the avenue for research
on optimizing the execution further and automatically tuning ML
parameters within this data-independent framework.

2 Background
Spark. Apache Spark [49] is an open-source, parallel process-
ing framework that competently scales for big data applications.
Spark optimizes query execution on top of its fault-tolerant and dis-
tributed nature with, e.g., in-memory caching, reduced I/O require-
ments. On the logical level, Spark primarily manipulates Resilient
Distributed Datasets (RDD), which are collections of arbitrary val-
ues. Unlike MapReduce, the data flow takes the generic form of
Directed Acyclic Graphs (DAG). On the physical level, the DAG is
evaluated with parallelism and batch processing on large clusters.
Spark covers a wide range of workloads such as batch applications,
iterative algorithms, interactive queries, streaming, and graph pro-
cessing.
Spark SQL. Spark also offers a higher abstraction and better perfor-
mance for highly structured data via DataFrames and the language
Spark SQL [50]. Unlike an RDD, a DataFrame is a collection of rows
that all share the same schema, allowing for recursively nested
arrays and structs, similar to Pandas [31]. Physically, the data is
stored in memory in a columnar format. The schema is exploited
with the Catalyst engine for cost-based optimizations and code
generation for query execution.
spark.ml. Spark’s generic DAG-based model also supports itera-
tive computations and is compatible with ML algorithms as they are
typically iterative. Spark supports ML with the spark.ml library [32].
In addition to many ML algorithms, it also offers statistics, optimiza-
tion, and linear-algebra-related primitives. spark.ml aims to make
ML practical, scalable, and easy to use. At a high level, spark.ml
comes out of the box with (i) tools for constructing, evaluating, and
tuning ML pipelines, (ii) Common ML algorithms such as classifica-
tion, regression, clustering, and collaborative filtering, (iii) feature
extraction, transformation, dimensionality reduction, and selection,
(iv) persistence for algorithms, models, and pipelines, (v) utilities
such as linear algebra, statistics, data handling [32].

spark.ml offers a high-level, uniform API for ML algorithms and
featurization. This facilitates the architecture of complete work-
flows while making it easier and simpler to comprehend how
spark.ml components fit together. The main components of spark.ml’s
pipeline API are (i) DataFrames: spark.ml manipulates ML datasets
as structured DataFrames, and a DataFrame can have different
columns storing text, feature vectors, true labels, and predictions.
spark.ml also extends the DataFrame type system with sparse and
dense vectors; (ii) transformers, which are off-the-shelf algorithms
that transform a DataFrame into another DataFrame. E.g., an ML
model is a Transformer that extends a DataFrame with predictions.
(iii) estimators, which are algorithms that are fit on a DataFrame
to produce a fitted model exposed as a transformer; (iv) pipelines,
which chain multiple transformers and estimators into an ML work-
flow; and (v) parameters that tune estimators and transformers with

3

a unified API. An example with the Tokenizer transformers is shown
in Figure 3.
JSONiq. JSONiq is a declarative, functional query language that
manipulates nested and/or heterogeneous datasets seamlessly. The
JSONiq data model (JDM) is based on every value returned by an
expression being a sequence of items. Items can be atomic values, ob-
jects, arrays, or function items. Sequences can be heterogeneous or
homogeneous, for example, objects valid against the same schema.
FLWOR expressions support the entire relational algebra, generaliz-
ing SQL’s SELECT FROM WHERE to denormalized data. JSONiq is
taught at several universities [18][47][13] and is an ideal candidate
to easily clean up, structure, normalize and validate messy data into
DataFrames .
RumbleDB. RumbleDB [35] is a querying engine that interfaces
the power of Apache Spark with the convenience and versatility
of the JSONiq language. Spark SQL only supports structured data,
leaving users to use lower-level RDDs for less structured datasets.
RumbleDB fixes this with a high-level approach across the entire
data normalization spectrum, and both at small and large scales.

3 Data preparation with JSONiq
The biggest challenge met by ML practitioners is data prepara-
tion [41][45] [9]. While there exist libraries for data cleaning and
preparation [26] [36] [7] [43] [44] [27] [8] [34] [14] [42], they often
require the data to already be available in tabular form and perform
tasks such as filling missing values, identity matching, and so on.
However, in reality, messy datasets need not be in tabular form and
will not be readily available in DataFrames (e.g., the Git Archive
dataset [30]). With RumbleML, the data preparation pipeline is sup-
ported as early as with nested, heterogeneous datasets, and even
starting with fully unstructured (text) datasets as can be seen in
the pipeline shown in Figure 2.

In this section, we show that JSONiq is capable of processing
messy datasets (in the sense of nested, heterogeneous, and/or tex-
tual) such as JSON Lines files [29], normalizing them, and validating
them into structured DataFrames, feeding these DataFrames as in-
put training sets and test sets to ML pipelines, and computing met-
rics to evaluate the output. In other systems, this requires different
tools [33][12], because trying to do all of this in the same program
is very cumbersome due to the many impedance mismatches. A
benchmark [17] was also performed to show the limitations and
shortcomings of other query languages and APIs, with a focus on
nestedness and a use case in high-energy physics.

We show a messy sample dataset in Figure 4. This dataset is only
implicitly structured and requires processing and conversion to a
more appropriate, DataFrame-based format. In the current state,
this is typically done with a complex Python script. An example
of the way it could look like when converted to LibSVM is shown
in Figure 5. The libSVM file can then be opened by Spark as a
DataFrame and fed into spark.ml pipelines.

With RumbleDB this all can be achieved with a single JSONiq

program, the one shown in Figure 1.

DataFrames have become very popular in Data Science APIs, not
only in Spark but also in Python (e.g., Pandas). For example, “pan-
das” ranks 32nd among StackOverflow tags at the time of writing
with 226,000+ questions [46], and “dataframe” 66th with 109,000+

D

readFile

tokenize

extractLabel

normalize

validate

vectorize

linear SVM

scorer

u

featurize

ytest

Figure 2: A schematic depicting the RumbleML end-to-end pipeline corresponding to the example code in Figure 1. The
pipeline starts with raw, unstructured data (see Figure 4) that it cleans, normalizes and validates into a DataFrame (yellow).
After the data cleaning stage, a linear support vector model is trained and evaluated (blue)

questions. They can be seen as an extension of the relational model
that supports nested data. DataFrames, like relational tables, have
named columns. Unlike relational tables, however, a DataFrame
column type need not be atomic (first normal form), but can recur-
sively be an array type whose elements are of any DataFrame type;
or a struct type whose keys are strings and whose values are also
arbitrary DataFrame types. While DataFrames support denormal-
ized data because of nestedness, they do not support heterogeneity,
i.e., values of different types in the same column or array. As a
consequence, many datasets, even if they are semi-structured, are
too messy to be opened as a DataFrame. For example, datasets in
the JSON format where the fields might not always have the same
type, be missing, or extraneous, can result in many string columns
containing serialized objects and arrays, pushing the burden onto
the user to parse it back selectively at a low level.

Figure 3: A tokenizer is a transformer that adds a column
containing arrays of tokens obtained from another column.

animal:0.7420,outdoor:0.9710,pet:0.6130,white:0.6790
-4.893 -3.803 -25.799 -34.55 -6.622 -13.547 ...
↩→
animal:0.1234,indoor:0.3413,pet:0.6130,black:0.87534
-8.311 15.133 2.973 -25.972 -11.422 -0.067 ...

↩→

Figure 4: Example of unprocessed text data used to generate
the 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶 dataset.

0 1:-4.893 2:-3.803 3:-25.799 4:-34.55 5:-6.622

6:-13.547 ...

↩→
1 1:-8.311 2:15.133 3:2.973 4:-25.972 5:-11.422

↩→

6:-0.067 ...

Figure 5: Example of the resulting LibSVM format for YFCC.

4

Note that DataFrame support in other implementations than
Apache Spark might behave differently, for example, in Snowflake
[10], DataFrames support heterogeneity with the VARIANT type to
some extent, but there is less support for nestedness (only for JSON
types). DataFrames can thus be seen as a special case of sequences
of objects, where this sequence of objects must have some structure,
e.g., must have been validated against a DataFrame schema.

Figure 6: Mapping a DataFrame to a sequence of object*.
The data corresponds to that processed in Figure 1, and was
cleaned up from the messy textual data shown in Figure 4.

An example showing the conversion between these two anal-
ogous representations is given in Figure 6. The sequence ("foo":1,
"foo": [ "bar"]) would, however, not have any DataFrame equivalent
because the field foo is associated with an integer in the first object,
and with an array in the second object.

RumbleDB leverages this mapping in a way seamless and trans-
parent to the user, as follows. JSONiq’s sequences of items at the log-
ical level have five different implementations at the physical level:
(i) directly returning an Item via a Java method call, (ii) volcano-
style iteration on Items, (iii) Spark RDD of Items, (iv) DataFrames
and (v) conversion to native Spark SQL queries. DataFrames effi-
ciently implement the special case where the sequence of items is a
homogeneous sequence of objects that are all valid against the same
schema. ML tasks, however, only work on DataFrames, i.e., on a
homogeneous sequence of valid objects. Thus, in JSONiq, cleaning
up and preparing the data corresponds to using the language to
transform the original messy sequence of items to a fully structured
sequence of valid objects. RumbleDB will automatically understand
that it can implement the latter internally as a DataFrame.

Figure 1 shows how, in JSONiq, it is possible to open our previous
example (in parallel), and structure every record to fit the schema,
finally validating it to an output DataFrame. The output can either
be written back to a structured format such as Parquet or Avro, or
can be further used in more JSONiq code, for example, fed into ML
pipelines also as shown in Figure 1.

RumbleDB was extended to support the JSound [24] schema
language. JSound was designed to be formally precise, while sim-
ple, and supports object and array subtypes with clean semantics.

JSound exists in a compact syntax for 90% of the cases, and a more
verbose syntax for the remaining ones. The compact syntax is de-
signed to seamlessly map, in most cases, to a DataFrame schema.
An example of schema compatible with DataFrames is shown in
Figure 1, dynamically built as the second parameter of the annotate
function. In other words, it is straightforward, when validating
a sequence of items against this schema, for RumbleDB to store
the resulting sequence in a DataFrame internally. Figure 7 shows
the mapping between JSONiq types and Spark DataFrame schema
types. RumbleDB was also extended to support the static definition
of named, user-defined types that can also be used for static type
analysis and further optimizations.

JSONiq type
byte
short
int
long
boolean
double
float
decimal
string
null
date
dateTime
date
hexBinary
array (or subtypes)
object (or subtypes)

DataFrame type
ByteType
ShortType
IntegerType
LongType
BooleanType
DoubleType
FloatType
DecimalType
StringType
NullType
DateType
TimestampType
DateType
BinaryType
ArrayType
StructType

Figure 7: Mapping of JSONiq types to Spark DataFrame types
upon validation.

The generated SparkSQL DataFrame schema serves as the blue-
print for mapping input Object Items into DataFrame rows. If the
input sequence is physically an RDD or a DataFrame, the validation
and mapping to a DataFrame are done in parallel. If it is a local
(possibly non-materialized) sequence, then the DataFrame is newly
created.

index

D

tokenize

contain

convert

D𝑐𝑙𝑒𝑎𝑛

Figure 8: A schematic depicting the data cleaning operations
required to convert our raw data to 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶.

Finally, Figure 1 shows how JSONiq also seamlessly handles
purely textual data in order to convert a messy textual file (from
the real world) and validate it to a DataFrame format.

Further examples of PySpark/spark.ml pipelines involving both
data preparation and ML that would be also covered well by Rum-
bleML can be found here [38].

5

4 The RumbleML data model
The RumbleML framework aims to cover the full capabilities of the
spark.ml library while providing a declarative API with JSONiq.
The core idea of implementing RumbleDB as a highly expressive
and performant facade to Spark is extended into the ML domain.
RumbleML’s data model is based on (i) manipulating structured
datasets as validated JSONiq sequences of objects, internally imple-
mented as DataFrames; (ii) manipulating transformers and estima-
tors to JSONiq function items, internally encapsulating spark.ml
transformers and estimators; (iii) mapping the action of fitting and
predicting to JSONiq dynamic function calls, internally forwarded
to spark.ml method calls, with the output wrapped back into a
JSONiq data model instance; and (iv) passing parameters to es-
timators and transformers, either at their creation or during the
fitting/transforming phase, as JSONiq objects, internally converted
to spark.ml ParamMaps.
Transformers, estimators as function items. A central contri-
bution of this paper is showing that the principal components of
spark.ml, namely estimators and transformers, map seamlessly to
JSONiq function items.

Estimators and transformers are standardized according to the
spark.ml pipeline API with specific methods for triggering exe-
cution. These methods are fit() for estimators and transform() for
transformers. They share an identical signature for their arguments:
The first argument is a DataFrame that serves as the input dataset,
while the second is a spark.ml parameter map that contains a col-
lection of ML parameters for tuning the operation. While their pa-
rameters are identical, return types of fit() and transform() methods
are fundamentally different. A transformer is essentially a simple
map function on an input dataset that returns an altered dataset.
An estimator is, at the logical level, a higher-order function that
returns a transformer, which is another function.

In the JSONiq data model, function items exhibit the standard
nature of a function as they possess a name, parameters, a body
to execute, and a return value. The executable body is the crux of
function items as it can encapsulate any complex logic. For Rum-
bleML, this executable body encapsulates estimator or transformer
execution by calling, under the hood, the fit() or transform() method
transparently to the user. Such a function item also accepts two
parameters, which are forwarded to fit() or transform() method.
Furthermore, a function item can have any sequence of items as
its return type; for function items encapsulating estimators and
transformers, the return type is more specifically a sequence of
objects that, internally, is implemented as the DataFrame returned
by spark.ml. This design harmoniously delegates computation to
spark.ml under the hood, while exposing a functional and elegant
API to RumbleDB users.

Figure 1 shows code where both an estimator and transformer

are created and then invoked on a training or test set.
Creating estimators and transformers. Next, we explain how to
create the function items that encapsulate estimators and transform-
ers. In our first version of RumbleML, we chose to make it possible
to look up an estimator or transformer by its name in spark.ml, in
order to immediately support the entire spectrum of spark.ml fea-
tures while being forward-compatible with future Spark versions.

Native Java Type RumbleDB item type
DataFrame
ParamMap
boolean
double[][]
double[]
double
float
int[]
int
long
String[]
String
Matrix
Vector
Transformer
Estimator

object*
object
boolean
[ [ "double"] ]
[ "double"]
double
double
[ "integer"]
integer
double
[ "string"]
string
[ [ "double"] ]
[ "double"]
function(object*, object) as object*
function(object*,
tion(object*, object) as object*

object)

as

func-

Figure 9: spark.ml native Java parameter types and their cor-
responding item types in JSONiq. Array types are expressed
with the JSound compact schema syntax, supported by Rum-
bleDB, for readability.

To this effect, RumbleML provides two static built-in functions get-
estimator and get-transformer that can be seen in action in Figure
1.

In a future release of RumbleML, we consider increasing data
independence by providing a library module with functions that pro-
vide a standard list of all estimators and transformers with names
documented in RumbleML; these can then be mapped not only to
spark.ml, but any other backend depending on its specific support,
decoupling the code from the specific distributed ML backend used.
Parameter maps as objects. In spark.ml the tuning parameters
for estimators and transformers are exposed as a ParamMap ob-
ject, which has a key-value format. This representation naturally
maps to JSON objects. ParamMap objects exposed in strongly typed
languages such as Java accommodate a variety of types. JSONiq
being strongly typed, the types of almost all parameters exposed
in spark.ml’s parameter API map naturally to item types in the
JDM, as shown in Figure 9. A few rare spark.ml parameters are
not supported at the time of this paper. This decision is motivated
by the fact that they appear in as little as 1% of the collection of
estimators and transformers offered by spark.ml.

5 Implementation
Let us first summarize the general architecture of RumbleDB. After
parsing, translating, and code generation, the executable code has
the form of a tree of runtime iterators. Coarsely, each runtime
iterator corresponds to one logical JSONiq expression, although
this correspondence is not exact for optimization purposes.

The runtime execution is triggered at the root iterator and re-
cursively propagates to child iterators. At each level, RumbleDB
switches seamlessly back and forth between all five execution

6

modes presented in Section 4, based on the strategy decided dur-
ing static analysis. The execution is lowered or lifted based on
the wishes (via the API call) of the consuming iterator and the
capabilities of the producing iterator.
Static and dynamic function calls. RumbleML embraces func-
tions for estimators and transformers. We thus extended RumbleDB
with support for user-defined functions and function items, for
static and dynamic function calls, and function name references.
These features follow already established W3C standards, namely
XQuery 3.0, and are a part of JSONiq.

An important challenge is how to select the execution modes
for each static function call. Currently, we opt for the assignment,
statically, of a specific execution mode to each function, recursively
based on the statically known execution mode of each actual pa-
rameter and on the propagation of execution modes to the function
body. In case of conflicts, a local execution mode takes precedence.
Because of cycles due to JSONiq’s support for recursive calls, this
inference is done in multiple passes until it converges. For built-in
functions, the execution mode is simply looked up in a catalog.

For the ML use case, estimators and transformers are higher-
order functions, and fitting or transforming involves a dynamic
function call. In a dynamic function call, the function is computed
dynamically from an expression of arbitrary complexity that re-
solves to a function item. This implies that the function to execute
and its execution mode are not known until runtime. This is an
undecidable problem, so we use a heuristic in our first prototype.
In most cases, static type analysis will often be able to determine
the signature of the function item being called. If it determines
that the return type of this function item has an arity of one, then
the execution mode is local. This happens in particular when the
function item is an estimator, as it returns the trained model as a
single function item. Otherwise, it may be a transformer and thus,
if the first parameter supports DataFrame execution mode, then
the dynamic function call does as well: we assume it returns the
DataFrame enriched with predictions. Else, its execution is local. If
the compile-time assumption is found to be incorrect at runtime, a
dynamic error is thrown. We found that this heuristic worked in all
our use cases, and will improve it if necessary in future iterations.
Estimator and transformer lookup. RumbleML makes builtin
estimators and transformers available via two builtin functions
"get-estimator()"and "get-transformer()". Both take two parameters:
the name of the desired estimator or transformer, and an object
containing parameters. For an estimator, the return type of the gen-
erated function item is a function type. For a transformer, its return
type is a sequence of objects. In either case, these are singleton
items, so the execution mode is the single-item mode.

RumbleML interprets the first string parameter as the simple
spark.ml class name of the estimator or transformer; it resolves it to
its fully qualified name, which it uses to instantiate it as a Java object
with the Java Reflection API. RumbleML then sets its parameters
with the parameter object supplied as the second parameter of the
lookup function. Parameters are explained in Section 5
Estimator and transformer execution. The body of the func-
tion returned by the lookup is a runtime iterator encapsulating
the spark.ml estimator or transformer. When the function item is
invoked dynamically, the iterator forwards the input to this estima-
tor (fit) or transformer object (transform), letting spark.ml do the

computations and map the output back to a JSONiq value. Section
5 explains how the second parameter is processed An exception is
thrown if the sequence of objects is not physically a DataFrame.
We explained in Section 3 how users can validate datasets with a
JSound schema in order to prepare their data as a DataFrame.

In the case of an estimator, the resulting model, a transformer, is
encapsulated inside a newly generated function item in a similar
way to the lookup of built-in transformers described above. Since
the end result is a single item as determined at compile-time, it
is executed with the single-item execution mode. In the case of
a transformer, the result is a transformed DataFrame, exposed as
a sequence of objects, meaning that the execution mode of this
iterator is DataFrame-based as determined at compile time.
Feature vectors. spark.ml enriches Spark’s DataFrame type system
with dense and sparse vectors to store features. The user creates new
feature columns in these formats by invoking the VectorAssembler
transformer with the name of the additional column, like any other
transformer. Then, this column name is passed as a parameter to
the next transformer or estimator in the pipeline. This is a very
natural workflow for the end ML user, and we also used this in our
experiments. On the logical level, these types are exposed to the
RumbleML user as JSONiq arrays of doubles, materialized only if
necessary.
Estimator and transformer parameters. The parameter object
passed as the second argument to many functions, allows the user
to specify parameters that modify the behavior of fitting or trans-
forming. It is converted by RumbleML to a spark.ml ParamMap that
is passed to the encapsulated spark.ml estimator or transformer.
spark.ml’s ParamMap API is standardized to be shared by all the
estimators and transformers.

RumbleML contains a mapping from each kind of estimator and
transformer to its parameters and the types thereof and validates
that the supplied parameters are valid before setting them, i.e., that
they apply to the estimator or transformer, and that their types
are correct. If the parameters are valid, then each argument value
provided as a JSONiq item is converted to the correct Java type.
Optimizations.

In order to match the performance of an ML pipeline using
spark.ml with an imperative language, several optimizations were
necessary.

One of them was described earlier and is the static choice of exe-
cution mode to detect fitting and transforming to preserve the use
of DataFrames. Indeed, in a previous round of iteration, all dynamic
function calls were executed locally. This forced a materialization
of the DataFrame output by a transformer, which either led to an
error if its size exceed the materialization cap, or to a slower, lo-
cal execution in any code consuming this DataFrame, or to extra
code to re-validate and re-create a DataFrame for use in subsequent
pipelines. The static detection of execution calls solves all three
issues in the majority of the cases in which the static detection
succeeds.

Another optimization was to extend the implementation of some
language features, such as predicates or FLWOR expressions, so
that they can produce a DataFrame upon DataFrame input, using
Spark SQL either with UDFs or even natively. For example, the
optimization of a predicate is always achievable with at least a
User-Defined Function encapsulating the predicate expression, and

in some cases can even directly be mapped to native Spark SQL,
making it even faster.

A study of the impact of both optimizations on the performance

of RumbleML is provided in Section 6.

6 Measurements and results

6.1 Experimental Setup
Platform. We conduct all experiments on a cluster of three m5.4xlarge
instances in AWS with EMR 6.4.0 and Spark 3.1.2 installed. These
instances run on machines with Intel Xeon® Platinum 8175 3.1GHz
CPUs and have 8 GiB of main memory per CPU core, and, for the
instance sizes we use, get up to 10 Gbps of network bandwidth.
Each instance has eight physical CPU cores and unless otherwise
mentioned, we process the data directly off of S3.
Datasets. We train and test our experiments on two large-scale
datasets. 𝐶𝑟𝑖𝑡𝑒𝑜 [28] is a sparse dataset of 50 GB, that contains
feature values that represent click feedback from millions of dis-
play ads. The data set contains 98M tuples with binary labels that
classify if a user has clicked on an ad or not. We use 92M tuples for
training and 6M for evaluation. Next, we use 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶, a
dense dataset of 50 GB, that contains 1M tuples of feature vector
representations (4096-dim) extracted from images of the YFCC100M
dataset [48]. For our experiments, we split the 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶 data
set equally into train and test data. The test set is chosen to be 25
GB in order to evaluate the impact of large inference and score
calculation at test time.
Data cleaning. To create 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶, several data cleaning
operations are required to create a standardized parquet format.
The raw data initially concatenates fc7 vectors of a pre-trained
CNN together with its multi-class predictions (see Figure 4). Thus,
our data cleaning process first tokenizes and splits the text into
predictions and features. Second, we check if predictions contain the
label 𝑜𝑢𝑡𝑑𝑜𝑜𝑟 or 𝑖𝑛𝑑𝑜𝑜𝑟 and convert it into a binary label. Third, we
index each feature to create a libsvm format. Lastly, we concatenate
the output and compress the result into a parquet file (see Figure 8).
Models. We train and evaluate a Logistic Regression, Linear SVC,
RandomForest, and Naive Bayes on 𝐶𝑟𝑖𝑡𝑒𝑜 and 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶.
All experimental results are run 5 times and aggregated together.
Pipelines. We randomly subsampled a real-world dataset of 500K
ML.NET pipelines and converted 12 end-to-end pipelines (see Figure
10) into JSONiq and spark.ml [25]. The first six pipelines are model
pipelines trained on a sparse dataset (𝐶𝑟𝑖𝑡𝑒𝑜) without (P1-P3) and
with an absolute scaler operator (P4-P6). The next six pipelines are
trained on a a dense dataset (𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶) without (P7-9) and
with an absolute scaler operator (P10-P12).

D

scaler

model

scorer

𝑢

Figure 10: A schematic depicting the operations that make
the scaler-model pipeline.

6.2 Code comparison
The YFCC pipeline script shown in Figure 1 was also written in
PySpark and spark.ml in order to compare the languages with a few

7

JSONiq SparkML

#characters
#lines
#clauses
#unique clauses

857
31
37
22

1146
39
66
26

Figure 11: Summary of pipeline scripts for RumbleML and
spark.ml.

metrics. In practice, we insist that, because of the complexity that
it involves, data scientists use different tools rather than a single
script.

We then compared the two scripts using different metrics as
shown in Figure 11. Thus, beyond considerations of functional vs.
imperative, it can be seen that JSONiq exposes more compact and
expressive constructs than PySpark and spark.ml.

6.3 Performance comparison
In the following, we report the runtime performance and accuracy
of our experiments described above. Note that for both, Spark and
Rumble, Random Forest did not converge for the sparse 𝐶𝑟𝑖𝑡𝑒𝑜
dataset with our initial cluster setup and parameters chosen. We
believe this is due to the underlying tree optimizer that cannot cope
with 1M sparse feature values. Furthermore, Naive Bayes models did
not support pipelines trained on the 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶 as it contains
negative feature values. Figure 12 show the end-to-end runtime for
𝐶𝑟𝑖𝑡𝑒𝑜 and 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑒𝑑 𝑌 𝐹𝐶𝐶 respectively. We did not observe any
runtime differences between spark.ml and RumbleML that can’t
be explained by variance. Furthermore, we did not observe any
differences in the resulting accuracy.

Figure 12: End-to-End performance on ML pipelines

6.4 Impact of Optimizations
In the following, we report runtime performance for RumbleML
with and without optimizations on the Criteo dataset (see Figure
13). Without any optimization (blue), RumbleML can only execute
queries locally, resulting in loss of performance and critical failure
due to memory limitations when the data size increases to larger
than 10K data points (thus no time measurements were able to
be performed for 100K). With DataFrame optimization turned on,
RumbleML is able to leverage the cluster structure (orange). Finally,
by integrating optimizations for filter queries (green), RumbleML
achieves a performance that is comparable to native spark.ml.

8

Figure 13: Ablation study on Criteo (P1)

7 Related and future work
Another high-level language for ML and AI use cases is the inkling
language, part of the Bonsai platform [23]. Alternative languages
for the cleanup of denormalized data include XQuery [5], AQL [16],
SQL++ [37], PartiQL [39], GraphQL [20], however most of them
do not support higher-order functions out of the box. There are
also other platforms than spark.ml, for example, TensorFlow [1],
PyTorch [40] and Weka [19], although, from our perspective, our
system can also be extended to build a JSONiq layer on top of them.
Also, none of the above systems cover both data preparation and
ML pipelines with a single, integrated functional language.

In future versions of RumbleML, we plan to extend it with the
ability for users to program their own ML algorithms directly in
JSONiq, that is, to implement estimators and transformers as reg-
ular user-defined functions in JSONiq with the same signatures.
Syntactically, this is already possible, and the work will mostly con-
sist in optimizing RumbleDB to automatically detect them. We will
also consider extending the support of transformers to take more
generic sequences as input (e.g., sequences of strings), in order to
integrate the data cleaning stage even more seamlessly into the
syntax to build the pipeline. We also plan to enhance static schema
detection in order to make user-defined schemas superfluous in
many cases.

We also plan to offer complete coverage of spark.ml’s estimators,
transformers, and parameters by extending the mapping to the
rarer types.

8 Conclusion
RumbleML is available as a free, open-source product, currently in
beta. The successful implementation of RumbleML proves yet again
that the JSONiq language is a good fit for programming lakehouse
systems and that it can be implemented with no loss of performance,
but gaining in productivity.

References
[1] Martı n Abadi et al. 2016. Tensorflow: a system for large-scale
machine learning. In 12th {USENIX} symposium on operating
systems design and implementation ({OSDI} 16), 265–283.
[2] Azza Abouzied and Paolo Papotti. 2018. Courting ML: Wit-
nessing the Marriage of Relational and Web Data Systems to
Machine Learning. https://wp.sigmod.org/?p=2243. [Online;
accessed 22-December-2021]. (2018).

P1P2P3P4P5P6P7P8P9P10P11P12pipelines0200400600800time (s)RumbleSpark1K10K100Kdataset size020406080time (s)no df + no sqlno sqlall[3]

Pulkit Agrawal et al. 2019. Data platform for machine learn-
ing. In Proceedings of the 2019 International Conference on
Management of Data (SIGMOD ’19). Association for Comput-
ing Machinery, Amsterdam, Netherlands, 1803–1816. isbn:
9781450356435. doi: 10.1145/3299869.3314050. https://doi.
org/10.1145/3299869.3314050.

[4] Michael Armbrust, Ali Ghodsi, Reynold Xin, and Matei Za-
haria. 2021. Lakehouse: a new generation of open platforms
that unify data warehousing and advanced analytics. In
CIDR.
Scott Boag and Don Chamberlin. 2002. Xquery 1.0: an xml
query language.

[5]

[7]

[6] Matthias Boehm et al. 2016. Systemml: declarative machine
learning on spark. Proceedings of the VLDB Endowment, 9,
13, 1425–1436.
Emily Caveness, Paul Suganthan G. C., Zhuo Peng, Neoklis
Polyzotis, Sudip Roy, and Martin Zinkevich. 2020. Tensorflow
data validation: data analysis and validation in continuous
ml pipelines. In Proceedings of the 2020 ACM SIGMOD In-
ternational Conference on Management of Data (SIGMOD
’20). Association for Computing Machinery, Portland, OR,
USA, 2793–2796. isbn: 9781450367356. doi: 10.1145/3318464.
3384707. https://doi.org/10.1145/3318464.3384707.

[8] Xu Chu and Ihab F Ilyas. 2016. Qualitative data cleaning.
Proceedings of the VLDB Endowment, 9, 13, 1605–1608.
[9] Xu Chu, Ihab F Ilyas, Sanjay Krishnan, and Jiannan Wang.
2016. Data cleaning: overview and emerging challenges. In
Proceedings of the 2016 international conference on manage-
ment of data, 2201–2206.

[10] Benoit Dageville et al. 2016. The snowflake elastic data ware-
house. In Proceedings of the 2016 International Conference on
Management of Data (SIGMOD ’16). Association for Com-
puting Machinery, San Francisco, California, USA, 215–226.
isbn: 9781450335317. doi: 10.1145/2882903.2903741. https:
//doi.org/10.1145/2882903.2903741.

[11] Chris J Date. 1987. A guide to the SQL Standard: a user’s guide
to the standard relational language SQL. Addison-Wesley
Longman Publishing Co., Inc.

[12] Xin Luna Dong and Theodoros Rekatsinas. 2018. Data inte-
gration and machine learning: a natural synergy. In Proceed-
ings of the 2018 international conference on management of
data, 1645–1650.

[13] Daniela Florescu and Ghislain Fourny. 2013. Jsoniq: the his-
tory of a query language. IEEE Internet Computing, 17, (Sep-
tember 12, 2013), 86–90, 5, (September 12, 2013). https://
ieeexplore.ieee.org/abstract/document/6596494.

[14]

[15]

Floris Geerts, Giansalvatore Mecca, Paolo Papotti, and Do-
natello Santoro. 2013. The llunatic data-cleaning framework.
Proceedings of the VLDB Endowment, 6, 9, 625–636.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. 2014. Generative adversarial nets. In Ad-
vances in Neural Information Processing Systems. Z. Ghahra-
mani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Wein-
berger, (Eds.) Volume 27. Curran Associates, Inc. doi: 10.
5555/2969033.2969125.

[16] Gösta Grahne, Raul Hakli, Matti Nykänen, and Esko Ukko-
nen. 1998. Aql: an alignment based language for querying
string databases. In Proc. 9th International Conference on
Management of Data: Databases for the millennium 2000, CO-
MAD’98. Citeseer, 16–18.

[17] Dan Graur, Ingo Müller, Proffitt Mason, Ghislain Fourny,
Gordon Watts, and Gustavo Alonso. 2022. Evaluating query
languages and systems for high-energy physics data. Ac-
cepted for publication PVLDB, 15.

[18] Torsten Grust, Benjamin Dietrich, and Dennis Butterstein.
2021. Datenbanksysteme I. https://db.inf.uni-tuebingen.de/
teaching/DatenbanksystemeIWS2015-2016.html. [Online;
accessed 22-December-2021]. (2021).

[19] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer,
Peter Reutemann, and Ian H Witten. 2009. The weka data
mining software: an update. ACM SIGKDD explorations newslet-
ter, 11, 1, 10–18.

[20] Olaf Hartig and Jorge Pérez. 2018. Semantics and complex-
ity of graphql. In Proceedings of the 2018 World Wide Web
Conference, 1155–1164.

[22]

[21] Geoffrey Hinton. 2007. Learning multiple layers of repre-
sentation. Trends in cognitive sciences, 11, (November 2007),
428–34. doi: 10.1016/j.tics.2007.09.004.
Ihab Ilyas. 2018. Data cleaning is a machine learning problem
that needs data systems help! https://wp.sigmod.org/?p=
2288. [Online; accessed 22-December-2021]. (2018).
[n. d.] Inkling programming language reference. https://docs.
microsoft.com/en-us/bonsai/inkling/. Accessed: 2021-10-12.
().
[n. d.] Jsound - the complete reference. https://www.jsoniq.
org/docs/JSound/html-single/. Accessed: 2021-10-12. ().

[24]

[23]

[25] Bojan Karlaš, Peng Li, Renzhi Wu, Nezihe Merve Gürel, Xu
Chu, Wentao Wu, and Ce Zhang. 2020. Nearest neighbor clas-
sifiers over incomplete information: from certain answers to
certain predictions. Proceedings of the VLDB Endowment.

9

[26]

[27]

Sanjay Krishnan, Jiannan Wang, Michael J Franklin, Ken
Goldberg, Tim Kraska, Tova Milo, and Eugene Wu. 2015.
Sampleclean: fast and reliable analytics on dirty data. IEEE
Data Eng. Bull., 38, 3, 59–75.
Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J Franklin,
and Ken Goldberg. 2016. Activeclean: interactive data clean-
ing for statistical modeling. Proceedings of the VLDB Endow-
ment, 9, 12, 948–959.

[28] Romain Lerallut, Diane Gasselin, and Nicolas Le Roux. 2015.
Large-scale real-time product recommendation at criteo. In
Proceedings of the 9th ACM Conference on Recommender Sys-
tems, 232–232.
Jason Long. 2021. JSON Lines. https://jsonlines.org/. [Online;
accessed 22-December-2021]. (2021).

[29]

[30] Vadim Markovtsev and Waren Long. 2018. Public git archive:
a big code dataset for all. In Proceedings of the 15th Interna-
tional Conference on Mining Software Repositories, 34–37.

[31] Wes McKinney et al. 2011. Pandas: a foundational python
library for data analysis and statistics. Python for high per-
formance and scientific computing, 14, 9, 1–9.

[32] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks,
and Shivaram Venkataraman. 2016. Mllib: machine learning
in apache spark. Journal of Machine Learning Research, 17, 1–
7. http://www.jmlr.org/papers/volume17/15-237/15-237.pdf.
[33] Microsoft. 2018. What are Azure Machine Learning pipelines?!
https://docs.microsoft.com/en-us/azure/machine-learning/
concept-ml-pipelines. [Online; accessed 22-December-2021].
(2018).

[34] Heiko Müller, Sonia Castelo, Munaf Qazi, and Juliana Freire.
2021. From papers to practice: the openclean open-source
data cleaning library. Proceedings of the VLDB Endowment,
14, 12, 2763–2766.
Ingo Müller, Ghislain Fourny, Stefan Irimescu, Can Berker
Cikis, and Gustavo Alonso. 2020. Rumble: data independence
for large messy data sets. Proceedings of the VLDB Endowment,
14, 4, 498–506.

[35]

[36] Randy Olson. 2021. DataCleaner library for Python and Pan-
das. https://pypi.org/project/datacleaner/. [Online; accessed
22-December-2021]. (2021).

[37] Kian Win Ong, Yannis Papakonstantinou, and Romain Vernoux.

2014. The sql++ unifying semi-structured query language,
and an expressiveness benchmark of sql-on-hadoop, nosql
and newsql databases. CoRR, abs/1405.3631.

[38]

Szilard Pafka. 2021. Simple/limited/incomplete benchmark
for scalability, speed and accuracy of machine learning li-
braries for classification. https://github.com/szilard/benchm-
ml. [Online; accessed 22-December-2021]. (2021).
[n. d.] Partiql. https://partiql.org/. Accessed: 2021-10-12. ().
[39]
[40] Adam Paszke et al. 2017. Automatic differentiation in py-

torch.

[41] Gil Press. 2016. Cleaning Big Data: Most Time-Consuming,
Least Enjoyable Data Science Task, Survey Says. https://
www.forbes.com/sites/gilpress/2016/03/23/data-preparation-
most-time-consuming-least-enjoyable-data-science-task-
survey-says/. [Online; accessed 22-December-2021]. (2016).
[42] Nataliya Prokoshyna, Jaroslaw Szlichta, Fei Chiang, Renée J
Miller, and Divesh Srivastava. 2015. Combining quantitative
and logical data cleaning. Proceedings of the VLDB Endow-
ment, 9, 4, 300–311.

[43] Theodoros Rekatsinas, Xu Chu, Ihab F Ilyas, and Christopher
Ré. 2017. Holoclean: holistic data repairs with probabilistic
inference. arXiv preprint arXiv:1702.00820.

[45]

[44] El Kindi Rezig, Mourad Ouzzani, Walid G Aref, Ahmed K
Elmagarmid, Ahmed R Mahmood, and Michael Stonebraker.
2021. Horizon: scalable dependency-driven data cleaning.
Proceedings of the VLDB Endowment, 14, 11, 2546–2554.
Shazia Sadiq and Marta Indulska. 2017. Open data: quality
over quantity. International Journal of Information Manage-
ment, 37, (June 2017), 150–154. doi: 10.1016/j.ijinfomgt.2017.
01.003.
StackOverflow. 2021. Tags. https://stackoverflow.com/tags.
[Online; accessed 22-December-2021]. (2021).

[46]

[47] Raj Sunderraman. 2021. Databases and the Web. http : / /
tinman.cs.gsu.edu/~raj/8711/sp21. [Online; accessed 22-
December-2021]. (2021).

[48] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin
Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia
Li. 2016. Yfcc100m: the new data in multimedia research.
Communications of the ACM, 59, 2, 64–73.

[49] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin,
Scott Shenker, and Ion Stoica. 2010. Spark: cluster comput-
ing with working sets. HotCloud’10 Proceedings of the 2nd
USENIX conference on Hot topics in cloud computing. https:
//dl.acm.org/citation.cfm?id=1863103.1863113.

[50] Matei Zaharia et al. 2016. Apache spark: a unified engine
for big data processing. Commun. ACM, 59, 11, (October
2016), 56–65. issn: 0001-0782. doi: 10.1145/2934664. https:
//doi.org/10.1145/2934664.

10

