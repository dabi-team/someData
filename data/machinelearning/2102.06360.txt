Fine-grained Pseudo-code Generation Method via
Code Feature Extraction and Transformer

Guang Yang†, Yanlin Zhou†, Xiang Chen†‡§∗, Chi Yu†
†School of Information Science and Technology, Nantong University, China
‡Key Laboratory of Safety-Critical Software (Nanjing University of Aeronautics and Astronautics),
Ministry of Industry and Information Technology, China
§The Key Laboratory of Cognitive Computing and Intelligent Information Processing of Fujian Education Institutions,
Wuyi University, China
Email: 1930320014@stmail.ntu.edu.cn, 1159615215@qq.com, xchencs@ntu.edu.cn, yc struggle@163.com

1
2
0
2

p
e
S
1
2

]
E
S
.
s
c
[

3
v
0
6
3
6
0
.
2
0
1
2
:
v
i
X
r
a

Abstract—Pseudo-code written by natural language is helpful
for novice developers’ program comprehension. However, writing
such pseudo-code is time-consuming and laborious. Motivated by
the research advancements of sequence-to-sequence learning and
code semantic learning, we propose a novel deep pseudo-code
generation method DeepPseudo via code feature extraction and
Transformer. In particular, DeepPseudo utilizes a Transformer
encoder to perform encoding for source code and then use a code
feature extractor to learn the knowledge of local features. Finally,
it uses a pseudo-code generator to perform decoding, which can
generate the corresponding pseudo-code. We choose two corpora
(i.e., Django and SPoC) from real-world large-scale projects as
our empirical subjects. We ﬁrst compare DeepPseudo with
seven state-of-the-art baselines from pseudo-code generation and
neural machine translation domains in terms of four performance
measures. Results show the competitiveness of DeepPseudo.
Moreover, we also analyze the rationality of the component
settings in DeepPseudo.

Index Terms—Program Comprehension, Pseudo-code genera-

tion, Deep learning, Transformer, Code feature extraction

I. INTRODUCTION

Comments written by natural language can help novice
developers’ program comprehension since these developers
may not be familiar with the grammar of the programming
languages and the domain knowledge of the projects. In this
study, we solve the problem of line-to-line pseudo-code gen-
eration by building a neural network model. In the algorithms’
textbook, pseudo-code is often used to introduce the problem’s
solution since the pseudo-code has higher readability than
the source code. Then the developers can write the source
code quickly according to the description of the pseudo-
code. Moreover, for program comprehension, the developers
can easily understand the pseudo-code than the code. Finally,
novice developers can also learn how to write code according
to the description of the pseudo-code. Fig. 1 shows the
example of the code snippet (in the left part of Fig. 1) and
its corresponding pseudo-code in English (in the right part of
Fig. 1) in our used two parallel corpora Django and SPoC.

However, most of the project’s source code has no cor-
responding pseudo-code since writing pseudo-code is time-
consuming and laborious. If pseudo-code can be generated

∗ Xiang Chen is the corresponding author.

automatically,
corresponding efforts.

the developers can substantially reduce the

Motivated by the research progress of neural machine
translation (NMT) and code semantic learning, we propose
a deep pseudo-code generation method DeepPseudo via
code feature extraction and Transformer [1]. In particular,
DeepPseudo utilizes a Transformer encoder to perform
encoding for source code and uses a code feature extractor to
learn the knowledge of local features. Then it uses a pseudo-
code generator to perform decoding, which can generate the
corresponding pseudo-code.

of

our

characteristic

proposed method
The main
DeepPseudo is
the usage of code feature extractor
via the convolutional neural network (CNN). Inspired by
the idea of multi-model fusion (i.e., different models with
different structures often learn different features) [2], we fuse
the local feature representation learned by the code feature
extractor with the global context representation learned by
the Transformer encoder to improve the generalization ability
of the model. Moreover,
to ensure the robustness of the
model, we add adversarial training data to the embedding
layers of the encoder and decoder by using the method
PGD (Projected Gradient Descent) [3]. The inclusion of the
adversarial training data can both improve the robustness of
the model in response to malicious adversarial samples and
act as a regularization to reduce the overﬁtting problem [4].
Finally, we also consider different attention mechanisms to
further improve the performance of DeepPseudo.

To verify the effectiveness of our proposed method
DeepPseudo, we choose two corpora (i.e., Django [5] and
SPoC [6]) gathered from real-world large-scale projects as our
experimental subjects. We ﬁrst compare DeepPseudo with
seven state-of-the-art baselines. Three baselines are chosen
from the pseudo-code generation domain, and the remain-
ing baselines are chosen from neural machine translation
domains. We conduct performance comparison in terms of
four performance measures, which have been used in previous
studies on code comment generation [7] [8]. Results show
the competitiveness of our proposed method. Then, we also
design the research questions to verify the component setting
rationality in DeepPseudo, such as the usage of code feature

 
 
 
 
 
 
Fig. 1. The example of the code snippet and its corresponding pseudo-code written in English from two corpora Django and SPoC

extractor and the chosen attention mechanism. The ﬁnal results
show that the usage of the code feature extractor and the
norm-attention mechanism can achieve the best performance
in DeepPseudo.

To our best knowledge, we summarize the main contribu-

tions as follows.

• We propose a method DeepPseudo based on code
feature extraction and Transformer. In the encoder part,
DeepPseudo utilizes both Transformer encoder and
code feature extractor to perform encoding for source
code. In the decoder part, DeepPseudo resorts to the
beam search algorithm for pseudo-code generation.

• We choose two corpora from real-world project as our
experimental subject. Comparison results with seven
baselines show the competitiveness of DeepPseudo in
terms of four performance measures.

• We share our scripts, trained model, and corpora1 to
facilitate the replication of our study and encourage more
follow-up studies on this research topic.

The rest of this paper is organized as follows. Section II
summarized the related work for automated pseudo-code
generation and code comment generation. Section III shows
the framework of our proposed method DeepPseudo and
its details for each component. Section IV illustrates the
experimental setup, including research questions, experimental
subjects, performance measures, implementation details, and
running platform. Section V performs result analysis for each
research question. Section VI investigates the inﬂuence of
different parameter values on DeepPseudo. Section VII an-
alyzes potential threats to the validity of our empirical studies.
Section VIII concludes our study and provides potential future
directions.

II. RELATED WORK
In this section, we ﬁrst summarize the related work for code
comment generation. Then we summarize the related work for
pseudo-code generation. Finally, we emphasize the novelty of
our study.

1https://github.com/NTDXYG/DeepPseudo

A. Code Comment Generation

Similar to pseudo-code generation, code comment gener-
ation also aims to generate comments for the target code
snippet. The comment aims to summarize the code snippet
and describe the code’s functionality and purpose. In the early
stage, researchers mainly focused on the template-based meth-
ods and information retrieval-based methods. In recent years,
researchers mainly considered deep learning-based methods.
Iyer et al. [9] ﬁrst proposed a deep learning-based method
code-NN. Allamanis et al. [10] considered a convolutional
attention network. Zheng et al. [11] introduced a code attention
mechanism. Liang and Zhu [12] considered Code-RNN and
Code-GRU for encoder and decoder. Hu et al. [13] [7] pro-
posed the method DeepCom by analyzing abstract syntax trees
(ASTs). Leclair et al. [14] proposed the method ast-attendgru,
which combines words from code with the code structure from
ASTs. Leclair et al. [15] then used a graph neural network
(GNN), which can effectively analyze the AST structure.
Jiang et al. [16] automatically generated commit messages via
diffs by using a neural machine translation algorithm. Then,
Xu et al. [17] utilized the copying mechanism, and Liu et
al. [18] utilized information retrieval-based to further improve
the message quality. Liu et al. [19] aimed to generate pull
request descriptions. Other researchers aim to utilize different
methods to improve the model performance. Wan et al. [20]
proposed the method Hybrid-DRL, which considers hybrid
code representation and deep reinforcement learning. Then,
Wang et al. [21] further extended the method Hybrid-DRL.
Hu et al. [22] proposed the method TL-CodeSum, which can
utilize API knowledge learned in a related task to improve
the quality of code comments. Zhang et al. [8] proposed
a retrieval-based neural code comment generation method.
This method enhances the model with the most similar code
segments retrieved from the training set from syntax and
semantics aspects.

B. Pseudo-code Generation

Code comment generation aims to generate the description
of function and purpose for the code and reduces developers’

time for reading code. While the pseudo-code generation can
help novice developers understand the source code’s details at
the ﬁne-grained level. According to pseudo-code, the develop-
ers can understand what each statement does.

To generate pseudo-code, Oda et al. [5], [23] resorted to
statistical machine translation. They mainly considered phrase-
based machine translation and tree-to-string machine transla-
tion frameworks. Hajie et al. [24] implemented a sequence-to-
sequence model via LSTM to generate comments for code
fragments. Alhefdhi et al. [25] proposed an approach to
automatically generate pseudo-code from source code by using
LSTM and Neural Machine Translation. Xu et al. [26] treated
the pseudo-code generation task as a language translation
task and considered a sophisticated neural machine translation
model and attention seq2seq model for this task. Li et al. [27]
considered the replication mechanism and sketch generation
for ﬁne-grained semantic learning of code. Rai et al. [28] pro-
posed an approach to generate pseudo-code from the python
source code. In the ﬁrst step, they converted python code
into XML code for better code information extraction. Next,
important information extracted from the XML code was later
used to generate pseudo-code with the help of pseudo-code
templates. Xiong et al. [29] proposed the method Code2Text.
The pseudo-code is generated by constructing two encoders
to learn the syntactic and semantic information of the code
respectively. Deng et al. [30] proposed a type-aware sketch-
based seq2seq learning method. This method can generate
the natural language description of the source code, which
combines the type of the source code and the mask mechanism
with the LSTM.

C. Novelty of Our Study

In this study, we want to resort to the deep learning-based
method to generate pseudo-code. Compared with previous
studies, DeepPseudo uses the Transformer’s encoder to
learn global code semantics and adds a stacked convolutional
network (i.e., code feature extractor) to extract local code
features. Finally, in the decoding layer, DeepPseudo used
the Beam search algorithm to generate ﬁne-grained pseudo-
code for each line of code based on learned code semantic.

III. OUR PROPOSED METHOD DE E PPS E U D O

A. Framework of Our Proposed Method

The overall framework of DeepPseudo can be found in
Fig. 2. In this ﬁgure, we can ﬁnd DeepPseudo includes three
components: transformer encoder, code feature extractor, and
pseudo-code generator. Then we can use the gathered corpora
to train the model via this framework. Specially, we use
the method PGD (Projected Gradient Descent) [3] to perturb
the word embedding layer of the encoder and decoder to
improve the generalization ability of pseudo-code generation.
Finally, we use an example in Fig. 2 to show the prediction
process. Here, the input of the source code is “view func
=getattr(mod, func name) ”, and the generated pseudo-code
is “get func name attribute from the mod object, substitute
it for view func.”.

B. Transformer Encoder

Let X = (x1, x2, · · · , xn) be the input (i.e., the tokens of
[1] [31]. Through word
the code) of Transformer encoder
embedding layer and positional encoding, DeepPseudo can
get a vector X:

X = Embedding(X) + P ositionalEncoding(X)

(1)

Then, DeepPseudo inputs the vector X into the multi-
head attention layer. The attention function is computed si-
multaneously based on three matrices: Q (queries), K (keys),
and V (values).

Q, K, V = Linear(X)

Xattention = Attention(Q, K, V )

(2)

(3)

Later, DeepPseudo uses residual connection [32] and
layer normalization [33] to make the matrix operation dimen-
sion consistent and normalize the hidden layer in the network
to a standard normal distribution, which can accelerate the
model training speed and the convergence.

Xattention = X + Xattention

Xattention = LayerN orm(Xattention)

(4)

(5)

In the fourth step, DeepPseudo passes the feed-forward
layer and two linear mapping layers. Then DeepPseudo uses
the activation function to generate the vector Xhidden.

Xhidden = Activate(Linear(Linear(Xattention)))

(6)

Finally, DeepPseudo performs a residual connection [32]
and layer normalization [33] to obtain the ﬁnal context vector
C = Xhidden = (c1, c2, · · · , cn).

Xhidden = Xhidden + Xattention

C = Xhidden = LayerN orm(Xhidden)

(7)

(8)

C. Code Feature Extractor

For the pseudo-code generation problem, we additionally
introduced the code feature extractor (CFER) to effectively
capture the source code’s local feature information as external
knowledge for knowledge fusion with Transformer, which
can not be captured by the Transformer Encoder. Therefore,
CFER is attached to the original Transformer Encoder in our
proposed method DeepPseudo.

Recently, more studies [13], [22] on code comment gen-
eration resorted to fusing semantic and syntactic information
learned from the code to improve the quality of the generated
code comments. In particular, they all aimed to extract the
information from the abstract syntax tree (AST) of code

Fig. 2. Framework of our proposed method DeepPseudo

fragments for syntactic representation learning. However, ex-
tracting this kind of information may be not feasible in pseudo-
code generation tasks since the code line is ﬁne-grained. For
example, the code statement “from logging import NullHan-
dler”, which imports a third-party library, cannot extract its
AST information. Therefore, we aim to combine CNN with
Transformer in the ﬁeld of pseudo-code generation.

First, we utilized the local feature information extracted by
CNN via static convolution kernel. According to previous stud-
ies [34] [35], CNN and self-attention differ in the extraction of
feature information. The feature vectors of different tokens in
a Conv Layer have only additive and subtractive operations on
each other. Feature vectors of different tokens in self-attention
have inner product operations on each other. The focus of
self-attention is more on how to decide where to project more
attention in a global context, whereas convolution in CNNs
focuses more on obtaining another local representation of the
input text. Then, We refer to the use of convolution in the
ConvS2S model for local feature extraction by using cascading
one-dimensional convolution to obtain local feature knowledge
of the source code line. We fuse the feature representation
learned by the local code feature extractor with the global
context feature learned by the Transformer encoder to improve
the performance of the model.

In particular, we utilize the CNN-based structure proposed
by Gehrin et al. [36]. The stack CNN can form a hierarchical
representation, and every convolutional layer includes a one-
dimensional convolution, a gated linear unit (GLU) [37], and
a residual connection.

F = X + GLU (Conv(X))

(9)

The input of CFER (i.e., the vector X obtained through the
word embedding layer and positional encoding) is shared with
the Transformer encoder. The next step of CFER is to enter the
convolutional layer for feature extraction. After completing the
extraction, a linear layer is used to make the feature vector F ,
which has the same dimension as the context vector C. Finally,
the two features are merged through matrix addition, and the
code representation vector Z is obtained.

Z = C + Linear(F )

(10)

D. Pseudo-code Generator

Previous studies [38] showed generating the text by using
the neural network’s maximum probability distribution often
leads to low-quality results. Recently, most studies [13] [22]
resorted to beam search, and these studies can achieve state-
of-the-art performance on text generation tasks. Therefore
we also use the Transformer Decoder and Beam Search
algorithm [39] to generate pseudo-code for the source code
in our proposed method DeepPseudo. The deep learning
model has poor interpretability, but the effectiveness of the
beam search algorithm has been veriﬁed via theory analysis,
and the beam search algorithm has an obvious connection to a
cognitive science theory (i.e., the uniform information density
hypothesis) [40].

IV. EXPERIMENTAL SETUP

In our empirical studies, we aim to investigate the following

three research questions (RQs).
RQ1: Can our proposed method DeepPseudo outperform
state-of-the-art baselines?

Motivation. In this RQ, we ﬁrst want to show the effective-
ness of our proposed method DeepPseudo by comparing
state-of-the-art baselines on pseudo-code generation. Here we
mainly consider statistical machine translation-based meth-
ods proposed by Oda et al. [5] and a type-aware sketch-
based sequence-to-sequence method proposed by Deng et
al. [30]. Moreover, we also want to compare our proposed
method DeepPseudo with other state-of-the-art sequence-
to-sequence methods (i.e., RNN based methods with/without
the attention mechanism, ConvS2S, and Transformer).
RQ2: Whether using the code feature extractor component
can improve the performance of DeepPseudo?
Motivation. After analyzing the pseudo-code generation prob-
lem, we additional add the code feature extractor component
in our proposed method DeepPseudo. Therefore, we want
to verify whether using the CFER component
to extract
code feature information can improve the performance of
DeepPseudo after comparing the method DeepPseudo
without the CFER component.
RQ3: What is the effect of the attention mechanism on
our proposed method DeepPseudo?
Motivation. The Transformer is based on a multi-head atten-
tion mechanism. Each input is divided into multiple heads, and
each head uses the attention mechanism. Therefore, we want to
investigate the impact of the different attention mechanisms on
our proposed method DeepPseudo. In this RQ, we consider
the traditional self-attention mechanism and other recently
proposed attention mechanisms [41] [42] [43]. Based on
empirical results, we can select the most suitable mechanism
for our proposed method DeepPseudo.

A. Experimental Subjects

In our empirical study, we choose two source code/pseudo
code parallel corpora shared by Oda et al. [5] (i.e., Django)
and Kulal et al. [6] (i.e., SPoC). These corpora were gathered
from real-world large-scale projects. Speciﬁcally, the code in
Django is written by Python and the code in SPoC is written
by C++. To improve the quality of these two corpora, we
remove the replicated code snippets to prevent that the same
code snippet appears both in the training set and the test set.
Then we divide the training set, validation set and test set
by 80%:10%:10%. Table I and Table II show the statistical
information (such as average, mode, and median of the length,
the percentage of code/pseudo-code when the length is smaller
than 20, 50, and 100 respectively) in our used corpora.

B. Performance Measures

To quantitatively compare the performance between our
proposed method and the baselines, we choose the following
four performance measures, which have been widely used in
previous neural machine translation studies [44]. Notice the
higher the value of the performance measure, the better the
performance of the corresponding method. To ensure the im-
plementation correctness of these performance measures, we

TABLE I
STATISTICS OF CORPORA USED IN OUR EMPIRICAL STUDY

Statistics for Code Length

Django
SPoC

Avg
9.04
9.51

Mode Median

7
6

6
8

< 20

< 50
< 100
94.18% 99.66% 99.93%
100%
93.14% 99.99%

Statistics for Pseudo-code Length

Django
SPoC

Avg
14.29
8.53

Mode Median

11
4

6
7

< 20

< 50
< 100
75.54% 93.12% 98.68%
100%
95.66% 99.98%

TABLE II
CORPORA SPLIT RESULTS IN OUR EMPIRICAL STUDY

Corpus

Training

Validation

Test

Django
SPoC

11,826
98,673

1,590
12,334

1,590
12,334

use the nlg-eval library2, which implements these performance
measures for natural language generation tasks.
BLEU. BLEU (Bilingual Evaluation Understudy) [45] is a
variant of the precision measure. This performance measure
can calculate the similarity by computing the n-gram precision
of a candidate sentence to the reference sentence, with a
penalty for the overly short length.
METEOR. METEOR (Metric for Evaluation of sentences
with Explicit Ordering) [46] uses knowledge sources (such
as WordNet) to expand the synset while taking into account
the shape of words.
ROUGE. ROUGE (Recall-Oriented Understudy for Gisting
Evaluation) [47] is a set of indicators for evaluating automatic
abstracts and machine sentences. This study uses ROUGE-
L as our evaluation measure, where L means LCS (longest
common subsequence).
CIDER. CIDER (Consensus-based Image Description Evalu-
ation) [48] is a collection of BLEU and vector space model.
It treats each sentence as a document, and then calculates the
tf-idf value of the n-gram phrase, and calculates the similarity
between the candidate text and the reference text through the
cosine distance. Finally, the average value is calculated based
on n-grams of different lengths and used as the ﬁnal result.

C. Implementation Details and Running Platform

In our empirical study, we use Pytorch 1.6.0 to imple-
ment our proposed method. Our proposed method’s hyper-
parameters are mainly in three categories (i.e., the hyper-
parameters for the model structure, the hyper-parameters in the
model training phase, and the hyper-parameters in the model
test phase). The hyper-parameters for each category and their
value are summarized in Table III, where n layers means the
number of layers (i.e., the depth of the model), d model means
the model size (i.e., the width of the model).

2https://github.com/Maluuba/nlg-eval

All the experiments run on a computer with an Inter(R)
Core(TM) i7-9750H 4210 CPU and a GeForce GTX1660ti
GPU with 6 GB memory. The running OS platform is Win-
dows 10.

TABLE III
HYPER-PARAMETERS AND THEIR VALUE IN OUR EMPIRICAL STUDY

Category

Hyper-parameter

Parameter Value

Model Structure

Model Training Phase

n layers
n heads
d model
hidden size
kernel size
scale

dropout
optimizer
learning rate
batch size

Model Test Phase

beam size

3
8
256
512
5
√
0.5

0.25
Adam
0.001
128

3

V. RESULT ANALYSIS

A. Result Analysis for RQ1

RQ1: Can our proposed method DeepPseudo outperform
state-of-the-art baselines?
Method.
In this RQ, we ﬁrst compare our proposed
method DeepPseudo with three state-of-the-art methods
(i.e., Reduced-T2SMT method [5], Code-NN method [9] and
Code2NL method [30]) in pseudo-code generation domain.
Since these three studies all shared their code, we re-run
their code on our corpora for a fair comparison. Then we
also choose four baselines from the neural machine trans-
lation domain. According to the previous studies’ descrip-
tion, we re-implemented these four baselines. Moreover, the
value of hyper-parameters is consistent with that of our pro-
posed method DeepPseudo for a fair comparison. Note that
Reduced-T2SMT and Code2NL are customized with syntactic
templates for python syntax,
therefore these two methods
cannot be applied to SPoC corpus, since the code is written by
C++. The details of these baselines are summarized as follows.
(1) Reduced-T2SMT. This baseline [5] is based on the
framework of statistical machine translation, which can gen-
erate natural language expressions for Python source codes.

(2) Code-NN. This baseline [9] is the ﬁrst deep learning-
based method, which uses LSTM with the attention mecha-
nism to generate the summary of code snippets.

(3) Code2NL. This baseline [30] is an asynchronous learn-
ing model, which learns the code semantics and generates a
ﬁne-grained natural language description for each line of code.
It adopts a type-aware sketch-based sequence-to-sequence
learning method.

(4) RNN-seq2seq with (w)/without (w/o) attention mech-
anism. These RNN-based baselines [49] [50] use RNN to
construct seq2seq models, and we compare the performance
of these models with or without the attention mechanism.

(5) ConvS2S. This baseline [36] uses CNN to construct
seq2seq models. We also compare the performance of these
models with or without the attention mechanism.

(6) Transformer. This baseline [1] only uses encoder-
decoder framework and the attention mechanism to achieve
good performance. The most signiﬁcant advantage of this
baseline is that it can be efﬁciently parallelized.
Results. The comparison results between our proposed method
DeepPseudo and baselines can be found in Table IV. For
Django, in terms of BLEU, DeepPseudo can improve the
performance by 9.19% to 39.29%. In terms of METEOR,
DeepPseudo can improve the performance by 0.84% to
32.04%. In terms of ROUGE-L, DeepPseudo can improve
the performance by 0.27% to 16.64%. In terms of CIDER,
DeepPseudo can improve the performance by 14.90% to
51.82%. For SPoC, in terms of BLEU, DeepPseudo can
improve the performance by 6.21% to 44.69%. In terms of
METEOR, DeepPseudo can improve the performance by
4.31% to 39.70%. In terms of ROUGE-L, DeepPseudo
can improve the performance by 2.48% to 13.83%. In terms
of CIDER, DeepPseudo can improve the performance by
6.11% to 35.79%.

TABLE IV
THE COMPARISON RESULTS BETWEEN OUR PROPOSED METHOD
DE E PPS E U D O AND BASELINE METHODS

DATASET

METHOD

BLEU-4(%) METEOR(%) ROUGE-L(%) CIDER

Django

SPoC

Reduced-T2SMT
Code-NN
Code2NL
Seq2Seq w/o Atten.
Seq2Seq w Atten.
ConvS2S
Transformer
DeepPseudo

Code-NN
Seq2Seq w/o Atten.
Seq2Seq w Atten.
ConvS2S
Transformer
DeepPseudo

45.082
40.512
46.541
36.483
43.960
37.455
43.464
50.817

32.105
33.761
41.007
34.197
43.738
46.454

37.072
30.705
37.460
31.431
38.874
29.689
35.466
39.201

29.211
34.037
37.892
33.323
39.124
40.809

77.185
70.471
77.562
67.141
73.036
66.679
72.910
77.773

55.721
56.611
59.852
56.546
61.896
63.430

5.021
4.209
5.186
3.710
4.902
3.829
5.059
5.813

3.029
3.078
3.556
3.150
3.876
4.113

Summary for RQ1: Our proposed DeepPseudo can
outperform baselines in terms of four performance
metrics both in Django and SPoC.

B. Result Analysis for RQ2

RQ2: Whether using the code feature extractor can im-
prove the performance of DeepPseudo?
Method. To verify whether using code feature extractor can
improve the performance of DeepPseudo, we compared
the performance between DeepPseudo with CFER and
DeepPseudo without CFER. Here, for these methods, the
values of the hyper-parameters are consistent and all use the
norm-attention as the attention mechanism (detailed discus-
sion on the inﬂuence of different attention mechanisms on
DeepPseudo can be found in Section V-C).

Results. The comparison results between DeepPseudo with
(w) CFE and DeepPseudo without (w/o) CFER can be found
in Table V. For Django, in terms of BLEU, DeepPseudo
using CFER can improve the performance by 18.83%. In terms
of METEOR, DeepPseudo using CFER can improve the
performance by 1.16%. In terms of ROUGE-L, DeepPseudo
using CFER can improve the performance by 5.83%. In
terms of CIDER, DeepPseudo using CFER can improve
the performance by 14.84%. For SPoC, in terms of BLEU,
DeepPseudo using CFER can improve the performance by
5.18%. In terms of METEOR, DeepPseudo using CFER can
improve the performance by 0.32%. In terms of ROUGE-L,
DeepPseudo using CFER can improve the performance by
1.01%. In terms of CIDER, DeepPseudo using CFER can
improve the performance by 4.36%.

TABLE V
THE COMPARISON RESULTS BETWEEN DE E PPS E U D O WITH CFER AND
DE E PPS E U D O WITHOUT CFER

DATASET

Name

BLEU(%) METEOR(%) ROUGE(%) CIDER

Django

SPoC

w/o CFER
w CFER

w/o CFER
w CFER

42.765
50.817

44.168
46.454

38.752
39.201

40.677
40.809

73.492
77.773

62.799
63.430

5.062
5.813

3.941
4.113

Summary for RQ2: Using the code feature extractor
in our proposed method DeepPseudo can improve
the performance by effectively capturing code features.

C. Result Analysis for RQ3

RQ3: What is the effect of attention mechanism on our
proposed method DeepPseudo?
Method. In this RQ, we consider the impact of four different
attention mechanisms on pseudo-code generation task. In par-
ticular, we consider the traditional attention mechanism (i.e.,
the self-attention mechanism) and state-of-the-art attention
mechanisms (i.e., the other three attention mechanisms [41]
[42] [43]). We summarize the characteristics of the considered
attention mechanisms as follows.

(1) Self-Attention. Self-Attention [1] essentially introduces
contextual information for the current word to enhance the
representation of the current word and write more information.

Self-Attention (Q, K, V ) = softmax

(cid:18) QK T
√
dk

(cid:19)

V

(11)

(2) Linear-Attention. By using linear multi-head atten-
tion [41], the self-attention mechanism can be approximated by
a low-rank matrix. Therefore, this kind of attention mechanism
can reduce the overall self-attention time complexity and space
complexity from O(n2) to O(n).

(cid:18) Q(EK)T
√

(cid:19)

F V

dk

Linear-Attention (Q, K, V ) = softmax

For the Linear-Attention,

(12)
two linear projection matrices
E, F ∈ Rn×k are introduced when calculating key and value.
This reduces the time and space complexity of the algorithm
from O(n2) to O(nk). If we can choose a small projection
dimension k << n, we can signiﬁcantly reduce the time and
space complexity.

(3) Synthesizer-Attention. Synthesizer-Attention [42] no
longer calculates the dot product between two tokens vectors
but learns to synthesize a self-alignment matrix (i.e., a syn-
thetic self-attention matrix). Synthesizer is a generalization of
the standard Transformer. The input of the model is X ∈ Rl×d,
and the output is Y ∈ Rl×d, where l is the length of the
sentence and d is the dimension of word embedding. The
whole process can be divided into two steps: In the ﬁrst step,
it calculates the attention weight of each token and uses the
parameterized function F (X) to map the dimension of X from
the d dimension to the l dimension.

B = F(X) = W2 (σR (W1(X) + b)) + b

(13)

In the second step, it calculates the output result based on the
attention score.

Y = Softmax(B)G(X)

(14)

where B ∈ Rl×l,G(X) is another parameterized function,
which is analogous to V in the self-attention mechanism.

(4) Norm-Attention. Norm-Attention [43] applies L2 nor-
malization to Q and K, therefore Q and K become ˆQ and
ˆK:

ˆQi =

Qi
(cid:107)Qi(cid:107)

, ˆKi =
√

Ki
(cid:107)Ki(cid:107)

(15)

Different from dividing by

dk in the Self-Attention mech-
anism, the Norm-Attention mechanism uses a learnable param-
eter to expand. The initial value of this parameter depends on
the length of the sequence in the training data.

g0 = log2

(cid:0)L2 − L(cid:1)

(16)

where L is the 97.5th percentile sequence length of all training
data in the source and the target, therefore the formula of
Norm-Attention is deﬁned as follows.

where Q is query, K is key, V is value, Q, K, V ∈ Rn×dm ,
they are the input word embedding matrix, n is the length of
the sentence, and dm is the dimension of the word embedding.
The result of the dot product of Q and K is used to reﬂect the
degree of inﬂuence of the context word on the central word,
and then normalized by softmax.

Norm-Attention (Q, K, V ) = softmax

(cid:16)

g ∗ ˆQ ˆK T (cid:17)

V (17)

The structure of these four different attention mechanisms

can be found in Fig. 3.
Results. The comparison results between different attention
mechanisms in our proposed method DeepPseudo can be

Fig. 3. Structure of different attention mechanisms

found in Table VI. In terms of a speciﬁc performance measure,
we mark the best result in the bold type. For Django, in terms
of BLEU, the Norm-Attention mechanism can improve the
performance by 1.40% to 6.16%. In terms of METEOR, the
Norm-Attention mechanism can improve the performance by
2.34% to 6.92%. In terms of ROUGE-L, the Norm-Attention
mechanism can improve the performance by 0.41% to 5.84%.
the Norm-Attention mechanism can
In terms of CIDER,
improve the performance by 3.60% to 26.29%. For SPoC, in
terms of BLEU, the Norm-Attention mechanism can improve
the performance by 0.74% to 5.06%. In terms of METEOR,
the Norm-Attention mechanism can improve the performance
by 3.83% to 8.71%. In terms of ROUGE-L,
the Norm-
Attention mechanism can improve the performance by 1.56%
to 4.88%. In terms of CIDER, the Norm-Attention mechanism
can improve the performance by 5.16% to 14.15%.

Moreover, we surprisingly ﬁnd that the two state-of-the-art
attention mechanisms (i.e., the Linear-Attention mechanism
and the Synthesizer-Attention mechanism) cannot achieve bet-
ter performance than the original Self-Attention mechanism.
Therefore, we recommend the usage of the Norm-Attention
mechanism in our proposed method DeepPseudo. Therefore,
except for low-resource machine translation tasks [43], the
Norm-Attention mechanism is also applicable to our studied
pseudo-code generation problem.

Summary for RQ3: Using the Norm-Attention mech-
anism can achieve the best performance in our pro-
posed DeepPseudo.

VI. DISCUSSIONS

In this section, we further conduct experiments by analyzing
the parameter effects on DeepPseudo. These parameters are
the model size d model, the number of layers N , and the
number of kernel size K. The comparison results between

TABLE VI
THE COMPARISON RESULTS BETWEEN DIFFERENT ATTENTION
MECHANISMS

DATASET

Name

BLEU(%) METEOR(%) ROUGE(%) CIDER

Django

SPoC

Self-Attention
Linear-Attention
Synthesizer-Attention
Norm-Attention

Self-Attention
Linear-Attention
Synthesizer-Attention
Norm-Attention

50.115
48.215
47.868
50.817

46.115
44.215
45.868
46.454

38.305
37.541
36.664
39.201

39.305
37.541
37.664
40.809

77.458
75.252
73.481
77.773

62.458
61.252
60.481
63.430

5.611
5.077
4.603
5.813

3.911
3.717
3.603
4.113

different hyperparameters can be found in Table VII. The
optimal value of these parameters is set as follows. d model
is 256, N is 3, and K is 5. When analyzing the impact of a
speciﬁc hyperparameter, we keep other hyperparameters’ value
as the optimal value.

The ﬁnal results can be found in Table VII. We ﬁnd that it
is harder to train the generation model when N is large (i.e.,
depth of the model) and d model is large (i.e., width of the
model). This is because the gradient disappearance problem
tends to occur when training the generation model, even
though the DeepPseudo adds layer normalization, gating
mechanism, and residual connection to alleviate this problem.
This is the reason why we choose small value for these
network parameters, which are more suitable for this pseudo-
code generation task. Regarding the value of kernel size, We
ﬁnd that the performance of the model will decrease if the
value of this parameter is large or small. Therefore, we set the
value of the parameter kernel size to 5 in DeepPseudo.

VII. THREATS TO VALIDITY

In this section, we mainly analyze the potential threats to

the validity of our empirical study.

TABLE VII
SENSITIVITY ANALYSIS ON THE HYPERPARAMETERS (I.E., THE MODEL
SIZE, THE NUMBER OF LAYERS, AND THE NUMBER OF KERNEL SIZE)

DATASET

VALUE

BLEU(%) METEOR(%)

ROUGE-L(%)

CIDER

Django

SPoC

Django

SPoC

Django

SPoC

Varying the model size(d model)

50.817
47.310
0

46.454
36.185
0

39.201
37.349
2.279

40.809
38.241
1.242

Varying the number of layers(N )

50.817
44.815
0

46.454
33.860
0

39.201
35.671
2.279

40.809
35.935
1.242

Varying the number of kernel size(K)

49.799
50.817
50.551

39.706
46.454
35.661

38.706
39.201
39.165

39.690
40.809
36.629

77.773
75.208
5.066

63.430
61.742
0.624

77.773
73.409
5.066

63.430
57.176
0.624

76.976
77.773
77.795

59.852
63.430
57.444

256
512
768

256
512
768

3
6
9

3
6
9

3
5
7

3
5
7

5.813
5.396
0.004

4.113
3.764
0.003

5.813
4.944
0.004

4.113
3.327
0.003

5.739
5.813
5.799

3.731
4.113
3.352

Internal Threats. The ﬁrst internal threat is the potential
defects in the implementation of DeepPseudo. To alleviate
this threat, we check our implementation carefully and use
mature libraries, such as PyTorch and TorchText. The second
internal threat is the implementation of chosen baselines. To
alleviate this threat, we use the implementation shared by
previous pseudo-code generation studies [5].

External Threats. The main external threat is the choice of
corpora. To alleviate this threat, we select the popular two
corpora provided by Oda et al. [5] and Kulal et al. [6] and these
corpora have been widely used in previous studies on pseudo-
code generation. In the future, we want to gather more corpora
from other commercial or open-source projects and verify the
effectiveness of our proposed method DeepPseudo.

Construct Threats. The construct threat in this study is the
performance measures used to evaluate the performance of
DeepPseudo. To alleviate these threats, we choose three
performance measures, which have been used by the previous
pseudo-code generation studies. Moreover, we also consider a
performance measure, CIDER, which has been more used in
recent code comment generation studies [7] [51] [52].

Conclusion Threats. The conclusion threat in our study is
we do not perform cross-validation (CV) in our research.
Using CV can comprehensively evaluate our proposed method
DeepPseudo, since different split may result in a diverse
training set, validation set, and testing set. However, this model
evaluation method has not been commonly used for neural
machine translation due to high training computational cost.

VIII. CONCLUSION AND FUTURE WORK

In this study, we propose a novel deep pseudo-code genera-
tion method DeepPseudo via Transformer and code feature
extraction. In the encoder part, DeepPseudo utilizes both
Transformer encoder and code feature extractor to perform
encoding for source code. In the decoder part, DeepPseudo
resorts to the beam search algorithm for pseudo-code gen-
eration. Empirical results veriﬁed the effectiveness of our
proposed method. Moreover, we also design the experiments
to justify the component setting’s rationality in DeepPseudo.
to improve the effectiveness of
our proposed method DeepPseudo by considering more ad-
vanced attention mechanisms and pre-training models. More-
over, we also want to verify the effectiveness of DeepPseudo
by considering corpora gathered from other commercial or
open-source projects developed by other programming lan-
guages.

In the future, we want

ACKNOWLEDGMENT

Guang Yang and Yanlin Zhou have contributed equally
to this work and they are co-ﬁrst authors. This work is
supported in part by National Natural Science Foundation of
China (Grant nos. 61872263 and 61202006), The Open Project
of Key Laboratory of Safety-Critical Software for Nanjing
University of Aeronautics and Astronautics, Ministry of In-
dustry and Information Technology (Grant No. NJ2020022),
the Open Project Program of The Key Laboratory of Cognitive
Computing and Intelligent Information Processing of Fujian
Education Institutions (KLCCIIP201802). .

REFERENCES

[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings
of the 31st International Conference on Neural Information Processing
Systems, 2017, pp. 6000–6010.

[2] F. Ding, X. Kang, S. Nishide, Z. Guan, and F. Ren, “A fusion model for
multi-label emotion classiﬁcation based on bert and topic clustering,” in
International Symposium on Artiﬁcial Intelligence and Robotics 2020,
vol. 11574, 2020, p. 115740D.

[3] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in International
Conference on Learning Representations, 2018.

[4] C. Zhu, Y. Cheng, Z. Gan, S. Sun, T. Goldstein, and J. Liu, “Freelb:
Enhanced adversarial training for natural language understanding,” in
International Conference on Learning Representations, 2019.

[5] Y. Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and
S. Nakamura, “Learning to generate pseudo-code from source code using
statistical machine translation (t),” in 2015 30th IEEE/ACM International
Conference on Automated Software Engineering (ASE).
IEEE, 2015,
pp. 574–584.

[6] S. Kulal, P. Pasupat, K. Chandra, M. Lee, O. Padon, A. Aiken, and P. S.
Liang, “Spoc: Search-based pseudocode to code,” Advances in Neural
Information Processing Systems, vol. 32, pp. 11 906–11 917, 2019.
[7] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment generation
with hybrid lexical and syntactical information,” Empirical Software
Engineering, vol. 25, no. 3, pp. 2179–2217, 2020.

[8] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, “Retrieval-based neu-
ral source code summarization,” in 2020 IEEE/ACM 42nd International
Conference on Software Engineering.

IEEE, 2020, pp. 1385–1397.

[9] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing source
code using a neural attention model,” in Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics, 2016, pp.
2073–2083.

[10] M. Allamanis, H. Peng, and C. Sutton, “A convolutional attention
network for extreme summarization of source code,” in International
conference on machine learning, 2016, pp. 2091–2100.

[11] W. Zheng, H.-Y. Zhou, M. Li, and J. Wu, “Code attention: Translat-
ing code to comments by exploiting domain features,” arXiv preprint
arXiv:1709.07642, 2017.

[12] Y. Liang and K. Zhu, “Automatic generation of text descriptive com-
ments for code blocks,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, vol. 32, no. 1, 2018.

[13] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment gener-
ation,” in 2018 IEEE/ACM 26th International Conference on Program
Comprehension.

IEEE, 2018, pp. 200–210.

[14] A. LeClair, S. Jiang, and C. McMillan, “A neural model for gener-
ating natural language summaries of program subroutines,” in 2019
IEEE/ACM 41st International Conference on Software Engineering.
IEEE, 2019, pp. 795–806.

[15] A. LeClair, S. Haque, L. Wu, and C. McMillan, “Improved code
summarization via a graph neural network,” in Proceedings of the 28th
International Conference on Program Comprehension, 2020, pp. 184–
195.

[16] S. Jiang, A. Armaly, and C. McMillan, “Automatically generating
commit messages from diffs using neural machine translation,” in
2017 32nd IEEE/ACM International Conference on Automated Software
Engineering.

IEEE, 2017, pp. 135–146.

[17] S. Xu, Y. Yao, F. Xu, T. Gu, H. Tong, and J. Lu, “Commit message gen-
eration for source code changes,” in 28th International Joint Conference
on Artiﬁcial Intelligence, 2019, pp. 3975–3981.

[18] S. Liu, C. Gao, S. Chen, N. L. Yiu, and Y. Liu, “Atom: Commit message
generation based on abstract syntax tree and hybrid ranking,” IEEE
Transactions on Software Engineering, 2020.

[19] Z. Liu, X. Xia, C. Treude, D. Lo, and S. Li, “Automatic generation
of pull request descriptions,” in 2019 34th IEEE/ACM International
Conference on Automated Software Engineering.
IEEE, 2019, pp. 176–
188.

[20] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu,
“Improving automatic source code summarization via deep reinforce-
ment learning,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, 2018, pp. 397–407.

[21] W. Wang, Y. Zhang, Y. Sui, Y. Wan, Z. Zhao, J. Wu, P. Yu, and
G. Xu, “Reinforcement-learning-guided source code summarization via
hierarchical attention,” IEEE Transactions on Software Engineering,
2020.

[22] X. HU, G. LI, X. XIA, D. LO, S. LU, and Z. JIN, “Summarizing source
code with transferred api knowledge,” in Proceedings of the Twenty-
Seventh International Joint Conference on Artiﬁcial Intelligence, vol. 19,
2018, pp. 2269–2275.

[23] H. Fudaba, Y. Oda, K. Akabe, G. Neubig, H. Hata, S. Sakti, T. Toda, and
S. Nakamura, “Pseudogen: A tool to automatically generate pseudo-code
from source code,” in 2015 30th IEEE/ACM International Conference
on Automated Software Engineering.

IEEE, 2015, pp. 824–829.

[24] T. Haije, B. O. K. Intelligentie, E. Gavves, and H. Heuer, “Automatic
translation model,” Inf. Softw.

comment generation using a neural
Technol, vol. 55, no. 3, pp. 258–268, 2016.

[25] A. Alhefdhi, H. K. Dam, H. Hata, and A. Ghose, “Generating pseudo-
code from source code using deep learning,” in 2018 25th Australasian
Software Engineering Conference.

IEEE, 2018, pp. 21–25.

[26] S. Xu and Y. Xiong, “Automatic generation of pseudocode with atten-
tion seq2seq model,” in 2018 25th Asia-Paciﬁc Software Engineering
Conference.

IEEE, 2018, pp. 711–712.

[27] L. Dong and M. Lapata, “Coarse-to-ﬁne decoding for neural semantic
parsing,” in 56th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 2018, pp. 731–
742.

[28] S. Rai and A. Gupta, “Generation of pseudo code from the python source
code using rule-based machine translation,” arXiv e-prints, pp. arXiv–
1906, 2019.

[29] Y. Xiong, S. Xu, K. Rong, X. Liu, X. Kong, S. Li, P. Yu, and Y. Zhu,
“Code2text: Dual attention syntax annotation networks for structure-
aware code translation,” in International Conference on Database Sys-
tems for Advanced Applications. Springer, 2020, pp. 87–103.

[30] Y. Deng, H. Huang, X. Chen, Z. Liu, S. Wu, J. Xuan, and Z. Li, “From
code to natural language: Type-aware sketch-based seq2seq learning,”
in International Conference on Database Systems for Advanced Appli-
cations. Springer, 2020, pp. 352–368.

[31] K. Cao, C. Chen, S. Baltes, C. Treude, and X. Chen, “Automated
query reformulation for efﬁcient search based on query logs from
stack overﬂow,” in 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE).

IEEE, 2021, pp. 1273–1285.

[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[33] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv

preprint arXiv:1607.06450, 2016.

[34] J.-B. Cordonnier, A. Loukas, and M. Jaggi, “On the relationship between
self-attention and convolutional layers,” in International Conference on
Learning Representations, 2019.

[35] J. Gong, X. Qiu, X. Chen, D. Liang, and X.-J. Huang, “Convolutional
language inference,” in Proceedings
interaction network for natural
of the 2018 Conference on Empirical Methods in Natural Language
Processing, 2018, pp. 1576–1585.

[36] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Con-
volutional sequence to sequence learning,” in International Conference
on Machine Learning. PMLR, 2017, pp. 1243–1252.

[37] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling
with gated convolutional networks,” in International conference on
machine learning. PMLR, 2017, pp. 933–941.

[38] F. Stahlberg and B. Byrne, “On nmt search errors and model errors: Cat
got your tongue?” in Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing, 2019, pp. 3356–3362.

[39] S. Wiseman and A. M. Rush, “Sequence-to-sequence learning as beam-
search optimization,” in Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing, 2016, pp. 1296–1306.

[40] C. Meister, R. Cotterell, and T. Vieira, “If beam search is the answer,
what was the question?” in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP), 2020,
pp. 2173–2185.

[41] S. Wang, B. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-
attention with linear complexity,” arXiv preprint arXiv:2006.04768,
2020.

[42] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng,
“Synthesizer: Rethinking self-attention in transformer models,” arXiv
preprint arXiv:2005.00743, 2020.

[43] A. Henry, P. R. Dachapally, S. Pawar, and Y. Chen, “Query-key
the Association for

normalization for transformers,” in Findings of
Computational Linguistics: EMNLP 2020, 2020.

[44] S. Yang, Y. Wang, and X. Chu, “A survey of deep learning techniques
for neural machine translation,” arXiv preprint arXiv:2002.07526, 2020.
[45] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics, 2002,
pp. 311–318.

[46] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evalua-
tion with improved correlation with human judgments,” in Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for
machine translation and/or summarization, 2005, pp. 65–72.

[47] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”

in Text summarization branches out, 2004, pp. 74–81.

[48] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-
based image description evaluation,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2015, pp. 4566–4575.
[49] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
information processing

with neural networks,” Advances in neural
systems, vol. 27, pp. 3104–3112, 2014.

[50] D. Bahdanau, K. H. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” in 3rd International Conference
on Learning Representations, 2015.

[51] Z. Li, Y. Wu, B. Peng, X. Chen, Z. Sun, Y. Liu, and D. Yu, “Secnn: A
semantic cnn parser for code comment generation,” Journal of Systems
and Software, vol. 181, p. 111036, 2021.

[52] G. Yang, X. Chen, J. Cao, S. Xu, Z. Cui, C. Yu, and K. Liu, “Comformer:
Code comment generation via transformer and fusion method-based
hybrid code representation,” arXiv preprint arXiv:2107.03644, 2021.

