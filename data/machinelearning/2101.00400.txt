An Elo-like System for Massive Multiplayer Competitions

Aram Ebtekar
Vancouver, BC, Canada
aramebtech@gmail.com

Paul Liu
Stanford University
Stanford, CA, USA
paul.liu@stanford.edu

1
2
0
2

n
a
J

2

]

R

I
.
s
c
[

1
v
0
0
4
0
0
.
1
0
1
2
:
v
i
X
r
a

ABSTRACT
Rating systems play an important role in competitive sports and
games. They provide a measure of player skill, which incentivizes
competitive performances and enables balanced match-ups. In this
paper, we present a novel Bayesian rating system for contests with
many participants. It is widely applicable to competition formats
with discrete ranked matches, such as online programming competi-
tions, obstacle courses races, and some video games. The simplicity
of our system allows us to prove theoretical bounds on robustness
and runtime. In addition, we show that the system aligns incentives:
that is, a player who seeks to maximize their rating will never want
to underperform. Experimentally, the rating system rivals or sur-
passes existing systems in prediction accuracy, and computes faster
than existing systems by up to an order of magnitude.

1 INTRODUCTION
Competitions, in the form of sports, games, and examinations, have
been with us since antiquity. Many competitions grade perfor-
mances along a numerical scale, such as a score on a test or a
completion time in a race. In the case of a college admissions exam
or a track race, scores are standardized so that a given score on
two different occasions carries the same meaning. However, in
events that feature novelty, subjectivity, or close interaction, stan-
dardization is difficult. The Spartan Races, completed by millions
of runners, feature a variety of obstacles placed on hiking trails
around the world [8]. Rock climbing, a sport to be added to the
2020 Olympics, likewise has routes set specifically for each com-
petition. DanceSport, gymnastics, and figure skating competitions
have a panel of judges who rank contestants against one another;
these subjective scores are known to be noisy [26]. Most board
games feature considerable inter-player interaction. In all these
cases, scores can only be used to compare and rank participants at
the same event. Players, spectators, and contest organizers who are
interested in comparing playersâ€™ skill levels across different com-
petitions will need to aggregate the entire history of such rankings.
A strong player, then, is one who consistently wins against weaker
players. To quantify skill, we need a rating system.

Good rating systems are difficult to create, as they must bal-
ance several mutually constraining objectives. First and foremost,
the rating system must be accurate, in that ratings provide use-
ful predictors of contest outcomes. Second, the ratings must be
efficient to compute: in video game applications, rating systems
are predominantly used for matchmaking in massively multiplayer
online games (such as Halo, CounterStrike, League of Legends,
etc.) [20, 24, 28]. These games have hundreds of millions of players
playing tens of millions of games per day, necessitating certain
latency and memory requirements for the rating system [9]. Third,
the rating system must align incentives. That is, players should

not modify their performance to â€œgameâ€ the rating system. Rating
systems that can be gamed often create disastrous consequences to
player-base, more often than not leading to the loss of players from
the game [3]. Finally, the ratings provided by the system must be
human-interpretable: ratings are typically represented to players as
a single number encapsulating their overall skill, and many players
want to understand and predict how their performances affect their
rating [16].

Classically, rating systems were designed for two-player games.
The famous Elo system [13], as well as its Bayesian successors
Glicko and Glicko-2, have been widely applied to games such as
Chess and Go [16â€“18]. Both Glicko versions model each playerâ€™s
skill as a real random variable that evolves with time according
to Brownian motion. Inference is done by entering these variables
into the Bradley-Terry model, which predicts probabilities of game
outcomes. Glicko-2 refines the Glicko system by adding a rating
volatility parameter. Unfortunately, Glicko-2 is known to be flawed
in practice, potentially incentivising players to lose. This was most
notably exploited in the popular game of Pokemon Go [3]; see
Section 5.1 for a discussion of this issue.

The family of Elo-like methods just described only utilize the
binary outcome of a match. In settings where a scoring system
provides a more fine-grained measure of match performance, Ko-
valchik [22] has shown variants of Elo that are able to take advan-
tage of score information. For competitions consisting of several set
tasks, such as academic olympiads, ForiÅ¡ek [14] developed a model
in which each task gives a different â€œresponseâ€ to the player: the to-
tal response then predicts match outcomes. However, such systems
are often highly application-dependent and hard to calibrate.

Though Elo-like systems are widely used in two-player contests,
one neednâ€™t look far to find competitions that involve much more
than two players. Aside from the aforementioned sporting examples,
there are video games such as CounterStrike and Halo, as well as
academic olympiads: notably, programming contest platforms such
as Codeforces, TopCoder, and Kaggle [2, 5, 7]. In these applications,
the number of contestants can easily reach into the thousands.
Some more recent works present interesting methods to deal with
competitions between two teams [11, 19, 21, 23], but they do not
present efficient extensions for settings in which players are sorted
into more than two, let alone thousands, of distinct places.

In a many-player ranked competition, it is important to note that
the pairwise match outcomes are not independent, as they would be
in a series of 1v1 matches. Thus, TrueSkill [20] and its variants [12,
24, 25] model a playerâ€™s performance during each contest as a single
random variable. The overall rankings are assumed to reveal the
total order among these hidden performance variables, with various
strategies used to model ties and teams. These TrueSkill algorithms
are efficient in practice, successfully rating userbases that number

 
 
 
 
 
 
well into the millions (the Halo series, for example, has over 60
million sales since 2001 [4]).

The main disadvantage of TrueSkill is its complexity: originally
developed by Microsoft for the popular Halo video game, TrueSkill
performs approximate belief propagation on a factor graph, which
is iterated until convergence [20]. Aside from being less human-
interpretable, this complexity means that, to our knowledge, there
are no proofs of key properties such as runtime and incentive align-
ment. Even when these properties are discussed [24], no rigorous
justification is provided. In addition, we are not aware of any work
that extends TrueSkill to non-Gaussian performance models, which
might be desirable to limit the influence of outlier performances
(see Section 5.2).

It might be for these reasons that platforms such as Codeforces
and TopCoder opted for their own custom rating systems. These sys-
tems are not published in academia and do not come with Bayesian
justifications. However, they retain the formulaic simplicity of Elo
and Glicko, extending them to settings with much more than two
players. The Codeforces system includes ad hoc heuristics to dis-
tinguish top players, while curbing rampant inflation. TopCoderâ€™s
formulas are more principled from a statistical perspective; how-
ever, it has a volatility parameter similar to Glicko-2, and hence
suffers from similar exploits [14]. Despite their flaws, these sys-
tems have been in place for over a decade, and have more recently
gained adoption by additional platforms such as CodeChef and
LeetCode [1, 6].

Our contributions. In this paper, we describe the Elo-MMR rat-
ing system, obtained by a principled approximation of a Bayesian
model very similar to TrueSkill. It is fast, embarrassingly parallel,
and makes accurate predictions. Most interesting of all, its simplic-
ity allows us to rigorously analyze its properties: the â€œMMRâ€ in
the name stands for â€œMassiveâ€, â€œMonotonicâ€, and â€œRobustâ€. â€œMas-
siveâ€ means that it supports any number of players with a runtime
that scales linearly; â€œmonotonicâ€ means that it aligns incentives
so that a rating-maximizing player always wants to perform well;
â€œrobustâ€ means that rating changes are bounded, with the bound
being smaller for more consistent players than for volatile play-
ers. Robustness turns out to be a natural byproduct of accurately
modeling performances with heavy-tailed distributions, such as
the logistic. TrueSkill is believed to satisfy the first two properties,
albeit without proof, but fails robustness. Codeforces only satisfies
aligned incentives, and TopCoder only satisfies robustness.

Experimentally, we show that Elo-MMR achieves state-of-the-art
performance in terms of both prediction accuracy and runtime. In
particular, we process the entire Codeforces database of over 300K
rated users and 1000 contests in well under a minute, beating the ex-
isting Codeforces system by an order of magnitude while improving
upon its accuracy. A difficulty we faced was the scarcity of efficient
open-source rating system implementations. In an effort to aid
researchers and practitioners alike, we provide open-source imple-
mentations of all rating systems, datasets, and additional processing
used in our experiments at https://github.com/EbTech/EloR/.

Organization. In Section 2, we formalize the details of our Bayesian
model. We then show how to estimate player skill under this model
in Section 3, and develop some intuitions of the resulting formulas.

2

As a further refinement, Section 4 models skill evolutions from play-
ers training or atrophying between competitions. This modeling is
quite tricky as we choose to retain playersâ€™ momentum while en-
suring it cannot be exploited for incentive-misaligned rating gains.
Section 5 proves that the system as a whole satisfies several salient
properties, the most critical of which is aligned incentives. Finally,
we present experimental evaluations in Section 6.

2 A BAYESIAN MODEL FOR MASSIVE

COMPETITIONS

We now describe the setting formally, denoting random variables
by capital letters. A series of competitive rounds, indexed by ğ‘¡ =
1, 2, 3, . . ., take place sequentially in time. Each round has a set of
participating players Pğ‘¡ , which may in general overlap between
rounds. A playerâ€™s skill is likely to change with time, so we repre-
sent the skill of player ğ‘– at time ğ‘¡ by a real random variable ğ‘†ğ‘–,ğ‘¡ .

.

In round ğ‘¡, each player ğ‘– âˆˆ Pğ‘¡ competes at some performance
level ğ‘ƒğ‘–,ğ‘¡ , typically close to their current skill ğ‘†ğ‘–,ğ‘¡ . The deviations
are assumed to be i.i.d. and independent of {ğ‘†ğ‘–,ğ‘¡ }ğ‘– âˆˆ Pğ‘¡
{ğ‘ƒğ‘–,ğ‘¡ âˆ’ğ‘†ğ‘–,ğ‘¡ }ğ‘– âˆˆ Pğ‘¡
Performances are not observed directly; instead, a ranking gives
. In particular,
the relative order among all performances {ğ‘ƒğ‘–,ğ‘¡ }ğ‘– âˆˆ Pğ‘¡
ties are modelled to occur when performances are exactly equal,
a zero-probability event when their distributions are continuous.1
This ranking constitutes the observational evidence ğ¸ğ‘¡ for our
Bayesian updates. The rating system seeks to estimate the skill ğ‘†ğ‘–,ğ‘¡
of every player at the present time ğ‘¡, given the historical round
rankings ğ¸ â‰¤ğ‘¡ := {ğ¸1, . . . , ğ¸ğ‘¡ }.

We overload the notation Pr for both probabilities and probabil-
ity densities: the latter interpretation applies to zero-probability
events, such as in Pr(ğ‘†ğ‘–,ğ‘¡ = ğ‘ ). We also use colons as shorthand for
collections of variables differing only in a subscript: for instance,
ğ‘ƒ:,ğ‘¡ := {ğ‘ƒğ‘–,ğ‘¡ }ğ‘– âˆˆ Pğ‘¡
. The joint distribution described by our Bayesian
model factorizes as follows:

Pr(ğ‘†:,:, ğ‘ƒ:,:, ğ¸:)

(cid:214)

=

Pr(ğ‘†ğ‘–,0)

ğ‘–

(cid:214)

ğ‘–,ğ‘¡

Pr(ğ‘†ğ‘–,ğ‘¡ | ğ‘†ğ‘–,ğ‘¡ âˆ’1)

(cid:214)

ğ‘–,ğ‘¡

Pr(ğ‘ƒğ‘–,ğ‘¡ | ğ‘†ğ‘–,ğ‘¡ )

Pr(ğ¸ğ‘¡ | ğ‘ƒ:,ğ‘¡ ),

(cid:214)

ğ‘¡

(1)

where Pr(ğ‘†ğ‘–,0) is the initial skill prior,
Pr(ğ‘†ğ‘–,ğ‘¡ | ğ‘†ğ‘–,ğ‘¡ âˆ’1) is the skill evolution model (Section 4),

Pr(ğ‘ƒğ‘–,ğ‘¡ | ğ‘†ğ‘–,ğ‘¡ ) is the performance model, and
Pr(ğ¸ğ‘¡ | ğ‘ƒ:,ğ‘¡ ) is the evidence model.

For the first three factors, we will specify log-concave distributions
(see Definition 3.1). The evidence model, on the other hand, is a
deterministic indicator. It equals one when ğ¸ğ‘¡ is consistent with
the relative ordering among {ğ‘ƒğ‘–,ğ‘¡ }ğ‘– âˆˆ Pğ‘¡

, and zero otherwise.

Finally, our model assumes that the number of participants |Pğ‘¡ | is
large. The main idea behind our algorithm is that, in sufficiently mas-
sive competitions, from the evidence ğ¸ğ‘¡ we can infer very precise
estimates for {ğ‘ƒğ‘–,ğ‘¡ }ğ‘– âˆˆ Pğ‘¡
. Hence, we can treat these performances
as if they were observed directly.

That is, suppose we have the skill prior at round ğ‘¡:

ğœ‹ğ‘–,ğ‘¡ (ğ‘ ) := Pr(ğ‘†ğ‘–,ğ‘¡ = ğ‘  | ğ‘ƒğ‘–,<ğ‘¡ ).

(2)

1The relevant limiting procedure is to treat performances within ğœ–-width buckets as
ties, and letting ğœ– â†’ 0. This technicality appears in the proof of Theorem 3.2.

Now, we observe ğ¸ğ‘¡ . By Equation (1), it is conditionally indepen-

dent of ğ‘†ğ‘–,ğ‘¡ , given ğ‘ƒğ‘–, â‰¤ğ‘¡ . By the law of total probability,

3 A TWO-PHASE ALGORITHM FOR SKILL

ESTIMATION

Pr(ğ‘†ğ‘–,ğ‘¡ = ğ‘  | ğ‘ƒğ‘–,<ğ‘¡ , ğ¸ğ‘¡ )

âˆ«

=

Pr(ğ‘†ğ‘–,ğ‘¡ = ğ‘  | ğ‘ƒğ‘–,<ğ‘¡ , ğ‘ƒğ‘–,ğ‘¡ = ğ‘) Pr(ğ‘ƒğ‘–,ğ‘¡ = ğ‘ | ğ‘ƒğ‘–,<ğ‘¡ , ğ¸ğ‘¡ ) dğ‘

â†’ Pr(ğ‘†ğ‘–,ğ‘¡ = ğ‘  | ğ‘ƒğ‘–, â‰¤ğ‘¡ )

almost surely as |Pğ‘¡ | â†’ âˆ.

The integral is intractable in general, since the performance poste-
rior Pr(ğ‘ƒğ‘–,ğ‘¡ = ğ‘ | ğ‘ƒğ‘–,<ğ‘¡ , ğ¸ğ‘¡ ) depends not only on player ğ‘–, but also
on our belief regarding the skills of all ğ‘— âˆˆ Pğ‘¡ . However, in the limit
of infinite participants, Doobâ€™s consistency theorem [15] implies
that it concentrates at the true value ğ‘ƒğ‘–,ğ‘¡ . Since our posteriors are
continuous, the convergence holds for all ğ‘  simultaneously.
Indeed, we donâ€™t even need the full evidence ğ¸ğ‘¡ . Let ğ¸ğ¿

ğ‘–,ğ‘¡ = { ğ‘— âˆˆ
P : ğ‘ƒ ğ‘—,ğ‘¡ > ğ‘ƒğ‘–,ğ‘¡ } be the set of players against whom ğ‘– lost, and
ğ¸ğ‘Š
ğ‘–,ğ‘¡ = { ğ‘— âˆˆ P : ğ‘ƒ ğ‘—,ğ‘¡ < ğ‘ƒğ‘–,ğ‘¡ } be the set of players against whom
ğ‘– won. That is, we only see who wins, draws, and loses against ğ‘–.
ğ‘ƒğ‘–,ğ‘¡ remains identifiable using only (ğ¸ğ¿
ğ‘–,ğ‘¡ ), which will be more
convenient for our purposes.

ğ‘–,ğ‘¡ , ğ¸ğ‘Š

Passing to the limit |Pğ‘¡ | â†’ âˆ serves to justify some common
simplifications made by algorithms such as TrueSkill: since condi-
tioning on ğ‘ƒğ‘–, â‰¤ğ‘¡ makes the skills of different players independent of
one another, it becomes accurate to model them as such. In addition
to simplifying derivations, this fact ensures that a playerâ€™s poste-
rior is unaffected by rounds in which they are not a participant,
arguably a desirable property in its own right. Furthermore, ğ‘ƒğ‘–, â‰¤ğ‘¡
being a sufficient statistic for skill prediction renders any additional
information, such as domain-specific raw scores, redundant.

âˆšï¸ƒ

log 1

ğ›¿ ) participants.

Finally, a word on the rate of convergence. Suppose we want
our estimate to be within ğœ– of ğ‘ƒğ‘–,ğ‘¡ , with probability at least 1 âˆ’ ğ›¿.
By asymptotic normality of the posterior [15], it suffices to have
ğ‘‚ ( 1
ğœ– 2
When the initial prior, performance model, and evolution model
are all Gaussian, treating ğ‘ƒğ‘–,ğ‘¡ as certain is the only simplifying
approximation we will make; that is, in the limit |Pğ‘¡ | â†’ âˆ, our
method performs exact inference on Equation (1). In the following
sections, we focus some attention on generalizing the performance
model to non-Gaussian log-concave families, parametrized by lo-
cation and scale. We will use the logistic distribution as a running
example and see that it induces robustness; however, our framework
is agnostic to the specific distributions used.

The rating ğœ‡ğ‘–,ğ‘¡ of player ğ‘– after round ğ‘¡ should be a statistic that
summarizes their posterior distribution: weâ€™ll use the maximum a
posteriori (MAP) estimate, obtained by setting ğ‘  to maximize the
posterior Pr(ğ‘†ğ‘–,ğ‘¡ = ğ‘  | ğ‘ƒğ‘–, â‰¤ğ‘¡ ). By Bayesâ€™ rule,

ğœ‡ğ‘–,ğ‘¡ := arg max

ğ‘ 

ğœ‹ğ‘–,ğ‘¡ (ğ‘ ) Pr(ğ‘ƒğ‘–,ğ‘¡ | ğ‘†ğ‘–,ğ‘¡ = ğ‘ ).

(3)

This objective suggests a two-phase algorithm to update each player
ğ‘– âˆˆ Pğ‘¡ at round ğ‘¡. In phase one, we estimate ğ‘ƒğ‘–,ğ‘¡ from (ğ¸ğ¿
ğ‘–,ğ‘¡ ). By
Doobâ€™s consistency theorem, our estimate is extremely precise when
|Pğ‘¡ | is large, so we assume it to be exact. In phase two, we update
our posterior for ğ‘†ğ‘–,ğ‘¡ and the rating ğœ‡ğ‘–,ğ‘¡ according to Equation (3).
We will occasionally make use of the prior rating, defined as

ğ‘–,ğ‘¡ , ğ¸ğ‘Š

ğœ‡ğœ‹
ğ‘–,ğ‘¡ := arg max

ğ‘ 

ğœ‹ğ‘–,ğ‘¡ (ğ‘ ).

3

3.1 Performance estimation
In this section, we describe the first phase of Elo-MMR. For nota-
tional convenience, we assume all probability expressions to be
conditioned on the prior context ğ‘ƒğ‘–,<ğ‘¡ , and omit the subscript ğ‘¡.
Our prior belief on each playerâ€™s skill ğ‘†ğ‘– implies a prior distribu-

tion on ğ‘ƒğ‘– . Letâ€™s denote its probability density function (pdf) by

ğ‘“ğ‘– (ğ‘) := Pr(ğ‘ƒğ‘– = ğ‘) =

âˆ«

ğœ‹ğ‘– (ğ‘ ) Pr(ğ‘ƒğ‘– = ğ‘ | ğ‘†ğ‘– = ğ‘ ) dğ‘ ,

(4)

where ğœ‹ğ‘– (ğ‘ ) was defined in Equation (2). Let

ğ¹ğ‘– (ğ‘) := Pr(ğ‘ƒğ‘– â‰¤ ğ‘) =

âˆ« ğ‘

âˆ’âˆ

ğ‘“ğ‘– (ğ‘¥) dğ‘¥,

be the corresponding cumulative distribution function (cdf). For the
purpose of analysis, weâ€™ll also define the following â€œlossâ€, â€œdrawâ€,
and â€œvictoryâ€ functions:

ğ‘™ğ‘– (ğ‘) :=

ğ‘‘ğ‘– (ğ‘) :=

ğ‘£ğ‘– (ğ‘) :=

d
dğ‘

d
dğ‘
d
dğ‘

ln(1 âˆ’ ğ¹ğ‘– (ğ‘)) =

âˆ’ğ‘“ğ‘– (ğ‘)
1 âˆ’ ğ¹ğ‘– (ğ‘)

,

ln ğ‘“ğ‘– (ğ‘) =

ln ğ¹ğ‘– (ğ‘) =

ğ‘“ â€²
ğ‘– (ğ‘)
ğ‘“ğ‘– (ğ‘)
ğ‘“ğ‘– (ğ‘)
ğ¹ğ‘– (ğ‘)

,

.

Evidently, ğ‘™ğ‘– (ğ‘) < 0 < ğ‘£ğ‘– (ğ‘). Now we define what it means for

the deviation ğ‘ƒğ‘– âˆ’ ğ‘†ğ‘– to be log-concave.

Definition 3.1. An absolutely continuous random variable on a
convex domain is log-concave if its probability density function ğ‘“ is
positive on its domain and satisfies

ğ‘“ (ğœƒğ‘¥ + (1 âˆ’ ğœƒ )ğ‘¦) > ğ‘“ (ğ‘¥)ğœƒ ğ‘“ (ğ‘¦)1âˆ’ğœƒ , âˆ€ğœƒ âˆˆ (0, 1), ğ‘¥ â‰  ğ‘¦.

We note that log-concave distributions appear widely, and in-
clude the Gaussian and logistic distributions used in Glicko, TrueSkill,
and many others. Weâ€™ll see inductively that our prior ğœ‹ğ‘– is log-
concave at every round. Since log-concave densities are closed
under convolution [10], the independent sum ğ‘ƒğ‘– = ğ‘†ğ‘– + (ğ‘ƒğ‘– âˆ’ ğ‘†ğ‘– ) is
also log-concave. The following lemma (proved in the appendix)
makes log-concavity very convenient:

Lemma 3.1. If ğ‘“ğ‘– is continuously differentiable and log-concave,

then the functions ğ‘™ğ‘–, ğ‘‘ğ‘–, ğ‘£ğ‘– are continuous, strictly decreasing, and

ğ‘™ğ‘– (ğ‘) < ğ‘‘ğ‘– (ğ‘) < ğ‘£ğ‘– (ğ‘) for all ğ‘.

For the remainder of this section, we fix the analysis with respect
to some player ğ‘–. As argued in Section 2, ğ‘ƒğ‘– concentrates very
narrowly in the posterior. Hence, we can estimate ğ‘ƒğ‘– by its MAP,
choosing ğ‘ so as to maximize:

Pr(ğ‘ƒğ‘– = ğ‘ | ğ¸ğ¿

ğ‘– , ğ¸ğ‘Š
ğ‘–

) âˆ ğ‘“ğ‘– (ğ‘) Pr(ğ¸ğ¿

ğ‘– , ğ¸ğ‘Š
ğ‘–

| ğ‘ƒğ‘– = ğ‘).

Define ğ‘— â‰» ğ‘–, ğ‘— â‰º ğ‘–, ğ‘— âˆ¼ ğ‘– as shorthand for ğ‘— âˆˆ ğ¸ğ¿

ğ‘– , ğ‘— âˆˆ ğ¸ğ‘Š
,
ğ‘–
) (that is, ğ‘ƒ ğ‘— > ğ‘ƒğ‘– , ğ‘ƒ ğ‘— < ğ‘ƒğ‘– , ğ‘ƒ ğ‘— = ğ‘ƒğ‘– ), respectively.

ğ‘— âˆˆ P \ (ğ¸ğ¿
The following theorem yields our MAP estimate:

ğ‘– âˆª ğ¸ğ‘Š
ğ‘–

A logistic distribution with variance ğ›¿ 2

ğ¹ ğ‘— (ğ‘¥) =

ğ‘“ğ‘— (ğ‘¥) =

1

ğ‘— )/ Â¯ğ›¿ ğ‘—
1 + ğ‘’âˆ’(ğ‘¥âˆ’ğœ‡ğœ‹
ğ‘— )/ Â¯ğ›¿ ğ‘—
ğ‘’ (ğ‘¥âˆ’ğœ‡ğœ‹
(cid:16)1 + ğ‘’ (ğ‘¥âˆ’ğœ‡ğœ‹

Â¯ğ›¿ ğ‘—

ğ‘— has cdf and pdf:
(cid:33)
(cid:32)

1 + tanh

1
2

=

,

ğ‘¥ âˆ’ ğœ‡ğœ‹
ğ‘—
2 Â¯ğ›¿ ğ‘—
ğ‘¥ âˆ’ ğœ‡ğœ‹
ğ‘—
2 Â¯ğ›¿ ğ‘—

.

ğ‘— )/ Â¯ğ›¿ ğ‘— (cid:17)2 =

1
4 Â¯ğ›¿ ğ‘—

sech2

Figure 1: ğ¿2 versus ğ¿ğ‘… for typical values (left). Gaussian ver-
sus logistic probability density functions (right).

Theorem 3.2. Suppose that for all ğ‘—, ğ‘“ğ‘— is continuously differen-
tiable and log-concave. Then the unique maximizer of Pr(ğ‘ƒğ‘– = ğ‘ |
ğ‘– , ğ¸ğ‘Š
ğ¸ğ¿
ğ‘–

) is given by the unique zero of

ğ‘„ğ‘– (ğ‘) :=

ğ‘™ ğ‘— (ğ‘) +

âˆ‘ï¸

ğ‘— â‰»ğ‘–

âˆ‘ï¸

ğ‘—âˆ¼ğ‘–

ğ‘‘ ğ‘— (ğ‘) +

ğ‘£ ğ‘— (ğ‘).

âˆ‘ï¸

ğ‘— â‰ºğ‘–

The proof is relegated to the appendix. Intuitively, weâ€™re saying
that the performance is the balance point between appropriately
weighted wins, draws, and losses. Letâ€™s look at two specializations
of our general model, to serve as running examples in this paper.

Gaussian performance model. If both ğ‘† ğ‘— and ğ‘ƒ ğ‘— âˆ’ ğ‘† ğ‘— are assumed
to be Gaussian with known means and variances, then their inde-
pendent sum ğ‘ƒ ğ‘— will also be a known Gaussian. It is analytic and
log-concave, so Theorem 3.2 applies.

We substitute the well-known Gaussian pdf and cdf for ğ‘“ğ‘— and ğ¹ ğ‘— ,
respectively. A simple binary search, or faster numerical techniques
such as the Illinois algorithm or Newtonâ€™s method, can be employed
to solve for the maximizing ğ‘.

Logistic performance model. Now we assume the performance
deviation ğ‘ƒ ğ‘— âˆ’ ğ‘† ğ‘— has a logistic distribution with mean 0 and vari-
ance ğ›½2. In general, the rating system administrator is free to set ğ›½
differently for each contest. Since shorter contests tend to be more
variable, one reasonable choice might be to make 1/ğ›½2 proportional
to the contest duration.

Given the mean and variance of the skill prior, the independent
sum ğ‘ƒ ğ‘— = ğ‘† ğ‘— + (ğ‘ƒ ğ‘— âˆ’ ğ‘† ğ‘— ) would have the same mean, and a variance
thatâ€™s increased by ğ›½2. Unfortunately, weâ€™ll see that the logistic
performance model implies a form of skill prior from which itâ€™s
tough to extract a mean and variance. Even if we could, the sum
does not yield a simple distribution.

For experienced players, we expect ğ‘† ğ‘— to contribute much less
variance than ğ‘ƒ ğ‘— âˆ’ ğ‘† ğ‘— ; thus, in our heuristic approximation, we take
ğ‘ƒ ğ‘— to have the same form of distribution as the latter. That is, we
take ğ‘ƒ ğ‘— to be logistic, centered at the prior rating ğœ‡ğœ‹
ğ‘— = arg max ğœ‹ ğ‘— ,
with variance ğ›¿ 2
ğ‘— + ğ›½2, where ğœ ğ‘— will be given by Equation (8).
This distribution is analytic and log-concave, so the same methods
âˆš
based on Theorem 3.2 apply. Define the scale parameter Â¯ğ›¿ ğ‘— :=
3
ğœ‹ ğ›¿ ğ‘— .

ğ‘— = ğœ2

4

The logistic distribution satisfies two very convenient relations:
ğ‘— (ğ‘¥) = ğ‘“ğ‘— (ğ‘¥) = ğ¹ ğ‘— (ğ‘¥)(1 âˆ’ ğ¹ ğ‘— (ğ‘¥))/ Â¯ğ›¿ ğ‘— ,
ğ¹ â€²
ğ‘— (ğ‘¥) = ğ‘“ğ‘— (ğ‘¥)(1 âˆ’ 2ğ¹ ğ‘— (ğ‘¥))/ Â¯ğ›¿ ğ‘— ,
ğ‘“ â€²

from which it follows that

ğ‘‘ ğ‘— (ğ‘) =

1 âˆ’ 2ğ¹ ğ‘— (ğ‘)
Â¯ğ›¿

=

âˆ’ğ¹ ğ‘— (ğ‘)
Â¯ğ›¿

+

1 âˆ’ ğ¹ ğ‘— (ğ‘)
Â¯ğ›¿

= ğ‘™ ğ‘— (ğ‘) + ğ‘£ ğ‘— (ğ‘).

In other words, a tie counts as the sum of a win and a loss. This
can be compared to the approach (used in Elo, Glicko, TopCoder,
and Codeforces) of treating each tie as half a win plus half a loss.2

Finally, putting everything together:

ğ‘„ğ‘– (ğ‘) =

ğ‘™ ğ‘— (ğ‘) +

âˆ‘ï¸

ğ‘— âª°ğ‘–

âˆ‘ï¸

ğ‘— âª¯ğ‘–

ğ‘£ ğ‘— (ğ‘) =

âˆ’ğ¹ ğ‘— (ğ‘)
Â¯ğ›¿ ğ‘—

âˆ‘ï¸

ğ‘— âª°ğ‘–

+

âˆ‘ï¸

ğ‘— âª¯ğ‘–

1 âˆ’ ğ¹ ğ‘— (ğ‘)
Â¯ğ›¿ ğ‘—

.

Our estimate for ğ‘ƒğ‘– is the zero of this expression. The terms on
the right correspond to probabilities of winning and losing against
each player ğ‘—, weighted by 1/ Â¯ğ›¿ ğ‘— . Accordingly, we can interpret
(cid:205)ğ‘— âˆˆ P (1 âˆ’ ğ¹ ğ‘— (ğ‘))/ Â¯ğ›¿ ğ‘— as a weighted expected rank of a player whose
performance is ğ‘. Similar to the performance computations in Code-
forces and TopCoder, ğ‘ƒğ‘– can thus be viewed as the performance
level at which oneâ€™s expected rank would equal ğ‘–â€™s actual rank.

3.2 Belief update
Having estimated ğ‘ƒğ‘–,ğ‘¡ in the first phase, the second phase is rather
simple. Ignoring normalizing constants, Equation (3) tells us that the
pdf of the skill posterior can be obtained as the pointwise product
of the pdfs of the skill prior and the performance model. When both
factors are differentiable and log-concave, then so is their product.
Its maximum is the new rating ğœ‡ğ‘–,ğ‘¡ ; letâ€™s see how to compute it for
the same two specializations of our model.

Gaussian skill prior and performance model. When the skill prior
and performance model are Gaussian with known means and vari-
ances, multiplying their pdfs yields another known Gaussian. Hence,
the posterior is compactly represented by its mean ğœ‡ğ‘–,ğ‘¡ , which coin-
cides with the MAP and rating; and its variance ğœ2
ğ‘–,ğ‘¡ , which is our
uncertainty regarding the playerâ€™s skill.

Logistic performance model. When the performance model is
non-Gaussian, the multiplication does not simplify so easily. By
Equation (3), each round contributes an additional factor to the
belief distribution. In general, we allow it to consist of a collection
of simple log-concave factors, one for each round in which player ğ‘–
has participated. Denote the participation history by

Hğ‘–,ğ‘¡ := {ğ‘˜ âˆˆ {1, . . . , ğ‘¡ } : ğ‘– âˆˆ Pğ‘˜ }.

2Elo-MMR, too, can be modified to split ties into half win plus half loss. Itâ€™s easy to
check that Lemma 3.1 still holds if ğ‘‘ ğ‘— (ğ‘) is replaced by ğ‘¤ğ‘™ ğ‘™ ğ‘— (ğ‘) + ğ‘¤ğ‘£ ğ‘£ğ‘— (ğ‘) for some
ğ‘¤ğ‘™ , ğ‘¤ğ‘£ âˆˆ [0, 1] with |ğ‘¤ğ‘™ âˆ’ ğ‘¤ğ‘£ | < 1. In particular, we can set ğ‘¤ğ‘™ = ğ‘¤ğ‘£ = 0.5. The
results in Section 5 wonâ€™t be altered by this change.

L2LR-4-22424681012NormalLogistic-4-2240.10.20.30.4Since each player can be considered in isolation, weâ€™ll omit the
subscript ğ‘–. Specializing to the logistic setting, each ğ‘˜ âˆˆ Hğ‘¡ con-
tributes a logistic factor to the posterior, with mean ğ‘ğ‘˜ and variance
ğ›½2
. We still use a Gaussian initial prior, with mean and variance
ğ‘˜
denoted by ğ‘0 and ğ›½2
0, respectively. Postponing the discussion of
skill evolution to Section 4, for the moment we assume that ğ‘†ğ‘˜ = ğ‘†0
for all ğ‘˜. The posterior pdf, up to normalization, is then

(cid:214)

ğœ‹0 (ğ‘ )

Pr(ğ‘ƒğ‘˜ = ğ‘ğ‘˜ | ğ‘†ğ‘˜ = ğ‘ )

ğ‘˜ âˆˆHğ‘¡
(cid:32)

âˆ exp

âˆ’

(ğ‘  âˆ’ ğ‘0)2
2ğ›½2
0

(cid:33)

(cid:214)

ğ‘˜ âˆˆHğ‘¡

sech2 (cid:18) ğœ‹
âˆš
12

ğ‘  âˆ’ ğ‘ğ‘˜
ğ›½ğ‘˜

(cid:19)

.

(5)

Maximizing the posterior density amounts to minimizing its

negative logarithm. Up to a constant offset, this is given by
(cid:18) ğ‘  âˆ’ ğ‘0
ğ›½0

(cid:18) ğ‘  âˆ’ ğ‘ğ‘˜
ğ›½ğ‘˜

ğ¿(ğ‘ ) := ğ¿2

âˆ‘ï¸

ğ¿ğ‘…

+

(cid:19)

(cid:19)

,

ğ‘˜ âˆˆHğ‘¡

where ğ¿2 (ğ‘¥) :=

1
2

ğ‘¥ 2 and ğ¿ğ‘… (ğ‘¥) := 2 ln

(cid:18)

cosh

(cid:19)

ğœ‹ğ‘¥
âˆš

12

Thus, ğ¿â€²(ğ‘ ) =

ğ‘  âˆ’ ğ‘0
ğ›½2
0

âˆ‘ï¸

+

ğ‘˜ âˆˆHğ‘¡

ğœ‹
âˆš
ğ›½ğ‘˜

3

tanh

(ğ‘  âˆ’ ğ‘ğ‘˜ )ğœ‹
âˆš
12
ğ›½ğ‘˜

.

.

(6)

ğ¿â€² is continuous and strictly increasing in ğ‘ , so its zero is unique:
it is the MAP ğœ‡ğ‘¡ . Similar to what we did in the first phase, we can
solve for ğœ‡ğ‘¡ with either binary search or Newtonâ€™s method.

We pause to make an important observation. From Equation (6),
the rating carries a rather intuitive interpretation: Gaussian factors
in ğ¿ become ğ¿2 penalty terms, whereas logistic factors take on a
more interesting form as ğ¿ğ‘… terms. From Figure 1, we see that the
ğ¿ğ‘… term behaves quadratically near the origin, but linearly at the
extremities, effectively interpolating between ğ¿2 and ğ¿1 over a scale
of magnitude ğ›½ğ‘˜

It is well-known that minimizing a sum of ğ¿2 terms pushes the
argument towards a weighted mean, while minimizing a sum of
ğ¿1 terms pushes the argument towards a weighted median. With
ğ¿ğ‘… terms, the net effect is that ğœ‡ğ‘¡ acts like a robust average of the
historical performances ğ‘ğ‘˜ . Specifically, one can check that

ğœ‡ğ‘¡ =

(cid:205)ğ‘˜ ğ‘¤ğ‘˜ğ‘ğ‘˜
(cid:205)ğ‘˜ ğ‘¤ğ‘˜

ğ‘¤ğ‘˜ :=

ğœ‹
(ğœ‡ğ‘¡ âˆ’ ğ‘ğ‘˜ )ğ›½ğ‘˜

âˆš
3

tanh

, where ğ‘¤0 :=

1
ğ›½2
0
(ğœ‡ğ‘¡ âˆ’ ğ‘ğ‘˜ )ğœ‹
âˆš

ğ›½ğ‘˜

12

and

for ğ‘˜ âˆˆ Hğ‘¡ .

(7)

ğ‘¤ğ‘˜ is close to 1/ğ›½2
ğ‘˜

for typical performances, but can be up to
ğœ‹ 2/6 times more as |ğœ‡ğ‘¡ âˆ’ ğ‘ğ‘˜ | â†’ 0, or vanish as |ğœ‡ğ‘¡ âˆ’ ğ‘ğ‘˜ | â†’ âˆ.
This feature is due to the thicker tails of the logistic distribution,
as compared to the Gaussian, resulting in an algorithm that resists
drastic rating changes in the presence of a few unusually good or
bad performances. Weâ€™ll formally state this robustness property in
Theorem 5.7.

Estimating skill uncertainty. While there is no easy way to com-
pute the variance of a posterior in the form of Equation (5), it will
be useful to have some estimate ğœ2
ğ‘¡ of uncertainty. There is a simple
formula in the case where all factors are Gaussian. Since moment-
matched logistic and normal distributions are relatively close (c.f.

5

Figure 1), we apply the same formula:

1
ğœ2
ğ‘¡

:=

âˆ‘ï¸

ğ‘˜ âˆˆ {0}âˆªHğ‘¡

.

1
ğ›½2
ğ‘˜

(8)

4 SKILL EVOLUTION OVER TIME
Factors such as training and resting will change a playerâ€™s skill
over time. If we model skill as a static variable, our system will
eventually grow so confident in its estimate that it will refuse to
admit substantial changes. To remedy this, we introduce a skill
evolution model, so that in general ğ‘†ğ‘¡ â‰  ğ‘†ğ‘¡ â€² for ğ‘¡ â‰  ğ‘¡ â€². Now rather
than simply being equal to the previous roundâ€™s posterior, the skill
prior at round ğ‘¡ is given by

âˆ«

ğœ‹ğ‘¡ (ğ‘ ) =

Pr(ğ‘†ğ‘¡ = ğ‘  | ğ‘†ğ‘¡ âˆ’1 = ğ‘¥) Pr(ğ‘†ğ‘¡ âˆ’1 = ğ‘¥ | ğ‘ƒ<ğ‘¡ ) dğ‘¥ .

(9)

The factors in the integrand are the skill evolution model and the
previous roundâ€™s posterior, respectively. Following other Bayesian
rating systems (e.g., Glicko, Glicko-2, and TrueSkill [17, 18, 20]),
we model the skill diffusions ğ‘†ğ‘¡ âˆ’ ğ‘†ğ‘¡ âˆ’1 as independent zero-mean
Gaussians. That is, Pr(ğ‘†ğ‘¡ | ğ‘†ğ‘¡ âˆ’1 = ğ‘¥) is a Gaussian with mean ğ‘¥ and
some variance ğ›¾ 2
ğ‘¡ proportionally to the
time elapsed since the last update, corresponding to a continuous
Brownian motion. Codeforces and TopCoder simply set ğ›¾ğ‘¡ to a con-
stant when a player participates, and zero otherwise, corresponding
to changes that are in proportion to how often the player competes.
Now we are ready to complete the two specializations of our rating
system.

ğ‘¡ . The Glicko system sets ğ›¾ 2

Gaussian skill prior and performance model. If both the prior
and performance distributions at round ğ‘¡ âˆ’ 1 are Gaussian, then
the posterior is also Gaussian. Adding an independent Gaussian
diffusion to our posterior on ğ‘†ğ‘¡ âˆ’1 yields a Gaussian prior on ğ‘†ğ‘¡ . By
induction, the skill belief distribution forever remains Gaussian.
This Gaussian specialization of the Elo-MMR framework lacks the
R for robustness (see Theorem 5.7), so we call it Elo-MMğœ’.

Logistic performance model. After a playerâ€™s first contest round,
the posterior in Equation (5) becomes non-Gaussian, rendering the
integral in Equation (9) intractable.

A very simple approach would be to replace the full posterior in
Equation (5) by a Gaussian approximation with mean ğœ‡ğ‘¡ (equal to
the posterior MAP) and variance ğœ2
ğ‘¡ (given by Equation (8)). As in
the previous case, applying diffusions in the Gaussian setting is a
simple matter of adding means and variances.

With this approximation, no memory is kept of the individual
performances ğ‘ƒğ‘¡ . Priors are simply Gaussian, while posterior den-
sities are the product of two factors: the Gaussian prior, and a
logistic factor corresponding to the latest performance. To ensure
robustness (see Section 5.2), ğœ‡ğ‘¡ is computed as the argmax of this
posterior before replacement by its Gaussian approximation. We
call the rating system that takes this approach Elo-MMR(âˆ).

As the name implies, it turns out to be a special case of Elo-
MMR(ğœŒ). In the general setting with ğœŒ âˆˆ [0, âˆ), we keep the full
posterior from Equation (5). Since we cannot tractably compute the
effect of a Gaussian diffusion, we seek a heuristic derivation of the
next roundâ€™s prior, retaining a form similar to Equation (5) while
satisfying many of the same properties as the intended diffusion.

4.1 Desirable properties of a â€œpseudodiffusionâ€
We begin by listing some properties that our skill evolution algo-
rithm, henceforth called a â€œpseudodiffusionâ€, should satisfy. The
first two properties are natural:

â€¢ Aligned incentives. First and foremost, the pseudodiffusion must
not break the aligned incentives property of our rating system.
That is, a rating-maximizing player should never be motivated
to lose on purpose (Theorem 5.5).

â€¢ Rating preservation. The pseudodiffusion must not alter the arg max
of the belief density. That is, the rating of a player should not
change: ğœ‡ğœ‹

ğ‘¡ = ğœ‡ğ‘¡ âˆ’1.

In addition, we borrow four properties of Gaussian diffusions:
â€¢ Correct magnitude. Pseudodiffusion with parameter ğ›¾ 2 must in-
crease the skill uncertainty, as measured by Equation (8), by ğ›¾ 2.
â€¢ Composability. Two pseudodiffusions applied in sequence, first
2 , must have the same effect
1 + ğ›¾ 2
2 .

with parameter ğ›¾ 2
as a single pseudodiffusion with parameter ğ›¾ 2

1 and then with ğ›¾ 2

â€¢ Zero diffusion. In the limit as ğ›¾ â†’ 0, the effect of pseudodiffusion

must vanish, i.e., not alter the belief distribution.

â€¢ Zero uncertainty. In the limit as ğœğ‘¡ âˆ’1 â†’ 0 (i.e., when the previous
rating ğœ‡ğ‘¡ âˆ’1 is a perfect estimate of ğ‘†ğ‘¡ âˆ’1), our belief on ğ‘†ğ‘¡ must
become Gaussian with mean ğœ‡ğ‘¡ âˆ’1 and variance ğ›¾ 2. Finer-grained
information regarding the prior history ğ‘ƒ â‰¤ğ‘¡ must be erased.
In particular, Elo-MMR(âˆ) fails the zero diffusion property because
it simplifies the belief distribution, even when ğ›¾ = 0. In the proof of
Theorem 4.1, weâ€™ll see that Elo-MMR(0) fails the zero uncertainty
property. Thus, it is in fact necessary to have ğœŒ strictly positive and
finite. In Section 5.2, weâ€™ll come to interpret ğœŒ as a kind of inverse
momentum.

4.2 A heuristic pseudodiffusion algorithm
Each factor in the posterior (see Equation (5)) has a parameter ğ›½ğ‘˜ .
Define a factorâ€™s weight to be ğ‘¤ğ‘˜ := 1/ğ›½2
, which by Equation (8)
ğ‘˜
contributes to the total weight (cid:205)ğ‘˜ ğ‘¤ğ‘˜ = 1/ğœ2
ğ‘¡ . Here, unlike in
Equation (7), ğ‘¤ğ‘˜ does not depend on |ğœ‡ğ‘¡ âˆ’ ğ‘ğ‘˜ |.

The approximation step of Elo-MMR(âˆ) replaces all the logistic
factors by a single Gaussian whose variance is chosen to ensure
that the total weight is preserved. In addition, its mean is chosen
to preserve the playerâ€™s rating, given by the unique zero of Equa-
tion (6). Finally, the diffusion step of Elo-MMR(âˆ) increases the
Gaussianâ€™s variance, and hence the playerâ€™s skill uncertainty, by ğ›¾ 2
ğ‘¡ ;
this corresponds to a decay in the weight.

To generalize the idea, we interleave the two steps in a contin-
uous manner. The approximation step becomes a transfer step:
rather than replace the logistic factors outright, we take away the
same fraction from each of their weights, and place the sum of
removed weights onto a new Gaussian factor. The diffusion step
becomes a decay step, reducing each factorâ€™s weight by the same
fraction, chosen such that the overall uncertainty is increased by
ğ›¾ 2
ğ‘¡ .

To make the idea precise, we generalize the posterior from Equa-
tion (5) with fractional multiplicities ğœ”ğ‘˜ , initially set to 1 for each
ğ‘˜ âˆˆ {0} âˆª Hğ‘¡ . The ğ‘˜â€™th factor is raised to the power ğœ”ğ‘˜ ; in Equa-
tions (6) and (8), the corresponding term is multiplied by ğœ”ğ‘˜ . In

Algorithm 1 Elo-MMR(ğœŒ, ğ›½, ğ›¾)

for all rounds ğ‘¡ do

for all players ğ‘– âˆˆ Pğ‘¡ in parallel do

if ğ‘– has never competed before then
ğœ‡ğ‘–, ğœğ‘– â† ğœ‡ğ‘›ğ‘’ğ‘¤ğ‘ğ‘œğ‘šğ‘’ğ‘Ÿ , ğœğ‘›ğ‘’ğ‘¤ğ‘ğ‘œğ‘šğ‘’ğ‘Ÿ
ğ‘ğ‘–, ğ‘¤ğ‘– â† [ğœ‡ğ‘– ], [1/ğœ2
ğ‘– ]

diffuse(ğ‘–, ğ›¾, ğœŒ)
âˆšï¸ƒ
ğœ‡ğœ‹
ğ‘– , ğ›¿ğ‘– â† ğœ‡ğ‘–,

ğœ2
ğ‘– + ğ›½2

for all ğ‘– âˆˆ Pğ‘¡ in parallel do

update(ğ‘–, ğ¸ğ‘¡ , ğ›½)
Algorithm 2 diffuse(ğ‘–, ğ›¾, ğœŒ)

ğœ… â† (1 + ğ›¾ 2/ğœ2
ğ‘– )âˆ’1
ğ‘¤ğº, ğ‘¤ğ¿ â† ğœ…ğœŒğ‘¤ğ‘–,0, (1 âˆ’ ğœ…ğœŒ ) (cid:205)ğ‘˜ ğ‘¤ğ‘–,ğ‘˜
ğ‘ğ‘–,0 â† (ğ‘¤ğº ğ‘ğ‘–,0 + ğ‘¤ğ¿ğœ‡ğ‘– )/(ğ‘¤ğº + ğ‘¤ğ¿)
ğ‘¤ğ‘–,0 â† ğœ… (ğ‘¤ğº + ğ‘¤ğ¿)
for all ğ‘˜ â‰  0 do

ğ‘¤ğ‘–,ğ‘˜ â† ğœ…1+ğœŒğ‘¤ğ‘–,ğ‘˜
âˆš
ğœ…

ğœğ‘– â† ğœğ‘– /

Algorithm 3 update(ğ‘–, ğ¸, ğ›½)
(cid:18)

(cid:18)

ğ‘ â† zero

(cid:205)ğ‘— âª¯ğ‘–

1
ğ›¿ ğ‘—

tanh

ğ‘¥âˆ’ğœ‡ğœ‹
ğ‘—
2 Â¯ğ›¿ ğ‘—

(cid:19)

âˆ’ 1

+ (cid:205)ğ‘— âª°ğ‘–

1
ğ›¿ ğ‘—

(cid:18)

tanh

ğ‘¥âˆ’ğœ‡ğœ‹
ğ‘—
2 Â¯ğ›¿ ğ‘—

(cid:19)(cid:19)

+ 1

ğ‘ğ‘– .push(ğ‘)
ğ‘¤ğ‘– .push(1/ğ›½2)
ğœ‡ğ‘– â† zero (cid:16)

ğ‘¤ğ‘–,0 (ğ‘¥ âˆ’ ğ‘ğ‘–,0) + (cid:205)ğ‘˜â‰ 0

ğ‘¤ğ‘–,ğ‘˜ ğ›½ 2
Â¯ğ›½

tanh ğ‘¥âˆ’ğ‘ğ‘–,ğ‘˜
2 Â¯ğ›½

(cid:17)

other words, the latter equation is replaced by

1
ğœ2
ğ‘¡

:=

âˆ‘ï¸

ğ‘˜ âˆˆ {0}âˆªHğ‘¡

ğ‘¤ğ‘˜, where ğ‘¤ğ‘˜ :=

.

ğœ”ğ‘˜
ğ›½2
ğ‘˜

For ğœŒ âˆˆ [0, âˆ], the Elo-MMR(ğœŒ) algorithm continuously and
simultaneously performs transfer and decay, with transfer proceed-
ing at ğœŒ times the rate of decay. Holding ğ›½ğ‘˜ fixed, changes to ğœ”ğ‘˜
can be described in terms of changes to ğ‘¤ğ‘˜ :

(cid:164)ğ‘¤0 = âˆ’ğ‘Ÿ (ğ‘¡)ğ‘¤0 + ğœŒğ‘Ÿ (ğ‘¡)

âˆ‘ï¸

ğ‘¤ğ‘˜,

ğ‘˜ âˆˆHğ‘¡

(cid:164)ğ‘¤ğ‘˜ = âˆ’(1 + ğœŒ)ğ‘Ÿ (ğ‘¡)ğ‘¤ğ‘˜

for ğ‘˜ âˆˆ Hğ‘¡ ,

where the arbitrary decay rate ğ‘Ÿ (ğ‘¡) can be eliminated by a change
of variable dğœ = ğ‘Ÿ (ğ‘¡)dğ‘¡. After some time Î”ğœ, the total weight will
have decayed by a factor ğœ… := ğ‘’âˆ’Î”ğœ , resulting in the new weights:

ğ‘¤ğ‘›ğ‘’ğ‘¤
0

= ğœ…ğ‘¤0 +

ğ‘¤ğ‘›ğ‘’ğ‘¤
ğ‘˜

= ğœ…1+ğœŒğ‘¤ğ‘˜

ğœ… âˆ’ ğœ…1+ğœŒ (cid:17) âˆ‘ï¸
(cid:16)
ğ‘˜ âˆˆHğ‘¡
for ğ‘˜ âˆˆ Hğ‘¡ .

ğ‘¤ğ‘˜,

In order for the uncertainty to increase from ğœ2
must solve ğœ…/ğœ2

ğ‘¡ âˆ’1 to ğœ2
ğ‘¡ ) for the decay factor:

ğ‘¡ âˆ’1 = 1/(ğœ2

ğ‘¡ âˆ’1 + ğ›¾ 2

ğ‘¡ âˆ’1 + ğ›¾ 2

ğ‘¡ , we

(cid:32)

ğœ… =

1 +

(cid:33)âˆ’1

.

ğ›¾ 2
ğ‘¡
ğœ2
ğ‘¡ âˆ’1

In order for this operation to preserve ratings, the transferred
weight must be centered at ğœ‡ğ‘¡ âˆ’1; see Algorithm 2 for details.

6

Algorithm 1 details the full Elo-MMR(ğœŒ) rating system. The main
loop runs whenever a round of competition takes place. First, new
players are initialized with a Gaussian prior. Then, changes in player
skill are modeled by Algorithm 2. Given the round rankings ğ¸ğ‘¡ , the
first phase of Algorithm 3 solves an equation to estimate ğ‘ƒğ‘¡ . Finally,
the second phase solves another equation for the rating ğœ‡ğ‘¡ .

The hyperparameters ğœŒ, ğ›½, ğ›¾ are domain-dependent, and can be
set by standard hyperparameter search techniques. For convenience,
we assume ğ›½ and ğ›¾ are fixed and use the shorthand Â¯ğ›½ğ‘˜ :=

âˆš
3
ğœ‹ ğ›½ğ‘˜ .

Theorem 4.1. Algorithm 2 with ğœŒ âˆˆ (0, âˆ) meets all of the prop-

erties listed in Section 4.1.

Proof. We go through each of the six properties in order.
â€¢ Aligned incentives. This property will be stated in Theorem 5.5.
To ensure that its proof carries through, the relevant facts to
note here are that the pseudodiffusion algorithm ignores the
performances ğ‘ğ‘˜ , and centers the transferred Gaussian weight at
the rating ğœ‡ğ‘¡ âˆ’1, which is trivially monotonic in ğœ‡ğ‘¡ âˆ’1.

â€¢ Rating preservation. Recall that the rating is the unique zero of ğ¿â€²,
defined in Equation (6). To see that this zero is preserved, note
that the decay and transfer operations multiply ğ¿â€² by constants
(ğœ… and ğœ…ğœŒ , respectively), before adding the new Gaussian term,
whose contribution to ğ¿â€² is zero at its center.

â€¢ Correct magnitude. Follows from our derivation for ğœ….
â€¢ Composability. Follows from correct magnitude and the fact that
every pseudodiffusion follows the same differential equations.
â€¢ Zero diffusion. As ğ›¾ â†’ 0, ğœ… â†’ 1. Provided that ğœŒ < âˆ, we also

have ğœ…ğœŒ â†’ 1. Hence, for all ğ‘˜ âˆˆ {0} âˆª Hğ‘¡ , ğ‘¤ğ‘›ğ‘’ğ‘¤

ğ‘˜ â†’ ğ‘¤ğ‘˜ .

â€¢ Zero uncertainty. As ğœğ‘¡ âˆ’1 â†’ 0, ğœ… â†’ 0. The total weight decays
ğ‘¡ âˆ’1 to ğ›¾ 2. Provided that ğœŒ > 0, we also have ğœ…ğœŒ â†’ 0, so
from 1/ğœ2
these weights transfer in their entirety, leaving behind a Gaussian
with mean ğœ‡ğ‘¡ âˆ’1, variance ğ›¾ 2, and no additional history.
â–¡

5 THEORETICAL PROPERTIES
In this section, we see how the simplicity of the Elo-MMR formu-
las enables us to rigorously prove that the rating system aligns
incentives, is robust, and is computationally efficient.

5.1 Aligned incentives
To demonstrate the need for aligned incentives, letâ€™s look at the con-
sequences of violating this property in the TopCoder and Glicko-2
rating systems. These systems track a â€œvolatilityâ€ for each player,
which estimates the variance of their performances. A player whose
recent performance history is more consistent would be assigned a
lower volatility score, than one with wild swings in performance.
The volatility acts as a multiplier on rating changes; thus, play-
ers with an extremely low or high performance will have their
subsequent rating changes amplified.

While it may seem like a good idea to boost changes for players
whose ratings are poor predictors of their performance, this fea-
ture has an exploit. By intentionally performing at a weaker level, a
player can amplify future increases to an extent that more than com-
pensates for the immediate hit to their rating. A player may even
â€œfarmâ€ volatility by alternating between very strong and very weak
performances. After acquiring a sufficiently high volatility score,
the strategic player exerts their honest maximum performance over

7

a series of contests. The amplification eventually results in a rating
that exceeds what would have been obtained via honest play. This
type of exploit was discovered in both TopCoder competitions and
the Pokemon Go video game [3, 14]. For a detailed example, see
Table 5.3 of [14].

Remarkably, Elo-MMR combines the best of both worlds: weâ€™ll
see in Section 5.2 that, for ğœŒ âˆˆ (0, âˆ), Elo-MMR(ğœŒ) also boosts
changes to inconsistent players. And yet, as weâ€™ll prove in this sec-
tion, no such strategic incentive exists in any version of Elo-MMR.
Recall that, for the purposes of the algorithm, the performance ğ‘ğ‘–
is defined to be the unique zero of the function ğ‘„ğ‘– (ğ‘) := (cid:205)ğ‘— â‰»ğ‘– ğ‘™ ğ‘— (ğ‘)+
(cid:205)ğ‘—âˆ¼ğ‘– ğ‘‘ ğ‘— (ğ‘) + (cid:205)ğ‘— â‰ºğ‘– ğ‘£ ğ‘— (ğ‘), whose terms ğ‘™ğ‘–, ğ‘‘ğ‘–, ğ‘£ğ‘– are contributed by
opponents against whom ğ‘– lost, drew, or won, respectively. Wins
(losses) are always positive (negative) contributions to a playerâ€™s
performance score:

Lemma 5.1. Adding a win term to ğ‘„ğ‘– (Â·), or replacing a tie term by
a win term, always increases its zero. Conversely, adding a loss term,
or replacing a tie term by a loss term, always decreases it.

Proof. By Lemma 3.1, ğ‘„ğ‘– (ğ‘) is decreasing in ğ‘. Thus, adding a
positive term will increase its zero whereas adding a negative term
will decrease it. The desired conclusion follows by noting that, for
all ğ‘— and ğ‘, ğ‘£ ğ‘— (ğ‘) and ğ‘£ ğ‘— (ğ‘) âˆ’ ğ‘‘ ğ‘— (ğ‘) are positive, whereas ğ‘™ ğ‘— (ğ‘) and
â–¡
ğ‘™ ğ‘— (ğ‘) âˆ’ ğ‘‘ ğ‘— (ğ‘) are negative.

While not needed for our main result, a similar argument shows
that performance scores are monotonic across the round standings:

Theorem 5.2. If ğ‘– â‰» ğ‘— (that is, player ğ‘– beats ğ‘—) in a given round,

then player ğ‘– and ğ‘—â€™s performance estimates satisfy ğ‘ğ‘– > ğ‘ ğ‘— .

Proof. If ğ‘– â‰» ğ‘— with ğ‘–, ğ‘— adjacent in the rankings, then

ğ‘„ğ‘– (ğ‘) âˆ’ ğ‘„ ğ‘— (ğ‘) =

(ğ‘‘ğ‘˜ (ğ‘) âˆ’ ğ‘™ğ‘˜ (ğ‘)) +

âˆ‘ï¸

ğ‘˜âˆ¼ğ‘–

âˆ‘ï¸

ğ‘˜âˆ¼ğ‘—

(ğ‘£ğ‘˜ (ğ‘) âˆ’ ğ‘‘ğ‘˜ (ğ‘)) > 0.

for all ğ‘. Since ğ‘„ğ‘– and ğ‘„ ğ‘— are decreasing functions, it follows that
ğ‘ğ‘– > ğ‘ ğ‘— . By induction, this result extends to the case where ğ‘–, ğ‘— are
â–¡
not adjacent in the rankings.

What matters for incentives is that performance scores be coun-
terfactually monotonic; meaning, if we were to alter the round
standings, a strategic player will always prefer to place higher:

Lemma 5.3. In any given round, holding fixed the relative ranking
of all players other than ğ‘– (and holding fixed all preceding rounds),
the performance ğ‘ğ‘– is a monotonic function of player iâ€™s prior rating
and of player ğ‘–â€™s rank in this round.

Proof. Monotonicity in the rating follows directly from mono-
tonicity of the self-tie term ğ‘‘ğ‘– in ğ‘„ğ‘– . Since an upward shift in the
rankings can only convert losses to ties to wins, monotonicity in
â–¡
contest rank follows from Lemma 5.1.

Having established the relationship between round rankings
and performance scores, the next step is to prove that, even with
hindsight, players will always prefer their performance scores to
be as high as possible:

Lemma 5.4. Holding fixed the set of contest rounds in which a
player has participated, their current rating is monotonic in each of
their past performance scores.

Proof. The playerâ€™s rating is given by the zero of ğ¿â€² in Equa-
tion (6). The pseudodiffusions of Section 4 modify each of the ğ›½ğ‘˜ in
a manner that does not depend on any of the ğ‘ğ‘˜ , so they are fixed
for our purposes. Hence, ğ¿â€² is monotonically increasing in ğ‘  and
decreasing in each of the ğ‘ğ‘˜ . Therefore, its zero is monotonically
increasing in each of the ğ‘ğ‘˜ .

This is almost what we wanted to prove, except that ğ‘0 is not
a performance. Nonetheless, it is a function of the performances:
specifically, a weighted average of historical ratings which, using
this same lemma as an inductive hypothesis, are themselves mono-
tonic in past performances. By induction, the proof is complete. â–¡

Finally, we conclude that the playerâ€™s incentives are aligned with

optimizing round rankings, or raw scores:

Theorem 5.5 (Aligned Incentives). Holding fixed the set of
contest rounds in which each player has participated, and the historical
ratings and relative rankings of all players other than ğ‘–, player ğ‘–â€™s
current rating is monotonic in each of their past rankings.

Proof. Choose any contest round in player ğ‘–â€™s history, and con-
sider improving player ğ‘–â€™s rank in that round while holding every-
thing else fixed. It suffices to show that player ğ‘–â€™s current rating
would necessarily increase as a result.

In the altered round, by Lemma 5.3, ğ‘ğ‘– is increased; and by
Lemma 5.4, player ğ‘–â€™s post-round rating is increased. By Lemma 5.3
again, this increases player ğ‘–â€™s performance score in the following
round. Proceeding inductively, we find that performance scores and
â–¡
ratings from this point onward are all increased.

In the special cases of Elo-MMğœ’ or Elo-MMR(âˆ), the rating sys-
tem is â€œmemorylessâ€: the only data retained for each player are the
current rating ğœ‡ğ‘–,ğ‘¡ and uncertainty ğœğ‘–,ğ‘¡ ; detailed performance his-
tory is not saved. In this setting, we present a natural monotonicity
theorem. A similar theorem was stated for the Codeforces system
in [2], but no proofs were given.

Theorem 5.6 (Memoryless Monotonicity Theorem). In ei-
ther the Elo-MMğœ’ or Elo-MMR(âˆ) system, suppose ğ‘– and ğ‘— are two
participants of round ğ‘¡. Suppose that the ratings and corresponding un-
certainties satisfy ğœ‡ğ‘–,ğ‘¡ âˆ’1 â‰¥ ğœ‡ ğ‘—,ğ‘¡ âˆ’1, ğœğ‘–,ğ‘¡ âˆ’1 = ğœ ğ‘—,ğ‘¡ âˆ’1. Then, ğœğ‘–,ğ‘¡ = ğœ ğ‘—,ğ‘¡ .
Furthermore, if ğ‘– â‰» ğ‘— in round ğ‘¡, then ğœ‡ğ‘–,ğ‘¡ > ğœ‡ ğ‘—,ğ‘¡ . On the other hand,
if ğ‘— â‰» ğ‘– in round ğ‘¡, then ğœ‡ ğ‘—,ğ‘¡ âˆ’ ğœ‡ ğ‘—,ğ‘¡ âˆ’1 > ğœ‡ğ‘–,ğ‘¡ âˆ’ ğœ‡ğ‘–,ğ‘¡ âˆ’1.

Proof. The new contest round will add a rating perturbation
ğ‘¡ , followed by a new performance with variance ğ›½2
ğ‘¡ .

with variance ğ›¾ 2
As a result,

(cid:32)

ğœğ‘–,ğ‘¡ =

1
ğ‘–,ğ‘¡ âˆ’1 + ğ›¾ 2
ğœ2

ğ‘¡

+

1
ğ›½2
ğ‘¡

(cid:33)âˆ’ 1

2

(cid:32)

=

1
ğ‘—,ğ‘¡ âˆ’1 + ğ›¾ 2
ğœ2

ğ‘¡

+

1
ğ›½2
ğ‘¡

(cid:33)âˆ’ 1

2

= ğœ ğ‘—,ğ‘¡ .

The remaining conclusions are consequences of three properties:
memorylessness, aligned incentives (Theorem 5.5), and translation-
invariance (ratings, skills, and performances are quantified on a
common interval scale relative to one another).

Since the Elo-MMğœ’ or Elo-MMR(âˆ) systems are memoryless, we
may replace the initial prior and performance histories of players
with any alternate histories of our choosing, as long as our choice is
compatible with their current rating and uncertainty. For example,
both ğ‘– and ğ‘— can be considered to have participated in the same

set of rounds, with ğ‘– always performing at ğœ‡ğ‘–,ğ‘¡ âˆ’1. and ğ‘— always
performing at ğœ‡ ğ‘—,ğ‘¡ âˆ’1. Round ğ‘¡ is unchanged.

Suppose ğ‘– â‰» ğ‘—. Since ğ‘–â€™s historical performances are all equal or

stronger than ğ‘—â€™s, Theorem 5.5 implies ğœ‡ğ‘–,ğ‘¡ > ğœ‡ ğ‘—,ğ‘¡ .

Suppose ğ‘— â‰» ğ‘–. By translation-invariance, if we shift each of
ğ‘—â€™s performances, up to round ğ‘¡ and including the initial prior,
upward by ğœ‡ğ‘–,ğ‘¡ âˆ’1 âˆ’ ğœ‡ ğ‘—,ğ‘¡ âˆ’1, the rating changes between rounds will
be unaffected. Players ğ‘– and ğ‘— now have identical histories, except
that we still have ğ‘— â‰» ğ‘– at round ğ‘¡. Therefore, ğœ‡ ğ‘—,ğ‘¡ âˆ’1 = ğœ‡ğ‘–,ğ‘¡ âˆ’1 and,
by Theorem 5.5, ğœ‡ ğ‘—,ğ‘¡ > ğœ‡ğ‘–,ğ‘¡ . Subtracting the equation from the
â–¡
inequality proves the second conclusion.

5.2 Robust response
Another desirable property in many settings is robustness: a playerâ€™s
rating should not change too much in response to any one con-
test, no matter how extreme their performance. The Codeforces
and TrueSkill systems lack this property, allowing for unbounded
rating changes. TopCoder achieves robustness by clamping any
changes that exceed a cap, which is initially high for new players
but decreases with experience.

When ğœŒ > 0, Elo-MMR(ğœŒ) achieves robustness in a natural,
smoother manner. It comes out of the interplay between Gaussian
and logistic factors in the posterior; ğœŒ > 0 ensures that the Gaussian
contribution doesnâ€™t vanish. Recall the notation used to describe
the general posterior in Equations (5) and (6), enhanced with the
fractional multiplicities ğœ”ğ‘˜ from Section 4.2.

Theorem 5.7. In the Elo-MMR(ğœŒ) rating system, let

Î”+ := lim

ğ‘ğ‘¡ â†’+âˆ

ğœ‡ğ‘¡ âˆ’ ğœ‡ğ‘¡ âˆ’1, Î”âˆ’ := lim

ğ‘ğ‘¡ â†’âˆ’âˆ

ğœ‡ğ‘¡ âˆ’1 âˆ’ ğœ‡ğ‘¡ .

Then,

ğœ‹
âˆš
3
ğ›½ğ‘¡

ğœ‹ 2
6

+

1
ğ›½2
0

âˆ‘ï¸

ğ‘˜ âˆˆHğ‘¡ âˆ’1

ğœ”ğ‘˜
ğ›½2
ğ‘˜

(cid:169)
(cid:173)
(cid:171)

âˆ’1

ğœ‹ ğ›½2
0
âˆš
3

.

â‰¤ Î”Â± â‰¤

(cid:170)
(cid:174)
(cid:172)
tanh(ğ‘¥) â‰¤ 1, differentiating

ğ›½ğ‘¡

Proof. Using the fact that 0 < ğ‘‘
ğ‘‘ğ‘¥

Equation (6) yields

1
ğ›½2
0

â‰¤ ğ¿â€²â€²(ğ‘ ) â‰¤

ğœ‹ 2
6

+

1
ğ›½2
0

âˆ‘ï¸

ğ‘˜ âˆˆHğ‘¡ âˆ’1

.

ğœ”ğ‘˜
ğ›½2
ğ‘˜

For every ğ‘  âˆˆ R, in the limit as ğ‘ğ‘¡ â†’ Â±âˆ, the new term cor-
responding to the performance at round ğ‘¡ will increase ğ¿â€²(ğ‘ ) by
âˆ“ ğœ‹
. Since ğœ‡ğ‘¡ âˆ’1 was a zero of ğ¿â€² without this new term, we now
âˆš
ğ›½ğ‘¡
have ğ¿â€²(ğœ‡ğ‘¡ âˆ’1) â†’ âˆ“ ğœ‹
. Dividing by the former inequalities yields
âˆš
ğ›½ğ‘¡
the desired result.

â–¡

3

3

The proof reveals that the magnitude of Î”Â± depends inversely
on that of ğ¿â€²â€² in the vicinity of the current rating, which in turn
is related to the derivative of the tanh terms. If a playerâ€™s perfor-
mances vary wildly, then most of the tanh terms will be in their tails,
which contribute small derivatives, enabling larger rating changes.
Conversely, the tanh terms of a player with a very consistent rat-
ing history will contribute large derivatives, so the bound on their
rating change will be small.

Thus, Elo-MMR(ğœŒ) naturally caps the rating change of all players,
and puts a smaller cap on the rating change of consistent players.
The cap will increase after an extreme performance, providing a

8

similar â€œmomentumâ€ to the TopCoder and Glicko-2 systems, but
without sacrificing aligned incentives (Theorem 5.5).

By comparing against Equation (8), we see that the lower bound
in Theorem 5.7 is on the order of ğœ2
ğ‘¡ /ğ›½ğ‘¡ , while the upper bound is
on the order of ğ›½2
0/ğ›½ğ‘¡ . As a result, the momentum effect is more
pronounced when ğ›½0 is much larger than ğœğ‘¡ . Since the decay step
increases ğ›½0 while the transfer step decreases it, this occurs when
the transfer rate ğœŒ is comparatively small. Thus, ğœŒ can be chosen in
inverse proportion to the desired strength of momentum.

5.3 Runtime analysis and optimizations
Letâ€™s look at the computation time needed to process a round with
participant set P, where we again omit the round subscript. Each
player ğ‘– has a participation history Hğ‘– .

Estimating ğ‘ƒğ‘– entails finding the zero of a monotonic function
with ğ‘‚ (|P |) terms, and then obtaining the rating ğœ‡ğ‘– entails finding
the zero of another monotonic function with ğ‘‚ (|Hğ‘– |) terms. Using
the Illinois or Newton methods, solving these equations to precision
ğœ– takes ğ‘‚ (log log 1
ğœ– ) iterations. As a result, the total runtime needed
to process one round of competition is

ğ‘‚

(cid:32)

âˆ‘ï¸

ğ‘– âˆˆ P

(|P | + |Hğ‘– |) log log

(cid:33)

.

1
ğœ–

This complexity is more than adequate for Codeforces-style com-
petitions with thousands of contestants and history lengths up to a
few hundred. Indeed, we were able to process the entire history of
Codeforces on a small laptop in less than half an hour. Nonetheless,
it may be cost-prohibitive in truly massive settings, where |P | or
|Hğ‘– | number in the millions. Fortunately, it turns out that both
functions may be compressed down to a bounded number of terms,
with negligible loss of precision.

Adaptive subsampling. In Section 2, we used Doobâ€™s consistency
theorem to argue that our estimate for ğ‘ƒğ‘– is consistent. Specifically,
we saw that ğ‘‚ (1/ğœ–2) opponents are needed to get the typical error
below ğœ–. Thus, we can subsample the set of opponents to include in
the estimation, omitting the rest. Random sampling is one approach.
A more efficient approach chooses a fixed number of opponents
whose ratings are closest to that of player ğ‘–, as these are more likely
to provide informative match-ups. On the other hand, if the setting
requires aligned incentives to hold exactly, then one must avoid
choosing different opponents for each player.

History compression. Similarly, itâ€™s possible to bound the number
of stored factors in the posterior. Our skill-evolution algorithm
decays the weights of old performances at an exponential rate.
Thus, the contributions of all but the most recent ğ‘‚ (log 1
ğœ– ) terms
are negligible. Rather than erase the older logistic terms outright, we
recommend replacing them with moment-matched Gaussian terms,
similar to the transfers in Section 4 with ğœ… = 0. Since Gaussians
compose easily, a single term can then summarize an arbitrarily
long prefix of the history.

Substituting 1/ğœ–2 and log 1

ğœ– for |P | and |Hğ‘– |, respectively, the

runtime of Elo-MMR with both optimizations becomes

ğ‘‚

(cid:18) |P |
ğœ–2

log log

(cid:19)

.

1
ğœ–

9

Dataset
Codeforces
TopCoder
Reddit
Synthetic

# contests
1087
2023
1000
50

avg. # participants / contest
2999
403
20
2500

Table 1: Summary of test datasets.

Finally, we note that the algorithm is embarrassingly parallel,
with each player able to solve its equations independently. The
threads can read the same global data structures, so each additional
thread only contributes ğ‘‚ (1) memory overhead.

6 EXPERIMENTS
In this section, we compare various rating systems on real-world
datasets, mined from several sources that will be described in Sec-
tion 6.1. The metrics are runtime and predictive accuracy, as de-
scribed in Section 6.2.

We compare Elo-MMğœ’ and Elo-MMR(ğœŒ) against the industry-
tested rating systems of Codeforces and TopCoder. For a fairer
comparison, we hand-coded efficient versions of all four algorithms
in the safe subset of Rust, parellelized using the Rayon crate; as
such, the Rust compiler verifies that they contain no data races [27].
Our implementation of Elo-MMR(ğœŒ) makes use of the optimizations
in Section 5.3, bounding both the number of sampled opponents
and the history length by 500. In addition, we test the improved
TrueSkill algorithm of [25], basing our code on an open-source
implementation of the same algorithm. The inherent seqentiality of
its message-passing procedure prevented us from parallelizing it.

Hyperparameter search. To ensure fair comparisons, we ran a
separate grid search for each triple of algorithm, dataset, and metric,
over all of the algorithmâ€™s hyperparameters. The hyperparameter
set that performed best on the first 10% of the dataset, was then
used to test the algorithm on the remaining 90% of the dataset.

The experiments were run on a 2.0 GHz 24-core Skylake ma-
chine with 24 GB of memory. Implementations of all rating systems,
hyperparameters, datasets, and additional processing used in our
experiments can be found at https://github.com/EbTech/EloR/.

6.1 Datasets
Due to the scarcity of public domain datasets for rating systems,
we mined three datasets to analyze the effectiveness of our system.
The datasets were mined using data from each source websiteâ€™s
inception up to October 12th, 2020. We also created a synthetic
dataset to test our systemâ€™s performance when the data generating
process matches our theoretical model. Summary statistics of the
datasets are presented in Table 1.

Codeforces contest history. This dataset contains the current en-
tire history of rated contests ever run on CodeForces.com, the dom-
inant platform for online programming competitions. The Code-
Forces platform has over 850K users, over 300K of whom are rated,
and has hosted over 1000 contests to date. Each contest has a couple
thousand competitors on average. A typical contest takes 2 to 3
hours and contains 5 to 8 problems. Players are ranked by total
points, with more points typically awarded for tougher problems
and for early solves. They may also attempt to â€œhackâ€ one anotherâ€™s

submissions for bonus points, identifying test cases that break their
solutions.

TopCoder contest history. This dataset contains the current en-
tire history of algorithm contests ever run on the TopCoder.com.
TopCoder is a predecessor to Codeforces, with over 1.4 million
total users and a long history as a pioneering platform for program-
ming contests. It hosts a variety of contest types, including over
2000 algorithm contests to date. The scoring system is similar to
Codeforces, but its rounds are shorter: typically 75 minutes with 3
problems.

SubRedditSimulator threads. This dataset contains data scraped
from the current top-1000 most upvoted threads on the website
reddit.com/r/SubredditSimulator/. Reddit is a social news aggrega-
tion website with over 300 million users. The site itself is broken
down into sub-sites called subreddits. Users then post and comment
to the subreddits, where the posts and comments receive votes from
other users. In the subreddit SubredditSimulator, users are language
generation bots trained on text from other subreddits. Automated
posts are made by these bots to SubredditSimulator every 3 minutes,
and real users of Reddit vote on the best bot. Each post (and its asso-
ciated comments) can thus be interpreted as a round of competition
between the bots who commented.

Synthetic data. This dataset contains 10K players, with skills
and performances generated according to the Gaussian generative
model in Section 2. Playersâ€™ initial skills are drawn i.i.d. with mean
1500 and variance 300. Players compete in all rounds, and are ranked
according to independent performances with variance 200. Between
rounds, we add i.i.d. Gaussian increments with variance 35 to each
of their skills.

6.2 Evaluation metrics
To compare the different algorithms, we define two measures of
predictive accuracy. Each metric will be defined on individual con-
testants in each round, and then averaged:

aggregate(metric) :=

(cid:205)ğ‘¡ (cid:205)ğ‘– âˆˆ Pğ‘¡ metric(ğ‘–, ğ‘¡)
(cid:205)ğ‘¡ |Pğ‘¡ |

.

Pair inversion metric [20]. Our first metric computes the fraction
of opponents against whom our ratings predict the correct pairwise
result, defined as the higher-rated player either winning or tying:

pair_inversion(ğ‘–, ğ‘¡) :=

# correctly predicted matchups
|Pğ‘¡ | âˆ’ 1

Ã— 100%.

This metric was used in the evaluation of TrueSkill [20].

Rank deviation. Our second metric compares the rankings with
the total ordering that would be obtained by sorting players accord-
ing to their prior rating. The penalty is proportional to how much
these ranks differ for player ğ‘–:

rank_deviation(ğ‘–, ğ‘¡) :=

|actual_rank âˆ’ predicted_rank|
|Pğ‘¡ | âˆ’ 1

Ã— 100%.

In the event of ties, among the ranks within the tied range, we use
the one that comes closest to the rating-based prediction.

6.3 Empirical results
Recall that Elo-MMğœ’ has a Gaussian performance model, matching
the modeling assumptions of TopCoder and TrueSkill. Elo-MMR(ğœŒ),
on the other hand, has a logistic performance model, matching
the modeling assumptions of Codeforces and Glicko. While ğœŒ was
included in the hyperparameter search, in practice we found that
all values between 0 and 1 produce very similar results.

To ensure that errors due to the unknown skills of new players
donâ€™t dominate our metrics, we excluded players who had competed
in less than 5 total contests. In most of the datasets, this reduced the
performance of our method relative to the others, as our method
seems to converge more accurately. Despite this, we see in Table 2
that both versions of Elo-MMR outperform the other rating systems
in both the pairwise inversion metric and the ranking deviation
metric.

We highlight a few key observations. First, significant perfor-
mance gains are observed on the Codeforces and TopCoder datasets,
despite these platformsâ€™ rating systems having been designed specif-
ically for their needs. Our gains are smallest on the synthetic dataset,
for which all algorithms perform similarly. This might be in part
due to the close correspondence between the generative process
and the assumptions of these rating systems. Furthermore, the
synthetic players compete in all rounds, enabling the system to
converge to near-optimal ratings for every player. Finally, the im-
proved TrueSkill performed well below our expectations, despite
our best efforts to improve it; we suspect that the message-passing
algorithm breaks down in contests with a large number of distinct
ranks. To our knowledge, we are the first to present experiments
with TrueSkill on contests where the number of distinct ranks is in
the hundreds or thousands. In preliminary experiments, TrueSkill
and Elo-MMR score about equally when the number of ranks is less
than about 60.

Now, we turn our attention to Table 3, which showcases the com-
putational efficiency of Elo-MMR. On smaller datasets, it performs
comparably to the Codeforces and TopCoder algorithms. However,
the latter suffer from a quadratic time dependency on the number
of contestants; as a result, Elo-MMR outperforms them by almost
an order of magnitude on the larger Codeforces dataset.

Finally, in comparisons between the two Elo-MMR variants, we
note that while Elo-MMR(ğœŒ) is more accurate, Elo-MMğœ’ is always
faster. This has to do with the skill drift modeling described in
Section 4, as every update in Elo-MMR(ğœŒ) must process ğ‘‚ (log 1
ğœ– )
terms of a playerâ€™s competition history.

7 CONCLUSIONS
This paper introduces the Elo-MMR rating system, which is in
part a generalization of the two-player Glicko system, allowing an
unbounded number of players. By developing a Bayesian model
and taking the limit as the number of participants goes to infinity,
we obtained simple, human-interpretable rating update formulas.
Furthermore, we saw that the algorithm is asymptotically fast, em-
barrassingly parallel, robust to extreme performances, and satisfies
the important aligned incentives property. To our knowledge, our
system is the first to rigorously prove all these properties in a set-
ting with more than two individually ranked players. In terms of

10

Dataset

Codeforces
TopCoder
Reddit
Synthetic

Codeforces
pair inv.
78.3%
72.6%
61.5%
81.7%

rank dev.
14.9%
18.5%
27.3%
12.9%

TopCoder
pair inv.
78.5%
72.3%
61.4%
81.7%

rank dev.
15.1%
18.7%
27.4%
12.8%

TrueSkill
pair inv.
61.7%
68.7%
61.5%
81.3%

rank dev.
25.4%
20.9%
27.2%
13.1%

Elo-MMğŒ
pair inv.
78.5%
73.0%
61.6%
81.7%

rank dev.
14.8%
18.3%
27.3%
12.8%

Elo-MMR(ğ†)
pair inv.
78.6%
73.1%
61.6%
81.7%

rank dev.
14.7%
18.2%
27.3%
12.8%

Table 2: Performance of each rating system on the pairwise inversion and rank deviation metrics. Bolded entries denote the
best performances (highest pair inv. or lowest rank dev.) on each metric and dataset.

Dataset
Codeforces
TopCoder
Reddit
Synthetic

CF
212.9
9.60
1.19
3.26

TC
72.5
4.25
1.14
1.00

TS
67.2
16.8
0.44
2.93

Elo-MMğŒ
31.4
7.00
1.14
0.81

Elo-MMR(ğ†)
35.4
7.52
1.42
0.85

Table 3: Total compute time over entire dataset, in seconds.

practical performance, we saw that it outperforms existing industry
systems in both prediction accuracy and computation speed.

This work can be extended in several directions. First, the choices
we made in modeling ties, pseudodiffusions, and opponent subsam-
pling are by no means the only possibilities consistent with our
Bayesian model of skills and performances. Second, one may obtain
better results by fitting the performance and skill evolution models
to application-specific data.

Another useful extension would be to team competitions. While
itâ€™s no longer straightforward to infer precise estimates of an indi-
vidualâ€™s performance, Elo-MMğœ’ can simply be applied at the team
level. To make this useful in settings where players may form new
teams in each round, we must model teams in terms of their individ-
ual members. In the case where a teamâ€™s performance is modeled
as the sum of its membersâ€™ independent Gaussian contributions,
elementary facts about multivariate Gaussian distributions enable
posterior skill inferences at the individual level. Generalizing this
approach remains an open challenge.

Over the past decade, online competition communities such as
Codeforces have grown exponentially. As such, considerable work
has gone into engineering scalable and reliable rating systems.
Unfortunately, many of these systems have not been rigorously
analyzed in the academic community. We hope that our paper and
open-source release will open new explorations in this area.

ACKNOWLEDGEMENTS
The authors are indebted to Daniel Sleator and Danica J. Sutherland
for initial discussions that helped inspire this work, and to Nikita
Gaevoy for the open-source improved TrueSkill upon which our
implementation is based. Experiments in this paper were funded by
a Google Cloud Research Grant. The second author is supported by
a VMWare Fellowship and the Natural Sciences and Engineering
Research Council of Canada.

APPENDIX

Lemma 3.1. If ğ‘“ğ‘– is continuously differentiable and log-concave,

then the functions ğ‘™ğ‘–, ğ‘‘ğ‘–, ğ‘£ğ‘– are continuous, strictly decreasing, and

ğ‘™ğ‘– (ğ‘) < ğ‘‘ğ‘– (ğ‘) < ğ‘£ğ‘– (ğ‘) for all ğ‘.

11

Proof. Continuity of ğ¹ğ‘–, ğ‘“ğ‘–, ğ‘“ â€²

ğ‘– implies that of ğ‘™ğ‘–, ğ‘‘ğ‘–, ğ‘£ğ‘– . Itâ€™s known [10]

that log-concavity of ğ‘“ğ‘– implies log-concavity of both ğ¹ğ‘– and 1 âˆ’ ğ¹ğ‘– .
As a result, ğ‘™ğ‘– , ğ‘‘ğ‘– , and ğ‘£ğ‘– are derivatives of strictly concave functions;
therefore, they are strictly decreasing. In particular, each of

ğ‘£ â€²
ğ‘– (ğ‘) =

ğ‘“ â€²
ğ‘– (ğ‘)
ğ¹ğ‘– (ğ‘)

âˆ’

ğ‘“ğ‘– (ğ‘)2
ğ¹ğ‘– (ğ‘)2

,

ğ‘™ â€²
ğ‘– (ğ‘) =

âˆ’ğ‘“ â€²
ğ‘– (ğ‘)
1 âˆ’ ğ¹ğ‘– (ğ‘)

âˆ’

ğ‘“ğ‘– (ğ‘)2
(1 âˆ’ ğ¹ğ‘– (ğ‘))2

,

are negative for all ğ‘, so we conclude that

ğ‘‘ğ‘– (ğ‘) âˆ’ ğ‘£ğ‘– (ğ‘) =

ğ‘“ â€²
ğ‘– (ğ‘)
ğ‘“ğ‘– (ğ‘)

âˆ’

ğ‘“ğ‘– (ğ‘)
ğ¹ğ‘– (ğ‘)

ğ‘™ğ‘– (ğ‘) âˆ’ ğ‘‘ğ‘– (ğ‘) = âˆ’

ğ‘“ â€²
ğ‘– (ğ‘)
ğ‘“ğ‘– (ğ‘)

âˆ’

ğ‘“ğ‘– (ğ‘)
1 âˆ’ ğ¹ğ‘– (ğ‘)

=

=

ğ¹ğ‘– (ğ‘)
ğ‘“ğ‘– (ğ‘)
1 âˆ’ ğ¹ğ‘– (ğ‘)
ğ‘“ğ‘– (ğ‘)

ğ‘£ â€²
ğ‘– (ğ‘) < 0,

ğ‘™ â€²
ğ‘– (ğ‘) < 0.

â–¡

Theorem 3.2. Suppose that for all ğ‘—, ğ‘“ğ‘— is continuously differen-
tiable and log-concave. Then the unique maximizer of Pr(ğ‘ƒğ‘– = ğ‘ |
ğ‘– , ğ¸ğ‘Š
ğ¸ğ¿
ğ‘–

) is given by the unique zero of

ğ‘„ğ‘– (ğ‘) =

ğ‘™ ğ‘— (ğ‘) +

âˆ‘ï¸

ğ‘— â‰»ğ‘–

âˆ‘ï¸

ğ‘—âˆ¼ğ‘–

ğ‘‘ ğ‘— (ğ‘) +

ğ‘£ ğ‘— (ğ‘).

âˆ‘ï¸

ğ‘— â‰ºğ‘–

Proof. First, we rank the players by their buckets according to

âŒŠğ‘ƒ ğ‘— /ğœ–âŒ‹, and take the limiting probabilities as ğœ– â†’ 0:

Pr(âŒŠ

Pr(âŒŠ

ğ‘ƒ ğ‘—
ğœ–

ğ‘ƒ ğ‘—
ğœ–

âŒ‹ > âŒŠ

âŒ‹ < âŒŠ

1
ğœ–

Pr(âŒŠ

ğ‘ƒ ğ‘—
ğœ–

âŒ‹ = âŒŠ

ğ‘
ğœ–

ğ‘
ğœ–

ğ‘
ğœ–

âŒ‹) = Pr(ğ‘ ğ‘— â‰¥ ğœ– âŒŠ
ğ‘
ğœ–

= 1 âˆ’ ğ¹ ğ‘— (ğœ– âŒŠ

âŒ‹) = Pr(ğ‘ ğ‘— < ğœ– âŒŠ

ğ‘
ğœ–

âŒ‹ + ğœ–)

âŒ‹ + ğœ–) â†’ 1 âˆ’ ğ¹ ğ‘— (ğ‘),

ğ‘
ğœ–

âŒ‹)

= ğ¹ ğ‘— (ğœ– âŒŠ

ğ‘
ğœ–

âŒ‹) â†’ ğ¹ ğ‘— (ğ‘),

âŒ‹) =

=

1
ğœ–
1
ğœ–

ğ‘
ğœ–

Pr(ğœ– âŒŠ

(cid:16)

ğ¹ ğ‘— (ğœ– âŒŠ

âŒ‹ â‰¤ ğ‘ƒ ğ‘— < ğœ– âŒŠ
ğ‘
ğœ–

âŒ‹ + ğœ–) âˆ’ ğ¹ ğ‘— (ğœ– âŒŠ

ğ‘
ğœ–

âŒ‹ + ğœ–)

(cid:17)

âŒ‹)

ğ‘
ğœ–

â†’ ğ‘“ğ‘— (ğ‘).

Let ğ¿ğœ–
ğ‘ƒ ğ‘—
ğœ– âŒ‹ < âŒŠ

ğ‘
ğœ– âŒ‹,
ğ‘
ğœ– âŒ‹. respectively. These correspond to a
âŒŠ
player who performs at ğ‘ losing, winning, and drawing against ğ‘—,

ğ‘—ğ‘ be shorthand for the events âŒŠ

ğ‘—ğ‘ , and ğ·ğœ–
ğ‘ƒ ğ‘—
ğœ– âŒ‹ = âŒŠ

ğ‘—ğ‘ , ğ‘Š ğœ–
ğ‘
ğœ– âŒ‹, and âŒŠ

ğ‘ƒ ğ‘—
ğœ– âŒ‹ > âŒŠ

[16] Mark E Glickman. 1995. A comprehensive guide to chess ratings. American Chess

Journal 3, 1 (1995), 59â€“102.

[17] Mark E Glickman. 1999. Parameter estimation in large dynamic paired compari-

son experiments. Applied Statistics (1999), 377â€“394.

[18] Mark E Glickman. 2012. Example of the Glicko-2 system. Boston University

(2012), 1â€“6.

[19] Linxia Gong, Xiaochuan Feng, Dezhi Ye, Hao Li, Runze Wu, Jianrong Tao,
Changjie Fan, and Peng Cui. 2020. OptMatch: Optimized Matchmaking via
Modeling the High-Order Interactions on the Arena. In KDD 2020. 2300â€“2310.
[20] Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkillTM: A Bayesian

Skill Rating System. In NeurIPS 2006. 569â€“576.

[21] Tzu-Kuo Huang, Chih-Jen Lin, and Ruby C. Weng. 2006. Ranking individuals by

group comparisons. In ICML 2006. ACM, 425â€“432.

[22] Stephanie Kovalchik. 2020. Extension of the Elo rating system to margin of

victory. International Journal of Forecasting (2020).

[23] Yao Li, Minhao Cheng, Kevin Fujii, Fushing Hsieh, and Cho-Jui Hsieh. 2018.
Learning from Group Comparisons: Exploiting Higher Order Interactions. In
NeurIPS 2018. 4986â€“4995.

[24] Tom Minka, Ryan Cleven, and Yordan Zaykov. 2018. TrueSkill 2: An improved
Bayesian skill rating system. Technical Report MSR-TR-2018-8. Microsoft.
[25] Sergey I. Nikolenko, Alexander, and V. Sirotkin. 2010. Extensions of the TrueSkill
TM rating system. In In Proceedings of the 9th International Conference on Appli-
cations of Fuzzy Systems and Soft Computing. 151â€“160.

[26] Jerneja PremelÄ, Goran VuÄkoviÄ‡, Nic James, and Bojan LeskoÅ¡ek. 2019. Reliability

of judging in DanceSport. Frontiers in psychology 10 (2019), 1001.

[27] Josh Stone and Nicholas D Matsakis. The Rayon library (Rust Crate). crates.io/

crates/rayon

[28] Lin Yang, Stanko Dimitrov, and Benny Mantin. 2014. Forecasting sales of new vir-
tual goods with the Elo rating system. Journal of Revenue and Pricing Management
13, 6 (Dec. 2014), 457â€“469.

respectively, when outcomes are determined by ğœ–-buckets. Then,
Pr(ğ·ğœ–
ğœ–

| ğ‘ƒğ‘– = ğ‘) = lim
ğœ–â†’0

Pr(ğ¸ğ‘Š
ğ‘–

Pr(ğ‘Š ğœ–

Pr(ğ¿ğœ–

, ğ¸ğ¿
ğ‘–

ğ‘—ğ‘ )

ğ‘—ğ‘ )

(cid:214)

(cid:214)

(cid:214)

ğ‘—ğ‘ )

Pr(ğ‘ƒğ‘– = ğ‘ | ğ¸ğ¿

ğ‘– , ğ¸ğ‘Š
ğ‘–

=

(cid:214)

ğ‘— â‰»ğ‘–

ğ‘— â‰»ğ‘–
(1 âˆ’ ğ¹ ğ‘— (ğ‘))

ğ‘— â‰ºğ‘–
(cid:214)

ğ‘—âˆ¼ğ‘–,ğ‘—â‰ ğ‘–

ğ¹ ğ‘— (ğ‘)

(cid:214)

ğ‘“ğ‘— (ğ‘),

) âˆ ğ‘“ğ‘– (ğ‘) Pr(ğ¸ğ¿

ğ‘– , ğ¸ğ‘Š
ğ‘–
(1 âˆ’ ğ¹ ğ‘— (ğ‘))

=

(cid:214)

ğ‘— â‰»ğ‘–

ğ‘—âˆ¼ğ‘–,ğ‘—â‰ ğ‘–

ğ‘— â‰ºğ‘–
| ğ‘ƒğ‘– = ğ‘)

(cid:214)

ğ¹ ğ‘— (ğ‘)

ğ‘“ğ‘— (ğ‘),

(cid:214)

ğ‘—âˆ¼ğ‘–

d
dğ‘

ln Pr(ğ‘ƒğ‘– = ğ‘ | ğ¸ğ¿

ğ‘– ,ğ¸ğ‘Š
ğ‘–

) =

ğ‘™ ğ‘— (ğ‘) +

âˆ‘ï¸

ğ‘— â‰»ğ‘–

ğ‘— â‰ºğ‘–

âˆ‘ï¸

ğ‘— â‰ºğ‘–

ğ‘£ ğ‘— (ğ‘) +

âˆ‘ï¸

ğ‘—âˆ¼ğ‘–

ğ‘‘ ğ‘— (ğ‘) = ğ‘„ğ‘– (ğ‘).

Since Lemma 3.1 tells us that ğ‘„ğ‘– is strictly decreasing, it only
remains to show that it has a zero. If the zero exists, it must be
ğ‘– , ğ¸ğ‘Š
unique and it will be the unique maximum of Pr(ğ‘ƒğ‘– = ğ‘ | ğ¸ğ¿
).
ğ‘–
To start, we want to prove the existence of ğ‘âˆ— such that ğ‘„ğ‘– (ğ‘âˆ—) <
0. Note that itâ€™s not possible to have ğ‘“ â€²
ğ‘— (ğ‘) â‰¥ 0 for all ğ‘, as in that
case the density would integrate to either zero or infinity. Thus, for
each ğ‘— such that ğ‘— âˆ¼ ğ‘–, we can choose ğ‘ ğ‘— such that ğ‘“ â€²
ğ‘— (ğ‘ ğ‘— ) < 0, and
so ğ‘‘ ğ‘— (ğ‘ ğ‘— ) < 0. Let ğ›¼ = âˆ’ (cid:205)ğ‘—âˆ¼ğ‘– ğ‘‘ ğ‘— (ğ‘ ğ‘— ) > 0.

Let ğ‘› = |{ ğ‘—

ğ‘— â‰º ğ‘–}|. For each ğ‘— such that ğ‘— â‰º ğ‘–, since
limğ‘â†’âˆ ğ‘£ ğ‘— (ğ‘) = 0/1 = 0, we can choose ğ‘ ğ‘— such that ğ‘£ ğ‘— (ğ‘ ğ‘— ) < ğ›¼/ğ‘›.
Let ğ‘âˆ— = maxğ‘— âª¯ğ‘– ğ‘ ğ‘— . Then,
ğ‘™ ğ‘— (ğ‘âˆ—) â‰¤ 0, âˆ‘ï¸
ğ‘—âˆ¼ğ‘–

ğ‘‘ ğ‘— (ğ‘âˆ—) â‰¤ âˆ’ğ›¼, âˆ‘ï¸
ğ‘— â‰ºğ‘–

ğ‘£ ğ‘— (ğ‘âˆ—) < ğ›¼ .

âˆ‘ï¸

:

ğ‘— â‰»ğ‘–
Therefore,

ğ‘„ğ‘– (ğ‘âˆ—) =

âˆ‘ï¸

ğ‘™ ğ‘— (ğ‘âˆ—) +

âˆ‘ï¸

ğ‘‘ ğ‘— (ğ‘âˆ—) +

ğ‘— â‰»ğ‘–

ğ‘—âˆ¼ğ‘–
< 0 âˆ’ ğ›¼ + ğ›¼ = 0.

ğ‘£ ğ‘— (ğ‘âˆ—)

âˆ‘ï¸

ğ‘— â‰ºğ‘–

By a symmetric argument, there also exists some ğ‘âˆ— for which
ğ‘„ğ‘– (ğ‘âˆ—) > 0. By the intermediate value theorem with ğ‘„ğ‘– continuous,
there exists ğ‘ âˆˆ (ğ‘âˆ—, ğ‘âˆ—) such that ğ‘„ğ‘– (ğ‘) = 0, as desired.
â–¡

REFERENCES
[1] CodeChef Rating System. codechef.com/ratings
[2] Codeforces Rating System. codeforces.com/blog/entry/20762
[3] Farming Volatility: How a major flaw in a well-known rating system takes over
the GBL leaderboard. reddit.com/r/TheSilphRoad/comments/hwff2d/farming_
volatility_how_a_major_flaw_in_a/

[4] Halo Xbox video game franchise: in numbers. telegraph.co.uk/technology/video-

games/11223730/Halo-in-numbers.html

[5] Kaggle Progression System. kaggle.com/progression
[6] LeetCode Rating System.

leetcode.com/discuss/general-discussion/468851/New-

Contest-Rating-Algorithm-(Coming-Soon)

[7] TopCoder Algorithm Rating System.

topcoder.com/community/competitive-

programming/how-to-compete/ratings

[8] Why Are Obstacle-Course Races So Popular? theatlantic.com/health/archive/

2018/07/why-are-obstacle-course-races-so-popular/565130/

[9] Sharad Agarwal and Jacob R. Lorch. 2009. Matchmaking for online games and

other latency-sensitive P2P systems. In SIGCOMM 2009. 315â€“326.

[10] Mark Yuying An. 1997. Log-concave probability distributions: Theory and statis-
tical testing. Duke University Dept of Economics Working Paper 95-03 (1997).
[11] Shuo Chen and Thorsten Joachims. 2016. Modeling Intransitivity in Matchup

and Comparison Data. In WSDM 2016. 227â€“236.

[12] Pierre Dangauthier, Ralf Herbrich, Tom Minka, and Thore Graepel. 2007. TrueSkill
Through Time: Revisiting the History of Chess. In NeurIPS 2007. 337â€“344.

[13] Arpad E. Elo. 1961. New USCF rating system. Chess Life 16 (1961), 160â€“161.
[14] RNDr Michal ForiÅ¡ek. 2009. Theoretical and Practical Aspects of Programming

Contest Ratings. (2009).

[15] David A Freedman. 1963. On the asymptotic behavior of Bayesâ€™ estimates in the

discrete case. The Annals of Mathematical Statistics (1963), 1386â€“1403.

12

