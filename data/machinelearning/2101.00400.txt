An Elo-like System for Massive Multiplayer Competitions

Aram Ebtekar
Vancouver, BC, Canada
aramebtech@gmail.com

Paul Liu
Stanford University
Stanford, CA, USA
paul.liu@stanford.edu

1
2
0
2

n
a
J

2

]

R

I
.
s
c
[

1
v
0
0
4
0
0
.
1
0
1
2
:
v
i
X
r
a

ABSTRACT
Rating systems play an important role in competitive sports and
games. They provide a measure of player skill, which incentivizes
competitive performances and enables balanced match-ups. In this
paper, we present a novel Bayesian rating system for contests with
many participants. It is widely applicable to competition formats
with discrete ranked matches, such as online programming competi-
tions, obstacle courses races, and some video games. The simplicity
of our system allows us to prove theoretical bounds on robustness
and runtime. In addition, we show that the system aligns incentives:
that is, a player who seeks to maximize their rating will never want
to underperform. Experimentally, the rating system rivals or sur-
passes existing systems in prediction accuracy, and computes faster
than existing systems by up to an order of magnitude.

1 INTRODUCTION
Competitions, in the form of sports, games, and examinations, have
been with us since antiquity. Many competitions grade perfor-
mances along a numerical scale, such as a score on a test or a
completion time in a race. In the case of a college admissions exam
or a track race, scores are standardized so that a given score on
two different occasions carries the same meaning. However, in
events that feature novelty, subjectivity, or close interaction, stan-
dardization is difficult. The Spartan Races, completed by millions
of runners, feature a variety of obstacles placed on hiking trails
around the world [8]. Rock climbing, a sport to be added to the
2020 Olympics, likewise has routes set specifically for each com-
petition. DanceSport, gymnastics, and figure skating competitions
have a panel of judges who rank contestants against one another;
these subjective scores are known to be noisy [26]. Most board
games feature considerable inter-player interaction. In all these
cases, scores can only be used to compare and rank participants at
the same event. Players, spectators, and contest organizers who are
interested in comparing players’ skill levels across different com-
petitions will need to aggregate the entire history of such rankings.
A strong player, then, is one who consistently wins against weaker
players. To quantify skill, we need a rating system.

Good rating systems are difficult to create, as they must bal-
ance several mutually constraining objectives. First and foremost,
the rating system must be accurate, in that ratings provide use-
ful predictors of contest outcomes. Second, the ratings must be
efficient to compute: in video game applications, rating systems
are predominantly used for matchmaking in massively multiplayer
online games (such as Halo, CounterStrike, League of Legends,
etc.) [20, 24, 28]. These games have hundreds of millions of players
playing tens of millions of games per day, necessitating certain
latency and memory requirements for the rating system [9]. Third,
the rating system must align incentives. That is, players should

not modify their performance to “game” the rating system. Rating
systems that can be gamed often create disastrous consequences to
player-base, more often than not leading to the loss of players from
the game [3]. Finally, the ratings provided by the system must be
human-interpretable: ratings are typically represented to players as
a single number encapsulating their overall skill, and many players
want to understand and predict how their performances affect their
rating [16].

Classically, rating systems were designed for two-player games.
The famous Elo system [13], as well as its Bayesian successors
Glicko and Glicko-2, have been widely applied to games such as
Chess and Go [16–18]. Both Glicko versions model each player’s
skill as a real random variable that evolves with time according
to Brownian motion. Inference is done by entering these variables
into the Bradley-Terry model, which predicts probabilities of game
outcomes. Glicko-2 refines the Glicko system by adding a rating
volatility parameter. Unfortunately, Glicko-2 is known to be flawed
in practice, potentially incentivising players to lose. This was most
notably exploited in the popular game of Pokemon Go [3]; see
Section 5.1 for a discussion of this issue.

The family of Elo-like methods just described only utilize the
binary outcome of a match. In settings where a scoring system
provides a more fine-grained measure of match performance, Ko-
valchik [22] has shown variants of Elo that are able to take advan-
tage of score information. For competitions consisting of several set
tasks, such as academic olympiads, Forišek [14] developed a model
in which each task gives a different “response” to the player: the to-
tal response then predicts match outcomes. However, such systems
are often highly application-dependent and hard to calibrate.

Though Elo-like systems are widely used in two-player contests,
one needn’t look far to find competitions that involve much more
than two players. Aside from the aforementioned sporting examples,
there are video games such as CounterStrike and Halo, as well as
academic olympiads: notably, programming contest platforms such
as Codeforces, TopCoder, and Kaggle [2, 5, 7]. In these applications,
the number of contestants can easily reach into the thousands.
Some more recent works present interesting methods to deal with
competitions between two teams [11, 19, 21, 23], but they do not
present efficient extensions for settings in which players are sorted
into more than two, let alone thousands, of distinct places.

In a many-player ranked competition, it is important to note that
the pairwise match outcomes are not independent, as they would be
in a series of 1v1 matches. Thus, TrueSkill [20] and its variants [12,
24, 25] model a player’s performance during each contest as a single
random variable. The overall rankings are assumed to reveal the
total order among these hidden performance variables, with various
strategies used to model ties and teams. These TrueSkill algorithms
are efficient in practice, successfully rating userbases that number

 
 
 
 
 
 
well into the millions (the Halo series, for example, has over 60
million sales since 2001 [4]).

The main disadvantage of TrueSkill is its complexity: originally
developed by Microsoft for the popular Halo video game, TrueSkill
performs approximate belief propagation on a factor graph, which
is iterated until convergence [20]. Aside from being less human-
interpretable, this complexity means that, to our knowledge, there
are no proofs of key properties such as runtime and incentive align-
ment. Even when these properties are discussed [24], no rigorous
justification is provided. In addition, we are not aware of any work
that extends TrueSkill to non-Gaussian performance models, which
might be desirable to limit the influence of outlier performances
(see Section 5.2).

It might be for these reasons that platforms such as Codeforces
and TopCoder opted for their own custom rating systems. These sys-
tems are not published in academia and do not come with Bayesian
justifications. However, they retain the formulaic simplicity of Elo
and Glicko, extending them to settings with much more than two
players. The Codeforces system includes ad hoc heuristics to dis-
tinguish top players, while curbing rampant inflation. TopCoder’s
formulas are more principled from a statistical perspective; how-
ever, it has a volatility parameter similar to Glicko-2, and hence
suffers from similar exploits [14]. Despite their flaws, these sys-
tems have been in place for over a decade, and have more recently
gained adoption by additional platforms such as CodeChef and
LeetCode [1, 6].

Our contributions. In this paper, we describe the Elo-MMR rat-
ing system, obtained by a principled approximation of a Bayesian
model very similar to TrueSkill. It is fast, embarrassingly parallel,
and makes accurate predictions. Most interesting of all, its simplic-
ity allows us to rigorously analyze its properties: the “MMR” in
the name stands for “Massive”, “Monotonic”, and “Robust”. “Mas-
sive” means that it supports any number of players with a runtime
that scales linearly; “monotonic” means that it aligns incentives
so that a rating-maximizing player always wants to perform well;
“robust” means that rating changes are bounded, with the bound
being smaller for more consistent players than for volatile play-
ers. Robustness turns out to be a natural byproduct of accurately
modeling performances with heavy-tailed distributions, such as
the logistic. TrueSkill is believed to satisfy the first two properties,
albeit without proof, but fails robustness. Codeforces only satisfies
aligned incentives, and TopCoder only satisfies robustness.

Experimentally, we show that Elo-MMR achieves state-of-the-art
performance in terms of both prediction accuracy and runtime. In
particular, we process the entire Codeforces database of over 300K
rated users and 1000 contests in well under a minute, beating the ex-
isting Codeforces system by an order of magnitude while improving
upon its accuracy. A difficulty we faced was the scarcity of efficient
open-source rating system implementations. In an effort to aid
researchers and practitioners alike, we provide open-source imple-
mentations of all rating systems, datasets, and additional processing
used in our experiments at https://github.com/EbTech/EloR/.

Organization. In Section 2, we formalize the details of our Bayesian
model. We then show how to estimate player skill under this model
in Section 3, and develop some intuitions of the resulting formulas.

2

As a further refinement, Section 4 models skill evolutions from play-
ers training or atrophying between competitions. This modeling is
quite tricky as we choose to retain players’ momentum while en-
suring it cannot be exploited for incentive-misaligned rating gains.
Section 5 proves that the system as a whole satisfies several salient
properties, the most critical of which is aligned incentives. Finally,
we present experimental evaluations in Section 6.

2 A BAYESIAN MODEL FOR MASSIVE

COMPETITIONS

We now describe the setting formally, denoting random variables
by capital letters. A series of competitive rounds, indexed by 𝑡 =
1, 2, 3, . . ., take place sequentially in time. Each round has a set of
participating players P𝑡 , which may in general overlap between
rounds. A player’s skill is likely to change with time, so we repre-
sent the skill of player 𝑖 at time 𝑡 by a real random variable 𝑆𝑖,𝑡 .

.

In round 𝑡, each player 𝑖 ∈ P𝑡 competes at some performance
level 𝑃𝑖,𝑡 , typically close to their current skill 𝑆𝑖,𝑡 . The deviations
are assumed to be i.i.d. and independent of {𝑆𝑖,𝑡 }𝑖 ∈ P𝑡
{𝑃𝑖,𝑡 −𝑆𝑖,𝑡 }𝑖 ∈ P𝑡
Performances are not observed directly; instead, a ranking gives
. In particular,
the relative order among all performances {𝑃𝑖,𝑡 }𝑖 ∈ P𝑡
ties are modelled to occur when performances are exactly equal,
a zero-probability event when their distributions are continuous.1
This ranking constitutes the observational evidence 𝐸𝑡 for our
Bayesian updates. The rating system seeks to estimate the skill 𝑆𝑖,𝑡
of every player at the present time 𝑡, given the historical round
rankings 𝐸 ≤𝑡 := {𝐸1, . . . , 𝐸𝑡 }.

We overload the notation Pr for both probabilities and probabil-
ity densities: the latter interpretation applies to zero-probability
events, such as in Pr(𝑆𝑖,𝑡 = 𝑠). We also use colons as shorthand for
collections of variables differing only in a subscript: for instance,
𝑃:,𝑡 := {𝑃𝑖,𝑡 }𝑖 ∈ P𝑡
. The joint distribution described by our Bayesian
model factorizes as follows:

Pr(𝑆:,:, 𝑃:,:, 𝐸:)

(cid:214)

=

Pr(𝑆𝑖,0)

𝑖

(cid:214)

𝑖,𝑡

Pr(𝑆𝑖,𝑡 | 𝑆𝑖,𝑡 −1)

(cid:214)

𝑖,𝑡

Pr(𝑃𝑖,𝑡 | 𝑆𝑖,𝑡 )

Pr(𝐸𝑡 | 𝑃:,𝑡 ),

(cid:214)

𝑡

(1)

where Pr(𝑆𝑖,0) is the initial skill prior,
Pr(𝑆𝑖,𝑡 | 𝑆𝑖,𝑡 −1) is the skill evolution model (Section 4),

Pr(𝑃𝑖,𝑡 | 𝑆𝑖,𝑡 ) is the performance model, and
Pr(𝐸𝑡 | 𝑃:,𝑡 ) is the evidence model.

For the first three factors, we will specify log-concave distributions
(see Definition 3.1). The evidence model, on the other hand, is a
deterministic indicator. It equals one when 𝐸𝑡 is consistent with
the relative ordering among {𝑃𝑖,𝑡 }𝑖 ∈ P𝑡

, and zero otherwise.

Finally, our model assumes that the number of participants |P𝑡 | is
large. The main idea behind our algorithm is that, in sufficiently mas-
sive competitions, from the evidence 𝐸𝑡 we can infer very precise
estimates for {𝑃𝑖,𝑡 }𝑖 ∈ P𝑡
. Hence, we can treat these performances
as if they were observed directly.

That is, suppose we have the skill prior at round 𝑡:

𝜋𝑖,𝑡 (𝑠) := Pr(𝑆𝑖,𝑡 = 𝑠 | 𝑃𝑖,<𝑡 ).

(2)

1The relevant limiting procedure is to treat performances within 𝜖-width buckets as
ties, and letting 𝜖 → 0. This technicality appears in the proof of Theorem 3.2.

Now, we observe 𝐸𝑡 . By Equation (1), it is conditionally indepen-

dent of 𝑆𝑖,𝑡 , given 𝑃𝑖, ≤𝑡 . By the law of total probability,

3 A TWO-PHASE ALGORITHM FOR SKILL

ESTIMATION

Pr(𝑆𝑖,𝑡 = 𝑠 | 𝑃𝑖,<𝑡 , 𝐸𝑡 )

∫

=

Pr(𝑆𝑖,𝑡 = 𝑠 | 𝑃𝑖,<𝑡 , 𝑃𝑖,𝑡 = 𝑝) Pr(𝑃𝑖,𝑡 = 𝑝 | 𝑃𝑖,<𝑡 , 𝐸𝑡 ) d𝑝

→ Pr(𝑆𝑖,𝑡 = 𝑠 | 𝑃𝑖, ≤𝑡 )

almost surely as |P𝑡 | → ∞.

The integral is intractable in general, since the performance poste-
rior Pr(𝑃𝑖,𝑡 = 𝑝 | 𝑃𝑖,<𝑡 , 𝐸𝑡 ) depends not only on player 𝑖, but also
on our belief regarding the skills of all 𝑗 ∈ P𝑡 . However, in the limit
of infinite participants, Doob’s consistency theorem [15] implies
that it concentrates at the true value 𝑃𝑖,𝑡 . Since our posteriors are
continuous, the convergence holds for all 𝑠 simultaneously.
Indeed, we don’t even need the full evidence 𝐸𝑡 . Let 𝐸𝐿

𝑖,𝑡 = { 𝑗 ∈
P : 𝑃 𝑗,𝑡 > 𝑃𝑖,𝑡 } be the set of players against whom 𝑖 lost, and
𝐸𝑊
𝑖,𝑡 = { 𝑗 ∈ P : 𝑃 𝑗,𝑡 < 𝑃𝑖,𝑡 } be the set of players against whom
𝑖 won. That is, we only see who wins, draws, and loses against 𝑖.
𝑃𝑖,𝑡 remains identifiable using only (𝐸𝐿
𝑖,𝑡 ), which will be more
convenient for our purposes.

𝑖,𝑡 , 𝐸𝑊

Passing to the limit |P𝑡 | → ∞ serves to justify some common
simplifications made by algorithms such as TrueSkill: since condi-
tioning on 𝑃𝑖, ≤𝑡 makes the skills of different players independent of
one another, it becomes accurate to model them as such. In addition
to simplifying derivations, this fact ensures that a player’s poste-
rior is unaffected by rounds in which they are not a participant,
arguably a desirable property in its own right. Furthermore, 𝑃𝑖, ≤𝑡
being a sufficient statistic for skill prediction renders any additional
information, such as domain-specific raw scores, redundant.

√︃

log 1

𝛿 ) participants.

Finally, a word on the rate of convergence. Suppose we want
our estimate to be within 𝜖 of 𝑃𝑖,𝑡 , with probability at least 1 − 𝛿.
By asymptotic normality of the posterior [15], it suffices to have
𝑂 ( 1
𝜖 2
When the initial prior, performance model, and evolution model
are all Gaussian, treating 𝑃𝑖,𝑡 as certain is the only simplifying
approximation we will make; that is, in the limit |P𝑡 | → ∞, our
method performs exact inference on Equation (1). In the following
sections, we focus some attention on generalizing the performance
model to non-Gaussian log-concave families, parametrized by lo-
cation and scale. We will use the logistic distribution as a running
example and see that it induces robustness; however, our framework
is agnostic to the specific distributions used.

The rating 𝜇𝑖,𝑡 of player 𝑖 after round 𝑡 should be a statistic that
summarizes their posterior distribution: we’ll use the maximum a
posteriori (MAP) estimate, obtained by setting 𝑠 to maximize the
posterior Pr(𝑆𝑖,𝑡 = 𝑠 | 𝑃𝑖, ≤𝑡 ). By Bayes’ rule,

𝜇𝑖,𝑡 := arg max

𝑠

𝜋𝑖,𝑡 (𝑠) Pr(𝑃𝑖,𝑡 | 𝑆𝑖,𝑡 = 𝑠).

(3)

This objective suggests a two-phase algorithm to update each player
𝑖 ∈ P𝑡 at round 𝑡. In phase one, we estimate 𝑃𝑖,𝑡 from (𝐸𝐿
𝑖,𝑡 ). By
Doob’s consistency theorem, our estimate is extremely precise when
|P𝑡 | is large, so we assume it to be exact. In phase two, we update
our posterior for 𝑆𝑖,𝑡 and the rating 𝜇𝑖,𝑡 according to Equation (3).
We will occasionally make use of the prior rating, defined as

𝑖,𝑡 , 𝐸𝑊

𝜇𝜋
𝑖,𝑡 := arg max

𝑠

𝜋𝑖,𝑡 (𝑠).

3

3.1 Performance estimation
In this section, we describe the first phase of Elo-MMR. For nota-
tional convenience, we assume all probability expressions to be
conditioned on the prior context 𝑃𝑖,<𝑡 , and omit the subscript 𝑡.
Our prior belief on each player’s skill 𝑆𝑖 implies a prior distribu-

tion on 𝑃𝑖 . Let’s denote its probability density function (pdf) by

𝑓𝑖 (𝑝) := Pr(𝑃𝑖 = 𝑝) =

∫

𝜋𝑖 (𝑠) Pr(𝑃𝑖 = 𝑝 | 𝑆𝑖 = 𝑠) d𝑠,

(4)

where 𝜋𝑖 (𝑠) was defined in Equation (2). Let

𝐹𝑖 (𝑝) := Pr(𝑃𝑖 ≤ 𝑝) =

∫ 𝑝

−∞

𝑓𝑖 (𝑥) d𝑥,

be the corresponding cumulative distribution function (cdf). For the
purpose of analysis, we’ll also define the following “loss”, “draw”,
and “victory” functions:

𝑙𝑖 (𝑝) :=

𝑑𝑖 (𝑝) :=

𝑣𝑖 (𝑝) :=

d
d𝑝

d
d𝑝
d
d𝑝

ln(1 − 𝐹𝑖 (𝑝)) =

−𝑓𝑖 (𝑝)
1 − 𝐹𝑖 (𝑝)

,

ln 𝑓𝑖 (𝑝) =

ln 𝐹𝑖 (𝑝) =

𝑓 ′
𝑖 (𝑝)
𝑓𝑖 (𝑝)
𝑓𝑖 (𝑝)
𝐹𝑖 (𝑝)

,

.

Evidently, 𝑙𝑖 (𝑝) < 0 < 𝑣𝑖 (𝑝). Now we define what it means for

the deviation 𝑃𝑖 − 𝑆𝑖 to be log-concave.

Definition 3.1. An absolutely continuous random variable on a
convex domain is log-concave if its probability density function 𝑓 is
positive on its domain and satisfies

𝑓 (𝜃𝑥 + (1 − 𝜃 )𝑦) > 𝑓 (𝑥)𝜃 𝑓 (𝑦)1−𝜃 , ∀𝜃 ∈ (0, 1), 𝑥 ≠ 𝑦.

We note that log-concave distributions appear widely, and in-
clude the Gaussian and logistic distributions used in Glicko, TrueSkill,
and many others. We’ll see inductively that our prior 𝜋𝑖 is log-
concave at every round. Since log-concave densities are closed
under convolution [10], the independent sum 𝑃𝑖 = 𝑆𝑖 + (𝑃𝑖 − 𝑆𝑖 ) is
also log-concave. The following lemma (proved in the appendix)
makes log-concavity very convenient:

Lemma 3.1. If 𝑓𝑖 is continuously differentiable and log-concave,

then the functions 𝑙𝑖, 𝑑𝑖, 𝑣𝑖 are continuous, strictly decreasing, and

𝑙𝑖 (𝑝) < 𝑑𝑖 (𝑝) < 𝑣𝑖 (𝑝) for all 𝑝.

For the remainder of this section, we fix the analysis with respect
to some player 𝑖. As argued in Section 2, 𝑃𝑖 concentrates very
narrowly in the posterior. Hence, we can estimate 𝑃𝑖 by its MAP,
choosing 𝑝 so as to maximize:

Pr(𝑃𝑖 = 𝑝 | 𝐸𝐿

𝑖 , 𝐸𝑊
𝑖

) ∝ 𝑓𝑖 (𝑝) Pr(𝐸𝐿

𝑖 , 𝐸𝑊
𝑖

| 𝑃𝑖 = 𝑝).

Define 𝑗 ≻ 𝑖, 𝑗 ≺ 𝑖, 𝑗 ∼ 𝑖 as shorthand for 𝑗 ∈ 𝐸𝐿

𝑖 , 𝑗 ∈ 𝐸𝑊
,
𝑖
) (that is, 𝑃 𝑗 > 𝑃𝑖 , 𝑃 𝑗 < 𝑃𝑖 , 𝑃 𝑗 = 𝑃𝑖 ), respectively.

𝑗 ∈ P \ (𝐸𝐿
The following theorem yields our MAP estimate:

𝑖 ∪ 𝐸𝑊
𝑖

A logistic distribution with variance 𝛿 2

𝐹 𝑗 (𝑥) =

𝑓𝑗 (𝑥) =

1

𝑗 )/ ¯𝛿 𝑗
1 + 𝑒−(𝑥−𝜇𝜋
𝑗 )/ ¯𝛿 𝑗
𝑒 (𝑥−𝜇𝜋
(cid:16)1 + 𝑒 (𝑥−𝜇𝜋

¯𝛿 𝑗

𝑗 has cdf and pdf:
(cid:33)
(cid:32)

1 + tanh

1
2

=

,

𝑥 − 𝜇𝜋
𝑗
2 ¯𝛿 𝑗
𝑥 − 𝜇𝜋
𝑗
2 ¯𝛿 𝑗

.

𝑗 )/ ¯𝛿 𝑗 (cid:17)2 =

1
4 ¯𝛿 𝑗

sech2

Figure 1: 𝐿2 versus 𝐿𝑅 for typical values (left). Gaussian ver-
sus logistic probability density functions (right).

Theorem 3.2. Suppose that for all 𝑗, 𝑓𝑗 is continuously differen-
tiable and log-concave. Then the unique maximizer of Pr(𝑃𝑖 = 𝑝 |
𝑖 , 𝐸𝑊
𝐸𝐿
𝑖

) is given by the unique zero of

𝑄𝑖 (𝑝) :=

𝑙 𝑗 (𝑝) +

∑︁

𝑗 ≻𝑖

∑︁

𝑗∼𝑖

𝑑 𝑗 (𝑝) +

𝑣 𝑗 (𝑝).

∑︁

𝑗 ≺𝑖

The proof is relegated to the appendix. Intuitively, we’re saying
that the performance is the balance point between appropriately
weighted wins, draws, and losses. Let’s look at two specializations
of our general model, to serve as running examples in this paper.

Gaussian performance model. If both 𝑆 𝑗 and 𝑃 𝑗 − 𝑆 𝑗 are assumed
to be Gaussian with known means and variances, then their inde-
pendent sum 𝑃 𝑗 will also be a known Gaussian. It is analytic and
log-concave, so Theorem 3.2 applies.

We substitute the well-known Gaussian pdf and cdf for 𝑓𝑗 and 𝐹 𝑗 ,
respectively. A simple binary search, or faster numerical techniques
such as the Illinois algorithm or Newton’s method, can be employed
to solve for the maximizing 𝑝.

Logistic performance model. Now we assume the performance
deviation 𝑃 𝑗 − 𝑆 𝑗 has a logistic distribution with mean 0 and vari-
ance 𝛽2. In general, the rating system administrator is free to set 𝛽
differently for each contest. Since shorter contests tend to be more
variable, one reasonable choice might be to make 1/𝛽2 proportional
to the contest duration.

Given the mean and variance of the skill prior, the independent
sum 𝑃 𝑗 = 𝑆 𝑗 + (𝑃 𝑗 − 𝑆 𝑗 ) would have the same mean, and a variance
that’s increased by 𝛽2. Unfortunately, we’ll see that the logistic
performance model implies a form of skill prior from which it’s
tough to extract a mean and variance. Even if we could, the sum
does not yield a simple distribution.

For experienced players, we expect 𝑆 𝑗 to contribute much less
variance than 𝑃 𝑗 − 𝑆 𝑗 ; thus, in our heuristic approximation, we take
𝑃 𝑗 to have the same form of distribution as the latter. That is, we
take 𝑃 𝑗 to be logistic, centered at the prior rating 𝜇𝜋
𝑗 = arg max 𝜋 𝑗 ,
with variance 𝛿 2
𝑗 + 𝛽2, where 𝜎 𝑗 will be given by Equation (8).
This distribution is analytic and log-concave, so the same methods
√
based on Theorem 3.2 apply. Define the scale parameter ¯𝛿 𝑗 :=
3
𝜋 𝛿 𝑗 .

𝑗 = 𝜎2

4

The logistic distribution satisfies two very convenient relations:
𝑗 (𝑥) = 𝑓𝑗 (𝑥) = 𝐹 𝑗 (𝑥)(1 − 𝐹 𝑗 (𝑥))/ ¯𝛿 𝑗 ,
𝐹 ′
𝑗 (𝑥) = 𝑓𝑗 (𝑥)(1 − 2𝐹 𝑗 (𝑥))/ ¯𝛿 𝑗 ,
𝑓 ′

from which it follows that

𝑑 𝑗 (𝑝) =

1 − 2𝐹 𝑗 (𝑝)
¯𝛿

=

−𝐹 𝑗 (𝑝)
¯𝛿

+

1 − 𝐹 𝑗 (𝑝)
¯𝛿

= 𝑙 𝑗 (𝑝) + 𝑣 𝑗 (𝑝).

In other words, a tie counts as the sum of a win and a loss. This
can be compared to the approach (used in Elo, Glicko, TopCoder,
and Codeforces) of treating each tie as half a win plus half a loss.2

Finally, putting everything together:

𝑄𝑖 (𝑝) =

𝑙 𝑗 (𝑝) +

∑︁

𝑗 ⪰𝑖

∑︁

𝑗 ⪯𝑖

𝑣 𝑗 (𝑝) =

−𝐹 𝑗 (𝑝)
¯𝛿 𝑗

∑︁

𝑗 ⪰𝑖

+

∑︁

𝑗 ⪯𝑖

1 − 𝐹 𝑗 (𝑝)
¯𝛿 𝑗

.

Our estimate for 𝑃𝑖 is the zero of this expression. The terms on
the right correspond to probabilities of winning and losing against
each player 𝑗, weighted by 1/ ¯𝛿 𝑗 . Accordingly, we can interpret
(cid:205)𝑗 ∈ P (1 − 𝐹 𝑗 (𝑝))/ ¯𝛿 𝑗 as a weighted expected rank of a player whose
performance is 𝑝. Similar to the performance computations in Code-
forces and TopCoder, 𝑃𝑖 can thus be viewed as the performance
level at which one’s expected rank would equal 𝑖’s actual rank.

3.2 Belief update
Having estimated 𝑃𝑖,𝑡 in the first phase, the second phase is rather
simple. Ignoring normalizing constants, Equation (3) tells us that the
pdf of the skill posterior can be obtained as the pointwise product
of the pdfs of the skill prior and the performance model. When both
factors are differentiable and log-concave, then so is their product.
Its maximum is the new rating 𝜇𝑖,𝑡 ; let’s see how to compute it for
the same two specializations of our model.

Gaussian skill prior and performance model. When the skill prior
and performance model are Gaussian with known means and vari-
ances, multiplying their pdfs yields another known Gaussian. Hence,
the posterior is compactly represented by its mean 𝜇𝑖,𝑡 , which coin-
cides with the MAP and rating; and its variance 𝜎2
𝑖,𝑡 , which is our
uncertainty regarding the player’s skill.

Logistic performance model. When the performance model is
non-Gaussian, the multiplication does not simplify so easily. By
Equation (3), each round contributes an additional factor to the
belief distribution. In general, we allow it to consist of a collection
of simple log-concave factors, one for each round in which player 𝑖
has participated. Denote the participation history by

H𝑖,𝑡 := {𝑘 ∈ {1, . . . , 𝑡 } : 𝑖 ∈ P𝑘 }.

2Elo-MMR, too, can be modified to split ties into half win plus half loss. It’s easy to
check that Lemma 3.1 still holds if 𝑑 𝑗 (𝑝) is replaced by 𝑤𝑙 𝑙 𝑗 (𝑝) + 𝑤𝑣 𝑣𝑗 (𝑝) for some
𝑤𝑙 , 𝑤𝑣 ∈ [0, 1] with |𝑤𝑙 − 𝑤𝑣 | < 1. In particular, we can set 𝑤𝑙 = 𝑤𝑣 = 0.5. The
results in Section 5 won’t be altered by this change.

L2LR-4-22424681012NormalLogistic-4-2240.10.20.30.4Since each player can be considered in isolation, we’ll omit the
subscript 𝑖. Specializing to the logistic setting, each 𝑘 ∈ H𝑡 con-
tributes a logistic factor to the posterior, with mean 𝑝𝑘 and variance
𝛽2
. We still use a Gaussian initial prior, with mean and variance
𝑘
denoted by 𝑝0 and 𝛽2
0, respectively. Postponing the discussion of
skill evolution to Section 4, for the moment we assume that 𝑆𝑘 = 𝑆0
for all 𝑘. The posterior pdf, up to normalization, is then

(cid:214)

𝜋0 (𝑠)

Pr(𝑃𝑘 = 𝑝𝑘 | 𝑆𝑘 = 𝑠)

𝑘 ∈H𝑡
(cid:32)

∝ exp

−

(𝑠 − 𝑝0)2
2𝛽2
0

(cid:33)

(cid:214)

𝑘 ∈H𝑡

sech2 (cid:18) 𝜋
√
12

𝑠 − 𝑝𝑘
𝛽𝑘

(cid:19)

.

(5)

Maximizing the posterior density amounts to minimizing its

negative logarithm. Up to a constant offset, this is given by
(cid:18) 𝑠 − 𝑝0
𝛽0

(cid:18) 𝑠 − 𝑝𝑘
𝛽𝑘

𝐿(𝑠) := 𝐿2

∑︁

𝐿𝑅

+

(cid:19)

(cid:19)

,

𝑘 ∈H𝑡

where 𝐿2 (𝑥) :=

1
2

𝑥 2 and 𝐿𝑅 (𝑥) := 2 ln

(cid:18)

cosh

(cid:19)

𝜋𝑥
√

12

Thus, 𝐿′(𝑠) =

𝑠 − 𝑝0
𝛽2
0

∑︁

+

𝑘 ∈H𝑡

𝜋
√
𝛽𝑘

3

tanh

(𝑠 − 𝑝𝑘 )𝜋
√
12
𝛽𝑘

.

.

(6)

𝐿′ is continuous and strictly increasing in 𝑠, so its zero is unique:
it is the MAP 𝜇𝑡 . Similar to what we did in the first phase, we can
solve for 𝜇𝑡 with either binary search or Newton’s method.

We pause to make an important observation. From Equation (6),
the rating carries a rather intuitive interpretation: Gaussian factors
in 𝐿 become 𝐿2 penalty terms, whereas logistic factors take on a
more interesting form as 𝐿𝑅 terms. From Figure 1, we see that the
𝐿𝑅 term behaves quadratically near the origin, but linearly at the
extremities, effectively interpolating between 𝐿2 and 𝐿1 over a scale
of magnitude 𝛽𝑘

It is well-known that minimizing a sum of 𝐿2 terms pushes the
argument towards a weighted mean, while minimizing a sum of
𝐿1 terms pushes the argument towards a weighted median. With
𝐿𝑅 terms, the net effect is that 𝜇𝑡 acts like a robust average of the
historical performances 𝑝𝑘 . Specifically, one can check that

𝜇𝑡 =

(cid:205)𝑘 𝑤𝑘𝑝𝑘
(cid:205)𝑘 𝑤𝑘

𝑤𝑘 :=

𝜋
(𝜇𝑡 − 𝑝𝑘 )𝛽𝑘

√
3

tanh

, where 𝑤0 :=

1
𝛽2
0
(𝜇𝑡 − 𝑝𝑘 )𝜋
√

𝛽𝑘

12

and

for 𝑘 ∈ H𝑡 .

(7)

𝑤𝑘 is close to 1/𝛽2
𝑘

for typical performances, but can be up to
𝜋 2/6 times more as |𝜇𝑡 − 𝑝𝑘 | → 0, or vanish as |𝜇𝑡 − 𝑝𝑘 | → ∞.
This feature is due to the thicker tails of the logistic distribution,
as compared to the Gaussian, resulting in an algorithm that resists
drastic rating changes in the presence of a few unusually good or
bad performances. We’ll formally state this robustness property in
Theorem 5.7.

Estimating skill uncertainty. While there is no easy way to com-
pute the variance of a posterior in the form of Equation (5), it will
be useful to have some estimate 𝜎2
𝑡 of uncertainty. There is a simple
formula in the case where all factors are Gaussian. Since moment-
matched logistic and normal distributions are relatively close (c.f.

5

Figure 1), we apply the same formula:

1
𝜎2
𝑡

:=

∑︁

𝑘 ∈ {0}∪H𝑡

.

1
𝛽2
𝑘

(8)

4 SKILL EVOLUTION OVER TIME
Factors such as training and resting will change a player’s skill
over time. If we model skill as a static variable, our system will
eventually grow so confident in its estimate that it will refuse to
admit substantial changes. To remedy this, we introduce a skill
evolution model, so that in general 𝑆𝑡 ≠ 𝑆𝑡 ′ for 𝑡 ≠ 𝑡 ′. Now rather
than simply being equal to the previous round’s posterior, the skill
prior at round 𝑡 is given by

∫

𝜋𝑡 (𝑠) =

Pr(𝑆𝑡 = 𝑠 | 𝑆𝑡 −1 = 𝑥) Pr(𝑆𝑡 −1 = 𝑥 | 𝑃<𝑡 ) d𝑥 .

(9)

The factors in the integrand are the skill evolution model and the
previous round’s posterior, respectively. Following other Bayesian
rating systems (e.g., Glicko, Glicko-2, and TrueSkill [17, 18, 20]),
we model the skill diffusions 𝑆𝑡 − 𝑆𝑡 −1 as independent zero-mean
Gaussians. That is, Pr(𝑆𝑡 | 𝑆𝑡 −1 = 𝑥) is a Gaussian with mean 𝑥 and
some variance 𝛾 2
𝑡 proportionally to the
time elapsed since the last update, corresponding to a continuous
Brownian motion. Codeforces and TopCoder simply set 𝛾𝑡 to a con-
stant when a player participates, and zero otherwise, corresponding
to changes that are in proportion to how often the player competes.
Now we are ready to complete the two specializations of our rating
system.

𝑡 . The Glicko system sets 𝛾 2

Gaussian skill prior and performance model. If both the prior
and performance distributions at round 𝑡 − 1 are Gaussian, then
the posterior is also Gaussian. Adding an independent Gaussian
diffusion to our posterior on 𝑆𝑡 −1 yields a Gaussian prior on 𝑆𝑡 . By
induction, the skill belief distribution forever remains Gaussian.
This Gaussian specialization of the Elo-MMR framework lacks the
R for robustness (see Theorem 5.7), so we call it Elo-MM𝜒.

Logistic performance model. After a player’s first contest round,
the posterior in Equation (5) becomes non-Gaussian, rendering the
integral in Equation (9) intractable.

A very simple approach would be to replace the full posterior in
Equation (5) by a Gaussian approximation with mean 𝜇𝑡 (equal to
the posterior MAP) and variance 𝜎2
𝑡 (given by Equation (8)). As in
the previous case, applying diffusions in the Gaussian setting is a
simple matter of adding means and variances.

With this approximation, no memory is kept of the individual
performances 𝑃𝑡 . Priors are simply Gaussian, while posterior den-
sities are the product of two factors: the Gaussian prior, and a
logistic factor corresponding to the latest performance. To ensure
robustness (see Section 5.2), 𝜇𝑡 is computed as the argmax of this
posterior before replacement by its Gaussian approximation. We
call the rating system that takes this approach Elo-MMR(∞).

As the name implies, it turns out to be a special case of Elo-
MMR(𝜌). In the general setting with 𝜌 ∈ [0, ∞), we keep the full
posterior from Equation (5). Since we cannot tractably compute the
effect of a Gaussian diffusion, we seek a heuristic derivation of the
next round’s prior, retaining a form similar to Equation (5) while
satisfying many of the same properties as the intended diffusion.

4.1 Desirable properties of a “pseudodiffusion”
We begin by listing some properties that our skill evolution algo-
rithm, henceforth called a “pseudodiffusion”, should satisfy. The
first two properties are natural:

• Aligned incentives. First and foremost, the pseudodiffusion must
not break the aligned incentives property of our rating system.
That is, a rating-maximizing player should never be motivated
to lose on purpose (Theorem 5.5).

• Rating preservation. The pseudodiffusion must not alter the arg max
of the belief density. That is, the rating of a player should not
change: 𝜇𝜋

𝑡 = 𝜇𝑡 −1.

In addition, we borrow four properties of Gaussian diffusions:
• Correct magnitude. Pseudodiffusion with parameter 𝛾 2 must in-
crease the skill uncertainty, as measured by Equation (8), by 𝛾 2.
• Composability. Two pseudodiffusions applied in sequence, first
2 , must have the same effect
1 + 𝛾 2
2 .

with parameter 𝛾 2
as a single pseudodiffusion with parameter 𝛾 2

1 and then with 𝛾 2

• Zero diffusion. In the limit as 𝛾 → 0, the effect of pseudodiffusion

must vanish, i.e., not alter the belief distribution.

• Zero uncertainty. In the limit as 𝜎𝑡 −1 → 0 (i.e., when the previous
rating 𝜇𝑡 −1 is a perfect estimate of 𝑆𝑡 −1), our belief on 𝑆𝑡 must
become Gaussian with mean 𝜇𝑡 −1 and variance 𝛾 2. Finer-grained
information regarding the prior history 𝑃 ≤𝑡 must be erased.
In particular, Elo-MMR(∞) fails the zero diffusion property because
it simplifies the belief distribution, even when 𝛾 = 0. In the proof of
Theorem 4.1, we’ll see that Elo-MMR(0) fails the zero uncertainty
property. Thus, it is in fact necessary to have 𝜌 strictly positive and
finite. In Section 5.2, we’ll come to interpret 𝜌 as a kind of inverse
momentum.

4.2 A heuristic pseudodiffusion algorithm
Each factor in the posterior (see Equation (5)) has a parameter 𝛽𝑘 .
Define a factor’s weight to be 𝑤𝑘 := 1/𝛽2
, which by Equation (8)
𝑘
contributes to the total weight (cid:205)𝑘 𝑤𝑘 = 1/𝜎2
𝑡 . Here, unlike in
Equation (7), 𝑤𝑘 does not depend on |𝜇𝑡 − 𝑝𝑘 |.

The approximation step of Elo-MMR(∞) replaces all the logistic
factors by a single Gaussian whose variance is chosen to ensure
that the total weight is preserved. In addition, its mean is chosen
to preserve the player’s rating, given by the unique zero of Equa-
tion (6). Finally, the diffusion step of Elo-MMR(∞) increases the
Gaussian’s variance, and hence the player’s skill uncertainty, by 𝛾 2
𝑡 ;
this corresponds to a decay in the weight.

To generalize the idea, we interleave the two steps in a contin-
uous manner. The approximation step becomes a transfer step:
rather than replace the logistic factors outright, we take away the
same fraction from each of their weights, and place the sum of
removed weights onto a new Gaussian factor. The diffusion step
becomes a decay step, reducing each factor’s weight by the same
fraction, chosen such that the overall uncertainty is increased by
𝛾 2
𝑡 .

To make the idea precise, we generalize the posterior from Equa-
tion (5) with fractional multiplicities 𝜔𝑘 , initially set to 1 for each
𝑘 ∈ {0} ∪ H𝑡 . The 𝑘’th factor is raised to the power 𝜔𝑘 ; in Equa-
tions (6) and (8), the corresponding term is multiplied by 𝜔𝑘 . In

Algorithm 1 Elo-MMR(𝜌, 𝛽, 𝛾)

for all rounds 𝑡 do

for all players 𝑖 ∈ P𝑡 in parallel do

if 𝑖 has never competed before then
𝜇𝑖, 𝜎𝑖 ← 𝜇𝑛𝑒𝑤𝑐𝑜𝑚𝑒𝑟 , 𝜎𝑛𝑒𝑤𝑐𝑜𝑚𝑒𝑟
𝑝𝑖, 𝑤𝑖 ← [𝜇𝑖 ], [1/𝜎2
𝑖 ]

diffuse(𝑖, 𝛾, 𝜌)
√︃
𝜇𝜋
𝑖 , 𝛿𝑖 ← 𝜇𝑖,

𝜎2
𝑖 + 𝛽2

for all 𝑖 ∈ P𝑡 in parallel do

update(𝑖, 𝐸𝑡 , 𝛽)
Algorithm 2 diffuse(𝑖, 𝛾, 𝜌)

𝜅 ← (1 + 𝛾 2/𝜎2
𝑖 )−1
𝑤𝐺, 𝑤𝐿 ← 𝜅𝜌𝑤𝑖,0, (1 − 𝜅𝜌 ) (cid:205)𝑘 𝑤𝑖,𝑘
𝑝𝑖,0 ← (𝑤𝐺 𝑝𝑖,0 + 𝑤𝐿𝜇𝑖 )/(𝑤𝐺 + 𝑤𝐿)
𝑤𝑖,0 ← 𝜅 (𝑤𝐺 + 𝑤𝐿)
for all 𝑘 ≠ 0 do

𝑤𝑖,𝑘 ← 𝜅1+𝜌𝑤𝑖,𝑘
√
𝜅

𝜎𝑖 ← 𝜎𝑖 /

Algorithm 3 update(𝑖, 𝐸, 𝛽)
(cid:18)

(cid:18)

𝑝 ← zero

(cid:205)𝑗 ⪯𝑖

1
𝛿 𝑗

tanh

𝑥−𝜇𝜋
𝑗
2 ¯𝛿 𝑗

(cid:19)

− 1

+ (cid:205)𝑗 ⪰𝑖

1
𝛿 𝑗

(cid:18)

tanh

𝑥−𝜇𝜋
𝑗
2 ¯𝛿 𝑗

(cid:19)(cid:19)

+ 1

𝑝𝑖 .push(𝑝)
𝑤𝑖 .push(1/𝛽2)
𝜇𝑖 ← zero (cid:16)

𝑤𝑖,0 (𝑥 − 𝑝𝑖,0) + (cid:205)𝑘≠0

𝑤𝑖,𝑘 𝛽 2
¯𝛽

tanh 𝑥−𝑝𝑖,𝑘
2 ¯𝛽

(cid:17)

other words, the latter equation is replaced by

1
𝜎2
𝑡

:=

∑︁

𝑘 ∈ {0}∪H𝑡

𝑤𝑘, where 𝑤𝑘 :=

.

𝜔𝑘
𝛽2
𝑘

For 𝜌 ∈ [0, ∞], the Elo-MMR(𝜌) algorithm continuously and
simultaneously performs transfer and decay, with transfer proceed-
ing at 𝜌 times the rate of decay. Holding 𝛽𝑘 fixed, changes to 𝜔𝑘
can be described in terms of changes to 𝑤𝑘 :

(cid:164)𝑤0 = −𝑟 (𝑡)𝑤0 + 𝜌𝑟 (𝑡)

∑︁

𝑤𝑘,

𝑘 ∈H𝑡

(cid:164)𝑤𝑘 = −(1 + 𝜌)𝑟 (𝑡)𝑤𝑘

for 𝑘 ∈ H𝑡 ,

where the arbitrary decay rate 𝑟 (𝑡) can be eliminated by a change
of variable d𝜏 = 𝑟 (𝑡)d𝑡. After some time Δ𝜏, the total weight will
have decayed by a factor 𝜅 := 𝑒−Δ𝜏 , resulting in the new weights:

𝑤𝑛𝑒𝑤
0

= 𝜅𝑤0 +

𝑤𝑛𝑒𝑤
𝑘

= 𝜅1+𝜌𝑤𝑘

𝜅 − 𝜅1+𝜌 (cid:17) ∑︁
(cid:16)
𝑘 ∈H𝑡
for 𝑘 ∈ H𝑡 .

𝑤𝑘,

In order for the uncertainty to increase from 𝜎2
must solve 𝜅/𝜎2

𝑡 −1 to 𝜎2
𝑡 ) for the decay factor:

𝑡 −1 = 1/(𝜎2

𝑡 −1 + 𝛾 2

𝑡 −1 + 𝛾 2

𝑡 , we

(cid:32)

𝜅 =

1 +

(cid:33)−1

.

𝛾 2
𝑡
𝜎2
𝑡 −1

In order for this operation to preserve ratings, the transferred
weight must be centered at 𝜇𝑡 −1; see Algorithm 2 for details.

6

Algorithm 1 details the full Elo-MMR(𝜌) rating system. The main
loop runs whenever a round of competition takes place. First, new
players are initialized with a Gaussian prior. Then, changes in player
skill are modeled by Algorithm 2. Given the round rankings 𝐸𝑡 , the
first phase of Algorithm 3 solves an equation to estimate 𝑃𝑡 . Finally,
the second phase solves another equation for the rating 𝜇𝑡 .

The hyperparameters 𝜌, 𝛽, 𝛾 are domain-dependent, and can be
set by standard hyperparameter search techniques. For convenience,
we assume 𝛽 and 𝛾 are fixed and use the shorthand ¯𝛽𝑘 :=

√
3
𝜋 𝛽𝑘 .

Theorem 4.1. Algorithm 2 with 𝜌 ∈ (0, ∞) meets all of the prop-

erties listed in Section 4.1.

Proof. We go through each of the six properties in order.
• Aligned incentives. This property will be stated in Theorem 5.5.
To ensure that its proof carries through, the relevant facts to
note here are that the pseudodiffusion algorithm ignores the
performances 𝑝𝑘 , and centers the transferred Gaussian weight at
the rating 𝜇𝑡 −1, which is trivially monotonic in 𝜇𝑡 −1.

• Rating preservation. Recall that the rating is the unique zero of 𝐿′,
defined in Equation (6). To see that this zero is preserved, note
that the decay and transfer operations multiply 𝐿′ by constants
(𝜅 and 𝜅𝜌 , respectively), before adding the new Gaussian term,
whose contribution to 𝐿′ is zero at its center.

• Correct magnitude. Follows from our derivation for 𝜅.
• Composability. Follows from correct magnitude and the fact that
every pseudodiffusion follows the same differential equations.
• Zero diffusion. As 𝛾 → 0, 𝜅 → 1. Provided that 𝜌 < ∞, we also

have 𝜅𝜌 → 1. Hence, for all 𝑘 ∈ {0} ∪ H𝑡 , 𝑤𝑛𝑒𝑤

𝑘 → 𝑤𝑘 .

• Zero uncertainty. As 𝜎𝑡 −1 → 0, 𝜅 → 0. The total weight decays
𝑡 −1 to 𝛾 2. Provided that 𝜌 > 0, we also have 𝜅𝜌 → 0, so
from 1/𝜎2
these weights transfer in their entirety, leaving behind a Gaussian
with mean 𝜇𝑡 −1, variance 𝛾 2, and no additional history.
□

5 THEORETICAL PROPERTIES
In this section, we see how the simplicity of the Elo-MMR formu-
las enables us to rigorously prove that the rating system aligns
incentives, is robust, and is computationally efficient.

5.1 Aligned incentives
To demonstrate the need for aligned incentives, let’s look at the con-
sequences of violating this property in the TopCoder and Glicko-2
rating systems. These systems track a “volatility” for each player,
which estimates the variance of their performances. A player whose
recent performance history is more consistent would be assigned a
lower volatility score, than one with wild swings in performance.
The volatility acts as a multiplier on rating changes; thus, play-
ers with an extremely low or high performance will have their
subsequent rating changes amplified.

While it may seem like a good idea to boost changes for players
whose ratings are poor predictors of their performance, this fea-
ture has an exploit. By intentionally performing at a weaker level, a
player can amplify future increases to an extent that more than com-
pensates for the immediate hit to their rating. A player may even
“farm” volatility by alternating between very strong and very weak
performances. After acquiring a sufficiently high volatility score,
the strategic player exerts their honest maximum performance over

7

a series of contests. The amplification eventually results in a rating
that exceeds what would have been obtained via honest play. This
type of exploit was discovered in both TopCoder competitions and
the Pokemon Go video game [3, 14]. For a detailed example, see
Table 5.3 of [14].

Remarkably, Elo-MMR combines the best of both worlds: we’ll
see in Section 5.2 that, for 𝜌 ∈ (0, ∞), Elo-MMR(𝜌) also boosts
changes to inconsistent players. And yet, as we’ll prove in this sec-
tion, no such strategic incentive exists in any version of Elo-MMR.
Recall that, for the purposes of the algorithm, the performance 𝑝𝑖
is defined to be the unique zero of the function 𝑄𝑖 (𝑝) := (cid:205)𝑗 ≻𝑖 𝑙 𝑗 (𝑝)+
(cid:205)𝑗∼𝑖 𝑑 𝑗 (𝑝) + (cid:205)𝑗 ≺𝑖 𝑣 𝑗 (𝑝), whose terms 𝑙𝑖, 𝑑𝑖, 𝑣𝑖 are contributed by
opponents against whom 𝑖 lost, drew, or won, respectively. Wins
(losses) are always positive (negative) contributions to a player’s
performance score:

Lemma 5.1. Adding a win term to 𝑄𝑖 (·), or replacing a tie term by
a win term, always increases its zero. Conversely, adding a loss term,
or replacing a tie term by a loss term, always decreases it.

Proof. By Lemma 3.1, 𝑄𝑖 (𝑝) is decreasing in 𝑝. Thus, adding a
positive term will increase its zero whereas adding a negative term
will decrease it. The desired conclusion follows by noting that, for
all 𝑗 and 𝑝, 𝑣 𝑗 (𝑝) and 𝑣 𝑗 (𝑝) − 𝑑 𝑗 (𝑝) are positive, whereas 𝑙 𝑗 (𝑝) and
□
𝑙 𝑗 (𝑝) − 𝑑 𝑗 (𝑝) are negative.

While not needed for our main result, a similar argument shows
that performance scores are monotonic across the round standings:

Theorem 5.2. If 𝑖 ≻ 𝑗 (that is, player 𝑖 beats 𝑗) in a given round,

then player 𝑖 and 𝑗’s performance estimates satisfy 𝑝𝑖 > 𝑝 𝑗 .

Proof. If 𝑖 ≻ 𝑗 with 𝑖, 𝑗 adjacent in the rankings, then

𝑄𝑖 (𝑝) − 𝑄 𝑗 (𝑝) =

(𝑑𝑘 (𝑝) − 𝑙𝑘 (𝑝)) +

∑︁

𝑘∼𝑖

∑︁

𝑘∼𝑗

(𝑣𝑘 (𝑝) − 𝑑𝑘 (𝑝)) > 0.

for all 𝑝. Since 𝑄𝑖 and 𝑄 𝑗 are decreasing functions, it follows that
𝑝𝑖 > 𝑝 𝑗 . By induction, this result extends to the case where 𝑖, 𝑗 are
□
not adjacent in the rankings.

What matters for incentives is that performance scores be coun-
terfactually monotonic; meaning, if we were to alter the round
standings, a strategic player will always prefer to place higher:

Lemma 5.3. In any given round, holding fixed the relative ranking
of all players other than 𝑖 (and holding fixed all preceding rounds),
the performance 𝑝𝑖 is a monotonic function of player i’s prior rating
and of player 𝑖’s rank in this round.

Proof. Monotonicity in the rating follows directly from mono-
tonicity of the self-tie term 𝑑𝑖 in 𝑄𝑖 . Since an upward shift in the
rankings can only convert losses to ties to wins, monotonicity in
□
contest rank follows from Lemma 5.1.

Having established the relationship between round rankings
and performance scores, the next step is to prove that, even with
hindsight, players will always prefer their performance scores to
be as high as possible:

Lemma 5.4. Holding fixed the set of contest rounds in which a
player has participated, their current rating is monotonic in each of
their past performance scores.

Proof. The player’s rating is given by the zero of 𝐿′ in Equa-
tion (6). The pseudodiffusions of Section 4 modify each of the 𝛽𝑘 in
a manner that does not depend on any of the 𝑝𝑘 , so they are fixed
for our purposes. Hence, 𝐿′ is monotonically increasing in 𝑠 and
decreasing in each of the 𝑝𝑘 . Therefore, its zero is monotonically
increasing in each of the 𝑝𝑘 .

This is almost what we wanted to prove, except that 𝑝0 is not
a performance. Nonetheless, it is a function of the performances:
specifically, a weighted average of historical ratings which, using
this same lemma as an inductive hypothesis, are themselves mono-
tonic in past performances. By induction, the proof is complete. □

Finally, we conclude that the player’s incentives are aligned with

optimizing round rankings, or raw scores:

Theorem 5.5 (Aligned Incentives). Holding fixed the set of
contest rounds in which each player has participated, and the historical
ratings and relative rankings of all players other than 𝑖, player 𝑖’s
current rating is monotonic in each of their past rankings.

Proof. Choose any contest round in player 𝑖’s history, and con-
sider improving player 𝑖’s rank in that round while holding every-
thing else fixed. It suffices to show that player 𝑖’s current rating
would necessarily increase as a result.

In the altered round, by Lemma 5.3, 𝑝𝑖 is increased; and by
Lemma 5.4, player 𝑖’s post-round rating is increased. By Lemma 5.3
again, this increases player 𝑖’s performance score in the following
round. Proceeding inductively, we find that performance scores and
□
ratings from this point onward are all increased.

In the special cases of Elo-MM𝜒 or Elo-MMR(∞), the rating sys-
tem is “memoryless”: the only data retained for each player are the
current rating 𝜇𝑖,𝑡 and uncertainty 𝜎𝑖,𝑡 ; detailed performance his-
tory is not saved. In this setting, we present a natural monotonicity
theorem. A similar theorem was stated for the Codeforces system
in [2], but no proofs were given.

Theorem 5.6 (Memoryless Monotonicity Theorem). In ei-
ther the Elo-MM𝜒 or Elo-MMR(∞) system, suppose 𝑖 and 𝑗 are two
participants of round 𝑡. Suppose that the ratings and corresponding un-
certainties satisfy 𝜇𝑖,𝑡 −1 ≥ 𝜇 𝑗,𝑡 −1, 𝜎𝑖,𝑡 −1 = 𝜎 𝑗,𝑡 −1. Then, 𝜎𝑖,𝑡 = 𝜎 𝑗,𝑡 .
Furthermore, if 𝑖 ≻ 𝑗 in round 𝑡, then 𝜇𝑖,𝑡 > 𝜇 𝑗,𝑡 . On the other hand,
if 𝑗 ≻ 𝑖 in round 𝑡, then 𝜇 𝑗,𝑡 − 𝜇 𝑗,𝑡 −1 > 𝜇𝑖,𝑡 − 𝜇𝑖,𝑡 −1.

Proof. The new contest round will add a rating perturbation
𝑡 , followed by a new performance with variance 𝛽2
𝑡 .

with variance 𝛾 2
As a result,

(cid:32)

𝜎𝑖,𝑡 =

1
𝑖,𝑡 −1 + 𝛾 2
𝜎2

𝑡

+

1
𝛽2
𝑡

(cid:33)− 1

2

(cid:32)

=

1
𝑗,𝑡 −1 + 𝛾 2
𝜎2

𝑡

+

1
𝛽2
𝑡

(cid:33)− 1

2

= 𝜎 𝑗,𝑡 .

The remaining conclusions are consequences of three properties:
memorylessness, aligned incentives (Theorem 5.5), and translation-
invariance (ratings, skills, and performances are quantified on a
common interval scale relative to one another).

Since the Elo-MM𝜒 or Elo-MMR(∞) systems are memoryless, we
may replace the initial prior and performance histories of players
with any alternate histories of our choosing, as long as our choice is
compatible with their current rating and uncertainty. For example,
both 𝑖 and 𝑗 can be considered to have participated in the same

set of rounds, with 𝑖 always performing at 𝜇𝑖,𝑡 −1. and 𝑗 always
performing at 𝜇 𝑗,𝑡 −1. Round 𝑡 is unchanged.

Suppose 𝑖 ≻ 𝑗. Since 𝑖’s historical performances are all equal or

stronger than 𝑗’s, Theorem 5.5 implies 𝜇𝑖,𝑡 > 𝜇 𝑗,𝑡 .

Suppose 𝑗 ≻ 𝑖. By translation-invariance, if we shift each of
𝑗’s performances, up to round 𝑡 and including the initial prior,
upward by 𝜇𝑖,𝑡 −1 − 𝜇 𝑗,𝑡 −1, the rating changes between rounds will
be unaffected. Players 𝑖 and 𝑗 now have identical histories, except
that we still have 𝑗 ≻ 𝑖 at round 𝑡. Therefore, 𝜇 𝑗,𝑡 −1 = 𝜇𝑖,𝑡 −1 and,
by Theorem 5.5, 𝜇 𝑗,𝑡 > 𝜇𝑖,𝑡 . Subtracting the equation from the
□
inequality proves the second conclusion.

5.2 Robust response
Another desirable property in many settings is robustness: a player’s
rating should not change too much in response to any one con-
test, no matter how extreme their performance. The Codeforces
and TrueSkill systems lack this property, allowing for unbounded
rating changes. TopCoder achieves robustness by clamping any
changes that exceed a cap, which is initially high for new players
but decreases with experience.

When 𝜌 > 0, Elo-MMR(𝜌) achieves robustness in a natural,
smoother manner. It comes out of the interplay between Gaussian
and logistic factors in the posterior; 𝜌 > 0 ensures that the Gaussian
contribution doesn’t vanish. Recall the notation used to describe
the general posterior in Equations (5) and (6), enhanced with the
fractional multiplicities 𝜔𝑘 from Section 4.2.

Theorem 5.7. In the Elo-MMR(𝜌) rating system, let

Δ+ := lim

𝑝𝑡 →+∞

𝜇𝑡 − 𝜇𝑡 −1, Δ− := lim

𝑝𝑡 →−∞

𝜇𝑡 −1 − 𝜇𝑡 .

Then,

𝜋
√
3
𝛽𝑡

𝜋 2
6

+

1
𝛽2
0

∑︁

𝑘 ∈H𝑡 −1

𝜔𝑘
𝛽2
𝑘

(cid:169)
(cid:173)
(cid:171)

−1

𝜋 𝛽2
0
√
3

.

≤ Δ± ≤

(cid:170)
(cid:174)
(cid:172)
tanh(𝑥) ≤ 1, differentiating

𝛽𝑡

Proof. Using the fact that 0 < 𝑑
𝑑𝑥

Equation (6) yields

1
𝛽2
0

≤ 𝐿′′(𝑠) ≤

𝜋 2
6

+

1
𝛽2
0

∑︁

𝑘 ∈H𝑡 −1

.

𝜔𝑘
𝛽2
𝑘

For every 𝑠 ∈ R, in the limit as 𝑝𝑡 → ±∞, the new term cor-
responding to the performance at round 𝑡 will increase 𝐿′(𝑠) by
∓ 𝜋
. Since 𝜇𝑡 −1 was a zero of 𝐿′ without this new term, we now
√
𝛽𝑡
have 𝐿′(𝜇𝑡 −1) → ∓ 𝜋
. Dividing by the former inequalities yields
√
𝛽𝑡
the desired result.

□

3

3

The proof reveals that the magnitude of Δ± depends inversely
on that of 𝐿′′ in the vicinity of the current rating, which in turn
is related to the derivative of the tanh terms. If a player’s perfor-
mances vary wildly, then most of the tanh terms will be in their tails,
which contribute small derivatives, enabling larger rating changes.
Conversely, the tanh terms of a player with a very consistent rat-
ing history will contribute large derivatives, so the bound on their
rating change will be small.

Thus, Elo-MMR(𝜌) naturally caps the rating change of all players,
and puts a smaller cap on the rating change of consistent players.
The cap will increase after an extreme performance, providing a

8

similar “momentum” to the TopCoder and Glicko-2 systems, but
without sacrificing aligned incentives (Theorem 5.5).

By comparing against Equation (8), we see that the lower bound
in Theorem 5.7 is on the order of 𝜎2
𝑡 /𝛽𝑡 , while the upper bound is
on the order of 𝛽2
0/𝛽𝑡 . As a result, the momentum effect is more
pronounced when 𝛽0 is much larger than 𝜎𝑡 . Since the decay step
increases 𝛽0 while the transfer step decreases it, this occurs when
the transfer rate 𝜌 is comparatively small. Thus, 𝜌 can be chosen in
inverse proportion to the desired strength of momentum.

5.3 Runtime analysis and optimizations
Let’s look at the computation time needed to process a round with
participant set P, where we again omit the round subscript. Each
player 𝑖 has a participation history H𝑖 .

Estimating 𝑃𝑖 entails finding the zero of a monotonic function
with 𝑂 (|P |) terms, and then obtaining the rating 𝜇𝑖 entails finding
the zero of another monotonic function with 𝑂 (|H𝑖 |) terms. Using
the Illinois or Newton methods, solving these equations to precision
𝜖 takes 𝑂 (log log 1
𝜖 ) iterations. As a result, the total runtime needed
to process one round of competition is

𝑂

(cid:32)

∑︁

𝑖 ∈ P

(|P | + |H𝑖 |) log log

(cid:33)

.

1
𝜖

This complexity is more than adequate for Codeforces-style com-
petitions with thousands of contestants and history lengths up to a
few hundred. Indeed, we were able to process the entire history of
Codeforces on a small laptop in less than half an hour. Nonetheless,
it may be cost-prohibitive in truly massive settings, where |P | or
|H𝑖 | number in the millions. Fortunately, it turns out that both
functions may be compressed down to a bounded number of terms,
with negligible loss of precision.

Adaptive subsampling. In Section 2, we used Doob’s consistency
theorem to argue that our estimate for 𝑃𝑖 is consistent. Specifically,
we saw that 𝑂 (1/𝜖2) opponents are needed to get the typical error
below 𝜖. Thus, we can subsample the set of opponents to include in
the estimation, omitting the rest. Random sampling is one approach.
A more efficient approach chooses a fixed number of opponents
whose ratings are closest to that of player 𝑖, as these are more likely
to provide informative match-ups. On the other hand, if the setting
requires aligned incentives to hold exactly, then one must avoid
choosing different opponents for each player.

History compression. Similarly, it’s possible to bound the number
of stored factors in the posterior. Our skill-evolution algorithm
decays the weights of old performances at an exponential rate.
Thus, the contributions of all but the most recent 𝑂 (log 1
𝜖 ) terms
are negligible. Rather than erase the older logistic terms outright, we
recommend replacing them with moment-matched Gaussian terms,
similar to the transfers in Section 4 with 𝜅 = 0. Since Gaussians
compose easily, a single term can then summarize an arbitrarily
long prefix of the history.

Substituting 1/𝜖2 and log 1

𝜖 for |P | and |H𝑖 |, respectively, the

runtime of Elo-MMR with both optimizations becomes

𝑂

(cid:18) |P |
𝜖2

log log

(cid:19)

.

1
𝜖

9

Dataset
Codeforces
TopCoder
Reddit
Synthetic

# contests
1087
2023
1000
50

avg. # participants / contest
2999
403
20
2500

Table 1: Summary of test datasets.

Finally, we note that the algorithm is embarrassingly parallel,
with each player able to solve its equations independently. The
threads can read the same global data structures, so each additional
thread only contributes 𝑂 (1) memory overhead.

6 EXPERIMENTS
In this section, we compare various rating systems on real-world
datasets, mined from several sources that will be described in Sec-
tion 6.1. The metrics are runtime and predictive accuracy, as de-
scribed in Section 6.2.

We compare Elo-MM𝜒 and Elo-MMR(𝜌) against the industry-
tested rating systems of Codeforces and TopCoder. For a fairer
comparison, we hand-coded efficient versions of all four algorithms
in the safe subset of Rust, parellelized using the Rayon crate; as
such, the Rust compiler verifies that they contain no data races [27].
Our implementation of Elo-MMR(𝜌) makes use of the optimizations
in Section 5.3, bounding both the number of sampled opponents
and the history length by 500. In addition, we test the improved
TrueSkill algorithm of [25], basing our code on an open-source
implementation of the same algorithm. The inherent seqentiality of
its message-passing procedure prevented us from parallelizing it.

Hyperparameter search. To ensure fair comparisons, we ran a
separate grid search for each triple of algorithm, dataset, and metric,
over all of the algorithm’s hyperparameters. The hyperparameter
set that performed best on the first 10% of the dataset, was then
used to test the algorithm on the remaining 90% of the dataset.

The experiments were run on a 2.0 GHz 24-core Skylake ma-
chine with 24 GB of memory. Implementations of all rating systems,
hyperparameters, datasets, and additional processing used in our
experiments can be found at https://github.com/EbTech/EloR/.

6.1 Datasets
Due to the scarcity of public domain datasets for rating systems,
we mined three datasets to analyze the effectiveness of our system.
The datasets were mined using data from each source website’s
inception up to October 12th, 2020. We also created a synthetic
dataset to test our system’s performance when the data generating
process matches our theoretical model. Summary statistics of the
datasets are presented in Table 1.

Codeforces contest history. This dataset contains the current en-
tire history of rated contests ever run on CodeForces.com, the dom-
inant platform for online programming competitions. The Code-
Forces platform has over 850K users, over 300K of whom are rated,
and has hosted over 1000 contests to date. Each contest has a couple
thousand competitors on average. A typical contest takes 2 to 3
hours and contains 5 to 8 problems. Players are ranked by total
points, with more points typically awarded for tougher problems
and for early solves. They may also attempt to “hack” one another’s

submissions for bonus points, identifying test cases that break their
solutions.

TopCoder contest history. This dataset contains the current en-
tire history of algorithm contests ever run on the TopCoder.com.
TopCoder is a predecessor to Codeforces, with over 1.4 million
total users and a long history as a pioneering platform for program-
ming contests. It hosts a variety of contest types, including over
2000 algorithm contests to date. The scoring system is similar to
Codeforces, but its rounds are shorter: typically 75 minutes with 3
problems.

SubRedditSimulator threads. This dataset contains data scraped
from the current top-1000 most upvoted threads on the website
reddit.com/r/SubredditSimulator/. Reddit is a social news aggrega-
tion website with over 300 million users. The site itself is broken
down into sub-sites called subreddits. Users then post and comment
to the subreddits, where the posts and comments receive votes from
other users. In the subreddit SubredditSimulator, users are language
generation bots trained on text from other subreddits. Automated
posts are made by these bots to SubredditSimulator every 3 minutes,
and real users of Reddit vote on the best bot. Each post (and its asso-
ciated comments) can thus be interpreted as a round of competition
between the bots who commented.

Synthetic data. This dataset contains 10K players, with skills
and performances generated according to the Gaussian generative
model in Section 2. Players’ initial skills are drawn i.i.d. with mean
1500 and variance 300. Players compete in all rounds, and are ranked
according to independent performances with variance 200. Between
rounds, we add i.i.d. Gaussian increments with variance 35 to each
of their skills.

6.2 Evaluation metrics
To compare the different algorithms, we define two measures of
predictive accuracy. Each metric will be defined on individual con-
testants in each round, and then averaged:

aggregate(metric) :=

(cid:205)𝑡 (cid:205)𝑖 ∈ P𝑡 metric(𝑖, 𝑡)
(cid:205)𝑡 |P𝑡 |

.

Pair inversion metric [20]. Our first metric computes the fraction
of opponents against whom our ratings predict the correct pairwise
result, defined as the higher-rated player either winning or tying:

pair_inversion(𝑖, 𝑡) :=

# correctly predicted matchups
|P𝑡 | − 1

× 100%.

This metric was used in the evaluation of TrueSkill [20].

Rank deviation. Our second metric compares the rankings with
the total ordering that would be obtained by sorting players accord-
ing to their prior rating. The penalty is proportional to how much
these ranks differ for player 𝑖:

rank_deviation(𝑖, 𝑡) :=

|actual_rank − predicted_rank|
|P𝑡 | − 1

× 100%.

In the event of ties, among the ranks within the tied range, we use
the one that comes closest to the rating-based prediction.

6.3 Empirical results
Recall that Elo-MM𝜒 has a Gaussian performance model, matching
the modeling assumptions of TopCoder and TrueSkill. Elo-MMR(𝜌),
on the other hand, has a logistic performance model, matching
the modeling assumptions of Codeforces and Glicko. While 𝜌 was
included in the hyperparameter search, in practice we found that
all values between 0 and 1 produce very similar results.

To ensure that errors due to the unknown skills of new players
don’t dominate our metrics, we excluded players who had competed
in less than 5 total contests. In most of the datasets, this reduced the
performance of our method relative to the others, as our method
seems to converge more accurately. Despite this, we see in Table 2
that both versions of Elo-MMR outperform the other rating systems
in both the pairwise inversion metric and the ranking deviation
metric.

We highlight a few key observations. First, significant perfor-
mance gains are observed on the Codeforces and TopCoder datasets,
despite these platforms’ rating systems having been designed specif-
ically for their needs. Our gains are smallest on the synthetic dataset,
for which all algorithms perform similarly. This might be in part
due to the close correspondence between the generative process
and the assumptions of these rating systems. Furthermore, the
synthetic players compete in all rounds, enabling the system to
converge to near-optimal ratings for every player. Finally, the im-
proved TrueSkill performed well below our expectations, despite
our best efforts to improve it; we suspect that the message-passing
algorithm breaks down in contests with a large number of distinct
ranks. To our knowledge, we are the first to present experiments
with TrueSkill on contests where the number of distinct ranks is in
the hundreds or thousands. In preliminary experiments, TrueSkill
and Elo-MMR score about equally when the number of ranks is less
than about 60.

Now, we turn our attention to Table 3, which showcases the com-
putational efficiency of Elo-MMR. On smaller datasets, it performs
comparably to the Codeforces and TopCoder algorithms. However,
the latter suffer from a quadratic time dependency on the number
of contestants; as a result, Elo-MMR outperforms them by almost
an order of magnitude on the larger Codeforces dataset.

Finally, in comparisons between the two Elo-MMR variants, we
note that while Elo-MMR(𝜌) is more accurate, Elo-MM𝜒 is always
faster. This has to do with the skill drift modeling described in
Section 4, as every update in Elo-MMR(𝜌) must process 𝑂 (log 1
𝜖 )
terms of a player’s competition history.

7 CONCLUSIONS
This paper introduces the Elo-MMR rating system, which is in
part a generalization of the two-player Glicko system, allowing an
unbounded number of players. By developing a Bayesian model
and taking the limit as the number of participants goes to infinity,
we obtained simple, human-interpretable rating update formulas.
Furthermore, we saw that the algorithm is asymptotically fast, em-
barrassingly parallel, robust to extreme performances, and satisfies
the important aligned incentives property. To our knowledge, our
system is the first to rigorously prove all these properties in a set-
ting with more than two individually ranked players. In terms of

10

Dataset

Codeforces
TopCoder
Reddit
Synthetic

Codeforces
pair inv.
78.3%
72.6%
61.5%
81.7%

rank dev.
14.9%
18.5%
27.3%
12.9%

TopCoder
pair inv.
78.5%
72.3%
61.4%
81.7%

rank dev.
15.1%
18.7%
27.4%
12.8%

TrueSkill
pair inv.
61.7%
68.7%
61.5%
81.3%

rank dev.
25.4%
20.9%
27.2%
13.1%

Elo-MM𝝌
pair inv.
78.5%
73.0%
61.6%
81.7%

rank dev.
14.8%
18.3%
27.3%
12.8%

Elo-MMR(𝝆)
pair inv.
78.6%
73.1%
61.6%
81.7%

rank dev.
14.7%
18.2%
27.3%
12.8%

Table 2: Performance of each rating system on the pairwise inversion and rank deviation metrics. Bolded entries denote the
best performances (highest pair inv. or lowest rank dev.) on each metric and dataset.

Dataset
Codeforces
TopCoder
Reddit
Synthetic

CF
212.9
9.60
1.19
3.26

TC
72.5
4.25
1.14
1.00

TS
67.2
16.8
0.44
2.93

Elo-MM𝝌
31.4
7.00
1.14
0.81

Elo-MMR(𝝆)
35.4
7.52
1.42
0.85

Table 3: Total compute time over entire dataset, in seconds.

practical performance, we saw that it outperforms existing industry
systems in both prediction accuracy and computation speed.

This work can be extended in several directions. First, the choices
we made in modeling ties, pseudodiffusions, and opponent subsam-
pling are by no means the only possibilities consistent with our
Bayesian model of skills and performances. Second, one may obtain
better results by fitting the performance and skill evolution models
to application-specific data.

Another useful extension would be to team competitions. While
it’s no longer straightforward to infer precise estimates of an indi-
vidual’s performance, Elo-MM𝜒 can simply be applied at the team
level. To make this useful in settings where players may form new
teams in each round, we must model teams in terms of their individ-
ual members. In the case where a team’s performance is modeled
as the sum of its members’ independent Gaussian contributions,
elementary facts about multivariate Gaussian distributions enable
posterior skill inferences at the individual level. Generalizing this
approach remains an open challenge.

Over the past decade, online competition communities such as
Codeforces have grown exponentially. As such, considerable work
has gone into engineering scalable and reliable rating systems.
Unfortunately, many of these systems have not been rigorously
analyzed in the academic community. We hope that our paper and
open-source release will open new explorations in this area.

ACKNOWLEDGEMENTS
The authors are indebted to Daniel Sleator and Danica J. Sutherland
for initial discussions that helped inspire this work, and to Nikita
Gaevoy for the open-source improved TrueSkill upon which our
implementation is based. Experiments in this paper were funded by
a Google Cloud Research Grant. The second author is supported by
a VMWare Fellowship and the Natural Sciences and Engineering
Research Council of Canada.

APPENDIX

Lemma 3.1. If 𝑓𝑖 is continuously differentiable and log-concave,

then the functions 𝑙𝑖, 𝑑𝑖, 𝑣𝑖 are continuous, strictly decreasing, and

𝑙𝑖 (𝑝) < 𝑑𝑖 (𝑝) < 𝑣𝑖 (𝑝) for all 𝑝.

11

Proof. Continuity of 𝐹𝑖, 𝑓𝑖, 𝑓 ′

𝑖 implies that of 𝑙𝑖, 𝑑𝑖, 𝑣𝑖 . It’s known [10]

that log-concavity of 𝑓𝑖 implies log-concavity of both 𝐹𝑖 and 1 − 𝐹𝑖 .
As a result, 𝑙𝑖 , 𝑑𝑖 , and 𝑣𝑖 are derivatives of strictly concave functions;
therefore, they are strictly decreasing. In particular, each of

𝑣 ′
𝑖 (𝑝) =

𝑓 ′
𝑖 (𝑝)
𝐹𝑖 (𝑝)

−

𝑓𝑖 (𝑝)2
𝐹𝑖 (𝑝)2

,

𝑙 ′
𝑖 (𝑝) =

−𝑓 ′
𝑖 (𝑝)
1 − 𝐹𝑖 (𝑝)

−

𝑓𝑖 (𝑝)2
(1 − 𝐹𝑖 (𝑝))2

,

are negative for all 𝑝, so we conclude that

𝑑𝑖 (𝑝) − 𝑣𝑖 (𝑝) =

𝑓 ′
𝑖 (𝑝)
𝑓𝑖 (𝑝)

−

𝑓𝑖 (𝑝)
𝐹𝑖 (𝑝)

𝑙𝑖 (𝑝) − 𝑑𝑖 (𝑝) = −

𝑓 ′
𝑖 (𝑝)
𝑓𝑖 (𝑝)

−

𝑓𝑖 (𝑝)
1 − 𝐹𝑖 (𝑝)

=

=

𝐹𝑖 (𝑝)
𝑓𝑖 (𝑝)
1 − 𝐹𝑖 (𝑝)
𝑓𝑖 (𝑝)

𝑣 ′
𝑖 (𝑝) < 0,

𝑙 ′
𝑖 (𝑝) < 0.

□

Theorem 3.2. Suppose that for all 𝑗, 𝑓𝑗 is continuously differen-
tiable and log-concave. Then the unique maximizer of Pr(𝑃𝑖 = 𝑝 |
𝑖 , 𝐸𝑊
𝐸𝐿
𝑖

) is given by the unique zero of

𝑄𝑖 (𝑝) =

𝑙 𝑗 (𝑝) +

∑︁

𝑗 ≻𝑖

∑︁

𝑗∼𝑖

𝑑 𝑗 (𝑝) +

𝑣 𝑗 (𝑝).

∑︁

𝑗 ≺𝑖

Proof. First, we rank the players by their buckets according to

⌊𝑃 𝑗 /𝜖⌋, and take the limiting probabilities as 𝜖 → 0:

Pr(⌊

Pr(⌊

𝑃 𝑗
𝜖

𝑃 𝑗
𝜖

⌋ > ⌊

⌋ < ⌊

1
𝜖

Pr(⌊

𝑃 𝑗
𝜖

⌋ = ⌊

𝑝
𝜖

𝑝
𝜖

𝑝
𝜖

⌋) = Pr(𝑝 𝑗 ≥ 𝜖 ⌊
𝑝
𝜖

= 1 − 𝐹 𝑗 (𝜖 ⌊

⌋) = Pr(𝑝 𝑗 < 𝜖 ⌊

𝑝
𝜖

⌋ + 𝜖)

⌋ + 𝜖) → 1 − 𝐹 𝑗 (𝑝),

𝑝
𝜖

⌋)

= 𝐹 𝑗 (𝜖 ⌊

𝑝
𝜖

⌋) → 𝐹 𝑗 (𝑝),

⌋) =

=

1
𝜖
1
𝜖

𝑝
𝜖

Pr(𝜖 ⌊

(cid:16)

𝐹 𝑗 (𝜖 ⌊

⌋ ≤ 𝑃 𝑗 < 𝜖 ⌊
𝑝
𝜖

⌋ + 𝜖) − 𝐹 𝑗 (𝜖 ⌊

𝑝
𝜖

⌋ + 𝜖)

(cid:17)

⌋)

𝑝
𝜖

→ 𝑓𝑗 (𝑝).

Let 𝐿𝜖
𝑃 𝑗
𝜖 ⌋ < ⌊

𝑝
𝜖 ⌋,
𝑝
𝜖 ⌋. respectively. These correspond to a
⌊
player who performs at 𝑝 losing, winning, and drawing against 𝑗,

𝑗𝑝 be shorthand for the events ⌊

𝑗𝑝 , and 𝐷𝜖
𝑃 𝑗
𝜖 ⌋ = ⌊

𝑗𝑝 , 𝑊 𝜖
𝑝
𝜖 ⌋, and ⌊

𝑃 𝑗
𝜖 ⌋ > ⌊

[16] Mark E Glickman. 1995. A comprehensive guide to chess ratings. American Chess

Journal 3, 1 (1995), 59–102.

[17] Mark E Glickman. 1999. Parameter estimation in large dynamic paired compari-

son experiments. Applied Statistics (1999), 377–394.

[18] Mark E Glickman. 2012. Example of the Glicko-2 system. Boston University

(2012), 1–6.

[19] Linxia Gong, Xiaochuan Feng, Dezhi Ye, Hao Li, Runze Wu, Jianrong Tao,
Changjie Fan, and Peng Cui. 2020. OptMatch: Optimized Matchmaking via
Modeling the High-Order Interactions on the Arena. In KDD 2020. 2300–2310.
[20] Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkillTM: A Bayesian

Skill Rating System. In NeurIPS 2006. 569–576.

[21] Tzu-Kuo Huang, Chih-Jen Lin, and Ruby C. Weng. 2006. Ranking individuals by

group comparisons. In ICML 2006. ACM, 425–432.

[22] Stephanie Kovalchik. 2020. Extension of the Elo rating system to margin of

victory. International Journal of Forecasting (2020).

[23] Yao Li, Minhao Cheng, Kevin Fujii, Fushing Hsieh, and Cho-Jui Hsieh. 2018.
Learning from Group Comparisons: Exploiting Higher Order Interactions. In
NeurIPS 2018. 4986–4995.

[24] Tom Minka, Ryan Cleven, and Yordan Zaykov. 2018. TrueSkill 2: An improved
Bayesian skill rating system. Technical Report MSR-TR-2018-8. Microsoft.
[25] Sergey I. Nikolenko, Alexander, and V. Sirotkin. 2010. Extensions of the TrueSkill
TM rating system. In In Proceedings of the 9th International Conference on Appli-
cations of Fuzzy Systems and Soft Computing. 151–160.

[26] Jerneja Premelč, Goran Vučković, Nic James, and Bojan Leskošek. 2019. Reliability

of judging in DanceSport. Frontiers in psychology 10 (2019), 1001.

[27] Josh Stone and Nicholas D Matsakis. The Rayon library (Rust Crate). crates.io/

crates/rayon

[28] Lin Yang, Stanko Dimitrov, and Benny Mantin. 2014. Forecasting sales of new vir-
tual goods with the Elo rating system. Journal of Revenue and Pricing Management
13, 6 (Dec. 2014), 457–469.

respectively, when outcomes are determined by 𝜖-buckets. Then,
Pr(𝐷𝜖
𝜖

| 𝑃𝑖 = 𝑝) = lim
𝜖→0

Pr(𝐸𝑊
𝑖

Pr(𝑊 𝜖

Pr(𝐿𝜖

, 𝐸𝐿
𝑖

𝑗𝑝 )

𝑗𝑝 )

(cid:214)

(cid:214)

(cid:214)

𝑗𝑝 )

Pr(𝑃𝑖 = 𝑝 | 𝐸𝐿

𝑖 , 𝐸𝑊
𝑖

=

(cid:214)

𝑗 ≻𝑖

𝑗 ≻𝑖
(1 − 𝐹 𝑗 (𝑝))

𝑗 ≺𝑖
(cid:214)

𝑗∼𝑖,𝑗≠𝑖

𝐹 𝑗 (𝑝)

(cid:214)

𝑓𝑗 (𝑝),

) ∝ 𝑓𝑖 (𝑝) Pr(𝐸𝐿

𝑖 , 𝐸𝑊
𝑖
(1 − 𝐹 𝑗 (𝑝))

=

(cid:214)

𝑗 ≻𝑖

𝑗∼𝑖,𝑗≠𝑖

𝑗 ≺𝑖
| 𝑃𝑖 = 𝑝)

(cid:214)

𝐹 𝑗 (𝑝)

𝑓𝑗 (𝑝),

(cid:214)

𝑗∼𝑖

d
d𝑝

ln Pr(𝑃𝑖 = 𝑝 | 𝐸𝐿

𝑖 ,𝐸𝑊
𝑖

) =

𝑙 𝑗 (𝑝) +

∑︁

𝑗 ≻𝑖

𝑗 ≺𝑖

∑︁

𝑗 ≺𝑖

𝑣 𝑗 (𝑝) +

∑︁

𝑗∼𝑖

𝑑 𝑗 (𝑝) = 𝑄𝑖 (𝑝).

Since Lemma 3.1 tells us that 𝑄𝑖 is strictly decreasing, it only
remains to show that it has a zero. If the zero exists, it must be
𝑖 , 𝐸𝑊
unique and it will be the unique maximum of Pr(𝑃𝑖 = 𝑝 | 𝐸𝐿
).
𝑖
To start, we want to prove the existence of 𝑝∗ such that 𝑄𝑖 (𝑝∗) <
0. Note that it’s not possible to have 𝑓 ′
𝑗 (𝑝) ≥ 0 for all 𝑝, as in that
case the density would integrate to either zero or infinity. Thus, for
each 𝑗 such that 𝑗 ∼ 𝑖, we can choose 𝑝 𝑗 such that 𝑓 ′
𝑗 (𝑝 𝑗 ) < 0, and
so 𝑑 𝑗 (𝑝 𝑗 ) < 0. Let 𝛼 = − (cid:205)𝑗∼𝑖 𝑑 𝑗 (𝑝 𝑗 ) > 0.

Let 𝑛 = |{ 𝑗

𝑗 ≺ 𝑖}|. For each 𝑗 such that 𝑗 ≺ 𝑖, since
lim𝑝→∞ 𝑣 𝑗 (𝑝) = 0/1 = 0, we can choose 𝑝 𝑗 such that 𝑣 𝑗 (𝑝 𝑗 ) < 𝛼/𝑛.
Let 𝑝∗ = max𝑗 ⪯𝑖 𝑝 𝑗 . Then,
𝑙 𝑗 (𝑝∗) ≤ 0, ∑︁
𝑗∼𝑖

𝑑 𝑗 (𝑝∗) ≤ −𝛼, ∑︁
𝑗 ≺𝑖

𝑣 𝑗 (𝑝∗) < 𝛼 .

∑︁

:

𝑗 ≻𝑖
Therefore,

𝑄𝑖 (𝑝∗) =

∑︁

𝑙 𝑗 (𝑝∗) +

∑︁

𝑑 𝑗 (𝑝∗) +

𝑗 ≻𝑖

𝑗∼𝑖
< 0 − 𝛼 + 𝛼 = 0.

𝑣 𝑗 (𝑝∗)

∑︁

𝑗 ≺𝑖

By a symmetric argument, there also exists some 𝑞∗ for which
𝑄𝑖 (𝑞∗) > 0. By the intermediate value theorem with 𝑄𝑖 continuous,
there exists 𝑝 ∈ (𝑞∗, 𝑝∗) such that 𝑄𝑖 (𝑝) = 0, as desired.
□

REFERENCES
[1] CodeChef Rating System. codechef.com/ratings
[2] Codeforces Rating System. codeforces.com/blog/entry/20762
[3] Farming Volatility: How a major flaw in a well-known rating system takes over
the GBL leaderboard. reddit.com/r/TheSilphRoad/comments/hwff2d/farming_
volatility_how_a_major_flaw_in_a/

[4] Halo Xbox video game franchise: in numbers. telegraph.co.uk/technology/video-

games/11223730/Halo-in-numbers.html

[5] Kaggle Progression System. kaggle.com/progression
[6] LeetCode Rating System.

leetcode.com/discuss/general-discussion/468851/New-

Contest-Rating-Algorithm-(Coming-Soon)

[7] TopCoder Algorithm Rating System.

topcoder.com/community/competitive-

programming/how-to-compete/ratings

[8] Why Are Obstacle-Course Races So Popular? theatlantic.com/health/archive/

2018/07/why-are-obstacle-course-races-so-popular/565130/

[9] Sharad Agarwal and Jacob R. Lorch. 2009. Matchmaking for online games and

other latency-sensitive P2P systems. In SIGCOMM 2009. 315–326.

[10] Mark Yuying An. 1997. Log-concave probability distributions: Theory and statis-
tical testing. Duke University Dept of Economics Working Paper 95-03 (1997).
[11] Shuo Chen and Thorsten Joachims. 2016. Modeling Intransitivity in Matchup

and Comparison Data. In WSDM 2016. 227–236.

[12] Pierre Dangauthier, Ralf Herbrich, Tom Minka, and Thore Graepel. 2007. TrueSkill
Through Time: Revisiting the History of Chess. In NeurIPS 2007. 337–344.

[13] Arpad E. Elo. 1961. New USCF rating system. Chess Life 16 (1961), 160–161.
[14] RNDr Michal Forišek. 2009. Theoretical and Practical Aspects of Programming

Contest Ratings. (2009).

[15] David A Freedman. 1963. On the asymptotic behavior of Bayes’ estimates in the

discrete case. The Annals of Mathematical Statistics (1963), 1386–1403.

12

