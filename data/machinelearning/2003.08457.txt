0
2
0
2

r
a

M
8
1

]

C
O
.
h
t
a
m

[

1
v
7
5
4
8
0
.
3
0
0
2
:
v
i
X
r
a

MALICIOUS EXPERTS VERSUS THE MULTIPLICATIVE WEIGHTS
ALGORITHM IN ONLINE PREDICTION

ERHAN BAYRAKTAR, H. VINCENT POOR, AND XIN ZHANG

Abstract. We consider a prediction problem with two experts and a forecaster. We assume that
one of the experts is honest and makes correct prediction with probability µ at each round. The
other one is malicious, who knows true outcomes at each round and makes predictions in order
to maximize the loss of the forecaster. Assuming the forecaster adopts the classical multiplicative
weights algorithm, we ﬁnd an upper bound (3.12) for the value function of the malicious expert,
and also a lower bound (4.1). Our results imply that the multiplicative weights algorithm cannot
resist the corruption of malicious experts. We also show that an adaptive multiplicative weights
algorithm is asymptotically optimal for the forecaster, and hence more resistant to the corruption
of malicious experts.

1. Introduction

Prediction with expert advice is classical and fundamental in the ﬁeld of online learning, and we
refer the reader to [6] for a nice survey. In this problem, a forecaster makes predictions based on
advices of experts so as to minimize his loss, i.e., the cumulative diﬀerence between his predictions
and true outcomes. A standard performance criterion is the regret: the diﬀerence between the loss
of the forecaster and the minimum among losses of all experts. The prediction problem is often
studied in the so-called adversarial setting and the stochastic setting. In the adversarial setting,
the advice of experts is chosen by an adversary so as to maximize the regret of the forecaster, and
therefore the problem can be viewed as a zero-sum game between the forecaster and the adversary
(see e.g. [12] [9] [8] [5] [4]). In the stochastic setting, the losses of each expert are drawn independent
and identically distributed (i.i.d.) over time from a ﬁxed but unknown distribution, and smaller
regrets can be achieved compared with the adversarial setting (see e.g. [7] [10] [13]).

In this paper, we consider the model in [14] which considers a mix of adversarial and stochastic
settings. It is a learning system with two experts and a forecaster. One of the experts is honest,
who at each round makes a correct prediction with probability µ. The other one is malicious,
who knows the true outcome at each round and makes his predictions so as to maximize the loss
of the forecaster. Here we assume that the forecaster adopts the classical multiplicative weights
algorithm, and study its resistance to the corruption of the malicious expert. Denote by V α(N, ρ)
the expected cumulative loss for the forecaster, where α is the strategy chosen by the malicious
expert, N is the ﬁxed time horizon, and ρ is the initial weight of the malicious expert. Instead of
regret, we analyze the asymptotic maximal loss lim
N→∞
It was proved in [14] that if the malicious expert is only allowed to adopt oﬄine policies, i.e., to
decide whether to tell the true outcome at each round at the beginning of the game, then we have
V α(N,1/2)
= 1 − µ. It implies that the extra power of the malicious expert cannot incur
lim
N
N→∞
extra losses to the forecaster.

V α(N,1/2)
N

max
α

max
α

.

Here we allow the malicious expert to adopt online policies, i.e., at each round, the malicious
expert chooses whether to tell the truth based on all the prior histories. To ﬁnd an upper bound on
asymptotic losses, we rescale dynamic programming equations of the problem and obtain a partial
diﬀerential equation (PDE). Then we prove that the unique solution of this PDE provides us an
1

 
 
 
 
 
 
2

upper bound

lim sup
N→∞

max
α

V α(N, 1/2)
N

≤ 1 − µ2.

For the lower bound, we design a simple strategy for the malicious expert and prove that

V α(N, 1/2)
N
which implies that the malicious expert can incur extra losses to the forecaster when online poli-
cies are admissible. To make the forecaster more resistant to the malicious expert, we consider
an adaptive multiplicative weights algorithm and prove that it is asymptotically optimal for the
forecaster.

lim inf
N→∞

> 1 − µ,

max
α

The rest of the paper is organized as follows. In Section 2, we mathematically formulate this
problem and develop its dynamic programming equations. In Section 3, we show the upper bound
of asymptotic losses, and in Section 4 we ﬁnd the lower bound.
In Section 5, we consider the
malicious expert versus the adaptive multiplicative weights algorithm. In Section 6, we summarize
our results and their implications.

2. Problem Formulation

In this section, we introduce the mathematical model as in [14]. Consider a learning system with
two experts and a forecaster. For each round t ∈ N+, denote the prediction of expert i ∈ {1, 2} by
xi
t ∈ {0, 1}, and the true outcome by yt ∈ {0, 1}.
Suppose that the forecaster adopts the multiplicative weights algorithm. For each round t ∈ N+,
t = 1. Then the prediction of the forecaster is

t the weight of expert i ∈ {1, 2}, p1

denote by pi

t + p2
2

Given ǫ ∈ (0, 1), the weights evolve as follows

Xi=1

ˆyt :=

txi
pi
t.

pi
t+1 =

t−yt|

tǫ|xi
pi
t −yt| + p2

t ǫ|x1
p1

t ǫ|x2

t −yt|

,

i = 1, 2.

Denote the entire history up to round t − 1 by

Gt := {p1

l , p2

l , x1

l , x2

l , yl : l = 1, . . . t − 1} ∪ {p1

t , p2

t }.

Assume expert 2 is honest, and at each round t ∈ N+ make correct predictions with probability
µ ∈ (0, 1) independently of Gt, i.e.,

x2
t =

with probability µ,

yt
1 − yt with probability 1 − µ.

(

Expert 1 is malicious and knows the accuracy µ of expert 2 and the outcome yt at each round. At
each stage t ∈ N+, based on the information Gt, the malicious expert can choose to lie, i.e., make
x1
t = 1 − yt, or to tell the truth, i.e., make x1
t = yt. Denote by At the space of functions from Gt to
{T, L}, where T (truth) and L (lie) represent x1
t = 1 − yt respectively.

At each round t ∈ N+, the loss of the forecaster is l(ˆyt, yt) := |ˆyt − yt|, which is also the gain of

t = yt and x1

the malicious expert. It can be easily veriﬁed that

(2.1)

p1
t
1
0
1 − p1
t

if αt = L, x2
if αt = L, x2
if αt = T, x2
if αt = T, x2

t = yt,
t = 1 − yt,
t = yt,
t = 1 − yt.

l(ˆyt, yt) = 



3

And the evolution of p1

t is as follows:

(2.2)

where

p1
t+1 = 


g(p1
t )
g(−1)(p1
t )
p1
t

if αt = L, x2
if αt = T, x2
otherwise,

t = yt,
t = 1 − yt,

g(p1

t ) =


1
1 + (1/p1
t − 1)/ǫ

,

g(−1)(p1

t ) =

1
1 + (1/p1

t − 1)ǫ

.

For a ﬁxed time horizon N , the goal of the malicious expert is to maximize the cumulative loss
of the forecaster by choosing a sequence of strategies α = {(α1, α2, . . . ) : αt ∈ At, t ∈ N+}, i.e.,
solving the optimization problem

According to (2.1), we obtain the expected current loss

V (N, ρ) := max

α

Eα

"

N

l(ˆyt, yt)

p1
1 = ρ

.

#

t=1
X

(cid:12)
(cid:12)

(2.3)

Eαt [l(ˆyt, yt)|Gt] =

(1 − µ + µp1
t )
(1 − µ)(1 − p1
t )

(

if αt = L,
if αt = T.

In combination with (2.2), we get dynamic programming equations

(2.4)

V (t + 1, ρ) = max{(1 − µ + µρ) + µV (t, g(ρ)) + (1 − µ)V (t, ρ),

together with initial conditions V (0, ρ) = 0.

(1 − µ)(1 − ρ) + (1 − µ)V

t, g(−1)(ρ)
(cid:17)

(cid:16)

+ µV (t, ρ)},

3. Upper bound on the Value function

In this section, we properly rescale the (2.4) and obtain a PDE (HJB). We explicitly solve this

equation, and show that its solution (3.5) provides an upper bound on lim sup
N→∞

V (N,1/2)
N

.

3.1. Limiting PDE. To appropriately rescale (2.4) and follow the formulation of [2], we change
the variable

x =

ln(1/ρ − 1)
ln(1/ǫ)

,

ρ =

1
1 + (1/ǫ)x ,

and deﬁne

Then (2.4) becomes

˜V (t, x) := −V

t,

1
1 + (1/ǫ)x

.

(cid:19)

(cid:18)

(3.1)

˜V (t + 1, x) = min

−

1 − µ +

µ
1 + (1/ǫ)x

+ µ ˜V (t, x + 1) + (1 − µ) ˜V (t, x),

(cid:19)
+ (1 − µ) ˜V (t, x − 1) + µ ˜V (t, x)

.

(cid:18)
(cid:26)
−(1 − µ)

1 −

(cid:18)

1
1 + (1/ǫ)x

Deﬁne scaled value functions via the equation

that

(3.2)

˜V δ(t + δ, x) = min

−δ

1 − µ +

(cid:19)
˜V δ(δt,δx)
δ

= ˜V (t, x). Substituting in (3.1), we obtain

(cid:27)

+ µ ˜V δ(t, x + δ) + (1 − µ) ˜V δ(t, x)

(cid:19)
+ (1 − µ) ˜V δ (t, x − δ)) + µ ˜V δ(t, x)

.

(cid:27)

(cid:26)

(cid:18)

−δ(1 − µ)

1 −

(cid:18)

µ
1 + (1/ǫ)x/δ

1
1 + (1/ǫ)x/δ

(cid:19)

4

Taking δ to 0 in (3.2), we obtain a ﬁrst order PDE

(3.3)

0 = vt(t, x)+ max {1 − µ + µs(x) − µvx(t, x),

(1 − µ)(1 − s(x)) + (1 − µ)vx(t, x)} ,

where v(0, x) = 0, and

s(x) =

0,
1,

(

if x > 0,
if x < 0.

Deﬁne Ω1 = {x > 0}, Ω2 = {x < 0}, H = {x = 0}, and Hamiltonians
H1(x, p) = max{1 − µ − µp, 1 − µ + (1 − µ)p},
H2(x, p) = max{1 − µp, (1 − µ)p},

x ∈ ¯Ω2.

x ∈ ¯Ω1,

Then (3.3) becomes

(3.4)

vt + Hi(x, vx) = 0

for x ∈ Ωi, i = 1, 2.

Following Ishii’s deﬁnition of viscosity solutions to discontinuous Hamiltonians, we complement
(3.4) by

min{vt + H1(x, vx), vt + H2(x, vx)} ≤ 0
max{vt + H1(x, vx), vt + H2(x, vx)} ≥ 0

for x ∈ H,

for x ∈ H,

where min and max should be understood in the sense of viscosity solutions.

Solving (3.4) by the method of characteristics and assuming that the value function is diﬀeren-

tiable with respect to x on H, we conjecture the solution

(3.5)

v(t, x) = 


if x ∈ [(1 − µ)t, ∞),

−(1 − µ)t,
−(1 − µ2)t + µx if x ∈ [−µt, (1 − µ)t],
−t,

if x ∈ (−∞, −µt].

Proposition 3.1. A viscosity solution of



for x ∈ Ωi, i = 1, 2,

vt + Hi(x, vx) = 0,
min{vt + H1(x, vx), vt + H2(x, vx)} ≤ 0
max{vt + H1(x, vx), vt + H2(x, vx)} ≥ 0
v(0, x) = 0.

for x ∈ H,
for x ∈ H,

(HJB)

is given by (3.5).





Proof. The initial condition v(0, x) = 0 is trivially satisﬁed. We show that v is a subsolution.
Suppose φ : [0, ∞) × R → R is diﬀerentiable, and v − φ achieves a local maximum 0 at (t0, x0) ∈
(0, ∞) × R. Since v is diﬀerentiable in the domain O := {(t, x) : t > 0, x 6= (1 − µ)t, x 6= −µt}, we
have φt(t0, x0) = vt(t0, x0), φx(t0, x0) = vx(t0, x0) if (t0, x0) ∈ O. Then it is can be easily veriﬁed
that φt + Hi(x, φx) = 0 at (t0, x0), where i = 1 if x0 ≥ 0, and i = 2 if x0 ≤ 0.
Suppose (t0, x0) is on the line {(t, x) : t > 0, x = (1 − µ)t}. Note that
t (t0, x0) = −(1 − µ2),

t v(t0, x0) = −(1 − µ), ∂+
∂−
∂−
x v(t0, x0) = µ,

∂+
x v(t0, x0) = 0.

Since (t0, x0) is a local maximum of u − φ, we must have

(φt(t0, x0), φx(t0, x0)) ∈ {(r, p) : r ∈ [−(1 − µ2), −(1 − µ)], p ∈ [0, µ]}.

Take ∆x = (1 − µ)∆t. As a result of

v(t0 + ∆t, x0 + ∆x) − φ(t0 + ∆t, x0 + ∆x) ≤ 0,

5

we obtain that

Since we can choose ∆t to be either positive or negative, it can be easily deduced that

−(1 − µ)∆t − φt∆t − φx∆x + O(∆t) ≤ 0.

Substituting into H1, we obtain that

−(1 − µ) − φt − (1 − µ)φx = 0.

φt(t0, x0) + H1(x0, φx(t0, x0)) = φt(t0, x0) + (1 − µ) + (1 − µ)φx(t0, x0) = 0.

If (t0, x0) is on the line {(t, x) : t > 0, x = −µt}, we have sub/super diﬀerentials of v,

t v(t0, x0) = −1, ∂+
∂−
t (t0, x0) = −(1 − µ2),
∂−
∂+
x v(t0, x0) = 0,
x v(t0, x0) = µ.

Therefore v − φ cannot achieve a local maximal on the line {(t, x) : t > 0, x = −µt}. Hence we have
proved that v is a subsolution of (HJB), and similarly, we can show that v is a supersolution. (cid:3)

3.2. Control problem. In this subsection, we show that there is a unique viscosity solution of
(HJB) by applying results from [1] and [2]. First, we interpret (HJB) as a control problem.

In the domain Ωi, i = 1, 2, we take Ai = [0, 1] as the space of controls, and

bi(x, αi) = αiµ − (1 − αi)(1 − µ), αi ∈ Ai,
as the controlled dynamics. For x ∈ H, deﬁne the space of controls A := A1 × A2 × [0, 1], and the
dynamics

bH(x, (α1, α2, c)) := cb1(x, α1) + (1 − c)b2(x, α2),

(α1, α2, c) ∈ A.

The running cost in the domain Ω1 is given by l1(x, α1) = −(1−µ), in the domain Ω2 by l2(x, α2) =
−α2, and in H by

lH(x, (α1, α2, c)) = cl1(x, α1) + (1 − c)l2(x, α2),

where (α1, α2, c) ∈ A.

In order to let trajectories stay on the boundary H for a while, for x ∈ H, we denote

A0(x) := {a = (α1, α2, c) ∈ A : bH(x, (α1, α2, c)) = 0}.

We say a control a ∈ A0(x) is regular if b1(x, α1) ≤ 0, b2(x, α2) ≥ 0, and denote

(x) := {a = (α1, α2, c) ∈ A0(x) : (−1)ibi(x, αi) ≥ 0}.
Deﬁne A := L∞([0, 1]; A). We say a Lipschitz function Xx : [0, 1] → R, Xx(0) = x, an admissible

Areg
0

trajectory if there exists some control process a(·) ∈ A, such that for a.e. t ∈ [0, 1]
˙Xx(t) =b1(Xx(t), α1(t))1{Xx(t)∈Ω1} + b2(Xx(t), α2(t))1{Xx(t)∈Ω2}

(3.6)

+ bH(Xx(t), (α1(t), α2(t), c(t))1{Xx (t)∈H}.

According to [2, Theorem 2.1], we have a(t) ∈ A0(Xx(t)) for a.e. t ∈ {s : Xx(s) ∈ H}. Denote by
Tx the set of admissible controlled trajectories starting from x, i.e.,

Tx := {(Xx(.), a(.)) ∈ Lip([0, 1]; R) × A such that (3.6) is satisﬁed and Xx(0) = x}.

Let us also introduce the set of regular trajectories,
:= {(Xx(.), a(.)) ∈ Tx : a(t) ∈ Areg

T reg
x

0

(Xx(t)) for a.e. t ∈ {s : Xx(s) ∈ H}}

For each x ∈ R, t ∈ [0, 1), we deﬁne two value functions

(3.7)

(3.8)

V −(x, t) :=

V +(x, t) :=

inf
(Xx(.),a(.))∈Tx Z
inf
(Xx(.),a(.))∈T reg

0

0
x Z

t

l(Xx(s), a(s) ds,

t

l(Xx(s), a(s) ds,

6

where the cost function l is given by

l(Xx(s), a(s)) :=

li(Xx(s), αi(s))1{Xx(s)∈Ωi} + lH(Xx(s), a(s))1{Xx(s)∈H}.

Note that in Ωi, i = 1, 2, the associated Hamiltonian of (3.7) and (3.8)

Xi=1,2

(x, p) 7→ sup
αi∈Ai

{−bi(x, αi)p − li(x, αi)}

coincides with Hi in the last subsection. Then according to [2, Theorem 3.3], both V − and V +
are viscosity solutions of (HJB). We will show that they are actually equal and there is only one
viscosity solution of (HJB).

Proposition 3.2. V − = V + is the unique viscosity solution of (HJB), and V − is the minimal
supersolution of (HJB).

Proof. The argument is an application of results from [2]. Deﬁne the Hamiltonians on H via

HT (x) := sup
A0(x)
T (x) := sup
Areg
(x)
0

H reg

{−lH(x, a)},

{−lH(x, a)},

Let us compute HT (x). Suppose a = (α1, α2, c) ∈ A0(x). Then it can be easily veriﬁed that
maximizing −lH(x, a) over A0(x) is equivalent to maximizing

(3.9)

subject to constraints,

(3.10)

c(1 − µ) + (1 − c)α2,

c(α1 + µ − 1) + (1 − c)(α2 + µ − 1) = 0,
c, α1, α2 ∈ [0, 1].

We ﬁrst ﬁx α2 and suppose α2 > (1 − µ). Due to the equality

c(1 − µ) + (1 − c)α2 = (1 − µ − α2)c + α2,

and the fact that the coeﬃcient before c is negative, maximizing (3.9) is equivalent to minimizing
c under the constraints. It can be easily seen that the minimum c can be obtained if and only
if α1 = 0. Therefore the equation (3.10) becomes 1 + α2c = α2 + µ, and hence (3.9) is equal to
(1 + c)(1 − µ). Now ﬁx α1 = 0. In order to obtain the maximum of c, we have to take α2 = 1. In
that case α1 = 0, α2 = 1, c = µ and c(1 − µ) + (1 − c)α2 = 1 − µ2.

If α2 ≤ (1 − µ), we have c(1 − µ) + (1 − c)α2 ≤ (1 − µ) < 1 − µ2. Since (0, 1, µ) is a regular

control, we conclude that

HT (x) = H reg

T (x) = 1 − µ2.

We say a continuous function v is viscosity solution of

(3.11)

vt + H−(x, vx) = 0 in (0, 1) × R,
vt + H+(x, vx) = 0 in (0, 1) × R

resp.,

if it satisﬁes (HJB) and

(cid:2)

(cid:3)

vt + HT (x) = 0 on [0, 1] × H,
vt + H reg
T (x) = 0 on [0, 1] × H].

[resp.,

According to [2, Theorem 3.3], V + is a viscosity subsolution of vt + H+(x, vx) = 0, and hence also
a viscosity subsolution of (3.11) since HT = H reg
in our case. As a result of [2, Theorem 4.2, 4.4],
V − is the viscosity solution of (3.11), and the comparison result holds for (3.11). Therefore we
conclude that V + ≤ V −. Then according to their deﬁnitions (3.7) and (3.8), they must be equal.

T

Finally according to [2, Theorem 4.4], V − is the minimal supersolution of (HJB) and V + is
the maximal subsolution of (HJB). Then if v is a viscosity solution of (HJB), we must have
(cid:3)
V − ≤ v ≤ V + and hence v = V − = V +.

7

3.3. Upper bound (3.12). In this subsection, we show that

v(t, x) := lim inf

(s,y,δ)→(t,x,0)

˜V δ(s, y)

is a viscosity supersolution of (HJB). Then according to Proposition 3.2, we obtain that v(t, x) ≥
v(t, x), and hence

lim inf
N→∞

˜V (N, N x)
N

≥ v(1, x).

In particular, if we take x = 0, then the above inequality becomes

(3.12)

lim sup
N→∞

V (N, 1/2)
N

≤ 1 − µ2.

Proposition 3.3. v is a viscosity supersolution of (HJB).

Proof. The proof is almost the same as [3, Theorem 2.1], and we record here for completeness.
Fixing arbitrary T > 0, we show that v is a viscosity supersolution over [0, T ] × R. Assume
b ([0, T ] × R). As a result of
that (t0, x0) is a strict local minimum of v − φ for some φ ∈ C∞
(3.2), it can be easily seen that v(t, x) ∈ [−t, 0]. Without loss of generality, we assume that
t0 ∈ (0, T ), v(t0, x0) = φ(t0, x0), and there exists some r > 0 such that

(i) φ ≤ −2T outside the ball B((t0, x0), r) := {(t, x) : (t − t0)2 + (x − x0)2 ≤ r2},
(ii) v − φ ≥ 0 = (t0, x0) − φ(t0, x0) in the ball B((t0, x0), r).
Then there exists a sequence of (tn, xn, δn) such that (tn, xn, δn) → (t0, x0, 0) and (tn, xn) is a global
minimum of ˜V δn − φ. Due to the deﬁnition of v, we have that ξn := ˜V δn(tn, xn) − φ(tn, xn) → 0
and ˜V δn(t, x) ≤ φ(t, x) + ξn for any (t, x) ∈ [0, T ] × R.

According to (3.2), we obtain that

0 ≤ φ(tn, xn)+ max

δn

1 − µ +

(3.13)

(cid:26)
(cid:18)
δn(1 − µ)

(cid:18)

1 −

1
1 + (1/ǫ)xn/δn

(cid:19)

µ
1 + (1/ǫ)xn/δn

− µφ(tn − δn, xn + δn) − (1 − µ)φ(tn − δn, xn),

(cid:19)
− (1 − µ)φ (tn − δn, xn − δn)) − µφ(tn − δn, xx)

.

We prove for the case x0 = 0, and the proof for x 6= 0 is the same. Since

1
1+(1/ǫ)xn/δn

n
we can take a convergent subsequence. For simplicity, we still denote it by

n≥0

o
1
1+(1/ǫ)xn /δn

assume it converges to some s ∈ [0, 1]. Letting n → ∞ in (3.13), we obtain that

n

0 ≤ φt(t0, x0) + max {1 − µ + µs − µφx(t0, x0), (1 − µ)(1 − s) + (1 − µ)φx(t0, x0)} .

Note that if

then we have

and hence

Similarly if

1 − µ + µs − µφx(t0, x0) ≥ (1 − µ)(1 − s) + (1 − µ)φx(t0, x0),

H2(x0, φx(t0, x0)) ≥ 1 − µφx(t0, x0) ≥ 1 − µ + µs − µφx(t0, x0),

φt(t0, x0) + H2(x0, φx(t0, x0) ≥ 0.

(1 − µ)(1 − s) + (1 − µ)φx(t0, x0) ≥ 1 − µ + µs − µφx(t0, x0),

(cid:27)
∈ [0, 1],

, and

n≥0

o

8

then

H1(x0, φx(t0, x0)) ≥ 1 − µ + (1 − µ)φx(t0, x0) ≥ (1 − µ)(1 − s) + (1 − µ)φx(t0, x0),

and hence

Therefore, we have shown that

φt(t0, x0) + H1(x0, φx(t0, x0) ≥ 0.

max{φt(t0, x0) + H1(x0, φx(t0, x0)), φt(t0, x0) + H2(x0, φx(t0, x0))} ≥ 0.

(cid:3)

4. Lower Bound on the Value function

It was proved in [14] that the asymptotic average value is (1 − µ) for any oﬄine strategy of the
1 = 1/2. Here we provide a lower bound on the value

malicious expert if starting with weight p1
functions for the corresponding online problem

(4.1)

lim inf
N→∞

V (N, ρ)
N

≥ 1 − µ + µ(1 − µ)(ρ − g(ρ)) > 1 − µ,

which shows that the malicious expert has more advantages when he adopts online policies.

This lower bound can be achieved if the malicious expert chooses to lie at state ρ and chooses

to tell the truth at state g(ρ). For p1

1 = ρ, deﬁne the corresponding strategies by

(4.2)

αρ
t (Gt) =

L if p1
T if p1

t = ρ,
t = g(ρ),

and αρ := (αρ

1, αρ

(
2, . . . ). We denote the value function associated with αρ by

Proposition 4.1.

V αρ

(N, ρ) = Eαρ

N

"
Xt=1

l(ˆyt, yt)

p1
1 = ρ

(cid:12)
(cid:12)

.

#

V αρ

(N, ρ)
N

lim
N→∞

= 1 − µ + µ(1 − µ)(ρ − g(ρ)).

t }t∈N is a Markov chain with two states {ρ, g(ρ)} starting with p1

0 = ρ,

Proof. Under strategy αρ, {p1
and its transition probability is given by
p1
t+1 = ρ | p1
t+1 = ρ | p1
p1

(cid:3)
(cid:2)
Denote its distribution at time t by
(cid:2)

t = ρ
t = g(ρ)

P

P

(cid:3)

= 1 − µ,

P

t+1 = g(ρ) | p1
p1
t+1 = g(ρ) | p1
p1

t = ρ
t = g(ρ)
(cid:3)

= µ,

= µ.

= 1 − µ, P
(cid:2)

(cid:2)
t = ρ), P(p1

P(p1

t = g(ρ))

(cid:3)

.

πt :=

It can be easily seen that (1 − µ, µ) is the stationary distribution of {p1
Theorem 4.9], the distribution πt converges to (1 − µ, µ) as t → ∞. Due to the equality

t }t∈N. According to [11,

(cid:1)

(cid:0)

N

Eαρ

"

t=0
X

l(ˆyt, yt)

p1
0 = ρ

it can be easily veriﬁed that
V αρ

(cid:12)
(cid:12)

lim
N→∞

=

#

N

t=0
X

P(p1

t = ρ)(1 − µ + µρ) +

P(p1

t = g(ρ))(1 − µ)(1 − g(ρ)),

N

t=0
X

(N, ρ)
N

= (1 − µ)(1 − µ + µρ) + µ(1 − µ)(1 − g(ρ))

= 1 − µ + µ(1 − µ)(ρ − g(ρ)) ≥ 1 − µ.

(cid:3)

5. asymptotically optimal strategy for the forecaster

In this section, we show that an adaptive multiplicative weightsed algorithm can resist corruptions
of the malicious expert. Diﬀerent from the multiplicative weights algorithm in Section 2, the
adaptive multiplicative weightsed algorithm updates the weights pi

t, i = 1, 2, as follows:

9

pi
t+1 =

te−ηt|xi
pi
t e−ηt|x1
t −yt| + p2
p1

t−yt|
t e−ηt|x2

t −yt|

,

where ηt =
malicious expert under the adaptive multiplicative weightsed algorithm. Deﬁne

8(ln 2)/t, t ∈ N+ is time-varying. Denote by V ∗(N, ρ) the value function for the

p

gt(p1

t ) =

1
1 + (1/p1
t − 1)eηt

,

g(−1)
t

(p1

t ) =

1
1 + (1/p1
t − 1)e−ηt

.

It can be easily veriﬁed that V ∗(N, ρ) is the solution to dynamic programming equations

V ∗(t + 1, ρ) = max{(1 − µ + µρ) + µV ∗(t, gt(ρ)) + (1 − µ)V ∗(t, ρ),
+ µV ∗(t, ρ)},

(1 − µ)(1 − ρ) + (1 − µ)V ∗

(ρ)

t, g(−1)
t
(cid:16)

(cid:17)

together with initial conditions V ∗(0, ρ) = 0.

Proposition 5.1.

(5.1)

lim
N→∞

V ∗(N, 1/2)
N

= 1 − µ,

which implies that this adaptive multiplicative weights algorithm is asymptotically optimal for the
forecaster.

Proof. Suppose the malicious expert keeps lying, i.e. taking strategies αt(Gt) = L, t ∈ N+. Then
according to (2.3), it can be easily seen that the cumulative loss under this strategy is greater than
or equal to (1 − µ)N , and hence

lim inf
N→∞

V ∗(N, 1/2)
N

≥ 1 − µ.

To prove the other inequality, for any path GN +1 with p1

1 = p2

1 = 1/2, we deﬁne

N

N

ˆLN :=

l(ˆyt, yt), Li

N :=

l(xi

t, yt), i = 1, 2.

Applying [6, Chapter 2, Theorem 2.3], we obtain that

Xt=1

Xt=1

and hence

ˆLN − min
i=1,2

Li

N ≤ 2

r

N
2

ln 2 +

ln 2
8

,

r

Therefore for any strategy α, we obtain

ˆLN ≤ L2

N + 2

N
2

ln 2 +

ln 2
8

.

r

r

Eα

ˆLN

p1
1 = 1/2

≤ Eα

L2
N

p1
1 = 1/2

h

(cid:12)
(cid:12)

i

(cid:2)

(cid:12)
(cid:12)

= (1 − µ)N + 2

r

(cid:3)
N
2

+ 2

r

ln 2 +

N
2

r

ln 2 +

ln 2
8

r

ln 2
8

,

10

and also

lim sup
N→∞

V ∗(N, 1/2)
N

≤ 1 − µ.

6. Conclusions

(cid:3)

In this paper, we studied an online prediction problem with two experts of whom one is malicious.
At each round, based on all the prior history, the malicious expert chooses to tell the true outcome
or not so as to maximize the loss. We showed that the multiplicative weights algorithm cannot resist
the corruption of the malicious expert by explicitly ﬁnding upper and lower bounds on the value
function; see (3.12) and (4.1). We also proved that an adaptive multiplicative weights algorithm
can resist the corruption; see Proposition 5.1.

References

[1] G. Barles, A. Briani, and E. Chasseigne, A Bellman approach for two-domains optimal control problems in

RN , ESAIM Control Optim. Calc. Var., 19 (2013), pp. 710–739.

[2]

, A Bellman approach for regional optimal control problems in RN , SIAM J. Control Optim., 52 (2014),

pp. 1712–1744.

[3] G. Barles and P. E. Souganidis, Convergence of approximation schemes for fully nonlinear second order

equations, Asymptotic Anal., 4 (1991), pp. 271–283.

[4] E. Bayraktar, I. Ekren, and X. Zhang, Finite-time 4-expert prediction problem, Communications in Partial

Diﬀerential Equations, (2020).

[5] E. Bayraktar, I. Ekren, and Y. Zhang, On the asymptotic optimality of the comb strategy for prediction

with expert advice, To appear in Annals of Applied Probability, (2020).

[6] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games, Cambridge University Press, Cambridge,

2006.

[7] S. de Rooij, T. van Erven, P. D. Gr¨unwald, and W. M. Koolen, Follow the leader if you can, hedge if

you must, J. Mach. Learn. Res., 15 (2014), pp. 1281–1316.

[8] N. Drenska and R. V. Kohn, Prediction with expert advice: A pde perspective, Journal of Nonlinear Science,

(2019).

[9] N. Gravin, Y. Peres, and B. Sivan, Towards optimal algorithms for prediction with expert advice, in Proceed-
ings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, ACM, New York, 2016,
pp. 528–547.

[10] W. M. Koolen, P. Gr¨unwald, and T. van Erven, Combining adversarial guarantees and stochastic fast
rates in online learning, in Proceedings of the 30th International Conference on Neural Information Processing
Systems, NIPS’16, Red Hook, NY, USA, 2016, Curran Associates Inc., pp. 4464–4472.

[11] D. A. Levin and Y. Peres, Markov chains and mixing times, American Mathematical Society, Providence, RI,
2017. Second edition, With contributions by Elizabeth L. Wilmer, With a chapter on “Coupling from the past”
by James G. Propp and David B. Wilson.

[12] N. Littlestone and M. K. Warmuth, The weighted majority algorithm, Inform. and Comput., 108 (1994),

pp. 212–261.

[13] J. Mourtada and S. Ga¨ıffas, On the optimality of the hedge algorithm in the stochastic regime, J. Mach.

Learn. Res., 20 (2019), pp. Paper No. 83, 28.

[14] S. Rasoul Etesami, N. Kiyavash, and H. V. Poor, Adversarial Policies in Learning Systems with Malicious

Experts, arXiv:2001.00543, (2020).

Department of Mathematics, University of Michigan
E-mail address: erhan@umich.edu

Department of Electrical Engineering, Princeton University
E-mail address: poor@princeton.edu

Department of Mathematics, University of Michigan
E-mail address: zxmars@umich.edu

