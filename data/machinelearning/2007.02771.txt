0
2
0
2

l
u
J

6

]

G
L
.
s
c
[

1
v
1
7
7
2
0
.
7
0
0
2
:
v
i
X
r
a

Certifying Decision Trees Against Evasion
Attacks by Program Analysis(cid:63)

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

Universit`a Ca’ Foscari Venezia

Abstract. Machine learning has proved invaluable for a range of diﬀer-
ent tasks, yet it also proved vulnerable to evasion attacks, i.e., maliciously
crafted perturbations of input data designed to force mispredictions. In
this paper we propose a novel technique to verify the security of decision
tree models against evasion attacks with respect to an expressive threat
model, where the attacker can be represented by an arbitrary imperative
program. Our approach exploits the interpretability property of decision
trees to transform them into imperative programs, which are amenable
for traditional program analysis techniques. By leveraging the abstract
interpretation framework, we are able to soundly verify the security guar-
antees of decision tree models trained over publicly available datasets.
Our experiments show that our technique is both precise and eﬃcient,
yielding only a minimal number of false positives and scaling up to cases
which are intractable for a competitor approach.

Keywords: Adversarial machine learning · Decision trees · Security of
machine learning · Program analysis.

1

Introduction

Machine learning (ML) learns predictive models from data and has proved in-
valuable for a range of diﬀerent tasks, yet it also proved vulnerable to evasion
attacks, i.e., maliciously crafted perturbations of input data designed to force
mispredictions [25]. To exemplify, let us assume a credit company decides to use
a ML model to automatically assess whether customers qualify for a loan or not.
A malicious customer who somehow realises or guesses that the model privileges
unmarried people over married people could cheat about her marital status to
improperly qualify for a loan.

The research community recently put a lot of eﬀort in the investigation of
adversarial ML, e.g., techniques to train models which are resilient to attacks or
assess the security properties of models. In the present paper we are interested in
the security certiﬁcation of a popular class of models called decision trees, i.e., we
investigate formally sound techniques to quantify the resilience of such models
against evasion attacks. Speciﬁcally, we propose the ﬁrst provably sound certiﬁ-
cation technique for decision trees with respect to an expressive threat model,

(cid:63) Accepted to ESORICS 2020

 
 
 
 
 
 
2

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

where the attacker can be represented by an arbitrary imperative program. Veri-
fying ML techniques with respect to highly expressive threat models is nowadays
one of the most compelling research directions of adversarial ML [16,12]. This
is an important step forward over previous work, which either proposed empiri-
cal techniques without formal guarantees or only focused on artiﬁcial attackers
expressed as mathematical distances (see Section 6 for full details).

Our approach exploits the interpretability property of decision trees, i.e.,
their amenability to be easily understood by human experts, which makes their
translation into imperative programs a straightforward task. Once a decision
tree is translated into an imperative program, it is possible to leverage state-of-
the-art program analysis techniques to certify its resilience to evasion attacks.
In particular we leverage the abstract interpretation framework [9,10] to auto-
matically extract a sound abstraction of the behaviour of the decision tree under
attack. This allows us to eﬃciently compute an over-approximated, yet precise,
estimate of the resilience of the decision tree against evasion attacks.

Contributions. We speciﬁcally contribute as follows:

1. We propose a general technique to certify the security guarantees of deci-
sion trees against evasion attacks attempted by an attacker expressed as an
arbitrary imperative program. We exemplify the technique at work on an
expressive threat model based on rewriting rules (Section 3).

2. We implement our technique into a new tool called TreeCert. Given a de-
cision tree, an attacker and a test set of instances used to estimate predic-
tion errors, TreeCert outputs an over-approximation of the error rate that
the attacker can force on the decision tree. TreeCert implements a context-
insensitive analysis computing a single over-approximation of the attacker’s
behavior and reuses it in the analysis of all the test instances, thus boosting
eﬃciency without missing attacks (Section 4).

3. We experimentally prove the eﬀectiveness of TreeCert against publicly avail-
able datasets. Our results show that TreeCert is extremely precise, since it
can compute tight over-approximations of the actual error rate under attack,
with a diﬀerence of at most 0.02 over it on cases which are small enough to
be analyzed without approximated techniques. Moreover, TreeCert is much
faster than a competitor approach [5] and scales to intractable cases, avoiding
the exponential blow-up of non-approximated techniques (Section 5).

2 Background

2.1 Security of Supervised Learning

In this paper, we deal with the security of supervised learning, i.e., the task of
learning a classiﬁer from a set of labeled data. Formally, let X ⊆ Rd be a d-
dimensional space of real-valued features and Y be a ﬁnite set of class labels; a
classiﬁer is a function f : X → Y which assigns a class label to each element

Certifying Decision Trees Against Evasion Attacks by Program Analysis

3

of the vector space (also called instance). The correct label assignment for each
instance is modeled by an unknown function g : X → Y, called target function.
Given a training set of labeled data Dtrain = {(x1, g(x1)), . . . , (xn, g(xn))}
and a hypothesis space H, the goal of supervised learning is ﬁnding the classiﬁer
ˆh ∈ H which best approximates the target function g. Speciﬁcally, we let ˆh =
argminh∈H L(h, Dtrain), where L is a loss function which estimates the cost of
the prediction errors made by h on Dtrain. Once ˆh is found, its performance is
assessed by computing L(ˆh, Dtest), where Dtest is a test set of labeled, held-out
data drawn from the same distribution of Dtrain.

Within the context of security certiﬁcation, one should measure the accuracy
of ˆh by taking into account all the actions that an attacker could take to fool the
classiﬁer into mispredicting, a so-called evasion attack [1,2]. To provide a more
accurate evaluation of the performance of the classiﬁer under attack, the loss L
can thus be replaced by the loss under attack LA [21]. Formally, the attacker
can be modeled as a function A : X → 2X mapping each instance into a set of
perturbed instances which might fool the classiﬁer. The test set Dtest can thus
be corrupted into any dataset obtained by replacing each (xi, yi) ∈ Dtest with
any (x(cid:48)
i ∈ A(xi); we let A(Dtest) stand for the set of all such
datasets. The loss under attack LA is thus deﬁned by making the pessimistic
assumption that the attacker is able to craft the most damaging perturbations,
as follows:

i, yi) such that x(cid:48)

LA(ˆh, Dtest) = max

D(cid:48)∈A(Dtest)

L(ˆh, D(cid:48)).

(1)

Unfortunately, computing LA by enumerating A(Dtest) is intractable, given
the huge number of perturbations available to the attacker: for example, if the
attacker can ﬂip K binary features, then each instance can be perturbed in 2K
diﬀerent ways, leading to 2K · |Dtest| possible attacks.

2.2 Decision Trees

A powerful set of hypotheses H is the set of the decision trees [4]. We focus on
traditional binary decision trees, whose internal nodes perform thresholding over
feature values. Such trees can be inductively deﬁned as follows: a decision tree t
is either a leaf λ(ˆy) for some label ˆy ∈ Y or a non-leaf node σ(f, v, tl, tr), where
f ∈ [1, d] identiﬁes a feature, v ∈ R is the threshold for the feature f and tl, tr
are decision trees. At test time, an instance x = (x1, . . . , xd) traverses the tree t
until it reaches a leaf λ(ˆy), which returns the prediction ˆy, denoted by t(x) = ˆy.
Speciﬁcally, for each traversed tree node σ(f, v, tl, tr), x falls into the left tree tl
if xf ≤ v, and into the right tree tr otherwise.

Figure 1 represents an example decision tree, which assigns the instance (6,8)
with label −1 to its correct class. In fact, (i) the ﬁrst node checks whether the
second feature (whose value is 8) is lesser than or equal to 10 and then takes the
left sub-tree, and (ii) the second node checks whether the ﬁrst feature (whose
value is 6) is lesser than or equal to 5 and then takes the right leaf, classifying the
instance with label −1. However, note that an attacker who was able to corrupt

4

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

x2 ≤ 10

x1 ≤ 5

+1

+1

−1

Fig. 1. Example of decision tree

(6,8) into (5,8) could force the decision tree into changing its output, leading to
the prediction of the wrong class +1.

2.3 Abstract Interpretation

In the abstract interpretation framework, the concrete behavior of a program is
approximated through abstract values of a given abstract domain with a lattice
structure, rather than concrete values. For example, the Sign domain abstracts
numbers with their sign, as formalized by the following abstraction and con-
cretization functions (α and γ respectively):

α(V ) =






⊥ if V = ∅
+ if ∀v ∈ V : v > 0
0
if ∀v ∈ V : v = 0
− if ∀v ∈ V : v < 0
(cid:62) otherwise

γ(a) =






R
{n ∈ R | n > 0}
{0}
{n ∈ R | n < 0}
∅

if a = (cid:62)
if a = +
if a = 0
if a = −
if a = ⊥

Notice that for all sets of concrete values V ⊆ R we have V ⊆ γ(α(V )), i.e.,
the abstraction function provides an over-approximation of the concrete values.
Operations over concrete values like the sum operation + are over-approximated
by abstract counterparts ⊕ over the abstract domain, which deﬁne the abstract
semantics. For example, the sum of two positive numbers is certainly positive,
while the sum of a positive number and a negative number can be positive,
negative or 0; this lack of information is modeled by (cid:62). Hence, ⊕ is deﬁned such
that + ⊕ + = + and + ⊕ − = (cid:62). A sound deﬁnition of ⊕, here omitted, must
ensure that ∀V1, V2 ⊆ R : {v1 + v2 | v1 ∈ V1 ∧ v2 ∈ V2} ⊆ γ(α(V1) ⊕ α(V2)), i.e.,
abstract operations must over-approximate operations over concrete values. By
simulating the program over the abstract domain, abstract interpretation ensures
a fast convergence to an over-approximation of all the reachable program states.
In particular, the analysis consists in computing the ﬁxpoint of the abstract
semantics over the abstract domain, making use of a widening operator – usually
if the upper bound operator does not converge in a given threshold [9,10].

Certifying Decision Trees Against Evasion Attacks by Program Analysis

5

Thanks to its modular approach, abstract interpretation allows one to deﬁne
multiple abstractions of the same concrete domain. Therefore, several abstract
domains approximating numerical values have been proposed in the literature.
For instance Octagons [22] and Polyhedra [11] track diﬀerent types of (linear)
relations among numerical variables, and have been applied to diﬀerent con-
texts. Apron [18] is a library of numerical abstract domains comprising the main
domains leveraged in this work.

3 Security Veriﬁcation of Decision Trees

3.1 Threat Model

Our approach is general enough to be applied to attackers represented as arbi-
trary imperative programs. To exemplify, we show how it can be applied to an
expressive threat model based on rewriting rules [5]. This relatively new threat
model goes beyond traditional distance-based models, which are plausible for
perceptual tasks like image recognition, but are inappropriate for non-perceptual
tasks (e.g., loan assignment) where mathematical distances do not capture useful
semantic properties of the domain of interest.

We model the attacker A as a pair (R, K), where R is a set of rewriting rules,
deﬁning how instances can be corrupted, and K ∈ R+ is a budget, limiting the
amount of alteration the attacker can apply to each instance. Each rule r ∈ R
has form:

[a, b]

f
−→k [δl, δu],

where: (i) [a, b] and [δl, δu] are intervals on R ∪ {−∞, +∞}, with the former
deﬁning the precondition for the application of the rule and the latter deﬁning
the magnitude of the perturbation enabled by the rule; (ii) f ∈ [1, d] is the index
of the feature to perturb; and (iii) k ∈ R+ is the cost of the rule. The semantics
of the rewriting rule can be explained as follows: if an instance x = (x1, . . . , xd)
satisﬁes the condition xf ∈ [a, b], then the attacker can corrupt it by adding any
v ∈ [δl, δu] to xf and spending k from the available budget. The attacker can
corrupt each instance by using as many rewriting rules as desired in any order,
possibly multiple times, up to budget exhaustion.

According to this attacker model, we can deﬁne A(x), the set of the attacks

against the instance x, as follows.

Deﬁnition 1 (Attacks). Given an instance x and an attacker A = (R, K), we
let A(x) be the set of the attacks that can be obtained from x, i.e., the set of the
instances x(cid:48) such that there exists a sequence of rewriting rules r1, . . . , rn ∈ R
and a sequence of instances x0, . . . , xn where:

1. x0 = x and xn = x(cid:48);
2. for all i ∈ [1, n], the instance xi−1 can be corrupted into the instance xi by

using the rewriting rule ri, as described above;

3. the sum of the costs of r1, . . . , rn is not greater than K.

6

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

1

2

3

4

5

6

7

8

9

int predict ( ﬂoat [] x) {
if (x [2] <= 10) {
if (x [1] <= 5)
return +1;

else

return −1;

}
else

return +1;

10

}

Fig. 2. Translation of the decision tree in Figure 1 into an imperative program

Notice that x ∈ A(x) for any A by picking an empty sequence of rewriting rules.

Example 1. Consider the attacker A = ({r1, r2}, 10), where:

– r1 = [0, 10] 1−→5 [−1, 0] allows the attacker to corrupt the ﬁrst feature by
adding any value in [−1, 0], provided that the feature value is in [0, 10] and
the available budget is at least 5;

– r2 = [5, 10] 2−→4 [0, 1] allows the attacker to corrupt the second feature by
adding any value in [0, 1], provided that the feature value is in [5, 10] and
the available budget is at least 4.

The attacker A can force the decision tree in Figure 1 to change its original
prediction (−1) on the instance (6, 8). In particular, we can show that (5, 8) is a
possible attack against (6, 8), since A can apply r1 once by spending 5 from the
budget, and (5, 8) is classiﬁed as +1 by the decision tree.

3.2 Conversion to Imperative Program

Our analysis technique exploits the interpretability property of decision trees,
i.e., their amenability to be easily understood by human experts. In particular,
it is straightforward to convert any decision tree into an equivalent, loop-free
imperative program. To exemplify, Figure 2 shows the translation of the decision
tree in Figure 1 into an equivalent function.

We can then model the attacker as an imperative program which has access to
the function representing the decision tree to analyse. In particular, we observe
that the attacker A = (R, K) can be represented by means of a non-deterministic
program which behaves as follows:

1. Select a random rewriting rule r ∈ R.
2. Let [a, b]

f
−→k [δl, δu] be the selected rule r and let x = (x1, . . . , xd) be the
instance to perturb. If xf ∈ [a, b] and the available budget is at least k, then
select a random δ ∈ [δl, δu], replace xf with xf + δ and subtract k from the
available budget.

Certifying Decision Trees Against Evasion Attacks by Program Analysis

7

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

ﬂoat [] attack ( ﬂoat [] x) {

ﬂoat K = 10;
boolean done = false;
while (!done) {

int rule = random int(1,3);
switch ( rule ) {
case 1:

if (x [1] >= 0 && x[1] <= 10 && K >= 5) {

ﬂoat delta = random ﬂoat(−1,0);
x [1] = x[1] + delta;
K = K − 5;

}
break;

case 2:

if (x [2] >= 5 && x[2] <= 10 && K >= 4) {

ﬂoat delta = random ﬂoat(0,1);
x [2] = x[2] + delta;
K = K − 4;

}
break;

case 3:

// this models non−deterministic termination
done = true;

}

}
return x;

}

int predict under attack ( ﬂoat [] x) {

ﬂoat [] x’ = attack(x);
return predict (x’ );

}

Fig. 3. Encoding predictions under attack into an imperative program

3. Non-deterministically go to step 1 or terminate the process. This stop con-
dition allows the attacker to spare part of the budget, which is needed to
enforce termination when the entire budget cannot be spent (or does not
need to).

This encoding is exempliﬁed in Figure 3, where lines 1-27 show how the
attacker of Example 1 can be modeled as an imperative program, using standard
functions for random number generation. Once the attacker has been modeled,
we can ﬁnally encode the behavior of the decision tree under attack: this is shown
in lines 29-32, where we let the attacker corrupt the input instance before it is
fed to the decision tree for prediction.

8

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

3.3 Proving Security by Program Analysis

Given a decision tree t, an attacker A and a test set Dtest, we can compute an
over-approximation of LA(t, Dtest) as follows.

We ﬁrst translate the decision tree t together with the attacker A into an
imperative program P modeling the decision tree under attack, as discussed in
Section 3.2. For each instance (xi, yi) ∈ Dtest, we build an abstract state α({xi})
representing xi in the chosen abstract domain and we analyze P with such entry
state. Then, the output of the analysis might be either of the following:

1. only leaves of the decision tree with the correct class label yi are reachable.
This means that, for all possible attacks against xi, the decision tree always
classiﬁes the instance correctly;

2. leaves with the wrong label are reachable as well. If t correctly classiﬁes the
instance in the unattacked setting, this might happen either because there
is indeed an attack leading to a misprediction or for a loss of precision due
to the over-approximation of the static analysis.

Since our approach relies on sound static analysis engines, it is not possible
to miss attacks, i.e., every instance which can be mispredicted upon attack must
fall in the second case of our analysis. Let P #(xi) = Yi stand for the set of labels
Yi returned by the analysis of P on the instance xi.

By using this information, we can construct an abstraction of the behaviour

of t under attack on Dtest deﬁned as follows:

∀(xi, yi) ∈ Dtest : t#(xi) =

(cid:40)

yi
y (cid:54)= yi

if P #(xi) = {yi}
otherwise

By construction, we have that LA(t, Dtest) ≤ L(t#, Dtest) for any loss func-
tion which depends just on the number of mispredictions, like the error rate,
i.e., the fraction of wrong predictions among all the performed predictions. This
means that after building t# we have an eﬃcient way to over-approximate the
loss under attack LA by computing just a traditional loss L, which does not
require the computation of the set of attacks.

3.4 Extensions

We discuss here possible extensions of our approach to diﬀerent popular settings.
We leave the implementation of these extensions to future work, since they are
essentially an engineering eﬀort.

Regression. The regression task requires one to learn a regressor rather than
a classiﬁer from the training data. The key diﬀerence between a regressor and a
classiﬁer is that the former does not assign a class from a ﬁnite set Y, but rather
infers a numerical quantity from an unbound set, e.g., estimates the salary of
an employee based on her features. Regression can be modeled by revising the

Certifying Decision Trees Against Evasion Attacks by Program Analysis

9

Fig. 4. The architecture of TreeCert.

abstraction t# such that it returns an abstract value over-approximating all the
values in the predictions found in the leaves which are reachable upon attack.
Formally, this means requiring t#(xi) = (cid:116)yi∈P #(xi)α({yi}), where (cid:116) stands for
the upper bound operator on the abstract domain.

Tree Ensembles. Ensemble methods train multiple decision trees and combine
them to improve prediction accuracy. Traditional ensemble approaches include
random forest [3] and gradient boosting [14]. Irrespective of how an ensemble is
trained, its ﬁnal predictions are performed just by aggregating the predictions
of the individual trees, e.g., using majority voting or averaging. This means that
it is possible to readily generalize our analysis technique to ensembles by trans-
lating each tree therein and by aggregating their predictions in the generated
imperative program.

4

Implementation

Figure 4 depicts the architecture of TreeCert. The inputs are: (i) the attacker,
expressed in the threat model of Section 3.1 using a JSON ﬁle, (ii) a decision
tree to analyse, serialized through the joblib library, and (iii) a test set in CSV
format. TreeCert reports for each test instance whether it is always correctly
classiﬁed for each possible attack or it might be wrongly classiﬁed.

4.1 TreeCoder

The ﬁrst step of TreeCert is to encode the attacker and the decision tree as Java
programs through the module TreeCoder, as described in Section 3.2. TreeCoder
is a Python script that, given a JSON attacker model and a joblib decision tree,

10

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

produces two distinct Java ﬁles encoding the attacker (see method attack in
Figure 3) and the decision tree (see method predict in Figure 2).

There are only two small technical diﬀerences over the previous presentation.
First, given that all instances of the same dataset share the same set of features,
instances are not encoded as arrays, but rather modeled using a distinct local
variable for each feature, which simpliﬁes the static analysis; speciﬁcally, we let
variable xi represent the initial value of the i-th feature and variable x(cid:48)
i represent
its value after the attack. In addition, each time a rewriting rule r is applied,
we increment a counter r counter, initially set to 0, which allows one to capture
useful analysis invariants. Clearly, these changes do not aﬀect the semantics of
the generated program, so we did not include them in Figure 3 for simplicity.

4.2 AttackerAnalyzer

The encoded attacker is then passed to the AttackerAnalyzer module, a static
analyzer based on abstract interpretation. The analyzer interfaces with Apron,
a standard library implementing many popular abstract domains. The analyzer
then computes a ﬁxpoint over the Java program representing the attacker, using
the Polka implementation1 of the Polyhedra domain [11].

Polka tracks linear equalities and inequalities over an arbitrary number of
variables. These invariants allow AttackerAnalyzer to infer the upper and lower
bounds of each attacked feature, based on how many times a feature can be
attacked using the available budget. To exemplify, pick the attacker in Figure 3.
AttackerAnalyzer infers on such program that after the attack has been per-
formed: (i) the value of the ﬁrst feature may have been decreased by at most
r1 counter (formally, x(cid:48)
1 ∈ [x1 − 1 ∗ r1 counter, x1]), (ii) the second feature may
have been increased by at most r2 counter (x(cid:48)
2 ∈ [x2, x2 + 1 ∗ r2 counter]), (iii)
both the counters are non-negative (r1 counter ≥ 0 ∧ r2 counter ≥ 0), and (iv)
the budget spent in the application of the two rewriting rules is less than or equal
to the initial budget (5 ∗ r1 counter + 4 ∗ r2 counter ≤ 10). Note that the last
invariant is inferred only if the calculation of a ﬁxpoint over the abstract seman-
tics did not require to apply the Polyhedra widening operator to convergence.
Otherwise, the analysis would drop such information to ensure termination.

4.3 TreeAnalyzer

The attacker invariants are then passed to the TreeAnalyzer module together
with the test set. Like AttackerAnalyzer, TreeAnalyzer performs a static analysis
using the Polka implementation of the Polyhedra abstract domain. For each test
instance x, TreeAnalyzer (i) adds to the attacker invariants the initial values
of the features of x, (ii) computes the ﬁxpoint over the program encoding the
decision tree t under attack, and (iii) uses it to return the output of t#(x).

To clarify, consider again Example 1, where the test instance (6, 8) is correctly
classiﬁed as −1 by the decision tree in Figure 1, but can be misclassiﬁed upon

1 http://apron.cri.ensmp.fr/library/0.9.10/mlapronidl/Polka.html

Certifying Decision Trees Against Evasion Attacks by Program Analysis

11

1 ∈ [6 − r1 counter, 6] and x(cid:48)

attack. First of all, TreeAnalyzer adds the invariants x1 = 6 and x1 = 8 to
the inferred attacker invariants, leading to an initial Polyhedra state tracking
that x(cid:48)
2 ∈ [8, 8 + r2 counter] with 5 ∗ r1 counter +
4 ∗ r2 counter ≤ 10. Then the static analysis of the encoded tree starts with
the evaluation of the condition x(cid:48)
2 ≤ 10, inferring that such condition is always
evaluated to true: indeed, x2 could be greater than 10 only if r2 counter was
strictly greater than 2, but then 5 ∗ r1 counter + 4 ∗ r2 counter ≤ 10 could not
hold since r1 counter ≥ 0. TreeAnalyzer then analyzes the condition x(cid:48)
1 ≤ 5. In
this case, it cannot deﬁnitely conclude that the condition is always evaluated to
false, since x1 can be less than or equal to 5 if r1 counter ≥ 1, which is allowed
by the invariant 5 ∗ r1 counter + 4 ∗ r2 counter ≤ 10. TreeAnalyzer then concludes
that the test case might be wrongly classiﬁed, since a branch that classiﬁes the
test case with +1 could be reached.

5 Experimental Evaluation

5.1 Methodology

We evaluate our proposal on three public datasets: Census, House and Wine,
which are described in Section 5.2. Our methodology includes multiple steps. We
start with a preliminary threat modeling phase, where we deﬁne the attacker’s
capabilities by means of a set of rewriting rules R and a set of possible budgets
{K1, . . . , Kn}, as explained in Section 3.1. Our attackers are primarily designed
to perform an experimental evaluation of TreeCert, yet they are representative
of plausible attack scenarios which do not ﬁt traditional distance-based models
and are instead readily supported by the expressiveness of our threat model.

Datasets are divided into Dtrain and Dtest by using 90-10 spitting with strat-
iﬁed sampling (80-20 splitting is used for the smaller Wine dataset). We ﬁrst
train a decision tree t on Dtrain using the popular scikit-learn library, tuning
the maximum number of leaves in the set {21, 22, . . . , 210} through cross vali-
dation on Dtrain. We then evaluate the tree resilience to attacks against each
attacker A = (R, Ki) on Dtest, using a non-approximated technique. Given the
expressiveness of our threat model, the only available solution for this is the
algorithm in [5]. In particular, the algorithm computes A(xi), the set of repre-
sentative attacks against t, for each instance xi in Dtest. This is a comparatively
small subset of the attacks A(xi), which suﬃces to detect the successful eva-
sions attacks. We refer to this method as Representative Attacks. We observe
and we experimentally conﬁrm that computing even the representative attacks
is intractable in general, which motivates the need for approximated analyses,
yet being able to do it in a few cases is useful to assess the precision of TreeCert
against a ground truth.

Finally, we compute the abstraction t# on Dtest for each attacker A = (R, Ki)

by using TreeCert. This allows us to classify each (xi, yi) ∈ Dtest as follows:

– True Positive (TP ): TreeCert states that the instance xi can be misclassiﬁed
upon attack and this conclusion is correct. Formally, t#(xi) (cid:54)= yi ∧ ∃x(cid:48)
i ∈
A(xi) : t(x(cid:48)

i) (cid:54)= yi.

12

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

– False Positive (FP ): TreeCert states that the instance xi can be misclassiﬁed
upon attack, but this conclusion is wrong. Formally, t#(xi) (cid:54)= yi ∧ ∀x(cid:48)
i ∈
A(xi) : t(x(cid:48)

i) = yi.

– True Negative (TN ): TreeCert states that the instance xi cannot be mis-
classiﬁed upon attack and this conclusion is correct. Formally, t#(xi) =
yi ∧ ∀x(cid:48)

i ∈ A(xi) : t(x(cid:48)

i) = yi.

– False Negative (FN ): TreeCert states that the instance xi cannot be misclas-
siﬁed upon attack, but this conclusion is wrong. Formally, t#(xi) = yi∧∃x(cid:48)
i ∈
A(xi) : t(x(cid:48)

i) (cid:54)= yi.

Since our analysis is sound, we cannot have FN . We then assess the quality
of TreeCert by computing its False Positive Rate FPR and False Discovery Rate
FDR as follows:

FPR =

FP
FP + TN

,

FDR =

FP
FP + TP

.

We also compare the value of the loss under attack LA(t, Dtest) against its
over-approximation L(t#, Dtest), focusing on the error rate, i.e., the fraction of
wrong predictions. Finally, we compare the execution times of TreeCert against
the time spent in the computation of the set of the representative attacks.

5.2 Datasets

We perform our experiments on three publicly available datasets. The precon-
ditions of the rewriting rules and the magnitude of the perturbations have been
set after a preliminary data exploration step, based on the observed data distri-
bution in the dataset. Statistics about the datasets are in Table 1.

Census. The Census2 dataset includes demographic information about Amer-
ican citizens. The prediction task is estimating whether the income of a citizen
is above 50,000$ per year. For this dataset, we deﬁne four rewriting rules:

– cost 5: if the capital gain is in [0,100000], a citizen can raise it by 200;
– cost 5: if the capital loss is in [0,100000], a citizen can lower it by 200;
– cost 10: if the number of work hours is in [0,40], a citizen can raise it by 1;
– cost 10: if the age is in [0,40], a citizen can raise it by 1.

We consider 20, 40, 60, 80 as possible values of the attacker’s budget.

House. The House3 dataset contains house sale prices for the King County
area. The prediction task is inferring whether a house costs at least the median
house price. For this dataset, we deﬁne four rewriting rules:

2 http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.

data

3 https://www.kaggle.com/harlfoxem/housesalesprediction

Certifying Decision Trees Against Evasion Attacks by Program Analysis

13

Table 1. Properties of datasets used in the experiments.

Dataset #Instances #Features Maj. class
Census
House
Wine

29169
21613
6497

0.75
0.51
0.63

51
19
12

– cost 5: if the square footage of the living space of the house is in [0,3000], it

can be increased by 50;

– cost 5: if the square footage of the land space is in [0,2000], it can be increased

by 50;

– cost 5: if the average square footage of the living space of the 15 closest

houses is in [0,2000], it can be increased by 50;

– cost 5: if the construction year is in [1900,1970], it can be increased by 10.

We consider 10, 20, 30, 40 as possible values of the attacker’s budget.

Wine. The Wine4 dataset represents diﬀerent types of wines. The prediction
task is detecting whether a wine has quality score at least 6 on a scale 0–10. For
this dataset, we deﬁne four rewriting rules:

– cost 2: if the residual sugar is in [2,4], it can be lowered by 0.01;
– cost 5: if the alcohol level is in [0,11], it can be increased by 0.01;
– cost 5: if the volatile acidity is in [0,1], it can be lowered by 0.01;
– cost 5: if the free sulfur dioxide is in [20,40], it can be lowered by 0.1.

We consider 20, 30, 40, 50, 60 as possible values of the attacker’s budget.

5.3 Experimental Results

Precision. Table 2 reports for all datasets and budgets a number of measures
computed for the trained decision tree t:

1. the traditional loss in absence of attacks L(t, Dtest). This is the fraction of

wrong predictions returned by t on Dtest in the unattacked setting;

2. the loss under attack LA(t, Dtest), computed by enumerating all the repre-
sentative attacks using the algorithm in [5]. This is the fraction of wrong
predictions returned by t on Dtest upon attack;

3. the over-approximation of the loss under attack L(t#, Dtest), computed using

the program analysis of TreeCert;

4. the false positive rate of TreeCert, noted FPR;
5. the false discovery rate of TreeCert, noted FDR.

The experimental results clearly conﬁrm the quality of the analysis performed
by TreeCert. In particular, we observe that the FPR is remarkably low, stand-
ing well below 5%, where 10% is considered a state-of-the-art reference for static

4 https://www.openml.org/data/get_csv/49817/wine_quality.arff

14

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

Table 2. Accuracy results across dataset.

Dataset Budget L(t, Dtest) LA(t, Dtest) L(t#, Dtest) FPR FDR
0.00
0.01
0.01
0.01

0.14
0.14
0.14
0.14

0.17
0.17
0.18
0.20

0.17
0.17
0.18
0.21

0.00
0.00
0.00
0.00

20
40
60
80

Census

House

Wine

10
20
30
40

20
30
40
50
60

0.10
0.10
0.10
0.10

0.24
0.24
0.24
0.24
0.24

0.12
0.14
0.16
0.18

0.30
0.34
0.36
0.37
0.38

0.12
0.15
0.17
0.19

0.31
0.35
0.37
0.39
0.40

0.00
0.01
0.01
0.02

0.01
0.02
0.02
0.03
0.03

0.02
0.04
0.06
0.08

0.02
0.03
0.04
0.05
0.05

analysis techniques [24]. Indeed, in Census we measured an absolute number of
false positives never greater than 5. This is interesting, because it shows that for
many instances there is a simple security proof, i.e., TreeCert is able to prove
that they cannot be successfully attacked (i.e., they are TN ), which signiﬁcantly
drops the FPR. As to the FDR, we observe that it also scores extremely well on
all datasets, though it tends to be slightly higher than FPR. However, this is not
a major problem in our application setting: contrary to what happens in tradi-
tional program analysis, where users are forced to investigate all false alarms to
identify possible bugs, here we are rather interested in the aggregated analysis
results, i.e., the ﬁnal over-approximation of the loss under attack. Even on the
House dataset, where FDR tends to be higher, we observe that the loss under at-
tack is appropriately approximated by TreeCert, since there is a diﬀerence of at
most 0.01 between the actual value and its over-approximation. Remarkably, our
experiments also show that the quality of the over-approximation is not signiﬁ-
cantly aﬀected by the attacker’s budget, which is important because it suggests
that TreeCert likely generalizes to cases where computing the actual value of the
loss under attack is computationally intractable, which is the intended use case
of such an analysis tool.

Eﬃciency. To show the eﬃciency of our approach, we compare in Figure 5 the
running time of TreeCert against the time taken to compute the full set of the
representative attacks. It is possible to clearly see that the two curves exhibit
completely diﬀerent trends. The time taken to construct the representative at-
tacks has an exponential trend: the approach is eﬃcient and feasible when the
attacker’s budget is low, but blows up to intractability very quickly. For ex-
ample, each increase in the attacker’s budget multiplies the execution time of
a 3x factor in the case of Census and we experimentally conﬁrmed that more

Certifying Decision Trees Against Evasion Attacks by Program Analysis

15

Fig. 5. Running time of TreeCert against the enumeration of representative attacks.

than 12 hours of computation are needed when the budget grows to 100 (not
plotted). Conversely, the execution time of TreeCert is only marginally aﬀected
when increasing the attacker’s budget, since the analysis always converges in
less than one hour. In the case of the House dataset, computing the set of the
representative attacks is even less feasible: even for small budgets, the running
time is remarkably high, due to the fact that the trained decision tree uses many
diﬀerent thresholds, which makes the number of representative attacks blow up.
Finally, also the Wine dataset shows similar ﬁgures, though the execution times
there are lower due to its smaller size. This conﬁrms that brute-force approaches
based on the exhaustive enumeration of the representative attacks do not scale,
yet luckily they can be replaced by more eﬃcient abstraction techniques with
good precision.

6 Related Work

Verifying the security guarantees of machine learning models is an important
task, which received signiﬁcant attention by the research community in the last
few years. In particular, many papers proposed techniques to verify the security
of deep neural networks [28,27,17,20,15]; we refer to a recent survey for more
work in this research area [29]. As of now, however, comparatively less attention
has been received by the security veriﬁcation of decision trees models.

The closest related work to our approach is a very recent paper by Ranzato
and Zanella [23]. Their work also focuses on decision trees and builds on the ab-
stract interpretation framework. However, their approach can only be applied to
attackers who admit a simple mathematical characterization as a set of pertur-
bations, e.g., based on distances. In particular, their soundness theorem relies on
the hypothesis that, for each test instance x, one has A(x) ⊆ γ(α({x})), i.e., the
abstraction of x must cover all the possible attacks. Checking this condition for
distance-based attackers is straightforward, yet it is computationally infeasible
in general. For example, in the case of the rewriting rules we considered, A(x) is
unknown a priori, but is induced by the application of the rules. Indeed, their

20406080Budget0100200300Time (minutes)Census10203040Budget0100200300400Time (minutes)House2030405060Budget01020304050Time (minutes)WineRepresentative AttacksTreeCert16

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

tool silva only supports attackers based on the inﬁnity-norm L∞, which has a
compact mathematical characterization as a set, but falls short of representing
realistic threats. Instead, our approach is general enough to work on attackers
modeled as arbitrary imperative programs.

Other approaches also deal with the veriﬁcation of decision trees, but are not
based on abstract interpretation. For example, Einzinger et al. use SMT solving
to verify the robustness of gradient-boosted models [13]. Their approach also
requires to explicitly encode the set of attacks A(x) in closed form, which is
only easily doable for artiﬁcial distance-based attackers. Moreover, SMT solving
suﬀers from scalability issues, which required the authors to develop custom
optimizations to make their approach practical. It is unclear whether this line of
work can be adapted and scale to more expressive attackers or not, also because
their tool is not publicly available. Other notable work include the robustness
veriﬁcation algorithm by Chen et al. [8], which only works for attackers based
on the inﬁnity-norm L∞, and the abstraction-reﬁnement approach by T¨ornblom
and Nadjm-Tehrani [26], which is not proved sound.

Finally, it is worth mentioning adversarial learning algorithms which train
decision trees more resilient to evasion attacks by construction [19,5,7,6]. This
line of work is orthogonal to the security veriﬁcation of decision trees, i.e., our
approach can also be applied to estimate the improved robustness guarantees of
trees trained using such algorithms.

7 Conclusion

We proposed a technique to certify the security of decision trees against evasion
attacks by leveraging the abstract interpretation framework. This is the ﬁrst
solution which is both sound and expressive enough to deal with sophisticated
attackers represented as arbitrary imperative programs. Our experiments showed
that our technique is both precise and eﬃcient, yielding only a minimal number
of false positives and scaling up to cases which are intractable for a competitor [5].
We foresee several avenues for future work. First, we plan to extend our
approach to the analysis of regression tasks and tree ensembles: though this is
straightforward from an engineering perspective, we want to analyze the preci-
sion and the eﬃciency of our solution in such settings. Moreover, we will inves-
tigate techniques to automatically infer the minimal attacker’s budget required
to induce a given error rate on the test set, so as to eﬃciently provide security
analysts with this useful information. Finally, we will investigate the trade-oﬀ be-
tween the precision and the eﬃciency of TreeCert by testing more sophisticated
abstract domains and analysis techniques, e.g., trace partitioning.

References

1. Biggio, B., Corona, I., Maiorca, D., Nelson, B., Srndic, N., Laskov, P., Giacinto,
G., Roli, F.: Evasion attacks against machine learning at test time. In: Proceedings
of ECML PKDD. pp. 387–402 (2013)

Certifying Decision Trees Against Evasion Attacks by Program Analysis

17

2. Biggio, B., Roli, F.: Wild patterns: Ten years after the rise of adversarial machine

learning. Pattern Recognit. 84, 317–331 (2018)

3. Breiman, L.: Random forests. Machine Learning 45(1), 5–32 (2001)
4. Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.: Classiﬁcation and Regres-

sion Trees. Wadsworth (1984)

5. Calzavara, S., Lucchese, C., Tolomei, G.: Adversarial training of gradient-boosted

decision trees. In: Proceedings of CIKM. ACM (2019)

6. Calzavara, S., Lucchese, C., Tolomei, G., Abebe, S.A., Orlando, S.: Treant: Training
evasion-aware decision trees. CoRR abs/1907.01197 (2019), http://arxiv.org/
abs/1907.01197

7. Chen, H., Zhang, H., Boning, D.S., Hsieh, C.: Robust decision trees against adver-

sarial examples. In: Proceedings of ICML. PMLR (2019)

8. Chen, H., Zhang, H., Si, S., Li, Y., Boning, D.S., Hsieh, C.: Robustness veriﬁcation

of tree-based models. In: Proceedings of NeurIPS. pp. 12317–12328 (2019)

9. Cousot, P., Cousot, R.: Abstract Interpretation: A Uniﬁed Lattice Model for Static
Analysis of Programs by Construction or Approximation of Fixpoints. In: Proceed-
ings of POPL. ACM (1977)

10. Cousot, P., Cousot, R.: Systematic Design of Program Analysis Frameworks. In:

Proceedings of POPL. ACM (1979)

11. Cousot, P., Halbwachs, N.: Automatic discovery of linear restraints among variables

of a program. In: Proceedings of POPL. ACM Press (1978)

12. Dreossi, T., Jha, S., Seshia, S.A.: Semantic adversarial deep learning. In: Proceed-

ings of CAV. Springer (2018)

13. Einziger, G., Goldstein, M., Sa’ar, Y., Segall, I.: Verifying robustness of gradient
boosted models. In: Proceedings of AAAI. pp. 2446–2453. AAAI Press (2019)
14. Friedman, J.H.: Greedy function approximation: a gradient boosting machine. An-

nals of statistics pp. 1189–1232 (2001)

15. Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., Vechev,
M.T.: AI2: safety and robustness certiﬁcation of neural networks with abstract
interpretation. In: Proceedings of Security and Privacy. IEEE Computer Society
(2018)

16. Goodfellow, I., McDaniel, P., Papernot, N.: Making machine learning robust

against adversarial inputs. Commun. ACM 61(7) (Jul 2018)

17. Huang, X., Kwiatkowska, M., Wang, S., Wu, M.: Safety veriﬁcation of deep neural

networks. In: Proceedings of CAV. Springer (2017)

18. Jeannet, B., Min´e, A.: Apron: A library of numerical abstract domains for static

analysis. In: Proceedings of CAV. Springer (2009)

19. Kantchelian, A., Tygar, J.D., Joseph, A.D.: Evasion and hardening of tree ensemble

classiﬁers. In: Proceedings of ICML. JMLR.org (2016)

20. Katz, G., Barrett, C.W., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: An
eﬃcient SMT solver for verifying deep neural networks. In: Proceedings of CAV.
Springer (2017)

21. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning
models resistant to adversarial attacks. In: Proceedings of ICLR. OpenReview.net
(2018)

22. Min´e, A.: The octagon abstract domain. Higher-Order and Symbolic Computation

(2006)

23. Ranzato, F., Zanella, M.: Abstract interpretation of decision tree ensemble classi-

ﬁers. In: Proceedings of AAAI. AAAI Press (2020)

24. Sadowski, C., Aftandilian, E., Eagle, A., Miller-Cushon, L., Jaspan, C.: Lessons
from building static analysis tools at google. Commun. ACM 61(4) (Mar 2018)

18

Stefano Calzavara, Pietro Ferrara, and Claudio Lucchese

25. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.J.,
Fergus, R.: Intriguing properties of neural networks. In: Proceedings of ICLR (2014)
26. T¨ornblom, J., Nadjm-Tehrani, S.: An abstraction-reﬁnement approach to formal
veriﬁcation of tree ensembles. In: Proceedings of SAFECOMP. Springer (2019)
27. Wang, S., Pei, K., Whitehouse, J., Yang, J., Jana, S.: Eﬃcient formal safety analysis

of neural networks. In: Proceedings of NeurIPS 2018 (2018)

28. Wang, S., Pei, K., Whitehouse, J., Yang, J., Jana, S.: Formal security analysis
of neural networks using symbolic intervals. In: Proceedings of USENIX Security.
USENIX Association (2018)

29. Xiang, W., Musau, P., Wild, A.A., Lopez, D.M., Hamilton, N., Yang, X., Rosen-
feld, J.A., Johnson, T.T.: Veriﬁcation for machine learning, autonomy, and neural
networks survey. CoRR abs/1810.01989 (2018), http://arxiv.org/abs/1810.
01989

