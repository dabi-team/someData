1
2
0
2

r
a

M
3

]

G
L
.
s
c
[

2
v
3
6
1
4
0
.
6
0
0
2
:
v
i
X
r
a

GENERALIZED SPECTRAL CLUSTERING VIA
GROMOV-WASSERSTEIN LEARNING

Samir Chowdhury
Stanford University

Tom Needham
Florida State University

March 4, 2021

ABSTRACT

We establish a bridge between spectral clustering and Gromov-Wasserstein Learning (GWL), a recent
optimal transport-based approach to graph partitioning. This connection both explains and improves
upon the state-of-the-art performance of GWL. The Gromov-Wasserstein framework provides prob-
abilistic correspondences between nodes of source and target graphs via a quadratic programming
relaxation of the node matching problem. Our results utilize and connect the observations that the
GW geometric structure remains valid for any rank-2 tensor, in particular the adjacency, distance,
and various kernel matrices on graphs, and that the heat kernel outperforms the adjacency matrix in
producing stable and informative node correspondences. Using the heat kernel in the GWL framework
provides new multiscale graph comparisons without compromising theoretical guarantees, while
immediately yielding improved empirical results. A key insight of the GWL framework toward
graph partitioning was to compute GW correspondences from a source graph to a template graph
with isolated, self-connected nodes. We show that when comparing against a two-node template
graph using the heat kernel at the inﬁnite time limit, the resulting partition agrees with the partition
produced by the Fiedler vector. This in turn yields a new insight into the k-cut graph partitioning
problem through the lens of optimal transport. Our experiments on a range of real-world networks
achieve comparable results to, and in many cases outperform, the state-of-the-art achieved by GWL.

1

INTRODUCTION

The Gromov-Wasserstein (GW) problem is a nonconvex quadratic program whose solution yields the GW distance, a
variant of Wasserstein distance from classical optimal transport (OT) which is able to compare distributions deﬁned on
different metric spaces. This is accomplished by replacing classical Wasserstein loss with a loss function deﬁned in
terms of relational information coming from metric data. Because it is able to compare distributions deﬁned on a priori
incomparable spaces, GW distance is increasingly ﬁnding applications for learning problems on irregular domains such
as graphs (Hendrikson, 2016; Tsitsulin et al., 2018; Vayer et al., 2019a; Xu et al., 2019b; Xu, 2020). In this context,
a graph can be considered as a metric space by endowing it with geodesic distance. A soft matching between nodes
of two different graphs is obtained by choosing distributions on each graph’s nodes (e.g., uniform distributions) and
computing the GW optimal transport plan between them.

Applications of GW distance have been bolstered by the observation that the GW problem does not fundamentally
require a metric to operate (Peyr´e et al., 2016); i.e., the deﬁnition of the GW loss function extends to other forms of
relational data (Chowdhury and M´emoli, 2019). Xu et al. (2019a) used this observation to produce the state-of-the-art
Scalable Gromov-Wasserstein Learning framework for graph matching and partitioning, which fundamentally uses
the the adjacency matrix (as opposed to the shortest path distance matrix) of a graph. There are many ways to derive
relational data from a graph beyond its distance and adjacency matrices, such as its various graph Laplacians and their
corresponding heat kernels. A limitation of the current literature is a lack of tools for ascertaining if the adjacency
matrix (or any other rank-2 tensor derived from a graph) is optimal in some sense beyond empirical benchmarks. Thus
the potential ﬂexibility of the GW framework for graph analysis remains largely unexplored.

Here we study the GW graph OT problem by representing graphs via heat kernels rather than adjacency matrices. This
amounts to ﬁnding soft correspondences between the nodes of two graphs by comparing spectral, rather than adjacency,
data. We refer to this as the SpecGWL framework, reserving GWL for the adjacency-based framework of Xu et al.
(2019a).

 
 
 
 
 
 
MARCH 4, 2021

Numerical experiments1 demonstrate that SpecGWL outperforms GWL in graph partitioning tasks. Moreover, a
main goal of this paper is to introduce tools for studying the GW problem in a more rigorous manner. We use a
Markov Chain sampling technique to explore the energy landscape of the GW loss function for adjacency and heat
kernel graph representations, showing empirically that SpecGWL loss has fewer spurious local minima and a 10x
acceleration in convergence of gradient descent over GWL loss. We also introduce a visualization technique which
allows one to ascertain the quality of soft node matchings obtained by any GW method. Examples of this technique
intuitively demonstrate the idea that heat kernel-based matchings more faithfully preserve global graph structure
than adjacency-based matchings. Finally, we establish theoretical results on the sparsity of optimal couplings in the
SpecGWL framework and on the precise relation of SpecGWL graph partitioning to classical spectral clustering. The
latter result creates a novel connection between spectral clustering and optimal transport.

Related literature. Gromov-Wasserstein distance—as used in current ML settings—was introduced by M´emoli (2007)
as a convex relaxation scheme for the Gromov-Hausdorff distance between different metric spaces. Further theoretical
development was carried out by M´emoli (2011a); Sturm (2012), and a related formulation was introduced by Sturm
(2006) to study the convergence of sequences of metric measure spaces. The idea of using heat kernels for GW
matching goes back to the Spectral Gromov-Wasserstein distance introduced by M´emoli (2011b) in the context of
Riemannian manifolds to further develop the notion of Shape-DNA introduced by Reuter et al. (2006). A related
theoretical construction also appeared in Kasue and Kumura (1994). The Riemannian heat kernel has been celebrated
for its multiscale and informative properties (Sun et al., 2009)—the former refers to the observation that the heat
kernel deﬁnes a family of Gaussian ﬁlters that get progressively shorter and wider as t → ∞, and the latter refers to
the classical lemma of Varadhan showing that at the small time limit, the log-heat kernel approximates the geodesic
distance on a Riemannian manifold. A surprising result due to Sun et al. (2009) in this direction is that under mild
conditions, the collection of traces of the heat kernel forms an isometry invariant of a manifold despite giving up most
of the information contained in the heat kernel. Heat kernel traces have recently been used for graph comparison by
Tsitsulin et al. (2018, 2020), who showed that the desirable properties of the heat kernel for Riemannian manifolds have
natural and informative analogues in the setting of graphs. Heat kernels have been incorporated into GW-based graph
matching in recent work of Barbe et al. (2020), where they are used to augment the matching process in the Fused GW
framework of Vayer et al. (2018). Our work is distinguished from that of Barbe et al. (2020) in that we use heat kernels
directly to deﬁne a GW loss function for graph matching, whereas Barbe et al. (2020) employs heat kernels to improve
feature space matchings for attributed graphs. There is a deep literature on spectral graph comparison, and a few other
references include works of Patro and Kingsford (2012); Bronstein and Glashoff (2013); Hu et al. (2014); Nassar et al.
(2018); Dong and Sawin (2020).

Starting from its early applications in computer vision (M´emoli, 2007, 2011a; Schmitzer and Schn¨orr, 2013), GW
distance has been applied to alignment of word embedding spaces (Alvarez-Melis and Jaakkola, 2018), learning of
generative models across different domains (Bunne et al., 2019), graph factorization (Xu et al., 2019b), and learning
autoencoders (Xu et al., 2020). Related theoretical directions include the sliced GW of Vayer et al. (2019b) and the
Gromov-Monge problem studied by M´emoli and Needham (2018). Sturm (2012) studied the structure of geodesics and
gradient ﬂows in GW geometry, and recently these techniques were utilized by Chowdhury and Needham (2020) to
create a Riemannian framework for performing averaging and tangent PCA across different graph-structured domains.
Unbalanced formulations of the GW problem permitting input probability measures of different total sums have been
studied by Chapel et al. (2020); De Ponti and Mondino (2020); S´ejourn´e et al. (2020).

2 SPECTRAL GW DISTANCES

Gromov-Wasserstein Distance. The following can be stated for Borel probability measures on Polish spaces; however,
we are interested in the ﬁnite setting where measure-theoretic complications do not arise, so we freely use matrix-vector
notation. Given probability distributions p, q on ﬁnite sets X, Y , a coupling of p and q is a joint probability measure
C on X × Y with marginals p and q; i.e., C = (Cij) ∈ R|X|×|Y | satisﬁes equality constraints C1|Y |×1 = p and
C T 1|X|×1 = q (1m×n denoting the m × n matrix of all ones), and entrywise inequality constraints 0 (cid:22) C (cid:22) 1. The set
of all couplings of p and q is denoted C(p, q)—this is a convex polytope in R|X|×|Y |. Given functions f X : X ×X → R,
f Y : Y × Y → R, written as square matrices F X , F Y , the GW problem solves:

min
C∈C(p,q)

(cid:88)

(cid:88)

i,k

j,l

(F X

ik − F Y

jl )2CijCkl.

(1)

The GW problem is a nonconvex quadratic program over a convex domain for which approximate solutions may be
obtained via projected gradient descent (Peyr´e et al., 2016) or Sinkhorn iterations with an entropy (Solomon et al.,

1https://github.com/trneedham/Spectral-Gromov-Wasserstein

2

2016) or KL divergence regularizer (Xu et al., 2019b). Computational implementations can be found in the Python
Optimal Transport library of Flamary and Courty (2017).

MARCH 4, 2021

Historically, the GW problem was deﬁned by M´emoli (2007) in the setting of metric measure (mm) spaces, where its
solution leads to a metric known as the Gromov-Wasserstein distance. A ﬁnite mm space (X, d, p) consists of a ﬁnite
set of points X, a metric function d written as a matrix d = (dik) ∈ R|X|×|X|, and a probability distribution p on X
written as a vector p = (pi) ∈ [0, 1]|X|×1. Given mm spaces (X, dX , p), (Y, dY , q), the Gromov-Wasserstein distance
is deﬁned as:
(cid:88)

(cid:88)

dGW(X, Y )2 := min

(dX

ik − dY

jl)2CijCkl.

(2)

C∈C(p,q)

i,k

j,l

Peyr´e et al. (2016) observed that solving the GW problem (1) with input matrices that are not strictly distances—in
particular kernel matrices—still leads to a discrepancy that is informative when comparing matrices of different sizes
and arising in incompatible domains. This idea was pushed further by the theoretical work of Chowdhury and M´emoli
(2019), which showed that any square matrix representation of a graph, including the adjacency matrix, can be used
in the GW problem (1) to obtain a bona ﬁde distance (actually a pseudometric). This observation was used by Xu
et al. (2019b,a) to create a uniﬁed Gromov-Wasserstein Learning (GWL) framework for unsupervised learning tasks on
graphs—e.g. ﬁnding node correspondences between unlabeled graphs and graph partitioning—with state-of-the-art
performance. We now formulate the GWL framework precisely.

Gromov-Wasserstein Learning on Graphs. Let G = (V, E) be a ﬁnite, unweighted, possibly directed graph. We
refer to V as the set of nodes and E as the set of edges. Let A : V × V → {0, 1} and D : V → Z denote the adjacency
and degree functions deﬁned as A(v, w) := 1 if (v, w) ∈ E, 0 otherwise, and D(v) := |{w : (v, w) ∈ E}|. Given
an ordering on V , these functions can be represented as matrices in R|V |×|V |, and we will switch between both the
function and matrix forms. In addition to the adjacency function, which fully encodes a graph, there are numerous
derived representations which capture information about a graph. An example is the geodesic distance function d which
contains all the shortest path lengths in the graph. In particular, the graph geodesic distance representation was used
by Hendrikson (2016) to solve graph matching problems via the original mm-space GW distance formulation (2) of
M´emoli (2007).

One of the key ideas in the GWL framework is to use adjacency matrices in the GW problem. Speciﬁcally, let G and H
be graphs with distributions p and q on their nodes and let AG and AH denote their adjacency matrices. The GWL
framework considers the loss

Adj = AdjG,p,H,q : C(p, q) → R

deﬁned by

Adj(C) =

(cid:88)

(cid:88)

i,k

j,l

(AG

ik − AH

jl )2CijCkl,

(3)

which we refer to as adjacency loss. The minimum of Adj(C)1/2 deﬁnes a distance between the pairs (G, p) and (H, q)—
we refer to such pairs as measure graphs and assume for convenience that distributions are fully supported. If p and q
are themselves derived from adjacency data then the minimizer C of (3) provides a natural soft correspondence between
the nodes of G and H. For example, Xu et al. (2019a) consider the family of node distributions p = (p1, . . . , pn)T ,
where

pj =

pj
k=1 pk

(cid:80)n

,

pj = (deg(vj) + a)b,

(4)

where pj = p(vj) for vj a vertex of G, a ≥ 0 is used to enforce the full support condition and the exponent b ∈ [0, 1]
allows interpolation between the uniform distribution and the degree distribution.

Heat Kernels. Our contributions begin by studying the structure imposed on the GW problem (1) by spectral losses that
we describe next. Given an undirected graph G = (V, E), let L2(V ) denote the linear space of functions f : V → R.
The Laplacian of G is the operator L : L2(V ) → L2(V ) deﬁned by

L(φ)(v) := D(v)φ(v) −

(cid:88)

φ(w).

(v,w)∈E

After ﬁxing an ordering on V , we can use matrix-vector notation to write R|V |×|V | (cid:51) L = D − A and φ ∈ R|V |×1. The
Laplacian is symmetric positive semideﬁnite, so the spectral theorem guarantees an eigendecomposition L = ΦΛΦT
with real, nonnegative eigenvalues λ1 ≤ λ2 ≤ . . . ≤ λn arranged on the diagonal of Λ. The corresponding eigenvectors
φ1, φ2, . . . , φn are arranged as the columns of Φ. There are several variants of the Laplacian with analogous properties,
including the normalized Laplacian given by I − D−1/2AD−1/2. This is the version that we will typically use for
experiments.

3

MARCH 4, 2021

For a (strongly connected) directed graph G = (V, E), we use the normalized Laplacian deﬁned by Chung (2005)
via the language of random walks. Consider the transition probability matrix P deﬁned by writing Pij = 1/Di if
(i, j) ∈ E, 0 otherwise. By Perron-Frobenius theory, there is a unique left eigenvector ψ with all entries positive such
that ψT P = ψT . The directed graph Laplacian is deﬁned as L := I − (Ψ1/2P Ψ−1/2 + Ψ−1/2P T Ψ1/2)/2, where
Ψ = diag(ψ).

In what follows, we use L generically to refer to any of the Laplacians deﬁned above. The heat equation on a graph G
(either undirected or directed) is then given as du/dt = −Lu, where u ∈ L2(V ×R>0). If u represents a time-dependent
heat distribution on the nodes of G, the heat equation describes heat diffusion according to Newton’s Law. The heat
kernel is the fundamental solution to this heat equation, given in closed form as K t = exp(−tL) = Φ exp(−tΛ)ΦT .

Spectral GW Distance. Given measure graphs (G, p) and (H, q), we consider the spectral loss
Spect = Spect

G,p,H,q : C(p, q) → R

deﬁned by

Spect(C) =

(cid:88)

(cid:88)

i,k

j,l

(K G,t

ik − K H,t

jl )2CijCkl

(5)

for each t > 0, where K G,t and K H,t are the heat kernels of G and H, written in matrix form. We then obtain a one
parameter family of pseudometrics

dspec
GW [t]((G, p), (H, q)) := min

C∈C(p,q)

Spect(C)1/2

(6)

on the space of measure graphs. As in the GWL framework, choosing node distributions p and q from the family (4)
yields minimizing couplings C which give meaningful correspondences between the nodes of G and H. As t varies,
one obtains multiscale couplings between nodes, with small t encoding local and large t encoding global structure (this
is made precise in Section 3). Intuitively, the heat kernel replaces each node of a graph with a Gaussian ﬁlter with
width controlled by t, progressively “smoothing” the graph. Different smoothing levels emphasize multiscale geometric
and topological features that drive coupling optimization; Figure 6 in the Supplementary Materials shows clusters in
coupling space corresponding to matchings at different scales. The “best” choice of t emphasizes the feature scale
which is most important for a given task.

One can further deﬁne an overall pseudometric by, say, considering the function

fG,p,H,q(t) = dspec

GW [t]((G, p), (H, q))
and deﬁning dspec
GW := (cid:107)ξ · fG,p,H,q(cid:107) for an appropriately chosen norm on functions R → R and normalizing function ξ
preventing blow up of the norm; for example, taking ξ(t) = exp(−(t + t−1)) and the (cid:96)∞ norm here is analogous to the
spectral GW distance between Riemannian manifolds deﬁned by M´emoli (2011b). This is an interesting direction of
future research, but we found it most useful in our applications to instead treat t as a scale parameter that is tuned via
cross-validation.
Properties of SpecGWL. For measure graphs (G, p) and (H, q), ﬁx t > 0 and write J := K G,t, K := K H,t. After
expanding the square and invoking the marginalization constraints (Solomon et al. (2016) provide an explicit derivation),
one sees that minimizing (5) is equivalent to maximizing (cid:104)JC, CK(cid:105) subject to C ∈ C(p, q), where (cid:104)·, ·(cid:105) denotes the
Frobenius inner product. Because the graph heat kernel is symmetric positive deﬁnite, we take Cholesky decompositions
J = U T U , K = V T V to write

(cid:104)JC, CK(cid:105) = tr((JC)T CK) = tr(C T U T U CV T V )

= tr(V C T U T U CV T ) = (cid:107)U CV T (cid:107)2.

The map C (cid:55)→ (cid:107)U CV T (cid:107)2 is convex. We record this as the following lemma, which appeared previously as (Alvarez-
Melis et al., 2019, Lemma 4.3).
Lemma 1. For each t > 0, spectral loss (5) is minimized over the convex polytope C(p, q) by a maximizer of the convex
function C (cid:55)→ (cid:104)K G,tC, CK H,t(cid:105).

Optimization problems of this type are not tractable to solve deterministically, but we demonstrate experimentally that
approximation via gradient descent enjoys faster convergence and fewer spurious local minima than adjacency loss (3).
It has been empirically observed by Xu et al. (2019b) that (local) minimizers of adjacency loss tend to become sparse,
and this sparsity is crucial in the gradient descent-based algorithm of Chowdhury and Needham (2020) for computing
averages of networks. A key advantage of our spectral setting (5) over (2) or (3) is that this empirical observation of
sparsity admits a formal proof.
Theorem 2. Let (G, p) and (H, q) be measure graphs on ∼ n nodes. Then for any t > 0, there is a minimizer of
spectral loss with o(n) nonzero entries.

4

MARCH 4, 2021

Complexity of SpecGWL. Assuming graphs of comparable size n, the eigendecomposition incurs a time complexity
of O(n3) and memory complexity of Θ(n2). Computing the GW loss using gradient descent involves computing
∇(cid:104)JC, CK(cid:105) = J T CK + JCK T , which incurs a time complexity of O(n3 log(n)) (Kolouri et al., 2017) and memory
complexity of O(n2). Regularized methods with Sinkhorn iterations still require paying a cost for matrix multiplication
(Peyr´e et al., 2016). However, accelerations have already been proposed: Tsitsulin et al. (2018) suggest methods for
approximating heat kernels in O(n2) operations and Xu et al. (2019a) propose a recursive divide-and-conquer approach
to reduce the complexity of the GW comparison to O(n2 log n). Note that because heat kernel matrices are dense, we
lose the advantages of sparse matrix operations.

3 GRAPH PARTITIONING

Graph Partitioning Method. Graph partitioning is a crucial unsupervised learning task used for community detection
in social and biological networks (Girvan and Newman, 2002). The goal is to partition the vertices of a graph into some
number of clusters m in accordance with the maximum modularity principle—edges within clusters are dense, while
edges between clusters are sparse. Xu et al. (2019a) proposed a GW-based approach to graph partitioning where an
m-way partition of a measured graph (G, p) is obtained by minimizing the following variant of adjacency loss (3):
(cid:88)

(cid:88)

C(p, q) (cid:51) C (cid:55)→

(Aik − Qjl)2CijCkl,

(7)

i,k

j,l

where A is the adjacency matrix of G, Q = diag(q) and q is a distribution estimated by sorting the weights of p,
sampling m values via linear interpolation and renormalizing. Intuitively, Q is the weighted adjacency matrix of a
graph on m nodes with only self-loops—an ideally clustered template graph. The heuristic for choosing the distribution
q in this manner is that if within-cluster nodes of the graph G have similar degrees then this method allows a node in Q
to accept all of the mass from this cluster. A minimizer C of (7) deﬁnes an m-way partition of G: each node vi of G is
assigned a label in {1, . . . , m} according to the column index of the maximum weight in row i of C.

Soft-matching nodes of the target graph to an ideally clustered template is intuitively appealing and it was shown by Xu
et al. (2019b) that this method achieves state-of-the-art performance. We propose a variant of the algorithm: letting K t
denote the heat kernel for G at some t > 0, we minimize
(cid:88)

(cid:88)

C(p, q) (cid:51) C (cid:55)→

(K t

ik − Qjl)2CijCkl,

(8)

i,k

j,l

with Q deﬁned as above. For each t > 0, we obtain an optimal coupling which is used to partition G as described above.
Experimental results in Section 4 show that this change to the algorithm gives a signiﬁcant performance boost over the
adjacency-based version.

Connection to Spectral Clustering. Let G be an undirected, connected graph with graph Laplacian L. The connectivity
of G implies that L has exactly one zero eigenvalue with constant eigenvector. Assume for simplicity that the multiplicity
of the smallest positive eigenvalue of L is one. A fundamental concept in spectral graph theory is that the corresponding
eigenvector—the Fiedler vector of G—gives a 2-way partitioning of G with good theoretical properties: nodes of G
are partitioned according to the sign of their entry in the Fiedler vector (Fiedler, 1973). We refer to this as the Fiedler
partition of G, and ﬁnd that it arises as a special case of spectral GW partitioning:
Theorem 3. Let G be a connected graph whose ﬁrst positive eigenvalue has multiplicity one, endowed with the uniform
node probability distribution p. For sufﬁciently large t, the 2-way partition of G derived from a minimizer of (8) agrees
with the Fiedler partitioning.

The theorem demonstrates a novel connection between optimal transport and classical spectral clustering and shows
that partitioning graphs via spectral GW matching (8) is a generalization of these classical methods. For the small-t
regime, note that the heat kernel of G has Taylor expansion K t = In + tL + O(t2), where In is the n × n identity
matrix. Thus for low values of t, spectral GW partitioning is driven by matchings of graph Laplacians, which contain
local adjacency information.

4 EXPERIMENTS

We present several numerical experiments demonstrating the boost in performance obtained by using heat kernels in the
GW problem rather than adjacency matrices. In experiments with undirected graphs, we used the normalized graph
Laplacian to construct heat kernels—results were qualitatively similar using heat kernels of the standard Laplacian, but
we found some boost in quantitative performance in graph partitioning when using the normalized version. Experiments
with directed graphs use Chung’s normalized Laplacian.

5

MARCH 4, 2021

Table 1: Results of Energy Landscape Experiment.

Loss

Time (s) Err. (%)

Prod. Err. (%)

Adj
Spec5
Spec10
Spec20

.0230
.0014
.0013
.0010

24.21
22.35
3.86
0.05

4.81
6.00
1.34
.02

Figure 1: Visualizations of GW graph matchings via adjacency loss (3) (left column) and spectral loss (5) with t = 20
(right column). For each loss, we illustrate matchings between binary trees (Top), circular graphs (Middle) and IMDB
graphs (Bottom). In each case, the interpolation is generated from node matchings inferred from the optimal GW
coupling. Uniform node distributions are used in all examples.

Energy Landscapes and Convergence Rates. Adjacency loss (3) is highly nonconvex, while Lemma 1 shows that
minimizing spectral loss (5) is equivalent to maximizing a convex function over a convex polytope. While the latter
optimization problem is still intractable, we now demonstrate that its approximation via gradient descent is well-behaved.
In each trial, two random graphs from the IMDB-Binary (Yanardag and Vishwanathan, 2015) actor collaboration graph
dataset (1000 graphs with 19.77 nodes and 96.53 edges on average) are selected. The nodes of the graphs are endowed
with uniform distributions p and q (results obtained when using other distributions from the family (4) are similar). An
ensemble of couplings between these measures is generated by running a custom Markov Chain Monte Carlo (MCMC)
hit-and-run sampler (Smith, 1984) on the coupling polytope C(p, q) (details in Supplementary Materials). We sampled
100 points in the polytope by running 100,000 MCMC steps and subsampling uniformly. Using each coupling in the
ensemble as an initialization, we run projected gradient descent on adjacency loss (3) and spectral loss (5) with t = 5,
10 and 20 and record the loss at the local minimum from each initialization. This process is repeated 100 times (100
choices of pairs of graphs).

Statistics for the experiment are reported in Table 1. For each method, we report the mean time for convergence of each
gradient descent. For each trial, we obtain a distribution of losses from the 100 initializations in the ensemble, with the
minimum loss treated as the putative global minimum. The “Worst Error” for each trial is (max loss−min loss)/min loss.
We report the mean Worst Error over 100 trials. In available packages, gradient descent for GW matching is by default
initialized with the product coupling C = pqT , so we also report the “Product Error” (product loss−min loss)/min loss,
averaged over all trials. We note that the absolute losses of the adjacency and heat kernel methods are not directly
comparable, which is why we report these relative errors. The heat kernel representations provide an order of magnitude
speed up of convergence, with decreasing error as the t parameter increases; e.g., for t = 20, all initializations converge
to a coupling with less than 0.1% error. More details are provided in the Supplementary Materials.

Graph Matching and Averaging. Let (G, p) and (H, q) be measure graphs. Minimizers of the loss functions (3)
and (5) are couplings C giving soft node correspondences between G and H. To assess the intuitive meaning of
such a coupling, it is useful to visualize this node correspondence at the graph level. We produce such visualizations
using ideas from Chowdhury and Needham (2020), as shown in Figure 1. The ﬁgure shows six separate examples.
For any particular example, we display an interpolation between graph G (on the left) and graph H (on the right).
The optimal coupling C is used to interpolate node positions from G to H, with new edges phasing in during the
interpolation—see Supplementary Materials for the details of the visualization algorithm as well as another experiment
illustrating improved stability of the graph averaging algorithm of Peyr´e et al. (2016) when using spectral loss. We

6

MARCH 4, 2021

Table 2: Node Correctness, Mean ± St. Dev. (Time).

Dataset

Proteins
Enzymes
Reddit
Collab

Adj

.68 ± .22 (31.9)
.70 ± .18 (8.9)
.29 ± .21 (3941.7)
.50 ± .27 (4.3)

Spec10
.78 ± .22 (5.1)
.79 ± .17 (1.4)
.50 ± .11 (206.1)
.50 ± .27 (5.6)

Figure 2: Plots of AMI and modularity for stochastic block model graphs across t parameters with clearly correlated
peaks. Inset: Sample adjacency matrices for SBMs with increasing cross-block densities.

observe that in all cases, the matching produced via spectral loss preserves large scale qualitative features of the graphs
more faithfully than adjacency-based matchings.

We also assess the quality of node correspondences quantitatively. In this experiment, we consider two biological graph
databases Proteins (Borgwardt et al., 2005) (1113 graphs with 39.06 nodes and 72.82 edges on average) and Enzymes
(Dobson and Doig, 2003; Schomburg et al., 2004) (600 graphs with 32.63 nodes and 62.14 edges on average), and
two social graph databases Reddit (subset of 500 graphs with 375.9 nodes and 449.3 edges on average) and Collab
(subset of 1000 graphs with 63.5 nodes and 855.6 edges on average), both from Yanardag and Vishwanathan (2015).
All datasets were downloaded from Kersting et al. (2016). For each graph G = (V, E), we assign a node distribution p
from (4) (parameters tuned for the best performance in each method). A “new” measure graph (H, q) is created by
randomly permuting node labels of G. There is a ground truth node correspondence between G and H and the goal is
to measure the ability of GWL and SpecGWL to recover it. Given a coupling C of (G, p) and (H, q), we measure its
performance by the node correctness score |S ∩ SGT |/|S|, where S = {(i, j) | Cij > (cid:15)}, for (cid:15) > 0 a small threshold
parameter, is the set of node correspondences from C and SGT comprises ground truth node correspondences. Xu et al.
(2019a) showed that the adjacency-based GWL framework achieves state-of-the-art graph matching performance with
respect to this metric.

For each graph G and permuted version H, we compute couplings minimizing adjacency loss (3) and spectral loss (5)
with t = 10 and compute their node correctness scores. Mean scores for each dataset are provided in Table 2, where
we see that spectral loss outperforms adjacency, except on the dense Collab graph where results agree. Minimizers of
spectral loss in SpecGWL were computed via standard gradient descent. Minimizers of adjacency loss in GWL were
computed via both gradient descent and the regularized proximal gradient descent method of Xu et al. (2019a), with
the best results we obtained reported in Table 2 (proximal for the biological graphs, standard for the social graphs).
We have empirically found that t = 10 gives good matching performance for graphs with tens or hundreds of nodes,
and moreover that performance is generally quite robust to this parameter choice (cf. illustration in Supplementary
Materials).

Optimizing the Scale Parameter. SpecGWL requires determining an appropriate scale parameter t, and here we
propose two methods to this end in the context of graph partitioning. We ﬁrst propose a supervised method for tuning t

7

MARCH 4, 2021

Table 3: GRP model results; averaged AMI (time).

pout
.08
.10
.12
.15

Infomap

GWL

SpecGWL

.676 (.34)
—
—
—

.611 (1.44)
.624 (1.56)
.615 (1.64)
.611 (1.82)

.845 (.62)
.820 (.70)
.820 (.75)
.788 (.87)

Table 4: Comparison of adjusted mutual information scores across a variety of datasets.

Dataset

Fluid

FastGreedy

Louvain

Infomap

Wikipedia

EU-email

sym, raw
sym, noisy
asym, raw
asym, noisy

sym, raw
sym, noisy
asym, raw
asym, noisy

—
—
—
—

—
—
—
—

0.382
0.341
—
—

0.312
0.251
—
—

0.377
0.329
—
—

0.447
0.382
—
—

0.332
0.329
0.332
0.329

0.374
0.379
0.443
0.356

Amazon

Village

raw
noisy

—
0.347

0.692
0.441
0.801∗
0.758
*Slight improvements possible with proximal gradient, but overall performance rankings are preserved.

0.881
0.190

0.881
0.778

0.881
0.827

0.637
0.573

0.622
0.584

raw
noisy

0.940
0.463

—
—

SpecGWL
0.442∗
0.395
0.376
0.307

0.487
0.425
0.437
0.377

GWL

0.312
0.285
0.178
0.170

0.451
0.404
0.420
0.422
0.443∗
0.352
0.606∗
0.560

via cross-validation. We generate a dataset (illustration in Supplementary Materials) of n = 10 stochastic block model
networks with blocks of uniformly random sizes in the range [20, 50], within-block edge density 0.5, and uniformly
random across-block edge densities in the range [0, 0.35]. Following a leave-one-out cross-validation scheme, we take
n subsets of the data and run n trials. In trial j we use subset j as a training set to optimize the scale parameter t
via a grid search. Speciﬁcally, the optimal t was the one that maximized the sum of the adjusted mutual information
(AMI) scores computed between the ground truth clusterings and the clusterings recovered by SpecGWL across the
training set. We then evaluate the performance of SpecGWL on the test set according to the AMI score. Additionally
we evaluate the following baselines: Fluid (Par´es et al., 2017), FastGreedy (Clauset et al., 2004), Louvain (Blondel
et al., 2008), Infomap (Rosvall and Bergstrom, 2008), and the adjacency-based GWL (Xu et al., 2019a). Average
AMI scores were: (Fluid, 0.59), (FastGreedy, 0.48), (Louvain, 0.65), (Infomap, 0.0), (GWL, 0.62), (SpecGWL, 0.65),
where the results for Fluid and Louvain were tuned via gridsearch over their parameters (number of communities and
resolution, respectively). Additionally, SpecGWL outperformed all other baselines in 5 out of 10 trials, with Louvain
winning 4 and GWL winning the remaining trial. Moreover, SpecGWL outperformed GWL in 7 out of 10 trials.

We also propose a fully unsupervised method to tune the scale parameter via modularity maximization: here we
use the Newman modularity Q-score as a proxy for AMI—Figure 2 illustrates how the peaks of AMI and modularity
occur together across the scale parameter axis using synthetic data generated from a stochastic block model. In our
unsupervised tuning method, the number of clusters k is selected by maximizing the modularity (which requires no
ground truth partition) of a SpecGWL partitioning with respect to a ﬁxed t-value (we use t = 10 at this stage, which we
generally found to be stable to parameter perturbation for graphs on the order of 100s to 1000s of nodes) over a range
of k-values. Once a k has been selected, the scale parameter t is also selected by maximizing modularity over a range
of t values. To illustrate this method with an unsupervised task, we test partitioning performance on Gaussian Random
Partition model networks. This is a parametric model for random graphs, where the parameters are number of nodes
(we use 1000), mean number of nodes per cluster (we use 150), a probability of within cluster edges (we use 0.5) and a
probability of out-of-cluster edges pout ∈ {.08, .10, .12, .15}. For each value of pout, we construct a random graph on
which SpecGWL is trained to determine number of clusters and optimal t-value. We then create 10 random graphs,
partition them and compute AMI against ground truth clusters. Since this is a directed model, the only algorithms that
apply are Infomap, GWL and SpecGWL . Average AMI scores and compute times for each pout are reported in Table 3.

Graph Partitioning on Real Data. We compare the performance of SpecGWL on a graph partitioning task against
the baseline methods described earlier for four real-world datasets. The ﬁrst is a directed Wikipedia hyperlink network

8

MARCH 4, 2021

(Leskovec and Krevl, 2014) that we preprocessed by choosing 15 webpage categories and extracting their induced
subgraphs. The resulting digraph had 1998 nodes and 2700 edges. The second was obtained from an Amazon product
network (Leskovec and Krevl, 2014) by taking the subgraph induced by the top 12 product categories. The resulting
graph had 1501 nodes and 4626 edges. The third dataset was a digraph of email interactions between 42 departments
of a European research institute (EU-email), and it comprised 1005 nodes and 25571 edges. The ﬁnal dataset was a
real-world network of interactions (8423 edges) among 1991 residents of 12 Indian villages (Banerjee et al., 2013),
which we refer to as the Village dataset. We created noisy versions of each graph by adding up to 10% additional edges,
and also created symmetrized versions of the Wikipedia and EU-email graphs by adding reciprocal edges.

The quality of each graph partition was measured by computing the AMI score against the ground-truth partition. The
scores reported in Table 4 are obtained using parameters selected through the unsupervised procedure described in the
previous section. Reported results use standard gradient descent to compute GWL and SpecGWL scores. Despite issues
with numerical instability, we also computed scores via the regularized proximal gradient method of Xu et al. (2019a)
where possible. Slight score improvements are possible in some cases, but overall score rankings between methods are
unchanged—more experimental details are provided in the Supplementary Materials. We ﬁnd that SpecGWL is the
most consistent leader across all methods; in particular, it consistently produces improved results compared to GWL,
the only other comprehensive method which is also applicable to graph matching and averaging. The performance of
SpecGWL on directed graphs is especially relevant, considering that its closest competitor Infomap is a state-of-the-art
method for digraph partitioning.

5 DISCUSSION

We have introduced a spectral notion of GW distance for graph comparison problems based on comparing heat kernels
rather than adjacency matrices. This spectral variant is shown qualitatively and quantitatively to improve performance
in graph matching and partitioning tasks. The techniques introduced here should be useful for studying further variants
of GW distances. For example, work of Peyr´e et al. (2016) suggests that replacing the L2-type spectral loss (5)
with a loss based on KL divergence could have beneﬁts when performing statistical analysis on graph datasets. We
also remark that, while spectral GW is faster than its adjacency counterpart on smaller graphs, it may not enjoy the
same scalability properties due to the lack of sparse matrix operations. A signiﬁcant direction of future work will be
to construct multiscale approaches to analyze large scale graphs through both adjacency and spectral methods: the
divide-and-conquer techniques of Xu et al. (2019a) can break graphs into manageable chunks, and then SpecGWL can
ﬁnish the computation with improved runtime and accuracy. On the theoretical front, Theorems 2 and 3 suggest that it
is tractable to study spectral GW rigorously, and developing Theorem 3 into a larger theory should illuminate further
connections between optimal transport and spectral graph theory. It will also be interesting to explore more fully the
dependence of our algorithms on the scale parameter t and to incorporate multiple t-values into computations.

Acknowledgments We would like to thank the reviewers on this work for their numerous helpful comments. We wish
to also thank Facundo M´emoli for suggesting references related to the original work on Spectral GW.

References

David Alvarez-Melis and Tommi Jaakkola. Gromov-Wasserstein alignment of word embedding spaces. In Proceedings

of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1881–1890, 2018.

David Alvarez-Melis, Stefanie Jegelka, and Tommi S Jaakkola. Towards optimal transport with global invariances. In

The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 1870–1879. PMLR, 2019.

Abhijit Banerjee, Arun G Chandrasekhar, Esther Duﬂo, and Matthew O Jackson. The diffusion of microﬁnance. Science,

341(6144):1236498, 2013.

Am´elie Barbe, Marc Sebban, Paulo Gonc¸alves, Pierre Borgnat, and R´emi Gribonval. Graph diffusion Wasserstein
distances. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in
Databases, 2020.

Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in

large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008, 2008.

Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch¨onauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel.

Protein function prediction via graph kernels. Bioinformatics, 21(suppl 1):i47–i56, 2005.

Michael M Bronstein and Klaus Glashoff. Heat kernel coupling for multiple graph analysis. arXiv preprint

arXiv:1312.3035, 2013.

9

MARCH 4, 2021

Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative models across

incomparable spaces. In International Conference on Machine Learning, pages 851–861, 2019.

Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal tranport with applications on positive-unlabeled

learning. Advances in Neural Information Processing Systems, 33, 2020.

L´ena¨ıc Chizat. Transport optimal de mesures positives: mod`eles, m´ethodes num´eriques, applications. PhD thesis,

Universit´e Paris-Dauphine, 2017.

Samir Chowdhury and Facundo M´emoli. The Gromov–Wasserstein distance between networks and stable network

invariants. Information and Inference: A Journal of the IMA, 8(4):757–787, 2019.

Samir Chowdhury and Tom Needham. Gromov-Wasserstein averaging in a Riemannian framework. In Proceedings of

the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 842–843, 2020.

Fan Chung. Laplacians and the Cheeger inequality for directed graphs. Annals of Combinatorics, 9(1):1–19, 2005.

Aaron Clauset, Mark EJ Newman, and Cristopher Moore. Finding community structure in very large networks. Physical

review E, 70(6):066111, 2004.

Nicol´o De Ponti and Andrea Mondino. Entropy-transport distances between unbalanced metric measure spaces. arXiv

preprint arXiv:2009.10636, 2020.

Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal

of molecular biology, 330(4):771–783, 2003.

Yihe Dong and Will Sawin. COPT: Coordinated optimal transport on graphs. Advances in Neural Information

Processing Systems, 33, 2020.

Miroslav Fiedler. Algebraic connectivity of graphs. Czechoslovak mathematical journal, 23(2):298–305, 1973.

R´emi Flamary and Nicolas Courty. POT: Python Optimal Transport library. 2017. URL https://github.com/

rflamary/POT.

Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks. Proceedings of the

National Academy of Sciences, 99(12):7821–7826, 2002.

Reigo Hendrikson. Using Gromov-Wasserstein distance to explore sets of networks. Master’s thesis, University of

Tartu, 2016.

Nan Hu, Raif M Rustamov, and Leonidas Guibas. Stable and informative spectral signatures for graph matching. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2305–2312, 2014.

Atsushi Kasue and Hironori Kumura. Spectral convergence of Riemannian manifolds. Tohoku Mathematical Journal,

Second Series, 46(2):147–179, 1994.

Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Benchmark data sets for

graph kernels, 2016. URL http://graphkernels.cs.tu-dortmund.de.

Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K Rohde. Optimal mass transport: Signal

processing and machine-learning applications. IEEE signal processing magazine, 34(4):43–59, 2017.

Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.

edu/data, June 2014.

Facundo M´emoli. On the use of Gromov-Hausdorff distances for shape comparison. The Eurographics Association,

2007.

Facundo M´emoli. Gromov–Wasserstein distances and the metric approach to object matching. Foundations of

Computational Mathematics, 11(4):417–487, 2011a.

Facundo M´emoli. A spectral notion of Gromov–Wasserstein distance and related methods. Applied and Computational

Harmonic Analysis, 30(3):363–401, 2011b.

Facundo M´emoli and Tom Needham. Gromov-Monge quasi-metrics and distance distributions. arXiv preprint

arXiv:1810.09646, 2018.

10

MARCH 4, 2021

Huda Nassar, Nate Veldt, Shahin Mohammadi, Ananth Grama, and David F Gleich. Low rank spectral network

alignment. In Proceedings of the 2018 World Wide Web Conference, pages 619–628, 2018.

Ferran Par´es, Dario Garcia Gasulla, Armand Vilalta, Jonatan Moreno, Eduard Ayguad´e, Jes´us Labarta, Ulises Cort´es,
and Toyotaro Suzumura. Fluid communities: a competitive, scalable and diverse community detection algorithm. In
International Conference on Complex Networks and their Applications, pages 229–240. Springer, 2017.

Rob Patro and Carl Kingsford. Global network alignment using multiscale spectral signatures. Bioinformatics, 28(23):

3105–3114, 2012.

Gabriel Peyr´e, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein averaging of kernel and distance matrices. In

International Conference on Machine Learning, pages 2664–2672, 2016.

Martin Reuter, Franz-Erich Wolter, and Niklas Peinecke. Laplace–Beltrami spectra as ‘Shape-DNA’of surfaces and

solids. Computer-Aided Design, 38(4):342–366, 2006.

Martin Rosvall and Carl T Bergstrom. Maps of random walks on complex networks reveal community structure.

Proceedings of the National Academy of Sciences, 105(4):1118–1123, 2008.

Bernhard Schmitzer and Christoph Schn¨orr. Modelling convex shape priors and matching based on the Gromov-

Wasserstein distance. Journal of mathematical imaging and vision, 46(1):143–159, 2013.

Ida Schomburg, Antje Chang, Christian Ebeling, Marion Gremse, Christian Heldt, Gregor Huhn, and Dietmar Schom-
burg. Brenda, the enzyme database: updates and major new developments. Nucleic acids research, 32(suppl 1):
D431–D433, 2004.

Thibault S´ejourn´e, Franc¸ois-Xavier Vialard, and Gabriel Peyr´e. The Unbalanced Gromov Wasserstein distance: Conic

formulation and relaxation. arXiv preprint arXiv:2009.04266, 2020.

Robert L Smith. Efﬁcient Monte Carlo procedures for generating points uniformly distributed over bounded regions.

Operations Research, 32(6):1296–1308, 1984.

Justin Solomon, Gabriel Peyr´e, Vladimir G Kim, and Suvrit Sra. Entropic metric alignment for correspondence

problems. ACM Transactions on Graphics (TOG), 35(4):72, 2016.

Karl-Theodor Sturm. On the geometry of metric measure spaces. Acta mathematica, 196(1):65–131, 2006.

Karl-Theodor Sturm. The space of spaces: curvature bounds and gradient ﬂows on the space of metric measure spaces.

arXiv preprint arXiv:1208.0434, 2012.

Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise and provably informative multi-scale signature based on

heat diffusion. In Computer graphics forum, volume 28, pages 1383–1392. Wiley Online Library, 2009.

Anton Tsitsulin, Davide Mottin, Panagiotis Karras, Alexander Bronstein, and Emmanuel M¨uller. NetLSD: hearing the
shape of a graph. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pages 2347–2356. ACM, 2018.

Anton Tsitsulin, Marina Munkhoeva, Davide Mottin, Panagiotis Karras, Alex Bronstein, Ivan Oseledets, and Emmanuel
M¨uller. The shape of data: Intrinsic distance for data distributions. In ICLR 2020: Proceedings of the International
Conference on Learning Representations, 2020.

Titouan Vayer, Laetita Chapel, R´emi Flamary, Romain Tavenard, and Nicolas Courty. Fused Gromov-Wasserstein
distance for structured objects: theoretical foundations and mathematical properties. arXiv preprint arXiv:1811.02834,
2018.

Titouan Vayer, Nicolas Courty, Romain Tavenard, and R´emi Flamary. Optimal transport for structured data with

application on graphs. In International Conference on Machine Learning, pages 6275–6284, 2019a.

Titouan Vayer, R´emi Flamary, Nicolas Courty, Romain Tavenard, and Laetitia Chapel. Sliced Gromov-Wasserstein. In

Advances in Neural Information Processing Systems, pages 14726–14736, 2019b.

Hongteng Xu. Gromov-Wasserstein factorization models for graph clustering. In AAAI, pages 6478–6485, 2020.

Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable Gromov-Wasserstein learning for graph partitioning and

matching. In Advances in Neural Information Processing Systems, pages 3046–3056, 2019a.

11

MARCH 4, 2021

Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin. Gromov-Wasserstein learning for graph matching and

node embedding. In International Conference on Machine Learning, pages 6932–6941, 2019b.

Hongteng Xu, Dixin Luo, Ricardo Henao, Svati Shah, and Lawrence Carin. Learning autoencoders with relational

regularization. In International Conference on Machine Learning, pages 10576–10586. PMLR, 2020.

Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International

Conference on Knowledge Discovery and Data Mining, pages 1365–1374, 2015.

12

A Proofs of Theorems

A.1 Theorem 2

MARCH 4, 2021

Proof. Suppose G = (V G, EG, p) and H = (V H , EH , q) have m, n nodes, respectively. Let C ∈ C(p, q). Then
C ∈ [0, 1]m×n and satisﬁes (m + n − 1) linear equality constraints coming from the row and column sums. By Lemma
1, at least one of the minimizers of spectral loss (5) is located at an extreme point of the convex polytope C(p, q). This
polytope lies in an (mn − (m + n − 1))-dimensional afﬁne subspace of mn-dimensional space. The equality constraints
automatically ensure that each Cij < 1, where the strict inequality holds because the graphs are fully supported and
thus each pi, qj < 1. Therefore, estimating the number of zero entries is equivalent to estimating the number k of
active nonnegativity constraints. An extreme point corresponds to the intersection of k hyperplanes in general position
with this afﬁne subspace, and this intersection has dimension mn − (m + n − 1) − k. Because the extreme point has
dimension 0, we have k = mn − (m + n − 1). If the hyperplanes are not in general position, then the number of active
nonnegativity constraints, i.e. the number of zeros, is greater than or equal to mn − (m + n − 1). Next suppose m ∼ n.
Then the ratio of nonzero entries to total entries of C is roughly n2−k

, and this term tends to 0 as n → ∞.

n2 = 2n−1
n2

A.2 Theorem 3

Proof. Let G = (V, E), with |V | = n, be a graph satisfying the assumptions, endowed with uniform vertex distribution
p and let K t denote the heat kernel of G. By deﬁnition,

K t = Φe−tΛΦT =

n
(cid:88)

j=1

e−tλj φjφT
j ,

where Φ is a matrix whose columns are the orthonormal eigenvectors φ1, . . . , φn of the graph Laplacian L of G and Λ
is the diagonal matrix of sorted eigenvalues 0 = λ1 < λ2 < λ3 ≤ λ4 ≤ · · · ≤ λn of L. Let Q be the 2-way partitioning
template from (7). Since p is uniform, the estimated distribution q is also uniform and Q = 1
2 I2, where I2 is the 2 × 2
identity matrix.

The 2-way spectral GW partitioning of G is obtained from a coupling minimizing the spectral partitioning loss (8).
Using Lemma 1, we see that this optimization task is equivalent to maximizing

C (cid:55)→ (cid:104)K tC, CQ(cid:105) =

1
2

(cid:104)K tC, C(cid:105)

over the coupling polytope C(p, q). Since the factor of 1
simplify the objective function as

2 does not effect the optimization, we supress it and further

(cid:104)K tC, C(cid:105) = tr (cid:0)(K tC)T C(cid:1) = tr


CC T

n
(cid:88)

j=1



e−tλj φjφT
j

 =

n
(cid:88)

j=1

e−tλj tr (cid:0)CC T φjφT

j

(cid:1) .

Since the leading eigenvector is φ1 = 1√
tr(CC T φ1φT
quantity

n 1n×1 (the normalized vector of all ones), it is easy to check that the term
1 ) is constant for all C ∈ C(p, q). The objective therefore becomes to maximize over C ∈ C(p, q) the

n
(cid:88)

j=2

e−tλj tr (cid:0)CC T φjφT

j

(cid:1) = e−tλ2

which is in turn equivalent to maximizing


tr (cid:0)CC T φ2φT

2

(cid:1) +

n
(cid:88)

j=3

e−t(λj −λ2)tr (cid:0)CC T φjφT

j

(cid:1)

 ,



C (cid:55)→ tr (cid:0)CC T φ2φT

2

(cid:1) +

n
(cid:88)

j=3

e−t(λj −λ2)tr (cid:0)CC T φjφT

j

(cid:1) .

(9)

Observe that the summation term goes to zero as t → ∞ (using λj > λ2 for all j ≥ 3), while the ﬁrst term is
independent of t. It follows that, for sufﬁciently large t, maximization of (9) is equivalent to maximizing

C (cid:55)→ tr (cid:0)CC T φ2φT

2

(cid:1)

(10)

over C(p, q). It then remains to study the structure of maximizers of (10).

13

MARCH 4, 2021

We further simplify the objective function (10) as

tr (cid:0)CC T φ2φT

2

(cid:1) = tr (cid:0)(C T φ2)T (C T φ2)(cid:1) = (cid:107)C T φ2(cid:107)2,

where the norm in the last line is the Frobenius norm. We denote the column vectors of C by C1, C2 ∈ R1×n, so that

(cid:107)C T φ2(cid:107)2 =

(cid:13)
(cid:18) C1 · φ2
(cid:13)
(cid:13)
C2 · φ2
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

,

where the norm on the right is the Euclidean norm. Since C ∈ C(p, q) and p is uniform, we have C2 = 1
whence

n 1n×1 − C1,

C2 · φ2 =

1n×1 − C1

· φ2 = −C1 · φ2,

(cid:18) 1
n

(cid:19)

√

since φ2 is orthogonal to 1n×1 =

nφ1. The objective (10) is ﬁnally reduced to
C (cid:55)→ 2(C1 · φ2)2.
2 be the vector of positive entries of φ2 with all negative entries thresholded to zero and likewise deﬁne φ−

Let φ+
2 to
be the vector of negative entries of φ2. Assume without loss of generality that (cid:107)φ+
2 (cid:107) (the other case follows
entirely similarly). Then in order to maximize (11), one should set each entry of C1 to be nonzero if and only if the
corresponding entry of φ2 is positive. The spectral GW partitioning therefore agrees with the Fiedler partitioning, and
the proof is complete.

2 (cid:107) ≥ (cid:107)φ−

(11)

B An MCMC Sampler for Couplings.

Both the adjacency (3) and spectral (5) loss functions are nonconvex, and solving such problems effectively often relies
on a clever choice of initialization. A limitation of the current practice is that this initialization is often chosen to be
the product coupling pqT , which we empirically ﬁnd to be sub-optimal in even simple cases. This is accomplished by
running gradient descent from each point in an ensemble of initializations generated by a Markov Chain Monte Carlo
Hit-And-Run sampler (Smith, 1984). This algorithm is well-known, but we describe it below for the convenience of the
reader. Our code includes a lean Python implementation written speciﬁcally for sampling the coupling polytope; we
hope such an implementation will be useful to the broader optimal transport community.

Algorithm 1 Markov chain sampler

1: function MARKOVSTEP(A, p, q, C)
// A: matrix of linear constraints
2:
// p, q : m × 1, n × 1 probability vectors
3:
// C : m × n initial coupling matrix
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: return C + γV
15: end function

V ← random m × n matrix as direction
Q ← o.n. basis for row space of A
V ← V − QQT V
pos ← indices where V > 0
neg ← indices where V < 0
α ← max(−C[pos]/V [pos])
β ← min(−C[neg]/V [neg])
γ ← random element of [α, β]

(cid:46) project V to correct subspace

(cid:46) [α, β] is maximal range of step sizes

(cid:46) new coupling matrix

C Additional experiments and implementation details

C.1 Additional Landscape Results

Figure 3 gives a more detailed view of the results reported in Table 1. For each plot, the x-axis is (Worst or Product)
error percentage. The y-axis shows the percentage of samples whose error was above the relative error rate. We see
that a signiﬁcant number of samples have high error rates for adjacency loss (3) and spectral loss (5) with t = 5. For
spectral loss with t = 10 or 20, these error rates are greatly decreased. In particular, spectral loss with t = 20 has
essentially zero samples with error rate above 2%.

14

MARCH 4, 2021

Figure 3: Results of energy landscape experiments.

Figure 4: Improvement (blue cells) obtained by using spectral loss (5) instead of adjacency loss (3) across a range of t
values (x axis).

C.2 Additional ﬁgures

Here we add some ﬁgures that help better understand some of the quantitative results presented in the main text. Figure
4 shows that the improvement in the graph matching experiment obtained via SpecGWL remains stable across a wide
range of scale parameters. Speciﬁcally, we computed SpecGWL loss for t ∈ {10, 20, . . . , 90} for each of the four
datasets. The Collab dataset was the only one where there was no appreciable improvement from using SpecGWL, but
there was no signiﬁcant decrease in performance either for scales in the range t ∈ {10, 20, 30}.

Figure 5 shows the 10 stochastic block model networks used in the synthetic graph partitioning experiment. Each
network has 5 blocks, and the block sizes were chosen uniformly at random from the range [20, 50] at the beginning of
the experiment. Within-block edge densities were ﬁxed at 0.5, and across-block edge densities were chosen uniformly
at random in the range [0, 0.3].

C.3 Visualizing Graph Matchings

Here we describe how the interpolations used to visualize coupling quality in Figure 1 were produced. Let (G, p) and
(H, q) be measure graphs and C ∈ C(p, q) a coupling. To produce an interpolation, we ﬁrst “blow up” C so that it has
the form of a weighted permutation matrix. This is done by ﬁrst scanning across rows; any row with more than a single
nonzero entry is split into “dummy” copies, each of which contains a single nonzero entry from the original row. The
splits allow us to split nodes of G into dummy copies, with weights given by entries in the corresponding row of C.
The same procedure is applied to split columns of C and to split nodes of H. The result is a pair of expanded measure
graphs (G(cid:48), p(cid:48)) and (H (cid:48), q(cid:48)) together with an expanded coupling C (cid:48) which provides a bijective correspondence between
the nodes of G(cid:48) and H (cid:48). Once such a bijective correspondence is obtained, we position each graph G(cid:48) and H (cid:48) in the
plane using a common embedding modality and then performing Procrustes alignment of the resulting embeddings. To
interpolate the graphs, we simply interpolate positions of the bijectively matched nodes, while phasing in new edges
that are formed. This visualization method has strong theoretical justiﬁcation: building on work of Sturm (2012), it

15

MARCH 4, 2021

Figure 5: Stochastic block models used in the supervised partitioning task with cross-validation

is shown by Chowdhury and Needham (2020) that this process represents a geodesic (in the metric geometry sense)
in the space of edge-weighted measure graphs. We observe that the conclusion of Lemma 1 is useful here, since the
theoretical guarantee on the sparsity of C implies that C will not get too large in the “blow up” phase of the algorithm.

To produce each example in Figure 1, we sampled 100 couplings from the coupling polytope via the MCMC algorithm
(1000 MCMC steps between each coupling) as initializations. We then computed an optimal coupling between the
graphs by optimizing the relevant loss function from each initialization and keeping the coupling with the lowest loss
from the resulting ensemble.

C.4 Dependence of Matchings on t-Value

To understand the landscape of optimal couplings that occur for different scale parameters, we took two graphs from
the Enzymes dataset and computed optimal couplings for spectral loss using 100 linearly spaced t values in the range
[0, 50]. Figure 6 shows a t-SNE embedding (with perplexity = 15) obtained after ﬂattening these couplings into vectors
in Euclidean space. The inset coupling matrices are representatives of the points in each signiﬁcant cluster. Interpolation
visualizations for some of the coupling matrices from different clusters are provided in Figure 7.

C.5 Averaging

We use the observations regarding the energy landscape and the quality of matchings to show that in the GW averaging
problem, using the heat kernel leads to 10x faster convergence than the adjacency matrix, and moreover, the heat kernel
yields a more “unique” barycenter. Speciﬁcally, given measure network representations X1, X2, . . . , Xn, a Fr´echet
i dGW(X, Xi)2. The objective of the GW averaging problem is to compute this
mean is an element of arg minX
barycenter, i.e. an average representation. In the Python OT package (Flamary and Courty, 2017), this barycenter
is computed iteratively from a random initialization (cf.
the gromov barycenter function). As a proxy for the
“uniqueness” of the barycenter, we compute the barycenter for multiple random initializations, and then take the variance
of the distribution of Fr´echet losses achieved by the barycenters.

(cid:80)

We demonstrate this claim on the Village dataset. We ran a bootstrapping procedure to sample 10 sets of 30 nodes, and
took the induced subgraphs to obtain 10 subgraphs. To keep the samples from being too sparse, we ﬁrst sorted the nodes
in order of decreasing betweenness centrality, and then selected 30 nodes (for each iteration) from the top 40 nodes
with the highest centrality. Next we computed both adjacency and heat kernel representations (for t = 3, 7, 11) of these
subgraphs. Then we used the gromov barycenter function to compute averages of the adjacency and heat kernel
representations. Each call to gromov barycenter uses a random initialization. Using this randomness as a source of
stochasticity, we repeated the set of barycenter computations 10 times to obtain four distribution of Fr´echet losses. After
mean-centering the distributions, the variance of the adjacency distribution was found to be two orders of magnitude
higher than any of the heat kernel distributions, and each of the three comparisons was found to be statistically signiﬁcant
by computing Bartlett tests for unequal variance (p < 10−6 for all, adjusted for multiple comparison via Bonferroni
correction). Boxplots of the results are shown in Figure 8.

16

MARCH 4, 2021

Figure 6: t-SNE embedding of optimal couplings between two graphs from the Enzymes dataset obtained via spectral
loss (5) using 100 equally spaced t values in the range [0, 50]. The ground metric is the l2 norm between vectorized
representations of the couplings. The range [0, 50] is mapped linearly from the blue to red color scheme.

Figure 7: Interpolation visualizations obtained via some of the representative couplings in Figure 6 arranged top to
bottom with increasing t. Note that the interpolation in the third row, corresponding to the largest t value, represents
global structure more faithfully.

17

Low resLow resFigure 8: Left: Differences in Fr´echet loss of the GW average across representations. Center: Mean-centered Fr´echet
loss, indicating the greater variance and sensitivity to initialization for the adjacency representation. Right: Distribution
of runtimes shows 10x speedup for the heat kernel.

MARCH 4, 2021

Table 5: Comparison between runtime of GWL and average runtime of SpecGWL across t parameters. “-Prox” rows
use regularized proximal gradient with Sinkhorn iterations as used by Xu et al. (2019a). Other rows use vanilla gradient
descent.

Method

Wikipedia

EU-email

Amazon

Village

sym

asym

sym

asym

raw noisy raw noisy raw noisy raw noisy raw noisy raw noisy

GWL
SpecGWL

14.1 16.1
1.8

2.3

16.0 14.2
2.4

2.4

GWL-Prox
SpecGWL-Prox

— —
2.6
2.9

— —
2.9
2.9

1.5
1.0

0.9
0.9

6.7
0.9

0.9
0.9

1.6
1.0

0.8 — —
1.0
1.0
1.0

8.6
1.4

1.2
1.3

13.1
0.9

1.0
1.8

3.3
1.8

2.2
1.8

5.4
2.7

2.2
2.0

C.6 Graph Partitioning

Runtimes for GWL and SpecGWL on the graph partitioning experiment are reported in Table 5. For SpecGWL
, the times are averaged over several values of t, with the idea that ﬁnding the correct t-value is a preprocessing
hyperparameter tuning step. For both GWL and SpecGWL , partitionings were obtained using standard projected
gradient descent. Speedups are obtained for GWL via the regularized proximal gradient method, but we were not able
to obtain results on all datasets with this method due to numerical issues (see below). Runtimes for this method are also
reported as GWL-Prox. We observe that spectral loss provides up to 10x acceleration in convergence rate for standard
gradient descent and even outperforms the proximal gradient in compute time.

When employing the regularized proximal gradient method, we found that the results were sensitive to the choice
of regularization parameter β (as is also observed by Xu et al. (2019a)), leading to numerical blowups if not chosen
carefully. In reporting each of the results below, we hand-tuned β after testing in the 10−1, 10−2, 10−3, . . . , 10−9
regimes. For Wikipedia, we used β = 2 · 10−5 for SpecGWL, but were unable to ﬁnd a β that provided stable results
for GWL. For EU-email, we used 2 · 10−7 for GWL and 3 · 10−8 for SpecGWL. For Amazon, we used β = 4 · 10−3
for GWL and 1.5 · 10−6 SpecGWL. Finally, for Village we used β = 5 · 10−6 for SpecGWL. This β led to numerical
instability for GWL, but β = 5 · 10−5 worked and yielded the results we report below. In summary, it appears that the
structure of the graph has a signiﬁcant effect on the optimal choice of regularization parameter (e.g. the Wikipedia graph
is relatively very sparse). Because the numerical instability issues are very sensitive to the regularization, one avenue
for future work could be to incorporate the strategies described in the PhD thesis of Chizat (2017) (e.g. “absorption into
the log domain”) to stabilize the regularized proximal gradient method.

18

adjHK3HK7HK11representation0.000000.000050.000100.000150.000200.00025lossVillage data: loss-representationadjHK3HK7HK11representation0.000030.000020.000010.000000.000010.000020.00003centered-lossVillage data: centered loss-representationadjHK3HK7HK11representation123456runtimeVillage data: runtime-representationMARCH 4, 2021

Table 6: Performance of GWL and SpecGWL using regularized proximal gradient descent and Sinkhorn iterations as in
Xu et al. (2019a). ’—’ denotes that an AMI score could not be calculated due to numerical instability.

Method

Wikipedia

EU-email

Amazon

Village

asym

sym

asym

raw noisy raw noisy raw noisy raw noisy raw noisy raw noisy

GWL-Prox
SpecGWL-Prox

— —
0.51 0.39

— —
0.39 0.29

0.45 0.40 — —
0.01 0.01

0.03 0.03

0.49 0.39
0.66 0.43

0.72* 0.58
0.84 0.72

*The code provided by Xu et al. (2019a) included a representation matrix as database[‘cost’], and this yielded the score of 0.72.
However, this matrix was asymmetric and not equal to the symmetrized adjacency matrix that was used in experiments with other
benchmarks. When using a symmetrized adjacency matrix, the score drops from 0.72 to 0.66.

19

