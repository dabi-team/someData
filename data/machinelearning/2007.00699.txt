0
2
0
2

l
u
J

1

]

G
L
.
s
c
[

1
v
9
9
6
0
0
.
7
0
0
2
:
v
i
X
r
a

Accelerated Message Passing for Entropy-Regularized MAP Inference

Jonathan N. Lee 1 Aldo Pacchiano 2 Peter Bartlett 2 3 Michael I. Jordan 2 3

Abstract

(MAP)

Maximum a posteriori
inference in
discrete-valued Markov random ﬁelds is a funda-
mental problem in machine learning that involves
identifying the most likely conﬁguration of ran-
dom variables given a distribution. Due to the dif-
ﬁculty of this combinatorial problem, linear pro-
gramming (LP) relaxations are commonly used
to derive specialized message passing algorithms
that are often interpreted as coordinate descent on
the dual LP. To achieve more desirable computa-
tional properties, a number of methods regularize
the LP with an entropy term, leading to a class
of smooth message passing algorithms with con-
vergence guarantees. In this paper, we present
randomized methods for accelerating these algo-
rithms by leveraging techniques that underlie clas-
sical accelerated gradient methods. The proposed
algorithms incorporate the familiar steps of stan-
dard smooth message passing algorithms, which
can be viewed as coordinate minimization steps.
We show that these accelerated variants achieve
faster rates for ﬁnding (cid:15)-optimal points of the un-
regularized problem, and, when the LP is tight,
we prove that the proposed algorithms recover
the true MAP solution in fewer iterations than
standard message passing algorithms.

1. Introduction

Discrete undirected graphical models are extensively used in
machine learning since they provide a versatile and powerful
way of modeling dependencies between variables (Wain-
wright & Jordan, 2008). In this work we focus on the im-
portant class of discrete-valued pairwise models. Efﬁcient

1Department of Computer Science, Stanford University, USA
2Department of Electrical Engineering and Computer Sciences,
University of California, Berkeley, USA 3Department of Statis-
tics, University of California, Berkeley, USA. Correspondence
to: Jonathan Lee <jnl@stanford.edu>, Aldo Pacchiano <pacchi-
ano@berkeley.edu>.

Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).

inference in these models has multiple applications, ranging
from computer vision (Jegelka & Bilmes, 2011), to sta-
tistical physics (Mezard & Montanari, 2009), information
theory (MacKay, 2003) and even genome research (Torada
et al., 2019).

In this paper we study and propose efﬁcient methods for
maximum a posteriori (MAP) inference in pairwise, discrete-
valued Markov random ﬁelds. The MAP problem corre-
sponds to ﬁnding a conﬁguration of all variables achieving
a maximal probability and is a key problem that arises when
using these undirected graphical models. There exists a vast
literature on MAP inference spanning multiple communities,
where it is known as constraint satisfaction (Schiex et al.,
1995) and energy minimization (Kappes et al., 2013). Even
in the binary case, the MAP problem is known to be NP-hard
to compute exactly or even to approximate (Kolmogorov &
Zabin, 2004; Cooper, 1990).

As a result, there has been much emphasis on devising meth-
ods that may work on settings under which the problem
becomes tractable. A popular way to achieve this goal is to
express the problem as an integer program and then relax
this to a linear program (LP). If the LP constraints are set to
the convex hull of marginals corresponding to all global set-
tings, also known as the marginal polytope (Wainwright &
Jordan, 2008), then the LP would yield the optimal integral
solution to the MAP problem. Unfortunately, writing down
this polytope would require exponentially many constraints
and therefore it is not tractable. We can consider larger
polytopes deﬁned over subsets of the constraints required
to deﬁne the marginal polytope. This is a popular approach
that underpins the family of LP relaxations known as the
Sherali-Adams (SA) hierarchy (Sherali & Adams, 1990).
Instead of enforcing global consistency, we enforce only
pairwise consistency via the local polytope, thus yielding
pseudo-marginals that are pairwise consistent but may not
correspond to any true global distribution. Despite the local
polytope requiring a number of constraints that is linear
in the number of edges of the input graph, the runtime re-
quired for solving this linear program for large graphs may
be prohibitive in practice (Yanover et al., 2006). These limi-
tations have motivated the design and theoretical analysis of
message passing algorithms that exploit the structure of the
problem. In this paper we study a class of smooth message
passing algorithms, derived from a regularized version of the

 
 
 
 
 
 
Accelerated Message Passing for Entropy-Regularized MAP Inference

local polytope LP relaxation (Ravikumar et al., 2010; Meshi
et al., 2012; Savchynskyy et al., 2011; Hazan & Shashua,
2008).

The technique of using entropy penalties to regularize linear
programs has a long and successful history. It has been
observed, both practically and in theory that, in some prob-
lems, solving a regularized linear program yields algorithms
with computational characteristics that make them prefer-
able to simply using an LP solver, particularly with large
scale problems. Previous work has studied and analyzed
convergence rates, and even rounding guarantees for simple
message passing algorithms (Ravikumar et al., 2010; Lee
et al., 2020; Meshi et al., 2012) based on iterative Breg-
man projections onto the constraints. These algorithms are
sometimes described as being smooth, as the dual objective
is smooth in the dual variable as a result of the entropy
regularization. Inspired by accelerated methods in optimiza-
tion (Lee & Sidford, 2013; Nesterov, 2012) we propose and
analyze two new variants of accelerated message passing
algorithms Accel-EMP and Accel-SMP. In this paper, we
are able to show our methods drastically improve upon the
convergence rate of previous message passing algorithms.

1.1. Related Work

MAP Inference The design and analysis of convergent
message passing algorithms has attracted a great deal of
attention over the years. Direct methods of deriving asymp-
totically convergent algorithms have been extensively ex-
plored. Examples include tree-reweighted message passing
(Kolmogorov, 2006), max-sum diffusion (Werner, 2007),
MP-LP (Globerson & Jaakkola, 2008), and other general
block-coordinate ascent methods (Kappes et al., 2013; Son-
tag et al., 2011). Our work builds upon regularized inference
problems that directly regularize the linear objective with
strongly convex terms, often leading to "smooth" message
passing (Savchynskyy et al., 2011; 2012; Hazan & Shashua,
2008; Weiss et al., 2007; Meshi et al., 2015). This formaliza-
tion has led to a number of computationally fast algorithms,
but often with asymptotic guarantees.

The focus of this paper is non-asymptotic convergence guar-
antees for families of these algorithms. Ravikumar et al.
(2010) provided one of the ﬁrst directions towards this goal
leveraging results for proximal updates, but ultimately the
rates were not made explicit due to the approximation at
every update. Meshi et al. (2012) provided a comprehen-
sive analysis of a message passing algorithm derived from
the entropy-regularized objective and later a gradient-based
one for the quadratically regularized objective (Meshi et al.,
2015). However, the convergence rates were only given for
the regularized objective, leaving the guarantee on the un-
regularized problem unknown. Furthermore, the objective
studied by Meshi et al. (2015) did not ultimately yield a

message passing (or coordinate minimization) algorithm,
which could be more desirable from a computational stand-
point. Lee et al. (2020) provided rounding guarantees for a
related message passing scheme derived from the entropy
regularized objective, but did not consider convergence on
the unregularized problem either. Savchynskyy et al. (2011)
studied the direct application of the acceleration methods
of Nesterov (2018); however, this method also forwent the
message passing scheme and convergence on the unregu-
larized problem was only shown asymptotically. Jojic et al.
(2010) gave similar results for a dual decomposition method
that individually smooths subproblems. Acceleration was
applied to get fast convergence but on the dual problem.

In addition to problem-speciﬁc message passing algorithms,
there are numerous general purpose solvers that can be ap-
plied to MAP inference to solve the LP with strong theoreti-
cal guarantees. Notably, interior point methods (Karmarkar,
1984; Renegar, 1988; Gondzio, 2012) offer a promising al-
ternative for faster algorithms. For example recent work
provides a (cid:101)O(
rank) iteration complexity by Lee & Sid-
ford (2014), where rank is the rank of the constraint matrix.
In this paper, we only consider comparisons between mes-
sage passing algorithms; however, it would be interesting to
compare both empirical and theoretical differences between
message passing and interior point methods in the future.

√

Accelerating Entropy-Regularized Linear Programs
We also highlight a connection with similar problems in
other ﬁelds. Notably, optimal transport also admits an LP
form and has seen a surge of interest recently. As in MAP in-
ference, these approximations are conducive to empirically
fast algorithms, such as the celebrated Sinkhorn algorithm,
that outperforms generic solvers (Cuturi, 2013; Benamou
et al., 2015; Genevay et al., 2016). In theory, Altschuler et al.
(2017) showed convergence guarantees for Sinkhorn and
noted that it can be viewed as a block-coordinate descent
algorithm on the dual, similar to the MAP problem. Since
this work, several methods have striven to obtain faster rates
(Lin et al., 2019; Dvurechensky et al., 2018), which can be
viewed as building on the seminal acceleration results of
Nesterov (2018) for general convex functions. It is interest-
ing to note that the entropy-regularized objectives in optimal
transport and MAP inference effectively become softmax
minimization problems, which have also been studied gen-
erally in the context of smooth approximations (Nesterov,
2005) and maximum ﬂow (Sidford & Tian, 2018).

1.2. Contributions

For the case of MAP inference from entropy-regularized
objectives, we address the question: is it possible to di-
rectly accelerate message passing algorithms with faster
non-asymptotic convergence and improved rounding guar-
antees? We answer this question afﬁrmatively from a theo-

Accelerated Message Passing for Entropy-Regularized MAP Inference

retical standpoint. We propose a method to directly acceler-
ate standard message passing schemes, inspired by Nesterov.
We prove a convergence guarantee for standard schemes on
the unregularized MAP objective over L2, showing con-
vergence on the order of (cid:101)O(m5/(cid:15)3) iterations where m is
the number of edges, assuming the number of vertices and
labels and the potential functions are ﬁxed. We then prove
that the accelerated variants converge in expectation on the
order of (cid:101)O(m9/2/(cid:15)2) iterations. We conclude by showing
that the accelerated variants recover the true MAP solution
with high probability in fewer iterations compared to prior
message passing analyses when the LP relaxation is tight
and the solution is unique (Lee et al., 2020).

1.3. Notation

(cid:111)

(cid:110)

p ∈ Rd

+ : (cid:80)

Let R+ denote the set of non-negative reals. The d-
dimensional probability simplex over the ﬁnite set χ is
Σd :=
x∈χ p(x) = 1
. A joint distribution,
P ∈ Σd×d, is indexed by xc = (xp, xq) ∈ χ2. The trans-
portation polytope of p, q ∈ Σd is deﬁned as the set of
pairwise joint distributions that marginalize to p and q,
written as Ud(p, q) = {P ∈ Rd×d
P (xp, x) =
q(x), (cid:80)
+, we
write the entropy as H(p) := − (cid:80)
x p(x)(log p(x) − 1).
While this is a somewhat unusual deﬁnition, it simpliﬁes
terms later and has been used by (Benamou et al., 2015; Lee
et al., 2020). We will use (cid:104)·, ·(cid:105) generally to mean the sum of
the elementwise products between two equal-length index-
able vectors. For any two vectors p, q ∈ Rd
+, the Hellinger
q(cid:107)2. The vector 1d ∈ Rd
distance is h(p, q) := 1√
2
consists of all ones.

+
P (x, xq) = p(x)}. For any vector p ∈ Rd

: (cid:80)

p −

√

√

xp

xq

(cid:107)

2. Coordinate Methods for MAP Inference

2.1. Smooth Approximations

(cid:81)

e∈E φe(xe) (cid:81)

For the pairwise undirected graph G = (V, E) with n :=
|V | and m := |E|1, we associate each vertex i ∈ V with
the random variable Xi on the ﬁnite set of labels χ of
size d = |χ| ≥ 2 and a distribution that factorizes as
i∈V φi(xi) where φi ∈ Rd
p(xV ) = 1
Z(φ)
and φe ∈ Rd2
. For any edge e ∈ E, we use i ∈ e to denote
that i is one of the two endpoints of e. For i ∈ V , we deﬁne
Ni := {e ∈ E : i ∈ e}, as the set of all incident edges to
i. We assume that each vertex has at least one edge. MAP
inference refers to the combinatorial problem of identifying
the conﬁguration that maximizes this probability distribu-
tion. In our case, we cast it as the following minimization

1In this paper we study pairwise models with only vertices
and edges, implying only pairwise interaction between variables.
However, our results can be extended to more general graphs.

problem:

minimize
xV ∈χ|V |

(cid:80)

i∈V Ci(xi) + (cid:80)

e∈E Ce(xe),

(MAP)

where C = − log φ, i.e., we view C as indexable by vertices,
It can be shown
edges, and their corresponding labels.
(Wainwright & Jordan, 2008) that (MAP) is equivalent to
the following linear program:

min
µ∈M

(cid:104)C, µ(cid:105)

s.t.

µ ∈ M,

where µ ∈ RrP for rP = nd + md2 is known as a marginal
vector, and (cid:104)C, µ(cid:105) = (cid:80)
xi∈χ Ci(xi)µi(xi) +
(cid:80)
xe∈χ2 Ce(xe)µe(xe), and M is the marginal poly-

(cid:80)

(cid:80)

e∈E

i∈V

tope deﬁned as

(cid:26)

M :=

µ : ∃ P s.t.

PXi(xi) = µi(xi), ∀i, xi
PXiXj (xe) = µe(e), ∀e, xe

(cid:27)

.

Here, P is any valid distribution over the random variables
{Xi}i∈V . Since M is described by exponentially many
constraints in the graph size, outer-polytope relaxations are
a standard paradigm to approximate the above problem by
searching instead over the local polytope:

L2 :=

(cid:26)

µ :

µi ∈ Σd
∀i ∈ V
µe ∈ Ud(µi, µj) ∀e = ij ∈ E

(cid:27)

.

The local polytope L2 enforces only pairwise consistency
between variables while M requires the marginal vector
to be generated from a globally consistent distribution of
{Xi}i∈V , so that M ⊆ L2. We refer the reader to the
survey of Wainwright & Jordan (2008, §3) for details. Thus,
our primary objective throughout the paper will be ﬁnding
solutions to the approximate problem

minimize

(cid:104)C, µ(cid:105)

s.t.

µ ∈ L2.

(P)

Let (cid:15) > 0. We say that a point (cid:98)µ ∈ L2 is (cid:15)-optimal for (P)
if it satisﬁes (cid:104)C, (cid:98)µ(cid:105) ≤ minµ∈L2 (cid:104)C, µ(cid:105) + (cid:15). For a random (cid:98)µ,
we say that it is expected (cid:15)-optimal if

E [(cid:104)C, (cid:98)µ(cid:105)] ≤ min
µ∈L2

(cid:104)C, µ(cid:105) + (cid:15).

Despite the simple form of the linear program, it has been
observed to be difﬁcult to solve in practice for large graphs
even with state-of-the-art solvers (Yanover et al., 2006),
motivating researchers to study an approximate version with
entropy regularization:

minimize

(cid:104)C, µ(cid:105) −

1
η

H(µ)

s.t.

µ ∈ L2,

(Reg-P)

where η ∈ R+ controls the level of regularization. Intu-
itively, the regularization encourages µi and µe to be closer
to the uniform distribution for all vertices and edges.

Accelerated Message Passing for Entropy-Regularized MAP Inference

The dual problem takes on the succinct form of an un-
constrained log-sum-exp optimization problem. Thus,
when combined, the local polytope relaxation and entropy-
regularizer result in a smooth approximation.
Proposition 1. The dual objective of (Reg-P) can be written
as

minimize
λ

L(λ),

(Reg-D)

where L is deﬁned as

L(λ) =

+

1
η

1
η

(cid:88)

i∈V
(cid:88)

log

log

(cid:88)

x∈χ
(cid:88)

exp (cid:0)−ηCi(x) + (cid:80)

e∈Ni

λe,i(x)(cid:1)

exp (cid:0)−ηCe(x) − (cid:80)

i∈e λe,i(xi)(cid:1) .

e∈E

x∈χ2

Algorithm 1 Standard-MP(Update, η, P, K)
1: λ(0) = 0
2: for k = 0, 1, . . . , K − 1 do
Set λ(k+1) = λ(k)
3:
Sample block-coordinate bk ∼ P
4:
Set λ(k+1)
= Updateη
bk
bk

(λ(k))

5:
6: end for
7: return arg minλ∈{λ(k)}

(cid:80)

e∈E,i∈e (cid:107)νλ

e,i(cid:107)2
1

algorithms can effectively be viewed as block-coordinate
descent, except that a full minimization is typically taken at
each step. Here we outline two variants.

Furthermore, primal variables can be recovered directly by

2.2.1. EDGE MESSAGE PASSING

i (xi) ∝ exp (cid:0)−ηCi(xi) + η (cid:80)
µλ
e (xe) ∝ exp (cid:0)−ηCe(xe) − η (cid:80)
µλ

e∈Ni

λe,i(xi)(cid:1)
i∈e λe,i((xe)i)(cid:1) .

For convenience we let rD = 2md denote the dimension
of the dual variables λ ∈ RrD . This is in contrast to the
dimension rP of the primal marginal vectors deﬁned earlier.
We use Λ∗ ⊆ RrD to denote the set of solutions to (Reg-D).

There is a simple interpretation to dual optimality coming
directly from the Lagrangian: a dual variable λ is optimal if
the candidate primal solution is primal feasible: µλ ∈ L2.
It can be seen that the derivative of the dual function L(λ)
captures the slack of a µλ:

∂L(λ)
∂λe,i(xi)

= µλ

i (xi) − Sλ

e,i(xi)

(1)

e,i(xi) := (cid:80)

xj ∈χ µλ

where we deﬁne Sλ
e (xi, xj). The gradi-
ent captures the amount and direction of constraint violation
in L2 by µλ. In order to discuss this object concisely and
intuitively, we formally deﬁne the notion of a slack vec-
tor, which is simply the negative of the gradient, and a
slack polytope (Lee et al., 2020), which describes the same
polytope as L2 if the constraints were offset by exactly the
amount by which µλ is offest.
Deﬁnition 1 (Slack vector and slack polytope). For λ ∈
RrD , the slack vector νλ ∈ RrD of λ is deﬁned as νλ
e,i(x) =
e,i(x) − µλ
Sλ

i (x) for all e ∈ E, i ∈ e, and x ∈ χ.

The slack polytope for a slack vector ν is deﬁned as

(cid:26)

Lν

2 :=

µ ∈ RrP

+ :

µi ∈ Σd
µe ∈ Ud(µi + νe,i, µj + νe,j)

(cid:27)

2.2. Entropy-Regularized Message Passing

Edge message passing (EMP) algorithms reduce to block-
coordinate methods that minimize (Reg-D) for a speciﬁc
edge e = {i, j} ∈ E and endpoint vertex i ∈ e, while keep-
ing all other dual variables ﬁxed. Let Le,i(·; λ) : Rd → R
denote the block-coordinate loss of L ﬁxed at λ except for
free variables {λe,i(x)}x∈χ. For each i ∈ V , e ∈ Ni we
deﬁne the EMP operator for λ ∈ RrD :
Le,i(λ(cid:48)

EMPη

e,i(·); λ).

e,i(λ) ∈ arg min
e,i∈Rd

λ(cid:48)

Proposition 2. The operator EMPη

is satisﬁed by λ(cid:48)

e,i(xi) = λe,i(xi) + 1

e,i(·) ∈ Rd
e,i : λ (cid:55)→ λ(cid:48)
Sλ
e,i(xi)
i (xi) .
2η log
µλ

In the entropy-regularized setting, this update rule has been
studied by Lee et al. (2020); Ravikumar et al. (2010). Non-
regularized versions based on max-sum diffusion have much
earlier roots and also been studied by Werner (2007; 2009);
however, we do not consider these particular unregularized
variants. EMP offers the following improvement on L.
Lemma 1. Let λ(cid:48) be the result of applying EMPη
e,i(λ) to λ,
keeping all other coordinates ﬁxed. Then, L(λ) − L(λ(cid:48)) ≥
1
4η (cid:107)νλ

e,i(cid:107)2
1.

2.2.2. STAR MESSAGE PASSING

Star message passing (SMP) algorithms consider block-
coordinates that include all edges incident to a particular
vertex i ∈ V . For λ ∈ RrD and i ∈ V let Li(·; λ) : Rd →
R denote the block-coordinate loss of L ﬁxed at λ. For a
given i ∈ V and arbitrary λ ∈ RrD , we deﬁne the SMP
operator:

SMPη

i (λ) ∈ arg min
·,i∈Rd|Ni|
λ(cid:48)

Li(λ(cid:48)

·,i(·); λ)

The results in this paper will be primarily concerned with
algorithms that approximately solve (MAP) by directly solv-
ing (Reg-D). For solving this objective, message passing

That is, SMP is the minimization over the block-coordinate
at vertex i for all edges incident to i in Ni and all possible
labels in χ.

Accelerated Message Passing for Entropy-Regularized MAP Inference

k−1

√

−θ2

3:
4:
5:
6:
7:

Algorithm 2 Accel-EMP(G, C, η, K)
1: λ(0) = 0, v(0) = 0, θ−1 = 1
2: for k = 0, 1, . . . , K − 1 do
k−1+4θ2
θ4
k−1+
θk =
2
y(k) = θkv(k) + (1 − θk)λ(k)
Sample (ek, ik) ∼ Unif {(e, i) : e ∈ E, i ∈ e}.
Set λ(k+1) = λ(k)
λ(k+1)
(·) = EMPη
e,i
v(k+1) = v(k)
8:
v(k+1)
= v(k)
9:
ek,ik
ek,ik
10: end for
11: return λ(K)

νy(k)
ek,ik

+ 1

(y(k))

2mηθk

ek,ik

Proposition 3. The operator SMPη
i
Rd|Ni| is, for all e ∈ Ni and xi ∈ χ, satisﬁed by

: λ (cid:55)→ λ(cid:48)

·,i(·) ∈

k−1

√

−θ2

Algorithm 3 Accel-SMP(G, C, η, K)
1: λ(0) = 0, v(0) = 0, θ−1 = 1
2: for k = 0, 1, . . . , K − 1 do
k−1+4θ2
θ4
k−1+
θk =
2
y(k) = θkv(k) + (1 − θk)λ(k)
Sample ik ∼ {pi}i∈V
Set λ(k+1) = λ(k)
λ(k+1)
(·) = SMPη
ik
·,i
v(k+1) = v(k)
for e ∈ Nik do
= v(k)
e,ik

+ minj |Nj |

3:
4:
5:
6:
7:

(y(k))

8:
9:

10:

2pik θkηN νy(k)

e,ik

v(k+1)
e,ik
end for

11:
12: end for
13: return λ(K)

λ(cid:48)

e,i(xi) = λe,i +

1
η

log Sλ

e,i(xi)

−

1
η(|Ni| + 1)

log (cid:0)µλ

i (xi) (cid:81)

e(cid:48)∈Ni

3. Accelerating Entropy-Regularized Message

e(cid:48),i(xi)(cid:1) ,
Sλ

Passing

The proof is similar to the previous one and is deferred
to the appendix. Meshi et al. (2012) gave a concise def-
inition and analysis of algorithms from this update rule,
and similar star-based algorithms have existed much earlier
(Wainwright & Jordan, 2008), such as MP-LP (Globerson
& Jaakkola, 2008). Due to Meshi et al. (2012), SMP also
has an improvement guarantee.
Lemma 2. Let λ(cid:48) be the result of applying SMPη
i to λ,
keeping all other coordinates ﬁxed. Then, L(λ) − L(λ(cid:48)) ≥

(cid:80)

1
8|Ni|η

e∈Ni

(cid:107)νλ

e,i(cid:107)2
1.

2.3. Randomized Standard Algorithms

The message passing updates described in the previous sub-
section can be applied to each block-coordinate in many
different orders. In this paper, we consider using the up-
dates in a randomized manner, adhering to the generalized
procedure presented in Algorithm 1. The algorithm takes
as input the update rule Update, which could be EMP or
SMP, and a regularization parameter η. It also requires a
distribution P over block-coordinates bk for each iteration
k ≤ K. In this paper, we will use the uniform distribution
over edge-vertex pairs for EMP:

bk = (ek, ik) ∼ Unif({(e, i) : e ∈ E, i ∈ e}).

(2)

For SMP, we use a categorical distribution over vertices
based on the number of neighbors of each vertex:

bk = ik ∼ Cat(V, {pi}i∈V ),

(3)

where pi =

(cid:80)

|Ni|
j∈V |Nj | for each i ∈ V .

We now present a scheme for accelerating message passing
algorithms in the entropy-regularized formulation. The key
idea is to leverage the block-coordinate nature of standard
message passing algorithms. We draw inspiration from
both the seminal work of Nesterov (2018) on accelerating
gradient methods and accelerating randomized coordinate
gradient methods similar to Lee & Sidford (2013); Lu &
Xiao (2015); however, the presented method is specialized
to incorporate full block-coordinate minimization at each
round, so as to be consistent with existing message passing
algorithms used in practice. Furthermore, we can leverage
the same simple sampling procedures for selecting the block-
coordinates. The presented scheme can thus be viewed as
a direct method of acceleration in that standard message
passing algorithms can be plugged in.

The scheme is presented in Algorithm 2 for EMP and Algo-
rithm 3 for SMP. In Accel-EMP, at each round k, a random
coordinate block is sampled uniformly. That coordinate
block for λ is then updated with a step of EMPη
evalu-
ated at y(k). A block-coordinate gradient step evaluated at
y(k) in the form of the slack vector νy(k)
is also applied to
ek,ik
v(k). Accel-SMP works similarly but we instead sample
from the non-uniform distribution deﬁned in (3). The choice
of distributions ultimately determines the step size for v.

ek,ik

As in the case of the standard smooth message passing al-
gorithms, which optimize the dual function (Reg-D), the
returned solutions may not be primal feasible in ﬁnite itera-
tions. To obtain feasible solutions, we consider a projection
operation Proj, shown in Algorithm 4, that is simply a re-
peated application of Algorithm 2 of Altschuler et al. (2017),
originally designed for optimal transport. The method ef-

Accelerated Message Passing for Entropy-Regularized MAP Inference

Algorithm 4 Proj(µ, ν)
1: Set (cid:98)µi = µi for all i ∈ V
2: for (i, j) = e ∈ E do
3:

Compute (cid:98)µe by applying Algorithm 2 of (Altschuler
et al., 2017) on µe with transportation polytope
Ud(µi + νe,i, µj + νe,j)

4: end for
5: return (cid:98)µ

fectively ﬁnds an edge marginal (cid:98)µe that sums to the given
vertex marginals µi and µj for e = (i, j) plus some optional
slack ν. For all practical purposes, we would always set the
slack to be ν = 0, ensuring that Proj outputs a point in L2;
however, it will become useful to project points from L2
into a particular slack polytope Lνλ
for the analysis. When
2
projecting onto L2, Proj does not require modifying the ver-
tex marginals, so there is no ambiguity if the approximate
solution is ultimately rounded to an integral solution using
a simple vertex rounding scheme2.

4. Main Results

We now present iteration complexities for the above algo-
rithms for ﬁnding (cid:15)-optimal solutions to the original unregu-
larized problem (P) over L2. These guarantees make it easy
to compare various algorithms as they do not inherently de-
pend on the tightness of the relaxation, rounding heuristics
to ﬁnd integral solutions, or arbitrary choices of η.

4.1. Standard Algorithms

Our ﬁrst result bounds the number of iterations required to
compute (cid:15)-optimal solutions to (P). To recapitulate, prior
work by Lee et al. (2020) for EMP only provided an iter-
ation guarantee for bounding the norm of the slack vector.
They also gave guarantees for the number of iterations re-
quired to round to the optimal MAP solution when it is
available. Meshi et al. (2012) gave a guarantee on both the
primal and dual regularized problems (Reg-P) and (Reg-D),
but not the original (P) and without rounding or tuning of
η. Additionally, both works focused mostly on the (cid:15) depen-
dence in the rate rather than actually specifying the graph
parameters m, n, and d.

In contrast to these prior works, we give a guarantee on
optimality for (P) for the standard randomized algorithms,
specifying exactly the dependence on graph parameters. The
purpose of this extension is to standardize convergence guar-
antees for the true relaxed problem, which will ultimately
be handy for comparing to our primary contribution on the

2Ambiguity could arise for more sophisticated rounding

schemes but we do not consider those here.

accelerated algorithms.

Theorem 1. Let (cid:98)λ be the result of running Algo-
rithm 1 with EMP, uniform sampling distribution (2) and
η = 4(m+n) log d
. Let (cid:98)µ = Proj(µ(cid:98)λ, 0) be its projection
onto L2. Then, the number of iterations sufﬁcient for (cid:98)µ to
be expected (cid:15)-optimal is

(cid:15)

O (cid:0)md2(m + n)4(cid:107)C(cid:107)3

∞(cid:15)−3 log d(cid:1) .

If (cid:98)λ is the output of Algorithm 1 using SMP and sampling
distribution (3), and (cid:98)µ := Proj(µ(cid:98)λ, 0) then (cid:98)µ is expected
(cid:15)-optimal in the same order of iterations.

The rate harbors a O(1/(cid:15)3) dependence, which at ﬁrst ap-
pears to be worse those of Meshi et al. (2012) and Lee et al.
(2020); however, their convergence guarantees hold only for
the regularized objective. The extra O(1/(cid:15)) in our guaran-
tee occurs in the conversion to the original unregularized
problem (P), which is a stronger result. It is interesting to
observe that the guarantees are effectively the same for both
variants, despite having somewhat different analyses. We
hypothesize that this is due to the fact that the “smoothness”
constant for SMP in Lemma 2 is greater than that of EMP
in Lemma 1. Therefore, the larger block-coordinate size is
effectively canceled by the smaller improvement per step.

We now describe the proof brieﬂy here since the ﬁrst part is
fairly standard while the second will be covered in the proof
of our main acceleration result. The full proof is found in
Appendix D. The basic idea is to use Lemma 1 to lower
bound the expected improvement each iteration, which can
be done in terms of the average squared norms of the slack
(cid:107)νe,i(cid:107)2
1. We can guarantee improvement on L by at least
((cid:15)(cid:48))2 until the norms are on average below (cid:15)(cid:48). Knowing that
the slack norms are small, we can prove that the projection
(cid:98)µ of µ(cid:98)λ onto L2 is not too far from µ(cid:98)λ and so the expected
value of (cid:104)C, (cid:98)µ(cid:105) is not much worse than that of (cid:104)C, µλ(cid:105). We
then prove that (cid:104)C, µλ(cid:105) is small with respect to the slack
norms up to some entropy term, and we set η so the entropy
term is sufﬁciently small with respect to a given (cid:15) > 0.

4.2. Accelerated Algorithms

Our primary result gives improved iteration complexities for
the accelerated versions of EMP and SMP. To do so, we rely
on the classic estimate sequence method initially developed
by Nesterov. In particular, we turn to a randomized variant,
which has appeared before in the literature on randomized
coordinate gradient methods by Lee & Sidford (2013); Lu
& Xiao (2015) for the strongly convex and general cases
respectively. Our main contributions are both extending
these results for the full minimization of message passing
to achieve the fast rates and also proving the accelerating
guarantee on the original relaxed problem (P) rather than
the regularized problems.

Accelerated Message Passing for Entropy-Regularized MAP Inference

Theorem 2. Let (cid:98)λ be the output of Algorithm 2 with
. Let ˆµ = Proj(µˆλ, 0) be its projec-
η = 4(m+n) log d
tion onto L2. Then, the number of iterations sufﬁcient for ˆµ
to be expected (cid:15)-optimal is

(cid:15)

(cid:16)

O

m3/2d2(m + n)3(cid:107)C(cid:107)2

(cid:17)
∞(cid:15)−2 log d

.

If (cid:98)λ is the output of Algorithm 3 and (cid:98)µ := Proj(µ(cid:98)λ, 0), then
(cid:98)µ is expected (cid:15)-optimal in the same order of iterations.

The primary difference between the iteration complexities
for the standard algorithms and the accelerated ones is the
dependence on (cid:15). For the accelerated algorithms, we are
left with only a O(1/(cid:15)2) dependence versus the O(1/(cid:15)3)
dependence for the standard algorithms. This can lead to
far fewer iterations to get the same level of accuracy on
the original relaxed problem (P). In addition, the bounds in
Theorem 2 are strictly better in dependence on the number
of edges as well for both EMP and SMP. For m + n ≈
m, the accelerated algorithms shave off a m1/2 factor. It
is interesting to observe that these improved guarantees
come with virtually no extra computation per iteration. For
example, to update the sequences λ(k), v(k), and y(k) in
EMP at each iteration, we need only compute the primal
variables µλ(k)
ek,ik once to use in both the slack
ik
vector νλ(k)

ek,ik and the update rule EMPη

and Sλ(k)

(λ(k)).

ek,ik

We will give the proof for Accel-EMP to convey the main
idea. The analogous Accel-SMP case can be found in the
appendix. First, we will derive a faster convergence rate on
the dual objective, which in turn implies that we can bound
the slack norms by the same (cid:15)(cid:48) > 0 in fewer iterations.
In the second part, we will bound the approximation error
caused by the entropy regularization. Finally, we put these
pieces together to determine the appropriate choice of (cid:15)(cid:48) and
η in terms of (cid:15) to recover the ﬁnal rate.

4.2.1. FASTER CONVERGENCE ON THE DUAL

The ﬁrst steps will involve deﬁning a randomized estimate
sequence to work with and then using this sequence to prove
the faster convergence rate on the dual objective.
Deﬁnition 2. Let φ0 : RrD → R be an arbitrary determin-
istic function. A sequence {φk, δk}K
k=0 of random functions
φk : RrD → R for k ≥ 1 and deterministic real values
δk ∈ R+ is a randomized sequence for L(λ) if it satisﬁes
k→ 0 and, for all k, E[φk(λ)] ≤ (1−δk)L(λ)+δkφ0(λ).
δk

If we are given a random estimate sequence {φk, δk}K
and a
E[L(λ(k))] ≤ minλ E [φk(λ)], then

random sequence {λ(k)}K

k=0
satisﬁes

that

k=0

E[L(λ(k))] − L(λ∗) ≤ min
λ

E[φk(λ)] − L(λ∗)

≤ δk(φ0(λ∗) − L(λ∗))

(4)

k→ 0. We
This expected error converges to zero since δk
now identify a candidate estimate sequence. Let λ(0) = 0,
δ0 = 1, λ∗ ∈ Λ∗, and q := 2m. Let the sequence {θk}K
be as it is deﬁned in Algorithm 2 and let {y(k)}K
be arbitrary. Consider {φk, δk}K

k=0 ⊂ RrD
k=0 deﬁned recursively as

k=0

(ek, ik) ∼ Unif{(e, i) : e ∈ E, i ∈ e}

δk+1 = (1 − θk)δk

φk+1(λ) = (1 − θk)φk(λ) + θkL(y(k))
, λek,ik − y(k)
ek,ik

− qθk(cid:104)νy(k)
ek,ik

(cid:105)

(5)

2 (cid:107)λ(0) − λ(cid:107)2

where φ0(λ) = L(λ(0)) + γ0
2 for γ0 = 2q2η.
Lemma 3. The sequence {φk, δk}K
k=0 deﬁned in (5) is a
random estimate sequence. Furthermore, it maintains the
form φk(λ) = ωk + γk

2 (cid:107)λ − v(k)(cid:107) for all k where

γk+1 = (1 − θk)γk
(cid:40)

v(k+1)
e,i =

v(k)
e,i + qθk
γk+1
v(k)
e,i

νy(k)
e,i

if (e, i) = (ek, ik)
otherwise

ωk+1 = (1 − θk)ωk + θkL(y(k)) −

(θkq)2
2γk+1

(cid:107)νy(k)
ek,ik

(cid:107)2
2

− θkq(cid:104)νy(k)
ek,ik

, v(k)
ek,ik

− y(k)
ek,ik

(cid:105)

The proof is similar to what is given by Lee & Sidford
(2013), but since we consider the non-strongly convex case
and slightly different deﬁnitions, we give a full proof in
Appendix C for completeness. We can use this fact to show
a rate of convergence on the dual objective.

k=0 and {y(k)}K

Lemma 4. For the random estimate sequence in (5), let
{λ(k)}K
k=0 be deﬁned as in Algorithm 2
with λ(0) = 0. Then, the dual objective error in expectation
can be bounded as E[L(λ(k)) − L(λ∗)] ≤ G(η)2
(k+2)2 , where
√
G(η) := 24md(m + n)(

η(cid:107)C(cid:107)∞ + log d

√

η ).

Proof. It sufﬁces to show that the sequence in (5) with
the deﬁnitions of {λ(k)}K
k=0 satisﬁes
E[L(λ(k))] ≤ minλ E [φk(λ)]. To do this, we will use
induction and Lemma 1. Note that minλ φk(λ) = ωk. The
base case holds trivially with E[ω0] = L(λ(0)). Suppose
E (cid:2)L(λ(k))(cid:3) ≤ E [ωk] at iteration k. For k + 1, we have

k=0 and {y(k)}K

E[ωk+1]
≥ (1 − θk)E[L(λ(k))] + θkE[L(y(k))]
2 − θkq(cid:104)νy(k)
(cid:107)2

(cid:107)νy(k)
ik,ek

− E

(cid:20) (θkq)2
2γk+1

ek,ik

(cid:21)
, v(k) − y(k)(cid:105)

.

Accelerated Message Passing for Entropy-Regularized MAP Inference

The above inequality uses the inductive hypothesis. Then,

(cid:20)
L(y(k)) −

≥ E

+ (1 − θk)E
(cid:104)

+ θkE
(cid:20)
L(y(k)) −

= E

(cid:21)

(cid:107)νy(k)
ik,ek

(θkq)2
2γk+1
(cid:104)
(cid:104)∇L(y(k)), λ(k) − y(k)(cid:105)

(cid:107)2
2

(cid:105)

(cid:105)
(cid:104)∇L(y(k)), v(k) − y(k)(cid:105)

θ2
kq
2γk+1

(cid:107)∇L(y(k))(cid:107)2
2

(cid:21)

.

This second inequality uses convexity of L and then applies
the law of total expectation to the dot products, condition-
ing on randomness {es, is}k−1
s=1 . The last identity uses the
deﬁnition of y(k) and total expectation again on the norm.

Using Lemma 1 and the deﬁnition of λ(k+1) from Algo-
rithm 2, we have

E[L(λ(k+1)] ≤ E[L(y(k)) −

= E[L(y(k)) −

(cid:107)νy(k)
ek,ik

(cid:107)2
2]

1
4η
1
4qη

Proposition 4. Let µ∗ ∈ L2 be optimal, λ ∈ RrD ,
(cid:98)µ = Proj(µλ, 0) ∈ L2, and δ = maxe∈E,i∈e (cid:107)νλ
e,i(cid:107)1. The
following inequality holds:

(cid:104)C, (cid:98)µ − µ∗(cid:105) ≤ 16(m + n)d(cid:107)C(cid:107)∞δ
(cid:88)

+ 4(cid:107)C(cid:107)∞

(cid:107)νλ

e,i(cid:107)1 +

e∈E,i∈e

4.2.3. COMPLETING THE PROOF

n log d + 2m log d
η

.

Proof of Theorem 2 for EMP. Let (cid:98)λ be the output from Al-
gorithm 2 after K iterations. From Lemma 1, we can lower
1] ≤ G(η)
bound the result in Lemma 4 with 1
(K+2)2
4η
for all e ∈ E, i ∈ e. Then, for (cid:15)(cid:48) > 0, we can ensure that
and E (cid:88)

1 ≤ 2m((cid:15)(cid:48))2

e,i(cid:107)1] ≤ (cid:15)(cid:48)

E[(cid:107)ν (cid:98)λ

E[(cid:107)ν (cid:98)λ

e,i(cid:107)2

e,i(cid:107)2

(cid:107)ν (cid:98)λ

e∈E,i∈e

√

4ηG(η)
(cid:15)(cid:48)

iterations. Let (cid:98)µ ∈ L2 be the projected
in K =
version of µ(cid:98)λ. Taking the expectation of both sides of the
result in Proposition 4 gives us

(cid:107)∇L(y(k))(cid:107)2
2]

E[(cid:104)C, (cid:98)µ − µ∗(cid:105)] ≤ (cid:107)C(cid:107)∞ (16(m + n)dE[δ] + 8m(cid:15)(cid:48))

k := γk+1

The equality uses the law of total expectation. There-
2q2η ensures that minλ E[φk+1(λ)] ≥
fore, taking θ2
E[ωk+1] ≥ E[L(λ(k+1)]. By setting γ0 = 2q2η, we ensure
that θk need only satisfy θ2
k−1, which oc-
curs in Algorithm 2. From Nesterov (2018, Lemma 2.2.4),

k = (1 − θk)θ2

this choice of γ0 and θk ensures δk ≤
(k+2)2 . From (4) and the deﬁnition of φ0, we get

1 + k
q

4

(cid:16)

(cid:17)−2

(cid:113) γ0
8η

=

E[L(λ(k)) − L(λ∗)] ≤

4L(0) − 4L(λ∗) + 16m2η(cid:107)λ∗(cid:107)2
2
(k + 2)2

.

The proof that the numerator can be bounded by G(η)2 is
deferred to the appendix, Lemma 7.

+

n log d + 2m log d
η

,

where E [δ]2 ≤ E[δ2] ≤ E (cid:80)
Then we can conclude

e∈E,i∈e (cid:107)ν (cid:98)λ

e,i(cid:107)2

1 ≤ 2m((cid:15)(cid:48))2.

E[(cid:104)C, (cid:98)µ − µ∗(cid:105)] ≤ 16

√

2m(m + n)d(cid:107)C(cid:107)∞(cid:15)(cid:48)

+ 8m(cid:107)C(cid:107)∞(cid:15)(cid:48) +

n log d + 2m log d
η
2m(m + n)d(cid:107)C(cid:107)∞(cid:15)(cid:48)

√

≤ 24

+

n log d + 2m log d
η

.

Therefore, (cid:98)µ is expected (cid:15)-optimal with η as deﬁned in the
statement and (cid:15)(cid:48) =
. Substituting these
48
values into K and G(η) yields the result.

2m(m+n)d(cid:107)C(cid:107)∞

√

(cid:15)

4.2.2. APPROXIMATION ERROR DUE TO ENTROPY

5. Rounding to Integral Solutions

Recall that our end goal is to ensure that (cid:98)µ, the projection of
µ(cid:98)λ onto L2, is expected (cid:15)-optimal for (cid:15) > 0. To show this,
we need to develop some relations which we outline here for
brevity but state formally and prove in Appendix C.2. The
ﬁrst relation is how close (cid:98)µ ∈ L2 is to the algorithm’s output
µ(cid:98)λ, which is in the slack polytope Lν (cid:98)λ
2 . This is essentially a
direct extension of Altschuler et al. (2017, Lemma 7), which
tells us that we can bound the projection by the norm of
the slacks. Then, we show that there exists a point (cid:98)µ∗ in
the slack polytope Lν (cid:98)λ
that is close to the optimal point
2
µ∗ ∈ L2 with respect to the norm of the slacks also. We
use the relations to conclude a bound on the original relaxed
problem (P) for any realization of (cid:98)µ.

Inspired by recent results regarding the approximation er-
ror achieved by entropy regularization in linear program-
ming (Weed, 2018), we are able to derive rounding guar-
antees for our algorithms under the assumption that the
LP relaxation is tight and the solution is unique. We use
a simple rounding scheme: for any µ that may not lie in
L2, (round(µ))i = arg maxx µi(x). The main challenge
in achieving the results we present here is surpassing the
difﬁculty in obtaining bounds for the l1 distance between
(cid:98)µ = µ(cid:98)λ, the candidate solution obtained from the ﬁnal it-
erate (cid:98)λ resulting from our algorithms, and µ∗, the optimal
solution of (P). Deﬁne µλ∗
be the solution to the regularized
problem where λ∗ ∈ Λ∗.

Accelerated Message Passing for Entropy-Regularized MAP Inference

Figure 1. (Left column): The original primal objective (P) on a log scale is compared for the standard algorithms and their accelerated
variants, as well as standard (accelerated) coordinate descent, over 10 trials on an Erd˝os-Rényi random graph with n = 100. Error bars
denote standard deviation. (Center and right columns): The log-competitive ratio of EMP and SMP with respect to their accelerated
variants, Accel-EMP and Accel-SMP, on the primal objective (P).

We proceed in two steps. First, we bound the approxima-
tion error (cid:107)µλ∗
− µ∗(cid:107)1 using recent results on the quality
of solutions for entropy regularized L2 (Lee et al., 2020).
Then we bound the optimization error of (cid:98)µ using the results
derived in the previous section. The proof and a compar-
ison to standard EMP are in Appendix F. Let deg denote
the maximum degree of the graph G and deﬁne ∆ as the
suboptimality gap of the LP over L2.
Theorem 3. Let δ ∈ (0, 1). If L2 is tight for potential vector
C and there exist a unique solution to the MAP problem and
η = 16(m+n)(log(m+n)+log(d))
then with probability 1 − δ
in at most

∆

O

(cid:18) d3m7 deg2 (cid:107)C(cid:107)2

∞ log2 dm

(cid:19)

δ∆

iterations of Accel-EMP, round((cid:98)µ) is the optimal MAP
assignment.

6. Numerical Experiments

Our goal now is to understand the empirical differences
between the above algorithms and also where certain the-
oretical guarantees can likely be improved, if at all. To
this end, we compare the convergence rates of EMP and
SMP and their accelerated variants on several synthesized
Erd˝os-Rényi random graphs. First, we constructed a graph
with n = 100 vertices and then generated edges between
each pair of vertices with probability 1.1 log n
n . We con-
sidered the standard multi-label Potts model with d = 3
labels. The cost vector C was initialized randomly in the
following way: Ci(xi) ∼ Unif([−0.01, 0.01]), ∀xi ∈ χ
and Cij(xi, xj) ∼ Unif({−1.0, 1.0}), ∀xi, xj ∈ χ.

We consider two different metrics. (1) The ﬁrst is the orig-
inal primal objective value (P). This metric computes the
objective value of the projection (cid:98)µ = Proj(µλ, 0). (2) The
second reports the log-competitive ratio between the stan-
dard and accelerated variants. The competitive ratio is com-
(cid:17)
, where (cid:98)µEMP and (cid:98)µAccel-EMP
puted as log

(cid:16) (cid:104)C,(cid:98)µEMP−µ∗(cid:105)
(cid:104)C,(cid:98)µAccel-EMP−µ∗(cid:105)

are the projections due to Proj. Positive values indicate that
the accelerated variant has lower error. We implemented
the four message passing algorithms exactly as they are
described in Algorithms 1, 2, and 3. We also implemented
block-coordinate descent and its accelerated variant as base-
lines (Lee & Sidford, 2013) with a step size of 1/η. Each
algorithm used η = 1000 over 10 trials, measuring means
and standard deviations. We computed the ground-truth
optimal value of (P) using a standard solver in CVXPY.

Figure 1 depicts convergence on the primal objective in
the left column. SMP achieves convergence in the fewest
iterations, and EMP converges faster than coordinate de-
scent. Interestingly, we ﬁnd that the accelerated variants,
including accelerated coordinate descent, appear to have
marginal improvement on this metric. However, the com-
petitive ratio ﬁgures conﬁrm that the accelerated variants
are consistently faster, especially for SMP. These results
suggest that, at least for this particular problem, the upper
bounds for standard algorithms may be overly conservative.
It would be interesting to investigate tighter bounds for the
standard algorithms in future work. Further details can be
found in the appendix.

7. Conclusion

We analyze the convergence of message passing algorithms
on the MAP inference problem over L2. In addition to pro-
viding a novel rate of convergence rate for standard schemes
derived from entropy regularization, we show that they can
be directly accelerated in the sense of Nesterov with signif-
icant theoretical improvement. In future work it would be
interesting to consider accelerating greedy message passing
algorithms; however, Lu et al. (2018) suggest that, despite
empirical success, proving accelerated rates for greedy meth-
ods is an open question even in the basic coordinate descent
case. The tightness of the presented guarantees is also an
open question, motivated by the empirical results here. Fi-
nally, we conjecture that reductions from the active area of
optimal transport could yield novel, faster algorithms.

010002000300040005000Updates101102(P) errorPrimal ObjectiveEMPAccel-EMPSMPAccel-SMPCDAccel-CD020004000600080001000012000Updates0.020.010.000.010.02Log(comp. ratio)EMP Competitive Ratio010002000300040005000Updates0.000.050.100.150.200.250.30Log(comp. ratio)SMP Competitive RatioAccelerated Message Passing for Entropy-Regularized MAP Inference

References

Altschuler, J., Weed, J., and Rigollet, P. Near-linear time ap-
proximation algorithms for optimal transport via sinkhorn
iteration. In Advances in Neural Information Processing
Systems, pp. 1964–1974, 2017.

Benamou, J.-D., Carlier, G., Cuturi, M., Nenna, L., and
Peyré, G. Iterative bregman projections for regularized
transportation problems. SIAM Journal on Scientiﬁc Com-
puting, 37(2):A1111–A1138, 2015.

Cooper, G. F. The computational complexity of probabilis-
tic inference using bayesian belief networks. Artiﬁcial
intelligence, 42(2-3):393–405, 1990.

Cuturi, M. Sinkhorn distances: Lightspeed computation
of optimal transport. In Advances in Neural Information
Processing Systems, pp. 2292–2300, 2013.

Dvurechensky, P., Gasnikov, A., and Kroshnin, A. Com-
putational optimal transport: Complexity by accelerated
gradient descent is better than by sinkhorn’s algorithm.
In 35th International Conference on Machine Learning,
ICML 2018, pp. 2196–2220, 2018.

Genevay, A., Cuturi, M., Peyré, G., and Bach, F. Stochastic
In Ad-
optimization for large-scale optimal transport.
vances in Neural Information Processing Systems, pp.
3440–3448, 2016.

Globerson, A. and Jaakkola, T. S. Fixing max-product:
Convergent message passing algorithms for map lp-
relaxations. In Advances in Neural Information Process-
ing Systems, pp. 553–560, 2008.

Gondzio, J. Interior point methods 25 years later. European
Journal of Operational Research, 218(3):587–601, 2012.

Hazan, T. and Shashua, A. Convergent message-passing
algorithms for inference over general graphs with con-
vex free energies. In Proceedings of the Twenty-Fourth
Conference on Uncertainty in Artiﬁcial Intelligence, pp.
264–273, 2008.

Jegelka, S. and Bilmes, J. Submodularity beyond submod-
ular energies: coupling edges in graph cuts. In IEEE
Conference on Computer Vision and Pattern Recognition,
pp. 1897–1904. IEEE, 2011.

Jojic, V., Gould, S., and Koller, D. Accelerated dual decom-
position for map inference. In Proceedings of the 27th
International Conference on International Conference on
Machine Learning, pp. 503–510, 2010.

Kappes, J., Andres, B., Hamprecht, F., Schnorr, C.,
Nowozin, S., Batra, D., Kim, S., Kausler, B., Lellmann,
J., Komodakis, N., et al. A comparative study of modern

inference techniques for discrete energy minimization
problems. In IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1328–1335, 2013.

Karmarkar, N. A new polynomial-time algorithm for linear
programming. In Proceedings of the sixteenth annual
ACM symposium on Theory of computing, pp. 302–311,
1984.

Kolmogorov, V. Convergent tree-reweighted message pass-
ing for energy minimization. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 28(10):1568–
1583, 2006.

Kolmogorov, V. and Zabin, R. What energy functions can be
minimized via graph cuts? IEEE Transactions on Pattern
Analysis and Machine Intelligence, 26(2):147–159, 2004.

Lee, J., Pacchiano, A., and Jordan, M. Convergence rates
of smooth message passing with rounding in entropy-
regularized map inference. In International Conference
on Artiﬁcial Intelligence and Statistics, pp. 3003–3014,
2020.

Lee, Y. T. and Sidford, A. Efﬁcient accelerated coordinate
descent methods and faster algorithms for solving lin-
ear systems. In 2013 IEEE 54th Annual Symposium on
Foundations of Computer Science, pp. 147–156. IEEE,
2013.

Lee, Y. T. and Sidford, A. Path ﬁnding methods for lin-
ear programming: Solving linear programs in o (vrank)
iterations and faster algorithms for maximum ﬂow. In
2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 424–433. IEEE, 2014.

Lin, T., Ho, N., and Jordan, M. I. On the acceleration of the
sinkhorn and greenkhorn algorithms for optimal transport.
arXiv preprint arXiv:1906.01437, 2019.

Lu, H., Freund, R., and Mirrokni, V. Accelerating greedy
coordinate descent methods. In International Conference
on Machine Learning, pp. 3257–3266, 2018.

Lu, Z. and Xiao, L. On the complexity analysis of random-
ized block-coordinate descent methods. Mathematical
Programming, 152(1-2):615–642, 2015.

MacKay, D. J. Information theory, inference and learning

algorithms. Cambridge university press, 2003.

Meshi, O., Globerson, A., and Jaakkola, T. S. Convergence
rate analysis of map coordinate minimization algorithms.
In Advances in Neural Information Processing Systems,
pp. 3014–3022, 2012.

Meshi, O., Mahdavi, M., and Schwing, A. Smooth and
strong: Map inference with linear convergence. In Ad-
vances in Neural Information Processing Systems, pp.
298–306, 2015.

Accelerated Message Passing for Entropy-Regularized MAP Inference

Wainwright, M. J. and Jordan, M. I. Graphical models, ex-
ponential families, and variational inference. Foundations
and Trends R(cid:13) in Machine Learning, 1(1–2):1–305, 2008.

Weed, J. An explicit analysis of the entropic penalty in
linear programming. In Conference On Learning Theory,
pp. 1841–1855, 2018.

Weiss, Y., Yanover, C., and Meltzer, T. Map estimation, lin-
ear programming and belief propagation with convex free
energies. In Proceedings of the Twenty-Third Conference
on Uncertainty in Artiﬁcial Intelligence, pp. 416–425,
2007.

Werner, T. A linear programming approach to max-sum
problem: A review. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 29(7):1165–1179, 2007.

Werner, T. Revisiting the linear programming relaxation
approach to gibbs energy minimization and weighted con-
straint satisfaction. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 32(8):1474–1488, 2009.

Yanover, C., Meltzer, T., and Weiss, Y. Linear program-
ming relaxations and belief propagation–an empirical
study. Journal of Machine Learning Research, 7(Sep):
1887–1907, 2006.

Mezard, M. and Montanari, A. Information, physics, and

computation. Oxford University Press, 2009.

Nesterov, Y. Smooth minimization of non-smooth functions.
Mathematical programming, 103(1):127–152, 2005.

Nesterov, Y. Efﬁciency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.

Nesterov, Y. Lectures on convex optimization, volume 137.

Springer, 2018.

Ravikumar, P., Agarwal, A., and Wainwright, M. J. Message-
passing for graph-structured linear programs: Proximal
methods and rounding schemes. Journal of Machine
Learning Research, 11(Mar):1043–1080, 2010.

Renegar, J. A polynomial-time algorithm, based on new-
ton’s method, for linear programming. Mathematical
programming, 40(1-3):59–93, 1988.

Savchynskyy, B., Kappes, J., Schmidt, S., and Schnörr, C. A
study of nesterov’s scheme for lagrangian decomposition
In IEEE Conference on Computer
and map labeling.
Vision and Pattern Recognition, pp. 1817–1823. IEEE,
2011.

Savchynskyy, B., Schmidt, S., Kappes, J., and Schnörr, C.
Efﬁcient mrf energy minimization via adaptive diminish-
ing smoothing. arXiv preprint arXiv:1210.4906, 2012.

Schiex, T., Fargier, H., and Verfaillie, G. Valued constraint
satisfaction problems: hard and easy problems. In Pro-
ceedings of the 14th International Joint Conference on
Artiﬁcial Intelligence, pp. 631–637. Morgan Kaufmann
Publishers Inc., 1995.

Sherali, H. D. and Adams, W. P. A hierarchy of relaxations
between the continuous and convex hull representations
for zero-one programming problems. SIAM Journal on
Discrete Mathematics, 3(3):411–430, 1990.

Sidford, A. and Tian, K. Coordinate methods for accelerat-
ing (cid:96)∞ regression and faster approximate maximum ﬂow.
In 2018 IEEE 59th Annual Symposium on Foundations of
Computer Science (FOCS), pp. 922–933. IEEE, 2018.

Sontag, D., Globerson, A., and Jaakkola, T. Introduction
to dual composition for inference. In Optimization for
Machine Learning. MIT Press, 2011.

Torada, L., Lorenzon, L., Beddis, A., Isildak, U., Pattini, L.,
Mathieson, S., and Fumagalli, M. Imagene: a convolu-
tional neural network to quantify natural selection from
genomic data. BMC bioinformatics, 20(9):337, 2019.

Accelerated Message Passing for Entropy-Regularized MAP Inference

A. Numerical Experiments Details

All experiments were run on an Intel Core i7 processor with 16 GB RAM. We consider two different metrics.

1. The ﬁrst metric is the original primal objective value (P), which is the actual value we wish to minimize. Since the
optimization is done in the dual variables, we use Proj to project the point µλ onto L2 and report the projection’s
function value, additively normalized with respect to the optimal value.

2. The second metric emphasizes the ﬁrst by reporting the log-competitive ratio between the standard and accelerated
, where (cid:98)µEMP and (cid:98)µAccel-EMP
variants of the the algorithms. The competitive ratio is computed as log
are the projections due to Proj of the outputs of EMP and Accel-EMP, respectively, and µ∗ is a minimizer over L2.
The same is computed for SMP and Accel-SMP. Thus, positive values at a given time indicate that the accelerated
variant has lower error on the original objective.

(cid:16) (cid:104)C,(cid:98)µEMP−µ∗(cid:105)
(cid:104)C,(cid:98)µAccel-EMP−µ∗(cid:105)

(cid:17)

Message Passing Algorithms We implemented the message pass-
ing algorithms with their update rules exactly as prescribed in Algo-
rithms 1, 2, and 3. The algorithms are compared with respect to the
number of updates (i.e. iterations); however, we note that the cost
of each update is greater for SMP and Accel-SMP since they both
require computing slacks of the entire neighborhood surrounding a
give vertex.

Block-Coordinate Methods
In addition to studying the empirical
properties of the message passing algorithms, we present a supple-
mentary empirical comparison with block-coordinate descent and its
accelerated variant (Lee & Sidford, 2013). The purpose of this inclu-
sion is to standardize how much we expect acceleration to improve
the algorithms. We chose a stepsize of 1/η. We note that each update
in block-coordinate descent is essentially as expensive as an update
of EMP.

This choice of cost vectors C ensures that vertices cannot be trivially
set to their minimal vertex potentials to achieve a reasonable objective
value; the MAP algorithm must actually consider pairwise interactions.
We evaluated each of the four algorithms on the same graph with η =
1000. Due to the inherent stochasticity of the randomized algorithms,
we ran each one 10 times and took the averages and standard deviations. Since the graphs are small enough, we computed
the ground-truth optimal value of (P) using a standard LP solver in CVXPY.

Figure 2. The competitive ratio of SMP with respect to
Accel-SMP on (P) is compared across random graphs
of varying sizes n = 9, 36, and 81.

In order to understand the effect of the graph size on the competitive ratio between the standard and accelerated algorithms,
we generated random graphs of sizes n = 9, 36, and 81 with the same randomly drawn edges and cost vectors. We ran
SMP and Accel-SMP for a ﬁxed number of iterations over 10 random trials and computed the average log competitive ratio.
Again, we used η = 1000. Figure 2 shows that Accel-SMP runs faster in all regimes, especially at beginning, and then the
performance improvement eventually tapers after many iterations.

B. Omitted Proofs and Derivations of Section 2

B.1. Proof of Proposition 1

Recall that the primal objective is to solve the following:

minimize

(cid:104)C, µ(cid:105) −

1
η

H(µ)

s.t.

µ ∈ L2,

(obj)

where

H(µ) = −

(cid:88)

(cid:88)

i∈V

xi∈χ

µi(xi)(log µi(xi) − 1) −

(cid:88)

(cid:88)

e∈E

xe∈χ2

µe(xe)(log µe(xe) − 1).

05001000150020002500Updates0.050.000.050.100.15Log(comp. ratio)SMP Competitive Ratio, Varying Graph Sizen = 9n = 36n = 81Accelerated Message Passing for Entropy-Regularized MAP Inference

Though it is not strictly necessary, we will also include a normalization constraint on the pseudo-marginal edges, which
amounts to (cid:80)

µe(xe) = 1 for all e ∈ E. The Lagrangian is therefore

xe

L(µ, λ, ξ) = (cid:104)C, µ(cid:105) −

1
η

H(µ) +

(cid:88)

λ(cid:62)

e,i(Se,i − µi) +

e∈E,i∈e

(cid:88)

(cid:88)

ξi(

i∈V

xi

µi(xi) − 1) +

(cid:88)

(cid:88)

ξe(

e∈E

xe

µe(xe) − 1)

Taking the derivative w.r.t µ yields

∂L(µ, λ, ξ)
∂µi(xi)

∂L(µ, λ, ξ)
∂µe(xe)

= Ci(xi) +

= Ce(xe) +

1
η

1
η

log µi(xi) + ξi −

(cid:88)

λe,i(xi)

e∈Ni
(cid:88)

log µe(xe) + ξe +

i∈e

λe,i((xe)i).

(6)

(7)

Here we are using (xe)i to denote selecting the label associated with endpoint i ∈ V from the pair of labels xe ∈ χ2. The
necessary conditions for optimality imply that

(cid:32)

µλ,ξ
i

(xi) = exp

−ηCi(xi) − ηξi + η

(cid:33)

λe,i(xi)

(cid:88)

e∈Ni

(cid:32)

µλ,ξ
e

(xe) = exp

−ηCe(xe) − ηξe − η

(cid:33)

λe,i((xe)i)

,

(cid:88)

i∈e

(8)

(9)

where we use the superscripts to show that these optimal values are dependent on the dual variables, λ and ξ. The dual
problem then becomes

maximize
λ,ξ

−

1
η

(cid:88)

(cid:88)

i

xi

µλ,ξ
i

(xi) −

1
η

(cid:88)

(cid:88)

c

xc

µλ,ξ
c

(xc) −

(cid:88)

i∈V

ξi −

(cid:88)

e∈E

ξe

Note that we can solve exactly for ξ as well, which simply normalizes the individual pseudo-marginals for each edge and
vertex so that

ξi =

ξe =

1
η

1
η

(cid:88)

log

xi

(cid:88)

log

xe

(cid:32)

exp

−ηCi(xi) − ηξi + η

(cid:33)

λe,i(xi)

(cid:88)

e∈Ni

(cid:32)

exp

−ηCe(xe) − ηξe − η

(cid:33)

λe,i((xe)i)

(cid:88)

i∈e

Plugging this into µλ,ξ ensures that each local vertex and edge distribution is normalized to 1. Therefore the ﬁnal objective
becomes

minimize
λ

m + n
η

+

1
η

(cid:88)

i∈V

(cid:88)

log

(cid:32)

exp

−ηCi(xi) − ηξi + η

xi

(cid:32)

exp

−ηCe(xe) − ηξe − η

(cid:88)

i∈e

(cid:33)

λe,i(xi)

(cid:88)

e∈Ni

(cid:33)

λe,i((xe)i)

,

+

1
η

(cid:88)

e∈E

(cid:88)

log

xe

and we can ignore the constant.

B.2. Entropy-Regularized Message Passing Derivations

In this section, we derive the standard message passing algorithms that will be the main focus of the paper. Both come from
simply computing the gradient and choosing additive updates to satisfy the optimality conditions directly.
Sλ
e,i(xi)
i (xi) .
µλ

Proposition 2. The operator EMPη

e,i(·) ∈ Rd is satisﬁed by λ(cid:48)

e,i(xi) = λe,i(xi) + 1

e,i : λ (cid:55)→ λ(cid:48)

2η log

Accelerated Message Passing for Entropy-Regularized MAP Inference

Proof. From (1), the partial gradient of L with respect to coordinate (e, i, xi) yields the following necessary and sufﬁcient
optimality condition:

Suppose that λ(cid:48) satisﬁes this condition, and thus minimizes Le,i(·; λ). We can decompose λ(cid:48) at coordinate (e, i, xi)
e,i(xi) = λe,i(xi) + δe,i(xi). From the deﬁnition of µλ, the optimality condition becomes
additively as λ(cid:48)

Sλ
e,i(xi) = µλ

i (xi).

exp(2ηδc,i(xi)) =

(cid:80)

e (xi, xj)

xj ∈χ µλ
µλ
i (xi)

Rearranging to ﬁnd δe,i(xi) and then substituting into λ(cid:48)

e,i(xi) yields the desired result.

Now, we can derive a lower bound on the improvement on the dual objective L from applying an update of EMP.
Lemma 1. Let λ(cid:48) be the result of applying EMPη
1
4η (cid:107)νλ

e,i(λ) to λ, keeping all other coordinates ﬁxed. Then, L(λ) − L(λ(cid:48)) ≥

e,i(cid:107)2
1.

Proof. Let (cid:101)µ denote the unnormalized marginals. From the deﬁnition of L,
(cid:32)

(cid:33)

L(λ) − L(λ(cid:48)) =

log

(cid:88)

1
η

exp

−ηCi(xi) + η

λe,i(xi)

+

log

(cid:88)

xi

e∈Ni

1
η

(cid:32)

exp

−ηCe(xe) − η

(cid:33)

λe,i(xi)

(cid:88)

i∈e

(cid:88)

xe

−

−

1
η

1
η

(cid:88)

log

xi

(cid:88)

log

xe

(cid:32)

exp

−ηCi(xi) + ηδe,i(xi) + η

(cid:32)

exp

−ηCe(xe) − ηδe,i(xi) − η

(cid:33)

λe,i(xi)

(cid:88)

e∈Ni

(cid:33)

λe,i(xi)

(cid:88)

i∈e

i (xi) = exp (cid:0)−ηCi(xi) + (cid:80)

Deﬁne (cid:101)µλ
can then be written as

e∈E λe,i(xi)(cid:1) and (cid:101)µe(xe) = exp (cid:0)−ηCe(xe) − (cid:80)

i∈e λe,i(xi)(cid:1). The cost difference

L(λ) − L(λ(cid:48)) = −

= −

= −

1
η

1
η

2
η

log

log

log

(cid:32)

(cid:88)

xi

(cid:32)

(cid:88)

xi

(cid:32)

(cid:88)

xi

(cid:33)

−

1
η

log

(cid:32)

(cid:88)

i (xi)eηδe,i(xi)
(cid:101)µλ
(cid:80)
i (cid:101)µλ
x(cid:48)
(cid:115)

i (x(cid:48)
i)

µλ

i (xi)

Se,i(xi)
µλ
i (xi)

(cid:33)

−

1
η

log

(cid:33)

e (xe)e−ηδe,i(xc)
(cid:101)µλ
(cid:80)
e (x(cid:48)
e (cid:101)µλ
e)
x(cid:48)
(cid:115)

µλ

e (xe)

(cid:33)

µλ
i (xi)
Sλ
e,i(xi)

xe
(cid:32)

(cid:88)

xe

(cid:113)

e,i(xi)µλ
Sλ

i (xi)

(cid:33)

Note that the right-hand contains the Bhattacharyya coefﬁcient BC(p, q) := (cid:80)
i
with the Hellinger distance: BC(p, q) = 1 − h2(p, q).

√

piqi which has the following relationship

The inequality then follows from exponential inequalities:

L(λ) − L(λ(cid:48)) = −

2
η

log(1 − h2(Sλ

e,i, µλ

i )) ≥ −

2
η

log exp(−h2(Sλ

e,i, µλ

i )) =

2
η

h2(Sλ

e,i, µλ
i )

Furthermore, the Hellinger inequality gives us

We conclude the result by applying this inequality with p = Sλ

e,i and q = µλ
i .

1
4

(cid:107)p − q(cid:107)2

1 ≤ 2h2(p, q).

Accelerated Message Passing for Entropy-Regularized MAP Inference

Proposition 3. The operator SMPη

i : λ (cid:55)→ λ(cid:48)

·,i(·) ∈ Rd|Ni| is, for all e ∈ Ni and xi ∈ χ, satisﬁed by

λ(cid:48)

e,i(xi) = λe,i +

1
η

log Sλ

e,i(xi)

−

1
η(|Ni| + 1)

log (cid:0)µλ

i (xi) (cid:81)

e(cid:48)∈Ni

e(cid:48),i(xi)(cid:1) ,
Sλ

Proof. The optimality conditions require, for all e ∈ Ni,

e,i(xi) = µλ(cid:48)
Sλ(cid:48)

i (xi),

which implies that

e,i(xi) = µλ
Sλ

i (xi) exp

ηδe,i(xi) + η

(cid:32)

(cid:33)

δe(cid:48),i(xi)

,

(cid:88)

e(cid:48)∈Ni

where δe,i(xi) = λ(cid:48)

e,i(xi) − λe,i(xi). Then, let e1, e2 ∈ Ni. At optimality, it holds that

Sλ
e1,i(xi)
Sλ
e2,i(xi)

=

exp(ηδe1,i(xi))
exp(ηδe2,i(xi))

Substituting each δe2,i(xi) in terms of δe1,i(xi), we then have

e,i(xi) = µλ
Sλ

i (xi) exp(ηδe,i(xi))

(cid:89)

e(cid:48)∈Ni

Sλ
e(cid:48),i(xi)
Sλ
e,i(xi)

exp(ηδe,i(xi)).

Collecting and then rearranging the above results in

exp((|Ni| + 1)ηδe,i(xi)) = (Sλ

e,i(xi))|Ni|+1

µλ

i (xi)

(cid:32)

(cid:33)−1

Sλ

e(cid:48),i(xi)

.

(cid:89)

e(cid:48)∈Ni

In additive form, the update equation is

δe,i(xi) =

1
η

log Sλ

e,i(xi) −

1
η(|Ni| + 1)

(cid:32)

log

µλ

i (xi)

(cid:33)

Sλ

e(cid:48),i(xi)

.

(cid:89)

e(cid:48)∈Ni

C. Omitted Proofs of Technical Lemmas of Section 4

C.1. Proof of Random Estimate Sequences Lemma 3

Lemma 3. The sequence {φk, δk}K
φk(λ) = ωk + γk

2 (cid:107)λ − v(k)(cid:107) for all k where

k=0 deﬁned in (5) is a random estimate sequence. Furthermore, it maintains the form

γk+1 = (1 − θk)γk
(cid:40)

v(k+1)
e,i =

v(k)
e,i + qθk
γk+1
v(k)
e,i

νy(k)
e,i

if (e, i) = (ek, ik)
otherwise

ωk+1 = (1 − θk)ωk + θkL(y(k)) −

(θkq)2
2γk+1

(cid:107)νy(k)
ek,ik

(cid:107)2
2

− θkq(cid:104)νy(k)
ek,ik

, v(k)
ek,ik

− y(k)
ek,ik

(cid:105)

Accelerated Message Passing for Entropy-Regularized MAP Inference

Proof. First we show that it is an estimate sequence by induction. Clearly this holds for the base case φ0 when δ0 = 1.
Then, we assume the inductive hypothesis that E[φk(λ)] ≤ (1 − δk)L(λ) + δkφ0(λ). From, here we can show

E[φk+1(λ)] = (1 − θk)E[φk(λ)] + θkE

= (1 − θk)E[φk(λ)] + θkE

(cid:104)

(cid:104)

L(y(k)) − (cid:104)qνy(k)
ek,ik

, λek,ik − y(k)
ek,ik
(cid:105)
L(y(k)) + (cid:104)∇L(y(k)), λ − y(k)(cid:105)

(cid:105)

(cid:105)

≤ (1 − θk)((1 − δk)L(λ) + δkφ0(λ)) + θkL(λ)
= (1 − δk+1)L(λ) + δk+1φ0(λ)

The ﬁrst line uses the deﬁnition of φk+1 and the second line uses the law of total expectation and the fact that (ek, ik) is
sampled uniformly. The inequality leverages the inductive hypothesis and convexity of L. From Nesterov (2018, §2), we
know that the deﬁnition of δk from θk ensures that δk

k→ 0. Therefore, {φk, δk}K

k=0 is a random estimate sequence.

As noted, the identities are fairly standard (Nesterov, 2018; Lee & Sidford, 2013). We prove each claim in order.

• From deﬁnition of φk+1, computing the second derivative of the combination shows that it is constant at (1 − θk)γk.

• Computing the gradient with respect to block-coordinate λek,ik of the combination shows, at optimality, we have

which implies

(1 − θk)γk(λek,ik − v(k)
ek,ik

) − qθkνy(k)
ek,ik

= 0

λek,ik = v(k)
ek,ik

+

qθk
γk+1

νy(k)
ek,ik

For any other block-coordinate (e, i), the optimality condition simply implies λe,i = v(k)
e,i .

• The last claim can be show by inserting the minimizer, v(k)
ek,ik

, into φk+1. Therefore, we have

ωk+1 := min

λ

φk+1

= φk+1(v(k+1))

= (1 − θk)ωk +

= (1 − θk)ωk +

γk+1
2
(qθk)2
2γk+1

(cid:107)v(k) − v(k+1)(cid:107)2

2 + θkL(y(k)) − θkq(cid:104)νy(k)

ek,ik

, v(k+1)
ek,ik

− y(k)
ek,ik

(cid:105)

(cid:107)νy(k)
ek,ik

2 + θkL(y(k)) − θkq(cid:104)νy(k)
(cid:107)2

ek,ik

, v(k)
ek,ik

+

qθk
γk+1

νy(k)
ek,ik

− y(k)
ek,ik

(cid:105)

= (1 − θk)ωk + θkL(y(k)) −

(qθk)2
2γk+1

(cid:107)νy(k)
ek,ik

2 − qθk(cid:104)νy(k)
(cid:107)2

ek,ik

, v(k)
ek,ik

− y(k)
ek,ik

(cid:105)

C.2. Proof of L2 Projection Lemma 5 and Lλ

2 Projection Lemma 6

Lemma 5. For λ ∈ RrD and µλ ∈ Lνλ

2 , Algorithm 4 returns a point (cid:98)µ = Proj(µλ, 0) such that (cid:98)µi = µλ

i for all i ∈ V and

(cid:88)

e∈E

(cid:107)µλ

e − (cid:98)µe(cid:107)1 ≤ 2

(cid:88)

(cid:107)νλ

e,i(cid:107)1.

e∈E,i∈e

Proof. Since ν = 0, we know that µλ
Algorithm 2 of Altschuler et al. (2017) to generate (cid:98)µe from µλ
Lemma 7):

i + νe,i = µλ

i ∈ ∆n for all i ∈ V and e ∈ Ni. For any (i, j) = e ∈ E, Proj applies
e with the following guarantee due to Altschuler et al. (2017,

(cid:107)(cid:98)µe − µλ

e (cid:107)1 ≤ 2(cid:107)Sλ

e,i − µλ

i (cid:107)1 + 2(cid:107)Sλ

e,j − µλ

j (cid:107)1

and (cid:98)µe ∈ Ud(µλ

i , µλ

j ). Applying this guarantee for all edges in E gives the result.

Accelerated Message Passing for Entropy-Regularized MAP Inference

Lemma 6. Let µ ∈ L2 and λ ∈ RrD . Deﬁne δ = maxe∈E,i∈e (cid:107)νλ

e,i(cid:107)1 There exists (cid:98)µ in the slack polytope Lνλ

2

such that

(cid:107)µ − (cid:98)µ(cid:107)1 ≤ 16(m + n)dδ + 2

(cid:88)

(cid:107)νλ

e,i(cid:107)1

e∈E,i∈e

Proof. For convenience, we just write ν for the slack, dropping the notational dependence on λ. We will proceed with this
proof by constructing such a (cid:98)µ in two cases. We would like to show that the edge marginals µe can be modiﬁed to give
(cid:98)µ ∈ Lν
2. To do this, we aim to use Algorithm 4 to match (cid:98)µe to the modiﬁed marginals µi + νe,i for every e ∈ E and i ∈ e.
As long as µi + νe,i ∈ ∆d, setting µ(cid:48)
2 that satisﬁes
the condition by Lemma 5.

e = µe and computing (cid:98)µ = Proj((cid:101)µ, ν) would return (cid:98)µ ∈ Lν

i = µi and µ(cid:48)

However, if µi + νe,i (cid:54)∈ ∆d, then ∃ x ∈ χ such that µi(x) + νe,i(x) (cid:54)∈ [0, 1]. Consider the case where δ ≤ 1
create a temporary marginal vector µ(cid:48) which is made by modifying µi appropriately until the slack can be added to µ(cid:48)
maintaining a valid distribution. To do this, we set µ(cid:48)

i as the convex combination with the uniform distribution

2d . We aim to
i while

µ(cid:48)

i = (1 − θ)µi + θ Unif(χ)

for some θ ∈ [0, 1]. Choosing θ = dδ ensures that

δ ≤ µ(cid:48)(x) ≤ 1 − δ ∀ x ∈ χ,

Furthermore, µ(cid:48)

i ∈ ∆d because ∆d is convex and we have

(cid:107)µ(cid:48)

i − µi(cid:107)1 =

≤

(cid:88)

x∈χ
(cid:88)

x

δ|1 − dµi(x)|

δ + δdµi(x)

= 2dδ

Then we set µ(cid:48)
have that

e = µe for all e ∈ E. Using Algorithm 4, we compute (cid:98)µ = Proj(µ(cid:48), ν) ∈ Lν

2. Together with Lemma 5, we

(cid:107)(cid:98)µ − µ(cid:107)1 =

(cid:88)

i∈V

(cid:107)(cid:98)µi − µi(cid:107)1 +

(cid:88)

(cid:107)(cid:98)µe − µe(cid:107)1

≤ 2ndδ + 2

(cid:88)

e∈E,i∈e

e∈E
(cid:107)νe,i(cid:107)1 + (cid:107)µ(cid:48)

i − µi(cid:107)1

≤ (n + 8m)dδ + 2

(cid:88)

(cid:107)νe,i(cid:107)1

e∈E,i∈e

On the other hand, consider the case where δ > 1
all i ∈ V and µ(cid:48)
which ensures

e = µe for all e ∈ E, which ensures that µ(cid:48)

2d . Then instead we choose the temporary marginal vector as µ(cid:48)

i for
i +νe,i ∈ ∆d by deﬁnition of ν. We then compute (cid:98)µ = Proj(µ(cid:48), ν),

i = µλ

(cid:107)(cid:98)µ − µ(cid:107)1 ≤

(cid:88)

i∈V

(cid:107)µi − µλ

i (cid:107)1 + 2

(cid:88)

(cid:107)µi − µλ

i (cid:107)1 + (cid:107)νe,i(cid:107)1

≤ 2n + 8m + 2

(cid:88)

e∈E,i∈e

(cid:107)νe,i(cid:107)1

e∈E,i∈e

≤ 4ndδ + 16mdδ + 2

(cid:88)

(cid:107)νe,i(cid:107)1

e∈E,i∈e

where the second inequality uses the fact that the l1 distance is bounded by 2 and the last inequality uses the assumption that
δ > 1

2d . We take the worst of these two cases for the ﬁnal result.

Accelerated Message Passing for Entropy-Regularized MAP Inference

C.3. Proof of Proposition 4
Proposition 4. Let µ∗ ∈ L2 be optimal, λ ∈ RrD , (cid:98)µ = Proj(µλ, 0) ∈ L2, and δ = maxe∈E,i∈e (cid:107)νλ
inequality holds:

e,i(cid:107)1. The following

(cid:104)C, (cid:98)µ − µ∗(cid:105) ≤ 16(m + n)d(cid:107)C(cid:107)∞δ
(cid:88)

+ 4(cid:107)C(cid:107)∞

(cid:107)νλ

e,i(cid:107)1 +

e∈E,i∈e

n log d + 2m log d
η

.

Proof. Consider µλ, which may not lie in L2. It does, however, lie within its own slack polytope Lνλ
2
Therefore, it can be seen that µλ is a solution to

from Deﬁnition 1.

minimize

(cid:104)C, µ(cid:105) −

1
η

H(µ)

s.t.

µ ∈ Lλ
2

Then, consider the point (cid:98)µ = Proj(µλ, 0) ∈ L2. Let µ∗ ∈ arg minµ∈L2(cid:104)C, µ(cid:105). We have

(cid:104)C, (cid:98)µ − µ∗(cid:105) = (cid:104)C, (cid:98)µ − µλ + µλ − µ∗(cid:105)

≤ (cid:107)C(cid:107)∞(cid:107)(cid:98)µ − µλ(cid:107)1 + (cid:104)C, µλ − µ∗(cid:105).

Note that the last term in the right-hand side can be written as

(cid:104)C, µλ − µ∗(cid:105) = (cid:104)C, µλ − (cid:98)µ∗ + (cid:98)µ∗ − µ∗(cid:105)

≤ (cid:107)C(cid:107)∞(cid:107)(cid:98)µ∗ − µ∗(cid:107)1 + (cid:104)C, µλ − (cid:98)µ∗(cid:105),

(10)

(11)

(12)

where (cid:98)µ∗ ∈ Lνλ
2
we further have

is the existing vector from Lemma 6 using µ∗ ∈ L2 and slack from λ. Because µλ is the solution to (10),

(cid:104)C, µλ − (cid:98)µ∗(cid:105) ≤

≤

1
(H(µλ) − H((cid:98)µ∗))
η
n log d + 2m log d
η

(13)

Combining inequalities (11), (12), and (13) shows that

(cid:104)C, (cid:98)µ − µ∗(cid:105) ≤ (cid:107)C(cid:107)∞((cid:107)(cid:98)µ − µλ(cid:107)1 + (cid:107)(cid:98)µ∗ − µ∗(cid:107)1) +

n log d + 2m log d
η

Using Lemma 5 and 6, we can further bound this as



(cid:104)C, (cid:98)µ − µ∗(cid:105) ≤ (cid:107)C(cid:107)∞

16(m + n)dδ +



4(cid:107)νλ

e,i(cid:107)1

 +

(cid:88)

e,i

n log d + 2m log d
η

.

C.4. Proof of G(η) Upper Bound

In the proof of Lemma 4, we used the fact that the numerator of the ﬁnal convergence rate can be bounded by G(η)2. Here,
we formally state this result and prove it.

Lemma 7. It holds that

where G(η) := 24md(m + n)(

√

4L(0) − 4L(λ∗) + 16m2η(cid:107)λ∗(cid:107)2

2 ≤ G(η)2,

η(cid:107)C(cid:107)∞ + log d

√

η ).

Accelerated Message Passing for Entropy-Regularized MAP Inference

The proof requires bounding both L(0) − L(λ∗), which we have already done in Lemma 10, and bounding the norm (cid:107)λ∗(cid:107)2
2.
We rely on the following result from Meshi et al. (2012).

Lemma 8. There exists λ∗ ∈ Λ∗ such that

(cid:107)λ∗(cid:107)1 ≤ 2d(n + m)(cid:107)C(cid:107)∞ +

4d(m + n)
η

log d

≤

4d(m + n)
η

(η(cid:107)C(cid:107)∞ + log d)

Proof. Modifying Meshi et al. (2012, Supplement Lemma 1.2) for our deﬁnition of H gives us

(cid:107)λ∗(cid:107)1 ≤ 2d(L(0) − n − m − (cid:104)C, µλ∗

(cid:105) +

H(µλ∗

)).

1
η

Using Cauchy-Schwarz and maximizing over the entropy yields the result.

Proof of Lemma 7. Using these results, we can prove the claim. We bound the square root of the numerator, multiplied by
√

η:

(cid:113)

η (4L(0) − 4L(λ∗) + 16m2η(cid:107)λ∗(cid:107)2

2) ≤

(cid:113)

8(m + n)(η(cid:107)C(cid:107)∞ + log d) + 16m2 (4d(m + n) (η(cid:107)C(cid:107)∞ + log d))2

≤ (cid:112)8(m + n)(η(cid:107)C(cid:107)∞ + log d) + 16md(m + n)(η(cid:107)C(cid:107)∞ + log d)
≤ 24md(m + n)(η(cid:107)C(cid:107)∞ + log d)

The ﬁrst inequality used Lemma 10 and Lemma 8. The second inequality uses the fact that
The last inequality uses the fact that the ﬁrst term is greater than 1 under the assumption d ≥ 2. Dividing through by
gives the result.

b for a, b ≥ 0.
η

a + b ≤

a +

√

√

√

√

D. Proof of Theorem 1

We begin with a complete proof of Theorem 1 for EMP and then show how to modify it slightly for SMP. The result also
builds on some of the same technical lemmas used in the proof of Theorem 2.

D.1. Edge Message Passing

The cornerstone of the proof is showing that the expected slack norms can be bounded over iterations.

Lemma 9. Let L∗ = minλ L(λ) and let (cid:98)λ be the output of Algorithm 1 after K iterations with EMP and a uniform
distribution. For any e ∈ E and i ∈ e, the expected norm of the constraint violation in L2 is bounded as

E (cid:88)

e∈E,i∈e

(cid:107)S (cid:98)λ

e,i − µ(cid:98)λ

i (cid:107)2

1 ≤

8mη(L(0) − L∗)
K

Proof. From Lemma 1, we have that the expected improvement is lower bounded at each iteration

(cid:105)
(cid:104)
L(λ(k)) − L(λ(k+1))

E

≥

E

(cid:104)
(cid:107)∇ek,ik L(λ(k))(cid:107)2
1

(cid:105)

,

1
4η

Accelerated Message Passing for Entropy-Regularized MAP Inference

Then, using that ∇e,iL(λ) = µλ

i − Sλ

e,i, we apply the bound k = 1, 2, . . . , K:

L(0) − L∗ ≥

1
4η

K−1
(cid:88)

k=0

E

(cid:104)
(cid:107)Sλ(k)
ek,ik

− µλ(k)
ik

(cid:107)2
1

(cid:105)

K−1
(cid:88)

k=0

=

≥

1
8mη

K
8mη

e∈E,i∈e

(cid:88)

E

(cid:104)
(cid:107)Sλ(k)

e,i − µλ(k)

i

(cid:105)

(cid:107)2
1

e∈E,i∈e
(cid:104)
(cid:107)S (cid:98)λ

E

(cid:88)

e,i − µ(cid:98)λ

i (cid:107)2
1

(cid:105)

The equality uses the law of total expectation, conditioning on λ(k). The second inequality uses the fact that (cid:98)λ is chosen to
minimize the sum of squared constraint violations.

Next, we provide a bound on the initial function value gap.
Lemma 10. For λ∗ ∈ Λ∗ it holds that L(0) − L(λ∗) ≤ 2(m + n)(cid:107)C(cid:107)∞ + 2

η (m + n) log d.

Proof. We will bound both L(0) and L(λ∗) individually. First, from the deﬁnition

L(0) =

≤

1
η

1
η

(cid:88)

i
(cid:88)

i

log

log

(cid:88)

xi
(cid:88)

xi

exp(−ηCi(xi)) +

exp(η(cid:107)C(cid:107)∞) +

1
η

= (n + m)(cid:107)C(cid:107)∞ +

n
η

log d +

2m
η

1
η

e
(cid:88)

log

e

log d.

(cid:88)

log

(cid:88)

exp(−ηCe(xe))

xe

(cid:88)

xe

exp(η(cid:107)C(cid:107)∞)

For L(λ∗), we recognize that L is simply the negative of the primal problem (Reg-P), shifted by a constant amount. In
particular, we have

−L(λ∗) −

1
η

(n + m) = (cid:104)C, µ∗(cid:105) −

1
η

H(µ∗)

For some µ∗ that solves (Reg-P). Note that H is offset with a linear term (different from the usual deﬁnition of the entropy)
that exactly cancels the − 1
η (n + m) on the left-hand side. We then conclude −L(λ∗) ≤ (m + n)(cid:107)C(cid:107)∞ by Cauchy-Schwarz.
Summing these two gives the desired result.

Proof of Theorem 1 for EMP. Fix (cid:15)(cid:48) > 0. Lemma 9 and Lemma 10 ensure that

after

E (cid:88)

e∈E,i∈e

(cid:107)ν (cid:98)λ

e,i(cid:107)2

1 ≤ ((cid:15)(cid:48))2

K =

16m(m + n)(η(cid:107)C(cid:107)∞ + log d)
((cid:15)(cid:48))2

(14)

iterations. By Jensen’s inequality and recognizing that the norms are non-negative, this also implies that

after the same number of iterations.

E[(cid:107)ν (cid:98)λ

e,i(cid:107)1] ≤ (cid:15)(cid:48) ∀e ∈ E, i ∈ e

Accelerated Message Passing for Entropy-Regularized MAP Inference

Now, we use the upper bound due to the approximation from Proposition 4 and take the expectation, giving

E [(cid:104)C, (cid:98)µ − µ∗(cid:105)] ≤ (cid:107)C(cid:107)∞ (8m(cid:15)(cid:48) + 16(m + n)dE[δ])
n log d + 2m log d
η

+

,

where

Therefore, the bound becomes

E [δ]2 ≤ E[δ2] ≤ E (cid:88)

e∈E,i∈e

(cid:107)ν (cid:98)λ

e,i(cid:107)2

1 ≤ ((cid:15)(cid:48))2.

E [(cid:104)C, (cid:98)µ − µ∗(cid:105)] ≤ 24(m + n)d(cid:107)C(cid:107)∞(cid:15)(cid:48)
n log d + 2m log d
+
η

We conclude the result from substituting into (14), using the deﬁnition of η and choosing (cid:15)(cid:48) =

(cid:15)
48(m+n)d(cid:107)C(cid:107)∞

D.2. Star Message Passing

The signiﬁcant difference between the SMP proof and the EMP proof is that there is variable improvement at each update,
dependent on the degree of the node being updated. Using the distribution from (3), we ensure that the improvement
becomes uniform in expectation. This analysis is similar to weighting coordinates by their coordinate-wise smoothness
coefﬁcients in coordinate gradient algorithms (Nesterov, 2012).

A slight modiﬁcation of the proof of (Meshi et al., 2012) is required the get the tighter l1-norm lower bound.
Lemma 2. Let λ(cid:48) be the result of applying SMPη

i to λ, keeping all other coordinates ﬁxed. Then, L(λ) − L(λ(cid:48)) ≥

(cid:80)

1
8|Ni|η

e∈Ni

(cid:107)νλ

e,i(cid:107)2
1.

Proof. Meshi et al. (2012) show that

L(λ) − L(λ(cid:48)) = −



log

(cid:88)



(cid:32)

µλ
i

(cid:89)

xi

e∈Ni

1
η

(cid:33) 1

|Ni|+1



|Ni|+1

Sλ

e,i(xi)



,

and further

|Ni| − |Ni|



(cid:88)



xi

(cid:32)

µλ
i

(cid:89)

e∈Ni

(cid:33) 1

|Ni|+1



|Ni|+1

Sλ

e,i(xi)





1 −

(cid:32)

(cid:88)

(cid:113)

xi

(cid:88)

≥

e∈Ni

i (xi)Sλ
µλ

e,i(xi)

(cid:33)2


We recognize the inner term of the square as the Bhattacharyya coefﬁcient which satisﬁes BC ∈ [0, 1]. Therefore,



1 −

(cid:32)

(cid:88)

(cid:113)

xi

(cid:88)

e∈Ni

(cid:33)2

µλ
i (xi)Sλ

e,i(xi)

 ≥

=

(cid:88)

e∈Ni
(cid:88)

e∈Ni

(cid:32)

1 −

(cid:32)

(cid:88)

(cid:113)

(cid:33)(cid:33)

µλ
i (xi)Sλ

e,i(xi)

xi
i , Sλ

e,i)

h2(µλ

Then,



(cid:88)



xi

(cid:32)

µλ
i

(cid:89)

e∈Ni

(cid:33) 1

|Ni|+1



|Ni|+1

Sλ

e,i(xi)



≤ 1 −

1
Ni

(cid:88)

e∈Ni

h2(µλ

i , Sλ

e,i)

Accelerated Message Passing for Entropy-Regularized MAP Inference

Finally, we lower bound the original difference of values

L(λ) − L(λ(cid:48)) ≥ −

(cid:32)

log

1 −

1
η

1
Ni

(cid:88)

e∈Ni

(cid:33)

h2(µλ

i , Sλ

e,i)

(cid:88)

h2(µλ

i , Sλ

e,i)

≥

≥

1
Niη

1
8Niη

e∈Ni
(cid:88)

e∈Ni

(cid:107)Sλ

e,i − µλ

i (cid:107)2
1

Lemma 11. Let L∗ = minλ L(λ) and let (cid:98)λ be the output of Algorithm 1 after K iterations with SMP and distribution (3).
Deﬁne N = (cid:80)

j∈V |Nj|. For any e ∈ E and i ∈ e, the expected norm of the constraint violation in L2 is bounded as

E (cid:88)

e∈E,i∈e

(cid:107)S (cid:98)λ

e,i − µ(cid:98)λ

i (cid:107)2

1 ≤

8N η(L(0) − L∗)
K

Proof. Lemma 2 gave us the following lower bound on the improvement:

(cid:104)

E

L(λ(k)) − L(λ(k+1))

(cid:105)

≥ E





1
8|Nik |η

(cid:88)

e∈Nik



(cid:107)νλ(k)
e,ik

(cid:107)2
1



Then, since ik is chosen with probability pi = |Ni|

N , we can apply the bound for k = 1, 2, . . . , K and expand the expectations:




L(0) − L∗ ≥

E



1
8|Nik |η

(cid:107)νλ(k)
e,ik

(cid:107)2
1



(cid:88)

e∈Nik

K−1
(cid:88)

k=0

=

1
8N η

≥

1
8N η

K−1
(cid:88)

E (cid:88)

k=0

e∈E,i∈e

(cid:107)νλ(k)

e,i (cid:107)2
1

K−1
(cid:88)

(cid:88)

k=0

e∈E,i∈e

E

(cid:104)
(cid:107)ν (cid:98)λ

e,i(cid:107)2
1

(cid:105)

The equality uses the law of total expectation, conditioning on λ(k). The second inequality uses the fact that (cid:98)λ is chosen to
minimize the sum of squared constraint violations.

The rest of the proof for SMP proceeds in an identical manner to the case for EMP; however, we simply replace the 8mη
with 8N η everywhere. This stems from the fact that we can now guarantee

E (cid:88)

e∈E,i∈e

(cid:107)S (cid:98)λ

e,i − µ(cid:98)λ

i (cid:107)2

1 ≤ ((cid:15)(cid:48))2

in 8N η(L(0)−L∗)
((cid:15)(cid:48))2
choices of (cid:15)(cid:48) and η as in EMP to get the result.

iterations instead. We can then use the same upper bound from Lemma 10 and substitute in the same

E. Proof of Theorem 2 for SMP

The proof for SMP essentially follows the same structure, but it requires deﬁning the estimate sequence in slightly different
way. Deﬁne the probability distribution {pi}i∈V over V with pi =

|Ni|
j∈V |Nj | . We propose the candidate:

(cid:80)

ik ∼ Cat(V, {pi}i∈V )

δk+1 = (1 − θk)δk

φk+1(λ) = (1 − θk)φk(λ) + θkL(y(k)) −

θk
pik

(cid:104)νy(k)
·,ik

, λ·,ik − y(k)
·,ik

(cid:105)

(15)

Accelerated Message Passing for Entropy-Regularized MAP Inference

Then, we show that this is indeed an estimate sequence with a conducive structure.
Lemma 12. The sequence {φk, δk}K
φk(λ) = ωk + γk

2 (cid:107)λ − v(k)(cid:107) for all k where

k=0 deﬁned in (15) is a random estimate sequence. Furthermore, it maintains the form

γk+1 = (1 − θk)γk
(cid:40)

v(k+1)
·,i

=

v(k)
·,i + θk
v(k)
·,i

piγk+1

νy(k)
·,i

ωk+1 = (1 − θk)ωk + θkL(y(k)) −

if i = ik
otherwise
θ2
k
2γk+1p2
ik

(cid:107)νy(k)
·,ik

(cid:107)2
2 −

θk
pik

(cid:104)νy(k)
·,ik

, v(k)
·,ik

− y(k)
·,ik

(cid:105)

Proof. To show that this is an estimate sequence, the proof is essentially identical to the EMP case. The only exception is
that we take expectation over V with distribution {pi}i∈V . However, this ensures that

E[

θk
pik

(cid:104)νy(k)
·,ik

, λ·,ik − y(k)
·,ik

(cid:105)] = θkE[(cid:104)∇L(y(k)), λ − y(k)(cid:105)]

by the law of total expectation. So the the proof that this is an estimate sequence remains the same.

To show that it retains the desired quadratic structure, we again analyze all terms of interest

• γk+1 is identical to the EMP case so the result holds.

• Taking the gradient with respect to λ·,i, we have that the optimality conditions, for i = ik, are

and, for all other i, they are

γk+1(λ·,ik − v(k)
·,ik

) −

θk
pik

νy(k)
·,ik

= 0

γk+1(λ·,ik − v(k)
·,ik

) = 0.

These conditions imply the given construction for v(k+1).

• We can then compute ωk+1 by plugging in the choice for v(k+1) again:

ωk+1 := min

λ

φk+1

= φk+1(v(k+1))

= (1 − θk)ωk +

γk+1
2

(cid:107)v(k) − v(k+1)(cid:107)2

2 + θkL(y(k)) −

(cid:104)νy(k)
·,ik

, v(k+1)
·,ik

− y(k)
·,ik

(cid:105)

= (1 − θk)ωk +

θ2
k
2γk+1p2
ik

(cid:107)νy(k)
·,ik

2 + θkL(y(k)) −
(cid:107)2

= (1 − θk)ωk + θkL(y(k)) −

θ2
k
2γk+1p2
ik

(cid:107)νy(k)
·,ik

(cid:107)2
2 −

θk
pik
θk
pik

, v(k)
·,ik

+

θk
γk+1pik

νy(k)
·,ik

− y(k)
·,ik

(cid:105)

(cid:104)νy(k)
·,ik

, v(k)
·,ik

− y(k)
·,ik

(cid:105)

θk
pik
(cid:104)νy(k)
·,ik

We now provide a faster convergence guarantee on the dual objective function for SMP which depends on N = (cid:80)
Lemma 13. For the random estimate sequence in (15), let {λ(k)}K
λ(0) = 0. Then, the dual objective error in expectation can be bounded as

j∈V |Nj|.
k=0 be deﬁned as in Algorithm 3 with

k=0 and {y(k)}K

E[L(λ(k)) − L(λ∗)] ≤

GSMP(η)2
(k + 2)2 ,

where GSMP(η) := 24N d(m + n)(

√

η(cid:107)C(cid:107)∞ + log d

√

η ) and N = (cid:80)

j∈V |Nj|.

Accelerated Message Passing for Entropy-Regularized MAP Inference

Proof. As in the EMP proof, it sufﬁces to show that E[ωk+1] ≥ E[L(λ(k+1))] by induction. As before we have

E[ωk+1] ≥ (1 − θk)E[L(λ(k))] + θkE[L(y(k))] − E

(cid:20)

θ2
k
2γk+1p2
ik

(cid:107)νy(k)
·,ik

(cid:107)2
2 −

θk
pik

(cid:104)νy(k)
·,ik

(cid:21)
, v(k) − y(k)(cid:105)

+ (1 − θk)E

(cid:104)

(cid:105)
(cid:104)∇L(y(k)), λ(k) − y(k)(cid:105)

(cid:104)

+ θkE

(cid:104)∇L(y(k)), v(k) − y(k)(cid:105)

(cid:105)

(cid:107)νy(k)
·,ik

(cid:21)

(cid:107)2
2

(cid:20)
L(y(k)) −

≥ E

(cid:34)
L(y(k)) −

= E

θ2
k
2γk+1p2
ik
θ2
k
2γk+1pi

(cid:88)

i∈V

(cid:107)νy(k)

·,i (cid:107)2
2

(cid:35)

,

where the last line comes from the deﬁnition of y(k). Choosing θk such that θ2

k = γk+1 minj |Nj |

4ηN 2

results in

(cid:34)

E[ωk+1] ≥ E

L(y(k)) −

(cid:88)

(cid:35)

(cid:107)νy(k)

·,i (cid:107)2
2

1
8N η

(cid:21)

(cid:107)∇L(y(k)(cid:107)2
2

i∈V
1
8N η

(cid:20)
L(y(k)) −

= E

Recall, from the improvement in Lemma 2, we have

E[L(λ(k+1))] ≤ E[L(y(k))] − E

= E[L(y(k))] − E

(cid:20)

1
8|Nik |η
(cid:20) 1
8N η

(cid:21)

(cid:107)νy(k)
·,ik

(cid:107)2
2

(cid:21)

(cid:107)∇L(y(k))(cid:107)2
2

Therefore, by this induction, the inequality E[L(λ(k+1))] ≤ E[ωk+1] holds for all k. Furthermore, by choosing γ0 =
minj |Nj | , we ensure that θk can be updated recursively as in Algorithm 3 and the update equation for v is simpliﬁed to

4N 2η

(cid:40)

v(k+1)
·,i

=

4pik θkηN νy(k)

v(k)
·,i + minj |Nj |
v(k)
·,i

·,i

if i = ik

otherwise

.

Using the property of randomized estimate sequences derived in Section 4, we can bound the expected error in the dual
norm as

E(L(λ(k))] − L∗ ≤

=

≤

4
(k + 2)2
4
(k + 2)2
4
(k + 2)2

(cid:16)

L(0) − L∗ +

(cid:17)

(cid:107)λ(cid:107)2
2

γ0
2

(cid:18)

L(0) − L∗ +

2N 2η
minj |Nj|
(cid:0)L(0) − L∗ + 2N 2η(cid:107)λ∗(cid:107)2

(cid:1)

2

(cid:19)

(cid:107)λ∗(cid:107)2
2

The numerator can then be bounded in an identical manner to the EMP proof by replacing 4m2 with 2N 2 in Lemma 7,
instead yielding GSMP(η) = 40md(m + n)(

η ), which is only different by a constant. We then have

η(cid:107)C(cid:107)∞ + log d

√

√

E(L(λ(k))] − L∗ ≤

GSMP(η)
(k + 2)2

With these tools, we are ready to present the proof of Theorem 2 for SMP.

Accelerated Message Passing for Entropy-Regularized MAP Inference

Proof of Theorem 2 for SMP. . Let (cid:98)λ be the output from Algorithm 3 after K iterations. From Lemma 2, we can lower
bound the result in Lemma 13 with

for all i ∈ V . This further implies that

1
8η|Ni|

E

(cid:34)

(cid:88)

e∈Ni

(cid:35)

(cid:107)ν (cid:98)λ

e,i(cid:107)2
1

≤ E[L((cid:98)λ)] − L∗

≤

GSMP(η)
(K + 2)2

(cid:104)

E

1
8η|Ni|

(cid:107)ν (cid:98)λ

e,i(cid:107)2
1

(cid:105)

≤

GSMP(η)
(K + 2)2

for all e ∈ E and i ∈ e. Then, for (cid:15)(cid:48) > 0, we can ensure that

E[(cid:107)ν (cid:98)λ

e,i(cid:107)1] ≤ |Ni|(cid:15)(cid:48)
1 ≤ N ((cid:15)(cid:48))2
·,i(cid:107)2
(cid:107)ν (cid:98)λ

E (cid:88)

i∈V,e∈Ni

in K =

√

8ηG(η)
(cid:15)(cid:48)

iterations. Letting (cid:98)µ ∈ L2 be the projected version of µ(cid:98)λ,




(cid:104)C, (cid:98)µ − µ∗(cid:105) ≤ (cid:107)C(cid:107)∞

16(m + n)dδ +

4(cid:107)ν (cid:98)λ

e,i(cid:107)1

 +

(cid:88)

e,i

n log d + 2m log d
η

.

Taking the expectation of both sides gives us

E[(cid:104)C, (cid:98)µ − µ∗(cid:105)] ≤ (cid:107)C(cid:107)∞ (16(m + n)dE[δ] + 4N (cid:15)(cid:48)) +

n log d + 2m log d
η

,

where

Then we can conclude

E [δ]2 ≤ E[δ2] ≤ E (cid:88)

e∈E,i∈e

(cid:107)ν (cid:98)λ

e,i(cid:107)2

1 ≤ N ((cid:15)(cid:48))2.

E[(cid:104)C, (cid:98)µ − µ∗(cid:105)] ≤ 16

≤ 24

√

√

N (m + n)d(cid:107)C(cid:107)∞(cid:15)(cid:48) + 4N (cid:107)C(cid:107)∞(cid:15)(cid:48) +

n log d + 2m log d
η

N (m + n)d(cid:107)C(cid:107)∞(cid:15)(cid:48) +

n log d + 2m log d
η

.

The last inequality uses the fact that N = 2m. Therefore, (cid:98)µ is expected (cid:15)-optimal with η as deﬁned in the statement and
(cid:15)(cid:48) =

. Substituting these values into K and G(η) yields the result.

√

(cid:15)

48

N (m+n)d(cid:107)C(cid:107)∞

F. Rounding to Integral Solutions Proofs

In this section, we prove the bound on the number of iterations sufﬁcient to recover the MAP solution using Accel-EMP
and rounding the output of the algorithm. We then compare with standard EMP.

F.1. Approximation Error

2 be the set of optimal vertices with respect to C. Denote by ∆ =
(cid:104)C, V1 − V2(cid:105) the suboptimality gap. Let R1 = maxµ∈L2 (cid:107)µ(cid:107)1, and RH = maxµ,µ(cid:48)∈L2 H(µ) − H(µ(cid:48)).

2 ,V2∈V ∗
2

Let V2 be the set of vertices of L2 and V ∗
minV1∈V2\V ∗
Deﬁne deg to be the maximum degree of the graph. The following holds:
Theorem 4 (Theorem 1 of (Lee et al., 2020)). If L2 is tight, |V ∗
1
8 and therefore the rounded solution round(µ∗

η) is a MAP assignment.

2 | = 1 and η ≥ 2R1 log(64R1)+2R1+2RH

∆

, then (cid:107)µ∗

η − µ∗(cid:107)1 ≤

Accelerated Message Passing for Entropy-Regularized MAP Inference

F.2. Estimation Error for Accelerated Message Passing

To bound the estimation error, we invoke the accelerated convergence guarantees presented in the previous section. In
particular, we showed that

(cid:104)

E

(cid:107)ν (cid:98)λ

e,i(cid:107)1

(cid:105)

≤ (cid:15)(cid:48) ∀e ∈ E, i ∈ e

√

after K =
for all e ∈ E and i ∈ e. From Theorem 3 of Lee et al. (2020), we require

iterations for Accel-EMP. Markov’s inequality implies that with probability 1 − δ, (cid:107)ν (cid:98)λ

4ηG(η)
(cid:15)(cid:48)

e,i(cid:107)1 ≤ 2m(cid:15)(cid:48)

δ

:= (cid:15)

Furthermore, the theorem of the previous subsection implies we can set

(cid:15) < O (cid:0)d−2m−2 deg−2 max(1, η(cid:107)C(cid:107)∞)−1(cid:1)

η =

16(m + n)(log(m + n) + log(d))
∆

Then, by setting

the condition is satisﬁed. Therefore, plugging into

√

4ηG(η) yields

(cid:15)(cid:48) ≤ O (cid:0)d−2m−4δ deg−2 max(1, (cid:107)C(cid:107)∞/∆)−1(log dm)−1(cid:1)

which implies, with probability 1 − δ,

(cid:112)4ηG(η) = O

(cid:18) dm3(cid:107)C(cid:107)∞ log dm
∆

(cid:19)

K = O

(cid:18) d3m7 deg2 (cid:107)C(cid:107)2

∞ log2 dm

(cid:19)

δ∆

These conditions of (cid:15)(cid:48) and η guarantee that the round(µ(cid:98)λ) is the MAP solution by invoking Theorem 3 of Lee et al. (2020).

F.3. Comparison to Standard Methods

Using standard EMP, we require the same conditions be satisﬁed on (cid:15)(cid:48) and η to guarantee recover of the MAP solution.
However, the rate of convergence differs, requiring K = L(0)−L(λ∗)

iterations, as seen previously. Note that

((cid:15)(cid:48))2

L(0) − L(λ∗) = O

(cid:18) m3(cid:107)C(cid:107)∞ log dm
∆

(cid:19)

. Note that there is no additional d dependence. It holds that with probability 1 − δ, the MAP solution is recovered by EMP
in at most

K = O

(cid:18) d4m11 deg4 (cid:107)C(cid:107)3

∞ log3 dm

(cid:19)

δ2∆

iterations. We emphasize that this iteration bound is only a sufﬁcient condition by directly applying the technique developed
in this section. We suspect it can be greatly improved.

