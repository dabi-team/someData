2
2
0
2

r
a

M
1

]
h
p
-
t
n
a
u
q
[

3
v
1
4
5
0
1
.
7
0
1
2
:
v
i
X
r
a

Qsun: an open-source platform towards practical quantum machine learning
applications

Chuong Nguyen Quoc,1, ∗ Le Bin Ho,2, 3, † Lan Nguyen Tran,2 and Hung Q. Nguyen4, ‡

1Vietnamese-German University, Ho Chi Minh City, Vietnam, 70000
2Ho Chi Minh City Institute of Physics, National Institute of Mechanics and Informatics,
Vietnam Academy of Science and Technology, Ho Chi Minh City, 700000, Vietnam
3Research Institute of Electrical Communication, Tohoku University, Sendai, Japan, 980-8577
4Nano and Energy Center, VNU University of Science,
Vietnam National University, Hanoi, Vietnam, 120401
(Dated: March 2, 2022)

Currently, quantum hardware is restrained by noises and qubit numbers. Thus, a quantum virtual
machine that simulates operations of a quantum computer on classical computers is a vital tool
for developing and testing quantum algorithms before deploying them on real quantum computers.
Various variational quantum algorithms have been proposed and tested on quantum virtual machines
to surpass the limitations of quantum hardware. Our goal is to exploit further the variational
quantum algorithms towards practical applications of quantum machine learning using state-of-the-
art quantum computers. In this paper, we ﬁrst introduce a quantum virtual machine named Qsun,
whose operation is underlined by quantum state wavefunctions. The platform provides native tools
supporting variational quantum algorithms. Especially using the parameter-shift rule, we implement
quantum diﬀerentiable programming essential for gradient-based optimization. We then report two
tests representative of quantum machine learning: quantum linear regression and quantum neural
network.

Keywords: quantum virtual machine, quantum machine
learning, quantum diﬀerentiable programming, quantum
linear regression, quantum neural network

I.

INTRODUCTION

The advent of quantum computers has opened a signiﬁ-
cant turning point for exponentially speeding up comput-
ing tasks that classical computers need thousand years to
execute [1, 2]. Although mankind has witnessed tremen-
dous development in this ﬁeld theoretically and exper-
imentally in the last few years, most state-of-the-art
quantum computers still rely on noisy intermediate-scale
quantum computers NISQ. Noises and qubit-number con-
straints prevent to build high-ﬁdelity quantum comput-
ers capable of substantially implementing quantum al-
gorithms [3–6]. To bypass these constraints, various hy-
brid quantum-classical algorithms that use classical com-
puters to optimize quantum circuits have been proposed
[7–9]. Among these, variational quantum algorithms
(VQAs) may be the most promising ones in the NISQ
era.

VQAs generally consist of three essential steps: (i) ini-
tializing quantum states for a given wavefunction ansatz,
(ii) measuring a cost function suitable for problems be-
ing considered, and (iii) minimizing the cost function and
updating new parameters. The self-consistency is per-
formed until convergence. VQAs have been extensively

∗ Electronic address: quoc.chuong1413017@gmail.com
† Electronic address: binho@riec.tohoku.ac.jp
‡ Electronic address: hungngq@hus.edu.vn

employed to tackle numerous tasks, including the vari-
ational quantum eigensolvers (VQEs) [10–16], quantum
dynamics simulation [17–22], mathematical applications
[23–31], quantum machine learning (QML) [32–40], and
new frontiers in quantum foundations [7, 41–47].

Typically, VQAs employ variational quantum circuits
to measure the cost function on a quantum computer
and outsource its optimization to a classical computer.
While one can manipulate gradient-free optimizers, such
as Nelder-Mead simplex [48], to minimize the cost func-
tion, using gradient-based methods like gradient descent
can help us speed up and guarantee the convergence of
the optimization. Several quantum algorithms have been
proposed to evaluate the cost function gradient measured
on quantum computers [49–54]. Among those methods,
quantum diﬀerentiable programming (QDP) has been in-
troduced and utilized extensively [32, 49, 50, 55–59]. It
relies on a technique called the parameter-shift rule that
evaluates the derivative of any diﬀerentiable function us-
ing quantum circuits [32, 55–59]. Therefore, this method
is beneﬁcial for developing “on-circuit” gradient-based op-
timization techniques, especially for quantum machine
learning (QML) applications where various methods like
quantum neural networks (QNNs) demand the derivative
information of the cost function.

While quantum algorithms should be performed on
quantum computers, the current limitation of NISQ com-
puters cause challenges in developing and testing new
quantum algorithms, demanding the use of virtual alter-
natives called quantum virtual machines (QVMs). Be-
sides, QVMs are necessary for modeling various noisy
channels to characterize the noises and the eﬃciency of
quantum error correction. One can classify QVMs into
two types according to the way to build them: (i) the

 
 
 
 
 
 
matrix multiplication approach [4, 56, 60–65], and (ii)
the wavefunction approach [66–72]. While the former
performs matrix multiplication for all qubits in quan-
tum circuits, the latter represents quantum circuits by
corresponding wavefunctions. On the one hand, the for-
mer can signiﬁcantly reduce the memory capacity using
tensor network contraction [73, 74], and the cost to pay
is exponentially increasing the computational time. On
the other hand, the latter, in principle, can require less
computational time in a limit of qubits number and a
suﬃciently large memory size that can store the total
quantum wavefunction. Therefore, both approaches ex-
ist side by side and a possible hybrid approach [1, 75] for
convenient purposes.

There are several QVM’s libraries developed for QML
orientation, such as TensorFlow Quantum library [76]
implemented in Cirq [61], Pennylane [56] designed for
photonics devices, and TensorNetwork [65, 77]. These
libraries are all constructed in the matrix-multiplication
type of QVMs, designed to submit quantum tasks to the
developed hardware conveniently.

In this work, we develop a QVM platform named Qsun
using the wavefunction approach towards the QML ap-
plications.
In Qsun, a quantum register is represented
by a wavefunction, and quantum gates are manipulated
directly by updating the amplitude of the wavefunction.
Measurement results rely on probabilities of the wave-
function. Our simple approach yields faster computa-
tion speed for a small number of qubit when compared
to other QVMs such as Qiskit, ProjectQ, or Pennylane.
Basing on this generic QVM, we aim to exploit the ad-
vantages of QDP with the parameter-shift rule as the
core engine towards practical applications in quantum
machine learning. Two representative examples of QML
are demonstrated: quantum linear regression and quan-
tum neural network. These algorithms are compared to
standard progams: Qiskit, ProjectQ, and Pennylane, and
classical algorithms when applicable. In these compar-
isons, Qsun performs slightly better for QDP, QLR, and
QNN. All in all, Qsun is an eﬃcient combination of QVM
with QDP features that is oriented toward machine learn-
ing problems. In the following, we introduce the QVM
platform Qsun and its performance compared to others
in Sec. II, followed by an introduction to the QDP imple-
mentation within Qsun in Sec. III. We then discuss some
QML applications of the Qsun package in Sec. IV.

II. QUANTUM VIRTUAL MACHINE
IMPLEMENTATION IN QSUN

In practice, quantum computers work on quantum al-
gorithms by composing a quantum register, or qubits,
operated by a sequence of quantum gates. Results are
then traced out from quantum measurements. We now
introduce our quantum virtual machine (QVM) named
Qsun, an open-source platform simulating the operation
of a generic quantum computer [78]. We aim the plat-

2

form to the development of quantum machine learning
(QML) and related problems. We develop it in Python
and employ the Numpy library for fast matrix operations
and numerical computations.

A. Simulating quantum computers using
wavefunction basis

Unlike widely-used approaches based on matrix multi-
plication [4, 56, 60–65], our platform is developed using
the class of “wavefunction” approach [66–72], in which
a quantum register is represented by its wavefunction.
The operation of quantum gates is simulated by updat-
ing the wavefunction’s amplitude, and output results are
obtained by measuring wavefunction’s probabilities. We
expect that working directly on wavefunction is beneﬁcial
for QML applications, especially for building and training
variational quantum circuits in quantum neural networks
(QNNs). As depicted in Fig. 1, Qsun consists of three
main modules Qwave, Qgates, and Qmeas for quantum
register, quantum gates, and quantum measurement, re-
spectively.

Qwave

In general,

for a quantum register with N qubits,
its quantum states are represented in the 2N -dimension
Hilbert space as

|ψ(cid:105) =

2N −1
(cid:88)

j=0

αj|j(cid:105),

(1)

j=0

where αj are complex amplitudes obeying a completeness
relation (cid:80)2N −1
|αj|2 = 1, and vectors |j(cid:105) are elements of
the computational basis. We integrate quantum state’s
information into the class Wavefunction as described in
Fig. 1 and Table. I. The class allows us to access and up-
date amplitudes directly according to the evolution of the
quantum state under the action of the unitary quantum
gates. It also measures probabilities that contain output
information.

Qgates

To manipulate a single-qubit gate U =

(cid:19)

(cid:18)a c
b d

act-

ing on the nth qubit, the nonzero elements in amplitude
array are updated as [70]

(cid:19)

(cid:18) αsi
αsi+2n

→ U

(cid:18) αsi
αsi+2n

(cid:19)

,

(2)

where si = ﬂoor(i/2n)2n+1 + (i mod 2n), for all i ∈
[0, 2N −1 − 1]. Here, ﬂoor(x) is the standard ﬂoor func-
tion taking the greatest integer less than or equal to the

3

Algorithm 1: Operation of a single-qubit gate:
Result: Wavefunction with new probability

amplitudes
w ← Wavefunction;
states ← w.state; ampl ← w.amplitude;
N ← size(state[i][0]); n ← target qubit;
n_ampl ← [0, ..., 0], size(n_ampl) = size(ampl);
cut ← 2N −n−1;
for i ← 0, size(ampl) do

if state[i][n] == 0 then

n_ampl[i] ← n_ampl[i] + a*ampl[i];
n_ampl[i + cut] ← n_ampl[i + cut] +

b*ampl[i];

else

n_ampl[i] ← n_ampl[i] + d*ampl[i];
n_ampl[i − cut] ← n_ampl[i − cut] +

c*ampl[i];

end

end
w.amplitude ← n_ampl

Algorithm 2: Operation of Hadamard gate:
Result: Wavefunction with new probability

amplitudes corresponding to Hadamard state

w ← Wavefunction;
states ← w.state; ampl ← w.amplitude;
N ← size(state[i][0]); n ← target qubit;
n_ampl ← [0, ..., 0], size(n_ampl) = size(ampl);
cut ← 2N −n−1;
for i ← 0, size(ampl) do

if state[i][n] == 0 then

n_ampl[i] ← n_ampl[i] + (1/
n_ampl[i + cut] ← n_ampl[i + cut] +

2)*ampl[i];

√

√

n_ampl[i] ← n_ampl[i] + (−1/
n_ampl[i − cut] ← n_ampl[i − cut] +

2)*ampl[i];

(1/

2)*ampl[i];

else

√

√

(1/

2)*ampl[i];

end
w.amplitude ← n_ampl

end

Figure 1. Qsun quantum virtual machine (QVM) consists of
three main modules Qwave, Qgates, and Qmeas, to simulate
quantum circuits with a quantum register (qubits), quantum
gates, and quantum measurements, respectively. In Qsun, a
quantum register is constructed by its wavefunction through
the Wavefunction class and its containing objects, as illus-
trated in the ﬁgure.

Table I. List of methods used in the class Wavefunction.

Methods
amplitude

state

probabilities

print_state

visual_circuit

Description
a NumPy array of complex numbers
that stores the amplitudes of quantum
states.
a NumPy array of strings that labels
the quantum states’s basis.
return a list of corresponding prob-
abilities for each basis vector in the
superposition.
return a string representing a quan-
tum state of the system in bra-ket
notations.
print a visualization of a quantum
circuit.

real of x. We unify the implementation of single- and
multiple-qubit gates into a common framework. We out-
line the operation of single-qubit gates in Algorithm 1
and an example of the Hadamard gate in Algorithm 2.
We emphasize that Qgates only update nonzero com-
ponents of wavefunction amplitudes. This way, we can
avoid demanding matrix multiplications that escalate ex-
ponentially (2N ×2N ) with the number of N qubits. This
generic algorithm allows us to implement arbitrary uni-
tary gates without decomposing them into universal ones,
which may be advantageous to model a general class of
neural networks using quantum circuits.

To mimic the actual operation of quantum computers,
we introduce noises into the wavefunctions. In Qsun, the
standard quantum depolarizing channel is implemented
as a single-qubit gate E that is a part of quantum circuits
acting on the wavefunctions. For a given noisy probabil-

ity p, applying the gate E on a mixed quantum state ρ
will transform it to [79]

ρ E−→ (1 − p)ρ +

p
2

I,

(3)

where I is an 2 × 2 identity matrix.
In general, one
can decompose the depolarizing channel into the bit-ﬂip,
phase-ﬂip, and phase-bit-ﬂip as

ρ E−→ (1 − p)ρ + pxXρX + pyY ρY + pzZρZ,

(4)

where px, py, pz are the probabilities of bit-ﬂip, phase-
bit-ﬂip, and phase-ﬂip, respectively [4, 79]. In Qsun, we
use px = py = pz = p/3 and apply to every qubits in the
circuit each after the action of a quantum gate on the
circuit.

ClassObjectsupdate++Table II. Comparison between Qsun and other simulators

4

Algorithms
Standard algorithms

(cid:51)
Quantum diferentiable programming (cid:51)
(cid:51)
(cid:51)

Quantum Linear Regression
Quantum Neural Network

(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)

Qsun ProjectQ Qiskit Pennylane

Similarly, the probability for getting the outcome |1(cid:105)
reads

p(1) = 1 − p(0),

and the post-state is

(cid:80)2N −1−1
i=0

|ψ(cid:48)(cid:105) =

|jsi+2n (cid:105)(cid:104)jsi+2n |ψ(cid:105)
(cid:112)p(1)

.

(7)

(8)

For all-qubit measurement, the post-quantum state
will collapse to one of {|j(cid:105)} with the probability of
|αj|2. In Qsun, we build these two measurements onto
measure_one and measure_all, respectively.

B. Assessing the QVM performance

√

Let us now assess the performance of Qsun and com-
pare it with three existing ones in Pennylane, Qiskit, and
ProjectQ. Note that Qsun and ProjectQ belong to the
wavefunction class, while the others are in the matrix
multiplication class. We have adopted the testing cir-
cuit from Ref. [80] composed of the Hadamard,
X, and
CNOT gates acting on each qubit. We have ﬁxed the
depth of the circuit at 10 and varied the number of qubits
N . The code for this test is shown in A.

Fig. 2 represents the change of computational time
when the number N of qubits increases. In general, there
are two magnitudes of slope corresponding to two ways
of QVM implementation. While the wavefunction-based
approach is faster than the matrix-multiplication one for
small numbers of qubits (N < 8), the opposite behavior
is observed for larger numbers of qubits. This observa-
tion reﬂects the basic properties of these two approaches
as discussed above and see also Ref.
[66]. However,
there are some available techniques to improve the per-
formance of the wavefunction approach for larger num-
bers of qubits, such as, SIMD (single-instruction, multi-
ple data) optimization and multi-threading [66]. We fur-
ther summarize a comparison between Qsun and other
simulators in terms of practical quantum algorithms in
Table II.

Figure 2. Comparing computational time t (in seconds) of
diﬀerent QVMs as increasing the number of qubits N . Inset:
X, and a CNOT gates
The testing circuit including an H, a
acts on each qubit. The depth is ﬁxed at 10, and the number
N of qubits is varied to assess the QVM performance.

√

Qmeas

The module Qmeas is designed to execute quantum
measurements on a single qubit or all qubits in the quan-
tum circuit. For a measurement on a single qubit n, the
probability for that its outcome is |0(cid:105) reads

p(0) =

2N −1−1
(cid:88)

(cid:104)ψ|jsi(cid:105)(cid:104)jsi|ψ(cid:105) =

i=0

2N −1−1
(cid:88)

i=0

|αsi|2,

(5)

with |jsi(cid:105) as the basis element and the post-quantum
state after the measurement is given as

|ψ(cid:48)(cid:105) =

(cid:80)2N −1−1
i=0

|jsi(cid:105)(cid:104)jsi|ψ(cid:105)

(cid:112)p(0)

.

(6)

III. QUANTUM DIFFERENTIABLE
PROGRAMMING IMPLEMENTATION IN QSUN

Given a quantum state |ψ((cid:126)θ)(cid:105) with (cid:126)θ as variational
parameters and an observable (cid:98)C, the task is to seek

the global minimum of the expectation value C((cid:126)θ) =

QsunPennylaneQiskitProjectQx10...(cid:104)ψ((cid:126)θ)| (cid:98)C|ψ((cid:126)θ)(cid:105) with respect to parameters (cid:126)θ. For exam-
ple, if (cid:98)C is a Hamiltonian, its global minimum is the
In general, C((cid:126)θ) is called as the
ground state energy.
cost function, and minimizing the cost function requires
its derivative with respect to parameters (cid:126)θ, ∂C((cid:126)θ)/∂θ. In
classical computing, if the analytical form of ∂C((cid:126)θ)/∂θ
is unknown, ﬁnite diﬀerence methods are often used to
evaluate the derivative approximately. Although this ap-
proximation is fast and easy to implement, its accuracy
depends on discretization steps. In contrast to the classi-
cal ﬁnite diﬀerentiation, quantum diﬀerentiable program-
ming (QDP) is an automatic and exact method to com-
pute the derivative of a function. QDP is thus essential
for accurate gradient computation in multiple VQAs, in-
cluding QML models.

Algorithm 3: Quantum diﬀerentiable
programming implementation in Qsun

Result: Derivative of a function
f ← Function; c ← Quantum Circuit;
p ← Params; s ← Shift;
diﬀ ← [0, ..., 0], size(diﬀ) = size(p);
for i ← 0, size(dif f ) do
p_plus ← copy(p);
p_subs ← copy(p);
p_plus[i] ← p[i] + s;
p_subs[i] ← p[i] − s;
diﬀ[i] ← (f (c, p_plus) −f (c, p_subs))/(2*sin(s));

end

The heart of QDP is the parameter-shift rule that is
analytically computed using quantum circuits. The al-
gorithm is outlined in Algorithm 3. Let us introduce a
parameterized generator (cid:98)V independent of (cid:126)θ such that
|ψ((cid:126)θ)(cid:105) = ei(cid:126)θ (cid:98)V |ψ(cid:105). The cost function is then rewritten as

C((cid:126)θ) = (cid:104)ψ|e−i(cid:126)θ (cid:98)V (cid:98)Cei(cid:126)θ (cid:98)V |ψ(cid:105) = Tr(cid:0)

(cid:98)CeZ [ρ](cid:1),

(9)

with ρ = |ψ(cid:105)(cid:104)ψ|; Z = i(cid:126)θ (cid:98)V , and the superoperator eZ [ρ] =
e−Z ρeZ [81]. The parameter-shift rule for each θ ∈ (cid:126)θ
states that

∂C((cid:126)θ)
∂θ

= Tr(cid:0)

(cid:98)C∂θeZ [ρ](cid:1) = c(cid:2)C((cid:126)θ + s) − C((cid:126)θ − s)(cid:3), (10)

where c = 1/(2 sin(s)), and s is determined based on
the superoperator and independent of (cid:126)θ. The values of
the cost function C at (cid:126)θ ± s are measured on quantum
computers by implementing two quantum circuits as
follows

|ψ(cid:105)

ei((cid:126)θ+s) (cid:98)V

C((cid:126)θ + s)

(cid:104) (cid:98)C(cid:105)

|ψ(cid:105)

ei((cid:126)θ−s) (cid:98)V

C((cid:126)θ − s)

5

Figure 3. Computational time t in seconds of the QDP im-
plementation in Qsun, ProjectQ using an engine Simulator()
and Pennylane using a default.qubit device when the number
of qubits increases. Inset: quantum circuits used for the QDP
test.

The derivative is ﬁnally obtained by subtracting mea-
surement results from the two circuits. An advantage
of the parameter-shift rule is that it can compute the
derivative of the given function exactly while the shift s
can be chosen arbitrarily large [82].

We combine the QDP implemented in Qsun with a
classical optimizer like adaptive moment (Adam) opti-
mization and implement quantum gradient descent, mak-
ing the optimization procedure fully quantum. Let us
now examine the performance of the QDP in comparison
with Pennylane, another QML-oriented platform. In this
test, we measure the computational time spent for opti-
mizing the ground state of an Ising model. The code for
running this test is given in B, and a tutorial explaining
how to use QDP with sample codes is written in C.

Fig 3 presents the change of computational time when
increasing the number of qubits N for three cases of
Qsun, Pennylane, and ProjectQ. As shown in the inset
of Fig. 3, for each qubit, we have two variational param-
eters (cid:126)θ, meaning that the number of variational param-
eters is twice of N . Qsun is faster than Pennylane for
N < 5, whereas it is slower for larger numbers of qubits
(N > 5). Among the three cases, ProjectQ consumes
the most computational time for all N . Of course, the
exponential growth with the number of qubits is not in-
evitable.

IV. QUANTUM MACHINE LEARNING
APPLICATIONS USING QSUN

Various quantum machine learning models can be
developed with quantum diﬀerentiable programming
(QDP) implementations to evaluate the gradient and em-
ploy gradient-based optimization. This section demon-
strates QML applications using Qsun in two well-known
models: quantum linear regression (QLR) and quantum

QsunNneural network (QNN). Its performances are compared
to other standard tools.

Before digging into detailed examples, let us derive the
QDP implementation in derivative of a standard mean
squared error cost function:

C((cid:126)θ) =

1
M

M −1
(cid:88)

(cid:16)

i=0

(cid:17)2

,

yi − (cid:98)yi

(11)

where (cid:126)θ is a tuple of variational parameters, yi represents
(cid:98)yi stands for prediction value, with
the true value and
i ∈ [0, M − 1], M the number of samples in the dataset to
be trained. Concretely, we also consider the prediction
value is a composition function of an activation function
f ((cid:126)θ), i.e.,

(cid:98)yi(f ((cid:126)θ)). Then, we derive at a chain rule:
∂C((cid:126)θ)
∂θ

∂(cid:98)yi(f )
∂f

∂f ((cid:126)θ)
∂θ

M −1
(cid:88)

1
M

=

∂C((cid:98)yi)
∂(cid:98)yi

i=0

,

where θ ∈ (cid:126)θ, with

∂C((cid:98)yi)
∂(cid:98)yi

= −2[yi − (cid:98)yi],

6

where k is the scaling factor that transforms the output
data from [−1, 1] into [−k, k]. Its value should be chosen
so that the R.H.S can cover all of the true values {yi}.
Here, w, b are functions of variational parameters (cid:126)θ, for
example w ≡ w((cid:126)θ) = (cid:104)ψ((cid:126)θ)|(cid:98)σz|ψ((cid:126)θ)(cid:105) with the initial state
|ψ(cid:105) = |0(cid:105), (see the inset Fig. 4). The cost function is the
mean squared error as described in Eq. (11).

After having the model, we run the circuit in the inset
Fig. 4 to ﬁnd the optimized values for w and b via updat-
ing (cid:126)θ. We ﬁrst calculate their derivatives using the chain
rule as shown in Eq. (12) and the parameter-shift rule in
the QDP as depicted in Algorithm 3, and then update
new parameters using gradient descent

θt+1 = θt − η

∂C((cid:126)θt)
∂θ

,

(17)

(12)

(13)

where η the learning rate, (cid:126)θt is the set of parameters at
time step t. They are continuously updated until the
optimized values are found. The quantum circuit that
used to train the model is represented in the inset Fig.
4, and the code is given in D.

For comparison, we next provide the analytical solu-
tion for minimizing the cost function (11) with (15). We
vectorize:

the derivative ∂ (cid:98)yi(f )
the activation function, and

∂f

depends on the particular form of

(cid:19)

(cid:18)w
b

= (X T X)−1X T Y

(18)

∂f ((cid:126)θ)
∂θ

=

=

(cid:88)

k
(cid:88)

k

∂mk((cid:126)θ)
∂θ
(cid:104)

(cid:105)
mk((cid:126)θ + sk) − mk((cid:126)θ − sk)

ck

where

,

(14)

X =







1
x0
...
...

 ,
xM −1 1

Y =






y0
...
yM −1




 ,

(19)

where mk((cid:126)θ) = (cid:104)ψ((cid:126)θ)| (cid:99)Mk|ψ((cid:126)θ)(cid:105) is a measurement out-
come of an operator (cid:99)Mk for a measurement set {k}. To
arrive at the second equality in Eq. (14), we have applied
the parameter-shift rule (10).

A. Quantum linear regression

We implement quantum linear regression on a “dia-
betes” dataset [83] available in the scikit-learn package
[84] with 400 samples for the training set and 10 samples
for the testing set. We write the linear regression model
in the form

(15)

(cid:98)y = wx + b,
where w and b are the slope and intercept of the linear
regression need to be obtained. To evaluate them on
quantum computers, we store their values in two qubits.
The values of w and b now become the expectation values
(cid:98)σz. The quantum version of linear
of the Pauli matrix
regression model (15) states

(cid:98)y((cid:126)θ) = k(cid:0)(cid:104) (cid:98)w(cid:105)x + (cid:104)(cid:98)b(cid:105)(cid:1),

(16)

and T the transpose operator.

The performance of Qsun is compared to those of Pen-
nylane, ProjectQ, and analytical solution using the same
set of parameters. We use the maximum number of it-
erations 1000 and k = 10. For Pennylane, we use Gra-
dientDescentOptimizer() with its default conﬁguration.
For ProjectQ, we use the same optimize algorithm as we
have used for Qsun, which is shown in D. As we can
see from Fig. 4, Qsun’s result is closer to the analyti-
cal result than the Pennylane and ProjectQ one, which
implies a high performance of Qsun.
In the same case
of the “wavefunction” approach, Qsun has a better eﬃ-
ciency than ProjectQ in the speedup as shown in Fig. 2
and Fig. 3, and in the optimization process.

B. Quantum neural network

In this subsection, we show the ability of Qsun for deep
learning by building and training a quantum neural net-
work (QNN). We model the QNN as a variational quan-
tum circuit (VQC) parameterized with multiple variables
that are optimized to train the model. Fig. 5 represents
the process for building and training our QNNs. It is a

7

(cid:101)xj ∈ [0, 1]. We map this
Here, the normalized value
value into the amplitudes of a qubit as |ψj(cid:105) = (cid:112)
(cid:101)xj|0(cid:105) +
(cid:112)1 − (cid:101)xj|1(cid:105). Then, the quantum state for N features
reads

|Ψ(cid:105) =

N −1
(cid:79)

j=0

|ψj(cid:105) =

N −1
(cid:79)

j=0

(cid:16)(cid:112)

(cid:101)xj |0(cid:105) + (cid:112)1 − (cid:101)xj |1(cid:105)

(cid:17)

.

(21)

The encoding implementation is given in Algorithms 4:

Algorithm 4: Encoding data in Qsun

Result: Probablity amplitudes of QNN’s initial

quantum states

sample ← sample_scaled;
N ← number_of_features;
ampl ← [0, ..., 0], size(ampl) = N ;
for i ← 0, N-1 do

ampl[i] ← [sqrt(sample[i]), sqrt(1-sample[i])];

end
ampl_vec ← ampl[0] ⊗ ampl[1] ⊗ · · · ⊗ ampl[N -1];

Building the QNN circuits

Figure 4. Linear regression models trained by quantum pro-
gramming using Qsun (blue), Pennylane (green), ProjectQ
(orange) and analytical solution (dashed purple). We use the
diabetes dataset [83] with 400 samples for the training set
and 10 samples for the testing set. The boundary k = 10
with 1000 iterations. Inset: the quantum circuit that trains
the quantum linear regression models.

hybrid quantum-classical scheme with ﬁve steps summa-
rized as follows:

• The quantum part:

Step 1: Quantizing and encoding dataset into quan-
tum states using the amplitude encoding method
[36, 85].

Step 2: Building a QNN circuit and measurement.

Step 3: Evaluating the derivative of measurement
results using QDP.

The QNN model uses N qubits for N features and in-
cludes multiple layers. It is parameterized through a set
of variables (cid:126)θ = {θkjj(cid:48)} with k as the layer index, j as
qubit (feature) indices, and j(cid:48) indicates the number of ro-
tation gates implementing on each qubit. Each layer has
one Rx and Ry gates for each qubit followed by CNOT
gates generating all possible entanglements between them
if the number of qubits is more than one.

• The classical part:

Step 4: Deriving the derivative of the deﬁned cost
(loss) function.

Step 5: Running a gradient-based optimization and
updating parameters.

Decoding probabilities into predictions

Here, we map measurement results from the previous
step into classical predictions by using an activation func-
tion. We consider the expectation value of the projection
operator (cid:98)Π = |1(cid:105)(cid:104)1| as

Data quantization and amplitude encoding

j ≡ (cid:104)(cid:98)Π(cid:105) = (cid:104)ψ(i)
p(i)

j

|1(cid:105)(cid:104)1|ψ(i)

j (cid:105) = |(cid:104)1|ψ(i)

j (cid:105)|2.

(22)

Set x(i)
j

as the ith sample (i ∈ [0, M − 1]) of the jth
feature (j ∈ [0, N − 1]) in the dataset of N features with
M samples for each feature. Since we only analyze one
sample (other samples are treated similarly), we can omit
the indicator i and rewrite x(i)
as xj. We now map xj into
j
a qubit by normalizing its value in range [x(min)
, x(max)
]
j
into [0, 1]. Using the Min-Max normalization, we obtain

j

Note that |ψ(i)
j (cid:105) is the ﬁnal state of qubit j at sample
i. We also emphasize that one can choose the projec-
tion operator (cid:98)Π arbitrarily. We use the sigmoid function,
S(x) = 1/(1 + e−x) to transform the measurement data
into predictions. In our concrete example below, we have
two features represented by two qubits, i.e, j = 0, 1. We
use the prediction rule as

Si(p) = S(cid:0)γ(p(i)

0 − p(i)

1 )(cid:1),

(23)

(cid:101)xj =

xj − x(min)
j

x(max)
j

− x(min)
j

.

(20)

where γ is a scaling shape for the sigmoid function such
that for large γ then S(x) becomes a Heaviside step func-

Qsun8

Figure 5. A Quantum Neural Network framework. The blue box is the quantum part of the QNN, and the green box is the
classical part of the QNN. At ﬁrst, quantify and encode the dataset (training and testing) into quantum states of qubits |ψi(cid:105).
Each feature in the dataset encodes into one qubit. The employed rotation gates will parameterize the quantum circuit, and
the CNOT gates cause entangled in the circuit. These gates repeat L times for L layers in the quantum neural network. After
that, we measure the circuit and give the corresponding probabilities pk. We employ a QDP scheme with the pentameter-shift
rule to calculate the derivative of pk and send the results to a classical computer to derive the derivative of the loss function.
After that, we implement a gradient-based optimization to obtain new parameters. When the scheme is not optimal yet, we
update the circuit with new parameters; when it is optimal, we turn it to the testing process.

(27)

(28)

(29)

tion. To use a soft prediction, we introduce a label-
conditional probability for a prediction value as

with

(cid:98)yi(l) =

(cid:26) Si(p),

for l = 0
1 − Si(p), for l = 1

.

(24)

∂C(yi)
∂(cid:98)yi(l)
∂(cid:98)yi(l)
∂p

= −2[1 − (cid:98)yi(l)],

= (−1)l

(cid:98)yi(l)[1 − (cid:98)yi(l)],

Training the QNN model

In the current version of Qsun, we are using a quantum-
classical hybrid scheme combining QDP and the classical
Adam optimization to train our QNN model. The cost
function is deﬁned by

C((cid:126)θ) =

1
M

M −1
(cid:88)

i=0

(cid:2)1 − (cid:98)yi(yi)(cid:3)2

,

(25)

which is a function of (cid:126)θ. Here, yi ∈ {0, 1} are true values,
(cid:98)yi(yi) are predicted probability conditioned on the label
yi. We then evaluate the derivative of the cost function
using QDP, followed by a gradient-descent procedure to
search for optimal parameters (cid:126)θ.

The derivative of the cost function concerning each
θkjj(cid:48) ∈ (cid:126)θ following the chain rule (12) gives (hereafter,
we omit its indices)

and

∂p((cid:126)θ)
∂θ

= γ

= γ

∂
∂θ
(cid:110)

(cid:16)

0 − p(i)
p(i)

1

(cid:17)

(cid:2)p(i)

c0

0 (θ + s0) − p(i)
1 (θ + s1) − p(i)

0 (θ − s0)(cid:3)
1 (θ − s1)(cid:3)(cid:111)
,

(cid:2)p(i)

− c1

here, we have applied the parameter-shift rule (10). We
ﬁnally update the model with new parameters using the
Adaptive Moment (Adam) optimization algorithm [86]
as follows. Let gt+1 be the gradient of the cost function
w.r.t. θ at time step t+1, (t starts from 0):

gt+1 ≡

∂C((cid:126)θt)
∂θ

,

(30)

where (cid:126)θt is the set of parameters at time step t. Next,
we estimate the ﬁrst and second order moments of the
gradient as

∂C((cid:126)θ)
∂θ

=

1
M

M −1
(cid:88)

i=0

∂C(yi)
∂(cid:98)yi(l)

∂(cid:98)yi(l)
∂p

∂p((cid:126)θ)
∂θ

,

(26)

vt+1 = β1vt + (1 − β1)gt+1
t+1,

wt+1 = β2wt + (1 − β2)g2

(31)

(32)

and compute

(cid:98)vt+1 =

vt+1
1 − βt+1

1

, and

(cid:98)wt+1 =

wt+1
1 − βt+1

2

.

(33)

Here, β1, β2 and (cid:15) are nonnegative weights, originally sug-
gested as β1 = 0.9, β2 = 0.999 and (cid:15) = 10−8. Also,
v0 = w0 = 0. Finally, the parameter θ is updated to

θt+1 = θt −

(cid:112)

η(cid:98)vt+1
(cid:98)wt+1 + (cid:15)

,

(34)

where η is the learning rate. The implementation of
Adam algorthims in a combination with QDP is given
in Algorithm 5.

Algorithm 5: Adam optimization
implementation in Qsun

Result: Updated parameters
c ← Quantum Circuit;
p ← Params; s ← Shift;
β1, β2, (cid:15), η ← Beta1, Beta2, Epsilon, Eta;
v_adam, s_adam ← v_adam, s_adam;
t ← tth iteration;
diﬀ ← zero_matrix, size(diﬀ) = size(p);
for i ← 0, size(param) do

for j ← 0, size(param[i]) do

for k ← 0, size(param[i][j]) do

p_plus ← copy(p);
p_subs ← copy(p);
p_plus[i][j][k] ← p[i][j][k] + s;
p_subs[i][j][k] ← p[i][j][k] − s;
diﬀ[i][j][k] ← QDP(p_plus, p_subs, c, s);

end

end

end
p, v_adam, s_adam ← Adam(diﬀ, p, v_adam,

s_adam, η, β1, β2, (cid:15), t) ;

9

We also compare their results with those from a classi-
cal model to explore advantages of QNN. For the classical
NN model, we use the MLPClassiﬁer function from scikit-
learn that has 100 nodes per layer with the ReLU acti-
vation function. For the QNN model, we have 4 nodes
per layer with the sigmoid activation function. We ﬁx
s0 = s1 = π/20 in the parameter-shift rule and η = 0.1.
For all optimization, we use the Adam algorithm with
β1 = 0.9, β2 = 0.99, and (cid:15) = 10−6.

Fig. 6 represents the loss function versus iteration for
diﬀerent layers as shown in the ﬁgure for three cases:
a classical NN model (a) and a QNN model trained by
Pennylane (b) and Qsun (c). In general, the loss function
will reduce and get unchanged after suﬃcient layers. In
our example, the loss function becomes saturated after
5 layers. As expected, the loss function of QNN model
is much smaller than the the classical one. Interestingly,
when comparing the two quantum approaches (see insets
(d, e) for a zoom in), the Qsun loss is converged faster
than the Pennylane one.

We next focus on our QNN model with ﬁxed 5 lay-
ers.
In Fig. (7)(a), we show the reduction of cost
functions and activation functions during the iteration
for the training and testing processes. They both be-
have similarly and become saturated after 100 iterations.
Fig. (7)(b, c) represent results for the training and test-
ing sets. In the ﬁgure, both the true value y and the pre-
(cid:98)y are labeled as ‘0’ and ‘1’ for no − purchase
dicted value
and purchase, respectively. y(cid:98)y = ‘00’ or ‘11’ indicates a
correct prediction, whereas y(cid:98)y = ‘01’ or ‘10’ indicates a
wrong prediction. It can be seen from Fig. 7 that our
QNN model has a good performance, reﬂected by large
values of main diagonal elements y(cid:98)y = ‘00’ and ‘11’. Im-
portantly, unlike the classical neural network [87], it does
not require many nodes in each layer, which is one of the
main advantages of QNN.

V. CONCLUSION

Preliminary QNN results

We apply the QNN model on the Social Network Ads
dataset [87]. This is a categorical dataset to determine
whether a user purchases a particular product or not. We
consider two features Age and EstimatedSalary to train
the model with the output P urchased. We use 400 sam-
ples and split them into 323 samples for the training set
and 80 samples for the testing set. Once the data is nor-
malized, we encode Age and EstimatedSalary into two
qubits |ψj(cid:105), j = 0, 1, while y = P urchased. We evaluate
the predicted value
(cid:98)y and minimize the loss function (25)
to train the QNN model. The full code is given in E.
For the comparison of performance, we train the QNN
model using Qsun and Pennylane. We note that while the
QDP and Adam algorithm are implemented in Qsun and
Pennylane separately, the QNN circuit, encoding, and
decoding procedures are the same for both approaches.

We have developed Qsun, an open-source platform of
quantum virtual machine, that simulates the operation
of a real quantum computer using the wavefunction ap-
proach. For small qubit numbers, the current version of
Qsun runs standard tasks signiﬁcantly faster than other
platforms do. The quantum diﬀerentiable programming
is implemented as a built-in function of Qsun, allowing us
to execute quantum machine learning applications eﬀec-
tively. We have employed Qsun to implement two stan-
dard models in machine learning:
linear regression and
neural network. For the former, Qsun yields a quantum
regression model nearly overlap with the classical refer-
ence, and somewhat better than the Pennylane one. For
the latter, the QNN model trained using Qsun shows a
good performance with a less number of nodes in each
layer than the classical neural network. Although Qsun
is aimed to quantum machine learning problems, as a
generic quantum virtual machine, it can be used for mul-

10

Figure 6. (a) Classical NN model; (b) and (c) QNN models using Pennylane and Qsun, respectively; (d) and (e) are the zoom-in
of (b) and (c), respectively. Each plot shows from 1 layer (1L) to 5 layers (5Ls). See text for more details about parameters
used for each model.

Figure 7. Results from our QNN model with ﬁve layers and two features on the Social Network Ads dataset. (a) Plot of loss
and accuracy versus iteration for the training and testing process. (b, c) The confusion matrices for the training and testing
sets. Here we use 400 samples and split them into 320 samples for training and 80 samples for testing with 400 iterations.

tiple other purposes, such as developing and testing vari-
ational quantum algorithms for electronic structures or
quantum information. It is well-ﬁt to personal use thanks
to its light weight. Qsun is under active development to
cover a wide range of contents in machine learning. Our

code is open source and available on GitHub [78].

ACKNOWLEDGMENTS

This work was supported by JSPS KAKENHI Grant

Number 20F20021.

[1] F. Arute

et al.

programmable superconducting processor.
574(7779):505–510, Oct 2019.

Quantum supremacy using a
Nature,

[2] Zhong et al. Quantum computational advantage using

photons. Science, 370(6523):1460–1463, 2020.

[3] John Preskill. Quantum Computing in the NISQ era and

beyond. Quantum, 2:79, August 2018.

[4] N. Khammassi, I. Ashraf, X. Fu, C.G. Almudever, and
K. Bertels. Qx: A high-performance quantum com-

puter simulation platform. In Design, Automation Test
in Europe Conference Exhibition (DATE), 2017, pages
464–469, 2017.

[5] Jianwei Wang, Stefano Paesani, Yunhong Ding, Raﬀaele
Santagati, Paul Skrzypczyk, Alexia Salavrakos, Jordi
Tura, Remigiusz Augusiak, Laura Mančinska, Davide
Bacco, Damien Bonneau, Joshua W. Silverstone, Qi-
huang Gong, Antonio Acín, Karsten Rottwitt, Leif K.
Oxenløwe, Jeremy L. O’Brien, Anthony Laing, and

1L2Ls3Ls4Ls5LsClassicalPennylaneQsun(b) Training set(c) Testing set(a)Mark G. Thompson. Multidimensional quantum en-
tanglement with large-scale integrated optics. Science,
360(6386):285–291, 2018.

[6] Abhinav Kandala, Kristan Temme, Antonio D. Córcoles,
Antonio Mezzacapo, Jerry M. Chow, and Jay M. Gam-
betta. Error mitigation extends the computational reach
of a noisy quantum processor. Nature, 567(7749):491–
495, Mar 2019.

[7] M. Cerezo, Andrew Arrasmith, Ryan Babbush, Si-
mon C. Benjamin, Suguru Endo, Keisuke Fujii, Jarrod R.
McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio,
and Patrick J. Coles. Variational quantum algorithms.
Nature Reviews Physics, Aug 2021.

[8] Michael Lubasch, Jaewoo Joo, Pierre Moinier, Martin
Kiﬀner, and Dieter Jaksch. Variational quantum algo-
rithms for nonlinear problems. Phys. Rev. A, 101:010301,
Jan 2020.

[9] Jacob Biamonte. Universal variational quantum compu-

tation. Phys. Rev. A, 103:L030401, Mar 2021.

[10] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-
Hong Yung, Xiao-Qi Zhou, Peter J. Love, Alán Aspuru-
Guzik, and Jeremy L. O’Brien. A variational eigen-
value solver on a photonic quantum processor. Nature
Communications, 5(1):4213, Jul 2014.

[11] Ken M. Nakanishi, Kosuke Mitarai, and Keisuke Fujii.
Subspace-search variational quantum eigensolver for ex-
cited states. Phys. Rev. Research, 1:033062, Oct 2019.

[12] William M. Kirby, Andrew Tranter, and Peter J. Love.
Contextual Subspace Variational Quantum Eigensolver.
Quantum, 5:456, May 2021.

[13] Ilya G. Ryabinkin, Scott N. Genin, and Artur F.
Constrained variational quantum eigen-
Izmaylov.
solver: Quantum computer search engine in the fock
space. Journal of Chemical Theory and Computation,
15(1):249–255, Jan 2019.

[14] Kazuhiro Seki, Tomonori Shirakawa, and Seiji Yunoki.
Symmetry-adapted variational quantum eigensolver.
Phys. Rev. A, 101:052340, May 2020.

[15] Bryan T. Gard, Linghua Zhu, George S. Barron,
Nicholas J. Mayhall, Sophia E. Economou, and Edwin
Barnes. Eﬃcient symmetry-preserving state prepara-
tion circuits for the variational quantum eigensolver al-
gorithm. npj Quantum Information, 6(1):10, Jan 2020.

[16] Nikolay V. Tkachenko, James Sud, Yu Zhang, Sergei Tre-
tiak, Petr M. Anisimov, Andrew T. Arrasmith, Patrick J.
Coles, Lukasz Cincio, and Pavel A. Dub. Correlation-
informed permutation of qubits for reducing ansatz depth
in the variational quantum eigensolver. PRX Quantum,
2:020337, Jun 2021.

[17] Ying Li and Simon C. Benjamin. Eﬃcient variational
quantum simulator incorporating active error minimiza-
tion. Phys. Rev. X, 7:021050, Jun 2017.

[18] Sam McArdle, Tyson Jones, Suguru Endo, Ying Li, Si-
mon C. Benjamin, and Xiao Yuan. Variational ansatz-
based quantum simulation of imaginary time evolution.
npj Quantum Information, 5(1):75, Sep 2019.

[19] Hirofumi Nishi, Taichi Kosugi, and Yu-ichiro Matsushita.
Implementation of quantum imaginary-time evolution
method on nisq devices by introducing nonlocal approx-
imation. npj Quantum Information, 7(1):85, Jun 2021.

[20] Yong-Xin Yao, Niladri Gomes, Feng Zhang, Cai-Zhuang
Wang, Kai-Ming Ho, Thomas Iadecola, and Peter P.
Orth. Adaptive variational quantum dynamics simula-
tions. PRX Quantum, 2:030307, Jul 2021.

11

[21] Zi-Jian Zhang, Jinzhao Sun, Xiao Yuan, and Man-Hong
Yung. Low-depth hamiltonian simulation by adaptive
product formula, 2020.

[22] Chee Kong Lee, Pranay Patil, Shengyu Zhang, and
Chang Yu Hsieh. Neural-network variational quantum al-
gorithm for simulating many-body dynamics. Phys. Rev.
Research, 3:023095, May 2021.

[23] Kenji Kubo, Yuya O. Nakagawa, Suguru Endo, and
Variational quantum simulations
Phys. Rev. A,

Shota Nagayama.
of stochastic diﬀerential equations.
103:052425, May 2021.

[24] Hsin-Yuan Huang, Kishor Bharti, and Patrick Reben-
trost. Near-term quantum algorithms for linear systems
of equations, 2019.

[25] Yi ğit Subaş ı, Rolando D. Somma, and Davide Orsucci.
Quantum algorithms for systems of linear equations in-
spired by adiabatic quantum computing. Phys. Rev.
Lett., 122:060504, Feb 2019.

[26] Oleksandr Kyriienko, Annie E. Paine, and Vincent E.
Elfving. Solving nonlinear diﬀerential equations with dif-
ferentiable quantum circuits. Phys. Rev. A, 103:052416,
May 2021.

[27] Carlos Bravo-Prieto, Ryan LaRose, M. Cerezo, Yigit
Subasi, Lukasz Cincio, and Patrick J. Coles. Variational
quantum linear solver, 2020.

[28] Xiaosi Xu, Jinzhao Sun, Suguru Endo, Ying Li, Simon C.
Benjamin, and Xiao Yuan. Variational algorithms for
linear algebra, 2019.

[29] Rolando D. Somma and Yi ğit Subaş ı. Complexity of
quantum state veriﬁcation in the quantum linear systems
problem. PRX Quantum, 2:010315, Jan 2021.

[30] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost.
Quantum principal component analysis. Nature Physics,
10(9):631–633, Sep 2014.

[31] Ryan LaRose, Arkin Tikku, Étude O’Neel-Judy, Lukasz
Cincio, and Patrick J. Coles. Variational quantum state
diagonalization. npj Quantum Information, 5(1):57, Jun
2019.

[32] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quan-
tum circuit learning. Phys. Rev. A, 98:032309, Sep 2018.
[33] Edward Farhi and Hartmut Neven. Classiﬁcation with
quantum neural networks on near term processors, 2018.
[34] Maria Schuld and Nathan Killoran. Quantum machine
learning in feature hilbert spaces. Phys. Rev. Lett.,
122:040504, Feb 2019.

[35] Vojtěch Havlíček, Antonio D. Córcoles, Kristan Temme,
Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and
Jay M. Gambetta. Supervised learning with quantum-
enhanced feature spaces. Nature, 567(7747):209–212,
Mar 2019.

[36] Maria Schuld, Alex Bocharov, Krysta M. Svore, and
Nathan Wiebe. Circuit-centric quantum classiﬁers. Phys.
Rev. A, 101:032308, Mar 2020.

[37] Iris Cong, Soonwon Choi, and Mikhail D. Lukin. Quan-
tum convolutional neural networks. Nature Physics,
15(12):1273–1278, Dec 2019.

[38] Arthur Pesah, M. Cerezo, Samson Wang, Tyler Volkoﬀ,
Andrew T. Sornborger, and Patrick J. Coles. Absence
of barren plateaus in quantum convolutional neural net-
works, 2020.

[39] Kaining Zhang, Min-Hsiu Hsieh, Liu Liu, and Dacheng
Tao. Toward trainability of quantum neural networks,
2020.

[40] Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, To-
bias J. Osborne, Robert Salzmann, Daniel Scheiermann,
and Ramona Wolf. Training deep quantum neural net-
works. Nature Communications, 11(1):808, Feb 2020.
[41] Andrew Arrasmith, Lukasz Cincio, Andrew T. Sorn-
borger, Wojciech H. Zurek, and Patrick J. Coles. Varia-
tional consistent histories as a hybrid algorithm for quan-
tum foundations. Nature Communications, 10(1):3438,
Jul 2019.

[42] Mark Wilde. Quantum information theory. Cambridge

University Press, 2017.

[43] Carlos Bravo-Prieto, Diego García-Martín, and José I.
Latorre. Quantum singular value decomposer. Phys. Rev.
A, 101:062310, Jun 2020.

[44] Bálint Koczor, Suguru Endo, Tyson Jones, Yuichiro Mat-
suzaki, and Simon C Benjamin. Variational-state quan-
tum metrology. New Journal of Physics, 22(8):083038,
aug 2020.

[45] Raphael Kaubruegger, Pietro Silvi, Christian Kokail,
Rick van Bijnen, Ana Maria Rey, Jun Ye, Adam M. Kauf-
man, and Peter Zoller. Variational spin-squeezing algo-
rithms on programmable quantum sensors. Phys. Rev.
Lett., 123:260505, Dec 2019.

[46] Jacob L. Beckey, M. Cerezo, Akira Sone, and Patrick J.
Coles. Variational quantum algorithm for estimating the
quantum ﬁsher information, 2020.

[47] Johannes Jakob Meyer, Johannes Borregaard, and Jens
A variational toolbox for quantum multi-
npj Quantum Information,

Eisert.
parameter estimation.
7(1):89, Jun 2021.

[48] Nelder A. John and R. Mead. A simplex method for
function minimization. Computer Journal, 7(4):308–313,
1965.

[49] Shaopeng Zhu, Shih-Han Hung, Shouvanik Chakrabarti,
and Xiaodi Wu. On the principles of diﬀerentiable quan-
tum programming languages. In Proceedings of the 41st
ACM SIGPLAN Conference on Programming Language
Design and Implementation, PLDI 2020, page 272–285,
New York, NY, USA, 2020. Association for Computing
Machinery.

[50] Frank Schäfer, Michal Kloc, Christoph Bruder, and Niels
Lörch. A diﬀerentiable programming method for quan-
tum control. Machine Learning: Science and Technology,
1(3):035009, aug 2020.

[51] James Stokes, Josh Izaac, Nathan Killoran, and Giuseppe
Carleo. Quantum Natural Gradient. Quantum, 4:269,
May 2020.

[52] Anna Lopatnikova and Minh-Ngoc Tran. Quantum nat-

ural gradient for variational bayes, 2021.

[53] Naoki Yamamoto. On the natural gradient for variational

quantum eigensolver, 2019.

[54] Aram W. Harrow and John C. Napp. Low-depth gradi-
ent measurements can improve convergence in variational
hybrid quantum-classical algorithms. Phys. Rev. Lett.,
126:140502, Apr 2021.

[55] Gian Giacomo Guerreschi and Mikhail Smelyanskiy.
Practical optimization for hybrid quantum-classical al-
gorithms, 2017.

[56] Ville Bergholm, Josh Izaac, Maria Schuld, Chris-
tian Gogolin, M. Sohaib Alam, Shahnawaz Ahmed,
Juan Miguel Arrazola, Carsten Blank, Alain Delgado, So-
ran Jahangiri, Keri McKiernan, Johannes Jakob Meyer,
Zeyue Niu, Antal Száva, and Nathan Killoran. Pen-
nylane: Automatic diﬀerentiation of hybrid quantum-

12

classical computations, 2020.

[57] Jun Li, Xiaodong Yang, Xinhua Peng, and Chang-Pu
Sun. Hybrid quantum-classical approach to quantum op-
timal control. Phys. Rev. Lett., 118:150503, Apr 2017.

[58] Maria Schuld, Ville Bergholm, Christian Gogolin, Josh
Izaac, and Nathan Killoran. Evaluating analytic gradi-
ents on quantum hardware. Phys. Rev. A, 99:032331,
Mar 2019.

[59] Leonardo Banchi and Gavin E. Crooks. Measuring Ana-
lytic Gradients of General Quantum Evolution with the
Stochastic Parameter Shift Rule. Quantum, 5:386, Jan-
uary 2021.

[60] Qiskit: Open-source quantum development.
[61] Cirq Developers. Cirq, August 2021. See full list of
authors on Github: https://github .com/quantumlib/-
Cirq/graphs/contributors.

[62] Robert S. Smith, Michael J. Curtis, and William J. Zeng.
A practical quantum instruction set architecture, 2017.
[63] Krysta Svore, Alan Geller, Matthias Troyer, John
Azariah, Christopher Granade, Bettina Heim, Vadym
Kliuchnikov, Mariia Mykhailova, Andres Paz, and Mar-
tin Roetteler. Q#: Enabling scalable quantum comput-
ing and development with a high-level dsl. In Proceedings
of the Real World Domain Speciﬁc Languages Workshop
2018, RWDSL2018, New York, NY, USA, 2018. Associa-
tion for Computing Machinery.

[64] Benjamin Villalonga, Sergio Boixo, Bron Nelson,
Christopher Henze, Eleanor Rieﬀel, Rupak Biswas, and
Salvatore Mandrà. A ﬂexible high-performance simulator
for verifying and benchmarking quantum circuits imple-
mented on real hardware. npj Quantum Information,
5(1):86, Oct 2019.

[65] Chase Roberts, Ashley Milsted, Martin Ganahl, Adam
Zalcman, Bruce Fontaine, Yijian Zou, Jack Hidary,
Guifre Vidal, and Stefan Leichenauer. Tensornetwork:
A library for physics and machine learning, 2019.

[66] Yasunari Suzuki, Yoshiaki Kawase, Yuya Masumura,
Yuria Hiraga, Masahiro Nakadai, Jiabao Chen, Ken M.
Nakanishi, Kosuke Mitarai, Ryosuke Imai, Shiro Tamiya,
Takahiro Yamamoto, Tennin Yan, Toru Kawakubo,
Yuya O. Nakagawa, Yohei Ibe, Youyuan Zhang, Hirot-
sugu Yamashita, Hikaru Yoshimura, Akihiro Hayashi,
and Keisuke Fujii. Qulacs: a fast and versatile quantum
circuit simulator for research purpose, 2020.

[67] Gian Giacomo Guerreschi, Justin Hogaboam, Fabio
Baruﬀa, and Nicolas P D Sawaya. Intel quantum sim-
ulator:
a cloud-ready high-performance simulator of
quantum circuits. Quantum Science and Technology,
5(3):034007, may 2020.

[68] Damian S. Steiger, Thomas Häner, and Matthias Troyer.
ProjectQ: an open source software framework for quan-
tum computing. Quantum, 2:49, January 2018.

[69] Adam Kelly.
opencl, 2018.

Simulating quantum computers using

[70] Tyson Jones, Anna Brown, Ian Bush, and Simon C. Ben-
jamin. Quest and high performance simulation of quan-
tum computers. Scientiﬁc Reports, 9(1):10736, Jul 2019.
[71] K. De Raedt, K. Michielsen, H. De Raedt, B. Trieu,
G. Arnold, M. Richter, Th. Lippert, H. Watanabe, and
N. Ito. Massively parallel quantum computer simula-
tor. Computer Physics Communications, 176(2):121–136,
2007.

[72] Hans De Raedt, Fengping Jin, Dennis Willsch, Madita
Willsch, Naoki Yoshioka, Nobuyasu Ito, Shengjun Yuan,

and Kristel Michielsen. Massively parallel quantum com-
puter simulator, eleven years later. Computer Physics
Communications, 237:47–61, 2019.
[73] Igor L. Markov and Yaoyun Shi.

Simulating quan-
tum computation by contracting tensor networks. SIAM
Journal on Computing, 38(3):963–981, 2008.

[74] Feng Pan, Pengfei Zhou, Sujie Li, and Pan Zhang. Con-
tracting arbitrary tensor networks: General approximate
algorithm and applications in graphical models and quan-
tum circuit simulations. Phys. Rev. Lett., 125:060503,
Aug 2020.

[75] Igor L. Markov, Aneeqa Fatima, Sergei V. Isakov, and
Sergio Boixo. Quantum supremacy is both closer and
farther than it appears, 2018.

[76] Michael Broughton, Guillaume Verdon, Trevor McCourt,
Antonio J. Martinez, Jae Hyeon Yoo, Sergei V. Isakov,
Philip Massey, Murphy Yuezhen Niu, Ramin Halavati,
Evan Peters, Martin Leib, Andrea Skolik, Michael Streif,
David Von Dollen, Jarrod R. McClean, Sergio Boixo,
Dave Bacon, Alan K. Ho, Hartmut Neven, and Masoud
Mohseni. Tensorﬂow quantum: A software framework for
quantum machine learning, 2020.

[77] William Huggins, Piyush Patil, Bradley Mitchell, K Bir-
gitta Whaley, and E Miles Stoudenmire. Towards quan-
tum machine learning with tensor networks. Quantum
Science and Technology, 4(2):024001, jan 2019.

[78] Chuong Nguyen Quoc, Le Bin Ho, Lan Nguyen Tran,
and Hung Q. Nguyen. Qsun, a platform towards quan-
tum machine learning, 2021.
https://github.com/
ChuongQuoc1413017/Quantum_Virtual_Machine.

[79] Michael A. Nielsen and Isaac L. Chuang. Quantum
computation and quantum information. Cambridge Uni-
versity Press, 2019.

[80] Ryan LaRose. Overview and Comparison of Gate
Level Quantum Software Platforms. March 2019.
arXiv:1807.02500 [quant-ph].

[81] Willard Miller. Symmetry groups and their applications.

Academic Press, 1990.

[82] Gavin E. Crooks. Gradients of parameterized quantum
gates using the parameter-shift rule and gate decompo-
sition. May 2019. arXiv:1905.13311 [quant-ph].

[83] Bradley Efron, Trevor Hastie,

Iain Johnstone, and
Robert Tibshirani. Least angle regression. Annals of
Statistics, page 407, 2004.

[84] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

[85] Ryan LaRose and Brian Coyle. Robust data encodings
for quantum classiﬁers. Phys. Rev. A, 102:032420, Sep
2020.

[86] Diederik P. Kingma and Jimmy Ba. Adam: A method

for stochastic optimization, 2017.

[87] Social

network

neural
https://www.kaggle.com/sunilsharanappa/
social-network-ads-knn-lr-neural-network.

knn,

ads

lr,

network.

13

Appendix A: Code for benchmarking

In this section, we present the code used to bench-
mark the performance of Qsun and compared it to other
packages such as Qiskit, ProjectQ, and Pennylane. The
criteria are the time to execute a certain quantum circuit
X, and a
of constant depth. The circuit contain an H, a
CNOT with depth = 10 and measurements on all qubits.
With the number of qubits as a variable, the time needed
to execute the circuit is estimated, yielding the package’s
performance. The number of qubits varies from 2 to 15
qubits, which means we have to implement 14 circuits for
each simulation. The implementation’s time is stored in
a Pandas’s Dataframe and is shown in Fig. 2 in the main
text.

√

1 # import module for visualization
2 import pandas as pd
3 import seaborn as sns
4 import time
5 import matplotlib . pyplot as plt
6
7 # import Qiskit module
8 from qiskit import IBMQ , BasicAer
9 from qiskit . providers . ibmq import least_busy
10 from qiskit import QuantumCircuit ,

ClassicalRegister , QuantumRegister ,
execute

11
12 # import ProjectQ module
13 from projectq import MainEngine
14 import projectq . ops as ops
15 from projectq . backends import Simulator
16
17 # import Qsun module
18 from Qsun . Qcircuit import *
19 from Qsun . Qgates import *
20
21 # import Pennylane module
22 import pennylane as qml
23 from pennylane import numpy as np
24
25 # Qsun circuit
26 def qvm_circuit ( n_qubit , depth ) :
27

circuit = Qubit ( n_qubit )
for m in range ( depth ) :

28

29

30

31

32

33

34

for i in range ( n_qubit ) :

H ( circuit , i )
Xsquare ( circuit , i )
for i in range (1 , n_qubit ) :
CNOT ( circuit , i , 0)

return circuit . probabilities ()

35
36 # Pennylane circuit
37 dev = qml . device ( ’ default . qubit ’ , wires =

qubit_max )

38
39 @qml . qnode ( dev )
40 def qml_circuit ( n_qubit , depth ) :
for m in range ( depth ) :
41

42

43

44

45

46

47

48

for i in range ( n_qubit ) :

qml . Hadamard ( wires = i )
qml . SX ( wires = i )

for i in range (1 , n_qubit ) :

qml . CNOT ( wires =[ i , 0])
return qml . probs ( wires = range ( n_qubit ) )

52

53

54

55

56

57

58

59

60

61

62

63

68

69

70

71

72

73

74

75

76

77

84

85

86

49 # Qiskit circuit
50 def qiskit _circuit ( n_qubit , depth ) :
51

qr = QuantumRegister ( n_qubit )
cr = Cl ass ica lRe gis ter ( n_qubit )
circuit = QuantumCircuit ( qr , cr )
for m in range ( depth ) :

for i in range ( n_qubit ) :
circuit . h ( qr [ i ])
circuit . rx ( math . pi /2 , qr [ i ])

for i in range (1 , n_qubit ) :

circuit . cx ( qr [ i ] , qr [0])

circuit . measure_all ()
backend = BasicAer . get_backend ( ’
qasm_simulator ’)
result = execute ( circuit , backend = backend
, shots =1024) . result ()
return result . get_counts ( circuit )

64
65 # ProjectQ circuit
66 def p r oje c tq_circuit ( n_qubit , depth ) :
67

eng = MainEngine ( backend = Simulator (
gate_fusion = True ) , engine_list =[])
qbits = eng . allocate_qureg ( n_qubit )
for level in range ( depth ) :

for q in qbits :
ops . H | q
ops . SqrtX | q
if q != qbits [0]:

ops . CNOT |( q , qbits [0])

for q in qbits :

ops . Measure | q

eng . flush ()

78
79 # run test for Qsun
80 data_qvm = []
81
82 for n_qubit in range ( qubit_min , qubit_max ) :
83

start = time . time ()
qvm_circuit ( n_qubit , depth )
end = time . time ()
data_qvm . append ( end - start )

87
88 # define the test parametes
89 qubit_min = 2
90 qubit_max = 10
91 depth = 10
92
93 # run test for Pennylane
94 data_qml = []
95
96 for n_qubit in range ( qubit_min , qubit_max ) :
97

start = time . time ()
qml_circuit ( n_qubit , depth )
end = time . time ()
data_qml . append ( end - start )

101
102 # run test for Qiskit
103 data_qiskit = []
104
105 for n_qubit in range ( qubit_min , qubit_max ) :
106

start = time . time ()
qiskit_circuit ( n_qubit , depth )
end = time . time ()
data_qiskit . append ( end - start )

110
111 # run test for ProjectQ
112 data_projectq = []
113
114 for n_qubit in range ( qubit_min , qubit_max ) :
115

start = time . time ()

98

99

100

107

108

109

14

116

117

118

projectq_circuit ( n_qubit , depth )
end = time . time ()
data_projectq . append ( end - start )

119
120 df = pd . DataFrame ({ ’ QVM ’: data_qvm , ’

Pennylane ’: data_qml ,

121

’ Qiskit ’: data_qiskit , ’

ProjectQ ’: data_projectq } , index = range (
qubit_min , qubit_max ) )

122
123 sns . lineplot ( data = df )
124 plt . xlabel ( ’ Number of Qubits with depth = ’+

str ( depth ) )
125 plt . ylabel ( ’ Time ( s ) ’)
126 plt . grid ()
127 plt . savefig ( ’ compare . png ’)

Code example 1. Implementation of the benchmarking test
languages: Qsun, Qiskit, ProjectQ, and
on four typical
Pennylane.

Appendix B: Code for QDP benchmarking

For this code, we run the QDP algorithm on Qsun and
measure how long it will take to update parameters. We
implement the circuit described in Fig. 3 and measure
the expected values of output, then record the time. The
number of qubits varies from 1 to 10, and the maximum
number of iterations is 1000. Parameters for QDP, in this
case, are η = 0.1 and s = π/20.

1 from Qsun . Qcircuit import *
2 from Qsun . Qgates import *
3 import numpy as np
4 import time
5
6 def circuit ( params , n ) :
c = Qubit ( n )
7
for j in range (0 , 2* n , 2) :
RX (c , j //2 , params [ j ])
RY (c , j //2 , params [ j +1])

9

8

10

11

return c

12
13 def output ( params ) :
14

c = circuit ( params , len ( params ) //2)
prob = c . probabilities ()
return - np . sum ([ i * prob [ i ] for i in range (
len ( prob ) ) ])

17
18 def cost ( params ) :
19

expval = output ( params )
return expval

21
22 def grad ( params , shift , eta ) :
23

for i in range ( len ( params ) ) :
params_1 = params . copy ()
params_2 = params . copy ()
params_1 [ i ] += shift
params_2 [ i ] -= shift
diff [ i ] = ( cost ( params_1 ) - cost (

params_2 ) ) /(2* np . sin ( shift ) )
for i in range ( len ( params ) ) :

params [ i ] = params [ i ] - eta * diff [ i ]

return params

15

16

20

24

25

26

27

28

29

30

31

32
33 time_qvm = []

34 n = 10
35 for i in range (1 , n +1) :
36

37

38

39

40

41

42

43

44

45

46

start = time . time ()
params = np . random . normal ( size =(2* i ,) )
diff = np . random . normal ( size =(2* i ,) )

for i in range (1000) :

params = grad ( params , np . pi /20 , eta

=0.01)
end = time . time ()

time_qvm . append ( end - start )
print ( cost ( params ) )

example 2.

Code
diﬀerentiable programming test in Qsun.

Implementation of

the quantum

Appendix C: QDP Tutorial Appendix

This appendix demonstrates how to run a gradient de-
scent algorithm using QDP in Qsun. We use η = 0.1 and
s = π/20 to ﬁnd the derivative of a circuit. By doing
that, we maximize the expectation values of one qubit.
So the objective function we want to maximize is |(cid:104)1|ψ(cid:105)|2,
where |ψ(cid:105) is a quantum state of that qubit.

7

8

9

13

14

18

19

20

24

25

26

27

28

29

30

31

1 import numpy as np
2 from Qsun . Qcircuit import *
3 from Qsun . Qgates import *
4
5 def circuit ( params ) :
6

c = Qubit (1)
RX (c , 0 , params [0])
RY (c , 0 , params [1])
return c

10
11 def output ( params ) :
12

c = circuit ( params )
prob = c . probabilities ()
return 0.* prob [0] + 1* prob [1]

15
16 def cost ( params ) :
17

c = circuit ( params )
prob = c . probabilities ()
expval = output ( params )
return np . abs ( expval - 1) **2

21
22 def grad ( params , shift , eta ) :
23

for i in range ( len ( params ) ) :
params_1 = params . copy ()
params_2 = params . copy ()
params_1 [ i ] += shift
params_2 [ i ] -= shift
diff [ i ] = ( cost ( params_1 ) - cost (

params_2 ) ) /(2* np . sin ( shift ) )
for i in range ( len ( params ) ) :

params [ i ] = params [ i ] - eta * diff [ i ]

return params

32
33 params = np . random . normal ( size =(2 ,) )
34 diff = np . random . normal ( size =(2 ,) )
35
36 for i in range (1000) :
37

params = grad ( params , np . pi /20 , eta =0.01)

38
39 print ( ’ Circuit output : ’ , output ( params ) )

15

40 print ( ’ Final parameter : ’ , params )
41
42 >>> Circuit output : -0.9381264201123851
43 >>> Final parameter : [ 0.29017649

-2.93657549]

Code example 3.
algorithm by using QDP in Qsun.

Implementation of the Gradient Descent

Appendix D: Quantum Linear Regression Appendix

is pro-
Here, a Quantum Linear Regression model
grammed in Qsun, with results shown in Fig. 4.
Its
parameters are k = 10, η = 0.1, and s = π/20. The
optimization algorithm used in this code is Gradient De-
scent.

1 from Qsun . Qcircuit import *
2 from Qsun . Qgates import *
3 from sklearn import datasets
4
5 def circuit ( params ) :
6

c = Qubit (1)
RX (c , 0 , params [0])
RY (c , 0 , params [1])
return c

10
11 def output ( params ) :
12

c = circuit ( params )
prob = c . probabilities ()
return 1* prob [0] - 1* prob [1]

7

8

9

13

14

15
16 def predict ( x_true , coef_params ,

intercept_params , boundary =10) :
coef = boundary * output ( coef_params )
intercept = boundary * output (
intercept_params )
return coef * x_true . flatten () + intercept

17

18

19

20
21 def errors ( x_true , y_true , coef_params ,
intercept_params , boundary ) :
return m e an _ sq u ar e d_ e rr o r ( y_true , predict
( x_true , coef_params , intercept_params ,
boundary ) )

22

23
24 def grad ( x_true , y_true , coef_params ,

intercept_params , shift , eta , boundary ) :

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

coef_diff = np . zeros ((2 ,) )
intercept_diff = np . zeros ((2 ,) )

for i in range ( len ( coef_params ) ) :

coef_params_1 = coef_params . copy ()
coef_params_2 = coef_params . copy ()
coef_params_1 [ i ] += shift
coef_params_2 [ i ] -= shift
for x , y in zip ( x_true , y_true ) :

coef_diff [ i ] -= x *( y - predict (x ,

coef_params , intercept_params , boundary ) )
*( output ( coef_params_1 ) - output (
coef_params_2 ) ) /(2* np . sin ( shift ) )

for i in range ( len ( coef_params ) ) :

i nt e rc e pt _ pa r am s _1 = intercept_params

. copy ()

16

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

i nt e rc e pt _ pa r am s _2 = intercept_params

. copy ()

i nt e rc e pt _ pa r am s _1 [ i ] += shift
i nt e rc e pt _ pa r am s _2 [ i ] -= shift
for x , y in zip ( x_true , y_true ) :

intercept_diff [ i ] -= (y - predict (x
, coef_params , intercept_params , boundary
) ) *( output ( i n te r ce pt _ pa r am s _1 ) - output (
i nt e rc e pt _ pa r am s _2 ) ) /(2* np . sin ( shift ) )

coef_diff = coef_diff * boundary *2/ len (
y_true )
intercept_diff = intercept_diff * boundary
*2/ len ( y_true )

for i in range ( len ( coef_params ) ) :

coef_params [ i ] = coef_params [ i ] - eta

* coef_diff [ i ]

for i in range ( len ( intercept_para ms ) ) :

in tercept_params [ i ] =

in te rce pt_params [ i ] - eta * intercept_diff [
i ]

return coef_params , intercept_params

56
57 X , y = datasets . load_diabetes ( return_X_y = True

)

58 y = ( y - np . min ( y ) ) / ( np . max ( y ) - np . min ( y ) )

59 # Use only one feature
60 X = X [: , np . newaxis , 2]
61
62 # Split the data into training / testing sets
63 X_train = X [:400]
64 X_test = X [ -10:]
65
66 # Split the targets into training / testing

sets

67 y_train = y [:400]
68 y_test = y [ -10:]
69
70 coef_params = np . random . normal ( size =(2 ,) )
71 intercept_params = np . random . normal ( size =(2 ,)

)

72
73 start = time . time ()
74 for i in range (1000) :
75

coef_params , intercept_params = grad (
X_train , y_train , coef_params ,
intercept_params , np . pi /20 , eta =0.01 ,
boundary =10)

76 end = time . time ()
77
78 y_pred = predict ( X_test , coef_params ,

intercept_params , boundary =10)

Code example 4.
model in qSUN.

Implementation of the linear regression

Appendix E: Quantum Neural Network Appendix

The code showing in this appendix implements a process from training a QNN model with 5 layers and 2 qubits on the
"Social_Network_Ads" dataset. From this dataset, we use a quantum neural network to predict whether a customer
buys a car or not. The function train_test_split() splits the original dataset into the training and testing datasets.
MinMaxScaler() scales the training dataset to [0, 1] before feeding it into the QNN circuit. The optimization algorithm
in this model is the Adam optimization algorithm, as presented in the main text. The parameters used in the model are:
test_size = 0.2; random_state = 0; s = π/20; η = 0.1; β1 = 0.9, β2 = 0.99; (cid:15) = 10−6; number_of_iterations = 150.

1 # import libraries
2 from Qsun . Qcircuit import *
3 from Qsun . Qgates import *
4 from Qsun . Qmeas import *
5 from sklearn . model_selection import train_test_split
6 from sklearn . preprocessing import MinMaxScaler
7 from sklearn . metrics import confusion_matrix
8 import pandas as pd
9 import seaborn as sn
10 import matplotlib . pyplot as plt
11 import time
12
13 # one layer with full entanglement
14 def layer ( circuit , params ) :
circuit_layer = circuit
15
n_qubit = len ( params )
for i in range ( n_qubit ) :

17

16

18

19

20

21

22

23

RX ( circuit_layer , i , params [ i ][0])
RY ( circuit_layer , i , params [ i ][1])

for i in range ( n_qubit -1) :

CNOT ( circuit_layer , i , i +1)

CNOT ( circuit_layer , n_qubit -1 , 0)
return circuit_layer

24
25 # encoding the features
26 def initial_state ( sample ) :

27

28

29

30

31

32

cir cu i t_initial = Qubit ( len ( sample ) )
ampli_vec = np . array ([ np . sqrt ( sample [0]) , np . sqrt (1 - sample [0]) ])
for i in range (1 , len ( sample ) ) :

ampli_vec = np . kron ( ampli_vec , np . array ([ np . sqrt ( sample [ i ]) , np . sqrt (1 - sample [ i ]) ]) )

cir cu i t_initial . amplitude = ampli_vec
return circuit_initial

17

33
34 # QNN circuit
35 def qnn ( circuit , params ) :
n_layer = len ( params )
36
circuit_qnn = circuit
for i in range ( n_layer ) :

37

38

39

40

circuit_qnn = layer ( circuit_qnn , params [ i ])

return circuit_qnn

41
42 # QNN model
43 def qnn_model ( sample , params ) :
44

circuit_model = initial_state ( sample )
circuit_model = qnn ( circuit_model , params )
return circuit_model

47
48 # activation function
49 def sigmoid ( x ) :
50

return 1 / (1 + math . exp ( -10* x ) )

51
52 # make a prediction
53 def predict ( circuit ) :
54

prob_0 = measure_one ( circuit , 0)
prob_1 = measure_one ( circuit , 1)
expval_0 = prob_0 [1]
expval_1 = prob_1 [1]
pred = sigmoid ( expval_0 - expval_1 )
return [ pred , 1 - pred ]

60
61 # make a prediction for exp
62 def predict_ex ( circuit ) :
63

prob_0 = measure_one ( circuit , 0)
prob_1 = measure_one ( circuit , 1)
expval_0 = prob_0 [1]
expval_1 = prob_1 [1]
return [ expval_0 , expval_1 ]

68
69 # loss function
70 def square_loss ( labels , predictions ) :
71

loss = 0
for l , p in zip ( labels , predictions ) :
loss = loss + (1 - p [ l ]) ** 2

loss = loss / len ( labels )
return loss

76
77 # loss function of QNN
78 def cost ( params , features , labels ) :
79

45

46

55

56

57

58

59

64

65

66

67

72

73

74

75

80

85

86

87

88

89

90

91

92

93

94

95

96

preds = [ predict ( qnn_model (x , params ) ) for x in features ]
return square_loss ( labels , preds )

81
82 # https :// d2l . ai / c h a p t e r _ o p t i m i z a t i o n / adam . html ? highlight = adam
83 def adam ( X_true , y_true , params , v , s , shift , eta , drop_rate , beta1 , beta2 , eps , iter_num ) :
84

diff = np . zeros ( params . shape )
for i in range ( len ( params ) ) :

for j in range ( len ( params [ i ]) ) :

for k in range ( len ( params [ i ][ j ]) ) :

dropout = np . random . choice ([0 , 1] , 1 , p = [1 - drop_rate , drop_rate ]) [0]
if dropout == 0:

params_1 = params . copy ()
params_2 = params . copy ()
params_1 [ i ][ j ][ k ] += shift
params_2 [ i ][ j ][ k ] -= shift
for x , y in zip ( X_true , y_true ) :

circuit = qnn_model (x , params )
circuit_1 = qnn_model (x , params_1 )

circuit_2 = qnn_model (x , params_2 )
ex_plus = predict_ex ( circuit_1 )
ex_subs = predict_ex ( circuit_2 )
pred = predict ( circuit )
diff_loss = (( -1) ** y ) *( -2) *(1 - pred [ y ]) * pred [ y ]*(1 - pred [ y ])
diff_ex = 10*(( ex_plus [0] - ex_subs [0]) - ( ex_plus [1] - ex_subs [1]) ) /(2*

18

np . sin ( shift ) )

diff [ i ][ j ][ k ] += diff_loss * diff_ex

diff /= len ( y_true )
v = beta1 * v + (1 - beta1 ) * diff
s = beta2 * s + (1 - beta2 ) * ( diff **2)
v_bias_corr = v / (1 - beta1 ** ( iter_num +1) )
s_bias_corr = s / (1 - beta2 ** ( iter_num +1) )
params -= eta * v_bias_corr / ( np . sqrt ( s_bias_corr ) + eps )

return params , v , s

97

98

99

100

101

102

103

104

105

106

107

108

109

110

111

112

113
114 # source : https :// www . kaggle . com / rakeshrau / social - network - ads
115 dataset = pd . read_csv ( ’ So c ia l _N e tw or k _A d s . csv ’)
116 X = dataset . iloc [: , 2: -1]. values
117 y = dataset . iloc [: , -1]. values
118
119 # splitting dataset
120 X_train , X_test , y_train , y_test = train_test_split (X , y , test_size = 0.2 , random_state = 0)
121
122 # scaling feature
123 scaler = MinMaxScaler ()
124 X_train = scaler . fit_transform ( X_train )
125 X_test = scaler . transform ( X_test )
126
127 # create parameters
128 n_layer = 5
129 params = np . random . normal ( size =( n_layer , len ( X_train [0]) , 2 ,) )
130 v = np . zeros ( params . shape )
131 s = np . zeros ( params . shape )
132
133 # training model
134 start = time . time ()
135 for i in range (150) :
136

params , v , s = adam ( X_train , y_train , params , v , s ,

137

iter_num = i )

shift = np . pi /20 , eta =0.1 , drop_rate =0.0 , beta1 =0.9 , beta2 =0.999 , eps =1 e -6 ,

138
139 # confusion matrix
140 label = y_test
141 pred = [ predict ( qnn_model (x , params ) ) for x in X_test ]
142 pred = np . argmax ( pred , axis =1)
143 con = con f usion_matrix ( label , pred )
144 sn . heatmap ( con , annot = True , cmap = " Blues " )

Code example 5. Implementation of the QNN with two layers and two features in Qsun.

