Semantic prefetching using forecast slices

Leeor Peled†
†Electrical Engineering

Uri Weiser†

Yoav Etsion†§

§Computer Science

Technion — Israel Institute of Technology
{leeor@tx, uri.weiser@ee, yetsion@tce}.technion.ac.il

0
2
0
2

y
a
M
3
1

]

C
D
.
s
c
[

1
v
2
0
1
6
0
.
5
0
0
2
:
v
i
X
r
a

Abstract—
Modern prefetchers identify memory access patterns in order
to predict future accesses. However, many applications exhibit
irregular access patterns that do not manifest any form of spatio-
temporal locality in the memory address space. Such applications
usually do not fall under the scope of existing prefetching
techniques, which observe only the stream of addresses dis-
patched by the memory unit but not the code ﬂows that produce
them. Similarly, temporal correlation prefetchers detect recurring
relations between accesses, but do not track the chain of causality
in program code that manifested the memory locality. Conversely,
like runahead execution, are
techniques that are code-aware,
limited to the basic program functionality and are bounded by
the machine depth.

In this paper we show that contextual analysis of the code
ﬂows that generate memory accesses can detect recurring code
patterns and expose their underlying semantics even for irregular
access patterns. Moreover, program locality artifacts can be
used to enhance the memory traversal code and predict future
accesses. We present the semantic prefetcher that analyzes pro-
grams at run-time and learns their memory dependency chains
and address calculation ﬂows. The prefetcher then constructs
forecast slices and injects them at key points to trigger timely
prefetching of future contextually-related iterations. We show
how this approach takes the best of both worlds, augmenting code
injection with forecast functionality and relying on context-based
temporal correlation of code slices. This combination allows us
to overcome critical memory latencies in access patterns that are
currently not covered by any other prefetcher.

Our evaluation of the semantic prefetcher using an industrial-
grade, cycle-accurate x86 simulator shows that the semantic
prefetcher improves performance by 24% on average over SPEC
2006 (with outliers up to 3.7×), and 16% on average over SPEC
2017 (with outliers up to 1.85×), using only ∼6KB of structures.

I. INTRODUCTION

Existing prefetchers are designed to analyze the memory ac-
cess stream and identify speciﬁc types of access patterns, rang-
ing from sequential and strided ones to traversals over linked
data structures. Most of them target spatio-temporal locality
and temporal correlation between addresses or address-space
artifacts (e.g., address deltas), based on the observation that
temporally or spatially adjacent accesses tend to repeat [11].
Many applications, however, make use of data structures
and algorithms whose physical layout and data access pat-
terns are not plainly observable in the memory address-space
domain (e.g., linked lists, arrays of pointers, sparse graphs,
cross-indexed tables) and require deeper analysis in order to
understand the causal relations between accesses to objects
in memory. These causal relations may involve complicated
arithmetic computations or a chain of memory dereferences.

Such relations between accesses exhibit semantic locality [27]
if they represent consequential steps along a data structure or
an algorithm. These steps are characterized by the existence of
some program code ﬂow that traverses from one data object to
the next 1. The set of all semantic relations within a program
can be said to span its data structures, describing all the steps
that the program may employ to walk through them.

In this paper we argue that the semantic relations between
memory accesses can be represented through the code seg-
ments (referred to as code slices) that generate the memory
traversals. The set of all slices effectively forms an abstract
guide to the program’s data layout, but we can further combine
or extrapolate these ﬂows to create forecast slices with more
complex “lookahead” semantics that can predict program
behavior over longer periods.

Following our observation, we present

the semantic
prefetcher that dynamically constructs and injects prefetching
code for arbitrary memory traversals. The prefetcher ana-
lyzes program code at run-time, identiﬁes the dependency
chains forming all address calculations, and detects locality
artifacts within that code based on contextual similarities.
The prefetcher then generates compact and optimized forecast
slices which are code constructs that did not exist in the
original program and enhance the code to generate longer
memory traversal steps capable of reaching future iterations.
The semantic prefetcher generates the forecast slices using
hardware-managed binary optimization. The slices are con-
structed to have no lingering side effects. Once the prefetcher
reaches sufﬁcient conﬁdence in their correctness and structural
stability, it injects them at certain interception points to trigger
prefetches.

The semantic prefetcher is fundamentally different from
previous prefetchers that aim to reconstruct address relations
or code sequences such as temporal-correlation prefetchers [3],
[25], [35], [38] and runahead-based prefetchers [1], [9], [10],
[13], [23], [24], [39]. Unlike temporal correlation prefetchers,
which detect correlations between addresses,
the semantic
prefetcher correlates program states (speciﬁc code locations
with speciﬁc history and context) with the generated code
slices. Similarly, unlike runahead-based prefetchers that run
the program (or its address generation code) in parallel to

1Spatio-temporal locality represents a speciﬁc case where the relations
are purely arithmetic and can be detected through address comparison, but
semantic locality encompasses all forms of algorithmic and data structural
relations (such as proximity within linked data structures or connectivity in
cross-indexed tables).

1

 
 
 
 
 
 
reach future iterations earlier (but are ultimately constrained
by ﬁnite out-of-order depths), the semantic prefetcher can peek
into future iso-context iterations without having to execute
everything in the middle.

The semantic prefetcher was implemented on an industrial-
grate, cycle-accurate x86 simulator that represents a modern
micro-architecture. It provides a 24% IPC speedup on average
over SPEC 2006 (outliers of up to 3.7×), and 16% on average
over SPEC 2017 (outliers of up to 85%).

Our contributions in this paper are as follows:
• We present a novel scheme of prefetching using forecast
slices. We utilize internal locality artifacts to extrapolate
the code slices and create new functional behavior with
lookahead semantics.

• We present the design of the semantic prefetcher that
injects forecast slices directly into the execution stream.
We describe its architecture: ﬂaky load detection, slice
generation, binary optimization, and dynamic prefetching
depth control.

• We demonstrate how the forecast slices can reproduce
complex patterns prevalent in common applications, and
show that these patterns are not addressed by existing
prefetchers.

• We model the semantic prefetcher using a cycle accurate
simulator. We show that it outperforms ﬁve competing
state-of-the-art prefetchers, some of which target irregular
access patterns.

The remainder of this paper is organized as follows: Sec-
tion II discusses semantic locality and its manifestation in
forecast slices. Section III presents the semantic prefetcher
and its architecture. Section IV explains the experimental
methodology. Section V shows the evaluation results and
discussion. Section VI describes related work. We conclude
in Section VII.

II. EXTRACTING SEMANTIC LOCALITY FROM MEMORY
ACCESS PATTERNS

Existing memory prefetchers scan the stream of memory
accesses and extract spatio-temporal correlations in order to
identify patterns and predict future memory accesses [11].
Some prefetchres [2], [25] also associate memory accesses
with program context (e.g., instruction pointer) to further reﬁne
their predictions.

However, basing predictions solely on the stream of mem-
ory accesses that the memory unit emits makes prefetchers
oblivious to the underlying program code semantics. Indeed,
most existing prefetchers ignore the data and control ﬂows
that generate the memory access sequences they are meant
to detect. A small number of exceptional prefetchers capable
of detecting more elaborate or irregular relations focus only
on speciﬁc access patterns such as indirect accesses (for e.g.,
A[B[i]]) [40] and linked data structures [4], [31], [32].

In this section we argue that a more fundamental form of
locality can be extracted even when no spatio-temporal locality
is present. Semantic locality [27], [28] correlates memory

Fig. 1. Critical BFS loop in graph500 showing a 4-level indirection. The
top box shows the source code, the middle shows the pseudo code subset
that comprises the slice. The bottom box shows the actual slice generated at
run-time.

accesses through their dependency within the program’s ab-
stract data layout and usage ﬂow, such as being adjacent
steps on a data structure traversal path or being consequential
steps in the execution of an algorithm. These accesses do
not necessarily exhibit any spatio-temporal correlation. While
prior work attempted to approximate semantic locality through
memoization and correlative program context cues, we show
that extracting this form of locality requires following the
set of operations that constitutes the relation between two
memory addresses. To this end, we deﬁne a code slice as
the minimal subset of the dynamic code preceding a certain
memory operation that is required to generate its memory
address. Notably, this subset can be described through the data
dependency chain that starts with the address calculation, and
goes backwards through all relevant sources at each step.

As semantic locality usually describes program constructs
such as data structures or algorithms, the relations it captures
often have strong recurrence. Extracting that form of locality
can therefore be achieved by analysis of the address-generation
code slice between two recurring consequential loads.

The remainder of this section demonstrates how program
introspection can generate short, explicit code slices that can
be replayed to generate memory accesses, or manipulated
to create forecast slices that generate future accesses at an
arbitrary distance. These code slices can be injected into the
code sequence at run time to issue memory accesses ahead of
time. Finally, the memory access stream of typical programs is
shown to be adequately covered by a small number of distinct
code slices.

A. How code slices describe dependency chains

Memory access patterns can often be tightly incorporated
in a way that makes it difﬁcult for a simple address scan
to distinguish between them without understanding program
semantics. Figure 1 demonstrates this over a breadth-ﬁrst
search (BFS) code taken from the Graph500 benchmark. The

2

Fig. 3. Example data structures and some of their spanning slices in pseudo
arithmetic.

• Strided values that were shown to have a constant stride
or are produced by a simple add/sub operation with
constant or immediate sources.

• When a loop wraps around to the same operation where
the dependency chain can
the analysis started from,
usually stop as it would also repeat itself. Linked data
structures may iterate a few time to create a deeper chain.

Before the code slice can be used to produce future accesses,
it needs to be clean of any side effects. The code is sanitized
by performing two ﬁnal steps: ﬁrst the destination registers
are replaced with temporary ones (which are guaranteed not
to be used by the original code) and their occurrences as
sources within the slice are renamed accordingly. Second,
all memory writes are eliminated from the slice. Since the
code was generated through a dependency chain, all writes to
memory were added to resolve younger loads reading from
the same address. Therefore, a simple memory-renaming may
be performed to replace such store-load operations with move
operations to a reserved register. For the sake of simplicity
partial address overlaps are ignored.

When the base slices are ready, they may be converted into
forecast slices. To this end, any detected stride is extended by
the lookahead factor: If a certain operation in the slice was
detected to induce a stride of N , that stride is replaced by L ×
N where L is the current lookahead applied for that slice. This
lookahead variable is initialized to point a few iterations ahead,
but its value dynamically changes to allow further lookahead
based on the average hit depth for that slice. The hit depth is
updated dynamically as explained in Section III.

C. Data-structure spanning code slices

Code slice generation is ﬂexible and generalized enough
to cover whole applications efﬁciently, with a relatively low
amount of code slices. Any given data structure has a set of
operations that deﬁne all forms of traversals across it within
a given program. We deﬁne this set of operations as spanning
the data structure. Some examples are shown in Figure 3.

A linked list, for example, is spanned by the action of
dereferencing its next elements pointer. A tree is spanned by
the actions of descending from a node to any given child. The

Fig. 2. Dynamic ﬂow of graph500 (Figure 1) unrolled to show a possible
lookahead prefetch based on the forecast slice. Changing the stride at the
beginning of the slice can predict accesses on far iterations. Furthermore,
iterations with similar contexts (such as branch history) will invoke their future
counterparts.

main while loop traverses an array of vertices. An internal loop
then scans each vertex’s outgoing edges to ﬁnd its neighboring
vertices and check their BFS depth.

Notably,

the top level access pattern (array “vlist”) is
sequential, but the deeper levels are accessed through data de-
pendent patterns (the edge loop is also sequential but relatively
short, making the ﬁrst edge of each vertex the critical element).
These accesses have no spatial locality and very little temporal
reuse. Even contextual cues such as the program counter
do not help in correlating the accesses. However, the ﬁgure
shows that the dependency chain within each iteration, whose
accesses are increasingly critical to program performance, can
be represented using a short code slice.

The use of the extracted code slice is demonstrated in
Figure 2. Thanks to the sequential nature of the top loop that
exposes spatial locality within the ﬁrst load in the slice, a
simple change in the stride delta can create a forecast slice that
predicts accesses in the next iterations at the top loop. Overall,
the example detailed in Figures 1 and 2 demonstrates how
code slices can represent the dependency chain of irregular
data structures, and how these slices can generate lookahead
semantics within the algorithm.

B. Forecast slice creation

Tracking all dependency chains for a given load would
construct a graph of instructions that may span back to the
beginning of the program. To generate concise and useful
slices, history tracking is limited by breaking the dependency
chain in the following cases:

• Constant values remaining static during analysis.

3

Fig. 4. Number of unique slices sampled in SPEC 2006/2017. Collected over
slices with at least 1k prefetches sent. Since any recurring load would attempt
constructing a slice, this represents the number of slices required for coverage
of all recurring loads.

Fig. 6. Semantic prefetcher block diagram (existing core blocks in gray).

Fig. 5. Breakdown of slices in Figure 4 according to their load-chain depth
and number of arithmetic operations, measured over SPEC 2006/2017. The
front line marks the rough range covered by existing prefetchers.

Fig. 7. PIE lifetime ﬂow chart.

semantic relation must capture all data structures required to
complete any recurring traversal step.

We demonstrate the effectiveness of code slices in Figure 4,
which shows the number of unique slices needed to cover
all accesses to the main data structures in the SPEC 2006
and 2017 benchmarks (sampling methodology is explained
in Section IV). The results were obtained by running the
construction ﬂow on each load that has a sufﬁcient level of
recurrence (recurring at least three times and passing seven
validation phases to conﬁrm that its slice is invariant). We
ﬁltered out low-usage strides (below 1k of actual hits on
a generated slice). Figure 4 demonstrates the efﬁciency of
memory access coverage of code slices. Speciﬁcally, 39 of
the 46 benchmarks require only up to ∼100 slices to cover all
recurring loads, and only one benchmark requires more than
300 slices. This indicates that a prefetcher constructing code
slices can cover a large code base with reasonable storage
requirements. The average slice size is 6.6 operations, and the
median is 3.5 operations.

Detecting semantic locality through code slices generalizes
the existing paradigms of data locality. Figure 5 classiﬁes the
code slices (observed in Figure 4) according to their memory
dereference depth (longest dependent load chain within the
slice) and the number of arithmetic operations. The circle sizes
indicate relative number of slices within each bucket. The spe-

cial case of (1,1) represents slices that have a single arithmetic
operation and a single load based on it, which for the most part
conform with the common stride pattern. Another interesting
case is the (2,1) and (2,2) data points (two loads and one or two
arithmetic operation), which includes most examples of array-
of-arrays/pointers (A[B[i]] accesses): one index stride, one
internal array reference, possible index/pointer math and outer
array access. These are potentially covered by IMP [40] or
similar dereference based prefetchers like Jump-pointers [32].
Notably, while the largest data points pertain to loads that
are addressed by existing prefetchers (37% of all loads in the
ﬁgure fall under the stride pattern; 13% are within the two
simple single-dereference patterns), there are still many cases
left outside that are not targeted by any existing prefetcher.
Semantic analysis can cover all these cases using the same
mechanism, thereby generalizing existing prefetchers without
having to manually design for each use-case.

III. PREFETCHER ARCHITECTURE

In this section we describe the architecture of the semantic
prefetcher. Figure 6 shows the high level block diagram of the
components: 1) The ﬂakiness detector for tracking recurring
loads; 2) A cyclic History queue, tracking retired code ﬂow;
3) The prefetch injection entries (PIE) array, storing slices;
4) Several walker FSMs, generating and validating slices; 5)

4

The slice injector. and 6) The prefetch queue for tracking
usefulness and providing feedbacks.

A. Flaky load detection

The ﬁrst component

is the ﬂakiness detector, which is
responsible for isolating loads that have both high recurrence
and miss rates. The unit identiﬁes and tracks load context by a
combination of its instruction pointer (IP) and a branch history
register (BHR). Our BHR tracks up to 6 recent branches. Each
branch is represented by the lowest 4 bits of its IP, with the
least signiﬁcant bit XOR-ed with the binary outcome of that
branch (taken or not). Together, the load IP and the BHR
represent an instance of load within a speciﬁc program context.
This method can distinguish between different occurrences
within nested loops or complex control ﬂows, which may
affect the access pattern and the generated slice. The IP and
BHR are concatenated and hashed to create an index that
would identify the load throughout the prefetcher mechanisms.
For each load missing the L1, the prefetcher allocates a
prefetch injection entry (PIE) in the PIE array. These entries
serve to track potential loads (within some context) and, if
considered useful, construct a slice for them and store it for
prefetching. Figure 7 describes the life cycle of a single PIE.
Once allocated, the entry starts at the ”Active” state. The
ﬂakiness detector tracks recurrence and miss rate for each
of the active loads. Once a PIE has been qualiﬁed as ﬂaky
(above-threshold miss rate) and hot (high recurrence over a
time window) its state switches to ”Gen” and it is assigned a
walker to construct its slice of code (a PIE slice).

B. PIE slice generation

The slice that generates the load address consists of a subset
of the code path prior to that load. The prefetcher tracks the
program code ﬂow at retirement using a cyclic history queue,
although in modern processors this can be replaced with
existing debug features such as Intel’s Real-Time Instruction
Tracing (RTIT) [20]. Once a PIE is switched to ”Gen” state
and needs to construct a slice it is assigned one of the free
walker ﬁnite-state-machines (FSMs). The walker traverses the
history queue from the youngest instruction (the ﬂaky load
itself) to the oldest and constructs the PIE slice.

To track data dependency,

the walker uses 1) a source
bitmap which assigns one bit per register to track the ac-
tive sources (only general-purpose and ﬂags registers); 2) a
renaming cache that tracks memory operations for potential
memory renaming [37]; 3) a Temporary register map that
tracks architectural registers replaced with temporary ones.
The walker also has storage for 16 operations that serves as the
local copy of the slice during construction. Finally, the walker
has an index pointing to the history queue (for the traversal),
an index for the local slice (for construction), and a counter
of temporary registers used for memory renaming.

The walker ﬁrst sets the bits representing the load sources,
and then traverses the history queue backwards (from youngest
to oldest instruction). On each instruction that writes to a
register in the bitmap, the walker does the following:

• Pushes the instruction to its local code slice (using an
index that starts from the last entry and going backwards
from the end of the slice).

• Clears the destination register from the sources bitmap

marking that its producer has been added.

• Sets all the registers corresponding with the current in-
struction sources. This ensures older operations producing
these sources will also be added.

• Records the destination value. This will be checked for

constants or strides during the next phases.

Loads that are added to the slice record their address and
their index within the local slice in the rename cache. This
structure can host 16 addresses in the form of a set-associative
cache. The walker then performs memory renaming whenever
an older store is observed (further along the walk) that matches
an adderess in the rename cache. The renaming is done by
extracting the index of the matching load from the structure
and replacing both the store and the load operations in the slice
with a move to and from (respectively) an available temporary
register. It should be noted that reducing the store/load pair
further by moving the store data directly to the load destination
is not possible, since the load destination register may be
reused between the store and the load (and therefore override
the data).

The walker completes the traversal upon 1) reaching the tail
of the cyclic history queue; 2) when there are no longer valid
sources marked in the source bitmap; or 3) when the loop
completes a round-trip and the same load within the same
BHR context is encountered. Upon successful completion, the
walker switches the PIE to ”Validate” phase. When the same
load context is encountered again, the prefetcher assigns a
walker to perform the walk once more to validate that the code
slice did not change. The PIE remains in validation phase for
several encounters to ensure the code is stable and to identify
constant/strided values (the strides themselves may be caused
by code beyond the scope of the slice).

The prefetcher performs three validation rounds. Other
values (up to seven) were tested, indicating that prime numbers
work better, especially when no BHR context is used, as they
may avoid some loop patterns from confusing the validation
process. However, a lower value was chosen as overall perfor-
mance beneﬁts more from the speed of generating new slices
than from the additional accuracy that may accompany longer
validation.

After ﬁnishing all validation rounds the entry is switched to
the ”Trim” phase. The ”Trim” phase is the only step allowed
to change the PIE slice since it was ﬁrst generated. It performs
the same walk, but stops tracking sources when reaching
constants or strides that were discovered during the validation
passes and replaces them with a simple immediate move or
add/sub. As a result, some branches of the data dependency
ﬂow may be removed from the PIE slice.

Another change performed during trimming is renaming the
destinations to temporary registers to avoid any side effects.
The walker performs a forward traversal over the constructed
slice and converts each destination register to the next available

5

The slice operations are then injected in order, with no linger-
ing side effects as the temporary registers used are not part
of the architectural state. Any memory operation is allowed
to lookup the TLBs and caches and, if needed, perform page
walks and allocate line ﬁll buffers. These accesses may, by
themselves, act as prefetches.

The injected operations may be executed by the normal
machine out-of-order resources. However, this may incur a
substantial cost to the actual program performance due to
added stress over critical execution resources. Instead, an
internal execution engine was added to perform the arithmetic
operations without interfering with the normal core activity
(other than stalling allocation). We evaluate both the shared-
resources and the private-resources modes in Section V.

During the injection, sources marked as constants use the
recorded constant value, but operations marked as having a
stride are adjusted by having their stride value multiplied
by a dynamic lookahead factor. The dynamic lookahead is
initialized for each slice to one, meaning that by default
the prefetcher injects the PIE slice as-observed, with no
extrapolation (thereby performing the address computation of
the next iteration). However, if the PIE is eventually detected
as non-timely (as explained in the next section) the lookahead
factor will increase gradually up to a maximum of 64 (chosen
to allow prefetching far enough ahead of time, but not too far
as to exceed the cache lifetime). All strides within a slice are
always multiplied by the same lookahead factor so that the
ratios between strides are always kept as they were detected
over a single iteration.

The ﬁnal operation in the slice is a copy of the original
load that the slice was constructed from, but since any strided
sources were enhanced to apply a lookahead over their strides,
the load address would belong to some future iteration. This
becomes the ﬁnal prefetch address and is sent to the memory
unit as a prefetch. In parallel, it is also pushed to the prefetch
queue along with its predicting PIE-id for usefulness tracking.

D. Usefulness tracking

The generated prefetches must be correct and timely (i.e.,
the address should be used later by a demand, and do so
within a sufﬁciently short time period as to avoid being ﬂushed
from the cache). We solve both requirements by tracking
the prefetches in the prefetch queue. Each demand address
is checked against the queue to ﬁnd the ﬁrst (most recent)
matching prefetch, and the entry is marked as hit. If the hit is
within useful distance (determined by a reward function as in
the context-RL prefetcher [27]), the PIE receives conﬁdence
upgrade based on the reward score. On the other hand, if a
prefetch entry reaches the end of the prefetch queue without
ever being hit, it is considered useless. The number of sent
and useless prefetches is tracked in the PIE (the counters are
both right-shifted whenever they are about to exceed in order
to preserve their ratio). When a PIE goes below the usefulness
threshold (we used 10% in our experiments), it is reset, but
allowed to regenerate the slice in case the current code ﬂow
changed compared to when it was originally constructed.

Fig. 8. Slice generation example. The history queue holds all instructions
in the dynamic ﬂow and is walked backwards from the triggering load.
The sources bitmap is shown on the right during the walk (steps 1 through
5). After the walk we receive an intermediate PIE slice (6) including only
the dependency chain (R9’s multiply was dropped). We then populate the
stride/const values during the validation steps (7). The ﬁnal PIE slice shows the
post-TRIM slice (8), in which RDX was discovered as constant and allowed
eliminating the load fetching it.

temp register. The conversions are recorded in the temporary
register map. During the following traversal steps, all younger
slice instruction will rename any matching sources to read
from the corresponding temporary register. After trimming is
done, the entry is switched to ”Armed” state.

We assume that

the walker FSM can handle up to 8
instructions per cycle without exceeding timing restrictions
(based on similar existing mechanisms like branch recovery
walks), so the full history walk should take up to 16 cycles.
However, to ensure feasibility and allow larger history queue
sizes, our evaluation assumes that a walk may take up to 64
cycles. Since the prefetcher may encounter additional loads
during that time, it may use several parallel walkers, assigned
to generation or validation phases based on availability.

Figure 8 shows an example of slice generation over code
striding across an array that requires double dereference (since,
for e.g., it was passed by pointer). The dependency chain is
discovered by walking the history queue as shown on the
right hand side (removing the unrelated multiply operation
but identifying all other operations as part of the dependency
chain). The intermediate slice remains consistent during sev-
eral validation phase iterations. During that phase the add
operation is detected as a stride of one and the two middle
loads are identiﬁed as constants. Since RBX is constant, the
Trim phase replaces it with a move and stops processing its
dependencies (thereby also eliminating the RDX load). The
ﬁnal slice is therefore only three operations long.

C. Slice injection

Once a slice has been armed, each encounter with its load
context (i.e., hitting the same IP while having the same branch
history) triggers the PIE slice injection. The allocation stops
immediately prior to the triggering load (thus preserving the
same register roles and meaning as seen during construction).

6

If a PIE is reset more than 25 times,
is considered a
stale PIE, and its state becomes Disabled, preventing it from
reconstructing.

it

Another form of ﬁltering is tracking recurring addresses.
The last address emitted by each slice is saved, and if the
slice generates it again multiple times in a row, the slice is
reset due to low usefulness.

E. Dropping PIE slices

Multiple issues could stop a slice construction process or
reset an already constructed one. Construction can be aborted
due to the following reasons:

• Slice is inconsistent during validation. This may indicate
insufﬁcient context length, the code having no useful
recurrence, or a complex control ﬂow.

• Timeout while waiting for another validation pass, may

indicate the load is not as hot as predicted.

• Slice is too long (over 16 operations).
• Complex instruction (for e.g., non-trivial side effects).
• Too many temporary registers (over 8) are needed.

Resets during slice construction are considered transient,
meaning that a later attempt may still construct a useful
slice. Conversely, slices can also be reset during run-time. If
too many prefetches fall off the prefetch queue without ever
being hit by demands, the slice may have failed capturing the
semantic relation. The minimal usefulness ratio is conﬁgurable
and by default a threshold of 10% is used. The failure rate is
tracked using 2 counters: Failures and sent-prefetches. Both
counters saturate at 64, and both shift right by 1 whenever any
of them reaches that limit. If, after the counters reach steady-
state,
the ratio between them drops below the usefulness
threshold, the prefetcher resets the entry.

Alternatively, If the same code slice produces the exact
same address over and over, the slice is no longer consid-
ered meaningful. This may occur when reaching the history
limit during construction, when the walker cannot include a
source being changed. Aborting a slice at run-time provides
information that triggering a prefetch on the initiating context
might harm performance. Therefore, the PIE array records
these resets and keeps them (unless the PIE is overridden
by another load context). If too many run-time resets occur,
the PIE switches to disabled state and no longer accepts re-
construction attempts for that context.

F. Prefetcher area

The parameters used for the prefetcher are summarized in
Table I. For the sake of this paper each stored micro-operation
is assumed to be represented with 64 bits including all data
required for reproduction. The history queue entry also has
to store the result 64-bit value (for const/stride detection),
and therefore requires 128 bits. A 128-entries history queue
requires 2kB.

Each PIE slice requires 16 operations, a context tag for
indexing, a walker ID (used during generation and validation)
and additional bits for state and reset tracking. Overall size is
140 Bytes. Since the PIEs are relatively large, the PIE array

History queue
BHR size
Mem. renaming cache
Walkers
PIE array size
Total size (kB)
hot/ﬂaky thresholds

128 instructions, 2kB
24 bit (4b × 6 last branches)
16 × (64 + 4) bits = 1kB
4 × 280B = 0.6kB
16, 2.25kB
6kB
2 appearances / 1 miss

TABLE I
PREFETCHER PARAMETERS

Core type
Queue sizes

MSHRs
L1 cache
L2 cache
L3 cache
Memory

GHB (all) [25]

SMS [36]

VLDP [34]
Context RL [27]

OoO, 4-wide fetch, 3.2Ghz
224 ROB, 97 RS, 180/168 int/FP regs,
72 load buffer, 56 store buffer
(estimated) 10 L1, 20 L2
32kB Data + 32kB Code, 8 way, 2 cycles
256kB, 4 ways
4MB, 16 ways x 2 slices
LPDDR3, 2-channel, 8GB, 1600 Mhz
Competing prefetchers
GHB size: 2K, History length: 3
Prefetch degree: 3, Overall size: 32kB
PHT size: 2K, AGT size: 32, Filter: 32
Regions size: 2kB, Overall size: 20kB
3 DPTs × 64 entries
CST size: 2K entries x 4 links (18kB),
Reducer: 16K entries (12kB)

TABLE II
SIMULATOR PARAMETERS BASED ON SKYLAKE

holds only 16 entries, with a total size of ∼2.25kB. This is
sufﬁcient for most of the applications since the number of
slices presented in Figure 4 refers to the entire lifetime of the
application, but at any given program phase only a few slices
are actively used. This is demonstrated later in Section V-C.
Future work may ﬁnd ways to reduce the size of each entry (for
example by compressing identical operations, as some slices
may share parts of their history). The storage size is therefore
not a fundamental issue.

The slice generation FSM (walker) requires a source bitmap
(32 bits), a memory renaming cache (16 entries with a 64b
tag + 4b index each = 136 bytes), and a temporary registers
map (40 bits). Each FSM also has a slice storage for the
construction process, reaching a total of ∼280B. Having 2
parallel walkers would therefore require ∼0.6kB.
Power consideration are reviewed in section V.

IV. METHODOLOGY

The semantic prefetcher was implemented in a proprietary
cycle-accurate x86 simulator conﬁgured to match the Skylake
micro-architecture [21] and validated against real hardware
over a wide range of applications (showing an average error
margin within 2%). Table II speciﬁes the parameters used. All
prefetchers support L1 triggering and virtual addresses and can
use TLBs/page walks when needed.

The prefetcher was tested over the SPEC 2006 and 2017
benchmark suites, compiled on Linux with ICC 14 and 16
(respectively). Each application had 5-8 different traces chosen
by a SimPoint-equivalent tool based on workload characteri-
zation. The traces are weighted to represent overall application
performance while measuring 5M instructions each (following

7

Fig. 11. Breakdown of reset causes.

Fig. 9. Slice coverage over SPEC-2006/2017. The left Y-axis measures the
normalized portion of each outcome of slice construction. The right Y-axis
(and overlaid line) show the absolute count of successfully armed slices.

Fig. 10. Number of slices failing validation per each context length. Longer
context contributes to the consistency of slices (since iso-context slices are
more likely to share the same locality behavior)

a warmup of about ∼1B memory accesses and ∼20-50M
actual instructions).

To test multithreaded performance we also run combina-
tions of workloads on different threads, although we do not
implement the ability to share the learnings across threads. For
that purpose, we use combinations of traces from the same
applications to measure SPEC-rate behavior (where several
copies of the same application are being run independently).
We run the traces with separate physical memory address
ranges to avoid data collisions. We also offset the run phases
by a few million instructions to ensure some heterogeneity.

V. EVALUATION

To evaluate the beneﬁts of the semantic prefetcher, we ﬁrst
need to determine its ability to cover enough performance-
critical
loads within common workloads. Figure 9 shows
the coverage of different SPEC 2006/2017 workloads: each

Fig. 12. Semantic prefetcher speedup vs competition. Showing workloads
with any prefetcher gaining over 7% (average is over full SPEC Int/FP).

application shows the number of dynamic loads analyzed
by the semantic prefetcher, and the break-down by analysis
outcome. The Non-Flaky component counts loads not deemed
hot enough or not having enough misses to be interesting. The
Timing component shows slices failing during the validation
period due to low rate of recurrence or hash conﬂicts. The
Non-Stable component shows slices failing validation due to
variability of code path (results are shown with zero context,
increasing the context is shown later to reduce this component
as the code paths are more consistent when compared across
recurrences with the same branch history). The remaining
component at the top shows the number of armed slices per
workload.

The absolute number of slice validation failures is shown
in Figure 10 for the 7 SPEC workloads that have the highest

8

Fig. 13. Ratio of injected operations out of the overall, compared to speedup.
Showing applications with speedup ≥ 15%.

failure rate when using no context (all having over 40% of
their slices reset during slice generation). The number of
failures is compared having between 0 to 24 context bits (i.e.,
indexing loads based on the history of the last 0 to 6 branches
and their resolutions). Adding the full context length reduced
between 30% (gcc) to 98% (milc) of the failures, indicating
that recurrences with the same branch history are more likely
to have consistent code slice behavior.

The overall breakdown of reset causes appears in Figure 11.
The ﬁrst element
is failures due to hash collisions: new
dynamic loads matching the PIE index of an existing slice
that is under construction, causing it to drop (armed slices
are protected from overwrite and can only be reset due to
is variance in code
low usefulness). The second element
ﬂow during slice generation. The third is timeout during the
construction: a slice that was not armed within 100k cycles is
reset due to low relevance. The last reset cause, Too-Many-
Failures, is a run-time reset cause, occurring after a slice was
validated and armed, as explained in Section III-D.

It should be noted that the various reset thresholds and
parameters have shown very little sensitivity to tuning. This
happens because most slice stabilization and resets occur
during warmup and are therefore negligible on longer runs. On
the other hand, ﬂakiness and usefulness parameters are more
sensitive to tuning, and show better performance the more
aggressive they are dialed (i.e., building and maintaining slices
for more loads). However, optimizing with a higher cost of
slice injection (especially without dedicated execution) would
likely lead to more conservative thresholds.

The speedup of the semantic prefetcher (with 0 and 16
bits context) is shown in Figure 12 across SPEC 2006/2017
benchmarks. Several competing prefetchers are also shown.
The overall gain is signiﬁcantly higher on SPEC 2006 (24.5%),
mostly due to the lack of software prefetching by the older
compiler, but the improvement exists also on SPEC 2017.

Slice injection also adds computation work. Figure 13
shows the injected instructions out of the overall instructions
executed, compared with the performance gain. In most cases
there is good correlation (i.e., the performance gain is propor-
tional to the added work), but some applications with relatively
simple slices are able to gain signiﬁcantly more than their

Fig. 14. Semantic prefetcher speedup over MT SPEC06 workload combina-
tions (4 cores).

overhead. If we assume the prefetcher’s steady-state power
cost (disregarding slice generation) is equivalent to the added
operations, then the power/performance score has on average
2.5× more IPC gain than power cost.

Finally, the semantic prefetcher improves performance also
on multi-threaded runs. Figure 14 shows the speedup over
SPEC-rate simulation (4 traces from the same application over
4 physical cores). In some cases prefetching provides a higher
gain than on a single thread (in h264, for example). There
is no sharing of generated slices across physical cores, so
the only gain comes from increasing the effective prefetching
depth. On MT runs the system becomes more saturated and
memory bandwidth usually becomes a more critical bottleneck
compared to memory latency. This may reduce the efﬁciency
of prefetching (or even the chances of issuing the prefetches).
On the other hand, prefetches may also serve as cache hints
that increase the lifetime of required lines, thereby reducing
overall bandwidth and improving MT performance. On the
highest MT gainer (libquantum), the prefetcher reduced L1
MPKI by 40%.

A. Comparison with other prefetchers

We compare the speedup of the semantic prefetcher with
other prefetcher with different approaches and coverage. The
semantic prefetcher wins over most SPEC workloads, scor-
ing on average more than twice the speedup of the next
best prefetcher. However, on some workloads the semantic
prefetcher loses to one of the competing prefetchers. In
gemsFDTD, a simple stride prefetcer is able to gain almost
60% speedup while the semantic prefetcher gains only ∼17%
at best. The reason for that is short nested loops, where the
inner recurrence is too long to ﬁt in the context history length,
but too short to allow the chance for re-learning the inner loop
on every outer iteration. This control ﬂow gives an advantage
to simple and fast prefetchers that need only a few loop
recurrences to learn and trigger (the stride prefetcher can start
issuing prefetches after the 3rd inner iteration). The semantic
prefetcher can still
learn the code pattern given sufﬁcient
context, and in fact begins to gain performance by covering at
least some of the cases with a context of 32 bits and above,
but such context length begins to stress the physical design
and does not solve the general case where inner loops can be
much longer. This can be solved by having the context support
compressed representation of loop patterns.

9

Another competing prefetcher that gains over the semantic
prefetcher over some workloads is the Best-Offset prefetcher.
BOP has several outliers in its favor, most notably zeusMP.
Unlike GHB and other fast-learning prefetches, BOP also takes
a while to discover the optimal depth, but once it does, it has a
throttling effect where it eliminates unnecessary prefetches (at
sub-optimal offsets). The gains in zeusMP (and to a lesser
extent also in dealII, sphinx3 and cactusADM) are mostly
through reduction of BW from excessive prefetching. For
the same reason adding context to the semantic prefetcher
also helps on zeusMP by eliminating badly constructed slices
emitting some useless prefetches.

Finally, IMP presents an interesting point: within the SPEC
applications it wins only on WRF (and only by 8%), but the
graph500 example had array dereferences that make IMP quite
useful, except on the longest ones.

B. Lookahead method

The lookahead multiplier (the number of strides we prefetch
ahead) plays a key role in the prefetcher speedup. However,
when applying a ﬁxed multipliers we noticed that different
workloads were favoring different values, and optimizing the
best method required dynamic tuning. We implemented the
following approaches:

• Constant lookahead: we set a constant value and always

perform the lookahead according to it.

• Hit-depth normalization: we measure the average depth
of hits within the prefetch queue which indicate the actual
distance between a prefetch and the demands access using
it. We then increase or decrease the lookahead value to
normalize this hit depth to the desired range (if we hit
too early, around the beginning of the prefetch queue, we
need to extend our lookahead and vice versa).

The difference between the policies is shown in Figure 15.
Lookahead 1 and 16 are dynamically adjusting the lookahead
distance while starting from a multiplier of 1 or 16 iterations
ahead (respectively) and increasing from there. The ﬁxed
lookahead policy (always 32 iterations ahead) has some minor
gains in cases where it starts from a more effective prefetching
depth while the dynamic policy takes a while to reach there,
but it is ultimately inferior on most runs where the dynamic
approach is more adaptable.

C. Execution resources

The semantic prefetcher uses a dedicated generic ALU as
the execution engine of the PIE slices. However, if the slice
execution latency becomes a critical factor, the area addition
is too high, and the overall execution bandwidth is sufﬁcient,
we may choose to simply inject the slice code into the main
out-of-order engine and let the existing resources do the work
for us. Figure 16 shows the penalty of dropping the dedicated
HW and sharing the core execution resources.

Another tradeoff is the number of walkers performing the
slice generation and validation. Figure 17 shows how many
parallel walks (traversals over the history queue) and PIE
entries (slices tracked for constructions) are needed using

Fig. 15. Gain with different lookahead policies.

Fig. 16. Using existing vs. dedicated ALU.

cumulative time histograms. Overall cycles with any number
of walks consume only 0.5% of the run-time, (also indicating
that the power consumption of the walk itself is negligible).
The results indicate that two walkers are sufﬁcient. In the same
way, 16 PIE entries are enough to cover 99.4% of the run.

VI. RELATED WORK

A. Program semantics

Multiple researches attempt to automate the analysis and
understanding of software applications. Shape analysis [33]
attempts to build a set of properties and rules representing
the program’s data sets in order to facilitate program veriﬁ-
cation (mostly of memory object allocation and management,
bounds/coherence checking and functional correctness).

Newer approaches attempt to represent programs in ab-
stract forms derived from their code and behavioral analy-
sis [5], [15], in order to ﬁnd similarities for code sugges-
tion/completion, anti-plagiarism, or algorithm identiﬁcation.
These approaches may be useful in high level dynamic tuning
(adjusting HW properties such as the type of prefetcher used,

Fig. 17. Amount of parallel PIEs and active walks required per cycle.

10

or optimal aggressiveness), but they do not yet assist in the
analysis of the access pattern or address generation.

B. Using code slices

Collecting and using slices of actual code has already been
proposed for various purposes. Trace cache [29] is a form
of constructing and efﬁciently caching selective code paths
based on run-time analysis. In the realm of branch prediction,
Zilles et al. [41] proposed using code slices to execute ahead
the predicted code path to resolve hard-to-predict branches. A
similar approach suggested by Peled et al. [26] is based on
injecting code slices to resolve data-dependent branches by
prefetching the data, using it to resolve the branch condition,
and queuing the resolution for overriding the prediction.

This method was also proposed for memory latency mit-
igation. Carlson et al. [6] proposed a similar mechanism
that dynamically learns load dependency chains in order to
expedite their execution. However, their approach was based
on in-order cores, and motivated to extract a small portion of
the ILP available to full out-of-order cores by execution only
load address dependencies out of order.

Prefetching can also be achieved by executing actual code
(or even just critical subsets of it) ahead of time as proposed
by Mutlu and Hashemi et al. in their set of Runahead tech-
niques [13], [14], [24], and by Collins et al. in their speculative
precomputation technique [10] (extended by Atta et al. [1]
to include also limited control ﬂow). That work relied on
continued execution of the same program context past memory
stalls, and focused on managing a complicated speculative
execution state for that purpose. It did not modify the executed
code but most approaches did ﬁlter out code not required for
memory address calculation. It also did no extrapolation, and
thus was limited in range to what could ﬁt in the enhanced
out-of-order window.

Prefetching based on actual code slices can also be done
by helper threads (Runahead ﬂavor by Xekalakis et al. [39]
and slice-based processors by Moshovos et al. [23]). Similar
decoupled approaches were also used for branch prediction
by Chappell et al. [7], [8] and Farcy et al. [12]. However, this
form is asynchronous with the progress of the main thread and
will not be able to ensure ﬁxed (or even positive) prefetching
distance.

Another form of expedited execution through other threads
is Hardware scouting [9], which is intended for highly
multithreaded machines and uses other threads to run ahead
of execution. However this approach attempts to optimize MT
throughput, and not address single-threaded performance.

C. Prefetching techniques

Falsaﬁ and Wenisch classiﬁed prefetching techniques into

the following groups [11]:

• Stream/stride

prefetchers utilize

locality,
stride patterns. The sandbox
by detecting constant
prefetcher [30], the best-offset prefetcher (BOP) [22], and
Access Map Pattern Matching (AMPM) [16], proposed
various methods of testing different strides and choosing

spatial

the optimal one, thereby covering complex ﬂows through
common recurrence deltas. Other prefetchers such as the
variable length delta prefetcher (VLDP) [34] enhanced
that ability to varying stride patterns.

• Address-correlating

detect

prefetchers
correlation
within sequences of recurring accesses. This form of lo-
cality has the ability to cover some semantic relations, but
is ultimately limited to the storage capacity of correlated
addresses. Examples include the Markov predictor [18],
the Global History Buffer Address-Correlation ﬂavors
[25], and prefetchers targeting linked data
(GHB/AC)
structures through partial memoization, such as that by
Roth, Moshovos and Sohi [31], [32], and Bekerman et
al. [4]. An extension of address-correlation is context-
correlation [27], [28] which seeks to correlate a larger
context vector with future addresses.

• Spatially-correlated prefetchers use an extension of
temporal locality that correlates between spatial patterns
instead of absolute addresses. These prefetchers seek out
recurring spatial patterns that are not part of a long
consecutive sequence but may repeat locally, such as
accesses to the same ﬁelds of a structure across different
instances. Examples of this family are Spatial Memory
Streaming (SMS) [36] and the DC ﬂavors of GHB [25].
• Irregular data patterns prefechers target speciﬁc data
structured that do not have spatio-temporal locality. IMP
[40] prefetches future elements within an array of indexes
(A[B[i]]). Other data-driven prefetchers include the Irreg-
ular Stream Buffer (ISB)
[17], which restructures the
dataset spatially. Another form of irregular prefetching
based on context if B-Fetch [19] by Kadjo et al. which
uses branch history to detect strides in registers used for
address generation. However, since it does not execute
actual code it is limited to simple register strides and
cannot reconstruct complex value manipulations or see
through memory dereferences.

VII. CONCLUSIONS

This paper presents the semantic prefetcher. The prefetcer
is designed to utilize the most generalized form of locality:
semantic locality, which is not limited to spatial or temporal
artifacts of the memory address sequence, but instead attempts
to extract the code slices responsible for invoking consequen-
tial memory accesses along the data structure traversal path or
algorithmic ﬂow. We then combine and manipulate these slices
into forecast slices by utilizing locality artifacts within their
code to create new functionality that can predict the memory
behavior of future iterations.

While some existing prefetchers attempt to capture access
patterns that belong to speciﬁc use-cases, whether spatio-
temporal relations, temporal correlations, or even irregular
and data-dependent patterns, there is currently no general-
ized technique that can capture all such cases. The semantic
prefetcher attempts to solve that by observing the code path
directly, imitating any program functionality and extending it
to create lookahead functionality. Based on that technique, the

11

semantic prefetcher can extend the coverage provided by ex-
isting prefetchers (including irregular ones) through inherently
supporting complex address generation ﬂows and multiple
memory dereferences. The semantic prefetcher provides a
speedup of 24.5% over SPEC-2006 and 16% over SPEC-2017,
exceeding gains from other prefetchers by over 2×.

REFERENCES

[1] I. Atta, X. Tong, V. Srinivasan, I. Baldini, and A. Moshovos, “Self-
contained, accurate precomputation prefetching,” in Proceedings of the
48th International Symposium on Microarchitecture, ser. MICRO-48.
New York, NY, USA: ACM, 2015, pp. 153–165. [Online]. Available:
http://doi.acm.org/10.1145/2830772.2830816

[2] J.-L. Baer and T.-F. Chen, “An effective on-chip preloading scheme to
reduce data access penalty,” in Supercomputing’91: Proceedings of the
1991 ACM/IEEE Conference on Supercomputing.
IEEE, 1991, pp.
176–186.

[3] M. Bakhshalipour, P. Lotﬁ-Kamran, and H. Sarbazi-Azad, “Domino
temporal data prefetcher,” in Symp. on High-Performance Computer
Architecture (HPCA), Feb 2018, pp. 131–142.

[4] M. Bekerman, S. Jourdan, R. Ronen, G. Kirshenboim, L. Rappoport,
A. Yoaz, and U. Weiser, “Correlated load-address predictors,” in Intl.
Symp. on Computer Architecture (ISCA), May 1999.

[5] T. Ben-Nun, A. S.

Jakobovits, and T. Hoeﬂer, “Neural code
comprehension: A learnable representation of code semantics,” CoRR,
vol. abs/1806.07336, 2018. [Online]. Available: http://arxiv.org/abs/
1806.07336

[6] T. E. Carlson, W. Heirman, O. Allam, S. Kaxiras, and L. Eeckhout,
“The load slice core microarchitecture,” in Intl. Symp. on Computer
Architecture (ISCA), June 2015, pp. 272–284.

[7] R. S. Chappell, F. Tseng, A. Yoaz, and Y. N. Patt, “Difﬁcult-path branch
prediction using subordinate microthreads,” in Proceedings 29th Annual
International Symposium on Computer Architecture, May 2002, pp. 307–
317.

[8] R. S. Chappell, J. Stark, S. P. Kim, S. K. Reinhardt, and Y. N. Patt,
“Simultaneous subordinate microthreading (ssmt),” in Proceedings of
the 26th International Symposium on Computer Architecture (Cat. No.
99CB36367).

IEEE, 1999, pp. 186–195.

[9] S. Chaudhry, P. Caprioli, S. Yip, and M. Tremblay, “High-performance
throughput computing,” IEEE Micro, vol. 25, no. 3, pp. 32–45, May
2005.

[10] J. D. Collins, H. Wang, D. M. Tullsen, C. Hughes, Y.-F. Lee, D. Lavery,
and J. P. Shen, “Speculative precomputation: Long-range prefetching of
delinquent loads,” in Proceedings 28th Annual International Symposium
on Computer Architecture.

IEEE, 2001, pp. 14–25.

[11] B. Falsaﬁ and T. F. Wenisch, “A primer on hardware prefetching,”
Synthesis Lectures on Computer Architecture, vol. 9, no. 1, 2014.
[12] A. Farcy, O. Temam, R. Espasa, and T. Juan, “Dataﬂow analysis
of branch mispredictions and its application to early resolution of
branch outcomes,” in Proceedings. 31st Annual ACM/IEEE International
Symposium on Microarchitecture.

IEEE, 1998, pp. 59–68.

[13] M. Hashemi, O. Mutlu,

and Y. N.

“Continuous
runahead: Transparent hardware acceleration for memory intensive
IEEE/ACM International
workloads,”
Symposium on Microarchitecture, ser. MICRO-49.
Piscataway, NJ,
[Online]. Available:
USA:
http://dl.acm.org/citation.cfm?id=3195638.3195712

IEEE Press, 2016, pp. 61:1–61:12.

Annual

Patt,

49th

The

in

[14] M. Hashemi and Y. N. Patt, “Filtered runahead execution with a
runahead buffer,” in Proceedings of the 48th International Symposium
on Microarchitecture, 2015, pp. 358–369.

[15] J. Henkel, S. K. Lahiri, B. Liblit, and T. W. Reps, “Code vectors:
understanding programs through embedded abstracted symbolic traces,”
in ESEC/SIGSOFT FSE, 2018.

[16] Y. Ishii, M. Inaba, and K. Hiraki, “Access map pattern matching for

data cache prefetch,” Jun 2009.

[17] A. Jain and C. Lin, “Linearizing irregular memory accesses for improved
correlated prefetching,” in Intl. Symp. on Microarchitecture (MICRO),
Dec 2013.

[18] D. Joseph and D. Grunwald, “Prefetching using markov predictors,” in

Intl. Symp. on Computer Architecture (ISCA), Jun 1997.

12

[19] D. Kadjo, J. Kim, P. Sharma, R. Panda, P. Gratz, and D. Jimenez, “B-
fetch: Branch prediction directed prefetching for chip-multiprocessors,”
in 2014 47th Annual IEEE/ACM International Symposium on Microar-
chitecture, Dec 2014, pp. 623–634.

[20] T. Kurts, O. Levy, I. Kazachinsky, G. Malka, Z. Sperber, and J. W.
Brandt, “Real time instruction trace processors, methods, and systems,”
Patent, Feb. 16, 2016, uS Patent 9,262,163.

[21] J. Mandelblat, “Technology insight: Intel’s next generation microarchi-
tecture code name skylake,” in Intel Developer Forum, San Francisco,
2015.

[22] P. Michaud, “Best-offset hardware prefetching,” in Symp. on High-
Performance Computer Architecture (HPCA), March 2016, pp. 469–480.
[Online]. Available: doi.ieeecomputersociety.org/10.1109/HPCA.2016.
7446087

[23] A. Moshovos, D. N. Pnevmatikatos, and A. Baniasadi, “Slice-
processors: An implementation of operation-based prediction,” in
Proceedings of the 15th International Conference on Supercomputing,
ser. ICS ’01. New York, NY, USA: ACM, 2001, pp. 321–334.
[Online]. Available: http://doi.acm.org/10.1145/377792.377856

[24] O. Mutlu, J. Stark, C. Wilkerson, and Y. N. Patt, “Runahead execution:
an alternative to very large instruction windows for out-of-order pro-
cessors,” in The Ninth International Symposium on High-Performance
Computer Architecture, 2003. HPCA-9 2003. Proceedings., Feb 2003,
pp. 129–140.

[25] K. J. Nesbit and J. E. Smith, “Data cache prefetching using a global
history buffer,” in Symp. on High-Performance Computer Architecture
(HPCA), Feb 2004.

[26] L. Peled, G. Haber, and Y. Sazeides, “Branch predictor with branch

resolution code injection,” Jun. 21 2018, uS Patent App. 15/385,011.

[27] L. Peled, S. Mannor, U. Weiser, and Y. Etsion, “Semantic locality and
context-based prefetching using reinforcement learning,” in Intl. Symp.
on Computer Architecture (ISCA), Jun 2015.

[28] L. Peled, U. Weiser, and Y. Etsion, “A neural network prefetcher
for arbitrary memory access patterns,” ACM Trans. Archit. Code
Optim., vol. 16, no. 4, pp. 37:1–37:27, Oct. 2019. [Online]. Available:
http://doi.acm.org/10.1145/3345000

[29] A. Peleg and U. Weiser, “Dynamic ﬂow instruction cache memory
organized around trace segments independent of virtual address line,”
Jan 1995, uS Patent 5,381,533.

[30] S. H. Pugsley, Z. Chishti, C. Wilkerson, P. f. Chuang, R. L. Scott,
A. Jaleel, S. L. Lu, K. Chow, and R. Balasubramonian, “Sandbox
prefetching: Safe run-time evaluation of aggressive prefetchers,” in
Symp. on High-Performance Computer Architecture (HPCA), Feb 2014.
[31] A. Roth, A. Moshovos, and G. S. Sohi, “Dependence based prefetching
for linked data structures,” in Intl. Conf. on Arch. Support for Program-
ming Languages & Operating Systems (ASPLOS), Oct 1998.

[32] A. Roth and G. S. Sohi, “Effective jump-pointer prefetching for linked
data structures,” in Intl. Symp. on Computer Architecture (ISCA), May
1999.

[33] M. Sagiv, T. Reps, and R. Wilhelm, “Parametric shape analysis via
3-valued logic,” in Proceedings of the 26th ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, ser. POPL ’99.
New York, NY, USA: ACM, 1999, pp. 105–118. [Online]. Available:
http://doi.acm.org/10.1145/292540.292552

[34] M. Shevgoor, S. Koladiya, R. Balasubramonian, C. Wilkerson, S. H.
Pugsley, and Z. Chishti, “Efﬁciently prefetching complex address pat-
terns,” in Intl. Symp. on Microarchitecture (MICRO), Dec 2015.
[35] S. Somogyi, T. F. Wenisch, A. Ailamaki, and B. Falsaﬁ, “Spatio-
temporal memory streaming,” in Intl. Symp. on Computer Architecture
(ISCA), ser. ISCA ’09, 2009, pp. 69–80.

[36] S. Somogyi, T. F. Wenisch, A. Ailamaki, B. Falsaﬁ, and A. Moshovos,
“Spatial memory streaming,” in Intl. Symp. on Computer Architecture
(ISCA), Jun 2006.

[37] G. S. Tyson and T. M. Austin, “Improving the accuracy and
performance of memory communication through renaming,”
in
Proceedings of the 30th Annual ACM/IEEE International Symposium
on Microarchitecture, ser. MICRO 30. Washington, DC, USA:
IEEE Computer Society, 1997, pp. 218–227.
[Online]. Available:
http://dl.acm.org/citation.cfm?id=266800.266821

[38] T. F. Wenisch, M. Ferdman, A. Ailamaki, B. Falsaﬁ, and A. Moshovos,
“Practical off-chip meta-data for temporal memory streaming,” in Symp.
on High-Performance Computer Architecture (HPCA), Feb 2009, pp.
79–90.

[39] P. Xekalakis, N. Ioannou, and M. Cintra, “Combining thread level
speculation helper threads and runahead execution,” in Proceedings of
the 23rd International Conference on Supercomputing, ser. ICS ’09.
New York, NY, USA: ACM, 2009, pp. 410–420. [Online]. Available:
http://doi.acm.org/10.1145/1542275.1542333

[40] X. Yu, C. J. Hughes, N. Satish, and S. Devadas, “Imp: Indirect memory
prefetcher,” in Proceedings of the 48th International Symposium on
Microarchitecture. ACM, 2015, pp. 178–190.

[41] C. Zilles and G. Sohi, “Execution-based prediction using speculative
slices,” in Intl. Symp. on Computer Architecture (ISCA), ser. ISCA
’01. New York, NY, USA: ACM, 2001, pp. 2–13. [Online]. Available:
http://doi.acm.org/10.1145/379240.379246

13

