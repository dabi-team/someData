2
2
0
2

p
e
S
9
1

]
L
D
.
s
c
[

4
v
6
8
0
4
0
.
9
0
1
2
:
v
i
X
r
a

Mapping the Structure and Evolution of Software Testing
Research Over the Past Three Decades

Alireza Salahirada, Gregory Gayb,∗, Ehsan Mohammadic

aDepartment of Computer Science & Engineering, University of South Carolina, USA
bDepartment of Computer Science and Engineering, Chalmers | University of Gothenburg, Sweden
cSchool of Information Science, University of South Carolina, USA

Abstract

Background: The field of software testing is growing and rapidly-evolving.

Aims: Based on keywords assigned to publications, we seek to identify predominant

research topics and understand how they are connected and have evolved.

Method: We apply co-word analysis to map the topology of testing research as a net-

work where author-assigned keywords are connected by edges indicating co-occurrence

in publications. Keywords are clustered based on edge density and frequency of con-

nection. We examine the most popular keywords, summarize clusters into high-level

research topics, examine how topics connect, and examine how the field is changing.

Results: Testing research can be divided into 16 high-level topics and 18 subtopics.

Creation guidance, automated test generation, evolution and maintenance, and test ora-

cles have particularly strong connections to other topics, highlighting their multidisci-

plinary nature. Emerging keywords relate to web and mobile apps, machine learning,

energy consumption, automated program repair and test generation, while emerging

connections have formed between web apps, test oracles, and machine learning with

many topics. Random and requirements-based testing show potential decline.

Conclusions: Our observations, advice, and map data offer a deeper understanding of

the field and inspiration regarding challenges and connections to explore.

Keywords: Software Testing, Bibliometrics, Co-Word Analysis

∗Corresponding author
Email addresses: alireza@email.sc.edu (Alireza Salahirad), greg@greggay.com (Gregory

Gay), ehsan2@sc.edu (Ehsan Mohammadi)

Preprint submitted to Journal of Systems and Software

September 20, 2022

 
 
 
 
 
 
1. Introduction

Software testing refers to the application of input to a system to identify issues

affecting its correctness or its ability to deliver services [1]. While many quality assur-

ance techniques exist, testing remains the primary means of assessing software quality.

From nearly the beginning of software development as a discipline, researchers and

practitioners have reasoned about testing and quality assurance [2]. Today, testing is

one of the largest areas of software engineering research [3], and the field is rapidly

evolving as new software and hardware advances are introduced. It is useful, therefore,

to understand (a) what the predominant research topics are of the field, (b) how those

topics are connected, and (c), how the predominant topics have evolved over time.

“Science of science” describes a research methodology where text, author, and

publication metadata are analyzed using quantitative bibliometric and scientometric

techniques [4, 5]. Computational methods, such as text mining and citation analysis,

map the topical structure of a research field, enabling the discovery of invisible patterns

and relationships in the publications that form that field [6, 7].

We have applied co-word analysis to visualize and analyze the topology of 35 years

of software testing research, based on the author-assigned keywords of Scopus-indexed

publications. Co-word analysis yields an undirected network where the nodes—author-

assigned keywords—represent targeted research concepts. Weighted edges connect

keywords, based on their co-occurrence on publications. Finally, keywords are grouped

into clusters, representing densely-connected regions of the network.

Our analysis maps keywords into dense clusters, from which emerge high-level re-

search topics—themes that characterize each cluster—and makes clear the connections

between keywords and topics within and across clusters. It also characterizes the pe-

riods in which low-level keywords and high-level topics have emerged—identifying

emerging research areas, as well as those where research interest has decreased. The

goal of this study is to provide both current and future researchers with perspectives

about testing field, built on a quantitative base. For researchers, a snapshot of important

disciplinary trends can provide valuable insight into the state of the field, suggest topics

to explore, and identify connections (or lack thereof) between keywords and topics that

2

may reveal new insights. Among others, we have made the following observations:

• Both the most common author-assigned keywords and the keywords that attract

the most citations, on average, tend to relate to automation, test creation and

assessment guidance, assessment of system quality, and cyber-physical systems.

• These keywords can be clustered into 16 topics: automated test generation,

creation guidance, evolution and maintenance, machine learning and predictive

modeling, model-based testing, GUI testing, processes and risk, random testing,

reliability, requirements, system testing, test automation, test case types, test ora-

cles, verification and program analysis, and web application testing. Below these

lie 18 more subtopics.

• Creation guidance, automated test generation, evolution and maintenance, and

test oracles are particularly multidisciplinary topics, with dense connections to

many other topics. Twenty keywords connect topics, reflecting multidisciplinary

concepts, common test activities, and test creation information.

• Emerging research particularly relates to web and mobile applications, ML and

AI—including autonomous vehicles—energy consumption, automated program

repair, or fuzzing and search-based test generation. Web applications require

targeted testing approaches and practices, leading to emerging connections to

many topics. Test oracles are also a rapidly-evolving topic with many emerging

connections. ML has emerging potential to support automation.

• Research related to random and requirements-based testing may be in decline.

We believe that these insights—and the rich underlying networks of keywords—

can inspire both current and future researchers in the field of software testing. We

additionally make our data available so that others may make their own observations

or broaden the horizons of their own research.1

The remainder of this publication is structured as follows. In Section 2, we discuss

background concepts and related work. In Section 3, we explain our methodology.

1A package containing our data is available at https://doi.org/10.5281/zenodo.7091926.

3

Section 4 answers our research questions.

In Section 5, we provide advice on the

use of this data, as well as exploratory analyses related to under-explored and missing

connections. Section 6 details threats to validity. In Section 7, we offer our conclusions.

2. Background and Related Work

2.1. Bibliometrics and Co-Word Analysis

Bibliometric analysis is “the application of mathematical and statistical methods to

books and other means of communication” [8]. Bibliometric studies perform quanti-

tative analysis of publications and associated metadata—e.g., keywords, authors, in-

stitutions, and citations—to identify themes and patterns within a research field [9].

Such analysis is often combined with mapping techniques to visualize hidden struc-

tures in the metadata of a particular field [10]. The most common analysis methods

used include citation-based, co-word (also known as keyword co-occurrence), and co-

authorship analysis [11]. We focus on co-word analysis.

In co-word analysis, natural language processing and text mining techniques are

used to discover the most meaningful noun phrases in a collection of documents and

visualize their meaning in a two-dimensional map [12].

In this map, co-occurring

terms are connected, with “closer” placement resulting from stronger co-occurrence.

Co-word analysis is generally based on the number of research publications where two

keywords are used together to describe the research performed [13]. Because keywords

succinctly capture the context of a publication, co-word analysis is an effective method

of revealing connections between publications [14] and identifying trends in a field [7].

Scholars have previously used co-word analysis to depict the structure of fields

including renewable energy [15], global warming [16], nanoscience and nanotechnol-

ogy [17], human computer interaction [18], and big data [19, 20]. Our study is the first

to apply such techniques to software testing.

2.2. Bibliometrics and Software Engineering

Our study is the first to apply scientometric or bibliometric techniques to the soft-

ware testing field. However, bibliometric techniques have been applied to other aspects

4

Topic

Field

Method

Research Topics

Topic Connections

Keyword Clustering

Keyword Connections

Popular Keywords

Emerging Topics

Declining Topics

Underexplored Con.s

Potential Connections

Popular Papers

Top Authors

Author Location

Pub. Venue

This Study

[21]

[22]

[23, 24, 25, 26]

[27, 28]

[29]

[30]

[31]

[32]

[3]

Testing

All SE All SE

Quan., Qual. Quan.

Quan.

All SE

Quan.

All SE

Sci. SW

SBSE Testing Testing Testing

Quan.

Quan., Qual. Quan.

Qual.

Qual.

Qual.

✔

✔

✔

✔

✔

✔

✔

✔

✔

✖

✖

✖

✖

✔

✖

✖

✖

✖

✖

✖

✖

✖

✖

✖

✔

✖

✖

✖

✖

✖

✖

✖

✖

✖

✖

✖

✖

✔

✔

✖

✖

✖

✖

✖

✖

✖

✖

✖

✖

✔

✔

✖

✖

✖

✖

✖

✖

✖

✖

✖

✖

✖

✔

✔

✖

✔

✖

✖

✖

✖

✖

✖

✖

✖

✖

✔

✔

✔

✖

✖

✖

✖

✖

✖

✖

✖

✖

✔

✔

✖

✔

✖

✖

✖

✖

✖

✔

✖

✖

✖

✖

✖

✖

✔

✔

✖

✖

✖

✖

✔

✖

✖

✖

✖

✖

✖

✖

✔

✖

✖

✖

✖

✔

✖

✖

✖

✖

✖

✖

✖

Table 1: Comparison of our study to other related work, based on the research field, methodologies, and

analyses performed.

of software engineering (SE). In Table 1, we contrast our study to related work. Below,

we further elaborate on the specific studies. In general, we focus on analysis of research

topics and the connections between topics, and do not analyze authorship trends. Our

focus and chosen analysis methods enable a deep characterization of the connections

between topics and low-level publication keywords in software testing.

Garousi and M¨antyl¨a performed a bibliometric analysis of more than 70,000 gen-

eral SE publications, finding that the most popular research topics were web applica-

tions, mobile and cloud computing, industrial case studies, source code, and automated

test generation [21]. Our identified research topics include all of these except source

code—which is subsumed by other topics—and case studies. In our study, case studies

would be categorized based on the problems they address. They also found that a small

number of large countries produce the majority of publications, while small European

countries are proportionally the most active in the field.

Garousi and Fernandes used the same set of publications to assess questions re-

lated to quantity versus the impact of SE research [22]. They broadly found that jour-

nal articles have more impact than conference publications and that publications from

English-speaking researchers have more visibility and impact. Both studies also used

Scopus to gather publications, but had a different focus from our study (all of “soft-

5

ware”, rather than software testing). The studies also differ in their analysis methods.

Rather than co-word analysis, the authors of both studies used citation-based analyses.

Co-word analysis allows examination of the connections between topics.

Karanatsiou et al. targeted SE publications from 2010-2017 for analysis, identify-

ing top institutions and scholars from this period [23]. Wong et al. did the same for

the periods of 2001-2005 [24], 2002-2006 [25], and 2003-2007 and 2004-2008 [26].

Garousi et al. also performed bibliometric analysis, specifically, on the SE research

communities in Canada [27] and Turkey [28]. These studies differ from our own in

their focus on the authors of publications, rather than research topics.

Farhoodi et al. reviewed literature related to scientific software, finding that many

SE techniques are being applied in the field and that there is still a need to explore the

usefulness of specific techniques in this context [29]. Their focus differs in both the

analysis techniques, and in their focus on a specific software domain. In Section 4.4,

we do observe the emergence of testing research related to scientific software.

De Freitas and de Souza performed a bibliometric analysis on the first ten years

of research in search-based software engineering—the use of optimization techniques

to automate tasks [30]. They identified the most cited papers, most prolific authors,

and analyzed the distribution of the SBSE publications among conference proceedings,

journals, and books. They described networks of collaborations and distributions of

publications in various venues and identified the distribution of the number of works

published by authors. Their study differs from ours in its focus on a particular research

domain, as well as its focus on authors and venues over research topics.

2.3. Other Related Work

Purely qualitative analyses of testing research have also been performed. In Table 1,

we contrast our study to those discussed below. None of these studies perform a full

summarization or mapping of the testing field. Instead, they point out research areas

that are emerging or that have had a major impact. The topics they discuss tend to

form a subset of those in our characterization of the field. In addition, our quantitative

analysis methods enable elaborate analyses of the field and the connections between

topics not explored in these studies.

6

Harrold, in 2000, examined past research to identify areas of focus for future re-

search [31]. These areas include improvements in integration testing, use of pre-code

artifacts (e.g., specifications) to plan and implement testing activities, development of

tools for estimating, predicting, and performing testing on evolving systems, and pro-

cess improvements. Many of these predictions are now established topics in our map,

such as black box testing, evolution and maintenance, and processes and risk.

Bertolino provided a summary of testing research in 2007, and identified achieve-

ments in the testing process, reliability testing, protocol testing, test criteria, object-

oriented testing, and component-based testing as major advances [32]. She identified

outstanding challenges related to testing education, testing patterns, cost of testing,

controlled evolution, leveraging users, test input and oracle generation, model-based

testing, and testing of specialized domains, among others. Many of her achievements

and challenges appear in our map as either keywords or full research topics.

Orso and Rothermel assessed research performed in the field between 2000-2014,

asking colleagues what they believed were the most significant contributions and the

greatest challenges and opportunities [3]. The research contributions were categorized

into the areas of automated test generation, testing strategies, regression testing, and

support for empirical publications. The first three of those areas reflect research top-

ics in our map. Challenges identified included better testing of modern, real-world

systems, generation of test oracles, analysis of probabilistic programs, testing non-

functional properties (e.g., performance), testing of specialized domains (e.g., mobile),

and leveraging of the cloud and crowd. Some of these challenges—e.g., mobile and

performance testing—are now research topics in our map.

3. Methodology

Software testing is one of the most popular and fast-growing areas of software en-

gineering research [3]. Although there are many surveys, mapping studies, and system-

atic literature reviews on individual topics, there is a lack of quantitative examination

of the field as a whole—mapping research topics and their connections.

Our primary goal is to provide and analyze a “map” of the field of software testing,

7

based on the many distinct research keywords that form the field and the connections

between these keywords, linked through research publications. Our mapping is based

on a quantitative method, co-word analysis, that places co-occurring phrases—in our

case, author-supplied keywords—in a network. Within this network, keywords appear

as nodes, with weighted edges indicating how often keywords are linked in publica-

tions. Sets of strongly co-occurring keywords form distinct clusters. This network

structure offers a quantitative method to characterize the research field, which can be

used as the basis of both qualitative and quantitative analyses.

Using this map, we examine how keywords are linked into clusters, characterize

clusters using high-level research topics, examine the connections between keywords

within and across clusters, and examine how interest in particular keywords and topics

have changed over time. Specifically, we address the following research questions:

RQ1: What are the most popular individual keywords in software testing, as indicated

by the number of publications or citations?

RQ2: What topics characterize the keywords connected within each cluster in the map?

RQ3: How are keywords and research topics most strongly linked across clusters?

RQ4: What keywords, topics, and connections have emerged or grown in popularity

over the past five years?

RQ5: Which keywords and topics have shown the greatest decline in interest?

We begin, in RQ1, by examining the individual keywords targeted by authors. We

are interested in identifying which keywords have been selected most often, and which

receive the most citations per publication on average. We then move into analyses and

characterization of the connections between keywords.

The goal of RQ2 is to summarize each cluster. Keywords within a single clus-

ter are highly interconnected, providing a basis for identifying research topics that

encapsulate connected keywords. A topic as a keyword or phrase that connects multi-

ple keywords. For example, “automated test generation” is not just a single keyword,

but also a topic that connects other keywords such as “ant colony optimization” and

8

“genetic algorithm” within the same cluster.2 RQ3, then, focuses on the connections

across clusters, and characterizes how keywords and research topics connect.

RQ4 and RQ5 focus on an additional dimension, the average age of publications

associated with each keyword. In RQ4, we identify keywords, topics, and connections

between keywords and topics that have emerged or grown in popularity in the past

five years. In RQ5, we examine keywords and topics with the oldest average date of

publication—those with a potential decline in interest. These emerging and declining

concepts offer insight into how the field is evolving.

To answer these questions, we (1) collected publications related to software testing

(Section 3.1), (2) constructed a map, using co-word analysis, of clusters of connected

keywords (Section 3.2), (3) removed unrelated or redundant topics (Section 3.3), and

(4), analyzed the map and underlying data (Section 3.4).

3.1. Data Collection

To gain an inclusive overview of software testing, we gathered publications from

the Scopus database. Scopus is a comprehensive meta-database, covering many con-

ference and journal publication venues. We retrieved all publications returned for the

search term “software testing” on September 26, 2020. Only publications published in

English were used. This collection included 57,233 publications.

Following a manual cleaning stage (see Section 3.3), 49,802 publications were in-

cluded, including 36,774 conference papers, 11,640 journal articles, and 1,388 other

articles. Figure 1 gives an overview of the number of publications published per year.

Our aim was to capture a representative sample of the field, not all possible articles

on software testing. When we quote specific numbers of publications, these numbers

should not be taken as absolutes, but as the approximate commonality of a topic.

For each study, we gathered the title, author data (names, affiliations, locations),

keywords, publication date, venue metadata (e.g., publisher, venue, volume, page num-

bers), number of citations, DOI, link, and language.

2Both are algorithms often used to generate tests, linking all three keywords as part of the same topic.

9

Figure 1: Number of publications per year retrieved from Scopus.

3.2. Map Construction

To map testing research, we used co-word analysis [12]. Co-word analysis is a nat-

ural language processing method that extracts important phrases from a textual dataset

and identifies their relationships in a network based on the number of times that two

terms co-occur together in all documents. This technique assumes that terms that co-

occur more often are more strongly related to each other. As a result, all identified

terms are classified into clusters using co-occurrence to measure term similarity and

depict the extracted terms and their relationship in a two-dimensional visualization.

We used VOSviewer (Visualization of Similarities Viewer) to analyze the collected

data. VOSviewer is a tool that creates maps based on network data [33]. These maps

provide visualizations that allow researchers to explore items and relationships. There

are various methods for establishing connections between items in these networks, in-

cluding co-authorship, co-occurrence, citation, bibliographic coupling, and co-citation.

We tested title, abstracts, index keywords, and author-supplied keywords as the

unit of analysis and found that author-supplied keywords are the most promising way

to identify research topics and their connections.

In this analysis, we considered 20 as the minimum threshold of keyword occur-

10

YearNo of Publications010002000300040005000198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020rences. This threshold places a minimal barrier before a keyword is “important” enough

to incorporate. Keywords appearing in fewer than 20 publications were omitted. This

threshold was chosen after experimentation as a way to control the level of noise and

difficulty of interpretation of the dataset and map, while still avoiding potential loss

of interesting and emerging topics. We then iteratively removed keywords that were

unrelated to software testing (e.g., publications that used software as part of classroom

testing) and merged redundant keywords (e.g., “automated test generation” and “auto-

mated test case generation”)—see Section 3.3—leaving a final set of 406 keywords.

VOSviewer produces maps based on a co-occurrence matrix—a two-dimensional

matrix where each column and row represents an item—a keyword, in our case–and

each cell indicates the number of times two keywords co-occur. This map construction

consists of three steps.

In the first step, a similarity matrix is created from the co-

occurrence matrix. A map is then formed by applying the VOS mapping technique to

the similarity matrix. Finally, the map is translated, rotated, and reflected. We provide

technical details on VOSviewer’s algorithm in Appendix A.

In VOSviewer, a map is visualized in three ways: The network visualization, the

overlay visualization, and the density visualization [34]. We have used the network and

overlay visualizations in this study, as well as the raw underlying data.

The network visualization is the standard view, displaying clusters of related items,

connected with edges based on their co-occurrence. Figure 3 shows the full network

visualization that is produced. In Figure 2, we highlight a small portion to explain how

to interpret the map data.

In this map, each node is a keyword. Figure 2 focuses on the keyword “software

reliability”. All keywords with a sufficiently strong connection to the targeted keyword

are highlighted, while unrelated keywords are made partially transparent. The size of

a node is based on the number of occurrences of the keyword. In Figure 2, software

reliability is targeted in approximately twice as many publications as “optimization”.

Keywords are organized into clusters according to the process described above.

Individual keywords can be linked across different clusters. However, the keywords

within a cluster tend to be very closely linked with several other keywords within the

same cluster. The color of the node indicates its cluster. In Figure 2, software reliability

11

Figure 2: Topics associated with software reliability

is marked in light blue, and other nodes with the same color belong to the same cluster

(e.g., “software reliability growth model”).

Keywords that co-occur in publications are illustrated with an undirected edge. The

thickness of the edge indicates how many publications have targeted both keywords.

In Figure 2, software reliability and software reliability growth model share a stronger

association (co-occurring in 35 publications) than software reliability with coverage

criteria (5 publications). A user-controlled threshold determines the minimum con-

nection strength for visible edges. We used the default, four publications, to control

the level of noise when using the visualization for interpretation. When performing

quantitative analyses, we consider all connections, regardless of strength.

The overlay visualization uses colors to indicate certain properties of a node, like

the average number of citations that publications targeting a keyword have received,

instead of using colors to show the cluster. In our case, we use this visualization to

analyze the average age of publications targeting a keyword.

12

3.3. Data Cleanup

The initial data included keywords that were either redundant or irrelevant:

• There are a small number of keywords unrelated to software testing, as the ini-

tial sample was gathered using a broad search string. For example, there were

keywords related to software-based student examination or software-based test-

ing of hardware. Additional keywords are either too generic to be considered

as specific research concepts—e.g., “software testing”—or are research-related

terms—e.g., “case study”, “empirical study”.

• Multiple keywords can refer to the same concept, and can be streamlined into a

single keyword—e.g., “automated test generation” and “automated test case gen-

eration”. The same keyword can appear in singular and plural form—e.g., “test

case” and “test cases”. There are also American and British English spellings

(e.g., “prioritisation” and “prioritization”).

To handle irrelevant and redundant keywords, we performed an iterative process.

The authors discussed each keyword and came to a consensus. We removed irrele-

vant keywords from the map, as well as those considered too broad or generic. We

removed publications targeting only those keywords, but retained publications that had

additional keywords that remained in our set.

We merged redundant keywords. In performing this process, we limited merging to

cases where a redundancy was obvious—primarily pluralization and British/American

English. This was to limit the risk of biasing the underlying data that we are using

to draw conclusions. We discussed each keyword and its alternatives, and came to a

consensus on which keyword to use in all cases. We then replaced the merged keywords

with the final keyword for each study and recreated the maps. We performed this

process multiple times until we were satisfied that redundant keywords did not remain.

3.4. Data Analyses

RQ1 (Popular Keywords): To identify the most common keywords, we sort the key-

words by the number of publications that targeted that keyword, and examine those

13

that are targeted in ≥ 0.50% of publications, or ≥ 250 publications. This threshold was

chosen by examining the drop-off in significance over the ten most popular keywords

and by considering the trade-off between clarity and giving a thorough impression of

the testing field. A total of 20 keywords fall above this threshold (4.93% of keywords).

We also have examined which keywords have received the most citations per publi-

cation, on average. Here, we examine all keywords with an average number of citations

≥ 20. This threshold yields 23 keywords, and was chosen because it yields a similar

quantity to the number of most common keywords, enabling clearer comparison.

RQ2 (Characterization of Clusters into Topics): We perform a qualitative charac-

terization to summarize the field of software testing, supported by the clusters. We

perform this summarization by assigning a small number of high-level “topics” to each

cluster—keywords or phrases that connect multiple keywords. We have chosen these

topics based on our interpretation of the keywords within each cluster. For example,

“performance testing” is a keyword that connects, e.g., “load testing”, “cloud comput-

ing”, and “cloud testing”.3 Because that keyword summarizes many other keywords

and connections within that cluster, “performance testing” can also serve as a topic that

describes the cluster as a whole.

Some clusters can be summarized by a single topic, while others represent multiple

topics. There is no case where all keywords are connected to all other keywords in

the same cluster. Often, two keywords are only indirectly connected through other

keywords—e.g., “random testing” is connected to “reliability” and “adaptive random

testing”, but the latter two are not directly connected in our sample.

We often observed small sub-groupings of keywords within a topic. In such cases,

we assign both a topic and a subtopic. For example, the topic of test creation guidance

can be broken into four distinct types of creation guidance. A keyword can also belong

to multiple topics or subtopics, linking topics within a cluster.

To assign topics, all authors examined the keywords within a cluster. We then

grouped keywords into one or more groupings. To be grouped, keywords must have a

3“load testing” and “cloud testing” are forms of performance testing that target “cloud computing”.

14

direct or indirect (via a shared keyword) edge. This grouping was made based on our

experience, literature, and the map data. We grouped keywords if they were used to

perform the same activity, were a technology used to perform an activity, were a source

of data for an activity, or had some other clear shared purpose. The proposed groupings

were discussed until a consensus could be reached. We then identified either a keyword

or concept that characterized each grouping. We discussed the options and reached a

consensus on the topic to assign. In cases where a grouping could be split into smaller,

but still distinct, groupings, subtopics were identified.

As all keywords must be grouped into a cluster, there are situations where a small

number of individual keywords do not relate to the topics assigned to that cluster. We

have tried to select topics that are as inclusive as possible.

RQ3 (Connections Across Clusters): In this question, we are interested in charac-

terizing how keywords and topics are connected across clusters. To do so, we have

examined two concepts—measurement of the density of connections between clusters,

and the identification of keywords that are connected to many other keywords.

Connection Density: We can examine how clusters are connected by identifying the

cases where the largest percentage of possible connections exist between keywords in

two clusters, A and B:

(Number of Connections Between Keywords in Clusters A and B)
(Number of Keywords in Cluster A) ∗ (Number of Keywords in Cluster B)

(1)

A measurement of 1.00 means that all keywords in Cluster A are connected to all

keywords in Cluster B.4

To identify the most densely interconnected clusters, we measured the connection

density between all pairs of clusters. We then focus on the connections with a density

≥ 0.12 (where at least 12% of all possible connections exist). This threshold was

chosen after examination of the measurements—23 of the 45 pairs of clusters ( 50%)

have a connection density above this threshold.

Connecting Keywords: Some keywords are singular research concepts, while others

4We employ the density because different clusters contain different numbers of keywords. This measure-

ment offers a fairer basis of comparison than the raw number of connections between pairs of clusters.

15

serve as “connecting keywords” that link many keywords together. We further charac-

terize connections across clusters by identifying these connecting keywords.

To identify these keywords, we measure the number of keywords that each keyword

co-occurs with outside of the cluster where that keyword is located. We analyze all key-

words connected to ≥ 100 keywords in external clusters. This threshold was chosen as

it yielded the same number of keywords as the threshold for the most popular keyword

in RQ1 (20 keywords), enabling direct comparison.

RQ4 (Emerging Keywords, Topics, and Connections): In this question, we are inter-

ested in identifying emerging trends in testing research. We can do this by examining

individual keywords, research topics, and connections between keywords.

We have classified any keywords with an average date of publication later than

June 2015 as our set of emerging keywords. This captures an approximate five year

period ending with the date we took our sample of publications. A recent date implies

one of two things about a keyword: (1) this is a new keyword, or (2), this is a older

keyword that has received more attention in recent years.

We are also interested in examining the connections between keywords and topics,

as related to the set of emerging keywords. There are a total of 2,029 connections

where at least one of the connected keywords is an emerging keyword, of which 1,412

are cross-cluster connections and 617 are within-cluster connections. To focus our

analysis, we focus on the cross-cluster connections, allowing us to also examine and

characterize the emerging connections between topics.

To identify a subset of those 1,412 connections for further exploration, we use

the cross-cluster connection density to identify the pairs of clusters with the highest

proportion of emerging connections. We have selected the ten pairs of clusters with

the highest proportion of emerging connections for further examination, corresponding

to a threshold of ≥ 3.5% of all connections between the two clusters consisting of

emerging connections. For each pair of clusters, we group the connections by topic,

then examine how the connection between these two topics is being shaped by the

emerging connections between low-level keywords.

RQ5 (Declining Keywords and Topics): We address this question following a similar

16

process to RQ4, based on oldest average dates of publication. 66 keywords met the

threshold in RQ4. Therefore, we also examine the 66 keywords with the oldest dates

of publication. This corresponds to a period from November 2001–May 2011.

These keywords represent concepts that are no longer receiving as much interest.

This does not imply with certainty that such concepts are no longer relevant, or that they

correspond to “solved” challenges. A keyword could represent (a) a topic or concept

in decline (e.g., an older technology or approach that has been potentially superseded),

(b) a well-established topic or concept with steady—but not growing—activity, or (c),

a topic or concept that had a “boom” period in the past and a lower level of activity

in recent years. Keywords may experience a resurgence. However, they have reduced

relevancy to current development or testing trends, challenges, and research topics.

Before stating that a particular topic is in decline, we compare the list of keywords

and topics with those in RQ4. We say that a topic is declining in interest if both (a)

it has several keywords with older average publication dates, and (b), lacks keywords

with recent average dates. By examining both the oldest and newest keywords, we can

more carefully discuss whether a topic is potentially in decline.

4. Results and Discussion

Our analyses are based on 406 keywords, which are mapped into 11 clusters. We

analyze this map by identifying the most popular keywords by occurrences and cita-

tions (Section 4.1) and the overarching research topics of each cluster (Section 4.2),

examining how keywords and topics are linked across clusters (Section 4.3), and ex-

ploring keywords, topics, and connections that are emerging or in potential decline

(Sections 4.4-4.5). We also offer advice and further exploratory analyses in Section 5.

A visualization of the keyword map is shown in Figure 3. An interactive version

of this map can be accessed at https://greg4cr.github.io/other/

2021-TestingTrends/topics.html or in the replication package.

17

Figure 3: A visualization of the connections between publication keywords.

4.1. RQ1: Popular Keywords

We begin by identifying the most popular individual keywords, sorted by the num-

ber of publications (and percentage of the total sample). These keywords are listed in

Table 2, along with a description, percentage of the sample, average age of publication

(rounded to the year), and average number of citations per publication.

These keywords are those that the authors considered important enough to note as

one of the core focuses of their work. There are certainly more than 326 publications in

this sample that use machine learning, for example. However, the authors may not have

listed machine learning as a keyword. Therefore, these keywords should be interpreted

as the research concepts the authors felt were the most important and relevant.

18

Keyword

# Pubs. Percent Citations Date Description

Automated Test Generation

1068

2.14%

16.36

2013 The use of tools to generate full or partial test cases [35].

Regression Testing

701

1.41%

14.03

2014

Mutation Testing

Test Automation

Model-based Testing

Genetic Algorithm

Fault Injection

Software Quality

596

567

552

519

477

445

Software Reliability

Test Case Prioritization

Verification

440

418

366

1.20%

14.83

2014

A practice where tests are re-executed when code changes to

ensure that working code operates correctly [36].

A practice where synthetic faults are seeded into systems to

assess the sensitivity of tests [37].

1.14%

9.71

2014 Tools and practices that enable automation of test execution [38].

1.11%

8.38

2014

Use of behavioral models to analyze the system, to design or

generate test cases, or to judge results of testing [35].

1.10%

10.10

2014

An optimization algorithm that models how populations evolve over

time [39]. Often used to automate tasks.

0.96%

6.12

2015

Injection of faults into a system for analysis [40].

0.89%

8.14

2012

0.88%

12.71

2010

Means to define, measure, and assure the quality of software [41].

Encompasses correctness and quality (e.g., performance or scalability).

Simulated execution of a system. May encompass how to simulate [42],

testing in simulation [43], or obtaining realistic results [44].

Means to define, measure, and assess the how quality changes

over time [45].

0.84%

17.42

2015 Automated techniques that select a subset of tests for execution [46].

0.73%

16.58

2012

Techniques that assess whether software possesses a property of interest,

often using formal specifications [1]. Testing is one verification technique.

Simulation

442

0.89%

4.53

2013

Coverage Criteria

362

0.73%

13.21

2012

Combinatorial Testing

349

0.70%

14.35

2015

Machine Learning

326

0.65%

8.32

2017

Reliability

306

0.62%

13.95

2013

Measurements used to assess the strength of a test suite based on how

tests exercise code elements [47].

A technique for generating or selecting test input, based on coverage

of representative values [48].

Algorithms that make inferences from patterns detected in data. Used in,

e.g., automation [49], predictive modeling [50], or evaluation [51].

Often a synonym for software quality, but can also refer to hardware

quality or a blend of hardware/software.

Symbolic Execution

Embedded Software

Neural Networks

295

268

266

0.59%

14.34

2014

Analyses where software is executed in an abstract form where

one symbolic input matches many real inputs [52].

0.54%

4.86

2014 Complex self-contained hardware and software systems [53].

0.53%

8.53

2015

Network structures inspired by the human brain, used in

machine learning [54].

Security

265

0.52%

7.60

2015

Practices, tools, and techniques intended to prevent misuse of a system’s

capabilities or data [55].

Table 2: Keywords targeted in at least 0.50% of publications (≥ 250 publications). Each keyword is named

and described, and the number of publications where the keyword is targeted, percentage of the sample,

average date of publication, and average number of citations per study are included.

RQ1 (Popular Keywords): The most common keywords tend to relate to

automation, test creation and assessment guidance, assessment of system quality,

and cyber-physical systems.

Automation offers promise for increasing the quality and efficiency of testing, and

19

many keywords (e.g., automated test generation, test automation) relate to automation.

Additionally, genetic algorithms and symbolic execution often enable automation. Test

case prioritization enables efficient test execution, and regression testing is a process

performed as part of a test execution pipeline. Combinatorial testing suggests an im-

portant subset of test input to apply, often as part of automated generation. Models

are often used to generate test input. Machine learning, including neural networks,

supports prediction tasks related to automation.

Many of the remaining keywords relate to assessments of testing effectiveness or

test creation guidance, including mutation testing, fault injection, and coverage criteria.

Other keywords (software quality, reliability, verification, security) relate to the overall

quality of the system, including its correctness, performance, and security. Finally,

embedded software and simulation relate to systems combining software and hardware

elements, which have high safety demands and unique testing activities [44].

The average number of publications per keyword is 75 (0.15%)—far below the

number of publications targeting the top 20 keywords, indicating their importance. It

is interesting that the most popular keyword is only a target of 2.14% of the sample,

and that only five keywords are targets of over 1% of the sample. We believe this

is an indication of the breadth of testing research. There are many challenges asso-

ciated with testing, from test creation to automation, execution, assessment, and pro-

cess. There are many ways to address each challenge, including algorithms and tools,

human-driven activities, and studies of those working in the field. Even the median—

40 publications—is a reasonable body of work on any single concept.

We can compare the most-common keywords with the most-cited. In Table 3, we

identify keywords that receive, on average, the most citations per publication.

RQ1 (Popular Keywords): The most-cited keywords also relate to automation,

test creation and assessment guidance, and assessment of system quality.

Some of these keywords are linked to the most common keywords—fault-based

testing, test suite minimization, covering arrays, partition testing, evolutionary testing,

and regression test selection, in particular. However, no keywords appear in both lists.

20

Keyword

Citations

# Pubs. Date Description

Testing Strategies

55.50

Testing and Debugging

48.48

Partition Testing

35.59

Fault-based Testing

34.48

Constraints

Test Suite Minimization

Random Testing

33.76

32.97

32.88

Software Fault Prediction

31.82

Covering Arrays

28.50

Compiler Testing

27.28

Object Oriented Modeling

25.75

Evolutionary Testing

23.93

Test Design

23.42

Regression Test Selection

23.40

Monte Carlo

Alloy

Automated Debugging

22.78

22.18

21.76

Adaptive Random Testing

21.51

Data Flow Testing

Data Flow

Software Standards

21.47

21.42

20.92

Synchronization

20.07

Sensitivity Analysis

20.00

26

21

54

33

25

39

218

38

96

32

20

46

33

58

23

22

38

95

43

36

26

29

36

2010 Guiding principles for test design and the testing process [56].

Debugging practices isolate and diagnose faults in the source code [57].

This keyword relates to the combination of testing and debugging techniques.

Test input selection based on division of the system’s input domain

into partitions, based on a set of rules [58].

Use of pre-specified faults in a program to create and evaluate test

suites [59]. Mutation testing is an automated form of fault-based testing.

Conditions that must be met to accomplish a goal, e.g., for input to take

2013

2005

2005

2011

a particular path in a program [52].

2014

Process of reducing test suite size by eliminating redundant test cases [60].

2011 Testing software by generating random input [61].

2016

Prediction of fault-prone code using software metrics and fault metadata [62].

2013 The set of test specifications selected during combinatorial testing [48].

Specialized testing practices for compilers, e.g., the selection of input

2014

programs to ensure the compiler conforms to its target language’s

semantics and syntax [63].

2004 Model formats based on object-oriented design and object interaction [64]

The use of evolutionary algorithms (e.g., genetic algorithms) to generate test

2011

input or automate other tasks [35].

2013 The process of defining test cases [65].

2014

2015

Practices to test cases for use during regression testing (e.g., only execute

tests for changed code) [66].

A family of algorithms used for optimization, numeric integration, and

probability assessment [67].

2014 Language for expressing complex behavior and constraints in software [68].

2012 Automated debugging techniques [69].

Random testing techniques designed to ensure input is evenly spread over

the input domain[70].

Testing based on the flow of information between variable definitions

2012

2011

and usages [71].

2008 Metrics for tracking the flow of information [71]

2009

2015

2012

Constraints, rules, and requirements that software or testing is expected

to meet [72].

Practices for ensuring components are able to coordinate when completing

tasks in parallel [73].

Study of how uncertainty in system output can be traced to sources of

uncertainty in its inputs [74].

Table 3: Keywords that received more than 20 citations on average per publication, with description, average

number of citations, number of publications where the keyword is targeted, and average date of publication.

However, both the most common and most-cited keywords share common themes.

Many of the keywords relate to automation (e.g., test suite minimization, random test-

ing), test creation and assessment (e.g., testing strategies, data flow testing), or quality

assessment (e.g., software fault prediction, sensitivity analysis). For example, Figure 4

21

Figure 4: A subset of keywords connected to automated test generation, colored by the average number of

citations. Nodes in yellow attract a high number of citations (≥ 14).

illustrates that many keywords associated with automated test generation receive a rel-

atively high number of citations on average.

In general, the keywords in Table 3 are associated with a relatively small number

of publications. They also have an older average date of publication, approximately

2010 versus 2014, allowing more time to attract citations. We hypothesize that these

particular keywords (a) are related to themes that attract attention, and (b) are attached

to a small set of publications containing a subset of particularly influential publications.

4.2. RQ2: Characterization of Clusters into Topics

By examining the connections between keywords, we can understand the context

in which keywords form, grow, and thrive. Therefore, we have identified research top-

ics characterizing the keyword clusters. These topics are detailed in Tables 4-5. We

note a cluster ID assigned by VOSviewer, the number of keywords in that cluster, the

density of connections between keywords within each cluster (the ratio of the number

22

Figure 5: Identified research topics (middle layer) and subtopics (final layer), colored by cluster.

23

Software TestingTest OraclesModel-Based TestingWeb Application TestingGUI TestingEvolution and MaintenanceRegression TestingTest PrioritizationAutomated Test GenerationSoftware ReliabilityRandom TestingPerformance TestingCreation GuidanceWhite-Box TestingBlack-Box TestingMutation TestingSecurity TestingAutomated Program RepairVerification and Program AnalysisConcurrencyFault InjectionApplications of MLData Used for MLML TechniquesML and Predictive ModelingTest Case TypesTest AutomationProcesses and RiskModel-Driven DevelopmentRequirementsSystem TestingQuality AttributesEmbedded System TestingOther Specialized Domains and Tech.Mobile TestingCluster Num

Density Topics and

Example Keywords

Description

Keywords

(Subtopics)

11

10

4

16

100%

Test Oracles

test oracle,

metamorphic relation

Test component that issues a verdict of correctness [49].

39%

Model-Based

model transformation,

Use of behavioral models to analyze the system,

Testing

timed automata

to design or generate test cases, or as oracles [35].

9

18

31%

Web Testing

GUI Testing

web applications,

Testing techniques, tools, and activities focused on

javascript

verification of web-based applications [75].

graphical user interface,

Test design or generation techniques focused on

finite state machine

exercising a system through its graphical interface [76].

Evolution and

program comprehension,

Practices for controlling and maintaining quality

Maintenance

change impact analysis

as the system changes over time [77].

8

19

47%

(Regression Testing)

(Test Prioritization)

regression testing,

A practice where tests are re-executed when code changes

regression test selection

to ensure that working code operates correctly [36].

test case prioritization,

Automated techniques that select a subset of tests

test case selection

for execution [46].

7

28

48%

Automated Test

genetic algorithms,

Generation

branch coverage

(Automated Program

fault localization,

Repair)

genetic programming

The use of tools to generate full or partial test cases [35].

Automated generation of patches for faulty programs [78].

Reliability

reliability growth,

Means to define, measure, and assess the how quality

quality control

changes over time [45].

6

30

24%

(Performance Testing)

Random Testing

Creation Guidance

(White-Box Testing)

5

32

30%

(Black-Box Testing)

(Mutation Testing)

(Security Testing)

load testing,

cloud testing

Testing to assess performance and scalability of a system

under different operating conditions [79].

adaptive random testing,

Generation of random input for various purposes

statistical testing

(e.g., assessing reliability or performance) [35]

certification,

test adequacy

coverage criteria,

data flow

Guidance for how a tester might approach test design—e.g.,

goals, input selection, and assessing test strength.

Test creation based on source code [47].

specification-based testing,

Test creation based on requirements and

black-box testing

other documentation [80].

mutation score,

Test creation based on synthetic faults

mutation operators

seeded into a system [37]

penetration testing,

Test creation to assess the ability of a system to

software vulnerability

prevent exploitation of vulnerabilities [81].

Verification

dynamic analysis,

Analyses performed to ensure that software possesses

and Analysis

static analysis

properties of interest (e.g., correctness, resilience) [1].

4

35

31%

(Concurrency)

(Fault Injection)

parallelization,

synchronization

fault model,

fault tolerance

Analyses of programs that execute over parallel

threads or processes [82].

Injection of faults into a system for analysis [40].

Table 4: An overview of clusters 4 - 11, including the cluster ID from VOSViewer, the number of keywords,

inter-cluster connection density (percentage of possible connections between keywords), identified topics

and subtopics, example keywords for each topic, and a brief description of each topic. Clusters are ordered

from smallest to largest.

24

Cluster Num

Density Topics and

Example Keywords

Description

Keywords

(Subtopics)

3

48

26%

2

58

21%

Machine Learning

machine learning

(Applications)

(Data Used)

(ML Techniques)

Test Case Types

Test Automation

(Mobile Testing)

defect prediction,

estimation

metrics,

complexity

neural networks,

deep learning

unit testing,

exploratory testing

test execution,

testing tools

mobile testing,

android testing

Algorithms that make inferences from patterns

detected in data [50].

Applications of ML in software testing.

Sources of data used to draw conclusions with ML.

ML techniques used in testing research.

Practices and levels of granularity for test design.

Tools and practices that enable automation of

test execution [38].

Testing techniques, tools, and activities focused on

verification of mobile applications [83].

Processes and Risk

software quality,

The organization, management, and testing process

test-driven development

of a development team [1].

(Model-Driven

model-driven development,

Development process based on use of models for

Development)

model-driven testing

analysis, code generation, and testing [84]

Requirements

requirements engineering,

Requirements indicate correct behavior. Verification

Engineering

traceability

often assesses conformance of code to requirements [1].

System Testing

(Quality Attributes)

system testing,

user interfaces

usability,

Test cases that interact with an external system interface [1].

Non-functional properties of a system assessed as

software performance

part of quality assurance [85]

1

118

20%

(Embedded Systems)

real-time system,

simulation

Complex self-contained hardware and software systems [53].

(Other Specialized

Domains)

open source software,

System types (e.g., databases, virtual reality, operating

image processing,

systems) or technologies (e.g., XML, Java) with

autonomous vehicles

dedicated testing approaches.

Table 5: An overview of clusters 1 - 3, including the cluster ID from VOSViewer, the number of keywords,

inter-cluster connection density (percentage of possible connections between keywords), identified topics

and subtopics, example keywords for each topic, and a brief description of each topic. Clusters are ordered

from smallest to largest.

of existing within-cluster connections to the total possible within-cluster connections,
)︁), and the topics and subtopics assigned to that cluster. For each topic or
(︁Keywords
2

subtopic, we list two example keywords that fall within that topic, and we briefly de-

scribe the meaning of the topic. For additional clarity, Figure 5 outlines these topics,

colored by the cluster they emerged from.

RQ2 (Characterization of Clusters into Topics): Based on keyword clustering,

testing research can be divided into 16 topics, with a further 18 subtopics.

25

While some of the topics within a cluster may seem independent, they are linked by

connections between the underlying keywords. It is important, therefore, to examine

both topics and keywords to come to a full understanding of a particular cluster. For

example, random testing is a topic with widespread applicability. However, it is linked

to Cluster 6 because random testing is often used to assess reliability or performance.

Within Cluster 2, the test automation topic encapsulates the emerging subtopic of

mobile testing. Mobile testing is not as well-established as web application testing,

but is clearly growing as a distinct research area. In the future, it may emerge as an

independent research topic—perhaps even as its own cluster. Additionally, the model-

driven development subtopic in Cluster 2 is related to—but also separate from—the

model-based testing topic in Cluster 10. The latter focuses on technical aspects of

modelling, while the former focuses on process and practices that may use these tech-

nologies. There are connections between the two, but they contain different keywords.

Cluster 1 is the least cohesive cluster. However, we can categorize many keywords

under a core topic of system testing. In Cluster 2, there is a topic centered around

test case types (e.g., unit testing). System testing is often grouped with these test case

types. However, it is also a broader concept encompassing many different types of sys-

tems and system interfaces (e.g., embedded systems, operating systems, or databases).

Several topics in our characterization also relate to system-level practices or domains,

e.g., web, GUI, and performance testing. Those topics are established enough to stand

independently, while the system testing topic in Cluster 1 acts as a broad umbrella.

4.3. RQ3: Connections Across Clusters

We analyze connections across clusters by measuring the connection density be-

tween pairs of clusters, and by identifying keywords that bridge clusters.

Connection Density: Table 6 shows all cross-cluster densities, with those ≥ 12% are

highlighted. 23 of the 45 pairs of clusters meet this threshold, indicating that many

research topics are densely connected.

26

Cluster

1

0.20

1

2

3

4

5

6

7

8

9

10

11

2

0.08

0.21

3

0.09

0.08

0.26

4

0.11

0.09

0.10

0.31

5

0.06

0.10

0.08

0.15

0.30

6

0.06

0.08

0.08

0.08

0.08

0.24

7

0.06

0.09

0.12

0.12

0.20

0.10

0.48

8

0.07

0.13

0.13

0.12

0.15

0.11

0.23

0.47

9

0.07

0.10

0.05

0.07

0.13

0.06

0.14

0.13

0.31

10

0.09

0.14

0.07

0.10

0.14

0.06

0.14

0.15

0.10

0.39

11

0.07

0.11

0.15

0.12

0.21

0.12

0.15

0.20

0.17

0.09

1.00

Table 6: Connection density between pairs of clusters. Cross-cluster densities ≥ 0.12 are highlighted.

Densities in italics represent within-cluster densities for each cluster.

RQ3 (Connections Across Clusters): Clusters 5 (creation guidance), 7

(automated test generation), 8 (evolution and maintenance), and 11 (test oracles)

are densely connected to several clusters. These clusters represent particularly

multidisciplinary topics.

Cluster 8 appears in eight pairs, Clusters 7 and 11 appear in seven, and Cluster

5 appears in six.

In particular, the pairings between Clusters 7 and 8 and between

Clusters 5 and 11 have a higher connection density than the within-cluster densities of

Clusters 1 and 2, indicating the dense interconnection between these topics.

As the most common keyword identified in our analysis, automated test generation

has connections to keywords and topics in every other cluster.

If a testing method

exists, there will be an interest in generating tests for it (e.g., Clusters 5, 8, 9, and

10). Oracle creation often requires manual effort, leading to an interest in automated

generation or reuse of oracles (Cluster 11). Further, machine learning offers the means

to assist or enable automated test generation (Cluster 3).

Test oracles are a necessary component of almost all test cases, leading to dense

connections with Clusters 5 (creation guidance), 6 (reliability, performance, random

testing), 8 (regression testing), and 9 (web and GUI testing). In addition to the above-

mentioned connection to automated generation, machine learning offers a means to

27

automate the creation of oracles (Cluster 3). Oracles are also a natural part of verifica-

tion and different program analyses (Cluster 4).

Maintenance has implications on multiple aspects of testing, such as costs and qual-

ity. Maintenance needs affect the tasks performed during test automation (Cluster 2).

Test prioritization also uses the same information that guides test creation to select tests

(Cluster 5), and can be assisted using machine learning (Cluster 3). Both regression

testing and test prioritization are performed for GUIs and web applications (Cluster 9),

and can make use of models (Cluster 10). Further, analyses related to program and test

evolution are often connected to other analyses in Cluster 4.

Test creation practices (Cluster 5) also connect broadly. Beyond automated test

generation, test oracles, and test prioritization, several creation practices either have

adaptations for model-based testing (Cluster 10) or for web and GUI testing (Cluster

9). In addition, there are connections between verification and test creation practices

(Cluster 4)—e.g., black box testing and verification are connected through specifica-

tions, and security testing and analysis are related.

Clusters 1, 2, 3, and 6 have the least-dense connections to other clusters. Clusters

1 and 2 are both large clusters with multiple topics and subtopics that are distinct, but

closely-related. Connections exist to other clusters, but may be less common, as these

two clusters already represent a broad set of keywords. Reliability and performance

testing (Cluster 6) and various forms of predictive modelling in Cluster 3 are also often

pursued as standalone topics, but can be connected to other topics. Out of all density

measurements, the lowest was between Cluster 3 (machine learning) and Cluster 9

(web and GUI testing), with 5% of possible connections existing in publications.

Connecting Keywords: In Table 7, we list all keywords that are connected to at least

100 keywords in external clusters.

RQ3 (Connections Across Clusters): Twenty keywords serve as “connectors”

between clusters, reflecting multidisciplinary concepts (e.g., software quality),

common test activities (e.g., unit testing), and common sources of information for

test creation (e.g., coverage criteria).

28

Keyword

Connected Connected Position in Description

(External)

(All)

Table 1

Automated Test Generation

Software Quality

Mutation Testing

Regression Testing

Test Automation

Model-based Testing

Coverage Criteria

Verification

Genetic Algorithm

Machine Learning

Test Case Prioritization

Software Maintenance

Debugging

Unit Testing

Software Reliability

Reliability

Fault Injection

Static Analysis

Mutation Analysis

217

164

164

157

152

150

147

143

139

132

119

119

117

114

114

108

106

101

101

Unified Modeling Language

101

243

202

184

174

200

163

168

164

161

161

134

130

135

136

132

125

128

120

116

111

1

8

3

2

4

5

13

12

6

15

11

-

-

-

10

16

7

-

-

-

See Table 2.

See Table 2.

See Table 2.

See Table 2.

See Table 2.

See Table 2.

See Table 2.

See Table 2.

See Table 2.

See Table 2.

See Table 2.

Practices for controlling and maintaining quality

as the system changes over time [77].

See Table 3.

A practice where tests are created for a small,

isolated unit of code (typically a class) [1].

See Table 2.

See Table 2.

See Table 2.

Analyses performed without executing the code

(e.g., inspection or symbolic execution) [1].

Analyses of programs or tests performed

using injected mutations [86].

A family of techniques for modelling and

analyzing program behavior [64].

Table 7: Keywords that are connected to at ≥ 100 keywords in clusters other than the one where the keyword

is assigned (both keywords are targeted in at least one study). Each keyword is named and described, and

the number of connected keywords (in external clusters, and in total) are listed.

For comparison, we also list the total number of connected keywords, and the po-

sition that the keyword had in Table 2 (if it appeared in the most commonly-targeted

keywords). Many of the connecting keywords are also among the most common occur-

ring keywords, with automated test generation on top of both lists. The exact positions

of keywords shift in the ordering, but 14 of the 20 most common keywords are also

connecting keywords. The most common keywords tended to relate to automation, test

creation and assessment guidance, assessment of system quality, and cyber-physical

systems. These concepts—especially the first three—are broad, with wide-ranging ap-

29

plicability. That suggests that popularity of a keyword is not only a reflection of a

particular concept, but on its multidisciplinary applicability.

In contrast to Table 1, we see a notable rise in the position of software quality,

coverage criteria, and machine learning. Software quality and machine learning are

both very broad concepts, while coverage criteria are a common source of information

and a target for testing, with applications in test creation guidance, automated test

generation, quality assessment, prediction, and other areas.

We also see several keywords emerge: software maintenance, debugging, unit test-

ing, static analysis, mutation analysis, and unified modeling language. These include

broadly applicable concepts (maintenance, debugging, static analysis, mutation anal-

ysis), a common source of information (unified modeling language), and a common

testing activity (unit testing).

Six of the most common keywords do not meet the threshold for connecting keywords—

simulation (93 external connections), combinatorial testing (96), symbolic execution

(83), embedded software (85), neural networks (83), and security (82). All six are

multidisciplinary concepts, but are more specific—rather than broad—concepts (com-

binatorial testing, symbolic execution, embedded software, neural networks).

4.4. RQ4: Emerging Keywords, Topics, and Connections

A visualization of the map of keywords, colored by average year of publication, is

shown in Figure 6. Yellow nodes have an average date of 2016 or newer. Blue nodes

have an average date of 2010 or earlier. A gradient between blue and yellow repre-

sents 2010–2016. We examine keywords, topics, and connections that have emerged

or grown in interest since June 2015.

An interactive version of this map can be accessed as an overlay at https://

greg4cr.github.io/other/2021-TestingTrends/topics.html

by selecting “Avg. Pub. Year” under the “Color” option.

Keywords and topics: Sixty-six keywords (16.26%) represent new emerging concepts

or have received significant recent attention. Figure 7 links these keywords to their

30

Figure 6: The map of keywords, colored by the average year of publication. Note that “2010” should be read

as ≤ 2010 and “2016” should be read as ≥ 2016.

respective research topic. From these results, we can make several observations:

• Many of the growth areas map to shifts in technology. There is growing interest

in web applications, relating to technologies (JavaScript), testing tools (Sele-

nium), and testing techniques. There is a similar emergence of mobile applica-

tions, in both the subtopic of mobile testing in Cluster 2 (android testing, mobile

testing) and technologies in Cluster 1 (mobile applications, smartphone).

• Machine learning has advanced many fields. Unsurprisingly, it is also one of

the largest growth areas in testing. The keyword “machine learning” has an

average publication date of October 2016, and keywords have emerged related

to applications, data, and specific techniques for ML. “Deep learning” is one of

the newest keywords (average date of September 2018).

• Keywords have also emerged targeting ML and AI-based systems. From the

embedded systems and “other domains” topics, we see keywords related to au-

31

Figure 7: Keywords with an average publication date newer than June 2015, along with their associated

research topic. The number next to the list of keywords indicates the number of emerging keywords. Topics

colored in gray are those without emerging keywords.

32

Test OraclesModel-Based TestingWeb Application TestingGUI TestingEvolution and MaintenanceRegression TestingTest PrioritizationAutomated Test GenerationSoftware ReliabilityRandom TestingPerformance TestingCreation GuidanceWhite-Box TestingBlack-Box TestingMutation TestingSecurity TestingAutomated Program RepairVerification and Program AnalysisConcurrencyFault InjectionApplications of MLData Used for MLML TechniquesML and Predictive ModelingTest Case TypesTest AutomationProcesses and RiskModel-Driven DevelopmentRequirementsSystem TestingQuality AttributesEmbedded System TestingOther Specialized Domains and Tech.Mobile Testing2016: metamorphic relation; 2015: scientific software22015: specification mining12016: selenium, test optimization, automation testing; 2015: benchmark, web, javascript62016: manual testing12016: natural language processing12016: swarm intelligence; 2015: ant colony optimization22017: automated program repair; 2015: program synthesis22017: big data; 2015: cloud computing22016: benchmarking12016: fuzzing12016: equivalent mutants, software fault localization; 2015: mutation score32016: software vulnerability, fuzzing; 2015: penetration testing32016: python, llvm, resilience; 2015: gpu42016: soft error, vulnerability; 2015: fault simulation32016: high performance computing12016: mining software repositories, defect prediction, software fault prediction32016: energy12018: deep learning; 2015: feature selection, support vector machine32017: android testing, mobile testing22015: unit tests12018: devops; 2016: continuous integration22018: devops; 2016: technical debt; 2015: software development lifecycle32015: model-driven engineering12017: fault injection attack; 2015: sustainability, energy efficiency32017: internet of things; 2016: software-defined networking, autonomous vehicles, openflow, cyber-physical systems; 2015: automotive, energy efficiency72017: path planning, gamification; 2016: augmented reality, information system, android, autonomous vehicles, mobile applications, computer vision, crowdsourcing; 2015: smartphone, image processing112016: machine learning1tonomous vehicles, computer vision, image processing, and augmented reality.

All of these areas require specialized testing approaches. Autonomous vehicles,

in particular, may grow into its own independent subtopic in the future.

• There is growing interest in energy consumption. This is connected to mobile

applications, and a shift to portable devices that rely on batteries. This also

reflects growing interest in sustainability and environmental impact of software.

• Automated program repair has emerged as a subtopic. The core keyword has

one of the newest average publication dates (March 2017), and its connected

keywords (e.g., program synthesis) also have recent dates.

• Fuzzing and search-based approaches (swarm intelligence, ant colony optimiza-

tion) have emerged as test generation techniques. Fuzzing, notably, has seen

application in general and security-focused testing topics. Security-related key-

words are also active and growing.

RQ4 (Emerging Keywords, Topics, and Connections): Emerging keywords

and topics relate to, or incorporate, web and mobile applications, machine

learning and AI—including autonomous vehicles—energy consumption,

automated program repair, or fuzzing and search-based test generation.

Connections: We focus our examination on ten pairs of clusters with the highest pro-

portion of emerging connections to the number of possible connections (≥ 3.5%). The

connected clusters, and their associated topics, have a rapidly evolving relationship.

• Cluster 11 (test oracles) with Clusters 5 (creation guidance; 8.59% of connec-

tions are emerging), 3 (machine learning; 6.77%), 8 (evolution and maintenance;

5.26%), 6 (reliability; 4.17%), 9 (web application and GUI testing; 4.17%), 2

(test case types, test automation, processes and risk, and model-driven develop-

ment; 3.88%), and 4 (verification and program analysis; 3.57%).

• Cluster 7 (automated test generation) with Clusters 9 (4.36%) and 3 (3.57%).

• Cluster 5 with Cluster 9 (4.69%).

33

Figure 8: Emerging connections, connected by research topic with test oracles, for the cluster pairings with

highest ratio of emerging to total connections.

34

Test OraclesEvolution and MaintenanceCreation GuidanceWhite-Box TestingMutation TestingSecurity TestingML and Predictive ModelingSoftware ReliabilityPerformance TestingWeb Application TestingTest Case TypesTest AutomationProcesses and RiskRequirements(scientific software and test types)scientific software,unit testing(scientific software and processes)agile development,scientific softwarescientific software,test-driven development(test oracles and processes)metamorphic relation,software quality(scientific software and automation)continuous integration,scientific software(scientific software and requirements)scientific software,software quality(test oracles and requirements)metamorphic relation,verification and validationmetamorphic relation,software qualityVerification and Program AnalysisFault Injection(test oracles and ML)machine learning,metamorphic testingdeep learning,test oracle(scientific software and ML)machine learning,scientific software(test oracles and fault injection)fault tolerance,metamorphic relationtest oracle,vulnerability(test oracles and reliability)cloud computing,metamorphic testing(scientific software and reliability)scientific software,software reliability(test oracles and performance testing)metamorphic relation,performance testing(test oracles and security testing)metamorphic testing,software vulnerabilitysoftware vulnerability,test oracle(test oracles and creation technologies)fuzzing,metamorphic testing(test oracles and white-box testing)code coverage,metamorphic relationcontrol flow graph,metamorphic relation(scientific software and white-box testing)coverage criteria,scientific software(test oracles and mutation testing)metamorphic relation,mutation analysismetamorphic relation,mutation testing(scientific software and mutation testing)mutation testing,scientific software(test oracles and analysis)metamorphic relation,verificationtest oracle,vulnerability(scientific software and analysis)python,scientific softwareruntime verification,scientific software(test oracles and maintenance)metamorphic relation,software maintenance(test oracles and NLP)metamorphic testing,natural language processingnatural language processing,test oracle(test oracles and web apps)javascript,metamorphic testingtest oracle,webFigure 9: Emerging connections, connected by research topic (excluding test oracles), for the cluster pairings

with highest ratio of emerging to total connections.

The highlighted connections between topics are shown in Figure 8 for topics connected

with test oracles, and in Figure 9 for other topics. For each connection between topics,

a small number of example connections between keywords are shown.

RQ4 (Emerging Keywords, Topics, and Connections): Web applications and

scientific computing require targeted testing approaches and practices, leading to

emerging connections to many topics. Test oracles are also a rapidly-evolving

topic with many emerging connections. Machine learning has emerging potential

to support automation.

We make several observations about these emerging connections:

35

White-Box TestingMutation TestingSecurity TestingML and Predictive ModelingWeb Application TestingAutomated Test GenerationAutomated Program RepairApplications of ML(ML to support automated program repair)automated program repair,deep learningautomated program repair,neural networks(ML to support fault localization)deep learning,fault localizationfault localization,machine learningData Used for ML(data for automated program repair)automated program repair,defectsautomated program repair,software faults(ML to support automated generation)automated test generation,deep learningmachine learning,search-based software testing(optimization algorithms for prediction)defect prediction,search-based software engineeringgenetic algorithm,software fault prediction(optimization algorithms for feature selection)feature selection,genetic algorithmfeature selection,multi-objective optimizationGUI Testing(automated generation for GUIs)ant colony optimization,gui testing(automated generation for web apps)automated test generation,javascriptautomation testing,genetic algorithmBlack-Box Testing(black-box testing for web apps)black-box testing,webspecification-based testing,test optimization(white-box testing for web apps)coverage criteria,javascriptjavascript,symbolic execution(mutation testing for web apps)equivalent mutants,web applicationsjavascript,mutation operators(security testing for web apps)penetration testing,web applicationssoftware vulnerability,web application testing• Test oracles appear often because (a) Cluster 11 is a small cluster, (b) this topic

has the largest percentage of emerging keywords, and (c), this topic is naturally

connected to all other topics. Research interest in test oracles is growing [49, 87],

and effective oracles are needed for emerging domains such as web applications.

The relationship between oracles and different testing practices is not well un-

derstood yet, leading to many emerging connections. Further, interest is growing

in the use of machine learning to generate test oracles [87].

• The keyword “scientific computing” is part of Cluster 11, due to its frequent

connection with metamorphic testing. Inspection of the emerging connections

makes it clear that software testing for scientific computing is emerging as a

distinct domain of interest, with major connections to Cluster 2 and 5.

• As in many other areas of software development, machine learning offers the

potential to automate tasks that traditionally require significant human effort,

such as test and oracle generation and program repair.

• Test creation practices of many types (including white-box, black-box, mutation,

and security) are emerging for web applications.

• New test generation approaches are emerging for GUIs and web applications.

4.5. RQ5: Declining Keywords and Topics

Figure 10 shows the 66 keywords with the oldest average date, with their associated

research topic. In particular, we highlight three research topics or subtopics that we

hypothesize may currently be in decline.

RQ5 (Declining Keywords and Topics): Older average dates of publication and

lack of emerging keywords suggest that keywords and topics related to random

and requirements-based testing may be in decline.

Briefly, we examine these areas:

36

Figure 10: Keywords with an average publication date earlier than June 2011, along with their associated

research topic. Topics colored in gray are those without declining keywords. Topics with both declining

keywords and a lack of emerging keywords are highlighted.

37

Test OraclesModel-Based TestingWeb Application TestingGUI TestingEvolution and MaintenanceRegression TestingTest PrioritizationAutomated Test GenerationSoftware ReliabilityRandom TestingPerformance TestingCreation GuidanceWhite-Box TestingBlack-Box TestingMutation TestingSecurity TestingAutomated Program RepairVerification and Program AnalysisConcurrencyFault InjectionApplications of MLData Used for MLML TechniquesML and Predictive ModelingTest Case TypesTest AutomationProcesses and RiskModel-Driven DevelopmentRequirementsSystem TestingQuality AttributesEmbedded System TestingOther Specialized Domains and Tech.Mobile Testing2010: unified modeling language, object-oriented2011: conformance testing32010: vhdl 12010: data flow testing; 2011: constraints, evolutionary testing32009: software reliability; 2011: software reliability assessment22007: data flow; 2009: program instrumentation; 2011: structural testing32005: fault-based testing12011: aspect4j, program transformation22009: error detection12009: software testability; 2010: testability, intrusion detection; 2011: software development management42003: software measurement; 2009: software testability; 2010: testability; 2011: stress42006: predictive models12011: software analysis, software reuse22009: testing strategies; 2010: testing process; 2011: productivity32010: formal methods12006: computational modeling; 2007: automata; 2008: hardware, formal specification; 2009: control systems, benchmark testing; 2010: emulation, real-time system8  2002: application software; 2003: object oriented modeling; 2005: protocols; 2006: operating systems, internet; 2007: software libraries; 2008: graphical user interfaces; 2009: xml, distributed system; 2010: cots, user interfaces, java, mobile computing, navigation 142004: partition testing; 2010: random testing22007: concurrent programs; 2009: specification based testing; 2011: database testing32010: formal methods; 2011: software analysis, software reuse32006: system testing12001: costs; 2007: runtime, inspection; 2008: formal specification, software standards; 2009: software design; 2010: software architecture, software prototyping, performance evaluation, performance analysis, rapid prototyping 11• Traditional random testing has been supplanted, to some extent, by semi-random

approaches. As shown in Figure 7, search-based and fuzzing techniques are

growing in popularity. Both use sampling heuristics instead of applying pure

random generation, retaining some of the benefits of random testing (e.g., scala-

bility) while potentially yielding more effective results.

• Many of the keywords related to requirements and black-box testing have older

average publication dates, indicating potential stagnation. Agile processes favor

lightweight requirements (e.g., user stories) over formal and complex require-

ments. We hypothesize that this may have led to a shift in attention towards

other sources of information for test creation.

We hesitate to state that these topics are “dying” or are solved challenges. However,

we do see evidence that they have not seen notable growth in popularity or the emer-

gence of new keywords in recent years. New application areas, techniques, or changes

in development processes may lead to a resurgence in interest in the future.

5. Further Analysis and Advice to Researchers

Both the high-level topic overview and the low-level map of connections between

keywords can serve as inspiration for prospective and experienced researchers. We

offer the following advice on how this data could inspire new research.

An overview of the testing field: For inexperienced researchers, the high-level topics

offer an immediate “snapshot” that can be used to guide exploration of different re-

search areas. The keywords illustrate key concepts that form research topics, and offer

targeted suggestions on terms the researcher should examine in detail. Connections be-

tween those keywords illustrate how those concepts have been connected in practice,

which may encourage critical reflection on both the individual concepts and how they

relate. The emerging keywords and topics suggest areas that researchers may wish to

pay attention to, and emerging connections clarify how these keywords fit into the field.

Understanding the context of a keyword or topic: Researchers can analyze the map

to gain a data-driven view of the field for further planing and development. As a starting

38

point, those interested in a keyword or topic can examine how that keyword or topic

fits into the broader context of testing research.

• What keywords are often associated with a keyword of interest? This may il-

lustrate the type of research often conducted on this concept (or its associated

topic), and natural areas of synergy between keywords or topics.

• Is interest in this keyword or topic growing, declining, or stable? The average

date of publication may suggest the current level of interest (or lack thereof).

Identification of under-explored connections between keywords or topics: We hy-

pothesize that the map data could potentially inspire future research through analyses

of connections between keywords and topics. There are many ways connections could

be analyzed. One is to identify keywords that have under-explored connections.

Specifically, an under-explored connection is one where (a) at least one publication

has connected the keywords, but (b), the specific number of publications connecting

those two keywords is relatively low—indicating potential for additional research ex-

ploration. Under-explored connections may serve as inspiration, suggesting concepts

that could be connected in further research:

• Within a cluster, under-explored connections may suggest ways that concepts

within a particular research topic could be more closely linked. For example, dif-

ferent mechanisms from automated test generation algorithms could be blended

into hybrid algorithms.5 An examination of under-explored connections could

offer similar inspiration.

• Across clusters, we could identify either pairs of clusters or topics that could

be more deeply connected in future research. In some cases, these may be top-

ics that are already connected (e.g., automated test generation and white-box

testing), but where there are opportunities for new research related to specific

keywords or aspects of the topic (e.g., specific test generation algorithms).

5As has been done for concolic execution and search-based test generation [88].

39

There are different ways that under-explored connections could be identified and ana-

lyzed. As an initial exploration, first, one must identify a lower and upper bound on the

number of publications linking keywords. As an example, in the network visualization,

four publications are needed for an edge to be shown (by default). Therefore, one could

adopt four publications as the threshold for this analysis and capture all connections in

a short range of this threshold—e.g., 4-6 publications targeting a pair of keywords.

718 connections have a strength of 4-6 publications. To identify a subset for ini-

tial exploration, we can (a) focus on cross-cluster connections, and (b), use the cross-

cluster connection density to identify the pairs of clusters with the most under-explored

connections. Here, we specifically focus on the cluster pairings where ≥ 2% of all con-

nections between the two clusters consist of under-explored connections. Six cluster

pairings met the threshold: Cluster 11 (test oracles) with Cluster 9 (web and GUI test-

ing, 2.63%), Cluster 5 (creation guidance, 2.34%), and Cluster 3 (machine learning,

2.08%), and Cluster 7 (automated test generation) with Cluster 8 (evolution and main-

tenance, 3.38%), Cluster 10 (model-based testing, 2.46%), and Cluster 5 (2.01%).

For these cluster pairings, we grouped the connections by research topic, then ex-

amined the meaning and potential application of the connections. In Figure 11, we

illustrate the identified connections, categorized by their associated research topics.

We make several observations about these connections. First, specific suggestions

emerge for exploring connections in future research, including (among others):

• The relationship between mutations and test oracles.

• Use of mutation as part of automation program repair and test generation.

• Test generation based on specific modeling formats (e.g., object-oriented models

such as activity diagrams).

• Reduction techniques for generated test suites and test cases.

• The relationship between test generation and program evolution (e.g., how often

tests should be generated, how tests should be maintained).

• Generation of tests for regression testing.

40

Figure 11: Under-explored connections (keywords connected by 4-6 publications), connected by research

topic, for the six cluster pairings with highest ratio of under-explored to total connections.

41

Test OraclesModel-Based TestingWeb Application TestingEvolution and MaintenanceRegression TestingTest PrioritizationAutomated Test GenerationCreation GuidanceWhite-Box TestingMutation TestingSecurity TestingAutomated Program RepairApplications of MLML TechniquesML and Predictive Modeling(test oracles and web apps)metamorphic testing,web servicestest oracle,web services(test oracles and ML)machine learning,metamorphic relationmachine learning,test oraclemetamorphic testing,support vector machineneural networks,test oracle(test oracles and creation guidance)coverage criteria,test oraclemetamorphic testing,mutation analysismutation analysis,test oracle(test generation/program repair and creation guidance)automated program repair,mutation testingautomated test generation,security testingcompiler testing,fuzzinggenetic algorithm,symbolic executionprogram synthesis,symbolic execution(test generation and white-box testing)automated test generation,control flow graphautomated test generation,test adequacybranch coverage,symbolic executioncode coverage,genetic algorithmcoverage criteria,test datasearch-based software testing,structural testing(test generation and mutation testing)automated test generation,fault-based testingautomated test generation,mutation operatorsfitness function,mutation testingcombinatorial testing,mutation testingconstraint solving,mutation testing(test generation and models)activity diagram,automated test generationgenetic algorithm,model-based testingmodel-based testing,search-based software engineeringautomated test generation,model transformationconstraint solving,model-based testingalloy,automated test generationautomated test generation,object-orientedevolutionary testing,object-orientedgenetic algorithm,object-orientedobject-oriented,search-based software engineeringaccess control,automated test generation(test generation and test prioritization)automated test generation,test case reductionautomated test generation,test suite reductiongenetic algorithm,test suite minimizationcombinatorial testing,test case selectioncovering arrays,test case prioritizationmulti-objective optimization,test case prioritizationparticle swarm optimization,test case prioritizationsearch-based software testing,test case prioritizationsearch-based software testing,test case selectionfault localization,test case prioritization(test generation and regression testing)regression testing,search-based software engineeringregression testing,search-based software testingmulti-objective optimization,regression testingcombinatorial testing,regression testing(test generation and evolution)automated test generation,software maintenancefault localization,software evolutionautomated test generation,program slicingautomated test generation,service-oriented architecture• The use of specific optimization algorithms for test case prioritization.

In some cases, “under-explored” coincides with “emerging”—for example, test or-

acles with machine learning and web services. There are also cases where topics are

well-connected in research (e.g., test generation and white-box testing) through dif-

ferent keywords (e.g., “coverage criteria” instead of “code coverage”). We retained

keywords with minor differences in meaning, as even minor distinctions may be im-

portant. However, some connections may be well-explored under a different keyword.

Even in such cases, there may be opportunity for further exploration related to these

keyword differences, or connections based on concepts and technologies than have not

been explored previously (e.g., specific generation algorithms or coverage criteria).

Identifying new connections between keywords: The absence of a connection be-

tween two keywords does not imply that the concepts cannot be connected. Consider

keywords within a single cluster. Keywords lacking a direct connection may repre-

sent entirely incompatible concepts. However, in other cases, there may be a natural

synergy between the two concepts that had not yet been considered. While the map

cannot directly inform researchers which keywords can be connected, or how they can

be connected, it can serve as a means to prompt brainstorming.

As an example, we can inspect keywords within a cluster that lack a direct connec-

tion to specific other keywords in their cluster. Cluster 8 (evolution and maintenance,

with subtopics of regression testing and test prioritization) contains 19 keywords. There

are 180 cases where two keywords lack a direct link within Cluster 8—e.g., “change

impact analysis” and “test case reduction” are not directly connected in publications.

Not all of these cases offer obvious ideas for new research, but consideration of

these cases may lead to inspiration. For example, we identified the following ideas:

• The use of change impact analysis as part of program comprehension, test case

reduction, test suite minimization, or test suite reduction.

• The use of information retrieval and natural language processing to provide in-

formation for test case and suite reduction, selection, and minimization and for

regression test selection.

42

• The use of regression test selection techniques for use as part of test case and

suite reduction and test case selection.

• The use of program comprehension techniques for regression test selection.

• The relationship between evolution and maintenance of software with test case

prioritization, minimization, and reduction.

• Service-oriented architecture and web services appear in this cluster because of

close association with particular keywords (e.g., regression testing), but are only

indirectly connected to the majority of the other keywords. The missing con-

nections suggest the need for targeted test case prioritization, selection, reduc-

tion, and minimization approaches for service-oriented architectures and web

services, as well as examination of the evolution and maintenance of service-

oriented architectures and web services.

Similar ideas may emerge from inspecting missing connections within other clusters.

There are many ways that this map could potentially be analyzed beyond the simple

exploration in this section. We suggest that researchers attempt to analyze different

connection types, connection strength thresholds, and other aspects of the collected

metadata (e.g., publication age or number of citations) in order to gain inspiration for

new research or insight into the field.

6. Threats to Validity

Conclusion Validity: VOSviewer was used to perform visualization. The design of

this tool and the visualizations it produces could potentially bias the observations made.

However, the tool is based on well-understood and established computational princi-

ples. Further, it has been used in over 500 bibliometric studies (e.g., [20, 17]), in a

large variety of fields and its assumptions have been verified by experts in these fields.6

We have made efforts to verify the assumptions behind the analyses performed.

6A full list of publications is maintained at https://www.vosviewer.com/publications.

43

External Validity: Our study examined publications from the Scopus database, po-

tentially omitting relevant venues for software testing research. Scopus is the one of

the most comprehensive databases covering research publications [89], indexing con-

tent from 24,600 active conferences or journals and 5,000 publishers.7 Specifically,

Scopus coverage for computer science research has been found to be better than other

databases [90]. Scopus also enables efficient export of the data we use to perform our

mapping. Although some venues may not be indexed, many of the most important

journals and conferences in the software testing field are included.

We used a single search string to build our sample. Other search strings (e.g., “soft-

ware test”) could have complemented the search process. However, our goal is not to

capture all studies ever published in software testing. Rather, we require a sufficiently

representative sample. We hypothesize that the additional value would be minimal

compared to the filtering effort required. We believe that our sample of 57,233 publi-

cations is sufficiently large and representative to perform this analysis.

Internal Validity: We based our analysis on publications retrieved using the term

“software testing”. This pool of papers included publications unrelated to software

testing, e.g., the use of software to test hardware or as part of student examination.

We performed a manual process to remove unrelated keywords from the mapping.

However, it is possible that some publications remain that are unrelated to the targeted

research field. We believe that these are not enough to influence our observations.

Our analysis is based on author-supplied keywords, and not other sources of topic

information, e.g., titles or abstracts. The use of keywords introduces a risk that publi-

cations are mislabeled (e.g., authors used the wrong term), or that important concepts

are omitted. Still, author-supplied keywords are a clear and appropriate means to cap-

ture the structure of software testing research. Author-supplied keywords are regularly

used in other bibliometric analyses [20, 91, 19] and have been found to effectively

reflect structures in research fields [19, 92]. Even if relevant keywords are omitted,

the concepts the authors felt were most important are reflected. While there is poten-

7List of covered journals and conferences: https://www.scopus.com/sources.uri.

44

tial inaccuracy, it is likely that the selected keywords are close to correct. Alternative

methods carry similar risks. Automated or external categorization can also be inaccu-

rate and potentially violates the intent of authors. Other sources of information, such

as titles or abstracts, introduce noise and are difficult to use to categorize publications.

We applied a threshold of a minimum of 20 studies before a keyword appeared

in our dataset or map. We used this threshold to omit minor or highly obscure key-

words and to control the level of noise in the map. This risks also omitting emerging

keywords. We tried lower and higher thresholds then we concluded that the current

threshold is enough to cover terms with lower frequency and provide a meaningful and

lower scatter network of the keywords. It should be noted when we tested lower and

higher thresholds, the overall patterns did not change significantly.

7. Conclusion

Testing is the primary means of assessing software correctness and quality. Re-

search in software testing is growing and rapidly-evolving. Based on the keywords

assigned to publications, we seek to identify predominant research topics and under-

stand how they are connected and have evolved.

We have applied co-word analysis to characterize the topology of software testing

research over four decades of research publications. In this map, nodes represent key-

words, while edges indicate that publications have co-targeted keywords. Nodes are

clustered based on density and strength of edges. We examined the most common key-

words, summarized clusters into research topics, examined how clusters connect, and

identified emerging and declining keywords, topics, and connections.

We found that the most popular keywords tend to relate to automation, test creation

and assessment guidance, assessment of system quality, and cyber-physical systems.

The clusters of keywords suggest that software testing research can be divided into

16 core topics. All topics are connected, but creation guidance, automated test gen-

eration, evolution and maintenance, and test oracles have particularly strong connec-

tions to other topics, highlighting their multidisciplinary nature. Emerging keywords

and topics relate to web and mobile applications, machine learning, energy consump-

45

tion, automated program repair and test generation, while emerging connections have

formed between web applications, test oracles, and machine learning with many topics.

Random and requirements-based testing show evidence of decline.

These insights and the underlying map can inspire researchers in software testing by

clarifying concepts and their relationships, or by facilitating analyses of the field (e.g.,

through identification of under-explored and missing connections). In future work, we

will broaden the type and scope of analyses of this map data, and we make our data

available so that others can do so as well.

References

[1] M. Pezze, M. Young, Software Test and Analysis: Process, Principles, and Tech-

niques, John Wiley and Sons, 2006.

[2] A. Turing, Checking a Large Routine, MIT Press, Cambridge, MA, USA, 1989,

p. 70–72.

[3] A. Orso, G. Rothermel, Software testing: A research travelogue (2000–2014),

in: Proceedings of the on Future of Software Engineering, FOSE 2014, ACM,

New York, NY, USA, 2014, pp. 117–132. URL: http://doi.acm.org/10.

1145/2593882.2593885. doi:10.1145/2593882.2593885.

[4] S. Fortunato, C. T. Bergstrom, K. B¨orner,

J. A. Evans, D. Helbing,

S. Milojevi´c, A. M. Petersen, F. Radicchi, R. Sinatra, B. Uzzi, A. Vespig-

nani, L. Waltman, D. Wang, A.-L. Barab´asi,

Science of science,

Sci-

ence

359

(2018). URL:

https://science.sciencemag.org/

content/359/6379/eaao0185.

doi:10.1126/science.aao0185.

arXiv:https://science.sciencemag.org/content/359/6379/eaao0185.full.pdf.

[5] C.

L. Borgman,

J.

Furner,

Scholarly

communication

bibliometrics,

Annual Review of

Information

Science

and

and

Technology

36

(2002)

2–72.

URL:

https://asistdl.

onlinelibrary.wiley.com/doi/abs/10.1002/aris.

46

1440360102. doi:https://doi.org/10.1002/aris.1440360102.

arXiv:https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/aris.1440360102.

[6] H. F. Moed, Citation analysis in research evaluation, volume 9, Springer Science

& Business Media, 2006.

[7] Y. Ding, X. Liu, C. Guo, B. Cronin,

The distribution of references

across texts: Some implications for citation analysis,

Journal of Infor-

metrics 7 (2013) 583–592. URL: https://www.sciencedirect.

com/science/article/pii/S1751157713000230.

doi:https:

//doi.org/10.1016/j.joi.2013.03.003.

[8] A. Pritchard, et al., Statistical bibliography or bibliometrics, Journal of documen-

tation 25 (1969) 348–349.

[9] N. De Bellis, Bibliometrics and citation analysis: from the science citation index

to cybermetrics, scarecrow press, 2009.

[10] N. Donthu, S. Kumar, D. Pattnaik, Forty-five years of journal of business re-

search: a bibliometric analysis, Journal of Business Research 109 (2020) 1–14.

[11] N. J. Van Eck, L. Waltman, Visualizing bibliometric networks,

in: Measuring

scholarly impact, Springer, 2014, pp. 285–320.

[12] H. Peters, A. van Raan,

Co-word-based science maps of chemical engi-

neering. part

i: Representations by direct multidimensional scaling,

Re-

search Policy 22 (1993) 23–45. URL: https://www.sciencedirect.

com/science/article/pii/004873339390031C.

doi:https:

//doi.org/10.1016/0048-7333(93)90031-C.

[13] E. Whittaker, A History of the Theories of Aether and Electricity: Vol. I: The

Classical Theories; Vol. II: The Modern Theories, 1900-1926, volume 1, Courier

Dover Publications, 1989.

[14] H.-N. Su, P.-C. Lee, Mapping knowledge structure by keyword co-occurrence:

a first look at journal papers in technology foresight, Scientometrics 85 (2010)

65–79.

47

[15] L. M. Romo-Fern´andez, V. P. Guerrero-Bote, F. Moya-Aneg´on, Co-word based

thematic analysis of renewable energy (1990–2010), Scientometrics 97 (2013)

743–765.

[16] W. Marx, R. Haunschild, L. Bornmann, Global warming and tea produc-

tion—the bibliometric view on a newly emerging research topic, Climate 5

(2017). URL: https://www.mdpi.com/2225-1154/5/3/46. doi:10.

3390/cli5030046.

[17] E. Mohammadi, Knowledge mapping of the iranian nanoscience and technology:

a text mining approach, Scientometrics Scientometrics 92 (01 Sep. 2012) 593 –

608. URL: https://akjournals.com/view/journals/11192/92/

3/article-p593.xml. doi:10.1007/s11192-012-0644-6.

[18] Y. Liu, J. Goncalves, D. Ferreira, B. Xiao, S. Hosio, V. Kostakos, Chi 1994-2013:

Mapping two decades of intellectual progress through co-word analysis, in: Pro-

ceedings of the SIGCHI Conference on Human Factors in Computing Systems,

CHI ’14, Association for Computing Machinery, New York, NY, USA, 2014,

p. 3553–3562. URL: https://doi.org/10.1145/2556288.2556969.

doi:10.1145/2556288.2556969.

[19] H. Liao, M. Tang, L. Luo, C. Li, F. Chiclana, X.-J. Zeng,

A biblio-

metric analysis and visualization of medical big data research, Sustainabil-

ity 10 (2018). URL: https://www.mdpi.com/2071-1050/10/1/166.

doi:10.3390/su10010166.

[20] E. Mohammadi, A. Karami,

Exploring research trends in big data

across disciplines: A text mining analysis,

Journal of Information Sci-

ence

0

(2020)

0165551520932855. URL: https://doi.org/10.

1177/0165551520932855.

doi:10.1177/0165551520932855.

arXiv:https://doi.org/10.1177/0165551520932855.

[21] V. Garousi, M. V. M¨antyl¨a,

Citations, research topics and active coun-

tries in software engineering: A bibliometrics study,

Computer Science

48

Review 19

(2016)

56–77. URL: https://www.sciencedirect.

com/science/article/pii/S1574013715300654.

doi:https:

//doi.org/10.1016/j.cosrev.2015.12.002.

[22] V. Garousi, J. M. Fernandes,

Quantity versus impact of software en-

gineering papers:

a quantitative study,

Scientometrics 112 (2017)

963–1006. URL: https://doi.org/10.1007/s11192-017-2419-6.

doi:10.1007/s11192-017-2419-6.

[23] D. Karanatsiou, Y. Li, E.-M. Arvanitou, N. Misirlis, W. E. Wong,

A

bibliometric assessment of software engineering scholars and institutions

(2010–2017),

Journal of Systems and Software 147 (2019) 246–261. URL:

https://www.sciencedirect.com/science/article/pii/

S0164121218302334.

doi:https://doi.org/10.1016/j.jss.

2018.10.029.

[24] W. E. Wong, T. Tse, R. L. Glass, V. R. Basili, T. Chen,

An as-

sessment of systems and software engineering scholars and institutions

(2001–2005),

Journal of Systems and Software 81 (2008) 1059–1062. URL:

https://www.sciencedirect.com/science/article/pii/

S0164121207002300.

doi:https://doi.org/10.1016/j.jss.

2007.09.018, agile Product Line Engineering.

[25] W. E. Wong, T. Tse, R. L. Glass, V. R. Basili, T. Chen,

An as-

sessment of systems and software engineering scholars and institutions

(2002–2006),

Journal of Systems and Software 82 (2009) 1370–1373. URL:

https://www.sciencedirect.com/science/article/pii/

S0164121209001265.

doi:https://doi.org/10.1016/j.jss.

2009.06.018, sI: Architectural Decisions and Rationale.

[26] W. E. Wong, T. Tse, R. L. Glass, V. R. Basili, T. Chen, An assessment of

systems and software engineering scholars and institutions (2003–2007 and

2004–2008),

Journal of Systems and Software 84 (2011) 162–168. URL:

https://www.sciencedirect.com/science/article/pii/

49

S0164121210002682.

doi:https://doi.org/10.1016/j.jss.

2010.09.036, information Networking and Software Services.

[27] V. Garousi, T. Varma, A bibliometric assessment of canadian software engineer-

ing scholars and institutions (1996-2006), Computer and Information Science 3

(2010) 19.

[28] V. Garousi,

A bibliometric

analysis of

the

turkish software

en-

gineering research community,

Scientometrics 105 (2015) 23–49.

URL:

https://doi.org/10.1007/s11192-015-1663-x.

doi:10.1007/s11192-015-1663-x.

[29] R. Farhoodi, V. Garousi, D. Pfahl,

J. Sillito,

Development of sci-

entific software: A systematic mapping, a bibliometrics study, and a

paper

repository,

International

Journal of Software Engineering and

Knowledge Engineering 23 (2013) 463–506. URL: https://doi.org/

10.1142/S0218194013500137. doi:10.1142/S0218194013500137.

arXiv:https://doi.org/10.1142/S0218194013500137.

[30] F. G. de Freitas, J. T. de Souza, Ten years of search based software engineering:
A bibliometric analysis, in: M. B. Cohen, M. ´O Cinn´eide (Eds.), Search Based

Software Engineering, Springer Berlin Heidelberg, Berlin, Heidelberg, 2011, pp.

18–32.

[31] M. J. Harrold, Testing: A roadmap, in: Proceedings of the Conference on The Fu-

ture of Software Engineering, ICSE ’00, Association for Computing Machinery,

New York, NY, USA, 2000, p. 61–72. URL: https://doi.org/10.1145/

336512.336532. doi:10.1145/336512.336532.

[32] A. Bertolino, Software testing research: Achievements, challenges, dreams, in:

Future of Software Engineering (FOSE ’07), 2007, pp. 85–103. doi:10.1109/

FOSE.2007.25.

[33] N. J. Van Eck, L. Waltman, Software survey: Vosviewer, a computer program for

bibliometric mapping, scientometrics 84 (2010) 523–538.

50

[34] N.

J.

van Eck,

L. Waltman, Visualizing Bibliometric Networks,

Springer

International Publishing, Cham,

2014,

pp. 285–320. URL:

https://doi.org/10.1007/978-3-319-10377-8_13.

doi:10.1007/978-3-319-10377-8_13.

[35] S. Anand, E. K. Burke, T. Y. Chen, J. Clark, M. B. Cohen, W. Grieskamp, M. Har-

man, M. J. Harrold, P. McMinn, An orchestrated survey of methodologies for

automated software test case generation, Journal of Systems and Software 86

(2013) 1978–2001.

[36] B. Korel, A. M. Al-Yami, Automated regression test generation,

in: Pro-

ceedings of the 1998 ACM SIGSOFT International Symposium on Software

Testing and Analysis, ISSTA ’98, ACM, New York, NY, USA, 1998, pp. 143–

152. URL: http://doi.acm.org/10.1145/271771.271803. doi:10.

1145/271771.271803.

[37] R. Just, The major mutation framework: Efficient and scalable mutation anal-

ysis for java,

in: Proceedings of the 2014 International Symposium on Soft-

ware Testing and Analysis, ISSTA 2014, ACM, New York, NY, USA, 2014, pp.

433–436. URL: http://doi.acm.org/10.1145/2610384.2628053.

doi:10.1145/2610384.2628053.

[38] M. Fewster, D. Graham, Software test automation, Addison-Wesley Reading,

1999.

[39] M. Mitchell, An Introduction to Genetic Algorithms, MIT Press, Cambridge,

MA, USA, 1998.

[40] J. Voas, Fault injection for the masses, Computer 30 (1997) 129–130. doi:10.

1109/2.642820.

[41] B. Kitchenham, S. Pfleeger, Software quality: the elusive target [special issues

section], IEEE Software 13 (1996) 12–21. doi:10.1109/52.476281.

51

[42] W. Herzner, R. Schlick, A. Le Guennec, B. Martin, Model-based simulation of

distributed real-time applications, in: 5th IEEE Int’l Conf. on Industrial Infomat-

ics, 2007, pp. 989 – 994.

[43] A. K. Mok, D. Stuart, Simulation vs. verification: Getting the best of both worlds,

in: Proceedings of the Eleventh Annual Conf. on Computer Assurance, COM-

PASS 96, 1996.

[44] G. Gay, S. Rayadurgam, M. P. E. Heimdahl, Automated steering of model-based

test oracles to admit real program behaviors,

IEEE Transactions on Software

Engineering 43 (2017) 531–555. doi:10.1109/TSE.2016.2615311.

[45] M. Crossley, The Desk Reference of Statistical Quality Methods, ASQ Quality

Press, 2000.

[46] G. Rothermel, M. J. Harrold, J. von Ronne, C. Hong, Empirical studies of test-

suite reduction, Software Testing, Verification and Reliability 12 (2002) 219–

249. URL: http://dx.doi.org/10.1002/stvr.256. doi:10.1002/

stvr.256.

[47] G. Gay, M. Staats, M. Whalen, M. Heimdahl, The risks of coverage-directed

test case generation, Software Engineering, IEEE Transactions on PP (2015).

doi:10.1109/TSE.2015.2421011.

[48] C. Nie, H. Leung,

A survey of combinatorial

testing,

ACM Com-

put. Surv. 43 (2011). URL: https://doi.org/10.1145/1883612.

1883618. doi:10.1145/1883612.1883618.

[49] E. Barr, M. Harman, P. McMinn, M. Shahbaz, S. Yoo, The oracle problem in soft-

ware testing: A survey, IEEE Transactions on Software Engineering 41 (2015)

507–525. doi:10.1109/TSE.2014.2372785.

[50] B. Turhan, T. Menzies, A. B. Bener, J. Di Stefano, On the relative value of cross-

company and within-company data for defect prediction, Empirical Software

Engineering 14 (2009) 540–578.

52

[51] A. Salahirad, H. Almulla, G. Gay,

Choosing the fitness

func-

tion

for

the

job:

Automated

generation

of

test

suites

that

de-

tect

real

faults,

Software Testing, Verification and Reliability 29

(2019)

e1701.

URL:

https://onlinelibrary.wiley.com/

doi/abs/10.1002/stvr.1701.

doi:10.1002/stvr.1701.

arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1701,

e1701 stvr.1701.

[52] C. Cadar, D. Dunbar, D. Engler, Klee: Unassisted and automatic generation

of high-coverage tests for complex systems programs,

in: Proceedings of the

8th USENIX Conference on Operating Systems Design and Implementation,

OSDI’08, USENIX Association, Berkeley, CA, USA, 2008, pp. 209–224. URL:

http://dl.acm.org/citation.cfm?id=1855741.1855756.

[53] J. Zander, I. Schieferdecker, P. J. Mosterman, Model-based testing for embedded

systems, CRC press, 2011.

[54] T. Fukuda, Theory and applications of neural networks for industrial control

systems, IEEE Transactions on Industrial Electronics (1992) 472–489.

[55] A. van Lamsweerde, Engineering requirements for system reliability and security,

Software System Reliability and Security 9 (2007).

[56] M. A. Jamil, M. Arif, N. S. A. Abubakar, A. Ahmad, Software testing techniques:

A literature review,

in: 2016 6th international conference on information and

communication technology for the Muslim world (ICT4M), IEEE, 2016, pp. 177–

182.

[57] A. Zeller, R. Hildebrandt, Simplifying and isolating failure-inducing input, IEEE

Transactions on Software Engineering 28 (2002) 183–200. doi:10.1109/32.

988498.

[58] E. Weyuker, B. Jeng, Analyzing partition testing strategies,

IEEE Trans. on

Software Engineering 17 (1991) 703–711.

53

[59] L. Morell, A theory of fault-based testing, IEEE Transactions on Software Engi-

neering 16 (1990) 844–857.

[60] S. Yoo, M. Harman, Regression testing minimization, selection and prioritization:

a survey, Software testing, verification and reliability 22 (2012) 67–120.

[61] A. Arcuri, L. C. Briand, Adaptive random testing: An illusion of effectiveness?,

in: ISSTA, 2011.

[62] C. Catal, Software fault prediction: A literature review and current trends, Expert

systems with applications 38 (2011) 4626–4636.

[63] J. Chen, J. Patra, M. Pradel, Y. Xiong, H. Zhang, D. Hao, L. Zhang, A survey of

compiler testing, ACM Computing Surveys (CSUR) 53 (2020) 1–36.

[64] J. Rumbaugh, I. Jacobson, G. Booch, The Unified Modeling Language Reference

Manual, Object Technology Series, Addison-Wesley, 1998.

[65] L. Copeland, A practitioner’s guide to software test design, Artech House, 2004.

[66] G. Rothermel, M. J. Harrold, Analyzing regression test selection techniques,

IEEE Transactions on software engineering 22 (1996) 529–551.

[67] B. S. Ahmed, E. Enoiu, W. Afzal, K. Z. Zamli, An evaluation of monte carlo-

based hyper-heuristic for interaction testing of industrial embedded software ap-

plications, Soft Computing 24 (2020) 13929–13954.

[68] D. Jackson, Alloy: a language and tool for exploring software designs, Commu-

nications of the ACM 62 (2019) 66–76.

[69] A. Zeller, Automated debugging: Are we close?, Computer 34 (2001) 26–31.

[70] T. Y. Chen, F.-C. Kuo, R. G. Merkel, T. Tse, Adaptive random testing: The art of

test case diversity, Journal of Systems and Software 83 (2010) 60–66.

[71] T. Su, K. Wu, W. Miao, G. Pu, J. He, Y. Chen, Z. Su, A survey on data-flow

testing, ACM Computing Surveys (CSUR) 50 (2017) 1–35.

54

[72] RTCA/DO-178C, Software considerations in airborne systems and equipment

certification, 2012.

[73] S. Hong, J. Ahn, S. Park, M. Kim, M. J. Harrold, Testing concurrent programs to

achieve high synchronization coverage, in: Proceedings of the 2012 International

Symposium on Software Testing and Analysis, 2012, pp. 210–220.

[74] D. Douglas-Smith, T. Iwanaga, B. F. Croke, A. J. Jakeman, Certain trends in un-

certainty and sensitivity analysis: An overview of software tools and techniques,

Environmental Modelling & Software 124 (2020) 104588.

[75] M. Bozkurt, M. Harman, Y. Hassoun, et al., Testing web services: A survey,

Department of Computer Science, King’s College London, Tech. Rep. TR-10-01

(2010).

[76] I. A. Qureshi, A. Nadeem, Gui testing techniques: a survey, International Journal

of Future computer and communication 2 (2013) 142.

[77] E. Murphy-Hill, C. Parnin, A. P. Black, How we refactor, and how we know

it, IEEE Transactions on Software Engineering 38 (2012) 5–18. doi:10.1109/

TSE.2011.41.

[78] M. Martinez, T. Durieux, R. Sommerard, J. Xuan, M. Monperrus, Au-

tomatic repair of real bugs in java:

a large-scale experiment on the de-

fects4j dataset,

Empirical Software Engineering 22 (2017) 1936–1964.

URL: https://doi.org/10.1007/s10664-016-9470-4. doi:10.

1007/s10664-016-9470-4.

[79] M. Helali Moghadam, M. Saadatmand, M. Borg, M. Bohlin, B. Lisper, Machine

learning to guide performance testing: An autonomous test framework, in: 2019

IEEE International Conference on Software Testing, Verification and Validation

Workshops (ICSTW), 2019, pp. 164–167.

[80] M. Whalen, A. Rajan, M. Heimdahl, Coverage metrics for requirements-based

testing, in: Proceedings of Int’l Symposium on Software Testing and Analysis,

ACM, 2006, pp. 25–36.

55

[81] A. Takanen, J. DeMott, C. Miller, Fuzzing for Software Security Testing and

Quality Assurance, Artech House, Inc., 2008.

[82] A. S. E. M. Clarke, E.A. Emerson, Automatic verification of finite-state concur-

rent systems using temporal logic specifications, ACM Transactions on Program-

ming Languages and Systems (1986) 244–263.

[83] N. Alshahwan, X. Gao, M. Harman, Y. Jia, K. Mao, A. Mols, T. Tei, I. Zorin, De-

ploying search based software engineering with sapienz at facebook, in: Search-

Based Software Engineering, Springer International Publishing, Cham, 2018, pp.

3–45.

[84] R. France, B. Rumpe, Model-driven development of complex systems: A re-

search roadmap, in: L. Briand, A. Wolf (Eds.), Future of Software Engineering

2007, IEEE-CS Press, 20007.

[85] L. Bass, P. Clements, R. Kazman, Software Architecture in Practice, Addison-

Wesley, 1998.

[86] R. Just, F. Schweiggert, G. M. Kapfhammer, Major: An efficient and exten-

sible tool for mutation analysis in a java compiler,

in: Proceedings of the

2011 26th IEEE/ACM International Conference on Automated Software Engi-

neering, ASE ’11, IEEE Computer Society, Washington, DC, USA, 2011, pp.

612–615. URL: http://dx.doi.org/10.1109/ASE.2011.6100138.

doi:10.1109/ASE.2011.6100138.

[87] A. Fontes, G. Gay, Using machine learning to generate test oracles: A system-

atic literature review, in: Proceedings of the 1st International Workshop on Test

Oracles, TORACLE 2021, Association for Computing Machinery, New York,

NY, USA, 2021, p. 1–10. URL: https://doi.org/10.1145/3472675.

3473974. doi:10.1145/3472675.3473974.

[88] J. P. Galeotti, G. Fraser, A. Arcuri,

Improving search-based test suite gen-

eration with dynamic symbolic execution,

in: 2013 IEEE 24th International

56

Symposium on Software Reliability Engineering (ISSRE), 2013, pp. 360–369.

doi:10.1109/ISSRE.2013.6698889.

[89] M. Thelwall, P. Sud, Scopus 1900–2020: Growth in articles, abstracts, countries,

fields, and journals, Quantitative Science Studies (2022) 1–14. URL: https:

//doi.org/10.1162/qss_a_00177.

doi:10.1162/qss_a_00177.

arXiv:https://direct.mit.edu/qss/article-pdf/doi/10.1162/qss a 00177/1992507/qss a 00177.pdf.

[90] A. Cavacini, What is the best database for computer science journal articles?, Sci-

entometrics 102 (2015) 2059–2071. URL: https://doi.org/10.1007/

s11192-014-1506-1. doi:10.1007/s11192-014-1506-1.

[91] H. Jamali, C. Steel, E. Mohammadi, Wine research and its relationship

with wine production:

a scientometric analysis of global

trends,

Aus-

tralian Journal of Grape and Wine Research 26 (2020) 130–138. URL:

https://onlinelibrary.wiley.com/doi/abs/10.1111/

ajgw.12422.

doi:https://doi.org/10.1111/ajgw.12422.

arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajgw.12422.

[92] H. Li, H. An, Y. Wang, J. Huang, X. Gao, Evolutionary features of academic

articles co-keyword network and keywords co-occurrence network: Based

on two-mode affiliation network,

Physica A: Statistical Mechanics and its

Applications 450 (2016) 657–669. URL: https://www.sciencedirect.

com/science/article/pii/S037843711600025X.

doi:https:

//doi.org/10.1016/j.physa.2016.01.017.

[93] I. Borg, P. J. Groenen, Modern multidimensional scaling: Theory and applica-

tions, Springer Science & Business Media, 2005.

[94] M. E. Newman, Fast algorithm for detecting community structure in networks,

Physical review E 69 (2004) 066133.

[95] L. Waltman, N. J. van Eck,

A smart

local moving algorithm for

large-scale modularity-based community detection,

The European Physi-

57

cal Journal B 86 (2013) 471. URL: https://doi.org/10.1140/epjb/

e2013-40829-0. doi:10.1140/epjb/e2013-40829-0.

Appendix A. VOSViewer Technical Details

VOSviewer produces maps based on a co-occurrence matrix—a two-dimensional

matrix where each column and row represents an item—a keyword, in our case–and

each cell indicates the number of times two keywords co-occur. This map construction

consists of three steps.

In the first step, a similarity matrix is created from the co-

occurrence matrix. A map is then formed by applying the VOS mapping technique to

the similarity matrix. Finally, the map is translated, rotated, and reflected.

Forming the similarity matrix: VOSviewer takes as input a similarity matrix. This

similarity matrix is obtained from the co-occurrence matrix through normalization.

Normalization is done by correcting the matrix for differences in the total number of

occurrences or co-occurrences of keywords. VOSviewer uses the association strength

as its similarity measure [34]—in this case, the number of publications where two key-

words are targeted together. Using the association strength, the similarity si,j between

two keywords i and j is calculated as:

si,j =

2mci,j
wiwj

(A.1)

where m represents the total weight of all edges in the network (the total number of

co-occurrences of all keywords), ci,j denotes the weight of the edge between keywords

i and j (the total number of co-occurrences of the keywords), and wi and wj denote

the total weight of all edges of keywords i or j (the total number of occurrences of

keywords i or j and the total number of co-occurrences of these keywords with all

keywords that they co-occur with). Specifically:

wi =

∑︂

ci,j

j

m =

1
2

∑︂

wi

i

58

(A.2)

(A.3)

The similarity between keywords i and j calculated using Equation A.1 is proportional

to the ratio between the observed number of co-occurrences of keywords i and j and

the expected number of co-occurrences of keywords i and j under the assumption that

occurrences of the two keywords are independent.

Map formation: The VOS mapping technique constructs a two-dimensional map in

which keywords 1, ..., n (where n is the total number of keywords) are placed such

that the distance between any pair of keywords i and j reflects their similarity si,j as

accurately as possible. Keywords with a high similarity are located close to each other,

while keywords with a low similarity are located far from each other.

The goal of the VOS mapping technique is to minimize the weighted sum of the

squared Euclidean distances between all pairs of keywords [34]. The higher the simi-

larity between the two keywords, the higher the weight of their squared distance in the

summation. The specific function minimized by the mapping technique is:

V (x1, ..., xn) =

si,j∥xi − xj∥2

∑︂

i<j

(A.4)

where xi denotes the location of keyword i in a two-dimensional space, and where

∥xi − xj∥ denotes the Euclidean distance between keywords i and j. To avoid trivial

maps in which all keywords have the same location, minimization is subject to the con-

straint that the average distance between two keywords must be equal to 1. Specifically:

2
n(n − 1)

∑︂

i<j

∥xi − xj∥ = 1

(A.5)

The constrained optimization problem—minimizing Equation A.4, subject to Equa-

tion A.5—is solved in two steps [33]. The constrained problem is first converted into

an unconstrained problem. Second, the unconstrained problem is solved using a vari-

ant of the SMACOF algorithm, an optimization algorithm commonly used in multidi-

mensional scaling to produce human-understandable network or graph layouts through

minimization of a stress function over the positions of nodes in the graph [93].

Clustering of Keywords: Keywords are assigned to clusters, and the number of clus-

ters is determined, through optimization. This is a common approach for clustering

nodes in a network [94]. Potential assignments of keywords to clusters are assessed

59

using the function:

V (c1, ..., cn) =

∑︂

i<j

δ(ci, cj)(si,j − γ)

(A.6)

where ci is the cluster that keyword i has been assigned to. δ(ci, cj) is a difference

function that yields 1 if ci = cj and 0, otherwise. γ determines the level of clustering,

with higher values yielding a larger number of clusters. This equation is unified with

the mapping function minimized in Equation A.4, and includes the same similarity

measurement si,j calculated in Equation A.1.

There is no maximum number of keywords per cluster. The minimum number

of keywords is controlled using a user-specified parameter. We used the default, a

minimum of one keyword. The clustering algorithm will merge small clusters in cases

where merging does not affect the result of Equation A.6. Therefore, any small cluster

that remain are ones that affect the results of the equation.

Equation A.6 is maximized using the smart local moving algorithm [95]. Following

the optimization, the assignment of keywords to clusters that maximizes Equation A.6

is returned. This process yields a small number of clusters containing keywords that

are targeted disproportionately often together in publications.

Translation, rotation, and reflection: The optimization problem introduced in Equa-

tion A.4 does not have a single global optimal solution [33]. However, consistent

results are desirable. To ensure that the same co-occurrence matrix always yields the

same map, three transformations are applied after optimization:

• Translation: The solution is translated to be centered at the origin.

• Rotation: Principle component analysis is applied in order to maximize variance

on the horizontal dimension.

• Reflection: If the median of x1,1, ..., xn,1 is larger than 0, the solution is reflected

in the vertical axis. If the median of x1,2, ..., xn,2 is larger than 0, the solution is

reflected in the horizontal axis.

60

