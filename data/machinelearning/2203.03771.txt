2
2
0
2

r
a

M
7

]

G
L
.
s
c
[

1
v
1
7
7
3
0
.
3
0
2
2
:
v
i
X
r
a

STATIC PREDICTION OF RUNTIME ERRORS
BY LEARNING TO EXECUTE PROGRAMS
WITH EXTERNAL RESOURCE DESCRIPTIONS

David Bieber
Google Research
dbieber@google.com

Rishab Goel
Mila
rgoel0112@gmail.com

Daniel Zheng
Google Research
danielzheng@google.com

Hugo Larochelle
Google Research
hugolarochelle@google.com

Daniel Tarlow
Google Research
dtarlow@google.com

ABSTRACT

The execution behavior of a program often depends on external resources, such
as program inputs or ﬁle contents, and so cannot be run in isolation. Neverthe-
less, software developers beneﬁt from fast iteration loops where automated tools
identify errors as early as possible, even before programs can be compiled and
run. This presents an interesting machine learning challenge: can we predict run-
time errors in a “static” setting, where program execution is not possible? Here,
we introduce a real-world dataset and task for predicting runtime errors, which
we show is difﬁcult for generic models like Transformers. We approach this task
by developing an interpreter-inspired architecture with an inductive bias towards
mimicking program executions, which models exception handling and “learns to
execute” descriptions of the contents of external resources. Surprisingly, we show
that the model can also predict the location of the error, despite being trained only
on labels indicating the presence/absence and kind of error. In total, we present
a practical and difﬁcult-yet-approachable challenge problem related to learning
program execution and we demonstrate promising new capabilities of interpreter-
inspired machine learning models for code.

1

INTRODUCTION

We investigate applying neural machine learning methods to the static analysis of source code for
the prediction of runtime errors. The execution behavior of a program is in general not fully deﬁned
by its source code in isolation, because programs often rely on external resources like inputs, the
contents of ﬁles, or the network. Nevertheless, software developers beneﬁt from fast iteration loops
where automated tools identify errors early, even when program execution is not yet an option.
Therefore we consider the following machine learning challenge: can we predict runtime errors in a
“static” setting, where program execution is not possible?

Recent work has made considerable progress toward applying machine learning to learning for code
tasks (Allamanis et al., 2018a). Large language models have begun showing promise in source
code generation, but have struggled on tasks that require reasoning about the execution behavior
of programs (Chen et al., 2021a; Austin et al., 2021), requiring detailed step-by-step supervision to
make headway on the task (Anonymous, 2022). We introduce the runtime error prediction task as a
challenge problem, and in line with these ﬁndings, observe that it is a challenging task for generic
models like Transformers.

This runtime error prediction task, along with the corresponding dataset that we introduce, is well
suited as a challenge problem because it is difﬁcult-yet-approachable, has immediate real-world
value for software developers, requires novel modeling considerations that we hypothesize will be
applicable to a range of learning for code tasks, and with this work, now has a suitable large dataset
of real-world code. The task is to learn from data to predict whether provided source code is liable

1

 
 
 
 
 
 
to exhibit a runtime error when it is run; even when static analysis cannot provide guarantees of an
error in the code, patterns learned from data may point to likely errors. Our dataset, Python Runtime
Errors, consists of 2.4 million Python 3 programs written by competitive programmers selected
from Project CodeNet (Puri et al., 2021). We have run all programs in a sandboxed environment
on sample inputs to determine their error classes, ﬁnding the programs exhibit 26 distinct error
classes including “no error”. Each program relies on an external resource, the stdin input stream,
and we pair the programs with a natural language description of the behavior of the stream. We
make the task and prepared dataset, along with all models considered in this work, available for the
research community to facilitate reproduction of this work and further research1. To make progress
on this challenging task, we identify a promising class of models from prior work, interpreter-
inspired models, and we demonstrate they perform well on the task. Instruction Pointer Attention
Graph Neural Network (IPA-GNN) (Bieber et al., 2020) models simulate the execution of a program,
following its control ﬂow structure, but operating in a continuous embedding space. In the IPA-
GNN, a single step (or layer) of the model corresponds to a single step of execution in a traditional
interpreter – the execution of a single line – and to accommodate the complexity of execution of
real-world programs, we consider modeling up to 174 steps.

We make a number of improvements to IPA-GNN: scaling up to handle real-world code, adding
the ability to “learn to execute” descriptions of the contents of external resources, and extending
the architecture to model exception handling. We perform evaluations comparing these interpreter-
inspired architectures against Transformer, LSTM, and GGNN baselines. Results show that our
combined improvements lead to a large increase in accuracy in predicting runtime errors and to
interpretability that allows us to predict the location of errors even though the models are only
trained on the presence or absence and class of error.

In total, we summarize our contributions as:

• We introduce the runtime error prediction task and Python Runtime Errors dataset, with runtime

error labels for millions of competition Python programs.

• We demonstrate for the ﬁrst time that IPA-GNN architectures are practical for processing real
world programs, scaling them to a depth of 174 layers, and ﬁnding them to outperform generic
models on the task.

• We demonstrate that external resource descriptions such as descriptions of the contents of stdin

can be leveraged to improve performance on the task across all model architectures.

• We extend the IPA-GNN to model exception handling, resulting in the Exception IPA-GNN, which
we ﬁnd can localize errors even when only trained on error presence and kind, not error location.

2 RELATED WORK

Execution behavior for identifying and localizing errors in source code Program analysis is a
rich family of techniques for detecting defects in programs, including static analyses which are per-
formed without executing code (Livshits & Lam, 2005; Xie & Aiken, 2006; Ayewah et al., 2008) and
dynamic analyses which are performed at runtime (Cadar et al., 2008; Sen et al., 2005; Godefroid
et al., 2005). Linters and type checkers are popular error detection tools that use static analysis.

There has been considerable recent interest in applying machine learning to identifying and localiz-
ing errors in source code (Allamanis et al., 2018a). Puri et al. (2021) makes a large dataset of real
world programs available, which we build on in constructing the Python Runtime Errors dataset.
Though dozens of tasks such as variable misuse detection and identifying “simple stupid bugs”
(Chen et al., 2021b; Allamanis et al., 2018b; Karampatsis & Sutton, 2020; Hellendoorn et al., 2020)
have been considered, most do not require reasoning about execution behavior, or else do so on syn-
thetic data (Zaremba & Sutskever, 2014; Bieber et al., 2020), or using step-by-step trace supervision
(Anonymous, 2022). Chen et al. (2021a) and Austin et al. (2021) ﬁnd program execution to be a
challenging task even for large-scale multiple billion parameter models. In contrast with the tasks
so far considered, the task we introduce requires reasoning about the execution behavior of real pro-
grams. Several works including Vasic et al. (2019) and Wang et al. (2021) perform error localization
in addition to identiﬁcation, but they use localization supervision, whereas this work does not.

1https://github.com/google-research/runtime-error-prediction

2

Interpreter-inspired models Several neural architectures draw inspiration from program inter-
preters (Bieber et al., 2020; Graves et al., 2014; Boˇsnjak et al., 2017; Gaunt et al., 2017; Łukasz
Kaiser & Sutskever, 2016; Dehghani et al., 2019; Reed & de Freitas, 2016; Graves et al., 2016).
Our work is most similar to Bieber et al. (2020) and Boˇsnjak et al. (2017), focusing on how inter-
preters handle control ﬂow and exception handling, rather than on memory allocation and function
call stacks.

3 RUNTIME ERROR PREDICTION

The goal in the runtime error prediction task is to determine statically whether a program is liable
to encounter a runtime error when it is run. The programs will not be executable, as they depend on
external resources which are not available. Descriptions of the contents of these external resources
are available, which makes reasoning about the execution behavior of the programs plausible, de-
spite there being insufﬁcient information available to perform the execution. Examples of external
resources include program inputs, ﬁle contents, and the network.

Other approaches to ﬁnding runtime errors include using mocks to write unit tests, and applying
static analysis techniques to reason about the set of plausible executions. By instead framing this
task as a machine learning challenge, it becomes applicable to the broad set of programs that lack
unit tests, and it admits analysis even when these static analysis techniques are not applicable, for
example in Python and when constraints are available only as natural language. The other beneﬁt
of the task is as a challenge for machine learning models trained on code. While there are many
datasets in the literature that test understanding of different aspects of code, we believe this task ﬁlls
a gap: it is large-scale (millions of examples), it has real-world implications and presents a practical
opportunity for improvement using ML-based approaches, and it tests a combination of statistical
reasoning and reasoning about program execution.

We treat the task as a classiﬁcation task, with each error type as its own class, with “no error” as one
additional class, and with each program having only a single target.

3.1 DATASET: PYTHON RUNTIME ERRORS

We construct the Python Runtime Errors dataset using submissions to competitive programming
problems from Project CodeNet (Puri et al., 2021). Since this is an application-motivated task, we
are interested in methods that can scale to handle the real-world complexity of these human-authored
programs. Project CodeNet contains 3.28 million Python submissions attempting to solve 3,119
distinct competitive programming problems. Each submission is a single-ﬁle Python program that
reads from stdin and writes to stdout. We run each Python submission in a sandboxed environment
on a single input to determine the ground truth runtime error class for that program. Programs are
restricted to one second of execution time, and are designated as having a timeout error if they exceed
that limit. We ﬁlter the submissions keeping only those written in Python 3, which are syntactically
valid, which do not make calls to user-deﬁned functions, and which do not exceed a threshold length
of 512 tokens once tokenized. This results in a dataset of 2.44 million submissions, each paired
with one of 26 target classes. The “no error” target is most common, accounting for 93.4% of
examples. For examples with one of the other 25 error classes, we additionally note the line number
at which the error occurs, which we use as the ground truth for the unsupervised localization task of
Section 5.3. A consequence of using only a single input for each program is the ground truth labels
under-approximate the full set of errors liable to appear at runtime, and a future improvement on the
dataset might run each program on a wider range of valid inputs.

We divide the problems into train, validation, and test splits at a ratio of 80:10:10. All submissions to
the same problem become part of the same split. This reduces similarities between examples across
splits that otherwise could arise from the presence of multiple similar submissions for the same
problem. Since there is a strong class imbalance in favor of the no error class, we also produce a
balanced version of the test split by sampling the no error examples such that they comprise roughly
50% of the test split. We use this balanced test split for all evaluations.

We report the number of examples having each target class in each split in Table 1. We describe the
full dataset generation and ﬁltering process in greater detail in Appendix A.

3

Table 1: Distribution of target classes in the Python Runtime Errors dataset. † denotes examples in
the balanced test split.

Target Class

Train #

Valid #

Test #

No error

1881303

207162

205343 /
13289†

AssertionError
AttributeError
EOFError
FileNotFoundError
ImportError
IndentationError
IndexError
KeyError
MemoryError
ModuleNotFoundError
NameError
numpy.AxisError
OSError
OverﬂowError
re.error
RecursionError
RuntimeError
StopIteration
SyntaxError
TypeError
UnboundLocalError
ValueError
ZeroDivisionError
Timeout
Other

47
10026
7676
259
7645
10
7505
362
8
1876
21540
20
19
62
5
2
24
3
74
21414
8585
25087
437
7816
18

4
509
727
37
285
0
965
39
7
186
2427
2
2
6
0
0
5
0
4
2641
991
3087
47
1072
8

8
1674
797
22
841
12
733
22
1
110
2422
3
2
11
0
1
3
1
3
2603
833
2828
125
691
2

4 APPROACH: IPA-GNNS AS RELAXATIONS OF INTERPRETERS

We make three modiﬁcations to the Instruction Pointer Attention Graph Neural Network (IPA-GNN)
architecture. These modiﬁcations scale the IPA-GNN to real-world code, allow it to incorporate ex-
ternal resource descriptions into its learned executions, and add support for modeling exception
handling. The IPA-GNN architecture is a continuous relaxation of “Interpreter A” deﬁned by the
pseudocode in Algorithm 1, minus the magenta text. We frame these modiﬁcations in relation
to speciﬁc lines of the algorithm: scaling the IPA-GNN to real-world code (Section 4.1) and in-
corporating external resource descriptions (Section 4.2) both pertain to interpreting and executing
statement xp at Line 3, and modeling exception handling adds the magenta text at lines 4-6 to yield
“Interpreter B” (Section 4.3). We showcase the behavior of both Interpreters A and B on a sample
program in Figure 1, and illustrate an execution of the same program by a continuous relaxation of
Interpreter B alongside it.

4.1 EXTENDING THE IPA-GNN TO REAL-WORLD CODE

Bieber et al. (2020) interprets the IPA-GNN architecture as a message passing graph neural network
operating on the statement-level control ﬂow graph of the input program x. Each node in the graph
corresponds to a single statement in the program. At each step t of the architecture, each node
performs three steps: it executes the statement at that node (Line 3, Equation 2), computes a branch
decision (Lines 7-8, Equation 4), and performs mean-ﬁeld averaging over the resulting states and
instruction pointers (Equations 5 and 6 in Bieber et al. (2020)).

Unlike in Bieber et al. (2020) where program statements are simple enough to be uniformly encoded
as four-tuples, the programs in the Python Runtime Errors dataset consist of arbitrarily complex
Python statements authored by real programmers in a competition setting. The language features

4

Interpreter implemented by Exc. IPA-GNN

h ← Evaluate(xp, h)
if Raises(xp, h) then

Algorithm 1
Input: Program x
1: h ← ∅; p ← 0, t ← 0
2: while p /∈ {nexit, nerror} do
3:
4:
5:
6:
7:
8:
9:
10:
11:

else
if Branches(xp, h) then

p ← p + 1

t ← t + 1

else

p ← GetRaiseNode(xp, h)

p ← GetBranchNode(xp, h)

(cid:46) Initialize the interpreter.

(cid:46) Evaluate the current line.

(cid:46) Raise exception.

(cid:46) Follow branch.

(cid:46) Proceed to next line.

n

1
2
3
4
5
6
7
8

SOURCE

x = input()
if x > 0:

y = 4/3 * x

else:

y = abs(x)
z = y + sqrt(x)
<exit>
<raise>

(a) Sample program illus-
trative of Algorithm 1 be-
havior.

STDIN
STDIN DESCRIPTION

-3
"A S I N G L E I N T E G E R -10..10"

(b) Resource description indicates likely values the sample program will re-
ceive on stdin.

t

hA,B
{}
{x: -3}
{x: -3}
{x: -3, y: 3}

0
1
2
3
4 ValueError(lineno=6)

pA

pB

h ˜B

1
2
5
6
7

1
2
5
6
8

p ˜B
[10000000]
[01000000]
[00001000]
[00000100]
[00000001]

(c) Step-by-step execution of the sample program according to Interpreters A
and B, and under continuous relaxation ˜B. Distinct colors represent distinct
embedding values.

Figure 1: A sample program and its execution under discrete interpreters A and B (Algorithm 1) and
under a continuous relaxation ˜B of Interpreter B.

used are numerous and varied, and so the statement lengths vary substantially, with a mean statement
length of 6.7 tokens; we report the full distribution of statement lengths in Figure 4.

The IPA-GNN architecture operates on a program x’s statement-level control ﬂow graph, and so
requires per-statement embeddings Embed(xn) for each statement xn. We ﬁrst apply either a local
or global Transformer encoder to produce per-token embeddings, and we subsequently apply one
of four pooling variants to a span of such embeddings to produce a node embedding per statement
in a program. In the local approach, we apply an attention mask to limit the embedding of a token
in a statement to attending to other tokens in the same statement. In the global approach, no such
attention mask is applied, and so every token may attend to every other token in the program. We
consider four types of pooling in our hyperparameter search space: ﬁrst, sum, mean, and max. The
resulting embedding is given by

Embed(xn) = Pool(cid:0)Transformer(x)Span(x,n)

(cid:1) .

(1)

First pooling takes the embedding of the ﬁrst token in the span of node n. Sum, mean, and max
pooling apply their respective operations to the embeddings of all tokens in the span of node n.

Finally we ﬁnd that real-world code requires as many as 174 steps of the IPA-GNN under the model’s
heuristic for step limit T (x) (Appendix C). To reduce the memory requirements, we apply remate-
rialization at each step of the model (Griewank & Walther, 2000; Chen et al., 2016).

5

4.2 EXECUTING WITH RESOURCE DESCRIPTIONS

Each program x in the dataset is accompanied by a description of what values stdin may contain
at runtime, which we tokenize to produce embedding d(x). Analogous to Line 1 of Algorithm 1,
IPA-GNN architectures initialize with per-node hidden states h0,: = 0 and soft instruction pointer
p0,n = 1{n = 0}. Following initialization, each step of an IPA-GNN begins by simulating execution
(Line 3) of each non-terminal statement with non-zero probability under the soft instruction pointer
to propose a new hidden state contribution
a(1)
t,n = RNN(ht−1,n, Modulate(Embed(xn), d(x), ht−1,n)).
(2)
Here, the text in magenta shows our modiﬁcation to the IPA-GNN architecture to incorporate exter-
nal resource descriptions. We consider both Feature-wise Linear Modulation (FiLM) (Perez et al.,
2017) and cross-attention (Lee et al., 2019) for the Modulate function, which we deﬁne in Ap-
pendix B. This modulation allows the IPA-GNN to execute differently at each step depending on
what information the resource description provides, whether that be type information, value ranges,
or candidate values.

We also consider one additional method that applies to any model: injecting the description as a
docstring at the start of the program. This method yields a new valid Python program, and so any
model can accommodate it.

4.3 MODELING EXCEPTION HANDLING

The ﬁnal modiﬁcation we make to the IPA-GNN architecture is to model exception handling. In
Algorithm 1, this corresponds to adding the magenta text to form Interpreter B, computing a raise
decision (Lines 4-6, Equation 3). We call the architecture that results the “Exception IPA-GNN”.

Whereas execution always proceeds from statement to next statement in Interpreter A and in the
IPA-GNN, Interpreter B admits another behavior. In Interpreter B and the Exception IPA-GNN,
execution may proceed from any statement to a surrounding “except block”, if it is contained in
a try/except frame, or else to a special global error node, which we denote nerror. In the sample
execution in Figure 1c we see at step t = 4 the instruction pointer pB updates to nerror = 8.

We write that the IPA-GNN makes raise decisions as

bt,n,r(n), (1 − bt,n,r(n)) = softmax

(cid:16)

Dense(a(1)
t,n)

(cid:17)

.

(3)

The dense layer here has two outputs representing the case that an error is raised and that no error
is raised. Here r(n) denotes the node that statement n raises to in program x; r(n) = nerror if n
is not contained in a try/except frame, and bt,n,n(cid:48) denotes the probability assigned by the model to
transitioning from executing n to n(cid:48).

Next the model makes soft branch decisions in an analogous manner; the dense layer for making
branch decisions has distinct weights from the layer for making raise decisions.
(cid:17)

(cid:16)

bt,n,n1, bt,n,n2 = (1 − bt,n,r(n)) · softmax

Dense(a(1)
t,n)

.

(4)

The text in magenta corresponds to the “else” at Line 6. The model has now assigned probability
to up to three possible outcomes for each node: the probability that n raises an exception bt,n,r(n),
the probability that the true branch is followed bt,n,n1 , and the probability that the false branch is
followed bt,n,n2. In the common case where a node is not a control node and has only one successor,
rather than separate true and false branches, the probability of reaching that successor is simply
1 − bt,n,r(n).

Finally, we assign each program a step limit T (x) using the same heuristic as Bieber et al. (2020),
detailed in Appendix C. After T (x) steps of the architecture, the model directly uses the probability
mass at nexit and nerror to predict whether the program raises an error, and if so it predicts the error
type using the hidden state at the error node. We write the modiﬁed IPA-GNN’s predictions as

P (no error) ∝ pT (x),nexit and

P (error) ∝ pT (x),nerror , with
P (error = k | error) = softmax (cid:0)Dense(hT (x),nerror )(cid:1).

(5)

(6)

(7)

We train with a cross-entropy loss on the class predictions, treating “no error” as its own class.

6

4.4 UNSUPERVISED LOCALIZATION OF ERRORS

When we model exception handling explicitly in the IPA-GNN, the model makes soft decisions as
to when to raise exceptions. We can interpret these decisions as predictions of the location where
a program might raise an error. We can then evaluate how closely these location predictions match
the true locations where exceptions are raised, despite never training the IPA-GNN with supervision
that indicates error locations.

For programs that lack try/except frames, we compute the localization predictions of the model by
summing, separately for each node, the contributions from that node to the exception node across
all time steps. This gives an estimate of exception provenance as

p(error at statement n) =

(cid:88)

t

pt,n · bt,n,nerror .

(8)

For programs with a try/except frame, we must trace the exception back to the statement that origi-
nally raised it. We provide the calculation for this in Appendix D.

5 EXPERIMENTS

In our experiments we evaluate the following research questions:

RQ1: How does the adaptation of the IPA-GNN to real-world code compare against standard archi-
tectures like GGNN, LSTM, and Transformer? (Section 5.1)

RQ2: What is the impact of including resource descriptions? What methods for incorporating them
work best? (Section 5.2)

RQ3: How interpretable are the latent instruction pointer quantities inside the Exception IPA-GNN
for localizing where errors arise? How does unsupervised localization with the Exception IPA-GNN
compare to alternative unsupervised approaches to localization based on multiple instance learning
and standard architectures? (Section 5.3)

5.1 EVALUATION OF IPA-GNN AGAINST BASELINES

We describe our experimental setup for our ﬁrst experiment, comparing the IPA-GNN architec-
tures with Transformer (Vaswani et al., 2017), GGNN (Li et al., 2017), and LSTM (Hochreiter &
Schmidhuber, 1997) baselines. In all approaches, we use the 30,000 token vocabulary constructed
in Appendix A, applying Byte-Pair Encoding (BPE) tokenization (Sennrich et al., 2016) to tok-
enize each program into a sequence of token indices. The Transformer operates on this sequence
of token indices directly, with its ﬁnal representation computed via mean pooling. For all other
models (GGNN, LSTM, IPA-GNN, and Exception IPA-GNN), the token indices are ﬁrst combined
via a masked (local) Transformer to produce per-node embeddings, and the model operates on these
per-node embeddings, as in Section 4.1. Following Bieber et al. (2020) we encode programs for a
GGNN using six edge types, and use a two-layer LSTM for both the LSTM baseline and for the
RNN in all IPA-GNN variants.

For each approach, we perform an independent hyperparameter search using random search. We list
the hyperparameter space considered and model selection criteria in Appendix C. The models are
each trained to minimize a cross-entropy loss on the target class using stochastic gradient descent
for up to 500,000 steps with a mini-batch size of 32. In order to more closely match the target
class distribution found in the balanced test set, we sample mini-batches such that the proportion of
examples with target “no error” and those with an error target is 1:1 in expectation. We evaluate the
selected models on the balanced test set, and report the results in Table 2 (see rows without check
marks). Weighted F1 score weights per-class F1 scores by class frequency, while weighted error F1
score restricts consideration to examples with a runtime error.

We perform additional evaluations using the same experimental setup but distinct initializations to
compute measures of variance, which we detail in Appendix E.

RQ1: The interpreter-inspired architectures show signiﬁcant gains over the GGNN, Transformer,
and baseline approaches on the runtime error prediction task. We attribute this to the model’s induc-
tive bias toward mimicking program execution.

7

Table 2: Accuracy, weighted F1 score (W. F1), and weighted error F1 score (E. F1) on the Python
Runtime Errors balanced test set.

MODEL

R.D.? ACC. W. F1

E. F1

S GGNN
E
N

-
E
S
A
B

I
L

TRANSFORMER
LSTM

S
N
O

I
T
A
L
B
A

S
R
U
O

GGNN
TRANSFORMER
LSTM
IPA-GNN
E. IPA-GNN

IPA-GNN
E. IPA-GNN

(cid:34)
(cid:34)
(cid:34)

(cid:34)
(cid:34)

62.8
63.6
66.1

68.3
67.3
68.1
68.3
68.7

71.4
71.6

58.9
60.4
61.4

66.5
65.1
66.8
64.8
64.9

70.1
70.9

45.8
48.1
48.4

56.8
54.7
58.3
53.8
53.3

62.2
63.5

Table 3: A comparison of early and late fusion methods for incorporating external resource descrip-
tion information into interpreter-inspired models.

MODEL

BASELINE
ACC. W. F1

DOCSTRING

E. F1 ACC. W. F1

FILM
E. F1 ACC. W. F1

CROSS-ATTENTION

E. F1 ACC. W. F1

E. F1

IPA-GNN
68.3
E. IPA-GNN 68.7

64.8
64.9

53.8
53.3

71.4
71.6

70.1
70.9

62.2
63.5

71.6
70.9

70.3
68.8

62.9
59.8

72.0
73.8

70.3
72.3

62.6
64.7

5.2

INCORPORATING RESOURCE DESCRIPTIONS

We next evaluate methods of incorporating resource descriptions into the models. For each archi-
tecture we apply the docstring approach of processing resource descriptions of Section 4.2. This
completes a matrix of ablations, allowing us to distinguish the effects due to architecture change
from the effect of the resource description. We follow the same experimental setup as in Section 5.1,
and show the results in Table 2 (compare rows with check marks to those without).

We also consider the FiLM and cross-attention methods of incorporating resource descriptions into
the IPA-GNN. Following the same experimental setup again, we show the results of this experiment
in Table 3. Note that the best model overall by our model selection criteria on validation data was
the IPA-GNN with cross-attention, though the Exception IPA-GNN performed better on test.

RQ2: Across all architectures it is clear that external resource descriptions are essential for improved
performance on the runtime error prediction task. On the IPA-GNN architectures, we see further
improvements by considering architectures that incorporate the resource description directly into
the execution step of the model, but these gains are inconsistent. Critically, using any resource
description method is better than none at all.

To understand how the resource descriptions lead to better performance, we compare in Figure 2
the instruction pointer values of two Exception IPA-GNN models on a single example (shown in
Table 5). The model with the resource description recognizes that the input() calls will read
input beyond the end of the stdin stream. In contrast, the model without the resource description
has less reason to suspect an error would be raised by those calls. The descriptions of stdin in the
Python Runtime Errors dataset also frequently reveal type information, expected ranges for numeric
values, and formatting details about the inputs. We visualize additional examples in Appendix G.

5.3

INTERPRETABILITY AND LOCALIZATION

We next investigate the behavior of the Exception IPA-GNN model, evaluating its ability to localize
runtime errors without any localization supervision. In unsupervised localization, the models predict
the location of the error despite being trained only with error presence and kind supervision.

8

Multiple Instance Learning Baselines The unsupervised localization task may be viewed as mul-
tiple instance learning (MIL) (Dietterich et al., 1997). To illustrate, consider the subtask of predict-
ing whether a particular line contains an error. In an n-line program, there are n instances of this
subtask. The available supervision only indicates if any one of these subtasks has an error, but does
not specify which one. So, treating each subtask as an instance, the group of subtasks forms a bag
of instances, and we view the setting as MIL.

We thus consider as baselines two variations on the Transformer architecture, each using multiple
instance learning. The ﬁrst is the “Local MIL Transformer”, in which each statement in the program
is encoded individually, as in the local node embeddings computation of Section 4.1. The second is
the “Global MIL Transformer”, in which all tokens in the program may attend to all other tokens in
the Transformer encoder. In both cases, the models make per-line predictions, which are aggregated
to form an overall prediction as deﬁned in Appendix F.

Localization Experiment We use the same experimental protocol as in Section 5.1, and train each
of the MIL Transformer and Exception IPA-GNN models. As before, the models are trained only
to minimize cross-entropy loss on predicting error kind and presence, receiving no supervision as
to the location of the errors. We report the localization results in Table 4. Localization accuracy
(“LOCAL.” in the table) computes the percent of test examples with errors for which the model
correctly predicts the line number of the error.

Table 4: Localization accuracy (%) for the MIL Transformers and Exception IPA-GNN on the
Python Runtime Errors balanced test split.

MODEL

R.D.?

LOCAL.

LOCAL MIL TRANSFORMER
LOCAL MIL TRANSFORMER
GLOBAL MIL TRANSFORMER
GLOBAL MIL TRANSFORMER
E. IPA-GNN
E. IPA-GNN + DOCSTRING
E. IPA-GNN + FILM
E. IPA-GNN + C.A.

(cid:33)

(cid:33)

(cid:33)
(cid:33)
(cid:33)

33.0
48.9
48.2
48.8
50.8
64.7
64.5
68.8

RQ3: The Exception IPA-GNN’s unsupervised localization capabilities far exceed that of baseline
approaches. In Figure 2 we see the ﬂow of instruction pointer mass during the execution of a sample
program (Table 5) by two Exception IPA-GNN models, including the steps where the models raise
probability mass to nerror. Tallying the contributions to nerror from each node yields the exception
provenance values in the right half of Table 5. This shows how the model’s internal state resembles
plausible program executions and allows for unsupervised localization. As a beneﬁcial side-effect
of learning plausible executions, the Exception IPA-GNN can localize the exceptions it predicts.

6 DISCUSSION

In this work, we introduce the new task of predicting runtime errors in competitive programming
problems and advance the capabilities of interpreter-inspired models. We tackle the additional com-
plexity of real-world code and demonstrate how natural language descriptions of external resources
can be leveraged to reduce the ambiguity that arises in a static analysis setting. We show that the
resulting models outperform standard alternatives like Transformers and that the inductive bias built
into these models allows for interesting interpretability in the context of unsupervised localization.

Though they perform best, current IPA-GNN models require taking many steps of execution, up
to 174 on this dataset. A future direction is to model multiple steps of program execution with a
single model step, to reduce the number of model steps necessary for long programs. Extending the
interpreter-inspired models with additional interpreter features, or to support multi-ﬁle programs or
programs with multiple user-deﬁned functions is also an interesting avenue for future work.

Learning to understand programs remains a rich area of inquiry for machine learning research be-
cause of its complexity and the many aspects of code. Learning to understand execution behavior

9

Table 5: A sample program from the Python Runtime Errors validation split. Error node contri-
butions are shown for the selected BASELINE and DOCSTRING Exception IPA-GNN variants. The
target error kind is EOFERROR, occuring on line 2 (n = 2). BASELINE incorrectly predicts NO
ERROR with conﬁdence 0.708. R.D. correctly predicts EOFERROR with conﬁdence 0.988, local-
ized at line 3 (n = 3). The input description shows the cause for error: there are more input()
calls than the number of expected inputs (one).

STDIN DESCRIPTION

n

SOURCE

Input is given from Standard Input in the

Input:
following format Constraints:
is A or B. |S| = 3

Each character of S

BASELINE

R.D.

0
1
2
3,4
5
6
7
8

9
10

a = str(input())
q = int(input())
s = [input().split() for i in range(q)]
for i in range(q):

if int(s[i][0]) == 1 and len(a)>1:

a = a[::-1]

elif int(s[i][0])== 2 and int(s[i][1])==1:

a=s[i][2]+a

else:

a=a+s[i][2]

print(a)

16.9
3.2
0.5
6.4
0.1
0.7
0.1
0.2

0.0
1.1

0.4
0.3
99.3
0.0
0.0
0.0
0.0
0.0

0.0
0.0

BASELINE

RESOURCE DESCRIPTION

Figure 2: Heatmap of instruction pointer values produced by BASELINE and DOCSTRING Exception
IPA-GNNs for the example in Table 5. The x-axis represents timesteps and the y-axis represents
nodes, with the last two rows respectively representing nexit and nerror. The BASELINE instruction
pointer value is diffuse, with most probability mass ending at nexit. With the resource description,
the instruction pointer value is sharp, with almost all probability mass jumping to nerror from node 2.

is particularly challenging as programs grow in complexity, and as they depend on more external
resources whose contents are not present in the code. Our work presents a challenging problem
and advances interpreter-inspired models, both of which we hope are ingredients towards making
progress on these difﬁcult and important problems.

10

0510152002468101205101520024681012REFERENCES

Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine

learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):81, 2018a.

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs

with graphs. In International Conference on Learning Representations, 2018b.

Anonymous. Show your work: Scratchpads for intermediate computation with language models.
In Submitted to The Tenth International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=iedYJm92o0a. under review.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large
language models, 2021.

Nathaniel Ayewah, William Pugh, David Hovemeyer, J. David Morgenthaler, and John Penix. Using

static analysis to ﬁnd bugs. IEEE Software, 25(5):22–29, 2008. doi: 10.1109/MS.2008.130.

David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. Learning to execute programs
In Advances in Neural Information

with instruction pointer attention graph neural networks.
Processing Systems, 2020.

Matko Boˇsnjak, Tim Rockt¨aschel, Jason Naradowsky, and Sebastian Riedel. Programming with a

differentiable forth interpreter, 2017.

Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al. Klee: unassisted and automatic generation
of high-coverage tests for complex systems programs. In OSDI, volume 8, pp. 209–224, 2008.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,
Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex
Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec
Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-
Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large
language models trained on code, 2021a.

Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear

memory cost, 2016.

Zimin Chen, Vincent J Hellendoorn, Petros Maniatis, Pascal Lamblin, Pierre-Antoine Manzagol,
Danny Tarlow, and Subhodeep Moitra. Plur: A unifying, graph-based view of program learning,
understanding, and repair. 2021b.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal

transformers, 2019.

Thomas G. Dietterich, Richard H. Lathrop, and Tom´as Lozano-P´erez. Solving the multiple in-
stance problem with axis-parallel rectangles. Artiﬁcial Intelligence, 89(1):31–71, 1997. ISSN
doi: https://doi.org/10.1016/S0004-3702(96)00034-3. URL https://www.
0004-3702.
sciencedirect.com/science/article/pii/S0004370296000343.

Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, and Daniel Tarlow. Differentiable pro-

grams with neural libraries, 2017.

Patrice Godefroid, Nils Klarlund, and Koushik Sen. Dart: Directed automated random testing.
In Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and
implementation, pp. 213–223, 2005.

Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.

11

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwi´nska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
Adri`a Puigdom`enech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,
Helen King, Christopher Summerﬁeld, Phil Blunsom, Koray Kavukcuoglu, and Demis Hass-
abis. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471–476, October 2016. ISSN 00280836. URL http://dx.doi.org/10.1038/
nature20101.

Andreas Griewank and Andrea Walther. Algorithm 799: Revolve: An implementation of check-
pointing for the reverse or adjoint mode of computational differentiation. ACM Trans. Math.
Softw., 26(1):19–45, mar 2000. ISSN 0098-3500. doi: 10.1145/347837.347846. URL https:
//doi.org/10.1145/347837.347846.

Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, and Petros Maniatis. Global relational mod-

els of source code. In International Conference on Learning Representations, 2020.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):
doi: 10.1162/neco.1997.9.8.1735. URL

1735–1780, November 1997.
https://doi.org/10.1162/neco.1997.9.8.1735.

ISSN 0899-7667.

Rafael-Michael Karampatsis and Charles Sutton. How often do single-statement bugs occur? Pro-
ceedings of the 17th International Conference on Mining Software Repositories, Jun 2020. doi:
10.1145/3379597.3387491. URL http://dx.doi.org/10.1145/3379597.3387491.

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set

transformer: A framework for attention-based permutation-invariant neural networks, 2019.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural

networks, 2017.

V Benjamin Livshits and Monica S Lam. Finding security vulnerabilities in java applications with

static analysis. In USENIX security symposium, volume 14, pp. 18–18, 2005.

Flemming Nielson and Hanne Riis Nielson. Interprocedural control ﬂow analysis. In ESOP, 1999.

Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual

reasoning with a general conditioning layer, 2017.

Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov,
Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh
Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. Codenet: A large-scale
ai for code dataset for learning a diversity of coding tasks, 2021.

Scott Reed and Nando de Freitas. Neural programmer-interpreters, 2016.

Koushik Sen, Darko Marinov, and Gul Agha. Cute: A concolic unit testing engine for c. ACM

SIGSOFT Software Engineering Notes, 30(5):263–272, 2005.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with

subword units, 2016.

Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural program
repair by jointly learning to localize and repair. In International Conference on Learning Repre-
sentations, 2019.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
arXiv e-prints, art.

Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need.
arXiv:1706.03762, Jun 2017.

Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, and Tegawend´e F. Bis-
syand´e. Beep: Fine-grained ﬁx localization by learning to predict buggy code elements, 2021.

Yun Wang, Juncheng Li, and Florian Metze. Comparing the max and noisy-or pooling functions in

multiple instance learning for weakly supervised sequence learning tasks, 2018.

12

Yichen Xie and Alex Aiken. Static detection of security vulnerabilities in scripting languages. In

USENIX Security Symposium, volume 15, pp. 179–192, 2006.

Wojciech Zaremba and Ilya Sutskever. Learning to execute, 2014.

Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms, 2016.

13

A PYTHON RUNTIME ERROR DATASET DETAILS

We describe in detail the construction of the Python Runtime Error dataset from the submissions
in Project CodeNet (Puri et al., 2021). The Project CodeNet dataset contains over 14 million sub-
missions to 4,053 distinct competitive programming problems, with the submissions spanning more
than 50 programming languages. We partition the problems into train, valid, and test splits at an
80:10:10 ratio. By making all submissions to the same problem part of the same split we mitigate
concerns about potential data leakage from similar submissions to the same problem. We restrict our
consideration to Python submissions, which account for 3,286,314 of the overall Project CodeNet
submissions, with 3,119 of the problems receiving at least one submission in Python. In preparing
the dataset we execute approximately 3 million problems in a sandboxed environment to collect their
runtime error information, we perform two stages of ﬁltering on the dataset, syntactic and complex-
ity ﬁltering, and we construct a textual representation of the input space for each problem from the
problem description.

A.1 SYNTACTIC FILTERING

In this ﬁrst phase of ﬁltering, we remove submissions in Python 2 as well as those which fail to
parse and run from our dataset. We remove 76,888 programs because they are in Python 2, 59,813
programs because they contain syntax errors, 2,011 programs that result in runtime errors during
parsing, and 6 additional programs for which the python-graphs library fails to construct a control
ﬂow graph. A program may result in a runtime error during parsing if it contains return, break,
continue keywords outside of an appropriate frame.

A.2 PROGRAM EXECUTION

We attempt to run each submission in a sandboxed environment using the sample input provided
in the Project CodeNet dataset. The environment is a custom harness running on a Google Cloud
Platform (GCP) virtual environment. This allows us to collect standard out and standard error, to
monitor for timeouts, and to catch and serialize any Python exceptions raised during execution.
We restrict execution of each program to 1 second, marking any program exceeding this time as a
timeout error. If the program encounters a Python exception, we use the name of that exception as
the target class for the program. If an error type occurs only once in the dataset, we consider the
target class to be Other. Programs not exhibiting an error or timeout are given target class “no error”.

In addition to collecting the target class, we record for each runtime error the line number at which
the error occurs. We use these line numbers as the ground truth for the unsupervised error localiza-
tion task considered in Section 5.3.

A.3 PARSING INPUT SPACE INFORMATION FROM PROBLEM STATEMENTS

For each problem, we parse the problem statement to extract the input description and input con-
straints, if they exist. Problem statements are written either in English or Japanese, and so we
write our parser to support both languages. When one or more of these two sections are present
in the problem statement, we construct an input space description containing the contents of the
present sections. For the experiments that use input space information as a docstring, we prepend
to each submission our the input space description for its corresponding problem. Similarly the in-
put space descriptions are used in the experiments that process input space information with either
cross-attention or FiLM.

A.4 VOCABULARY CONSTRUCTION AND COMPLEXITY FILTERING

All experiments use the same vocabulary and tokenization procedure. For this, we select the stan-
dard Byte-Pair Encoding (BPE) tokenization procedure (Sennrich et al., 2016). We construct the
vocabulary using 1,000,000 submissions selected from the training split, along with the input space
descriptions constructed for all problems in the train split. We use a vocabulary size of 30,000.

We then apply size-based ﬁltering, further restricting the set of programs considered. First, the
program length after tokenization is not to exceed 512 tokens, the number of nodes and edges in

14

Figure 3: A histogram showing the distribution
of program lengths, measured in lines, repre-
sented in the Python Runtime Errors train split.

Figure 4: The distribution of statement lengths,
measured in tokens, in the Python Runtime Er-
rors train split.

the control ﬂow graph are each not to exceed 128, and the step limit T (x) for a program computed
in Appendix C is not to exceed 174. We select these numbers to trim the long tail of exceptionally
long programs, and this ﬁltering reduces the total number of acceptable programs by less than 1%.
To achieve consistent datasets comparable across all experiments, we use the longest form of each
program (the program augmented with its input space information as a docstring), when computing
the program sizes for size-based submission ﬁltering.

We further impose the restriction that no user-deﬁned functions (UDFs) are called in a submission;
this further reduces the number of submissions by 682,220. A user-deﬁned function is a function
deﬁned in the submission source code, as opposed to being a built-in or imported from a third party
module. Extending the IPA-GNN models to submissions with UDFs called at most once is trivially
achieved by replacing the program’s control ﬂow graph with its interprocedural control ﬂow graph
(ICFG) (Nielson & Nielson, 1999). We leave the investigation of modelling user-deﬁned functions
to further work.

A.5 FINAL DATASET DETAILS

After applying syntactic ﬁltering (only keeping Python 3 programs that parse) and complexity ﬁl-
tering (eliminating long programs and programs that call user-deﬁned functions), we are left with a
dataset of 2,441,130 examples. The division of these examples by split and by target class is given
in Table 1. Figure 3 shows the distribution of program lengths in lines represented in the completed
dataset, with an average program length of 14.2 lines. The average statement length is 6.7 tokens,
with full distribution shown in Figure 4.

B INPUT MODULATION

We consider cross-attention (Lee et al., 2019) and Feature-wise Linear Modulation (FiLM) (Perez
et al., 2017) as the Modulate function. After embedding the node and the resource description we
use cross-attention as follows to modulate the input.

MultiHead(Embed(xn), d(x), ht−1,n) = Concat(Concat(head1, ..., headh)W O, Embed(xn))

where headi = softmax

(cid:32)

(cid:33)

V

(cid:48)

QK
√
dk

Q = W Q
K = W K
V = W V

i Concat(Embed(xn), ht−1,n)
i d(x)
i d(x)

15

(9)

(10)

(11)

(12)

(13)

Here, W O ∈ Rhdv×dmodel , W Q
i ∈
Rdv×dd(x) are learnable parameters. Similarly, for FiLM we modulate the input with the resource
description as follows:

i ∈ Rdk×(dmodel+dEmbed(xn)), W K

i ∈ Rdk×dd(x), and W V

FiLM(Embed(xn), d(x), ht−1,n) = Concat(β · d(x) + γ, Embed(xn))

where β = σ(Wβ Concat(xn, ht−1,n) + bβ),
γ = σ(Wγ Concat(xn, ht−1,n) + bγ),

(14)
(15)
(16)

where Wγ ∈ Rdd(x)×(dmodel+dEmbed(xn )), and Wγ ∈ Rdd(x)×(dmodel+dEmbed(xn )) are learnable pa-
rameters.

C HYPERPARAMETER SELECTION

We select hyperparameters by performing a random search independently for each model architec-
ture. The hyperparameters considered by the search are listed in Table 7. All architectures use a
Transformer encoder, and the Transformer sizes considered in the search are listed in Table 7 and
deﬁned further in Table 6.

Table 6: Hyperparameter settings for each of the three Transformer sizes.

HYPERPARAMETER

T-128

T-256

T-512

EMBEDDING DIMENSION
NUMBER OF HEADS
NUMBER OF LAYERS
QKV DIMENSION
MLP DIMENSION

128
4
2
128
512

256
4
2
256
1024

512
8
6
512
2048

For the IPA-GNN and Exception IPA-GNN, the function T (x) represents the number of execution
steps modeled for program x. We reuse the deﬁnition of T (x) from Bieber et al. (2020) as closely
as possible, only modifying it to accept arbitrary Python programs, rather than being restricted to
the subset of Python features considered in the dataset of the earlier work.

D LOCALIZATION BY MODELING EXCEPTION HANDLING

For programs that lack try/except frames, we compute the localization predictions of the Exception
IPA-GNN model by summing, separately for each node, the contributions from that node to the
exception node across all time steps. This gives an estimate of exception provenance as

p(error at statement n) =

(cid:88)

t

pt,n · bt,n,nerror .

(17)

Table 7: Hyperparameters considered for random search during model selection.

HYPERPARAMETER VALUE(S) CONSIDERED

ARCHITECTURE(S)

OPTIMIZER
BATCH SIZE
LEARNING RATE
LEARNING RATE
GRADIENT CLIPPING
HIDDEN SIZE
RNN LAYERS
GNN LAYERS
SPAN ENCODER POOLING
CROSS-ATTENTION NUMBER OF HEADS
MIL POOLING
TRANSFORMER DROPOUT RATE
TRANSFORMER ATTENTION DROPOUT RATE
TRANSFORMER SIZE

{SGD}
{32}
{0.01, 0.03, 0.1, 0.3}
{0.001, 0.003, 0.01, 0.03} GGNN
{0, 0.5, 1, 2}
{64, 128, 256}
{2}
{8, 16, 24}
{F I R S T, M E A N, M A X, S U M}
{1, 2}
{M A X, M E A N, L O G S U M E X P}
{0, 0.1}
{0, 0.1}
{T-128, T-256, T-512}

ALL
ALL
LSTM, TRANSFORMERS, IPA-GNNS

ALL
ALL
LSTM, IPA-GNNS
GGNN
ALL
IPA-GNNS WITH CROSS-ATTENTION
MIL TRANSFORMERS
ALL
ALL
ALL

16

For programs with a try/except frame, however, we must trace the exception back to the statement
that originally raised it. To do this, we keep track of the exception provenance at each node at each
time step; when an exception raises, it becomes the exception provenance at the statement that it
raises to, and when a statement with non-zero exception provenance executes without raising, it
propagates its exception provenance to the next node unchanged.
Deﬁne vt,n,n(cid:48) as the amount of “exception probability mass” at time step t at node n(cid:48) attributable to
an exception starting at node n. Then we write

vt,n,n(cid:48) =

(cid:88)

k∈Nin(n(cid:48))

vt−1,n,k · bt,k,n(cid:48) · pt,k + (1 −

(cid:88)

vt−1,:,n) · bt,n,n(cid:48) · pt,n · 1{n(cid:48) = r(n)}. (18)

The ﬁrst term propagates exception provenance across normal non-raising execution, while the sec-
ond term introduces exception provenance when an exception is raised. We then write precisely

allowing the Exception IPA-GNN to make localization predictions for any program in the dataset.

p(error at statement n) = vT (x),n,nerror ,

(19)

E METRIC VARIANCES

Under the experimental conditions of Section 5.1, we perform three additional training runs to cal-
culate the variance for each metric for each baseline model, and for the Exception IPA-GNN model
using the docstring strategy for processing resource descriptions. For these new training runs, we
use the hyperparameters obtained from model selection. We vary the random seed between runs (0,
1, 2), thereby changing the initialization and dropout behavior of each model across runs. We report
the results in Table 8; ± values are one standard deviation.

Table 8: Mean and standard deviation for each metric is calculated from three training runs per
model, using the hyperparameters selected via model selection.

METHOD

R.D.?

ACC.

W. F1

E. F1

GGNN
TRANSFORMER
LSTM
EXCEPTION IPA-GNN

(cid:34)

61.98 ± 1.24
63.82 ± 0.62
66.43 ± 0.60
71.44 ± 0.15

56.62 ± 2.96
59.86 ± 0.52
62.33 ± 1.12
70.78 ± 0.07

41.24 ± 5.51
46.75 ± 0.93
50.10 ± 1.94
63.54 ± 0.03

F MULTIPLE INSTANCE LEARNING

The Local Transformer and Global Transformer models each compute per-statement node embed-
dings Embed(xn) given by Equation 1. In the multiple instance learning setting, these are trans-
formed into unnormalized per-statement class predictions

φ(class = k, lineno = l) = Dense (Embed(xn)) .

(20)

We consider three strategies for aggregating these per-statement predictions into an overall predic-
tion for the task. Under the logsumexp strategy, we treat φ as logits and write

log p(class = k) ∝ log

log p(lineno = l) ∝ log

(cid:32)

(cid:88)

l

(cid:32)

(cid:88)

(cid:33)

exp φ(k, l)

,

(cid:33)

exp φ(k, l)

k∈K

where K is the set of error classes.

The max and mean strategies meanwhile follow Wang et al. (2018) in asserting

p(class = k | lineno = l) = softmax (φ(k, l)) ,

17

(21)

(22)

(23)

compute the location probabilities as

p(lineno = l) ∝

(cid:88)

k∈K

p(class = k | lineno = l),

and compute the outputs as

log p(class = k) ∝ log max

l

p(class = k | lineno = l), and

log p(class = k) ∝ log

1
L

(cid:88)

l

p(class = k | lineno = l)

(24)

(25)

(26)

respectively, where L denotes the number of statements in x. As with all methods considered,
the MIL models are trained to minimize the cross-entropy loss in target class prediction, but these
methods still allow reading off predictions of p(lineno).

G EXAMPLE VISUALIZATIONS

Additional randomly sampled examples from the Python Runtime Error dataset validation split are
shown here. As in Figure 2, prediction visualizations for these examples are shown for the selected
BASELINE and DOCSTRING Exception IPA-GNN model variants.

In instruction pointer value heatmaps, the x-axis represents timesteps and the y-axis represents
nodes, with the last two rows respectively representing the exit node nexit and the exception node
nerror. Note that for loop statements are associated with two spans in the statement-level control
ﬂow graph, one for the construction of the iterator, and a second for assignment to the loop variable.
Hence we list two indexes for each for loop statement in these ﬁgures, and report the total error
contribution for the line.

STDIN DESCRIPTION

Input:
following format:
Constraints:
N , a i <= 100

Input is given from Standard Input in the

N a 1 a 2 ...

a N

All values in input are integers.

1 <=

n

0
1

2
3,4
5
6

SOURCE

BASELINE Error contrib.

R.D. Error contrib.

N = int(input())
A = list(map(int,
input().split()))
res = 0
for i in range(1, len(A)+1, 2):

res += A[i] % 2

print(res)

2.9
0.8

3.0
9.8
0.3
0.2

0.2
0.0

63.3
6.3
0.1
2.2

BASELINE

RESOURCE DESCRIPTION

Figure 5: The target error kind is INDEXERROR, occuring on line 5 (n = 5). BASELINE incorrectly
predicts NO ERROR with conﬁdence 0.808. DOCSTRING correctly predicts INDEXERROR with con-
ﬁdence 0.693, but localizes to line 3 (n = 2). Both BASELINE and DOCSTRING instruction pointer
values start out sharp and become diffuse when reaching the for-loop. The BASELINE instruction
pointer value ends with most probability mass at nexit. The DOCSTRING instruction pointer value
has a small amount of probability mass reaching nexit, with most probability mass ending at nerror.

18

0.02.55.07.510.012.515.017.520.0024680.02.55.07.510.012.515.017.520.002468STDIN DESCRIPTION

Input is given from Standard Input in the

Input:
following format:
Constraints:
<= 10ˆ4
All values in input are integers.

H N A 1 A 2 ...

A N

1 <= H <= 10ˆ9 1 <= N <= 10ˆ5 1 <= A i

n

0

1,2
3

4
5
6

7

SOURCE

H,N,A = list(map(int,
input().split()))
for i in A[N]:
if H <= 0:
break

else:

H -= A[i]

if set(A):

print("Yes")

else:

print("No")

BASELINE Error contrib.

R.D. Error contrib.

9.7

43.7
2.9

6.0
0.2
9.3

3.3

3.4

83.0
2.8

0.0
0.1
0.7

0.2

BASELINE

RESOURCE DESCRIPTION

Figure 6: The target error kind is VALUEERROR, occuring on line 1 (n = 0). BASELINE incorrectly
predicts INDEXERROR with conﬁdence 0.319 on line 1 (n = 0). DOCSTRING correctly predicts
VALUEERROR with conﬁdence 0.880 on line 2 (n = 1), corresponding to A[n]. Both BASELINE
and DOCSTRING instruction pointer values start out sharp and quickly shift most of the probability
mass to the exception node.

19

051015200246810051015200246810STDIN DESCRIPTION

dm Two integers n and m are

n m d1 d2 ...

Input:
given in the first line.
are given in the second line.
1 <= n <= 50000 1 <= m <= 20 1 <=
Constraints:
denomination <= 10000 The denominations are all
different and contain 1.

The available denominations

n

0

1

2

3
4

5
6
7,8
9,10
11
12
13

14

SOURCE

BASELINE Error contrib.

R.D. Error contrib.

from itertools import
combinations with replacement
as C
n, m = map(int,
input().split())
coin = sorted(list(map(int,
input().split())))
if n in coin:
print(1)

else:

end = n // coin[0] + 1
b = False

for i in range(2, end):

for tup in list(C(coin, i)):

if sum(tup) == n:

print(i)
b = True
break
if b: break

40.1

2.3

7.2

2.0
2.0

0.3
0.1
2.4
3.4
0.3
0.3
0.6

0.1

1.3

7.1

2.8

0.2
1.5

0.1
0.3
0.7
1.2
0.0
0.1
0.9

1.4

BASELINE

RESOURCE DESCRIPTION

Figure 7: The target error kind is NO ERROR. BASELINE correctly predicts NO ERROR with conﬁ-
dence 0.416. DOCSTRING also correctly predicts NO ERROR with conﬁdence 0.823. The BASELINE
instruction pointer value makes its largest probability mass contribution to nerror at n = 0 and ends
up with mass split between nexit and nerror. The DOCSTRING instruction pointer value accumulates
little probability in nerror and ends up with most probability mass in nexit.

20

051015200246810121416051015200246810121416