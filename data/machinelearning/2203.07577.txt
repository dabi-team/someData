2
2
0
2

r
a

M
5
1

]

G
L
.
s
c
[

1
v
7
7
5
7
0
.
3
0
2
2
:
v
i
X
r
a

Proceedings of Machine Learning Research vol 167:1–29, 2022

33rd International Conference on Algorithmic Learning Theory

Efﬁcient and Optimal Fixed-Time Regret with Two Experts

Laura Greenstreet
Nicholas J. A. Harvey
Victor Sanches Portella
University of British Columbia, Department of Computer Science

LAURA.GREENSTREET@GMAIL.COM
NICKHAR@CS.UBC.CA
VICTORSP@CS.UBC.CA

Editors: Sanjoy Dasgupta and Nika Haghtalab

Abstract
Prediction with expert advice is a foundational problem in online learning. In instances with T
rounds and n experts, the classical Multiplicative Weights Update method suffers at most
regret when T is known beforehand. Moreover, this is asymptotically optimal when both T and
n grow to inﬁnity. However, when the number of experts n is small/ﬁxed, algorithms with better
regret guarantees exist. Cover showed in 1967 a dynamic programming algorithm for the two-
T /2π + O(1) regret with O(T 2)
costs that suffers at most
experts problem restricted to
pre-processing time. In this work, we propose an optimal algorithm for prediction with two experts’
advice that works even for costs in [0, 1] and with O(1) processing time per turn. Our algorithm
builds up on recent work on the experts problem based on techniques and tools from stochastic
calculus.
Keywords: experts, Cover, online learning, optimal, ﬁxed-time

0, 1

p

p

}

{

(T /2) ln n

1. Introduction

The foundational problem in online learning of prediction with expert advice (or simply experts’
problem) consists of a sequential game between a player and an adversary. In each turn, the player
chooses (possibly randomly) one of n experts to follow. Concurrently, the adversary chooses for
each expert a cost in [0, 1]. At the end of a turn, the player sees the costs of all experts and suffers
the cost of the expert they followed. The performance of the player is usually measured by the
regret: the difference between their cumulative loss and the cumulative loss of the best expert in
hindsight. In this case, we are interested in strategies for the player whose (expected) regret against
any adversary is sublinear in the total number of rounds of the game.

A well-known strategy for the player is the Multiplicative Weights Update (MWU) method
(Arora et al., 2012). In the ﬁxed-time setting — that is, when the player knows beforehand the total
number of rounds T — MWU with a carefully-chosen ﬁxed step-size suffers at most
(T /2) ln n
regret. Additionally, this regret bound is asymptotically optimal when both n and T grow to inﬁn-
ity (Cesa-Bianchi et al., 1997). Yet, if the number of experts n is ﬁxed/small, better regret guarantees
may be possible. From a theoretical standpoint, there is a clear motivation for the case where n is
(T /2) ln n for any n as the number of rounds T
ﬁxed: MWU can suffer regret arbitrarily close to
grows1 (Gravin et al., 2017). This means that different ideas are necessary for player strategies to
p
guarantee smaller regret.

p

1. This is known to hold when MWU is used with a ﬁxed or decreasing step-size, which are the usual cases when MWU
is applied to the experts’ problem. When the step-size of MWU is allowed to be arbitrary, Gravin et al. (2017) show
that MWU can suffer regret arbitrarily close to (2/3)p(T /2) ln n as the number of rounds T grows.

© 2022 L. Greenstreet, N. J. A. Harvey & V.S. Portella.

 
 
 
 
 
 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

1.1. The Case of Two Experts

p

(T /2) ln n is even possible as T
Of course, a natural question is whether regret smaller than
grows even if n is ﬁxed. Cover (1967) showed that for two experts (that is, for n = 2) a regret of
T /2π+O(1) is the best possible in the worst-case by showing an algorithm for the case with costs
. In related work, the attainable worst-case regret bounds for three (Abbasi-Yadkori et al.,
in
0, 1
p
}
{
2017; Kobzar et al., 2020) and four (Bayraktar et al., 2020) experts were recently improved, respec-
T π/8 up to lower-order terms. Although optimal, Cover’s
tively, to
algorithm is based in a dynamic programming approach that takes O(T 2) time in total. In compar-
ison, MWU takes O(1) time per round to compute the probabilities to assign to the two experts at
each round. Finally, one can adapt Cover’s algorithm for costs in [0, 1], but the standard approach
is to randomly round to costs to either 0 or 1.
In this case, the regret guarantees only hold in
expectation.

8T /(9π) + O(ln T ) and

p

p

In a related line of work, Harvey et al. (2020b) study the 2-experts problem in the anytime set-
ting, that is, in the case where the player/algorithm does not know the total number of rounds T
N is
ahead of time. They showed an optimal strategy for the player whose regret on any round t
at most (γ/2)√t, where the constant γ
1.30693 arises naturally in the study of Brownian motion
(see Mörters and Peres, 2010 for an introduction to the ﬁeld and historical references). Moreover,
their algorithm can be computed (up to machine precision) in O(1) time since it boils down to the
evaluation of well-known mathematical functions such as the exponential function and the imagi-
nary error function. In this work, we combine similar ideas based on stochastic calculus together
with Cover’s algorithm to propose an efﬁcient and optimal ﬁxed-time algorithm for 2-experts.

≈

∈

Known result (Cover, 1967): There is a dynamic programming algorithm for the 2-experts prob-
lem with costs in
T /2π + 0.5 regret in games with T rounds and
0, 1
}
{
requires O(T 2) pre-processing time.

that suffers at most

p

Our contribution: An algorithm for the two experts’ problem with costs in [0, 1] that suffers at
T /2π + 1.3 regret in games with T rounds. This new algorithm has running time O(T ) and
most
is based on discretizing a continuous-time solution obtained using ideas from stochastic calculus.

p

More precisely, one of the key steps is deriving a player in the continuous-time setting from Harvey et al.

(2020b) that exploits the knowledge of the time-horizon to obtain regret bounds better than in the
anytime setting. However, unlike the anytime setting, discretizing this algorithm leads to non-
negative discretization error. Another key contribution of our paper is showing that this discretiza-
tion error is small. Finally, the connections to Cover’s classical algorithm sheds new intuition into
the classical optimal solution. Interestingly, our results could be formally presented without resort-
ing to stochastic calculus. Yet, it is the stochastic calculus point of view that guides us through the
design and analysis of the algorithm.

Text organization: We ﬁrst formally deﬁne the experts problem and discuss some assumptions
and simpliﬁcations in Section 2. In Section 3 we present a brief summary of Cover’s optimal algo-
rithm for two experts. In Section 4 we deﬁne an analogous continuous-time problem and describe
a solution inspired by Cover’s algorithm. Finally, in Section 5 we present and analyze a discretized
version of the continuous-time algorithm, showing it enjoys optimal worst-case regret bounds.

2

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

2. Prediction with Expert Advice

A

∈
that, given cost vectors ℓ1, . . . , ℓt ∈

In this section we shall more precisely deﬁne the problem of prediction with expert advice. The
N of experts. A (strategy for the) player is
problem is parameterized by a ﬁxed number n
[0, 1]n chosen by the adversary in previous
a function
∆n :=
rounds, outputs a probability distribution over the n experts represented by a vector xt+1 ∈
that, given
x
{
previous player’s choices x1, . . . , xt ∈
∆n of distributions over the experts, outputs a vector ct+1 ∈
[0, 1]n of expert costs for round t + 1, where ℓt+1(i) is the cost of expert i
.
1, . . . , n
}
{
The performance of a player strategy
is
measured by the regret, deﬁned as

n
. Similarly, a (strategy for the) adversary is a function
i=1 x(i) = 1
}

∈
N rounds against an adversary

in a game with T

[0, 1]n :

[n] :=

P

A

∈

∈

B

B

Regret(T,

,

A

B

T

) :=

ℓ

T

t xt −

min
[n]
i
∈

T

ℓt(i),

t=1
X

(x1, . . . , xt

t=1
X
where above, and for the remainder of this section2 we have xt

:=
[T ]. Moreover, whenever the loss vectors ℓ1, . . . , ℓT are clear from
B
t
context, we deﬁne the cumulative loss of expert i
[T ] by Lt(i) :=
j=1 ℓt(i).
AT for the player that suffers regret at
In this text, for each T
most sublinear in T against any adversary in a game with T rounds. That is, we want a family of
strategies

∈
N we want to devise a strategy

[n] at round t

1) for all t

(ℓ1, . . . , ℓt

N such that

1) and ℓt

P

:=

A

∈

∈

∈

−

−

{AT }T

∈

1
T

lim
T
→∞

sup

Regret(T,

B

AT ,

B

) = 0,

(1)

where the supremum ranges over all possible adversaries, even those that have full knowledge of
(and may even be adversarial to) the player’s strategy.

2.1. Restricted Adversaries

∈

[T ].

(Rn)T , we denote by

In (1), the supremum ranges over all the possible adversaries for a game with T rounds. How-
ever, we need only consider in the supremum oblivious adversaries (Karlin and Peres, 2017, Sec-
whose choice on each round depends only on the round number
tion 18.5.4), that is, adversaries
B
and not on the choices of the player. For any ℓ = (ℓ1, . . . , ℓT )T
Bℓ the
oblivious adversary that plays ℓt on round t
In fact, we may restrict our attention to even smaller sets of adversaries (for details on these
reductions, see Gravin et al., 2016 and Karlin and Peres, 2017, Section 18.5.3). First, in (1) we
need only to consider binary adversaries, that is, adversaries which can assign only costs in
0, 1
}
{
to the experts. Furthermore, to obtain the value of the optimal regret for two experts we only need
, which we call restricted
to consider adversaries that pick vector costs in
}
binary adversaries.
Intuitively, the adversary can do no better by placing equal costs on both
experts at any given round. The optimal algorithm for two experts proposed by Cover (1967) heavily
relies on the assumption that the adversary is a restricted binary one and does not extend to general
costs in [0, 1] without resorting to randomly rounding the costs — which makes the regret guarantees
hold only in expectation.

(1, 0)T, (0, 1)T
{

:=

L

∈

2. If no speciﬁc strategies A or B are clear from the context, one may take A and B to be arbitrary strategies and we

shall omit A and B when they are clear from context.

3

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

In this work we design an algorithm the suffers at most

T /(2π) + O(1) regret for arbitrary
[0, 1] costs. Our initial analysis handles only restricted binary adversaries, but simple concavity
arguments extend the upper bound to general adversaries. Throughout this text we ﬁx a time horizon
T

N.

p

∈

2.2. The Gap Between Experts

−

Lt(1)
|

The case where we have only 2 experts admits a simpliﬁcation that aids us greatly in the design of
upper- and lower-bounds on the optimal regret. Namely, the gap (between experts) at round t
[T ]
, where Lt is the cumulative loss vector at round t as deﬁned in Section 2.
is given by
Lt(2)
|
Furthermore, we denote by lagging expert (on round t
[T ]) an expert with maximum cumulative
loss on round t among both experts. Similarly, we denote by leading expert (on round t
[T ])
an expert with minimum cumulative loss on round t. The following proposition from Harvey et al.
(2020b) shows that, for the restricted binary adversaries described earlier, the regret can be almost
fully characterized by the expert gaps and the player’s choices of distributions on the experts. In the
next proposition (and throughout the remainder of the text), for any predicate P we deﬁne [P ] to be
1 if P is true and 0 otherwise.

∈

∈

∈

Proposition 1 (Harvey et al., 2020a, Proposition 2.3) Fix T
let ℓ1, . . . , ℓT ∈ {
xt :=
(ℓ1, . . . , ℓt
A
on round t, and let gt be the gap between experts on round t. Then,

(1, 0)T, (0, 1)T
}
1), let pt ∈ {
xt(1), xt(2)
}

be a player strategy, and
[T ], set
be the probability mass placed on the lagging expert

be the expert costs chosen by the adversary. For each t

A

∈

∈

−

N, let

T

Regret(T ) =

[gt

−

1 > 0]pt ·

(gt −

gt

−

1) +

1 = 0]ℓ

T
t xt,

[gt

−

T

Xt=1

Xt=1

where g0 := 0. In particular, if for every t

[T ] with gt

−

∈

1 = 0 we have xt(1) = xt(2) = 1/2, then

Regret(T ) =

T

t=1
X

pt ·

(gt −

gt

−

1).

3. An Overview of Cover’s Algorithm

Although in this section we give only a brief overview of Cover’s algorithm, for the sake of com-
pleteness we provide a full description and analysis of the algorithm in Appendix A. The key idea
in Cover’s algorithm is to compute optimal decisions for all possible scenarios beforehand. This
is a feasible approach when we know the total number of rounds and the adversary is a (restricted)
Ap parameterized
binary adversary. More precisely, we will focus our attention to player strategies
[0, 1] which place p(t, g) probability mass on the lagging
by functions p : [T ]
0, . . . , T
expert on round t if the gap between experts is g, and 1
p(t, g) mass on the leading expert. Then
the “regret-to-be-suffered” by

1
} →

× {

−

−
Ap at any round t with a given gap between experts g is
Lt(2)
Bℓ) : ℓ
Ap,
|

Lt(1)
|

Regret(t,

Ap,

Bℓ)

T s.t.

∈ L

−

−

Vp[t, g] := sup
{

Regret(T,

= g

.
}

(2)

We can compute all entries of Vp as deﬁned above via a dynamic programming approach, starting
with Vp[T, g] for all g
and then computing these values for earlier rounds. More-
over, there is a simple strategy p∗ that minimizes the worst-case regret Vp[0, 0]. Interestingly, the

0, . . . , T

1
}

∈ {

−

4

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

worst-case regret of
a symmetric random walk (of length t starting at g) is a sequence of random variables (Si)t
Si := g + X1 +
on
. The next theorem summarizes the guarantees on the regret of
1
}
no more than
any better asymptotically in T (for a complete proof of the lower bound, see Appendix A.5).

Ap∗ given by V ∗[0, 0] is tightly connected with symmetric random walks, where
i=0 with
[t] are i.i.d. uniform random variables
+ Xi for each i
Xj}j
{
∈
Ap∗, showing that it suffers
T /2π + O(1) regret. Moreover, it is worth noting that no player strategy can do

0, . . . , t

∈ {

and

{±

· · ·

}

p

N, let
Theorem 2 (Cover, 1967, and Karlin and Peres, 2017, Section 18.5.3) For every r, g
the random variable Zr(g) be the number of passages through 0 of a symmetric random walk of
length r starting at position g. Then V ∗[t, g] = 1
2

t(g)] for every t, g

N. In particular,

E[ZT

∈

−

∈

V ∗[0, 0] =

1
2

E[ZT (0)]

T
2π

+

1
2

.

≤ r

Finally, although not more efﬁciently computable than the dynamic programming approach, p∗

has a closed form solution (see Karlin and Peres, 2017, Section 18.5.3) given, for t
0, . . . , T
{

, by
1
}

−

p∗(t, g) =

P(ST

t = g) + P(ST

t > g),

−

−

1
2

[T ] and g

∈

∈

(3)

where ST
inspiration for our continuous-time algorithm.

t is a symmetric random walk of length T

−

t. This closed-form solution will serve as

−

4. A Continuous-Time Problem

Cover’s player strategy is optimal, but it is deﬁned only for restricted binary adversaries. It is likely
that it can be extended to binary adversaries, but it is deﬁnitely less clear how to extend such an
algorithm for general adversaries picking costs in [0, 1]. Moreover, even when Cover’s algorithm
can be used, it is quite inefﬁcient: we either need to compute V ∗ which has O(T 2) entries, or at each
round we need to compute the probabilities in (3). In the latter case, in the ﬁrst round we already
need O(T 2) time to exactly compute the probabilities related to a length T

1 random walk.

To devise a new algorithm for the two experts problem, we ﬁrst look at an analogous continuous-
time problem, ﬁrst proposed by Harvey et al. (2020b) and with a similar setting previously studied
by Freund (2009). The main idea is to translate the random walk connection from the discrete case
into a stochastic problem in continuous time, and then exploit the heavy machinery of stochastic
calculus to derive a continuous time solution.

−

4.1. Regret as a Discrete Stochastic Integral

Let us begin by further connecting Cover’s algorithm to random walks. Let
Ap be a player strategy
induced by some function p : [T ]
,
1
0, . . . , T
1
}
} →
then Proposition 1 tells us that, for any restricted binary adversary sequence of gaps g1, . . . , gT ∈
R

R. If p(t, 0) = 1/2 for all t

0, . . . , T

∈ {

× {

−

−

0 and for g0 := 0 we have

≥

Regret(T ) =

T

t=1
X

p(t, gt

−

1)(gt −

gt

−

1).

(4)

5

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

In fact, if (gt)T

The right-hand side of the above equation is a discrete analog of the Riemman-Stieltjes integral
t=0 is a random sequence3, the above is also known as a
of p with respect to g.
discrete stochastic integral. In particular, consider the case where (gt)T
t=0 is a length T reﬂected
(i.e., absolute value of a) symmetric random walk. Then, any possible sequence of deterministic
gaps has a positive probability of being realized by (gt)T
t=0. In other words, any sequence of gaps
is in the support of (gt)T
Ap is equivalent to bounding
almost surely the value of (4) when (gt)T
t=0 is a reﬂected symmetric random walk. This idea will
prove itself powerful in continuous-time even though it is not very insightful for the discrete time
problem.

t=0. Thus, bounding the worst-case regret of

4.2. A Continuous-Time Problem

A stochastic process that can be seen as the continuous-time analogue of symmetric random walks
is Brownian motion (Revuz and Yor, 1999; Mörters and Peres, 2010). We ﬁx a Brownian motion
0 throughout the remainder of this text. Inspired by the observation that the discrete regret
(Bt)t
boils down to a discrete stochastic integral, Harvey et al. (2020b) deﬁne a continuous analogue of
regret as a continuous stochastic integral. More speciﬁcally, given a function p : [0, T )
[0, 1]
such that p(t, 0) = 1/2 for all t

0, deﬁne the continuous regret at time T by

→

×

R

R

≥

∈

≥

ContRegret(T, p) := lim
0
↓

ε

0

Z

T

ε

−

p(t,

) d

Bt|
|

,
Bt|
|

≥

−

ε) of
where the term in the limit of the right-hand above is the stochastic integral (from 0 to T
p with respect to the process (
0. We take the limit as a mere technicality: p need not be
)t
Bt|
|
deﬁned at time T and we want to ensure left-continuity of the continuous regret (the limit is well-
deﬁned since a stochastic integral with respect to a reﬂected Brownian motion is guaranteed to have
limits from the left and to be continuous from the right). It is worth noting that usually stochastic
integrals are deﬁned with respect to martingales or local martingales, but (
0 is neither. Still,
)t
Bt|
|
0 happens to be a semi-martingale, which roughly means that it can be written as a sum
(
)t
Bt|
|
of two processes: a local-martingale and a process of bounded variation. In this case one can still
deﬁne stochastic integrals in a way that foundational results such as Itô’s formula still hold and
details can be found in Revuz and Yor (1999). We do not give the precise deﬁnition of a stochastic
integral since we shall not deal with its deﬁnition directly. Still, one may think intuitively of such
integrals as random Riemann-Stieltjes integrals, although the precise deﬁnition of stochastic integral
is more delicate.

≥

≥

R

[0, 1] with p(t, 0) = 1/2 for all t

Let us now look for a continuous function p : [0, T )

0
with small continuous regret. Note that without the conditions of continuity or the requirement of
p(t, 0) = 1/2 for t
0, the problem would be trivial. If we did not require p(t, 0) = 1/2 for
[0, T ), then taking p(t, g) := 0 everywhere would yield 0 continuous regret. Moreover,
all t
dropping this requirement would go against the analogous conditions needed in the discrete case,
where regret could be written as a “discrete stochastic integral” on Proposition 1 only when the
player chooses (1/2, 1/2)T in rounds with 0 gap. Finally, requiring continuity of the p is a way to
avoid technicalities and “unfair” player strategies.

→

×

≥

≥

∈

3. We usually also require some kind of restriction on (gt)T

t=0, such as requiring it to be a martingale or a local martin-

gale.

6

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

When working with Riemann integrals, instead of manipulating the deﬁnitions directly we use
powerful and general results such as the Fundamental Theorem of Calculus (FTC). Analogously,
the following result, known as Itô’s formula, is one of the main tools we use when manipulating
stochastic integrals and which can be seen as an analogue of the FTC (and shows how stochastic
integrals do not always follow the classical rules of calculus). We denote by C 1,2 the class of
bivariate functions that are continuously differentiable with respect to their ﬁrst argument and twice
continuously differentiable with respect to their second argument. Moreover, for any function f
∈
C 1,2 we denote by ∂tf the partial derivative of f with respect to its ﬁrst argument, and we denote by
∂gf and ∂ggf , respectively, the ﬁrst and second derivatives of f with respect to its second argument.

Theorem 3 (Itô’s Formula, see Revuz and Yor, 1999, Theorem IV.3.3) Let f : [0, T )
be in C 1,2 and let T ′ ∈

[0, T ). Then, almost surely,

R

×

→

R

T ′

T ′

) + 1

0

Z

+

−

) d

)
|

0
Z

) =

f (0,

f (T ′,

∂tf (t,

Bt|
|

Bt|
|

∂gf (t,

B0|
|

BT ′
|

Bt|
|
Note that the ﬁrst integral in the equation of the above theorem resembles the deﬁnition of the
continuous regret. In fact, the above result shows an alternative way to write the continuous regret
C 1,2 with ∂gR = p. However,
at time T of a function p : [0, T )
it might be hard to compute (or even to bound) the second integral on (5). A straightforward way
to circumvent this problem is to look for functions such that the second integral in (5) is 0. For that,
R,
it sufﬁces to consider functions R
that is,

C 1,2 that satisfy the backwards heat equation on [0, T )

[0, 1] such that there is R

2 ∂ggf (t,

)
Bt|
|

dt. (5)

→

×

×

R

∈

∈

(cid:3)

(cid:2)

1
2
×
We summarize the above discussion and its implications in the following lemma.

∗∆R(t, g) := ∂tR(t, g) +

∂ggR(t, g) = 0,

[0, T )

(t, g)

∈

∀

R.

(BHE)

Lemma 4 Let R : [0, T )
(t, g)
×
ContRegret(T, p) almost surely.

[0, T )

R

∈

≥

0, such that (BHE) holds and such that R(0, 0) = 0. Then limt
T R(t,
↑

[0, 1] be in C 1,2 and such that ∂gR(t, g) = p(t, g) for all
) =

→

×

Bt|
|

R

4.3. A Solution Inspired by Cover’s Algorithm

In the remainder of this text we will make extensive use of a well-known function related to the
Gaussian distribution known as complementary error function, deﬁned by

erfc(z) := 1

2
√π

−

z

0

Z

x2

e−

dx =

2
√π

∞

x2

e−

dx,

R.

z

∀

∈

z
Z
, T )

In Section 4.4 we will show that the function Q : (

−∞

R

×

→

[0, 1] in C 1,2 given by

Q(t, g) :=

1
2

erfc

g
2(T

,

t) !

−

(t, g)

∀

(
−∞

∈

, T )

×

R

p

satisﬁes ContRegret(T, Q) =
is enlightening to see how Q is related to Cover’s algorithm.

T /(2π) almost surely. Before bounding the continuous regret, it

Speciﬁcally, let p∗ be as in (3). Due to the Central Limit Theorem, Q can be seen as an approxi-
1 and

mation of p∗. To see why, let (St)∞t=0 be a symmetric random walk, and deﬁne Xt := St −

St

−

p

7

 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

1. Note that Yt follows a Bernoulli distribution with parameter 1/2
1. Moreover, let Z be a Gaussian random variable with mean 0 and variance 1. Then,

≥

µ)2] = 1, the Central Limit Theorem guarantees

≥

Yt := (Xt + 1)/2 for each t
for any t
by setting µ := E[2Y1] = 1 and σ2 := E[(2Y1 −
1
√L

1
√L

1
√L

SL =

Xi =

L

L

(2Yi −

Xi=1

Xi=1

1) =

√L
σ  

1
L

L

Xi=1

2Yi −

µ

!

L
→∞
−→

Z,

where the limit holds in distribution. Thus, we roughly have that SL and √LZ have similar distri-
butions. Then,

p∗(t, g) =

1
2

P(ST

−

t = g) + P(ST

t > g)

−

≈

P((√T

−

t)Z = g) + P((√T

t)Z > g)

−

1
2
g

= P

Z >

(cid:18)

g
√T

−

t

(cid:19)

=

1
2

erfc

= Q(t, g).

2(T

t) !

−

p

One may already presume that using Q in place of p∗ in the discrete experts’ problem should yield
a regret bound close enough to the guarantees on the regret of Cover’s algorithm. Indeed, using
Berry-Esseen’s Theorem (Durrett, 2019, Section 3.4.4) to more precisely bound the difference be-
tween p∗ and Q yields a O(√T ) regret bound with suboptimal constants against binary adversaries.
However, it is not clear if the approximation error would yield the optimal constant in the regret
bound. Additionally, these guarantees do not naturally extend to arbitrary experts’ costs in [0, 1]. In
Section 5 we will show how to use an algorithm closely related to Q that enjoys a clean bound on
the discrete-time regret.

Deriving Q directly from a PDE. We have derived Q by a heuristic argument to approximate p∗.
Yet, one can derive the same solution without ever making use of p∗ by approaching the problem
directly from the stochastic calculus point of view. Namely, consider player strategies that satisfy the
BHE, are non-negative, and that place 1/2 mass on each expert when the gap is 0. With only these
conditions we would end up with anytime solutions similar to the ones considered by Harvey et al.
(2020b). In the ﬁxed-time case we can “invert time” by a change of variables t
t. Then the
BHE becomes the traditional heat equation, which Q satisﬁes together with the boundary conditions.

←

−

T

4.4. Bounding the Continuous Regret

Interestingly, not only is Q in C 1,2, but it also satisﬁes the backwards heat equation, even though
we have never explicitly required such a condition to hold. Since the proof of this fact boils down
to technical but otherwise straightforward computations, we defer it to Appendix C.

Lemma 5 For all t

[0, T ) and g

∈

R

≥

∈

0 we have ∗∆Q(t, g) = 0.

However, recall that to use Lemma 4 we need a function R

C 1,2 with ∂gR = Q that satisﬁes
the backwards heat equation, not necessarily Q itself needs to satisfy the backwards heat equation.
Luckily enough, the following lemma shows how to obtain such a function R based on Q.

∈

Proposition 6 (Harvey et al., 2020a, Lemma 5.6) Let h : [0, T )

R

×

→

R be in C 1,2 and deﬁne

g

f (t, g) :=

h(t, y) dy

0
Z

1
2

−

t

0

Z

Then,

∂gh(s, 0) ds,

(t, g)

∀

∈

[0, T )

R.

×

8

 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

(i) f

∈

C 1,2,

(ii) If h satisﬁes (BHE), then so does f ,

(iii) h = ∂gf .

In light of the above proposition, for all (t, g)

∈

g

R(t, g) :=

Q(t, x) dx

0

Z

R deﬁne

, T )

×

t

∂gQ(s, 0) ds.

(
−∞
1
2

−

0
Z

In the case above, we can evaluate these integrals and obtain a formula for R that is easier to analyze.
y
Although we defer a complete proof of the next equation to Appendix C, using that
0 erfc(x) dx =
x2 we
2
y erfc(y)
√π e−
−
can show for every t

√π (Olver et al., 2010, Section 7.7(i)) and that d
+ 1
(
−∞

dx erfc(x) =
R

, T ) and g

1
√π e−

R that

−

∈

∈

y2

R(t, g) =

g
2

erfc

g

2(T

t

T

−
2π

exp

g2

 −

2(T

t) !

−

T
2π

.

+

r

t) ! − r

−

(6)

Since R satisﬁes (BHE), Lemma 4 shows the continuous regret of Q is given exactly by R. The
following lemma shows a bound on R and, thus, a bound on the continuous regret of Q.

p

Lemma 7 We have R(0, 0) = 0 and

R(t, g)

T
2π

,

≤ r

(t, g)

∀

∈

[0, T )

R.

×

Proof The facts that R(0, 0) = 0 and that R(t, g)
For the bound on R for g > 0, note ﬁrst that for any z > 0 we have

0 for g

≤

≤

0 and t

[0, T ) are easily veriﬁable.

∈

erfc(z) =

2
√π

Therefore, for all (t, g)

z
Z

∈

∞

ex2

dx =

2
√π

z

Z

∞

2x
2x

ex2

dx

1
z√π

≤

z

Z

∞

2xex2

dx =

z2

e−
z√π

.

[0, T )

×

R>0 we have

g
2

erfc

g
2(T

t) ! ≤

−

g
2 · p

2(T

−

t) exp

g√π

(cid:16)

Applying the above to (6) yields the desired bound.

p

g2

−
2(T

t)

−

=

(cid:17)

t

T

−
2π

r

exp

−
2(T

(cid:18)

g2

−

.

t)

(cid:19)

Combining these results we get the desired bound on the continuous regret of Q, which we

summarize in the following theorem.

Theorem 8 We have ContRegret(T, Q)

T /(2π) almost surely.

≤

p

9

 
 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

5. From Continuous to Discrete Time

In the continuous time algorithm we have that R(t, g) is the continuous regret at time t with gap g
of the strategy that places probability mass on the lagging expert4 according to Q(t, g) = ∂gR(t, g).
At the same time, for Cover’s algorithm we have V ∗[t, g] as an upper-bound on the regret when
the mass on the lagging expert is given by p∗(t, g). Furthermore, similar to the relation between Q
and R, we can write p∗ as a function of V ∗ (details can be found on Appendix A): at round t with
gap g at round t

1, the probability mass placed on the lagging expert in Cover’s algorithm is5

−

p∗(t, g) =

V ∗[t, g

1]

−

−
2

V ∗[t, g + 1]

∂gV ∗[t, g].

≈

That is, p∗ is a sort of discrete derivative of V ∗ with respect to its second argument. From this
analogy, one might expect that a discrete derivative of R with respect to its second argument yields
a good strategy for the player in the original experts’ problem. As we shall see, this is exactly the
case. Additionally, computing the discrete derivative of R amounts to a couple of evaluations of the
complementary error function, which we can assume to be computable (up to machine precision) in
constant time.

In this section we shall describe the discretized algorithm and give an upper-bound on its re-
.
gret against restricted binary adversaries, that is, adversaries that choose costs in
}
Luckily, unlike Cover’s algorithm, the strategy we shall see in this section smoothly extends to gen-
eral costs in [0, 1] while preserving its performance guarantees. Since the details of this extension
amounts to concavity arguments, we defer the details of this extension to Appendix E.

(0, 1)T, (1, 0)T
{

5.1. Discrete Itô’s Formula

In Section 4, the main tool to relate the continuous regret to the function R was Itô’s formula. Sim-
ilarly, one of the main tools for the analysis of the discretized continuous-time algorithm will be a
discrete version of Itô’s formula. In order to state such a formula and to describe the algorithm, some
standard notation to denote discrete derivative will be useful. Namely, for any function f : R2
R
and any t, g

R, deﬁne

→

∈

fg(t, g) :=

f (t, g + 1)

f (t, g

1)

,

−

−
2
f (t

ft(t, g) := f (t, g)
fgg(t, g) := f (t, g + 1) + f (t, g

−

−

1, g),

1)

−

−

2f (t, g).

We are now in place to state a discrete analogue of Itô’s formula. One important assumption
R are such that successive values have absolute difference
of the next theorem is that g0, . . . , gT ∈
equal to 1.
In the case where g0, . . . , gT are gaps in a 2-experts problem, this means that the
adversary needs to be a restricted binary adversary. The version of the next theorem as stated —
including the dependence on t — can be found in Harvey et al. (2020b, Lemma 3.7). Yet, this
theorem is a slight generalization of earlier results such as the ones due to Fujita (2008, Section 2)
and Kudzhma (1982, Theorem 2)

4. We have never formally deﬁned lagging and leading experts in continuous time, and we do not intend to do so. Here
we are extrapolating the view given by Proposition 1 of regret as a stochastic integral of the probability put on the
lagging expert with respect to the gaps for the sake of intuition.

5. For g = 0 this does not follow directly, but our goal at the moment is only to build intuition.

10

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Theorem 9 (Discrete Itô’s Formula) Let g0, g1, . . . , gT ∈
R. Then,
t
∈

[T ] and let f : R2

→

R be such that

gt −
|

gt

−

1|

= 1 for every

f (T, gT )

−

f (0, g0) =

T

t=1
X

fg(t, gt

−

1)(gt −

gt

−

1) +

T

t=1
X

(cid:0)

1
2 fgg(t, gt
−

1) + ft(t, gt

−

1)

.

(cid:1)

The ﬁrst summation in the right-hand side of discrete Itô’s formula can be seen as a discrete
stochastic integral when (gt)T
t=0 is a random sequence. Remarkably, this term is extremely sim-
ilar to the regret formula from Proposition 1. Thus, if we were to use discrete Itô’s formula to
bound the regret, it would be desirable for the second term to (approximately) satisfy an analogue
of (BHE). In fact, the potential V ∗ from Cover’s algorithm satisﬁes the discrete BHE (with some
care needed when the gap is zero, see Appendix A.3). Furthermore, the connection between BHE
seems to extend to other problems in online learning: in recent work, Zhang et al. (2022) showed
how coin-betting with potentials that satisfy the BHE yield optimal algorithms for unconstrained
online learning.

Since R satisﬁes (BHE), one might hope that R would also satisfy such a discrete backwards-
heat inequality, yielding an upper-bound on the regret of the strategy given by Rg. In the work of
Harvey et al. (2020b) in the anytime setting, it was the case that the terms in the second sum were
non-negative, which in a sense means that the discretized algorithm suffers negative discretization
error. In the ﬁxed-time setting we are not as lucky.

5.2. Discretizing the Algorithm

Based on the discussion at the beginning of this section, a natural way to discretize the algorithm
from Section 4 is to deﬁne the function q : [T ]

0, . . . , T

R by

× {

1
} →

−

q(t, g) :=

Rg(t, g)
[g = 0] 1
2

(

if t < T,
if t = T,

t

∀

∈

[T ],

g

∀

0, . . . , T

∈ {

,
1
}

−

where we need to treat the case at the very last step differently since R is not deﬁned on
It is not clear from its deﬁnition, but we indeed have q(t, 0) = 1/2 for all t
(relatively technical) proof of the next result to Appendix D.

∈

R.
T
{
[T ]. We defer the

} ×

Lemma 10 We have q(t, 0) = 1/2 for all t

[T ].

∈

Our goal now is to combine Proposition 1 and the discrete Itô’s formula to bound the regret
Aq. Since R satisﬁes the (BHE), one might hope that R is close to satisfying the discrete version
(
−∞
∈

of
of this equation. To formalize this idea, for all t

, T ) and g

R deﬁne

rgg(t, g) := ∂ggR(t, g)

Rgg(t, g)

and

−

∈
rt(t, g) := ∂tR(t, g)

Rt(t, g).

−

The above terms measure how well the ﬁrst derivative with respect to the ﬁrst variable and the second
derivative with respect to the second variable are each approximated by their discrete analogues.
That is, these are basically the discretization errors on the derivatives of R. Then, combining the
fact that R satisﬁes (BHE) together with Proposition 1 yields the following theorem.

11

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Theorem 11 Consider a game of
by g0, g1, g2, . . . , gT ∈ {

0, . . . , T

}

Aq with a restricted binary adversary with gap sequence given
such that g0 = 0 and
1|
T

= 1 for all t

[T ]. Then,

gt −
|

gt

∈

−

T

1

1

Regret(T )

≤ r

T
2π

+

+

1
2

1
2

−

t=1
X

rgg(t, gt

−

1) +

rt(t, gt

−

1).

−

t=1
X

Proof Lemma 10 and Proposition 1 yield

Regret(T ) =

T

t=1
X

q(t, gt

−

1)(gt −

1)

gt

−

≤

q(t, gt

−

1)(gt −

gt

−

1) +

1
2

,

T

1

−

t=1
X

(7)

(8)

where in the last inequality we used that q(T, gT
formula (Theorem 9), we have

1)

−

≤

1/2. Furthermore, by the discrete Itô’s

R(T

1, gT

1)

−

−

−

T

1

−

R(0, g0) =

t=1
X
T

(BHE)
=

Rg(t, gt

−

1)(gt −

gt

−

1) +

1

−

Xt=1

q(t, gt

−

1)(gt −

1)

gt

−

T

1

−

t=1
X
(cid:0)
1
T
−

−

Xt=1

(cid:0)

1
2 Rgg(t, gt
−

1) + Rt(t, gt

−

1)

1
2 rgg(t, gt
−

1) + rt(t, gt

−

(cid:1)

1)

(cid:1)

(8)

≥

Regret(T )

1
2 −

−

1
2 rgg(t, gt
−

1) + rt(t, gt

−

1)

.

T

1

−

t=1
X

(cid:1)
Rearranging and using the facts given by Lemma 7 that R(0, 0) = 0 and that R(T

(cid:0)

1, gT

1)

−

−

≤

T /(2π) yield the desired bound on the regret.

p

5.3. Bounding the Discretization Error

In light of Theorem 11, it sufﬁces to bound the accumulated discretization error of the derivatives
to obtain potentially good bounds on the regret of
Aq. The next two lemmas show that both rt(t, g)
and rgg(t, g) are in O((T

3/2). Since

t)−

−

T

1

−

(T

t=0

Z

−

t)−

3/2 dt = 2

1

(cid:18)

1
√T (cid:19)

−

2,

≤

(9)

≤

T

−

1
(T

3/2

t)−

−

t=1
X
Aq suffers at most

this will show that
relatively technical but otherwise not considerably insightful, we defer them to Appendix D.

T /2π + O(1) regret.6 Since the proof of these bounds are

p

(
−∞

, T ) and g

∈

R we have

Lemma 12 For any t

rt(t, g)

≤

∈
√2
8√π ·

1
t)3/2

−

(T

and

rgg(t, g)

2√2
3√π ·

≤

(T

1
t)3/2

−

.

Combining the above lemmas together with Theorem 11 yields the following regret bound.

6. This together with Prop. 1 also shows that the difference between in the regret of Aq and AQ is in O(1).

12

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Theorem 13 Deﬁne q(t, g) := Rg(t, g) for all (t, g)
against a restricted binary adversary. Then,

0, . . . , T

∈ {

2, consider a game of
1
}

Aq

−

Regret(T )

T
2π

≤ r

+ 1.24.

Proof Let g1, g2, . . . , gT ∈ {

0, . . . , T

}

be the gap sequence and set g0 := 0. We have

Regret(T )

T
2π

T
2π

T
2π

+

+

+

1
2

1
2

1
2

≤ r

≤ r

≤ r

+

1
2

+

+

T

1

−

T

1

−

1) +

rgg(t, gt

−

t=1
X
√2
8√π

√2
4√π

+

+

t=1
X
1

(T

T

−

√2
3√π !
2√2
3√π ! ≤ r

t=1
X

1
t)3/2

−

T
2π

+ 1.24,

rt(t, gt

−

1)

(by Theorem 11),

(by Lemma 12),

(by (9)).

6. On Optimal Regret for More than Two Experts

In this paper we presented an efﬁcient and optimal algorithm for two experts in the ﬁxed time-
setting. A natural question is whether similar techniques can be used to ﬁnd the minimax regret
when we have more than two experts. Encouragingly, techniques from stochastic calculus were also
used to ﬁnd the optimal regret for 4 experts (Bayraktar et al., 2020). Yet, it is not clear how to use
similar techniques for cases with arbitrary number of experts. The approach used in this paper and
by Harvey et al. (2020b) heavily relies on the gap parameterization for the problem. Although there
is an analogous parameterization of the n experts’ problem into n
1 gaps that yields a claim similar
to Proposition 1, it is not clear what would be an analogous continuous-time problem to guide us
in the algorithm design process since the gap process are not independent—even with independent
costs on the experts. Moreover, many of the approaches in related work (Abbasi-Yadkori et al.,
2017; Harvey et al., 2020b; Bayraktar et al., 2020) focus on speciﬁc adversaries such as the comb
adversary. However, the latter does not seem to be a worst-case adversary for cases such as for
ﬁve experts (Chase, 2019). We are not aware of adversaries that could yield worst-case regret for
arbitrary ﬁxed number of experts, although asymptotically in n and T it is well-known that assigning
0, 1
}
{

costs at random is minimax optimal (Cesa-Bianchi and Lugosi, 2006)

−

Acknowledgments

We would like to thank the anonymous ICML 2022 reviewers for their insightful comments. In
particular, reviewer 1 suggested the use of Berren-Esseen-like results to derive O(√T ) regret, noted
the O(1) regret difference between
N. Harvey was supported by an NSERC Discovery Grant.

AQ, and found a calculation mistake.

Aq and

13

 
 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

References

Yasin Abbasi-Yadkori, Peter L. Bartlett, and Victor Gabillon. Near minimax optimal players for the
ﬁnite-time 3-expert prediction problem. In Annual Conference on Neural Information Processing
Systems (NIPS), pages 3033–3042, 2017.

Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-

algorithm and applications. 8:121–164, 2012.

Erhan Bayraktar, Ibrahim Ekren, and Xin Zhang. Finite-time 4-expert prediction problem. Commu-

nications in Partial Differential Equations, 45(7):714–757, 2020.

Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge University

Press, 2006. ISBN 978-0-521-84108-5.

Nicolò Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and
Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427–485, 1997.

Zachary Chase. Experimental evidence for asymptotic non-optimality of comb adversary strategy.

12 2019. URL http://arxiv.org/abs/1912.01548.

Thomas M. Cover. Behavior of sequential predictors of binary sequences. In Trans. Fourth Prague
Conf. on Information Theory, Statistical Decision Functions, Random Processes (Prague, 1965),
pages 263–272. Academia, Prague, 1967.

Rick Durrett. Probability—theory and examples, volume 49 of Cambridge Series in Statistical and

Probabilistic Mathematics. Cambridge University Press, Cambridge, 2019. Fifth edition.

Yoav Freund.

A method for hedging in continuous time.

October 2009.

URL

http://arxiv.org/abs/0904.3356.

Takahiko Fujita. A random walk analogue of Lévy’s theorem. Studia Sci. Math. Hungar., 45(2):

223–233, 2008.

Nick Gravin, Yuval Peres, and Balasubramanian Sivan. Towards optimal algorithms for prediction
with expert advice. In Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 528–547. ACM, New York, 2016.

Nick Gravin, Yuval Peres, and Balasubramanian Sivan. Tight lower bounds for multiplicative
weights algorithmic families. In 44th International Colloquium on Automata, Languages, and
Programming (ICALP), volume 80, pages Art. No. 48, 14. 2017.

J. A. Harvey, Christopher Liaw,

Nicholas
hawa.
anytime
https://arxiv.org/abs/2002.08994v2.

Optimal

regret with two experts.

Edwin Perkins,

and Sikander Rand-
URL

February 2020a.

Nicholas J. A. Harvey, Christopher Liaw, Edwin A. Perkins, and Sikander Randhawa. Optimal
anytime regret for two experts. In 61st IEEE Annual Symposium on Foundations of Computer
Science (FOCS), pages 1404–1415. IEEE, 2020b.

14

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Anna R. Karlin and Yuval Peres. Game theory, alive. American Mathematical Society, Providence,

RI, 2017.

Vladimir A. Kobzar, Robert V. Kohn, and Zhilei Wang. New potential-based bounds for prediction
with expert advice. In Conference on Learning Theory, (COLT), volume 125 of Proceedings of
Machine Learning Research, pages 2370–2405. PMLR, 2020.

R. Kudzhma. Itô’s formula for a random walk. Litovsk. Mat. Sb., 22(3):122–127, 1982.

Peter Mörters and Yuval Peres. Brownian motion, volume 30 of Cambridge Series in Statistical and

Probabilistic Mathematics. Cambridge University Press, Cambridge, 2010.

Frank W. J. Olver, Daniel W. Lozier, Ronald F. Boisvert, and Charles W. Clark, editors. NIST hand-
book of mathematical functions. U.S. Department of Commerce, National Institute of Standards
and Technology, Washington, DC; Cambridge University Press, Cambridge, 2010.

Daniel Revuz and Marc Yor. Continuous martingales and Brownian motion, volume 293 of
Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sci-
ences]. Springer-Verlag, Berlin, third edition, 1999.

Herbert Robbins. A remark on Stirling’s formula. Amer. Math. Monthly, 62:26–29, 1955.

Zhiyu Zhang, Ashok Cutkosky, and Ioannis Paschalidis. PDE-based optimal strategy for uncon-
strained online learning. January 2022. URL https://arxiv.org/abs/2201.07877.

Appendix A. Cover’s Algorithm for Two Experts

In this section, we shall review the optimal algorithm for the 2-experts problem originally proposed
by Cover (1967) and the matching lower-bound.

A.1. A Dynamic Programming View

In the ﬁxed-time setting we know the total number of rounds before the start of the game. Thus, we
may compute ahead of time all the possible states the game can be on each round and decide the
probabilities on the experts the player should choose in each case to minimize the worst-possible
[0, 1] that represents our
regret. More speciﬁcally, we start with a function p : [T ]
0, . . . , T
1
} →
player strategy: for any t
,
[T ], if at the end of a round t
1 the experts’ gap is g
1
0, . . . , T
∈ {
}
on round t the player places p(t, g) probability mass on the lagging7 expert and 1
p(t, g) probability
Ap such a player strategy deﬁned by p. Now, for all
on the leading expert, and we denote by
, denote by Vp[t, g] the maximum regret-to-be-suffered by the
t
}
player strategy deﬁned by p on rounds t + 1, . . . , T given that at the end of round t the gap between
experts is g. Slightly more formally, we have

0, . . . , T

0, . . . , T

×{
−

and g

∈ {

∈ {

−

−

−

∈

}

Vp[t, g] := sup
{

Regret(T,

Ap,

Bℓ)

−

Regret(t,

Ap,

Bℓ) : ℓ

∈ L

T such that

Lt(1)
|

Lt(2)
|

−

= g

,
}
(10)

7. When the gap is 0, which means that both experts have the same cumulative loss, we break ties arbitrarily. For the
optimal algorithm we shall ultimately derive this will not matter since we will have p(t, 0) = 1/2 for all t ∈ [T ].

15

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

(1, 0)T, (0, 1)T
{

:=

where
. Above we take the supremum instead of the maximum only to
}
account for cases where the set we are considering is empty (and, thus, the supremum evaluates to
), such as when t and g have distinct parities or when g > t. Note that by deﬁnition of Vp we

L

−∞
have

Vp[0, 0] = max
T
∈L

ℓ

Regret(T,

Ap,

Bℓ).

Ap against restricted
Thus, if we compute Vp[0, 0], then we have a bound on the worst-case regret of
binary adversaries. The following proposition shows how we can compute this value in a dynamic
programming style.

Theorem 14 For any p : [T ]
Vp[t, g]

we have

=

−∞

0, . . . , T

× {

1
} →

−

[0, 1], and for all t, g

0, . . . , T

∈ {

}

such that

Vp[t, g] = 0

Vp[t, g] = max

Vp[t + 1, g + 1] + p(t + 1, g)
p(t + 1, g)
1]
Vp[t + 1, g

(

if t = T,

(11)

if t < T and g > 0,

(12)

−
Vp[t, g] = Vp[t + 1, 1] + max

−

if t < T and g = 0, .

(13)

p(t + 1, 0), 1
{

p(t + 1, 0)
}

∈ {

−
Proof First, note that (11) clearly holds by the deﬁnition of Vp. To show that equations (12) and (13)
T be a sequence of cost vectors
hold, let t, g
=
0, . . . , T
such that we have gt :=
= g. First, suppose t < T and g > 0. Then there are two
−
cases for ℓ: either the gap gt+1 :=
goes up and gt+1 = gt + 1, or it goes down
and gt+1 = gt −
Regret(T )

be such that Vp[t, g]
Lt(2)
|
Lt+1(1)
|

1. This together with Proposition 1 and the fact that gt = g > 0 implies

Regret(t) = Regret(T )

Lt+1(2)
|

}
Lt(1)
|

. Let ℓ

−∞

∈ L

−

gt)

−
Regret(T )
Regret(T )

=

(

Regret(t + 1) + p(t + 1, gt)(gt+1 −
Regret(t + 1) + p(t + 1, g),
−
p(t + 1, g),
Regret(t + 1)
−

−

if gt+1 = g + 1
if gt+1 = g
1.

−

−

By taking the maximum over all possible cost vectors with gap g at round t we obtain (12). Now
suppose t < T and g = 0. In this case, suppose without loss of generality that 1 is the expert to
whom
Ap breaks ties arbitrarily when the gap is 0).
Proposition 1 together with the fact that ℓt ∈ L

Ap assigns mass p(t, 0) (recall that the strategy

(1, 0)T, (0, 1)T
{

=

Regret(T )

−

Regret(t) = Regret(T )

Regret(t + 1) + ℓ

−
Regret(T )
Regret(T )

=

(

Regret(t + 1) + p(t + 1, g)
Regret(t + 1) + 1

p(t + 1, g)

−

−
−

if ℓt(1) = 1,
if ℓt(1) = 0.

Since the gap on round t + 1 is certainly 1 in this case, taking the maximum over all the adversaries
with gap 0 on round t yields (13).

For the sake of convenience, we redeﬁne Vp[t, g] for all t, g such that Vp[t, g] =

to, instead,
be the value given by the equations from the above theorem.8 This does not affect any of our results
and makes it less cumbersome to design and analyze the algorithm.

−∞

8. There will be places where this deﬁnition requires access to undeﬁned or “out-of-bounds” entries (such as for entries

with gap T and time t < T ). In such cases, we set such undeﬁned/out-of-bounds values to 0.

16

}
T
t xt

6
6
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

A.2. Picking Optimal Probabilities

0, . . . , T

We are interested in a function p∗ : [T ]
[0, 1], if any, that minimizes Vp[0, 0].
To see that there is indeed such a function, note that we can formulate the problem of minimizing
Vp[0, 0] as a linear program using Theorem 14 to design the constraints. Such a linear program
is certainly bounded (the regret is always between 0 and T ) and feasible. Thus, let p∗ : [T ]
0, . . . , T
{
proposition shows that V ∗ can be computed recursively and show how to obtain p∗ from V ∗.

×
[0, 1] be a function that attains minp Vp[0, 0] and deﬁne V ∗ := Vp∗. The next

1
} →

1
} →

× {

−

−

Theorem 15 For each t, g

0, . . . , T

∈ {

}

V ∗[t, g] = 0
1
2

V ∗[t, g] =

(V ∗[t + 1, g + 1] + V ∗[t + 1, g

V ∗[t, 0] = V ∗[t + 1, 1] +

1
2

if t = T or g = T,

1])

−

if t < T and 0 < g < T,

if t < T.

Furthermore, if we deﬁne p∗ :

0, . . . , T
{

2
}

→

[0, 1] by

p∗(t, g) :=

1
2 (Vp∗[t, g
1
2

(

1]

−

−

Vp∗[t, g + 1]),

if g > 0,
if g = 0,

then Vp∗ = V ∗.

t

∀

∈

[T ],

g

∀

0, . . . , T

∈ {

,
1
}

−

Proof Let us show that p∗ as deﬁned in the statement of the theorem attains inf p Vp[0, 0], where the
inﬁmum ranges over all functions from [T ]
to [0, 1]. Note that smaller values of any
0, . . . , T
entry of Vp∗ can only make Vp∗[0, 0] smaller. Thus, showing that p∗(t + 1, g) minimizes Vp∗[t, g] for
all t, g
) sufﬁces
}
to that p∗ minimizes9 Vp∗[0.0]. Moreover, by Theorem 14 and the deﬁnition of p∗ we have that Vp∗
obeys the formulas on the statement of this theorem. Thus, we only need to show that this choice of
p∗ indeed minimizes the entries of Vp∗.

×{
(given that Vp∗[t′, g′] is ﬁxed for t′ ≥

t + 1 and g′ ∈ {

0, . . . , T

0, . . . , T

1
}

1
}

∈ {

−

−

Let us ﬁrst show that

Vp∗[t, g

1]

−

−

Vp∗[t, g + 1]

For t = T , since Vp∗[T,
g

1, . . . , T

]
·
. If g = 1 we have
1
}

∈ {

≡

−

[0, 1] for all g

1, . . . , T

∈

∈ {
0, the above claim clearly holds. Let t

−

1
}

and t

0, . . . , T

∈ {

.
}

(14)

0, . . . , T

∈ {

1
}

−

and

Vp∗[t, g

1]

−

−

Vp∗[t, g + 1] = Vp∗[t + 1, g] +

(Vp∗[t + 1, g] + Vp∗[t + 1, g + 2])

1
2 −

1
2

=

1
2

(Vp∗[t + 1, g]

−

Vp∗[t + 1, g + 2]) +

1
2

.

∗

9. One may fear that choosing p

(t + 1, g) to minimize Vp∗[t, g] might increase other entries, making this argument
> t. Thus, it is not
invalid. However, note that Vp∗ [t, g] depends only on p
hard to see that by proceeding from higher to smaller values of t ∈ {0, . . . , T }, we can in fact pick p∗(t + 1, g) to
minimize Vp∗ [t, g].

(t + 1, g) and entries Vp∗ [t

] with t

, g

∗

′

′

′

17

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

The last term above, by the induction hypothesis, is in [0, 1]. Similarly, if T

1

−

≥

g

≥

2 we have

=

Vp∗[t, g
1
2
1
2

=

1]

−

−

Vp∗[t, g + 1]

(Vp∗[t + 1, g] + Vp∗[t + 1, g

2])

−

−

(Vp∗[t + 1, g

2]

−

−

Vp∗[t + 1, g]) +

1
2
1
2

(Vp∗[t + 1, g] + Vp∗[t + 1, g + 2])

(Vp∗[t + 1, g]

−

Vp∗[t + 1, g + 2]),

and the last term is in [0, 1] by the induction hypothesis. This completes the proof of (14).

∈

Let t

0, . . . , T

[T ] and g

. Let us now show that p∗(t, g) minimizes Vp∗[t, g] given that
1
−
}
are ﬁxed. For g = 0, Theorem 14
entries of the form Vp∗[t′, g′] for t′ ≥
t + 1 and g′ ∈ {
shows that Vp∗[t, 0] = Vp∗[t + 1, 1] + max
[0, 1], which is minimized when
α, α
}
−
α = 1/2 = p∗(t, 0). For g > 0, Theorem 14 tells us that

0, . . . , T
}
for some α

1
{

∈ {

∈

Vp∗[t, g] = max

Vp∗[t + 1, g + 1] + p∗(t + 1, g), Vp∗ [t + 1, g
{
0 and V ∗[t + 1, g + 1]

1] by (14), p∗(t + 1, g) certainly
Since p∗(t + 1, g)
≤
minimizes V ∗[t, g] since it makes both terms in the maximum equal. Finally, (14) guarantees that10
p∗(t + 1, g)

V ∗[t + 1, g

[0, 1].

−

≥

1]

.
p∗(t + 1, g)
}

−

−

∈

A.3. Discrete Backwards Heat Equation

Interestingly, the potential function V ∗ for Cover’s optimal algorithm satisﬁes the discrete back-
wards heat equation when the gap is not . For simplicity, let us focus on the simpler case with
t

[T ] and gap g

1, . . . , T

. Then, taking V [t, T + 1] = 0 we have
1
}

∈ {

−

∈

V ∗t [t, g] = V ∗[t, g]

= V ∗[t, g]

V ∗[t
1
2

−

−

1, g]

−

(V ∗[t, g + 1] + V [t, g

1])

−

(by Theorem 15)

=

=

1
2
1
2

−

−

2V ∗[t, g] + V ∗[t, g + 1] + V [t, g

(
−

1])

−

V ∗gg[t, g].

−

1. Namely, set V [t,
1]

The same holds for the case where the gap is zero, but we need to extend V ∗ for when the gap
1] = V [t, 1] + 1 for t < T . This guarantees that p(t, 0) = 1/2 =
is
1]), so the cases with zero
(1/2)(V [t,
gap agree with the formulas for non-zero gaps in Theorem 15. Interestingly, one may verify that
p∗(t, g) also satisﬁes the discrete BHE by setting p(t,

V [t, 1]) and V ∗[t, 0] = 1/2(V [t + 1, 1] + V [t + 1,

p(t, g) for g

g) = 1

−

−

−

−

0.

−

−

≥

A.4. Connecting the Regret with Random Walks

As argued before, to give an upper-bound on the regret of
Ap∗, where p∗ is as in Theorem 15, we
need only to bound the value of Vp∗[0, 0] = V ∗[0, 0]. Interestingly, the entries of V ∗ have a strong
connection to random walks, and this helps us give an upper-bound on the value of V ∗[0, 0]. In the

10. In fact, it guarantees that p

∗

(t, g) ≤ 1/2. Intuitively this makes sense since we want to give more probability to the

current best/leading expert than to the lagging expert.

18

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

[t] are i.i.d. random variables taking values in
∈

next theorem and for the remainder of the text, a random walk (of length t
0)
≥
is a sequence of random variables (Si)t
and
∈ {
. If we do not specify a starting point of
Xj}j
1
{
}
a random walk, take it to be 0. We say that St is symmetric if P(X1 = 1) = P(X1 =
1) = 1/2.
−
Moreover, a reﬂected random walk (of length t) is the sequence of random variables (
)i
Si|
|
i=0 is a random walk. Finally, we say that a random walk (Si)t
where (Si)t
if the event

∈{
i=0 passes through g

i=0 where Si := g +X1 +

∈
+Xi for each i

happens for some i

∈
0, . . . , t

0,...,t
}
N

0, . . . , t

{±

· · ·

∈

}

N starting at g

Z

The following lemma gives numeric bounds on the expected number of passages through 0 of a
symmetric random walk. Its proof boils down to careful applications of Stirling’s formula and can
be found in Appendix B.

∈ {

.
}

Si = g
{

}

Lemma 16
symmetric random walk of length T . Then,

Let the random variable ZT (0) be the number of passages through 0 of a reﬂected

2T
π

+

3
5 ≤

r

E[ZT (0)]

1 +

≤

r

2T
π

.

We are now in position to prove an upper-bound on the performance of

Ap∗.

Theorem 17 For every r, g
0 of a reﬂected symmetric random walk of length r starting at position g. Then V ∗[t, g] = 1
2
for every t, g

N, let the random variable Zr(g) be the number of passages through
t
−

N. In particular,

E[ZT

∈

−

1(g)]

∈

V ∗[0, 0] =

1
2

E[ZT

−

1(0)]

T
2π
≤ r
1(g)] for all t, g

−

Proof Let us show that V ∗[t, g] = (1/2)E[ZT
t. For t = T we have ZT
T
−
have (1/2)Z0(g) = 0 = V ∗[T
now t < T
case T > g > 0. By Theorem 15 and by the induction hypothesis, we have

1(g) = 0. Assume t = T
1, g]. If g = 0, we have (1/2)Z0 = 1/2 = V ∗[T

1. If g = T , then we have V ∗[t, g] = 0 = (1/2)ZT

by induction on
. If g > 0 we
}
1, 0]. Suppose
1(g). Now let us look at the

∈ {
1 and let g

0, . . . , T

0, . . . , T

t
−
−

∈ {

t
−

t
−

−

−

−

}

−

−

=

V ∗[t, g]
1
2
1
2
1
2

=

=

1
2

(cid:18)
P(ST

−
(cid:16)
+ P(ST

=

1
2

E[ZT

t(g)].

−

(V ∗[t + 1, g + 1] + V ∗[t + 1, g

1])

E[ZT

2(g + 1)] +

t
−

−

1
2

−
E[ZT

2 = ST

t
−

t
−

−

1 + 1)E[ZT

t
−

−
1)E[ZT

2 = ST

t
−

t
−

1 −

−

−

2(g

t
−

−

−

1)]

(cid:19)
2(g + 1)]

−

2(g

t
−

−

1)]

(cid:17)

Similarly, for the case when g = 0 we have

V ∗[t, 0] = V ∗[t + 1, 1] +

1
2

=

1
2

E[ZT

t
−

−

2(1) + 1] =

1
2

E[ZT

1(0)].

t
−

−

In particular, we have V ∗[0, 0] = (1/2)E[ZT (0)] and Lemma 16 gives us the desired numerical
bound.

19

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

A.5. Lower Bound on the Optimal Regret

T /(2π) + O(1).
In the previous section we showed that Cover’s algorithm suffers regret at most
In fact, by the deﬁnition of Vp (see (10)) and V ∗ we have that V ∗[0, 0] is the minimum regret
p
algorithms of the form
[0, 1] is some function, suffer in the
0, . . . , T
worst-case scenario. However, this does not tell us whether more general player strategies can do
better or not. The next theorem shows that any player strategy suffers, in the worst case, at least
O(1) regret. The proof of the theorem boils down to lower-bounding the expected

Ap, where p : [T ]

1
} →

T /(2π)

× {

−

regret of a random adversary that plays uniformly from
p

=

L

(0, 1)T, (1, 0)T
{

.
}

−

Theorem 18 Let
T such that
ℓ

∈ L

A

be a player strategy for a 2-experts game with T

N rounds. Then, there is

∈

Regret(T,

,

Bℓ)

A

≥ r

T
2π − r

1
2π −

1
5

T

˜ℓt}
Proof Let
{
chosen uniformly at random and let ˜
B
t. We shall show that

t=1 be i.i.d. random variables such that ˜ℓt is equal to a vector in

(0, 1)T, (1, 0)T
}
{
be the (randomized) oblivious adversary that plays ˜ℓt at round

=

L

E[Regret(T,

, ˜
)]
B

T
2π − r

1
2π −

1
5

,

A

≥ r
which implies the existence of a deterministic adversary as described in the statement. For each
on
0, . . . , T
t
round t, deﬁne xt :=
[2] is a lagging expert
A
on round t. It is worth to already note that (˜gt)T
t=0 is a reﬂected random walk of length T . By
Proposition 1 we have

, let the random variable ˜gt be the gap between experts due to the costs of ˜
B
}
1), and set pt := xt(it) where it ∈

(˜ℓ1, . . . , ˜ℓt
−

∈ {

(15)

E[Regret(T )] =

T

E

t=1
X

(cid:2)

[˜gt

−

1 > 0]pt ·

(˜gt −

˜gt

−

1)

+

(cid:3)

T

E

t=1
X

(cid:2)

T
1 = 0]˜ℓ
t xt

,

[˜gt

−

(cid:3)

where we recall that for any predicate P we have [P ] equals 1 if P is true, and equals 0 otherwise.
First, let us show that

E

[˜gt

1 > 0]pt ·

(˜gt −

˜gt

1)

= 0,

t

∀

∈

[T ].

0, . . . , T

−
(cid:2)
˜ℓ1, . . . , ˜ℓt], that is, Et is the conditional expectation
] := E[
, deﬁne Et[
For each t
}
·|
·
,
given the choices of the random adversary on rounds 1, . . . , t. Let t
˜gt
1 > 0
{
}
−
1 is independent of ˜ℓ1, . . . , ˜ℓt
. This
one can see that ˜gt −
1
}
{±
−
together with the fact that pt is a function of ˜ℓ1, . . . , ˜ℓt
−

[T ]. On the event
1 and is uniformly distributed on
1 implies

∈ {

˜gt

∈

−

−

(cid:3)

(16)

E

(cid:2)

[˜gt

−

1 > 0]pt ·

(˜gt −

1)

˜gt

−

= E

(cid:3)

= E

h

[˜gt

1

Et
−
(cid:2)
ptEt
1
−

−

(˜gt −
1 > 0]pt ·
˜gt
1 > 0](˜gt −
[˜gt
˜gt

1 > 0](˜gt −

−

−

1)

˜gt

1)

−

1)

−

(cid:3)i

(cid:3)i

(cid:3)i

h

ptE
= E
h
= E[pt ·

(cid:2)
[˜gt
−

0] = 0.
(cid:2)

20

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

This ends the proof of (16). Let us now show that

T

t=1
X

E

[˜gt

−

T
1 = 0]˜ℓ
t xt

=

1
2

E[ZT

1(0)].

−

(17)

(cid:2)
(cid:3)
[T ], since xt is a function of ˜ℓ1, . . . , ˜ℓt
−

1 and ˜ℓt is independent of ˜ℓ1, . . . , ˜ℓt
−

1, we

T
1 = 0]˜ℓ
t xt
T

T
1 = 0]˜ℓ
t xt

[˜gt

−

(cid:3)

= E
h
= E
h
= E
h
= E
h

1

1

−

[˜gt

[˜gt

Et
−
−
(cid:2)
1 = 0]Et
−
˜ℓt
1 = 0]E
1
(cid:2)
2

1 = 0]

[˜gt

[gt

−

−

(cid:16)

(cid:3)i
xt

˜ℓt
T
(cid:2)

(cid:3)
xt

(cid:3)

i
xt(1) +

i

1
2

xt(2)

=

(cid:17)i

1
2

P(˜gt
−

1 = 0).

For each t
have

∈

E

(cid:2)

Thus,

T

E

[˜gt

−

T
1 = 0]˜ℓ
t xt

=

1
2

Xt=1

(cid:2)

(cid:3)

T

Xt=1

P(˜gt
−

1 = 0) =

E

1
2

h

T

Xt=1

=

1
2

E[ZT

1(0)].

−

1

˜gt−1=0
}
{

i

This completes the proof of (17) and the desired numerical lower-bound is given by Lemma 16.

Appendix B. On the Passages Through Zero of a Symmetric Random Walk

In this section we shall prove Lemma 16, which bounds the expected number of passages through
0 of a symmetric random walk. First, we need a simple corollary of Stirling’s formula (which we
state here for convenience) to bound binomial terms.

Theorem 19 (Stirling’s Formula, Robbins, 1955) For any n
n
e

e1/(12n+1) < n! < √2πn

√2πn

n

Corollary 20 For any n

∈

(cid:17)
(cid:16)
N we have

∈
n
e

N we have
n

e1/(12n).

(cid:17)

(cid:16)

22n
√πn

2
15n

1

−

2n
n

≤

(cid:18)

≤

(cid:19)

22n
√πn

.

(cid:19)
N. For the upper-bound, we have

(cid:18)

Proof Let n

∈

2n
n

(cid:18)

(cid:19)

=

<

=

≤

2n!
(n!)2
√2πn
2πn
·
22n√2
√2πn ·
22n
√πn ·

(2n)2n
·
n2n
e−

e−

2n

e1/24n
·
e2/(12n+1)

·
2n

·
exp

·

1

24n −
1
24n

(cid:19)

(cid:18)

−

(cid:18)

exp

2
12n + 1
22n
√πn

.

≤

(cid:19)

21

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Similarly, for the lower-bound we have

2n
n

(cid:18)

(cid:19)

=

>

=

=

≥

2n!
(n!)2
√2πn

22n√2
√2πn ·
22n
√πn ·
22n
√πn

1

(cid:18)

(2n)2n
n2n

·
2πn

·
exp

(cid:18)

exp

(cid:18)

−
2
15n

−

2n

2n

e−
·
e−
·
1

24n −
4
30n

(cid:19)

e1/(24n+1)
·
e2/12n
·
1
6n

(cid:19)

,

(cid:19)

(Since e−

x

1

−

≥

x for x

0).

≥

We are now ready to prove Lemma 16, which we restate for convenience.

Lemma 21 Let the random variable ZT (0) be the number of passages through 0 of a reﬂected
symmetric random walk of length T . Then,

2T
π

+

3
5 ≤

r

E[ZT (0)]

1 +

≤

r

2T
π

.

Proof Let
that

T

St}
{

t=0 be a symmetric random walk and deﬁne Xi := Si −

1 for every i

Si

−

∈

[T ]. Note

P(
St|
|

Therefore,

= 0) = P(St = 0) = P(

Xi = 1 : i

[t]

=

Xi =

1 : i

[t]

)
}|

∈

−

|{

|{
0

=

t
t/2

t

2−

(

(cid:0)

(cid:1)

∈
}|
if t is odd,
if t is even.

T

E[ZT (0)] =

P(St = 0) =

T /2
⌋
⌊

P(S2k = 0) =

T /2
⌋
⌊

2k
k

1
22k .

Xk=0
Using Corollary 20, a consequence of Stirling’s approximation to the factorial function, we can
show upper- and lower-bounds to the above quantity. Namely, for the upper-bound we have

t=0
X

Xk=0 (cid:18)

(cid:19)

T /2
⌋
⌊

Xk=0 (cid:18)

2k
k

(cid:19)

1
22k ≤

1 +

T /2
⌋
⌊

Xk=1

22k
√πk

1
22k = 1 +

1
√π  

T /2
⌋
⌊

Xk=1

1
√k ! ≤

1 +

1
√π  Z

0

T /2

1
√x dx !

= 1 +

2
r

T /2

k
π !(cid:12)
k=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 1 +

2T
π

.

r

22

 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

We proceed similarly for the lower-bound. By setting β :=

∞k=1 k−

3/2 we get

T /2
⌋
⌊

Xk=0 (cid:18)

2k
k

(cid:19)

1
22k ≥

≤

=

T /2
⌋
⌊

1 +

Xk=1
1
√π  Z
2T
2
15
π −

1 +

r

22k
√πk (cid:18)

2
15k

1

−

(cid:19)

P
1
22k = 1 +

1
√π  

T /2
⌋
⌊

Xk=1

1
√k −

T /2
⌋
⌊

Xk=1

2
15k3/2 !

T /2

1

√x dx −

2
15

∞

3/2

k−

Xk=1

=

!

2
r

0

β + 1.

2
15

−

β + 1

T /2

k
π !(cid:12)
k=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

To conclude the proof, some simple calculations yield

β =

1
k3/2

∞

Xk=1

= 1 +

∞

1

k3/2 ≤

Xk=2

∞

1
x3/2

1 +

1
Z

dx = 1 +

= 3.

2
√x

−

(cid:18)

∞

(cid:19)(cid:12)
x=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Appendix C. Missing Proofs for Section 4

Lemma 22 For all t

∈

Proof Fix t

∈

[0, T ) and g

R

≥

∈

≥

∈
0. Then,

[0, T ) and g

R

0 we have ∗∆Q(t, g) = 0.

∂tQ(t, g) =

1
2

∂t erfc

g
2(T

=

1
√π

−

t) !

−
g2

2(T

t)

(cid:19)

−

(2(T

g

−

g2

2(T

t)

(cid:19)

−

∂t

g
2(T

t) !

−

p

−

exp

(cid:18)
t))3/2 .

=

1
√π

−

exp

p
−

(cid:18)

Similarly,

∂gQ(t, g) =

1
2

∂g erfc

g

2(T

=

1
√π

−

exp

g2

2(T

−

(cid:18)

t)

(cid:19)

−

∂g

1

t) !

−
g2

g

2(T

t) !

−

p

=

1
√π

−

exp

p
−

(cid:18)

and

2(T

t)

(cid:19)

−

2(T

t)

−

p

∂ggQ(t, g) =

1
2π(T

−

=

1
p
2π(T

p

as desired.

Let us now prove (6).

∂g exp

t)

−

exp

(cid:18)

t)

−

g2

(cid:18)

2(T
−
g2

t)

−

−

2(T

t)

T

(cid:19)

−

23

(cid:19)
g

−

=

t

−

2∂tQ(t, g),

 
 
 
 
 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Lemma 23 For all t

(
−∞

∈

, T ) and g

∈

R, we have

R(t, g) =

g
2

erfc

g

2(T

t

T

−
2π

exp

t) ! − r

−

Proof Fix (t, g)
×
2010, Section 7.7(i)), we have

(
−∞

, T )

∈

p
R. Using that

g
0 erfc(x) dx = g erfc(g)

 −

2(T

g2

−

−

+

t) !

r

T
2π

.

g2

1
√π e−

+ 1

√π (Olver et al.,

t)

−

t)

−

g

0

Z

Q(t, x) dx =

=

=

2(T
2

2(T
2

p

p

g
2

erfc

R
g/√2(T −t)

erfc(y) dy

0

Z

x erfc(x)

−

g
2(T

−

t) ! − r

x2

e−
√π !(cid:12)
x=0
(cid:12)
(cid:12)
t
(cid:12)
(cid:12)

exp

T

−
2π

x=g/√2(T −t)

g2

2(T

t) !

−

t

T

−
2π

+

r

 −

for g

≥

0. Similarly, using that d

p

dx erfc(x) =

2
√π e−

−

x2 we have

1
2

t

0

Z

∂gQ(s, 0) ds =

1
2

t

1

0  −
Z

2π(T

s) !

−

ds =

T

t
−
2π − r

T
2π

.

r

Plugging both equations into the deﬁnition of R concludes the proof.

p

Appendix D. Missing Proofs for Section 5

Let us begin by proving a crucial condition on the function q deﬁned on Section 5.

Lemma 24 We have q(t, 0) = 1/2 for all t

[T ].

∈

Proof The claim follows directly from the deﬁnition of q for t = T . Let t
the deﬁnition of q, we have

0, . . . , T

∈ {

. From
1
}

−

1

−

−

0

Z

Q(t, x) dx +

1
2

t

0

Z

∂gQ(s, 0) ds

(cid:19)

q(t, 0) = Rg(t, 0) =

1
2

(R(t, 1)

1

1

1

0

(cid:18)Z

0

(cid:18)Z

0
(cid:18)Z
1

=

=

=

=

1
2
1
2
1
2
1
2

Q(t, x) dx

Q(t, x) dx

−

−

−
1
2

0
Z
1
−

0
Z

1

R(t,

1))

−

t

∂gQ(s, 0) ds

Q(t, x) dx

(cid:19)

x) dx

−

(cid:19)

Q(t, x) dx +

Q(t,

0
Z

Q(t, x) + Q(t,

0

Z

(cid:0)

x)

dx.

−

(cid:1)

24

 
 
 
EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Moreover, note that for any z

erfc(

−

z) = 1

−

∈
2
√π

R we have

x2

e−

dx = 1 +

z

−

0
Z

2
√π

z

0
Z

x2

e−

dx = 2

erfc(z).

−

(18)

Therefore, Q(t,

x) = 1

−

−

Q(t, x) for any x

R and

∈

q(t, 0) =

1
2

1

0

Z

(cid:0)

Q(t, x) + Q(t,

x)

dx =

−

(cid:1)

1

1
2

0

Z

1 dx =

1
2

.

This section contains the proofs on the bounds on rt and rgg. We start by bounding rt.

Lemma 25 For any t

rt(t, g)

≤

(
−∞

∈
√2
8√π ·

(T

1
t)3/2

, T ) and g

∈

R we have

and

rgg(t, g)

2√2
3√π ·

≤

(T

1
t)3/2

.

∈

Proof Fix t
(
−∞
ﬁrst argument on [t
there is t′ ∈

−

(t

−

1, t) such that

, T ) and g

−
R. Note that R is continuously differentiable with respect to its
1, t). Thus, by Taylor’s Theorem,

−

1, t] and two times differentiable on (t

∈

−

R(t

−

1, g) = R(t, g) + (

−

1)∂tR(t, g) + (

−

1)2 ∂ttR(t′, g)
2

,

where ∂ttR(t′, g) denotes the second derivative of R with respect to its ﬁrst argument at (t′, g).
Therefore,

rt(t, g) = ∂tR(t, g)

(R(t, g)

R(t

−

−

1, g)) =

−

∂ttR(t′, g)
2

.

Thus, to bound rt(t, g) we need only to bound (1/2)∂ttR(t′, g). Computing the derivatives yields

∂tR(t′, g) =

1

2

2π(T

t′)

−

g2

−

2(T

t′)

−

(cid:17)

exp

(cid:16)

p

g2

−

(cid:16)

−

(cid:16)

t′)

2(T

−
g2

2(T

t′)

−

(cid:17)(cid:0)

(cid:17)(cid:0)

g2

−

(cid:1)

T

T

t′

t′

−

−

(cid:1)

and

∂ttR(t′, g) =

≤

≤

√2

8√π(T

−
√2

8√π(T

−
√2

t′)5/2 exp

t′)5/2 exp

t′)3/2

8√π(T

−
√2

≤

8√π(T

t)3/2

−

(Since T

t′ > 0),

−

(Since T

t′ > T

t).

−

−

To bound rgg, we will need to be slightly more careful. First, we will need the following simple
xex2.

lemma about Lipschitz continuity of x

R

∈

7→

25

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Lemma 26 Let K > 0 and deﬁne f (α) := αe−
continuous.

α2/K for every α

∈

R. Then f is 2-Lipschitz

Proof Let α
eβ

∈
1 + β for any β

≥

R we have

∈

R. First, note that f ′(α) = e−

α2/K (1

−

2α2/K). Therefore, using the fact that

f ′(α)
|
|

=

1

exp

α2
K

(cid:16)

(cid:17)

2α2
K (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
1 + α2
K

≤

(cid:12)
(cid:12)
(cid:12)

2α2
K (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

2

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

α2
K
−
1 + α2
K

2.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

We are now ready to bound rgg. Fix t

g R
(
−∞
the third partial derivative of R with respect to its third argument. By Taylor’s Theorem, there are
g′+ ∈

(g, g + 1) and g′

1, g) such that

, T ) and g

R. Moreover, denote by ∂(3)

− ∈

(g

−

∈

∈

R(t, g + 1) = R(t, g) + ∂gR(t, g) +

R(t, g

−

1) = R(t, g)

∂gR(t, g) +

−

1
2
1
2

∂ggR(t, g) +

∂ggR(t, g)

−

1
3!
1
3!

∂(3)
g R(t, g′+)

and

∂(3)
g R(t, g′
−

).

Therefore,

rgg(t, g) = ∂ggR(t, g)

(R(t, g + 1) + R(t, g

1)

−

−

2R(t, g)) =

−

1
3!

(∂(3)

g R(t, g′
−

)

−

∂(3)
g R(t, g′+)).

Let g′ ∈

R. To compute the partial derivatives, ﬁrst note that

∂gR(t, g′) = Q(t, g′) =

Thus, one may check that

1
2

erfc(g′/

2(T

t)).

−

p

(g′)2

2(T

−

(cid:16)

t)

−

1
2(T

(cid:17)
(g′)2
p

t)

−

exp

(cid:16)

t)

−

−

2(T

t)

(cid:17)

−

∂ggR(t, g′) =

1
2

2
√π

−

exp

=

1

−

2π(T

and

p

∂(3)
g R(t, g′) =

1

2π(T

t)
−
g′

exp

−

(cid:16)
exp

(g′)2

2(T

t)
−
(cid:17)
(g′)2

2g′

2(T

t)

−

.

(T

t)3/2

−

2(T

t)

=

1
p
√2π

By Lemma 26, we know that ∂(3)
t)−

3/2. Therefore,

g R(t,

−

−
) is Lipschtiz continuous with Lipschitz constant 2(2π)−
·

(cid:17)

(cid:16)

rgg(t, g) =

1
3!

(∂(3)

g R(t, g′
−

)

−

∂(3)
g R(t, g′+))

2
3√2π(T

≤

−

g′
− −

t)3/2 |

g′+| ≤

2√2

3√π(T

−

.

t)3/2

26

(19)

1/2(T

−

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

Appendix E. Extending the Regret Analysis for General Costs

+1,
{

In Section 5, we relied on the fact the gap values were in
. This assumption was fun-
1
}
damental for the version of the discrete Itô’s Formula that we have uses (see the assumption on
g0, . . . , gT in the statement of Theorem 9). It was also required by Proposition 1 to connect the
regret with the “discrete stochastic integral”. To extend the upper-bound on the regret of the algo-
rithm from Section 5 to general costs we will follow the same techniques used by Harvey et al. to
extend the guarantees of their algorithm to general costs: we shall use a more general version of
the discrete Itô’s formula, concavity of R with respect to its second argument, and a lemma relating
the per-round regret to terms that appear in the more general version of the discrete Itô’s formula
(see Harvey et al., 2020b, Section 3.3 for details on these arguments).

−

As in the work of Harvey et al., we will rely on a more general version of the discrete Itô’s
formula that holds for general [0, 1] costs. The main issue with this general formula is that more
work is needed to relate it to the regret of our player strategy.

Theorem 27 (General Discrete Itô’s Formula, Harvey et al., 2020a, Lemma 3.13) Let f : R2
R be a function and let g0, g1, . . . , gT ∈

R. Then,

→

T

f (0, g0) =

f (t, gt)

f (T, gT )

−

f (t, gt

−

1 + 1) + f (t, gt
2

−

1 −

1)

−

(cid:17)

Xt=1(cid:16)

T

+

t=1
X
(cid:0)

1
2 fgg(t, gt
−

1) + ft(t, gt

−

1)

.

(cid:1)

Fix T

N, ﬁx gaps g

∈
be regarding a game of

RT , and set g0 := 0. For the remainder of this section all results will
0.

Aq against an oblivious adversary with gap sequence g0, g1, . . . , gT ∈

R

∈

≥

For every t

0, . . . , T

∈ {

, deﬁne the per-round regret (at round t) by
}
∆Regret(t) := Regret(t)

Regret(t

1).

−
Our goal in this section is to prove the following lemma.

−

Lemma 28 For every t

∆Regret(t)

∈

≤

[T ] we have

R(t, gt)

R(t, gt

−

1 + 1) + R(t, gt
2

−

1 −

1)

,

−

t

∀

∈

[T ].

(20)

Combining the above lemma with Theorem 27 and the fact that R satisﬁes (BHE) yields

T

Regret(T ) =

∆Regret(t)

Xt=1

R(T

1, gT

−

−

≤

1) + ∆Regret(T ) +

T

−

1
( 1
2 rgg(t, gt
−

Xt=1

1) + rt(t, gt

−

1)).

Since q(T, g) = [g = 0](1/2) for any g
1/2. At this
0, . . . , T
point, the exact same proof of Theorem 13 applies and we obtain the same regret bound. Thus, it
only remains to prove Lemma 28. In order to prove Lemma 28, we will use the following result
from Harvey et al. (2020a).

, we have ∆Regret(T )
1
}

∈ {

−

≤

Proposition 29 (Harvey et al., 2020a, Lemma 3.14) Let gt
rounds t
round t by the player (with q(t, 0) = 1/2). For all t

1 and t, respectively, and let q(t, gt

−

1,

−

1 and gt be the values of the gap on
1) be the probability mass put in the worst expert at

−

≥

27

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

1. If a best expert at time t

−

1 remains a best expert at time t, then,

2. If a best expert at time t

∆Regret(t) = q(t, gt

1)(gt −

gt

1).

−
1 remains a best expert at time t, then gt + gt
−
∆Regret(t) = gt −

1)(gt + gt

q(t, gt

1).

−

−

−

1 and

1 ≤

−

We shall also make use of the following fact about concave function.

Lemma 30 Let f : R
.
f (α), f (β)
min
}
{

R be a concave function and let α < β be real numbers. Then f (x)

≥

→

Proof [Proof of Lemma 28] To prove (20), we will consider each one of the cases from Proposi-
tion 29 separately.

Case 1.

In this case, (20) is equivalent to

0

q(t, gt

−

1)(gt −

gt

−

≤ −

1) + R(t, gt)

R(t, gt

−

1 + 1) + R(t, gt
2

−

1 −

1)

.

−

(21)

Since the ﬁrst term in the right-hand side of the above inequality is linear in gt and since R(t,
)
·
is concave (by (19) we know that ∂ggR(t,
) is negative everywhere), we conclude that the whole
·
right-hand side is concave as a function of gt. Thus, by Fact 30 it sufﬁces to prove the above
1 + 1]. But for
inequality for gt ∈ {
gt
1 −
−
1).
1 + 1
gt
gt ∈ {
1 −
}
Case 2.
In this case, (20) is equivalent to

1, gt
1 −
the right-hand side of (20) becomes exactly q(t, gt
−

to prove that it holds for gt ∈

1, gt
1)(gt −

1 + 1
}

1, gt

[gt

gt

−

−

−

−

−

−

0

≤ −

gt + q(t, gt

−

1)(gt + gt

−

1) + R(t, gt)

R(t, gt

−

1 + 1) + R(t, gt
2

−

1 −

1)

.

−

(22)

Again, the right-hand side of the above inequality is concave as a function of gt. Since gt ≥
gt + gt
gt ∈ {

0 and
1]. Thus, it sufﬁces to prove the above inequality for

1, we know that gt ∈
gt

[0, 1
. For gt = 0 we have

1 ≤
−

−
0, 1

gt

−

−

−

1}
gt + q(t, gt

1)(gt + gt

−

−

1) + R(t, gt)

q(t, gt

−

1)(gt −

gt

−

1) + R(t, gt)

−

−
R(t, gt

R(t, gt

−

1 + 1) + R(t, gt
2

1)

1 −

−

−

1 + 1) + R(t, gt
2

−

1 −

1)

,

and in the previous case we showed that the above is non-negative for all gt ∈
Since gt
gt = 1

1 + 1].
1 in this case, we have in particular that the above holds for gt = 0. Suppose now that
1. Since q(t, gt

1), we have that (22) is equivalent to

1) = Rg(t, gt

1 −

1, gt

[gt

−

−

−

−

=

1 ≤
gt
−

−
−

−

−

0

≤ −

gt + q(t, gt

−

1)(gt + gt

−

1) + R(t, gt)

−

R(t, gt

−

1 + 1) + R(t, gt
2
1 + 1) + R(t, gt
2

1 −

−

1 −

−

1)

1)

R(t, gt

−

1)

+ R(t, 1

1)

gt

−

−

= gt

−

1 −

1 + q(t, gt

1) + R(t, 1

= gt

−

1 −

1 +

−
R(t, gt

−

1 + 1)

−
2

gt

1)

−
−
R(t, gt

−
1 −

−

1)

1 −

−

R(t, gt

−

−
1 + R(t, 1

1 + 1) + R(t, gt
2
1)

gt

−

−

−

R(t, gt

1).

1 −

−

= gt

−

1 −

28

EFFICIENT AND OPTIMAL FIXED-TIME REGRET WITH TWO EXPERTS

By the deﬁnition of R and since erfc(

−

z) = 2

−

erfc(z) for all z

∈

R (see (18)), we have

−

−
1

1

gt
−
2(T

1
t) ! −

1

gt

−

1 −
2

erfc

gt
−
2(T

1 −
−

1
t) !

gt
−
2(T

1
t) !

−

−

+ erfc

gt
−
2(T

p
1 −
−

1
t) !!

p
gt

1.

p

−

−

R(t, 1

1)

gt

−

−

−

R(t, gt

−

1 −

1) =

1

−

1

gt
2

−

erfc

1

1

1

1

−

gt
2

−

−

gt
2

−

=

=

(cid:18)

(cid:18)

p
erfc

(cid:19) 

2 = 1

(cid:19)

This concludes the proof of (22).

29

 
 
 
 
