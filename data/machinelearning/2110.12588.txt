QuantifyML: How Good is my Machine Learning Model?

Muhammad Usman
University of Texas at Austin, USA

Divya Gopinath
KBR Inc., CMU, Nasa Ames

Corina S. P˘as˘areanu
KBR Inc., CMU, Nasa Ames

muhammadusman@utexas.edu

divya.gopinath@nasa.gov

corina.s.pasareanu@nasa.gov

The efﬁcacy of machine learning models is typically determined by computing their accuracy on test
data sets. However, this may often be misleading, since the test data may not be representative of
the problem that is being studied. With QuantifyML we aim to precisely quantify the extent to which
machine learning models have learned and generalized from the given data. Given a trained model,
QuantifyML translates it into a C program and feeds it to the CBMC model checker to produce
a formula in Conjunctive Normal Form (CNF). The formula is analyzed with off-the-shelf model
counters to obtain precise counts with respect to different model behavior. QuantifyML enables i)
evaluating learnability by comparing the counts for the outputs to ground truth, expressed as logical
predicates, ii) comparing the performance of models built with different machine learning algorithms
(decision-trees vs. neural networks), and iii) quantifying the safety and robustness of models.

1 Introduction

Recent years have seen a surge in the use of machine learning algorithms in a variety of applications to
analyze and learn from large amounts of data. For instance, decision-trees are a popular class of super-
vised learning that can learn easily-interpretable rules from data. They have found success in areas such
as medical diagnosis and credit scoring [21, 4]. Deep Neural Networks (DNN) also have gained popu-
larity in diverse ﬁelds such as banking, health-care, image and speech recognition, as well as perception
in self-driving cars [14, 19].

Such machine learning models are typically evaluated by computing their accuracy on held-out test
data sets, to determine how well the model learned and generalized from the training data. However,
this is an imperfect measure, as they may not cover well the desired input space. Furthermore, it is
often not clear which learning algorithm or trained model is better suited for a particular problem (e.g.,
neural networks vs. decision trees), and simply comparing the accuracy of different models may lead to
misleading results. It may also be the case that well-trained models may be vulnerable to adversarial
attacks [27, 24, 15] or they may violate desired safety properties [23]. It is unclear how to quantify the
extent to which these vulnerabilities affect the performance of a model, as evaluating the model on the
available test or adversarial data sets may again give imprecise results.

We present QuantifyML, an analysis tool that aims to precisely quantify the learnability, safety and
robustness of machine learning models. In this tool, a given trained model is translated into a C program,
enabling the application of the CBMC tool [20] to obtain a formula in Conjunctive Normal Form (CNF),
which in turn can be analyzed with approximate and exact model counters [11, 6, 10] to obtain precise
counts of the inputs that lead to different outputs. Figure 1 gives a high-level description of QuantifyML.
We demonstrate QuantifyML in the context of decision trees and neural networks for the problems of
learning relational properties of graphs, image classiﬁcation and aircraft collision avoidance.

We derive inspiration from a recent paper [29] which presents Model Counting meets Machine Learn-
ing (MCML) to evaluate the learnability of binary decision trees. With QuantifyML we generalize MCML
by providing a more general tool that can handle more realistic multi-class problems, such as decision

M. Farrell & M. Luckcuck (Eds.):
Third Workshop on Formal Methods
for Autonomous Systems (FMAS2021)
EPTCS 348, 2021, pp. 92–100, doi:10.4204/EPTCS.348.6

© Muhammad Usman, Divya Gopinath and Corina S. P˘as˘areanu
This work is licensed under the
Creative Commons Attribution License.

Muhammad Usman, Divya Gopinath and Corina S. P˘as˘areanu

93

Figure 1: QuantifyML Framework

trees with non-binary inputs and with more than two output decisions, and also neural networks. Other
learning algorithms can be accommodated provided that the learned models are translated into C pro-
grams. QuantifyML’s applications extend beyond MCML and include: (i) comparison of the perfor-
mance of different models, built with different learning algorithms, (ii) quantiﬁcation of robustness in
image classiﬁers, and (iii) quantiﬁcation of safety of neural network models.

2 Background

Decision Trees: Decision tree learning [26] is a supervised learning technique for extracting rules that
act as classiﬁers. Given a set of data labeled to respective classes, decision tree learning aims to discover
rules in terms of the attributes of the data to discriminate one label from the other. It builds a tree such
that each path of the tree encodes a rule as a conjunction of predicates on the data attributes. Each rule
attempts to cluster or group inputs that belong to a certain label.
Neural Networks: Neural networks [12] are machine learning algorithms that can be trained to perform
different tasks such as classiﬁcation and regression. Neural networks consist of multiple layers, starting
from the input layer, followed by one or more hidden layers (such as convolutional, dense, activation,
and pooling), and a ﬁnal decision layer. Each layer consists of a number of computational units, called
neurons. Each neuron applies an activation function on a weighted sum of its inputs; N(X) = σ (∑i wi ·
Ni(X) + b) where Ni denotes the value of the ith neuron in the previous layer of the network and the
coefﬁcients wi and the constant b are referred to as weights and bias, respectively; σ represents the
activation function. The ﬁnal decision layer (also known as logits) typically uses a specialized function
(e.g., max or softmax) to determine the decision or the output of the network.
Bounded Model Checking for C programs: Bounded model checking [5] is a popular technique for
verifying safety properties of software systems. Given a bound on the input domain and a bound on the
length of executions, a boolean formula is generated that is satisﬁable if there exists an error trace or
counter-example to the given property. The formula is checked using off-the-shelf decision procedures.
CBMC [8] is a tool that performs analysis of programs written in a high-level language such as C, C++
or Java by applying bounded model checking. The program is ﬁrst converted into a control ﬂow graph
(CFG) representation and formulas are built for the paths in the CFG leading to assertions. The model
checking problem is reduced to determining the validity of a set of bit-vector equations, which are then
ﬂatted out to conjunctive normal form (CNF) and checked for satisﬁability. In this work, we leverage
CBMC to build the CNF formulas corresponding to the paths in the C program representation of a
machine learning model. We then pass on the formulas corresponding to the respective output classes to
a model counting tool in order to quantify the number of solutions.

94

QuantifyML: How Good is my Machine Learning Model?

Projected model counting: Many tools, including CBMC, translate a boolean formula to CNF by intro-
ducing auxiliary variables. These variables do not affect the satisﬁability of the boolean formula but do
affect the model counts. In such scenarios, projected model counting [3] needs to be used. Consider the
set M consisting of all variables in a boolean formula and N be a subset of variables in the formula. The
solutions in which the value of at least one variable in N is different is considered a unique solution in
the projected model counting problem. The variables in N are known as primary variables and the rest
of the variables are known as auxiliary variables. Please refer to [28] for a detailed discussion on model
counting, projected model counting and model counters. In our work we use projected model counting,
where the inputs to the model are considered as primary variables. We used two state-of-the-art model
counters i.e., projMC [22] and ApproxMC [7].
MCML: MCML [29] uses model counting to perform a quantitative assessment of the performance of
decision-tree classiﬁer models. The ground truth (φ ) is translated by the Alloy analyzer with respect to
bound b into a CNF formula cn fφ . It then translates the relevant parts of decision tree with respect to
the desired metrics (True Positives, False Positives, False Negatives, True Negatives) into a CNF for-
mula cn fd. It then combines these two formulas to create the CNF formula cn fφ ,d which is an input to
the model counter that outputs the number of solutions that satisfy the formula. This count quantiﬁes
the true performance of the decision tree. MCML is limited to binary decision trees and has been used
on decision-tree models when used to learn relational properties of graphs. QuantifyML goes beyond
MCML as it enables quantiﬁcation of the performance of more general machine learning models, that
may have non-binary inputs and multi-class outputs. Our evaluation presents applications such as ro-
bustness analysis of decision-tree models on an image-classiﬁcation problem (MNIST), comparison of
neural network and decision-tree models for learning relational properties of graphs, and evaluation of
safety for collision avoidance, which cannot be achieved with the MCML tool.

3 Approach

Quantifying the learnability of machine learning models: QuantifyML can be used to quantify the
learnability of models, provided that a predicate is given which describes the ground-truth output for
any input and ﬁnite bounds on the input space. Consider a model classifying a given input into one
of L labels. For each output label l, two predicate functions are generated; φl(x) which returns 1 if
the output of the model is l for a given input x and returns 0 otherwise, and ψl(x) which returns 1
if the ground-truth for the given input x is l and returns 0 otherwise. These predicates are used to
encode the following metrics for each label l; True Positives (TP): MC( CNF( ψl(x) ∧ φl(x) ), N),
False Positives (FP): MC( CNF( ¬ψl(x) ∧ φl(x) ), N), True Negatives (TN): MC( CNF( ¬ψl(x) ∧
¬φl(x) ), N), and False Negatives (FN): MC(CNF( ψl(x) ∧ ¬φl(x) ), N). N is the scope or bound on the
input domain, CNF represents a function that translates C program to formulas in the CNF form, and MC
represents a function that uses the projected model counter to return the number of solutions projected to
the input variables. QuantifyML then uses these counts to assess the quality of the model using standard
T P+T N
measures such as Accuracy, Precision, Recall and F1-score for the model. Accuracy=
T P+FP+T N+FN ,
Precision= T P
Quantifying the safety of machine learning models: QuantifyML can also be used to quantify the
extent to which input-output safety properties are satisﬁed for a model. Assume a property p of the form
(Pre => Post) where Pre is a condition on the input variables and Post is a condition on the output of
the model, such as a classiﬁer producing a certain label. We can use QuantifyML to obtain the following
counts: i) QuantifyMLS denoting the portion of the inputs for which the model satisﬁes the given property,

T P+FN and F1-score= 2∗Precision∗Recall
Precision+Recall .

T P+FP , Recall= T P

Muhammad Usman, Divya Gopinath and Corina S. P˘as˘areanu

95

QuantifyMLS
QuantifyMLN +QuantifyMLS

and ii) QuantifyMLN denoting the portion of the inputs for which the model violates the property. These
counts are then used to obtain an accuracy metric; QuantifyMLAcc=
, which is a
measure of the extent to which the network satisﬁes the property.
Quantifying Local Robustness: The challenge with the analysis of more realistic models is that we
typically do not have the ground truth. Image classiﬁcation is such a problem, where it is not feasible
to deﬁne a speciﬁcation that can automatically generate the ground-truth label for any arbitrary image.
However, images that are similar to, or are in close proximity (in terms of distance in the input space) to
an image with a known label can be expected to have the same label. This property is called robustness
in the literature. Current techniques [13, 2, 9] typically search for the existence of an adversarial input
(x(cid:48)) within an ε ball surrounding a labelled input (x); e.g., ||x − x(cid:48)||∞ ≤ ε (here the distance is in terms of
the L∞ metric) such that the output of the model on x and x(cid:48) is different. When no such input exists, the
model is declared robust, however, in the presence of an adversarial input there is no further information
available. QuantifyML can be used to quantify robustness of machine learning models, where instead
of using a predicate encoding the ground truth, we encode the local robustness requirement that the
model should give the same output within the region deﬁned by ||x − x(cid:48)||∞ ≤ ε. In order to quantify local
robustness around a concrete n-dimensional input x = (x0, x1, ..xn), we ﬁrst deﬁne an input region Rε by
constraining the inputs across each dimension to be within [xi − ε, xi + ε] in the translated C program. We
then deﬁne Robustnessε as MC(CNF(φl (x)),Rε )
, where φl(x) is deﬁned as before as a predicate which returns
1 if the output of the model is l and 0 otherwise, Rε deﬁnes the scope for the check, and |Rε | quantiﬁes
its size. Intuitively, Robustnessε quantiﬁes the portion of the input on which the model is robust, within
the small region described by Rε .

|Rε |

Please check longer version of this paper [1] for more details on the approach. The tool currently

supports decision trees trained using Scikit-Learn [25] and neural networks trained in Keras [18].

4 Evaluation

We present experiments we have performed to evaluate the beneﬁts of using QuantifyML in the applica-
tions of quantifying learnability, safety and robustness of machine learning models.
Quantifying the learnability of machine learning models: This study aims to assess QuantifyML in
quantifying the true performance of models and enabling one to compare different models, and different
learning algorithms, for a given problem.

We evaluated the performance of trained models against ground truth predicates on the problem of
learning relational properties of graphs. We considered 11 relational properties of graphs including An-
tisymmetric, Connex, Equivalence, Irreﬂexive, NonStrictOrder, PartialOrder, PreOrder, Reﬂexive, Stric-
tOrder, TotalOrder and Transitive (refer [1]). We used the Alloy tool [16] to create datasets containing
positive and negatives solutions for each of these properties. Each input in the dataset corresponds to a
graph with a ﬁnite number of nodes and is represented as an adjacency matrix. Each input has a corre-
sponding binary label (1 if the graph satisﬁes the respective property, 0 otherwise). Please refer [1] for
more details on the setup. The problem of learning relational properties of graphs albeit seems fairly
simple with binary decisions and binary input features, it is not immediately apparent which learning
algorithm would work best to learn a suitable classiﬁer. We applied two different learning algorithms,
decision-trees and neural networks, to learn classiﬁcation models for the same set of properties using
the same dataset for training. We were unable to apply QuantifyML to analyze neural network models
with greater than 16 features due to the limitation in the scalability of the model counters. Therefore we
restricted the size of the graphs to have 4 nodes.

96

QuantifyML: How Good is my Machine Learning Model?

Table 1: Quantifying the learnability of Decision Trees on graph (4-node) properties with projMC. Diff
shows the difference between Statistical (Stat) and QuantifyML (QML) metrics.

Property

Antisymmetric
Connex
Irreﬂexive
NonStrictOrder
PartialOrder
PreOrder
Reﬂexive
StrictOrder
Transitive

Accuracy
QML
1.0000
0.8179
1.0000
0.9721
0.9919
0.9693
1.0000
0.9721
0.9799

Diff
0.0000
-0.1752
0.0000
-0.0279
-0.0038
-0.0307
0.0000
0.0175
-0.0051

Stat
1.0000
0.9932
1.0000
1.0000
0.9957
1.0000
1.0000
0.9545
0.9850

Precision
QML
1.0000
0.4219
1.0000
0.1069
0.8690
0.1499
1.0000
0.1069
0.7524

Diff
0.0000
-0.5646
0.0000
-0.8931
-0.1226
-0.8501
0.0000
-0.8131
-0.2285

Stat
1.0000
0.9865
1.0000
1.0000
0.9916
1.0000
1.0000
0.9200
0.9810

Stat
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9904

Recall
QML
1.0000
0.0625
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9990

Diff
0.0000
-0.9375
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0086

F1-score
QML
1.0000
0.1089
1.0000
0.1932
0.9299
0.2607
1.0000
0.1932
0.8583

Diff
0.0000
-0.8843
0.0000
-0.8068
-0.0659
-0.7393
0.0000
-0.7651
-0.1273

Stat
1.0000
0.9932
1.0000
1.0000
0.9958
1.0000
1.0000
0.9583
0.9856

Table 2: Quantifying the learnability of Neural Networks on graph (4-node) properties with projMC. Diff
shows the difference between Statistical (Stat) and QuantifyML (QML) metrics.

Property

Antisymmetric
Connex
Irreﬂexive
NonStrictOrder
PartialOrder
PreOrder
Reﬂexive
StrictOrder
Transitive

Accuracy
QML
0.7614
0.7866
1.0000
0.9054
0.8303
0.8825
1.0000
0.9409
0.7903

Diff
-0.0445
-0.1791
0.0000
-0.0719
0.0500
-0.0753
0.0000
-0.0136
0.0181

Stat
0.8058
0.9658
1.0000
0.9773
0.7803
0.9577
1.0000
0.9545
0.7722

Precision
QML
0.4211
0.2326
1.0000
0.0338
0.2002
0.0433
1.0000
0.0535
0.1864

Diff
-0.3309
-0.7033
0.0000
-0.9245
-0.6364
-0.8870
0.0000
-0.8665
-0.6198

Stat
0.7520
0.9359
1.0000
0.9583
0.8367
0.9302
1.0000
0.9200
0.8063

Stat
0.9095
1.0000
1.0000
1.0000
0.7051
1.0000
1.0000
1.0000
0.7404

Recall
QML
0.9093
0.0865
1.0000
0.9909
0.7260
0.9803
1.0000
1.0000
0.7258

Diff
-0.0002
-0.9135
0.0000
-0.0091
0.0210
-0.0197
0.0000
0.0000
-0.0145

F1-score
QML
0.5756
0.1261
1.0000
0.0654
0.3139
0.0829
1.0000
0.1016
0.2967

Diff
-0.2476
-0.8408
0.0000
-0.9133
-0.4514
-0.8810
0.0000
-0.8567
-0.4753

Stat
0.8233
0.9669
1.0000
0.9787
0.7652
0.9639
1.0000
0.9583
0.7719

Tables 1 and 2 presents the results. We can observe the beneﬁt of QuantifyML over pure statisti-
cal results (Stat) for both decision-tree and neural-network models. The decision-tree models for the
Antisymmetric, Irreﬂexive, Reﬂexive, NonStrictOrder and PreOrder properties have accuracy and F1-
scores of 100%. However, the counts computed by QuantifyML highlight that for the NonStrictOrder
and PreOrder properties, the models in fact have less than 100% accuracy and more importantly have
poor precision indicating large number of false positives. The decision trees for StrictOrder seem to have
the lowest accuracy and F1-score when calculated statistically. However, the QuantifyML scores indi-
cate that this is mis-leading and the decision-tree for the Connex property has the lowest accuracy and
F1-score. For the neural networks, in all cases except Irreﬂexive and Reﬂexive properties, the accuracies
calculated using QuantifyML highlight that the true performance is mostly worse and in some cases bet-
ter (PartialOrder, Transitive) than the respective statistical accuracy metric values. The statistical results
give a false impression of good generalizability of the respective models, while in truth the F1-scores are
less than 50% for most of the properties (refer F1-score column in table 2).

The models for the Irreﬂexive and Reﬂexive properties have 100% accuracy and F1-score. These
are very simple graph properties. However, decision-tree models have the ability to learn more com-
plex properties such as Antisymmetric and StrictOrder as well. Overall the decision-tree models seem
to have better accuracy and generalizability than the respective neural network models. Note, that while
such a comparison can be done using the statistical metrics, their lack of precision may lead to wrong
interpretations. For instance, for the StrictOrder property, the statistical accuracy, precision, recall and
F1-scores are exactly the same for the neural network and decision-tree models, however, the corre-
sponding QuantifyML metrics highlight that for this problem, the decision-tree model is in fact better
than neural network in terms of the true performance.
Quantifying adversarial robustness for image classiﬁcation models: We trained a decision-tree clas-
siﬁer on the popular MNIST benchmark, which is a collection of handwritten digits classiﬁed to one

Muhammad Usman, Divya Gopinath and Corina S. P˘as˘areanu

97

Table 3: Quantifying robustness for the MNIST model.

Actual
Label
0
1
2
3
4
5
6
7
8
9

Total
countε
3.32 × 10270
9.59 × 10247
3.53 × 10258
1.92 × 10272
3.42 × 10264
4.02 × 10259
1.04 × 10258
1.17 × 10262
9.99 × 10266
1.84 × 10253

Correctly
classiﬁed countε
2.21 × 10270
3.15 × 10247
8.81 × 10257
1.92 × 10272
3.42 × 10264
4.02 × 10259
5.22 × 10257
1.17 × 10262
9.99 × 10266
6.89 × 10252

Robustnessε
%
66.67
32.81
25.00
99.99
99.99
99.99
50.00
99.99
99.99
37.50

Accuracyε Accuracyε Accuracyε
1000
67.90
32.10
25.90
93.70
61.50
100.00
47.40
100.00
100.00
38.20

100
65.00
34.00
32.00
93.00
52.00
100.00
47.40
100.00
100.00
38.20

10000
67.05
32.66
25.30
93.62
63.58
100.00
50.21
100.00
100.00
38.12

Accuracy
(TestSet)
92.65
96.12
83.91
77.52
82.08
76.23
85.39
85.60
73.72
80.67

of 10 labels (0 through 9). The overall accuracy of this model on the test set was 83.64%. We selected
(randomly) an image for each of the 10 labels and considered regions around these inputs for ε = 1; these
represent all the inputs that can be generated by altering each pixel of the given image by +/- 1. Table 3
presents the results. Column Total countε shows the number of images in the ε = 1 neighborhood of each
input. We then employ QuantifyML to quantify the number of inputs within the ε = 1 neighborhood that
are given the correct label (Column Correctly classiﬁed countε ). The corresponding Robustnessε value
shows the accuracy with which the model classiﬁes the inputs in the region to the same label. The results
indicate that the robustness of the model is poor or the model is more vulnerable to attacks around the
inputs corresponding to labels 1, 2 and 9 respectively.

We also computed an accuracy metric statistically by perturbing each image within ε to randomly
generate sample sets of size 100, 1000 and 10000 images respectively. We then executed the model
on each set to determine the corresponding labels and computed the respective accuracies as shown
in column Accuracyε (size). The statistically computed accuracies are close to the Robustnessε values
for most of the labels. However, for labels 5,7 and 8, they are 100% respectively which gives a false
impression of adversarial robustness around these inputs. The corresponding Robustnessε of 99.99%
indicates that there are subtle adversarial inputs which get missed when the robustness is determined
statistically. The last column, Accuracy (TestSet), shows the accuracy of the model per label when
evaluated statistically on the whole MNIST test set. We can observe that although the model may have
high statistical accuracy, it can have low adversarial robustness.
Quantifying the safety of machine learning classiﬁcation models: ACAS Xu is a safety-critical col-
lision avoidance system for unmanned aircraft control [23]. It receives sensor information regarding the
drone (the ownship) and any nearby intruder drones, and then issues horizontal turning advisories (one
of the ﬁve labels; Clear-of-Conﬂict (COC), weak right, strong right, weak left, and strong left) aimed at
preventing collisions. Previous work [17] presents 10 input-output properties that the networks need to
satisfy. We used a data-set comprising of 324193 inputs and used one of the original ACAS Xu networks
to obtain the labels for them. We used this dataset to train a smaller neural network with 4 layers that
is amenable to a quantitative analysis. The overall accuracy of this model on the test set was 96.0%.
We selected 9 properties of ACAS Xu (see [1] for details on the properties) and employed our tool to
evaluate the extent to which the smaller neural network model complies to each of them.

Table 4 documents the results. We ﬁrst evaluated each property statistically on a test set of size
162096 inputs (randomly selected). Column StatN shows the subset of inputs in InpSetP# that violate the
property, StatS shows the number of inputs in InpSetP# that satisﬁes it, and StatAcc shows the respective
statistical accuracy. For each property, we calculate the QuantifyML metrics as described in section 3.

98

QuantifyML: How Good is my Machine Learning Model?

Table 4: Quantifying the safety of Neural Networks on ACAS Xu dataset. “-” shows a timeout of 5000
seconds (ApproxMC). Properties 1 - 9 represent properties φ2 to φ10 from [17].

StatAcc(%) QuantifyMLN QuantifyMLS QuantifyMLAcc(%) QuantifyMLTime (s)

Property
1
2
3
4
5
6
7
8
9

StatN
0
0
0
0
1
5680
0
0
0

StatS
228
0
0
0
4062
140563
218
1
62

100.00
N/A
N/A
N/A
99.98
96.12
100.00
100.00
100.00

9.00 × 1093
5.67 × 1088
3.37 × 1067
1.18 × 1074
0
-
0
4.24 × 1073
0

2.50 × 1094
2.79 × 1088
1.32 × 1065
0
2.25 × 1086
6.67 × 1094
8.15 × 1090
8.62 × 1074
4.17 × 1079

73.56
32.94
0.39
0.00
100.00
-
100.00
95.31
100.00

3347.1
4067.5
2791.8
2918.4
1005.2
-
1753.5
2073.3
812.2

The QuantifyML counts represent the portion of the input space deﬁned by the property for which the
property is satisﬁed or violated. For properties 2, 3 and 4, there were no inputs in the test set that belonged
to the input region as deﬁned in the property, therefore the statistical accuracy could not be calculated,
whereas we were able to use QuantifyML to evaluate the model on these properties. Results show that
the neural network never satisﬁes property 4. This highlights the beneﬁt of using our technique to obtain
precise counts without being dependent on a set of inputs.
On-Going work and challenges: MCML [29] is a tool that shares the same goal as QuantifyML of
quantiﬁcation of learnability but has a dedicated implementation to decision-trees. We performed a com-
parison of the two tools for decision-tree models used for learning the relational graph properties. Please
refer [1] for results. We observed that the results from the two tools matched exactly for all properties,
however, QuantifyML is less efﬁcient than MCML. With projMC as the model counter, QuantifyML takes
more time for each property and times out (after 5000 secs) for three additional properties as compared
to MCML. This is because the CNF formulas generated by the CBMC tool after the analysis of the C
program representation of the machine learning model is larger than that produced by MCML, which
has a custom implementation for decision-trees. We alleviated this issue by using the ApproxMC model
counter, which is faster but produces approximate counts. The analysis times for QuantifyML are greatly
reduced and we are able to obtain results for all the properties.

The analysis of neural network models was particularly challenging. The model counters (both
exact and approximate) timed out while analyzing the networks for the graph problem with more than 4
nodes. For image classiﬁcation, QuantifyML could not handle neural network models, while we could
only handle a small model for ACAS Xu. For the MNIST network, we attempted to reduce the state
space of the model by changing the representation of weights and biases (e.g., from ﬂoats to longs). We
also attempted partial evaluation by making a portion of the image pixels concrete or ﬁxed to certain
values and propagating these values to simplify computations in C program representation of the neural
network. Making 10% of the pixels concrete, led to a 51.37% decrease in the number of variables and a
53.08% decrease in number of clauses. However, the model counters could still not process the resulting
formula in reasonable amount of time. To address the scalability problem we plan to investigate slicing
and/or compositional analysis of the C program representation of the models.

5 Conclusion

We presented QuantifyML for assessing the learnability, safety and robustness of machine learning mod-
els. Our experiments show the beneﬁt of precise quantiﬁcation over statistical measures and also high-
light how QuantifyML enables comparison of different learning algorithms.

Muhammad Usman, Divya Gopinath and Corina S. P˘as˘areanu

99

References

[1] QuantifyML GitHub. Available at https://github.com/muhammadusman93/quantifyml.

[2] Mahdieh Abbasi, Arezoo Rajabi, Christian Gagn´e & Rakesh B. Bobba (2020): Toward Adversarial Ro-
bustness by Diversity in an Ensemble of Specialized Deep Neural Networks.
In Cyril Goutte & Xiao-
dan Zhu, editors: Advances in Artiﬁcial Intelligence, Springer International Publishing, Cham, pp. 1–14,
doi:10.1007/978-3-030-47358-7 1.

[3] Rehan Abdul Aziz, Geoffrey Chu, Christian Muise & Peter James Stuckey (2015): #SAT: Projected Model

Counting. In: SAT, doi:10.1007/978-3-319-24318-4 10.

[4] Joao Bastos (2007): Credit scoring with boosted decision trees. Available at https://mpra.ub.

uni-muenchen.de/8156/1/MPRA_paper_8156.pdf.

[5] Armin Biere, Alessandro Cimatti, Edmund M. Clarke, Ofer Strichman & Yunshan Zhu (2003): Bounded

model checking. Adv. Comput. 58, pp. 117–148, doi:10.1016/S0065-2458(03)58003-2.

[6] B. Bonakdarpour & S. S. Kulkarni (2007): Exploiting Symbolic Techniques in Automated Synthesis of Dis-

tributed Programs with Large State Space. In: ICDCS, doi:10.1109/ICDCS.2007.109.

[7] Supratik Chakraborty, Kuldeep S. Meel & Moshe Y. Vardi (2013): A Scalable Approximate Model Counter.
In Christian Schulte, editor: Principles and Practice of Constraint Programming, Springer Berlin Heidelberg,
Berlin, Heidelberg, pp. 200–216, doi:10.1007/978-3-642-40627-0 18.

[8] Edmund M. Clarke, Daniel Kroening & Flavio Lerda (2004): A Tool for Checking ANSI-C Programs. In:
Tools and Algorithms for the Construction and Analysis of Systems, 10th International Conference, TACAS
2004, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2004,
Barcelona, Spain, March 29 - April 2, 2004, Proceedings, pp. 168–176, doi:10.1007/978-3-540-24730-2 15.

[9] Jeremy Cohen, Elan Rosenfeld & Zico Kolter (2019): Certiﬁed Adversarial Robustness via Randomized
Smoothing. In Kamalika Chaudhuri & Ruslan Salakhutdinov, editors: Proceedings of the 36th International
Conference on Machine Learning, Proceedings of Machine Learning Research 97, PMLR, pp. 1310–1320.
Available at http://proceedings.mlr.press/v97/cohen19c.html.

[10] Patrice Godefroid & Sarfraz Khurshid (2002): Exploring Very Large State Spaces Using Genetic Algorithms.
In Joost-Pieter Katoen & Perdita Stevens, editors: Tools and Algorithms for the Construction and Analysis
of Systems, Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 266–280, doi:10.1007/3-540-46002-0 19.

[11] Carla P Gomes, Ashish Sabharwal & Bart Selman (2009): Model counting. In: Handbook of satisﬁability,

IOS press, pp. 633–654, doi:10.3233/978-1-58603-929-5-633.

[12] Ian Goodfellow, Yoshua Bengio & Aaron Courville (2016): Deep Learning. MIT Press. Available at http:

//www.deeplearningbook.org.

[13] Divya Gopinath, Guy Katz, Corina S. P˘as˘areanu & Clark Barrett (2018): DeepSafe: A Data-Driven Ap-
proach for Assessing Robustness of Neural Networks.
In Shuvendu K. Lahiri & Chao Wang, editors:
Automated Technology for Veriﬁcation and Analysis, Springer International Publishing, Cham, pp. 3–19,
doi:10.1007/978-3-030-01090-4 1.

[14] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath & B. Kingsbury (2012): Deep Neural Networks for Acoustic Modeling in Speech Recogni-
tion: The Shared Views of Four Research Groups.
IEEE Signal Processing Magazine 29(6), pp. 82–97,
doi:10.1109/MSP.2012.2205597.

[15] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu
& Xinping Yi (2020): A survey of safety and trustworthiness of deep neural networks: Veriﬁcation,
testing, adversarial attack and defence, and interpretability. Computer Science Review 37, p. 100270,
doi:10.1016/j.cosrev.2020.100270.

[16] Daniel Jackson (2002): Alloy: a lightweight object modelling notation. ACM Trans. Softw. Eng. Methodol.

11(2), doi:10.1145/505145.505149.

100

QuantifyML: How Good is my Machine Learning Model?

[17] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian & Mykel J. Kochenderfer (2017): Reluplex: An
Efﬁcient SMT Solver for Verifying Deep Neural Networks.
In: Computer Aided Veriﬁcation - 29th Inter-
national Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I, pp. 97–117,
doi:10.1007/978-3-319-63387-9 5.

[18] Nikhil Ketkar (2017):

Introduction to keras.

In: Deep learning with Python, Springer, pp. 97–111,

doi:10.1007/978-1-4842-2766-4 7.

[19] Alex Krizhevsky, Ilya Sutskever & Geoffrey E Hinton (2012): ImageNet Classiﬁcation with Deep Convolu-
tional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou & K. Q. Weinberger, editors: Advances in
Neural Information Processing Systems, 25, Curran Associates, Inc., pp. 1097–1105, doi:10.1145/3065386.
[20] Daniel Kroening & Michael Tautschnig (2014): CBMC–C bounded model checker. In: International Con-
ference on Tools and Algorithms for the Construction and Analysis of Systems, Springer, pp. 389–391,
doi:10.1007/978-3-642-54862-8 26.

[21] Wen-Jia Kuo, Ruey-Feng Chang, Dar-Ren Chen & Cheng Chun Lee (2001): Data mining with decision trees
for diagnosis of breast tumor in medical ultrasonic images. Breast cancer research and treatment 66(1), pp.
51–57, doi:10.1023/A:1010676701382.

[22] Jean-Marie Lagniez & Pierre Marquis (2019): A recursive algorithm for projected model count-
Intelligence, 33, pp. 1536–1543,
the AAAI Conference on Artiﬁcial

Proceedings of

In:

ing.
doi:10.1609/aaai.v33i01.33011536.

[23] M. P. Owen, A. Panken, R. Moss, L. Alvarez & C. Leeper (2019): ACAS Xu: Integrated Collision Avoidance
and Detect and Avoid Capability for UAS. In: 2019 IEEE/AIAA 38th Digital Avionics Systems Conference
(DASC), pp. 1–10, doi:10.1109/DASC43569.2019.9081758.

[24] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik & Anan-
In: EuroS&P,

thram Swami (2016): The Limitations of Deep Learning in Adversarial Settings.
doi:10.1109/EuroSP.2016.36.

[25] Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg et al. (2011): Scikit-learn: Machine
learning in Python.
the Journal of machine Learning research 12, pp. 2825–2830. Available at https:
//www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf.

[26] S Rasoul Safavian & David Landgrebe (1991): A survey of decision tree classiﬁer methodology.

IEEE

transactions on systems, man, and cybernetics 21(3), pp. 660–674, doi:10.1109/21.97458.

[27] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow & R. Fergus (2013): Intriguing
Properties of Neural Networks. Available at http://arxiv.org/abs/1312.6199. Technical Report.
[28] Muhammad Usman, Wenxi Wang & Sarfraz Khurshid (2020): TestMC: Testing Model Counters using Differ-
ential and Metamorphic Testing. In: 2020 35th IEEE/ACM International Conference on Automated Software
Engineering (ASE), IEEE, pp. 709–721, doi:10.1145/3324884.3416563.

[29] Muhammad Usman, Wenxi Wang, Marko Vasic, Kaiyuan Wang, Haris Vikalo & Sarfraz Khurshid
(2020): A Study of the Learnability of Relational Properties: Model Counting Meets Machine Learn-
ing (MCML). PLDI 2020, Association for Computing Machinery, New York, NY, USA, p. 1098–1111,
doi:10.1145/3385412.3386015.

