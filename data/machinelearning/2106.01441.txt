1
2
0
2

n
u
J

2

]
E
S
.
s
c
[

1
v
1
4
4
1
0
.
6
0
1
2
:
v
i
X
r
a

Optimization of Heterogeneous Systems with AI
Planning Heuristics and Machine Learning: A
Performance and Energy Aware Approach

Suejb Memeti1 and Sabri Pllana2

1Blekinge Tekniska Hogskola, Department of Computer Science,
371 79 Karlskrona, Sweden; E-mail: suejb.memeti@bth.se
2Linnaeus University, Department of Computer Science and Media
Tech., 351 95 Vaxjo, Sweden; E-mail: sabri.pllana@lnu.se

Preprint

Abstract

Heterogeneous computing systems provide high performance and en-
ergy eﬃciency. However, to optimally utilize such systems, solutions that
distribute the work across host CPUs and accelerating devices are needed.
In this paper, we present a performance and energy aware approach that
combines AI planning heuristics for parameter space exploration with a
machine learning model for performance and energy evaluation to deter-
mine a near-optimal system conﬁguration. For data-parallel applications
our approach determines a near-optimal host-device distribution of work,
number of processing units required and the corresponding scheduling
strategy. We evaluate our approach for various heterogeneous systems
accelerated with GPU or the Intel Xeon Phi. The experimental results
demonstrate that our approach ﬁnds a near-optimal system conﬁguration
by evaluating only about 7% of reasonable conﬁgurations. Furthermore,
the performance per Joule estimation of system conﬁgurations using our
machine learning model is more than 1000x faster compared to the system
evaluation by program execution.

1

Introduction

Accelerators (such as, GPU or Intel Xeon Phi) are often used collaboratively
with general-purpose CPUs to increase the overall system performance and im-
prove energy eﬃciency. Currently, three of the top ﬁve most powerful computers
in the TOP500 list [8] are heterogeneous computers that use GPUs as acceler-
ators. Eﬃcient programming of heterogeneous systems and splitting the work
between the CPUs and accelerators are domains of particular interest [7, 21, 2].

1

 
 
 
 
 
 
(a) Brute-force search

(b) AI heuristic search

Figure 1: System performance tuning. (a) Evaluation of all possible conﬁgura-
tions with program execution, (b) AI heuristic selection of candidate conﬁgura-
tions and model-based evaluation.

Due to diﬀerent architectural characteristics and the large number of system
parameter conﬁgurations (such as, the number of threads, thread aﬃnity, work-
load partitioning between multi-core processors of the host and the accelerating
devices), achieving a good workload distribution that results with optimal per-
formance and energy eﬃciency on heterogeneous systems is a non-trivial task
[27, 33]. An optimal system conﬁguration that results with the highest through-
put may not necessarily be the most energy eﬃcient. Furthermore, the optimal
system conﬁguration is most likely to change for diﬀerent types of applications,
input problem sizes, and available resources.

Figure 1 depicts the process of system performance tuning. Traditionally the
process of ﬁnding the optimal system parameters involved many iterations of
selecting parameter values, program execution, and performance analysis (Fig-
ure 1a). A brute-force search requires the program execution for all parameter
values of interest, and consequently for real-world programs and systems it may
take an unreasonable long time to ﬁnd the optimum. In contrast to brute-force
search, an AI heuristic search [1, 34] enables to ﬁnd a near-optimum solution
with fewer performance experiments based on nature-inspired algorithms (for
instance, simulated annealing [20] or Artiﬁcial Bee Colony [18]). Figure 1b de-
picts our approach that combines AI heuristic parameter value selection with
a machine learning model for performance evaluation. The advantage of our
approach is two-fold:

• heuristic parameter value selection reduces signiﬁcantly the number of

performance experiments compared to brute-force search;

• system performance evaluation using a model is usually much faster than
the program execution on a real-world system; moreover, availability of
the system under study during the performance optimization process is
not required once that the model has been developed.

In [26] authors provide a comprehensive review of literature that describes

2

various approaches for using heuristics or machine learning for optimization of
parallel computing systems from single-node systems [13, 19] to large scale Grid
and Cloud computing infrastructures [10, 14, 31, 38, 11, 32]. The related work
usually uses for system optimization either heuristics [30] or machine learning
[13] and there is insuﬃcient research in combining these two methods in partic-
ular for optimization of heterogeneous computing systems. Preliminary results
of our optimization approach are discussed in [25]. However, [25] was limited to
only performance optimization of systems accelerated with the Intel Xeon Phi.
This paper extends our previous work as follows,

• we present a new machine learning model that estimates the performance
per Joule of the system (that is, it considers also energy consumption,
beside the performance);

• we demonstrate that our approach is general-applicable and it is not lim-
ited to Intel Xeon Phi, by applying our method also to GPU accelerated
systems;

• we demonstrate the applicability of our approach with two applications:

Pearson correlation coeﬃcient and parallel pattern matching.

We use an AI heuristic search technique for parameter space exploration.
The optimization process involves generation of system conﬁguration based on
random selection of parameter values and the system performance evaluation
using a machine learning model. Our method guides the process of intelligent
navigation through the parameter space towards the determination of the near-
optimal system conﬁguration. As a result, our method requires only a small
fraction of possible performance experiments. The empirical evaluation of our
approach demonstrates that by using only about 7% of the total brute-force
experiments we are able to determine a near-optimal system conﬁguration with
respect to the performance per Joule. Moreover, using our machine learning
model for evaluation of system conﬁgurations is more than 1000x faster than
the system evaluation by program execution.

This paper is organized as follows. Section 2 introduces computing systems
and applications that we use to illustrate our approach. We describe our opti-
mization approach in Section 3 and experimentally evaluate it in Section 4. We
discuss the related research in Section 5. Section 6 provides a summary of the
paper and a description of the future work.

2 Heterogeneous Systems and Applications

In this section, we describe heterogeneous computing systems and applications
that we use in this paper to illustrate and evaluate our approach. In addition,
we describe input data-sets and parameters that deﬁne system conﬁgurations.

3

2.1 Heterogeneous Computing Systems

Table 1 lists the major properties of heterogeneous computing systems Emil and
Ida that we use for experimentation in this paper.

Table 1: Major features of Ida and Emil.

Processor
Core Frequency
Number of cores
Number of threads
Cache size
Mem. Bandwidth
Memory size
TDP

Ida
Intel Xeon E5 GeForce GPU
GTX Titan X
E5-2650 v4
1 - 1.1 GHz
2.2 - 2.9 GHz
3072
12
/
24
/
30 MB
336.5 GB/s
76.8 GB/s
12 GB
384 GB
250 W
105 W

Emil

Intel Xeon E5
E5-2695 v2
2.4 - 3.2 GHz
12
24
30 MB
59.7 GB/s
128 GB
115 W

Intel Xeon Phi
7120P
1.2 - 1.3 GHz
61
244
30.5 MB
352 GB/s
16 GB
300 W

Ida comprises two Intel Xeon E5-2650 v4 general purpose CPUs on the
host, and one GeForce GTX Titan X GPU as accelerator. Each CPU has 12
physical cores, and each core of CPU supports two threads. In total, the host
CPUs provide 24 cores and 48 threads. The GPU device has 24 Streaming
Multiprocessors (SM), and in total 3072 CUDA cores running at base frequency
of 1 GHz.

Emil comprises two Intel Xeon E5-2695 v2 general purpose CPUs on the
host, and one Intel Xeon Phi 7120P co-processing device. Similar to Ida, Each
CPU of Emil has 12 cores that support two threads per core (known as logical
cores) that amounts to a total of 48 threads. The Xeon Phi device has 61 cores
running at 1.2 GHz base frequency; each core supports four hardware threads.
In total, the Xeon Phi supports 244 threads. The lightweight Linux operating
system installed on the Xeon Phi device runs on one of the cores.

The parameters that deﬁne the system conﬁguration for our optimization

approach are shown in Table 2. All the parameters are discrete.

The considered system parameters for the PCC application executed on
Ida include the CPU workload and GPU workload. The workload fraction
parameter can have any number in the range {0, .., 100}, such that if 60% of the
workload is assigned for processing to the host CPU, the remaining 100 − 60 =
40% is assigned to the GPU device.

The considered parameters for the pattern matching application executed
on Emil include the number of CPU threads and accelerator threads, the CPU
and accelerator thread aﬃnity, and the CPU and accelerator workload fraction.
The system parameter values for the host CPU threads are {12, 24, 36, 48},
whereas for the accelerator are {60, 120, 180, 240}. The thread aﬃnity can vary
between {none (0), compact (1), scatter (2)} for on host CPUs, and {balanced
(0), compact (1), scatter (2)} on accelerator. Similar to the Ida system, the
CPU and accelerator workload fraction can have values in the range {0, .., 100}.

4

Table 2: The set of considered parameters and their values for our target sys-
tems.

System Parameters

Ida

CPU workload fraction (CPU-W)
GPU workload fraction (GPU-W)

Parameter values

{0...100}
{100 - CPU-W}

Emil

CPU threads (CPU-T)
Accelerator threads (ACC-T)
CPU thread aﬃnity (CPU-A)
Accelerator thread aﬃnity (ACC-A)
CPU workload fraction (CPU-W)
Accelerator workload fraction (ACC-W)

{12, 24, 36, 48}
{60, 120, 180, 240}
{none (0), scatter (1), compact (2)}
{balanced (0), scatter (1), compact (2)}
{0...100}
{100 - CPU-W}

2.2 Applications

In this section, we describe two applications that we have prepared for execution
in hybrid mode (that is, the workload is shared between the host CPUs and the
accelerating device): (1) Pearson Correlation Coeﬃcient (PCC) and (2) parallel
pattern matching. While the PCC application is adapted to GPU-accelerated
systems, the pattern matching application targets systems accelerated with the
Intel Xeon Phi.

2.2.1 Pearson Correlation Coeﬃcient (PCC)

The Pearson Correlation Coeﬃcient [5], which is also known as Pearson product-
moment correlation, is a statistical method for measuring the strength of a
relationship between two variables x and y. The correlation may be positive
(1), negative (-1), or there is no correlation (0). Positive correlation means that
if variable x is increased, there will be an increase on the y variable, whereas the
negative correlation means that an increase on x would result with a decrease
on y. A value of 0 for the correlation means that the two variables are not
related and increase on one variable has no eﬀect on the other one.

The strength of the relationship is measured using the absolute value of the
correlation, for instance, abs(-0.23) = 0.23 is a stronger relationship than 0.2.
The Pearson correlation coeﬃcient is calculated using the following formula

[9]:

ρxy =

i=1(xi − ¯x)(yi − ¯y)))

(cid:80)n
i=1(xi − ¯x)2)(cid:112)(cid:80)n

i=1(yi − ¯y)2)

(cid:112)(cid:80)n

(1)

The Pearson coeﬃcient correlation has a wide range of applications. For
instance, [9] show its application to Magnetic Resonance Imaging (MRI) for
analyzing the functional connectivity of diﬀerent brain regions.

In this paper, we apply the Pearson coeﬃcient correlation to rows of matrices.
Note that for each row, we calculate the relationship with all other rows below in
the matrix. We added the glue code to the CPU and GPU PCC implementations
for enabling the hybrid execution. The input matrix is split row-wise between
the CPUs on the host and GPU.

5

It is worth to mention, that for this speciﬁc application, a 50% - 50% work-
load distribution does not mean that the work is equally divided between the
host and the accelerator. This is because, for instance row 0 compares itself
with rows 1, 2, 3 ... n. However, row 50 compares itself only with rows 51,
52, ... n. As the program execution progresses through the rows of the matrix,
there is less and less work to perform.

2.2.2 Parallel Pattern Matching

Pattern matching is the process of ﬁnding multiple and overlapping occurrences
of sub-strings in a string. Common uses of pattern matching are in search
and replace functions, determining the location of a pattern, or highlighting
important information out of huge data sets [37]. Pattern matching is also used
in intrusion detection systems (to identify potential threats) or computational
biology (to identify the location of some patterns in large DNA sequences).

In this paper, we use a pattern matching implementation [23, 24] to ﬁnd sub-
sequences of strings in the real-world human DNA sequence (3.17GB) obtained
from the GenBank sequence database of the National Center for Biological In-
formation [28].

Please note that the 50%-50% workload distribution of the pattern matching
application means that the workload is equally divided between the host and
the accelerator.

3 Our Approach for Optimization of Heteroge-

neous Systems

In this section, we ﬁrst motivate with one experiment the need for system-
atic optimization approaches, and thereafter we describe our approach for opti-
mization of heterogeneous computing systems that combines AI heuristic search
techniques with machine learning.

3.1 Motivational Experiment

Heterogeneous computing systems may comprise several CPUs and accelerators.
Optimal work-sharing among available CPUs and accelerators is not obvious.
Furthermore, considering both execution time and energy consumption makes
the work-sharing problem more complex.

Figure 2 illustrates this problem with the PCC application running on the
Ida heterogeneous system that comprises two general-purpose CPUs and one
GPU as accelerator. For simplicity, we only show the results for a particular
input size (that is a matrix with 1024 rows and 8192 columns) and we only
vary the parameter CPU Fraction (that is the number of rows of the matrix
mapped to the CPU). Furthermore, we consider only CPU Fraction values that
are product of 32.

From this experiment, we may derive the following insights:

6

• When we consider the T hroughput [M B/s] only, the optimal value for
the CPU fraction is 160 rows and rest of the work is mapped to the GPU
(Figure 2a).

• When optimizing for the P ower [W ] consumption, the execution on the
CPU only is preferable (Figure 2b); please note that also the overhead of
data transfers between CPU and GPU is considered for optimization.

• When optimizing for Energy Ef f iciency [M B/J] that considers both the
throughput and the power consumption, the optimal value for the CPU
fraction is 256 (Figure 2c).

3.2 System Optimization with AI heuristics and Machine

Learning (AML)

In this section, we propose an intelligent approach for determination of near-
optimal system conﬁguration using AI heuristic search techniques and machine
learning. We contrast traditional approaches (also known as brute-force search)
that we refer to as Enumeration and Measurements (EM), with our approach
of combining AI heuristics with Machine Learning (AML).

The EM technique (also known, as brute-force search) is depicted in Figure 3.
Compartment Enumeration visualizes the process of selecting each feasible value
of various system parameters. Thereafter, for each combination of parameter
values the system performance is measured. Basically, EM involves program
execution on a real computing system for all feasible system parameter values.
For real-world programs and systems, EM may result with a very large number
of performance measurement experiments. Furthermore, program execution on
real computing systems may be time-consuming. Usually computing systems
are shared among various users and it may be impractical to block other users
for an unreasonable large amount of time while we execute our performance
experiments. Key EM drawbacks include,

• a prohibitively large number of performance experiments,

• requires a dedicated access to the system under study.

Determining the optimal system conﬁguration using brute-force may be pro-
hibitively time expensive. The number of all possible system conﬁgurations is
a product of parameter value ranges,

n
(cid:89)

i=1

Rci = Rc1 × Rc2 × .. × Rcn

(2)

where C = {c1, c2, ..., cn} is a set of parameters and each ci has a value range

Rci.

7

(a) Max throughput is achieved when 160 rows are mapped to CPU and the rest to
GPU.

(b) All 1024 rows of matrix mapped to CPU results with the minimal power consump-
tion. Data transfer between CPU and GPU is prohibitive.

(c) Max energy eﬃciency is achieved when 256 rows are mapped to CPU and the rest
to GPU.

Figure 2: The CPU fraction that delivers the highest throughput (MB/s), lowest
power consumption (W), or highest energy eﬃciency (MB/J) is not the same.
The ﬁgure shows the experiments for the PCC application running on Ida for
matrix size 1024x8192. Measurements include also the performance overhead
for data transfers between CPU and GPU.

8

03264961281601922242562883203523844164484805125445766086406727047367688008328648969289609921024CPU Fraction0100200300400Throughput (MB/s)WinnerOptimal CPU fraction with respect to throughput (Matrix size: 1024x8192)03264961281601922242562883203523844164484805125445766086406727047367688008328648969289609921024CPU Fraction050100150200Power (W)WinnerOptimal CPU fraction with respect to power (Matrix size: 1024x8192)03264961281601922242562883203523844164484805125445766086406727047367688008328648969289609921024CPU Fraction0.000.250.500.751.001.251.501.752.00Energy efficiency (MB/J)WinnerOptimal CPU fraction with respect to energy_efficiency (Matrix size: 1024x8192)Figure 3: Enumeration and measurements may help to determine the optimal
system conﬁguration, however the required eﬀort may be high due to the large
parameter space.

The aim is to ﬁnd an optimization approach that is able to ﬁnd a near-
optimal system conﬁguration without having to measure the system perfor-
mance for all feasible parameter values. Our approach AML is depicted in
Figure 4. Key AML features include,

• requires only a small fraction of possible performance experiments,

• uses a machine learning model for performance evaluation.

AML uses a AI heuristic search technique for parameter space exploration.
The optimization process involves generation of system conﬁguration based on
random selection of parameter values and the system performance evaluation
using a machine learning model. AML guides the process of intelligent naviga-
tion through the parameter space towards the determination of the near-optimal
system conﬁguration. As a result, AML requires only a small fraction of possible
performance experiments.

In what follows, we describe into more details the two most important com-
ponents of our approach, which are the Simulated Annealing heuristic for design
space exploration and boosted decision tree machine learning algorithm for sys-
tem performance evaluation.

3.2.1 Using Simulated Annealing (SA) for Parameter Space Explo-

ration

Simulated Annealing (SA) [20] algorithm is a probabilistic optimization tech-
nique for approximately determining the global optimum of a function. SA is a

9

Measurement BasedEvaluationCompare the evaluation resultsTry New SolutionFigure 4: Our approach uses an AI heuristic search technique for design space
exploration and machine learning for system performance evaluation.

popular technique for searching for optimum in a discrete space, where gradient-
based methods are not applicable. An important feature of SA is the capability
to ﬁnd a global optimum also for functions that have many local optimums.

SA is inspired from the process of annealing in metallurgy, a technique that
requires heating and controlled cooling of materials [34]. At high temperatures
T particles of the material have more freedom of movement, and as the temper-
ature decreases the movement of particles is restricted. When the material is
cooled slowly, the particles are ordered in the form of a crystal that represents
minimal energy state of the material.

Algorithm 1 shows the pseudo-code of SA-based method for system con-
ﬁguration space exploration. A system conﬁguration is determined by speciﬁc
parameter values (see Table 2.) Since the optimal workload distribution for
some of the experiments is to either run everything on the CPU or GPU, we
initially evaluate the CPU and accelerator only system conﬁgurations (line 4).
The process starts by initializing the temperature T (line 5) and generating
a random system conﬁguration x (line 6). New solutions x(cid:48) are generated by
randomly modifying the parameter values of current solution x (line 8). We
evaluate each of the generated system conﬁgurations using our machine learn-
ing based prediction model (line 9). If the performance per Joule of x(cid:48) is better
than the one of x, we replace x with x(cid:48) unconditionally (line 11), otherwise we
consider accepting x(cid:48) based on probability p that is described by Equation 3.

p = exp((x − x(cid:48))/T )

(3)

The acceptance criterion (also known as Boltzmann’s probability distribu-
tion [34]) allows simulated annealing to get out of local optimums in favor of
a global optimum. The temperature variable plays a decisive role in the ac-

10

Machine Learning Based EvaluationCompare the evaluation resultsGenerate New SolutionAlgorithm 1 Our SA-based algorithm for exploration of system conﬁguration
space.

1: T : SA parameter for accepting a system conﬁguration as improvement
2: x: intermediate system conﬁguration
3: winner: optimal system conﬁguration
4: Initially evaluate CPU only and accelerator only solutions
5: Set the initial temperature T
6: Randomly generate the initial system conﬁguration x
7: while T > 1 do
8:
9:
10:

Generate a new solution x’ by randomly modifying x
Evaluate x’ using our machine learning model
if x’ better than x or acceptance criterion is met then

Replace x with x’
update winner

11:
12:
13:
14:
15: end while

end if
Decrease T

ceptance criterion. If the temperature T is high, the system is more likely to
accept solutions that are worse than the current one. After each new solution
we decrease the temperature (line 14). Steps 8-14 are repeated as long as the
temperature T is greater than one (line 15).

Figure 5: An example of the conﬁguration space exploration for PCC on Ida
using our SA-based method. We may observe the presence of multiple local
optimums, and that the SA determined the global optimum.

Figure 5 illustrates the process of space exploration using SA for PCC appli-
cation on Ida system (see Section 2.2.1). While there are many local optimums,
our SA-based method manages to avoid them and ﬁnds the global optimum.

Our optimization goal is to maximize the performance per Joule, which is

11

In
the rate of computation that can be delivered for every consumed watt.
HPC community, the standard metric for computationally-intensive application
is the ﬂoating point operations per second (FLOPS). As we target data-intensive
applications we use megabytes-per-second (MB/s) as a metric to indicate the
application’s throughput at run time. Beside the application performance we
also consider the energy consumption, therefore the optimization goal metric is
indicated by the total amount of data that can be processed for every consumed
watt, that is megabytes-per-watt-second (MB/Ws). Section 4.1 introduces eval-
uation metrics.

3.2.2 Using Machine Learning for Evaluation of System Conﬁgura-

tion

In this section, we describe our supervised machine learning model that is used
for evaluation of performance and energy consumption of data-parallel applica-
tions.

Our goal is to build a machine learning model that is able to predict the
energy eﬃciency for a target data-parallel application running on target hetero-
geneous system based on characteristics of the given workload, and the available
resources. Instead of using analytical models that are tightly coupled to a par-
ticular problem and environment, we use supervised machine learning that can
be trained for various data-parallel programs and heterogeneous systems.

Preliminary results that compare various machine learning models (including
linear regression, Poisson regression, and boosted decision tree regression) have
suggested that for this particular type of problem the decision tree regression
machine learning algorithm results with highest prediction scores. Hence, the
decision tree regression is used in our approach.

We use the python science kit [29] (scikit-learn) to develop our machine
learning model. The decision tree regression model is boosted with the Adaboost
ensemble method, which results with further improvement with respect to the
prediction accuracy.

To validate the results, we have used two types of well-established validation
techniques, the k-fold cross-validation [3] (where k=10 ) and the train test split
technique (where 80% of the data is used for training and 20% of the data is
used for testing). In both cases, the model resulted with high (more than 95%)
prediction accuracy scores, which we consider acceptable for the purpose of this
study. Note that for evaluating the prediction model, we use the R squared [36]
metric, also known as the coeﬃcient of determination, which basically measures
how close the data are to the ﬁtted regression line. Also, note that there is still
room for improvement with respect to the accuracy of the prediction model,
for instance by tuning the hyper parameters, or providing more training data
(especially for the data-set for the example of PCC running on GPU), which
was out of scope of this paper.

The data pre-processing transforms the string-type input values (such as
thread aﬃnity, which value is balanced, scatter, or compact) into integer types
(see Table 2 for the full set of parameters and their corresponding values).

12

4 Evaluation

In this section, we evaluate experimentally our proposed optimization approach
for workload distribution on heterogeneous computing platforms.

4.1 Evaluation Metrics

In this section, we describe a collection of metrics that we use for performance
evaluation.

Time: is determined by the execution time of the slowest processing unit

(that is, CPU or accelerator),

T ime(s) = max(CP UT ime, ACCT ime)

(4)

CP UT ime is workload processing time on CPU, and ACCT ime is workload
processing time on accelerator; ACCT ime includes also the overhead for trans-
ferring data between CPU and accelerator.

Throughput: is the amount of data processed within a given time,

T hroughput(M B/s) = W orkload size/T ime

(5)

We can also express the throughput separately for each available processing

unit as follows,

CP UT hroughput(M B/s) = CP UW orkload size/CP UT ime
ACCT hroughput(M B/s) = ACCW orkload size/ACCT ime

(6)

Energy: is the total energy consumed by all used processing unit. We use
MeterPU for energy measurement of host CPUs and GPUs, and x-MeterPU [22]
for energy measurements on Intel Xeon Phi.

Energy(J) = CP UEnergy + ACCEnergy

Power: is the consumed energy per time unit.

CP UP ower(W ) = CP UEnergy/CP UT ime
ACCP ower(W ) = ACCEnergy/ACCT ime
P ower(W ) = CP UP ower + ACCP ower

(7)

(8)

CP UP ower is power consumed by CPU, ACCP ower is power consumed by
accelerator, and P ower is the total power consumption for workload processing
by CPU and accelerator.

Energy Eﬃciency: is the ratio of throughput to power,

Energy Ef f iciency(M B/J) = T hroughput(M B/s)/P ower(W )

(9)

13

4.2 Performance Comparison of AML and EM

In this section we,

• demonstrate experimentally the prediction accuracy of the ML-based per-

formance model for Ida and Emil,

• demonstrate that it is signiﬁcantly faster to evaluate the performance using
the ML-based prediction model in comparison to the execution of the real
program on Emil and Ida,

• demonstrate that AML is capable to determine a near-optimal system con-
ﬁguration with a small fraction of performance experiments in comparison
to EM (that is, brute force search).

4.2.1 Prediction Accuracy and Time-eﬃciency of the ML-based Per-

formance Model

To predict the energy eﬃciency of PCC application on Ida computing system, we
have developed a prediction model based on Boosted Decision Tree Regression.
In this section, we address two concerns: (1) are the model-based predicted
results comparable to those obtained by measurements, and (2) how much faster
is to evaluate the performance using the model compared to measurement?

Figure 6 depicts measured and predicted energy eﬃciency for PCC applica-
tion on Ida system for various matrix sizes and fractions of matrix rows processed
by CPU. We may observe that there is good matching of results obtained from
the model with the measurement results.

Figure 6: Measured and predicted energy eﬃciency for PCC application on Ida
system. Energy eﬃciency is predicted for various matrix sizes and fractions
of matrix rows processed by CPU using our prediction model built based on
Boosted Decision Tree Regression.

EM involves running PCC on Ida to determine energy eﬃciency for vari-
ous parameter values. Execution of 2912 EM experiments in this study took

14

64x4096,16512x2048,08192x65536,32008192x32768,80644096x2048,37128192x65536,7552512x16384,0256x16,2564096x64,40968192x256,34562048x32,19848192x512,640128x32768,162048x1024,12162048x512,1088128x32,1044096x1024,32004096x8192,21762048x64,5124096x256,12801024x4096,2882048x2048,3842048x8192,641024x8192,3842048x8192,02048x16384,704256x256,04096x8192,3712128x4096,722048x16,832512x128,648192x32768,8962048x2048,7042048x1024,2564096x64,3200128x512,96128x16384,881024x64,384512x64,1928192x512,65288192x32,49928192x32768,29442048x2048,2562048x16,1088Rows x Columns, CPU_Fraction012345Energy efficiency (MB/J)Boosted Decision Tree Regressionmeasuredpredicted13404 seconds (approximately 3.7 hours). In contrast to EM, AML uses a ma-
chine learning model to predict energy eﬃciency for various parameter values;
model-based evaluation lasts 3.5 milliseconds. Regarding time-eﬃciency, for
2912 performance experiments EM and AML compare as follows,

• EM: 13404 [s]

• AML: 10 [s]

We may observe that in this study AML is more than 1300 times faster than

EM.

4.2.2 Determining Near-optimal System Conﬁguration for PCC on

Ida

In this section, we compare AML with EM with respect to search for a near-
optimal system conﬁguration for PCC application on Ida computing system.
The optimization goal is the energy eﬃciency. Major properties of PCC and Ida
are described in Section 2. AML and EM optimization techniques are described
in Section 3.2.

Figure 7: Examples of estimation of near-optimal CPU Fraction with AML and
EM for PCC application on Ida. CPU Fraction is the percentage of matrix
rows that is mapped for processing to CPU. The optimization goal is the energy
eﬃciency.

We performed experiments for various numbers of matrix rows (16, 32, 64,
128, 256, 512, 1024, 2048, 4096, 8192) and columns (16, 32, 64, 128, 256, 512,

15

512x327681024x40961024x81921024x163842048x20482048x40962048x81924096x1284096x2564096x5124096x10244096x20484096x40964096x81924096x163848192x1288192x2568192x5128192x10248192x20488192x40968192x81928192x163848192x65536Workload size010203040CPU Fraction (%)0.50.10.21.20.43.10.61.51.31.12.30.61.01.62.63.40.31.80.12.02.11.615.60.0DifferenceEMAMLTable 3: Examples of achieved energy eﬃciency with EM and AML for various
workload sizes of PCC application on Ida.

Workload: Matrix (R x C)
512 x 32768
1024 x 4096
1024 x 8192
1024 x 16384
2048 x 2048
2048 x 4096
2048 x 8192
4096 x 128
4096 x 256
4096 x 512
4096 x 1024
4096 x 2048
4096 x 4096
4096 x 8192
4096 x 16384
8192 x 128
8192 x 256
8192 x 512
8192 x 1024
8192 x 2048
8192 x 4096
8192 x 8192
8192 x 16384
8192 x 65536

EM
3.169
2.072
2.021
1.750
1.186
1.079
0.994
0.439
0.633
0.602
0.597
0.569
0.529
0.522
0.487
0.233
0.252
0.280
0.298
0.279
0.268
0.267
0.255
0.529

AML
3.169
2.067
2.021
1.744
1.076
1.059
0.990
0.433
0.561
0.589
0.580
0.547
0.523
0.505
0.487
0.211
0.243
0.268
0.276
0.270
0.260
0.256
0.252
0.529

|Dif f erence|
0.00000
0.00474
0.00000
0.00552
0.10936
0.01982
0.00362
0.00587
0.07285
0.01367
0.01656
0.02169
0.00684
0.01669
0.00034
0.02153
0.00980
0.01191
0.02204
0.00907
0.00828
0.01145
0.00338
0.00000

1024, 2048, 4096, 8192, 16384, 32768, 65536). The matrix elements are ini-
tialized with random values of ﬂoat type. Note that the performance results
reported in this section do not include the eﬀort required to initialize the ma-
trix. Considering parameter values ranges in this study there are,

• 14677 AML experiments, and

• 212914 EM experiments.

Figure 7 depicts examples of estimation of near-optimal CPU fraction with
AML and EM for PCC application on Ida for various matrix sizes. CPU Fraction
is the percentage of matrix rows that is mapped for processing to CPU; the rest
of matrix is processed by GPU. The optimization goal is the energy eﬃciency
(see Section 4.1).

Please note that the Figure 7 shows only examples of results where the CPU
fraction determined by AML diﬀers from the one determined by EM; that is
only 24 matrix sizes out of 130 in total. The data points depicted as small
rectangles correspond to the EM results, whereas the data points depicted as
small circles show the AML values. The vertical red lines show the diﬀerence
between AML and EM.

We may observe that in most of the cases AML can determine near-optimal
CPU fraction that yields the energy eﬃciency that is comparable to the one
determined by EM. For 106 matrix sizes there was no diﬀerence between AML
and EM.

The highest observed diﬀerence is 15.6% in the case of matrix size 8192x16384;
the EM suggests to map all the matrix rows to the GPU (that is, CPU fraction

16

is 0), but the AML suggests to map 15.6% of the workload to the CPU and the
rest to the GPU . While the CPU fraction diﬀerence of 15.6% is substantial,
the diﬀerence between AML and EM with respect to the achieved optimization
goal (that is, energy eﬃciency) is insigniﬁcant:

• AML achieved energy eﬃciency: 0.252 (MB/J)

• EM achieved energy eﬃciency: 0.255 (MB/J)

The corresponding energy eﬃciency results for workload sizes depicted in
the Figure 7 are listed in Table 3. We may observe that the diﬀerence between
EM and AML with respect to energy eﬃciency is not signiﬁcant; that is AML is
capable of determining system conﬁgurations that result with energy eﬃciency
comparable to EM.

4.2.3 Determining Near-optimal System Conﬁguration for Parallel

Pattern Matching on Emil

In this section, we compare AML with EM with respect to search for a near-
optimal system conﬁguration for parallel pattern matching on Emil computing
system. The optimization goal is the energy eﬃciency. We have described
parallel pattern matching and Emil computing system in Section 2.

In this study, despite the fact that we tested only what we considered reason-
able parameter values (listed on Table 2 in Section 2.1), 5590 experiments were
required when we used EM. Our heuristic-guided approach AML that is based
on Simulated Annealing and Machine Learning leads to comparatively good
performance results, while requiring only a relatively small set of experiments
to be performed.

Figure 8: Comparison of AML with EM for various percentages of possible
system conﬁgurations of Emil. Note that the total number of conﬁgurations is
5590 and by trying only about 7% of all conﬁgurations using AML we manage to
ﬁnd a near optimal system conﬁguration that delivers up to 97.55% of the energy
eﬃciency that is determined by the EM by trying all possible conﬁgurations.

17

12345678910Percentage of configurations [%]404142434445Energy efficiency (MB/J)EMAMLMax/MinFigure 8 depicts the energy eﬃciency of the pattern matching application
when running using the system conﬁguration suggested by AML. The solid hori-
zontal line indicates the energy eﬃciency of the system conﬁguration determined
by EM. EM ﬁnds the optimal solution by a kind of brute-force search. We may
observe that after evaluating only about 7% of the possible conﬁgurations, AML
is able to determine a system conﬁguration that results with energy eﬃciency
that is close to the one of the system conﬁguration determined by EM. The dif-
ference between AML and EM with respect to the achieved optimization goal
(that is, energy eﬃciency) is as follows:

• AML achieved energy eﬃciency: 43,87 (MB/J)

• EM achieved energy eﬃciency: 44,97 (MB/J)

Please note that AML uses a stochastic search algorithm for global opti-
mization and to avoid ending at a local optima during the search sometimes it
accepts a worse system conﬁguration (see Section 3.2.1).

5 Related Work

In this section, ﬁrst we discuss related research that addresses optimization
of heterogeneous computing systems. We are interested to know whether the
optimization involves host CPUs and device accelerators (such as, GPU or Intel
Xeon Phi), AI heuristics are used for searching for optimal system parameters,
machine learning is used for optimization, and whether the optimization goals
are power / energy consumption and performance. A summary of related work
is provided in Table 4. Thereafter, we brieﬂy describe how our approach that
is presented in this paper diﬀers from the related work.

Huang et al.

[17] propose a strategy for improvement of GPU power con-
sumption based on dynamic voltage and frequency scaling (DVFS). They de-
veloped a DVFS model for energy optimization based on proportional-integral-
derivative neural network (PIDNN). Three NVIDIA GPUs are used for experi-
mental evaluation: Quadro FX 380, GTX 460, GTX 680. Authors have applied
their method only to single GPU systems, and they plan to study multi-GPU
systems in future.
Haidar et al.

[15] study the relationship between energy consumption and
performance for various BLAS kernels on Intel Xeon Phi Knights Landing
(KNL). Authors use the PAPI library for power and performance monitoring.
A conclusion of this study is that to achieve a good performance with minimal
power consumption it is important to maximize the use of MCDRAM memory
on KNL device and minimize the use of DDR4 RAM of the system.

In [19], Kasichayanula et al. propose a software approach for real-time energy
consumption measurements and analysis on individual GPU components. They
use small subset of real-time statistics, and are able to infer about the activity
of other micro-components using a linear-regression based prediction model.

18

Pereira et al.

[30] propose an approach for determining optimal CUDA
block dimension for execution of a wind ﬁeld calculation program on GPU.
Authors use particle swarm optimization (PSO) heuristic to ﬁnd the CUDA
block dimension that results with minimal program execution time. Using PSO
they achieve 2x speed up compared to the previous GPU version of the program.
Hong and Kim [16] proposed an integrated energy and performance predic-
tion model for GPU accelerators that is able to predict the optimal number of
processors for a given memory-bound application that results with the peak per-
formance and lowest energy consumption. Their experimental results with ﬁve
various memory bound application benchmarks on a GPU architecture show up
to 10% reduction of energy consumption.

Cerotti et al. [6] study the performance modeling of GPU accelerated com-
puting systems. Authors present a language for description of system compo-
nents and workload elements. Properties of system components are described
with a set of parameters. For modeling systems that comprise CPUs and GPUs
authors use queuing networks.

Benkner et al. [4] developed PEPPHER that is a programming framework
for heterogeneous systems that comprise CPUs and accelerators (such as, GPU
or Intel Xeon Phi). PEPPHER involves source-to-source compilation and a run-
time system capable of mapping code components on an extensible set of target
processor architectures. To address the performance portability, PEPPHER
uses optimized component implementation variants for each execution context.
Regression analysis of historical performance data from previous component
executions is used to continuously learn associated performance models for run-
time scheduling decisions.

The distribution of computation in heterogeneous systems accelerated with
GPU devices for energy optimization has been studied by Ge et al.
[12]. Au-
thors propose the so called PEACH model that distributes the computations by
splitting the workload between host and accelerators, and adaptively schedules
these computations with regards to the computation units speed and energy
consumption. Authors use analytical models to optimally determine the work-
load distribution and scheduling that aims at minimal energy consumption and
best performance.

Ravi and Agrawal [35] propose a dynamic scheduling framework for het-
erogeneous computing systems that comprise various processing elements (such
as, CPU or GPU). Authors speciﬁcally address data parallel loops. Schedul-
ing decisions consider architectural trade-oﬀs, computation and communication
patterns. Performance optimization aims at minimizing data transfer between
CPU and GPU, reducing the number of kernel invocation on GPU, and reducing
the idle time of resources.

Grewe and O’Boyle [13] study task partitioning for OpenCL programs. Au-
thors address task partitioning and mapping on heterogeneous systems that
comprise a CPU and a GPU. Static analysis is used for code features extrac-
tion (such as, int and ﬂoat operations, or data transfer size), and thereafter a
machine learning model that is developed based on support vector machines
(SVM) is used to map code features to work partition. The model predicts

19

the percentages of work that should be assigned to GPU and CPU to achieve
optimal performance.

Table 4: A summary of related work with respect to whether optimization in-
volves host CPUs and device accelerators (such as, GPU or Intel Xeon Phi),
heuristic search is used for optimal system parameters, machine learning is used
for optimization, and whether the optimization goals are power / energy con-
sumption and performance.

Reference

Host

Device

Heuristic
search

Machine
learning

Power,
energy

Performance

Huang [17]
Haidar [15]
Kasichayanula [19]
Pereira [30]
Hong [16]
Cerotti [6]
Benkner [4]
Ge [12]
Ravi [35]
Grewe [13]
This paper

no
no
no
no
no
yes
yes
yes
yes
yes
yes

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

no
no
no
yes
no
no
no
no
no
no
yes

yes
no
yes
no
no
no
yes
no
no
yes
yes

yes
yes
yes
no
yes
no
no
yes
no
no
yes

no
yes
no
yes
yes
yes
yes
yes
yes
yes
yes

Compared to the related work summarized in Table 4, in addition to using
machine learning for modeling and evaluation of performance and power con-
sumption, we use heuristics to search for a near-optimal system conﬁguration.
In a large parameter space for system conﬁguration, heuristics have advantages
(such as, time to solution or general applicability) compared to time-prohibitive
brute-force approaches that aim at considering all possible conﬁgurations or
compared to ad hoc approaches that are limited to a speciﬁc case.

6 Summary and Future Work

Heterogeneous architectures are a viable way of building computer systems with
a high peak performance and a lower energy consumption. Optimal work-
sharing among available CPUs and accelerators is not obvious, and consider-
ing both performance and energy consumption further complicates the work-
sharing problem. We have presented an approach that uses a probabilistic
heuristic search technique for parameter space exploration. The optimization
process involves generation of system conﬁguration based on random selection
of parameter values and the system performance evaluation using a machine
learning model. We have evaluated our approach experimentally on heteroge-
neous systems that are accelerated with the Intel Xeon Phi or GPU for two
applications: Pearson correlation coeﬃcient and parallel pattern matching. In
our experiments we have observed that,

• the more than 95% accuracy of the developed machine learning model

enables to inform the search for optimal system conﬁguration,

20

• by considering only about 7% of the possible system conﬁgurations our
method ﬁnds a near-optimal conﬁguration with respect to performance
per Joule;

• our method is 1300x faster than the brute-force search combined with

measurement of program execution;

• applicability of our approach is not limited to one type of accelerator or
application; the model can be trained for various accelerated systems.

Future work will study work-sharing in the context of the Edge, Fog and

Cloud systems.

References

[1] E. Alba. How can metaheuristics help software engineers? In T. E. Colanzi
and P. McMinn, editors, Search-Based Software Engineering, pages 89–105,
Cham, 2018. Springer International Publishing.

[2] V. Amaral, B. Norberto, M. Goul˜ao, M. Aldinucci, S. Benkner, A. Brac-
ciali, P. Carreira, E. Celms, L. Correia, C. Grelck, H. Karatza, C. Kessler,
P. Kilpatrick, H. Martiniano, I. Mavridis, S. Pllana, A. Resp´ıcio, J. Sim˜ao,
L. Veiga, and A. Visa. Programming languages for data-intensive HPC
applications: A systematic mapping study. Parallel Computing, 91:102584,
2020.

[3] D. Anguita, L. Ghelardoni, A. Ghio, L. Oneto, and S. Ridella. The’k’in

k-fold cross validation. In ESANN, 2012.

[4] S. Benkner, S. Pllana, J. Traﬀ, P. Tsigas, U. Dolinsky, C. Augonnet,
B. Bachmayer, C. Kessler, D. Moloney, and V. Osipov. PEPPHER: Ef-
ﬁcient and Productive Usage of Hybrid Computing Systems. Micro, IEEE,
31(5):28–41, 09 2011.

[5] S. Boslaugh. Statistics in a nutshell: A desktop quick reference. ” O’Reilly

Media, Inc.”, 2012.

[6] D. Cerotti, M. Gribaudo, M. Iacono, and P. Piazzolla. Modeling and
analysis of performances for concurrent multithread applications on multi-
core and graphics processing unit systems. Concurrency and Computation:
Practice and Experience, 28(2):438–452, 2016.

[7] P. Czarnul. Parallel Programming for Modern High Performance Comput-

ing Systems. CRC Press, Boca Raton, Florida, USA, 2018.

[8] J. J. Dongarra, H. W. Meuer, E. Strohmaier, et al. TOP500 Supercomputer

Sites. http://www.top500.org/, Nov. 2020. Accessed: Jan. 2021.

21

[9] T. Eslami, M. G. Awan, and F. Saeed. Gpu-pcc: A gpu based technique
to compute pairwise pearson’s correlation coeﬃcients for big fmri data. In
Proceedings of the 8th ACM International Conference on Bioinformatics,
Computational Biology, and Health Informatics, pages 723–728, 2017.

[10] T. Fahringer, A. Jugravu, S. Pllana, R. Prodan, C. Seragiotto Jr, and H.-L.
Truong. Askalon: a tool set for cluster and grid computing. Concurrency
and Computation: Practice and Experience, 17(2-4):143–169, 2005.

[11] T. Fahringer, S. Pllana, and J. Testori. Teuta: Tool Support for Perfor-
mance Modeling of Distributed and Parallel Applications.
In Computa-
tional Science - ICCS 2004, volume 3038 of Lecture Notes in Computer
Science, pages 456–463. Springer Berlin Heidelberg, 2004.

[12] R. Ge, X. Feng, M. Burtscher, and Z. Zong. Performance and energy
modeling for cooperative hybrid computing. In Networking, Architecture,
and Storage (NAS), 2014 9th IEEE International Conference on, pages
232–241, Aug 2014.

[13] D. Grewe and M. F. O(cid:48)Boyle. A static task partitioning approach for
In Compiler Construction, pages

heterogeneous systems using OpenCL.
286–305. Springer, 2011.

[14] D. Grzonka, A. Jak´obik, J. Ko(cid:32)lodziej, and S. Pllana. Using a multi-agent
system and artiﬁcial intelligence for monitoring and improving the cloud
performance and security. Future Generation Computer Systems, 86:1106–
1117, 2018.

[15] A. Haidar, H. Jagode, A. YarKhan, P. Vaccaro, S. Tomov, and J. Dongarra.
Power-aware computing: Measurement, control, and performance analysis
for Intel Xeon Phi. In 2017 IEEE High Performance Extreme Computing
Conference (HPEC), pages 1–7, 2017.

[16] S. Hong and H. Kim. An integrated gpu power and performance model. In
ACM SIGARCH Computer Architecture News, volume 38, pages 280–289.
ACM, 2010.

[17] Y. Huang, B. Guo, and Y. Shen. Gpu energy consumption optimization
with a global-based neural network method. IEEE Access, 7:64303–64314,
2019.

[18] D. Karaboga and B. Basturk. Artiﬁcial Bee Colony (ABC) Optimization
Algorithm for Solving Constrained Optimization Problems. In Proceedings
of the 12th International Fuzzy Systems Association World Congress on
Foundations of Fuzzy Logic and Soft Computing, IFSA 07, pages 789—-
798, Berlin, Heidelberg, 2007. Springer-Verlag.

[19] K. Kasichayanula, D. Terpstra, P. Luszczek, S. Tomov, S. Moore, and G. D.
Peterson. Power aware computing on gpus. In Application Accelerators in

22

High Performance Computing (SAAHPC), 2012 Symposium on, pages 64–
73. IEEE, 2012.

[20] S. Kirkpatrick, D. Gelatt, and M. Vecchi. Optimization by Simulated An-

nealing. Science, 220(4598):671–680, 1983.

[21] S. Markidis, S. Chien, E. Laure, I. Peng, and J. Vetter. NVIDIA Tensor
Core Programmability, Performance Precision. In 2018 IEEE International
Parallel and Distributed Processing Symposium Workshops (IPDPSW),
pages 522–531, 2018.

[22] S. Memeti, L. Li, S. Pllana, J. Kolodziej, and C. Kessler. Benchmark-
ing OpenCL, OpenACC, OpenMP, and CUDA: Programming Productiv-
In Proceedings of the 2017
ity, Performance, and Energy Consumption.
Workshop on Adaptive Resource Management and Scheduling for Cloud
Computing, ARMS-CC ’17, pages 1–6, New York, NY, USA, 2017. ACM.

[23] S. Memeti and S. Pllana. PaREM: A Novel Approach for Parallel Regular
Expression Matching. In 17th International Conference on Computational
Science and Engineering (CSE-2014), pages 690–697, Chengdu, China, Dec
2014.

[24] S. Memeti and S. Pllana. Accelerating dna sequence analysis using intel(r)
xeon phi(tm). In 2015 IEEE Trustcom/BigDataSE/ISPA, volume 3, pages
222–227. IEEE, 2015.

[25] S. Memeti and S. Pllana. Combinatorial optimization of work distribution
In 45th International Conference on Parallel

on heterogeneous systems.
Processing Workshops (ICPPW), pages 151–160, Aug 2016.

[26] S. Memeti, S. Pllana, A. Binotto, J. Ko(cid:32)lodziej, and I. Brandic. Using
meta-heuristics and machine learning for software optimization of parallel
computing systems: a systematic literature review. Computing, Apr 2018.

[27] S. Mittal and J. S. Vetter. A Survey of CPU-GPU Heterogeneous Com-
puting Techniques. ACM Comput. Surv., 47(4):69:1–69:35, July 2015.

[28] NCBI. National Center for Biotechnology Information U.S. National Li-
brary of Medicine. http://www.ncbi.nlm.nih.gov/genbank, 2015. Ac-
cessed: Dec. 2015.

[29] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn:
Machine learning in python.
the Journal of machine Learning research,
12:2825–2830, 2011.

[30] C. Pereira, A. Pinheiro, and R. Schirru. Automatic block dimensioning on
GPU-accelerated programs through particle swarm optimization. Informa-
tion and Software Technology, 123:106299, 2020.

23

[31] S. Pllana, S. Benkner, E. Mehofer, L. Natvig, and F. Xhafa. Towards an In-
telligent Environment for Programming Multi-core Computing Systems. In
Euro-Par Workshops, volume 5415 of Lecture Notes in Computer Science,
pages 141–151. Springer, 2008.

[32] S. Pllana, I. Brandic, and S. Benkner. A Survey of the State of the Art in
Performance Modeling and Prediction of Parallel and Distributed Comput-
ing Systems. International Journal of Computational Intelligence Research
(IJCIR), 4(1):17–26, 01 2008.

[33] S. Pllana and F. Xhafa. Programming Multicore and Many-core Computing
Systems. John Wiley & Sons, Inc., Hoboken, New Jersey, USA, 1 edition,
2017.

[34] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Nu-
merical Recipes 3rd Edition: The Art of Scientiﬁc Computing. Cambridge
University Press, New York, 3 edition, 2007.

[35] V. T. Ravi and G. Agrawal. A dynamic scheduling framework for emerging
In High Performance Computing (HiPC), 2011

heterogeneous systems.
18th International Conference on, pages 1–10. IEEE, 2011.

[36] O. Renaud and M.-P. Victoria-Feser. A robust coeﬃcient of determination
for regression. Journal of Statistical Planning and Inference, 140(7):1852–
1862, 2010.

[37] S. Vitabile, M. Marks, D. Stojanovic, S. Pllana, J. M. Molina, M. Krzysz-
ton, A. Sikora, A. Jarynowski, F. Hosseinpour, A. Jakobik, A. Stojnev Ilic,
A. Respicio, D. Moldovan, C. Pop, and I. Salomie. Medical Data Processing
and Analysis for Remote Health and Activities Monitoring, pages 186–220.
Springer International Publishing, Cham, 2019.

[38] F. Xhafa, J. A. Gonzalez, K. P. Dahal, and A. Abraham. A ga(ts) hybrid
algorithm for scheduling in computational grids. In E. Corchado, X. Wu,
E. Oja, ´A. Herrero, and B. Baruque, editors, Hybrid Artiﬁcial Intelligence
Systems, pages 285–292, Berlin, Heidelberg, 2009. Springer Berlin Heidel-
berg.

24

