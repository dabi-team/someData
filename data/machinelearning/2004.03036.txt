2
2
0
2

l
u
J

1

]

M
E
.
n
o
c
e
[

7
v
6
3
0
3
0
.
4
0
0
2
:
v
i
X
r
a

Double Debiased Machine Learning Nonparametric Inference with
Continuous Treatments∗

Kyle Colangelo

Ying-Ying Lee†

June 2022

Abstract

We propose a nonparametric inference method for causal eﬀects of continuous treatment vari-
ables, under unconfoundedness and nonparametric or high-dimensional nuisance parameters. Our
double debiased machine learning (DML) estimators for the average dose-response function (or
the average structural function) and the partial eﬀects are asymptotically normal with nonpara-
metric convergence rates. The nuisance estimators for the conditional expectation function and
the conditional density can be nonparametric or ML methods. Utilizing a kernel-based doubly
robust moment function and cross-ﬁtting, we give high-level conditions under which the nuisance
estimators do not aﬀect the ﬁrst-order large sample distribution of the DML estimators. We
further provide suﬃcient low-level conditions for kernel, series, and deep neural networks. We
propose a data-driven bandwidth to consistently estimate the optimal bandwidth that minimizes
the asymptotic mean squared error. We justify the use of kernel to localize the continuous treat-
ment at a given value by the Gateaux derivative. We implement various ML methods in Monte
Carlo simulations and an empirical application on a job training program evaluation.

Keywords: Average structural function, cross-ﬁtting, dose-response function, doubly robust, high
dimension.
JEL Classiﬁcation: C14, C21, C55

∗The ﬁrst version was circulated as Lee (February 2019), “Double machine learning nonparametric inference
on continuous treatment eﬀects.” We are grateful to Max Farrell, Whitney Newey, and Takuya Ura for valuable
discussion. We thank seminar participants at Harvard-MIT, UC Irvine, Bonn-Mannheim, USC, Essex, and Oxford,
conference participants in 2019: Barcelona Summer Forum workshop on Machine Learning for Economics, North
American Summer Meeting of the Econometric Society, Vanderbilt/CeMMAP/UCL conference on Advances in
Econometrics, Midwest Econometrics Group, California Econometrics Conference, and 2020/2022 North American
Winter Meeting of the Econometric Society, 2021 IAAE.

†Ying-Ying Lee, Department of economics, University of California Irvine. E-mail: yingying.lee@uci.edu. Kyle

Colangelo, Amazon.

1

 
 
 
 
 
 
1

Introduction

We propose a nonparametric inference method for continuous treatment or structural eﬀects, under
the unconfoundedness assumption1 and in the presence of nonparametric or high-dimensional
nuisance parameters. We focus on the heterogenous eﬀect with respect to the continuous treatment
or policy variables T . To identify the causal eﬀects, it is plausible to allow the number of the control
variables X to be large relative to the sample size n. To achieve valid inference and to employ
machine learning (ML) methods, we use a double debiased ML approach that combines a doubly
robust moment function and cross-ﬁtting.

We consider a nonparametric and model-free outcome equation Y = g(T, X, ε). No functional
form assumption is imposed on the unobserved disturbances ε, such as restrictions on dimension-
ality, monotonicity, or separability. The potential outcome is Y (t) = g(t, X, ε) indexed by the
hypothetical treatment value t. The object of interest is the average dose-response function as a
function of t, deﬁned as the expected value of the potential outcome across observations with the
observed and unobserved heterogeneity (X, ε), i.e., βt ≡ E[Y (t)] = (cid:82) (cid:82) g(t, X, ε)dFXε. It is also
known as the average structural function in nonseparable models in Blundell and Powell (2003).
The well-studied average treatment eﬀect of switching from treatment t to s is βs − βt. We deﬁne
the partial (or marginal) eﬀect of the ﬁrst component of the continuous variable T at t = (t1, ...tdt)(cid:48)
to be the partial derivative θt ≡ ∂βt/∂t1. In program evaluation, the average dose response func-
tion βt shows how participants’ labor market outcomes vary with the length of exposure to a job
training program. In demand analysis when T contains price and income, the average structural
function βt can be the Engel curve. The partial eﬀect θt reveals the average price elasticity at
given values of price and income and hence captures the unrestricted heterogenous eﬀects.

A main contribution of this paper is a formal inference theory for the fully nonparametric
causal eﬀects of continuous variables, allowing for high-dimensional nuisance parameters. To
uncover the causal eﬀect of the continuous variable T on Y , our nonparametric nonseparable
model Y = g(T, X, ε) can be compared to the partially linear model Y = θT + g(X) + ε in
Robinson (1988) that speciﬁes the homogenous eﬀect by θ and hence is a semiparametric problem.
The important partially linear model has many applications and is one of the leading examples in
the recent ML literature, where the nuisance function g(X) can be high-dimensional and estimated
by a ML method.2 Another semiparametric parameter of interest is the weighted average of βt

1This commonly used identifying assumption based on observational data, also known as conditional indepen-
dence and selection on observables, assumes that conditional on observables, the treatment variable is as good as
randomly assigned, or conditionally exogenous.

2See Chernozhukov et al. (2018), Chernozhukov et al. (2018), and references therein. Demirer et al. (2019) and
Oprescu et al. (2019) extend to more general functional forms. Cattaneo et al. (2018a), Cattaneo et al. (2018b),

2

or θt over a range of treatment values t, such as the average derivative that summarizes certain
aggregate eﬀects (Powell et al., 1989) and the bound of the average welfare eﬀect in Chernozhukov
et al. (2019). In contrast, our average structural function βt and the partial eﬀect θt capture the
fully nonparametric heterogenous eﬀects of T .

We are among the ﬁrst to apply the double debiased ML approach to inference on the average
structural function βt and the partial eﬀect θt of continuous variables, to our knowledge. They
are non-regular nonparametric objects that cannot be estimated at a root-n convergence rate.
We propose a kernel-based double debiased machine learning (DML) estimator that utilizes a
doubly robust moment function and cross-ﬁtting via sample-splitting. The DML estimator uses
the moment function

γ(t, Xi) +

Kh(Ti − t)
fT |X(t|Xi)

(cid:0)Yi − γ(t, Xi)(cid:1),

(1)

where the conditional expectation function γ(t, x) ≡ E[Y |T = t, X = x] and the conditional
density fT |X(t|x), also known as the generalized propensity score (GPS). A kernel Kh(Ti − t)
weights observation i with treatment value around t in a distance of h. The number of such
observations shrinks as the bandwidth h vanishes with the sample size n. A L-fold cross-ﬁtting
splits the sample into L subsamples. The nuisance estimators for γ(t, Xi) and fT |X(t|Xi) use
observations in the other L − 1 subsamples that do not contain the observation i. The DML
estimator averages over the subsamples. Then we estimate the partial eﬀect θt by a numerical
diﬀerentiation.

The doubly robust moment function in equation (1) has appeared in Kallus and Zhou (2018)
without asymptotic theory and has been extensively studied in Su et al. (2019) for Lasso-type
nuisance estimators. We utilize cross-ﬁtting and provide high-level and low-level conditions that
facilitate a variety of nonparametric and ML methods. As each ML method has its strength
and weakness depending on the data generating process and applications, it is desired to ﬂexibly
employ various nuisance estimators. High-dimensional control variables are accommodated via
the nuisance estimators; for example, Lasso allows the dimension of X grows with the sample size.
We show that the proposed kernel-based DML estimators are asymptotically normal and pro-
vide high-level conditions under which the nuisance estimators for γ(t, Xi) and fT |X(t|Xi) do not
aﬀect the ﬁrst-order asymptotic distribution. Speciﬁcally the high-level conditions on the conver-
gence rates use a partial L2 norm that ﬁxes the treatment value at t, in contrast to the standard
L2 norm that integrates over the joint distribution of (T, X), i.e., the root-mean-squared rate.
We further give low-level conditions for nonparametric kernel, series estimators, and the deep

Cattaneo et al. (2019), and Farrell et al. (2021a) propose diﬀerent approaches.

3

neural networks in Farrell et al. (2021b). These results on the convergence rates of the nuisance
estimators are new to the literature, to our best knowledge.

Furthermore, we propose a Simulated DML (SDML) estimator that replaces γ(t, Xi) in (1)
with γ(Ui, Xi) where a simulated variable Ui localizes the realized treatment values around the
target value t.
Introducing such a local variation enables the standard L2 convergence rate of
the nuisance estimator that is available for nonparametric kernel and series estimators, as well as
recent ML estimators, such as Lasso (Bickel et al., 2009), neural networks (Chen and White, 1999;
Schmidt-Hieber, 2020; Farrell et al., 2021b), random forests (Syrgkanis and Zampetakis, 2020),
and empirical L2 rate for boosting in Luo and Spindler (2016), as discussed in Chernozhukov et al.
(2018) (CCDDHNR, hereafter) and Chernozhukov et al. (2022a).3

In addition, we propose generic ML estimators for the reciprocal of the conditional density
1/fT |X(t|x) when dt = 1 and for fT |X(t|x) when dt > 1 respectively, which may be of independent
interest. We also propose a data-driven bandwidth to consistently estimate the optimal bandwidth
that minimizes the asymptotic mean squared error.

We aim for a tractable inference procedure that is ﬂexible to employ nonparametric or ML
nuisance estimators and delivers a reliable distributional approximation in practice. Toward that
end, the DML method contains two key ingredients: a doubly robust moment function and cross-
ﬁtting. The doubly robust moment function reduces sensitivity in estimating βt with respect
to nuisance parameters.4 Cross-ﬁtting further removes bias induced by overﬁtting and achieves
stochastic equicontinuity without strong entropy conditions.5

Our work builds on the results for semiparametric models in Ichimura and Newey (2022),
Chernozhukov et al. (2018) (CEINR, hereafter), and CCDDHNR and extends the literature to
nonparametric continuous treatment/structural eﬀects. It is useful to note that the doubly robust
estimator for a binary/multivalued treatment replaces the kernel Kh(Ti − t) with the indicator
function 1{Ti = t} in equation (1) and has been widely studied, especially in the recent ML liter-
ature. We show that the advantageous properties of the DML estimator for the binary treatment

3We are grateful to Whitney Newey for the idea of simulating Ui from the probability density function fU (u) =

fT ((u − t)/h) /h.

4Our estimator is doubly robust in the sense that it consistently estimates βt if either one of the nuisance functions
E[Y |T, X] or fT |X is misspeciﬁed. The rapidly growing ML literature has utilized this doubly robust property to
reduce regularization and modeling biases in estimating the nuisance parameters by ML or nonparametric methods;
for example, Belloni et al. (2014), Farrell (2015), Belloni et al. (2017), Farrell et al. (2021b), Chernozhukov et al.
(2018), CCDDHNR, Rothe and Firpo (2019), and references therein.

5CCDDHNR point out that the commonly used results in empirical process theory, such as Donsker properties,
could break down in high-dimensional settings. For example, Belloni et al. (2017) show how cross-ﬁtting weakens
the entropy condition and hence the sparsity assumption on nuisance Lasso estimator. The beneﬁt of cross-ﬁtting
is further investigated by Wager and Athey (2018) for heterogeneous causal eﬀects, Newey and Robins (2018) for
double cross-ﬁtting, and Cattaneo and Jansson (2019) for cross-ﬁtting bootstrap.

4

carry over to the continuous treatments case. Our DML estimator utilizes the kernel function
Kh(Ti − t) for the continuous treatments T of ﬁxed low dimension and averages out the possibly
high-dimensional covariates X. Our kernel-based estimator appears to be a simple modiﬁcation of
the binary treatment case in practice, we show that one important distinct feature of non-regular
nonparametric parameters is that the Gateaux derivative and the Riesz representer are not unique,
depending on what we choose to approximate the continuous treatment distribution of a point
mass. And the kernel function is a natural choice for localization at t. Neyman orthogonality
holds as h → 0 (Neyman, 1959). Therefore we provide a foundational justiﬁcation for the pro-
posed kernel-based DML estimator for βt, relative to alternative approaches, such as Kennedy
et al. (2017) and Semenova and Chernozhukov (2020).6

There is a small yet growing literature on employing the DML approach for non-regular non-
parametric objects. Hsu et al. (2022) propose a test for monotonicity of βt. Chernozhukov et al.
(2022b), Semenova and Chernozhukov (2020), Fan et al. (2021), and Zimmert and Lechner (2019)
study the conditional average binary treatment eﬀect E[Y (1) − Y (0)|X1] for a low-dimensional
subset X1 of X. Despite the advantageous theoretical properties, we note potential drawbacks
of the DML approach: Double robustness often requires additional nuisance estimation, such as
the conditional density in the Riesz representer, which could introduce additional variation and
implementation complication. Cross-ﬁtting could result in a small eﬀective sample size. For ex-
ample, the Monte Carlo simulations in Fan et al. (2021) show that cross-ﬁtting does not improve
the ﬁnite-sample performance.

Our paper adds to the literature on continuous treatment eﬀects estimation. In low-dimensional
settings, see Imbens (2000), Hirano and Imbens (2004), Flores (2007), and Lee (2018) for examples
of a class of regression estimators n−1 (cid:80)n
i=1 ˆγ(t, Xi). Galvao and Wang (2015) and Hsu et al. (2020)
study a class of inverse probability weighting estimators. The empirical applications in Flores et al.
(2012) and Kluve et al. (2012) focus on semiparametric results. We extend this literature to high-
dimensional settings and enable ML methods for nonparametric inference in practice.

The paper proceeds as follows. We introduce the framework and estimation procedure in Sec-
tion 2. Section 3 presents the asymptotic theory of the DML estimators and low-level conditions
for various nuisance estimators. Section 4 introduces the simulated DML estimator and the cor-
responding inference theory. Section 5 demonstrates the usefulness of our DML estimator with

6Kennedy et al. (2017) construct a “pseudo-outcome” that is motivated from the doubly robust and eﬃcient
inﬂuence function of the regular semiparametric parameter (cid:82) βtfT (t)dt. Then they locally regress the pseudo-
outcome on T at t using a kernel to estimate βt. Semenova and Chernozhukov (2020) illustrate in an example
to estimate βt by the best linear projection of an “orthogonal signal of the outcome” which is the same “pseudo-
outcome” proposed by Kennedy et al. (2017). In contrast, we motivate the moment function of our DML estimator
directly from βt via the Gateaux derivative or the ﬁrst-step adjustment.

5

various ML methods in Monte Carlo simulations and an empirical example on the Job Corps
program evaluation. All the proofs are in the Appendix.

2 Setup and estimation

i=1 be an i.i.d. sample from Z = (Y, T (cid:48), X (cid:48))(cid:48) ∈ Z = Y × T0 × X ⊆ R1+dt+dx from
Let {Yi, Ti, Xi}n
a population P with a cumulative distribution function (CDF) FZ(Z). We give assumptions and
introduce the double debiased machine learning estimator. Consider a set of treatment values of
interest T ⊂ T0.

Assumption 1 (i) (Conditional independence) T and ε are independent conditional on X.
(ii) (Common support) There exists a positive constant c such that inf t∈T ess inf x∈X fT |X(t|x) ≥ c.
(iii) fZ(y, t, x) and E(Y |T = t, X = x) are three-times diﬀerentiable with respect to t with all
three derivatives being bounded uniformly over (y, t(cid:48), x(cid:48))(cid:48) ∈ Z. (iv) var(Y |T = t, X = x) and its
derivatives with respect to t are bounded uniformly over (t, x)(cid:48) ∈ T × X .

Deﬁne the product kernel as Kh(Ti−t) ≡ Πdt

j=1k((Tji−tj)/h)/hdt, where Tji is the jth component
of Ti and the kernel function k() satisﬁes Assumption 2. At the cost of notational complexity, we
can allow diﬀerent bandwidth hj for each Tj, j = 1, ..., dt. Practitioners often divide each variable
Tj by its sample standard deviation and apply the same bandwidth. Denote the roughness of k
as Rk ≡ (cid:82) ∞

−∞ k(u)2du.

Assumption 2 (Kernel) The second-order symmetric kernel function k() (i.e., (cid:82) ∞
−∞ uk(u)du = 0, and 0 < (cid:82) ∞
(cid:82) ∞
constants C, ¯U , and for some ν > 1, |dk(u)/du| ≤ C|u|−ν for |u| > ¯U .

−∞ k(u)du = 1,
−∞ u2k(u) < ∞.) is bounded diﬀerentiable. For some ﬁnite positive

Assumption 2 is standard in nonparametric kernel estimation and holds for commonly used
kernel functions, such as Epanechnikov and Gaussian. By Assumptions 1-2 and the same reasoning
for the binary treatment, it is straightforward to show the identiﬁcation for t ∈ T ,

βt ≡ E[Y (t)] =

(cid:90)

X

E[Y |T = t, X]dFX(X) = E(cid:2)γ(t, X)(cid:3)
(cid:90)

= lim
h→0

Kh(T − t)Y
fT |X(t|X)

Z

dFZ(Z) = lim
h→0

E

(cid:20)Kh(T − t)Y
fT |X(t|X)

(cid:21)

.

(2)

(3)

The expression in equation (2) motivates the class of regression (or imputation) based estimators,
while equation (3) motivates the class of inverse probability weighting estimators; see Section D.1

6

in the Appendix for further discussion. Now we introduce the double debiased machine learning
estimator.

Estimation procedure

Step 1. (Cross-ﬁtting) For some ﬁxed L ∈ {2, ..., n}, randomly partition the observation in-
dices into L distinct groups I(cid:96), (cid:96) = 1, ..., L, such that the sample size of each group
is the largest integer smaller than n/L. For each (cid:96) = 1, ..., L, the estimators ˆγ(cid:96)(t, x) for
γ(t, x) ≡ E[Y |T = t, X = x] and ˆf(cid:96)(t|x) for fT |X(t|x) use observations not in I(cid:96) and satisfy
Assumption 3 below.

Step 2. (Double robustness) The double debiased ML (DML) estimator is deﬁned as

ˆβt ≡

1
n

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

(cid:40)

ˆγ(cid:96)(t, Xi) +

Kh(Ti − t)
ˆf(cid:96)(t|Xi)

(cid:41)

(Yi − ˆγ(cid:96)(t, Xi))

.

(4)

Step 3. (Partial eﬀect) Let t+ ≡ (t1 + η/2, t2, ..., tdt)(cid:48) and t− ≡ (t1 − η/2, t2, ..., tdt)(cid:48), where η is a
positive sequence converging to zero as n → ∞. We estimate the partial eﬀect of the ﬁrst
component of the continuous treatment θt ≡ ∂βt/∂t1 by ˆθt ≡ ( ˆβt+ − ˆβt−)/η.

The estimators ˆγ(cid:96) and ˆf(cid:96) in Step 1 satisfy Assumption 3 below. We deﬁne the partial L2(tX) norm
X (ˆγ(cid:96)(t, x) − γ(t, x))2fT X(t, x)dx(cid:1)1/2
for any t ∈ T as (cid:107)ˆγ(cid:96) − γ(cid:107)FtX ≡ (cid:107)ˆγ(cid:96)(T, X) − γ(T, X)(cid:107)FtX ≡ (cid:0) (cid:82)
X ( ˆf(cid:96)(t|x)−fT |X(t|x))2fT X(t, x)dx(cid:1)1/2, where
and (cid:107) ˆf(cid:96) −fT |X(cid:107)FtX ≡ (cid:107) ˆf(cid:96)(T |X)−fT |X(T |X)(cid:107)FtX ≡ (cid:0) (cid:82)
the joint distribution FT X(t, X) is evaluated at a ﬁxed value of T equal to t.

Assumption 3 For each (cid:96) = 1, ..., L and for any t ∈ T ,

(i) (cid:13)
(cid:13)ˆγ(cid:96) − γ(cid:13)
√
nhdt(cid:13)

(cid:13)FtX
(cid:13) ˆf(cid:96) − fT |X

(ii)

p

→ 0 and (cid:13)

p
→ 0.

(cid:13) ˆf(cid:96) − fT |X
(cid:13)ˆγ(cid:96) − γ(cid:13)
(cid:13)

(cid:13)FtX

(cid:13)
(cid:13)FtX
p
→ 0.

(cid:13)
(cid:13)FtX

In Section 3.1, we provide suﬃcient low-level conditions for Assumption 3 when the nuisance
estimators are kernel estimators, series, and the deep neural networks in Farrell et al. (2021b).
The partial L2 convergence rate also appears in in the conditional average treatment eﬀect in Fan
et al. (2021) and covariate adjustments in regression discontinuity designs in Noack et al. (2021).
When there is no sample splitting (L = 1), ˆγ1 and ˆf1 use all observations in the full sample.
Then the DML estimator ˆβt in (4) is the doubly robust estimator considered in Kallus and Zhou
(2018) and Su et al. (2019). The number of folds in cross-ﬁtting L is not random and typically

7

small, such as ﬁve or ten in practice; see, e.g., Section 5.1 or CCDDHNR. The numerical diﬀeren-
tiation estimator ˆθt is simple and avoids estimating the derivatives of the nuisance parameters.

Our results are readily extended to include binary/multivalued treatments D at the cost of
notational complication, e.g., the low-dimensional setting in Cattaneo (2010). Speciﬁcally, the fre-
(cid:8)ˆγl(t, d, Xi)+
quency method replaces the kernel with an indicator function: ˆβtd = n−1 (cid:80)L
(t, d|Xi)(cid:9), where γ(t, d, Xi) = E[Y |T = t, D = d, X =
1{Di = d}Kh(Ti −t)(cid:0)Yi − ˆγl(t, d, Xi)(cid:1)/ ˆfT D|X l
Xi] and fT D|X(t, d|Xi) = fT |DX(t|d, Xi)Pr(D = d|X = Xi).7

(cid:80)

i∈I(cid:96)

(cid:96)=1

Remark 1 (Common support) Assumption 1(ii) implies that we need to observe suﬃcient
individuals in the population who can ﬁnd a match sharing the same value of the control variable X
and receiving the counterfactual value t. An analogous assumption in the binary treatment case
is that the propensity score is bounded away from zero, e.g., Hirano et al. (2003). Although the
common support assumption is standard, we note that it should be made with care in practice
and is strong in high-dimensional settings. For the binary treatment case, Khan and Tamer (2010)
study extensively irregular identiﬁcation and inverse weight estimation, when the propensity score
can be close to zero as a small denominator. For the continuous treatment case, the convergence
rate of ˆβt might similarly be aﬀected if the generalized propensity score can be close to zero. We
believe that this interesting extension is beyond the scope of the paper and is worthy of a separate
research project. See also Su et al. (2019) for a related discussion.

Another possible approach to relaxing Assumption 1(ii) is to deﬁne the object of interest by
a common support via ﬁxed trimming, e.g., Lee (2018). Without imposing this common support
assumption, we instead focus on the causal object deﬁned by a common support for the subpopu-
lation whose control variable takes values in a common support X ∗ ≡ {x : inf t∈T ∗ fT |X(t|x) ≥ c} ⊆
∩t∈T ∗Supp(X|T = t), which is a subset of the intersection of the supports of X conditional on T = t
for t ∈ T ∗ ⊂ T for a positive constant c. Intuitively it is reasonable to focus on the subpopulation
who has nontrivial chance to receive the counterfactual value t. More speciﬁcally, deﬁne the trim-
ming function π(x) ≡ 1{inf t∈T ∗ fT |X(t|x) ≥ c} to select the common support X ∗. Then deﬁne the
causal object of interest by (cid:82)
E[Y (t)|X = x]π(x)fX(x)dx = (cid:82)
E[Y |T = t, X = x]π(x)fX(x)dx.
When X ∗ or π(x) are known, it is straightforward to include π(x) in all our results at the cost
of notational complication. When π(x) is estimated, additional conditions on the preliminary
estimator of fT |X are required for the estimation error to be asymptotically negligible.

X

X

Remark 2 (Kernel localization) We discuss the construction of the doubly robust moment

7There is a literature on the kernel smoothing of discrete (categorical) variables (Aitchison and Aitken (1976),
Ouyang et al. (2009) and reference therein); such extension to smoothing discrete treatments is out of the scope of
this paper.

8

function (1) by Gateaux derivative and a local Riesz representer; details are in Section D in
the Appendix. From the literature on estimating regular parameters, the Gateaux derivative is
fundamental to construct estimators with desired properties, such as bias reduction and double
robustness (Carone et al., 2018; Ichimura and Newey, 2022). The partial mean βt is a marginal
integration over the conditional distribution of Y given (T, X) and the marginal distribution of X,
ﬁxing the value of T at t. As a result, the Gateaux derivative and the Riesz representer depend
on the choice of the distribution f h
T that belongs to a family of distributions approaching a point
mass at T as h → 0. We construct the locally robust estimator based on the inﬂuence function
derived by the Gateaux derivative, so the asymptotic distribution of ˆβt depends on the choice of
f h
T that is the kernel function Kh(T − t). Moreover, to construct the DML estimator of a linear
functional of βt that preserves the good properties, the corresponding moment function is simply
the linear functional of the moment function of βt. To the best of our knowledge, this is the ﬁrst
explicit calculation of Gateaux derivative for such a non-regular nonparametric parameter.

A second motivation of the moment function is adding to the inﬂuence function of the regression
(or imputation) estimator n−1 (cid:80)n
i=1 ˆγ(t, Xi) the adjustment term from a kernel-based estimator
ˆγ under the low-dimensional case when the dimension of Xi is ﬁxed. A series estimator ˆγ yields
a diﬀerent adjustment. These distinct features of continuous treatments are in contrast to the
regular binary treatment case, where diﬀerent nonparametric nuisance estimators ˆγ result in the
same eﬃcient inﬂuence function.

2.1 Conditional density estimation

We propose an estimator for the reciprocal of the generalized propensity score (GPS), i.e., 1/fT |X(t|x),
when dt = 1. The estimator avoids plugging in a small estimate in the denominator, and the esti-
mate is positive by construction. When dt > 1, we propose an estimator for the GPS. We can use
various nonparametric and ML methods designed for the conditional mean. We provide a root-
mean-squared convergence rate.
In Section 3.1, we demonstrate these generic GPS estimators
using the deep neural networks in Farrell et al. (2021b) and how Assumption 3 can be veriﬁed.

In essence, our approach estimates the ratio Kh(Ti − t)/fT |X(t|Xi) (or the Riesz representer
in Section D) with ﬂexible nonparametric and ML methods. The theory of ML methods in
estimating the conditional density is less developed compared with estimating the conditional
mean. Alternative estimators for estimating the GPS can be the kernel density estimator, the
artiﬁcial neural networks in Chen and White (1999), or the Lasso methods in Su et al. (2019) and
Belloni et al. (2019).

It is known that for any CDF F ,

d

du F −1(u) =

1
F (cid:48)(F −1(u))

for u ∈ (0, 1). So

1

fT |X (t|x) =

9

T |X(u|x)(cid:12)

∂

∂u F −1
ical diﬀerentiation estimator, labelled as ReGPS,

(cid:12)u=FT |X (t|x). Inspired by the idea in Koenker (1994),8 we estimate

1

fT |X (t|x) by a numer-

(cid:92)1
fT |X(t|x)

=

ˆF −1
T |X

(cid:0) ˆFT |X(t|x) + (cid:15)(cid:12)

(cid:12)x(cid:1) − ˆF −1
2(cid:15)

T |X

(cid:0) ˆFT |X(t|x) − (cid:15)(cid:12)

(cid:12)x(cid:1)

,

(ReGPS)

where (cid:15) = (cid:15)n is a positive sequence vanishing as n grows and ˆFT |X(t|x) ± (cid:15) ∈ (0, 1). Let ˆµ(W ; x)
be an estimator of the conditional mean E [W |X = x] for a random variable W . The conditional
CDF is estimated by ˆFT |X(t|x) = ˆµ
, where Φ is the CDF of a standard normal
; x
random variable and h1 = h1n is a bandwidth sequence vanishing as n grows. The conditional u-
T |X(u|x) is estimated by the generalized inverse function ˆF −1
quantile function F −1
T |X(u|x) = inf t∈T {t :
ˆFT |X(t|x) ≥ u}. When ˆFT |X(t|x) is continuous in t, ˆF −1
T |X(u|x) is strictly increasing. Then the
resulting estimator (cid:92)1/fT |X(t|x) > 0.

(cid:16) t−T
h1

Φ

(cid:17)

(cid:17)

(cid:16)

Denote the standard root-mean-squared norm, or the L2(X) norm, of a random vector X
≡ (cid:0)(cid:82)
for

X (ˆµ(W ; x) − E[W |X = x])2 fX(x)dx(cid:1)1/2

with distribution FX as (cid:107)ˆµ(W ; X) − E[W |X](cid:107)FX
a random variable W .

Lemma 1 (ReGPS) Let ˆµ
(cid:16) t−T
h1

(cid:13)
(cid:13)
(cid:13)ˆµ

Φ

(cid:16)

(cid:16)

(cid:16) t−T
h1
(cid:17)

Φ
(cid:17)

(cid:17)

(cid:17)

; x
(cid:104)

− E

be an estimator of E
(cid:105)(cid:13)
(cid:16) t−T
(cid:13)
(cid:13)FX
h1

(cid:17) (cid:12)

Φ

; X

= Op(R1n) for a sequence of constants
t ∈ T , and supt∈T
R1n. Assume FT |X to be three-times diﬀerentiable with respect to t with all three derivatives being
bounded uniformly over (t, x) ∈ T × X . Let T0 = [T , T ] ⊂ R. Then the ReGPS estimator (cid:92)1/fT |X
= Op(R1n(cid:15)−1 + h2
=
satisﬁes
Op(R1n(cid:15)−1 + h2

(cid:13)
(cid:13)
(cid:92)1/fT |X(t|X) − 1/fT |X(t|X)
(cid:13)
(cid:13)
(cid:13)FX
(cid:13)
1(cid:15)−1 + (cid:15)2) for t ∈ T .

(cid:13)
(cid:92)1/fT |X − 1/fT |X
(cid:13)
(cid:13)

1(cid:15)−1 + (cid:15)2) and

(cid:13)
(cid:13)
(cid:13)FtX

(cid:12)X

(cid:104)
Φ

(cid:16) t−T
h1

(cid:17) (cid:12)

(cid:12)X = x

(cid:105)
, continuous in

Assumption 3 speciﬁes the root-mean-squared convergence rate of our conditional density esti-
mator. Thus as long as the root-mean-squared convergence rate of a ML method (R1n) is available,
Assumption 3 can be satisﬁed with a suitable bandwidth h1 and (cid:15). Then we are able to use such
a ML method to estimate the conditional density, as illustrated in Section 3.1.

Note that ˆF −1

T |X(u|x) estimates the conditional quantile function. The ReGPS estimator in-
verses the CDF estimate and allows to apply various ML methods for conditional mean functions.
Alternatively we can estimate the conditional quantile function directly by a (cid:96)1-penalized quantile
regression; for example, the conditional density function estimation in Belloni et al. (2019).

The above ReGPS estimator works for dt = 1. When dt > 1, we propose a direct estimator for

8In the context of regression quantiles, the asymptotic variance involves the reciprocal of a density function

evaluated at the quantile of interest that is also known as the sparsity function or the quantile-density function.

10

the conditional density function fT |X(t|x) by

ˆfT |X(t|x) = h−dt

1 ˆµ (cid:0)hdt

1 gh1(T − t); x(cid:1) ,

(MultiGPS)

labelled as MultiGPS, where the bandwidth h1 is a positive sequence vanishing as n grows, the
product kernel gh1(Ti − t) ≡ Πdt
1 . We can choose g() to be the Gaussian
kernel.9

j=1g((Tji − tj)/h1)/hdt

1 gh1(T − t)|X](cid:13)

1 gh1(T − t); x) be an estimator of E[hdt

Lemma 2 (MultiGPS) Let ˆµ(hdt
1 gh1(T − t)|X = x] and
(cid:13)
1 gh1(T − t); X(cid:1) − E[hdt
(cid:13)ˆµ (cid:0)hdt
= Op(R1n) for a sequence of constants R1n and
t ∈ T . Let g() satisfy Assumption 2 with g() in place of k() and with an unbounded sup-
port. Assume fT |X(t|x) to be three-times diﬀerentiable with respect to t with all three deriva-
tives being bounded uniformly over (t, x) ∈ T × X . Then the MultiGPS estimator ˆfT |X satisﬁes
(cid:13)
(cid:13)
ˆfT |X(t|X) − fT |X(t|X)
(cid:13)
(cid:13)
1) for
(cid:13)FX
(cid:13)
t ∈ T .

(cid:13)
ˆfT |X − fT |X
(cid:13)
(cid:13)

= Op(R1nh−dt

= Op(R1nh−dt

1 + h2

1 + h2

(cid:13)
(cid:13)
(cid:13)FtX

1) and

(cid:13)FX

3 Asymptotic theory

We ﬁrst derive the asymptotically linear representation and asymptotic normality. We provide
low-level conditions for various nuisance estimators in Section 3.1. We discuss uniform asymptotic
theory over t in Section 3.2.

Theorem 1 (Asymptotic normality) Let Assumptions 1-3 hold. Let h → 0, nhdt → ∞, and
nhdt+4 → C ∈ [0, ∞). Then for any t ∈ T ,

√

nhdt

(cid:16) ˆβt − βt

(cid:17)

=

(cid:114)

hdt
n

n
(cid:88)

i=1

(cid:26) Kh(Ti − t)
fT |X(t|Xi)

(Yi − E[Y |T = t, X = Xi])

+ E[Y |T = t, X = Xi] − βt

(cid:27)

+ op(1).

(5)

Furthermore let E(cid:2)|Y − γ(T, X)|3(cid:12)
formly over (t, x) ∈ T × X . Let (cid:82) ∞
(cid:104) 1
where Bt ≡ (cid:80)dt
∂2
E [Y |T = t, X] + ∂
E
∂t2
∂tj
2
j
and Vt ≡ E(cid:2)var(Y |T = t, X)/fT |X(t|X)(cid:3)Rdt
k .

j=1

(cid:12)T = t, X(cid:3) and its derivatives with respect to t be bounded uni-
(cid:17) d−→ N (0, Vt),
−∞ k(u)3du < ∞. Then
(cid:105) (cid:82) ∞
−∞ u2k(v)dv

(cid:16) ˆβt − βt − h2Bt
fT |X(t|X)/fT |X(t|X)

E [Y |T = t, X] ∂
∂tj

nhdt

√

9A possible drawback of MultiGPS is that the estimate could be negative or small in ﬁnite samples. We may
adopt the trimming/ﬂooring approaches to addressing this concern in the literature. For example, following Hsu
et al. (2020), we can use the estimate max{ ˆfT |X (t|Xi), δn} for some positive sequence δn → 0.

11

√

√

n) = op(1/

Note that the second part in the inﬂuence function10 in (5) n−1 (cid:80)n

E[Y |T = t, X = Xi]−βt =
nhdt) and hence does not contribute to the ﬁrst-order asymptotic variance Vt.
Op(1/
We keep these smaller-order terms to show that the nuisance estimators have no ﬁrst-order inﬂu-
ence on the asymptotic distribution of ˆβt. This is in contrast to the binary treatment case where
Kh(Ti − t) is replaced by 1{Ti − t} in ˆβt, so ˆβt converges at a root-n rate. Then the second part in
(5) is of ﬁrst-order for a binary treatment, resulting in the well-studied eﬃcient inﬂuence function
in estimating the binary treatment eﬀect in Hahn (1998).

i=1

Theorem 1 is fundamental for inference, such as constructing conﬁdence intervals and the
optimal bandwidth h that minimizes the asymptotic mean squared error. To estimate the leading
bias Bt, let the notation ˆβt = ˆβt,b be explicit on the bandwidth b. Inspired by the idea in Powell
and Stoker (1996), we propose an estimator for Bt:

ˆBt ≡

ˆβt,b − ˆβt,ab
b2(1 − a2)

with a pre-speciﬁed ﬁxed scaling parameter a ∈ (0, 1). Theorem 2 below shows the consistency of
ˆBt under Assumption 4(iv).

(cid:96)=1

i∈I(cid:96)

(cid:80)

We can estimate the asymptotic variance Vt by the sample variance of the estimated inﬂuence
function ˆVt ≡ hdtn−1 (cid:80)L
i(cid:96), where ˆψi(cid:96) ≡ Kh(Ti −t)(Yi − ˆγ(cid:96)(t, Xi))/ ˆf(cid:96)(t|Xi)+ ˆγ(cid:96)(t, Xi)− ˆβt.
ˆψ2
Assumption 4(i)-(iii) are for the consistency of ˆVt. The condition (i) strengthens Assumption 3(i),
and (ii) is mild bounded conditions that are implied when ˆγ(cid:96) and ˆf(cid:96) are bounded uniformly. These
conditions are satisﬁed if the Step 1 estimators ˆγ(cid:96)(t, x) and ˆf(cid:96)(t|x) are uniformly consistent. In
practice, we may use diﬀerent Step 1 estimators in ˆVt and ˆβt due to diﬀerent high-level conditions.
(cid:1)(cid:1)1/(dt+4)n−1/(dt+4) to consistently
estimate the optimal bandwidth that minimizes the asymptotic mean squared error (AMSE) given
in Theorem 2.

Then we propose a data-driven bandwidth ˆht ≡ (cid:0)dt

(cid:14)(cid:0)4ˆB2

ˆVt

t

Assumption 4 For each (cid:96) = 1, ..., L and for any t ∈ T , (i) (cid:107)(ˆγ(cid:96) − γ)( ˆf(cid:96) − f )(cid:107)FtX = op(1),
(ii) (cid:107)(ˆγ(cid:96) − γ)2( ˆf(cid:96) − fT |X)2(cid:107)FtX = Op(1), (cid:107)(ˆγ(cid:96) − γ)2(cid:107)FtX = Op(1), and (cid:107)( ˆf(cid:96) − fT |X)2(cid:107)FtX = Op(1).
(iii) E[(Y − γ(t, x))4|T = t, X = x] and its derivatives with respect to t are bounded uniformly
over (t, x) ∈ T × X . (cid:82) ∞
−∞ k(u)4du < ∞. (iv) b → 0 and nbdt+4 → ∞. (cid:82) ∞
−∞ k(u)k(u/a)du < ∞ for
a ∈ (0, 1).

Theorem 2 (AMSE optimal bandwidth) Let the conditions in Theorem 1 hold. For t ∈ T ,

√

10For our non-regular parameters, we borrow the terminology “inﬂuence function” in estimating a regular pa-
n-estimable. An inﬂuence function gives the ﬁrst-order asymptotic eﬀect of a single observation
rameter that is
on the estimator. The estimator is asymptotically equivalent to a sample average of the inﬂuence function. See
Hampel (1974) and Ichimura and Newey (2022), for example.

12

if Bt is non-zero, then the bandwidth that minimizes the asymptotic mean squared error is h∗
(cid:0)dtVt
(cid:14)(cid:0)4B2
for Vt, Bt, and h∗

t =
(cid:1)(cid:1)1/(dt+4)n−1/(dt+4). Further let Assumption 4 hold. Then ˆVt, ˆBt, and ˆht are consistent

t respectively.

t

A common approach is to choose an undersmoothing bandwidth h smaller than h∗

the bias is ﬁrst-order asymptotically negligible, i.e., h2

t such that
nhdt → 0. Then we can construct the

√
(cid:104) ˆβt ± Φ−1(1 − α/2)

(cid:113)

(cid:105)
ˆVt/(nhdt)

, where Φ
usual (1 − α) × 100% point-wise conﬁdence interval
is the CDF of N (0, 1). Alternatively, we may consider a further bias correction to allow for a
wider range of bandwidth choice so that we may implement ˆht in practice. Speciﬁcally we may
use the above bias estimator ˆBt and account for its variation in the asymptotic theory of the bias-
corrected estimator ˆβt − h2 ˆBt. Calonico et al. (2018) show that the AMSE optimal bandwidth
of the original estimator is feasible in diﬀerent contexts. Such robust bias-corrected inference is
reserved for future research.

Next we present the asymptotic theory for ˆθt. We consider two conditions for the tuning
t g(t, ·) ≡ ∂νg(t, ·)/∂tν denote the

parameter η via η/h → ρ for (i) ρ = 0 and (ii) ρ ∈ (0, ∞]. Let ∂ν
νth partial derivative of a generic function g with respect to t, and ∂t ≡ ∂1
t .

Theorem 3 (Partial eﬀect) Let the conditions in Theorem 1 hold. Assume that fZ(y, t, x) is
four-times diﬀerentiable with respect to t with all four derivatives being bounded uniformly over
(y, t(cid:48), x(cid:48))(cid:48) ∈ Z.

(i) Let η/h → 0, nhdt+2 → ∞, nhdt+2η2 → 0, and nhdt+6 → C ∈ [0, ∞). Assume that for each
(cid:13)ˆγ(cid:96) − γ(cid:13)
p
→ 0;
p
→ 0. Let T0 = [T , T ]dt ⊂ Rdt. Let the kernel

(cid:96) = 1, ..., L and for any t ∈ T , (a) η−1h(cid:13)
(cid:13) ˆf(cid:96) − fT |X
(b) η−1h
(cid:13)FtX
function k have bounded support and (cid:82) ∞
−∞ k(cid:48)(cid:48)(cid:48)(u)2du < ∞. Then for any t ∈ T ,

→ 0 and η−1h(cid:13)

(cid:13) ˆf(cid:96) − fT |X

(cid:13)ˆγ(cid:96) − γ(cid:13)
(cid:13)

nhdt(cid:13)

(cid:13)
(cid:13)FtX

(cid:13)
(cid:13)FtX

(cid:13)FtX

√

p

√

nhdt+2(ˆθt − θt) =

(cid:114)

hdt+2
n

n
(cid:88)

i=1

∂
∂t1

Kh(Ti − t)

Yi − γ(t, Xi)
fT |X(t|Xi)

+ op(1)

(6)

√

nhdt+2(ˆθt − θt − h2Bθ
t )

t ), where Bθ
and
∂t1γ(t, X)fT |X(t|X)/2 +
∂tj ∂t1γ(t, X)∂tj fT |X(t|X)+∂tj γ(t, X)(cid:0)∂tj ∂t1fT |X(t|X)−∂tj fT |X(t|X)∂t1fT |X(t|X)fT |X(t|X)−1(cid:1)(cid:17)
fT |X(t|X)−1(cid:105) (cid:82) u2k(u)du and Vθ

t = E (cid:2)var(Y |T = t, X)/fT |X(t|X)(cid:3) (cid:82) k(cid:48)(u)2duRdt−1

d−→ N (0, Vθ

t = (cid:80)dt

∂2
tj

j=1

E

.

k

(cid:104)(cid:16)

(ii) Let η/h → ρ ∈ (0, ∞], nhdtη2 → ∞, and nhdtη4 → 0. Then for any t ∈ T , (cid:112)nhdtη2(ˆθt −
with the

t = 2E(cid:2)var(Y |T = t, X)/fT |X(t|X)(cid:3)(cid:0)Rk − ¯k(ρ)(cid:1)Rdt−1

d−→ N (0, Vθ

t ), where Vθ

θt)

k

13

convolution kernel ¯k(ρ) ≡ (cid:82) ∞

−∞ k(u)k(u − ρ)du.11

Theorem 3(i) is for the case when η is chosen to be of smaller order than h. The conditions
(a) and (b) strengthen Assumption 3 for βt and imply that η cannot be too small and depends on
the precision of the nuisance estimators. In Theorem 3(ii) when η/h → ∞, ¯k (ρ) = 0 and hence
Vθ
t = 2Vt. This is in line with the special case of a ﬁxed η implied by the result in Theorem 1.

3.1 Nuisance estimators

We illustrate that the high-level conditions on the convergence rates in Assumption 3 are attainable
by the nonparametric and ML methods: kernel, series, and the deep neural networks in Farrell
et al. (2021b). Lasso methods have been extensively studied in Su et al. (2019), Sasaki and Ura
(2021), and Sasaki et al. (2021). These ML methods require diﬀerent low-level conditions, such as
dimensionality and smoothness.

Consider the conditional density estimator MultiGPS ˆfT |X given in Section 2.1 for example.

By Lemma 2, Assumption 3(ii) requires

√

nhdt(cid:107) ˆf − fT |X(cid:107)FtX (cid:107)ˆγ − γ(cid:107)FtX =

√

nhdt (cid:0)R1nh−dt

1 + h2
1

(cid:1) (cid:107)ˆγ − γ(cid:107)FtX

p
→ 0.

Therefore, we need to obtain the partial L2(tX) convergence rate (cid:107)ˆγ − γ(cid:107)FtX and the standard
L2(X) convergence rate R1n.

(cid:13)ˆµ (cid:0)hdt

We seek theoretical results, such as the above rate conditions, for insights on selection of
the tuning parameters in practice that is challenging and under-developed in the ML litera-
ture. We may use the optimal choices for the nuisance estimators, as they do not aﬀect the
ﬁrst-order asymptotics. A common method is cross-validation. We may choose the optimal
rates for the bandwidths for ˆγ and ˆµ(hdt
1 gh1(T − t); x) that respectively minimize (cid:107)ˆγ − γ(cid:107)FtX
and (cid:13)
1 gh1(T − t); X(cid:1) − E[hdt
, which might be available in the literature.
Similarly we can derive the optimal h∗

1 gh1(T − t)|X](cid:13)
(cid:13)FX
1 ∝ R1/(2+dt)
.
We illustrate the low-level conditions of conventional kernel estimators in Section E.1 in the
Appendix. We derive the L2(tX) convergence rate for series in Section E.2 in the Appendix.
Next we propose a deep MLP-ReLU network kernel estimator for γ(t, x) and derive its L2(tX)
convergence rate. These results on series and neural networks are new and non-trivial extensions
of existing results in the literature.

1n

11The leading bias of ˆθt is h2∂Bt/∂t1, with Bt given in Theorem 1. The conditions here implied that (cid:112)nhdtη2h2 →

0, so the bias is asymptotically ignorable.

14

We consider the deep neural networks in Farrell et al. (2021b) (FLM, hereafter) that use the
fully connected feedforward neural networks (multilayer perceptron, or MLP) and the nonsmooth
rectiﬁed linear units (ReLU) activation function. We propose a deep MLP-ReLU network kernel
estimator for γ(t, x). The proposed estimator serves the purpose to conveniently apply the L2(T X)
convergence rate given in FLM to obtain the L2(tX) convergence rate. So we can deliver valid
asymptotic inference for βt and θt following deep learning. In this section, we closely follow the
notations in FLM for easy reference, by slightly abusing our notations.

We consider a kernel-weighted loss function for any t ∈ T ,

(cid:96)tb(f, Z) ≡

1
2

(Y − f (X))2 Kb (T − t) ,

where a product kernel Kb(T − t) ≡ Πdt
j=1k((Tj − tj)/b)/bdt with a kernel function k and a positive
sequence of bandwidth b = bn vanishing as n grows. We deﬁne the deep MLP-ReLU network
kernel estimator for any t ∈ T as

ˆftb ∈ arg

min
fθ∈FM LP ,(cid:107)fθ(cid:107)∞≤2M

n
(cid:88)

i=1

(cid:96)tb(fθ, Zi),

(7)

where FM LP is the MLP class, M is an absolute constant, and θ depending on t collects the
weights and constants over all nodes. We refer the details of the MLP-ReLU network estimators
to FLM. Then we obtain ˆγ(t, x) = ˆftb(x).

Denote the derivative of a function f (x) as Dα

xf (x) = ∂|α|f (x)
αdx
1 ···∂x
dx

∂xα1

, where α = (α1, ..., αdx) and

|α| = α1 + ... + αdx.

Assumption 5 (DNN) For any t ∈ T ,

(i) The second derivatives of fT |X(t, x) and γ(t, x) with respect to t are bounded and continuous

uniformly over x ∈ X .

(ii) X are continuously distributed with support X = [−1, 1]dx for ﬁxed dx. For an absolute

constant M > 0, Y ⊂ [−M, M ].

(iii) For β ∈ N+, maxα,|α|≤β ess supy∈Y,t∈T ,x∈X , |Dα

xfY T |X(y|t, x)| ≤ c for some ﬁnite positive con-

stant c.

(iv) (cid:107)fT |X(t|·)(cid:107)∞ ≤ c for some ﬁnite positive constant c.

15

Assumption 5(i) is due to the kernel weight Kb(T −t) in the loss function. Assumption 5(ii)-(iv)
collect assumptions for applying Theorem 1 in FLM. Detailed discussion on these assumptions is
referred to FLM. To simplify exposition, X is assumed to be continuous, but discrete variables
with a ﬁxed number of supporting points are allowed. Theorem 1 in Farrell et al. (2021a) ﬁnd
that the rate only depends on the dimension of the continuously distributed components. As
discussed in FLM, it is standard in nonparametric analysis to assume the true function to be
estimated is bounded. The choice of M may be arbitrarily large and is simply a formalization of
the requirement that the optimizer is not allowed to diverge on the function level in the sup-norm
sense. For practical implementation, we do not impose such bound. But there is a practice for
rescaling the output variable which is generally dependent on the activation function being used,
e.g., the domain of the activation function. A common practice for variable transformation is
standardization (subtracting mean and dividing by standard deviation) or scaling to a speciﬁc
range (generally chosen between 0 and 1).

The smoothness condition (iii) implies that for any t ∈ T , γ(t, ·) lies in the H¨older ball
W β,∞([−1, 1]dx) ≡ (cid:8)f : maxα,|α|≤β ess supx∈[−1,1]dx |Dαf (x)| ≤ 1(cid:9), where Dαf is the weak deriva-
tive (Yarotsky, 2017), as in Assumption 1 in FLM. We use the standard strong derivative for
theoretical analysis due to the kernel localization Kb(T − t).

Theorem 4 (DNN) Let ˆftb be the deep MLP-ReLU network kernel estimator deﬁned by (7). Let
Assumption 5 hold. Let k() satisfy Assumption 2 with k() in place of k() and with a bounded
2(β+dx) log2(nb2dt) and depth L (cid:16) log(nb2dt). Then for any t ∈ T ,
support. Let width H (cid:16) (nb2dt)
(cid:110)(cid:0)nb2dt(cid:1)− β
(cid:107) ˆftb − γ(cid:107)2
≤ C ·
with probability approaching one
as n → ∞, for a constant C > 0 independent of n, which may depend on dx, M , and other ﬁxed
constants.

β+dx log8 n + log log n/ (cid:0)nbdt(cid:1) + b2(cid:111)

FtX

dx

We can apply deep neural networks to the GPS estimation proposed in Section 2.1. The
ReGPS estimator can use ˆµ (Φ((t − T )/h1); x) = ˆfM LP −ReGP S(x) the MLP estimator in FLM with
the unweighted loss function:

ˆfM LP −ReGP S ∈ arg

min
fθ∈FM LP ,(cid:107)fθ(cid:107)∞≤2M

n
(cid:88)

i=1

(cid:0)Φ((t − Ti)/h1) − fθ(Xi)(cid:1)2.

The MultiGPS estimator can use ˆµ (cid:0)hdt

1 gh1(T − t); x(cid:1) = ˆfM LP −M ultiGP S(x):

ˆfM LP −M ultiGP S ∈ arg

min
fθ∈FM LP ,(cid:107)fθ(cid:107)∞≤2M

n
(cid:88)

i=1

(cid:0)hdt

1 gh1(Ti − t) − fθ(Xi)(cid:1)2.

(8)

(9)

16

Lemma 3 (Theorem 1 in Farrell et al. (2021b)) Let X be continuously distributed with sup-
2(β+dx) log2 n and depth L (cid:16) log n. Let
port X = [−1, 1]dx for ﬁxed dx. Let width H (cid:16) n
1n = n− β
R2

β+dx log8 n + log log n/n.

dx

(i) For ˆfM LP −ReGP S deﬁned by (8), assume (a) maxα,|α|≤β ess supt∈T ,x∈[−1,1]dx |Dα
for some ﬁnite positive constant c. Let T0 = [T , T ] ⊂ R. Then supt∈T
E[Φ((t − T )/h1)|X](cid:13)

= Op(R1n).

xFT |X(t|x)| ≤ c
(cid:13)
(cid:13) ˆfM LP −ReGP S(X) −

(cid:13)FX

(ii) For ˆfM LP −M ultiGP S deﬁned by (9), assume (b) maxα,|α|≤β ess supt∈T ,x∈[−1,1]dx |Dα

xfT |X(t|x)| ≤ c
for some ﬁnite positive constant c. Let g() satisfy Assumption 2 in place of k() and with an
(cid:13)
(cid:13) ˆfM LP −M ultiGP S(X) − E[hdt
= Op(R1n).
unbounded support. Then supt∈T

1 gh1(t − T )|X](cid:13)

(cid:13)FX

(cid:16)

(nb2)− β

β+dx log8 n + log log n/(nb) + b2(cid:17)(cid:16)

We are ready to show that Assumption 3 is attainable by the MLP-ReLU network estimators.
Take the MultiGPS estimator in (9) for example, and consider dt = 1 for simplicity. Assump-
β+dx log8 n + log log n/n(cid:1) + h4
tion 3(ii) is nh
→ 0
1
by Theorem 4, Lemmas 2 and 3. Assumption 3(i) is implied by nb2 → ∞ and nβ/(β+dx)h2
1 → ∞.
When h = h1 = b, Assumption 3(i) holds by letting nβ/(β+dx)h2 → ∞, and (ii) holds by letting
smoothness β > dx. FLM discuss the same condition β > dx for the average treatment eﬀect of a
binary treatment variable. Similarly we note that this condition is not minimal but to justify the
practical use of the MLP-ReLU network estimators for valid inference on the average structural
function and the partial eﬀect of continuous treatments by our approach.

(cid:0)n− β

h−2
1

(cid:17)

3.2 Uniform asymptotic theory

We extend the asymptotic theory to uniformity over t ∈ T which is a compact subset of the support
of T . The uniform asymptotic representation in Theorem 5 is the basis for a uniform inference
procedure for βt and θt. Assumption 6 strengthens Assumption 3 for the nuisance estimators.

Assumption 6 For each (cid:96) = 1, ..., L,

√

(i) supt∈T

nhdt(cid:13)

(cid:13) ˆf(cid:96) − fT |X

(cid:13)
(cid:13)FtX

(cid:13)ˆγ(cid:96) − γ(cid:13)
(cid:13)

(cid:13)FtX

p
→ 0.

(ii) There exist positive sequences A1n → 0 and A2n → 0 such that sup(t,x)∈T ×X |ˆγ(cid:96)(t, x) −

γ(t, x)| = Op(A1n) and sup(t,x)∈T ×X | ˆf(cid:96)(t|x) − fT |X(t|x)| = Op(A2n).

(iii) ˆγl(t, x) and ˆfl(t|x) are Lipschitz continuous in t ∈ T for any x ∈ X with the Lipschitz

constant independent of x.

17

Assumptions 6(i) is the uniform analog of Assumptions 3(ii). In addition, Assumption 6(ii) requires
the nuisance estimators to converge uniformly at some rates. Fan et al. (2021) make similar
assumptions for the conditional average binary treatment eﬀect. As discussed in Section 3.1, for
a speciﬁc nuisance estimator to satisfy Assumption 6 we may require additional conditions, such
as compact X . We may verify Assumption 6 for the kernel and series estimators by extending
our results in Sections E.1 and E.2 in the Appendix, respectively, and using the existing results in
the literature (e.g., Newey (1994b) and Cattaneo et al. (2020)). Verifying Assumption 6 for the
modern ML methods, such as the deep neural networks in Farrell et al. (2021b), is more involved
and beyond the scope of this paper.

Theorem 5 Let the conditions in Theorem 1 and Assumption 6 hold. Then (i) the asymptotically
linear representation of ˆβt in (5) holds uniformly in t ∈ T . (ii) Furthermore let the conditions
in Theorem 3 hold. Then the asymptotically linear representation of ˆθt in (6) holds uniformly in
t ∈ T .

We consider a multiplier bootstrap method for uniform inference on βt and θt over t ∈ T . The
method and proof closely follow Su et al. (2019) and Theorem 4.1 in Fan et al. (2021), where the
moment functions share a similar structure with that of ˆβt. Let {ξi}n
i=1 be a sequence of i.i.d.
random variables satisfying Assumption 7.

Assumption 7 (Multiplier bootstrap) The random variable ξi is independent of the sample
i=1, E∗[ξi] = var∗(ξi) = 1, and its distribution P ∗ has sub-exponential tails, i.e.,
{(Yi, Ti, Xi)}n
P ∗(|ξi| > u) ≤ c1 exp(−c2u) for every u and some constants c1 and c2.

Assumption 7 is standard for multiplier bootstrap inference and is satisﬁed by a normal random
variable, for example. Then we compute

ˆβ∗
t =

1
n

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

(cid:40)

ξi

ˆγ(cid:96)(t, Xi) +

Kh(Ti − t)
ˆf(cid:96)(t|Xi)

(cid:41)

(Yi − ˆγ(cid:96)(t, Xi))

.

√

√

nhdt( ˆβ∗

t − ˆβt) to simulate the limiting process of

We use
is, repeat the above procedure for B times and obtain a bootstrap sample of { ˆβ∗
For the partial eﬀect, compute the numerical diﬀerentiation estimator ˆθ∗
t following Step 3 in
√
nhdt+2(ˆθt − θt) by
t . Then we simulate the limiting process of
t − ˆθt). Theorem 6 below presents the bootstrap version of Theorems 1 and 6 and forms

the estimation procedure with ˆβ∗
√

nhdt( ˆβt − βt) indexed by t ∈ T . That

nhdt+2(ˆθ∗

t,b}B

b=1.

the basis of inference.

Theorem 6 (Multiplier bootstrap) Let the conditions in Theorem 5 and Assumption 7 hold.

18

√

nhdt(cid:0) ˆβ∗
√

t − βt
nhdt+2(ˆθ∗

Then
op(cid:63)(1) and
uniformly in t ∈ T , where p(cid:63) is for the joint distribution of the full sample {(Yi, Ti, Xi)}n
{ξi}n

(cid:9) +
(cid:0)Yi − γ(t, Xi)(cid:1)/fT |X(t|Xi) + op(cid:63)(1)
i=1 and

(cid:8)Kh(Ti − t)(Yi − γ(t, Xi)/fT |X(t|Xi) + γ(t, Xi) − βt

(cid:1) = (cid:112)hdt/n (cid:80)n
t − θt) = (cid:112)hdt+2/n (cid:80)n

i=1 ξi∂Kh(Ti − t)/∂t1

i=1 ξi

i=1.

√

√

√

√

nhdt(cid:0) ˆβ∗
t − ˆβt
nhdt(cid:0) ˆβt − βt

Next we discuss inference using the multiplier bootstrap in Su et al. (2019) and Fan et al.
(cid:1) and
t − ˆθt) converge in distribution
nhdt+2(ˆθ∗
(2021). Theorem 6 implies that
(cid:1) and
nhdt+2(ˆθt − θt), respectively, conditional
to the limiting distribution of
on the sample path with probability approaching one (in the sense of Section 2.9 in van der
Vaart and Wellner (1996)). Following Su et al. (2019), obtain ˆqt(α) as the αth quantile of the
sequence (cid:8) ˆβ∗
b=1. The standard 100(1 − α)% percentile bootstrap conﬁdence interval for βt
is (cid:0) ˆβt + ˆqt(α/2), ˆβt + ˆqt(1 − α/2)(cid:1) or (cid:0) ˆβt − ˆqt(α/2), ˆβt + ˆqt(α/2)(cid:1). As shown in Theorem 4.1 in Su
et al. (2019), these pointwise conﬁdence intervals for t ∈ T are valid under undersmoothing, i.e.,
using a bandwidth h smaller the AMSE optimal bandwidth given in Theorem 2.

t,b − ˆβ(cid:9)B

√

We can follow the approach in Fan et al. (2021) to construct uniform conﬁdence bands. Specif-
(cid:9)B
ically obtain ˆQ(α) as the αth quantile of the sequence (cid:8) supt∈T
b=1, where
ˆσ2
is a uniformly consistent estimator of Vt. The supremum is approximated by the maximum
t
over a chosen ﬁne grid over T . Then construct the 100(1 − α)% uniform conﬁdence band as
(cid:0) ˆβt − ˆQ(1 − α)ˆσt/
t = ˆVt the sample
variance estimator described in Section 3. Following the proof of Theorem 3.2 in Fan et al. (2021),
one could show supt∈T |ˆVt − Vt| = op(1). Based on Theorem 6, the asymptotic validity of the
conﬁdence band could follow the proof of Theorem 4.2 in Fan et al. (2021).12 We do not include
the formal theoretical details in this paper to conserve space and focus on the new results.

nh(cid:1). For example, we could use ˆσ2

nh, ˆβt + ˆQ(1 − α)ˆσt/

t,b − ˆβt

nhdt(cid:12)

(cid:12)
(cid:12)/ˆσt

(cid:12) ˆβ∗

√

√

4 Simulated DML estimator

We introduce Simulated DML estimator ˇβt that enables the high-level rate conditions based on the
standard L2(T X) norm, rather than the partial L2(tX) norm. Following the estimation procedure
given in Section 2, Step 1 computes the nuisance estimators ˆγ(cid:96) and ˆf(cid:96). In Step 2, let Ui ≡ Tih0 + t
where {Ti}i∈I(cid:96) are i.i.d. draws from {Ti}i∈I(cid:96) with replacement, for i ∈ I(cid:96), (cid:96) ∈ {1, ..., L}, and h0
is a positive sequence converging to zero as n → ∞. The Simulated DML (SDML) estimator is

12Speciﬁcally with more complicated notations, we will use the results in Chernozhukov et al. (2014b) that study
the problem of approximating suprema of empirical processes by a sequence of suprema of Gaussian processes.
Then we will use their companion paper Chernozhukov et al. (2014a) that provide multiplier bootstrap methods
for computation of Gaussian approximations.

19

deﬁned as

ˇβt ≡

1
n

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

(cid:40)

ˆγ(cid:96)(Ui, Xi) +

Kh(Ti − t)
ˆf(cid:96)(t|Xi)

(cid:41)

(Yi − ˆγ(cid:96)(Ui, Xi))

.

The corresponding partial eﬀect estimator ˇθt ≡ ( ˇβt+ − ˇβt−)/η as in Step 3.

To get intuition, the SDML estimator ˇβt uses ˆγ(cid:96)(Ui, Xi) rather than ˆγ(cid:96)(t, Xi) as in ˆβt that
ﬁxes the treatment value at the target t. The simulated Ui localizes the realized treatment values
around t.
Introducing such local variation enables the standard L2 rate of ˆγ. Speciﬁcally Ti
deﬁned above follows the empirical distribution function of {Ti}i∈I(cid:96), denoted as ˆFT (cid:96). Therefore Ui
follows a CDF conditional on the sample {Zi = (Yi, Xi, Ti)}n
i=1) = P (Tih0 + t ≤
u|{Zi}n

i=1, P (Ui ≤ u|{Zi}n

i=1) = ˆFT (cid:96)((u − t)/h0).

The high-level conditions on the nuisance estimators for γ(t, x) and fT |X(t|x) are in Assump-
and

X (ˆγ(cid:96)(t, x) − γ(t, x))2 fT X(t, x)dxdt(cid:1)1/2

(cid:82)

T

tion 8 below. Let the L2 norm be (cid:107)ˆγ(cid:96) − γ(cid:107)FT X ≡ (cid:0)(cid:82)
(cid:18)
(cid:17)2
(cid:82)
(cid:107) ˆf(cid:96)(t|·) − fT |X(t|·)(cid:107)FX ≡
X

(cid:16) ˆf(cid:96)(t|x) − fT |X(t|x)

fX(x)dx

for t ∈ T .

(cid:19)1/2

Assumption 8 For each (cid:96) = 1, ..., L and for t ∈ T ,

(i) h−dt/2
0
√

(cid:107)ˆγ(cid:96) − γ(cid:107)FT X = op(1) and (cid:107) ˆf(cid:96)(t|·) − fT |X(t|·)(cid:107)FX = op(1).

(ii)

nhdth−dt/2
0

(cid:107)ˆγ(cid:96) − γ(cid:107)FT X (cid:107) ˆf(cid:96)(t|·) − fT |X(t|·)(cid:107)FX = op(1).

(iii) (cid:107)(ˆγ(cid:96) − γ)2(cid:107)FT X = Op(1).

Theorem 7 (SDML-Asymptotic normality) Let Assumptions 1, 2, and 8 hold. Let ∂γ(t, x)/∂t
be bounded uniformly over (t, x) ∈ T × X . Let h → 0, nhdt → ∞, and nhdt+4 → C ∈ [0, ∞). Let
nhdth2h0 → 0. Then the results in Theorem 1 hold with ˇβt in place
h0 → 0, nhdt
of ˆβt.

0 → ∞, and

√

. It is interesting to note that when h0 = h, the rate condition (ii)

Assumption 8(i) strengthens the consistency condition in Assumption 3(i) with a penalty
n(cid:107)ˆγ(cid:96) − γ(cid:107)FT X (cid:107) ˆf(cid:96)(t|·) −
h−dt/2
0
fT |X(t|·)(cid:107)FX = op(1), which is the same rate condition for the regular semiparametric models in
CCDDHNR and Chernozhukov et al. (2022a), e.g., ˆfT |X(t|x) is replaced with the propensity score
P (T = t|X = x) for a discrete treatment. Assumption 8(iii) is a boundedness condition that is
implied if ˆγ is uniformly bounded; for example, deep neural networks in FLM and random forests
for a uniformly bounded Y . Speciﬁcally, FLM provide the corresponding L2(T X) convergence
rate (cid:107)ˆγ(cid:96) − γ(cid:107)2
with β deﬁned in Assumption 5(iii) and

β+dx+dt log8 n + log log n/n

= Op

n−

√

(cid:16)

(cid:17)

β

FT X

20

illustrate its usefulness for semiparametric inference on the average treatment eﬀect of a binary
treatment. We could use this rate to verify the high-level conditions in Assumption 8 for the
SDML estimator.

We can estimate the AMSE optimal bandwidth h∗

t given in Theorem 2 by the SDML ap-
ˇβt,b− ˇβt,ub
proach. We can estimate the leading bias Bt by ˇBt ≡
b2(1−u2) with a pre-speciﬁed positive scaling
parameter u (cid:54)= 1. We can estimate the asymptotic variance Vt by ˇVt ≡ hdtn−1 (cid:80)L
ˇψ2
i(cid:96),
where ˇψi(cid:96) ≡ Kh(Ti − t)(Yi − ˆγ(cid:96)(Ui, Xi))/ ˆf(cid:96)(t|Xi) + ˆγ(cid:96)(Ui, Xi) − ˇβt. Then a data-driven bandwidth
ˇht ≡ (cid:0)dt
(cid:1)(cid:1)1/(dt+4)n−1/(dt+4). Theorem 8 below shows the consistency of ˇBt, ˇVt, and ˇht,
under Assumption 9 that is modiﬁed from Assumption 4.

(cid:14)(cid:0)4ˇB2

ˇVt

(cid:80)

i∈I(cid:96)

(cid:96)=1

t

(cid:107)(ˆγ(cid:96) − γ)( ˆf(cid:96)(t|·) − f (t|·))(cid:107)FT X =
Assumption 9 For each (cid:96) = 1, ..., L and for any t ∈ T , (i) h−dt/2
(cid:107)(ˆγ(cid:96) −γ)2( ˆf(cid:96)(t|·)−fT |X(t|·))2(cid:107)FT X = Op(1), (cid:107)(ˆγ(cid:96) −γ)4(cid:107)FT X = Op(1), and (cid:107)( ˆf(cid:96)(t|·)−
op(1), (ii) h−dt/2
fT |X(t|·))2(cid:107)FX = Op(1). (iii) E[(Y − γ(t, x))4|T = t, X = x]fT |X(t|x) is bounded uniformly over
x ∈ X . (iv) b → 0 and nbdt+4 → ∞.

0

0

Theorem 8 (SDML-AMSE optimal bandwidth) Let the conditions in Theorem 7 and As-
sumption 9 hold. Then for t ∈ T , ˇVt, ˇBt, and ˇht are consistent for Vt, Bt, and h∗
t given in
Theorem 2 respectively.

We can similarly obatin the asymptotic theory for ˇθt.

Theorem 9 (SDML-Partial eﬀect) Let the conditions in Theorems 3 and 7 hold. When η/h →
(cid:13)
0 in Theorem 3(i), assume that for each (cid:96) = 1, ..., L and for any t ∈ T , (a) η−1hh−dt/2
(cid:13)ˆγ(cid:96) −
(cid:13)
γ(cid:13)
p
→ 0; (b) η−1h
(cid:13)ˆγ(cid:96) −
(cid:13)FT X
γ(cid:13)
(cid:13)FT X

→ 0 and η−1h(cid:13)
nhdth−dt/2
0
→ 0. Then the results in Theorem 3 hold with ˇθt in place of ˆθt.

(cid:13) ˆf(cid:96)(t|·) − fT |X(t|·)(cid:13)

(cid:13) ˆf(cid:96)(t|·) − fT |X(t|·)(cid:13)
(cid:13)

(cid:13)FX

(cid:13)FX

√

p

p

0

5 Numerical examples

This section provides numerical examples of Monte Carlo simulations and an empirical illustration.
The estimation procedure of the proposed double debiased machine learning (DML) estimator is
described in Section 2. To estimate the conditional mean function γ(t, x) = E[Y |T = t, X = x] and
the conditional density (or the generalized propensity score GPS) fT |X by MultiGPS as described
in Section 2.1, we employ three machine learning methods: Lasso, the generalized random forests
in Athey et al. (2019), and the deep neural networks based on Farrell et al. (2021b) as described
in Section 3.1. We implement our DML estimator with these three algorithms respectively in
Python, using the packages scikit-learn, pytorch, numpy, pandas, rpy2 and scipy. We use the R

21

package “grf” for the generalized random forest implementation, implementing it in Python via
the rpy2 package. Software is available from the authors.

5.1 Simulation study

We describe the nuisance estimators in more detail.

Lasso: The penalization parameter is chosen via grid search utilizing tenfold cross validation
for ˆγ and ˆfT |X separately. The basis functions contain third-order polynomials of X and T , and
interactions among X and T .

Generalized Random forest (GRF): We use the generalized random forests in Athey et al.
(2019), with 2,000 trees and all other parameters chosen via cross validation in every Monte Carlo
replication. The parameters tuned via cross validation are: The fraction of data used for each
tree, the number of variables tried for each split, the minimum number of observations per leaf,
whether or not to use “honesty splitting,” whether or not to prune trees such that no leaves are
empty, the maximum imbalance of a split, and the amount of penalty for an imbalanced split.
Unlike Lasso, we do not add any additional basis functions as inputs into GRF.

Deep neural network (DNN): We use the deep neural networks described in Section 3.1 with
four hidden layers. Each hidden layer has 10 neurons and uses rectiﬁed linear units (ReLU)
activation functions. The weights are ﬁt using stochastic gradient descent with a weight decay of
0.2 and a learning rate of 0.01.13 For the selection of the neural network models, we perform a
train-test split of the data and chose the models based on out-of-sample performance.

We consider the data-generating process: ν ∼ N (0, 1), ε ∼ N (0, 1),

X = (X1, ..., X100)(cid:48) ∼ N (0, Σ), T = Φ(3X (cid:48)θ) + 0.75ν − 0.5, Y = 1.2T + 1.2X (cid:48)θ + T 2 + T X1 + ε,

where θj = 1/j2, diag(Σ) = 1, the (i, j)-entry Σij = 0.5 for |i−j| = 1 and Σij = 0 for |i−j| > 1 for
i, j = 1, ..., 100, and Φ is the CDF of N (0, 1). Thus the potential outcome Y (t) = 1.2t + 1.2X (cid:48)θ +
t2 + tX1 + ε. Let the parameter of interest be the average dose response function at t = 0, i.e.,
β0 = E[Y (0)] = 0.

We compare estimations with cross-ﬁtting and without cross-ﬁtting, and with a range of band-
widths to demonstrate robustness to bandwidth choice. We consider sample size n ∈ {500, 1000}
and the number of subsamples used for cross-ﬁtting L ∈ {1, 2, 5}. We use the second-order
Epanechnikov kernel with bandwidth h. For the MultiGPS estimator described in Section 2.1,

13Weight decay is a form of regularization to prevent overﬁtting. Weight decay is a penalty where after each
iteration the weights in the network are multiplied by (1 − αλ) before adding the adjustment in the direction of
the gradient, where α is the learning rate (step size) and λ is the weight decay.

22

we choose bandwidth h1 = h. Let the bandwidth of the form h = cσT n−0.2 for a constant
c ∈ {0.5, 0.75, 1.0, 1.25, 1.5} and the standard deviation σT of T . We computed the AMSE-optimal
bandwidth h∗
0 given in Theorem 2(i) that has the corresponding c∗ = 1.43. Thus using some un-
dersmoothing bandwidth with c < c∗, the 95% conﬁdence interval (cid:2) ˆβt ± 1.96s.e.(cid:3) is asymptotically
valid, where the standard error (s.e.) is computed using the sample analogue of the estimated
inﬂuence function, as described in Section 3.

Table 1 reports the results based on 1,000 Monte Carlo replications. The estimators using
these machine learning methods perform well in the case of cross-ﬁtting (L = 2, 5), with coverage
rates near the nominal 95% for small bandwidths. Under no cross-ﬁtting (L = 1), the conﬁdence
intervals generally have lower coverage rates than under cross-ﬁtting. The coverage rate and bias
are improved the most for GRF with cross-ﬁtting. Cross-ﬁtting should improve our estimation in
the case that the machine learning algorithm is over-ﬁtting. Given that cross-ﬁtting only results
in small improvements for Lasso and DNN, it might suggest that those algorithms do not have a
severe over-ﬁtting problem for this data-generating process, but GRF does. It may be possible to
alleviate this over-ﬁtting via more regularization.

We do not ﬁnd signiﬁcant diﬀerence between L = 2 and L = 5. However one issue with
choosing a smaller L is that the machine learning algorithm is ﬁt on a smaller subset of the data.
For example, we see some large RMSE for Lasso for L = 2 in n = 500. CCDDHNR also discuss
that the ﬁvefold cross-ﬁtting estimates use more observations to learn the nuisance functions than
twofold cross-ﬁtting and thus are likely learn them more precisely.

Table 2 presents results for Lasso using the ReGPS estimator. The performance is improved
in terms of bias and RMSE and is more robust to bandwidth. Furthermore we no longer see the
instability observed for lasso under the MultiGPS estimator for small bandwidths, e.g., L = 2, n =
500, c = 0.5.

All three methods seem somewhat robust to bandwidth choice under cross-ﬁtting. Overall
these results demonstrate consistency with the theoretical results of this paper, conﬁrming the
importance of cross-ﬁtting for these ML methods.

5.2 Empirical illustration

We illustrate our method by re-analyzing the Job Corps program in the United States, which was
conducted in the mid-1990s. The Job Corps program is the largest publicly funded job training
program, which targets disadvantaged youth. The participants are exposed to diﬀerent numbers
of actual hours of academic and vocational training. The participants’ labor market outcomes may
diﬀer if they accumulate diﬀerent amounts of human capital acquired through diﬀerent lengths of

23

Table 1: Simulation Results

Lasso

Random Forest
Bias RMSE Coverage Bias RMSE Coverage Bias RMSE Coverage

Neural Network

0.010
0.010
0.017
0.027
0.040

1.026
-0.359
-0.020
-0.055
0.037

-0.161
-0.003
0.013
0.025
0.038

0.010
0.014
0.018
0.025
0.034

-0.061
0.007
0.018
0.025
0.035

0.008
0.013
0.018
0.025
0.034

0.195
0.134
0.125
0.120
0.118

86.508
9.620
0.598
2.720
0.124

5.782
0.200
0.130
0.122
0.119

0.121
0.106
0.097
0.093
0.092

1.566
0.158
0.098
0.094
0.092

0.124
0.106
0.097
0.093
0.092

0.934
0.938
0.946
0.950
0.938

0.973
0.965
0.950
0.956
0.941

0.950
0.947
0.944
0.951
0.943

0.954
0.949
0.951
0.935
0.924

0.960
0.952
0.954
0.936
0.919

0.956
0.952
0.952
0.933
0.923

0.127
0.106
0.095
0.090
0.089

-0.001
0.004
0.007
0.009
0.014

0.008
0.010
0.011
0.013
0.018

0.109
0.091
0.084
0.081
0.080

0.004
0.007
0.010
0.013
0.017

0.001
0.006
0.010
0.014
0.019

0.231
0.203
0.186
0.174
0.168

0.211
0.170
0.149
0.138
0.130

0.254
0.209
0.185
0.168
0.157

0.184
0.161
0.146
0.139
0.134

0.150
0.125
0.111
0.102
0.096

0.188
0.156
0.137
0.125
0.118

0.850
0.859
0.892
0.907
0.911

0.950
0.957
0.963
0.960
0.965

0.951
0.958
0.960
0.964
0.966

0.789
0.827
0.843
0.855
0.853

0.948
0.956
0.955
0.952
0.951

0.951
0.947
0.952
0.952
0.950

-0.061
-0.072
-0.076
-0.080
-0.081

-0.092
-0.088
-0.088
-0.088
-0.087

-0.076
-0.077
-0.080
-0.082
-0.083

-0.072
-0.072
-0.073
-0.074
-0.075

-0.083
-0.084
-0.085
-0.086
-0.087

-0.075
-0.075
-0.076
-0.076
-0.077

0.359
0.298
0.265
0.242
0.227

0.308
0.251
0.222
0.205
0.194

0.365
0.303
0.269
0.245
0.229

0.288
0.240
0.210
0.192
0.181

0.235
0.200
0.178
0.165
0.156

0.291
0.242
0.212
0.194
0.182

0.962
0.960
0.951
0.955
0.956

0.957
0.960
0.947
0.945
0.946

0.965
0.958
0.953
0.950
0.954

0.947
0.947
0.943
0.941
0.935

0.955
0.944
0.936
0.922
0.926

0.946
0.947
0.942
0.943
0.933

n

500

L

1

2

5

1000

1

2

5

c

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

Notes: L = 1: no cross-ﬁtting. L = 2: twofold cross-ﬁtting. L = 5: ﬁvefold cross-ﬁtting. The
bandwidth is h = cσT n−0.2, and c = 1.43 for the AMSE-optimal bandwidth. The nominal coverage
rate of the conﬁdence interval is 0.95.

24

Table 2: Simulation Results with ReGPS

n

500

L

1

2

5

1000

1

2

5

Lasso

c

Bias RMSE Coverage

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.50
0.75
1.00
1.25
1.50

0.011
0.015
0.020
0.029
0.042

-0.021
-0.011
0.011
0.024
0.042

0.007
0.011
0.017
0.026
0.039

0.006
0.010
0.015
0.022
0.032

0.011
0.015
0.022
0.030
0.040

0.005
0.009
0.015
0.022
0.032

0.159
0.136
0.125
0.118
0.116

0.222
0.188
0.155
0.138
0.133

0.172
0.143
0.129
0.121
0.118

0.118
0.102
0.093
0.089
0.088

0.121
0.102
0.092
0.088
0.087

0.120
0.103
0.094
0.089
0.088

0.953
0.952
0.950
0.951
0.942

0.951
0.941
0.951
0.944
0.940

0.948
0.955
0.953
0.950
0.944

0.949
0.947
0.948
0.946
0.937

0.969
0.956
0.960
0.947
0.940

0.953
0.953
0.951
0.949
0.936

Notes: L = 1: no cross-ﬁtting. L = 2: twofold cross-ﬁtting. L = 5: ﬁvefold cross-ﬁtting. The
bandwidth is h = cσT n−0.2, and c = 1.43 for the AMSE-optimal bandwidth. The nominal coverage
rate of the conﬁdence interval is 0.95.

25

exposure. We estimate the average dose response functions to investigate the relationship between
employment and the length of exposure to academic and vocational training. As our analysis builds
on Flores et al. (2012), Hsu et al. (2020), and Lee (2018), we refer the readers to the reference
therein for further details of Job Corps.

We use the same dataset in Hsu et al. (2020). We consider the outcome variable (Y ) to be
the proportion of weeks employed in the second year following the program assignment. The
continuous treatment variable (T ) is the total hours spent in academic and vocational training in
the ﬁrst year. We follow the literature to assume the conditional independence Assumption 1(i),
meaning that selection into diﬀerent levels of the treatment is random, conditional on a rich set
of observed covariates, denoted by X. The identifying Assumption 1 is indirectly assessed in
Flores et al. (2012). Our sample consists of 4,024 individuals who completed at least 40 hours
(one week) of academic and vocational training. There are 40 covariates measured at the baseline
survey. Figure 1 shows the distribution of T by a histogram, and Table 3 provides brief descriptive
statistics.

Implementation details We estimate the average dose response function βt = E[Y (t)] and
partial eﬀect θt = ∂E[Y (t)]/∂t by the proposed DML estimator with ﬁvefold cross-ﬁtting. We
implement three DML estimators Lasso, the generalized random forest, and the deep neural net-
work. The parameters for these three methods are selected as described in Section 5.1. For the
deep neural networks described in Section 3.1, the regression estimation of γ uses a neural network
with two hidden layers and a weight decay of 0.1. The ﬁrst hidden layer has 100 neurons and the
second hidden layer has 20 neurons. The GPS estimation uses a network with four hidden layers
and a weight decay of 0.1. Each layer has 10 neurons.

We use the second-order Epanechnikov kernel with bandwidth h. For the MultiGPS estimator,
we use the Gaussian kernel with bandwidth h1 = h. We compute the optimal bandwidth that
minimizes an asymptotic integrated MSE. For practical implementation, consider a weight function
w(t) = 1{t ∈ [t, ¯t]}/(¯t − t) that is the density of U nif orm[t, ¯t] on a subset of the support of T .
(cid:1)w(t)dt is
The bandwidth that minimizes the asymptotic integrated MSE (cid:82)
w = (cid:0)dtVw
h∗
t w(t)dt.14 Set m
equally spaced grid points over [t, ¯t]: (cid:8)t = t1, t2, ..., tm = ¯t(cid:9). Following the approach given in
Section 3, we estimate Vtj with h = 3ˆσT n−0.2 = 548.52 and Btj with b = 2h and a = 0.5, for
(cid:14)(cid:0)4ˆBw
j = 1, ..., m. A plug-in estimator ˆh∗
ˆVtj and
ˆBw = m−1 (cid:80)m
ˆB2
. We use [t, ¯t] = [160, 1840] and tj − tj−1 = 40 in this empirical application.
tj

(cid:0)Vt/(nhdt) + h4B2
T B2

(cid:1)(cid:1)1/(dt+4)n−1/(dt+4), where Vw ≡ (cid:82)

(cid:1)(cid:1)1/5n−1/5, where ˆVw = m−1 (cid:80)m

T Vtw(t)dt and Bw ≡ (cid:82)

w = (cid:0)ˆVw

(cid:14)(cid:0)4Bw

j=1

j=1

T

t

(cid:0)h4B2
14Following Theorem 2, the asymptotic integrated MSE is (cid:82)
T
function w(t) : T (cid:55)→ R. Solving the ﬁrst-order conditions yields h∗
w.

t + Vt/(nhdt)(cid:1)w(t)dt for an integrable weight

26

We then obtain under-smoothing bandwidths 0.8ˆh∗
generalized random forest, and 208.87 for the deep neural network.

w that are 170.76 for Lasso, 219.11 for the

Results Figure 2 presents the estimated average dose response function βt along with 95%
point-wise conﬁdence intervals. The results for the three ML nuisance estimators have similar
patterns. The estimates suggest an inverted-U relationship between the employment and the
length of participation. DNN estimates appear to be the most erratic, possibly due to the smaller
bandwidth compared with other estimators.

Figure 3 reports the partial eﬀect estimates ˆθt with step size η = 160 (one month). Across all
procedures, we see positive partial eﬀects when hours of training are less than around 500 (three
months) and negative partial eﬀect around 1,500 hours (9 months). Taking the estimates by DNN
for example, ˆβ400 = 47.69 with standard error s.e. = 1.54 and ˆθ400 = 0.0320 with s.e. = 0.0117.
This estimate implies that increasing the training from two months to three months increases the
average proportion of weeks employed in the second year by 5.12% (about two and half working
weeks) with s.e. = 1.872%.

Lee (2009) ﬁnds that the program had a negative impact on employment propensities in the
short term (104 weeks since random assignments) and a positive eﬀect in the long term (104-208
weeks). Note that Lee (2009) considers a binary treatment variable of being in the program or not,
with the outcome variable 1{Y ≥ 0} in our notations. We focus on the employment proportion in
the second year following the program assignment (52-104 weeks) and estimate the heterogenous
eﬀects of the total hours spent in academic and vocational training in the ﬁrst year.

We note that the empirical practice has focused on semiparametric estimation; see Flores et al.
(2012), Hsu et al. (2020), Lee (2018), for example. The semiparametric methods are subject to
the risk of misspeciﬁcation. Our DML estimator provides a feasible approach to implementing a
fully nonparametric inference in practice.

6 Conclusion and outlook

This paper provides a nonparametric inference method for continuous treatment eﬀects under
unconfoundedness and in the presence of high-dimensional or nonparametric nuisance parameters.
The proposed kernel-based double debiased machine learning (DML) estimator uses a doubly
robust moment function and cross-ﬁtting. We provide tractable high-level conditions for the
nuisance estimators and asymptotic theory for inference on the average dose-response function (or
the average structural function) and the partial eﬀect. A main contribution is to provide low-level
conditions for kernel estimator, series, and the deep neural networks in Farrell et al. (2021b). We

27

justify the use of the kernel function by calculating the Gateaux derivative. We further introduce
a Simulated DML estimator that enables high-level conditions on the standard mean-squared
convergence rates of the nuisance estimators.

For a future extension, our DML estimator serves as the preliminary element for policy learning
and optimization with a continuous decision, following Manski (2004), Hirano and Porter (2009),
Kitagawa and Tetenov (2018), Kallus and Zhou (2018), Demirer et al. (2019), Athey and Wager
(2019), Farrell et al. (2021b), among others.

Another extension is robustness against multiway clustering, where the conventional cross-
ﬁtting does not ensure the independence between observations in I(cid:96) from I c
(cid:96) . We may adopt
the K 2-fold multiway cross-ﬁtting proposed by Chiang et al. (2021) that focus on regular DML
estimators as in CCDDHNR. Since the form of estimators and the proofs of asymptotic theories
for our continuous treatment case are similar to those studied in Chiang et al. (2021), we expect
that their proposed algorithm works for ˆβt; a formal extension is out of the scope of this paper.
When unconfoundedness is violated, we can use the control function approach in triangular
simultaneous equations models by including in the covariates some estimated control variables
using instrumental variables. In particular, Lee (2009) studies the issue of sample selection for the
wage eﬀects of the Job Corps program. To extend our empirical application to the wage eﬀect of
the length of exposure to the program, we may follow Lee (2009) to estimate bounds on the wage
eﬀect of the continuous treatment using the excess number of individuals who are induced to be
selected.15 A closer approach to our estimator is Das et al. (2003), who study a nonparametric
sample selection model with endogeneity and show that the propensity score and reduced form
residuals lead to a control function method to account for both selection and endogeneity. Imbens
and Newey (2009) show that the conditional independence assumption holds when the covariates
X include the additional control variable V = FT |Z(T |Z), the conditional distribution function of
the endogenous variable given the instrumental variables Z. The inﬂuence function that accounts
for estimating the control variables as generated regressors has derived in Corollary 2 in Lee
(2015). Lee (2015) shows that the adjustment terms for the estimated control variables are of
smaller order in the inﬂuence function of the ﬁnal estimator, but it may be important to include
them to achieve local robustness. This is a distinct feature of the average structural function of
continuous treatments, as discussed in Section 3. Using such an inﬂuence function to construct

15One key identiﬁcation assumption in Lee (2009) is monotonicity that assumes the potential sample selection
indicator to be weakly monotonic in the treatment value, i.e., 1{Y (t) > 0} ≥ 1{Y (t(cid:48)) > 0} for t > t(cid:48). In words,
suppose an individual is employed in the second year. The monotonicity assumption requires that this individual
must be employed if he/she received more hours of training. Such monotonicity assumption is not testable but
may not be supported by our estimation results. Das et al. (2003) use an exclusion assumption for instrumental
variables.

28

the corresponding DML estimator is left for future research.

Figure 1: Histogram of Hours of Training

Table 3: Descriptive statistics

Variable

Mean

Median

StdDev Min

Max

share of weeks employed in 2nd year (Y )
total hours spent in 1st-year training (T )

44.00
1219.80

40.38
992.86

37.88
961.62

0
40

100
6188.57

Notes: Summary statistics for 4,024 individuals who completed at least 40 hours of academic and vocational
training.

References

Aitchison, J. and C. G. G. Aitken (1976). Multivariate binary discrimination by the kernel method.

Biometrika 63, 413–420.

Athey, S., G. W. Imbens, and S. Wager (2018). Approximate residual balancing: De-biased
inference of average treatment eﬀects in high dimensions. Journal of the Royal Statistical Society
Statistical Methodology Series B 80 (4).

Athey, S., J. Tibshirani, and S. Wager (2019). Generalized random forests. Annals of Statis-

tics 47 (2), 1148–1178.

Athey, S. and S. Wager (2019). Eﬃcient policy learning. arxiv:1702.02896.

29

Figure 2: Estimated average dose response functions and 95% conﬁdence intervals

Figure 3: Estimated partial eﬀects and 95% conﬁdence interval

30

Belloni, A., V. Chernozhukov, I. Fern´andez-Val, and C. Hansen (2017). Program evaluation and

causal inference with high-dimensional data. Econometrica 85 (1), 233–298.

Belloni, A., V. Chernozhukov, and C. Hansen (2014). Inference on Treatment Eﬀects after Selection

among High-Dimensional Controls. The Review of Economic Studies 81 (2), 608–650.

Belloni, A., V. Chernozhukov, and K. Kato (2019). Valid post-selection inference in high-
dimensional approximately sparse quantile regression models. Journal of the American Sta-
tistical Association 114 (526), 749–758.

Bickel, P. J., Y. Ritov, and A. B. Tsybakov (2009). Simultaneous analysis of Lasso and Dantzig

selector. Annals of Statistics 37 (4), 1705–1732.

Blundell, R. and J. L. Powell (2003). Endogeneity in nonparametric and semiparametric regres-
sion models. In L. H. M. Dewatripont and S.J.Turnovsky (Eds.), Advances in Economics and
Econometrics, Theory and Applications, Eighth World Congress, Volume II. Cambridge Univer-
sity Press, Cambridge, U.K.

Bravo, F., J. Escanciano, and I. van Keilegom (2020). Two-step semiparametric likelihood infer-

ence. Annals of Statistics 48, 1–26.

Calonico, S., M. D. Cattaneo, and M. H. Farrell (2018). On the eﬀect of bias estimation on coverage
accuracy in nonparametric inference. Journal of the American Statistical Association 113 (522),
767–779.

Carone, M., A. R. Luedtke, and M. J. van der Laan (2018). Toward computerized eﬃcient
estimation in inﬁnite-dimensional models. Journal of the American Statistical Association 0 (0),
1–17.

Cattaneo, M. D. (2010). Eﬃcient semiparametric estimation of multi-valued treatment eﬀects

under ignorability. Journal of Econometrics 155 (2), 138–154.

Cattaneo, M. D., M. H. Farrell, and Y. Feng (2020). Large sample properties of partitioning-based

series estimators. Annals of Statistics 48 (3), 1718–1741.

Cattaneo, M. D. and M. Jansson (2019). Average density estimators: Eﬃciency and bootstrap

consistency. arxiv:1904.09372v1.

Cattaneo, M. D., M. Jansson, and X. Ma (2019). Two-step estimation and inference with possibly

many included covariates. Review of Economic Studies 86 (3), 1095–1122.

Cattaneo, M. D., M. Jansson, and W. Newey (2018a). Alternative asymptotics and the partially

linear model with many regressors. Econometric Theory 34 (2), 277–301.

Cattaneo, M. D., M. Jansson, and W. Newey (2018b).

Inference in linear regression models
with many covariates and heteroscedasticity. Journal of the American Statistical Associa-
tion 113 (523), 1350–1361.

31

Chen, X. (2007). Chapter 76 Large sample sieve estimation of semi-nonparametric models. Vol-

ume 6 of Handbook of Econometrics, pp. 5549–5632. Elsevier.

Chen, X., Z. Liao, and Y. Sun (2014). Sieve inference on possibly misspeciﬁed semi-nonparametric

time series models. Journal of Econometrics 178, 639–658.

Chen, X. and D. Pouzo (2015). Sieve wald and QLR inferences on semi/nonparametric conditional

moment models. Econometrica 83 (3), 1013–1079.

Chen, X. and H. White (1999). Improved rates and asymptotic normality for nonparametric neural

network estimators. IEEE Transactions on Information Theory 45.

Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duﬂo, C. Hansen, W. Newey, and J. Robins
(2018). Double/debiased machine learning for treatment and structural parameters. The Econo-
metrics Journal 21 (1), C1–C68.

Chernozhukov, V., D. Chetverikov, and K. Kato (2014a). Anti-concentration and honest, adaptive

conﬁdence bands. Annals of Statistics 42 (5), 1787–1818.

Chernozhukov, V., D. Chetverikov, and K. Kato (2014b). Gaussian approximation of suprema of

empirical processes. Annals of Statistics 42 (4), 1564–1597.

Chernozhukov, V., J. C. Escanciano, H. Ichimura, W. K. Newey, and J. M. Robins (2018). Locally

robust semiparametric estimation. arxiv:1608.00033.

Chernozhukov, V., J. A. Hausman, and W. K. Newey (2019). Demand analysis with many prices.

cemmap Working Paper, CWP59/19.

Chernozhukov, V., W. Newey, and R. Singh (2022a). Automatic debiased machine learning of

causal and structural eﬀects. Econometrica 90 (3), 967–1027.

Chernozhukov, V., W. Newey, and R. Singh (2022b). Debiased machine learning of global and
local parameters using regularized Riesz representers. Econometrics Journal , forthcoming.

Chiang, H. D., K. Kato, Y. Ma, and Y. Sasaki (2021). Multiway cluster robust double/debiased

machine learning. Journal of Business & Economic Statistics 0 (0), 1–11.

Das, M., W. K. Newey, and F. Vella (2003). Nonparametric estimation of sample selection models.

Review of Economic Studies 70 (1), 33–58.

Demirer, M., V. Syrgkanis, G. Lewis, and V. Chernozhukov (2019). Semi-parametric eﬃcient

policy learning with continuous actions. arxiv:1905.10116v1.

Fan, Q., Y.-C. Hsu, R. P. Lieli, and Y. Zhang (2021). Estimation of conditional average treatment
eﬀects with high-dimensional data. Journal of Business & Economic Statistics, forthcoming.

Farrell, M. H. (2015). Robust inference on average treatment eﬀects with possibly more covariates

than observations. Journal of Econometrics 189 (1), 1–23.

32

Farrell, M. H., T. Liang, and S. Misra (2021a). Deep learning for individual heterogeneity: An

automatic inference framework. arxiv:2010.14694.

Farrell, M. H., T. Liang, and S. Misra (2021b). Deep neural networks for estimation and inference.

Econometrica 89 (1), 181–213.

Flores, C. A. (2007). Estimation of dose-response functions and optimal doses with a continuous

treatment. Working paper.

Flores, C. A., A. Flores-Lagunes, A. Gonzalez, and T. C. Neumann (2012). Estimating the eﬀects
of length of exposure to instruction in a training program: The case of job corps. The Review
of Economics and Statistics 94 (1), 153–171.

Galvao, A. F. and L. Wang (2015). Uniformly semiparametric eﬃcient estimation of treatment
eﬀects with a continuous treatment. Journal of the American Statistical Association 110 (512),
1528–1542.

Hahn, J. (1998). On the role of the propensity score in eﬃcient semiparametric estimation of

average treatment eﬀects. Econometrica 66 (2), 315–332.

Hampel, F. R. (1974). The inﬂuence curve and its role in robust estimation. Journal of the

American Statistical Association 69, 383–393.

Hansen, B. E. (2022). Econometrics. Princeton University Press.

Hirano, K. and G. W. Imbens (2004). The propensity score with continuous treatments. In A. Gel-
man and X.-L. Meng (Eds.), Applied Bayesian Modeling and Causal Inference from Incomplete-
Data Perspectives, pp. 73–84. New York: Wiley.

Hirano, K., G. W. Imbens, and G. Ridder (2003). Eﬃcient estimation of average treatment eﬀects

using the estimated propensity score. Econometrica 71 (4), 1161–1189.

Hirano, K. and J. Porter (2009). Asymptotics for statistical treatment rules. Econometrica 77,

1683–1701.

Hsu, Y.-C., M. Huber, Y.-Y. Lee, and L. Lettry (2020). Direct and indirect eﬀects of continuous
treatments based on generalized propensity score weighting. Journal of Applied Economet-
rics 35 (7), 814–840.

Hsu, Y.-C., M. Huber, Y.-Y. Lee, and C.-A. Liu (2022). Testing monotonicity of mean potential

outcomes in a continuous treatment with high-dimensional data. arxiv:2106.04237.

Hsu, Y.-C., T.-C. Lai, and R. P. Lieli (2020). Estimation and inference for distribution and
quantile functions in endogenous treatment eﬀect models. Econometric Reviews 0 (0), 1–38.

Ichimura, H. and W. K. Newey (2022). The inﬂuence function of semiparametric estimators.

Quantitative Economics 13 (1), 29–61.

33

Imbens, G. (2000). The role of the propensity score in estimating dose-response functions.

Biometrika 87 (3), 706–710.

Imbens, G. W. and W. K. Newey (2009). Identiﬁcation and estimation of triangular simultaneous

equations models without additivity. Econometrica 77 (5), 1481–1512.

Kallus, N. and A. Zhou (2018). Policy evaluation and optimization with continuous treatments.
Proceedings of the 21st International Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS) 84, 1243–1251.

Kennedy, E. H., Z. Ma, M. D. McHugh, and D. S. Small (2017). Nonparametric methods for doubly
robust estimation of continuous treatment eﬀects. Journal of the Royal Statistical Society: Series
B 79 (4), 1229–1245.

Khan, S. and E. Tamer (2010). Irregular identiﬁcation, support conditions, and inverse weight

estimation. Econometrica 78 (6), 2021–2042.

Kitagawa, T. and A. Tetenov (2018). Who should be treated? empirical welfare maximization

methods for treatment choice. Econometrica 86, 591–616.

Kluve, J., H. Schneider, A. Uhlendorﬀ, and Z. Zhao (2012). Evaluating continuous training
programs using the generalized propensity score. Journal of the Royal Statistical Society: Series
A (Statistics in Society) 175 (2), 587–617.

Koenker, R. (1994). Conﬁdence intervals for regression quantiles.

In H. M. Mandl P. (Ed.),

Asymptotic Statistics. Contributions to Statistics. Physica, Heidelberg.

Lee, D. S. (2009, 07). Training, Wages, and Sample Selection: Estimating Sharp Bounds on

Treatment Eﬀects. The Review of Economic Studies 76 (3), 1071–1102.

Lee, Y.-Y. (2015). Partial mean processes with generated regressors: Continuous treatment eﬀects

and nonseparable models. Working paper.

Lee, Y.-Y. (2018). Partial mean processes with generated regressors: Continuous treatment eﬀects

and nonseparable models. arxiv:1811.00157.

Lee, Y.-Y. and H.-H. Li (2018). Partial eﬀects in binary response models using a special regressor.

Economics Letters 169, 15–19.

Luo, Y. and M. Spindler (2016).

High-dimensional

l2boosting: Rate of convergence.

arxiv:1602.08927.

Manski, C. F. (2004). Statistical treatment rules for heterogeneous populations. Economet-

rica 72 (4), 1221–1246.

Newey, W. (1994a). The asymptotic variance of semiparametric estimators. Econometrica 62 (6),

1349–1382.

34

Newey, W. K. (1994b). Kernel estimation of partial means and a general variance estimator.

Econometric Theory 10 (2), 233–253.

Newey, W. K. (1997). Convergence rates and asymptotic normality for series estimators. Journal

of Econometrics 79 (1), 147–168.

Newey, W. K. and J. R. Robins (2018). Cross-ﬁtting and fast remainder rates for semiparametric

estimation. arxiv:1801.09138.

Neyman, J. (1959). Optimal asymptotic tests of composite statistical hypotheses. Probability and

Statistics 213 (57).

Noack, C., T. Olma, and C. Rothe (2021). Flexible covariate adjustments in regression disconti-

nuity designs. 2107.07942.

Oprescu, M., V. Syrgkanis, and Z. S. Wu (2019). Orthogonal random forest for causal inference.

arxiv:1806.03467v3.

Ouyang, D., Q. Li, and J. S. Racine (2009). Nonparametric estimation of regression functions

with discrete regressors. Econometric Theory 25 (1), 1–42.

Powell, J. L., J. H. Stock, and T. M. Stoker (1989). Semiparametric estimation of index coeﬃcients.

Econometrica 57 (6), 1403–30.

Powell, J. L. and T. M. Stoker (1996). Optimal bandwidth choice for density-weighted averages.

Journal of Econometrics 75 (2), 291–316.

Robinson, P. M. (1988). Root-N-consistent semiparametric regression. Econometrica 56 (4), 931–

954.

Rothe, C. and S. Firpo (2019). Properties of doubly robust estimators when nuisance functions

are estimated nonparametrically. Econometric Theory 35 (5), 1048–1087.

Sasaki, Y. and T. Ura (2021). Estimation and inference for policy relevant treatment eﬀects.

Journal of Econometrics, forthcoming.

Sasaki, Y., T. Ura, and Y. Zhang (2021). Unconditional quantile regression with high dimensional

data. arxiv:2007.13659.

Schmidt-Hieber, J. (2020). Nonparametric regression usingdeep neural networkswith reluactivation

function. Annals of Statistics 48 (4), 1875–1897.

Semenova, V. and V. Chernozhukov (2020). Estimation and inference about conditional average

treatment eﬀect and other causal functions. arxiv:1702.06240v3.

Su, L., T. Ura, and Y. Zhang (2019). Non-separable models with high-dimensional data. Journal

of Econometrics 212 (2), 646–677.

35

Syrgkanis, V. and M. Zampetakis (2020). Estimation and inference with trees and forests in high

dimensions. arxiv:2007.03210.

van der Vaart, A. W. and J. A. Wellner (1996). Weak Convergence and Empirical Processes: with

Application to Statistics. New York: Springer-Verlag.

Wager, S. and S. Athey (2018). Estimation and inference of heterogeneous treatment eﬀects using

random forests. Journal of the American Statistical Association 113 (523), 1228–1242.

Yarotsky, D. (2017). Error bounds for approximations with deep relu networks. Neural networks:

the oﬃcial journal of the International Neural Network Society 94, 103–114.

Zimmert, M. and M. Lechner (2019). Nonparametric estimation of causal heterogeneity under

high-dimensional confounding. arxiv:1908.08779.

Appendix

The Appendix is organized as follows. Sections A, B, and C collects proofs in Sections 2, 3,
and 4, respectively. Section D discusses the kernel localization to justify our kernel-based DML
estimators. Section E presents the kernel and series nuisance estimators. We illustrate how these
two conventional nonparametric estimators satisfy Assumption 3.

A Proofs in Section 2

Proof of Lemma 1 We suppress subscripts for notational simplicity, e.g., F (t|x) = FT |X(t|x).
First uniformly in t ∈ T , (cid:107)E[Φ((t−T )/h1)|X = ·]−FT |X(t|·)(cid:107)2
FX
T0
FT |X(t|x))2fX(x)dx = (cid:82)
h−1
1 φ((t−T )/h1)FT |X(T |x)dT −FT |X(t|x))2fX(x)dx =
X (Φ((t − T )/h1) + (cid:82) ∞
(cid:82)
−∞ φ((u)FT |X(t + uh1|x)du − FT |X(t|x))2fX(x)dx = O(h4
1), using integration
by parts, change of variables, a Taylor series expansion, and assuming ∂3FT |X(t|x)/∂t3 to be
uniformly bounded over T × X .
By the triangle inequality,

X (Φ((t−T )/h1)+(cid:82)

Φ((t−T )/h1)fT |X(T |x)dT −

X ((cid:82)

= (cid:82)

T0

(cid:13)
(cid:13)
(cid:92)1/fT |X(t|·) − 1/fT |X(t|·)
(cid:13)
(cid:13)
(cid:13)
(cid:13)FX

(cid:26) (cid:90)

≤

1
2(cid:15)

(cid:16) ˆF −1( ˆF (t|x) + (cid:15)|x) − ˆF −1( ˆF (t|x) − (cid:15)|x)

X

− (cid:0)F −1(F (t|x) + (cid:15)|x) − F −1(F (t|x) − (cid:15)|x)(cid:1) (cid:17)2

fX(x)dx

(cid:27)1/2

(10)

(cid:40)(cid:90)

+

X

(cid:18) F −1(F (t|x) + (cid:15)|x) − F −1(F (t|x) − (cid:15)|x)
2(cid:15)

−

1
f (t|x)

(cid:19)2

(cid:41)1/2

fX(x)dx

.

(11)

36

For (10), we focus on (cid:13)

(cid:13) ˆF −1( ˆF (t|·)+(cid:15)|·)−F −1(F (t|·)+(cid:15)|·)(cid:13)

. Denote s = ˆF −1( ˆF (t|x)+(cid:15)|x) ∈ T
by construction. So ˆF (s|x) = ˆF (t|x) + (cid:15). Denote the pth partial derivative of the conditional u-
quantile function F −1(u|x) with respect to u as ∂pF −1(u|x) = ∂p
∂vp F −1(v|x)|v=u. By the mean value
theorem, s = F −1(F (s|x)|x) = F −1( ˆF (s|x)|x) + (cid:0)F (s|x) − ˆF (s|x)(cid:1)∂1F −1( ¯F |x) with ¯F between
F (s|x) and ˆF (s|x). So

(cid:13)FX

s = ˆF −1( ˆF (t|x) + (cid:15)|x)

= F −1( ˆF (t|x) + (cid:15)|x) + (cid:0)F (s|x) − ˆF (s|x)(cid:1)∂1F −1( ¯F |x)
= F −1(F (t|x) + (cid:15)|x) + (cid:0) ˆF (t|x) − F (t|x)(cid:1)∂1F −1( ˜F |x) + (cid:0)F (s|x) − ˆF (s|x)(cid:1)∂1F −1( ¯F |x)

with ˜F between F (t|x) + (cid:15) and ˆF (t|x) + (cid:15) by the mean value theorem. As ∂1F −1 is assumed
to be bounded above uniformly, we obtain that (cid:13)
=
(cid:0) supt∈T
Op
supt∈T (cid:107)E[Φ((t − T )/h1)|X = ·] − F (t|·)(cid:107)FX
h2
1/(cid:15)).

(cid:13) ˆF −1( ˆF (t|·) + (cid:15)|·) − F −1(F (t|·) + (cid:15)|·)(cid:13)
(cid:13)FX
(cid:13)ˆµ(Φ((t − T )/h1); ·) − E[Φ((t − T )/h1)|X = ·](cid:13)
(cid:13)
(cid:13)FX
(cid:1) = Op(R1n + h2

+
1). So the term in (10) is Op(R1n/(cid:15) +

(cid:13) ˆF (t|·) − F (t|·)(cid:13)
(cid:13)

(cid:0) supt∈T

(cid:1) = Op

(cid:13)FX

Next consider (11). By a Taylor series expansion,

F −1(F (t|x) + (cid:15)|x) = F −1(F (t|x)|x) + (cid:15)∂1F −1(F (t|x)|x) + (cid:15)2∂2F −1(F (t|x)|x)/2 + (cid:15)3∂3F −1( ¯F (t|x)|x)/3!
F −1(F (t|x) − (cid:15)|x) = F −1(F (t|x)|x) − (cid:15)∂1F −1(F (t|x)|x) + (cid:15)2∂2F −1(F (t|x)|x)/2 − (cid:15)3∂3F −1( ˜F (t|x)|x)/3!

with ¯F (t|x) ∈ [F (t|x), F (t|x) + (cid:15)] and ˜F (t|x) ∈ [F (t|x) − (cid:15), F (t|x)]. So (cid:0)F −1(F (t|x) + (cid:15)|x) −
F −1(F (t|x) − (cid:15)|x)(cid:1)(cid:0)2(cid:15)(cid:1) − 1/f (t|x) = (cid:15)2(cid:0)∂3F −1( ˜F (t|x)|x) + ∂3F −1( ˜F (t|x)|x)(cid:1)/(2 · 3!). Since we
assume ∂3F −1(u|x) to be uniformly bounded over (u, x) ∈ (0, 1) × X , the term in (11) is O((cid:15)2).

As we assume that there exists a positive constant C such that sup(t,x)∈T ×X fT |X(t|x) ≤ C, we

(cid:110) (cid:82)

obtain that
X
1(cid:15)−1 + (cid:15)2).
Op(R1n(cid:15)−1 + h2

(cid:0) (cid:92)1/fT |X(t|x)−1/fT |X(t|x)(cid:1)2fT X(t, x)dx

(cid:111)1/2

≤

(cid:13)
(cid:13)
(cid:92)1/fT |X(t|·)−1/fT |X(t|·)
(cid:13)
(cid:13)
(cid:13)FX
(cid:13)

√

C =
(cid:3)

37

Proof of Lemma 2 By the assumptions, there exists a ﬁnite positive constant C such that
sup(t,x)∈T ×X fT |X(t|x) ≤ C. By the triangle inequality,

(cid:26)(cid:90)

(cid:0) ˆfT |X(t|x) − fT |X(t|x)(cid:1)2fT X(t, x)dx

(cid:27)1/2

X
(cid:26)(cid:90)

(cid:0) ˆfT |X(t|x) − fT |X(t|x)(cid:1)2fX(x)dxC

(cid:27)1/2

≤

≤

X

C
h2dt
1
(cid:26)

C

(cid:40)

+

(cid:90)

(cid:18)

ˆµ

X

(cid:90)

(cid:16)

E

X

(cid:104)
gh1 (T − t)
(cid:1) ,

= Op

(cid:0)h−dt

1 R1n + h2
1

(cid:18)

Πdt

j=1g

(cid:18) Tj − tj
h1

(cid:19)

(cid:19)

; x

− E

(cid:20)
Πdt

j=1g

(cid:18) Tj − tj
h1

(cid:19) (cid:12)
(cid:12)
(cid:12)X = x

(cid:21)(cid:19)2

fX(x)dx

(cid:41)1/2

(cid:105)

(cid:12)
(cid:12)
(cid:12)X = x

− fT |X(t|x)

(cid:17)2

fX(x)dx

(cid:27)1/2

(12)

For the term in (12) to be O(h2
the proof of Lemma 1.

1), we follow the standard algebra for kernel as the arguments in
(cid:3)

B Proofs in Section 3

Asymptotically linear representation We give an outline of deriving the asymptotically lin-
ear representation in Theorem 1, following CEINR. The moment function for identiﬁcation is
m(Zi, βt, γ) ≡ γ(t, Xi) − βt by equation (2), i.e., E[m(Zi, βt, γ(t, Xi))] = 0 uniquely deﬁnes βt. The
adjustment term φ(Zi, βt, γ, λ) ≡ Kh(Ti − t)λ(t, Xi) (Yi − γ(t, Xi)), where λ(t, x) ≡ 1/fT |X(t|x).
The doubly robust moment function ψ(Zi, βt, γ, λ) ≡ m(Zi, βt, γ(t, Xi))+φ(Zi, βt, γ(t, Xi), λ(t, Xi)),
as in equation (1).

Let Z c

(cid:96) denote the observations zi for i (cid:54)= I(cid:96). Let ˆγi(cid:96) ≡ ˆγ(cid:96)(t, Xi) and ˆλi(cid:96) ≡ 1/ ˆf(cid:96)(t|Xi) using
(cid:96) for i ∈ I(cid:96). Let γi ≡ γ(t, Xi) and λi ≡ λ(t, Xi). We can write ˆβt = L−1 (cid:80)L
ˆβt(cid:96), where ˆβt(cid:96) =
Z c
√
(cid:80)
(cid:96)=1( ˆβt(cid:96) − βt) =
n−1
(cid:96)
i∈I(cid:96)
L−1/2 (cid:80)L
ψ(Zi, βt, γi, λi) +
op(1) for each (cid:96) ∈ {1, ..., L}. Since L is ﬁxed and {I(cid:96)}(cid:96)=1,...,L are randomly partitioned distinct sub-
(cid:112)hdt/n(cid:96)
groups, the result follows from
(cid:80)

ψ(Zi, βt, ˆγi(cid:96), ˆλi(cid:96)) + βt and n(cid:96) = n/L. Then
√
n(cid:96)hdt( ˆβt(cid:96) − βt). We show below

nhdt( ˆβt − βt) =
n(cid:96)hdt( ˆβt(cid:96) − βt) = (cid:112)hdt/n(cid:96)
√

n(cid:96)hdt( ˆβt(cid:96)−βt) = L−1/2 (cid:80)L

nhdt( ˆβt−βt) = L−1/2 (cid:80)L

nhdtL−1 (cid:80)L

ψ(Zi, βt, γi, λi) + op(1) = (cid:112)hdt/n (cid:80)L

ψ(Zi, βt, γi, λi) + op(1).

(cid:80)

(cid:80)

i∈I(cid:96)

√

√

√

(cid:96)=1

(cid:96)=1

(cid:96)=1

(cid:96)=1

(cid:96)=1

i∈I(cid:96)

i∈I(cid:96)

38

We decompose the remainder term for each (cid:96) ∈ {1, ..., L},

(cid:112)

n(cid:96)hdt

1
n(cid:96)

(cid:115)

=

hdt
n(cid:96)

(cid:88)

i∈I(cid:96)

(cid:88)

(cid:110)

ψ(Zi, βt, ˆγi(cid:96), ˆλi(cid:96)) − ψ(Zi, βt, γi, λi)

(cid:111)

i∈I(cid:96)
(cid:26)

ˆγi(cid:96) − γi − E[ˆγi(cid:96) − γi|Z c

(cid:96) ] + Kh(Ti − t)λi(γi − ˆγi(cid:96)) − E(cid:2)Kh(Ti − t)λi(γi − ˆγi(cid:96))(cid:12)

(cid:12)Z c
(cid:96)

(cid:27)

(cid:3)

Kh(Ti − t)(ˆλi(cid:96) − λi)(Yi − γi) − E(cid:2)Kh(Ti − t)(ˆλi(cid:96) − λi)(Yi − γi)(cid:12)

(cid:12)Z c
(cid:96)

(R1-1)

(R1-2)

(cid:27)
(cid:3)

E[(ˆγi(cid:96) − γi)(1 − Kh(Ti − t)λi)|Z c

(cid:96) ] + E[(ˆλi(cid:96) − λi)Kh(Ti − t)(Yi − γi)|Z c
(cid:96) ]

(cid:27)

(cid:115)

(cid:115)

+

+

hdt
n(cid:96)

hdt
n(cid:96)

(cid:26)

(cid:88)

i∈I(cid:96)

(cid:26)

(cid:88)

i∈I(cid:96)

(cid:115)

−

hdt
n(cid:96)

(cid:88)

i∈I(cid:96)

Kh(Ti − t)(cid:0)ˆλi(cid:96) − λi

(cid:1)(cid:0)ˆγi(cid:96) − γi

(cid:1).

(R1-DR)

(R2)

The remainder terms (R1-1) and (R1-2) are stochastic equicontinuous terms that are controlled
to be op(1) by the mean square consistency conditions in Assumption 3(i) and cross-ﬁtting. The
second-order remainder term (R2) is controlled by Assumption 3(ii).

The remainder term (R1-DR) is controlled by the doubly robust property. Note that in the
binary treatment case when Kh(Ti − t) is replaced by 1{Ti = t}, the term (R1-DR) is zero because
ψ is the Neyman-orthogonal inﬂuence function. In our continuous treatment case, the Neyman
(cid:0)E[(ˆγ(cid:96)(t, X) −
orthogonality holds as h → 0. Under the conditions in Theorem 1, (R1-DR) is Op
√
γ(t, X))|Z c

nh4+dt(cid:1) = op(1).
We show that by a standard algebra of kernel and the assumed conditions, for any t ∈ T ,
Kh(s − t)fT X(s, x)ds = fT X(t, x) + O(h2) uniformly in x ∈ X . We will use the same arguments
T0
in the proofs. We use change of variables u = (u1, u2, ..., udt) = (T − t)/h, a Taylor expansion, the
mean value theorem where ¯t is between t and t + uh, and the second derivatives of fT X(t, x) being

(cid:96) ] + E[(ˆλ(cid:96)(t, X) − λ(t, X))|Z c
(cid:96) ])

(cid:82)

39

bounded uniformly over (t, x) ∈ T × X to show that

(cid:90)

Kh(s − t)fT X(s, x)ds

T0
(cid:90)

=

Rdt

(cid:90)

=

Rdt

Πdt

j=1k(uj)fT X(t + uh, x)du

(cid:18)

Πdt

j=1k(uj)

fT X(t, x) + h

dt(cid:88)

j=1

uj

∂fT X(t, x)
∂tj

+

h2
2

dt(cid:88)

u2
j

∂2fT X(t, x)
∂t2
j

j=1
= fT X(t, x) + O(h2)

(cid:12)
(cid:12)
(cid:12)t=¯t

+

h2
2

dt(cid:88)

dt(cid:88)

j=1

l=1,l(cid:54)=j

ujul

∂2fT X(t, x)
∂tj∂tl

(cid:19)

(cid:12)
(cid:12)
(cid:12)t=¯t

du1 · · · dudt

(13)

for any t ∈ T , uniformly over x ∈ X .

Let ∂ν

t g(t, ·) ≡ ∂νg(t, ·)/∂tν denote the νth partial derivative of a generic function g with

respect to t, and ∂t ≡ ∂1
t .

Proof of Theorem 1 The proof modiﬁes Assumptions 4 and 5 and extends Lemma A1, Lemma
12, and Theorem 13 in CEINR. The statements in the following proof hold for i ∈ I(cid:96), (cid:96) ∈ {1, ...., L},
and for all t. Deﬁne ∆i(cid:96) = ˆγi(cid:96) − γi − E [ˆγi(cid:96) − γi|Z c
(cid:96) and
(cid:96) ] = 0 and E[∆i(cid:96)∆j(cid:96)|Z c
Zi for i ∈ I(cid:96), E[∆i(cid:96)|Z c
X (ˆγ(cid:96)(t, x) −
(cid:17)2 (cid:12)
γ(t, x))2fX(x)dx) = op(1) by Assumptions 1(ii) and 3(i). Then E
(cid:12)
(cid:12)Z c
=
(cid:104)
X (ˆγ(cid:96)(t, x)−γ(t, x))2fX(x)dx(cid:1) = op(1). The conditional Markov’s
(cid:0)hdt (cid:82)
(hdt/n) (cid:80)
E
∆2
i(cid:96)
inequality implies that (cid:112)hdt/n (cid:80)

(cid:96) ]. By construction and independence of Z c
i(cid:96)|Z c
(cid:20)(cid:16)(cid:112)hdt/n (cid:80)

(cid:96) ] = Op(hdt (cid:82)
∆i(cid:96)

(cid:96) ] = 0 for i, j ∈ I(cid:96). hdtE[∆2

(cid:12)
(cid:12)
(cid:12)Z c

= Op

∆i(cid:96) = op(1).

i∈I(cid:96)

i∈I(cid:96)

The analogous results hold for ∆i(cid:96) = Kh(Ti − t)λi(γi − ˆγi(cid:96)) − E [Kh(Ti − t)λi(γi − ˆγi(cid:96))|Z c

(cid:96) ] in
(R1-1) and ∆i(cid:96) = Kh(Ti − t)(ˆλi(cid:96) − λi)(Yi − γi) − E(cid:2)Kh(Ti − t)(ˆλi(cid:96) − λi)(Yi − γi)(cid:12)
(cid:3) in (R1-2). In
particular for (R1-2), a standard algebra using change of variables, a Taylor expansion, the mean

(cid:12)Z c
(cid:96)

i∈I(cid:96)

(cid:21)

(cid:105)

(cid:96)

(cid:96)

40

value theorem, and Assumption 1 yields that

hdtE[∆2

≤ hdtE
(cid:90)

= hdt

i(cid:96)|Z c
(cid:96) ]
(cid:104)
Kh(Ti − t)2(ˆλi(cid:96) − λi)2(Yi − γi)2|Z c
(cid:96)
(cid:90)
Kh(s − t)2 (cid:16)ˆλ(cid:96)(t, x) − λ(t, x)
(cid:17)2
j=1k(uj)2 (cid:16)ˆλ(cid:96)(t, x) − λ(t, x)
Πdt

(cid:17)2

T0

X

(cid:105)

=

=

(cid:90)

(cid:90)

X

Rdt

(cid:90)

(cid:90)

Rdt

X
(cid:32)

E (cid:2)(Y − γ(t, x))2 (cid:12)

(cid:12)T = s, X = x(cid:3) fT X(s, x)dsdx

E (cid:2)(Y − γ(t, x))2 (cid:12)

(cid:12)T = t + uh, X = x(cid:3) fT X(t + uh, x)dudx

(cid:32)

Πdt

j=1k(uj)2

var(Y |T = t, X = x) +

dt(cid:88)

j=1

ujh

∂
∂tj

var(Y |T = t, X = x)(cid:12)

(cid:33)

(cid:12)t=¯t

×

fT X(t, x) +

dt(cid:88)

j=1

ujh

∂
∂tj

(cid:33)

fT X(t, x)(cid:12)

(cid:12)t=`t

(cid:16)ˆλ(cid:96)(t, x) − λ(t, x)

(cid:17)2

du

dx

= Op

(cid:16)

Rdt
k

(cid:90)

X

(cid:0)ˆλ(cid:96)(t, x) − λ(t, x)(cid:1)2fT X(t, x)dx

(cid:17)

,

(14)

where ¯t and `t are between t and t + uh. So hdtE[∆2
(R1-1)= op(1) and (R1-2)= op(1).

i(cid:96)|Z c

(cid:96) ] = op(1) by Assumption 3(i). Then

For (R2),

E

(cid:20)(cid:12)
(cid:12)
(cid:12)

(cid:112)hdt/n(cid:96)

(cid:88)

(cid:12)
Kh(Ti − t)(ˆλi(cid:96) − λi)(γi − ˆγi(cid:96))
(cid:12)
(cid:12)

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z c
(cid:96)

i∈I(cid:96)
(cid:90)

(cid:90)

X
(cid:16) (cid:90)

T0
(cid:90)

(cid:112)

n(cid:96)hdt

(cid:112)

n(cid:96)hdt

≤

≤

(cid:12)
(cid:12)
(cid:12)(ˆλ(cid:96)(t, x) − λ(t, x))(γ(t, x) − ˆγ(cid:96)(t, x))
(cid:12)
(cid:12)
(cid:12)Kh(s − t)fT X(s, x)dsdx

(ˆλ(cid:96)(t, x) − λ(t, x))2Kh(s − t)fT X(s, x)dsdx

(cid:17)1/2

(cid:16) (cid:90)

(cid:90)

×

X

T0

(γ(t, x) − ˆγ(cid:96)(t, x))2Kh(s − t)fT X(s, x)dsdx

(cid:17)1/2

X

(cid:112)

n(cid:96)hdt

=

T0
(cid:16) (cid:90)

X

= op(1)

(ˆλ(cid:96)(t, x) − λ(t, x))2fT X(t, x)dx

(cid:17)1/2(cid:16) (cid:90)

X

(ˆγ(cid:96)(t, x) − γ(t, x))2fT X(t, x)dx

(cid:17)1/2

+ op(h2)

(15)

by Cauchy-Schwartz inequality and Assumption 3(ii), and (13). So (R2)= op(1) follows by the
conditional Markov’s and triangle inequalities.

For (R1-DR), in the ﬁrst part E(cid:2)1 − Kh(Ti − t)λi

(cid:12)
(cid:12)Xi

(cid:3) = E(cid:2)fT |X(t|Xi) − Kh(Ti − t)(cid:12)

(cid:12)Xi

(cid:3)λi =

(cid:82) ∞
−∞ u2k(u)du/2+Op(h3). A similar argument yields (R1-DR)= Op((E[(ˆγ(cid:96)(t, X)−

√

(cid:96) ] + E[(ˆλ(cid:96)(t, X) − λ(t, X))|Z c
(cid:96) ])

nhdth2) = op(1).

By the triangle inequality, we obtain the asymptotically linear representation
nhdtn−1 (cid:80)n

(cid:0) ˆψ(Zi, βt, ˆγt, ˆλt) − ψ(Zi, βt, γt, λt)(cid:1) = op(1).

i=1

h2∂2fT |X(t|Xi)/∂t2λi
γ(t, X))|Z c
√

41

For Bt, E

(cid:105)
(cid:104) Kh(T −t)
fT |X (t|X) (Y − γ(t, X))
dard algebra for kernel as (13) yields

(cid:104)

= E

1
fT |X (t|X)

(cid:105)
E [Kh(T − t) (γ(T, X) − γ(t, X)) |X]

. A stan-

E [Kh(T − t) (γ(T, x) − γ(t, x)) |X = x]

(cid:90)

T0

(cid:90)

Kh(s − t) (γ(s, x) − γ(t, x)) fT |X(s|x)ds

k(u) (γ(t + uh, x) − γ(t, x)) fT |X(t + uh|x)du

=

=

=

Rdt

(cid:90)

Rdt
(cid:32)

k(u1) · · · k(udt)

(cid:32)

dt(cid:88)

j=1

huj∂tj γ(t, x) +

(cid:33)

h2
2

j ∂2
u2
tj

γ(t, x) +

h2
2

dt(cid:88)

l=1,l(cid:54)=j

(cid:33)

ujul∂tj ∂tlγ(t, x)

×

fT |X(t|x) +

ujh∂tj fT |X(t|x)

du1 · · · dudt + O(h3)

dt(cid:88)

j=1

= h2

(cid:90) ∞

−∞

u2k(u)du

dt(cid:88)

j=1

(cid:18)

∂tj γ(t, X)∂tj fT |X(t|x) +

γ(t, x)fT |X(t|x)

(cid:19)

+ O(h3)

1
2

∂2
tj

uniformly over x ∈ X . Thus

E [Kh(T − t) (γ(T, X) − γ(t, X)) |X]

(cid:21)

(cid:20)

E

1
fT |X(t|X)
(cid:90) ∞

= h2

u2k(u)du

−∞

j=1

dt(cid:88)

(cid:20)
∂tj γ(t, X)

E

∂tj fT |X(t|X)
fT |X(t|X)

+

1
2

∂2
tj

(cid:21)

γ(t, X)

+ O(h3).

The asymptotic variance is determined by hdtE

(cid:104)(cid:0)(Y − γ(t, X))Kh(Ti − t)/fT |X(t|X)(cid:1)2(cid:105)

. A

standard algebra for kernel as (14) yields Vt.

√

Asymptotic normality follows from the Lyapunov central limit theorem with the third ab-
solute moment. Speciﬁcally, E(cid:2)(cid:12)
nhdtn−1ψ(Zi, βt, γt, λt)(cid:12)
3(cid:3) = O(cid:0)(n−1hdt)3/2E(cid:2)Kh(T − t)3|Y −
(cid:12)
(cid:12)
γ(t, X)|3fT |X(t|X)−3(cid:3)(cid:1) = O((n3hdt)−1/2), by the same arguments as (14) under the condition
that E[|Y − γ(T, X)|3|T = t, X] and its ﬁrst derivative with respect to t are bounded uniformly
i=1 var(cid:0)√
nhdtn−1ψ(Zi, βt, γt, λt)(cid:1) = hdtvar(ψ) = Vt + o(1). Thus the
in x ∈ X . Let s2
√
E(cid:2)(cid:12)
Lyapunov condition holds: (cid:80)n
(cid:3)
(cid:12)

nhdtn−1ψ(Zi, βt, γt, λt)(cid:12)
(cid:12)

n = O((nhdt)−1/2) = o(1).

n ≡ (cid:80)n

3(cid:3)/s3

i=1

Proof of Theorem 2 By Theorem 1, the asymptotic MSE is h4B2
ﬁrst-order conditions yields the optimal bandwidth h∗
t .

t + Vt/(nhdt). Solving the

Given the consistency of ˆVt and ˆBt, the plug-in estimator ˆht for the optimal bandwidth h∗

t is

consistent. Below we show the consistency of ˆVt and ˆBt.

Consistency of ˆVt. Let ˆVt = L−1 (cid:80)L
i(cid:96). It suﬃces to show
that ˆVt(cid:96) is consistent for Vt as n(cid:96) → ∞, for (cid:96) = 1, ..., L. Toward that end, we show that

ˆVt(cid:96), where ˆVt(cid:96) ≡ hdtn−1

ˆψ2

(cid:80)

i∈I(cid:96)

(cid:96)=1

(cid:96)

42

(cid:80)
(cid:80)

(cid:96)

i∈I(cid:96)

(I) hdtn−1
(II) hdtn−1
i − E[ ˆψ2
ψ2

ψ2
i − Vt = op(1), where ψi ≡ Kh(Ti − t)(Yi − γ(t, Xi))/fT |X(t|Xi) + γ(t, Xi) − βt,
E[ ˆψ2
i(cid:96) −
(cid:96)
i∈I(cid:96)
i |Z c
i(cid:96) − ψ2
(cid:96) ].

∆i(cid:96) = op(1), where ∆i(cid:96) ≡ ˆψ2

(cid:96) ] = op(1), and (III) hdtn−1

i(cid:96) − ψ2

i |Z c

(cid:80)

i∈I(cid:96)

(cid:96)

For (I), as computed in the proof of Theorem 1, hdtE[ψ2

i ] = Vt + o(1). By similar arguments in

(14) and Assumption 4(iii)(iv), E[ψ4
o(h−3dt) = O(h−3dt). Then by the Markov’s inequality, for any (cid:15) > 0, P (|hdtn−1
E[ψ4
(cid:15)) ≤ (cid:15)−2h2dtn−1

i ] = h−3dtE[E[(Y −γ(t, X))4|T = t, X]/fT |X(t|X)3]((cid:82) ∞
−∞ k(u)4du)dt+
ψ2
i − Vt| >

(cid:80)

i∈I(cid:96)

i ]) = O(n−1

(cid:96) h−dt) = o(1).

i ) = O(h2dtn−1
(cid:96)

(cid:96) var(ψ2

(cid:96)

For (II), ﬁrst compute

hdtE[ ˆψ2

i(cid:96)|Z c

(cid:96) ] = E[E[(Y − ˆγ(cid:96)(t, X))2|T = t, X, Z c

(cid:96) ]fT |X(t|X)/ ˆf(cid:96)(t|X)2|Z c

(cid:96) ]Rdt

k + op(1).

To show (II), it suﬃces to show that

(cid:34)

E[(E[(Y − ˆγ(cid:96)(t, X))2|T = t, X, Z c
(Y − ˆγ(cid:96)(t, X))2
ˆf(cid:96)(t|X)2

(cid:96) ]fT |X(t|X)/ ˆf(cid:96)(t|X)2|Z c
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(Y − γ(t, X))2
fT |X(t|X)2

T = t, X, Z c
(cid:96)

= E

−

E

(cid:34)

(cid:96) ] − E[E (cid:2)(Y − γ(t, X))2|T = t, X(cid:3) /fT |X(t|X)]
(cid:35)

(cid:35)

(cid:12)
(cid:12)
fT |X(t|X)
(cid:12)
(cid:12)

Z c
(cid:96)

= op(1).

To simplify notations without loss of clarity, let λ ≡ 1/fT |X(t|X), ˆλ ≡ 1/ ˆf(cid:96)(t, X), γ ≡ γ(t, X),

and ˆγ ≡ ˆγ(t, X). We decompose

Y − ˆγ(cid:96)(t, X)
ˆf(cid:96)(t|X)

Therefore

≡ (Y − ˆγ)ˆλ = (Y − γ)λ + (γ − ˆγ)λ + (Y − γ)(ˆλ − λ) + (γ − ˆγ)(ˆλ − λ).

(16)

(cid:34)

(cid:34)

E

E

(Y − ˆγ(cid:96)(t, X))2
ˆf(cid:96)(t|X)2

−

(Y − γ(t, X))2
fT |X(t|X)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

T = t, X, Z c
(cid:96)

(cid:35)

(cid:35)

(cid:12)
(cid:12)
fT |X(t|X)
(cid:12)
(cid:12)

Z c
(cid:96)

(cid:20)
= E

(cid:20) (cid:16)

E

(Y − γ)λ + (γ − ˆγ)λ + (Y − γ)(ˆλ − λ) + (γ − ˆγ)(ˆλ − λ)

(cid:17)2

− ((Y − γ)λ)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

T = t, X, Z c
(cid:96)

(cid:21)

fT |X(t|X)

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z c
(cid:96)

is op(1) by Assumption 3 and Assumption 4(i).

For (III), E[∆i(cid:96)|Z c

(cid:96) ] = 0. By (16), Assumption 4(ii)(iii), and using the similar arguments as

for E[ψ4

i ] in (I), we obtain E[∆2

i(cid:96)|Z c

Op(h−3dt). Then var(hdtn−1
follows by the conditional Markov’s inequality.

i∈I(cid:96)

(cid:96)

(cid:80)

(cid:18)

(cid:20)

E

(cid:96) ] = Op
∆i(cid:96)|Z c

(cid:18)

Kh(T − t)4

(Y −ˆγ(cid:96)(t,X))2

ˆf(cid:96)(t|X)2 − (Y −γ(t,X))2

fT |X (t|X)2

(cid:21)(cid:19)

(cid:19)2(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z c
(cid:96)

=

(cid:96) ) = Op(h2dtn−1

(cid:96) h−3dt) = Op(n−1

(cid:96) h−dt) = op(1). The result

43

Consistency of ˆBt. Theorem 1 provides E[ ˆβt,ab] = b2a2Bt+o(b2) and E[ˆBt] = Bt+o(1). Compute

(cid:3)

E(cid:2) ˆβt,b
ˆβt,ab
(cid:18)

= O

n−1E

(cid:32)

(cid:20)(cid:90)

T0

= O

n−1b−dta−dt

= O(cid:0)(nbdt)−1(cid:1)

Kb(s − t)Kab(s − t)E (cid:2)(Y − ˆγ(cid:96)(t, X))2(cid:12)

(cid:12)T = s, X, Z c
(cid:96)

(cid:3) fT |X(s|X)ds/ ˆf(cid:96)(t|X)2

(cid:21)(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z c
(cid:96)

(cid:90)

Rdt

Πdt

j=1k(uj)k

(cid:34)

(cid:17)

(cid:16) uj
a

duE

E (cid:2)(Y − ˆγ(cid:96)(t, X))2(cid:12)

(cid:12)T = t, X, Z c
(cid:96)

(cid:35)(cid:33)

(cid:3) fT |X(t|X)
ˆf(cid:96)(t|X)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z c
(cid:96)

by the same arguments as (13). So cov( ˆβt,b, ˆβt,ab) = O(var( ˆβt,ab)) = O(cid:0)(nbdt)−1(cid:1). It follows that
var(ˆBt) = b−4(1 − a2)−2(cid:0)var( ˆβt,b) + var( ˆβt,ab) − 2cov( ˆβt,b, ˆβt,ab)(cid:1) = O(b−4(nbdt)−1).

By the Markov’s inequality and Assumption 4(iv), P (|ˆBt − E[ˆBt]| > (cid:15)) ≤ var(ˆBt)/(cid:15)2 =
(cid:3)

O((nbdt+4)−1) = o(1). So ˆBt − Bt = op(1).

√

Proof of Theorem 3 We decompose ˆθt−θt = (ˆθt−θtη)+(θtη −θt), where θtη ≡ (βt+ −βt−)/η. By
a Taylor expansion of βt+ around βt and the mean value theorem, βt+ = βt + ∂t1βtη/2 + ∂2
t1β¯tη2/4
for some ¯t between t and t+. Take the same expansion of βt− around βt. As we assume ∂2
t1βt is
nhdt+2(θtη − θt) =
uniformly bounded, the second part θtη − θt = O(η). We assume that in (i)
nhdt+2η) = o(1) under η/h → 0, and in (ii) (cid:112)nhdtη2(θtη − θt) = O((cid:112)nhdtη2η) = o(1) under
O(
η/h → ρ ∈ (0, ∞].

Let ˆβt − βt = n−1 (cid:80)L

(cid:1), where ψti ≡ ψ(Zi, βt, γi, λi),
ˆψti(cid:96) ≡ ψ(Zi, βt, ˆγi(cid:96), ˆλi(cid:96)), and the remainder terms in Rti(cid:96) are deﬁned in (R1-1), (R1-2), (R1-DR),
(cid:1). Denote ft|Xi ≡ fT |X(t|Xi).
and (R2). Thus ˆθt−θtη = η−1n−1 (cid:80)L

(cid:0)ψt+i−ψt−i+Rt+i(cid:96)−Rt−i(cid:96)

ˆψti(cid:96) = n−1 (cid:80)L

(cid:0)ψti + Rti(cid:96)

(cid:80)

(cid:80)

(cid:80)

i∈I(cid:96)

i∈I(cid:96)

√

(cid:96)=1

(cid:96)=1

(cid:96)=1

i∈I(cid:96)

(i) Consider the case when η/h → 0. By Taylor expansions of ψt+i and ψt−i around ψt respec-
tively and the mean value theorem, η−1n−1 (cid:80)n
t1ψˇti)/(233!), where ˜t ∈ (t, t + η/2) and ˇt ∈ (t − η/2, t). We ﬁrst show that var(cid:0)n−1 (cid:80)n
∂3
n−1E(cid:2)(∂t1ψti)2(cid:3) − n−1(cid:0)E[∂t1ψti](cid:1)2 = O(cid:0)n−1h−(dt+2) + n−1h4(cid:1) = O(cid:0)n−1h−(dt+2)(cid:1).

i=1 ∂t1ψti+η2n−1 (cid:80)n

i=1(∂3
i=1 ∂t1ψti

(cid:1) = n−1 (cid:80)n

(cid:0)ψt+i−ψt−i

t1ψ˜ti+
(cid:1) =

i=1

To show that E[∂t1ψti] = O(h2), observe that ∂Kh(T − t)/∂t1 = −∂Kh(T − t)/∂T1.

E[∂t1ψti] = E

(cid:20)

(cid:26)

∂t1

Kh(Ti − t)

(cid:27)

Yi − γ(t, Xi)
ft|Xi

+ ∂t1γ(t, Xi) − θt

(cid:21)

(cid:20)

(cid:26)

= −E

∂T1

Kh(Ti − t)

γ(Ti, Xi) − γ(t, Xi)
ft|Xi

(cid:27)(cid:21)

(cid:21)

∂T1 {Kh(s − t) (γ(s, Xi) − γ(t, Xi))} fT |X(s|Xi)ds/ft|Xi

.

(cid:20)(cid:90)

= −E

T0

Let dt = 1 here for simplicity without loss of generality. Using integration by parts, k with a
bounded support, change of variables, a Taylor expansion and the mean value theorem, then

44

uniformly over x ∈ X ,

(cid:90)

(cid:8)Kh(s − t)(cid:0)γ(s, x) − γ(t, x)(cid:1)(cid:9)fT |X(s|x)ds

∂T1

T0

= −

(cid:90)

Kh(s − t)(cid:0)γ(s, x) − γ(t, x)(cid:1)∂T1fT |X(s|x)ds

T0
(cid:90) ∞

= −

−∞

= O(h2),

k(u)(cid:0)uh∂t1γ(t, x) + u2h2∂2

t1γ(t, x)/2 + u3h3∂t1γ(¯t, x)/3!(cid:1)

× (cid:0)ft|x + uh∂t1ft|x + u2h2∂2

t1f`t|x/2(cid:1)du

where ¯t and `t are between t and t + uh. The same arguments yield E(cid:2)∂3

(cid:3) = O(h2).

t1ψti

By change of variables u = (u1, u2, ..., udt) = (T − t)/h,
(cid:104)(cid:0)∂t1Kh(T − t)(Y − γ(t, X))/ft|X
E
= E (cid:2)h−2dt−2k(cid:48)((T1 − t1)/h)2Πdt

(cid:1)2(cid:105)

j=2k((Tj − tj)/h)2E(cid:2)(Y − γ(t, X))2(cid:12)

k(cid:48)(u1)2Πdt

j=2k(uj)2E(cid:2)(Y − γ(t, X))2(cid:12)

(cid:12)T, X(cid:3)/f 2
(cid:12)T = t + uh, X(cid:3)fT |X(t + uh|X)du/f 2

t|X

(cid:3)

t|X

(cid:21)

(cid:20)
h−dt−2

(cid:90)

= E

= O (cid:0)h−dt−2Vθ

Rdt
(cid:1)

t

by the same arguments as (14) using (cid:82)
term of var(cid:0)n−1 (cid:80)n

i=1 ∂t1ψti

(cid:1) is n−1h−dt−2Vθ
t .

R k(cid:48)(u)2du < ∞ implied by Assumption 2. Thus the leading

By the same arguments and (cid:82) ∞

(cid:1) =
t1ψti)2(cid:3)(cid:1) = O(cid:0)η4n−1h−(dt+6)(cid:1) = O(cid:0)(η/h)4(cid:1) × O(cid:0)n−1h−(dt+2)(cid:1). Thus by η/h → 0, the

−∞ k(cid:48)(cid:48)(cid:48)(u)2du < ∞, we can derive var(cid:0)η2n−1 (cid:80)n

i=1 ∂3

t1ψti

O(cid:0)η4n−1E(cid:2)(∂3
leading term of the variance of

√

nhdt+2η−1n−1 (cid:80)n

i=1

(cid:0)ψt+i − ψt−i
nhdt+2η−1n−1 (cid:80)L

(cid:1) is Vθ
t .
(cid:80)

√

We follow the proof of Theorem 1 to control

i∈I(cid:96)
The conditions (a) and (b) give a crude bound such that hη−1(cid:112)hdt/n (cid:80)L
t = t+, t− respectively.

(cid:96)=1

(cid:0)Rt+i(cid:96) −Rt−i(cid:96)
(cid:80)

(cid:1) = op(1).
Rti(cid:96) = op(1) for

(cid:96)=1

i∈I(cid:96)

For the bias Bθ

t , we use ∂Kh(T − t)/∂t1 = −∂Kh(T − t)/∂T1, integration by parts, and the

45

same arguments in (13). Then uniformly in x ∈ X ,

(cid:90)

(cid:26)

∂t1Kh(s − t)

γ(s, x) − γ(t, x)
ft|x

+ Kh(s − t)∂t1

(cid:18)γ(s, x) − γ(t, x)
ft|x

(cid:19)(cid:27)

fs|xds

(cid:40)

Kh(s − t)

∂T1γ(s, x)
ft|x

fs|x +

γ(s, x) − γ(t, x)
ft|x

∂T1fs|x

∂t1γ(t, x)
ft|x

fs|x −

γ(s, x) − γ(t, x)
f 2
t|x

(cid:41)

∂t1ft|Xfs|x

ds

(cid:26)(cid:18)

ft|x +

(cid:26)

∂tj ft|Xujh + ∂2
tj

dt(cid:88)

j=1

ft|x

j h2
u2
2

+

dt(cid:88)

l=1,l(cid:54)=j

∂tj ∂tlft|xujul

(cid:27)(cid:19)

h2
2

∂tj ∂t1γ(t, x)ujh + ∂2
tj

∂t1γ(t, x)

u2
j h2
2

+

dt(cid:88)

l=1,l(cid:54)=j

∂tj ∂tl∂t1γ(t, x)ujul

(cid:27)(cid:19)

h2
2

(∂tj γ(t, x)ujh + ∂2
tj

γ(t, X)

j h2
u2
2

+

dt(cid:88)

l=1,l(cid:54)=j

∂tj ∂tlγ(t, x)ujul

(cid:27)(cid:19)

h2
2

T0
(cid:90)

T0

=

−

(cid:90)

=

×

+

Rdt
(cid:18) dt(cid:88)

(cid:26)

j=1

(cid:18) dt(cid:88)

(cid:26)

j=1

(cid:18)

×

∂t1ft|x +

dt(cid:88)

j=1

∂tj ∂t1ft|xujh −

(cid:16)

ft|x +

dt(cid:88)

j=1

∂tj ft|xujh

(cid:17)∂t1ft|x
ft|x

(cid:19)(cid:27) 1
ft|x

k(u1) · · · k(udt)du + O(h3)

dt(cid:88)

= h2

j=1
(cid:90) ∞

−∞

×

(cid:18) 1
2

∂2
tj

∂t1γ(t, x) + ∂tj ∂t1γ(t, x)

∂tj ft|x
ft|x

+

∂tj γ(t, x)
ft|x

(cid:18)

∂tj ∂t1ft|x − ∂tj ft|x

(cid:19)(cid:19)

∂t1ft|x
ft|x

u2k(u)du + O(h3).

Asymptotic normality follows from the Lyapunov central limit theorem with the third absolute

moment. Speciﬁcally, a standard algebra as in the proof of Theorem 1 yields E

3(cid:105)

(cid:12)
(cid:12)

t)(Y − γ(t, X))/ft|X
γ(t, X))/ft|X
holds: (cid:80)n

= O((n3hdt)−1/2). Let s2
(cid:1) = hdt+2var(∂t1Kh(T −t)(Y −γ(t, X))/ft|X) = Vθ
E(cid:2)(cid:12)
nhdt+2n−1∂t1Kh(T − t)(Y − γ(t, X))/ft|X
(cid:12)

√

(cid:12)
(cid:12)

n ≡ (cid:80)n

3(cid:3)/s3

i=1

i=1 var(cid:0)√

nhdt+2n−1∂t1Kh(T − t)(Y −
t +o(1). So the Lyapunov condition

n = O((nhdt)−1/2) = o(1).

√

(cid:104)(cid:12)
(cid:12)

nhdt+2n−1∂t1Kh(T −

√

nhdtn−1 (cid:80)n

(cid:0)ψt+i−ψt−i

(ii) Consider the case when η/h → ρ ∈ (0, ∞]. (cid:112)nhdtη2(ˆθt − θtη) =
βt−)(cid:1) =
(ψt+i − ψt−i)(cid:1) = hdt(cid:0)var(ψt+i) + var(ψt−i) − 2cov(ψt+i, ψt−i)(cid:1) = Vθ
convolution kernel in Vθ
where T2 is a (dt −1)×1 vector. We use changes of variables u1 = (T1 −t−
u = (u1, u(cid:48)

nhdt(cid:0) ˆβt+ − ˆβt− − (βt+ −
nhdtn−1 (cid:80)n
i=1
t + o(1). The term involved the
2)(cid:48),
1 )/h, u2 = (T2 −t2)/h, and
2)(cid:48) in the integrations below. In the proof of Theorem 1, we show that E[ψti] = O(h2)

(cid:1)+op(1) by Theorem 1. We show below that var(cid:0)√
t comes from cov(ψt+i, ψt−i)(cid:1) derived in the following. Let T = (T1, T (cid:48)

i=1

√

46

for Bt. So hdtcov(ψt+i, ψt−i)(cid:1) = hdtE(cid:2)ψt+iψt−i

(cid:3) + O(hdt+4) is bounded by the order of

(cid:20)

hdtE

Kh(T − t+)Kh(T − t−)

(Y − γ(t+, X))(Y − γ(t−, X))
ft+|Xft−|X

(cid:21)

= hdtE

(cid:20) (cid:90)

= E

(cid:20)
Kh(T1 − t+

1 )Kh(T1 − t−

1 )Kh(T2 − t2)2E(cid:2)(Y − γ(t+, X))(Y − γ(t−, X))(cid:12)

(cid:12)T, X(cid:3)

(cid:21)

1
ft+|Xft−|X

(cid:0)E[Y 2|T = t− + uh, X] − γ(t− + uh, X)(γ(t+, X) + γ(t−, X)) + γ(t+, X)γ(t−, X)(cid:1)

Rdt

(cid:16)

× k(u1)k

= ¯k (ρ) Rdt−1

k

(cid:17)

η
h

u1 −

j=1 k(u2j)2 fT |X(t− + uh|X)
Πdt−1
E (cid:2)var(Y |T = t, X)/fT |X(t|X)(cid:3) + O(h).

ft+|Xft−|X

du1du2

(cid:21)

Therefore var(cid:0)(cid:112)nhdtη2(ˆθt − θtη)(cid:1) = hdt(cid:0)var(ψt+i) + var(ψt−i) − 2cov(ψt+i, ψt−i)(cid:1) = E(cid:2)var(Y |T =
t, X)/fT |X(t|X)(cid:3)(cid:0)2Rdt
(cid:1) + o(1). Note that in (i) when η/h → ρ = 0, since ¯k (0) = Rk,
var(cid:0)(cid:112)nhdtη2(ˆθt − θtη)(cid:1) = o(1).

k − 2¯k(ρ)Rdt−1

k

The asymptotic normality follows from the proof of Theorem 1.

(cid:3)

Proof of Theorem 4 Let f∗b = f∗tb ≡ arg minf E[(cid:96)tb(f, Z)] = arg minf E(cid:2) (cid:82)
(γ(s, X) − f (X))2
Kb(s − t)fT |X(s|X)ds(cid:3) by the law of iterated expectations. For any x ∈ X , t ∈ T , and b > 0,
(cid:82)
(γ(s, x) − f (x))2Kb(s − t)fT |X(s|x)ds is a quadratic function of f (x). Under the assump-
T0
tions, the second-order condition (cid:82)
γ(s, x)Kb(s −
T0
Kb(s − t)fT |X(s|x)ds is well-deﬁned. We can show f∗tb ∈ W β,∞([−1, 1]dx) ≡ (cid:8)f :
t)fT |X(s|x)ds(cid:14) (cid:82)
maxα,|α|≤β ess supx∈[−1,1]dx |Dαf (x)| ≤ 1(cid:9), where Dαf is the weak derivative, as in Assumption
2 in FLM. Speciﬁcally, Assumption 5 and the Leibniz integral rule imply that |Dα
xfT |X(t, x)| and
|Dα

xf∗tb(x)| are bounded uniformly over x ∈ X , t ∈ T , and b > 0. Note that f∗tb /∈ FDN N .

Kb(s − t)fT |X(s|x)ds > 0. Thus f∗tb(x) = (cid:82)

T0

T0

T0

Let fn = fntb ≡ arg minf ∈FDN N ,(cid:107)f (cid:107)∞≤2M (cid:107)f − f∗b(cid:107)∞, and (cid:15)n = (cid:15)ntb ≡ (cid:15)DN N ≡ (cid:107)fn − f∗b(cid:107)∞.
Since we show the pointwise results for t ∈ T , to follow the notations in FLM for easy reference,
we suppress the subscript t (and b) in the proof when there is no confusion.

Let the bounded kernel function k() < ¯k for some constant ¯k > 0.
We modify equation (2.1) in FLM to the following:

|(cid:96)tb(f, Z) − (cid:96)tb(g, Z)| ≤ Kb (T − t) M |f (X) − g(X)| ≤
2 (E[(cid:96)tb(f, Z)] − E[(cid:96)tb(f∗b, Z)]) = (cid:107)f − γ(cid:107)2

+ O(b2).

FtX

¯kdt
bdt

M |f (X) − g(X)| ,

(2.1-1)

(2.1-2)

Lemma 8 in FLM and the bounded kernel k() imply the Lipschitz condition (2.1-1). The key of
our modiﬁcation is the condition (2.1-2) that replaces c1E[(f − f∗)2] ≤ E[(cid:96)(f, Z)] − E[(cid:96)(f∗, Z)] ≤
c2(cid:107) ˆf − f∗(cid:107)2
L2(X) in FLM’s (2.1). We prove (2.1-2) holds uniformly in uniformly bounded f at the
end of this proof. In the proof of Theorem 1 in FLM, the main decomposition in equation (A.1)
L2(X) ≤ E[(cid:96)( ˆf , Z)] − E[(cid:96)(f∗, Z)]. This
starts with the inequality in their equation (2.1): c1(cid:107) ˆf − f∗(cid:107)2
is the only place where this inequality is used. We modify it to (2.1-2) that implies (cid:107) ˆfb − γ(cid:107)2
=

FtX

47

(cid:16)

(cid:17)
E[(cid:96)tb( ˆfb, Z)] − E[(cid:96)tb(f∗b, Z)]

Op
E[(cid:96)tb( ˆfb, Z)] − E[(cid:96)tb(f∗b, Z)]. We modify (A.1) in FLM and bound

+ Op(b2). Thus we can bound (cid:107) ˆfb − γ(cid:107)2

FtX

using the bound of

E[(cid:96)tb( ˆfb, Z)] − E[(cid:96)tb(f∗b, Z)]
≤ (E − En)[(cid:96)tb( ˆfb, Z)] − E[(cid:96)tb(f∗b, Z)] + En[(cid:96)tb( ˆfn, Z) − (cid:96)tb(f∗b, Z)].

(17)

c2(cid:107) ˆf − f∗(cid:107)2
(cid:96)tb(f∗b, Z)] = Op((cid:107)fn − γ(cid:107)2

To bound the second bias term, FLM only use the inequality in (2.1) E[(cid:96)(f, Z)] − E[(cid:96)(f∗, Z)] ≤
L2(X) in the second inequality in their (A.2). We use (2.1-2) that implies E[(cid:96)tb(fn, Z) −

n + b2).
Next we modify the proof of Theorem 1 in FLM by replacing all ((cid:96), ˆf , f∗) with ((cid:96)tb, ˆfb, f∗b) for

) + O(b2) = Op((cid:15)2

FtX

any t ∈ T and b. We only point out the key modiﬁcations of their proof in the following.

(cid:0)M 2(cid:107)fn − f∗b(cid:107)2

∞/bdt(cid:1). Thus (A.2) in FLM is mod-

By (2.1-1), var[(cid:96)tb(fn, z) − (cid:96)tb(f∗b, z)] = Op
(cid:113) 2C2
(cid:96) ˜γ
nbdt + 7C(cid:96)M ˜γ
nbdt

n + (cid:15)n

iﬁed to c2(cid:15)2

.

Similarly the last equation on page 201 in FLM is modiﬁed to V[g] = E (cid:2)|(cid:96)tb(f, z) − (cid:96)tb(f∗b, z)|2(cid:3) ≤
E[(f − f∗b)2Kb(T − t)2] = O (cid:0)C 2
0/bdt(cid:1). Thus the statement for (A.7) in FLM is modiﬁed to
(cid:19)
6EηRnG +
.

C 2
(cid:96)
the following: we ﬁnd that (E−En)[(cid:96)tb( ˆf , z)−(cid:96)tb(f∗b, z)] = Op

(cid:96) M r2

(cid:113) 2C2

(cid:96) r2
0 ˜γ
nbdt + 23·3M C(cid:96)

˜γ
nbdt

(cid:18)

3

On page 202 of FLM, the bound of EnRnG is multiplied by b−dt. This is because in Lemma 2
and Lemma 3 in FLM, the Lipschitz condition is modiﬁed by |φ(f1)−φ(f2)| ≤ L|f1−f2|(¯k/b)dt.16 It

follows that (A.9) in FLM is modiﬁed to r0 ·

(cid:113) 2C2
(cid:96) ˜γ
nbdt +
nbdt . And the bound in (A.10) in FLM is multiplied by b−dt. Thus (A.14) in FLM is

(cid:113) W L log W
n

(cid:113) 2C(cid:96)˜γ
nbdt

log n +

+ c2(cid:15)2

n + (cid:15)n

(cid:19)

(cid:18)

bdt

K

√

C

˜γ

30M C(cid:96)
modiﬁed to

¯r =

8
c1

√

(cid:32)

K

(cid:114)

C

bdt

W L log W
n

log n +

(cid:114)

2C 2
(cid:96) ˜γ
nbdt

(cid:33)



(cid:115)

+



2(c2 ∨ 1)
c1

(cid:15)n +

(cid:114) 120M C(cid:96)
c1

˜γ
nbdt



 +

r∗
bdt

.

Therefore (A.17) in FLM is modiﬁed to

(cid:32)(cid:114)

C (cid:48)

W L log W
nb2dt

(cid:114)

log n +

log log n + γ
nbdt

+ (cid:15)n

(cid:33)

with some constant C (cid:48) > 0 that does not depend on n. Thus we can optimize the upper bound

16We might obtain a tighter bound by incorporating the kernel function in Lemma 2 and Lemma 3 in FLM. For
i=1 ηif (Xi)Kb(Ti − t)(cid:3).

example, in Lemma 2, we might bound the Rademacher complexity to 2LEη
Such extension is out of the scope of this paper.

(cid:2)supf ∈F n−1 (cid:80)n

48

on page 206 of FLM,



(cid:115)

¯r ≤ C (cid:48)




− 2dx
β
(cid:15)
n

(log(1/(cid:15)n) + 1)7

nb2dt

(cid:114)

log n +

log log n + γ
nbdt

+ (cid:15)n






by choosing (cid:15)n = (nb2dt)− β
we can bound (17)

2(β+dx) , H (cid:16) ·(nb2dt)

dx

2(β+dx) log2(nb2dt), L (cid:16) · log(nb2dt). Hence, w.p.a.1,

E[(cid:96)tb( ˆfb, Z)] − E[(cid:96)tb(f∗b, Z)] ≤ ¯r2 ≤ C

(cid:18)

(nb2dt)− β

β+dx log8 n +

log log n + γ
nbdt

(cid:19)

.

The remaining proof is to show (2.1-2). We add and subtract γ(T, X) to (Y − f (X)) in the loss
function, and by the law of iterated expectations, we obtain 2E[(cid:96)tb(f, Z)] = E[var(Y |T, X)Kb(T −
t)] + E[(γ(T, X) − f (X))2Kb(T − t)]. Since the ﬁrst term does not depend on f , we focus on the
second term Qb(f ) ≡ E[(γ(T, X) − f (X))2Kb(T − t)].

FtX

Let Q(f ) ≡ (cid:107)f − γ(cid:107)2

. We show that Qb(f ) = Q(f ) + b2B(f ) + o(b2) uniformly in uni-
formly bounded f , where B(f ) ≡ E(cid:2)(∂tγ)2ft|X + (γ − f )2∂2
t ft|X/2 + 2(γ − f )∂tγ∂tft|X + (γ −
(cid:3) (cid:82) u2k(u)du. By change of variables, a Taylor expansion, and the mean value theorem,
f )∂2
t γft|X
(cid:8)γ(t, X) − f (X) + ub∂tγ(t, X) +
Qb(f ) = E(cid:2) (cid:82)
t fT |X(˜t|X)/2(cid:9)du(cid:3), where ¯t and ˜t are be-
u2b2∂tγ(¯t, X)/2(cid:9)2k(u)(cid:8)fT |X(t|X) + ub∂tfT |X(t|X) + u2b2∂2
tween t and t + ub. Under Assumption 5(i), apply the dominated convergence theorem to the
terms associated with ∂tγ(¯t, X) and ∂2
t fT |X(t|X),
respectively, as b → 0.

T (γ(T, X) − f (X))2Kb(T − t)fT |X(T |X)dT (cid:3) = E(cid:2) (cid:82)

t fT |X(˜t|X) that converge to ∂2

t γ(t, X) and ∂2

T

Thus uniformly in uniformly bounded f ,

2 (E[(cid:96)tb(f, Z)] − E[(cid:96)tb(f∗b, Z)]) = Qb(f ) − Qb(f∗b)

= (cid:107)f − γ(cid:107)2

FtX

− (cid:107)f∗b − γ(cid:107)2

FtX

+ b2(B(f ) − B(f∗b)) + o(b2).

To show (2.1-2), it suﬃces to show that −(cid:107)f∗b − γ(cid:107)2

+ b2(B(f ) − B(f∗b)) = O(b2). Note
that γ(t, x) = arg minf limb→0 E[(cid:96)tb(f, Z)] = arg minf Q(f ). By the deﬁnition of the minimizers:
Q(f∗b) ≥ Q(γ) = 0 and Qb(γ) ≥ Qb(f∗b). So −(cid:107)f∗b − γ(cid:107)2
+ b2(B(f ) − B(f∗b)) = −Qb(f∗b) +
b2B(f )+o(b2) ≥ −Qb(γ)+b2B(f )+o(b2) = −Q(γ)−b2B(γ)+b2B(f )+o(b2) = b2(B(f )−B(γ))+o(b2).
(cid:3)
+ b2(B(f ) − B(f∗b)) ≤ b2(B(f ) − B(f∗b)). We obtain (2.1-2).
And −(cid:107)f∗b − γ(cid:107)2

FtX

FtX

FtX

Proof of Lemma 3 We use Theorem 1 in FLM by verifying their assumptions.

In FLM’s notation, f∗(x) = E[Φ((t − T )/h1)|X = x] for t ∈ T and h1 > 0. Assumption 1
(i)
in FLM holds, because supt∈T ,h1>0,x∈[−1,1]dx |E[Φ((t − T )/h1)|X = x]| ≤ 1 and supt∈T ,h1>0, Φ((t −
T )/h1) ≤ 1.

By integration by parts, change of variables, the Leibniz integral rule, and the condition (a),
xf∗(x)| = (cid:12)
1 φ((t−s)/h1)FT |X(s|x)ds(cid:9)(cid:12)
|Dα

T Φ((t−s)/h1)fT |X(s|x)ds(cid:12)
(cid:82) T

(cid:8)Φ((t−T )/h1)+(cid:82) T

T h−1

(cid:12) = (cid:12)

(cid:12)Dα
x

(cid:12)Dα
x

(cid:12) =

49

−∞ φ(u)(cid:12)
(cid:82) ∞
Thus for any t ∈ T and h1 > 0, f∗ ∈ W β,∞([−1, 1]dx) and Assumption 2 in FLM holds.

(cid:12)du ≤ 1, where φ is the standard normal probability density function.

xFT |X(t + uh1|x)(cid:12)

(cid:12)Dα

(cid:13) ˆfM LP −ReGP S(X) − E[Φ((T − t)/h1)|X](cid:13)

All the conditions in Theorem 1 in FLM hold for any t ∈ T and h1 > 0. By Theorem 1 in
dx
β+dx log8 n), for n large
FLM, for any t ∈ T and h1 > 0, with probability at least 1 − exp(−n
enough, (cid:13)
≤ C · R1n, for a constant C > 0 independent
(cid:13)FX
of n, which may depend on dx, M , and other ﬁxed constants. Since t and h1 do not appear as
ﬁxed constants in the conditions of Theorem 1 in FLM, C does not depend on t and h1. Therefore
we obtain the ﬁrst result.

In FLM’s notation, f∗(x) = E[hdt

(ii)
implies that for an absolute constant M > 0, supx∈X |E[hdt
t) < M , so Assumption 1 in FLM holds.

1 gh1(T − t)|X = x] for t ∈ T and h1 > 0. Assumption 2
1 gh1(T −

1 gh1(T − t)|X = x]| ≤ M and hdt

1 gh1(s − t)fT |X(s|x)ds(cid:12)

By change of variables, the Leibniz integral rule, and the condition (b), we show (cid:12)
(cid:82)
T hdt
(cid:12)Dα

xf∗(x)| =
(cid:12)
(cid:12)Dα
1 . Thus for any t ∈ T
x
and h1 ∈ (0, 1], f∗ ∈ W β,∞([−1, 1]dx), and Assumption 2 in FLM holds. All the conditions in
Theorem 1 in FLM hold for t ∈ T and h1 ∈ (0, 1]. The same arguments in (i) yield the second
(cid:3)
result.

xfT |X(t + uh1|x)(cid:12)

T g(u)(cid:12)

1 ≤ hdt

(cid:12) = (cid:82)

(cid:12)duhdt

(cid:12)Dα

Proof of Theorem 5

(i) We show the remainder terms (R1-1), (R1-2), (R1-DR), and (R2) are op(1) uniformly over
t ∈ T . Denote the nuisance estimators ˆγi(cid:96)t = ˆr(cid:96)(t, Xi) and ˆλi(cid:96)t = 1/ ˆf(cid:96)(t|Xi) that use Z c
(cid:96) for i ∈ I(cid:96).
Denote ˆg(t) = n−1 (cid:80)L
Kh(Ti−t)Υi(cid:96)(t) and Wi(cid:96)(t) = Kh(Ti−t)Υi(cid:96)(t)−E [Kh(Ti − t)Υi(cid:96)(t)],
where Υi(cid:96)(t) = (ˆλi(cid:96)t − λit)(ˆγi(cid:96)t − γit) for (R2).

(cid:80)

i∈I(cid:96)

(cid:96)=1

We follow similar decompositions in the proof of Theorem 3.1 in Fan et al. (2021). First, to
E[ˆg(t)] = op((cid:112)ln(n)/(nhdt)), the same argument in (15) holds uniformly over

show that supt∈T
t ∈ T by Assumption 6(i).

Since T is compact, it can be covered by a ﬁnite number Mn of cubes Ck,n with centered tk,n

and length mn, for k = 1, ..., Mn. So Mn ∝ 1/mdt

(cid:12)ˆg(t) − E[ˆg(t)](cid:12)
(cid:12)

sup
t∈T

(cid:12) = max
1≤k≤Mn

≤ max

1≤k≤Mn

sup
t∈T ∩Ck,n
sup
t∈T ∩Ck,n

n . Decompose
(cid:12)ˆg(t) − E[ˆg(t)](cid:12)
(cid:12)
(cid:12)
(cid:12)ˆg(t) − ˆg(tk,n)(cid:12)
(cid:12)
(cid:12)
(cid:12)ˆg(tk,n) − E[ˆg(tk,n)](cid:12)
(cid:12)
(cid:12)
(cid:12)E[ˆg(tn,k)] − E[ˆg(t)](cid:12)
(cid:12)
(cid:12).

+ max

1≤k≤Mn

+ max

1≤k≤Mn

sup
t∈T ∩Ck,n

(18)

(19)

(20)

For a positive constant C and positive sequences A1n and A2n given in Assumption 6(ii),
let Fn(C) ≡ {(γ†, λ†) : sup(t,x)∈T ×X |γ†(t, x) − γ(t, x)| ≤ CA1n, sup(t,x)∈T ×X |λ†(t, x) − λ(t, x)| ≤
CA2n.}. Let A(cid:96)n(C) ≡ {(ˆγ(cid:96)(·, ·), ˆλ(cid:96)(·, ·)) ∈ Fn(C)} and An(C) ≡ ∩L
(cid:96)=1A(cid:96)n. On An(C), i.e.,
(ˆγi(cid:96)t, ˆλi(cid:96)t) ∈ Fn(C) for (cid:96) = 1, ..., L, supt∈T ,i∈I(cid:96)
|Υi(cid:96)(t)| ≤ C 2A1nA2n ≡ C 2An. Assumption 6(ii)

50

implies that for any ε > 0, there exists a positive constant C, such that P (An(C)) ≥ 1 − ε for n
large enough.

Note that the expectation E is based on the distribution P of the full sample {(Yi, Ti, Xi)}n

i=1.

Observe that for (cid:96) ∈ {1, ..., L}

P (cid:0)n−1Wi(cid:96)(t) > ηn, A(cid:96)n(C)(cid:1) = E (cid:2)P (n−1Wi(cid:96)(t) > ηn|Z c

(cid:96) )1{A(cid:96)n(C)}(cid:3)

(cid:20)(cid:90)

= E

Z

1{n−1Wi(cid:96)(t) > ηn}fZ(Zi)dZi1{A(cid:96)n(C)}

(cid:21)

(21)

by the law of iterated expectations and cross-ﬁtting with (ˆγ(cid:96), ˆλ(cid:96)) using Z c
(cid:96) .

We will use the following inequalities. By exp(w) ≤ 1+w+w2 for |w| ≤ 1/2 and 1+w ≤ exp(w)

for w ≥ 0, we have

E[exp(W )] ≤ 1 + E[W ] + E[W 2] ≤ exp(E[W 2])

(22)

for a random variable W satisfying |W | ≤ 1/2 and E[W ] = 0. The Markov’s inequality for any
positive sequence an: P (W > ηn) ≤ E[exp(anW )]/ exp(anηn).

E[ˆg(t)](cid:12)

First consider (19). For any ηn > 0, P (cid:0) max1≤k≤Mn
(cid:12) > ηn, An(C)(cid:1) + ε. We show that for t ∈ T ,
P (cid:0)(cid:12)
(cid:12) > ηn, An(C)(cid:1)

(cid:12)ˆg(t) − E[ˆg(t)(cid:12)

(cid:12)ˆg(tk,n)−E[ˆg(tk,n)](cid:12)
(cid:12)

(cid:12) > ηn

(cid:1) ≤ Mn supt∈T P (cid:0)(cid:12)

(cid:12)ˆg(t)−

= P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:32)

= P

n−1

≤

≤

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

n−1

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

(cid:12)
(cid:12)
(cid:12)
Wi(cid:96)(t)
(cid:12)
(cid:12)

(cid:33)

> ηn, An(C)

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

Wi(cid:96)(t) > ηn, An(C)

+ P

n−1

(cid:33)

(cid:32)

Wi(cid:96)(t) < −ηn, An(C)

(cid:33)

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

(cid:8)P (cid:0)n−1Wi(cid:96)(t) > ηn, A(cid:96)n(C)(cid:1) + P (cid:0)−n−1Wi(cid:96)(t) > ηn, A(cid:96)n(C)(cid:1)(cid:9)

E (cid:2)1{A(cid:96)n(C)}E (cid:2)exp (cid:0)ann−1Wi(cid:96)(t)(cid:1) + exp(−ann−1Wi(cid:96)(t))(cid:12)

(cid:12)Z c
(cid:96)

(cid:3)(cid:3) (cid:14) exp(anηn)

≤ 2

L
(cid:88)

(cid:96)=1

n(cid:96) exp(−anηn)E (cid:2)1{A(cid:96)n(C)}E (cid:2)exp (cid:0)a2

nn−2E (cid:2)Wi(cid:96)(t)2(cid:12)

(cid:12)Z c
(cid:96)

(cid:3)(cid:1)(cid:3)(cid:3) .

(23)

The second inequality uses (21) and the conditional Markov’s inequality. Due to cross-ﬁtting,
(cid:96) , ˆγ(cid:96) and ˆλ(cid:96) are ﬁxed functions. When 1{A(cid:96)n(C)} = 1, we can choose an =
conditional on Z c
(cid:112)ln(n)nhdt/An such that |ann−1Wi(cid:96)(t)| ≤ 1/2, for all t, i, (cid:96) and for n large enough. So by (22),
the last inequality holds.

We next choose ηn such that anηn → ∞ and anηn ≥ a2

supt∈T P (cid:0)(cid:12)
exists a positive constant c1 such that E [Wi(cid:96)(t)2|Z c

(cid:12)ˆg(t) − E[ˆg(t)](cid:12)

(cid:96) ] for all t, (cid:96), so
(cid:1) = op(1). When 1{A(cid:96)n(C)} = 1, for n large enough, there
n. We can choose anηn = c2 ln(n)

(cid:96) ] ≤ c1h−dtA2

nn−2E [Wi(cid:96)(t)2|Z c

(cid:12) > ηn, A(cid:96)n

51

for some positive constant c2, so we let ηn = c2
(cid:12)ˆg(tk,n) − E[ˆg(tk,n)](cid:12)
(cid:12)
such that P (cid:0)max1≤k≤Mn
(cid:12) > ηn
2Mnn−(c2−c1n−1−1) + ε ≤ 2ε for c2 ≥ 2 and n large enough. So max1≤k≤Mn
Op(ηn) = op((cid:112)ln(n)/(nhdt)).

(cid:112)ln(n)/(nhdt)An. Then we can choose Mn
(cid:1) ≤ Mn2n exp(−c2 ln(n) + c1 ln(n)/n) + ε ≤
(cid:12)ˆg(tk,n) − E[ˆg(tk,n)](cid:12)
(cid:12)
(cid:12) =

For (18), the Lipschitz condition in Assumption 6(iii) implies supt∈T ∩Ck,n

(cid:12)
(cid:12)Kh(Ti − t)Υi(cid:96)(t) −
Kh(Ti − tk,n)Υi(cid:96)(tk,n))(cid:12)
(cid:107)t − tk,n(cid:107) ≤ c3h−(dt+1)mn, for some constant
c3 > 0 and the Euclidean norm of a vector (cid:107) · (cid:107). By choosing mn = o((cid:112)ln(n)h(dt+2)/n),
max1≤k≤Mn supt∈T ∩Ck,n

(cid:12) ≤ c3h−(dt+1)mn = op((cid:112)ln(n)/(nhdt)).
By the same argument, we can show that for (20) max1≤k≤Mn supt∈T ∩Ck,n

(cid:12) ≤ c3h−(dt+1) supt∈T ∩Ck,n

(cid:12)E[ˆg(tn,k)]−E[ˆg(t)](cid:12)
(cid:12)

(cid:12)ˆg(t) − ˆg(tk,n)(cid:12)
(cid:12)

(cid:12) =

op((cid:112)ln(n)/(nhdt)).

The same arguments apply to (R1-1) and (R1-2) by deﬁning Υi(cid:96)(t) accordingly: let Υi(cid:96)(t) =

λi(cid:96)t(ˆγi(cid:96)t − γi(cid:96)t) for (R1-1) and Υi(cid:96)(t) = (ˆλi(cid:96)t − λi(cid:96)t)(Yt − γi(cid:96)t) for (R1-2).

For (R1-DR), the argument for the pointwise convergence in the proof of Theorem 1 can be

extended to uniform convergence by Assumption 6(ii).

(ii) The results in Theorem 3 can be extended to uniformity in t by the same arguments in (i).
(cid:3)

Proof of Theorem 6 The proof follows closely the proof of Theorem 5, so we only notice
the diﬀerence to conserve space. The derivations proceed conditional on ξi and using the law
of iterated iterations. For notations, let the expectation E based on the distribution P of the
full sample {(Yi, Ti, Xi)}n
i=1, and the
expectation E(cid:63) based on the joint distribution P (cid:63) of the full sample {(Yi, Ti, Xi)}n
i=1.
Let ˆg(t) = n−1 (cid:80)L
ξiKh(Ti − t)Υi(cid:96)(t). The main diﬀerence is in (19). We show that for

i=1, the expectation E∗ based on the distribution P ∗ of {ξi}n

i=1 and {ξi}n

(cid:80)

(cid:96)=1

i∈I(cid:96)

52

t ∈ T ,

P (cid:63)(cid:0)(cid:12)

(cid:12)ˆg(t) − E[ˆg(t)(cid:12)

(cid:12) > ηn, An(C)(cid:1)

n−1

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

(cid:12)
(cid:12)
(cid:12)
ξiWi(cid:96)(t)
(cid:12)
(cid:12)

(cid:33)

> ηn, An(C)

= P (cid:63)

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:32)

= P (cid:63)

n−1

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

ξiWi(cid:96)(t) > ηn, An(C)

+ P (cid:63)

n−1

(cid:33)

(cid:32)

ξiWi(cid:96)(t) < −ηn, An(C)

(cid:33)

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

L
(cid:88)

(cid:88)

(cid:96)=1

i∈I(cid:96)

L
(cid:88)

(cid:88)

≤

=

(cid:8)P (cid:63) (cid:0)n−1ξiWi(cid:96)(t) > ηn, A(cid:96)n(C)(cid:1) + P (cid:63) (cid:0)−n−1ξiWi(cid:96)(t) > ηn, A(cid:96)n(C)(cid:1)(cid:9)

E∗(cid:104)

P (cid:0)n−1Wi(cid:96)(t) > ηn/ξi, A(cid:96)n(C)|ξi

(cid:1) 1{ξi ≥ 0}

(cid:96)=1

i∈I(cid:96)

+ P (cid:0)−n−1Wi(cid:96)(t) > −ηn/ξi, A(cid:96)n(C)|ξi
+ P (cid:0)−n−1Wi(cid:96)(t) > ηn/ξi, A(cid:96)n(C)|ξi

(cid:1) 1{ξi < 0}

(cid:1) 1{ξi ≥ 0} + P (cid:0)n−1Wi(cid:96)(t) > −ηn/ξi, A(cid:96)n(C)|ξi

(cid:105)
(cid:1) 1{ξi < 0}

L
(cid:88)

(cid:88)

E(cid:63)(cid:104)

≤

1{A(cid:96)n(C)}E

(cid:96)=1

i∈I(cid:96)

(cid:104)

(cid:12)
exp (cid:0)ann−1Wi(cid:96)(t)(cid:1) + exp(−ann−1Wi(cid:96)(t))
(cid:12)Z c
(cid:12)

(cid:96) , ξi

(cid:105)

exp (−anηn/|ξi|)

(cid:105)

≤ 2

L
(cid:88)

(cid:96)=1

n(cid:96)E(cid:63) [exp(−anηn/|ξi|)] 1{A(cid:96)n(C)}E (cid:2)exp (cid:0)a2

nn−2E (cid:2)Wi(cid:96)(t)2(cid:12)

(cid:12)Z c
(cid:96)

(cid:3)(cid:1)(cid:3) .

The same arguments following (23) are valid conditional on ξi. Due to ξi, here we choose
(cid:112)ln(n)/(nhdt)An ln(n). Next
anηn = c2 ln(n) ln(n) for some positive constant c2. So ηn = c2
(cid:12)ˆg(tk,n) − E(cid:63)[ˆg(tk,n)](cid:12)
(cid:12)
we show that we can choose Mn and c2 such that P (cid:63) (cid:0)max1≤k≤Mn
(cid:1) ≤
Mn2nE∗(cid:2) exp(−c2 ln(n) ln(n)/|ξi| + c1 ln(n)/n)(cid:3) + ε ≤ 2MnE∗(cid:2)n−c2 ln(n)/|ξi|+c1/n+1(cid:3) + ε ≤ 2ε for n
large enough.

(cid:12) > ηn

By Assumption 7, there exist some constants c4, c5 such that P ∗ (−c2 ln(n)/|ξi| + c1/n + 1 ≥ 0) ≤
P ∗ (|ξi| ≥ c2 ln(n)/(c1 + 1)) ≤ c4 exp(−c5c2 ln(n)/(c1+1)) ≤ c4n−c5c2/(c1+1). So E∗ (cid:2)n−c2 ln(n)/|ξi|+c1/n+1(cid:3)
≤ E∗ (cid:2)n−c2 ln(n)/|ξi|+c1/n+11 {−c2 ln(n)/|ξi| + c1/n + 1 ≤ 0}(cid:3)+nc1+1P ∗ (−c2 ln(n)/|ξi| + c1/n + 1 ≥ 0),
where the second term ≤ c4nc1+1−c5c2/(c1+1) = o(1) by choosing c2. Therefore we show that we can
choose Mn and c2 such that P (cid:63) (cid:0)max1≤k≤Mn
(cid:1) ≤ 2ε for n large enough.

(cid:12)
(cid:12)ˆg(tk,n) − E[ˆg(tk,n)](cid:12)
(cid:12) > ηn
(cid:12)ξiKh(Ti − t)Υi(cid:96)(t) − ξiKh(Ti − tk,n)Υi(cid:96)(tk,n))(cid:12)
(cid:12)

For (18), supt∈T ∩Ck,n

(cid:107)t −
tk,n(cid:107)|ξi| ≤ c3h−(dt+1)mn|ξi|, for some constant c3 > 0. We can choose mn = o((cid:112)ln(n)h(dt+2)/n)
i=1 |ξi| = op(cid:63)((cid:112)ln(n)/(nhdt)),
such that max1≤k≤Mn supt∈T ∩Ck,n
since n−1 (cid:80)n

(cid:12) ≤ c3h−(dt+1)mnn−1 (cid:80)n

(cid:12) ≤ c3h−(dt+1) supt∈T ∩Ck,n

(cid:12)ˆg(t)−ˆg(tk,n)(cid:12)
(cid:12)

i=1 |ξi| = Op∗(1).

Putting together, we obtain that the remainder term
ψ(Zi, βt, γi(cid:96), λi(cid:96))(cid:9) = op(cid:63)(1) uniformly in t ∈ T . Therefore
op(cid:63)(1), where ψi ≡ Kh(Ti − t)(Yi − γ(t, Xi)/fT |X(t|Xi) + γ(t, Xi) − βt.

n(cid:96)hdtn−1
nhdt(cid:0) ˆβ∗

(cid:96)
t − βt

(cid:80)

ξi

i∈I(cid:96)
(cid:1) = (cid:112)hdt/n (cid:80)n

(cid:8)ψ(Zi, βt, ˆγi(cid:96), ˆλi(cid:96)) −
i=1 ξiψi +

√
√

53

√

√

nhdt(cid:0) ˆβ∗

It follows that

t − ˆβt
1)ψi + op(cid:63)(1). Conditional on the sample path, (cid:112)hdt/n (cid:80)n
√
(cid:1) for any t ∈ T .
the limiting distribution of

nhdt(cid:0) ˆβt − βt

nhdt(cid:0) ˆβt−βt

(cid:1) = −

(cid:1)+(cid:112)hdt/n (cid:80)n

i=1 ξiψi+op(cid:63)(1) = (cid:112)hdt/n (cid:80)n
i=1(ξi − 1)ψi converges in distribution to

i=1(ξi−

The result for the partial eﬀect follows the same arguments.

(cid:3)

C Proofs in Section 4

Proof of Theorem 7 Deﬁne ˇψi(cid:96) ≡ ˆγ(cid:96)(Ui, Xi) + Kh(Ti−t)
ˆf(cid:96)(t|Xi)
Kh(Ti−t)
fT |X (t|Xi) (Yi − γ(Ui, Xi))−βt, and ψi ≡ γ(t, Xi)+ Kh(Ti−t)
totically linear representation of
op(1), we decompose ˇψi(cid:96) = ( ˇψi(cid:96) − ˜ψi) + ( ˜ψi − ψi) + ψi.

nhdt( ˇβt − βt) = (cid:112)hdt/n (cid:80)L

(Yi − ˆγ(cid:96)(Ui, Xi)) − βt, ˜ψi ≡ γ(Ui, Xi) +
fT |X (t|Xi) (Yi − γ(t, Xi))−βt. To show the asymp-
i=1 ψi +

ˇψi(cid:96) = (cid:112)hdt/n (cid:80)n

(cid:80)

i∈I(cid:96)

√

(cid:96)=1

√

nhdtn−1 (cid:80)L

We ﬁrst show that

( ˇψi(cid:96) − ˜ψi) = op(1). The proof closely follows the
proof of Theorem 1, by replacing γi ≡ γ(t, Xi) with γi ≡ γ(Ui, Xi), replacing ˆγi(cid:96) ≡ ˆγ(cid:96)(t, Xi) with
ˆγi(cid:96) ≡ ˆγ(cid:96)(Ui, Xi), and using the rate conditions in Assumption 8. Speciﬁcally, for (cid:96) ∈ {1, ..., L},
conditional on Z c

(cid:96) , we need a rate for bounding

(cid:80)

i∈I(cid:96)

(cid:96)=1

(cid:90)

(cid:90) ∞

−∞
(cid:90)

(cid:90)

X

=

=

≡

X
1
n(cid:96)

1
n(cid:96)

T0
(cid:88)

i∈I(cid:96)
(cid:88)

i∈I(cid:96)

(ˆγ(cid:96)(u, x) − γ(u, x))2 fU X(u, x)dudx

(ˆγ(cid:96)(Tih0 + t, x) − γ(Tih0 + t, x))2 d ˆFT (cid:96)(Ti)fX(x)dx
(cid:90)

(ˆγ(cid:96)(Tih0 + t, x) − γ(Tih0 + t, x))2 fX(x)dx

X

Wi(cid:96),

(24)

where Wi(cid:96) ≡ (cid:82)
n−1
(cid:96)
i∈I(cid:96)

(cid:80)

X (ˆγ(cid:96)(Tih0 + t, x) − γ(Tih0 + t, x))2 fX(x)dx. We show that conditional on Z c
(cid:96) ,
(cid:3)+op(1) below by the conditional Markov’s inequality. First we compute

Wi(cid:96) = E(cid:2)Wi(cid:96)

(cid:12)
(cid:12)Z c
(cid:96)

E (cid:2)Wi(cid:96)

(cid:12)
(cid:12)Z c
(cid:96)

(cid:3) =

=

≤

(cid:90)

(cid:90)

T0

(cid:90)

X
(cid:90)

Rdt
(cid:90)

(cid:90)

(ˆγ(cid:96)(sh0 + t, x) − γ(sh0 + t, x))2 fX(x)dxfT (s)ds

(ˆγ(cid:96)(u, x) − γ(u, x))2 fX(x)dxfT

X
(ˆγ(cid:96)(u, x) − γ(u, x))2 fT X(u, x)dxduh−dt

0

¯fT /fT |X

(cid:18) u − t
h0

(cid:19)

duh−dt
0

T0
= Op

X
0 (cid:107)ˆγ(cid:96) − γ(cid:107)2

(cid:0)h−dt

FT X

(cid:1) .

The inequality is by Assumption 1(ii)(iii) so that ¯fT /fT |X < ∞, where ¯fT ≡ supt∈T0 fT (t) and

54

fT |X ≡ inf (t,x)∈T ×X fT |X(t|x). Similarly

(cid:90)

(cid:18)(cid:90)

(ˆγ(cid:96)(sh0 + t, x) − γ(sh0 + t, x))2 fX(x)dx

(cid:19)2

fT (s)ds

X
(ˆγ(cid:96)(sh0 + t, x) − γ(sh0 + t, x))4 fX(x)dxfT (s)ds

(ˆγ(cid:96)(u, x) − γ(u, x))4 fX(x)dxfT

X
(ˆγ(cid:96)(u, x) − γ(u, x))4 fT X(u, x)dxduh−dt

0

¯fT /fT |X

(cid:18) u − t
h0

(cid:19)

duh−dt
0

E(cid:2)W 2

i(cid:96)

(cid:12)
(cid:12)Z c
(cid:96)

(cid:3) =

≤

≤

≤

T0

(cid:90)

(cid:90)

T0

(cid:90)

X
(cid:90)

Rdt
(cid:90)

(cid:90)

X

T0
= Op(h−dt

0

)

by Assumption 8(iii) and Jensen’s inequality.

Using the same arguments for (R1-1) in the proof of Theorem 1, let ∆i(cid:96) ≡ Wi(cid:96) − E(cid:2)Wi(cid:96)

Then E(cid:2)(cid:0)n−1
(cid:0)E(cid:2)W 2
∆i(cid:96)
i(cid:96)
By the conditional Markov’s inequality, n−1

(cid:3) = n−1

(cid:12)Z c
(cid:96)

(cid:80)

i∈I(cid:96)

(cid:1)2(cid:12)

(cid:96)

(cid:96)

(cid:12)
(cid:12)Z c
(cid:96)
(cid:80)

(cid:96)

i∈I(cid:96)

∆i(cid:96) = op(1).

(cid:3) − E(cid:2)Wi(cid:96)

(cid:12)
(cid:12)Z c
(cid:96)

(cid:3)2(cid:1) = op(1) by assuming nhdt

We obtain a rate for (24), (cid:82)

(cid:82)

op(1) = Op

(cid:0)h−dt

0 (cid:107)ˆγ(cid:96) − γ(cid:107)2

FT X

U (ˆγ(cid:96)(u, x) − γ(u, x))2 fU X(u, x)dudx = n−1

(cid:96)

X

(cid:1) . This gives the modiﬁed conditions in Assumption 8(i)(ii).
√

i∈I(cid:96)

(cid:80)

Wi(cid:96) = E(cid:2)Wi(cid:96)

(cid:12)
(cid:12)Z c
(cid:96)

(cid:3)+

(cid:12)
(cid:3).
(cid:12)Z c
(cid:96)
0 → ∞.

The same arguments in the proof of Theorem 1 yield

Next we show

√

nhdtn−1 (cid:80)n

i=1( ˜ψi − ψi) ≡

nhdtn−1 (cid:80)n

√

where Wi ≡ (γ(Ui, Xi) − γ(t, Xi))(1 − Kh(Ti − t)/fT |X(t|Xi)).

We ﬁrst calculate E[Kh(T − t)|X = x] = (cid:82) ∞

uniformly over x ∈ X by Assumption 1(iii).

nhdtn−1 (cid:80)L
√

i=1 Wi =

(cid:80)

(cid:96)=1

i∈I(cid:96)

( ˇψi(cid:96) − ˜ψi) = op(1).

nhdtE[Wi] + op(1) = op(1),

−∞ k(u)fT |X(t + uh|x)du = fT |X(t|x) + Op(h2)

E[Wi] = E (cid:2)E [γ(Ui, Xi) − γ(t, Xi)|Xi] E (cid:2)1 − Kh(Ti − t)/fT |X(t|Xi)|Xi

(cid:3)(cid:3)

(cid:20)(cid:90)

= E

(cid:18)

T0

h2

= O

(γ(sh0 + t, Xi) − γ(t, Xi)) fT (s)ds (cid:0)1 − E[Kh(Ti − t)|Xi]/fT |X(t|Xi)(cid:1)
(cid:19)
(cid:90)

(cid:90)

sh0∂tγ(¯t, x)fT (s)fX(x)dsdx

(cid:21)

X
= O(h2h0)

T0

for some ¯t between t and t + sh0. Let

√

nhdth2h0 = o(1).

Following the same arguments as above, let ∆i(cid:96) ≡ Wi − E[Wi]. We show

op(1) by the conditional Markov’s inequality.

√

nhdtn−1 (cid:80)n

i=1 ∆i(cid:96) =

55


(cid:32)√


E

nhdtn−1

n
(cid:88)

i=1

(cid:33)2

∆i(cid:96)

 = hdtE[∆2

i(cid:96)] ≤ hdtE[W 2
i ]

= E (cid:2)E (cid:2)(γ(Ui, Xi) − γ(t, Xi))2|Xi

(cid:3) hdtE (cid:2)(1 − Kh(Ti − t)/fT |X(t|Xi))2|Xi
(cid:33)

(cid:3)(cid:3)

(γ(Tih0 + t, x) − γ(t, x))2fX(x)dx

(cid:32)(cid:90)

X

n−1
(cid:96)

(cid:88)

i∈I(cid:96)

= Op

= Op

= Op

(cid:18)(cid:90)

(cid:90)

T0

(cid:18)(cid:90)

X

(cid:90)

T0

X

(γ(sh0 + t, x) − γ(t, x))2fX(x)dxfT (s)ds

s2h2

0∂tγ(¯t, x)2fX(x)dxfT (s)ds

(cid:19)

(cid:19)

(25)

(26)

= O(h2
0)

for some ¯t between t and t + sh0. The equality for (25) is because hdtE[(1 − Kh(T − t))2|X =
x] = hdt (cid:82)
j=1k(uj))2fT |X(t + uh|x)duhdt = O(1). The equality for (26) comes from
(cid:80)
n−1
X (γ(Tih0+t, x)−γ(t, x))2fX(x)dx, by Markov’s inequal-
(cid:96)
(cid:80)
ity: E[(n−1
0∂tγ(¯t, x)2fX(x)
i∈I(cid:96)
dx(cid:1)2(cid:3) = O(n−1h2
(cid:3)

Rdt (1 − h−dtΠdt
Ai−E[Ai] = op(1), where Ai ≡ (cid:82)
(cid:80)
Ai − E[Ai])2] = var(n−1
(cid:96)
0) = o(1).

Ai) ≤ n−1

i ] = n−1
(cid:96)

E(cid:2)(cid:0) (cid:82)

X T 2

E[A2

i h2

i∈I(cid:96)

i∈I(cid:96)

(cid:96)

(cid:96)

Proof of Theorem 8 The proof of the consistency of ˇVt follows the proof of Theorem 2 by
modifying Assumption 4(i)(ii) to Assumption 9(i)(ii). Assumption 9(i)(ii) are obtained by the
same arguments for (24). The consistency of ˇBt and ˇht are directly implied by the proof of
(cid:3)
Theorem 2.

Proof of Theorem 9 The proof follows the proof of Theorem 3 directly by modifying the
(cid:3)
conditions (a) and (b) in Theorem 3(i).

D Kernel localization

We discuss the construction of the doubly robust moment function by Gateaux derivative and
a local Riesz representer. Importantly the expression in (27) is the building block to construct
estimators for βt and the linear functionals of βt. Section D.1 discusses the adjustment for the ﬁrst-
step kernel estimators in the moment functions of the regression estimator and inverse probability
weighting estimator that do not use the doubly robust moment function and cross-ﬁtting. We
illustrate how the DML estimator assumes weaker conditions.

One way to obtain the inﬂuence function is to calculate the limit of the Gateaux derivative with
respect to a smooth deviation from the true distribution, as the deviation approaches a point mass,
following Carone et al. (2018) and Ichimura and Newey (2022). For any t ∈ T , let βt(·) : F → R,
where F is a set of CDFs of Z that is unrestricted except for regularity conditions. The estimator
converges to βt(F ) for some F ∈ F, which describes how the limit of the estimator varies as the

56

distribution of a data observation varies. Let F 0 be the true CDF of Z. Let the CDF F h
a point mass at Z as h → 0. Consider F τ h = (1 − τ )F 0 + τ F h
enough τ , F τ h ∈ F and the corresponding pdf f τ h = f 0 + τ (f h
derivative of the functional βt(F τ h) with respect to a deviation F h
F 0.

Z approach
Z for τ ∈ [0, 1] such that for all small
Z − f 0). We calculate the Gateaux
Z − F 0 from the true distribution

Below we show that the Gateaux derivative for the direction f h

Z − f 0 is

d
dτ

(cid:12)
βt(F τ h)
(cid:12)
(cid:12)τ =0

lim
h→0

= γ(t, X) − βt + lim
h→0

= γ(t, X) − βt +

(cid:90)

(cid:90)

y − γ(t, x)
fT |X(t|x)

Y

X
Y − γ(t, X)
fT |X(t|X)

lim
h→0

f h
T (t).

f h
Z(y, t, x)dydx

(27)

Note that the last term in (27) is a partial mean that is a marginal integration over Y × X ,
ﬁxing the value of T at t. Thus the Gateaux derivative depends on the choice of f h
T . We then
choose f h
T (t) =
limh→0 Kh(T − t).

Z(z) = Kh(Z − z)1{f 0(z) > h}, following Ichimura and Newey (2022), so limh→0 f h

Ichimura and Newey (2022) show that if a semiparametric estimator is asymptotically lin-
ear and locally regular, then the inﬂuence function is limh→0 dβt(F τ h)/dτ |τ =0. Here, we use the
Gateaux derivative limit calculation to motivate our moment function that depends on F h
T . Then
we show that our estimator is asymptotically equivalent to a sample average of the moment func-
tion.

Remark 3 (Linear functional of βt) Consider a non-regular function-valued linear functional
of βt, denoted by αt = A[βt] for a linear operator A. So αt is also a nonparametric function of t.
To construct the DML estimator of αt, the Gateaux derivative of αt is simply the linear functional
of the Gateaux derivative of βt in (27), i.e.,

d
dτ

(cid:12)
αt(F τ h)
(cid:12)
(cid:12)τ =0

lim
h→0

= lim
h→0

d
dτ

A (cid:2)βt(F τ h)(cid:3) (cid:12)
(cid:12)
(cid:12)τ =0

= A[γ(t, X)] − A[βt] + lim
h→0

A

(cid:20)Y − γ(t, X)
fT |X(t|X)

(cid:21)
f h
T (t)

.

(cid:3).

We may work out the close-form expression of this Gateaux derivative of αt and use its estimated
sample analogue to construct the DML estimator of αt. An alternative DML estimator is simply
ˆαt = A(cid:2) ˆβt

Note that the partial eﬀect can be expressed as a linear functional of βt: θt = A[βt] = ∂βt/∂t1.
An example of a weighted conditional average partial derivative given T1 = t1 can be deﬁned as
αt = (cid:82) θtw(t)dt2...dtdt that is a function of t1 with a weight function w(t) = w(t1, t2, ..., tdt). In
contrast, the weighted average derivative αt = (cid:82)
T θtw(t)dt = α does not depend on t. The DML
estimation for such regular real-valued parameter α has been well-studied in the semiparametric
literature. We contribute to the DML literature by focusing on the function-valued non-regular
nonparametric objects based on βt.

Remark 4 (Local Riesz representer) The above discussion on the Gateaux derivative sug-
gests that the Riesz representer for the non-regular βt is not unique and depends on the kernel or
other methods for localization at t. We deﬁne the “local Riesz representer” to be αth(T, X) =

57

(cid:82)

(cid:82)
X

f h
T (t)/fT |X(T |X) = Kh(T − t)/fT |X(T |X) indexed by the evaluation value t and the band-
width of the kernel h. Our local Riesz representer αth(T, X) satisﬁes βt = (cid:82)
X γ(t, X)dFX(X) =
limh→0
T αth(T, X)γ(T, X)dFT X(T, X) for all γ with ﬁnite second moment, following the in-
sight of the local Riesz representation theorem for a regular parameter (Newey, 1994a). Then we
can obtain the inﬂuence function by adding an adjustment term αth(T, X)(Y − γ(T, X)), which
is the product of the local Riesz representer and the regression residual. For a series localization,
Chen et al. (2014) deﬁne the sieve Riesz representer for plug-in sieve M estimators of irregular
functionals; see also Chen and Pouzo (2015) for general semi/nonparametric conditional moment
models.

Gateaux derivative Let the Dirac delta function δt(T ) = ∞ for T = t, δt(T ) = 0 for T (cid:54)= t,
and (cid:82) g(s)δt(s)ds = g(t), for any continuous compactly supported function g.17 For any F ∈ F,

βt(F ) =

=

=

(cid:90)

X

(cid:90)

E[Y |T = t, X = x]fX(x)dx
(cid:90)

E[Y |T = s, X = x]δt(s)dsfX(x)dx

X

(cid:90)

T0

(cid:90)

(cid:90)

X

T0

Y

yδt(s)

fZ(y, s, x)fX(x)
fT X(s, x)

dydsdx.

d
dτ

βt(F τ h) =

=

(cid:90)

(cid:90)

(cid:90)

X

(cid:90)

T0

(cid:90)

Y
(cid:90)

yδt(s)

d
dτ
yδt(s)
fT X(s, x)

(cid:18) fZ(y, s, x)fX(x)
fT X(s, x)

(cid:19)

dydsdx

Z(y, s, x)(cid:1) fX(x)

X

T0

Y

+ fZ(y, s, x) (cid:0)−f 0
(cid:90)

(cid:90)

(cid:90)

−

yδt(s)

X

T0

Y

(cid:0) (cid:0)−f 0

Z(y, s, x) + f h
X(x)(cid:1) (cid:1)dydsdx
(cid:0)−f 0

X(x) + f h
fZ(y, s, x)fX(x)
fT X(s, x)2

T X(s, x) + f h

T X(s, x)(cid:1) dydsdx.

The inﬂuence function can be calculated as

d
dτ

(cid:12)
βt(F τ h)
(cid:12)
(cid:12)τ =0

lim
h→0

= γ(t, X) − βt + lim
h→0

= γ(t, X) − βt +

(cid:90)

(cid:90)

y − γ(t, x)
fT |X(t|x)

Y

X
Y − γ(t, X)
fT |X(t|X)

lim
h→0

f h
T (t).

f h
Z(y, t, x)dydx

In particular, we specify F h

j=1k(Zj/h)/h,
where Z = (Z1, ..., Zdz )(cid:48) and k satisﬁes Assumption 2 and is continuously diﬀerentiable of all orders
with bounded derivatives. Let F τ h = (1 − τ )F 0 + τ F h
Z with pdf with respect to a product measure
Z(z) = Kh(Z − z)/f 0(z), a ratio of a sharply
Z(z), where δh
given by f τ h(z) = (1 − τ )f 0(z) + τ f 0(z)δh

Z following Ichimura and Newey (2022). Let Kh(Z) = Πdz

17Note that a nascent delta function to approximate the Dirac delta function is Kh(T − t) = k((T − t)/h)/h such

that δt(T ) = limh→0 Kh(T − t).

58

peaked pdf to the true density. Thus f h
limh→0 f h

T (t) = limh→0 Kh(T − t) and

Z(y, t, x) = Kh(Y − y)Kh(T − t)Kh(X − x). It follows that

(cid:90)

(cid:90)

X

Y

lim
h→0

y − γ(t, x)
fT |X(t|x)

f h
Z(y, t, x)dydx =

Y − γ(t, X)
fT |X(t|X)

lim
h→0

Kh(T − t).

E(cid:2) d

dτ βt(F τ h)(cid:12)
holds a h → 0.

(cid:12)τ =0

(cid:3) = E(cid:2)γ(t, X) − βt + Y −γ(t,X)

fT |X (t|X) Kh(T − t)(cid:3) = O(h2). So Neyman orthogonality

D.1 Adjustment for ﬁrst-step kernel estimation

We discuss another motivation of our moment function. We consider two alternative estimators for
the dose response function, or the average structural function, βt: the regression estimator ˆβREG
=
n−1 (cid:80)n
i=1 ˆγ(t, Xi) that is based on the identiﬁcation in (2), and the inverse probability weighting
i=1 Kh(Ti − t)Yi/ ˆfT |X(t|Xi) that is based on the identiﬁcation
(IPW) estimator ˆβIP W
in (3). Adding the inﬂuence function that accounts for the ﬁrst-step estimation partials out the
ﬁrst-order eﬀect of the ﬁrst-step estimation on the ﬁnal estimator, as discussed in CEINR and
Bravo et al. (2020) for the semiparametric empirical likelihood inference in a low dimensional
nonparametric setting.

= n−1 (cid:80)n

t

t

For ˆβREG
t

t

, consider ˆγ(t, x) to be a local constant or local polynomial estimator with band-
width h for low-dimensional X. Newey (1994b) and Lee (2018) have derived the asymptotically
linear representation of ˆβREG
that is ﬁrst-order equivalent to that of our DML estimator given
in Theorem 1. Speciﬁcally we can obtain the adjustment term by the inﬂuence function of the
partial mean (cid:82)
i=1 Kh(Ti − t)(Yi − γ(t, Xi))/fT |X(t|Xi) + op((nhdt)−1/2)
with a suitably chosen h and regularity conditions. Thus the moment function can be constructed
by adding the inﬂuence function adjustment for estimating the nuisance function γ(t, X) to the
original moment function γ(t, X).

X ˆγ(t, x)f (x)dx = n−1 (cid:80)n

Similarly for ˆβIP W

, when ˆfT |X is a standard kernel density estimator with bandwidth h, Hsu
et al. (2020) derive the asymptotically linear representation of ˆβIP W
that is ﬁrst-order equiva-
lent to our DML estimator. We can show that the partial mean (cid:82)
Z Kh(T − t)Y / ˆfT |X(t|X)dFZ =
i=1 γ(t, Xi) (cid:0)1 − Kh(Ti − t)/fT |X(t|Xi)(cid:1) + op((nhdt)−1/2) with a suitably chosen h and regu-
n−1 (cid:80)n
larity conditions. Thus the moment function can be constructed by adding the inﬂuence function
adjustment for estimating the nuisance function fT |X to the original moment function Kh(T −
t)Y /fT |X(t|X).

t

t

Remark 5 (First-step bias reduction) In general, nonparametric estimation of an inﬁnite-
dimensional nuisance parameter contributes a ﬁnite-sample bias to the ﬁnal estimator. It is note-
worthy that although the kernel function in the DML estimator ˆβt introduces the ﬁrst-order bias
h2Bt, ˆβt requires a weaker bandwidth condition for controlling the bias of the ﬁrst-step estima-
tor than the regression estimator ˆβREG
. Our DML estimator for
continuous treatments inherits this advantageous property from the DML estimator for a binary
treatment. Therefore the DML estimator can be less sensitive to variation in tuning parameters
of the ﬁrst-step estimators. To illustrate with an example of ˆβREG
, consider the ﬁrst-step ˆγ to be

and the IPW estimator ˆβIP W

t

t

t

59

a local constant estimator with bandwidth h1 and a kernel of order r. To control the bias of ˆγ to
1 → 0. In contrast, when ˆγ and ˆfT |X
be asymptotically negligible for ˆβREG
in the DML estimator ˆβt are local constant estimators with bandwidth h1 and a kernel of order
r, Assumption 3(ii) requires h2r
nhdt → 0. It follows that the DML estimator need not under-
1
smooth the nuisance estimators, while the regression estimator ˆβREG
requires an undersmoothing
√
t
ˆγ. Moreover we observe that the condition is weaker than hr
n → 0 for the binary treatment
1
that has a regular root-n convergence rate.

, we assume hr
1

nhdt

(cid:113)

√

t

Remark 6 (First-step series estimation) When ˆγ(t, x) is a series estimator in ˆβREG
, com-
puting the partial mean (cid:82)
X ˆγ(t, x)f (x)dx for the inﬂuence function results in a diﬀerent adjust-
ment term than the kernel estimation discussed above.18 Heuristically, let us consider a basis
function b(T, X), including raw variables (T, X) as well as interactions and other transforma-
tions of these variables. Computing (cid:82)
X ˆγ(t, x)f (x)dx implies the adjustment term of the form
(cid:0)yi −
i=1 b(Ti, Xi)(cid:48)(cid:0)yi − γ(Ti, Xi)(cid:1) = n−1 (cid:80)n
E[b(t, X)] (n−1 (cid:80)n
γ(Ti, Xi)(cid:1), resulting in a form of an average weighted residuals in estimation or projection of
the residual on the space generated by the basis functions. Notice that the conditional density
fT |X(t|X) is not explicit in the weight λti. Such adjustment term may motivate diﬀerent estimators
of βt; for example, the approximate residual balancing estimator in Athey et al. (2018), CEINR,
and Demirer et al. (2019).

i=1 b(Ti, Xi)b(Ti, Xi)(cid:48))−1 n−1 (cid:80)n

i=1 λti

t

E Supplement for Section 3.1 Nuisance estimators

E.1 Kernel

The asymptotic distribution of a kernel-based estimator is well-studied (see, e.g., Chapter 19 in
Hansen (2022)). Consider the Nadaraya-Watson regression estimator ˆγ(t, x) = (cid:80)n
i=1 YiKhγ (Ti −
t)Khγ (Xi − x)(cid:14) (cid:80)n
i=1 Khγ (Ti − t)Khγ (Xi − x) with a bandwidth hγ and a kernel of order rγ.
Estimate the GPS fT |X by the standard kernel estimator ˆfT |X(t|x) = (cid:80)n
i=1 Khf (Ti − t)Khf (Xi −
x)(cid:14) (cid:80)n

i=1 Khf (Xi − x) with a bandwidth hf and a kernel of order rf . Let d = dt + dx.

f → ∞, nhd+2rf

Assumption 10 (First-step kernel) (i) hγ → 0, nhd
= O(1). (ii) hf → 0,
nhd
= O(1). Over (t, x) ∈ T × X that is bounded, (iii) var(Y |T = t, X = x)
is continuous and uniformly bounded; (iv) the rγth derivatives of γ(t, x) exist and are continu-
ous, uniformly bounded; (v) the rf th derivatives of fT |X(t|x) exist and are continuous, uniformly
bounded.

γ → ∞, nhd+2rγ

γ

f

Theorem 10 Under Assumption 10 and rf ≥ rγ − 1, (cid:107)ˆγ − γ(cid:107)FtX = Op
(cid:107) ˆfT |X − fT |X(cid:107)FtX = Op

(cid:1), for t ∈ T .

f )−1/2 + hrf

(cid:0)(nhd

f

(cid:0)(nhd

γ)−1/2 + hrγ

γ

(cid:1) and

18For example, Lee and Li (2018) derive the asymptotic theory of a partial mean of a series estimator, in

estimating the average structural function with a special regressor.

60

Assumption 3(ii) requires

(cid:1) → 0. It further implies
f )−1/2 + hrf
that we may choose the AMSE optimal bandwidths for ˆγ and ˆfT |X, i.e., hγ ∝ n−1/(2rγ +d) and
hf ∝ n−1/(2rf +d).

γ)−1/2 + hrγ

nhdt(cid:0)(nhd

(cid:1)(cid:0)(nhd

γ

f

√

We note that the kernel density estimator has a tighter bound on the convergence rate, com-
pared with the rate of the MultiGPS estimator. Let dt = 1 and hf = h1 = h for simplicity.
Theorem 10 shows that (cid:107) ˆfT |X − fT |X(cid:107)FtX = Op((nhdx+1)−1/2 + hrf ). Lemma 2 shows that for the
MultiGPS estimator, (cid:107) ˆfT |X − fT |X(cid:107)FtX = Op(R1nh−1 + h2), where R1n = (nhdx)−1/2 + hrf ) for
a local constant estimator. Thus the convergence rates provided in Lemmas 1 and 2 might not
be sharp. We may attain a tighter bound if we could directly compute the mean-squared error
of a speciﬁc estimator. Nevertheless we oﬀer convenient bounds for the convergence rates of the
ReGPS and MultiGPS estimators that could employ various ML methods.

E.2 Series

We illustrate how a series estimator satisﬁes Assumption 3 using the results in Newey (1997)
summarized in Chapter 20 in Hansen (2022). For a detailed review of series methods, see Chen
(2007).

Let ZK ≡ ZK(T, X) be a K × 1 vector of regressors obtained by making transformations of
(T, X), such as polynomial. The series approximation to γ(t, x) is γK(t, x) ≡ ZK(t, x)(cid:48)βK, where
βK ≡ E[ZKZ (cid:48)
i=1 ZKiYi
and ˆγK(t, x) ≡ ZK(t, x)(cid:48) ˆβK.

K]−1E[ZKY ]. Consider a least squares estimator ˆβK ≡ ((cid:80)n

Ki)−1 (cid:80)n

i=1 ZKiZ (cid:48)

To analyze the asymptotic properties, deﬁne QK ≡ (cid:82)

T ×X ZK(t, x)ZK(t, x)(cid:48)dFT X(t, x), ζK ≡
K ZK(t, x)(cid:1)1/2, and the projection approximation error rK(t, x) ≡ γ(t, x) −

(cid:0)ZK(t, x)(cid:48)Q−1

sup(t,x)
ZK(t, x)(cid:48)βK.

Assumption 11 (Series) (i) The smallest eigenvalue of QK is bounded away from zero. (ii)
ζ 2
K log(K)/n → 0 as n, K → ∞. (iii) There are α and βK such that sup(t,x)∈T ×X |rK(t, x)| =
O(K −α) as K → ∞. (iv) var(Y |T = t, X = x) and fT |X(t, x) is bounded above uniformly over
T × X .
Theorem 11 (Series) Under Assumption 11, (cid:107)ˆγK − γ(cid:107)FtX = Op((cid:112)K/n + K −α) for t ∈ T .

Theorem 20.7 in Hansen (2022) provides the convergence rate of an integrated squared error
which is deﬁned as (cid:107)ˆγK − γ(cid:107)2
, i.e., under Assumption 11, the L2(T X) convergence rate (cid:107)ˆγK −
γ(cid:107)FT X = Op((cid:112)K/n + K −α). Theorem 11 contributes the convergence rate for the partial L2(tX)
norm for a series estimator that is the same as the standard L2(T X) convergence rate.

FT X

We can apply series methods to the GPS estimation proposed in Section 2.1. The ReGPS
estimator can use a logistic series estimator for the conditional mean of Φ((t − T )/h1) given X,
with the standard L2(X) convergence rate R1n implied in Hirano et al. (2003). When dt > 1, the
MultiGPS estimator can be a series regression of hdt
1 gh1(Ti − t) on X, with the standard L2(X)
convergence rate R1n given in Theorem 20.7 in Hansen (2022).

To verify Assumption 11(ii), ζK ≤ O(K) for power series and ζK ≤ O(K 1/2) for splines under
the assumption that fT X(t, x) is strictly positive on T × X (Theorem 20.3 in Hansen (2022)).

61

Assumption 11(iii) is from Assumption 3 in Newey (1997) and is satisﬁed for splines and power
series by α = s/d, where s is the number of continuous derivatives of γ(t, x).
It implies that
(cid:107)rK(cid:107)FT X = O(K −α) and (cid:107)rK(cid:107)FtX = O(K −α). Note that Assumption 11(iii) is suﬃcient to obtain
(cid:107)rK(cid:107)FtX for the partial L2(tX) norm, but it may be stronger than necessary; see Theorem 20.2 in
Hansen (2022) for the L2(T X) norm (cid:107)rK(cid:107)FT X .

E.3 Proofs
Proof of Theorem 10 The result is obtained by Markov’s inequality with a bound on E(cid:2)(cid:107)ˆγ(t, x)−
γ(t, x)(cid:107)2
(cid:82)

(cid:3) equals the integrated mean squared error

(cid:3). Note that E(cid:2)(cid:107)ˆγ(t, x) − γ(t, x)(cid:107)2

FtX

(cid:16)

FtX
h2rγ
γ B(t, x)2 + (nhd

γ)−1(cid:1) = O(cid:0)h2rγ

γ + (nhd

γ)−1 (cid:82) ∞
−∞ K(u)2du · var(Y |T = t, X =
γ)−1(cid:1), where the leading bias B(t, x)

X

E(cid:2)(ˆγ(t, x) − γ(t, x))2(cid:3)fT X(t, x)dx = (cid:82)
X
γ + (nhd

fT X(t, x)dx + o(cid:0)h2rγ

(cid:17)

x)/fT X(t, x)
is analyzed below.

The explicit expression of B(t, x) for rγ = 2 is given in Theorem 19.1 in Hansen (2022). It is
straightforward to derive B(t, x) for rγ > 2 at the cost of notational complication. The derivation is
based on the proof of Theorem 19.1 in Hansen (2022), and we only note the modiﬁcation to conserve
space. To simplify notation, let r = rγ, z = (t, x), f (z) = fT X(t, x), and γ(r)(z) be the rth partial
derivative of γ(z). By two Taylor series expansions, equation (19.31) in Hansen (2022) becomes
(cid:82) ∞
−∞ K(u)(γ(z+hu)−γ(z))f (z+hu)du = (cid:82) ∞
−∞ K(u)(γ(1)(z)hu+γ(2)(z)h2u2/2!+...+γ(r)(z)hrur/r!+
o(hr)(cid:1)(cid:0)f (z) + f (1)(z)hu + ... + f (r−1)(z)hr−1ur−1/(r − 1)! + o(hr−1)(cid:1)du = hrB(t, x) + o(hr). Under
Assumption 10 and rf ≥ rγ − 1, B(t, x) is uniformly bounded. Thus the above asymptotic
integrated mean squared error is ﬁnite.

The same arguments apply to the result for ˆfT |X.

(cid:3)

Proof of Theorem 11 The proofs of Theorem 20.6 and Theorem 20.7 in Hansen (2022) analyze
(cid:107)ˆγK − γ(cid:107)2

. We follow the same argument to analyze the L2(tX) norm.

FT X

We can write Y = Z (cid:48)

KβK + eK, where eK is the projection error. Deﬁne (cid:101)ZKi ≡ Q−1/2

K ZKi,

(cid:101)QK ≡ n−1 (cid:80)n

i=1 (cid:101)ZKi (cid:101)Z (cid:48)

Ki, and QKt ≡ (cid:82)

X ZK(t, x)ZK(t, x)(cid:48)fT X(t, x)dx.

(cid:107)ˆγK − γ(cid:107)2

FtX

=

(cid:90)

X

(cid:0)ZK(t, x)(cid:48)( ˆβK − βK) − rK(t, x)(cid:1)2fT X(t, x)dx
(cid:19)

(cid:18)(cid:90)

= ( ˆβK − βK)(cid:48)

ZK(t, x)ZK(t, x)(cid:48)fT X(t, x)dx

X
(cid:18)(cid:90)

X

− 2( ˆβK − βK)(cid:48)

ZK(t, x)rK(t, x)fT X(t, x)dx

rK(t, x)2fT X(t, x)dx

(cid:90)

+

X
(cid:16)

= Op

( ˆβK − βK)(cid:48)QKt( ˆβK − βK) + (cid:107)rK(cid:107)2

(cid:17)

.

FtX

( ˆβK − βK)
(cid:19)

(28)

(29)

62

Assumption 11(iii) implies (cid:107)rK(cid:107)FtX = O(K −α). Consider the term in (29). For the L2(T X)
T ×X ZK(t, x)rK(t, x)dFT X(t, x) = 0 due to the regression and projection errors. But this

norm, (cid:82)
is not the case for the L2(tX) norm.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

( ˆβK − βK)(cid:48)

(cid:18)(cid:90)

X

ZK(t, x)rK(t, x)fT X(t, x)dx

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

≤

X

|(ˆγK(t, x) − γK(t, x)) rK(t, x)| fT X(t, x)dx

≤ (cid:107) (ˆγK − γK) rK(cid:107)FtX
≤ (cid:107)ˆγK − γK(cid:107)FtX (cid:107)rK(cid:107)FtX ,

where (cid:107)ˆγK − γK(cid:107)2
FtX
Write ˆβK = (Z(cid:48)

= (cid:82)
KZK)−1 Z(cid:48)

X

term in (28)

(cid:0)Zk(t, x)(cid:48)( ˆβK − βK)(cid:1)2fT X(t, x)dx = ( ˆβK − βK)(cid:48)QKt( ˆβK − βK).

KY, where ZK ≡ (Z1, ..., Zn)(cid:48) and Y ≡ (Y1, ..., Yn)(cid:48). We show the

( ˆβK − βK)(cid:48)QKt( ˆβK − βK) = (e(cid:48)

KZK)(Z(cid:48)

KeK)

KZK)−1(Z(cid:48)
(cid:101)Q−1
K ((cid:101)Z(cid:48)
(cid:17)(cid:17)2 (cid:16)

KZK)−1QKt(Z(cid:48)
K QKtQ−1/2
K Q−1/2
K QKtQ−1/2
(cid:101)Q−1
K
(cid:1)
K Z(cid:48)
KeK

KZKQ−1

K Q−1/2

K

K

KeK)

= n−2(e(cid:48)
(cid:16)

K (cid:101)ZK) (cid:101)Q−1
(cid:16)

(cid:101)Q−1
≤
λmax
= Op(1) (cid:0)n−2e(cid:48)
= Op(K/n).

n−2e(cid:48)

K (cid:101)ZK (cid:101)Z(cid:48)

KeK

(cid:17)

The last equality follows the proof of Theorem 20.7 in Hansen (2022). The above inequality is by
the Quadratic Inequality, where λmax(Q) denotes the largest eigenvalue of a matrix Q. By the
(cid:1) ≤ λmax
(cid:1)
Schwarz Matrix Inequality, λmax
(cid:1)λmax
(cid:1), which is Op(1) by Assumption 11(i) and Theorem 20.5 in Hansen (2022).
(cid:101)Q−1
×λmax
K
We notice that QKt does not aﬀect the bound.

K QKtQ−1/2

K Q−1/2

(cid:0)Q−1/2

(cid:0)Q−1/2

(cid:1)λmax

(cid:1)λmax

(cid:0)QKt

(cid:101)Q−1
K

(cid:101)Q−1
K

(cid:101)Q−1

(cid:0)

(cid:0)

(cid:0)

K

K

K

(cid:3)

Putting together, we obtain (cid:107)ˆγ − γ(cid:107)2

FtX

= Op(K/n + K −2α).

63

