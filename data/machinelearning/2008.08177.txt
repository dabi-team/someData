0
2
0
2

g
u
A
8
1

]

G
L
.
s
c
[

1
v
7
7
1
8
0
.
8
0
0
2
:
v
i
X
r
a

Scalable Combinatorial Bayesian Optimization with
Tractable Statistical models

Aryan Deshwal
Washington State University

Syrine Belakaria
Washington State University

Janardhan Rao Doppa
Washington State University

aryan.deshwal@wsu.edu

syrine.belakaria@wsu.edu

jana.doppa@wsu.edu

Abstract

We study the problem of optimizing expensive blackbox functions over combinatorial spaces
(e.g., sets, sequences, trees, and graphs). BOCS Baptista and Poloczek (2018) is a state-
of-the-art Bayesian optimization method for tractable statistical models, which performs
semi-deﬁnite programming based acquisition function optimization (AFO) to select the
next structure for evaluation. Unfortunately, BOCS scales poorly for large number of
binary and/or categorical variables. Based on recent advances in submodular relaxation
Ito and Fujimaki (2016) for solving Binary Quadratic Programs, we study an approach
referred as Parametrized Submodular Relaxation (PSR) towards the goal of improving the
scalability and accuracy of solving AFO problems for BOCS model. PSR approach relies on
two key ideas. First, reformulation of AFO problem as submodular relaxation with some
unknown parameters, which can be solved eﬃciently using minimum graph cut algorithms.
Second, construction of an optimization problem to estimate the unknown parameters with
close approximation to the true objective. Experiments on diverse benchmark problems
show signiﬁcant improvements with PSR for BOCS model. The source code is available at
https://github.com/aryandeshwal/Submodular Relaxation BOCS.

1. Introduction

Many real-world science and engineering applications involve optimizing combinatorial spaces
(e.g., sets, sequences, trees, and graphs) using expensive black-box evaluations. For example,
molecular optimization guided by physical lab experiments, where molecules are represented
as graphs or SMILE strings G´omez-Bombarelli et al. (2018). Similarly, in hardware design
optimization, we need to appropriately place the processing elements and communication
links for achieving high performance guided by expensive computational simulations to
emulate the real hardware.

Bayesian optimization (BO) Shahriari et al. (2016) is a popular framework for solving
expensive black-box optimization problem. BO framework consists of three key elements:
1) Statistical model (e.g., Gaussian Process) learned from past function evaluations; 2)
Acquisition function (AF) (e.g., expected improvement) to score the potential utility of
evaluating an input based on the statistical model; and 3) Acquisition function optimization
(AFO) to select the best candidate input for evaluation. In each BO iteration, the selected
input is evaluated and the statistical model is updated using the aggregate training data.
Most of the prior work on BO is focused on optimizing continuous spaces. There are two

1

 
 
 
 
 
 
key challenges to extend BO framework to combinatorial spaces. First, deﬁning a surrogate
statistical model over combinatorial objects. Second, search through the combinatorial space
to select the next structure for evaluation given such a statistical model.

Prior work on combinatorial BO has addressed these two challenges as follows. SMAC
Hutter et al. (2010, 2011) is a canonical baseline that employs complex statistical model in
the form of random forest and executes a hand-designed local search procedure for optimizing
the acquisition function. A recent work referred as COMBO Oh et al. (2019) proposed a
novel combinatorial graph representation for discrete spaces which allows using Gaussian
process with diﬀusion kernels. Reduction to continuous BO G´omez-Bombarelli et al. (2018)
employs an encoder-decoder architecture to learn continuous representation of combinatorial
objects from data and performs BO in this latent space. Unfortunately, this approach
requires a large dataset of combinatorial objects, for learning the latent space, which is
impossible to acquire for many real-world applications. BOCS Baptista and Poloczek (2018)
is another method that employs a tractable statistical model deﬁned over binary variables
and Thompson sampling as the acquisition function. These two choices within BOCS leads to
a semi-deﬁnite programming (SDP) based solution for solving AFO problems. Unfortunately,
BOCS approach scales poorly for large number of binary variables and for categorical
variables due to one-hot encoding representation.

Our work is inspired by the success of submodular relaxation based inference methods
in the structured prediction literature Kolmogorov and Zabih (2004); Tang et al. (2014);
Freedman and Drineas (2005); Gorelick et al. (2014a).
In this paper, we employ the
submodular relaxation based Binary Quadratic optimization approach proposed in Ito
and Fujimaki (2016) to improve the computational-eﬃciency and accuracy of solving AFO
problems for BOCS model. We refer to this approach as Parametrized Submodular Relaxation
(PSR) algorithm. First, we reformulate the AFO problem as submodular relaxation with
some parameters. This relaxed problem can be solved eﬃciently using minimum graph cut
algorithms Kolmogorov and Zabih (2004); Boykov and Kolmogorov (2004). The accuracy
of this relaxed problem critically depends on the unknown parameters. Therefore, we
solve an outer optimization problem to ﬁnd the values of unknown parameters with close
approximation to the true objective. To the best of our knowledge, this is the ﬁrst application
of submodular relaxation to solve combinatorial BO problems. Experimental results on
diverse real-world benchmarks show the eﬃcacy of PSR to improve the state-of-the-art on
combinatorial BO with tractable statistical models in terms of both computational-eﬃciency
and accuracy.

Contributions. The main contributions of this paper are:

• By leveraging the recent advances in submodular relaxation, we study the parametrized
submodular relaxation approach to improve the scalability and accuracy of solving
AFO problems for BOCS, the state-of-the-method for tractable statistical models.

• We perform comprehensive experiments on real-world benchmarks to show computational-
eﬃciency and accuracy improvements over existing BOCS method. The source code is
available on the GitHub repository https://github.com/aryandeshwal/Submodular
Relaxation BOCS.

2

2. Problem Setup

We are given a combinatorial space of structures X (e.g., sets, sequences, trees, graphs).
Without loss of generality, let each combinatorial structure x ∈ X be represented using n
discrete variables x1, x2, · · · , xn, where each variable xi takes k candidate values from the
set CV(xi). For binary variables, k equals 2 and for categorical variables, k is greater than 2.
We assume the availability of an unknown objective function F : X (cid:55)→ (cid:60) to evaluate each
combinatorial object x ∈ X . Each evaluation is expensive and results in outcome y = F(x).
For example, in hardware design optimization, x is a graph corresponding to the placement
of processing elements and communication links, and F(x) corresponds to an expensive
computational simulation. The overall goal is to minimize the number of objective function
evaluations to uncover a structure x ∈ X that approximately optimizes F. We consider
minimizing the objective F for the sake of technical exposition and consistent notation.

3. Related Work

There is very limited work on BO over discrete spaces when compared to continuous
space BO, which has seen huge growth over the last few years Frazier (2018); Frazier and
Wang (2016); Snoek et al. (2012); Wang and Jegelka (2017); Song et al. (2019); Hern´andez-
Lobato et al. (2016); Belakaria et al. (2019, 2020). SMAC Hutter et al. (2010, 2011) is one
canonical baseline which employs random forest as surrogate model and a hand-designed
local search procedure for optimizing the acquisition function. BOCS Baptista and Poloczek
(2018) employs a parametric statistical models over binary variables, which allows pricipled
acquisition function optimization based on semi-deﬁnite program solvers. COMBO Oh et al.
(2019) is a state-of-the-art non-parametric approach that employs Gaussian processes with
diﬀusion kernels deﬁned over discrete spaces as its surrogate model. However, COMBO
employs local search with random restarts for acquisition function optimization.

COMBO was shown to achieve better performance than BOCS for complex domains that
require modeling higher-order dependencies between discrete variables. However, BOCS
achieves good performance whenever the modeling assumptions (e.g., lower-order interactions
among variables) are met. Furthermore, the AFO problem in BOCS is a Binary Quadratic
Programming (BQP) problem which is well-studied in many ﬁelds including computer vision
Kolmogorov and Zabih (2004) and prescriptive price optimization Ito and Fujimaki (2016).
In comparison, the acquisition function optimization is much more challenging (results in
general non-linear combinatorial optimization problem) for methods such as COMBO and
SMAC that employs non-parametric statistical models. A learning to search framework
referred as L2S-DISCO Deshwal et al. (2020) was introduced recently to solve the challenges
of AFO problems with complex statistical models (e.g., GPs with discrete kernels and random
forest). The key insight behind L2S-DISCO is to directly tune the search via learning during
the optimization process to select the next structure for evaluation by leveraging the close
relationship between AFO problems across BO iterations (i.e., amortized AFO). Since the
main focus of this paper is on improving the scalability and accuracy of AFO for the tractable
statistical model introduced in BOCS, we describe the details of this approach below.

BOCS Approach. We now brieﬂy explain the BOCS method Baptista and Poloczek (2018)
that we intend to improve on. BOCS instantiates the three key elements of BO framework as

3

follows. 1) Surrogate statistical model: A linear Bayesian model deﬁned over binary variables
is employed as the surrogate model. The model is described as:

fα(x ∈ X ) = α0 +

(cid:88)

j

αjxj +

(cid:88)

i,j>i

αijxixj

(1)

where X = {0, 1}n and x ∈ X is a binary vector and α variables are drawn from a
sparsity-inducing horseshoe prior Carvalho et al. (2010). It was experimentally found that
the above second-order model provides an excellent trade-oﬀ between expressiveness and
accuracy. The α variables quantify the uncertainty of the model. 2) Acquisition function:
Thompson sampling Russo et al. (2018) is employed as the acquisition function because of
its proven theoretical and empirical properties in the context of BO. 3) Acquisition function
optimization: In each BO iteration, we select a candidate structure x ∈ X for evaluation that
minimizes the acquisition function. In the case of BOCS method, the acquisition function
optimization (AFO) problem becomes:

arg min
x∈X

fα(x) + λP (x)

(2)

where λP (x) being a regularization term commonly seen in multiple applications. BOCS
employs a semi-deﬁnite programming (SDP) based relaxation approach to solve the above
AFO problem.

Scalability Challenges of BOCS. There are multiple challenges associated with SDP
approach used for solving AFO problems in BOCS formulation. First, the time complexity
of a standard SDP solver grows at the rate of O(n6) Vandenberghe and Boyd (1996); Ito
and Fujimaki (2016), which is prohibitive for large dimensions. Second, the approximation
error for SDP based solution is known to be at most O(log n) Baptista and Poloczek (2018);
Charikar and Wirth (2004), which clearly grows as the dimensions increase, resulting in
the loss of accuracy as well. These scaling issues arise when the number of binary variables
are large. Since BOCS represents categorical variables using one-hot encoding, even a
small number of categorical variables can lead to a large number of binary variables (e.g.,
placement of processing elements in hardware design).

Our goal in this paper is to provide an algorithmic approach to improve the computational-

eﬃciency and accuracy of solving AFO problems for BOCS method.

4. Parametrized Submodular Relaxation

In this section, we describe our proposed parameterized submodular relaxation (PSR)
algorithm to improve the eﬃciency and accuracy of solving acquisition function optimization
(AFO) problems for BOCS model. We ﬁrst provide a high-level overview of PSR algorithm.
Subsequently, we describe the details of key algorithmic steps behind PSR.

4.1 High-level Overview of PSR Algorithm

The overall idea of using submodular relaxations for optimizing BQP problems is extensively
employed in computer vision Gorelick et al. (2014b). However, we employ the concrete
instantiation of this general framework as proposed in the context of prescriptive price

4

optimization Ito and Fujimaki (2016). To the best of our knowledge, this is the ﬁrst
application of submodular relaxation concepts to solve combinatorial BO problems.

Recall that AFO problem for BOCS method is:

arg min
x∈X

fα(x) + λP (x)

(3)

where λP (x) is a regularization term commonly seen in multiple applications. For example,
by choosing P (x) = (cid:107)x(cid:107)1, the optimization problem in 3 becomes a Binary Quadratic
Program (BQP) as given below:

arg min
x∈X

arg min
x∈X

(cid:88)

α0 +

(αj + λ)xj +

j
xT Ax + bT x

(cid:88)

i,j>i

αijxixj

(4)

(5)

In the general case, BQP is NP-hard Charikar and Wirth (2004); Garey and Johnson (2002).
We propose using an eﬃcient submodular relaxation with some unknown parameters (matrix
Λ) for solving this problem. A key advantage of this relaxation is that it allows us to
leverage minimum graph cut algorithms to eﬃciently solve it. The accuracy of this relaxed
problem critically depends on the unknown parameters Λ. Therefore, we can utilize an outer
optimization problem to ﬁnd the values of unknown parameters with close approximation to
the true acquisition function. We solve this optimization problem using an iterative algorithm
(steps 5-9 in Algorithm 1). We perform two algorithmic steps in each iteration. First, we
solve the parametrized submodular relaxation of the AFO problem using a minimum graph
cut algorithm (step 7). Second, we update the values of unknown parameters Λ using
proximal gradient descent (step 8). Algorithm 1 provides the complete pseudo-code of
combinatorial BO using PSR algorithm.

Advantages of PSR algorithm. When compared to the semi-deﬁnite programming
(SDP) relaxation approach to solve AFO problems in BOCS, PSR algorithm has signiﬁcant
advantages in terms of both computational-eﬃciency and accuracy of solving AFO problems.
First, PSR relies on a small number of calls (ﬁve to ten iterations based on our experiments)
to a minimum graph cut solver, which has relatively very low time-complexity (e.g. O(n3)
for preﬂow-push algorithms or O(n3 log n) for Dinic’s algorithm Ahuja et al. (1988)) which
is signiﬁcantly better than O(n6) for SDP approach. Second, our experiments show that
PSR algorithm signiﬁcantly improves the accuracy over SDP approach with increased
dimensionality.

4.2 Key Algorithmic Steps

The two main algorithmic steps of PSR algorithm are: submodularation of the objective
with unknown parameters and ﬁnding optimized parameters to improve the accuracy of
relaxation. We describe their details below.

4.2.1 Submodularization of the Objective

The binary quadratic objective function (xT Ax + bT x in 5) is called as submodular if all the
elements of the matrix A are non-positive, i.e., aij ∈ A ≤ 0 ∀i, j. Such functions are also

5

Algorithm 1 Combinatorial BO via PSR Algorithm
Input: X = Discrete space, F(x) = expensive objective function, statistical model fα(x ∈ X )
Output:

(xbest, F(xbest)), the best uncovered input xbest with its corresponding function value

1: Initialize statistical model fα with a small number of input-output examples; and t ← 0
2: repeat
3:
4:

Sample α from posterior of fα
Compute the next input to evaluate via acquisition function optimization:
xt+1 ← arg minx∈X AF (fα, x)
Initialize parameters Λ
repeat

Solve parametrized submodular relaxation of the AFO problem using graph cuts
Update Λ via proximal gradient descent

5:
6:
7:
8:
9:
10:
11:
12:
13: until convergence or maximum iterations
14: xbest ← arg minxt∈D yt
15: return the best uncovered input xbest and the corresponding function value F(xbest)

Evaluate objective function F(x) at xt+1 to get yt+1
Aggregate the data: Dt+1 ← Dt ∪ {(xt+1, yt+1)} and update the model
t ← t + 1

until convergence of optimization over Λ

called as regular functions in the computer vision community Kolmogorov and Zabih (2004).
Note that this deﬁnition can also be derived from the popular deﬁnition of submodularity
deﬁned for set functions. This can be achieved by deﬁning inclusion of an element in a set
by 1 and exclusion by 0 thereby resulting in an equivalence between binary valued functions
and set functions. However, in our setting, it is not necessary that the BQP objective in
Equation 5 follows the submodularity property. As mentioned earlier, we approximate the
objective by constructing a submodular relaxation in the same way as Ito and Fujimaki
(2016). This relaxation is parametrized by a matrix Λ, which is directly related to the quality
of approximation.

Consider the objective in 5 written as a sum of two terms decomposed over the positive

(A+) and non-positive (A−) terms of matrix A:

xT Ax + bT x = xT A+x + xT A−x + bT x

(6)

where A+ + A− = A and A+ and A− are deﬁned as follows:

A+ =

(cid:26) aij
0

if aij > 0
if aij ≤ 0

∀aij ∈ A

A− =

(cid:26) aij
0

if aij ≤ 0
if aij > 0

∀aij ∈ A

The second term in Equation 6 (xT A−x + bT x) is a submodular function. Similar to
the strategy employed for prescriptive price optimization Ito and Fujimaki (2016), we
construct a submodular relaxation of the ﬁrst term by bounding it below by a linear
function h(x) such that h(x) ≤ xT A+x ∀x ∈ {0, 1}n. It can be easily seen that h(x) =
xT (A+ ◦ Λ)1 + 1T (A+ ◦ Λ)x − 1T (A+ ◦ Λ)1 is an aﬃne lower bound to xT A+x, where ◦

6

represents Hadamard product and Λ is a matrix deﬁned as follows: Λ = [λij]n×n, where
λij ∈ [0, 1] is a parameter satisfying the following inequality:

Using h(x) as the aﬃne lower bound, our new optimization problem becomes:

λij(xi + xj − 1) ≤ xixj

h(x) + xT A−x + bT x

min
x∈X

(7)

(8)

We use hΛ(x) for denoting the combination of two linear terms in (8) i.e. hΛ(x) = h(x)+bT x,
along with signifying the dependence on Λ parameter.

hΛ(x) + xT A−x

min
x∈X

(9)

It should be noted again that the objective in (9) is a lower bound of the original objective
in (6). This relaxed submodular objective can be solved exactly by turning it into a
minimum graph cut problem and utilizing an eﬃcient minimum graph cut algorithm
Boykov and Kolmogorov (2004). For a given Λ, we employ a standard graph construction
strategy Kolmogorov and Zabih (2004) dependent on the α parameters sampled from
the surrogate model fα at each BO iteration. A graph G is constructed with n + 2
vertices: V = {s, t, v1, · · · , vn} where each non-terminal vertex vi encode one discrete
variable xi ∈ {0, 1}. For each term depending on one variable xi (each non-zero entry of
hΛ(x) in (9)), an edge is added in the graph from s to vi with capacity hΛ(xi) if hΛ(xi)
is positive or from vi to t with capacity −1 · hΛ(xi) if it is negative. Further, each term
depending on pair of variables xixj (each non-zero entry of A− in (9)) is represented by two
edges i.e. edge vi to vj and edge vj to t with capacity −1 · A−
ij.

4.2.2 Optimizing Parameters to Improve Accuracy

The quality of approximation of the above-mentioned submodular relaxation objective
Ito and Fujimaki (2016) constructed an outer
(9) critically depends on Λ parameters.
optimization problem to improve the accuracy of this relaxation and maximized the objective
w.r.t Λ to achieve the best approximation possible. We use the same procedure which is
described below. By including the outer optimization over Λ, the overall problem becomes:

max
Λ∈[0,1]n×n

(min
x∈D

hΛ(x) + xT A−x)

(10)

Equivalently, the outer maximization can be turned into minimization by considering the
negative of the outer objective.

min
Λ∈[0,1]n×n

−1 · (min
x∈D

hΛ(x) + xT A−x)

(11)

This optimization problem can be solved eﬃciently using an iterative algorithm that alternates
between solving the inner optimization over x via graph cut formulation and proximal gradient
descent over Λ. If xi is the solution of the submodularized inner objective in (11) at the ith
iteration for a ﬁxed Λi, the update equation for Λ is given as follows:

Λi+1 = (Λi − ηiGi)⊥

(12)

7

where ηi is the step size, Gi is the sub-gradient of the outer objective in (11) deﬁned as
Gi = A+ ◦ (11T − xi1T − 1xT
i ) and ⊥ is the projection operator for any matrix P deﬁned
as P ⊥
ij = {0 if Pij < 0, 1 if Pij > 1, and Pij otherwise} . We employ proximal gradient
descent because it scales gracefully, is amenable to recent advances in auto-diﬀerentiation
tools, and fast convergence Li and Lin (2015). We require few iterations ( 5-10) of proximal
gradient descent and each inner optimization is very fast because of strongly polynomial
graph cut algorithms. Indeed, our experiments validate this claim over multiple real-world
benchmarks.

5. Experiments and Results

In this section, we describe our experimental setup, and present results comparing state-of-
the-art BOCS method with SDP relaxation and our proposed parameterized submodular
relaxation (PSR) algorithm.

5.1 Experimental Setup

Benchmark domains. We employ four diverse synthetic and real-world benchmarks for
our empirical evaluation.

1. Binary quadratic programming (BQP). The goal in binary quadratic program-
ming (BQP) Baptista and Poloczek (2018) is to maximize a binary quadratic function
with l1 regularization: maxx∈{0,1}n(xT Qx − λ(cid:107)x(cid:107)1), where Q is a randomly generated
matrix deﬁned as Hadamard product of two matrices (M and K); Q = M ◦ K, where
M ∈ Rn×n, Mij = N (0, 1), N (0, 1) stands for the standard Gaussian distribution and
K ∈ Rn×n, Kij = exp(−(i − j)2/α2), α is the correlation length parameter.

2. Contamination. This problem considers a food supply with n stages, where a
binary {0,1} decision (xi) must be made at each stage to prevent the food from being
contaminated with pathogenic micro-organisms Hu et al. (2010). Each prevention eﬀort at
stage i can be made to decrease the contamination by a given random rate Γi and incurring
a cost ci. The contamination spreads with a random rate Λi if no prevention eﬀort is taken.
The overall goal is to ensure that the fraction of contaminated food at each stage i does not
exceed an upper limit Ui with probability at least 1 − (cid:15) while minimizing the total cost of
all prevention eﬀorts. Following Baptista and Poloczek (2018), the lagrangian relaxation
based problem formulation is given below:

arg min

x

(cid:34)

n
(cid:88)

i=1

cixi +

ρ
T

T
(cid:88)

k=1

(cid:35)

1{Zk>Ui}

+ λ(cid:107)x(cid:107)1

where λ is a regularization coeﬃcient, Zi is the fraction of contaminated food at stage i,
violation penalty coeﬃcient ρ=1, and T =100.

3. Sparsiﬁcation of zero-ﬁeld Ising models (Ising). The distribution of a zero
ﬁeld Ising model p(z) for z ∈ {−1, 1}n is characterized by a symmetric interaction matrix J p
whose support is represented by a graph Gp = ([n], Ep) that satisﬁes (i, j) ∈ Ep if and only
if J p
ij (cid:54)= 0 holds Baptista and Poloczek (2018). The overall goal in this problem is to ﬁnd a
close approximate distribution q(z) while minimizing the number of edges in Eq. Therefore,

8

the objective function in this case is a regularized KL-divergence between p and q as given
below:

DKL(p||qx) =

(cid:88)

(i,j)∈Ep

(J p

ij − J q

ij)Ep[zizj] + log(Zq/Zp)

where Zq and Zp are partition functions corresponding to p and q respectively, and x ∈
{0, 1}Eq is the decision variable representing whether each edge is present in Eq or not.

4. Low auto-correlation binary sequences (LABS). The problem is to ﬁnd a
binary {+1,-1} sequence S = (s1, s2, · · · , sn) of given length n that maximizes merit factor
deﬁned over a binary sequence as given below:

Merit Factor(S) =

where E(S) =

n2
E(S)
(cid:32)n−k
(cid:88)

n−1
(cid:88)

(cid:33)2

sisi+k

k=1

i=1

The LABS problem has multiple applications in diverse scientiﬁc disciplines Packebusch and
Mertens (2015).

Algorithmic setup. For the sake of consistency, we convert all benchmark problems to
minimization. Note that this can be achieved by minimizing the negative of the original
objective if the true goal is to maximize the objective. We built our code on top of the
open-source Python implementation of BOCS 1. We employed the Boykov-Kolmogorov
algorithm from graph-tool library 2 for solving minimum graph cut formulation of the
relaxed submodular acquisition function objective noting that any minimum cut algorithm
can be used to the same eﬀect. We employed two initializations (random and 11T /2) for
optimizing Λ parameter within PSR noting that both gave similar results. We ran proximal
gradient descent procedure for a maximum of 10 iterations on all benchmarks and achieved
convergence. All the reported results are averaged over 10 random runs.

Evaluation metrics. We demonstrate the advantages of our PSR algorithm for combina-
torial BO by comparing it with the state-of-the-art BOCS approach along two fronts.

1) Scalability and accuracy of AF optimization. We compare PSR and SDP
approaches for solving acquisition function optimization problems within BOCS method.
To evaluate scalability for AFO, we report the average AFO time across all BO iterations
normalized w.r.t PSR. Suppose TSDP and TP SR stand for average AFO time for some
input dimensionality d. We normalize TSDP and TP SR using the AFO time of PSR for
the smallest dimension. To evaluate the accuracy of solving AFO problems, we report the
average percentage improvement in the AF objective achieved by PSR when compared to
the corresponding AF objective from SDP.

2) Overall BO accuracy. We use the best function value achieved after a given
number of BO iterations (function evaluations) as a metric to evaluate the two methods:

1. https://github.com/baptistar/BOCS
2. https://graph-tool.skewed.de/

9

(a) BQP

(b) Contamination

(c) Ising

(d) LABS

Figure 1: Results comparing PSR algorithm and SDP approach on average AFO time
normalized w.r.t PSR. The title of each ﬁgure refers to the benchmark with corresponding
parameter (if any).

BOCS w/ SDP and BOCS w/ PSR. Note that BOCS is already shown to signiﬁcantly improve
over SMAC Baptista and Poloczek (2018). The method that uncovers high-performing
combinatorial structures with less number of function evaluations is considered better. We
use the total number of BO iterations similar to BOCS Baptista and Poloczek (2018).

5.2 Results for Acquisition Function Optimization

We present a canonical result for each benchmark domain in Figure 1 and 3 by ﬁxing the
objective parameter (if any) to a particular value, e.g., α = 1, λ = 0.001 for BQP and
λ = 0.0001 for Contamination and Ising in Figure 1 while noting that all experiments
followed the same pattern.

Average AFO time. Figure 1 shows the results of PSR and SDP approaches as a function
of increasing dimension. Recall that we normalize the average AFO time w.r.t that of PSR
for smallest dimension (base case). We can clearly see that the proposed PSR approach
requires signiﬁcantly low computation time when compared to the SDP approach and the
gap increases with increasing input dimensions. This supports our claim that PSR algorithm

10

10203040Number of input variables (dimension)0102030Normalized AFO time w.r.t PSRBQP (=1,=0.001)SDPPSR255070Number of input variables (dimension)0100200300400Normalized AFO time w.r.t PSRContamination (=0.001)SDPPSR122440Number of input variables (dimension)2468Normalized AFO time w.r.t PSRIsing (=0.001)SDPPSR203040Number of input variables (dimension)246810Normalized AFO time w.r.t PSRLABSSDPPSR(a) BQP

(b) Contamination

(c) Ising

(d) LABS

Figure 2: Results comparing the accuracy of the AF minimizers obtained by PSR and SDP
approach measured in terms of Average percent improvement in the AF objective over all
iterations of the BO procedure.

improves the scalability of AFO problems in combinatorial BO setting. It should be noted
that AFO problem is solved at each BO iteration. For example, if we run BO for 250
iterations, we need to solve 250 AFO problems. Therefore, the computational-eﬃciency of
PSR is compounded across the entire BO procedure.

Average percentage improvement in AF objective. PSR algorithm also ﬁnds better
optimized value for AFO problems on each benchmark domain as shown in Figure 2. The
vertical axis of the plots in Figure 2 represent the average percentage improvement in AF
objective obtained by PSR when compared to that obtained by SDP (higher the better).
PSR algorithm always ﬁnds a minimizer with lower AF value when compared to SDP’s
minimizer on all benchmarks. Furthermore, this accuracy gap increases with increasing
dimensions reinforcing the ability of PSR to scale to large dimensions while also improving
the accuracy.

5.3 Results for Overall BO Accuracy

The main goal in BO is to ﬁnd best accuracy on the true expensive black-box function
O. Ideally, the gains in accuracy for solving AFO problems as shown in previous section

11

10152025303540Number of input variables (dimension)24681012Average percentage improvementBQP(=1, =0)(=1, =0.0001)(=1, =0.01)3040506070Number of input variables (dimension)2.62.83.03.2Average percentage improvementContamination(=0)(=0.0001)(=0.01)3040506070Number of input variables (dimension)50100150200Average percentage improvementIsing(=0)(=0.0001)(=0.01)2025303540Number of input variables (dimension)681012Average percentage improvementLABS(a) BQP

(b) Contamination

(c) Ising

(d) LABS

Figure 3: Results comparing BOCS with PSR algorithm and BOCS with SDP approach on
best function value achieved versus number of iterations. The horizontal axis (x-axis) depicts
number of iterations while the vertical axis (y-axis) depicts the best function value.

should reﬂect in the overall BO performance using the proposed PSR approach. Indeed,
Figure 3 clearly shows that using the BOCS model with PSR algorithm improves the overall
accuracy of the BO procedure on all benchmark domains. This is a direct consequence of
the improved accuracy achieved by the PSR algorithm in solving AFO problems at each BO
iteration. All the reported results are averaged over 10 random runs.

6. Conclusions

This paper studied a principled approach referred as parametrized submodular relaxation
(PSR) to improve the scalability and accuracy of the state-of-the-art combinatorial Bayesian
optimization algorithm with tractable statistical models called BOCS. The key idea is to
reformulate the acquisition function optimization to select the next structure for evaluation
as submodular relaxation with some parameters, and perform search over these parameters
to improve the accuracy of this relaxed problem. Our experimental results on diverse

12

050100150200250Number of iterations1816141210864Best function valueBQP (dimension = 40, =1 and =0.0001)BOCS w/ SDPBOCS w/ PSR050100150200250Number of iterations63646566Best function valueContamination (dimension = 70 and =0.0001)BOCS w/ SDPBOCS w/ PSR0255075100125150Number of iterations0.00.51.01.52.02.5Best function valueIsing (dimension = 24 and =0.0001)BOCS w/ SDPBOCS w/ PSR050100150200250Number of iterations0.120.100.080.060.040.02Best function valueLABS (dimension = 40)BOCS w/ SDPBOCS w/ PSRbenchmarks showed that PSR algorithm signiﬁcantly improved the computational-eﬃciency
and accuracy of BOCS.

Acknowledgements. The ﬁrst author would like to thank Shinji Ito for useful discussions
related to this research. The authors gratefully acknowledge the support from National
Science Foundation (NSF) grants IIS-1845922 and OAC-1910213. The views expressed are
those of the authors and do not reﬂect the oﬃcial policy or position of the NSF.

References

Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. Network ﬂows. 1988.

Ricardo Baptista and Matthias Poloczek. Bayesian optimization of combinatorial structures.
In Proceedings of the 35th International Conference on Machine Learning (ICML), pages
462–471, 10–15 Jul 2018.

Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search
for multi-objective Bayesian optimization. In Proceedings of International Conference on
Neural Information Processing Systems (NeurIPS), 2019.

Syrine Belakaria, Aryan Deshwal, Nitthilan Kannappan Jayakodi, and Janardhan Rao
Doppa. Uncertainty-aware search framework for multi-objective Bayesian optimization.
In AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 10044–10052, 2020.

Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of min-cut/max-ﬂow
algorithms for energy minimization in vision. IEEE Transactions on Pattern Analysis &
Machine Intelligence, (9):1124–1137, 2004.

Carlos M Carvalho, Nicholas G Polson, and James G Scott. The horseshoe estimator for

sparse signals. Biometrika, 97(2):465–480, 2010.

Moses Charikar and Anthony Wirth. Maximizing quadratic programs: Extending
grothendieck’s inequality. In 45th Annual IEEE Symposium on Foundations of Computer
Science, pages 54–60. IEEE, 2004.

Aryan Deshwal, Syrine Belakaria, Janardhan Rao Doppa, and Alan Fern. Optimizing discrete
spaces via expensive evaluations: A learning to search framework. In AAAI Conference
on Artiﬁcial Intelligence (AAAI), pages 3773–3780, 2020.

Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.

Peter I Frazier and Jialei Wang. Bayesian optimization for materials design. In Information

Science for Materials Discovery and Design, pages 45–75. Springer, 2016.

Daniel Freedman and Petros Drineas. Energy minimization via graph cuts: Settling what is
possible. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR’05), volume 2, pages 939–946. IEEE, 2005.

Michael R Garey and David S Johnson. Computers and intractability, volume 29. freeman

New York, 2002.

13

Rafael G´omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos´e Miguel Hern´andez-Lobato,
Benjam´ın S´anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D
Hirzel, Ryan P Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a
data-driven continuous representation of molecules. ACS central science, 4(2):268–276,
2018.

Lena Gorelick, Yuri Boykov, Olga Veksler, Ismail Ben Ayed, and Andrew Delong. Sub-
modularization for binary pairwise energies. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 1154–1161, 2014a.

Lena Gorelick, Yuri Boykov, Olga Veksler, Ismail Ben Ayed, and Andrew Delong. Sub-
modularization for binary pairwise energies. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 1154–1161, 2014b.

Daniel Hern´andez-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive
entropy search for multi-objective Bayesian optimization. In International Conference on
Machine Learning, pages 1492–1501, 2016.

Yingjie Hu, JianQiang Hu, Yifan Xu, Fengchun Wang, and Rong Zeng Cao. Contamination
control in food supply chain. In Proceedings of the Winter Simulation Conference, WSC
’10, pages 2678–2681, 2010. ISBN 978-1-4244-9864-2. URL http://dl.acm.org/citation.
cfm?id=2433508.2433840.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for
general algorithm conﬁguration (extended version). Technical Report TR-2010-10, Uni-
versity of British Columbia, Department of Computer Science, 2010. Available online:
http://www.cs.ubc.ca/˜hutter/papers/10-TR-SMAC.pdf.

Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization
for general algorithm conﬁguration. In International conference on learning and intelligent
optimization, pages 507–523, 2011.

Shinji Ito and Ryohei Fujimaki. Large-scale price optimization via network ﬂow. In Advances

in Neural Information Processing Systems, pages 3855–3863, 2016.

Vladimir Kolmogorov and Ramin Zabih. What energy functions can be minimized via graph
cuts? IEEE Transactions on Pattern Analysis & Machine Intelligence, (2):147–159, 2004.

Huan Li and Zhouchen Lin. Accelerated proximal gradient methods for nonconvex program-

ming. In Advances in Neural Information Processing Systems, pages 379–387, 2015.

Changyong Oh, Jakub Tomczak, Efstratios Gavves, and Max Welling. Combinatorial bayesian
In Advances in Neural Information

optimization using the graph cartesian product.
Processing Systems, pages 2914–2924, 2019.

Tom Packebusch and Stephan Mertens. Low autocorrelation binary sequences. Journal
of Physics A: Mathematical and Theoretical, 49 (2016) 165001, 2015. doi: 10.1088/
1751-8113/49/16/165001.

14

Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A
tutorial on thompson sampling. Foundations and Trends R(cid:13) in Machine Learning, 11(1):
1–96, 2018.

Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking
the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE,
104(1):148–175, 2016.

Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of
machine learning algorithms. In Proceedings of the 26th Annual Conference on Advances
in Neural Information Processing Systems., pages 2960–2968, 2012.

Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian

optimization with Gaussian processes. In AISTATS, 2019.

Meng Tang, Ismail Ben Ayed, and Yuri Boykov. Pseudo-bound optimization for binary
energies. In European Conference on Computer Vision, pages 691–707. Springer, 2014.

Lieven Vandenberghe and Stephen Boyd. Semideﬁnite programming. SIAM review, 38(1):

49–95, 1996.

Zi Wang and Stefanie Jegelka. Max-value entropy search for eﬃcient Bayesian optimization.

In International Conference on Machine Learning (ICML), 2017.

15

