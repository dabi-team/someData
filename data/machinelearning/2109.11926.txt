1
2
0
2

p
e
S
4
2

]

C
O
.
h
t
a
m

[

1
v
6
2
9
1
1
.
9
0
1
2
:
v
i
X
r
a

Sinkhorn Distributionally Robust Optimization

Jie Wang
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA 30332, jwang3163@gatech.edu

Rui Gao
Department of Information, Risk, and Operations Management, University of Texas at Austin, Austin, TX 78712,
rui.gao@mccombs.utexas.edu

Yao Xie
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA 30332, yao.xie@isye.gatech.edu

We study distributionally robust optimization with Sinkorn distance‚Äîa variant of Wasserstein distance based on
entropic regularization. We derive convex programming dual reformulations when the nominal distribution is an
empirical distribution and a general distribution, respectively. Compared with Wasserstein DRO, it is computationally
tractable for a larger class of loss functions, and its worst-case distribution is more reasonable. To solve the dual
reformulation, we propose an eÔ¨Écient batch gradient descent with a bisection search algorithm. Finally, we provide
various numerical examples using both synthetic and real data to demonstrate its competitive performance.

Key words: Wasserstein distributionally robust optimization, Sinkhorn distance, Duality theory

1. Introduction

Decision-making problems under uncertainty have broad applications in operations research, machine
learning, engineering, and economics. When the data involves uncertainty due to measurement error,
insuÔ¨Écient sample size, contamination, and anomalies, or model misspeciÔ¨Åcation, distributionally
robust optimization (DRO) is a promising approach to data-driven optimization, by seeking a minimax
robust optimal decision that minimizes the expected loss under the most adverse distribution within
a given set of relevant distributions, called ambiguity set. It provides a principled framework to
produce a solution with more promising out-of-sample performance than the traditional sample average
approximation (SAA) method for stochastic programming [86]. We refer to [81] for a recent survey on
DRO.

At the core of DRO is the choice of the ambiguity set. Ideally, a good ambiguity set should take
account of the properties of practical applications while maintaining the computational tractability
of resulted DRO formulation; and it should be rich enough to contain all distributions relevant to
the decision-making but, at the same time, should not include unnecessary distributions that lead
to overly conservative decisions. Various DRO formulations have been proposed in the literature.
Among them, the ambiguity set based on Wasserstein distance has recently received much attention
[104, 67, 17, 46]. The Wasserstein distance incorporates the geometry of sample space, and thereby
is suitable for comparing distributions with non-overlapping supports and hedging against data
perturbations [46]. Nice statistical performance guarantees have been established for Wasserstein
DRO both asymptotically [16, 19, 18], non-asymptotically [44, 24, 84], and empirically in a variety of
applications in operations research [13, 30, 87, 73, 88, 100], machine learning [85, 25, 66, 14, 72, 93],
stochastic control [106, 1, 91, 38, 107, 99], etc; see [61] and references therein for more discussions.
We also provide a more detailed literature survey by the end of this section.

On the other hand, the current Wasserstein DRO framework is not without limitation. First, from the
computational eÔ¨Éciency perspective, the tractability of Wasserstein DRO is usually available only under
somewhat stringent conditions on the loss function, as its dual formulation involves a subproblem that
requires the global supremum of some regularized loss function over the sample space. In particular,
for 1-Wasserstein DRO, a convex reformulation is only known when the loss function can be expressed

1

 
 
 
 
 
 
2

as a pointwise maximum of Ô¨Ånitely many concave functions [67] and eÔ¨Écient Ô¨Årst-order algorithm is
proposed only for special loss functions such as logistic loss [62]; and for 2-Wasserstein DRO, eÔ¨Écient
Ô¨Årst-order algorithms have been developed only for smooth loss functions and suÔ¨Éciently small radius
(or equivalently, suÔ¨Éciently large Lagrangian multiplier) so that the involved subproblem becomes
strongly convex [89, 20, 63, 26]. Second, from the modeling perspective, for data-driven Wasserstein
DRO in which the nominal distribution is Ô¨Ånitely supported (usually the empirical distribution),
the worst-case distribution is shown to be a discrete distribution [46], despite that the underlying
true distribution in many practical applications may well be continuous. This raises the concern of
whether Wasserstein DRO hedges the right family of distribution and whether it causes potentially
over-conservative performance.

To address these potential issues while maintaining the advantages of Wasserstein DRO, in this
paper, we propose Sinkhorn DRO, which hedges against distributions that are close to some nominal
distribution in Sinkhorn distance [35]. The Sinkhorn distance can be viewed as a smoothed Wasserstein
distance, deÔ¨Åned as the cheapest transport cost between two distributions associated with an optimal
transport problem with entropic regularization (see DeÔ¨Ånition 1 in Section 2). As far as we know, this
paper is the Ô¨Årst to study the DRO formulation using the Sinkhorn distance. Our main contributions
are summarized as follows.

(I) We derive a strong duality reformulation for Sinkhorn DRO (Theorem 1), both when the
nominal distribution is a data-driven empirical distribution (Section 3.2) and when the nominal
distribution is any arbitrary distribution (Section 3.3). The Sinkhorn dual objective smooths
the maximization subproblem in the Wasserstein dual objective, and converges to Wasserstein
dual objective as the entropic regularization parameter goes to zero (Remark 3). Moreover, the
dual objective of Sinkhorn DRO is upper bounded by that of the KL-divergence DRO with the
nominal distribution being a kernel density estimator (Remark 4).

(II) As a byproduct of our duality proof, we characterize the worst-case distribution of the Sinkhorn
DRO (Remark 5), which is absolutely continuous with respect to some reference measure such
as Lebesgue or counting measure. Compared with Wasserstein DRO, the worst-case distribution
of Sinkhorn DRO is not necessarily Ô¨Ånitely supported even when the nominal distribution is a
Ô¨Ånitely supported distribution. This indicates that Sinkhorn DRO is a more Ô¨Çexible modeling
choice for many applications.

(III) On the algorithmic aspect, we propose a computationally eÔ¨Écient Ô¨Årst-order method for solving
the Sinkhorn DRO problem (Section 4), based on batch gradient descent and bisection search.
Its convergence guarantees are also developed. Compared with Wasserstein DRO, the dual
problem of Sinkhorn DRO is computationally tractable for any measurable loss functions.
(IV) We provide experiments (Section 5) to validate the performance of the proposed Sinkhorn
DRO model in the context of newsvendor problem, mean-risk portfolio optimization, and
semi-supervised learning, using both synthetic and real data sets. Numerical results demonstrate
its superior out-of-sample performances compared with several benchmarks including SAA,
Wasserstein DRO, and KL-divergence DRO.

Related Literature

On DRO Models Construction of ambiguity sets plays a key role in the performance of DRO models.
Generally, there are two ways to construct ambiguity sets in literature. First, ambiguity sets can be
deÔ¨Åned using descriptive statistics, such as the support information [11], moment conditions [83,
36, 52, 112, 103, 29, 12], shape constraints [80, 94], marginal distributions [43, 69, 2, 39]. Second, a
more recently popular approach that makes full use of the available data is to consider distributions
within a pre-speciÔ¨Åed statistical distance from a nominal distribution, usually chosen as the empirical
distribution of samples. Commonly used statistical distances used in literature include ùúô-divergence [57,
10, 102, 9, 41], Wasserstein distance [78, 104, 67, 110, 17, 46, 28, 105], and maximum mean discrepancy

3

[92, 111]. Our proposed Sinkhorn DRO can be viewed as a variant of Wasserstein DRO. In the
literature on Wasserstein DRO, besides the computational tractability, its regularization eÔ¨Äects and
statistical inference have also be investigated. In particular, it has been shown that Wasserstein DRO is
asymptotically equivalent to a statistical learning problem with variation regularization [45, 16, 84],
and when the radius is chosen properly, the worst-case loss of Wasserstein DRO serves as an upper
conÔ¨Ådence bound on the true loss [16, 19, 44, 18] . Other variants of Wasserstein DRO have been
explored, by combining with other information such as moment information [48, 96] and marginal
distributions [47, 42].

Finally, we remark that a recent work [42] on distributionally robust optimization with given
marginals share a somewhat similar spirit as our work. They start from a dual formulation and propose
to replace its supremum subproblem with a smooth penalization, and then dualize the dual problem to
obtain a primal problem that penalizes the entropy of the distribution. The main diÔ¨Äerences between
their formulation and ours are that: (i) we do not impose marginal distribution constraints in the
primal formulation; (ii) our entropic regularization is on the transport plan (joint distribution) between
the nominal distribution and a candidate distribution in the ambiguity set, but their entropic penalty
is imposed only on the candidate distributions in the ambiguity set; (iii) our dual formulation smooths
the supremum subproblem by log-exp-sum function, which is not covered in their considered family of
penalizations.

On Sinkhorn Distance Sinkhorn distance [35] is proposed to improve the computational com-
plexity of Wasserstein distance by regularizing the original mass transportation problem with relative
entropy penalty on the transport mapping. In particular, this distance can be computed from its dual
form by optimizing two blocks of decision variables alternatively, which only requires simple matrix-
vector products and therefore signiÔ¨Åcantly improves the computation speed [77]. Such an approach
Ô¨Årst aroused in the areas of economics and survey statistics [60, 109, 37, 7], and its convergence
analysis is attributed to the mathematician Sinkhorn [90], which gives the name of Sinkhorn distance.
A recent work [3] further designs an accelerated algorithm to compute Sinkhorn distance in near-linear
time. Using Sinkhorn distance other than Wasserstein distance has been demonstrated to be beneÔ¨Åcial
because of lower computational cost in various applications, including domain adaptations [32, 33, 31],
generative modeling [50, 76, 65, 75], dimensionality reduction [64, 97, 98, 58], etc. To the best of
our knowledge, the study of Sinkhorn distance for distributionally robust optimization is new in the
literature.

The rest of the paper is organized as follows. In Section 2, we describe the main formulation for
the Sinkhorn DRO model. In Section 3, we develop its strong dual reformulation. In Section 4, we
propose a Ô¨Årst-order optimization algorithm that solves the reformulation eÔ¨Éciently. We report several
numerical results in Section 5, and conclude the paper in Section 6. All omitted proofs can be found in
Appendix.

2. Model Setup

Notation. Assume that the logarithm function log is taken with base ùëí. For a positive integer ùëÅ ,
we write [ùëÅ ] for {1, 2, . . . , ùëÅ }. For a measurable set Z, denote by M(Z) the set of measures (not
necessarily probability measures) on Z, and P (Z) the set of probability measures on Z. Given a
probability distribution ‚Ñô and a measure ùúá, we denote supp(‚Ñô) the support of ‚Ñô, and write ‚Ñô (cid:28) ùúá
if ‚Ñô is absolutely continuous with respect to ùúá. For a given element ùë•, denote by ùõøùë• the one-point
probability distribution supported on {ùë• }. Denote ‚Ñô ‚äó ‚Ñö as the product measure of two probability
distributions ‚Ñô and ‚Ñö. Denote by Proj1#ùõæ and Proj2#ùõæ the Ô¨Årst and the second marginal distributions
of ùõæ, respectively. For a given set ùê¥, deÔ¨Åne the characteristic function 1ùê¥ (ùë•) such that 1ùê¥ (ùë•) = 1 when
ùë• ‚àà ùê¥ and otherwise 1ùê¥ (ùë•) = 0, and deÔ¨Åne the indicator function ùúèùê¥ (ùë•) such that ùúèùê¥ (ùë•) = 0 when ùë• ‚àà ùê¥

4

and otherwise ùúèùê¥ (ùë•) = ‚àû. DeÔ¨Åne the distance between two sets ùê¥ and ùêµ in the Euclidean space as
Dist(ùê¥, ùêµ) = supùë• ‚ààùê¥ inf ùë¶ ‚ààùêµ (cid:107)ùë• ‚àí ùë¶ (cid:107)2.

We Ô¨Årst review the deÔ¨Ånition of Sinkhorn distance.

Definition 1 (Sinkhorn Distance). Let Z be a measurable set. Consider distributions ‚Ñô, ‚Ñö ‚àà P (Z),
and let ùúá, ùúà ‚àà M(Z) be two reference measures such that ‚Ñô (cid:28) ùúá, ‚Ñö (cid:28) ùúà. For regularization parameter
ùúñ ‚â• 0, the Sinkhorn distance between two distributions ‚Ñô and ‚Ñö is deÔ¨Åned as

Wùúñ (‚Ñô, ‚Ñö) = inf

ùõæ ‚ààŒì (‚Ñô,‚Ñö)

(cid:8)ùîº(ùëã ,ùëå )‚àºùõæ [ùëê (ùëã, ùëå )] + ùúñùêª (ùõæ | ùúá ‚äó ùúà)(cid:9) ,

where Œì(‚Ñô, ‚Ñö) denotes the set of joint distributions whose Ô¨Årst and second marginal distributions are
‚Ñô and ‚Ñö respectively, ùëê (ùë•,ùë¶) denotes the cost function, and ùêª (ùõæ | ùúá ‚äó ùúà) denotes the relative entropy of
ùõæ with respect to the product measure ùúá ‚äó ùúà:

ùêª (ùõæ | ùúá ‚äó ùúà) =

‚à´

log

(cid:18) dùõæ (ùë•,ùë¶)

(cid:19)

dùúá (ùë•) dùúà (ùë¶)

dùõæ (ùë•,ùë¶).

Remark 1 (Variants of Sinkhorn Distance). Sinkhorn distance in DeÔ¨Ånition 1 is based on general
reference measures ùúá and ùúà. Special forms of the distance has been investigated in the literature, for
instance, when the reference measures ùúá and ùúà were chosen to be ‚Ñô, ‚Ñö, i.e., marginal distributions
of ùõæ, respectively [49, Section 2]. The relative entropy regularization term can also be considered
as a hard-constrained variant for the optimal transport problem, which has been discussed in [35,
DeÔ¨Ånition 1] and [8]:

‚ô¶

W Info
ùëÖ

(‚Ñô, ‚Ñö) = inf

ùõæ ‚ààŒì (‚Ñô,‚Ñö)

(cid:8)ùîº(ùëã ,ùëå )‚àºùõæ [ùëê (ùëã, ùëå )] : ùêª (ùõæ | ‚Ñô ‚äó ‚Ñö) ‚â§ ùëÖ(cid:9) ,

where ùëÖ ‚â• 0 quantiÔ¨Åes the upper bound for the relative entropy between distributions ùõæ and ‚Ñô ‚äó ‚Ñö. An-
other variant of the optimal transport problem is to consider the negative entropy for regularization [35,
Equation (2)]:

W Ent
ùúñ

(‚Ñô, ‚Ñö) = inf

ùõæ ‚ààŒì (‚Ñô,‚Ñö)

(cid:8)ùîº(ùëã ,ùëå )‚àºùõæ [ùëê (ùëã, ùëå )] + ùúñùêª (ùõæ)(cid:9) ,

where ùêª (ùõæ) = ‚à´ log
dùõæ (ùë•,ùë¶) and dùë•, dùë¶ are Lebesgue measures if the corresponding marginal
distributions are continuous, or counting measures if the marginal distributions are discrete. For given
‚Ñô and ‚Ñö, the two regularized optimal transport distances above are equivalent up to a constant.
‚ô£

(cid:16) dùõæ (ùë•,ùë¶)
dùë• dùë¶

(cid:17)

In this paper, we study the Sinkhorn DRO model. Given a loss function ùëì , a nominal distribution (cid:98)‚Ñô
and the Sinkhorn radius ùúå, the primal form of the worst-case expectation problem of Sinkhorn DRO is
given by

ùëâ := sup

ùîºùëß‚àº‚Ñô [ùëì (ùëß)],

‚Ñô‚ààùîπùúå,ùúñ ((cid:98)‚Ñô)

(Sinkhorn DRO)

where ùîπùúå,ùúñ ((cid:98)‚Ñô) = (cid:8)‚Ñô : Wùúñ ((cid:98)‚Ñô, ‚Ñô) ‚â§ ùúå(cid:9),
where ùîπùúå,ùúñ ((cid:98)‚Ñô) is the Sinkhorn ball of the radius ùúå centered at the nominal distribution (cid:98)‚Ñô. Due to the
convex entropic regularizer [34] in Wùúñ ((cid:98)‚Ñô, ‚Ñô), the Sinkhorn distance Wùúñ ((cid:98)‚Ñô, ‚Ñô) is convex in ‚Ñô and the
Sinkhorn ball is a convex set. Therefore, the problem (Sinkhorn DRO) is an (inÔ¨Ånite-dimensional)
convex program.

Remark 2 (Choice of Reference Measures). We discuss below the choices of the two references
measures ùúá and ùúà in DeÔ¨Ånition 1.

For the reference measure ùúá, observe from the deÔ¨Ånition of relative entropy and the law of probability,

we can see that the regularization term in Wùúñ ((cid:98)‚Ñô, ‚Ñô) can be written as

5

ùêª (ùõæ | ùúá ‚äó ùúà) =

‚à´

log

‚à´

=

log

(cid:32)

(cid:32)

(cid:33)

(cid:33)

dùõæ (ùë•,ùë¶)
d(cid:98)‚Ñô(ùë•) dùúà (ùë¶)
dùõæ (ùë•,ùë¶)
d(cid:98)‚Ñô(ùë•) dùúà (ùë¶)

log

(cid:32)

(cid:33)

(cid:98)‚Ñô(ùë•)
dùúá (ùë•)

dùõæ (ùë•,ùë¶)

dùõæ (ùë•,ùë¶) +

‚à´

log

(cid:32)

(cid:33)

(cid:98)‚Ñô(ùë•)
dùúá (ùë•)

d(cid:98)‚Ñô(ùë•).

Therefore, any choice of the reference measure ùúá satisfying (cid:98)‚Ñô (cid:28) ùúá is equivalent up to a constant. For
simplicity, in the sequel we will take ùúá = (cid:98)‚Ñô.

For the reference measure ùúà, observe that the worst-case solution ‚Ñô in (Sinkhorn DRO) should satisfy
that ‚Ñô (cid:28) ùúà since otherwise the entropic regularization in DeÔ¨Ånition 1 is undeÔ¨Åned. As a consequence,
we can choose ùúà such that the underlying true distribution is absolutely continuous with respect to it.
Typical choices include the Lebesgue measure or Gaussian measure for continuous random variables,
and counting measure for discrete measures. See [79, Section 3.6] for the construction of a general
reference measure.
‚ô£

In the following sections, we Ô¨Årst derive the tractable formulation of the Sinkhorn DRO model and
then develop an eÔ¨Écient Ô¨Årst-order method to solve it. Finally, we examine its performance by several
numerical examples.

3. Strong Duality Reformulation

Problem (Sinkhorn DRO) is an inÔ¨Ånite-dimensional optimization problem over probability distributions.
To obtain a more tractable form, in this section, we derive a strong duality result for (Sinkhorn DRO).

Our main goal is to derive the strong dual problem

ùëâD := inf
ùúÜ ‚â•0

(cid:26)

ùúÜùúå + ùúÜùúñ

‚à´

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

(cid:27)

,

d(cid:98)‚Ñô(ùë•)

(Dual)

where the dual decision variable ùúÜ corresponds to the Sinkhorn distance constraint in (Sinkhorn DRO),
and by convention we deÔ¨Åne the dual objective evaluated at ùúÜ = 0 as the limit of the objective values
with ùúÜ ‚Üì 0, which equals the essential supremum of the objective function with respect to the measure
ùúà; and we deÔ¨Åne the constant

ùúå := ùúå + ùúñ

‚à´

(cid:18)‚à´

log

ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß)

(cid:19)

d(cid:98)‚Ñô(ùë•),

and the kernel probability distribution

d‚Ñöùë•,ùúñ (ùëß) :=

ùëí‚àíùëê (ùë•,ùëß)/ùúñ
‚à´ ùëí‚àíùëê (ùë•,ùë¢)/ùúñ dùúà (ùë¢)

dùúà (ùëß).

(1)

(2)

To make the above primal (Sinkhorn DRO) and dual (Dual) problems well-deÔ¨Åned, we introduce

the following assumptions on the cost function ùëê, the reference measure ùúà, and the loss function ùëì .

Assumption 1.

(I) ùúà {ùëß : 0 ‚â§ ùëê (ùë•, ùëß) < ‚àû} = 1 for (cid:98)‚Ñô-almost every ùë•;

(II) ‚à´ ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) < ‚àû for (cid:98)‚Ñô-almost every ùë•;
(III) Z is a measurable space, and the function ùëì : Z ‚Üí ‚Ñù ‚à™ {‚àû} is measurable.

6

Assumption 1(I) ensures that the Sinkhorn distance is well-deÔ¨Åned. If Assumption 1(II) is not satisÔ¨Åed,
then the Sinkhorn ball ùîπùúå,ùúñ ((cid:98)‚Ñô) = P (Z) and the problem (Sinkhorn DRO) has a simple optimal value
ùëâ = supùëß ‚ààZ
ùëì (ùëß). Assumption 1(III) ensures the expected loss ùîºùëß‚àº‚Ñô [ùëì (ùëß)] to be well-deÔ¨Åned and lower
bounded for any distribution ‚Ñô. In Appendix A, we present suÔ¨Écient conditions for Assumption 1 that
are easy to verify.

To distinguish the cases ùëâùê∑ < ‚àû and ùëâùê∑ = ‚àû, we introduce the following light-tail condition on ùëì .

Condition 1. There exists ùúÜ > 0 such that ùîº‚Ñöùë•,ùúñ [ùëí ùëì (ùëß)/(ùúÜùúñ) ] < ‚àû for (cid:98)‚Ñô-almost every ùë•.

Our main result in this section is as follows.

Theorem 1 (Strong Duality). Let (cid:98)‚Ñô ‚àà P (Z). Assume Assumption 1 is in force. Then the following holds:

(I) The primal problem (Sinkhorn DRO) is feasible if and only if ùúå ‚â• 0;
(II) Whenever ùúå ‚â• 0, it holds that ùëâ = ùëâùê∑ .
(III) If, in addition, Condition 1 holds, then ùëâ = ùëâD < ‚àû; otherwise ùëâ = ùëâD = ‚àû.

We remark that if ùúå < 0, by convention, ùëâ = ‚àí‚àû and ùëâùê∑ = ‚àí‚àû as well by Lemma 2 in Section 3.3 below.
Therefore, we have ùëâ = ùëâùê∑ as long as Assumption 1 holds.

3.1. Discussions

Before we present the proof of Theorem 1, we would like to make several remarks.

Remark 3 (Connection with Wasserstein DRO). As the regularization parameter ùúñ ‚Üí 0, the dual
objective of the Sinkhorn DRO converges to the dual formulation of the Wasserstein DRO problem [46,
Theorem 1]

‚à´

ùúÜùúå +

(cid:8)ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß)(cid:9) d(cid:98)‚Ñô(ùë•).

sup
ùëß

The proof is given in Appendix EC.3, which essentially follows from the fact that the log-sum-exp
function is a smooth approximation of the supremum. There are several advantages of Sinkhorn DRO.

(I) As we will demonstrate in Section 4, Sinkhorn DRO is tractable for a large class of loss functions.
For the empirical nominal distribution, the worst-case loss can be evaluated eÔ¨Éciently for any
measurable loss function ùëì . In contrast, the main computational diÔ¨Éculty in Wasserstein DRO
is to solve the maximization problem inside the integration above. In fact, 1-Wasserstein DRO is
shown to be tractable only when the loss function can be expressed as a pointwise maximum
of Ô¨Ånitely many concave functions [67, Theorem 4.2], and 2-Wasserstein DRO is shown to be
tractable only when the loss function is smooth and the radius of the ambiguity set is suÔ¨Éciently
small [20, Theorem 3].

(II) The strong duality of Sinkhorn DRO holds in an even more general setting. Essentially, the only
requirements on the space Z and the nominal distribution (cid:98)‚Ñô are measurability. In contrast, the
strong duality for Wasserstein DRO ([46, Theorem 1], [17, Theorem 1]) requires the nominal
distribution (cid:98)‚Ñô to be a Borel probability measure and the set Z to be a Polish space.

We remark that Sinkorn DRO and Wasserstein DRO result in diÔ¨Äerent conditions for Ô¨Ånite worst-case
values. From Condition 1 we see that the Sinkhorn DRO is Ô¨Ånite if and only if under a light-tail condition
on ùëì , while the Wasserstein DRO [46, Theorem 1] is Ô¨Ånite iÔ¨Ä and only if the loss function satisÔ¨Åes a
growth condition ùëì (ùëß) ‚â§ ùêøùëê (ùëß, ùëß0) + ùëÄ, ‚àÄùëß ‚àà Z for some constants ùêø, ùëÄ > 0 and some ùëß0 ‚àà Z.
‚ô£
Remark 4 (Connection with KL-DRO). Using Jensen‚Äôs inequality, we can see that the dual objective
function of the Sinkhorn DRO model can be upper bounded as

ùúÜùúå + ùúÜùúñ log

(cid:18)‚à´

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105)
(cid:104)

ùîº‚Ñöùë•,ùúñ

(cid:19)

,

d(cid:98)‚Ñô(ùë•)

which corresponds to the dual objective function [57] for the following KL-divergence DRO

(cid:8)ùîºùëß‚àº‚Ñô [ùëì (ùëß)] : ùê∑KL(‚Ñô(cid:107)‚Ñô0) ‚â§ ùúå/ùúñ(cid:9) ,

sup
‚Ñô

7

where ‚Ñô0 satisÔ¨Åes d‚Ñô0(ùëß) = ‚à´
estimation constructed from (cid:98)‚Ñô. Particularly, when (cid:98)‚Ñô = 1
ùëñ=1 ùõø ÀÜùë•ùëñ
kernel density estimator with Gaussian kernel and bandwidth ùúñ:

d‚Ñöùë•,ùúñ (ùëß) d(cid:98)‚Ñô(ùë•), which can be viewed as a non-parametric kernel density
2, ‚Ñô0 is

, Z = ‚Ñùùëë and ùëê (ùë•,ùë¶) = (cid:107)ùë• ‚àí ùë¶ (cid:107)2

(cid:80)ùëõ

ùëõ

ùë•

d‚Ñô0(ùëß)
dùëß

=

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùêæùúñ (ùëß ‚àí ùë•ùëñ) ,

ùëß ‚àà ‚Ñùùëë,

where ùêæùúñ (ùë•) ‚àù exp(‚àí(cid:107)ùë• (cid:107)2
2/ùúñ) represents the Gaussian kernel. By decomposing ùõæ (ùë•, ùëß) = (cid:98)‚Ñô(ùë•) ‚äó ùõæùë• (ùëß),
similar to the proof in Section 3.2 below, (Sinkhorn DRO) can be reformulated as a generalized
KL-divergence DRO problem:

ùëâ =

sup
ùõæùë• ‚ààP (Z),‚àÄùë• ‚ààZ

(cid:26)‚à´

ùîºùëß‚àºùõæùë• [ùëì (ùëß)] d(cid:98)‚Ñô(ùë•) :

‚à´

ùê∑KL(ùõæùë• (cid:107)‚Ñöùë• ) d(cid:98)‚Ñô(ùë•) ‚â§ ùúå/ùúñ

(cid:27)

.

(3)

Using Divergence inequality [34, Theorem 2.6.3], we can see the Sinkhorn DRO with ùúå = 0 is reduced
to the following SAA model based on the distribution ‚Ñô0:

ùëâ = ùîº‚Ñô0 [ùëì (ùëß)] =

‚à´

ùîº‚Ñöùë•,ùúñ [ùëì (ùëß)] d(cid:98)‚Ñô(ùë•).

(4)

In non-parameteric statistics, the optimal bandwidth to minimize the mean-squared-error between
the estimated distribution and the underlying true one is at rate ùúñ = ùëÇ (ùëõ‚àí1/(ùëë+4) ) [54, Theorem 4.2.1].
However, such an optimal choice for the kernel density estimator may not be the optimal choice for
optimizing the out-of-sample performance of the Sinkhorn DRO. In our numerical experiments in
Section 5, we select ùúñ based on cross-validation.
‚ô£

Let us illustrate our result for a linear loss function ùëì , which turns out to be equivalent to a simple

optimization problem.
Example 1. Suppose that ùëì (ùëß) = ùëéTùëß, Z = ‚Ñùùëë and ùúà is the corresponding Lebesgue measure, and the
cost function is the Mahalanobis distance, i.e., ùëê (ùë•,ùë¶) = 1
2 (ùë• ‚àíùë¶)TŒ©(ùë• ‚àíùë¶), where Œ© is a positive deÔ¨Ånite
matrix. In this case, we have the reference measure

As a consequence, the dual problem can be written as

‚Ñöùë•,ùúñ ‚àº N (ùë•, ùúñ Œ©‚àí1).

ùëâD = inf
ùúÜ>0

(cid:26)

ùúÜùúå + ùúÜùúñ

‚à´

Œõùë• (ùúÜ) d(cid:98)‚Ñô(ùë•)

(cid:27)

,

Œõùë• (ùúÜ) = log

(cid:16)

ùîº(ùë•,ùúñ Œ©‚àí1)

ùëíùëé(cid:62)ùëß/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

ùëéTùë•
ùúÜùúñ

ùëéTŒ£ùëé
2ùúÜ2ùúñ2

.

+

=

where

Therefore

ùëâD = ùëéTùîº

(cid:98)‚Ñô [ùë•] + ‚àöÔ∏Å2ùúå‚àöÔ∏ÅùëéTŒ©‚àí1ùëé := ùîº

(cid:98)‚Ñô [ùëéTùë•] + ‚àöÔ∏Å2ùúå ¬∑ (cid:107)ùëé(cid:107)Œ©‚àí1 .

This indicates that the Sinkhorn DRO is equivalent to an empirical risk minimization with norm
regularization, and can be solved using eÔ¨Éciently using algorithms for the second-order cone
program.
‚ô£

In the following, we Ô¨Årst show that ùëâ = ùëâD when (cid:98)‚Ñô is an empirical distribution supported on ùëõ points,
as it is relatively straightforward to prove. Then we show that the same results hold when (cid:98)‚Ñô is a
general distribution, as it provides more insights on the worst-case distribution.

8

3.2. Proof for an Empirical Nominal distribution
Given data points ÀÜùë•ùëñ for ùëñ = 1, . . . , ùëõ, denote 1
ùëñ=1 ùõø ÀÜùë•ùëñ
as the corresponding empirical distribution. In
ùëõ
this subsection, we discuss the dual reformulation provided that the nominal distribution (cid:98)‚Ñô is taken in
this form. Although our strong duality result holds for arbitrary nominal distribution, this is still an
interesting case, as the proof is relatively simple and (cid:98)‚Ñô is often chosen as the empirical distribution in
practice.

(cid:80)ùëõ

The key to the proof is to write the primal problem in a Lagrangian form and then apply the minimax
inequality to obtain a weak dual. Observe that the primal can be reformulated as a generalized
KL-divergence DRO problem. Hence, by leveraging the strong duality result for the existing DRO
model [57], the minimax inequality does not incur any duality gap.
(cid:80)ùëõ

Proof of Theorem 1 when (cid:98)‚Ñô = 1

ùëñ=1 ùõø ÀÜùë•ùëñ . Based on DeÔ¨Ånition 1, we reformulate ùëâ as
(cid:41)
(cid:40)

(cid:33)(cid:35)

(cid:32)

ùëõ

ùëâ =

sup
ùõæ ‚ààP (Z√óZ):Proj1#ùõæ =(cid:98)‚Ñô

ùîº‚Ñô [ùëì (ùëß)] : ùîºùõæ

(cid:34)
ùëê (ùë•, ùëß) + ùúñ log

‚â§ ùúå

.

dùõæ (ùë•, ùëß)
d(cid:98)‚Ñô(ùë•) dùúà (ùëß)
(cid:80)ùëõ

By the disintegration theorem [23] we represent the joint distribution ùõæ = 1
ùëñ=1 ùõø ÀÜùë•ùëñ ‚äó ùõæùëñ, where ùõæùëñ is the
ùëõ
conditional distribution of ùõæ given the Ô¨Årst marginal of ùõæ equals ÀÜùë•ùëñ. Thereby the constraint is equivalent
to

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

(cid:20)
ùëê ( ÀÜùë•ùëñ, ùëß) + ùúñ log

ùîºùõæùëñ

(cid:19)(cid:21)

(cid:18) dùõæùëñ (ùëß)
dùúà (ùëß)

‚â§ ùúå,

ùõæùëñ ‚àà P (Z), ùëñ ‚àà [ùëõ].

We remark that any feasible solution ùõæ satisÔ¨Åes that ùõæ (cid:28) (cid:98)‚Ñô ‚äó ùúà and hence ùõæùëñ (cid:28) ùúà. Consequently the
is well-deÔ¨Åned. For notational simplicity, we write ‚Ñöùëñ for ‚ÑöÀÜùë•ùëñ,ùúñ . Based on the change-
term log
(cid:17)
and the expression of ‚Ñöùëñ, the constraint

= log

+ log

(cid:17)

(cid:17)

(cid:17)

(cid:16) dùõæùëñ (ùëß)
dùúà (ùëß)
of-measure identity log
can be reformulated as

(cid:16) dùõæùëñ (ùëß)
d‚Ñöùëñ (ùëß)

(cid:16) d‚Ñöùëñ (ùëß)
dùúà (ùëß)

(cid:16) dùõæùëñ (ùëß)
dùúà (ùëß)

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùîºùõæùëñ

(cid:34)
ùëê ( ÀÜùë•ùëñ, ùëß) + ùúñ log

(cid:32)

(cid:33)

ùëí‚àíùëê (ùë•,ùëß)/ùúñ
‚à´ ùëí‚àíùëê (ùë•,ùë¢)/ùúñ dùúà (ùë¢)

+ ùúñ log

(cid:19)(cid:35)

(cid:18) dùõæùëñ (ùëß)
d‚Ñöùëñ (ùëß)

‚â§ ùúå,

ùõæùëñ ‚àà P (Z), ùëñ ‚àà [ùëõ].

Combining the Ô¨Årst two terms within the expectation term and substituting the expression of ùúå, it is
equivalent to

ùúñ
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùîºùõæùëñ

(cid:20)

log

(cid:19)(cid:21)

(cid:18) dùõæùëñ (ùëß)
d‚Ñöùëñ (ùëß)

‚â§ ùúå,

ùõæùëñ ‚àà P (Z), ùëñ ‚àà [ùëõ].

Similarly, the objective function of (Sinkhorn DRO) can be written as 1
ùëñ=1 ùîºùõæùëñ [ùëì (ùëß)]. Consequently,
ùëõ
the primal problem (Sinkhorn DRO) can be reformulated as a generalized KL-divergence DRO problem

(cid:80)ùëõ

ùëâ =

sup
ùõæùëñ ‚ààP (Z),ùëñ ‚àà [ùëõ]

(cid:40) 1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùîºùõæùëñ [ùëì (ùëß)] :

(cid:41)

ùê∑KL(ùõæùëñ (cid:107)‚Ñöùëñ) ‚â§ ùúå

.

ùúñ
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

Then Theorem 1(I) holds based on the non-negativity of KL-divergence.

Introducing the Lagrange multiplier ùúÜ associated with the constraint, we reformulate (Sinkhorn DRO)

as

ùëâ =

sup
ùõæùëñ ‚ààP (Z),ùëñ ‚àà [ùëõ]

inf
ùúÜ ‚â•0

(cid:40)

ùúÜùúå +

(cid:20)

ùîºùõæùëñ

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùëì (ùëß) ‚àí ùúÜùúñ log

(cid:18) dùõæùëñ (ùëß)
d‚Ñöùëñ (ùëß)

(cid:19)(cid:21) (cid:41)

.

Interchanging the supremum and inÔ¨Åmum operators, we have that

(cid:40)

ùëâ ‚â§ inf
ùúÜ ‚â•0

ùúÜùúå +

sup
ùõæùëñ ‚ààP (Z),ùëñ ‚àà [ùëõ]

(cid:40) 1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

(cid:20)

ùîºùõæùëñ

ùëì (ùëß) ‚àí ùúÜùúñ log

(cid:18) dùõæùëñ (ùëß)
d‚Ñöùëñ (ùëß)

(cid:19)(cid:21) (cid:41)(cid:41)

.

9

Since the optimization over ùõæùëñ, ùëñ ‚àà [ùëõ] is separable, by deÔ¨Åning for each ùëñ

ùë£ùëñ (ùúÜ) := sup

ùõæùëñ ‚ààP (Z)

(cid:26)

(cid:20)

ùîºùõæùëñ

ùëì (ùëß) ‚àí ùúÜùúñ log

(cid:18) dùõæùëñ (ùëß)
d‚Ñöùëñ (ùëß)

(cid:19)(cid:21) (cid:27)

,

it holds that

(cid:40)

ùëâ ‚â§ inf
ùúÜ ‚â•0

ùúÜùúå +

(cid:41)

ùë£ùëñ (ùúÜ)

.

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

When Condition 1 holds, leveraging a well-known results on entropy regularized linear optimization
(Lemma EC.1), we can see that

ùë£ùëñ (ùúÜ) = ùúÜùúñ log

(cid:16)

ùîº‚Ñöùëñ

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

< ‚àû,

hence we obtain the weak duality ùëâ ‚â§ ùëâD < ‚àû. Otherwise, for any ùúÜ > 0, there exists an index ùëñ such
that
ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105)
(cid:104)

= ‚àû.

ùîº‚Ñöùëñ

We also obtain that ùëâ ‚â§ ùëâD = ‚àû, and the weak duality still holds.

Next we prove the strong duality. Recall that the primal problem is a generalized KL-divergence DRO
problem. By leveraging the strong duality result from [57], the minimax inequality above does not
incur any duality gap when ùúå > 0. When ùúå = 0, since ùê∑KL(ùõæùëñ (cid:107)‚Ñöùëñ) = 0 if and only if ùõæùëñ = ‚Ñöùëñ, one can see
that

ùëâ =

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùîº‚Ñöùëñ [ùëì (ùëß)].

On the other hand, denote by ‚Ñé(ùúÜ) the objective function for the dual problem. Then we have the
inequality

ùëâD ‚â§ lim
ùúÜ‚Üí‚àû

‚Ñé(ùúÜ) =

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùîº‚Ñöùëñ [ùëì (ùëß)] = ùëâ.

This, together with the weak duality result, completes the proof for Theorem 1(II). Theorem 1(III) also
(cid:3)
follows based on the discussion of the Ô¨Åniteness of ùëâD.

When the sample space Z is Ô¨Ånite, the following result presents a conic programming reformulation.
Corollary 1 (Conic Reformulation for Finite Sample Space). Suppose that the sample space con-
tains ùêø elements, i.e., Z = {ùëß‚Ñì }ùêø
‚Ñì=1. If Condition 1 holds and ùúå ‚â• 0, the dual problem (Dual) can be
formulated as the following conic optimization:

ùëâD = min
ùúÜ ‚â•0,ùë† ‚àà‚Ñùùëõ,
ùëé ‚àà‚Ñùùëõ√óùêø

ùúÜùúå +

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùë†ùëñ

s.t. ùúÜùúñ ‚â•

ùêø
‚àëÔ∏Å

‚Ñì=1

ùëûùëñ,‚Ñìùëéùëñ,‚Ñì, ùëñ ‚àà [ùëõ],

(ùúÜùúñ, ùëéùëñ,‚Ñì, ùëì (ùëß‚Ñì ) ‚àí ùë†ùëñ) ‚àà Kexp, ùëñ ‚àà [ùëõ], ‚Ñì ‚àà [ùêø].

(5)

where ùëûùëñ,‚Ñì := Prùëß‚àº‚Ñö ÀÜùë•ùëñ ,ùúñ {ùëß = ùëß‚Ñì }, with the distribution ‚ÑöÀÜùë•ùëñ,ùúñ deÔ¨Åned in (2), and Kexp denotes the exponential
cone Kexp = {(ùúà, ùúÜ, ùõø) ‚àà ‚Ñù+ √ó ‚Ñù+ √ó ‚Ñù : exp(ùõø/ùúà) ‚â§ ùúÜ/ùúà }.
Problem (5) is a convex program that minimizes a linear function with respect to linear and conic
constraints, which can be solved using interior point algorithms [71, 95]. We will develop a customized
Ô¨Årst-order optimization algorithm in Section 4 that is able to solve a more general problem (without a
Ô¨Ånite sample space).

10

3.3. Proof for a General Nominal Distribution
In this subsection, we outline the proof of Theorem 1 when (cid:98)‚Ñô is an arbitrary nominal distribution and
discuss the worst-case distribution.

The feasibility result in Theorem 1(I) can be easily shown using the reformulation (3). To show
ùëâ = ùëâD, it is easy to show the weak duality result following a similar argument as in Section 3.2, by
replacing the Ô¨Ånite-sum with the integration with respect to (cid:98)‚Ñô.

When Condition 1 holds, we prove the strong duality by constructing the worst-case distribution. We
Ô¨Årst show the existence of the dual minimizer (Lemma 2), and then build the corresponding Ô¨Årst-order
optimality condition (Lemma 3 and Lemma 4). Those results help us to construct a primal optimal
solution for (Sinkhorn DRO) that shares the same optimal value as ùëâD, which completes the Ô¨Årst part
of Theorem 1(III). When Condition 1 does not hold, we construct a sequence of DRO problems with
Ô¨Ånite optimal values converging into ùëâ and consequently ùëâ = ùëâD = ‚àû, which completes the second part
of Theorem 1(III). Putting these two parts together imply Theorem 1(II).
Lemma 1 (Weak Duality). Assume Assumption 1 holds. Then ùëâ ‚â§ ùëâD.

Below we provide the proof of the Ô¨Årst part of Theorem 1(III) for the case ùúå > 0 under Condition 1,
defer proofs of other degenerate cases to Appendix EC.4. To prove the strong duality, we will construct
a feasible solution of (Sinkhorn DRO) whose loss coincides with ùëâD. To this end, we Ô¨Årst show that the
dual minimizer exists.
Lemma 2 (Existence of Dual Minimizer). Suppose ùúå > 0 and Condition 1 is satisÔ¨Åed, then the dual
minimizer ùúÜ‚àó exists, which either equals to 0 or satisÔ¨Åes Condition 1.

We separate two cases: ùúÜ‚àó > 0 and ùúÜ‚àó = 0, corresponding to whether the Sinkhorn distance constraint

in (Sinkhorn DRO) is binding or not.

Lemma 3 below presents a necessary and suÔ¨Écient condition for the dual minimizer ùúÜ‚àó = 0,
corresponding to the case where the Sinkhorn distance constraint in (Sinkhorn DRO) is not binding.
Lemma 3 (Necessary and SuÔ¨Écient Condition for ùúÜ‚àó = 0). Suppose ùúå > 0 and Condition 1 is satisÔ¨Åed,
then the dual minimizer ùúÜ‚àó = 0 if and only if all the following conditions hold:

ùëì (cid:44) inf{ùë° : ùúà {ùëì (ùëß) > ùë° } = 0} < ‚àû.

(I) ess sup
(II) ùúå (cid:48) = ùúå + ùúñ ‚à´ log (cid:0)ùîº‚Ñöùë•,ùúñ [1ùê¥](cid:1) d(cid:98)‚Ñô(ùë•) ‚â• 0, where ùê¥ := {ùëß : ùëì (ùëß) = ess sup

ùúà

ùëì }.

ùúà

Recall that we have the convention that the dual objective evaluated at ùúÜ = 0 equals ess sup
Condition (I) ensures that the dual objective function evaluated at the minimizer is Ô¨Ånite. When the
minimizer ùúÜ‚àó = 0, the Sinkhorn ball should be large enough to contain at least one distribution with
objective value ess sup

ùëì , and the condition (II) characterizes the lower bound of ùúå.

ùëì . Thus

ùúà

Lemma 4 below considers the optimality condition when the dual minimizer ùúÜ‚àó > 0, obtained by

ùúà

simply setting the derivative of the dual objective function to be zero.
Lemma 4 (First-order Optimality Condition when ùúÜ‚àó > 0). Suppose ùúå > 0 and Condition 1 is satisÔ¨Åed,
and assume further that the dual minimizer ùúÜ‚àó > 0, then ùúÜ‚àó satisÔ¨Åes
‚à´ ùîº‚Ñöùë•,ùúñ

‚à´

(cid:16)

(cid:20)

(cid:21)

ùúÜ‚àó

ùúå + ùúñ

log

ùîº‚Ñöùë•,ùúñ

ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•)

=

d(cid:98)‚Ñô(ùë•).

(6)

(cid:2)ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) ùëì (ùëß)(cid:3)
(cid:2)ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) (cid:3)

ùîº‚Ñöùë•,ùúñ

Now we are ready to prove Theorem 1.
Proof of Theorem 1(III) under Condition 1 with ùúå > 0. The proof is separated for two cases: ùúÜ‚àó > 0

or ùúÜ‚àó = 0. For each case we prove by constructing a primal (approximate) optimal solution.

When ùúÜ‚àó > 0, we take a probability measure ùõæ‚àó such that

dùõæ‚àó(ùë•, ùëß) =

‚à´ exp

exp

(cid:16) ùúô (ùúÜ‚àó;ùë•,ùëß)
ùúÜ‚àóùúñ
(cid:16) ùúô (ùúÜ‚àó;ùë•,ùë¢)
ùúÜ‚àóùúñ

(cid:17)

(cid:17)

dùúà (ùë¢)

dùúà (ùëß) d(cid:98)‚Ñô(ùë•), where ùúô (ùúÜ; ùë•, ùëß) = ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß).

Also deÔ¨Åne the primal (approximate) optimal distribution

‚Ñô‚àó := Proj2#ùõæ‚àó.

Recall the expression of the Sinkhorn distance in DeÔ¨Ånition 1, one can verify that

11

Wùúñ ((cid:98)‚Ñô, ‚Ñô‚àó)
(cid:40)

=

ùîºùõæ

inf
ùõæ ‚ààŒì ((cid:98)‚Ñô,‚Ñô‚àó)
(cid:34)
ùëê (ùë•, ùëß) + ùúñ log

‚â§ ùîºùõæ‚àó

= ùîºùõæ‚àó

1
ùúÜ‚àó

=

Ô£Æ
Ô£Ø
ùëê (ùë•, ùëß) + ùúñ log (cid:169)
Ô£Ø
(cid:173)
Ô£Ø
(cid:173)
Ô£Ø
Ô£Ø
(cid:171)
Ô£∞
‚à¨ ùëì (ùëß) exp
Ô£±Ô£¥Ô£¥Ô£≤
‚à´ exp
Ô£¥Ô£¥
Ô£≥

(cid:34)
ùëê (ùë•, ùëß) + ùúñ log

(cid:32)

(cid:33)(cid:35) (cid:41)

dùõæ (ùë•, ùëß)
d(cid:98)‚Ñô(ùë•) dùúà (ùëß)

(cid:33)(cid:35)

(cid:32) dùõæ‚àó(ùë•, ùëß)
d(cid:98)‚Ñô(ùë•) dùúà (ùëß)
exp

‚à´ exp

(cid:16) ùúô (ùúÜ‚àó;ùë•,ùëß)
ùúÜ‚àóùúñ
(cid:16) ùúô (ùúÜ‚àó;ùë•,ùë¢)
ùúÜ‚àóùúñ
(cid:17)

(cid:17)

(cid:17)

dùúà (ùë¢)

Ô£π
Ô£∫
(cid:170)
Ô£∫
(cid:174)
Ô£∫
(cid:174)
Ô£∫
Ô£∫
(cid:172)
Ô£ª

(cid:16) ùúô (ùúÜ‚àó;ùë•,ùëß)
ùúÜ‚àóùúñ
(cid:17)

(cid:16) ùúô (ùúÜ‚àó;ùë•,ùëß)
ùúÜ‚àóùúñ

dùúà (ùëß)

dùúà (ùëß) d(cid:98)‚Ñô(ùë•) ‚àí ùúÜ‚àóùúñ

‚à´

(cid:18)‚à´

log

exp

(cid:18)ùúô (ùúÜ‚àó; ùë•,ùë¢)
ùúÜ‚àóùúñ

(cid:19)

(cid:19)

dùúà (ùë¢)

d(cid:98)‚Ñô(ùë•)

,

Ô£ºÔ£¥Ô£¥Ô£Ω
Ô£¥Ô£¥
Ô£æ

where the second relation is because ùõæ‚àó is a feasible solution in Œì((cid:98)‚Ñô, ‚Ñô‚àó), the third and the fourth
relation is by substituting the expression of ùõæ‚àó. Since ùúå > 0 and the dual minimizer ùúÜ‚àó > 0, the optimality
condition in (6) holds, which implies that Wùúñ ((cid:98)‚Ñô, ‚Ñô‚àó) ‚â§ ùúå, i.e., the distribution ‚Ñô‚àó is primal feasible for
the problem (Sinkhorn DRO). Moreover, we can see that the primal optimal value is lower bounded by
the dual optimal value:

ùëì (ùëß) dùõæ‚àó(ùë•, ùëß)
(cid:33)

dùúà (ùëß) d(cid:98)‚Ñô(ùë•)

ùëâ ‚â• ùîº‚Ñô‚àó [ùëì (ùëß)] =

‚à´

‚à¨

‚à¨

=

=

ùëì (ùëß)

(cid:32) dùõæ‚àó(ùë•, ùëß)
d(cid:98)‚Ñô(ùë•) dùúà (ùëß)

(cid:17)

exp

(cid:16) ùúô (ùúÜ‚àó;ùë•,ùëß)
ùúÜ‚àóùúñ
(cid:16) ùúô (ùúÜ‚àó;ùë•,ùë¢)
ùúÜ‚àóùúñ

(cid:17)

ùëì (ùëß)

‚à´ exp

‚à´

(cid:20)

ùúå + ùúñ

=ùúÜ‚àó

(cid:18)‚à´

log

exp

=ùëâD,

dùúà (ùëß) d(cid:98)‚Ñô(ùë•)

dùúà (ùë¢)
(cid:20)ùúô (ùúÜ‚àó; ùë•, ùëß)
ùúÜ‚àóùúñ

(cid:21)

(cid:19)

dùúà (ùëß)

(cid:21)

d(cid:98)‚Ñô(ùë•)

where the third equality is based on the optimality condition in Lemma 4. This, together with the
weak duality result, completes the proof for ùúÜ‚àó > 0.

When ùúÜ‚àó = 0, the optimality condition in Lemma 3 holds. We construct the primal (approximate)

solution ‚Ñô‚àó = Proj2#ùõæ‚àó, where ùõæ‚àó satisÔ¨Åes

dùõæ‚àó(ùë•, ùëß) = dùõæ ùë•

‚àó (ùëß) d(cid:98)‚Ñô(ùë•), where dùõæùë•

‚àó (ùë¶) =

(cid:40)0,

ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß)
‚à´ ùëí‚àíùëê (ùë•,ùë¢)/ùúñ 1ùê¥ dùúà (ùë¢)

,

if ùëß ‚àâ ùê¥,
if ùëß ‚àà ùê¥.

We can verify easily that the primal solution is feasible based on the optimality condition ùúå (cid:48) ‚â• 0 in
Lemma 3. Moreover, we can check that the primal optimal value is lower bounded by the dual optimal
value:

‚à´

ùëâ ‚â•

ùëì (ùëß) dùõæ‚àó(ùë•, ùëß) =

‚à¨

ùëì (ùëß) dùõæùë•

‚àó (ùëß) d(cid:98)‚Ñô(ùë•) =

‚à¨

ess sup
ùúà

ùëì dùõæ ùë•

‚àó (ùëß) d(cid:98)‚Ñô(ùë•) = ess sup

ùëì = ùëâD,

ùúà

12

where the second equality is because that ùëß ‚àà ùê¥ so that ùëì (ùëß) = ess sup
duality result, completes the proof for ùúÜ‚àó = 0.

ùúà

ùëì . This, together with the weak

(cid:3)

Remark 5 (Worst-case Distribution). From the proof presented above we observe that when
ùúÜ‚àó > 0, the worse-case distribution for (Sinkhorn DRO) can be expressed as

d‚Ñô‚àó(ùëß) =

‚à´

ùë•

(cid:32)ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) d‚Ñöùë•,ùúñ (ùëß)
(cid:2)ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) (cid:3)

ùîº‚Ñöùë•,ùúñ

(cid:33)

d(cid:98)‚Ñô(ùë•),

from which we can see that the worst-case distribution shares the same support as the measure ùúà.
Particularly, when (cid:98)‚Ñô is the empirical distribution 1
(cid:80)ùëõ
and ùúà is any continuous distribution on ‚Ñùùëë ,
ùëõ
the worst-case distribution ‚Ñô‚àó is supported on the entire ‚Ñùùëë . In contrast, the worst-case distribution
for Wasserstein DRO is supported on at most ùëõ + 1 points [46]. This is another diÔ¨Äerence, or advantage
possibly, of Sinkhorn DRO compared with Wasserstein DRO. Indeed, for many practical problems,
the underlying distribution can be modeled as a continuous distribution. The worst-case distribution
for Wasserstein DRO is often Ô¨Ånitely supported, raising the concern of whether it hedges against the
wrong family of distributions and thus results in suboptimal solutions. The numerical results in Section
5 demonstrate some empirical advantages of Sinkhorn DRO.
‚ô£

ùëñ=1 ùõø ÀÜùë•ùëñ

4. EÔ¨Écient First-order Algorithm for Data-driven Sinkhorn Robust Learning
In this section, we consider the data-driven Sinkhorn robust learning problem, where we seek an
optimal decision to minimize the worst-case risk

inf
ùúÉ ‚ààŒò

sup
‚Ñô‚ààùîπùúå,ùúñ ((cid:98)‚Ñô)

ùîºùëß‚àº‚Ñô [ùëìùúÉ (ùëß)],

(7)

where the feasible set Œò contains all possible candidates of decision vector ùúÉ , and we take (cid:98)‚Ñô as the
empirical distribution corresponding to sample points ÀÜùë•ùëñ, ùëñ = 1, . . . , ùëõ. Based on our strong dual (Dual),
we reformulate (7) as

(cid:40)

inf
ùúÉ ‚ààŒò,ùúÜ ‚â•0

ùêπ (ùúÜ, ùúÉ ) := ùúÜùúå +

(cid:16)

ùúÜùúñ log

ùîº‚Ñö ÀÜùë•ùëñ ,ùúñ

ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

(cid:41)

,

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

(8)

where the constant ùúå and the distribution ‚ÑöÀÜùë•ùëñ,ùúñ are deÔ¨Åned in (1) and (2), respectively.

In Example 1 we have seen an instance of (8) where we can get a closed-form expression for the
above integration. In general, when a closed-form expression is not available, in the following we
present a Ô¨Årst-order algorithm to solve this problem, and discuss some alternatives in Section 4.1.
Observe that the objective function of (8) involves a nonlinear transformation of the expectation, thus
an unbiased gradient estimate could be challenging when ‚ÑöÀÜùë•ùëñ,ùúñ is a general probability distribution.
We propose to solve the following approximation

(cid:40)

inf
ùúÉ ‚ààŒò,ùúÜ ‚â•0

ÀÜùêπ (ùëö) (ùúÜ, ùúÉ ) := ùúÜùúå +

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùúÜùúñ log

(cid:16)

ùîº ÀÜ‚Ñöùëö

ùëñ

ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

(cid:41)

,

(9)

(cid:80)ùëö

ùëñ = 1
ùëö

ùëó=1 ùõøÀÜùëßùëñ,ùëó

denotes the empirical distribution constructed from {ÀÜùëßùëñ,ùëó }ùëö

where ÀÜ‚Ñöùëö
ùëó=1, independent
and identically distributed samples from ‚ÑöÀÜùë•ùëñ,ùúñ . In many cases, generating samples from ‚ÑöÀÜùë•ùëñ,ùúñ is easy.
For instance, choosing the cost function ùëê (¬∑, ¬∑) = 1
2 and Z = ‚Ñùùëë , then the distribution ‚ÑöÀÜùë•ùëñ,ùúñ
becomes a Gaussian distribution N ( ÀÜùë•ùëñ, ùúñùêºùëë ). Otherwise we can generate samples by, for example, the
acceptance-rejection method [5]. As a brief summary of our proposed method, we Ô¨Årst simulate a batch
of samples to approximate the original Sinkhorn DRO dual objective function, and then use projected

2 (cid:107) ¬∑ ‚àí ¬∑ (cid:107)2

Algorithm 1 A batch gradient descent with bisection search for solving (9)
Require: An interval [ùúÜùëô, ùúÜùë¢] so that 0 < ùúÜùëô ‚â§ ùúÜ‚àó ‚â§ ùúÜùë¢, where ùúÜ‚àó is an optimal dual variable for (9).

Terminating tolerance Œî > 0.

13

1: for ùë° = 0, 1, . . . ,ùëá ‚àí 1 do
ùúÜ0 ‚Üê (ùúÜùëô + ùúÜùë¢)/2.
2:
Solve the subproblem (10) at ùúÜ0 to get ùúÉ0.
3:
ÀÜùêπ (ùëö) (ùúÜ0, ùúÉ0).
Compute ùëé ‚àà ùúï
4:
ùúïùúÜ
Terminate the iteration if ùëé = 0 or ùúÜùë¢ ‚àí ùúÜùëô < Œî.
5:
Let ùë°ùë¢ ‚Üê ùë°0 when ùëé > 0. Let ùë°ùëô ‚Üê ùë°0 when ùëé < 0.
6:
7: end for

Return ùúÜ0 and ùúÉ0.

gradient descent with bisection search to solve the approximated problem. Hence our method is named
the batch gradient descent with bisection search.

When the function ùëìùúÉ (ùëß) is convex in ùúÉ , problem (9) is a Ô¨Ånite convex programming because the
second term of the objective function ÀÜùêπ (ùëö) (ùúÜ, ùúÉ ) in (9) is a perspective transformation [22, Section 2.3.3]
of the log-sum-exp function in composition of a convex function. We develop a customized batch
gradient descent with bisection search method to solve problem (9) eÔ¨Éciently. The gradient of the
objective function of (9) can be calculated as

‚àáùúÉ ÀÜùêπ (ùëö) (ùúÜ, ùúÉ ) =

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

(cid:2)ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) ‚àáùúÉ ùëìùúÉ (ùëß)(cid:3)
(cid:2)ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:3)

ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ
ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ

,

ùúï
ùúïùúÜ

ÀÜùêπ (ùëö) (ùúÜ, ùúÉ ) = ùúå +

(cid:18)

log

ùúñ
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ

ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:19)
(cid:104)

‚àí

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ

(cid:2)ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) ùëìùúÉ (ùëß)(cid:3)
(cid:2)ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:3)

.

ùúÜùîº ÀÜ‚Ñöùëö

ÀÜùë•ùëñ

For Ô¨Åxed ùúÜ > 0, denote ÀÜùêπ (ùëö)

ùúÜ

as the optimal value of the following problem

ÀÜùêπ (ùëö)
ùúÜ

= ùúÜùúå + inf
ùúÉ ‚ààŒò

(cid:40) 1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùúÜùúñ log

(cid:18)

ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ

(cid:104)

ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:19)(cid:41)

.

(10)

ùúÜ

is convex in ùúÜ. This, together with the fact that ÀÜùêπ (ùëö)

By the convexity of ÀÜùêπ (ùëö) (ùúÜ, ùúÉ ), the function ÀÜùêπ (ùëö)
is
sub-diÔ¨Äerentiable with respect to ùúÜ, motivates us to use the bisection search method in Algorithm 1 to
solve problem (9). In particular, in each iteration we Ô¨Årst Ô¨Ånd the optimal solution to problem (10) for
a Ô¨Åxed ùúÜ, and then use the bisection method to search for the optimal ùúÜ. When the gradient ‚àáùúÉ ùëìùúÉ (ùëß) is
bounded for any ùúÉ and ùëß, the sub-problem (10) can be solved using the projected gradient method
ùëÄ), where ùëÄ denotes the number of inner iterations [59, Theorem 2.5].
with convergence rate ùëÇ (1/
We also argue that the iteration points of Algorithm 1 converge to the optimal solution of infùúÜ ‚â•0 ÀÜùêπ (ùëö)
with a linear rate.
Proposition 1. Suppose the function ùëìùúÉ (ùëß) is convex in ùúÉ . Then the iteration points {ùúÜùë° }ùë° of Algorithm 1
converge into ùúÜ‚àó linearly, i.e., |ùúÜùë° ‚àí ùúÜ‚àó| = ùëÇ (2‚àíùë° ), where ùúÜ‚àó is the optimal solution of infùúÜ ‚â•0 ÀÜùêπ (ùëö)

‚àö

ùúÜ

ùúÜ

.

ùúÜ

Our last result in this section establishes the consistency for the Monte Carlo approximation (9).

Proposition 2. Let S ‚àó and ùëâ ‚àó be the set of optimal solutions and the corresponding optimal value of
problem (8), respectively. And let S (ùëö) and ùëâ (ùëö) be the set of optimal solutions and the optimal value to
problem (9), respectively. Assume that

(I) The function ùëìùúÉ (ùëß) is random lower semi-continuous, and convex in ùúÉ . The set Œò is closed, convex,

and contains a non-empty interior.

14

(II) The value function ùêπ (ùúÜ, ùúÉ ) deÔ¨Åned in (8) is lower semi-continuous, and there exists a point ( ¬ØùúÜ, ¬ØùúÉ ) ‚àà Œû

such that ùêπ (ùúÜ, ùúÉ ) < ‚àû for all (ùúÜ, ùúÉ ) in a neighborhood of ( ¬ØùúÜ, ¬ØùúÉ ).

(III) The set S ‚àó is non-empty and bounded.
Then as ùëö ‚Üí ‚àû, ùëâ (ùëö) ‚Üí ùëâ ‚àó almost surely and Dist(S (ùëö), S ‚àó) ‚Üí 0 almost surely.

This indicates that we can obtain a near-optimal solution of (Sinkhorn DRO) with an arbitrarily
small sub-optimality gap as long as we increase the number of simulation times for Monte Carlo
approximation. The proof leverages results from [86], while the key diÔ¨Äerence is that the objective
function studied in this paper involves the nonlinear transform of the expectation such that the existing
result in [86] cannot be applied directly. A detailed proof can be found in Appendix EC.5.

4.1. Alternative Algorithmic Choices
In this subsection, we discuss some other possibilities for designing an algorithm to solve (8).

As the objective in (8) involves the composition of two expectations, a natural idea to solve this
problem is to design algorithms leveraging techniques from stochastic compositional optimization
problem [101, 51, 108], but they cannot be applied directly here because they assume that the inner
expected-value is independent of the randomness in the outer expectation, while the inner expectation
of our objective in (8) depends on samples ÀÜùë•ùëñ, ùëñ = 1, . . . , ùëõ. The recent conditional stochastic compositional
(CSCO) optimization [55, 56], which aims to minimize a composition of two expected-value functions
with the inner expectation taken with respect to a conditional distribution, also opens the door for
designing eÔ¨Écient algorithms for solving (8). Although the objective in (8) cannot be written as
a CSCO problem, it naturally Ô¨Åts the structure when the dual variable ùúÜ > 0 is Ô¨Åxed. However, we
Ô¨Ånd that it takes relatively long time to obtain an optimal variable ùúÉ when ùúÜ is Ô¨Åxed, as the lack of
strong-convexity and smoothness structure of the objective makes the state-of-the-art CSCO algorithm
(BSGD in [55]) diÔ¨Écult to converge empirically. Since we need to optimize variables (ùúÜ, ùúÉ ) jointly, the
global convergence of CSCO for the problem (8) is even more challenging. In extensive simulations
that we omit, we Ô¨Ånd that our proposed method solves the Sinkhorn DRO problem more eÔ¨Éciently
than the state-of-the-art CSCO algorithm and standard projected gradient descent without bisection
search.

In our algorithm, we optimize ùúÉ for a Ô¨Åxed ùúÜ. An alternative would be to optimize (ùúÜ, ùúÉ ) jointly.
However, as pointed out in [68], for ùúÜ of a small value, the variance of the gradient estimate of the
objective function with respect to ùúÜ is unstable. Hence, we develop a bisection method to update ùúÜ in
outer iterations in Algorithm 1.

Moreover, we observe that in each inner iteration for solving the sub-problem (10), the most
computationally expansive step is to obtain the gradient of ÀÜùêπ (ùëö)
, the complexity of which is of ùëÇ (ùëõùëö).
For large-scale statistical learning problems, it is therefore promising to use the projected stochastic
gradient method instead of projected gradient descent to solve the sub-problem (10), but the condition
that guarantees convergence is more restrictive, which can be a topic for future study. For small-scale
problems, one can also write the problem in standard conic optimization form and use the interior
point method [70, 53] to solve it.

ùúÜ

It is worth mentioning that the strategy in [21] can be applied to obtain an unbiased gradient
estimate for the expectation term in (8), but the presence of the logarithm makes it not easy to use
standard stochastic gradient descent with unbiased gradient estimate for optimization. Instead, we
use the Monte Carlo samples problem (9) to approximate (8). It is promising to use other numerical
integration techniques such as Quasi-Monte Carlo method [74] and Wavelet method [6] to approximate
the objective (8) eÔ¨Éciently, which is of research interest for future study.

5. Applications
In this section, we apply our methodology on three applications: the newsvendor model, mean-
risk portfolio optimization, and semi-supervised learning. We examine the performance of the
(Sinkhorn DRO) model by comparing it with three benchmarks: (i) the classical sample average

approximation (SAA) model; (ii) the Wasserstein DRO model; and (iii) the KL-divergence DRO model.
Unless otherwise speciÔ¨Åed, the cost function is chosen to be ùëê (¬∑, ¬∑) = 1
2 (cid:107) ¬∑ ‚àí ¬∑ (cid:107)2, and the reference
measure ùúà for the Sinkhorn distance is chosen to be the Lebesgue measure.

For each of the three applications, with ùëõ training samples, we select the pair of hyper-parameters
(ùúñ, ùúå) using the ùêæ-fold cross-validation method with ùêæ = 10. Since the grid search for an optimal pair
of hyper-parameters is more costly than the search for a single hyper-parameter, we Ô¨Årst tune the
hyper-parameter ùúñ while Ô¨Åxing ùúå = 0, which corresponds to the SAA problem in (4). Then for the
chosen ùúñ, we tune the Sinkhorn radius ùúå. We run the repeated experiments for 200 times.

15

In Section 5.1 and Section 5.2, we measure the out-of-sample performance of a solution ùúÉ by its
, where ùêΩ ‚àó denotes the true optimal value when the true distribution

relative performance gap ùêΩ (ùúÉ )‚àíùêΩ ‚àó
(cid:12)ùêΩ ‚àó(cid:12)
(cid:12)

1+(cid:12)

is known exactly, and ùêΩ (ùúÉ ) the expected loss of the solution ùúÉ under the true distribution, estimated
through an SAA objective value with 105 testing samples. Thus, the smaller the relative performance
gap is, the better out-of-sample performance the solution has.

Further details are included in Appendix EC.1.

5.1. Newsvendor Model
We consider the following distributionally robust newsvendor model:

min
ùúÉ

max
‚Ñô‚ààùîπùúå,ùúñ ((cid:98)‚Ñô)

(cid:2)ùëòùúÉ ‚àí ùë¢ min(ùúÉ, ùëß)(cid:3),

ùîº‚Ñô

where the random variable ùëß stands for the random demand; its empirical distribution (cid:98)‚Ñô consists of
(cid:1),
ùë† exp (cid:0)‚àí ùë•
ùëõ = 20 independent samples from an exponential distribution ‚Ñô‚àó with the density ùëì (ùë•;ùë†) = 1
where ùë† ‚àà {0.25, 0.5, 0.75, 1, 2, 4}; the decision variable ùúÉ represents the inventory level; and ùëò = 5,ùë¢ = 7
are constants corresponding to overage and underage costs, respectively.

ùë†

Values of hyper-parameters are recorded in Table 1, from which we can see that the optimal entropic
regularization parameter ùúñ increases when the distribution scale parameter ùë† increases. This is because
a distribution ‚Ñô‚àó with larger value of ùë† has larger variance, and to achieve better out-of-sample
performance, a larger entropic regularization parameter ùúñ is needed to encourage larger spread of
probability mass.

Table 1

Values of selected hyper-parameters by cross-validation for the newsvendor problem.

Parameter ùë† Regularization ùúñ Sinkhorn Radius ùúå Wasserstein Radius ùúå KL-DRO Radius ùúÇ

0.25
0.5
0.75
1
2
4

2e-2
5e-2
8e-2
2e-1
4e-1
1e-0

1e-2
6e-3
1e-3
6e-3
1e-2
6e-2

1e-3
2e-3
1e-2
3e-1
3e-1
4e-1

2e-3
7e-3
2e-2
1e-2
4e-2
5e-3

We report the violin plots, i.e., box plots with shapes formed by kernel density estimation, for the
relative performance gap across diÔ¨Äerent approaches in Fig. 1. We Ô¨Ånd that Sinkhorn DRO has the
best out-of-sample mean/median performance in all Ô¨Ågures, and has the most stable performance as
indicated by the most concentrated violin plot. Wasserstein DRO, one the other hand, has a comparable
performance as SAA when ùë† is small (and small radius ùúå, as indicated in Table 1), but is even worse
than SAA for large ùë† (and large ùúå, as indicated in Table 1). This is because for small ùúå, the Wasserstein
robust solution coincides with the SAA solution and thus does not regularizer the problem, similar to
the observation made in [67, Remark 6.7] when the support of ‚Ñô‚àó is ‚Ñù; whereas for large ùúå, it hedges
distributions that are too extreme (which puts positive probability mass on zero demand), leading to

16

Figure 1

Out-of-sample performances for the newsvendor model with parameters ùë† ‚àà {0.25, 0.5, 0.75, 1, 2, 4} and the Ô¨Åxed
sample size ùëõ = 20.

an overly conservative solution. Moreover, the KL-divergence DRO does not have much improvement
compared with the SAA model, likely because the induced worst-case distribution shares the same
support as the empirical distribution.

5.2. Mean-risk Portfolio Optimization
We consider the following distributionally robust mean-risk portfolio optimization problem

min
ùúÉ

max
‚Ñô‚ààùîπùúå,ùúñ ((cid:98)‚Ñô)
s.t.

ùîº‚Ñô‚àó [‚àíùúÉ Tùëß] + ùúö ¬∑ ‚Ñô-CVaRùõº (‚àíùúÉ Tùëß)

ùúÉ ‚àà Œò = {ùúÉ ‚àà ‚Ñùùëë

+ : ùúÉ T1 = 1},

where the random vector ùëß ‚àà ‚Ñùùëë stands for the returns of assets; the decision variable ùúÉ ‚àà Œò represents
the portfolio strategy that invests a certain percentage ùúÉùëñ of the available capital in the ùëñ-th asset; and
the term ‚Ñô-CVaRùõº (‚àíùúÉ Tùëß) quantiÔ¨Åes conditional value-at-risk [82], i.e., the average of the ùõº √ó 100%
worst portfolio losses under the distribution ‚Ñô. We follow a similar setup as in [67]. SpeciÔ¨Åcally, we set
ùõº = 0.2, ùúö = 10. The random asset ùëß ‚àº ‚Ñô‚àó can be decomposed into a systematic risk factor ùúì ‚àà ‚Ñù and
idiosyncratic risk factors ùúñ ‚àà ‚Ñùùëë :

ùëßùëñ = ùúì + ùúñùëñ,

ùëñ = 1, 2, . . . , ùê∑,

where ùúì ‚àº N (0, 0.02) and ùúñùëñ ‚àº N (ùëñ √ó 0.03, ùëñ √ó 0.025). We Ô¨Åxed the training sample size ùëõ = 20 and vary
the number of assets ùê∑ ‚àà {10, 20, 50}. We solve this problem using Algorithm 1, in which the projected
gradient descent step to update ùúÉ follows the implementation in [27] to project onto the probability
simplex Œò.

The violin plots for the relative performance gap across diÔ¨Äerent approaches are reported in Fig. 2.
Similar as the Ô¨Ånding in Section 5.1, both the Wasserstein DRO and KL-divergence DRO models do not
outperform the SAA method too much, while Sinkhorn DRO has the best out-of-sample performance
for all plots as indicated by the smallest mean/median as well as the most concentrated violin plots,
and the contrast is more apparent when the dimension ùê∑ is large.

SAASinkhornWassersteinKL-divergenceMethod0.000.020.040.060.080.10Relative Performance Gapn=20 and s=0.25SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.20Relative Performance Gapn=20 and s=0.5SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.25Relative Performance Gapn=20 and s=0.75SAASinkhornWassersteinKL-divergenceMethod0.050.000.050.100.150.200.250.30Relative Performance Gapn=20 and s=1SAASinkhornWassersteinKL-divergenceMethod0.00.10.20.30.40.5Relative Performance Gapn=20 and s=2SAASinkhornWassersteinKL-divergenceMethod0.00.20.40.6Relative Performance Gapn=20 and s=417

Figure 2

Out-of-sample performances for the portfolio optimization problem with the dimension ùê∑ ‚àà {10, 20, 50} and a
Ô¨Åxed sample size ùëõ = 20.

5.3. Semi-supervised Learning
Our last example is a semi-supervised learning task following a similar setup as in [15]. Suppose we
have a training data set Dùëõ = {(ùëãùëñ, ùëåùëñ)}ùëõ
ùëñ=1, where ùëåùëñ ‚àà {‚àí1, 1} denotes the label of the ùëñ-th observation.
Additionally, we have a set of unlabeled observations {ùëãùëñ }ùëÅ
ùëñ=ùëõ+1 ‚à™
{(ùëãùëñ, ‚àí1)}ùëÅ
ùëñ=ùëõ+1, which means we replicate each unlabeled data point twice, recognizing that the
missing label can be any of the two available alternatives. Then we formulate an empirical distribution
consisting of samples from the set XùëÅ = Dùëõ ‚à™ EùëÅ ‚àíùëõ, which is denoted as (cid:98)‚Ñô. Considering the following
distributionally robust formulation:

ùëñ=ùëõ+1. We build the set EùëÅ ‚àíùëõ = {(ùëãùëñ, 1)}ùëÅ

min
ùúÉ

max
‚Ñô‚ààùîπùúå,ùúñ ((cid:98)‚Ñô)
where

(cid:2)‚Ñì (ùúÉ ; (ùëã, ùëå ))(cid:3)

ùîº‚Ñô

‚Ñì (ùúÉ ; (ùëã, ùëå )) = log(1 + exp(‚àíùëå ¬∑ ùúÉ Tùëã )).

We also solve this task using the other three benchmark models. The cost function over this subsection
is set to be

ùëê ((ùë•,ùë¶), (ùë• (cid:48),ùë¶ (cid:48))) =

(cid:107)ùë• ‚àí ùë• (cid:48)(cid:107)2

21{ùë¶ = ùë¶ (cid:48)} + ùúÖ1{ùë¶ ‚â† ùë¶ (cid:48)},

1
2

where the parameter ùúÖ = ‚àû means that there is no labeling error. In this case, in the duality result for
the Sinkhorn DRO, we only need to robustify the feature vectors ùëã .

We consider three performance measures for the obtained classiÔ¨Åers: (i) the training error of samples
with known labels; (ii) the training error of samples with unknown labels; and (iii) the testing error
for new observations. The experiment is conducted using 4 binary classiÔ¨Åcation real data sets from UCI
machine learning data base [40]. In each of the repeated experiments for each data set, we randomly
partition the collected samples into training and testing data sets.

ClassiÔ¨Åcation results for these diÔ¨Äerent approaches are reported in Table 2, where the Ô¨Årst number
of each entry represents the average classiÔ¨Åcation error, and the second number of entry represents
the half-length of the 95% conÔ¨Ådence interval. Detailed parameters for the settings of this task and
the choices of hyper-parameters are reported in Appendix EC.1. We observe that though the Sinkhorn
DRO does not have the best in-sample performance (as indicated by the training error of samples with
known labels), it has the best out-of-sample performance for all data sets (as indicated by the smallest
training error of samples with unknown labels and the smallest testing error).

6. Concluding Remarks
In this paper, we investigated a new distributionally robust optimization framework based on the
Sinkhorn distance. By developing a strong dual reformulation and a customized batch gradient
descent with bisection search algorithm, we have shown that the resulting DRO problem is tractable

SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.250.300.35Relative Performance Gapn=20 and D=10SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.250.300.35Relative Performance Gapn=20 and D=20SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.25Relative Performance Gapn=20 and D=5018

Table 2

ClassiÔ¨Åcation results on real datasets for the semi-supervised learning task. Each experiment is repeated for 200

independent trials, and 95% conÔ¨Ådence intervals of classiÔ¨Åcation errors are reported for diÔ¨Äerent approaches.

SAA

Sinkhorn

Wasserstein

KL-divergence

Train (Labeled)

.058 ¬± .061

Breast Cancer

Train (Unlabeled) .20 ¬± .068

Test Error

.19 ¬± .073

Train (Labeled)

.17 ¬± .12

Magic

Train (Unlabeled) .28 ¬± .082

Test Error

.28 ¬± .064

Train (Labeled)

.12 ¬± .067

QSAR Bio

Train (Unlabeled) .25 ¬± .057

Test Error

.25 ¬± .062

Train (Labeled)

.10 ¬± .046

Spambase

Train (Unlabeled) .19 ¬± .038

Test Error

.19 ¬± .032

.051 ¬± .065
.12 ¬± .068
.12 ¬± .068
.12 ¬± .068
.11 ¬± .067
.11 ¬± .067
.11 ¬± .067

.18 ¬± .11
.25 ¬± .091
.25 ¬± .091
.25 ¬± .091
.25 ¬± .074
.25 ¬± .074
.25 ¬± .074

.15 ¬± .076
.22 ¬± .063
.22 ¬± .063
.22 ¬± .063
.22 ¬± .065
.22 ¬± .065
.22 ¬± .065

.10 ¬± .048
.14 ¬± .046
.14 ¬± .046
.14 ¬± .046
.14 ¬± .036
.14 ¬± .036
.14 ¬± .036

.051 ¬± .063
.051 ¬± .063
.051 ¬± .063

.057 ¬± .060

.17 ¬± .073

.17 ¬± .075

.17 ¬± .11

.27 ¬± .077

.27 ¬± .058

.16 ¬± .073

.23 ¬± .073

.23 ¬± .079

.096 ¬± .045
.096 ¬± .045
.096 ¬± .045

.16 ¬± .036

.16 ¬± .028

.19 ¬± .038

.19 ¬± .073

.15 ¬± .12
.15 ¬± .12
.15 ¬± .12

.26 ¬± .078

.27 ¬± .066

.11 ¬± .066
.11 ¬± .066
.11 ¬± .066

.25 ¬± .037

.25 ¬± .042

.10 ¬± .043

.18 ¬± .034

.18 ¬± .042

under mild assumptions, greatly spans the tractability of Wasserstein DRO. Analysis on the worst-case
distribution indicates that Sinkhorn DRO hedges a more reasonable set of adverse scenarios and
thus less conservative compared with Wasserstein DRO, which is then demonstrated via extensive
numerical experiments. Based on theoretical and numerical Ô¨Åndings, we conclude that the Sinkhorn
distance is a promising candidate for modeling distributional ambiguities in decision-making under
uncertainty from the perspective of computational tractability, modeling rationality and out-of-sample
performance.

In the meantime, several topics worth in-depth investigating are left for future works. It is interesting
to design eÔ¨Écient Ô¨Årst-order algorithms for Sinkhorn DRO when the nominal distribution is arbitrary
or when the loss function is non-convex in the decision variable. Moreover, a meaningful research
question is the choice of the optimal hyper-parameters in Sinkhorn DRO, such as the radius of the
ambiguity set ùúå, the entropic regularization parameters ùúñ, and reference measures ùúà. This paper
focuses on regularizing Wasserstein distance with the entropic regularization ‚Äì the Sinkhorn distance,
but extensions to other types of regularization are possible. Exploring and discovering the beneÔ¨Åts of
Sinkhorn DRO in other types of applications may lead to future research directions.

Appendix A: SufÔ¨Åcient condition for Assumption 1
Proposition 3. Assumption 1 holds if there exists ùëù ‚â• 1 so that the following conditions are satisÔ¨Åed:

(I) For any ùë•,ùë¶, ùëß ‚àà Z, ùëê (ùë•,ùë¶) ‚â• 0, and

(ùëê (ùë•,ùë¶))1/ùëù ‚â§ (ùëê (ùë•, ùëß))1/ùëù + (ùëê (ùëß,ùë¶))1/ùëù .

(II) The nominal distribution (cid:98)‚Ñô has a Ô¨Ånite mean, denoted as ùë•. Moreover, ùúà {ùëß : 0 ‚â§ ùëê (ùë•, ùëß) < ‚àû} = 1

and

Pr ùë•‚àº(cid:98)‚Ñô{ùëê (ùë•, ùë•) < ‚àû} = 1.

19

(III) There exists ùúÜ > 0 such that

‚à´

ùëí ùëì (ùëß)/(ùúÜùúñ)ùëí‚àí21‚àíùëùùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) < ‚àû.

We make some remarks for the suÔ¨Écient conditions listed above. The Ô¨Årst condition can be satisÔ¨Åed
by taking the cost function as the ùëù-th power of the metric deÔ¨Åned on Z for any ùëù ‚â• 1. The second
condition requires the nominal distribution (cid:98)‚Ñô is Ô¨Ånite almost surely, e.g., it can be a subguassian
distribution with respect to the cost function ùëê. Combining three conditions together and levering
concentration arguments completes the proof of Proposition 3.

References

[1] Abdullah MA, Ren H, Ammar HB, Milenkovic V, Luo R, Zhang M, Wang J (2019) Wasserstein robust

reinforcement learning. arXiv preprint arXiv:1907.13196 .

[2] Agrawal S, Ding Y, Saberi A, Ye Y (2012) Price of correlations in stochastic optimization. Operations Research

60(1):150‚Äì162.

[3] Altschuler J, Weed J, Rigollet P (2017) Near-linear time approximation algorithms for optimal transport via

sinkhorn iteration. Advances in Neural Information Processing Systems, 1961‚Äì1971.

[4] ApS M (2021) Mosek modeling cookbook 3.2.3. https://docs.mosek.com/modeling-cookbook/index.

html#.

[5] Asmussen S, Glynn PW (2007) Stochastic simulation: algorithms and analysis, volume 57 (Springer Science

& Business Media).

[6] Aziz I, Haq F, et al. (2010) A comparative study of numerical integration based on haar wavelets and hybrid

functions. Computers & Mathematics with Applications 59(6):2026‚Äì2036.

[7] Bacharach M (1965) Estimating nonnegative matrices from marginal data. International Economic Review

6(3):294‚Äì310.

[8] Bai Y, Wu X, Ozgur A (2020) Information constrained optimal transport: From talagrand, to marton, to

cover. 2020 IEEE International Symposium on Information Theory (ISIT), 2210‚Äì2215.

[9] Bayraksan G, Love DK (2015) Data-driven stochastic programming using phi-divergences. The Operations

Research Revolution, 1‚Äì19 (INFORMS).

[10] Ben-Tal A, den Hertog D, De Waegenaere A, Melenberg B, Rennen G (2013) Robust solutions of optimization

problems aÔ¨Äected by uncertain probabilities. Management Science 59(2):341‚Äì357.

[11] Bertsimas D, Natarajan K, Teo CP (2006) Persistence in discrete optimization under data uncertainty.

Mathematical programming 108(2):251‚Äì274.

[12] Bertsimas D, Sim M, Zhang M (2019) Adaptive distributionally robust optimization. Management Science

65(2):604‚Äì618.

[13] Blanchet J, Chen L, Zhou XY (2018) Distributionally robust mean-variance portfolio selection with

wasserstein distances. arXiv preprint arXiv:1802.04885 .

[14] Blanchet J, Glynn PW, Yan J, Zhou Z (2019) Multivariate distributionally robust convex regression under

absolute error loss. Advances in Neural Information Processing Systems, volume 32, 11817‚Äì11826.

[15] Blanchet J, Kang Y (2020) Semi-supervised learning based on distributionally robust optimization. Data

Analysis and Applications 3 1‚Äì33.

[16] Blanchet J, Kang Y, Murthy K (2019) Robust wasserstein proÔ¨Åle inference and applications to machine

learning. Journal of Applied Probability 56(3):830‚Äì857.

[17] Blanchet J, Murthy K (2019) Quantifying distributional model risk via optimal transport. Mathematics of

Operations Research 44(2):565‚Äì600.

[18] Blanchet J, Murthy K, Nguyen VA (2021) Statistical analysis of wasserstein distributionally robust estimators.

arXiv preprint arXiv:2108.02120 .

20

[19] Blanchet J, Murthy K, Si N (2021) ConÔ¨Ådence regions in wasserstein distributionally robust estimation.

arXiv preprint arXiv:1906.01614 .

[20] Blanchet J, Murthy K, Zhang F (2021) Optimal transport based distributionally robust optimization:

Structural properties and iterative schemes. arXiv preprint arXiv:1810.02403 .

[21] Blanchet JH, Glynn PW (2015) Unbiased monte carlo for optimization and functions of expectations via

multi-level randomization. 2015 Winter Simulation Conference (WSC), 3656‚Äì3667.
[22] Boyd S, Vandenberghe L (2004) Convex optimization (Cambridge university press).
[23] Chang JT, Pollard D (2001) Conditioning as disintegration. Statistica Neerlandica 51(3):287‚Äì317.
[24] Chen R, Paschalidis IC (2018) A robust learning approach for regression models based on distributionally

robust optimization. Journal of Machine Learning Research 19(13):1‚Äì48.

[25] Chen R, Paschalidis IC (2019) Selecting optimal decisions via distributionally robust nearest-neighbor

regression. Advances in Neural Information Processing Systems.

[26] Chen Y, Li W (2021) Natural gradient in wasserstein statistical manifold. arXiv preprint arXiv:1805.08380 .
[27] Chen Y, Ye X (2011) Projection onto a simplex. arXiv preprint arXiv:1101.6081 .
[28] Chen Z, Kuhn D, Wiesemann W (2018) Data-driven chance constrained programs over wasserstein balls.

arXiv preprint arXiv:1809.00210 .

[29] Chen Z, Sim M, Xu H (2019) Distributionally robust optimization with inÔ¨Ånitely constrained ambiguity

sets. Operations Research 67(5):1328‚Äì1344.

[30] Cherukuri A, Cort√©s J (2019) Cooperative data-driven distributionally robust optimization. IEEE Transactions

on Automatic Control 65(10):4400‚Äì4407.

[31] Courty N, Flamary R, Habrard A, Rakotomamonjy A (2017) Joint distribution optimal transportation for

domain adaptation. Advances in Neural Information Processing Systems.

[32] Courty N, Flamary R, Tuia D (2014) Domain adaptation with regularized optimal transport. Joint European

Conference on Machine Learning and Knowledge Discovery in Databases, 274‚Äì289.

[33] Courty N, Flamary R, Tuia D, Rakotomamonjy A (2016) Optimal transport for domain adaptation. IEEE

Transactions on Pattern Analysis and Machine Intelligence 39(9):1853‚Äì1865.
[34] Cover TM, Thomas JA (2006) Elements of Information Theory (Wiley-Interscience).
[35] Cuturi M (2013) Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural

information processing systems, volume 26, 2292‚Äì2300.

[36] Delage E, Ye Y (2010) Distributionally robust optimization under moment uncertainty with application to

data-driven problems. Operations Research 58(3):595‚Äì612.

[37] Deming WE, Stephan FF (1940) On a least squares adjustment of a sampled frequency table when the

expected marginal totals are known. The Annals of Mathematical Statistics 11(4):427‚Äì444.

[38] Derman E, Mannor S (2020) Distributional robustness and regularization in reinforcement learning. arXiv

preprint arXiv:2003.02894 .

[39] Doan XV, Natarajan K (2012) On the complexity of nonoverlapping multivariate marginal bounds for

probabilistic combinatorial optimization problems. Operations research 60(1):138‚Äì149.

[40] Dua D, GraÔ¨Ä C (2017) UCI machine learning repository. [Online]. Available: http://archive.ics.uci.

edu/ml .

[41] Duchi JC, Glynn PW, Namkoong H (2021) Statistics of robust optimization: A generalized empirical

likelihood approach. Mathematics of Operations Research 0(0).

[42] Eckstein S, Kupper M, Pohl M (2020) Robust risk aggregation with neural networks. Mathematical Finance

30(4):1229‚Äì1272.

[43] Fr√©chet M (1960) Sur les tableaux dont les marges et des bornes sont donn√©es. Revue de l‚ÄôInstitut

international de statistique 10‚Äì32.

[44] Gao R (2020) Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the

curse of dimensionality. arXiv preprint arXiv:2009.04382 .

21

[45] Gao R, Chen X, Kleywegt AJ (2020) Wasserstein distributionally robust optimization and variation

regularization. arXiv preprint arXiv:1712.06050 .

[46] Gao R, Kleywegt AJ (2016) Distributionally robust stochastic optimization with Wasserstein distance. arXiv

preprint arXiv:1604.02199 .

[47] Gao R, Kleywegt AJ (2017) Data-driven robust optimization with known marginal distributions. Working

paper. Available at https://faculty.mccombs.utexas.edu/rui.gao/copula.pdf .

[48] Gao R, Kleywegt AJ (2017) Distributionally robust stochastic optimization with dependence structure. arXiv

preprint arXiv:1701.04200 .

[49] Genevay A, Cuturi M, Peyr√© G, Bach F (2016) Stochastic optimization for large-scale optimal transport.

Advances in Neural Information Processing Systems, volume 29.

[50] Genevay A, Peyre G, Cuturi M (2018) Learning generative models with sinkhorn divergences. Proceedings
of the Twenty-First International Conference on ArtiÔ¨Åcial Intelligence and Statistics, volume 84 of Proceedings
of Machine Learning Research, 1608‚Äì1617 (PMLR).

[51] Ghadimi S, Ruszczynski A, Wang M (2020) A single timescale stochastic approximation method for nested

stochastic optimization. SIAM Journal on Optimization 30(1):960‚Äì979.

[52] Goh J, Sim M (2010) Distributionally robust optimization and its tractable approximations. Operations

Research 58(4-part-1):902‚Äì917.

[53] Grant M, Boyd S (2014) CVX: Matlab software for disciplined convex programming, version 2.1. http:

//cvxr.com/cvx.

[54] H√§rdle W (1990) Applied nonparametric regression (Cambridge university press).
[55] Hu Y, Chen X, He N (2020) Sample complexity of sample average approximation for conditional stochastic

optimization. SIAM Journal on Optimization 30(3):2103‚Äì2133.

[56] Hu Y, Zhang S, Chen X, He N (2020) Biased stochastic Ô¨Årst-order methods for conditional stochastic
optimization and applications in meta learning. Advances in Neural Information Processing Systems,
volume 33, 2759‚Äì2770.

[57] Hu Z, Hong LJ (2012) Kullback-leibler divergence constrained distributionally robust optimization. Opti-

mization Online preprint Optimization Online:2012/11/3677 .

[58] Huang M, Ma S, Lai L (2021) A riemannian block coordinate descent method for computing the projection
robust wasserstein distance. Proceedings of the 38th International Conference on Machine Learning, 4446‚Äì
4455.

[59] Jain P, Kar P (2017) Non-convex optimization for machine learning. Foundations and Trends in Machine

Learning 10(3-4):142‚Äì363.

[60] Kruithof J (1937) Telefoonverkeersrekening. De Ingenieur 52:15‚Äì25.
[61] Kuhn D, Esfahani PM, Nguyen VA, ShaÔ¨Åeezadeh-Abadeh S (2019) Wasserstein distributionally robust
optimization: Theory and applications in machine learning. Operations Research & Management Science in
the Age of Analytics, 130‚Äì166 (INFORMS).

[62] Li J, Huang S, So AMC (2019) A Ô¨Årst-order algorithmic framework for wasserstein distributionally robust
logistic regression. Proceedings of the 33rd International Conference on Neural Information Processing Systems,
3937‚Äì3947.

[63] Li W, Montufar G (2021) Natural gradient via optimal transport. arXiv preprint arXiv:1803.07033 .
[64] Lin T, Fan C, Ho N, Cuturi M, Jordan M (2020) Projection robust wasserstein distance and riemannian

optimization. Advances in Neural Information Processing Systems, volume 33, 9383‚Äì9397.

[65] Luise G, Rudi A, Pontil M, Ciliberto C (2018) DiÔ¨Äerential properties of sinkhorn approximation for learning

with wasserstein distance. Advances in Neural Information Processing Systems.

[66] Luo F, Mehrotra S (2019) Decomposition algorithm for distributionally robust optimization using wasserstein
metric with an application to a class of regression models. European Journal of Operational Research
278(1):20‚Äì35.

22

[67] Mohajerin Esfahani P, Kuhn D (2017) Data-driven distributionally robust optimization using the wasserstein
metric: performance guarantees and tractable reformulations. Mathematical Programming 171(1):115‚Äì166.
[68] Namkoong H, Duchi JC (2016) Stochastic gradient methods for distributionally robust optimization with

f-divergences. Advances in Neural Information Processing Systems, volume 29, 2208‚Äì2216.

[69] Natarajan K, Song M, Teo CP (2009) Persistency model and its applications in choice modeling. Management

Science 55(3):453‚Äì469.

[70] Nemirovski A (2002) Lectures on modern convex optimization. Society for Industrial and Applied Mathematics

(SIAM.

[71] Nesterov Y, Nemirovskii A (1994) Interior-point polynomial algorithms in convex programming (SIAM).
[72] Nguyen VA, Si N, Blanchet J (2020) Robust bayesian classiÔ¨Åcation using an optimistic score ratio.

International Conference on Machine Learning, 7327‚Äì7337.

[73] Nguyen VA, Zhang F, Blanchet J, Delage E, Ye Y (2021) Robustifying conditional portfolio decisions via

optimal transport. arXiv preprint arXiv:2103.16451 .

[74] Niederreiter H (1992) Random number generation and quasi-Monte Carlo methods (SIAM).
[75] Patrini G, van den Berg R, Forre P, Carioni M, Bhargav S, Welling M, Genewein T, Nielsen F (2020) Sinkhorn

autoencoders. Uncertainty in ArtiÔ¨Åcial Intelligence, 733‚Äì743.

[76] Petzka H, Fischer A, Lukovnikov D (2018) On the regularization of wasserstein GANs. International

Conference on Learning Representations.

[77] Peyre G, Cuturi M (2019) Computational optimal transport: With applications to data science. Foundations

and Trends in Machine Learning 11(5-6):355‚Äì607.

[78] PÔ¨Çug G, Wozabal D (2007) Ambiguity in portfolio selection. Quantitative Finance 7(4):435‚Äì442.
[79] Pichler A, Shapiro A (2021) Mathematical foundations of distributionally robust multistage optimization.

arXiv preprint arXiv:2101.02498 .

[80] Popescu I (2005) A semideÔ¨Ånite programming approach to optimal-moment bounds for convex classes of

distributions. Mathematics of Operations Research 30(3):632‚Äì657.

[81] Rahimian H, Mehrotra S (2019) Distributionally robust optimization: A review. arXiv preprint

arXiv:1908.05659 .

[82] Rockafellar RT, Uryasev S, et al. (1999) Optimization of conditional value-at-risk. Journal of risk 2:21‚Äì42.
[83] Scarf H (1957) A min-max solution of an inventory problem. Studies in the mathematical theory of inventory

and production .

[84] ShaÔ¨Åeezadeh-Abadeh S, Kuhn D, Esfahani PM (2019) Regularization via mass transportation. Journal of

Machine Learning Research 20(103):1‚Äì68.

[85] ShaÔ¨Åeezadeh Abadeh S, Mohajerin Esfahani PM, Kuhn D (2015) Distributionally robust logistic regression.

Advances in Neural Information Processing Systems, volume 28.

[86] Shapiro A, Dentcheva D, Ruszczy≈Ñski A (2014) Lectures on stochastic programming: modeling and theory

(SIAM).

[87] Singh D, Zhang S (2020) Tight bounds for a class of data-driven distributionally robust risk measures. arXiv

preprint arXiv:2010.05398 .

[88] Singh D, Zhang S (2021) Distributionally robust proÔ¨Åt opportunities. Operations Research Letters 49(1):121‚Äì

128.

[89] Sinha A, Namkoong H, Duchi J (2018) CertiÔ¨Åable distributional robustness with principled adversarial

training. International Conference on Learning Representations.

[90] Sinkhorn R (1964) A relationship between arbitrary positive matrices and doubly stochastic matrices. The

annals of mathematical statistics 35(2):876‚Äì879.

[91] Smirnova E, Dohmatob E, Mary J (2019) Distributionally robust reinforcement learning. arXiv preprint

arXiv:1902.08708 .

23

[92] Staib M, Jegelka S (2019) Distributionally robust optimization and generalization in kernel methods.

Advances in Neural Information Processing Systems 32:9134‚Äì9144.

[93] Taskesen B, Nguyen VA, Kuhn D, Blanchet J (2020) A distributionally robust approach to fair classiÔ¨Åcation.

arXiv preprint arXiv:2007.09530 .

[94] Van Parys BP, Goulart PJ, Kuhn D (2015) Generalized gauss inequalities via semideÔ¨Ånite programming.

Mathematical Programming 156(1-2):271‚Äì302.

[95] Vandenberghe L, Boyd S (1995) SemideÔ¨Ånite programming. SIAM review 38(1):49‚Äì95.
[96] Wang C, Gao R, Qiu F, Wang J, Xin L (2018) Risk-based distributionally robust optimal power Ô¨Çow with

dynamic line rating. IEEE Transactions on Power Systems 33(6):6074‚Äì6086.

[97] Wang J, Gao R, Xie Y (2021) Two-sample test using projected wasserstein distance. 2021 IEEE International

Symposium on Information Theory (ISIT).

[98] Wang J, Gao R, Xie Y (2021) Two-sample test with kernel projected wasserstein distance. arXiv preprint

arXiv:2102.06449 .

[99] Wang J, Gao R, Zha H (2021) Reliable oÔ¨Ä-policy evaluation for reinforcement learning. arXiv preprint

arXiv:2011.04102 .

[100] Wang J, Jia Z, Yin H, Yang S (2021) Small-sample inferred adaptive recoding for batched network coding.

2021 IEEE International Symposium on Information Theory (ISIT).

[101] Wang M, Fang EX, Liu H (2016) Stochastic compositional gradient descent: algorithms for minimizing

compositions of expected-value functions. Mathematical Programming 161(1-2):419‚Äì449.

[102] Wang Z, Glynn PW, Ye Y (2015) Likelihood robust optimization for data-driven problems. Computational

Management Science 13(2):241‚Äì261.

[103] Wiesemann W, Kuhn D, Sim M (2014) Distributionally robust convex optimization. Operations Research

62(6):1358‚Äì1376.

[104] Wozabal D (2012) A framework for optimization under ambiguity. Annals of Operations Research 193(1):21‚Äì

47.

[105] Xie W (2019) On distributionally robust chance constrained programs with wasserstein distance. Mathe-

matical Programming 186(1):115‚Äì155.

[106] Yang I (2017) A convex optimization approach to distributionally robust markov decision processes with

wasserstein distance. IEEE control systems letters 1(1):164‚Äì169.

[107] Yang I (2020) Wasserstein distributionally robust stochastic control: A data-driven approach. IEEE

Transactions on Automatic Control 66(8):3863‚Äì3870.

[108] Yang S, Wang M, Fang EX (2019) Multilevel stochastic gradient methods for nested composition optimization.

SIAM Journal on Optimization 29(1):616‚Äì659.

[109] Yule GU (1912) On the methods of measuring association between two attributes. Journal of the Royal

Statistical Society 75(6):579‚Äì652.

[110] Zhao C, Guan Y (2018) Data-driven risk-averse stochastic optimization with wasserstein metric. Operations

Research Letters 46(2):262‚Äì267.

[111] Zhu J, Jitkrittum W, Diehl M, Sch√∂lkopf B (2021) Kernel distributionally robust optimization: Generalized
duality theorem and stochastic approximation. Proceedings of The 24th International Conference on ArtiÔ¨Åcial
Intelligence and Statistics, 280‚Äì288.

[112] Zymler S, Kuhn D, Rustem B (2013) Distributionally robust joint chance constraints with second-order

moment information. Mathematical Programming 137(1):167‚Äì198.

ec1

Supplementary for ‚ÄúSinkhorn Distributionally Robust Optimization‚Äù

Appendix EC.1: Detailed Experiment Setup
All the experiments are preformed on a MacBook Pro laptop with 32GB of memory running python 3.7.
Candidates of hyper-parameters for DRO models are listed as follows. In each experiment we pick
the regularization term ùúñ spaced from 1e-3 to 9e-1 in exponentially increasing steps. The Sinkhorn
radius ùúå is chosen spaced from 1e-5 to 1e-1 in exponentially increasing steps. The Wasserstein
radius ùúå and KL-DRO radius ùúÇ are chosen spaced from 1e-3 to 9e-1 in exponentially increasing steps.
Hyper-parameters for the second and third experiments are reported in Table EC.1 and Table EC.2,
respectively. To obtain the Monte Carlo approximated objective function for the Sinkhorn DRO model,
we take the nominal distribution (cid:98)‚Ñô as the empirical distribution based on collected samples, and the
inner batch size ùëö = 20. We use the projected gradient descent method to solve the subproblem in
(10). For portfolio optimization problems we try the step size ùúÇ‚Ñì = 1
for the ‚Ñì-th inner iteration.
‚àö
‚Ñì+1
Otherwise we try the step size ùúÇ‚Ñì = 1
‚Ñì+1 during the ‚Ñì-th inner iteration. Denote by obj‚Ñì the objective
function obtained at the ‚Ñì-th iteration. The inner iteration is terminated when (cid:107)obj‚Ñì +1‚àíobj‚Ñì (cid:107)
1+ (cid:107)obj‚Ñì (cid:107) ‚â§ 1e-3.
The SAA, Wasserstein DRO, and KL-divergence DRO models are solved exactly based on the interior
point method-based solver Mosek [4]. In particular, based on [67, Corollary 5.1], the Wasserstein DRO
formulation for the newsvendor problem in Section 5.1 becomes

min
ùúÉ,ùúÜ,ùë†,ùõæ

ùúÜùúå +

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùë†ùëñ

s.t.

(ùëò ‚àí ùë¢)ùúÉ + ùõæùëñ,1ÀÜùëßùëñ ‚â§ ùë†ùëñ, ùëñ ‚àà [ùëõ],

ùëòùúÉ ‚àí ùë¢ÀÜùëßùëñ + ùõæùëñ,1ÀÜùëßùëñ ‚â§ ùë†ùëñ, ùëñ ‚àà [ùëõ],

ùõæùëñ,1 ‚â§ ùúÜ, ùëñ ‚àà [ùëõ],

| ‚àí ùõæùëñ,2 + ùë¢ | ‚â§ ùúÜ, ùëñ ‚àà [ùëõ],

ùõæ ‚â• 0,

where {ÀÜùëß1, . . . , ÀÜùëßùëõ} denotes collected samples from (cid:98)‚Ñô‚àó. From [67, Eq. (27)] we can see that the
Wasserstein DRO formulation for the portfolio optimization problem becomes

min
ùúÉ,ùúè,ùúÜ,ùë†

ùúÜùúå +

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùë†ùëñ

s.t.

ùúÉ ‚àà Œò,

ùëè ùëóùúè + ùëé ùëó (cid:104)ùúÉ, ÀÜùëßùëñ(cid:105) ‚â§ ùë†ùëñ, ùëñ ‚àà [ùëõ], ùëó ‚àà [ùêª ],

(cid:107)ùëé ùëóùúÉ (cid:107)2 ‚â§ ùúÜ, ùëó ‚àà [ùêª ].

Recall that the KL-divergence DRO problem with radius ùúÇ ‚â• 0 has the following tractable formulation:

min
ùúÉ ‚ààŒò,ùúÜ ‚â•0

(cid:110)

ùúÜùúÇ + ùúÜ log

(cid:16)

ùîº
(cid:98)‚Ñô

ùëí ùëìùúÉ (ùëß)/ùúÜ(cid:105) (cid:17)(cid:111)
(cid:104)

.

ec2

Table EC.1

Values of selected hyper-parameters by cross-validation for the portfolio optimization problem.

Dimension ùê∑ Regularization ùúñ Sinkhorn Radius ùúå Wasserstein Radius ùúå KL-DRO Radius ùúÇ

10

20

50

5e-2

7e-1

4e-1

2e-4

3e-4

2e-4

6e-3

5e-2

1e-3

1e-3

2e-3

2e-3

Table EC.2

Values of classiÔ¨Åcation parameters and hyper-parameters for DRO models.

Breast Cancer

Magic

QSAR Bio

Spambase

Number of Predictors

Train Size (Labeled)

Train Size (Unlabeled)

Test Size

30

40

200

329

10

30

300

18690

30

80

500

475

56

150

600

3850

Sinkhorn Para. (ùúå, ùúñ)

(2e-1,2e-5)

(5e-1,6e-2)

(8e-1,2e-3)

(9e-3,2e-5)

Wasserstein Para. ùúå

KL-DRO Para. ùúÇ

1e-3

2e-3

3e-3

5e-3

3e-1

3e-2

1e-2

4e-2

ec3

Appendix EC.2: Proofs of Technical Results in Section 3.2
In order to show the strong duality result in Theorem 1 when (cid:98)‚Ñô is an empirical distribution, we present
the following technical lemma.
Lemma EC.1. For Ô¨Åxed ùúè and a reference probability distribution ‚Ñö ‚àà P (Z), consider the optimization
problem

ùë£ (ùúè) = sup

‚Ñô‚ààP (Z)

(cid:26)

(cid:20)

ùîº‚Ñô

ùëì (ùëß) ‚àí ùúè log

(cid:19)(cid:21) (cid:27)

(cid:18) d‚Ñô
d‚Ñö

(ùëß)

.

(EC.1)

(I) When ùúè = 0,

(II) When ùúè > 0 and

it holds that

ùë£ (0) = ess sup

(ùëì ) (cid:44) inf{ùë° ‚àà ‚Ñù : Pr ùëß‚àº‚Ñö{ùëì (ùëß) > ùë° } = 0}.

‚Ñö

ùëí ùëì (ùëß)/ùúè (cid:105)
(cid:104)

< ‚àû,

ùîº‚Ñö

ùë£ (ùúè) = ùúè log

(cid:16)

ùîº‚Ñö

ùëí ùëì (ùëß)/ùúè (cid:105) (cid:17)
(cid:104)

,

and limùúè ‚Üì0 ùë£ (ùúè) = ùë£ (0). The optimal solution in (EC.1) has the expression

(III) When ùúè > 0 and

we have that ùë£ (ùúè) = ‚àû.

d‚Ñô(ùëß) =

ùëí ùëì (ùëß)/ùúè
‚à´ ùëí ùëì (ùë¢)/ùúè d‚Ñö(ùë¢)

d‚Ñö(ùëß).

ùëí ùëì (ùëß)/ùúè (cid:105)
(cid:104)

= ‚àû,

ùîº‚Ñö

Proof of Lemma EC.1 We reformulate ùë£ (ùúè) based on the importance sampling trick:
(cid:27)

(cid:26)‚à´

‚à´

(cid:2)ùëì (ùëß)ùêø(ùëß) ‚àí ùúèùêø(ùëß) log ùêø(ùëß)(cid:3) d‚Ñö(ùëß) :

ùêø(ùëß) d‚Ñö(ùëß) = 1

.

ùë£ (ùúè) = sup
ùêø: ùêø ‚â•0

Then the remaining part follows the discussion in [57, Section 2.1].

(cid:3)

Proof of Corollary 1 We now introduce the epi-graphical variables ùë†ùëñ, ùëñ = 1, . . . , ùëõ to reformulate ùëâD as

inf
ùúÜ ‚â•0,ùë†ùëñ

ùúÜùúå +

1
ùëõ

s.t. ùúÜùúñ log

ùëõ
‚àëÔ∏Å

ùë†ùëñ

ùëñ=1
(cid:16)

ùîº‚Ñöùëñ,ùúñ

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

‚â§ ùë†ùëñ, ‚àÄùëñ

ùëâD =

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥
Ô£≥

For Ô¨Åxed ùëñ, the ùëñ-th constraint can be reformulated as
ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:111)
(cid:104)
(cid:21) (cid:27)

(cid:16) ùë†ùëñ
ùúÜùúñ

exp

(cid:26)

(cid:110)

(cid:17)

‚â• ùîº‚Ñöùëñ,ùúñ
(cid:20)
ùëí

(cid:2)ùëì (ùëß)‚àíùë†ùëñ

(cid:3) /(ùúÜùúñ)

(cid:20)

ùúÜùúñùëí

(cid:2)ùëì (ùëß)‚àíùë†ùëñ

(cid:3) /(ùúÜùúñ)

(cid:21) (cid:27)

=

1 ‚â• ùîº‚Ñöùëñ,ùúñ

(cid:26)

=

ùúÜùúñ ‚â• ùîº‚Ñöùëñ,ùúñ
(cid:40)

ùêø
‚àëÔ∏Å

‚Ñì=1

=

ùúÜùúñ ‚â•

‚Ñöùëñ,ùúñ (ùëß‚Ñì )ùëéùëñ,‚Ñì

(cid:41)

(cid:92)

(cid:26)
ùëéùëñ,‚Ñì ‚â• ùúÜùúñ exp

(cid:19)

(cid:18) ùëì (ùëß‚Ñì ) ‚àí ùë†ùëñ
ùúÜùúñ

(cid:27)

,

, ‚àÄ‚Ñì

where the second constraint set can be formulated as

(ùúÜùúñ, ùëéùëñ,‚Ñì, ùëì (ùëß‚Ñì ) ‚àí ùë†ùëñ) ‚àà Kexp.

Substituting this expression into ùëâD completes the proof.

(cid:3)

ec4

Appendix EC.3: Proof of the Technical Result in Section 3.1
Proof of Remark 3 We can reformulate the dual objective function as

ùë£ (ùúÜ; ùúñ) = ùúÜùúå + ùúÜùúñ

‚à´

(cid:18)‚à´

log

exp

(cid:18) ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß)
ùúÜùúñ

(cid:19)

(cid:19)

dùúà (ùëß)

d(cid:98)‚Ñô(ùë•).

We take limit for the second term in ùë£ (ùúÜ; ùúñ) to obtain:

lim
ùúñ‚Üí0
‚à´

‚à´

ùúÜùúñ

log

ùúÜ
ùõΩ

log

lim
ùõΩ‚Üí‚àû

(cid:18)‚à´

(cid:32)‚à´

exp

exp

‚à´

ùúÜ‚àá log

lim
ùõΩ‚Üí‚àû

(cid:32)‚à´

exp

(cid:19)

(cid:18) ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß)
ùúÜùúñ
(cid:32) (cid:2)ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß)(cid:3) ùõΩ
ùúÜ
(cid:32) (cid:2)ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß)(cid:3) ùõΩ
ùúÜ

(cid:33)

=

=

dùúà (ùëß)

(cid:19)

d(cid:98)‚Ñô(ùë•)
(cid:33)

dùúà (ùëß)

d(cid:98)‚Ñô(ùë•)

(cid:33)

(cid:33)

dùúà (ùëß)

d(cid:98)‚Ñô(ùë•)

‚à´

=

‚à´

=

lim
ùõΩ‚Üí‚àû

Ô£Æ
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞
sup
ùëß

‚à´ exp

(cid:18) (cid:2)ùëì (ùëß)‚àíùúÜùëê (ùë•,ùëß)(cid:3) ùõΩ
ùúÜ

(cid:19)

(cid:2)ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß)(cid:3) dùúà (ùë¶)

‚à´ exp

(cid:18) (cid:2)ùëì (ùëß)‚àíùúÜùëê (ùë•,ùëß)(cid:3) ùõΩ
ùúÜ

(cid:19)

dùúà (ùë¶)

d(cid:98)‚Ñô(ùë•)

Ô£π
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

(cid:2)ùëì (ùëß) ‚àí ùúÜùëê (ùë•, ùëß)(cid:3) d(cid:98)‚Ñô(ùë•).

Hence, we conclude that the dual objective function of the Sinkhorn DRO problem converges into that
(cid:3)
of the Wasserstein DRO problem.

ec5

Appendix EC.4: Proofs of Technical Results in Section 3.3

Proof of Lemma 1 Recall from Remark 4 that the primal problem ùëâ can be reformulated as

(cid:26)‚à´

ùëâ =

sup
ùõæùë• ‚ààP (Z),‚àÄùë• ‚ààZ

ùîºùõæùë• [ùëì (ùëß)] d(cid:98)‚Ñô(ùë•) : ùúñ

‚à´

ùîºùõæùë•

(cid:20)

log

(cid:19)(cid:21)

(cid:18) dùõæùë• (ùëß)
d‚Ñöùëñ (ùëß)

d(cid:98)‚Ñô(ùë•) ‚â§ ùúå

(cid:27)

.

Introducing the Lagrange multiplier ùúÜ associated to the constraint, we reformulate ùëâ as

ùëâ =

sup
ùõæùë• ‚ààP (Z),‚àÄùë• ‚ààZ

(cid:26)

(cid:26)

inf
ùúÜ ‚â•0

‚à´

ùúÜùúå +

(cid:20)

ùîºùõæùë•

ùëì (ùëß) ‚àí ùúÜùúñ log

(cid:19)(cid:21)

(cid:18) dùõæùë• (ùëß)
d‚Ñöùë•,ùúñ (ùëß)

(cid:27)(cid:27)

d(cid:98)‚Ñô(ùë•)

Interchanging the order of the supremum and inÔ¨Åmum operators, we have that

(cid:40)

ùëâ ‚â§ inf
ùúÜ ‚â•0

ùúÜùúå +

sup
ùõæùë• ‚ààP (Z),‚àÄùë• ‚ààZ

(cid:26)‚à´

(cid:20)

ùîºùõæùë•

ùëì (ùëß) ‚àí ùúÜùúñ log

(cid:19)(cid:21)

(cid:18) dùõæùë• (ùëß)
d‚Ñöùë•,ùúñ (ùëß)

(cid:27)(cid:41)

d(cid:98)‚Ñô(ùë•)

.

.

Since the optimization over ùõæùë•, ‚àÄùë• is separable for each ùë•, by deÔ¨Åning

ùë£ùë• (ùúÜ) = sup

ùõæùë• ‚ààP (Z)

(cid:26)

(cid:20)

ùîºùõæùë•

ùëì (ùëß) ‚àí ùúÜùúñ log

(cid:19)(cid:21) (cid:27)

(cid:18) dùõæùë• (ùëß)
d‚Ñöùë•,ùúñ (ùëß)

, ‚àÄùë•,

and swap the supremum and the integration, we obtain

‚à´

(cid:26)

ùúÜùúå +

ùë£ùë• (ùúÜ) d(cid:98)‚Ñô(ùë•)

(cid:27)

.

ùëâ ‚â§ inf
ùúÜ ‚â•0

(EC.2)

When there exists ùúÜ > 0 such that Condition 1 holds, by leveraging a well-known reformulation on
entropy regularized linear optimization in Lemma EC.1, we can see that almost surely,

ùë£ùë• (ùúÜ) = ùúÜùúñ log

(cid:16)

ùîº‚Ñöùë•,ùúñ

(cid:104)
ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)

< ‚àû.

Substituting this expression into (EC.2) implies that ùëâ ‚â§ ùëâD < ‚àû. Suppose on the contrary that for any
ùúÜ > 0,

Pr ùë•‚àº‚Ñô

(cid:110)
ùë• : ùîº‚Ñöùë•,ùúñ

(cid:104)
ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105)

(cid:111)

= ‚àû

> 0,

then intermediately we obtain ùëâ ‚â§ ùëâD = ‚àû, and the weak duality still holds.

(cid:3)
(cid:3)

Proof of Lemma 2 We Ô¨Årst show that ùúÜ‚àó < ‚àû. Denote by ùë£ (ùúÜ) the objective function for the dual

problem, then

ùë£ (ùúÜ) = ùúÜùúå + ùúÜùúñ

‚à´

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•).

The integrability condition for the dominated convergence theorem is satisÔ¨Åed, which implies

ùúÜùúñ

lim
ùúÜ‚Üí‚àû
‚à´

lim
ùõΩ‚Üí0

‚à´

ùúñ
ùõΩ

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•)

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

ùëí ùõΩ ùëì (ùëß)/ùúñ (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•)

‚à´

‚à´

‚à´

ùúñ‚àáùõΩ log

lim
ùõΩ‚Üí0

(cid:16)

ùîº‚Ñöùë•,ùúñ

ùëí ùõΩ ùëì (ùëß)/ùúñ (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•)

ùúñ

lim
ùõΩ‚Üí0

ùîº‚Ñöùë•,ùúñ

(cid:18)

1
(cid:2)ùëí ùõΩ ùëì (ùëß)/ùúñ (cid:3)

ùîº‚Ñöùë•,ùúñ

(cid:20) ùëì (ùëß)
ùúñ

(cid:21) (cid:19)

ùëí (ùõΩ ùëì (ùëß))/ùúñ

d(cid:98)‚Ñô(ùë•)

ùîº‚Ñöùë•,ùúñ [ùëì (ùëß)] d(cid:98)‚Ñô(ùë•),

=

=

=

=

ec6

where the Ô¨Årst equality follows from the change-of-variable technique with ùõΩ = 1/ùúÜ, the second equality
follows from the L‚ÄôHospital rule the third and the last equality follows from the dominated convergence
theorem. As a consequence, as long as ùúå > 0, we have

We can take ùúÜ satisfying Condition 1 and then ùë£ (ùúÜ) < ‚àû, which guarantees the existence of the dual
(cid:3)
minimizer. Hence ùúÜ‚àó < ‚àû, which implies that either ùúÜ‚àó = 0 or ùúÜ‚àó satisÔ¨Åes Condition 1.

lim
ùúÜ‚Üí‚àû

ùë£ (ùúÜ) = ‚àû.

Proof of Lemma 3 Suppose the dual minimizer ùúÜ‚àó = 0, then taking the limit of the dual objective

function gives

where

‚à´

lim
ùúÜ‚Üí0

ùë£ (ùúÜ) =

ùêªùë¢ (ùë•) d(cid:98)‚Ñô(ùë•) < ‚àû,

ùêªùë¢ (ùë•) := inf{ùë° : ‚Ñöùë•,ùúñ {ùëì (ùëß) > ùë° } = 0} (cid:44) ess sup

ùëì .

‚Ñöùë•,ùúñ

For notational simplicity we take ùêªùë¢ = ess sup
for any ùë° so that ‚Ñöùë•,ùúñ {ùëì (ùëß) > ùë° } = 0, we have that

ùúà

ùëì . One can check that ùêªùë¢ (ùë•) ‚â° ùêªùë¢ for any ùë• ‚àà supp((cid:98)‚Ñô):

‚à´

1{ùëì (ùëß) > ùë° }ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) = 0,

which, together with the fact that ùúà {ùëê (ùë•, ùëß) < ‚àû} = 1 for Ô¨Åxed ùë•, implies

‚à´

1{ùëì (ùëß) > ùë° } dùúà (ùëß) = 0.

On the contrary, for any ùë° so that ùúà {ùëì (ùëß) > ùë° } = 0, we have that

‚à´

0 ‚â§

1{ùëì (ùëß) > ùë° }ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) ‚â§

‚à´

1{ùëì (ùëß) > ùë° } dùúà (ùëß) = 0,

where the second inequality is because that ùúà {ùëê (ùë•, ùëß) ‚â• 0} = 1. As a consequence, ‚Ñöùë•,ùúñ {ùëì (ùëß) > ùë° } = 0.
Hence we can assert that ùêªùë¢ (ùë•) = ùêªùë¢ for all ùë• ‚àà supp((cid:98)‚Ñô), which implies

Then we show that almost surely for all ùë•,

ùë£ (ùúÜ) = ùêªùë¢ < ‚àû.

lim
ùúÜ‚Üí0

ùîº‚Ñöùë•,ùúñ [1ùê¥] > 0, where ùê¥ = {ùëß : ùëì (ùëß) = ùêªùë¢ }.

Denote by ùê∑ the collection of samples ùë• so that ùîº‚Ñöùë•,ùúñ [1ùê¥] = 0. Assume the condition above does not
hold, which means that (cid:98)‚Ñô{ùê∑ } > 0. For any ùúè > 0 and ùë• ‚àà ùê∑, there exists ùêª ùëô (ùë•) < ùêªùë¢ such that

0 < ùî•ùë• := ùîº‚Ñöùë•,ùúñ [1ùêµ (ùë•) ] ‚â§ ùúè, where ùêµ(ùë•) = {ùëß : ùêª ùëô (ùë•) ‚â§ ùëì (ùëß) ‚â§ ùêªùë¢ }.

DeÔ¨Åne ùêª gap(ùë•) = ùêªùë¢ ‚àí ùêª ùëô (ùë•), ùî•ùëê

ùë• = 1 ‚àí ùî•ùë• . Then we Ô¨Ånd that for ùë• ‚àà ùê∑,

ùë£ùë• (ùúÜ) = ùúÜùúñ log

(cid:16)

ùîº‚Ñöùë•,ùúñ
(cid:16)

(cid:105)

(cid:104)
ùëí ùëì (ùëß)/(ùúÜùúñ) 1ùêµ (ùë•)
ùî•ùë• + ùëí‚àíùêª gap (ùë•)/(ùúÜùúñ) ùî•ùëê
ùë•

+ ùîº‚Ñöùë•,ùúñ
(cid:17)

.

(cid:104)
ùëí ùëì (ùëß)/(ùúÜùúñ) 1ùêµ (ùë•)ùëê

(cid:105) (cid:17)

‚â§ ùêªùë¢ + ùúÜùúñ log

ec7

Since (cid:98)‚Ñô{ùê∑ } > 0, the dual objective function for ùúÜ > 0 is upper bounded as
‚à´

ùë£ (ùúÜ) = ùúÜùúå +

ùë£ùë• (ùúÜ) d(cid:98)‚Ñô(ùë•)
‚à´

ùê∑

‚â§ ùêªùë¢ + ùúÜùúå + ùúÜùúñ

log

(cid:16)

ùî•ùë• + ùëí‚àíùêª gap (ùë•)/(ùúÜùúñ) ùî•ùëê
ùë•

(cid:17)

d(cid:98)‚Ñô(ùë•).

We can see that

and

lim
ùúÜ‚Üí0

ùúÜùúå + ùúÜùúñ

‚à´

ùê∑

(cid:16)

log

ùî•ùë• + ùëí‚àíùêª gap (ùë•)/(ùúÜùúñ) ùî•ùëê
ùë•

(cid:17)

d(cid:98)‚Ñô(ùë•) = 0,

lim
ùúÜ‚Üí0

=ùúå + ùúñ

(cid:20)

‚àá

‚à´

ùê∑

ùúÜùúå + ùúÜùúñ

‚à´

ùê∑

(cid:16)

log

ùî•ùë• + ùëí‚àíùêª gap (ùë•)/(ùúÜùúñ) ùî•ùëê
ùë•

(cid:21)

(cid:17)

d(cid:98)‚Ñô(ùë•)

log (ùî•ùë• ) d(cid:98)‚Ñô(ùë•)

‚â§ùúå + ùúñ log(ùúè)(cid:98)‚Ñô{ùê∑ } ‚â§ ‚àíùúå < 0,

where the second inequality is by taking the constant ùúè = exp
that

(cid:16)

‚àí

(cid:17)

2ùúå
ùúñ(cid:98)‚Ñô{ùê∑ }

. Hence, there exists ùúÜ > 0 such

ùë£ (ùúÜ) ‚â§ ùêªùë¢ + ùúÜùúå + ùúÜùúñ

‚à´

ùê∑

(cid:16)

log

ùî•ùë• + ùëí‚àíùêª gap (ùë•)/(ùúÜùúñ) ùî•ùëê
ùë•

(cid:17)

d(cid:98)‚Ñô(ùë•) < ùë£ (0),

which contradicts to the optimality of ùúÜ‚àó = 0. As a result, almost surely for all ùë•, we have that

ùîº‚Ñöùë•,ùúñ [1ùê¥] > 0.

To show the second condition, we re-write the dual objective function for ùúÜ > 0 as

ùë£ (ùúÜ) = ùúÜùúå + ùúÜùúñ

‚à´ (cid:104)

(cid:16)

log

ùîº‚Ñöùë•,ùúñ [1ùê¥] + ùîº‚Ñöùë•,ùúñ

(cid:104)
ùëí [ùëì (ùëß)‚àíùêªùë¢ ]/(ùúÜùúñ) 1ùê¥ùëê

(cid:105) (cid:17)(cid:105)

d(cid:98)‚Ñô(ùë•) + ùêªùë¢ .

The gradient of ùë£ (ùúÜ) becomes

‚àáùë£ (ùúÜ) = ùúå + ùúñ

+

‚à´ (cid:104)

log

‚à´ ùîº‚Ñöùë•,ùúñ

(cid:16)

(cid:104)
ùëí [ùëì (ùëß)‚àíùêªùë¢ ]/(ùúÜùúñ) 1ùê¥ùëê
ùîº‚Ñöùë•,ùúñ [1ùê¥] + ùîº‚Ñöùë•,ùúñ
(cid:2)ùëí [ùëì (ùëß)‚àíùêªùë¢ ]/(ùúÜùúñ) 1ùê¥ùëê (ùêªùë¢ ‚àí ùëì (ùëß))/(ùúÜ)(cid:3)

ùîº‚Ñöùë•,ùúñ [1ùê¥] + ùîº‚Ñöùë•,ùúñ

(cid:2)ùëí [ùëì (ùëß)‚àíùêªùë¢ ]/(ùúÜùúñ) 1ùê¥ùëê (cid:3)

(cid:105) (cid:17)(cid:105)

d(cid:98)‚Ñô(ùë•)

d(cid:98)‚Ñô(ùë•).

We can see that limùúÜ‚Üí‚àû ‚àáùë£ (ùúÜ) = ùúå. Take

ùë£1,ùë• (ùúÜ) = ùîº‚Ñöùë•,ùúñ

(cid:104)
ùëí [ùëì (ùëß)‚àíùêªùë¢ ]/(ùúÜùúñ) 1ùê¥ùëê

(cid:105)

.

Then limùúÜ‚Üí0 ùë£1,ùë• (ùúÜ) = 0 and ùë£1,ùë• (ùúÜ) ‚â• 0. Take

ùë£2,ùë• (ùúÜ) =

ùîº‚Ñöùë•,ùúñ

(cid:2)ùëí [ùëì (ùëß)‚àíùêªùë¢ ]/(ùúÜùúñ) 1ùê¥ùëê (ùêªùë¢ ‚àí ùëì (ùëß))/(ùúÜ)(cid:3)

ùîº‚Ñöùë•,ùúñ [1ùê¥] + ùîº‚Ñöùë•,ùúñ

(cid:2)ùëí [ùëì (ùëß)‚àíùêªùë¢ ]/(ùúÜùúñ) 1ùê¥ùëê (cid:3)

.

Then limùúÜ‚Üí0 ùë£2,ùë• (ùúÜ) = 0 and ùë£2,ùë• (ùúÜ) ‚â• 0. It follows that

‚àáùë£ (ùúÜ) = ùúå + ùúñ

lim
ùúÜ‚Üí0

‚à´

log (cid:0)ùîº‚Ñöùë•,ùúñ [1ùê¥](cid:1) d(cid:98)‚Ñô(ùë•) = ùúå (cid:48).

ec8

Hence, if the last condition is violated, based on the mean value theorem, we can Ô¨Ånd ùúÜ > 0 so that
‚àáùë£ (ùúÜ) = 0, which contradicts to the optimality of ùúÜ‚àó = 0.

Now we show the converse direction. For any ùúÜ > 0, we Ô¨Ånd that

‚àáùë£ (ùúÜ) = ùúå + ùúñ

‚à´

(cid:2)log (cid:0)ùîº‚Ñöùë•,ùúñ [1ùê¥] + ùë£1,ùë• (ùúÜ)(cid:1)(cid:3) d(cid:98)‚Ñô(ùë•) +

‚à´

ùë£2,ùë• (ùúÜ) d(cid:98)‚Ñô(ùë•).

For Ô¨Åxed ùë•, when ùîº‚Ñöùë•,ùúñ [1ùê¥] = 1, we can see that ùë£1,ùë• (ùúÜ) = ùë£2,ùë• (ùúÜ) = 0, then
ùúå + ùúñ (cid:2)log (cid:0)ùîº‚Ñöùë•,ùúñ [1ùê¥] + ùë£1,ùë• (ùúÜ)(cid:1)(cid:3) + ùë£2,ùë• (ùúÜ) = ùúå > 0.

When ùîº‚Ñöùë•,ùúñ [1ùê¥] ‚àà (0, 1), we can see that ùë£1,ùë• (ùúÜ) > 0, ùë£2,ùë• (ùúÜ) > 0. Then

ùúå + ùúñ (cid:2)log (cid:0)ùîº‚Ñöùë•,ùúñ [1ùê¥] + ùë£1,ùë• (ùúÜ)(cid:1)(cid:3) + ùë£2,ùë• (ùúÜ) > ùúå + ùúñ log(ùîº‚Ñöùë•,ùúñ [1ùê¥]) = ùúå (cid:48) ‚â• 0.

Therefore, ‚àáùë£ (ùúÜ) > 0 for any ùúÜ > 0. By the convexity of ùë£ (ùúÜ), we conclude that the dual minimizer
ùúÜ‚àó = 0.

(cid:3)

Proof of Lemma 4. Since ùúÜ‚àó > 0, based on the optimality condition of the dual problem, we have

that

0 = ‚àáùúÜ

(cid:20)

ùúÜùúå + ùúÜùúñ

‚à´

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

(cid:104)
ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)

d(cid:98)‚Ñô(ùë•)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)ùúÜ=ùúÜ‚àó

.

Or equivalently, we have that

‚à´

ùúå+ùúñ

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•) ‚àí

Re-arranging the term completes the proof.

‚à´ ùîº‚Ñöùë•,ùúñ

(cid:2)ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) ùëì (ùëß)(cid:3)
(cid:2)ùëí ùëì (ùëß)/(ùúÜ‚àóùúñ) (cid:3)

ùúÜ‚àóùîº‚Ñöùë•,ùúñ

d(cid:98)‚Ñô(ùë•) = 0.

(cid:3)

Proof of Theorem 1. The feasibility result in Theorem 1(I) can be easily shown by considering the

reformulation of ùëâ in (3) and the non-negativity of KL-divergence. When ùúå = 0, one can see that

ùëâD ‚â§ lim
ùúÜ‚Üí‚àû

ùúÜùúñ

‚à´

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•) = ùîºùëß‚àº‚Ñô0 [ùëì (ùëß)] = ùëâ.

Therefore, the strong duality result holds in this case. The proof for ùúå > 0 can be found in the main
context. It remains to show the second part of Theorem 1(III). We consider a sequence of real numbers
{ùëÖ ùëó } ùëó such that ùëÖ ùëó ‚Üí ‚àû and take the objective function ùëìùëó (ùëß) = ùëì (ùëß)1{ùëß ‚â§ ùëÖ ùëó }. Hence, there exists ùúÜ > 0
(cid:2)ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:3) = ‚àû(cid:9) = 0. According to the necessary condition in Lemma 3, the
satisfying Pr ùë•‚àº(cid:98)‚Ñô
ùëó > 0 for suÔ¨Éciently large index ùëó. Then we can apply the duality result
corresponding dual minimizer ùúÜ‚àó
in the Ô¨Årst part of Theorem 1(III) to show that for suÔ¨Éciently large ùëó, it holds that

(cid:8)ùë• : ùîº‚Ñöùë•,ùúñ

(cid:8)ùîºùëß‚àº‚Ñô [ùëìùëó (ùëß)](cid:9) ‚â• ùúÜ‚àó

ùëó ùúå + ùúÜ‚àó
ùëó ùúñ

‚à´

(cid:16)

log

ùîº‚Ñöùë•,ùúñ

ùëí ùëìùëó (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

d(cid:98)‚Ñô(ùë•).

sup
‚Ñô‚ààùîπùúå,ùúñ ((cid:98)‚Ñô)

Taking ùëó ‚Üí ‚àû both sides implies that ùëâ = ‚àû, which completes the proof.

(cid:3)

Appendix EC.5: Proofs of Technical Results in Section 4

ec9

Proof of Proposition 1 For any Ô¨Åxed ùúÜ0 > 0, denote by ùúÉ0 the optimal solution to problem (10). We
at ùúÜ = ùúÜ0. For any ùúÜ > 0, let ùúÉ (ùúÜ) be

can argue that for any ùëé ‚àà ùúï
ùúïùúÜ
the optimal solution for ÀÜùêπ (ùëö)

ÀÜùêπ (ùëö) (ùúÜ0, ùúÉ0), ùëé is a subgradient of ÀÜùêπ (ùëö)
. Then we can see

ùúÜ

ùúÜ

ÀÜùêπ (ùëö)
ùúÜ

= ÀÜùêπ (ùëö) (ùúÜ, ùúÉ (ùúÜ)) ‚â• ÀÜùêπ (ùëö) (ùúÜ0, ùúÉ0) + ùëé(ùúÜ ‚àí ùúÜ0) + (cid:104)‚àá ÀÜùêπ (ùëö) (ùúÜ0, ùúÉ0), ùúÉ (ùúÜ) ‚àí ùúÉ0(cid:105)

‚â• ÀÜùêπ (ùëö) (ùúÜ0, ùúÉ0) + ùëé(ùúÜ ‚àí ùúÜ0),

where ‚àá ÀÜùêπ (ùëö) (ùúÜ0, ùúÉ0) denotes the subdiÔ¨Äerential of ÀÜùêπ (ùëö) with respect to ùúÉ = ùúÉ0, the Ô¨Årst inequality is
by the convexity of ÀÜùêπ (ùëö) (ùúÜ, ùúÉ ) with respect to (ùúÜ, ùúÉ ), and the second inequality is by the optimality
condition for ùúÉ0.

, which means that ùúÜ0 is the minimizer of ÀÜùêπ (ùëö)
When ùëé = 0, we immediately obtain that 0 ‚àà ùúï ÀÜùêπ (ùëö)
.
ùúÜ0
Otherwise, the algorithm will update the interval so that ùëé(ùúÜ ‚àí ùúÜ0) ‚â§ 0 for any ùúÜ within it. We claim
that this interval will contain ùúÜ‚àó. Suppose on the contrary that ùëé(ùúÜ‚àó ‚àí ùúÜ0) > 0. By the convexity of ÀÜùêπ (ùëö)
,

ùúÜ

¬∑

ÀÜùêπ (ùëö)
ùúÜ‚àó

‚â• ÀÜùêπ (ùëö)
ùúÜ0

+ ùëé(ùúÜ‚àó ‚àí ùúÜ0) > ÀÜùêπ (ùëö)
ùúÜ0

,

which contradicts to the optimality of ùúÜ‚àó. As a result, the interval length ùëôùë° = ùúÜùë¢ ‚àí ùúÜùëô at the ùë°-th iteration
in Algorithm 1 vanishes at the rate ùëôùë° = (1/2)ùë°ùëô0, which indicates that the algorithm will converge into
the optimal solution of infùúÜ ‚â•0 ÀÜùêπ (ùëö)
(cid:3)

linearly.

ùúÜ

Proof of Proposition 2. For notational simplicity, we write ùë† = (ùúÜ, ùúÉ ) and Œû = ‚Ñù+ √ó Œò. We Ô¨Årst show

the second part of this theorem. To begin with, we introduce the following functions:

Àúùêπùëö (ùë†) = ÀÜùêπ (ùëö) (ùë†) + ùúèŒû(ùë†),

¬Øùêπ (ùë†) = ùêπ (ùë†) + ùúèŒû(ùë†).

We build the pointwise law of large numbers (LLN) for Àúùêπùëö. By the strong law of large numbers (LLN),
for each ùëñ and every (ùúÜ, ùúÉ ) ‚àà Œû,

(cid:104)
ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) a.s.

‚àí‚àí‚Üí ùîº‚Ñöùëñ,ùúñ

(cid:104)
ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105)

.

ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ

Then by the continuous mapping theorem, for each ùëñ and every (ùúÜ, ùúÉ ) ‚àà Œû,

ùúÜùúñ log

(cid:18)

ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ

ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:19) a.s.
(cid:104)

‚àí‚àí‚Üí ùúÜùúñ log

(cid:16)

ùîº‚Ñöùëñ,ùúñ

(cid:104)
ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)

.

Therefore, using the addition and scalar multiplication rule of almost sure convergence, for every
(ùúÜ, ùúÉ ) ‚àà Œû, it holds that

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùúÜùúñ log

(cid:18)

ùîº ÀÜ‚Ñöùëö
ÀÜùë•ùëñ

ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:19) a.s.
(cid:104)
‚àí‚àí‚Üí

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùúÜùúñ log

(cid:16)

ùîº‚Ñöùëñ,ùúñ

ùëí ùëìùúÉ (ùëß)/(ùúÜùúñ) (cid:105) (cid:17)
(cid:104)

,

which reveals that ÀÜùêπ (ùëö) (ùë†)
Àúùêπùëö (ùë†).

a.s.
‚àí‚àí‚Üí ùêπ (ùë†) for each ùë† ‚àà Œû. This further implies the corresponding LLN for

Now take a compact subset ùê∂ so that S ‚àó is contained in the interior of ùê∂. Such a set exists because
S ‚àó is bounded. Denote by ÀúSùëö the set of minimizers of Àúùêπùëö over ùê∂. By the lower semi-continuity, together
with the pointwise LLN of Àúùêπùëö, we Ô¨Ånd Àúùêπùëö is Ô¨Ånite-valued on S ‚àó for large ùëö, which implies that the set
ÀúSùëö is non-empty for large ùëö. We show that Dist( ÀúSùëö, S ‚àó) ‚Üí 0 almost surely. Let ùúî = {ùëßùëñ,ùëó }ùëñ,ùëó be such that
ùëí
Àúùêπùëö (¬∑, ùúî)
‚àí‚Üí ¬Øùêπ (¬∑). This event holds almost surely for all ùúî. Suppose on the contrary that for any ùëö, there
exists a minimizer Àúùë†ùëö (ùúî) of Àúùêπùëö over ùê∂ such that Dist(Àúùë†ùëö, S ‚àó) ‚â• ùúÄ. Due to the compactness of ùê∂, there

ec10

exists a sub-sequence {Àúùë†ùëö ùëó } ùëó that converges into a point ùë†‚àó ‚àà ùê∂, but ùë†‚àó ‚àâ S ‚àó. On the other hand, we can
¬Øùêπ (ùë†) = S ‚àó by applying [86, Proposition 7.26]. Then we obtain a contradiction.
argue that ùë†‚àó ‚àà arg minùë† ‚ààùê∂
Then we show that ÀúSùëö = S (ùëö) for large ùëö. Because of the convexity assumption, any minimizer
of Àúùêπùëö over ùê∂ which lies inside of the interior of ùê∂, is also an optimal solution to the problem 9.
Hence, for large ùëö we have that ÀúSùëö = S (ùëö) . This, together with the fact that Dist( ÀúSùëö, S ‚àó) ‚Üí 0 implies
Dist(S (ùëö), S ‚àó) ‚Üí 0. Moreover, it suÔ¨Éces to restrict the feasible set into the compact set ùê∂ ‚à© Œû. By the
ùëé.ùë†.
convexity of ùêπ (ùë†) in ùë†, ÀÜùêπ (ùëö)
‚àí‚àí‚Üí ùêπ holds uniformly on ùê∂ ‚à© Œû. As a consequence, the Ô¨Årst part of this
(cid:3)
proposition can be proved by applying [86, Proposition 5.2].

Appendix EC.6: Proof of the Technical Result in Appendix A
We Ô¨Årst present an useful technical lemma before showing Proposition 3.

Lemma EC.2. Under the Ô¨Årst condition of Proposition 3, for any ùë• ‚àà Z, it holds that
ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) ‚â• ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùë•)/ùúñ ‚à´

ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùëß)/ùúñ dùúà (ùëß).

‚à´

Proof of Lemma EC.2 Based on the inequality (ùëé + ùëè)ùëù ‚â§ 2ùëù‚àí1(ùëéùëù + ùëèùëù), we can see that

ùëê (ùë•, ùëß) ‚â§ (ùëê (ùë¶, ùëß)1/ùëù + ùëê (ùëß,ùë¶)1/ùëù)ùëù ‚â§ 2ùëù‚àí1(ùëê (ùë¶, ùëß) + ùëê (ùëß,ùë¶)), ‚àÄùë•,ùë¶, ùëß ‚àà Z.

Since ùëê (ùë•, ùëß) ‚â§ 2ùëù‚àí1(ùëê (ùë•, ùëß) + ùëê (ùë•, ùë•)), we can see that

‚à´

ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) ‚â• exp

(cid:16)

‚àí2ùëù‚àí1ùëê (ùë•, ùë•)/ùúñ

(cid:17) ‚à´

ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùëß)/ùúñ dùúà (ùëß).

The proof is completed.

Proof of Proposition 3 One can see that for any ùë• ‚àà supp((cid:98)‚Ñô), it holds that

ec11

(cid:3)

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105)
(cid:104)

ùîº‚Ñöùë•,ùúñ
‚à´

‚à´

‚à´

=

‚â§

‚â§

=

ùëí ùëì (ùëß)/(ùúÜùúñ)

ùëí‚àíùëê (ùë•,ùëß)/ùúñ
‚à´ ùëí‚àíùëê (ùë•,ùë¢)/ùúñ dùúà (ùë¢)

dùúà (ùëß)

ùëí ùëì (ùëß)/(ùúÜùúñ)

ùëí‚àíùëê (ùë•,ùëß)/ùúñ
‚à´ ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùëß)/ùúñ dùúà (ùëß).
ùëí ùëì (ùëß)/(ùúÜùúñ) ùëí‚àí21‚àíùëùùëê (ùë•,ùëß)/ùúñùëíùëê (ùë•,ùë•)/ùúñ
‚à´ ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùëß)/ùúñ dùúà (ùëß)
‚à´

dùúà (ùëß)

dùúà (ùëß)

ùëíùëê (ùë•,ùë•) (1+2ùëù‚àí1)/ùúñ
‚à´ ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùëß)/ùúñ dùúà (ùëß)

ùëí ùëì (ùëß)/(ùúÜùúñ)ùëí‚àí21‚àíùëùùëê (ùë•,ùëß)/ùúñ dùúà (ùëß),

where the Ô¨Årst inequality is based on the lower bound in Lemma EC.2, the second inequality is based
on the triangular inequality ùëê (ùë•, ùëß) ‚â• 21‚àíùëùùëê (ùë•, ùëß) ‚àí ùëê (ùë•, ùë•). Note that almost surely for all ùë• ‚àà supp((cid:98)‚Ñô),
ùëê (ùë•, ùë•) < ‚àû. Moreover,

‚à´

0 <

ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) ‚â§

‚à´

ùëí‚àíùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) < ‚àû,

where the lower bound is because ùëê (ùë•, ùëß) < ‚àû almost surely for all ùëß, the upper bound is because
ùëê (ùë•, ùëß) ‚â• 0 almost surely for all ùëß. Based on these observations, we have that

ùëí ùëì (ùëß)/(ùúÜùúñ) (cid:105)
(cid:104)

‚â§

ùîº‚Ñöùë•,ùúñ

ùëíùëê (ùë•,ùë•) (1+2ùëù‚àí1)/ùúñ
‚à´ ùëí‚àí2ùëù‚àí1ùëê (ùë•,ùëß)/ùúñ dùúà (ùëß)

‚à´

ùëí ùëì (ùëß)/(ùúÜùúñ)ùëí‚àí21‚àíùëùùëê (ùë•,ùëß)/ùúñ dùúà (ùëß) < ‚àû

almost surely for all ùë• ‚àº (cid:98)‚Ñô.

(cid:3)

