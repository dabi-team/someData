1
2
0
2

p
e
S
4
2

]

C
O
.
h
t
a
m

[

1
v
6
2
9
1
1
.
9
0
1
2
:
v
i
X
r
a

Sinkhorn Distributionally Robust Optimization

Jie Wang
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA 30332, jwang3163@gatech.edu

Rui Gao
Department of Information, Risk, and Operations Management, University of Texas at Austin, Austin, TX 78712,
rui.gao@mccombs.utexas.edu

Yao Xie
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA 30332, yao.xie@isye.gatech.edu

We study distributionally robust optimization with Sinkorn distanceâ€”a variant of Wasserstein distance based on
entropic regularization. We derive convex programming dual reformulations when the nominal distribution is an
empirical distribution and a general distribution, respectively. Compared with Wasserstein DRO, it is computationally
tractable for a larger class of loss functions, and its worst-case distribution is more reasonable. To solve the dual
reformulation, we propose an eï¬ƒcient batch gradient descent with a bisection search algorithm. Finally, we provide
various numerical examples using both synthetic and real data to demonstrate its competitive performance.

Key words: Wasserstein distributionally robust optimization, Sinkhorn distance, Duality theory

1. Introduction

Decision-making problems under uncertainty have broad applications in operations research, machine
learning, engineering, and economics. When the data involves uncertainty due to measurement error,
insuï¬ƒcient sample size, contamination, and anomalies, or model misspeciï¬cation, distributionally
robust optimization (DRO) is a promising approach to data-driven optimization, by seeking a minimax
robust optimal decision that minimizes the expected loss under the most adverse distribution within
a given set of relevant distributions, called ambiguity set. It provides a principled framework to
produce a solution with more promising out-of-sample performance than the traditional sample average
approximation (SAA) method for stochastic programming [86]. We refer to [81] for a recent survey on
DRO.

At the core of DRO is the choice of the ambiguity set. Ideally, a good ambiguity set should take
account of the properties of practical applications while maintaining the computational tractability
of resulted DRO formulation; and it should be rich enough to contain all distributions relevant to
the decision-making but, at the same time, should not include unnecessary distributions that lead
to overly conservative decisions. Various DRO formulations have been proposed in the literature.
Among them, the ambiguity set based on Wasserstein distance has recently received much attention
[104, 67, 17, 46]. The Wasserstein distance incorporates the geometry of sample space, and thereby
is suitable for comparing distributions with non-overlapping supports and hedging against data
perturbations [46]. Nice statistical performance guarantees have been established for Wasserstein
DRO both asymptotically [16, 19, 18], non-asymptotically [44, 24, 84], and empirically in a variety of
applications in operations research [13, 30, 87, 73, 88, 100], machine learning [85, 25, 66, 14, 72, 93],
stochastic control [106, 1, 91, 38, 107, 99], etc; see [61] and references therein for more discussions.
We also provide a more detailed literature survey by the end of this section.

On the other hand, the current Wasserstein DRO framework is not without limitation. First, from the
computational eï¬ƒciency perspective, the tractability of Wasserstein DRO is usually available only under
somewhat stringent conditions on the loss function, as its dual formulation involves a subproblem that
requires the global supremum of some regularized loss function over the sample space. In particular,
for 1-Wasserstein DRO, a convex reformulation is only known when the loss function can be expressed

1

 
 
 
 
 
 
2

as a pointwise maximum of ï¬nitely many concave functions [67] and eï¬ƒcient ï¬rst-order algorithm is
proposed only for special loss functions such as logistic loss [62]; and for 2-Wasserstein DRO, eï¬ƒcient
ï¬rst-order algorithms have been developed only for smooth loss functions and suï¬ƒciently small radius
(or equivalently, suï¬ƒciently large Lagrangian multiplier) so that the involved subproblem becomes
strongly convex [89, 20, 63, 26]. Second, from the modeling perspective, for data-driven Wasserstein
DRO in which the nominal distribution is ï¬nitely supported (usually the empirical distribution),
the worst-case distribution is shown to be a discrete distribution [46], despite that the underlying
true distribution in many practical applications may well be continuous. This raises the concern of
whether Wasserstein DRO hedges the right family of distribution and whether it causes potentially
over-conservative performance.

To address these potential issues while maintaining the advantages of Wasserstein DRO, in this
paper, we propose Sinkhorn DRO, which hedges against distributions that are close to some nominal
distribution in Sinkhorn distance [35]. The Sinkhorn distance can be viewed as a smoothed Wasserstein
distance, deï¬ned as the cheapest transport cost between two distributions associated with an optimal
transport problem with entropic regularization (see Deï¬nition 1 in Section 2). As far as we know, this
paper is the ï¬rst to study the DRO formulation using the Sinkhorn distance. Our main contributions
are summarized as follows.

(I) We derive a strong duality reformulation for Sinkhorn DRO (Theorem 1), both when the
nominal distribution is a data-driven empirical distribution (Section 3.2) and when the nominal
distribution is any arbitrary distribution (Section 3.3). The Sinkhorn dual objective smooths
the maximization subproblem in the Wasserstein dual objective, and converges to Wasserstein
dual objective as the entropic regularization parameter goes to zero (Remark 3). Moreover, the
dual objective of Sinkhorn DRO is upper bounded by that of the KL-divergence DRO with the
nominal distribution being a kernel density estimator (Remark 4).

(II) As a byproduct of our duality proof, we characterize the worst-case distribution of the Sinkhorn
DRO (Remark 5), which is absolutely continuous with respect to some reference measure such
as Lebesgue or counting measure. Compared with Wasserstein DRO, the worst-case distribution
of Sinkhorn DRO is not necessarily ï¬nitely supported even when the nominal distribution is a
ï¬nitely supported distribution. This indicates that Sinkhorn DRO is a more ï¬‚exible modeling
choice for many applications.

(III) On the algorithmic aspect, we propose a computationally eï¬ƒcient ï¬rst-order method for solving
the Sinkhorn DRO problem (Section 4), based on batch gradient descent and bisection search.
Its convergence guarantees are also developed. Compared with Wasserstein DRO, the dual
problem of Sinkhorn DRO is computationally tractable for any measurable loss functions.
(IV) We provide experiments (Section 5) to validate the performance of the proposed Sinkhorn
DRO model in the context of newsvendor problem, mean-risk portfolio optimization, and
semi-supervised learning, using both synthetic and real data sets. Numerical results demonstrate
its superior out-of-sample performances compared with several benchmarks including SAA,
Wasserstein DRO, and KL-divergence DRO.

Related Literature

On DRO Models Construction of ambiguity sets plays a key role in the performance of DRO models.
Generally, there are two ways to construct ambiguity sets in literature. First, ambiguity sets can be
deï¬ned using descriptive statistics, such as the support information [11], moment conditions [83,
36, 52, 112, 103, 29, 12], shape constraints [80, 94], marginal distributions [43, 69, 2, 39]. Second, a
more recently popular approach that makes full use of the available data is to consider distributions
within a pre-speciï¬ed statistical distance from a nominal distribution, usually chosen as the empirical
distribution of samples. Commonly used statistical distances used in literature include ğœ™-divergence [57,
10, 102, 9, 41], Wasserstein distance [78, 104, 67, 110, 17, 46, 28, 105], and maximum mean discrepancy

3

[92, 111]. Our proposed Sinkhorn DRO can be viewed as a variant of Wasserstein DRO. In the
literature on Wasserstein DRO, besides the computational tractability, its regularization eï¬€ects and
statistical inference have also be investigated. In particular, it has been shown that Wasserstein DRO is
asymptotically equivalent to a statistical learning problem with variation regularization [45, 16, 84],
and when the radius is chosen properly, the worst-case loss of Wasserstein DRO serves as an upper
conï¬dence bound on the true loss [16, 19, 44, 18] . Other variants of Wasserstein DRO have been
explored, by combining with other information such as moment information [48, 96] and marginal
distributions [47, 42].

Finally, we remark that a recent work [42] on distributionally robust optimization with given
marginals share a somewhat similar spirit as our work. They start from a dual formulation and propose
to replace its supremum subproblem with a smooth penalization, and then dualize the dual problem to
obtain a primal problem that penalizes the entropy of the distribution. The main diï¬€erences between
their formulation and ours are that: (i) we do not impose marginal distribution constraints in the
primal formulation; (ii) our entropic regularization is on the transport plan (joint distribution) between
the nominal distribution and a candidate distribution in the ambiguity set, but their entropic penalty
is imposed only on the candidate distributions in the ambiguity set; (iii) our dual formulation smooths
the supremum subproblem by log-exp-sum function, which is not covered in their considered family of
penalizations.

On Sinkhorn Distance Sinkhorn distance [35] is proposed to improve the computational com-
plexity of Wasserstein distance by regularizing the original mass transportation problem with relative
entropy penalty on the transport mapping. In particular, this distance can be computed from its dual
form by optimizing two blocks of decision variables alternatively, which only requires simple matrix-
vector products and therefore signiï¬cantly improves the computation speed [77]. Such an approach
ï¬rst aroused in the areas of economics and survey statistics [60, 109, 37, 7], and its convergence
analysis is attributed to the mathematician Sinkhorn [90], which gives the name of Sinkhorn distance.
A recent work [3] further designs an accelerated algorithm to compute Sinkhorn distance in near-linear
time. Using Sinkhorn distance other than Wasserstein distance has been demonstrated to be beneï¬cial
because of lower computational cost in various applications, including domain adaptations [32, 33, 31],
generative modeling [50, 76, 65, 75], dimensionality reduction [64, 97, 98, 58], etc. To the best of
our knowledge, the study of Sinkhorn distance for distributionally robust optimization is new in the
literature.

The rest of the paper is organized as follows. In Section 2, we describe the main formulation for
the Sinkhorn DRO model. In Section 3, we develop its strong dual reformulation. In Section 4, we
propose a ï¬rst-order optimization algorithm that solves the reformulation eï¬ƒciently. We report several
numerical results in Section 5, and conclude the paper in Section 6. All omitted proofs can be found in
Appendix.

2. Model Setup

Notation. Assume that the logarithm function log is taken with base ğ‘’. For a positive integer ğ‘ ,
we write [ğ‘ ] for {1, 2, . . . , ğ‘ }. For a measurable set Z, denote by M(Z) the set of measures (not
necessarily probability measures) on Z, and P (Z) the set of probability measures on Z. Given a
probability distribution â„™ and a measure ğœ‡, we denote supp(â„™) the support of â„™, and write â„™ (cid:28) ğœ‡
if â„™ is absolutely continuous with respect to ğœ‡. For a given element ğ‘¥, denote by ğ›¿ğ‘¥ the one-point
probability distribution supported on {ğ‘¥ }. Denote â„™ âŠ— â„š as the product measure of two probability
distributions â„™ and â„š. Denote by Proj1#ğ›¾ and Proj2#ğ›¾ the ï¬rst and the second marginal distributions
of ğ›¾, respectively. For a given set ğ´, deï¬ne the characteristic function 1ğ´ (ğ‘¥) such that 1ğ´ (ğ‘¥) = 1 when
ğ‘¥ âˆˆ ğ´ and otherwise 1ğ´ (ğ‘¥) = 0, and deï¬ne the indicator function ğœğ´ (ğ‘¥) such that ğœğ´ (ğ‘¥) = 0 when ğ‘¥ âˆˆ ğ´

4

and otherwise ğœğ´ (ğ‘¥) = âˆ. Deï¬ne the distance between two sets ğ´ and ğµ in the Euclidean space as
Dist(ğ´, ğµ) = supğ‘¥ âˆˆğ´ inf ğ‘¦ âˆˆğµ (cid:107)ğ‘¥ âˆ’ ğ‘¦ (cid:107)2.

We ï¬rst review the deï¬nition of Sinkhorn distance.

Definition 1 (Sinkhorn Distance). Let Z be a measurable set. Consider distributions â„™, â„š âˆˆ P (Z),
and let ğœ‡, ğœˆ âˆˆ M(Z) be two reference measures such that â„™ (cid:28) ğœ‡, â„š (cid:28) ğœˆ. For regularization parameter
ğœ– â‰¥ 0, the Sinkhorn distance between two distributions â„™ and â„š is deï¬ned as

Wğœ– (â„™, â„š) = inf

ğ›¾ âˆˆÎ“ (â„™,â„š)

(cid:8)ğ”¼(ğ‘‹ ,ğ‘Œ )âˆ¼ğ›¾ [ğ‘ (ğ‘‹, ğ‘Œ )] + ğœ–ğ» (ğ›¾ | ğœ‡ âŠ— ğœˆ)(cid:9) ,

where Î“(â„™, â„š) denotes the set of joint distributions whose ï¬rst and second marginal distributions are
â„™ and â„š respectively, ğ‘ (ğ‘¥,ğ‘¦) denotes the cost function, and ğ» (ğ›¾ | ğœ‡ âŠ— ğœˆ) denotes the relative entropy of
ğ›¾ with respect to the product measure ğœ‡ âŠ— ğœˆ:

ğ» (ğ›¾ | ğœ‡ âŠ— ğœˆ) =

âˆ«

log

(cid:18) dğ›¾ (ğ‘¥,ğ‘¦)

(cid:19)

dğœ‡ (ğ‘¥) dğœˆ (ğ‘¦)

dğ›¾ (ğ‘¥,ğ‘¦).

Remark 1 (Variants of Sinkhorn Distance). Sinkhorn distance in Deï¬nition 1 is based on general
reference measures ğœ‡ and ğœˆ. Special forms of the distance has been investigated in the literature, for
instance, when the reference measures ğœ‡ and ğœˆ were chosen to be â„™, â„š, i.e., marginal distributions
of ğ›¾, respectively [49, Section 2]. The relative entropy regularization term can also be considered
as a hard-constrained variant for the optimal transport problem, which has been discussed in [35,
Deï¬nition 1] and [8]:

â™¦

W Info
ğ‘…

(â„™, â„š) = inf

ğ›¾ âˆˆÎ“ (â„™,â„š)

(cid:8)ğ”¼(ğ‘‹ ,ğ‘Œ )âˆ¼ğ›¾ [ğ‘ (ğ‘‹, ğ‘Œ )] : ğ» (ğ›¾ | â„™ âŠ— â„š) â‰¤ ğ‘…(cid:9) ,

where ğ‘… â‰¥ 0 quantiï¬es the upper bound for the relative entropy between distributions ğ›¾ and â„™ âŠ— â„š. An-
other variant of the optimal transport problem is to consider the negative entropy for regularization [35,
Equation (2)]:

W Ent
ğœ–

(â„™, â„š) = inf

ğ›¾ âˆˆÎ“ (â„™,â„š)

(cid:8)ğ”¼(ğ‘‹ ,ğ‘Œ )âˆ¼ğ›¾ [ğ‘ (ğ‘‹, ğ‘Œ )] + ğœ–ğ» (ğ›¾)(cid:9) ,

where ğ» (ğ›¾) = âˆ« log
dğ›¾ (ğ‘¥,ğ‘¦) and dğ‘¥, dğ‘¦ are Lebesgue measures if the corresponding marginal
distributions are continuous, or counting measures if the marginal distributions are discrete. For given
â„™ and â„š, the two regularized optimal transport distances above are equivalent up to a constant.
â™£

(cid:16) dğ›¾ (ğ‘¥,ğ‘¦)
dğ‘¥ dğ‘¦

(cid:17)

In this paper, we study the Sinkhorn DRO model. Given a loss function ğ‘“ , a nominal distribution (cid:98)â„™
and the Sinkhorn radius ğœŒ, the primal form of the worst-case expectation problem of Sinkhorn DRO is
given by

ğ‘‰ := sup

ğ”¼ğ‘§âˆ¼â„™ [ğ‘“ (ğ‘§)],

â„™âˆˆğ”¹ğœŒ,ğœ– ((cid:98)â„™)

(Sinkhorn DRO)

where ğ”¹ğœŒ,ğœ– ((cid:98)â„™) = (cid:8)â„™ : Wğœ– ((cid:98)â„™, â„™) â‰¤ ğœŒ(cid:9),
where ğ”¹ğœŒ,ğœ– ((cid:98)â„™) is the Sinkhorn ball of the radius ğœŒ centered at the nominal distribution (cid:98)â„™. Due to the
convex entropic regularizer [34] in Wğœ– ((cid:98)â„™, â„™), the Sinkhorn distance Wğœ– ((cid:98)â„™, â„™) is convex in â„™ and the
Sinkhorn ball is a convex set. Therefore, the problem (Sinkhorn DRO) is an (inï¬nite-dimensional)
convex program.

Remark 2 (Choice of Reference Measures). We discuss below the choices of the two references
measures ğœ‡ and ğœˆ in Deï¬nition 1.

For the reference measure ğœ‡, observe from the deï¬nition of relative entropy and the law of probability,

we can see that the regularization term in Wğœ– ((cid:98)â„™, â„™) can be written as

5

ğ» (ğ›¾ | ğœ‡ âŠ— ğœˆ) =

âˆ«

log

âˆ«

=

log

(cid:32)

(cid:32)

(cid:33)

(cid:33)

dğ›¾ (ğ‘¥,ğ‘¦)
d(cid:98)â„™(ğ‘¥) dğœˆ (ğ‘¦)
dğ›¾ (ğ‘¥,ğ‘¦)
d(cid:98)â„™(ğ‘¥) dğœˆ (ğ‘¦)

log

(cid:32)

(cid:33)

(cid:98)â„™(ğ‘¥)
dğœ‡ (ğ‘¥)

dğ›¾ (ğ‘¥,ğ‘¦)

dğ›¾ (ğ‘¥,ğ‘¦) +

âˆ«

log

(cid:32)

(cid:33)

(cid:98)â„™(ğ‘¥)
dğœ‡ (ğ‘¥)

d(cid:98)â„™(ğ‘¥).

Therefore, any choice of the reference measure ğœ‡ satisfying (cid:98)â„™ (cid:28) ğœ‡ is equivalent up to a constant. For
simplicity, in the sequel we will take ğœ‡ = (cid:98)â„™.

For the reference measure ğœˆ, observe that the worst-case solution â„™ in (Sinkhorn DRO) should satisfy
that â„™ (cid:28) ğœˆ since otherwise the entropic regularization in Deï¬nition 1 is undeï¬ned. As a consequence,
we can choose ğœˆ such that the underlying true distribution is absolutely continuous with respect to it.
Typical choices include the Lebesgue measure or Gaussian measure for continuous random variables,
and counting measure for discrete measures. See [79, Section 3.6] for the construction of a general
reference measure.
â™£

In the following sections, we ï¬rst derive the tractable formulation of the Sinkhorn DRO model and
then develop an eï¬ƒcient ï¬rst-order method to solve it. Finally, we examine its performance by several
numerical examples.

3. Strong Duality Reformulation

Problem (Sinkhorn DRO) is an inï¬nite-dimensional optimization problem over probability distributions.
To obtain a more tractable form, in this section, we derive a strong duality result for (Sinkhorn DRO).

Our main goal is to derive the strong dual problem

ğ‘‰D := inf
ğœ† â‰¥0

(cid:26)

ğœ†ğœŒ + ğœ†ğœ–

âˆ«

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

(cid:27)

,

d(cid:98)â„™(ğ‘¥)

(Dual)

where the dual decision variable ğœ† corresponds to the Sinkhorn distance constraint in (Sinkhorn DRO),
and by convention we deï¬ne the dual objective evaluated at ğœ† = 0 as the limit of the objective values
with ğœ† â†“ 0, which equals the essential supremum of the objective function with respect to the measure
ğœˆ; and we deï¬ne the constant

ğœŒ := ğœŒ + ğœ–

âˆ«

(cid:18)âˆ«

log

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§)

(cid:19)

d(cid:98)â„™(ğ‘¥),

and the kernel probability distribution

dâ„šğ‘¥,ğœ– (ğ‘§) :=

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ–
âˆ« ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘¢)/ğœ– dğœˆ (ğ‘¢)

dğœˆ (ğ‘§).

(1)

(2)

To make the above primal (Sinkhorn DRO) and dual (Dual) problems well-deï¬ned, we introduce

the following assumptions on the cost function ğ‘, the reference measure ğœˆ, and the loss function ğ‘“ .

Assumption 1.

(I) ğœˆ {ğ‘§ : 0 â‰¤ ğ‘ (ğ‘¥, ğ‘§) < âˆ} = 1 for (cid:98)â„™-almost every ğ‘¥;

(II) âˆ« ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) < âˆ for (cid:98)â„™-almost every ğ‘¥;
(III) Z is a measurable space, and the function ğ‘“ : Z â†’ â„ âˆª {âˆ} is measurable.

6

Assumption 1(I) ensures that the Sinkhorn distance is well-deï¬ned. If Assumption 1(II) is not satisï¬ed,
then the Sinkhorn ball ğ”¹ğœŒ,ğœ– ((cid:98)â„™) = P (Z) and the problem (Sinkhorn DRO) has a simple optimal value
ğ‘‰ = supğ‘§ âˆˆZ
ğ‘“ (ğ‘§). Assumption 1(III) ensures the expected loss ğ”¼ğ‘§âˆ¼â„™ [ğ‘“ (ğ‘§)] to be well-deï¬ned and lower
bounded for any distribution â„™. In Appendix A, we present suï¬ƒcient conditions for Assumption 1 that
are easy to verify.

To distinguish the cases ğ‘‰ğ· < âˆ and ğ‘‰ğ· = âˆ, we introduce the following light-tail condition on ğ‘“ .

Condition 1. There exists ğœ† > 0 such that ğ”¼â„šğ‘¥,ğœ– [ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) ] < âˆ for (cid:98)â„™-almost every ğ‘¥.

Our main result in this section is as follows.

Theorem 1 (Strong Duality). Let (cid:98)â„™ âˆˆ P (Z). Assume Assumption 1 is in force. Then the following holds:

(I) The primal problem (Sinkhorn DRO) is feasible if and only if ğœŒ â‰¥ 0;
(II) Whenever ğœŒ â‰¥ 0, it holds that ğ‘‰ = ğ‘‰ğ· .
(III) If, in addition, Condition 1 holds, then ğ‘‰ = ğ‘‰D < âˆ; otherwise ğ‘‰ = ğ‘‰D = âˆ.

We remark that if ğœŒ < 0, by convention, ğ‘‰ = âˆ’âˆ and ğ‘‰ğ· = âˆ’âˆ as well by Lemma 2 in Section 3.3 below.
Therefore, we have ğ‘‰ = ğ‘‰ğ· as long as Assumption 1 holds.

3.1. Discussions

Before we present the proof of Theorem 1, we would like to make several remarks.

Remark 3 (Connection with Wasserstein DRO). As the regularization parameter ğœ– â†’ 0, the dual
objective of the Sinkhorn DRO converges to the dual formulation of the Wasserstein DRO problem [46,
Theorem 1]

âˆ«

ğœ†ğœŒ +

(cid:8)ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§)(cid:9) d(cid:98)â„™(ğ‘¥).

sup
ğ‘§

The proof is given in Appendix EC.3, which essentially follows from the fact that the log-sum-exp
function is a smooth approximation of the supremum. There are several advantages of Sinkhorn DRO.

(I) As we will demonstrate in Section 4, Sinkhorn DRO is tractable for a large class of loss functions.
For the empirical nominal distribution, the worst-case loss can be evaluated eï¬ƒciently for any
measurable loss function ğ‘“ . In contrast, the main computational diï¬ƒculty in Wasserstein DRO
is to solve the maximization problem inside the integration above. In fact, 1-Wasserstein DRO is
shown to be tractable only when the loss function can be expressed as a pointwise maximum
of ï¬nitely many concave functions [67, Theorem 4.2], and 2-Wasserstein DRO is shown to be
tractable only when the loss function is smooth and the radius of the ambiguity set is suï¬ƒciently
small [20, Theorem 3].

(II) The strong duality of Sinkhorn DRO holds in an even more general setting. Essentially, the only
requirements on the space Z and the nominal distribution (cid:98)â„™ are measurability. In contrast, the
strong duality for Wasserstein DRO ([46, Theorem 1], [17, Theorem 1]) requires the nominal
distribution (cid:98)â„™ to be a Borel probability measure and the set Z to be a Polish space.

We remark that Sinkorn DRO and Wasserstein DRO result in diï¬€erent conditions for ï¬nite worst-case
values. From Condition 1 we see that the Sinkhorn DRO is ï¬nite if and only if under a light-tail condition
on ğ‘“ , while the Wasserstein DRO [46, Theorem 1] is ï¬nite iï¬€ and only if the loss function satisï¬es a
growth condition ğ‘“ (ğ‘§) â‰¤ ğ¿ğ‘ (ğ‘§, ğ‘§0) + ğ‘€, âˆ€ğ‘§ âˆˆ Z for some constants ğ¿, ğ‘€ > 0 and some ğ‘§0 âˆˆ Z.
â™£
Remark 4 (Connection with KL-DRO). Using Jensenâ€™s inequality, we can see that the dual objective
function of the Sinkhorn DRO model can be upper bounded as

ğœ†ğœŒ + ğœ†ğœ– log

(cid:18)âˆ«

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105)
(cid:104)

ğ”¼â„šğ‘¥,ğœ–

(cid:19)

,

d(cid:98)â„™(ğ‘¥)

which corresponds to the dual objective function [57] for the following KL-divergence DRO

(cid:8)ğ”¼ğ‘§âˆ¼â„™ [ğ‘“ (ğ‘§)] : ğ·KL(â„™(cid:107)â„™0) â‰¤ ğœŒ/ğœ–(cid:9) ,

sup
â„™

7

where â„™0 satisï¬es dâ„™0(ğ‘§) = âˆ«
estimation constructed from (cid:98)â„™. Particularly, when (cid:98)â„™ = 1
ğ‘–=1 ğ›¿ Ë†ğ‘¥ğ‘–
kernel density estimator with Gaussian kernel and bandwidth ğœ–:

dâ„šğ‘¥,ğœ– (ğ‘§) d(cid:98)â„™(ğ‘¥), which can be viewed as a non-parametric kernel density
2, â„™0 is

, Z = â„ğ‘‘ and ğ‘ (ğ‘¥,ğ‘¦) = (cid:107)ğ‘¥ âˆ’ ğ‘¦ (cid:107)2

(cid:80)ğ‘›

ğ‘›

ğ‘¥

dâ„™0(ğ‘§)
dğ‘§

=

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ¾ğœ– (ğ‘§ âˆ’ ğ‘¥ğ‘–) ,

ğ‘§ âˆˆ â„ğ‘‘,

where ğ¾ğœ– (ğ‘¥) âˆ exp(âˆ’(cid:107)ğ‘¥ (cid:107)2
2/ğœ–) represents the Gaussian kernel. By decomposing ğ›¾ (ğ‘¥, ğ‘§) = (cid:98)â„™(ğ‘¥) âŠ— ğ›¾ğ‘¥ (ğ‘§),
similar to the proof in Section 3.2 below, (Sinkhorn DRO) can be reformulated as a generalized
KL-divergence DRO problem:

ğ‘‰ =

sup
ğ›¾ğ‘¥ âˆˆP (Z),âˆ€ğ‘¥ âˆˆZ

(cid:26)âˆ«

ğ”¼ğ‘§âˆ¼ğ›¾ğ‘¥ [ğ‘“ (ğ‘§)] d(cid:98)â„™(ğ‘¥) :

âˆ«

ğ·KL(ğ›¾ğ‘¥ (cid:107)â„šğ‘¥ ) d(cid:98)â„™(ğ‘¥) â‰¤ ğœŒ/ğœ–

(cid:27)

.

(3)

Using Divergence inequality [34, Theorem 2.6.3], we can see the Sinkhorn DRO with ğœŒ = 0 is reduced
to the following SAA model based on the distribution â„™0:

ğ‘‰ = ğ”¼â„™0 [ğ‘“ (ğ‘§)] =

âˆ«

ğ”¼â„šğ‘¥,ğœ– [ğ‘“ (ğ‘§)] d(cid:98)â„™(ğ‘¥).

(4)

In non-parameteric statistics, the optimal bandwidth to minimize the mean-squared-error between
the estimated distribution and the underlying true one is at rate ğœ– = ğ‘‚ (ğ‘›âˆ’1/(ğ‘‘+4) ) [54, Theorem 4.2.1].
However, such an optimal choice for the kernel density estimator may not be the optimal choice for
optimizing the out-of-sample performance of the Sinkhorn DRO. In our numerical experiments in
Section 5, we select ğœ– based on cross-validation.
â™£

Let us illustrate our result for a linear loss function ğ‘“ , which turns out to be equivalent to a simple

optimization problem.
Example 1. Suppose that ğ‘“ (ğ‘§) = ğ‘Tğ‘§, Z = â„ğ‘‘ and ğœˆ is the corresponding Lebesgue measure, and the
cost function is the Mahalanobis distance, i.e., ğ‘ (ğ‘¥,ğ‘¦) = 1
2 (ğ‘¥ âˆ’ğ‘¦)TÎ©(ğ‘¥ âˆ’ğ‘¦), where Î© is a positive deï¬nite
matrix. In this case, we have the reference measure

As a consequence, the dual problem can be written as

â„šğ‘¥,ğœ– âˆ¼ N (ğ‘¥, ğœ– Î©âˆ’1).

ğ‘‰D = inf
ğœ†>0

(cid:26)

ğœ†ğœŒ + ğœ†ğœ–

âˆ«

Î›ğ‘¥ (ğœ†) d(cid:98)â„™(ğ‘¥)

(cid:27)

,

Î›ğ‘¥ (ğœ†) = log

(cid:16)

ğ”¼(ğ‘¥,ğœ– Î©âˆ’1)

ğ‘’ğ‘(cid:62)ğ‘§/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

ğ‘Tğ‘¥
ğœ†ğœ–

ğ‘TÎ£ğ‘
2ğœ†2ğœ–2

.

+

=

where

Therefore

ğ‘‰D = ğ‘Tğ”¼

(cid:98)â„™ [ğ‘¥] + âˆšï¸2ğœŒâˆšï¸ğ‘TÎ©âˆ’1ğ‘ := ğ”¼

(cid:98)â„™ [ğ‘Tğ‘¥] + âˆšï¸2ğœŒ Â· (cid:107)ğ‘(cid:107)Î©âˆ’1 .

This indicates that the Sinkhorn DRO is equivalent to an empirical risk minimization with norm
regularization, and can be solved using eï¬ƒciently using algorithms for the second-order cone
program.
â™£

In the following, we ï¬rst show that ğ‘‰ = ğ‘‰D when (cid:98)â„™ is an empirical distribution supported on ğ‘› points,
as it is relatively straightforward to prove. Then we show that the same results hold when (cid:98)â„™ is a
general distribution, as it provides more insights on the worst-case distribution.

8

3.2. Proof for an Empirical Nominal distribution
Given data points Ë†ğ‘¥ğ‘– for ğ‘– = 1, . . . , ğ‘›, denote 1
ğ‘–=1 ğ›¿ Ë†ğ‘¥ğ‘–
as the corresponding empirical distribution. In
ğ‘›
this subsection, we discuss the dual reformulation provided that the nominal distribution (cid:98)â„™ is taken in
this form. Although our strong duality result holds for arbitrary nominal distribution, this is still an
interesting case, as the proof is relatively simple and (cid:98)â„™ is often chosen as the empirical distribution in
practice.

(cid:80)ğ‘›

The key to the proof is to write the primal problem in a Lagrangian form and then apply the minimax
inequality to obtain a weak dual. Observe that the primal can be reformulated as a generalized
KL-divergence DRO problem. Hence, by leveraging the strong duality result for the existing DRO
model [57], the minimax inequality does not incur any duality gap.
(cid:80)ğ‘›

Proof of Theorem 1 when (cid:98)â„™ = 1

ğ‘–=1 ğ›¿ Ë†ğ‘¥ğ‘– . Based on Deï¬nition 1, we reformulate ğ‘‰ as
(cid:41)
(cid:40)

(cid:33)(cid:35)

(cid:32)

ğ‘›

ğ‘‰ =

sup
ğ›¾ âˆˆP (ZÃ—Z):Proj1#ğ›¾ =(cid:98)â„™

ğ”¼â„™ [ğ‘“ (ğ‘§)] : ğ”¼ğ›¾

(cid:34)
ğ‘ (ğ‘¥, ğ‘§) + ğœ– log

â‰¤ ğœŒ

.

dğ›¾ (ğ‘¥, ğ‘§)
d(cid:98)â„™(ğ‘¥) dğœˆ (ğ‘§)
(cid:80)ğ‘›

By the disintegration theorem [23] we represent the joint distribution ğ›¾ = 1
ğ‘–=1 ğ›¿ Ë†ğ‘¥ğ‘– âŠ— ğ›¾ğ‘–, where ğ›¾ğ‘– is the
ğ‘›
conditional distribution of ğ›¾ given the ï¬rst marginal of ğ›¾ equals Ë†ğ‘¥ğ‘–. Thereby the constraint is equivalent
to

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

(cid:20)
ğ‘ ( Ë†ğ‘¥ğ‘–, ğ‘§) + ğœ– log

ğ”¼ğ›¾ğ‘–

(cid:19)(cid:21)

(cid:18) dğ›¾ğ‘– (ğ‘§)
dğœˆ (ğ‘§)

â‰¤ ğœŒ,

ğ›¾ğ‘– âˆˆ P (Z), ğ‘– âˆˆ [ğ‘›].

We remark that any feasible solution ğ›¾ satisï¬es that ğ›¾ (cid:28) (cid:98)â„™ âŠ— ğœˆ and hence ğ›¾ğ‘– (cid:28) ğœˆ. Consequently the
is well-deï¬ned. For notational simplicity, we write â„šğ‘– for â„šË†ğ‘¥ğ‘–,ğœ– . Based on the change-
term log
(cid:17)
and the expression of â„šğ‘–, the constraint

= log

+ log

(cid:17)

(cid:17)

(cid:17)

(cid:16) dğ›¾ğ‘– (ğ‘§)
dğœˆ (ğ‘§)
of-measure identity log
can be reformulated as

(cid:16) dğ›¾ğ‘– (ğ‘§)
dâ„šğ‘– (ğ‘§)

(cid:16) dâ„šğ‘– (ğ‘§)
dğœˆ (ğ‘§)

(cid:16) dğ›¾ğ‘– (ğ‘§)
dğœˆ (ğ‘§)

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ”¼ğ›¾ğ‘–

(cid:34)
ğ‘ ( Ë†ğ‘¥ğ‘–, ğ‘§) + ğœ– log

(cid:32)

(cid:33)

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ–
âˆ« ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘¢)/ğœ– dğœˆ (ğ‘¢)

+ ğœ– log

(cid:19)(cid:35)

(cid:18) dğ›¾ğ‘– (ğ‘§)
dâ„šğ‘– (ğ‘§)

â‰¤ ğœŒ,

ğ›¾ğ‘– âˆˆ P (Z), ğ‘– âˆˆ [ğ‘›].

Combining the ï¬rst two terms within the expectation term and substituting the expression of ğœŒ, it is
equivalent to

ğœ–
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ”¼ğ›¾ğ‘–

(cid:20)

log

(cid:19)(cid:21)

(cid:18) dğ›¾ğ‘– (ğ‘§)
dâ„šğ‘– (ğ‘§)

â‰¤ ğœŒ,

ğ›¾ğ‘– âˆˆ P (Z), ğ‘– âˆˆ [ğ‘›].

Similarly, the objective function of (Sinkhorn DRO) can be written as 1
ğ‘–=1 ğ”¼ğ›¾ğ‘– [ğ‘“ (ğ‘§)]. Consequently,
ğ‘›
the primal problem (Sinkhorn DRO) can be reformulated as a generalized KL-divergence DRO problem

(cid:80)ğ‘›

ğ‘‰ =

sup
ğ›¾ğ‘– âˆˆP (Z),ğ‘– âˆˆ [ğ‘›]

(cid:40) 1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ”¼ğ›¾ğ‘– [ğ‘“ (ğ‘§)] :

(cid:41)

ğ·KL(ğ›¾ğ‘– (cid:107)â„šğ‘–) â‰¤ ğœŒ

.

ğœ–
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

Then Theorem 1(I) holds based on the non-negativity of KL-divergence.

Introducing the Lagrange multiplier ğœ† associated with the constraint, we reformulate (Sinkhorn DRO)

as

ğ‘‰ =

sup
ğ›¾ğ‘– âˆˆP (Z),ğ‘– âˆˆ [ğ‘›]

inf
ğœ† â‰¥0

(cid:40)

ğœ†ğœŒ +

(cid:20)

ğ”¼ğ›¾ğ‘–

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘“ (ğ‘§) âˆ’ ğœ†ğœ– log

(cid:18) dğ›¾ğ‘– (ğ‘§)
dâ„šğ‘– (ğ‘§)

(cid:19)(cid:21) (cid:41)

.

Interchanging the supremum and inï¬mum operators, we have that

(cid:40)

ğ‘‰ â‰¤ inf
ğœ† â‰¥0

ğœ†ğœŒ +

sup
ğ›¾ğ‘– âˆˆP (Z),ğ‘– âˆˆ [ğ‘›]

(cid:40) 1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

(cid:20)

ğ”¼ğ›¾ğ‘–

ğ‘“ (ğ‘§) âˆ’ ğœ†ğœ– log

(cid:18) dğ›¾ğ‘– (ğ‘§)
dâ„šğ‘– (ğ‘§)

(cid:19)(cid:21) (cid:41)(cid:41)

.

9

Since the optimization over ğ›¾ğ‘–, ğ‘– âˆˆ [ğ‘›] is separable, by deï¬ning for each ğ‘–

ğ‘£ğ‘– (ğœ†) := sup

ğ›¾ğ‘– âˆˆP (Z)

(cid:26)

(cid:20)

ğ”¼ğ›¾ğ‘–

ğ‘“ (ğ‘§) âˆ’ ğœ†ğœ– log

(cid:18) dğ›¾ğ‘– (ğ‘§)
dâ„šğ‘– (ğ‘§)

(cid:19)(cid:21) (cid:27)

,

it holds that

(cid:40)

ğ‘‰ â‰¤ inf
ğœ† â‰¥0

ğœ†ğœŒ +

(cid:41)

ğ‘£ğ‘– (ğœ†)

.

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

When Condition 1 holds, leveraging a well-known results on entropy regularized linear optimization
(Lemma EC.1), we can see that

ğ‘£ğ‘– (ğœ†) = ğœ†ğœ– log

(cid:16)

ğ”¼â„šğ‘–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

< âˆ,

hence we obtain the weak duality ğ‘‰ â‰¤ ğ‘‰D < âˆ. Otherwise, for any ğœ† > 0, there exists an index ğ‘– such
that
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105)
(cid:104)

= âˆ.

ğ”¼â„šğ‘–

We also obtain that ğ‘‰ â‰¤ ğ‘‰D = âˆ, and the weak duality still holds.

Next we prove the strong duality. Recall that the primal problem is a generalized KL-divergence DRO
problem. By leveraging the strong duality result from [57], the minimax inequality above does not
incur any duality gap when ğœŒ > 0. When ğœŒ = 0, since ğ·KL(ğ›¾ğ‘– (cid:107)â„šğ‘–) = 0 if and only if ğ›¾ğ‘– = â„šğ‘–, one can see
that

ğ‘‰ =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ”¼â„šğ‘– [ğ‘“ (ğ‘§)].

On the other hand, denote by â„(ğœ†) the objective function for the dual problem. Then we have the
inequality

ğ‘‰D â‰¤ lim
ğœ†â†’âˆ

â„(ğœ†) =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ”¼â„šğ‘– [ğ‘“ (ğ‘§)] = ğ‘‰.

This, together with the weak duality result, completes the proof for Theorem 1(II). Theorem 1(III) also
(cid:3)
follows based on the discussion of the ï¬niteness of ğ‘‰D.

When the sample space Z is ï¬nite, the following result presents a conic programming reformulation.
Corollary 1 (Conic Reformulation for Finite Sample Space). Suppose that the sample space con-
tains ğ¿ elements, i.e., Z = {ğ‘§â„“ }ğ¿
â„“=1. If Condition 1 holds and ğœŒ â‰¥ 0, the dual problem (Dual) can be
formulated as the following conic optimization:

ğ‘‰D = min
ğœ† â‰¥0,ğ‘  âˆˆâ„ğ‘›,
ğ‘ âˆˆâ„ğ‘›Ã—ğ¿

ğœ†ğœŒ +

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘ ğ‘–

s.t. ğœ†ğœ– â‰¥

ğ¿
âˆ‘ï¸

â„“=1

ğ‘ğ‘–,â„“ğ‘ğ‘–,â„“, ğ‘– âˆˆ [ğ‘›],

(ğœ†ğœ–, ğ‘ğ‘–,â„“, ğ‘“ (ğ‘§â„“ ) âˆ’ ğ‘ ğ‘–) âˆˆ Kexp, ğ‘– âˆˆ [ğ‘›], â„“ âˆˆ [ğ¿].

(5)

where ğ‘ğ‘–,â„“ := Prğ‘§âˆ¼â„š Ë†ğ‘¥ğ‘– ,ğœ– {ğ‘§ = ğ‘§â„“ }, with the distribution â„šË†ğ‘¥ğ‘–,ğœ– deï¬ned in (2), and Kexp denotes the exponential
cone Kexp = {(ğœˆ, ğœ†, ğ›¿) âˆˆ â„+ Ã— â„+ Ã— â„ : exp(ğ›¿/ğœˆ) â‰¤ ğœ†/ğœˆ }.
Problem (5) is a convex program that minimizes a linear function with respect to linear and conic
constraints, which can be solved using interior point algorithms [71, 95]. We will develop a customized
ï¬rst-order optimization algorithm in Section 4 that is able to solve a more general problem (without a
ï¬nite sample space).

10

3.3. Proof for a General Nominal Distribution
In this subsection, we outline the proof of Theorem 1 when (cid:98)â„™ is an arbitrary nominal distribution and
discuss the worst-case distribution.

The feasibility result in Theorem 1(I) can be easily shown using the reformulation (3). To show
ğ‘‰ = ğ‘‰D, it is easy to show the weak duality result following a similar argument as in Section 3.2, by
replacing the ï¬nite-sum with the integration with respect to (cid:98)â„™.

When Condition 1 holds, we prove the strong duality by constructing the worst-case distribution. We
ï¬rst show the existence of the dual minimizer (Lemma 2), and then build the corresponding ï¬rst-order
optimality condition (Lemma 3 and Lemma 4). Those results help us to construct a primal optimal
solution for (Sinkhorn DRO) that shares the same optimal value as ğ‘‰D, which completes the ï¬rst part
of Theorem 1(III). When Condition 1 does not hold, we construct a sequence of DRO problems with
ï¬nite optimal values converging into ğ‘‰ and consequently ğ‘‰ = ğ‘‰D = âˆ, which completes the second part
of Theorem 1(III). Putting these two parts together imply Theorem 1(II).
Lemma 1 (Weak Duality). Assume Assumption 1 holds. Then ğ‘‰ â‰¤ ğ‘‰D.

Below we provide the proof of the ï¬rst part of Theorem 1(III) for the case ğœŒ > 0 under Condition 1,
defer proofs of other degenerate cases to Appendix EC.4. To prove the strong duality, we will construct
a feasible solution of (Sinkhorn DRO) whose loss coincides with ğ‘‰D. To this end, we ï¬rst show that the
dual minimizer exists.
Lemma 2 (Existence of Dual Minimizer). Suppose ğœŒ > 0 and Condition 1 is satisï¬ed, then the dual
minimizer ğœ†âˆ— exists, which either equals to 0 or satisï¬es Condition 1.

We separate two cases: ğœ†âˆ— > 0 and ğœ†âˆ— = 0, corresponding to whether the Sinkhorn distance constraint

in (Sinkhorn DRO) is binding or not.

Lemma 3 below presents a necessary and suï¬ƒcient condition for the dual minimizer ğœ†âˆ— = 0,
corresponding to the case where the Sinkhorn distance constraint in (Sinkhorn DRO) is not binding.
Lemma 3 (Necessary and Suï¬ƒcient Condition for ğœ†âˆ— = 0). Suppose ğœŒ > 0 and Condition 1 is satisï¬ed,
then the dual minimizer ğœ†âˆ— = 0 if and only if all the following conditions hold:

ğ‘“ (cid:44) inf{ğ‘¡ : ğœˆ {ğ‘“ (ğ‘§) > ğ‘¡ } = 0} < âˆ.

(I) ess sup
(II) ğœŒ (cid:48) = ğœŒ + ğœ– âˆ« log (cid:0)ğ”¼â„šğ‘¥,ğœ– [1ğ´](cid:1) d(cid:98)â„™(ğ‘¥) â‰¥ 0, where ğ´ := {ğ‘§ : ğ‘“ (ğ‘§) = ess sup

ğœˆ

ğ‘“ }.

ğœˆ

Recall that we have the convention that the dual objective evaluated at ğœ† = 0 equals ess sup
Condition (I) ensures that the dual objective function evaluated at the minimizer is ï¬nite. When the
minimizer ğœ†âˆ— = 0, the Sinkhorn ball should be large enough to contain at least one distribution with
objective value ess sup

ğ‘“ , and the condition (II) characterizes the lower bound of ğœŒ.

ğ‘“ . Thus

ğœˆ

Lemma 4 below considers the optimality condition when the dual minimizer ğœ†âˆ— > 0, obtained by

ğœˆ

simply setting the derivative of the dual objective function to be zero.
Lemma 4 (First-order Optimality Condition when ğœ†âˆ— > 0). Suppose ğœŒ > 0 and Condition 1 is satisï¬ed,
and assume further that the dual minimizer ğœ†âˆ— > 0, then ğœ†âˆ— satisï¬es
âˆ« ğ”¼â„šğ‘¥,ğœ–

âˆ«

(cid:16)

(cid:20)

(cid:21)

ğœ†âˆ—

ğœŒ + ğœ–

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥)

=

d(cid:98)â„™(ğ‘¥).

(6)

(cid:2)ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) ğ‘“ (ğ‘§)(cid:3)
(cid:2)ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) (cid:3)

ğ”¼â„šğ‘¥,ğœ–

Now we are ready to prove Theorem 1.
Proof of Theorem 1(III) under Condition 1 with ğœŒ > 0. The proof is separated for two cases: ğœ†âˆ— > 0

or ğœ†âˆ— = 0. For each case we prove by constructing a primal (approximate) optimal solution.

When ğœ†âˆ— > 0, we take a probability measure ğ›¾âˆ— such that

dğ›¾âˆ—(ğ‘¥, ğ‘§) =

âˆ« exp

exp

(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘§)
ğœ†âˆ—ğœ–
(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘¢)
ğœ†âˆ—ğœ–

(cid:17)

(cid:17)

dğœˆ (ğ‘¢)

dğœˆ (ğ‘§) d(cid:98)â„™(ğ‘¥), where ğœ™ (ğœ†; ğ‘¥, ğ‘§) = ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§).

Also deï¬ne the primal (approximate) optimal distribution

â„™âˆ— := Proj2#ğ›¾âˆ—.

Recall the expression of the Sinkhorn distance in Deï¬nition 1, one can verify that

11

Wğœ– ((cid:98)â„™, â„™âˆ—)
(cid:40)

=

ğ”¼ğ›¾

inf
ğ›¾ âˆˆÎ“ ((cid:98)â„™,â„™âˆ—)
(cid:34)
ğ‘ (ğ‘¥, ğ‘§) + ğœ– log

â‰¤ ğ”¼ğ›¾âˆ—

= ğ”¼ğ›¾âˆ—

1
ğœ†âˆ—

=

ï£®
ï£¯
ğ‘ (ğ‘¥, ğ‘§) + ğœ– log (cid:169)
ï£¯
(cid:173)
ï£¯
(cid:173)
ï£¯
ï£¯
(cid:171)
ï£°
âˆ¬ ğ‘“ (ğ‘§) exp
ï£±ï£´ï£´ï£²
âˆ« exp
ï£´ï£´
ï£³

(cid:34)
ğ‘ (ğ‘¥, ğ‘§) + ğœ– log

(cid:32)

(cid:33)(cid:35) (cid:41)

dğ›¾ (ğ‘¥, ğ‘§)
d(cid:98)â„™(ğ‘¥) dğœˆ (ğ‘§)

(cid:33)(cid:35)

(cid:32) dğ›¾âˆ—(ğ‘¥, ğ‘§)
d(cid:98)â„™(ğ‘¥) dğœˆ (ğ‘§)
exp

âˆ« exp

(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘§)
ğœ†âˆ—ğœ–
(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘¢)
ğœ†âˆ—ğœ–
(cid:17)

(cid:17)

(cid:17)

dğœˆ (ğ‘¢)

ï£¹
ï£º
(cid:170)
ï£º
(cid:174)
ï£º
(cid:174)
ï£º
ï£º
(cid:172)
ï£»

(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘§)
ğœ†âˆ—ğœ–
(cid:17)

(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘§)
ğœ†âˆ—ğœ–

dğœˆ (ğ‘§)

dğœˆ (ğ‘§) d(cid:98)â„™(ğ‘¥) âˆ’ ğœ†âˆ—ğœ–

âˆ«

(cid:18)âˆ«

log

exp

(cid:18)ğœ™ (ğœ†âˆ—; ğ‘¥,ğ‘¢)
ğœ†âˆ—ğœ–

(cid:19)

(cid:19)

dğœˆ (ğ‘¢)

d(cid:98)â„™(ğ‘¥)

,

ï£¼ï£´ï£´ï£½
ï£´ï£´
ï£¾

where the second relation is because ğ›¾âˆ— is a feasible solution in Î“((cid:98)â„™, â„™âˆ—), the third and the fourth
relation is by substituting the expression of ğ›¾âˆ—. Since ğœŒ > 0 and the dual minimizer ğœ†âˆ— > 0, the optimality
condition in (6) holds, which implies that Wğœ– ((cid:98)â„™, â„™âˆ—) â‰¤ ğœŒ, i.e., the distribution â„™âˆ— is primal feasible for
the problem (Sinkhorn DRO). Moreover, we can see that the primal optimal value is lower bounded by
the dual optimal value:

ğ‘“ (ğ‘§) dğ›¾âˆ—(ğ‘¥, ğ‘§)
(cid:33)

dğœˆ (ğ‘§) d(cid:98)â„™(ğ‘¥)

ğ‘‰ â‰¥ ğ”¼â„™âˆ— [ğ‘“ (ğ‘§)] =

âˆ«

âˆ¬

âˆ¬

=

=

ğ‘“ (ğ‘§)

(cid:32) dğ›¾âˆ—(ğ‘¥, ğ‘§)
d(cid:98)â„™(ğ‘¥) dğœˆ (ğ‘§)

(cid:17)

exp

(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘§)
ğœ†âˆ—ğœ–
(cid:16) ğœ™ (ğœ†âˆ—;ğ‘¥,ğ‘¢)
ğœ†âˆ—ğœ–

(cid:17)

ğ‘“ (ğ‘§)

âˆ« exp

âˆ«

(cid:20)

ğœŒ + ğœ–

=ğœ†âˆ—

(cid:18)âˆ«

log

exp

=ğ‘‰D,

dğœˆ (ğ‘§) d(cid:98)â„™(ğ‘¥)

dğœˆ (ğ‘¢)
(cid:20)ğœ™ (ğœ†âˆ—; ğ‘¥, ğ‘§)
ğœ†âˆ—ğœ–

(cid:21)

(cid:19)

dğœˆ (ğ‘§)

(cid:21)

d(cid:98)â„™(ğ‘¥)

where the third equality is based on the optimality condition in Lemma 4. This, together with the
weak duality result, completes the proof for ğœ†âˆ— > 0.

When ğœ†âˆ— = 0, the optimality condition in Lemma 3 holds. We construct the primal (approximate)

solution â„™âˆ— = Proj2#ğ›¾âˆ—, where ğ›¾âˆ— satisï¬es

dğ›¾âˆ—(ğ‘¥, ğ‘§) = dğ›¾ ğ‘¥

âˆ— (ğ‘§) d(cid:98)â„™(ğ‘¥), where dğ›¾ğ‘¥

âˆ— (ğ‘¦) =

(cid:40)0,

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§)
âˆ« ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘¢)/ğœ– 1ğ´ dğœˆ (ğ‘¢)

,

if ğ‘§ âˆ‰ ğ´,
if ğ‘§ âˆˆ ğ´.

We can verify easily that the primal solution is feasible based on the optimality condition ğœŒ (cid:48) â‰¥ 0 in
Lemma 3. Moreover, we can check that the primal optimal value is lower bounded by the dual optimal
value:

âˆ«

ğ‘‰ â‰¥

ğ‘“ (ğ‘§) dğ›¾âˆ—(ğ‘¥, ğ‘§) =

âˆ¬

ğ‘“ (ğ‘§) dğ›¾ğ‘¥

âˆ— (ğ‘§) d(cid:98)â„™(ğ‘¥) =

âˆ¬

ess sup
ğœˆ

ğ‘“ dğ›¾ ğ‘¥

âˆ— (ğ‘§) d(cid:98)â„™(ğ‘¥) = ess sup

ğ‘“ = ğ‘‰D,

ğœˆ

12

where the second equality is because that ğ‘§ âˆˆ ğ´ so that ğ‘“ (ğ‘§) = ess sup
duality result, completes the proof for ğœ†âˆ— = 0.

ğœˆ

ğ‘“ . This, together with the weak

(cid:3)

Remark 5 (Worst-case Distribution). From the proof presented above we observe that when
ğœ†âˆ— > 0, the worse-case distribution for (Sinkhorn DRO) can be expressed as

dâ„™âˆ—(ğ‘§) =

âˆ«

ğ‘¥

(cid:32)ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) dâ„šğ‘¥,ğœ– (ğ‘§)
(cid:2)ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) (cid:3)

ğ”¼â„šğ‘¥,ğœ–

(cid:33)

d(cid:98)â„™(ğ‘¥),

from which we can see that the worst-case distribution shares the same support as the measure ğœˆ.
Particularly, when (cid:98)â„™ is the empirical distribution 1
(cid:80)ğ‘›
and ğœˆ is any continuous distribution on â„ğ‘‘ ,
ğ‘›
the worst-case distribution â„™âˆ— is supported on the entire â„ğ‘‘ . In contrast, the worst-case distribution
for Wasserstein DRO is supported on at most ğ‘› + 1 points [46]. This is another diï¬€erence, or advantage
possibly, of Sinkhorn DRO compared with Wasserstein DRO. Indeed, for many practical problems,
the underlying distribution can be modeled as a continuous distribution. The worst-case distribution
for Wasserstein DRO is often ï¬nitely supported, raising the concern of whether it hedges against the
wrong family of distributions and thus results in suboptimal solutions. The numerical results in Section
5 demonstrate some empirical advantages of Sinkhorn DRO.
â™£

ğ‘–=1 ğ›¿ Ë†ğ‘¥ğ‘–

4. Eï¬ƒcient First-order Algorithm for Data-driven Sinkhorn Robust Learning
In this section, we consider the data-driven Sinkhorn robust learning problem, where we seek an
optimal decision to minimize the worst-case risk

inf
ğœƒ âˆˆÎ˜

sup
â„™âˆˆğ”¹ğœŒ,ğœ– ((cid:98)â„™)

ğ”¼ğ‘§âˆ¼â„™ [ğ‘“ğœƒ (ğ‘§)],

(7)

where the feasible set Î˜ contains all possible candidates of decision vector ğœƒ , and we take (cid:98)â„™ as the
empirical distribution corresponding to sample points Ë†ğ‘¥ğ‘–, ğ‘– = 1, . . . , ğ‘›. Based on our strong dual (Dual),
we reformulate (7) as

(cid:40)

inf
ğœƒ âˆˆÎ˜,ğœ† â‰¥0

ğ¹ (ğœ†, ğœƒ ) := ğœ†ğœŒ +

(cid:16)

ğœ†ğœ– log

ğ”¼â„š Ë†ğ‘¥ğ‘– ,ğœ–

ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

(cid:41)

,

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

(8)

where the constant ğœŒ and the distribution â„šË†ğ‘¥ğ‘–,ğœ– are deï¬ned in (1) and (2), respectively.

In Example 1 we have seen an instance of (8) where we can get a closed-form expression for the
above integration. In general, when a closed-form expression is not available, in the following we
present a ï¬rst-order algorithm to solve this problem, and discuss some alternatives in Section 4.1.
Observe that the objective function of (8) involves a nonlinear transformation of the expectation, thus
an unbiased gradient estimate could be challenging when â„šË†ğ‘¥ğ‘–,ğœ– is a general probability distribution.
We propose to solve the following approximation

(cid:40)

inf
ğœƒ âˆˆÎ˜,ğœ† â‰¥0

Ë†ğ¹ (ğ‘š) (ğœ†, ğœƒ ) := ğœ†ğœŒ +

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğœ†ğœ– log

(cid:16)

ğ”¼ Ë†â„šğ‘š

ğ‘–

ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

(cid:41)

,

(9)

(cid:80)ğ‘š

ğ‘– = 1
ğ‘š

ğ‘—=1 ğ›¿Ë†ğ‘§ğ‘–,ğ‘—

denotes the empirical distribution constructed from {Ë†ğ‘§ğ‘–,ğ‘— }ğ‘š

where Ë†â„šğ‘š
ğ‘—=1, independent
and identically distributed samples from â„šË†ğ‘¥ğ‘–,ğœ– . In many cases, generating samples from â„šË†ğ‘¥ğ‘–,ğœ– is easy.
For instance, choosing the cost function ğ‘ (Â·, Â·) = 1
2 and Z = â„ğ‘‘ , then the distribution â„šË†ğ‘¥ğ‘–,ğœ–
becomes a Gaussian distribution N ( Ë†ğ‘¥ğ‘–, ğœ–ğ¼ğ‘‘ ). Otherwise we can generate samples by, for example, the
acceptance-rejection method [5]. As a brief summary of our proposed method, we ï¬rst simulate a batch
of samples to approximate the original Sinkhorn DRO dual objective function, and then use projected

2 (cid:107) Â· âˆ’ Â· (cid:107)2

Algorithm 1 A batch gradient descent with bisection search for solving (9)
Require: An interval [ğœ†ğ‘™, ğœ†ğ‘¢] so that 0 < ğœ†ğ‘™ â‰¤ ğœ†âˆ— â‰¤ ğœ†ğ‘¢, where ğœ†âˆ— is an optimal dual variable for (9).

Terminating tolerance Î” > 0.

13

1: for ğ‘¡ = 0, 1, . . . ,ğ‘‡ âˆ’ 1 do
ğœ†0 â† (ğœ†ğ‘™ + ğœ†ğ‘¢)/2.
2:
Solve the subproblem (10) at ğœ†0 to get ğœƒ0.
3:
Ë†ğ¹ (ğ‘š) (ğœ†0, ğœƒ0).
Compute ğ‘ âˆˆ ğœ•
4:
ğœ•ğœ†
Terminate the iteration if ğ‘ = 0 or ğœ†ğ‘¢ âˆ’ ğœ†ğ‘™ < Î”.
5:
Let ğ‘¡ğ‘¢ â† ğ‘¡0 when ğ‘ > 0. Let ğ‘¡ğ‘™ â† ğ‘¡0 when ğ‘ < 0.
6:
7: end for

Return ğœ†0 and ğœƒ0.

gradient descent with bisection search to solve the approximated problem. Hence our method is named
the batch gradient descent with bisection search.

When the function ğ‘“ğœƒ (ğ‘§) is convex in ğœƒ , problem (9) is a ï¬nite convex programming because the
second term of the objective function Ë†ğ¹ (ğ‘š) (ğœ†, ğœƒ ) in (9) is a perspective transformation [22, Section 2.3.3]
of the log-sum-exp function in composition of a convex function. We develop a customized batch
gradient descent with bisection search method to solve problem (9) eï¬ƒciently. The gradient of the
objective function of (9) can be calculated as

âˆ‡ğœƒ Ë†ğ¹ (ğ‘š) (ğœ†, ğœƒ ) =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

(cid:2)ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) âˆ‡ğœƒ ğ‘“ğœƒ (ğ‘§)(cid:3)
(cid:2)ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:3)

ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–
ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–

,

ğœ•
ğœ•ğœ†

Ë†ğ¹ (ğ‘š) (ğœ†, ğœƒ ) = ğœŒ +

(cid:18)

log

ğœ–
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–

ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:19)
(cid:104)

âˆ’

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–

(cid:2)ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) ğ‘“ğœƒ (ğ‘§)(cid:3)
(cid:2)ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:3)

.

ğœ†ğ”¼ Ë†â„šğ‘š

Ë†ğ‘¥ğ‘–

For ï¬xed ğœ† > 0, denote Ë†ğ¹ (ğ‘š)

ğœ†

as the optimal value of the following problem

Ë†ğ¹ (ğ‘š)
ğœ†

= ğœ†ğœŒ + inf
ğœƒ âˆˆÎ˜

(cid:40) 1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğœ†ğœ– log

(cid:18)

ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–

(cid:104)

ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:19)(cid:41)

.

(10)

ğœ†

is convex in ğœ†. This, together with the fact that Ë†ğ¹ (ğ‘š)

By the convexity of Ë†ğ¹ (ğ‘š) (ğœ†, ğœƒ ), the function Ë†ğ¹ (ğ‘š)
is
sub-diï¬€erentiable with respect to ğœ†, motivates us to use the bisection search method in Algorithm 1 to
solve problem (9). In particular, in each iteration we ï¬rst ï¬nd the optimal solution to problem (10) for
a ï¬xed ğœ†, and then use the bisection method to search for the optimal ğœ†. When the gradient âˆ‡ğœƒ ğ‘“ğœƒ (ğ‘§) is
bounded for any ğœƒ and ğ‘§, the sub-problem (10) can be solved using the projected gradient method
ğ‘€), where ğ‘€ denotes the number of inner iterations [59, Theorem 2.5].
with convergence rate ğ‘‚ (1/
We also argue that the iteration points of Algorithm 1 converge to the optimal solution of infğœ† â‰¥0 Ë†ğ¹ (ğ‘š)
with a linear rate.
Proposition 1. Suppose the function ğ‘“ğœƒ (ğ‘§) is convex in ğœƒ . Then the iteration points {ğœ†ğ‘¡ }ğ‘¡ of Algorithm 1
converge into ğœ†âˆ— linearly, i.e., |ğœ†ğ‘¡ âˆ’ ğœ†âˆ—| = ğ‘‚ (2âˆ’ğ‘¡ ), where ğœ†âˆ— is the optimal solution of infğœ† â‰¥0 Ë†ğ¹ (ğ‘š)

âˆš

ğœ†

ğœ†

.

ğœ†

Our last result in this section establishes the consistency for the Monte Carlo approximation (9).

Proposition 2. Let S âˆ— and ğ‘‰ âˆ— be the set of optimal solutions and the corresponding optimal value of
problem (8), respectively. And let S (ğ‘š) and ğ‘‰ (ğ‘š) be the set of optimal solutions and the optimal value to
problem (9), respectively. Assume that

(I) The function ğ‘“ğœƒ (ğ‘§) is random lower semi-continuous, and convex in ğœƒ . The set Î˜ is closed, convex,

and contains a non-empty interior.

14

(II) The value function ğ¹ (ğœ†, ğœƒ ) deï¬ned in (8) is lower semi-continuous, and there exists a point ( Â¯ğœ†, Â¯ğœƒ ) âˆˆ Î

such that ğ¹ (ğœ†, ğœƒ ) < âˆ for all (ğœ†, ğœƒ ) in a neighborhood of ( Â¯ğœ†, Â¯ğœƒ ).

(III) The set S âˆ— is non-empty and bounded.
Then as ğ‘š â†’ âˆ, ğ‘‰ (ğ‘š) â†’ ğ‘‰ âˆ— almost surely and Dist(S (ğ‘š), S âˆ—) â†’ 0 almost surely.

This indicates that we can obtain a near-optimal solution of (Sinkhorn DRO) with an arbitrarily
small sub-optimality gap as long as we increase the number of simulation times for Monte Carlo
approximation. The proof leverages results from [86], while the key diï¬€erence is that the objective
function studied in this paper involves the nonlinear transform of the expectation such that the existing
result in [86] cannot be applied directly. A detailed proof can be found in Appendix EC.5.

4.1. Alternative Algorithmic Choices
In this subsection, we discuss some other possibilities for designing an algorithm to solve (8).

As the objective in (8) involves the composition of two expectations, a natural idea to solve this
problem is to design algorithms leveraging techniques from stochastic compositional optimization
problem [101, 51, 108], but they cannot be applied directly here because they assume that the inner
expected-value is independent of the randomness in the outer expectation, while the inner expectation
of our objective in (8) depends on samples Ë†ğ‘¥ğ‘–, ğ‘– = 1, . . . , ğ‘›. The recent conditional stochastic compositional
(CSCO) optimization [55, 56], which aims to minimize a composition of two expected-value functions
with the inner expectation taken with respect to a conditional distribution, also opens the door for
designing eï¬ƒcient algorithms for solving (8). Although the objective in (8) cannot be written as
a CSCO problem, it naturally ï¬ts the structure when the dual variable ğœ† > 0 is ï¬xed. However, we
ï¬nd that it takes relatively long time to obtain an optimal variable ğœƒ when ğœ† is ï¬xed, as the lack of
strong-convexity and smoothness structure of the objective makes the state-of-the-art CSCO algorithm
(BSGD in [55]) diï¬ƒcult to converge empirically. Since we need to optimize variables (ğœ†, ğœƒ ) jointly, the
global convergence of CSCO for the problem (8) is even more challenging. In extensive simulations
that we omit, we ï¬nd that our proposed method solves the Sinkhorn DRO problem more eï¬ƒciently
than the state-of-the-art CSCO algorithm and standard projected gradient descent without bisection
search.

In our algorithm, we optimize ğœƒ for a ï¬xed ğœ†. An alternative would be to optimize (ğœ†, ğœƒ ) jointly.
However, as pointed out in [68], for ğœ† of a small value, the variance of the gradient estimate of the
objective function with respect to ğœ† is unstable. Hence, we develop a bisection method to update ğœ† in
outer iterations in Algorithm 1.

Moreover, we observe that in each inner iteration for solving the sub-problem (10), the most
computationally expansive step is to obtain the gradient of Ë†ğ¹ (ğ‘š)
, the complexity of which is of ğ‘‚ (ğ‘›ğ‘š).
For large-scale statistical learning problems, it is therefore promising to use the projected stochastic
gradient method instead of projected gradient descent to solve the sub-problem (10), but the condition
that guarantees convergence is more restrictive, which can be a topic for future study. For small-scale
problems, one can also write the problem in standard conic optimization form and use the interior
point method [70, 53] to solve it.

ğœ†

It is worth mentioning that the strategy in [21] can be applied to obtain an unbiased gradient
estimate for the expectation term in (8), but the presence of the logarithm makes it not easy to use
standard stochastic gradient descent with unbiased gradient estimate for optimization. Instead, we
use the Monte Carlo samples problem (9) to approximate (8). It is promising to use other numerical
integration techniques such as Quasi-Monte Carlo method [74] and Wavelet method [6] to approximate
the objective (8) eï¬ƒciently, which is of research interest for future study.

5. Applications
In this section, we apply our methodology on three applications: the newsvendor model, mean-
risk portfolio optimization, and semi-supervised learning. We examine the performance of the
(Sinkhorn DRO) model by comparing it with three benchmarks: (i) the classical sample average

approximation (SAA) model; (ii) the Wasserstein DRO model; and (iii) the KL-divergence DRO model.
Unless otherwise speciï¬ed, the cost function is chosen to be ğ‘ (Â·, Â·) = 1
2 (cid:107) Â· âˆ’ Â· (cid:107)2, and the reference
measure ğœˆ for the Sinkhorn distance is chosen to be the Lebesgue measure.

For each of the three applications, with ğ‘› training samples, we select the pair of hyper-parameters
(ğœ–, ğœŒ) using the ğ¾-fold cross-validation method with ğ¾ = 10. Since the grid search for an optimal pair
of hyper-parameters is more costly than the search for a single hyper-parameter, we ï¬rst tune the
hyper-parameter ğœ– while ï¬xing ğœŒ = 0, which corresponds to the SAA problem in (4). Then for the
chosen ğœ–, we tune the Sinkhorn radius ğœŒ. We run the repeated experiments for 200 times.

15

In Section 5.1 and Section 5.2, we measure the out-of-sample performance of a solution ğœƒ by its
, where ğ½ âˆ— denotes the true optimal value when the true distribution

relative performance gap ğ½ (ğœƒ )âˆ’ğ½ âˆ—
(cid:12)ğ½ âˆ—(cid:12)
(cid:12)

1+(cid:12)

is known exactly, and ğ½ (ğœƒ ) the expected loss of the solution ğœƒ under the true distribution, estimated
through an SAA objective value with 105 testing samples. Thus, the smaller the relative performance
gap is, the better out-of-sample performance the solution has.

Further details are included in Appendix EC.1.

5.1. Newsvendor Model
We consider the following distributionally robust newsvendor model:

min
ğœƒ

max
â„™âˆˆğ”¹ğœŒ,ğœ– ((cid:98)â„™)

(cid:2)ğ‘˜ğœƒ âˆ’ ğ‘¢ min(ğœƒ, ğ‘§)(cid:3),

ğ”¼â„™

where the random variable ğ‘§ stands for the random demand; its empirical distribution (cid:98)â„™ consists of
(cid:1),
ğ‘  exp (cid:0)âˆ’ ğ‘¥
ğ‘› = 20 independent samples from an exponential distribution â„™âˆ— with the density ğ‘“ (ğ‘¥;ğ‘ ) = 1
where ğ‘  âˆˆ {0.25, 0.5, 0.75, 1, 2, 4}; the decision variable ğœƒ represents the inventory level; and ğ‘˜ = 5,ğ‘¢ = 7
are constants corresponding to overage and underage costs, respectively.

ğ‘ 

Values of hyper-parameters are recorded in Table 1, from which we can see that the optimal entropic
regularization parameter ğœ– increases when the distribution scale parameter ğ‘  increases. This is because
a distribution â„™âˆ— with larger value of ğ‘  has larger variance, and to achieve better out-of-sample
performance, a larger entropic regularization parameter ğœ– is needed to encourage larger spread of
probability mass.

Table 1

Values of selected hyper-parameters by cross-validation for the newsvendor problem.

Parameter ğ‘  Regularization ğœ– Sinkhorn Radius ğœŒ Wasserstein Radius ğœŒ KL-DRO Radius ğœ‚

0.25
0.5
0.75
1
2
4

2e-2
5e-2
8e-2
2e-1
4e-1
1e-0

1e-2
6e-3
1e-3
6e-3
1e-2
6e-2

1e-3
2e-3
1e-2
3e-1
3e-1
4e-1

2e-3
7e-3
2e-2
1e-2
4e-2
5e-3

We report the violin plots, i.e., box plots with shapes formed by kernel density estimation, for the
relative performance gap across diï¬€erent approaches in Fig. 1. We ï¬nd that Sinkhorn DRO has the
best out-of-sample mean/median performance in all ï¬gures, and has the most stable performance as
indicated by the most concentrated violin plot. Wasserstein DRO, one the other hand, has a comparable
performance as SAA when ğ‘  is small (and small radius ğœŒ, as indicated in Table 1), but is even worse
than SAA for large ğ‘  (and large ğœŒ, as indicated in Table 1). This is because for small ğœŒ, the Wasserstein
robust solution coincides with the SAA solution and thus does not regularizer the problem, similar to
the observation made in [67, Remark 6.7] when the support of â„™âˆ— is â„; whereas for large ğœŒ, it hedges
distributions that are too extreme (which puts positive probability mass on zero demand), leading to

16

Figure 1

Out-of-sample performances for the newsvendor model with parameters ğ‘  âˆˆ {0.25, 0.5, 0.75, 1, 2, 4} and the ï¬xed
sample size ğ‘› = 20.

an overly conservative solution. Moreover, the KL-divergence DRO does not have much improvement
compared with the SAA model, likely because the induced worst-case distribution shares the same
support as the empirical distribution.

5.2. Mean-risk Portfolio Optimization
We consider the following distributionally robust mean-risk portfolio optimization problem

min
ğœƒ

max
â„™âˆˆğ”¹ğœŒ,ğœ– ((cid:98)â„™)
s.t.

ğ”¼â„™âˆ— [âˆ’ğœƒ Tğ‘§] + ğœš Â· â„™-CVaRğ›¼ (âˆ’ğœƒ Tğ‘§)

ğœƒ âˆˆ Î˜ = {ğœƒ âˆˆ â„ğ‘‘

+ : ğœƒ T1 = 1},

where the random vector ğ‘§ âˆˆ â„ğ‘‘ stands for the returns of assets; the decision variable ğœƒ âˆˆ Î˜ represents
the portfolio strategy that invests a certain percentage ğœƒğ‘– of the available capital in the ğ‘–-th asset; and
the term â„™-CVaRğ›¼ (âˆ’ğœƒ Tğ‘§) quantiï¬es conditional value-at-risk [82], i.e., the average of the ğ›¼ Ã— 100%
worst portfolio losses under the distribution â„™. We follow a similar setup as in [67]. Speciï¬cally, we set
ğ›¼ = 0.2, ğœš = 10. The random asset ğ‘§ âˆ¼ â„™âˆ— can be decomposed into a systematic risk factor ğœ“ âˆˆ â„ and
idiosyncratic risk factors ğœ– âˆˆ â„ğ‘‘ :

ğ‘§ğ‘– = ğœ“ + ğœ–ğ‘–,

ğ‘– = 1, 2, . . . , ğ·,

where ğœ“ âˆ¼ N (0, 0.02) and ğœ–ğ‘– âˆ¼ N (ğ‘– Ã— 0.03, ğ‘– Ã— 0.025). We ï¬xed the training sample size ğ‘› = 20 and vary
the number of assets ğ· âˆˆ {10, 20, 50}. We solve this problem using Algorithm 1, in which the projected
gradient descent step to update ğœƒ follows the implementation in [27] to project onto the probability
simplex Î˜.

The violin plots for the relative performance gap across diï¬€erent approaches are reported in Fig. 2.
Similar as the ï¬nding in Section 5.1, both the Wasserstein DRO and KL-divergence DRO models do not
outperform the SAA method too much, while Sinkhorn DRO has the best out-of-sample performance
for all plots as indicated by the smallest mean/median as well as the most concentrated violin plots,
and the contrast is more apparent when the dimension ğ· is large.

SAASinkhornWassersteinKL-divergenceMethod0.000.020.040.060.080.10Relative Performance Gapn=20 and s=0.25SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.20Relative Performance Gapn=20 and s=0.5SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.25Relative Performance Gapn=20 and s=0.75SAASinkhornWassersteinKL-divergenceMethod0.050.000.050.100.150.200.250.30Relative Performance Gapn=20 and s=1SAASinkhornWassersteinKL-divergenceMethod0.00.10.20.30.40.5Relative Performance Gapn=20 and s=2SAASinkhornWassersteinKL-divergenceMethod0.00.20.40.6Relative Performance Gapn=20 and s=417

Figure 2

Out-of-sample performances for the portfolio optimization problem with the dimension ğ· âˆˆ {10, 20, 50} and a
ï¬xed sample size ğ‘› = 20.

5.3. Semi-supervised Learning
Our last example is a semi-supervised learning task following a similar setup as in [15]. Suppose we
have a training data set Dğ‘› = {(ğ‘‹ğ‘–, ğ‘Œğ‘–)}ğ‘›
ğ‘–=1, where ğ‘Œğ‘– âˆˆ {âˆ’1, 1} denotes the label of the ğ‘–-th observation.
Additionally, we have a set of unlabeled observations {ğ‘‹ğ‘– }ğ‘
ğ‘–=ğ‘›+1 âˆª
{(ğ‘‹ğ‘–, âˆ’1)}ğ‘
ğ‘–=ğ‘›+1, which means we replicate each unlabeled data point twice, recognizing that the
missing label can be any of the two available alternatives. Then we formulate an empirical distribution
consisting of samples from the set Xğ‘ = Dğ‘› âˆª Eğ‘ âˆ’ğ‘›, which is denoted as (cid:98)â„™. Considering the following
distributionally robust formulation:

ğ‘–=ğ‘›+1. We build the set Eğ‘ âˆ’ğ‘› = {(ğ‘‹ğ‘–, 1)}ğ‘

min
ğœƒ

max
â„™âˆˆğ”¹ğœŒ,ğœ– ((cid:98)â„™)
where

(cid:2)â„“ (ğœƒ ; (ğ‘‹, ğ‘Œ ))(cid:3)

ğ”¼â„™

â„“ (ğœƒ ; (ğ‘‹, ğ‘Œ )) = log(1 + exp(âˆ’ğ‘Œ Â· ğœƒ Tğ‘‹ )).

We also solve this task using the other three benchmark models. The cost function over this subsection
is set to be

ğ‘ ((ğ‘¥,ğ‘¦), (ğ‘¥ (cid:48),ğ‘¦ (cid:48))) =

(cid:107)ğ‘¥ âˆ’ ğ‘¥ (cid:48)(cid:107)2

21{ğ‘¦ = ğ‘¦ (cid:48)} + ğœ…1{ğ‘¦ â‰  ğ‘¦ (cid:48)},

1
2

where the parameter ğœ… = âˆ means that there is no labeling error. In this case, in the duality result for
the Sinkhorn DRO, we only need to robustify the feature vectors ğ‘‹ .

We consider three performance measures for the obtained classiï¬ers: (i) the training error of samples
with known labels; (ii) the training error of samples with unknown labels; and (iii) the testing error
for new observations. The experiment is conducted using 4 binary classiï¬cation real data sets from UCI
machine learning data base [40]. In each of the repeated experiments for each data set, we randomly
partition the collected samples into training and testing data sets.

Classiï¬cation results for these diï¬€erent approaches are reported in Table 2, where the ï¬rst number
of each entry represents the average classiï¬cation error, and the second number of entry represents
the half-length of the 95% conï¬dence interval. Detailed parameters for the settings of this task and
the choices of hyper-parameters are reported in Appendix EC.1. We observe that though the Sinkhorn
DRO does not have the best in-sample performance (as indicated by the training error of samples with
known labels), it has the best out-of-sample performance for all data sets (as indicated by the smallest
training error of samples with unknown labels and the smallest testing error).

6. Concluding Remarks
In this paper, we investigated a new distributionally robust optimization framework based on the
Sinkhorn distance. By developing a strong dual reformulation and a customized batch gradient
descent with bisection search algorithm, we have shown that the resulting DRO problem is tractable

SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.250.300.35Relative Performance Gapn=20 and D=10SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.250.300.35Relative Performance Gapn=20 and D=20SAASinkhornWassersteinKL-divergenceMethod0.000.050.100.150.200.25Relative Performance Gapn=20 and D=5018

Table 2

Classiï¬cation results on real datasets for the semi-supervised learning task. Each experiment is repeated for 200

independent trials, and 95% conï¬dence intervals of classiï¬cation errors are reported for diï¬€erent approaches.

SAA

Sinkhorn

Wasserstein

KL-divergence

Train (Labeled)

.058 Â± .061

Breast Cancer

Train (Unlabeled) .20 Â± .068

Test Error

.19 Â± .073

Train (Labeled)

.17 Â± .12

Magic

Train (Unlabeled) .28 Â± .082

Test Error

.28 Â± .064

Train (Labeled)

.12 Â± .067

QSAR Bio

Train (Unlabeled) .25 Â± .057

Test Error

.25 Â± .062

Train (Labeled)

.10 Â± .046

Spambase

Train (Unlabeled) .19 Â± .038

Test Error

.19 Â± .032

.051 Â± .065
.12 Â± .068
.12 Â± .068
.12 Â± .068
.11 Â± .067
.11 Â± .067
.11 Â± .067

.18 Â± .11
.25 Â± .091
.25 Â± .091
.25 Â± .091
.25 Â± .074
.25 Â± .074
.25 Â± .074

.15 Â± .076
.22 Â± .063
.22 Â± .063
.22 Â± .063
.22 Â± .065
.22 Â± .065
.22 Â± .065

.10 Â± .048
.14 Â± .046
.14 Â± .046
.14 Â± .046
.14 Â± .036
.14 Â± .036
.14 Â± .036

.051 Â± .063
.051 Â± .063
.051 Â± .063

.057 Â± .060

.17 Â± .073

.17 Â± .075

.17 Â± .11

.27 Â± .077

.27 Â± .058

.16 Â± .073

.23 Â± .073

.23 Â± .079

.096 Â± .045
.096 Â± .045
.096 Â± .045

.16 Â± .036

.16 Â± .028

.19 Â± .038

.19 Â± .073

.15 Â± .12
.15 Â± .12
.15 Â± .12

.26 Â± .078

.27 Â± .066

.11 Â± .066
.11 Â± .066
.11 Â± .066

.25 Â± .037

.25 Â± .042

.10 Â± .043

.18 Â± .034

.18 Â± .042

under mild assumptions, greatly spans the tractability of Wasserstein DRO. Analysis on the worst-case
distribution indicates that Sinkhorn DRO hedges a more reasonable set of adverse scenarios and
thus less conservative compared with Wasserstein DRO, which is then demonstrated via extensive
numerical experiments. Based on theoretical and numerical ï¬ndings, we conclude that the Sinkhorn
distance is a promising candidate for modeling distributional ambiguities in decision-making under
uncertainty from the perspective of computational tractability, modeling rationality and out-of-sample
performance.

In the meantime, several topics worth in-depth investigating are left for future works. It is interesting
to design eï¬ƒcient ï¬rst-order algorithms for Sinkhorn DRO when the nominal distribution is arbitrary
or when the loss function is non-convex in the decision variable. Moreover, a meaningful research
question is the choice of the optimal hyper-parameters in Sinkhorn DRO, such as the radius of the
ambiguity set ğœŒ, the entropic regularization parameters ğœ–, and reference measures ğœˆ. This paper
focuses on regularizing Wasserstein distance with the entropic regularization â€“ the Sinkhorn distance,
but extensions to other types of regularization are possible. Exploring and discovering the beneï¬ts of
Sinkhorn DRO in other types of applications may lead to future research directions.

Appendix A: Sufï¬cient condition for Assumption 1
Proposition 3. Assumption 1 holds if there exists ğ‘ â‰¥ 1 so that the following conditions are satisï¬ed:

(I) For any ğ‘¥,ğ‘¦, ğ‘§ âˆˆ Z, ğ‘ (ğ‘¥,ğ‘¦) â‰¥ 0, and

(ğ‘ (ğ‘¥,ğ‘¦))1/ğ‘ â‰¤ (ğ‘ (ğ‘¥, ğ‘§))1/ğ‘ + (ğ‘ (ğ‘§,ğ‘¦))1/ğ‘ .

(II) The nominal distribution (cid:98)â„™ has a ï¬nite mean, denoted as ğ‘¥. Moreover, ğœˆ {ğ‘§ : 0 â‰¤ ğ‘ (ğ‘¥, ğ‘§) < âˆ} = 1

and

Pr ğ‘¥âˆ¼(cid:98)â„™{ğ‘ (ğ‘¥, ğ‘¥) < âˆ} = 1.

19

(III) There exists ğœ† > 0 such that

âˆ«

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–)ğ‘’âˆ’21âˆ’ğ‘ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) < âˆ.

We make some remarks for the suï¬ƒcient conditions listed above. The ï¬rst condition can be satisï¬ed
by taking the cost function as the ğ‘-th power of the metric deï¬ned on Z for any ğ‘ â‰¥ 1. The second
condition requires the nominal distribution (cid:98)â„™ is ï¬nite almost surely, e.g., it can be a subguassian
distribution with respect to the cost function ğ‘. Combining three conditions together and levering
concentration arguments completes the proof of Proposition 3.

References

[1] Abdullah MA, Ren H, Ammar HB, Milenkovic V, Luo R, Zhang M, Wang J (2019) Wasserstein robust

reinforcement learning. arXiv preprint arXiv:1907.13196 .

[2] Agrawal S, Ding Y, Saberi A, Ye Y (2012) Price of correlations in stochastic optimization. Operations Research

60(1):150â€“162.

[3] Altschuler J, Weed J, Rigollet P (2017) Near-linear time approximation algorithms for optimal transport via

sinkhorn iteration. Advances in Neural Information Processing Systems, 1961â€“1971.

[4] ApS M (2021) Mosek modeling cookbook 3.2.3. https://docs.mosek.com/modeling-cookbook/index.

html#.

[5] Asmussen S, Glynn PW (2007) Stochastic simulation: algorithms and analysis, volume 57 (Springer Science

& Business Media).

[6] Aziz I, Haq F, et al. (2010) A comparative study of numerical integration based on haar wavelets and hybrid

functions. Computers & Mathematics with Applications 59(6):2026â€“2036.

[7] Bacharach M (1965) Estimating nonnegative matrices from marginal data. International Economic Review

6(3):294â€“310.

[8] Bai Y, Wu X, Ozgur A (2020) Information constrained optimal transport: From talagrand, to marton, to

cover. 2020 IEEE International Symposium on Information Theory (ISIT), 2210â€“2215.

[9] Bayraksan G, Love DK (2015) Data-driven stochastic programming using phi-divergences. The Operations

Research Revolution, 1â€“19 (INFORMS).

[10] Ben-Tal A, den Hertog D, De Waegenaere A, Melenberg B, Rennen G (2013) Robust solutions of optimization

problems aï¬€ected by uncertain probabilities. Management Science 59(2):341â€“357.

[11] Bertsimas D, Natarajan K, Teo CP (2006) Persistence in discrete optimization under data uncertainty.

Mathematical programming 108(2):251â€“274.

[12] Bertsimas D, Sim M, Zhang M (2019) Adaptive distributionally robust optimization. Management Science

65(2):604â€“618.

[13] Blanchet J, Chen L, Zhou XY (2018) Distributionally robust mean-variance portfolio selection with

wasserstein distances. arXiv preprint arXiv:1802.04885 .

[14] Blanchet J, Glynn PW, Yan J, Zhou Z (2019) Multivariate distributionally robust convex regression under

absolute error loss. Advances in Neural Information Processing Systems, volume 32, 11817â€“11826.

[15] Blanchet J, Kang Y (2020) Semi-supervised learning based on distributionally robust optimization. Data

Analysis and Applications 3 1â€“33.

[16] Blanchet J, Kang Y, Murthy K (2019) Robust wasserstein proï¬le inference and applications to machine

learning. Journal of Applied Probability 56(3):830â€“857.

[17] Blanchet J, Murthy K (2019) Quantifying distributional model risk via optimal transport. Mathematics of

Operations Research 44(2):565â€“600.

[18] Blanchet J, Murthy K, Nguyen VA (2021) Statistical analysis of wasserstein distributionally robust estimators.

arXiv preprint arXiv:2108.02120 .

20

[19] Blanchet J, Murthy K, Si N (2021) Conï¬dence regions in wasserstein distributionally robust estimation.

arXiv preprint arXiv:1906.01614 .

[20] Blanchet J, Murthy K, Zhang F (2021) Optimal transport based distributionally robust optimization:

Structural properties and iterative schemes. arXiv preprint arXiv:1810.02403 .

[21] Blanchet JH, Glynn PW (2015) Unbiased monte carlo for optimization and functions of expectations via

multi-level randomization. 2015 Winter Simulation Conference (WSC), 3656â€“3667.
[22] Boyd S, Vandenberghe L (2004) Convex optimization (Cambridge university press).
[23] Chang JT, Pollard D (2001) Conditioning as disintegration. Statistica Neerlandica 51(3):287â€“317.
[24] Chen R, Paschalidis IC (2018) A robust learning approach for regression models based on distributionally

robust optimization. Journal of Machine Learning Research 19(13):1â€“48.

[25] Chen R, Paschalidis IC (2019) Selecting optimal decisions via distributionally robust nearest-neighbor

regression. Advances in Neural Information Processing Systems.

[26] Chen Y, Li W (2021) Natural gradient in wasserstein statistical manifold. arXiv preprint arXiv:1805.08380 .
[27] Chen Y, Ye X (2011) Projection onto a simplex. arXiv preprint arXiv:1101.6081 .
[28] Chen Z, Kuhn D, Wiesemann W (2018) Data-driven chance constrained programs over wasserstein balls.

arXiv preprint arXiv:1809.00210 .

[29] Chen Z, Sim M, Xu H (2019) Distributionally robust optimization with inï¬nitely constrained ambiguity

sets. Operations Research 67(5):1328â€“1344.

[30] Cherukuri A, CortÃ©s J (2019) Cooperative data-driven distributionally robust optimization. IEEE Transactions

on Automatic Control 65(10):4400â€“4407.

[31] Courty N, Flamary R, Habrard A, Rakotomamonjy A (2017) Joint distribution optimal transportation for

domain adaptation. Advances in Neural Information Processing Systems.

[32] Courty N, Flamary R, Tuia D (2014) Domain adaptation with regularized optimal transport. Joint European

Conference on Machine Learning and Knowledge Discovery in Databases, 274â€“289.

[33] Courty N, Flamary R, Tuia D, Rakotomamonjy A (2016) Optimal transport for domain adaptation. IEEE

Transactions on Pattern Analysis and Machine Intelligence 39(9):1853â€“1865.
[34] Cover TM, Thomas JA (2006) Elements of Information Theory (Wiley-Interscience).
[35] Cuturi M (2013) Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural

information processing systems, volume 26, 2292â€“2300.

[36] Delage E, Ye Y (2010) Distributionally robust optimization under moment uncertainty with application to

data-driven problems. Operations Research 58(3):595â€“612.

[37] Deming WE, Stephan FF (1940) On a least squares adjustment of a sampled frequency table when the

expected marginal totals are known. The Annals of Mathematical Statistics 11(4):427â€“444.

[38] Derman E, Mannor S (2020) Distributional robustness and regularization in reinforcement learning. arXiv

preprint arXiv:2003.02894 .

[39] Doan XV, Natarajan K (2012) On the complexity of nonoverlapping multivariate marginal bounds for

probabilistic combinatorial optimization problems. Operations research 60(1):138â€“149.

[40] Dua D, Graï¬€ C (2017) UCI machine learning repository. [Online]. Available: http://archive.ics.uci.

edu/ml .

[41] Duchi JC, Glynn PW, Namkoong H (2021) Statistics of robust optimization: A generalized empirical

likelihood approach. Mathematics of Operations Research 0(0).

[42] Eckstein S, Kupper M, Pohl M (2020) Robust risk aggregation with neural networks. Mathematical Finance

30(4):1229â€“1272.

[43] FrÃ©chet M (1960) Sur les tableaux dont les marges et des bornes sont donnÃ©es. Revue de lâ€™Institut

international de statistique 10â€“32.

[44] Gao R (2020) Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the

curse of dimensionality. arXiv preprint arXiv:2009.04382 .

21

[45] Gao R, Chen X, Kleywegt AJ (2020) Wasserstein distributionally robust optimization and variation

regularization. arXiv preprint arXiv:1712.06050 .

[46] Gao R, Kleywegt AJ (2016) Distributionally robust stochastic optimization with Wasserstein distance. arXiv

preprint arXiv:1604.02199 .

[47] Gao R, Kleywegt AJ (2017) Data-driven robust optimization with known marginal distributions. Working

paper. Available at https://faculty.mccombs.utexas.edu/rui.gao/copula.pdf .

[48] Gao R, Kleywegt AJ (2017) Distributionally robust stochastic optimization with dependence structure. arXiv

preprint arXiv:1701.04200 .

[49] Genevay A, Cuturi M, PeyrÃ© G, Bach F (2016) Stochastic optimization for large-scale optimal transport.

Advances in Neural Information Processing Systems, volume 29.

[50] Genevay A, Peyre G, Cuturi M (2018) Learning generative models with sinkhorn divergences. Proceedings
of the Twenty-First International Conference on Artiï¬cial Intelligence and Statistics, volume 84 of Proceedings
of Machine Learning Research, 1608â€“1617 (PMLR).

[51] Ghadimi S, Ruszczynski A, Wang M (2020) A single timescale stochastic approximation method for nested

stochastic optimization. SIAM Journal on Optimization 30(1):960â€“979.

[52] Goh J, Sim M (2010) Distributionally robust optimization and its tractable approximations. Operations

Research 58(4-part-1):902â€“917.

[53] Grant M, Boyd S (2014) CVX: Matlab software for disciplined convex programming, version 2.1. http:

//cvxr.com/cvx.

[54] HÃ¤rdle W (1990) Applied nonparametric regression (Cambridge university press).
[55] Hu Y, Chen X, He N (2020) Sample complexity of sample average approximation for conditional stochastic

optimization. SIAM Journal on Optimization 30(3):2103â€“2133.

[56] Hu Y, Zhang S, Chen X, He N (2020) Biased stochastic ï¬rst-order methods for conditional stochastic
optimization and applications in meta learning. Advances in Neural Information Processing Systems,
volume 33, 2759â€“2770.

[57] Hu Z, Hong LJ (2012) Kullback-leibler divergence constrained distributionally robust optimization. Opti-

mization Online preprint Optimization Online:2012/11/3677 .

[58] Huang M, Ma S, Lai L (2021) A riemannian block coordinate descent method for computing the projection
robust wasserstein distance. Proceedings of the 38th International Conference on Machine Learning, 4446â€“
4455.

[59] Jain P, Kar P (2017) Non-convex optimization for machine learning. Foundations and Trends in Machine

Learning 10(3-4):142â€“363.

[60] Kruithof J (1937) Telefoonverkeersrekening. De Ingenieur 52:15â€“25.
[61] Kuhn D, Esfahani PM, Nguyen VA, Shaï¬eezadeh-Abadeh S (2019) Wasserstein distributionally robust
optimization: Theory and applications in machine learning. Operations Research & Management Science in
the Age of Analytics, 130â€“166 (INFORMS).

[62] Li J, Huang S, So AMC (2019) A ï¬rst-order algorithmic framework for wasserstein distributionally robust
logistic regression. Proceedings of the 33rd International Conference on Neural Information Processing Systems,
3937â€“3947.

[63] Li W, Montufar G (2021) Natural gradient via optimal transport. arXiv preprint arXiv:1803.07033 .
[64] Lin T, Fan C, Ho N, Cuturi M, Jordan M (2020) Projection robust wasserstein distance and riemannian

optimization. Advances in Neural Information Processing Systems, volume 33, 9383â€“9397.

[65] Luise G, Rudi A, Pontil M, Ciliberto C (2018) Diï¬€erential properties of sinkhorn approximation for learning

with wasserstein distance. Advances in Neural Information Processing Systems.

[66] Luo F, Mehrotra S (2019) Decomposition algorithm for distributionally robust optimization using wasserstein
metric with an application to a class of regression models. European Journal of Operational Research
278(1):20â€“35.

22

[67] Mohajerin Esfahani P, Kuhn D (2017) Data-driven distributionally robust optimization using the wasserstein
metric: performance guarantees and tractable reformulations. Mathematical Programming 171(1):115â€“166.
[68] Namkoong H, Duchi JC (2016) Stochastic gradient methods for distributionally robust optimization with

f-divergences. Advances in Neural Information Processing Systems, volume 29, 2208â€“2216.

[69] Natarajan K, Song M, Teo CP (2009) Persistency model and its applications in choice modeling. Management

Science 55(3):453â€“469.

[70] Nemirovski A (2002) Lectures on modern convex optimization. Society for Industrial and Applied Mathematics

(SIAM.

[71] Nesterov Y, Nemirovskii A (1994) Interior-point polynomial algorithms in convex programming (SIAM).
[72] Nguyen VA, Si N, Blanchet J (2020) Robust bayesian classiï¬cation using an optimistic score ratio.

International Conference on Machine Learning, 7327â€“7337.

[73] Nguyen VA, Zhang F, Blanchet J, Delage E, Ye Y (2021) Robustifying conditional portfolio decisions via

optimal transport. arXiv preprint arXiv:2103.16451 .

[74] Niederreiter H (1992) Random number generation and quasi-Monte Carlo methods (SIAM).
[75] Patrini G, van den Berg R, Forre P, Carioni M, Bhargav S, Welling M, Genewein T, Nielsen F (2020) Sinkhorn

autoencoders. Uncertainty in Artiï¬cial Intelligence, 733â€“743.

[76] Petzka H, Fischer A, Lukovnikov D (2018) On the regularization of wasserstein GANs. International

Conference on Learning Representations.

[77] Peyre G, Cuturi M (2019) Computational optimal transport: With applications to data science. Foundations

and Trends in Machine Learning 11(5-6):355â€“607.

[78] Pï¬‚ug G, Wozabal D (2007) Ambiguity in portfolio selection. Quantitative Finance 7(4):435â€“442.
[79] Pichler A, Shapiro A (2021) Mathematical foundations of distributionally robust multistage optimization.

arXiv preprint arXiv:2101.02498 .

[80] Popescu I (2005) A semideï¬nite programming approach to optimal-moment bounds for convex classes of

distributions. Mathematics of Operations Research 30(3):632â€“657.

[81] Rahimian H, Mehrotra S (2019) Distributionally robust optimization: A review. arXiv preprint

arXiv:1908.05659 .

[82] Rockafellar RT, Uryasev S, et al. (1999) Optimization of conditional value-at-risk. Journal of risk 2:21â€“42.
[83] Scarf H (1957) A min-max solution of an inventory problem. Studies in the mathematical theory of inventory

and production .

[84] Shaï¬eezadeh-Abadeh S, Kuhn D, Esfahani PM (2019) Regularization via mass transportation. Journal of

Machine Learning Research 20(103):1â€“68.

[85] Shaï¬eezadeh Abadeh S, Mohajerin Esfahani PM, Kuhn D (2015) Distributionally robust logistic regression.

Advances in Neural Information Processing Systems, volume 28.

[86] Shapiro A, Dentcheva D, RuszczyÅ„ski A (2014) Lectures on stochastic programming: modeling and theory

(SIAM).

[87] Singh D, Zhang S (2020) Tight bounds for a class of data-driven distributionally robust risk measures. arXiv

preprint arXiv:2010.05398 .

[88] Singh D, Zhang S (2021) Distributionally robust proï¬t opportunities. Operations Research Letters 49(1):121â€“

128.

[89] Sinha A, Namkoong H, Duchi J (2018) Certiï¬able distributional robustness with principled adversarial

training. International Conference on Learning Representations.

[90] Sinkhorn R (1964) A relationship between arbitrary positive matrices and doubly stochastic matrices. The

annals of mathematical statistics 35(2):876â€“879.

[91] Smirnova E, Dohmatob E, Mary J (2019) Distributionally robust reinforcement learning. arXiv preprint

arXiv:1902.08708 .

23

[92] Staib M, Jegelka S (2019) Distributionally robust optimization and generalization in kernel methods.

Advances in Neural Information Processing Systems 32:9134â€“9144.

[93] Taskesen B, Nguyen VA, Kuhn D, Blanchet J (2020) A distributionally robust approach to fair classiï¬cation.

arXiv preprint arXiv:2007.09530 .

[94] Van Parys BP, Goulart PJ, Kuhn D (2015) Generalized gauss inequalities via semideï¬nite programming.

Mathematical Programming 156(1-2):271â€“302.

[95] Vandenberghe L, Boyd S (1995) Semideï¬nite programming. SIAM review 38(1):49â€“95.
[96] Wang C, Gao R, Qiu F, Wang J, Xin L (2018) Risk-based distributionally robust optimal power ï¬‚ow with

dynamic line rating. IEEE Transactions on Power Systems 33(6):6074â€“6086.

[97] Wang J, Gao R, Xie Y (2021) Two-sample test using projected wasserstein distance. 2021 IEEE International

Symposium on Information Theory (ISIT).

[98] Wang J, Gao R, Xie Y (2021) Two-sample test with kernel projected wasserstein distance. arXiv preprint

arXiv:2102.06449 .

[99] Wang J, Gao R, Zha H (2021) Reliable oï¬€-policy evaluation for reinforcement learning. arXiv preprint

arXiv:2011.04102 .

[100] Wang J, Jia Z, Yin H, Yang S (2021) Small-sample inferred adaptive recoding for batched network coding.

2021 IEEE International Symposium on Information Theory (ISIT).

[101] Wang M, Fang EX, Liu H (2016) Stochastic compositional gradient descent: algorithms for minimizing

compositions of expected-value functions. Mathematical Programming 161(1-2):419â€“449.

[102] Wang Z, Glynn PW, Ye Y (2015) Likelihood robust optimization for data-driven problems. Computational

Management Science 13(2):241â€“261.

[103] Wiesemann W, Kuhn D, Sim M (2014) Distributionally robust convex optimization. Operations Research

62(6):1358â€“1376.

[104] Wozabal D (2012) A framework for optimization under ambiguity. Annals of Operations Research 193(1):21â€“

47.

[105] Xie W (2019) On distributionally robust chance constrained programs with wasserstein distance. Mathe-

matical Programming 186(1):115â€“155.

[106] Yang I (2017) A convex optimization approach to distributionally robust markov decision processes with

wasserstein distance. IEEE control systems letters 1(1):164â€“169.

[107] Yang I (2020) Wasserstein distributionally robust stochastic control: A data-driven approach. IEEE

Transactions on Automatic Control 66(8):3863â€“3870.

[108] Yang S, Wang M, Fang EX (2019) Multilevel stochastic gradient methods for nested composition optimization.

SIAM Journal on Optimization 29(1):616â€“659.

[109] Yule GU (1912) On the methods of measuring association between two attributes. Journal of the Royal

Statistical Society 75(6):579â€“652.

[110] Zhao C, Guan Y (2018) Data-driven risk-averse stochastic optimization with wasserstein metric. Operations

Research Letters 46(2):262â€“267.

[111] Zhu J, Jitkrittum W, Diehl M, SchÃ¶lkopf B (2021) Kernel distributionally robust optimization: Generalized
duality theorem and stochastic approximation. Proceedings of The 24th International Conference on Artiï¬cial
Intelligence and Statistics, 280â€“288.

[112] Zymler S, Kuhn D, Rustem B (2013) Distributionally robust joint chance constraints with second-order

moment information. Mathematical Programming 137(1):167â€“198.

ec1

Supplementary for â€œSinkhorn Distributionally Robust Optimizationâ€

Appendix EC.1: Detailed Experiment Setup
All the experiments are preformed on a MacBook Pro laptop with 32GB of memory running python 3.7.
Candidates of hyper-parameters for DRO models are listed as follows. In each experiment we pick
the regularization term ğœ– spaced from 1e-3 to 9e-1 in exponentially increasing steps. The Sinkhorn
radius ğœŒ is chosen spaced from 1e-5 to 1e-1 in exponentially increasing steps. The Wasserstein
radius ğœŒ and KL-DRO radius ğœ‚ are chosen spaced from 1e-3 to 9e-1 in exponentially increasing steps.
Hyper-parameters for the second and third experiments are reported in Table EC.1 and Table EC.2,
respectively. To obtain the Monte Carlo approximated objective function for the Sinkhorn DRO model,
we take the nominal distribution (cid:98)â„™ as the empirical distribution based on collected samples, and the
inner batch size ğ‘š = 20. We use the projected gradient descent method to solve the subproblem in
(10). For portfolio optimization problems we try the step size ğœ‚â„“ = 1
for the â„“-th inner iteration.
âˆš
â„“+1
Otherwise we try the step size ğœ‚â„“ = 1
â„“+1 during the â„“-th inner iteration. Denote by objâ„“ the objective
function obtained at the â„“-th iteration. The inner iteration is terminated when (cid:107)objâ„“ +1âˆ’objâ„“ (cid:107)
1+ (cid:107)objâ„“ (cid:107) â‰¤ 1e-3.
The SAA, Wasserstein DRO, and KL-divergence DRO models are solved exactly based on the interior
point method-based solver Mosek [4]. In particular, based on [67, Corollary 5.1], the Wasserstein DRO
formulation for the newsvendor problem in Section 5.1 becomes

min
ğœƒ,ğœ†,ğ‘ ,ğ›¾

ğœ†ğœŒ +

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘ ğ‘–

s.t.

(ğ‘˜ âˆ’ ğ‘¢)ğœƒ + ğ›¾ğ‘–,1Ë†ğ‘§ğ‘– â‰¤ ğ‘ ğ‘–, ğ‘– âˆˆ [ğ‘›],

ğ‘˜ğœƒ âˆ’ ğ‘¢Ë†ğ‘§ğ‘– + ğ›¾ğ‘–,1Ë†ğ‘§ğ‘– â‰¤ ğ‘ ğ‘–, ğ‘– âˆˆ [ğ‘›],

ğ›¾ğ‘–,1 â‰¤ ğœ†, ğ‘– âˆˆ [ğ‘›],

| âˆ’ ğ›¾ğ‘–,2 + ğ‘¢ | â‰¤ ğœ†, ğ‘– âˆˆ [ğ‘›],

ğ›¾ â‰¥ 0,

where {Ë†ğ‘§1, . . . , Ë†ğ‘§ğ‘›} denotes collected samples from (cid:98)â„™âˆ—. From [67, Eq. (27)] we can see that the
Wasserstein DRO formulation for the portfolio optimization problem becomes

min
ğœƒ,ğœ,ğœ†,ğ‘ 

ğœ†ğœŒ +

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘ ğ‘–

s.t.

ğœƒ âˆˆ Î˜,

ğ‘ ğ‘—ğœ + ğ‘ ğ‘— (cid:104)ğœƒ, Ë†ğ‘§ğ‘–(cid:105) â‰¤ ğ‘ ğ‘–, ğ‘– âˆˆ [ğ‘›], ğ‘— âˆˆ [ğ» ],

(cid:107)ğ‘ ğ‘—ğœƒ (cid:107)2 â‰¤ ğœ†, ğ‘— âˆˆ [ğ» ].

Recall that the KL-divergence DRO problem with radius ğœ‚ â‰¥ 0 has the following tractable formulation:

min
ğœƒ âˆˆÎ˜,ğœ† â‰¥0

(cid:110)

ğœ†ğœ‚ + ğœ† log

(cid:16)

ğ”¼
(cid:98)â„™

ğ‘’ ğ‘“ğœƒ (ğ‘§)/ğœ†(cid:105) (cid:17)(cid:111)
(cid:104)

.

ec2

Table EC.1

Values of selected hyper-parameters by cross-validation for the portfolio optimization problem.

Dimension ğ· Regularization ğœ– Sinkhorn Radius ğœŒ Wasserstein Radius ğœŒ KL-DRO Radius ğœ‚

10

20

50

5e-2

7e-1

4e-1

2e-4

3e-4

2e-4

6e-3

5e-2

1e-3

1e-3

2e-3

2e-3

Table EC.2

Values of classiï¬cation parameters and hyper-parameters for DRO models.

Breast Cancer

Magic

QSAR Bio

Spambase

Number of Predictors

Train Size (Labeled)

Train Size (Unlabeled)

Test Size

30

40

200

329

10

30

300

18690

30

80

500

475

56

150

600

3850

Sinkhorn Para. (ğœŒ, ğœ–)

(2e-1,2e-5)

(5e-1,6e-2)

(8e-1,2e-3)

(9e-3,2e-5)

Wasserstein Para. ğœŒ

KL-DRO Para. ğœ‚

1e-3

2e-3

3e-3

5e-3

3e-1

3e-2

1e-2

4e-2

ec3

Appendix EC.2: Proofs of Technical Results in Section 3.2
In order to show the strong duality result in Theorem 1 when (cid:98)â„™ is an empirical distribution, we present
the following technical lemma.
Lemma EC.1. For ï¬xed ğœ and a reference probability distribution â„š âˆˆ P (Z), consider the optimization
problem

ğ‘£ (ğœ) = sup

â„™âˆˆP (Z)

(cid:26)

(cid:20)

ğ”¼â„™

ğ‘“ (ğ‘§) âˆ’ ğœ log

(cid:19)(cid:21) (cid:27)

(cid:18) dâ„™
dâ„š

(ğ‘§)

.

(EC.1)

(I) When ğœ = 0,

(II) When ğœ > 0 and

it holds that

ğ‘£ (0) = ess sup

(ğ‘“ ) (cid:44) inf{ğ‘¡ âˆˆ â„ : Pr ğ‘§âˆ¼â„š{ğ‘“ (ğ‘§) > ğ‘¡ } = 0}.

â„š

ğ‘’ ğ‘“ (ğ‘§)/ğœ (cid:105)
(cid:104)

< âˆ,

ğ”¼â„š

ğ‘£ (ğœ) = ğœ log

(cid:16)

ğ”¼â„š

ğ‘’ ğ‘“ (ğ‘§)/ğœ (cid:105) (cid:17)
(cid:104)

,

and limğœ â†“0 ğ‘£ (ğœ) = ğ‘£ (0). The optimal solution in (EC.1) has the expression

(III) When ğœ > 0 and

we have that ğ‘£ (ğœ) = âˆ.

dâ„™(ğ‘§) =

ğ‘’ ğ‘“ (ğ‘§)/ğœ
âˆ« ğ‘’ ğ‘“ (ğ‘¢)/ğœ dâ„š(ğ‘¢)

dâ„š(ğ‘§).

ğ‘’ ğ‘“ (ğ‘§)/ğœ (cid:105)
(cid:104)

= âˆ,

ğ”¼â„š

Proof of Lemma EC.1 We reformulate ğ‘£ (ğœ) based on the importance sampling trick:
(cid:27)

(cid:26)âˆ«

âˆ«

(cid:2)ğ‘“ (ğ‘§)ğ¿(ğ‘§) âˆ’ ğœğ¿(ğ‘§) log ğ¿(ğ‘§)(cid:3) dâ„š(ğ‘§) :

ğ¿(ğ‘§) dâ„š(ğ‘§) = 1

.

ğ‘£ (ğœ) = sup
ğ¿: ğ¿ â‰¥0

Then the remaining part follows the discussion in [57, Section 2.1].

(cid:3)

Proof of Corollary 1 We now introduce the epi-graphical variables ğ‘ ğ‘–, ğ‘– = 1, . . . , ğ‘› to reformulate ğ‘‰D as

inf
ğœ† â‰¥0,ğ‘ ğ‘–

ğœ†ğœŒ +

1
ğ‘›

s.t. ğœ†ğœ– log

ğ‘›
âˆ‘ï¸

ğ‘ ğ‘–

ğ‘–=1
(cid:16)

ğ”¼â„šğ‘–,ğœ–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

â‰¤ ğ‘ ğ‘–, âˆ€ğ‘–

ğ‘‰D =

ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´
ï£³

For ï¬xed ğ‘–, the ğ‘–-th constraint can be reformulated as
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:111)
(cid:104)
(cid:21) (cid:27)

(cid:16) ğ‘ ğ‘–
ğœ†ğœ–

exp

(cid:26)

(cid:110)

(cid:17)

â‰¥ ğ”¼â„šğ‘–,ğœ–
(cid:20)
ğ‘’

(cid:2)ğ‘“ (ğ‘§)âˆ’ğ‘ ğ‘–

(cid:3) /(ğœ†ğœ–)

(cid:20)

ğœ†ğœ–ğ‘’

(cid:2)ğ‘“ (ğ‘§)âˆ’ğ‘ ğ‘–

(cid:3) /(ğœ†ğœ–)

(cid:21) (cid:27)

=

1 â‰¥ ğ”¼â„šğ‘–,ğœ–

(cid:26)

=

ğœ†ğœ– â‰¥ ğ”¼â„šğ‘–,ğœ–
(cid:40)

ğ¿
âˆ‘ï¸

â„“=1

=

ğœ†ğœ– â‰¥

â„šğ‘–,ğœ– (ğ‘§â„“ )ğ‘ğ‘–,â„“

(cid:41)

(cid:92)

(cid:26)
ğ‘ğ‘–,â„“ â‰¥ ğœ†ğœ– exp

(cid:19)

(cid:18) ğ‘“ (ğ‘§â„“ ) âˆ’ ğ‘ ğ‘–
ğœ†ğœ–

(cid:27)

,

, âˆ€â„“

where the second constraint set can be formulated as

(ğœ†ğœ–, ğ‘ğ‘–,â„“, ğ‘“ (ğ‘§â„“ ) âˆ’ ğ‘ ğ‘–) âˆˆ Kexp.

Substituting this expression into ğ‘‰D completes the proof.

(cid:3)

ec4

Appendix EC.3: Proof of the Technical Result in Section 3.1
Proof of Remark 3 We can reformulate the dual objective function as

ğ‘£ (ğœ†; ğœ–) = ğœ†ğœŒ + ğœ†ğœ–

âˆ«

(cid:18)âˆ«

log

exp

(cid:18) ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§)
ğœ†ğœ–

(cid:19)

(cid:19)

dğœˆ (ğ‘§)

d(cid:98)â„™(ğ‘¥).

We take limit for the second term in ğ‘£ (ğœ†; ğœ–) to obtain:

lim
ğœ–â†’0
âˆ«

âˆ«

ğœ†ğœ–

log

ğœ†
ğ›½

log

lim
ğ›½â†’âˆ

(cid:18)âˆ«

(cid:32)âˆ«

exp

exp

âˆ«

ğœ†âˆ‡ log

lim
ğ›½â†’âˆ

(cid:32)âˆ«

exp

(cid:19)

(cid:18) ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§)
ğœ†ğœ–
(cid:32) (cid:2)ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§)(cid:3) ğ›½
ğœ†
(cid:32) (cid:2)ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§)(cid:3) ğ›½
ğœ†

(cid:33)

=

=

dğœˆ (ğ‘§)

(cid:19)

d(cid:98)â„™(ğ‘¥)
(cid:33)

dğœˆ (ğ‘§)

d(cid:98)â„™(ğ‘¥)

(cid:33)

(cid:33)

dğœˆ (ğ‘§)

d(cid:98)â„™(ğ‘¥)

âˆ«

=

âˆ«

=

lim
ğ›½â†’âˆ

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°
sup
ğ‘§

âˆ« exp

(cid:18) (cid:2)ğ‘“ (ğ‘§)âˆ’ğœ†ğ‘ (ğ‘¥,ğ‘§)(cid:3) ğ›½
ğœ†

(cid:19)

(cid:2)ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§)(cid:3) dğœˆ (ğ‘¦)

âˆ« exp

(cid:18) (cid:2)ğ‘“ (ğ‘§)âˆ’ğœ†ğ‘ (ğ‘¥,ğ‘§)(cid:3) ğ›½
ğœ†

(cid:19)

dğœˆ (ğ‘¦)

d(cid:98)â„™(ğ‘¥)

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

(cid:2)ğ‘“ (ğ‘§) âˆ’ ğœ†ğ‘ (ğ‘¥, ğ‘§)(cid:3) d(cid:98)â„™(ğ‘¥).

Hence, we conclude that the dual objective function of the Sinkhorn DRO problem converges into that
(cid:3)
of the Wasserstein DRO problem.

ec5

Appendix EC.4: Proofs of Technical Results in Section 3.3

Proof of Lemma 1 Recall from Remark 4 that the primal problem ğ‘‰ can be reformulated as

(cid:26)âˆ«

ğ‘‰ =

sup
ğ›¾ğ‘¥ âˆˆP (Z),âˆ€ğ‘¥ âˆˆZ

ğ”¼ğ›¾ğ‘¥ [ğ‘“ (ğ‘§)] d(cid:98)â„™(ğ‘¥) : ğœ–

âˆ«

ğ”¼ğ›¾ğ‘¥

(cid:20)

log

(cid:19)(cid:21)

(cid:18) dğ›¾ğ‘¥ (ğ‘§)
dâ„šğ‘– (ğ‘§)

d(cid:98)â„™(ğ‘¥) â‰¤ ğœŒ

(cid:27)

.

Introducing the Lagrange multiplier ğœ† associated to the constraint, we reformulate ğ‘‰ as

ğ‘‰ =

sup
ğ›¾ğ‘¥ âˆˆP (Z),âˆ€ğ‘¥ âˆˆZ

(cid:26)

(cid:26)

inf
ğœ† â‰¥0

âˆ«

ğœ†ğœŒ +

(cid:20)

ğ”¼ğ›¾ğ‘¥

ğ‘“ (ğ‘§) âˆ’ ğœ†ğœ– log

(cid:19)(cid:21)

(cid:18) dğ›¾ğ‘¥ (ğ‘§)
dâ„šğ‘¥,ğœ– (ğ‘§)

(cid:27)(cid:27)

d(cid:98)â„™(ğ‘¥)

Interchanging the order of the supremum and inï¬mum operators, we have that

(cid:40)

ğ‘‰ â‰¤ inf
ğœ† â‰¥0

ğœ†ğœŒ +

sup
ğ›¾ğ‘¥ âˆˆP (Z),âˆ€ğ‘¥ âˆˆZ

(cid:26)âˆ«

(cid:20)

ğ”¼ğ›¾ğ‘¥

ğ‘“ (ğ‘§) âˆ’ ğœ†ğœ– log

(cid:19)(cid:21)

(cid:18) dğ›¾ğ‘¥ (ğ‘§)
dâ„šğ‘¥,ğœ– (ğ‘§)

(cid:27)(cid:41)

d(cid:98)â„™(ğ‘¥)

.

.

Since the optimization over ğ›¾ğ‘¥, âˆ€ğ‘¥ is separable for each ğ‘¥, by deï¬ning

ğ‘£ğ‘¥ (ğœ†) = sup

ğ›¾ğ‘¥ âˆˆP (Z)

(cid:26)

(cid:20)

ğ”¼ğ›¾ğ‘¥

ğ‘“ (ğ‘§) âˆ’ ğœ†ğœ– log

(cid:19)(cid:21) (cid:27)

(cid:18) dğ›¾ğ‘¥ (ğ‘§)
dâ„šğ‘¥,ğœ– (ğ‘§)

, âˆ€ğ‘¥,

and swap the supremum and the integration, we obtain

âˆ«

(cid:26)

ğœ†ğœŒ +

ğ‘£ğ‘¥ (ğœ†) d(cid:98)â„™(ğ‘¥)

(cid:27)

.

ğ‘‰ â‰¤ inf
ğœ† â‰¥0

(EC.2)

When there exists ğœ† > 0 such that Condition 1 holds, by leveraging a well-known reformulation on
entropy regularized linear optimization in Lemma EC.1, we can see that almost surely,

ğ‘£ğ‘¥ (ğœ†) = ğœ†ğœ– log

(cid:16)

ğ”¼â„šğ‘¥,ğœ–

(cid:104)
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)

< âˆ.

Substituting this expression into (EC.2) implies that ğ‘‰ â‰¤ ğ‘‰D < âˆ. Suppose on the contrary that for any
ğœ† > 0,

Pr ğ‘¥âˆ¼â„™

(cid:110)
ğ‘¥ : ğ”¼â„šğ‘¥,ğœ–

(cid:104)
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105)

(cid:111)

= âˆ

> 0,

then intermediately we obtain ğ‘‰ â‰¤ ğ‘‰D = âˆ, and the weak duality still holds.

(cid:3)
(cid:3)

Proof of Lemma 2 We ï¬rst show that ğœ†âˆ— < âˆ. Denote by ğ‘£ (ğœ†) the objective function for the dual

problem, then

ğ‘£ (ğœ†) = ğœ†ğœŒ + ğœ†ğœ–

âˆ«

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥).

The integrability condition for the dominated convergence theorem is satisï¬ed, which implies

ğœ†ğœ–

lim
ğœ†â†’âˆ
âˆ«

lim
ğ›½â†’0

âˆ«

ğœ–
ğ›½

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥)

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ›½ ğ‘“ (ğ‘§)/ğœ– (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥)

âˆ«

âˆ«

âˆ«

ğœ–âˆ‡ğ›½ log

lim
ğ›½â†’0

(cid:16)

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ›½ ğ‘“ (ğ‘§)/ğœ– (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥)

ğœ–

lim
ğ›½â†’0

ğ”¼â„šğ‘¥,ğœ–

(cid:18)

1
(cid:2)ğ‘’ ğ›½ ğ‘“ (ğ‘§)/ğœ– (cid:3)

ğ”¼â„šğ‘¥,ğœ–

(cid:20) ğ‘“ (ğ‘§)
ğœ–

(cid:21) (cid:19)

ğ‘’ (ğ›½ ğ‘“ (ğ‘§))/ğœ–

d(cid:98)â„™(ğ‘¥)

ğ”¼â„šğ‘¥,ğœ– [ğ‘“ (ğ‘§)] d(cid:98)â„™(ğ‘¥),

=

=

=

=

ec6

where the ï¬rst equality follows from the change-of-variable technique with ğ›½ = 1/ğœ†, the second equality
follows from the Lâ€™Hospital rule the third and the last equality follows from the dominated convergence
theorem. As a consequence, as long as ğœŒ > 0, we have

We can take ğœ† satisfying Condition 1 and then ğ‘£ (ğœ†) < âˆ, which guarantees the existence of the dual
(cid:3)
minimizer. Hence ğœ†âˆ— < âˆ, which implies that either ğœ†âˆ— = 0 or ğœ†âˆ— satisï¬es Condition 1.

lim
ğœ†â†’âˆ

ğ‘£ (ğœ†) = âˆ.

Proof of Lemma 3 Suppose the dual minimizer ğœ†âˆ— = 0, then taking the limit of the dual objective

function gives

where

âˆ«

lim
ğœ†â†’0

ğ‘£ (ğœ†) =

ğ»ğ‘¢ (ğ‘¥) d(cid:98)â„™(ğ‘¥) < âˆ,

ğ»ğ‘¢ (ğ‘¥) := inf{ğ‘¡ : â„šğ‘¥,ğœ– {ğ‘“ (ğ‘§) > ğ‘¡ } = 0} (cid:44) ess sup

ğ‘“ .

â„šğ‘¥,ğœ–

For notational simplicity we take ğ»ğ‘¢ = ess sup
for any ğ‘¡ so that â„šğ‘¥,ğœ– {ğ‘“ (ğ‘§) > ğ‘¡ } = 0, we have that

ğœˆ

ğ‘“ . One can check that ğ»ğ‘¢ (ğ‘¥) â‰¡ ğ»ğ‘¢ for any ğ‘¥ âˆˆ supp((cid:98)â„™):

âˆ«

1{ğ‘“ (ğ‘§) > ğ‘¡ }ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) = 0,

which, together with the fact that ğœˆ {ğ‘ (ğ‘¥, ğ‘§) < âˆ} = 1 for ï¬xed ğ‘¥, implies

âˆ«

1{ğ‘“ (ğ‘§) > ğ‘¡ } dğœˆ (ğ‘§) = 0.

On the contrary, for any ğ‘¡ so that ğœˆ {ğ‘“ (ğ‘§) > ğ‘¡ } = 0, we have that

âˆ«

0 â‰¤

1{ğ‘“ (ğ‘§) > ğ‘¡ }ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) â‰¤

âˆ«

1{ğ‘“ (ğ‘§) > ğ‘¡ } dğœˆ (ğ‘§) = 0,

where the second inequality is because that ğœˆ {ğ‘ (ğ‘¥, ğ‘§) â‰¥ 0} = 1. As a consequence, â„šğ‘¥,ğœ– {ğ‘“ (ğ‘§) > ğ‘¡ } = 0.
Hence we can assert that ğ»ğ‘¢ (ğ‘¥) = ğ»ğ‘¢ for all ğ‘¥ âˆˆ supp((cid:98)â„™), which implies

Then we show that almost surely for all ğ‘¥,

ğ‘£ (ğœ†) = ğ»ğ‘¢ < âˆ.

lim
ğœ†â†’0

ğ”¼â„šğ‘¥,ğœ– [1ğ´] > 0, where ğ´ = {ğ‘§ : ğ‘“ (ğ‘§) = ğ»ğ‘¢ }.

Denote by ğ· the collection of samples ğ‘¥ so that ğ”¼â„šğ‘¥,ğœ– [1ğ´] = 0. Assume the condition above does not
hold, which means that (cid:98)â„™{ğ· } > 0. For any ğœ > 0 and ğ‘¥ âˆˆ ğ·, there exists ğ» ğ‘™ (ğ‘¥) < ğ»ğ‘¢ such that

0 < ğ”¥ğ‘¥ := ğ”¼â„šğ‘¥,ğœ– [1ğµ (ğ‘¥) ] â‰¤ ğœ, where ğµ(ğ‘¥) = {ğ‘§ : ğ» ğ‘™ (ğ‘¥) â‰¤ ğ‘“ (ğ‘§) â‰¤ ğ»ğ‘¢ }.

Deï¬ne ğ» gap(ğ‘¥) = ğ»ğ‘¢ âˆ’ ğ» ğ‘™ (ğ‘¥), ğ”¥ğ‘

ğ‘¥ = 1 âˆ’ ğ”¥ğ‘¥ . Then we ï¬nd that for ğ‘¥ âˆˆ ğ·,

ğ‘£ğ‘¥ (ğœ†) = ğœ†ğœ– log

(cid:16)

ğ”¼â„šğ‘¥,ğœ–
(cid:16)

(cid:105)

(cid:104)
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) 1ğµ (ğ‘¥)
ğ”¥ğ‘¥ + ğ‘’âˆ’ğ» gap (ğ‘¥)/(ğœ†ğœ–) ğ”¥ğ‘
ğ‘¥

+ ğ”¼â„šğ‘¥,ğœ–
(cid:17)

.

(cid:104)
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) 1ğµ (ğ‘¥)ğ‘

(cid:105) (cid:17)

â‰¤ ğ»ğ‘¢ + ğœ†ğœ– log

ec7

Since (cid:98)â„™{ğ· } > 0, the dual objective function for ğœ† > 0 is upper bounded as
âˆ«

ğ‘£ (ğœ†) = ğœ†ğœŒ +

ğ‘£ğ‘¥ (ğœ†) d(cid:98)â„™(ğ‘¥)
âˆ«

ğ·

â‰¤ ğ»ğ‘¢ + ğœ†ğœŒ + ğœ†ğœ–

log

(cid:16)

ğ”¥ğ‘¥ + ğ‘’âˆ’ğ» gap (ğ‘¥)/(ğœ†ğœ–) ğ”¥ğ‘
ğ‘¥

(cid:17)

d(cid:98)â„™(ğ‘¥).

We can see that

and

lim
ğœ†â†’0

ğœ†ğœŒ + ğœ†ğœ–

âˆ«

ğ·

(cid:16)

log

ğ”¥ğ‘¥ + ğ‘’âˆ’ğ» gap (ğ‘¥)/(ğœ†ğœ–) ğ”¥ğ‘
ğ‘¥

(cid:17)

d(cid:98)â„™(ğ‘¥) = 0,

lim
ğœ†â†’0

=ğœŒ + ğœ–

(cid:20)

âˆ‡

âˆ«

ğ·

ğœ†ğœŒ + ğœ†ğœ–

âˆ«

ğ·

(cid:16)

log

ğ”¥ğ‘¥ + ğ‘’âˆ’ğ» gap (ğ‘¥)/(ğœ†ğœ–) ğ”¥ğ‘
ğ‘¥

(cid:21)

(cid:17)

d(cid:98)â„™(ğ‘¥)

log (ğ”¥ğ‘¥ ) d(cid:98)â„™(ğ‘¥)

â‰¤ğœŒ + ğœ– log(ğœ)(cid:98)â„™{ğ· } â‰¤ âˆ’ğœŒ < 0,

where the second inequality is by taking the constant ğœ = exp
that

(cid:16)

âˆ’

(cid:17)

2ğœŒ
ğœ–(cid:98)â„™{ğ· }

. Hence, there exists ğœ† > 0 such

ğ‘£ (ğœ†) â‰¤ ğ»ğ‘¢ + ğœ†ğœŒ + ğœ†ğœ–

âˆ«

ğ·

(cid:16)

log

ğ”¥ğ‘¥ + ğ‘’âˆ’ğ» gap (ğ‘¥)/(ğœ†ğœ–) ğ”¥ğ‘
ğ‘¥

(cid:17)

d(cid:98)â„™(ğ‘¥) < ğ‘£ (0),

which contradicts to the optimality of ğœ†âˆ— = 0. As a result, almost surely for all ğ‘¥, we have that

ğ”¼â„šğ‘¥,ğœ– [1ğ´] > 0.

To show the second condition, we re-write the dual objective function for ğœ† > 0 as

ğ‘£ (ğœ†) = ğœ†ğœŒ + ğœ†ğœ–

âˆ« (cid:104)

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ– [1ğ´] + ğ”¼â„šğ‘¥,ğœ–

(cid:104)
ğ‘’ [ğ‘“ (ğ‘§)âˆ’ğ»ğ‘¢ ]/(ğœ†ğœ–) 1ğ´ğ‘

(cid:105) (cid:17)(cid:105)

d(cid:98)â„™(ğ‘¥) + ğ»ğ‘¢ .

The gradient of ğ‘£ (ğœ†) becomes

âˆ‡ğ‘£ (ğœ†) = ğœŒ + ğœ–

+

âˆ« (cid:104)

log

âˆ« ğ”¼â„šğ‘¥,ğœ–

(cid:16)

(cid:104)
ğ‘’ [ğ‘“ (ğ‘§)âˆ’ğ»ğ‘¢ ]/(ğœ†ğœ–) 1ğ´ğ‘
ğ”¼â„šğ‘¥,ğœ– [1ğ´] + ğ”¼â„šğ‘¥,ğœ–
(cid:2)ğ‘’ [ğ‘“ (ğ‘§)âˆ’ğ»ğ‘¢ ]/(ğœ†ğœ–) 1ğ´ğ‘ (ğ»ğ‘¢ âˆ’ ğ‘“ (ğ‘§))/(ğœ†)(cid:3)

ğ”¼â„šğ‘¥,ğœ– [1ğ´] + ğ”¼â„šğ‘¥,ğœ–

(cid:2)ğ‘’ [ğ‘“ (ğ‘§)âˆ’ğ»ğ‘¢ ]/(ğœ†ğœ–) 1ğ´ğ‘ (cid:3)

(cid:105) (cid:17)(cid:105)

d(cid:98)â„™(ğ‘¥)

d(cid:98)â„™(ğ‘¥).

We can see that limğœ†â†’âˆ âˆ‡ğ‘£ (ğœ†) = ğœŒ. Take

ğ‘£1,ğ‘¥ (ğœ†) = ğ”¼â„šğ‘¥,ğœ–

(cid:104)
ğ‘’ [ğ‘“ (ğ‘§)âˆ’ğ»ğ‘¢ ]/(ğœ†ğœ–) 1ğ´ğ‘

(cid:105)

.

Then limğœ†â†’0 ğ‘£1,ğ‘¥ (ğœ†) = 0 and ğ‘£1,ğ‘¥ (ğœ†) â‰¥ 0. Take

ğ‘£2,ğ‘¥ (ğœ†) =

ğ”¼â„šğ‘¥,ğœ–

(cid:2)ğ‘’ [ğ‘“ (ğ‘§)âˆ’ğ»ğ‘¢ ]/(ğœ†ğœ–) 1ğ´ğ‘ (ğ»ğ‘¢ âˆ’ ğ‘“ (ğ‘§))/(ğœ†)(cid:3)

ğ”¼â„šğ‘¥,ğœ– [1ğ´] + ğ”¼â„šğ‘¥,ğœ–

(cid:2)ğ‘’ [ğ‘“ (ğ‘§)âˆ’ğ»ğ‘¢ ]/(ğœ†ğœ–) 1ğ´ğ‘ (cid:3)

.

Then limğœ†â†’0 ğ‘£2,ğ‘¥ (ğœ†) = 0 and ğ‘£2,ğ‘¥ (ğœ†) â‰¥ 0. It follows that

âˆ‡ğ‘£ (ğœ†) = ğœŒ + ğœ–

lim
ğœ†â†’0

âˆ«

log (cid:0)ğ”¼â„šğ‘¥,ğœ– [1ğ´](cid:1) d(cid:98)â„™(ğ‘¥) = ğœŒ (cid:48).

ec8

Hence, if the last condition is violated, based on the mean value theorem, we can ï¬nd ğœ† > 0 so that
âˆ‡ğ‘£ (ğœ†) = 0, which contradicts to the optimality of ğœ†âˆ— = 0.

Now we show the converse direction. For any ğœ† > 0, we ï¬nd that

âˆ‡ğ‘£ (ğœ†) = ğœŒ + ğœ–

âˆ«

(cid:2)log (cid:0)ğ”¼â„šğ‘¥,ğœ– [1ğ´] + ğ‘£1,ğ‘¥ (ğœ†)(cid:1)(cid:3) d(cid:98)â„™(ğ‘¥) +

âˆ«

ğ‘£2,ğ‘¥ (ğœ†) d(cid:98)â„™(ğ‘¥).

For ï¬xed ğ‘¥, when ğ”¼â„šğ‘¥,ğœ– [1ğ´] = 1, we can see that ğ‘£1,ğ‘¥ (ğœ†) = ğ‘£2,ğ‘¥ (ğœ†) = 0, then
ğœŒ + ğœ– (cid:2)log (cid:0)ğ”¼â„šğ‘¥,ğœ– [1ğ´] + ğ‘£1,ğ‘¥ (ğœ†)(cid:1)(cid:3) + ğ‘£2,ğ‘¥ (ğœ†) = ğœŒ > 0.

When ğ”¼â„šğ‘¥,ğœ– [1ğ´] âˆˆ (0, 1), we can see that ğ‘£1,ğ‘¥ (ğœ†) > 0, ğ‘£2,ğ‘¥ (ğœ†) > 0. Then

ğœŒ + ğœ– (cid:2)log (cid:0)ğ”¼â„šğ‘¥,ğœ– [1ğ´] + ğ‘£1,ğ‘¥ (ğœ†)(cid:1)(cid:3) + ğ‘£2,ğ‘¥ (ğœ†) > ğœŒ + ğœ– log(ğ”¼â„šğ‘¥,ğœ– [1ğ´]) = ğœŒ (cid:48) â‰¥ 0.

Therefore, âˆ‡ğ‘£ (ğœ†) > 0 for any ğœ† > 0. By the convexity of ğ‘£ (ğœ†), we conclude that the dual minimizer
ğœ†âˆ— = 0.

(cid:3)

Proof of Lemma 4. Since ğœ†âˆ— > 0, based on the optimality condition of the dual problem, we have

that

0 = âˆ‡ğœ†

(cid:20)

ğœ†ğœŒ + ğœ†ğœ–

âˆ«

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

(cid:104)
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)

d(cid:98)â„™(ğ‘¥)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)ğœ†=ğœ†âˆ—

.

Or equivalently, we have that

âˆ«

ğœŒ+ğœ–

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥) âˆ’

Re-arranging the term completes the proof.

âˆ« ğ”¼â„šğ‘¥,ğœ–

(cid:2)ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) ğ‘“ (ğ‘§)(cid:3)
(cid:2)ğ‘’ ğ‘“ (ğ‘§)/(ğœ†âˆ—ğœ–) (cid:3)

ğœ†âˆ—ğ”¼â„šğ‘¥,ğœ–

d(cid:98)â„™(ğ‘¥) = 0.

(cid:3)

Proof of Theorem 1. The feasibility result in Theorem 1(I) can be easily shown by considering the

reformulation of ğ‘‰ in (3) and the non-negativity of KL-divergence. When ğœŒ = 0, one can see that

ğ‘‰D â‰¤ lim
ğœ†â†’âˆ

ğœ†ğœ–

âˆ«

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥) = ğ”¼ğ‘§âˆ¼â„™0 [ğ‘“ (ğ‘§)] = ğ‘‰.

Therefore, the strong duality result holds in this case. The proof for ğœŒ > 0 can be found in the main
context. It remains to show the second part of Theorem 1(III). We consider a sequence of real numbers
{ğ‘… ğ‘— } ğ‘— such that ğ‘… ğ‘— â†’ âˆ and take the objective function ğ‘“ğ‘— (ğ‘§) = ğ‘“ (ğ‘§)1{ğ‘§ â‰¤ ğ‘… ğ‘— }. Hence, there exists ğœ† > 0
(cid:2)ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:3) = âˆ(cid:9) = 0. According to the necessary condition in Lemma 3, the
satisfying Pr ğ‘¥âˆ¼(cid:98)â„™
ğ‘— > 0 for suï¬ƒciently large index ğ‘—. Then we can apply the duality result
corresponding dual minimizer ğœ†âˆ—
in the ï¬rst part of Theorem 1(III) to show that for suï¬ƒciently large ğ‘—, it holds that

(cid:8)ğ‘¥ : ğ”¼â„šğ‘¥,ğœ–

(cid:8)ğ”¼ğ‘§âˆ¼â„™ [ğ‘“ğ‘— (ğ‘§)](cid:9) â‰¥ ğœ†âˆ—

ğ‘— ğœŒ + ğœ†âˆ—
ğ‘— ğœ–

âˆ«

(cid:16)

log

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ ğ‘“ğ‘— (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

d(cid:98)â„™(ğ‘¥).

sup
â„™âˆˆğ”¹ğœŒ,ğœ– ((cid:98)â„™)

Taking ğ‘— â†’ âˆ both sides implies that ğ‘‰ = âˆ, which completes the proof.

(cid:3)

Appendix EC.5: Proofs of Technical Results in Section 4

ec9

Proof of Proposition 1 For any ï¬xed ğœ†0 > 0, denote by ğœƒ0 the optimal solution to problem (10). We
at ğœ† = ğœ†0. For any ğœ† > 0, let ğœƒ (ğœ†) be

can argue that for any ğ‘ âˆˆ ğœ•
ğœ•ğœ†
the optimal solution for Ë†ğ¹ (ğ‘š)

Ë†ğ¹ (ğ‘š) (ğœ†0, ğœƒ0), ğ‘ is a subgradient of Ë†ğ¹ (ğ‘š)
. Then we can see

ğœ†

ğœ†

Ë†ğ¹ (ğ‘š)
ğœ†

= Ë†ğ¹ (ğ‘š) (ğœ†, ğœƒ (ğœ†)) â‰¥ Ë†ğ¹ (ğ‘š) (ğœ†0, ğœƒ0) + ğ‘(ğœ† âˆ’ ğœ†0) + (cid:104)âˆ‡ Ë†ğ¹ (ğ‘š) (ğœ†0, ğœƒ0), ğœƒ (ğœ†) âˆ’ ğœƒ0(cid:105)

â‰¥ Ë†ğ¹ (ğ‘š) (ğœ†0, ğœƒ0) + ğ‘(ğœ† âˆ’ ğœ†0),

where âˆ‡ Ë†ğ¹ (ğ‘š) (ğœ†0, ğœƒ0) denotes the subdiï¬€erential of Ë†ğ¹ (ğ‘š) with respect to ğœƒ = ğœƒ0, the ï¬rst inequality is
by the convexity of Ë†ğ¹ (ğ‘š) (ğœ†, ğœƒ ) with respect to (ğœ†, ğœƒ ), and the second inequality is by the optimality
condition for ğœƒ0.

, which means that ğœ†0 is the minimizer of Ë†ğ¹ (ğ‘š)
When ğ‘ = 0, we immediately obtain that 0 âˆˆ ğœ• Ë†ğ¹ (ğ‘š)
.
ğœ†0
Otherwise, the algorithm will update the interval so that ğ‘(ğœ† âˆ’ ğœ†0) â‰¤ 0 for any ğœ† within it. We claim
that this interval will contain ğœ†âˆ—. Suppose on the contrary that ğ‘(ğœ†âˆ— âˆ’ ğœ†0) > 0. By the convexity of Ë†ğ¹ (ğ‘š)
,

ğœ†

Â·

Ë†ğ¹ (ğ‘š)
ğœ†âˆ—

â‰¥ Ë†ğ¹ (ğ‘š)
ğœ†0

+ ğ‘(ğœ†âˆ— âˆ’ ğœ†0) > Ë†ğ¹ (ğ‘š)
ğœ†0

,

which contradicts to the optimality of ğœ†âˆ—. As a result, the interval length ğ‘™ğ‘¡ = ğœ†ğ‘¢ âˆ’ ğœ†ğ‘™ at the ğ‘¡-th iteration
in Algorithm 1 vanishes at the rate ğ‘™ğ‘¡ = (1/2)ğ‘¡ğ‘™0, which indicates that the algorithm will converge into
the optimal solution of infğœ† â‰¥0 Ë†ğ¹ (ğ‘š)
(cid:3)

linearly.

ğœ†

Proof of Proposition 2. For notational simplicity, we write ğ‘  = (ğœ†, ğœƒ ) and Î = â„+ Ã— Î˜. We ï¬rst show

the second part of this theorem. To begin with, we introduce the following functions:

Ëœğ¹ğ‘š (ğ‘ ) = Ë†ğ¹ (ğ‘š) (ğ‘ ) + ğœÎ(ğ‘ ),

Â¯ğ¹ (ğ‘ ) = ğ¹ (ğ‘ ) + ğœÎ(ğ‘ ).

We build the pointwise law of large numbers (LLN) for Ëœğ¹ğ‘š. By the strong law of large numbers (LLN),
for each ğ‘– and every (ğœ†, ğœƒ ) âˆˆ Î,

(cid:104)
ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) a.s.

âˆ’âˆ’â†’ ğ”¼â„šğ‘–,ğœ–

(cid:104)
ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105)

.

ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–

Then by the continuous mapping theorem, for each ğ‘– and every (ğœ†, ğœƒ ) âˆˆ Î,

ğœ†ğœ– log

(cid:18)

ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–

ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:19) a.s.
(cid:104)

âˆ’âˆ’â†’ ğœ†ğœ– log

(cid:16)

ğ”¼â„šğ‘–,ğœ–

(cid:104)
ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)

.

Therefore, using the addition and scalar multiplication rule of almost sure convergence, for every
(ğœ†, ğœƒ ) âˆˆ Î, it holds that

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğœ†ğœ– log

(cid:18)

ğ”¼ Ë†â„šğ‘š
Ë†ğ‘¥ğ‘–

ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:19) a.s.
(cid:104)
âˆ’âˆ’â†’

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğœ†ğœ– log

(cid:16)

ğ”¼â„šğ‘–,ğœ–

ğ‘’ ğ‘“ğœƒ (ğ‘§)/(ğœ†ğœ–) (cid:105) (cid:17)
(cid:104)

,

which reveals that Ë†ğ¹ (ğ‘š) (ğ‘ )
Ëœğ¹ğ‘š (ğ‘ ).

a.s.
âˆ’âˆ’â†’ ğ¹ (ğ‘ ) for each ğ‘  âˆˆ Î. This further implies the corresponding LLN for

Now take a compact subset ğ¶ so that S âˆ— is contained in the interior of ğ¶. Such a set exists because
S âˆ— is bounded. Denote by ËœSğ‘š the set of minimizers of Ëœğ¹ğ‘š over ğ¶. By the lower semi-continuity, together
with the pointwise LLN of Ëœğ¹ğ‘š, we ï¬nd Ëœğ¹ğ‘š is ï¬nite-valued on S âˆ— for large ğ‘š, which implies that the set
ËœSğ‘š is non-empty for large ğ‘š. We show that Dist( ËœSğ‘š, S âˆ—) â†’ 0 almost surely. Let ğœ” = {ğ‘§ğ‘–,ğ‘— }ğ‘–,ğ‘— be such that
ğ‘’
Ëœğ¹ğ‘š (Â·, ğœ”)
âˆ’â†’ Â¯ğ¹ (Â·). This event holds almost surely for all ğœ”. Suppose on the contrary that for any ğ‘š, there
exists a minimizer Ëœğ‘ ğ‘š (ğœ”) of Ëœğ¹ğ‘š over ğ¶ such that Dist(Ëœğ‘ ğ‘š, S âˆ—) â‰¥ ğœ€. Due to the compactness of ğ¶, there

ec10

exists a sub-sequence {Ëœğ‘ ğ‘š ğ‘— } ğ‘— that converges into a point ğ‘ âˆ— âˆˆ ğ¶, but ğ‘ âˆ— âˆ‰ S âˆ—. On the other hand, we can
Â¯ğ¹ (ğ‘ ) = S âˆ— by applying [86, Proposition 7.26]. Then we obtain a contradiction.
argue that ğ‘ âˆ— âˆˆ arg minğ‘  âˆˆğ¶
Then we show that ËœSğ‘š = S (ğ‘š) for large ğ‘š. Because of the convexity assumption, any minimizer
of Ëœğ¹ğ‘š over ğ¶ which lies inside of the interior of ğ¶, is also an optimal solution to the problem 9.
Hence, for large ğ‘š we have that ËœSğ‘š = S (ğ‘š) . This, together with the fact that Dist( ËœSğ‘š, S âˆ—) â†’ 0 implies
Dist(S (ğ‘š), S âˆ—) â†’ 0. Moreover, it suï¬ƒces to restrict the feasible set into the compact set ğ¶ âˆ© Î. By the
ğ‘.ğ‘ .
convexity of ğ¹ (ğ‘ ) in ğ‘ , Ë†ğ¹ (ğ‘š)
âˆ’âˆ’â†’ ğ¹ holds uniformly on ğ¶ âˆ© Î. As a consequence, the ï¬rst part of this
(cid:3)
proposition can be proved by applying [86, Proposition 5.2].

Appendix EC.6: Proof of the Technical Result in Appendix A
We ï¬rst present an useful technical lemma before showing Proposition 3.

Lemma EC.2. Under the ï¬rst condition of Proposition 3, for any ğ‘¥ âˆˆ Z, it holds that
ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) â‰¥ ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘¥)/ğœ– âˆ«

ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§).

âˆ«

Proof of Lemma EC.2 Based on the inequality (ğ‘ + ğ‘)ğ‘ â‰¤ 2ğ‘âˆ’1(ğ‘ğ‘ + ğ‘ğ‘), we can see that

ğ‘ (ğ‘¥, ğ‘§) â‰¤ (ğ‘ (ğ‘¦, ğ‘§)1/ğ‘ + ğ‘ (ğ‘§,ğ‘¦)1/ğ‘)ğ‘ â‰¤ 2ğ‘âˆ’1(ğ‘ (ğ‘¦, ğ‘§) + ğ‘ (ğ‘§,ğ‘¦)), âˆ€ğ‘¥,ğ‘¦, ğ‘§ âˆˆ Z.

Since ğ‘ (ğ‘¥, ğ‘§) â‰¤ 2ğ‘âˆ’1(ğ‘ (ğ‘¥, ğ‘§) + ğ‘ (ğ‘¥, ğ‘¥)), we can see that

âˆ«

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) â‰¥ exp

(cid:16)

âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥, ğ‘¥)/ğœ–

(cid:17) âˆ«

ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§).

The proof is completed.

Proof of Proposition 3 One can see that for any ğ‘¥ âˆˆ supp((cid:98)â„™), it holds that

ec11

(cid:3)

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105)
(cid:104)

ğ”¼â„šğ‘¥,ğœ–
âˆ«

âˆ«

âˆ«

=

â‰¤

â‰¤

=

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–)

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ–
âˆ« ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘¢)/ğœ– dğœˆ (ğ‘¢)

dğœˆ (ğ‘§)

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–)

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ–
âˆ« ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§).
ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) ğ‘’âˆ’21âˆ’ğ‘ğ‘ (ğ‘¥,ğ‘§)/ğœ–ğ‘’ğ‘ (ğ‘¥,ğ‘¥)/ğœ–
âˆ« ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§)
âˆ«

dğœˆ (ğ‘§)

dğœˆ (ğ‘§)

ğ‘’ğ‘ (ğ‘¥,ğ‘¥) (1+2ğ‘âˆ’1)/ğœ–
âˆ« ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§)

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–)ğ‘’âˆ’21âˆ’ğ‘ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§),

where the ï¬rst inequality is based on the lower bound in Lemma EC.2, the second inequality is based
on the triangular inequality ğ‘ (ğ‘¥, ğ‘§) â‰¥ 21âˆ’ğ‘ğ‘ (ğ‘¥, ğ‘§) âˆ’ ğ‘ (ğ‘¥, ğ‘¥). Note that almost surely for all ğ‘¥ âˆˆ supp((cid:98)â„™),
ğ‘ (ğ‘¥, ğ‘¥) < âˆ. Moreover,

âˆ«

0 <

ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) â‰¤

âˆ«

ğ‘’âˆ’ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) < âˆ,

where the lower bound is because ğ‘ (ğ‘¥, ğ‘§) < âˆ almost surely for all ğ‘§, the upper bound is because
ğ‘ (ğ‘¥, ğ‘§) â‰¥ 0 almost surely for all ğ‘§. Based on these observations, we have that

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–) (cid:105)
(cid:104)

â‰¤

ğ”¼â„šğ‘¥,ğœ–

ğ‘’ğ‘ (ğ‘¥,ğ‘¥) (1+2ğ‘âˆ’1)/ğœ–
âˆ« ğ‘’âˆ’2ğ‘âˆ’1ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§)

âˆ«

ğ‘’ ğ‘“ (ğ‘§)/(ğœ†ğœ–)ğ‘’âˆ’21âˆ’ğ‘ğ‘ (ğ‘¥,ğ‘§)/ğœ– dğœˆ (ğ‘§) < âˆ

almost surely for all ğ‘¥ âˆ¼ (cid:98)â„™.

(cid:3)

