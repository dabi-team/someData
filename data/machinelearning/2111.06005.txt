Agent Spaces

John C. Raisbeck∗, Matthew W. Allen, and Hakho Lee

Center for Systems Biology
Massachusetts General Hospital
Boston, Massachusetts

November 24, 2021

Abstract

Exploration is one of the most important tasks in Reinforcement Learning, but it is not
well-deﬁned beyond ﬁnite problems in the Dynamic Programming paradigm (see Subsection 2.4).
We provide a reinterpretation of exploration which can be applied to any online learning method.
We come to this deﬁnition by approaching exploration from a new direction. After ﬁnding
that concepts of exploration created to solve simple Markov decision processes with Dynamic
Programming are no longer broadly applicable, we reexamine exploration. Instead of extending
the ends of dynamic exploration procedures, we extend their means. That is, rather than
repeatedly sampling every state-action pair possible in a process, we deﬁne the act of modifying
an agent to itself be explorative. The resulting deﬁnition of exploration can be applied in inﬁnite
problems and non-dynamic learning methods, which the dynamic notion of exploration cannot
tolerate.

To understand the way that modiﬁcations of an agent aﬀect learning, we describe a novel
structure on the set of agents: a collection of distances (see footnote 7) da, a ∈ A, which represent
the perspectives of each agent possible in the process. Using these distances, we deﬁne a topology
and show that many important structures in Reinforcement Learning are well behaved under
the topology induced by convergence in the agent space.

1
2
0
2

v
o
N
1
1

]
I

A
.
s
c
[

1
v
5
0
0
6
0
.
1
1
1
2
:
v
i
X
r
a

∗Corresponding author, jraisbeck@mgh.harvard.edu

1

 
 
 
 
 
 
0

Introduction

Reinforcement Learning (RL) is the study of stochastic processes which are composed of a decision
process P, and an agent a. The Reinforcement Learning problem of a decision process P with reward
R is to ﬁnd an optimal agent a∗ which maximizes the expectation of a reward function R : Φ → R
with respect to the distribution of paths Φa drawn from the process (P, a). Typically, Reinforcement
Learning methods seek a∗ or otherwise high-reward agents via iterative learning processes, sometimes
described as trial-and-error [1].

In an online learning algorithm, learners send one or more agents to interact with the process
and collect information on those interactions. After studying these interactions, the learner develops
new agents to experiment with, seeking to improve the reward of successive generations of agents
(see Deﬁnition 1.15). In this pursuit, there is a trade-oﬀ between seeking high-quality agents during
learning (exploitation) and seeking new information about the decision process (exploration).

Exploitation, being closely related to general iterative optimization algorithms, is well-understood.
In simple problems, its trade-oﬀ with exploration has been extensively researched [1, 2]. However,
in more complex problems the conclusions reached by studying simple problems seem to have little
bearing; some of the exploration methods which are least eﬃcient in simple problems have been used
in the most impressive demonstrations of the power of Reinforcement Learning to date [3, 4].

This paper focuses on exploration, especially on methods of exploration which ignore a certain set
of the tenets of Dynamic Programming [5, 6]. We call these methods na¨ıve, and class among them
Novelty Search [7], which we discuss in Subsection 4.1. We begin in Section 1 and Section 2, rigorously
describing the problem of Reinforcement Learning and introducing the necessity for exploration.
In Subsection 2.4 and Section 3, we study the notions of exploration coming from the study of
Dynamic Programming, and consider their eﬃcacy in modern Reinforcement Learning. In Section 4,
we investigate the properties of a class of spaces based on what we call primitive behavior, and in
Section 5 introduce a general form of these spaces, which we call agent spaces. The agent space can be
deﬁned in a broad class of decision processes and eﬃciently describes several important features of an
agent. In Section 6, it is demonstrated that the distributions of truncated paths φt are continuous in
the agent, and in Section 7 it is demonstrated that certain functions of those paths (chieﬂy, expected
reward) are continuous in the agent space.

1 Reinforcement Learning

1.1 Deﬁnitions

Deﬁnition 1.1 (Decision Process). A discrete time decision process P is a controlled stochastic
process indexed by N. Associated with the process are a set of states S, a set of actions A, and a
state-transition function σ.

Deﬁnition 1.2 (State). A state s ∈ S is state possible in the decision process P.

Deﬁnition 1.3 (Action). An action a ∈ A is a control which an agent can exert on P.

Deﬁnition 1.4 (Path). A path (sometimes called a trajectory) φ ∈ Φ in a decision process P is a
sequence of state-action pairs generated by the interaction between the process and an agent a:

φ = (s0, a0), (s1, a1), (s2, a2)...

(1.1.1)

Remark 1.1. We will sometimes need to refer to truncated paths (φt), i.e. paths which contain only
the state-action pairs associated with indices i ≤ t. This is common when referring to the domains of
agents a and the state transition function σ. We will see that truncated paths suﬃce to describe the
domain of σ, but because agents act only when a new state has been generated, its domain is the set
of truncated prime paths (φ(cid:48)

t): paths which contain the t-th state, but not the t-th action.

2

1. φt = {(s0, a0), (s1, a1), . . . , (st−1, at−1), (st, at)}

2. φ(cid:48)

t = {(s0, a0), (s1, a1), . . . , (st−1, at−1), st}

see Figure 1

see Figure 2

Sometimes (as in equation (1.1.13)), it is necessary to refer to the t-th state or action of a path φ.
We denote the t-th state φt(s), and the t-th action φt(a), using st and at when unambiguous.
Because s0 has no antecedent, it is determined by the initial state distribution s0 ∼ σ0.

Deﬁnition 1.5 (Initial State Distribution). The initial state distribution σ0 of a process P is the
probability distribution over S which determines the ﬁrst state in a path, s0 = φ0(s) ∼ σ0.

Deﬁnition 1.6 (State-Transition Function). The state-transition function σ of a process P is the
function which takes the truncated path φt−1 to the distribution of states σ(φt−1) from which the
next state st = φt(s) is drawn.

Thus, the state at t is determined by

(cid:40)

σ0,
σ(φt−1),
Deﬁnition 1.7 (Agent). An agent 1 a ∈ A is a function from the set of truncated paths Φ(cid:48) = (cid:83)
t∈N Φ(cid:48)
t
(notice that Φ(cid:48) contains only truncated paths, and Φ contains only inﬁnite paths) of a process into
the set of actions A of the process:

if t = 0,
if t > 0.

(1.1.2)

st ∼

a :

(cid:91)

t∈N

Φ(cid:48)

t → A.

(cid:9) φ0

(cid:27)

φ1






φ2






φ3






φ4






φ5






φ

Process

0
1
2
3
4
5
...

σ0
σ(φ0)
σ(φ1)
σ(φ2)
σ(φ3)
σ(φ4)
...

0)
1)
2)
3)
4)
5)

a(φ(cid:48)
a(φ(cid:48)
a(φ(cid:48)
a(φ(cid:48)
a(φ(cid:48)
a(φ(cid:48)
...

(1.1.3)

(1.1.4)

Figure 1: Truncated paths φt are drawn from a distribution deﬁned by iteratively drawing states and
new actions from σ and a. The complete path φ is deﬁned by the collection of these.

Agents are judged by the quality of the control which they exert, as measured by the expectation

of the reward of their paths.

Deﬁnition 1.8 (Reward). A reward function is a function from the set of paths Φ of a process P
into the real numbers R:

Often [1], R can be described as a sum:

R : Φ → R

R(φ) =

R(φ) =

(cid:88)

t∈N
(cid:88)

t∈N

r(φt(s), φt(a)), or

r(φt(s), φt(a))ω(t),

(1.1.12)

(1.1.13)

(1.1.14)

1Some authors call this function a policy, referring to its embodiment as an agent. See Subsection 1.2.1, Deﬁnition 5.6,

and Subsection 7.1.

3

Process

0

s0

(cid:9) φ(cid:48)

0 → a0 ∼ a(φ(cid:48)
0)

Process

0

s0

a0

(cid:9) φ0 → s1 ∼ σ(φ0)

Process

a0

(cid:9) φ0

(cid:27)

s0
s1

1 → a1 ∼ a(φ(cid:48)
φ(cid:48)
1)

(cid:9) φ0

(cid:27)

φ1 → s2 ∼ σ(φ1)

Process

s0
s1

a0
a1

Process
















(cid:9) φ0

(cid:27)

φ1

a0
a1

s0
s1
s2

Process

s0
s1
s2

a0
a1
a2

Process

a0
a1
a2

s0
s1
s2
s3

...

(cid:9) φ0

(cid:27)

φ1

(cid:9) φ0

(cid:27)

φ1

...

...

2 → a2 ∼ a(φ(cid:48)
φ(cid:48)
2)

φ2 → s3 ∼ σ(φ2)





φ2

...

φ(cid:48)
3 → a3 ∼ a(φ(cid:48)
3)

...

...

...

0
1

0
1

0
1
2

0
1
2

0
1
2
3

...

Figure 2: The structure of a decision process.

(1.1.5)

(1.1.6)

(1.1.7)

(1.1.8)

(1.1.9)

(1.1.10)

(1.1.11)

where r : S × A → R is an immediate [8, p. 49] reward function, and ω : N → [0, 1] is a discount
function such that

Ω =

(cid:88)

t∈N

ω(t) < ∞.

(1.1.15)

Deﬁnition 1.9 (Expected Reward). The expected reward J is the expectation of the reward of paths
φ drawn from the distribution of paths Φa generated by the process (P, a):

J(a) = E

φ∼Φa

[R(φ)]

(1.1.16)

Reinforcement Learning is concerned with the optimal control of decision processes, pursuing an
optimal agent a∗ which achieves the greatest possible expected reward J(a). To this end, learning

4

algorithms (see Subsection 1.3) are employed. Many algorithms pursue such agents by searching
the agents which are “near” the agents which they have recently considered. In some contexts, this
suﬃces to guarantee a solution in ﬁnitely many steps (see Theorem 2.1). When this cannot be
guaranteed, the problem is often restated in terms of discovering “satisfactory” agents, or discovering
the highest quality agent possible under certain constraints.
Remark 1.2 (Table of Notation). We will frequently reference states, actions, paths, and sets and
distributions of the same:2

Individual

Set

Distribution

State

Action

Path

Prime Path

s

a

φ

φ(cid:48)
t

S

A

Φ

Φ(cid:48)
t

σ(φt)

a(φ(cid:48)
t)
Φa

Φ(cid:48)a
t

Often, the decision processes in Reinforcement Learning are assumed to be Markov Decision

Processes (MDPs); decision processes which satisfy the Markov property.

Deﬁnition 1.10 (Markov Property). A decision process has the Markov Property if, given the prior
state-action pair (st−1, at−1), the distribution of the state-action pair (st, at) is independent of the
pairs (s, a) ∈ {(si, ai) | i < t − 1}.

Remark 1.3 (A Simple Condition for the Markov Property). Sometimes a slightly stricter deﬁnition
of the Markov property is used: a decision process is guaranteed to be Markov if σ is a function of
(st, at) and a is a function of st+1 alone (which is itself a function of (st, at)). We call such agents
strictly Markov:

Deﬁnition 1.11 (Strictly Markov Agent). A strictly Markov agent a ∈ A is a function from the set
of states S into the set of actions A:

a : S → A.

(1.1.17)

1.2 Computational considerations

When studying modern Reinforcement Learning, it is important to keep the constraints of practice
in mind. Among the most important of these is the quantiﬁcation of states and actions. In practice,
the sets of states and actions are typically real (S ⊂ Rm, A ⊂ Rn), ﬁnite (|S| ∈ N, |A| ∈ N), or a
combination thereof.

1.2.1 The Agent

Agents can be represented in a variety of ways, but most modern algorithms represent agents with
real function approximators (often neural networks) parameterized by a set of real numbers θ,

aθ : Rm → Rn.

(1.2.1)

This is true even in cases where the sets of states or actions are not real. To reconcile an approxi-
mator’s domain and range with such sets, the approximator is ﬁt with an input function I and an
output function O, which mediate the interaction between a process and a function approximator:

(cid:91)

I :

Φ(cid:48)

t → Rm,

t∈N
O : Rn → A.

(1.2.2)

(1.2.3)

2Without superscripts, Φ represents the set of possible paths. Note that only truncated paths can be prime.

5

When composed with these functions, the approximator forms an agent:

a = O ◦ aθ ◦ I.

(1.2.4)

1.2.2 The Output Function

The choice of output function is important and requires consideration of several aspects of the process,
including the learning process and the action set A. One of the most consequential roles of an output
function O occurs in processes where |A| is ﬁnite. In such processes, the output function must map a
real vector to a discrete action.

Some learning algorithms, such as Q-Learning, invoke the action-value function Q (see Deﬁni-
tion 2.2) in their learning process, training an agent to estimate the expected reward of each action
at each state, given an agent a [9, p. 81]. Because the object of approximation in Q-learning is
deﬁned analytically, some output functions are less sensible than others. Other algorithms, like policy
gradients, have outputs with less ﬁxed meanings. Consequently, they can accommodate a variety
of output functions, and the output function has a substantial eﬀect on the agent itself. Let us
consider some of the most common output functions. For further discussion of output functions, see
Appendix A.1.

Deﬁnition 1.12 (Greedy Action Sampling). In Greedy Action Sampling, the function approximator’s
output is taken to indicate a single “greedy” action, and this greedy action is taken[1]. In problems
t) ∈ R|A|) and the action
with ﬁnite sets of actions, the range of the parameterized agent is real (aθ(φ(cid:48)
associated with the dimension of greatest magnitude (the greedy action) is taken.

Deﬁnition 1.13 (ε-Greedy Sampling). In ε-Greedy Sampling, ε ∈ [0, 1], the greedy action is taken
with probability 1 − ε. Otherwise, an action is drawn from the uniform distribution on A.[1]

Unlike the greedy sampling methods above, Thompson sampling requires a ﬁnite set of actions.

Deﬁnition 1.14 (Thompson Sampling). In Thompson Sampling, the range of the parametrized
agent aθ is {x ∈ R|A|| (cid:80)
i∈|A| xi = 1, xi > 0}, and the action is selected by drawing from the random
variable which gives action ai probability xi [10].

1.3 Learning Algorithms

We are concerned primarily with online learning algorithms, in which exploration is most sensible.
Whereas most dynamic programming based methods study only a single agent in each epoch of
training, we consider a broader class of learning algorithms:

Deﬁnition 1.15 (Learning Algorithm). A learning algorithm (sometimes optimizer ) is an algorithm
which generates sets of candidate agents An, observes their interactions with a process, and uses this
information to generate a set of candidates An+1 in the next epoch. This procedure is repeated for
each epoch n ∈ I to improve the greatest expected reward among considered agents,

sup
n∈I

sup
a∈An

J(a).

(1.3.1)

Remark 1.4 (Loci). Many learning algorithms have an additional property which can simplify
discussions of learning algorithms: they center generations of agents An around a locus agent, or
locus of optimization, denoted alocus.

Not every interesting algorithm falls within this deﬁnition (e.g. evolutionary methods with
multiple loci), but many of the discussions in this paper apply, mutatis mutandis, to a broader class
of methods.

6

1.4 Interpreting the Reinforcement Learning Problem

Many learning algorithms are inﬂuenced by a philosophy of optimization called Dynamic Program-
ming [5] (DP). Dynamic programming approaches control problems state by state, determining the
best action for each state according to an approximation of the state-action value function Qa(s, a)
(see Deﬁnition 2.2). DP has compelling guarantees in ﬁnite3 strictly Markov decision processes [11].
However, because DP methods require every [8, 12] action value be approximated during each step
of optimization, these guarantees cannot transfer to inﬁnite problems.

Although DP is dominant in Reinforcement Learning [1, p. 73], several trends indicate that it may
not be required for eﬀective learning. Broadly, the problems of interest in Reinforcement Learning
have shifted substantially from those Bellman considered in the early 1950s [9]. Among other changes,
the ﬁeld now studies many inﬁnite problems, agents are not generally tabular, and the problems of
interest are not in general Markov. Each of these changes independently prevents a problem from
satisfying the requirements of dynamic programming [12]. Simultaneously, approaches not based
on DP, like Evolution Strategies [13] have shown that DP methods are not necessary to achieve
performance on par with modern dynamic programming based methods in common reinforcement
learning problems. Taken together, these trends indicate that DP may not be uniquely eﬀective.

This paper considers exploration in the light of these developments. We distinguish DP methods
like Q-Learning, which treat reinforcement learning problems as collections of subproblems, one for
each state, from methods like Evolution Strategies [13], which treat reinforcement learning problems
as unitary. We call the former class dynamic, and the latter class na¨ıve.

2 Exploitation and its discontents

The study of Reinforcement Learning employs several abstractions to describe and compare the
dynamics of learning algorithms which may have little in common. One particularly important concept
is a division between two supposed tasks of learning: exploitation and exploration. Exploitation
designates behavior which targets greater reward in the short term and exploration designates
behavior which seeks to improve the learner’s understanding of the process. Frequently, these tasks
conﬂict; learning more (exploration) can exclude experimenting with agents which are estimated to
obtain high reward (exploitation) and vice versa. The problem of balancing these tasks is known as
the Exploration versus Exploitation problem. Let us consider exploitation, exploration, and their
conﬂicts.

2.1 Exploitation

Exploitation can be deﬁned in a version of the Reinforcement Learning problem in which the goal
is to maximize the cumulative total of rewards obtained by all sampled agents: let φa be a path
sampled from (P, a)

(cid:88)

R(φa).

a∈((cid:83)

n∈I An)

(2.1.1)

Deﬁnition 2.1 (Exploitation). A learning method is exploitative if its purpose is to increase the
reward accumulated by the agents considered in the epochs I.

Fortunately, this deﬁnition of exploitation remains relevant to our problem because it informs
the cumulative version of the problem. Exploitation advises the dispatch of agents which are
expected to be high-quality, rather than those whose behavior and quality are less certain. In eﬀect,
exploitation is conservative, preferring “safer” experimentation and incremental improvement to

3An MDP is called ﬁnite if |A| < ℵ0 and |S| < ℵ0.

7

6

4

2

a

J(x)

b

p

c

−6

−4

−2

2

4

6

Locus

−2

q

Sampling Radius

−4

−6

Figure 3: A depiction of the reward of agents parameterized by a real number x. Even though c has
higher expected reward, it is isolated from the locus. Within the sampling radius, the trend is clear:
lower values of x produce greater reward. Thus, a purely exploitative method will end at b.

potentially destructive exploration. While exploitation is crucial to reinforcement learning, explorative
experiments are often necessary because sometimes, exploitation alone fails.

2.2 Exploitation Fails

When exploitation fails, it is because its conservatism causes it to become stuck in local optima.
Because Reinforcement Learning methods change the locus agent gradually, and exploitative methods
typically generate sets of agents An near the locus, exploitative learners can become trapped in
local optima. Figure 3 provides a unidimensional representation of this problem: because the agents
a ∈ An fall within the sampling radius, and updates to the locus agent are generally interpolative, the
product of the learning process depends exclusively on the initial locus and the sampling distance.

2.2.1 The Policy Improvement Theorem

No discussion of local optima in Reinforcement Learning would be complete without a discussion of
the Policy Improvement Theorem [8, 12]. Let us begin with a deﬁnition:

Deﬁnition 2.2 (Action Value Function (Q)). Let P be a strictly Markov decision process (Re-
mark 1.3). Then, the Action Value Function is the expectation of reward4, starting at state s, taking
an action a, and continuing with an agent a: [8]

Qa(s, a) = E (cid:2)R(φ) | s0 = s,

a0 = a,

∀ t ∈ Z+(at ∼ a(st−1))(cid:3) .

(2.2.1)

4Reward is assumed to be summable (1.1.13), and ω(t) is assumed to be exponential, ω(t) = γt, 0 ≤ γ < 1.

8

Theorem 2.1 (Policy Improvement Theorem). For any agent a in a ﬁnite, discounted, bounded-
reward strictly Markov decision process, either there is an agent a(cid:48) such that

∀s ∈ S(cid:0)Qa(s, a(cid:48)(s)) ≥ Qa(s, a(s))(cid:1) ∧ ∃ s ∈ S (Qa(s, a(cid:48)(s)) > Qa(s, a(s)))

(2.2.2)

or a is an optimal agent.

This theorem also holds if this condition is appended:

∃!s ∈ S(a(cid:48)(s) (cid:54)= a(s)).

(2.2.3)

With this condition, Theorem 2.1 shows that every agent a either has a superior neighbor a(cid:48) (an
agent which diﬀers from a in its response to exactly one state), or it is optimal.

This means that any ﬁnite strictly Markov decision process has a discrete “convexity”; every
imperfect agent has a superior neighbor. Unfortunately, many of the problems of interest in modern
Reinforcement Learning are not ﬁnite or, if ﬁnite, are too large for the Policy Improvement Algorithm,
which exploits this theorem, to be tractable [8, p. 54].

2.3 The Assumptions of Dynamic Programming

Modern Reinforcement Learning confronts many problems which do not satisfy the assumptions
of the Policy Improvement Theorem. Many modern problems fail directly, using inﬁnite decision
processes. Many others technically satisfy the requirements of the theorem, but are so large as to
render the guarantees of policy improvement notional with modern computational techniques. Still
others fail the qualitative conditions, for example, by not being strictly Markov.

The calculations necessary to guarantee that one policy is an improvement upon another require
that the learner have knowledge of each state and action in the process, as well as of the dynamics of
the decision process. While excessive size of S or A are easiest to exhibit, the interplay of the size of
these sets tends to be the true source of the problem. Diﬃculties caused by this interplay are known
as the curse of dimensionality, a term which Bellman used to describe the way that problems with
many aspects can be more diﬃcult than the sum of their parts [6, p. ix]. For example, the size of the
set of agents grows exponentially in the number of states and polynomially in the number of actions:

|A| = |A||S|.

(2.3.1)

In large problems, the curse of dimensionality makes the guarantees of Dynamic Programming almost
impossible to achieve in practice. Curiously, the performance of DP methods seems to degrade slowly,
even as the information necessary for the guarantees of DP quickly becomes unachievable.

2.4 Incomplete Information and Dynamic Programming

One way to address the size of a Reinforcement Learning problem is to collect the information necessary
for Dynamic Programming more eﬃciently. Dynamic Programming methods which approximate Qa
rely on several types of information, all based in the superiority condition:

∀ s ∈ S(cid:0)Qa(s, a(cid:48)(s)) ≥ Qa(s, a(s))(cid:1).

(2.4.1)

Q-based methods rely on the learner’s ability to approximate the Qa-function. There are two ways
to approach this problem: Qa can be approximated directly (i.e. separately for each state), or it can
be calculated from knowledge or approximations of several aspects of the process:

1. The set of states, S,

2. The set of actions, A,

9

3. The immediate reward function, r,

4. The state-transition function, σ, and

5. The agent a.

Calculating the Qa function is, in a sense, more eﬃcient: if the quantities above are known to
an acceptable degree of precision, then a consistent action-value function can be imputed without
repeated sampling.

The complexity of this operation is a function of |S| × |A|. In a sense, this method is inexpensive;
the number of state-action pairs is much smaller than the number of possible sample paths, |Φt| =
|S × A|t > |S × A|. It is also typically much smaller than the number of agents, |A|S.

This kind of approximation is fundamental to Dynamic Programming methods, and serves as
the basis for many exploration methods. In general, reinforcement learning algorithms are assumed
to have full knowledge of A and a, but they may not have complete information about S, r, or σ.
Thus, exploration has sometimes been deﬁned as pursuing experiences of state-action pairs (s, a),
speciﬁcally the quadruplets which can be associated with them in a path, (st, a, r(st, a), st+1). If
enough of these are collected, it is possible to approximate the expected reward of each state-action
pair, as well as the transition probabilities σ(s, a).

Importantly, because these quadruplets can be induced by particular actions (i.e., (s, a) can be
generated by taking the action a at a time when s is visited), the task of collecting the information
about these quadruplets can be reduced to a simple two-step formula: ﬁrst, visit each state s ∈ S,
and second, attempt each action in that state. Under the right circumstances, proceeding in this way
results in experience of every state-action pair, {(s, a), |s ∈ S, a ∈ A}.

This simple notion of exploration is both eﬃcient and eﬀective in some circumstances: if S is small
and all of its states may be easily visited, and A is small, then it is easy to consider every possible
state-action pair. After enough sampling, this allows the Q-function to be approximated. Outside
of problems which satisfy those conditions, it is natural to consider other notions of exploration.
Sometimes, these deﬁnitions take a more descriptive form, for example in Thrun: “exploration seeks
to minimize learning time” [14]. Even without suﬃcient information about the process to guarantee
policy improvement, Dynamic Programming methods have performed admirably [3, 4].

These results could be seen as a testament to the eﬃcacy of Dynamic Programming under
non-ideal circumstances, however, there is a curious countervailing trend: in many of these problems,
simple or black-box methods such as Evolution Strategies [13] (an implementation of ﬁnite-diﬀerences
gradient approximation) have been able to match the performance of modern Dynamic Programming
methods. This conﬂicts with the present theory in two ways: ﬁrst, it challenges the idea that
Dynamic Programming is uniquely eﬃcient, or uniquely suited to Reinforcement Learning problems.
Second, because these methods are not dynamic, they lack the usual information requirements which
exploration is supposed to resolve, yet they are useless without exploration (as seen in Figure 3) - if
information about state-action pairs is not directly employed, what is the role of exploration?

In the next section we begin by describing the properties of the methods of Subsection 1.2.2 which
guarantee that eventually, every state-action pair will be experienced. We then discuss methods
which address circumstances where certain states are diﬃcult to handle

3 Exploration and contentment

Dynamic exploration can be divided into two categories: directed exploration and undirected
exploration [14]. Undirected methods explore by using random output functions like ε-Greedy
Sampling and Thompson Sampling (see Subsection 1.2.2) to experience paths which would be
impossible with the corresponding greedy agent. These are called undirected because the changes
which the output functions make to the greedy version of the agent are not intended to cause the
agent to visit particular states. Instead, these methods explore through a sort of serendipity.

10

Directed methods have speciﬁc goals and mechanisms more narrowly tailored to the optimization
paradigm they support. Some directed methods, like #Exploration [15] (see Deﬁnition A.5) seek
to experience particular state-action pairs by directing the learner through exploration bonuses to
consider agents which lead to state-action pairs which have been visited less in the learning history.
In order to apply the state-action pair formulation of exploration to large and even inﬁnite problems,
#Exploration employs a hashing function to simplify and discretize the set set of states.

Other directed exploration methods, like that of Stadie et al. [16] (see Deﬁnition A.6) employ
exploration bonuses to incentivize agents which visit states which are poorly understood. Stadie et
al. begin by modeling the state-transition function (in a strictly Markov process) σ with a function
M. In each step of the process, the model M(st, at) guesses the next state st+1. After guessing,
the model is trained on the transition from (st, at) to st+1. When the model is less accurate (i.e.
when the distance 5 between st+1 and M(st, at) is large), Stadie et al. reason, there is more to be
learned about st, and their method assigns exploration bonuses to encourage agents which visit
st. Unfortunately, none of these methods can recover the guarantees of Dynamic Programming
in problems which do not satisfy the requirements of the theory of Dynamic Programming. More
detailed descriptions of these methods may be found in Appendix A.

3.1 The Essence of Exploration

The brief survey above and in Appendix A is far from complete, but it contains the core strains
of most modern exploration methods. A more thorough discussion of exploration may be found
in [14] or [17]. In spite of its incompleteness, our survey suﬃces for us to reason generally about
exploration, and about its greatest mystery: why do methods which were developed to collect exactly
the information necessary for dynamic programming appear to help na¨ıve methods succeed?

Because na¨ıve methods do not make use of action-values, nor do they collect information about
particular state-action pairs, the dynamic motivations of these exploration methods cannot explain
their eﬃcacy when paired with na¨ıve methods of exploitation. Instead, there must be something
about the process of exploration itself which aids na¨ıve methods. That poses a further challenge to
the dynamic paradigm: if exploration is eﬀective in na¨ıve methods for non-dynamic reasons, to what
extent do those reasons contribute to their eﬀect in dynamic methods?

These exploration methods seem to share little beyond their motivations. One other thing which
they share - and which they by necessity share with every reinforcement learning algorithm - is
that their mechanism is, ultimately, aiding in the selection of the next set of agents An. Undirected
methods accomplish this by selecting an output function, and directed methods go slightly further in
inﬂuencing the parameters of the agents, but this is their shared fundamental mechanism.

What diﬀerentiates exploration methods from other reinforcement learning methods is that they
inﬂuence the selection of agents not to improve reward in the next epoch, as is standard in exploitation
methods, but to collect a more diverse range of information about the process. It is the combination
of this mechanism and purpose which makes a method explorative:

Deﬁnition 3.1 (Exploration). A reinforcement learning method is explorative if it inﬂuences the
agents a ∈ A for the purpose of information collection.

We now know two things about exploration: in Section 2 we established that exploration was a
process which sought additional information about the process. We have now added that exploration
is accomplished by changing the agents which the learner considers. This, however, does not resolve
our question: under the dynamic programming paradigm, these changes are made so as to collect
the information necessary to calculate action-values. What is the information which na¨ıve methods
require, and how does dynamic exploration collect it? To what extent does that other information
contribute to the eﬀectiveness of those methods in dynamic programming?

5Stadie et al. assume that S is a metric space.

11

To continue our study of exploration in na¨ıve learning, we begin in the next section with a
discussion of Novelty Search, an algorithm which uses a practitioner-deﬁned behavior function B to
explore behavior spaces. In Section 5, we describe a general substrate for na¨ıve exploration which is
general enough to contain other exploration substrates, and is equipped with a useful topological
structure.

4 Na¨ıve Exploration

One of the most prominent examples of na¨ıve exploration is an algorithm called Novelty Search [7].
In contrast to the other methods which we discuss in this work, its creators do not describe it as a
learning algorithm. Instead, they call novelty search an “open-ended” method. Nonetheless, methods
which incorporate Novelty Search can usually be analyzed as learning algorithms, since they typically
satisfy Deﬁnition 1.15, with the possible exception of the “purpose” of the method.

4.1 Novelty Search

Novelty Search is an algorithmic component of many learning algorithms which was introduced by
Joel Lehman and Kenneth O. Stanley [7, 18]. Unlike other learning methods, Novelty Search works
to encourage novel, rather than high-reward agents.

Deﬁnition 4.1 (Novelty Search). Novelty Search is a component which can be incorporated into
many learning algorithms which deﬁnes the behavior B(a) and novelty N (a) of agents which the
learning algorithm considers a ∈ An.

Because Novelty Search does not specify an optimizer, the details of implementation can vary,
but the “search” in Novelty Search refers to the way that Novelty Search methods seek agents with
higher novelty scores N (a). These scores are based upon the scarcity of an agent’s behavior B(a)
within a behavior archive χ (4.1.2).

Deﬁnition 4.2 (Behavior). The behavior of an agent is the image of that agent6 under a behavior
function (or behavior characterization)

a function from the set of agents A into a space of behaviors X equipped with a distance dX .7

The behavior archive χ in Novelty Search is a subset of the behaviors which have been observed

in the learning process,

B : A → X;

(4.1.1)

(cid:91)

χ ⊂

m≤n

{B(a) | a ∈ Am}.

(4.1.2)

In general, the archive is meant to summarize the behaviors which have been observed so far using
as few representative behaviors as possible, so as to minimize computational requirements.

Deﬁnition 4.3. The novelty N of a behavior B(a) is a measure of the sparsity of the behavior
archive around B(a). In Abandoning Objectives, Lehman and Stanley use the average distance7 from
that behavior to its k nearest neighbors in the archive, K ⊂ χ:

N (B(a)) =

1
k

(cid:88)

B(c)∈K

dX (B(a), B(c)).

(4.1.3)

6In practice, many behavior functions are functions of a sampled path of the agent, rather than the agent itself.
7 The literature on Novelty Search is most sensible when the range of the behavior function is assumed to be a
metric space, and novelty search is usually discussed under that pretense. However the “novelty metrics” employed
in Abandoning Objectives are not metrics in the mathematical sense (see Deﬁnition 5.1) Instead, they are squared
Euclidean distances - a symmetric [19]. We use the word distance to refer to a broader class of functions, and use the
word metric and its derivatives in the formal sense.

12

Novelty Search reveals something about na¨ıve methods as a class: because they do not operate
under the dynamic paradigm, individually manipulating the ways that agents respond to each
situation possible in the process, they must employ another structure to understand the agents which
it considers, and to determine An+1. For this purpose, Novelty Search uses the structure of the
chosen behavior space X. Let us consider the structures other na¨ıve methods might use.

In the case of exploitation, a simple structure is available: reward. The purpose of exploitation is
to improve reward, so reward is the relevant structure. Under the dynamic framework, reward is
decomposed into the immediate rewards r(s, a), and agents are speciﬁed in relation to these. In the
na¨ıve framework, that decomposition is not availed, so only the coarser reward of an agent, R(a),
can be used. In some problems, other structures may correlate with reward, but these correlations
can be inverted by changes to the state-transition function or reward function, so the only a priori
justiﬁable structure for exploitation is reward.

Exploration is more complex. In Dynamic Programming, the information necessary to solve a
Reinforcement Learning problem is well deﬁned, but in the na¨ıve framework, there is not a general
notion of information “suﬃcient” to solve a problem (except for exhaustion of the set of agents).
That is, na¨ıve exploration does not have a natural deﬁnition in the same way as na¨ıve exploitation or
dynamic exploration. Let us then consider a deﬁnition of exclusion:

Deﬁnition 4.4 (Na¨ıve Exploration). A learning method is a method of na¨ıve exploration if

i. The method itself is na¨ıve, and

ii. The set of agents is explored using a structure that is not induced by the expected reward.

Under this deﬁnition, Novelty Search is clearly a method of na¨ıve exploration. Let us consider it
further. Rather than treat it as a unique algorithm, we can consider Novelty Search to be a family of
exploration methods, each characterized by the way that it projects the set of agents A into a space
of behaviors X.

Novelty Search can be analyzed with respect to several goals: it may be viewed as an explorative
method, or, when paired with a method of optimization, it may be seen as an open-ended or learning
method in and of itself. Unfortunately, the capacity of Novelty Search to accomplish any of these
goals is compromised by the subjectivity of the behavior function. Because the behavior function
relies on human input, the exploration which is undertaken, the diversity which Novelty Search
achieves, and the reward at the end of a learning process involving Novelty Search all rely on the
beliefs of the practitioner. Instead of viewing this subjectivity as a problem, Lehman and Stanley
embrace it, suggesting that behavior functions must be determined manually for each problem,
writing:

There are many potential ways to measure novelty by analyzing and quantifying behaviors
to characterize their diﬀerences. Importantly, like the ﬁtness function, this measure must
-Lehman and Stanley [20]
be ﬁtted to the domain.

If followed, this advice would make it virtually impossible to disentangle Novelty Search as an
algorithm from either the problems to which it is applied or the practitioners applying it. Fortunately,
some authors have rejected this suggestion, pursuing more general notions of behavior.

We now present a brief overview of some of the behavior functions in the literature, including
those described in Lehman and Stanley’s pioneering Novelty Search papers [7, 18]. Appendix B
presents a more detailed discussion of these functions as well as some other behavior functions which
could not be included in this summary.

In their ﬂagship paper on Novelty Search, Lehman and Stanley [7] consider as their primary
example problem a two-dimension maze. As a secondary example they take a similar navigation
problem in three dimensions. Importantly for our discussion, behavior functions in both environments
admit a concept of the agent’s position. Lehman and Stanley introduce two behavior functions in

13

their study of Novelty Search, both of which are functions of the position of the agent throughout a
sampled truncated path φa

t ∼ Φa.

The simpler of these behavior function takes the agent’s ﬁnal position (i.e. the position of the
agent when the path is truncated), and the more complex behavior function takes as behavior a
list of positions throughout the path taken at temporal intervals. These functions provide insight
into Lehman and Stanley’s intuitions about behavior: to them, behavior relates to the state of the
process, rather than to the agent or its actions. Because the state of the process depends upon the
interaction of the process and the agent, these deﬁnitions assure that behavior reﬂects the interaction
between the process and the agent.

This is important. Notice that under the function-approximation framework (Subsection 1.2.2),
any function with appropriate range and domain could be treated as an agent. As a result, if behavior
were taken to be a matter of the function approximator alone, one would be forced to accept the
premise that the structure of the set of agents should be identical for any pair of processes with
the same sets of states and actions. Identical even when the state-transition functions diﬀer. In
other words, all three-dimensional navigation tasks would have the same space of agents. This issue
certainly suﬃces to explain Lehman and Stanley’s attitude toward behavior functions. It does not,
however, necessitate that approach.

Other authors have considered near-totally general notions of behavior. One group of these
focuses on collating as much information as possible about every point in a path. In the case of
Gomez et al. [21], this involves concatenating some number of observed states. Conti et al. [22] take
a similar approach, replacing the states of the decision process with RAM states - the version of the
state of the decision process stored by the computer.

Another class of general notions of behavior focuses instead on the actions of the agent itself, as

viewed across a subset of S. We call this class of functions Primitive Behavior.

4.2 Primitive Behavior

Primitive behavior functions deﬁne the behavior of a strictly Markov agent a as the restriction of the
agent to a subset of S:

Deﬁnition 4.5 (Primitive Behavior). A behavior function B : A → M is said to be primitive if it is
a collection of an agent’s actions in response to a ﬁnite set of states X ⊂ S; B is primitive iﬀ

B(a) = {(s, a(s)) | s ∈ X}, and

(4.2.1)

the distance on the set of behaviors is given by a weighted (by ws) sum of distances between the
actions of the agents on X:8

d(B(a), B(b)) =

(cid:88)

s∈X

wsdA (a(s), b(s)).

(4.2.2)

This notion of behavior, with slight modiﬁcations, has appeared in several papers in the Rein-
forcement Learning literature [23–26]. At least one existing work uses this notion of behavior in
Novelty Search [23]. Another [24] uses it for optimization with an algorithm other than Novelty
Search. [23, 25, 26] weight the constituent distances (i.e. ws is not constant), and [25] uses primitive
behavior to study the relationship between behavior and reward.

A number of important properties of primitive behavior have been described. For example, [24]
notes that agents with the same behavior may have diﬀerent parameters. [23] takes implicit advantage
of the fact that agents which do not encounter a state do not meaningfully have a response to it (a
fact we address in Item iib of Section 5), and [25] considers the states which an agent encounters an
important aspect of the agent, using them to create equivalence classes of agents.

8This assumes that the set of actions A is a metric space with dA .

14

We call this notion of behavior primitive because it is the simplest notion of behavior which
completely describes the interaction between the agent and the process [on X]. Thus, for an
appropriate set X, the primitive behavior contains all information relevant to (P, a). So long as a
deﬁnition of behavior only depends on the interaction between P and a, primitive behavior thus
suﬃces to determine every other notion of behavior.

Clearly, this does not follow the advice of Lehman and Stanley [7]; it is completely unﬁtted to
the underlying problem. In exchange for this lack of ﬁt, primitive behavior is fully general. Further,
because primitive behavior is simply a restriction of the agent itself, every other notion of behavior
is downstream of primitive behavior, provided that an appropriate set X is used.

However, because the selection of X, and of the weights ws is itself a matter of choice, primitive
behavior in general remains somewhat subjective. In ﬁnite problems, it is possible to assign to every
state a non-zero weight, which produces a sort of objective distance, but this remains problematic; two
agents might diﬀer on a state which neither of them ever visits. Should agents a and b which produce
identical processes (P, a) and (P, b) really be described as diﬀerent? We contend in Deﬁnition 5.6
that the answer is “no”.

Our task in the next several sections is to resolve this and other issues with primitive behavior.
In the next section we approach the matter of a general substrate for exploration from the ground up.
We begin with the simplest version of primitive behavior (that associated with a single state) and
proceed to a “complete” notion of behavior: a distance between agents which properly discriminates
between agents (see Deﬁnition 5.6). In Section 6, we demonstrate some properties of this completed
space, which we call the agent space.

5 Seeking a Structure for Na¨ıve Exploration

Under Deﬁnition 4.4, na¨ıve exploration is a category of exclusion. Any na¨ıve method which is not
exploitative, i.e. which does not use the structure induced by an agent’s expected reward, is a method
of na¨ıve exploration. Our task in this section is to develop a good structure for exploration. That is,
to develop a structure on the space of agents, other than reward, which captures important aspects
of the relationships of agents to one another and to the process. We seek a structure which:

i. Exists in every discrete-time decision process,

ii. Correctly describes equivalence relations between agents (see Deﬁnitions 5.1 and 5.6), i.e.

(a) Identiﬁes agents which diﬀer in aspects irrelevant to their processes,
(b) Distinguishes agents which diﬀer in aspects which are relevant to their processes, and

iii. Naturally describes important relations on the set of agents.

Such a structure would allow us to compare structures which are used in na¨ıve exploration methods,
including, for example, the various behavior functions which have been used in Novelty Search. If
computationally tractable, such a structure could also provide the basis for a new exploration method,
or perhaps even a new deﬁnition of exploration. Let us begin by considering one possible kind of
structure for this.

5.1 Prototyping the Agent Space

In contending with a generic discrete-time decision process, few assets are available to deﬁne the
structure of an agent space. At a basic level, there are only two types of interaction between an agent
and a process: the generation of a state, in which the decision process acts on the agent (P → a), and
the action of an agent, in which the agent acts on the decision process (a → P). Every other aspect
of a decision process may be regarded as a function of those interactions. Let us begin by using these
basic interactions to deﬁne a metric space, following the behavior functions used in Novelty Search.

15

Deﬁnition 5.1 (Metric). A metric on a set X is a function

d : X × X → R+ ∪ {0}

(5.1.1)

which satisﬁes the metric axioms:

1. d(x, y) = 0 ⇐⇒ x = y

2. d(x, y) = d(y, x)

3. d(x, y) ≤ d(x, z) + d(z, y)

Identity of Indiscernibles

Symmetry

Triangle Inequality

Let us begin with a simple case, comparing strictly Markov agents on their most basic elements:
their actions on the process in response to a single state s.

5.2 The Distance on s

Let A be a metric space with metric dA . Then, we deﬁne the distance between agents a and b in a
single-state decision on the state s:

Deﬁnition 5.2 (Distance on s). The distance on s between a and b is the distance of their actions
on s:

ds(a, b) = dA (a(s), b(s)).

(5.2.1)

Importantly, this distance is not a metric.7 Instead, it is a pseudometric; it cannot distinguish agents
which act identically on s but diﬀerently on another state s(cid:48).

Deﬁnition 5.3 (Pseudometric). A pseudometric on a set X is a function

d : X × X → R+ ∪ {0}

(5.2.2)

which satisﬁes the pseudometric axioms

1. d(x, y) = 0 ⇐= x = y

2. d(x, y) = d(y, x)

3. d(x, y) ≤ d(x, z) + d(z, y)

Indiscernibility of Identicals

Symmetry

Triangle Inequality

This distance describes the diﬀerences between a and b on the state s, but decision processes involve
many states, potentially inﬁnitely many. Certainly, the action which agents take in response to a
single state does not suﬃce to explain the diﬀerences between agents. Let us begin to resolve this by
comparing agents on a ﬁnite set of states X (|X| > 1).

5.3 The Distance on X

Having deﬁned the distance between agents on a single state s, we can deﬁne the distance on a set of
states X by summation. Denote the distance between agents on a ﬁnite set of states X as dX (a, b).

Deﬁnition 5.4 (Distance on X). The distance on X ⊂ S between a and b is the sum of the distances
between a and b on each element of the set:

dX (a, b) =

=

(cid:88)

s∈X
(cid:88)

s∈X

ds(a, b)

dA (a(s), b(s)).

16

(5.3.1)

(5.3.2)

Depending upon the process and the set X itself, this could contain all of the states in a process
(for decision processes with ﬁnite sets of states), or a set of important states. We can also extend this
notion of distance by weighting the distance at each state s ∈ X with a weight ws, producing the
primitive behavior of Subsection 4.2,

d(B(a), B(b)) =

(cid:88)

s∈X

wsdA (a(s), b(s)).

Consider a special case for X: let X be the set of states observed before time T in a path φ:

Deﬁnition 5.5 (The distance on φt).

dφt(a, b) =

(cid:88)

t∈Zt

dφi(s)(a, b),

(4.2.2)

(5.3.3)

Which is the distance between a and b over a truncated path. When φ is drawn from Φa, dφt is
especially interesting; it is a description of the way that b would have acted diﬀerently from a over a
path that a actually encountered, a sort of backseat-driver metric.

The distance on φt is powerful in a number of respects. Let us consider the case dφt(a, b) = 0.
Clearly, this implies that a and b do not diﬀer at all on this path - presented with the same initial
state, they would produce exactly the same truncated path. Unfortunately, the distance at φt still
fails to satisfactorily distinguish agents - it says nothing about paths in which other states are
encountered, or longer paths, or about the stochastic nature of decision processes. Let us state these
failings directly so that we may address them:

Remark 5.1 (Three Properties).

I. The distance on a truncated path φt drawn from Φa is not reciprocal; it describes how b diﬀers

from a, but not how a diﬀers from b,

II. This distance ignores the stochasticity of the process; the ways in which a and b diﬀer on φt do

not necessarily imply anything about the other paths which a experiences,

III. The distance on a truncated path cannot account for inﬁnite paths; if the agents are not assumed
to be strictly Markov, one can easily construct a pair of agents a and b which diﬀer only on
longer paths. Even in the strictly Markov case, some states might only be possible after t.

5.4 The Role of Time

It is common in Reinforcement Learning to treat problems with inﬁnite time horizons by weighting
sums with a discount function ω : N → [0, 1]. If the space of actions A is bounded,9 and
(cid:88)

ω(t) < ∞

(5.4.1)

Then we may deﬁne the distance between strictly Markov agents a and b on a complete path:

t∈N

dφ(a, b) =

(cid:88)

t∈N

ω(t)dφt(s)(a, b).

Clearly, if dA is bounded, then [27, p. 60]

dφ(a, b) ≤ sup
a1,a2∈A

{dA (a1, a2)} ·

ω(t) < ∞.

(cid:88)

t∈N

9A metric space A is bounded iﬀ sup{dA (x, y) | x, y ∈ A} < ∞

17

(5.4.2)

(5.4.3)

Thus, this pseudometric resolves the problem of Item III of Remark 5.1, describing the diﬀerences in
the action of agents over an inﬁnite path.

Our deﬁnition of dφ(a, b) readily admits a change that allows the distance on a path to be deﬁned

for agents which are not strictly Markov. All we must do is remove the symbols “(s)”:

dφ(a, b) =

(cid:88)

t∈N

ω(t)dφt(a, b).

(5.4.4)

Let us use the distance on φ to deﬁne a notion of distance which incorporates the stochastic aspects
of the interaction between an agent and a process, resolving the problem of Item II of Remark 5.1.

5.5 The Distance at a

Consider Φa, the distribution of paths generated by the interaction of an agent a with P. Given a
discount function ω satisfying (5.4.1), [27, p. 318]

E
φ∼Φa

[dφ(b, c)] = E

φ∼Φa

= E

φ∼Φa

= lim
n→∞

E
φ∼Φa

= lim
n→∞

(cid:88)

t<n

E
φ∼Φa

(cid:34)

(cid:34)

(cid:34)

(cid:35)

ω(t)dφt(b, c)

(cid:88)

t∈N

(cid:35)

ω(t)dφt(b, c)

lim
n→∞

(cid:88)

t<n

(cid:35)

ω(t)dφt(b, c)

(cid:88)

t<n

[ω(t)dφt(b, c)]

(5.5.1)

(5.5.2)

(5.5.3)

(5.5.4)

exists and is bounded above. We call this quantity the distance at a.

Because the expectation integrates the distance on φ over all of the paths of a, da compares b and
c on every part of the process which a experiences. This guarantees that the stochasticity involved in
the interaction of a and P is considered in the comparison. However, the stochasticity involved in
the processes (P, b) and (P, c) may be more relevant to their comparison than that produced by a.
In the next section, we begin to address this by considering the case a = b.

5.6 da(a, b)

In order to state our next result, we must introduce agent identity. Agent identity collapses two
artiﬁcial distinctions between representations of agents caused by the use of function approximators.
First, it treats approximators which are diﬀerently parameterized but identical as functions (e.g.
because of a permutation of the order of parameters, as noted by [24]) as identical. Second, in keeping
with the methods of [25] it treats functions which diﬀer, but only on a set of probability 0 as identical.

Deﬁnition 5.6 (Agent Identity (a ≡ b)). Let us say that the agents a and b are identical as agents
(a ≡ b) if and only if the set of paths where they diﬀer has probability 0 in Φa;

PΦa [{φ | ∃ t(a(φt) (cid:54)= b(φt))}] = 0.

(5.6.1)

That is, a and b are identical as agents if and only if the probability of a encountering a path where
they diﬀer is 0.

Remark 5.2 (Agent Identity and da(a, b)). Notice that this implies da(a, b) = 0.

Theorem 5.1 (Identical agents the same local distance). If a and b are identical as agents, then

da = db.

18

(5.6.2)

Proof by induction. [Base case:] Suppose da(a, b) = 0. Then,

da(a, b) = lim
n→∞

(cid:88)

t<n
(cid:18)

=⇒ ∀ t ∈ N

=⇒

E
φ∼Φa

E
φ∼Φa
E
φ∼Φa

[ω(t)dφt(b, c)] = 0

(cid:19)

[dφt(a, b)] = 0

[dφ0(a, b)] = dσ0(a, b) = 0.

(5.6.3)

(5.6.4)

(5.6.5)

That is, if da(a, b) = 0 (a and b are identical), then dσ0 (a, b) = 0 (they act identically on truncated
prime paths of length 0). Then, the joint distributions (Φ(cid:48)a
0)) =
(σ0, b(φ(cid:48)

0)) = (σ0, a(φ(cid:48)

0)) and (Φ(cid:48)b

0)) are identical.

0 , a(φ(cid:48)

0 , b(φ(cid:48)

Thus, the joint distributions of these and the next state, given by σ, are also identical: the total

variation distance of ((Φ(cid:48)a

0 , a(φ(cid:48)

0)), σ(Φ(cid:48)a

0 , a(φ(cid:48)

0))) = Φ(cid:48)a

1 and ((Φ(cid:48)b

0 , b(φ(cid:48)

0)), σ(Φ(cid:48)b

0 , b(φ(cid:48)

0))) = Φ(cid:48)b

1 is 0;

0

= 0

0 , Φ(cid:48)b
0 )

dΦ(cid:48)a
(a, b)
and TVD(Φ(cid:48)a
= TVD(σ0, σ0) = 0
=⇒ TVD(Φa
0) = 0
=⇒ TVD(Φ(cid:48)a
1 ) = 0.

0, Φb
1 , Φ(cid:48)b

(5.6.6)

(5.6.7)
(5.6.8)

(5.6.9)

(5.6.10)

(a, b) = 0.

The distribution Φa(cid:48)

1 determines the component of da at t = 1. By (5.6.4), we have dΦ(cid:48)a

1

Let t ∈ N and suppose that TVD(Φa(cid:48)

Φa
the resulting distributions Φ(cid:48)a

(a, b) = 0. Then, since
t are the joint distributions of these, they also have total variation 0, and since σ is ﬁxed,
t+1 also have total variation 0;

t ) = 0. By (5.6.4), we have dΦ(cid:48)a

t+1 and Φ(cid:48)b

t and Φb

t , Φb(cid:48)

t

t

dΦ(cid:48)a
(a, b)
and TVD(Φ(cid:48)a
=⇒ TVD(Φa
=⇒ TVD(Φ(cid:48)a

t , Φ(cid:48)b
t )
t , Φb
t)
t+1, Φ(cid:48)b

= 0

= 0

= 0

t+1) = 0.

By assumption (5.6.4), we have

and thus, the joint distributions also have total variation 0:

dΦ(cid:48)a

t+1

(a, b) = 0,

TVD(Φa

t+1, Φb

t+1).

The same holds for all t ∈ N, so we have

da(a, b) = 0 ⇐⇒ TVD(Φa, Φb) = 0

=⇒ ∀ x, y ∈ A(dΦa (x, y) = dΦb (x, y))
⇐⇒ da = db.

(5.6.11)

(5.6.12)

(5.6.13)

(5.6.14)

(5.6.15)

(5.6.16)

(5.6.17)

(5.6.18)

(5.6.19)

Q.E.D.

Corollary 5.1 (Identical agents are indiscernible under their shared local distance.). While the identity
of indiscernibles (see Deﬁnition 5.1) does not generally hold for da, it does hold if

1. We consider the agents as agents, and

19

2. The distance is taken at one of the considered agents, i.e. da(a, ·);

Proof. By Theorem 5.1,

da(a, b) = 0 ⇐⇒ a ≡ b.

da(a, b) = 0 ⇐⇒ ∀ x, y ∈ A (da(x, y) = db(x, y))
=⇒ db(a, b) = da(a, b)
=⇒ db(a, b) = 0

(5.6.20)

(5.6.21)

(5.6.22)

(5.6.23)

Q.E.D.

Remark 5.3 (Symmetry of da). Notice that because da is an integral of distances between agents,
which are symmetric, da is symmetric;

Corollary 5.1 demonstrates that the agent identity relation of Deﬁnition 5.6 is reﬂexive; for every

pair of agents a, b, a ≡ b ⇐⇒ b ≡ a.

da(b, c) = da(c, b).

(5.6.24)

5.7 dc(a, b) = 0: When Distance 0 Does Not Imply Agent Identity

The picture provided above when the distance is taken at one of the agents being compared is
complicated when the comparison is made from a diﬀerent vantage point. Let us consider some of
these cases:

1. Sometimes, identical agents may be distinguished by a local distance,

a ≡ b ∧ dc(a, b) > 0.

2. Sometimes, diﬀerent agents will not be distinguished,

a (cid:54)≡ b ∧ dc(a, b) = 0.

3. Sometimes, identical distances imply identical agents,

4. Sometimes, identical distances don’t,

da = db =⇒ a ≡ b.

da = db ∧ a (cid:54)≡ b.

All of these problems stem from the issues of state visitation mentioned in Subsection 4.2 and

[25]: unless a ≡ b, a may visit paths which b does not, and vice versa.

Example 5.1 (a ≡ b, but dc(a, b) > 0). In general, the functions a and b can be identical as agents,
while they diﬀer in their responses to unvisited paths. Then, from the perspective of an agent which
visits such paths, a and b appear diﬀerent.

Example 5.2 (a (cid:54)≡ b, but dc(a, b) = 0). Similarly, it is possible for a and b to be identical on every
path which c visits, but diﬀer when a or b control the process.

Example 5.3 (da = db =⇒ a ≡ b). In general, local distances form a bijection with the distributions
Φa and the processes (P, a), and thus da = db ⇐⇒ Φa = Φb ⇐⇒ (P, a) = (P, b) ⇐⇒ a ≡ b.

20

Example 5.4 (da = db, but a (cid:54)≡ b). However, when the set of agents is restricted, this is not necessarily
true. If, for example, agents are assumed to be strictly Markov (see Remark 1.3), then all that
matters to the equivalence of the functions da and db is the states which they visit; if σ(φ(cid:48)
t, a1) =
σ(φ(cid:48)
t could nonetheless produce
“identical” distances, when the range of the distance is restricted to pairs of strictly Markov agents.

t, a2), a1 (cid:54)= a2, then agents a and b which diﬀer in their response to φ(cid:48)

With so few assurances, the local distances may seem pointless. Are they nothing more than
markers of identity? No, they are much more; Subsection 6.1 demonstrates that da(a, b) = 0 ⇐⇒
a ≡ b is not a special case. Instead, the local distances themselves are continuous in the agent space:
as a and b approach one another under either local distance, i.e. as |da(a, b)| or |db(a, b)| goes to 0,
and so does supx,y∈A |da(x, y) − db(x, y)|.

6 The Agent Space

The collection of local distances described in Section 5 is an odd basis for the structure of an agent
space; rather than a single, objective notion of distance, each agent a deﬁnes its own local distance
da. When paired with the set of agents, each local distance deﬁnes a pseudometric space (A, da),
which describes the ways that agents diﬀer on Φa.

Subsection 5.6 establishes relationships between local distances, but only in the case of identical
agents which diﬀer as functions. We have not yet related the local distances of non-identical agents.
In particular, we have not established that a collection of local distances deﬁnes a single space.

One interpretation of the collection of local distances is as a premetric (see Subsection 6.2), in
a manner analogous to the Kullback-Leibler Divergence. However, da can also be treated as more
than a premetric; it need not be asymmetrical, nor need it violate the triangle inequality, because
each agent deﬁnes a local distance that describes an internally-consistent pseudometric space. We
continue this discussion in greater detail in Subsection 6.2, employing the premetric to provide a
simple topology equivalent to that deﬁned by convergence in the agent space (Deﬁnition 6.1).

In the next section, we unify the pseudometric spaces produced by each local distance to create an
objective agent space, whose topology is compatible with many important aspects of Reinforcement
Learning, including standard function approximators (e.g. neural networks) and standard formulations
of reward (see Equation 7.2.4).

6.1 Convergence in Agent Spaces

Theorem 5.1 proves that identical agents have the same local distance,

a ≡ b =⇒ da = db.

(6.1.1)

Corollary 5.1 gives an important condition for equivalence: agents are identical, and thus have
identical local distances, whenever da(a, b) = 0. The next step in our analysis of the local distance
is to consider the case where a and b are close to one another, but their distance is greater than 0.
Consider the case

0 < da(a, b) < δ,

(6.1.2)

with δ > 0. In order to simplify the remainder of this section, we restrict ourselves to stochastic
agents. Let the metric on A, dA , be the total variation distance TVD(a1, a2). In this case, the logic
of Theorem 5.1 can be extended. Theorem 5.1 demonstrates that two agents which are at every time
identical must produce identical distributions of paths, and, as a result, identical local distances.
Consider a pair of agents a and b, which have a distance less than δ on Φa, da(a, b) < δ, with ω(t) = 1.

21

Then,

0

0 < dΦ(cid:48)a
dΦ(cid:48)a
[TVD(a(φ(cid:48)

(a, b) ≤ da(a, b) < δ
(a, b) = dσ0(a, b) < δ
0), b(φ(cid:48)
0))] < δ

0

=⇒
⇐⇒ E

φ(cid:48)

0∼σ0

Since σ0 does not vary with the agent, we have

Φ(cid:48)a

0 = Φ(cid:48)b

0 ∧ E
φ(cid:48)

0∼σ0

[TVD(a(φ(cid:48)

0), b(φ(cid:48)

0))] < δ

Likewise,

should imply that

except that since

=⇒ TVD(Φ(cid:48)a

1 , Φ(cid:48)b

1 ) < δ.

dΦ(cid:48)a

1

(a, b) ≤ da(a, b) < δ

TVD(Φ(cid:48)a

2 , Φ(cid:48)b

2 ) < δ,

we must start from a baseline of δ, giving the bound 2δ. In general, we have

TVD(Φ(cid:48)a

1 , Φ(cid:48)b

1 ) < δ,

TVD(Φ(cid:48)a

t , Φ(cid:48)b

t ) < tδ.

(6.1.3)

(6.1.4)

(6.1.5)

(6.1.6)

(6.1.7)

(6.1.8)

(6.1.9)

(6.1.10)

(6.1.8)

(6.1.11)

This bound can be improved by noting that the total variation TVD(Φ(cid:48)a
by (and is in fact equal to) the smaller quantity

1 , Φ(cid:48)b

1 ) can be bounded above

yielding in the general case

dΦ(cid:48)a

0

(a, b) = dσ0 (a, b),

TVD(Φ(cid:48)a

t , Φ(cid:48)b

t ) <

(cid:88)

i∈Zt

dΦ(cid:48)a

i

(a, b).

(6.1.12)

(6.1.13)

With our assumption that ω(t) = 1, we can bound the right side of this inequality above, giving the
looser inequality

TVD(Φ(cid:48)a

t , Φ(cid:48)b

t ) <

(cid:88)

i∈Zt

dΦ(cid:48)a

i

(a, b) < da(a, b) < δ

=⇒ TVD(Φ(cid:48)a

t , Φ(cid:48)b

t ) < δ.

(6.1.14)

(6.1.15)

If the discount function is not the constant value 1 (i.e. if ω(t) < 1 for some t), as assumed above,
the sum above gains a factor of

1
ω(i) :

TVD(Φ(cid:48)a

t , Φ(cid:48)b

t ) <

1
ω(i)

(cid:88)

i∈Zt

dΦ(cid:48)a

i

(a, b).

(6.1.16)

For simplicity we now assume that ω(t) = γt, 0 < γ < 1, though the following results apply to a
more general family of functions (for example, they apply to all monotonic super-exponential decay
functions).

22

Lemma 6.1 (TVD(Φa

t , Φb

t) can be bound above by a function of da(a, b)). Notice that when

s < t =⇒ ω(s) ≥ ω(t) =⇒

1
ω(s)

≤

1
ω(t)

,

so for a ﬁxed distance da(a, b), the maximal total variation TVD(Φa
∀ s ≤ t (cid:0)dΦ(cid:48)a

(a, b) = 0(cid:1) ∧ dΦ(cid:48)a

(a, b) = da(a, b).

t , Φb

s

t

t) is achieved when

Thus, we can bound the total variation TVD(Φa

t , Φb

t) above by 1

γt da(a, b). Thus,

da(a, b) < δ =⇒ TVD(Φa

t , Φb

t) <

1
γt δ.

(6.1.17)

(6.1.18)

(6.1.19)

Lemma 6.1 enables us to prove our next theorem, the limit equivalent of Corollary 5.1. Let us

begin with a deﬁnition.

Deﬁnition 6.1 (Convergence in the Agent Space). We say that a sequence of agents xn converges
to an agent a if and only if the local distance between the agents in the sequence and a goes to 0;

xn → a ⇐⇒ ∀ ε > 0 ∃ m ∈ N (n > m =⇒ da(a, xn) < ε).

(6.1.20)

Theorem 6.1 (The Limit Behavior of Local Distances). Let xn be a sequence of agents converging
to a. Then,

1. ∀ t ∈ N(limxn→a TVD(Φxn
t

, Φa

t ) = 0),

2. dxn → da, and

3. dxn (xn, a) → 0.

Proof of 1. By (6.1.19), we have for any ﬁxed t and any agent xn

da(a, xn) < δ =⇒ TVD(Φa

t , Φxn

t ) <

1
γt δ.

(6.1.21)

By assumption, for every δ > 0 there is an m ∈ N with n > m =⇒ da(a, xn) < δ. For any ε > 0,
there is a δε > 0 with 1

γt δε < ε. Thus, we can select an n ∈ N with

m > n =⇒

=⇒ TVD(Φa

da(a, xn) < δε
t ) < ε.

t , Φxn

(6.1.22)

(6.1.23)

Q.E.D.

Proof of 2. Per the proof of 1, we have for any ε > 0 and any t ∈ N an m giving n > m =⇒
TVD(Φa
) < ε. Let Amax be the bound on A (in the case of total variation, Amax = 1). For each
path φ, we have

t , Φxm
t

dφ(a, b) − dφt(a, b) ≤

∞
(cid:88)

γi Amax

=

γt+1
1 − γ

Amax =

i=t+1
γt+1
1 − γ

,

and we have the analogous bound for Φa

da −

i=t
(cid:88)

i=0

dΦa

i

≤

γt+1
1 − γ

.

23

(6.1.24)

(6.1.25)

(6.1.26)

Remark 6.1 (Notation for the Distance on Distributions of Truncated Paths). We now need to
manipulate terms of this type, for which a bit of notation will be useful: Let

i=t
(cid:88)

dt
a =

dΦa

i

, and

i=0

i=∞
(cid:88)

dΦa

i

.

dt+
a =

i=t+1

Further, for any agent a we can decompose da into

t
(cid:88)

dΦa

t

∞
(cid:88)

+

dΦa

t

i=0

i=t+1
a + (da − dt
dt
a)
a + dt+
dt
a .

da =

da =
da =

Notice that the maximum value of dt

a is

t
(cid:88)

i=0

γtAmax =

1 − γt+1
1 − γ

Amax,

and we can bound dt+

a from above as well,

a ≤ max(da) − max(dt
dt+
a)
1 − γt+1
1 − γ

Amax −

=

Amax

1
1 − γ
γt+1
1 − γ

=

Amax.

Clearly, as t → ∞, this bound goes to 0.

Let ε > 0, ε(cid:48) =

ε(1−γ)

2Amax(1−γt+1) , and let δε(cid:48) be as above. Then, we have

da(a, b) < δε(cid:48)

=⇒ TVD(Φa

t , Φb

t) < ε(cid:48) =

ε(1 − γ)
Amax(1 − γt+1)

=⇒

a − dt
dt

b <

<

1 − γt+1
1 − γ
1 − γt+1
1 − γ

Thus, we have

Amax TVD(Φa

t , Φb
t)

Amaxε(cid:48) =

ε
2

.

(cid:21)

da − db = dt

a + dt+

a − (dt
(cid:20)

b + 2

b + dt+
b )
γt
1 − γ
(cid:21)

Amax

γt
1 − γ

.

≤ dt

a − dt
(cid:20)

<

ε
2

+ 2

Amax

24

(6.1.27)

(6.1.28)

(6.1.29)

(6.1.30)

(6.1.31)

(6.1.32)

(6.1.33)

(6.1.34)

(6.1.35)

(6.1.36)

(6.1.37)

(6.1.38)

(6.1.39)

(6.1.40)

(6.1.41)

(6.1.42)

Now, set t great enough that 2
n > m =⇒ da(a, xn) < δ ε

2

(cid:104)

Amax

(cid:105)

γt
1−γ

< ε

2 , and set b = xn. Per part 1, we can select an m with

. Then, combining lines (6.1.40) and (6.1.42), we have

Proof of 3. Applying 2, we have

Since da(a, xn) = 0, we have

da − dxm < ε.

dxn → da.

dxn (a, xn) → da(a, xn) = 0.

(6.1.43)

Q.E.D.

(6.1.44)

(6.1.45)

Q.E.D.

This theorem demonstrates that agents which are close in the agent space have close perspectives
and produce close local distances. In fact, the proof of Item 2 of Theorem 6.1 demonstrates that the
local distances da which represent those perspectives are uniformly continuous in the agent. Further,
Item 1 of Theorem 6.1 demonstrates that similar agents produce similar distributions of truncated
paths - not just similar distance functions.

In the next section we consider a loose method of interpreting the local distances: the interpretation
of the local distance as a function of two, rather than three, agents, ﬁxing the vantage point at the
ﬁrst agent being compared. This allows us to describe the local distance as a premetric. We use this
fact to deﬁne the topology of the agent space in Subsection 6.3.

6.2 dx(x, y) as a Premetric

A premetric is a generalization of a metric which relaxes several properties, giving the very general
deﬁnition

Deﬁnition 6.2 (Premetric). A function D : X → R+ ∪ {0} is called a premetric if [19, p. 23]

such a premetric is called separating if it also satisﬁes

D(x, x) = 0.

D(x, y) = 0 ⇐⇒ x = y.

(6.2.1)

(6.2.2)

Many important functions satisfy this deﬁnition, including the Kullback-Liebler Divergence.

Remark 6.2 (The Local Distance is a Premetric). Notice that the function

D(a, b) = da(a, b)

(6.2.3)

is a separating premetric.

Important for the practical use of the local distances, this premetric (along with the other
structures of the agent space) is able to describe the diﬀerences between agents and between the
distributions of paths which they produce without actually sampling those distributions; da(b, c)
compares the distributions Φb and Φc but only requires information about the distribution Φa the
functions b and c. This is valuable because in Reinforcement Learning it is typically simple to
calculate the actions which an agent would take from that agent’s parameters, but information about
the distribution usually needs to be sampled - an expensive operation. This is especially valuable if
many nearby agents need to be compared (e.g. because the agents being considered are based on a

25

single locus agent). Operations which involve comparing a pair of agents using the standard of a
third like this are common in Reinforcement Learning. For example, the Qa Deﬁnition 2.2 function
is often used to judge the quality of the actions of other agents b (cid:54)= a.

In the next section we describe a topology on the agent space which we will take as canonical
(i.e. as the topology of the agent space). There are two basic ways to understand the topology:
it may be understood as the topology of the premetric space given by the premetric on the agent
space described above, or it may be understood as the topology given by the convergence relation of
Deﬁnition 6.1. These are identical. In fact, Deﬁnition 6.1 can be deﬁned using only the premetric
description of the local distance.

6.3 The Topology of the Agent Space

Let us start by providing two equivalent deﬁnitions of the topology of the agent space: one deﬁnition
of its open sets, and another deﬁnition of its closed sets.

Deﬁnition 6.3 (The Topology of (A, d): Open Sets). We say that a set U ∈ A is open if and only if
about every point x ∈ U , U admits an open disk of positive radius:

∀ x ∈ U ∃ r > 0 ({y | dx(x, y) < r} ⊂ U ).

(6.3.1)

Deﬁnition 6.4 (The Topology of (A, d): Closed Sets). We say that a set X ∈ A is closed if and
only if it contains its limit points; iﬀ for every convergent sequence xn with xn ∈ A: X is closed iﬀ

∀ xn(∀ n ∈ N (xn ∈ X) =⇒ lim xn ∈ X).

(6.3.2)

These deﬁnitions suﬃce, in fact, to deﬁne the topology of any premetric (or metric). It may be
demonstrated that these deﬁnitions produce the same topology (for example, by remarking that open
sets in metric spaces may be characterized by the criterion (6.3.1)). It is important to note that this,
along with the premetric version of the agent space, represents a sort of lower-bound on the structure
which the local distances describe on the set of agents. In particular, the local distances may prove
useful beyond simple problems of limits. In [28], we employ the local distances for exploration in an
implementation of Novelty Search.

In the next section, we demonstrate that the topology of the agent space is compatible with many
of the most important aspects of Reinforcement Learning. In particular, we show that standard
formulations of reward are continuous in the agent space, and that the agent space itself is continuous
in the parameters of most agent approximators, demonstrating that the agent space is a valid
structure for the set of agents, and for Reinforcement Learning more generally.

7 Functions of the Agent

The topology of the agent space carries information about many important aspects of the decision
process and its interaction with agents, including the distributions of truncated paths. However, we
have not yet demonstrated any relationship between the agent space and the object of Reinforcement
Learning: the expected reward of the agent, J(a). In this section we demonstrate that an important
class of reward functions (summable reward functions, (1.1.13)) are continuous functions of the
agent in the topology of the agent space. We begin with a simple condition for the agent to be a
continuous function of the parameters of a function approximator. We then use the continuity of
ﬁnite distributions of paths established in Item 1 of Theorem 6.1 in the agent space to prove that the
expectation of reward is a continuous function of the agent.

26

7.1 Parameterized Agents

Let f be a function approximator parameterized by a set of real numbers θ, taking truncated prime
paths into a set of actions. Then,

f : Rn × Φ(cid:48) → A.
If we delay the selection of the truncated path, we may understand f as a function from Rn into the
set of agents:

(7.1.1)

f : Rn → A Φ(cid:48)

⇐⇒ f : Rn → A.

(7.1.2)

Notice that we have returned to the pre-quotient notion of an agent - the set of functions from the
set of truncated prime paths to the set of actions before the equivalence relation of Deﬁnition 5.6 is
applied. To better distinguish these functions, let us denote the pre-quotient set F . The matter of
demonstrating that a particular function approximator is a continuous function from its parameters
to the agent space may be divided into two parts:
it must be demonstrated that the function
approximator is a continuous function from the set of parameters to the set of functions, and it must
be demonstrated that the quotient operation itself is a continuous function from the set of functions
to the set of agents. We begin by demonstrating the continuity of the quotient operation.

Let us assume the L∞ metric on the set of functions and denote the map taking a function f to
an agent a by Q : (F, L∞) → (A, d). Then, the quotient operation which takes the set of agents to
the space of agents is continuous if and only if for every convergent sequence xn in (F, L∞), Q(xn)
converges.

Theorem 7.1 (The Agent Identity Quotient Operation is Continuous). The quotient operation
deﬁned by Deﬁnition 5.6 is a continuous function from the F to A.

Proof. Let xn be a L∞-convergent sequence of functions converging to x

xn : Φ(cid:48) → A,
∀ ε > 0 ∃ m ∈ N ∀ n > m (d∞(xn, x) < ε).

Then, we must show that

∀ ε > 0 ∃ m ∈ N ∀ n > m (dQ(x)(Q(xn), Q(x)) < ε).

Consider the deﬁnition of dx:

(cid:88)

dx =

ω(t)dΦ(cid:48)x

t

(xn, x)

dΦ(cid:48)x

t

Clearly, we have

t∈N
(xn, x) = E

φ(cid:48)

t∼Φ(cid:48)x
t

[dφ(cid:48)(xn, x)] .

dΦ(cid:48)x

t

(xn, x) ≤ sup
φ(cid:48)∈Φ(cid:48)

dφ(cid:48)(xn, x)

Thus, dx(xn, x) ≤ (cid:80)

= d∞(xn, x).
t∈N ω(t)d∞(xn, x). Recall that Ω = (cid:80)

t∈N ω(t) < ∞. Thus,

exists because an integer m which satisﬁes

∀ ε > 0 ∃ m ∈ N ∀ n > m(dx(xn, x) ≤ ε)

∀

ε
Ω

> 0 ∃ m ∈ N ∀ n > m

(cid:16)

d∞(xn, x) <

(cid:17)

ε
Ω

exists, by assumption.

27

(7.1.3)

(7.1.4)

(7.1.5)

(7.1.6)

(7.1.7)

(7.1.8)

(7.1.9)

(7.1.10)

(7.1.11)

Q.E.D.

To ﬁnish the demonstration that a particular function approximator gives agents continuous
in its parameters, then, it remains only to show that the function approximator is L∞-continuous
(uniformly continuous) in the parameters of the approximator. One class of function approximators
which satisﬁes this is feedforward neural networks, such as those discussed in [29].10

7.2 Reward and the Agent Space

In order for the agent space to be useful for the problem of Reinforcement Learning, it must be
related to the object of Reinforcement Learning: reward.

We noted in Deﬁnition 1.8 that reward can frequently be described by a sum,

R : Φ → R

R(φ) =

(cid:88)

t∈N

r(φt(s), φt(a)).

(1.1.12)

(1.1.13)

We also noted that this sum is often weighted by a discount function ω(t). Discount functions are
employed because they oﬀer general conditions under which the reward of a path (and thus its
expectation) is bounded: so long as Ω is ﬁnite and the immediate reward r is bounded, so too is the
sum (1.1.13).

This formulation of reward has several valuable properties which can be extracted: the reward

function can be extended from Φ to include truncated paths:

R : Φ ∪

(cid:91)

Φt → R

t∈N
(cid:88)

R(φt) =

i<t

r(φt(s), φt(a)).

Clearly, for any path φ for which R(φ) exists we have

lim
t→∞

R(φt) = R(φ).

(7.2.1)

(7.2.2)

(7.2.3)

If the immediate reward r is bounded and the sum is weighted by a discount function ω with ﬁnite
sum Ω then R is bounded and we have the stronger condition

∀ ε > 0 ∃ t ∈ N ∀ φ ∈ Φ (|R(φ) − R(φt)| < ε) .

(7.2.4)

That is, such a summable discount function converges uniformly to its value as t → ∞.

Theorem 7.2 (Functions Continuous in the Agent Space). Let R be a bounded real function of the
set of paths and truncated paths, and let J be the expectation of R on the distribution of paths Φa,

J(a) = E

[R(φ)] ,

φ∼Φa
(cid:91)

R : Φ ∪

Φt → R,

t∈N
|R(φ) − R(ϕ)| = R,

sup
φ,ϕ∈Φ∪(cid:83)

t∈N Φt

(7.2.5)

(7.2.6)

(7.2.7)

and let R satisfy (7.2.4)

∀ ε > 0 ∃ t ∈ N ∀ φ ∈ Φ (t(cid:48) > t =⇒ |R(φ) − R(φt(cid:48))| < ε) .

(7.2.4)

Then, J is a continuous function with respect to the agent space (A, d).

10Speciﬁcally, neural networks with continuous, bounded activation functions are uniformly continuous in their

parameters.

28

Proof. Let us demonstrate that J is a continuous function of a by showing that for any convergent
sequence xn converging to a,

lim
n→∞

J(xn) = J(a).

Thus, our goal is to demonstrate that

∀ ε ∃ m ∈ N (n > m =⇒ |J(a) − J(xn)| < ε) .

By assumption of (7.2.4),

∃ t ∈ N ∀ φ ∈ Φ

(cid:16)

t(cid:48) > t =⇒ |R(φ) − R(φt)| <

(cid:17)

.

ε
3

Consider the expectation of the reward of the truncated paths of a,

By (7.2.10), for an appropriate value of t we have

E
φ∼Φa

R(φt).

|R(φ) − R(φt)| <

ε
3

.

Thus, we have

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
φ∼Φa

R(φ) − E

φ∼Φa

(cid:12)
(cid:12)
R(φt)
(cid:12)
(cid:12)

J(a) − E

(cid:12)
(cid:12)
R(φt)
(cid:12)
(cid:12)
|R(φ) − R(φt)|

φ∼Φa

(cid:12)
(cid:12)
(cid:12)
(cid:12)
= E

φ∼Φa

< E

φ∼Φa

ε
3

=

ε
3

(7.2.8)

(7.2.9)

(7.2.10)

(7.2.11)

(7.2.12)

(7.2.13)

(7.2.14)

(7.2.15)

Now, let us consider Item 1 of Theorem 6.1, which demonstrates that for any t ∈ N and any sequence
of agents xn converging to a, TVD(Φa

t ) goes to 0 as n → ∞, so we have

t , Φxn
(cid:18)

∀ t ∈ N ∃ m ∈ N

n > m =⇒ TVD(Φa

t , Φxn

t ) <

(cid:19)

.

ε
3R

Then we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
φ∼Φxn

R(φt) − E

φ∼Φa

(cid:12)
(cid:12)
R(φt)
(cid:12)
(cid:12)

< R

ε
3R

=

ε
3

.

Thus, for suﬃciently large t and m, we have for n > m

J(a) − J(xn) +

(cid:18)

J(a) − E

|J(a) − J(xn)|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ε
3

φ∼Φa
ε
3

J(a) − E

ε
3

φ∼Φa

+

+

= ε.

=

=

<

<

(cid:12)
(cid:12)
R(φt)
(cid:12)
(cid:12)

(cid:18)

E
φ∼Φa
(cid:19)

R(φt)

−

R(φt) − E

φ∼Φa

(cid:19)

(cid:18)

R(φt)

+

(cid:18)

J(xn) − E

φ∼Φxn

R(φt)

+

E
φ∼Φxn
(cid:18)

(cid:19)

R(φt) − E

φ∼Φxn

R(φt)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

R(φt) − E

φ∼Φxn

R(φt)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
φ∼Φa

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

J(xn) − E

φ∼Φxn

(cid:12)
(cid:12)
R(φt)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E
φ∼Φa

R(φt) − E

φ∼Φxn

R(φt)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

29

(7.2.16)

(7.2.17)

(7.2.18)

(7.2.19)

(7.2.20)

(7.2.21)

(7.2.22)

A.T.C.R.

8 Conclusion

In this work we consider the problem of exploration in Reinforcement Learning. We ﬁnd that
exploration is understood and well-deﬁned in the dynamic paradigm of Richard Bellman [5], but
that it is not well-deﬁned for other optimization paradigms used in Reinforcement Learning. In
dynamic Reinforcement Learning, exploration serves to collect the information necessary for dynamic
programming, as described in Subsection 2.4. In non-dynamic Reinforcement Learning - what we call
na¨ıve Reinforcement Learning - the situation is more complex. We ﬁnd that dynamic methods of
exploration are eﬀective in na¨ıve methods, but that the explanation of their eﬀect oﬀered by dynamic
programming cannot explain their eﬃcacy in na¨ıve methods, which do not use the information
required by dynamic programming.

This leads us to several questions: Why are exploration methods designed to provide information
useless to na¨ıve Reinforcement Learning nonetheless eﬀective for na¨ıve methods? What should the
deﬁnition of exploration be for na¨ıve methods? To what extent does this more general kind of
exploration contribute to the eﬀectiveness of dynamic exploration in dynamic methods? To resolve
these questions, we consult the commonalities of several dynamic methods of exploration, ﬁnding
two: ﬁrst, their dynamic justiﬁcation, and second, their mechanism: considering diﬀerent agents
which are deemed likely to demonstrate diﬀerent distributions of paths.

Of these, only the mechanism might serve to explain dynamic exploration’s eﬃcacy in na¨ıve
methods, and we take this mechanism as the deﬁnition of na¨ıve exploration. This, deﬁnition, however,
leaves a gap: under it, totally random experimentation with agents is explorative. This may be
eﬀective in small problems, but it is unprincipled. We ﬁnd a principle in Novelty Search [7]: in
exploration one should consider agents which are novel relative to the agents which have already
been considered. To determine novelty, they use the distance between the behavior of an agent and
those considered in the past.

However, we ﬁnd their notion of novelty deﬁcient for the purpose of deﬁning na¨ıve exploration;
they require that function which determines the behavior of an agent be separately and manually
determined for each reinforcement learning problem. Fortunately, this view is not held universally in
the literature. We consider a cluster of behavior functions which we call primitive behavior [23–25].
Primitive behavior is powerful: because it is composed of the actions of an agent, it is possible
in some processes for primitive behavior to fully determine the distribution of paths, and thus to
determine every notion of behavior derived from (P, a).

Unfortunately, primitive behavior has several ﬂaws. First, only in certain ﬁnite processes may the
primitive behavior of an agent fully determine Φa. Second, primitive behavior can inappropriately
distinguish between agents (see Deﬁnition 5.6). Third, it necessarily retains the manual selection
requirement in decision processes with inﬁnitely many states. In Section 5, we describe a more general
notion of the distance between agents - one which does not require a behavior function. Instead, we
deﬁne a structure on the set of agents itself. We call the resulting structure an agent space.

In Section 6 and Section 7, we describe the topology of the agent space, demonstrating that
it carries information about many important aspects of Reinforcement Learning, including the
distribution of paths produced by an agent and standard formulations of the reward of an agent.
Using these facts, we demonstrate that, for many function approximators, reward is a continuous
function of the parameters of an agent.

In a future work [28], we use techniques described in Appendix C to join the agent space with
Novelty Search to perform Reinforcement Learning using a na¨ıve, scalable learning system similar to
Evolution Strategies [13]. We test this method in a variety of processes and ﬁnd that it performs
similarly to ES in problems which require little exploration, and is strictly superior to ES in problems
in which exploration is necessary.

30

References

1. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction Second Edi. isbn:
9780262039246. https : / / mitpress . mit . edu / books / reinforcement - learning - second -
edition (MIT Press, Cambridge, Massachusetts, 2020).

2. Gittins, J. & Jones, D. A Dynamic Allocation Index for the Discounted Multiarmed Bandit

Problem. Biometrika 66, 561–565. https://www.jstor.org/stable/2335176 (1979).

3. Vinyals, O. et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning.
Nature 575, 350–354. issn: 14764687. http://dx.doi.org/10.1038/s41586-019-1724-z
(2019).

4. OpenAI et al. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv: 1912.06680.

http://arxiv.org/abs/1912.06680 (2019).

5. Bellman, R. The Theory of Dynamic Programming in Summer Meeting of the American
Mathematical Society (Laramie, Wyoming, 1954). https://apps.dtic.mil/sti/citations/
AD0604386.

6. Bellman, R. Dynamic programming isbn: 069107951X (Princeton University Press, Princeton,

New Jersey, 1957).

7. Lehman, J. & Stanley, K. O. Abandoning Objectives: Evolution Through the Search for Novelty
Alone. Evolutionary Computation 19, 189–233. https://ieeexplore.ieee.org/document/
6793380 (2011).

8. Watkins, C. J. C. H. Learning From Delayed Rewards PhD thesis (King’s College, 1989).

https://www.researchgate.net/publication/33784417.

9. Bellman, R. E. & Dreyfus, S. E. Applied Dynamic Programming tech. rep. (RAND Corporation,

Santa Monica, California, 1962). https://www.rand.org/pubs/reports/R352.html.

10. Agrawal, S. & Goyal, N. Further Optimal Regret Bounds for Thompson Sampling in Proceedings
of the Sixteenth International Conference on Artiﬁcial Intelligence and Statistics (eds Carvalho,
C. M. & Ravikumar, P.) 31 (PMLR, Scottsdale, Arizona, 2013), 99–107. arXiv: 1209.3353.
http://proceedings.mlr.press/v31/agrawal13a.html.

11. Watkins, C. J. C. H. & Dayan, P. Technical Note Q,-Learning. Machine Learning 8, 279–292.

https://link.springer.com/article/10.1023/A:1022676722315 (1992).

12. Howard, R. A. Dynamic Programming and Markov Processes isbn: 978-0262080095. arXiv:

Lib.Cong.60-11030 (MIT Press and John Wiley & Sons, Inc., 1960).

13. Salimans, T., Ho, J., Chen, X., Sidor, S. & Sutskever, I. Evolution Strategies as a Scalable
Alternative to Reinforcement Learning, 1–13. arXiv: 1703.03864. https://openai.com/blog/
evolution-strategies/ (2017).

14. Thrun, S. B. in Handbook for Intelligent Control: Neural, Fuzzy and Adaptive Approaches (1992).
http://www.cs.cmu.edu/%7B~%7Dthrun/papers/thrun.exploration-overview.html.
15. Tang, H. et al. #Exploration: A Study of Count-Based Exploration for Deep Reinforcement
Learning in 31st Conference on Neural Information Processing Systems (NIPS 2017) (Nov.
2017). arXiv: 1611 . 04717. https : / / proceedings . neurips . cc / paper / 2017 / file /
3a20f62a0af1aa152670bab3c602feed-Paper.pdf.

16. Stadie, B. C., Levine, S. & Abbeel, P. Incentivizing Exploration In Reinforcement Learning
With Deep Predictive Models. arXiv: 1507.00814. https://ui.adsabs.harvard.edu/abs/
2015arXiv150700814S/abstract (July 2015).

17. Weng, L. Exploration Strategies in Deep Reinforcement Learning. lilianweng.github.io/lil-log.
https://lilianweng.github.io/lil- log/2020/06/07/exploration- strategies- in-
deep-reinforcement-learning.html (2020).

31

18. Lehman, J. & Stanley, K. O. Exploiting Open-Endedness to Solve Problems Through the Search
for Novelty in Proceedings of the Eleventh International Conference on Artiﬁcial Life XI (MIT
Press, Cambridge, Massachusetts, 2008). http://eplex.cs.ucf.edu/papers/lehman%7B%5C_
%7Dalife08.pdf.

19. Arkhangel’skiˇı, A. V. & Pontryagin, L. S. General Topology I 1st ed., 202. isbn: 978-3-642-61265-
7. https://link.springer.com/book/10.1007/978- 3- 642- 61265- 7%7B%5C#%7Dabout
(Springer-Verlag Berlin Heidelberg, 1990).

20. Lehman, J. & Stanley, K. O. in Genetic Programming Theory and Practice IX (eds Riolo,
R., Vladislavleva, E. & Moore, J. H.) 37–56 (Springer-Verlag, 2011). isbn: 978-1-4614-1769-9.
https://link.springer.com/chapter/10.1007/978-1-4614-1770-5%7B%5C_%7D3.

21. Gomez, F. J. Sustaining diversity using behavioral information distance in GECCO ’09: Pro-
ceedings of the 11th Annual conference on Genetic and evolutionary computation (Association
for Computing Machinery, Montr´eal, Qu´ebec, Canada, 2009), 113–120. isbn: 978-1-60558-325-9.
https://dl.acm.org/doi/abs/10.1145/1569901.1569918.

22. Conti, E. et al. Improving Exploration in Evolution Strategies for Deep Reinforcement Learning
via a Population of Novelty-Seeking Agents in Advances in Neural Information Processing
Systems 31 (2017). arXiv: 1712.06560. https://proceedings.neurips.cc/paper/2018/
hash/b1301141feffabac455e1f90a7de2054-Abstract.html.

23. Meyerson, E., Lehman, J. & Miikkulainen, R. Learning Behavior Characterizations for Novelty
Search in GECCO 2016 - Proceedings of the 2016 Genetic and Evolutionary Computation Con-
ference (Association for Computing Machinery, Inc, July 2016), 149–156. isbn: 9781450342063.
https://dl.acm.org/doi/10.1145/2908812.2908929.

24. Parker-Holder, J., Pacchiano, A., Choromanski, K. & Roberts, S. Eﬀective Diversity in Popu-
lation Based Reinforcement Learning. arXiv: 2002.00632. https://research.google/pubs/
pub49976/ (2020).

25. Stork, J., Zaeﬀerer, M., Bartz-Beielstein, T. & Eiben, A. E. Understanding the Behavior of
Reinforcement Learning Agents in International Conference on Bioinspired Methods and Their
Applications 2020 (eds Bogdan, F., Edmondo, M. & Massimiliano, V.) (Springer, Brussels,
Belgium, 2020), 148–160. isbn: 978-3-030-63709-5. https://link.springer.com/chapter/10.
1007/978-3-030-63710-1%7B%5C_%7D12.

26. Pacchiano, A. et al. Learning to score behaviors for guided policy optimization. 37th International
Conference on Machine Learning, ICML 2020 (eds Daum´e, H. I. & Singh, A.) 7445–7454. arXiv:
1906.04349. https://proceedings.mlr.press/v119/pacchiano20a.html (July 2020).
27. Rudin, W. Principles of Mathematical Analysis isbn: 007054235X. https://www.maa.org/

press/maa-reviews/principles-of-mathematical-analysis (McGraw-Hill, 1976).

28. Allen, M. W., Raisbeck, J. C. & Lee, H. Distributed Policy Reward & Strategy Optimization.

Unpublished. arXiv: TBD. (2021).

29. Hornik, K., Stinchcombe, M. & White, H. Multilayer feedforward networks are universal
approximators. Neural Networks 2, 359–366. issn: 0893-6080. https://www.sciencedirect.
com/science/article/abs/pii/0893608089900208 (1989).

30. Edman, M. & Dhir, N. Boltzmann Exploration Expectation–Maximisation. arXiv. issn: 2331-

8422. arXiv: 1912.08869. https://arxiv.org/abs/1912.08869 (2019).

31. Williams, R. J. & Peng, J. Function Optimization Using Connectionist Reinforcement Learning
Algorithms. Connection Science 3, 241–268. https://www.tandfonline.com/doi/pdf/10.
1080/09540099108946587 (1991).

32

32. Bellemare, M. G. et al. Unifying Count-Based Exploration and Intrinsic Motivation. Pro-
ceedings of the 30th International Conference on Neural Information Processing Systems,
1479–1487. arXiv: 1606 . 01868. https : / / proceedings . neurips . cc / paper / 2016 / file /
afda332245e2af431fb7b672a68b659d-Paper.pdf (June 2016).

33. Klenke, A. Probability Theory isbn: 9781447153603. https://link.springer.com/book/10.

1007%7B%5C%%7D2F978-1-84800-048-3 (Springer-Verlag, 2014).

Appendices

A Exploration Methods

This appendix contains a brief review of the exploration methods mentioned in Section 3.

A.1 Undirected Exploration

A.1.1

ε-Greedy

One of the simplest undirected exploration algorithms is the ε-greedy algorithm described in Deﬁni-
tion 1.13.

In Deﬁnition 1.13, we assumed that aθ was a real function approximator, but ε-Greedy can be
applied to a broader range of intermediates. All that is necessary is that the underlying function
approximator aθ indicate a single action - referred to as the “greedy” action, a reference to Rein-
forcement Learning algorithms which explicitly predict the value of actions (see Deﬁnition 2.2). In
such algorithms, the “greedy action” is the one which is predicted to have the highest value. We call
functions with this property deterministic, and the greedy action their deterministic action.

Deﬁnition A.1 (ε-Greedy). An ε-greedy output function renders a deterministic agent stochastic
by changing its action with probability ε ∈ [0, 1] to one drawn from a uniform distribution over the
set of actions, U , and retaining the deterministic action with probability 1 − ε:

(cid:40)

Oε(aθ(s)) =

Ogreedy(aθ(s)) with probability 1 − ε
U (A)

with probability

ε

(A.1.1)

where Ogreedy is the output function which takes aθ’s greedy action.11

Remark A.1 (ε = 0). Notice that the case ε = 0 collapses to the deterministic agent aθ, and the case
ε = 1 is the uniformly random agent.

The major beneﬁt of ε-greedy sampling is that in a ﬁnite decision process, every path has a
non-zero probability (provided ε > 0). Unfortunately, that can only be accomplished by assigning a
diminutive probability to each of those paths. As a path deviates further from the paths generated
by the agent aθ, its probability decreases exponentially with each action which deviates from aθ.

That restriction is not necessarily bad for optimization; by visiting paths which require few
changes to the actions of aθ, the newly discovered states are nearly accessible to aθ, which may
make them more poignant to learning algorithms, which typically change locus agents only by small
amounts in each epoch.

While ε-Greedy can be applied to processes with ﬁnite or inﬁnite sets of actions, the next method,

Thompson Sampling, can only be deﬁned for processes with ﬁnite sets of actions.

11For notational simplicity we assume that the function underlying a is parameterized (aθ). This is not necessary

to apply the methods of this section.

33

A.1.2 Thompson Sampling and Related Methods

Other major undirected exploration methods operate using a similar mechanism to ε-Greedy, to
very diﬀerent eﬀect. Just like ε-Greedy sampling, Thompson Sampling acts as O, taking the range
of a function aθ to the set of probability distributions of actions. Whereas ε-Greedy produces a
distribution which varies only in the agent’s deterministic12 action, Thompson Sampling produces
a distribution which varies with the agent’s output for each action; unlike ε-Greedy, Thompson
Sampling is continuous in the output of aθ.

Deﬁnition A.2 (Thompson Sampling). A Thompson Sampling output function produces a distri-
bution of actions from the output of a real function approximator aθ. Thompson Sampling requires
that aθ(s) be a real vector of dimension |A|, whose elements are nonnegative and have sum 1;
aθ(s) : S → {x ∈ R|A| | xi ≥ 0, (cid:80) xi = 1}. The Thompson Output Function produces the distribution
of actions

OThompson(aθ(s)) =






with probability aθ(s)1,
with probability aθ(s)2,
with probability aθ(s)3,

a1
a2
a3
...
a|A| with probability aθ(s)|A|.

(A.1.2)

Many function approximators do not naturally produce values which fall in the set of acceptable
inputs to OThompson. Several methods may be employed to rectify these approximators with Thompson
Sampling. One common method is known as Boltzmann Exploration (or as a softmax layer) [1, p. 37]:

Deﬁnition A.3 (Boltzmann Exploration). A Boltzmann Exploration output function produces
a distribution of actions from the output of a real function approximator aθ.
It is most easily
understood as a “pre-processing” for Thompson Sampling function. Let ρ be a real parameter (ρ is
sometimes called temperature [30]). Then,

softmax(x) : R|A| → {x ∈ R|A||xi ≥ 0,

(cid:88)

xi = 1},

softmax(aθ(s))i =

eaθ(s)iρ
(cid:80) eaθ(s)kρ

.

(A.1.3)

(A.1.4)

Which can be composed with the regular Thompson output function:

OBoltzmann(aθ(s)) = OThompson(softmax(aθ(s))).

(A.1.5)

Boltzmann Exploration is among the most common methods of creating functions which are
compatible with Thompson Sampling because of its beneﬁcial analytical properties: it is continuous,
has a simple derivative (especially important for back-propagation), and guarantees that every action
has a non-zero probability.

A diﬀerent kind of augmentation of Thompson Sampling and other stochastic output functions is
Entropy Maximization [31]. In contrast with Boltzmann Exploration, entropy maximization modiﬁes
the learning process itself through the immediate reward function.

Deﬁnition A.4 (Entropy Maximization). Entropy Maximization is a method used with stochastic
agents which adds the conditional entropy h(at|a, st) of the action with respect to the distribution
a(s) to the immediate reward,

rentropy(φt) = r(φt) + ρh(at|a, st),

(A.1.6)

where ρ is a positive real parameter of the optimizer [31].

12Diﬀerent learning algorithms approximate diﬀerent objects; in Q-Learning (Subsection 2.4), aθ approximates the

action-value of a state-action pair (s, a); in policy gradients, its meaning is dependent on the output function O.

34

These entropy bonuses cause the learner to consider both the reward which an agent attains and
its propensity to select a diversity of actions. The learner is thus encouraged to consider agents which
express greater “uncertainty” in their actions, slowing the convergence of the locus agent.

With respect to dynamic exploration, there is little diﬀerence between ε-Greedy and Thompson
Sampling. Both algorithms explore the process by selecting agents which allow them to experience
unexplored aspects of the process. From the na¨ıve perspective, this similarity is overshadowed by a
diﬀerence in their analytical properties: in ﬁnite problems, Thompson Sampling agents act from a
continuous set of actions, whereas ε-Greedy agents use a [modiﬁed] ﬁnite set of actions.

The exploration methods discussed in this section are fairly homogeneous, precisely because
they are undirected; the only way to explore without direction is to inject stochasticity into the
optimization process. Conversely, the methods of the next section are considerably more diverse;
there are many ways to direct an explorative process.

A.2 Directed Exploration

The variety of directed exploration methods make the genre diﬃcult to summarize. Perhaps the
simplest description of directed methods as the complement of undirected methods. Undirected
exploration methods use exclusively stochastic means to explore; they do not incorporate any
information speciﬁc to the process. Directed exploration methods thus include any exploration
method which does incorporate such information [14]. This section describes two major families
of directed exploration methods: count-based and prediction-based through a pair of representative
methods [17]. We begin with count-based exploration, exempliﬁed by #Exploration [15].

Count-based methods [15, 32] count the number of times that each state (or state-action pair, see
Subsection 2.4) has been observed in the course of learning, and use that count to inform the course
of learning, to encourage visitation of scarcely visited states. Count-based algorithms have appealing
guarantees in ﬁnite processes [32], but lose those guarantees in inﬁnite settings. Despite this, count-
based exploration continues to inspire exploration techniques in the inﬁnite setting. #Exploration is
a recent method which discretizes inﬁnite problems, imitating traditional count-based methods.

Deﬁnition A.5 (#Exploration). #Exploration [15] is an algorithm which augments the immediate
reward function with an exploration bonus in the same manner as the entropy bonus of Deﬁnition A.4.
However, instead of encouraging the learner to pursue agents which attempt new actions or visit rarely
visited states, #Exploration uses hash codes. The hash codes are generated by a hashing function
H(s) which discretizes an unmanageable (i.e. large or inﬁnite) set of states into a manageable ﬁnite
set of hash codes. Using these hash codes as a proxy for states, #Exploration assigns its exploration
bonus in much the same way as a traditional count-based method:

r#(a, s) = r(a, s) +

ρ
(cid:112)n(H(s))

.

(A.2.1)

Here, r# is the combination of the immediate reward function and the exploration bonus for that
state, n is the state-count function, a tally of the number of times that a state with the same hash
code as s, H(s), has been visited, and ρ is a positive real number. #Exploration pursues its goal as
a count-based method by assigning greater exploration bonuses to states which have been visited
fewer times.

The next class of exploration methods in this section is called prediction-based exploration.
Whereas count-based methods estimate the new information that an action will collect with a
measurement of the learner’s experience of each state, prediction-based methods attempt to estimate
the quality, rather than the mere quantity of the collected information. To do this, they employ a
separate modeling method which predicts the next state of the process. The better that prediction
is, the higher the quality of the information which the learner has about that part of the process.

35

Deﬁnition A.6. In Incentivizing Exploration[16], Stadie et al. estimate the quality of information
which the executor has gathered by using that information to train a dynamics model M to estimate
the next state st+1 from (st, at)13. They reason that if M accurately estimates the next state
(as measured by the distance between M(st, at) and st+1), then the executor has gathered better
information about that state-action pair. Thus, they assign exploration bonuses so as to encourage
consideration of agents which visit state-action pairs for which the distance ||M(st, at) − st+1|| is
large:

rStadie(st, at) = r(st, at) + ρ

||M(st, at) − st+1||
tC

,

(A.2.2)

where β is a constant, and C is decay constant (i.e. an increasing function of the learning epoch).

B Behavior Functions in the Literature

Many of the behavior functions which have been proposed have been inﬂuenced by the behavior
functions of Lehman and Stanley’s initial work, and by their advice on the subject in Abandoning
Objectives: Evolution Through the Search for Novelty Alone:

Although generally applicable, novelty search is best suited to domains with deceptive
ﬁtness landscapes, intuitive behavioral characterizations, and domain constraints on
- Lehman and Stanley [7, p. 200]
possible expressible behaviors.

This passage provides important insight for those who wish to apply Novelty Search to new domains
in the tradition of Lehman and Stanley, but their suggestions also make it diﬃcult to analyze
Novelty Search independent of the choice of behavior function. This is especially problematic for the
open-ended use of Novelty Search; without a general notion of behavior, there are few options for the
comparison of behavior functions to one another or their absolute evaluation. In a given decision
process, one can compare the outcomes of Novelty Search processes with diﬀerent behavior functions
by considering the diversity of behaviors which they produce, but this diversity must be measured by
one of these or a diﬀerent notion of behavior. One could consider each behavior function’s propensity
to ﬁnd high-quality agents, but this is a return to the just-abandoned objectives.

Lehman and Stanley are forced to compare their behavior functions in the maze environment
along these lines. In a discussion of the degrees of “conﬂation” (assignment of the same behavior to
diﬀerent agents) present in their behavior functions: “[I]f some dimensions of behavior are completely
orthogonal to the objective, it is likely better to conﬂate all such orthogonal dimensions of behavior
together rather than explore them.” To address these issues and make it possible to apply Novelty
Search to a wider range of decision processes, several authors have considered general behavior
functions [21–25].

The rest of this appendix provides a brief survey of some behavior functions present in the
literature, beginning with the speciﬁc functions of Abandoning Objectives [7], and proceeding to
general behavior functions, including those of [21, 22].

B.1 The Behavior Functions of Abandoning Objectives

The main decision processes in Abandoning Objectives are two-dimensional mazes. They consider
several behavior functions in this environment, all of which are based on the position of the agent.
The primary behavior function they consider is what we call the ﬁnal position behavior function:

Deﬁnition B.1 (Final Position Behavior).

13This algorithm uses “state encodings”, similar to the hash codes of #Exploration, rather than states.

B(a) = ptmax

(B.1.1)

36

Where ptmax is the position at the ﬁnal time in a sampled truncated trajectory φtmax.

They consider another positional behavior function: the position of the agent over time.

Deﬁnition B.2 (Position Over Time Behavior).

B(a) = (pt1 , pt2 , ..., ptN −1, ptN )

(B.1.2)

For 0 ≤ ti < ti+1 ≤ tmax

As noted in footnote 7, while these are functions into R2 in the case of ﬁnal position behavior,
and R2N in the case of position over time behavior, neither R2 nor R2N are treated as metric spaces.
Instead, both of these are equipped with a symmetric [19, p. 23]: the square of the usual Euclidean
distance.

These examples reveal the intuitions about behavior which Lehman and Stanley relied upon to
implement Novelty Search. First, rather than reﬂecting the actions of an agent alone, both of these
behavior functions reﬂect the results of the agent’s interaction with the process - in fact, they reﬂect
the position of the agent, a function of the state. Second, these behavior functions are distilled,
considering only one or a few points of time ti.

In the other environment, Biped Locomotion, Lehman and Stanley take a diﬀerent approach to
selecting the times ti, opting to collect spatial information once per simulated second. Explaining
that diﬀerence, they write: “Unlike in the maze domain, temporal sampling is necessary because
the temporal pattern is fundamental to walking.” This is a strange argument, since reinforcement
learning problems are deﬁned by their temporality (see Deﬁnition 1.1).

B.2 General Behavior Functions

Since the publication of [18], Lehman and Stanley’s ﬁrst paper on Novelty Search, many authors
have sought general notions of behavior [21–25]. This section analyzes several of these behavior
functions. Let us begin with a simple notion of distance on the set of agents which is deﬁned with
for any method using a parameterized agent:

Example B.1 (The distance of θ). Consider two agents, a and b, represented by function approximators
of the same form. Assume that they are parameterized by an ordered list of real numbers, and let
their parameters be θa and θb. Then,

is a behavior function and

B(aθa ) = θa

d(B(a), B(b)) = ||θa − θb||

(B.2.1)

(B.2.2)

is a metric on this set of behaviors.

Although it is a metric (though it does not satisfy the indiscernibility of identicals under the
quotient operation of Deﬁnition 5.6), this distance is unsatisfactory in several ways. For example, it
can assign an agent a non-zero distance from itself if the agent can be parameterized by two diﬀerent
sets of parameters (see Subsection 5.6 and [24]).

An early work of Gomez et al.

[21] introduced a behavior function which maps agents to a
concatenation of truncated trajectories. They then use the normalized compression distance (NCD)
as a metric on this set of ﬁnite sequences.

Deﬁnition B.3 (Gomez et al. Behavior). Gomez et al. [21] deﬁne the behavior of an agent as a
concatenation of a number of observed truncated paths,

B(a) = φn1

a,1|φn2

a,2|...

(B.2.3)

37

As the distance on this set of behaviors B(A), Gomez uses the normalized compression distance

NCD(B(a), B(b)), which is an approximation of the mutual information of a pair of strings:

NCD(a, b) =

C(B(a)|B(b)) − min{C(B(a)), C(B(b))}
max{C(B(a)), C(B(b))}

.

(B.2.4)

Where C is the length of the compressed sequence.

C Agent Spaces In Practice

While the agent space is in general not a metric space (see Section 5, Subsection 5.7, and Subsec-
tion C.1), this does not proscribe its use in e.g. Novelty Search, which has long used metric-adjacent
spaces (see footnote 7). In an upcoming work [28], we describe an approach to the Reinforcement
Learning problem based on an extension of Evolution Strategies [13] which combines na¨ıve reward
and Novelty Search [7] of the agent space, selecting the locus agent as the perspective for comparisons
during each epoch, to solve a variety of reinforcement learning problems (see Subsection C.3).

Novelty Search and the Agent Space to be na¨ıve artifacts, but we cannot a priori restrict
them to na¨ıve learning methods. For example, [25] uses several versions of primitive behavior (see
Subsection 4.2) to explain “reward behavior correlation[s]”, showing that agents which are similar
under certain primitive behavior functions perform similarly. In Section 7 we demonstrate that the
Agent Space completes this line of inquiry by demonstrating analytically that reward is a continuous
function of the agent in the agent space. The reasoning of the Agent Space in Deﬁnition 5.6 also
provides a clean explanation for the observation of [25] that certain states may be totally unimportant
to performance.

C.1 When da is Equivalent to db

While local distances do not generally produce homeomorphic topologies, it is worthwhile to note
that many basic problems in the literature do have agents which produce homeomorphic topologies,
especially in the Markov and strictly Markov cases. Let us begin by considering the equivalence of
the measures underlying local distances:

Deﬁnition C.1 (Equivalence of Measures). A pair of measures µ, ν each on a measurable space
(M, Σ) are said to be equivalent iﬀ [33, p. 157]

∀ X ∈ Σ(µ(X) = 0 ⇐⇒ ν(X) = 0)

(C.1.1)

When the measures underlying da and db, Φa and Φb, are equivalent, they produce equivalent
topologies. In general, this is rare. In most decision processes, some paths can only be visited by a
subset of agents. However, there are certain circumstances where these distances are guaranteed to
be equivalent.

Clearly, if two agents diﬀer with non-zero probability, then the distributions of truncated paths
which they produce must also diﬀer. However, this does not apply when only Markov agents are
considered (see Deﬁnition 1.10). In this case, if the distributions of states, rather than truncated
paths, are equivalent, then the distances produce equivalent topologies.

Remark C.1 (Notation for the Probability of a State). The next few results require a simple notation
for the probability of a state occurring in a distribution of paths. This is complicated by the fact that
in any path there are an inﬁnity of states, so that the sum of the probabilities of a state occurring at
each time t ∈ N might be inﬁnite. To resolve this, we weight the probability of a state at t by ω(t)
and then normalize these probabilities with Ω. Let

P(s | a) =

(cid:80)

t∈N P(φt(s) = s)ω(t)
Ω

.

38

(C.1.2)

In general, this gives

Theorem C.1 (A Condition for the Equivalence of da and db). In a Markov decision process with a
ﬁnite set of states, the local distances da and db are equivalent whenever the distributions of states
in Φa and Φb are equivalent.

This theorem has several important manifestations. Let us consider the case where the local

distances of all agents possible in the process are equivalent:

Lemma C.1 (Conditions for the Equivalence of all Local Distances). The most general condition
for the equivalence of all local distances is

∀ a ∈ A ∀ s ∈ S(P(s | a) > 0).

(C.1.3)

There are two common conditions which are more speciﬁc but easier to verify, which may help with
the application of this result. First, if every transition probability is greater than zero, then certainly
the probability of each state in the distribution of paths is greater than 0:

∀ φt ∈

(cid:91)

t∈N

Φt(P(s | σ(φt)) > 0).

(C.1.4)

Even more speciﬁcally, but also more easy to test, if the probability of each state at the beginning
of the process is non-zero, then the probability of each state in the distribution of paths is greater
than 0:

∀ s ∈ S(P(s | σ0)).

(C.1.5)

Of course, this result is of little importance if these local distances cease to be useful for the
analysis of the underlying decision process. By construction, these equivalent local distances are
relevant only to Markov processes. Importantly, these local distances cannot detect diﬀerences in
distributions of paths which are not caused by diﬀerences in distributions of states, or more speciﬁcally
of state-action pairs. Thus, these local distances cannot guarantee the continuity all of the functions
considered in Section 7, but they do apply to the cumulative reward functions described by (1.1.13).

C.2 Deterministic Agents

The theorems of Section 6 rely on stochastic agents to justify the topology of the Agent Space. This
reliance stems from the fact that we are concerned in that section primarily with distributions of
paths. Because paths consist of sequences of states and actions, distributions of paths can only
approach one another (with respect to total variation) if, in response to a single truncated prime
path φ(cid:48)

t, a distribution of actions is taken.

However, this does not mean that the agent space is useless when deterministic agents are
considered. For example, the distributions of states mentioned in Subsection C.1 may be continuous
even in an agent space composed entirely of deterministic agents, via the stochasticity of the state-
transition function. Thus, if the set of actions is connected and for every truncated prime path φ(cid:48)
t
the state-transition distribution is a continuous function of the ﬁnal action at, then the distributions
of states are continuous in the agent space. Then, an immediate reward function which considers
only the state would also be continuous in the agent space.

C.3 Sketch of the Novelty Search Methods of [28]

In an upcoming work, we use the Agent Space in conjunction with Novelty Search to develop a
distributed optimization algorithm for Reinforcement Learning problems. Our method evaluates
candidate agents with a path collected by the locus agent at each training epoch, resulting in a

39

non-stationary objective that encourages the agent to behave in ways that it has not yet behaved, on
states that it can currently encounter. The following pseudo-code is a summary of this method.

Algorithm 1 Strategy & Reward Optimization Algorithm

Set batch At := ∅
for desired batch size do

1: for each epoch do
2:
3:
4:
5:

Set ε ∼ N (0, I)
Gather a path φ and cumulative reward R(φ) with at + ε
Set N (at + ε) ← min
0<i≤t

[dat(at + ε, at−i)] for prior epochs t − i

Append (R(φ), N (at + ε)) to At

Compute ∇atG(at) := ∇atR(at) + ∇atN (at) via Finite Diﬀerences with At.
Update at by following ∇atG(at)

6:

7:

8:
9:

We approximate dat(at + ε, at−i) by evaluating candidate agents on a set of states that we gather
every epoch. To do this, we follow the agent at in the decision process until K total states have
been encountered, then store them in a set denoted ζ. The distance dat(at + ε, at−n) can then be
approximated by evaluating the responses of at + ε and at−i on only ζ, rather than by integration on
Φat.

40

