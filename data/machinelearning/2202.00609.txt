2
2
0
2

b
e
F
1

]
L
C
.
s
c
[

1
v
9
0
6
0
0
.
2
0
2
2
:
v
i
X
r
a

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Semantic of Cloud Computing services for Time
Series workﬂow

Manuel Parra, Francisco J. Bald ´an, Ghislain Atemezing and Jos ´e M. Ben´ıtez

Abstract—
Time series (TS) are present in many ﬁelds of knowledge, research, and engineering. The processing and analysis of TS are essential
in order to extract knowledge from the data and to tackle forecasting or predictive maintenance tasks among others The modeling of
TS is a challenging task, requiring high statistical expertise as well as outstanding knowledge about the application of Data Mining
(DM) and Machine Learning (ML) methods. The overall work with TS is not limited to the linear application of several techniques, but is
composed of an open workﬂow of methods and tests. These workﬂow, developed mainly on programming languages, are complicated
to execute and run effectively on different systems, including Cloud Computing (CC) environments. The adoption of CC can facilitate
the integration and portability of services allowing to adopt solutions towards services Internet Technologies (IT) industrialization. The
deﬁnition and description of workﬂow services for TS open up a new set of possibilities regarding the reduction of complexity in the
deployment of this type of issues in CC environments. In this sense, we have designed an effective proposal based on semantic
modeling (or vocabulary) that provides the full description of workﬂow for Time Series modeling as a CC service. Our proposal includes
a broad spectrum of the most extended operations, accommodating any workﬂow applied to classiﬁcation, regression, or clustering
problems for Time Series, as well as including evaluation measures, information, tests, or machine learning algorithms among others.

Index Terms—Time Series, Data Mining, workﬂow, Cloud Computing, Services Description, Service Industrialization, Linked Data,
Semantic Web Services

(cid:70)

1 INTRODUCTION

F ORECASTING of weather conditions, the estimation of

the value of stock market shares, or the detection of
anomalies on industrial processes among others, are part
of the set ﬁelds where the TS data analysis plays a basic role
to tackle knowledge extraction.

Time series is a sequence or sequences of data spaced out
in time; events, activities, or devices continuously generate
information that is temporarily logged and stored for real-
time or post-processed study. The work with TS is one of
the fastest growing at the moment, due to the proliferation
of the so-called IoT [1], for instance. Increasingly in interest,
the use of mobile devices, autonomous vehicles, modern
agriculture, or intelligent machines, will produce a huge
amount of information in the coming years and a signiﬁcant
percentage of this information will be in the form of TS data
[2].

At the present time, when CC has practically been inte-
grated in a totally transparent way in our relationship with
Information Technologies (IT) and Internet, the activities
related to data analysis are incrementally being added to
the spectrum of services offered by the CC platforms and
providers. Analysis and study of TS will need to be pro-
cessed as CC services following the NIST [3] recommenda-
tions such as ﬂexibility, scalability, portability, and security.
The rise of CC in parallel with the increase in computing
capability has led to the deployment of tools and platforms

• Manuel Parra-Roy´on, Francisco J. Bald´an and Jos´e M. Ben´ıtez are with
the Department of Computer Sciences and Artiﬁcial Intelligence of the
University of Granada, Spain.
E-mail: manuelparra,fjbaldan,j.m.benitez@decsai.ugr.es

• Ghislain Atemezing is the head of R&D at Mondeca, Paris, France.

Manuscript received April 19, 2005; revised August 26, 2015.

for data mining and data analysis. Both offer a wide range
of methods, functions and algorithms to perform data pro-
cessing at all scales, either from the desktop [4], [5], large
clusters [6], [7], [8] or from service platforms in CC [9].

Within the area of DM, these CC providers and platforms
offer barely TS services and methods in a catalog of services
[9]. This means having to implement speciﬁc methods and
algorithms for working with TS on each provider platform
or to migrate all the source code developed in a speciﬁc
programming language and to ensure the entire service
deployment will work properly. TS modeling can be a
highly complex and it supposes a non-linear analysis tasks
[10] including a set of methods of the application of DM and
ML techniques [11], models, evaluations of performance or
precision measurements, among other, that can be seen as a
workﬂow of tasks.

The services of TS analysis along CC service providers
address a lack of integration from heterogeneous and non-
standardized cloud computing platforms. When migrating
services from one CC provider to another, the ideal solution
would be to harmonized the description of services and
workﬂow related to TS on CC deployments, abstracting the
programming language or the architecture of deployment.
This would allow the interoperability of these services be-
tween providers to be exploited more efﬁciently and offer
all the scalability and ﬂexibility advantages provided by the
CC paradigm. With this idea we pursue the industrialization
of IT services through pre-designed and pre-conﬁgured
solutions that are highly automated and repeatable, scalable
and reliable, by meeting the needs of users or organizations.
The aim of this paper is to propose a deﬁnition of
services for TS workﬂow in CC environments based on
semantic technology, according to the Linked Data [12]

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

proposal. To address this deﬁnition, an exhaustive set of
functions and algorithms related to the TS and the workﬂow
modeling have been speciﬁed and implemented (such as
data pre-processing, visualization, modeling, or precision
measures, among other). For the description of the differ-
ent components and structures, existing service description
schemes [13] have been re-used,
improving the overall
comprehension of the services. As a result, the tswf-schema
proposal has been developed, a workﬂow deﬁnition and TS
modelling, which together with dmcc-schema [14] allow to
cover the complete deﬁnition of CC services for workﬂow.

This paper is organized as follow: in the next section (2),
we present the related work within the scope of TS such
as its modelling and analysis, data mining workﬂow and
aspects of TS and CC. Section 3 presents the proposed CC
service deﬁnition of TS for workﬂow, including each of the
elements considered for TS modelling, such as seasonality
analysis, prediction methods or pre-processing, among oth-
ers. In the section ??, several use cases are implemented.
Finally, the conclusions of the work and the proposals for
future work are set out in section 6.

2 RELATED WORK

Time Series analysis and modeling is a very dynamic area
that is attracting the interest of the scientiﬁc community in
an incremental manner, due to the rise of IoT environments
and knowledge extraction from data sources in real time or
ofﬂine. In this way, an important part of the work carried out
with time series has been developed within distinct ﬁelds
such as business, economics, stock market, environmental
sciences, industrial monitoring, or engineering among oth-
ers [15], [16], [17].

The analysis and modeling of TS is a complex task that
includes the application of various operations, techniques
and algorithms. This procedure can be seen as a workﬂow,
covering a large number of methods and techniques to
apply and focused on solving TS problems [18].

The analysis of the time series has been studied in depth
and there is no single criterion that establishes which is the
procedure to be carried out for the workﬂow in this type
of problem. There are different methodologies to tackle the
problem of modelling and the approach to TS resolution.
The most widely used proposal is the Box-Jenkins method-
ology [19]. This methodology can be considered as linear
workﬂow. Box-Jenkins is used in the construction process
of the ARIMA [20] model for the TS covering aspects such
as identiﬁcation, estimation, error-testing and application
of methods and modelling [21]. Focusing on modelling,
techniques related to ARIMA, such as AR, MA or ARMA
[22], are also used as part of the TS workﬂow process. The
modelling of TS from a non-linear perspective has been
approached with the use of ANN [23], bi-linear model, TAR
or ARCH [24]. Other modelling techniques based on DM
and ML have been proposed including Random Forest [25],
Support Vector Machine [26], Neural Networks [23], or the
more modern Deep Learning one as in [27]. The TS analysis
also includes a comprehensive set of extract,transform,load
(ETL) data processing tasks [28].

TODO: add missing [REF] and ﬁgures in the document
For the resolution of this type of analysis, programming

2

languages and tools have been widely used, in addition to
DM platforms [29]. These utilities include all the required
components and functions for the relative TS workﬂow.
Languages such as R (with its task-view for TS) [30] or
Python (TS-speciﬁc libraries) [31], software packages such
as SAS [REF] or MathLab [REF] and DM environments
such as KNIME [32] or WEKA [4], offer the tools to make
effective the work with TS. In all of them, it is possible to
design a workﬂow where you can specify the application
of operations, visualize the results, validate errors and ap-
ply multiple algorithms for the modelling and subsequent
forecasting, classiﬁcation or clustering [33].

Most traditional time series analysis tools are designed to
work with desktop computers and are not ready to be used
in CC environments. Leveraging the computing capabilities
of organizations, part of that vast set of resources and infras-
tructure are being allocated to DM and ML as on-demand
CC services [9]. TS analysis is no exception and more and
more CC providers are including speciﬁc functions and
algorithms for working with TS services in their catalog
[34]. Currently, there is a growing demand for services that
allow creating workﬂow to be deployed over CC, such as
TS [35], [36]. These workﬂow are very interesting because of
their scalable character existing a clear need to move much
of the data processing to cloud platforms, abstracting the
underlying computing infrastructure and scalability needs,
both of which will be assured [37], [38].

One of the main problems of CC services is the lack of
a consistent and standardized deﬁnition of these services
among the different CC providers and it has been widely
studied [39], [40]. This happens in the same way with the
description of workﬂow and experimentation with data in
CC in the analysis of TS. The portability of services and
the ability to abstract the underlying infrastructure makes it
necessary to validate this type of problem on CC platforms
[41] .

workﬂow modeling for DM experimentation is consid-
ered in [42], [43] performing workﬂow as CC services giving
the user the ability to deploy a work of experimentation in
an integral way. Several approaches manage the problem
of the description of workﬂow dealing with ontology-based
such us RDF or Turtle [44]. Those languages for workﬂow
deﬁnition have been discussed in [45] and [46].

The deﬁnition and description of generic CC services
that integrate multiple aspects of experimental work for
DM have been worked from the scope of the Linked Data
[12] proposal. Research papers such as DMOP [47], Expos´e
[48], dmcc [14] or MLSchema [49] are examples of languages
proposals for the deﬁnition of generic data mining services
and workﬂow, related to ML. These provide an adequate
deﬁnition of services using a highly ﬂexible deﬁnition lan-
guage and linked to the natural development of the CC.

Most of the proposals allow the development of experi-
mental analysis, including part of the usual workﬂow with
data processing and algorithms [50]. In these works the
workﬂow modeling with TS is not taken into account being
a fundamental element for the integration of these services
within CC platforms [51]. These types of problems need to
bring together the different techniques and algorithms of TS
modeling, pre-processing [52], performance measurements
[REF], visualization [REF], or predictive models among

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 1. General schema for Time Series workﬂow modelling

others. In our work, a proposal of workﬂow modeling for
TS in CC has been developed, which allows to tackle the
work with the TS experimentation and modeling using
Linked Data recommendation on services description in CC
platforms.

3 TIME SERIES SERVICE DESCRIPTION

Time Series modeling is a challenging task that integrates
different actions following a dynamic workﬂow. In this
work a complete proposal for workﬂow modeling with TS
in CC has been made. The proposed schema is called tswf-
schema and has been developed using a semantic language
based on ontology, following the Linked Data guidelines
for the deﬁnition and description of concepts, entities and
relationships to other schemes. A complete diagram has
been deﬁned allowing any workﬂow with TS to be modeled,
as can be seen in the section on ??, in which several examples
of working with time series are developed and modelled
using tswf-schema .

The scheme is divided into several parts that facilitate
its integration and modularity, these are: data pre-processing,
data visualization,
functions for information analysis, work
with seasonality analysis, predictive model selection, learning
problem information, data entry, and performance evaluation
measures, among others. Each of these parts is developed
in detail throughout this section. In ﬁgure 1 you can see
the general diagram of the ﬂow rate modeling with time
series that has been designed. For reasons of space, the
complete scheme has not been displayed, given its size,
so that each of the main container classes has been shown.
Following the Linked Data speciﬁcation, other vocabularies
have been used from other schemes, completing and broad-
ening the deﬁnition of the scheme. The scheme provided
by MLSchema [49] and dmcc-schema [14] has been used as
base for the workﬂow of experimentation with TS, in addi-
tion to other vocabularies such as SKOS [53] or schema.org
[54] among other.

For the deﬁnition of workﬂow modeling has been taken
into account a high volume of research papers and books
related to TS modeling and analysis. For the container
classes and other parts of the scheme, research related to
the area has been reviewed, along with actual TS analysis
work from various sources such as [11] [55] [56]. This has
made it possible to extract a large part of the operations and
methods used to model TS, also from multiples knowledge
domains. In the proposal developed in tswf-schema we have

Fig. 2. Preprocessing classes for Time Series workﬂow modelling

integrated the greatest number of functionalities, together
with the most common ones during the workﬂow process
with TS. It also comprises widespread modeling such as
the Box-Jenkings [19] methodology as well as workﬂow-
based experimentation for processing DM problems and ML
applied to TS [57] [58].

tswf-schema contains all key elements in the description
of cloud computing services, such as interaction points,
prices, instances or SLAs, among others, and serves as a
complement to the integration of a workﬂow with time
series as a Service on CC platforms. This means that it allows
you to have the complete description of CC services, both
from a functional and a business point of view.

In the following subsections, all the main components of

the tswf-schema deﬁnition are detailed:

Pre-processing. Part of the analysis and work with TS
requires operations on the data, where they apply transfor-
mations, reductions, cleaning, or imputations among others.
In the diagram in ﬁgure 2 you can see all the elements
most commonly used in data level time series processing.
It has basically been segmented into various parts, such as
imputation, outliers, spectral analysis, scaling, noise reduc-
tion or smoothing. Each of the sub-parts contains several
of the functionalities that have been considered the most
commonly used in the processing of these types of data,
according to the work of [52].

Analysis of the information and analysis of seasonality.
Most of the statistical studies with TS are based on the
Box-Jenkins methodology [19], where a series of studies on
the data of the time series is applied to check the trend,
seasonality among others, as well as different statistical
tests necessary to correctly identify the time series. As
shown in ﬁgures 3 and 4, they have been divided into two
parts, on the one hand, the analysis of the information,
which includes correlation, seasonality and trend tests. On
the other hand, it also includes statistical tests, integrating
stationarity, normality, randomness or non-linearity among
others.

Evaluation measures. The output of the analysis work-
ﬂow of a TS can be identiﬁed according to the TS study
problem in question. Four main groups of measures have

PreprocessingPredictiveModelInformationAnalysisDataExperimentTSAnalysisisPartOfStudyisPartOfTSStudycompriseshasInputentailsPreprocesingTaskachievesEvaluationSpeciﬁcationhasInformationAnalysisperformsStationaryAnalysishasStationaryAnalysisEvaluationProcedureEvaluationMeasurehasParthasParthasOutputTSVisualizationhasPlotdeﬁnesJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Fig. 3. Information analysis classes for Time Series workﬂow modelling

Fig. 6. Predictive models classes for Time Series work-ﬂow modelling

Fig. 4. Stationary analysis classes for Time Series work-ﬂow modelling

been considered, related to those most used in TS problems,
such as classiﬁcation, similarity measurements [59], preci-
sion of the forecast [60], performance of the clustering [61],
or analysis of the residues. Classiﬁcation measures such as
F1-Score, ROC, or matrix confusion have been taken into
account. For similarity measures, others such as DTW [62],
Edit Distance [63] or Jaccard [64] have been implemented,
as they are common in problems of this type. As regards
the Error measures, which identify the quality of the forecast
adjustment, all the measures considered in the work [65]
have been included. If the problem being addressed is
related to time series clustering, different measures such
as APN, AD/ADM or Silhouette W have been taken into
account. Figure 5 details the set of evaluation measures that
have been included in tswf-schema.

Predictive model. When instantiating a model that tries
to ﬁt the data of the TS, a multitude of algorithms and

Fig. 5. Evaluation measures classes for TS workﬂow modelling

Fig. 7. Visualization classes for Time Series work-ﬂow modelling

statistical methods can be applied. In our work we have in-
tegrated the algorithms and methods most commonly used
in this area of TS data analysis. From widespread statistical
methods such as ARIMA or variants of it (ARIMA, SARIMA,
ARIMAX, etc.), as well as others such as ETS among others.
Regression analysis has also been implemented with meth-
ods related to time series considered in [66], such as AR,
LASSO or MARS among others. Finally, an important part
of Machine Learning methods has been considered, mainly
from [67] studies. These algorithms include the application
of techniques based on Neural Networks, Random Forest or
SVM among the most outstanding. The complete diagram
of the methods and algorithms included in tswf-schema can
be seen in ﬁgure 5.

Visualization of the data. One part of the work is to
perform repeated visualizations of the data to assess the
state of the data and to use different plot analyses to better
understand the shape of the data. This allows to observe
some of the visual
information such as decomposition,
differentiation or STL plots. Figure 7 implements the set of
visualizations available.

Workﬂow. The study of data generally does not follow a
linear work scheme as indicated with this particular type of
data, so that the data engineer or data scientist generates
different ﬂows of operations with which to compose the
work during the analysis. To facilitate this task in tswf-
schema, all operations and tasks that are performed can be
composed as a linear or nonlinear sequence. If we think at a
high level, this feature is what will provide the CC service
for TS with the necessary functionality to visually compose
(as building blocks) each operation or task to be performed
at each time to produce a complete analysis. In ﬁgure X can
be seen how this functionality intrinsic to tswf-schema serves
to manage the ﬂow of operations contained in the scheme.

TSVisualizationPlotACFPlotPACFSTLPlotRegularDecomposeDifferentiationJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

Fig. 8. Components of a generic CC service together with the function-
ality elements of a TS service in a service catalogue.

4 DEPLOYING TIME SERIES SERVICES

Once all the components of tswf-schema have been deﬁned
in the previous section, it is necessary to present a set of ex-
amples that highlight the potential of the scheme designed
to describe TS workﬂows.

To address the presentation of these cases, two models
will be used, on the one hand a) the straightforward tran-
scription of examples proposed in a programming language
into tswf-schema, putting the value on the ease of translating
any TS-based study into a semantic agnostic model ready
to be ported, deployed or executed on CC providers, and b)
the modelling of a cloud service with key components of the
service, so that a CC service is speciﬁed and it is ready to
be part of a CC provider’s catalogue. For the examples we
will use R and Python language, since they are one of the
most extended languages and for the transcription of those
example codes to the tswf-schema scheme we will use json-ld.
In order to make the process of developing a CC service
based on TS more comprehensible, the ﬁgure 8 shows the
different elements that make up a CC service. This ﬁgure
shows several aspects related to the deﬁnition of the service
management logic, more closely linked to the deﬁnition of
CC, and on the other hand the description of the service
functionality itself, in this case, a workﬂow with time se-
ries that runs as a service within a CC provider’s service
catalogue.

4.1 Baseline of the structure of a TS workﬂow

The general minimum structure for designing and instan-
tiating a TS analysis using tswf-schema can be seen in the
listing 1, according to the ﬁgure 1 that outlines the overall
diagram of the scheme. This skeleton contains the basic
components that will be instantiated to show a functional
example of the description of a workﬂow with TS, where
in line 2-3 is established which will be the context of vo-
cabulary used for the deﬁnition in json-ld, from line 6 to
10 are added the different operations of pre-processing of
input data and various studies on them. Then in lines 11
and 12 are stated the methods that will be used to create the
models and from line 15 the workﬂow information output.
Each of the skeleton components is detailed in the following
sections.

Listing 1. Skeleton of the component instantiation in json-ld
{

"@context": {

Fig. 9. Diagram of the operations and tasks carried out for the case study
use of Lake Huron.

"tswf": "http://dicits.ugr.es/linkeddata/tswf-

schema/"

},
...
"tswf:hasInput": {

"tswf:hasPlot": {...},
"tswf:hasInformationAnalysis": {...},
"tswf:hasStationaryAnalysis": {...}

},

"tswf:performs": {

"tswf:hasTSAnalysis": {...},
...
}

"tswf:hasOutput": {

},
...

}

3

4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

4.2 A model of TS analysis
For this use case we will use an example of an actual time series
analysis, implemented in the R language. This example consists of
a series of basic steps that reproduce the modeling of a simple TS
example. This workﬂow can be seen visually in the ﬁgure 9.

The full code in R and Python of the workﬂow implementation
can be found in the scheme repository [68]. In the description of
the use case, we represent some parts of the modeling as a service,
so we can compare the R-language code transcription to the tswf-
schema scheme with json-ld as a semantic description language. This
is not to replace one language with another, but with tswf-schema a
complete description of a high level workﬂow is provided, abstracting
the programming support, the platform or the computing environment.
One of the advantages of using semantic models is that you can
indicate the level of detail you want in each deﬁned aspect. This is
more expressive and less complex to understand, leaving more core
aspects to the implementation of the underlying system that manages
the schema, such as the default settings.

One of the advantages of using semantic description technologies
to deﬁne services or functions is that you can indicate the level of detail
you want in each deﬁned aspect. This is more expressive and less com-
plex to understand, leaving more core aspects to the implementation
of the underlying system that manages the schema, such as the default
settings.

4.2.1 TS Analysis: Workﬂow information and description
The deﬁnition of services in general terms requires a minimum of basic
information describing what is being implemented. In our case, the
ﬁrst step to deﬁne the workﬂow instantiation with tswf-schema is the
description of the analysis, through the main class TSAnalysis.With
TSAnalysis and its properties all the attributes considered key in the
deﬁnition of what is going to be deployed are declared so that it can

1
2

Cloud Computing Provider CataloguePricesSLAInteraction PointsAuthenticationTime Series Workﬂow Service…JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

3, a simple instantiation of the data entry is shown, where over the
line number 2 is indicated the type of entry (tswf:Data), the type of
source (tswf:CSVFile in this case) in lines 3-4 and where physically
are the data by means of the property tswf:src (line 5). Then, with
tswf:fields the features and ﬁelds of the data source are set within
the lines 6-12.

Listing 3. Instantiation of the data entry source and its parameterization.
...
"tswf:hasInput": {

"@type": "tswf:Data",
"tswf:source": {

"@type": "tswf:CSVFile",
"tswf:src": "///dicits/examples/lakehuron.csv",
"tswf:fields": {
"@set": [

{"@value": "Year",

"@type": "tswf:datetime"},

{"@value": "Level",

"@type": "tswf:integer"}

]

}

},
...

The transformation of the initial data intake part into tswf-schema is
done at a higher level, since semantically each operation is labeled with
its properties. This allows not having to deﬁne in advance each detail
of the parameterization (or indeed the application of the TS type), so
that this complexity can be left to the underlying system or platform as
the service responsible for making these tweaks.

1
2

3
4

5
6

7

4.2.3 Preliminary exploratory analysis
A basic part of the analysis of TS data is exploratory data analysis
where the data can be plotted, checked for patterns or trends, veriﬁed
for seasonality or the presence of cycles, among others. In the example,
we are dealing with, through the input data we make several key
charts to visually understand the problem. We consider for instance
PACF (Partial Autocorrelation Function), ACF (Autocorrelation Function),
STL (Seasonal and Trend decomposition using Loess) as well as a visual
representation of the data.

The graphical representation of the input data is done from the
tswf:hasPlot property. In tswf-schema there is a set of possible
8
deﬁnitions of the most commonly used diagrams and charts within
9
TS analysis, so to instantiate a type of chart that you want to display
10
is only needed to include the list of types that are required for the
11
analysis as shown in listing 4. Note that the level of detail selected
12
to instantiate these types of chars does not include parameterization
13
except in tswf:PlotPACF, this is because the internal implementation
14
will be in charge of using the correct visualization with the default
15
parameters of each chart type for the input data. On the other hand,
for tswf:PlotPACF a lag parameter (tswf:parameters) and its
speciﬁc value have been indicated (lines from 6 to 12).

Listing 4. Set of diagrams/charts that are deployed
...
"tswf:hasPlot":{

"@type": "tswf:TSPlot",
"@set":[

{"@type":"tswf:PlotSTL"},
{"@type":"tswf:PlotACF"},
{"@type": "tswf:PlotPACF",

"tswf:parameters": {

"@set": [

{"tswf:name":"lag", "@value": 10}

]

}

},
{"@type":"tswf:PlotRegular"}

]

},
...

Listing 5. TS information analysis.
...
"tswf:hasInformationAnalysis": {

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

1
2

Fig. 10. Diagram of the components and operations of a simple TS
analysis use case using tswf-schema.

be described in the most efﬁcient way possible and with a friction-less
integration in catalogs or marketplace of services in Cloud Computing.
Aspects related to the automatic discovery of services for CC would
be deﬁned from this class. For space reasons not all attributes of
TSAnalysis will be exposed, so only some basic ones are shown in
the listing 2.

Listing 2. Deﬁnition of the basic information of time series analysis.
"@context": {

"tswf": "http://dicits.ugr.es/linkeddata/tswf-

schema/"

},
"@id": "http://dicits.ugr.es/tswf-marketplace/#

TS_eb09t74",

"@type": "tswf:TSAnalysis",
"tswf:name": "TS Analysis base with code TS_eb09t7

4",

"tswf:description": "This is an example of TS an."

,

"tswf:author": "This is an example of TS an.",
"tswf:dateCreated": "2020-09-01 10:30:00",
"tswf:version": "Test version 1.0",
"tswf:codeRepository": {

"@type":"tswf:url",
"@value":"https://"
}

"tswf:hasInput": ...

In the ﬁrst lines (lines 1 to 3) the context of the overall TSAnalysis
description is declared in which tswf-schema will be used. The con-
text allows applications to use a set of terms to communicate with
one another more efﬁciently, but without losing accuracy. Then, each
instance of TSAnalysis created with tswf-schema must be marked
with an identiﬁer that uniquely relates this workﬂow, for example,
within a particular catalog, for which @id is speciﬁed (line 4). The
@type property indicates that all the remaining content corresponds
to a TSAnalisys and subsequently part of its attributes are declared
(lines from 5 to 13), such as the TS name, description of the workﬂow,
or the repository where the code of the described service will be hosted,
to take the description of the ﬂow to execute it (lines 11-14). The
rest of the properties of the workﬂow components are instantiated
in the following subsections (from line 15, with ”tswf:hasInput”,
”tswf:hasOutput”, etc. as described within ﬁgure X).

4.2.2 Data input
To perform the ingestion of the data of a time series, basically in the R
language the data can be loaded from different sources, such as CSV
format for example. In CC environments or other computing platforms
it is possible that data can come from databases, TS databases, or
even data steaming services. In this way, just as R or Python supports
data ingest from multiple sources, tswf-schema has the possibility to
include very diverse data sources. For the data input instantiating it
is necessary to at least deﬁne the data source, the source type and the
ﬁelds that will be used during the whole analysis process. In the listing

lakehurontsdatahasInputts_plot_pacfts_plot_acfts_plot_stlts_plot_regularhasPlotts_info_pacfts_info_acfts_info_lagts_info_trendstdhasInfoAnalysists_statisticaltesthasStationaryAnalysists_dickyfullerts_jarqueverats_jungboxPredictive ModelMLAnalysisTSRegressionTSAnalysisperformsARSVMARIMAEvaluationMeasurehasOutput1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

"@type": "tswf:InformationAnalysis",
"@set": [

{"@type": "tswf:LagStudy"},
{"@type": "tswf:TrendSTL"},
{"@type": "tswf:ACF"},
{"@type": "tswf:PACF"}

]

},
...

Further on, in the listings 5 and 6 are shown how it is possible to
add different studies used in the analysis, such as Information Analysis,
or Stationary Analysis. For both types of studies, following the scheme
shown in ﬁgures 3 and 4, it is possible to include their corresponding
tests and operations. For the analysis of the information, as shown in
list 5, four basic studies have been added in the work with TS (lines 4
to 8). In the same way as with the descriptions of the charts, we have
taken all the default parameters for this example. On the other hand, in
the study of stationarity analysis shown in list 6 four basic tests have
been added (lines 4 to 8).

Listing 6. TS stationary analysis.
...
"tswf:hasStationaryAnalysis": {

"@type": "tswf:StatitionaryAnalysis",
"@set": [

{"@type": "tswf:StatisticalTest"},
{"@type": "tswf:DickeyFuller"},
{"@type": "tswf:JarqueBera"},
{"@type": "tswf:JungBox"}

]

},
...

4.2.4 Predictive model and evaluation measures
Once the exploratory analysis is done, the selection and adjustment of
the model is performed. The selection of the best model depends on the
availability of historical data or the dependence of variables. Hence it
is often necessary to compare several predictive models to see how the
data actually behave. For this example the R code of the three models
displayed in listing 9, where an AR regression model, an ARIMA
modeling and ﬁnally an SVM algorithm are used. The consequent code
in the tswf-schema scheme corresponds to the listing code 9. For each of
the methods, it is possible to specify the parameterization, in this case
only for the ARIMA model has been speciﬁed. In listing 7, from line 2
to line 5, a model based on Machine Learning functions/operations is
speciﬁed, such as SVM (tswf:SVM). Another regression-based model is
indicated in lines 10-12 (tswf:AR) and ﬁnally, with tswf:ARIMA (lines
7-15) an ARIMA model is instantiated, with a partial parameterization
(tswf:parameters) as it appears in lines 11-13 including order,
seasonal and lambda.

Listing 7. TS predictive model.
...
"tswf:performs": {

"@type": "tswf:PredictiveModel",
"tswf:hasMLAnalysis": {
"@type": "tswf:SVM"

},
"tswf:hasTSAnalysis": {

"@type": "tswf:ARIMA",
"tswf:parameters": {

"@set": [

{"tswf:name": "order", "@value": [0,0,1]

)},

{"tswf:name": "seasonal", "@value": [0,0

,1])},

{"tswf:name": "lambda", "@value": 0)}

]

}

},
"tswf:hasTSRegression": {

"@type": "tswf:AR"

}

},

...

Finally, as part of the explicit output of the workﬂow, using
3
the tswf:hasOutput property, it is possible to indicate what kind
4
of additional
information can be analyzed as a result of the full
5
process (listing ??, line 2). This has included in the output of the
6
workﬂow the prediction function, which is called to return the re-
7
sults on how is the forecasting on different horizons, by giving
8
the user of different measures to evaluate the performance of the
9
forecasts, for example (see ﬁgure 5). In the listing ??,
lines 5-10
10
we have chosen to include two measures (RMSE and MSE) for the
11
evaluation (tswf:EvaluationMeasures) with which you can verify
the accuracy of the forecast (tswf:ForecastAccuracy), made with
the models generated in the previous stage through the property
tswf:performs (in this instance AR, ARIMA and SVN).

Listing 8. TS evaluation measures.
...

"tswf:hasOutput": {

"@type": "tswf:EvaluationMeasures",
"@set": [

{

"id": "tswf:TSFCastAccu",
"@type": "tswf:ForecastAccuracy",
"tswf:hasMeasures": [

}

}

]

]

...

{"@type": "tswf:RMSE"},
{"@type": "tswf:MSE"}

1
2
3
4
5
6
7
8
9
10
11
4.3 A CC service for TS
The TS example provided in the section 3 is deﬁning a workﬂow as
if we were doing it in a programming language like R or Python, but
using json-ld as a description language. With tswf-schema it is possible
to package TS workﬂows and make them portable between different
computing platforms by reading the description and operating each
high-level component on an underlying platform or by using a Broker to
decide which SVM or ARIMA implementation at the low-level will be
used to process a particular element. This gives an idea of the potential
of using this type of semantic technology to tackle the deployment of
services in CC. In this manner for this use case we want to put the value
how tswf-schema can be integrated to support a CC service for TS.

Following the principles of Linked Data for semantic data, we will
reuse another scheme called dmcc-schema [69] that allows unifying
all the basic aspects of the deﬁnition and management of a CC service
including costs/prices, catalog or SLA among others, together with the
functionality such as a TS analysis.

1
2
3
4
5
6
7
8
9
10
11

12

dmcc+tswfcc.png

In ﬁgure 4.3 a diagram can be seen combining both schemes to
complete the description of a TS workﬂow with the management of a
CC service.

13
14
15
16
17
5 VALIDATION
18
19
In addition to giving in the previous section of examples of TS mod-
20
eling using tswf-schema, to reinforce the validity of the scheme as a
21
mechanism for the deﬁnition of TS analysis services in CC, we have

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

carried out two additional validation actions, such as Marketplace of
TS services for CC with tswf-schema and a set of competence questions
(CQs) .

The platform is available for use in production in the website of
the project [70] and the source code is available in a Github repository
[71], in this way it is possible to deploy the TS Marketplace in any other
provider of CC services.

5.1 TS Marketplace
Dealing with semantic deﬁnitions and descriptions, a test of the validity
of the technology for practical purposes is to deploy these components
effectively within service catalogs. These catalogs have a twofold func-
tion, on the one hand, to serve as a repository of service deﬁnitions
and on the other hand to provide a point of service discovery by
programmatic entities having the ability to understand the services
exposed. An example of a service catalog with descriptions that can
be imported, exchanged, discovered, and consumed has been designed
as part of a TS Marketplace. The design of this Marketplace contains 3
key features:

•

Import, where the platform allows users to include their own
tswf-schema instances in json-ld or ttl format. With this tool,
service descriptions are validated and included in the catalog.
Once in the catalog, it is possible to use the composition tool
where the workﬂow description can be modeled or viewed
using the tswf-schema elements in a visual way. Figure 5.1
shows the workﬂow import diagram to the TS Marketplace
and a part of the service catalog of available TS containing
all the basic attributes and details deployed. For each TS it
is possible to explore the deﬁnition and the components that
integrate it, as well as the possibility to share/download each
service available in the catalog.

5.2 Competency Questions (CQs)
Competency questions (CQs) [72] are used to specify the knowledge
that has to be entailed in the ontology/vocabulary and thus can be
considered to be requirements on the content of the ontology. A way
to validate a semantic schema is the creation of a series of CQs to test
whether the tswf-schema correctly ﬁts the problem domain and is able to
accurately solve the queries made to it. With these CQs we try to cover
an important part of the deﬁnition of a TS service in CC , considering
key elements such as, the study, analysis, or predictive models and
also the management of the workﬂow of the TS itself within the CC
provider (aspects like CC service management is shown in section 4.3).
The selected 10 CQs are the following:

CQ01 How many operations [tasks] does the X TS workﬂow for

predictive analysis have?
Response: .

CQ02 What services from the catalog [of a CC provider] enable the use

of TS processing functions?
Response: .

CQ03 Does the TS service provide algorithms [functions] for predictive
analysis using Deep Neural Networks and does it provide an
SVM algorithm?
Response: .

CQ04 What data input does the TS workﬂow need with the X-

identiﬁer?
Response: .

CQ05 What are the outputs of the X-identiﬁer of a TS workﬂow for the

CC service?
Response: .

CQ06 What is the estimated economic cost of running a TS workﬂow

importﬂow.png

CQ07

analysis for the CC provider X?
Response: .
Is there authentication for the execution of the TS service for
provider X?
Response: .

• Composition, similar to other platforms, composition offers a
complementary visual tool for creating/modifying TS work-
ﬂows and services. In this sense it makes it much easier to
compose workﬂows in general, since visually it is much more
comfortable to use a visual tool than to code it manually. An
example of the platform showing a workﬂow with example
TS is shown in the Figure 5.1. In this ﬁgure a set of workﬂow
operations that the service is capable of deploying are depicted.

composeﬂow.png

•

Service consumption, once the descriptions of tswf-schema
instances are published, they are available for consumption
either manually or automatically, for example, to deploy them
from CC providers.

CQ08 What is the parameterization of the ARIMA algorithm?

CQ09

Response: .
Can you display the prediction data for a horizon of X days for
the Y workﬂow?
Response: .

CQ10 Which predictive model of the analysis produces the results with

the lowest RMSE error?
Response: .

The aim is to conﬁrm that with tswf-schema is possible to capture
partially the features of a speciﬁc TS service and it can be integrated
into a service deﬁnition, through which make queries and manage this
type of services for multiple CC providers. Dataset, CQs and SparQL
queries (endpoint) are available on the project website [70].

6 CONCLUSIONS AND FUTURE WORK
In this article we have introduced a scheme for the description of
services in CC speciﬁcally designed for TS processing named tswf-
schema. The scheme developed brings together all the commonly used
operations in the study and analysis of TS, which allows transforming
any implementation already developed in languages such as R or
Python into a description based on semantic technology much richer,
more homogeneous, and portable. This portability is key in terms of
the fact that a single deﬁnition of a workﬂow with tswf-schema would
be manageable and deployable in any CC service provider signiﬁcantly
facilitating the industrialization of services for CC environments.

With the scheme developed together with other complementary
ones such as dmcc-schema, it is possible to include in the deﬁnition
TS workﬂow all the elements related to the management of a CC service
such as prices, SLA or instances, among other, taking advantage of the
Linked Data proposal.

The selected use cases allow, on one hand, to show the user that any
workﬂow implementation can be transformed into a semantic speciﬁ-
cation with tswf-schema ready to be consumed in CC environments, and
on the other hand, it highlights the need to have a homogenization in
the deﬁnition of this type of computing services, which unfortunately

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

the CC service providers market lacks. In addition, thanks to CQs it
is possible to respond to a subset of desirable features that a service
engineer in Cloud Computing might need. Both effectiveness and
efﬁciency have been highlighted in the uses case validation.

Finally, as a future work, we propose the implementation of a Cloud
Computing services broker for TS that has the ability to optimize a
workﬂow written in tswf-schema taking advantage of algorithm imple-
mentations and computing capabilities over different CC platforms.

ACKNOWLEDGMENTS
M. Parra-Royon holds a ”Excelencia” scholarship from the Regional
Government of Andaluc´ıa (Spain). This work was supported by the Re-
search Projects P12-TIC-2958 and TIN2016-81113-R (Ministry of Econ-
omy, Industry and Competitiveness - Government of Spain).

REFERENCES

[1]

J. Gubbi, R. Buyya, S. Marusic, and M. Palaniswami, “Internet
of things (iot): A vision, architectural elements, and future direc-
tions,” Future generation computer systems, vol. 29, no. 7, pp. 1645–
1660, 2013.
“Capture
2018.
internet-of-things/audience-page/capture-the-value-in-iot
[3] P. Mell, T. Grance et al., “The nist deﬁnition of cloud computing,”

Aug
https://www.ericsson.com/en/

[Online]. Available:

ericsson,”

value

the

[2]

iot

in

-

NIST, 2011.

[4] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten, “The weka data mining software: an update,” ACM
SIGKDD explorations newsletter, vol. 11, no. 1, pp. 10–18, 2009.
I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal, Data Mining:
Practical machine learning tools and techniques. Morgan Kaufmann,
2016.

[5]

[6] P. Zikopoulos, C. Eaton et al., Understanding big data: Analytics for
enterprise class hadoop and streaming data. McGraw-Hill Osborne
Media, 2011.

[7] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu,
J. Freeman, D. Tsai, M. Amde, S. Owen et al., “Mllib: Machine
learning in apache spark,” The Journal of Machine Learning Research,
vol. 17, no. 1, pp. 1235–1241, 2016.

[8] W. McKinney et al., “Data structures for statistical computing in
python,” in Proceedings of the 9th Python in Science Conference, vol.
445. Austin, TX, 2010, pp. 51–56.
I. A. T. Hashem, I. Yaqoob, N. B. Anuar, S. Mokhtar, A. Gani, and
S. U. Khan, “The rise of “big data” on cloud computing: Review
and open research issues,” Information systems, vol. 47, pp. 98–115,
2015.

[9]

[10] J. D. Hamilton, Time series analysis.

Princeton university press

Princeton, NJ, 1994, vol. 2.

[11] R. Harris and R. Sollis, Applied time series modelling and forecasting.

Wiley, 2003.

[12] C. Bizer, T. Heath, and T. Berners-Lee, “Linked data: The story
so far,” in Semantic services, interoperability and web applications:
emerging concepts.

IGI Global, 2011, pp. 205–227.

[13] P.-Y. Vandenbussche, G. A. Atemezing, M. Poveda-Villal ´on, and
B. Vatant, “Linked open vocabularies (lov): a gateway to reusable
semantic vocabularies on the web,” Semantic Web, vol. 8, no. 3, pp.
437–452, 2017.

[14] M. Parra-Royon, G. Atemezing, and J. M. Ben´ıtez, “Data mining
deﬁnition services in cloud computing with linked data,” arXiv
preprint arXiv:1806.06826, 2018.

[15] F. Dominici, A. McDermott, S. L. Zeger, and J. M. Samet, “On the
use of generalized additive models in time-series studies of air
pollution and health,” American journal of epidemiology, vol. 156,
no. 3, pp. 193–203, 2002.

[16] S. Aljawarneh, V. Radhakrishna, P. V. Kumar, and V. Janaki, “A
similarity measure for temporal pattern discovery in time series
data generated by iot,” in Engineering & MIS (ICEMIS), Interna-
tional Conference on.

IEEE, 2016, pp. 1–4.
[17] X. Xu, S. Huang, Y. Chen, K. Browny, I. Halilovicy, and W. Lu,
“Tsaaas: Time series analytics as a service on iot,” in Web Services
(ICWS), 2014 IEEE International Conference On.
IEEE, 2014, pp.
249–256.

[18] D. C. Montgomery, C. L. Jennings, and M. Kulahci, Introduction to

time series analysis and forecasting.

John Wiley & Sons, 2015.

[19] Z. Tang, C. De Almeida, and P. A. Fishwick, “Time series forecast-
ing using neural networks vs. box-jenkins methodology,” Simula-
tion, vol. 57, no. 5, pp. 303–310, 1991.

[20] C. Chatﬁeld, The analysis of time series: an introduction. CRC press,

2016.

[21] S. Makridakis and M. Hibon, “Arma models and the box–jenkins
methodology,” Journal of Forecasting, vol. 16, no. 3, pp. 147–163,
1997.

[22] E. D. ¨Ubeylı and I. G ¨uler, “Spectral analysis of internal carotid
arterial doppler signals using fft, ar, ma, and arma methods,”
Computers in biology and medicine, vol. 34, no. 4, pp. 293–306, 2004.
[23] G. P. Zhang, “Time series forecasting using a hybrid arima and
neural network model,” Neurocomputing, vol. 50, pp. 159–175,
2003.

[24] R. Engle, “Garch 101: The use of arch/garch models in applied
econometrics,” Journal of economic perspectives, vol. 15, no. 4, pp.
157–168, 2001.

[25] A. Liaw, M. Wiener et al., “Classiﬁcation and regression by ran-

domforest,” R news, vol. 2, no. 3, pp. 18–22, 2002.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

[26] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf,
“Support vector machines,” IEEE Intelligent Systems and their appli-
cations, vol. 13, no. 4, pp. 18–28, 1998.

[27] J. Yang, M. N. Nguyen, P. P. San, X. Li, and S. Krishnaswamy,
“Deep convolutional neural networks on multichannel time series
for human activity recognition.” in Ijcai, vol. 15, 2015, pp. 3995–
4001.

[28] B. Hoey and N. Dassi, “System and methods for real-time detec-
tion, correction, and transformation of time series data,” Jan. 30
2014, uS Patent App. 13/916,513.

[29] A. Jovic, K. Brkic, and N. Bogunovic, “An overview of free
software tools for general data mining,” in Information and Commu-
nication Technology, Electronics and Microelectronics (MIPRO), 2014
37th International Convention on.

IEEE, 2014, pp. 1112–1117.

[30] T. Hothorn, “Cran task view: Machine learning & statistical learn-

ing,” 2018.

[31] J. VanderPlas, “gatspy: General tools for astronomical time series

in python,” Astrophysics Source Code Library, 2016.

[32] M. R. Berthold, N. Cebron, F. Dill, T. R. Gabriel, T. K ¨otter, T. Meinl,
P. Ohl, K. Thiel, and B. Wiswedel, “Knime-the konstanz informa-
tion miner: version 2.0 and beyond,” AcM SIGKDD explorations
Newsletter, vol. 11, no. 1, pp. 26–31, 2009.

[33] K. Rangra and K. Bansal, “Comparative study of data mining
tools,” International journal of advanced research in computer science
and software engineering, vol. 4, no. 6, 2014.

[34] F. Chen, P. Deng, J. Wan, D. Zhang, A. V. Vasilakos, and X. Rong,
“Data mining for the internet of things: literature review and
challenges,” International Journal of Distributed Sensor Networks,
vol. 11, no. 8, p. 431047, 2015.

[35] F. Marozzo, D. Talia, and P. Trunﬁo, “A workﬂow management
system for scalable data mining on clouds,” IEEE Transactions on
Services Computing, 2016.

[36] M. Chen, S. Mao, and Y. Liu, “Big data: A survey,” Mobile networks

and applications, vol. 19, no. 2, pp. 171–209, 2014.

[37] J. Kranjc, J. Smailovi´c, V. Podpeˇcan, M. Grˇcar, M. ˇZnidarˇsiˇc, and
N. Lavraˇc, “Active learning for sentiment analysis on data streams:
Methodology and workﬂow implementation in the clowdﬂows
platform,” Information Processing & Management, vol. 51, no. 2, pp.
187–203, 2015.

[38] J. Kranjc, V. Podpeˇcan, and N. Lavraˇc, “Clowdﬂows: a cloud
based scientiﬁc workﬂow platform,” in Joint European Conference on
Machine Learning and Knowledge Discovery in Databases.
Springer,
2012, pp. 816–819.

[39] T. Dillon, C. Wu, and E. Chang, “Cloud computing: issues and
challenges,” in Advanced Information Networking and Applications
(AINA), 2010 24th IEEE International Conference on.
Ieee, 2010, pp.
27–33.

[40] D. K. Barry, Web services, service-oriented architectures, and cloud

computing. Elsevier, 2003.

[41] A. Fox, R. Grifﬁth, A. Joseph, R. Katz, A. Konwinski, G. Lee,
D. Patterson, A. Rabkin, and I. Stoica, “Above the clouds: A berke-
ley view of cloud computing,” Dept. Electrical Eng. and Comput.
Sciences, University of California, Berkeley, Rep. UCB/EECS, vol. 28,
no. 13, p. 2009, 2009.

[42] D. Talia, “Clouds for scalable big data analytics,” Computer, vol. 46,

no. 5, pp. 98–101, 2013.

[43] B. Data, C. Catlett et al., “A cloud framework for big data analytics
workﬂows on azure,” Cloud Computing and Big Data, vol. 23, p.
182, 2013.

[44] W. W. W. Consortium et al., “Rdf 1.1 concepts and abstract syntax,”

W3C, 2014.

[45] L. Youseff, M. Butrico, and D. Da Silva, “Toward a uniﬁed
ontology of cloud computing,” in Grid Computing Environments
Workshop, 2008. GCE’08.

IEEE, 2008, pp. 1–10.

[46] A. Iosup, S. Ostermann, M. N. Yigitbasi, R. Prodan, T. Fahringer,
and D. Epema, “Performance analysis of cloud computing services
for many-tasks scientiﬁc computing,” IEEE Transactions on Parallel
and Distributed systems, vol. 22, no. 6, pp. 931–945, 2011.

[47] C. M. Keet, C. d’Amato, Z. C. Khan, and A. Lawrynowicz, “Ex-
ploring reasoning with the dmop ontology.” in ORE, 2014, pp.
64–70.

[48] P. Panov, L. Soldatova, and S. Dˇzeroski, “Ontology of core data
mining entities,” Data Mining and Knowledge Discovery, vol. 28, no.
5-6, pp. 1222–1265, 2014.

[49] D. Esteves, A. Lawrynowicz, P. Panov, L. Soldatova, T. Soru, and
J. Vanschoren, “Ml schema core speciﬁcation,” Technical report,

W3C, October 2016. http://www. w3. org/2016/10/mls, Tech.
Rep., 2016.

[50] J.-S. V ¨ockler, G. Juve, E. Deelman, M. Rynge, and B. Berriman,
“Experiences using cloud computing for a scientiﬁc workﬂow
application,” in Proceedings of the 2nd international workshop on
Scientiﬁc cloud computing. ACM, 2011, pp. 15–24.

[51] A. Khan, X. Yan, S. Tao, and N. Anerousis, “Workload char-
acterization and prediction in the cloud: A multiple time se-
ries approach,” in Network Operations and Management Symposium
(NOMS), 2012 IEEE.

IEEE, 2012, pp. 1287–1294.

[52] S. Garc´ıa, J. Luengo, and F. Herrera, Data preprocessing in data

mining. Springer, 2015.

[53] A. Miles and S. Bechhofer, “Skos simple knowledge organization
system reference,” W3C recommendation, vol. 18, p. W3C, 2009.
[54] R. V. Guha, D. Brickley, and S. Macbeth, “Schema. org: evolution of
structured data on the web,” Communications of the ACM, vol. 59,
no. 2, pp. 44–51, 2016.

[55] J. G. De Gooijer and K. Kumar, “Some recent developments in non-
linear time series modelling, testing, and forecasting,” International
Journal of Forecasting, vol. 8, no. 2, pp. 135–156, 1992.

[56] R. J. Hyndman, Y. Khandakar et al., Automatic time series for fore-
casting: the forecast package for R. Monash University, Department
of Econometrics and Business Statistics, 2007, no. 6/07.

[57] G. Bontempi, S. B. Taieb, and Y.-A. Le Borgne, “Machine learning
strategies for time series forecasting,” in European Business Intelli-
gence Summer School. Springer, 2012, pp. 62–77.

[58] N. K. Ahmed, A. F. Atiya, N. E. Gayar, and H. El-Shishiny, “An
empirical comparison of machine learning models for time series
forecasting,” Econometric Reviews, vol. 29, no. 5-6, pp. 594–621,
2010.

[59] J. Serra and J. L. Arcos, “An empirical evaluation of similarity
measures for time series classiﬁcation,” Knowledge-Based Systems,
vol. 67, pp. 305–314, 2014.

[60] A. S. Weigend, Time series prediction:

forecasting the future and

understanding the past. Routledge, 2018.

[61] P. Montero, J. A. Vilar et al., “Tsclust: An r package for time series
clustering,” Journal of Statistical Software, vol. 62, no. 1, pp. 1–43,
2014.

[62] A. Mueen and E. Keogh, “Extracting optimal performance from
dynamic time warping,” in Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining.
ACM, 2016, pp. 2129–2130.

[63] Z. Chen, W. Zuo, Q. Hu, and L. Lin, “Kernel sparse representation
for time series classiﬁcation,” Information Sciences, vol. 292, pp. 15–
26, 2015.

[64] S. Aghabozorgi, A. S. Shirkhorshidi, and T. Y. Wah, “Time-series
clustering–a decade review,” Information Systems, vol. 53, pp. 16–
38, 2015.

[65] M. V. Shcherbakov, A. Brebels, N. L. Shcherbakova, A. P. Tyukov,
T. A. Janovsky, and V. A. Kamaev, “A survey of forecast error
measures,” World Applied Sciences Journal, vol. 24, no. 24, pp. 171–
176, 2013.

[66] M. S. Paolella, Linear Models and Time-Series Analysis: Regression,

ANOVA, ARMA and GARCH.

John Wiley & Sons, 2018.

[67] J. G. De Gooijer and R. J. Hyndman, “25 years of time series
forecasting,” International journal of forecasting, vol. 22, no. 3, pp.
443–473, 2006.

[68] M. Parra-Roy ´on, “tswf-schema,” https://github.com/DiCITS/

tswf-schema, 2020.

[69] M. Parra-Royon, G. Atemezing, and J. M. Benitez-Sanchez, “Se-
mantics of data mining services in cloud computing,” IEEE Trans-
actions on Services Computing, 2020.

[70] “DataMining-LD.” [Online]. Available: https://dicits.ugr.es/

linkeddata/dmservices/

[71] “DiCITS/tswf-marketplace,” Sep. 2020, original-date: 2020-09-
03T18:17:10Z. [Online]. Available: https://github.com/DiCITS/
tswf-marketplace

[72] M. Uschold, M. Gruninger et al., “Ontologies: Principles, methods
and applications,” TECHNICAL REPORT-UNIVERSITY OF ED-
INBURGH ARTIFICIAL INTELLIGENCE APPLICATIONS INSTI-
TUTE AIAI TR, 1996.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

Manuel Parra-Royon is Computer Engineer
(2012) from the University of Granada (Spain).
He received the Master’s Degree in Information
Science and Computer Engineering in 2015 from
the University of Granada, Spain. He is currently
a Ph.D. candidate in the Department of Com-
puter Science and Artiﬁcial Intelligence, Univer-
sity of Granada. His research interests include
cloud computing services, data mining, machine
leaning, time series and large-data processing
on distributed computing.

Francisco J. Bald ´an received an M.Sc. in Data
Science and Computer Engineering in 2015 from
the University of Granada, Spain. He is currently
a Ph.D. student in the Department of Computer
Science and Artiﬁcial Intelligence, University of
Granada, Spain. His research interests include
Time Series, Data Mining, Data Science, Big
Data, and Cloud Computing.

Ghislain Atemezing obtained a Ph.D. in com-
puter science from Telecom ParisTech (France)
and is a recognized expert
in ontology web
language, semantic technologies, and triple
store systems. He joined Mondeca (a Se-
mantic software company based in Paris) in
2015. Dr. Atemezing is the Director of Re-
search & Development at Mondeca. He is
also in charge of promoting, supporting and
curating the Linked Open Vocabularies initia-
tive (http://lov.okfn.org/dataset/lov), the reusable
linked vocabularies ecosystem shared by ontology experts around the
globe. Main work on real-world projects includes ontology modelling,
Semantic annotation and reasoning, RDF stores benchmarking, data
model validation and ﬁne-tuning.

Jose Manuel Benitez-Sanchez (M’98) received
the M.S. and Ph.D. degrees in Computer Sci-
ence both from the Universidad de Granada,
Spain. He is currently an Associate Professor
at the Department of Computer Science and
Artiﬁcial Intelligence, Universidad de Granada.
the Dis-
Dr. Benitez-Sanchez is the head of
tributed Computational
Intelligence and Time
Series (DiCITS) lab. He has been the leading
reasearcher in a number of intenational and na-
tional projects as well as numerous contracts
with companies. He has co-authored over 70 papers published in in-
ternational journals with contributions ranging from foundations to real
world problem solutions. His research interests include Cloud Comput-
ing, Big Data, Data Science, Computational Intelligence, Time Series,
Cybersecurity and Biometrics.

