Predicting Merge Conﬂicts in Collaborative
Software Development

Moein Owhadi-Kareshk
University of Alberta, AB, Canada
owhadika@ualberta.ca

Sarah Nadi
University of Alberta, AB, Canada
nadi@ualberta.ca

Julia Rubin
University of British Columbia, BC, Canada
mjulia@ece.ubc.ca

9
1
0
2

l
u
J

4
1

]
E
S
.
s
c
[

1
v
4
7
2
6
0
.
7
0
9
1
:
v
i
X
r
a

Abstract—Background. During collaborative software develop-
ment, developers often use branches to add features or ﬁx bugs.
When merging changes from two branches, conﬂicts may occur
if the changes are inconsistent. Developers need to resolve these
conﬂicts before completing the merge, which is an error-prone
and time-consuming process. Early detection of merge conﬂicts,
which warns developers about resolving conﬂicts before they
become large and complicated, is among the ways of dealing with
this problem. Existing techniques do this by continuously pulling
and merging all combinations of branches in the background
to notify developers as soon as a conﬂict occurs, which is
a computationally expensive process. One potential way for
reducing this cost is to use a machine-learning based conﬂict
predictor that ﬁlters out the merge scenarios that are not likely
to have conﬂicts, i.e. safe merge scenarios.

Aims. In this paper, we assess if conﬂict prediction is feasible.
Method. We design a classiﬁer for predicting merge conﬂicts,
based on 9 light-weight Git feature sets. To evaluate our predictor,
we perform a large-scale study on 267, 657 merge scenarios from
744 GitHub repositories in seven programming languages.

Results. Our results show that we achieve high f1-scores,
varying from 0.95 to 0.97 for different programming languages,
when predicting safe merge scenarios. The f1-score is between
0.57 and 0.68 for the conﬂicting merge scenarios.

Conclusions. Predicting merge conﬂicts is feasible in practice,
especially in the context of predicting safe merge scenarios as a
pre-ﬁltering step for speculative merging.

Index Terms—Conﬂict Prediction, Git, Software Merging

I. INTRODUCTION

Modern software systems are commonly built by a large,
distributed teams of developers. Thus, improving the collab-
orative software development experience is important. Dis-
tributed Version Control Systems (VCSs), such as Git, and
social coding platforms, such as GitHub, have made such
collaborative software development easier. However, despite
its advantages, collaborative software development also gives
rise to several issues [1], [2], including merging and integration
problems [3], [4].

When two developers change the same part of the code
simultaneously, Git cannot decide which change to choose and
reports textual conﬂicts. In this situation, the developers need
to resolve the conﬂict manually, which is an error-prone and
time-consuming task that wastes resources [5], [6].

Given the cost of merge conﬂicts and integration problems,
many research efforts have advocated earlier resolution of

978-1-7281-2968-6/19/$31.00 c(cid:13)2019 IEEE

conﬂicts [5], [7], [8]. Previous work has shown that lack of
awareness of changes being done by other developers can
cause conﬂicts [9], and since infrequent merging can decrease
awareness, it increases the chance of conﬂicts. To address
that, proactive merge-conﬂict detection warns developers about
possible conﬂicts before they actually attempt to merge, i.e.,
before they try to push their changes or pull new changes. With
proactive conﬂict detection, developers get warned early about
conﬂicts so they can resolve them soon instead of waiting till
later when they get large and complicated.

In the literature, proactive conﬂict detection is typically
based on speculative merging [8], [10]–[12], where all com-
binations of available branches are pulled and merged in the
background. While a single textual merge operation is cheap,
constantly pulling and merging a large number of branch
combinations can quickly get prohibitively expensive. One
opportunity we foresee for decreasing this cost is to avoid
performing speculative merging on safe merge scenarios that
are unlikely to have conﬂicts. To accomplish this, we can
leverage machine learning techniques to design a classiﬁer for
predicting merge conﬂicts. The question is whether such a
classiﬁer works well in practice.

To the best of our knowledge, there have been two attempts
at predicting merge conﬂicts in the past [13], [14]. The ﬁrst
study [13] looked for correlations between various features
and merge conﬂicts and found that none of the features have a
strong correlation with merge conﬂicts. The authors concluded
that merge conﬂict prediction may not be possible. However,
we argue that lacking correlation does not necessarily preclude
a successful classiﬁer, especially since the study did not con-
sider the fact that the frequency of conﬂicts is low in practice
and most of the standard form of statistics and machine
learning techniques cannot handle imbalanced data well. The
second study [14] investigates the relationship between two
types of code changes, edits to the same method and edits to
dependent methods, and merge conﬂicts. The authors report
recall of 82.67% and precision of 57.99% based on counting
how often a merge scenario that had the given change was
conﬂicting. This means that this second study does not build
a prediction model that is trained on one set of data and
evaluated on unseen merge scenarios.

Since neither of the above work built a prediction model
that is suitable for imbalanced data and has been tested on
unseen data, it is still not clear if predicting merge conﬂicts

 
 
 
 
 
 
is feasible in practice, especially while using features that
are not computationally expensive to extract. In this paper,
we investigate if merge conﬂicts can be predicted using Git
features, i.e. information that can be inexpensively extracted
via Git commands. Speciﬁcally, we focus on the following two
research questions:

• RQ1: Which characteristics of merge scenarios have more

impact on conﬂicts?

• RQ2: Are merge conﬂicts predictable using only git

features?

To answer these questions, we study 744 well-engineered
repositories that are listed in the reaper dataset [15], and
that are written in 7 different programming languages (C,
C++, C#, Java, PHP, Python, and Ruby). We collect 267, 657
merge scenarios from these repositories and design a separate
classiﬁer for the repositories in each programming language.
To design our classiﬁers, we use a total of nine feature
sets that can be extracted solely through Git commands. We
intentionally use only features that can be extracted from
version control history so that our prediction process can
be efﬁcient (e.g., as opposed to features that may require
code analysis). Furthermore, we use Decision Tree [16] and
ensemble machine learning techniques, speciﬁcally Random
Forest [17], to take into account the speciﬁc characteristics of
merge data, such as being imbalanced, in our classiﬁers. To the
best of our knowledge, this work presents the largest merge-
conﬂict prediction study to date. We have almost 13 times the
number of merge scenarios used by recent work [13], and our
work is the ﬁrst that is evaluated with repositories written in
several programming languages. We publish all our code and
evaluation data in an online artifact page [18].

Despite conﬁrming the lack of signiﬁcant correlation found
by previous work [13], our prediction results show that lack
of strong correlation does not necessarily mean that a machine
learning classiﬁer would perform poorly. Our results show that
a Random Forest classiﬁer using all our feature sets predicts
conﬂicting merge scenarios with a precision of 0.48 to 0.63
and a recall from 0.68 to 0.83 across the different program-
ming languages. These numbers show that the predictors are
capable of identifying conﬂicting merge scenarios, but that
their performance may not be that reliable in practice. On
the other hand, our results show that the same classiﬁers can
identify safe merges (i.e., those without a conﬂict) extremely
well: a precision of 0.97 to 0.98, and recall between 0.93 to
0.96 across the different programming languages. The above
results mean that while the classiﬁers may not be able to
precisely predict conﬂicts, they can predict non-conﬂicting
scenarios with high accuracy. This is good news for speculative
merging, because when the predictor marks a merge scenario
as safe, speculative merging can conﬁdently avoid performing
the merge in the background for this scenario. This reduces
the number of merges that need to be done in the background,
which reduces some of the computational costs involved.
To summarize, the contributions of this paper are:

• We perform the largest merge-conﬂict prediction study to

date, using 267, 657 merge scenarios extracted from 744
GitHub well-engineered repositories written in different
programming languages.

• We create a set of potential predictive features for merge
conﬂicts based on the literature on software merging.
• We apply systematic statistical machine learning strate-
gies for handling the imbalance in software merging data.
• We design effective machine learning classiﬁers for tex-
tual conﬂicts in seven programming languages. Our clas-
siﬁers can be used as a pre-ﬁltering step in the context
of speculative merging.

II. RELATED WORK

In this section, we discuss the previous software merging
literature that is most related to our work. We ﬁrst explain
different merging techniques and then describe the work that
applies and analyzes these techniques in practice. Finally, we
discuss proactive conﬂict detection, since it can potentially
beneﬁt from effective merge-conﬂict predictors.

A. Merging Methods

We ﬁrst provide a brief summary of existing merging
techniques. For a more comprehensive classiﬁcation, we refer
the reader to the Mens’ survey on software merging [19].

Git is one of the most popular VCSs [20], and GitHub
is a social coding platform that hosts git-based projects.
Git uses line-based unstructured merging, triggered through
git merge, which is the most basic and popular merging
technique [21] [22]. Git is an unstructured merging tool that is
language-independent and can be employed for merging large
repositories containing a variety of text ﬁles such as code,
documentation, conﬁguration, etc. In other words, it does not
consider the structure of the code (or any underlying tracked
ﬁle); when the same text in a ﬁle has been simultaneously
edited, Git reports a textual conﬂict.

On the other hand, structured merge tools [23] [24], e.g.,
FSTMerge [25], leverage information about the underlying
code structure through analyzing the corresponding Abstract
Syntax Tree (AST). Since differencing a complete AST is
expensive, semi-structured merge tools, such as JDime [26],
improve performance by producing a partial AST that expands
only until the method level, with complete method bodies in
the leaves. Structured merge is then used for the main nodes
of the tree, while unstructured merge is used for the method
bodies in the leaves.

In this paper, we focus on textual conﬂicts as reported
by Git, since these are the most common types of conﬂicts
developers face in their typical work ﬂow. Note that when
describing our work after Section II, we often use the only
the term conﬂict for brevity.

B. Empirical Studies on Software Merging

Previous studies compared the above merge techniques in
practice in terms of speed, quality of resolutions, and the
complexity of reported conﬂicts. For example, Cavalcanti et
al. [27] focused on unstructured and semi-structured merge

tools and found that using semi-structured merge signiﬁcantly
reduces the number of conﬂicts. The authors also found that
the output of semi-structured merge is easier to understand and
resolve. In a follow-up work, Accioly et al. [4] investigated
the structure of code changes that
lead to conﬂicts with
semi-structured tools. The study showed that in most of the
conﬂicting merge scenarios, more than two developers are
involved. Moreover, this study showed that code cloning can
be a root cause of conﬂicts. While semi-structured merge is
faster than structured merge and more precise than unstruc-
tured merge, it is still not used in software industry due to
the effort that is needed in order to support new programming
languages. A recent large-scale empirical study by Ghiotto
et al. [28] also investigated various characteristics of textual
merge conﬂicts, such as their size and resolution types. The
results suggests that since merge conﬂicts vary greatly in terms
of their complexity and resolutions, having an automatic tool
that can resolve all types of conﬂicts is likely not feasible.

One approach for reducing the resolution time is selecting
the right developer to perform the merging based on their
previous performance and changes [29]. Other work looked
at speciﬁc types of changes that may affect merge conﬂicts.
For example, Dig et al. [30] introduced a refactoring-aware
merging technique that can resolve conﬂicts in the presence
of refactorings. A recent study also shows that 22% of the
analyzed Git conﬂicts involved refactoring operations in the
conﬂicting code [31].

C. Proactive Conﬂict Detection

There are several approaches to increase the awareness of
developers by detecting conﬂicts early. Awareness of changes
other team members may be making is a key factor in team
productivity and reduces the number of conﬂicts [9]. Syde [32]
is a tool for increasing awareness through sharing the code
changes present in other developers’ workspaces. Similarly,
Palantir [5] visually illustrates code changes and helps de-
velopers avoid conﬂicts by making them aware of changes
in private workspaces. Crystal [7] is a visual tool that uses
speculative analysis to help developers detect, manage, and
prevent various types of conﬂicts. Cassandra [12] is another
to minimize conﬂicts by optimizing task scheduling,
tool
with the goal of minimizing simultaneous edits to the same
ﬁles. MergeHelper [33] helps developers ﬁnd the root cause
of merge conﬂicts by providing them with the historic edit
operations that affected a given class member.

Guimar˜aes et al. [8] propose to continuously merge, com-
pile, and test committed and uncommitted changes to detect
conﬂicts as early as possible. However, such an approach is
likely expensive given the large number of combinations of
branches and developer changes in large projects.

Accioly et al. [14] investigate whether the occurrence of
events such as edits to the same method and edits to directly
dependent methods can be used to predict conﬂicts. However,
they do not actually build a prediction model. Instead, they
count the number of times each of the above features exists
when a conﬂict occurs versus when the merge is successful.

Based on such counts, their results show a precision of 57.99%
and a recall of 82.67%.

Leßenich et al. [13] investigate the correlation between var-
ious code and Git features and the likelihood of conﬂicts. To
create a list of features they investigated, they ﬁrst surveyed 41
developers. The developers mentioned seven features that can
potentially cause conﬂicts. However, after analyzing 21,488
merge scenarios in 163 Java repositories, the authors could not
ﬁnd a correlation between these features and the likelihood of
conﬂicts. We speculate that one reason for not capturing such
relationships is using stepwise-regression which may not be
an effective model for non-linear data, such as that collected
from merge scenarios.

In this paper, we investigate merge-conﬂict prediction by
creating a list of nine feature sets that can potentially impact
conﬂicts. Our list is based on previous work in the areas of
software merging and code review [5], [9], [13], [34], [35]. Our
work is different from all the above in that we use statistical
machine learning to create a classiﬁer, for each programming
language, that can predict conﬂicts in unseen merge scenarios.

III. BUILDING A MERGE-CONFLICT CLASSIFIER

Given a merge scenario based on two branches, our goal
is to predict whether a merge conﬂict will occur. In this
section, we describe how we prepare the data that is needed
for predicting merge conﬂicts, as well as how we train a
classiﬁer. Figure 1 shows an overview of our methodology,
which consists of three stages, as follows:

1) Collecting Merge Scenarios (Section III-A) As a ﬁrst
step, we need to collect merge scenarios. We do so by
mining the Git history of the target repositories.

2) Feature Extraction (Section III-B): In the second stage,
we extract the features that we will later use to build
the prediction model. Using the Git history, we extract
features from both branches being merged.

3) Prediction (Section III-C): In the last stage, we use
statistical machine learning techniques to build a predic-
tion model. Speciﬁcally, we use a binary classiﬁer that
aims to separate conﬂicting and safe merge scenarios.
Since conﬂicts happen in only a few numbers of merge
scenarios, the classiﬁer should be capable of handling
imbalanced data.

A. Collecting Merge Scenarios

In order to train a classiﬁer, we need a large set of labeled
merge scenarios. Fortunately, merge scenarios can be identiﬁed
from a repository’s Git history. However, unfortunately, not all
information about a merge scenario (e.g., whether there was a
conﬂict or not) is available in Git’s data. Therefore, to identify
merge scenarios and determine whether the merge resulted in
a conﬂict, we use a replaying method where we re-perform
the merge at that point of history and record the outcome.

The input for this stage is a list of Git repositories to
be analyzed. After cloning all repositories, we use MER-
GANSER, an open-source toolchain we developed for ex-
tracting merge-scenario data [36], to analyze their histories.

In Table I, we categorize the identiﬁed features into 9 feature
sets, along with the intuition behind them, as well as any
relevant related work that previously used this feature set
or a variation of it. The last column in the table shows the
dimension of each feature set (i.e., the number of individual
values, each corresponding to a feature, used as input to the
model) for the prediction task. The dimension of some of these
feature sets is one, which means that they are just a scalar
value. Some other feature sets have a dimension greater than
one in order to represent all the needed information; such
feature sets would be represented as a vector. For example,
feature set #4 is inspired from previous merging and code
review studies [34], [35] and indicates code churn. We include
this feature set since more code changes may increase the
chance of conﬂicts. It needs 5 values to represent number of
added, deleted, modiﬁed, copied, and renamed ﬁles. Feature
sets #4, #5, #7, and #8 are vectors with size 5, 2, 12, and 4,
respectively, and the other feature sets are scalars. In the end,
each merge scenario is represented by a total of 28 features.
We do not rely on language-speciﬁc features; all of our feature
sets are language-agnostic.

The feature sets shown in Table I are on different granularity
levels. Feature set #1, No. of simultaneously changed ﬁles, is
a merge-level feature set, which means that this feature set is
extracted once for a given merge scenario. All the other feature
sets are branch-level, which means that these feature sets are
extracted from each branch separately. Each feature set should
have a single value for each merge scenario. This means that
we need to combine the two values of branch-level feature
sets somehow. Since the choice of the combination operator
may impact the performance of the classiﬁer, we empirically
determine the best combination operator to use, as we describe
in Section IV.

For all the feature sets listed in the table, we use the git
log command, with different parameters depending on the
feature set, to extract their values. Our artifact page contains
the exact git log commands we use.

C. Prediction

The aim of the prediction phase is to train a binary classiﬁer
that is capable of predicting whether a merge scenario is safe
or conﬂicting after learning from the development history of a
different set of merge scenarios. Merge conﬂict data gathered
from Git history is highly imbalanced; speciﬁcally, the number
of merge scenarios without conﬂicts is much higher than merge
scenarios with conﬂicts. Imbalanced data prevents the standard
variation of most classiﬁcation methods from working well
for the minor class (i.e., the class with fewer data points).
There are several techniques in the ﬁeld of machine learning
that have been designed to overcome this problem [38],
including data resampling, tree-based models, an ensemble
learning approach, or alternative cost functions, Resampling
methods, such as Synthetic Minority Over-sampling Technique
(SMOTE) [39], are computationally expensive which is why
we opt for ensemble learning techniques that combine mul-
tiple simpler classiﬁcation models, allowing them to handle

Fig. 1. Methodology for Creating the Proposed Conﬂict Predictor

MERGANSER considers only 3-way merge scenarios and
ignores n-way merges, which are called octopus merges in
Git. In 3-way merge scenarios, the Merge Commit has two
parents (Parent #1 and Parent #2) and these parents have
a Common Ancestor. MERGANSER, thus, identiﬁes target
merge scenarios by looking for commits with two parents. It
then replays all identiﬁed 3-way merge scenarios by checking
out Parent #1 and then using the git merge command
to merge Parent #2’s changes. We use Git merge’s default
options, which uses the recursive merge strategy [37]. To
detect conﬂicts after replaying, our toolchain searches for the
phrase Automatic merge failed; fix conflicts
and then commit the result in the output of the
git merge command. In our experiments, we use open-
source GitHub repositories described in Section IV.

B. Feature Extraction

To train a classiﬁer, we need to extract potentially predictive
features from merge scenarios. Our goal is to use features
whose extraction is computationally inexpensive such that the
prediction can be used in practice. We identify these features
by surveying the literature on merge conﬂicts and related areas,
such as code evolution or software maintenance.

PredictionSection	III-C	Collecting	Merge	ScenariosSection	III-ALocalServersAncestorParent	#2Parent	#1MergedCode......Feature	ExtractionSection	III-BMerge	ScenariosGit	RepositoriesGit metadata FeaturesFeaturesReal-valued	VectorPredictorClassiﬁerConflictNot	ConflictTABLE I
FEATURE SETS USED FOR TRAINING THE MERGE CONFLICT PREDICTOR

[13], [34], [35] More code changes may increase the chance of con-

No.
1

Feature Set
No. of simultaneously changed ﬁles in two branches

References
[5], [13]

2

3

4

5

6

7

8

9

No. of commits between the ancestor and the last
commit in a branch
Commit density: No. of commits in the last week of
development of a branch
No. added, deleted, renamed, modiﬁed, and copied
ﬁles in a branch
No. added and deleted lines in a branch

[13], [34], [35]

[13]

[13], [35]

No. of active developers in a branch

[9], [34], [35]

The frequency of predeﬁned keywords in the commit
messages in a branch. We use 12 key-words: ﬁx,
bug, feature, improve, document, refactor, update, add,
remove, use, delete, and change.
The minimum, maximum, average, and median length
of commit messages in the branch
Duration of the developement of the branch in hours

[35]

[35]

[9]

Total number of features:

imbalanced data. Speciﬁcally, we use Random Forest, which
is an ensemble tree-based classiﬁcation method. We train a
separate classiﬁer for each programming language.

IV. DATA COLLECTION PROCESS

The ﬁrst step for applying our methodology from Section III
is to choose the target repositories to be analyzed. Since the
selected data may greatly impact our results, we dedicate this
section to describe the data collection process in detail. We
then present the speciﬁc methods used to answer each RQ in
Sections V and VI.

We focus on open-source repositories in this work. We, thus,
need to ensure that the selected repositories are of high quality
and reﬂect real-world development practices. As a proxy for
quality, we look for well-engineered repositories (i.e., real-
world engineered software projects [15]) that are also popular.
Speciﬁcally, we use the following criteria:

• Popularity: Intuitively, more active and useful reposito-
ries attract more attention, reﬂected in the number of
stars, issues, forks, etc. Similar to previous studies [4],
[14], we use the number of stars as a ﬁltering criterion.
• Quality: Even though the number of stars represents
some measure of quality, not all popular repositories
are suitable for our study. For instance,
there are a
number of repositories that only consist of code examples
and interview questions that are highly starred but are
not suitable for studying merge conﬂicts since they do
not represent a collaborative effort to build a software
system. Hence, we apply further quality measures for our
repository selection. We use reaper [15] to detect well-
engineered software repositories and avoid analyzing per-
sonal or toy repositories. Reaper uses various repository
characteristics such as community support, continuous in-
tegration, documentation, history, issues, license, and unit

Intuition for Including this Feature Set
The increase in simultaneously changed ﬁles increases
the chance of conﬂicts. If the value of this feature is
zero, no conﬂict can occur.
Having more commits means more changes in a
branch, which may increase conﬂicts
Lots of recent activity may increase the chance of
conﬂicting changes

ﬂicts
More code changes may increase the chance of con-
ﬂicts
Having more developers increases the chances of get-
ting inconsistent changes
These keywords can provide a high-level overview of
the types of code changes and their purpose. Certain
types of changes may be more prone to conﬂicts.

The length of a commit message can be an indicator
of its quality
The longer a branch exists for, the more likely it is
for inconsistent changes to happen in one of the other
branches

Dimension
1

1

1

5

2

1

12

4

1

28

testing to classify well-engineered software repositories
using a random forest classier. We use reaper’s released
dataset [40] (downloaded on September 15, 2018) and
select all repositories in that list that have been classiﬁed
as well-engineered repositories.

• Programming Language: We choose all seven program-
ming languages that the reaper dataset supports: C, C++,
C#, Java, PHP, Python, and Ruby.

Considering the three criteria mentioned above, we sort the
well-engineered repositories in each programming language
separately based on the number of stars. We then select the
top 150 repositories from each language, for a total of 1, 050
repositories as the initial list. For practical limitations with
respect to computational resources for replaying thousands
of merge scenarios from that many repositories, we only
consider repositories whose size is less than 1 GB and focus
on the latest 1, 000 merge scenarios in each repository. We
focus on active repositories and therefore eliminate any moved
or archived repositories from that initial list. Moreover, to
avoid analyzing the same merge scenario multiple times, we
only analyze the main repositories and eliminate the forked
versions. After these eliminations, we are left with a total of
744 repositories that we use for our study. The list of the
selected repositories we use in our experiments is available
on our artifact page.

After choosing the target repositories, we analyze their latest
1, 000 merge scenarios. We collect 267, 657 merge scenarios in
total. Figure 2 illustrates the distribution of merge scenarios for
repositories in different programming languages. While most
of the Java repositories have less than 200 merge scenarios,
the distribution of merges in languages such as C++, PHP, and
Python is close to uniform. It means that in these programming
languages, we can see the repositories with a different number
of merges, from zero to 1, 000 (our pre-deﬁned threshold)

Fig. 2. The Distribution of Merge Scenarios

Fig. 3. The Distribution of Conﬂicting Merge Scenarios

with relatively the same chance. In Figure 3, we show the
distribution of conﬂicting merge scenarios in the same way.
While the range of the conﬂicting merge scenarios is different
across the languages, maximum of 150 in C to more than 400
in Java, the shape of their distribution is the same and the
median is less than 50.

We show the number of repositories, merge scenarios,
conﬂicting merge scenarios, and the conﬂict rate of each
programming language in detail in Table II. Out of 267, 657
scenarios, 21, 734 have at least one conﬂict in their textual
ﬁles, such as code ﬁles or documentation ﬁles. In our data,
the conﬂict rate across the different programming languages is
8.12%. In such imbalanced data, we need to select and train the
proper prediction models to make sure that our classiﬁers can
perform well for correctly predicting both safe and conﬂicting
merge scenarios.

V. RQ1: WHICH CHARACTERISTICS OF MERGE SCENARIOS
HAVE MORE IMPACT ON CONFLICTS?

In RQ1, we are interested in identifying which feature sets
are more important for predicting conﬂicts. We ﬁrst describe
the analysis methods we use, given the data collected in
Section IV and then present the results.

A. Method

To answer RQ1, we analyze the 9 feature sets in Table I to
see which of them are more important. We analyze importance
in two ways. (1) We calculate Spearman’s rank-order corre-
lation [41], which is a non-parametric measure, between the
feature sets and the existence of conﬂicts. This is the same
correlation method used in previous work to determine the
effectiveness of various features for predicting conﬂicts [13].
(2) We use decision trees to analyze the importance of each
feature set, since the results of decision trees are easier to
interpret than other classiﬁers. A decision tree aims to ﬁnd a
single feature set in each level based on which it can classify
the data in the most optimized way. For feature sets that
have more than one feature, we calculate the average of the
importance of their individual features.

B. Results

1) Correlation-based analysis: We ﬁrst analyze the Spear-
man’s rank-order correlation between the feature sets and
merge conﬂicts, as shown in Table III. We calculate the
correlation and the corresponding p-value for each feature
set separately. The p-values of Feature Sets #7 to #9, for all
languages, are greater than 0.05 showing that there is no signif-
icant correlation between the conﬂicts and these feature sets.
Following previous work [13] and statistics guidelines [42],
we consider correlation coefﬁcients >= 0.6 as strong (and
them in the table), 0.4 − 0.59 as medium, and
highlight
0.2 − 0.39 as weak. We only consider statistically signiﬁcant
correlations whose p-value < 0.05.

The ﬁrst feature set has the highest correlation, between
0.52 to 0.60, for the different programming languages. How-
ever, this correlation is strong only in the case of Java and PHP.
Given that the higher the number of simultaneously edited
ﬁles, the higher the chances of conﬂicts, the high correlation
matches our intuition. The second feature set, which is the
number of commits that happened in each branch since they
diverged, has a weak correlation but this correlation is also
much higher than the remaining feature sets. This means
least a
that
weak correlation with merge conﬂicts. The other features are
have extremely low correlation coefﬁcients (< 0.2) or are
insigniﬁcant (p-value >= 0.05).

there are only two feature sets that have at

While we do not use the same exact features from the
previous work by Leßenich et al. [13], we can conﬁrm their
ﬁndings in terms of lacking correlation between Git features
of merge scenarios and conﬂicts. This gives us conﬁdence
the lack of correlations we ﬁnd for most features is
that
correct. However, we argue that this lack of correlation does
not necessarily mean merge conﬂicts are not predictable, as
we show later in the results of RQ2.

Although we report the correlation and importance of fea-
ture sets for different programming languages separately, it
is important to note that we do not expect to see signiﬁcant
differences between the feature sets in different programming

CC#C++JavaPHPPythonRubyProgramming Languages02004006008001000No. Merge ScenariosCC#C++JavaPHPPythonRubyProgramming Languages050100150200250300350400No. Conflicting Merge ScenariosTABLE II
DESCRIPTIVE STATISTICS OF OUR DATASSET

Programming Languages
C
C++
C#
Java
PHP
Python
Ruby
Sum
Weighted Average

# Repositories
80
109
110
120
112
106
106
744
-

# Merges
18,824
42,420
38,945
36,853
50,342
49,583
40,690
267,657
-

# Conﬂicting Merges
1,308
3,621
3,153
2,190
4,737
5,533
3,192
21,734
-

Conﬂict Rate (%)
6.95
8.54
8.1
5.94
9.41
11.16
7.84
-
8.12

languages since our feature sets are language-agnostic. Our
results in Table III conﬁrm that.

VI. RQ2: ARE MERGE CONFLICTS PREDICTABLE USING
ONLY GIT FEATURES?

2) Decision-tree based analysis: As a different way of
measuring feature importance, we use decision trees to deter-
mine the importance of our feature set for predicting conﬂicts
in each programming language. The results are shown in
Table IV, where the feature importance is a value between
0 and 1. Again, we ﬁnd that the number of simultaneously
changed ﬁles (Feature Set #1) is the most important feature
by far. The high impact of the number of simultaneously
changed ﬁles can be intuitively explained since more in-
parallel changes increase the likelihood of conﬂicts, and the
chance of conﬂicts is zero if there are no simultaneously
changed ﬁles. The number of commits (Feature Set #2), edited
ﬁles (Feature Set #4), and active developers (Feature Set #6)
are also slightly more important than the other ones. However,
apart from Feature Set #1, all features seem to have very
low importance for the classiﬁer. Similar to the correlation-
based analysis, we ﬁnd that the importance of feature sets is
relatively similar for all programming languages.

Our results suggest that commit message information (Fea-
important for detecting con-
ture Sets #7 and #8) is not
ﬂicts. Since commit messages contain information about the
evolution of a repository (e.g., indications of types of code
changes), we intuitively thought that they may have an impact
the feature sets related to commit
on conﬂicts. However,
messages we currently extract are intentionally lightweight to
keep execution time low. It is, therefore, hard to conclude
if commit messages are indeed altogether useless in this
context or if different types of feature sets (e.g., taking word
embedding of the commit messages into account) may lead to
more meaningful relationships. This ﬁnding is important since
unlike the other numerical features, analyzing the commit
messages is computationally expensive due to text processing.

The No. of simultaneously changed ﬁles is the most im-
portant feature for predicting merge conﬂicts. The No. of
commits in each branch shows a weak correlation, but a
much lower importance level by the decision tree. Remaining
feature sets show low correlation coefﬁcients and importance.

In RQ2, our goal is to determine if merge conﬂicts can be
predicted using our selected feature sets. We ﬁrst describe the
classiﬁers we compare and then present our results.

A. Method

We compare the performance of the decision tree and
random forest classiﬁers we built using all 9 collected feature
sets. Additionally, we also create two baselines to compare our
results against. The following summarizes the four classiﬁers
we compare.

• Decision Tree: A Decision Tree classiﬁer is one of the
simplest options for a binary classiﬁcation task that is
also robust to imbalanced data. We train a decision tree
with all of our 9 feature sets. Note that this is the same
classiﬁer used to determine importance in RQ1.

• Random Forest: To investigate if a more sophisticated
classiﬁer can make more use of the available features,
we use Random Forest.

• Baseline #1: The ﬁrst baseline we compare to is a
“dummy” classiﬁer that randomly labels the data. If the
data was balanced, the f 1-score of both classes would be
around 0.5, i.e. random guess. However, since the data is
not balanced, the f 1-score of this classiﬁer would be the
same as the imbalance rate in the data for the conﬂicting
class, i.e., 0.0812, and would be 0.9188 for the safe class.
We expect that any other predictor should be better than
this basic baseline in order to be useful in practice.

• Baseline #2: Recall that in the results of RQ1 (Sec-
tion V-B), we found that Feature Set #1, which is the
number of simultaneously changed ﬁles in two branches,
is the most important feature for the decision tree classi-
ﬁer. Therefore, as our second baseline, we use a decision
tree classiﬁer that uses only Feature Set #1 from Table I.
The goal of this baseline is to have a low-cost classiﬁer
that relies only on the most important feature. That way,
we can determine if having the other features improves
things, or is simply an added cost with no beneﬁt.
a) Hyper-parameters: The main hyper-parameters of de-
cision trees and random forests classiﬁers are (1) the minimum
samples in leaves, (2) the minimum sample split, (3) the max-
imum depth, and (4) the total number of estimators (just for

TABLE III
SPEARMAN’S RANK-ORDER CORRELATION COEFFICIENTS (CC) AND THE CORRESPONDING P -VALUES (P) BETWEEN EACH FEATURE SET AND THE
EXISTENCE OF CONFLICTS, SEPARATED BY LANGUAGE. THE CORRELATIONS THAT ARE EQUAL OR GREATER THAN 0.6 WITH P -VALUES (P) LESS THAN
0.05 ARE HIGHLIGHTED.

Feature
Set

1
2
3
4
5
6
7
8
9

C

C++

C#

Java

PHP

Python

Ruby

CC
0.53
0.30
-0.17
-0.15
-0.14
-0.02
0.01
0.0
0.0

p
0.0
0.0
0.0
0.0
0.0
0.01
0.13
0.69
0.98

CC
0.58
0.33
-0.17
-0.14
-0.13
0.02
0.0
0.0
0.0

p
0.0
0.0
0.0
0.0
0.0
0.0
0.95
0.98
0.99

CC
0.52
0.28
-0.07
-0.07
-0.07
0.03
0.01
0.01
0.0

p
0.0
0.0
0.0
0.0
0.0
0.0
0.25
0.32
0.55

CC
0.60
0.33
-0.14
-0.13
-0.14
0.03
0.0
0.01
0.0

p
0.0
0.0
0.0
0.0
0.0
0.0
0.57
0.25
0.76

CC
0.60
0.35
-0.19
-0.18
-0.19
0.05
0.01
0.0
0.0

p
0.0
0.0
0.0
0.0
0.0
0.0
0.22
0.75
0.67

CC
0.53
0.29
-0.08
-0.08
-0.08
0.0
0.0
0.01
0.01

p
0.0
0.0
0.0
0.0
0.0
0.89
0.58
0.24
0.11

CC
0.58
0.32
-0.01
-0.01
-0.10
0.01
0.01
0.0
0.0

p
0.0
0.0
0.0
0.0
0.0
0.21
0.22
0.75
0.65

TABLE IV
FEATURE IMPORTANCE BASED ON A DECISION TREE CLASSIFIER

Feature Set
1
2
3
4
5
6
7
8
9

C
0.93
0.01
0.01
0.01
0.01
0.01
0.01
0.01
0.01

C++
0.98
0.01
0.0
0.0
0.0
0.01
0.0
0.0
0.0

C#
0.95
0.0
0.01
0.01
0.01
0.01
0.01
0.01
0.0

Java
0.93
0.02
0.01
0.02
0.01
0.01
0.0
0.01
0.01

PHP
0.95
0.01
0.0
0.01
0.01
0.01
0.01
0.01
0.0

Python
0.94
0.01
0.01
0.01
0.01
0.02
0.01
0.0
0.0

Ruby
0.94
0.01
0.0
0.01
0.0
0.02
0.01
0.01
0.01

random forest). Determining the proper value for the number
of estimators is important since a low number of weak clas-
siﬁers may result in underﬁtting, while an unnecessarily high
number may result in overﬁtting. The other hyper-parameters
also balance the complexity of the models. Due to the im-
portance of these hyper-parameters, we use grid-search with
10-fold cross-validation to ﬁnd the right hyper-parameters to
use. The candidate values for each of these hyper-parameters
are selected based on the typical values explored for this
size of data. We use {2, 5, 10, 20, 35, 50} as the choices for
the minimum samples in leaves, {2, 3, 5, 10, 20, 35, 50, 75}
for minimum sample split, {1, 3, 5, 7, 11} for the maximum
depth, and {1, 3, 10, 50, 75, 100, 200, 300} as the choices for
the number of estimators. Our results show that the best hyper-
parameter values for the minimum samples in leaves is 10, for
minimum sample split is 5, for the maximum depth is 7, and
for the number of estimators is 75.

b) Combination operators: Recall from Section III that
since some of our feature sets are extracted for each branch,
we need to use a combination operator to combine them into a
single value for the whole merge scenario. To ﬁnd the suitable
combination operator to use, we train our predictors based
on each of seven common combination operators: Minimum,
Maximum, Average, Median, Norm- 1, Norm-2, and Concate-
nation operators. We then use grid search with 10-fold cross-
validation on all data points to determine the best combination
operator. We ﬁnd that Norm-1 is the best combination operator
for all seven programming languages.

c) Performance measures: It is important to note that
accuracy is not a good performance measure for imbalanced
data since the potential inﬂuence of misclassiﬁcation of con-
ﬂicting merges would be much lower than safe merges. For
example, imagine that there are 100 merge scenarios with 20
of them having conﬂicts and 80 without conﬂicts. A naive
classiﬁer that simply classiﬁes everything as not conﬂicting
would achieve a misleading accuracy of 80%. Hence, instead
of accuracy, we use precision, recall, and f 1-score to evaluate
the performance of the classiﬁers.

However, reporting the recall and precision only for the
conﬂicting class gives a partial view of how a detector would
perform in practice. Given that safe merge scenarios occur
much more often than conﬂicting ones, we also need to make
sure that we have good recall and precision for the safe class.
Therefore, we report all our performance measures for both
the conﬂicting (C) and safe (S) classes as follows.

For a given merge scenario, any of the binary classiﬁers we
compare predicts either a conﬂict or a not conﬂict (i.e., a safe
merge). After running a given classiﬁer on all our evaluation
data using 10-fold cross-validation, we consider each class
separately as the target class and calculate precision, recall,
and f 1-score according to the following deﬁnitions:

• True Positive (TP): The target class is labeled correctly.
• False Positive (FP): The non-target class is incorrectly

labeled as the target class.

• True Negative (TN): The non-target class is labeled

correctly.

• False Negative (FN): The target class is incorrectly

labeled as the non-target class.

The evaluation measures are:

precision =

T P
T P + F P

recall =

T P
T P + F N

f 1-score = 2 ∗

P recision ∗ Recall
P recision + Recall

(1)

(2)

(3)

For example, assume the ground truth is {C, S, S, C, S, S},
and a predictor labels the data as follows {S, S, C, C, S, C},

TABLE V
MERGE CONFLICT PREDICTION RESULTS. THE HIGHEST VALUES IN EACH CATEGORY ARE HIGHLIGHTED.

Programming
Language

C

C++

C#

Java

PHP

Python

Ruby

Classiﬁer

Baseline #2
Decision Tree
Random Forest
Baseline #2
Decision Tree
Random Forest
Baseline #2
Decision Tree
Random Forest
Baseline #2
Decision Tree
Random Forest
Baseline #2
Decision Tree
Random Forest
Baseline #2
Decision Tree
Random Forest
Baseline #2
Decision Tree
Random Forest

P recisionS
1.00
0.99
0.98
1.00
0.99
0.97
0.99
0.99
0.97
1.00
0.99
0.98
1.00
0.99
0.98
1.00
1.00
0.98
1.00
1.00
0.98

Safe (Not Conﬂicting)
RecallS
0.81
0.89
0.96
0.82
0.88
0.96
0.83
0.85
0.93
0.85
0.90
0.95
0.83
0.87
0.93
0.82
0.87
0.94
0.84
0.89
0.96

f 1-scoreS
0.90
0.94
0.97
0.90
0.93
0.97
0.90
0.92
0.95
0.92
0.94
0.97
0.91
0.93
0.95
0.90
0.93
0.96
0.91
0.94
0.97

P recisionC
0.28
0.37
0.56
0.34
0.41
0.63
0.32
0.35
0.48
0.36
0.44
0.58
0.38
0.44
0.54
0.29
0.36
0.49
0.33
0.41
0.59

Conﬂicting
RecallC
1.00
0.88
0.72
0.99
0.91
0.68
0.92
0.90
0.74
0.99
0.93
0.83
0.99
0.93
0.82
1.00
0.95
0.74
1.00
0.96
0.72

f 1-scoreC
0.44
0.52
0.63
0.51
0.57
0.66
0.48
0.51
0.57
0.53
0.60
0.68
0.55
0.59
0.65
0.45
0.52
0.59
0.50
0.57
0.65

then recallC = 1/2 = 0.5, precisionC = 1/3 = 0.33,
recallS = 2/4 = 0.5, precisionS = 2/3 = 0.67.

B. Results

Table V shows our results for RQ2. Note that we do not
show the results of Baseline #1 since it can be calculated based
on the bias in the data and serves as a minimum threshold that
any useful predictor needs to achieve.

1) Decision Trees vs. Baseline #2: We ﬁrst compare Base-
line #2, which is a simple decision tree that uses the most
important feature determined in Section V-B, to the Decision
Tree classiﬁer that uses all features. Table V shows that the
Decision Tree classiﬁer that uses all features achieves a higher
f 1-score for both classes when compared to Baseline #2. This
suggests that despite Feature set #1 being the most important
feature, adding the other features to the classiﬁer does improve
the results.

Additionally, both the Decision Tree classiﬁer and Base-
line #2 exceed the performance of Baseline #1, which is a
“dummy” classiﬁer that randomly labels the data by consider-
ing the imbalance rate. This shows that there is gained value
in designing a “real” classiﬁer.

2) Decision Trees vs. Random Forest: Given that the Deci-
sion Tree classiﬁer with all features outperforms Baseline #2,
we now compare the Decision Tree classiﬁer to Random Forest
to determine if a more sophisticated classiﬁer can achieve
better results. The results in Table V show that the Random
Forest classiﬁer achieves the highest f 1-score for both safe
and conﬂicting merges. This shows that using all features
along with a more advanced ensemble machine learning clas-
siﬁer does indeed achieve better results. Another observation
is that all the classiﬁers seem to perform consistently across
the different programming languages.

3) Conﬂicting class: We now focus on the Random Forest
classiﬁer and discuss the results for the conﬂicting class in
more detail. The table shows that recallC ranges from 0.68
to 0.83 for the different programming languages. This means
that the predictor can correctly identify most of the conﬂicting
merge scenarios. The table shows that precisionC is in a lower
range, varying from 0.48 to 0.63. Overall, the f 1-scoreC
ranges from 0.57 to 0.68 across the seven languages.

4) Safe class: In terms of not conﬂicting, or safe, merge
scenarios, Table V shows that Random Forest’s recallS is
between 0.93 to 0.96. This is a high recall rate and means
that the predictor is able to correctly identify most of the
automatically mergeable merge scenarios (i.e., those that will
not result in conﬂicts). The precision of this class is between
0.97 to 0.98 for different programming languages, meaning
that almost all of the merge scenarios that are predicted as safe
are actually safe. Overall, the f 1-scoreN C ranges between
0.95 to 0.97 for the different programming languages.

We ﬁnd that a Random Forest classiﬁer based on light-weight
Git features can successfully predict conﬂicts for different
programming languages. However, the f 1-score of the safe
class is much higher than the conﬂicting class.

We, ﬁnally, note that the average time for predicting the
status of a given merge scenario, including the feature ex-
traction process, is 0.1seconds±0.02seconds. This makes our
predictor fast enough to use in practice.

VII. IMPLICATIONS AND DISCUSSION

We now discuss what our prediction results may mean for

avoiding complex merge conﬂicts in practice.

The recall of merge conﬂicts is relatively high (0.68 to
0.83), which means that the classiﬁer can identify an accept-

able portion of conﬂicts, if it is used as a replacement of
speculative merging altogether. Notifying developers of these
potential conﬂicts would allow them to merge early and avoid
the conﬂict becoming more complex. The downside is that
the precision of predicting conﬂicts is lower (0.48 to 0.63),
which means that developers may perform a merge earlier
than needed (i.e., perform a merge when there is no conﬂict
to resolve). In practice, this may not be a big problem since
frequent merges are encouraged to avoid conﬂicts in the long
term.

However, instead of completely replacing speculative merg-
ing and running the risk of false positive notiﬁcations to
developers, we advocate for using a merge-conﬂict predictor
as a pre-ﬁltering step for speculative merging [7], [11] or
continuous merging [8] in developers’ work environments
(e.g., their IDE). Both recall and precision of our classiﬁer
for safe merges are considerably high (recall between 0.93 to
0.96 and precision in the range of 0.97 to 0.98). The precision
of safe merge scenarios in the context of pre-ﬁltering them out
from speculative merging is important, since we want to make
sure that eliminated merge scenarios are actually safe. Given
the high precision and the fact that conﬂict rates are typically
low (8.12%), this means that a subsequent proactive conﬂict
detection tool will accurately eliminate a large number of safe
merge scenarios from its analysis, thus potentially saving costs.

VIII. THREATS TO VALIDITY

We do not consider the chronological order or timeline of
commits in any way. In other words, we do not train our model
with a subset of merge scenarios and test them only with the
subsequent ones. While such time travel is often a threat in
prediction studies, we believe that the impact is low in our
context since most of the features we use in our prediction
model are not time sensitive. For example, the number of co-
modiﬁed ﬁles or the number of changed lines is not dependent
on the time in a project. In contrast, features such as the name
of the ﬁle or code component being modiﬁed (which we do
not use in our work) are time sensitive since they may change
signiﬁcantly over time in a project.

B. External Validity

While we have a large-scale empirical study, our evaluation
limited to 744 open-source repositories in GitHub
is still
in seven popular programming languages. Our results may
not address merge conﬂict prediction in other programming
languages. However, our work is, to the best of our knowledge,
the largest study for merge conﬂict prediction, to date, that also
studies multiple programming languages. While we need to
train a separate predictor for repositories in each programming
language, this does not have a negative impact on proactive
conﬂict detection in practice since the language of each
repository is known beforehand and the appropriate classiﬁer
can be used.

In this section, we discuss some of the potential threats to

IX. CONCLUSION

the validity of our study.

A. Internal Validity

git merge can use several merging algorithms, and the
choice of algorithm used may impact the results. We employ
the default one (recursive merging strategy) since developers
typically do not change the default conﬁguration of Git merge.
Rebasing is another strategy for integrating changes from
different branches. When git rebase is used instead of
git merge or when the --rebase option is used while
pulling, a linear history is created and no explicit merge
commits will exist. Therefore, there is a chance that we miss
some merge scenarios since we detect merge scenarios based
on the number of parents of a commit. Unfortunately, there
is no precise methodology to extract rebased merge scenarios
since there is no information in Git about them.

We eliminate n-way (octopus) merging and only focus on
3-way merging where each merge commit has exactly two
parents. This may eliminate some merge scenarios. However,
3-way merging happens more often in practice.

We use a set of candidate values for the hyper-parameters
we use for our classiﬁers and ﬁnd the best option by using
a grid search. We created these candidate values based on
our intuition and the heuristics in the literature about the
hyper-parameters for machine learning techniques. However,
we cannot guarantee that we found the globally optimal values
for our hyper-parameters.

In this paper, we investigated whether predicting merge
conﬂicts is feasible, with the long-term motivation of using
it in the context of proactive conﬂict detection. We extracted
267, 657 merge scenarios from 744 repositories, written in
seven programming languages, and used 28 light-weight fea-
tures from Git to design a classiﬁer for merge conﬂicts. We
compare a Random Forest classiﬁer to two variations of a
Decision Tree classiﬁer. While similar to previous work, we
could not ﬁnd a correlation between our feature sets and
conﬂicts, we were able to successfully design a classiﬁer for
merge conﬂicts. This shows that lack of correlation does not
necessarily mean that prediction is not possible. Our results
show that a Random Forest classiﬁer with all our selected
features outperformed the other classiﬁers we compared to.
Our high precision (0.97 to 0.98) for detecting safe merge
scenarios ensures that we can eliminate merge scenarios that
are labeled as safe from the speculative merging process.

As future work, we plan to investigate the characteristics of
conﬂicts in different domains to determine if the application
context can have any impact on merge conﬂicts. Moreover,
we want to integrate our conﬂict predictor with speculative
merging in developer’s IDEs.

X. ACKNOWLEDGEMENT

This project has been partially funded through a 2017

Samsung Global Research Outreach (GRO) program.

REFERENCES

[1] C. Bird, P. C. Rigby, E. T. Barr, D. J. Hamilton, D. M. German,
and P. Devanbu, “The promises and perils of mining git,” in Mining
Software Repositories, 2009. MSR’09. 6th IEEE International Working
Conference on.

IEEE, 2009, pp. 1–10.

[2] E. Kalliamvakou, G. Gousios, K. Blincoe, L. Singer, D. M. German, and
D. Damian, “The promises and perils of mining github,” in Proceedings
of the 11th working conference on mining software repositories. ACM,
2014, pp. 92–101.

[3] S. McKee, N. Nelson, A. Sarma, and D. Dig, “Software practitioner per-
spectives on merge conﬂicts and resolutions,” in Software Maintenance
and Evolution (ICSME), 2017 IEEE International Conference on. IEEE,
2017, pp. 467–478.

[4] P. Accioly, P. Borba, and G. Cavalcanti, “Understanding semi-structured
merge conﬂict characteristics in open-source java projects,” Empirical
Software Engineering, vol. 23, no. 4, pp. 2051–2085, 2018.

[5] A. Sarma, D. F. Redmiles, and A. Van Der Hoek, “Palantir: Early
detection of development conﬂicts arising from parallel code changes,”
IEEE Transactions on Software Engineering, vol. 38, no. 4, pp. 889–908,
2012.

[6] C. Bird and T. Zimmermann, “Assessing the value of branches with
what-if analysis,” in Proceedings of the ACM SIGSOFT 20th Interna-
tional Symposium on the Foundations of Software Engineering. ACM,
2012, p. 45.

[7] Y. Brun, R. Holmes, M. D. Ernst, and D. Notkin, “Early detection
of collaboration conﬂicts and risks,” IEEE Transactions on Software
Engineering, vol. 39, no. 10, pp. 1358–1375, 2013.

[8] M. L. Guimar˜aes and A. R. Silva, “Improving early detection of software
merge conﬂicts,” in Proceedings of the 34th International Conference
on Software Engineering.

IEEE Press, 2012, pp. 342–352.

[9] H. C. Estler, M. Nordio, C. A. Furia, and B. Meyer, “Awareness and
merge conﬂicts in distributed software development,” in Global Software
Engineering (ICGSE), 2014 IEEE 9th International Conference on.
IEEE, 2014, pp. 26–35.

[10] J. Baumgartner, R. Kanzelman, H. Mony, and V. Paruthi, “Incremental

speculative merging,” Apr. 26 2011, uS Patent 7,934,180.

[11] Y. Brun, R. Holmes, M. D. Ernst, and D. Notkin, “Proactive detection of
collaboration conﬂicts,” in Proceedings of the 19th ACM SIGSOFT sym-
posium and the 13th European conference on Foundations of software
engineering. ACM, 2011, pp. 168–178.

[12] B. K. Kasi and A. Sarma, “Cassandra: Proactive conﬂict minimization
through optimized task scheduling,” in Proceedings of the 2013 Inter-
national Conference on Software Engineering.
IEEE Press, 2013, pp.
732–741.

[13] O. Leßenich, J. Siegmund, S. Apel, C. K¨astner, and C. Hunsen, “In-
dicators for merge conﬂicts in the wild: survey and empirical study,”
Automated Software Engineering, vol. 25, no. 2, pp. 279–313, 2018.

[14] P. Accioly, P. Borba, L. Silva, and G. Cavalcanti, “Analyzing conﬂict
predictors in open-source java projects,” in Proceedings of the 15th
International Conference on Mining Software Repositories.
ACM,
2018, pp. 576–586.

[15] N. Munaiah, S. Kroh, C. Cabrey, and M. Nagappan, “Curating github for
engineered software projects,” Empirical Software Engineering, vol. 22,
no. 6, pp. 3219–3253, 2017.

[16] J. R. Quinlan, “Induction of decision trees,” Machine learning, vol. 1,

no. 1, pp. 81–106, 1986.

[17] A. Liaw, M. Wiener et al., “Classiﬁcation and regression by randomfor-

est,” R news, vol. 2, no. 3, pp. 18–22, 2002.

[18] “Artifact page,” https://github.com/ualberta-smr/conﬂict-prediction.
[19] T. Mens, “A state-of-the-art survey on software merging,” IEEE trans-
actions on software engineering, vol. 28, no. 5, pp. 449–462, 2002.
[20] S. Apel, J. Liebig, B. Brandl, C. Lengauer, and C. K¨astner, “Semistruc-
tured merge: rethinking merge in revision control systems,” in Proceed-
ings of the 19th ACM SIGSOFT symposium and the 13th European
conference on Foundations of software engineering. ACM, 2011, pp.
190–200.

[21] C. Brindescu, M. Codoban, S. Shmarkatiuk, and D. Dig, “How do
centralized and distributed version control systems impact software
changes?” in Proceedings of
the 36th International Conference on
Software Engineering. ACM, 2014, pp. 322–333.

[22] S. Apel, O. Leßenich, and C. Lengauer, “Structured merge with auto-
tuning: balancing precision and performance,” in Proceedings of the 27th
IEEE/ACM International Conference on Automated Software Engineer-
ing. ACM, 2012, pp. 120–129.

[23] B. Westfechtel, “Structure-oriented merging of revisions of software
the 3rd international workshop on

documents,” in Proceedings of
Software conﬁguration management. ACM, 1991, pp. 68–79.

[24] J. Buffenbarger, “Syntactic software merging,” in Software Conﬁguration

Management. Springer, 1995, pp. 153–172.

[25] “Fstmerge
fstmerge.

tool,” https://github.com/joliebig/featurehouse/tree/master/

[26] “Jdime tool,” http://fosd.net/JDime.
[27] G. Cavalcanti, P. Borba, and P. Accioly, “Evaluating and improving
semistructured merge,” Proceedings of the ACM on Programming Lan-
guages, vol. 1, no. OOPSLA, p. 59, 2017.

[28] G. G. L. Menezes, L. G. P. Murta, M. O. Barros, and A. Van Der Hoek,
“On the Nature of Merge Conﬂicts: a Study of 2,731 Open Source
Java Projects Hosted by GitHub,” IEEE Transactions on Software
Engineering, 2018.

[29] C. Costa, J. Figueiredo, L. Murta, and A. Sarma, “Tipmerge: recom-
mending experts for integrating changes across branches,” in Proceed-
ings of the 2016 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering. ACM, 2016, pp. 523–534.
[30] D. Dig, K. Manzoor, R. E. Johnson, and T. N. Nguyen, “Effective
software merging in the presence of object-oriented refactorings,” IEEE
Transactions on Software Engineering, vol. 34, no. 3, pp. 321–335, 2008.
[31] M. Mahmoudi, S. Nadi, and N. Tsantalis, “Are refactorings to blame?
an empirical study of refactorings in merge conﬂicts,” in Proc. of the
26th IEEE International Conference on Software Analysis, Evolution
and Reengineering (SANER ’19), 2019.

[32] L. Hattori and M. Lanza, “Syde: a tool for collaborative software
development,” in Proceedings of
the 32nd ACM/IEEE International
Conference on Software Engineering-Volume 2. ACM, 2010, pp. 235–
238.

[33] Y. Nishimura and K. Maruyama, “Supporting merge conﬂict resolu-
tion by using ﬁne-grained code change history,” in Software Analysis,
Evolution, and Reengineering (SANER), 2016 IEEE 23rd International
Conference on, vol. 1.

IEEE, 2016, pp. 661–664.

[34] O. Kononenko, T. Rose, O. Baysal, M. Godfrey, D. Theisen, and
B. de Water, “Studying pull request merges: a case study of shopify’s
active merchant,” in Proceedings of the 40th International Conference
on Software Engineering: Software Engineering in Practice. ACM,
2018, pp. 124–133.

[35] Y. Fan, X. Xia, D. Lo, and S. Li, “Early prediction of merged code
changes to prioritize reviewing tasks,” Empirical Software Engineering,
pp. 1–48, 2018.

[36] M. Owhadi-Kareshk and S. Nadi, “Scalable software merging studies
with merganser,” in Proceedings of the 16th International Conference
on Mining Software Repositories (MSR ’19), 2019.

[37] https://git-scm.com/docs/git-merge.
[38] G. Haixiang, L. Yijing, J. Shang, G. Mingyun, H. Yuanyue, and
G. Bing, “Learning from class-imbalanced data: Review of methods and
applications,” Expert Systems with Applications, vol. 73, pp. 220–239,
2017.

[39] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:
synthetic minority over-sampling technique,” Journal of artiﬁcial intel-
ligence research, vol. 16, pp. 321–357, 2002.

[40] “reaper dataset,” https://reporeapers.github.io/static/downloads/dataset.

csv.gz.

[41] M. G. Kendall, S. F. Kendall, and B. B. Smith, “The distribution of
spearman’s coefﬁcient of rank correlation in a universe in which all
rankings occur an equal number of times,” Biometrika, pp. 251–273,
1939.

[42] T. W. Anderson and J. D. Finn, The new statistical analysis of data.

Springer Science & Business Media, 2012.

