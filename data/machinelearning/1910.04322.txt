One Sample Stochastic Frank-Wolfe

Mingrui Zhang
Yale University
mingrui.zhang@yale.edu

Zebang Shen
University of Pennsylvania
zebang@seas.upenn.edu

Aryan Mokhtari
The University of Texas at Austin
mokhtari@austin.utexas.edu

Hamed Hassani
University of Pennsylvania
hassani@seas.upenn.edu

Amin Karbasi
Yale University
amin.karbasi@yale.edu

October 11, 2019

Abstract

One of the beauties of the projected gradient descent method lies in its rather simple
mechanism and yet stable behavior with inexact, stochastic gradients, which has led to
its wide-spread use in many machine learning applications. However, once we replace the
projection operator with a simpler linear program, as is done in the Frank-Wolfe method,
both simplicity and stability take a serious hit. The aim of this paper is to bring them back
without sacriﬁcing the eﬃciency. In this paper, we propose the ﬁrst one-sample stochastic
Frank-Wolfe algorithm, called 1-SFW, that avoids the need to carefully tune the batch size,
step size, learning rate, and other complicated hyper parameters.
In particular, 1-SFW
achieves the optimal convergence rate of O(1/(cid:15)2) for reaching an (cid:15)-suboptimal solution in the
stochastic convex setting, and a (1 − 1/e) − (cid:15) approximate solution for a stochastic monotone
DR-submodular maximization problem. Moreover, in a general non-convex setting, 1-SFW
ﬁnds an (cid:15)-ﬁrst-order stationary point after at most O(1/(cid:15)3) iterations, achieving the current
best known convergence rate. All of this is possible by designing a novel unbiased momentum
estimator that governs the stability of the optimization process while using a single sample at
each iteration.

9
1
0
2

t
c
O
0
1

]

C
O
.
h
t
a
m

[

1
v
2
2
3
4
0
.
0
1
9
1
:
v
i
X
r
a

1

 
 
 
 
 
 
1

Introduction

Projection-free methods, also known as conditional gradient methods or Frank-Wolfe (FW)
methods, have been widely used for solving constrained optimization problems [Frank and Wolfe,
1956, Jaggi, 2013, Lacoste-Julien and Jaggi, 2015].
Indeed, extending such methods to the
stochastic setting is a challenging task as it is known that FW-type methods are highly sensitive
to stochasticity in gradient computation [Hazan and Kale, 2012]. To resolve this issue several
stochastic variants of FW methods have been studied in the literature [Hazan and Kale, 2012,
Hazan and Luo, 2016, Reddi et al., 2016, Lan and Zhou, 2016, Braun et al., 2017, Hassani et al.,
2019, Shen et al., 2019a, Yurtsever et al., 2019]. In all these stochastic methods, the basic idea is
to provide an accurate estimate of the gradient by using some variance-reduction techniques that
typically rely on large mini-batches of samples where the size grows with the number of iterations
or is reciprocal of the desired accuracy. A growing mini-batch, however, is undesirable in practice
as requiring a large collection of samples per iteration may easily prolong the duration of each
iterate without updating optimization parameters frequently enough Defazio and Bottou [2018].
A notable exception to this trend is the the work of Mokhtari et al. [2018b] which employs a
momentum variance-reduction technique requiring only one sample per iteration; however, this
method suﬀers from suboptimal convergence rates. At the heart of this paper is the answer to
the following question:

Can we achieve the optimal complexity bounds for a stochastic variant of Frank-Wolfe
while using a single stochastic sample per iteration?

We show that the answer to the above question is positive and present the ﬁrst projection-free
method that requires only one sample per iteration to update the optimization variable and yet
achieves the optimal complexity bounds for convex, nonconvex, and monotone DR-submodular
settings.

More formally, we focus on a general non-oblivious constrained stochastic optimization problem

min
x∈K

F (x) (cid:44) min
x∈K

Ez∼p(z;x)[ ˜F (x; z)],

(1)

where x ∈ Rd is the optimization variable, K ⊆ Rd is the convex constraint set, and the objective
function F : Rd → R is deﬁned as the expectation over a set of functions ˜F . The function
˜F : Rd × Z → R is determined by x and a random variable z ∈ Z with distribution z ∼ p(z; x).
We refer to problem (1) as a non-oblivious stochastic optimization problem as the distribution of
the random variable z depends on the choice of x. When the distribution p is independent of x,
we are in the standard oblivious stochastic optimization regime where the goal is to solve

min
x∈K

F (x) (cid:44) min
x∈K

Ez∼p(z)[ ˜F (x; z)].

(2)

Hence, the oblivious problem (2) can be considered as a special case of the non-oblivious problem
(1). Note that non-oblivious stochastic optimization has broad applications in machine learning,
including multi-linear extension of a discrete submodular function [Hassani et al., 2019], MAP
inference in determinantal point processes (DPPs) [Kulesza et al., 2012], and reinforcement
learning [Sutton and Barto, 2018, Shen et al., 2019b].

Our goal is to propose an eﬃcient FW-type method for the non-oblivious optimization problem
(1). Here, the eﬃciency is measured by the number of stochastic oracle queries, i.e., the sample

2

Table 1: Convergence guarantees of stochastic Frank-Wolfe methods for constrained convex
minimization

Ref.

batch
[Hazan and Kale, 2012] O(1/(cid:15)2)
[Hazan and Luo, 2016] O(1/(cid:15)2)
[Mokhtari et al., 2018b]
[Yurtsever et al., 2019] O(1/(cid:15))
O(1/(cid:15))
[Hassani et al., 2019]
1
This paper

1

complexity oblivious non-oblivious

O(1/(cid:15)4)
O(1/(cid:15)3)
O(1/(cid:15)3)
O(1/(cid:15)2)
O(1/(cid:15)2)
O(1/(cid:15)2)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)

Table 2: Convergence guarantees of stochastic Frank-Wolfe methods for non-convex minimization

complexity oblivious non-oblivious

Ref.

batch
[Hazan and Luo, 2016] O(1/(cid:15)2)
[Hazan and Luo, 2016] O(1/(cid:15)4/3) O(1/(cid:15)10/3)

O(1/(cid:15)4)

[Shen et al., 2019a]

O(1/(cid:15))
[Yurtsever et al., 2019] O(1/(cid:15))
O(1/(cid:15))
[Hassani et al., 2019]
1
This paper

O(1/(cid:15)3)
O(1/(cid:15)3)
O(1/(cid:15)3)
O(1/(cid:15)3)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)

Table 3: Convergence guarantees for stochastic monotone continuous DR-submodular
function maximization

Ref.
[Hassani et al., 2017]
[Mokhtari et al., 2018b]
[Hassani et al., 2019]
This paper

batch
1
1
O(1/(cid:15))
1

utility
(1/2)OPT−(cid:15)
(1 − 1/e)OPT−(cid:15)
(1 − 1/e)OPT−(cid:15)
(1 − 1/e)OPT−(cid:15)

complexity
O(1/(cid:15)2)
O(1/(cid:15)3)
O(1/(cid:15)2)
O(1/(cid:15)2)

complexity of z. As we mentioned earlier, among the stochastic variants of FW, the momentum
stochastic Frank-Wolfe method proposed in [Mokhtari et al., 2018a,b] is the only method that
requires only one sample per iteration. However, the stochastic oracle complexity of this algorithm
is suboptimal, i.e., O(1/(cid:15)3) stochastic queries are required for both convex minimization and
monotone DR-submodular maximization problems. This suboptimal rate is due to the fact that
the gradient estimator in momentum FW is biased and it is necessary to use a more conservative
averaging parameter to control the eﬀect of the bias term.

Theoretical results of 1-SFW and other related works are summarized in Tables 1-3. These
results show that 1-SFW attains the optimal or best known complexity bounds in all the considered
settings, while requiring only one single stochastic oracle query per iteration and avoiding large
batch sizes altogether.

To resolve this issue, we propose a one-sample stochastic Frank-Wolfe method, called 1-SFW,
which modiﬁes the gradient approximation in momentum FW to ensure that the resulting gradient

3

estimation is an unbiased estimator of the gradient (Section 3). This goal has been achieved by
adding an unbiased estimator of the gradient variation ∆t = ∇F (xt) − ∇F (xt−1) to the gradient
approximation vector (Section 3.1). We later explain why coming up with an unbiased estimator
of the gradient diﬀerence ∆t could be a challenging task in the non-oblivious setting and show how
we overcome this diﬃculty (Section 3.2). We also characterize the convergence guarantees of 1-SFW
for convex minimization, nonconvex minimization and monotone DR-submodular maximization
(Section 4). In particular, we show that 1-SFW achieves the optimal convergence rate of O(1/(cid:15)2) for
reaching an (cid:15)-suboptimal solution in the stochastic convex setting, and a (1 − 1/e) − (cid:15) approximate
solution for a stochastic monotone DR-submodular maximization problem. Moreover, in a general
non-convex setting, 1-SFW ﬁnds an (cid:15)-ﬁrst-order stationary point after at most O(1/(cid:15)3) iterations,
achieving the current best known convergence rate. Finally, we study the oblivious problem in (2)
and show that our proposed 1-SFW method becomes signiﬁcantly simpler and the corresponding
theoretical results hold under less strict assumptions. We further highlight the similarities between
the variance reduced method in [Cutkosky and Orabona, 2019] also known as STORM and the
oblivious variant of 1-SFW. Indeed, our algorithm has been originally inspired by STORM.

2 Related Work

As a projection-free algorithm, Frank-Wolfe method [Frank and Wolfe, 1956] has been studied
for both convex optimization [Jaggi, 2013, Lacoste-Julien and Jaggi, 2015, Garber and Hazan,
2015, Hazan and Luo, 2016, Mokhtari et al., 2018b] and non-convex optimization problems
[Lacoste-Julien, 2016, Reddi et al., 2016, Mokhtari et al., 2018c, Shen et al., 2019b, Hassani et al.,
2019]. In large-scale settings, distributed FW methods were proposed to solve speciﬁc problems,
including optimization under block-separable constraint set [Wang et al., 2016], and learning
low-rank matrices [Zheng et al., 2018]. The communication-eﬃcient distributed FW variants were
proposed for speciﬁc sparse learning problems in Bellet et al. [2015], Lafond et al. [2016], and for
general constrained optimization problems in [Zhang et al., 2019]. Zeroth-order FW methods
were studied in [Sahu et al., 2018, Chen et al., 2019].

Several works have studied diﬀerent ideas for reducing variance in stochastic settings. The
SVRG method was proposed by Johnson and Zhang [2013] for the convex setting and then
extended to the nonconvex setting by several other works [Allen-Zhu and Hazan, 2016, Reddi
et al., 2016, Zhou et al., 2018]. The StochAstic Recursive grAdient algoritHm (SARAH) was
studied in [Nguyen et al., 2017a,b]. Then as a variant of SARAH, the Stochastic Path-Integrated
Diﬀerential Estimator (SPIDER) technique was proposed by Fang et al. [2018]. Based on SPIDER,
various algorithm for convex and non-convex optimization problems have been studied [Shen
et al., 2019a, Hassani et al., 2019, Yurtsever et al., 2019].

In this paper, we also consider optimizing an important subclass of non-convex objectives,
known as continuous DR-submodular functions that generalize the diminishing returns property
to the continuous domains. Continuous DR-submodular functions can be minimized exactly
[Bach, 2015, Staib and Jegelka, 2017], and maximized approximately [Bian et al., 2017b,a, Hassani
et al., 2017, Mokhtari et al., 2018a]. They have interesting applications in machine learning,
including experimental design [Chen et al., 2018], MAP inference in determinantal point processes
(DPPs) [Kulesza et al., 2012], and mean-ﬁeld inference in probabilistic models [Bian et al., 2018].

4

3 One Sample SFW Algorithm

In this section, we introduce our proposed one sample SFW (1-SFW) method. We ﬁrst present the
mechanism for computing a variance reduced unbiased estimator of the gradient ∇F (xt). Then,
we explain the procedure for computing an unbiased estimator of the gradient variation ∆t =
∇F (xt) − ∇F (xt−1) in a non-oblivious setting which is required for the gradient approximation
of 1-SFW. Then, we present the complete description of our proposed method.

3.1 Stochastic gradient approximation

In our work, we build on the momentum variance reduction approach proposed in [Mokhtari
et al., 2018a,b] to reduce the variance of the one-sample method. To be more precise, in the
momentum FW method [Mokhtari et al., 2018a], we update the gradient approximation dt at
round t according to the update

dt = (1 − ρt)dt−1 + ρt∇ ˜F (xt; zt),

(3)

where ρt is the averaging parameter and ∇ ˜F (xt; zt) is a one-sample estimation of the gradient.
Since dt is a weighted average of the previous gradient estimation dt−1 and the newly updated
stochastic gradient, it has a lower variance comparing to one-sample estimation ∇ ˜F (xt; zt). In
particular, it was shown by Mokhtari et al. [2018a] that the variance of gradient approximation
in (3) approaches zero at a sublinear rate of O(t−2/3). The momentum approach reduces the
variance of gradient approximation, but it leads to a biased gradient approximation, i.e., dt is not
an unbiased estimator of the gradient ∇F (xt). Consequently, it is necessary to use a conservative
averaging parameter ρt for momentum FW to control the eﬀect of the bias term which leads to a
sublinear error rate of O(t−1/3) and overall complexity of O(1/(cid:15)3).

To resolve this issue and come up with a fast momentum based FW method for the non-
oblivious problem in (1), we slightly modify the gradient estimation in (3) to ensure that the
resulting gradient estimation is an unbiased estimator of the gradient ∇F (xt). Speciﬁcally, we add
the term ˜∆t, which is an unbiased estimator of the gradient variation ∆t = ∇F (xt) − ∇F (xt−1),
to dt−1. This modiﬁcation leads to the following gradient approximation

dt = (1 − ρt)(dt−1 + ˜∆t) + ρt∇ ˜F (xt; zt).

(4)

To verify that dt is an unbiased estimator of ∇F (xt) we can use a simple induction argument.
Assuming that dt−1 is an unbiased estimator of ∇F (xt) and ˜∆t is an unbiased estimator of
∇F (xt) − ∇F (xt−1) we have E[dt] = (1 − ρt)(∇F (xt−1) + (∇F (xt) − ∇F (xt−1))) + ρt∇F (xt) =
∇F (xt). Hence, the gradient approximation in (4) leads to an unbiased approximation of the
gradient. Let us now explain how to compute an unbiased estimator of the gradient variation
∆t = ∇F (xt) − ∇F (xt−1) for a non-oblivious setting.

3.2 Gradient variation estimation

The most natural approach for estimating the gradient variation ∆t = ∇F (xt) − ∇F (xt−1)
using only one sample z is computing the diﬀerence of two consecutive stochastic gradients, i.e.,
∇ ˜F (xt; z) − ∇ ˜F (xt−1; z). However, this approach leads to an unbiased estimator of the gradient
variation ∆t only in the oblivious setting where p(z) is independent of the choice of x, and would

5

introduce bias in the more general non-oblivious case. To better highlight this issue, assume that
z is sampled according to distribution p(z; xt). Note that ∇ ˜F (xt; z) is an unbiased estimator
of ∇F (xt), i.e., E[∇F (xt; z)] = ∇F (xt), however, ∇ ˜F (xt−1; z) is not an unbiased estimator of
∇F (xt−1) since p(z; xt−1) may be diﬀerent from p(z; xt).

To circumvent this obstacle, an unbiased estimator of ∆t was introduced in Hassani et al.
[2019]. To explain their proposal for approximating the gradient variation using only one sample,
note that the diﬀerence ∆t = ∇F (xt) − ∇F (xt−1) can be written as

∆t =

(cid:90) 1

0

∇2F (xt(a))(xt − xt−1)da =

(cid:20)(cid:90) 1

(cid:21)
∇2F (xt(a))da

(xt − xt−1),

0

where xt(a) = axt + (1 − a)xt−1 for a ∈ [0, 1]. According to this expression, one can ﬁnd an
unbiased estimator of (cid:82) 1
0 ∇2F (xt(a))da and use its product with (xt − xt−1) to ﬁnd an unbiased
estimator of ∆t. It can be easily veriﬁed that ∇2F (xt(a))(xt − xt−1) is an unbiased estimator of
∆t if a is chosen from [0, 1] uniformly at random. Therefore, all we need is to come up with an
unbiased estimator of the Hessian ∇2F .

By basic calculus, we can show that ∀x ∈ K and z with distribution p(z; x), the matrix

˜∇2F (x; z) deﬁned as

˜∇2F (x; z) = ˜F (x; z)[∇ log p(z; x)][∇ log p(z; x)](cid:62) + ∇2 ˜F (x; z) + [∇ ˜F (x; z)][∇ log p(z; x)](cid:62)

+ ˜F (x; z)∇2 log p(z; x) + [∇ log p(z; x)][∇ ˜F (x; z)](cid:62),

(5)

is an unbiased estimator of ∇2F (x). Note that the above expression requires only one sample of
z. As a result, we can construct ˜∆t as an unbiased estimator of ∆t using only one sample

˜∆t (cid:44) ˜∇2

t (xt − xt−1),
(6)
t = ˜∇2F (xt(a); zt(a)), and zt(a) follows the distribution p(zt(a); xt(a)). By using this
where ˜∇2
procedure, we can indeed compute the vector dt in (4) with only one sample of z per iteration.
We note that if we only use one sample of z per iteration, i.e., zt = zt(a), the stochastic
gradient in (3) becomes ∇ ˜F (xt; zt(a)). It is not an unbiased estimator of ∇F (xt), since ∇F (xt) =
˜F (xt; z), while zt(a) ∼ p(z; xt(a)). Through a completely diﬀerent analysis from the
Ez∼p(z;xt)
ones in [Mokhtari et al., 2018a, Hassani et al., 2019], we show that the modiﬁed dt is still a good
gradient estimation (Lemma 2), which allows the establishment of the optimal stochastic oracle
complexity for our proposed algorithm.

Another issue of this scheme is that in (5) and (6), we need to calculate ∇2 ˜F (xt(a); zt(a))(xt −
xt−1) and ∇2 log p(xt(a); zt(a))(xt − xt−1), where computation of Hessian is involved. When
exact Hessian is not accessible, however, we can resort to an approximation by the diﬀerence of
two gradients.

Precisely, for any function ψ : Rd → R, any vector u ∈ Rd with (cid:107)u(cid:107)≤ D, and some δ > 0

small enough, we have

φ(δ; ψ) (cid:44) ∇ψ(x + δu) − ∇ψ(x − δu)

2δ

≈ ∇2ψ(x)u.

If we assume that ψ is L2-second-order smooth, i.e., (cid:107)∇2ψ(x)−∇2ψ(y))(cid:107)≤ L2(cid:107)x−y(cid:107), ∀x, y ∈ Rd,
we can upper bound the approximation error quantitatively:

(cid:107)∇2ψ(x)u − φ(δ; ψ)(cid:107)= (cid:107)∇2ψ(x)u − ∇2ψ(˜x)u)(cid:107)≤ D2L2δ,

(7)

6

where ˜x is obtained by the mean-value theorem. In other words, the approximation error can be
suﬃciently small for proper δ. So we can estimate ∆t by

˜∆t = ˜F (x; z)[∇ log p(z; x)][∇ log p(z; x)](cid:62)ut + φ(δt, ˜F (x; z)) + [∇ ˜F (x; z)][∇ log p(z; x)](cid:62)ut

+ ˜F (x; z)φ(δt, log p(z, x)) + [∇ log p(z; x)][∇ ˜F (x; z)](cid:62)ut,

(8)

where ut = xt − xt−1, x, z, δt are chosen appropriately. We also note that since computation of
gradient diﬀerence has a complexity of O(d), while that for Hessian is O(d2), this approximation
strategy can also help to accelerate the optimization process.

3.3 Variable update

Once the gradient approximation dt is computed, we can follow the update of conditional gradient
methods for computing the iterate xt. In this section, we introduce two diﬀerent schemes for
updating the iterates depending on the problem that we aim to solve.

For minimizing a general (non-)convex function using one sample stochastic FW, we update

the iterates according to the update

xt+1 = xt + ηt(vt − xt), where vt = arg min

v∈K

{v(cid:62)dt}.

(9)

In this case, we ﬁnd the direction that minimizes the inner product with the current gradient
approximation dt over the constraint set K, and the updated variable xt+1 by descending in the
direction of vt − xt with step size ηt.

For monotone DR-submodular maximization, the update rule is slightly diﬀerent, and a
stochastic variant of the continuous greedy method [Vondrák, 2008] can be used. Using the
same stochastic estimator dt as in the (non-)convex case, the update rule for DR-Submodular
optimization is given by

xt+1 = xt + ηtvt, where vt = arg max

v∈K

{v(cid:62)dt}

(10)

where ηt = 1/T . Note that in this case we ﬁnd the direction that maximizes the inner product
with the current gradient approximation dt over the constraint set K, and move towards that
direction with step size ηt = 1/T . Hence, if we start from the origin, after T steps the outcome
will be a feasible point as it can be written as the average of T feasible points.

The description of our proposed 1-SFW method for smooth (non-)convex minimization as well

as monotone DR-submodular maximization is outlined in (1).

4 Main Results

Before presenting the convergence results of our algorithm, we ﬁrst state our assumptions on the
constraint set K, the stochastic function ˜F , and the distribution p(z; x).

Assumption 1. The constraint set K ⊆ Rd is compact with diameter D = maxx,y∈K(cid:107)x − y(cid:107),
and radius R = maxx∈K(cid:107)x(cid:107).
Assumption 2. The stochastic function ˜F (x; z) has uniformly bounded function value, i.e.,
| ˜F (x; z)|≤ B for all x ∈ K, z ∈ Z.

7

Algorithm 1 One-Sample SFW
Input: Step sizes ρt ∈ (0, 1), ηt ∈ (0, 1), initial point x1 ∈ K, total number of iterations T
Output: xT +1 or xo, where xo is chosen from {x1, x2, · · · , xT } uniformly at random
1: for t = 1, 2, . . . , T do
if t = 1 then
2:

Sample a point z1 according to p(z1, x1)
Compute d1 = ∇ ˜F (x1; z1)

else

Choose a uniformly at random from [0, 1]
Compute xt(a) = axt + (1 − a)xt−1
Sample a point zt according to p(z; xt(a))
Compute ˜∆t either by ˜∇2

t = ˜∇2F (xt(a); zt) based on (5) and ˜∆t = ˜∇2

t (xt − xt−1)

(Exact Hessian Option); or by Eq. (8) with x = xt(a), z = zt (Gradient Diﬀerence Option)

dt = (1 − ρt)(dt−1 + ˜∆t) + ρt∇ ˜F (xt, zt)

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

end if
(non-)convex min.: Update xt+1 based on (9)
DR-sub. max.: Update xt+1 based on (10)

13:
14: end for

Assumption 3. The stochastic gradient ∇ ˜F has uniformly bound norm, i.e., (cid:107)∇ ˜F (x; z)(cid:107)≤
G ˜F , ∀x ∈ K, ∀z ∈ Z. The norm of the gradient of log p has bounded fourth-order moment, i.e.,
Ez∼p(z;x)(cid:107)∇ log p(z; x)(cid:107)4≤ G4
p. We also deﬁne G = max{G ˜F , Gp}.

Assumption 4. The stochastic Hessian ∇2 ˜F has uniformly bounded spectral norm: (cid:107)∇2 ˜F (x; z)(cid:107)≤
L ˜F , ∀x ∈ K, ∀z ∈ Z. The spectral norm of the Hessian of log p has bounded second-order moment:
Ez∼p(z;x)(cid:107)∇2 log p(z; x)(cid:107)2≤ L2
p. We also deﬁne L = max{L ˜F , Lp}.

We note that in Assumptions 2-4, we assume that the stochastic function ˜F has uniformly
bounded function value, gradient norm, and second-order diﬀerential. Moreover, with these
assumptions, we can establish an upper bound for the second-order moment of the spectral norm
of the Hessian estimator ˜∇2F (x; z), which is deﬁned in (5).

Lemma 1. [Lemma 7.1 of [Hassani et al., 2019]] Under Assumptions 2-4, for all x ∈ K, we have

Ez∼p(z;x)[(cid:107) ˜∇2F (x; z)(cid:107)2] ≤ 4B2G4 + 16G4 + 4L2 + 4B2L2 (cid:44) ¯L.

Note that the result in Lemma (1) also implies the ¯L-smoothness of F , since

(cid:107)∇2F (x)(cid:107)2= (cid:107)Ez∼p(z;x)[ ˜∇2F (x; z)](cid:107)2≤ Ez∼p(z;x)[(cid:107) ˜∇2F (x; z)(cid:107)2] ≤ ¯L2.

In other words, the conditions in Assumptions 2-4 implicitly imply that the objective function F
is ¯L-smooth.

To establish the convergence guarantees for our proposed 1-SFW algorithm, the key step is to
derive an upper bound on the errors of the estimated gradients. To do so, we prove the following
lemma, which provides the required upper bounds in diﬀerent settings of parameters.

8

Lemma 2. Consider the gradient approximation dt deﬁned in (4). Under Assumptions 1-4, if we
run Algorithm 1 with Exact Hessian Option in Line 9, and with parameters ρt = (t − 1)−α, ∀t ≥ 2,
and ηt ≤ t−α, ∀t ≥ 1 for some α ∈ (0, 1], then the gradient estimation dt satisﬁes

E[(cid:107)∇F (xt) − dt(cid:107)2] ≤ Ct−α,

(11)

where the constant C is given by

C = max

(cid:40)

2(2G+D ¯L)2
2−2−α −α

(cid:20)

,

2
2−2−α −α

(cid:21)4

, [2D( ¯L+L)]4

(cid:41)
.

Lemma (2) shows that with appropriate parameter setting, the gradient error converges to
zero at a rate of t−α. With this unifying upper bound, we can obtain the convergence rates of
our algorithm for diﬀerent kinds of objective functions.

If in the update of 1-SFW we use the Gradient Diﬀerence Option in Line 9 of Algorithm 1 to
estimate ˜∆t, as pointed out above, we need one further assumption on second-order smoothness
of the functions ˜F and log p.

Assumption 5. The stochastic function ˜F is uniformly L2, ˜F - second-order smooth: (cid:107)∇2 ˜F (x; z)−
∇2 ˜F (y; z)(cid:107)≤ L2, ˜F (cid:107)x − y(cid:107), ∀x, y ∈ K, ∀z ∈ Z. The log probability log p(z; x) is uniformly L2,p-
second-order smooth: (cid:107)∇2 log p(z; x) − ∇2 log p(z; y)(cid:107)≤ L2,p(cid:107)x − y(cid:107), ∀x, y ∈ K, ∀z ∈ Z. We also
deﬁne L2 = max{L2, ˜F , L2,p}.

We note that under (5), the approximation bound in (7) holds for both ˜F and log p. So for
δt suﬃciently small, the error introduced by the Hessian approximation can be ignored. Thus
similar upper bound for errors of estimated gradient still holds.

Lemma 3. Consider the gradient approximation dt deﬁned in (4). Under Assumptions 1-
5, if we run Algorithm 1 with Gradient Diﬀerence Option in Line 9, and with parameters
ρt = (t − 1)−α, δt =
DL2(1+B) , ∀t ≥ 2, and ηt ≤ t−α, ∀t ≥ 1 for some α ∈ (0, 1], then the gradient
estimation dt satisﬁes

3ηt−1 ¯L

√

E[(cid:107)∇F (xt) − dt(cid:107)2] ≤ Ct−α,

(12)

where the constant C is given by

C = max

(cid:26)8(D2 ¯L2 + G2 + GD ¯L)
2−2−α −α

,

(cid:18)

2
2−2−α −α

(cid:19)4
,

[4D( ¯L+L)]4

(cid:27)
.

Lemma 3 shows that with Gradient Diﬀerence Optionin Line 9 of Algorithm 1, the error of
estimated gradient can obtain the same order of convergence rate as that with Exact Hessian
Option. So in the following three subsections, we will present the theoretical results of our
proposed 1-SFW algorithm with Exact Hessian Option, for convex minimization, non-convex
minimization, and monoton DR-submodular maximization, respectively. The results of Gradient
Diﬀerence Option only diﬀer in a factor of constant.

9

4.1 Convex Minimization

For convex minimization problems, to obtain an (cid:15)-suboptimal solution, (1) only requires at most
O(1/(cid:15)2) stochastic oracle queries, and O(1/(cid:15)2) linear optimization oracle calls. Or precisely, we
have

Theorem 1 (Convex). Consider the 1-SFW method outlined in Algorithm 1 with Exact Hessian
Option in Line 9. Further, suppose the conditions in Assumptions 1-4 hold, and assume that F is
convex on K. Then, if we set the algorithm parameters as ρt = (t − 1)−1 and ηt = t−1, then the
output is feasible xT +1 ∈ K and satisﬁes

E[F (xT +1) − F (x∗)] ≤

2

√
√

CD
T

+

¯LD2(1 + ln T )
2T

,

where C = max{4(2G + D ¯L)2, 256, [2D( ¯L + L)]4}, and x∗ is a minimizer of F on K.

The result in Theorem 1 shows that the proposed one sample stochastic Frank-Wolfe method,
in the convex setting, has an overall complexity of O(1/(cid:15)2) for ﬁnding an (cid:15)-suboptimal solution.
Note that to prove this claim we used the result in Lemma 2 for the case that α = 1, i.e., the
variance of gradient approximation converges to zero at a rate of O(1/t).

4.2 Non-Convex Minimization

For non-convex minimization problems, showing that the gradient norm approaches zero, i.e.,
(cid:107)∇F (xt)(cid:107)→ 0, implies convergence to a stationary point in the unconstrained setting. Thus, it
is usually used as a measure for convergence. In the constrained setting, however, the norm of
gradient is not a proper measure for deﬁning stationarity and we instead used the Frank-Wolfe
Gap [Jaggi, 2013, Lacoste-Julien, 2016], which is deﬁned by

G(x) = max
v∈K

(cid:104)v − x, −∇F (x)(cid:105).

We note that by deﬁnition, G(x) ≥ 0, ∀x ∈ K. If some point x ∈ K satisﬁes G(x) = 0, then it is a
ﬁrst-order stationary point.

In the following theorem, we formally prove the number of iterations required for one sample
stochastic FW to ﬁnd an (cid:15)-ﬁrst-order stationary point in expectation, i.e., a point x that satisﬁes
E[G(x)] ≤ (cid:15).

Theorem 2 (Non-Convex). Consider the 1-SFW method outlined in Algorithm 1 with Exact
Hessian Option in Line 9. Further, suppose the conditions in Assumptions 1-4 hold. Then, if
we set the algorithm parameters as ρt = (t − 1)−2/3, and ηt = T −2/3, then the output is feasible
xo ∈ K and satisﬁes

where the constant C is given by

E[G(xo)] ≤

2B + 3

CD/2

√

T 1/3

+

¯LD2
2T 2/3

,

C = max






2(2G+D ¯L)2
3 − 2− 2

4

3

(cid:34)

,

2
3 − 2− 2

4

3

(cid:35)4

, [2D( ¯L+L)]4



.


10

We remark that Theorem (2) shows that Algorithm 1 ﬁnds an (cid:15)-ﬁrst order stationary points
after at most O(1/(cid:15)3) iterations, while uses exactly one stochastic gradient per iteration. Note
that to obtain the best performance guarantee in Theorem (2), we used the result of Lemma 2
for the case that α = 2/3, i.e., the variance of gradient approximation converges to zero at a rate
of O(T −2/3).

4.3 Monotone DR-Submodular Maximization

In this section, we focus on the convergence properties of one-sample stochastic Frank-Wolfe or
one-sample stochastic Continuous Greedy for solving a monotone DR-submodular maximization
problem. Consider a diﬀerentiable function F : X → R≥0, where the domain X (cid:44) (cid:81)d
i=1 Xi, and
each Xi is a compact subset of R≥0. We say F is continuous DR-submodular if for all x, y ∈ X
that satisfy x ≤ y and every i ∈ {1, 2, · · · , d}, we have ∂F
∂xi

(x) ≥ ∂F
∂xi

An important property of continuous DR-submodular function is the concavity along the
non-negative directions [Calinescu et al., 2011, Bian et al., 2017b]: for all x, y ∈ X such that
x ≤ y, we have F (y) ≤ F (x) + (cid:104)∇F (x), y − x(cid:105). We say F is monotone if for all x, y ∈ X such
that x ≤ y, we have F (x) ≤ F (y).

(y).

For continuous DR-submodular maximization, it has been shown that approximated solution
within a factor of (1 − e−1 + (cid:15)) can not be obtained in polynomial time [Bian et al., 2017b]. As a
result, we analyze the convergence rate for the tight (1 − e−1)OPT approximation. To achieve a
(1 − e−1)OPT − (cid:15) approximation guarantee, our proposed algorithm requires at most O(1/(cid:15)2)
stochastic oracle queries, and O(1/(cid:15)2) linear optimization oracle calls, as we show in the following
theorem.

Theorem 3 (Submodular). Consider the 1-SFW method outlined in Algorithm 1 with Exact
Hessian Option in Line 9 for maximizing DR-Submodular functions. Further, suppose the
conditions in Assumptions 1-4 hold, and further assume that F is monotone and continuous
DR-submodular on K. Then, if we set the algorithm parameters as ρt = (t − 1)−1 and ηt = T −1,
then the output is a feasible point xT +1 ∈ K and satisﬁes

E[F (xT +1)] ≥ (1 − e−1)F (x∗) −

√

4R
C
T 1/2

−

¯LR2
2T

,

where C = max{4(2G + R ¯L)2, 256, [2R( ¯L + L)]4}.

Finally, we note that Algorithm 1 can also be used to solve stochastic discrete submodular
maximization. Precisely, we can apply Algorithm 1 on the multilinear extension of the discrete
submodular functions, and round the output to a feasible set by lossless rounding schemes like
pipage rounding [Calinescu et al., 2011] and contention resolution method [Chekuri et al., 2014].

5 Oblivious Setting

In this section, we speciﬁcally study the oblivious problem introduced in (2) which is a special
case of the non-oblivious problem deﬁned in (1). In particular, we show that the proposed one
sample Frank-Wolfe method becomes signiﬁcantly simpler under the oblivious setting. Also, we
show that the theoretical results for one sample SFW hold under less strict assumptions when we
are in the oblivious regime.

11

Algorithm 2 One-Sample SFW (Oblivious Setting)

Input: Step sizes ρt ∈ (0, 1), ηt ∈ (0, 1), initial point x1 ∈ K, total number of iterations T
Output: xT +1 or xo, where xo is chosen from {x1, x2, · · · , xT } uniformly at random
1: for t = 1, 2, . . . , T do
2:

3:

4:

5:

6:

7:

8:

9:

Sample a point zt according to p(z)
if t = 1 then

Compute d1 = ∇ ˜F (x1; z1)

else

˜∆t = ∇ ˜F (xt; zt) − ∇ ˜F (xt−1; zt)
dt = (1 − ρt)(dt−1 + ˜∆t) + ρt∇ ˜F (xt, zt)

end if
(non-)convex min.: Update xt+1 based on (9)
DR-sub. max.: Update xt+1 based on (10)

10:
11: end for

5.1 Algorithm

As we discussed in Section 3, a major challenge that we face for designing a variance reduced
Frank-Wolfe method for the non-oblivious setting is computing an unbiased estimator of the
gradient variation ∆t = ∇F (xt) − ∇F (xt−1). This is indeed not problematic in the oblivious
setting, as in this case z ∼ p(z) is independent of x and therefore ∇ ˜F (xt; z) − ∇ ˜F (xt−1; z) is an
unbiased estimator of the gradient variation ∆t = ∇F (xt) − ∇F (xt−1). Hence, in the oblivious
setting, our proposed one sample FW uses the following gradient approximation

dt = (1 − ρt)(dt−1 + ˜∆t) + ρt∇ ˜F (xt; zt),

where ˜∆t is given by

˜∆t = ∇ ˜F (xt; zt) − ∇ ˜F (xt−1; zt).
The rest of the algorithm for updating the variable xt is identical to the one for the non-oblivious
setting. The description of our proposed algorithm for the oblivious setting is outlined in
Algorithm 2.

Remark 1. We note that by rewriting our proposed 1-SFW method for the oblivious setting, we
recover the variance reduction technique applied in the STORM method proposed by Cutkosky
and Orabona [2019] with diﬀerent settings of parameters. In [Cutkosky and Orabona, 2019],
however, the STORM algorithm was only combined with SGD to solve unconstrained non-convex
minimization problems, while our proposed 1-SFW method solves convex minimization, non-convex
minimization, and DR-submodular maximization in a constrained setting.

5.2 Theoretical results

In this section, we show that the variant of one sample stochastic FW for the oblivious setting
(described in Algorithm 2) recovers the theoretical results for the non-oblivious setting with less
assumptions. In particular, we only require the following condition for the stochastic functions ˜F
to prove our main results.

12

Assumption 6. The function ˜F has uniformly bound gradients, i.e., ∀x ∈ K, ∀z ∈ Z

(cid:107)∇ ˜F (x; z)(cid:107)≤ G.

Moreover, the function ˜F is uniformly L-smooth, i.e., ∀x, y ∈ K, ∀z ∈ Z

(cid:107)∇ ˜F (x; z) − ∇ ˜F (y; z)(cid:107)≤ L(cid:107)x − y(cid:107)

We note that as direct corollaries of Theorems 1 to 3, Algorithm 2 achieves the same optimal

convergence rates, which is stated in Theorem 4 formally.

Theorem 4. Consider the oblivious variant of 1-SFW outlined in Algorithm 2, and assume that
the conditions in Assumptions 1, 2 and 6 hold. Then we have

1. If F is convex on K, and we set ρt = (t − 1)−1 and ηt = t−1, then the output is feasible

xT +1 ∈ K and satisﬁes

E[F (xT +1) − F (x∗)] ≤ O(T −1/2).

2. If F is non-convex, and we set ρt = (t − 1)−2/3, and ηt = T −2/3, then the output is feasible

xo ∈ K and satisﬁes

E[G(xo)] ≤ O(T −1/3).

3. If F is monotone DR-submodular on K, and we set ρt = (t − 1)−1 and ηt = T −1, then the

output is a feasible point xT +1 ∈ K and satisﬁes

E[F (xT +1)] ≥ (1 − e−1)F (x∗) − O(T −1/2).

Theorem 4 shows that the oblivious version of 1-SFW requires at most O(1/(cid:15)2) stochastic
oracle queries to ﬁnd an (cid:15)-suboptimal solution for convex minimization, at most O(1/(cid:15)2) stochastic
gradient evaluations to achieve a (1 − 1/e) − (cid:15) approximate solution for monotone DR-submodular
maximization, and at most O(1/(cid:15)3) stochastic oracle queries to ﬁnd an (cid:15)-ﬁrst-order stationary
point for nonconvex minimization.

6 Conclusion

In this paper, we studied the problem of solving constrained stochastic optimization programs
using projection-free methods. We proposed the ﬁrst stochastic variant of the Frank-Wolfe
method, called 1-SFW, that requires only one stochastic sample per iteration while achieving
the optimal (or best known) complexity bounds for (non-)convex minimization and monotone
DR-submodular maximization. In particular, we proved that 1-SFW achieves the optimal oracle
complexity of O(1/(cid:15)2) for reaching an (cid:15)-suboptimal solution in the stochastic convex setting, and
a (1 − 1/e) − (cid:15) approximate solution for a stochastic monotone DR-submodular maximization
problem. Moreover, in a non-convex setting, 1-SFW ﬁnds an (cid:15)-ﬁrst-order stationary point after at
most O(1/(cid:15)3) iterations, achieving the best known overall complexity.

13

A Proof of Lemma 2

Proof. Let At = (cid:107)∇F (xt) − dt(cid:107)2. By deﬁnition, we have

At = (cid:107)∇F (xt−1) − dt−1 + ∇F (xt) − ∇F (xt−1) − (dt − dt−1)(cid:107)2.

Note that

dt − dt−1 = −ρtdt−1 + ρt∇ ˜F (xt, zt) + (1 − ρt) ˜∆t,

and deﬁne ∆t = ∇F (xt) − ∇F (xt−1), we have

At = (cid:107)∇F (xt−1) − dt−1 + ∆t − (1 − ρt) ˜∆t − ρt∇ ˜F (xt, zt) + ρtdt−1(cid:107)2

= (cid:107)∇F (xt−1) − dt−1 + (1 − ρt)(∆t − ˜∆t) + ρt(∇F (xt) − ∇ ˜F (xt, zt) + ρt(dt−1 − ∇F (xt−1)))(cid:107)2
= (cid:107)(1 − ρt)(∇F (xt−1) − dt−1) + (1 − ρt)(∆t − ˜∆t) + ρt(∇F (xt) − ∇ ˜F (xt, zt))(cid:107)2

Since ˜∆t is an unbiased estimator of ∆t, E[At] can be decomposed as

E[At] = E{(1 − ρt)2(cid:107)∇F (xt−1) − dt−1(cid:107)2+(1 − ρt)2(cid:107)∆t − ˜∆t(cid:107)2+ρ2

t (cid:107)∇F (xt) − ∇ ˜F (xt, zt)(cid:107)2

+ 2ρt(1 − ρt)(cid:104)∇F (xt−1) − dt−1, ∇F (xt) − ∇ ˜F (xt, zt)(cid:105)
+ 2ρt(1 − ρt)(cid:104)∆t − ˜∆t, ∇F (xt) − ∇ ˜F (xt, zt)(cid:105)}.

Then we turn to upper bound the items above. First, by Lemma 1, we have

E[(cid:107) ˜∆t − ∆t(cid:107)2] = E[(cid:107) ˜∇2
≤ E[(cid:107) ˜∇2
= E[(cid:107) ˜∇2
≤ η2
≤ η2

t−1D2E[(cid:107) ˜∇2
t−1D2 ¯L2.

t (cid:107)2]

t (xt − xt−1) − (∇F (xt) − ∇F (xt−1))](cid:107)2]
t (xt − xt−1)(cid:107)2]
t (ηt−1(vt−1 − xt−1))(cid:107)2]

By Jensen’s inequality, we have

E[(cid:107) ˜∆t − ∆t(cid:107)] ≤

(cid:113)

E[(cid:107) ˜∆t − ∆t(cid:107)2] ≤ ηt−1D ¯L,

and

E[(cid:107)∇F (xt) − dt(cid:107)] =

(cid:112)E[(cid:107)∇F (xt) − dt(cid:107)2] = (cid:112)E[At].

(13)

(14)

(15)

(16)

Note that zt is sampled according to p(z; xt(a)), where xt(a) = axt + (1 − a)xt−1. Thus
∇ ˜F (xt, zt) is NOT an unbiased estimator of ∇F (xt) when a (cid:54)= 1, which occurs with probability 1.
However, we will show that ∇ ˜F (xt, zt) is still a good estimator. Let Ft−1 be the σ-ﬁeld generated
by all the randomness before round t, then by Law of Total Expectation, we have

E[2ρt(1 − ρt)(cid:104)∇F (xt−1) − dt−1, ∇F (xt) − ∇ ˜F (xt, zt)(cid:105)]

=E[E[2ρt(1 − ρt)(cid:104)∇F (xt−1) − dt−1, ∇F (xt) − ∇ ˜F (xt, zt)(cid:105)|Ft−1, xt(a)]]
=E[2ρt(1 − ρt)(cid:104)∇F (xt−1) − dt−1, E[∇F (xt) − ∇ ˜F (xt, zt)|Ft−1, xt(a)](cid:105)],

(17)

14

where

E[∇F (xt) − ∇ ˜F (xt, zt)|Ft−1](cid:105)] = ∇F (xt) − ∇F (xt(a)) + ∇F (xt(a)) − E[∇ ˜F (xt, zt)|Ft−1, xt(a)].

By Lemma 1, F is ¯L-smooth, thus

(cid:107)∇F (xt) − ∇F (xt(a))(cid:107)≤ ¯L(cid:107)xt − xt(a)(cid:107)= ¯L(1 − a)(cid:107)ηt−1(vt−1 − xt−1)(cid:107)≤ ηt−1D ¯L.

We also have

(cid:107)∇F (xt(a)) − E[∇ ˜F (xt, zt)|Ft−1, xt(a)](cid:107) = (cid:107)
(cid:90)

(cid:90)

[∇ ˜F (xt(a); z) − ∇ ˜F (xt; z)]p(z; xt(a))dz(cid:107)

≤

(cid:107)∇ ˜F (xt(a); z) − ∇ ˜F (xt; z)(cid:107)p(z; xt(a))dz

(cid:90)

≤

L(cid:107)xt(a) − xt(cid:107)p(z; xt(a))dz

≤ ηt−1DL,

where the second inequality holds because of Assumption 4. Combine the analysis above with
Eq. (17), we have

E[2ρt(1 − ρt)(cid:104)∇F (xt−1) − dt−1, ∇F (xt) − ∇ ˜F (xt, zt)(cid:105)]

≤E[2ρt(1 − ρt)(cid:107)∇F (xt−1) − dt−1(cid:107)·(cid:107)E[∇F (xt) − ∇ ˜F (xt, zt)|Ft−1](cid:107)]
≤2ρt(1 − ρt)E[(cid:107)∇F (xt−1) − dt−1(cid:107)] · (ηt−1D ¯L + ηt−1DL)
≤2ηt−1ρt(1 − ρt)(cid:112)E[At−1]D( ¯L + L).

Finally, by Assumption 3, we have (cid:107)∇F (xt) − ∇ ˜F (xt, zt)(cid:107)≤ 2G. Thus

t (cid:107)∇F (xt) − ∇ ˜F (xt, zt)(cid:107)2≤ 4ρ2
ρ2

t G2,

(18)

(19)

and
E[2ρt(1 − ρt)(cid:104)∆t − ˜∆t, ∇F (xt) − ∇ ˜F (xt, zt)(cid:105)] ≤ E[2ρt(1 − ρt)(cid:107)∆t − ˜∆t(cid:107)·(cid:107)∇F (xt) − ∇ ˜F (xt, zt)](cid:107)

≤ 4ηt−1ρt(1 − ρt)GD ¯L.

(20)

Combine Eqs. (13), (14) and (18) to (20), we have

E[At] ≤ (1 − ρt)2E[At−1] + (1 − ρt)2η2
+ 4ηt−1ρt(1 − ρt)GD ¯L.

t−1D2 ¯L2 + ρ2

t 4G2 + 2ηt−1ρt(1 − ρt)(cid:112)E[At−1]D( ¯L + L)

For the simplicity of analysis, we replace t by t + 1, and have

E[At+1]

≤(1 − ρt+1)2E[At] + (1 − ρt+1)2η2
+ 4ηtρt+1(1 − ρt+1)GD ¯L

t D2 ¯L2 + ρ2

t+14G2 + 2ηtρt+1(1 − ρt+1)(cid:112)E[At]D( ¯L + L)

(21)

≤(1 −

1
tα )2E[At] +

D2 ¯L2 + 4G2 + 4GD ¯L
t2α

+

2D( ¯L + L)
t2α

(cid:112)E[At].

15

We claim that E[At] ≤ Ct−α, and prove it by induction. Before the proof, we ﬁrst analyze one
item in the deﬁnition of C : 2(2G+D ¯L)2
2−2−α−α . Deﬁne h(α) = 2−2−α −α. Since h(cid:48)(α) = 2−α ln(2)−1 ≤ 0
2−2−α−α ≤ 4.
for α ∈ (0, 1], so 1 = h(0) ≥ h(α) ≥ h(1) = 1/2 > 0, ∀α ∈ (0, 1]. As a result, 2 ≤

2

When t = 1, we have

E[A1] = E[(cid:107)∇F (x1) − ∇ ˜F (x1; z1)(cid:107)2] ≤ (2G)2 ≤

When t = 2, since ρ2 = 1, we have

E[A2] = E[(cid:107)∇ ˜F (x2, z2) − ∇F (x2)(cid:107)2] ≤ (2G)2 ≤

2(2G + D ¯L)2
2 − 2−α − α

/1 ≤ C · 1−α

2(2G + D ¯L)2
2 − 2−α − α

/2 ≤ C · 2−α.

Now assume for t ≥ 2, we have E[At] ≤ Ct−α, by Eq. (21) and the deﬁnition of C, we have

E[At+1] ≤ (1 −

1
tα )2 · Ct−α +
≤ Ct−α − 2Ct−2α + Ct−3α +

(2G + D ¯L)2
t2α

+

2D( ¯L + L)
t(5/2)α
(2 − 2−α − α)C
2t2α

+

√

C

C3/4
t(5/2)α

≤

≤

≤

C
tα +
C
tα +
C
tα −

−2C + Ct−α + (2 − 2−α − α)C/2 + t−α/2C/C1/4
t2α
C[−2 + 2−α + (2 − 2−α − α)/2 + (2 − 2−α − α)/2]
t2α

(22)

αC
t2α .

Deﬁne g(t) = t−α, then g(t) is a convex function for α ∈ (0, 1]. Thus we have g(t+1)−g(t) ≥ g(cid:48)(t),
i.e., (t + 1)−α − t−α ≥ −αt−(α+1). So we have

C
tα −

αC
t2α ≤ C(t−α − αt−(1+α)) ≤ C(t + 1)−α.

(23)

Combine with Eq. (22), we have E[At+1] ≤ C(t + 1)−α. Thus by induction, we have E[At] ≤
Ct−α, ∀t ≥ 1.

B Proof of Lemma 3

The only diﬀerence with the proof of Lemma 2 is the bound for E(cid:107) ˜∆t − ∆t(cid:107). Speciﬁcally, we have

t (xt − xt−1) + ˜∇2
t (xt − xt−1)(cid:107)2] + E[(cid:107) ˜∇2

E[(cid:107) ˜∆t − ∆t(cid:107)2] = E[(cid:107) ˜∆t − ˜∇2
= E[(cid:107) ˜∆t − ˜∇2
≤ [D2L2δt(1 + ˜F (xt(a), zt))]2 + η2
≤ (1 + B)2L2
t−1D2 ¯L2.
≤ 4η2

t−1D2 ¯L2

t + η2

2D4δ2

t−1D2 ¯L2

t (xt − xt−1) − (∇F (xt) − ∇F (xt−1))](cid:107)2]

t (xt − xt−1) − (∇F (xt) − ∇F (xt−1))(cid:107)2]

Then by the analysis same to the proof of Lemma 2, we have

E[At+1] ≤ (1 −

1
tα )2E[At] +

4(D2 ¯L2 + G2 + GD ¯L)
t2α

+

4D( ¯L + L)
t2α

(cid:112)E[At],

and thus E[At+1] ≤ C(t + 1)−α, where C = max

(cid:26)

8(D2 ¯L2+G2+GD ¯L)
2−2−α−α

(cid:104)

,

2
2−2−α−α

16

(cid:105)4

, [4D( ¯L+L)]4

(cid:27)
.

C Proof of Theorem 1

First, since xt+1 = (1 − ηt)xt + ηtvt is a convex combination of xt, vt, and x1 ∈ K, vt ∈ K, ∀ t,
we can prove xt ∈ K, ∀ t by induction. So xT +1 ∈ K.

Then we present an auxiliary lemma.

Lemma 4. Under the condition of Theorem 1, in Algorithm 1, we have

F (xt+1) − F (x∗) ≤ (1 − ηt)(F (xt) − F (x∗)) + ηtD(cid:107)∇F (xt) − dt(cid:107)+

¯LD2η2
t
2

.

By Jensen’s inequality and Lemma 2 with α = 1, we have

E[(cid:107)∇F (xt) − dt(cid:107)] ≤

(cid:112)E[(cid:107)∇F (xt) − dt(cid:107)2] ≤

√
√

C
t

,

where C = max{4(2G + D ¯L)2, 256, [2D( ¯L + L)]4}. Then by Lemma 4, we have

E[F (xT +1) − F (x∗)]

≤(1 − ηT )E[F (xT ) − F (x∗)] + ηT DE[(cid:107)∇F (xT ) − dT (cid:107)] +

¯LD2η2
T
2

T
(cid:89)

(1 − ηi)E[F (x1) − F (x∗)] + D

=

T
(cid:88)

k=1

ηkE[(cid:107)∇F (xk) − dk(cid:107)]

T
(cid:89)

(1 − ηi)

i=k+1

T
(cid:89)

η2
k

(1 − ηi)

(24)

i=1

+

¯LD2
2

T
(cid:88)

k=1

≤ 0 + D

√

CD
T

=

T
(cid:88)

k=1

T
(cid:88)

k=1

Since

and

i − 1
i

+

¯LD2
2

T
(cid:88)

k−2

T
(cid:89)

k=1

i=k+1

i − 1
i

i=k+1

T
(cid:89)

√
√

C
k

k−1

i=k+1
¯LD2
2T

T
(cid:88)

k=1

1
√
k

+

k−1.

T
(cid:88)

k=1

1
√
k

≤

(cid:90) T

0

x−1/2dx = 2

√

T ,

T
(cid:88)

k=1

k−1 ≤ 1 +

(cid:90) T

1

x−1dx = 1 + ln T,

by Eq. (24), we have

E[F (xT +1) − F (x∗)] ≤

2

√
√

CD
T

+

¯LD2
2T

(1 + ln T ).

17

D Proof of Theorem 2

First, since xt+1 = (1 − ηt)xt + ηtvt is a convex combination of xt, vt, and x1 ∈ K, vt ∈ K, ∀ t,
we can prove xt ∈ K, ∀ t by induction. So xo ∈ K.

t = arg minv∈K(cid:104)v, ∇F (xt)(cid:105), then G(xt) = (cid:104)v(cid:48)

t − xt, −∇F (xt)(cid:105) =

Note that if we deﬁne v(cid:48)
t − xt, ∇F (xt)(cid:105). So we have

−(cid:104)v(cid:48)

F (xt+1)

(a)
≤ F (xt) + (cid:104)∇f (xt), xt+1 − xt(cid:105) +

= F (xt) + (cid:104)∇F (xt), ηt(vt − xt)(cid:105) +

(b)
≤F (xt) + ηt(cid:104)∇F (xt), vt − xt(cid:105) +

¯L
2

(cid:107)ηt(vt − xt)(cid:107)2

(cid:107)xt+1 − xt(cid:107)2
¯L
2
¯Lη2
t D2
2

¯Lη2
t D2
2
¯Lη2
t D2
2

= F (xt) + ηt(cid:104)dt, vt − xt(cid:105) + ηt(cid:104)∇F (xt) − dt, vt − xt(cid:105) +

(c)
≤F (xt) + ηt(cid:104)dt, v(cid:48)
= F (xt) + ηt(cid:104)∇F (xt), v(cid:48)

t − xt(cid:105) + ηt(cid:104)∇F (xt) − dt, vt − xt(cid:105) +
t − xt(cid:105) + ηt(cid:104)dt − ∇F (xt), v(cid:48)
¯Lη2
t D2
2

t − xt(cid:105)

+ ηt(cid:104)∇F (xt) − dt, vt − xt(cid:105) +

= F (xt) − ηtG(xt) + ηt(cid:104)∇F (xt) − dt, vt − v(cid:48)

t(cid:105) +

(d)
≤ F (xt) − ηtG(xt) + ηt(cid:107)∇F (xt) − dt(cid:107)(cid:107)vt − v(cid:48)
t(cid:107)+
¯Lη2
t D2
2

(e)
≤F (xt) − ηtG(xt) + ηtD(cid:107)∇F (xt) − dt(cid:107)+

,

¯Lη2
t D2
2
¯Lη2
t D2
2

where we used the fact that F is ¯L-smooth in inequality (a). Inequalities (b), (e) hold because
of Assumption 1.
Inequality (c) is due to the optimality of vt, and in (d), we applied the
Cauchy-Schwarz inequality.

Rearrange the inequality above, we have

ηtG(xt) ≤ F (xt) − F (xt+1) + ηtD(cid:107)∇F (xt) − dt(cid:107)+

¯Lη2
t D2
2

.

(25)

Apply Eq. (25) recursively for t = 1, 2, · · · , T , and take expectations, we attain the following

inequality:

T
(cid:88)

t=1

ηtE[G(xt)] ≤ F (x1) − F (xT +1) + D

T
(cid:88)

t=1

ηtE[(cid:107)∇F (xt) − dt(cid:107)] +

¯LD2
2

T
(cid:88)

t=1

η2
t .

By Jensen’s inequality Lemma 2 with α = 2/3, we have

E[(cid:107)∇F (xt) − dt(cid:107)] ≤

(cid:112)E[(cid:107)∇F (xt) − dt(cid:107)2] ≤

√

C
t1/3

,

18

where C = max{ 2(2G+D ¯L)2
4/3−2−2/3 ,

(cid:16)

2
4/3−2−2/3

(cid:17)4

, [2D( ¯L + L)]4}. Since ηt = T −2/3, we have

E[G(xo)] =

(cid:80)T

t=1

E[G(xt)]
T

≤

≤

=

1
T · T −2/3

[F (x1) − F (xT +1) + D

T
(cid:88)

T −2/3

√

1
T 1/3
2B + 3

[2B + D
√

CD/2

T 1/3

T 2/3 +

t=1
¯LD2
2T 1/3

]

,

CT −2/3 3
2
¯LD2
2T 2/3

+

√

C
t1/3

+

¯LD2
2

T
(cid:88)

t=1

T −4/3]

where the second inequality holds because (cid:80)T

t=1 t−1/3 ≤ (cid:82) T

0 x−1/3dx = 3

2 T 2/3.

E Proof of Theorem 3

First, since xt+1 = xt + ηtvt = xt + T −1vt, we have xT +1 =
∈ K. Also, be-
cause now (cid:107)xt+1 − xt(cid:107)= (cid:107)ηtvt(cid:107)≤ ηtR, (rather than ηtD), Lemma 2 holds with new constant
C = max{ 2(2G+R ¯L)2
, [2R( ¯L + L)]4}. Since α = 1, we have C = max{4(2G +
2−2−α−α ,
R ¯L)2, 256, [2R( ¯L + L)]4}. Then by Jensen’s inequality, we have

2
2−2−α−α

(cid:17)4

(cid:16)

(cid:80)T

t=1 vt
T

E[(cid:107)∇F (xt) − dt(cid:107)] ≤

(cid:112)E[(cid:107)∇F (xt) − dt(cid:107)2] ≤

√
√

C
t

.

We observe that

F (xt+1)

(a)
≥ F (xt) + (cid:104)∇F (xt), xt+1 − xt(cid:105) −

(cid:107)xt+1 − xt(cid:107)

¯L
2

¯L
2T 2 (cid:107)vt(cid:107)

(cid:104)∇F (xt) − dt, vt(cid:105) −

(cid:104)∇F (xt) − dt, vt(cid:105) −

¯LR2
2T 2
¯LR2
2T 2

= F (xt) +

(b)
≥ F (xt) +

(c)
≥ F (xt) +

= F (xt) +

(d)
≥ F (xt) +

(e)
≥ F (xt) +

(f )
≥ F (xt) +

(cid:104)∇F (xt), vt(cid:105) −

1
T
1
T

(cid:104)dt, vt(cid:105) +

(cid:104)dt, x∗(cid:105) +

1
T
1
T
1
T
1
T
F (x∗) − F (xt)
T
F (x∗) − F (xt)
T
F (x∗) − F (xt)
T

1
T
1
T
1
T
1
T

−

−

−

(cid:104)∇F (xt), x∗(cid:105) +

(cid:104)∇F (xt) − dt, vt − x∗(cid:105) −

(26)

¯LR2
2T 2

(cid:104)∇F (xt) − dt, −vt + x∗(cid:105) −

(cid:107)∇F (xt) − dt(cid:107)·(cid:107)−vt + x∗(cid:107)−

2R(cid:107)∇F (xt) − dt(cid:107)−

¯LR2
2T 2 ,

¯LR2
2T 2
¯LR2
2T 2

where inequality (a) holds because of the ¯L-smoothness of F , inequalities (b), (e) comes from
Assumption 1. We used the optimality of vt in inequality (c), and applied the Cauchy-Schwarz

19

inequality in(e). Inequality (d) is a little involved, since F is monotone and concave in positive
directions, we have

F (x∗)−F (xt) ≤ F (x∗∨xt)−F (xt) ≤ (cid:104)∇F (xt), x∗∨xt−xt(cid:105) = (cid:104)∇F (xt), (x∗−xt)∨0(cid:105) ≤ (cid:104)∇F (xt), x∗(cid:105).

Taking expectations on both sides of Eq. (26),

E[F (xt+1)] ≥ E[F (xt)] +

F (x∗) − E[F (xt)]
T

−

2R
T

√
√

C
t

−

¯LR2
2T 2 .

Or

F (x∗) − E[F (xt+1)] ≤ (1 −

1
T

)[F (x∗) − E[F (xt)]] +

√
√

C
t

2R
T

+

¯LR2
2T 2

Apply the inequality above recursively for t = 1, 2, · · · , T , we have

F (x∗) − E[F (xT +1)] ≤ (1 −

1
T

≤ e−1F (x∗) +

)T [F (x∗) − F (x1)] +
√

¯LR2
2T

,

+

C
4R
T 1/2
t=1 t−1/2 ≤ (cid:82) T

√

2R
T

C

T
(cid:88)

t=1

t−1/2 +

¯LR2
2T

where the second inequality holds since (cid:80)T

0 x−1/2dx = 2T 1/2. Thus we have

E[F (xT +1)] ≥ (1 − e−1)F (x∗) −

√

4R
C
T 1/2

−

¯LR2
2T

.

References

Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning, pages 699–707, 2016.

Francis Bach. Submodular functions:

from discrete to continous domains. arXiv preprint

arXiv:1511.00394, 2015.

Aurélien Bellet, Yingyu Liang, Alireza Bagheri Garakani, Maria-Florina Balcan, and Fei Sha. A
distributed frank-wolfe algorithm for communication-eﬃcient sparse learning. In Proceedings of
the 2015 SIAM International Conference on Data Mining, pages 478–486. SIAM, 2015.

An Bian, Kﬁr Levy, Andreas Krause, and Joachim M Buhmann. Continuous dr-submodular
maximization: Structure and algorithms. In Advances in Neural Information Processing Systems,
pages 486–496, 2017a.

An Bian, Baharan Mirzasoleiman, Joachim M. Buhmann, and Andreas Krause. Guaranteed
non-convex optimization: Submodular maximization over continuous domains. In AISTATS,
February 2017b.

An Bian, Joachim M Buhmann, and Andreas Krause. Optimal dr-submodular maximization and

applications to provable mean ﬁeld inference. arXiv preprint arXiv:1805.07482, 2018.

20

Gábor Braun, Sebastian Pokutta, and Daniel Zink. Lazifying conditional gradient algorithms. In
Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
pages 566–575, 2017.

Gruia Calinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a monotone
submodular function subject to a matroid constraint. SIAM Journal on Computing, 40(6):
1740–1766, 2011.

Chandra Chekuri, Jan Vondrák, and Rico Zenklusen. Submodular function maximization via the
multilinear relaxation and contention resolution schemes. SIAM Journal on Computing, 43(6):
1831–1879, 2014.

Lin Chen, Hamed Hassani, and Amin Karbasi. Online continuous submodular maximization. In

AISTATS, pages 1896–1905, 2018.

Lin Chen, Mingrui Zhang, Hamed Hassani, and Amin Karbasi. Black box submodular maximiza-

tion: Discrete and continuous settings. arXiv preprint arXiv:1901.09515, 2019.

Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex

sgd. arXiv preprint arXiv:1905.10018, 2019.

Aaron Defazio and Léon Bottou. On the ineﬀectiveness of variance reduced optimization for deep

learning. arXiv preprint arXiv:1812.04529, 2018.

Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
In Advances in Neural

optimization via stochastic path-integrated diﬀerential estimator.
Information Processing Systems, pages 687–697, 2018.

Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research

Logistics (NRL), 3(1-2):95–110, 1956.

Dan Garber and Elad Hazan. Faster rates for the frank-wolfe method over strongly-convex sets.

In ICML, volume 15, pages 541–549, 2015.

Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi. Gradient methods for submodular

maximization. arXiv preprint arXiv:1708.03949, 2017.

Hamed Hassani, Amin Karbasi, Aryan Mokhtari, and Zebang Shen. Stochastic conditional

gradient++. arXiv preprint arXiv:1902.06992, 2019.

Elad Hazan and Satyen Kale. Projection-free online learning.

In Proceedings of the 29th
International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June
26 - July 1, 2012, pages 1843–1850, 2012.

Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization. In

ICML, pages 1263–1271, 2016.

Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML, pages

427–435, 2013.

21

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS, pages 315–323, 2013.

Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Foundations

and Trends R(cid:13) in Machine Learning, 5(2–3):123–286, 2012.

Simon Lacoste-Julien. Convergence rate of frank-wolfe for non-convex objectives. arXiv preprint

arXiv:1607.00345, 2016.

Simon Lacoste-Julien and Martin Jaggi. On the global linear convergence of frank-wolfe op-
timization variants. In Advances in Neural Information Processing Systems, pages 496–504,
2015.

Jean Lafond, Hoi-To Wai, and Eric Moulines. D-fw: Communication eﬃcient distributed
algorithms for high-dimensional sparse optimization. In Acoustics, Speech and Signal Processing
(ICASSP), 2016 IEEE International Conference on, pages 4144–4148. IEEE, 2016.

G. Lan and Y. Zhou. Conditional gradient sliding for convex optimization. SIAM Journal on

Optimization, 26(2):1379–1409, 2016.

Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Conditional gradient method for stochastic

submodular maximization: Closing the gap. In AISTATS, pages 1886–1895, 2018a.

Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554,
2018b.

Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained
optimization. In Advances in Neural Information Processing Systems, pages 3629–3639, 2018c.

Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáč. Sarah: A novel method for
machine learning problems using stochastic recursive gradient. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 2613–2621. JMLR. org, 2017a.

Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáč. Stochastic recursive gradient

algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261, 2017b.

Sashank J Reddi, Suvrit Sra, Barnabás Póczos, and Alex Smola. Stochastic frank-wolfe methods
for nonconvex optimization. In 2016 54th Annual Allerton Conference on Communication,
Control, and Computing (Allerton), pages 1244–1251. IEEE, 2016.

Anit Kumar Sahu, Manzil Zaheer, and Soummya Kar. Towards gradient free and projection free

stochastic optimization. arXiv preprint arXiv:1810.03233, 2018.

Zebang Shen, Cong Fang, Peilin Zhao, Junzhou Huang, and Hui Qian. Complexities in projection-
free stochastic non-convex minimization. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pages 2868–2876, 2019a.

Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy

gradient. In International Conference on Machine Learning, pages 5729–5738, 2019b.

22

Matthew Staib and Stefanie Jegelka. Robust budget allocation via continuous submodular

functions. In ICML, pages 3230–3240, 2017.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,

2018.

Jan Vondrák. Optimal approximation for the submodular welfare problem in the value oracle

model. In STOC, pages 67–74. ACM, 2008.

Yu-Xiang Wang, Veeranjaneyulu Sadhanala, Wei Dai, Willie Neiswanger, Suvrit Sra, and Eric
In International

Xing. Parallel and distributed block-coordinate frank-wolfe algorithms.
Conference on Machine Learning, pages 1548–1557, 2016.

Alp Yurtsever, Suvrit Sra, and Volkan Cevher. Conditional gradient methods via stochastic
path-integrated diﬀerential estimator. In International Conference on Machine Learning, pages
7282–7291, 2019.

Mingrui Zhang, Lin Chen, Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Quantized frank-
wolfe: Communication-eﬃcient distributed optimization. arXiv preprint arXiv:1902.06332,
2019.

Wenjie Zheng, Aurélien Bellet, and Patrick Gallinari. A distributed frank–wolfe framework for
learning low-rank matrices with the trace norm. Machine Learning, 107(8-10):1457–1475, 2018.

Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduced gradient descent
for nonconvex optimization. In Advances in Neural Information Processing Systems, pages
3921–3932, 2018.

23

