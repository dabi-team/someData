Optimal Multiple Stopping Rule for Warm-Starting Sequential Selection

Mathilde Fekom Nicolas Vayatis Argyris Kalogeratos

0
2
0
2

b
e
F
2
1

]
I

A
.
s
c
[

1
v
0
6
1
5
0
.
2
0
0
2
:
v
i
X
r
a

for a variant of

Abstract— In this paper we present the Warm-starting Dy-
namic Thresholding algorithm, developed using dynamic pro-
gramming,
the standard online selection
problem. The problem allows job positions to be either free or
already occupied at the beginning of the process. Throughout
the selection process, the decision maker interviews one after
the other the new candidates and reveals a quality score for
each of them. Based on that information, she can (re)assign
each job at most once by taking immediate and irrevocable
decisions. We relax the hard requirement of the class of dynamic
programming algorithms to perfectly know the distribution
from which the scores of candidates are drawn, by presenting
extensions for the partial and no-information cases, in which
the decision maker can learn the underlying score distribution
sequentially while interviewing candidates.

I. INTRODUCTION

Sequential Selection Problems (SSPs) occur in several real-
life situations. A common characteristic of such decision pro-
cesses is that decisions need to be immediate and irrevocable.
For instance, at an emergency healthcare unit, patients arrive
sequentially seeking for medical help [8]. Other examples are
found in kidney exchanges [4], auctions [9], robotic sampling
[5], and most straightforwardly in recruitment processes that
involve sequential interviews of job-seekers by a decision
maker (DM), who immediately takes hiring or rejection
decisions [3]. For clarity, we adopt the terminology of the
latter problem, e.g. candidates are interviewed, hiring means
selecting, ﬁring is deselecting, etc.

In the most well-known SSP, the secretary problem [7],
[10], n ∈ N∗ candidates are sequentially interviewed for a
single job position, and a hire puts an end to the recruitment
process. Its multi-choice extension, also called multiple
stopping problem, has been extensively studied [9], [2], [1],
and terminates when the desired number of b ∈ N∗ hired have
been decided. In the hiring problem [3], the DM’s task is to
grow the company as much as possible while keeping maximal
the average score of the hired employees; therefore, there is
no limit in the number of job positions. The hiring-above-the-
mean policy was proposed, which selects a candidate when
his score exceeds the average score of the current employees
(changes over time). This policy is an efﬁcient and easy to
implement, yet notably dependent on the score distribution
and sensitive to the quality of the ﬁrst hires.

What we call as the full-information case, assumes that
the distribution from which the scores of candidates are
sampled is known to the DM. In [11], dynamic programming

authors
CNRS,

are with Universit´e Paris-Saclay, ENS Paris-
France.
F-94235,
Centre

(cid:3) The
Saclay,
Cachan,
Emails: {fekom, vayatis, kalogeratos}@cmla.ens-cachan.fr.
(cid:3) Part of this work was funded by the French Railway Company, SNCF,
and the IdAML Chair hosted at ENS Paris-Saclay.

Borelli,

is employed to compute the optimal stopping rule for each
candidate, i.e. the score threshold to beat in order to be hired.
This threshold depends on his arrival time and on the number
of jobs positions that remain to be ﬁlled.

One important limitation of all the aforementioned settings
is that they begin with an empty selection set. The case of
performing a selection starting with a set of items at hand
was only recently introduced in [6]. There, a preselection
is composed of the pre-existing items that can then be
replaced, by selecting better items from those that appear
sequentially, to produce the ﬁnal selection. More speciﬁcally,
the authors present the so-called Warm-starting Sequential
Selection Problem (WSSP) where the DM can perform at
most one update per job position. This setting can also suit
well to multi-round applications where the output of a round
is the input of the subsequent one.

The Cutoff-based Cost Minimization algorithm from the
same work [6] makes no assumption on the distribution
of candidates’ scores. It splits the process into two phases:
the learning phase in which the DM learns an acceptance
threshold from candidates that are getting rejected by default,
and thereafter, the selection phase which uses that threshold
to accept or reject from the rest candidates. Despite its biased
rule, which is due to the disregard of the early-arriving
candidates, the strategy can be efﬁcient and robust to score
changes, provided an appropriately set learning phase size.
Important to note, when the score distribution is known, the
cutoff-based strategies are not optimal and policies derived
from dynamic programming are more suitable. To the best of
our knowledge, there exists no such warm-starting strategy
in the literature.

Contribution. Inspired by the warm-starting aspect of the
WSSP [6], we propose the Warm-starting Dynamic Thresh-
olding (WDT) algorithm that attributes to each incoming
candidate a threshold value to beat according to: i) its arrival
time, ii) the current number of empty job positions, and iii)
the current number of positions occupied by initial employees
(i.e. available employees that can be replaced). The threshold
value to beat is computed by means of dynamic programming,
adapted to the warm-starting scenario. The algorithm is easy
to implement and gives each candidate a chance to be hired
regardless his arriving time. WDT’s downside lies in the
assumption that the score distribution is known. We relax
this requirement by ﬁrst considering the partial information
case where the DM only knows the nature of the distribution,
and then the no-information case where the threshold is
purely rank-based. We show with simulations that the WDT
outperforms existing methods in the warm-starting scenario.

 
 
 
 
 
 
II. SETTING

The Sequential Selection Problem (SSP) of interest is the
following: n ∈ N∗ candidates are sequentially interviewed
by a decision maker (DM) who has the task of managing a
limited budget of b ∈ N∗, b ≤ n, interchangeable job positions.
Initially, r ≤ b positions are empty as some employees have
become permanently unavailable, while the rest b − r are
initially occupied by existing employees that form a set called
preselection. Available and unavailable employees constitute
a reference set for the DM. Once candidate j arrives, the
DM observes his score Sj ∈ R, which, like all scores, it is
assumed to follow a known distribution fS. In our notations,
an added dot on top of a variable refers to the preselection, e.g.
˙S = ( ˙S(1), ˙S(2), ..., ˙S(b−r)) gives the score of each preselected
employee. For convenience, the latter vector is considered
˙S(1) and ˙S(b−r) are
to be sorted in decreasing order, i.e.
respectively the best and the worst scores of the preselected
employees. Here are the basic rules of the game:
R1. Each decision (i.e. hire or not) shall be immediate and
irrevocable, which means that every position can be
assigned (or reassigned) at most once throughout the
selection process.

R2. The DM must at least ﬁll the empty job positions by

hiring r candidates.

R3. Once no empty position is left, the DM can still hire an
incoming candidate by removing one of the employees
of the preselection from his position (only ﬁre on hire).
The following deﬁnition is a score-based adaptation of
the Warm-starting Sequential Selection Problem (WSSP)
proposed in [6], where here the DM initially knows also
the distribution from which candidates’ scores are drawn.

Deﬁnition 1: Distribution-aware WSSP is the online selec-

tion process described by elements of three categories:
1) Background

B = (b, n, fS, ˙S): collection of elements initially at DM’s

knowledge or disposition, including:

• n ∈ N∗: ﬁnite number of candidates to appear;
• b ∈ N∗, b ≤ n: number of resources;
• fS: the distribution generating candidates’ scores;
• ˙S = ( ˙S(1), ..., ˙S(b−r)) ∈ Rb−r;

˙S(j) ∼ fS, ∀j: scores of

the preselection.

2) Process

• S = (S1, ..., Sn) ∈ Rn; Sj ∼ fS, ∀j: candidates’ scores;
• A = (A1, ..., An) ∈ {0, 1}n: sequence of decisions for
candidates, i.e. Aj = 1 if the j-th candidate gets hired.

3) Evaluation

• The reward is evaluated at the end of the process by:

φB(S, A) =

b−r
(cid:88)

i=1

˙S(i)

˙A(i),n +

n
(cid:88)

j=1

SjAj ≥ 0,

(1)

where ˙A(i),n ∈ {0, 1} indicating if the i-th preselected
employee kept his position after n candidate interviews.
The evaluation criterion to maximize is therefore the
expectation of the above reward function, E[φB(S, A)]. Note

Fig. 1. Demonstration of the WDT algorithm. The score of an incoming
candidate, Sj , is compared to an associated acceptance threshold T X,Y
,
where X and Y are respectively the current number of empty positions and
that of positions occupied by preselected employees. Accepted candidates
(blue circle) ﬁrst ﬁll empty positions and then, if they are competitive enough,
take a preselected employee’s positions (e.g. the 5-th candidate).

j

that, since the DM is forced to ﬁll the b positions by the end
of the process, it holds: (cid:80)b−r
i=1

˙A(j),n + (cid:80)n

j=1 Aj = b.

III. WARM-STARTING DYNAMIC THRESHOLDING

Here, we present the Warm-starting Dynamic Thresholding
(WDT) method to solve optimally the distribution-aware
WSSP of Deﬁnition 1. Without loss of generality, we consider
non-negative i.i.d. scores Sj ≥ 0, ∀j, and that the best- (resp.
worst-) skilled individual has the highest (resp. lowest) score.

A. Threshold-based algorithm

The idea behind the strategy is to ﬁnd the optimal
acceptance threshold Tj ∈ R that the j-th arriving candi-
date should beat in order to be hired, ∀j ∈ {1, ..., n}; see
the example of Fig. 2. Note that, in order to be optimal,
this threshold must depend on the state of the ongoing
selection process. Therefore, we write Tj = T Xj ,Yj
, where
Xj ∈ {0, ..., r} positions are still empty and Yj ∈ {0, ..., b−r}
jobs are still occupied by preselected employees, i.e. after
j − 1 ∈ N∗ interviews and while the j-th candidate is being
interviewed. Using the notations of Deﬁnition 1 we get:

j

(cid:32)

Xj := max

r −

j−1
(cid:88)

i=1

(cid:33)

Ai, 0

and Yj :=

r
(cid:88)

i=1

˙A(i),j.

(2)

To simplify our notations, we omit the dependency of Xj
and Yj to j, and we simply write X and Y to refer to their
value at the implied step j of the selection process.

Value function. The fundamental question remains how to
compute the thresholds optimally. The solution can be found
via dynamic programming where the value function, which
is the expected value of the regret here, is computed for each
possible scenario. Let V X,Y
∈ R be this value function when
j − 1 ∈ N∗ candidates have been interviewed so far.

j

Before going into the method’s details, we can deduce the
following remarks from the rules that constrain the process:

S1S22,3 1T2,3 2T1,3 3T0,3 4T0,2 5T0,2 6T0,1 7T0,1 8T…SScore to beat for cand. j when X jobs are empty  and Y preselected have a job rejected/accepted candidate with score SLegendEmptyFilled by a preselectedJob positionsFilled by a candidate$T_1^{2,3}$ \hspace{0.0mm}  $T_2^{2,3}$  \hspace{0.0mm}  $T_3^{1,3}$ \hspace{0.0mm} $T_4^{0,3}$ \hspace{0.0mm} $T_5^{0,2}$ \hspace{0.0mm} $T_6^{0,2}$ \hspace{0.0mm} $T_7^{0,1}$ \hspace{0.0mm} $T_8^{0,1}$/Jobs:S3S4S6S8S5S7SRemark 1: The second rule, R2, implies that if X ≥ n−j,
then V X,Y
= 0, ∀ Y . Thus, the last incoming candidates might
j
be accepted by default, and this leads to X = 0 when j = n.
Remark 2: R3 implies that if X (cid:54)= 0, then Y = b − r, ∀j.
Remark 3: From R2 and R3 we deduce that X and Y are
non-increasing with j: throughout the process the number of
job positions assigned to candidates cannot decrease, while
those for the preselected employees cannot increase.

j

To compute V X,Y

we work by means of backward induc-
tion. First, we consider the extreme case that n candidates
are automatically rejected, and compute the value function
V r,b−r
. Here, either X = 0 and r = 0, i.e. each position is
n
occupied by a preselected employee, or V r,b−r
= 0 because
one or more positions are empty. The induction then goes
to the ﬁrst non-zero value function, which is when n − r
candidates get automatically rejected: there, the only option
is to hire the r last candidates to arrive (see Remark 1), hence:

n



V r,b−r
n−r+1 = E



n
(cid:88)

b−r
(cid:88)

Sj +



˙S(i)

 = rµ +

j=n−r+1

i=1

b−r
(cid:88)

i=1

˙S(i),

(3)

where µ is the mean of the known score distribution fS.

n−r

One step back, V r,b−r

is evaluated by accounting every
option: either the (n − r)-th candidate gets rejected and the
last r candidates get hired, leading to the reward of V r,b−r
n−r+1
(Eq. 3), or the (n−r)-th candidate gets hired and in this case
we should, again, present two options. The DM can lower by
one either the number of empty job positions, or the number
of preselected employees that will keep their positions at
the end of the selection. This reasoning is generalized to the
following recurrent value functions ∀j ∈ {1, ...n}:

V X,Y
j = E

(cid:104)
max

(cid:16)

V X,Y
j+1 , Sj + max(V X−1,Y

j+1

, V X,Y −1
j+1

(cid:17)(cid:105)
)

. (4)

The ﬁrst term in the outer max(·) corresponds to the option
of rejecting the j-th candidate, while the second term is the
option of accepting him. Two observations can be made: i) if
the goal is to minimize instead of maximize the objective, then
max(·) should be replaced by min(·) functions; ii) results
of [11], and speciﬁcally their Theorem 3 in Sec. 3, can be
retrieved by setting Y = 0 that implies V X,Y −1

= 0.

j+1

Recurrence relation. Thanks to our backward induction
formulation, we enunciate a generic formula for the recurrence
relation of the value function.

Proposition 1: Let a WSSP with a population of i.i.d.
scores S ∈ [α, β], each drawn from a known distribution fS
with cumulative distribution function FS(x) = (cid:82) x
α fS(y)dy.
Having processed j−1 interviews, X ∈ {0, ..., r} job positions
are empty and Y ∈ {0, ..., b − r} are occupied by preselected
employees. Then, the recurrence relation of Eq. 4 becomes:

j

j+1 − max(V X−1,Y

Algorithm 1 Warm-starting Dynamic Thresholding (WDT)
Input: the evaluation table of the value function V X,Y
where b, r, and
n are the numbers of resp. job positions, initially empty among them, and
˙A0 = ( ˙A(1),0, ..., ˙A(b−r),0) = (1, ..., 1)
sequentially incoming candidates;
is the initial status of the preselected employees.
Output: the set of ﬁnal job assignments ˙An ∈ {0, 1}b−r, and A ∈ {0, 1}n.
1: X ← r
// nb. of empty jobs
2: Y ← b − r
// nb. of jobs occupied by preselected employees
3: for j = 1 to n do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for

Tj ← V X,Y
if Sj > Tj then
Aj ← 1
if X > 0 then

// remove job from a preselected employee

˙A(Y ),j ← 0
Y ← Y − 1

// ﬁll one empty position

X ← max(X − 1, 0)

// see Proposition 2

// accept candidate

, V X,Y −1
j+1

Aj ← 0

end if

end if

else

else

j+1

)

The proof1 uses max(A, B) = A1{A > B} + B1{A ≤ B} in
order to compute the expectation.

Optimal threshold. Evidently, the j-th candidate must be
accepted if the value function associated to the option of
accepting him is larger than that of rejecting him. According
to Eq. 4, the latter amounts to choosing the optimal threshold
deﬁned in the following proposition that maximize the
expectation of the reward.

Proposition 2: Let a WSSP where j − 1 interviews have
been processed, X ∈ {0, ..., r} job positions are empty, and
Y ∈ {0, ..., b−r} are occupied by preselected employees. The
optimal acceptance threshold for candidate j is deﬁned as:
j+1 − max(V X−1,Y

, V X,Y −1
j+1

= V X,Y

T X,Y
j

(6)

j+1

).

Essentially, the j-th candidate is accepted if his score beats
the corresponding threshold, i.e. Aj = 1{Sj > T X,Y

}.

j

The WDT procedure works with the optimal threshold

described above and is described in Algorithm 1.

B. Special case: uniform distribution

Consider that scores are uniformly distributed in [α, β].
The cumulative and probability density functions being easily
computed, allows the recurrence relation to get simpliﬁed.

Proposition 3: Set Sj ∼ U(α, β), ∀j ≤ n. Then, Proposi-

tion 1 becomes:

V X,Y
j

− V X,Y

j+1 =

− Z X,Y
j+1 ,

(7)

j+1 + β2

2

Z X,Y
j+1

− 2αZ X,Y
2(β − α)
j+1 − max(V X−1,Y

where Z X,Y

j+1 = V X,Y

, V X,Y −1
).
j+1
The proof?? uses the fact that FS(x) = x−α
β−α and fS(x) = 1
for a uniform distribution.

j+1

β−α

j − V X,Y
V X,Y

j+1 = Z X,Y

j+1

(cid:0)FS(Z X,Y

j+1 ) − 1(cid:1) +

(cid:90) β

ZX,Y
j+1

where Z X,Y

j+1 := V X,Y

j+1 − max(V X−1,Y

j+1

, V X,Y −1
j+1

).

sfS(s)ds, (5)

Example. Set α = 0 and β = 1, b = 3, r = 2, and n =
14. Tab. I displays the evaluation of the value function

1A separate Appendix, containing detailed proofs, can be found as a

supplement ﬁle.

Y X V X,Y

1

V X,Y
2

V X,Y
3

V X,Y
4

V X,Y
5

V X,Y
6

V X,Y
7

V X,Y
8

V X,Y
9

V X,Y
10

V X,Y
11

V X,Y
12

V X,Y
13

V X,Y
14

0

1

1
2

0
1
2

0.893
1.719

0.907
1.756
2.547

0.886
1.702

0.902
1.742
2.523

0.879
1.683

0.897
1.729
2.496

0.871
1.661

0.891
1.712
2.465

0.861
1.636

0.885
1.694
2.431

0.850
1.606

0.877
1.673
2.391

0.836
1.571

0.869
1.650
2.345

0.823
1.529

0.859
1.621
2.290

0.800
1.476

0.847
1.588
2.224

0.775
1.409

0.833
1.547
2.142

0.741
1.320

0.816
1.495
2.036

0.768
1.195

0.795
1.428
1.894

0.732
1.000

0.979
1.333
1.682

0.682
0.000

0.979
1.182
0.000

EVALUATION OF THE V X,Y

j

FUNCTION AT EACH STEP j = 1, ..., 14. THE DISTRIBUTION OF THE SCORES IS Sj ∼ U (0, 1), AND THERE IS b − r = 2 − 1 = 1
INITIALLY NON-EMPTY JOB POSITION OCCUPIED BY AN EMPLOYEE WITH SCORE ˙S(1) = 0.682.

TABLE I

V X,Y
at each step of the selection process. Recall that,
j
at the respective step j, X ∈ {1, ..., b} is the number of
empty job positions, and Y ∈ {0, 1} is the DM’s hiring
decision. Now, imagine the following sequence of scores:
S = (0.498, 0.858, 0.749, 0.398, ...) and ˙S = (0.682) is the
score of the preselected employee. We are determining the
acceptance threshold sequentially. Note that, since r (cid:54)= 0,
V 0,0
j = 0. For the ﬁrst incoming candidate, j = 1, r = 2 job
positions are empty, i.e. X = 2 and b − r = 1 position is
occupied by a preselected employee, i.e. Y = 1. The ﬁrst
candidate is rejected since T 2,1
) =
2.523 − max(1.742, 1.702) = 0.781 > 0.498. The second
threshold reads T 2,1
) = 2.496 −
max(1.729, 1.683) = 0.767 < 0.858, which allows the accep-
tance of the second candidate. The following threshold is
then, T 1,1
3 = 0.832, thus the third candidate is rejected. The
process continues this way until the sequence is ﬁnished.

3 − max(V 1,1

2 − max(V 1,1

2 = V 2,1

1 = V 2,1

, V 2,0
3

, V 2,0
2

3

2

Rank-based setting. When the score distribution is either
unknown (no-information case), or does not exhibit a closed-
form cumulative density function, the DM can only rely
on a relative evaluation of the candidates. In practice, she
can assign a relative rank to each incoming candidate by
comparing him to those already examined (let the best one
be ranked ﬁrst).

In that case, the acceptance threshold is rank-based, hence
T X,Y
stands for the absolute rank (which cannot be known,
j
though) that the j-th candidate needs to exceed to get selected.
Conveniently, the absolute ranks of a set follow a discrete
uniform distribution that exhibits a closed-form description.
Then, the threshold value for the j-th candidate is computed
using Proposition 2 and, following the same reasoning as in
Proposition 3, we get the following simpliﬁed expression:

V X,Y
j

= V X,Y

j+1 −

2

Z X,Y
j+1

− Z X,Y
j+1

2(n + b)

,

(8)

).

, V X,Y −1
j+1

where Z X,Y

j+1 = V X,Y

j+1 − min(V X−1,Y

j+1
As mentioned, the DM cannot know the absolute rank
of a candidate before ﬁnishing all interviews. She can still,
though, estimate it knowing his relative rank and by taking
into account the proportion of candidates that has already
been examined. More precisely, the j-th candidate has relative
j ∈ N∗ after the examination of j + b − r
rank denoted by Z rel
individuals (including the preselected employees), and the
absolute rank denoted by Z abs
that he would have after the
j
examination of n + b − r individuals. Hence, we set Z rel
j =

j+b−r
n+b−r Z abs
and, thereby, the practical threshold of relative
j
rank that the j-th candidate must exceed to be accepted is:

τ X,Y
j =

j + b − r
n + b − r
IV. SIMULATION RESULTS

T X,Y
j

.

(9)

A. Simulations parameters

The WSSP setting takes as input the reference set (contain-
ing available and unavailable employees) and the sequence of
candidates, and outputs a selection of size b. A very interesting
feature of this conﬁguration is that it enables multi-round
applications where the output selection of a round can be fed
as input for the consecutive round. For instance, it is natural
to imagine periodic or reoccurring recruitment processes
for large organizations that have several human resources to
manage. Inspired by [6], we thereby repeat the WSSP process
K ∈ N∗ times. Each repetition k ≤ K, called round, is in
essence a warm-starting selection. The round starts with b−r
available employees and assumes that unavailability occurs
uniformly at random among the b employees of the reference
set (those selected during the previous round). Overall, the
K rounds form a Multi-round Sequential Selection Problem
(MSSP) where the DM’s objective is the upkeep of a highly-
skilled group of employees throughout the entire process.

In the simulations, a population of N = 10000 job-seekers
is considered, and that n = 100 interviews take place in each
of the K = 10 considered rounds. The candidates’ scores
are drawn from a given distribution and remain ﬁxed during
the process. The preselected employees of the ﬁrst round
are chosen uniformly at random from the population, and
hence, carry an average quality score. We desire to compare
our online strategy, the WDT, to the best an ofﬂine strategy
achieves, i.e. in the case that the DM could examine the
candidates altogether as a batch. Therefore, instead of the
reward, in the ﬁgures we plot the regret deﬁned as φk =
|φoff,k − φk|, ∀k ≤ K, where φoff,k and φk are respectively
the ofﬂine and online reward.

B. Score-based setting

Fig. 2 displays the average regret φk in different settings,
namely Si,k ∼ U(0, 1) (top row) and Si,k ∼ Exp(1), ∀i, k
(bottom row). Let us start by focusing on the plain line curves,
one of which is the RAND baseline (grey line) that decides
for the hires at random. A ﬁrst straightforward observation
is that the average regret has similar inefﬁcient behavior for
both distributions. In the following description, we therefore
focus on the uniform distribution.

Secondly, the subﬁgures on the left assume r = 0, hence
the process always starts with b empty positions, whereas on
the right it is assumed r = b, thus the process starts with b
positions occupied by preselected employees. In the ﬁrst case,
since the employees do not quit their position in-between two
subsequent rounds, the DM cannot deteriorate the selection,
and might even improve the set of employees through time
by replacing initial employees with more skilled candidates.
The regret naturally goes to zero, and it does go faster for
the proposed WDT than for instance the MEAN or CCM∗
strategies. Note that CCM∗ is merely the Cutoff-based Cost
Minimization algorithm with optimal size of learning phase
[6]. In the second case (r = 0), the regret does not necessarily
converge towards zero; neither MEAN nor CCM∗ manage
to keep a low average regret. This phenomenon is explained
as follows: progressively, the average score of the employees
of the previous round gets so competitive for the incoming
candidates that it forces the DM to select the last-arriving ones
by default to prevent ending up with having empty positions
after all the interviews (see the rule R2 in Section II).

C. Full, partial, and no-information settings

In Fig. 2, we also show the performance of the algorithms
for cases with different levels of DM’s knowledge for the
score distribution fS. In the full-information case (plain line),
fS is perfectly known. In the partial information case (dashed
line), DM knows only the shape of the fS (e.g. uniform,
normal, exponential, etc.) and needs to learn its parameters
(e.g. lower and upper bounds, mean, etc.). Finally, the no-
information case (dotted line) is when the DM does not hold
any information about fS (e.g. the shape, as discussed in the
example of Section III-B), or not even a way to compute an
absolute score per candidate. In that case, the DM should
rather rely on relative ranks that are re-assessed after the
examination of each individual (see Section III-B).

We observe that, provided the DM knows the shape of
the score distribution (i.e. partial-information), the learning
of its parameters throughout the rounds is relatively fast,
and the regret quickly converges to that of full-information
(plain lines). Here, the strategy is much slower in converging
towards the full-information case, which is achieved for r = 0
after approximately 40 rounds. The slow convergence is due
to the DM’s inability to estimate each candidate’s absolute
rank before having examined all other candidates (see Eq. 9).

V. CONCLUSION AND DISCUSSION

In this paper we presented a new algorithm, called Warm-
starting Dynamic Thresholding (WDT), for the Warm-starting
Sequential Selection Problem (WSSP), considering the case
where the incoming candidates have scores following a known
distribution. The proposed algorithm is based on a dynamic
programming approach and achieves optimal threshold esti-
mation at each step of the sequence of interviewed candidates.
Experiments have been performed in the multi-round setting,
which is interesting for real-world reoccurring recruitment
processes. WDT demonstrated a clearly better performance
than existing algorithms, regardless the number of initially

(c) no resignations (r = 0)

(d) max resignations (r = b)

Fig. 2. Average regret φk versus the round number k. The number of job
positions is b = 5 for n = 100 candidates per round. Score distribution:
uniform, Si,k ∼ U (0, 1) (top row); exponential, Si,k ∼ Exp(1), ∀i, ∀k
(bottom row). The number of initially empty job positions is r = 0 (left)
and r = b = 5 (right). Purple lines use WDT in full-information (plain line),
partial-information (dashed line) and no-information (dotted line) settings.

empty job positions. We additionally proposed a rank-based
dynamic programming alternative that can go beyond the
need of knowing perfectly the distribution that generates the
scores, yet, resulting in satisfying outcomes.

REFERENCES

[1] M. Babaioff, N. Immorlica, D. Kempe, and R. Kleinberg, “A knapsack

secretary problem with applications,” in APPROX-RANDOM, 2007.

[2] J. N. Bearden, A. Rapoport, and R. O. Murphy, “Experimental studies of
sequential selection and assignment with relative ranks,” in Behavioral
Decision Making, vol. 19, 2006, pp. 229–250.

[3] A. Z. Broder, A. Kirsch, R. Kumar, M. Mitzenmacher, E. Upfal, and
S. Vassilvitskii, “The hiring problem and lake Wobegon strategies,”
SIAM Journal on Computing, vol. 39, pp. 1223–1255, 2009.

[4] D. Chisca, M. Lombardi, M. Milano, and B. O’Sullivan, “From
ofﬂine to online kidney exchange optimization,” IEEE International
Conference on Tools with Artiﬁcial Intelligence, pp. 587–591, 2018.

[5] J. Das, F. Py, J. B. Harvey, J. P. Ryan, A. Gellene, R. Graham, D. A.
Caron, K. Rajan, and G. S. Sukhatme, “Data-driven robotic sampling for
marine ecosystem monitoring,” The International Journal of Robotics
Research, vol. 34, no. 12, pp. 1435–1452, 2015.

[6] M. Fekom, N. Vayatis, and A. Kalogeratos, “The warm-starting
sequential selection problem and its multi-round extension,” arXiv
preprint, 2019.

[7] T. S. Ferguson, “Who solved the secretary problem?” in Statistical

Science, 1989.

[8] A. Gnanlet and W. G. Gilland, “Sequential and simultaneous decision
making for optimizing health care resource ﬂexibilities,” Decision
Sciences, vol. 40, no. 2, pp. 295–326, 2009.

[9] R. Kleinberg, “A multiple-choice secretary algorithm with applications
to online auctions,” in ACM-SIAM Symposium on Discrete Algorithms,
2005, pp. 630–631.

[10] D. V. Lindley, “Dynamic programming and decision theory,” in Applied

Statistics, vol. 101, 1961, pp. 39–51.

[11] M. L. Nikolaev and G. Y. Sofronov, “A multiple optimal stopping rule
for sums of independent random variables,” in Discrete Mathematics
and Applications, vol. 17, no. 5, 2007.

024681000.10.20.30.40.5024681000.10.20.30.40.5024681000.511.522.53024681000.511.522.53