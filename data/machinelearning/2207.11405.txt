2
2
0
2

l
u
J

3
2

]

G
L
.
s
c
[

1
v
5
0
4
1
1
.
7
0
2
2
:
v
i
X
r
a

A New Approach to Drifting Games, Based on Asymptotically
Optimal Potentials

Zhilei Wang*

Robert V. Kohn†

Courant Institute of Mathematical Sciences, New York University

Abstract

We develop a new approach to drifting games, a class of two-person games with many applications
to boosting and online learning settings, including Prediction with Expert Advice and the Hedge game.
Our approach involves (a) guessing an asymptotically optimal potential by solving an associated partial
differential equation (PDE); then (b) justifying the guess, by proving upper and lower bounds on the ﬁnal-
time loss whose difference scales like a negative power of the number of time steps. The proofs of our
potential-based upper bounds are elementary, using little more than Taylor expansion. The proofs of our
potential-based lower bounds are also rather elementary, combining Taylor expansion with probabilistic
or combinatorial arguments. Most previous work on asymptotically optimal strategies has used potentials
obtained by solving a discrete dynamic programming principle; the arguments are complicated by their
discrete nature. Our approach is facilitated by the fact that the potentials we use are explicit solutions
of PDEs; the arguments are based on basic calculus. Not only is our approach more elementary, but
we give new potentials and derive corresponding upper and lower bounds that match each other in the
asymptotic regime.

Keywords— Drifting games, Boosting, Online learning algorithms, Prediction with Expert Advice, Hedge game,

Potential-based bounds, Asymptotically optimal bounds, Partial differential equation (PDE)

1 Introduction

This paper develops a fresh approach to the analysis of some drifting games. Our focus is on the identiﬁcation of
asymptotically optimal potential-based strategies for some versions of this repeated two-person game. Our approach
involves (a) guessing an asymptotically optimal potential by solving an associated PDE (which is in general highly
nonlinear); then (b) justifying the guess, by proving upper and lower bounds on the ﬁnal-time loss whose difference
scales like a negative power of the number of time steps. Our upper bounds are based on potential-based strategies
for the player, and our lower bounds are similarly based on strategies for the adversary. Their proofs are rather el-
ementary, using Taylor expansion and the explicit character of the potential. Most previous work on asymptotically
optimal strategies has used potentials obtained by solving a discrete dynamic programming principle, which is com-
plicated and sometimes intractable. Our approach is facilitated by the fact that our potentials are explicit and the
arguments are based on basic calculus. Not only is our approach more elementary, but we give new potentials and
derive corresponding upper and lower bounds that match each other in the asymptotic regime.

Let us brieﬂy describe the class of drifting games that we shall consider. They are repeated two-person games
involving a player and an adversary, whose interaction governs the positions of N “chips”. The game is determined
by

*zhileiwang92@gmail.com
†kohn@cims.nyu.edu; partial support from NSF grant DMS-2009746 is gratefully acknowledged.

1

 
 
 
 
 
 
(i) the number of chips N ;

(ii) the permitted moves, a subset Z of the real line;

(iii) a nonnegative parameter δ, whose role will be revealed in a moment;

(iv) the number of time steps T , and
(v) the loss function L : R → R+ ∪ {0}.

When the game begins, all the chips are located at 0. In each round of the game,

• the player announces a weight for each chip, i.e., a probability distribution p = (p1, . . . , pN ); then

• the adversary moves each chip by zi subject to the restrictions that (a) for each i, the displacement zi of the ith

chip belongs to the set Z, and (b) taken together, the displacements satisfy (cid:80)N

i=1 pizi ≥ δ.

When the game stops (after T time steps), the position si of the ith chip is the sum of all its moves, and the player’s
loss is 1
i=1 L(si). The player’s goal is to minimize its loss, and the adversary’s goal is to maximize it.
N

(cid:80)N

We shall focus mainly on four versions of this game:

V1. Z = {±1}, δ ≥ 0, and L(s) = 11{s≤0};

V2. Z = [−1, 1], δ ≥ 0, and L(s) = 11{s≤0};

V3. Z = {±1}, δ = 0, and L(s) = 11{s≤−R} for some R ≥ 0; and

V4. Z = [−1, 1], δ = 0, and L(s) = 11{s≤−R} for some R ≥ 0.

V1 is related to classical boosting (c.f. [Freund 1990; Schapire 1999, 2001; Schapire and Freund 2012]); V2 is
closely related to a boosting game where the weak learners not only make predictions but also show their conﬁdence
by giving a number between [−1, 1]; V3 and V4 are related to various online learning settings (see e.g. [Cesa-Bianchi
et al. 1996; Schapire 1999, 2001; Mukherjee and Schapire 2008; Luo and Schapire 2014]).

For V1 and V2, we shall assume δ ≤ 1 since each |zi| ≤ 1. Moreover, we always scale δ with T so as to keep

constant. This choice is required for the condition (cid:80) pizi ≥ δ to be meaningful in the limit T → ∞; we shall brieﬂy
explain why in Section 2.2, and offer a different perspective in Section 7.

γ =

δ2T
2

(1)

Similarly for V3 and V4, we scale R so that

γ =

R2
2T

(1’)

is a constant. This choice comes from the intuition that R plays the role of δT in V1 and V2.

While a systematic review is beyond the scope of this paper, let us say a little more about the literature. Drifting
games were ﬁrst introduced in [Schapire 1999, 2001], as an abstraction which generalizes the “majority-vote game”
considered in [Freund 1990]. Connections to boosting are surveyed in [Schapire and Freund 2012]. A number of
online learning problems can also be studied this way. For example in prediction with expert advice problem the
binomial weights algorithm can be viewed as a potential-based strategy for the player of V3 [Schapire 1999, 2001]
and a “continuous variant” is studied in V4 [Mukherjee and Schapire 2008]. A general mechanism for the design
of online learning algorithms based on drifting games is proposed in [Luo and Schapire 2014]; examples considered
there include the Hedge game, multi-armed bandit problems and online convex optimization.

It is natural to ask: can we identify optimal strategies for the two players in a drifting game? An afﬁrmative
answer based on dynamic programming was obtained in [Schapire 1999, 2001]. In these papers, the player’s strategy
is given explicitly in terms of a time-dependent potential that solves a dynamic programming principle. The adversary’s
strategy is not given explicitly; rather, its existence is proved by a probabilistic argument, provided that the number of
chips is sufﬁciently large.

It is also natural to ask: can we understand the optimal strategies, or ﬁnd simpler, more explicit strategies that are

asymptotically optimal in a suitable limit? The existing literature offers a few contributions of this type:

2

• The papers [Schapire 1999, 2001] considered several special cases, including our V1 and V3. It found the
potentials for V1 and V3 explicitly, and showed that the associated optimal player strategies are actually ones
that had already been considered in earlier work – the “boost-by-majority” and “binomial weights” strategy.

• The paper [Freund and Opper 2002] took a scaling limit of the dynamic programming principle, then solved the
resulting differential equation in order to ﬁnd the asymptotically optimal outcome when number of time steps
T → ∞. Their approach works under certain hypotheses upon the structure of the game (which assume that the
set of possible moves is discrete, and assure that the limiting differential is at leading order a constant-coefﬁcient
heat equation). Thus their approach is not suitable for V2 and V4.

• To deal with a continuous version of prediction with expert advice, [Mukherjee and Schapire 2008] returned
to game V4. For that example, the paper identiﬁed the potential and the associated optimal strategies, which
involves a suitable truncation of the binomial weights algorithm.

Our work is related to – but different from – the developments just summarized. Brieﬂy, we offer a fresh approach
to the identiﬁcation of asymptotically optimal strategies for the player and the adversary, which can be used even in
cases like Z = [−1, 1]. In essence, our idea is to (a) guess a potential, by solving a highly-nonlinear PDE that emerges
from something similar to the arguments of [Freund and Opper 2002], then (b) show directly, by an argument based
on Taylor expansion combined with the minimax character of the game, that associated potential-based strategies for
the player or the adversary are asymptotically optimal in the limit T → ∞. This idea is implemented here for the four
versions of the drifting game, and our results are

Theorem. For V1 and V3 (with γ deﬁned as in Equation (1) and (1’), respectively), there exist potential based player
strategies such that

1
N

N
(cid:88)

I=1

L(si) ≤

1
√
π

(cid:90) ∞

√

γ

e−x2

dx + Error ;

when N is sufﬁciently large, there also exist adversary strategies such that

1
N

N
(cid:88)

I=1

L(si) ≥

1
√
π

(cid:90) ∞

√

γ

e−x2

dx − Error .

For V2 and V4 (with γ deﬁned as in Equation (1) and (1’), respectively), there exist potential based player

strategies such that

1
N

N
(cid:88)

I=1

L(si) ≤

2
√
π

(cid:90) ∞

√

γ

e−x2

dx + Error ;

when N is sufﬁciently large, there also exist adversary strategies such that

1
N

N
(cid:88)

I=1

L(si) ≥

2
√
π

(cid:90) ∞

√

γ

e−x2

dx − Error .

Our potentials are, not surprisingly, continuous-time limits of the binomial weight-based strategies found in [Cesa-
Bianchi et al. 1996], [Schapire 1999, 2001] and [Mukherjee and Schapire 2008]. Our Taylor-expansion-based ap-
proach has, however, some advantages over the analyses in those papers; in particular, since the potential is explicit,
all its properties are immediately evident.

We are not the ﬁrst to connect drifting games with the solutions of suitable PDEs. Indeed, the paper [Freund 2001]
found an adaptive version of the boost-by-majority algorithm by considering the limit of the majority vote game when
δ → 0, in other words, when the advantage of each vote over random guessing decreases to zero while the number
of boosting rounds goes to inﬁnity. The paper found a PDE that corresponds to this limit and name the algorithm
Brownboost since the PDE is closely related to Brownian motion with drift. Subsequently, [Freund and Opper 2002]
observed that when δ is small the recursion formula for the the potential (deﬁned at discrete times using the minimax
character of the game) has a particularly simple form. Taking the scaling limit of the recursion formula leads to a PDE.
Using the solutions of the PDE with different loss functions, this work successfully recovered some known boosting
algorithms and designed some new ones. A nice summary can be found in Chapter 14 of [Schapire and Freund 2012].

3

Our use of Taylor expansion is different from anything found in the work just summarized. However, similar ap-
plications of Taylor expansion were used in our recent papers on prediction with expert advice [Kobzar et al. 2020a,b],
and also on unconstrained online learning [Zhang et al. 2022].

The reader is surely wondering by now about the character of our potentials. For V1, our potential has the form

where f solves a constant-coefﬁcient linear heat equation in one space dimension similar to

Λ(s, t) =

1
N

N
(cid:88)

i=1

f (si, t).

(cid:40)

∂tf (s, t) + 1
f (s, 0) = 11{s≤0} .

2 f (cid:48)(cid:48)(s, t) = 0

(2)

(3)

Its explicit solution is discussed at the beginning of Section 3. While this potential is familiar from the literature on
boosting, our use of it is different from what one ﬁnds there: we establish its asymptotic optimality not by ﬁnding op-
timal discrete-time potentials then taking a limit, but instead by a rather elementary Taylor-expansion-based argument.
One might wonder how Taylor expansion could possibly be useful in a game whose moves are not small. The answer
is that the solution of f (s, t) is actually a function of s/(cid:112)|t|. Therefore when |t| is large, replacement of s by s ± 1
involves evaluation of a smooth function at points that are only 1/(cid:112)|t| apart.

For V2 our potential still has the separable form (2), but f solves a nonlinear PDE similar to

(cid:40)

∂tf (s, t) + 1
f (s, 0) = 11{s≤0} .

2 max(f (cid:48)(cid:48)(s, t), 0) = 0

(4)

The solution is again explicit, as we explain at the beginning of Section 4. Once again, our potential is familiar:
indeed, it is the scaling limit of the one found in [Mukherjee and Schapire 2008], which (as noted earlier) involves a
suitable truncation of the binomial weights algorithm. But once again, our use of this potential is different from that
of [Mukherjee and Schapire 2008].

The PDE (4) is very nonlinear, and the reader might quite properly wonder what we mean by “its solution.” Our
explicit solution is smooth away from s = 0, but merely continuous (not C 1) at s = 0. It solves Equation (4) in the
classical sense except at s = 0, where f (cid:48)(cid:48) is undeﬁned. We don’t need to discuss whether the PDE can be interpreted
even at s = 0, since our arguments use the explicit form of f , not the PDE. The optimal strategies for the player and
the adversary must be formulated with great care near s = 0, since the function f is not smooth there; however our
use of Taylor expansion is similar to what we do when Z = {±1}.

The preceding discussion emphasized the use of Taylor expansion to assess the player’s strategy by proving an
upper bound on the ﬁnal-time loss. The arguments used to formulate and assess the adversary’s strategy also use our
potential and Taylor expansion, but they require some additional arguments to know the existence of a good choice
of z for the adversary. In this area, we adopt methods that are already in the literature. For V1 we use a probabilistic
argument similar to that of [Schapire 2001]. For V2,V3 and V4 a simpler argument is possible, by arguing as in
[Mukherjee and Schapire 2008]. In both cases, it is necessary to assume that the number of chips is large enough (and
the required number increases as a positive power of T ).

It is natural to ask why the nonlinear PDE (4) is relevant when Z = [−1, 1], and to ask more generally how, in
other settings, one might use a suitable PDE to guess a good potential (whose validity might then be conﬁrmed using
the methods sketched above). This is addressed in Section 7. The discussion there is independent of Sections 2 – 6;
we have placed it last because the discussion there is merely heuristic, while the results in Sections 2 – 6 are entirely
rigorous.

The rest of the paper is organized as follows: Section 2 discusses the ideas that drive our analysis. Sections 3 – 6
provide careful statements of our results for the four versions of the drifting game that are our primary focus. Section
7 provides a heuristic derivation of the (nonlinear, in general) PDE that (conjecturally) describes the scaling limit of
a fairly general drifting game. Appendices A–D provide detailed proofs of our rigorous results, while Appendix E
justiﬁes a step in our heuristic identiﬁcation of the PDE under a reasonable assumption.

4

2 The Main Ideas

While our drifting games have already been formulated in the Introduction, we need more notation to be able to discuss
them efﬁciently; subsection 2.1 is devoted to this. Subsection 2.2 explains why δ should scale with T by the law (1).
Then we turn, in Sections 2.3–2.6, to a discussion of the key ideas that lie behind our analysis. The section closes with
a brief summary of some notational conventions.

2.1 The Drifting Game and its Minimax Loss

We have already introduced the game in the Introduction. It is convenient to let it end at time 0; therefore the game
starts at time −T and its ﬁnal round occurs at time −1. The player’s choice of p = (p1, . . . , pN ) at time t will be called
pt = (pt,1, . . . , pt,N ), and the adversary’s choice of z = (z1, . . . , zN ) at time t will be called zt = (zt,1, . . . , zt,N ).
Since each chip is initially located at 0, the chips’ positions at time t satisfy s−T,i = 0 and st+1,i = st,i + zt,i, and
their ﬁnal positions are s0,i; as usual, we shall write st = (st,1, . . . , st,N ).

We recall that at time t, the adversary may choose any zt such that zt,i ∈ Z for each i and pt · zt ≥ δ. It is

convenient to give the admissible set a name; we therefore deﬁne

Sδ(p) = {z ∈ Z N |p · z ≥ δ}.

We deﬁne Λd

δ (s, t) to be the player’s ﬁnal-time loss (assuming optimal play by both parties), if the chips’ locations

are s at time t. It is characterized by the dynamic programming principle

Λd

δ (s, t) = min

p

max
z∈Sδ(p)

Λd

δ (s + z, t + 1)

for t ≤ −1

(5)

combined with the ﬁnal-time condition

Λd

δ (s, 0) =

1
N

N
(cid:88)

i=1

L(si).

(The superscript d stands for “discrete.” We will use the similar notation Λ(s, t) for our potentials, since they are
intended to be approximations of Λd

δ ).

The dynamic programming principle (5) deﬁnes Λd

are interested in the ﬁnal-time loss when the chips are initially at 0; thus our goal is to estimate Λd
player’s minimax loss. Unpacking the dynamic programming principle, it is

δ (s, t) for all s ∈ RN and all negative integer times t. But we
δ (0, −T ). This is the

Λd

δ (0, −T ) = min
p−T

max
z−T ∈Sδ(p−T )

. . . min
p−1

max
z−1∈Sδ(p−1)

1
N

N
(cid:88)

L(cid:0)

−1
(cid:88)

i=1

t=−T

(cid:1).

zt,i

2.2 The Dependence of δ on T

It is already familiar from [Freund and Opper 2002; Schapire and Freund 2012] that when focusing on the asymptotic
behavior as T → ∞, the parameter δ should depend on T by (1). The argument there involves taking a scaling limit
of the game (in roughly the same way that Brownian motion arises as the scaling limit of random walk on a lattice).
We shall consider such a scaling limit in Section 7, where we discuss how our ideas might extend to a more general
class of drifting games.

It is, however, possible to understand the dependence of δ on T quite simply, as follows. As already noted in the
Introduction, our potentials are built from solutions of Equation (3) or (4), and in each case the solution has the form
f (s, t) = g(s/(cid:112)|t|). The constraint p · z ≥ δ forces the adversary to favor positive zi over negative ones (assuming
δ > 0). The effect of this bias accumulates over time; since s0,i = z−T,i + z−T +1,i + · · · + z−1,i, we expect the effect
on s0,i for a typical i to be of order T δ. Thus, if for δ = 0 the minimax loss is about g(s/
T ), then for positive δ it
(cid:17)
should be something like g
be held constant. Our law (1) is a restatement of this condition.

. For this to have a nontrivial limit as T → ∞, δT /

(s + δT )/

T should

T = δ

√

√

√

√

(cid:16)

T

5

2.3 The Role of Taylor Expansion, and Relevance of the PDEs

If the time steps and spatial increments were small, the dynamic programming principle (5) would resemble a numer-
ical scheme for solving some PDE. In numerical analysis we are typically given the PDE, and we choose a discrete
scheme that meets the test of consistency – in other words, that approximately solves the PDE, to an appropriate order
in Taylor expansion. Here our situation is different: it is the discrete scheme that is given to us. But the role of Taylor
expansion is exactly the same: to show that a potential Λ(s, t) is approximately equal to Λd
δ , we need only check that
it is close at the ﬁnal time and that it approximately satisﬁes the dynamic programming principle. The quality of the
approximation can be estimated by adding up the ﬁnal-time error and the sum of all the approximation errors at times
−T, −T + 1, . . . − 1.

We wrote, in the Introduction, that our potentials have the separable form (2) where f (s, t) solves a PDE “similar
to” Equation (3) or (4). But Taylor expansion requires smoothness, whereas the ﬁnal-time data for these PDEs (our
loss function) is discontinuous, so we cannot use the solution of the PDE directly. Rather, we use a shifted version of
it; speciﬁcally, rather than use the solution f of Equation (3) or (4) our potentials have the separable form (2) with f
replaced by ˜f (s, t) = f (s + c1, t − τ ) + c2, where τ , c1, and c2 are T -dependent constants. The constant τ > 0 is
chosen so that ˜f is sufﬁciently smooth for t = 0 (this requires τ to be large; in practice we use a positive power of
T ). The constants c1 and c2 are chosen so that ˜f (s, 0) ≥ L(s) for an upper-bound potential (used to identify a good
strategy for the player), or so that ˜f (s, 0) ≤ L(s) for a lower-bound potential (used to identify a good strategy for the
adversary).

We turn now to a more quantitative explanation of how Taylor expansion will be used. For any smooth potential

Λ(s, t), Taylor’s theorem gives

Λ(s + z, t + 1) − Λ(s, t) =[Λ(s + z, t + 1) − Λ(s, t + 1)] + [Λ(s, t + 1) − Λ(s, t)]

=∇Λ(s, t + 1) · z +

∂tΛ(s, t + 1) +

(cid:18)

z(cid:62)D2Λ(s, t + 1)z

(cid:19)

1
2

+ Et

(6)

where Et represents the “error” due to the chosen truncation of the Taylor expansion. We always choose our potentials
so these errors are controllable; therefore we shall ignore them in the rest of this discussion. Potentials made from
solutions of (4) are not globally smooth; rather, they are constant for s below a critical value and smooth for s above
that value, with a ﬁrst derivative that jumps at the critical value. Taylor expansion can be used for s above the critical
value, but a different argument is needed at the critical value. We shall ignore this issue for now, returning to it later in
this section.

The dynamic programming principle (5) says that when Λ is replaced by Λd
0. Therefore we would like the min-max of the right side to be 0 (or nearly so):

δ , the min-max of the left hand side is

min
p

max
z∈Sδ(p)

∇Λ(s, t + 1) · z +

∂tΛ(s, t + 1) +

(cid:18)

z(cid:62)D2Λ(s, t + 1)z

(cid:19)

≈ 0.

1
2

We emphasize that to prove such a statement, one must identify good strategies for both the player and the adversary.
The player’s strategy is a choice of p such that for every z ∈ Sδ(p), ∇Λ(s, t+1)·z+(cid:0)∂tΛ(s, t + 1) + 1
0 (modulo “error terms”). The adversary’s strategy is a way of choosing z ∈ Sδ(p) (given p), such that ∇Λ(s, t + 1) ·
z + (cid:0)∂tΛ(s, t + 1) + 1

2 z(cid:62)D2Λ(s, t + 1)z(cid:1) ≥ 0 (modulo “error terms”).

2 z(cid:62)D2Λ(s, t + 1)z(cid:1) ≤

The situation is simplest to understand when δ = 0 and Z = {±1}. Then the player (who chooses p and wants
to minimize) can make the ﬁrst-order term non-positive by choosing p proportional to −∇Λ. (We use here that our
potential has ∂Λ/∂si ≤ 0.) It is natural to guess that the adversary (who chooses z after hearing p, and wants to
maximize) can choose z so that the ﬁrst-order term is close enough to 0 to treat it as an “error term.” If the potential
has the form

˜f (si, t)

(7)

for some scalar-valued function ˜f (s, t) then

Λ(s, t) =

1
N

N
(cid:88)

i=1

z(cid:62)D2Λ(s, t + 1)z =

1
N

N
(cid:88)

i=1

˜f (cid:48)(cid:48)(si, t)

is independent of z (since each zi = ±1). So the min-max vanishes (modulo error terms) when ˜ft + 1
2

˜f (cid:48)(cid:48) = 0.

6

The situation is only a little different when δ = 0 and Z = [−1, 1] (provided we ignore the non-smoothness of
the potential). The player can still make the ﬁrst-order term nonpositive by choosing p proportional to −∇Λ, and
the adversary can still make the ﬁrst-order term close enough to zero that it becomes an “error term.” But when
Z = [−1, 1], the second-order Taylor expansion term is no longer independent of z, and

max
z∈[−1,1]N

z(cid:62)D2Λ(s, t + 1)z =

1
N

N
(cid:88)

i=1

max{ ˜f (cid:48)(cid:48)(si, t + 1), 0},

since the optimal zi is ±1 when ˜f (cid:48)(cid:48) ≥ 0 and 0 when ˜f (cid:48)(cid:48) ≤ 0. This is the origin of the nonlinear PDE (4).

2.4 Additional Ideas in Game V1

Replacing zi by zi − δ, it is equivalent to consider situation when Z = {−1 − δ, 1 − δ}, L(s) = 11{s≤−δT }, and the
adversary’s global constraint is p · z ≥ 0. The player’s strategy should, as explained above, be to choose p proportional
to −∇Λ. The adversary needs to choose zi ∈ {−1 − δ, 1 − δ} such that p · z is very near 0; our proof that this is
possible is probabilistic, adopting an argument from [Schapire 1999, 2001]. However, when δ > 0 the second-order
Taylor expansion term is no longer independent of the choice of zi ∈ {−1 − δ, 1 − δ}. Fortunately, δ is small when
˜f (cid:48)(cid:48) = 0, we can apply the argument sketched earlier,
T is large, as a result of the scaling (1). So if ˜f solves ˜ft + 1
2
estimating

z(cid:62)D2Λ(s, t + 1)z ∼

1
N

N
(cid:88)

i=1

˜f (cid:48)(cid:48)(si, t) + δ max
s∈R

| ˜f (cid:48)(cid:48)(s, t)|.

The second term is treated as an “error term” (alongside the errors associated with truncation of the Taylor expansion
and nonzero ∇Λ · z).

2.5 Additional Ideas in Game V2

As before, it convenient to replace zi by zi − δ, which leads us to consider the drifting game in which Z = [−1 −
δ, 1 − δ], L(s) = 11{s≤−δT }, and the adversary’s global constraint is p · z ≥ 0. Our potential has the separable
form (7) as usual, but ˜f = f (s + c1, t − τ ) + c2 where f solves a nonlinear PDE like (4). In particular, ˜f (s, t) is
constant (independent of both s and t) when s lies below a critical value, while ˜f is smooth (moreover, convex in s
and decreasing in t) when s lies above the critical value.

The player’s strategy is always the same: p should be proportional to −∇Λ. Note that when si is below the
critical value this gives pi = 0. The adversary’s strategy must, as usual, choose zi such that p · z is close enough to 0
so ∇Λ · z can be treated as an error term. We ﬁnd it convenient to limit the adversary’s strategy to zi taking only from
{0, ±(1 − δ)}. In fact, our adversary chooses zi = 0 when si is at or below the critical value, and zi ∈ {±(1 − δ)}
when si is above the critical value. Since the two nonzero possibilities are symmetric, the existence of such z such
that p · z is nearly 0 can be proved using a combinatorial argument previously used in [Mukherjee and Schapire 2008].
(This is simpler than the probabilistic argument of [Schapire 1999, 2001], and for a given error it requires a smaller
number of chips.)

Since ˜f (s, t) is only continuous (not C 1) at the critical value of s, our use of Taylor expansion needs to be re-
examined. When si is well below the critical value, ˜f is locally constant in both space and time so Taylor expansion
is not needed. When si is well above the critical value, our Taylor-expansion-based arguments are applicable. When
si is close enough to the critical value that si and si + zi can be on opposite sides of it, special attention is needed.
Fortunately, the required inequalities are available in this situation, by combining Taylor expansion restricted to s
greater than the critical value with the monotonicity of ˜f .

2.6 Comments on Game V3, V4 and Broader Classes of Drifting Games

Game V1 and V2 are associated with the loss function L(s) = 11{s≤0}, which is useful in boosting. Game V3, V4 are
closely related to some online learning algorithms (e.g. prediction with expert advice, Hedge), their loss function is
11{s≤−R} where R is a positive constant and δ = 0. The ideas already summarized extend straightforwardly to game
V3 and V4, as we explain in Section 5, 6.

7

We have thus far discussed a few speciﬁc examples of drifting games. One might wonder whether PDEs can be
used to determine the minimax loss in the limit T → ∞ for a more general class of drifting games. While we have
no rigorous results of this kind, Section 7 offers a suggestion. The arguments there use a scaled version of the game.
Not surprisingly, when specialized to the cases considered in this section, they reduce to the heuristic derivations of
the PDEs that we have given in this section.

2.7 Notation

We introduce some notation that are used throughout this paper. The square bracket [N ] refers to {1, . . . , N }. For a
function f (s, t) where s is the spatial variable and t is the time variable, we use ∂t, ∂tt, . . . to represent time derivatives
and f (cid:48), f (cid:48)(cid:48), f (3), f (4) . . . to represent spatial derivatives. For functions with more than one spatial variable, ∇ and D2
are used to represent the gradient and Hessian of the spatial variables. Bold letters, such as s, z and p, are vectors in
RN , and normal letters like s and t are scalars. Also, 1 and 0 are vectors of all 1’s and all 0’s respectively. For the sake
of brevity we sometimes omit the time subscript and write st,i, zt,i and pt,i as si, zi and pi respectively. We use C to
represent absolute constants and big O notation f = O(g) means f ≤ Cg for some C. Lastly, the ceiling function
(cid:100)x(cid:101) is the smallest integer greater than or equal to x.

3 Game V1

In this section we consider game V1 from the Introduction: δ ≥ 0, Z = {±1}, and L(s) = 11{s≤0}. This game is
related to classical boosting setting where the weak learners make predictions from {±1}. We give new player and
adversary strategies for this game (thus also for the classical boosting setting), and give (matching) upper and lower
bounds which can be seen as the limit of the discrete bounds given in [Schapire 1999, 2001] as T → ∞.

Replacing zi by zi − δ, V1 is equivalent to the game where Z = {−1 − δ, 1 − δ}, L(s) = 11{s≤−δT }, and the
adversary’s global constraint is p · z ≥ 0. We work on this equivalent version and build our potential using the solution
of the following Equation

(cid:40)

∂tf (s, t) + 1
f (s, 0) = 11{s≤−δT } .

2 f (cid:48)(cid:48)(s, t) = 0

(8)

This is a one dimensional backward heat equation and when δ = 0 it is exactly Equation (3). The explicit solution

of Equation (3) is

g(s, t) =

1
√
π

(cid:90) ∞

√

s/

−2t

e−x2

dx .

Figure 1: Solution g at t = 0, −1, −2

We can see from Figure 1 that g(·, t) − 1/2 is a strictly decreasing odd function. Also 0 ≤ g ≤ 1 and g is concave
when s < 0 and convex when s > 0; moreover as t → −∞, g(s, ·) is decreasing for s < 0 and increasing for s > 0,
and limt→−∞ g(s, t) = 1/2 for all s. The decaying rate of derivatives of g as t → −∞ is speciﬁed in the following
lemma.

8

Lemma 1. There exists constant C s.t. (cid:107)g(cid:48)(·, t)(cid:107)L∞ ≤ C√
|t|
(cid:107)∂ttg(·, t)(cid:107)L∞ ≤ C

|t|2 .

, (cid:107)g(cid:48)(cid:48)(·, t)(cid:107)L∞ ≤ C

|t| , (cid:107)g(3)(·, t)(cid:107)L∞ ≤ C

3
2

|t|

, and

For Equation (8), the solution is

f (s, t) = g(s + δT, t) ,

which serves as the building block of the upper and lower bound potentials.

In the statements of the theorems in this section γ is deﬁned as in Equation (1), i.e.

γ =

δ2T
2

,

and it is a constant as T → ∞.

3.1 Upper Bound

As mentioned in 2.3, we can not use f directly, rather we use a shifted version of f to build the upper bound potential
ΛU :

ΛU (s, t) =

1
N

N
(cid:88)

i=1

f (si − β, t − τ ) + 1 − f (−δT − β, −τ ) ,

with β > 0, τ > 0 to be determined. Note that we shift up the graph by 1 − f (−δT − β, −τ ) so that

ΛU (s, 0) ≥

1
N

N
(cid:88)

i=1

L(si) .

Moreover ΛU is a strictly decreasing function on all the spatial coordinates, which implies that pt ∼ −∇ΛU (s, t + 1)
is a valid probability distribution,

We give the following theorem which is a continuous analogue of Theorem 2 in [Schapire 1999, 2001].

Theorem 2. For any θ ∈ (0, 1) the player strategy proportional to the negative gradient of ΛU with β = τ = T θ
satisﬁes

2

1
N

N
(cid:88)

i=1

L(s0,i) ≤ 1 −

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx + O(T − θ

4 ) ,

for any adversary strategy A. Here the range of the integral is speciﬁed by

a(γ, T, θ) = −

(cid:113)

γ/ (cid:0)1 + T θ/2−1(cid:1) +

T (θ−1)/2
2 (cid:0)1 + T θ/2−1(cid:1)

(cid:113)

b(γ, T, θ) =T θ/4/

√

2 .

As T → ∞, the leading order term

1 −

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx → 1 −

1
√
π

(cid:90) ∞

√

−

γ

e−x2

dx =

1
√
π

(cid:90) ∞

√

γ

e−x2

dx .

(cid:82) ∞
√

γ e−x2

The main term 1√
π

dx in Theorem 2 also appeared in the analysis of Brownboost [Freund 2001] and
section 14.2 of [Schapire and Freund 2012], and it is the limit of bound obtained by the boost-by-majority algorithm
[Freund 1990]. In the boosting game setting our algorithm can be seen as a continuous variant of boost-by-majority
algorithm [Freund 1990].

9

3.2 Lower Bound

Similar to the upper bound analysis, we use another shifted version of f to construct the lower bound potential. More
speciﬁcally, we deﬁne

ΛL(s, t) =

1
N

N
(cid:88)

i=1

f (si + β, t − τ ) − f (−δT + β, −τ ) .

with β > 0, τ > 0 to be determined. Note that we shift down the graph by f (−δT + β, −τ ) so that

ΛL(s, 0) ≤

1
N

N
(cid:88)

i=1

L(si) .

Besides the Taylor expansion arguments used in upper bound derivation, we use a probabilistic argument to show
that ΛL(s + z, t + 1) − ΛL(s, t) is bounded from below, under the condition that N is large enough with the help of
Hoeffding’s inequality (Lemma 11).

Our lower bound is a continuous analogue of theorem 3 in [Schapire 1999, 2001].

Theorem 3. For any θ ∈ (0, 1), if the number of chips satisﬁes

N > 8T 2+θ/2 log(1 −

(cid:112)

e−T −2−θ/2 )−1 ,

(*)

then for any player strategy there exists an adversarial strategy A using ΛL with β = τ = T θ/2 such that

1
N

N
(cid:88)

i=1

L(s0,i) ≥

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx − O(T −θ/4) .

Here the range of the integral is speciﬁed by

√

a(γ, T, θ) =
b(γ, T, θ) =T θ/4/

√

2 .

γ + T (θ−1)/2/

2

√

As T → ∞, the leading order term

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx →

1
√
π

(cid:90) ∞

√

γ

e−x2

dx .

Remark 1. The main terms as T → ∞ in Theorem 2 and 3 coincide, thus the player strategy and the adversary
strategy are both asymptotically optimal, and the limit of the minimax loss is 1√
π

γ e−x2

(cid:82) ∞
√

dx.

4 Game V2

In this section we consider game V2 from the Introduction: δ ≥ 0, Z = [−1, 1], and L(s) = 11{s≤0}. This game can
be seen as a variant of boosting setting where the weak learners can take values in [−1, 1], the sign represents their
prediction and absolute value represents their conﬁdence. The player and adversary strategy we propose in this section
are also strategies for this boosting game and our (matching) upper and lower bounds are new upper and lower bounds
of the booster’s error rate.

Replacing zi by zi − δ, V2 is equivalent to the game where Z = [−1 − δ, 1 − δ], L(s) = 11{s≤−δT }, and the
adversary’s global constraint is p · z ≥ 0. We work on this equivalent version and build our potential using the solution
of the following Equation

(cid:40)

∂tf (s, t) + 1
f (s, 0) = 11{s≤−δT } .

2 max(f (cid:48)(cid:48)(s, t), 0) = 0

(9)

10

Comparing to Equation (8) there is a max operator on the second order derivative which makes it a nonlinear
equation. and when δ = 0 it is exactly Equation (4). One may expect there is no smooth solution for Equation (4), and
instead we construct a piece-wise smooth solution

˜g(s, t) =

(cid:40)

2g(s, t)
1

s > 0
s ≤ 0 ,

It is continuous at origin, but not differentiable. On the right of origin ˜g(·, t) is convex, while on the left ˜g(·, t) = 1 is
a constant.

Figure 2: Solution ˜g at t = 0, −1, −2

Since ˜g is a multiple of g when s ≥ 0, we have the following lemma thanks to Lemma 1.

Lemma 4. There exists constant C s.t. (cid:107)˜g(cid:48)(·, t)(cid:107)L∞(R+) ≤ C√
|t|
and (cid:107)∂tt˜g(·, t)(cid:107)L∞(R+) ≤ C

|t|2 .

For Equation (9) with δ > 0, the solution can be written as

, (cid:107)˜g(cid:48)(cid:48)(·, t)(cid:107)L∞(R+) ≤ C

|t| , (cid:107)˜g(3)(·, t)(cid:107)L∞(R+) ≤ C

3
2

|t|

,

which serves as the building block of the upper and lower bound potentials.

f (s, t) = ˜g(s + δT, t) ,

As in Section 3, in our statements

γ :=

δ2T
2

,

and it is a constant as T → ∞.

4.1 Upper Bound

We use a shifted version as the upper bound potential

ΛU (s, t) =

1
N

N
(cid:88)

i=1

f (si, t − τ ) ,

where τ > 0 is a constant to be determined. Note that ΛU is a decreasing function in all the spatial variables and

ΛU (s, 0) ≥

1
N

N
(cid:88)

i=1

L(si) .

The player imposes the following distribution: if at least one chip is on the right of −δT ,

pi ∼

(cid:40)
0
−f (cid:48)(si, t + 1 − τ )

si ≤ −δT
si > −δT ;

(10)

otherwise if all the chips are on the right of −δT , any probability distribution is ﬁne.

With a more reﬁned analysis compared to Section 3.1 we get

11

Theorem 5. For any θ ∈ (0, 1), the player strategy speciﬁed in (10) with τ = T θ satisﬁes

1
N

N
(cid:88)

i=1

L(s0,i) ≤

2
√
π

(cid:90) ∞

a(γ,T,θ)

e−x2

dx + O(T − θ

2 ) ,

for any adversary strategy A. Here the lower range of the integral is

As T → ∞, the leading order term

a(γ, T, θ) =

(cid:114) γ

1 + T θ−1 .

2
√
π

(cid:90) ∞

a(γ,T,θ)

e−x2

dx →

2
√
π

(cid:90) ∞

√

γ

e−x2

dx .

4.2 Lower Bound

The lower bound potential ΛL is deﬁned using a shifted version of f ,

ΛL(s, t) =

1
N

N
(cid:88)

i=1

f (si + β, t − τ ) − f (−δT + β, −τ ) ,

with β > 0, τ > 0 to be determined. Note that we shift down the graph by f (−δT + β, −τ ) so that

ΛL(s, 0) ≤

1
N

N
(cid:88)

i=1

L(si) .

The adversary has more choices compared to the case Z = {±1 − δ} in Section 3.2. Interestingly, we show
existence of an asymptotically optimal adversary strategy that only chooses from {−1 + δ, 0, 1 − δ}. We can utilize the
symmetry of this special action set and give a combinatorial argument (c.f. Lemma 12) without relying on Hoeffding’s
inequality .

Theorem 6. For any T and θ ∈ (0, 1), if the number of chips N ≥ T
adversarial strategy A associated with ΛL with

θ+2
4 , then for any player strategy there exists an

such that

β = τ = (cid:100)

δT + T θ
1 − δ

2

(cid:101)(1 − δ) − δT

1
N

N
(cid:88)

i=1

L(s0,i) ≥

2
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx − O(T − θ

4 ) .

Here the range of the integral is speciﬁed by

a(γ, T, θ) =

√

γ +

b(γ, T, θ) =T θ/4/

θ
2 + 1

(cid:17)

√
/

2T

(cid:16)

T
√

2 .

Moreover, A takes zi = 0 when si ≤ −(cid:100) δT +T
As T → ∞, the leading order term

θ
2

1−δ (cid:101)(1 − δ) and |zi| = 1 − δ otherwise.

2
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx →

2
√
π

(cid:90) ∞

√

γ

e−x2

dx .

Remark 2. Theorem 6 conﬁrms that the upper bound proposed in Theorem 5 is asymptotic optimal when N is large
enough. On the other hand side, the adversary obtains asymptotically optimal lower bound by choosing from Z =
{−1 + δ, 0, 1 − δ}. In other words, granting more abundant action set [−1 − δ, 1 − δ] doesn’t give the adversary
stronger power compared to Z = {−1 + δ, 0, 1 − δ}.

Remark 3. Since the adversary has more power compared to binary case, the leading order term obtained in game
V2 is larger than game V1, and it is actually twice as large.

12

5 Game V3

In this section we consider game V3: δ = 0, Z = {±1}, and L(s) = 11{s≤−R}. Game V3 is related to the prediction
with expert advice game in which the experts make binary decisions. We give new player and adversary strategies for
this game and give (matching) upper and lower bounds which can be seen as the limit of the discrete bounds given in
[Cesa-Bianchi et al. 1996] as T → ∞.

The technical details here are very similar to Section 3. We use

f (s, t) = g(s + R, t) =

1
√
π

(cid:90) ∞

s+R√
−2t

e−x2

,

which satisﬁes the following PDE,

(cid:40)

∂tf (s, t) + 1
f (s, 0) = 11{s≤−R} ,

2 f (cid:48)(cid:48)(s, t) = 0

(11)

and serves as the building block.

In the statements of the theorems in this section γ is deﬁned as in Equation (1’), i.e.

γ =

R2
2T

,

and it is a constant as T → ∞.

5.1 Upper Bound

We use a shifted version of f for the upper bound potential ΛU .

ΛU (s, t) =

1
N

N
(cid:88)

i=1

f (si − β, t − τ ) + 1 − f (−R − β, −τ ) ,

with β > 0, τ > 0 to be determined. Note that we shift up the graph by 1 − f (−R − β, −τ ) so that

ΛU (s, 0) ≥

1
N

N
(cid:88)

i=1

L(si)

and it is a decreasing function on all the spatial coordinates. This implies that pt ∼ −∇ΛU (s, t + 1) is a valid
probability distribution.

We give the following theorem which is a continuous analogue of binomial weight algorithm in [Cesa-Bianchi

et al. 1996].

Theorem 7. For any θ ∈ (0, 1) the player strategy proportional to the negative gradient of ΛU with β = τ = T θ
satisﬁes

2

1
N

N
(cid:88)

i=1

L(s0,i) ≤ 1 −

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx + O(T − θ

4 ) ,

for any adversary strategy A. Here the range of the integral is speciﬁed by

a(γ, T, θ) = −

(cid:113)

b(γ, T, θ) =T θ/4/

2 .

γ/ (cid:0)1 + T θ/2−1(cid:1) + T (θ−1)/2/
√

(cid:113)

2 (cid:0)1 + T θ/2−1(cid:1)

As T → ∞, the leading order term

1 −

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx → 1 −

1
√
π

(cid:90) ∞

√

−

γ

e−x2

dx =

1
√
π

(cid:90) ∞

√

γ

e−x2

dx .

13

5.2 Lower Bound

We use a different shift of f to construct the lower bound potential. More speciﬁcally, we deﬁne

ΛL(s, t) =

1
N

N
(cid:88)

i=1

f (si + β, t − τ ) − f (−R + β, −τ ) .

with β > 0, τ > 0 to be determined. Note that we shift down the graph by f (−R + β, −τ ) so that

ΛL(s, 0) ≤

1
N

N
(cid:88)

i=1

L(si) .

Compared to the other binary case V1, the adversary’s choices here ({±1}) are symmetric about 0, so we can

derive lower bound with the help of Lemma 12.

Theorem 8. For any T and any θ ∈ (0, 1), if the number of chips satisﬁes

N ≥ T

2+θ
4

,

then for any player strategy there exists an adversarial strategy A associated with ΛL where β = τ = T θ/2 such that

1
N

N
(cid:88)

i=1

L(s0,i) ≥

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx − O(T −θ/4) .

Here the range of the integral is speciﬁed by

√

a(γ, T, θ) =
b(γ, T, θ) =T θ/4/

γ + T
√

√

θ−1
2 /

2

2 .

As T → ∞, the leading order term

6 Game V4

1
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx →

1
√
π

(cid:90) ∞

√

γ

e−x2

dx .

In this section we consider game V4: δ = 0, Z = [−1, 1], and L(s) = 11{s≤−R} for some constant R > 0. This
game is related to a prediction with expert advice setting in which each expert’s prediction takes value between [−1, 1]
([Mukherjee and Schapire 2008]) and also related to the Hedge game ([Luo and Schapire 2014]). The strategies and
bounds here can be seen as a continuous analogue of [Mukherjee and Schapire 2008].

Motivated by the upper bound potential deﬁned in Section 4.1, we deﬁne

which solves

f (s, t) = ˜g(s + R, t) ,

(cid:40)

∂tf (s, t) + 1
f (s, 0) = 11{s≤−R} .

2 max(f (cid:48)(cid:48)(s, t), 0) = 0

(12)

and serves as the building block.

As in Section 5, in our statements of theorems

and it is a constant as T → ∞.

γ :=

R2
2T

,

14

6.1 Upper bound

We deﬁne the upper bound potential ΛU using a shifted version of f

ΛU (s, t) =

1
N

N
(cid:88)

i=1

f (si, t − τ ) ,

where τ > 0 is a constant to be determined. Note that ΛU is a decreasing function in all the spatial variables and

ΛU (s, 0) ≥

1
N

N
(cid:88)

i=1

L(si) .

The player imposes the following distribution: if at least one chip is on the right of −R,

(cid:40)

pi ∼

0
−f (cid:48)(si, t + 1 − τ )

si ≤ −R
si > −R ;

(13)

otherwise if all the chips are on the right of −R, any probability distribution is ﬁne.

Theorem 9. For any θ ∈ (0, 1), the player strategy following (13) with τ = T θ satisﬁes

1
N

N
(cid:88)

i=1

L(s0,i) ≤

2
√
π

(cid:90) ∞

a(γ,T,θ)

e−x2

dx + O(T − θ

2 ) ,

for any adversary strategy A. Here the range of the integral is speciﬁed by

As T → ∞, the leading order term

a(γ, T, θ) =

(cid:114) γ

1 + T θ−1 .

2
√
π

(cid:90) ∞

a(γ,T,θ)

e−x2

dx →

2
√
π

(cid:90) ∞

√

γ

e−x2

dx .

6.2 Lower Bound

We deﬁne the lower bound potential ΛL as

ΛL(s, t) =

1
N

N
(cid:88)

i=1

f (si + β, t − τ ) − f (−R + β, −τ ) ,

with β > 0, τ > 0 to be determined. Note that we shift down the graph by f (−R + β, −τ ) so that

ΛL(s, 0) ≤

1
N

N
(cid:88)

i=1

L(si) .

We again use Lemma 12 and derive the following lower bound.

Theorem 10. For any T and θ ∈ (0, 1), if the number of chips N ≥ T
adversarial strategy A associated with ΛL in which β = τ = (cid:100)R + T θ/2(cid:101) − R such that

2+θ
4

then for any player strategy there exists an

1
N

N
(cid:88)

i=1

L(s0,i) ≥

2
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx − O(T − θ

4 ) .

15

Here the range of the integral is speciﬁed by

a(γ, T, θ) =

√

b(γ, T, θ) =T

T θ
2 + 1
√
2T

2 .

γ +
√

θ
4 /

Moreover, A takes zi = 0 when chip i is on the right of −(cid:100)R + T θ

2 (cid:101), and takes |zi| = 1 otherwise.

As T → ∞, the leading order term

2
√
π

(cid:90) b(γ,T,θ)

a(γ,T,θ)

e−x2

dx →

2
√
π

(cid:90) ∞

√

γ

e−x2

dx

Remark 4. Theorem 10 conﬁrms that the upper bound proposed is asymptotic optimal when N is large enough. On
the other hand side, the adversary obtains asymptotically optimal lower bound by choosing from Z = {−1, 0, 1}. In
other words, granting more abundant action set [−1, 1] doesn’t give the adversary stronger power.

Remark 5. The leading order term here is twice as large as in game V3. In [Mukherjee and Schapire 2008] they also
observe this quantitative relation.

7 Heuristic PDE Derivations

This paper has thus far considered a restricted class of drifting games, in which the moves z ∈ RN are restricted to a
set of the form Z × · · · × Z, where Z ⊂ R, and the ﬁnal loss has the form 1
i=1 L(si). It is natural to ask what
N
becomes of our PDE-based approach when the set of permitted moves does not have this structure. This section offers
some thoughts in that direction.

(cid:80)N

Our method is to consider a scaled version of the game, scaling moves by (cid:15) and time by (cid:15)2 where (cid:15) = 1/

T , and
to assume that the scaled minimax loss has a limit at (cid:15) → 0. This is not a new idea: our scaling is the same as the one
used in [Freund 2001], [Freund and Opper 2002], and [Schapire and Freund 2012]. It is also like the one used to study
prediction with expert advice in [Drenska and Kohn 2020], where a heuristic calculation analogous to the one in this
section was given a mathematically rigorous justiﬁcation.

√

We start, in Section 7.1, by introducing the scaled game; then, in Section 7.2, we derive a nonlinear PDE that
(conjecturally) describes its limiting behavior, provided the set of permitted moves contains a neighborhood of the
origin. When this last hypothesis fails we do not ﬁnd an asymptotic PDE, however we do ﬁnd a PDE that the player
might reasonably use to determine his strategy; this is the focus of Section 7.3.

7.1 The Scaled Game

The drifting games we consider in this section are in most respects the same as introduced in the Introduction. The
only changes are:

• the set of possible moves zN is a bounded subset of RN (where N is the number of chips); and

• the ﬁnal loss, denoted as LN (s), is scaling invariant and is a decreasing function of each variable

Before scaling, the game’s minimax loss is determined by the analogue of Equation (5):

Λd

δ (s, t) = min

p

max
z∈Sδ(p)

Λd

δ (s + z, t + 1)

for t ≤ −1,

with the obvious extension of our previous notation

and the ﬁnal-time condition

Sδ(p) = {z ∈ ZN |p · z ≥ δ}

Λd

δ (s, 0) = LN (s).

16

(14)

(15)

(16)

Our goal is to understand the limiting behavior of Λd

δ = (cid:112)2γ/T . With this in mind, we set (cid:15) = 1/

√

δ (0, −T ) in the limit when T → ∞ and δ → 0 with

T and introduce the scaled position and time variables

and the scaled minimax loss

σ = (cid:15)s,

τ = (cid:15)2t

Λ(cid:15)(σ, τ ) = Λd
δ

(cid:16) σ
(cid:15)

,

τ
(cid:15)2

(cid:17)

.

A moment’s thought reveals that the dynamic programming principle deﬁning Λd

δ is equivalent to

Λ(cid:15)(σ, τ ) = min

p

max
z∈Sδ(cid:15) (p)

Λ(cid:15)(σ + (cid:15)z, t + (cid:15)2)

where Sδ(cid:15) is deﬁned by (15) with the parameter δ set equal to

δ(cid:15) = (cid:112)2γ(cid:15)

and the ﬁnal-time condition is

(17)

(18)

(19)

Λ(cid:15)(σ, 0) = LN (σ).
(We use here the scale-invariance of LN , i.e. the fact that its value at σ ∈ RN is the same as its value at σ/(cid:15) for any
(cid:15) > 0). One can view Λ(cid:15) as the minimax loss of a scaled version of the drifting game, in which the permitted moves
at a given step are the vectors (cid:15)z where z ∈ Sδ(cid:15)(p). Note that the function Λ(cid:15)(σ, τ ) is deﬁned when τ is a negative
integer multiple of (cid:15)2, and understanding Λd

δ (0, −T ) as T → ∞ is equivalent to understanding lim(cid:15)→0 Λ(cid:15)(0, −1).

The preceding discussion used the hypothesis that δ = (cid:112)2γ/T , which we justiﬁed heuristically in Section 2.2.
Let us offer here another argument why δ(cid:15) should depend linearly on (cid:15). At the ﬁnal time of the scaled game, the
ﬁnal-time loss LN is evaluated at (cid:15)(z−1 + z−1+(cid:15)2 + . . . + z−(cid:15)2). Since the adversary must choose z such that p · z ≥ δ(cid:15)
at each step, the bias introduced by δ(cid:15) at a single step is of order (cid:15)δ(cid:15) and this bias accumulates over (cid:15)−2 steps to
(cid:15)−2(cid:15)δ(cid:15) = (cid:15)−1δ(cid:15). For a nontrivial result in the limit (cid:15) → 0, we evidently need δ(cid:15) to be linear in (cid:15). (Otherwise the
accumulated bias would dominate and the ﬁnal-time loss function would be evaluated near −∞ or near +∞.) Since
(cid:15) = 1/

T , this justiﬁes once again why δ must be proportional to 1/

T .

√

√

We remark that the minimax loss Λd

δ and its scaled version Λ(cid:15) are non-increasing functions of each “spatial”
variable (si for the former, σi for the latter) at each time. This is easily proved by backward induction in time, using
the fact that the ﬁnal-time loss LN has this property. Thus if Λ(cid:15) is differentiable then ∂iΛ(cid:15) ≤ 0 for each i.

7.2 The PDE Assuming ZN Contains a Neighborhood of Origin
We suppose now that the set of possible moves ZN contains a neighborhood of the origin in RN . This discussion
generalizes what we did earlier in the paper for ZN = [−1, +1]N . We shall Taylor-expand the function Λ(cid:15), ignoring
the possibility that it might not be smooth, and assuming that the quantities we consider have limits as (cid:15) → 0. This
is, of course, purely formal, however analogous arguments are known to give correct results for many optimal control
problems.

Substituting

Λ(cid:15)(σ + (cid:15)z, τ + (cid:15)2) = Λ(cid:15)(σ, τ ) + (cid:15)∇Λ(cid:15)(σ, τ ) · z + (cid:15)2(∂τ Λ(cid:15)(σ, τ ) +

1
2

z(cid:62)D2Λ(cid:15)(σ, τ )z) + O((cid:15)3)

into (18) and dividing by (cid:15) gives

(cid:18)

0 = min

p

max
z∈Sδ(cid:15) (p)

∇Λ(cid:15)(σ, τ ) · z + (cid:15)(cid:0)∂τ Λ(cid:15)(σ, τ ) +

z(cid:62)D2Λ(cid:15)(σ, τ )z(cid:1) + O((cid:15)2)

(cid:19)
.

1
2

(20)

The leading order term is ∇Λ(cid:15)(σ, τ ) · z. Since (cid:15) is tending to 0, this term dominates both players’ considerations.

It is convenient to write zi = z(cid:48)

i + δ(cid:15), and to note that z ∈ ZN is equivalent to z(cid:48) ∈ Z (cid:48)

N = ZN − δ(cid:15)1. Since

∇Λ(cid:15) · z = δ(cid:15)

N
(cid:88)

i=1

∂iΛ(cid:15) + ∇Λ(cid:15) · z(cid:48)

17

and the ﬁrst term on the right is independent of both p and z, the leading-order min-max reduces to

min
p

max
N , p·z(cid:48)≥0

z(cid:48)∈Z (cid:48)

∇Λ(cid:15)(σ, τ ) · z(cid:48).

(21)

We show in Appendix E that the value of this min-max is 0, and it is achieved only when p is proportional to −∇Λ
N satisﬁes the additional condition z(cid:48)⊥∇Λ. The limiting PDE is therefore provided by the order-ε part of
and z(cid:48) ∈ Z (cid:48)
(20). Remembering that δ(cid:15) =
N → ZN as (cid:15) → 0, we conclude (heuristically) that lim(cid:15)→0 Λ should
solve

2γ(cid:15) and that Z (cid:48)

√




∂τ Λ(σ, τ ) +

√

2γ (cid:80)N

i=1 ∂iΛ(σ, τ ) + 1
2

max
∇Λ(σ,τ )⊥z, z∈ZN

z(cid:62)D2Λ(σ, τ )z = 0

(22)



Λ(σ, 0) = LN (σ)

√

2γ (cid:80)N

√

2γτ . The optimal z for (22) cannot necessarily be used at ﬁnite (cid:15), since Z (cid:48)

i=1 ∂iΛ(σ, τ ) can be eliminated by changing variables from (σ, τ ) to (σ(cid:48), τ ) with σ(cid:48)

i =
The ﬁrst-order term
σi +
N is slightly different from ZN .
Thus our situation is slightly different from the prediction with expert advice problem considered in [Drenska and
Kohn 2020], where the asymptotically optimal adversary strategy is admissible at ﬁnite (cid:15). (We remark in passing that
for small numbers of experts, asymptotically optimal strategies are in fact known explicitly [Bayraktar et al. 2020a,b;
Kobzar et al. 2020a,b].)

The PDE (22) is highly nonlinear due to the maximization in z. When ZN = [−1, 1]N it is natural to ask whether
its solution has the form Λ(σ, τ ) = 1
2 max{f (cid:48)(cid:48), 0} = 0. The answer
i=1 f (σi, τ ) where f solves ∂τ f +
N
appears to be no: to get this separable solution, one would need to replace the maximization over z in the second-
z(cid:62)D2Λ(σ, τ )z (changing the equation, and therefore presumably its solution). Evidently: when
order term by max
z∈ZN
(cid:80)N
i=1 11{σi≤0}, the present discussion reduces to the one in Section 4 only in the limit N → ∞ (since by

LN (σ) = 1
N
Lemma 12 ignoring the constraint z(cid:48)⊥∇Λ makes very little difference when N is large enough).

2γf (cid:48) + 1

(cid:80)N

√

7.3 An Upper Bound Potential

When zN does not contain a neighborhood of the origin, one can begin as in the previous subsection, but the optimal
value of the leading-order min-max (21) is unlikely to be 0. (The probabilistic argument used for our lower bound in
Section 3 suggests that it should approach 0 in the limit as N → 0; however, to discuss an asymptotic PDE we must
hold the value of N ﬁxed.)

It is natural to ask whether our PDE-based approach can nevertheless be useful in this setting. We argue in this
subsection that it can be used to design a good potential for the player. The key point is that if the player chooses p to
be a multiple of −∇Λ(cid:15) then

max
N , p·z(cid:48)≥0

z(cid:48)∈Z (cid:48)

∇Λ(cid:15)(σ, τ ) · z(cid:48) ≤ 0.

(23)

While the optimal p might be better – it might make the value of (21) negative – the (heuristic) argument of the
previous subsection combined with (23) suggests that lim(cid:15)→0 Λ(cid:15) should satisfy

(cid:40)∂τ Λ(σ, τ ) +

√

2γ (cid:80)N

i=1 ∂iΛ(σ, τ ) + 1

2 max
z∈ZN

z(cid:62)D2Λ(σ, τ )z ≥ 0

Λ(σ, 0) = LN (σ).

In viscosity-solution language, a function satisfying (24) is a subsolution of the associated PDE

(cid:40)

√

2γ (cid:80)N

∂τ Λ(σ, τ ) +
Λ(σ, 0) = LN (σ).

i=1 ∂iΛ(σ, τ ) 1

2 maxz∈ZN z(cid:62)D2Λ(σ, τ )z = 0

(24)

(25)

By the comparison principle, a solution of (25) should provide an upper bound for lim(cid:15)→0 Λ(cid:15).

The PDE (25) is nonlinear, in general, since it involves a maximization over z. However in the separable case
2 f (cid:48)(cid:48) = 0. When
i=1 11{σi≤0}, this

zN = {±1}N it is easy to see that Λ(σ, τ ) = 1
N
2γτ and LN (σ) = 1
the ﬁrst-order term is eliminated by the change of variables σ(cid:48)
N
reduces to the linear heat equation whose solution we used to design our potentials in Section 3.

i=1 f (σi, τ ) where f solves ∂τ f +

2γf (cid:48) + 1
(cid:80)N

i = σi +

(cid:80)N

√

√

18

Our analysis of the separable case ZN = {±1}N in Section 3 used a probabilistic argument to see that the leading-
order min-max (21) is very close to 0 when N is sufﬁciently large. While that discussion was limited to ZN = {±1}N ,
we suppose a similar argument could be used for other choices of ZN . When it works, such an argument would suggest
that our lower bound potential (25) approaches the asymptotic minimax loss in the limit as N → ∞.

19

A Proofs for Section 3

First we control the derivatives of the solution of Equation (3)

Proof of lemma 1. Doing some algebra we have

g(cid:48)(s, t) = −

√

1
−2πt

s2
2t

e

g(cid:48)(cid:48)(s, t) = − 2∂tg(s, t) = −

1
√
t

π

s2
2t

e

√

s
−2t

g(3)(s, t) = −

2
−2πt

√
t

s2
2t (1/2 +

e

s2
2t

)

∂ttg(s, t) =

1
4

g(4)(s, t) = −

1
2t2 e

s2
2t

√

s
−2πt

(3/2 +

s2
2t

);

thus there exists constant C such that (cid:107)g(cid:48)(·, t)(cid:107)L∞ ≤ C√
|t|
C
|t|2 .

Next we prove the upper bound

, (cid:107)g(cid:48)(cid:48)(·, t)(cid:107)L∞ ≤ C

|t| , (cid:107)g(3)(·, t)(cid:107)L∞ ≤ C

3
2

|t|

, and (cid:107)∂ttg(·, t)(cid:107)L∞ ≤

Proof of Theorem 2. For the sake of simplicity, at any ﬁxed time step t, we omit t in the subscripts of s, p, z, E (which
stands for the Taylor expansion error). We write the increment of ΛU from t to t + 1 as

ΛU (s + z, t + 1) − ΛU (s, t)

=

=

=

1
N

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

The error term

f (si − β + zi, t + 1 − τ ) − f (si − β, t − τ )

f (si − β + zi, t + 1 − τ ) − f (si − β, t + 1 − τ ) + f (si − β, t + 1 − τ ) − f (si − β, t − τ )

f (cid:48)(si − β, t + 1 − τ )zi +

1
2

f (cid:48)(cid:48)(si − β, t + 1 − τ )z2

i + ∂tf (si − β, t + 1 − τ ) +

1
N

N
(cid:88)

i=1

Ei .

Ei =

1
6

f (3)(˜si, t + 1 − τ )z3

i −

1
2

∂ttf (si − β, ˜ti) ,

where ˜si is between si − β, si − β + zi and ˜ti is between t − τ and t + 1 − τ . Using Lemma 1 and the fact that
|zi| ≤ 1 + δ, we can bound 1
N

i=1 Ei by

(cid:80)N

|

1
N

N
(cid:88)

i=1

Ei| ≤ sup
i∈[N ]

|Ei| ≤

C
|t + 1 − τ | 3

2

+

C
|t + 1 − τ |2 ,

where C is some absolute constant.

Recall that the player set pi ∼ −f (cid:48)(si − β, t + 1 − τ ) and since p · z ≥ 0, we have

N
(cid:88)

i=1

f (cid:48)(si − β, t + 1 − τ )zi ≤ 0 .

20

Moreover, as f satisﬁes Equation (8), we have

N
(cid:88)

i=1

N
(cid:88)

i=1

f (cid:48)(si − β, t + 1 − τ )zi +

1
2

f (cid:48)(cid:48)(si − β, t + 1 − τ )z2

i + ∂tf (si − β, t + 1 − τ )

∂tf (si − β, t + 1 − τ ) +

1
2

f (cid:48)(cid:48)(si − β, t + 1 − τ )

1
N

N
(cid:88)

i=1

f (cid:48)(cid:48)(si − β, t + 1 − τ )

z2
i − 1
2

1
N

1
N

+

≤

≤

3δ
2

≤

|f (cid:48)(cid:48)(si − β, t + 1 − τ )|

sup
i∈[N ]
√
C

γ

|t + 1 − τ |

.

√

T

The second inequality used the fact that δ ∈ [0, 1] thus δ2 ≤ δ, and the last inequality used Lemma 1 and the deﬁnition
of γ in Equation (1).

Combining the above analysis together we now add up the increment of ΛU from −T to −1,

1
N

N
(cid:88)

i=1

L(s0,i) ≤ ΛU (s0, 0) = ΛU (0, −T ) +

−1
(cid:88)

t=−T

ΛU (st+1, t + 1) − ΛU (st, t)

≤ ΛU (0, −T ) + C

= ΛU (0, −T ) + O

−τ
(cid:88)

t=−T +1−τ
(cid:18) 1
1
√
τ
τ

+

1
|t|2 +
√

+

1
|t| 3

2

γ log τ
√

T

+

(cid:19)

√
γ
√

|t|

T

.

Note that

ΛU (0, −T ) = f (−β, −T − τ ) + 1 − f (−δT − β, −τ )

= 1 −

1
√
π

(cid:90) β

√

2τ

β−δT√

2(T +τ )

e−x2

dx .

β√
We want
2τ
β = τ = T θ
2 we get

(cid:29) 1 and

β√
2T

→ 0. This can be achieved by setting β = τ = T θ

2 where θ ∈ (0, 1). Plugging in

1
N

N
(cid:88)

i=1

L(s0,i) ≤ 1 −

1
√
π

(cid:90) T θ/4/

√

2

(cid:113)

−

γ/(1+T θ/2−1)+ T (θ−1)/2

(cid:114)

2(1+T θ/2−1)

e−x2

dx + O(T − θ

4 ) .

To prove the lower bound we ﬁrst state the well-known Hoeffding’s inequality (c.f. [Hoeffding 1963]).

Lemma 11 (Hoeffding’s inequality). Suppose X1, . . . , XN be independent random variables with Xi taking values
in [ai, bi] for all i ∈ [1, N ]. Then for any (cid:15) > 0, the following inequalities hold for SN = (cid:80)N

i=1 Xi

P r[SN − E[SN ] ≥ (cid:15)] ≤ e−2(cid:15)2/ (cid:80)N
P r[SN − E[SN ] ≤ −(cid:15)] ≤ e−2(cid:15)2/ (cid:80)N

i=1(bi−ai)2

i=1(bi−ai)2

Now we proceed to lower bound.

21

Proof of Theorem 3. We construct the adversarial strategy A by ﬁrst introducing a randomized adversary which as-
signs value to each zi in an i.i.d fashion with the following distribution

(cid:40)

zi =

w.p. 1+δ+α
1 − δ
−1 − δ w.p. 1−δ−α

2

2

where α > 0 is a parameter to be determined.

For any player strategy p, we have Ep · z = α and by Hoeffding’s inequality

P r(cid:2)p · z < 0(cid:3) ≤ e

−α2
2(cid:107)p(cid:107)2 ≤ e−α2/2

Next we take the expectation of ΛL(s + z, t + 1) − ΛL(s, t),

E [ΛL(s + z, t + 1) − ΛL(s, t)]

=

1
N

+

N
(cid:88)

i=1

α
2N

1 + δ
2

f (si + β + 1 − δ, t + 1 − τ ) +

1 − δ
2

f (si + β − 1 − δ, t + 1 − τ ) − f (si + β, t − τ )

N
(cid:88)

i=1

f (si + β + 1 − δ, t + 1 − τ ) − f (si + β − 1 − δ, t + 1 − τ ) .

The second summation can be bounded by simply using the fact that f ∈ [0, 1]. To estimate the ﬁrst summation

we use the Taylor expansion

f (si + β + 1 − δ, t + 1 − τ ) +

1 − δ
2

f (si + β − 1 − δ, t + 1 − τ ) − f (si + β, t − τ )

(f (si + β + 1 − δ, t + 1 − τ ) − f (si + β, t + 1 − τ ))

1 + δ
2
1 + δ
2
1 − δ
2

+

=

=

(f (si + β − 1 − δ, t + 1 − τ ) − f (si + β, t + 1 − τ ))

f (cid:48)(si + β, t + 1 − τ ) +

+ f (si + β, t + 1 − τ ) − f (si + β, t − τ )
1 − δ2
2
1 − δ2
2

f (cid:48)(si + β, t + 1 − τ ) +

(1 − δ2)(1 − δ)
4

(1 − δ2)(1 + δ)
4

−

f (cid:48)(cid:48)(si + β, t + 1 − τ )

f (cid:48)(cid:48)(si + β, t + 1 − τ )

+ ∂tf (si + β, t + 1 − τ ) + Ei

=∂tf (si + β, t + 1 − τ ) +

(1 − δ2)
2

f (cid:48)(cid:48)(si + β, t + 1 − τ ) + Ei

= −

δ2
2

f (cid:48)(cid:48)(si + β, t + 1 − τ ) + Ei,

where Ei is the remainder term consisting of ∂ttf and f (3). Using Lemma 1 and the deﬁnition of γ in Equation (1)
we can bound






δ2
2 |f (cid:48)(cid:48)(si + β, t + 1 − τ )| ≤
C
|t+1−τ |2 ,
|Ei| ≤

+

C

|t+1−τ |

3
2

Cγ
|t+1−τ |T

for some constant C.

By Hoeffding’s inequality we also have

P r(cid:2)ΛL(s + z, t + 1) − ΛL(s, t) < EΛL(s + z, t + 1) − ΛL(s, t) − α/2(cid:3) ≤ e−α2N/8.

Thus when the number of chips N > 8

α2 log(

1

1−e−α2/2 ) we get

e−α2/2 + e−α2N/8 < 1.

22

As a consequence, there exists z = (z1, . . . , zN ) ∈ {−1 − δ, 1 − δ}N s.t.

(cid:40)p · z ≥ 0

ΛL(s + z, t + 1 − τ ) − ΛL(s, t − τ ) ≥ − Cγ

|t+1−τ |T −

C
(t+1−τ )2 −

C

|t+1−τ |

− α.

3
2

Now we can bound the ﬁnal loss from below

1
N

N
(cid:88)

i=1

L(s0,i) ≥ ΛL(s0, 0) = ΛL(0, −T ) +

−1
(cid:88)

t=−T

ΛL(st+1, t + 1) − ΛL(st, t)

≥ ΛL(0, −T ) −

−τ
(cid:88)

t=−T +1−τ

(

Cγ
|t|T

+

= ΛL(0, −T ) − αT − O(

γ log τ
T

+

+ α)

C
|t|2 +
1
√
τ

+

C
|t| 3

2

1
τ

) .

Note that

ΛL(0, −T ) = f (β, −T − τ ) − f (β − δT, −τ )

=

≥

1
√
π

1
√
π

(cid:90) β

√

2τ

δT +β√

2(T +τ )

e−x2

dx

(cid:90) β

√

2τ

δT +β
√
2T

e−x2

dx .

β√
2τ

(cid:29) 1 and β√
2T

We want
αT → 0, so we can, for example, let α = T −1−θ/4. As a consequence, for N > 8T 2+θ/2 log(1 −
we have

→ 0. This can be achieved by setting β = τ = T θ

2 where θ ∈ (0, 1). Also we want
e−T −2−θ/2 )−1

√

1
N

N
(cid:88)

i=1

L(s0,i) ≥

1
√
π

(cid:90) T θ/4/

√

2

√

γ+T (θ−1)/2/

√

2

e−x2

dx − O(T −θ/4) .

B Proofs for Section 4

Proof of Theorem 5. For the sake of simplicity, at any ﬁxed time step t, we omit t in the subscripts of s, p, z, E. We
write the increment of ΛU from t to t + 1 as

ΛU (s + z, t + 1) − ΛU (s, t) =

1
N

(cid:88)

si>−δT,si+zi>−δT

f (si + zi, t + 1 − τ ) − f (si, t − τ )

+

+

1
N

1
N

(cid:88)

si>−δT,si+zi≤−δT

f (si + zi, t + 1 − τ ) − f (si, t − τ )

(cid:88)

si≤−δT
(cid:88)

f (si + zi, t + 1 − τ ) − f (si, t − τ )

f (si + zi, t + 1 − τ ) − f (si, t − τ )

≤

1
N

si>−δT,si+zi>−δT

f (si + zi, t + 1 − τ ) − f (si, t − τ )

+

1
N

(cid:88)

si>−δT,si+zi≤−δT

=: A1 + A2 ,

23

where the inequality is due to the fact that

(cid:88)

si≤−δT

f (si + zi, t + 1 − τ ) − f (si, t − τ ) ≤

(cid:88)

si≤−δT

f (si + zi, t + 1 − τ ) − 1 ≤ 0 .

For A1, note that we can apply Taylor expansion since f (·, t + 1 − τ ) is smooth between si and si + zi, and f (si, ·)

is smooth on (t − τ, t + 1 − τ ). Therefore

A1 =

=

=

1
N

1
N

1
N

+

(cid:88)

si>−δT,si+zi>−δT
(cid:88)

si>−δT,si+zi>−δT
(cid:88)

si>−δT,si+zi>−δT
1
N

(cid:88)

si>−δT,si+zi>−δT

f (si + zi, t + 1 − τ ) − f (si, t − τ )

f (si + zi, t + 1 − τ ) − f (si, t + 1 − τ ) + f (si, t + 1 − τ ) − f (si, t − τ )

f (cid:48)(si, t + 1 − τ )zi +

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )z2

i + ∂tf (si, t + 1 − τ )

Ei ,

where Ei is the remainder term consisting of ∂ttf and f (3).

For A2, ﬁrst note that when si + zi ≤ −δT ,

Therefore we can write

f (si + zi, t + 1 − τ ) = 1 = f (−δT, t + 1 − τ ) .

A2 =

=

1
N

1
N

(cid:88)

si>−δT,si+zi≤−δT
(cid:88)

si>−δT,si+zi≤−δT

f (si + zi, t + 1 − τ ) − f (si, t − τ )

f (−δT, t + 1 − τ ) − f (si, t − τ ) .

Since f (·, t + 1 − τ ) is smooth on (−δT, si) and f (si, ·) is smooth on (t − τ, t + 1 − τ ) we use Taylor expansion on
A2

A2 =

=

=

1
N

1
N

1
N

+

(cid:88)

si>−δT,si+zi≤−δT
(cid:88)

si>−δT,si+zi≤−δT
(cid:88)

si>−δT,si+zi≤−δT
1
N

(cid:88)

si>−δT,si+zi≤−δT

f (−δT, t + 1 − τ ) − f (si, t − τ )

f (−δT, t + 1 − τ ) − f (si, t + 1 − τ ) + f (si, t + 1 − τ ) − f (si, t − τ )

−f (cid:48)(si, t + 1 − τ )(δT + si) +

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )(δT + si)2 + ∂tf (si, t + 1 − τ )

Ei ,

where Ei is the remainder term consisting of ∂ttf and f (3). Note that si > −δT, si + zi ≤ −δT implies −zi ≥
si + δT > 0. Also since f (cid:48)(si, t + 1 − τ ) ≤ 0 and f (cid:48)(cid:48)(si, t + 1 − τ ) ≥ 0 when si > −δT , we have the following
inequalities

(cid:40)

−(δT + si)f (cid:48)(si, t + 1 − τ ) ≤ zif (cid:48)(si, t + 1 − τ )
f (cid:48)(cid:48)(si, t + 1 − τ )(δT + si)2 ≤ f (cid:48)(cid:48)(si, t + 1 − τ )z2
i .

As a consequence

A2 ≤

1
N

+

(cid:88)

f (cid:48)(si, t + 1 − τ )zi +

si>−δT,si+zi≤−δT
1
N

(cid:88)

si>−δT,si+zi≤−δT

Ei ,

24

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )z2

i + ∂tf (si, t + 1 − τ )

(cid:88)

f (cid:48)(si, t + 1 − τ )zi +

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )z2

i + ∂tf (si, t + 1 − τ )

Combining inequalities for A1, A2 together we get

ΛU (s + z, t + 1) − ΛU (s, t) ≤

1
N

+

si>−δT
1
N

(cid:88)

Ei .

si>−δT
1
N

(cid:88)

si>−δT

=:A3 +

Ei .

For the remainder 1
N
a constant C such that

(cid:80)

si>−δT Ei, using Lemma 4 and the fact that |zi| ≤ 1 + δ, i ∈ [N ] we can conclude there exists

|

1
N

(cid:88)

si>−δT

Ei| ≤ sup

si>−δT

|Ei| ≤

C
|t + 1 − τ | 3

2

+

C
|t + 1 − τ |2 .

Recall that the player set pi = −

(cid:80)

f (cid:48)(si,t+1−τ )

sj >−δT f (cid:48)(sj ,t+1−τ ) for si > −δT and p · z ≥ 0, we have

(cid:88)

si>−δT

f (cid:48)(si, t + 1 − τ )zi ≤ 0 .

Therefore

∂tf (si, t + 1 − τ ) +

1
2

f (cid:48)(cid:48)(si, t + 1 − τ ) +

z2
i − 1
2

f (cid:48)(cid:48)(si, t + 1 − τ )

z2
i − 1
2

f (cid:48)(cid:48)(si, t + 1 − τ )

3δ
2

f (cid:48)(cid:48)(si, t + 1 − τ )

1
N

1
N

1
N

(cid:88)

si>−δT

(cid:88)

si>−δT
(cid:88)

si>−δT
√
C
γ

A3 ≤

≤

≤

≤

|t + 1 − τ |

.

√

T

The second inequality used the fact that f satisﬁes (4) when si > −δT ; the third inequality used the fact that f (cid:48)(cid:48)(si, t+
1 − τ ) ≥ 0 and z2

i − 1 ≤ 2δ + δ2 ≤ 3δ; the last inequality used Equation (1) and Lemma 4.

Combining the above analysis together we now add up the increment of ΛU from −T to −1,

1
N

N
(cid:88)

i=1

L(s0,i) ≤ ΛU (s0, 0) = ΛU (0, −T ) +

−1
(cid:88)

t=−T

ΛU (st+1, t + 1) − ΛU (st, t)

≤ ΛU (0, −T ) + C

≤ ΛU (0, −T ) + O

−τ
(cid:88)

t=−T +1−τ
√
(cid:18) log τ
γ
√
T

√
γ
√

|t|

T

+

+

1
√
τ

+

+

1
|t|2

.

1
|t| 3
(cid:19)

2

1
τ

The main temr on the right is

ΛU (0, −T ) = f (0, −T − τ ) =

2
√
π

(cid:90) ∞

√

δT /

2(T +τ )

e−x2

dx

We set τ = T θ where θ ∈ (0, 1), then

1
N

N
(cid:88)

i=1

L(s0,i) ≤

2
√
π

(cid:90) ∞

(cid:113) γ

1+T θ−1

e−x2

dx + O(T − θ

2 ) .

25

Before proving lower bound we give another key ingredient.

Lemma 12. [Lemma 2 of [Mukherjee and Schapire 2008]] For any sequence a1, . . . , an belonging to [0, U ] for some
constant U > 0, the following holds

min
P ⊂[n]

|

(cid:88)

i∈P

ai −

(cid:88)

j∈[n]\P

aj| ≤ U .

Now we give the proof of the lower bound

Proof of Theorem 6. We consider the adversary that only takes {−1 + δ, 0, 1 − δ}. With this choice of action set
the chips always lie on multiples of 1 − δ. Moreover, the adversary assigns zi = 0 whenever si ≤ −δT − β and
|zi| = 1 − δ otherwise.

We bound the increment of ΛL as following. First note that when si ≤ −δT − β, the adversary chooses zi = 0,

which implies that

As a consequence

f (si + zi + β, t + 1 − τ ) = f (si + β, t − τ ) = 1 .

ΛL(s + z, t + 1) − ΛL(s, t) =

1
N

(cid:88)

si>−δT −β

f (si + zi + β, t + 1 − τ ) − f (si + β, t − τ ) .

Also note that si is a multiple of 1 − δ and by our choice of β, −δT − β = −(cid:100) δT +T

1−δ (cid:101)(1 − δ) is also a multiple
of 1 − δ. As a consequence when si > −δT − β we have si + zi ≥ si − (1 − δ) ≥ −δT − β. Therefore f (·, t + 1 − τ )
is smooth between si + β and si + zi + β, and f (si + β, ·) is smooth on (t − τ, t + 1 − τ ). We apply Taylor expansion
in the case si > −δT − β

θ
2

1
N

1
N

+

=

(cid:88)

f (si + zi + β, t + 1 − τ ) − f (si + β, t − τ )

si>−δT −β

(cid:88)

(cid:18)

zif (cid:48)(si + β, t + 1 − τ ) + ∂tf (si + β, t + 1 − τ ) +

si>−δT −β
1
(cid:88)
N

si>−δT −β

Ei

(1 − δ)2
2

f (cid:48)(cid:48)(si + β, t + 1 − τ )

(cid:19)

=

≥

1
N

1
N

(cid:88)

si>−δT −β
(cid:88)

si>−δT −β

zif (cid:48)(si + β, t + 1 − τ ) +

−2δ + δ2
2

f (cid:48)(cid:48)(si + β, t + 1 − τ ) +

1
N

(cid:88)

Ei

si>−δT −β

zif (cid:48)(si + β, t + 1 − τ ) − δf (cid:48)(cid:48)(si + β, t + 1 − τ ) +

1
N

(cid:88)

Ei ,

si>−δT −β

In the second equality we used the fact that f satisﬁes Equation (4) and in the inequality we used the fact that f (cid:48)(cid:48)(si +
β, t + 1 − τ ) ≥ 0.

Ei is the remainder term consisting of ∂ttf and f (3). Using Lemma 4 and the fact that |zi| ≤ 1 − δ there exists a

constant C such that

|

1
N

(cid:88)

si>−δT −β

Ei| ≤ sup

si>−δT −β

|Ei| ≤

C
|t + 1 − τ | 3

2

+

C
|t + 1 − τ |2 .

We can bound the second order term

−δf (cid:48)(cid:48)(si + β, t + 1 − τ ) ≥ −

√

γ

C

|t + 1 − τ |

,

√

T

by Equation (1) and Lemma 4.

To bound the ﬁrst order term (cid:80)

our case ai = −f (cid:48)(si + β, t + 1 − τ ) and U =

si>−δT −β zif (cid:48)(si + β, t + 1 − τ ) we apply Lemma 12. More speciﬁcally, in
by Lemma 4. Lemma 12 conﬁrms there exists a subset

C√

|t+1−τ |

26

P ⊂ {i : si > −δT − β} (note that we can always make the ﬁrst inequality below holds by swapping P and
{k : sk > −δT − β}\P ) such that

(cid:40)(cid:80)
| (cid:80)

i∈P pi − (cid:80)
i∈P ai − (cid:80)

j∈{k:sk>−δT −β}\P pj ≥ 0
j∈{k:sk>−δT −β}\P aj| ≤ U

Thus by setting zi = 1 − δ for i ∈ P and zi = −1 + δ for i ∈ {k : sk > −δT − β}\P , the adversary makes

(cid:40)(cid:80)

(cid:80)

si>−δT −β zif (cid:48)(si + β, t + 1 − τ ) ≥ −
si>−δT −β pi · zi ≥ 0

C√

|t+1−τ |

Moreover since zi = 0 for si ≤ −δT − β, we have

(cid:40)(cid:80)

i zif (cid:48)(si + β, t + 1 − τ ) ≥ −

C√

|t+1−τ |

p · z ≥ 0

As a consequence, we can bound the ﬁnal loss from below

1
N

N
(cid:88)

i=1

L(s0,i) ≥ ΛL(s0, 0) = ΛL(0, −T ) +

−1
(cid:88)

t=−T

ΛL(st+1, t + 1) − ΛL(st, t)

≥ ΛL(0, −T ) − C

−τ
(cid:88)

1
N |t| 1
2
√

√
γ
√

|t|

T

+

+

1
|t| 3

2

γ log τ
√

T

+

1
√
τ

+

1
τ

+

(cid:19)

1
|t|2

.

t=−T +1−τ

(cid:18) √

T + τ
N

+

= ΛL(0, −T ) − O

Now we compute the main term

ΛL(0, −T ) =f (β, −T − τ ) − f (−δT + β, −τ )

=

2
√
π

≥

2
√
π

(cid:90) β

√

2τ

δT +β√

2(T +τ )

e−x2

dx

(cid:90) β

√

2τ

δT +β
√
2T

e−x2

dx .

2+θ
4 we get

e−x2

dx − O(T − θ

4 ) .

Finally plugging in β = τ = (cid:100) δT +T

1−δ (cid:101)(1 − δ) − δT and N = T

θ
2

√

2

(cid:90) T θ/4/
(cid:18)

√

γ+

T

θ
2 +1

(cid:19)

/

√

2T

1
N

N
(cid:88)

i=1

L(s0,i) ≥

2
√
π

C Proofs for Section 5

We ﬁrst prove the upper bound

27

Proof of Theorem 7. We write the increment of ΛU from t to t + 1 as

ΛU (s + z, t + 1) − ΛU (s, t)

=

=

=

1
N

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

f (si − β + zi, t + 1 − τ ) − f (si − β, t − τ )

f (si − β + zi, t + 1 − τ ) − f (si − β, t + 1 − τ ) + f (si − β, t + 1 − τ ) − f (si − β, t − τ )

f (cid:48)(si − β, t + 1 − τ )zi +

1
2

f (cid:48)(cid:48)(si − β, t + 1 − τ )z2

i + ∂tf (si − β, t + 1 − τ ) +

1
N

N
(cid:88)

i=1

Ei .

The remainder term Ei consists of f (3) and ∂ttf .

Recall that the player set pi = − f (cid:48)(si−β,t+1−τ )

(cid:80)N

j=1 f (cid:48)(sj −β,t+1−τ ) and p · z ≥ 0, we have

N
(cid:88)

i=1

f (cid:48)(si − β, t + 1 − τ )zi ≤ 0 .

Therefore

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

≤

≤0 ,

f (cid:48)(si − β, t + 1 − τ )zi +

1
2

f (cid:48)(cid:48)(si − β, t + 1 − τ )z2

i + ∂tf (si − β, t + 1 − τ )

∂tf (si − β, t + 1 − τ ) +

1
2

f (cid:48)(cid:48)(si − β, t + 1 − τ )

where the ﬁrst inequality used the fact that |zi|2 = 1 and the second inequality used Equation (11).

For the remainder 1
N

constant C such that

(cid:80)N

i=1 Ei, using Lemma 1 and the fact that |zi| ≤ 1, i ∈ [N ] we can conclude there exists a

|

1
N

N
(cid:88)

i=1

Ei| ≤ sup
i∈[N ]

|Ei| ≤

C
|t + 1 − τ | 3

2

+

C
|t + 1 − τ |2 .

Combining the above analysis together we now add up the increment of ΛU from −T to −1, we have

1
N

N
(cid:88)

i=1

L(s0,i) ≤ ΛU (s0, 0) = ΛU (0, −T ) +

−1
(cid:88)

t=−T

ΛU (st+1, t + 1) − ΛU (st, t)

≤ ΛU (0, −T ) + C

≤ ΛU (0, −T ) + O

−τ
(cid:88)

t=−T +1−τ
(cid:18) 1
1
√
τ
τ

+

1
|t| 3

2

1
|t|2 +
(cid:19)

.

Note that

ΛU (0, −T ) = f (−β, −T − τ ) + 1 − f (−R − β, −τ )
(cid:90) β

√

2τ

= 1 −

1
√
π

e−x2

dx .

β−R√

2(T +τ )

Plugging in β = τ = T θ

2 we get

1
N

N
(cid:88)

i=1

L(s0,i) ≤ 1 −

1
√
π

(cid:90) T θ/4/

√

2

(cid:113)

−

γ/(1+T θ/2−1)+ T (θ−1)/2

(cid:114)

2(1+T θ/2−1)

e−x2

dx + O(T − θ

4 ) .

28

Next we proceed to lower bound.

Proof of Theorem 8. We bound the increment of ΛL using Taylor expansion

ΛL(s + z, t + 1) − ΛL(s, t)

=

=

1
N

1
N

+

N
(cid:88)

i=1

f (si + zi + β, t + 1 − τ ) − f (si + β, t − τ )

N
(cid:88)

(cid:18)

zif (cid:48)(si + β, t + 1 − τ ) + ∂tf (si + β, t + 1 − τ ) +

i=1

1
N

N
(cid:88)

i=1

Ei

=

1
N

N
(cid:88)

i=1

zif (cid:48)(si + β, t + 1 − τ ) +

1
N

N
(cid:88)

i=1

Ei ,

f (cid:48)(cid:48)(si + β, t + 1 − τ )

(cid:19)

1
2

In the second equality we used the fact that |zi| = 1 and in the last equality we used the fact that f satisﬁes Equation
(11). The remainder term Ei consists of f (3)(·, t+1−τ ) and ∂ttf (si −β, ·). Using Lemma 1 and the fact that |zi| = 1
there exists a constant C such that

N
(cid:88)

|

1
N

Ei| ≤ sup
i∈[N ]

|Ei| ≤

C
|t + 1 − τ | 3

2

+

C
|t + 1 − τ |2 .

i=1
To bound the ﬁrst order term (cid:80)N
ai = −f (cid:48)(si + β, t + 1 − τ ) and U =

i=1 zif (cid:48)(si + β, t + 1 − τ ) we apply Lemma 12. More speciﬁcally, in our case
by Lemma 1. Lemma 12 conﬁrms there exists a subset P ⊂ [N ]

C√

|t+1−τ |

(note that we can always make the ﬁrst inequality below holds by swapping P and [N ]\P ) such that
i∈P pi − (cid:80)
i∈P ai − (cid:80)

j∈[N ]\P pj ≥ 0
j∈[N ]\P aj| ≤ U

(cid:40)(cid:80)
| (cid:80)

Thus by setting zi = 1 for i ∈ P and zi = −1 for i ∈ [N ]\P , the adversary makes

(cid:40)(cid:80)N

i=1 zif (cid:48)(si + β, t + 1 − τ ) ≥ −
i=1 pi · zi ≥ 0 .

(cid:80)N

C√

|t+1−τ |

As a consequence, we bound the ﬁnal loss from below:

1
N

N
(cid:88)

i=1

L(s0,i) ≥ ΛL(s0, 0) = ΛL(0, −T ) +

−1
(cid:88)

t=−T

ΛL(st+1, t + 1) − ΛL(st, t)

≥ ΛL(0, −T ) − C

−τ
(cid:88)

1
N |t| 1

2

+

t=−T +1−τ

(cid:18) √

T + τ
N

+

1
√
τ

+

+

1
|t|2

.

1
|t| 3
(cid:19)

2

1
τ

Now we compute the main term

= ΛL(0, −T ) − O

ΛL(0, −T ) =f (β, −T − τ ) − f (−R + β, −τ )

=g(β + R, −T − τ ) − g(β, −τ )

=

1
√
π

≥

1
√
π

(cid:90) β

√

2τ

R+β√

2(T +τ )

e−x2

dx

e−x2

dx .

(cid:90) β

√

2τ

R+β
√
2T

29

Finally plugging in β = τ = T θ

2 and N = T

2+θ
4 we get,
√

θ
4 /

2

(cid:90) T

L(s0,i) ≥

1
√
π

√

γ+T

θ−1

2 /

√

2

e−x2

dx − O(T − θ

4 ) .

1
N

N
(cid:88)

i=1

D Proofs for Section 6

Proof of Theorem 9. We follow the proof in Theorem 5 and replace δT by R. The increment of the potential from t
to t + 1 is bounded as

ΛU (s + z, t + 1) − ΛU (s, t) ≤

1
N

+

(cid:88)

f (si + zi, t + 1 − τ ) − f (si, t − τ )

si>−R,si+zi>−R
1
N

(cid:88)

si>−R,si+zi≤−R

f (si + zi, t + 1 − τ ) − f (si, t − τ )

For A1, we apply Taylor expansion and get

= : A1 + A2 .

A1 =

1
N

+

(cid:88)

f (cid:48)(si, t + 1 − τ )zi +

si>−R,si+zi>−R
1
N

(cid:88)

si>−R,si+zi>−R

Ei ,

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )z2

i + ∂tf (si, t + 1 − τ )

where Ei, in the case of si > −R, si + zi > −R, is the remainder term consisting of f (3) and ∂ttf .

For A2, we can write

A2 =

1
N

+

≤

1
N

+

(cid:88)

f (cid:48)(si, t + 1 − τ )(−R − si) +

si>−R,si+zi≤−R
1
N

(cid:88)

si>−R,si+zi≤−R

Ei

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )(−R − si)2 + ∂tf (si, t + 1 − τ )

(cid:88)

f (cid:48)(si, t + 1 − τ )zi +

si>−R,si+zi≤−R
1
N

(cid:88)

si>−R,si+zi≤−R

Ei .

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )z2

i + ∂tf (si, t + 1 − τ )

The remainder term Ei consists of f (3) and ∂ttf , and the inequality holds since

(cid:40)

−(R + si)f (cid:48)(si, t + 1 − τ ) ≤ zif (cid:48)(si, t + 1 − τ )
f (cid:48)(cid:48)(si, t + 1 − τ )(R + si)2 ≤ f (cid:48)(cid:48)(si, t + 1 − τ )z2
i .

As a consequence,

ΛU (s + z, t + 1) − ΛU (s, t) ≤

1
N

+

(cid:32)

(cid:88)

f (cid:48)(si, t + 1 − τ )zi +

si>−R
1
N

(cid:88)

si>−R

Ei .

1
2

f (cid:48)(cid:48)(si, t + 1 − τ )z2

i + ∂tf (si, t + 1 − τ )

(cid:33)

f (cid:48)(cid:48)(si, t + 1 − τ )

+

1
2

1
N

(cid:88)

Ei

si>−R

(cid:88)

∂tf (si, t + 1 − τ ) +

≤

≤

1
N

si>−R
(cid:88)

si>−R

Ei .

30

The second inequality used the fact that

(cid:40)(cid:80)

si>−R f (cid:48)(si, t + 1 − τ )zi ≤ 0

f (cid:48)(cid:48)(si, t + 1 − τ )z2

i ≤ f (cid:48)(cid:48)(si, t + 1 − τ ) f or si > −R ;

and the last inequality holds since f satisﬁes Equation (12).

Using Lemma 4 and the fact that |zi| ≤ 1, i ∈ [N ] we get

|

1
N

(cid:88)

si>−R

Ei| ≤ sup
si>−R

|Ei| ≤

C
|t + 1 − τ | 3

2

+

C
|t + 1 − τ |2

We now repeat the calculation done in Theorem 5,

1
N

N
(cid:88)

i=1

L(s0,i) ≤ ΛU (s0, 0) = ΛU (0, −T ) +

−1
(cid:88)

t=−T

ΛU (st+1, t + 1) − ΛU (st, t)

1
|t| 3

2

≤ ΛU (0, −T ) + C

−τ
(cid:88)

t=−T −τ

≤ ΛU (0, −T ) + O(

= ΛU (0, −T ) + O(

1
√
τ
1
√
τ

+

+

1
|t|2 +
1
τ
1
τ

)

)

Note that

ΛU (0, −T ) = f (0, −T − τ ) =

2
√
π

(cid:90) ∞

R√

2(T +τ )

e−x2

dx

We set τ = T θ where θ ∈ (0, 1), then

1
N

N
(cid:88)

i=1

L(s0,i) ≤

2
√
π

(cid:90) ∞

(cid:113) γT

T +T θ

e−x2

dx + O(T − θ

2 ) .

Next we show the proofs for lower bounds.

Proof of Theorem 10. We consider the adversary that only takes {−1, 0, 1}. With this choice of action set the chips
always lie on integer points. Moreover, the adversary assigns zi = 0 whenever si ≤ −R − β and |zi| = 1 otherwise.
We bound the increment of ΛL as following. First note that when si ≤ −R − β, the adversary chooses zi = 0,

which implies that

As a consequence

f (si + zi + β, t + 1 − τ ) = f (si + β, t − τ ) = 1 .

ΛL(s + z, t + 1) − ΛL(s, t) =

1
N

(cid:88)

si>−R−β

f (si + zi + β, t + 1 − τ ) − f (si + β, t − τ ) .

Also note that si is an integer and by our choice of β, R + β = (cid:100)R + T θ

2 (cid:101) is also an integer. Therefore when
si > −R − β we have si + zi ≥ si − 1 ≥ −R − β. As a consequence f (·, t + 1 − τ ) is smooth between si + β and

31

si + zi + β, and f (si + β, ·) is smooth on (t − τ, t + 1 − τ ). We apply Taylor expansion in the case si > −R − β

1
N

1
N

+

=

=

1
N

(cid:88)

si>−R−β

(cid:88)

si>−R−β
1
(cid:88)
N

si>−R−β
(cid:88)

si>−R−β

f (si + zi + β, t + 1 − τ ) − f (si + β, t − τ )

(cid:18)

zif (cid:48)(si + β, t + 1 − τ ) + ∂tf (si + β, t + 1 − τ ) +

f (cid:48)(cid:48)(si + β, t + 1 − τ )

(cid:19)

1
2

Ei

zif (cid:48)(si + β, t + 1 − τ ) +

1
N

(cid:88)

Ei ,

si>−R−β

In the second equality we used the fact that f satisﬁes Equation 12. The remainder term Ei consists of f (3)(·, t+1−τ )
and ∂ttf (si + β, ·). Using Lemma 4 and the fact that |zi| ≤ 1 there exists a constant C such that

|

1
N

(cid:88)

Ei| ≤ sup

si>−R−β

|Ei| ≤

C
|t + 1 − τ | 3

2

+

C
|t + 1 − τ |2 .

si>−R−β
To bound the ﬁrst order term (cid:80)

our case ai = −f (cid:48)(si + β, t + 1 − τ ) and U =

|t+1−τ |

si>−R−β zif (cid:48)(si + β, t + 1 − τ ) we apply Lemma 12. More speciﬁcally, in
by Lemma 4. Lemma 12 conﬁrms there exists a subset

C√

P ⊂ {k : sk > −R − β} (note that we can always make the ﬁrst inequality below holds by swapping P and
{k : sk > −R − β}\P ) such that

(cid:40)(cid:80)
| (cid:80)

i∈P pi − (cid:80)
i∈P ai − (cid:80)

j∈{k:sk>−R−β}\P pj ≥ 0
j∈{k:sk>−R−β}\P aj| ≤ U

Thus by setting zi = 1 for i ∈ P and zi = −1 for i ∈ {j : sj > −R − β}\P , the adversary makes

(cid:40)(cid:80)

(cid:80)

si>−R−β zif (cid:48)(si + β, t + 1 − τ ) ≥ −
si>−R−β pi · zi ≥ 0

C√

|t+1−τ |

Moreover since zi = 0 for si ≤ −R − β, we have

(cid:40)(cid:80)

i zif (cid:48)(si + β, t + 1 − τ ) ≥ −

C√

|t+1−τ |

p · z ≥ 0

As a consequence, we can bound the ﬁnal loss from below

1
N

N
(cid:88)

i=1

L(s0,i) ≥ ΛL(s0, 0) = ΛL(0, −T ) +

−1
(cid:88)

t=−T

ΛL(st+1, t + 1) − ΛL(st, t)

≥ ΛL(0, −T ) − C

−τ
(cid:88)

= ΛL(0, −T ) − O

1
N |t| 1

2

+

t=−T +1−τ

(cid:18) √

T + τ
N

+

1
√
τ

+

+

1
|t|2

.

1
|t| 3
(cid:19)

2

1
τ

Now we compute the main term

ΛL(0, −T ) =f (β, −T − τ ) − f (−R + β, −τ )

=

2
√
π

≥

2
√
π

(cid:90) β

√

2τ

R+β√

2(T +τ )

e−x2

dx

e−x2

dx .

(cid:90) β

√

2τ

R+β
√
2T

32

Finally plugging in β = τ = (cid:100)R + T θ

2 (cid:101) − R and N = T

2+θ
4 we get,

1
N

N
(cid:88)

i=1

L(s0,i) ≥

2
√
π

√

θ
4 /

2

(cid:90) T

√

γ+ T

θ
2 +1√
2T

e−x2

dx − O(T − θ

4 ) .

E The leading-order min-max in Section 7.2

We want to show that for any nonzero ξ ∈ RN with nonpositive components, and any bounded A ⊂ RN containing a
neighborhood of the origin,

min
p∈∆N

max
z∈A, p·z≥0

ξ · z = 0,

(26)

and this value is achieved only when p = −ξ/(cid:107)ξ(cid:107)1 and p · z = 0. (This assertion was used in Section 7.2 with
A = ZN − δ(cid:15)1 and ξ = ∇Λ(cid:15); see Equation (21) and the text just after it.)

We ﬁrst prove the following geometric lemma.

Lemma 13. Suppose a, b ∈ RN are non-zero vectors and only have non-negative components, moreover if they are
not parallel, then there exists a vector v such that a · v > 0 and b · v < 0.

Proof of lemma 13. We assume v = µa − b, µ > 0. To satisfy a · v > 0 and b · v < 0, µ must be such that

(cid:40)

a · b < µ(cid:107)a(cid:107)2
2
a · b < 1
µ (cid:107)b(cid:107)2
2.

If a · b = 0 then the above inequalities hold for any µ > 0. For the case of a · b > 0, since a and b are not parallel,

Set µ = µ0 = a·b
(cid:107)a(cid:107)2
2

> 0, we have

(a · b)2 < (cid:107)a(cid:107)2

2(cid:107)b(cid:107)2
2.

(cid:40)

a · b = µ(cid:107)a(cid:107)2
2
a · b < 1
µ (cid:107)b(cid:107)2
2.

Thus setting µ to be slightly larger than µ0 will meet the constraints.

Turning now to (26), consider ﬁrst what happens if p ∈ ∆N and −ξ are not parallel. Then by the Lemma, there

exists a vector v such that

(cid:40)

p · v > 0
ξ · v > 0.

Replacing v by λv for λ > 0 leaves the conclusion unchanged. Since A contains a neighborhood of the origin, we
conclude if p and −ξ are not parallel, then

On the other hand, if p is parallel to −ξ, i.e. p = −ξ/(cid:107)ξ(cid:107), then it’s clear that

max
z∈A, p·z≥0

∇ξ · z > 0.

and equality is obtained exactly when p · z = 0. (There actually exists such z(cid:48), since by A contains a neighborhood of
the origin.) This completes the veriﬁcation of our assertion.

max
z∈A, p·z≥0

∇ξ · z = 0,

33

References

Erhan Bayraktar, Ibrahim Ekren, and Xin Zhang. Finite-time 4-expert prediction problem. Communications in
Partial Differential Equations, 45(7):714–757, 2020a. doi: 10.1080/03605302.2020.1712418. URL https:
//doi.org/10.1080/03605302.2020.1712418.

Erhan Bayraktar, Ibrahim Ekren, and Yili Zhang. On the asymptotic optimality of the comb strategy for prediction
with expert advice. The Annals of Applied Probability, 30(6):2517 – 2546, 2020b. doi: 10.1214/20-AAP1565.
URL https://doi.org/10.1214/20-AAP1565.

Nicol`o Cesa-Bianchi, Yoav Freund, David Helmbold, and Manfred Warmuth. On-line prediction and conversion

strategies. Machine Learning, 25:71–110, 10 1996. doi: 10.1023/A:1018348209754.

Nadejda Drenska and Robert V. Kohn. Prediction with Expert Advice: A PDE perspective. Journal of Nonlinear

Science, 30:137–173, 2020.

Yoav Freund. Boosting a weak learning algorithm by majority. In Proceedings of the Third Annual Workshop on
Computational Learning Theory, COLT ’90, page 202–216, San Francisco, CA, USA, 1990. Morgan Kaufmann
Publishers Inc. ISBN 1558601465.

Yoav Freund. An adaptive version of the boost by majority algorithm. Mach. Learn., 43(3):293–318, June 2001. ISSN
0885-6125. doi: 10.1023/A:1010852229904. URL https://doi.org/10.1023/A:1010852229904.

Yoav Freund and Manfred Opper. Drifting games and brownian motion. Journal of Computer and System Sci-
ISSN 0022-0000. doi: https://doi.org/10.1006/jcss.2001.1802. URL http:

ences, 64(1):113 – 132, 2002.
//www.sciencedirect.com/science/article/pii/S0022000001918021.

Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical

Association, 58(301):13–30, March 1963. URL http://www.jstor.org/stable/2282952?

Vladimir A. Kobzar, Robert V. Kohn, and Zhilei Wang. New potential-based bounds for the geometric-stopping version
of prediction with expert advice. In Proceedings of The First Mathematical and Scientiﬁc Machine Learning Confer-
ence, volume 107 of Proceedings of Machine Learning Research, pages 537–554, Princeton University, Princeton,
NJ, USA, 20–24 Jul 2020a. PMLR. URL http://proceedings.mlr.press/v107/kobzar20a.html.

Vladimir A. Kobzar, Robert V. Kohn, and Zhilei Wang. New potential-based bounds for prediction with expert advice.
In Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learn-
ing Research, pages 2370–2405. PMLR, 09–12 Jul 2020b. URL http://proceedings.mlr.press/v125/
kobzar20a.html.

Haipeng Luo and Robert E Schapire.

A drifting-games analysis for online learning and applications
In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
to boosting.
editors, Advances in Neural
Information Processing Systems 27, pages 1368–1376. Curran Associates,
Inc., 2014. URL http://papers.nips.cc/paper/5469-a-drifting-games-analysis-for-
online-learning-and-applications-to-boosting.pdf.

Indraneel Mukherjee and Robert Schapire. Learning with continuous experts using drifting games. In Algorithmic
ISBN 978-3-540-87986-2. doi: 10.1007/978-3-540-

Learning Theory, volume 411, pages 240–255, 10 2008.
87987-9 22.

Robert E. Schapire. Drifting games. In Proceedings of the Twelfth Annual Conference on Computational Learning
ISBN

Theory, COLT ’99, page 114–124, New York, NY, USA, 1999. Association for Computing Machinery.
1581131674. doi: 10.1145/307400.307421. URL https://doi.org/10.1145/307400.307421.

Robert E Schapire. Drifting games. Machine Learning, 43(3):265–291, 2001. URL https://doi.org/10.1023/

A:1010800213066.

34

Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.

ISBN

0262017180.

Zhiyu Zhang, Ashok Cutkosky, and Ioannis Paschalidis. PDE-based optimal strategy for unconstrained online learn-
ing. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,
Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine
Learning Research, pages 26085–26115. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/
v162/zhang22d.html.

35

