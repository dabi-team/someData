1
2
0
2

y
a
M
8
2

]

G
L
.
s
c
[

1
v
7
2
1
4
1
.
5
0
1
2
:
v
i
X
r
a

Risk-Aware Transfer in Reinforcement Learning
using Successor Features

Michael Gimelfarb∗
University of Toronto
mike.gimelfarb@mail.utoronto.ca

André Barreto
DeepMind
andrebarreto@google.com

Scott Sanner∗
University of Toronto
ssanner@mie.utoronto.ca

Chi-Guhn Lee
University of Toronto
cglee@mie.utoronto.ca

Abstract

Sample efﬁciency and risk-awareness are central to the development of practical
reinforcement learning (RL) for complex decision-making. The former can be
addressed by transfer learning and the latter by optimizing some utility function of
the return. However, the problem of transferring skills in a risk-aware manner is not
well-understood. In this paper, we address the problem of risk-aware policy transfer
between tasks in a common domain that differ only in their reward functions, in
which risk is measured by the variance of reward streams. Our approach begins
by extending the idea of generalized policy improvement to maximize entropic
utilities, thus extending policy improvement via dynamic programming to sets of
policies and levels of risk-aversion. Next, we extend the idea of successor features
(SF), a value function representation that decouples the environment dynamics
from the rewards, to capture the variance of returns. Our resulting risk-aware
successor features (RaSF) integrate seamlessly within the RL framework, inherit
the superior task generalization ability of SFs, and incorporate risk-awareness into
the decision-making. Experiments on a discrete navigation domain and control of
a simulated robotic arm demonstrate the ability of RaSFs to outperform alternative
methods including SFs, when taking the risk of the learned policies into account.

1

Introduction

Reinforcement learning (RL) is a general framework for solving sequential decision-making problems,
in which an agent interacts with an environment and receives continuous feedback in the form of
rewards. However, many classical algorithms in RL do not explicitly address the need for safety,
making them unreliable and difﬁcult to deploy in many real-world applications [10]. One reason for
this is the relative sample inefﬁciency of model-free RL algorithms, which often require millions of
costly or dangerous interactions with the environment or fail to converge altogether [42, 44]. Transfer
learning addresses these problems by incorporating prior knowledge or skills [22, 39]. Despite this,
using the expected return as a measure of optimality could still lead to undesirable behavior such as
excessive risk-taking, since low-probability catastrophic outcomes with negative reward and high
variance could be underrepresented [28]. For this reason, risk-awareness is becoming an important
aspect in the design of practical RL [14]. Thus, an ultimate goal of developing reliable systems
should be to ensure that they are both sample efﬁcient and risk-aware.

∗Afﬁliate to Vector Institute, Toronto, Canada.

Preprint. Under review.

 
 
 
 
 
 
We take a step in this direction by studying the problem of risk-aware policy transfer between
tasks with different goals. A powerful way to tackle this problem in the risk-neutral setting is the
GPI/GPE framework, of which successor features (SF) are a notable example [2]. Here, generalized
policy improvement (GPI) provides a theoretical framework for transferring policies with monotone
improvement guarantees, while generalized policy evaluation (GPE) facilitates the efﬁcient evaluation
of policies on novel tasks and is a key component in satisfying the assumptions of GPI in practice.
Together, GPI/GPE provide strong transfer beneﬁts in novel task instances even before any direct
interaction with them has taken place, a phenomenon we call task generalization. The key to the
superb generalization of GPI/GPE lies in their ability to directly exploit the structure of the task
space, taking advantage of subtle differences and commonalities between task goals to transfer skills
seamlessly in a composable manner. This property could be an effective way of tackling problems
in ofﬂine RL [23], such as the transfer of skills learned in a simulator to a real-world environment.
However in many cases, such as helicopter ﬂight control for example [16], making one wrong decision
could lead to catastrophic outcomes. Hence, being risk-aware could offer one way to avoid worst-case
outcomes when transferring skills in real-world settings.

Contributions. We contribute a novel successor feature framework for transferring policies with
the goal of maximizing the entropic utility of return in episodic (Section 2.2) and discounted (Section
A.2) MDPs. Intuitively, the entropic utility encourages agents to follow policies with predictable
and controllable returns characterized by low variance, thus providing a natural way to incorporate
risk-awareness. Furthermore, while our theoretical framework could be extended to other classes
of utility functions, the entropic utility has many favorable mathematical properties [13, 21] that we
exploit directly in this work to achieve optimal transfer (Lemma 1, Theorem 1 and 2). We ﬁrst show
that risk-neutral policy evaluation can break the optimality guarantees of GPI with respect to the
entropic utility, even when the source policies optimize entropic utilities (Section 3.1). We then show
that by incorporating risk-sensitive policy evaluation into GPE, the strong theoretical guarantees of
GPI carry through to the risk-aware setting (Section 3.2). Next, we derive a form of risk-aware GPE
based on the mean-variance approximation, in which the sufﬁcient statistics of the return distribution
can be computed directly (Section 3.3) or by leveraging recent developments in distributional RL [6].

Our resulting approach, which we call Risk-Aware Successor Features (RaSF), exploits the task
structure to achieve task generalization, where emphasis is placed on avoiding high volatility of
returns. Our approach is also complementary to other advances in successor features, including feature
learning [3], universal approximation [8], exploration [20], and non-stationary reward preferences
[4]. Empirical evaluations on discrete navigation and continuous robot control domains (Section 4)
demonstrate the ability of RaSFs to better manage the trade-off between return and risk and avoid
catastrophic outcomes, while providing excellent generalization on novel tasks in the same domain.

Related Work. The entropic and mean-variance objectives are popular ways of incorporating
risk-awareness in RL [7, 11, 19, 26, 27, 29, 34, 37, 43]. However, transferring learned skills between
tasks while taking risk into account is a difﬁcult problem. One way to implement risk-aware transfer
is to learn a critic [36] or teacher [41] that can guide an agent toward safer behaviors on future tasks.
The risk-aware transfer of a policy from a simulator to a real-world setting has also been studied in the
area of robotics [17]. Another approach for reusing policies is the probabilistic policy reuse of García
and Fernández [15], but requires strong assumptions on the task space. Hierarchical RL (HRL) is
another related approach that relies on hierarchical abstractions, enabling an agent to decompose
tasks into a hierarchy of sub-tasks, and facilitating the transfer of temporally-extended skills from
sub-tasks to the parent task. The CISR approach of Mankowitz et al. [25] is the ﬁrst to investigate
safety explicitly within HRL, followed up by work on safe options [18, 24]. However, none of the
existing work takes advantage of the compositional structure of task rewards to transfer skills while
optimizing the variance-adjusted return, which is the problem we tackle in this paper (see Table 1).

2 Preliminaries

2.1 Markov Decision Process

Sequential decision-making in this paper follows the Markov decision process (MDP), deﬁned as
R is
a four-tuple
a bounded reward function, where r(s, a, s(cid:48)) is the immediate reward received upon transitioning

is a ﬁnite set of actions; r :

is a set of states;

S × A × S →

, r, P

:
(cid:105)

(cid:104)S

A

A

S

,

2

RL [7, 11, 19, 26, 27, 29, 34, 37, 43]

Transfer [15, 17, 18, 24, 25, 36, 41]

Successor Features [2–4, 8]

RaSF (Ours)

Transfers Skills
(cid:55)

(cid:51)

(cid:51)

(cid:51)

Exploits Task Structure Risk-Sensitive

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

Table 1: Comparison of RaSF with relevant work in transfer learning and risk-aware RL.

to state s(cid:48) after taking action a in state s; and P :
for state dynamics, where P (s(cid:48)
taking action a in state s.

|

) is the transition function
s, a) is the probability of transitioning to state s(cid:48) immediately after

S × A × S →

[0,

∞

In the episodic MDP setting, decisions are made over a horizon
We deﬁne a stochastic Markov policy as a mapping π :
set of all probability distributions over
π :
sum of future rewards after initially taking action a in state s,

N.
) denotes the
A
. Similarly, a deterministic Markov policy is a mapping
. In the risk-neutral setting, the goal is to ﬁnd a policy π that maximizes the expected

0, 1, . . . T
}
{
), where P(

S × T → A

=
T
P(

S × T →

where T

A

A

∈

Qπ

h(s, a) = E

st+1

P (

∼

st,at)

·|

(cid:34) T

(cid:88)

t=h

r(st, at, st+1)

(cid:12)
(cid:12)
(cid:12) sh = s, ah = a, at ∼

(cid:35)

πt(st)

.

In this case, it is possible to show that a deterministic Markov policy π∗ is optimal [31]. The
theoretical framework in this paper also allows for time-dependent reward or transition functions.

2.2 Entropic Utility Maximization

We incorporate risk-awareness into the decision-making by maximizing the entropic utility Uβ of the
cumulative reward, deﬁned for a ﬁxed β

log E (cid:2)eβR(cid:3) ,

(1)

∈
Uβ[R] =

R as
1
β

R. An important property of the
for real-valued random variables R on a bounded support Ω
entropic utility is the Taylor expansion Uβ[R] = E[R] + β
2 Var[R] + O(β2). Interpreting the risk as
return variance, β can now be interpreted as the risk aversion of the agent: choosing β < 0 (β > 0)
leads to risk-averse (risk-seeking) behavior, while β = 0 is risk-neutral, e.g. U0[R] = E[R].
Specializing (1) to the MDP setting, the goal is to maximize

⊂

π
h,β(s, a) = Uβ
Q

(cid:34) T

(cid:88)

(cid:35)

r(st, πt(st), st+1)

(2)

t=h
over all policies starting from sh = s and ah = a. As in the risk-neutral setting, it is possible to show
π
h,β can be computed iteratively
that a deterministic Markov policy is optimal [5]. Furthermore,
Q
through time using the Bellman equation [9, 30]:

π
h,β(s, a) = Uβ
Q
1
β

=

(cid:2)r(s, a, s(cid:48)) +

log E
s(cid:48)

P (

s,a)

∼

·|

h+1,β(s(cid:48), πh+1(s(cid:48)))(cid:3)
π
Q
(cid:2)exp (cid:8)β (cid:0)r(s, a, s(cid:48)) +

h+1,β(s(cid:48), πh+1(s(cid:48)))(cid:1)(cid:9)(cid:3) ,

π
Q

(3)

π
T +1,β(s, a) = 0. In fact, (1) is the only utility function that has this equivalence
starting with
Q
and other key properties (Lemma 1) while also satisfying time consistency that ensures the learned
risk-aware behaviors remain consistent across time [21]. In this paper, we use (3) to establish a
general GPI framework for risk-aware transfer learning with provable guarantees, and leverage
approximations of (2) to learn portable policy representations.

In reinforcement learning, the Bellman equation is not applied directly since it suffers from the curse
of dimensionality when
is high-dimensional or continuous, and since neither the dynamics nor the
reward function are often known. Instead, the agent interacts with the environment using a stochastic
T
exploration policy πe, collects trajectories
Qh,β(st, at) via
t=0 , and updates
−
}
sample approximations ˆUβ ≈
Uβ [34]. Our goal is to ameliorate the relative sample-inefﬁciency of
RL through transfer learning that we aim to generalize to the risk-aware setting.

(st, at, st+1, rt+1)
{

S

1

3

2.3 Transfer Learning

M

We now formalize the general transfer learning problem. Let
be the set of all MDPs with
shared transition function P but different (bounded) reward functions. A ﬁxed set of source tasks
is instantiated, and their corresponding optimal policies π1, . . . πn are estimated.
M1, . . . Mn ∈ M
Our main goal is to transfer these resulting source policies to a new target task Mn+1 ∈ M
, to obtain
a policy π∗n+1 whose utility is better than one learned from scratch using only a ﬁxed number of
samples from Mn+1. We refer to this outcome as positive transfer.
As discussed earlier, a standard way to implement transfer learning is the GPI/GPE framework of
Barreto et al. [2]. The core mechanism that enables positive transfer in the risk-neutral setting is called
generalized policy improvement (GPI). Speciﬁcally, the set of source policies π1, . . . πn are evaluated
on the target task Mn+1 to obtain corresponding values Qπ1
n+1. Given a mechanism that
can perform this policy evaluation step efﬁciently with some small error ε — namely successor
features discussed and extended in Section 3.3 — an agent then selects actions in a greedy manner by
n+1(s, a) in state s. The policy π corresponds to a
following policy π(s)
strict policy improvement operator, and thus fulﬁlls our requirements for positive transfer.

arg maxa maxj=1...n Qπj

n+1, . . . Qπn

∈

3 Risk-Aware Transfer Learning

An obvious challenge of applying GPI in the risk-aware setting is that transferring optimal risk-
neutral source policies does not guarantee risk-aware optimality in the target task. A much stronger
observation is that, even if the source policies πj are risk-aware, performing the policy evaluation
step in a risk-neutral way can still break the risk-awareness of GPI. This makes the extension of GPI
to the risk-aware setting a non-trivial problem.

3.1 A Motivating Example

To see this, consider the MDP shown in Figure 1, which
involves navigating from an initial state ‘S’ to a goal state
‘G’ in a grid with stochastic transitions. Traps of two types
(X, Y) are placed in ﬁxed cells, which upon entry terminate
the episode with ﬁxed costs, summarized compactly as pairs
(c1, c2). We deﬁne two source tasks with costs (20, 20)
and (0, 0) and a target task with costs (20, 0). The optimal
policies for β =
0.1 induce three different trajectories:
safe (blue) and hazardous routes (red) for the source tasks,
and a relatively safe route (green) for the target task.

−

−

We compute the GPI policies π with risk-averse (β =
0.1)
and risk-neutral (β = 0) policy evaluation, as shown in the
top row on the two rightmost plots in Figure 1. The bottom
plot shows the distribution of returns collected over 5,000
test runs by acting according to the two GPI policies. Risk-
averse policy evaluation results in an optimal risk-averse GPI
policy corresponding to the green trajectory, whereas risk-
neutral policy evaluation does not, even though both source
policies are optimal on their corresponding tasks. Interest-
ingly, the risk-averse GPI policy is a non-trivial stitching of
the two source policies.

3.2 Risk-Aware Generalized Policy Improvement

Figure 1: Comparing risk-aware and
risk-neutral GPI. The risk-aware (for
0.1) GPI policy is shown in the
β =
top-middle plot, while the risk-neutral
(for β = 0) GPI policy is shown in the
top-right plot.

−

Motivated by this example, we conjecture that the risk-awareness of the policy π is primarily
dependent on the way in which the source policies are evaluated in target task instances. In this
section, we describe theoretical results that generalize the concept of generalized policy iteration to
the problem of maximizing the entropic utility of returns.

We begin by summarizing key properties necessary for establishing convergence of risk-aware GPI in
the following lemma.

4

SGXXYGXXYGXXY−30−20−10010episodereturn0.00.10.20.3densityrisk-awareGPIrisk-neutralGPILemma 1. Let β

R and X, Y be arbitrary random variables on Ω. Then:

∈
(1) (monotonicity) if P(X
Uβ[Y ]
≥
(2) (cash invariance) Uβ[X + c] = Uβ[X] + c for every c
(3) (convexity) if β < 0 (β > 0) then Uβ is a concave (convex) function
(4) (non-expansion) for f, g : Ω

Y ) = 1 then Uβ[X]

Ω, it follows that

≥

R

∈

→

Uβ[f (X)]
|

−

Uβ[g(X)]

| ≤

sup
PX (Ω)

EP |

f (X)

g(X)
|

,

−

P
∈
where PX (Ω) is the set of all probability distributions on Ω that are absolutely continuous
w.r.t. the true distribution of X.

A proof is provided in Appendix B.2. Properties (1)-(3) characterize concave utilities [13], which
intuitively can be seen as minimal requirements for rational decision-making: (1) a lottery that pays
off more than another in every possible state of the world should always be preferred; (2) adding cash
to a position should not increase its risk; and (3) the overall utility can be improved by diversifying
across different risks. Property (4) is a derivative of the ﬁrst three, and thus the theoretical results in
this work could be extended further to the broad class of iterated concave utilities [32].

We can now state the ﬁrst main result of the paper.
Theorem 1 (GPI for Entropic Utility). Let π1, . . . πn be arbitrary deterministic Markov policies
with utilities ˜
ε
Q
, a
for all s

πn
h,β evaluated in an arbitrary task M , such that
. Deﬁne

, i = 1 . . . n and h

πi
h,β(s, a)

πi
h,β(s, a)

−Q

˜
Q

| ≤

|

π1

h,β, . . . ˜
Q
∈ A

∈ S

max
i=1...n

πi
h,β(s, a),

˜
Q

s
∀

.

∈ S

(4)

πh(s)

∈

∈ T
arg max

a

∈A

Then,

π
h,β(s, a)
Q

≥

max

i Q

πi
h,β(s, a)

2(T

−

−

h + 1)ε,

h

T.

≤

Thus, evaluating the risk of source policies using Uβ provides monotone improvement guarantees for
GPI, and thus satisﬁes our deﬁnition of positive transfer. Another signiﬁcant property of the bound
in Theorem 1, and the one in Theorem 2 below, is the linear separation between the optimal utility
and the approximation error ε. Knowing how the optimality of π explicitly depends on ε and how
errors are propagated throughout the transfer learning process is critical for developing reliable RL
with predictable behavior, and highlights a key advantage of making GPI risk-aware. The additive
dependence of the optimality on the approximation error in Theorem 1 further explains why SFs
are robust to approximation errors. This becomes particularly advantageous in our setting, since
estimating utilities Uβ accurately with GPE is more complicated than the risk-neutral setting.
A stronger result for GPI can be derived when the source policies π1, π2 . . . πn are ε-optimal, and
policy evaluation is once again performed using Uβ. In this case, the optimality of GPI is determined
by the similarity δr between the source and target task instances.

Q

Theorem 2. Let
M with reward function r(s, a, s(cid:48)). Furthermore, let ˜
Q
for all s
, a
∈ T
let δr = mini=1...n sups,a,s(cid:48)

π∗
h,β be the utilities of optimal Markov policies π∗i from task Mi but evaluated in task
i
< ε
and i = 1 . . . n, and let π be the corresponding policy in (4). Finally,
r(s, a, s(cid:48))
|
π
h,β(s, a)
Q

. Then,
ri(s, a, s(cid:48))
|

π∗
h,β be such that
i

π∗
h,β(s, a)
i
|

−
∗h,β(s, a)

h + 1)(δr + ε),

π∗
h,β(s, a)
i

− Q

∈ A

−Q

˜
Q

∈ S

2(T

, h

T.

−

≤

≤

h

(cid:12)
(cid:12)

(cid:12)
(cid:12)

|

These results are proved in Appendix B.2 for the episodic setting and in Appendix B.3 for the
discounted setting. Finally, while not required in this work, the above results could be extended to
more general settings in risk-averse control [32], though practical implementation of GPE in these
settings remains an open problem.

3.3 Risk-Aware Generalized Policy Evaluation

Following Barreto et al. [2], let φ :
map, and consider the following linear representation of rewards,

S × A × S

→

(cid:48)

Rd be a bounded and task-independent feature

(cid:124)
r(s, a, s(cid:48)) = φ(s, a, s(cid:48))

w,

s, a, s(cid:48),

∀

5

Rd is a task-dependent reward parameter. The risk-neutral return becomes:

(cid:124)
φ(st, at, st+1)

w

(cid:12)
(cid:12)
(cid:12) sh = s, ah = a, at ∼

(cid:35)

πt(st)

where w

∈

Qπ

h(s, a) = EP

(cid:34) T

(cid:88)

t=h

(cid:34) T

VarP [Ψπ

β
2
(cid:124)
VarP [Ψπ

(cid:35)(cid:124)

t=h

(cid:88)

= EP

φ(st, at, st+1)

(cid:12)
(cid:12)
(cid:12) sh = s, ah = a, at ∼
where ψπ
h (s, a) are the successor features (SFs) associated with the policy π. The linear dependence
of the return on w allows for instantaneous policy evaluation in novel tasks with arbitrary reward
preferences w, making it a particular — and perhaps the canonical — instantiation of GPE. More
critically, ψπ
h can be seen as a task-independent and highly portable linear feature representation of
policies, and is the key to the generalization ability of SFs on novel task instances.

(cid:124)
h (s, a)

w = ψπ

πt(st)

(5)

w,

The concept of GPE can be generalized to incorporate distributions of the return. Repeating the above
derivation for the entropic utility (2), we have:

π
h,β(s, a) = Uβ
Q

(cid:34) T

(cid:88)

(cid:35)

r(st, πt(st), st+1)

= Uβ [Ψπ

h(s, a)

(cid:124)

w] ,

(6)

t=h
corresponding to the random vector Ψπ
t=h φt of unrealized feature returns. Thus, we
have transformed the problem of estimating the utility of returns into the problem of estimating the
distribution of Ψπ
w. The key question now is how to estimate this distribution for fast GPE.

h(s, a) = (cid:80)T

(cid:124)

h(s, a)

A natural way to do this is by applying a second-order Taylor expansion for Uβ, since it allows us to
precompute and cache the necessary moments of the return distribution:
(cid:124)

Uβ [Ψπ

(cid:124)
h(s, a)

w] = EP [Ψπ

(cid:124)
h(s, a)

w] +

h(s, a)

w] + O(β2)

(cid:124)

ψπ

h (s, a)

w +

w

β
2

h(s, a)]w = ˜
π
h,β(s, a),
Q

(7)

h and Σπ

h and Σπ

≈
h(s, a)] = Σπ
in which VarP [Ψπ
h(s, a) is interpreted as a covariance matrix for SFs. In the context
of Theorems 1 and 2, the term ε encapsulates the errors in the approximation of ψπ
h, plus
the terms contained in O(β2) above. However, the main advantage of (7) is that, like (5), it is also
analytic in w and allows for instantaneous policy evaluation with arbitrary reward preferences w.
Now, ψπ
h together provide task-independent and portable representations of policies, while
also accounting for exogenous risk. This is the key to preserving the task generalization ability of
SFs in the risk-aware setting, and (7) can now be seen as a particular instantiation of GPE. We call
this overall approach Risk-aware Successor Features (RaSF).
The simplest approaches for estimating Σπ
h in the exact (e.g. tabular) Q-learning setting are based on
dynamic programming [35, 38], which would allow the overall approach to be easily integrated into
existing SF implementations. In particular, the covariance satisﬁes the Bellman equation
sh = s, ah = a(cid:3) ,
(cid:124)
(8)
h is known to converge to the

h (s, a). The approximation ˜ψπ

h+1(s(cid:48), πh+1(s(cid:48)))

+ Σπ

Σπ

s,a)

P (

|

h , and a similar result also holds for updating the covariance based on (8).

·|

∼

(cid:2)δhδh
h(s, a) = E
s(cid:48)
where δh are the Bellman residuals of ψπ
true value ψπ
Theorem 3 (Convergence of Covariance). Let
pose there exists ε :
s,a)[˜δh( ˜ψπ
E
(cid:107)
·|
(cid:13)
(cid:13)
(cid:13)Σπ

h (s(cid:48), πh+1(s(cid:48)))
E

(cid:107) · (cid:107)
) such that
[0,
∞
(cid:124)
ψπ
h (s(cid:48), πh+1(s(cid:48))))
(cid:104)˜δh ˜δ
h + ˜Σπ

S × A × T →

h(s, a)

s,a)

−

P (

P (

s(cid:48)

s(cid:48)

∼

(cid:124)

−

h (s, a)

be a matrix-compatible norm, and sup-
˜ψπ
εh(s, a) and
(cid:107)
]
(cid:107) ≤
h+1(s(cid:48), πh+1(s(cid:48)))

2
h (s, a)
(cid:107)

3εh(s, a).

ψπ

≤

∼

h requires accurate estimates of ψπ

·|
A proof can be found in Appendix B.4. Appendix A.1 describes how ˜ψπ
h can be learned online
from environment interactions, while Appendix A.4 discusses further generalizations of (7). There
are, however, several limitations of estimating Σπ
h in this way. First, obtaining accurate estimates of
Σπ
h (thus estimating one quantity on top of another), making this
approach difﬁcult to apply with deep function approximation. This claim is further substantiated by
Theorem 3 and preliminary experiments. A second issue that occurs is double sampling, when the
same transitions are used to update the mean and covariance, resulting in accumulation of bias in the
latter [1, 45]. Our experiments on the reacher domain avoid these issues by leveraging distributional
RL to approximate the mean and variance in (7), while maintaining computational efﬁciency.

−
εh(s, a). Then,
(cid:105)(cid:13)
(cid:13)
(cid:13) ≤
h and ˜Σπ

6

4 Experiments

To evaluate the performance of RaSF, we revisit the benchmark domains in Barreto et al. [2], adapted
for learning and evaluating risk-aware behaviors. Further details can be found in Appendix C.

Four-Room. The ﬁrst domain consists of a set of naviga-
tion tasks deﬁned on a discrete 2-D space divided into four
rooms, as illustrated on the left in Figure 2. The environment
has additional objects that can be picked up by the agent
by occupying their cells. These objects belong to one of
three possible classes, drawn as different shapes in Figure
2, which determine their reward. The position of the objects
remains ﬁxed, but the rewards of their classes are reset every
20,000 transitions to random values sampled uniformly in
1, +1]. To incorporate risk, traps are placed in ﬁxed cells
[
−
marked with X. For every time instant during which the
agent occupies a trap cell, the trap activates spontaneously
with a small probability, resulting in an immediate penalty
and termination of the episode (we refer to this event as a
failure). The goal is to maximize the total reward accumu-
lated over 128 random task instances while minimizing the
number of failures.

Figure 2: The four-room and reacher
domains. Four-Room: the shapes of
the objects represent their classes, ‘S’
is the start state and ‘G’ is the ﬁnal goal
state, and (X) mark the states with high
risk. Reacher: colored and gray cir-
cles represent training and test targets,
respectively, while shaded regions rep-
resent areas of high risk.

In order to demonstrate the power of our approach in the
absence of approximation errors, we deﬁne a simple instance
of RaSFs in which ψπi and Σπi are learned exactly using lookup tables and dynamic programming
(equation (8)). We also apply modest discounting of rewards to ensure that the Q-function converges,
as is standard in RL and discussed further in Appendix A.2. The w are also learned using immediate
reward feedback and exact, sparse state features φ provided to the agent. Due to its similarity to
standard Q-learning, we call this approach RaSFQL. To provide a challenging baseline for comparison,
we implemented another policy reuse algorithm (PRQL) [12]. Further replacing the risk-neutral
action selection mechanism of PRQL with smart exploration [16] allows PRQL to be sensitive to
reward volatility, which we refer to as RaPRQL.

The performance of these algorithms is shown in Figure 3. The cumulative reward obtained by
RaSFQL is generally lower than SFQL, as expected since a risk-averse agent should avoid the
objects in the bottom-left and top-right rooms and forgo their associated rewards. Interestingly, the

Figure 3: Average return, cumulative return and number of failures per task in the four-room domain,
2. Shaded bands show one standard error over 30 independent runs.
for β = ω =

−

7

XGXXXXXXXXXXSX0123456−2500250500AverageReturnRaSFQLSFQLRaPRQLPRQL24252627282930122123124125126127128−2500250500012345605001000ReturnperTask242526272829301221231241251261271280500100001234560204060FailuresperTask24252627282930TaskInstance1221231241251261271280204060Figure 4: Performance on training and test tasks for the reacher domain. Left: Normalized return on
training tasks. Faded curves correspond to C51 performance. Middle: Normalized return averaged
across all test tasks. Right: Total failures across all test tasks. Shaded bands show one standard error
over 10 independent runs with different seeds.

performance of RaSFQL far exceeds that of RaPRQL and even PRQL, suggesting that the beneﬁts
of task generalization provided by GPI/GPE are quite strong. Furthermore, the number of failures
observed by RaSFQL gradually decreases over the task instances, while the number of failures of
SFQL slightly increases. This is consistent with Theorem 1 that guarantees monotone improvement
in the risk-adjusted return of RaSFQL. On the other hand, while RaPRQL also learns to avoid risk, it
fails slightly more often than RaSFQL. This suggests that the beneﬁts of task generalization promised
by GPI/GPE even allow risk-aware behaviors to emerge sooner than by using generic policy reuse
methods that are unable to exploit the task structure, namely PRQL. This aspect becomes critical for
minimizing failures when deploying a trained policy library on novel task instances in a real-world
setting. Further analysis and ablation studies are provided in Appendix D.1.

Reacher. The second domain consists of a set of tasks based on the MuJoCo physics engine [40]
that involve the maneuver of a robotic arm toward a ﬁxed target location. As illustrated in the
rightmost plot in Figure 2, the agent is only allowed to train on 4 tasks, whose target locations are
indicated by colored circles, and must be able to perform well on 8 test tasks whose target locations
are indicated by the grey circles. Furthermore, we incorporate two sources of reward volatility: (1)
actions are perturbed by additive Gaussian noise; and (2) ﬁxed regions around some of the target
locations randomly incur negative rewards (failures), illustrated by faded circles in Figure 2. Please
note that most of these high-volatility regions are centered on target locations of test tasks from which
the agent never learns directly. This stresses the agent’s ability to avoid unforeseen dangers in the
environment, in additional to performing well on previously unseen task instances.
As discussed earlier, it is difﬁcult to compute Σπi using
(8) in the deep RL setting. A computationally tractable
way to avoid these issues is to ﬁrst approximate the den-
sity of Ψπi(s, a) using the distributional RL framework,
and then extract the moment information needed for the
calculation of (8). Speciﬁcally, we apply C51 [6] by model-
ing Ψπi
d (s, a) using histograms for each (s, a).
However, Ψπi are high-dimensional, so we avoid the curse
of dimensionality by modeling Ψπi
j without their interaction
effects. This still turns out to be an effective way of detect-
ing high-variance scenarios in the environment. The ﬁnal
architecture is illustrated in Figure 5, where the marginal distributions of Ψπi are modeled as separate
output heads with a shared state encoder. The rest of the training protocol is identical to the SFDQN
of Barreto et al. [2], except that DQN is replaced by the C51 architecture above, as further detailed in
Appendix A.3. The risk-averse and risk-neutral instances of this approach for modeling successor
features are referred to as RaSFC51 and SFC51, respectively, while RaC51 and C51 replace successor
features with universal value functions [33] for generalization across target locations.

Figure 5: Architecture for Ψπ(s, a).

1 (s, a), . . . Ψπi

The performance of these algorithms on the reacher domain is illustrated in Figure 4. We ﬁrst observe
that the performance of all four training policies for RaSFC51 improves almost immediately, obtaining
returns that exceed 75% of the performance of a fully trained C51 agent, an observation that correlates
highly with the original SFDQN. Interestingly, RaC51 is unable to achieve satisfactory performance

8

01234TrainingTaskInstance−0.50−0.250.000.250.500.751.00NormalizedReturnTask1Task2Task3Task401234TrainingTaskInstance−0.20.00.20.40.60.81.0NormalizedReturnRaSFC51SFC51RaC51C5101234TrainingTaskInstance020406080100120FailuresRaSFC51SFC51RaC51C51sΨπa1a2a|A|…Ψπ1(s,·)…Ψπ2(s,·)...…Ψπd(s,·)•wreturnFigure 6: Left: Normalized average test return for the reacher domain, for different values of
β (legend values indicate the negative value of β). Middle: Normalized average test return for
the reacher domain, showing the improvement obtained by replacing DQN by C51 as a function
approximator for SFs. Right: Evolutions of the arm tip position in three rollouts of the reacher
domain according to the GPI policy obtained after training on all 4 tasks (yellow pentagons indicate
the initial states in each rollout). Only one training task and two test tasks are shown. The risk-averse
agent learns to hover close to the goal while avoiding the high-volatility shaded regions.

on two out of the four training tasks, even after fully trained on all 4 tasks. Furthermore, the
performance of RaSFC51 on the testing tasks far exceeds that of both RaC51 and C51, demonstrating
the superior generalization ability of SFs in the risk-aware setting. Finally, the total number of
failures across the test task instances is also considerably lower for RaSFC51 than it is for SFC51,
and remains low during the entire training horizon. RaC51 and C51 fail frequently at the beginning
of training, but less often later in training. While this may suggest that C51 is learning adequate
risk-sensitive policies, it is mainly due to the fact that C51 is not able to generalize nearly as well as
SFC51 for some of the test target locations, as elaborated in Appendix D.2.

We also conducted several ablation studies, summarized in Figure 6. The performance of RaSFC51
and RaC51 decays gracefully as β is increased in magnitude, but the performance of RaSFC51
uniformly outperforms RaC51 for every value of β tested. We also compared the beneﬁt of replacing
DQN by C51 as the architecture for learning SFs, and found that this simple modiﬁcation can
signiﬁcantly improve SFs. It is likely that learning the full distribution of SF returns provides
additional stability of the Bellman backups in stochastic domains, and thus allows SFs to inherit the
advantages of distributional RL previously reported by Bellemare et al. [6]. The ﬁnal plot on the right
shows that RaSFC51 learns safer policies than SFC51 on both training and test instances. Speciﬁcally,
RaSFC51 learns to hover as close to the goal as possible in most cases, while still avoiding the
high-volatility shaded regions. Further analysis of this domain can be found in Appendix D.2.

5 Conclusion

We presented Risk-aware Successor Features (RaSFs) for realizing policy transfer between tasks
with shared dynamics and different goals, where the overall objective is to optimize the trade-off
between expected return and risk, measured by the variance of return. We extended generalized
policy improvement to the risk-aware setting, providing monotone guarantees and optimality of GPI,
provided that source policies can be evaluated in a risk-aware manner. To facilitate policy evaluation,
we extended the notion of generalized policy evaluation to the mean-variance objective. Together,
risk-aware GPI and GPE inherit the superior task generalization abilities of successor features, while
also learning to avoid dangerous and unpredictable situations in the environment, as analysis on
discrete navigation and robotic control domains has showed.

Our implementation relied heavily on modern deep reinforcement learning, thus requiring discounting,
limitation to stationary policies, and the independence assumption in the SFC51 architecture for
tractability. One way to mitigate some of these concerns is to pursue a model-based direction for
better sample-efﬁciency and better policy search through planning. More generally, incorporating risk
and safety in sequential decision-making is a complex problem. The entropic utility objective studied
in this work does not capture other well-known utility functions such as CVaR, which would be
difﬁcult to incorporate into GPE due to their reliance on percentiles and lack of asymptotic expansions
that are exploited in this work. Incorporating other notions of risk in the GPI/GPE framework could
form interesting and challenging future extensions of our work.

9

01234TrainingTaskInstance−0.20.00.20.40.60.81.0NormalizedReturnSFC51(4)SFC51(3)SFC51(2)SFC51(1)SFC51(0)C51(4)C51(3)C51(2)C51(1)C51(0)01234TrainingTaskInstance0.00.20.40.60.81.0NormalizedReturnSFC51SFDQNtraintask1testtask1testtask2RaSFC51SFC51Acknowledgements

The authors would like to thank Daniel Mankowitz for suggesting relevant research in the area of
robust and risk-aware reinforcement learning and for providing insightful comments throughout the
development of the paper.

References

[1] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine

Learning Proceedings 1995, pages 30–37. Elsevier, 1995.

[2] André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, David Silver, and Hado P van

Hasselt. Successor features for transfer in reinforcement learning. In NeurIPS, 2017.

[3] Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz,
Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and
generalised policy improvement. In ICML, pages 501–510, 2018.

[4] André Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement learning

with generalized policy updates. PNAS, 117(48):30079–30087, 2020.

[5] Nicole Bäuerle and Ulrich Rieder. More risk-sensitive markov decision processes. Mathematics of

Operations Research, 39(1):105–120, 2014.

[6] Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning.

In ICML, pages 449–458, 2017.

[7] L Bisi, L Sabbioni, E Vittori, M Papini, and M Restelli. Risk-averse trust region optimization for

reward-volatility reduction. In IJCAI, pages 4583–4589, 2020.

[8] Diana Borsa, Andre Barreto, John Quan, Daniel J Mankowitz, Hado van Hasselt, Remi Munos, David

Silver, and Tom Schaul. Universal successor features approximators. In ICLR, 2018.

[9] Oscar Dowson, David P Morton, and Bernardo K Pagnoncelli. Multistage stochastic programs with the

entropic risk measure, 2020.

[10] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement

learning. ICML Workshop RL4RealLife, 2019.

[11] Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie. Risk-sensitive reinforcement

learning: Near-optimal risk-sample tradeoff in regret. NeurIPS, 33, 2020.

[12] Fernando Fernández and Manuela Veloso. Probabilistic policy reuse in a reinforcement learning agent. In

AAMAS, pages 720–727, 2006.

[13] Hans Föllmer and Alexander Schied. Convex measures of risk and trading constraints. Finance and

stochastics, 6(4):429–447, 2002.

[14] Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning. Journal

of Machine Learning Research, 16(1):1437–1480, 2015.

[15] Javier García and Fernando Fernández. Probabilistic policy reuse for safe reinforcement learning. ACM

Trans. Auton. Adapt. Syst., 13(3), March 2019. ISSN 1556-4665. doi: 10.1145/3310090.

[16] Clement Gehring and Doina Precup. Smart exploration in reinforcement learning using absolute temporal

difference errors. In AAMAS, pages 1037–1044, 2013.

[17] David Held, Zoe McCarthy, Michael Zhang, Fred Shentu, and Pieter Abbeel. Probabilistically safe policy

transfer. In ICRA, pages 5798–5805. IEEE, 2017.

[18] Arushi Jain, Khimya Khetarpal, and Doina Precup. Safe option-critic: learning safety in the option-critic

architecture. The Knowledge Engineering Review, 36, 2021.

[19] Arushi Jain, Gandharv Patil, Ayush Jain, Khimya Khetarpal, and Doina Precup. Variance penalized

on-policy and off-policy actor-critic. arXiv preprint arXiv:2102.01985, 2021.

[20] David Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato, and Sebastian
Tschiatschek. Successor uncertainties: exploration and uncertainty in temporal difference learning. In
NeurIPS, 2019.

10

[21] Michael Kupper and Walter Schachermayer. Representation results for law invariant time consistent

functions. Mathematics and Financial Economics, 2(3):189–210, 2009.

[22] Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement

Learning, pages 143–173. Springer, 2012.

[23] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,

review. and Perspectives on Open Problems, 2020.

[24] Daniel Mankowitz, Timothy Mann, Pierre-Luc Bacon, Doina Precup, and Shie Mannor. Learning robust

options. In AAAI, volume 32, 2018.

[25] Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. Situational awareness by risk-conscious skills. arXiv

preprint arXiv:1610.02847, 2016.

[26] Shie Mannor and John N Tsitsiklis. Algorithmic aspects of mean–variance optimization in markov decision

processes. European Journal of Operational Research, 231(3):645–653, 2013.

[27] Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh. Variance

reduction for reinforcement learning in input-driven environments. In ICLR, 2019.

[28] Teodor Mihai Moldovan and Pieter Abbeel. Risk aversion in markov decision processes via near optimal

chernoff bounds. In NeurIPS, pages 3140–3148, 2012.

[29] David Nass, Boris Belousov, and Jan Peters. Entropic risk measure in policy search. In 2019 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pages 1101–1106. IEEE, 2019.

[30] Takayuki Osogami. Robustness and risk-sensitivity in markov decision processes. NeurIPS, 25:233–241,

2012.

[31] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &

Sons, 2014.

[32] Andrzej Ruszczy´nski. Risk-averse dynamic programming for markov decision processes. Mathematical

programming, 125(2):235–261, 2010.

[33] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In

ICML, pages 1312–1320. PMLR, 2015.

[34] Yun Shen, Michael J Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement learning.

Neural computation, 26(7):1298–1328, 2014.

[35] Craig Sherstan, Dylan R Ashley, Brendan Bennett, Kenny Young, Adam White, Martha White, and
Richard S Sutton. Comparing direct and indirect temporal-difference methods for estimating the variance
of the return. In UAI, 2018.

[36] Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn. Learning to be safe:

Deep rl with a safety critic. arXiv preprint arXiv:2010.14603, 2020.

[37] Aviv Tamar, Dotan Di Castro, and Shie Mannor. Policy gradients with variance related risk criteria. In

ICML, pages 1651–1658, 2012.

[38] Aviv Tamar, Dotan Di Castro, and Shie Mannor. Learning the variance of the reward-to-go. The Journal of

Machine Learning Research, 17(1):361–396, 2016.

[39] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.

Journal of Machine Learning Research, 10(7), 2009.

[40] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE,
2012.

[41] Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe reinforcement

learning via curriculum induction. NeurIPS, 33, 2020.

[42] Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil.

Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.

[43] S Whiteson, S Zhang, and B Liu. Mean- variance policy iteration for risk- averse reinforcement learning.

In AAAI. Association for the Advancement of Artiﬁcial Intelligence, 2021.

11

[44] Yang Yu. Towards sample efﬁcient reinforcement learning. In IJCAI, pages 5739–5743, 2018.

[45] Yuhua Zhu and Lexing Ying. Borrowing from the future: An attempt to address double sampling. In

Mathematical and scientiﬁc machine learning, pages 246–268. PMLR, 2020.

12

Risk-Aware Transfer in Reinforcement Learning
using Successor Features

Supplementary Material

Michael Gimelfarb, André Barreto, Scott Sanner and Chi-Guhn Lee

Abstract

This part of the paper discusses algorithmic details for the total reward episodic
setting and the discounted setting that were not included in the main paper due to
space limitations. It includes proofs of all main theoretical claims in the paper, as
well as additional experimental details and parameter settings used to reproduce
the experiments.

Contents

A Mathematical and Algorithmic Details

A.1 Mean-Variance Approximation for Episodic MDPs . . . . . . . .

A.2 Mean-Variance Approximation for Discounted MDPs . . . . . . .

A.3 Histogram Representations for Successor Features . . . . . . . . .

A.4 Possible Generalizations of the Mean-Variance Approximation . .

B Proofs of Theoretical Results

B.1 Proof of Lemma 1 . . .

. . . . . . . . . . . . . . . . . . . . . . .

B.2 Proofs of Theorem 1 and 2 for Episodic MDPs

. . . . . . . . . .

B.3 Proofs of Theorem 1 and 2 for Discounted MDPs . . . . . . . . .

B.4 Proof of Theorem 3 .

. . . . . . . . . . . . . . . . . . . . . . . .

C Experiment Details

C.1 Motivating Example

. . . . . . . . . . . . . . . . . . . . . . . .

C.2 Four-Room .

C.3 Reacher

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . .

. . . .

. . . . . . . . . . . . . . . . . . . . . . .

C.4 Additional Details for Reproducibility . . . . . . . . . . . . . . .

D Additional Ablation Studies and Plots

D.1 Four-Room .

D.2 Reacher

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . .

. . . .

. . . . . . . . . . . . . . . . . . . . . . .

14

14

14

17

19

21

21

22

24

26

28

28

28

30

32

33

33

34

13

A Mathematical and Algorithmic Details

In this section, we outline the ways in which successor features and their covariance matrices can be
learned in practical RL settings. Speciﬁcally, we introduce Bellman updates and the distributional RL
framework, and also discuss the mean-variance approximation under more general assumptions in
parameteric density estimation.

A.1 Mean-Variance Approximation for Episodic MDPs

Directly Computing Successor Features and Covariances.
In the tabular setting, we compute
estimates of the mean ˜ψπ
h(s, a) by extending the analysis in Sherstan et al.
[35] to the d-dimensional setting. For a transition (s, a, φ, s(cid:48), a(cid:48)), where φ = φ(s, a, s(cid:48)), and learning
rate α > 0, the update for successor features is:

h (s, a) and covariance ˜Σπ

˜δh = φ + ˜ψπ

h+1(s(cid:48), a(cid:48))
h (s, a) + α˜δh.
By virtue of the covariance Bellman equation (8), a per-sample update of the covariance matrix can
be computed by using the Bellman residuals ˜δh as a pseudo-reward:

h (s, a) = ˜ψπ

h (s, a),

˜ψπ

(9)

−

˜ψπ

(cid:124)
∆h = ˜δh ˜δ
h(s, a) = ˜Σπ

˜Σπ

h(s, a) + ¯α∆h.

h + ˜Σπ

h+1(s(cid:48), a(cid:48))

˜Σπ

h(s, a),

−

(10)

In practice, ¯α is usually set much smaller than α, e.g. ¯α = ρα for some positive ρ

1.

(cid:28)

Computing Reward Parameters. When the reward parameters w are unknown, they can be
learned by solving a regression problem. Speciﬁcally, for known or estimated features φ(s, a, s(cid:48)) and
an observed reward r, the objective function to minimize is the mean-squared error,

(w) =

L

(cid:0)r

1
2

−

(cid:124)
φ(s, a, s(cid:48))

w(cid:1)2

,

that can be minimized using stochastic gradient descent (SGD). Introducing a learning rate αw > 0,
an update of SGD on a single transition (s, a)

s(cid:48) is

w

←

w + αw(r

−

→
φ(s, a, s(cid:48))

(cid:124)

w)φ(s, a, s(cid:48)).

Pseudocode. The general routine for performing risk-aware transfer learning in an online setting,
which we call Risk-Aware Successor Feature Q-Learning (RaSFQL), can now be fully described.
Pseudocode adapted for the total-reward episodic MDP setting is given as Algorithm 1. Please note
that our approach closely follows the risk-neutral SFQL in Barreto et al. [2].

Both the discounted and total reward episodic settings are amenable to function approximation.
However, as discussed in the main text, this “residual" method is usually not advisable as the
approximation errors in the residuals ˜δh can dominate the environment uncertainty. While this could
be useful for handling epistemic or model uncertainty in successor features [20], the intent of our
work is to learn the aleatory or environment uncertainty. Therefore, a more precise method — based
on the projected Bellman equation — will be introduced in Appendix A.3.

A.2 Mean-Variance Approximation for Discounted MDPs

Bellman Principle and Augmented MDP. The utility objective in the discounted inﬁnite-horizon
setting becomes

π
β,γ(s, a) = Uβ
Q

(cid:35)

γtr(st, at, st+1)

,

(cid:34)

∞(cid:88)

t=0

where γ

(0, 1) is a discount factor for future rewards.

∈

In the discounted setting, it is necessary to accumulate and keep track of the discounting over time.
This can be implemented by augmenting the state space of the original MDP [5]. Speciﬁcally, deﬁne

14

Algorithm 1 RaSFQL with Mean-Variance Approximation
N, ε
1: Requires m, T, Ne ∈
2: for t = 1, 2 . . . m do

[0, 1], α, ¯α, αw > 0, β

∈

∈

R, φ

Rd, M1, . . . Mm ∈ M

∈

Initialize successor features and covariance for the current task

\\
if t = 1 then Initialize ˜ψt, ˜Σt to small random values else Initialize ˜ψt, ˜Σt to ˜ψt
−
Initialize ˜wt to small random values
Commence training on task Mt

1, ˜Σt

−

1

\\
for ne = 1, 2 . . . Ne do

Initialize task Mt with initial state s
for h = 0, 1 . . . T do

\\
c

←

Select the source task Mc using GPI
˜wt −
arg maxj maxb{
Sample action from the epsilon-greedy policy based on πc

˜Σj
h(s, b) ˜wt}

(cid:124)
h(s, b)

(cid:124)
β ˜w
t

˜ψj

Bernoulli(ε)

\\
random_a
∼
) else
if random_a then a
Uniform(
(cid:124)
A
∼
(cid:124)
˜ψc
˜Σc
h(s, b) ˜wt}
β ˜w
˜wt −
a
h(s, b)
arg maxb{
t
Take action a in Mt and observe r and s(cid:48)
Update reward parameters for the current task
(cid:124)
φ(s, a, s(cid:48))

˜wt)φ(s, a, s(cid:48))

˜wt + αw(r

←

\\
˜wt ←
\\
a(cid:48)
←
Update ˜ψt

\\
if c

−
arg maxb maxj{

Update the successor features and covariance for the current task
˜wt −

˜Σj
h(s(cid:48), b) ˜wt}

(cid:124)
h(s(cid:48), b)

β ˜w

˜ψj

(cid:124)
t

h, ˜Σt

h on (s, a, φ, s(cid:48), a(cid:48)) using (9) and (10)
Update the successor features and covariance for task Mc
= t then
a(cid:48)
←
Update ˜ψc

˜Σc
h(s(cid:48), b) ˜wc}
h on (s, a, φ, s(cid:48), a(cid:48)) using (9) and (10)

arg maxb{
h, ˜Σc

˜wc −

h(s(cid:48), b)

β ˜w(cid:124)
c

˜ψc

(cid:124)

3:
4:

5:
6:
7:

8:

9:
10:

11:

12:

13:
14:

15:
16:
17:
18:
19:
20:
21:
22: end for

end if
s
s(cid:48)
←
end for

end for

= [0, 1] and let z
Z
the augmented MDP

∈ Z
(cid:48),
(cid:104)S

and dynamics

denote the state of discounting. For a given MDP

, r(cid:48), P (cid:48)

, with state space
(cid:105)

S

(cid:48) =

A

S × Z

r(cid:48)((s, z), a, (s(cid:48), z(cid:48))) = zr(s, a, s(cid:48)),

,
(cid:104)S
, action space

, we deﬁne
, r, P
(cid:105)
, reward function

A
A

(s, z), a) = P (s(cid:48)
where δ is the Dirac delta function. Applying this augmentation transformation to a set of MDPs with
common transition function and common discount factor implies that the set of augmented MDPs
will also have the same transition functions.

s, a)δγz(z(cid:48)),
|

P (cid:48)((s(cid:48), z(cid:48))
|

Moreover, the following Bellman equation can be derived for the augmented MDP [5]:

J

π
β (s, a, z) = Uβ
1
β

=

(cid:2)zr(s, a, s(cid:48)) +

log E
s(cid:48)

P (

s,a)

π

β (s(cid:48), π(s(cid:48), γz), γz)(cid:3)
J
(cid:2)exp (cid:8)β (cid:0)zr(s, a, s(cid:48)) +

π

β (s(cid:48), π(s(cid:48), γz), γz)(cid:1)(cid:9)(cid:3) .

(11)

J
π
β (s, a, 1). Furthermore, the Bellman
Then, we can recover the original utility with
equation above converges to a unique ﬁxed point, and so the search for optimal policies can be
restricted to stationary Markov policies π :

π
β,γ(s, a) =
Q

J

∼

·|

.

S × Z → A

However, learning general policies π :
introduces additional difﬁculties in the function
approximation setting. In this case, successor features and their covariance matrices would have to
be functions of z. For a single transition, their corresponding updates would also require a sweep
over all possible values of z, e.g. z = 1, γ, γ2, . . . , and would be computationally demanding. On
alleviates this computational
the other hand, restricting the search to stationary policies π :

S × Z → A

S → A

15

(cid:54)
burden, making the overall time and space complexity per update comparable to the risk-neutral SF
representation, and also allows off-the-shelf RL algorithms to be used to learn successor features.
This also facilitates more precise estimation of risk using the distributional framework discussed in
the next section.

Fortunately, the restriction to z-independent source policies does not affect the validity of Theorem 1,
since policy improvement was shown for arbitrary admissible policies. This implies that monotone
policy improvement is guaranteed even for z-independent policies, provided that their utilities can be
estimated. In the case of Theorem 2, the approximation error ε generally arises from two sources of
additive error, namely that of restricting optimal policies π∗i to z-independent optimal policies ¯π∗i ,
and that of approximating utilities using function approximation, e.g.

ε =

=

≤
=

(cid:12)
(cid:12)
(cid:12)

− J

(cid:12)
¯π∗
˜
(cid:12)
β (s, a, 1)
i
(cid:12)
J
(cid:12)
¯π∗
˜
(cid:12)
β (s, a, 1)
i
(cid:12)
J
(cid:12)
¯π∗
˜
(cid:12)
β (s, a, 1)
i
(cid:12)
J
(cid:110)
approximation error of

π∗
β (s, a, 1)
i
¯π∗
β (s, a, 1) +
i
J
(cid:12)
(cid:12)
¯π∗
(cid:12)
(cid:12)
(cid:12) +
β (s, a, 1)
i
(cid:12)J
(cid:111)
¯π∗
+
i
β

− J

− J

J

¯π∗
β (s, a, 1)
i

π∗
β (s, a, 1)
i

(cid:12)
(cid:12)
(cid:12)

− J

¯π∗
β (s, a, 1)
i

π∗
β (s, a, 1)
i

(cid:12)
(cid:12)
(cid:12)
absolute difference between utilities of ¯π∗i and π∗i }

− J

{

.

The ﬁrst source of error arises solely due to the method of function approximation, and can be
reduced by using architectures whose training parameters and capacity are well-calibrated for each
problem. The second source of error is in general irreducible, but whether it can be tolerated should
be traded-off against the difﬁculty of learning z-dependent policies. In general, the learning of
z-dependent policies tractably is a challenging problem, which we leave for future investigation.

Incorporating Moment Information into GPE in Discounted MDPs. We now apply the idea of
generalized policy evaluation to discounted objectives. First, observe that, for ﬁxed π :

:

π
β (s, a, 1) = Uβ

J

γtr(st, π(st), st+1)

(cid:35)

(cid:34)

∞(cid:88)

t=0

S → A

(cid:124)
= Uβ [Ψπ(s, a)

w] ,

(12)

corresponding to the random vector Ψπ(s, a) = (cid:80)∞t=0 γtφt of unrealized feature returns. Thus, we
have again transformed the problem of estimating the utility of rewards into the problem of estimating
(cid:124)
the moments of the random variable Ψπ(s, a)

w.

Next, computing the Taylor expansion of Uβ:

π

β (s, a, 1) = EP [Ψπ(s, a)
J

(cid:124)

w] +

(cid:124)
ψπ(s, a)

w +

≈

β
2

w

(cid:124)
VarP [Ψπ(s, a)

β
2
(cid:124)
VarP [Ψπ(s, a)]w = ˜
J

w] + O(β2)

π
β (s, a, 1).

(13)

From a practical point of view, the mean-variance approximation in the discounted setting is identical
to the episodic total-reward setting, with the exception that the successor features and covariance are
discounted (and also time-independent). As in the undiscounted case, (13) induces an error of O(β2),
but is now another instantiation of GPE. However, restricting the search to z-independent policies
introduces additional approximation error that can also be absorbed into ε, as discussed previously.
Crucially, the theoretical results proved for the discounted setting (Appendix B.3) will now also hold
for z-independent stationary policies.

Bellman Updates for Covariance in Discounted MDPs. The covariance matrix satisﬁes the
covariance Bellman equation

Σπ

h(s, a) = E
s(cid:48)

(cid:2)δhδh

(cid:124)

s,a)

+ γ2Σπ

h+1(s(cid:48), πh+1(s(cid:48)))

sh = s, ah = a(cid:3) ,

|

P (

∼

·|

Similar to (9) and (10), in the discounted setting the successor features can be computed as [2]:

˜δh = φ + γ ˜ψπ

h+1(s(cid:48), a(cid:48))

˜ψπ

h (s, a) = ˜ψπ

h (s, a) + α˜δh.

˜ψπ

h (s, a),

−

16

(14)

(15)

Once again, the covariance matrix can be updated per sample following (14):

(cid:124)
∆h = ˜δh ˜δ
h(s, a) = ˜Σπ

h + γ2 ˜Σπ
h(s, a) + ¯α∆h.

˜Σπ

h+1(s(cid:48), a(cid:48))

˜Σπ

h(s, a),

−

(16)

In the context of Algorithm 1, all calls to (9) and (10) would be replaced with (15) and (16),
respectively.

The convergence of the covariance matrix in the discounted setting (14) is established in the following
result that can be easily proved using the techniques in Appendix B.4 for the episodic setting.
Theorem 4 (Convergence of Covariance). Let
pose there exists ε :
s,a)[γ ˜δh( ˜ψπ
E
(cid:107)
·|
(cid:13)
(cid:13)
(cid:13)Σπ

be a matrix-compatible norm, and sup-
(cid:107) · (cid:107)
˜ψπ
) such that
εh(s, a) and
[0,
S × A × T →
(cid:107)
(cid:124)
ψπ
h (s(cid:48), πh+1(s(cid:48))))
]
(cid:107) ≤
−
(cid:104)˜δh ˜δ
h + γ2 ˜Σπ
h+1(s(cid:48), πh+1(s(cid:48)))

ψπ
2
h (s, a)
(cid:107)
εh(s, a). Then,
(cid:105)(cid:13)
(cid:13)
(cid:13) ≤

h (s(cid:48), πh+1(s(cid:48)))
E

3εh(s, a).

h (s, a)

h(s, a)

∞

s,a)

−

≤

−

P (

P (

s(cid:48)

s(cid:48)

∼

∼

·|

(cid:124)

Please note that this result is identical to Theorem 3, with the exception of the discount factor.

A.3 Histogram Representations for Successor Features

The theoretical framework for distributional RL is discussed in details in the relevant literature [6]. In
this appendix, we discuss how this framework can be applied to learn distributions over successor
features, and how to use these distributions to select actions in a risk-aware manner.

Learning Distributions over Successor Features. As discussed in the main text, the goal is to
estimate the distribution of each component in the discounted inﬁnite horizon setting

Ψπ

i (s, a) =

∞(cid:88)

t=0

γtφi(st, at, st+1),

starting from s0 = s, a0 = a, where at = π(st) is selected according to a policy π. Treating
Ψπ

d as value functions, we are now able to apply distributional RL.

1 , . . . Ψπ

Speciﬁcally, suppose that each state feature component is bounded in a compact interval, e.g.
i (s, a) by bounding
φi(s, a, s(cid:48))
the terms of its geometric series representation above:

]. Then, we may deﬁne corresponding bounds on Ψπ

, φmax
i

[φmin
i

∈

Ψmin

i =

φmin
i
1

φmax
i
1
γ
−
h(s, a) by using a discrete distribution parameterized by N

= Ψmax
i

i (s, a)

γ ≤

Ψπ

−

≤

.

N and

∈

], whose support is deﬁned by a set of atoms

Now, we may model each Ψπ
[Ψmin
i

, Ψmax
i
Zi = (cid:8)zj,i = Ψmin

i + j∆zi : 0

j < N (cid:9) , ∆zi =

≤

Ψmax
i
N

Ψmin
i
1

,

i = 1, . . . d.

∀

−
−

Finally, the atom probabilities for zj,i are given by a parameteric model θj,i :

Zθ,i(s, a) = zj,i w.p. pj,i(s, a) =

eθj,i(s,a)
j eθj,i(s,a) ,

(cid:80)

RN , e.g.

S × A →

(17)

where the softmax layer ensures that probabilities are non-negative and sum to one.

In order to update pj,i on environment transitions (s, a, φi, s(cid:48)), we project the Bellman updates for
each i onto the support of
Zi. To do this, given a sample (s, a, φi, s(cid:48)), we compute the projected
Bellman update, clipped to the interval [Ψmin
Tizj,i = clip (cid:0)φi + γzj,i; [Ψmin
ˆ

and then distribute its probability pj,i(s(cid:48), π(s(cid:48))) to the immediate neighbors of ˆ
follow Bellemare et al. [6] and deﬁne the projected operator Φ with j-th component equal to

Tizj,i. Here, we again

, Ψmax
i

, Ψmax
i

](cid:1) ,

]

i

i

(Φ ˆ

TiZθ,i(s, a))j =



clip

1

−

(cid:12)
(cid:12)
(cid:12)

zj,i

(cid:12)
(cid:12)
(cid:12)

ˆ
Tizk,i −
∆zi

N
1
(cid:88)
−

k=0



; [0, 1]

 pk,i(s(cid:48), π(s(cid:48))).

17

As standard in deep RL, we view the target distribution pk,i(s(cid:48), π(s(cid:48))) as parameterized by a set of
frozen parameters θ(cid:48). Then, the loss function to optimize for the sample (s, a, φi, s(cid:48)) is given as the
cross-entropy term

that can be easily optimized using gradient descent.

Li(θ) = DKL

Φ ˆ
TiZθ(cid:48),i(s, a)

(cid:16)

(cid:13)
(cid:17)
(cid:13)
(cid:13) Zθ,i(s, a)

,

Calculating Utilities. The calculation of (7) is a trivial matter given the distribution (17). In
particular, we have:

(18)

(19)

E[Zθ,i(s, a)p] =

1
N
(cid:88)
−

j=0

(zj,i)ppj,i(s, a),

N,

p

∈

from which we can easily compute the variance

Var[Ψπ

i (s, a)] = Var[Zθ,i(s, a)] = E[Zθ,i(s, a)2]

E[Zθ,i(s, a)]2.

−

Recall that by the independence assumption, the cross-covariance terms are ignored in these calcula-
tions, and thus Σπ
i (s, a) is represented as a diagonal matrix with entries on the i-th diagonal term
equal to Var[Ψπ

i (s, a)].

Another possibility is to compute the entropic utility Uβ exactly. In particular, using the independence
assumption of Ψπ

i (s, a) again, we have:

(cid:124)
Uβ[Ψπ(s, a)

w] =

log E

(cid:104)
eβΨπ(s,a)

(cid:124)w(cid:105)

1
β

d
(cid:88)

i=1

1
β

≈

log E

(cid:104)
eβΨπ

i (s,a)wi

(cid:105)

d
(cid:88)

=

i=1, wi

=0

wi

1
βwi

log E

(cid:104)

e(βwi)Ψπ

i (s,a)(cid:105)

=

d
(cid:88)

i=1

wiUβwi[Ψπ

i (s, a)],

(20)

and can be seen as another risk-sensitive instantiation of GPE. Crucially, the utility terms in (20) can
be calculated efﬁciently in the C51 framework using (17)

Uβ[Ψπ

i (s, a)] =

log E

eβZθ,i(s,a)(cid:105)
(cid:104)

=

1
β

1
β

log

N
1
(cid:88)
−

j=0

eβzj,ipj,i(s, a).

i

, and underﬂow for zj,i close to Ψmax

However, this quantity is difﬁcult to compute numerically, since for negative β, the terms eβzj,i often
suffer from overﬂow at zj,i close to Ψmin
. This becomes
considerably more problematic for β of larger magnitude, such as when risk-awareness is a priority, or
for rewards w of larger magnitude. We also ﬁnd that the log-sum-exp trick, a standard computational
device used for calculations of this form, offers relatively little improvement. A similar issue has
also been previously pointed out in other work using the entropic utility [50]. For this reason, we
use the mean-variance approximation, which provides an excellent approximation to the entropic
utility for various values of β, as we demonstrated experimentally, and without suffering from the
aforementioned issues above.

i

Pseudocode. The approach described above can be applied to compute the distribution of successor
features for every component i = 1, . . . d across all training task instances. This results in a new
algorithm that we call SFC51. Generally, the training procedure of SFC51 is identical in structure
to SFDQN in Barreto et al. [2], except the deterministic DQN update of successor features [53] is
replaced by the distributional C51 update described above. Therefore, the overall training procedure
is similar to Algorithm 1, but with a few subtle differences. First, instead of learning w, it is provided
to the agent as done in SFDQN. Second, every sample (s, a, φ, s(cid:48)) collected from any training is
used to update all successor feature distributions simultaneously, as also done in SFDQN. Finally,
the utility of returns can be used to select actions, rather than the expected return as done in DQN.
Applying this last modiﬁcation to SFC51 leads our proposed algorithm, which we call Risk-aware
SFC51 (RaSFC51). Of course, SFC51 can be recovered by simply setting β = 0. A complete
description of RaSFC51 with the mean-variance approximation is provided in Algorithm 22.

2In practice, the double for loop starting in lines 18 and 19 can be implemented efﬁciently by vectoring the

computation of mj,i, in languages that support vectorized arithmetic operations

18

(cid:54)
, . . . φmin

d

, φmax
d

R, φ

∈

∈

Rd, γ

∈

Algorithm 2 RaSFC51 with Mean-Variance Approximation
1: Requires m, T, N, Ne ∈
(0, 1), M1, . . . Mm ∈ M
\\

N, ε
∈
with w1, . . . wm ∈

Initialize atoms and their probability distributions

[0, 1], β, φmin

2: Initialize θ1(s, a), . . . θm(s, a) to random values
3: for i = 1, 2 . . . d do Ψmin
4: for i = 1, 2 . . . d do for j = 0, 1 . . . N

γ , Ψmax

i ←

φmax
i
1

φmin
i
1

1
Rd

, φmax
1

−

−

i ←
−

1 do zj,i ←

Main training loop

\\

5: for t = 1, 2 . . . m do

Commence training on task Mt

\\
for ne = 1, 2 . . . Ne do

Initialize task Mt with initial state s
for h = 0, 1 . . . T do

γ , ∆zi ←
Ψmin

Ψmax
i −
N
−
i + j∆zi

Ψmin
i
1

Extract sufﬁcient statistics from θt(s,

) and select the source task Mc using GPI

\\
for j = 1, 2 . . . m do Compute ˜ψj(s,
wt −
c
arg maxj maxb{
Sample action from the epsilon-greedy policy based on πc

(cid:124) ˜Σj(s, b)wt}

·
), ˜Σj(s,
·
βwt

(cid:124)
˜ψj(s, b)

) using θj(s,

←

·

) and (18) and (19)
·

Bernoulli(ε)

\\
random_a
∼
if random_a then a
Uniform(
(cid:124)
∼
˜ψc(s, b)
a
wt −
arg maxb{
Take action a in Mt and observe r and s(cid:48)
Update θ1(s, a), . . . θm(s, a)

A
βwt

←

) else
(cid:124) ˜Σc(s, b)wt}

\\
for c = 1, 2 . . . m do

Extract sufﬁcient statistics from θc(s(cid:48),
), ˜Σc(s(cid:48),
) using θc(s(cid:48),
(cid:124)
·
·
˜ψc(s(cid:48), b)
βwc
wc −

(cid:124) ˜Σc(s(cid:48), b)wc}
←
Apply Categorical Algorithm to update θc(s, a)
1 do for i = 1, . . . d do mj,i ←

\\
Compute ˜ψc(s(cid:48),
arg maxb{
a(cid:48)
\\
for j = 0, 1 . . . N
−
for i = 1, 2 . . . d do
for j = 0, 1 . . . N

1 do

·

0

) and select action a(cid:48) = πc(s(cid:48))
·
) and (18) and (19)

Tizj,i onto the support

Zi

, Ψmax
i

](cid:1)

−

Compute the projection of ˆ

i

\\
clip (cid:0)φi(s, a, s(cid:48)) + γzj,i; [Ψmin
ˆ
Tizj,i ←
( ˆ
)/∆zi
Tizj,i −
bj,i ←
, u
bj,i(cid:99)
l
← (cid:100)
← (cid:98)
Distribute probability of ˆ
\\
ml,i + pc
ml,i ←
mu,i + pc
mu,i ←

Ψmin
i
bj,i(cid:101)
Tizj,i
bj,i)
j,i(s(cid:48), a(cid:48))(u
−
l)
j,i(s(cid:48), a(cid:48))(bj,i −

end for

end for
Backpropagate through

(cid:80)

j,i mj,i log pc

j,i(s, a) to update θc(s, a)

−

6:
7:
8:

9:
10:

11:
12:

13:

14:

15:
16:

17:
18:
19:

20:

21:
22:

23:
24:
25:
26:
27:
28:
29:
30:
31:
32: end for

end for
s
s(cid:48)
←
end for

end for

A.4 Possible Generalizations of the Mean-Variance Approximation

Cumulant-Generating Functions. The quantity KR(β) = log E[eβR] in (1) is often referred to as
the cumulant-generating function. The cumulant generating function admits the well-known Taylor
expansion:

Uβ[R] =

1
β

KR(β) =

1
β

∞(cid:88)

n=1

κR(n)

βn
n!

=

∞(cid:88)

n=1

κR(n)

1

βn
−
n!

,

(21)

19

where κR(n) is the n-th cumulant of the random variable R [60]. The mean-variance approximation
(7) then follows directly from (21) by ignoring all terms of order n
3. Another way to look at the
mean-variance approximation is that it is the result of applying a Laplace approximation to the return
distribution prior to calculating its utility [46]. While it is also possible to approximate (21) using
orders of n greater than 2, such approximations would no longer provide “instantaneous" GPE. In
particular, cumulants are much harder to compute as functions of w for n = 3 [47], and no closed
formulas are even known to us for n

≥

4.

≥

(ψπ

h(s, a)

Elliptical Distributions. The mean-variance approximation (7) results from making the distribu-
tional assumption Ψπ
h(s, a)). Since the normal distribution is a member
of the class of elliptical distributions, a natural question to ask is whether GPE can apply to other
members of this class of distributions as well.
Formally, a random variable X has an elliptical distribution on Rd if there exists µ
deﬁnite Σ
∈
has the form

Rd, positive
R, and the characteristic function of X

d and a positive-valued function ξ : R

h (s, a), Σπ

∼ N

Rd

→

∈

×

(22)
Equivalently, for any random variable with characteristic function (22), there exists a positive function
gd : R

R such that the density of X is

Σt),

t
∀

∈

E[eit(cid:124)

X ] = eit(cid:124)µξ(t

(cid:124)

Rd.

µ)(cid:1) .

(cid:0)(x

(cid:124)

Σ

−

µ)

−
|

∝ |

Σ−

1/2gd

1(x
fX (x)
∼ Ed(µ, Σ, ξ). One advantage of this parameterization is that µ corre-
In either case, we write X
sponds exactly to the mean of X, e.g. E[X] = µ. Furthermore, if the covariance of X exists, then it
is equal to Σ up to a positive multiplicative constant, e.g. Var[X] = cΣ for some c > 03.
In order to connect this to the SF framework, we parameterize Ψπ
h (s, a), Σπ
h(s, a), ξ).
Then, using the linearity property (6, 12), GPE evaluates the entropic utilities of the random variables
Ψπ
w. Fortunately, afﬁne transforms of elliptically distributed random variables are univariate
elliptically distributed [55].
Lemma 2. Let X

Rd. Then, X (cid:124)w

∼ Ed(ψπ

(cid:124)
h(s, a)

h(s, a)

−

→

∼ E1(µ(cid:124)w, w(cid:124)Σw, ξ).

∼ Ed(µ, Σ, ξ) and w

∈

Applying Lemma 2 and then substituting t =
utility becomes

−

iβ, the entropic

1
β

(cid:124)

(cid:124)

◦

−

w +

β2w

β log

h (s, a)

Uβ[Ψπ

log ξ (cid:0)

w] = ψπ

(cid:124)
h(s, a)

h(s, a)w(cid:1) ,
Σπ
(23)
and is also a mean-variance approximation. However, unlike (7) in
which 1
ξ is the identity mapping, (23) is allowed to depend
non-linearly on the variance of returns. For heavy-tailed distribu-
tions, ξ should increase super-linearly for sufﬁciently large return
variances, and thus (23) will often be more sensitive to variance than
(7). This phenomenon is clearly illustrated in Figure 7 by compar-
ing the variance penalties of the normal and Laplace distributions.
Another advantage of this generalization is that the methodologies
for estimating successor features and their covariances (Appendix
A.1 and A.3) can be directly applied to this more general setting.

Figure 7: Comparing 1
ξ
◦
in (23) for normal and Laplace
distributions, for β = 1.

β log

Skew-Elliptical Distributions. The elliptical distributions represent a well-known class of symmet-
ric probability distributions, containing both heavy-tailed and light-tailed members as special cases.
However, they cannot capture skew in the return distribution that often arises in strictly discounted
MDPs. The class of generalized skew-elliptical distributions (GSE) can model skew by extending the
characteristic function (22) to

E[eit(cid:124)

X ] = eit(cid:124)µξ(t

(cid:124)

Σt)kd(t),

Rd,

(24)

t
∀

∈

R is some positive-valued function. In this case, we write X
where kd : Rd
As for the elliptical distributions (Lemma 2), it is possible to show that GSE distributions are also
closed under afﬁne transforms [57].

∼ SE d(µ, Σ, ξ).

→

3This implies that the Bellman updates (10) or (16) can still be used to learn Σπ

h, but now the resulting

estimates must be scaled by c when computing the utilities, if c is not one.

20

0.00.51.01.52.0wTΣw012345variancepenaltynormallaplaceName

Parameters

multivariate normal
multivariate Student
multivariate Laplace
multivariate logistic

µ, Σ
µ, Σ, ν
µ, Σ
µ, Σ

1
β log
ξ
◦
2 w(cid:124)Σw
β
does not exist

1
β log(1

β2
2 w(cid:124)Σw)
β√w(cid:124)Σw, 1 + β√w(cid:124)Σw)

−

−
1
β log B(1

−

Table 2: Table of common elliptical distributions with corresponding variance penalties. Here B
denotes the Beta function.

Lemma 3. Let X
function E[eitX

∼ SE d(µ, Σ, ξ) and w

∈

(cid:124)w] = eitX

(cid:124)µξ (cid:0)t2w(cid:124)Σw(cid:1) k(t; w, Σ) for some real-valued function k.

Rd. Then, X (cid:124)w is univariate GSE with characteristic

By using Lemma 3 and the substitution t =

iβ,

Uβ[Ψπ

(cid:124)
h(s, a)

w] = ψπ

(cid:124)
h (s, a)

w +

1
β

β2w

(cid:124)

h(s, a)w(cid:1) +
Σπ

−

1
β

log k(

−

iβ; w, Σπ

h(s, a)).

−
log ξ (cid:0)

This new approximation generalizes (23) through the introduction of the term log k, which intuitively
captures the skew of the return distribution.

Mixtures Densities. One signiﬁcant limitation of elliptical (and skew-elliptical) distributions to
model returns is that they are unimodal, and can fail to capture multimodal risks in the environment.
Consider the following mixture of elliptical distributions on Rd:

I
I = k

CategoricalK(π),

∼
∼ Ed(µk, Σk, ξk),

X
|
0 and (cid:80)

∈

RK satisﬁes πk ≥

where π
k πk = 1 and k = 1, . . . K deﬁne the possible modes of the
distribution. In other words, each component of the mixture is a member of an elliptical distribution.
This model extends the standard Gaussian mixture model, and can approximate any continuous
distribution to arbitrary accuracy provided K is chosen sufﬁciently large [56, 59]. In the context
of risk-aware transfer (Theorem 1, Theorem 2) this means that the approximation error terms in ε
associated with approximating Uβ could in principle be driven to zero.
Applying Lemma 2 to each component, X (cid:124)w
(cid:124)w, w(cid:124)Σkw, ξk), and so X (cid:124)w is a
mixture of univariate elliptical distributions. Now, (6, 12) can be computed using the law of total
expectation,

∼ E1(µk

I = k

|

Uβ[Ψπ

(cid:124)
h(s, a)

w] =

1
β

log

K
(cid:88)

k=1

k (s, a)eβψπ
ππ

k (s,a)

(cid:124)wξk(

(cid:124)

β2w

Σπ

k (s, a)w).

−

This expression does not simplify further unless K = 1 and is thus not a mean-variance approximation,
although it is a generalization of (23). It can be computed numerically by using the log-sum-exp
trick, and the parameters of its associated mixture density could be learned in the Bellman framework
using expectation propagation [54].

B Proofs of Theoretical Results

In this section, we verify all the theoretical claims stated in the main paper for the episodic MDPs. For
ease of exposition and due to space limitations, we also state and prove similar results for discounted
MDPs in this section4.

B.1 Proof of Lemma 1

Lemma 1. Let β

∈

R and X, Y be arbitrary random variables on Ω. Then:

4While the analysis of GPI in the risk-neutral setting [2] follows Strehl and Littman [58], our analysis of

risk-aware GPI is also inspired by Huang and Haskell [51].

21

A1 (monotonicity) if P(X
Uβ[Y ]
≥
A2 (cash invariance) Uβ[X + c] = Uβ[X] + c for every c
A3 (convexity) if β < 0 (β > 0) then Uβ is a concave (convex) function
A4 (non-expansion) for f, g : Ω

Y ) = 1 then Uβ[X]

Ω, it follows that

≥

R

∈

→

Uβ[f (X)]
|

−

Uβ[g(X)]

| ≤

P

sup
PX (Ω)

EP |

f (X)

g(X)
|

,

−

∈
where PX (Ω) is the set of all probability distributions on Ω that are absolutely continuous
w.r.t. the true distribution of X.

Proof. The ﬁrst three properties are derived in Föllmer and Schied [13]. As for the fourth property,
we use A3 and convex duality [13], [52] to write
(cid:26)

(cid:27)

Uβ[R] = sup

PR(Ω)

P

∈

EP [R]

1
β

−

D(P

P ∗)

||

,

(25)

where PR(Ω) is the set of all probability distributions on Ω absolutely continuous w.r.t. the true
distribution P ∗ of R, D is the KL-divergence between P and P ∗. Now, for f, g : Ω
Ω, f and g
are bounded and hence P -integrable for any P

PR(Ω), and using (25):

→

∈

Uβ[g(X)]
|
(cid:26)
EP [f (X)]

EP [f (X)]

1
β

D(P

P ∗)

−
||
EP [g(X)]
|

−

Uβ[f (X)]
|

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

=

≤

P

∈

sup
PX (Ω)

∈
sup
PX (Ω) |
sup
PX (Ω)

≤

EP |
∈
This completes the proof.

P

f (X)

g(X)

.
|

−

(cid:27)

−

P

sup
PX (Ω)

∈

(cid:26)

EP [g(X)]

1
β

−

D(P

P ∗)

||

(cid:27)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B.2 Proofs of Theorem 1 and 2 for Episodic MDPs

Theorem 1. Let π1, . . . πn be arbitrary deterministic Markov policies with utilities ˜
Q
evaluated in an arbitrary task M , such that
ε for all s
i = 1 . . . n and h

πi
h,β(s, a)

πi
h,β(s, a)

. Deﬁne

˜
Q
|

− Q

| ≤

π1

πn
h,β

h,β, . . . ˜
Q
,
, a
∈ A

∈ S

∈ T

πh(s)

∈

arg max

a

∈A

max
i=1...n

πi
h,β(s, a),

˜
Q

s
∀

.

∈ S

Then,

π
h,β(s, a)
Q

≥

Proof. We have for all h that

max

i Q

πi
h,β(s, a)

2(T

−

−

h + 1)ε,

h

T.

≤

max

i Q

πi
h (s, a)

|

max
i

−

πi
h (s, a)

˜
Q

| ≤

max
i

πi
h (s, a)

|Q

−

˜
Q

πi
h (s, a)

| ≤

ε

We proceed by induction on h. Clearly, the desired result holds for h = T + 1 since
˜
π
h+1,β(s, a)
Q
Q
holds uniformly at time h + 1. Using A1 and A2 of Lemma 1:

πi
T +1,β(s, a) = 0 uniformly. Next, suppose that

maxi Q

Q
πi
h+1,β(s, a)

≥

πi
T +1,β(s, a) =
h)
2ε(T

−

−

π
h,β(s, a) = Uβ[r(s, a, s(cid:48)) +
Q

π
h+1,β(s(cid:48), πh+1(s(cid:48)))]
Q

Uβ[r(s, a, s(cid:48)) + max

Uβ[r(s, a, s(cid:48)) + max

≥
= Uβ[r(s, a, s(cid:48)) + max

Uβ[r(s, a, s(cid:48)) + max

i

i Q
˜
Q
max
i
max

a(cid:48)

a(cid:48)

≥

≥

πi
h+1,β(s(cid:48), πh+1(s(cid:48)))
πi
h+1,β(s(cid:48), πh+1(s(cid:48)))

˜
Q

πi
h+1,β(s(cid:48), a(cid:48))
πi
h+1,β(s(cid:48), a(cid:48))

2ε(T

h)]

−

−

2ε(T

h)

ε]

−
2ε(T

2ε(T

−

−

−

−

−
h)

−
ε]

−
h + 1)]

i Q

22

≥

≥

≥
=

Uβ[r(s, a, s(cid:48)) + max

i

Uβ[r(s, a, s(cid:48)) + max

max

πi
h+1,β(s(cid:48), a(cid:48))]

−
a(cid:48) Q
πi
h+1,β(s(cid:48), πi,h+1(s(cid:48)))]

i Q
πi
h+1,β(s(cid:48), πi,h+1(s(cid:48)))]

2ε(T

h + 1)

−
2ε(T

h + 1)

−
2ε(T

−
h + 1)

−

−

Uβ[r(s, a, s(cid:48)) +

πi
h,β(s, a)

Q

−

Q
2ε(T

h + 1).

−

Since i is arbitrary, the proof is complete.

Lemma 4. Let

Q

ij
h be the utility of policy π∗i evaluated in task j at time h. Then for all i, j,
(cid:12)
jj
(cid:12)
h (s, a)
(cid:12) ≤

(cid:12)
(cid:12)
ii
h (s, a)
(cid:12)Q

h + 1)δij.

sup
s,a

− Q

(T

−

Proof. Let ∆ij(h) = sups,a |Q
distributions that are absolutely continuous with respect to P (

jj
. Deﬁne Ps,a to be the set of probability
h (s, a)
|
s, a). Then, using A4 from Lemma 1:

ii
h (s, a)

− Q

·|

∆ij(h)

= sup
s,a

= sup
s,a

sup
s,a

≤

sup
s,a

≤

− Q

(cid:12)
jj
(cid:12)
h (s, a)
(cid:12)

(cid:12)
(cid:12)
ii
h (s, a)
(cid:12)Q
(cid:12)
(cid:12)
(cid:12)Uβ[ri(s, a, s(cid:48)) + max
a(cid:48) Q
(cid:12)
(cid:12)
(cid:12)ri(s, a, s(cid:48))

sup

s,a)

P (cid:48)(

E

s(cid:48)

P (cid:48)

∈
sup
s(cid:48)

∼

Ps,a
(cid:12)
(cid:12)
(cid:12)ri(s, a, s(cid:48))

·|

−

sup
s,a,s(cid:48) |

≤

ri(s, a, s(cid:48))

−

rj(s, a, s(cid:48))
|

(cid:12)
(cid:12)
ii
h+1(s, a)
(cid:12)Q

= δij + sup
s,a
= δij + ∆ij(h + 1).

− Q

+ sup
s,a,s(cid:48)
(cid:12)
jj
(cid:12)
h+1(s, a)
(cid:12)

ii
h+1(s(cid:48), a(cid:48))]

−

Uβ[rj(s, a, s(cid:48)) + max

(cid:12)
jj
(cid:12)
h+1(s(cid:48), a(cid:48))]
(cid:12)

a(cid:48) Q

rj(s, a, s(cid:48)) + max

ii
h+1(s(cid:48), a(cid:48))

a(cid:48) Q

−

max

a(cid:48) Q

−

(cid:12)
jj
(cid:12)
h+1(s(cid:48), a(cid:48))
(cid:12)

rj(s, a, s(cid:48)) + max

ii
h+1(s(cid:48), a(cid:48))

(cid:12)
jj
(cid:12)
h+1(s(cid:48), a(cid:48))
(cid:12)

max

a(cid:48) Q

−

ii
h+1(s(cid:48), a(cid:48))

(cid:12)
jj
(cid:12)
h+1(s(cid:48), a(cid:48))
(cid:12)

max

a(cid:48) Q

−

a(cid:48) Q
(cid:12)
(cid:12)
(cid:12)max

a(cid:48) Q

Starting with ∆ij(T + 1) = 0 and proceeding by backward induction, we have ∆ij(h)
h + 1) for all h.

≤

δij(T

−

Lemma 5.

(cid:12)
(cid:12)
(cid:12)Q

jj
h (s, a)

(cid:12)
ji
(cid:12)
h (s, a)
(cid:12) ≤

(T

−

− Q

sup
s,a

h + 1)δij.

Proof. Deﬁne Γij(h) = sups,a |Q
Γij(h)

jj
h (s, a)

ji
. Then using A4 from Lemma 1:
h (s, a)
|

− Q

Uβ[ri(s, a, s(cid:48)) +

−

(cid:12)
ji
(cid:12)
h+1(s(cid:48), π∗j,h+1(s(cid:48)))]
(cid:12)

Q

rj(s, a, s(cid:48)) +

jj
h+1(s(cid:48), π∗j,h+1(s(cid:48)))

Q

− Q

ji
h+1(s(cid:48), π∗j,h+1(s(cid:48)))

(cid:12)
(cid:12)
(cid:12)

jj
h+1(s(cid:48), π∗j,h+1(s(cid:48)))
(cid:12)
(cid:12)
(cid:12)Q

jj
h+1(s(cid:48), π∗j,h+1(s(cid:48)))

ji
h+1(s(cid:48), π∗j,h+1(s(cid:48)))

− Q

(cid:12)
(cid:12)
(cid:12)

ji
h+1(s(cid:48), π∗j,h+1(s(cid:48)))

− Q

(cid:12)
(cid:12)
(cid:12)

= sup
s,a

= sup
s,a

sup
s,a

≤

sup
s,a

≤

(cid:12)
ji
(cid:12)
h (s, a)
(cid:12)

jj
h (s, a)

(cid:12)
(cid:12)
(cid:12)Q
(cid:12)
(cid:12)
(cid:12)Uβ[rj(s, a, s(cid:48)) +

− Q

Q

jj
h+1(s(cid:48), π∗j,h+1(s(cid:48)))]
(cid:12)
(cid:12)
(cid:12)ri(s, a, s(cid:48))

−

s(cid:48)

P (cid:48)(

s,a)

·|

sup

E

P (cid:48)

∈
sup
s(cid:48)

∼

Ps,a
(cid:12)
(cid:12)
(cid:12)ri(s, a, s(cid:48))

rj(s, a, s(cid:48)) +

−

Q

sup
s,a,s(cid:48) |

ri(s, a, s(cid:48))

−

≤

rj(s, a, s(cid:48))

jj
h+1(s, a)

− Q

δij + sup
≤
s,a
= δij + Γij(h + 1).
Thus, Γij(h)
δij(T

(cid:12)
(cid:12)
(cid:12)Q

≤

|

+ sup
s,a,s(cid:48)
(cid:12)
ji
(cid:12)
h+1(s, a)
(cid:12)

h + 1) as claimed.

−

23

Theorem 2. Let
Q
Furthermore, let ˜
Q
h
δr = mini=1...n sups,a,s(cid:48)

π∗
h,β be the utilities of optimal Markov policies π∗i evaluated in task M .
i
π∗
π∗
,
h,β be such that
h,β(s, a)
i
i
∈ A
− Q
|
and i = 1 . . . n. Similarly, let π be the corresponding policy in (4). Finally, let
. Then,
ri(s, a, s(cid:48))
|
2(T

< ε for all s

π∗
h,β(s, a)
i

h + 1)(δr + ε),

−
∗h,β(s, a)(cid:12)
(cid:12)

r(s, a, s(cid:48))
|
π
h,β(s, a)
Q

˜
Q
|

∈ S

∈ T

− Q

, a

T.

≤

−

≤

h

(cid:12)
(cid:12)

Proof. Using Theorem 1 and the triangle inequality:

(cid:12)
(cid:12)

π
h,β(s, a)
Q

− Q

∗h,β(s, a)(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)Q

π∗
j
h,β(s, a)

≤

∗h,β(s, a)

− Q

(cid:12)
(cid:12)
(cid:12) + 2(T

−

h + 1)ε.

ii
h (s, a)

The goal now is to bound the ﬁrst term. By the triangle inequality and Lemma 4 and Lemma 5,
h + 1)δij.
|
− Q
|Q
Finally, designating j as source task j and i as target task, and substituting this bound into the ﬁrst
inequality above yields the desired result.

ji
h (s, a)
|

jj
h (s, a)

jj
h (s, a)

ji
h (s, a)

ii
h (s, a)

| ≤ |Q

= 2(T

− Q

− Q

|Q

−

+

B.3 Proofs of Theorem 1 and 2 for Discounted MDPs

Theorem 5. Let π1, . . . πn be arbitrary deterministic Markov policies with utilities ˜
J
evaluated in an arbitrary task M , such that
Deﬁne

˜
πi
β (s, a, z)
J
|

πi
β (s, a, z)

− J

| ≤

π1

β , . . . ˜
πn
β
J

εz for all s, a, z and i.

Then,

π(s, z)

∈

arg max

a

∈A

max
i=1...n

˜
πi
β (s, a, z),
J

s
∀

, z

.

∈ Z

∈ S

(26)

π
β (s, a, z)

J

max

i J

≥

πi
β (s, a, z)

−

1

2ε

−

z.

γ

Proof. Deﬁne
have:

max
β
J

(s, a, z) = maxi J

πi

β (s, a, z) and ˜
J

max
β

(s, a, z) = maxi ˜
J

πi
β (s, a, z). We

max
β
|J

(s, a, z)

˜
max
β
J

−

(s, a, z)

| ≤

max
i

πi
β (s, a, z)

|J

˜
πi
β (s, a, z)
J

−

| ≤

εz.

Let T π

β be the operator corresponding to (11). Then using A1 and A2 of Lemma 1 leads to:

(cid:104)

(cid:104)

(cid:104)

T π
β

˜
max
β
J

(s, a, z) = Uβ

= Uβ

Uβ

Uβ

≥

≥

zr(s, a, s(cid:48)) + ˜
max
β
J

zr(s, a, s(cid:48)) + max

(cid:105)
(s(cid:48), π(s(cid:48), γz), γz)
(cid:105)
(s(cid:48), a(cid:48), γz)

˜
max
β
J

a(cid:48)

zr(s, a, s(cid:48)) + max
(cid:2)zr(s, a, s(cid:48)) +
(cid:104)

J

max
β
a(cid:48) J
max
β

(cid:105)
(s(cid:48), a(cid:48), γz)
−
(s(cid:48), πi(s(cid:48), γz), γz)(cid:3)
(cid:105)
πi
β (s(cid:48), πi(s(cid:48), γz), γz)
J
γεz

−

γεz

γεz

−
γεz

zr(s, a, s(cid:48)) +

πi
β (s, a, z)

Uβ
≥
= T πi
β J
πi
=
β (s, a, z)
πi
β (s, a, z)

J
max

−

−
γεz

γεz

−
εz

(s, a, z)

γεz

≥

i J
˜
max
β
J

−
Finally, using A1 of Lemma 1 and the fact that T π
β has a unique ﬁxed point [5]:

−

≥

π
β (s, a, z) = lim
→∞

k

J

(T π

β )k ˜
max
β
J

(s, a, z)

˜
max
β
J

≥

(s, a, z)

(1 + γ)

−

ε

−

1

z

γ

max
β
≥ J

(s, a, z)

−

1

and is the desired result.

2ε

−

z,

γ

24

Lemma 6. Deﬁne
evaluated in the augmented MDP j. Furthermore, let δij = sups,a,s(cid:48)
Then,

i
j (s, a, z) be the utility of optimal policy π∗i on the augmented MDP i when
.
rj(s, a, s(cid:48))
|

ri(s, a, s(cid:48))
|

−

J

(cid:12)
(cid:12)
(cid:12)J

sup
s,a

i
i (s, a, z)

j
j (s, a, z)

− J

(cid:12)
(cid:12)
(cid:12) ≤

δij

1

−

z.

γ

Proof. Deﬁne ∆ij(z) = sups,a
butions for the one-step transitions of the augmented MDP, e.g. P ((s(cid:48), z(cid:48))
continuous w.r.t. the true distribution. Since P ((s(cid:48), z(cid:48))
(s, z), a) = P (s(cid:48)
|
absolutely continuous only w.r.t. itself, the set Ps,a consists of all products P (s(cid:48)
|
s, a) is absolutely continuous w.r.t. the true dynamics of the original MDP.
P (s(cid:48)

(cid:12)
j
(cid:12). Let Ps,a be the set of probability distri-
(cid:12)
j (s, a, z)
(s, z), a) that are absolutely
|
s, a)δzγ(z(cid:48)), and δzγ(z(cid:48)) is
|
s, a)δzγ(z(cid:48)), where

i
i (s, a, z)

(cid:12)
(cid:12)
(cid:12)J

− J

|

∆ij(z)

= sup
s,a

= sup
s,a

Now, using A4 from Lemma 1:

(cid:12)
j
(cid:12)
j (s, a, z)
(cid:12)

i
i (s, a, z)

(cid:12)
(cid:12)
(cid:12)J
(cid:12)
(cid:12)
(cid:12)Uβ[zri(s, a, s(cid:48)) + max

− J

i
i (s(cid:48), a(cid:48), γz)]

−

Uβ[zrj(s, a, s(cid:48)) + max

(cid:12)
j
(cid:12)
j (s(cid:48), a(cid:48), γz)]
(cid:12)

a(cid:48) J

a(cid:48) J
(cid:12)
(cid:12)
(cid:12)zri(s, a, s(cid:48))

−

zrj(s, a, s(cid:48)) + max

i
i (s(cid:48), a(cid:48), γz)

a(cid:48) J

(cid:12)
j
(cid:12)
j (s(cid:48), a(cid:48), γz)
(cid:12)

max

−

a(cid:48) J
(cid:12)
j
(cid:12)
j (s(cid:48), a(cid:48), γz)
(cid:12)

ri(s, a, s(cid:48))

−

rj(s, a, s(cid:48))
|

i
i (s(cid:48), a(cid:48), γz)

max

a(cid:48) J

−

sup
s,a

P

sup
Ps,a

E

s(cid:48)

∼

P (

s,a)

·|

≤

≤

∈
z sup

s,a,s(cid:48) |

= zδij + sup
s,a

(cid:12)
(cid:12)
(cid:12)J

i
i (s, a, γz)

− J

= zδij + ∆ij(γz).

(cid:12)
(cid:12)
+ sup
(cid:12)max
s,a,s(cid:48)
(cid:12)
j
(cid:12)
j (s, a, γz)
(cid:12)

a(cid:48) J

Repeating the above bounding procedure leads to:

∆ij(z)

≤

≤
...

≤

zδij + ∆ij(γz)
zδij + γzδij + ∆ij(γ2z)

zδij + γzδij + γ2zδij +

=

· · ·

δij

1

−

z,

γ

(cid:12)
(cid:12)
(cid:12)J

sup
s,a

j
j (s, a, z)

(cid:12)
j
(cid:12)
i (s, a, z)
(cid:12) ≤

− J

δij

1

−

z.

γ

and completes the proof.

Lemma 7.

Proof. Deﬁne Γij(z) = sups,a |J
technique from Lemma 6:

j
j (s, a, z)

j
. Then, using A4 from Lemma 1 and the
i (s, a, z)
|

− J

Γij(z)

= sup
s,a

= sup
s,a

(cid:12)
j
(cid:12)
i (s, a, z)
(cid:12)

j
j (s, a, z)

(cid:12)
(cid:12)
(cid:12)J
(cid:12)
(cid:12)
(cid:12)Uβ[zrj(s, a, s(cid:48)) +

− J

J
(cid:12)
(cid:12)
(cid:12)zri(s, a, s(cid:48))

sup
s,a

P

sup
Ps,a

E

s(cid:48)

P (

∼

s,a)

·|

≤

≤

∈
z sup

s,a,s(cid:48) |

ri(s, a, s(cid:48))

−

rj(s, a, s(cid:48))
|

zδij + sup
s,a

≤

(cid:12)
(cid:12)
(cid:12)J

j
j (s, a, γz)

− J

+ sup
s,a,s(cid:48)

(cid:12)
(cid:12)
(cid:12)J
(cid:12)
j
(cid:12)
i (s, a, γz)
(cid:12)

j
j (s(cid:48), π∗j (s(cid:48), γz), γz)]

Uβ[zri(s, a, s(cid:48)) +

−

(cid:12)
j
(cid:12)
i (s(cid:48), π∗j (s(cid:48), γz), γz)]
(cid:12)

J

zrj(s, a, s(cid:48)) +

−

j
j (s(cid:48), π∗j (s(cid:48), γz), γz)

J

− J

(cid:12)
j
(cid:12)
i (s(cid:48), π∗j (s(cid:48), γz), γz)
(cid:12)

j
j (s(cid:48), π∗j (s(cid:48), γz), γz)

j
i (s(cid:48), π∗j (s(cid:48), γz), γz)

− J

(cid:12)
(cid:12)
(cid:12)

25

= zδij + Γij(γz)

δij

z.

1

γ

≤
The proof is complete.

−

π∗
i
β

J

Theorem 6. Let
π∗
M . Furthermore, let ˜
i
β
J
, z
S
δr = mini=1...n sups,a,s(cid:48)

∈ A

∈ Z

, a

be the utilities of optimal Markov policies π∗i evaluated in some task

∈
and i = 1 . . . n, and π be the corresponding policy in (26). Finally, let

− J

|

|

π∗
˜
β (s, a, z)
i
J

π∗
β (s, a, z)
i

< εz for all s

be such such that

r(s, a, s(cid:48))
|

−

(cid:12)
(cid:12)

J

π
β (s, a, z)

. Then,
ri(s, a, s(cid:48))
|
∗β (s, a, z)(cid:12)
(cid:12)

− J

≤

2(δr + ε)

γ

1

−

z.

Proof. Using Theorem 5:

π
β (s, a, z)

J

− J

∗β (s, a, z) =

π∗
j
β (s, a, z) +

π∗
j
β (s, a, z)

J

− J

∗β (s, a, z)

J

π
β (s, a, z)
2ε

z +

≥

1

γ

−

− J
π∗
j
β (s, a, z)

J

∗β (s, a, z).

− J

i
The goal now is to bound the difference between the last two terms. Let
j (s, a) be the entropic
utility of the optimal policy π∗i evaluated in the augmented MDP for task j. Then, by the triangle
j
i
.
inequality,
i (s, a, z)
i (s, a, z)
− J
|
i
γ z, and the result
i (s, a, z)

j
+
j (s, a, z)
|
j
i (s, a, z)

J
j
j (s, a, z)
2δij
1

j
i (s, a, z)

i
i (s, a, z)

Applying Lemma 6 and Lemma 7, we have
follows.

| ≤ |J

− J

− J

− J

| ≤

|J

|J

|J

−

B.4 Proof of Theorem 3

The proofs depend on the following result adapted from Sherstan et al. [35].
Lemma 8. Let X be a random vector in Rd that depends only on sh, ah, rh and sh+1. Then,

E (cid:2)X(Ψπ

h+1(s(cid:48), πh+1(s(cid:48)))

ψπ

h+1(s(cid:48), πh+1(s(cid:48)))

−

sh = s, ah = a(cid:3) = 0.

|

(cid:124)

We ﬁrst demonstrate that the Bellman equation (8) is correct for our problem.
Lemma 9.

Σπ

h(s, a) = E
s(cid:48)

P (

∼

s,a)

·|

(cid:2)δhδh

(cid:124)

+ Σπ

h+1(s(cid:48), πh+1(s(cid:48)))

sh = s, ah = a(cid:3) .

|

h(s, a) = φh + φh+1 + . . . and deﬁne ξπ

h (s, a) = Ψπ

h(s, a)

ψπ

h (s, a). By deﬁnition

−

Proof. Let Ψπ
of successor features, we have:
h (s, a) = Ψπ
ξπ

ψπ

−

Σπ

ψπ

ψπ

h(s, a)

h (s, a))(Ψπ

h (s, a)
−
h+1(s(cid:48), πh+1(s(cid:48)))
h+1(s(cid:48), πh+1(s(cid:48)))

h(s, a)
= φh + ψπ
= δh + ξπ
By deﬁnition, the covariance is:
h(s, a) = E [(Ψπ
h(s, a)
−
−
(cid:124)
= E [ξπ
h (s, a)ξπ
h (s, a)
sh = s, ah = a]
= E (cid:2)(δh + ξπ
h+1(s(cid:48), πh+1(s(cid:48))))(δh + ξπ
= E (cid:2)δhδh
h+1(s(cid:48), πh+1(s(cid:48)))ξπ
+ ξπ
+ E (cid:2)δhξπ
+ E (cid:2)ξπ
= E (cid:2)δhδh
= E (cid:2)δhδh

h+1(s(cid:48), πh+1(s(cid:48)))ξπ
h+1(s(cid:48), πh+1(s(cid:48)))
where the second-last line follows from Lemma 8.

(cid:124)
h+1(s(cid:48), πh+1(s(cid:48)))
(cid:124)
h+1(s(cid:48), πh+1(s(cid:48)))δh

+ ξπ
+ Σπ

(cid:124)

(cid:124)

(cid:124)

|

|

|

|

26

h (s, a) + (Ψπ

h+1(s(cid:48), πh+1(s(cid:48)))

ψπ

h+1(s(cid:48), πh+1(s(cid:48))))

−

ψπ

h (s, a))

(cid:124)

|

sh = s, ah = a]

(cid:124)
h+1(s(cid:48), πh+1(s(cid:48))))

sh = s, ah = a(cid:3)

h+1(s(cid:48), πh+1(s(cid:48)))

|

(cid:124)

sh = s, ah = a(cid:3)

sh = s, ah = a(cid:3)
sh = s, ah = a(cid:3)

|

|

h+1(s(cid:48), πh+1(s(cid:48)))
sh = s, ah = a(cid:3) ,

(cid:124)

sh = s, ah = a(cid:3)

Theorem 3. Let
ε

(cid:107) · (cid:107)
S × A × T →

:

be a matrix-compatible norm,
[0,
h (s(cid:48), πh+1(s(cid:48)))

˜ψπ
) such that
h (s, a)
(cid:107)
(cid:124)
ψπ
h (s(cid:48), πh+1(s(cid:48))))
]
(cid:104)˜δh ˜δ

∞
−

(cid:107) ≤

h + ˜Σπ

h+1(s(cid:48), πh+1(s(cid:48)))

and suppose
2
h (s, a)
−
(cid:107)
εh(s, a). Then,
(cid:105)(cid:13)
(cid:13)
(cid:13) ≤

3εh(s, a).

E

s(cid:48)

∼

−

ψπ

s,a)

≤

P (

·|

(cid:124)

h(s, a)

s,a)[˜δh( ˜ψπ
·|
(cid:13)
(cid:13)
(cid:13)Σπ

E
(cid:107)

s(cid:48)

P (

∼

exists
there
εh(s, a) and

Proof. We start by decomposing the true covariance matrix:

(cid:104)
h(s, a) = E

Σπ

(Ψπ

h(s, a)

−

˜ψπ

h (s, a) + ψπ

h (s, a)

ψπ

h (s, a))

˜ψπ

h (s, a) + ψπ

h (s, a)

(Ψπ

h(s, a)
(cid:104)

= E

(Ψπ

−
h(s, a)

+ ( ˜ψπ
h (s, a)
(cid:104)
+ 2E
Ψπ
(cid:104)

−
h(s, a)

(Ψπ

h(s, a)

= E

( ˜ψπ

h (s, a)

−

−

−
ψπ

˜ψπ

h (s, a))(Ψπ
h (s, a))( ˜ψπ
˜ψπ
h (s, a)

|

h (s, a))(Ψπ
h (s, a))( ˜ψπ

−
ψπ

−
˜ψπ

(cid:105)
sh = s, ah = a

(cid:105)
sh = s, ah = a

(cid:124)

|

(cid:124)

ψπ

( ˜ψπ
h (s, a)
(cid:124)

−

h (s, a))
(cid:105)
sh = s, ah = a

−
h (s, a))

ψπ

(cid:124)

−
h(s, a)

˜ψπ

|
h (s, a))
(cid:124)

h (s, a))

−
ψπ

h (s, a)

−
(cid:105)
sh = s, ah = a

˜ψπ

h (s, a))
(cid:124)

h (s, a))

−
ψπ

|

h(s, a)

h (s, a)
(cid:104)

−
Ψπ

where in the last step we use the identity E
E [Ψπ
sh = s, ah = a] + ψπ
h(s, a)
h (s, a)
|
−
˜ψπ
we deﬁne ˜ξπ
h (s, a) = Ψπ
h(s, a)
term above as:

ψπ

−

−
h (s, a) = ψπ
h (s, a). Now,
h (s, a), then follow the derivations in Lemma 9 to write the ﬁrst

|
h (s, a)

h (s, a)

−

−

(cid:105)
sh = s, ah = a
˜ψπ

=

h(s, a)
˜ψπ

˜ψπ

h (s, a)

h (s, a)

(cid:105)
sh = s, ah = a

(cid:124)

|

(cid:104) ˜ξπ

E

= E

(cid:124)

h (s, a) ˜ξπ
(cid:104)˜δh ˜δ
(cid:104)˜δh ˜ξπ
(cid:104)˜δh ˜δ
(cid:104)˜δh ˜ξπ

(cid:124)

h + ˜ξπ

h+1(s(cid:48), πh+1(s(cid:48))) ˜ξπ

h+1(s(cid:48), πh+1(s(cid:48)))

+ E

h+1(s(cid:48), πh+1(s(cid:48)))

= E

h + ˜Σπ

h+1(s(cid:48), πh+1(s(cid:48)))

(cid:105)
sh = s, ah = a

+ E
(cid:105)
sh = s, ah = a

|

(cid:124)

(cid:105)
sh = s, ah = a
|
(cid:104) ˜ξπ

h+1(s(cid:48), πh+1(s(cid:48)))˜δ

(cid:124)
h |

+ E

h+1(s(cid:48), πh+1(s(cid:48)))

(cid:105)
sh = s, ah = a

+ E

(cid:104) ˜ξπ

h+1(s(cid:48), πh+1(s(cid:48)))˜δ

(cid:124)

|

(cid:124)

|

(cid:105)
sh = s, ah = a

(cid:105)
sh = s, ah = a

.

(cid:124)
h |

Finally, we norm bound the desired difference as follows:
(cid:104)˜δh ˜δ

h+1(s(cid:48), πh+1(s(cid:48)))

h + ˜Σπ

(cid:124)

(cid:13)
(cid:13)
(cid:13)Σπ

sh = s, ah = a

|

(cid:105)(cid:13)
(cid:13)
(cid:13)

E
−
(cid:104)˜δh ˜ξπ

−

h (s, a)
(cid:104)˜δh(Ψπ
(cid:104)˜δh(ψπ
E

h (s, a)
(cid:104)˜δh(ψπ

−

2

E

h(s, a)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)( ˜ψπ
(cid:13)
(cid:13)
E
(cid:13)
(cid:13)

2

+

+

(cid:13)
(cid:13)
+ 2
(cid:13)
(cid:13)
(cid:13)( ˜ψπ
(cid:13)
(cid:13)
E
(cid:13)
(cid:13)
(cid:13)
˜ψπ
(cid:13)
(cid:13)

+

2

≤

≤

≤

h+1(s(cid:48), πh+1(s(cid:48)))

sh = s, ah = a

(cid:124)

|
h (s, a))( ˜ψπ
h (s, a)

ψπ

(cid:105)(cid:13)
(cid:13)
(cid:13)
(cid:124)(cid:13)
(cid:13)
(cid:13)

ψπ

h (s, a))

−

h+1(s(cid:48), πh+1(s(cid:48)))

−
h+1(s(cid:48), πh+1(s(cid:48)))

ψπ

(cid:124)
h+1(s(cid:48), πh+1(s(cid:48))))

|
(cid:124)
h+1(s(cid:48), πh+1(s(cid:48))))

˜ψπ

ψπ

h (s, a))( ˜ψπ

−
h (s, a)

ψπ

h (s, a))

(cid:124)(cid:13)
(cid:13)
(cid:13)

−

˜ψπ

(cid:124)
h+1(s(cid:48), πh+1(s(cid:48))))

sh = s, ah = a

sh = s, ah = a

|

(cid:105)(cid:13)
(cid:13)
(cid:13)
(cid:105)(cid:13)
(cid:13)
(cid:13)

sh = s, ah = a

(cid:105)(cid:13)
(cid:13)
(cid:13)

|

h+1(s(cid:48), πh+1(s(cid:48)))

−

h (s, a)

ψπ

(cid:13)
2
(cid:13)
h (s, a))
(cid:13)

−

2εh(s, a) + εh(s, a) = 3εh(s, a).

≤
This is the desired result.

27

C Experiment Details

In this section, we describe the setup of the domains discussed in the main paper in greater detail. We
also provide detailed descriptions of baseline algorithms, as well as all hyper-parameters used and
how they were selected.

C.1 Motivating Example

The motivating example is a 5-by-5 grid-world domain with discrete states and discrete actions
described by the four possible directions of movement into an adjacent cell. The environment is
made stochastic by introducing random action noise as follows. Desired actions are taken only with
probability 0.8, while the remaining time a random action is taken. Furthermore, transitions that
would take the agent outside of the boundaries of the grid leave the agent in its current position. The
cost structure is deﬁned as follows. The goal state is terminal and provides a reward of +20. Each
time step incurs a ﬁxed penalty of

1, on top of any other rewards or costs incurred.

To recover the properties of risk-aware and risk-neutral GPI claimed in the main text, we ﬁrst learn
π2
the source policies π1 and π2 and their utilities
β using a variant of the classic value
Q
iteration algorithm adapted to maximize the entropic utility (see Algorithm 3). We consider the
12 is achieved
non-discounted setting (γ = 1), and iterate until an absolute error less than εexit = 10−
between two consecutive iterations5. The two source policies are then recovered by acting greedily
with respect to the learned utilities.

π1
β and

Q

−

In order to implement GPI, we evaluate these two resulting policies on the target task by adapting
the iterative procedure in Algorithm 3 for policy evaluation. Essentially, line 10 of the algorithm is
. We repeat this procedure twice to produce
replaced by r(s, a, s(cid:48)) + γ
∈ {
two sets of value functions: a set
0.1.
−
The two GPI policies are then deﬁned as πβ(s)
0.1].
−
∈
Finally, we generate the histogram of returns by simulating episodes of length T = 35, throughout
which actions are selected from πβ, and computing the cumulative reward obtained on each.

(s(cid:48), π(s(cid:48))) for π
π2
0 }

{Q
arg maxa maxi=1,2 Q

π1, π2}
for β = 06 and a set

0.1,
πi
−
β (s, a) for β

for β =
[0,

0.1}

π1
0 ,

{Q

Q

Q

Q

∈

π1

π2

−

C.2 Four-Room

M

∈ {

2.
}

0, . . . 12

deﬁned as
The four-room domain consists of a family of discrete-state discrete-action MDPs
follows. The world is deﬁned as a set of discrete cells arranged in a grid of dimensions 13-by-13,
such that at each time instant, the agent occupies a speciﬁc cell with some x- and y-coordinates
(px, py)
As the agent explores the space, it can collect objects belonging to one of 3 possible classes. While
the initial positions of these objects remains ﬁxed throughout the experiment, their existence is
determined by whether or not they have already been collected by the agent in a given episode (the
same object cannot be picked up multiple times in a given episode). In our conﬁguration, there are 6
instances of objects belonging to each class, for a total of no = 18 collectible objects. Therefore,
2 consists of the concatenation of the agent’s current
the state space
position (px, py) and a set of binary variables indicating whether or not each object has already been
picked up by the agent. All objects are reset at the beginning of each episode. Actions are deﬁned as
that move the agent to an adjacent cell in the corresponding direction.
A
In the case that the destination cell lies outside the grid, then the agent remains in the current cell at
the next time instant.

left, up, right, down
{

0, . . . 12
}

0, 1
{

no
}

× {

=

=

S

}

The goal cell ‘G’ provides a ﬁxed reward of +1 and immediately terminates the episode upon entry.
The reward rc associated with each object class c
is reset every time a new task begins,
∈ {
1, +1]. Occupying a trap cell that triggers at a
and is sampled from a uniform distribution on [
−
particular time instant deﬁnes a failure, and is communicated to the agent by incurring a penalty of
2 and immediately terminating the episode. However, occupying a trap cell does not automatically
−
guarantee a failure. Instead, a failure is only triggered with probability 0.05 independently at every
time instant during which the agent occupies a trap cell. This additional reward stochasticity can be

1, 2, 3
}

5Please note that convergence of value iteration is guaranteed due to the existence of absorbing states and

because the underlying MDP is ergodic.

6For β = 0, Algorithm 3 reduces to standard value iteration.

28

Perform one iteration of (11) with the greedy policy derived from

Q

Algorithm 3 Value Iteration for Entropic Utility Maximization

1: Requires εexit > 0, γ
do
2: for s
∈ S
3: for n = 1, 2 . . .
do
Update

, a

[0, 1], β
∈
0

∈
(s, a)
Q

∈ A
←
∞
(s, a) for all state-action pairs
Q
, a

do

\\
for s

R,

,

(cid:104)S

, r, P

A

(cid:105) ∈ M

∈ S
(cid:48)(s, a)

\\
Q
for s(cid:48)

∈ A
0
do

←

if s(cid:48) is terminal then

r(s, a, s(cid:48))

∈ S
target

else

target

end if

(cid:48)(s, a)

←

←

Q
end for
(cid:48)(s, a)

Q
end for

←

← Q
1
β log

(cid:48)(s, a)

Q

r(s, a, s(cid:48)) + γ maxb Q
s, a) eβ
(cid:48)(s, a) + P (s(cid:48)

×

(s(cid:48), b)

target

|

Check for convergence in utility values
\\
maxs,a |Q
ε
if ε < εexit then return
If not converged, then continue with value iteration

(cid:48)(s, a)

←

(s, a)
− Q
|
(cid:48)
Q
(s, a)

, a

∈ S

∈ A

do

Q

(cid:48)(s, a)

← Q

\\
for s
18:
19: end for

4:

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

16:
17:

implemented without breaking the existing successor feature framework by introducing a ﬁctitious
terminal state sf to indicate failure, which is reached at random when in cells marked ‘X’. This
state augmentation induces a modiﬁed MDP with a deterministic reward of
2 on arrival to state sf ,
whose associated transitions are stochastic in nature. Crucially, this state augmentation transformation
applies uniformly to all task instances, and thus does not break our assumptions about
. We use a
discount factor of γ = 0.95.

M

−

}

∈ {

1, 2, 3

Exact state features φ(s, a, s(cid:48)) are provided directly to the agent. Speciﬁcally, we deﬁne φc(s, a, s(cid:48))
for every class of objects c
to take the value 1 if the agent occupies a cell with an object
of class c in state s(cid:48) and 0 otherwise. Similarly, we deﬁne φg(s, a, s(cid:48)) to take the value 1 if s(cid:48)
corresponds to the goal cell and 0 otherwise. Unlike Barreto et al. [2], the four-room domain also
contains an additional failure state with non-zero reward, as described above, and this must also
be incorporated into the SF representation. This can be done by deﬁning φf (s, a, s(cid:48)) that takes the
R5 are then the
value 1 if s(cid:48) corresponds to the state sf and 0 otherwise7. The state features φ
concatenation of φc, φg and φf . These features are sparse, but can represent the reward functions of
2, and it
all possible task instances in
(cid:124)
is now clear that r(s, a, s(cid:48)) = φ(s, a, s(cid:48))
Each time a new task is created, a new ˜ψπ and ˜Σπ are created. The training loop of RaSFQL then
proceeds according to Algorithm 1. We set α = 0.5 and ε = 0.12, based on preliminary experiments
for Q-learning. We also set ¯α = 0.1 for learning ˜Σπ and αw = 0.5 for learning w with gradient
descent. Rollouts are limited to T = 200 steps for all algorithms.

exactly. Finally, we deﬁne wc = rc, wg = 1 and wf =

w holds.

M

−

∈

The baseline used for comparison is the probabilistic policy reuse framework of Fernández and Veloso
[12] (PRQL), here adapted for learning risk-sensitive behaviors. In order to do this, we incorporate the
smart exploration strategy of Gehring and Precup [16]. This strategy is fundamentally similar to our
mean-variance approach, since it also incorporates second-moment or reward-variance information
into action selection in a similar way. The controllability bonus C π(s, a) in each state-action pair is
as
learned using a Q-learning approach by using the negative of the absolute Bellman residuals

δ
−|

|

7It is not practical to redeﬁne the task space with the augmented state sf in an actual implementation. Instead,
we simulate this by providing the state features φ with a binary variable indicating failure. This does not change
the SF implementation, since the occurrence of a failure event can be deduced using the done ﬂag (indicating
arrival in a terminal state) and the state s(cid:48).

29

N, ε, η

[0, 1], α, ρ, τ > 0, ω

Algorithm 4 RaPRQL with Smart Exploration
1: Requires m, T, Ne ∈
2: for t = 1, 2 . . . m do
3:
4:
5:
6:

Initialize Qt(s, a), C t(s, a) to small random or zero values
for k = 1, 2 . . . t do scorek ←
c
←
R
←
Commence training on task Mt

0, usedk ←

t
0

∈

0

\\
for ne = 1, 2 . . . Ne do

Initialize Mt with initial state s
for h = 0, 1 . . . T do

R, M1, . . . Mm ∈ M

∈

Select actions according to Q-values plus controllability bonus
= t then use_prev_policy

Bernoulli(η) else use_prev_policy

\\
if c
if use_prev_policy then

∼

false

←

\\
a
else

←

Action is selected from πc, the source policy being used
arg maxb{
Action is selected from πt, the policy being learned

h(s, b) + ωC c

h(s, b)

Qc

}

Uniform(

) else a

arg maxb{

←

Qt

h(s, b) + ωC t

h(s, b)
}

\\
random_a
if random_a then a

∼

Bernoulli(ε)

∼

A

end if
Take action a in Mt and observe r and s(cid:48)

Update the Q-values for the current task
h+1(s(cid:48), b)
h(s, a) + αδh

r + maxb Qt
Qt

\\
δh ←
Qt
h(s, a)

h(s, a)

Qt

−

Update the controllability bonus for the current task

C t

h(s, a) + αρ(

δh| −

−|

C t

h(s, a))

←

\\
C t
h(s, a)
R
←
s
←
end for

←
R + r
s(cid:48)

Update average return obtained by following policy πc

7:
8:
9:

10:
11:

12:
13:

14:
15:
16:
17:

18:
19:

20:
21:
22:
23:

24:

25:

scorec

usedc+R

×
usedc+1
Sample a new source policy

\\
scorec ←
\\
for k = 1, 2 . . . t do pk ←
c
∼
usedc ←
0
R
←
end for

usedc + 1

Multinomial(p1, p2, . . . pt)

eτ ×scorek
j eτ ×scorej

(cid:80)

26:
27:
28:
29:
30: end for

pseudo-rewards, and learned in parallel to the Q-values in practical implementations. The penalty for
C(s, a) is denoted as ω, and is fundamentally similar to β used by SFQL. The resulting algorithm,
which we call RaPRQL, is described in detail in Algorithm 4.
Similar to RaSFQL, every time a new task is created, a new Qπ and C π are created for RaPRQL for
learning new policies. We set α = 0.5 for fair comparison with RaSFQL, and ρ = 0.1 based on the
original implementation [16]. The performance is highly sensitive to the parameters η and τ used by
PRQL. To select these two hyper-parameters, we follow Barreto et al. [2] and run a grid search for
, selecting the combination of η and τ that resulted in the
η
}
highest cumulative return over 128 task instances. This validation experiment is repeated for every
value of ω.

0.1, 0.3, 0.5
}

1, 10, 100

and τ

∈ {

∈ {

C.3 Reacher

The physics simulator used for the reacher domain is provided by the open-source pybullet and
pybullet-gym packages [48, 49]. We adapted the Python environment in the latter package to

30

(cid:54)
−

handle multiple target goal locations as required in our problem setting. Please note that this package
is released under the MIT license.

R4 consists of the angles and angular velocities of the robotic arm’s two
The state space
S ⊂
1, +1]2 is discretized using 3 values per dimension,
joints. The two-dimensional action space
1) and zero torque for each actuator,
corresponding to maximum positive (+1) and negative (
resulting in a total of 9 possible actions. At the beginning of each episode, the angle of the central
joint is sampled from a uniform distribution on [
π, +π], while the angle of the outer joint is sampled
from a uniform distribution on [
π/2, +π/2], and the angular velocities are initialized to zero.
Furthermore, state transitions are made stochastic by adding zero-mean Gaussian noise to actions
with standard deviation 0.03, and then clipping the actions to [

1, +1].

A ⊂

[
−

−

−

−

The reward received at each time step is 1
4δ, where δ is the Euclidean distance between the target
position and the tip of the robotic arm. We deﬁne 12 target locations, of which 4 are used for training
and the remaining 8 for testing. Furthermore, circular regions of radius δf = 0.06 are placed around
6 of the 12 target locations (2 training and 4 testing) in which failures occur spontaneously with
probability pf = 0.035. Once a failure occurs, a cost of cf = 3 is incurred and the episode continues
without termination. This implies that the expected reward, as a function of the distance δ, is8

−

R(δ) =

(cid:26)1
1

4δ
4δ

−
−

if δ > δf
if δ
δf .

cf ×
Therefore, a rational9 risk-neutral agent would prefer to enter inside the failure region if it holds that
1

4δf , or in other words if

pf

−

≤

1

cf ×

pf ≥

−

−

pf ≤
Clearly, given our choice of values for cf , pf and δf , the above condition holds in our setting. Setting
up the reward structure and risk in this way makes it possible to control the trade-off between risk
and reward, and thus the anticipated behavior of the agents, in a principled way. We also apply
discounting of future reward using γ = 0.9.

cf ×

4δf .

R13, in which the ﬁrst 12 components consist of 1

The state features are vectors φ(s, a, s(cid:48))
4δg,
where δg are the Euclidean distances to each of the goal locations g. The last component takes the
value 1 if a failure event occurs and 0 otherwise. As done in the four-room experiment, state features
R13 are not learned in this instance,
are provided to the agent. However, target goal locations w
but provided directly to the agent as well. Speciﬁcally, we set wg = 1 for the goal with index g
and w13 = cf =
3, and set all other elements to zero. This recovers the correct reward function
r(s, a, s(cid:48)) for all task instances as described above.

−

−

∈

∈

×

×

×

51

The overall training and testing procedures closely mimic Barreto et al. [2]. The successor features
ψπ and their distribution Ψπ are represented as multi-layer perceptrons (MLP) with two hidden
layers of size 256 and tanh non-linearities. The SFC51 and RaSFC51 architectures are generally
identical and require output layers of dimensions R9
13, with a softmax activation function
applied with respect to the second dimension. Similarly, C51 and RaC51 also require output layers
but of dimensions R9
51 and softmax applied with respect to the second dimension. For SFDQN,
the output of the network is linear with dimensions R9
13. We also use target networks for both
SFC51/RaSFC51 and C51/RaC51, which are updated every 1,000 transitions by copying weights
from the learning networks. For SFC51, RaSFC51 and SFDQN, separate MLPs are used to learn
each policy. To allow C51 and RaC51 to generalize across target locations, we apply universal
value function approximation [33] and incorporate the target position into the state. This makes
C51 essentially identical to the DQN baseline in Barreto et al. [2], except that DQN is replaced by
C51. For C51-based agents, recall that the range of possible values of φ must also be speciﬁed. For
SFC51 and RaSFC51, we use φd
min = 0
and φ13
max = 1, which corresponds to a relatively tight bound for state features described in the
previous paragraph. For C51 and RaC51, we set the bounds to Vmin =
30 and Vmax = 10, which
−
corresponds to a tight bound for the discounted return. These intervals are discretized into N = 51
atoms for learning histograms, as recommended in the original paper [6].

max = 1 for d = 1, 2 . . . 12 and use φ13

1 and φd

min =

−

×

8The reasoning here has simpliﬁed some of the aspects of the environment, ignoring the effects of multiple

risk regions that could alter the trajectories, limited-length episodes and discounting.

9Of course, a rational agent would want to keep the tip as close to the target location as possible, and so

would want δ = 0.

31

Agents are trained on all 4 training task instances sequentially one at a time, for 200,000 time steps
per task using an epsilon-greedy policy with ε = 0.1. Analogous to Barreto et al. [2], every sample is
used to train all 4 policies simultaneously for SFC51, RaSFC51 and SFDQN. A randomized replay
buffer of inﬁnite capacity stores all previously-observed transitions (s, a, φ, s(cid:48)) from all 4 training
tasks, to avoid “catastrophic forgetting" of previously learned task instances. Each update of the
network is based on a mini-batch of size 32 sampled uniformly from the replay buffer, and uses the
3. Please note that these parameters, and those in the
Adam optimizer with a learning rate of 10−
previous paragraph, are generally identical to those used in Barreto et al. [2]. Testing follows an
epsilon-greedy policy with ε = 0.03 and greedy actions are selected according to risk-aware GPI,
. Recall that test rewards wj
e.g. a∗
wj + βwj
are provided to the agent. We set the episode length to T = 500 time steps for training and testing.
All visualizations are based on estimating the test return at regular intervals of 5,000 time steps,
calculated as the average performance of 5 independent rollouts.

(cid:124) ˜Σπi(s, a)wj}

arg maxa maxi

˜ψπi(s, a)

1,...4

}{

∈

∈{

(cid:124)

Since the performance varies for different target locations, Barreto et al. [2] applies a normalization
procedure to compare the performance between tasks in a fair manner. We apply the same procedure,
by ﬁrst training a standard C51 agent from scratch on each training and test task 10 times, and
recording the average performance at the beginning and end of training, ¯Gb and ¯Ga, respectively.
The normalized return illustrated in all ﬁgures is then calculated as Gn = (G

¯Gb).

¯Gb)/( ¯Ga −

−

C.4 Additional Details for Reproducibility

Reproducing Four-Room. The four-room experiment was run on an Alienware m17 R3, whose
software and hardware speciﬁcations are provided in Table 3. Please note that while this machine has
a GPU and tensorﬂow installed, neither were used in this experiment.

Component

Operating System
Python
tensorﬂow

Description

Windows 10 Home
3.8.5 (Anaconda)
2.3.1

Quantity

System Memory
Hard Disk
CPU
GPU

32 GB
953.9GB
Intel i7-10875H @ 2.30GHz (turbo-boost @ 5.1GHz)
Nvidia RTX 2080 Super 8GB

1
1
1

Table 3: Software and hardware conﬁguration used to run all experiments for the four-room domain.

Reproducing Reacher. The reacher experiment was run on a Lenovo ThinkStation P920 worksta-
tion, whose software and hardware speciﬁcations are described in Table 4.

Component

Operating System
Python
tensorﬂow

Description

Ubuntu 18.04
3.8.5
2.4.0

Quantity

System Memory
Hard Disk
CPU
GPU

187 GB
953.9GB
Intel Xeon Gold 6234 @ 3.30GHz (turbo-boost @ 4GHz)
Nvidia Quadro RTX 8000 48GB

5
32
2

Table 4: Software and hardware conﬁguration used to run all experiments for the reacher domain.

Other Factors. Please note that seeds were not ﬁxed during the experiment but generated in each
trial using Python’s default seed generation algorithm. This allows us to average the performance of
all algorithms over different seed values and initializations. No internal modiﬁcations to the Python
environment nor to any of its installed packages were made. No effort to overclock the machines’

32

Figure 8: Left: cumulative reward collected across all training tasks in the four-room domain, for
various values of β for RaSFQL (ω for RaPRQL). Right: cumulative number of failures across all
training tasks in the four-room domain, for various values of β, ω. Please note that legend entries in
parentheses indicate the negative values of β and ω. Shaded error bars indicate one standard error
over 30 independent runs of each algorithm.

CPUs or GPUs beyond their factory settings were made in order to decrease the overall computation
time (see below).

Computation Time. The majority of the computation time in running the experiment was allocated
to the reacher domain, partially because of the size of the network architectures required to learn
meaningful policies (2 hidden layers consisting of 256 neurons), and the number of samples required
to draw meaningful conclusions for all baselines. The computation time is considerably greater for
RaSFC51 (around 28-36 hours per trial) than it is for RaC51 (around 6-8 hours per trial), which is
expected since the former must train 4 neural networks while the latter must train only one. This
could potentially lead to negative environmental impacts if the model is to be deployed on complex
problems in real-world settings. At the same time, the potential speed-ups demonstrated by RaSFC51
as compared to RaC51 could reduce the overall training time considerably and offset the total energy
requirement of learning policies with a satisfactory variance-adjusted return. Parallelization of the
training loop could also be beneﬁcial and provide signiﬁcant time and cost savings in practice.

D Additional Ablation Studies and Plots

In this section, we include the full details and results of the ablation studies described in the main
text, and additional analysis that had to be left out of the main paper due to space limitations.

D.1 Four-Room

1,

2,

−

−

−

−

0,

∈ {

4
}

1/2,

We can study the effect of β on the return performance and risk-sensitivity of the learned behaviors by
repeating the four-room experiment (Appendix C.2) for various values of β. In particular, we trained
RaSFQL for β
(ω for RaPRQL), and recorded the cumulative reward
and number of failures across all 128 training task instances. The results of these experiments are
summarized in Figure 8. We see that the performance of RaSFQL degrades gracefully as β decreases
(a relative drop in cumulative reward of approximately 25% is observed when β is decreased from 0
4), while the corresponding degradation for RaPRQL is considerably more pronounced (a relative
to
drop in cumulative reward of roughly 75% is observed for an identical change in ω). Meanwhile, the
number of cumulative failures of RaSFQL is generally lower than RaPRQL for every pair of identical
values of β and ω. In fact, for β
, the cumulative numbers of failures are increasing
4
}
at sub-linear rates, which implies that risk-avoidance behavior is becoming more prominent as the
number of training task instances increases.

∈ {−

−

−

−

2,

1,

In order to better understand the kind of risk-averse behaviors being learned, we instantiated 27
for every object class i = 1, 2, 3. We then
novel test task instances by enumerating wi ∈ {−

1, 0, 1

}

33

020406080100120TaskInstance020000400006000080000CumulativeRewardRaSFQL(4)RaSFQL(2)RaSFQL(1)RaSFQL(0.5)SFQLRaPRQL(4)RaPRQL(2)RaPRQL(1)RaPRQL(0.5)PRQL020406080100120TaskInstance01000200030004000500060007000CumulativeFailuresRaSFQL(4)RaSFQL(2)RaSFQL(1)RaSFQL(0.5)SFQLRaPRQL(4)RaPRQL(2)RaPRQL(1)RaPRQL(0.5)PRQLFigure 9: Visitation counts over 100 rollouts from behavior/training policies (epsilon-greedy with
ε = 0.12) derived from GPI after training on all 128 task instances. The behavior policies are
.
illustrated on 27 novel task instances in which the reward w varies, e.g. w1, w2, w3 ∈ {−
}
Left: Behavior policies derived from GPI for RaSFQL with β =
2. Right: Behavior policies
derived from GPI for standard SFQL. Visitation counts are averaged over 30 independent runs for
each algorithm.

1, 0, 1

−

tested the performance of the GPI policy obtained from the training procedure described in the main
paper, by simulating 100 rollouts following the epsilon-greedy policy with ε = 0.1 on each of the
test tasks. Please note that no training was ever performed on the test tasks. The state visitation
counts across all 100 trajectories were computed for every task instance and arranged in a 3D-lattice
as indicated in Figure 9. We repeated this procedure twice: once for RaSFQL with β =
2 and
once for SFQL. Interestingly, RaSFQL and SFQL learn behaviors that are similar to each other when
looking at the same task, but each of them exploits different regions of the state space depending on
the reward. However, RaSFQL almost always learns to avoid the dangerous objects in the bottom-left
and top-right rooms, whereas SFQL does not necessarily do so. This discrepancy is most evident, for
instance, when w2 = 1 and for (w1, w2, w3) = (1,

1) and (w1, w2, w3) = (1,

1, 0).

−

1,

−

−

−

D.2 Reacher

0,

1,

For the reacher domain, we conducted a similar analysis of the risk-avoidance behaviors elicited
by the entropic utility by repeating the experiment (Appendix C.3) for different values of β. The
left plot in Figure 10 illustrates the total number of failures across all test tasks for RaSFC51 and
RaC51 for β
. The number of breakdowns decreases predictably as β is
decreased from zero. Unlike the four-room domain however, the number of failures of RaSFC51 is
modestly greater than RaC51 for the same values of β. In order to better understand how efﬁciently
the trade-off between risk and reward is handled by these two approaches, we decided to compute
an alternative measure of return by dividing the normalized return by the total number of failures.
Intuitively, this quantity provides an estimate of the expected reward collected between between
successive failure events. The right plot contained in Figure 10, which compares this quantity for

4
}

∈ {

3,

2,

−

−

−

−

34

−10+1w1−10+1w2−10+1w3−10+1w1−10+1w2−10+1w3Figure 10: Ablation study for the number of failures on test tasks in the reacher domain, as a function
of β. Left: Total failures across all test tasks for various values of β. Legend entries in parentheses
indicate the negative values of β. Right: In order to assess the trade-off between return and possibility
of failure, we divide the normalized return, averaged across all test tasks, by the total number of
failures for each value of β. The resulting measure is compared between RaSFC51 and RaC51. The
x-axis indicates the negative values of β.

RaSFC51 and RaC51 for different values of β, shows that RaSFC51 is actually much more efﬁcient
at managing the trade-off between risk and reward for larger-magnitude values of β. This is not
surprising, given that RaSFC51 can obtain much high return than RaC51 for a comparable number of
failures, when β =
4, the number of failures of RaSFC51 and
RaC51 become equivalent as both methods learn sufﬁciently conservative policies. Even in this case,
successor features combined with GPI allow RaSFC51 to generalize much better on novel tasks than
RaC51.

4. In fact, for β =

3 and β =

−

−

−

As suggested in the main text, one possible conjecture is that RaSFC51 learns to correctly solve
the test tasks, requiring the robotic arm to hover closer to the edge of the risky areas, while RaC51
does not. The presence of environment stochasticity, errors in function approximation, and the
stochasticity of the epsilon-greedy policy used during testing could exacerbate this. Comparing the
rollouts produced by RaSFC51 and RaC51 in training tasks in Figure 11, and testing tasks in Figure
12, conﬁrms that RaSFC51 is much better at task generalization than RaC51. Here, RaSFC51 learns
to hover right at the boundaries of the high-variance regions, preferring not to enter them whenever
possible. On the other hand, risk-neutral SFC51 is completely unaware of the risky areas, focusing
exclusively on minimizing the distance to the target location, but is able to successfully locate all
targets. RaC51 demonstrates similar risk-aware behaviors as RaSFC51, but cannot reliably locate the
target on some of the test task instances.

A similar conclusion can also be drawn by observing the heat-maps of the learned mean-variance
objectives in Figure 13. For SFC51, these objectives take the highest values precisely at the target
locations, whereas for RaSFC51 these take the highest values slightly away from the targets in regions
of low volatility. This is expected as the utility of hovering very close to a target location centered in
a risky region should be lower than hovering outside the risky region, for a sufﬁciently risk-averse
agent. Moreover, the ﬁrst 4 rows correspond to training task values and the last 8 correspond to test
task values. Because a similar pattern described above can also be observed in test tasks, the ability
of SFs to generalize expected return estimates to novel task instances also extends to higher-order
sufﬁcient statistics, namely the variance of return. Finally, the aggregated plots located in the top
half in Figure 14 show that RaSFC51 learns the return variance correctly after having trained on all
4 task instances. On the other hand, the SFDQN architecture that learns the covariance using the
residual method (8) is unable to learn the variance correctly, likely due to the propagation of errors
and overestimation bias in ˜ψπi(s, a) as discussed in the main paper.

References

[46] Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.

35

01234TrainingTaskInstance020406080100120FailuresSFC51(4)SFC51(3)SFC51(2)SFC51(1)SFC51(0)C51(4)C51(3)C51(2)C51(1)C51(0)01234Risk-Aversion0.00.10.20.30.40.5NormalizedReturnperFailureRaSFC51RaC51Figure 11: Evolutions of the robotic arm tip position in three rollouts of the reacher domain according
to the GPI policy obtained after training on all 4 tasks. Here, all 4 training tasks are shown.

Figure 12: Evolutions of the robotic arm tip position in three rollouts of the reacher domain according
to the GPI policy obtained after training on all 4 tasks. Here, all 8 test tasks are shown.

[47] Kris Boudt, Dries Cornilly, and Tim Verdonck. A coskewness shrinkage approach for estimating the
skewness of linear combinations of random variables. Journal of Financial Econometrics, 18(1):1–23,
2020.

[48] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and

machine learning. http://pybullet.org, 2016–2021.

[49] Benjamin Ellenberger. Pybullet gymperium. https://github.com/benelot/pybullet-gym, 2018–

2019.

[50] Abhijit A Gosavi, Sajal K Das, and Susan L Murray. Beyond exponential utility functions: A variance-
adjusted approach for risk-averse reinforcement learning. In 2014 IEEE Symposium on Adaptive Dynamic
Programming and Reinforcement Learning (ADPRL), pages 1–8. IEEE, 2014.

[51] Wenjie Huang and William B Haskell. Stochastic approximation for risk-aware markov decision processes.

IEEE Transactions on Automatic Control, 2020.

[52] Fabio Maccheroni, Massimo Marinacci, and Aldo Rustichini. Ambiguity aversion, robustness, and the

variational representation of preferences. Econometrica, 74(6):1447–1498, 2006.

36

RaSFC51SFC51RaC51RaSFC51SFC51RaC51(cid:124)

wj

wj −

Figure 13: Each plot located in column i and row j illustrates the value of the mean-variance objective
(cid:124)Σπi (s, a)wj as a function of the robotic arm tip position in (x, y) coordinates
ψπi(s, a)
for the reacher domain, after training each agent on all 4 tasks. In other words, the ﬁrst 4 rows
illustrate the value functions learned on the training task instances, while the last 8 rows illustrate the
value functions learned on the test tasks. Left: mean-variance objective computed by RaSFC51 with
β =

3. Right: mean-variance objective computed by SFC51.

−

37

Figure 14: Each plot located in column i and row j illustrates the value of the variance
(cid:124)Σπi(s, a)wj as a function of the robotic arm tip position in (x, y) coordinates for the reacher
wj
domain, after training on 1, 2, 3 and 4 source tasks (respectively, left to right). Top: variance
computed by RaSFC51 with β =

3. Bottom: variance computed by SFDQN using (8).

−

38

[53] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie,
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis
Hassabis. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.

[54] Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.

Parametric return density estimation for reinforcement learning. In UAI, pages 368–375, 2010.

[55] Joel Owen and Ramon Rabinovitch. On the class of elliptical distributions and their applications to the

theory of portfolio choice. The Journal of Finance, 38(3):745–752, 1983.

[56] David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley & Sons,

2015.

[57] Tomer Shushi. Generalized skew-elliptical distributions are closed under afﬁne transformations. Statistics
& Probability Letters, 134:1–4, 2018. ISSN 0167-7152. doi: https://doi.org/10.1016/j.spl.2017.10.012.
URL https://www.sciencedirect.com/science/article/pii/S0167715217303267.

[58] Alexander L Strehl and Michael L Littman. A theoretical analysis of model-based interval estimation. In

ICML, pages 856–863, 2005.

[59] D Michael Titterington, Adrian FM Smith, and UE Makov. Statistical analysis of ﬁnite mixture distributions,

volume 198. John Wiley & Sons Incorporated, 1985.

[60] Eric W. Weisstein. Cumulant. from mathworld—a wolfram web resource. URL https://mathworld.

wolfram.com/Cumulant.html. Last visited on 13/4/2021.

39

