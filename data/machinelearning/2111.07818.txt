A Teacher-Student Markov Decision Process-based Framework
for Online Correctional Learning*

Inˆes Lourenc¸o, Rebecka Winqvist, Cristian R. Rojas, and Bo Wahlberg1

2
2
0
2

r
a

M
9
2

]

G
L
.
s
c
[

2
v
8
1
8
7
0
.
1
1
1
2
:
v
i
X
r
a

Abstract— A classical learning setting typically concerns an
agent/student who collects data, or observations, from a system
in order to estimate a certain property of interest. Correctional
learning is a type of cooperative teacher-student framework
where a teacher, who has partial knowledge about the system,
has the ability to observe and alter (correct) the observations
received by the student in order to improve the accuracy of its
estimate. In this paper, we show how the variance of the estimate
of the student can be reduced with the help of the teacher.
We formulate the corresponding online problem – where the
teacher has to decide, at each time instant, whether or not to
change the observations due to a limited budget – as a Markov
decision process, from which the optimal policy is derived
using dynamic programming. We validate the framework in
numerical experiments, and compare the optimal online policy
with the one from the batch setting.

I. INTRODUCTION

With the rapid growth of smart systems and IoT, we
are able to collect enormous amounts of data like never
before. These data may range from medical images captured
by camera sensors to distance measures from e.g. lidars
and radars. From this data, agents can learn to perform
tasks such as cancer detection and prognosis [1], [2], and
autonomous driving [3]. These are only some examples and
the application domain is much more extensive.

In the Oxford dictionary [4], the term learning is deﬁned
as the “acquisition of knowledge or skills through study,
experience, or being taught”. In this work we consider
a combination of the latter two; “experience”, by using
dynamic programming to train a teacher, and “being taught”,
by letting the trained teacher transfer its knowledge to a
student agent. Setups that involve the presence of aiding
expert agents are commonly denoted cooperative learning
problems.

Cooperative problems play an important role in our lives.
Indeed, most tasks we perform require some sort of col-
laboration; acquiring a new skill, such as learning how to
drive, social
learning, search and rescue operations, and
much more. Solving these tasks is, however, not trivial, and
the aid of an external agent can be very helpful. In the
literature, the two most famous paradigms of cooperative
learning that use a teacher-student framework are learning
from demonstration [5] and imitation learning [6], in which

*This work was supported by the Wallenberg AI, Autonomous Systems
and Software Program (WASP), the Swedish Research Council Research
Environment NewLEADS under contract 2016-06079, and the Digital
Futures project EXTREMUM.

1The

authors

Systems, KTH Royal
{ineslo,rebwin,crro,bo}@kth.se

are with the Division of Decision and Control
Institute of Technology, Stockholm, Sweden

the role of the teacher is to accelerate the learning of the
student by means of showing it the optimal policy.

In this work, we consider a different

teacher-student
paradigm. We study how the teacher can assist the student by
intercepting, and altering, the data collected by the student.
This approach is denoted correctional learning and was
recently proposed by [7], in an effort to tackle the problem of
assisting an agent in situations where transmitting knowledge
directly might be impossible or undesirable. This correc-
tional learning framework opens up for new possibilities.
In the traditional learning setting, helping an agent learn
a policy, the parameters of a system, or the state of the
world, are some of the potential applications. These problems
are called reinforcement learning, system identiﬁcation, and
ﬁltering, respectively. Other examples are manual output-
error correction of machine learning models, cooperative
learning for task-performing, and estimation of user prefer-
ences and ratings. Alternatively, the correctional framework
may be viewed as a means for diversifying the information
presented to a user – in social media applications and search
engines, it could tackle the growing issue of echo chambers
or conﬁrmation bias, and the spreading of “fake news”.
Financial applications are another ﬁeld of interest, in which
the framework could be used to e.g. inﬂuence an investor’s
market state predictions for stock portfolio allocation.

In most of these ﬁelds, however, immediate (online) action
is typically required as observations arrive sequentially, due
to the need of a learning process that adapts and rapidly
changes. Online algorithms often make learning faster and
computationally cheaper. In this paper, we thus present the
online correctional learning framework where the teacher has
to decide, at each time instant, whether or not to alter the
corresponding observation.

The research question we answer in this paper is then:

How should a teacher modify, at each instant and under
budget constraints, the data received by a student in order
to assist its learning process?

The main contributions of this paper are as follows.
• Computation of a theoretical bound for how much the
teacher can improve the estimation of the student in the
case of discrete systems.

• Formulation of a Markov decision process (MDP) for
the correctional learning framework performed in an
online setting.

• Demonstration of the results in two numerical experi-
ments; in particular, the optimal policy of the teacher
obtained using dynamic programming.

 
 
 
 
 
 
• Comparison of the proposed online correctional learning

framework with the batch framework.

The rest of the paper is organised as follows. In Section II,
the correctional learning problem is formulated. In Sec-
tion III, we derive bounds for how much the teacher can help
the student, and, in Section IV, the proposed algorithm for
performing online correctional learning is derived. Finally,
Section V validates the presented methods in numerical
experiments and Section VI concludes the paper.

A. Related work

Incorporating external information in decision-making is a
commonly studied problem, not least in the teacher-student
frameworks previously mentioned [5], [6]. Despite being
of a similar nature, our proposed framework differs from
these in its nature of correcting the observations. Below are
examples of areas that inspire our work and which resemble
the correctional behavior of the teacher when selecting the
inputs to intercept and alter.

Feature selection is a technique used in learning and
classiﬁcation tasks to select
the most relevant and non-
redundant features to improve learning [8]. Similarly to our
correctional learning framework, this family of methods has
seen a shift from batch to online techniques [9] – which
represent a more promising group of efﬁcient and scalable
machine learning algorithms for large-scale applications.

Our work is positioned around other important methods
such as input design for system identiﬁcation [10], [11],
where the input signals are designed to guarantee a certain
model accuracy; active learning [12], where the learner
queries the teacher for the desired labels; counterfactual
explanations [13], which is a branch within explainable
artiﬁcial intelligence that uses feature importance to explain
how a small perturbation of an input datapoint affects the
output of a machine learning model; and learning with side
information [14], in which additional information is provided
to the learner to help its learning process. The concept of
side information is also very popular in information theory
(in connection to communication problems) [15]. Examples
of other areas are active fault diagnosis [16], consisting of
the design of an input signal for minimizing the time and
energy required to detect and isolate faults in the outputs of
a system; anomaly detection [17], which aims to improve
the performance of the model by removing anomalies from
the training sample; and controlled sensing [18], where the
decision-maker can choose at each time instant which sensor
to use to obtain the next measurement.

In the next sections we show how we use these techniques
as motivation and inspiration to create a simple and efﬁcient
online mechanism for sequentially correcting observations in
a wide variety of applications.

II. PROBLEM FORMULATION

In this section, we deﬁne the notation and formally intro-
duce the correctional learning problem: a teacher, who has
knowledge about a system of interest, aims to help a student’s
learning process by altering the data it receives.

Original
observations

Corrected
observations

System

θ0

=
D
yk
{

}

N
k=1

˜
=
D
˜yk
{

N
k=1
}

Correction

Student
(Estimation
algorithm)

Teacher
θ0, yk, bk

Estimate

˜θ

ˆθ

→

Fig. 1. Schematic representation of the correctional learning framework.

A. Notation

All vectors are column vectors and inequalities between
vectors are considered element-wise. The i-th element of
a vector v is [v]i. A generic probability density (or mass)
) and P
function is denoted as p(
is the probability of
·
{·}
event
) is the uniform distribution. The vector of
ones is denoted as 1, the set of natural numbers including
zero as N0, and the indicator function as I(
). The ℓ1 norm of
·
a vector v is denoted as
k1 and card(S) is the cardinality
of set

. Unif(

k

v

·

·

.

S

B. Introduction to Correctional Learning

}

D

∈ Y

N
k=1, yk

A learning agent (student) is sequentially collecting in-
formation from a source in the form of observations
=
, throughout N time steps, and estimating
yk
{
characteristics of interest ˆθ about that system. An expert
agent (teacher) has more knowledge about the system and
its goal is to assist the estimation process of the student.
However, for several reasons it might be impossible or
undesirable for the teacher to transmit its knowledge directly
to the student. For example, the expert’s knowledge might
be too abstract (teaching someone to drive), or too complex
to be transmitted, the communication might be restricted
due to privacy concerns, or the teacher and student might
operate in different model classes or parameterizations [7].
The teacher, therefore, instead has the ability to interfere
by intercepting, and altering, the observations collected by
the student to ˜
N
k=1. A schematic representation
D
is shown in Figure 1. Improving the estimation thus means
obtaining an altered estimate ˜θ closer to the true estimate
(
), or which converges to the true
θ0||
θ0|| ≤ ||
||
˜θ
). This is of particular
one in less iterations (var
}
{
importance when the teacher has studied the system of
interest for a longer time than the student and thus has
a more accurate estimate. However, if this estimate is not
perfect, rather than giving the student its estimate, it instead
corrects the information acquired by the student to better
match its own. One can also imagine that the student uses
few training examples to study the behaviour of an agent,
and might thus include exploratory actions in its analysis –
which biases its estimation and illustrate the importance of
a teacher interfering to alter these.

< var
{

ˆθ
}

˜θN

ˆθN

˜yk

=

−

−

{

}

Additionally, altering the data might be expensive or dan-
gerous for privacy concerns. Therefore, correctional learning

)

b,

B(

includes the budget constraint
, ˜
D
where B is a distance measure between two sequences which
represents the budget, b, that the teacher has on how much it
can interfere with the observations obtained from the system.
If the observations are discrete, B can be deﬁned as the l1-
norm 1
N

(1)

˜yk

yk

≤

D

b.

N
k=1 |

−

| ≤

C. Batch Correctional Learning

P

In batch correctional learning, multiple observations are
intercepted simultaneously. This can be the case in, for
example, a communication channel, where multiple bits can
be delayed on the way from the source to the receiver. In
[7], the batch problem was solved by minimizing the distance
between the true parameter and the empirical estimate com-
puted by the student, V (θ0, ˜θ), according to the following
optimization problem:

V (θ0, ˜θ)

min
˜D
s.t.

, for all ˜yk

˜yk

B(

∈ Y
, ˜
D

D

)

b,

≤

,

˜
D

∈

(2)

where B is the distance measure from (1). In [7] it was shown
that the resulting set ˜D of corrected observations was the
optimal one for the case of binomial data, and by how much
the variance of the corrected estimate is decreased compared
to the original one. In this paper, we formulate an MDP to
solve the problem in an online setting and for an extensive
variety of applications.

III. CORRECTIONAL LEARNING BOUNDS
FOR DISCRETE SYSTEMS

In this section we analyse how much the teacher can ef-
fectively help the student, for the case when the observations
are discrete.

The following theorem relates the estimates of the mean
values of two sequences of observations – the original, Y /N ,
and the corrected one by the teacher, ˜Y /N . Knowing how
much the variance of the corrected estimate is decreased is
a measure of how much the teacher can help reducing the
error of the estimation of the student.

Theorem 1 (Variance decrease of the altered estimate):
0, 1, . . . , M
{
}
+ XN . Let

Let X1, . . . , XN be i.i.d. random variables in
with mean µ,
0, 1, . . . , N
B
˜Y =

and Y = X1 +
, and

N µ

∈ {

· · ·

Z

}

arg min
{Z∈{0,...,N }:|Y −Z|≤B}|

−

.

|

(3)

(4)

2B2
N M 2

−
Unif (
{

(cid:20)

.

(cid:21)

Then,

var[ ˜Y /N ]

M 2 exp

≤

) (this
Let us further assume that Xi
assumption is not crucial but provides a special case that is
easier to understand). Then,

0, . . . , M

∼

}

var[ ˜Y /N ]
var[Y /N ] ≤

6M
5M + 1

exp

2B2
N M 2

.

(cid:21)

−

(cid:20)

(5)

The proof of Theorem 1 is given in Appendix I. This
theorem provides an upper bound for the decrease in variance
of the estimate computed by the student due to the help of
the teacher, according to its budget B. It implies that:

i) the teacher’s ability to improve the learning of the

student increases with its budget;

ii) for a given budget,

the improvement becomes less
important as N grows. This is reasonable, since the av-
(1/√N ),
erage deviation of Y /N around µ is of order
while the improvement due to the teacher can be at most
B/N ;

O

iii) for a ﬁxed budget and N , the improvement degrades as
M increases, since the variance of Y /N increases with
M , which makes it increasingly harder for a teacher to
compensate for “bad” samples.

After computing by how much the teacher can improve the
estimation process of the student in a discrete setting, we
next propose a framework for how the teacher can achieve
this by altering the observations in real time.

IV. ONLINE CORRECTIONAL LEARNING

In this section we present a framework for computing an
optimal online policy for the teacher. Unlike the batch case,
the online setting is a more realistic scenario in which the
observations are obtained sequentially and the expert has to
make, at each time instant, the decision of whether or not to
change the current observation.

A. Formulation of the Markov Decision Process

We are now ready to present the second main result of
our paper, which is the formulation of an MDP to describe
the teacher’s policy for the online correctional
learning
problem when the student samples discrete observations
from a system to estimate the
0, 1, . . . , M
yk
true parameter θ0:

∈ {

=

Y

}

States:

s = (x1:k, bk, yk)

Actions: a =

Time-horizon: N

keep yk, change to ¯yk

{

}

Reward function:

˜θN
Constraint: number of times the action

θ0||1

− ||

−

Transition probabilities:

“change to ¯yk”is taken
see (8).

b

≤

S

In more details:
of the MDP are tuples containing: i)
• States: The states
1 vector with the number of times each
x1:k – an M
N0 – the
observation has been seen until time k; ii) bk
current budget left to use at time k; iii) yk – the observation
received at time k. The number of states, card(
), is ﬁnite
and upper bounded by N M+1b. However, the constraint
N renders many of these states invalid, which

×

∈

S

M
l=1[x]l

results in a much smaller and tight upper bound: card(
P

S

)

≤

≤

card(x)bN . Here, card(x) can be computed using multiset
coefﬁcients as

N

card(x) =

N

=

M
n

M (M + 1) . . . (M + n

1)

,

−

n!

(cid:19)(cid:19)

n=1 (cid:18)(cid:18)
X

n=1
X
(6)
which are the N -permutations of M with repetitions and
which satisfy the previous constraint.

• Terminal states: These are the ones where all the observa-

tions have been received, that is, where

M

[x]l = N.

(7)

l=1
X

• Actions: The possible actions are to keep the last observa-
. The number

tion yk or change it to a certain value ˜yk
of actions is card(A) = M .

∈ Y

• Reward function: The reward is zero in all states except
in the terminal states, where it is inversely proportional
to the error of the estimate computed after the teacher’s
alterations.

• Transition probabilities: If the action is “keep yk”, the
next state depends, with probability p(yk+1), on the next
received observation yk+1. The value of the next state is
obtained by simply replacing the last value of the previous
state yk by the new observation received, and adding one
to that entry of the vector x, [x′]yk+1 = [x]yk+1 + 1. If the
action is “change to ˜yk”, the next state will have the same
probability as in the previous case, where one is added
to [x]yk+1. However, it will now have a one subtracted
from the previous observation in [x]yk and a one added in
[x]˜yk (since yk was altered to ˜yk), as well as a budget of
bk+1 = bk
P

1. We can write these mathematically as:

s = (x, b, yk), a = “keep yk”

(x′, b, yk+1)

−

{

|

where [x′]yk+1 = [x]yk+1 + 1

= p(yk+1),

}

P

{

−

1, yk+1)

s = (x, b, yk), a = “˜yk”

(x′, b
where [x′]yk+1 = [x]yk+1 + 1, [x′]yk = [x]yk −
[x′]˜yk = [x]˜yk + 1,

}

|

= p(yk+1),
1, and

and P

others
}

{

= 0.

(8)
Note that the chosen formulation of the states and actions
satisﬁes the Markovian property.

• Constraint: The constraint is enforced by attributing an
inﬁnitely negative reward to transitions to states where
the budget would be bk+1 < 0.

The optimal policy for the online correctional learning
problem represented as the previous MDP can be obtained
using dynamic programming [19].

This framework can be translated to different scenarios by
adjusting the reward function to a representative description
of the student’s goal in the task at hand.

1

0.5

r
o
r
r
E

ˆθ − θ0
˜θ − θ0 (online)
θ∗
− θ0 (batch)

0

10

20

30

40

50

Experiment number

Fig. 2.
The blue line shows the estimation errors obtained during 50
experiments with the original sequence of observations (b = 0). The orange
line represents how much this error is decreased with the help of the teacher
with b = 1, that is, by changing one observation. The online policy learned
by the teacher thus allows it to reduce the errors considerably, closely
approaching the batch case shown by the dashed black line. The larger
the budget b, the closer the orange and black curves become to emin (9).

Remark 1: This framework can also be used when the
observations are continuous, by discretizing the observation
space and changing the constraint to the total amount of
b.
correction

˜yk

yk

N
k=1 |

−

| ≤

P

V. NUMERICAL RESULTS
In this section, we validate the framework proposed in
Section IV by showing signiﬁcant gains in using the teacher
to improve the learning of the student in two different tasks.
We ﬁrst consider the simple example of computing the mean
of bi- and multi-nomial data, since it grants us, due to its
simplicity, the derivation of explicit solutions for the batch
and online settings, as well as a thorough analysis of the
intrinsic workings of the framework. We then apply the
proposed framework to a problem of biological parameter
estimation,
to illustrate its application to more complex
scenarios. The simulations were performed using Python 3.7
and a 1.90 GHz CPU.

A. Example: multinomial data

Figure 2 presents the results of the MDP proposed in
Section IV for performing correctional learning in an online
setting. The ﬁgure shows the error of the estimate of the
student with, in blue, the original sequence of observations
– that is, without the help of the teacher – and, in orange,
the corrected sequence. The estimates θ are computed as
M
the mean of the observations, [θ]i =
k=1 I(yk = i)/N ,
and we consider that an observation is randomly sampled
N = 5 times from a multinomial distribution with parameter
θ0 = [0.4; 0.3; 0.3], over 50 experiments. Unlike in the
binomial case, where we can compute a closed form solution
for the minimum attainable error (see Appendix II), in the
multinomial case this error is given by the batch error, which
we computed using (8) from [7] and with the l1-norm in
the objective function for consistency. The minimum error,
independent of ˆθ and b, can, however, be computed as

P

emin(N, θ0) =

θ0 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

[θ0N ]
N

1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= 0.2,

(9)

0.04

}

˜

θ

{

r
a
v

0.02

0

5

10

15
Number of observations, N

20

25

0

Actions

b=0
b=1
b=2

0.3

0.2

)
0
θ

|

a
(
p

0.1

θ0 = 1

θ0 = 4

θ0 = 8

0.3

0.2

0.1

0

Actions

0.3

0.2

0.1

0

Actions

Action 1

Action 2

Action 3

Action 4

Fig. 4. Effect of the parameter θ0 on the behaviour a of the agent. These
statistics are the base of our model and were computed over one training
simulation with 2000 episodes. For the rest of this example we assume that
θ0 = 4 is the true parameter, being the observations sampled from the
distribution on the middle.

Fig. 3. Reduction of the variance of the estimate for increasing budgets
b as N increases. The case b = 0 corresponds to when the teacher cannot
assist the student.

which is achieved by θ∗ = [0.4; 0.4; 0.2] or [0.4; 0.2; 0.4]. In
(9), the brackets [
] without subscript mean rounding to the
·
closest integer value, subject to the constraint 1T θmin = 1
where θmin = [θ0N ]
N .

Intuitively, one would expect the teacher’s optimal policy
to be delaying as much as possible spending its budget. In
the binomial case the online policy learned,

µ∗ =

(

ak = keep yk,
ak = alter to ˜yk = 1

if bk

≤
yk,

−

0 or [x]yk ≤

otherwise,
(cid:2)

[θ0]yk N

,

y b = 0
˜θ = 1

b = 2
˜θ = 4

c
n
e
u
q
e
r
f

d
e
v
r
e
s
b
O

8
6
4
2
0

Actions

Actions

)
θ
;

(
L

H
0.36
−
0.34

0.32

0.3

,
d
o
o
h
i
l
e
k
i
L

1

(cid:3)
(10)

b=0

b=1

b=2

4
θ

8

is optimal since it coincides with the policy computed using
batch correctional learning, as is shown in Appendix II. In
the multinomial case from Figure 2, both differ only in a
limited amount of scenarios, when a less expected sample
that has a small reward is obtained. Note that in experiment
11 (marked with an arrow in the ﬁgure), the altered estimate
˜θ is even worse than the original one, ˆθ. The teacher chose
to alter the fourth observation of 1, 2, 0, 2, ? to a 0 since the
expected value was larger (receiving a 1 or a 2 at k = 5 had
a large probability and maximum reward), but the less likely
observation, 0, was received instead.

Figure 3 shows that, as expected, the variance of the
estimate decreases as the number of observations increases.
However, as the budget of the teacher increases, the variance
is further decreased. This result illustrates the conclusions
from Theorem 1.

B. More complex example: biological parameter estimation

Biological internal models have taken a major role in
the exploration and validation of neuroscientiﬁc theories,
e.g., when it comes to understanding the role of the Cere-
belum in motor control [20], or predicting and treating
neurological diseases [21]. In the next example, we apply
online correctional learning to a scenario where the student
estimates biological neural parameters θ from observing the
actions a performed by biological agents, such as animals
are in this case the
or other humans. The observations
history
of actions a, and problems like this are called
inverse problems [22]. Using a forward model of behaviour,
the actions distribution p(a
θ) can be computed, and, from
|
; θ). Maximizing this likelihood
there, the likelihood L(

H

Y

H

Fig. 5. On the left is shown an example of the action distributions seen
by the student before and after the teacher corrections, for a budget of
b = 2 for N = 10. On the right, is shown the effect of these corrections
on the corresponding negative likelihood, with the estimated parameter (the
one that has the minimum negative likelihood) marked with a dot. For a
budget of b = 2, the action distribution is closer to the true distribution
corresponding to θ0 = 4 from Figure 4, and therefore the minimum negative
likelihood parameter becomes ˜θ = θ0 = 4 instead of ˜θ = 1.

(or minimizing its negative value), originates the student’s
estimate ˆθ. In [23], the authors use this inverse method to
estimate parameters of neural time perception mechanisms.
The model [24] proposed to replicate these mechanisms
generates the data from Figure 4 over 2000 episodes. As
the student observes the actions of the animal throughout
the task, the teacher uses the framework proposed in Section
IV to correct certain observations (obtaining a corrected
history ˜
), in order to improve the student’s estimation of
H
the animal’s biological parameters – correct the sampled
distribution to be more similar to the distribution from Figure
4 that corresponds to the true parameter θ0. In this example,
the observations are the actions performed by the animal
(M is the number of possible actions), and θ0 is the number
of microstimuli of its time perception mechanism [25]. The
reward function is given by the difference between θ0 and
˜θ, where the latter is computed from

˜θ = arg min

L( ˜
H

; θ),

θ −
and which corresponds to the difference between the stu-
dent’s corrected estimate and the true parameter.

(11)

The left plot of Figure 5 shows the total number of
times each action was observed by the student in a certain

n
o
i
t
a
c
ﬁ
i
s
s
a
l
c
s
i

M

]

%

[

s
r
o
r
r
e

40

20

0

0

1
Budget, b

2

Fig. 6. The percentage of times that the student incorrectly estimates the
true parameter decreases with the increase of the budget of the teacher,
averaged over 1000 experiments and plotted for different sample sizes N .

experiment, and the corresponding corrections of the teacher
as its budget increases. The right plot illustrates how these
corrections alter the likelihood of the student estimating each
parameter, converging to ˜θ = θ0 = 4 for budgets b larger
than changing 1 out of N = 10 actions. Figure 6 shows how
the estimation error decreases over multiple episodes as the
budget allocated to help the student increases.

C. Other applications

The two previous examples demonstrate the application
of the framework when the observations are samples from a
system of interest or actions performed by an agent. These
settings extend to a large variety of problems, such as assisted
language learning or improved hypothesis testing, and bring
together a variety of ﬁelds such as input design and active
learning. When training neural networks, for example, cor-
recting the inputs could be compared to input design methods
presented in Section I-A. In reinforcement learning tasks, a
teacher could use online correctional learning to accelerate
the learning of the student in real time. The framework can
also be easily translated to an adversarial setting, where
the teacher ﬁnds the perturbation of the observations that
maximizes the impact on the student’s estimate – e.g., data
poisoning [26, Section 6.1].

VI. CONCLUSION

In this work we considered that an expert agent, a teacher,
can observe the observations collected by a student agent
from a certain system of interest. We used correctional
learning to study how the teacher can alter these observations
in real time and under budget constraints in order to improve
the learning process of the student. We bounded by how
much the teacher can help the estimation of the student
by reducing the variance of its estimate, and derived an
MDP that gives the optimal policy to perform correctional
learning in an online setting using dynamic programming.
We illustrated the improvement of the estimation when using
multinomial data (Figure 3), and in a biological parameter
estimation setting to illustrate the success of the framework
in more complex settings (Figure 6).

The way is now paved for extending this method to several
interesting applications mentioned throughout the paper, such
as correctional reinforcement learning and comparison with
related approaches. Tackling the dimensionality problem of
MDPs will be an important step along the way.

REFERENCES

[1] L. Shen, L. R. Margolies, J. Rothstein, E. Fluder, R. B. McBride,
and W. Sieh, “Deep learning to improve breast cancer detection on
screening mammography,” Scientiﬁc Reports, vol. 9, 2019.

[2] K. Kourou, T. P. Exarchos, K. P. Exarchos, M. V. Karamouzis, and
D. I. Fotiadis, “Machine learning applications in cancer prognosis
and prediction,” Computational and Structural Biotechnology Journal,
vol. 13, pp. 8–17, 2015.

[3] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? the kitti vision benchmark suite,” in 2012 IEEE Conference
on Computer Vision and Pattern Recognition, 2012, pp. 3354–3361.

[4] “learn, v.” in OED Online. Oxford University Press, Oct. 2021.
[5] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey
learning from demonstration,” Robotics and Autonomous

of robot
Systems, vol. 57, no. 5, pp. 469–483, 2009.

[6] A. Hussein, M. Gaber, E. Elyan, and C. Jayne, “Imitation learning: A
survey of learning methods,” ACM Computing Surveys, vol. 50, 2017.
[7] I. Lourenc¸o, R. Mattila, C. R. Rojas, and B. Wahlberg, “Cooperative
system identiﬁcation via correctional learning,” 19th IFAC Symposium
on System Identiﬁcation, vol. 54, no. 7, pp. 19–24, 2021.

[8] K. Kira and L. A. Rendell, “A practical approach to feature selection,”
in Machine learning proceedings 1992. Elsevier, 1992, pp. 249–256.
[9] J. Wang, P. Zhao, S. C. Hoi, and R. Jin, “Online feature selection
and its applications,” IEEE Transactions on knowledge and data
engineering, vol. 26, no. 3, pp. 698–710, 2013.

[10] L. Pronzato, “Optimal experimental design and some related control

problems,” Automatica, vol. 44, no. 2, pp. 303–325, 2008.

[11] H. Hjalmarsson, “System identiﬁcation of complex and structured

systems,” European journal of control, vol. 15, pp. 275–310, 2009.

[12] C. C. Aggarwal, X. Kong, Q. Gu, J. Han, and S. Y. Philip, “Active
learning: A survey,” in Data Classiﬁcation. Chapman and Hall/CRC,
2014, pp. 599–634.

[13] S. Verma, J. Dickerson, and K. Hines, “Counterfactual explanations
for machine learning: A review,” arXiv preprint arXiv:2010.10596,
2020.

[14] P. Kuusela and D. Ocone, “Learning with side information: PAC
learning bounds,” Journal of Computer and System Sciences, vol. 68,
no. 3, pp. 521–545, 2004.

[15] T. M. Cover and J. A. Thomas, Elements of Information Theory, second

edition. Wiley Interscience, 2006.

[16] T. A. N. Heirung and A. Mesbah, “Input design for active fault
diagnosis,” Annual Reviews in Control, vol. 47, pp. 35–50, 2019.
[17] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A
survey,” ACM computing surveys, vol. 41, no. 3, pp. 1–58, 2009.
[18] V. Krishnamurthy, Partially Observed Markov Decision Processes:
From Filtering to Controlled Sensing. Cambridge University Press,
2016.

[19] M. L. Puterman, Markov Decision Processes: Discrete Stochastic

Dynamic Programming.

John Wiley & Sons, 2014.

[20] Q. Welniarz, Y. Worbe, and C. Gallea, “The forward model: a unifying
theory for the role of the cerebellum in motor control and sense of
agency,” Frontiers in Systems Neuroscience, vol. 15, 2021.

[21] P. Lanillos, D. Oliva, A. Philippsen, Y. Yamashita, Y. Nagai, and
G. Cheng, “A review on neural network models of schizophrenia and
autism spectrum disorder,” Neural Networks, vol. 122, pp. 338–363,
2020.

[22] H. W. Engl, C. Flamm, P. K¨ugler, J. Lu, S. M¨uller, and P. Schuster,
“Inverse problems in systems biology,” Inverse Problems, vol. 25,
no. 12, p. 123014, 2009.

[23] I. Lourenc¸o, R. Mattila, R. Ventura, and B. Wahlberg, “A biologically-
inspired computational model of time perception,” IEEE Transactions
on Cognitive and Developmental Systems, 2021.

[24] I. Lourenc¸o, R. Ventura, and B. Wahlberg, “Teaching robots to
perceive time: A twofold learning approach,” in 2020 Joint IEEE
10th International Conference on Development and Learning and
Epigenetic Robotics (ICDL-EpiRob), 2020.

[25] E. A. Ludvig, R. S. Sutton, and E. J. Kehoe, “Stimulus representation
and the timing of reward-prediction errors in models of the dopamine
system,” Neural computation, vol. 20, no. 12, pp. 3034–3054, 2008.
[26] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z.
Kolter, “Differentiable convex optimization layers,” NEURIPS, vol. 32,
2019.

APPENDIX I
BOUNDING THE DECREASE IN VARIANCE

Let us ﬁrst present two known lemmas used to solve

Theorem 1.

Therefore, from Lemma 2,

var[ ˜Y ] = E[(X

λ)2]

2

≤

−

Lemma 1: Let X be a nonnegative random variable

= 2

exp

(R.V.). Then,

E[X] =

∞

P (X

Proof: Let F be the CDF of X, i.e., F (τ ) = P (X

0

Z

τ )dτ.

≥

τ ). Then, by integration by parts

E[X] =

∞

0
Z

τ dF (τ ) =

∞

−

0
Z

τ d[1

−

F (τ )

]

=P (X>τ )

=

τ [1

−

−

F (τ )]

∞

∞

+

τ =0

0
Z

|
d[1

}
F (τ )

{z
−

]

=P (X>τ )

∞

=

P (X > τ )dτ.

|

{z

}

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0
Z
Note that P (X
τ ) = P (X > τ ) + P (X = τ ), where
P (X = τ ) > 0 for at most a countable number of values of
τ , so

≥

∞

and

P (X = τ )dτ = 0

0
Z

E[X] =

∞

0

Z

P (X

≥

τ )dτ.

Lemma 2: Let X be a R.V., and λ a constant. Then,
∞

E[(X

λ)2] =

−

0
Z

X

P (
|

λ

−

| ≥

√τ )dτ.

Proof: Use Lemma 1 with X replaced by (X

λ)2.

−

Let us now restate Theorem 1 and prove it in three parts.

Theorem 1: Let X1, . . . , XN
0, 1, . . . , M

with mean µ, and Y = X1 +

be

i.i.d. R.V.’s

in
+ XN . Let

· · ·

{
B

∈ {

}

0, 1, . . . , N
˜Y =

, and

}

arg min
{Z∈{0,...,N }:|Y −Z|≤B}|

Z

N µ

.

|

−

Then, var[ ˜Y /N ]
. Let us further assume
Unif ([0, . . . , M ]) (this assumption is not crucial
that Xi
i
but provides a special case that is easier to understand). Then,

M 2 exp

≤

∼

−

h

2B2
N M 2

var[ ˜Y /N ]
var[Y /N ] ≤

6M
5M + 1

exp

2B2
N M 2

.

(cid:21)

−

(cid:20)

Proof: Let us start by computing var[ ˜Y ]. Using Ho-

effding’s inequality, we have that

˜Y

P (
|

| ≥

N µ
−
= P ( ˜Y
= P (Y

2 exp

≤

√τ ) =
N µ + √τ ) + P ( ˜Y
N µ + √τ + B) + P (Y
2(√τ + B)2
N M 2

≤

.

−

(cid:21)

≥

≥

(cid:20)

N µ

√τ )

−
N µ

≤

√τ

B)

−

−

∞

exp

−

(cid:20)
B) du

0
Z
2(u

du

·

−

2u2
N M 2
(cid:21)
2u2
N M 2

−

(cid:21)

0
Z
e−v dv = N M 2 exp

∞

0
Z

∞

0
Z

∞

2B2
N M 2

Z

−

(cid:20)
u exp

−
(cid:20)
N M 2
4

= 4

4

≤

≤

2(√τ + B)2
N M 2

dτ

(cid:21)

∞

4B

exp

2u2
N M 2

du

(cid:21)

.

−

(cid:20)
2B2
N M 2

−

(cid:21)
Unif([0, . . . , M ]),

(cid:20)

∼

Let us now compute var[Y ]. If Xi

M

var[Y ] =

=

=

1
M + 1

1
M + 1

M
2

k

−

2

(cid:21)

1
M + 1

[k2

kM + M 2]

k=0 (cid:20)
X
M

−

k=0
X
[M 2(M + 1)

M (M + 1)
2

+

M (M + 1)(2M + 1)]

M

−

+

1
6

= M 2

+

M (2M + 1) =

M 2
2

1
6

−
Finally, we conclude that

5M 2 + M
6

.

var[ ˜Y /N ]
var[Y /N ] ≤

M 2 exp

2B2
N M 2

−
5M 2+M
h
6

=

i

6M
5M + 1

exp

2B2
N M 2

.

(cid:21)

−

(cid:20)

APPENDIX II
RESULTS FOR BINOMIAL DATA

We exemplify now the case in which the observations
are randomly sampled from a binomial distribution with
parameter θ0 = 0.5, for comparison with the batch case from
[7]). We deﬁne N = 10 and repeat the algorithm over 50
experiments.

Figure 7 shows the results of the MDP proposed in
Section IV for performing online correctional learning now
for a binomial distribution instead of a multinomial as in
Section V. A main difference in this case is that we can prove
that the policy learned by the teacher is optimal, as stated
in Section IV, since the error of the estimate of the student
with the corrected sequence of observations (in orange) –
that is, without the help of the teacher – is always exactly
the minimum error attainable for the original sequence (in
black). The exact expression for this error is:

e(N, θ0, b, ˆθ) = max

θ0 −
where the second term is the emin from (9). This error is now
always never larger than the error with the original sequence
of observations (in blue).

ˆθ
k1 −

, emin

(12)

(cid:27)

(cid:26)

k

.

2b
N

Here, an analysis of the policy values obtained using
the dynamic programming algorithm shows that indeed the
teacher chooses the optimal policy, deﬁned in (V-A), of
delaying spending its budget as much as possible and doing

so only once too many of one of the outcomes is sampled.
In this case switching to the other value does not represent a
risk, unlike in the multinomial case where a speciﬁc outcome
to change to has to be chosen. A speciﬁc example from
Figure 7 is the sequence 1101110001, where the teacher
alters the orange value that was a 1 to a 0 regardless the
value of the next sample.

ˆθ − θ0
˜θ − θ0 (online)
θ∗
− θ0 (batch)

0.4

r
o
r
r
E

0.2

0

0

10

20

30

40

50

Experiment number

Fig. 7.
Error of the estimate obtained online by the student with (in
orange), and without (in blue) the help of the teacher. The former coincides
with the black line, which is the theoretical best estimate for that situation
(12) (b = 1, N = 10). The bigger the b, the closer the orange and black
lines become to the minimum error emin (9).

This figure "Figure_8.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/2111.07818v2

