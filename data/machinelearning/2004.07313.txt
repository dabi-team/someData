Evaluation of Generalizability of Neural Program Analyzers
under Semantic-Preserving Transformations

Md. Rafiqul Islam Rabin
University of Houston
Houston, TX, USA

Mohammad Amin Alipour
University of Houston
Houston, TX, USA

1
2
0
2

r
a

M
8
1

]
E
S
.
s
c
[

2
v
3
1
3
7
0
.
4
0
0
2
:
v
i
X
r
a

ABSTRACT
The abundance of publicly available source code repositories, in
conjunction with the advances in neural networks, has enabled
data-driven approaches to program analysis. These approaches,
called neural program analyzers, use neural networks to extract
patterns in the programs for tasks ranging from development pro-
ductivity to program reasoning. Despite the growing popularity
of neural program analyzers, the extent to which their results are
generalizable is unknown.

In this paper, we perform a large-scale evaluation of the gener-
alizability of two popular neural program analyzers using seven
semantically-equivalent transformations of programs. Our results
caution that in many cases the neural program analyzers fail to
generalize well, sometimes to programs with negligible textual
differences. The results provide the initial stepping stones for quan-
tifying robustness in neural program analyzers.

KEYWORDS
neural models, code representation, evaluation, program transfor-
mation

ACM Reference Format:
Md. Rafiqul Islam Rabin and Mohammad Amin Alipour. 2018. Evaluation of
Generalizability of Neural Program Analyzers under Semantic-Preserving
Transformations. In Proceedings of (Work-in-progress). ACM, New York, NY,
USA, 11 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Abundance of publicly available source code repositories has en-
abled a surge in data-driven approaches to programs analysis tasks.
Those approaches aim to discover common programming patterns
for various downstream applications[2], e.g., prediction of data
types in dynamically typed languages [14], detection of the vari-
able naming issues [3], or repair of software defects [9]. The advent
of deep neural networks has accelerated the innovation in this area
and has greatly enhanced the performance of these approaches.
The performance of deep neural networks in cognitive tasks such
as method name prediction or variable naming has reached or
exceeded the performance of other data-driven approaches. The
performance of neural networks has encouraged researchers to

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Work-in-progress, ,
¬© 2018 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/1122445.1122456

increasingly adopt the neural networks in the program analysis
giving rise to increasing use of neural program analyzers.

While the performance of the neural program analyzers contin-
ues to improve, the extent to which they can generalize to new,
unseen programs is still unknown. This problem is of more impor-
tance if we want to use them in downstream safety-critical tasks,
such as malware detection. This problem is particularly hard, as the
interpretation of neural models that constitute the core reasoning
engine of neural program analyzers remains to be challenging‚Äî
especially for the complex neural networks (e.g., RNN) that are
commonly used for processing source code.

Reliable application of neural program analyzers requires aware-
ness of the limits of these analyzers. A complete understanding of
the extent of usefulness of such approaches would help developers
to know when to use data-driven approaches and when to resort
to traditional deductive methods of program analysis. It also would
help researchers to focus their efforts on devising techniques to
alleviate the shortcomings of these analyzers. Lack of knowledge
about the limits of the neural program analyzers may exaggerate
their capability and cause careless applications of the analyzers
on the domains that they are not suited for; or, spending time and
efforts on developing neural program analyzers while a traditional,
more understandable technique can perform equally well or better.
Recently, we have seen a growing interest in the rigorous evalu-
ation of neural program analyzers. Wang and Christodorescu [35]
compared the robustness of different program representation un-
der compiler optimization transformations. They found that the
program representations based on static code features are more
sensitive to such changes than dynamic code features. Allamanis
[1] evaluated the impact of code duplication in various neural pro-
gram analyzers and found that code duplication in the training and
test datasets inflated the performance of almost all current neural
program analyzers. More recently, more preliminary studies in this
field started to emerge; e.g., Rabin et al. [28] proposed the idea of
testing neural program analyzers using semantic-preserving trans-
formations, and Yefet et al. [39] followed and proposed adversarial
example generation for neural program analyzers using prediction
attribution [32].
Goal: In this paper, we attempt to understand the limits of generaliz-
ability of neural program analyzers by comparing their behavior un-
der semantic-preserving transformations; that is, how the result of
a neural program analyzer generalizes to a semantically-equivalent
program. We should note that intent from generalizability differs
from Kang et al. [19]. Kang et al., in fact, evaluate the usefulness
of a neural program analyzer in downstream tasks, while we eval-
uate their generalizability to semantically-equivalent programs.
Moreover, this work, while related to, does not address the neural
robustness [33] in neural program analyzers, as robustness requires

 
 
 
 
 
 
Work-in-progress, ,

Md. Rafiqul Islam Rabin and Mohammad Amin Alipour

public int compareTo ( ApplicationAttemptId other) {

public int compareTo ( ApplicationAttemptId var0) {

int compareAppIds = this . getApplicationId ()
. compareTo (other. getApplicationId () );

int compareAppIds = this . getApplicationId ()
. compareTo (var0. getApplicationId () );

if ( compareAppIds == 0) {

if ( compareAppIds == 0) {

return this . getAttemptId () - other.

return this . getAttemptId () - var0.

getAttemptId () ;

} else {

return compareAppIds ;

}

}

getAttemptId () ;

} else {

return compareAppIds ;

}

}

Prediction before transformation: compareTo

Prediction after transformation: getCount

Figure 1: Variable Renaming on java-small/test/hadoop/ApplicationAttemptId.java file.

imperceptible changes to programs. Although the impact of our
transformations on the semantics of the program is imperceptible,
the change on the textual structure of the program is not necessar-
ily imperceptible. Nonetheless, this work can be considered as a
stepping stone towards the robustness of neural program analyzers.
In this paper, we report the results of a study on the generaliz-
ability of two highly-cited neural program analyzers: code2vec [5]
and code2seq [4]. In this study, we transform programs from the
original dataset code2seq is trained on to generate semantically-
equivalent counterparts. We employ seven semantic-preserving
transformations that impact the structure of programs (i.e. abstract
syntax tree) with varying degrees.

Our results suggest that the models evaluated in this study are
sensitive to the transformation and in many cases, transformations
of a program lead the neural program analyzers to produce a differ-
ent prediction than the neural program analyzers would produce
on the original program. This sensitivity remains an issue even in
the case of very small changes to the programs, such as renaming
variables or reordering independent statements in a basic block.
The result of this study is a cautionary tale that reveals that the
generalizability of neural program analyzers is still far from ideal
and require more attention from the research community to devise
robust models of code and canonicalized program representation.
Contributions. This paper makes the following contributions.

‚Ä¢ We introduce the notion of generalizability in neural pro-

gram analyzers.

‚Ä¢ We perform a large-scale study to evaluate the state-of-the-

art neural program analyzers.

‚Ä¢ We discuss the practical implication of our results.

2 MOTIVATING EXAMPLE
We use code2vec [5] for exposition in this section. The code2vec [5]
is a neural program analyzer that predicts the name of a Java method
given its body. Such neural program analyzer, in addition to other
developer productivity tools, can be useful in classification of code,
and code similarity detection.

Figure 1 shows two semantically-identical functions that imple-
ment compareTo function. The only difference between them is in
the name of one of the variables. The snippet on the left uses other,
while the code on the right uses var0. The results of code2vec
however on these semantically equivalent programs are drastically

different. code2vec predicts the snippet on the left to be compareTo
function, and the function on the right to be getCount. It seems
that code2vec heavily relied on the variable name other for its
correct prediction.

Lack of robustness to modest changes hampers the wider appli-
cation of neural program analyzers beyond developer productivity
tasks, particularly in the mission-critical problem settings where
higher levels of robustness and generalizability are required e.g.,
malware detection. Despite the significant progresses made in novel
application of neural networks for program analysis tasks, their
generalizability with respect to program transformations have not
been adequately explored.

3 BACKGROUND
Most neural program analyzers are essentially classifiers that take a
code snippet or a whole program as an input, and make predictions
about some of its characteristics; e.g., a bug prediction classifier
that predicts the buggy-ness of statements in the input program.
Performance of a neural program analyzer mainly depends on
three main components: quality of data, the neural network archi-
tecture and its learning parameters, and the representation of data
for the neural network.

Currently, most studies use open-source projects usually in main-
stream programming languages, e.g., C#, Java, C, or JavaScript. The
available standard datasets for these tasks are still very young and
their quality is somewhat unknown. For example, a recent study
by Allamanis [1] showed that virtually all available datasets suffer
from code duplication that can greatly impact the performance of
neural program analyzers.

Another factor in the performance of neural program analyzers
is source code representation. Since neural networks can only take
vectors of numbers, source code embeddings are used to produce a
vector representation of source code. The representation determines
which program features and how to be represented in the vector
embeddings. The representations can be broadly categorized into
two categories: static and dynamic. Static program representations
consider only the features that can be extracted from parsing text of
the programs, while dynamic representations include some features
pertaining to the real execution of programs.

The third building block in building a neural program analyzer
is a neural network architecture and learning parameters. There
are numerous choices of network architectures each with different

Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations

Work-in-progress, ,

Original
Programs

Transformation
Engine

Transformed
Programs

Neural pro-
gram analyzer

Figure 2: The workflow of evaluation in this study.

properties. It seems that the class of recurrent neural networks (e.g.,
LSTM) and graph neural networks are among the most popular
architecture in neural program analyzers.

4 EVALUATION APPROACH
In this section, we describe the experimental setting and our evalu-
ation approach. Figure 2 depicts an overall view of the evaluation
process. Our approach relies on metamorphic relations that state
the output of a neural program analyzer should not substantially
differ on semantically-equivalent programs. It is similar to the no-
tion of local fidelity of classifiers that state that the behavior of
classifiers should not change substantially in the vicinity of an in-
put [29]. The approach can broadly be divided into two main steps:
(1) semantic-preserving program transformation, and (2) discrep-
ancy identification (Oracle).

4.1 Transformations
We have used the following seven transformations to generate
semantically-equivalent programs.

‚Ä¢ Variable Renaming (VN) renames the name of a variable.
The new name of the variable will be in the form of varN
for a value of N such that N that has not been defined in the
scope.

‚Ä¢ Loop Exchange (LX) replaces for loops with while loops

or vice versa.

‚Ä¢ Switch to If (SF) replaces a switch statement in the pro-

gram with its equivalent if statement.

‚Ä¢ Boolean Exchange (BX) switches the value of a boolean
variable from true to false or vice versa, and propagates
this change in the program to ensure a semantic equivalence
of the transformed program with the original program.
‚Ä¢ Permute Statement (PS) swaps two independent statements

(i.e., with no dependence) in a basic block.

‚Ä¢ Try-Catch Insertion (TC) adds try-catch statements to
a basic block. The catch captures the highest exception in
Java, i.e. Exception. Note that although this transformation
does not change the behavior of the program in the nor-
mal executions, it alters the error-handling behavior of the
program.

‚Ä¢ Unused Statement (UN) inserts an unused string declara-

tion to a random basic block in the program.

Note that each transformation has a different impact on the

structure of programs:

‚Ä¢ The Variable Renaming transformation only changes the
terminal values and does not affect the structure of AST.

‚Ä¢ The Boolean Exchange transformation alters the value of
true or false and modifies the structure of AST by remov-
ing or inserting unary-not nodes.

‚Ä¢ The Loop Exchange transformation extensively impacts the

AST by removing and inserting nodes.

‚Ä¢ The Switch to If also impacts the AST of the program sub-

stantially by removing and inserting nodes.

‚Ä¢ The Permute Statement transformation does not change the
nodes in AST, rather it only reorders two subtrees in the
AST.

‚Ä¢ The Try-Catch Insertion transformation modifies the struc-
ture of AST by adding additional nodes and branches to
realize the try-catch block.

‚Ä¢ The Unused Statement transformation adds a few nodes into

the AST which increases the number of paths.

4.2 Oracle
Given the original program and the transformed, we try both in the
neural program analyzer and compare the results. We compare the
predicted label of the analyzer in both original and transformed pro-
grams. Ideally, the neural model should behave similarly with both
the original and the transformed program. A discrepancy between
the predicted label on the original program and the transformed
program is considered prediction change.

5 EXPERIMENTAL SETTING
In this section, we describe the models and datasets that we used
in the evaluation.

5.1 Subject Neural Program Analyzers
We have used code2vec [5] and code2seq [4] neural program ana-
lyzers for the experimentation. Their task is to predict a method‚Äôs
name given the body of a method.

code2vec uses a bag of AST paths to model the source code.
Each path consists of a pair of terminals in the abstract syntax tree
and their corresponding path between them in the AST. The path,
along with source and destination terminals are mapped into its
vector embeddings which are learned jointly with other network
parameters during training. The three separate vectors of each
path-context are then concatenated to a single context vector using
a fully connected layer which is learned during training with the
network. An attention vector is also learned with the network
which is used to score each path-context and aggregate multiple
path-contexts to a single code vector representing the method‚Äôs
body. After that, the model predicts the probability of each target
method‚Äôs name given the code vector of method‚Äôs body with a
softmax-normalization between the code vector and each of the
embeddings of target method‚Äôs name.

code2vec uses monolithic path embeddings and only generates
a single label at a time, while the code2seq model uses an encoder-
decoder architecture to encode paths node-by-node and generate
label as sequences at each step. The encoder represents a method‚Äôs
body as a set of AST paths where each path is compressed to a
fixed-length vector using a bi-directional LSTM which encodes
paths node-by-node. The decoder uses attention to select relevant

Work-in-progress, ,

Md. Rafiqul Islam Rabin and Mohammad Amin Alipour

Table 1: Characteristics of the models in neural program analyzer.

Model

Dataset

Precision Recall

F1 Score

code2vec

code2seq

Java-Small
Java-Med
Java-Large

Java-Small
Java-Med
Java-Large

28.36
42.55
45.17

46.30
59.94
64.03

22.37
30.85
32.28

38.81
48.03
55.02

25.01
35.76
37.65

42.23
53.33
59.19

paths while decoding and predicts sub-tokens of target sequence at
each step when generating the method‚Äôs name.

5.2 Datasets
The datasets published along with code2vec only contains the pro-
grams in preprocessed format, but, for this study, we needed the
raw Java files to make the transformations. Fortunately, the datasets
accompanied by code2seq contained the java files, and code2seq and
code2vec shared the same code pre-processing techniques. There-
fore, we used the code2seq dataset for training neural program
analyzers for the study.

There are three Java datasets:Java-Small, Java-Med, and Java-

Large.

‚Ä¢ Java-Small: This dataset contains 9 Java projects for training,
1 for validation and 1 for testing. Overall, it contains about
700K methods. The compressed size is about 366MB and the
extracted size is about 1.9GB.

‚Ä¢ Java-Med: This dataset contains 800 Java projects for train-
ing, 100 for validation and 100 for testing. Overall, it contains
about 4M examples. The compressed size is about 1.8GB and
the extracted size is about 9.3GB.

‚Ä¢ Java-Large: This dataset contains 9000 Java projects for
training, 200 for validation and 300 for testing. Overall, it
contains about 16M examples. The compressed size is about
7.2GB and the extracted size is about 37GB.

5.3 Training Models per Datasets
The authors of code2vec and code2seq have made the programs for
training a neural model and evaluating it public. We use their tools
on three aforementioned datasets to train and create three code2vec
neural program analyzers and three code2seq code2seq.

We train each model up to 100 epochs and save after each epoch.
We stop in an earlier epoch if the F1 score, in that epoch, is rea-
sonably close to the score reported in the corresponding papers.
Otherwise, we continue for 100 epochs and select the model in
the epoch with the highest F1 score. Table 1 summarizes the char-
acteristics of the trained models. While the performance of our
trained models for code2seq is on par with to the ones reported in
the corresponding paper [4], the performance of code2vec, after 100
epochs did not reach the performance reported in [5], perhaps due
to the differences in the dataset. However, the performance of our
trained code2vec models is similar to the one reported in [4].

5.4 Population of Transformed Programs
We apply the applicable transformations to the program in the test-
ing data of the above-mentioned datasets. The number of original
programs in our study is 2,088,411 and we used transformation to
create 4,075,949 transformed programs. The types and number of
applicable transformations vary from a program to another. There-
fore, in our approach, different methods, based on the language
features that they use, produce a different number of transformed
programs.

Overall, the number of original programs with incorrect predic-
tions is, on average, 2.4 times higher than the number of programs
with correct predictions. Moreover, programs with incorrect predic-
tions are amenable to, on average, 1.4 times more transformations.
It may suggest that programs with correct predictions are smaller
and simpler. In total, the number of transformed programs from the
program with wrong initial predictions is much higher (3.4x and
higher) than the number of transformed programs from programs
with correct initial predictions.

5.5 Research Questions
In this paper, we seek to answer the following research questions.

RQ1 How do the transformations impact the prediction of neural

program analyzers?

RQ2 When transformations are most effective in modifying the

prediction of the neural program analyzers?

RQ3 How does method length impact the neural program ana-

lyzer‚Äôs generalizability?

6 RESULTS
6.1 RQ1: Impact of Transformation on the

Models

Tables 2‚Äì7 show the changes of prediction for each transforma-
tion in all code2vec and code2seq neural program analyzers that we
trained. Note that since the inputs to code2vec and code2seq are
body of methods, we use terms methods and programs interchange-
ably, in this section. For each transformation, ‚Äú# Original programs‚Äù
denotes the number of programs eligible for the transformation, ‚Äú#
Transformed programs‚Äù denotes the number of transformed pro-
grams. Note that # Transformed programs can be larger than #
Original programs, as a program may have more than one place
where the transformation is applicable. ‚Äú# Prediction changing pro-
grams‚Äù provides raw number of transformed programs that the
prediction of neural program analyzer on the original and trans-
formed program differ. ‚ÄúPrediction change(%)‚Äù denotes the percent-
age of transformed program that changed the output of the neural
program analyzer.

code2vec is most sensitive to Permute Statement transformation
on all datasets. On the other hand, the code2seq is most vulnerable
to Switch to If , Variable Renaming, and Boolean Exchange transfor-
mation on Java-Small, Java-Med, and Java-Large dataset, respec-
tively. Variable Renaming changes the text of terminals that change
the embedding of path-context as well. However, Try-Catch Inser-
tion and Unused Statement add some additional nodes and paths
in AST. If models give less attention to those new paths, then the

Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations

Work-in-progress, ,

Table 2: Change of prediction for code2vec on Java-Small dataset.

Table 3: Change of prediction for code2vec on Java-Med dataset.

Transformation

#

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Try-Catch Insertion

Unused Statement

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

Total

31113
123123
67622
54.92

1158
1519
818
53.85

3699
5160
3064
59.38

246
259
178
68.73

15325
74950
53791
71.77

32078
32078
15039
46.88

44426
44426
17755
39.97

Transformation

#

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Try-Catch Insertion

Unused Statement

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

Total

235961
771208
358984
46.55

6407
8840
4451
50.35

17107
23533
14772
62.77

3312
3839
2300
59.91

88865
366840
232054
63.26

232769
232769
99878
42.91

351621
351621
125880
35.8

change is less effective. The Unused Statement and Try-Catch In-
sertion keep the existing AST mostly intact, consequently, in most
cases, the average percentage of changes in prediction of Unused
Statement and Try-Catch Insertion is comparatively less than other
transformations.

"Total" column in Tables 2‚Äì7 supports that Permute Statement is
more powerful than Variable Renaming in code2vec model whereas
Variable Renaming is more effective than Permute Statement to
code2seq model on all datasets. The real-value embeddings of paths
are different for code2vec and code2seq. In code2vec, an embedding
matrix is initialized randomly for paths and learned during training,
that contains rows that are mapped to each of the AST paths. On
the other hand, in code2seq, each node of path comes from a learned
embedding matrix and then a bi-directional LSTM is used to encode
each of the AST paths separately. The bi-directional LSTM reads
the path once from beginning to the end (as original order) and
once from end to beginning (in reverse order). Therefore, the order
change by Permute Statement becomes less sensitive in code2seq
than code2vec.

Additionally, the Loop Exchange seems more effective than Switch
to If on Java-Med and Java-Large dataset, but turned opposite on
Java-Small dataset, for all models. Moreover, the Boolean Exchange

shows poor performance on Java-Small and Java-Med dataset,
but becomes comparatively more effective on Java-Large dataset,
for all models. Another observation is that almost in all cases, the
percentage of prediction change for all transformations are higher
on Java-Small dataset, and then significantly drops on Java-Med
and Java-Large dataset, respectively.
(cid:27)

(cid:24)

Observation 1: code2vec is most sensitive to Permute State-
ment transformation on all datasets. On the other hand, the
code2seq is most vulnerable to Switch to If , Variable Renaming,
and Boolean Exchange transformation on Java-Small, Java-
Med, and Java-Large dataset, respectively.

(cid:26)

(cid:25)

6.2 RQ2: When Transformations are most

Effective?
Single place transformation vs All place transformation. In
6.2.1
our analysis, thus far, if a program has multiple candidates for a
transformation, say ùëõ candidates, for transformation, we only apply
them one at the time and end up with ùëõ distinct programs. We
call this single-place transformation. Alternatively, we can replace
all the candidate in a single transformation, and end up with one
transformed program for each transformation per program. We call

Work-in-progress, ,

Md. Rafiqul Islam Rabin and Mohammad Amin Alipour

Table 4: Change of prediction for code2vec on Java-Large dataset.

Table 5: Change of prediction for code2seq on Java-Small dataset.

Transformation

#

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Try-Catch Insertion

Unused Statement

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

Total

252725
916565
385466
42.06

8868
12107
5787
47.8

35565
49665
23104
46.52

10478
11165
3386
30.33

98669
428263
243574
56.87

247092
247092
88609
35.86

370927
370927
115781
31.21

Transformation

#

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Try-Catch Insertion

Unused Statement

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

Total

31113
123123
70371
57.16

1158
1519
825
54.31

3699
5160
2711
52.54

246
259
160
61.78

15325
74950
42685
56.95

32078
32078
15490
48.29

44426
44426
20257
45.6

this all-place transformation. In the next experiment, we evaluate
the generalizability of neural program analyzers under all-place
transformation for the following transformations: Loop Exchange,
Variable Renaming, Boolean Exchange, and Switch to If . Note that
the all-place transformation is not applicable to Permute Statement,
Try-Catch Insertion, and Unused Statement transformations, as we
apply the Permute Statement transformation on a pair of statements
and the Try-Catch Insertion and Unused Statement on a randomly
selected statement.

Figure 3 compares the impact of single-place transformation
and all-place transformation on the change of prediction in all
neural program analyzers that we studied. For the code2vec model,
the percentage of prediction change for all-place transformation
is higher than the single-place transformation by a good margin
for all the cases. Similarly, for the code2seq model, the prediction
change percentage for all-place transformation is higher than the
single-place transformation by a good margin except for the case
(Switch to If , Java-Small). After a closer examination of Java-Small
dataset and Switch to If transformation, we observe that the number
of transformed methods for all-place is only 13, which is too low to
provide comparative insight.

(cid:15)

Observation 2: All-place transformation is more likely to in-
duce changes in the predictions than sing-place transforma-
tions.

(cid:14)

(cid:12)

(cid:13)

6.2.2 Correctly predicted methods vs Incorrectly predicted meth-
ods. Figure 4 depicts the changes in prediction after prediction on
correctly vs. incorrectly predicted methods in all neural program
analyzers. In code2vec neural program analyzers, the percentage of
prediction changes after transformation in the correctly predicted
methods ranges from 10.45% to 42.86%, while, in the incorrectly
predicted methods, a larger portion of transformations, 38.18% to
73.25%, change the prediction of code2vec. In code2seq neural pro-
gram analyzers, while the percentages of changes in predictions
after transformation on the correctly predicted methods ranges
from 9.19% to 36.36%, the percentages range from 44.18% to 62.9%
in the incorrectly predicted methods.
(cid:11)

(cid:8)

Observation 3: The changes in prediction happens more fre-
quently in the originally incorrect methods.

(cid:10)

(cid:9)

Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations

Work-in-progress, ,

(a) code2vec

(b) code2seq

Figure 3: Change of prediction after transformation in single-place vs all-place.

(a) code2vec

(b) code2seq

Figure 4: Change of prediction after transformation on correctly vs incorrectly predicted methods.

6.3 RQ3: Impact of Method Length on

Generalizability

An important metric of interest might be the generalizability in
terms of the number of statements in the methods. Figure 5 de-
picts the relation between length of methods and percentage of
prediction changes. In the figure, the ‚ÄúNumber of statements in
method‚Äù denotes the number of executable lines in the body of
methods before transformation. The trend in the figure suggests
that there is a direct correlation between the size of programs and
the percentage of predication changes after transformation. Note
that there are only a handful of programs larger than 500 lines,

therefore their behavior in Figures 5-b and 5-e is an outlier and can
be ignored.

For Java-Small dataset, the prediction change of models for all
transformations is more superior when a method has around 100
statements and there is no method more than 500 statements.

For Java-Med dataset, the prediction change of models for all
transformations is more superior when a method exceeds 100 state-
ments but drops significantly once a method exceeds 500 statements,
except for the code2vec model on Permute Statement transformation
and the code2seq model on Loop Exchange transformation where the
prediction change increases even method exceeds 500 statements.

Work-in-progress, ,

Md. Rafiqul Islam Rabin and Mohammad Amin Alipour

Table 6: Change of prediction for code2seq on Java-Med dataset.

Table 7: Change of prediction for code2seq on Java-Large dataset.

Transformation

#

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Try-Catch Insertion

Unused Statement

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

Total

235961
771208
375939
48.75

6407
8840
3952
44.71

17107
23533
10659
45.29

3312
3839
1597
41.6

88865
366840
170619
46.51

232769
232769
91176
39.17

351621
351621
141511
40.25

For Java-Large dataset, the prediction change of code2vec model
for all transformations is more superior when a method exceeds
100 statements but drops significantly once a method exceeds 500
statements, except for the Permute Statement and Switch to If trans-
formation where the prediction change increases even method
exceeds 500 statements. On the other hand, the prediction change
of code2seq model for all transformations is increasing whether
method exceeds 500 statements.

As shown in Figure 5, most of the cases the models exhibit
notable increases in prediction change for all transformations as
the number of lines in the program increases.
(cid:15)

(cid:12)

Observation 4: There is a direct correlation between the size
of methods and their susceptibility of changes in the prediction
under transformations.

(cid:14)

(cid:13)

Table 8 shows the full breakdown of percentage of changes after
transformations in code2seq (Java-Large) for all transformed pro-
grams. Note that we only refer to the code2seq (Java-Large) model
in this section for its highest F1 score showed in Table 1. In this table,
CCP, CIP, WWSP, WCP, and WWDO respectively denote the per-
centage of correct predictions that stay correct, percentage of cor-
rect predictions that become incorrect, percentage of wrong labels

Transformation

#

Variable Renaming

Boolean Exchange

Loop Exchange

Switch to If

Permute Statement

Try-Catch Insertion

Unused Statement

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

# Original programs
# Transformed programs
# Prediction changing programs
Prediction change(%)

Total

252725
916565
431131
47.04

8868
12107
6227
51.43

35565
49665
21112
42.51

10478
11165
3247
29.08

98669
428263
186411
43.53

247092
247092
87357
35.35

370927
370927
138865
37.44

Table 8: Detailed percentages of changes in code2seq (Java-Large).

Transformation CCP

CIP WWSP WCP WWDP

VN
BX
LX
SF
PS
TC
UN

14.54
10.14
18.87
52.23
14.77
24.17
26.57

4.69
2.92
2.49
5.28
2.7
3.33
5.95

38.42
38.42
38.62
18.68
41.7
40.47
35.99

2.32
2.92
2.3
2.8
2.82
2.1
1.81

40.03
45.59
37.71
20.99
38.0
29.93
29.68

ùê∂ùêº ùëÉ

that stay the same after transformations, percentage of wrong pre-
dictions that become correct, and percentage of wrong predictions
that change to another wrong prediction after the transformation.
ùê∂ùê∂ùëÉ +ùê∂ùêº ùëÉ calculates the percent of cases that the neural program
analyzer‚Äôs prediction has switched from correct to incorrect. In 9%
to 36% (average %18) of cases, the neural program analyzer switches
from a correct prediction to a wrong one in code2seq (Java-Large).
ùëä ùëä ùëÜùëÉ +ùëä ùëä ùê∑ùëÉ +ùëä ùê∂ùëÉ calculates switching from a wrong prediction
to a correct prediction after transformations. Overall, in less than
3% of transformations, this switch happens.

ùëä ùê∂ùëÉ

Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations

Work-in-progress, ,

(a) code2vec (Java-Small)

(b) code2vec (Java-Med)

(c) code2vec (Java-Large)

(d) code2seq (Java-Small)

(e) code2seq (Java-Med)

(f) code2seq (Java-Large)

Figure 5: Change of prediction for the number of statements in method.

(cid:11)

Observation 5: In less than 3% of cases, a transformation
switches from a wrong prediction to correct prediction.

(cid:10)

(cid:8)

(cid:9)

7 DISCUSSION
In this paper, we study the current state of generalizability in two
neural program analyzers. Although limited, it provides interesting
insights. In this section, we first discuss why neural networks have
become a popular, or perhaps the de-facto, tool for processing
programs, and what are the implications of using neural networks
in processing source code.

Neural networks constitute a powerful class of machine learning
models with a large hypothesis class. For instance, a multi-layer
feed-forward network is called a universal approximator, meaning,
it can essentially represent any function [16]. Unlike traditional
learning techniques that require extensive feature engineering and
tuning, deep neural networks facilitate representation learning.
That is that they are capable of performing feature extraction out of
the raw data completely on their own [23]. Given a sufficiently large
dataset, neural networks with adequate capacity can substantially
reduce the burden of feature engineering. Availability of a large
number of code repositories makes data-driven program analysis a
good application for neural networks. However, it is still unknown
if neural networks are the best way to process programs [15] vs.
[20].

Although the large hypothesis class of neural networks and
feature learning make them very appealing to use, the complex

models built by neural networks are still too difficult to understand
and interpret. Therefore as we apply neural networks in program
analysis, we should develop specialized tools and techniques to
enhance its interpretability of neural program analyzers.

7.1 Are we there yet?
Are neural program analyzers ready for widespread use in program
analysis? Our results suggest: not yet. The models that we experi-
mented are brittle to even very small changes in the AST. A correct
prediction of the neural program analyzers in 9.19% to 36.36% cases
could change to an incorrect one. Although substantial progress
has been made in developing neural program analyzers for various
program analysis and processing tasks, the literature lacks tech-
niques to rigorously evaluate the reliability of such techniques. The
recent line of work by Nghi et al. [6] in interpretability of neural
program analyzers, Rabin et al. [28] in testing them, and Yefet et al.
[39] are much needed steps in a right direction.

7.2 Generalizability vs. Robustness
There is a substantial line of work on evaluating the robustness
of neural networks especially in the domain of vision and pattern
recognition [33]. The key insight in such domains is that small, im-
perceptible changes in input should not impact the result of output.
While this observation can be true for domains such as vision, it
might not be directly applicable to the discrete domain of neural
program analyzers, since some minor changes to a program can

Work-in-progress, ,

Md. Rafiqul Islam Rabin and Mohammad Amin Alipour

drastically change the semantic and behavior of the program. Quan-
tifying the imperceptibility of source code is our future research
goal.

7.3 Code Representation
The performance of models used in neural program analyzers, such
as ones used in this study, is relatively low compared to the per-
formance of neural models in domains such as natural language
understanding [30], text classification [22]. To improve their per-
formance, we would need novel code representations that better
capture interesting characteristics of program.

8 RELATED WORK
Robustness of neural networks There is a substantial line of
work on robustness of AI systems in general and deep neural net-
works in particular. Szegedy et al. [33] is the first to discover deep
neural networks are vulnerable to small perturbations that are im-
perceptible to human eyes. They developed the L-BFGS method for
systematic generation of such adversarial examples. Goodfellow
et al. [12] proposes a more efficient method, called Fast Gradient
Sign Method that exploits the linearity of deep neural networks.
Many following up works [7, 10, 21, 26] further demonstrated the
severity of the robustness issues with a variety of attacking methods.
While aforementioned approaches only apply to models for image
classification, new attacks have been proposed that target models
in other domains, such as natural language processing [18, 24, 40]
and graphs [8, 41].

Automated verification research community has proposed tech-
niques to offer guarantees for robustness of neural networks by
adapting bounded model checking [31], abstract interpretation [11],
and Satisfiability Modulo Theory [17].
Models of Code Early works directly adopted NLP models to dis-
cover textual patterns existed in the source code [13, 27]. Those
methods unfortunately do not account for the structural informa-
tion programs exhibit. Following approaches address this issue by
generalizing from the abstract syntax trees [4, 5, 25]. As Graph
Neural Networks (GNN) have been gaining increasing popularity
due to its remarkable representation capacity, many works have
leveraged GNN to tackle challenging tasks like program repair and
bug finding, and obtained quite promising results [3, 9, 38]. In par-
allel, Wang et al.developed a number of models [34, 36, 37] that
feed off the run time information for enhancing the precision of
semantic representation for model inputs.

9 CONCLUSION
In this paper, we perform a large-scale, comprehensive evaluation
on the generalizability of code2vec and code2seq. In particular, we
apply semantics preserving program transformations to produce
new programs on which we expect models to keep their original
predictions. We find that such program transformations frequently
sway the predictions of both models, indicating the serious gener-
alization issues that could negatively impact the wider applications
of deep neural networks in program analysis tasks. We believe our
work can motivate the future research on training not only accurate
but also robust deep models of code.

REFERENCES
[1] Miltiadis Allamanis. 2019. The Adverse Effects of Code Duplication in Machine
Learning Models of Code. In Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software (Onward! 2019). Association for Computing Machinery, New York, NY,
USA, 143‚Äì153. https://doi.org/10.1145/3359591.3359735

[2] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv.
51, 4, Article Article 81 (July 2018), 37 pages. https://doi.org/10.1145/3212695

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2017. Learning
to represent programs with graphs. arXiv preprint arXiv:1711.00740 (2017).
[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-
erating sequences from structured representations of code. arXiv preprint
arXiv:1808.01400 (2018).

[5] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. code2vec: Learn-
ing Distributed Representations of Code. arXiv preprint arXiv:1803.09473 (2018).
[6] N. D. Q. Bui, Y. Yu, and L. Jiang. 2019. AutoFocus: Interpreting Attention-Based
Neural Networks by Code Perturbation. In 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE). 38‚Äì41. https://doi.org/10.
1109/ASE.2019.00014

[7] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and privacy (sp). IEEE,
39‚Äì57.

[8] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song.
2018. Adversarial Attack on Graph Structured Data. Proceedings of the 35th
International Conference on Machine Learning, PMLR 80 (Jul 2018). http://par.nsf.
gov/biblio/10095927

[9] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang.
2020. HOPPITY: LEARNING GRAPH TRANSFORMATIONS TO DETECT AND
FIX BUGS IN PROGRAMS. In International Conference on Learning Representations.
https://openreview.net/forum?id=SJeqs6EFvB

[10] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
Jianguo Li. 2018. Boosting adversarial attacks with momentum. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 9185‚Äì9193.
[11] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of
neural networks with abstract interpretation. In 2018 IEEE Symposium on Security
and Privacy (SP). IEEE, 3‚Äì18.

[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and

harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).

[13] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:

Fixing Common C Language Errors by Deep Learning.

[14] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. 2018.
Deep learning type inference. In Proceedings of the 2018 26th acm joint meeting
on european software engineering conference and symposium on the foundations of
software engineering. 152‚Äì162.

[15] Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are Deep Neural
Networks the Best Choice for Modeling Source Code?. In Proceedings of the
2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE
2017). Association for Computing Machinery, New York, NY, USA, 763‚Äì773.
https://doi.org/10.1145/3106237.3106290

[16] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feed-
forward networks are universal approximators. Neural networks 2, 5 (1989),
359‚Äì366.

[17] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
Verification of Deep Neural Networks. In Computer Aided Verification, Rupak
Majumdar and Viktor Kunƒçak (Eds.). Springer International Publishing, Cham,
3‚Äì29.

[18] Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading

comprehension systems. arXiv preprint arXiv:1707.07328 (2017).

[19] H. J. Kang, T. F. Bissyand√©, and D. Lo. 2019. Assessing the Generalizability of
Code2vec Token Embeddings. In 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE). 1‚Äì12. https://doi.org/10.1109/ASE.2019.
00011

[20] Rafael-Michael Karampatsis and Charles Sutton. 2019. Maybe Deep Neural
Networks are the Best Choice for Modeling Source Code. arXiv:cs.SE/1903.05734
[21] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial machine

learning at scale. arXiv preprint arXiv:1611.01236 (2016).

[22] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent Convolutional
Neural Networks for Text Classification. In Proceedings of the Twenty-Ninth AAAI
Conference on Artificial Intelligence (AAAI‚Äô15). AAAI Press, 2267‚Äì2273.

[23] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature

521, 7553 (2015), 436.

[24] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks

through representation erasure. arXiv preprint arXiv:1612.08220 (2016).

[25] Chris Maddison and Daniel Tarlow. 2014. Structured generative models of natural

source code. In International Conference on Machine Learning. 649‚Äì657.

Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations

Work-in-progress, ,

[26] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
Deepfool: a simple and accurate method to fool deep neural networks. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition. 2574‚Äì2582.
[27] Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay.
2016. sk_p: a neural program corrector for MOOCs. In Companion Proceedings
of the 2016 ACM SIGPLAN International Conference on Systems, Programming,
Languages and Applications: Software for Humanity. ACM, 39‚Äì40.

[28] Md Rafiqul Islam Rabin, Ke Wang, and Mohammad Amin Alipour. 2019. Test-
ing Neural Program Analyzers. In 34th IEEE/ACM International Conference on
Automated Software Engineering (Late Breaking Research-Track).

[29] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should i
trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and data mining.
1135‚Äì1144.

[30] Ruhi Sarikaya, Geoffrey E. Hinton, and Anoop Deoras. 2014. Application of Deep
Belief Networks for Natural Language Understanding. IEEE/ACM Trans. Audio,
Speech and Lang. Proc. 22, 4 (April 2014), 778‚Äì784. https://doi.org/10.1109/TASLP.
2014.2303296

[31] Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. [n.d.].

Towards Verification of Artificial Neural Networks.

[32] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution
for deep networks. In Proceedings of the 34th International Conference on Machine

Learning. 3319‚Äì3328.

[33] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).

[34] Ke Wang. 2019. Learning Scalable and Precise Representation of Program Se-

mantics. arXiv preprint arXiv:1905.05251 (2019).

[35] Ke Wang and Mihai Christodorescu. 2019. COSET: A Benchmark for Evaluating

Neural Program Embeddings. arXiv:cs.LG/1905.11445

[36] Ke Wang, Rishabh Singh, and Zhendong Su. 2017. Dynamic neural program

embedding for program repair. arXiv preprint arXiv:1711.07163 (2017).

[37] Ke Wang and Zhendong Su. 2019. Learning Blended, Precise Semantic Program

Embeddings. arXiv preprint arXiv:1907.02136 (2019).

[38] Yu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang. 2019. Learning a Static

Bug Finder from Data. arXiv preprint arXiv:1907.05579 (2019).

[39] Noam Yefet, Uri Alon, and Eran Yahav. 2019. Adversarial Examples for Models

of Code. arXiv preprint arXiv:1910.07517 (2019).

[40] Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating Natural Ad-
versarial Examples. In International Conference on Learning Representations.
https://openreview.net/forum?id=H1BLjgZCb

[41] Daniel Z√ºgner, Amir Akbarnejad, and Stephan G√ºnnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2847‚Äì2856.

