Double Machine Learning based Program Evaluation
under Unconfoundedness

Michael C. Knaus†

First version: March 9, 2020
This version: June 3, 2022

Abstract

This paper reviews, applies and extends recently proposed methods based on

Double Machine Learning (DML) with a focus on program evaluation under uncon-

foundedness. DML based methods leverage ﬂexible prediction models to adjust for

confounding variables in the estimation of (i) standard average eﬀects, (ii) diﬀerent

forms of heterogeneous eﬀects, and (iii) optimal treatment assignment rules. An

evaluation of multiple programs of the Swiss Active Labour Market Policy illustrates

how DML based methods enable a comprehensive program evaluation. Motivated by

extreme individualised treatment eﬀect estimates of the DR-learner, we propose the

normalised DR-learner (NDR-learner) to address this issue. The NDR-learner ac-

knowledges that individualised eﬀect estimates can be stabilised by an individualised

normalisation of inverse probability weights.

Keywords: Causal machine learning, conditional average treatment eﬀects,

policy learning, individualized treatment rules, multiple treatments, DR-learner

JEL classiﬁcation: C21

2
2
0
2

n
u
J

2

]

M
E
.
n
o
c
e
[

5
v
1
9
1
3
0
.
3
0
0
2
:
v
i
X
r
a

∗Financial support from the Swiss National Science Foundation (SNSF) is gratefully acknowledged (grant
number SNSF 407540_166999). I thank Petyo Bonev, Martin Huber, Edward Kennedy, Michael Lechner,
Vira Semenova, Anthony Strittmatter, Stefan Wager, and Michael Zimmert for helpful comments and
suggestions. The usual disclaimer applies.
†University of St. Gallen. Michael C. Knaus is also aﬃliated with IZA, Bonn, michael.knaus@unisg.ch.

1

 
 
 
 
 
 
1

Introduction

The adaptation of so-called machine learning to causal inference has been a productive

area of methodological research in recent years. The resulting new methods complement

the existing econometric toolbox for program evaluation along at least two dimensions (see

for recent overviews Athey & Imbens, 2017, 2019; Abadie & Cattaneo, 2018). On the one

hand, they provide ﬂexible methods to estimate standard average eﬀects. In particular,

they provide a data-driven approach to variable and model selection in studies that rely

on an unconfoundedness assumption1 for identiﬁcation. On the other hand, they enable a

more comprehensive evaluation by providing new methods for the ﬂexible estimation of

heterogeneous eﬀects and of treatment assignment rules.

This paper considers Double Machine Learning (DML) (Chernozhukov, Chetverikov,

et al., 2018) as a framework for ﬂexible and comprehensive program evaluation. The DML

framework seems attractive because (i) it can be combined with a variety of standard

supervised machine learning methods, (ii) it covers average eﬀects for binary (e.g. Belloni,

Chernozhukov, & Hansen, 2014; Belloni, Chernozhukov, Fernández-Val, & Hansen, 2017;

Chernozhukov, Chetverikov, et al., 2018), multiple (e.g. Farrell, 2015) as well as continuous

treatments (e.g. Kennedy, Ma, McHugh, & Small, 2017; Colangelo & Lee, 2019; Semenova

& Chernozhukov, 2021), (iii) it naturally extends to the estimation of heterogeneous

treatment eﬀects of diﬀerent forms like canonical subgroup eﬀects, the best linear prediction

of eﬀect heterogeneity, or nonparametric eﬀect heterogeneity (e.g Fan, Hsu, Lieli, & Zhang,

2020; Zimmert & Lechner, 2019; Foster & Syrgkanis, 2019; Oprescu, Syrgkanis, & Wu,

2019; Semenova & Chernozhukov, 2021; Kennedy, 2020; Curth & van der Schaar, 2021),

and (iv) it can be used to estimate optimal treatment assignment rules (e.g. Dudik,

Langford, & Li, 2011; Athey & Wager, 2021; Zhou, Athey, & Wager, 2018). All these

DML based methods have favourable statistical properties and allow the use of standard

tools like t-tests, OLS, kernel regression, series regression, or supervised machine learning

for estimating causal parameters of interest after ﬂexibly adjusting for confounding.

This paper starts with a review of DML based methods, then applies these methods

in a standard labour economic setting, and comes back to the methods by proposing the

1Also known as exogeneity, selection on observables, ignorability, or conditional independence assumption.

2

Figure 1: Stylised workﬂow of Double Machine Learning based program evaluation

normalised DR-learner as a potential ﬁx to a ﬁnite sample problem encountered in the

application. Thus, it contributes to the steadily growing literature of causal machine

learning for program evaluation in three ways. First, the review highlights that methods

for diﬀerent parameters build on the same doubly robust score. The construction of this

score might be computationally expensive because it requires the estimation of outcomes

and treatment probabilities via machine learning methods. However, once constructed the

score can be reused to estimate a variety of interesting parameters. This paper focuses on

methods that build on the doubly robust score because they allow to leverage conceptual

and computational synergies. The result is a comprehensive pipeline for program evaluation

within the same framework as Figure 1 illustrates. This is currently not possible with the

variety of more specialised alternatives that integrate machine learning in the estimation

of average treatment eﬀects (e.g. van der Laan & Rubin, 2006; Athey, Imbens, & Wager,

2018; Avagyan & Vansteelandt, 2017; Tan, 2020; Ning, Peng, & Imai, 2020), heterogeneous

treatment eﬀects (e.g. Tian, Alizadeh, Gentles, & Tibshirani, 2014; Athey & Imbens, 2016;

Chernozhukov, Demirer, Duﬂo, & Fernandez-Val, 2017; Wager & Athey, 2018; Athey,

Tibshirani, & Wager, 2019; Künzel, Sekhon, Bickel, & Yu, 2019; Nie & Wager, 2021) and

optimal treatment assignment (e.g. Bansak et al., 2018; Kallus, 2018).

Second, we use DML based methods to provide a comprehensive and computationally

convenient evaluation of four programs of the Swiss Active Labour Market Policy (ALMP)

in a standard dataset (Lechner et al., 2020). The evaluation in this paper illustrates the

potential of DML based methods for program evaluations under unconfoundedness and

provides a potential blueprint for similar analyses. This adds to a small but steadily

growing literature that applies causal machine learning to program evaluation in general

3

PredictedoutcomesPredictedtreatmentprobabilitiesDoublyrobust scoreAverage effectsHeterogeneouseffects:•subgroup•bestlinear prediction•nonparametricOptimal treatmentassignmentrules(e.g. Bertrand, Crépon, Marguerie, & Premand, 2017; Strittmatter, 2018; Gulyas & Pytka,

2019; Knittel & Stolper, 2019; Davis & Heller, 2020; Baiardi & Naghi, 2021; Farbmacher,

Kögel, & Spindler, 2021) and to evaluations based on unconfoundedness in particular

(e.g Kreif & DiazOrdaz, 2019; Cockx, Lechner, & Bollens, 2020; Knaus, Lechner, &

Strittmatter, 2020; Knaus, 2021).

Third, we contribute to the methodological literature on the ﬂexible estimation of

individualised treatment eﬀects (see for a recent overview Knaus, Lechner, & Strittmatter,

2021) by proposing the normalised DR-learner (NDR-learner), which builds on the recent

DR-learner of Kennedy (2020). The application reveals that the plain DR-learner produces

few extreme eﬀect estimates. It turns out that individualised eﬀect estimates can be

stabilised by an individualised normalisation of inverse probability weights. Thus, the

NDR-learner can be considered as a generalisation of the popular Hájek (1971) normalisa-

tion for inverse probability weighting estimators for average eﬀects. The increased stability

comes at the price that the NDR-learner limits the class of permissible machine learning

methods for eﬀect heterogeneity estimation to methods that form predictions as convex

combination of outcomes (e.g. Random Forests).

Overall, we ﬁnd that DML based methods provide a promising set of methods for

program evaluation. The estimated average program eﬀects are in line with the previous

literature. We ﬁnd that computer, vocational and language courses increase employment

in the 31 months after programs start, while the eﬀects of job search trainings are mostly

negative. The heterogeneity analysis additionally reveals substantial heterogeneities by

gender, nationality, previous labour market success and qualiﬁcation. These are picked up

by the estimated optimal assignment rules.

The paper proceeds as follows. Section 2 deﬁnes the estimands of interest and their

identiﬁcation under unconfoundedness. Section 3 reviews DML based methods for esti-

mation and introduces the NDR-learner. Section 4 presents the application. Section 5

describes the implementation of the methods. Section 6 reports the results. Section 7

concludes. The Appendix provides additional explanations and results. The R-package

causalDML implements the applied estimators. An R notebook replicating the analysis is

provided.

4

2 Estimands of interest

2.1 Deﬁnition

We deﬁne the estimands of interest in the multiple treatment version of the potential

outcomes framework (Rubin, 1974; Imbens, 2000; Lechner, 2001). Let W = {0, ..., T }

denote a set of multiple programs and Di(w) = 1(Wi = w) a binary variable indicating

in which program individual i (i = 1, ..., N ) is actually observed.2 We assume that each

individual has a potential outcome Yi(w) for all w ∈ W. Without loss of generality, the

discussion below assumes that higher outcome values are desirable.

The ﬁrst estimand of interest is the average potential outcome (APO), γw = E[Yi(w)].

It answers the question about the average outcome if the whole population was assigned

to program w. However, the more interesting question is usually to compare diﬀerent

programs w and w0. To this end, we take the diﬀerence of the according individual potential

outcomes, Yi(w) − Yi(w0),3 and aggregate them to diﬀerent estimands: First, the average

treatment eﬀect (ATE), δw,w0 = E[Yi(w) − Yi(w0)]. Second, the average treatment eﬀect on

the treated (ATET), θw,w0 = E[Yi(w) − Yi(w0) | Wi = w]. Third, the conditional average

treatment eﬀect (CATE), τw,w0(z) = E[Yi(w) − Yi(w0) | Zi = z], where Zi ∈ Z is a vector

of observed pre-treatment variables.4

The diﬀerent aggregations accommodate the notion that treatment eﬀects might be

heterogeneous. ATE represents the average eﬀect in the population, while ATET shows

it for the subpopulation that is actually observed in program w. Thus, the comparison

of ATE and ATET can be informative about the quality of the program assignment

mechanism. For example, ATET being larger than ATE indicates that the observed

program assignment is better than random.

The ATET is deﬁned by the observed program assignment and thus not subject to

the choice of the researcher. In contrast, the conditioning variables Zi of the CATE are

speciﬁed by the researcher to investigate potentially heterogeneous eﬀects across the groups

2For DML based estimation with continuous treatments see, e.g Kennedy et al. (2017), Colangelo and Lee
(2019), and Semenova and Chernozhukov (2021).
3This would be Yi(1) − Yi(0) in the canonical binary treatment setting.
4We focus in this study on expectations of the individual treatment eﬀects. DML based methods for
quantile treatment eﬀects can be found, e.g. in Belloni et al. (2017) and Kallus, Mao, and Uehara (2019).

5

of individuals that are deﬁned by diﬀerent values of Zi. Such heterogeneous eﬀects can be

indicative for underlying mechanisms. Further, CATEs characterise which groups win and

which lose by how much by receiving program w instead of w0.

The diﬀerent average eﬀects above provide a comprehensive evaluation of programs

under the current program assignment policy. In many applications, however, we want

to conclude the analysis with a recommendation how the assignment policy could be

improved. This can either be done using the evidence on the diﬀerent average eﬀects

deﬁned above or by formally deﬁning the objective of an optimal assignment rule. The

latter is pursued by the literature on statistical treatment rules (e.g. Manski, 2004; Hirano

& Porter, 2009; Stoye, 2009, 2012; Kitagawa & Tetenov, 2018; Athey & Wager, 2021,

and references therein). Here we focus on the case with multiple treatment options as

considered by Zhou et al. (2018).

Let π(Zi) be a policy that assigns individuals to programs according to their charac-

teristics Zi or, put more formally, the function π(Zi) maps observable characteristics to a

program: π : Z → W. In principle, the policy rule can be completely ﬂexible and in the

ideal world we would assign each individual to the program with the highest conditional

APO, E[Yi(w) | Zi = z]. However, in many cases we want to restrict the set of candidate

policy rules denoted by Π to be interpretable for the communication with decision makers

or to incorporate costs or fairness constraints. Each of these candidate policy rules has

a policy value function denoted by Q(π) = E[Yi(π(Zi))] = E [P
w

1(π(Zi) = w)Yi(w)].

Q(π) quantiﬁes the average population outcome if policy rule π would be used to assign

programs. The estimand of interest is then the optimal policy rule π∗ with the highest

value function for the set of candidate policy rules, or formally π∗ = arg maxπ∈Π Q(π).

2.2 Identiﬁcation

The previous section deﬁned the estimands of interest in terms of potential outcomes.

However, each individual is only observed in one program. Thus, only one potential outcome

per individual is observable and the other potential outcomes remain latent. This is the

fundamental problem of causal inference (Holland, 1986) and we need further assumptions

to identify the estimands of interest. In this paper, we consider the unconfoundedness

6

assumption that assumes access to a vector of pre-treatment variables Xi ∈ X containing

Zi such that the following standard assumptions hold (e.g. Imbens & Rubin, 2015):

Assumption 1

(a) Unconfoundedness: Yi(w) ⊥⊥ Wi | Xi = x, ∀ w ∈ W, and x ∈ X .

(b) Common support: 0 < P [Wi = w | Xi = x] ≡ ew(x), ∀ w ∈ W and x ∈ X .

(c) Stable Unit Treatment Value Assumption (SUTVA): Yi = Yi(Wi).

The unconfoundedness assumption requires that Xi contains all confounding variables

that jointly aﬀect program assignment and the outcome. Common support states that it

must be possible to observe each individual in all programs. SUTVA rules out interference.

These assumptions allow the identiﬁcation of the average potential outcome (APO)

conditional on confounders in three common ways:

E[Yi(w) | Xi = x] = E [Yi | Wi = w, Xi = x] ≡ µ(w, x)
" Di(w)Yi
ew(x)

Xi = x

= E

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

"
µ(w, x) +

= E

|

Di(w)(Yi − µ(w, x))
ew(x)

}

{z
≡ Γ(w, x)

(1)

(2)

(3)

#

Xi = x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Equation 1 shows that the conditional APO is identiﬁed as a conditional expectation of

the observed outcome. Equation 2 shows that it is identiﬁed by reweighting the observed

outcome with the inverse treatment probability. Finally, Equation 3 adds the reweighted

outcome residual to the conditional outcome representation of Equation 1. This seems

redundant because we can check that the reweighted residual has expectation zero under

unconfoundedness. However, this identiﬁcation result is doubly robust in the sense that it

still holds if we replace either µ(w, x) or ew(x) in Equation 3 by arbitrary functions of x.5

This doubly robust structure plays a crucial role for the estimation procedures that we

discuss in the next section.

From an identiﬁcation perspective, Γ(w, x) deﬁned in Equation 3 suﬃces to identify

all estimands of interest stated in the previous subsection:

5Appendix A reviews identiﬁcation and identiﬁcation double robustness of Equation 3 for completeness.

7

• APO: γw = E[Yi(w)] = E[Γ(w, Xi)]

• ATE: δw,w0 = E[Yi(w) − Yi(w0)] = E[Γ(w, Xi) − Γ(w0, Xi)]

• ATET: θw,w0 = E[Yi(w) − Yi(w0) | Wi = w] = E[Γ(w, Xi) − Γ(w0, Xi) | Wi = w]

• CATE: τw,w0(z) = E[Yi(w) − Yi(w0) | Zi = z] = E[Γ(w, Xi) − Γ(w0, Xi) | Zi = z]

• Policy value: Q(π) = E[Yi(π(Zi))] = E[P
w

1(π(Zi) = w)Γ(w, Xi)]

• Optimal policy: π∗ = arg maxπ∈Π Q(π) = arg maxπ∈Π E[P

w

1(π(Zi) = w)Γ(w, Xi)]

3 Estimation based on Double Machine Learning

3.1 The doubly robust scores

All Double Machine Learning (DML) based estimators for the estimands of interest build on

the doubly robust scores of Robins, Rotnitzky, and Zhao (1994, 1995) and their Augmented

Inverse Probability Weighting (AIPW) estimator in particular. In the following, large

Greek letters denote the scores corresponding to the small Greek letters used to deﬁne the

estimands in Section 2.1.

The construction of the doubly robust scores requires the input of so-called nuisance

parameters that are usually of secondary interest and considered as tool to eventu-

ally obtain the parameters of interest.

In our case, the two nuisance parameters are

µ(w, x) = E[Yi | Wi = w, Xi = x] and ew(x) = P [Wi = w | Xi = x] for all w. µ(w, x) is

the conditional outcome mean for the subgroup observed in program w. ew(x) is the

conditional probability to be observed in program w, also known as the propensity score.

Usually these functions are unknown and need to be estimated. Following Chernozhukov,

Chetverikov, et al. (2018) they are estimated based on K-fold cross-ﬁtting: (i) randomly

divide the sample in K folds of similar size, (ii) leave out fold k and estimate models for

the nuisance parameters in the remaining K − 1 folds, (iii) use these models to predict

ˆµ−k(w, x) and ˆe−k

w (x) in the left out fold k, and (iv) repeat (i) to (iii) such that each

fold is left out once. This procedure avoids overﬁtting in the sense that no observation

is used to predict its own nuisance parameters. To avoid notational clutter, we ignore

8

the dependence on the speciﬁc fold in the following notation and refer to the cross-ﬁtted

nuisance parameters as ˆµ(w, x) and ˆew(x).

The main building block of the following estimators is the doubly robust score of the

APO, which replaces the true nuisance parameters in Equation 3 by their cross-ﬁtted

predictions:

ˆΓi,w = ˆµ(w, Xi) +

Di(w)(Yi − ˆµ(w, Xi))
ˆew(Xi)

.

(4)

The ATE score for the comparison of treatment w and w0 is then constructed as the

diﬀerence of the respective APO scores:

ˆ∆i,w,w0 = ˆΓi,w − ˆΓi,w0

(5)

The only estimator we consider that uses the same nuisance parameter but plugs them

into a diﬀerent score is the ATET estimator. Although the identiﬁcation result with the

doubly robust APO score in the previous section holds, it is not doubly robust. However,

the doubly robust score for the ATET exists and is deﬁned as

ˆΘi,w,w0 =

Di(w)(Yi − ˆµ(w0, Xi))
ˆew

−

Di(w0)ˆew(Xi)(Yi − ˆµ(w0, Xi))
ˆewˆew0(Xi)

,

(6)

where ˆew = Nw/N is the unconditional treatment probability with Nw counting the

number of individuals observed in program w (see also, e.g. Farrell, 2015).

3.2 Average potential outcomes and treatment eﬀects

The estimation of the APOs, ATEs and ATETs boils down to taking the means of the

previously deﬁned doubly robust scores. For statistical inference, we can rely on standard

one-sample t-tests. Thus, the score’s mean and the variance of this mean are the point

and the variance estimate of the respective estimand of interest:

• APO: ˆµw = N −1 P

i ˆΓi,w and ˆσ2

µw = N −1 P

i(ˆΓi,w − ˆµw)2

• ATE: ˆδw,w0 = N −1 P

i ˆ∆i,w,w0 and ˆσ2

δw,w0 = N −1 P

i( ˆ∆i,w,w0 − ˆδw,w0)2

• ATET: ˆθw,w0 = N −1 P

i ˆΘi,w,w0 and ˆσ2

θw,w0 = N −1 P

i( ˆΘi,w,w0 − ˆθw,w0)2

9

Note that the estimated variances require no adjustment for the fact that we have

estimated the nuisance parameters in a ﬁrst step. The resulting estimators are consistent,

asymptotically normal and semiparametrically eﬃcient under the main assumption that the

estimators of the cross-ﬁtted nuisance parameters are consistent and converge suﬃciently

fast (Belloni et al., 2014; Farrell, 2015; Belloni et al., 2017; Chernozhukov, Chetverikov,

et al., 2018). In particular, the product of the convergence rates of the outcome and

propensity score estimators must be faster than n1/2. This allows to apply machine

learning to estimate the nuisance parameters.6 Flexible machine learning estimators

converge usually slower than the parametric rate n1/2 but several are known to be able to

achieve n1/4 and faster, which would be suﬃcient if both nuisance parameter estimators

achieve it.7

It is well known that estimators using doubly robust scores and parametric models

for the nuisance parameters are doubly robust in the sense that they remain consistent

if one of the parametric models is misspeciﬁed (see, e.g. Glynn & Quinn, 2009). The

diﬀerence of the DML version is that it exploits what Smucler, Rotnitzky, and Robins

(2019) call ’rate double robustness’. This robustness allows to estimate the parameters

of interest at the parametric rate n1/2 even if the nuisance parameters are estimated at

slower rates using machine learning methods that do not require the speciﬁcation of an

actual parametric model.

The rate double robustness is the consequence of the so-called Neyman orthogonality

of the doubly robust score. Neyman orthogonality is at the heart of the general DML

framework of (Chernozhukov, Chetverikov, et al., 2018). Scores with this orthogonality

are immune against small errors in the estimation of nuisance parameters and thus allow

them to be estimated via machine learning. Appendix A.2 revisits what this means in

formal terms.

6Further results, regularity conditions and discussions can be found in section 5.1 of Chernozhukov,
Chetverikov, et al. (2018).
7For example, versions of Lasso (Belloni & Chernozhukov, 2013), Boosting (Luo & Spindler, 2016),
Random Forests (Wager & Walther, 2015; Syrgkanis & Zampetakis, 2020), Neural Nets (Farrell, Liang,
& Misra, 2021), forward model selection (Kozbur, 2020) or ensembles of those can be shown to achieve
the required rates under conditions stated in the original papers.

10

3.3 Conditional average treatment eﬀects

3.3.1 DR-learner

We can reuse the ATE score of Equation 5 to estimate conditional eﬀects. The so-called

DR-learner was introduced for binary treatments but directly translates also to multiple

treatments settings. It exploits that the conditional expectation of the score with known

nuisance parameters equals CATE: τw,w0(z) = E[∆i,w,w0 | Zi = z].8 Thus, a natural way

to estimate CATEs is to use the score with estimated nuisance parameters, ˆ∆i,w,w0, as

pseudo-outcome in a general regression framework:

ˆτ dr(Zi) = arg min

τ

N
X

i=1

(cid:16) ˆ∆i,w,w0 − τ (Zi)

(cid:17)2

(7)

From a conceptual and estimation perspective it is instructive to distinguish two special

cases of CATEs at this point (see also Knaus et al., 2021): (i) Group average treatment

eﬀects (GATE) provide the average eﬀects for pre-speciﬁed, usually low-dimensional,

groups.9 This covers standard subgroup analysis comparing, e.g., eﬀects of men and women,

or heterogeneity along pre-speciﬁed continuous variables like age. (ii) Individualised average

treatment eﬀects (IATEs) aim for the most detailed eﬀect heterogeneity considering all

confounders as heterogeneity variables, i.e. Zi = Xi and thus IAT E(x) = τw,w0(x) =

E[Yi(w) − Yi(w0) | Xi = x].

OLS, series or kernel regressions of the pseudo-outcome on low-dimensional heterogene-

ity variables estimate GATEs. The outputs of such regressions can be interpreted in the

standard way. The only diﬀerence is that instead of modelling the level of an outcome,

they now model the level of a causal eﬀect. Most importantly standard statistical inference

applies as is shown for OLS and series regression by Semenova and Chernozhukov (2021)

as well as for kernel regression by Fan et al. (2020) and Zimmert and Lechner (2019).

Similar to the discussion in the previous section, the Neyman orthogonality of ∆i,w,w0

allows to ignore that nuisance parameters are estimated with ﬂexible methods potentially

8Note that this does not work for the ATET score in Equation 6 and suitable adaptations are beyond the
scope of this paper.
9Note that the GATE is diﬀerent to the Sorted Group Average Treatment Eﬀect (GATES) of Chernozhukov
et al. (2017).

11

converging slower than n1/2 when calculating standard errors. The details about the

required convergence rates are discussed in the referenced papers.

IATEs may be estimated using the pseudo-outcome in supervised machine learning

regressions with the full set of confounders as predictors. As discussed by Chernozhukov

et al. (2017) statistical inference is not yet well understood for low-dimensional Zi and

even harder for high-dimensional Zi when machine learning is used to solve Equation 7.

However, Kennedy (2020) shows that the doubly robust structure of the ATE score results

in favourable bounds on the mean squared error for the estimated IATEs that would not

be attainable by outcome regression or IPW based methods alone.10

We consider two variants of the DR-learner for IATEs. First, we reuse the pseudo-outcome

in one supervised machine learning regression to estimate IATEs in-sample. This full

sample procedure is computationally convenient but prone to overﬁtting. Thus, the second

variant produces out-of-sample IATE predictions for each individual in the sample. Follow-

ing Algorithm 1 of Kennedy (2020), this requires a four-fold cross-ﬁtting scheme that is

detailed in Algorithm 2 of Appendix B. The computational downside of this procedure is

that we cannot reuse the same nuisance parameter predictions as for the average estimator

and need to estimate them for the IATE only. However, the results below suggest that

this computational eﬀort is important to avoid severe overﬁtting.

3.3.2 Normalised DR-learner

Note that the point estimates of the plain DR-learner can be expressed as ˆτ dr(z) =

PN

i=1 αi ˆ∆i,w,w0 if the weight αi that each observation receives can be calculated. For

example, the ATE estimator as special case of the DR-learner with Zi being a constant

uses αi = 1/N , the least squares regression uses αi = z(Z 0Z)−1Z 0 with Z being the stacked

covariate matrix, and the kernel regression uses αi = Kh(Zi−z)

PN

i=1

Kh(Zi−z)

with Kh representing

a proper kernel function. The class of estimators with a known weighted representation

is called linear smoothers (see e.g. Buja, Hastie, & Tibshirani, 1989). Popular machine

learners like tree-based methods (regression trees, Random Forests or boosted trees),

10The Orthogonal Random Forest of Oprescu et al. (2019) is another estimator that is based on the
pseudo-outcome idea and can be asymptotically normal under the assumption of parameteric nuisance
parameters. We focus in this paper on the more general DR-learner. See also Curth and van der Schaar
(2021) for a more nuanced analysis of the DR-learner in comparison to other alternatives.

12

Ridge or any method that runs OLS after variable selection like Post-Lasso (Belloni &

Chernozhukov, 2013) have this structure.11 Also for these methods we know the weight

αi(x) that each observation receives in predicting the (pseudo-)outcome at x. These weights

usually sum up to one, i.e. PN

i=1 αi(x) = 1. Using such outcome weighting predictors in

the ﬁnal step allows to express the DR-learner estimated IATE as

ˆτ drl
w,w0(x) =

=

N
X

i=1
N
X

i=1

αi(x) ˆ∆i,w,w0

αi(x)[ˆµ(w, Xi) − ˆµ(w0, Xi)]

+

N
X

i=1

αi(x)Di(w)
ˆew(Xi)
{z
λw
i (x)

}

|

˜Yi(w, Xi) −

N
X

i=1

αi(x)Di(w0)
ˆew0(Xi)
{z
λw0
i (x)

|

}

˜Yi(w0, Xi),

(8)

where ˜Yi(w, Xi) = Yi − ˆµ(w, Xi) denotes the outcome residual.

The DR-learner shares the problem of all estimators that involve reweighting by the

inverse of the propensity score. In ﬁnite samples, λw

i (x) and λw0

i (x) usually do not sum to

one, i.e. PN

i=1 λw

i (x) 6= 1 and PN

i=1 λw0

i (x) 6= 1. This is especially problematic if it sums

to something much greater than one. In this case the weighted residuals receive much

more weight than the outcome regressions. This might result in implausibly large eﬀect

estimates that even could fall outside of the possible bounds of a given outcome variable

(Kang & Schafer, 2007; Robins, Sued, Lei-Gomez, & Rotnitzky, 2007).12

For average eﬀects the Hájek (1971) normalisation is recommended to stabilise estima-

tors using inverse probability weights (e.g. Imbens, 2004; Lunceford & Davidian, 2004;

Robins et al., 2007; Busso, DiNardo, & McCrary, 2014). However, Equation 8 highlights

that the inverse probability weights become x-speciﬁc and such a one time normalisation

that targets the average eﬀect does not solve the problem for the individualised eﬀect.

This can be problematic as ﬁnite sample imbalances are more likely to occur on the

individualised level. Thus, we propose the normalised DR-learner (NDR-learner) as a

stabilised complement to the DR-learner.

11In practice most of these methods are applied with data-driven selection of tuning parameters, which
makes them strictly speaking non-linear smoothers (Buja et al., 1989). However, this does not aﬀect our
results.

12For bounded outcomes, the eﬀects must lie in the interval [Ymin − Ymax, Ymax − Ymin], with Ymin and

Ymax denoting the minimum and maximum values of the outcome, respectively.

13

The NDR-learner normalises the weighted residuals by the sum of weights:

ˆτ ndrl
w,w0(x) =

N
X

i=1

αi(x)[ˆµ(w, Xi) − ˆµ(w0, Xi)]

+

  N
X

i=1

!−1 N
X

λw
i (x)

i=1

i (x) ˜Yi(w, Xi) −
λw

  N
X

i=1

!−1 N
X

λw0
i (x)

i=1

λw0
i (x) ˜Yi(w0, Xi) (9)

This ensures that the weights of the residuals sum up to one under the condition that

weights αi(x) are non-negative. Thus, methods like Ridge or Post-Lasso with potentially

negative weights might not be applicable.

The NDR-learner is more demanding from a computational point of view because

it requires to calculate the weights αi(x) and the normalisation for each x of interest

(Algorithm 2 in Appendix B provides the details of the implementation). However, the

application below shows that the normalisation deals well with the cases where outcome

residuals receive high weights leading to implausibly large eﬀect estimates. Thus, the

NDR-learner is an interesting alternative to the DR-learner if eﬀect sizes become suspicious.

3.4 Optimal treatment assignment

The APO score of Section 3.1 can also be reused to estimate optimal treatment assignment.

To this end, note that the value function of any policy rule π(Zi) can be estimated as

ˆQ(π) = N −1

N
X

T
X

i=1

w=0

1(π(Zi) = w)ˆΓi,w.

This means each individual contributes the score of the treatment that she is assigned

to under this policy rule. However, we are not necessarily interested in the value function

of some policy rule, but want to estimate the optimal policy rule that maximises this value

function, ˆπ∗ = arg maxπ∈Π

ˆQ(π). This requires to search over all candidate policy rules to

ﬁnd the optimum as there exists no closed form solution.

Example: Consider the case where Zi is a binary covariate and Wi is a binary treatment.

We have four diﬀerent policy rules: treat nobody (π1), treat only those with Zi = 1 (π2),

treat only those with Zi = 0 (π3), or treat everybody (π4). We illustrate this using two

representative observations, i = 1 with Z1 = 0, and i = 2 with Z2 = 1 in Table 1. The

14

columns three to six show the assignments under the four potential assignment rules. For

example, the ﬁrst observation receives no treatment under policy rules π1 and π2, but is

treated under policy rules π3 and π4. To ﬁnd the optimal rule, we compare the means of

the APO scores in the last four columns and pick the policy rule that corresponds to the

largest mean. The number of policy values to compare increases dramatically in settings

with multiple treatments and Zi being a vector of potentially non-binary variables.

Table 1: Example of DML based optimal treatment assignment

i Zi π1 π2 π3 π4

1
2
...

0
1
...

0
0
...

0
1
...

1
0
...

1
1
...

ˆQ(π1)
ˆΓ1,0
ˆΓ2,0
...

ˆQ(π2)
ˆΓ1,0
ˆΓ2,1
...

ˆQ(π3)
ˆΓ1,1
ˆΓ2,0
...

ˆQ(π4)
ˆΓ1,1
ˆΓ2,1
...

We expect that the estimated policy in ﬁnite samples and with estimated nuisance

parameters does not coincide with the true optimal policy rule. This is conceptualised as

the ’regret’ deﬁned as the diﬀerence between the true and the estimated optimal value

function, R(ˆπ∗) = Q(π∗) − Q(ˆπ∗).

Zhou et al. (2018) show that the DML based procedure minimises the maximum regret

asymptotically under two main conditions: First, the same convergence conditions for the

nuisance parameters that are required for ATE estimation (the product of the nuisance

parameter convergence rates achieves n1/2). Second, the set of candidate policy rules Π

is not too complex. In particular, Zhou et al. (2018) show that decision trees with ﬁxed

depth are a suitable class of policy rules. Again the double robustness of the used scores

results in statistical guarantees that are not achievable for methods based on outcome

regressions or IPW alone.

4 Application: Swiss Active Labour Market Policy

We use a standard observational dataset of Swiss Active Labour Market Policy (ALMP)

that is already basis of previous studies (Huber, Lechner, & Mellace, 2017; Lechner, 2018;

15

Table 2: Descriptive statistics of selected variables by program type

No. of observations

Outcome: months employed of 31

Female (binary)

Age

Foreigner (binary)

Employability

Past income in CHF 10,000

No program Job search Vocational Computer Language

(1)

(2)

47,620

11,610

14.7

0.44

36.6

0.36

1.93

4.25

14.4

0.44

37.3

0.33

1.98

4.67

(3)

858

18.4

0.33

37.5

0.30

1.93

4.87

(4)

905

19.2

0.60

39.1

0.21

1.97

4.32

(5)

1504

13.5

0.55

35.3

0.66

1.85

3.73

Note: Employability is an ordered variable with one indicating low employability, two medium
employability and three high employability. The exchange rate USD/CHF was roughly 1.3 at that
time. The full set of variables is reported in Table C.1.

Knaus et al., 2020) to estimate the eﬀect of diﬀerent programs on employment.13

In

particular, we start with the sample of 100,120 unemployed individuals of Huber et al.

(2017) that consists of 24 to 55 year old individuals registered unemployed in 2003.14 We

consider non-participants and participants of four diﬀerent program types: job search,

vocational training, computer programs and language courses.15 As the assignment policies

diﬀer substantially across the three language regions, we focus only on individuals living

in the German speaking part and remove those in the French and Italian speaking part to

avoid common support problems.

We evaluate the ﬁrst program participation within the ﬁrst six months after the begin of

the unemployment spell. One problem of this deﬁnition is that non-participants comprise

people that quickly come back into employment before they would be assigned to a training

program. This could result in an overly optimistic evaluation of non-participation. We

follow Lechner (1999) and Lechner and Smith (2007) and assign pseudo program starting

points to the non-participants and keep only those who are still unemployed at this point.16

This results in a ﬁnal sample size of 62,497 observations.

The outcome of interest is the cumulated number of months in employment in the 31

13Gerﬁn and Lechner (2002), Lalive, van Ours, and Zweimüller (2008) and Knaus et al. (2020) among

others provide a more detailed description of the surrounding institutional setting.

14The dataset is available as restricted use ﬁle via the platform FORSbase (Lechner et al., 2020).
15The dataset contains also participants of an employment program and personality training. However,

we leave them out to keep the number of obtained results manageable.

16The assignment of the pseudo starting point is based on estimated probabilities to start a program at a
speciﬁc time. The probability depends also on covariates and is estimated using the same random forest
speciﬁcation that is discussed later in Section 5.

16

months after program start, which is the maximum available time span in the dataset.

Row one of Table 2 provides the number of observations in each group. Roughly 75%

participate in no program. By far the largest program is the job search program, which

is also called basic program. The more speciﬁc programs are much smaller with roughly

1000 observations each. Row two shows that the average outcomes substantially diﬀer

by diﬀerent groups. However, it is not clear whether this is only due to selection eﬀects

because the observable characteristics are not comparable across groups, as the remaining

rows show. Especially the share of females, the share of foreigners and past income

diﬀer quite substantially across programs. The confounders comprise 45 variables and are

reported in Table C.1 of Appendix C. They consist of socio-economic characteristics of

the unemployed individuals, caseworker characteristics, information about the assignment

process, information about the previous job, and regional economic indicators.

5

Implementation

The nuisance parameters are estimated via Random Forest (Breiman, 2001) using the

implementation with honest splitting in the grf R-package (Athey et al., 2019) and 5-fold

cross-ﬁtting. The tuning parameters in each regression are selected by out-of-bag validation.

All regressions apply the full set of confounders. We run the outcome regressions for each

treatment group separately to obtain ˆµ(w, x). Also the propensity scores are separately

estimated for each treatment using a treatment indicator as outcome in the random forest.

The propensity scores are then normalised to sum to one within an individual.

We estimate CATEs at diﬀerent granularity. First, we investigate GATEs for subgroups

by gender, foreigners and three categories of employability. These are regularly used in

the program evaluation literature and usually investigated by re-estimating everything

in the subgroups. However, it can be performed at very low computational costs after

DML for average eﬀects using only a standard OLS regression with the pseudo-outcome

as described in Section 3.3.1 and using dummy variables for all groups but the reference

group as covariates. Second, we estimate kernel regression and spline regression GATEs

for the continuous variables age and past income based on the R-packages np (Hayﬁeld &

17

Table 3: Steps of implementation

Step

Input

Operation

1.
2.

3.

4.

5.

6.

7.

8.

9.

Predict treatment probabilities
Predict treatment speciﬁc outcomes

Wi, Xi
Yi, Wi, Xi
Yi, Wi, ˆew(x), ˆµ(w, x) Plug into Equation 4
ˆΓi,w
ˆΓi,w
ˆ∆i,w,w0
ˆ∆i,w,w0, Zi
ˆ∆i,w,w0, Xi
ˆΓi,w, Zi

Optimal decision tree

Take diﬀerence

Mean, one-sample t-test

Mean, one-sample t-test

OLS/Kernel/Series regression

Supervised Machine Learning

Output

ˆew(x)
ˆµ(w, x)
ˆΓi,w
APOs
ˆ∆i,w,w0
ATEs

GATEs

IATEs

Optimal treament rule

Racine, 2008) and crs (Racine & Nie, 2021), respectively. The kernel regressions apply a

second-order Gaussian kernel function and use 0.9 of the cross-validated bandwidth for

undersmoothing as suggested by Zimmert and Lechner (2019). The spline regressions use

B-splines with cross-validated degree and number of knots. Third, we specify an OLS

regression in which all the ﬁve previously used variables enter linearly. Finally, we go

beyond the handpicked variables and estimate the IATEs using all 45 confounders in the

DR-learner and the NDR-learner. Both are implemented with the honest Random Forest

because the grf package allows to extract the prediction weights αi(x) required for the

NDR-learner. We apply both variants described in Section 3.3.1. Once we estimate the

IATE for each observation using DR- and NDR-learner in the full sample and once we

predict them out-of-sample. For the latter, Appendix B provides a detailed description of

the underlying DR- and NDR-learner algorithms.

The optimal treatment assignment rule is estimated as decision trees of depth one,

two and three. We follow Algorithm 2 for exact tree-search of Zhou et al. (2018) that is

implemented in the policytree R-package (Sverdrup, Kanodia, Zhou, Athey, & Wager,

2020). We estimate the trees ﬁrst with the ﬁve handpicked variables. However, these

variables include gender and foreigner status that might be too sensitive to include in

practice. Thus, we investigate another set of 16 variables that includes only the objective

measures of education and labour market history of the unemployed persons that would

be available for recommendations from the administrative records.

Table 3 summarises all required implementation steps. It highlights that a comprehen-

18

sive DML based program evaluation can be run with few lines of code in any statistical

software program that is capable of the operations in the third column. Thus, researchers

can build their customised analyses in a modular fashion based on established code. Alter-

natively, the R-package causalDML already implements the required steps as showcased in

the replication notebook accompanying this paper. Most importantly the package provides

a fast implementation of the individualised normalisation required for the NDR-learner in

C++.

6 Results

6.1 Average eﬀects

We focus here on the eﬀect estimates and discuss the nuisance parameters in Appendix

C.2. Throughout this section, we compare the four programs to non-participation.17

Recall that the outcome of interest is the cumulated number of months employed in the

31 months after program start. Figure 2 depicts ATE and ATET estimates and shows

substantial diﬀerences in the eﬀectiveness of programs. The job search program decreases

the months in employment on average by about one month. In contrast, other programs

that teach hard skills show substantial improvements with roughly three additional months

in employment on average.18

Comparing ATE and ATET shows no big diﬀerences for most programs. This suggests

that there is either no eﬀect heterogeneity correlated with observables or that the assignment

does not take advantage of this heterogeneity. We would expect to see ATETs being higher

than ATEs if program assignment is well targeted. However, we ﬁnd only evidence for the

opposite as the actual participants of a language course show a 1.5 months lower treatment

eﬀect compared to the population. This diﬀerence suggests that there is substantial eﬀect

heterogeneity to uncover and the potential to improve treatment assignment.

17The underlying APOs are shown in Figure C.5 of Appendix C.
18For a better understanding of the underlying dynamics, Figure C.3 of Appendix C reports and discusses

the eﬀects of program participation on the employment probabilities over time.

19

Figure 2: Average treatment eﬀects of participation vs. non-participation

Note: The ﬁgure shows the point estimates of the average treatment eﬀects of participating
in the program labeled on the x-axis vs. non-participation and their 95% conﬁdence intervals.
Numeric results in Panels B and C of Table C.5 of Appendix C.

6.2 Heterogeneous eﬀects

6.2.1 Subgroup GATEs

This subsection studies eﬀect heterogeneity at diﬀerent granularity. We start by estimating

group average treatment eﬀects (GATEs) for discrete subgroups. Panel A of Table 4

shows the result of an OLS regression with a female dummy as covariate, ˆ∆i,w,w0 =

β0 + β1f emalei + errori. The constant (β0) provides the GATE for the reference group

men and the female coeﬃcient (β1) describes how much the GATE diﬀers for women.

The results show substantial gender diﬀerences in the eﬀectiveness of programs. Women

signiﬁcantly suﬀer less or proﬁt more from job search and computer program participation.

This gender gap in the eﬀectiveness of ALMPs is also well-documented in the literature

(Crépon & van den Berg, 2016; Card, Kluve, & Weber, 2018). In contrast to this, we ﬁnd

that women proﬁt on average signiﬁcantly less from language courses than men.

Panel B replaces the female dummy in the regression by a foreigner dummy. Strikingly,

Swiss citizens as reference group show a big positive eﬀect for participating in language

courses but the eﬀect disappears for foreigners. After adding the coeﬃcient for foreigners

to the constant, the foreigners’ GATE is only 0.72 (3.56 − 2.84, standard error: 0.69). A

crucial information to better understand this ﬁnding would be to know which languages

20

024Job searchVocationalComputerLanguageAverage treatment effectsEstimandATEATETTable 4: Group average treatment eﬀects

Panel A:
Constant

Female

Panel B:
Constant

Foreigner

Panel C:
Constant

Medium employability

High employability

F-statistic

Job search Vocational Computer Language

(1)

(2)

(3)

(4)

-1.29∗∗∗
(0.17)
0.60∗∗
(0.25)

-1.27∗∗∗
(0.16)
0.70∗∗∗
(0.26)

-0.18
(0.33)
-0.93∗∗∗
(0.36)
-1.50∗∗∗
(0.50)
5.04∗∗∗

3.82∗∗∗
(0.55)

-1.27
(0.87)

2.48∗∗∗
(0.53)
2.17∗∗
(0.90)

5.48∗∗∗
(1.04)
-2.41∗∗
(1.16)
-4.42∗∗∗
(1.52)
4.31∗∗

2.33∗∗∗
(0.60)
2.49∗∗∗
(0.85)

3.75∗∗∗
(0.50)

-0.88
(0.93)

5.76∗∗∗
(1.10)
-2.67∗∗
(1.20)
-3.59∗∗
(1.69)
2.96∗

3.40∗∗∗
(0.46)
-1.97∗∗
(0.77)

3.56∗∗∗
(0.52)
-2.84∗∗∗
(0.72)

2.61∗∗∗
(0.85)

-0.18
(0.96)

0.59
(1.48)

0.18

Note: This table shows OLS coeﬃcients and their heteroscedasticity robust
standard errors (in parentheses) of regressions run with the pseudo-outcome
deﬁned as described in Section 3.3. ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01

they learn.19 However, this information is unfortunately not available in the dataset.

Panel C shows the results of a similar regression but now with two dummies indicating

medium and high employability such that low employability becomes the reference group.

The F-statistic in the last line tests the joint signiﬁcance of the two dummies. It is statisti-

cally signiﬁcant at least at the 10%-level for the programs in the ﬁrst three columns. They

all show a common gradient that individuals with low employability beneﬁt substantially

more or at least suﬀer less from program participation.

6.2.2 Nonparametric GATEs for continuous heterogeneity variables

While subgroup analyses are standard in program evaluations, the estimation of nonpara-

metric GATEs using kernel or series regression is rarely pursued. We estimate such GATEs

along two continuous variables past income and age. We ﬁnd no notable heterogeneity for

the latter.20 However, eﬀect sizes are clearly associated with past income. Figure 3 shows

19See Heiler and Knaus (2021) for a discussion about how treatment heterogeneity could drive eﬀect

heterogeneity.

20Figure C.4 shows the according results.

21

Figure 3: Eﬀect heterogeneity regarding past income

(a) Job search (Kernel)

(b) Job search (Spline)

(c) Vocational (Kernel)

(d) Vocational (Spline)

(e) Computer (Kernel)

(f) Computer (Spline)

(g) Language (Kernel)

(h) Language (Spline)

Note: Dotted line indicates point estimate of the respective average treatment eﬀect. Grey area
shows 95%-conﬁdence interval.

22

−505100250005000075000Past incomeConditional average treatment effect−505100250005000075000Past incomeConditional average treatment effect−505100250005000075000Past incomeConditional average treatment effect−505100250005000075000Past incomeConditional average treatment effect−505100250005000075000Past incomeConditional average treatment effect−505100250005000075000Past incomeConditional average treatment effect−505100250005000075000Past incomeConditional average treatment effect−505100250005000075000Past incomeConditional average treatment effectTable 5: Best linear prediction of CATEs

Constant

Female

Age

Foreigner

Medium employability

High employability

Past income in CHF 10,000

F-statistic

Job search Vocational Computer Language

(1)

-0.48
(0.70)

0.25
(0.27)

0.02
(0.01)
0.50∗
(0.27)
-0.65∗
(0.37)
-1.03∗∗
(0.51)
-0.26∗∗∗
(0.06)

6.95∗∗∗

(2)

4.46∗
(2.40)
-2.12∗∗
(0.92)
0.09∗
(0.05)
1.60∗
(0.90)

-1.64
(1.18)
-3.13∗∗
(1.55)
-0.62∗∗∗
(0.23)

4.12∗∗∗

(3)

4.93∗∗
(2.37)
1.89∗∗
(0.90)

0.05
(0.05)

-1.20
(0.96)
-2.36∗
(1.22)
-3.15∗
(1.71)
-0.39∗∗
(0.19)

(4)

6.07∗∗∗
(2.09)
-1.61∗∗
(0.80)

-0.07
(0.05)
-2.77∗∗∗
(0.74)

-0.78
(0.99)

-0.47
(1.53)
0.31∗
(0.18)

3.35∗∗∗

5.22∗∗∗

Note: This table shows OLS coeﬃcients and their heteroscedasticity robust
standard errors (in parentheses) of regressions run with the pseudo-outcome.
∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01

on the left the results of the kernel regression and on the right of the series regression.

Both estimators agree by and large. They document that eﬀects decrease with higher past

income for all but for language programs. The latter have only a small positive eﬀect

for individuals with low past income but it increases with higher income. One potential

explanation for these ﬁndings is that the value of language skills is larger for high-skilled

workers in multilingual countries like Switzerland because they reduce information costs

across language borders (see, e.g. Isphording, 2014).

6.2.3 Best linear prediction of GATEs

The GATEs considered so far were nonparametric but only univariate. Now we model the

GATE by specifying a multivariate OLS regression with the previously used covariates

entering linearly. It is most likely misspeciﬁed and thus estimates the best linear predictor

(BLP) of GATEs with respect to these variables. However, it provides a compact and

accessible summary of the eﬀect heterogeneities. Additionally, it holds the other included

variables constant. Consider for example the coeﬃcients for being female in Table 5.

Compared to Table 4, the coeﬃcients in the ﬁrst three columns are smaller and the one

23

Figure 4: Boxplot of out-of-sample predicted IATEs by DR- and NDR-learner

Note: The ﬁgure shows the distribution of IATEs for participating in the program labeled on the
x-axis vs. non-participation estimated by the DR-learner (DRL) and the NDR-learner (NDRL).
The dashed line indicates the possible range of the IATE of [-31,31] to illustrate that several
DR-learner estimated IATEs lie outside this bound.

for language courses is larger (for example for job search it is 0.25 instead of 0.60). The

reason is that it represents a partial eﬀect that holds other variables like past income

ﬁxed. The subgroup female coeﬃcient in Table 4 partly picks up that women have lower

past income and that lower income is associated with higher treatment eﬀects for all

but language courses. This example illustrates that the same strategies that are usually

applied to interpret an outcome OLS regression can now be used to interpret the eﬀect

OLS regression.

6.2.4 Individualised average treatment eﬀects

We focus on the results based on the out-of-sample variant of the DR- and NDR-learner

as the full sample variant leads to severe overﬁtting with predicted IATEs ranging from

-265 to 161 that are up to eight times larger than what is possible given that the outcome

is bounded between zero and 31.21 However, Figure 4 shows that the DR-learner produces

impossible eﬀect sizes even out-of-sample, which motivates the proposal of the NDR-learner

as stabilised variant. Figure 4 provides boxplots of the predicted IATEs and shows outliers

21See Appendix C.5 for results and discussion of the full sample.

24

−20020Job searchVocationalComputerLanguageIndividualized average treatment effectDRLNDRLTable 6: Classiﬁcation analysis of IATEs

Past income
Previous job: unskilled worker
Mother tongue other than German, French, Italian
Qualiﬁcation: some degree
Swiss citizen
Fraction of months employed last 2 years
Qualiﬁcation: unskilled

Job search Vocational Computer Language

(1)

-1.32
1.02
0.69
-0.88
-0.66
-1.06
0.81

(2)

-0.84
0.68
0.68
-0.65
-0.60
-0.37
0.41

(3)

-1.17
0.34
0.00
-0.41
0.12
-0.47
0.32

(4)

1.01
-1.24
-1.17
1.15
1.11
0.30
-1.02

Note: Table shows the diﬀerences in means of standardized covariates between the ﬁfth and the ﬁrst
quintile of the respective estimated IATE distribution.

lying below the smallest possible value of -31. However, the descriptive statistics provided

in Table C.6 and the joint and marginal distributions depicted in Figure C.6 document

that besides the outliers the distributions are quite similar and correlate with at least

0.90. Not surprisingly, the impact of normalised weights is much larger for the three

smaller programs and nearly negligible for job search programs. Still, we base the following

discussion for all programs on the more stable results of the NDR-learner.

We conduct a classiﬁcation analysis as proposed by Chernozhukov, Fernandez-Val, and

Luo (2018) to understand which variables are most predictive of eﬀect sizes. To this end,

we split the predicted IATE distributions in quintiles and compare the covariate means of

the observations falling into the ﬁfth and ﬁrst quintile. For comparability, we normalise

all covariates to have mean zero and variance one. Table 6 shows the seven variables

that have at least one absolute diﬀerence between the highest and lowest quintile that

is larger than one standard deviation. For example, we observe that the group with the

highest eﬀects (the ﬁfth quintile) of a job search program has a 1.32 standard deviations

lower past income compared to the lowest IATE group (the ﬁrst quintile). Also the other

variables conﬁrm the patterns that we document already in previous subsections. The

eﬀects of job search, vocational and computer training are higher for unskilled workers

with lower previous labour market success and foreigners, while the opposite holds for

language programs.22

22Table C.7 shows the classiﬁcation analysis for all variables.

25

Figure 5: Optimal treatment assignment decision trees of depth two and three

(a) Depth 1 & 5 covariates

(b) Depth 2 & 5 covariates

(c) Depth 1 & 16 covariates

(d) Depth 2 & 16 covariates

Notes: Optimal assignment rules estimated following the procedure deﬁned in Section 3.4.

6.3 Optimal treatment assignment

The previous section documented substantial heterogeneities in the program eﬀects. To

leverage this heterogeneity for better targeting, we apply the DML based optimal policy

algorithm of Section 3.4. Figure 5a shows the simplest decision tree with only one split for

the ﬁve handpicked covariates. It would allocate men to vocational training and women to

computer courses. This split is probably similar to what we would have suggested given

the evidence presented in Table 4. For a tree of depth two, such an eyeballing approach has

its limits and the algorithmic approach provides a systematic way to arrive at an estimated

optimal decision tree. The depth two tree in Figure 5b splits ﬁrst on past income and then

recommends to send low earning men to vocational and low earning women to computer

training, while high earners (more than CHF 55k) are recommended to participate in

language training. In the absence of the possibility to split on gender, the depth one tree

in Figure 5c splits on past income roughly at the same value where the nonparametric

GATEs of computer and language training intersect in Figure 3.23

23Appendix C.6 provides the trees of depth three.

26

Female <= 0VocationalTrueComputerFalsePast income <= 50260Female <= 0TruePast income <= 54740FalseVocationalComputerComputerLanguagePast income <= 43340ComputerTrueLanguageFalsePrevious job: unskilledworker <= 0Past income <= 43340TruePast income <= 20220FalseComputerLanguageComputerVocationalTable 7: Description of estimated optimal policies

No program Job search Vocational Computer Language

(1)

(2)

(3)

(4)

(5)

Panel A: Percent allocated to program

Depth 1 & 5 variables

Depth 2 & 5 variables

Depth 3 & 5 variables

Depth 1 & 16 variables

Depth 2 & 16 variables

Depth 3 & 16 variables

0

0

0

0

0

0

0

0

0

0

0

0

Panel B: Cross-validated diﬀerence to APOs

Depth 1 & 5 variables

Depth 2 & 5 variables

Depth 3 & 5 variables

Depth 1 & 16 variables

Depth 2 & 16 variables

Depth 3 & 16 variables

3.75∗∗∗
(0.40)
3.99∗∗∗
(0.40)
3.50∗∗∗
(0.41)
3.72∗∗∗
(0.41)
3.63∗∗∗
(0.43)
3.94∗∗∗
(0.42)

4.77∗∗∗
(0.42)
5.01∗∗∗
(0.42)
4.52∗∗∗
(0.43)
4.73∗∗∗
(0.42)
4.65∗∗∗
(0.44)
4.96∗∗∗
(0.43)

56

32

47

0

23

42

0.49
(0.44)

0.73
(0.48)

0.24
(0.45)

0.46
(0.52)

0.37
(0.47)

0.68
(0.47)

44

43

37

54

37

30

0.32
(0.48)

0.56
(0.46)

0.07
(0.49)

0.29
(0.48)

0.20
(0.53)

0.51
(0.49)

0

25

17

46

40

27

1.21∗∗
(0.49)
1.45∗∗∗
(0.46)
0.96∗∗
(0.47)
1.19∗∗∗
(0.40)
1.10∗∗∗
(0.42)
1.41∗∗∗
(0.46)

Note: Panel A shows the percentage of individuals being assigned to a speciﬁc program.
Panel B shows a t-test of the diﬀerence of the cross-validated policy (standard errors in
parentheses) and the APOs of the programs. ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01

Panel A of Table 7 summarises the results of the diﬀerent trees. It shows the percentage

of individuals that are placed in the diﬀerent programs. Not surprisingly, all individuals are

recommended to be placed into one of the three positively evaluated hard skill enhancing

programs.

One yet unsolved challenge is how to conduct statistical inference about the quality and

stability of the decision trees. Athey and Wager (2021) propose a form of cross-validation.

To this end, we take the same folds that were used in the cross-ﬁtting procedure to estimate

the nuisance parameters. We build the decision tree in four folds and evaluate the value

in the left out fold. First, we inspect how often the recommendations based on these trees

coincide with the full sample policy rules. Figures C.8 and C.10 of Appendix C.6 show

that the cross-validated trees are not identical to the full sample ones.

Zhou et al. (2018) propose another validation idea and test whether the optimal policy

27

rules perform signiﬁcantly better than sending all individuals to the same program. This

is achieved by taking the diﬀerence of the APO score of the cross-validated policy rule and

the APO score of the program w: ˆ∆cv

i,w(π) = PT

t=0

1(ˆπcv(Zi) = t)ˆΓi,t − ˆΓi,w, where ˆπcv(Zi)

is the policy rule that is estimated without individual i. A standard t-test on the mean of

ˆ∆cv

i,w(π) tests then whether the cross-validated policy rules are signiﬁcantly better than

sending everybody to the same program. Note that the cross-validated policy rules do not

necessarily coincide with the trees in the full sample and the cross-validation estimates

not the value function for that speciﬁc tree. This requires to hold out a test set, which

would be viable for an application with bigger programs.

The results are provided in Panel B of Table 7. We can interpret the mean of ˆ∆cv

i,w(π)

as average treatment eﬀect comparing a regime under the estimated assignment rule or a

regime where everybody is sent to the program w. This eﬀect is always positive indicating

that the estimated rules can leverage the eﬀect heterogeneities to improve the allocation.

However, the cross-validated policy rules perform not signiﬁcantly better than sending just

everybody into vocational or computer programs. This would probably change if we could

take costs or capacity constraints into account. However, we do not observe costs in this

dataset and the optimal decision tree algorithm is currently not capable of incorporating

capacity constraints in a systematic way. We leave both extensions for future research

using a more detailed database on both costs and capacity constraints.

7 Discussion and conclusions

This paper considers recent methodological developments based on Double Machine Learn-

ing (DML) through the lens of a standard program evaluation under unconfoundedness.

DML based methods provide a convenient toolbox for a comprehensive program evaluation

as diﬀerent parameters of interest can be estimated using the same framework and a

combination of standard statistical software. The application to an Active Labour Market

Policy evaluation shows that the methods also produce plausible results in practice. The

only exception is the DR-learner that required a modiﬁcation, the newly introduced

NDR-learner, before producing stable results for all individualised treatment eﬀects. How-

28

ever, several conceptual and implementational issues remain open for investigation and

reﬁnement.

In general, we know little about how to choose the estimator for the nuisance parame-

ters. The pool of potential machine learning algorithms and their combinations is large

and little is known, e.g., about the trade-oﬀ between high prediction performance and

computation time in the causal setting. Also clear recommendations for the implemen-

tation of cross-ﬁtting are missing. Another open question is how to deal with common

support in general and for each estimand speciﬁcally. The literature on trimming rules is

well developed for propensity score based methods estimating average eﬀects. However, we

are not only interested in average eﬀects and the propensity score is not the only nuisance

parameter of DML. It remains an open question whether the established trimming methods

are also sensible for DML when common support becomes an issue.

The estimators for ﬂexible heterogeneous treatment eﬀects provide interesting new tools.

However, it is currently not clear to what extent we can actually explore heterogeneity

or to what extent we need to pre-deﬁne the heterogeneity of interest. The possibility to

summarise pre-deﬁned heterogeneity of interest using OLS, kernel or series regressions

provide clearly valuable and easy to use options in applications. The instability of methods

that aim for individualised heterogeneous eﬀects shows that they should be used with

caution and more research is required to investigate whether adjustments like the proposed

NDR-learner are useful beyond the application of this paper.

The estimation of optimal treatment assignment rules is mostly unexplored in practice

and many interesting issues in applications regarding inference, the implementation of

diﬀerent constraints, more ﬂexible rules than decision trees, or the choice of variables

that could or should enter the set of policy variables, which could be explored in future

research.

The investigation of these DML speciﬁc questions but also the comparison with other

more specialised causal machine learning methods for each estimand provides another

interesting direction of future research. Such evidence would help to understand and guide

which choices are critical in applications similar to the one in this paper.

29

References

Abadie, A., & Cattaneo, M. D. (2018). Econometric methods for program evaluation.

Annual Review of Economics, 10 , 465–503.

Athey, S., & Imbens, G. (2019). Machine learning methods that economists should know

about. Annual Review of Economics, 11 , 685–725.

Athey, S., & Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal eﬀects.

Proceedings of the National Academy of Sciences, 113 (27), 7353–7360.

Athey, S., & Imbens, G. W. (2017). The state of applied econometrics: causality and

policy evaluation. Journal of Economic Perspectives, 31 (2), 3–32.

Athey, S., Imbens, G. W., & Wager, S. (2018). Approximate residual balancing: Debiased

inference of average treatment eﬀects in high dimensions. Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 80 (4), 597–632.

Athey, S., Tibshirani, J., & Wager, S. (2019). Generalized random forests. Annals of

Statistics, 47 (2), 1148 - 1178.

Athey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica,

89 (1), 133–161.

Avagyan, V., & Vansteelandt, S. (2017). Honest data-adaptive inference for the av-

erage treatment eﬀect under model misspeciﬁcation using penalised bias-reduced

double-robust estimation. arXiv:1708.03787 . Retrieved from http://arxiv.org/

abs/1708.03787

Baiardi, A., & Naghi, A. A. (2021). The value added of machine learning to causal

inference: Evidence from revisited studies. arXiv:2101.00878 .

Bansak, K., Ferwerda, J., Hainmueller, J., Dillon, A., Hangartner, D., Lawrence, D., &

Weinstein, J. (2018). Improving refugee integration through data-driven algorithmic

assignment. Science, 359 (6373), 325–329.

Belloni, A., & Chernozhukov, V.

(2013). Least squares after model selection in

high-dimensional sparse models. Bernoulli, 19 (2), 521–547.

Belloni, A., Chernozhukov, V., Fernández-Val, I., & Hansen, C. (2017). Program evaluation

and causal inference with high-dimensional data. Econometrica, 85 (1), 233–298.

30

Belloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment eﬀects

after selection among high-dimensional controls. Review of Economic Studies, 81 (2),

608–650.

Bertrand, M., Crépon, B., Marguerie, A., & Premand, P. (2017). Contemporaneous

and post-program impacts of a public works program: Evidence from Côte d’Ivoire.

World Bank Working Paper.

Breiman, L. (2001). Random forests. Machine Learning, 45 (1), 5–32.

Buja, A., Hastie, T., & Tibshirani, R. (1989). Linear smoothers and additive models. The

Annals of Statistics, 17 (2), 453–510.

Busso, M., DiNardo, J., & McCrary, J. (2014). New evidence on the ﬁnite sample properties

of propensity score reweighting and matching estimators. Review of Economics and

Statistics, 96 (5), 885–897.

Card, D., Kluve, J., & Weber, A. (2018). What works? A meta analysis of recent active

labor market program evaluations. Journal of the European Economic Association,

16 (3), 894–931.

Chernozhukov, V., Chetverikov, D., Demirer, M., Duﬂo, E., Hansen, C., Newey, W., &

Robins, J. (2018). Double/Debiased machine learning for treatment and structural

parameters. The Econometrics Journal, 21 (1), C1-C68.

Chernozhukov, V., Demirer, M., Duﬂo, E., & Fernandez-Val, I. (2017). Generic machine

learning inference on heterogenous treatment eﬀects in randomized experiments.

arXiv:1712.04802 . Retrieved from http://arxiv.org/abs/1712.04802

Chernozhukov, V., Fernandez-Val, I., & Luo, Y. (2018). The sorted eﬀects method:

Discovering heterogeneous eﬀects beyond their averages. Econometrica, 86 (6),

1911–1938.

Cockx, B., Lechner, M., & Bollens, J. (2020). Priority to unemployed immigrants? A

causal machine learning evaluation of training in Belgium. CEPR Discussion Paper

No. DP14270 .

Colangelo, K., & Lee, Y.-Y. (2019). Double debiased machine learning nonparametric

inference with continuous treatments. arXiv:2004.03036 .

Crépon, B., & van den Berg, G. J. (2016). Active labor market policies. Annual Review

31

of Economics, 8 , 521–546.

Curth, A., & van der Schaar, M. (2021). Nonparametric estimation of heterogeneous

treatment eﬀects: From theory to learning algorithms. In Proceedings of the 24th inter-

national conference on artiﬁcial intelligence and statistics (Vol. 130, pp. 1810–1818).

PMLR.

Davis, J. M. V., & Heller, S. B. (2020). Rethinking the beneﬁts of youth employment

programs: The heterogeneous eﬀects of summer jobs. The Review of Economics and

Statistics, 102 (4), 664–677.

Dudik, M., Langford, J., & Li, L. (2011). Doubly robust policy evaluation and learning.

arXiv:1103.4601 . Retrieved from http://arxiv.org/abs/1103.4601

Fan, Q., Hsu, Y.-C., Lieli, R. P., & Zhang, Y. (2020). Estimation of conditional average

treatment eﬀects with high-dimensional data. Journal of Business & Economic

Statistics, published ahead of print 14 September 2020.

Farbmacher, H., Kögel, H., & Spindler, M. (2021). Heterogeneous eﬀects of poverty on

cognition. Labour Economics, 71 , 102028.

Farrell, M. H. (2015). Robust inference on average treatment eﬀects with possibly more

covariates than observations. Journal of Econometrics, 189 (1), 1–23.

Farrell, M. H., Liang, T., & Misra, S. (2021). Deep neural networks for estimation and

inference. Econometrica, 89 (1), 181–213.

Foster, D. J., & Syrgkanis, V. (2019). Orthogonal statistical learning. arXiv:1901.09036 .

Retrieved from http://arxiv.org/abs/1901.09036

Gerﬁn, M., & Lechner, M. (2002). A microeconometric evaluation of the active labour

market policy in Switzerland. Economic Journal, 112 (482), 854–893.

Glynn, A. N., & Quinn, K. M. (2009). An introduction to the augmented inverse propensity

weighted estimator. Political Analysis, 18 (1), 36–56.

Gulyas, A., & Pytka, K. (2019). Understanding the sources of earnings losses after job

displacement: A machine-learning approach. Discussion Paper Series – CRC TR

224 No. 131 .

Hájek, J. (1971). Comment on “An essay on the logical foundations of survey sampling,

part one”. In V. P. Godambe & D. A. Sprott (Eds.), Foundations of statistical

32

inference (p. 236). Toronto: Holt, Rinehart and Winston.

Hayﬁeld, T., & Racine, J. S. (2008). Nonparametric econometrics: The np package.

Journal of Statistical Software, 27 (5).

Heiler, P., & Knaus, M. C. (2021). Eﬀect or treatment heterogeneity? Policy evaluation

with aggregated and disaggregated treatments. arXiv:2110.01427 . Retrieved from

http://arxiv.org/abs/2110.01427

Hirano, K., & Porter, J. R. (2009). Asymptotics for statistical treatment rules. Economet-

rica, 77 (5), 1683–1701.

Holland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical

Association, 81 (396), 945–960.

Huber, M., Lechner, M., & Mellace, G. (2017). Why do tougher caseworkers increase

employment? The role of program assignment as a causal mechanism. Review of

Economics and Statistics, 99 (1), 180–183.

Imbens, G. W. (2000). The role of the propensity score in estimating dose-response

functions. Biometrika, 87 (3), 706–710.

Imbens, G. W. (2004). Nonparametric estimation of average treatment eﬀects under

exogeneity: A review. Review of Economics and Statistics, 86 (1), 4–29.

Imbens, G. W., & Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical

sciences. Cambridge University Press.

Isphording, I. E. (2014). Language and labor market success. IZA Discussion Papers No.

8572 (8572).

Kallus, N. (2018). Balanced policy evaluation and learning.

In Advances in neural

information processing systems (pp. 8895–8906).

Kallus, N., Mao, X., & Uehara, M. (2019). Localized Debiased Machine Learning: Eﬃcient

Estimation of Quantile Treatment Eﬀects, Conditional Value at Risk, and Beyond.

arXiv:1912.12945 . Retrieved from http://arxiv.org/abs/1912.12945

Kang, J. D. Y., & Schafer, J. L. (2007). Demystifying double robustness: A comparison

of alternative strategies for estimating a population mean from incomplete data.

Statistical Science, 22 (4), 523–539.

Kennedy, E. H. (2020). Optimal doubly robust estimation of heterogeneous causal eﬀects.

33

arXiv:2004.14497 . Retrieved from http://arxiv.org/abs/2004.14497

Kennedy, E. H., Ma, Z., McHugh, M. D., & Small, D. S. (2017). Non-parametric methods

for doubly robust estimation of continuous treatment eﬀects. Journal of the Royal

Statistical Society: Series B (Statistical Methodology), 79 , 1229–1245.

Kitagawa, T., & Tetenov, A. (2018). Who should be treated? Empirical welfare maxi-

mization methods for treatment choice. Econometrica, 86 (2), 591–616.

Knaus, M. C. (2021). A double machine learning approach to estimate the eﬀects of

musical practice on student’s skills. Journal of the Royal Statistical Society. Series

A: Statistics in Society, 184 (1), 282–300.

Knaus, M. C., Lechner, M., & Strittmatter, A.

(2020). Heterogeneous employment

eﬀects of job search programmes: A machine learning approach. Journal of Human

Resources, 0718-9615R, published ahead of print 26 March 2020.

Knaus, M. C., Lechner, M., & Strittmatter, A. (2021). Machine Learning Estimation of

Heterogeneous Causal Eﬀects: Empirical Monte Carlo Evidence. The Econometrics

Journal, 24 (1), 134–161.

Knittel, C. R., & Stolper, S. (2019). Using machine learning to target treatment: The

case of household energy use. NBER Working Paper No. 26531 .

Kozbur, D. (2020). Analysis of testing-based forward model selection. Econometrica,

88 (5), 2147–2173.

Kreif, N., & DiazOrdaz, K. (2019). Machine learning in policy evaluation: new tools

for causal inference. arXiv:1903.00402 . Retrieved from http://arxiv.org/abs/

1903.00402

Künzel, S. R., Sekhon, J. S., Bickel, P. J., & Yu, B. (2019). Metalearners for estimating

heterogeneous treatment eﬀects using machine learning. Proceedings of the National

Academy of Sciences, 116 (10), 4156–4165.

Lalive, R., van Ours, J., & Zweimüller, J. (2008). The impact of active labor market

programs on the duration of unemployment. Economic Journal, 118 (525), 235–257.

Lechner, M. (1999). Earnings and employment eﬀects of continuous gﬀ-the-job training in

East Germany after uniﬁcation. Journal of Business & Economic Statistics, 17 (1),

74–90.

34

Lechner, M. (2001). Identiﬁcation and estimation of causal eﬀects of multiple treatments

under the conditional independence assumption. In M. Lechner & E. Pfeiﬀer (Eds.),

Econometric evaluation of labour market policies (pp. 43–58). Heidelberg: Physica.

Lechner, M. (2018). Modiﬁed causal forests for estimating heterogeneous causal eﬀects.

arXiv:1812.09487 . Retrieved from https://arxiv.org/abs/1812.09487

Lechner, M., Knaus, M., Huber, M., Frölich, M., Behncke, S., Mellace, G., & Strittmatter,

A. (2020). Swiss Active Labor Market Policy Evaluation [Dataset]. Distributed by

FORS, Lausanne. Retrieved from https://doi.org/10.23662/FORS-DS-1203-1

Lechner, M., & Smith, J. (2007). What is the value added by caseworkers? Labour

Economics, 14 (2), 135–151.

Lunceford, J. K., & Davidian, M. (2004). Stratiﬁcation and weighting via the propensity

score in estimation of causal treatment eﬀects: A comparative study. Statistics in

Medicine, 23 (19), 2937–2960.

Luo, Y., & Spindler, M. (2016). High-dimensional L2-boosting: Rate of Convergence.

arXiv:1602.08927 . Retrieved from http://arxiv.org/abs/1602.08927

Manski, C. F. (2004). Statistical treatment rules for heterogeneous populations. Econo-

metrica, 72 (4), 1221–1246.

Nie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment eﬀects.

Biometrika, 108 (2), 299–319.

Ning, Y., Peng, S., & Imai, K.

(2020). Robust estimation of causal eﬀects via

high-dimensional covariate balancing propensity score. Biometrika, 107 (3), 533–554.

Oprescu, M., Syrgkanis, V., & Wu, Z. S. (2019). Orthogonal random forest for causal infer-

ence. 36th International Conference on Machine Learning, ICML 2019 , 2019-June,

8655–8696.

Racine, J. S., & Nie, Z. (2021). crs: Categorical Regression Splines. Retrieved from

https://cran.r-project.org/package=crs

Robins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coeﬃcients

when some regressors are not always observed. Journal of the American Statistical

Association, 89 (427), 846–866.

Robins, J. M., Rotnitzky, A., & Zhao, L. P. (1995). Analysis of semiparametric regression

35

models for repeated outcomes in the presence of missing data. Journal of the

American Statistical Association, 90 (429), 106–121.

Robins, J. M., Sued, M., Lei-Gomez, Q., & Rotnitzky, A. (2007). Comment: Performance

of double-robust estimators when "inverse probability" weights are highly variable.

Statistical Science, 22 (4), 544–559.

Rubin, D. B. (1974). Estimating causal eﬀects of treatments in randomized and nonran-

domized studies. Journal of Educational Psychology, 66 (5), 688–701.

Semenova, V., & Chernozhukov, V. (2021). Debiased machine learning of conditional

average treatment eﬀects and other causal functions. The Econometrics Journal,

24 (2), 264–289.

Smucler, E., Rotnitzky, A., & Robins, J. M. (2019). A unifying approach for doubly-robust

L1 regularized estimation of causal contrasts. arXiv:1904.03737 . Retrieved from

http://arxiv.org/abs/1904.03737

Stoye, J.

(2009). Minimax regret treatment choice with ﬁnite samples. Journal of

Econometrics, 151 (1), 70–81.

Stoye, J. (2012). Minimax regret treatment choice with covariates or with limited validity

of experiments. Journal of Econometrics, 166 (1), 138–156.

Strittmatter, A. (2018). What is the value added by using causal machine learning

methods in a welfare experiment evaluation? arXiv:1812.06533 . Retrieved from

http://arxiv.org/abs/1812.06533

Sverdrup, E., Kanodia, A., Zhou, Z., Athey, S., & Wager, S. (2020). policytree: Policy

learning via doubly robust empirical welfare maximization over trees. Journal of

Open Source Software, 5 (50), 2232.

Syrgkanis, V., & Zampetakis, M. (2020). Estimation and inference with trees and forests

in high dimensions. In Conference on learning theory (pp. 3453–3454). PMLR.

Retrieved from http://arxiv.org/abs/2007.03210

Tan, Z. (2020). Model-assisted inference for treatment eﬀects using regularized calibrated

estimation with high-dimensional data. The Annals of Statistics, 48 (2), 811–837.

Tian, L., Alizadeh, A. A., Gentles, A. J., & Tibshirani, R. (2014). A simple method

for estimating interactions between a treatment and a large number of covariates.

36

Journal of the American Statistical Association, 109 (508), 1517–1532.

van der Laan, M. J., & Rubin, D.

(2006). Targeted maximum likelihood learning.

International Journal of Biostatistics, 2 (1).

Wager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment eﬀects

using random forests. Journal of the American Statistical Association, 113 (523),

1228–1242.

Wager, S., & Walther, G.

(2015). Adaptive concentration of regression trees, with

application to random forests. arXiv:1503.06388 . Retrieved from http://arxiv

.org/abs/1503.06388

Wunsch, C. (2016). How to minimize lock-in eﬀects of programs for unemployed workers.

IZA World of Labor.

Zhou, Z., Athey, S., & Wager, S. (2018). Oﬄine multi-action policy learning: Generalization

and optimization. arXiv:1810.04778 . Retrieved from http://arxiv.org/abs/

1810.04778

Zimmert, M., & Lechner, M. (2019). Nonparametric estimation of causal heterogeneity

under high-dimensional confounding. arXiv:1908.08779 . Retrieved from http://

arxiv.org/abs/1908.08779

37

Appendices

A Identiﬁcation and Neyman orthogonality

A.1 Doubly robust identiﬁcation

To revisit identiﬁcation and identiﬁcation double robustness of Equation 2.3 under

Assumption 2.1, rewrite the conditional average potential outcome in the following

way, where µw(x) = E[Yi(w) | Xi = x], µ(w, x) = E[Yi

| Wi = w, Xi = x] and

ew(x) = E [Di(w)|Xi = x] = P [Wi = w|Xi = x]

1b
> 0:

µw(x) = E

"
µ(w, x) +

Di(w)(Yi − µ(w, x))
ew(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

Xi = x

"
Yi(w) − Yi(w) + µ(w, x) +

= E

"

1c
= E

Yi(w) − Yi(w) + µ(w, x) +

Di(w)(Yi − µ(w, x))
ew(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

Xi = x

Di(w)(Yi(w) − µ(w, x))
ew(x)

#

Xi = x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= E [Yi(w) | Xi = x] + E

(Yi(w) − µ(w, x))

"

"

= µw(x) + E

(Yi(w) − µ(w, x))

  Di(w) − ew(x)
ew(x)

  Di(w) − ew(x)
ew(x)
!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi = x

#

#

Xi = x

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(10)

The conditional average potential outcome is thus identiﬁed if the second part of

Equation 10 equals zero. This happens under three scenarios:

1. Correct propensity score and correct outcome regression:

"
(Yi(w) − µ(w, x))

E

 Di(w) − ew(x)
ew(x)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

Xi = x

1a
= E [(Yi(w) − µ(w, x))|Xi = x] E

" Di(w) − ew(x)
ew(x)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

Xi = x

= (E [Yi(w) | Xi = x] − µ(w, x))

  E [Di(w) | Xi = x] − ew(x)
ew(x)

!

= (µw(x) − µ(w, x))

  ew(x) − ew(x)
ew(x)

!

1a
= (µw(x) − µw(x))
{z
}
=0

|

 ew(x) − ew(x)
ew(x)
{z
=0

|

!

}

= 0

38

2. Correct propensity score but instead of correct outcome regression µ(w, x), use some

function g(x):

"

E

(Yi(w) − g(x))

 Di(w) − ew(x)
ew(x)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

Xi = x

1a
= E [(Yi(w) − g(x))|Xi = x] E

" Di(w) − ew(x)
ew(x)

#

Xi = x

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (E [Yi(w) | Xi = x] − g(x))

  E [Di(w) | Xi = x] − ew(x)
ew(x)

!

= (µw(x) − g(x))

 ew(x) − ew(x)
ew(x)
{z
=0

|

!

}

= 0

3. Correct outcome regression but instead of correct propensity score ew(x), use some

function h(x):

"

E

(Yi(w) − µ(w, x))

  Di(w) − h(x)
h(x)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

Xi = x

1a
= E [(Yi(w) − µ(w, x))|Xi = x] E

"  Di(w) − h(x)
h(x)

#

Xi = x

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (E [Yi(w) | Xi = x] − µ(w, x))

  E [Di(w) | Xi = x] − h(x)
h(x))

!

= (µw(x) − µ(w, x))

  ew(x) − h(x)
h(x)

!

1a
= (µw(x) − µw(x))
{z
}
=0

|

  ew(x) − h(x)
h(x)

!

= 0

A.2 Neyman orthogonality

We revisit Neyman orthogonality of the APO score as the other scores follow similarly.

The score deﬁning the APO is


 µ(w, Xi) +

E

|

Di(w)(Yi − µ(w, Xi))
ew(Xi)
{z
ψ(Yi,Wi,µ(w,Xi),ew(Xi))


 = 0

− γw

}

⇒ γw = E


µ(w, Xi) +

Di(w)(Yi − µ(w, Xi))
ew(Xi)





39

(11)

(12)

Neyman-orthogonality of a score ψ(·) means that the Gateaux derivative with respect

to the nuisance parameters is zero in expectation at the true nuisance parameters (NP).

In our case this means that

∂rE[ψ(Yi, Wi, µ + r(˜µ − µ), e + r(˜e − e))|Xi = x]|r=0 = 0

(13)

where we suppress the dependencies of NPs and denote by, e.g., ˜µ a value of the outcome

nuisance that is diﬀerent to the true value µ. We can show that Equation 13 holds with

the following steps:

First, add perturbations to the true nuisance parameters in the score

ψ(Yi, Wi, µ + r(˜µ − µ), e + r(˜e − e))

= (µ + r(˜µ − µ)) +

Di(w)Yi
e + r(˜e − e)

−

Di(w)(µ + r(˜µ − µ))
e + r(˜e − e)

− γw

Second, take the conditional expectation

E [ψ(Yi, Wi, µ + r(˜µ − µ), e + r(˜e − e))|Xi = x]

"
(µ + r(˜µ − µ)) +

= E

Di(w)Yi
e + r(˜e − e)

−

Di(w)(µ + r(˜µ − µ))
e + r(˜e − e)

− ˆγw

#

(cid:12)
(cid:12)
(cid:12)
Xi = x
(cid:12)
(cid:12)

= (µ + r(˜µ − µ)) +

eµ
e + r(˜e − e)

−

e(µ + r(˜µ − µ))
e + r(˜e − e)

− γw

where we use that E[Di(w)Yi

| Xi = x] = E[Di(w) P

w Di(w)Yi(w) | Xi = x] =

E[Di(w)Yi(w) | Xi = x]

1a
= eµ.

Third, take the derivative with respect to r

∂rE [ψ(Yi, Wi, µ + r(˜µ − µ), e + r(˜e − e))|Xi = x]

= (˜µ − µ) −

eµ(˜e − e)
(e + r(˜e − e))2 −

e(˜µ − µ)(e + r(˜e − e)) − e(µ + r(˜µ − µ))(˜e − e)
(e + r(˜e − e))2

40

Finally, evaluate at the true nuisance values, i.e. set r = 0 to show that 13 holds

∂rE[ψ(Yi, Wi, µ + r(˜µ − µ), e + r(˜e − e))|Xi = x]|r=0

e(˜µ − µ)(e + 0(˜e − e)) − e(µ + 0(˜µ − µ))(˜e − e)
(e + 0(˜e − e))2

eµ(˜e − e)
(e + 0(˜e − e))2 −
eµ(˜e − e)
e2
eµ(˜e − e)
e2

−

−

e(˜µ − µ)e − eµ(˜e − e)
e2
e2
e2 (˜µ − µ) +

eµ(˜e − e)
e2

= (˜µ − µ) −

= (˜µ − µ) −

= (˜µ − µ) −

= 0

41

B DR- and NDR-learner

This Appendix describes the algorithms that are applied to estimate out-of-sample IATEs

using the DR- and NDR-learner. It mostly follows Algorithm 1 of Kennedy (2020) and

adapts it to the situation that we are interested in estimating IATEs for all observations

without using them in the estimation step.

Algorithm 1 (DR-learner) Let (SN

1 , SN

2 , SN

3 , SN

4 ) denote four independent samples of

N observations of Oi = (Xi, Wi, Yi).

Step 1. Nuisance training:

(a) Construct a model ˆew(x) of the propensity scores ew(x) using SN
1 .

(b) Construct a model (ˆµ(w, x), ˆµ(w0, x)) of the regression functions (µ(w, x), µ(w0, x))

using SN
2 .

Step 2. Pseudo-outcome regression: Construct the pseudo-outcome for every observation i in

subsample SN

3 using the models of step 1

ˆ∆i,w,w0 = ˆµ(w, Xi) − ˆµ(w0, Xi) +

Di(w)
ˆew(Xi)

˜Yi(w, Xi) −

Di(w0)
ˆew0(Xi)

˜Yi(w0, Xi),

regress it on covariates Xi in SN

3 , and use the model to predict IATEs in SN

4 , ˆτ 4,1

w,w0(x).

Step 3. Cross-ﬁtting: Repeat steps 1–2 twice, ﬁrst using SN
2

for the propensity score, SN
3

for the outcome regression and SN

1 as subsample to obtain IATE predictions in SN
4

ˆτ 4,2
w,w0(x), and then using SN
3

for the propensity score, SN
1

for the outcome regression

and SN

2 as subsample to obtain IATE predictions in SN

4 , ˆτ 4,3

w,w0(x).

Step 4. Prediction: Predict IATEs in SN

4 as the average of the three predictions ˆτ drl

w,w0(x) =

1/3ˆτ 4,1

w,w0(x) + 1/3ˆτ 4,2

w,w0(x) + 1/3ˆτ 4,3

w,w0(x).

Step 5. Iteration: Repeat steps 1–4 three times. First, with SN

1 , SN

2 and SN

4 to predict IATEs

for SN

3 , second with SN

1 , SN

3 and SN
4

to predict IATEs for SN

2 and ﬁnally with SN
2 ,

3 and SN
SN
4

to predict IATEs for SN
1 .

42

The NDR-learner follows the same basic steps but modiﬁes step two:

Algorithm 2 (NDR-learner) Let (SN

1 , SN

2 , SN

3 , SN

4 ) denote four independent samples

of N observations of Oi = (Xi, Wi, Yi).

Step 1. Nuisance training:

(a) Construct a model ˆew(x) of the propensity scores ew(x) using SN
1 .

(b) Construct a model (ˆµ(w, x), ˆµ(w0, x)) of the regression functions (µ(w, x), µ(w0, x))

using SN
2 .

Step 2a. Pseudo-outcome regression: Construct the pseudo-outcome for every observation i in

subsample SN

3 using the models of step 1

ˆ∆i,w,w0 = ˆµ(w, Xi) − ˆµ(w0, Xi) +

Di(w)
ˆew(Xi)

˜Yi(w, Xi) −

Di(w0)
ˆew0(Xi)

˜Yi(w0, Xi),

regress it on covariates Xi in SN

3 , and use the model to predict IATEs in SN
4 .

Step 2b. Normalisation: For every observation j in SN

4 : (i) extract the weights underlying

its prediction αi(Xj) and (ii) use it to calculate the normalised DR-learner given in

Equation 3.9, where the sum goes over observations in SN

3 , to obtain ˆτ 4,1

w,w0(Xj).

Step 3. Cross-ﬁtting: Repeat steps 1–2 twice, ﬁrst using SN
2

for the propensity score, SN
3

for the outcome regression and SN

1 as subsample to obtain IATE predictions in SN
4

ˆτ 4,2
w,w0(x), and then using SN
3

for the propensity score, SN
1

for the outcome regression

and SN

2 as subsample to obtain IATE predictions in SN

4 , ˆτ 4,3

w,w0(x).

Step 4. Prediction: Predict IATEs in SN

4 as the average of the three predictions ˆτ drl

w,w0(x) =

1/3ˆτ 4,1

w,w0(x) + 1/3ˆτ 4,2

w,w0(x) + 1/3ˆτ 4,3

w,w0(x).

Step 5. Iteration: Repeat steps 1–4 three times. First, with SN

1 , SN

2 and SN

4 to predict IATEs

for SN

3 , second with SN

1 , SN

3 and SN
4

to predict IATEs for SN

2 and ﬁnally with SN
2 ,

3 and SN
SN
4

to predict IATEs for SN
1 .

43

C Results

C.1 Descriptives

Table C.1 provides the means of all confounders by program participation. It documents

that especially measures of past labour market success like past income are associated

with program participation.

Table C.1: Means of control variables by program

Age
Mother tongue in canton’s language
Lives in big city
Lives in medium city
Lives in no city
Caseworker age
Caseworker cooperative
Caseworker education: above vocational training
Caseworker education: tertiary track
Caseworker female
Missing caseworker characteristics
Caseworker has own unemployemnt experience
Caseworker tenure
Caseworker education: vocational degree
Fraction of months employed last 2 years
Number of employment spells last 5 years
Employability
Female
Foreigner with temporary permit
Foreigner with permanent permit
Cantonal GDP p.c.
Married
Mother tongue other than German, French, Italian
Past income
Previous job: manager
Missing sector
Previous job in primary sector
Previous job in secondary sector
Previous job in tertiary sector
Previous job: self-employed
Previous job: skilled worker
Previous job: unskilled worker
Qualiﬁcation: semiskilled
Qualiﬁcation: some degree
Qualiﬁcation: unskilled
Qualiﬁcation: skilled without degree
Swiss citizen
Allocation of unemployed to caseworkers: by industry
Allocation of unemployed to caseworkers: by occupation
Allocation of unemployed to caseworkers: by age
Allocation of unemployed to caseworkers: by employability
Allocation of unemployed to caseworkers: by region
Allocation of unemployed to caseworkers: other
Number of unemployment spells last 2 years
Cantonal unemployment rate (in %)

Note: Program speciﬁc means.

No

(1)

36.6
0.10
0.19
0.12
0.68
44.1
0.48
0.46
0.19
0.43
0.05
0.62
5.48
0.26
0.81
1.21
1.93
0.44
0.13
0.23
0.52
0.47
0.33
42528.0
0.08
0.18
0.09
0.12
0.61
0.01
0.60
0.29
0.16
0.58
0.23
0.03
0.64
0.60
0.51
0.04
0.09
0.13
0.09
0.57
3.52

JS

(2)

37.3
0.12
0.19
0.13
0.68
44.1
0.50
0.45
0.21
0.47
0.05
0.63
5.44
0.27
0.84
0.97
1.98
0.44
0.11
0.22
0.53
0.46
0.29
46693.1
0.08
0.15
0.06
0.14
0.65
0.00
0.65
0.24
0.14
0.62
0.20
0.03
0.67
0.67
0.57
0.04
0.07
0.09
0.07
0.39
3.59

Voc

(3)

37.5
0.11
0.21
0.12
0.67
44.8
0.41
0.44
0.17
0.39
0.04
0.64
5.73
0.22
0.83
0.93
1.93
0.33
0.12
0.18
0.51
0.48
0.31
48653.8
0.10
0.15
0.09
0.15
0.61
0.00
0.65
0.22
0.17
0.63
0.17
0.02
0.70
0.58
0.46
0.04
0.10
0.09
0.08
0.52
3.41

Comp

(4)

39.1
0.11
0.11
0.15
0.73
44.6
0.42
0.48
0.16
0.44
0.05
0.61
5.83
0.25
0.84
0.86
1.97
0.60
0.04
0.17
0.53
0.45
0.18
43212.8
0.09
0.16
0.05
0.13
0.67
0.00
0.75
0.15
0.14
0.72
0.12
0.02
0.79
0.51
0.45
0.06
0.08
0.13
0.10
0.37
3.36

Lang

(5)

35.3
0.04
0.23
0.15
0.63
44.6
0.45
0.48
0.21
0.47
0.05
0.63
5.61
0.22
0.72
0.78
1.85
0.55
0.44
0.23
0.54
0.72
0.64
37300.5
0.07
0.29
0.05
0.12
0.54
0.00
0.43
0.48
0.15
0.38
0.40
0.07
0.34
0.64
0.57
0.05
0.06
0.11
0.09
0.43
3.63

44

C.2 Nuisance parameters

Nuisance parameters are only a tool to remove confounding but it is still informative to

investigate which variables are most predictive of treatment probabilities and outcome.

This is less straightforward for ﬂexible tools like random forests than for the well-known

regression outputs of parametric models. We conduct a classiﬁcation analysis as proposed

by Chernozhukov, Fernandez-Val, and Luo (2018). To this end, we split the predicted

nuisance parameter distributions in quintiles and compare the covariate means of the

observations falling into the ﬁfth and ﬁrst quintile. For comparability, we normalise all

covariates to have mean zero and variance one and order the variables by their largest

absolute diﬀerence between the highest and lowest quintile.

Table C.2 shows that measures of citizenship, qualiﬁcation and previous labour market

success are important predictors of program selection. In line with intuition the former

seems to drive a large part of the selection into language courses. Also Table C.3 showing

the classiﬁcation analysis for outcome predictions shows intuitive patterns. Again measures

of citizenship, qualiﬁcation and previous labour market success seem predictive for future

employment with suggested correlations pointing in the expected directions. For example,

Swiss citizens, individuals with a degree and high past income are overrepresented in the

upper quintile, while individuals with a non-Swiss mother tongue and no qualiﬁcation are

underepresented in the upper quintile.

Finally, we investigate the propensity score distributions for all programs. Figure C.1

shows that propensity scores are quite variable. This indicates that selection into programs

is not negligible. Further, Table C.4 shows that some of the propensity scores get quite

small with the smallest one being 0.003 for a computer training participant. This is not

surprising given that already the unconditional participation probabilities for computer

and vocational training are only about 0.015. However, the small propensity score per

se is not an indicator of poor overlap. The residual with the smallest propensity score

receives a weight of ∼ 1/0.003 = 333, which is only 0.5% of the total weights. Note

that we could easily increase the smallest propensity score by randomly removing a large

fraction of non-participants and participants of the job search program. This would discard

valuable information and shows that the mere focus on the smallest propensity score can

45

be misleading in cases with imbalanced treatment group sizes. More importantly, we

observe overlap in the sense that all treatment groups contain individuals with similarly

low propensity scores. Thus, overlap seems not to be a major issue in our application, at

least for the low dimensional parameters of interest.

Table C.2: Classiﬁcation analysis of propensity scores

No program Job search Vocational Computer

Language

Foreigner with temporary permit
Swiss citizen
Mother tongue other than German, French, Italian
Previous job: unskilled worker
Past income
Previous job: skilled worker
Qualiﬁcation: some degree
Qualiﬁcation: unskilled
Female
Married
Cantonal unemployment rate (in %)
Foreigner with permanent permit
Age
Cantonal GDP p.c.
Number of employment spells last 5 years
Allocation of unemployed to caseworkers: by occupation
Allocation of unemployed to caseworkers: by region
Fraction of months employed last 2 years
Allocation of unemployed to caseworkers: by industry
Previous job: manager
Employability
Previous job in tertiary sector
Caseworker cooperative
Missing sector
Lives in big city
Number of unemployment spells last 2 years
Caseworker female
Qualiﬁcation: skilled without degree
Lives in no city
Previous job in primary sector
Caseworker tenure
Qualiﬁcation: semiskilled
Previous job in secondary sector
Caseworker age
Allocation of unemployed to caseworkers: by employability
Caseworker education: tertiary track
Caseworker has own unemployemnt experience
Caseworker education: vocational degree
Mother tongue in canton’s language
Allocation of unemployed to caseworkers: other
Caseworker education: above vocational training
Allocation of unemployed to caseworkers: by age
Lives in medium city
Previous job: self-employed
Missing caseworker characteristics

(1)

-0.22
0.08
0.10
0.31
-0.72
-0.20
-0.18
0.04
-0.08
-0.23
-0.71
0.08
-0.34
-0.62
0.73
-0.49
0.53
-0.26
-0.45
-0.21
-0.44
-0.20
-0.08
0.15
0.07
0.39
-0.30
-0.08
-0.01
0.30
-0.02
0.24
-0.13
-0.05
0.18
-0.18
-0.14
-0.10
-0.01
0.05
0.12
-0.02
-0.07
0.01
0.06

(2)

-0.34
0.22
-0.42
-0.43
0.78
0.31
0.31
-0.16
-0.06
-0.03
0.73
0.03
0.33
0.65
-0.53
0.50
-0.53
0.47
0.45
0.18
0.49
0.27
0.11
-0.29
-0.08
-0.33
0.27
-0.02
0.05
-0.26
0.00
-0.22
0.15
0.03
-0.19
0.18
0.16
0.14
0.10
-0.04
-0.13
0.01
0.02
0.01
-0.06

(3)

-0.32
0.43
-0.33
-0.61
1.42
0.30
0.53
-0.55
-1.07
-0.17
-0.90
-0.23
0.35
-0.73
-0.41
-0.64
0.02
0.53
-0.49
0.52
0.14
0.01
-0.43
-0.41
-0.04
-0.06
-0.39
-0.10
-0.02
0.30
0.24
-0.04
0.21
0.12
0.11
-0.12
0.07
-0.15
-0.03
-0.13
0.04
-0.06
0.06
0.02
0.05

(4)

-1.25
1.59
-1.57
-1.52
0.28
1.31
1.26
-1.16
1.00
-0.60
-0.53
-0.81
0.79
-0.16
-0.46
-0.21
0.06
0.40
-0.53
0.17
0.36
0.45
-0.21
-0.35
-0.42
-0.40
0.14
-0.27
0.33
-0.26
0.12
-0.24
-0.05
0.08
0.01
-0.17
-0.02
0.07
0.03
-0.00
0.11
0.08
0.03
0.04
-0.02

(5)

1.93
-1.86
1.76
0.99
-0.07
-0.90
-0.93
0.86
0.41
1.07
-0.14
0.56
-0.25
-0.14
-0.31
0.08
0.03
-0.45
0.01
0.09
-0.25
-0.31
-0.02
0.43
0.09
0.02
-0.03
0.36
-0.12
-0.02
0.05
0.10
-0.02
0.19
-0.01
0.00
0.02
-0.14
0.14
-0.07
0.02
-0.00
0.06
-0.07
0.00

Note: Table shows the diﬀerences in means of normalized covariates between the ﬁfth and the ﬁrst quintile of the respective
propensity score distribution. Variables are ordered according to the largest absolute diﬀerence.

46

Table C.3: Classiﬁcation analysis of outcome predictions

No program Job search Vocational Computer

Language

Mother tongue other than German, French, Italian
Qualiﬁcation: some degree
Swiss citizen
Previous job: unskilled worker
Past income
Qualiﬁcation: unskilled
Previous job: skilled worker
Foreigner with permanent permit
Number of unemployment spells last 2 years
Married
Foreigner with temporary permit
Fraction of months employed last 2 years
Employability
Age
Cantonal unemployment rate (in %)
Lives in big city
Cantonal GDP p.c.
Missing sector
Number of employment spells last 5 years
Lives in no city
Previous job: manager
Qualiﬁcation: semiskilled
Female
Previous job in tertiary sector
Mother tongue in canton’s language
Allocation of unemployed to caseworkers: by occupation
Qualiﬁcation: skilled without degree
Previous job in secondary sector
Caseworker age
Caseworker female
Caseworker tenure
Previous job in primary sector
Lives in medium city
Caseworker education: vocational degree
Allocation of unemployed to caseworkers: by employability
Allocation of unemployed to caseworkers: by industry
Caseworker education: above vocational training
Missing caseworker characteristics
Allocation of unemployed to caseworkers: by region
Caseworker cooperative
Allocation of unemployed to caseworkers: by age
Caseworker education: tertiary track
Caseworker has own unemployemnt experience
Previous job: self-employed
Allocation of unemployed to caseworkers: other

(1)

-1.44
1.80
1.37
-1.72
1.49
-1.43
1.23
-0.88
-0.93
-0.98
-0.84
1.01
0.93
-0.72
-0.10
-0.24
-0.04
-0.50
-0.54
0.26
0.54
-0.63
-0.42
0.31
-0.21
0.18
-0.35
0.08
0.02
0.01
-0.05
0.05
-0.08
0.11
0.04
0.11
0.04
-0.04
0.04
-0.03
0.00
0.04
0.03
-0.03
-0.02

(2)

-1.66
1.96
1.60
-1.76
1.38
-1.57
1.27
-1.11
-0.80
-1.14
-0.89
0.78
0.84
-0.76
-0.05
-0.22
0.02
-0.48
-0.42
0.23
0.52
-0.67
-0.32
0.36
-0.25
0.22
-0.38
0.05
0.03
0.04
-0.06
-0.04
-0.05
0.08
0.03
0.12
0.04
-0.05
-0.04
-0.04
-0.01
0.03
0.04
-0.01
-0.03

(3)

-1.16
1.57
1.13
-1.55
1.16
-1.53
1.21
-0.71
-1.25
-0.60
-0.72
0.91
0.46
-0.25
-0.75
-0.74
-0.74
-0.58
-0.68
0.68
0.40
-0.26
-0.44
0.14
-0.22
0.32
-0.22
0.31
0.27
-0.20
-0.11
0.18
-0.08
0.15
0.15
0.08
0.11
-0.11
0.09
-0.07
0.04
0.02
-0.00
-0.03
-0.01

(4)

-2.06
1.64
1.94
-1.65
0.87
-1.36
1.34
-1.34
-0.57
-1.19
-1.08
0.66
0.49
-0.34
-0.02
-0.36
0.04
-0.53
-0.45
0.43
0.32
-0.49
0.23
0.50
-0.45
0.39
-0.34
0.01
-0.17
0.26
-0.25
-0.17
-0.17
0.07
0.05
0.11
0.12
0.02
-0.02
0.04
0.06
-0.05
-0.03
-0.02
-0.02

(5)

-1.89
1.85
1.83
-1.87
1.73
-1.52
1.36
-1.15
-0.56
-1.04
-1.16
0.87
0.62
-0.29
-0.04
-0.30
0.06
-0.71
-0.41
0.33
0.67
-0.59
-0.60
0.51
-0.33
0.25
-0.35
0.07
0.07
-0.02
-0.06
-0.01
-0.12
0.08
0.04
0.09
0.06
-0.06
-0.03
-0.05
0.02
-0.00
0.03
0.02
0.01

Note: Table shows the diﬀerences in means of normalized covariates between the ﬁfth and the ﬁrst quintile of the respective
outcome prediction distribution. Variables are ordered according to the largest absolute diﬀerence.

Table C.4: Summary statistics of propensity score distributions

No program Job search Vocational Computer Language

Mean
SD
Minimum
Q1
Q25
Q50
Q75
Q99
Maximum

0.764
0.093
0.321
0.439
0.727
0.779
0.824
0.910
0.938

0.184
0.094
0.027
0.045
0.120
0.170
0.224
0.515
0.622

0.014
0.006
0.005
0.007
0.009
0.012
0.017
0.031
0.052

0.015
0.008
0.003
0.004
0.009
0.013
0.018
0.038
0.091

0.024
0.030
0.005
0.006
0.009
0.015
0.025
0.109
0.490

Note: The table provides summary statistics of the program speciﬁc propensity
score distributions. The rows denoted by Q show the respective quantiles.

47

Figure C.1: Distribution of propensity scores

(a) No program

(b) Job search

(c) Vocational

(d) Computer

(e) Language

48

02460.000.250.500.75Propensity scoredensity0123450.00.20.40.6Propensity scoredensity03060901200.000.010.020.030.040.05Propensity scoredensity0204060800.0000.0250.0500.075Propensity scoredensity010203040500.00.10.20.30.40.5Propensity scoredensityC.3 Average treatment eﬀects

Figure C.2 shows the APO estimates and Figure C.3 shows the eﬀects of program par-

ticipation on the employment probabilities over time. The latter documents that all

program show the well-known lock-in eﬀect within the ﬁrst months after program start

(e.g. Wunsch, 2016). However, participants of the hard skill programs catch up and show

a sustained increase in employment rates of up to 10 percentage points.

Figure C.2: Average potential outcomes

Notes: Average potential outcomes with 95% conﬁdence intervals. Numeric results in Panel A of
Table C.5.

49

141516171819No programJob searchVocationalComputerLanguageAverage potential outcomeFigure C.3: Average treatment eﬀects over time

(a) Job search

(b) Vocational

(c) Computer

(d) Language

Notes: Solid lines show ATE, dashed lines ATET of the respective programs compared to
nonparticipation on employment probability in the 31 months after program start. Grey area
depicts the 95% conﬁdence intervals.

50

−0.10.00.10.20102030MonthAverage treatment effects−0.10.00.10.20102030MonthAverage treatment effects−0.10.00.10.20102030MonthAverage treatment effects−0.10.00.10.20102030MonthAverage treatment effectsTable C.5: Average eﬀects

Estimate
(1)

Standard error
(2)

Panel A: APO
No program
Job search
Vocational
Computer
Language

14.78
13.76
18.04
18.21
17.32

Panel B: ATE
Job search - no program -1.02∗∗∗
Vocational - no program 3.26∗∗∗
3.43∗∗∗
Computer - no program
2.54∗∗∗
Language - no program

Panel C: ATET
Job search - no program -0.98∗∗∗
Vocational - no program 2.79∗∗∗
3.47∗∗∗
Computer - no program
1.08∗∗∗
Language - no program

0.06
0.12
0.42
0.43
0.37

0.13
0.43
0.43
0.38

0.12
0.39
0.40
0.29

Note: Table shows DML based point estimates and
standard errors of average eﬀects. ∗p<0.1; ∗∗p<0.05;
∗∗∗p<0.01

51

C.4 GATEs

Figure C.4 shows that the kernel and spline regressions detect no substantial heterogeneity

for individuals of diﬀerent age. All programs besides language courses show basically ﬂat

curves or at least the 95% conﬁdence intervals includes the ATE. Only language courses

show a larger positive eﬀect for job seekers in the late twenties. Both kernel and spline

regressions agree regarding this but the relatively erratic curves prevent deriving strong

conclusions from these results.

52

Figure C.4: Eﬀect heterogeneity regarding age

(a) Job search (Kernel)

(b) Job search (Spline)

(c) Vocational (Kernel)

(d) Vocational (Spline)

(e) Computer (Kernel)

(f) Computer (Spline)

(g) Language (Kernel)

(h) Language (Spline)

Dotted line indicates point estimate of the respective average treatment eﬀect. Grey area shows
95%-conﬁdence interval.

53

−50510304050AgeConditional average treatment effect−50510304050AgeConditional average treatment effect−50510304050AgeConditional average treatment effect−50510304050AgeConditional average treatment effect−50510304050AgeConditional average treatment effect−50510304050AgeConditional average treatment effect−50510304050AgeConditional average treatment effect−50510304050AgeConditional average treatment effectC.5

IATEs

Figure C.5 documents the extreme IATE predictions obtained using the full sample.

Especially the DR-learner produces very extreme estimates for vocational training ranging

from -265 to 161. Also in this extreme case, the NDR-learner mitigates the problem

substantially. However, Table C.6 documents that it still produces implausibly high values

ranging from -19 to 20. The out-of-sample prediction of IATEs is thus preferred and

discussed in the main text.

Figure C.5: Boxplot of IATEs estimated by DR- and NDR-learner

Note: The ﬁgure shows the distribution of IATEs for participating in the program labeled
on the x-axis vs. non-participation estimated by the DR-learner (DRL) and the NDR-learner
(NDRL). The ﬁrst two boxplots of a group are obtained using the out-of-sample (oos) procedure
of Appendix B and the other two from the full sample. The dashed line indicates the possible
range of the IATE of [-31,31] to illustrate that several DR-learner estimated IATEs lie outside
this bound.

Table C.6 and Figure C.6 provide a detailed comparison of the IATEs estimated by

DR- and NDR-learner. We see that the diﬀerences are mainly driven by few outliers as

indicated by the much larger kurtosis for the DR-learner IATEs. However, most of the

estimates are quite similar as the correlations of at least 0.90 provided in the last row of

panel A in Table C.6 and the scatter plots in Figure C.6 document.

Table C.7 shows the full results of the classiﬁcation analysis. In line with previous

results of Knaus et al. (2020), we observe that the caseworker characteristics play a

negligible role in explaining variation in IATEs. All caseworker related variables are in the

54

−200−1000100Job searchVocationalComputerLanguageIndividualized average treatment effectDRL.oosNDRL.oosDRL.fullNDRL.fullTable C.6: Summary statistics of IATE distributions

Job search

Vocational

Computer

Language

DRL NDRL

DRL

NDRL

DRL

NDRL

DRL

NDRL

Panel A: Out-of-sample
Mean
SD
Minimum
Q1
Q25
Q50
Q75
Q99
Maximum
Kurtosis
Correlation

-0.97
0.90
-4.75
-2.96
-1.57
-1.03
-0.42
1.34
3.66
3.35

0.99

-0.99
0.86
-4.68
-2.91
-1.57
-1.04
-0.45
1.22
3.76
3.35

3.17
2.46
-33.72
-3.04
1.65
3.22
4.77
8.78
20.99
5.63

3.13
2.20
-6.71
-2.14
1.68
3.14
4.58
8.36
12.86
3.22

3.68
2.13
-31.04
-1.61
2.34
3.69
5.04
8.65
19.24
6.37

3.54
1.95
-5.33
-1.01
2.25
3.51
4.80
8.31
12.58
3.28

2.55
2.12
-14.03
-2.67
1.24
2.57
3.85
7.90
16.28
4.28

0.90

0.92

0.93

Panel B: Full sample
Mean
SD
Minimum
Q1
Q25
Q50
Q75
Q99
Maximum
Kurtosis
Correlation

-1.03
1.20
-10.42
-3.89
-1.80
-1.07
-0.29
1.98
6.53
4.07

-1.04
1.14
-8.21
-3.71
-1.79
-1.08
-0.33
1.82
6.45
3.86

3.15
4.07
-71.74
-6.18
1.27
3.24
5.19
11.57
62.01
45.54

3.08
3.03
-16.35
-4.78
1.22
3.14
5.04
10.14
17.92
4.02

3.46
2.31
-30.67
-2.06
2.08
3.48
4.88
8.79
45.33
13.18

3.32
2.17
-12.26
-2.05
1.94
3.34
4.73
8.45
16.30
3.80

2.50
7.51
-265.23
-11.07
0.41
2.53
4.61
16.98
160.92
228.23

0.99

0.86

0.94

0.67

2.50
1.88
-5.79
-1.95
1.21
2.57
3.80
6.74
10.75
2.98

2.40
3.42
-18.58
-6.46
0.23
2.52
4.70
10.05
19.63
3.96

Note: The table provides summary statistics for the distributions of IATEs estimated
by the DR-learner (DRL) and NDR-learner (NDRL). The rows denoted by Q show
the respective quantiles. Correlation is calculated between the DR-learner and the
NDR-learner. Bold numbers indicate values that are outside the possible range.

55

lower half of the table.

Table C.7: Classiﬁcation analysis of IATEs

Job search Vocational Computer

Language

Past income
Previous job: unskilled worker
Mother tongue other than German, French, Italian
Qualiﬁcation: some degree
Swiss citizen
Fraction of months employed last 2 years
Qualiﬁcation: unskilled
Previous job: skilled worker
Missing sector
Female
Cantonal GDP p.c.
Foreigner with temporary permit
Cantonal unemployment rate (in %)
Married
Foreigner with permanent permit
Previous job in tertiary sector
Employability
Number of employment spells last 5 years
Number of unemployment spells last 2 years
Previous job: manager
Lives in no city
Lives in big city
Age
Qualiﬁcation: semiskilled
Caseworker age
Previous job in primary sector
Allocation of unemployed to caseworkers: by occupation
Caseworker female
Allocation of unemployed to caseworkers: by region
Lives in medium city
Previous job in secondary sector
Mother tongue in canton’s language
Qualiﬁcation: skilled without degree
Caseworker education: above vocational training
Caseworker education: tertiary track
Caseworker cooperative
Allocation of unemployed to caseworkers: by employability
Caseworker education: vocational degree
Caseworker tenure
Allocation of unemployed to caseworkers: by age
Allocation of unemployed to caseworkers: by industry
Missing caseworker characteristics
Previous job: self-employed
Caseworker has own unemployemnt experience
Allocation of unemployed to caseworkers: other

(1)

-1.32
1.02
0.69
-0.88
-0.66
-1.06
0.81
-0.85
0.89
0.62
0.31
0.55
0.41
0.32
0.31
-0.45
-0.45
0.53
0.47
-0.27
-0.40
0.27
0.01
0.20
0.10
-0.33
0.17
0.07
-0.27
0.23
-0.10
0.06
0.15
-0.01
0.00
0.01
-0.12
-0.12
0.04
-0.05
0.10
0.05
0.06
0.03
-0.04

(2)

-0.84
0.68
0.68
-0.65
-0.60
-0.37
0.41
-0.41
0.05
0.01
-0.78
0.35
-0.72
0.53
0.40
-0.31
-0.55
0.22
0.11
-0.37
0.26
-0.38
0.35
0.37
0.33
0.21
0.02
-0.26
0.06
0.08
0.22
0.09
0.09
0.09
-0.15
-0.06
0.10
-0.01
0.02
0.00
-0.09
-0.09
0.00
-0.02
0.03

(3)

-1.17
0.34
0.00
-0.41
0.12
-0.47
0.32
-0.09
0.21
0.88
0.01
0.09
0.02
0.11
-0.21
0.03
-0.55
0.04
0.13
-0.36
0.10
-0.08
0.37
0.19
-0.00
-0.28
0.23
0.29
-0.05
-0.04
-0.06
-0.18
-0.00
0.11
-0.12
0.15
0.02
-0.11
-0.05
0.10
-0.02
0.05
-0.02
-0.04
0.01

(4)

1.01
-1.24
-1.17
1.15
1.11
0.30
-1.02
0.94
-0.53
-0.48
0.26
-0.75
0.13
-0.69
-0.67
0.58
0.22
-0.08
-0.15
0.40
0.06
-0.09
-0.01
-0.30
0.08
-0.19
0.30
-0.09
-0.10
0.03
-0.07
-0.08
-0.17
0.16
-0.16
-0.02
0.05
-0.01
-0.10
0.04
-0.03
-0.09
0.03
0.06
0.00

Note: Table shows the diﬀerences in means of normalized covariates between the ﬁfth and the ﬁrst quintile of the
respective estimated IATE distribution. Variables are ordered according to the largest absolute diﬀerence.

56

Figure C.6: Joint and marginal distributions of IATEs

(a) Job search

(b) Vocational

(c) Computer

(d) Language

Notes: Figures show the joint and marginal distributions of IATEs estimated by DR-learner and
NDR-learner.

57

C.6 Optimal treatment assignment

Figure C.7: Optimal decision tree of depth three with ﬁve covariates

Notes: Optimal assignment rules estimated following the procedure deﬁned in Section 3.4.

Figure C.8: Overlap of cross-validated policy rules with ﬁve covariates

Notes: Figure shows the fraction of cross-validated policies that agree with the full sample
policy.

58

Age <= 43Past income <= 62000TruePast income <= 51000FalseFemale <= 0Age <= 39Past income <= 15000Employability <= 1VocationalComputerLanguageComputerComputerVocationalComputerLanguage0.00.10.20.30.40.50.00.20.40.60.8OverlapFractionDepth 1Depth 2Depth 3Figure C.9: Optimal decision tree of depth three with 16 covariates

Notes: Optimal assignment rules estimated following the procedure deﬁned in Section 3.4.

Figure C.10: Overlap of cross-validated policy rules with 16 covariates

Notes: Figure shows the fraction of cross-validated policies that agree with the full sample
policy.

59

Past income <= 43000Fraction of months employed last 2 years <= 0.92TruePrevious job in tertiary sector <= 0FalsePast income <= 18000Number of employment spells last 5 years <= 1Missing sector <= 0Previous job: unskilled worker <= 0ComputerVocationalComputerVocationalVocationalComputerLanguageComputer0.00.10.20.30.40.50.00.20.40.60.81.0OverlapFractionDepth 1Depth 2Depth 3