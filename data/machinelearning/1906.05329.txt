9
1
0
2

n
u
J

2
1

]

G
L
.
s
c
[

1
v
9
2
3
5
0
.
6
0
9
1
:
v
i
X
r
a

Sub-Goal Trees – a Framework for Goal-Directed
Trajectory Prediction and Optimization

Tom Jurgenson
EE Department
Technion

Edward Groshev
Osaro Inc.

Aviv Tamar
EE Department
Technion

Abstract

Many AI problems, in robotics and other domains, are goal-directed, essentially
seeking a trajectory leading to some goal state. In such problems, the way we
choose to represent a trajectory underlies algorithms for trajectory prediction and
optimization. Interestingly, most all prior work in imitation and reinforcement
learning builds on a sequential trajectory representation – calculating the next state
in the trajectory given its predecessors. We propose a different perspective: a
goal-conditioned trajectory can be represented by ﬁrst selecting an intermediate
state between start and goal, partitioning the trajectory into two. Then, recursively,
predicting intermediate points on each sub-segment, until a complete trajectory
is obtained. We call this representation a sub-goal tree, and building on it, we
develop new methods for trajectory prediction, learning, and optimization. We show
that in a supervised learning setting, sub-goal trees better account for trajectory
variability, and can predict trajectories exponentially faster at test time by leveraging
a concurrent computation. Then, for optimization, we derive a new dynamic
programming equation for sub-goal trees, and use it to develop new planning and
reinforcement learning algorithms. These algorithms, which are not based on the
standard Bellman equation, naturally account for hierarchical sub-goal structure in
a task. Empirical results on motion planning domains show that the sub-goal tree
framework signiﬁcantly improves both accuracy and prediction time.

1

Introduction

Many AI problems can be characterized as learning or optimizing goal-directed trajectories of a
dynamical system. For example, robotic skill learning seeks trajectories that perform some task, such
as hitting a table-tennis ball [21] or opening a door [8], while in motion planning a trajectory that
reaches some goal without colliding into obstacles is sought [17]. For such problems, AI algorithms
such as imitation learning (IL) and reinforcement learning (RL) must represent the trajectory in some
computational structure, and most prior work has built on a trajectory representation that mirrors the
underlying dynamical system: trajectory states evolve in a sequential manner, where the next state is
computed based on the previous states [3, 32].

However, the sequential representation has several drawbacks. When learning to imitate trajectories,
for example, it is known that accumulating errors can lead to diverging trajectories [26], and enforcing
the condition that the goal is reached can require additional mechanisms, such as modeling the goal
as an attractor in a dynamical system [9]. In domains where fast prediction is required, such as
robotic motion planning or autonomous driving, the sequential trajectory prediction can be a limiting
factor. In RL, which relies on the sequential Bellman equation, learning to optimize goal-conditioned
policies is a challenging problem [28, 2].

In this work, we challenge the conventional representation of a trajectory as a sequential process, and
posit that for goal-directed problems, a structure that naturally segments the trajectory into sub-goals

Preprint. Under review.

 
 
 
 
 
 
Figure 1: Trajectory prediction methods. Upper row: a conventional Sequential representation.
Lower row: Sub-Goal Tree representation. Solid arrows indicate predicted segments, while dashed
arrows indicate segments that still require to be predicted. By concurrently predicting sub-goals, a
Sub-Goal Tree requires 3 sequential computations, while the sequential requires 8.

is more appropriate. Our main insight is that a goal-directed trajectory can be represented as a tree
structure, where ﬁrst, an intermediate sub-goal between the initial state and the goal is computed,
segmenting the trajectory into two. Then, for each segment, an additional sub-goal is computed.
This divide-and-conquer process continues recursively until a complete trajectory is obtained (see
Figure 1 for an illustration.). We call this representation a sub-goal tree, and propose it as a basis for
developing prediction and optimization algorithms for both the supervised learning and RL settings.

Sub-goal tree trajectories are much faster to predict at test time, since sub-goals can be predicted
concurrently for multiple segments in a trajectory, a feature that is important for domains where fast
prediction is required. Additionally, sub-goal trees can be used as an anytime prediction algorithm,
as the sub goals provide a meaningful approximation of the trajectory from start to goal. In the
supervised learning setting, the sub-goal tree representation naturally leads to an algorithm that learns
to predict sub-goals in a trajectory. We show that this approach can better account for multi-modal
variations in the trajectory, and leads to improved accuracy.

In the optimization setting, we consider the all-pairs shortest path problem on a graph (APSP;[17]),
and propose a new dynamic programming (DP) principle for APSP based on sub-goal trees. Then,
based on this formulation, we develop an approximate DP (a.k.a. RL) algorithm for continuous-state
goal-based problems, which naturally accounts for any hierarchical sub-goal structure in the problem.
We show that this algorithm can learn signiﬁcantly better than conventional goal-based RL algorithms
that rely on Bellman’s sequential DP equation [4]. To the best of our knowledge, this is the ﬁrst APSP
algorithm that can work with function approximation.

2 Problem Formulation

In this work we are interested in learning and optimizing trajectories of dynamical systems. We
present a uniform formulation based on controlled dynamical systems. Consider a deterministic
controlled dynamical system in discrete time, deﬁned over a state space S and an action space U :

st+1 = f (st, ut),
where st, st+1 ∈ S are the states at time t and t + 1, respectively, ut ∈ U is the control at time t, and
f is a stationary state transition function.1

(1)

In the Imitation Learning (IL) setting, our goal is to imitate trajectories of (1) where actions are
selected by an expert. Speciﬁcally, we are given a dataset Dπ∗
i=1 of N trajectory demonstra-
tions, provided by an expert policy π∗. Each trajectory demonstration τi from the dataset Dπ∗
, is a
sequence of Ti states, i.e. τi = si
.2 In this work, we assume a goal-based setting, that is,

= {τi}N

0, si

1 . . . si
Ti

1We leave the extension of our ideas to stochastic and time varying systems for future work.
2We note that limiting our discussion to states only can be easily extended to include actions as well by
concatenating states and actions. However, we refrain from that in this work in order to simplify the notations.

2

we assume that the expert policy generates trajectories that lead to a goal state which is the last state
in each trajectory demonstration. Our goal is to learn a policy that, given a pair of current state and
goal state (s, g), predicts the trajectory that π∗ would have chosen to reach g from s.

In many practical settings, however, expert demonstrations are difﬁcult to obtain. In such cases
we resort to an RL setting, where an optimal policy has to be learned from data. In this case, we
extend the dynamical system (1) to include costs. Given some initial state s0 and a goal g, an optimal
trajectory reaches the goal while minimizing the costs:

min
T,u0,...,uT

T
(cid:88)

t=0

c(st, ut),

s.t.

st+1 = f (st, ut),

sT = g,

(2)

where c(s, u) is a non-negative cost function. We assume that the dynamics function f is not known
in advance. Instead, we are given a data set of transitions D = {s, u, c, s(cid:48)}, where s(cid:48) = f (s, u)
and c = c(s, u), sampled from some arbitrary distribution. From this data, our goal is to learn
a goal-conditioned policy π(s, g) : S × S → U that, when used to select actions in the system,
produces an optimal trajectory. This setting is also known as batch-RL [16, 7, 25].

3

Imitation Learning with Sub-Goal Trees

In this section, we consider the imitation learning problem, and show that a method based on sub-goal
trees is more efﬁcient than the conventional approach. We focus on the Behavioral Cloning (BC)[23]
approach to IL, where a parametric model for a policy ˆπ with parameters θ is learned by maximizing
the log-likelihood of observed trajectories in Dπ∗
θ∗ = arg max

τi∼Dπ∗ (cid:2)log Pˆπ(τi = si
E

T ; θ)(cid:3).

1, . . . si

T |si

0, si

0, si

, i.e.,

(3)

θ

Denote the horizon T as the maximal number of states in a trajectory. For ease of notation we assume
T to be the same for all trajectories3. Also, let si = si
0 and gi = si
denote the start and goal states
Ti
for τi. We ask – how to best represent the distribution Pˆπ(τ |s, g; θ)?

The sequential trajectory representation [23, 26, 35, 24], a popular approach, decomposes
Pˆπ(s0, s1, . . . sT |s, g; θ) by sequentially predicting states in the trajectory conditioned on previ-
ous predictions. Concretely, let ht = s0, s1, . . . , st denote the history of the trajectory at time
index t, the decomposition assumed by the sequential representation is Pˆπ(s0, s1, . . . sT |s, g; θ) =
Pˆπ(s1|h0, g; θ)Pˆπ(s2|h1, g; θ) . . . Pˆπ(sT |hT −1, g; θ). Using this decomposition, (3) becomes:

θ∗ = arg max

θ

E

τi∼Dπ∗

(cid:34) T

(cid:88)

t=1

log Pˆπ(si

t+1|hi

(cid:35)
t, gi; θ)

.

(4)

We can learn Pˆπ using a batch stochastic gradient descent (SGD) method. To generate a sample
(st, ht−1, g) in a training batch, a trajectory τi is sampled from Dπ∗
t is
further sampled from τi. Next, the history hi
t−1 and goal gi are extracted from τi. After learning,
sampling a trajectory between s and g is a straight-forward iterative process, where the ﬁrst prediction
is given by s1 ∼ Pˆπ(s|h0 = s, g; θ), and every subsequent prediction is given by st+1 ∼ Pˆπ(s|ht =
(s, s1, . . . st), g; θ). This iterative process stops once some stopping condition is met (such as a target
prediction horizon is reached, or a prediction is ’close-enough’ to g). Pseudo-code for learning and
prediction is further detailed in the supplementary material.

, and an observed state si

This sequential decomposition has two major drawbacks. First, since prediction at time t requires
the history ht−1, the computation time of predicting an entire trajectory is linear in the horizon T .
Furthermore, as each prediction accumulates error, the trajectory can drift signiﬁcantly away from
the demonstrations [26]. Both drawbacks could be mitigated by using an alternative non-sequential
decomposition of the trajectory distribution, as we describe next.

The Sub-Goal Tree Representation: We propose an alternative representation for a trajectory, based
on a ’divide-and-conquer’ approach. Let τ (s1, s2) denote the sub-trajectory of τ starting in s1 and

3For trajectories with Ti < T assume sTi
i

data until Ti = T

repeats T − Ti times, alternatively, generate middle states from

3

ending in s2. We note that for any mid-point sm in a trajectory τ with start state s and goal state
g, τ can be written as a concatenation of two sub-trajectories: [τ (s, sm), τ (sm, g)].4 To simplify
notation, we consider the case where the horizon T is a power of two. 5 We therefore choose to
decompose the probability of a trajectory recursively as (cf. Figure 1) Pˆπ(s0, s1, . . . sT |s, g; θ) =
Pˆπ(sT /2|s, g; θ)Pˆπ(sT /4|s, sT /2; θ)Pˆπ(s3T /4|sT /2, g; θ) . . . , the recursion ends when the start and
stop indices are equal and we set the probability to Pˆπ(st|st, st; θ) = 1. Applying this decomposition
on Eq.3 results in:

θ∗=arg max

θ

E

(cid:104)
τi∼Dπ∗

log Pˆπ(si

T /2|si, gi; θ)+log Pˆπ(si

T /4|si, si

T /2; θ)+log Pˆπ(si

3T /4|si

T /2, gi; θ). . .
(5)

(cid:105)

To organize our data for optimizing Eq.5, we ﬁrst sample a trajectory τi from Dπ∗
the batch. From τi we sample two states si
b and obtain their midpoint si
sub-goal tree learning is provided in Section B of the supplementary material.

a and si

a+b
2

for each sample in
. Pseudo-code for

A clear advantage of Sub-goal trees over sequential representations is in prediction, which can exploit
parallel computation. In predicting a sub-goal tree between s and g, we ﬁrst predict a midpoint
sm. Then, the predictions of midpoints for the two segments (s, sm) and (sm, g) are independent,
and can be computed concurrently. This computation can be described by a binary tree rooted at
τ (s, g). Recursively, at each level k of the tree we can predict 2k midpoints concurrently, resulting in
an exponential speedup in prediction time (requiring O(log(T )) to predict T -horizoned trajectory,
as opposed to O(T ) for sequential prediction). Algorithm 1 shows the prediction process of the
Sub-Goal Tree approach, and Figure 1 shows the difference in prediction strategies.

1

2

1

2

Algorithm

Input: parameters θ of parametric distribution Pˆπ, start state s, goal state g, max depth K
return [s] + PredictSGT(θ, s, g, K) +[g]

Procedure PredictSGT(θ, s1, s2, k)

if k > 0 then

Predict midpoint sm ∼ Pˆπ(sm|s1, s2; θ)
return PredictSGT(θ, s1, sm, k − 1) + [sm] + PredictSGT(θ, sm, s2, k − 1)

end

Algorithm 1: Sub-Goal Tree BC Trajectory Prediction

4 Trajectory Optimization with Sub-Goal Trees

We next discuss how to solve the batch-RL problem using sub-goal trees. We start from an underlying
dynamic programming principle and then proceed to develop learning algorithms.

4.1 A Dynamic Programming Principle for Sub-Goal Trees

In this section we derive a dynamic programming principle for goal-conditioned trajectory optimiza-
tion based on sub-goal trees. To simplify our derivation, we restrict ourselves to a discrete-state
formulation of Problem (2), also known as the all-pairs shortest path (APSP) problem on a weighted
graph. We will later extend our approach to the continuous case using function approximation.
Consider a directed and weighted graph with N nodes s1, . . . , sN , and denote by c(s, s(cid:48)) ≥ 0 the
weight of edge s → s(cid:48).6 To simplify notation, we replace unconnected edges by edges with weight
∞, creating a complete graph. The APSP problem seeks the shortest paths (i.e., a path with minimum
sum of costs) from any start node s to any goal node g in the graph. Note the similarity to Problem
(2), where feasible transitions of the dynamical system are now represented by edges between states
in the graph. We next derive an APSP algorithm based on sub-goal trees.

4Here and throughout the rest of the paper, the concatenation [τ (s, sm), τ (sm, g)] is understood to contain

the midpoint sm only once.

5To handle a general horizon, one can repeat elements to obtain a power of 2 horizon. Alternatively,

interpolation between states can be performed.

6Technically, and similarly to standard APSP algorithms [27], we only require that there are no negative

cycles in the graph. To simplify our presentation, however, we restrict c to be non-negative.

4

Let Vk(s, s(cid:48)) denote the shortest path from s to s(cid:48) in 2k steps or less. Note that by our convention
about unconnected edges above, if there is no such trajectory in the graph then Vk(s, s(cid:48)) = ∞.
We observe that Vk obeys a dynamic programming relation, which we term sub-goal tree dynamic
programming (STDP), as established next.
Theorem 1. Consider a weighted graph with N nodes and no negative cycles. Let Vk(s, s(cid:48)) denote
the cost of the shortest path from s to s(cid:48) in 2k steps or less, and let V ∗(s, s(cid:48)) denote the cost of the
shortest path from s to s(cid:48). Then, Vk can be computed according to the following equations:
∀s, s(cid:48) : s (cid:54)= s(cid:48);

V0(s, s(cid:48)) = c(s, s(cid:48)),
∀s;
Vk(s, s) = 0,
{Vk−1(s, sm) + Vk−1(sm, s(cid:48))} ,
Vk(s, s(cid:48)) = min
sm

∀s, s(cid:48) : s (cid:54)= s(cid:48).

(6)

Furthermore, for k ≥ log2(N ) we have that Vk(s, s(cid:48)) = V ∗(s, s(cid:48)) for all s, s(cid:48).

Proof. First, by deﬁnition, the shortest path from s to itself is 0. In the following, therefore, we
assume that s (cid:54)= s(cid:48). We will show by induction that each Vk(s, s(cid:48)) in Algorithm (6) is the cost of the
shortest path from s to s(cid:48) in 2k steps or less.
Let τk(s, s(cid:48)) denote a shortest path from s to s(cid:48) in 2k steps or less, and let Vk(s, s(cid:48)) denote its
corresponding cost. Our induction hypothesis is that Vk−1(s, s(cid:48)) is the cost of the shortest path from
s to s(cid:48) in 2k−1 steps or less. We will show that Vk(s, s(cid:48)) = minsm {Vk−1(s, sm) + Vk−1(sm, s(cid:48))}.
Assume by contradiction that there was some s∗ such that Vk−1(s, s∗) + Vk−1(s∗, s(cid:48)) < Vk(s, s(cid:48)).
Then the concatenated trajectory [τk−1(s, s∗), τk−1(s∗, s(cid:48))] would have 2k steps or less, contradicting
the fact that τk(s, s(cid:48)) is a shortest path from s to s(cid:48) in 2k steps or less. So we have that Vk(s, s(cid:48)) ≤
{Vk−1(s, sm) + Vk−1(sm, s(cid:48))}
∀sm. Since the graph is complete, τk(s, s(cid:48)) can be split into two
trajectories of length 2k−1 steps or less. Let sm be a midpoint in such a split. Then we have that
Vk(s, s(cid:48)) = {Vk−1(s, sm) + Vk−1(sm, s(cid:48))}. So equality can be obtained and thus we must have
Vk(s, s(cid:48)) = minsm {Vk−1(s, sm) + Vk−1(sm, s(cid:48))}. To complete the induction argument, we need to
show that V0(s, s(cid:48)) = c(s, s(cid:48)). This holds since for k = 0, for each s, s(cid:48), the only possible trajectory
between them of length 1 goes through the edge s, s(cid:48).
Finally, since there are no negative cycles in the graph, for any s, s(cid:48), the shortest path has at most N
steps. Thus, for k = log2(N ), we have that Vk(s, s(cid:48)) is the cost of the shortest path in N steps or
less, which is the shortest path between s and s(cid:48).

The STDP algorithm is built on the sub-goal tree idea. To see this, note that in Eq. (6), the min-
imization over sm is effectively a search for the next sub-goal in the sub-goal tree. Furthermore,
predicting a sub-goal tree of depth k between a pair of states (s, g) can be done by recursively
computing sub-goals according to Eq. (6), starting from Vk(s, g). The main beneﬁt of STDP however,
is that it lends to a simple approximate dynamic programming (a.k.a. RL) formulation using function
approximation, as we show next.

4.2 Batch RL with Sub-Goal Trees

We now describe a batch RL algorithm with function approximation based on the STDP algorithm
above. Our approach is inspired by the ﬁtted-Q algorithm for ﬁnite horizon Markov decision processes
([34]; see also [7, 25] for the discounted horizon case). Assume that we have some estimate ˆVk(s, s(cid:48))
of the value function of depth k in STDP. Then, for any pair of start and goal states s, g, we can
estimate ˆVk+1(s, g) as

ˆVk+1(s, g) = min
sm

(cid:110) ˆVk(s, sm) + ˆVk(sm, g)

(cid:111)

(7)

Thus, if our data consisted of start and goal pairs, we could use (7) to generate regression targets for
the next value function, and use any regression algorithm to ﬁt ˆVk+1(s, s(cid:48)). This is the essence of the
approximate STDP algorithm (Algorithm 2). Since our data does not contain explicit goal states, we
simply deﬁne goal states to be randomly selected states from within the data.

The ﬁrst iteration k = 0 in STDP, however, requires special attention. We need to ﬁt the cost function
for neighboring states, yet make sure that states which are not reachable in a single transition have a

5

high cost. To this end, we ﬁt the observed costs c to the observed transitions s, s(cid:48) in the data, and a
high cost Cmax to transitions from the observed states to randomly selected states.

We also need a method to approximately solve the minimization problem in (7). In our experiments,
we discretized the state space and performed a simple grid search. Other methods could be used in
general. For example, if Vk is represented as a neural network, then one can use gradient descent.
Naturally, the quality of the approximate STDP solution will depend on the quality of solving this
minimization problem.

1

2

3

4

5

6

7

Algorithm

Input: dataset D = {s, u, c, s(cid:48)}, Maximum path cost Cmax
Create transition data set Dtrans = {s, s(cid:48)} and targets Ttrans = {c} with s, s(cid:48), c taken from D
Create random transition data set Drandom = {s, srand} and targets Trandom = {Cmax} with
s, srand randomly chosen from states in D
Create self transition data set Dself = {s, s} and targets Tself = {0} with s taken from D
Fit ˆV0(s, s(cid:48)) to data in Dtrans, Drandom, Dself and targets Ttrans, Trandom, Tself
for k : 1...K do

Create goal data set Dgoal = {s, g} and targets
Tgoal = {minsm{ ˆVk−1(s, sm)+ ˆVk−1(sm, g)}} with s, g randomly chosen from states in D
Fit ˆVk(s, s(cid:48)) to data in Dgoal and targets in Tgoal

end

Algorithm 2: Approximate STDP

Why not use the Floyd-Warshall Algorithm? At this point, the reader may question why we do
not build on the Floyd-Warshall (FW) algorithm for the APSP problem. The FW method maintains a
value function VF W (s, s(cid:48)) of the shortest path from s to s(cid:48), and updates the value using the relaxation
VF W (s, s(cid:48)) := min {VF W (s, s(cid:48)), VF W (s, sm) + VF W (sm, s(cid:48))}. If the updates are performed over
all sm, s, and s(cid:48) (in that sequence), VF W will converge to the shortest path, requiring O(N 3)
computations [27]. One can also perform relaxations in an arbitrary order, as was suggested by
Kaelbling [11], and more recently in [6], to result in an RL style algorithm. However, as was already
observed in [11], the FW relaxation requires that the values always over-estimate the optimal costs,
and any under-estimation error, due to noise or function approximation, gets propagated through the
algorithm without any way of recovery, leading to instability. Indeed, both [11, 6] showed results
only for table-lookup value functions, and in our experiments we have found that replacing the STDP
update with a FW relaxation (reported in the supplementary) leads to instability when used with
function approximation. On the other hand, the complexity of STDP is O(N 3 log N ), but the explicit
dependence on k in the value function allows for a stable update when using function approximation.

5 Related work

Various trajectory representations have been investigated for learning robotic skills [21, 31] and navi-
gation [24]. Dynamical movement primitives [9] is a popular approach that represents a continuous
trajectory as a dynamical system with an attractor at the goal, and has been successfully used for IL
and RL [14, 21, 22]. The temporal segment approach of [20], on the other hand, predicts segments of
a trajectory sequentially. Recently, in the context of video prediction, Jayaraman et al. [10] proposed
to predict salient frames in a goal-conditioned setting by a supervised learning loss that focuses on
the ‘best’ frames. This was used to predict a list of sub-goals for a tracking controller. In contrast, we
propose to recursively predict sub-goals. Note that the method of [10] can be combined within our
approach to learn the most salient sub-goal tree.

In RL, the idea of sub-goals has mainly been investigated under the options framework [33]. In this
setting, the goal is typically ﬁxed (i.e., given by the reward in the MDP), and useful options are
discovered using some heuristic such as bottleneck states [18, 19] or changes in the value function [15].
Universal value functions [28, 2] learn a goal-conditioned value function using the Bellman equation.
In contrast, in this work we propose a principled motivation for sub-goals based on the APSP problem,
and develop a new RL formulation based on this principle. A connection between RL and the APSP
has been suggested in [11, 6], based on the Floyd-Warshall algorithm. However, as discussed in
Section 4, these approaches become unstable once function approximation is introduced.

6

(a)

(b)

(c)

(d)

Figure 2: Experiment domains and results. (a+b) IL results. The simple domain (a) and hard domain
(b). A point robot should move only on the white free-space from a room on one side to the room on
the other side while avoiding the blue obstacles. SGT plan (blue) executed successfully, sequential
plan (green) collides with the obstacles. (c+d) RL results. In the domain (c) a robot needs to navigate
between the (hatched) obstacles. We show an example sub-goal tree (blue) and trajectory tracking
it using an inverse model (green). In (d) we show the approximate value ˆVk(s, g = [0.9, 0.9]) for
several values of k. Note how the reachable region to the goal (non-yellow) grows with k.
Severity (%)

Success Rate (%)
SGT
Sequential
0.946
Simple
0.541
0.266
Hard - 2G 0.013
0.247
Hard - 4G 0.011
Table 1: Results for IL experiments.

Prediction Times (Seconds)
Sequential
487.179
811.523
1052.62

SGT
28.641
52.22
53.539

Sequential
0.0327
0.0803
0.0779

SGT
0.0381
0.0666
0.0362

6 Experiments

We report our results for imitation learning and reinforcement learning in a motion planning scenario.

Imitation Learning Experiments: We compare the sequential and Sub-Goal Tree(SGT) ap-
proaches for BC. Consider a point-robot motion-planning problem in a 2D world, with two obstacle
scenarios termed simple and hard, as shown in Figure 2. In simple, the distribution of possible
motions from left to right is uni-modal, while in hard, at least 4 modalities are possible.

For both scenarios we collected a set of 111K (100K train + 10K validation + 1K test) expert
trajectories from random start and goal states using a state-of-the-art motion planner (OMPL’s[30]
Lazy Bi-directional KPIECE [29] with one level of discretization).

To account for different trajectory modalities, we chose a Mixture Density Network (MDN)[5] as the
parametric distribution of the predicted next state, both for the sequential and the SGT representations.
We train the MDN by maximizing likelihood using Adam [12]. To ensure the same model capacity
for both representations, we used the same network architecture, and both representations were
trained and tested with the same data. Since the dynamical system is Markovian, the current and goal
states are sufﬁcient for predicting the next state in the plan, so we truncated the state history in the
sequential model’s input to contain only the current state.

For simple, the MDNs had a uni-modal multivariate-Gaussian distribution, while for hard, we
experimented with 2 and 4 modal multivariate-Gaussian distributions, denoted as hard-2G, and hard-
4G, respectively. As we later show in our results the SGT representation captures the demonstrated
distribution well even in the hard-2G scenario, by modelling a bi-modal sub-goal distribution.

We evaluate the models using unseen start-goal pairs taken from the test set. To generate a trajectory,
we follow Algorithms 4 and 1, and connect the points using linear interpolation. We call a trajectory
successful if it does not collide with an obstacle en-route to the goal. For a failed trajectory, we
further measure the severity of collision by the percentage of the trajectory being in collision.

Table 1 summarizes the results for both representations. The SGT representation is superior in all
three evaluation criteria - motion planning success rate, trajectory prediction times (total time in
seconds for the 1K trajectory predictions), and severity. Upon closer look at the two hard scenarios,
the Sub Goal Tree with a bi-modal MDN outperforms sequential with 4-modal MDN, suggesting
that the SGT trajectory decomposition better accounts for multi-modal trajectories. Finally, the
supplementary section contains further experiments considering additional baselines for the BC case.

7

Average Distance to Goal Average Collision Rate

Sub-goal Tree + Inverse Model
Sub-goal Tree + Q-value controller
Q-value controller

0.13
0.29
0.58

Table 2: Results for RL-based controllers.

0.25
0.06
0.02

Batch RL Experiments: Next, we evaluate the approximate STDP algorithm. We consider a 2D
particle moving in an environment with obstacles, as shown in Figure 2c. The particle can move a
distance of 0.025 in one of the eight directions, and suffers a constant cost of 0.025 in free space,
and a large cost of 10 on collisions. The task is to reach from any starting point to within a 0.15
distance of any goal point without colliding. This simple domain is a continuous-state optimal control
Problem (2), and for start and goal positions that are distant, as shown in Figure 2c, it requires
long-horizoned planning, making it suitable for studying batch RL algorithms.

To generate data, we sampled states and actions uniformly and independently, resulting in 125K
(s, u, c, s(cid:48)) tuples. As function approximation, we opted for simplicity, and used K-nearest neighbors
(KNN) for all our experiments, with Kneighbors = 5. To solve the minimization over states in
approximate STDP, we discretized the state space and searched over a 50 × 50 grid of points.

A natural baseline in this setting is ﬁtted-Q iteration [7, 25]. We veriﬁed that for a ﬁxed goal, ﬁtted-Q
obtains near perfect results with our data. Then, to make it goal-conditioned, we used a universal
Q-function [28], requiring only a minor change in the algorithm (see supplementary for pseudo-code).

To evaluate the different methods, we randomly chose 200 start and goal points, and measured the
distance from the goal that the policies reach, and whether they collide with obstacles or not along the
way. For ﬁtted-Q, we used the greedy policy with respect to the learned Q function. The approximate
STDP method, however, does not automatically provide a policy. Thus, we experimented with two
methods for extracting a policy from the learned sub-goal tree. The ﬁrst is training an inverse model
fIM (s, s(cid:48)) – a mapping from s, s(cid:48) to u, using our data, and the same KNN function approximation.
To reach a sub-goal g from state s, we simply run fIM (s, g) until we are close enough to g (we set
the threshold to 0.15). An alternative method is ﬁrst using ﬁtted-Q to learn a goal-based policy, as
described above, and then running this policy on the sub-goals. The idea here is that the sub-goals
learned by approximate STDP can help ﬁtted-Q overcome the long-horizon planning required in this
task. Note that all methods use exactly the same data, and the same function approximation (KNN),
making for a fair comparison.

In Table 2 we report our results. Fitted-Q did not succeed in reaching any but the very closest
goals, resulting in a high average distance to goal. Approximate STDP, on the other hand, computed
meaningful sub-goals for almost all test cases, resulting in a low average distance to goal when
tracked by the inverse model. Figure 2 shows an example sub-goal tree and a corresponding tracked
trajectory. The ﬁtted-Q policy did learn not to hit obstacles, resulting in the lowest collision rate. This
is expected, as colliding leads to an immediate high cost, while the inverse model is not trained to
take cost into account. Interestingly, combining the ﬁtted-Q policy with the sub-goals improves both
long-horizon planning and short horizoned collision avoidance. In Figure 2d we plot the approximate
value function ˆVk for different k and a speciﬁc goal. Note how the reachable parts of the state space
to the goal expand with k.

7 Discussion

We have shown that by viewing a trajectory as a hierarchical composition of sub-goals, improved
learning and optimization algorithms can be derived. We believe that these ideas would be important
for robotics and autonomous driving, and other domains where fast predictions are important.

Our novel approach to RL based on the APSP problem raises many directions for future research.
The optimization over sub-goal states motivates an actor-critic approach, where an actor would learn
to predict the sub-goals. It is interesting whether our approach could be extended to high-dimensional
state observations, such as in goal-based video prediction. Intuitively, predicting some features of
sub-goals along the trajectory seems like a natural step before predicting the full video trajectory.
Finally, we believe that stochastic systems can also be handled by learning the probability of reaching
different sub-goals.

8

References

[1] A. F. Agarap. Deep learning using rectiﬁed linear units (relu). CoRR, abs/1803.08375, 2018.
[2] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,
O. P. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information
Processing Systems, pages 5048–5058, 2017.

[3] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from

demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.
[4] D. Bertsekas. Dynamic programming and optimal control: Volume II. 2005.

[5] C. M. Bishop. Mixture density networks. Technical report, Citeseer, 1994.

[6] V. Dhiman, S. Banerjee, J. M. Siskind, and J. J. Corso. Floyd-warshall reinforcement learning
learning from past experiences to reach new goals. arXiv preprint arXiv:1809.09318, 2018.
[7] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal

of Machine Learning Research, 6(Apr):503–556, 2005.

[8] S. Gu, E. Holly, T. Lillicrap, and S. Levine. Deep reinforcement learning for robotic manipula-
tion with asynchronous off-policy updates. In 2017 IEEE international conference on robotics
and automation (ICRA), pages 3389–3396. IEEE, 2017.

[9] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal. Dynamical movement
primitives: learning attractor models for motor behaviors. Neural computation, 25(2):328–373,
2013.

[10] D. Jayaraman, F. Ebert, A. A. Efros, and S. Levine. Time-agnostic prediction: Predicting

predictable video frames. In ICLR, 2019.

[11] L. P. Kaelbling. Learning to achieve goals. In IJCAI, pages 1094–1099. Citeseer, 1993.
[12] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[13] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,

2014.

[14] J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. The

International Journal of Robotics Research, 32(11):1238–1274, 2013.

[15] G. Konidaris, S. Kuindersma, R. Grupen, and A. Barto. Robot learning from demonstration by
constructing skill trees. The International Journal of Robotics Research, 31(3):360–375, 2012.
[16] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of machine learning

research, 4(Dec):1107–1149, 2003.

[17] S. M. LaValle. Planning algorithms. Cambridge university press, 2006.
[18] A. McGovern and A. G. Barto. Automatic discovery of subgoals in reinforcement learning

using diverse density. 2001.

[19] I. Menache, S. Mannor, and N. Shimkin. Q-cut—dynamic discovery of sub-goals in rein-
forcement learning. In European Conference on Machine Learning, pages 295–306. Springer,
2002.

[20] N. Mishra, P. Abbeel, and I. Mordatch. Prediction and control with temporal segment models.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages
2459–2468. JMLR. org, 2017.

[21] K. Mülling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking
movements in robot table tennis. The International Journal of Robotics Research, 32(3):263–
279, 2013.

[22] J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural

networks, 21(4):682–697, 2008.

[23] D. A. Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In NIPS, pages

305–313, 1989.

[24] A. H. Qureshi, M. J. Bency, and M. C. Yip. Motion planning networks. arXiv preprint

arXiv:1806.05767, 2018.

9

[25] M. Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient neural reinforce-
ment learning method. In European Conference on Machine Learning, pages 317–328. Springer,
2005.

[26] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction
to no-regret online learning. In Proceedings of the fourteenth international conference on
artiﬁcial intelligence and statistics, pages 627–635, 2011.

[27] S. J. Russell and P. Norvig. Artiﬁcial Intelligence - A Modern Approach (3. internat. ed.).

Pearson Education, 2010.

[28] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In

International conference on machine learning, pages 1312–1320, 2015.

[29] I. A. ¸Sucan and L. E. Kavraki. Kinodynamic motion planning by interior-exterior cell explo-

ration. In Algorithmic Foundation of Robotics VIII, pages 449–464. Springer, 2009.

[30] I. A. ¸Sucan, M. Moll, and L. E. Kavraki. The Open Motion Planning Library. IEEE Robotics &
Automation Magazine, 19(4):72–82, December 2012. http://ompl.kavrakilab.org.

[31] J. Sung, S. H. Jin, and A. Saxena. Robobarista: Object part based transfer of manipulation
trajectories from crowd-sourcing in 3d pointclouds. In Robotics Research, pages 701–720.
Springer, 2018.

[32] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press

Cambridge, 1998.

[33] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999.

[34] J. N. Tsitsiklis and B. Van Roy. Regression methods for pricing complex american-style options.

IEEE Transactions on Neural Networks, 12(4):694–703, 2001.

[35] H. Zhang, E. Heiden, R. Julian, Z. He, J. J. Lim, and G. S. Sukhatme. Auto-conditioned recurrent
mixture density networks for complex trajectory generation. arXiv preprint arXiv:1810.00146,
2018.

10

A Imitation Learning - Full Experimental Settings

In this appendix we summarize all the technical details required to reproduced the imitation learning
results discussed in Section 6. We start by providing a detailed description of our neural network
structure and training process (we use the same structure for all the scenarios and all the representation
variants).

The network consists of 4 Fully-Connected hidden layers connecting the inputs with the outputs (the
outputs are the distribution parameters deﬁned by the MDN according to the scenario: 1, 2 and 4
multivariate-Gaussians for the simple, hard-2N and hard-4N respectively). We used Rectiﬁed-Linear
Units[1] (ReLU) as our activation function.

The training procedure used Adam[13] with a batch of 50, and a learning rate of 0.001 that decays by
a factor of 0.8 after the validation loss does not improve after 6 consecutive test times. We also deﬁned
a minimal learning rate of 0.00001. We also deﬁned gradient clipping - gradients with L2-norm
greater 200 are re-normalized to 200.

We further wanted to emphasize the following details:

1. We always connect the an endpoint to the nearest endpoints. Basically, this means that for
the sequential model we always connect the last prediction to the goal state. This allows
us to circumvent the issue that sequential models have with not reaching to the goal state
directly.

2. The models reported in the main text continue to make prediction even after collision. This
gives them the chance to "recover" by minimizing the distance traveled in collision. This
also allows us to have the severity metric to further analyze which model is beneﬁcial for
which scenarios.

3. Early stopping: we use the 10K validation trajectories to evaluate the loss of the model
during training for trajectories not included in the training set. Every time we discover a
model that scores a lower validation loss we save it. The resulting model is the one with
lowest validation loss.

4. Sampling: our models are a mixture of Gaussians, however, during test time we are only
interested in the safest trajectory a model can ﬁnd. Thus, to sample from the distribution, we
sample a mode (i.e. a speciﬁc Gaussian from the mixture) and we take the mean prediction
for that Gaussian.

B Behavioral Cloning Algorithms

In this appendix we explicitly write the BC algorithms used in Section 3 that are not in the main text.
We start with the algorithms for the sequential approach. Algorithm 3 ﬁrst provides a sequential
training method based on Stochastic Gradient Descent. Next, Algorithm 4, provides a prediction
algorithm that predicts the next state conditioned on the current prediction and the goal.

Next, we explicitly describe the pseudo-code for training a Sub-Goal Tree in Algorithm 5, and we also
provide an inference pseudo-code that emphasizes parallelism as opposed to the recursive version
provided in the main text (Algorithm 6).

C Baseline RL Algorithms

In Algorithm 7 we present a goal-based versions of ﬁtted-Q iteration [7] using universal function
approximation [28], which we used as a baseline in our experiments.

Next, in Algorithm 8 we present an approximate dynamic programming version of Floyd-Warshall
RL [11] that corresponds to the batch RL setting we investigate. This algorithm was not stable in our
experiments, as the value function converged to zero for all states (when removing the self transition
ﬁtting in line 7 of the algorithm, the values converged to a constant value).

11

Algorithm

Input: dataset D = (cid:8)τi = si
Initialize parameters θ for parametric distribution Pˆπ(sm|s, g; θ)
for i : 1...M do

(cid:9)N
i=1, train steps M , batch size B

1 . . . si
Ti

0, si

Sample batch of size B, each sample: τi ∼ D, st ∼ τi
g ← Get goal state according to τi for all items in batch
ht−1 ← Get history of st according to τi for all items in batch
Update θ by minimizing the negative log-likelihood loss:

L = −

1
B

·

B
(cid:88)

b=1

(cid:53)θ log Pˆπ(sb

t|hb

t−1, gb; θ)

Algorithm 3: Sequential BC SGD-Based Training

end
return θ

Algorithm

Input: parameters θ of parametric distribution Pˆπ, start state s, goal state g, max steps K
Initialize empty list L
Append s to L
for k : 1...K do

Predict next state s ∼ Pˆπ(s|h, g; θ)
/* can add a stopping condition if required

*/

Append s to L

end
return L

Algorithm 4: Sequential BC Trajectory Prediction

Algorithm

Input: dataset D = (cid:8)τi = si
Initialize parameters θ for parametric distribution Pˆπ(sm|s, g; θ)
for i : 1...M do

(cid:9)N
i=1, train steps M , batch size B

1 . . . si
Ti

0, si

Sample batch of size B, each sample: τi ∼ D, s1, s2 ∼ τi
sm ← Get midpoint of [s1, s2] according to τi for all items in batch
Update θ by minimizing the negative log-likelihood loss:

1

2

3

4

5

6

1

2

3

4

5

6

1

2

3

4

5

L = −

1
B

·

B
(cid:88)

b=1

(cid:53)θ log Pˆπ(sb

m|sb

1, sb

2; θ)

end
return θ

Algorithm 5: Sub-Goal Tree BC SGD-Based Training

12

1

2

3

4

5

6

7

8

9

1

2

3

4

5

1

2

3

4

5

6

7

8

Algorithm

Input: parameters θ of parametric distribution Pˆπ, start state s, goal state g, max depth K
Initialize empty lists qnext, qcurrent
Put segment [s, g] in qcurrent
for k : 1...K do

/* execute while body in parallel for all items in qcurrent

*/

while qcurrent not empty do

Pop segment [s1, s2] from qcurrent
Predict midpoint sm ∼ Pˆπ(sm|s1, s2; θ)
Put segment [s1, sm] and [sm, s2] to qnext

end
qcurrent ← qnext
Empty qnext

end
return segments in qcurrent

Algorithm 6: Sub-Goal Tree BC Trajectory Prediction (Parallel version)

Algorithm

Input: dataset D = {s, u, c, s(cid:48)}, Goal reached threshold δ
Create transition data set Dtrans = {s, u, s(cid:48)} and targets Ttrans = {c} with s, s(cid:48), c taken from
D
Fit ˆQ(s, u, s(cid:48)) to data in Dtrans
for k : 1...K do

Create random goal data set Dgoal = {s, u, g} and targets

(cid:110)(cid:110)

Tgoal =

c(s, u) + min

u(cid:48)

ˆQ(s(cid:48), u(cid:48), g)11||s(cid:48)−g||>δ

(cid:111)(cid:111)

with s, u, s(cid:48) taken from D and g randomly chosen from states in D
Fit ˆQ(s, u, s(cid:48)) to data in Dgoal

end

Algorithm 7: Fitted Q with Universal Function Approximation

Algorithm

Input: dataset D = {s, u, c, s(cid:48)}, Maximum path cost Cmax
Create transition data set Dtrans = {s, s(cid:48)} and targets Ttrans = {c} with s, s(cid:48), c taken from D
Create random transition data set Drandom = {s, srand} and targets Trandom = {Cmax} with
s, srand randomly chosen from states in D
Create self transition data set Dself = {s, s} and targets Tself = {0} with s taken from D
Fit ˆV (s, s(cid:48)) to data in Dtrans, Drandom, Dself
for k : 1...K do

Create random goal and mid-point data set Dgoal = {s, g} and targets
(cid:110) ˆV (s, g), ˆV (s, sm) + ˆV (sm, g)

Tgoal =

min

(cid:110)

(cid:111)(cid:111)

with s, sm, g randomly chosen from states in D
Create self transition data set Dself = {s, s} and targets Tself = {0} with s taken from D
Fit ˆV (s, s(cid:48)) to data in Dgoal, Dself

end

Algorithm 8: Approximate Floyd Warshall

13

D Additional Baseline for BC

In this section we provide some additional baselines to the supervised learning experiments described
in the main text in Section 6. We answer these questions:

1. What is the contribution of the iterative conditioning in the SGT approach?
2. If the model stops predicting when a collision is encountered (but the prediction are still

connected to subsequent endpoint) how would this affect the results?

D.1 Contribution of the iterative prediction of the Sub-Goal Tree

Each representation discussed, produces for trajectory generation a list of segments’ endpoints. The
advantage of SGT over the sequential approach, is that the order and conditioning on those endpoints
allows a concurrent computation to predict exponentially faster. Taking this view to the extreme,
we design the direct representation, which attempts to predict all intermediate state of the trajectory
directly from the start and goal states. The advantage of this method is that given a machine capable
of computing all the endpoints in parallel, we obtain a prediction model that generates a trajectory in
O(1) time units.

the

and the index of

Keeping the same notations as in Section 3,

trajectory distribution to endpoints predictions

in the direct approach we
that depend
to predict,
Pˆπ(s0, s1, . . . sT |s, g; θ) = Pˆπ(s1|s, g, 1; θ)Pˆπ(s2|s, g, 2; θ) . . . Pˆπ(sT |s, g, T ; θ).
=
τi∼Dπ∗ (cid:2)log Pˆπ(si

Model:
directly decompose
only on the start s and goal states g,
namely:
Applying this decomposition on Eq.3 and taking the
arg maxθ E
This model can be trained in an SGD algorithm similar to the SGT Algorithm 5. The only difference
is that the model should now consider more than the mid-state as the target (or label) for the loss
function. For instance, if our goal is to create an algorithm that predicts 64 segments, we should
train such model to predict on 63 intermediate states in every trajectory τi in the data. Inference for
such an algorithm is straight-forward: given s and g, we generate a prediction for every index and
concatenate.

log results
2|si, gi, 2; θ) · · · + log Pˆπ(si

in:
T |si, gi, T ; θ)(cid:3).

1|si, gi, 1; θ) + log Pˆπ(si

the endpoint

θ∗

We investigate the trade-offs between fast prediction and accuracy in the experiments at the end of
this appendix. Our ﬁndings suggest that the model is much faster but far less accurate than SGT (due
to the different decompositions assumed by each model). As a ﬁnal note, one might combine both
approaches in order to extend the SGT from a binary-tree, to a k-ary-tree, predicting more endpoints
at each call of the recursive process. Complexity-wise this will only affect the base of the logarithm,
changing it from 2 to k, but we leave this investigation for future research.

D.2 Additional Imitation Learning Experiments

In this appendix we deﬁned the following variations of our supervised model:

1. Stop on collision policy: we wanted to investigate the performance trade-offs of the sequen-
tial and SGT representations of planning time vs. success rate. For this we deﬁne two model
variations sequential:stop-coll and SGT:stop-coll.

2. direct prediction - predicting intermediate states directly from the start and goal as described

previously.

We start with the analysis of the ’stop-on-collision’. Table 3 shows the results for the sequential:stop-
coll, and SGT:stop-coll approaches. As we can see, both methods are faster than their non-stopping
variants, although the SGT is only marginally faster. Moreover, we can see that the severity scores
do worsen. Both of these demonstrate the trade-off of accuracy vs. trajectory prediction times, and
motivate our selection that the variants that continue to predict after collision should be considered as
the proposed variants.

Next, we investigate the other proposed model, the direct prediction. The results of this model are in
Table 4. First, we see that indeed the trajectory prediction time is much shorter. Further, we can see
that the model is good in some easy cases, as it was able to predict better than the sequential model
for the easy scenario by a large margin. However, as the prediction problem becomes harder, it seems

14

Sequential:
stop-coll
Simple
0.541
Hard - 2G 0.014
Hard - 4G 0.009

Success Rate (%)
SGT:
stop-coll
0.946
0.255
0.232

Prediction Times (Seconds)
Sequential:
stop-coll
322.347
217.494
252.718

SGT:
stop-coll
28.542
39.925
53.58

Severity (%)

Sequential:
stop-coll
0.4551
0.5897
0.55

SGT:
stop-coll
0.0875
0.3479
0.0362

Table 3: Success rates, prediction times (seconds) and severity by representation type for all scenarios
for 1K test trajectories.

Success Rate (%)
0.716

Simple
Hard - 2G 0.0
Hard - 4G 0.004

Prediction Times (Seconds)
13.206
27.345
28.718

Severity (%)
0.0343
0.1172
0.1195

Table 4: Success rates, prediction times (seconds) and severity for the direct prediction approach for
all scenarios for 1K test trajectories.

the extra conditioning of the sequential prediction allows it to make better prediction as the direct
approach attains almost zero success rate.

15

