Springer Nature 2021 LATEX template

2
2
0
2

l
u
J

0
2

]
P
E
.
h
p
-
o
r
t
s
a
[

1
v
5
6
6
9
0
.
7
0
2
2
:
v
i
X
r
a

ExoSGAN and ExoACGAN: Exoplanet
Detection using Adversarial Training
Algorithms

Cicy K Agnes1*, Akthar Naveed V1* and Anitha Mary M O
Chacko2

1Division of Computer Science and Engineering, Cochin
University of Science and Technology, Kalamassery, Kochi,
682022, Kerala, India.
2Assistant Professor, Division of Computer Science and
Engineering, Cochin University of Science and Technology,
Kalamassery, Kochi, 682022, Kerala, India.

*Corresponding author(s). E-mail(s): cicykagnes@ug.cusat.ac.in;
akthercse2018@ug.cusat.ac.in;
Contributing authors: anithamarychacko@cusat.ac.in;

Abstract

Exoplanet detection opens the door to the discovery of new habitable
worlds and helps us understand how planets were formed. With the
objective of ﬁnding earth-like habitable planets, NASA launched Kepler
space telescope and its follow up mission K2. The advancement of obser-
vation capabilities has increased the range of fresh data available for
research, and manually handling them is both time-consuming and diﬃ-
cult. Machine learning and deep learning techniques can greatly assist in
lowering human eﬀorts to process the vast array of data produced by the
modern instruments of these exoplanet programs in an economical and
unbiased manner. However, care should be taken to detect all the exo-
planets precisely while simultaneously minimizing the misclassiﬁcation of
non-exoplanet stars. In this paper, we utilize two variations of generative
adversarial networks, namely semi-supervised generative adversarial net-
works and auxiliary classiﬁer generative adversarial networks, to detect
transiting exoplanets in K2 data. We ﬁnd that the usage of these mod-
els can be helpful for the classiﬁcation of stars with exoplanets. Both
of our techniques are able to categorize the light curves with a recall

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

ExoSGAN and ExoACGAN

and precision of 1.00 on the test data. Our semi-supervised technique
is beneﬁcial to solve the cumbersome task of creating a labeled dataset.

Keywords: exoplanets, photometric, detection, deep learning

1 Introduction

Exoplanets, also known as extrasolar planets, are planets that orbit stars out
side our solar system. For centuries humans have questioned if additional solar
systems exist among the billions of stars in the universe. Despite numerous
dubious claims of exoplanet discovery, Wolszczan and Frail are credited with
discovering the ﬁrst veriﬁed exoplanet in 1992. Exoplanet discovery is still in
its early phases. The hunt for exoplanets helps us comprehend planet forma-
tion and the discovery of Earth-like habitable planets. It aids in the collection
of statistical data and information about exoplanet atmospheres and composi-
tions, as well as their host stars. It can also help us understand how the solar
system formed. In total, ∼4884 exoplanets have been identiﬁed as of December
20211. The standard exoplanet detection techniques include direct imaging,
Doppler Spectroscopy , astrometry, microlensing, and the transit method [1].
Most of the known planets have been discovered by the transit method. If we
detect periodic drops in ﬂux intensities of the star when a planet passes in
front of it, we can conﬁrm that a planet transits around the star. In this paper
we use the transit method of analysing light curves. However, analysing the
ﬂux curve to ﬁnd them is a very tedious task because vast amounts of data
are produced from the observatories that are often noisy.

The search of exoplanets took a step forward with the help of observatories
such as Kepler [2], the CoRoT spacecraft [3], the Transiting Exoplanet Survey
Satellite (TESS) [4], and others [5–7]. Although the Kepler mission concluded
a few years ago and the data is well-systemised and publicly available, it is
still far from being completely put to use. These data can provide insights
that could pave the way for future discoveries in the coming decades. Usually
professional teams manually inspect the light curves for possible planet candi-
dates. They vote for each other to reach a ﬁnal decision after concluding their
study [8, 9]. To properly and quickly determine the presence of these planets
without any manual eﬀorts, it will be necessary to automatically and reliably
assess the chance that individual candidates are, in fact, planets, regardless of
low signal-to-noise ratios-the ratio of the expected dip of the transit to the pre-
dictable error of the observed average of the light intensities within the transit.
Furthermore, exoplanet classiﬁcation is an example of an imbalanced binary
classiﬁcation problem. The issue with the data is that the available number of
stars with exoplanets is far lesser than the number of stars with no exoplan-
ets. Fortunately, the advancements in machine learning help us to automate

1https://exoplanetarchive.ipac.caltech.edu/docs/counts detail.html

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

3

Fig. 1 Transit photometry: The ﬂux intensity of the star varies at diﬀerent point of time
as the planet moves around the star. The presence of periodic dips in the intensity curves
can conﬁrm the presence of an exoplanet.

the complex computational tasks of learning and predicting patterns in light
curves within a short time. In 2016, a robotic vetting program was developed
to imitate the human analysis employed by some of the planet candidate cat-
alogs in the Kepler pipeline [10]. The Robovetter project was a decision tree
meant to eliminate the false-positive ‘threshold crossing events’(TCEs) and to
determine whether there are planets having a size similar to that of the earth
present around any Sun-like stars. Likewise, in the past several other notable
attempts such as the autovetter [11] were made to propound the applications
of machine-learning methodologies in spotting exoplanets in Kepler light curve
data.

Since the introduction of the Astronet, a deep learning model that pro-
duced great results in automatic vetting of Kepler TCEs [12] the advantages
of using deep learning models for the classiﬁcation of the light curves are
more investigated. Despite the fact that deep learning methods are computa-
tionally expensive, since they oﬀer better results for complex problems, many
researchers attempt to shift their attention from classical machine learning

Springer Nature 2021 LATEX template

4

ExoSGAN and ExoACGAN

methods to deep learning methods. Also, as deep learning is a dynamic ﬁeld
that is constantly evolving, there are possibilities of new techniques other than
the conventional models that might prove to be more eﬃcient in exoplanet
detection. Also, as astronomical projects include long-term strategies and
are critical in educational, environmental and economical aspects, researches
should adhere to accuracy and ethics. The biased research could cynically
aﬀect the scientiﬁc training, education and ﬁnancing of future astronomers.
In exoplanet detection we should not falsify or leave out any potential exo-
planet candidates and fully harness the data available from the missions. Being
motivated by these, we propose two deep learning models, Semi Supervised
Generative Adversarial Networks (SGAN) and Auxiliary Classiﬁer Generative
Adversarial Networks (ACGAN), for the classiﬁcation of exoplanets. We call
these methods ExoSGAN and ExoACGAN respectively. They produce com-
parable and sometimes better classiﬁcation results to the techniques so far
used.

The paper is organized into six sections. Section 2 includes relevant works in
this ﬁeld. Section 3 contains an explanation of the methodologies utilized in this
article. Section 4 discusses the materials used, data preprocessing prior to cre-
ating our model along with the architecture used to create the models. Section
5 discusses the evaluation measures utilized and the ﬁndings obtained. The
study closes with a section that summarises the ﬁndings and future directions
in this research ﬁeld.

2 Related Works

A lot has happened in the realm of astrophysics as well as deep learning in
the last 25 years. Many researchers are investigating the use of deep learning
algorithms for exoplanet discovery. One of the widely used techniques for the
detection of extrasolar planets include Box-ﬁtting Least Squares(BLS)[13]. The
approach is based on the box-shape of recurrent light curves and takes into
account instances with low signal-to-noise ratios. They use binning to deal
with the massive number of observations. They point out that an appreciable
identiﬁcation of a planetary transit necessitates a signal-to-noise ratio of at
least 6. Cases that appear to be a good ﬁt are then manually assessed. However,
the method is exposed to the risk of false-positive detections generated by
random cosmic noise patterns.

A transit detection technique that makes use of the random forest algorithm
is the signal detection using random forest algorithm (SIDRA) [14]. SIDRA
was trained on a total of 5000 simulated samples comprising 1000 samples
from each class namely constant stars, transiting light curves, variable stars,
eclipsing binaries and microlensing light curves. 20,000 total samples from
these diﬀerent classes were tested where a success ratio of 91 percent is achieved
for transits and 95-100 percent for eclipsing binaries, microlensing, variables
and constant light curves. They recommended that SIDRA should only be

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

5

used in conjunction with the BLS method for detecting transiting exoplanets
since SIDRA had generated low results on transiting light curves.

In 2017, Sturrock et al [15] conducted a study on the use of the machine
learning methods namely, random forest (RF), k-nearest neighbour (KNN) and
Support Vector Machine (SVM) in classifying cumulative KOI data (Kepler
Objects of Interest). In terms of the fraction of observations identiﬁed as exo-
planets, SVM did not give the desired prediction results. The RF model has
the risk of overﬁtting the dataset. Inorder to reduce overﬁtting, measures such
as StratiﬁedShuﬄeSplit, feature reduction, cross validation etc had to be done.
It was found that random forest gave a cross-validated accuracy of 98 per-
cent which is the best among the other three methods. Using Azure Container
Instance and an API (application programming interface) the random forest
classiﬁer was made accessible to public.

In 2018, Shallue and Vanderburg [12] researched about identifying exoplan-
ets with deep learning. This is a notable work in the ﬁeld and is considered to be
the state of the art method. They used NASA Exoplanet Archive: Autovetter
Planet Candidate Catalog dataset and introduced a deep learning architec-
ture called Astronet that utilizes convolutional neural networks(CNN). There
are 3 separate input options to the network: global view, local view and both
the global and local views. These input representations were created by fold-
ing the ﬂattened light curves on the TCE period and binning them to produce
a 1 dimensional vector. During training they augmented the training dataset
by using random horizontal reﬂections. They used the google-vizier system
to automatically tune the hyperparameters including the number of bins, the
number of fully connected layers, number of convolutional layers, dropout
probabilities etc. After model optimization they used model averaging on inde-
pendent copies of the model with diﬀerent parameter initializations to improve
the performance by not making the model depend upon diﬀerent regions of
input space. The best convolutional model received AUC (Area Under the
Curve) of 0.988 and accuracy of 0.960 on the test set.

Astronet K2, a one-dimensional CNN with maxpooling, was trained in 2019
to discover planet candidates in K2 data [18]. This deep learning architecture is
adapted from the model created by Shallue and Vanderburg [12]. It resulted in
a 98 percent accuracy on the test set. EPIC 246151543 b and EPIC 246078672
b were discovered as genuine exoplanets by Astronet K2. They are both in
between the size range of the Earth and Neptune. Looking at the precision
and recall of Astronet K2, if the classiﬁcation threshold is set such that the
model oﬀers a recall of 0.9 in the test set of Kepler data, the precision rate
will only be 0.5. For Astronet K2 to estimate planetary occurrence rates in
K2 data the model should be enhanced to produce a recall of 90 percent while
simultaneously maintaining a precision of 95 percent because K2 data contains
more false positive samples.

Later in 2020, ’TSfresh’ library and ’lightbgm’ tool was used in exoplanet
detection by Malik et al[19] . An ensemble of decision trees called gradient
boosted trees (GBT) and 10-fold CV (cross validation) technique were used to

Springer Nature 2021 LATEX template

6

ExoSGAN and ExoACGAN

Table 1 Table summarizing the relevant works done in the ﬁeld of exoplanet discoveries

Year and author

Methodology

Results

Mislis et al, 2015 [14]

McCauliﬀ et al, 2015
[11]

Thompson et al, 2015
[16]

Presented SIDRA, a random
forest based algorithm for
detection and classiﬁcation of
light curves.

Presented
Autovetter
the
project: a random forest model
for classiﬁcation of exoplanets
using Kepler light curves

Dimensionality reduction and
k nearest neighbour technique
were used to detect transit sig-
nals in Kepler light curves

Armstrong et al, 2016
[17]

Employed
organizing
Self
maps and random forests to
rank transiting exoplanet can-
in Next generation
didates
transit survey (NGTS) data

Success ratio : 91 percent for
transiting exoplanets

Overall error rate - 5.85 per-
cent

the TCEs

eliminates over 90 percent
of
that do not
have transiting exoplanet sig-
nals while keeping over 99 per-
cent of the planet candidates
and almost 99 percent of tran-
sit signals injected into the
light curve signals

AUC score - 97.6 percent

Sturrock et al, 2018
[15]

Created
a Random forest
model to classify KOI data
and and deployed it to public

Resulted in a cross validated
Accuracy of 98 percent

Shallue and Vander-
burg, 2018 [12]

identify transit

signals
To
of exoplanets, a CNN model
named Astronet, trained on
Kepler light curves, was pro-
posed.

AUC - 0.988,
Accuracy 96 percent on Kepler
false positives. and Accuracy
94.9 on kepler planet candi-
dates.

Dattilo et al, 2019 [18]

Developed Astronet K2 : a 1-
d CNN model adapted from
Astronet and applied it to K2
data

Accuracy - 98 percent

Malik et al, 2021 [19]

validation

Built
a Gradient Boosted
Tree(GBT) and used 10-fold
cross
tran-
sit detection in Kepler and
TESS(NASA’s
Transiting
Exoplanet Survey Satellite)
data

for

Yip et al, 2020 [20]

Priyadarshini
Puri, 2021 [21]

and

Armstrong et al, 2020
[22]

used a GAN model to generate
a suitable dataset for discov-
ering exoplanets via the direct
imaging technique and trained
a CNN model
for the exo-
planet classiﬁcation task using
the new dataset.

a CNN based
Developed
ensemble model for K2 data
by using RF, SVM, Decision
trees and Multilayer Percep-
tron as base model and CNN
as metamodel

Demonstrated use of GPC
(Gaussian Process Classiﬁer)
supported by other machine
learning models
for Kepler
light curves

Kepler data: AUC-
Recall - 0.96, Precision - 0.82
TESS data: Recall - 0.82, Pre-
cision - 0.63

0.948,

When
signal-to-noise
ratio(SNR) of the training set
is 0.75, the accuracy on the
test set (over diﬀerent SNR) is
0.896 to 0.967

Accuracy - 99.62 percent

GPC model
: AUC - 0.999,
Precision - 0.984, Recall - 0.995

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

7

create a model to classify light curves into planet candidates and false positives.
Their prediction results in an AUC of 0.948, precision of 0.82 and recall of 0.96
in Kepler data. They provide comparable results to Shallue and Vanderburg
[12] and also prove that their method is more eﬃcient than the conventional
BLS( box least squares ﬁtting). However, the performance of this method was
poor on the class imbalanced TESS data.

Yip et al [20] used generative adversarial networks (GAN) to detect planets
through direct imaging technique. GAN was used to create a suitable dataset
which is further trained using convolutional neural networks classiﬁer that can
locate planets over a broad range of signal-to-noise ratios.

Most of the studies related to using artiﬁcial intelligence in exoplanet detec-
tion utilize the random forest algorithm [23, 24] and CNNs [25, 26]. Other
heuristic approaches to ﬁnd extrasolar planets using stellar intensities include
KNN [16] and self-organizing maps [17]. In 2021 Priyadarshini and Puri [21]
used an ensemble-CNN model on the Kepler light curves. Diﬀerent machine
learning algorithms such as Decision Tree, Logistic Regression, MLP (Mul-
tilayer Perceptron), SVM( Support Vector Machines, CNN, random forest
classiﬁer and their proposed ensemble-CNN model were implemented and com-
pared with each other. They used a stacked model that is trained similarly
as k-fold validation. The decision tree, RF, SVM, and MLP models were the
meta learners and CNN model was employed as base learner in their proposed
work. The model was capable to produce an accuracy of 99.62 percent. How-
ever training ensemble models can be expensive and hard to interpret since we
deal with light curves.

Furthermore, the use of semi supervised generative adversarial algorithm
(SGAN) have been proved to be eﬃcient in retrieving potential radio pulsar
candidates [27]. The study indicates that SGAN outperforms standard super-
vised algorithms in real world classiﬁcation. The best performing model in the
study gives an overall F-score of 0.992. This model has been already incor-
porated into the HTRU-S Lowslat survey post-processing pipeline, and it has
found eighteen additional pulsars.

Moreover, auxiliary classiﬁer generative adversarial networks (ACGAN)
have been useful in situations where imbalanced data is available for study.
For instance, Wang et al [28] developed a framework that gives improved
and balanced performance for detecting cardiac arrhythmias using a data
augmentation model and ACGAN. They also ﬁnd that ACGAN based data
augmentation framework gives better classiﬁcation results while addressing
imbalanced data. Additionally, Sutedja et al [29] suggested an ACGAN model
to tackle an imbalanced classiﬁcation issue of fraud detection in debit card
transaction data. The study also compares the classiﬁcation performance of
the ACGAN model used with that of a CNN model-having similar architecture
to the ACGAN discriminator model. They believe that because the ACGAN
model produces better outcomes, it may also be used to solve unbalanced
classiﬁcation issues.

Springer Nature 2021 LATEX template

8

ExoSGAN and ExoACGAN

Table 1 summarises some of the main works done in the ﬁeld so far. These
studies reveal that exoplanet detection issue has been approached using stan-
dard artiﬁcial intelligence techniques but it remains a challenge that deserves
more attention. Hardly any exploration has been done that reveals the use
of novel deep learning techniques in astronomy. We capitalize the usage of
the irrefutably appealing eﬃciency of semi-supervised generative adversarial
networks and auxiliary classiﬁer generative adversarial networks to tackle the
issue. These are already proved to be useful in biomedical applications and
other ﬁelds.

3 Methods

3.1 Generative Adverserial Networks

Goodfellow et al (2014) [30] introduced Generative Adversarial Networks in
2014. A generator G and a discriminator D are the two major components of
this machine learning architecture. Both of them are playing a zero-sum game
in which they are competing to deceive one other. The concept of the game
can be summarized roughly as follows: The generator generates images and
attempts to fool the discriminator into believing that the produced images
are real. Given an image, the discriminator attempts to identify whether it is
real or generated. The notion is that by playing this game repeatedly, both
players will improve, which implies that the generator will learn to produce
realistic images and the discriminator will learn to distinguish the fake from
the genuine.

G is a generative model which maps from latent space z to the data space X
as G(z; θ1) and tries to make the generative samples G(z) as close as possible
to the real data distribution Pdata(x). Pz(z) is the noise distribution. D [given
by D(x; θ2)] discriminates between the real samples of data(labeled 1) and the
fake samples (labeled 0) generated from G and outputs D(x) ∈ (0, 1). The loss
function of GAN can be derived from the loss function of binary cross entropy.

L(ˆy, y) = [y log ˆy + (1 − y) log(1 − ˆy)]

(1)

where y is the original data and ˆy is the reconstructed data. The labels from
Pdata(x) is 1. D(x) and D(G(z)) are the discriminator’s probabilities that x is
real and G(z) is real. Substituting them in equation 1 for real and fake samples,
we get the two terms log(D(x)) and log(1 − D(G(z)). Inorder to classify real
and fake samples D tries to maximize the two terms. Generator’s task is to
make D(G(z)) close to 1, therefore it should minimize the two terms above.
Thereby, they follow a two player minmax game. The loss function of GAN
can be expressed as follows,

min
G

max
D

V (D, G) = Ex∼Pdata(x)[ log D(x) ] + Ez∼Pz(z)[ log(1 − (D(G(z)) ]

(2)

E represents expectation.

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

9

Fig. 2 An overview of the GAN framework at a high level. The generator generates fake
samples. Discriminator accepts false and real data as input and returns the likelihood that
the provided input is real.

3.2 Semi-Supervised Generative Adversarial

Networks(SGAN)

An extension to GAN was made by Odena (2016) [31] called SGAN to learn
a generative model and a classiﬁcation job at the same time. Unlike the tradi-
tional GAN that uses sigmoid activation function to distinguish between real
and fake samples, SGAN uses softmax function to yield N+1 outputs. These
N+1 include the classes 1 to N and the fake class. Salimans et al (2016) [32]
presented a state of the art method for classiﬁcation using SGAN on MNIST,
CIFAR-10 and SVHN datasets at that time. In our case SGAN should pro-
duce outputs-exoplanets, non exoplanets and fake samples. Taking x as input,
a standard supervised classiﬁer produces the class probabilities (stars hav-
ing exoplanets and stars having no exoplanets). We achieve semi-supervised
learning here by providing the generated fake samples from G labeled y =
N+1 to the D and categorising them as fake samples thereby introducing an
additional dimension. Let the model predictive distribution be (Pmodel(y|x))
as in case of a standard classiﬁer where y is the label corresponding to x.
Pmodel(y = N + 1|x) gives the probability that x is fake. The loss function of
SGAN consists of an unsupervised and a supervised component.

L = Lunsupervised + Lsupervised

(3)

The unsupervised component has two parts. Given the data being real, one
minus the expectation of the model producing the result as fake constitutes
the ﬁrst part. The next part is the expectation of the model producing the
result as fake when the data is from G. We can notice that substituting D(x) =
1 − Pmodel(y = N + 1|x) into the Lunsupervised yields equation 2.

Lunsupervised = − Ex∼Pdata(x) log[1 − Pmodel(y = N + 1|x)]

+ Ex∼G log[Pmodel(y = N + 1|x)]

(4)

Springer Nature 2021 LATEX template

10

ExoSGAN and ExoACGAN

Fig. 3 Illustration of Architecture of ExoSGAN

If the data is from any of the N classes, the supervised loss component is the
negative log probability of the label.

Lsupervised = −Ex,y∼Pdata(x,y) log Pmodel(y|x, y < K + 1)

(5)

Therefore, the loss function of SGAN can be given as

L = − Ex,y∼Pdata(x,y)[log Pmodel(y|x)]
− Ex∼G[log Pmodel (y = N + 1|x)]

(6)

The output of the softmax does not change when a common function f(x) is
subtracted from each of the classiﬁer’s output logits. So if we ﬁx the logits of the
N+1th class to 0, the value doesn’t change. Finally if Z(x) = (cid:80)k
k=1 exp[lk(x)]
is the normalized sum of the exponential outputs, the discriminator can be
given as

D(x) =

Z(x)
Z(x) + 1

(7)

3.3 Auxillary Classiﬁer Generative Adversarial

Networks(ACGAN)

Mirza and Osindero [33] presented conditional GAN (CGAN), an enhancement
to GAN, to conditionally generate samples of a speciﬁc class. The noise z
along with class labels c are given as input to generator of CGAN to generate
fake samples of a speciﬁc class (Xf ake = G(c, z)). This variation improves
the stability and speed while training GAN. The discriminator of CGAN is
fed with class labels as one of the inputs. ACGAN is a CGAN extension in

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

11

Fig. 4 Illustration of Architecture of ExoACGAN

which instead of inputting class labels into D, class labels are predicted. It was
introduced by Odena et al [34]. The discriminator of ACGAN can be thought
of as having two classiﬁcation functions: one that predicts whether the input is
real or false(probability P (S|X)), and another that classiﬁes the samples into
one of the classes(probability P (C|X)). LC, the log-likelihood of the correct
class and LS, the log likelihood of the source make up the two parts of the loss
function of ACGAN.

LS = E[log P (S = real|Xreal)] + E[log P (S = f ake|Xf ake)]

(8)

LC = E[log P (C = c|Xreal)] + E[logP (C = c|Xf ake)]
(9)
The generator and discriminator play a minmax game on LS as in case of
normal GAN. They try to maximise Lc. In contrast to the conditional GAN,
the resultant generator learns a latent space representation irrespective of the
class label.

4 Data Sources and Implementation

4.1 Dataset

Kepler Mission

NASA began the Kepler mission on March 7, 2009, to study stars and hunt
for terrestrial planets, particularly habitable planets with liquid water. Two
of Kepler’s four reaction wheels malfunctioned by May 11, 2013. At least
three of them should be in good shape to keep the spaceship pointed in the
right direction. Because the damage could not be repaired, the K2 -”Second
Light”- was initiated (February 4, 2014), which takes advantage of Kepler’s
remaining capabilities. K2 conducted ecliptic-pointed ‘Campaigns’ of 80 days
duration. The light intensities were recorded every 30 minutes. These stellar

Springer Nature 2021 LATEX template

12

ExoSGAN and ExoACGAN

Fig. 5 Plot of the light curves of a few stars from the dataset with exoplanets and without
exoplanets-before preprocessing. X-axis gives the time and Y-axis shows the ﬂux intensities.
The curves are not normalized and standardized. Also note the presence of huge outliers.

light intensities are used to look for dips in order to detect any probable exo-
planets transiting the star. A total of 477 conﬁrmed planets were found by K2
as of December 2021. We use data from K2’s third campaign, which began
on November 12th, 2014, for this study. A few samples from other campaigns
were also used to enhance the number of stars containing exoplanets. How-
ever, Campaign 3 accounts for nearly all of the data. Almost 16,000 stars were
included in the ﬁeld-of-view(FOV) of the third Campaign. Kepler’s data was
made public by NASA via the Mikulski Archive2. We used the open-sourced
data from Kaggle [35] which was created by transposing the PDC-SAP-FLUX
column(Pre-search Data Conditioning) of the original FITS ﬁle.

Train and Test data:

The train set includes 5087 observations, each with 3198 features. The class
labels are listed in the ﬁrst column. Label 2 denotes stars having exoplanets,
whereas label 1 denotes stars without exoplanets. The same is true for the test
set. The following columns 2-3198 show the intensity of light emitted by the
stars at equal time intervals. As the Campaign lasted 80 days, the ﬂux points
were taken around 36 minutes apart (3197/80). There are 545 samples in the
test data, 5 of which contain exoplanets. These are the unseen data used to
test the method’s outcomes. We have renamed the class labels, 1 for exoplanet-
stars and 0 for nonexoplanet-stars for convenience. Figure 5 illustrates few
samples from train data.

2https://archive.stsci.edu/missions-and-data/kepler

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

13

Fig. 6 Plot of the light curves of a few stars with exoplanets and without exoplanets-after
preprocessing. X-axis shows the frequency and Y-axis plots the ﬂux intensity. The curves are
normalized and standardized as well as converted into frequency domain. The major upper
outliers are also removed.

4.2 Data Pre-processing

1. The data contains no duplicates and null values as it has already been

de-noised by NASA.

2. When we take a look at the data, we notice that there are many outliers in
intensity values. Since we search for dips, it is important not to eliminate
outlier clusters caused by transits that naturally belong to the target star.
So we remove a tiny fraction of the upper outliers and replace them with
mean of adjacent points. Later Fast Fourier Transform, followed by gaussian
ﬁlter is applied to convert the raw ﬂux from temporal to frequency domain
and smoothen the curve.

3. As the features are in various ranges, we should normalize the data such
that the row values range between -1 and 1. We may also standardize the
data by using StandardScaler to ensure the column values have a standard
deviation of one. Figure 6 illustrates the light curves after the preprocessing
steps.

4.3 Model Architecture and Training : ExoSGAN

The generator G takes noise from a random normal distribution (standard
deviation 0.02) in the latent space and generates fake ﬂux curves, which are
one of three inputs to D. The true dataset, which comprises the ﬂux curve as
well as its labels, is the next input to D. The ﬁnal set of inputs comes from
real samples as well, but this time they are unlabeled.

Springer Nature 2021 LATEX template

14

ExoSGAN and ExoACGAN

A convolutional neural network with two output layers, one with loss func-
tion binary cross entropy and the other with sparse categorical cross entropy,
makes up the discriminator network. The second output layer uses a softmax
activation function to predict the stars with and without exoplanets while the
ﬁrst output layer uses sigmoid activation to ﬁnd the realness of the data. In the
generator a transpose of 1 dimensional convolutional layer followed by dense
layers is used. We also use BatchNormalisation and LeakyReLu with a slope
of 0.2. A tanh activation function is employed in the output layer.

We get a better performance model when we increase the number of labeled
samples to 148 (74 positive and 74 negative samples). To slightly increase the
positive labeled real samples we reverse the order of the ﬂux points preserving
the shape and add them to the original dataset. Now the train dataset contains
74 light curves with exoplanets and 5050 light curves with no exoplanets. Still
the imbalance remains the same with ratio 1:100 exoplanet and non exoplanet
stars.

The training procedure of GAN includes holding D constant while training
G and vice versa. We use a learning rate η = 4e − 6 and β1 = 0.5. G is
trained via D and Discriminator D is a stacked model with shared weights
where the results can be reused. The unsupervised model is stacked on top of
the supervised model before the softmax activation function where the former
takes output from the latter. Figure 3 shows the SGAN model architecture
used in this paper and Table 2 gives the hyperparameters used in Exo-SGAN
model.

Table 2 Hyperparameters used in SGAN model

Operation

Generator

Dense
Transposed 1-d Convolution
Dense
Dense
Dense

Discriminator

Transposed 1-d Convolution
Max-Pooling 1d
Transposed 1-d Convolution
Max-Pooling 1d
Dense
Dense
Dense
Dense
Lambda

Kernel

Strides

Feature Map

BN? Dropout

Non-linearity

N/A
7 × 7
N/A
N/A
N/A

7 × 7
2 × 2
7 × 7
2 × 2
N/A
N/A
N/A
N/A
N/A

N/A
2 × 2
N/A
N/A
N/A

1 × 1
1 × 1
1 × 1
1 × 1
N/A
N/A
N/A
N/A
N/A

800
128
1065
1598
3197

32
32
32
64
128
64
32
2
1

NO
YES
YES
YES
NO

NO
NO
NO
NO
NO
NO
NO
NO
NO

0.0
0.0
0.0
0.0
0.0

0.0
0.25
0.0
0.25
0.0
0.2
0.0
0.0
0.0

NO
LeakyRelu
LeakyRelu
LeakyRelu
tanh

LeakyRelu
-
LeakyRelu
-
LeakyRelu
LeakyRelu
LeakyRelu
softmax
custom-activation

Generator Optimizer
Discriminator Optimizer
Batch size
iteration
LeakyReLu slope
Activation noise Standerd deviation

Adam (η = 4e − 6, β1 = 0.5)
Adam (η = 4e − 5, β1 = 0.5)
32
3200
0.1
[0.2]

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

15

4.4 Model Architecture and training: ExoACGAN

In the architecture adopted here, we feed random noise and the class labels
of exoplanet and non-exoplanet light curves into the generator, which sub-
sequently produces synthetic data. The discriminator receives created ﬂux
intensity curves as well as real ﬂux intensity curves as input. It guesses if the
data is authentic or bogus and distinguishes between light curves with and
without exoplanets.

The discriminator is implemented as a 1-dimensional convolutional network
with input shape(3197,1). The learning rate of D is set to η = 4e − 5 while
that of G is η = 4e − 6. Similar to Exo-SGAN, D has two output layers where
the ﬁrst predicts real/fake class and the second gives the probability of the
stars having exoplanets. The generator model takes in latent space with 100
dimensions and the single integer class labels. The latent vector is given to a
dense layer and later reshaped. We use an embedding layer in G to feature
map the class labels with a dimension of 10 (arbitrary). This can then be
interpreted by a dense layer. Afterwards, the tensors formed by noise and class
labels are concatenated producing an additional channel and sent through a 1-
dimensional convolutional transpose layer, which is ﬂattened and then passed
through 3 dense layers later to ﬁnally produce curves of 3197 ﬂux points.Also,
LeakyRelu and Batch normalisation are used to provide regularization. The
output layer is designed with a hyperbolic tangent activation function. ACGAN
also uses a composite model like SGAN where the generator is trained via the
discriminator. Figure 4 shows the SGAN model architecture used in this paper
and Table 3 gives the hyperparameters used in Exo-SGAN model.

Table 3 Hyperparameters used in ACGAN model

Operation

Generator

Dense
Embedding
Dense
Concatenate
Transposed 1-d Convolution
Dense
Dense
Dense

Discriminator

Transposed 1-d Convolution
Transposed 1-d Convolution
Transposed 1-d Convolution
Transposed 1-d Convolution
Transposed 1-d Convolution
Dense
Dense

Kernel

Strides

Feature Map

BN? Dropout Non-linearity

N/A
N/A
N/A
N/A
7 × 7
N/A
N/A
N/A

7 × 7
7 × 7
7 × 7
7 × 7
7 × 7
N/A
N/A

N/A
N/A
N/A
N/A
2 × 2
N/A
N/A
N/A

2 × 2
2 × 2
2 × 2
2 × 2
2 × 2
N/A
N/A

500
10
500
N/A
128
1065
1598
3197

32
64
128
128
128
1
2

NO
NO
NO
NO
YES
YES
YES
NO

YES
YES
YES
YES
YES
NO
NO

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

0.0
0.0
0.0
0.0
0.8
0.0
0.0

NO
NO
Relu
NO
LeakyRelu
LeakyRelu
LeakyRelu
tanh

LeakyRelu
LeakyRelu
LeakyRelu
LeakyRelu
LeakyRelu
sigmoid
softmax

Generator Optimizer
Discriminator Optimizer
Batch size
iteration
LeakyReLu slope
Activation noise Standerd deviation

Adam (η = 4e − 6, β1 = 0.5)
Adam (η = 4e − 5, β1 = 0.5)
32
3200
0.2
[0.2]

Springer Nature 2021 LATEX template

16

ExoSGAN and ExoACGAN

5 Evaluation

5.1 Metrics used for evaluation

1. Confusion Matrix: The summarization of the classiﬁcation performed by
Exo-SGAN and Exo-ACGAN can be given by a confusion matrix. It pro-
vides information not only on the faults produced, but also on the sorts
of errors made. For binary classiﬁcation, it is a 2x2 matrix of actual and
predicted positive and negative classiﬁcations as given in Table 4: where,

TP : True positive (A star with exoplanet predicted as a star with

exoplanet)

FP: False positive (A star without exoplanet predicted as a star with

exoplanet)

FN: False negative (A star without exoplanet predicted as star with

exoplamet)

TN: True negative (A star without exoplanet predicted as a star without

exoplanet)

2. Accuracy: When evaluating a model for classiﬁcation issues, accuracy is
frequently used. As the name implies, accuracy is the number of right pre-
dictions divided by the total number of predictions. An accuracy of 1.00
means that all of the samples were properly categorized. However, in this
situation, as the data we have is very imbalanced, accuracy alone cannot be
used to evaluate the performance of the models since an 99 accuracy of 0.99
might also suggest that all of the samples are placed into the majority class.

Accuracy =

T P + T N
T P + F P + T N + F N

3. Precision: Precision seeks to address the issue of how many positive iden-
tiﬁcations were truly correct. A precision of 1 indicates that the model
produced no FP. To ﬁnd precision we use the formula

P recision =

T P
T P + F P

.

Precision is the ratio between the true positives to the total of true positives
and false positives.

Table 4 Confusion matrix for exoplanet classiﬁcation

Predicted
Negative Positive

l Negative
a
u
t
c
A

Positive

TP

FN

FP

TN

Total

TP+FP

FN+TN

Total

TP+FN

FP+TN

N

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

17

4. Recall: The proportion of positive samples recovered is referred to as recall.
It is a useful metric when we need to accurately categorise all of the positive
samples. Sensitivity is another term for recall.

Recall =

T P
T P + F N

5. Speciﬁcity: Speciﬁcity is found by dividing the true negatives by the total
number of actual negative samples. Speciﬁcity answers the question of how
many stars with no exoplanets did the model accurately predict.

Specif icity =

T N
T N + F P

6. F-beta score: The harmonic mean of precision and recall is used to get the
F-score. F-beta is an abstraction of the F-score in which the balance of
precision and recall in the computation of the harmonic mean is regulated
by a coeﬃcient called β. In the computation of the score, a beta value
greater than 1.0, such as 2.0, provides more weight to recall and less weight
to precision.

Fβ =

(1 + β2)(P recision × recall)
β2P recision + Recall

F-2 Score can be obtained by substituting β = 2

F2 =

5

4
P recision

+

1
Recall

5.2 Results and Discussion

The ExoSGAN and ExoACGAN models produce encouraging results in the
classiﬁcation of exoplanets. It should be noted that to the best of our knowl-
edge semi-supervised generative adversarial networks and auxiliary classiﬁer
generative adversarial networks have not yet been used for the classiﬁcation of
exoplanet stars. Based on our ﬁndings, we believe that these approaches are
extremely promising for detecting exoplanets from light curves.

We argue that, among the assessment criteria described above, recall is the
most important since we should not overlook any stars with exoplanets. As a

Table 5 Confusion matrix of Exo-SGAN model on training dataset

Predicted
without exoplanet with exoplanet

l without exoplanet
a
u
t
c
A

with exoplanet

Total

5032
0

5032

18
74

92

Total

5050
74

5124

Springer Nature 2021 LATEX template

18

ExoSGAN and ExoACGAN

Table 6 Confusion matrix of Exo-ACGAN model on training dataset

Predicted
with exoplanet without exoplanet

with exoplanet
without exoplanet

l
a
u
t
c
A

Total

5041
0

5041

9
74

83

Total

5050
74

5124

Table 7 Confusion matrix of Exo-SGAN and Exo-ACGAN model on testing data

Predicted
without exoplanet with exoplanet

Total

l without exoplanet
a
u
t
c
A

with exoplanet

Total

565
0

565

0
5

5

565
5

190

Table 8 Results of the performance of Exo-SGAN model on train and test data

Exo-SGAN Training Testing

Accuracy
Precision
Recall
Speciﬁcity
F-score

.996
0.804
1.00
0.996
0.954

100.0
1.00
1.00
1.00
1.00

result, we trained our models to provide the highest possible recall. Maximum
recall is at the expense of precision. A model with high recall may have low
precision. This is known as the precision-recall trade-oﬀ. In our case, having a
few stars without exoplanets labelled as stars with exoplanets can be compro-
mised if we can properly categorise each star with exoplanets. It is important to
remember that the data is extremely biased with a majority of non exoplanet
candidate stars. Therefore as mentioned earlier, accuracy cannot be consid-
ered as a proper metric for evaluation. As we place greater emphasis on recall,
we increase the weight of recall in the F-score and set the beta value to 2.

The models, both ExoSGAN and ExoACGAN, output probabilities of a
star containing exoplanet. The default threshold for such a classiﬁcation task
is often set at 0.5. Since we deal with imbalanced data and aim to take out
all the lighcurves containing the transit, we should optimize the threshold. We
choose our threshold to be 0.81 for the semi-supervised classiﬁcation model
and 0.77 for the auxiliary classiﬁer generative adversarial network. Both of
these models give us a perfect recall of 1.00 on the train and test set for the
chosen threshold. We get a precision of 0.802 from Exo-SGAN and 0.892 from
Exo-ACGAN on the train data. While training, Exo-ACGAN results in an
accuracy of 99.8 whereas Exo-SGAN gives an accuracy of 99.6. When we test

Springer Nature 2021 LATEX template

Table 9 Results of the performance of Exo-SGAN model on train and test data

ExoSGAN and ExoACGAN

19

Exo-ACGAN Training Testing

Accuracy
Precision
Recall
Speciﬁcity
F-score

.998
0.892
1.00
0.998
.976

100.0
1.00
1.00
1.00
1.00

Fig. 7 A comparison of the outcomes of each of the Exo-SGAN and Exo-ACGAN
assessment metrics using train data

our models on unseen data both Exo-ACGAN and Exo-SGAN give a perfect
accuracy of 100 percent. All the 565 non-exoplanet stars and 10 exoplanet
stars in the test set are classiﬁed correctly. Therefore, the precision, recall,
speciﬁcity, and f-score turns out to be 1.00 for both Exo-SGAN and Exo-
ACGAN on test data as shown in Table 8 and 9. On the train samples, out of
5050 non-exoplanet stars only 8 have been misclassiﬁed by Exo-ACGAN, and
only 18 have been misclassiﬁed by Exo-SGAN. The confusion matrix in Table
5, 6 and 7 tabulates the classiﬁcation results of our experiments.

When we compare Exo-SGAN and Exo-ACGAN, we can ﬁnd that ACGAN
marginally surpasses Exo-SGAN. Both have a recall of one, but the latter has a
gain of 0.2, 0.09, and 0.09 in accuracy, precision, and speciﬁcity while training.
It can be speculated that during the training and testing procedure of Exo-
ACGAN, the adversarial training technique assisted in reducing the impact of
the imbalanced dataset. As a result, the prejudice towards the majority class
is reduced. The discriminator model is taught to learn patterns from both
actual and generated data in an adversarial manner. As a consequence, in
order to generalize the patterns from the training dataset, the discriminator
model learnt a greater number of richer patterns.

Springer Nature 2021 LATEX template

20

ExoSGAN and ExoACGAN

Fig. 8 A comparison of the outcomes of each of the Exo-SGAN and Exo-ACGAN
assessment metrics using test data

6 Conclusions and Future works

In this paper, we utilise the Semi-Supervised Generative Adversarial frame-
work and Auxilary Classiﬁer Generative Adversarial framework to detect
exoplanets using the ﬂux curves of stars. The performance of both methods
is noteworthy since both of them properly detect all lightcurves with tran-
sit. Exo-SGAN and Exo-ACGAN both has a recall rate of 100. The capacity
to use unlabeled candidates to get better outcomes is the key advantage of
our proposed network Exo-SGAN. We anticipate that, as the number of exo-
planet candidates increase and maintaining a big labelled dataset becomes
more diﬃcult, this method will become even more beneﬁcial for future exo-
planet detection. Even though the data is extremely unbalanced, Exo-ACGAN
and Exo-SGAN are able to produce better results even without the use of any
upsampling approaches like SMOTE.

As for the next steps, these architectures can be tried out on the
raw lightcurves from the Kepler mission, provided Graphical Processing
Units(GPU) and hardware of great performance since a large amount of
data must be handled. The hyperparameters can also be tuned to give bet-
ter performance. Additionally, experimenting with various data-preprocessing
approaches for eliminating outliers may improve the performance. One method
to attempt in order to solve the class imbalance issue is to optimize the loss
function to produce data only from the minority class while training [36]. This
can bring a balance between the samples of both classes. Yet another tech-
nique to improve the classiﬁcation accuracy is the use of Bad GAN [37] in
which the generator’s goal is more focused to produce data that complements
the discriminator’s data.

However, our models can provide a very reliable system for ﬁnding all the
true positives in our data at the current stage. It is hoped that these deep

Springer Nature 2021 LATEX template

learning approaches would pave the way for a new era in the ﬁeld of exoplanet
hunting.

ExoSGAN and ExoACGAN

21

Declarations

We declare that this manuscript is original, has not been published before, and
is not currently being considered for publication elsewhere.
• Funding: No funding was received for conducting this study.
• Conﬂict of interest: The authors declare no conﬂict of interest.
• Ethics approval: NA
• Consent to participate: NA
• Authors’ contributions: Conceptualization: Cicy K Agnes; Methodology:
Akthar Naveed V; Formal analysis and investigation: Cicy K Agnes, Akthar
Naveed V; Project adimistration and Validation: Cicy K Agnes; Supervi-
sion: Anitha Mary M O Chacko; Writing - original draft preparation: Cicy
K Agnes; Writing - review and editing: Anitha Mary M O Chacko, Akthar
Naveed V;

References

[1] Fischer, D.A., Howard, A.W., Laughlin, G.P., Macintosh, B., Mahade-
van, S., Sahlmann, J., Yee, J.C.: Exoplanet detection techniques. Pro-
tostars and Planets VI (2014). https://doi.org/10.2458/azu uapress
9780816531240-ch031

[2] Borucki, W.J.: KEPLER Mission: development and overview. Reports
on Progress in Physics 79(3), 036901 (2016). https://doi.org/10.1088/
0034-4885/79/3/036901

[3] Grziwa, S., P¨atzold, M., Carone, L.: The needle in the haystack:
searching for transiting extrasolar planets in CoRoT stellar light
curves. Monthly Notices of the Royal Astronomical Society 420(2),
1045–1052
https://arxiv.org/abs/https://academic.oup.
com/mnras/article-pdf/420/2/1045/3059183/mnras0420-1045.pdf.
https://doi.org/10.1111/j.1365-2966.2011.19970.x

(2012)

[4] Ricker, G.R., Winn, J.N., Vanderspek, R., Latham, D.W., Bakos, G. ´A.,
Bean, J.L., Berta-Thompson, Z.K., Brown, T.M., Buchhave, L., But-
ler, N.R., Butler, R.P., Chaplin, W.J., Charbonneau, D., Christensen-
Dalsgaard, J., Clampin, M., Deming, D., Doty, J., De Lee, N., Dressing,
C., Dunham, E.W., Endl, M., Fressin, F., Ge, J., Henning, T., Holman,
M.J., Howard, A.W., Ida, S., Jenkins, J.M., Jernigan, G., Johnson, J.A.,
Kaltenegger, L., Kawai, N., Kjeldsen, H., Laughlin, G., Levine, A.M., Lin,
D., Lissauer, J.J., MacQueen, P., Marcy, G., McCullough, P.R., Morton,
T.D., Narita, N., Paegert, M., Palle, E., Pepe, F., Pepper, J., Quirren-
bach, A., Rinehart, S.A., Sasselov, D., Sato, B., Seager, S., Sozzetti,

Springer Nature 2021 LATEX template

22

ExoSGAN and ExoACGAN

A., Stassun, K.G., Sullivan, P., Szentgyorgyi, A., Torres, G., Udry, S.,
Villasenor, J.: Transiting Exoplanet Survey Satellite (TESS). Journal of
Astronomical Telescopes, Instruments, and Systems 1, 014003 (2015).
https://doi.org/10.1117/1.JATIS.1.1.014003

[5] Collier Cameron, A., Pollacco, D., Street, R.A., Lister, T.A., West, R.G.,
Wilson, D.M., Pont, F., Christian, D.J., Clarkson, W.I., Enoch, B., Evans,
A., Fitzsimmons, A., Haswell, C.A., Hellier, C., Hodgkin, S.T., Horne, K.,
Irwin, J., Kane, S.R., Keenan, F.P., Norton, A.J., Parley, N.R., Osborne,
J., Ryans, R., Skillen, I., Wheatley, P.J.: A fast hybrid algorithm for
exoplanetary transit searches. Monthly Notices of the Royal Astronomical
Society 373(2), 799–810 (2006) https://arxiv.org/abs/astro-ph/0609418
[astro-ph]. https://doi.org/10.1111/j.1365-2966.2006.11074.x

[6] Hartman, J.D., Bakos, G. ´A., Torres, G.: Blend analysis of hatnet transit
candidates. EPJ Web of Conferences 11, 02002 (2011). https://doi.org/
10.1051/epjconf/20101102002

[7] Maciejewski, G., Fern´andez, M., Aceituno, F. J., Ohlert, J., Puchalski, D.,
Dimitrov, D., Seeliger, M., Kitze, M., Raetz, St., Errmann, R., Gilbert, H.,
Pannicke, A., Schmidt, J.-G., Neuh¨auser, R.: No variations in transit times
for qatar-1 b. A&A 577, 109 (2015). https://doi.org/10.1051/0004-6361/
201526031

[8] Crossﬁeld, I.J.M., Guerrero, N., David, T., Quinn, S.N., Feinstein, A.D.,
Huang, C., Yu, L., Collins, K.A., Fulton, B.J., Benneke, B., Peterson,
M., Bieryla, A., Schlieder, J.E., Kosiarek, M.R., Bristow, M., Newton, E.,
Bedell, M., Latham, D.W., Christiansen, J.L., Esquerdo, G.A., Berlind,
P., Calkins, M.L., Shporer, A., Burt, J., Ballard, S., Rodriguez, J.E.,
Mehrle, N., Dressing, C.D., Livingston, J.H., Petigura, E.A., Seager, S.,
Dittmann, J., Berardo, D., Sha, L., Essack, Z., Zhan, Z., Owens, M.,
Kain, I., Isaacson, H., Ciardi, D.R., Gonzales, E.J., Howard, A.W., de,
M.C.J.V.: VizieR Online Data Catalog: Variable stars and cand. planets
from K2 (Crossﬁeld+, 2018). VizieR Online Data Catalog, 239–5 (2018)

[9] Yu, L., Vanderburg, A., Huang, C., Shallue, C.J., Crossﬁeld, I.J.M.,
Gaudi, B.S., Daylan, T., Dattilo, A., Armstrong, D.J., Ricker, G.R.,
Vanderspek, R.K., Latham, D.W., Seager, S., Dittmann, J., Doty, J.P.,
Glidden, A., Quinn, S.N.: VizieR Online Data Catalog: Automated triage
and vetting of TESS candidates (Yu+, 2019). VizieR Online Data Catalog,
158–25 (2019)

[10] Coughlin, J.L., Mullally, F., Thompson, S.E., Rowe, J.F., Burke, C.J.,
Latham, D.W., Batalha, N.M., Oﬁr, A., Quarles, B.L., Henze, C.E.,
Wolfgang, A., Caldwell, D.A., Bryson, S.T., Shporer, A., Catanzarite,
J., Akeson, R., Barclay, T., Borucki, W.J., Boyajian, T.S., Campbell,

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

23

J.R., Christiansen, J.L., Girouard, F.R., Haas, M.R., Howell, S.B., Huber,
D., Jenkins, J.M., Li, J., Patil-Sabale, A., Quintana, E.V., Ramirez,
S., Seader, S., Smith, J.C., Tenenbaum, P., Twicken, J.D., Zamu-
dio, K.A.: Planetary Candidates Observed by Kepler. VII. The First
Fully Uniform Catalog Based on the Entire 48-month Data Set (Q1-
Q17 DR24). The Astrophysical Journal Supplement Series 224(1), 12
(2016) https://arxiv.org/abs/1512.06149 [astro-ph.EP]. https://doi.org/
10.3847/0067-0049/224/1/12

[11] McCauliﬀ, S.D., Jenkins, J.M., Catanzarite, J., Burke, C.J., Cough-
lin, J.L., Twicken, J.D., Tenenbaum, P., Seader, S., Li, J., Cote, M.:
AUTOMATIC CLASSIFICATION OFKEPLERPLANETARY TRAN-
SIT CANDIDATES. The Astrophysical Journal 806(1), 6 (2015). https:
//doi.org/10.1088/0004-637x/806/1/6

[12] Shallue, C.J., Vanderburg, A.: Identifying exoplanets with deep learning:
A ﬁve-planet resonant chain around kepler-80 and an eighth planet around
kepler-90. The Astronomical Journal 155(2), 94 (2018). https://doi.org/
10.3847/1538-3881/aa9e09

[13] Kov´acs, G., Zucker, S., Mazeh, T.: A box-ﬁtting algorithm in the
search for periodic transits. Astronomy and Astrophysics 391, 369–
377 (2002) https://arxiv.org/abs/astro-ph/0206099 [astro-ph]. https://
doi.org/10.1051/0004-6361:20020802

[14] Mislis, D., Bachelet, E., Alsubai, K.A., Bramich, D.M., Parley, N.: sidra:
a blind algorithm for signal detection in photometric surveys. Monthly
Notices of the Royal Astronomical Society 455(1), 626–633 (2015) https:
//arxiv.org/abs/https://academic.oup.com/mnras/article-pdf/455/1/
626/3084442/stv2333.pdf. https://doi.org/10.1093/mnras/stv2333

[15] Sturrock, G.C., Manry, B., Raﬁqi, S.: Machine learning pipeline for
exoplanet classiﬁcation. SMU Data Science Review 2(1) (2018) https:
//arxiv.org/abs/https://scholar.smu.edu/datasciencereview/vol2/iss1/9

[16] Thompson, S.E., Mullally, F., Coughlin, J., Christiansen, J.L., Henze,
C.E., Haas, M.R., Burke, C.J.: A Machine Learning Technique to Iden-
tify Transit Shaped Signals. The Astrophysical Journal 812(1), 46
(2015) https://arxiv.org/abs/1509.00041 [astro-ph.EP]. https://doi.org/
10.1088/0004-637X/812/1/46

[17] Armstrong, D.J., Pollacco, D., Santerne, A.: Transit shapes and self-
organizing maps as a tool for ranking planetary candidates: application
to Kepler and K2. Monthly Notices of the Royal Astronomical Soci-
ety 465(3), 2634–2642 (2016) https://arxiv.org/abs/https://academic.
oup.com/mnras/article-pdf/465/3/2634/8420208/stw2881.pdf. https://
doi.org/10.1093/mnras/stw2881

Springer Nature 2021 LATEX template

24

ExoSGAN and ExoACGAN

[18] Dattilo, A., Vanderburg, A., Shallue, C.J., Mayo, A.W., Berlind, P.,
Bieryla, A., Calkins, M.L., Esquerdo, G.A., Everett, M.E., Howell, S.B.,
et al.: Identifying exoplanets with deep learning. ii. two new super-earths
uncovered by a neural network in k2 data. The Astronomical Journal
157(5), 169 (2019). https://doi.org/10.3847/1538-3881/ab0e12

[19] Malik, A., Moster, B.P., Obermeier, C.: Exoplanet detection using
machine learning. Monthly Notices of the Royal Astronomical Soci-
https://arxiv.org/abs/https://academic.oup.com/mnras/
ety
advance-article-pdf/doi/10.1093/mnras/stab3692/41836763/stab3692.
pdf. https://doi.org/10.1093/mnras/stab3692. stab3692

(2021)

[20] Yip, K.H., Nikolaou, N., Coronica, P., Tsiaras, A., Edwards, B., Changeat,
Q., Morvan, M., Biller, B., Hinkley, S., Salmond, J., Archer, M., Sumption,
P., Choquet, E., Soummer, R., Pueyo, L., Waldmann, I.P.: Pushing the
limits of exoplanet discovery via direct imaging with deep learning. In:
Brefeld, U., Fromont, E., Hotho, A., Knobbe, A., Maathuis, M., Robardet,
C. (eds.) Machine Learning and Knowledge Discovery in Databases, pp.
322–338. Springer, Cham (2020)

[21] Priyadarshini, I., Puri, V.: A convolutional neural network (cnn) based
ensemble model for exoplanet detection. Earth Science Informatics 14(2),
735–747 (2021). https://doi.org/10.1007/s12145-021-00579-5

[22] Armstrong, D.J., Gamper, J., Damoulas, T.: Exoplanet validation with
machine learning: 50 new validated Kepler planets. Monthly Notices
of the Royal Astronomical Society 504(4), 5327–5344 (2020) https:
//arxiv.org/abs/https://academic.oup.com/mnras/article-pdf/504/4/
5327/37975376/staa2498.pdf. https://doi.org/10.1093/mnras/staa2498

[23] Caceres, G.A., Feigelson, E.D., Babu, G.J., Bahamonde, N., Christen, A.,
Bertin, K., Meza, C., Cur´e, M.: Autoregressive planet search: Application
to the kepler mission. The Astronomical Journal 158(2), 58 (2019). https:
//doi.org/10.3847/1538-3881/ab26ba

[24] Schanche, N., Cameron, A.C., H´ebrard, G., Nielsen, L., Triaud,
A.H.M.J., Almenara, J.M., Alsubai, K.A., Anderson, D.R., Arm-
strong, D.J., Barros, S.C.C., Bouchy, F., Boumis, P., Brown, D.J.A.,
Faedi, F., Hay, K., Hebb, L., Kiefer, F., Mancini, L., Maxted,
P.F.L., Palle, E., Pollacco, D.L., Queloz, D., Smalley, B., Udry,
S., West, R., Wheatley, P.J.: Machine-learning approaches to exo-
planet transit detection and candidate validation in wide-ﬁeld ground-
based surveys. Monthly Notices of the Royal Astronomical Society
483(4), 5534–5547 (2018) https://arxiv.org/abs/https://academic.oup.
com/mnras/article-pdf/483/4/5534/27496899/sty3146.pdf. https://doi.
org/10.1093/mnras/sty3146

Springer Nature 2021 LATEX template

ExoSGAN and ExoACGAN

25

[25] Ansdell, M., Ioannou, Y., Osborn, H.P., Sasdelli, M., Smith, J.C., Cald-
well, D., Jenkins, J.M., R¨aissi, C., Angerhausen, D., and: Scientiﬁc domain
knowledge improves exoplanet transit classiﬁcation with deep learning.
The Astrophysical Journal 869(1), 7 (2018). https://doi.org/10.3847/
2041-8213/aaf23b

[26] Chaushev, A., Raynard, L., Goad, M.R., Eigm¨uller, P., Armstrong, D.J.,
Briegal, J.T., Burleigh, M.R., Casewell, S.L., Gill, S., Jenkins, J.S.,
Nielsen, L.D., Watson, C.A., West, R.G., Wheatley, P.J., Udry, S., Vines,
J.I.: Classifying exoplanet candidates with convolutional neural networks:
application to the Next Generation Transit Survey. Monthly Notices of the
Royal Astronomical Society 488(4), 5232–5250 (2019) https://arxiv.org/
abs/1907.11109 [astro-ph.EP]. https://doi.org/10.1093/mnras/stz2058

[27] Balakrishnan, V., Champion, D., Barr, E., Kramer, M., Sengar, R., Bailes,
M.: Pulsar candidate identiﬁcation using semi-supervised generative
adversarial networks. Monthly Notices of the Royal Astronomical Society
505(1), 1180–1194 (2021). https://doi.org/10.1093/mnras/stab1308

[28] Wang, P., Hou, B., Shao, S., Yan, R.: Ecg arrhythmias detection
using auxiliary classiﬁer generative adversarial network and residual net-
work. IEEE Access 7, 100910–100922 (2019). https://doi.org/10.1109/
ACCESS.2019.2930882

[29] Sutedja, I., Heryadi, Y., Wulandhari, L.A., Abbas, B.: Imbalanced data
classiﬁcation using auxiliary classiﬁer generative adversarial networks.
International Journal of Advanced Trends in Computer Science and
Engineering 9(2) (2020) https://arxiv.org/abs/http://www.warse.org/
IJATCSE/static/pdf/ﬁle/ijatcse26922020.pdf. https://doi.org/10.30534/
ijatcse/2020/26922020

[30] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
D., Ozair, S., Courville, A., Bengio, Y.: Generative Adversarial Networks
(2014)

[31] Odena, A.: Semi-Supervised Learning with Generative Adversarial Net-

works (2016)

[32] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A.,

Chen, X.: Improved Techniques for Training GANs (2016)

[33] Mirza, M., Osindero, S.: Conditional Generative Adversarial Nets (2014)

[34] Odena, A., Olah, C., Shlens, J.: Conditional Image Synthesis With

Auxiliary Classiﬁer GANs (2017)

[35] Dataset: Kaggle, Kepler Labelled Time Series Data. https://www.kaggle.

Springer Nature 2021 LATEX template

26

ExoSGAN and ExoACGAN

com/keplersmachines/kepler-labelled-time-series-data. Accessed: 2021-
09-27 (2017)

[36] Zhou, T., Liu, W., Zhou, C., Chen, L.: Gan-based semi-supervised for
imbalanced data classiﬁcation. In: 2018 4th International Conference on
Information Management (ICIM), pp. 17–21 (2018). https://doi.org/10.
1109/INFOMAN.2018.8392662

[37] Dai, Z., Yang, Z., Yang, F., Cohen, W.W., Salakhutdinov, R.: Good Semi-

supervised Learning that Requires a Bad GAN (2017)

