2
2
0
2

l
u
J

5

]

M
D
.
s
c
[

1
v
7
8
0
2
0
.
7
0
2
2
:
v
i
X
r
a

Learning to Accelerate Approximate Methods for
Solving Integer Programming via Early Fixing

Longkang Li, Baoyuan Wu, Member, IEEE

1

Abstract—Integer programming (IP) is an important but challenging problem. Approximate methods have shown promising performance
on solving the IP problem. However, we observed that a large fraction of variables solved by some iterative approximate methods
ﬂuctuate around their ﬁnal converged discrete states in very long iterations. It implies that these approximate methods could be
signiﬁcantly accelerated by early ﬁxing these ﬂuctuated variables to their converged states, while not signiﬁcantly harming the solution
quality. To this end, we propose an innovative framework of learning to early ﬁx variables along with the approximate method. Speciﬁcally,
we formulate the early ﬁxing process as a Markov decision process, and train it using imitation learning, where a policy network evaluates
the posterior probability of each free variable concerning its discrete candidate states in each block of iterations. Extensive experiments
on three typical IP applications are conducted, including constrained linear programming, MRF energy minimization and sparse
adversarial attack, covering moderate and large-scale IP problems. The results demonstrate that our method could not only signiﬁcantly
accelerate the previous approximate method up to over 10 times in most cases, but also produce similar or even better solutions. The
implementation of our method is publicly available at https://github.com/SCLBD/Accelerated-Lpbox-ADMM.

Index Terms—Integer programming, learning to accelerate, early ﬁxing, imitation learning.

(cid:70)

1 INTRODUCTION

I NTEGER programming (IP) is an important and challeng-

ing problem in many ﬁelds, such as machine learning
[1] and computer vision [2]. IP can be a versatile modeling
tool for discrete or combinatorial optimization problems
with a wide variety of applications, and thus has attracted
considerable interests from the theory and algorithm design
communities over the years [3]. There are rich literature and
a wide range of developed methods and theories for solving
IP. Generally, we could divide them into two categories: exact
methods and approximate methods. Some exact methods are
widely utilized, such as branch-and-bound [4] , cutting plane
[5] and branch-and-cut [6] methods. The branch-and-bound
method [4] is an approach that partitions the feasible solution
space into smaller subsets of solutions. The cutting-plane
method [5] is any of a variety of optimization methods that
iteratively reﬁne a feasible set or objective function by means
of linear inequalities, termed cuts. The branch-and-cut [6]
method combines branch-and-bound and the cutting plane
method. These exact methods are able to get the optimal
solutions, however, they are usually suffering from time-
consuming issues due to the repeated solving of relaxed
linear problems. Thereafter, in the recent years, more and
more research focuses on the approximate methods, where a
feasible solution is obtained within the limited time. Linear
relaxation [7] relaxes the binary constraints x ∈ {0, 1} to
the box constraints x ∈ [0, 1]. Spectral relaxation [8] relaxes
the binary constraint to the (cid:96)2-ball, leading to a non-convex
constraint. As regard to the SDP relaxation [9], the binary

•

Longkang Li and Baoyuan Wu are with the School of Data Science, The
Chinese University of Hong Kong, Shenzhen, China and also with Secure
Computing Lab of Big Data, Shenzhen Research Institute of Big Data,
Shenzhen, China.

• Correspondence to Baoyuan Wu (wubaoyuan@cuhk.edu.cn).

constraints are substituted with a positive semi-deﬁnite
matrix constraint, i.e., X (cid:60) 0.

Besides the above mentioned relaxation-based approxi-
mate methods for integer programming, recently there has
been increasing attention to another type of approximate
methods, which is iterative and based on the alternating
direction method of multipliers (ADMM) [10]. ADMM is
a powerful algorithm for distributed convex optimization,
with an attempt to blend the beneﬁts of dual decomposition
[11] and augmented Lagrangian methods [12]. ADMM
coordinates the solutions of small local subproblems to ﬁnd
a solution of the large global problem. Many variants of
ADMM have been proposed with different purposes of better
acceleration, convergence, or stability, and have been applied
to solving different types of optimization tasks. Bethe ADMM
[13] was proposed for tree decomposition based parallel
MAP inference, which used an inexact ADMM augmented
with a Bethe-divergence based proximal function. Bregman
ADMM [14] was then proposed, which provided a uniﬁed
framework for ADMM. Bregman ADMM then has a number
of variants such as generalized ADMM and inexact ADMM.
Linearized ADMM [15] [16] was also proposed for convex
optimization. One state-of-the-art ADMM method for solving
IP is (cid:96)p-Box ADMM [17], where the binary constraints are
equivalently replaced by the intersection of a box constraint
and a (cid:96)p-norm sphere constraint.

Towards those ADMM based approximate methods, we
observed that regarding the approximate methods a large
fraction of variables ﬂuctuated around theirs ﬁnal converged
states in very long iterations. In Fig. 1, we solve a constrained
linear programming instance with 500 variables by (cid:96)p-box
ADMM, which converges after 7827 iterations. The left ﬁgure
illustrates the objective changes with respect to the iterations.
And we can see that the convergence reaches after a long
ﬂuctuation. In the right ﬁgure, We introduce "Flip number"

 
 
 
 
 
 
2

Fig. 1. A constrained linear programming instance with 500 variables is solved by (cid:96)p-box ADMM, converging after 7827 iterations. Left: We record
how the objective changes with respect to the iterations. Right: a ﬂip histogram. We use "Flip number" to evaluate the iterating stableness of the
variable. For one variable, if the values of two adjacent iterations go across 0.5, we call it a ’Flip’. When converged, each variable gets its Flip number.
We build the percentage histogram of all these 500 variables according to their Flip number, where the minimum is 0, the maximum is 450 Flips, and
the horizontal axis has 5 Flips as a interval. The results show that 59.6% of variables have [0,5) Flips, among which 34.0% have 0 Flip. We also
present 4 different variable iterating processes, corresponding to 0, 20, 90, 450 Flip(s).

to evaluate the iterating stableness of the variable. For one
variable, if the values of two adjacent iterations go across 0.5,
we call it a ’Flip’. For example, the the variable value at t-th
iteration is 0.9 and that at t + 1-th iteration is 0.3, thus it is
a Flip. Each variable corresponds to one Flip number, the
smaller the Flip number, the more stable the variable iterating.
According to the Flip histogram, 59.6% of variables have [0,5)
Flips, among which 34% have 0 Flip. Most variables has small
Flip numbers. To that end, we believe that a large proportion
of variables are ﬂuctuating around their ﬁnal converged
states (0 or 1) within small ranges. Currently, one solution
for algorithmic acceleration is to early stop the iterations
[18] [19]. However, early stopping has two shortcomings:
1). There is trade-off between the objective efﬁciency and
the runtime effectiveness, when stopping much earlier the
objective accuracy may decrease more. 2). It always use the
whole set of variables to consider whether and when to stop.

Inspired by this observation, we were thinking: why not
take every single variable independently and then asynchronously
ﬁx them instead of early stopping all of the variables at one
iteration? To the end, we propose an early ﬁxing framework,
which aims to accelerate the approximate method by early
ﬁxing these ﬂuctuated variables to their converged states
while not signiﬁcantly harming the converged performance.
Fig. 2 shows the comparison between early stopping and
early ﬁxing. And there are mainly three differences: Firstly,
early stopping does only consider the depth of optimization,
i.e., the number of iterations, while early ﬁxing also thinks
over the width of optimization, i.e., the dimension of
variables. Secondly, early stopping regards the set of variables
as a whole, while early ﬁxing treats every single variable
independently. And thirdly, decisions on whether to early
stop are made in every single iteration, while those for early
ﬁxing are once every block of iterations, i.e., β iterations.

Under our proposed early ﬁxing framework, in each
block of iterations, given the iterative values of the variables
within the past β iterations, a policy network will evaluate
the posterior probability of each variable concerning all
discrete candidate states. If the posterior probability with
respect to one state exceeds the ﬁxing threshold, then the
action of ﬁxing this variable to that discrete state will be
conducted, and this variable will not be updated in later
iterations; otherwise, no ﬁxing action will be conducted
and this variable will be further updated. Speciﬁcally, for

each variable, the continuously iterative values within the
past β iterations are sequential according to the time se-
ries. Recently, the Transformer structure [20] has exhibited
powerful performance in the sequential networks though
the multi-headed attention (MHA) mechanism. Herein, we
incorporate the attention layers in our policy network. When
solving a problem with early ﬁxing framework, one block of
iterations only decides a proportion of variables to conduct
early ﬁxing, thus the process is episodic until termination.
We can regard the solving process as a Markov decision
process [21] and train it using imitation learning [22]. Since
the input of policy network only requires the iterative values
of variables, without any other constraint information, thus
our early ﬁxing framework can be versatile enough, available
to all the IP problems of any orders or types, no matter linear
or quadratic, constrained or unconstrained.

In this paper, in order to accelerate the ADMM based
approximate methods for solving the IP problems, especially
improving the scalability of the IP problems, we propose the
early ﬁxing framework combined with learning techniques.
The main contributions of this paper are four-fold:

(i) To the best of our knowledge, we are the ﬁrst to
propose an early ﬁxing framework to accelerate the
approximate methods, where the variables are treated
independently with one another and we can asyn-
chronously ﬁx them according to their continuously
iterative values within the past series of iterations.
Once ﬁxed, the variables will not be further updated.
And free variables will continue iterating and updat-
ing.

(ii) We formulate the whole early ﬁxing process when
accelerating solving an IP problem as a Markov deci-
sion process, and train it using behaviour cloning as a
method of imitation learning. We also incorporate the
weighted binary cross-entropy (WBCE) loss during
the training.

(iii) We adopt the learning techniques with the attention
layers in our policy network, to decide whether to ﬁx
the variable or not.

(iv) We apply our proposed early ﬁxing framework to
three different IP applications: constrained linear
programming, MRF energy minimization and sparse
adversarial attack. The former one is linear IP prob-
lem, while the latter two are quadratic IP problems.

Flip number0500PercentageVariable (a): 0 FlipIterationsIterationsIterationsIterationsVariable (b): 20 FlipsVariable (c): 90 FlipsVariable (d): 450 Flips100200300400Variable valueVariable valueVariable valueVariable valueIterationsObjective3

Fig. 2. Comparison between Early Stopping (Left) and Early Fixing (Right): 1) Early stopping only considers the depth (iteration steps), while early
ﬁxing also thinks over the width (variable dimensions). 2) Early stopping regards the set of variables as a whole, while early ﬁxing treats every single
variable independently. 3) Decisions on whether to early stop are made in every iteration, while those for early ﬁxing are once every block of iterations.

We extend the problem scale from regular size to sig-
niﬁcantly large size. The extensive experiments reveal
the competitiveness of our early ﬁxing framework:
the runtime speeds up signiﬁcantly, while the solution
quality does not degrade much, even in some cases it
is available to obtain better solutions.

The rest of this paper is organized as follows. Section
2 brieﬂy reviews the related work. Some background in-
formation is given in Section 3. The details of our early
ﬁxing framework are described in Section 4. Section 5-7 show
the extensive applications of our proposed framework in
the areas of constrained linear programming, MRF energy
minimization and sparse adversarial attack, respectively. We
present the conclusion in Section 8.
This is the testing a a δ ∈ ≥. R

2 RELATED WORK
Integer programming There are rich literature and a wide
range of developed methods and theories for solving IP.
Generally, we could divide them into two categories: exact
methods and approximate methods. Some exact methods
are widely utilized, such as branch-and-bound [4] , cutting
plane [5] and branch-and-cut [6] methods. The branch-and-
bound method [4] is an approach that partitions the feasible
solution space into smaller subsets of solutions. The cutting-
plane method [5] is any of a variety of optimization methods
that iteratively reﬁne a feasible set or objective function
by means of linear inequalities, termed cuts. The branch-
and-cut [6] method combines branch-and-bound and the
cutting plane method. These exact methods are able to get
the optimal solutions, however, they are much too slow in
the runtime due to the repeated solving of relaxed linear
problems. In order to obtain feasible solutions within the
given time, some approximate methods are proposed. Linear
relaxation [7] relaxes the binary constraints x ∈ {0, 1} to
the box constraints x ∈ [0, 1]. Spectral relaxation [8] relaxes
the binary constraint to the (cid:96)2-ball, leading to a non-convex
constraint. As regard to the SDP relaxation [9], the binary
constraints are substituted with a positive semi-deﬁnite
matrix constraint, i.e., x (cid:60) 0.

Integer programming by ADMM Besides the above men-
tioned relaxation-based approximate methods for integer
programming, recently there has been increasing attention
to another type of approximate methods, which is iterative
and based on the alternating direction method of multipliers
(ADMM) [10]. ADMM is a powerful algorithm for distributed
convex optimization. With an attempt to blend the beneﬁts
of dual decomposition [11] and augmented Lagrangian
methods [12], ADMM coordinates the solutions of small
local subproblems to ﬁnd a solution of the large global
problem. Many variants of ADMM have been proposed
with different purposes of better acceleration, convergence,
or stability, and have been applied to solving different types
of optimization tasks. Bethe ADMM [13] was proposed for
tree decomposition based parallel MAP inference, which
used an inexact ADMM augmented with a Bethe-divergence
based proximal function. Bregman ADMM [14] was then
proposed, which provided a uniﬁed framework for ADMM
and its variants, including generalized ADMM, inexact
ADMM and Bethe ADMM. Linearized ADMM [15] [16]
was also proposed for convex optimization. One state-of-
the-art ADMM method for solving IP is (cid:96)p-box ADMM [17],
where the binary constraints are equivalently replaced by
the intersection of a box constraint Sb and a (cid:96)p-norm sphere
constraint Sp. The equivalent replacement can be formulated
as: x ∈ {0, 1}n ⇔ x ∈ Sb ∩ x ∈ Sp ⇔ x ∈ [0, 1]n ∩ {x :
(cid:107)x − 1
2p }, then additional variables are introduced
to separate the constraints, so as to coordinate with ADMM
method. This method could give a quasi-optimal solution
at the globally converged point. (cid:96)p-box ADMM has wide
application in MAP inference [23], sparse adversarial attack
[24], model compression [25], feature selection [26] and
etc. Though (cid:96)p-Box ADMM builds the bridge between IP
and continuous optimization algorithms and achieves great
performances in solving IP tasks, it still remains the issues of
scalability.

p = n

2 1(cid:107)p

Integer programming meets learning The application of
machine learning (ML) to discrete optimization has been
a popular topic with various approaches in the literature
[27]. Learning to branch is an interesting topic attracting

Set of Variables𝒙𝟎…CriteriasatisfiedEarly StoppingCriteriaConvergedsatisfiedCriteriasatisfiedEarly FixingCriteriaConvergedsatisfiedFree Variables…Fixed Variables……𝒙𝟏𝒙𝟐𝒙𝒏%𝟏t=0t=1t=0………t=𝜷t=2𝜷t=𝑻′-1t=𝑻-1IterationsIterationst=0t<𝑻-1TABLE 1
Summary of notations

4

Notation Meaning

Notation Meaning

n
t
T
δ

f (·)
x
b
d

π(·)
y, y
α
dh

k
ˆzk
L
H

z
u
r
γ

e
w
I
ξ
ζ

number of variables, n>0.
iteration index, t ∈ {0, ..., T−1}.
maximum iteration without early ﬁxing, T > 0.
ﬁxing threshold, deciding whether to ﬁx, δ ∈[0.5,1].

objective function, linear or quadratic.
set of variables, x ∈ Rn.
vector in objective function, b ∈ Rn.
vector in constraint set, d ∈ Rm.

policy network for early ﬁxing.
y is the iterative values of variable y. y ∈ Rβ×1
node number for iteration embedding, α > 1.
node dimension, dh≥1.

position index, k ∈ {1, ..., α}.
node input in attention layers, ˆzk ∈ R2dh
number of layers, L>1.
number of heads in MHA sublayer, H =8.
concatenated embedding, z ∈ R(α·dn)×1.
number of free variables, u > 0, u ≤ n.
number of rounds for conducting early ﬁxing, r > 1.
number of blocks, used in the network training, γ>1.

instance index, e ∈ {0, ..., N−1}.
weight for training, a scalar.
one instance as Formulation (1).
a constant for linear programming.
the vector of perturbation magnitudes.

m
i
T (cid:48)
β

C
A
C
(cid:78)

θ
z
ˆz
dn

number of constraints, m ≥0.
variable index, i ∈ {0, ..., n−1}.
maximum iteration with early ﬁxing, T (cid:48) > 0.
block size, denoting one block of iterations, β>1.
set of constraints, C ∈ Rm.
matrix in objective function, A ∈ Rn×n.
matrix in constraint set, C ∈ Rm×n.
any relational symbol. <, >, =, ≥ or ≤.

weights of policy network.
iteration embedding of one variable, z ∈ Rα×dh .
iteration embedding with positional encoding, ˆz ∈ Rα×2dh .
hidden dimension, dn=128.

j
hk, ˆhk
(cid:96)
dn(cid:48)

dimension index for positional encoding, 2j ≤ dh.
node embeddings in attention layers. hk, ˆhk ∈ Rdn .
layer index in attention layers, (cid:96) ∈ {1, ..., L}.
hidden dimensions in FF sublayer, dn(cid:48) =512.

p
v
M
N

q
J (·)
N’
(cid:15)
η

the probability vector, each element pi ∈ [0, 1].
number of ﬁxed variables, v > 0, v ≤ n.
the approximate method to be accelerated.
number of training instances, N>1.

one element of loss, a scalar.
the binary cross entropy loss for network training.
number of instances for inference, N’>1.
perturbation for sparse adversarial attack.
the vector of perturbed positions.

a great deal of attention. Khalil et al. [28] took the ﬁrst
step towards statistical learning of branching rules in BB.
Alvarez et al. [29] and Gasse et al. [30] learn a branching
rule ofﬂine on a collection of similar instances, and the
branching policy is learned by the imitation of the strong
branching expert. Graph Neural Network (GNN) approach
for learning to branch has successfully reduced the runtime
[30]. Gupta et al. [31] consider the availability of expensive
GPU resources for training and inference, thus devise an
alternate computationally inexpensive CPU-based model
that retains the predictive power of the GNN architecture.
Tang et al. [32] utilize reinforcement learning for learning
to cut. Recently there is one set of approaches focusing on
directly learning the mapping from an IP to its approximate
optimal solution, instead of solving the IP by any exact
or approximate solvers. Vinyals et al. [33] introduce the
pointer network as a model that uses attention to output a
permutation of the input, and train this model ofﬂine to solve
the TSP problem. Bello et al. [34] introduce an Actor-Critic
algorithm to train the pointer network without supervised
solutions. Kool et al. [35] propose a model based on attention
layers [20] to solve the routing problems. Andrychowicz
et al. [36] and Li et al. [37] propose learning to optimize
or learning to learn, casting an optimization problem as a
learning problem. Nowak et al. [38] train a Graph Neural
Network in a supervised manner to directly output a tour
as an adjacency matrix, which is converted into a feasible
solution by a beam search.

Most of the above mentioned papers use the networks
to entirely substitute with the optimizer, different from that,
in this paper we simply utilize the network to accelerate the
optimization, just like an attachment module.

Imitation learning Imitation learning (IL) techniques aim to
mimic the hebaviour from an expert or teacher in a given
task [39]. IL and reinforcement learning (RL) both work for
the Markov decision processes (MDP). RL tends to have
the agent learn from scratch through its exploration with a
speciﬁed reward function, however, the agent of IL does not
receive task reward but learn by observing and mimicing
[40]. Similar to traditional supervised learning (SL) where the
samples represent pairs of features and ground-truth labels,
IL has the samples demonstrating pairs of states and actions.
One fundamental difference between SL and IL is that: SL
follows the assumption that the training and test data are
independent and identically distributed (IID), while those of
IL are Non-IID where the current state is only correlated to
the previous state. Broadly speaking, research in the IL can
be split into two main categories: behavioral cloning (BC)
[22], and inverse reinforcement learning (IRL) [41]. In this
paper, we choose BC for the training.

Early exiting and early stopping The term "early exiting"
originally comes from computer vision and image recogni-
tion, which is mainly aimed at improving the computation
efﬁciency based on speciﬁc architectures during the inference
phase [18] [19]. As networks continue to get deeper and
larger, these costs become more prohibitive for real-time
applications. To address the issue, the proposed architecture
exits the network early via additional branch classiﬁers
with high conﬁdence [42]. Early stopping is a form of
regularization used to avoid overﬁtting when training a
learner with an iterative method, such as gradient descent
[43], [44]. When a certain criterion is satisﬁed, early stopping
will be conducted before the ultimate convergence. Kaya et al.
[45] proposes to avoiding “over-thinking” by early stopping,

where the deep neural networks can reach correct predictions
before the ﬁnal layers to save the running time. In optimal
control literature, optimal stopping [46] is a problem of
choosing a time to take a given action based on sequentially
observed random variables in order to maximize an expected
payoff. Optimal stopping can be seen as a special case of
early stopping. Becker et al. [47] and Chen et al. [48] use
deep reinforcement learning to learn the optimal stopping
policy. Besides, Chen et al. [48] provide a variational Bayes
perspective to combine learning to predict with learning
to stop. In this paper, we propose a novel early ﬁxing
framework for accelerating solving the generic IP problems.
Whether early exiting or early stopping, the models focus on
the depth of iterations, while our proposed early ﬁxing also
considers the width of variable dimensions.
Fix-and-optimize Fix-and-optimize [49] is a metaheuristic,
ﬁrstly proposed by Gintner et. al. for solving mixed integer
linear programming (MIP), which iteratively decomposes
a problem into smaller subproblems. In each iteration, a
decomposition process is applied aiming at ﬁxing most of
the decision variables at their value in the current solution.
Since the resulting subproblem is composed only by a small
group of free variables to be optimized, each subproblem
can be solved fairly quickly by a MIP solver, when compared
with the full model. The solution obtained in each iteration
becomes the current solution when it improves the objective
value. In further iterations, a different group of variables
is selected to be optimized. This process is repeated until
a termination condition is satisﬁed. The ﬁx-and-optimize
algorithm has wide applications in lot sizing problem [50],
timetabling problem [51], and etc. Different from the ﬁx-and-
optimize algorithm where the ﬁxed variables in previous
iteration will be released in the next iteration, our early
ﬁxing framework requires that the ﬁxed variables will stay
ﬁxed and not appear in the following iterations.
Variable ﬁxing The strategy of ﬁxing variables [52] within
optimization algorithms often proves useful for enhancing
the performance of methods for solving constraint satisfac-
tion and optimization problems. Such a strategy has come to
be one of the basic strategies associated with Tabu search [53].
Two of the most important features in Tabu search are how
to score the variables (variable scoring) and which variables
should be ﬁxed (variable ﬁxing). Similar to ﬁx-and-optimize
algorithm, the ﬁxed variables in Tabu search could possibly
be freed, while those in our early ﬁxing framework will keep
ﬁxed and not released in the next iterations.

3 BACKGROUND

3.1 Problem deﬁnition

Throughout this paper, we focus on the problem of generic
IP problems, which can be generally formulated as a binary
mathematical optimization problem as follows:

arg max
x

f (x), s.t. x∈C, x∈{0, 1}n,

(1)

where C ∈ Rm is the set of constraints, and x ∈ Rn is the
set of binary variables. n, m denotes the number of variables
and constraints, respectively. In this paper, we mainly focus
on the linear and quadratic IP problems.

5

Fig. 3. Solving an instance with Early Fixing via Markov decision process.

3.2 Approximate methods

The approximate methods for solving IP problems are based
on a bunch of iterations, which is a mathematical procedure
that uses an initial value to generate a sequence of improving
approximate solutions until the convergence. In recent year,
there have been a great deal of works utilizing ADMM to
solve the IP problems [14], [15], [16], [17], [23], [26]. Therein,
in this paper we propose the early ﬁxing framework, aiming
to accelerate these ADMM-based approximate methods.

4 EARLY FIXING FRAMEWORK

In this section, we give an overview of how our proposed
early ﬁxing framework is utilized to accelerate the iterative
approximate methods for solving generic IP problems. We
ﬁrstly formulate the early ﬁxing process as a Markov decision
process (MDP), as described in subsection 4.1. We present the
overview of the policy network in subsection 4.2. We then
present how to train the policy network with the weighted
binary cross-entropy loss in a imitation learning method in
subsection 4.3. Last but not least, we exhibit how to do the
inference with our early ﬁxing framework in subsection 4.4,
some mathematical propositions and proofs are given as
regard to the problem reformulations.

4.1 Markov decision process formulations

The sequential decisions made in each early-ﬁxing process
actually construct a Markov decision process [21], as shown
in Fig. 3. Considering the approximate method M to be
accelerated as the environment, we deﬁne the states, actions,
and policy for early ﬁxing as following:

•

States: state st (t ∈ {0, ..., T’}) denotes the state at
the iteration t, which is a state set of all variables.
The state of each variable st,i could be anyone out of
the ﬁve possible states: being ﬁxed/converged to 1,
being ﬁxed/converged to 0, or staying free. We denote
st = {st,i}n−1
i=0 . The initial state s0 exhibits that all
the variables are free, while the terminate state sT (cid:48)

…………t=0t=!"!…"""#"$%"t=2!t=3!t=#-1VariablesIterationsmin !":Weighted Binary Cross-Entropy (WBCE) Loss"!"$%"…"!"$%"…"!"$%"…"!"$%"!=1!=12!=13!=1&!"∗t=0t=!t=1t=2t=3…………t=$!…t=0t=1t=%t=&&t=&&+1t=&&+%$∈ℝ!×('∈ℝ%×&!!×Approximate method iterating(b) Node constructionNetwork training#MHAFFConcatenationProjectionSkip connectionMessage passingAttention queryNode inputNode outputNode embeddingAttention layers$∈ℝ!×(Node construction'∈ℝ%×&!MLP layer(Positional encoding(c) Policy network!"!#$%!#…!&)$%…!&)"#"&)!"={$":&'((,$#:&'((,……$$%&:&'((,$$%#:&'((}!'={$":&,$(-./0,$#:&,$(-./1,……$$%&:&'((,$$%#:&'((}!(!={$":&,$(-./0,$#:&,$(-./1,……$$%&:2/34('5(-./0,$$%#:2/34('5(-./1}6(8)Inference via MDPObtain 6(8)Collecting weighted state-action pairs6

Fig. 4. Policy network: the input is with dimension β. (a) After node construction, there are α nodes, each with dimension dh. With the positional
encoding, the dimension turns to 2dh. (b) Then go through L attention layers, including a multi-headed attention (MHA) sublayer and a feed-forward
(FF) sublayer. (c) The nodes are concatencated as one vector, and then fed into the multi-layer perceptron (MLP) layers. Finally, the output is a
probability p ∈ [0, 1], determining whether to ﬁx the input variable or not, according to its past β iterations.

represents that all the variables are ﬁxed/converged,
no matter to 0 or 1.

• Actions: the action at (t ∈ {1, ..., T’}) denotes the
transitions from one state to the next state. Since
our early ﬁxing process is carried out once every β
iterations, then the action is also conducted once every
β iterations. The terminate iteration could trigger the
actions because the variables converge. at is also a set
of actions of each variable: at = {at,i}n−1
i=0 .

• Policy: the policy π(at|st), is given by the policy
network as described in subsection 4.2, which inputs
the past β iteration values of all the currently free
variables, and outputs the probabilities of these free
variables whether to ﬁx them or not. We use p to
denote the output: p = π(at|st) where pi ∈ [0, 1], i ∈
{1, ..., u}, u is the number of free variables. We set a
ﬁxing threshold δ ∈ [0.5, 1]. If pi > δ, early ﬁx this
variable to 1; If pi < (1 − δ), early ﬁx this variable
to 0; Otherwise, do not conduct early ﬁxing and this
variable stays free.

According to the policy π(at|st), we may update the variable
states, converting some variables from free to ﬁxed. The
iteration will terminate when the stopping criterion reaches
convergence, or all the variables have been ﬁxed. As a
Markov decision process, this early ﬁxing process is episodic,
where each episode amounts to solving the IP problem. An
ideal approach to ﬁnd an early-ﬁxing policy is reinforcement
learning, here we adopt an imitation learning scheme to train
the network. Instead of training the network from scratch,
we learn the features of early iterations and directly map to
the converged solutions in order to achieve the purpose of
acceleration.

4.2 Policy network with attention

The policy network provides with the probability for deter-
mining whether to ﬁx the input variable or not at current
state. The input of the policy network is the past β iterative
values of the given variable, and the output is a probability
p ∈ [0, 1]. Taking one variable y as an example, we ﬁrstly
collect the past β continuously iterative values of the variable,

t=0t=!t=1t=2t=3…………t=0t=1t=%t=&&t=&&+1t=&&+%ℝ!×(ℝ%×&!)×Node constructionMHAFFConcatenationProjectionSkip connectionMessage passingAttention queryNode inputNode outputNode embeddingℝ!×((a) Node constructionℝ%×&!Output: :Positional encodingPolicy networkAttention layersℝ%×*&!ℝ%×&$ℝ%×&$ℝ%×&$Input: $(b) Attention layers(c) MLP layersℝ%×*&!ℝ%×&$ℝ%&$×(ℝ(∈[,,.]w/o attention7

which are sequential according to the time series. At this
point, the dimension is β × 1. Inspired by the sequential
model [54], we use the sliding window to convert the
y ∈ Rβ×1 to z ∈ Rα×dh , where z is the iteration embedding
of y, α denotes the node number, and dh is the node
dimension.
Positional encoding There is no recurrence and no convolu-
tion in the attention models, so we need positional encoding.
Before feeding z into the attention layers, we inject some
information about the relative position of the tokens in the
sequence of z, so as to make the most use of the order of the
sequence. We follow the setup in Vaswani et al. [20] and add
the positional encodings to the iteration embeddings. Let k
be the node number, dh be the embedding dimension. Then
the positional encodings have the same dimensions dh as the
embeddings. We use sine and cosine functions of different
frequencies:






P E(k,2j)

(cid:16)

= sin

k/100002j/dh

P E(k,2j+1) = cos

k/100002j/dh

(cid:16)

(cid:17)

(cid:17)

,

(2)

where k is the position, k∈{1, ..., α} and j is the dimension,
2j ≤ dh. The wavelengths form a geometric progression from
2π to 10000×2π. After adding the positional encoding, the
iteration embedding turns from z ∈ Rα×dh to ˆz ∈ Rα×2dh .
Attention layers Then we apply the encoder part of
Transformer-alike attention architecture [35] to our network
to extract better iteration embeddings. First of all, to make it
consistent with the dimensions, through a learned linear
projection, one node input ˆzk is projected to one node
embedding hk, where ˆzk ∈ R2dh and hk ∈ Rdn , and
the dimension dn=128. Then all the node embeddings go
through L attention layers. Each layer consists of two
sublayers: a multi-head attention (MHA) layer that executes
message passing between the nodes, and a node-wise fully
connected feed-forward (FF) layer. Each sublayer adds a
skip-connection [55] and a batch normalization (BN) [56]:


ˆhk = BN(cid:96) (cid:16)ˆh((cid:96)−1)
+ MHA(cid:96)

k
(cid:17)
k = BN(cid:96) (cid:16)ˆhk + FF(cid:96)(ˆhk)
h((cid:96))


k

(cid:16)ˆh((cid:96)−1)
.

1

, ..., ˆh((cid:96)−1)
α

(cid:17)(cid:17)

(3)

Any two layers do not share their parameters. Layer index
(cid:96) ∈ {1, ..L}, node index k ∈ {1, ..., α}. The MHA sublayer
uses H = 8 heads, each with dimension dn
H = 16. Moreover,
the fully connected FF sublayer, which is applied to each
node embedding separately and identically, consists of two
linear transformations with a ReLU activation function in
between: it ﬁrst maps the node embedding from dimension
dn to hidden dimension dn(cid:48) = 512, then transforms from
dn(cid:48) back to dn. After L attentions layers, we get α node
embeddings, each with dimension dn. At this point, we
do the concatenation and get embedded variable z with
dimension (α·dn) × 1.
MLP layers After the attention layers, the variable y
obtains a new embedding z ∈ R(α·dn)×1, which will be
fed into another multi-layer perceptron (MLP). At this stage,
we utilize three fully connected FF sublayers along with
decreasing dimensions (256 − 128 − 16) with the ReLU
activation functions between the hidden layers. Finally, in the

Fig. 5. Network training with weighted binary cross-entropy (WBCE) loss.

last layer through a sigmoid function, we obtain a probability
py ∈ [0, 1], determining whether to ﬁx the input variable y
or not.

If the probability is greater than the ﬁxing threshold δ,
then the action of ﬁxing this variable to 1 will be conducted.
we can interpret this ﬁxing threshold as a symmetric ﬁxing
conﬁdence, i.e., if the probability is less than 1 − δ, then
the action of ﬁxing this variable to 0 will be conducted.
Otherwise, no ﬁxing action will be conducted and this
variable will be further updated. The early ﬁxing process is
given in the Algorithm 1.

4.3 Training: imitation learning

We train the policy network π(θ) as shown in Fig. 5.
Speciﬁcally, our policy network is trained by behavioral
cloning [22] as a method of expert-driven imitation learning,
and here we use the approximate method to be accelerated
M itself as the expert rule. Subsection 4.1 has explained
the meaning for states and actions. Then mathematically,
for those free variables, we assign s as the past β iterative
values, and assign a∗ as the ultimately converged discrete
solutions of the approximate method M, i.e., the expert
solutions. We ﬁrst run the expert on a collection of N training
instances, and pick the dataset of expert state-action pairs
D={(se,r,i, a∗
e=0 }. And the policy is learned

e,r,i)|n−1

r=0 |N −1

i=0 |γ−1

…………t=0t=!"!…"""#"$%"t=2!t=3!t=#-1VariablesIterationsmin !":Weighted Binary Cross-Entropy (WBCE) Loss"!"$%"…"!"$%"…"!"$%"…"!"$%"!=1!=12!=13!=1&!"∗t=0t=!t=1t=2t=3…………t=$!…t=0t=1t=%t=&&t=&&+1t=&&+%$∈ℝ!×('∈ℝ%×&!!×Approximate method iterating(b) Node constructionNetwork training#MHAFFConcatenationProjectionSkip connectionMessage passingAttention queryNode inputNode outputNode embeddingAttention layers$∈ℝ!×(Node construction'∈ℝ%×&!MLP layer(Positional encoding(c) Policy network!"!#$%!#…!&)$%…!&)"#"&)!"={$":&'((,$#:&'((,……$$%&:&'((,$$%#:&'((}!'={$":&,$(-./0,$#:&,$(-./1,……$$%&:&'((,$$%#:&'((}!(!={$":&,$(-./0,$#:&,$(-./1,……$$%&:2/34('5(-./0,$$%#:2/34('5(-./1}6(8)Inference via MDPObtain 6(8)Collecting weighted state-action pairsby minimizing the weighted binary cross-entropy (WBCE)
loss:

J (θ) = −

1
N ·γ·n

N −1
(cid:88)

γ−1
(cid:88)

n−1
(cid:88)

e=0

r=0

i=0

we,r,iqe,r,i,

(4)

qe,r,i = a∗ log πθ(a|s) + (1−a∗) log(1−πθ(a|s)),

(5)

where for a∗, a, s, we hide the subscripts e,r,i for readability.
we,r,i is the weight, we,r,i = 1
r+1 . Mathematically, se,r,i =
x(r,e)
(r−1)β:rβ. a∗
T −1. We call one problem shown as in
Formulation (1), as one instance. N is the number of training
instances.

e,r,i = x(r,e)

4.4 Inference with early ﬁxing

The general inference stage of early ﬁxing framework is
given in the Algorithm 1, and we also present the process
as a MDP in Fig. 3. Our early ﬁxing framework takes each
variable independently, and decisions on whether to early ﬁx
are once every block of iterations. In each block of iterations,
given the iterative values of the variables within the past
β iterations, the policy network will evaluate the posterior
probability of each variable concerning all discrete candidate
states (0 or 1). If the posterior probability with respect to one
state exceeds a threshold, namely the ﬁxing threshold δ, then
the action of ﬁxing this variable to that discrete state will
be conducted, and this variable will not be updated in later
iterations; otherwise, no ﬁxing action will be conducted and
this variable will be further updated.

When a certain number of variables are ﬁxed in previous
iterations, then how to update the problem into a smaller-
sized one will be discussed in this subsection. Our early
ﬁxing framework is available for both linear programming
and quadratic programming, no matter constrained or un-
constrained. We will give the mathematical assumptions and
propositions based on a constrained quadratic programming
problem:

arg max
x

x(cid:62)Ax+b(cid:62)x, s.t. Cx

(cid:79)

d, x∈{0, 1}n.

(6)

Notations. We denote the matrices and vectors in formula-
tion (6) as: A =
, x =
(cid:20)x1
x2

, where vectors x1 refers to the set of free variables and

(cid:20)A1 A2
A3 A4

, C = (cid:2)C1 C2

(cid:20)b1
b2

(cid:3)
, b =

(cid:21)

(cid:21)

(cid:21)

x2 refers to the set of ﬁxed variables, and the same around
for other vectors and matrices. Let u, v be the number of free
and ﬁxed variables, then b1, x1∈Ru, b2, x2∈Rv, A1∈Ru×u,
A2∈Ru×v, A3∈Rv×u, A4∈Rv×v, C1∈Rm×u, C2∈Rm×v, (cid:78)
denotes any relational symbol such as <, >, =, ≥ or ≤.
Proposition 1. Problem reformulation: when doing the early
ﬁxing once every β iterations, let r be the rounds for
conducting early ﬁxing, then we can propose that:

(r)
1

x(r+1) = x
•
• A(r+1) = A(r)
1
b(r+1) = (A(r)
•
• C(r+1) = C(r)
• d(r+1) = d(r) − C(r)
2 x

2 + A(cid:62)(r)

3

1

(r)
2

)x

(r)

2 + b(r)

1

8

Algorithm 1 Inference: Early Fixing Framework
Input: accelerated approximate method M, instance I,
policy network πθ, total variable number n, block size β,
ﬁxing threshold δ ∈ [0.5, 1]

Early ﬁx variable xi to 1, v ← v+1

if t%β == 0 then
xt−β:t ← M(I)
st ← xt−β:t
p ← πθ(at|st)
for i = 1 to u do
if pi > δ then

Output: x∗
1: u ← n, v ← 0, t ← β
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: until (converged or u ≤ 0)
19: Record all the ﬁxed/converged discrete solutions x∗
20: return x∗.

end if
end for
Update the instance I as Subsection 4.4.
u ← u−v, v ← 0, t ← t+β

Early ﬁx variable xi to 0, v ← v+1

end if
if pi < 1−δ then

end if

Proof 1. We eliminate the superscript (r) for readability.

(i)

For the objective function:

x(cid:62)Ax + b(cid:62)x
x2(cid:62)(cid:3)

= (cid:2)x(cid:62)

1

(cid:20)A1 A2
A3 A4

(cid:21)

(cid:21) (cid:20)x1
x2

+ (cid:2)b(cid:62)
1

(cid:3)

b(cid:62)
2

= x(cid:62)

1 A1x1 + x(cid:62)

2 A3x1 + x(cid:62)

+ x(cid:62)
1 A1x1 + ((A2 + A(cid:62)

= x(cid:62)

1 A2x2
2 A4x2 + b(cid:62)
3 )x2 + b1)(cid:62)x1
2 A4x2 + b(cid:62)
+ (x(cid:62)

2 x2),

1 x1 + b(cid:62)

2 x2

(10)

(cid:20)x1
x2

(7)
(cid:21)

(8)

(9)

(r)

1 , A(r+1) = A(r)

1 , b(r+1) = (A(r)

thus: x(r+1) = x
2 +
A(cid:62)(r)
2 + b(r)
(r)
)x
1 . Since previously ﬁxed variables can
3
be seen as the constants in the following iterations,
then (x(cid:62)
2 A4x2 + b(cid:62)
2 x2) is constant.
For the constraints:

(ii)

(cid:79)

Cx
(cid:21) (cid:79)
(cid:20)x1
x2

d

d

(cid:2)C1 C2

(cid:3)

(cid:79)

C1x1

d − C2x2

(11)

(12)

(13)

thus: C(r+1) = C(r)

1 , d(r+1) = d(r) − C(r)
2 x

(r)
2 .

Proof ends.
Remark 1. As regard to the updating for b, if A is a
symmetric matrix, then A2 = A(cid:62)
3 . At that point, the
(r)
updating can be simpliﬁed as: b(r+1) = 2A(r)
2 x

2 + b(r)
1 .

Size →

Model

TABLE 2
Performance evaluations for constrained linear programming on generated regular-sized Dataset I. The time limit is set to 1 hour.

Size →

Model

RPB [57]
FiLM [31]
GCNN [30]

(cid:96)p-box ADMM [17]
(cid:96)p-box ADMM + LEF(w/o Att.)
(cid:96)p-box ADMM + LEF(with Att.)

n = 500

n = 1000

n = 1500

n = 4000

Obj.↑

Obj. Gap↓

Time↓ Obj.↑ Obj. Gap↓

Time↓ Obj.↑ Obj. Gap↓

Time↓

Obj.↑ Obj. Gap↓

Time↓

7464.8
7464.8
7464.8

6953.7
6804.9
6883.5

N/A
N/A
N/A

N/A
0.69%
1.96%

2.79s
2.08s
1.78s

1.17
0.18s
0.27s

14888
14888
14888

14430
13768
13803

N/A
N/A
N/A

N/A
4.10%
3.96%

23.17s
19.78s
15.22s

2.54s
0.38s
0.44s

21231
21231
21231

20930
20415
20500

N/A
N/A
N/A

N/A
2.40%
1.85%

169.16s
185.79s
144.74s

4.04s
0.43s
0.56s

59772
58809
60105

56188
53195
54020

N/A
N/A
N/A

N/A
5.28%
3.81%

3600s
3600s
3600s

11.15s
1.39s
2.47s

9

TABLE 3
Performance evaluations for constrained linear programming on generated large-sized Dataset II. A negative objective gap means a better objective.

Size →

Model

Obj.↑

Obj. Gap↓

Time↓

Speedup↑

#Sol. Diff.↓ Accuracy↑

Obj.↑

Obj. Gap↓

Time↓

Speedup↑

#Sol. Diff.↓ Accuracy↑

n = 1e4

n = 5e4

(cid:96)p-box ADMM [17]
(cid:96)p-box ADMM + LEF(w/o Att.)
(cid:96)p-box ADMM + LEF(with Att.)

9665.8
9691.4
9691.6

N/A
-0.25%
-0.26%

11.4s
0.9s
0.9s

N/A
12.6×
12.6×

n = 1e5

N/A
10.9
9.1

N/A
99.8910%
99.9090%

48372
48445
48465

N/A
-0.15%
-0.19%

111.5s
5.4s
5.7s

N/A
20.6×
19.6×

N/A
66.0
55.0

N/A
99.8680%
99.8900%

n = 2e5

Obj.↑

Obj. Gap↓

Time↓

Speedup↑

#Sol. Diff.↓ Accuracy↑

Obj.↑

Obj. Gap↓

Time↓

Speedup↑

#Sol. Diff.↓ Accuracy↑

(cid:96)p-box ADMM [17]
(cid:96)p-box ADMM + LEF(w/o Att.)
(cid:96)p-box ADMM + LEF(with Att.)

97579
97631
97682

N/A
-0.05%
-0.11%

327.3s
13.8s
14.4s

N/A
23.7×
22.7×

N/A
129.8
126.8

195445
N/A
99.8702%
195710
99.8732% 195758

N/A
-0.14%
-0.16%

990.0s
39.4s
41.3s

N/A
25.1×
24.0×

N/A
247.1
220.2

N/A
99.8765%
99.8899%

5 CONSTRAINED LINEAR PROGRAMMING

5.1 Setup

Accelerated method and datasets In this section, we will
accelerate the approximate method: (cid:96)p-box ADMM [17] for
solving constrained linear integer programming. For the
datasets, we choose the combinatorial auction problems [30]
with the following formulation:

arg max
x

b(cid:62)x, s.t. Cx(cid:54)d, x∈{0, 1}n,

(14)

where C ∈ Rm×n, m, n refer to the number of constraints
and variables. We follow the experimental setup of Gasse et
al. [30], and generate two sets of instances in difference sizes.

b,C,d ← G(bid=n, item=ξ·m).

(15)

We generate the instances, namely, b,C,d, according to the
generator G. The instance size is determined by two key
parameters, bid and item. The bid number is equivalent
to variable number n, while the item number is not equiv-
alent but directly proportional to the constraint number
m. ξ is a constant. In order to ensure the feasibility and
optimality of the instance, the constraint number m could
be different for different instances, given the same item
number. We generate two sets of instances. For datasets
I, there are regular-sized instances: (bid=500, item=100),
(bid=1000,
(bid=4000,
(bid=1500,
item=800). For datasets II, there are extra large scale in-
stances: (bid=1e4, item=100), (bid=5e4, item=500), (bid=1e5,
item=1000), (bid=2e5, item=2000).
Training details We train our model only on the smallest
sized instances with n=500, and generalize to all other larger-
sized instances. We train on N = 100 instances, and do the
inference on N (cid:48) = 20 instances for all the datasets. γ, β, δ, L
are set to 10, 100, 0.9, 2, respectively. We train for 10 epochs.
The learning rate is set to 1e−4. For our learning-based early
ﬁxing methods, training without attention for one epoch costs
77s, while training with attention costs 89s. We implement

item=200),

item=300),

the functions of the accelerated methods in C++ and call the
functions in Python via Cython interfaces. All the learning
modules are implemented in Python.

obj1

Evaluations We evaluate all the different sizes, each with 20
instances. We set the time limit to 1 hour. For all datasets, we
evaluate the objective, the objective gap and runtime, and
we record the mean value of all instances. The objective gap
is used to exhibit the gap between the (cid:96)p-box ADMM with
learning-based early ﬁxing (LEF) and the (cid:96)p-box ADMM
without early ﬁxing, given as: obj1−obj2
, where obj1, obj2
refer to the objective obtained by (cid:96)p-box ADMM, (cid:96)p-box
ADMM + LEF, respectively. A negative objective gap means
achieving a better objective with early ﬁxing. Speciﬁcally,
for datasets II, we also evaluate the Speedup, the number of
solution difference (#Sol. Diff.) and the accuracy. The speedup
is the time speedup, simply dividing the runtime of (cid:96)p-box
ADMM by that of (cid:96)p-box ADMM + LEF. #Sol. Diff is the
number of solution difference where the variable solution
by (cid:96)p-box ADMM is different from that by (cid:96)p-box ADMM
+ LEF, the maximum number should be the total variable
number n. The accuracy is to evaluate the correct solution
accuracy, given as: n−nd
n × 100%, nd refers to (#Sol. Diff.). In
the hyperparameter study, we also evaluate the number of
infeasible constraints.

Baselines For datasets I, we compare against three exact
methods based on the branch-and-bound algorithm. Then
for Datasets II, since the size is too large for them to obtain a
solution, we only compare with the approximate methods.
Reliability pseudocost branching (RPB) [57] is a variant of
hybrid branching which is used by default in SCIP, and
we choose SCIP 6.0.1 [58] as the backend solver. Graph
convolutional neural networks (GCNN) [30] are applied
to learning branch-and-bound variable selection policies.
Feature-wise Linear Modulation (FiLM) [31] [59] layers
are used to construct the neural network for learning to
branch which is purely CPU-based, but shows competitive
performances against GPU-based neural networks.

10

Fig. 6. Convergence on different methods for constrained linear programming: how the objective changes with respect to the iterations during the
inference stages for four different sized instances. LEF method leads to much faster convergence, and LEF with attention layers achieves better
objective.

Fig. 7. Hyperparameter study on ﬁxing threshold δ and ablation study on weighted loss: (a) How the number of infeasible constraints go with the
ﬁxing threshold. Different color refers to different ﬁxing threshold. (b) How the objective gaps and runtime speedup of different sizes change when
training with weighted loss or no weighted loss. No weighted loss means that all weights are equal to 1.

5.2 Experimental results

Comparative study As shown in Table 2 and Table 3, we
ﬁrstly compare against three exact methods: RPB, FiLM and
GCNN. GCNN generally outperforms RPB and FiLM with
higher efﬁciency in runtime. The three exact methods have
one obvious shortcoming in common: time-consuming, espe-
cially when the problem size increases, which is unacceptable
in real life and large scale applications. Then, we compare
our proposed LEF with the base method (cid:96)p-box ADMM. The
results turn out that the LEF outperforms the base method
regarding objective gaps, which reveals the effectiveness
of our early ﬁxing framework. In Fig. 6, we record how
the objective changes with respect to the iterations during
the inference stages for different sized instances. From the
ﬁgure, we can see that our LEF method leads to much
faster convergence. When zooming in 10× in the left two
ﬁgure, we obtain a general view about the LEF ﬂuctuations.
When zooming in 100× in the right two ﬁgure, we can even
clearly see that our LEF with attention layers achieves better
objective.
Ablation study We also compare our LEF with or without
attention layers. From the experiments, we can see that
the extra attention layers will costs more runtime, while
obviously achieving a better objective. The negative objective
gaps refer to a better objective. Interestingly, for all the large-
sized Datasets II, we could even achieve a better objective
than the expert method, (cid:96)p-box ADMM itself. And with
the problem size n increases, the runtime speedup is also
increasing. And with our attention layers, the accuracy of
datasets II can be greater than 99.8%, which is impressive.
These results exhibit the efﬁciency of attention layers within
our early ﬁxing framework. We also evaluate the efﬁciency
of weighted loss in training, as shown in Fig. 7(b). We set
the ﬁxing threshold to 0.9, where all the constraints are

feasible. From the ﬁgures, we can tell that the weighted
loss generally achieves a smaller objective gap and a larger
runtime speedup, compared to the no weighted loss.
Hyperparameter study We analyze one of the most im-
portant hyperparameters in our early ﬁxing framework:
the ﬁxing threshold θ, as shown in Fig. 7(a). We evaluate
how the number of infeasible constraints go with the ﬁxing
threshold, for different problem sizes in dataset II. We set
ﬁxing threshold from 0.5 to 0.9. When θ is 0.5, all the variables
are ﬁxed after the ﬁrst block of iterations. When θ is 1, no
variable is early ﬁxed at all. From the results, only when θ
is greater than 0.9, the number of infeasible constraints is 0,
which means that the solution with early ﬁxing is feasible.

6 MRF ENERGY MINIMIZATION
6.1 Formulations

We consider the pairwise Markov Random Field (MRF)
energy minimization problem based on a graph, which can
be generally formulated as [63]:

E(x) = x(cid:62)Ax + b(cid:62)x,

arg min
x
s.t. Cx=1, x∈{0, 1}nK×1.

(16)

where x is a concatenation of all indicator vectors for the
states κ ∈ {1, ..., K} and all n nodes. If xi
κ = 1, then node
i is on the state κ; otherwise, xi
κ = 0. Each node can only
take on one state, therefore we ensure that (cid:80)K
κ = 1
for ∀i, i ∈ {1, ..., n}. Thus we have the constraint set as
in Formulation 16. As for the objective function, A is the
un-normalized Laplacian of the graph, A = D − W, W is
the matrix of node-to-node similarities. When K = 2, the
problem turns to a submodular minimization problem and it
can be globally optimized using the min-cut algorithm [60]
in polynomial time, however, it cannot be guaranteed when
K > 2.

κ=1 xi

!=1$4!=5$4!=1$5!=2$5zoom in 10×zoom in 10×zoom in 100×zoom in 100×zoom in 10×Fixing threshold 𝛿Speedup# Infeasible constraint-1 * Obj. Gap.𝑛=1𝑒4𝑛=5𝑒4𝑛=1𝑒5𝑛=2𝑒5𝛿=0.9(a)𝛿=0.9(b)Variable size 𝑛TABLE 4
Performance evaluations for image segmentation on the PASCAL Visual Object Classes Challenge 2012 datasets (VOC2012).

11

Size →

Model

min-cut [60]
spectral relaxation [61]
linear relaxation [62]
(cid:96)p-box ADMM [17]

n = 1e4

n = 5e4

n = 1e5

n = 5e5

Lang.

Energy↓

Time↓

Energy↓

Time↓

Energy↓

Time↓

Energy↓

M.
M.
M.
M.

8778
10753
9157
8864

0.1s
0.1s
1.0s
1.8s

41049
51192
42275
41181

0.2s
0.2s
5.5s
7.1s

79737
105528
81631
79897

0.4s
0.3s
12.1s
17.8s

378439
448737
472253
379860

Energy↓ Gap↓

Time↓

Energy↓ Gap↓

Time↓

Energy↓ Gap↓

Time↓

Energy↓ Gap↓

(cid:96)p-box ADMM
(cid:96)p-box ADMM + LEF(w/o Att.)
(cid:96)p-box ADMM + LEF(with Att.)

C.+P.
C.+P.
C.+P.

8864 N/A
9341 6.7%
9124 3.5%

1.3s
0.1s
0.1s

41181 N/A
43168 6.4%
42334 3.6%

1.4s
0.5s
0.5s

79897 N/A
83612 6.5%
82009 3.5%

3.1s
1.0s
1.1s

379860 N/A
395511 6.1%
388722 3.4%

Size →

Model

min-cut [60]
spectral relaxation [61]
linear relaxation [62]
(cid:96)p-box ADMM [17]

n = 1e6

n = 5e6

n = 1e7

n = 5e7

Lang.

Energy↓

M.
M.
M.
M.

741741
1075301
883686
744442

Time↓

3.7s
2.4s
482.9s
170.3s

Energy↓

4036901
4248567
4405514
4056762

Time↓

22.8s
11.6s
1152.5s
1170.2s

Energy↓

8006436
8263794
N/A
8037384

Time↓

46.3s
25.6s
3600s
2183.1s

Energy↓

48931748
57834141
N/A
N/A

Energy↓ Gap↓

Time↓

Energy↓ Gap↓

Time↓

Energy↓ Gap↓

Time↓

Energy↓ Gap↓

Time↓

1.8s
1.1s
121.0s
98.9s

Time↓

20.2s
5.0s
5.5s

Time↓

952.0s
986.9s
3600s
3600s

Time↓

(cid:96)p-box ADMM [17]
(cid:96)p-box ADMM + LEF(w/o Att.)
(cid:96)p-box ADMM + LEF(with Att.)

C.+P.
C.+P.
C.+P.

744442 N/A
773693 5.9%
760334 3.1%

41.9s
10.6s
11.3s

4056762 N/A
4191890 4.9%
4123729 2.1%

293.6s
56.7s
59.9s

8037384 N/A
8289911 4.7%
8153911 1.9%

548.1s
144.5s
145.4s

49075089 N/A
3143.1s
981.1s
49339894 4.5%
49286140 1.6% 1108.14s

6.2 Experiments for Image segmentation

Accelerated method and datasets (cid:96)p-Box ADMM [17] has
been proved to be efﬁcient in image segmentation when
K = 2. Thus in our experiment, we choose (cid:96)p-Box ADMM to
be accelerated. We also regard the min-cut [60] as the ground-
truth algorithm (K=2) for comparisons. The PASCAL Visual
Object Classes Challenge 2012 (VOC2012) dataset [64] has
been widely used in computer vision tasks, such as object
classiﬁcation, object detection and object segmentation. We
thus choose VOC2012 for our experiments, where 2913 im-
ages are available for segmentation. We then randomly select
100, 20, 20 images for the training, validation and testing, re-
spectively. We resize the testing images to different sizes and
do the testing, including n = 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, 1e7
and 5e7.
Training details We train our model only on the smallest
sized images with n = 1e4, and generalize to all other
larger-sized instances. γ, β, δ, L are set to 5, 10, 0.9, 2,
respectively. The learning rate is set to 1e−4. We train for 20
epochs. For our learning-based early ﬁxing methods, training
without attention for one epoch costs 122s, while training
with attention costs 129s. We implement the functions of
the accelerated methods in C++ and call the functions in
Python via Cython interfaces. All the learning modules are
implemented in Python.
Baselines We compare our method against two generic IP
solvers, namely linear relaxation [62] and spectral relax-
ation [61], and a state-of-the-art and widely used min-cut
algorithm [60]. The linear relaxation is implemented using
the built-in function quadprog in MATLAB. The closed-form
solution based on eigen-decomposition is implemented for
spectral relaxation. We also compare against the (cid:96)p-box
ADMM in both MATLAB and C++. We then apply our
learning-based early ﬁxing framework to (cid:96)p-box ADMM,
with or without attention layers. We implement the functions
of the accelerated methods in C++ and call the functions in
Python via Cython interfaces.
Evaluations We evaluate all the different sizes, each with

20 instances. We evaluate the energy and the runtime for
MATLAB implementations, and for our early ﬁxing we also
evaluate the energy gap towards the accelerated method
itself. We report the mean values for all the 20 instances of
different sizes. The time limit is set to 1 hour.
Comparison study The experimental results are shown in
Table 4. We compare all methods in terms of their ﬁnal energy
value and runtime in the case of binary submodular MRF
(K=2). From the results, we can see that min-cut achieves
the lowest energy, regarded as the ground-truth methods for
the K=2 image segmentation task. Among all the methods,
spectral relaxation achieves the worst performance, though
it is fast in computation. Linear relaxation runs fast in
small sized instances, while running slow for large scale
instances. Linear relaxation obtains smaller energy than
spectral relaxation, while it is much worse than (cid:96)p-box
ADMM. The C++ implementation of (cid:96)p-box ADMM runs
much faster than MATLAB version with signiﬁcant speedup.
Compared to the accelerated (cid:96)p-box ADMM, our early ﬁxing
framework generally achieves from 3× to 5× speedup in
runtime. Especially for the smallest sized instances with
n = 1e4, the runtime speedup is even greater than 10×. In
Fig. 9, we record how the energy changes with respect to
the iterations during the inference stages for one image in
different sizes. From the ﬁgure, we can see that our LEF
method leads to much faster convergence, and LEF with
attention layers achieves lower energy.
Ablation study We also show the ablation study of our
learning-based early ﬁxing. We compare the results with or
without attention layers. Equipping with the attention layers
will cost a little bit more runtime, however, the energy will
decrease a bit more. For LEF without attention layers, the
average energy gap of all different sizes is 5.7%, while the
average energy gap with attention layers is only 2.8%. The
gap is overall decreased by 2.9%.
Performance exhibition We exhibit some segmented images
of different shapes by the optimal min-cut algorithm, (cid:96)p-box
ADMM as well as our learning-based early ﬁxing with or

12

Fig. 8. Performance exhibition of different methods for image segmentation on the PASCAL Visual Object Classes Challenge 2012 datasets
(VOC2012). n=1e5. Min-cut can obtain optimal solutions when K=2. (cid:96)p-box ADMM is the method to be accelerated.

Fig. 9. Convergence on different methods for image segmentation: how the objective changes with respect to the iterations during the inference
stages for one images in four different sizes. LEF method leads to much faster convergence, and LEF with attention layers achieves lower energy.

without attention layers. From Figure 8, we can see that
(cid:96)p-box ADMM generally achieves a great segmentation
performance. And with our early ﬁxing framework, the
segmentation efﬁciency is also excellent.

7 SPARSE ADVERSARIAL ATTACK
7.1 Background

We consider the sparse adversarial attack [24], which gen-
erates adversarial perturbations onto partial positions of
the clean image, where the perturbed image is incorrectly
predicted by the deep model. There are two challenges
lying in the sparse adversarial attack. One is where to
perturb and the other is how to determine the perturbation
magnitude. Some works manually or heuristically deter-
mined the perturbed positions, and optimized the magnitude
using an appropriate algorithm designed for the dense
adversarial attack. However, Fan et. al. [24] proposed to
factorize the perturbation at each pixel to the product of two
variables, including the perturbation magnitude and one
binary selection factor (i.e., 0 or 1). One pixel is perturbed
if its selection factor is 1, otherwise not perturbed. The
perturbation (cid:15) can be factorized as:

(17)
where ζ ∈ Rn denotes the vector of perturbation magnitudes,
and η ∈ {0, 1}n denotes the vector of perturbed positions. (cid:12)

(cid:15) = ζ (cid:12) η,

represents the element-wise product. Then the sparse attack
problem can be formulated as a mixed integer programming
(MIP) by jointly optimizing the continuous perturbation
magnitudes ζ and the binary selection factors η of all pixels.
Inspired by (cid:96)p-box ADMM [17], they proposed to reformulate
the MIP problem to an equivalent continuous optimization
problem. They update the ζ by gradient descent, and update
the η by ADMM. At this point, we are going to accelerate
the η updating parts with our early ﬁxing framework.

7.2 Adversarial attack experiments

Datasets and models We follow the setup in SAPF (Sparse
adversarial Attack via Perturbation Factorization) [24], and
use the CIFAR-10 [65] and ImageNet [66] for the experiments.
There are 50k training images and 10k validation images
covering 10 classes for CIFAR-10. We randomly select 1000
images from the validation set for our experiments. Each
image has 9 target classes except its ground-truth class.
Thus there are totally 9000 adversarial examples for the
adversarial attack method. ImageNet contains 1000 classes,
with 1.28 million images for training and 50k images for
validation. We randomly choose 100 images covering 100
different classes from the validation set. To reduce the time
complexity, we randomly select 9 target classes for each
image in ImageNet, resulting in 900 adversarial examples.
As regard to the classiﬁcation model, on CIFAR-10, we follow

Originalℓ!-box ADMMMin-CutSpectral RelaxationLinearRelaxation111721244516ℓ!-box ADMM+ LEF(w/o Att.)ℓ!-box ADMM+ LEF(with Att.)83345656.png14𝑛=1𝑒4𝑛=5𝑒4𝑛=1𝑒5𝑛=2𝑒5zoom in 10×zoom in 10×zoom in 100×zoom in 100×𝑛=1𝑒5𝑛=5𝑒5𝑛=1𝑒6𝑛=5𝑒6TABLE 5
Performance comparison of targeted sparse adversarial attack on CIFAR-10 and ImageNet. We consider the ASR and (cid:96)p-norm (p = 0, 1, 2, ∞) of the
learned perturbation.

13

Dataset

Method

ASR(%)

CIFAR-10

ImageNet

One-pixel [69]
CornerSearch [70]
PGD (cid:96)0+(cid:96)∞ [70]
SparseFool [71]
C&W-(cid:96)0 [67]
StrAttack [72]
SAPF [24]
SAPF + LEF(w/o Att.)
SAPF + LEF(with Att.)

One-pixel [69]
CornerSearch [70]
PGD (cid:96)0+(cid:96)∞ [70]
SparseFool [71]
C&W-(cid:96)0 [67]
StrAttack [72]
SAPF [24]
SAPF + LEF(w/o Att.)
SAPF + LEF(with Att.)

15.0
60.4
99.4
100
100
100
100
100
100

0
4
95
97
100
100
100
100
100

Best case

Average case

Worse case

(cid:96)0

3
537
555
255
614
391
387
149
276

3
58658
56922
34205
73407
38354
37275
4146
5311

(cid:96)1

1.57
69.70
18.11
11.87
6.95
4.94
4.61
5.23
4.43

1.19
5962.46
798.89
174.17
133.79
77.28
70.25
54.21
47.44

(cid:96)2

0.96
3.34
0.97
0.67
0.43
0.30
0.25
0.46
0.25

0.80
28.06
4.21
0.92
0.79
0.69
0.59
1.19
0.95

(cid:96)∞ ASR(%)

0.68
0.34
0.12
0.05
0.09
0.05
0.04
0.10
0.04

0.66
0.44
0.06
0.01
0.05
0.06
0.04
0.10
0.08

5.5
59.3
98.6
99.9
100
100
100
100
100

0
1.3
95.6
80.6
100
100
100
100
100

(cid:96)0

3
549
555
555
603
543
603
303
510

3
58792
56919
59940
70885
58581
56218
4074
5344

(cid:96)1

2.19
73.64
23.17
25.81
13.07
9.49
8.51
8.48
8.37

1.88
6018.31
854.67
305.18
199.20
127.59
112.16
78.32
67.26

(cid:96)2

1.29
3.48
1.17
1.04
0.81
0.52
0.44
0.64
0.44

1.18
28.29
4.51
1.22
1.12
0.97
0.72
1.36
1.08

(cid:96)∞ ASR(%)

(cid:96)0

0.82
0.34
0.12
0.05
0.16
0.09
0.06
0.10
0.06

0.83
0.44
0.06
0.01
0.06
0.08
0.04
0.10
0.08

0.7
63.2
99.3
99.8
100
100
100
100
100

0
2
96
46
100
100
100
100
100

3
77.57
555
852
598
476
471
459
506

3
58920
56920
82576
69947
67348
56843
4570
5582

(cid:96)1

2.66
561
26.82
39.67
18.60
12.44
10.39
11.69
10.24

2.56
6076.07
925.27
420.44
269.10
171.25
150.55
111.09
88.16

(cid:96)2

1.54
3.62
1.35
1.34
1.14
0.77
0.60
0.62
0.55

1.51
28.53
4.90
1.45
1.46
1.28
1.12
1.75
1.28

(cid:96)∞

0.92
0.34
0.13
0.05
0.22
0.14
0.10
0.10
0.09

0.93
0.44
0.06
0.01
0.07
0.10
0.04
0.10
0.08

TABLE 6
Runtime comparison of targeted sparse adversarial attack on CIFAR-10
and ImageNet. We record the runtime for ζ updating, η updating, and
total. The fourth column is the time speedup for ζ updating which uses
our early ﬁxing to accelerate (cid:96)p-box ADMM.

Dataset

Method

η Updating

η Speedup

ζ Updating

Total

CIFAR-10

ImageNet

SAPF [24]
SAPF + LEF(w/o Att.)
SAPF + LEF(with Att.)

SAPF [24]
SAPF + LEF(w/o Att.)
SAPF + LEF(with Att.)

79.6s
14.1s
14.4s

499.2s
204.4s
231.2s

N/A
5.6×
5.5×

N/A
2.4×
2.2×

81.3s
81.5s
79.7s

560.1s
561.5s
565.3s

160.8s
95.6s
94.1s

1059.3s
765.9s
796.5s

the setting of C&W [67] and train a network that consists
of four convolution layers, three fully-connected layers, and
two max-pooling layers. The input size of the network is 32
x 32 x 3. On ImageNet, we use a pre-trained Inception-v3
network [68]. The input size of the network is 299 x 299 x 3.
All other experimental hyper-parameters for SAPF are set to
default [24].
Training details As regard to our early ﬁxing framework, we
randomly pick 20 images from CIFAR-10. And each image
corresponds to one MIP problem. By default, solving one MIP
problem for adversarial attack is with 6 search loops for G
updating. Thus we record these 20*6=120 instances for early
ﬁxing training. γ, β, δ, L are set to 3, 50, 0.9, 2, respectively.
The learning rate is set to 1e−4. We train for 20 epochs. For
our learning-based early ﬁxing methods, training without
attention for one epoch costs 4min 28s, while training with
attention costs 4min 20s. All the implementations are based
on Python. We use the pre-trained model on CIFAR-10 to
test on both CIFAR-10 and ImageNet.
Baselines and evaluations We compare whether to use
our learning-based early ﬁxing (LEF) on SAPF or not.
We also compare whether to use attention layers or not.
Besides, we also record the results by other attack paradigms,
including one-pixel [69], corner search [70], PGD (cid:96)0+(cid:96)∞ [70],
SparseFool [71], C&W-(cid:96)0 [67], StrAttack [72]. Those results
of other attacks are from the SAPF paper [24].

As regard to the evaluations, the (cid:96)p-norm (p = 0, 1, 2, ∞)
of perturbations and the attack success rate (ASR) are utilized
to evaluate the attack performance of different methods. We

Fig. 10. Examples of perturbations generated by the SAPF method, the
SAPF with our early ﬁxing framework, equipped without or with attention
layers. From the top to the bottom, the ground-truth class and target
class label pairs are: (siamang, zucchini), (gorilla, Brabancon griffon),
(hognose snake, red-backed sandpiper), (coyote, impala), (barn spider,
greenhouse), (great grey owl, capuchin).

8179，369318，2428177，36639732，5428916，27223692，73Original ImageSAPFSAPF + LEF(w/o Att.)SAPF + LEF(with Att.)14

harming the converged performance. To the best of our
knowledge, we are the ﬁrst to propose the framework of
early ﬁxing for solving integer programming. We construct
the whole early ﬁxing process as a Markov decision process,
and incorporate the imitation learning paradigm for training
with the weighted binary cross-entropy loss. Speciﬁcally, we
adopt the powerful attention layers in the policy network.
We conduct the extensive experiments for our proposed
early ﬁxing framework to three different IP applications:
constrained linear programming, MRF energy minimization
and sparse adversarial attack. The experimental results in
different scenarios demonstrate the efﬁciency of our pro-
posed learning-based early ﬁxing framework. In the future
work, we would like to apply the early ﬁxing framework to
more approximate methods, and discover more possibilities
to improve the efﬁciency and effectiveness of these methods.
Meanwhile, there is a plenty of room to mitigate the objective
gap when using some other efﬁcient policy networks.

REFERENCES

[1] Elias B Khalil. Machine learning for integer programming. In IJCAI,

pages 4004–4005, 2016.

[2] Xinchao Wang, Engin Türetken, François Fleuret, and Pascal Fua.
Tracking interacting objects optimally using integer programming.
In European Conference on Computer Vision, pages 17–32. Springer,
2014.

[3] Oktay Günlük, Jayant Kalagnanam, Minhan Li, Matt Menickelly,
and Katya Scheinberg. Optimal decision trees for categorical data
via integer programming. Journal of Global Optimization, pages 1–28,
2021.

[4] Eugene L Lawler and David E Wood. Branch-and-bound methods:

[5]

[6]

[7]

A survey. Operations research, 14(4):699–719, 1966.
James E Kelley, Jr. The cutting-plane method for solving convex
programs. Journal of the society for Industrial and Applied Mathematics,
8(4):703–712, 1960.
John E Mitchell. Branch and cut. Wiley encyclopedia of operations
research and management science, 2010.
Stephen Boyd and Lieven Vandenberghe. Convex optimization.
Cambridge university press, 2004.

[8] Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, and Horst D
Simon. Spectral relaxation for k-means clustering. In Advances in
neural information processing systems, pages 1057–1064, 2001.
Jean B Lasserre. An explicit exact sdp relaxation for nonlinear 0-1
programs. In International Conference on Integer Programming and
Combinatorial Optimization, pages 293–303. Springer, 2001.

[9]

[10] Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization
and statistical learning via the alternating direction method of multipliers.
Now Publishers Inc, 2011.

[11] Anders Rantzer. Dynamic dual decomposition for distributed
control. In 2009 American Control Conference, pages 884–888. IEEE,
2009.

[12] Nikolaos Chatzipanagiotis, Darinka Dentcheva, and Michael M
Zavlanos. An augmented lagrangian method for distributed
optimization. Mathematical Programming, 152(1):405–434, 2015.
[13] Qiang Fu, Huahua Wang, and Arindam Banerjee. Bethe-admm for
tree decomposition based parallel map inference. arXiv preprint
arXiv:1309.6829, 2013.

[14] Huahua Wang and Arindam Banerjee. Bregman alternating
direction method of multipliers. The Conference on Neural Information
Processing Systems, 2014.

[15] Xingyu Xie, Jianlong Wu, Guangcan Liu, Zhisheng Zhong, and
Zhouchen Lin. Differentiable linearized admm. In International
Conference on Machine Learning, pages 6902–6911. PMLR, 2019.
[16] Qinghua Liu, Xinyue Shen, and Yuantao Gu. Linearized admm
for nonconvex nonsmooth optimization with convergence analysis.
IEEE Access, 7:76131–76144, 2019.

[17] Baoyuan Wu and Bernard Ghanem. (cid:96)p-box admm: A versatile
framework for integer programming. IEEE transactions on pattern
analysis and machine intelligence, 41(7):1695–1708, 2019.

Fig. 11. Convergence on different methods for sparse adversarial attack:
how the objective (loss) changes with respect to the iterations during the
inference stages for one image in CIFAR-10 (left) and ImageNet (right).

follow the same setting in [24]. We keep increasing the upper
bound of (cid:96)p-norm of perturbations until the attack is success.
We compare different attack algorithms in terms of the (cid:96)p-
norm of perturbations under 100% ASR, though some sparse
attack methods fail to generate 100% ASR. Moreover, for each
image, we evaluate three different cases, i.e., the average case:
the average performance of all 9 target classes; the best case:
the performance w.r.t. the target class that is the easiest to
attack; and the worst case: the performance w.r.t. the target
class that is the most difﬁcult to attack.
Results analysis We exhibit the attack performances of
best/average/worst cases with (cid:96)p-norm and ASR in Table 5.
We present the runtime comparisons whether to use LEF or
not on SAPF in Table 6. We show the examples of generated
perturbations by different methods in Figure 10. From the
tables, we see that SAPF method achieve 100% attack success
rate under all three cases, in both CIFAR-10 and ImageNet
datasets. And with our LEF, the ASR still remains 100% in
different cases, no matter with attention layers or not.

The (cid:96)∞-norm of the one-Pixel-Attack is the largest among
all algorithms and it achieves the lowest attack success rate.
It is difﬁcult to perform targeted adversarial attacks by only
perturbing one pixel (the (cid:96)0 = 3 relates to three channels),
even on the tiny database CIFAR-10. The CornerSearch and
PGD (cid:96)0+(cid:96)∞ also fails to generate 100% success attack rate.
Comparing to all adversarial attack algorithms except one-
Pixel-Attack, SAPF method achieves the best (cid:96)1-norm and
(cid:96)2-norm under all three cases. On CIFAR-10, with our LEF
with attention layers, it achieves the better (cid:96)1-norm and (cid:96)2-
norm under all cases compared to SAPF. On ImageNet, it
achieves the better (cid:96)1-norm under all cases.

More importantly, with the aim of algorithmic accelera-
tion, we can see the obvious time speedup on the η updating
part, by Table 6. On CIFAR-10 dataset, the time speedup is
more than 5×, while on ImageNet dataset, the time speedup
is more than 2×. Those results demonstrate the efﬁciency
and effectiveness of the proposed LEF method. In Fig. 11,
we record the ﬁrst loop out of the 6 search loops for η
Updating. The ﬁgure reveals that our LEF method leads to
faster convergence than the SAPF itself, when they all have
quite similar objective (loss).

8 CONCLUSIONS AND FUTURE WORK

We propose an early ﬁxing framework, which aims to accel-
erate the approximate method by early ﬁxing the ﬂuctuated
variables to their converged states, while not signiﬁcantly

CIFAR-10ImageNet[18] Amir R Zamir, Te-Lin Wu, Lin Sun, William B Shen, Bertram E
Shi, Jitendra Malik, and Silvio Savarese. Feedback networks. In
Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 1308–1317, 2017.

[19] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens
van der Maaten, and Kilian Q Weinberger. Multi-scale dense
networks for resource efﬁcient image classiﬁcation. arXiv preprint
arXiv:1703.09844, 2017.

[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
[21] Ronald A Howard. Dynamic programming and markov processes.

1960.

[22] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning

from observation. arXiv preprint arXiv:1805.01954, 2018.

[23] Baoyuan Wu, Li Shen, Tong Zhang, and Bernard Ghanem. Map
inference via (cid:96)2-sphere linear program reformulation. International
Journal of Computer Vision, 128(7):1913–1936, 2020.

[24] Yanbo Fan, Baoyuan Wu, Tuanhui Li, Yong Zhang, Mingyang
Li, Zhifeng Li, and Yujiu Yang. Sparse adversarial attack via
perturbation factorization. In European conference on computer vision,
pages 35–50. Springer, 2020.

[25] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and
Wei Liu. Compressing convolutional neural networks via factorized
convolutional ﬁlters. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 3977–3986, 2019.
[26] Xiaoqin Zhang, Mingyu Fan, Di Wang, Peng Zhou, and Dacheng
Tao. Top-k feature selection framework using robust 0–1 integer
programming. IEEE Transactions on Neural Networks and Learning
Systems, 32(7):3005–3019, 2020.

[27] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine
learning for combinatorial optimization: a methodological tour
d’horizon. European Journal of Operational Research, 2020.

[28] Elias Khalil, Pierre Le Bodic, Le Song, George Nemhauser, and
Bistra Dilkina. Learning to branch in mixed integer programming.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 30, 2016.

[29] Alejandro Marcos Alvarez, Quentin Louveaux, and Louis Wehenkel.
A machine learning-based approximation of strong branching.
INFORMS Journal on Computing, 29(1):185–195, 2017.

[30] Maxime Gasse, Didier Chételat, Nicola Ferroni, Laurent Charlin,
and Andrea Lodi. Exact combinatorial optimization with graph
convolutional neural networks. arXiv preprint arXiv:1906.01629,
2019.

[31] Prateek Gupta, Maxime Gasse, Elias B Khalil, M Pawan Kumar,
Andrea Lodi, and Yoshua Bengio. Hybrid models for learning to
branch. arXiv preprint arXiv:2006.15212, 2020.

[32] Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforcement
learning for integer programming: Learning to cut. In International
Conference on Machine Learning, pages 9367–9376. PMLR, 2020.
[33] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer

networks. arXiv preprint arXiv:1506.03134, 2015.

[34] Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and
Samy Bengio. Neural combinatorial optimization with reinforce-
ment learning. arXiv preprint arXiv:1611.09940, 2016.

[35] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn

to solve routing problems! arXiv preprint arXiv:1803.08475, 2018.

[36] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W
Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and
Nando De Freitas. Learning to learn by gradient descent by
gradient descent. arXiv preprint arXiv:1606.04474, 2016.

[37] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint

arXiv:1606.01885, 2016.

[38] Alex Nowak, Soledad Villar, Afonso S Bandeira, and Joan Bruna. A
note on learning algorithms for quadratic assignment with graph
neural networks. stat, 1050:22, 2017.

[39] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and
Chrisina Jayne. Imitation learning: A survey of learning methods.
ACM Computing Surveys (CSUR), 50(2):1–35, 2017.

[40] Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in
imitation learning from observation. arXiv preprint arXiv:1905.13566,
2019.

[41] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via
inverse reinforcement learning. In Proceedings of the twenty-ﬁrst
international conference on Machine learning, page 1, 2004.

[42] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung.
Branchynet: Fast inference via early exiting from deep neural

15

networks. In 2016 23rd International Conference on Pattern Recognition
(ICPR), pages 2464–2469. IEEE, 2016.

[43] Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks

of the trade, pages 55–69. Springer, 1998.

[44] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early
stopping in gradient descent learning. Constructive Approximation,
26(2):289–315, 2007.

[45] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. Shallow-deep
networks: Understanding and mitigating network overthinking.
In International Conference on Machine Learning, pages 3301–3310.
PMLR, 2019.

[46] Albert N Shiryaev. Optimal stopping rules, volume 8. Springer

Science & Business Media, 2007.

[47] Sebastian Becker, Patrick Cheridito, and Arnulf Jentzen. Deep
optimal stopping. Journal of Machine Learning Research, 20:74, 2019.
[48] Xinshi Chen, Hanjun Dai, Yu Li, Xin Gao, and Le Song. Learning to
stop while learning to predict. In International Conference on Machine
Learning, pages 1520–1530. PMLR, 2020.

[49] Vitali Gintner, Natalia Kliewer, and Leena Suhl. Solving large
multiple-depot multiple-vehicle-type bus scheduling problems in
practice. OR Spectrum, 27(4):507–523, 2005.

[50] Stefan Helber and Florian Sahling. A ﬁx-and-optimize approach for
the multi-level capacitated lot sizing problem. International Journal
of Production Economics, 123(2):247–256, 2010.

[51] Árton P Dorneles, Olinto CB de Araújo, and Luciana S Buriol. A
ﬁx-and-optimize heuristic for the high school timetabling problem.
Computers & Operations Research, 52:29–38, 2014.

[52] Yang Wang, Zhipeng Lü, Fred Glover, and Jin-Kao Hao. Effective
variable ﬁxing and scoring strategies for binary quadratic pro-
gramming. In European Conference on Evolutionary Computation in
Combinatorial Optimization, pages 72–83. Springer, 2011.

[53] Michel Gendreau and Jean-Yves Potvin. Tabu search. In Search

methodologies, pages 165–186. Springer, 2005.

[54] Jen-Tzung Chien and Chun-Wei Wang. Hierarchical and self-
IEEE Transactions on Pattern

attended sequence autoencoder.
Analysis and Machine Intelligence, 2021.

[55] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 770–778,
2016.

[56] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerat-
ing deep network training by reducing internal covariate shift. In
International conference on machine learning, pages 448–456. PMLR,
2015.

[57] Tobias Achterberg and Timo Berthold. Hybrid branching.

In
International Conference on AI and OR Techniques in Constriant
Programming for Combinatorial Optimization Problems, pages 309–
311. Springer, 2009.

[58] Tobias Achterberg. Scip: solving constraint integer programs.

Mathematical Programming Computation, 1(1):1–41, 2009.

[59] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and
Aaron Courville. Film: Visual reasoning with a general conditioning
layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 32, 2018.

[60] Yuri Boykov and Vladimir Kolmogorov. An experimental compari-
son of min-cut/max-ﬂow algorithms for energy minimization in
vision. IEEE transactions on pattern analysis and machine intelligence,
26(9):1124–1137, 2004.

[61] Jianbo Shi and Jitendra Malik. Normalized cuts and image
IEEE Transactions on pattern analysis and machine

segmentation.
intelligence, 22(8):888–905, 2000.

[62] George Dantzig. Linear programming and extensions. In Linear
programming and extensions. Princeton university press, 2016.
[63] Daphne Koller and Nir Friedman. Probabilistic graphical models:

principles and techniques. MIT press, 2009.

[64] Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI
Williams, John Winn, and Andrew Zisserman. The pascal visual
object classes challenge: A retrospective. International journal of
computer vision, 111(1):98–136, 2015.

[65] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers

of features from tiny images. 2009.

[66] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
Imagenet: A large-scale hierarchical image database. In 2009 IEEE
conference on computer vision and pattern recognition, pages 248–255.
Ieee, 2009.

16

[67] Nicholas Carlini and David Wagner. Towards evaluating the
robustness of neural networks. In 2017 ieee symposium on security
and privacy (sp), pages 39–57. IEEE, 2017.

[68] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens,
and Zbigniew Wojna. Rethinking the inception architecture for
computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2818–2826, 2016.

[69] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One
pixel attack for fooling deep neural networks. IEEE Transactions on
Evolutionary Computation, 23(5):828–841, 2019.

[70] Francesco Croce and Matthias Hein. Sparse and imperceivable
adversarial attacks. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 4724–4732, 2019.

[71] Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
Frossard. Sparsefool: a few pixels make a big difference.
In
Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 9087–9096, 2019.

[72] Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Quanfu
Fan, Deniz Erdogmus, Yanzhi Wang, and Xue Lin. Structured
adversarial attack: Towards general implementation and better
interpretability. arXiv preprint arXiv:1808.01664, 2018.

