ROBUST MAHALANOBIS METRIC LEARNING VIA GEOMETRIC
APPROXIMATION ALGORITHMS

A PREPRINT

Diego Ihara Centurion ˚
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607
dihara2@uic.edu

Neshat Mohammadi ˚
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607
nmoham24@uic.edu

Francesco Sgherzi ˚
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607
fsgher2@uic.edu

Anastasios Sidiropoulos ˚
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607
sidiropo@uic.edu

March 3, 2020

ABSTRACT

Learning Mahalanobis metric spaces is an important problem that has found numerous applications.
Several algorithms have been designed for this problem, including Information Theoretic Metric
Learning (ITML) [Davis et al. 2007] and Large Margin Nearest Neighbor (LMNN) classiﬁcation
[Weinberger and Saul 2009]. We study the problem of learning a Mahalanobis metric space in the
presence of adversarial label noise. To that end, we consider a formulation of Mahalanobis metric
learning as an optimization problem, where the objective is to minimize the number of violated
similarity/dissimilarity constraints. We show that for any ﬁxed ambient dimension, there exists a
fully polynomial-time approximation scheme (FPTAS) with nearly-linear running time. This result
is obtained using tools from the theory of linear programming in low dimensions. As a consequence,
we obtain a fully-parallelizable algorithm that recovers a nearly-optimal metric space, even when a
small fraction of the labels is corrupted adversarially. We also discuss improvements of the algorithm
in practice, and present experimental results on real-world, synthetic, and poisoned data sets.

1 Introduction

0
2
0
2

b
e
F
9
2

]

G
L
.
s
c
[

3
v
9
8
9
9
0
.
5
0
9
1
:
v
i
X
r
a

Learning metric spaces is a fundamental computational primitive that has found numerous applications and has re-
ceived signiﬁcant attention in the literature. We refer the reader to [Kulis et al.(2013), Li and Tian(2018)] for detailed
exposition and discussion of previous work. At the high level, the input to a metric learning problem consists of some
universe of objects X, together with some similarity information on subsets of these objects. Here, we focus on pair-
wise similarity and dissimilarity constraints. Speciﬁcally, we are given S, D Ă
, which are sets of pairs of objects
that are labeled as similar and dissimilar respectively. We are also given some u, ℓ ą 0, and we seek to ﬁnd a mapping
˘
f : X Ñ Y , into some target metric space pY, ρq, such that for all x, y P S,

X
2

`

and for all x, y P D,

˚Authors sorted in alphabetical order.

ρpf pxq, f pyqq ď u,

ρpf pxq, f pyqq ě ℓ.

 
 
 
 
 
 
Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

In the case of Mahalanobis metric learning, we have X Ă Rd, with |X| “ n, for some d P N, and the mapping
f : Rd Ñ Rd is linear. Speciﬁcally, we seek to ﬁnd a matrix G P Rdˆd, such that for all tp, qu P S, we have

and for all tp, qu P D, we have

1.1 Our Contribution

}Gp ´ Gq}2 ď u,

}Gp ´ Gq}2 ě ℓ.

(1)

(2)

In general, there might not exist any G that satisﬁes all constraints of type 1 and 2. We are thus interested in ﬁnding
a solution that minimizes the fraction of violated constraints, which corresponds to maximizing the accuracy of the
mapping. We develop a p1 ` εq-approximation algorithm for optimization problem of computing a Mahalanobis
metric space of maximum accuracy, that runs in near-linear time for any ﬁxed ambient dimension d P N. This
algorithm is obtained using tools from geometric approximation algorithms and the theory of linear programming in
small dimension. The following summarizes our result.
Theorem 1.1. For any d P N, ε ą 0, there exists a randomized algorithm for learning d-dimensional Mahalanobis
metric spaces, which given an instance that admits a mapping with accuracy r˚, computes a mapping with accuracy
at least r˚ ´ ε, in time dOp1qnplog n{εqOpdq, with high probability.

The above algorithm can be extended to handle various forms of regularization. We also propose several modiﬁcations
of our algorithm that lead to signiﬁcant performance improvements in practice. The ﬁnal algorithm is evaluated
experimentally on both synthetic and real-world data sets, and in a data poisoning scenario, and is compared against
the currently best-known algorithms for the problem.

1.2 Related Work

Several algorithms for learning Mahalanobis metric spaces have been proposed. Notable examples include the
SDP based algorithm of Xing et al. [Xing et al.(2003)Xing, Jordan, Russell, and Ng], the algorithm of Globerson
and Roweis for the fully supervised setting [Globerson and Roweis(2006)], Information Theoretic Metric Learning
(ITML) by Davis et al. [Davis et al.(2007)Davis, Kulis, Jain, Sra, and Dhillon], which casts the problem as a particular
optimization minimizing LogDet divergence, as well as Large Margin Nearest Neighbor (LMNN) by Weinberger et
al. [Weinberger et al.(2006)Weinberger, Blitzer, and Saul], which attempts to learn a metric geared towards optimizing
k-NN classiﬁcation. We refer the reader to the surveys [Kulis et al.(2013), Li and Tian(2018)] for a detailed discussion
of previous work. Our algorithm differs from previous approaches in that it seeks to directly minimize the number of
violated pairwise distance constraints, which is a highly non-convex objective, without resorting to a convex relaxation
of the corresponding optimization problem.

1.3 Organization

The rest of the paper is organized as follows. Section 2 describes the main algorithm and the proof of Theorem
1.1. Section 3 discusses practical improvements used in the implementation of the algorithm. Section 4 presents the
experimental evaluation.

2 Mahalanobis Metric Learning as an LP-Type Problem

In this Section we present an approximation scheme for Mahalanobis metric learning in d-dimensional Euclidean
space, with nearly-linear running time. We begin by recalling some prior results on the class of LP-type problems,
which generalizes linear programming. We then show that linear metric learning can be cast as an LP-type problem.

2.1 LP-type Problems

Let us recall the deﬁnition of an LP-type problem. Let H be a set of constraints, and let w : 2H Ñ R Y t´8, `8u,
such that for any G Ă H, wpGq is the value of the optimal solution of the instance deﬁned by G. We say that pH, wq
deﬁnes an LP-type problem if the following axioms hold:

(A1) Monotonicity. For any F Ď G Ď H, we have wpF q ď wpGq.
(A2) Locality. For any F Ď G Ď H, with ´8 ă wpF q “ wpGq, and any h P H, if wpGq ă wpG Y thuq, then

wpF q ă wpF Y thuq.

2

Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

More generally, we say that pH, wq deﬁnes an LP-type problem on some H1 Ď H, when conditions (A1) and (A2)
hold for all F Ď G Ď H1.
A subset B Ď H is called a basis if wpBq ą ´8 and wpB1q ă wpBq for any proper subset B1 Ĺ B. A basic
operation is deﬁned to be one of the following:

(B0) Initial basis computation. Given some G Ď H, compute any basis for G.
(B1) Violation test. For some h P H and some basis B Ď H, test whether wpB Y thuq ą wpBq (in other words,

whether B violates h).

(B2) Basis computation. For some h P H and some basis B Ď H, compute a basis of B Y thu.

2.2 An LP-type Formulation

We now show that learning Mahalanobis metric spaces can be expressed as an LP-type problem. We ﬁrst note that we
can rewrite (1) and (2) as

and

where A “ GT G is positive semideﬁnite.

pp ´ qqT App ´ qq ď u2,

pp ´ qqT App ´ qq ě ℓ2,

(3)

(4)

Rd
2

We deﬁne H “ t0, 1u ˆ
, where for each p0, tp, quq P H, we have a constraint of type (3), and for every
p1, tp, quq P H, we have a constraint of type (4). Therefore, for any set of constraints F Ď H, we may associate the
˘
set of feasible solutions for F with the set AF of all positive semideﬁnite matrices A P Rnˆn, satisfying (3) and (4)
for all constraints in F .
Let w : 2H Ñ R, such that for all F P H, we have

`

wpF q “

inf APAF rT Ar
8

"

if AF ‰ H
if AF “ H

,

where r P Rd is a vector chosen uniformly at random from the unit sphere from some rotationally-invariant probability
measure. Such a vector can be chosen, for example, by ﬁrst choosing some r1 P Rd, where each coordinate is sampled
from the normal distribution N p0, 1q, and setting r “ r1{}r1}2.
Lemma 2.1. When w is chosen as above, the pair pH, wq deﬁnes an LP-type problem of combinatorial dimension
Opd2q, with probability 1. Moreover, for any n ą 0, if each ri is chosen using Ωplog nq bits of precision, then for each
F Ď H, with n “ |F |, the assertion holds with high probability.

Proof. Since adding constraints to a feasible instance can only make it infeasible, it follows that w satisﬁes the mono-
tonicity axiom (A1).

We next argue that the locality axion (A2) also holds, with high probability. Let F Ď G Ď H, with ´8 ă wpF q “
wpGq, and let h P H, with wpGq ă wpG Y thuq. Let AF P AF and AG P AG be some (not necessarily unique)

Algorithm 1 An exact algorithm for Mahalanobis metric learning.

procedure Exact-LPTML(F ; B)

if F “ H then

A Ð Basic-LPTMLpBq

else

choose h P F uniformly at random
A Ð Exact-LPTMLpF ´ thuq
if A violates h then

A :“ Exact-LPTMLpF ´ thu; B Y thuq

end if

end if
return A
end procedure

3

Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

inﬁmizers of wpAq, when A ranges in AF and AG respectively. The set AF , viewed as a convex subset of Rd2
, is
the intersection of the SDP cone with n half-spaces, and thus AF has at most n facets. There are at least two distinct
inﬁmizers for wpAGq, when AG P AG, only when the randomly chosen vector r is orthogonal to a certain direction,
which occurs with probability 0. When each entry of r is chosen with c log n bits of precision, the probability that r is
orthogonal to any single hyperplane is at most 2´c log n “ n´c; the assertion follows by a union bound over n facets.
This establishes that axiom (A2) holds with high probability.
It remains to bound the combinatorial dimension, κ. Let F Ď H be a set of constraints. For each A P AF , deﬁne the
ellipsoid

EA “ tv P Rd : }Av}2 “ 1u.
For any A, A1 P AF , with EA “ EA1, and A “ GT G, A1 “ G1T G1, we have that for all p, q P Rd, }Gp ´ Gq}2 “
pp ´ qqT App ´ qq “ pp ´ qqT A1pp ´ qq “ }G1p ´ G1q}2. Therefore in order to specify a linear transformation G,
up to an isometry, it sufﬁces to specify the ellipsoid EA.

Each tp, qu P S corresponds to the constraint that the point pp ´ qq{u must lie in EA. Similarly each tp, qu P D
corresponds to the constraint that the point pp ´ qq{ℓ must lie either on the boundary or the exterior of EA. Any
ellipsoid in Rd is uniquely determined by specifying at most pd ` 3qd{2 “ Opd2q distinct points on its boundary (see
[Welzl(1991), Chazelle(2000)]). Therefore, each optimal solution can be uniquely speciﬁed as the intersection of at
most Opd2q constraints, and thus the combinatorial dimension is Opd2q.

Lemma 2.2. Any initial basis computation (B0), any violation test (B1), and any basis computation (B2) can be
performed in time dOp1q.

Proof. The violation test (B1) can be performed by solving one SDP to compute wpBq, and another to compute
wpB Y thuq. By Lemma 2.1 the combinatorial dimension is Opd2q, thus each SDP has Opd2q constraints, and be
solved in time dOp1q.

The basis computation step (B2) can be performed starting with the set of constraints B Y thu, and iteratively remove
every constraint whose removal does not decrease the optimum cost, until we arrive at a minimal set, which is a basis.
In total, we need to solve at most d SDPs, each of size Opd2q, which can be done in total time dOp1q.

Finally, by the choice of w, any set containing a single constraint in S is a valid initial basis.

2.3 Algorithmic implications

Using the above formulation of Mahalanobis metric learning as an LP-type problem, we can obtain our approximation
scheme. Our algorithm uses as a subroutine an exact algorithm for the problem (that is, for the special case where we
seek to ﬁnd a mapping that satisﬁes all constraints). We ﬁrst present the exact algorithm and then show how it can be
used to derive the approximation scheme.

An exact algorithm.
[Welzl(1991)] obtained a simple randomized linear-time algorithm for the minimum enclosing
ball and minimum enclosing ellipsoid problems. This algorithm naturally extends to general LP-type problems (we
refer the reader to [Har-Peled(2011), Chazelle(2000)] for further details).

With the interpretation of Mahalanobis metric learning as an LP-type problem given above, we thus obtain a linear time
algorithm for in Rd, for any constant d P N. The resulting algorithm on a set of constraints F Ď H is implemented

Algorithm 2 An approximation algorithm for Mahalanobis metric learning.

procedure LPTML(F )

for i “ 0 to log1`ε n do
p Ð p1 ` εq´i
for j “ 1 to logOpd2

q n do

subsample Fj Ď F , where each element is chosen independently with probability p
Ai,j Ð Exact-LPTMLpFjq

end for

end for
return a solution out of tAi,jui,j, violating the minimum number of constraints in F

end procedure

4

Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

by the procedure Exact-LPTMLpF ; Hq, which is presented in Algorithm 1. The procedure LPTMLpF ; Bq takes as
input sets of constraints F, B Ď H. It outputs a solution A P Rdˆd to the problem induced by the set of constraints
F Y B, such that all constraints in B are tight (that is, they hold with equality); if no such solution solution exists, then
it returns nil. The procedure Basic-LPTMLpBq computes LPTMLpH; Bq. The analysis of [Welzl(1991)] implies that
when Basic-LPTMLpBq is called, the cardinality of B is at most the combinatorial dimension, which by Lemma 2.1
is Opd2q. Thus the procedure Basic-LPTML can be implemented using one initial basis computation (B0) and Opd2q
basis computations (B2), which by Lemma 2.2 takes total time dOp1q.

An p1 ` εq-approximation algorithm.
It is known that the above exact linear-time algorithm leads to an nearly-
linear-time approximation scheme for LP-type problems. This is summarized in the following. We refer the reader to
[Har-Peled(2011)] for a more detailed treatment.
Lemma 2.3 ([Har-Peled(2011)], Ch. 15). Let A be some LP-type problem of combinatorial dimension κ ą 0, deﬁned
by some pair pH, wq, and let ε ą 0. There exists a randomized algorithm which given some instance F Ď H, with
|F | “ n, outputs some basis B Ď F , that violates at most p1 ` εqk constraints in F , such that wpBq ď wpB1q, for any
basis B1 violating at most k constraints in F , in time O
, where t0
t0 `
´
is the time needed to compute an arbitrary initial basis of A, and t1, t2, and t3 are upper bounds on the time needed
to perform the basic operations (B0), (B1) and (B2) respectively. The algorithm succeeds with high probability.

, logκ`2 n
kε2κ`2

logκ`1 n
ε2κ

n ` n min

pt1 ` t2q

)¯

!

¯

´

For the special case of Mahalanobis metric learning, the corresponding algorithm is given in Algorithm 2. The approx-
imation guarantee for this algorithm is summarized in 1.1. We can now give the proof of our main result.

Proof of Theorem 1.1. Follows immediately by Lemmas 2.2 and 2.3.

Regularization. We now argue that the LP-type algorithm described above can be extended to handle certain types
of regularization on the matrix A. In methods based on convex optimization, introducing regularizers that are convex
functions can often be done easily. In our case, we cannot directly introduce a regularizing term in the objective
function that is implicit in Algorithm 2. More speciﬁcally, let costpAq denote the total number of constraints of type (3)
and (4) that A violates. Algorithm 2 approximately minimizes the objective function costpAq. A natural regularized
version of Mahalanobis metric learning is to instead minimize the objective function cost1pAq :“ costpAq`η ¨regpAq,
for some η ą 0, and regularizer regpAq. One typical choice is regpAq “ trpACq, for some matrix C P Rdˆd; the
case C “ I corresponds to the trace norm (see [Kulis et al.(2013)]). We can extend the Algorithm 2 to handle any
regularizer that can be expressed as a linear function on the entries of A, such as trpAq. The following summarizes
the result.
Theorem 2.4. Let regpAq be a linear function on the entries of A, with polynomially bounded coefﬁcients. For any
d P N, ε ą 0, there exists a randomized algorithm for learning d-dimensional Mahalanobis metric spaces, which
given an instance that admits a solution A0 with cost1pA0q “ c˚, computes a solution A with cost1pAq ď p1 ` εqc˚,
in time dOp1qnplog n{εqOpdq, with high probability.

, for sufﬁciently large constant t1 ą 0, since there are at most

Proof. If η ă εt, for sufﬁciently large constant t ą 0, since the coefﬁcients in regpAq are polynomially bounded, it fol-
lows that the largest possible value of η ¨regpAq is Opεq, and can thus be omitted without affecting the result. Similarly,
if η ą p1{εqnt1
constraints, it follows that the term
costpAq can be omitted form the objective. Therefore, we may assume w.l.o.g. that regpA0q P rεOp1q, p1{εqnOp1qs.
We can guess some i “ Oplog n ` logp1{εqq, such that regpA0q P pp1 ` εqi´1, p1 ` εqis. We modify the SDP used
in the proof of Lemma 2.2 by introducing the constraint regpAq ď p1 ` εqi. Guessing the correct value of i requires
Oplog n ` logp1{εqq executions of Algorithm 2, which implies the running time bound.

n
2

˘

`

3 Practical Improvements and Parallelization

We now discuss some modiﬁcations of the algorithm described in the previous section that signiﬁcantly improve its
performance in practical scenarios, and have been integrated in our implementation.

Move-to-front and pivoting heuristics. We use heuristics that have been previously used in algorithms for linear
programming [Seidel(1990), Clarkson(1995)], minimum enclosing ball in R3 [Megiddo(1983)], minimum enclosing
ball and ellipsoid is Rd, for any ﬁxed d P N [Welzl(1991)], as well as in fast implementations of minimum enclosing

5

Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

s
t
n
i
a
r
t
s
n
o
c
d
e
t
a
l
o
i
v
f
o
n
o
i
t
c
a
r
F

d “ 4
d “ 8
d “ 12

0.5

0.4

0.3

10

20

30

40

50

Iterations

(a) Fraction of violated constraints

N
N

-
k
f
o
y
c
a
r
u
c
c
A

0.8

0.6

0.4

0.2

d “ 4
d “ 8
d “ 12

10

20

30

40

50

Iterations

(b) Accuracy

Figure 1: Fraction of violated constraints (left) and accuracy (right) as the number of iterations of LPTML increases (average over
10 executions). Each curve corresponds to the Wine data reduced to d dimensions using PCA.

100

0

´100

100

0

´100

10

0

´100 ´50

0

´100 ´50

0

(a) Poisoned data

(b) After LPTML

´10

´125 ´105

´5

5

(c) After LPTML
(magniﬁed view)

Figure 2: Data poisoning scenario. On the left, the data set before learning. The center image shows the correct transformation
recovered by LPTML. On the right, a magniﬁed view of the recovered data.

ball algorithms [G¨artner(1999)]. The move-to-front heuristic keeps an ordered list of constraints which gets reorga-
nized as the algorithm runs; when the algorithm ﬁnds a violation, it moves the violating constraint to the beginning
of the list of the current sub-problem. The pivoting heuristic further improves performance by choosing to add to the
basis the constraint that is “violated the most”. For instance, for similarity constraints, we pick the one that is mapped
to the largest distance greater than u; for dissimilarity constraints, we pick the one that is mapped to the smallest
distance less than ℓ.

Approximate counting. The main loop of Algorithm 2 involves counting the number of violated constraints in each
iteration. In problems involving a large number of constraints, we use approximate counting by only counting the
number of violations within a sample of Oplog 1{εq constraints.

Early termination. A bottleneck of Algorithm 2 stems from the fact that the inner loop needs to be executed for
logOpd2
q n iterations. In practice, we have observed that a signiﬁcantly smaller number of iterations is needed to
achieve high accuracy. We denote by LPTMLt for the version of the algorithm that performs a total of t iterations of
the inner loop.

Parallelization. Algorithm 2 consists of several executions of the algorithm Exact-LPTML on independently sam-
pled sub-problems. Therefore, Algorithm 2 can trivially be parallelized by distributing a different set of sub-problems
to each machine, and returning the best solution found overall.

6

Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

Data set

Iris
Soybean
Synthetic

ITML

LMNN

0.96 ˘ 0.01
0.95 ˘ 0.04
0.97 ˘ 0.02

0.96 ˘ 0.02
0.96 ˘ 0.04
1.00 ˘ 0.00

LPTMLt“2000
0.94 ˘ 0.04
0.90 ˘ 0.05
1.00 ˘ 0.00

Table 1: Average accuracy and standard deviation over 50 executions of ITML, LMNN and LPTML.

N
N
´
k
f
o
y
c
a
r
u
c
c
A

1

0.8

0.6

Euclidean

ITML

LMNN LPTMLt“100

Without noise With noise

Figure 3: Accuracy in a data poisoning scenario. See Figure 2a.

4 Experimental Evaluation

We have implemented Algorithm 2, incorporating the practical improvements described in Section 3, and performed
experiments on synthetic and real-world data sets. Our LPTML implementation and documentation can be found in
our repository. We now describe the experimental setting and discuss the main ﬁndings.

4.1 Experimental Setting

Classiﬁcation task. Each data set used in the experiments consists of a set of labeled points in Rd. The label of each
point indicates its class, and there is a constant number of classes. The set of similarity constraints S (respt. dissimilar-
ity constraints D) is formed by uniformly sampling pairs of points in the same class (resp. from different classes). We
use various algorithms to learn a Mahalanobis metric for a labeled input point set in Rd, given these constraints. The
values u and ℓ are chosen as the 90th and 10th percentiles of all pairwise distances. We used 2-fold cross-validation:
At the training phase we learn a Mahalanobis metric, and in the testing phase we use k-NN classiﬁcation, with k “ 4,
to evaluate the performance.

Data sets. We have tested our algorithm on the following synthetic and real-world data sets:

1. Real-world: We have tested the performance of our implementation on the Iris, Wine, Ionosphere and Soybean

data sets from the UCI Machine Learning Repository2.

2. Synthetic: Next, we consider a synthetic data set that is constructed by ﬁrst sampling a set of 100 points from
a mixture of two Gaussians in R2, with identity covariance matrices, and with means p´3, 0q and p3, 0q
respectively; we then apply a linear transformation that stretches the y axis by a factor of 40. This linear
transformation reduces the accuracy of k-NN on the underlying Euclidean metric with k “ 4 from 1 to 0.68.

3. Data poisoning: We modify the above synthetic data set by introducing a small fraction of points in an adversarial
manner, before applying the linear transformation. Figure 2b depicts the noise added as ﬁve points labeled as
one of the classes, and sampled from a Gaussian with identity covariance matrix and mean p´100, 0q (Figure
2a).

Algorithms. We compare the performance of our algorithm against ITML and LMNN. We used the implementations
provided by the authors of these works, with minor modiﬁcations.

2https://archive.ics.uci.edu/ml/datasets.php

7

Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

)
c
e
s
(

e
m

i
t
g
n
i
n
n
u
R

1,000

500

0

LPTMLt“100
LPTMLt“500
LPTMLt“1000
LPTMLt“1500
LPTMLt“2000

2 3 4 5 6 7 8 9 10 11 12 13

Dimension d

Figure 4: Average running time of 10 executions of LPTML at different levels of dimensionality reduction using PCA on the Wine
data set. Each curve corresponds to LPTML limited to t iterations.

Ionosphere
Soybean
Wine

)
c
e
s
(

e
m

i
t
g
n
i
n
n
u
R

4,000

3,000

2,000

1,000

1

2

4

8

Number of machines

Figure 5: Running time for parallel LPTML on an increasing number of machines (average over 10 executions).

4.2 Results

Accuracy. Algorithm 2 minimizes the number of violated pairwise distance constraints. It is interesting to examine
the effect of this objective function on the accuracy of k-NN classiﬁcation. Figure 1 depicts this relationship for the
Wine data set. We observe that, in general, as the number of iterations of the main loop of LPTML increases, the
number of violated pairwise distance constraints decreases, and the accuracy of k-NN increases. This phenomenon
remains consistent when we ﬁrst perform PCA to d “ 4, 8, 12 dimensions.

Comparison to ITML and LMNN. We compared the accuracy obtained by LPTMLt, for t “ 2000 iterations, against
ITML and LMNN.

Table 1 summarizes the ﬁndings on the real-world and data sets and the synthetic data set without adversarial noise.
We observe that LPTML achieves accuracy that is comparable to ITML and LMNN.
We observe that LPTML outperforms ITML and LMNN on the poisoned data set. This is due to the fact that the
introduction of adversarial noise causes the relaxations used in ITML and LMNN to be biased towards contracting the
x-axis. In contrast, the noise does not “fool” LPTML because it only changes the optimal accuracy by a small amount.
The results are summarized in Figure 3.

The effect of dimension. The running time of LPTML grows with the dimension d. This is caused mostly by
the fact that the combinatorial dimension of the underlying LP-type problem is Opd2q, and thus performing each
basic operation requires solving an SDP with Opd2q constraints. Figure 4 depicts the effect of dimensionality in the
running time, for t “ 100, . . . , 2000 iterations of the main loop. The data set used is Wine after performing PCA to d
dimensions, for d “ 2, . . . , 13.

Parallel implementation. We implemented a massively parallel version of LPTML in the MapReduce model.
The program maps different sub-problems of the main loop of LPTML to different machines.
In the reduce

8

Robust Mahalanobis Metric Learning via Geometric Approximation Algorithms

A PREPRINT

step, we keep the result with the minimum number of constraint violations. The implementation uses the mrjob
[Yelp and Contributors(2019)] package. For these experiments, we used Amazon cloud computing instances of type
m4.xlarge, AMI 5.20.0 and conﬁgured with Hadoop. As expected, the training time decreases as the number of avail-
able processors increases (Figure 5). All technical details about this implementation can be found in the parallel
section of the documentation of our code.

5 Conclusions

We have shown that the problem of learning a Mahalanobis metric space can be cast as an LP-type problem. This
formulation allows us to obtain an efﬁcient approximation scheme using tools from the theory of linear programming
in low dimensions. Speciﬁcally, we present a near-linear time p1 ` εq-approximation algorithm that minimizes the
number of violated constraints. Experimental evaluation demonstrates that when compared to prior work, our method
is signiﬁcantly more robust against small adversarial modiﬁcations of the input labelling. Our approach also leads to
a fully parallelizable algorithm.

It is an interesting research direction to extend our approximation algorithm to other classes of metric learning prob-
lems. One such case is when the input is speciﬁed as a set of ordered triples px, y, zq, and the goal is to ﬁnd a mapping
f with }f pxq ´ f pyq}2 ď }f pxq ´ f pzq}2 ´ m, for some margin m ą 0 (see [?]). Another important direction is to
obtain geometric approximation algorithms for non-linear metric learning primitives, such as mappings computed by
small depth neural networks.

References

[Chazelle(2000)] B. Chazelle. The discrepancy method: randomness and complexity. Cambridge University Press,

2000.

[Clarkson(1995)] K. L. Clarkson. Las vegas algorithms for linear and integer programming when the dimension is

small. Journal of the ACM (JACM), 42(2):488–499, 1995.

[Davis et al.(2007)Davis, Kulis, Jain, Sra, and Dhillon] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.
Information-theoretic metric learning. In Proceedings of the 24th international conference on Machine learn-
ing, pages 209–216. ACM, 2007.

[G¨artner(1999)] B. G¨artner. Fast and robust smallest enclosing balls. In European Symposium on Algorithms, pages

325–338. Springer, 1999.

[Globerson and Roweis(2006)] A. Globerson and S. T. Roweis. Metric learning by collapsing classes. In Advances

in neural information processing systems, pages 451–458, 2006.

[Har-Peled(2011)] S. Har-Peled. Geometric approximation algorithms. Number 173. American Mathematical Soc.,

2011.

[Kulis et al.(2013)] B. Kulis et al. Metric learning: A survey. Foundations and Trends R(cid:13) in Machine Learning, 5(4):

287–364, 2013.

[Li and Tian(2018)] D. Li and Y. Tian. Survey and experimental study on metric learning methods. Neural Networks,

105:447–462, 2018.

[Megiddo(1983)] N. Megiddo. Linear-time algorithms for linear programming in rˆ3 and related problems. SIAM

journal on computing, 12(4):759–776, 1983.

[Seidel(1990)] R. Seidel. Linear programming and convex hulls made easy.
symposium on Computational geometry, pages 211–215. ACM, 1990.

In Proceedings of the sixth annual

[Weinberger et al.(2006)Weinberger, Blitzer, and Saul] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric
learning for large margin nearest neighbor classiﬁcation. In Advances in neural information processing systems,
pages 1473–1480, 2006.

[Welzl(1991)] E. Welzl. Smallest enclosing disks (balls and ellipsoids). In New results and new trends in computer

science, pages 359–370. Springer, 1991.

[Xing et al.(2003)Xing, Jordan, Russell, and Ng] E. P. Xing, M. I. Jordan, S. J. Russell, and A. Y. Ng. Distance metric
In Advances in neural information processing

learning with application to clustering with side-information.
systems, pages 521–528, 2003.

[Yelp and Contributors(2019)] Yelp and Contributors. Mrjob library, 2019. URL http://pythonhosted.org/mrjob/.

v0.6.8.dev0.

9

