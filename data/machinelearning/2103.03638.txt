43

2
2
0
2

b
e
F
8
2

]
I

A
.
s
c
[

3
v
8
3
6
3
0
.
3
0
1
2
:
v
i
X
r
a

PRIMA: General and Precise Neural Network Certification
via Scalable Convex Hull Approximations

MARK NIKLAS MÜLLER∗, ETH Zurich, Switzerland
GLEB MAKARCHUK∗, ETH Zurich, Switzerland
GAGANDEEP SINGH, UIUC and VMware Research, United States
MARKUS PÜSCHEL, ETH Zurich, Switzerland
MARTIN VECHEV, ETH Zurich, Switzerland

Formal verification of neural networks is critical for their safe adoption in real-world applications. However,
designing a precise and scalable verifier which can handle different activation functions, realistic network
architectures and relevant specifications remains an open and difficult challenge.

In this paper, we take a major step forward in addressing this challenge and present a new verification
framework, called Prima. Prima is both (i) general: it handles any non-linear activation function, and (ii) precise:
it computes precise convex abstractions involving multiple neurons via novel convex hull approximation
algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity,
yield fewer constraints, and minimize precision loss.

We evaluate the effectiveness of Prima on a variety of challenging tasks from prior work. Our results show
that Prima is significantly more precise than the state-of-the-art, verifying robustness to input perturbations
for up to 20%, 30%, and 34% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks,
respectively. Further, Prima enables, for the first time, the precise verification of a realistic neural network for
autonomous driving within a few minutes.

CCS Concepts: • Theory of computation → Abstraction; Program verification; • Computing method-
ologies → Neural networks.

Additional Key Words and Phrases: Robustness, Convexity, Polyhedra, Abstract Interpretation

ACM Reference Format:
Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev. 2022. PRIMA:
General and Precise Neural Network Certification via Scalable Convex Hull Approximations. Proc. ACM
Program. Lang. 6, POPL, Article 43 (January 2022), 33 pages. https://doi.org/10.1145/3498704

1 INTRODUCTION

The growing adoption of neural networks (NNs) in many safety critical domains highlights the
importance of providing formal, deterministic guarantees about their safety and robustness when
deployed in the real world [Szegedy et al. 2014]. While the last few years have seen significant

∗Equal contribution

Authors’ addresses: Mark Niklas Müller, Department of Computer Science, ETH Zurich, Zurich, Switzerland, mark.mueller@
inf.ethz.ch; Gleb Makarchuk, Department of Computer Science, ETH Zurich, Zurich, Switzerland, gleb.makarchuk@gmail.
com; Gagandeep Singh, UIUC and VMware Research, United States, ggnds@illinois.edu, gasingh@vmware.com; Markus
Püschel, Department of Computer Science, ETH Zurich, Switzerland, pueschel@inf.ethz.ch; Martin Vechev, Department of
Computer Science, ETH Zurich, Zurich, Switzerland, martin.vechev@inf.ethz.ch.

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,
contact the owner/author(s).
© 2022 Copyright held by the owner/author(s).
2475-1421/2022/1-ART43
https://doi.org/10.1145/3498704

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

 
 
 
 
 
 
43:2

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

Single-Neuron

Disjunct Multi-Neuron

Overlapping Multi-Neuron

Optimal Convex

(a)

(b)

(c)

(d)

Fig. 1. Illustration of the tightness of different abstraction strategies, for a layer of four neurons (grey dots).
Strong interdependencies between neurons that can be captured directly or indirectly are shown as solid
or dashed lines, respectively. Individual single-neuron, multi-neuron or optimal convex abstractions are
illustrated in blue and the resulting overall layer-wise abstraction in green.

progress in formal verification of NNs, existing deterministic methods (see Urban and Miné [2021]
for a survey) still either do not scale to or are too imprecise when handling realistic networks.

Key challenge: handling non-linearities. Neural
networks interleave affine and non-linear activa-
tion layers (e.g., ReLU, Sigmoid), leading to highly
non-linear behaviours. Because affine layers can
be captured exactly using linear constraints, the
key challenge in neural network verification rests
in designing methods that can handle the effect
of these non-linear activations in a precise and
scalable manner.

𝑦

𝑦 = max(0, 𝑥)

𝑦 ≤ 𝑢𝑥
𝑢𝑥 −𝑙𝑥

(𝑥 − 𝑙𝑥 )

𝑙𝑥

𝑦 ≥ 0

𝑦 ≥ 𝑥

𝑥

𝑢𝑥

Fig. 2. Convex single-neuron approximation (blue)
of a ReLU (black) with bounded inputs 𝑥 ∈ [𝑙𝑥 , 𝑢𝑥 ].

Exact verification, e.g., [Anderson et al. 2019, 2020; Bunel et al. 2020b; Ehlers 2017; Katz et al.
2017; Singh et al. 2019c; Tjeng et al. 2019; Wang et al. 2018, 2021], has, in the worst-case, exponential
complexity in the (large) number of non-linear activations due to a combinatorial blow-up of
case distinctions (e.g., for ReLUs) and complex shapes for general activations (e.g., for Sigmoids).
Therefore exact verifiers typically only handle piecewise linear activations and do not scale to
larger networks.

To overcome this limitation, state-of-the-art verifiers, e.g., [Singh et al. 2019a,b; Tjandraatmadja
et al. 2020; Weng et al. 2018; Xu et al. 2020; Zhang et al. 2018], often sacrifice completeness for
scalability and leverage abstract interpretation [Cousot 1996] to over-approximate the effect of each
activation layer with convex polyhedra. Naturally, the scalability and precision of these incomplete
methods are tied to the particular polyhedral fragment they utilize.

Below, we contrast different state-of-the-art abstraction approaches with our work by comparing
the strong inter-neuron dependencies they can capture directly or indirectly, illustrated as solid
or dashed lines, respectively, in Figure 1 for a layer of four neurons. Individual abstractions are
visualized in blue and the resulting layer-wise shape in green.

Optimal convex approximation. Assume a layer of 𝑛 neurons, each applying the scalar, univariate,
non-linear activation function 𝑓 : R → R and the most precise polyhedral abstraction P of the
layer’s inputs 𝒙. The most precise convex abstraction of the layer output is then given by the convex
hull of all input-output vector pairs conv({(𝒙, 𝒇 (𝒙)) | 𝒙 ∈ P ⊆ R𝑛 }), illustrated in Figure 1 (d),
where all interactions are fully captured. Computing this 2𝑛-dimensional convex hull, however, is
intractable due to the exponential cost O (𝑛𝑣 log(𝑛𝑣) +𝑛𝑛
𝑣 ) [Chazelle 1993] in the number of neurons
𝑛, where the number of vertices 𝑛𝑣 = O (𝑛𝑛
𝑐 ) of the input polytope P is at worst also exponential in
𝑛 [Seidel 1995] (𝑛𝑐 is the number of constraints of the input polytope P).

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:3

Single-Neuron approximation. Most incomplete verifiers are fundamentally based on single-
neuron convex abstractions, i.e., activations are approximated separately. The tightest single-neuron
abstractions maintain upper and lower bounds 𝑙𝑥, 𝑢𝑥 for each input 𝑥 and compute convex hulls of
all input-output tuples: conv({(𝑥, 𝑓 (𝑥)) | 𝑥 ∈ [𝑙𝑥, 𝑢𝑥 ] ⊆ R}), as illustrated in Figure 2 for a ReLU.
The union of the obtained constraints is the final abstraction of the layer. Geometrically, it is the
Cartesian product of the convex hulls for each ReLU. This abstraction is significantly larger in
volume (exponential in 𝑛) than the optimal convex hull discussed earlier, the key reason being that
the interdependencies between neurons in the same layer are ignored, as illustrated in Figure 1 (a).
Thus, the approximation error can grow exponentially with each layer, accumulating significant
imprecision.

Multi-Neuron approximation. To mitigate this limitation for ReLU networks, recent works [Palma
et al. 2021; Singh et al. 2019a; Tjandraatmadja et al. 2020] introduced multi-neuron abstractions as
a first compromise between the optimal but intractable layer-wise and the imprecise but scalable
neuron-wise abstraction. Singh et al. [2019a] partition the neurons of an activation layer into
small sets of size 𝑛𝑠 ≤ 5, form groups of 𝑘 ≤ 3 neurons for each partition, approximate the
group’s input with octahedra [Clarisó and Cortadella 2007], and then compute exact convex hulls
jointly approximating the output of 𝑘 ReLUs for this input. These exact convex hull computations
are computationally expensive and yield complex constraints, limiting the approach to only a
few, mostly disjoint neuron groups, and restricting the number of captured dependencies, see
Figure 1 (b). Tjandraatmadja et al. [2020] and Palma et al. [2021] merge the activation layer with
the preceding affine layer and compute a convex approximation over the resulting multivariate
activation layer for a hyperbox approximation of its input. This coarse input abstraction effectively
restricts their approach to interactions over a single affine layer at a time. While both approaches
currently yield state-of-the-art precision, they are limited to ReLU activations and lack scalability
as they require small instances of the NP-hard convex hull problem to be solved exactly or large
instances to be solved partially. They also do not address the problem of capturing enough neuron-
interdependencies within a layer to come as close as possible to the optimal convex abstraction.

This work: precise multi-neuron approximations. In this work, we present the first general verifi-
cation framework for networks with arbitrary, bounded, multivariate activation functions called
Prima (PRecIse Multi-neuron Abstraction). Prima builds on the group-wise approximations from
Singh et al. [2019a] and leverages the key insight that most interdependencies between neurons
can be captured by considering a large number of relatively small, overlapping neuron-groups.
While not achieving the tightness of the optimal convex approximation, Prima yields much tighter
layer-wise approximations than previous methods, as shown in Figure 1 (c).

The key technical contributions of our work are: (i) PDDM (Partial Double Description Method)
– a general, precise, and fast convex hull approximation method for polytopes that enables the
consideration of many neuron groups, and (ii) SBLM (Split-Bound-Lift Method) – a novel decompo-
sition approach that builds upon the PDDM to quickly compute multi-neuron constraints. While
we combine these methods with abstraction refinement approaches in Prima, we note that they
are also of general interest (beyond neural networks) and can be used independently of each other.
Prima can be applied to any network with bounded, multivariate activation functions and
arbitrary specifications expressible as polyhedra such as individual fairness [Ruoss et al. 2020b];
global safety properties [Katz et al. 2017]; and acoustic [Ryou et al. 2020], geometric [Balunovic
et al. 2019], spatial [Ruoss et al. 2020a], and ℓ𝑝 -norm bounded perturbations [Gehr et al. 2018]. Our
experimental evaluation shows that Prima achieves state-of-the-art precision on the majority of
our ReLU-based classifiers while remaining competitive on the rest. For Sigmoid- and Tanh-based
networks, Prima significantly outperforms prior work on all benchmarks. Further, Prima enables,
for the first time, precise and scalable verification of a realistic architecture for autonomous driving

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:4

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

containing > 100k neurons in a regression setting. Finally, while Prima is incomplete, it can be
used for boosting the scalability of state-of-the-art complete verifiers [Singh et al. 2019c; Wang
et al. 2021] for ReLU-based networks that benefit from more precise convex abstractions.

Main contributions. Our key contributions are:
(1) PDDM, a precise method for approximating the convex hull of polytopes, with worst-case
polynomial time- and space-complexity and exactness guarantees in low dimensions.
(2) Split-Bound-Lift Method, a technique which efficiently computes joint constraints over
groups of non-linear functions, by decomposing the underlying convex hull problem into
lower-dimensional spaces.

(3) Prima, a novel verifier combining these approaches with a sparse neuron grouping technique
and abstraction refinement, to obtain the first multi-neuron verifier for arbitrary, bounded,
multivariate non-linear activations (e.g., ReLU, Sigmoid, Tanh, and MaxPool).

(4) An evaluation of Prima on a range of activations and network architectures (e.g., fully
connected, convolutional, and residual). We show that Prima is significantly more precise
than state-of-the-art, with gains of up to 20%, 30%, and 34% for ReLU-, Sigmoid-, and Tanh-
based networks, while being effective in a regression setting, scaling to large networks, and
enabling verification in real-world settings such as autonomous driving.

We release our code as part of the open-source framework ERAN at https://github.com/eth-sri/eran.

2 BACKGROUND

In this section, we establish the terminology we use to discuss polyhedra, neural networks (NNs)
and their verification.

Notation. We use lower case Latin or Greek letters 𝑎, 𝑏, 𝑥, . . . , 𝜆, . . . for scalars, bold for vectors
𝒂, capitalized bold for matrices 𝑨, and calligraphic A or blackboard bold A for sets. Similarly, we
denote scalar functions as 𝑓 : R𝑑 → R and vector valued functions bold as 𝒇 : R𝑑 → R𝑘 .

Neural networks. We focus our discussion on networks 𝒉(𝒙) : X → R|Y | that map input samples
(images) 𝒙 ∈ X to numerical scores 𝒚 ∈ R|Y |. For a classification task, the network 𝒉 classifies an
input 𝒙 by applying argmax to its output: 𝑐 (𝒙) = arg max𝑗 𝒉(𝒙) 𝑗 . While our methods can refine the
abstraction of activation functions in arbitrary neural architectures [Xu et al. 2020], for simplicity,
we discuss a feedforward architecture which is an interleaved composition of affine functions
𝒈(𝒙) = 𝑾𝒙 + 𝒃, such as normalization, linear, convolutional, or average pooling layers, with
non-linear activation layers 𝒇 (𝒙) such as ReLU, Tanh, Sigmoid, or MaxPool:

2.1 Neural Network Verification

𝒉(𝒙) = 𝒈𝐿 ◦ 𝒇𝐿 ◦ 𝒈𝐿−1 ◦ ... ◦ 𝒇1 ◦ 𝒈0(𝒙).

Prima is an optimization-based verification approach and supports any safety specification (pre- and
post-condition) which can be expressed as a convex polyhedron. Examples of such specifications
include but are not limited to individual fairness [Ruoss et al. 2020b], global safety properties [Katz
et al. 2017], acoustic [Ryou et al. 2020], geometric [Balunovic et al. 2019], spatial [Ruoss et al. 2020a],
and ℓ𝑝 -norm bounded perturbations [Gehr et al. 2018].

At its core, Prima is based on accumulating linear constraints encoding the whole network for a
given (convex) pre-condition, defining a linear optimization objective representing the property to
be verified, and finally using an LP solver to derive a bound on this objective. If this bound satisfies
a predetermined threshold (that depends on the property), the property is verified.

While all affine layers (e.g., linear, convolutional, and normalization layers) can be encoded
exactly using linear constraints, non-linearities have to be over-approximated via constraints in
their input-output space. That is, for an activation layer 𝒇 : R𝑛 → R𝑑 and a given set of inputs

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:5

Pin ⊆ R𝑛, we need to derive sound output constraints, that represent a set Pin-out ⊆ R𝑑+𝑛 which
includes all possible input-output pairs that can be obtained by applying 𝒇 to the inputs in Pin.

We show an over-approximation for a single ReLU in Figure 2. In the concrete, the ReLU maps
input 𝑥 to 𝑦 = max(0, 𝑥). If the bounds 0 > 𝑙𝑥 ≤ 𝑥 ≤ 𝑢𝑥 > 0 are known, the best convex
approximation is given by the blue triangle. In this work we present novel methods to compute
tighter shapes by considering multiple neurons jointly in a higher dimensional space.

2.2 Overview of Convex Polyhedra

We now introduce the necessary background on polyhedra. A polyhedron can be represented as the
convex hull of its extremal points, called the vertex- or V-representation, or as the subspace satisfy-
ing a set of linear constraints, called the halfspace constraint or H -representation. Simultaneously
maintaining both representations of the same polyhedron is called double description.

Vertex representation. A polyhedron P ⊆ R𝑑 is the closed convex hull of a set of generators called

vertices R = {𝑥𝑖 ∈ R𝑑 }:

P = P (R) =

(cid:26)∑︁

𝑖

𝜆𝑖𝒙𝑖 | 𝒙𝑖 ∈ R, ∑︁

𝑖

𝜆𝑖 = 1, 𝜆𝑖 ∈ R+
0

(cid:27)

,

where R+
linear span of a set of generators called rays R = {𝑥𝑖 ∈ R𝑑 } and always includes the origin:

0 are the positive real numbers including 0. A polyhedral cone P ⊆ R𝑑 is the positive

P = P (R) =

𝜆𝑖𝒙𝑖 | 𝒙𝑖 ∈ R, 𝜆𝑖 ∈ R+
0

(cid:27)

.

(cid:26)∑︁

𝑖

Halfspace representation. Alternatively, a polyhedron can be described as the set P ⊆ R𝑑 satisfying
a system of linear inequalities (or constraints) defined by 𝑨 ∈ R𝑚×𝑑 and 𝒃 ∈ R𝑚:

P = P (𝑨, 𝒃) ≡ {𝒙 ∈ R𝑑 | 𝑨𝒙 ≥ 𝒃}.
Geometrically, P is the intersection of 𝑚 closed affine halfspaces H𝑖 = {𝑥 ∈ R𝑑 | 𝒂𝑖𝒙 ≥ 𝑏𝑖 } with
𝒂𝑖 ∈ R𝑑 and 𝑏𝑖 ∈ R. For a polyhedral cone we have 𝒃 = 0. For convenience, a polyhedron P (𝑨, 𝒃)
can be equivalently described in so-called homogenized coordinates 𝒙 ′ = [1, 𝒙], where it can be
expressed as P (𝑨′) = {𝒙 ′ ∈ R𝑑+1 | 𝑨′𝒙 ′ ≥ 0} with the new constraint matrix 𝑨′ = [−𝒃, 𝑨].

A 𝑘-face F of a 𝑑-dimensional polyhedron is a 𝑘-dimensional subset F ⊆ P satisfying 𝑑 − 𝑘
linearly independent constraints1 with equality. We call a 0-face a vertex and a (𝑑 − 1)-face a facet
[Edelsbrunner 2012]. The rank of a ray or vertex in a 𝑑-dimensional polyhedron is the number of
linearly independent constraints it satisfies with equality. We call a ray of rank 𝑑 − 1 and a vertex of
rank 𝑑 extremal. A ray of rank 𝑑 − 𝑛 can be represented as the positive combination of 𝑛 extremal
rays and a vertex of rank 𝑑 − 𝑛 as the convex combination of 𝑛 + 1 extremal points.

Double description. Polyhedra static analysis [Fukuda and Prodon 1995; Motzkin et al. 1953; Singh
et al. 2017] usually maintains both representations (H and V) in a pair (𝑨′, R), called double
description. This is useful as computing the convex hull in the V-representation is trivial (union of
generator sets), but computing intersections is NP-hard. Conversely, computing intersections in
the H -representation is trivial (union of constraints), but computing the convex hull is NP-hard.
The transformation from the V- to the H -representation is called the convex hull problem and the
reverse the vertex enumeration problem. Both are NP-hard in general.

Inclusion. We define the inclusion of a polytope Q in a polytope P as: Q ⊆ P or equivalently,

∀𝒙 ∈ Q, 𝒙 ∈ P. In this setting, we say P over-approximates Q and Q under-approximates P.

1We call a set of constraints 𝒂𝑖 𝒙 ≥ 𝑏𝑖 linearly independent, if the 𝒂𝑖 are linearly independent.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:6

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

3 OVERVIEW OF PRIMA

We now present an overview of Prima, our framework for faster and more precise verification
of neural networks with arbitrary, bounded, multivariate, non-linear activations. We provide a
complete formal description of its main components PDDM and SBLM in Sections 4 and 5, and of
Prima in Section 6. In our explanations, we follow the setup outlined in Section 1: an activation
layer consisting of 𝑛 neurons representing non-linear activations 𝑓 (𝑥) (e.g., ReLU, Tanh, Sigmoid).
Computing a convex approximation of a whole layer. Conceptually, given an 𝑛-dimensional
polytope S constraining the input to the activation layer, Prima computes a set of multi-neuron
constraints, forming a convex over-approximation of this layer, as follows:

(1) Group decomposition: Decompose the set of 𝑛 activations in the layer into overlapping groups

(subsets) of size 𝑘.

(2) Octahedral projection: For each such group 𝑖, compute an octahedral over-approximation P𝑖

of the projection of S to the input-space of group 𝑖.

(3) Split-Bound-Lift Method (SBLM): Then, for each polytope P𝑖 , compute a joint convex over-
approximation K𝑖 of the group output in the H -representation using our novel SBLM method.
This method decomposes the problem into lower dimensions and leverages our novel Partial
Double Description Method (PDDM) with polynomial complexity to compute fast and scalable
convex hull approximations. Both SBLM and PDDM are also key to making Prima applicable
to non-piecewise-linear activations.

(4) Combine constraints: Finally, take the intersection of all output constraints K𝑖 (a union of all

constraints) to obtain an over-approximation of the entire layer output.

Verification is performed by solving an LP problem which combines the generated multi-neuron
constraints with an LP encoding of the whole network (evaluated in Section 7). We now explain
the basic workings of each step and illustrate the key concepts on a running example.

Group decomposition. Computing convex hulls for large sets of activations (e.g., a whole layer) is
infeasible. Thus, we consider groups of size 𝑘, typically 𝑘 = 3 or 4. The key idea here is to capture
dependencies between activation inputs and outputs ignored by neuron-wise approximations
and thus achieve tighter approximations. The tightness increases with the number of groups and,
importantly, the degree of overlap between them. Considering all possible (cid:0)𝑛
(cid:1) groups for every
𝑘
layer is too expensive; thus we define the parameters partition size 𝑛𝑠 and group overlap 𝑠 for
tuning the cost and precision of our approximations. We first partition the activations of a layer
into sets of size 𝑛𝑠 (sorting by volume of the single neuron abstraction) and then for every set2
choose a subset of all (cid:0)𝑛𝑠
𝑘

(cid:1) groups that pairwise overlap by at most 𝑠, 0 ≤ 𝑠 < 𝑘.

Octahedral projection. Projecting the layer-wise input poly-
tope S onto the input dimensions of every group is generally
intractable due to the high dimensionality and large number
of constraints. Therefore, we follow the idea of [Singh et al.
2019a] and over-approximate the projection. Empirically we
find that multidimensional octahedra [Clarisó and Cortadella
2007], yielding 3𝑘 − 1 input constraints per group of 𝑘 neurons,
provide a good trade-off between accuracy and complexity.
Such a projection is illustrated in Figure 3 for a layer of 𝑛 = 3
neurons and 𝑘 = 2.

Exact Input
Polytope S
𝑥2

(Approximate)
Projection P𝑖
𝑥2

𝑥1

𝑥1

𝑥3

Fig. 3. Exact projection of S ∈ R3 (left)
to 𝑘 = 2 variables (green) and its octa-
hedral over-approximation P𝑖 (blue).

2For piecewise-linear activations, typically 𝑛𝑠 is chosen large enough such that there is only one set.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:7

Input Polytope P𝑖
𝑥2

𝑥1

𝑥1 + 𝑥2 ≥ −2
−𝑥1 + 𝑥2 ≥ −2
𝑥1 − 𝑥2 ≥ −2
.
.
.

Splitting

𝑥2

P

𝑥1

P1

P2

split

extend

P1,1

P1,2

bound
P′
1,1

convex
hull

K′
1,1

𝑦2

P′
1,2

K′
1,2

P2,1

P′
2,1

K′
2,1

extend,
bound

Lifting

𝑥2

𝑥1
K1

K′
1

convex
hull

K

𝑥2

K2

𝑥1

𝑦2

K′
2

Omitting superscript ·𝑖 here.

P2,2

P′
2,2

K′
2,2

Output Constraints K𝑖

𝑥2

𝑥1

𝑦2

𝑥1 + 𝑥2 − 2𝑦1 − 2𝑦2 ≥ −2
0.375𝑥2 − 𝑦2 ≥ −0.75
−𝑥1 + 𝑦1 ≥ 0
.
.
.

Fig. 4. Illustration of the Split-Bound-Lift Method for a group of 𝑘 = 2 neurons and a ReLU activation.

3.1 Split-Bound-Lift Method
The next and most demanding step takes a 𝑘-dimensional input polytope for a given 𝑘-activation
group, and computes a 2𝑘-dimensional convex over-approximation of the output of the correspond-
ing 𝑘 activations. We introduce a new technique, called Split-Bound-Lift Method, and illustrate
its workings in Figure 4 on an example. We assume ReLU activations, group size 𝑘 = 2, and an
octahedral input polytope P𝑖 (left panel in Figure 4) described by

P𝑖 = {𝑥1 + 𝑥2 ≥ −2, −𝑥1 + 𝑥2 ≥ −2, 𝑥1 − 𝑥2 ≥ −2, −𝑥1 − 𝑥2 ≥ −2, −𝑥2 ≥ −1.2}.

Our method has three main components explained next.

Split the input polytope. We first split P𝑖 into regions, which we call quadrants, for which tight
or even exact, linear bounds of the activation functions are available. Choosing the right splits is
essential for ensuring tight approximations. For piecewise-linear activation functions (like ReLU),
splitting into their linear regions even yields exact bounds in every quadrant, leading to the
tightest approximations. For our example with ReLU activations, this corresponds to splitting
along hyperplanes where the input variables 𝑥1 and 𝑥2 are 0. We (randomly) choose the ordering
{𝑦1, 𝑦2} of output variables and split P𝑖 (in the following we omit the superscript 𝑖) along the
corresponding hyperplanes. That is, we first intersect P with the halfspaces {𝒙 ∈ R2 | 𝑥1 ≥ 0}
and {𝒙 ∈ R2 | 𝑥1 ≤ 0}, obtaining P1 and P2, and then P1 and P2 with {𝒙 ∈ R2 | 𝑥2 ≥ 0} and
{𝒙 ∈ R2 | 𝑥2 ≤ 0}. These intersections generate a tree of polytopes visualized in the first three
columns in the central panel of Figure 4 with the quadrants as leafs (third column). For brevity, we
only follow the bottom half. There, the two quadrants P2,1 and P2,2 are described by

P2,1 = {𝑥1 − 𝑥2 ≥ −2, −𝑥1 ≥ 0, − 𝑥2 ≥ −1.2, 𝑥2 ≥ 0},
P2,2 = {𝑥1 + 𝑥2 ≥ −2, −𝑥1 ≥ 0, −𝑥2 ≥ 0}.

In the second part of the algorithm, we lift these quadrants step-by-step from the space of only
their inputs to the space of both their inputs and outputs. We will now describe one step of lifting
consisting of extending, bounding and computing a convex hull.

Extend and bound the quadrants. We extend3 the quadrants one output variable at a time, which,
as we will see later, enables significant gains in speed while reducing the approximation error. In
our example, we first trivially extend all quadrants from the (𝑥1, 𝑥2)-space to the (𝑦2, 𝑥1, 𝑥2)-space
(fourth column in Figure 4). Next, we bound the quadrants in the added dimension using the linear
bounds (parametrically defined, see Section 5) corresponding to applying (an approximation of) the
activation in the quadrant. Here, 𝑦2 ≤ 𝑥2 and 𝑦2 ≥ 𝑥2 for the quadrant P2,1 (since 𝑥2 ≥ 0) and 𝑦2 ≤ 0
and 𝑦2 ≥ 0 for the quadrant P2,2 (since 𝑥2 ≤ 0). Note that in this case the bounds we apply on every

3Extending a 𝑑-dimensional polytope by a variable defines it in the 𝑑 + 1-dimensional space, where it is (initially) unbounded
in the dimension of the added variable.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:8

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

quadrant are exact, yielding the two polytopes (fifth column) with 0 volume in their 3𝑑-space (in
general, the bounds need not be exact):

K ′
K ′

2,1 = {𝑥1 − 𝑥2 ≥ −2, −𝑥1 ≥ 0, −𝑥2 ≥ −1.2, 𝑥2 ≥ 0, 𝑥2 − 𝑦2 ≥ 0, −𝑥2 + 𝑦2 ≥ 0},
2,2 = {𝑥1 + 𝑥2 ≥ −2, −𝑥1 ≥ 0, −𝑥2 ≥ 0, −𝑦2 ≥ 0, 𝑦2 ≥ 0}.

𝑥1

𝑥1

𝑦2

𝑦2

2-neuron
𝑥2

single-neuron
𝑥2

2,1 and K ′

Approximate convex hull. Next, we compute the con-
vex hull of K ′
2,2. Instead of using an exact
method, we utilize our PDDM to compute precise over-
approximations, leveraging the concept of duality, ideas
from computational geometry and our novel PDD poly-
hedron representation (explained below and in more
detail in Section 4). Note that because the considered
quadrants are only extended one variable at a time,
the computation takes place in 3𝑑 despite the group-
output being in the 4𝑑 (𝑦1, 𝑦2, 𝑥1, 𝑥2)-space. This yields
two main benefits: (i) precision – directly computing 2𝑘-dimensional convex hulls with PDDM
will lose more precision than our decomposed method, because PDDM is exact for polytopes of
dimension up to three and loses precision only slowly for higher dimensions, and (ii) speed – a
lower-dimensional polytope with fewer constraints and generally also fewer vertices significantly
reduces the time required for the individual convex hull computations.

Fig. 5. Comparison of 2-neuron and 1-neuron
constraints projected into 𝑦2-𝑥1-𝑥2-space for
a ReLU activation, given input polytope P𝑖 .

Importantly, our approximate method scales quartically as O{𝑛4

𝑎)} in the number
of input constraints 𝑛𝑎 and linear in the number of vertices 𝑛𝑣 (see Theorem 4.5) while optimal
exact methods are in O (𝑛𝑣 log(𝑛𝑣) + 𝑛 ⌊𝑑/2⌋
) [Chazelle 1993], i.e., exponential in the number of
dimensions and superlinear in the number of input vertices.

𝑎 · 𝑛𝑣 + 𝑛2

𝑎 log(𝑛2

𝑣

Note that for non-piecewise-linear functions (e.g., Tanh or Sigmoid), the number of vertices
doubles when extending by a dimension. This makes exact methods intractable and approximate
methods not using the decompositional SBLM approach (that is, extending by all dimensions at the
same time) slow (see our evaluation in Section 7).

We now obtain the convex hull (sixth column) of the two polytopes K ′

2,1 and K ′

2,2 which is exact

in our 3𝑑 case:

K2 = {𝑥1 + 𝑥2 − 2𝑦2 ≥ −2, −𝑥1 ≥ 0, 0.375𝑥2 − 𝑦2 ≥ −0.75, −𝑥2 + 𝑦2 ≥ 0, 𝑦2 ≥ 0}.

We compute K1 analogously, thus completing the first step of lifting. The next and in this case
final step of lifting starts with extending K1 and K2 by 𝑦1 into the (𝑦1, 𝑦2, 𝑥1, 𝑥2)-space, where we
apply bounds on 𝑦1 yielding (in 4𝑑 and thus not illustrated as figure)

K ′

1 = { − 𝑥1 + 𝑥2 − 2𝑦2 ≥ −2, 𝑥1 ≥ 0, 0.375𝑥2 − 𝑦2 ≥ −0.75,
− 𝑥2 + 𝑦2 ≥ 0, 𝑦2 ≥ 0, 𝑥1 − 𝑦1 ≥ 0, −𝑥1 + 𝑦1 ≥ 0},
2 = {𝑥1 + 𝑥2 − 2𝑦2 ≥ −2, −𝑥1 ≥ 0, 0.375𝑥2 − 𝑦2 ≥ −0.75,

K ′

− 𝑥2 + 𝑦2 ≥ 0, 𝑦2 ≥ 0, −𝑦1 ≥ 0, 𝑦1 ≥ 0}.

Completing the second and final step of lifting by computing their convex hull yields the final tight
2-neuron constraints:

K = {𝑥1 + 𝑥2 − 2𝑦1 − 2𝑦2 ≥ −2, 0.375𝑥2 − 𝑦2 ≥ −0.75,

− 𝑥1 + 𝑦1 ≥ 0, −𝑥2 + 𝑦2 ≥ 0, 𝑦1 ≥ 0, 𝑦2 ≥ 0}.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:9

(a) Constraints - 𝑨

(b) Generators - R

(c) Unsound V-representation (d) Sound V-representation

(e) A-irredundant a)

(f) A-irredundant b)

Fig. 6. Illustration of the Partial Double Description. Input constraints 𝑨 (a), exact vertex enumeration
R𝐷𝐷 (b), unsound partial vertex enumeration violating the PDD definition (c), partial or approximate vertex
enumeration R𝑃𝐷𝐷 (d), and A-irredundant versions of the partial vertex enumeration (e).

Naturally, the region K is tighter than the tightest single-neuron approximations (triangle relaxation,
discussed earlier). We illustrate this point by comparing their projections into the (𝑦2, 𝑥1, 𝑥2)-space
in Figure 5.

3.2 Partial Double Description Method (PDDM)
We now introduce the new PDDM for computing fast, precise, and sound over-approximations of
the convex hull of two polyhedra. This is in contrast to existing approximation methods, which
either optimize for closer approximations [Bentley et al. 1982; Khosravani et al. 2013; Sartipizadeh
and Vincent 2016; Zhong et al. 2014] but sacrifice the soundness required for verification, or have
exponential complexity [Xu et al. 1998], making them too expensive for our application.

Double description method. The well-known Double Description Method (DDM) [Fukuda and
Prodon 1995; Motzkin et al. 1953] for computing the convex hull of two polyhedra in Double
Description works as follows: (i) translate both polyhedra to their dual representation (explained
in Section 4), (ii) intersect them in dual space by adding the constraints of one to the other, one-
at-a-time, computing full Double Descriptions at every intermediate step, and (iii) translate the
result back to primal space. Every step of adding an additional constraint generates quadratically
many new vertices, leading to an overall increase exponential in the number of constraints (in dual
space).

Partial double description. We introduce the Partial Double Description (PDD), which guarantees
soundness and also allows an approximate much cheaper intersection in dual space. We com-
bine an exact H -representation, as their intersection is trivial, with an under-approximating4
V-representation, as their exact intersection carries exponential cost. We illustrate this in Fig-
ure 6, where we show the constraints 𝑨 describing a polytope in (a), the corresponding exact
V-representation in (b), an unsound approximate V-representation in (c), and three sound ones in
(d), (e), and (f). Note that this definition of the PDD allows many different V-representations for a
given H -representation (see (d), (e), and (f) in Figure 6) some of which are quite imprecise (see (e)
and (f)).

Partial double description method. Now, we define the PDDM to compute approximate convex
hulls in PDD leveraging two key ideas: (i) instead of intersecting in dual space by adding the
constraints of one polytope to the other one-at-a-time (as per DDM), we add them all in a single
step. Crucially, this leads to an overall number of vertices at most quadratic (instead of exponential)
in the number of original vertices (in dual space), and (ii) this single-step approach is asymmetric and
we can greatly increase the intersection accuracy, by performing it in both directions and combining
the resulting vertices. Overall, our approach yields a polynomial complexity (see Theorem 4.5)
algorithm for sound convex hull approximations (see Theorem 4.1), guarantees exactness for low

4An under-approximation in dual space corresponds to an over-approximation in primal space, due to inclusion reversion.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:10

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

primal

dual

adding constraints

ray shooting

combine vertices

A-irredundancy

primal

Partial Double Description Method

Fig. 7. Partial Double Description Method for a 2-dimensional example. The input polytopes (1st column)
are translated to their dual representation (2nd column), then all their constraints are added to the other
dual polytope (3rd column). The points are separated based on whether they are included in the intersection
of the H -representations. Now ray-shooting is used to discover vertices on the rays between points in the
intersection (blue points) to those outside (red points) by intersecting the rays with the constraints added
in the previous step (4th column). The vertices of both V-representations are then combined (5th column)
before A-irredundancy is enforced (6th column) and the result is translated back to primal space (7th column).

dimensions (see Theorem 4.4), and empirically is two orders of magnitude faster for the challenging
cases in our experiments (see Figure 16c), while losing precision only slowly as dimensionality
increases (see Figure 16b). We illustrate the Partial Double Description Method in Figure 7 and
provide more technical details in Section 4.

3.3 Layerwise Abstraction

So far we have seen how to compute the multi-neuron convex approximation for a single group
of 𝑘 activations. To compute the final abstraction of the whole activation layer, we combine the
constraints forming the H -representations of the computed output polyhedra of each group, thereby
obtaining the H -representation of the polytope describing the layerwise over-approximation.

4 THE PARTIAL DOUBLE DESCRIPTION METHOD

In this section, we explain our PDDM
for computing convex hull approxi-
mations in greater detail. First, we in-
troduce the needed notion of duality
and our novel Partial Double Descrip-
tion (PDD) representation for polyhe-
dra. Then, we explain the PDDM step
by step as illustrated in Figure 7.

Primal

Dual

𝑥1 − 𝑥2 ≥ −1.5

5𝑥1 − 𝑥2 ≥ −5.5
5𝑥1 + 𝑥2 ≥ −5.5

−𝑥1 + 𝑥2 ≥ −1.5

𝑥2

−1.5𝑥1 − 𝑥2 ≥ −0.75

𝑥1

−1.5𝑥1 + 𝑥2 ≥ −0.75

1.5𝑥0 + 𝑥1 − 𝑥2 ≥ 0

0.75𝑥0 − 1.5𝑥1 − 𝑥2 ≥ 0

e
p
o
t
y
l
o
P

e
n
o
c

e
p
o
t
y
l
o
P

e
n
o
c

5.5𝑥0 + 5𝑥1 − 𝑥2 ≥ 0
5.5𝑥0 + 5𝑥1 + 𝑥2 ≥ 0

l
a
r
d
e
h
y
l
o
P

l
a
r
d
e
h
y
l
o
P

𝑥0 = 1

1.5𝑥0 − 𝑥1 + 𝑥2 ≥ 0

0.75𝑥0 − 1.5𝑥1 + 𝑥2 ≥ 0

The PDDM computes the convex
hull of two 𝑑-dimensional polytopes
P1 = P (𝑨1, 𝒃1) and P2 = P (𝑨2, 𝒃2),
but uses the equivalent homogenized
representation (see Section 2.2) of
(𝑑 + 1)-dimensional cones P ′
1 =
P (𝑨′
2). Vertices in the original polytope now correspond to rays in the cone.
In the following explanations we will use either term, depending on convenience. The original
polytope can be recovered from the cone, by intersecting it with the hyperplane 𝑥 ′
0 = 1 in primal,
or with 𝑥 ′
0 = −1 in dual space (explained next) as visualized in Figure 8.

Fig. 8. Top: polytope in primal (left) and dual (right) space. Bottom:
equivalent polyhedral cones in homogenized coordinates. In red:
the plane the cone can be intersected with to recover the polytope.

2 = P (𝑨′

1) and P ′

Duality. The dual P of a polytope P with a minimal set (containing no redundancy) of extremal
vertices R enclosing the origin but not containing it in its boundary (to ensure a bounded dual) is
defined as

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:11

P = {𝒚 ∈ R𝑑 | 𝒙⊤𝒚 ≤ 1 ∀𝒙 ∈ P} =

{𝒚 ∈ R𝑑 | 𝒙⊤𝒚 ≤ 1},

(cid:217)

𝑥 ∈R

and for polyhedral cones P ′ as [Genov 2015]

P ′ = {𝒚′ ∈ R𝑑+1 | 𝒙 ′⊤𝒚′ ≤ 0 ∀𝒙 ′ ∈ P ′}.

(1)

(2)

Figure 8 shows an example of the dual of a polytope. Important for the remaining section are
four properties of the transform between primal and dual. 1) The dual of a polyhedron is also a
polyhedron. 2) It is inclusion reversing: P ⊂ Q if and only if Q ⊂ P, 3) the V-representation of
the dual corresponds to the H -representation of the primal and vice versa: P = P (𝑨′, R ′) implies
P = P (R ′⊤, 𝑨′⊤), where (·)⊤ denotes transpose (note that this implies that the vertices of the
primal correspond to the supporting hyperplanes of the dual and vice-versa), and 4) the dual of the
dual of a polyhedron is the original primal polyhedron P = P.

Partial double description. We leverage these duality properties in two ways: We translate the
convex hull problem in primal space to an intersection problem in dual space (only involving a
transpose given a DD or PDD) where we compute a V-representation under-approximating the
intersection in dual space to obtain an H -representation over-approximating the convex hull in
primal space (using inclusion reversion). To compute these intersections efficiently, we introduce
the Partial Double Description (PDD) as a relaxation of the Double Description (DD) (Section 2.2)
as discussed in the overview.

Formally, the PDD of a (𝑑 + 1)-dimensional polyhedral cone is the pair of constraints and
rays (𝑨′, R ′) with 𝑨′ ∈ R𝑚×(𝑑+1) and R ′ ∈ R𝑛×(𝑑+1) where the V-representation is an under-
approximation of the H -representation or more formally, where for any row 𝒓 ∈ R ′ and constraint
𝒂 ∈ 𝑨′, 𝒂𝒓 ≥ 0 holds.

We call constraints 𝒂 𝑗 ∈ 𝑨′ active for a given ray 𝒓𝑖 ∈ R ′, if they are fulfilled with equality, that
is 𝒂 𝑗 𝒓𝑖 = 0. We store this relationship as part of the PDD in what we call the incidence matrix
I ∈ {0, 1}𝑛×𝑚: I𝑖,𝑗 = 1 if 𝒂 𝑗 𝒓𝑖 = 0 and I𝑖,𝑗 = 0 otherwise. Further, we define the partial ordering on
I: I𝑖 ⊆ I𝑗 iff I𝑖,𝑘 ≤ I𝑗,𝑘 , ∀ 1 ≤ 𝑘 ≤ 𝑚. Intuitively this corresponds to a row in the incidence matrix
being only lesser than another if the set of active constraints of the associated ray is a strict subset
of that of the other. Next, we describe PDDM as illustrated in Figure 7.

4.1 Conversion to Dual
Given the two polyhedral cones P1 and P2 in PDD representation (𝑨′
2) (1st column
1
in Figure 7), the first step of the PDDM is to convert them to their dual space representations
(R ′⊤
1

2 ) [Fukuda 2020] (2nd column).

1) and (𝑨′

, 𝑨′⊤

, 𝑨′⊤

, R ′

, R ′

2

2

1 ) and (R ′⊤
4.2 Intersection

The next step in the PDDM is the intersection in dual space (columns 3 to 5 in Figure 7). Recall that
the standard approach (DDM) for the intersection of polyhedra in DD is to sequentially add the
constraints of one polytope to the other, computing exact V-representations at every step. This
however can increase the number of vertices quadratically in every step resulting in an exponential
size of the intermediate representation. Instead, we add all constraints jointly in one step, leveraging
our PDD. In the following description of the intersection, we adopt the polytope (not cone) view
and consider a general polytope (𝑨, R).

Batch intersection. To intersect a polytope (𝑨, R) in PDD with a batch of constraints represented
by the matrix (cid:101)𝑨 and inducing the polyhedron P ( (cid:101)𝑨), we separate the vertices in R into three sets
depending on whether they satisfy all to-be-added constraints with inequality (R+), some only

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:12

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

(a) Adding Multiple Constraints

(b) Exact Result

(c) Discovered Vertices

(d) A-Irredundant

Fig. 9. Adding a batch of three constraints (blue thick lines) to a polytope in PDD. Vertices are separated into
R ′
+ (black), R ′
∗ (blue), avoiding the superfluous
0
green points, but missing an extremal vertex (yellow) (a). Exact intersection (b), result of joint constraint
processing (c), and under-approximation after enforcing A-irredundancy (d).

− (red). Ray-shooting discovers new vertices R ′

(none), and R ′

(a) Input PDDs

(b) First Intersection

(c) Second Intersection

(d) Combining Intersections

(e) Enforcing A-Irredundancy

Fig. 10. Boosting intersection precision by combining both directions of batch intersection. Input polytopes
in PDD with exact H -representation (black) and approximate V-representation (P1 green and P2 red) (a),
batch intersection of P1 with the H -representation of P2 (b), batch intersection in the opposite direction (c),
combining both intersections (d), and applying A-irredundancy (e).

with equality (R0), or violate at least one (R−). This corresponds to these points lying inside, on
the boundary of, or outside of the polyhedron P ( (cid:101)𝑨). An example is shown in Figure 9(a): the three
added constraints are shown in blue and the vertices in R ′

− (red).
−−−→
Now we employ a technique called ray-shooting [Maréchal and Périn 2017] and shoot a ray
𝒓+𝒓−
from a vertex 𝒓+ ∈ R+ inside the intersection P (𝑨 ∩ (cid:101)𝑨) to a vertex 𝒓− ∈ R− outside the intersection.
We record the first hyperplane H = {𝒙 ∈ R𝑑 | (cid:101)𝒂𝑖𝒙 = 0} corresponding to one of the new constraints
−−−→
(cid:101)𝒂𝑖 ∈ (cid:101)𝑨 that intersects with the ray
𝒓+𝒓− intersects H to the set
of discovered points R∗. Doing so for all combinations of (𝒓+, 𝒓−) ∈ R+ × R− yields the set of points

−−−→
𝒓+𝒓−. We add the point 𝒓∗ at which

0 (none), and R ′

+ (black), R ′

R∗ = {𝒓∗ = −−−→

𝒓+𝒓− ∩ H | (𝒓+, 𝒓−) ∈ R ′

+ × R ′

−}.

The V-representation of the resulting intersection is now the union R+ ∪ R0 ∪ R∗. In Figure 9 (a)
−−−→
𝒓+𝒓− are dashed lines from all black to all red vertices and discover new vertices R∗ (blue).
the rays
Only using the first intersections, immediately discards the green points, however, we also do
not discover the yellow point, which is an extremal vertex of the exact intersection (b), obtaining
instead the under-approximation (c).

Boosting precision. Batch intersection is asymmetric: The PDD of one polytope is intersected with
the H -representation of another, to obtain an exact H -representation and under-approximating V-
representation of the intersection (compare Figure 10 (b) and (c)). By performing it in both directions,
i.e., intersecting (R ′⊤
2 ) and vice-versa in our example, we obtain two different
1
under-approximations of the intersection (see Figure 10 (b) and (c)). Their convex hull (obtained
by the union of vertices) is still a sound under-approximation of the exact intersection and more
precise than the individual under-approximations. This is illustrated in Figure 10, where the exact

1 ) with (R ′⊤

, 𝑨′⊤

, 𝑨′⊤

2

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:13

intersection (blue in (d)) of the two H -representations (grey in (a)) is recovered despite the union of
the input V-representations (green and red in (a)) not covering it. This is due to the synergy between
PDD and PDDM: the under-approximate V-representation of the first polytope is intersected with
the exact H -representation of the second one and vice versa. We see the same behaviour in Figure 7,
where both uni-directional intersections (4th column) are under-approximations, but their union is
exact (5th column).

Empirically we find that this is crucial to minimize the precision loss due to using approximations.
Further, the intersection results are exact for small dimensions 𝑑 ≤ 4 of cones (see Theorem 4.4).

4.3 Enforcing A-Irredundancy

Despite using batch intersection, the number of vertices can grow quickly when computing multiple
convex hulls sequentially in the Split-Bound-Lift Method. Therefore, some notion of redundancy is
needed to efficiently reduce the representation size. The standard definitions of irredundancy are:
1) the set of unique extremal rays of the cone P (𝑨′) are irredundant, and 2) a ray 𝒓𝑖 is irredundant
if removing it leads to a different cone P (R ′) ≠ P (R ′ \ 𝒓𝑖 ). For an exact DD, an irredundant
representation does not lose precision and can be computed by retaining only rays with rank 𝑑 − 1
(which can be cheaply computed using the incidence matrix I). However, a PDD (𝑨′, R ′) usually
does not include all or even any extremal rays of the cone P (𝑨′). Consequently, enforcing the
first irredundancy definition could remove all rays. Enforcing the second definition is expensive
to compute in the absence of a full set of extremal rays, as the full convex hull problem has to be
solved to assess the removal af a ray.

Therefore, we propose A-irredundancy requiring for all rays 𝒓𝑖 ∈ R ′ that there may not be
another generator 𝒓 𝑗 ∈ R ′ with a larger (by inclusion) active constraint set. Formally and using the
partial ordering defined above, we require for an A-irredundant PDD:

I𝑖 ⊈ I𝑗,

for all 𝑖, 𝑗 ∈ {1, ..., 𝑛}, 𝑖 ≠ 𝑗 .

Any ray fulfilling a subset (including the same) constraints with equality as another ray, is removed
until the above definition is satisfied to obtain an A-irredundant representation. Extremal rays will
always be retained as they have the maximum number of active constraints and there are never
two with the same active set. Intuitively, this enforces that no two rays lie in the interior of the
same face of the polyhedron.

We illustrate the effect of enforcing A-irredundancy once in Figure 10 where we use it to obtain
the polytope 10 (e) from 10 (d) and see that all extremal rays are retained and no precision is lost.
In Figure 9 we apply it to polytope 9 (c) where the PDD misses one extremal vertex to obtain 9 (d)
and see that here the resulting reduction in generator set size can come at the cost of a precision loss.
Enforcing A-irredundancy in the 6th column of Figure 7 (removing the red vertices), recovers the
minimal set of extremal rays. Note that for rays of equal incidence there are multiple possibilities
which to retain, as is illustrated in Figure 6 (e) and (f).

4.4 Conversion to Primal

Translating the A-irredundant PDD obtained as described above, back to primal space concludes
the PDDM and yields the (generally) approximate convex hull of P1 and P2 illustrated in the 7th
column of Figure 7.

4.5 Formal Guarantees

In this subsection, we first show that the PDDM is sound and exact in low dimensions, before
analysing its worst-case complexity.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:14

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

else if 𝑚𝑖𝑛(𝑨𝑞𝒓) > 0 then

if 𝑚𝑖𝑛(𝑨𝑞𝒓) < 0 then
Add 𝑟 to R−

Algorithm 1: Batch Intersection
Result: Intersected polytope (𝑨′, R ′
𝑝 )
Input: polytope (𝑨𝑝, R𝑝 ), constraint matrix 𝑨𝑞
Initialize R−, R0, R+, R∗ = ∅, ∅, ∅, ∅
for 𝑟 in R𝑝 do

Soundness guarantee. Computing a
sound over-approximation of the convex
hull of two polytopes in primal space, by
inclusion-inversion, is equivalent to com-
puting a sound under-approximation of
the intersection of their dual space rep-
resentations. Since the primal-dual con-
version employed in the PDDM is exact,
a sound under-approximation of the in-
tersection of two polytopes in PDD in
dual space implies overall soundness. En-
forcing A-irredundancy on a polytope P
to yield Q can only remove generators,
yielding Q ⊆ P. It follows directly that
Q is a sound under-approximation, if P
is. If both polytopes P ′
𝑝 generated
by the vertex sets obtained for the two
directions of batch intersection are sound
under-approximations of the true inter-
section of the exact H -representations,
it follows that their union P ′ is also a sound under-approximation. Hence, the soundness of the
PDDM follows from the soundness of the batch intersection step:

Construct new PDD (𝑨𝑝 ∪ 𝑨𝑞, R0 ∪ R+ ∪ R∗)
Make PDD A-irredundant
return PDD

Compute 𝑟∗ via ray-shooting from 𝑟+ to 𝑟−
Add 𝑟∗ to R∗

for 𝑟− in R− do

for 𝑟+ in R+ do

Add 𝑟 to R0

Add 𝑟 to R+

𝑞 and P ′

else

Theorem 4.1. The batch intersection P ′

𝑝 ) of a polytope P in PDD (𝑨𝑝, R𝑝 ) with the
𝑝 = (𝑨′, R ′
exact constraints 𝑨𝑞 of a polytope Q computed as described above and detailed in Algorithm 1, is a
sound under-approximation of the intersection of the two exact H -representations 𝑨𝑝 and 𝑨𝑞:

{𝒙 ∈ R𝑑 |𝑨′𝒙 ≥ 0} = {𝒙 ∈ R𝑑 |𝑨𝑝𝒙 ≥ 0 ∧ 𝑨𝑞𝒙 ≥ 0},
∑︁

⊆ {𝒙 ∈ R𝑑 |𝑨𝑝𝒙 ≥ 0 ∧ 𝑨𝑞𝒙 ≥ 0}.

𝜆𝑖 ≤ 1, 𝜆𝑖 ∈ R+
0

(cid:27)

(cid:26) ∑︁

𝜆𝑖 𝒓𝑖 |

𝒓𝑖 ∈R′
𝑝

𝑖

Proof. Recall that a PDD consists of an exact H -representation and an under-approximate
V-representation. The intersection of two polytopes in H -representation is simply the union of all
constraints, allowing for an exact intersection of the H -representations. Hence, it remains to show
that the resulting V-representation R ′
𝑝 is a sound under-approximation of the H -representation
𝑨′. For this, it is sufficient to show that, by construction, every vertex 𝒓 ∈ R ′
𝑝 satisfies all constraints
in 𝑨′. Recall that R ′

𝑝 is the union of three groups of vertices (see Section 4.2 or Algorithm 1):

R+ vertices of the generating set R𝑝 that satisfy all constraints in 𝑨𝑞 strictly,
R0 vertices of the generating set R𝑝 that satisfy all constraints in 𝑨𝑞, at least one with equality,
R∗ the first intersections 𝒓∗ of rays from a vertex in 𝒓+ ∈ R+ to a vertex in 𝒓− ∈ R− (vertices in R𝑝
not satisfying all constraint in 𝑨𝑞) with the hyperplanes defined by 𝑨𝑞. Since 𝒓− lies outside
Q while 𝒓+ lies inside, an intersection 𝒓∗ is guaranteed to exist and lie between the two. By
convexity of P, 𝒓∗ satisfies all constraints of 𝑨𝑝 . Further, since 𝒓∗ is the first intersection of
the ray with a constraint in 𝑨𝑞 as seen from 𝒓+, which satisfies all constraints in 𝑨𝑞, 𝒓∗ also
satisfies all constraints in 𝑨𝑞.

Consequently, all vertices in the generating set R ′
that R ′

𝑝 ⊆ Q ∩ P and hence that the generated polytope is a sound under-approximation.

𝑝 satisfy all constraints of both P and Q. It follows
□

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:15

Exactness guarantee. Further, we can show
that for relatively low dimensional polyhe-
dra in Double Description, as they are often
encountered during the first step of lifting
in the SBLM, the PDDM as described above
is not only sound but actually exact. To this
end, let us first show the following guaran-
tee for the intersection of a cone in DD with
a matrix of constraints:

Algorithm 2: PDDM Intersection
Result: Intersected polytope (𝑨𝑝 ∪ 𝑨𝑞, R ′)
Input: polytope (𝑨𝑝, R𝑝 ) and (𝑨𝑞, R𝑞)
Compute (𝑨′, R ′
Compute (𝑨′, R ′
Construct new PDD (𝑨′, R ′
Make PDD A-irredundant
return PDD

𝑝 ) = (𝑨𝑝 ∪ 𝑨𝑞, R𝑝 ) with Alg.1
𝑞) = (𝑨𝑝 ∪ 𝑨𝑞, R𝑞) with Alg.1
𝑝 ∪ R ′
𝑞)

Theorem 4.2. Given a Double Description (𝑨𝑝, R𝑝 ) of a polyhedral cone and the constraint matrix
𝑨𝑞, adding all constraints jointly as per Algorithm 1 is guaranteed to yield a double description
𝑝 ) enumerating all extremal rays 𝑟 ′ of the 𝑨𝑝 ∪ 𝑨𝑞-induced cone with one of the following
(𝑨𝑝 ∪ 𝑨𝑞, R ′
properties:

(1) 𝑟 ′ is extremal (rank 𝑑 − 1) in the 𝑨𝑝 -induced cone.
(2) 𝑟 ′ is of rank 𝑑 − 2 in the 𝑨𝑝 -induced cone.

Proof. We can formally divide the rays of the new PDD R ′ into the two non-overlapping sets:
• R+ ∪ R0: Rays in R𝑝 not violating any constraint 𝑎 ∈ 𝑨𝑞
• R∗: Rays discovered by ray-shooting

Since (𝑨𝑝, R𝑝 ) is a DD of the 𝑨𝑝 -induced cone it enumerates all extremal rays. If 𝑟 ′ is extremal
in both the 𝑨-induced and the 𝑨𝑝 ∪ 𝑨𝑞-induced cones, it is included in R𝑝 and does not violate
any constraints. Therefore, it is included in the first group above and will be part of R ′
𝑝 , which
concludes the proof of the first point. Any ray of rank 𝑑 − 2 can, by definition, be represented as
a positive combination of two extremal rays, that is rays of rank 𝑑 − 1. As we assume ray 𝑟 ′ to
be extremal in the 𝑨𝑝 ∪ 𝑨𝑞-induced cone and therefore have rank 𝑑 − 1, it necessarily intersects
at least one constraint 𝒂 ∈ 𝑨𝑞 and is extremal to the 𝑨𝑝 ∪ 𝒂-induced cone. Consequently exactly
one of the extremal rays used to construct it has to lie on either side of thy hyperplane induced by
constraint 𝒂. Therefore, they will be included in the sets R+ and R− and the intersection will be
□
discovered as part of the ray-shooting, concluding the proof of the second point.

Using this result, we can proof the following guarantee for intersections of two cones in DD using
our batch intersection and precision boosting approach, described in Section 4.2 and Algorithm 2:

Theorem 4.3. Given the double descriptions (𝑨𝑝, R𝑝 ) and (𝑨𝑞, R𝑞) of two polyhedral cones, their
intersection computed as per Algorithm 2 is guaranteed to be a partial double description (𝑨𝑝 ∪ 𝑨𝑞, R ′)
enumerating all extremal rays 𝑟 ′ of the (𝑨𝑝 ∪ 𝑨𝑞)-induced cone with one of the following properties:

(1) 𝑟 ′ is extremal in the 𝑨𝑝 -induced cone.
(2) 𝑟 ′ is extremal in the 𝑨𝑞-induced cone.
(3) 𝑟 ′ is of rank 𝑑 − 2 in the 𝑨𝑝 -induced cone.
(4) 𝑟 ′ is of rank 𝑑 − 2 in the 𝑨𝑞-induced cone.

Proof. The proof follows directly from applying Lemma 4.2 to both applications of Algorithm 1,
the insight that every extremal ray discovered by either will be included in the final generating set
R ′ and the observation that the intersection of the exact H -representations, trivially is the union
□
of their respective constraints, leading to a valid partial double description.

Using these results, we can in turn proof that the intersection of two polyhedral cones of up to

dimension 4 in DD using the approach described above is exact:

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:16

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

Theorem 4.4. Given the Double Descriptions (𝑨𝑝, R𝑝 ) and (𝑨𝑞, R𝑞) of two polyhedral cones P
and Q of dimension 𝑑 ≤ 4, the PDD of their intersection (𝑨𝑝 ∪ 𝑨𝑞, R ′) computed as described above
and detailed in Algorithm 2 is an exact DD with an irredundant generating set R ′.

Proof. For briefness sake, we will only show the proof for 𝑑 = 4 here. Let R∗ be the set of
extremal rays of the (𝑨𝑝 ∪ 𝑨𝑞)-induced polyhedral cone. Consequently 𝒓 ∗ ∈ R∗ has the rank
𝑑 − 1 = 3 in this cone and therefore it fulfills 3 linearly independent constraints in 𝑨𝑝 ∪ 𝑨𝑞 with
equality. This leads to the following four exhaustive options:
(1) all 3 constraints are part of 𝑨𝑝 , 𝒓 ∗ is extremal in P𝑝 ,
(2) all 3 constraints are part of 𝑨𝑞, 𝒓 ∗ is extremal in P𝑞,
(3) 2 constraints are part of 𝑨𝑝 and 1 of 𝑨𝑞, 𝒓 ∗ is of rank 𝑑 − 2 = 2 in P𝑝 ,
(4) 2 constraints are part of 𝑨𝑞 and 1 of 𝑨𝑝 , 𝒓 ∗ is of rank 𝑑 − 2 = 2 in P𝑞.

All of those are enumerated by Algorithm 4.3. Hence, R ′ will include all extremal rays of the
□
(𝑨𝑝 ∪ 𝑨𝑞)-induced cone. In this case A-irredundancy is equivalent to irredundancy.

Complexity analysis. Finally, we can show that computing an over-approximation of the convex
hull of two 𝑑-dimensional, bounded polytopes in PDD using the PDDM has polynomial complexity:

Theorem 4.5. Given the PDD of two 𝑑-dimensional, bounded polytopes with a V-representation
of at most 𝑛𝑣 vertices and an H -representation of at most 𝑛𝑎 constraints, computing a sound over-
approximation of their convex hull using the PDDM as described above and detailed in Algorithm 2
has a worst-case time complexity of O (𝑛𝑣 · 𝑛4

𝑎 + 𝑛2

𝑎 log(𝑛2

𝑎)).

Proof. The PDDM can be broken down into its six components illustrated in Figure 7:
(1) Conversion from primal to dual representation (Section 4.1)
(2) Adding the constraints of one polytope to the other, or more concretely separation of vertices

into the three sets R+, R0, and R− (Section 4.2 or first half of Algorithm 1)

(3) Discovery of new vertices via ray-shooting (Section 4.2 or second half of Algorithm 1)
(4) Combining the vertices of the two intersection directions (Section 4.2 or Algorithm 2)
(5) Enforcing of A-irredundancy (Section 4.3 or Algorithm 2)
(6) Conversion from dual to primal representation (Section 4.1)

Primal-dual conversions and combining of vertices can be computed in constant time, as this only
involves computing the transpose and concatenation which can be done implicitly by changing
the indexing of the corresponding matrices. Therefore, we will focus on the remaining three steps,
which are all conducted in dual space.

In the following we assume the setting, of two 𝑑-dimensional, bounded polytopes which in
dual-space are defined by P = (𝑨𝑝, R𝑝 ) and Q = (𝑨𝑞, R𝑞). For convenience’s sake, we assume the
number of vertices to be 𝑛𝑣 = max(|R𝑝 |, |R𝑞 |) and number of constraints 𝑛𝑎 = max(|𝑨𝑝 |, |𝑨𝑞 |).
Note that their roles are reversed compared to a primal space representation.

Adding constraints and separating vertices. Recall that in dual space we compute the intersection
of the two polytopes P and Q. The first step of intersecting P with Q is to split all points in R𝑝 into
the three groups R+, R0, and R− defined in Section 4.2 depending on whether the lie inside, on the
border of or outside the polytope defined by 𝑨𝑞 as per the first half of Algorithm 1. This requires
(at worst) evaluating 𝒂𝑖 𝒓 𝑗 − 𝑏𝑖 {>, =, <}0 for all 𝒓 𝑗 ∈ R𝑝 and 𝒂𝑖, 𝑏𝑖 ∈ 𝑨𝑞. Where the addition and
comparison are dominated by the 𝑑-dimensional dot-product between 𝒂𝑖 and 𝒓 𝑗 , leading to a total
complexity of this step of order O (𝑑 · 𝑛𝑎 · 𝑛𝑣). Note that incidence matrix columns corresponding
to the new constraints are added and populated without any extra computation with 0s for the
vertices in R+ and 1s for vertices in R0.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:17

Ray-shooting. Recall that to discover new generating vertices, the first intersections between the
rays shot from all generating vertices of P lying inside Q, 𝒓+ ∈ R+, to all vertices lying outside Q,
𝒓− ∈ R−, and all constraints in 𝑨𝑞 are computed. At worst there are no vertices in group R0 and
all vertices are spread equally between R+ and R−, leading to 𝑛2
𝑣/4 rays to be intersected with 𝑛𝑎
constraints where each intersection corresponds to computing a ratio of dot-products and is order
O (𝑑). Selecting the first intersection for each ray is linear in the intersection number. Consequently,
the ray-shooting process overall is O (𝑑 · 𝑛𝑎 · 𝑛2
𝑣). Note that this adds new incidence matrix rows
corresponding to the new vertices R∗, which can then be populated with the row obtained by the
elementwise 𝑎𝑛𝑑 of the two vertices generating the ray and a 1 in the column associated with the
constraint of the first intersection which is linear O (𝑛𝑣) and dominated by the previous term.

Enforcing A-irredundancy. The intermediate state prior to enforcing A-irredundancy contains
at most 𝑛 = 2(𝑛𝑣 + 𝑛2
𝑣/4) vertices, consisting of the at most 𝑛𝑣 vertices in R+ and the at most
𝑛2
𝑣/4 vertices in R∗, discovered during ray shooting, for both intersection directions. To enforce A-
irredundancy, vertices are first sorted in descending order by the number of active constraints which
is order O (𝑛 log(𝑛)). Then starting with the first vertex, row-wise inclusion of the corresponding
incidence matrix rows is checked for all following elements. Each check is O (𝑛𝑎) and (𝑛2 − 𝑛)/2
checks have to be performed in the worst case that is, if no element is removed. This leads to an
overall complexity of O (𝑛𝑎 · 𝑛4

𝑣)) for enforcing A-irredundancy.

𝑣 log(𝑛2

𝑣 + 𝑛2

PDDM complexity. Putting the three elements together and observing 𝑑 < 𝑛𝑣 for any 𝑑-dimensional,
bounded polytope, we observe that both the ray-shooting and the separation of vertices get
dominated by the last step of enforcing A-irredundancy. Swapping the roles of 𝑛𝑣 and 𝑛𝑎 to
derive an expression in terms of primal space entities, we arrive at an overall complexity of
□
O (𝑛𝑣 · 𝑛4

𝑎 + 𝑛2

𝑎 log(𝑛2

𝑎)).

group

a
their

bounding regions D and set of bounds B

Input: Variable ordering I, input polytope P, set of

5 SPLIT-BOUND-LIFT METHOD Algorithm 3: Split-Bound-Lift Method (SBLM)
In this
section, we explain the
Split-Bound-Lift Method in greater
detail. Recall that we use the SBLM
to compute k-neuron abstractions,
by approximating the convex hull
conv({(𝒙, 𝒇 (𝒙)) | 𝒙 ∈ P ⊆ [𝑙𝑥, 𝑢𝑥 ]𝑘 })
𝑘
neurons
of
for
and
functions
activation
𝒇 (𝒙) = [𝑓1(𝑥1), ..., 𝑓𝑘 (𝑥𝑘 )]⊤, assuming
that their inputs are constrained by
the polytope P.

Split region: P𝑖 = P ∩ D𝑖
Apply SBLM: K𝑖 ← SBLM(I1:𝑒𝑛𝑑, P𝑖, D, B)
Extend into space including 𝑦: K𝑖 ← K𝑖 × R
Apply bounds B𝑖 : K𝑖 ← K𝑖 ∩ B𝑖
Compute convex hull: K = PDDM({K𝑖 }𝑖 )
return K

Output: Jointly constraining polytope K
if |I| > 0 then

Get next output variable: 𝑦 ← I0
foreach D𝑖, B𝑖 in D, B do

At a high level, we first decompose
the input polytope into regions where
we can bound all activation functions
tightly. Then, we extend these regions into the output space and apply linear constraints corre-
sponding to the (relaxed) activations. Taking the convex hull of the resulting polytopes yields an
H -representation encoding the k-neuron abstraction.

return P

else

To increase the efficiency of this approach, we use a decomposition method we call splitting
and then recursively extend and bound the resulting polytopes by one output variable at a time,
which we call lifting. This minimizes the dimensionality in which we have to compute the convex
hulls. We formalize this in Algorithm 3 and explain both splitting and lifting below after stating
the prerequisites for the SBLM.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:18

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

5.1 Prerequisites
For simplicities’ sake, we assume just one type of activation function 𝑓 : D → R, with domain D, is
to be bounded. Now the SBLM requires a set of intervals D𝑖 (e.g., 𝑥 𝑗 ≤ 0, 𝑥 𝑗 ≥ 0 for ReLU), covering
the domain D (e.g., R for ReLU), and a pair of tight linear constraints B𝑖 upper and lower bounding
the function output (e.g., 𝑦 𝑗 ≤ 0 and 𝑦 𝑗 ≥ 0, and 𝑦 𝑗 ≤ 𝑥 𝑗 and 𝑦 𝑗 ≥ 𝑥 𝑗 , respectively, for ReLU) on
each of the intervals obtained by intersecting the interval [𝑙𝑥, 𝑢𝑥 ]𝑖 defined by the neuron-wise
bounds with the intervals D𝑖 . More formally, we require the intervals

𝑐𝑖, 𝑑𝑖 ∈ R and 𝑐𝑖 ≤ 𝑑𝑖,

D𝑖 = [𝑐𝑖, 𝑑𝑖 ],
(cid:216)
D𝑖,

D ⊆

𝑖

with the affinely extended real numbers R = R ∪ {−∞, ∞} and the bounds on these intervals

𝑎 { ≤, ≥ }
B𝑖 = (𝑎 ≤
𝑖 , 𝑎 ≥
𝑖 ),
𝑖
𝑖 (𝑥), ∀ 𝑥 ∈ (D𝑖 ∩ [𝑙𝑥, 𝑢𝑥 ]𝑖 ),
𝑖 (𝑥) ≤ 𝑓 (𝑥) ≤ 𝑎 ≥
𝑎 ≤

(𝑥) = 𝑎𝑥 + 𝑏, 𝑎, 𝑏 ∈ R 𝑠.𝑡 .

𝑖

to be provided to instantiate SBLM and by extension Prima. We note that the bounds 𝑐𝑖 and
𝑑𝑖 of the bounding regions can depend on the concrete input bounds 𝑙𝑥 and 𝑢𝑥 and the slope
𝑎 and intercept 𝑏 of 𝑎 { ≤, ≥ }
can in turn depend on the corresponding concrete interval bounds
[max(𝑙𝑥, 𝑐𝑖 ), min(𝑢𝑥, 𝑑𝑖 )].

Generalization. While we focus on the univariate case using only two bounding regions D1
and D2 in the following, SBLM and by extension Prima can be generalized to allow for neuron
groups combining different multivariate activation functions 𝑓 : D ⊆ R𝑑 → R. Further, more than
one upper- and lower-bound B𝑖 per bounding region can be provided and D𝑖 can be specified as
polyhedral regions instead of as intervals, as long as their union covers the domain D ⊆ (cid:208)𝑖 D𝑖 of
the individual functions 𝑓 .

5.2 Splitting the Input Polytope
To apply the bounds B𝑖 , the input polytope P has to be split into the regions for which the bounds
were specified. These regions correspond to the intersection of P with the k-Cartesian product of
the bounding regions D𝑖 , that is all combinations of neuron-wise bounding regions for the group of
k neurons. We choose an ordering of the output variables I and recursively split P by intersecting
with the bounding regions associated with these output variables.

𝑗 = {𝒙 ∈ R𝑘 | 𝑥 𝑗 ≥ 𝑐1} and D2

As every such split is equivalent on an abstract level, we will explain one case assuming the
parent polytope P1, the output variable 𝑦 𝑗 = 𝑓 (𝑥 𝑗 ), and the corresponding bounding regions
𝑗 = {𝒙 ∈ R𝑘 | 𝑥 𝑗 ≤ 𝑑2}. We compute the children nodes by
D1
intersecting P1 with D1
𝑗 and P1,2 = P1 ∩ D2
𝑗 . Starting with P at
the root and recursively applying this splitting rule for every 𝑦 𝑗 ∈ I, generates a polytope tree,
which we call the decomposition tree, with 2𝑘 leaf polytopes P{1,2}𝑘 , which we call quadrants. This
is illustrated in the blue portion of the central panel in Figure 4, where D1 and D2 are R+
0 and R−
0 ,
respectively.

𝑗 to obtain P1,1 = P1 ∩ D1

𝑗 and D2

5.3 Lifting

We now extend these quadrants P{1,2}𝑘 to the output space and bound them using the corre-
sponding constraints on the activation function B𝑖
𝑗 , before taking their convex hull. This yields a
polytope K, jointly constraining the inputs and outputs of a neuron group. The constraints of its
H -representation form the desired k-neuron abstraction. We call this process lifting and propose

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:19

a recursive approach: We lift sibling polytopes on the decomposition tree until only the desired
polytope K remains.

Again, we explain a single step of lifting, as they are equivalent. We assume the sibling polytopes
K1,1 and K1,2, corresponding to P1,1 and P1,2 in the decomposition tree, with the associated input-
and output-variables 𝑥 𝑗 and 𝑦 𝑗 , respectively, and the pairs of bounds B1
𝑗 instantiated for 𝑦 𝑗 .
A single step consist of three parts:

𝑗 and B2

• extending K1,1 and K1,2 by the output variable 𝑦 𝑗 ,
• bounding 𝑦 𝑗 on the extended polytopes, by intersecting them with the constraints B1

𝑗 and

B2

𝑗 to obtain K ′

1,1 and K ′

1,2,

• computing their (approximate) convex hull using the PDDM: K1 = conv(K ′
1,1

, K ′

1,2).

Applying this lifting rule recursively to the decomposition tree starting with K{1,2}𝑘 = P{1,2}𝑘 ,
combines all 2𝑘 quadrants into a single 2𝑘-dimensional polytope K, jointly constraining the inputs
and outputs, thereby concluding the Split-Bound-Lift Method. This is illustrated in the right portion
of the central panel in Figure 4. The decompositional approach has two benefits: Precision –
computing approximate convex hulls via the PDDM is exact for polytopes of dimension up to 3 and
starts to lose precision only slowly as dimensionality increases. Directly computing 2𝑘-dimensional
convex hulls with PDDM will therefore lose more precision than using our decomposed method.
Speed – a lower-dimensional polytope with fewer constraints and generally also fewer vertices
significantly reduces the runtime for the individual convex hull operations. In fact, computing
the convex hulls for the approximation of non-piecewise-linear functions directly in the input-
output space is intractable even for groups of only size 𝑘 = 3, as the number of vertices increases
exponentially with 𝑘 during the extension and bounding process in that case.

5.4 Instantiation for Various Functions

𝑦

We instantiate SBLM for common network
functions next.

neither convex
nor concave

concave

𝑦 = 𝑒𝑥
1+𝑒𝑥

𝑐

𝑙𝑥

𝑥

𝑢𝑥

ReLU. We can capture all univariate,
piecewise-linear functions, such as ReLU,
exactly on the intervals D𝑖 where they are
linear. Further, if the neuron-wise bounds
[𝑙𝑥, 𝑢𝑥 ] only contain one such linear region,
the neuron behaves linearly, can be encoded
exactly and is excluded from the k-neuron abstraction. Therefore, we consider 𝑦 = 𝑚𝑎𝑥 (𝑥, 0) with
𝑥 ∈ [𝑙𝑥, 𝑢𝑥 ] for 𝑙𝑥 < 0 < 𝑢𝑥 . We choose D1 = [−∞, 0] and D2 = [0, ∞], with B1 = (𝑦 ≥ 0, 𝑦 ≤ 0)
and B2 = (𝑦 ≥ 𝑥, 𝑦 ≤ 𝑥), obtaining exact bounds on both intervals.

Fig. 11. Interval-wise bounds for the Sigmoid function
on the intervals [𝑙𝑥 , 𝑐] and [𝑐, 𝑢𝑥 ].

Tanh and Sigmoid. Let 𝑓 be an S-curve function with domain [𝑙𝑥, 𝑢𝑥 ], that is 𝑓 ′′(𝑥) ≥ 0 for
𝑥 ≤ 0, 𝑓 ′′(𝑥) ≤ 0 for 𝑥 ≥ 0 and 𝑓 ′(𝑥) > 0 for 𝑥 ∈ [𝑙𝑥, 𝑢𝑥 ]. Both Sigmoid 𝜎 (𝑥) = 𝑒𝑥
𝑒𝑥 +1 and Tanh
tanh(𝑥) = 𝑒𝑥 −𝑒−𝑥
𝑒𝑥 +𝑒−𝑥 have these properties. We split the domain at 𝑐 ∈ [𝑙𝑥, 𝑢𝑥 ] into D1 = [−∞, 𝑐] and
D2 = [𝑐, ∞], choosing 𝑐 to minimize the area between upper and lower bound in the input-output
plane, using the bounds from Singh et al. [2019b]:

𝑓 (𝑥) ≤ 𝑎 ≤ = 𝑓 (𝑢𝑑 ) + (𝑥 − 𝑢𝑑 )

𝑓 (𝑥) ≥ 𝑎 ≥ = 𝑓 (𝑙𝑑 ) + (𝑥 − 𝑙𝑑 )

(cid:40) 𝑓 (𝑢𝑑 )−𝑓 (𝑙𝑑 )
𝑢𝑑 −𝑙𝑑

,
min(𝑓 ′(𝑢𝑑 ), 𝑓 ′(𝑙𝑑 )),
(cid:40) 𝑓 (𝑢𝑑 )−𝑓 (𝑙𝑑 )
𝑢𝑑 −𝑙𝑑

,
min(𝑓 ′(𝑢𝑑 ), 𝑓 ′(𝑙𝑑 )),

if 𝑢𝑑 ≤ 0,
else,

𝑙𝑑 ≥ 0,

if
else,

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:20

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

where we denote the lower bound of the intersection D𝑖 ∩ [𝑙𝑥, 𝑢𝑥 ] as 𝑙𝑑 and the upper one as
𝑢𝑑 . We show these bounds in Figure 11 for the Sigmoid function and, for illustration purposes, a
non-optimal 𝑐. In practice, we choose 𝑐 to minimize the area of the abstraction of a single neuron
in the input-output plane.

B1: 𝑦 = 𝑥1

MaxPool. Let MaxPool be the multivariate
function 𝑦 = max(𝑥1, 𝑥2, ..., 𝑥𝑑 ) on the domain
𝒙 ∈ P ⊆ [𝑙𝑥, 𝑢𝑥 ]𝑑 . Note that here the general-
ized formulation is required. We chose the poly-
hedral bounding regions D𝑖 = {𝒙 ∈ R𝑑 |𝑥𝑖 ≥
𝑥 𝑗, 1 ≤ 𝑗 ≤ 𝑑, 𝑖 ≠ 𝑗 }𝑖 , separating the domain
into the 𝑑 regions where one variable dominates
all others (illustrated for 𝑑 = 2 in Figure 12). On
each of these regions, MaxPool can be bounded
exactly with 𝑦 ≤ 𝑥𝑖 and 𝑦 ≥ 𝑥𝑖 . During the splitting process, this increased number of bounding
regions leads to a decomposition tree where every parent node has 𝑑 child nodes.

𝑙𝑥2
Fig. 12. Polyhedral bounding regions D𝑖 and corre-
sponding bounds B𝑖 for the 2𝑑 MaxPool function on
the input region [𝑙𝑥1
, 𝑢𝑥1 ] × [𝑙𝑥2

B2: 𝑦 = 𝑥2

, 𝑢𝑥2 ].

D1: 𝑥1 ≥ 𝑥2

D2: 𝑥2 ≥ 𝑥1

𝑢𝑥1

𝑥1

𝑙𝑥1

𝑥2
𝑢𝑥2

6 PRIMA VERIFICATION FRAMEWORK

Prima is based on three high-level steps: (i) accumulate a set of constraints encoding a (convex)
abstraction of the network for a given pre-condition (as discussed so far), (ii) define a linear
optimization objective representing the post-condition, and (iii) use an LP or MILP solver to derive
a bound on this optimization objective. If this bound exceeds a threshold depending on the post-
condition, certification succeeds, otherwise, if the optimal solution violates this bound, it could be a
true counterexample or a false positive due to approximation. Hence, we evaluate any such possible
counterexample with the concrete network to determine whether it is a true counterexample.

While all affine layers are encoded exactly, two considerations have to be balanced when encoding
non-linear activation layers with Prima: more precise encodings (e.g., considering more or larger
neuron groups) improve the optimal bound of the optimization problem, but the increased number
of constraints can make this problem impractical to solve. We navigate this trade-off by leveraging
abstraction refinement – using increasingly more precise but also more costly methods until we
are able to either decide a property (verify or falsify) or reach a timeout.

6.1 Abstraction Refinement Approaches

Fundamentally, we can refine our abstraction in three ways: (i) compute tighter abstractions of the
group-wise inputs, (ii) compute tighter layer-wise multi-neuron constraints for the given input
abstraction from (i), and (iii) encode part of the network using an exact MILP encoding.

Input bound refinement. Since SBLM and PDDM abstract a group of neurons for a given polyhedral
input region, the tightness of the resulting constraints depends directly on the tightness of the input
abstraction. These are computed using a fast, incomplete verifier (e.g., [Müller et al. 2021; Singh
et al. 2019b; Xu et al. 2020]) based on single-neuron abstractions and can be tightened significantly
by computing more precise neuron-wise bounds [Singh et al. 2019c] using an LP or MILP encoding.
Tighten multi-neuron constraints. The layer-wise tightness of our multi-neuron constraints
depends on (i) the tightness of the group-wise constraints, mostly determined by the quality of
the input region, and (ii) on capturing the important neuron-interdependencies with the chosen
groups. Using larger neuron groups (increasing 𝑘) and considering more groupings by allowing
more overlap (increasing 𝑠) and partitioning the neurons into fewer sets before grouping (increasing
𝑛𝑠 ), allows capturing more and more complex interactions. While the constraints themselves can
be computed quickly, the resulting LP problems become harder to solve.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:21

Network encoding. Prima encodes non-linear activations in four different ways: (i) exact encoding
via equality constraints for stable (those exhibiting linear behavior) piecewise-linear activations,
(ii) single-neuron constraints, (iii) multi-neuron constraints computed via SBLM and PDDM, and
(iv) exact (for piecewise-linear functions) MILP encodings. While stable activations are always
encoded exactly and all unstable activations are encoded using both the single- and multi-neuron
constraints, we only selectively use a MILP encoding on the (typically relatively narrow) last layers
of convolutional networks due to their large computational cost.

6.2 Abstraction Refinement Cascade

Prima leverages our multi-neuron constraints as part of an abstraction refinement cascade using
increasingly more precise and expensive approaches: We first attempt verification using single-
neuron constraints via DeepPoly [Singh et al. 2019b] or GPUPoly [Müller et al. 2021]. If this fails,
we encode all activation layers using our multi-neuron constraints and solve the resulting LP. If this
also fails, we attempt to decide the property by tightening the multi-neuron constraints Section 6.1,
encoding the final network layer(s) using MILP, and refining individual neuron bounds.

7 EXPERIMENTAL EVALUATION

Table 1. Neural network architectures used in experiments.

In this section, we evaluate the effective-
ness of Prima and show that it signifi-
cantly improves over state-of-the-art ver-
ifiers on a range of challenging bench-
marks yielding up to 14%, 30% and 34%
precision gains on ReLU-, Sigmoid-, and
Tanh-based networks, respectively. Fur-
ther, we show that Prima can scale to real-
world problems, obtaining tight bounds
in an autonomous driving steering-angle-
prediction task. Finally, we demonstrate
the effectiveness and benefits of comput-
ing relaxations with SBLM and PDDM
compared to directly using the exact con-
vex hull.

7.1 Experimental Setup

Dataset

Model

Type

Neurons

Layers Activation

MNIST

FC
FC
FC

5 × 1005
6 × 100
8 × 1005
9 × 100
5 × 2005
6 × 200
8 × 2005
ConvSmall Conv
Conv

FC
FC
FC
FC

ConvBig

CIFAR10

ConvSmall Conv
CNN-A-Mix Conv

CNN-B-Adv Conv
Conv
Residual

ConvBig

ResNet

510
600
810

900
1 010
1 200
1 610
3 604
48 064

4 852
6 244

16 634
62 464
107 496

5
6
8

9
5
6
8
3
6

3
3

3
6
10

ReLU
Tanh/Sigm
ReLU

Tanh/Sigm
ReLU
Tanh/Sigm
ReLU
Relu/Tanh/Sigm
ReLU

ReLU
ReLU

ReLU
ReLU
ReLU

Self-Driving

DAVE

Conv

107 032

8

ReLU + Tanh

The neural network certification benchmarks for fully connected networks were run on a 20 core
2.20GHz Intel Xeon Silver 4114 CPU with 100 GB of main memory and those for convolutional
networks on a 16 Core 3.6GHz Intel i9-9900K with 64GB of main memory and an NVIDIA RTX
2080Ti. We use Gurobi 9.0 for solving MILP and LP problems [Gurobi Optimization, LLC 2018].

7.2 Benchmarks

We evaluate Prima on a wide range of networks based on ReLU, Tanh, and Sigmoid activations:
• The set of fully-connected and convolutional ReLU networks5 from [Singh et al. 2019a]
trained using DiffAI [Mirman et al. 2018], PGD [Madry et al. 2018], Wong [Wong et al. 2018],
and natural training (see results on MNIST and CIFAR10 in Table 2).

5The networks referred to as 6 × · 00 and 9 × · 00 in previous work only include 5 and 8 hidden layers, respectively, and
have therefore been renamed.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:22

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

Table 2. Number of verified adversarial regions of the first 1 000 samples and runtime for Prima, OptC2V
[Tjandraatmadja et al. 2020], and kPoly [Singh et al. 2019a]. Natural (NOR), adversarial (PGD [Madry et al.
2018]), or provable (DiffAI [Mirman et al. 2018], Wong [Wong et al. 2018]) training was used.

Dataset Model

Training Accuracy

𝜖

𝑛𝑠

kPoly

OptC2V †

Prima (ours)

# Upper Bound

MNIST

5 × 100
NOR
8 × 100
NOR
5 × 200
NOR
8 × 200
NOR
ConvSmall NOR

ConvBig

DiffAI

CIFAR10

ConvSmall

ConvBig

ResNet

PGD
PGD
Wong

960
947
972
950
980
929

630
631
290

# Ver Time

# Ver Time

# Ver Time

0.026
0.026
0.015
0.015
0.120
0.300

2/255
2/255
8/255

100
100
50
50
100
100

100
100
50

441
369
574
506
347
736

399
459
245

307
171
187
464
477
40

86
346
91

429
384
601
528
436
771

398
n/a†
n/a†

137
759
403
3451
55
102

105
n/a†
n/a†

510
428
690
612
640
775

458
482
248

159
301
224
395
51
5.5

16
128
1.9

842
820
901
911
733
790

481
550
248

†The OptC2V [Tjandraatmadja et al. 2020] code has not been released; we report their runtimes and results where available.

• The published set of CIFAR10 convolutional networks from [Dathathri et al. 2020], trained
using either just PGD or a mix of standard and PGD training (see results on CIFAR10 in
Table 3).

• The set of fully-connected and convolutional Tanh and Sigmoid networks from [Singh et al.

2019a] trained using natural training (see results on MNIST in Table 5).

• The NVIDIA self-driving car network architecture DAVE [Bojarski et al. 2016] trained on a
steering angle prediction task using the Udacity self-driving car dataset [Udacity 2016] with
31 834 train and 1 974 test samples6, an input resolution of 3 × 66 × 200, and PGD [Madry
et al. 2018] training (see results in Table 6).

While we evaluate performance for the widely considered and challenging ℓ∞ perturbations7, Prima
can also be applied to other specifications including individual fairness [Ruoss et al. 2020b], global
safety properties [Katz et al. 2017], acoustic [Ryou et al. 2020], geometric [Balunovic et al. 2019],
and spatial [Ruoss et al. 2020a] based perturbations.

For classification tasks and ReLU networks, we compare Prima with a range of state-of-the-
art incomplete verifiers notably also the ReLU-specialized kPoly [Singh et al. 2019a], OptC2V
[Tjandraatmadja et al. 2020], and additionally the highly optimized and fully GPU-based 𝛽-Crown
[Wang et al. 2021] (in incomplete mode). For classification using Tanh and Sigmoid activations,
fewer verifiers are available and thus we compare with the state-of-the-art incomplete verifier
DeepPoly [Singh et al. 2019b]. Few verification methods consider the regression setting and to the
best of our knowledge, we are the first to analyze the full-size DAVE network. Neurify [Wang et al.
2018] analyses a heavily scaled-down version in a binary classification setting, but in complete
mode it does not scale to the much larger networks analysed here. In incomplete mode, it uses the
same bounds as DeepZono [Singh et al. 2018] and is less precise than GPUPoly [Müller et al. 2020]
to which we compare. 𝛽-Crown does not support regression tasks and while an extension might
be possible, it is non-trivial. It is also unclear if the approach scales to networks of this size.

6The labels of the original test set are not available (anymore), so we used videos 1, 2, 5, and 6 as train and video 4 (instead
of 3) as test dataset.
7That is, 𝑦 := 𝑐 (𝒙)𝑖 = 𝑐 (𝒙′), ∀𝒙′ ∈ B∞

| |𝒙 − 𝒙′ | |∞ ≤ 𝜖 } ⇔ min𝒙′∈B∞

𝜖 𝒉(𝒙′)𝑦 − 𝒉(𝒙′)𝑖 > 0, ∀𝑖 ≠ 𝑦

𝜖 := {𝒙 ∈ X |

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:23

(a) MNIST 5 × 100, 𝜖 = 0.026

(c) CIFAR10 ConvSmall, 𝜖 = 2/255
(b) MNIST ConvBig, 𝜖 = 0.3
Fig. 13. Comparison of the runtime/accuracy trade-off of Prima (ours), OptC2V [Tjandraatmadja et al. 2020],
FastC2V [Tjandraatmadja et al. 2020], kPoly [Singh et al. 2019a], RefinePoly [Singh et al. 2019b], DeepPoly
[Singh et al. 2019c] (equivalent bounds to Crown [Zhang et al. 2018] and CNN-Cert [Boopathy et al. 2019]),
RefineZono [Singh et al. 2019c] and DeepZono [Singh et al. 2018] (equivalent bounds to Fast-Lin [Weng
et al. 2018] and Neurify [Wang et al. 2018] in incomplete mode), evaluated on the first 1000 samples (100 for
RefinePoly) of the corresponding test sets. The tightest known upper bound to the certifiable accuracy is
shown as dashed line. Higher and further left is better.

For our experiments, we use the setup outlined in Section 6 which is similar to kPoly in [Singh
et al. 2019a]. We use DeepPoly or GPUPoly (for convolutional networks) to determine the octahedral
input bounds required to compute the multi-neuron constraints with Prima. For fully-connected
networks, we refine the neuron-wise bounds of unstable neurons using the MILP encoding from
Tjeng et al. [2019] for the second activation layer (the first layer bounds are already exact) and an
LP encoding for the remaining layers. We note that encoding more layers with MILP does not scale
on these networks. For convolutional networks, we encode some of the neurons in the last one or
two layers using the MILP encoding from [Tjeng et al. 2019]. We note that the concurrent bound
optimization in 𝛽-Crown corresponds to simultaneous bound-refinement on all neurons of all
layers, which is orthogonal to our approach and a promising direction to be explored in future work
(though intractable without a GPU-based LP solver). We report as Accuracy the number of correctly
classified samples out of the considered test set, as # Upper Bound the number of properties that
could not be falsified and hence form an upper bound to the number of certifiable properties, as #
Ver the number of verified regions, and as Time the average runtime per correctly classified sample
in seconds.

7.3 Image Classification with ReLU Activation
We compare Prima against the state-of the art methods kPoly and OptC2V in Table 2 and 𝛽-Crown
in Table 3. Computing multi-neuron constraints for groups of 𝑘 = 4 ReLU neurons becomes feasible
with SBLM and PDDM reducing the time per group from several minutes, when directly computing
exact convex hulls as in kPoly, to less than 50 milliseconds. Nevertheless, we find empirically that
the best strategy to leverage this speed-up is to evaluate a large variety of small groups. Unless
reported differently, we consider overlapping groups of size 𝑘 = 3 with 𝑛𝑠 = 100.

Comparison with the state-of-the-art. Figure 13 shows scatter plots comparing the runtime and
precision of Prima with those of other state-of-the-art verifiers on the robustness certification of a
normally trained 5 × 100 MLP, a provably trained ConvBig (MNIST) and an adversarially trained
ConvSmall (CIFAR10). We note that adversarially and provably trained networks sacrifice accuracy
for ease of certification, making normally trained networks more relevant and challenging. Here,
fast, purely propagation-based, incomplete verifiers like DeepPoly verify only about 16% of the
images. In contrast, Prima verifies 51% in < 160 seconds per image. The closest verifiers in terms
of precision are kPoly and OptC2V, which verify 44% and 43% of samples and take around 310

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

10−1100101102103104MeanTimeperSample[s]0102030405060CertiﬁedAccuracy[%]OptC2VFastC2VkPolyPRIMADeepPolyReﬁnePoly*DeepZonoReﬁneZono10−1100101102103104MeanTimeperSample[s]30405060708090CertiﬁedAccuracy[%]OptC2VFastC2VkPolyPRIMADeepPolyReﬁnePoly*DeepZonoReﬁneZono100101102103104MeanTimeperSample[s]3035404550CertiﬁedAccuracy[%]OptC2VFastC2VkPolyPRIMADeepPolyReﬁnePoly*DeepZonoReﬁneZono43:24

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

𝜖

43

57

and 140 seconds, respectively. Based on these observations, we compare Prima with kPoly and
OptC2V on the remaining benchmarks from [Singh et al. 2019a].

Acc

Model

Table 3. Number of verified adversarial regions of the 100
random samples from the CIFAR10 test set evaluated by
[Wang et al. 2021]. CNN-A-Mix is trained using a combina-
tion of adversarial and natural training and CNN-B-Adv only
adversarially. Both are taken from [Dathathri et al. 2020].

Comparison with kPoly and OptC2V.
For all normally trained networks, Prima
is significantly more accurate than both
kPoly [Singh et al. 2019a] and OptC2V
[Tjandraatmadja et al. 2020], verifying be-
tween 44 and 201 more regions than the
better of the two while sometimes also be-
ing significantly faster. These results are
summarized in Table 2. For the, compara-
tively easy to verify (as can be seen in Fig-
ure 13(b)), DiffAI trained ConvBig MNIST
network, we gain less precision verifying only 4 more regions than OptC2V. However, the easier
proofs come at the cost of reduced accuracy, making them less relevant for real-world applications.
For both PGD-trained CIFAR10 networks, Prima verifies between 23 and 59 more regions than
kPoly and OptC2V while being around four times faster. On the provably trained ResNet, Prima
is 50x faster than kPoly and able to decide all properties. However, this network is so heavily
regularized that even complete verification via a MILP encoding is tractable. In summary, Prima is
usually faster than kPoly and OptC2V, especially on larger networks, and is always more precise,
sometimes substantially so.

2/255
2/255

# Ver Time

# Ver Time

Prima (ours)

𝛽-Crown

CNN-B-Adv

CNN-A-Mix

209
234

100
100

53
260

# Bound

43
46

68
81

# layers

# neurons

Refinement

Partial MILP

Table 4. Evaluation of a range of parameters for group-
ing set size 𝑛𝑠 , group size 𝑘, and overlap 𝑠, partial MILP
refinement, and neuron-wise bound refinement for the
first 100 samples of the MNIST test set and the normally
trained 5 × 100. Of the first 100 samples, 99 are classified
correctly and for 9 of those a counterexample is known.

Comparison with 𝛽-Crown. 𝛽-Crown
[Wang et al. 2021] is a highly optimized, fully
GPU-based complete BaB [Morrison et al.
2016] solver, supporting only ReLU activa-
tions8 and the classification setting. When
comparing complete and incomplete verifiers
on accuracy, it is crucial to ensure that similar
runtimes were achieved, as complete verifiers
can, given sufficient time, decide any prop-
erty. The GPU-based LP solver underlying
𝛽-Crown is an orthogonal development to
the Prima multi-neuron constraints. Prima
currently uses a much slower CPU-based
solver which is the main bottleneck for large
networks as the runtime for computing multi-
neuron constraints becomes small via our im-
proved algorithms (see Section 7.8). We con-
sider combining the GPU-based solver from
𝛽-Crown with our multi-neuron approxima-
tions as an interesting item for future work.
Despite the discrepancy in LP-solver perfor-
mance distorting the comparison, Prima is
still significantly faster on CNN-A-Mix while also achieving notably higher precision. On the larger
network CNN-B-Adv, where LP-solver performance is more dominant, 𝛽-Crown achieves slightly

2.56
5.75
6.52
67.79
54.05
16.59

24.15
99.40
115.24
189.21

4.58
42.00
44.03
117.37

1
10
20
20
20
100

21
26
28
28
28
28

1
100
100
100

-
-
-
100

1
100
100
100

30
30
100
100

# Ver Time [s]

27
45
54
60

23
30
30
35

LP MILP

1
3
3
3
4
3

-
1
1
2
1
1

-
-
-
-
-
-

-
-
-
-
-
-

-
-
-
-
-
-

-
-
-
-
-
-

y
y
y
y

-
-
y
y

1
3
3
3

-
-
-
2

1
3
3
3

1
1
1
2

-
1
1
1

-
1
1
1

-
-
-
-

-
-
-
-

𝑛𝑠

𝑘

𝑠

8Extensions to piecewise-linear activations with more than 𝑚 = 2 linear regions would significantly increase runtime
(O (𝑚𝑑 ) with split depth 𝑑), while precision would be significantly lower for non-piecewise linear activations.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:25

higher precision and smaller runtime. Unfortunately, we could not run the public version of
𝛽-Crown without soundness issues on the networks from [Singh et al. 2019a] and consequently
only compare on networks they provide. The recent SDP-based (semidefinite programming) SDP-FO
[Dathathri et al. 2020] takes many hours per sample and is outperformed by 𝛽-Crown. Thus we
do not compare to it directly.

7.4 Parameter Study

In Table 4, we compare the effect of different parameter combinations on runtime and accuracy for
the 5 × 100 MLP, which allows also more expensive settings to be evaluated while still representing
a challenging verification problem with 𝜖 = 0.026. Using the single-neuron triangle relaxation
(𝑘 = 1) only 21 regions can be verified. Adding our multi-neuron constraints with partition sizes
of 𝑛𝑠 = 10 and 𝑛𝑠 = 20 increases this to 26 and 28 regions, respectively. Neither considering a
larger overlap (𝑠 = 2), nor larger groups (𝑘 = 4), nor larger partition sizes (𝑛𝑠 = 100) can increase
the number of verified regions, despite significantly increased the runtime. While using triangle
relaxations with a partial MILP encoding is relatively fast it also only increases the accuracy to
23 regions. In contrast, combining a partial MILP encoding with multi-neuron constraints yields,
depending on the exact setting, an almost 75% increase to 35 verified regions, although at the
price of increased runtime. Refining the neuron-wise bounds using a triangle relaxation and LP
encoding only improves the number of verified regions to 27, while additionally using multi-neuron
constraints yields a significant jump to 45. This further improves to 54 when using MILP to refine
the second layer bounds and 60 when additionally encoding the last two layers with MILP. The
significant increase in precision when combining tight multi-neuron constraints computed via
SBLM and PDDM with other methods demonstrates their utility and highlights the potential of our
abstraction-refinement-based approach.

7.5 Effect of Grouping Strategy

We evaluate the sensitivity of Prima to the chosen neu-
ron groupings, by comparing the performance9 of random
groups with those generated by our sparse grouping heuris-
tic in Figure 14 for the first 100 test images of CIFAR10 and
the ConvSmall network. Concretely, we first generate a de-
terministic sparse grouping with our heuristic for a group
size of 𝑘 = 3, a partition size of 𝑛𝑠 = 100, and a maximum
overlap of 𝑠 = 1. Then we (randomly) reduce this grouping
to a fraction 𝑟 (x-axis in Figure 14) of the original number
of groups. The random groupings are generated to have
the same size (number of groups) by repeatedly drawing 𝑘
indices uniformly at random and rejecting duplicates.

We observe that considering fewer groups from our
heuristic (blue in Figure 14) reduces the bound improve-
ment notably, e.g., to 37% at 𝑟 = 0.1 (blue square). Choosing
random groups (orange in Figure 14) is consistently worse (vertical gap in Figure 14); by around
10% at 𝑟 = 1.0 (circles) closing to 3.4% at 𝑟 = 0.1 (squares). While our heuristic generates groups
with small overlap to evenly cover all neurons, random sampling can lead to some groups with
large overlap, while potentially not covering some neurons at all, leading to worse performance.

Fig. 14. Normalized bound improvement
over the fraction of groups used to com-
pute multi-neuron constraints, 𝑟 . Our
method is the blue circle, whose gain is
normalized to 100%.

9Concretely, we compare the obtained improvement of ℎ𝑦,𝑖 , the lower bound to the optimization objective min𝒙′∈B∞
𝒉(𝒙′)𝑖 , over the triangle relaxation (Δ) normalized using our standard sparse heuristic (Prima): (ℎ𝑦,𝑖 − ℎΔ
𝑦,𝑖 )/(ℎPrima

𝜖 𝒉(𝒙′)𝑦 −
− ℎΔ
𝑦,𝑖 )

𝑦,𝑖

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

0.00.51.01.52.0r=fractionofgroups0255075100125Normalizedboundimprovement[%]oursparseheuristicrandom43:26

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

Considering fewer groups makes overlaps between groups less likely, making the groupings re-
sulting from the two sampling strategies more similar and explaining the shrinking performance
gap. To obtain the same precision with random groups as with our heuristic, about twice as many
(𝑟 = 2.0, diamond) groups are needed (horizontal gap in Figure 14). We repeated these experiments
several times with different random seeds and obtained consistent results.

Overall, we conclude that while our heuristic consistently outperforms random groups, Prima is

relatively insensitive to the exact groupings, as long as sufficiently many are used.

7.6 Image Classification with Tanh and Sigmoid Activations

While using the exact convex hull algorithm
for ReLU relaxations is merely slow, it be-
comes infeasible for non-piecewise-linear ac-
tivations such as Tanh and Sigmoid. Com-
puting the constraints for a single group of
𝑘 = 3 neurons can take minutes using di-
rect exact convex hull computation, whereas
SBLM using PDDM takes only 10 millisec-
onds. This dramatic speed-up is a result of
SBLM’s decompositional approach of solving
the problem in lower dimensions (see Sec-
tion 5), significantly reducing its complexity.
Note that both methods compute only approx-
imations of the optimal group-wise convex
relaxation for these cases, as the underlying
interval-wise bounds are not exact.

Table 5. Number of verified adversarial regions and run-
time in seconds of Prima vs. DeepPoly for Tanh/Sigmoid
on 100 images from the MNIST dataset.

Act.

Model

Acc.

𝜖

DeepPoly

Prima

Ver.

Time Ver.

Time

Tanh

6 × 100
9 × 100
6 × 200

ConvSmall

Sigm 6 × 100
9 × 100
6 × 200

ConvSmall

97
98
98
99

99
99
99

99

0.006
0.006
0.002
0.005

0.015
0.015
0.012

0.014

38
18
39
16

30
38
43

30

0.3
0.4
0.6
0.4

0.3
0.5
1.0

0.5

61
52
68
30

53
56
73

51

72.5
186.0
170.0
27.8

96.9
336.4
267.0

47.0

We evaluate our method on normally trained, fully-connected and convolutional networks for
the MNIST dataset. We choose an 𝜖 for the 𝐵∞
𝜖 region such that the state-of-the-art verifier for Tanh
and Sigmoid activations, DeepPoly, verifies less than 50% of the regions. We remark that DeepPoly
is based on the same principles and has similar precision as other state-of-the-art verifiers for these
activations such as CNN-Cert [Boopathy et al. 2019] and Crown [Zhang et al. 2018].

We use overlapping groups with 𝑛𝑠 = 10 and again refine neuron-wise lower- and upper-bounds
for fully-connected networks. We verify between 14% and 34% more regions than the current state-
of-the-art, in some cases doubling the number of verified samples, while maintaining a reasonable
runtime comparable to that for ReLU networks (see Table 5).

7.7 Autonomous Driving

We evaluate Prima in the setting of au-
tonomous driving, deriving upper and
lower bounds to the predicted steering an-
gle under an ℓ∞ threat-model in a regres-
sion setting. We thereby demonstrate scal-
ability to large networks (> 100k neu-
rons and over 27 million connections)
and inputs (3 × 66 × 200) of real-world
relevance. We report the certified maxi-
mum absolute steering angle error and
the width of reachable steering angles. We

Table 6. Standard (std.), empirically maximal (emp.) and
certifiably maximal (cert.) mean absolute steering angle
error (MAE) (smaller is better) for Prima vs. GPUPoly eval-
uated on every 20𝑡ℎ sample and mean evaluation time.

𝜖

Method

std.
MAE

emp.
MAE

cert.
MAE

cert.
Width

Time [s]

1/255 GPUPoly

Prima

2/255 GPUPoly

Prima

7.37°
7.37°

7.37°
7.37°

9.41°
9.41°

10.35°
10.17°

5.75°
5.30°

11.46°
11.46°

18.35°
17.05°

19.63°
17.03°

1.55
154.2

2.41
239.5

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:27

(a) Number of constraints for indi-
vidual 𝑘-neuron abstractions.

(b) Volume of Prima and single-
neuron constraint polytopes com-
pared to the exact convex hull.

(c) Speedup of constraint compu-
tation using SBLM and PDD com-
pared to an exact convex hull.

Fig. 16. Case study: Analysis of the distribution of the number of discovered constraints, abstraction volume,
and runtime over all (≈ 360) individual 3-neuron groups processed during the verification of a single MNIST
image on the 5 × 100 ReLU network.

(a) Exact – MNIST 5 × 100

(b) SBLM + PDDM – MNIST 5 × 100

(c) SBLM + PDDM – CIFAR10 ConvSmall

Fig. 17. Comparison of the runtime contribution of the octahedral input constraint computation, multi-neuron
constraint computation and LP solve.

use PGD [Madry et al. 2018] to compute empirical bounds (emp). We use the CNN archi-
tecture proposed by Bojarski et al. [2016] and adversarial training [Madry et al. 2018] on
the Udacity autonomous driving dataset [Udacity 2016] to obtain the network evaluated here.

When the permissible perturbation size is
small and the standard error of the model is
larger than the perturbation effect, cheaper
methods such as GPUPoly already yield good
results. However, for larger perturbations,
Prima reduces the gap between empirical and
certified error around 20% (see Table 6). In Fig-
ure 15, we show two representative samples,
where the certified steering angle range for
𝜖 = 2/255 is shaded blue, the empirical bounds
on the steering angle are shown in red, the
target in green and the prediction on the un-
perturbed sample in blue. Qualitatively, we find
that while the network often still performs well
on unperturbed samples with poor lighting or contrast (see lower example in Figure 15) the sensi-
tivity to perturbations and consequently the width of the reachable steering angle range is much
larger than for samples in better conditions (see upper example in Figure 15).

Fig. 15. Samples from the self-driving car dataset. The
target steering angle is illustrated in green, the pre-
dicted one in blue. The empirical bounds for 𝜖 = 2/255
are shown in red and the certified range is shaded blue.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

020406080100Percentile0100200300400ConstraintNumberperGroupExactPRIMA020406080100Percentile246810NormalizedVolumeExactPRIMASingleNeuron020406080100Percentile100101102103Speed-up24x421x050100150200Time[s]0.000.020.040.060.080.100.12Density024681012Time[s]0.00.20.40.60.81.01.2Density05101520Time[s]0.00.20.40.60.81.0DensityOctahedronconstraintsMulti-NeuronconstraintsLPsolve43:28

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

Fig. 18. Runtime comparison of using SBLM vs. exact convex hull for computing relaxations in Prima.
Evaluated on 100 images and the MNIST 5 × 100 ReLU network.

7.8 Effectiveness of SBLM and PDDM for Convex Hull Computations

Computing approximations with SBLM using PDDM has two main advantages compared to the
direct convex hull approach: It is significantly faster and produces fewer constraints, making the
resulting LP easier to solve, while barely losing any precision.

For example, verifying the 5 × 100 network with Prima and comparing abstractions for groups
of 𝑘 = 3 computed with SBLM and PDDM or naively and neuron-wise triangle relaxation (Figure
16), we observe the following: Using SBLM and PDDM we reduce the mean number of constraints
computed per neuron-group by over 70% from 156 to 44 significantly reducing the number of con-
straints in the resulting LP, as many hundred such neuron groups are considered. The mean volume
of the constraint polytopes defined by these constraints in the 6-dimensional input-output space
of the individual neuron groups, meanwhile, is only around 5% larger. Single neuron constraints,
in contrast, yield 4-times larger volumes. Additionally, computing the approximate constraints is
about 200 times faster than the exact convex hull.

Not only are Prima constraints faster to generate and allow the verification of the same properties,
but a runtime analysis for the first 100 samples (illustrated in Figure 18) shows that they also speed
up the final LP solve 8-fold compared to the naive approach, as significantly fewer constraints have
to be considered. This effect is also observed in the time-intensive neuron-wise bound-refinement
where Prima constraints reduce the runtime by 70% while allowing 3 additional regions to be
verified. This can be explained by the fewer but more diverse Prima constraints also speeding up
the final LP solve in the refinement step reducing the number of timeouts and allowing tighter
neuron-wise bounds to be computed. Using neuron-refinement with Prima is in fact still quicker
than the naive approach without any refinement, while almost verifying twice as many samples.
SBLM combined with exact convex hulls computations already yields a small speed-up of around
20%, but the synergy with PDDM is key to unlock its full potential.

An analysis of the runtime contributions of the octahedral input constraint computation, the
multi-neuron constraint computation and the final LP solve (illustrated in Figure 17), shows the
following: Using the naive approach, the multi-neuron constraint computation clearly dominates
the runtime, while only contributing around 50% when using SBLM and PDDM. For larger net-
works, the input constraint computation and LP-solve become more expensive, reducing the
multi-neuron constraints computation runtime contribution further and further, e.g., 7% for the
CIFAR10 ConvSmall, and shifting the performance bottleneck to the LP-solver, especially when
neuron-wise bound-refinement or partial MILP encodings are used.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

0100200300400500Runtimeperimage[s]NaiveConvexHulSBLMConvexHulSBLM+PDDMNaiveConvexHull+ReﬁneSBLM+PDDM+Reﬁne28veriﬁed133.3s28veriﬁed107.0s28veriﬁed15.5s52veriﬁed313.0s55veriﬁed99.8sOctahedronconstraintsMulti-NeuronconstraintsNeuronreﬁnementFinalLPSolveMiscellaneousPRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:29

8 RELATED WORK

The importance of certifying the robustness of neural networks to input perturbations has created
a surge of research activity in recent years. The approaches with deterministic guarantees can be
divided into exact and incomplete methods. Incomplete methods are much faster and more scalable
than exact ones, but they can be imprecise, i.e., they may fail to certify a property even if it holds.
Complete methods are mostly based on satisfiability modulo theory (SMT) [Ehlers 2017; Huang
et al. 2017; Katz et al. 2017, 2019] or the branch-and-bound approach [Anderson et al. 2020; Botoeva
et al. 2020; Bunel et al. 2020b; Lu and Kumar 2020; Palma et al. 2021; Tjeng et al. 2019; Wang et al.
2021; Xu et al. 2021], often implemented using mixed integer linear programming (MILP). These
methods offer exactness guarantees but are based on solving NP-hard optimization problems, which
can make them intractable even for small networks. Incomplete methods can be divided into bound
propagation approaches [Gowal et al. 2019; Mirman et al. 2018; Müller et al. 2020; Singh et al. 2018,
2019b; Weng et al. 2018; Zhang et al. 2018] and those that generate polynomially-solvable optimiza-
tion problems [Bunel et al. 2020a; Dathathri et al. 2020; Lyu et al. 2020; Raghunathan et al. 2018;
Singh et al. 2019a; Tjandraatmadja et al. 2020; Xiang et al. 2018] such as linear programming (LP) or
semidefinite programming (SDP) optimization problems. Compared to deterministic certification
methods, randomized smoothing [Cohen et al. 2019; Lecuyer et al. 2018; Salman et al. 2019a] is a
defence method providing only probabilistic guarantees and incurring significant runtime costs at
inference time, with the generalization to arbitrary safety properties still being an open problem.
A new avenue towards more precision are methods [Palma et al. 2021; Singh et al. 2019a; Tjan-
draatmadja et al. 2020] breaking the so-called convex barrier [Salman et al. 2019b] by considering
activation functions jointly. However, their scalability is limited by the need to solve NP-hard con-
vex hull problems. There are many approaches for solving the convex hull problem for polyhedra
exactly [Avis and Fukuda 1991, 1992; Barber et al. 1993; Dantzig 1998; Edelsbrunner 2012; Fukuda
and Prodon 1995; Joswig 2003; Motzkin et al. 1953], in contrast to few approximate methods which
either sacrifice soundness [Bentley et al. 1982; Khosravani et al. 2013; Sartipizadeh and Vincent
2016; Zhong et al. 2014] or still exhibit exponential complexity [Xu et al. 1998], prohibiting their
use in neural network verification.

Our work follows the line of convex barrier-breaking methods, generalizing the concept to
arbitrary bounded, multivariate activations. In contrast to prior work, we decompose the underlying
convex hull problem into lower-dimensional spaces and solve it approximately using a novel relaxed
Double Description, irredundancy formulation, and a new ray-shooting-based algorithm to add
multiple constraints jointly. The resulting speed-ups make Prima tractable for non-piecewise-linear
activations, a first for convex barrier-breaking methods.

9 CONCLUSION

We presented Prima, a general framework that substantially advances the state-of-the-art in
neural network verification by providing efficient multi-neuron abstractions for arbitrary, bounded,
multivariate non-linear activation functions. Our key idea is to compute tighter overall abstractions
by considering many overlapping neuron groups thereby capturing more inter-neuron dependencies.
To enable this, we decompose the bottleneck convex hull computation into lower-dimensional
spaces and solve it approximately. Our extensive experimental evaluation shows that our algorithmic
advances shift the bottleneck to the LP-solver while significantly improving both precision and
scalability over prior work.

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:30

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

REFERENCES

Greg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri. 2019. Optimization and Abstraction: A Synergistic
Approach for Analyzing Neural Network Robustness. In Proc. Programming Language Design and Implementation (PLDI).
731–744. https://doi.org/10.1145/3314221.3314614

Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma. 2020. Strong mixed-integer
programming formulations for trained neural networks. Mathematical Programming (2020), 1–37. https://doi.org/10.
1007/s10107-020-01474-5

David Avis and Komei Fukuda. 1991. A basis enumeration algorithm for linear systems with geometric applications. Applied

Mathematics Letters 4, 5 (1991), 39–42. https://doi.org/10.1016/0893-9659(91)90141-H

David Avis and Komei Fukuda. 1992. A pivoting algorithm for convex hulls and vertex enumeration of arrangements and

polyhedra. Discrete & Computational Geometry 8, 3 (1992), 295–313. https://doi.org/10.1007/BF02293050

Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin T. Vechev. 2019. Certifying Geometric
Robustness of Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15287–15297.
https://proceedings.neurips.cc/paper/2019/hash/f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.html

C Bradford Barber, David P Dobkin, and Hannu Huhdanpaa. 1993. The quickhull algorithm for convex hull. Technical Report.

Technical Report GCG53, The Geometry Center, MN. https://doi.org/10.1145/235815.235821

Jon Louis Bentley, Franco P Preparata, and Mark G Faust. 1982. Approximation algorithms for convex hulls. Commun. ACM

25, 1 (1982), 64–68. https://doi.org/10.1145/358315.358392

Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel,
Mathew Monfort, Urs Muller, Jiakai Zhang, et al. 2016. End to end learning for self-driving cars. ArXiv preprint
abs/1604.07316 (2016). https://arxiv.org/abs/1604.07316

Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. 2019. CNN-Cert: An Efficient Framework for
Certifying Robustness of Convolutional Neural Networks. In The Thirty-Third AAAI Conference on Artificial Intelligence,
AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1,
2019. AAAI Press, 3240–3247. https://doi.org/10.1609/aaai.v33i01.33013240

Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener. 2020. Efficient Verification of
ReLU-Based Neural Networks via Dependency Analysis. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI
Press, 3291–3299. https://doi.org/10.1609/aaai.v34i04.5729

Rudy Bunel, Oliver Hinder, Srinadh Bhojanapalli, and Krishnamurthy Dvijotham. 2020a. An efficient nonconvex refor-
mulation of stagewise convex optimization problems. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/
paper/2020/hash/5d97f4dd7c44b2905c799db681b80ce0-Abstract.html

Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Pushmeet Kohli, P Torr, and P Mudigonda. 2020b. Branch and bound for piecewise

linear neural network verification. Journal of Machine Learning Research 21, 2020 (2020).

Bernard Chazelle. 1993. An optimal convex hull algorithm in any fixed dimension. Discrete & Computational Geometry 10, 4

(1993), 377–409. https://doi.org/10.1007/BF02573985

Robert Clarisó and Jordi Cortadella. 2007. The octahedron abstract domain. Science of Computer Programming 64, 1 (2007),

115–139. https://doi.org/10.1007/978-3-540-27864-1_23

Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. 2019. Certified Adversarial Robustness via Randomized Smoothing. In
Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
USA (Proceedings of Machine Learning Research), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 97. PMLR,
1310–1320. http://proceedings.mlr.press/v97/cohen19c.html

Patrick Cousot. 1996. Abstract Interpretation. ACM Comput. Surv. 28, 2 (1996), 324–328. https://doi.org/10.1145/234528.234740
George Bernard Dantzig. 1998. Linear programming and extensions. Vol. 48. Princeton university press. https://doi.org/10.

1515/9781400884179

Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy Bunel, Shreya
Shankar, Jacob Steinhardt, Ian J. Goodfellow, Percy Liang, and Pushmeet Kohli. 2020. Enabling certification of verification-
agnostic networks via memory-efficient semidefinite programming. In Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.
neurips.cc/paper/2020/hash/397d6b4c83c91021fe928a8c4220386b-Abstract.html

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:31

Herbert Edelsbrunner. 2012. Algorithms in combinatorial geometry. Vol. 10. Springer Science & Business Media. https:

//doi.org/10.1007/978-3-642-61568-9

Ruediger Ehlers. 2017. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium
on Automated Technology for Verification and Analysis. Springer, 269–286. https://doi.org/10.1007/978-3-319-68167-2_19

Komei Fukuda. 2020. Polyhedral Computation. https://doi.org/10.3929/ethz-b-000426218
Komei Fukuda and Alain Prodon. 1995. Double description method revisited. In Franco-Japanese and Franco-Chinese

Conference on Combinatorics and Computer Science. Springer, 91–111. https://doi.org/10.1007/3-540-61576-8_77

Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. 2018. Ai2:
Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security
and Privacy (SP). IEEE, 3–18. https://doi.org/10.1109/SP.2018.00058

Blagoy Genov. 2015. The convex hull problem in practice: improving the running time of the double description method. Ph.D.

Dissertation.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic,
Timothy Arthur Mann, and Pushmeet Kohli. 2019. Scalable Verified Training for Provably Robust Image Classification.
In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,
2019. IEEE, 4841–4850. https://doi.org/10.1109/ICCV.2019.00494

Gurobi Optimization, LLC. 2018. Gurobi Optimizer Reference Manual. http://www.gurobi.com
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety verification of deep neural networks. In
International Conference on Computer Aided Verification. Springer, 3–29. https://doi.org/10.1007/978-3-319-63387-9_1
Michael Joswig. 2003. Beneath-and-beyond revisited. In Algebra, Geometry and Software Systems. Springer, 1–21. https:

//doi.org/10.1007/978-3-662-05148-1_1

Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. 2017. Reluplex: An efficient SMT solver for
verifying deep neural networks. In International Conference on Computer Aided Verification. Springer, 97–117. https:
//doi.org/10.1007/978-3-319-63387-9_5

Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor,
Haoze Wu, Aleksandar Zeljić, et al. 2019. The marabou framework for verification and analysis of deep neural networks. In
International Conference on Computer Aided Verification. Springer, 443–452. https://doi.org/10.1007/978-3-030-25540-4_26
Hamid R Khosravani, António E Ruano, and Pedro M Ferreira. 2013. A simple algorithm for convex hull determination
in high dimensions. In 2013 IEEE 8th International Symposium on Intelligent Signal Processing. IEEE, 109–114. https:
//doi.org/10.1109/WISP.2013.6657492

Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2018. Certified Robustness to
Adversarial Examples with Differential Privacy. 2019 IEEE Symposium on Security and Privacy (S&P) (2018). https:
//doi.org/10.1109/SP.2019.00044

Jingyue Lu and M. Pawan Kumar. 2020. Neural Network Branching for Neural Network Verification. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https:
//openreview.net/forum?id=B1evfa4tPB

Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, and Luca Daniel. 2020. Fastened CROWN: Tightened
Neural Network Robustness Certificates. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 5037–5044.
https://doi.org/10.1609/aaai.v34i04.5944

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning
Models Resistant to Adversarial Attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=
rJzIBfZAb

Alexandre Maréchal and Michaël Périn. 2017. Efficient elimination of redundancies in polyhedra using raytracing.
Matthew Mirman, Timon Gehr, and Martin T. Vechev. 2018. Differentiable Abstract Interpretation for Provably Robust
Neural Networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research), Jennifer G. Dy and Andreas Krause (Eds.),
Vol. 80. PMLR, 3575–3583. http://proceedings.mlr.press/v80/mirman18b.html

David R Morrison, Sheldon H Jacobson, Jason J Sauppe, and Edward C Sewell. 2016. Branch-and-bound algorithms: A
survey of recent advances in searching, branching, and pruning. Discrete Optimization 19 (2016), 79–102. https:
//doi.org/10.1016/j.disopt.2016.01.005

Theodore S Motzkin, Howard Raiffa, Gerald L Thompson, and Robert M Thrall. 1953. The double description method.

Contributions to the Theory of Games 2, 28 (1953), 51–73. https://doi.org/10.1515/9781400881970-004

Christoph Müller, Francois Serre, Gagandeep Singh, Markus Püschel, and Martin Vechev. 2021. Scaling Polyhedral Neural

Network Verification on GPUs. Proc. Machine Learning and Systems (MLSys) (2021).

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

43:32

Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and Martin Vechev

Christoph Müller, Gagandeep Singh, Markus Püschel, and Martin Vechev. 2020. Neural Network Robustness Verification on

GPUs. arXiv:cs.LG/2007.10868

Alessandro De Palma, Harkirat S. Behl, Rudy R. Bunel, Philip H. S. Torr, and M. Pawan Kumar. 2021. Scaling the Convex
Barrier with Active Sets. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=uQfOy7LrlTR

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Semidefinite relaxations for certifying robustness to adversarial
examples. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 10900–10910. https://proceedings.neurips.cc/paper/
2018/hash/29c0605a3bab4229e46723f89cf59d83-Abstract.html

Anian Ruoss, Maximilian Baader, Mislav Balunović, and Martin Vechev. 2020a. Efficient Certification of Spatial Robustness.

ArXiv preprint abs/2009.09318 (2020). https://arxiv.org/abs/2009.09318

Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin T. Vechev. 2020b. Learning Certified Individually Fair Representa-
tions. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/55d491cf951b1b920900684d71419282-
Abstract.html

Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Dan, and Martin Vechev. 2020. Fast and effective
robustness certification for recurrent neural networks. ArXiv preprint abs/2005.13300 (2020). https://arxiv.org/abs/2005.
13300

Hadi Salman, Jerry Li, Ilya P. Razenshteyn, Pengchuan Zhang, Huan Zhang, Sébastien Bubeck, and Greg Yang. 2019a. Provably
Robust Deep Learning via Adversarially Trained Smoothed Classifiers. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman
Garnett (Eds.). 11289–11300. https://proceedings.neurips.cc/paper/2019/hash/3a24b25a7b092a252166a1641ae953e7-
Abstract.html

Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. 2019b. A Convex Relaxation Barrier to Tight
Robustness Verification of Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 9832–9842.
https://proceedings.neurips.cc/paper/2019/hash/246a3c5544feb054f3ea718f61adfa16-Abstract.html

Hossein Sartipizadeh and Tyrone L Vincent. 2016. Computing the approximate convex hull in high dimensions. ArXiv

preprint abs/1603.04422 (2016). https://arxiv.org/abs/1603.04422

Raimund Seidel. 1995. The upper bound theorem for polytopes: an easy proof of its asymptotic version. Computational

Geometry 5, 2 (1995), 115–116. https://doi.org/10.1016/0925-7721(95)00013-Y

Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin T. Vechev. 2019a. Beyond the Single Neuron Convex
Barrier for Neural Network Certification. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15072–15083.
https://proceedings.neurips.cc/paper/2019/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html

Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin T. Vechev. 2018. Fast and Effective Robustness
Certification. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 10825–10836. https://proceedings.neurips.cc/paper/
2018/hash/f2f446980d8e971ef3da97af089481c3-Abstract.html

Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. 2019b. An abstract domain for certifying neural
networks. Proceedings of the ACM on Programming Languages 3, POPL (2019), 1–30. https://doi.org/10.1145/3290354
Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. 2019c. Boosting Robustness Certification of Neural
Networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net. https://openreview.net/forum?id=HJgeEh09KQ

Gagandeep Singh, Markus Püschel, and Martin Vechev. 2017. Fast Polyhedra Abstract Domain. In Proc. Principles of

Programming Languages (POPL). 46–59. https://doi.org/10.1145/3009837.3009885

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus.
Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR
2014.
2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http:
//arxiv.org/abs/1312.6199

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations

43:33

Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo Vielma. 2020. The Convex
Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification. In Advances in
Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/f6c2a0c4b566bc99d596e58638e342b0-Abstract.html

Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. 2019. Evaluating Robustness of Neural Networks with Mixed Integer
Programming. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019. OpenReview.net. https://openreview.net/forum?id=HyGIdiRqtm

Udacity. 2016. Using Deep Learning to Predict Steering Angles. https://github.com/udacity/self-driving-car.
Caterina Urban and Antoine Miné. 2021. A Review of Formal Methods applied to Machine Learning. ArXiv preprint

abs/2104.02466 (2021). https://arxiv.org/abs/2104.02466

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Efficient Formal Safety Analysis of Neural
Networks. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 6369–6379. https://proceedings.neurips.cc/paper/
2018/hash/2ecd2bd94734e5dd392d8678bc64cdab-Abstract.html

Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. 2021. Beta-CROWN: Efficient
Bound Propagation with Per-neuron Split Constraints for Complete and Incomplete Neural Network Verification. ArXiv
preprint abs/2103.06624 (2021). https://arxiv.org/abs/2103.06624

Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane S. Boning, and Inderjit S. Dhillon.
2018. Towards Fast Computation of Certified Robustness for ReLU Networks. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of
Machine Learning Research), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 5273–5282. http://proceedings.
mlr.press/v80/weng18a.html

Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. 2018. Scaling provable adversarial defenses. In
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen
Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 8410–8419. https://proceedings.neurips.cc/paper/2018/hash/
358f9e7be09177c17d0d17ff73584307-Abstract.html

Weiming Xiang, Hoang-Dung Tran, and Taylor T Johnson. 2018. Output reachable set estimation and verification for
multilayer neural networks. IEEE transactions on neural networks and learning systems 29, 11 (2018), 5777–5783. https:
//doi.org/10.1109/TNNLS.2018.2808470

Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui
Hsieh. 2020. Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond. In Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/0cbc5671ae26f67871cb914d81ef8fc1-Abstract.html

Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. 2021. Fast and Complete: Enabling
Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers. In 9th International
Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
https:
//openreview.net/forum?id=nVZtXBI6LNn

Zong-Ben Xu, Jiang-She Zhang, and Yiu-Wing Leung. 1998. An approximate algorithm for computing multidimensional
convex hulls. Applied mathematics and computation 94, 2-3 (1998), 193–226. https://doi.org/10.1016/S0096-3003(97)10043-1
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. 2018. Efficient Neural Network Robustness
Certification with General Activation Functions. In Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, Samy
Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 4944–4953.
https://proceedings.neurips.cc/paper/2018/hash/d04863f100d59b3eb688a11f95b0ae60-Abstract.html

Jinhong Zhong, Ke Tang, and A Kai Qin. 2014. Finding convex hull vertices in metric space. In 2014 International Joint

Conference on Neural Networks (IJCNN). IEEE, 1587–1592. https://doi.org/10.1109/IJCNN.2014.6889699

Proc. ACM Program. Lang., Vol. 6, No. POPL, Article 43. Publication date: January 2022.

