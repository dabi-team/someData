0
2
0
2

c
e
D
0
1

]

C
O
.
h
t
a
m

[

1
v
8
9
3
5
0
.
2
1
0
2
:
v
i
X
r
a

Hardness results for Multimarginal Optimal Transport problems

Jason M. Altschuler

Enric Boix-Adser`a

Abstract

Multimarginal Optimal Transport (MOT) is the problem of linear programming over joint
probability distributions with ﬁxed marginals. A key issue in many applications is the complexity
of solving MOT: the linear program has exponential size in the number of marginals k and their
support sizes n. A recent line of work has shown that MOT is poly(n, k)-time solvable for certain
families of costs that have poly(n, k)-size implicit representations. However, it is unclear what
further families of costs this line of algorithmic research can encompass. In order to understand
these fundamental limitations, this paper initiates the study of intractability results for MOT.
Our main technical contribution is developing a toolkit for proving NP-hardness and inap-
proximability results for MOT problems. We demonstrate this toolkit by using it to establish the
intractability of a number of MOT problems studied in the literature that have resisted previous
algorithmic eﬀorts. For instance, we provide evidence that repulsive costs make MOT intractable
by showing that several such problems of interest are NP-hard to solve—even approximately.

1

Introduction

Multimarginal Optimal Transport (MOT) is the problem of linear programming over joint prob-
ability distributions with ﬁxed marginal distributions. That is, given k marginal distributions
µ1, . . . , µk in the simplex1 ∆n = {u ∈ Rn
⩾0 ∶ ∑n
i=1 ui = 1} and a cost tensor C in the k-fold tensor
product space (Rn)⊗k = Rn ⊗ ⋯ ⊗ Rn, compute

min
P ∈M(µ1,...,µk)

⟨P, C⟩

(MOT)

}

and j ∈

where M(µ1, . . . , µk) is the “transportation polytope” containing entrywise non-negative tensors
P ∈ (Rn)⊗k satisfying the marginal constraints ∑j1,...,ji−1,ji+1,...,jk Pj1,...,ji−1,j,ji+1,...,jk =
j for all
]
i ∈

1, . . . , k
{
This MOT problem has recently attracted signiﬁcant interest due to its many applications in
data science, applied mathematics, and the natural sciences; see for instance [2, 6, 31, 32] and the
many references within. However, a key issue that dictates the usefulness of MOT in applications
is its complexity. Indeed, while MOT can be easily solved in nΘ(k) time since it is a linear program
in nk variables and nk + nk constraints, this is far from scalable.

1, . . . , n
{

µi
[

.
}

In this paper and the literature, we are interested in “polynomial time” algorithms, where
polynomial means in the number of marginals k and their support sizes n (as well as the scale-
ε if we are considering ε additive approximations). An obvious obstacle
invariant quantity

C
∥

max
∥

/

The authors are with the Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of
Technology, Cambridge MA 02139. Work partially supported by NSF Graduate Research Fellowship 1122374, a
Siebel PhD Fellowship, and a TwoSigma PhD fellowship.

For notational simplicity, all µi are assumed to have the same support size. The general case is a straightforward

1

extension.

1

 
 
 
 
 
 
is that in general, one cannot even read the input to MOT—let alone solve MOT—in poly
time since the cost tensor C has nk entries.

n, k
(

)

Nevertheless, in nearly all applications of practical interest, the cost tensor C has a simple
structure that enables it to be input implicitly via a poly
-sized representation. Moreover,
)
a recent line of work has shown that in many applications where C has such a polynomial-size
implicit representation, the MOT problem can also be solved in polynomial time. A simple-to-
describe illustrative example is cost tensors C which have constant rank and are given as input in
factored form [2]. Other examples include computing generalized Euler ﬂows [2, 6], computing low-
dimensional Wasserstein barycenters [3, 14], solving MOT problems with tree-structured costs [25],
and solving MOT problems with decomposable costs [2].

n, k
(

A fundamental question is: What further families of succinctly representable costs lead to
n, k
time
(
? There are a number
)
-sized representation but
)
-time algorithms has resisted previous eﬀorts. The purpose of this paper is to
)

tractable MOT problems? As an illustrative example, can MOT still be solved in poly
if the cost C has rank that is low but not constant, say of size poly
of MOT problems studied in the literature for which C has a poly
developing poly
understand the fundamental limitations of this line of algorithmic research.

n, k
(
n, k
(

n, k
(

)

1.1 Contributions

This paper initiates the study of intractability results for MOT. Our main contributions are:

1. In §3, we develop a toolkit for proving NP-hardness and inapproximability results for MOT
-size implicit representations. This is the main
)

problems on costs C that have poly
technical contribution of the paper.

n, k
(

2. In §4, §5, and §6, we demonstrate this toolkit by using it to establish the intractability of a
number of MOT problems studied in the literature that have resisted previous algorithmic
eﬀorts.

We elaborate on each point below.

1.1.1 Reduction toolkit

)

µ
(

denote the problem of computing the optimal value of MOT for a cost tensor C ∈
Let MOTC
Rn
k. Informally, our main result establishes that for
)
)
(
any ﬁxed cost C, the following discrete optimization problem MINC can be (approximately) solved
in polynomial time if MOTC can be (approximately) solved in polynomial time.

⊗k and marginals µ =
)

µ1, . . . , µk
(

∆n
(

∈

Deﬁnition 1.1. For C ∈

Rn
(

⊗k and p =
)

p1, . . . , pk
(

)

∈ Rn×k, the problem MINC

is to compute

p
(

)

min
(j1,...,jk)∈{1,...,n}k

Cj1,...,jk −

k
∑
i=1[

pi

ji.
]

(1.1)

The upshot of our result is that it enables us to prove intractability results for MOTC by instead
proving intractability results for MINC. This is helpful since MINC is more directly amenable
to NP-hardness and inapproximability reductions because it is phrased as a more conventional
combinatorial optimization problem; examples in §4, §5, and §6.

We brieﬂy highlight the primary insight behind the proof: The convex relaxation of MINC is
exact and is a convex optimization problem whose objective can be evaluated by solving an auxiliary
MOTC problem. This means that if one can (approximately) solve MOTC, then one can use this in

2

a black-box manner to (approximately) solve MINC via zero-th order optimization. In §3, we show
how to perform this zero-th order optimization eﬃciently using the Ellipsoid algorithm if MOTC
can be computed exactly, and otherwise using recent developments on zero-th order optimization
of approximately convex functions [5, 34] if MOTC can be computed approximately.

We conclude this discussion with several remarks.

Remark 1.2 (Converse). There is no loss of generality when using our results to reduce proving the
intractability of MOTC to proving the intractability of MINC . This is because the MOTC and MINC
problems are polynomial-time equivalent—for any cost C, and for both exact and approximate
solving—because the converse of this reduction also holds [2, §3].

Remark 1.3 (Value vs solution for MOT). A desirable feature of our hardness results is that they
apply regardless of how an MOT solution is computed and (compactly) represented2. This is because
we show hardness for (approximately) computing the optimal value of MOT.

Remark 1.4 (Diﬀerences from classical LP theory). The intuition behind the MIN problem is that it
is the feasibility problem for the dual LP to MOT; see the preliminaries section. However, it should
be emphasized that our reductions rely on the particular structure of the LP deﬁning MOT, and do
not hold for a general LP and its dual feasibility oracle [23]. Moreover, our approximate reduction
is even further from the purview of classical LP theory since it can be used to prove hardness of
approximating to polynomially small error rather than exponentially small error.

0
)
(

Remark 1.5 (p = 0). The MINC
problem is to compute the minimum entry of the tensor C.
Thus, as a special case of our reductions, it follows that if (approximately) computing the minimum
entry of C is NP-hard, then so is (approximately) computing MOTC. In fact, in our applications
in §4, §5, and §6, we prove intractability of MINC —and thus of MOTC—by showing intractability
for this “simple” case p = 0. However, we mention that in general one cannot restrict only to the
case p = 0: In §7, we give a concrete example where MINC is tractable for p = 0 but not general p.

1.1.2 Applications

In §4, we demonstrate this toolbox on MOT problems with low-rank cost
Low-rank costs.
tensors given in factored form. Recent algorithmic work has shown that such MOT problems can
be solved to arbitrary precision ε > 0 in poly
time for any ﬁxed rank r [2]. However,
n, k, Cmax
(
this algorithm’s dependence on r is exponential, and it is a natural question whether such MOT
problems can be solved in time that is also polynomial in r. We show that unless P = NP, the answer
is no. Moreover, our hardness result extends even to approximate computation. This provides a
converse to the aforementioned algorithmic result.

ε
)

/

Pairwise-interaction costs.
into sums of pairwise interactions

In §5, we consider MOT problems with costs C that decompose

Cj1,...,jk = ∑

1⩽i<i′⩽k

gi,i′

ji, ji′
(

)

(1.2)

→ R. This cost structure appears in many applications; for instance
for some functions gi,i′ ∶
in Wasserstein barycenters [4] and the MOT relaxation of Density Functional Theory [12, 15].

n
[

n
[

×

]

]

2

Indeed, the representations produced by MOT algorithms often vary: e.g., the solution is polynomially-sparse
for the Ellipsoid and Multiplicative Weights algorithms; and is fully dense but has a polynomial-size representation
which supports certain eﬃcient operations for the Sinkhorn algorithm. See [2] for details.

3

Although these costs have poly
alone is not suﬃcient for solving MOT in polynomial time.

-size implicit representations, we show that this cost structure
)

n, k
(

One implication of this NP-hardness result is a converse to the algorithmic result of [2] which
shows that MOT problems can be solved in poly
time for costs C which are decomposable
n, k
(
into local interactions of low treewidth. This is a converse because the pairwise-interactions struc-
ture (1.2) also falls under the framework of MOT costs that decompose into local interactions, but
has high treewidth.

)

In §6, we consider MOT problems with “repulsive costs”. Informally, these are
Repulsive costs.
costs Cj1,...,jk which encourage diversity between the indices j1, . . . jk; we refer the reader to the
nice survey [17] for a detailed discussion of such MOT problems and their many applications. We
provide evidence that repulsive costs lead to intractable MOT problems by proving that several
such MOT problems of interest are NP-hard to solve. Again, our hardness results extend even to
approximate computation.

Speciﬁcally, in §6.1 we show this for MOT problems with the determinantal cost studied in [13,
17], and in §6.3 we show this for the popular MOT formulation of Density Functional Theory [8, 12,
15] with the Coulomb-Buckingham potential. Additionally, in §6.2, we observe that the classical
problem of evaluating the convex envelope of a discrete function is an instance of MOT, and we
leverage this connection to point out that MOT is NP-hard to approximate with supermodular costs,
yet tractable with submodular costs. This dichotomy provides further evidence for the intractability
of repulsive costs, since the intractable former problem has a “repulsive” cost, whereas the tractable
latter problem has an “attractive” cost.

To our knowledge, these are the ﬁrst results that rigorously demonstrate intractability of MOT
problems with repulsive costs. This provides the ﬁrst step towards explaining why—despite a
rapidly growing literature—there has been a lack of progress in developing polynomial-time algo-
rithms with provable guarantees for many MOT problems with repulsive costs.

1.2 Related work

Algorithms for MOT. The many applications of MOT throughout data science, mathematics,
and the sciences at large have motivated a rapidly growing literature around developing eﬃcient
algorithms for MOT. The algorithms in this literature can be roughly divided into two categories.
The ﬁrst category consists of MOT algorithms which work for generic “unstructured” costs.
While these algorithms work for any MOT problem, they inevitably cannot have polynomial runtime
in n and k since they read all nk entries of the cost tensor. A simple such algorithm is to solve MOT
using an out-of-the-box LP solver; this has nΘ(k) runtime. An alternative popular algorithm is the
natural multimarginal generalization of the Sinkhorn scaling algorithm, which similarly has nΘ(k)
runtime but can be faster than out-of-the-box LP solvers in practice, see e.g., [6, 7, 8, 9, 19, 27, 28, 37]
among many others. The exponential runtime dependence on n and k prohibits these algorithms
from being usable beyond very small values of n and k. For instance even n = k = 10, say, is at the
scalability limits of these algorithms.

The second category consists of MOT algorithms that can run in poly

time3 for MOT
)
problems with certain “structured” costs that have poly
-sized implicit representations. This
)
line of work includes for instance the algorithms mentioned earlier in the introduction, namely com-
puting generalized Euler ﬂows [2, 6], computing low-dimensional Wasserstein barycenters [3, 14],

n, k
(

n, k
(

3

Or poly(n, k, Cmax/ε) time for ε-approximate solutions.

4

solving MOT problems with tree-structured costs [25], and solving MOT problems with decompos-
able costs [2]. The purpose of this paper is to understand the fundamental limitations of this line
of work.

0, 1
}
{

entries, and the marginals are uniform µi = 1n

Connection to fractional hypergraph matching and complexity of sparse solutions.
Deciding whether MOT has a solution of sparsity exactly n is well-known to be NP-hard. This
NP-hardness holds even in the special case where the number of marginals k = 3, the cost tensor C
n. This is because in this special case,
has all
MOT is the natural convex relaxation of the NP-hard k-partite matching problem [21, 26], and in
particular n-sparse solutions to this MOT problem are in correspondence with optimal solutions
to this NP-hard problem. The NP-hardness of ﬁnding the sparsest MOT solution further extends
to “structured” MOT problems whose costs have poly
-size implicit representations, e.g., the
)
Wasserstein barycenter problem [10].

n, k
(

/

However, it is important to clarify that ﬁnding an MOT solution which is polynomially sparse
(albeit perhaps not the sparsest) is not necessarily NP-hard.
Indeed, sparse such solutions can
be computed in polynomial-time for e.g., low-dimensional Wasserstein barycenters [3] and MOT
problems with decomposable costs [2].

We emphasize that a desirable property of our hardness results is that they entirely bypass this
discussion about whether computing sparse solutions is tractable. This is because our results show
that it is NP-hard to even compute the value of certain structured MOT problems (see Remark 1.3),
which is clearly a stronger statement than NP-hardness of computing a sparse solution.

Other related work. We mention two tangentially related bodies of work in passing. First, the
transportation polytope (a.k.a., the constraint set in the MOT problem) is an object of signiﬁcant
interest in discrete geometry and combinatorics, see e.g., [16, 39] and the references within. Second,
linear programming problems over exponentially-sized joint probability distributions appear in
various ﬁelds such as game theory [30] and variational inference [38]. However, it is important to
note that the complexity of these linear programming problems is heavily aﬀected by the speciﬁc
linear constraints, which often diﬀer between problems in diﬀerent ﬁelds.

2 Preliminaries

1, . . . , n
{

⊗k. The set
P
(

Notation. The k-fold tensor product space Rn ⊗ ⋯ ⊗ Rn is denoted by
Rn
is denoted by
⩾0)
(
vector mi
µ1, . . . , µk
this notation, the transportation polytope in (MOT) is M
(
µi, ∀i ∈
j1, . . . , jk
. For shorthand, we often denote an index
by
(
we denote the maximum absolute value of its entries by Cmax = max⃗j ∣
poly

⊗k, and similarly for
)
Rn
⊗k is the
. The i-th marginal of a tensor P ∈
n
(
)
]
[
∈ Rn with j-th entry ∑j1,...,ji−1,j,ji+1,...,jk Pj1,...,ji−1,j,ji+1,...,jk, for i ∈
n
and j ∈
. In
]
[
⊗k ∶ mi
P
P ∈
=
)
(
{
Rn
⊗k,
)
(
. For shorthand, we write

k
]
[
Rn
⩾0)
)
(
j. For a tensor C ∈
⃗
C⃗j∣

to denote a function that grows at most polynomially fast in those parameters.

Rn
(

k
[

]}

=

}

)

)

t1, . . . , tm
(

)

MOT dual. The dual LP to (MOT-D) is

max
p1,...,pk∈Rn

k
∑
i=1⟨

pi, µi

⟩

subject to

C⃗j −

k
∑
i=1[

pi

j ∈
ji ⩾ 0, ∀
⃗
]

n
[

k.
]

(MOT-D)

Observe that p =
p
)
(
has non-negative value. This is the connection between the problem MINC and the feasibility oracle
for (MOT-D) alluded to in Remark 1.4.

∈ Rn×k is feasible for (MOT-D) if and only if the problem MINC

p1, . . . , pk
(

)

5

Bit complexity. Throughout, we assume for simplicity of exposition that all entries of the cost
C and the weights p ∈ Rnk inputted in the MINC problem have bit complexity at most poly
.
)
This implies that the distributions µ on which the MOTC oracle is queried in Theorems 3.1 and
3.2 also have polynomial bit complexity. The general case is a straightforward extension.

n, k
(

Computational complexity. BPP is the class of problems solvable by polynomial-time random-
ized algorithms with error probability that is < 1
2).
/
⊂ BPP” is a standard assumption in computational complexity and is the ran-
The statement “NP
/
domized version of P ≠ NP, i.e., that NP-hard problems do not have polynomial-time randomized
algorithms.

3 (or equivalently, any constant less than 1
/

3 Reducing MIN to MOT

Here we present the reduction toolkit overviewed in §1.1. Speciﬁcally, we show the following
two reductions from MINC to MOTC. The ﬁrst reduction is used for proving NP-hardness of
exactly solving MOTC, and the second reduction is used for proving inapproximability. These two
reductions are incomparable.

Theorem 3.1 (Exact reduction). There is a deterministic algorithm that, given access to an oracle
solving MOTC and weights p ∈ Rn×k, solves MINC

p
(
Theorem 3.2 (Approximate reduction). There is a randomized algorithm that, given ε > 0, access
to an oracle solving MOTC to additive accuracy ε, and weights p ∈ Rn×k, solves MINC
up to
ε ⋅ poly

time and oracle queries.

time and oracle queries.

3 in poly

n, k
(

in poly

p
(

)

)

)

n, k
(

)

additive accuracy with probability 2
/

n, k, Cmax
ε )
(

We make two remarks in passing about these theorems. First, they hold unchanged if the MINC
problem is modiﬁed to require computing the minimizing tuple rather than the minimum value
in (1.1). This is because these two problems are polynomial-time equivalent [2, Appendix A.1].
Second, the inapproximability reduction is probabilistic4, and thus shows inapproximability under
the standard complexity assumption NP
⊂ BPP, which is informally the stronger “randomized
/
version” of P ≠ NP.

3.1 Proof overview

As described brieﬂy in §1.1, the main idea is to reduce MINC to a convex optimization problem for
which the objective function can be evaluated by solving an auxiliary MOTC problem. Formally,
embed

∶=
where ej denotes the vector in Rn with a 1 on entry j, and 0’s on all other entries. Then MINC is
equivalent to minimizing f

T ,
ej1, . . . , ejk )
(

over the discrete set

j1, . . . , jk

((

))

−

φ

k into Rnk via
]

n
[

∶= Cφ−1(x)

x
(

)

=

n

S ∶= φ

nk ∶
k
,
0, 1
1 = 1
}
∥
}
{
)
]
∈ Rn×k as the vector in Rnk formed
where here we abuse notation slightly by viewing p =
p1, . . . , pk
(
by concatenating its columns p1, . . . , pk. Let F ∶ Rnk → R denote the convex envelope of f , i.e.,
the pointwise largest convex function over Rnk that is pointwise below f on the domain S of f .

x1, . . . , xk
(

1 = ⋅ ⋅ ⋅ =
∥

x =
{

xk
∥

x1
∥

([

)

p, x
⟨

⟩
T ∈
)

4

It is an interesting question whether the approximate reduction in Theorem 3.2 can be de-randomized. This

would enable showing our inapproximability results under the assumption P ≠ NP rather than NP /⊂ BPP.

6

By explicitly computing F as the Fenchel bi-conjugate of f , we obtain the following two useful
characterizations of F (proved in §3.2). Below, let ∆S denote the set of probability distributions
over S.
Lemma 3.3 (MOTC is convex envelope of MINC). For all µ =
Ex∼Df

∆n
(

k,
)

(3.1)

F

=

)

∈

µ1, . . . , µk
(
x
(

)

µ
(

)

min
D∈∆S s.t. Ex∼Dx=µ
+ MOTC

= −

µ, p
⟨

⟩

µ
(

.
)

(3.2)

The ﬁrst representation (3.1) of F gives a Choquet integral representation of F in terms of
f . Importantly, it implies that in order to (approximately) minimize f over its discrete domain
S—i.e., solve the (approximate) MINC oracle—it suﬃces to (approximately) minimize F over its
continuous domain, namely the convex hull conv
S
)
(
Now to (approximately) minimize F , we appeal to algorithmic results from zero-th order convex
optimization since the second representation (3.2) of F shows that (approximately) evaluating
F amounts to (approximately) solving an auxiliary MOTC problem. Speciﬁcally, we show how
to implement the zero-th order optimization using the Ellipsoid algorithm in the case of exact
oracle evaluations, and otherwise using the recent results [5, 34] on zero-th order optimization of
approximately convex functions in the case of approximate oracle evaluations.

k. See Corollary 3.5.
)

∆n
(

=

Remark 3.4 (Connection to submodular optimization). This proof is inspired by the classical idea
of minimizing a submodular function by minimizing its Lov´asz extension [22], which is a special case
of Theorem 3.1 for n = 2 and submodular costs C. In fact, in light of the equivalence between MOTC
and the Lov´asz extension in that special case (described in §6.2), this is arguably the appropriate
generalization thereof to optimizing general discrete functions over general ground sets of size n ⩾ 2.

3.2 Proofs

Proof of Lemma 3.3. The convex envelope F of f is equal to the Fenchel bi-conjugate f ∗∗ of f [35].
The Fenchel conjugate of f is f ∗ ∶ Rnk → R where f ∗
−
Cφ−1(x). Thus the Fenchel bi-conjugate f ∗∗ is
− f ∗

x, y + p
⟨

= maxx∈S

= maxx∈S

y, µ − x

x, y
⟨

x
(

y
(

− f

y, µ

= f ∗∗

= max

F

−

)

)

⟩

⟩

+ Cφ−1(x).

µ
(

)

µ
(

)

y∈Rnk⟨

⟩

y
(

)

= max
y∈Rnk

min
x∈S ⟨

x, p
⟨

⟩

⟩

By performing a convex relaxation over the inner minimization (to distributions D over the set S)
and then invoking LP strong duality, we obtain

F

µ
(

)

= min
D∈∆S

y, µ − Ex∼Dx

max
y∈Rnk⟨

−

Ex∼Dx, p
⟨

⟩

⟩

+ Ex∼DCφ−1(x)

Note that the inner maximization over y has unbounded cost +∞ unless Ex∼Dx = µ. Thus

F

µ
(

)

= −

+

µ, p
⟨

⟩

min
D∈∆S s.t. Ex∼Dx=µ

Ex∼DCφ−1(x),

This proves (3.1) by deﬁnition of f . Now (3.2) follows since distributions D over S with expectation
µ are in correspondence with joint distributions P ∈ M
, and under this correspondence
)
Ex∼DCφ−1(x) simply amounts to

µ1, . . . , µk
(

P, C
⟨

.
⟩

Corollary 3.5 (Minimizing F suﬃces for minimizing f ). The minimum value of F over
equal to the minimum value of f over S.

∆n
(

k is
)

Proof. By the Choquet representation (3.1) of F in Lemma 3.3, the set of minimizers of F over
∆n
(

k is equal to the convex hull of the minimizers of f over S.
)

7

3.2.1 Hardness of computation

)

∆n
(

Proof of Theorem 3.1. By Corollary 3.5, it suﬃces to minimize F over

k in the desired runtime.
)
To this end, we claim that F is the maximum of a ﬁnite number of linear functions, each of which
has polynomial encoding length in the sense of [23, §6.5]. To show this statement, it suﬃces to show
the same statement for the function µ ↦ MOTC
by the representation (3.2) of F in Lemma 3.3
µ
(
that equates F to a linear function plus MOTC. This latter statement follows by the dual MOT
formulation (MOT-D) and a standard LP argument. Speciﬁcally, the function µ ↦ MOTC
is
a linear function in µ with ﬁnitely many pieces, one for each vertex of the polyhedral feasible set
deﬁning (MOT-D) by the Minkowski-Weyl Theorem; and furthermore, the vertices are solutions to
linear systems in the constraints, and thus have polynomial bit complexity by Cramer’s Theorem.
k is a “well-described polyhedron” in the sense of [23, Deﬁnition 6.2.2],
)
we may apply the Ellipsoid algorithm in [23, Theorem 6.5.19]. That theorem shows that F can
k using polynomially many evaluations of F and polynomial additional
be minimized over
)
processing time. By appealing again to the representation (3.2) of F in Lemma 3.3, each evaluation
of F can be performed via a single MOTC computation and polynomial additional processing
time.

Therefore, since

∆n
(

∆n
(

µ
(

)

3.2.2 Hardness of approximation

Proof of Theorem 3.2. By Corollary 3.5, it suﬃces to compute the minimum value of F over
to additive accuracy Θ
computing

k
)
. By the representation of F in (3.2), this amounts to approximately
)

εnk
(

∆n
(

min
µ∈(∆n)k

MOTC

−

µ
(

)

µ, p
⟨

.
⟩

(3.3)

to that accuracy. Note that a query to the oracle computing MOTC to ε accuracy (plus polynomial-
time additional computation) computes the objective function in (3.3) to ε additive accuracy.
Therefore, this is an instance of the zero-th order optimization problem for approximately convex
functions studied in [5, 34]. The claimed runtime and approximation accuracy follow from their
results once we check that F has polynomial Lipschitz parameter, done next (we give a tighter
bound than needed since it may be of independent interest).

Lemma 3.6 (ℓ1-Lipschitzness of MOTC w.r.t. marginals). The function µ ↦ MOTC
is Lipschitz with respect to the entrywise ℓ1 norm with parameter 2Cmax.

µ
(

)

on

∆n
(

k
)

Proof. Let µ, µ′ ∈

∆n
(

k. By symmetry, it suﬃces to show that
)

min

P ′∈M(µ′

1,...,µ′

k)⟨

P ′, C

⩽

⟩

min
P ∈M(µ1,...,µk)⟨

P, C

⟩

+ 2Cmax

µ − µ′
∥

1.
∥

Let P ∗ be an optimal solution for the optimization over M
in [27], there exists ˆP ∈ M
µ′
1, . . . , µ′
(
Thus

k)

such that the entrywise ℓ1 norm

µ1, . . . , µk
(

. By the rounding algorithm
)
1.
∥

µ − µ′
1 ⩽ 2
∥
∥

ˆP − P ∗
∥

min

P ′∈M(µ′

1,...,µ′

k)⟨

P ′, C

⩽

ˆP , C
⟨

⟩

=

P ∗, C
⟨

⟩

+

ˆP − P ∗, C
⟨

.
⟩

⟩

By construction of P ∗, the ﬁrst term
P ∗, C
⟨
the construction of ˆP , the second term

⟩
ˆP − P ∗, C
⟨

= minP ∈M(µ1,...,µk)⟨
ˆP − P ∗
⩽ Cmax
∥

⟩

P, C

. By H¨older’s inequality and
⟩

1 ⩽ 2Cmax
∥

µ − µ′
∥

1.
∥

8

4 Application: costs with super-constant rank

Recent work has given a polynomial time algorithm for approximate MOT when the cost is a
constant-rank tensor given in factored form [2]. A natural algorithmic question is whether the
dependence on the rank can be improved: is there an algorithm whose runtime is simultaneously
polynomial in n, k, and the rank r? Here we show that, under standard complexity theory as-
sumptions, the answer is no. Our result provides a converse to [2], and justiﬁes the constant-rank
regime studied in [2].
Proposition 4.1 (Hardness of MOT for low-rank costs). Assuming P ≠ NP, there does not ex-
-time deterministic algorithm for solving MOTC for costs C given by a rank-r
ist a poly
)
factorization.

n, k, r
(

Our impossibility result further extends to approximate computation.

Proposition 4.2 (Hardness of approximate MOT for low-rank costs). Assuming NP
n, k, r, Cmax
does not exist a poly
ε )
(
accuracy for costs C given by a rank-r factorization.

⊂ BPP, there
/
-time randomized algorithm for approximating MOTC to ε additive

The proof encodes the hard problem of ﬁnding a large clique in a k-partite graph as an instance
of MOTC in which C has an explicit low-rank factorization. We deﬁne the following notation: for
⊗k denote the tensor
a k-partite graph G on nk vertices vi,j for i ∈
k
)
[
-th entry equal to the number of edges in the induced subgraph of G with vertices
j1, . . . , jk
with
)
(
v1,j1, . . . , vk,jk}
.
{
Lemma 4.3 (TG is low-rank). For any k-partite graph G on nk vertices, rank
TG
(
)
over, a factorization of TG with this rank is computable from G in poly
time.

, let TG ∈
]

⩽ n2k2. More-

and j ∈

Rn
(

n
[

]

between partitions i, i′ ∈

vi,ji, vi′,ji′
(

. Consider the rank-1 tensor
Proof. Consider an edge
]
formed by the outer product of the indicator vectors eji and eji′ on respective slices i and i′, and
∖
i, i′
the all-ones vector 1n on all other slices ℓ ∈
. This tensor takes value 1 on all tuples
}
{
k with i-th coordinate ji and i′-th coordinate ji′, and takes value 0 elsewhere. Summing
in
]
2—yields the desired
up such a rank-1 tensor for each edge of G—of which there are at most
)
factorization.

nk
(

n
[

k
[

k
[

)

]

n, k
(

)

n, k, r, Cmax
ε )
(

Lemma 4.4 (Hardness of MIN for low-rank costs). Assuming P ≠ NP, there is no poly
-time
)
deterministic algorithm for solving MINC for costs C given by a rank-r factorization. Moreover, as-
⊂ BPP, there is poly
suming NP
-time randomized algorithm for ε-approximate additive
/
computation.
Proof. Deciding whether there exists a k-clique in a k-partite graph G on nk vertices is NP-hard.
This problem reduces to computing the maximal entry in TG, which is equivalent to solving MINC
0
)
(
for C = −TG. The ﬁrst statement then follows since a low-rank factorization of −TG can be found
time by Lemma 4.3. For the second statement, note that since the entries of −TG are
in poly
integral, it is also NP-hard to solve MINC
to additive error Cmax

10k2 ⩽ 0.1.

n, k, r
(

n, k
(

)

0
)
(

/

Proof of Proposition 4.1. By Theorem 3.1, a poly
on rank-r costs implies a poly
Assuming P ≠ NP, this contradicts Lemma 4.4.

n, k, r
(

-time deterministic algorithm for MOTC
)
-time deterministic algorithm for MINC on rank-r costs.
)

n, k, r
(

Proof of Proposition 4.2. By Theorem 3.2, a poly
on rank-r costs C implies a poly
C. Assuming NP

n, k, r, Cmax
ε )
(

⊂ BPP, this contradicts Lemma 4.4.
/

n, k, r, Cmax
ε )
(

-time randomized algorithm for MOTC

-time randomized algorithm for MINC on rank-r costs

9

5 Application: costs with full pairwise interactions

Many studied MOT costs, such as the Wasserstein barycenter cost and Coulomb cost, have the
following structure: they decompose into a sum of pairwise interactions, as

Cj1,...,jk = ∑

1⩽i<i′⩽k

gi,i′

ji, ji′
(

)

(5.1)

for some functions gi,i′ ∶
→ R. This decomposability structure allows for a polynomial-size
implicit representation of the cost tensor. It is a natural question whether this generic structure
can be exploited to obtain polynomial-time algorithms for MOT. We show that the answer is no:
there are MOT costs that are decomposable into pairwise interactions, but are NP-hard to solve.

n
[

n
[

×

]

]

Proposition 5.1 (Hardness of MOT for pairwise-decomposable costs). Assuming P ≠ NP, there
-time deterministic algorithm for solving MOTC for costs C of the
does not exist a poly
)
form (5.1).

n, k
(

Our impossibility result further extends to approximate computation.

Proposition 5.2 (Hardness of approximate MOT for pairwise-decomposable costs). Assuming
NP
-time randomized algorithm for approximating
MOTC to ε additive accuracy for costs C of the form (5.1).

⊂ BPP, there does not exist a poly
/

n, k, Cmax
ε )
(

Proof of Propositions 5.1 and 5.2. The proofs of Propositions 5.1 and 5.2 are the same as the proofs
, the tensor −TG can be
of Propositions 4.1 and 4.2 using the fact that for any graph G =
)
written as a sum of pairwise interactions:
.
]

j1,...,jk = ∑1⩽i<i′⩽k
)

V, E
(
−1
[(

vi,ji, vi′,ji′

−TG
(

∈ E

)

Propositions 5.1 and 5.2 provides converses to the result of [2]. Speciﬁcally, [2, §4], considers
MOT costs C that decompose into local interactions as Cj1,...,jk = ∑S∈S gS
, and gives a
)
∶ i, i′ ∈
polynomial-time algorithm in the case that the graph with vertices
{(
S for some S ∈ S
has constant treewidth. Conversely, our hardness results in Propositions 4.1
and 4.2 show that bounded treewidth is necessary for polynomial-time algorithms. This is because
costs of the form (5.1) fall under the framework of decomposable costs in [2] with non-constant
treewidth of size k − 1.

ji
i∈S
}
and edges

k
[

i, i′

({

}

)

]

6 Application: repulsive costs

In this section, we investigate several MOT problems with repulsive costs that are of interest in the
literature. We prove intractability results that clarify why—despite a growing literature (see, e.g.,
the survey [17] and the references within)—these problems have resisted algorithmic progress.

6.1 Determinantal cost

A repulsive cost of interest in the MOT literature is the determinant cost (e.g., [13, 17]). This cost
is given by:

Cj1,...,jk = −

(6.1)
where x1, . . . , xn ∈ Rk and det
is the determinant of the k × k matrix whose columns
xj1, . . . , xjk )
(
are xj1, . . . , xjk . This is a repulsive cost in the sense that tuples with “similar” vectors are penalized
with higher cost, see the survey [17]. We prove that the MOT problem with this cost is NP-hard.
For convenience of notation, we think of the marginal distributions µ1, . . . , µk as distributions in
the simplex ∆n, and write

,
xj1, . . . , xjk )∣
(

det

∣

µi
[

j to mean the mass of µi on xj.
]

10

Proposition 6.1 (Hardness of MOT with determinant cost). Assuming P ≠ NP, then there is no
-time algorithm that given x1, . . . , xn ∈ Rk solves MOTC for the cost C in (6.1).
poly
)

n, k
(

Proof. By Theorem 3.1, it suﬃces to prove that the MINC problem is NP-hard. We show this is
in this case the MINC problem is to compute
true even if the input weights p are identically 0:
min⃗j C⃗j = − max⃗j ∣

given x1, . . . , xn ∈ Rk. This is NP-hard by [29].

xj1, . . . , xjk )∣
(

det

Rather than show additive inapproximability of MOT with determinant costs, we consider log-
determinant costs since additive error on the logarithmic scale amounts to multiplicative error on the
natural scale, which is more standard in the combinatorial-optimization literature on determinant
maximization. Below, we show inapproximability of MOT with such log-determinant costs. Note
that for technical reasons we upper-bound the cost at 0 to avoid unbounded costs for tuples with
null determinant:

Cj1,...,jk = min

0, − log
(

det

.
xj1, . . . , xjk )∣)
(

∣

(6.2)

Proposition 6.2 (Approximation hardness of MOT with log-determinant cost). Assuming NP
BPP, then there is no poly
MOTC to ε additive accuracy for the cost C in (6.2).

⊂
/
time algorithm that given x1, . . . , xn ∈ Rk approximates

n, k, Cmax
(

ε
)

/

∣

)

− log

n, k
(

xj1, . . . , xjk )∣
(

Proof. Let x1, . . . , xn ∈ Zk have poly
bits each. It is known to be NP-hard to approximate
to within additive error 0.0001 [36, Theorem 3.2]. Since x1, . . . , xn
det
minj1,...,jk
span Rk without loss of generality, this is equivalent to approximating minj1,...,jk Cj1,...,jk to within
additive error 0.0001. But by Theorem 3.2, given access to MOTC computations with additive
= minj1,...,jk Cj1,...,jk to within additive
accuracy Cmax
poly
error 0.0001 in poly
size here. Hence, assuming
BPP

, we can approximate MINC
)
/
randomized time since Cmax is of poly
)
⊂ NP there is no poly
n, k, Cmax
(
/

n, k
(
-time algorithm that solves MOTC to accuracy ε.
ε
)

n, k
(
n, k
(

0
)
(

)

/

6.2 Supermodular cost

We now consider MOT problems given by discrete functions that are either submodular or super-
modular (see, e.g., [20] for deﬁnitions). Speciﬁcally, consider an MOT problem with n = 2 and a
k → R that is submodular or supermodular.5 Since
cost C ∶
k corresponds to the power
0, 1
0, 1
}
{
}
{
. The MOT
k
, the cost C can be equivalently viewed as a set function on subsets S ⊆
k
set of
]
[
]
[
problem is of the form

min
P ∈M(Ber(x1),...,Ber(xk))

ES∼P C

S
(

)

(6.3)

k
[

0, 1
]
[

dictate the marginals µ1, . . . , µk, and Ber

where x1, . . . , xk ∈
denotes a Bernoulli distribu-
tion taking value 1 with probability p. In words, (6.3) is an optimization problem over distributions
P on subsets of

, where the linear cost is the expected value of C with respect to P .
]

We prove a dichotomy for MOT problems with these costs: MOT is polynomial-time solvable for
general submodular costs, but is intractable for general supermodular costs. This aligns with our
message that repulsive structure is a source of intractability in MOT, since submodular costs are
a prototypical example of “attractive” costs, whereas supermodular costs are often used to model
“repulsive” costs (see, e.g., [11, 33]).

p
(

)

Proposition 6.3. Consider a function C ∶
k → R given through oracle access for evaluation,
0, 1
}
{
0, 1
and marginal probabilities x1, . . . , xk ∈
.
]
[

Here, we index the marginals of the cost tensor C with the ground set {0, 1}, instead of {1, 2} as we would in the

5

rest of the paper, to match the notational convention for supermodular/submodular functions.

11

• If C is supermodular, then, assuming P ≠ NP, there is no poly

Moreover, assuming NP
ε-approximation.

⊂ BPP, there is no poly
/

k, Cmax
(

/

k
(

-time algorithm for MOTC.
)
ε
-time algorithm for computing an
)

• If C is submodular, then MOTC is solvable in poly

time.

k
(

)

)

S
(

0
)
(

0
)
(

= minS⊆{0,1}k C

is hard to approximate to ±0.49 error. Theorem 3.1 reduces computing MINC

Proof. To show the intractability of computing the MOTC problem (6.3) with supermodular costs,
consider the case in which C is the supermodular function encoding the NP-hard Max-Cut problem
In this case, C is integer-valued, so Max-Cut
for a graph on k vertices (refer to e.g., [18]).
to within, say, ±0.49 additive error. Thus,
reduces to approximating MINC
MINC
to exactly
computing MOTC, hence exact computation of MOTC is also NP-hard. Furthermore, Theorem 3.2
± 0.49 to 1
uses a randomized algorithm to reduce computing MINC
-approximating
poly
/
)
MOTC, since the range of C is bounded by Cmax ⩽ k2. Therefore, if NP
⊂ BPP, then there is no
/
poly

0
)
(
-time algorithm for ε-approximation of MOTC.
ε
)

k, Cmax
(
On the other hand, if C is submodular, then the problem is tractable. The proof hinges on
is equivalent to the
k → R of C at the point x =
0, 1
]
[
evaluations of C and
)
additional processing time by leveraging the equivalence of F to the Lov´asz extension of

the observation that the MOTC problem (6.3) with marginals µi = Ber
LP characterization of evaluating the convex envelope F ∶
x1, . . . , xk
(
k log k
O
(
C [23, §10.1].

[23, §10.1]. If C is submodular, then F can be evaluated in O

0
)
(

xi
(

k
(

k
(

)

)

)

/

6.3 Application to Density Functional Theory

A popular application of MOT is to formulate a relaxation of the Density Functional Theory prob-
lem (DFT) from quantum chemistry. We refer the reader to [15] for an introduction of the MOT
formulation of DFT, and sketch the simplest case below. In the simplest version of the MOT relax-
ation, we are given k distributions corresponding to k electron clouds in space, and the objective
is to couple the electron clouds in a way that minimizes the expected potential energy of the elec-
tron conﬁguration. Suppose the electron clouds are given as distributions µ1, . . . , µk supported on
x1, . . . , xn ∈ R3; again, for convenience of notation, we think of µ1, . . . , µk as distributions in the
j to mean the mass of µi on xj. Then, the MOT relaxation of DFT is
simplex ∆n, and write
]
to compute a minimum-cost coupling of µ1, . . . , µk, with cost given by the Coulomb potential

µi
[

Cj1,...,jk = ∑

1⩽i<i′⩽k

1
− xji′

.

2
∥

xji
∥

(6.4)

k such that xj1, . . . , xjn are spread as
This is a repulsive cost that encourages tuples
]
far as possible, since the Coulomb potential decreases as two electrons move farther apart. Despite
signiﬁcant algorithmic interest, provable polynomial-time algorithms have not yet been found. We
conjecture that in fact solving MOT with the Coulomb potential is NP-hard.

j1, . . . , jn
(

n
[

)

∈

Conjecture 6.4. Assuming P ≠ NP, there is no poly
Coulomb potential cost (6.4).

n, k
(

-time algorithm solving MOTC with the
)

In this section, we make progress towards the conjecture by proving hardness of DFT with the
related Coulomb-Buckingham potential, which is similar to the Coulomb potential, but has extra
energy terms that grow as 1
. The Coulomb-Buckingham potential is popular
/
for modeling the structures of ionic crystals [1], and is deﬁned for two particles at distance r with

r6 and exp

−Θ
(

r
(

))

12

charges q1, q2 ∈

−1, +1
}
{

as:

U

r, q1, q2
(

)

=

M,

Aq1q2
exp(Bq1 q2 r)

− Cq1q2

r6 + q1q2
r ,

r = 0
r > 0

,

⎧⎪⎪
⎨
⎪⎪⎩

where A+1, A−1, B+1, B−1, C+1, C−1 are constants determining the relative strengths of the terms in
the interaction, and M > 0 is a large constant (that should be intuitively thought of as inﬁnite)
penalizing two ions being in the same place. Given ions with charges qj ∈
at positions
xj ∈ R3, the corresponding MOT cost is given by:

−1, +1
}
{

Cj1,...,jk =

M,
⎧⎪⎪
∑1⩽i<i′⩽k U
⎨
⎪⎪⎩

− xji′

xji

(∥

2, qji, qji′
∥

∑i∈
, ∑i∈
)

k
[
k
[

]

]

qji ≠ 0
qji = 0

.

(6.5)

Proposition 6.5 (Hardness of DFT with Coulomb-Buckingham potential). Assuming P ≠ NP,
-time algorithm that, given positions x1, . . . , xn ∈ R3, charges q1, . . . , qn ∈
then there is no poly
)
−1, +1
, parameters A±1, B±1, C±1, M > 0, and marginals µ1, . . . , µk ∈ ∆n, solves MOTC with cost
{
}
C given by (6.5).

n, k
(

xj − xj′
∥

Proof. For the proof, we show NP-hardness even if the inputs x1, . . . , xn ∈ R3 are such that
⩽ M ⩽
n, k
min1⩽j′⩽j⩽n
(
poly
, so by Theorem 3.1 and
)
3.2, it suﬃces to show that computing MINC

. In the parameter regime above, we have Cmax = M ⩽ poly
)

n, k
(
Furthermore, in the parameter regime above, MINC

2 + A+1 + A−1 + C+1 + C−1
(

2 ⩾ 1, A±1, B±1, C±1 ⩽ poly
∥

, and 2k2
)

is NP-hard.

n, k
(

0
)
(

)

is equal to the following:

0
)
(

min ⎧⎪⎪
⎨
⎪⎪⎩

1
2

∑

j∈S,j′∈S∖
j
{

U

}

xj − xj′

(∥

2, qj, qj′
∥

)

∶ S ⊂

n
[

,
]

S
∣

∣

= k, ∑
j∈S

This optimization problem is NP-hard by [1, Theorem 5].

qj = 0⎫⎪⎪
⎬
⎪⎪⎭

(6.6)

A similar hardness result also holds for approximate computation, stated next.

Proposition 6.6 (Approximation hardness of DFT with Coulomb-Buckingham potential). If
NP
-time algorithm computing an ε-additive approximation to
ε
)
MOTC, where C is as in Proposition 6.5.

⊂ BPP, there is no poly
/

n, k, Cmax
(

/

The proof of Proposition 6.6 is identical to the proof of Proposition 6.5 once we show that the
MINC
problem is hard to solve approximately. While [1, Theorem 5] only shows hardness of
exactly computing MINC
, a slightly more careful analysis extends this hardness to approximate
0
)
(
computation; details are deferred to the appendix.

0
)
(

7 Neccesity of dual weights

This section ﬂeshes out the details for Remark 1.5. Namely, in §4, §5, and §6, we showed that
MOTC was hard to compute for some family of costs C by proving that MINC
was hard to
compute. Here, we show that such arguments do not use the full power of Theorems 3.1 and 3.2:
we construct a family of cost tensors C for which MOTC is NP-hard to compute yet MINC
is
polynomial-time computable.

0
)
(

0
)
(

The cost family is as follows: given a 2-SAT formula φ ∶

0, 1
, deﬁne
}
{

k →
0, 1
{
}
.
)

(7.1)

Cj1,...,jk = −φ
j1, . . . , jk
(

13

Proposition 7.1. Given a 2-SAT formula φ, it is NP-hard to solve MOTC for the cost (7.1).
However, MINC

can be computed in polynomial time.

0
)
(

0
)
(

= minj1,...,jk Cj1,...,jk = − maxj1,...,jk φ
j1, . . . , jk
(

Proof. Observe that MINC
problem for φ, which can be solved in polynomial-time since φ is a 2-SAT formula [24].
0, −1
∈ R2×k be given by p1 = p2 = ⋅ ⋅ ⋅ = pk =
(
−
j
j1, . . . , jk
maxj1,...,jk φ
1
∥⃗
∥
(
[

∈ R2.
ji = −
Then MINC
pi
p
solves
]
(
)
the problem of ﬁnding the minimum weight of a satisfying assignment to φ. This problem is
NP-hard [24], hence MINC

On the other hand, let p =
p1, . . . , pk
/(
(
−φ
j1, . . . , jk
/(
(
is NP-hard. Therefore MOTC is NP-hard by Theorem 3.1.

is the satisﬁability

= minj1,...,jk

2k
2k

)
− ∑k

))
)]

i=1[

)

)

)

p
(

)

Acknowledgements. We are grateful to Pablo Parrilo, Philippe Rigollet, and Kunal Talwar for
insightful conversations.

References

[1] D. Adamson, A. Deligkas, V. V. Gusev, and I. Potapov. On the hardness of energy minimisation for
crystal structure prediction. In International Conference on Current Trends in Theory and Practice of
Informatics, pages 587–596. Springer, 2020.

[2] J. M. Altschuler and E. Boix-Adser`a. Polynomial-time algorithms for Multimarginal Optimal Transport

problems with decomposability structure. arXiv pre-print arXiv:2008.03006, 2020.

[3] J. M. Altschuler and E. Boix-Adser`a. Wasserstein barycenters can be computed in polynomial time in

ﬁxed dimension. arXiv pre-print arXiv:2006.08012, 2020.

[4] E. Anderes, S. Borgwardt, and J. Miller. Discrete Wasserstein barycenters: Optimal transport for

discrete data. Mathematical Methods of Operations Research, 84(2):389–409, 2016.

[5] A. Belloni, T. Liang, H. Narayanan, and A. Rakhlin. Escaping the local minima via simulated annealing:
optimization of approximately convex functions. In Conference on Learning Theory, pages 240–265,
2015.

[6] J.-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyr´e.

Iterative Bregman projections for
regularized transportation problems. SIAM Journal on Scientiﬁc Computing, 37(2):A1111–A1138, 2015.

[7] J.-D. Benamou, G. Carlier, S. Di Marino, and L. Nenna. An entropy minimization approach to second-

order variational mean-ﬁeld games. arXiv preprint arXiv:1807.09078, 2018.

[8] J.-D. Benamou, G. Carlier, and L. Nenna. A numerical method to solve multi-marginal optimal trans-
In Splitting Methods in Communication, Imaging, Science, and

port problems with Coulomb cost.
Engineering, pages 577–601. Springer, 2016.

[9] J.-D. Benamou, G. Carlier, and L. Nenna. Generalized incompressible ﬂows, multi-marginal transport

and Sinkhorn algorithm. Numerische Mathematik, 142(1):33–54, 2019.

[10] S. Borgwardt and S. Patterson. On the computational complexity of ﬁnding a sparse Wasserstein

barycenter. arXiv preprint arXiv:1910.07568, 2019.

[11] A. Borodin, H. C. Lee, and Y. Ye. Max-sum diversiﬁcation, monotone submodular functions and

dynamic updates. In Symposium on Principles of Database Systems, pages 155–166, 2012.

[12] G. Buttazzo, L. De Pascale, and P. Gori-Giorgi. Optimal-transport formulation of electronic density-

functional theory. Physical Review A, 85(6):062502, 2012.

[13] G. Carlier and B. Nazaret. Optimal transportation for the determinant. ESAIM: Control, Optimisation

and Calculus of Variations, 14(4):678–698, 2008.

14

[14] G. Carlier, A. Oberman, and E. Oudet. Numerical methods for matching for teams and Wasserstein

barycenters. ESAIM: Mathematical Modelling and Numerical Analysis, 49(6):1621–1642, 2015.

[15] C. Cotar, G. Friesecke, and C. Kl¨uppelberg. Density functional theory and optimal transportation with

Coulomb cost. Communications on Pure and Applied Mathematics, 66(4):548–599, 2013.

[16] J. A. De Loera and E. D. Kim. Combinatorics and geometry of transportation polytopes: an update.

Discrete geometry and algebraic combinatorics, 625:37–76, 2014.

[17] S. Di Marino, A. Gerolin, and L. Nenna. Optimal transportation theory with repulsive costs. Topological

optimization and optimal transport, 17:204–256, 2017.

[18] U. Feige, V. S. Mirrokni, and J. Vondr´ak. Maximizing non-monotone submodular functions. SIAM

Journal on Computing, 40(4):1133–1153, 2011.

[19] S. Friedland. Tensor optimal transport, distance between sets of measures and tensor scaling. arXiv

pre-print arXiv:2005.00945, 2020.

[20] S. Fujishige. Submodular functions and optimization. Elsevier, 2005.

[21] M. R. Garey and D. S. Johnson. Computers and intractability, volume 174.

[22] M. Gr¨otschel, L. Lov´asz, and A. Schrijver. The ellipsoid method and its consequences in combinatorial

optimization. Combinatorica, 1(2):169–197, 1981.

[23] M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric algorithms and combinatorial optimization, vol-

ume 2. Springer Science & Business Media, 2012.

[24] D. Gusﬁeld and L. Pitt. A bounded approximation for the minimum cost 2-SAT problem. Algorithmica,

8(1-6):103–117, 1992.

[25] I. Haasler, A. Ringh, Y. Chen, and J. Karlsson. Multi-marginal optimal transport and Schr¨odinger

bridges on trees. arXiv preprint arXiv:2004.06909, 2020.

[26] R. M. Karp. Reducibility among combinatorial problems. In Complexity of computer computations,

pages 85–103. Springer, 1972.

[27] T. Lin, N. Ho, M. Cuturi, and M. I. Jordan. On the complexity of approximating multimarginal optimal

transport. arXiv preprint arXiv:1910.00152, 2019.

[28] L. Nenna. Numerical methods for multi-marginal optimal transportation. PhD thesis, 2016.

[29] C. H. Papadimitriou. The largest subdeterminant of a matrix. Bulletin of the Greek Mathematical

Society, 25(25):95–105, 1984.

[30] C. H. Papadimitriou and T. Roughgarden. Computing correlated equilibria in multi-player games.

Journal of the ACM (JACM), 55(3):1–29, 2008.

[31] B. Pass. Multi-marginal optimal transport: theory and applications. ESAIM: Mathematical Modelling

and Numerical Analysis, 49(6):1771–1790, 2015.

[32] G. Peyr´e and M. Cuturi. Computational optimal transport. Foundations and Trends in Machine

Learning, 2017.

[33] A. Prasad, S. Jegelka, and D. Batra. Submodular meets structured: Finding diverse subsets in
exponentially-large structured item sets. In Advances in Neural Information Processing Systems, pages
2645–2653, 2014.

[34] A. Risteski and Y. Li. Algorithms and matching lower bounds for approximately-convex optimization.

In Advances in Neural Information Processing Systems, pages 4745–4753, 2016.

15

[35] R. T. Rockafellar. Convex analysis. Number 28. Princeton University Press, 1970.

[36] M. D. Summa, F. Eisenbrand, Y. Faenza, and C. Moldenhauer. On largest volume simplices and

sub-determinants. In Symposium on Discrete algorithms, pages 315–323. SIAM, 2014.

[37] N. Tupitsa, P. Dvurechensky, A. Gasnikov, and C. A. Uribe. Multimarginal optimal transport by

accelerated alternating minimization. arXiv pre-print arXiv:2004.02294, 2020.

[38] M. J. Wainwright and M. I. Jordan. Variational inference in graphical models: The view from the
marginal polytope. In Allerton Conference on Communication, Control, and Computing, volume 41,
pages 961–971, 2003.

[39] V. Yemelicher, M. M. Kovalev, M. Dravtsov, and G. Lawden. Polytopes, graphs and optimisation.

Cambridge University Press, 1984.

A Proof of Proposition 6.6

Proof of Proposition 6.6. By Theorem 3.2, it suﬃces to prove that approximating MINC
⩽ Cmax
poly
suﬃces to prove that

up to
additive error is NP-hard. By the reasoning in the proof of Proposition 6.5, it

n, k
(

0
)
(

)

/

min

1
2

⎧⎪⎪
⎨
⎪⎪⎩
poly

∑

j∈S,j′∈S∖
j
{

xj − xj′

(∥

2, qj, qj′
∥

)

∶ S ⊂

n
[

,
]

S
∣

∣

= k, ∑
j∈S

U

}

⎫⎪⎪
qj = 0
⎬
⎪⎪⎭

(A.1)

/

n, k
(

n, k
(

n10 in the sense that for large enough n,

is NP-hard to Cmax
-approximate. We modify the proof of [1, Theorem 5] to prove
)
this. Note that for the parameters A±, B±, C± chosen in Lemma 1 of [1, Theorem 5], we have
Cmax = poly
, and also the inequalities (1)-(3) of [1] are met with a small polynomial gap of
)
at least 1
/
− C+
+ 1
n6
n
− C+
+ 1
r6
r
+ 1
r

A−
eB−√1+n2
A−
eB−√1+r2
A−
−
eB−√1+r2

1
√1 + n2
1
√1 + r2 ∣
1
√1 + r2

A+
eB+r
− C+
r6

C−
1 + n2
(
−

− C− − 1
∣

− C− − 1
∣

3
)
C−
1 + r2
(
C−
1 + r2
(

∣
A+
eB+r

A+
eB+n

r ⩾ √2n.

r ⩾ √2n

− 1
/

+ 1
/

A−
eB−

A−
eB−

3
)
−

> 1
/

n10,

n10,

n10

3
)

n2

−

−

+

+

+

−

⩽

⩾

∣

∣

Tracing through the reasoning of Lemmas 2, 3, and 4 of [1], this gap implies that a ±0.49
n10
/
approximation to the objective (A.1) suﬃces to determine whether or not the construction in [1,
Theorem 5] encodes a graph with an independent set of size k
2. This proves NP-hardness of
Cmax

approximation for (A.1).

poly

/

n, k
(

)

/

16

