1
2
0
2

c
e
D
8

]

G
L
.
s
c
[

1
v
3
2
1
4
0
.
2
1
1
2
:
v
i
X
r
a

ShinRL: A Library for Evaluating RL Algorithms
from Theoretical and Practical Perspectives

Toshinori Kitamura‚àó
Nara Institute of Science and Technology
Nara, Japan
kitamura.toshinori.kt6@is.naist.jp

Ryo Yonetani
OMRON SINIC X
Tokyo, Japan
ryo.yonetani@sinicx.com

Abstract

We present ShinRL, an open-source library specialized for the evaluation of reinforcement
learning (RL) algorithms from both theoretical and practical perspectives1. Existing RL libraries
typically allow users to evaluate practical performances of deep RL algorithms through returns.
Nevertheless, these libraries are not necessarily useful for analyzing if the algorithms perform
as theoretically expected, such as if Q learning really achieves the optimal Q function. In
contrast, ShinRL provides an RL environment interface that can compute metrics for delving
into the behaviors of RL algorithms, such as the gap between learned and the optimal Q
values and state visitation frequencies. In addition, we introduce a Ô¨Çexible solver interface
for evaluating both theoretically justiÔ¨Åed algorithms (e.g., dynamic programming and tabular
RL) and practically effective ones (i.e., deep RL, typically with some additional extensions
and regularizations) in a consistent fashion. As a case study, we show that how combining
these two features of ShinRL makes it easier to analyze the behavior of deep Q learning.
Furthermore, we demonstrate that ShinRL can be used to empirically validate recent theoretical
Ô¨Åndings such as the effect of KL regularization for value iteration [Kozuno et al., 2019] and for
deep Q learning [Vieillard et al., 2020a], and the robustness of entropy-regularized policies to
adversarial rewards [Husain et al., 2021]. The source code for ShinRL is available on GitHub:
https://github.com/omron-sinicx/ShinRL.

1

Introduction

Reinforcement learning (RL) [Sutton and Barto, 2018] has historically been, and still is, a very
active topic in machine learning research. Recent years have particularly seen remarkable progress
in research on deep RL, where highly-expressive neural networks are used to approximate policy
or Q functions to enable complex sequential decision making. Due to its advantages in dealing
with high-dimensional state spaces and learning policies that are generalizable to unseen testing
environments, the effectiveness of deep RL has been conÔ¨Årmed in a variety of practical applications,
such as robot control [Kober et al., 2013], game AI [Mnih et al., 2015], and economics [Zheng et al.,
2020], to name a few.

In parallel with research on deep RL for practical tasks, there has been increasing attention paid
to efforts to clarify its theoretical basis. Indeed, some state-of-the-art deep RL algorithms can be
viewed as an extension of the theoretical foundations of RL such as tabular RL (i.e., no function
approximation) and dynamic programming (DP; no exploration while assuming that the complete
speciÔ¨Åcation about state transitions is given). Concrete examples of such correspondences between
theoretically justiÔ¨Åed and practically effective algorithms include: from soft Q learning [Haarnoja
et al., 2017] to soft actor-critic (SAC) [Haarnoja et al., 2018], from safe policy iteration [Pirotta et al.,

‚àóWork done as an intern at OMRON SINIC X.
1TK devised the main conceptual idea, developed the library, and conducted all the experiments presented in

the paper. RY aided in shaping the research and worked in collaboration with TK to write the manuscript.

 
 
 
 
 
 
Figure 1: Analyzing DQL Results on MountainCar using ShinRL. (a) Return plot; (b1) Visual-
ization of the optimal Q values, (b2) learned Q values, and (c) state visitation frequency.

2013] to trust-region policy optimization (TRPO) [Schulman et al., 2015], and from conservative
value iteration (CVI) [Kozuno et al., 2019] to Munchausen Deep Q learning [Vieillard et al., 2020a].
In order to better understand a new deep RL algorithm that has been developed, it is critical to
identify its theoretical foundation and validate if it works theoretically as expected, typically under a
reasonably simpliÔ¨Åed setting.

To this end, one indispensable contribution is the development of open-source libraries that can
be used to evaluate RL algorithms from both theoretical and practical perspectives in a principled
fashion. Despite many RL libraries have been developed so far [Fujita et al., 2021, Castro et al.,
2018, Liang et al., 2018], they typically support evaluations of deep RL algorithms only through
returns (i.e., cumulative rewards) sampled from episodes. While such evaluations could allow users to
systematically compare performances across methods (e.g., Engstrom et al. [2020], Ceron and Castro
[2021]), sampled returns are not necessarily useful for assessing if the methods work as theoretically
expected. As a motivating example, suppose a scenario where a user performs a deep Q learning
(DQL) [Mnih et al., 2015] on the MountainCar environment [Brockman et al., 2016]. The user can
utilize existing libraries to implement such experiments easily and conÔ¨Årm that the trained network
received a high return as shown in Fig. 1(a). However, does this empirical success mean that the
network really achieved the optimal Q function? This is a simple but fundamental question to validate
the theoretical expectation that the original (i.e., tabular) Q learning has, which is nonetheless hard to
answer for deep RL with non-linear function approximation, even empirically from the returns alone.

Motivated by the observations above, we develop a new RL library named ShinRL. At its core, we
introduce an RL environment interface that can compute a variety of metrics such as optimal and
learned Q functions (Fig. 1(b1)(b2)) and state visitation frequency (Fig. 1(c)), which are crucial
for analyzing the behavior of RL algorithms but are not currently supported in existing libraries.
Additionally, ShinRL provides an RL solver interface to evaluate both theoretically-justiÔ¨Åed and
practical RL algorithms in a consistent fashion. By using this interface, users can easily ablate
various extensions from developed RL algorithms, such as by removing function approximation and
exploration, to empirically evaluate their theoretically-justiÔ¨Åed variants. ShinRL is implemented
JAX2 and can be used without expensive computational resources such as high-end CPUs and GPUs
to empirical validate theoretical results that are typically conÔ¨Årmed in reasonably simple environments
(eg, Vieillard et al. [2020b] and Bellemare et al. [2016]). Nevertheless, as its main components
are built on top of OpenAI Gym [Brockman et al., 2016], algorithms once implemented can be
immediately available for practical evaluations, such as the ones using Atari [Bellemare et al., 2013]
with few modiÔ¨Åcations.

In this paper, we overview the main features of ShinRL and present how they work in practice.
SpeciÔ¨Åcally, we Ô¨Årst use ShinRL to effectively analyze the behavior of DQL, by clearly visualizing
the effects of exploration, function approximation, and more advanced techniques such as double Q
learning [Hasselt, 2010, Van Hasselt et al., 2016]. Furthermore, we demonstrate how ShinRL can be
used to empirically validate recent theoretical Ô¨Åndings in a systematic fashion, such as the effect of
KL regularization for value iteration [Kozuno et al., 2019] and for DQL [Vieillard et al., 2020a], and
the robustness of entropy-regularized policies to adversarial rewards [Husain et al., 2021].

2We also provide another version written in PyTorch in a separate branch.

2

(a) Return(b1) Optimal Q(b2) Learned Q(c) State visitationsReturnStepsPositionPositionPositionVelocityVelocityVelocityPositionVelocity2 Background

2.1 Preliminaries

Throughout this paper, we consider an inÔ¨Ånite-horizon discounted Markov decision process (MDP)
represented by the tuple {S, A, P, r, Œ≥}, where S is a Ô¨Ånite state space, A is a Ô¨Ånite set of actions,
P (s(cid:48)|s, a) is a Markovian transition kernel (where s, s(cid:48) ‚àà S, a ‚àà A), r ‚àà RS√óA is a reward function,
and Œ≥ ‚àà (0, 1) is a discount factor. The objective of RL is to Ô¨Ånd the optimal policy œÄ‚àó that maximizes
t=0 Œ≥tr (St, At)]
the discounted return (i.e., cumulative reward) given by: œÄ‚àó = argmaxœÄ
where EœÄ is the expectation over all trajectories induced by policy œÄ. For a policy œÄ, the Q function
is deÔ¨Åned as QœÄ(s, a) = EœÄ [(cid:80)‚àû
t=0 Œ≥tr(St, At)|S0 = s, A0 = a], the state visitation frequency is
deÔ¨Åned by dœÄ(s) = (1 ‚àí Œ≥) (cid:80)‚àû
t=0 Œ≥tP (St = s|œÄ). Following Vieillard et al. [2020a], we introduce
the component-wise dot product notation (cid:104)f1, f2(cid:105) = ((cid:80)
a f1(s, a)f2(s, a))s ‚àà RS for some functions
f1, f2 ‚àà RS√óA. With this, the expectation of a Q function over a policy, the V function, can be
expressed simply as V (s) = (cid:104)œÄ, Q(cid:105)(s) = Ea‚àºœÄ(¬∑|s)[Q(s, a)]. Further, we introduce another form to
describe state transitions: P v = ((cid:80)

s(cid:48) P (s(cid:48)|s, a)v(s(cid:48)))s,a ‚àà RS√óA for v ‚àà RS .

EœÄ [(cid:80)‚àû

2.2 Dynamic programming, and its extensions to practical RL algorithms

Classical approaches based on dynamic programming (DP), such as value iteration (VI) and policy
iteration (PI), aim to Ô¨Ånd the optimal Q function as the optimal policy can easily be derived from it. In
VI, Q function Q ‚àà RS√óA is iteratively updated by applying a Bellman backup: Q ‚Üê r+Œ≥P maxa Q,
which is guaranteed to reach the optimality as its unique Ô¨Åxed point is Q‚àó. PI, on the other hand,
consists of the following two steps: policy evaluation and policy improvement. The policy evaluation
step applies the expected Bellman backup to the Q function: Q ‚Üê r + Œ≥P (cid:104)œÄ, Q(cid:105) where its unique
Ô¨Åxed point is QœÄ. The policy improvement step updates the policy with the Q function as follows:
œÄ ‚Üê argmaxœÄ(cid:104)œÄ, Q(cid:105). Alternating these steps leads to the optimal Q function Q‚àó.
Unlike DP-based approaches, RL algorithms typically assume that the state-transition kernel and
the reward function are unknown. To achieve the optimal policy or the optimal Q function, they
instead require transition samples (s, a, s(cid:48), r) collected by interacting with the MDP. Notably, many
of the RL algorithms are derived from DP. For example, Q-learning [Watkins and Dayan, 1992]
can be seen as a variant of VI with exploration, while actor-critic method [Sutton et al., 2000]
is a PI variant with exploration and function approximation of Q and œÄ. Many other deep RL
algorithms have also been developed by extending DP. Approximate dynamic programming (ADP)
is a framework to theoretically analyze RL algorithms using a DP update scheme [Munos and
Szepesv√°ri, 2008, Scherrer et al., 2015]. SpeciÔ¨Åcally, in the ADP framework, the exploration and
function approximation are ‚Äúapproximated‚Äù as an estimation error, allowing us to analyze how the
error propagates to the converged policy. Doing so has revealed that VI and PI are weak to such
errors [Munos and Szepesv√°ri, 2008, Scherrer et al., 2015], which further explains the instability of
recent deep Q learning algorithms [Mnih et al., 2015, Lillicrap et al., 2015, Fujimoto et al., 2018].
Some studies have then demonstrated the effectiveness of KL regularization against the error [Azar
et al., 2012, Ghavamzadeh et al., 2011, Bellemare et al., 2016, Vieillard et al., 2020b, Kozuno et al.,
2019], which led to recent KL-regularized deep RL algorithms [Schulman et al., 2015, Vieillard et al.,
2020c,a]. Our main motivation is to develop an open-source library that allows users to reproduce
and further explore such connections from theoretical results to practical algorithms.

3 ShinRL

As summarized in Fig. 2, ShinRL consists of two main modules: ShinEnv as an interface to
implement environments modeled by the MDP and Solver as an interface for solving the RL tasks
(i.e., Ô¨Ånding the optimal policy) on the environments with speciÔ¨Åed algorithms. In order to maximize
the simplicity and Ô¨Çexibility of the library, we keep the number of main modules as low as possible
in this way, while also implementing some basic RL necessities such as replay buffers, exploration
strategies, and samplers, partially by including external libraries such as cpprb [Yamada, 2019].
Using ShinEnv and Solver in combination gives users the ability to evaluate deep RL algorithms as
well as their tabular and DP variants through the same interface, making it possible to empirically

3

Figure 2: Overview of ShinRL.

analyze if developed algorithms work theoretically as expected. In what follows, we describe the
design and main features of each module.

3.1 Environments

ShinEnv is an interface to implement MDP environments built on top of Env class of OpenAI
Gym. We design it to extend Gym‚Äôs classic control environments with a relatively small state
space, such as CartPole and MountainCar, to give users access to the ‚Äúoracle‚Äù that can compute
exact quantities for returns, Q values, and state visitation frequencies, in an ofÔ¨Çine fashion. Indeed,
when we evaluate a new RL algorithm, we often validate the algorithm on simple and constrained
environments before assessing its practical performance under challenging settings (e.g., by using
Atari and Mujoco) [Vieillard et al., 2020d, Ceron and Castro, 2021]. With the existing libraries, we
can only collect samples through interactions with environments and only observe estimated returns,
typically of high variance, averaged over episodes. On the other hand, exact quantities provided by
ShinEnv can help to get more accurate insights into how the algorithm works.

Under the hood of ShinEnv, the oracle performs an exhaustive enumeration of all state-action pairs
and derives how learned or optimal policies act via sparse matrix calculations. By doing so, ShinEnv
provides the following methods:

‚Ä¢ calc_q computes a Q-value table containing all possible state-action pairs (i.e., QœÄ) given a
policy œÄ. This method accepts some additional input arguments to consider how strongly each
reward is affected by KL and entropy regularization imposed on RL algorithms.

‚Ä¢ calc_optimal_q computes the optimal Q-value table (i.e., Q‚àó) by exactly performing value
iteration for a speciÔ¨Åed number of Ô¨Ånite-horizon using the precomputed state transition and reward
matrices.

‚Ä¢ calc_visit calculates a state visitation frequency table containing all possible states, i.e., dœÄ for

a given policy œÄ.

‚Ä¢ calc_return is a shortcut for computing exact undiscounted returns for a given policy using
state transition and reward tables. This is useful as sampling-based approaches just give expected
returns typically with high variances.

Any environment can be inherited from OpenAI Gym‚Äôs Env to ShinEnv as long as its state space is
reasonably small. When the action space is continuous, we discretize the space with a user-deÔ¨Åned
number of bins to execute the above-mentioned methods while the environment itself can accept
the original continuous actions. Table 1 shows some default environments we already implemented.
While we developed ShinCartPole, MountainCar, and Pendulum by inheriting respective OpenAI
Gym‚Äôs Env classes, we create ShinMaze from scratch as an environment that solves an easy 2D
maze where agents need to arrive at predeÔ¨Åned goal locations while avoiding obstacles, like the one
implemented and evaluated in Fu et al. [2019]. For some environments, we also support state spaces
given by raw input images, which enforces solvers presented in the next section to automatically use
convolutional neural networks when approximating policy or Q functions.

4

OpenAIGym EnvBox2DAtari‚Ä¶(a) ShinEnvShinPendulumShinMountainCarShinMazeShinCartPole(b) SolverValue IterationPolicy IterationTabular DPDeepDPTabular RLDeep RLapprox: ‚Äútabular‚Äùexplore: ‚Äúoracle‚Äùapprox: ‚Äúnn‚Äùexplore: ‚Äúoracle‚Äùapprox: ‚Äútabular‚Äùexplore: ‚Äúeps_greedy‚Äùapprox: ‚Äúnn‚Äùexplore: ‚Äúeps_greedy‚ÄùSolver configurations‚Ä¢Function approximation (tabular/nn)‚Ä¢Exploration strategies (eps_greedy/oracle)‚Ä¢Regularizations (KL, entropy)‚Ä¢‚Ä¶Table 1: Default Environments Implemented in ShinEnv.

Environment

Discrete action Continuous action

Image observation Tuple observation

ShinMaze
ShinCartPole
ShinMountainCar
ShinPendulum

(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)

Figure 3: Mixin mechanism in ShinRL.

3.2 Solvers

Solvers is an interface on which a variety of DP and RL algorithms can be implemented. As sum-
marized in Fig. 2(b), Solvers has a hierarchical structure based on how the methods are theoretically
related with each other as introduced in Sec. 2.2. More concretely, the current version supports VI and
its extensions such as KL-regularized VI (also known as Dynamic Policy Programming [Azar et al.,
2012]) and conservative VI [Kozuno et al., 2019], as well as tabular Q learning, deep Q learning [Mnih
et al., 2015], and Munchausen RL [Vieillard et al., 2020a] that are all extended from VI. We also
implement PI as well as actor-critic [Konda and Tsitsiklis, 2000] and soft actor-critic [Haarnoja et al.,
2017] as variants of PI.

We design our solver interface to be Ô¨Çexible such that all of these algorithms can be used in a
consistent fashion by toggling the solvers‚Äô conÔ¨Åguration. For example, by disabling the function
approximation while enabling exploration, deep RL methods branch to their tabular RL variants.
Alternatively, disabling both function approximation and exploration turn the methods back into
DP variants. This is very unlike existing libraries that extensively but exclusively support rapid
prototyping of deep RL algorithms.

Concretely, we adopt the mixin mechanism to realize the Ô¨Çexible behavior as summarized in Fig. 3.
ShinRL‚Äôs solvers are instantiated by mixing a number of classes called mixin, which each deÔ¨Åne
and implement a single feature. The mixins are chosen according to the solver‚Äôs conÔ¨Åguration and
algorithms. For example, mixins for DQN are chosen by passing nn to approx and eps_greedy to
explore in the conÔ¨Åguration. Moreover, this mixin mechanism allows users to easily extend the
ShinRL‚Äôs code base. We will demonstrate its Ô¨Çexibility in the following case studies.

4 Case Studies

This section demonstrates how ShinRL can analyze DP and RL algorithms through three case studies.
All these studies can be implemented in the same fashion: A solver solves the MDP and then ShinEnv
analyzes the results, as summarized in Fig. 4.

4.1 Delving into the results of DQL

Deep Q learning (DQL) [Mnih et al., 2015] is a popular approach that still has much room for
improvement. While deep networks used to approximate the Q function are generally highly expres-
sive, they also need to be trained with diverse transition samples and therefore require a well-tuned
exploration strategy in practice.

5

Make MixInsaccording to the configurationTabular DP MixInMixInto compute Q targetsMixInto prepare Q tables...Value IterationTabular DPDeep DP MixInMixInto compute Q targetsMixInto prepare Q tables...Deep DPTabular RL MixInMixInto compute Q targetsMixInfor Exploration...Tabular RLDeep RLDeep RL MixInMixInto compute Q targetsMixInfor Exploration...approx: ‚Äúnn‚Äùexplore: ‚Äúeps_greedy‚ÄùFigure 4: Flow of solving a MDP and analyzing the results with ShinRL

Figure 5: Comparison of DQL with Different Settings on ShinMountainCar. Column (e) shows
the gap between the optimal Q values Q‚àó and learned ones QœÄ, where negative and positive values
(i.e., overestimation and underestimation) are highlighted in red and blue.

First, let us introduce how ShinRL can visualize the effectiveness of exploration strategies on the
ShinMountainCar environment. As the original DQL can be seen as an extension of Value Iteration
with neural network approximation and epsilon-greedy exploration, DQL can be built by passing
nn to approx and eps_greedy to explore in the conÔ¨Åguration. Instantiating the environment and
performing the DQL on it can be done simply in a few lines as shown below.

1 import gym
2 from shinrl import DiscreteViSolver
3
4 # instantiate an environment
5 env = gym . make ( " ShinMountainCar - v0 " )
6
7 # instantiate deep Q learning - based solver
8 dql_config = DiscreteViSolver . DefaultConfig ( approx = " nn " , explore = "

eps_greedy " )

9 dql_mixins = DiscreteViSolver . make_mixins ( env , dql_config )
10 dql_solver = DiscreteViSolver . factory ( env , dql_config , dql_mixins )
11
12 # run the solver
13 dql_solver . run ()

The epsilon-greedy exploration strategy highly depends on its value of epsilon, i.e., how likely the
agent takes random actions at each step. To understand how this epsilon affects DQL‚Äôs behaviors, we
run two DQL solvers with different constant values set to epsilon, (cid:15) = 0.0 and (cid:15) = 0.1, and observe
their state visitation frequencies. This can be done with calc_visit function as follows, which take
just about 1ms in a CPU environment3.

1 # compute state - action visitation table using learned policy
2 policy = dql_solver . tb_dict [ " ExplorePolicy " ]

3ConÔ¨Årmed with Intel(R) Core(TM) i7-8700K @ 3.70GHz.

6

approx: ‚Äúnn‚Äùexplore: ‚Äúeps_greedy‚ÄùConfigEnvShinPendulum-v0Make MixInsmixins= DiscreteViSolver.make_mixins(env, config)(Optional) Arrange MixInsmixins.insert(2, UserDifinedMixIn)Run a Solverdqn_solver= ...dqn_solver.run()mixins== [DeepRlStepMixIn,QTargetMixIn, ‚Ä¶](If ShinEnv) Oracle Analysisq = env.calc_q(‚Ä¶)env.plot_S(q)(ùúñ=0)(ùúñ=0.1)(ùúñ=0.1+ double Q)(a) Return(b) Loss(c) State visitations(d) ùëÑùúã(e) ùëÑùúã‚àíùëÑ‚àóReturnStepsReturnStepsReturnStepsLossStepsLossStepsLossStepsVelocityPositionVelocityPositionVelocityPositionVelocityPositionVelocityPositionVelocityPositionVelocityPositionVelocityPositionVelocityPositionFigure 6: Comparisons of VI Variants with Various Regularizations. (a) Return plots and (b)
optimality gaps given by (cid:107)QœÄk ‚àí Q‚àó(cid:107)‚àû for VI, KL-regularized VI, entropy-regularized VI, and CVI
on the ShinMaze environment. (c) DQL and M-DQL on the Breakout environment.

3 visit = env . calc_visit ( policy )

Figure 5(a) and (b) present plots for returns and losses, and the corresponding state visitation tables
are visualized in (c). A bit surprisingly, both solvers Ô¨Ånally solved the task (deÔ¨Åned by the return
arrived at ‚àí20), and their losses are almost comparable. Nevertheless, they demonstrate a clear
difference in state visitation frequencies, where the solver with (cid:15) = 0 leads to a poor exploration
policy that can potentially visit a limited set of states even after a large number of steps, while the
solver with (cid:15) = 0.1 can visit almost all possible states that the agent can reach. Now we are interested
in how this difference in state visitation frequencies affects the quality of learned Q functions. Here,
we visualize the difference between learned and optimal Q values as follows:

1 optimal_q = env . calc_optimal_q ()
2 learned_q = dql_solver . tb_dict [ " Q " ]
3 diff_q = learned_q - optimal_q

As shown in Fig. 5(e), Q values are inaccurate in many places due to underestimation under (cid:15) = 0
and overestimation under (cid:15) = 0.1. Overestimation is particularly a known phenomenon and can be
alleviated via double-Q learning [Hasselt, 2010, Van Hasselt et al., 2016] that learns two Q functions
with different sets of samples. The bottom row of Fig. 5 shows that the double-Q trick indeed
improves the accuracy of the learned Q values, except for some state-action pairs that were not visited
during learning. This further implies the importance of better dealing with out-of-distribution actions
such as done in ofÔ¨Çine RL [Levine et al., 2020], which we leave for future work.

4.2 Comparing VI, KL-regularized VI, CVI, and Munchausen DQL

As we introduced in Sec. 2.2, VI theoretically becomes robust to estimation errors, which typically
arise due to function approximation and exploration, by imposing KL regularization [Vieillard et al.,
2020b]. Furthermore, involving entropy as well as KL regularizations turns VI to conservative VI
(CVI) [Kozuno et al., 2019], which inspires the formulation of Munchausen DQL (M-DQL) [Vieillard
et al., 2020a] that extends conventional deep Q learning with these regularizations to improve the
stability. In this case study, we demonstrate how these theoretical Ô¨Åndings of VI, KL-regularized VI,
CVI, DQL, and M-DQL can be conÔ¨Årmed empirically and systematically using ShinRL.

To observe how VI changes its behavior with regularizations, let us Ô¨Årst introduce the following DP
update scheme:

(cid:26)œÄk+1 = argmaxœÄ ((cid:104)œÄ, Qk(cid:105) ‚àí œÑ KL(œÄ(cid:107)œÄk) + ŒªH(œÄ)) ,
Qk+1 = r + Œ≥P (cid:104)œÄk+1, Qk ‚àí œÑ KL(œÄk+1(cid:107)œÄk) + ŒªH(œÄk+1)(cid:105) + (cid:15)k,

(1)

where œÑ and Œª are coefÔ¨Åcients for KL and entropy regularization, respectively. (cid:15)k ‚àº N (0, œÉ) is a zero-
mean Gaussian error vector with standard deviation œÉ at k-th iteration, which models either function
approximation and/or exploration errors. Without the error term (cid:15)k, evaluating this regularized DP

7

(a) Returns on ShinMaze(b) Optimality gaps on ShinMaze(c) Average returns on BreakoutReturnStepsùëÑùúã‚àíùëÑ‚àó‚àûStepsAverage returnStepsVIKL-regularized VIEntropy-regularized VICVIDQLM-DQL4

5

3

7

8

9

with ShinRL is quite simple by just toggling the conÔ¨Ågurations of ViSolver, where the parameters
œÑ , Œª are respectively speciÔ¨Åed by kl_coef, er_coef. For example, mixins for CVI that comes with
both KL and entropy regularizations can be instantiated as follows:

1 # Instantiate VI
2 cvi_config = DiscreteViSolver . DefaultConfig (
3

# use tabular method
# use oracle for exploration

approx = " tabular " ,
explore = " oracle " ,
kl_coef =0.1 ,
er_coef =0.3 ,

6
7 )
8 cvi_mixins = DiscreteViSolver . make_mixins ( env , cvi_config )

The error term can be easily implemented by arranging the mixins. We implement Eq. (1) by adding
a mixin which add noise to computed Q values:

1 class ErrorMixIn :
2

# A mixin to add noise to the computed Q values
...

4
5 mixins = [ ErrorMixIn , * cvi_mixins ]
6 cvi_solver = DiscreteViSolver . factory ( env , cvi_config , mixins )
7 cvi_solver . run ()

# Add ErrorMixIn to CVI ‚Äôs mixins

Figure 6 shows (a) returns as well as (b) the optimality gap deÔ¨Åned by (cid:107)Q‚àó ‚àí QœÄ(cid:107)‚àû on the ShinMaze
environment. KL-regularized VI is indeed robust against noise empirically and can easily reach the
optimality. On the other hand, entropy regularization is less important than KL regularization, which
is also explained by Vieillard et al. [2020b]. Nevertheless, the next section will show the effectiveness
of entropy regularization when used in the SAC algorithm [Ceron and Castro, 2021].

Now the task is to compare DQL and M-DQL. They are deep RL algorithms that should be evaluated
in more challenging environments to best show their performances. To this end, ShinRL fully supports
OpenAI Gym, and can call the minatar environment [Young and Tian, 2019] that is a lightweight
testbed inspired by Atari games.

1 from shinrl import make_minatar
2 env = make_minatar ( " breakout " )
3
4 # M - DQL
5 mdql_config = DiscreteViSolver . DefaultConfig (
6

" approx " : " nn " ,
" explore " : " eps_greedy " ,
# the solver reduces to standard DQL by setting kl_coef and
er_coef to zero .
kl_coef =0.027 ,
er_coef =0.003

10
11 )
12 mdql_mixins = DiscreteViSolver . make_mixins ( env , mdql_config )
13 mdql_solver = DiscreteViSolver . factory ( env , mdql_config , mdql_mixins )
14 mdql_solver . run ()

Figure 6 (c) depicts return plots for the Breakout environment, demonstrating that M-DQL outper-
forms DQL thanks to KL and entropy regularizations.

4.3 Evaluating robustness of the SAC algorithm to adversarial rewards

Another family of algorithms that ShinRL supports extensively is policy iteration (PI), which is the
foundation of many recent deep RL algorithms. For example, the SAC algorithm [Haarnoja et al.,
2018] is an extension of PI with function approximation, exploration, and entropy regularization.
Some recent work shows that the SAC algorithm can learn a robust policy, both empirically [Haarnoja
et al., 2018] and theoretically [Husain et al., 2021], thanks to its entropy regularizer. In this case study,
we Ô¨Årst investigate how SAC changes its robustness, in particular to adversarial rewards presented
by Husain et al. [2021], with different entropy regularization coefÔ¨Åcient Œª in the ShinPendulum
environment. In ShinRL, mixins for SAC can be instantiated by PiSolver as follows:

8

Figure 7: SAC with Different CoefÔ¨Åcients Œª for Entropy Regularization: (a) Return plots and (b)
State visitation frequencies (higher frequencies highlighted in red) on the ShinPendulum environment.

1 # Instantiate SAC
2 sac_config = DiscretePiSolver . DefaultConfig (
3

# use neural network approximation

4

approx = " nn " ,
explore = " eps_greedy " ,
er_coef =0.1 ,

5
6 )
7 sac_mixins = DiscretePiSolver . make_mixins ( env , sac_config )

# use epsilon - greedy policy for exploration

Note that by setting er_coef to 0, the method reduces to the vanilla actor-critic algorithm [Konda
and Tsitsiklis, 2000]. As done in the experiments of Husain et al. [2021], we consider the following
adversarial reward radv by slightly modifying the original implementation of the pendulum:

radv =

(cid:26)r(s, a) + (cid:15)
r(s, a)

if r(s, a) ‚â§ ‚àí5
otherwise,

(2)

where (cid:15) is sampled from the normal distribution N (4.9, 0.1). This reward design promotes the agent
to stay the pendulum around its initial state while the optimal behavior is still swinging it up. Similar
to the previous case study, we implemented Eq. (2) on ShinRL‚Äôs SAC by adding another mixin which
assigns adversarial rewards to the collected data.

Figure 7(a) shows the learning curves of SAC with different coefÔ¨Åcients for entropy regularization.
We conÔ¨Årm that reasonably increasing the regularization strength improves the robustness against
adversarial rewards as conÔ¨Årmed by Husain et al. [2021].

Furthermore, we validate the Ô¨Ånding of Haarnoja et al. [2017] that empirically conÔ¨Årms the improve-
ment of exploration quality as the entropy regularization becomes stronger. By using count_visit
function, we can visualize the frequencies of state-action pairs stored in a replay buffer, making
it possible to assess if the exploration is sufÔ¨Åcient during the training. As shown in Fig. 7(b), we
conÔ¨Årm that a wider range of state-action pairs are visited as Œª becomes higher.

5 Conclusion

We presented ShinRL, an open-source library that can evaluate RL algorithms from both theoretical
and practical perspectives in a principled fashion. As shown in our case studies, ShinRL can be used
to analyze the behavior of deep RL algorithms through the lens of Q-value tables and state visitation
frequencies, which are not immediately available in existing RL libraries. Further, we empirically
conÔ¨Årm recent theoretical Ô¨Åndings of KL regularization and entropy regularization for RL [Kozuno
et al., 2019, Vieillard et al., 2020a, Husain et al., 2021] using our Ô¨Çexible RL solver interface. Future
work will seek to extend the library to deal with a wider variety of tasks and algorithms not only on
RL but also imitation learning and ofÔ¨Çine RL.

Acknowledgments

The authors would like to thank Masashi Hamaya for helpful feedback on the manuscript.

9

(a) Returns(b)StatevisitationsùúÜ=0.0ùúÜ=0.001ùúÜ=0.1ùúÜ=1.0ReturnAngleAngular VelocityAngleAngular VelocityAngleAngular VelocityAngleAngular VelocityùúÜ=0.001ùúÜ=0.0ùúÜ=0.1ùúÜ=1.0StepsReferences

Tadashi Kozuno, Eiji Uchibe, and Kenji Doya. Theoretical Analysis of EfÔ¨Åciency and Robustness
of Softmax and Gap-Increasing Operators in Reinforcement Learning. In Proceedings of the
International Conference on Machine Learning, pages 2995‚Äì3003, 2019.

Nino Vieillard, Olivier Pietquin, and Matthieu Geist. Munchausen Reinforcement Learning. In

Advances in Neural Information Processing Systems, pages 4235‚Äì4246, 2020a.

Hisham Husain, Kamil Ciosek, and Ryota Tomioka. Regularized Policies are Reward Robust. In
Proceedings of the International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages 64‚Äì72,
2021.

Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2nd

edition, 2018.

Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement Learning in Robotics: A Survey.

International Journal of Robotics Research, 32(11):1238‚Äì1274, 2013.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-Level
Control through Deep Reinforcement Learning. Nature, 518(7540):529‚Äì533, 2015.

Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C Parkes,
and Richard Socher. The AI Economist: Improving Equality and Productivity with AI-Driven Tax
Policies. arXiv preprint arXiv:2004.13332, 2020.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-based Policies. In Proceedings of the International Conference on Machine Learning,
pages 1352‚Äì1361, 2017.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy
Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Proceedings of the
International Conference on Machine Learning, pages 1861‚Äì1870, 2018.

Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe Policy Iteration.
In Proceedings of the International Conference on Machine Learning, pages 307‚Äì315, 2013.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust Region
Policy Optimization. In Proceedings of the International Conference on Machine Learning, pages
1889‚Äì1897, 2015.

Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. ChainerRL: A Deep
Reinforcement Learning Library. Journal of Machine Learning Research, 22(77):1‚Äì14, 2021.

Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Belle-
mare. Dopamine: A Research Framework for Deep Reinforcement Learning. arXiv preprint
arXiv:1812.06110, 2018.

Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph
Gonzalez, Michael Jordan, and Ion Stoica. RLlib: Abstractions for Distributed Reinforcement
Learning. In Proceedings of the International Conference on Machine Learning, pages 3053‚Äì3062,
2018.

Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph,
and Aleksander Madry. Implementation Matters in Deep Policy Gradients: A Case Study on PPO
and TRPO. In Proceedings of the International Conference on Learning Representations, 2020.

Johan Samir Obando Ceron and Pablo Samuel Castro. Revisiting Rainbow: Promoting More
Insightful and Inclusive Deep Reinforcement Learning Research. In International Conference on
Machine Learning, volume 139, pages 1373‚Äì1383, 2021.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.

10

Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R√©mi Munos, and Matthieu Geist.
Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning. In Advances
in Neural Information Processing Systems, pages 12163‚Äì12174, 2020b.

Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip Thomas, and R√©mi Munos. Increasing the
Action Gap: New Operators for Reinforcement Learning. In Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence, pages 1476‚Äì1483, 2016.

Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning
Environment: An Evaluation Platform for General Agents. Journal of ArtiÔ¨Åcial Intelligence
Research, 47:253‚Äì279, 2013.

Hado Hasselt. Double Q-Learning. In Advances in Neural Information Processing Systems, pages

2613‚Äì2621, 2010.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double
Q-Learning. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, page 2094‚Äì2100,
2016.

Christopher JCH Watkins and Peter Dayan. Q-Learning. Machine learning, 8(3-4):279‚Äì292, 1992.

Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy Gradient
In Advances in Neural

Methods for Reinforcement Learning with Function Approximation.
Information Processing Systems, pages 1057‚Äì1063, 2000.

R√©mi Munos and Csaba Szepesv√°ri. Finite-Time Bounds for Fitted Value Iteration. Journal of

Machine Learning Research, 9(27):815‚Äì857, 2008.

Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu Geist.
Approximate ModiÔ¨Åed Policy Iteration and its Application to the Game of Tetris. Journal of
Machine Learning Research, 16:1629‚Äì1676, 2015.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. In
Proceedings of the International Conference on Learning Representations, pages 1‚Äì14, 2015.

Scott Fujimoto, Herke Hoof, and David Meger. Addressing Function Approximation Error in
Actor-Critic Methods. In Proceedings of International Conference on Machine Learning, pages
1587‚Äì1596, 2018.

Mohammad Gheshlaghi Azar, Vicen√ß G√≥mez, and Hilbert J Kappen. Dynamic Policy Programming.

Journal of Machine Learning Research, 13(1):3207‚Äì3245, 2012.

Mohammad Ghavamzadeh, Hilbert Kappen, Mohammad Azar, and R√©mi Munos. Speedy Q-Learning.

In Advances in Neural Information Processing Systems, pages 2411‚Äì2419, 2011.

Nino Vieillard, Bruno Scherrer, Olivier Pietquin, and Matthieu Geist. Momentum in Reinforcement
Learning. In Proceedings of the International Conference on ArtiÔ¨Åcial Intelligence and Statistics,
pages 2529‚Äì2538, 2020c.

Hiroyuki Yamada. cpprb, 1 2019. URL https://gitlab.com/ymd_h/cpprb.

Nino Vieillard, Olivier Pietquin, and Matthieu Geist. Deep Conservative Policy Iteration.

In
Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34, pages 6070‚Äì6077,
2020d.

Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing Bottlenecks in Deep Q-
Learning Algorithms. In Proceedings of the International Conference on Machine Learning, pages
2021‚Äì2030, 2019.

Vijay R Konda and John N Tsitsiklis. Actor-Critic Algorithms. In Advances in Neural Information

Processing Systems, pages 1008‚Äì1014, 2000.

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OfÔ¨Çine Reinforcement Learning:
Tutorial, Review, and Perspectives on Open Problems. arXiv preprint arXiv:2005.01643, 2020.

Kenny Young and Tian Tian. Minatar: An Atari-Inspired Testbed for Thorough and Reproducible

Reinforcement Learning Experiments. arXiv preprint arXiv:1903.03176, 2019.

11

