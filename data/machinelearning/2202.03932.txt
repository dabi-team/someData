Are Attention Networks More Robust?
Towards Exact Robustness Veriﬁcation for Attention Networks*

Hsuan-Cheng Liao†‡, Chih-Hong Cheng§+, Hasan Esen†, Alois Knoll‡

2
2
0
2

p
e
S
1
2

]

G
L
.
s
c
[

2
v
2
3
9
3
0
.
2
0
2
2
:
v
i
X
r
a

Abstract— As an emerging type of Neural Networks (NNs),
Attention Networks (ATNs) such as Transformers have been
shown effective, in terms of accuracy, in many applications.
This paper further considers their robustness. More speciﬁcally,
we are curious about their maximum resilience against local
input perturbations compared to the more conventional Multi-
Layer Perceptrons (MLPs). Thus, we formulate the veriﬁcation
task into an optimization problem, from which exact robustness
values can be obtained. One major challenge, however, is the
non-convexity and non-linearity of NNs. While the existing
literature has handled the challenge to some extent with
level of
methods such as Branch-and-Bound, the additional
difﬁculty introduced by the quadratic and exponential functions
in the ATNs has not been tackled. Our work reduces this
gap by focusing on sparsemax-based ATNs, encoding them
into Mixed Integer Quadratically Constrained Programming
problems, and proposing two powerful heuristics for a speedup
of one order of magnitude. Finally, we train and evaluate several
sparsemax-based ATNs and similar-sized ReLU-based MLPs
for a lane departure warning task and show that the former is
surprisingly less robust despite generally higher accuracy.

I. INTRODUCTION

Over the past decade, Neural Networks (NNs) have been
widely adopted for many applications, including self-driving
cars. Lately, as an emerging type of NNs, Attention Networks
(ATNs) such as Transformers [1], are often found to be the
most effective models in these applications, compared to
the more conventional Multi-Layer Perceptrons (MLPs) [2].
Nevertheless, these studies primarily focus on accuracy per-
formances. Parallel research has shown that NNs often lack
robustness against input changes such as adversarial attacks
and domain shifts [3], [4], hence hindering the dependability
of the NN-based applications. Based on such background, a
natural question is whether ATNs are also more robust than
MLPs, given the better accuracy observed by many studies.
To answer this question, we evaluate the maximum resilience
of the NNs against local input perturbations generally mod-
eled by lp-distances, i.e., exact robustness veriﬁcation (where

†Hsuan-Cheng Liao and Hasan Esen are with DENSO AUTO-
MOTIVE Deutschland GmbH, 85386 Eching, Germany. h.liao,
h.esen@eu.denso.com
is
IKS,

Cognitive
chih-hong.cheng@iks.fraunhofer.de

for
Germany.

§Chih-Hong

Fraunhofer

Munich,

Systems

Institute

Cheng

80686

with

‡Hsuan-Cheng Liao and Alois Knoll are with Department of
In-
formatics, Technical University of Munich, 85748 Garching, Germany.
knoll@in.tum.de

+Key results are conducted during the author’s service in DENSO

AUTOMOTIVE Deutschland GmbH.

*

This project has received funding from the European Union’s
Horizon 2020 research and innovation programme under grant agreement
No 956123 - FOCETA.

p can be 1, 2, ..., ∞). Ultimately, the goal is to hold a direct
comparison of the robustness of the two kinds of NNs and
gain insights from the results.

In the literature, research efforts have been made to enable
exact robustness veriﬁcation for MLPs, particularly ones with
feed-forward layers and ReLU activation function [5], [6],
[7], [8]. The main approach is to employ an optimization
framework with which the optimal robustness performance
can be computed based on constraints according to the
network architecture and admissible input perturbations.
However, there is still a gap for ATNs due to the more
complex operations in the network, namely the dot product
between variables and the activation function in the Multi-
Head Self-Attention (MSA) block [1]. We attempt to close
the gap in this work but should state that our proposal is
merely a partial solution. To elaborate, having formulated
the task into a Mixed Integer Programming (MIP)-based
optimization problem as in [6], [7], we focus on the group
of ATNs that uses sparsemax (instead of softmax) for
MSA activation. This allows for a precise encoding of the
network into the MIP, or more particularly Mixed Integer
Quadratically Constrained Programming (MIQCP) due to
the remaining quadratic terms. Despite a limited resolution
(similar to the studies focusing only on ReLU-based MLPs),
our work shows that sparsemax-based ATNs tend to be
less robust than similar-sized MLPs despite generally higher
accuracy. Consequently, our ultimate call for contrasting
accuracy and robustness performances of NNs is served.

Additionally, in our work, we devise and apply two effec-
tive heuristics for accelerating the solving of the MIP prob-
lem by one order of magnitude. Notably, these heuristics are
not restricted to our work but can be commonly implemented
by related studies. Regarding the experiment, we consider
the industrial application of Lane Departure Warning (LDW),
which is essentially a time-series classiﬁcation and regression
problem. More speciﬁcally, the NN of interest has to predict
the direction and time to the lane departure, with a certain
duration of past driving information such as ego vehicle
velocity and time to collision to adjacent vehicles. Such an
LDW system can be used for human driving assistance or
runtime monitoring of automated vehicle control functions.
As mentioned, our empirical ﬁndings indicate that con-
ducting thorough studies and providing rigorous guarantees
is crucial before deploying any NN-based applications. In
summary, our contributions include the following:

• To implement exact robustness veriﬁcation for sparse-

max-based ATNs;

• To propose accelerating heuristics for general robustness

This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer
be accessible.

 
 
 
 
 
 
veriﬁcation;

• To benchmark ATN and MLP accuracy and robustness

with an industrial application (i.e., LDW).

II. RELATED WORK

This section provides an overview of related work, focus-
ing on robustness veriﬁcation for ReLU-based MLPs (i.e.,
piece-wise linear feed-forward NNs) and ATNs, respectively.

A. Robustness Veriﬁcation for Neural Networks

Following the common categorization [7], we introduce
two main branches of veriﬁcation methods: complete and
incomplete. To illustrate the difference, we ﬁrst assume an
adversarial polytope to be the exact set of NN outputs result-
ing from the norm-bounded set of perturbed inputs. To assert
the robustness of the NN on these perturbed inputs, complete
methods handle the adversarial polytope directly, attaining an
adversarial example or a robustness certiﬁcate for each of the
inputs when given sufﬁcient processing time. These methods
usually apply Mixed Integer Programming (MIP) [5], [6],
[7] or Satisﬁability Modulo Theory (SMT) [8], [9], which in
turn utilizes Linear Programming (LP) or Satisﬁability (SAT)
solvers with accelerating techniques such as interval analy-
sis [6], [7], [8] or region partitioning [10] in a Branch-and-
Bound (BnB) fashion [11]. By contrast, incomplete methods
reason upon an outer approximation of the adversarial poly-
tope. Such reasoning typically results in faster veriﬁcation
time yet possibly some robust queries being evaluated non-
robust due to the over approximation. Common methods in
this branch include duality [12], abstract interpretation [13]
and Semi-Deﬁnite Programming (SDP) [11]. For more de-
tails, interested readers are referred to the survey paper [14].

B. Robustness Veriﬁcation for Attention Networks

As ATNs contain more complex operations than other
NNs, such as MLPs, they are typically more challenging to
verify. From the literature, we are only aware of two lines of
work [15], [16], both conducted with sentiment classiﬁcation
in natural language processing. In [15], the authors calculate
linear intervals for all operations in a Transformer to ﬁnd
the lower bound of the difference between the softmax
values of the ground-truth class and the most-probable-other-
than-ground-truth class. Given an admissible perturbation
region (i.e.,
if the computed lower bound is
larger than zero, the model is guaranteed robust since the
prediction remains unchanged. Holding a similar strategy,
the most recent work [16] applies abstract interpretation and
suggests several techniques such as noise symbol reduction
and softmax summation constraint to achieve better speed
and precision during veriﬁcation.

input set),

Our work differs from these veriﬁcation frameworks in
two ways. First, we ﬁnd ATN’s maximum resilience (i.e.,
minimum adversarial perturbation) through precise MIP en-
coding and fasten the procedure with novel tactics, allowing
us to compare the robustness with common MLPs. Second,
we study LDW, an industrial application that has not been
used for robustness veriﬁcation.

III. PRELIMINARIES
This section provides a brief description of the ATN under
veriﬁcation. For a more elaborate illustration of general
ATNs, readers are encouraged to see [1], [2].

Fig. 1: The Attention Network under veriﬁcation [1], [2].

As shown in Fig. 1, an ATN typically processes an array of
embedded tokens with alternating blocks of MSA and MLP,
which are both preceded by Layer Normalization (LN) and
followed by residual connections1. Then, after another layer
normalization and token-wise mean extraction, the network
is appended with suitable afﬁne heads for downstream pre-
dictions such as classiﬁcation (CLS) and regression (REG).
Mathematically, given an input x ∈ RN×D, in which N is the
number of tokens and D the dimension of features, we write:

z0 = x
(cid:98)z(cid:96) = MSA(LN(z(cid:96)−1)) + z(cid:96)−1,
z(cid:96) = MLP(LN( (cid:98)z(cid:96))) + (cid:98)z(cid:96),

f CLS(x) = CLS( (cid:101)zL) ∈ RC,
f REG(x) = REG( (cid:101)zL) ∈ R,

(1)

(2)

(3)

(4)

(5)

in which (cid:101)zL is the token-wise mean of LN(zL) , C is the
number of predeﬁned classes and (cid:96) = 1, . . . , L is the layer
index.

As introduced, we consider sparsemax-based MSA and

provide its deﬁnition as follows:

[Qh, Kh, Vh] = zWQKV

h

,

Ah = sparsemax

(cid:16)
QhK(cid:62)
h /

√

(cid:17)

,

DH

SAh(z) = AhVh,
MSA(z) = [SA1(z), . . . , SAH (z)] WMSA,

(9)
where z ∈ RN×D is a general input matrix, Qh, Kh, Vh ∈ RDH
are the query, key and value matrices for the h-th self-
attention head, H is the number of self-attention heads,

1Placing LN before MSA and MLP is found to give better network

performance than placing it after residual addition [17].

(6)

(7)

(8)

h

∈ RD×(DH ·3)
DH is the dimension of each head, WQKV
and WMSA ∈ R(DH ·H)×D are the trainable weights, and h =
1, . . . , H is the head index. Essentially, sparsemax projects
an input vector u ∈ RD to an output vector p ∈ RD, where
p1 +· · ·+pD = 1 and pi ≥ 0 for i = 1, . . . , D (i.e., a probability
simplex). Technically, it can serve as a piece-wise linear
approximation to softmax [18]. Algorithm 1 and Fig. 2
provide the calculation steps for a closed-form solution and
the visualization of a 2D input-output relation.

Algorithm 1 Calculate sparsemax activation [18]
1: Input: u ∈ RD
2: Sort u into ˆu, where ˆu1 ≥ · · · ≥ ˆuD
3: Find s( ˆu) := max
(cid:16)

k ∈ [1, D] | 1 + k ˆuk > ∑k
s( ˆu)
j=1 ˆu j
s( ˆu)

4: Deﬁne τ( ˆu) =
5: Output: p ∈ RD, where pi = max(ui − τ( ˆu), 0)

j=1 ˆu j

−1

(cid:110)

∑

(cid:17)

(cid:111)

Fig. 2: softmax vs. sparsemax, given an input vector u = [t, 0] ∈
R2 (adapted from [18]).

2

1

1

)WMLP

2
∈ RD×DMLP, bMLP

To proceed with the ATN architecture2, MLP is a two-
layer NN with ReLU activation, written as MLP(v) =
∈ RN×D, where v ∈ RD
ReLU(vWMLP
2 +bMLP
1 +bMLP
∈ RDMLP,
is the input vector and WMLP
1
∈ RD the trainable parameters,
∈ RDMLP×D, bMLP
WMLP
2
and + the addition with broadcasting rules in common
NN implementation libraries. For LN, we take a linearized
variant, i.e., LNi(v) = wi × (vi − µv) + bi, where v ∈ RD is
the input vector, µv ∈ R its mean, LNi(v) the i-th element for
i = 1, ..., D, and w ∈ RD and b ∈ RD the trainable parameters.
Such modiﬁcation avoids the division over relatively small
input variance σv in the quadratic LN deﬁnition, thereby
allowing the variables to be bounded more tightly [15].

IV. METHODOLOGY
Having denoted the network operations, we now deﬁne
the robustness property for veriﬁcation. Subsequently, we
highlight our MIQCP encoding steps and two heuristics that
shall accelerate the solving of the encoded MIQCP3.
A. Problem Formulation

We formalize the problem of robustness veriﬁcation as
f (·) : RM → RN denote the network under
follows: Let
veriﬁcation, x ∈ RM the original data point on which the

2We write here the input in vector form instead of matrix form as in (3)

because the operations are applied vector-wise essentially.

3Due to the space limit, we present only the classiﬁcation model, but

regression cases can be derived similarly.

network is being veriﬁed, and x(cid:48) ∈ RM a perturbed input
which tries to deceive the network, we write:

Dp(x(cid:48), x)
min
x(cid:48)
subject to x(cid:48) ∈ Bp(x),

arg max
i
arg max
i

( f CLS
i

(x)) = gtCLS(x),

( f CLS
i

(x(cid:48))) (cid:54)= gtCLS(x),

(10)

(11)

(12)

(13)

where Dp(·, ·) is the lp-distance with commonly used p ∈
{1, 2, ∞} and Bp(x) = {x(cid:48) (cid:12)
(cid:12) (cid:107)x(cid:48) − x(cid:107)p ≤ ε} the lp-norm ball
of radius ε around x, gtCLS(x) ∈ {1, . . . ,C} the ground-truth
class label and f CLS
the i-th element of the classiﬁcation
head output. Conceptually, the optimizer’s main task is to
ﬁnd within an admissible perturbation region a perturbed data
point closest to the original one and fulﬁlls the misprediction
constraints.

i

B. MIQCP Encoding

For veriﬁcation, we need to encode all network opera-
tions into the formulated optimization problem (which will
essentially be a MIQCP problem). Most of them, such as
afﬁne transformation and ReLU, are discussed by the related
work [5], [6], [7]. Hence, we explain only the additional
operations introduced by sparsemax, namely sorting (Al-
gorithm 1, Line 2) and ﬁnding the support (Algorithm 1,
Line 3).

For sorting, we introduce a binary integer permutation
matrix P ∈ {0, 1}D×D and encode the following constraints:

D
∑
i=1
D
∑
j=1

Pi j = 1,

for j = 1, . . . , D;

Pi j = 1,

for i = 1, . . . , D;

ˆu = Pu,
ˆui ≥ ˆui+1,

for i = 1, . . . , D − 1,

(14)

(15)

(16)

(17)

where u ∈ RD is the (perturbed) input vector at sparsemax
and ˆu the sorted output. For the support, we ﬁrst deﬁne a vec-
tor ρ ∈ RD, where ρk = 1 + k ˆuk − ∑k
j=1 ˆu j (k = 1, . . . , D), and
then introduce another binary integer variable ζ ∈ {0, 1}D
such that:

ζk =

(cid:40)

1,
0,

if ρk > 0;
otherwise.

(18)

For
its actual computation, we implement
method [19] with large positive constants M+/−
positive constant η (e.g., 10−6) as follows:

k

the Big-M
and a small

ρk ≤ M+
−ρk + η ≤ M−

k × ζk,
k × (1 − ζk).

(19)

(20)

We defer the details for setting M+/−
in Lemma 1 and
Lemma 2. For now, ﬁnding the support is equivalent to sum-
ming up the binary integer vector as s( ˆu) = ∑D
k=1 ζk. Lastly,
Algorithm 1 Line 4 can be encoded with a linear constraint

k

for the summation term and a quadratic constraint for the
division. As such, we arrive at a plain MIQCP encoding for
quantifying the sparsemax-based ATN’s robustness.

k = 1.

Lemma 1. For all k = 1, . . . , D, the smallest value for the
Big-M encoding in (19) is opt M+
rewrite ρk = 1 + k ˆuk − ∑k
Proof. We ﬁrst
j=1 ˆu j = 1 +
∑k
j=1( ˆuk − ˆu j). Now, with the sorting result (i.e., ˆu1 ≥ ˆu2 ≥
. . . ≥ ˆuD), it follows that 1 = ρ1 ≥ ρ2 ≥ · · · ≥ ρD, hence the
lemma.
Lemma 2. For all k = 1, . . . , D, let the input of sparsemax
be bounded as uk ∈ [uk, uk] and η = 10−6. We ﬁrst deﬁne λ ∈
RD : λk = 1 + (k − 1)(u − u), where u = min(u1, . . . , uD) and
u = max(u1, . . . , uD). Then, the smallest value for the Big-M
encoding in (20) is opt M−
k = |λk| + η, if λk ≤ 0; otherwise,
(20) needs not be implemented.
Proof. After sorting on u, we can only rely on vector-wise
bounds for estimating ˆu, i.e., ˆuk ∈ [u, u]. Considering the
result of sorting (i.e., ˆu1 ≥ ˆu2 ≥ . . . ≥ ˆuD), we can then derive
ρk = 1 + k ˆuk − ∑k
j=1( ˆuk − ˆu j) ≥ 1 + (k − 1)(u −
u) = λk. Now, there are two cases: If λk ≤ 0, then the smallest
value for (20) is opt M−
k = |λk| + η. Otherwise, ρk ≥ λk > 0
and (20) is not needed.
C. Acceleration Heuristics

j=1 ˆu j = 1 + ∑k

As the prior art

indicates, one usually needs several
acceleration heuristics to solve an encoded MIP problem
efﬁciently. We present our proposals in the following.

1) Interval Analysis: Interval analysis has been widely
studied and proven effective in aiding MIP solving [6], [7].
The central idea is that with tight interval bounds propagated
across the network, non-linear functions such as ReLU or
max can be constrained to certain behaviors, thus avoiding
partial integer variables. We extend this line of research by
ﬁnding a novel bounding technique over the ATN’s activation
function4. Denoting the activation function as σ , (perturbed)
input vector as z ∈ RD, output vector as a, upper and lower
bounds as upper and lower bars, we have5:

ai ∈ [ai, ai] =[σi(z1, . . . , zi, . . . , zD),
σi(z1, . . . , zi, . . . , zD)],

(21)

where i = 1, . . . , D. Essentially,
this formula stems from
the observation that the lower (upper) bound of an output
element can be calculated by applying the activation to a
vector consisting of this element’s input lower (upper) bound
and other elements’ input upper (lower) bounds. In our
evaluation, we generally see much tighter intervals (usually
one-tenth of the simple ATN activation interval [0, 1]) with
this technique.

2) Progressive Veriﬁcation with Norm-Space Partitioning:
Space partitioning shares a similar goal to interval analysis,
attempting to tighten variable bounds and generate a faster
solution. The prior art has focused on how to do parti-
tioning and prioritizing [10]. Our work differs slightly by

observing that adversarial examples usually appear close to
the clean input and exploiting the nature of the formulated
optimization problem. We propose a progressive veriﬁcation
procedure based on norm-space partitioning. To illustrate, we
divide the admissible (cid:96)p-ball into disjoint sub-regions and
apply a divide-and-conquer strategy. For example, given a
partition step 0 < εstep ≤ ε, we ﬁrst set the current sub-region
lower bound εmin = 0 and the current sub-region upper bound
εmax = εmin + εstep, and then run the veriﬁcation process for
this sub-region. If the veriﬁer cannot ﬁnd a solution in the
current sub-region, we move on to the next one, by setting
εmin += εstep and εmax += εstep, until the entire admissible
region is covered. As such, we generally obtain a tighter
interval for the perturbed input variable (taking p = 1, for
instance):

0 ≤ εmin ≤ (cid:107)x(cid:48) − x(cid:107)1 = (cid:107)δ (cid:107)1 ≤ εmax ≤ ε

⇒ εmin ≤

D
∑
d=1

|δd| ≤ εmax

⇒ 0 ≤ |δd| ≤ εmax
⇒ − εmax ≤ δd ≤ εmax
(cid:104)
x(cid:48)
⇒ x(cid:48)
d, x(cid:48)
d

d ∈

(cid:105)

= (cid:2)xd − εmax, xd + εmax

(22)

(23)

(24)

(25)

(26)

(cid:3),

where d = 1, . . . , D. As seen, the tightness of the variable
intervals depends on the value of εmax. If εmax grows towards
ε,
the variable intervals shall fall back to the original
formulations. Nonetheless, as said, considering adversarial
examples often appear closely around the original data
point, we conjecture that a small region would have already
contained one of them, offering a high possibility for a quick
solution.

We summarize the progressive procedure in Algorithm 2.
Speciﬁcally, we use Gurobi 9.5 [20] as the solver for the
encoded MIQCP M and denote in Line 3 the returned
interim results from the solver, namely optimization status s
(optimal, timeout or infeasible), solution count n, objective
ob j, counterexample x(cid:48), solving gap α and execution time
texec. The status returned can be optimal (OPT), satisﬁed
(SAT), undetermined (UNDTM) and unsatisﬁed (UNSAT).
Additionally, to prevent the veriﬁer from executing unbound-
edly, we place a time limit tlimit on the overall algorithm.
Lastly, apart from fastening the veriﬁcation process, another
advantage of such a progressive procedure is that one can
still attain a good lower bound of the optimal objective if a
timeout occurs (Algorithm 2, Line 9).

V. EXPERIMENT RESULTS AND DISCUSSIONS
This section presents the experimental results of the pro-
posed method and techniques. We ﬁrst introduce the Lane
Departure Warning (LDW) task,
then show an accuracy
benchmark, and present an ablation study on the heuristics
and a robustness comparison in the end.

A. Lane Departure Warning

4This technique applies to both softmax and sparsemax.
5We write again in vector form only and omit the division over

√

DH

here as it is simply a constant.

We regard LDW, an industry-oriented use case, which is
fundamentally a time-series joint classiﬁcation and regres-
sion task [21]. For training and evaluating the NNs, we utilize

Algorithm 2 Verify with partitioning and a time limit

1: procedure VERIFYMODEL(

f , x, gt(x), ε, εstep, εmin, εmax,tlimit )

Encode f , x, gt(x) in MIQCP M as formulated
in (10)-(13) for the sub-region of εmin and εmax
Solve M with Gurobi under tlimit and obtain interim
results s, n, ob j, x(cid:48), gap,texec
if s is optimal then

return (OPT, ob j, x(cid:48))

else if s is timeout and n > 0 then

return (SAT, ob j, x(cid:48), α)

else if s is timeout and n = 0 then

return (UNDTM, εmin)
else if s is infeasible then

tlimit -= texec
εmin += εstep
εmax = min(εmax+εstep, ε)
if εmin > ε then

return (UNSAT)

else

return VERIFYMODEL(

2:

3:

4:
5:

6:
7:
8:
9:
10:
11:

12:
13:
14:
15:
16:

17:

f , x, gt(x), ε, εstep, εmin, εmax,tlimit )

end if

18:
end if
19:
20: end procedure
21: Input: f , x, gt(x), ε, εstep, tlimit
22: Initialize: εmin ← 0, εmax ← εstep
23: result =

TABLE I: Accuracy of various NN architectures. L denotes the
number of layers in the network as deﬁned in Section III. For layer
normalization LN, 2 denotes the quadratic variant, 1 denotes the
linear variant, and 0 means there is no layer normalization.

Network Activation

LN

ATN

MLP

sparsemax

softmax

ReLU

Tanh

2
1
0
2
1
0
2
1
0
2
1
0

L = 1

L = 2

CLS

CLS

REG

REG
98.01% 87.24% 98.50% 91.49%
98.31% 88.46% 98.52% 91.39%
98.34% 90.38% 98.54% 92.31%
98.01% 87.11% 98.55% 91.88%
97.95% 87.56% 98.61% 91.52%
98.21% 88.03% 98.64% 91.21%
97.62% 83.81% 97.81% 84.70%
97.46% 83.55% 97.89% 85.04%
97.68% 84.27% 97.92% 84.81%
97.80% 83.20% 97.99% 84.63%
97.32% 82.93% 97.69% 84.81%
97.23% 83.02% 97.85% 85.14%

MLPs, we replace MSA of the ATN with another MLP
of hidden-layer dimension DMLP = 16, resulting in similar
numbers of network parameters. We implement the NNs with
PyTorch [23], train them using the Adam optimizer and a
ﬁxed learning rate of 0.003 for 50 epochs, and report the
best results in Table I.

As observed, even for a relatively small application (con-
sidering the variable dimensions), the ATNs generally per-
form better than MLPs in terms of accuracy. Additionally,
networks with piece-wise linear activation functions (i.e.,
sparsemax and ReLU) are on a par with, if not stronger
than, the ones with softmax or Tanh.

VERIFYMODEL( f , x, gt(x), ε, εstep, εmin, εmax,tlimit )

24: Output: The resulting tuple result

C. Ablation Study

the High-D dataset6 [22], a drone-recorded bird’s-eye-view
highway driving dataset. For each vehicle in the recordings,
we process raw data into trajectory information, including
the past 10 steps of time-wise features x ∈ R10×14 (spanning
across one second), the direction gtCLS ∈ {0, 1, 2} of the
lane departure (where 0 is no departure, 1 a left departure,
and 2 a right departure), and the time gtREG ∈ [0, 1] to the
lane departure. More speciﬁcally, the 14 features include left
and right lane existence (2), ego distance to lane center (1),
longitudinal and latitudinal velocities and accelerations (4),
and time-to-collision to preceding, following, and alongside
vehicles (7). Based on these features, the NN’s task is to pre-
dict a potential lane departure direction (i.e., classiﬁcation)
and timing (i.e., regression) up to one second in the future.

B. Accuracy Benchmark

As a side experiment, we evaluate the accuracy of dif-
ferent NNs. For classiﬁcation, the model is accurate if its
predicted direction matches the ground-truth direction; for
regression, it is accurate if the predicted timing falls within
0.1 second from the ground-truth timing. For the ATNs, we
set H = 2, DH = 4, DMLP = 8 (as per Section III). For the

6The utilization of the High-D dataset in this paper is for knowledge

dissemination and scientiﬁc publication and is not for commercial use.

We now conduct an ablation study on the acceleration
heuristics described in Section IV-C, using the sparsemax-
based ATN with L = 1 and LN = 1. Additionally, we imple-
ment two heuristics offered by Gurobi, namely variable hints
and branching priorities [20]. The former provides the solver
a better foundation in searching, whereas the latter regulates
the order of the integer variables selected for branching
when their values are still fractional. Concretely, we run the
Projected Gradient Descent (PGD) attack [24] and feed an
adversarial example, if it exists, into the solver as variable
hints. Then, we set integer variables stemming from the
earlier layers of the NN to have higher branching priorities.
These two techniques have been found helpful by the prior
art [6], [25].

During our veriﬁcation, we only allow the ﬁnal token
of the input variable x to be perturbed, resulting in an
encoded MIQCP with roughly 4000 linear constraints, 700
quadratic constraints, 400 general constraints (e.g., max or
absolute operations) and 2500 binary variables. We test the
heuristics with ﬁve random samples from the curated dataset
and summarize the results in Table II. All experiments are run
with an Intel i9-10980XE CPU @ 3.0GHz using 18 threads
and 20 GB of RAM.

It is observed that the novel activation bounding technique
is effective. Additionally, progressive veriﬁcation with norm-
space partitioning further reduces veriﬁcation time, as ex-
pected. However, the best epsilon step might vary among test

TABLE II: Ablation study on the considered acceleration heuristics, including interval analysis without and with sparsemax activation
bounding (IA and IA-σ ) and norm-space partitioning with different epsilon steps (RP-0.001, RP-0.005 and RP-0.01). We set ε = 0.05 in
all experiments. We mark the best-performing numbers of each sub-group in italic fonts if they are better than the control and further
mark the best-performing numbers across all heuristics in bold fonts. For the listed random samples, we specify optimally solved cases
as OPT and augment undetermined cases (UNDTM) with a lower bound of the minimum adversarial distortion (better if higher) and
satisﬁed cases (SAT) with the MIQCP solution gap (better if lower).

Techniques

Time elpased (s)

Nodes explored

Random samples

Mean

Best Worst

Mean

Best

Worst

No. 1

No. 2

No. 3

Control 2367.14

89.63 3602.50 9326275 118329 16395255

(UNDTM, 0.0)

(SAT, 1.0) OPT

IA 1703.65 330.26 3605.29 5370138 911639 13103193
950.37 127.33 2936.72 3061462 247916 10298216

IA-σ

(UNDTM, 0.0)
OPT

RP-0.001
RP-0.005
RP-0.01

IA-σ +RP-0.001
IA-σ +RP-0.005
IA-σ +RP-0.01

755.66
278.20
793.76

852.67
296.90
222.68

7.08 3609.83
849968
15.41 1276.86
276310
51.39 1556.72 1825979

5.36 3608.60 1689582
505555
7.74 1413.32
494973
644.10

59.56

11517
13734
70243

14557
7156
99642

4058680 (UNDTM, 0.005)
1255818
OPT
OPT
3429947

6573066 (UNDTM, 0.004)
OPT
2413044
1479260
OPT

OPT OPT
OPT OPT

OPT OPT
OPT OPT
OPT OPT

OPT OPT
OPT OPT
OPT OPT

cases as we see its magnitude does not necessarily correlate
to the best-case veriﬁcation speed. A reason behind this
might be the excessive number of MIP instances created by
smaller epsilon steps. Lastly, combining interval analysis and
progressive veriﬁcation delivers a speedup of approximately
an order of magnitude.

D. NN Robustness Comparisons

For robustness comparisons, we ﬁrst verify and compare
the robustness of the sparsemax-based ATN and the ReLU-
based MLP of similar sizes, which have recorded 98.31% and
97.46% in accuracy for classiﬁcation in Section V-B7. We set
ε = 0.03 and p = 1 for the admissible (cid:96)p-ball8 and enable
IA-σ +RP-0.01 from the previous section during veriﬁcation.
We collect results from 60 random data points (on which
both the ATN and MLP predict correctly before perturbation)
and plot the robustness comparison diagram in Fig. 3. We
ﬁrst observe that the data samples are relatively distributed
across the upper and lower triangles. In addition, a dense
cluster is lying in the upper left corner, indicating that the
MLP demonstrates a much larger resilience than the ATN
for a signiﬁcant portion of the data.

We further train and verify the robustness of ﬁve ATNs
and ﬁve MLPs, each on 20 data points, giving 100 samples
for each NN type. For faster veriﬁcation, we set ε = 0.01 as
the threshold for being robust and surprisingly ﬁnd that all
MLPs are veriﬁed robust on all data points. In contrast, the
ATNs are veriﬁed robust on 69 data points and have a mean
robustness value of 0.0041 (excluding the robust ones). The
ﬁnding is opposed to the observations made by some related
work on vision tasks [26], [27]. It is, therefore, believed
that NNs generally perform differently in diverse domain

7Verifying the MLP follows similar steps in Section IV except that the
encoding is relatively more straightforward and can be solved by Mixed
Integer Linear Programming (MILP).

8The admissible perturbation region can be derived from input feature
values analytically for better physical interpretability. For example, we can
set ε as the normalized value of ego car lateral acceleration, considering it a
decisive feature for LDW. Perturbations in this context can stem from sensor
noises or hardware faults. Lastly, the binary features (i.e., lane existence)
are not perturbed.

≥ 0.03

P
L
M

0.02

0.01

0

0.01

0.02

≥ 0.03

ATN

exact
lower

Fig. 3: Robustness of the ATN and MLP on random data points
(better if larger). The dashed line separates regions where ATN
or MLP gives a better robustness performance. Since verifying the
ATN on some data points still takes much time, we report the lower
bounds of the robustness values for data points requiring more than
one hour to verify. This means that the points marked by “lower”
can be further pushed to the right if the veriﬁer is given more time.

tasks. As a result, it is necessary to conduct thorough studies
and give rigorous guarantees on NN accuracy and robustness
before deploying NN-based applications.

VI. CONCLUSION

This paper works towards exact robustness veriﬁcation for
ATNs. We focus on the sparsemax-based ATNs, encode
them into a MIQCP problem, and propose accelerating
heuristics for solving. When applied, our proposals fasten
the veriﬁcation process roughly one order of magnitude. We
conduct experiments with a lane departure warning system
and discover that ATNs are less robust than MLPs.

Still, there are certain limitations in our work. First, we
focus only on small-scale networks (approximately 1600
neurons) and datasets (with input of 14 × 10 dimensions).
Whether our observations remain true for larger-scale net-
works and datasets is yet to be explored. Second, the paper
only examines point-wise robustness veriﬁcation for NNs.
Such analyses can be combined with systematic sampling
and testing methods to give formal and statistical guaran-
tees on safety-critical applications. Lastly, our veriﬁcation
requires ground truths and works only in design time. How
to utilize the studied techniques in a run-time setting is an
open question.

REFERENCES

[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in
NeurIPS, 2017.

[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” in ICLR, 2021.

[3] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harness-

ing adversarial examples,” in ICLR, 2015.

[4] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
neural networks,” IEEE Trans. Evol. Comput., vol. 23, p. 828–841,
2019.

[5] A. Lomuscio and L. Maganti, “An approach to reachability analysis

for feed-forward relu neural networks,” 2017.

[6] C.-H. Cheng, G. N¨uhrenberg, and H. Ruess, “Maximum resilience of

artiﬁcial neural networks,” in AVTA, 2017.

[7] V. Tjeng, K. Xiao, and R. Tedrake, “Evaluating robustness of neural

networks with mixed integer programming,” in ICLR, 2019.

[8] R. Ehlers, “Formal veriﬁcation of piece-wise linear feed-forward

neural networks,” in AVTA, 2017.

[9] G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer, “Relu-
plex: An efﬁcient smt solver for verifying deep neural networks,” in
CAV, 2017.

[10] M. Everett, G. Habibi, and J. P. How, “Robustness analysis of
neural networks via efﬁcient partitioning with applications in control
systems,” IEEE Control Syst. Lett., vol. 5, pp. 2114–2119, 2021.
[11] S. Wang, H. Zhang, K. Xu, X. Lin, S. Jana, C.-J. Hsieh, and J. Z.
Kolter, “Beta-crown: Efﬁcient bound propagation with per-neuron split
constraints for complete and incomplete neural network veriﬁcation,”
2021.

[12] E. Wong and J. Z. Kolter, “Provable defenses against adversarial
examples via the convex outer adversarial polytope,” in ICML, 2018.
[13] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri,
and M. Vechev, “Ai2: Safety and robustness certiﬁcation of neural
networks with abstract interpretation,” in SP, 2018.

[14] X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M. Wu,
and X. Yi, “A survey of safety and trustworthiness of deep neural

networks: Veriﬁcation, testing, adversarial attack and defence, and
interpretability,” Comput. Sci. Rev., vol. 37, p. 100270, 2020.

[15] Z. Shi, H. Zhang, K.-W. Chang, M. Huang, and C.-J. Hsieh, “Robust-

ness veriﬁcation for transformers,” in ICLR, 2020.

[16] G. Bonaert, D. I. Dimitrov, M. Baader, and M. Vechev, “Fast and

precise certiﬁcation of transformers,” in PLDI, 2021.

[17] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang,
Y. Lan, L. Wang, and T.-Y. Liu, “On layer normalization in the
transformer architecture,” in ICLR, 2020.

[18] A. F. T. Martins and R. F. Astudillo, “From softmax to sparsemax:
A sparse model of attention and multi-label classiﬁcation,” in ICML,
2016.

[19] I. E. Grossmann, “Review of nonlinear mixed-integer and disjunctive

programming techniques,” Optim. Eng., vol. 3, pp. 227–252, 2002.

[20] Gurobi Optimization, LLC, “Gurobi optimizer reference manual,”

2021.

[21] V. Mahajan, C. Katrakazas, and C. Antoniou, “Prediction of lane-
changing maneuvers with automatic labeling and deep learning,” TRR
Journal, vol. 2674, pp. 336–347, 2020.

[22] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, “The highd dataset:
A drone dataset of naturalistic vehicle trajectories on german highways
for validation of highly automated driving systems,” in ITSC, 2018.

[23] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-
performance deep learning library,” in NeurIPS, 2019.

[24] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards deep learning models resistant to adversarial attacks,” in
ICLR, 2018.

[25] Y. Zhu, F. Wang, W. Wan, and M. Zhang, “Attack-guided efﬁcient
robustness veriﬁcation of relu neural networks,” in IJCNN, 2021.
[26] S. Bhojanapalli, A. Chakrabarti, D. Glasner, D. Li, T. Unterthiner,
and A. Veit, “Understanding robustness of transformers for image
classiﬁcation,” in ICCV, 2021.

[27] R. Shao, Z. Shi, J. Yi, P.-Y. Chen, and C.-J. Hsieh, “On the adversarial

robustness of vision transformers,” in UCCV, 2021.

