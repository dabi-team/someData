Preprint
scikit-dimension: a Python package for intrinsic dimension
estimation

Jonathan Bac 1,2,3*, Evgeny M. Mirkes 4,5

, Alexander N. Gorban 4,5

, Ivan Tyukin 4,5

and Andrei Zinovyev1,2,3*

1
2
0
2

p
e
S
6

]

G
L
.
s
c
[

1
v
6
9
5
2
0
.
9
0
1
2
:
v
i
X
r
a

1

2

Institut Curie, PSL Research University, Paris, France
INSERM, U900, Paris,France

3 CBIO-Centre for Computational Biology, Mines ParisTech, PSL Research University, Paris, France
4 Department of Mathematics, University of Leicester, Leicester, UK
5

Lobachevsky University, Nizhniy Novgorod, Russia

* Correspondence: jonathan.bac@cri-paris.org (J,B.); andrei.zinovyev@curie.fr (A.Z.); Tel.: +33-156-246-989 (A.Z.)

Abstract: Dealing with uncertainty in applications of machine learning to real-life data critically depends
on the knowledge of intrinsic dimensionality (ID). A number of methods have been suggested for the
purpose of estimating ID, but no standard package to easily apply them one by one or all at once
has been implemented in Python. This technical note introduces scikit-dimension, an open-source
Python package for intrinsic dimension estimation. scikit-dimension package provides a uniform
implementation of most of the known ID estimators based on scikit-learn application programming
interface to evaluate global and local intrinsic dimension, as well as generators of synthetic toy and
benchmark datasets widespread in the literature. The package is developed with tools assessing
the code quality, coverage, unit testing and continuous integration. We brieﬂy describe the package
and demonstrate its use in a large-scale (more than 500 datasets) benchmarking of methods for ID
estimation in real-life and synthetic data. The source code is available at https://github.com/j-bac/
scikit-dimension, the documentation is available at https://scikit-dimension.readthedocs.io/.

Keywords: Intrinsic dimension; Effective dimension; Python package; method benchmarking.

1. Introduction

We present scikit-dimension, an open-source Python package for global and local
intrinsic dimension (ID) estimation. The package has two main objectives: (i) foster research
in ID estimation by providing code to benchmark algorithms and a platform to share algo-
rithms; (ii) democratize the use of ID estimation by provigin user-friendly implementations of
algorithms using Scikit-Learn application programming interface (API) [1].

ID intuitively refers to the minimum number of parameters required to represent a
dataset with satisfactory accuracy. The meaning of “accuracy" can be different among various
approaches. ID can be more precisely deﬁned to be n if the data lies closely to a n-dimensional
manifold embedded in Rd with little information loss, which corresponds to the so called
“manifold hypothesis" [2,3]. ID can be, however, deﬁned without assuming the existence
of a data manifold. In this case, data point cloud characteristics (e.g., linear separability or
pattern of covariance) are compared to a model n-dimensional distribution (e.g., uniformly
sampled n-sphere or n-dimensional isotropic Gaussian distribution) and the term “effective
dimensionality" is sometimes used as such n which gives most similar characteristics to the
one measured in the studied point cloud [4,5]. In scikit-dimension, these two notions are
not distinguished.

The knowledge of ID is important to determine the choice of machine learning algorithm
and anticipate the uncertainty of its predictions. The well-known curse of dimensionality,
which states that many problems become exponentially difﬁcult in high dimensions, does
not depend on the number of features but on the dataset’s ID [6]. More precisely, the effects

 
 
 
 
 
 
2 of 12

of dimensionality curse are expected to be manifested when ID (cid:29) ln(M), where M is the
number of data points [7,8].

Current ID estimators have diverse operating principles (we refer the reader to [9] for an
overview). Each ID estimator is developed based on a selected feature (such as number of
data points in a sphere of ﬁxed radius, linear separability or expected normalized distance to
the closest neighbour) which scales with n: therefore, various ID estimation methods provide
different ID values. Each dataset can be characterized by a unique dimensionality proﬁle of ID
estimations according to different existing methods, which can serve as an important signature
for choosing the most appropriate data analysis method.

Dimensionality estimators that provide a single ID value for the whole dataset belong
to the category of global estimators. However, datasets can have complex organizations and
contain regions with varying dimensionality [7]. In such a case, they can be explored using
local estimators, which estimate ID in local neighborhoods around each point. The neighbour-
hoods are typically deﬁned by considering the k closest neighbours. Such approaches also
allow repurposing global estimators as local estimators.

The idea behind local ID estimation is to operate at a scale where the data manifold
can be approximated by its tangent space [10].
In practice, ID is sensitive to scale and
choosing neighbourhood size is a trade-off between opposite requirements [9,11]: ideally, the
neighbourhood should be big relative to the scale of the noise, and contain enough points.
At the same time, it should be small enough to be well approximated by a ﬂat and uniform
tangent space.

We perform benchmarking of 19 ID estimators on a large collection of real-life and
synthetic datasets. Previously, estimators were benchmarked based mainly on artiﬁcial
datasets representing uniformly sampled manifolds with known ID [4,9,12], comparing them
for the ability to estimate the ID value correctly. Several ID estimators were used on real-life
datasets to evaluate the degree of dimensionality curse in a study of various metrics in data
space[13]. Here we benchmark ID estimation methods focusing on their applicability to a wide
range of datasets of different origin, conﬁguration and size. We also look at how different
ID estimations are correlated, and show how scikit-dimension can be used to derive a
consensus measure of data dimensionality, by averaging multiple individual measures. The
latter can be a robust measure of data dimensionality in various applications.

scikit-dimension was applied in several recent studies for estimating intrinsic dimen-

sionality of real-life datasets [14,15].

2. Materials and Methods
2.1. Software features

scikit-dimension consists of two modules. The id module provides ID estimators and

the datasets module provides synthetic benchmark datasets.

2.1.1. id module

The id module contains estimators based on:

•
correlation (fractal) dimension (id.CorrInt) [16]
• manifold-adaptive fractal dimension (id.MADA) [17]
• method of moments (id.MOM) [18]
•
• maximum likelihood (id.MLE) [22–24]
• minimum spanning trees (id.KNN) [25]
•

principal component analysis (id.lPCA) [3,19–21]

estimators based on concentration of measure (id.MiND_ML, id.DANCo, id.ESS, id.TwoNN,
id.FisherS, id.TLE) [4,26–31].

3 of 12

The description of the method principles is provided together with the package docu-

mentation at https://scikit-dimension.readthedocs.io/ and in reviews [5,7,12].

2.1.2. datasets module

The datasets module allows user to test estimators on synthetic datasets. It can generate
several low-dimensional toy datasets to play with different estimators as well as a set of
synthetic manifolds commonly used to benchmark ID estimators, introduced by [12] and
further extended in [9,26].

Figure 1. Example usage: generating the Line-Disk-Ball dataset [8]) which has clusters of varying local ID, and coloring points
by estimates of local ID obtained by id.lPCA.

2.2. Development

scikit-dimension is built according to the scikit-learn API [1] with support for Linux,
MacOS, and Windows and Python >= 3.6. Code style and API design is based on the
guidelines of scikit-learn, with NumPy [32] documentation format, and continuous integration
on all three platforms. The online documentation is built using Sphinx and hosted with
ReadTheDocs.

2.3. Dependencies

scikit-dimension depends on a limited number of external dependencies on the user

side for ease of installation and maintenance:

• Matplotlib [33]
Pandas [34]
•
•
Scikit-learn [1]
• Numba [35]
•
• NumPy [32]

SciPy [36]

2.4. Related software

Related open source software for ID estimation have previously been developed in
different languages such as R, MATLAB or C++ and contributed to the development of
scikit-dimension.

4 of 12

In particular, [8,37,38], provide extensive collections of ID estimators and datasets for R
users, with [38] additionally focusing on dimension reduction algorithms. Similar resources
can be found for MATLAB users [39–42]. Benchmarking many of the methods for ID esti-
mation included in this package was performed in [13]. Finally there exist several packages
implementing standalone algorithms; in particular for Python, we refer the reader to com-
plementary implementations of the GeoMLE, full correlation dimension, GraphDistancesID
algorithms [43–45].

To our knowledge, scikit-dimension is the ﬁrst Python implementation of an extensive
collection of ID methods. Compared to similar efforts in other languages, the package puts
emphasis on estimators quantifying various properties of high-dimensional data geometry
such as concentration of measure. It is the only package to include ID estimation based on
linear separability of data, using Fisher discriminants [4,30,46,47].

3. Results
3.1. Benchmarking scikit-dimension on a large collection of datasets

In order to demonstrate applicability of scikit-dimension to a wide range of real-life
datasets of various conﬁgurations and sizes, we performed a large-scale benchmarking of
scikit-dimension using the collection of datasets from OpenML repository [48]. We selected
those datasets having at least 1000 observations and 10 features, without missing values. We
excluded those datasets which were difﬁcult to fetch either because of their size or an error in
the OpenML API. After ﬁltering out repetitive entries, 499 datasets were collected. The number
of observations in them varied from 1002 to 9199930 and the number of features varied from
10 to 13196. We focused only on numerical variables, and we subsampled the number of rows
in the matrix to maximum 100000. All dataset features were scaled to unit interval using
Min/Max scaler in Python. In addition, we ﬁltered out approximate non-unique columns and
rows in the data matrices since some of the ID methods could be affected by the presence of
identical (or approximately identical) rows or columns.

We added to the collection 18 datasets, containing single-cell transcriptomic measure-
ments, from the CytoTRACE study[49] and 4 largest datasets from The Cancer Genome
Atlas (TCGA), containing bulk transcriptomic measurements. Therefore, our ﬁnal collection
contained 521 datasets altogether.

3.1.1. scikit-dimension ID estimator method features

We systematically applied 19 ID estimation methods from scikit-dimension, with
default parameter values, including 7 methods based on application of principal component
analysis ("linear" or PCA-based ID methods), and 12 based on application of various other
principles, including correlation dimension and concentration of measure-based methods
("nonlinear" ID methods).

For KNN and MADA methods we had to further subsample the data matrix to maximum
20000 rows, otherwise they were too greedy in terms of memory consumption. Moreover,
DANCo and ESS methods appeared to be too slow, especially in the case of a large number of
variables: therefore, we made ID estimations in these cases on small fragments of data matrices.
Thus, for DANCo the maximum matrix size was set to 10000x100, and for ESS to 2000x20.
The number of features was reduced for these methods when needed, by using PCA-derived
coordinates, and the number of observations were reduced by random subsampling.

In the Table 1, we provide the summary of characteristics of the tested methods. In more

detail, the following method features have been evaluated (see Figure 2).

Firstly, we simply looked at the ranges of ID values produced by the methods across all
the datasets. These ranges varied signiﬁcantly between the methods, especially for the linear
ones (Figure 2,A).

5 of 12

Secondly, we tested the methods with respect to their ability to successfully compute ID
as a positive ﬁnite value. It appeared that certain methods (such as MADA and TLE), in a
certain number of cases produced a signiﬁcant fraction of uninterpretable estimates (such as
“nan" or negative value), Figure 2,B. We assume that in most of such cases, the problem with
ID estimation is caused by the method implementation, not anticipating certain relatively rare
data point conﬁgurations, rather than the methodology itself, and a reasonable ID estimate
always exists. Therefore, in case of a method implementation returning uninterpretable value,
for further analysis, we considered it possible to impute the ID value from the results of
application of other methods, see below.

Table 1: Summary table of ID methods characteristics. The qualitative score changes from "- - -" (worst) to "+++" (best).

Method
name

Short
name(s)

Ref(s)

Valid
result

Insensitivity
to
dancy

redun-

Uniform
ID estimate
in similar
datasets

Performance
with many
observations

Performance
with many
features

di-

par-

PCA
Fukunaga-
Olsen
PCA Fan
PCA maxgap
PCA ratio
PCA
ticipation
ratio
PCA Kaiser
PCA broken
stick
Correlation
(fractal)
mensionality
Fisher separa-
bility
K-nearest
neighbours
Manifold-
adaptive
fractal dimen-
sion
Minimum
neighbor dis-
tance—ML
Maximum
likelihood
Methods
moments
Estimation
within tight
localities
Minimal
neighbor-
hood informa-
tion
Angle
and
norm concen-
tration
Expected sim-
plex skewness

of

PCA FO, PFO [13,20]

+++

PFN
PMG
PRT
PPR

PKS
PBS

[21]
[52]
[53]
[53]

[50,54]
[51,55]

CorrInt, CID

[16]

FisherS, FSH

[4,30]

KNN

[25]

MADA, MDA [17]

MIND_ML,
MMk,
MMi
MLE

MOM

TLE

TwoNN,
TNN

DANCo,
DNC

ESS

[26]

[23]

[18]

[31]

[29]

[27]

[52]

+++
+++
+++
+++

+++
+++

+

++

++

-

+++

++

+++

- -

++

+

+++

+++

+++
- - -
+++
+++

-
- -

+++

+++

- -

+++

+++

+++

+++

+++

+++

+++

+++

+++

+++
+
+
++

+++
+++

++

+++

- -

+++

++

++

+++

+++

+++

+++

+++

+++

+++
+++
+++
+++

+++
+++

+

++

-

-

++

++

++

++

++

- - -

- - -

+++

+++
+++
+++
+++

+++
+++

+

+++

++

+

+

+

+

+

+++

- - -

- - -

6 of 12

Thirdly, for a small number of datasets we performed a test of their sensitivity to the
presence of strongly redundant features. For this purpose, we duplicated all features in a
matrix and recomputed ID. The resulting sensitivity is the ratio between the ID computed for
the larger matrix and the ID computed for the initial matrix, having no duplicated columns. It
appears that despite most of the methods being robust with respect to such matrix duplication,
some (such as PCA-based broken stick or the famous Kaiser methods popular in various ﬁelds
such as biology [50,51]) tend to be very sensitive, Figure 2,C, which is compliant with some
previous reports [13].

Some of the datasets in our collection could be combined in homogeneous groups accord-
ing to their origin, such as the data coming from Quantitative structure–activity relationship
(QSAR)-based quantiﬁcation of a set of chemicals. The size of the QSAR ﬁngerprint for the
molecules is the same in all such datasets (1024 features): therefore, we could assume that
the estimate of ID should not vary too much across the datasets from the same group. We
computed the coefﬁcient of variation of ID estimate across three such dataset groups, which
revealed that certain methods tend to provide less stable estimations than the others, Figure
2,D.

Finally, we recorded the computational time needed for each method. We found that the
computational time could be estimated with good precision (R2 > 0.93 for all ID estimators)
using multiplicative model: Time = c × Nα
var, where Nobj and Nvar are number of
objects and features in a dataset, correspondingly. Using this model ﬁt for each method, we
estimated the time needed to estimate ID for data matrices of four characteristic sizes, Figure
2,E.

obj × Nβ

Figure 2. Illustrating different ID method general characteristics: A) range of estimated ID values; B) ability to produce
interpretable (positive ﬁnite value) result; C) sensitivity to feature redundancy (after duplicating matrix columns); D) uniform
ID estimation across datasets of similar nature; E) computational time needed to compute ID for matrices of four characteristic
sizes.

7 of 12

Figure 3. Characterizing OpenML dataset collection in terms of ID estimates. A) PCA visualizations of datasets characterized by
vectors of 19 ID measures. Size of the point corresponds to the logarithm of the number of matrix entries (Nobj × Nvar). The
color corresponds to the mean ID estimate taken as the mean of all ID measure z-scores. B) Loadings of various methods into
the ﬁrst and the second principal component from A). C) Visualization of the mean ID score as a function of data matrix shape.
The color is the same as in A). D) Correlation matrix between different ID estimates computed over all analyzed datasets.

3.1.2. scikit-dimension ID estimates metanalysis

After application of scikit-dimension, each dataset was characterized by a vector of
19 measurements of intrinsic dimensionality. The resulting matrix of ID values contained
2.5% missing values which were imputed using the standard IterativeImputer from sklearn
Python package.

8 of 12

Figure 4. A gallery of UMAP plots computed for a selection of datasets from OpenML collection, with indication of ID estimates,
ranked by the ID value estimated using Fisher separability-based method (indicated in the left top corner). The ambient
dimension of the data (number of features Nvar) is indicated in the bottom left corner, and the color reﬂects the ID/Nvar ratio,
from red (close to 0.0 value) to green (close to 1.0). On the right from the UMAP plot, all 19 ID measures are indicated, with
color mapped to the value range, from green (small dimension) to red (high dimension).

Using the imputed matrix and scaling it to z-scores, we performed principal component
analysis (Figure 3,A,B). The ﬁrst principal component explained 42.6% percent of the total

9 of 12

variance in ID estimations, with all of the methods having positive and comparable loadings
to the ﬁrst principal component. This justiﬁes the computation of the “consensus" intrinsic
dimension measure, which we deﬁne here as the mean value of individual ID estimate z-
scores. Therefore, the mean ID can take negative or positive values, roughly dividing the
datasets into “lower-dimensional" and “higher-dimensional" (Figure 3,A,C). The consensus
ID estimate weakly negatively correlated with with the number of observations (Pearson
ρ = −0.25, p-value=10−9) and positively correlated with the number of features in the dataset
(r=0.44, p-value=10−25). Nevertheless, even for the datasets with similar matrix shapes, the
mean ID estimate could be quite different (Figure 3,C).

The second principal component explained 21.3% of the total variance in ID estimates.
The loadings of this component roughly differentiated between PCA-based ID estimates and
“non-linear" ID estimation methods, with one exception in the case of the KNN method.

We computed the correlation matrix between the results of application of different ID
methods (Figure 3,D), which also distinguished two large groups of PCA-based and “non-
linear" methods. Furthemore, non-linear methods were split into the group of methods produc-
ing results similar to correlation (fractal) dimension (CorrInt, MADA, MOM, TwoNN, MLE,
TLE) and methods based on concentration of measure phenomena (FisherS, ESS, DANCo,
MiND_ML).

In order to illustrate the relation between the dataset geometry and the intrinsic dimen-
sion, we produced a gallery of Uniform Manifold Approximation and Projection (UMAP)
dataset visualizations, with indication of the ambient dataset dimension (number of features)
and the estimated ID using all methods, Figure 4. One of the conclusions that can be made
from this analysis is that UMAP visualization is not insightfull for truely high-dimensional
datasets (starting from ID=10 estimated by FisherS method). Also, some datasets having large
ambient dimensions, were characterized by low ID, by most of the methods (e.g., ’hill-valley’
dataset).

4. Conclusions

scikit-dimension is the ﬁrst to our knowledge package implemented in Python, con-

taining implementations of most used estimators of data intrinsic dimensionality.

Benchmarking scikit-dimension on a large collection of real-life and synthetic datasets
revealed that different estimators of intrinsic dimensionality possess internal consistency and
that the ensemble of ID estimators allows us to achieve more robust classiﬁcation of datasets
into low- or high-dimensional.

Future releases of scikit-dimension will continuously seek to incorporate new estima-
tors and benchmark datasets introduced in the literature, or new features such as alternative
nearest neighbor search for local ID estimates. The package will also include new ID estima-
tors which can be derived using most recent achievements in understanding the properties of
high-dimensional data geometry[56,57].

Author Contributions: Conceptualization, J.B., A.Z., I.T., A.N.G.; methodology, J.B., E.M., A.N.G.;
software, J.B., E.M. and A.Z.; formal analysis, J.B., E.M., A.Z.; data curation, J.B. and A.Z.; writing—
original draft preparation, J.B. and A.Z.; writing—review and editing, all authors; supervision, A.Z. and
A.N.G. All authors have read and agreed to the published version of the manuscript.

Funding: The work was supported by the Ministry of Science and Higher Education of the Russian
Federation (Project No. 075-15-2021-634), by the French government under management of Agence Na-
tionale de la Recherche as part of the “Investissements d’Avenir" program, reference ANR-19-P3IA-0001
(PRAIRIE 3IA Institute), by the Association Science et Technologie, the Institut de Recherches Interna-
tionales Servier and the doctoral school Frontières de l’Innovation en Recherche et Education Programme
Bettencourt. I.T. was supported by the UKRI Turing AI Acceleration Fellowship (EP/V025295/1).

Data Availability Statement: The datasets used in this study were retrieved from public sources, namely
OpenML repository, CytoTRACE web-site https://cytotrace.stanford.edu/ (section "Downloads"), from
the Data Portal of National Cancer Institute https://portal.gdc.cancer.gov/.

Conﬂicts of Interest: The authors declare no conﬂict of interest.

10 of 12

References

1.

2.
3.

4.

5.
6.

7.

Intrinsic dimensionality extraction; in: P.R. Krishnaiah, L.N. Kanal (Eds.), Pattern Recognition and Reduction of

Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.;
others. Scikit-learn: Machine learning in Python. the Journal of machine Learning research 2011, 12, 2825–2830.
Bishop, C.M.; others. Neural networks for pattern recognition; Oxford university press, 1995. doi:10.5555/525960.
Fukunaga, K.
Dimensionality, Handbook of Statistics, Vol. 2, North-Holland, Amsterdam, 1982, pp. 347–362, 1982.
Albergante, L.; Bac, J.; Zinovyev, A. Estimating the effective dimension of large biological datasets using Fisher separability analysis.
2019 International Joint Conference on Neural Networks (IJCNN). IEEE, 2019, pp. 1–8.
Giudice, M.D. Effective Dimensionality: A Tutorial. Multivariate behavioral research 2020, pp. 1–16. doi:10.1080/00273171.2020.1743631.
Jiang, H.; Kim, B.; Guan, M.Y.; Gupta, M.R. To Trust Or Not To Trust A Classiﬁer.
5546–5557.
doi:10.5555/3327345.3327458.
Bac, J.; Zinovyev, A. Lizard Brain: Tackling Locally Low-Dimensional Yet Globally Complex Organization of Multi-Dimensional
Datasets. Frontiers in Neurorobotics 2020, 13. doi:10.3389/fnbot.2019.00110.

NeurIPS, 2018, pp.

8. Hino, H. ider: Intrinsic Dimension Estimation with R. The R Journal 2017, 9, 329–341. doi:10.32614/RJ-2017-054.
9.

Campadelli, P.; Casiraghi, E.; Ceruti, C.; Rozza, A. Intrinsic Dimension Estimation: Relevant Techniques and a Benchmark Framework.
Mathematical Problems in Engineering 2015, 2015, 1–21. doi:10.1155/2015/759567.

10. Camastra, F.; Staiano, A. Intrinsic dimension estimation: Advances and open problems.

Information Sciences 2016, 328, 26–41.

doi:10.1016/J.INS.2015.08.029.

11. Little, A.V.; Lee, J.; Jung, Y.; Maggioni, M. Estimation of intrinsic dimensionality of samples from noisy low-dimensional man-
ifolds in high dimensions with multiscale SVD. 2009 IEEE/SP 15th Workshop on Statistical Signal Processing, 2009, pp. 85–88.
doi:10.1109/SSP.2009.5278634.

12. Hein, M.; Audibert, J.Y. Intrinsic dimensionality estimation of submanifolds in Rd. Proceedings of the 22nd international conference

on Machine learning. ACM, 2005, pp. 289–296. doi:10.1145/1102351.1102388.

13. Mirkes, E.; Allohibi, J.; Gorban, A.N. Fractional Norms and Quasinorms Do Not Help to Overcome the Curse of Dimensionality.

Entropy (Basel, Switzerland) 2020, 22. doi:10.3390/e22101105.

14. Golovenkin, S.E.; Bac, J.; Chervov, A.; Mirkes, E.M.; Orlova, Y.V.; Barillot, E.; Gorban, A.N.; Zinovyev, A. Trajectories, bifurcations,
and pseudo-time in large clinical datasets: applications to myocardial infarction and diabetes data. GigaScience 2020, 9. giaa128,
doi:10.1093/gigascience/giaa128.

15. Zinovyev, A.; Sadovsky, M.; Calzone, L.; Fouché, A.; Groeneveld, C.S.; Chervov, A.; Barillot, E.; Gorban, A.N. Modeling Progression

of Single Cell Populations Through the Cell Cycle as a Sequence of Switches. bioRxiv 2021. doi:10.1101/2021.06.14.448414.

16. Grassberger, P.; Procaccia, I. Measuring the strangeness of strange attractors. Physica D: Nonlinear Phenomena 1983, 9, 189–208.

17.

doi:10.1016/0167-2789(83)90298-1.
Farahmand, A.M.; Szepesvári, C.; Audibert, J.Y. Manifold-adaptive dimension estimation. Proceedings of the 24th international
conference on Machine learning, 2007, pp. 265–272. doi:10.1145/1273496.1273530.

18. Amsaleg, L.; Chelly, O.; Furon, T.; Girard, S.; Houle, M.E.; Kawarabayashi, K.; Nett, M. Extreme-value-theoretic estimation of local

19.

20.

intrinsic dimensionality. Data Mining and Knowledge Discovery 2018, 32, 1768–1805.
Jackson, D.A. Stopping rules in principal components analysis: a comparison of heuristical and statistical approaches. Ecology 1993,
74, 2204–2214. doi:10.2307/1939574.
Fukunaga, K.; Olsen, D.R. An Algorithm for Finding Intrinsic Dimensionality of Data.
C-20, 176–183. doi:10.1109/T-C.1971.223208.

IEEE Transactions on Computers 1971,

21. Mingyu, F.; Gu, N.; Qiao, H.; Zhang, B. Intrinsic dimension estimation of data by principal component analysis. arXiv preprint

arXiv:1002.2050 2010.

22. Hill, B.M. A simple general approach to inference about the tail of a distribution. The annals of statistics 1975, pp. 1163–1174.

doi:10.1214/aos/1176343247.

23. Levina, E.; Bickel, P.J. Maximum Likelihood estimation of intrinsic dimension. Proceedings of the 17th International Conference on

Neural Information Processing Systems. MIT Press, 2004, pp. 777–784. doi:10.5555/2976040.2976138.

24. Haro, G.; Randall, G.; Sapiro, G. Translated poisson mixture model for stratiﬁcation learning. International Journal of Computer Vision

2008, 80, 358–374. doi:10.1007/s11263-008-0144-6.

11 of 12

25. Carter, K.M.; Raich, R.; Hero, A.O. On Local Intrinsic Dimension Estimation and Its Applications.

IEEE Transactions on Signal

Processing 2010, 58, 650–663. doi:10.1109/TSP.2009.2031722.

26. Rozza, A.; Lombardi, G.; Ceruti, C.; Casiraghi, E.; Campadelli, P. Novel high intrinsic dimensionality estimators. Machine learning

2012, 89, 37–65. doi:10.1007/s10994-012-5294-7.

27. Ceruti, C.; Bassis, S.; Rozza, A.; Lombardi, G.; Casiraghi, E.; Campadelli, P. DANCo: An intrinsic dimensionality estimator exploiting

28.

29.

angle and norm concentration. Pattern Recognition 2014, 47, 2569–2581. doi:10.1016/J.PATCOG.2014.02.013.
Johnsson, K. Structures in High-Dimensional Data: Intrinsic Dimension and Cluster Analysis. PhD thesis, Faculty of Engineering,
LTH, 2016.
Facco, E.; D’Errico, M.; Rodriguez, A.; Laio, A. Estimating the intrinsic dimension of datasets by a minimal neighborhood information.
Scientiﬁc Reports 2017, 7, 12140. doi:10.1038/s41598-017-11873-y.

30. Gorban, A.; Golubkov, A.; Grechuk, B.; Mirkes, E.; Tyukin, I. Correction of AI systems by linear discriminants: Probabilistic

foundations. Information Sciences 2018, 466, 303 – 322. doi:https://doi.org/10.1016/j.ins.2018.07.040.

31. Amsaleg, L.; Chelly, O.; Houle, M.E.; Kawarabayashi, K.; Radovanovi´c, M.; Treeratanajaru, W. Intrinsic dimensionality estimation

within tight localities. Proceedings of the 2019 SIAM International Conference on Data Mining. SIAM, 2019, pp. 181–189.

32. Harris, C.R.; Millman, K.J.; van der Walt, S.J.; Gommers, R.; Virtanen, P.; Cournapeau, D.; Wieser, E.; Taylor, J.; Berg, S.; Smith, N.J.;
Kern, R.; Picus, M.; Hoyer, S.; van Kerkwijk, M.H.; Brett, M.; Haldane, A.; del R’ıo, J.F.; Wiebe, M.; Peterson, P.; G’erard-Marchant,
P.; Sheppard, K.; Reddy, T.; Weckesser, W.; Abbasi, H.; Gohlke, C.; Oliphant, T.E. Array programming with NumPy. Nature 2020,
585, 357–362. doi:10.1038/s41586-020-2649-2.

33. Hunter, J.D. Matplotlib: A 2D graphics environment. Computing in Science & Engineering 2007, 9, 90–95. doi:10.1109/MCSE.2007.55.
34. pandas development team, T. pandas-dev/pandas: Pandas, 2020. doi:10.5281/zenodo.3509134.
35. Lam, S.K.; Pitrou, A.; Seibert, S. Numba: A llvm-based python jit compiler. Proceedings of the Second Workshop on the LLVM

Compiler Infrastructure in HPC, 2015, pp. 1–6. doi:10.1145/2833157.2833162.

36. Virtanen, P.; Gommers, R.; Oliphant, T.E.; Haberland, M.; Reddy, T.; Cournapeau, D.; Burovski, E.; Peterson, P.; Weckesser, W.; Bright,
J.; van der Walt, S.J.; Brett, W.; Wilson, J.; Millman, K.J.; Mayorov, N.; Nelson, A.R.J.; Jones, E.; Kern, R.; Larson, E.; Carey, C.J.;
Polat, I.; Feng, Y.; Moore, E.W.; VanderPlas, J.; Laxalde, D.; Perktold, J.; Cimrman, R.; Henriksen, I.; Quintero, E.A.; Harris, C.R.;
Archibald, A.M.; Ribeiro, A.H.; Pedregosa, F.; van Mulbregt, P.; SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for
Scientiﬁc Computing in Python. Nature Methods 2020, 17, 261–272. doi:10.1038/s41592-019-0686-2.
Johnsson, K. ider: Various Methods for Estimating Intrinsic Dimension (R package) (accessed Sept 6, 2021), 2019.

37.
38. You, K. Rdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation. arXiv preprint arXiv:2005.11107 2020.
39. M. Hein, J.Y.A. IntDim: intrindic dimensionality estimation https://www.ml.uni-saarland.de/code/IntDim/IntDim.htm (accessed

Sept 6, 2021), 2016.

40. Lombardi, G.

Intrinsic dimensionality estimation techniques (MATLAB package) https://fr.mathworks.com/matlabcentral/

41.

ﬁleexchange/40112-intrinsic-dimensionality-estimation-techniques (accessed Sept 6, 2021), 2013.
van der Maaten, L. drtoolbox: Matlab Toolbox for Dimensionality Reduction https://lvdmaaten.github.io/drtoolbox/ (accessed Sept
6, 2021), 2020.

42. Radovanovi´c, M. Tight Local Intrinsic Dimensionality Estimator (TLE) (MATLAB package) https://perun.pmf.uns.ac.rs/radovanovic/

tle/ (accessed Sept 6, 2021), 2020.

43. Gomtsyan, M.; Mokrov, N. Geometry-Aware Maximum Likelihood Estimation of Intrinsic Dimension (Python package) https:

//github.com/stat-ml/GeoMLE (accessed Sept 6, 2021), 2019.

44. Erba, V. pyFCI: A package for multiscale-full-correlation-integral intrinsic dimension estimation https://github.com/vittorioerba/

pyFCI (accessed Sept 6, 2021), 2019.

45. Granata, D. Intrinsic-Dimension (Python package) https://github.com/dgranata/Intrinsic-Dimension (accessed Sept 6, 2021), 2016.
46. Bac, J.; Zinovyev, A. Local intrinsic dimensionality estimators based on concentration of measure. 2020 International Joint Conference

on Neural Networks (IJCNN). IEEE, 2020, pp. 1–8.

47. Gorban, A.N.; Makarov, V.A.; Tyukin, I.Y. The unreasonable effectiveness of small neural ensembles in high-dimensional brain.

Physics of life reviews 2019, 29, 55–88. doi:10.1016/j.plrev.2018.09.005.

48. Vanschoren, J.; van Rijn, J.N.; Bischl, B.; Torgo, L. OpenML: Networked Science in Machine Learning. SIGKDD Explorations 2013,

15, 49–60. doi:10.1145/2641190.2641198.

49. Gulati, G.; Sikandar, S.; Wesche, D.; Manjunath, A.; Bharadwaj, A.; Berger, M.; Ilagan, F.; Kuo, A.; Hsieh, R.; Cai, S.; Zabala, M.;
Scheeren, F.; Lobo, N.; Qian, D.; Yu, F.; Dirbas, F.; Clarke, M.; Newman, A. Single-cell transcriptional diversity is a hallmark of
developmental potential. Science 2020, 24, 405–411. doi:10.1126/science.aax0249.

50. Giuliani, A. The application of principal component analysis to drug discovery and biomedical data. Drug Discovery Today 2017, 22(7).

doi:10.1016/j.drudis.2017.01.005.

51. Cangelosi, R.; Goriely, A. Component retention in principal component analysis with application to cDNA microarray data. Biology

Direct 2007, 2, 2. doi:10.1186/1745-6150-2-2.

12 of 12

52.

Johnsson, K.; Soneson, C.; Fontes, M. Low Bias Local Intrinsic Dimension Estimation from Expected Simplex Skewness. IEEE Trans.
Pattern Anal. Mach. Intell. 2015, 37(1), 196–202.
Jolliffe, I.T. Principal Component Analysis; Springer Series in Statistics, Springer, 2002.

53.
54. Kaiser, H. The Application of Electronic Computers to Factor Analysis. Educational and Psychological Measurement 1960, 20, 141 – 151.
Frontier, S. Étude de la décroissance des valeurs propres dans une analyse en composantes principales: Comparaison avec le modèle
55.
du bâton brisé. Journal of Experimental Marine Biology and Ecology 1976, 25, 67–75. doi:10.1016/0022-0981(76)90076-9.

56. Gorban, A.N.; Grechuk, B.; Mirkes, E.M.; Stasenko, S.V.; Tyukin, I.Y. High-Dimensional Separability for One- and Few-Shot Learning.

Entropy 2021, 23. doi:10.3390/e23081090.

57. Grechuk, B.; Gorban, A.N.; Tyukin, I.Y. General stochastic separation theorems with optimal bounds. Neural Networks 2021, 138, 33–56.

doi:10.1016/j.neunet.2021.01.034.

