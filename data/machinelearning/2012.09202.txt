2
2
0
2

l
u
J

5

]

C
O
.
h
t
a
m

[

3
v
2
0
2
9
0
.
2
1
0
2
:
v
i
X
r
a

Journal of Machine Learning Research 23 (2022) 1-23

Submitted 4/21; Revised 1/22; Published 6/22

Clustering with Semideﬁnite Programming and
Fixed Point Iteration

Pedro Felzenszwalb
School of Engineering and Department of Computer Science
Brown University
Providence, RI 02912, USA

Caroline Klivans
Division of Applied Mathematics
Brown University
Providence, RI 02912, USA

Alice Paul
Department of Biostatistics
Brown University
Providence, RI 02912, USA

Editor: Silvia Villa

pff@brown.edu

klivans@brown.edu

alice paul@brown.edu

Abstract

We introduce a novel method for clustering using a semideﬁnite programming (SDP) re-
laxation of the Max k-Cut problem. The approach is based on a new methodology for
rounding the solution of an SDP relaxation using iterated linear optimization. We show
the vertices of the Max k-Cut relaxation correspond to partitions of the data into at most
k sets. We also show the vertices are attractive ﬁxed points of iterated linear optimization.
Each step of this iterative process solves a relaxation of the closest vertex problem and
leads to a new clustering problem where the underlying clusters are more clearly deﬁned.
Our experiments show that using ﬁxed point iteration for rounding the Max k-Cut SDP
relaxation leads to signiﬁcantly better results when compared to randomized rounding.

Keywords: Clustering, Semideﬁnite programming, Optimization.

1. Introduction

Semideﬁnite programming (SDP) relaxations have led to signiﬁcant advances in the devel-
opment of combinatorial optimizaton algorithms. This includes a variety of problems with
applications to machine learning. Many challenging optimization problems can be approxi-
mately solved by a combination of an SDP relaxation and a rounding step. One of the best
examples of this paradigm is the celebrated Max Cut approximation algorithm of Goemans
and Williamson (1995).

From a theoretical point of view algorithms based on SDP relaxations can lead to strong
approximation guarantees. However, such approximation guarantees do not always translate
to practical solutions. Many algorithms with good theoretical guarantees rely on randomized
rounding methods that can produce solutions that have undesirable artifacts despite having
high objective value. This motivates the development of eﬀective deterministic methods for

©2022 Pedro Felzenszwalb, Caroline Klivans and Alice Paul.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v23/21-0402.html.

 
 
 
 
 
 
Felzenszwalb, Klivans and Paul

rounding the solutions of SDP relaxations. Recent advances based on the sum-of-squares
hierarchy have also motivated the development of new general methods for rounding the
solutions of SDP relaxations (see, e.g., Barak et al. (2014)).

In this paper we introduce a novel method for clustering using the Max k-Cut SDP
relaxation described in Frieze and Jerrum (1997). The approach is based on a new method-
ology for rounding the solution of an SDP relaxation introduced by the current authors in
Felzenszwalb et al. (2021). Our rounding method involves ﬁxed point iteration with a map
that optimizes a linear function over a convex body. Figure 1 shows a clustering example,
comparing the result of our ﬁxed point iteration method for rounding the solution of the
Max k-Cut relaxation to the result obtained using the randomized rounding method in
Frieze and Jerrum (1997).

The SDP relaxation for Max k-Cut involves linear optimization over a convex body that
we call the k-way elliptope. In Section 4 we show that the vertices of the k-way elliptope
correspond to partitions (clusterings) of the data into at most k sets, generalizing the result
from Laurent and Poljak (1995) for the elliptope (the k = 2 case).

As pointed out already by the authors of Frieze and Jerrum (1997), the randomized
rounding method for the Max k-Cut SDP relaxation has some signiﬁcant shortcomings.
The approximation factor of the randomized algorithm appears good on the surface, but is
not much better than the approximation factor one gets by simply randomly partitioning
the data. Randomized rounding often generates a partition with fewer than k sets. The
result in Figure 1(b) is a partition with 7 non-empty clusters despite the fact that k = 8.
We also see in Figure 1(b) that the resulting clusters are not compact. Instead diﬀerent
clusters have signiﬁcant overlap. In Section 6 we compare our ﬁxed point iteration method
to the randomized rounding procedure in several examples, showing that the ﬁxed point
approach can produce much better clusterings in practice.

The work in Mahajan and Ramesh (1999) gives a general method for derandomizing
approximation algorithms based on SDP relaxations, including the Max k-Cut relaxation
we use for clustering. Although the method in Mahajan and Ramesh (1999) is interesting
from a theoretical point of view, the approach is not practical for problems of non-trivial
size. More recent methods for derandomizing the Max Cut approximation algorithm include
Engebretsen et al. (2002) and Bhargava and Kosaraju (2005). These methods are all based
on the randomized rounding method of Goemans and Williamson (1995) but replace the
randomization with a search over a limited number of discretized choices. In contrast, our
ﬁxed point iteration method is not based on derandomization techniques and is instead
based on a novel approach for rounding the solution of an SDP relaxation.

Intuitively the problem of rounding a solution of the SDP relaxation for Max k-Cut
can be interpreted as a new clustering problem. This motivates the iterative nature of
our algorithm. Our rounding procedure solves a sequence of SDP problems where the
underlying clusters become more clearly deﬁned in each iteration. In Felzenszwalb et al.
(2021) we showed that iterated linear optimization in a convex region always converges to
a ﬁxed point. In the case of the k-way elliptope the integer solutions to the Max k-Cut
problem are attractive ﬁxed points. Additionally, when rounding the solution of the SDP
relaxation, we ideally round to the closest vertex (k-way partition). We show in Section 5
that each iteration of our rounding procedure corresponds to a relaxation of this objective.

2

Clustering with Semidefinite Programming

(a) Input data

(b) Randomized rounding (best of 50 trials)

(c) Fixed point iteration

(d) Sequence of solutions generated by ﬁxed point iteration

Figure 1: Clustering 120 points into 8 clusters using the Max k-Cut SDP relaxation: (a)
input data, (b) clustering using randomized rounding, and (c) clustering using
ﬁxed point iteration. Each cluster is shown with a diﬀerent color and a minimal
enclosing circle. A solution to the Max k-Cut SDP relaxation is a matrix in the
k-way elliptope. (d) illustrates the sequence of matrices obtained by ﬁxed point
iteration. The ﬁnal matrix is an integer solution deﬁning a clustering.

3

Felzenszwalb, Klivans and Paul

Mixon et al. (2017) also solve an SDP relaxation to cluster subgaussian mixtures. How-
ever, rather than round this SDP solution directly, they revert back to employing Lloyd’s
algorithm to partition the columns of the solution. Our method, instead, continues to use
the strength of semideﬁnite programming to recover a partition.

For some applications convex relaxations have been shown to recover the “true” hidden
structure in the data (see, e.g., Cand`es and Tao (2010)). For clustering applications it was
shown in Awasthi et al. (2015) and Mixon et al. (2017) that convex relaxations can recover
a ground truth clustering if the data is suﬃciently well-separated. However, in practice the
data is rarely well-separated (and there is often no ground truth clustering). Nonetheless,
good clusterings might exist that can be extremely useful for data processing, coding or
analysis. The data in Figure 1 illustrates an example of this situation. The data was
generated by sampling from several Gaussian distributions with signiﬁcant overlap. In this
case there is no way to recover the ground truth clustering, but the result of our ﬁxed point
iteration method still provides a good solution that can be used for subsequent analysis.

In Section 2 we discuss how Max k-Cut can be used to formulate the clustering problem.
In Section 3 we review the Max k-Cut SDP relaxation and the randomized rounding method
from Frieze and Jerrum (1997). In Section 4 we study the convex body that arises from
the SDP relaxation and show the vertices of the feasible region correspond to partitions. In
Section 5 we describe how iterated linear optimization leads to a deterministic method for
rounding the solution of the SDP relaxation. The approach solves a sequence of relaxations
to the closest vertex problem, and leads to clusters that are more clearly deﬁned in each
iteration. Finally, in Section 6 we illustrate experimental results of our new rounding method
and compare them to the randomized rounding approach from Frieze and Jerrum (1997).
We also compare the result of our method to the k-means algorithm for clustering images
in the MNIST handwritten digit dataset.

2. Clustering with Max k-Cut

Clustering problems are often formulated using pairwise measures of similarity or dissim-
ilarity between objects.
Intuitively we would like to partition the data so that pairs of
objects within a cluster are similar to each other while pairs of objects in diﬀerent clusters
are dissimilar. This natural idea leads to a variety of formulations of clustering as graph
partition problems. One beneﬁt of graph-based approaches to clustering is the ability to
deﬁne pairwise measures that incorporate both categorical and numerical variables (see,
e.g. Bertsimas et al. (2021)).

Let G = (V, E) be a weighted graph. Let [n] = {1, . . . , n}. To simplify notation
we assume throughout that V = [n] and that the graph is complete. A k-partition is a
partition of V into k disjoint sets (A1, . . . , Ak), some of which may be empty. Let M be a
symmetric matrix of pairwise non-negative weights. The weight of a partition P is the sum
of the weights of the pairs {i, j} ⊆ [n] that are split (or cut) by P ,

w(P ) =

(cid:88)

(cid:88)

(cid:88)

Mi,j.

1≤r<s≤k

i∈Ar

j∈As

The Max k-Cut problem is to ﬁnd a k-partition maximizing w(P ).

4

Clustering with Semidefinite Programming

As an example, let D = {x1, . . . , xn} be n points in Rd. One of the most commonly
used formulations for clustering data in Euclidean space involves optimizing the k-means
objective. For A ⊆ [n] let m(A) be the mean of the points indexed by A,

m(A) =

1
|A|

(cid:88)

i∈A

xi.

The k-means objective (1) is to partition the data into clusters to minimize the sum of
squared distances from each point to the center of its cluster,

argmin
(A1,...,Ak)

k
(cid:88)

(cid:88)

r=1

i∈Ar

||xi − m(Ai)||2.

(1)

This objective encourages partitions of the data into “compact” clusters, and is widely
used both for clustering and for vector quantization in coding applications (see, e.g., Lloyd
(1982); MacKay (2003); Jain and Dubes (1988); Arthur and Vassilvitskii (2007)).

Let Mi,j = ||xi − xj||2 and consider the Max k-Cut objective with these weights. Maxi-
mizing the weight of the pairs {i, j} that are split by a partition is the same as minimizing
the weights of the pairs {i, j} that are not split. Moreover, the sum of squared distances
between points within a set Ai can be expressed in terms of the sum of squared distances
between each point in Ai and the mean of the set:

argmax w(A1, . . . , Ak) = argmin
(A1,...,Ak)

= argmin
(A1,...,Ak)

k
(cid:88)

(cid:88)

||xi − xj||2,

r=1

i,j∈Ar

k
(cid:88)

r=1

|Ar|

(cid:88)

i∈Ar

||xi − m(Ar)||2.

(2)

(3)

The objective function (3) is similar to the k-means objective (1), except that in the case
of Max k-Cut there is a preference towards balanced partitions. In Section 6 we illustrate
the results of clustering experiments with Max k-Cut using this formulation.

The work in Wang and Sha (2011) also considered clustering with Max k-Cut and used
an SDP relaxation to solve the resulting problem. That work focused on an information
theoretical formulation of the clustering problem that could also be used within our frame-
work. SDP relaxations of the k-means objective for clustering have also been considered in
Peng and Wei (2007), Awasthi et al. (2015), and Mixon et al. (2017).

Most of the previous work on clustering using graph-based methods has focused on for-
mulations based on minimum cuts and spectral algorithms (Shi and Malik (2000); Meila
and Shi (2001); Ng et al. (2002); Weiss (1999); Kannan et al. (2004)). In this case the weight
of an edge represents similarity (or aﬃnity) between elements instead of dissimilarity. Min-
imum cut formulations often include some form of normalization (see, e.g., Shi and Malik
(2000); Kannan et al. (2004)) or a balance requirement to avoid trivial partitions (other-
wise the cut is minimized when one partition is very small). In contrast to minimum cut
formulations, clustering using a maximum cut formulation naturally encourages balanced
partitions, as they maximize the total number of edges that are split.

5

Felzenszwalb, Klivans and Paul

3. SDP Relaxation for Max k-Cut

The Goemans and Williamson (1995) approximation algorithm for Max Cut (clustering
into two clusters) is based on an SDP relaxation and a randomized rounding method. The
relaxation involves the optimization of a linear function over a convex body Ln known as
the elliptope. Laurent and Poljak (1995) showed that the vertices of Ln correspond to
bipartitions of [n]. The fact that the vertices of Ln are bipartitions gives an explanation as
to why in some cases the SDP relaxation can lead directly to an integer solution that is an
optimal solution to the Max Cut problem (see also Cifuentes et al. (2020)).

Frieze and Jerrum (1997) generalized the approximation algorithm of Goemans and
Williamson to a randomized approximation method for Max k-Cut. In this case the algo-
rithm involves linear optimization over a convex body Ln,k that we call the k-way elliptope.
Below we brieﬂy review the SDP relaxation and randomized rounding method for Max
k-Cut introduced by Frieze and Jerrum (1997). In Section 4 we show the vertices of the
k-way elliptope correspond to k-partitions, generalizing the result from Laurent and Poljak
(1995) for the elliptope. In Section 5 we describe a determinisic method for rounding the
solution of the SDP relaxation for Max k-Cut based on iterated linear optimzation.

The Max k-Cut SDP relaxation is based on a reformulation of the combinatorial problem
in terms of Gram matrices. Let a1, . . . , ak be the vertices of an equilateral simplex Σk in
Rk−1 centered around the origin and scaled such that ||ai|| = 1.

A k-partition P = (A1, . . . , Ak) can be encoded by n vectors (y1, . . . , yn) with yi = aj if
i ∈ Aj. Deﬁne the k-partition matrix X P to be the Gram matrix of (y1, . . . , yn). For i (cid:54)= j
we have ai · aj = −1/(k − 1). Therefore,

X P

i,j =

1 − X P

i,j =

(cid:40)
1
−1/(k − 1)

(cid:40)
0
k/(k − 1)

if {i, j} are together in P
if {i, j} are split by P

if {i, j} are together in P
if {i, j} are split by P

(4)

(5)

For two n by n matrices X and Y let

X · Y =

n
(cid:88)

n
(cid:88)

i=1

j=1

Xi,jYi,j.

Now the weight of a partition, w(P ), can be written as,

w(P ) =

k − 1
2k

(1 − X P ) · M,

where M is the symmetric matrix of pairwise weights.

Deﬁnition 1 The set of k-partition matrices Qn,k is the set of Gram matrices of n vectors
(y1, . . . , yn) with yi ∈ {a1, . . . , ak} for 1 ≤ i ≤ n.

We can reformulate the Max k-Cut problem as an optimization over k-partiton matrices,

argmax
X∈Qn,k

k − 1
2k

(1 − X) · M.

6

Clustering with Semidefinite Programming

The SDP relaxation of Max k-Cut is based on relaxing the requirement that yi ∈ {a1, . . . , an}
in the deﬁnition of Qn,k to allow yi to be any unit vector in Rn, with the additional constraint
that yi · yj ≥ −1/(k − 1).

Let S(n) ⊂ Rn×n be the set of n × n symmetric matrices.

Deﬁnition 2 The elliptope Ln is the subset of matrices in S(n) that are positive semidef-
inite and have all 1’s on the diagonal:

Ln = {X ∈ S(n) | X (cid:60) 0, Xi,i = 1}.

The matrices in Ln exactly correspond to Gram matrices of n unit vectors (y1, . . . , yn) in
Rn (Goemans and Williamson (1995); Laurent and Poljak (1995)).

Deﬁnition 3 The k-way elliptope Ln,k is the subset of matrices in Ln where every entry
is at least −1/(k − 1):

Ln,k = {X ∈ Ln | Xi,j ≥ −1/(k − 1)}.

The matrices in Ln,k correspond to Gram matrices of n unit vectors (y1, . . . , yn) in Rn with
yi · yj ≥ −1/(k − 1).

The randomized algorithm in Frieze and Jerrum (1997) involves the SDP relaxation,

argmax
X∈Ln,k

k − 1
2k

(1 − X) · M.

Let X be an optimal solution to the SDP relaxation. We can interpret X as the Gram
matrix of n unit vectors (y1, . . . , yn) in Rn, obtained using a Cholesky decomposition X =
V V T . To generate a k-partition the rounding method in Frieze and Jerrum (1997) selects
k unit vectors (u1, . . . , uk) independently from a uniform distribution and assigns yi to the
closest vector uj.

When k > 2 rounding a k-partition matrix X P using the randomized procedure above
can generate a partition that is diﬀerent from P because diﬀerent sets in P can be merged.
More generally randomized rounding often generates a partition with fewer than k sets
because some vector uj is not the closest vector to any vi. In Section 6 we show experimental
results that illustrate several problems that arise in practice when using the randomized
rounding procedure. Even when the cut value of the random partition is relatively high,
the resulting clustering can have undesirable artifacts.

4. The k-way elliptope

The main result of this section is that the vertices of Ln,k are the matrices in Qn,k and
correspond to k-partitions of [n]. Note that a k-partition may have some empty sets, so the
vertices of Ln,k correspond to partitions of [n] into at most k sets.

For a matrix X ∈ L3,k we have

X =





1 x y
x 1 z
1
z
y



 .

7

Felzenszwalb, Klivans and Paul

Figure 2: The 2-way elliptope L3,2 has 4 vertices (red points), corresponding to partitions

of 3 distinguished elements into at most 2 sets.

Figure 3: The 3-way elliptope L3,3 has 5 vertices (red points), corresponding to partitions

of 3 distinguished elements into at most 3 sets.

8

Clustering with Semidefinite Programming

Therefore we can visualize X as a point (x, y, z) ∈ R3.

Figure 2 illustrates L3,2. This convex body has 4 vertices, with one vertex for each

partition of 3 distinguished elements into at most 2 sets. The partitions are listed below.

P1 = ({1, 2, 3}, ∅)
P2 = ({1, 2}, {3})
P3 = ({1, 3}, {2})
P4 = ({2, 3}, {1})
Figure 3 illustrates L3,3. This convex body has 5 vertices, with one vertex for each

partition of 3 distinguished elements into at most 3 sets. The partitions are listed below.

P1 = ({1, 2, 3}, ∅, ∅)
P2 = ({1, 2}, {3}, ∅)
P3 = ({1, 3}, {2}, ∅)
P4 = ({2, 3}, {1}, ∅)
P5 = ({1}, {2}, {3})
Note that Ln,k is simply the elliptope intersected with an orthant. This characterization
is useful to understand the geometric and combinatorial structure of Ln,k. The diﬀerence
between Ln,r and Ln,s is the amount by which the intersecting orthant is translated.

If we translate the orthant continuously from 0 to −1/(n − 1) the vertex that represents
the grouping of all elements into a single set remains ﬁxed. This vertex is the matrix of all
1’s. On the other hand, the vertex that represents the partition of all elements into diﬀerent
sets only appears when the orthant reaches −1/(n−1). This vertex is the n-partition matrix
with 1’s on the diagonal and −1/(n − 1) in the oﬀ diagonal entries.

Let ∆ be a convex subset of Rn. For x ∈ ∆, the normal cone of ∆ at x is the set

N (∆, x) = {y ∈ Rn | y · x ≥ y · z ∀z ∈ ∆}.

A vertex of ∆ is (by deﬁnition) a point with a full-dimensional normal cone.
The following result shows that the normal cone of Ln,k at a k-partition matrix is full-

dimensional.

Proposition 4 If X ∈ Qn,k and ||Y − X|| < 1/(k − 1) then Y ∈ N (Ln,k, X).

Proof If ||Y − X|| < 1/(k − 1) then |(Y − X)i,j| < 1/(k − 1) for all {i, j} ⊆ [n]. Since Xi,j
is either 1 or −1/(k − 1) we can see that Y has the same sign pattern as X. If Z ∈ Ln,k all
of the entries in Z are between −1/(k − 1) and 1. Therefore Y · X ≥ Y · Z.

The next proposition shows that k-partition matrices are the only matrices in Ln,k where

every entry is either −1/(k − 1) or 1.

Proposition 5 If X ∈ Ln,k and Xi,j ∈ {−1/(k − 1), 1} for all {i, j} ⊆ [n] then X ∈ Qn,k.

Proof Suppose Xi,j ∈ {−1/(k−1), 1} for all {i, j} ⊆ [n]. Since X is a Gram matrix we know
that Xi,j = 1 deﬁnes an equivalence relation on [n]. Let (A1, . . . , Al) be the equivalence
classes of this relationship. If i ∈ Ar and j ∈ As with r (cid:54)= s then Xi,j = −1/(k − 1). This
means we have l unit vectors with the dot product between each pair equal to −1/(k − 1).
Lemma 4 in Frieze and Jerrum (1997) implies that l ≤ k. We conclude X ∈ Qn,k.

9

Felzenszwalb, Klivans and Paul

Let

T (M ) = argmax

M · X.

X∈Ln

Note that the argmax in the deﬁnition of T may not be unique. In this case T (M ) is set
valued. When we write X = T (M ) we allow X to be any element of T (M ).

We will use the following lemma from Felzenszwalb et al. (2021).

Lemma 6 Let M ∈ S(n) and X = T (M ).

Suppose X is the Gram matrix of n unit vectors (v1, . . . , vn). Then,

(a) There exists real values αi such that

(cid:88)

j(cid:54)=i

Mi,jvj = αivi.

(b) The vectors (v1, . . . , vn) are linearly dependent and rank(X) < n.

(c) There exists a diagonal matrix D such that,

M X = DX.

Now we are ready to prove the main result of this section, showing that the vertices of

Ln,k are in fact the integer solutions to the Max k-Cut problem.

Theorem 7 The vertices of Ln,k are the k-partition matrices.

Proof Suppose X ∈ Qn,k. Proposition 4 implies N (Ln,k, X) is full-dimensional.

Now suppose X (cid:54)∈ Qn,k. Proposition 5 implies that X must have at least one entry
Xi,j (cid:54)∈ {−1/(k − 1), 1} Since Ln,k ⊆ Ln we know X ∈ Ln. Note that Xi,j (cid:54)∈ {−1, 1}. Let J
be a matrix with Ji,j = Jj,i = 1 and Jk,l = 0 for {k, l} (cid:54)= {i, j}.

Let On,k = {X | Xi,j ≥ −1/(k − 1)}. Since Ln,k = Ln ∩ On,k we have

N (Ln,k, X) = {v + u, | v ∈ N (Ln, X), u ∈ N (On,k, X)}.

We consider N (Ln, X) and N (On,k, X) separately. We show J is not in the linear span of
N (Ln, X) and J is orthogonal to N (On,k, X). This implies J is not in the linear span of
N (Ln,k, X) and therefore N (Ln,k, X) is not full-dimensional.

For the sake of contradiction suppose J is in the linear span of N (Ln, X). Then J =
R − M with R and M both in N (Ln, X). This implies M + J = R and M + J ∈ N (Ln, X).
Since M ∈ N (Ln, X) we have X = T (M ) and by the lemma above X is the Gram matrix
of (v1, . . . , vn) where,

(cid:88)

vi ∝

Mi,kvk.

Since M + J ∈ N (Ln, X) we have X = T (M + J) and the lemma also implies,

k(cid:54)=i

vi ∝

(cid:88)

k(cid:54)=i

(Mi,k + Ji,k)vk ⇒ vi ∝ vj +

(cid:88)

k(cid:54)=i

Mi,kvk.

10

Clustering with Semidefinite Programming

Therefore

vi ∝ vj.

Since Xi,j (cid:54)∈ {−1, 1} the unit vectors vi and vj can not be proportional and we have a
contradiction. Therefore J is not in the linear span of N (Ln, X).

Now note

N (On,k, X) = cone({er,s | Xr,s = −1/(k − 1)}),

where er,s is the matrix that is 0 everywhere except in the (r, s) position which has value 1.
The dot product J · er,s equals 0 for all (r, s) with Xr,s = −1/(k − 1). Therefore N (On,k, X)
is orthogonal to J.

5. Iterated linear optimization and rounding

A key step in solving a combinatorial optimization problem via a convex relaxation involves
rounding a solution of the convex relaxation to a solution of the original combinatorial
optimization problem. In the case of Max k-Cut this involves mapping a solution X ∈ Ln,k
to a k-partition matrix Y ∈ Qn,k.

Recall that X ∈ Ln,k is the Gram matrix of n unit vectors (v1, . . . , vn). The problem
of rounding X can be seen as a new clustering problem, where we would like to partition
(v1, . . . , vn) into k sets. To solve this clustering problem we look for Y ∈ Qn,k that is close
to X. Relaxing this problem to Ln,k we obtain a new SDP with solution X (cid:48) ∈ Ln,k. Our
rounding method involves repeating this process multiple times. The process is guaranteed
to converge and the underlying unit vectors become more clearly clustered with each step.

5.1 Iterated linear optimization

Let ∆ ⊂ Rn be a compact convex subset containing the origin. Let T (x) be the map deﬁned
by linear optimization over ∆,

T (x) = argmax

x · y.

y∈∆

In Felzenszwalb et al. (2021) it was shown that ﬁxed point iteration with T always converges
to a ﬁxed point of T . Furthermore, when ∆ is the elliptope, T (X) solves a relaxation to
the closest vertex problem. Here we derive a similar result for the k-way elliptope, and
show that iterated linear optimization in Ln,k can be used to round a solution to the SDP
relaxation for Max k-Cut.

5.2 Deterministic rounding in Ln,k

Let X ∈ Ln,k be a solution to the SDP relaxation of a Max k-Cut problem. If X ∈ Qn,k then
X deﬁnes k-partition. Otherwise we look for Y ∈ Qn,k that is closest to X. By relaxing
this problem we obtain a new SDP. Solving the new SDP leads to a new solution Y ∈ Ln,k.
If Y ∈ Qn,k then Y is the closest k-partition matrix to X. Otherwise we recursively look
for a matrix Z ∈ Qn,k that is closest to Y . The approach leads to a ﬁxed point iteration
process with a map T (cid:48) that optimizes a linear function over Ln,k.

11

Felzenszwalb, Klivans and Paul

Let a = (1 − k/2)/(k − 1) and A be the matrix where every entry equals a. If Y ∈ Qn,k

then all entries in Y are in {−1/(k − 1), 1} and all entries in Y + A are in

(cid:26)

−

k/2
k − 1

,

k/2
k − 1

(cid:27)

.

Therefore (Y + A) · (Y + A) is constant.

Let Y ∈ Qn,k and consider the following expansion,

||X − Y ||2 = ||(X + A) − (Y + A)||2

= (X + A) · (X + A) + (Y + A) · (Y + A) − 2(X + A) · (Y + A).

Note that (X + A) · (X + A) does not depend on Y and (Y + A) · (Y + A) is constant.
Therefore the closest k-partition matrix to X is,

Y = argmin
Y ∈Qn,k

||X − Y ||2 = argmax
Y ∈Qn,k

(X + A) · (Y + A),

= argmax
Y ∈Qn,k

(X + A) · Y.

Relaxing this problem to Ln,k we obtain Y = T (X + A) where T is deﬁned over ∆ = Ln,k.

Let T (cid:48)(X) = T (X + A). That is,

T (cid:48)(X) = argmax
Y ∈Ln,k

(X + A) · Y.

Fixed point iteration Our rounding method involves ﬁxed point iteration with T (cid:48). That
is, we generate a sequence {Xt} where X0 = X and,

Xt+1 = T (cid:48)(Xt).

Note that iteration with T (cid:48) is equivalent to iteration with T in ∆ = Ln,k + A.
Figure 4 shows the result of the ﬁxed point iteration process in Ln,k for diﬀerent values
of k. In each case we start from the result of the SDP relaxation of Max k-Cut for a graph
with n = 50 vertices and random weights. In each example ﬁxed point iteration with T (cid:48)
converges to a k-partition matrix after a small number of iterations.

Figure 1 in Section 1 shows an example of the sequence of solutions generated by T (cid:48) for

a geometric clustering problem.

We say that a ﬁxed point x of a map f : ∆ → ∆ is attractive if ∃(cid:15) > 0 such that

||x − x0|| < (cid:15) implies that iteration with f starting at x0 converges to x.

In Felzenszwalb et al. (2021) we characterized all of the ﬁxed points of T (cid:48) when k = 2
(in this case T (cid:48) = T ) and showed the attractive ﬁxed points are exactly the k-partition
matrices. Here we consider the case when k ≥ 2.

Proposition 8 The k-partition matrices are attractive ﬁxed points of T (cid:48).

Proof Let X ∈ Qn,k be a k-partition matrix.

12

Clustering with Semidefinite Programming

(a) k = 2

(b) k = 5

(c) k = 10

Figure 4: Fixed point iteration with T (cid:48), starting from the solution of the SDP relaxation of
Max k-Cut for a graph with 50 vertices and random weights. To visualize a matrix
in Ln,k we show an n × n picture with a pixel for each entry in the matrix. Bright
yellow pixels indicate entries with high value, and dark blue pixels indicate entries
with low value. In each case we obtain a sequence of solutions that converge to a
k-partition matrix. The rows and columns in each example have been permuted
so the ﬁnal matrix is block diagonal to facilitate visualization.

Let Y ∈ Ln,k with ||Y − X|| < (k/2)/(k − 1). Then |(Y + A)i,j − (X + A)i,j| =
|(Y − X)i,j| < (k/2)/(k − 1). Since every entry in X + A is either −(k/2)/(k − 1) or
(k/2)/(k − 1) we see that Y + A has the same sign pattern of X + A.

For Z ∈ Ln,k we have |(Z + A)i,j| ≤ |(X + A)i,j|. Therefore (Y + A) · (Z + A) ≤
(Y + A) · (X + A). Moreover if Z (cid:54)= X we have (Y + A) · (Z + A) < (Y + A) · (X + A). This
implies X = T (cid:48)(Y ) and X is an attractive ﬁxed point of T (cid:48).

Deﬁne f : Ln,k → R as,

f (X) = (X + A) · (X + A) =

n
(cid:88)

n
(cid:88)

i=1

j=1

(Xi,j + a)2.

13

Felzenszwalb, Klivans and Paul

Consider the representation of X ∈ Ln,k as the Gram matrix of n unit vectors (v1, . . . , vn)

and recall that Xi,j = vi · vj ≥ −1/(k − 1).

If vi · vj = 1 (the vectors are as close as possible) then (Xi,j + a) = (k/2)/(k − 1). If
vi · vj = −1/(k − 1) (the vectors are as far as possible) then (Xi,j + a) = −(k/2)/(k − 1).
For any other choice −(k/2)/(k − 1) < (Xi,j + a) < (k/2)/(k − 1).

Proposition 5 shows that k-partition matrices are exactly the matrices in Ln,k where
every entry is in {−1/(k − 1), 1}. Therefore the matrices that maximize f (X) are the k-
partition matrices. We can see f (X) as a measure of how close X is to a k-partition matrix.
We can also interpret f (X) as a measure of how well clustered the vectors (v1, . . . , vn) are.
The convergence results from Felzenszwalb et al. (2021) imply the sequence {Xt} con-
verges to a ﬁxed point of T (cid:48). Additionally, the results from Felzenszwalb et al. (2021) imply
that f (X) increases in each iteration. Therefore the vectors deﬁning Xt become more clearly
clustered in each iteration.

We have shown that matrices in Qn,k are attractive ﬁxed points of T (cid:48). Although the
map T (cid:48) has other ﬁxed points, our numerical experiments suggest that these are the only
attractive ﬁxed points, and that ﬁxed point iteration with T (cid:48) starting from a generic point
in Ln,k always converges to a k-partition matrix.

We have done several numerical experiments iterating T (cid:48) to round a solution of the
Max k-Cut relaxation.
In one setting we generated 50 × 50 random weight matrices M
with independent weights sampled from a Gaussian distribution with mean 0 and standard
deviation 1 (note that in this case the matrix M may have negative weights). In another
setting we generated 50 × 50 random weight matrices M by sampling 50 points in R10
independently from the uniform distribution over [0, 1]10. We then set Mij = ||xi − xj||2.
We ran 100 diﬀerent trials with k = 5 in each setting. Fixed point iteration with T (cid:48) starting
from the solution of the Max k-Cut relaxation always converged to a k-partition matrix.
The number of iterations until convergence in the ﬁrst setting was between 3 and 10, with
an average of 7.02. The number of iterations until convergence in the second setting was
between 3 and 4, with an average of 3.01.

6. Clustering Experiments

In this section we illustrate the results of our clustering method on several datasets. The
algorithms were implemented in Python and run on a computer with an Intel i7 CPU @ 2.6
Ghz with 8GB of RAM. We use the cvxpy package for convex optimization together with
the SCS (splitting conic solver) package to solve SDPs. The ﬁxed point iteration process we
use for rounding involves solving a sequence of SDPs identical to the Max k-Cut SDP but
with a diﬀerent objective. For the examples below solving each SDP took 1 to 3 minutes.
The ﬁxed point iteration method converged after 1 to 5 iterations in each case.

In each clustering experiment we have a dataset D = {x1, . . . , xn} with xi ∈ Rd. As
discussed in Section 2 we cluster the data by deﬁning a Max k-Cut problem with weights
Mi,j = ||xi − xj||2. With this choice of weights the maximum weight partition should lead
to compact and balanced clusters, where points within a single cluster are close together
while points in diﬀerent clusters are far from each other. Section 6.1 illustrates clustering
results on synthetic data, while Section 6.2 evaluates the results of clustering a subset of
the MNIST handwritten digits (LeCun et al.).

14

Clustering with Semidefinite Programming

6.1 Synthetic data

We performed several experiments using synthetic data in R2 to facilitate the visualization
of the results. Figures 5, 6, and 7 illustrate the results of clustering a dataset of 200 points
into 5, 10 and 20 clusters respectively.1 In each case we compare the result obtained using
ﬁxed point iteration for rounding the solution of the Max k-Cut relaxation to the result
of randomized rounding. For the randomized rounding method we repeat the rounding
procedure 50 times and select the partition with highest weight generated over all trials.

In all of the experiments we see that the weight of the partition generated by the
ﬁxed point iteration method is higher than the weight of the best partition generated by
randomized rounding. When k = 5 (Figure 5) the results of the two methods are similar,
but the weight of the partition generated by ﬁxed point iteration is slightly better. When
k = 10 and k = 20 (Figures 6 and 7) the results of randomized rounding are signiﬁcantly
degraded while the results of the ﬁxed point iteration method remain very good.

Figure 8 illustrates the partitions obtained in diﬀerent trials of randomized rounding
for the case when k = 5. We can see there is a lot of variance in the results and that the
random rounding method often leads to poor clusterings even when k is relatively small.
The result of our ﬁxed point iteration method in the same data is shown in Figure 5(c).

Figure 9 shows an example where the input data has 160 points generated by sampling 20
points from 8 diﬀerent Gaussian distributions. The Gaussian distributions have standard
deviation σ = 0.2 and means arranged around a circle of radius 1.
In this case there
is a ground truth clustering where points are grouped according to the Gaussian used to
generate them. The overlap between the distributions is suﬃciently high that it is impossible
to recover the ground truth clustering perfectly, but the result of the ﬁxed point iteration
method is closely aligned with the ground truth. The results of randomized rounding are
not as good even when we select the best of 50 trials of the randomized procedure.

We repeated the last experiment 10 times to quantify the diﬀerence between the two
rounding methods. Each repetition was done with a new dataset generated from the mixture
of Gaussians. Table 1(a) summarizes the minimum, maximum, and mean value of the ratio
w(C)/w(D), where C is the partition produced by the ﬁxed point iteration method and D
is the result of the best of 50 trials of randomized rounding. In these experiments the ratio
w(C)/W (D) was always above one, showing that our ﬁxed point iteration method always
returns a partition with higher weight. We also compare the resulting clusterings to the
ground truth using the Rand index (Rand (1971)).

The Rand index is a number between 0 and 1 measuring the agreement between two
clusterings. Let A and B be clusterings. Let a be the number of pairs of elements that are
in the same cluster in both A and B while b is the number of pairs of elements that are in
diﬀerent clusters in both A and B. The Rand index is,

R(A, B) =

a + b
(cid:1) .
(cid:0)n
2

We see in Table 1(b) that our ﬁxed point rounding method consistently produces a clustering
that is more similar to the ground truth than the randomized rounding method.

1. We used a subset of the D-31 dataset from Veenman et al. (2002) with 10 points from each of 20 clusters.

15

Felzenszwalb, Klivans and Paul

(a) Input data

(b) Randomized (best of 50) w(C) = 3543294

(c) Fixed point iteration w(C) = 3589259

Figure 5: Clustering 200 points with k = 5.

16

Clustering with Semidefinite Programming

(a) Input data

(b) Randomized (best of 50) w(C) = 3587153

(c) Fixed point iteration w(C) = 3701677

Figure 6: Clustering 200 points with k = 10.

17

Felzenszwalb, Klivans and Paul

(a) Input data

(b) Randomized (best of 50) w(C) = 3658976

(c) Fixed point iteration w(C) = 3722073

Figure 7: Clustering 200 points with k = 20.

18

Clustering with Semidefinite Programming

(a) Trial 1

(b) Trial 2

(c) Trial 3

(d) Trial 4

(e) Trial 5

(f) Trial 6

Figure 8: Clustering 200 points with k = 5 and randomized rounding. The result of the
ﬁxed point iteration method in the same data is shown in Figure 5(c).

19

Felzenszwalb, Klivans and Paul

(a) Input data and ground truth clustering

(b) Randomized (best of 50) w(C) = 26837

(c) Fixed point iteration w(C) = 27158

Figure 9: Clustering 160 points sampled from 8 Gaussians with k = 8.

20

Clustering with Semidefinite Programming

(a) Partition weights

(b) Clustering accuracy

minimum
maximum
mean

w(C)/w(D)
1.005
1.026
1.014

ﬁxed point
randomized rounding

Rand index
0.972 ± 0.006
0.935 ± 0.018

Table 1: Experiments with 10 diﬀerent datasets generated by a mixture of Gaussians (see
Figure 9). (a) Minimum, maximum and mean value of w(C)/w(D), where C is
the result of the ﬁxed point iteration method and D is the result of the best of
50 trials of randomized rounding. (b) Mean and standard deviation of the Rand
index for both methods.

Figure 10: Subset of MNIST handwritten digits used in one of the clustering trials.

6.2 MNIST digits

In this section we evaluate the performance of our clustering method on a subset of the
MNIST handwritten digits dataset (LeCun et al.).

We performed multiple clustering trials with diﬀerent subsets of the MNIST training
data. In each trial we selected 20 random examples for each of 5 digits (0, 1, 2, 3 and 4).
Each example was represented by a 784 dimensional 0/1 vector encoding a 28x28 binary
image. Figure 10 shows the data from one of the trials.

In Table 2 we report the accuracy of the clustering results obtained using both our
ﬁxed point iteration method and the randomized method for rounding the Max k-Cut
relaxation. For comparison we also evaluate the results of clustering the data using the
k-means algorithm. We see that our ﬁxed point iteration method for rounding the Max

21

Felzenszwalb, Klivans and Paul

Method
Max k-Cut SDP with ﬁxed point iteration
Max k-Cut SDP with randomized rounding
k-means

Rand index
0.907 ± 0.026
0.874 ± 0.037
0.864 ± 0.034

Table 2: Clustering MNIST hardwritten digits. We report both the mean and standard

deviation of the Rand index over 20 trials with random subsets of the data.

k-Cut relaxation gives the best accuracy when compared both to the use of randomized
rounding and the k-means algorithm.2

References

David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In

ACM-SIAM Symposium on Discrete Algorithms, pages 1027–1035, 2007.

Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad
Villar, and Rachel Ward. Relax, no need to round: Integrality of clustering formulations.
In Conference on Innovations in Theoretical Computer Science, 2015.

Boaz Barak, Jonathan A Kelner, and David Steurer. Rounding sum-of-squares relaxations.

In ACM Symposium on Theory of Computing, pages 31–40, 2014.

Dimitris Bertsimas, Agni Orfanoudaki, and Holly Wiberg.

Interpretable clustering: an

optimization approach. Machine Learning, 110(1):89–138, 2021.

Ankur Bhargava and S Rao Kosaraju. Derandomization of dimensionality reduction and
SDP based algorithms. In Workshop on Algorithms and Data Structures, pages 396–408,
2005.

Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal
matrix completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.

Diego Cifuentes, Corey Harris, and Bernd Sturmfels. The geometry of SDP-exactness in

quadratic optimization. Mathematical Programming, 182:399–428, 2020.

Lars Engebretsen, Piotr Indyk, and Ryan O’Donnell. Derandomized dimensionality reduc-
tion with applications. In ACM-SIAM Symposium on Discrete Algorithms, page 705–712,
2002.

Pedro Felzenszwalb, Caroline Klivans, and Alice Paul. Iterated linear optimization. Quar-

terly of Applied Mathematics, 79:601–615, 2021.

Alan Frieze and Mark Jerrum. Improved approximation algorithms for max k-cut and max

bisection. Algorithmica, 18(1):67–81, 1997.

2. We used the scipy library implementation of k-means. Each trial involved 10 diﬀerent random initial-

izations of the initial cluster centers using the k-means++ method.

22

Clustering with Semidefinite Programming

Michel Goemans and David Williamson. Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM, 42
(6):1115–1145, 1995.

Anil K Jain and Richard C Dubes. Algorithms for clustering data. Prentice-Hall, 1988.

Ravi Kannan, Santosh Vempala, and Adrian Vetta. On clusterings: Good, bad and spectral.

Journal of the ACM, 51(3):497–515, 2004.

Monique Laurent and Svatopluk Poljak. On a positive semideﬁnite relaxation of the cut

polytope. Linear Algebra and its Applications, 223/224:439–461, 1995.

Yann LeCun, Corinna Cortes, and Christopher Burges. MNIST database of handwritten

digits. http://yann.lecun.com/exdb/mnist/.

Stuart Lloyd. Least squares quantization in PCM.

IEEE Transactions on Information

Theory, 28(2):129–137, 1982.

David MacKay. Information theory, inference and learning algorithms. Cambridge Univer-

sity press, 2003.

Sanjeev Mahajan and Hariharan Ramesh. Derandomizing approximation algorithms based
on semideﬁnite programming. SIAM Journal on Computing, 28(5):1641–1663, 1999.

Marina Meila and Jianbo Shi. A random walks view of spectral segmentation. In AISTATS,

2001.

Dustin G Mixon, Soledad Villar, and Rachel Ward. Clustering subgaussian mixtures by
semideﬁnite programming. Information and Inference: A Journal of the IMA, 6(4):389–
415, 2017.

Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an
algorithm. In Advances in Neural Information Processing Systems, pages 849–856, 2002.

Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideﬁnite pro-

gramming. SIAM journal on optimization, 18(1):186–205, 2007.

William M Rand. Objective criteria for the evaluation of clustering methods. Journal of

the American Statistical association, 66(336):846–850, 1971.

Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transac-

tions on Pattern analysis and Machine Intelligence, 22(8):888–905, 2000.

Cor J. Veenman, Marcel J. T. Reinders, and Eric Backer. A maximum variance cluster
algorithm. IEEE Transactions on pattern analysis and machine intelligence, 24(9):1273–
1280, 2002.

Meihong Wang and Fei Sha. Information theoretical clustering via semideﬁnite program-

ming. In AISTATS, 2011.

Yair Weiss. Segmentation using eigenvectors: a unifying view.

In IEEE International

Conference on Computer Vision, 1999.

23

