A Rule for Gradient Estimator Selection, with an Application to
Variational Inference

Tomas Geﬀner
University of Massachusetts, Amherst

Justin Domke
University of Massachusetts, Amherst

9
1
0
2

v
o
N
5

]

G
L
.
s
c
[

1
v
4
9
8
1
0
.
1
1
9
1
:
v
i
X
r
a

Abstract

Stochastic gradient descent (SGD) is the
workhorse of modern machine
learning.
Sometimes, there are many diﬀerent poten-
tial gradient estimators that can be used.
When so, choosing the one with the best
tradeoﬀ between cost and variance is impor-
tant. This paper analyzes the convergence
rates of SGD as a function of time, rather
than iterations. This results in a simple rule
to select the estimator that leads to the best
optimization convergence guarantee. This
choice is the same for diﬀerent variants of
SGD, and with diﬀerent assumptions about
the objective (e.g. convexity or smoothness).
Inspired by this principle, we propose a tech-
nique to automatically select an estimator
when a ﬁnite pool of estimators is given.
Then, we extend to inﬁnite pools of estima-
tors, where each one is indexed by control
variate weights. This is enabled by a reduc-
tion to a mixed-integer quadratic program.
Empirically, automatically choosing an esti-
mator performs comparably to the best esti-
mator chosen with hindsight.

1 Introduction

Convergence guarantees for stochastic optimization al-
gorithms depend on structural properties of the objec-
tive, such as convexity and smoothness, the variance
of the gradient estimates, and the number of iterations
performed. Naturally, lower variance and more itera-
tions result in better guarantees [14, 20, 2]. Given a
ﬁxed total optimization time, fewer iterations are pos-
sible with a slower estimator. Therefore, an estimator
with low variance and low computational cost is ideal.

Preliminary work.

This paper is inspired by stochastic gradient varia-
tional inference (SGVI), where there are multiple gra-
dient estimators with varying costs and variances. Es-
timators may be obtained using the reparameteriza-
tion trick [7, 16, 21], the score function method [24, 15],
or other techniques [22, 17, 19, 18, 1]. Also, many con-
trol variates can be added to the estimator to reduce
variance [8, 5, 9, 13, 23, 3].1

The cost and variance of an estimator signiﬁcantly af-
fects optimization convergence speed. Diﬀerent esti-
mators lead to diﬀerent performances, and the opti-
mal estimator is often situation-dependent. As an ex-
ample, Fig. 1 shows the results of running SGVI on
several models (described in Sec. 5.1). We compare
three common gradient estimators:

• (Rep) Plain reparameterization estimator,
• (Miller) Estimator proposed by Miller et al.

[8],

which adds a Taylor expansion control variate,

• (STL)

“Sticking-the-landing”
which avoids computing certain terms.

estimator

[17],

(Detailed descriptions of the estimators are in Section
3.4.) Table 1 gives the time costs of each estimator.
There is no universal best estimator. Diﬀerent esti-
mators perform better in diﬀerent situations. An esti-
mator’s variance depends on the model, dataset, and
current parameters, while the time cost also depends
on the implementation and computational platform.
Rather than rely on the user to navigate these trade-
oﬀs, we propose that estimator selection could be done
adaptively. This paper investigates how, given a pool
of gradient estimators, automatically choose one to get
the best convergence guarantee.

Section 2 introduces some background on SGVI and
control variates. Section 3 studies cost-variance trade-
oﬀs by analyzing the convergence rates of several vari-
ants of SGD. We do this with or without the structural

1In fact, it has been shown that the problem of select-
ing a gradient estimator for variational inference can be
phrased as choosing a set of control variates to use along
with their weights [3].

 
 
 
 
 
 
Gradient Estimator Selection

Model

Estimator

Rep Miller

STL

BNN-A
BNN-B
Log Reg (a1a)
Hier Poisson

8.2
7.1
6.9
2.6

25
21
120
7.9

11
9.5
12
3.8

Table 1: Diﬀerent estimators have diﬀerent
costs. Time costs (in ms) for estimators in Fig. 1.

properties of convexity, smoothness, or strong convex-
ity. We observe what we call the “G2T principle”.
This states that, given a set of candidate estimators,
the best convergence guarantee is obtained by the esti-
mator for which G2T is minimum, where G2 ≥ E (cid:107)g(cid:107)2
denotes a bound on an estimator’s expected squared
norm, and T its time cost. Put another way, estima-
tors with lower G2T lead to better convergence guar-
antees. This is true regardless of the objective’s struc-
tural properties (Section 3.2).

In theory, this principle could be directly used if a
pool of estimators with known values for G and T was
given.
In practice, these values are rarely known a
priori. In Section 3.3 we propose an estimator selec-
tion algorithm that uses estimates of these quantities
instead of the true values.

Finally, motivated by SGVI where control variates
play an important role, we look at the problem of con-
trol variate selection. If used with the right weights,
control variates lead to a reduction in the estimators
variance. However, they carry an extra computational
cost, which may or may not be worth paying depend-
ing on the situation.
In Section 4 we consider the
case where there is an inﬁnite pool of estimators, each
indexed by a set of control variate weights. Select-
ing an estimator involves selecting what control vari-
ates to use along with their weights. We show that
the problem of minimizing the estimated G2T values
over this inﬁnite set of estimators can be reduced to a
small mixed integer quadratically constrained program
(MIQCP) and then solved with oﬀ the shelf methods.
Section 5 gives an experimental evaluation.

1.1 Contributions

G2T principle: Given a set of gradient estimators
with diﬀerent G2 and T values, the best conver-
gence guarantee is achieved by the estimator with
minimum G2T . The same estimator is optimal re-
gardless of structural properties of the objective
and with diﬀerent variants of SGD (Section 3.2).

Figure 1: Diﬀerent estimators are better in dif-
SGVI on four models. Each
ferent situations.
plot compares three diﬀerent gradient estimators, de-
scribed in the text. More details on the optimization
algorithm and models are in Section 5.

automatically select one using empirical estimates
of G2 and T . (Algorithm 1, Section 3.3).

Control variate selection via MIQCP: Given an
inﬁnite set of estimators indexed by control vari-
ate weights, we propose an algorithm to automat-
ically select a gradient estimator using empirical
estimates of G2 and T . This is based on a re-
duction to a small MIQCP, which can be solved
eﬃciently in practice. (Algorithm 2, Section 4).

Empirical validation We apply the automatic es-
timator selection techniques to Bayesian logistic
regression, hierarchical regression, and Bayesian
neural network models. Empirically, automatic
estimator selection performs similarly to choosing
the best estimator with hindsight with ﬁnite pools
of estimators (Section 3.4) and similarly with in-
ﬁnite pools of estimators (Section 5).

Notation: We use g(w, ξ), where ξ is a random vari-
able, to denote an unbiased estimator of target’s gradi-
ent, G2(g) to denote a bound on g’s expected squared
norm, Eξ ||g(w, ξ)||2 ≤ G2(g) ∀w, and T (g) to de-
note the computational cost of computing estimator
g(w, ξ), measured in seconds. We drop the dependen-
cies of G2 and T on g when clear from context.

2 Preliminaries

2.1 Stochastic Gradient Variational Inference

Gradient estimator selection: Given a ﬁnite set of
gradient estimators, we propose an algorithm to

Given a model p(x, z), where x is observed data and
z are latent variables, the goal of variational inference

20406080100120140160180166164162160158156154ELBOSTLMillerRepBNN-A (red wine)6080100120140160180560550540530520510500490RepSTLMillerBNN-B (red wine)0102030405060Time (s)845.2845.0844.8844.6844.4844.2ELBOMillerSTLRepHierarchical Poisson (frisk)01020304050607080Time (s)610608606604602600598RepSTLMillerLog. Regression (a1a)Gradient Estimator Selection

(VI) is to ﬁnd parameters w to approximate p(z|x)
with a simpler distribution qw(z). Since

log p(x) = E

(cid:20)

log

p(x, Z)
qw(Z)

+KL(qw(Z)||p(Z|x)),

(cid:21)

(cid:125)

Z∼qw(z)
(cid:124)

(cid:123)(cid:122)
ELBO(w)

minimizing the KL-divergence is equivalent to maxi-
mizing the ELBO(w) (Evidence Lower BOund).

When p and q are complex, it is typically necessary to
optimize the ELBO using stochastic methods [9, 13,
15, 21, 10]. These require an unbiased estimator of
∇wELBO(w), which can be expressed as

∇wELBO = ∇w E

qw(z)

log p(x, Z) + ∇wH(qw)

(1)

= ∇w E

qw(z)

log p(x, Z)

−

(cid:20)
∇w E

qw(z)

log qv(Z) + ∇w E
qv(z)

(cid:21)
log qw(Z)

. (2)

v=w

Estimates for this gradient can be obtained through
the reparameterization trick [7, 21, 16], the general-
ized reparameterization trick [18], and the score func-
tion method [24], among other techniques [15, 8, 9, 10,
13, 23]. All of these express the gradient as an expecta-
tion and approximate it using Monte Carlo sampling.
For example, the popular reparameterization trick is
based on ﬁnding a function Tw that transforms ξ ∼ q0
into Z = Tw(ξ) ∼ qw. Then, it approximates terms
like ∇w Eqw(z) log p(x, Z) with the unbiased estimator
∇w log p(x, Tw(ξ)).

2.2 Control Variates for Variational Inference

Control variates [12] are random variables with mean
zero that can be used to reduce the variance of an esti-
mator. They are widely used in SGVI [3, 5, 8, 9, 13, 15,
23]. Take an estimator, which is some function gbase
such that Eξ gbase(w, ξ) = ∇wELBO(w). Take also the
control variate c such that Eξ c(w, ξ) = 0. The ﬁnal
gradient estimator is g(w, ξ) = gbase(w, ξ) + a c(w, ξ),
where a is a scalar. When multiple control variates
c1, ..., cJ are available, the ﬁnal gradient estimator can
be expressed as

log p(x, z), they suggest the control variate c(w, ξ) =
∇w Eqw u(Z) − ∇wu(Tw(ξ)). Since u is quadratic, this
can be computed exactly when qw is Gaussian. If the
Taylor expansion is accurate, c will greatly reduce the
estimator’s variance.

Other control variates proposed can be built using up-
per/lower bounds of log p(x, z) instead of a Taylor ex-
pansion [13], baselines [9, 15], or the diﬀerence between
two unbiased estimates of a term (expectation of some
function) [3, 5, 23], among others.

3 The G2T principle

3.1 Stochastic Optimization Review

Many diﬀerent ﬁrst order stochastic optimization algo-
rithms can be used to minimize an objective function.
The simplest one, SGD, requires an initial guess w0, a
step-size ηk, and an unbiased estimator of the objec-
tive’s gradient g(wk, ξk). It uses the update rule

wk+1 = wk − ηk g(wk, ξk).

(4)

Table 2 shows guarantees for diﬀerent variants of
SGD. These usually require two things: a bound on
E (cid:107)g(w, ξ)(cid:107)2, and some structural assumption about
the objective (the most common being convexity, λ-
strong convexity, and L-smoothness).

It will be useful for the analysis in the following section
to represent each of the guarantees in Table 2 as a func-
tion Θ(λ, L, C, K, G2), where λ is the strong-convexity
parameter (or 0), L is the smoothness parameter (or
∞), C is a boolean representing whether the problem
is convex, K is the number of iterations, and G2 rep-
resents the bound on the estimator’s expected squared
norm. For example, if the target is λ-strongly convex
G2
and L-smooth, then Θ(λ, L, C, K, G2) = 2L
K (ﬁrst
λ2
row of Table 2). All bounds presented in Table 2 can
be written as

Θ(λ, L, C, K, G2) = α(λ, L, C)

(cid:16) G2
K

(cid:17)p

,

(5)

where p > 0 and α depends on the properties of the
target and initialization, but not on the gradient esti-
mator used.

g(w, ξ) = gbase(w, ξ) + (cid:80)J

i=1 ai ci(w, ξ).

(3)

3.2 The G2T principle

Equivalently, we could write g = gbase + Ca, where
C(w, ξ) is a matrix with control variate ci as its i-th
column, and a is a vector containing the weights for
each control variate.

As a concrete example, Miller et al.
[8] proposed a
control variate to reduce the variance of the reparam-
eterization estimator gbase(w, ξ) = ∇w log p(x, Tw(ξ)).
Given a second-order Taylor approximation u(z) ≈

Given a set of gradient estimators with varying costs
and variances, our goal is to ﬁnd the one that gives
the best convergence guarantee for optimization algo-
rithms. Given a total optimization time Topt, an es-
timator g with cost T (g) can perform K ≈ Topt/T (g)
iterations. Inserting this value for K in eq. (5) results
in a convergence guarantee of

Θ(λ, L, C, K, G2(g)) = α(λ,L,C)

T p

opt

(cid:0)G2(g)T (g)(cid:1)p

.

(6)

Gradient Estimator Selection

Assumptions Needed

Algorithm

Guarantee

Convex

λ-strongly convex L-smooth

(cid:88)

(cid:88)

(cid:88)

−

−

−

(cid:88)

(cid:88)

−

−

−

−

(cid:88)

−

−

(cid:88)

(cid:88)

(cid:88)

SGD, ηk = 1/(λk) [14]

SGD, ηk = 1/(λk) [14]

SGD, ηk = Dw
√
G
K
(cid:113) 2Df

SGD, ηk =

[11]

LKG2 [2]

E F (wK ) − F (w∗) ≤ 2L
λ2

G2
K

E ||wK − w∗||2 ≤ 4
λ2

G2
K
E F ( ¯w) − F (w∗) ≤ Dw

G√
K

E 1
K

(cid:80)K

i=1 ||∇F (wi)||2 ≤ (cid:112)LDf

G√
K

SGD+Momentum (β) [25], E 1
K

ηk =

(cid:114) 2Df (1−β)4

(β2+(1−β)2)KLG2

(cid:80)K

i=1 ||∇F (wi)||2 ≤
(cid:114)

8Df L(β2+(1−β)2)
(1−β)2

SGD+Nesterov (β) [25],
(cid:114) 2Df (1−β)4

ηk =

(β4+(1−β)2)KLG2

(cid:80)K

E 1
K

i=1 ||∇F (wi)||2 ≤
(cid:114)

8Df L(β4+(1−β)2)
(1−β)2

G√
K

G√
K

Table 2: Convergence rates for stochastic optimization algorithms under diﬀerent assumptions about the objec-
tive. Constants in the results are Df = F (w0) − F (w∗) and Dw = ||w0 − w∗|| . K is the number of optimization
steps, and G2 is such that Eξ ||g(w, ξ)||2 ≤ G2 ∀w . The learning rates shown are the optimal ones.

(6)

the

only

equation

dependence

In
of
Θ(λ, L, C, K, G2) on the estimator used g comes
through the (G2(g)T (g))p factor. Thus, the best con-
vergence guarantee is achieved by using the estimator
with the smallest value of G2T , regardless of the
objective’s properties (λ, L, C) and total optimization
time (Topt). We call this the “G2T principle”.

G2T principle. Given

• A target function characterized by (λ, L, C),
which can be convex,
strongly convex,
strongly convex and smooth, or just smooth
(non-convex);

• An optimization time budget Topt;
• A pool of gradient estimators.

Then, for any (λ, L, C, Topt), provided the ap-
propriate algorithm is used, the best conver-
gence guarantee is obtained using the estima-
tor g with minimum G2T . That is, given two
estimators g1 and g2, K1 = Topt/T (g1) and
K2 = Topt/T (g2), we have
Θ (cid:0)λ, L, C, K1, G2(g1)(cid:1) < Θ (cid:0)λ, L, C, K2, G2(g2)(cid:1)

⇐⇒ G2(g1)T (g1) < G2(g2)T (g2),

regardless of the values of λ, L, C and Topt.

3.3 G2T for Gradient Estimator Selection

This section presents a gradient estimator selection al-
gorithm based on the G2T principle. Given a pool of
estimators with known G2 and T , the one with min-
imum G2T should be used. In practice, however, G2
and T are typically not known. We propose to use
estimates.

Assuming that the cost of an estimator g(w, ξ) is inde-
pendent of w, an estimate ˆT (g) of T (g) can be obtained
for each g ∈ G through a single initial proﬁling phase.

Dealing with G2(g) is more involved. Convergence
guarantees typically assume that E ||g(w, ξ)||2 ≤ G2(g)
for all w. Often (e.g. when w is unbounded) this is not
true for any ﬁnite G2(g). Even if a ﬁnite G2(g) exists,
the bound may be conservative. In practice, optimiza-
tion starting from w0 will only visit a restricted part
of the parameter space, W0. In that case, it is suﬃ-
cient to bound E ||g(w, ξ)||2 for this set of w that may
actually be encountered. Since E ||g(w, ξ)||2 tends to
decrease as optimization proceeds, we propose to use
G2(g, w0) = E ||g(w0, ξ)||2 as an eﬀective upper bound
for E ||g(w, ξ)||2 when w ∈ W0. With this eﬀective up-
per bound, we select an estimator by ﬁnding the one
with minimum G2(g, w0)T (g).
After k optimization steps, G2(g, wk) will typically be
less than G2(g, w0). Starting from the current solution
wk, only a smaller part of the parameter space will
be visited in the future, meaning a stronger bound
for future gradients may be possible. We propose
to use the new eﬀective upper bound G2(g, wk) =

Gradient Estimator Selection

E ||g(wk, ξ)||2, and to select the estimator g that now
minimizes G2(g, wk)T (g). We perform this update
only a few times throughout training 2. Finally, since
G2(g, wk) = E ||g(wk, ξ)||2 cannot be computed in
closed form, we use a Monte Carlo estimate, ˆG2(g, wk).
The ﬁnal algorithm is summarized in Algorithm 1.

Algorithm 1 SGD with minimum ˆG2 ˆT estimator.
Require: Set of estimators, G.
Require: Total opt time and times to re-select estimator.
Require: Number of MC samples M .
For all g ∈ G measure time ˆT (g).
for k = 1, 2, · · · do

if time to re-select estimator then
M
(cid:88)

∀g ∈ G, set ˆG2(g, wk) =

1
M

m=1

g ← arg min
g∈G

ˆG2(g, wk) × ˆT (g).

(cid:107)g(wk, ξkm)(cid:107)2.

end if
wk+1 = wk − ηk g(wk, ξk)

end for

3.4 Empirical Validation of the G2T principle

This section presents an empirical validation of Algo-
rithm 1. We consider the same setting as in Fig. 1.
We ran SGVI on four diﬀerent models: two variants
of a Bayesian neural network, a hierarchical Poisson
model, and Bayesian logistic regression. For the logis-
tic regression model we used a Gaussian distribution
with a full-rank covariance matrix as variational distri-
bution, and for the other models we used a factorized
Gaussian.3 We use three gradient estimators:

• (Rep) Plain reparameterization estimator, which
computes the entropy in closed form (eq. (1)),

• (Miller) Estimator proposed by Miller et al.

[8],
which adds a Taylor expansion control variate to
reduce (Rep)’s variance (see Section 2.2). As sug-
gested by Miller et al. we use this control variate
with a ﬁxed weight of 1.

• (STL)

“Sticking-the-landing”

[17],
which removes the last term of equation (2) (it is
always zero) and estimates the remaining terms
with reparameterization.

estimator

Additionally, we use Algorithm 1 with the set of es-
timators G = {(Rep), (Miller), (STL)}, which uses the

2In preliminary experiments, we tried computing the
optimal estimator just once by minimizing G2(g, w0)T (g),
and using that g for the full optimization. While this
works, it often does not ﬁnd the best estimator. This is
likely because the initial solution w0 is not so informative
about the performance of gradient estimators closer to the
solution.

3More details on the models and optimization parame-

ters in Section 5.1

Figure 2: Algorithm 1 leads to results compara-
ble to the results obtained using the best esti-
mator chosen retrospectively. SGVI on four mod-
els. Each plot compares Algorithm 1 with three diﬀer-
ent gradient estimators, as described in the text.

estimator g ∈ G with minimum ˆG2 ˆT . We use M = 400
and we re-select the optimal estimator three times
throughout training, initially, after 10% of training is
done, and after 50% of training is done. Results are
shown in Fig. 2. For all models the use of Algorithm
1 leads to results that are at least as good as the re-
sults obtained using the best estimator chosen with
hindsight.

4 G2T with Control Variates

It is possible to use multiple control variates to reduce
a gradient estimator’s variance [3]. However, some
control variates might be computationally expensive
but only result in a small reduction in variance.
It
may be better to remove them and accept a noisier
but cheaper estimator. We should select what control
variates to use adaptively.

Estimators with control variates can be expressed as

ga(w, ξ) = gbase(w, ξ) +

J
(cid:88)

i=1

aici(w, ξ)

= gbase(w, ξ) + C(w, ξ)a.

(7)

The set of available estimators is G = {ga : a ∈ RJ }.
In this case, the number of estimators ga ∈ G is inﬁ-
nite. Therefore, Algorithm 1 cannot be used (since we
cannot measure ˆT and ˆG2 for each estimator individ-
ually).
This section shows how to ﬁnd the minimum ˆG2 ˆT es-
timator when diﬀerent estimators are indexed by a set
of control variate weights. This is possible because

20406080100120140160180166164162160158156154ELBOAlg.1STLMillerRepBNN-A (red wine)6080100120140160180560550540530520510500490RepAlg.1STLMillerBNN-B (red wine)0102030405060Time (s)845.2845.0844.8844.6844.4844.2ELBOMillerAlg.1STLRepHierarchical Poisson (frisk)01020304050607080Time (s)610608606604602600598Alg.1RepSTLMillerLog. Regression (a1a)Gradient Estimator Selection

two properties hold: (i) ˆT (ga) and ˆG2(ga, w) can be
eﬃciently obtained for all estimators ga ∈ G through
the use of shared statistics (a ﬁnite number of evalua-
tions of the base gradients and control variates); and
(ii) Finding ga that minimizes ˆG2(ga, w) ˆT (ga) can be
reduced to a Mixed Integer Quadratically Constrained
Program (MIQCP), which can be solved quickly in
practice.
An expression for ˆT (ga) can be obtained by notic-
ing that computing ga only requires computing the
base gradient and the control variates with non-zero
weights. Then, for all ga ∈ G,

ˆG2(ga, wk) = 1
2 a(cid:62)Qa + r(cid:62)a + u, where Q, r and u
depend on the M evaluations of the base gradient gbase
and control variates C. Using this, the minimization
problem from equation (10) can be expressed as

a∗ = arg min

a∈RJ ,b∈{0,1}J

(cid:18) 1
2

s.t bi = 1[ai (cid:54)= 0],

a(cid:62)Qa + r(cid:62)a + u

(cid:19)

× (cid:0)t0 + t(cid:62)b(cid:1)

(11)

where t0 = ˆT (gbase), and ti = ˆT (ci). Going from eq.
(11) to a MIQCP is straightforward. Available solvers
can solve the ﬁnal MIQCP fast. The full proof and
resulting MIQCP problem are shown in the appendix.

ˆT (ga) = ˆT (gbase) +

J
(cid:88)

i=1

ˆT (ci) 1[ai (cid:54)= 0].

(8)

The ﬁnal algorithm is summarized in Algorithm 2.

Thus, we can compute ˆT (ga) for all ga ∈ G only by
proﬁling the base gradient and each control variate
individually.
Similarly, ˆG2(ga, w) is determined by the same set of
base gradient and control variate evaluations, regard-
less of the value of a. Suppose that, at iteration k, we
sample ξk1, ..., ξkM . Then, for all ga ∈ G,

ˆG2(ga, wk) =

1
M

M
(cid:88)

m=1

(cid:107)gbase(wk, ξkm)+C(wk, ξkm) a(cid:107)2.

(9)
Thus, we can compute ˆG2(ga, wk) for all ga ∈ G using
only M evaluations of the base gradient gbase and each
control variate ci.

Equations (8) and (9) characterize the (estimated) cost
and variance of the gradient estimator with weights a.
We ﬁnd the weights that result in the optimal cost-
variance tradeoﬀ by solving

a∗(w) = arg min

a∈RJ

ˆG2(ga, w) × ˆT (ga),

(10)

where ˆT (ga) and ˆG2(ga, w) are as in equations (8) and
(9). The solution a∗(w) indicates what control variates
to use (those with a∗

i (cid:54)= 0), and their weights.

Solving the (combinatorial) minimization problem in
equation (10) may be challenging. However, theorem
4.1 states that it can be reduced to a MIQCP, which
can be solved fast using solvers such as Gurobi [6].

Theorem 4.1. When diﬀerent gradient estimators
are indexed by a set of J control variate weights, the
problem of ﬁnding a∗(w) as in equation (10) can be
reduced to solving a mixed integer quadratically con-
strained program with 2J + 2 variables, one quadratic
constraint, and one linear constraint.

The proof idea is as follows: If one expands the norm,
equation (9) can be written as a quadratic function

Algorithm 2 SGD with minimum ˆG2 ˆT estimator; es-
timators indexed by control variates weights.
Require: Set of available gradient estimators G.
Require: Total opt time and times to re-select estimator.
Require: Number of MC samples M .

For gbase measure time t0.
For all i = 1, ..., J, measure ci time ti.
for k = 1, 2, · · · do

if time to re-select estimator then

a = argmina

ˆG2(ga, wk) ˆT (ga) (solve MIQCP).

end if
ga = gbase(w, ξk) + (cid:80)
wk+1 = wk − ηk ga(wk, ξk)

i:ai(cid:54)=0 aici(w, ξk)

end for

5 Experiments and Results

This section presents results that empirically vali-
date Alg. 2 for control variate selection. We tackle
variational inference problems on several probabilis-
tic models using SGVI. We perform stochastic gradi-
ent momentum while using Algorithm 2 to select the
gradient estimator at three diﬀerent times through-
out optimization. The set of candidate estimators is
GAuto = {ga|a ∈ R3}, where ga is as deﬁned in eq. (7),
the base estimator is plain reparameterization, and
there are three candidate control variates (c1, c2, c3).

The goal is to check if Alg. 2 successfully navigates
time / variance tradeoﬀs. We thus compare against
using ﬁxed subsets of control variates with the weights
that minimize the estimator’s variance (which can be
estimated eﬃciently [3]). We consider each possible
subset of control variates S ⊆ {c1, c2, c3}. This is es-
sentially the same as Algorithm 2, but with the set of
estimators GS, which contains all estimators that use
the ﬁxed subset of control variates S. Since all esti-
mators in GS use the same control variates, they all
have the same cost. Therefore, ﬁnding the estimator
with optimal cost variance tradeoﬀ (minimum ˆG2 ˆT )
reduces to ﬁnding the estimator with minimum ˆG.

5.1 Experimental details

5.2 Results

Gradient Estimator Selection

Tasks and datasets: We use SGVI on several prob-
abilistic models: (i) Bayesian logistic regression with
the datasets Mushrooms and a1a, with a standard nor-
mal prior on the weights; (ii) a hierarchical Poisson
model with the Frisk dataset [4]; (iii) a Bayesian neural
network with the Red-wine dataset, with a scaled stan-
dard normal prior on the weights and biases; and (iv)
a Bayesian neural network with the Red-wine dataset,
with a hierarchical distribution as prior on the weights
and biases. (The likelihoods for (iii) and (iv) are the
same, they only diﬀer in the prior used. We denote
them by BNN-A and BNN-B, respectively.) Details
for all models are given in the appendix.

Variational distribution qw(z): For the simpler lo-
gistic regression models we use a Gaussian distribution
with a full-rank covariance matrix as variational dis-
tribution. For the other more complex models we use
a factorized Gaussian (diagonal covariance matrix).

Base gradient estimator: As base gradient es-
timator, gbase, we use what seems to be the most
common estimator. We compute the entropy term
∇w Eqw [log qw(Z)] in closed form, and estimate the term
∇w Eqw [log p(x, Z)] with reparameterization.4

Control variates used:
c1: Diﬀerence between the entropy term computed
exactly and estimated using reparameterization:
c(w, ξ) = ∇w log qw(Tw(ξ)) − ∇w Eqw log qw(Z).
[8] based on a
c2: Control variate by Miller et al.
second order Taylor expansion of log p(x, z) (Sec 2.2).
term computed
c3: Diﬀerence between the prior
exactly and estimated using reparameterization:
c(w, ξ) = ∇w log p(Tw(ξ)) − ∇w Eqw log p(Z).

Algorithmic details: For Alg. 2 we use M = 400
to estimate ˆG2 (except for Logistic regression, where
we use M = 200). We re-select the optimal estima-
tor three times during training, initially, after 10% of
training is done, and after 50% of training is done.

Optimization details: We use SGD with momentum
(β = 0.9) with 5 samples z ∼ qw(z) to form the Monte
Carlo gradient estimates. For all models we ﬁnd an
initial set of parameters by optimizing with the base
gradient for 300 steps and a ﬁxed learning rate of 10−5.
This initialization was helpful in practice because w
tends to change rapidly at the beginning of optimiza-
tion. After this brief initialization, G2(g, w) tends to
change much more slowly, meaning our technique is
more helpful.

4Using Tw(ξ) = µ + D1/2ξ, where ξ ∼ N (0, I), µ is the
mean of qw and D1/2 is the Cholesky factorization of the
covariance of qw.

Fig.
3 shows the optimization results (“ELBO vs.
Time” plots). All simulations were performed inde-
pendently 20 times, and the average result is shown.
The performance of all algorithms depends on the step-
size. To give a fair comparison, Fig. 3 summarizes by
showing the results with the best step-sizes.5 It can
be observed that automatically selecting what control
variates to use and their weights (using Alg. 2) leads
to good results for all models, as good as the ones
obtained using the best ﬁxed set of control variates
chosen retrospectively.
It can also be observed that
results depend heavily on the model considered. For
instance, for some models using the base gradient with-
out any control variates appears to be the best choice
(BNN-B), while for others it is suboptimal (BNN-A).

Raw results for individual step-sizes are shown in the
appendix. The G2T principle is not ideal when the
step-sizes are chosen poorly. For example, if the step-
size is chosen very small, convergence may be “itera-
tion limited” rather than “variance limited”, so that
reducing variance of the gradient estimator does not
speed up convergence. This is all consistent with our
theory – the convergence rates in Section 3 are based
on well-chosen step-sizes.

Fig. 4 shows a summary of the results. For each
model, it shows the ﬁnal training ELBO achieved by
using each possible ﬁxed subset of control variates and
It can be observed
the automatically chosen ones.
that there is no ﬁxed subset of control variates that
performs well across all models. However, using the
automatically chosen control variates does.

Figures 3 and 4 indicate that Alg. 2 selects a “good”
set of control variates (and weights). What control
variates are being chosen? This is shown in Table 3.
It can be observed that the algorithm makes diﬀerent
choices for diﬀerent models, and that the control vari-
ates chosen depend on the stage of the training process
(initially, or later stages). As shown in Fig. 3, these
choices lead to good results.
6 Conclusion

Results show that there is no single optimal estimator.
The best estimator depends on several factors, such as
model, dataset, and implementation. Estimator selec-
tion must be done adaptively. We presented the “G2T
principle” and used it to derive two gradient estima-
tor selection algorithms. We showed empirically that
both these algorithms work well in practice on infer-
ence problems tackled using SGVI.

Despite the empirical success of the proposed algo-

512 stepsizes between 10−6 and 10−3 were considered.

Gradient Estimator Selection

Figure 3: Automatically selecting what control variates to use performs comparably to the best
ﬁxed set of control variates chosen with hindsight. (Higher ELBO is better.) “ELBO vs Time” plots
obtained using Alg. 2 to select what control variates to use and their weights (plotted in black). We compare
against using diﬀerent ﬁxed subsets of control variates with the weights that minimize the estimator’s variance.
Five models considered, described in the text. Lines are identiﬁed as follows: “Auto” stands for using Alg. 2 to
select what control variates to use and their weights, “Base” stands for optimizing using the base gradient alone,
“1” stands for using the ﬁxed set of control variates {c1} with the minimum variance weights, “12” stands for
using the ﬁxed set of control variates {c1, c2}, and so on. Overheads incurred by Algorithm 2 (proﬁling phase
and MIQCP solving) are included in the times. (Note: For the model logistic regression (mushrooms) computing
c2 is slow. When c2 is used, optimization does not converge in the amount of time provided.)

Model

Time of choice

T = 0

T = Topt/10

T = Topt/2

Log Reg (a1a)
Log Reg (mush)
Hier Poisson
BNN-A
BNN-B

{c1, c3}
∅
{c2}
{c3}
∅

{c1}
∅
{c2}
{c1}
∅

{c1}
{c1}
{c2}
{c1}
∅

Table 3: Algorithm 2 selects diﬀerent control
variates for diﬀerent models. Control variates cho-
sen by Algorithm 2. Each column shows the set of
control variates selected at some selection time. (In
diﬀerent runs diﬀerent choices may ocurr. We shows
the “most popular” choice across runs.)

rithms, several open questions remain: Can conver-
gence guarantees use “local” G2 values, as opposed to
(looser) global bounds? Should algorithms adapt (e.g.
by changing the step size) when the underlying gra-
dient estimator changes? We plan to address these
questions in future work.

Figure 4: The automatically chosen control vari-
ates perform well in all models. (Higher ELBO
is better.) For each model, we show the ELBO at the
ﬁnal time in Fig. 3, with the lowest ELBO scaled to
zero (bottom) and the highest scaled to one (top).

255075100125150175200168166164162160158156154ELBO13Auto1323123122BaseBNN-A (red wine)6080100120140160180200550540530520510500490AutoBase131321231223BNN-B (red wine)20406080100120845.6845.4845.2845.0844.8844.6844.4844.2844.022312312Auto1Base133Hierarchical Poisson (frisk)20406080100120Time (s)280260240220200180ELBOBaseAuto1313Log. Regression (mushrooms)020406080100120Time (s)615.0612.5610.0607.5605.0602.5600.0597.51AutoBase13322312123Log. Regression (a1a)BNN-ABNN-BHRLR-a1aLR-mushModel0.00.20.40.60.81.0Final ELBO (scaled)123121323123BaseAutoGradient Estimator Selection

References

[1] Felix V Agakov and David Barber. An auxiliary
variational method. In International Conference
on Neural Information Processing, pages 561–566.
Springer, 2004.

[2] L´eon Bottou, Frank E Curtis, and Jorge No-
cedal. Optimization methods for large-scale ma-
chine learning. Siam Review, 60(2):223–311, 2018.

[3] Tomas Geﬀner and Justin Domke. Using large
ensembles of control variates for variational infer-
ence. In Advances in Neural Information Process-
ing Systems, pages 9960–9970, 2018.

[4] Andrew Gelman, Jeﬀrey Fagan, and Alex Kiss.
An analysis of the new york city police depart-
ment’s “stop-and-frisk” policy in the context of
claims of racial bias. Journal of the American
Statistical Association, 102(479):813–823, 2007.

[5] Will Grathwohl, Dami Choi, Yuhuai Wu, Geoﬀ
Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for
black-box gradient estimation. In Proceedings of

the International Conference on Learning Repre-
sentations, 2018.

[6] LLC Gurobi Optimization. Gurobi optimizer ref-

erence manual, 2018.

[7] Diederik P Kingma and Max Welling. Auto-
encoding variational bayes. In Proceedings of the
International Conference on Learning Represen-
tations, 2013.

[8] Andrew Miller, Nick Foti, Alexander D’Amour,
and Ryan P Adams. Reducing reparameteriza-
tion gradient variance. In Advances in Neural In-
formation Processing Systems, pages 3708–3718,
2017.

[9] Andriy Mnih and Karol Gregor. Neural varia-
tional inference and learning in belief networks.
In International Conference on Machine Learn-
ing, 2014.

[10] Andriy Mnih and Danilo Rezende. Variational
In Inter-
inference for monte carlo objectives.
national Conference on Machine Learning, pages
2188–2196, 2016.

[11] Arkadi Nemirovski, Anatoli Juditsky, Guanghui
Lan, and Alexander Shapiro. Robust stochas-
tic approximation approach to stochastic pro-
gramming.
SIAM Journal on optimization,
19(4):1574–1609, 2009.

[12] Art B. Owen. Monte Carlo theory, methods and

examples. 2013.

[13] John Paisley, David Blei, and Michael Jordan.
Variational bayesian inference with stochastic
search.
the 29th Interna-
tional Conference on Machine Learning (ICML-
12), pages 1363–1370, 2012.

In Proceedings of

[14] Alexander Rakhlin, Ohad Shamir, and Karthik
Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In Pro-
ceedings of the 29th International Conference on
Machine Learning (ICML-12), pages 1571–1578,
2012.

[15] Rajesh Ranganath, Sean Gerrish, and David Blei.
Black box variational inference. In Artiﬁcial In-
telligence and Statistics, pages 814–822, 2014.

[16] Danilo Jimenez Rezende, Shakir Mohamed, and
Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative mod-
els. In Proceedings of the 31st International Con-
ference on Machine Learning (ICML-14), pages
1278–1286, 2014.

Gradient Estimator Selection

Sticking the landing: Simple,

[17] Geoﬀrey Roeder, Yuhuai Wu, and David K Du-
venaud.
lower-
variance gradient estimators for variational infer-
ence. In Advances in Neural Information Process-
ing Systems, pages 6925–6934, 2017.

[18] Francisco Ruiz, Titsias Michalis, and David Blei.
The generalized reparameterization gradient. In
Advances in Neural Information Processing Sys-
tems, pages 460–468, 2016.

[19] John Schulman, Nicolas Heess, Theophane We-
ber, and Pieter Abbeel. Gradient estimation us-
ing stochastic computation graphs. In Advances
in Neural Information Processing Systems, pages
3528–3536, 2015.

[20] Ohad Shamir and Tong Zhang. Stochastic gra-
dient descent for non-smooth optimization: Con-
vergence results and optimal averaging schemes.
In International Conference on Machine Learn-
ing, pages 71–79, 2013.

[21] Michalis Titsias and Miguel L´azaro-Gredilla.
Doubly stochastic variational bayes for non-
conjugate inference.
In Proceedings of the 31st
International Conference on Machine Learning
(ICML-14), pages 1971–1979, 2014.

[22] Michalis Titsias and Miguel L´azaro-Gredilla. Lo-
cal expectation gradients for black box variational
inference. In Advances in neural information pro-
cessing systems, pages 2638–2646, 2015.

[23] George Tucker, Andriy Mnih, Chris J Maddison,
John Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for dis-
crete latent variable models. In Advances in Neu-
ral Information Processing Systems, pages 2627–
2636, 2017.

[24] Ronald J Williams. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256,
1992.

[25] Tianbao Yang, Qihang Lin, and Zhe Li. Uni-
ﬁed convergence analysis of stochastic momentum
methods for convex and non-convex optimization.
arXiv preprint arXiv:1604.03257, 2016.

Gradient Estimator Selection

7 Appendix

7.1 Details on models used

Throughout the paper four diﬀerent models are con-
two variants of a Bayesian neural network
sidered:
(BNN-A and BNN-B), a hierarchical Poisson model,
and Bayesian logistic regression.

Bayesian logistic regression: We consider two
diﬀerent datasets: “a1a” and “Mushrooms”.
In all
cases the training set is given by D = {xi, yi}, where
yi is binary. The model is speciﬁed by

wi ∼ N (0, 1),
pi = (1 + exp(w0 + w · xi))−1 ,
yi ∼ Bernoulli(pi).

Hierarchical Poisson model: By Gelman et al.
[4]. The model measures the relative stop-and-frisk
events in diﬀerent precincts in New York city, for dif-
ferent ethnicities. The model is speciﬁed by

µ ∼ N (0, 102)
log σα ∼ N (0, 102),
log σβ ∼ N (0, 102),
αe ∼ N (0, σ2
α),
βp ∼ N (0, σ2
β),
λep = exp(µ + αe + βp + log Nep),
Yep ∼ Poisson(λep).

In this case, e stands for ethnicity and p for precinct,
Yep for the number of stops in precinct p within ethnic-
ity group e (observed), and Nep for the total number
of arrests in precinct p within ethnicity group e (ob-
served).

BNN-A: As done by Miller et al. [8] we use a subset
of 100 rows from the “Red-wine” dataset (regression).
We implement a neural network with one hidden layer
with 50 units and Relu activations. Let D = {xi, yi}
be the training set. The model is speciﬁed by

BNN-B: We use a subset of 200 rows from the “Red-
wine” dataset (regression). We implement a neural
network with one hidden layer with 50 units and Relu
activations. Let D = {xi, yi} be the training set. The
model is speciﬁed by

log τ ∼ N (0, 52),
wi ∼ N (0, 52),
ˆyi = FeedForward(xi, W ),
yi ∼ N ( ˆYi, τ 2).

(weights and biases)

The only diﬀerence between BNN-A and BNN-B is in
the prior used for the weights and biases.

7.2 Mixed Integer Quadratic Program

A mixed integer quadratic program is an optimiza-
tion problem in which the objective function and con-
straints are quadratic (or linear), and some (or all)
variables are restricted to be integers:

minimize
x

s.t.

1
2
1
2

x(cid:62)Q0x + r(cid:62)

0 x + u0

x(cid:62)Qix + r(cid:62)

i x + ui ≥ 0

i = 1, ..., m,

(12)

Ax + b = 0,

where x ∈ Rn, Q0, ..., Qm ∈ Rn×n, and some compo-
nents of x are restricted to be integers.

We now prove Theorem 4.1.

Theorem 4.1. When diﬀerent gradient estimators
are indexed by a set of J control variate weights, the
problem of ﬁnding a∗(w) as in equation (10) can be
reduced to solving a mixed integer quadratically con-
strained program with 2J + 2 variables, one quadratic
constraint, and one linear constraint.

Proof. Given

¯T (ga) = ¯T (gbase) +

J
(cid:88)

i=1

¯T (ci) 1[ai (cid:54)= 0]

(13)

log α ∼ N (0, 102),
log τ ∼ N (0, 102),
wi ∼ N (0, α2),
ˆyi = FeedForward(xi, W ),
yi ∼ N (ˆyi, τ 2).

and

¯G2(ga, w) =

1
M

M
(cid:88)

m=1

(weights and biases)

we want to ﬁnd

(cid:107)gbase(w, ξm) + C(w, ξm)a(cid:107)2,

(14)

a∗(w) = arg min

a∈RJ

¯G2(ga, w) × ¯T (ga).

(15)

Gradient Estimator Selection

To simplify notation, we use ¯G2 = ¯G2(ga, w), gbm =
gbase(w, ξm) and Cm = C(w, ξm). Expanding the
squared norm in eq. 14 we get

(cid:107)gbm + Cma(cid:107)2

(cid:0)g(cid:62)

bm gbm + 2g(cid:62)

bmCma + a(cid:62)C (cid:62)

mCma(cid:1)

(cid:107)gbm(cid:107)2

+

(cid:32)

2
M

M
(cid:88)

(cid:33)

g(cid:62)
bmCm

a

(16)

m=1
(cid:123)(cid:122)
r1

(cid:125)

(cid:125)

(cid:124)

¯G2 =

=

=

1
M

1
M

1
M
(cid:124)

M
(cid:88)

m=1

M
(cid:88)

m=1

M
(cid:88)

m=1
(cid:123)(cid:122)
u1

+

1
2

a(cid:62)

2
M

(cid:32)

(cid:124)

M
(cid:88)

(cid:33)

C (cid:62)

mCm

a

(cid:125)

m=1
(cid:123)(cid:122)
Q1
1
2

= u1 + r(cid:62)

1 a +

a(cid:62)Q1a.

Convexity: F is convex iﬀ it F (θx + (1 − θ)y) ≤

θF (x) + (1 − θ)F (y) ∀θ ∈ [0, 1].

F (y) ≥ F (x) + ∇F (x)(cid:62)(y − x) + λ

Strong convexity: F is λ-strongly convex iﬀ
2 ||y − x||2 ∀x, y.
continuous gra-
iﬀ

dient with constant L (F is L-smooth)
||∇F (y) − ∇F (x)|| ≤ L||y − x|| ∀x, y.

Smoothness: F has Lipschitz

We now state the convergence rates presented in Table
2 in more detail.

Theorem 7.1 (F strongly convex; decaying step–
size; By Rakhlin et al.
[14]). Let F be a λ-strongly
convex function over a convex set W . If we assume
Eξ[||g(w, ξ)||2] ≤ G2 ∀w ∈ W and set the step size
ηt = 1/(λt), then, after K updates of SGD, we have

(17)

E (cid:2)||wK − w∗||2(cid:3) ≤

4
λ2

G2
K

; where w∗ = argminwF (w)

On the other hand, equation 13 can be expressed as

Proof. See Rakhlin et al. [14].

¯T (ga) = t0 + t(cid:62)b, s.t. bi = 1[ai (cid:54)= 0],

(18)

where t0 = ¯T (gbase), and ti = ¯T (ci). Using equations
17 and 18, the minimization problem from equation 15
can be expressed as

(a∗, b∗) = arg min

a∈RJ ,b∈{0,1}J

(cid:18) 1
2

s.t bi = 1[ai (cid:54)= 0].

a(cid:62)Q1a + r(cid:62)

1 a + u1

(cid:19)

(cid:16)
t0 + t(cid:62)b

(cid:17)

,

×

(19)

Introducing two extra varaibles, VG and VT , we can
express the minimization problem in eq. 19 as

(a∗, b∗, V ∗

G, V ∗

T ) =

arg min
a∈RJ ,b∈{0,1}J ,VG∈R,VT ∈R

VG × VT ,

s.t VG ≥

1
2

a(cid:62)Q1a + r(cid:62)

1 a + u1

VT = t0 + t(cid:62)b
bi = 1[ai (cid:54)= 0].

(20)

The ﬁnal minimization problem shown in equation 20
has the form of a general MIQCP, shown in equa-
tion 12, with the exception of the last constraint
bi = 1[ai (cid:54)= 0]. Despite not being in the original deﬁ-
nition of a MIQCP, several solver accept constraints of
this type (Gurobi [6], the solver used in our simulation,
does).

7.3 SGD convergence rates

Corollary 7.1.1 (F strongly convex and smooth; de-
caying step-size; By Rakhlin et al. [14]). Let F be an
L-smooth and λ-strongly convex function over a con-
vex set W . If we assume Eξ[||g(w, ξ)||2] ≤ G2 ∀w ∈ W
and set the step size ηt = 1/(λt), then, after K updates
of SGD, we have

E [F (wK) − F (w∗)] ≤

2L
λ2

G2
K

.

Proof. See Rakhlin et al. [14].

Theorem 7.2 (F convex; constant step-size; Ne-
mirovski et al. [11]). Let F be a convex function over a
convex set W . Assume Eξ[||g(w, ξ)||2] ≤ G2 ∀w ∈ W .
Then, after K updates of SGD using the optimal learn-
ing rate η∗ = Dw
√
K

we have

G

E [F ( ¯w) − F (w∗)] ≤ Dw

G
√
K

,

where ¯w = 1
K

(cid:80)K

i=1 wi and Dw = ||w0 − w∗||.

Proof. See Nemirovski et al. [11].

Theorem 7.3 (F smooth; constant step-size). Let F
be an L-smooth function. Assume Eξ[||g(w, ξ)||2] ≤
G2 ∀w. Then, after K updates of SGD using the opti-
(cid:113) 2Df
mal learning rate η∗ =

LKG2 we have

(cid:34)

E

1
K

K
(cid:88)

i=1

||∇F (wi)||2

(cid:35)

≤ (cid:112)LDf

G
√
K

,

Recall the following deﬁnitions:

where Df = E[F (w0) − F (w∗)].

Gradient Estimator Selection

Theorem 7.4 (F smooth; constant step-size; stochas-
tic momentum methods; Yang et al.
[25]). Let F
be a (possibly non convex) L-smooth function, As-
sume Eξ[||g(w, ξ) − ∇F (w)||2] ≤ δ2 and ||∇F (w)||2 ≤
V 2. Then, after K updates of the proposed stochas-
tic momentum method (β) with learning rate η =

min

(cid:110) 1−β

2L , C√

T

(cid:111)

, we have

min
k=0,...,K

E ||∇F (wk)||2 ≤

2Df (1 − β)
T

max

(cid:40)

2L
1 − β

,

(cid:41)

√

T
C

E ||gt||2

+

C
√
T

Lβ2 ((1 − β)s − 1)2 (V 2 + δ2) + Lδ2(1 − β)2
(1 − β)3

,

where Df = F (w0) − F (w∗).

The results shown in Table 2 are obtained by: 1) set-
ting s = 0 (momentum) or s = 1 (Nesterov); 2) ﬁnding
the optimal C; 3) bounding Eξ ||g(w, ξ)||2 ≤ δ2 + V 2 =
G2; and 4) assuming that optimization is performed
for a large enough number of steps

K ≥ 4L2C2
1−β

(cid:16)

(cid:17)

.

7.4 Raw results for individual step-sizes

Proof. This proof is a straightforward adaptation from
the one by Bottou et al. [2].

F (wt+1) ≤ F (wt) + ∇F (wt)(cid:62)(wt+1 − wt) +

||wt+1 − wt||2

= F (wt) − ηt∇F (wt)(cid:62)gt +

= F (wt) − ηt∇F (wt)(cid:62)gt +

L
2
||ηtgt||2

||gt||2.

L
2
Lη2
t
2

Taking the expectation on both sides we get

E[F (wt+1) − F (wt)] ≤ −ηt∇ E[F (wt)(cid:62)gt] +

= −ηt E ||∇F (wt)||2 +

Lη2
t
2
Lη2
t
2

G2.

If we take ηt = η, and sum up both sides of the in-
equality we get

K−1
(cid:88)

t=0

E[F (wt+1) − F (wt)] ≤ −η

E[F (wK ) − F (w0)] ≤ −η

K−1
(cid:88)

t=0

K−1
(cid:88)

t=0

E ||∇F (wt)||2 +

E ||∇F (wt)||2 +

KLη2
2

G2

KLη2
2

G2.

Re-arranging and using the fact that F (w∗) ≤ F (wK)
gives

(cid:34)

E

1
K

K−1
(cid:88)

t=0

(cid:35)

||∇F (wt)||2

≤

=

Lη
2

Lη
2

G2 +

F (w0) − F (w∗)
ηK

G2 +

Df
ηK

.

Where Df = F (w0) − F (w∗). The value for η that
minimizes the right hand side of the last inequality is
η∗ =

LKG2 . Using η = η∗ yields

(cid:113) Df

(cid:34)

E

1
K

K−1
(cid:88)

t=0

(cid:35)

||∇F (wt)||2

≤

(cid:114)

L(F (w0) − F (w∗))G2
K

.

Finally, slightly diﬀerent versions of the last two
bounds in Table 2 were proposed by Yang et al. [25].
The authors carry out a uniﬁed analysis for stochas-
tic momentum methods using a parameter s; if s = 0
the algorithm results in the heavy ball method, and if
s = 1 in a stochastic variant of Nesterov’s accelerated
gradient. They state the following result:

Gradient Estimator Selection

020406080100120140Time (s)140012001000800600400ELBOBaseAuto311312312232Log. Regr. (mush) ; step-size: 1e-06020406080100120140Time (s)1000800600400200ELBOBaseAuto131323123212Log. Regr. (mush) ; step-size: 5e-06020406080100120140Time (s)900800700600500ELBO21212323Log. Regr. (mush) ; step-size: 1e-05Gradient Estimator Selection

020406080100120140Time (s)11001000900800700ELBOBaseAuto131323121232Log. Regr. (a1a) ; step-size: 1e-06020406080100120140Time (s)800775750725700675650625600ELBOBaseAuto131322312123Log. Regr. (a1a) ; step-size: 5e-06020406080100120140Time (s)680660640620600ELBO1Auto13Base312123232Log. Regr. (a1a) ; step-size: 1e-05020406080100120140Time (s)608606604602600ELBO223Log. Regr. (a1a) ; step-size: 5e-05Gradient Estimator Selection

020406080100120140Time (s)105010251000975950925900875850ELBOBase1313Auto21223123Hier. Regr. (frisk) ; step-size: 1e-06020406080100120140Time (s)870865860855850845ELBO1Base32Auto131223123Hier. Regr. (frisk) ; step-size: 5e-06020406080100120140Time (s)851850849848847846845ELBOAuto212231131233BaseHier. Regr. (frisk) ; step-size: 1e-05020406080100120140Time (s)856854852850848846844ELBO2Auto23123121133BaseHier. Regr. (frisk) ; step-size: 5e-05Gradient Estimator Selection

050100150200Time (s)440430420410400ELBOBase1Auto31321223123BNN-A ; step-size: 1e-06050100150200Time (s)420400380360340320ELBOBase1Auto31321223123BNN-A ; step-size: 5e-06050100150200Time (s)400380360340320300280260ELBOBase1Auto31321223123BNN-A ; step-size: 1e-05050100150200Time (s)320300280260240220200180ELBOBase1Auto31321223123BNN-A ; step-size: 5e-05050100150200Time (s)280260240220200180160ELBOBase1Auto31321223123BNN-A ; step-size: 0.0001050100150200Time (s)185180175170165160155ELBO1Auto31321223123BaseBNN-A ; step-size: 0.0005050100150200Time (s)180175170165160155ELBO31312323Auto12BaseBNN-A ; step-size: 0.001Gradient Estimator Selection

050100150200Time (s)114011201100108010601040ELBOAutoBase311322312312BNN-B ; step-size: 1e-06050100150200Time (s)11001000900800700ELBOAutoBase311322312312BNN-B ; step-size: 5e-06050100150200Time (s)1000900800700600ELBOAutoBase311322312123BNN-B ; step-size: 1e-05050100150200Time (s)675650625600575550525500ELBO1BaseAuto13312212323BNN-B ; step-size: 5e-05050100150200Time (s)540535530525520515510ELBO12113123AutoBase3232BNN-B ; step-size: 0.0001050100150200Time (s)575570565560555550ELBO123131BaseAuto312232BNN-B ; step-size: 0.0005050100150200Time (s)590585580575570ELBO231231332112BaseAutoBNN-B ; step-size: 0.001