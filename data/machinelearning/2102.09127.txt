Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Lingjiao Chen 1 Matei Zaharia 1 James Zou 1 2

2
2
0
2

l
u
J

6
1

]

G
L
.
s
c
[

2
v
7
2
1
9
0
.
2
0
1
2
:
v
i
X
r
a

Abstract

Multi-label classiﬁcation tasks such as OCR and
multi-object recognition are a major focus of the
growing machine learning as a service industry.
While many multi-label APIs are available, it is
challenging for users to decide which API to use
for their own data and budget, due to the hetero-
geneity in their prices and performance. Recent
work has shown how to efﬁciently select and com-
bine single-label APIs to optimize performance
and cost. However, its computation cost is expo-
nential in the number of labels, and is not suitable
for settings like OCR. In this work, we propose
FrugalMCT, a principled framework that adap-
tively selects the APIs to use for different data
in an online fashion while respecting the user’s
budget. It allows combining ML APIs’ predic-
tions for any single data point, and selects the
best combination based on an accuracy estimator.
We run systematic experiments using ML APIs
from Google, Microsoft, Amazon, IBM, Tencent,
and other providers for tasks including multi-label
image classiﬁcation, scene text recognition and
named entity recognition. Across these tasks, Fru-
galMCT can achieve over 90% cost reduction
while matching the accuracy of the best single
API, or up to 8% better accuracy while matching
the best API’s cost.

1. Introduction

Many machine learning users are starting to adopt machine
learning as a service (MLaaS) APIs to obtain high-quality
predictions. One of the most common tasks these APIs
target is multi-label classiﬁcation. For example, one can use
Google’s computer vision API (Goo) to tag an image with
a wide range of possible labels for $0.0015, or Microsoft’s

1Department of Computer Sciences, Stanford University, Stan-
ford, USA 2Department of Biomedical Data Science, Stanford
University, Stanford, USA. Correspondence to: Lingjiao Chen
<lingjiao@stanford.edu>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

API (Mic) for $0.0010. Another example is to extract all text
strings from an image for $0.005 via iFLYTEK’s API (Iﬂ) or
$0.021 via Tencent’s API (Ten). In practice, these APIs also
provide different performance on different types of input
data (e.g., English vs Chinese text). The heterogeneity in
APIs’ performance and prices makes it hard for users to
decide which API, or combination of APIs, to use for their
own datasets and budgets.

Recent work (Chen et al., 2020) proposed FrugalML, an
algorithmic framework that adaptively decides which APIs
to call for a data point to optimize accuracy and cost. Their
approach learns a fast decision rule for each possible output
label that can signiﬁcantly improve cost-performance over
the individual APIs. However, FrugalML requires a large
amount of training data and involves solving a non-convex
optimization problem with complexity exponential in the
number of distinct labels. This prevents it from being used
for tasks with large number of labels, such as multi-label
classiﬁcation. Furthermore, FrugalML ignores correlation
between different APIs’ predictions, potentially limiting its
accuracy. For example, APIs A and B may output {person,
car} and {car, bike} separately for an image whose true
keywords are {person, car, bike}. FrugalML would select
one of the two label sets, but combining them results in the
true label set and thus higher accuracy. Thus, this paper
aims to solve these signiﬁcant limitations and address the
question: how do we design efﬁcient ML API selection
strategies for multi-label classiﬁcation tasks to maximize
accuracy within a budget?

We propose FrugalMCT, a principled framework that learns
the relative strengths of different combinations of multi-
label classiﬁcation APIs and efﬁciently selects the optimal
combinations of APIs to call for different data and budget
constraints. As shown in Fig. 1 (a), FrugalMCT directly
estimates the accuracy of each API combination on a par-
ticular input based on the features and predicted labels of
that input. Then it uses a fast service selector based on the
estimated accuracy to balance accuracy and budget. For
example, we might ﬁrst call API A on an input. If A returns
person and teddy bear and the accuracy predictor gives rel-
atively high estimated accuracy (Fig. 1 (c)), then we stop
and report {person, teddy bear} as the label set. If A returns
person and tennis racket, and we predict that combining
it with API B’s output gives a much higher accuracy, then

 
 
 
 
 
 
Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Figure 1. Demonstration of FrugalMCT. (a): FrugalMCT workﬂow. (b): Performance of FrugalMCT on COCO, a multi-label image
dataset, using real commercial ML APIs. (c), (d): Examples of FrugalMCT’s behavior on different inputs. In (c), FrugalMCT estimates
that the accuracy of a cheap open source SSD model from GitHub is high, and thus directly returns its predictions. In (d), FrugalMCT
estimates that combining SSD’s results with the Everypixel API has a much higher estimated accuracy, and thus it invokes EveryPixel and
combines its results with SSD’s results.

we invoke API B and combine their prediction to obtain
{person, sports ball, tennis racket} (Fig. 1 (d)).

Contributions. FrugalMCT is an end-to-end approach
that integrates the selection of APIs and the combination
of their outputs for individual user queries. It leverages our
key new ﬁnding that current commercial APIs have comple-
mentary strengths and weaknesses, and that we can reliably
predict which APIs are likely to work well for a new query
based on easy-to-generate metadata about its input. Based
on this API accuracy predictor, FrugalMCT then leverages
an efﬁcient online algorithm to determine which combina-
tion of APIs to call for different user queries. We show that
the online algorithm enjoys an accuracy provably close to
the ofﬂine method as well as a small computational cost.
All components in FrugalMCT are trainable, making it easy
to customize for different applications. To our knowledge,
FrugalMCT is the ﬁrst work on how to effectively select
and combine multi-label ML APIs.

Empirically, FrugalMCT produces substantially better pre-
diction performance than individual APIs and than Fru-
galML adapted for multi-label tasks (Fig. 1 (b)). Extensive
experiments with real commercial APIs on several tasks,
including multi-label image classiﬁcations, scene text recog-
nition, and named entity recognition, show that FrugalMCT
typically provides over 60% (as high as 98%) cost reduction

when aiming to match the best commercial API’s perfor-
mance. Also, when targeting the same cost as the best
commercial API, FrugalMCT can improve performance up
to 8%. As a dataset contribution, we have also released 1 our
dataset of 295,212 samples annotated by commercial multi-
label APIs as the largest dataset and resource for studying
multi-label ML prediction APIs.

2. Related Work

MLaaS: With the growing importance and adoption of
MLaaS APIs (Ama; Ten; Goo; IBM; Mic), existing research
has largely focused on evaluating individual APIs for their
performance (Yao et al., 2017), robustness (Hosseini et al.,
2017), biases (Koenecke et al., 2020), performance estima-
tion (Chen et al., 2021), pricing (Chen et al., 2019),and
applications (Buolamwini & Gebru, 2018; Hosseini et al.,
2019; Reis et al., 2018). Recent work on FrugalML (Chen
et al., 2020) studies API calling strategies for single label
classiﬁcation. While their approach’s computational com-
plexity is exponential in the number of labels, FrugalMCT’s
complexity does not depend on the number of labels, mak-
ing it suitable for multi-label prediction APIs. In addition,
FrugalML selects only one API per user query, while Fru-
galMCT considers the combination of multiple APIs’ out-

1https://github.com/lchen001/FrugalMCT

(b): PerformanceInput ImagesAccuracyPredictorLabelCombinerServiceSelector(a): Proposed FrugalMCT$0.01$10$6$15person:0.12, teddy bear:0.27{person, teddy bear}GitHub(SSD)(c) Example 1 =[0.51, 0.51, 0.45, 0.54]person:0.46, tennis racket :0.18 {person, sports ball, tennis racket}GitHub (SSD)thres=0.08person:0.46sports ball :0.52weight=0.6=[0.58, 0.92, 0.92, 0.60](d) Example 2AccuracyPredictorServiceSelectorAccuracyPredictorServiceSelectorLabelCombinerAccuracyPredictorweight=0.4Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

put for each input data. This improves the overall accuracy
(as shown in Sec 5), but also creates unique optimization
challenges that we solve.

Ensembles for multi-label classiﬁcation: Ensemble learn-
ing is a natural approach to combine different predictors’
output. Several ensemble methods have been developed,
such as using pruned sets (Read et al., 2008), classiﬁer
chains (Read et al., 2011), and random subsets (Tsoumakas
& Vlahavas, 2007), with applications in image annota-
tions (Xu et al., 2011), document classiﬁcation (Chen et al.,
2017), and speech categorization (Liu et al., 2019). Moyano
et al. (2018) provide a detailed survey of this area. Almost
all of these ensemble methods require joint training of the
base classiﬁers, but MLaaS APIs are black box to the users.
Also, while ensemble methods focus only on improving
accuracy, FrugalMCT explicitly considers the cost of each
API and enforces a budget constraint.

Model cascades: A series of works (Viola & Jones,
2001a;b; Sun et al., 2013; Cai et al., 2015; Wang et al.,
2011; Xu et al., 2014; Chen et al., 2018; Kumar et al., 2018;
Chen et al., 2018) explores cascades (a sequence of mod-
els) to balance the quality and runtime of inference. Model
cascades use a single predicted quality score to avoid call-
ing computationally expensive models, but FrugalMCT’
strategies utilize both quality scores and predicted label sets
to select an expensive add-on service. While cascades do
not explicitly specify inference speed, FrugalMCT allows
users to explicitly incorporate different budget requirements.
Designing such strategies requires solving a signiﬁcantly
harder optimization problem, e.g., choosing how to divide
the available budget between classes (§4), but also improves
performance substantially over using the quality score alone
(§5).

AutoML for multi-label classiﬁcation: AutoML (Thorn-
ton et al., 2013) automates the customization of ML
pipelines,
including the selection, combination, and
parametrization of the learning algorithms. There is a rich
literature of AutoML techniques for standard single label
tasks, and fewer methods on multi-label predictions (Wever
et al., 2021) (e.g. genetic algorithms (de Sá et al., 2017)
and a neural network-based search scheme (Pakrashi &
Namee, 2019)). We refer interested readers to a recent sur-
vey (Wever et al., 2021) for more details. Applying AutoML
to use multiple ML APIs is underexplored, and FrugalMCT
can be viewed as the ﬁrst AutoML approch designed for
automating the selection of multiple mutlti-label ML APIs.
While most AutoML systems exclusively focus on predic-
tion performance, FrugalMCT optimizes accuracy and cost
jointly, which is desirable for cost-sensitive API users.

Multiple choice knapsack and integer programming:
Many resource allocation problems can be modeled as multi-
ple choice knapsack problem (MCKP) (Pamela H. Vance &

Toth), 1993), such as keyword bidding (Zhou & Naroditskiy,
2008) and quality of service control (Lee et al., 1999). While
NP-hard (Sinha & Zoltners, 1979), various approximations
have been proposed for MCKP, such as branch and bound
(Pamela H. Vance & Toth), 1993), convex hull relaxation
(Akbar et al., 2006) and bi-objective transformation (Bednar-
czuk et al., 2018). Inherently an integer linear programming
(ILP) problem, MCKP can also be tackled by ILP solvers,
motivated by online adwords searching (Devanur & Hayes,
2009), resource allocation (Devanur & Hayes, 2019) and
general linear programming (Li et al., 2020). The service
selector of FrugalMCT can be viewed as a MCKP with the
same item cost vector per item group, which we leverage to
obtain a customized fast and online solver. Our goal is to
not develop novel MCKP solver, but to efﬁciently adapt ILP
methods as a subroutine of our end-to-end FrugalMCT to
tackle a practical new application.

3. Preliminaries

Notation. We denote matrices and vectors in bold, and
scalars, sets, and functions in standard script. Given a matrix
A ∈ Rn×m, we let Ai,j denote its entry at location (i, j).
1(·) represents the indicator function.

Multi-label classiﬁcation Tasks. Throughout this paper,
we focus on multi-label classiﬁcation tasks: assigning a
label set Y ⊆ Y to any data point x ∈ X . In contrast to
basic supervised learning, in multi-label learning each data
point is associated with a set of labels instead of a single
label. Many MLaaS APIs target such tasks. Consider, for
example, image tagging, where X is a set of images and Y
is the set of all tags. Example label sets could be {person,
car} or {bag, train, sky}.

MLaaS Market. Consider a MLaaS market consisting of
K different ML services for some multi-label tasks. For a
data point x, the kth service returns to the user a set of labels
with their quality scores, denoted by Yk(x) ⊆ Y ×[0, 1]. For
example, one API for multi-label image classiﬁcation might
produce Yk(x) = {(person, 0.8), (car, 0.7)}, indicating the
label person with conﬁdence 0.8 and car with conﬁdence
0.7. Let the vector ccc ∈ RK denote the unit cost of all
services. E.g., ck = 0.01 means that users need to pay
$0.01 every time they call the kth service.

4. FrugalMCT Framework

In this section, we present FrugalMCT, a framework to
adaptively select ML APIs for multi-label classiﬁcation
tasks within a budget. All proofs are left to the appendix.
We generalize the scheme in Figure 1 (a) to K ML services.
As shown in Figure 2, FrugalMCT contains three main
components: an accuracy estimator, a service selector, and

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Figure 2. Overview of FrugalMCT. (a) shows how it works: Given a data point, FrugalMCT ﬁrst invokes a base service. An accuracy
predictor then estimates the performance of different APIs. Next, an add-on service is selected based on the predicted accuracy and budget.
Finally, the add-on and base services’ predictions are combined to return FrugalMCT’s prediction. (b) lists notation.

a label combiner.

Given a data point x, it ﬁrst calls some base service, de-
noted by base, which is one of the K APIs, and obtains
Ybase(x). Often, base is a cheap or free service, such as an
inexpensive open source model; we discuss how to choose
base out of multiple options in Section 4.4. Next, an ac-
curacy predictor produces a vector ˆaaa(x) ∈ [0, 1]K, whose
kth value estimates the accuracy of the label set produced
by the label combiner using base’s and kth API’s outputs.
The service selector s(·) : X (cid:55)→ [K] then decides if and
which add-on service needs to be invoked. Finally, a label
combiner generates a label set by combining the predictions
from the base and add-on APIs. Take Figure 1 (d) as an
example. The image is ﬁrst passed to the GitHub model,
which produces {(person, 0.46),(tennis racket,0.18)}, by
which the accuracy predictor predicts the accuracy of the
label set generated by combining each API’s output with
GitHub model’s. The service selector then decides to further
invoke Everypixel, which gives {(person, 0.46), (sports ball,
0.52)}. Finally, the label combiner uses both APIs’ output
for the ﬁnal prediction.

FrugalMCT allows users to customize the accuracy predic-
tor and the label combiner, depending on the applications.
For example, for the image tagging problem, one might use
image features (e.g., brightness and contrast) to build the
accuracy predictor, while word embeddings can be more
useful for named entity recognition. In the following sec-
tions, we explain the key of accuracy predictor, API selector
and the label combiner in more detail.

4.1. Accuracy prediction

The accuracy predictor ˆaaa(·) can be obtained by two steps.
The ﬁrst step is to generate a feature vector for every data
point in the training dataset XT r (cid:44) {xT r
N T r }.
Generally the feature vector can be any embedding of

2 , · · · , xT r

1 , xT r

the data point x and base service prediction Ybase(x).
In this paper we adopt a simple approach:
if the la-
bel set Y is bounded, a |Y| dimensional vector is gen-
erated using one hot encoding on Ybase(x).
For ex-
ample, given Y = {person, car, bike} and Ybase(x) =
{(person, 0.8), (car, 0.7)}, the generated feature vector is
[0.8, 0.7, 0]. For unbounded Y, word embedding is used
to generate a vector for every predicted label, and the sum
of them (weighted by their quality values) becomes the
corresponding feature vector.

The next step is to train the accuracy predictor. For each
n ∈ XT r, as its true label sets and prediction from each
xT r
API are available, we can construct its true accuracy vector
n ) ∈ [0, 1]K, whose kth element is the accuracy of the
aaa(xT r
label produced by the label combiner using base and kth
service predictions. Then we can train some regressor (e.g.,
random forest) to map the feature vector to the accuracy
vector. We use standard multi-label accuracy2 (Zhang &
Zhou, 2014) as a concrete metric. FrugalMCT can as easily
use another metric such as F1-score, precision or subset
accuracy.

4.2. The API selection problem

A core subroutine of FrugalMCT is the API selector s: given
a budget b and the estimated accuracy ˆaaa(x), which service
should be invoked? Let X (cid:44) {x1, x2, · · · , xN } be the entire
unlabeled dataset to be classiﬁed, and S (cid:44) {1, 2, · · · , K}X
be the set of all functions mapping each data point in X to
an API. Let base be the index of the base service. For any
s ∈ S, s(x) = base implies no add-on API is needed, and
s(x) = k (cid:54)= base implies kth API is invoked. Our goal is to
ﬁnd some s ∈ S to maximize the estimated accuracy while
satisfying the budget constraint, formally stated as below.

Deﬁnition 4.1. Let ZZZ ∗

n,k be the optimal solution to the

2 (cid:107)Y ∩Y (cid:48)(cid:107)
(cid:107)Y ∪Y (cid:48)(cid:107) where Y /Y (cid:48) is the true/predicted label set.

Data point th service AccuracypredictorLabelcombinerReturn th service, Serviceselector, SymbolMeaningdata pointbudgetservice indexesth service's labelsquality score functionestimated accuracyreturned labelsbudget (b)(a)pick base serviceEfﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

budget aware API selection problem

max
ZZZ∈RN ×K :

s.t.

1
N

1
N

N
(cid:88)

K
(cid:88)

ZZZ n,kˆaˆaˆak(xn)

n=1

k=1

N
(cid:88)

K
(cid:88)

n=1

k=1,k(cid:54)=base

ZZZ n,kccck + cccbase ≤ b;

(4.1)

K
(cid:88)

k=1

ZZZ n,k = 1, ∀n; ZZZ n,k ∈ {0, 1}, ∀n, k

Then the optimal FrugalMCT strategy is given by s∗(xn) (cid:44)
arg maxk ZZZ ∗

n,k.

Here, the objective quantiﬁes the average accuracy, the ﬁrst
constraint models the budget requirement, and the last two
constraints enforces only one add-on API is picked for each
data point. Base service is needed for every data point
and thus its cost cccbase appears for every n in the budget
constraint. Note that Problem 4.1 is a MCKP (and thus
integer linear program) and NP-hard in general.

4.3. An online algorithm for FrugalMCT

In many time-sensitive applications, the input data xn (as
well as the accuracy vector ˆaaa(xn)) comes sequentially, and
the API needs to be selected before observing the future
data. The selection process also needs to be fast.

To tackle this challenge, we present an efﬁcient online al-
gorithm, which requires O(K) computations per round and
gives a provably near-optimal solution. The key idea is to
explicitly balance between accuracy and cost at every itera-
tion. Speciﬁcally, for a given data point xn and p ∈ R, let us
deﬁne a strategy sp(xn) (cid:44) arg maxk ˆaaak(xn) − pccck1k(cid:54)=base
and break ties by picking k with smallest cost. Here, p is
a parameter to balance between accuracy ˆaaa(xn) and cost
ccc. When p = 0, sp(xn) selects the API with highest esti-
mated accuracy. When p is large enough sp(xn) enforces
to pick the base API. In fact, larger value of p implies more
weights on cost and smaller p favors more the accuracy. Let
r(s) (cid:44) 1
n=1 ˆaaas(xn)(xn) denote the average accuracy
N
achieved by a strategy s. We can show, interestingly, an
appropriate choice of p leads to small average accuracy loss.
Theorem 4.2. Assume the probability density of ˆaaa(x) is a
continuous function on [0, 1]K. Then with probability 1,
there exists p∗ such that sp∗
satisﬁes budget constraint, and
r(sp∗
) ≥ r(s∗) − 1
N .

(cid:80)N

In words, sp∗
(xn) gives a solution to the API selection
problem with accuracy loss at most 1
N . In practice, ˆaaa(x) is
continuous for standard ML models of accuracy predictors
(e.g., logistic regressors) and thus the assumption holds.
In addition, it is computationally efﬁcient: at iteration n,
it only requires computing ˆaaak(xn) − pccck1k(cid:54)=base for k =
1, 2, · · · , K, which takes only O(K) computations.

The remaining question is how to obtain p∗. As we cannot
see the future data to compute p∗, a natural idea is to es-
timate it using the training dataset. More precisely, given
the training dataset {xT r
2 , · · · , xT r
N T r }, let ˆp, ˆqqq be the
optimal solution to the following problem

1 , xT r

min
p,qqq

(1 − δ)(b − cccbase)p +

N Tr
(cid:88)

n=1

qqqn,

s.t.

ccck · 1k(cid:54)=base · p
N Tr
p ≥ 0, qqq ∈ RN T r

+ qqqn ≥

, qqq ≥ 0

ˆaaak(xTr
n )
N Tr

, ∀n, k

(4.2)

where δ ∈ (0, 1) is a small buffer to ensure that we don’t
exceed the budget (in practice we set δ ≤ 0.01). Technically,
Problem 4.2 is the dual problem to the linear programming
by relaxing the integer constraint in Problem 4.1 on the train-
ing dataset with budget (1 − δ)b, and ˆp corresponds to the
near-optimal strategy for the training dataset. If the training
and testing datasets are from the same distribution, then a
small δ can ensure with high probability, ˆp is slightly less
than p∗ and thus s ˆp satisﬁes the budget constraint. Given ˆp,
one can use s ˆp to select the APIs in an online fashion. The
details are given in Algorithm 1.

Algorithm 1 FrugalMCT Online API Selection Algorithm.
Input
Output :FrugalMCT online API selector so(·)
1: Compute ˆp by solving Problem 4.2 and set br = N (b −

N T r }, {x1, x2, · · · , xN }

2 , · · · , xT r

:ccc, b, {xT r

1 , xT r

cccbase).

2: At iteration n = 1, 2, · · · , N :
(cid:40)

so(xn) =

s ˆp(xn)
base
br = br − cccs ˆp(xn)1s ˆp(xn)(cid:54)=base

if br − cccs ˆp(xn) ≥ 0
o/w

3:

4:

Here, br is used to ensure the generated solution is always
feasible. The following theorem gives the performance
guarantee of the online solution.

Theorem 4.3. If δ = Θ

(cid:18)(cid:113) log N/(cid:15)

N +

(cid:113) log N T r/(cid:15)
N T r

(cid:19)

and

the probability density of ˆaaa(x) is a continuous func-
tion on [0, 1]K, then so satisﬁes the budget constraint,
and with probability at least 1 − (cid:15), r(so) ≥ r(s∗) −
(cid:19)

(cid:18)(cid:113) log N /(cid:15)

N +

O

(cid:113) log N T r/(cid:15)
N T r

.

Roughly speaking, so leads to an accuracy loss at most

(cid:18)(cid:113) log N

N +

(cid:19)

(cid:113) log N T r
N T r

O

compared to the optimal ofﬂine

strategy. For large training and testing datasets, such an
accuracy loss is often negligible, which is also veriﬁed by
our experiments on real world datasets.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

4.4. Base service selection and label combination

Now we describe how the base service is selected and how
the label combiner works. The base service can be picked
by an ofﬂine searching process. More precisely, for each
possible base service, we train a FrugalMCT strategy and
evaluate its performance on a validation dataset, and pick
the base service corresponding to the highest performance.

The label combiner contains two phases. First, a new label
set associated with its quality function is produced. The
label set is simply the union of that from the base ser-
vice and add-on service. The quality score is a weighted
sum of the score from both APIs, controlled by a hy-
perparameter w. For example, suppose the base pre-
dicts {(person, 0.8), (car, 0.7)} and the add-on predicts
{(car, 0.5), (bike, 0.4)}. Given w = 0.3, new conﬁdence
for person is 0.3 × 0.8 = 0.24, for car is 0.3 × 0.7 + 0.7 ×
0.5 = 0.46, and for bike is 0.7 × 0.4 = 0.28. Thus the
combined set is {(person, 0.24), (car, 0.46), (bike, 0.28)}.
Next, a threshold θ is applied to remove labels with low
conﬁdence. For example, given θ = 0.25, the label person
would be removed, and the ﬁnal predicted label set becomes
{car, bike}. The parameters w and θ are global hyperpa-
rameters for each dataset, and can be obtained by an efﬁcient
searching algorithm to maximize the overall performance.
The details are left to Appendix A.

5. Experiments

We compare the accuracy and incurred costs of FrugalMCT
to that of real world ML services for various tasks. Our goal
is to (i) understand when and why FrugalMCT can reduce
cost without hurting accuracy, (ii) investigate the trade-offs
between accuracy and cost achieved by FrugalMCT, and
(iii) assess the effect of training data size and accuracy
predictors on FrugalMCT’s performance.

Tasks, ML Services, and Datasets. We focus on three
common ML tasks in different domains: multi-label im-
age classiﬁcation (MIC), scene text recognition (STR), and
named entity recognition (NER). MIC aims at obtaining all
keywords associated with an image, STR seeks to recognize
all texts in an image, and NER desires to extract all entities
in a text paragraph. The ML services used for each task and
their prices are summarized in Table 1. For each task we
use three datasets, summarized in Table 2. More details can
be found in Appendix C.

Accuracy Predictors. Except when explicitly noted, we
use a random forest regressor as the accuracy predictor for
all the datasets. For MIC and STR datasets, we map each
possible label to an index, and create a feature vector whose
kth element is base service’s quality score for the label corre-
sponding to k. If a label is not predicted, the corresponding

Table 1. ML services used for each task. Price unit: USD/10,000
queries. A publicly available (and thus free) GitHub model is
also used per task: a single shot detector (SSD) (SSD) pretrained
on Open Images V4 (Kuznetsova et al., 2020) for MIC, a con-
volutional recurrent neural network (PP-OCR) (Pad) pretrained
on an industrial dataset (Du et al., 2020) for STR, and a convo-
lutional neural network (spaCy (Spa)) pretrained on OntoNotes
(Weischedel et al., 2017) for NER.

Task

ML Service

Price

ML Service

Price

MIC

STR

NER

SSD (SSD)

<0.01

Everypixel (Eve)

Microsoft (Mic)

10

Google (Goo)

PP-OCR (Pad)

<0.01

Google (Goo)

iFLYTEK (Iﬂ)

50

Tencent (Ten)

spaCy (Spa)

<0.01

Amazon (Ama)

Google (GoN)

10

IBM (IBM)

6

15

15

210

3

30

Table 2. Dataset Statistics.

Task

Dataset

Size

# Labels Dist Labels

MIC

STR

NER

PASCAL

MIR

COCO

MTWI

ReCTS

LSVT

CONLL

ZHNER

GMB

11540

25000

16682

92909

123287

357662

9742

20000

30000

10898

16915

47830

867727

555286

1878682

43968

147164

116225

20

24

80

4404

4134

4852

9910

4375

14376

value is 0. For NER datasets, we map each predicted label
to a 96-dimensional vector using a word embedding from
spaCy (Spa), and then use the sum weighted by their corre-
sponding quality scores as the feature vector. The accuracy
predictor is then trained on half of the datasets using the
feature vectors generated as above. Interestingly, we found
we are able to accurately predict which commercial API is
best for each instance using relatively simple features. This
makes the approach more broadly applicable. We will study
the effects of accuracy predictors later in this section.

Multi-label Image Classiﬁcation: A Case Study. Let
us start with multi-label image classiﬁcation on the COCO
dataset (Lin et al., 2014). We set budget b = 6, the price of
Everypixel, the cheapest commercial API (except the open
source model from GitHub). For comparison, we also use
the average quality score over all predicted labels as the
conﬁdence score and adapt FrugalML (Chen et al., 2020)
with the same budget (= 6) as another baseline .

Figure 3 demonstrates the learned FrugalMCT strategy. As
shown in Figure 3 (a), the learned FrugalMCT reduces the

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

accuracy than both the base and add-on APIs.

To understand the efﬁciency of FrugalMCT’s API selec-
tor, we compare it with three ILP solvers, namely, CBC,
MOSEK, and GUROBI. CBC (Forrest & Lougee-Heimer,
2005) is an integer linear programming package developed
based on cutting and branch. MOSEK (Andersen & Ander-
sen, 2000) was originally developed for sparse programming
and then extended for general mixed integer programming.
On the other hand, the focus of GUROBI (Bixby, 2007) is
parallelism optimization in integer programming. As shown
in Figure 3 (e), the API selector of FrugalMCT (Alg. 1) is
several orders of magnitude faster than those commercial
ILP solvers. This is beause it leverages the speciﬁc structure
of Problem 4.1.

Table 3. End-to-end runtime comparison on COCO.

Runtime

FrugalMCT

FrugalML Majority Vote

Training

Inference

60s

1.25s

6627s

1.24s

N/A

1.92s

The end-to-end runtime comparison of FrugalMCT with
FrugalML and an ensemble approach (majority vote) is
given in Table 3 . Majority vote does not need training,
but its inference time is high due to calling all ML APIs.
FrugalMCT enjoys a similar inference time with FrugalML
but a 100x smaller training time.

Table 4. Cost savings achieved by FrugalMCT that reaches same
accuracy as the best commercial API. On average the cost saving
across the evaluated datasets is 73%.

Task

Dataset

Acc (%)

Best API $ Our $

Save

MIC

STR

NER

PASCAL

MIR

COCO

MTWI

ReCTS

LSVT

CONLL

ZHNER

GMB

74.8

41.2

47.5

67.9

61.3

53.8

52.6

61.3

50.1

10

10

10

210

210

210

3

30

30

1.4

4.2

3

30

78

67

1.5

0.7

4.1

86%

58%

70%

86%

63%

68%

50%

98%

80%

Analysis of Cost Savings. Next, we evaluate how much
cost can be saved by FrugalMCT to reach the highest ac-
curacy produced by a single API on different tasks. As
shown in Table 4, FrugalMCT can typically save more than
60% of the cost. Interestingly, the cost saving can be up to
98% on the dataset ZHNER. This is probably because (i)
the accuracy estimator enables the API selector to identify
when the base service’s prediction is reliable and to avoid
unnecessarily calling add-on services, and (ii) when add-on

Figure 3. A FrugalMCT strategy learned on the dataset COCO.
(a) shows that FrugalMCT reduces cost by mostly calling the
Everypixel API (45.4%) or the GitHub API (22.1%) only. (b)
and (c) show how the accuracy and cost vary with weight p. The
blue point corresponds to 0.006, the learned ˆp. (d) shows the
accuracy and cost of FrugalMCT, FrugalML, Microsoft API, and
majority vote. (e) gives the runtime performance of our (online)
API selector and three commercial ILP solvers.

cost by mostly using the Everypixel API (45%, 6$) and
occasionally calling Microsoft API (32%, 10$), and rarely
invoking the Google API (0.4%, 15$). Note that its per-
formance depends on the threshold value ˆp. As shown in
Figure 3 (b) and (c), for small thresholds, FrugalMCT tends
to call the more accurate and expensive APIs. However, it
runs out of budget quickly, and for many data points only
base service can be used, leading to low accuracy. For large
thresholds, FrugalMCT tends to call cheaper but less accu-
rate APIs, failing to fully use the budget and thus causing
low accuracy too. The ˆp value learned by FrugalMCT (blue
point in Figure 3 (b) and (c)) produces the optimal accuracy
given the budget. Figure 3 (d) shows that FrugalMCT’s
accuracy (0.514) is higher than that of the best ML service
(MS, 0.475) and majority vote (Maj 0.501), while its cost is
much lower. This is primarily because FrugalMCT learns
when the cheaper APIs perform better and call them aptly.
FrugalMCT also outperforms FrugalML by exploiting the
label combination. This is due to (i) that FrugalML cannot
utilize the label information due to explosion of complexity,
and (ii) that the label combiner in FrugalMCT gives higher

 Input  Images weight=0.2weight=0.6weight=0.8 32.3%45.4%0.4%AccuracyPredictorLabelCombinerServiceSelector(a)(b)(c)(e)(d)Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

(a) PASCAL

(b) MIR

(c) COCO

(d) MTWI

(e) ReCTS

(f) LSVT

(g) CONLL

(h) ZHNER

(i) GMB

Figure 4. Accuracy cost trade-offs. The ofﬂine FrugalMCT (black) observes the full data and then make decisions. The online FrugalMCT
(red) matches the ofﬂine performance in all the experiments. DAP (grey) is an oblation of FrugalMCT where a dummy accuracy predictor
is used. FrugalML (orange) is the previous state-of-the-art method. The task of row 1, 2, 3 is MIC, STR, and NER.

API is invoked, the apt combination of the base and add-on
services leads to a high accuracy improvement.

Accuracy and Cost Trade-offs. Now we dive deeply into
the accuracy and cost trade-offs achieved by FrugalMCT,
shown in Figure 4. We compare with two ablations: “Of-
ﬂine”, where the full data is observed before making deci-
sion, “DAP”, where a dummy accuracy predictor is used,
which, for each API, always returns its mean accuracy on
the training dataset. We also compared with an adapted
version of the previous state-of-the-art for single label task,
FrugalML. To adapt it to multi-label tasks, we use the aver-
age quality score over all predicted labels as a single score,
and cluster all labels into a “superclass”.

Compared to any single API, FrugalMCT allows users to
pick any point in its trade-off curve and offers substantial
more ﬂexibility. In addition, FrugalMCT often achieves
higher accuracy than any ML services it calls. For example,
on COCO and ZHNER, more than 5% accuracy improve-
ment can be reached with the same cost of the best API.

Note that FrugalMCT also outperforms FrugalML with the
same budget. This is primarily because FrugalMCT (i) uti-
lizes a more principled way to use the features (learning an
accuracy estimator) than FrugalML (directly using the label
info), and (ii) adopts a label combiner designed for multi-
label tasks. Ensemble methods such as majority votes (in
the appendix C) produce accuracy similar to FrugalMCT,
but their cost is much higher. Note that there is little perfor-
mance difference between the online FrugalMCT strategy
and the ofﬂine approach, due to the carefully designed on-
line algorithm. This directly supports our theory.

Table 5. Performance of FrugalMCT’s accuracy predictor. Root
mean square error (RMSE) quantiﬁes the standard deviation of the
differences between the predicted and the true accuracy.
Data

RMSE

RMSE

RMSE

Data

Data

PASCAL

MTWI

CONLL

0.28

0.17

0.29

MIR

ReCTS

ZHNER

0.22

0.22

0.31

COCO

LSVT

GMB

0.24

0.19

0.28

01020Budget0.500.550.600.650.700.750.80AccuracySSDGoogleEverypixelMicrosoftFrugalMCTFrugalMCT (OFL)FrugalMCT (DAP)FrugalML01020Budget0.150.200.250.300.350.400.450.50AccuracySSDGoogleEverypixelMicrosoft01020Budget0.250.300.350.400.450.500.55AccuracySSDGoogleEverypixelMicrosoft0100200Budget0.600.650.700.75AccuracyPP-OCRGoogleiFLYTEKTencent0100200Budget0.500.550.600.65AccuracyPP-OCRGoogleiFLYTEKTencent0100200Budget0.450.500.550.60AccuracyPP-OCRGoogleiFLYTEKTencent0102030Budget0.400.450.500.550.60AccuracySpacyGoogleAmazonIBM0102030Budget0.500.550.600.650.700.75AccuracySpacyGoogleAmazonIBM0102030Budget0.400.450.500.55AccuracySpacyGoogleAmazonIBMEfﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

(a) PASCAL

(b) MTWI

(c) CONLL

Figure 5. Testing accuracy v.s.training data size. The ﬁxed budget is 6, 15, 3, respectively.

6. Conclusion

In this paper, we presented FrugalMCT, an algorithmic
framework to adaptively select and combine ML APIs for
multi-label classiﬁcation tasks within a budget constraint.
FrugalMCT integrates forecasts of API’s accuracy with
online constrained optimization to create an end-to-end al-
gorithm with strong empirical performance and theoretical
guarantees. How to efﬁciently use multi-label APIs is an
important problem in practice for the large number of ML
users who have chosen to rely on commercial prediction
APIs, and has not been studied heavily in the ML literature.
This work helps MLaaS users improve the overall accuracy
and cost of their applications. Extensive empirical evalu-
ation using real commercial APIs shows that FrugalMCT
signiﬁcantly improves both cost and accuracy.

To encourage more research on MLaaS, we also release the
dataset used to develop FrugalMCT, consisting of 295,212
samples annotated by commercial multi-label prediction
APIs. The dataset and our code can be accessed from
https://github.com/lchen001/FrugalMCT.

Acknowledgement

This work was supported in part by a Google PhD Fel-
lowship, a Sloan Fellowship, NSF CCF 1763191, NSF CA-
REER AWARD 1651570 and 1942926, NIH P30AG059307,
NIH U01MH098953, grants from the Chan-Zuckerberg Ini-
tiative, Sutherland, and afﬁliate members and other support-
ers of the Stanford DAWN project, including Meta, Google,
and VMware. We also thank anonymous reviewers for help-
ful discussion and feedback.

Effects of Accuracy Predictors. The accuracy predictors
play an important role in FrugalMCT’s performance. As
Table 5 shows, FrugalMCT provides nontrivial accuracy
estimates which enables its success. It’s interesting to note
that the accuracy predictor doesn’t need to be perfect for
FrugalMCT to do well; for example, the root mean square
error (RMSE) of the accuracy predictor is 0.28 on PASCAL
(and 0.29 on CONLL), but FrugalMCT still produces con-
sistently better accuracy than FrugalML. We also evaluated
FrugalMCT’s performance when the accuracy predictors
are obtained via two AutoML toolkits, auto-sklearn (Feurer
et al., 2015) and Auto-PyTorch (Mendoza et al., 2019) in-
stead of random forest, and observe a similar performance.

Effects of Training Sample Size. Finally we study how
the training dataset size affects FrugalMCT’s performance.
As shown in Figure 5, across different tasks, a few thousand
training samples are typically sufﬁcient to learn the optimal
FrugalMCT strategy. This is usually more efﬁcient than
training a customized ML model from scratch. It also only
takes a few minutes to train those FrugalMCT strategies,
which is much faster than training a model from scratch.
This is useful in latency-critical applications.

Training cost of FrugalMCT. Both dollar cost and com-
putation time of training are often much smaller than ML
APIs’ inference cost.This is because (i) training is a one-
time cost and (ii) FrugalMCT requires a small number of
label annotations (a few thousands see Figure 5). Consider
the image tagging task as an example: the dollar cost of
calling all APIs is $0.0006 + $0.001 + $0.0015 = $0.0031
per image. Labeling for (say) ﬁve thousands images takes
$0.0031 × 5000 = $15.5. Training a FrugalMCT strategy
on half of the COCO dataset takes 59.5s on the experiment
machine. This is much cheaper than calling the selected
APIs after at large scale (e.g., millions of images).

200040006000Training Data Size0.700.750.80AccuracyFrugalMCTFrugalMCT (OFL)20004000Training Data Size0.600.650.70Accuracy20004000Training Data Size0.500.550.60AccuracyEfﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

References

Amazon Comprehend API. https://aws.amazon.

com/comprehend. [Accessed Oct-2020].

Everypixel

Image Tagging API.

https://labs.

everypixel.com/api. [Accessed Oct-2020].

Google NLP API. https://cloud.google.com/

natural-language. [Accessed Oct-2020].

Google Vision API. https://cloud.google.com/

vision. [Accessed Oct-2020].

Bednarczuk, E. M., Miroforidis, J., and Pyzel, P. A multi-
criteria approach to approximate solution of multiple-
choice knapsack problem. Comput. Optim. Appl., 70(3):
889–910, 2018.

Bixby, B. The gurobi optimizer. Transp. Re-search Part B,

41(2):159–178, 2007.

Bos, J. The groningen meaning bank. In JSSP, pp. 2, 2013.

Buolamwini, J. and Gebru, T. Gender shades: Intersectional
accuracy disparities in commercial gender classiﬁcation.
In FAT, volume 81, pp. 77–91, 2018.

IBM NLP API.

https://www.ibm.com/cloud/
watson-natural-language-understanding.
[Accessed Oct-2020].

Cai, Z., Saberian, M. J., and Vasconcelos, N. Learning
complexity-aware cascades for deep pedestrian detection.
In ICCV, pp. 3361–3369, 2015.

iFLYTEK Text Recognition API. https://global.
xfyun.cn/products/wordRecg. [Accessed Oct-
2020].

Microsoft

computer

vision API.

https:

//azure.microsoft.com/en-us/services/
cognitive-services/computer-vision.
[Accessed Oct-2020].

PaddleOCR, a text recgonition tool from GitHub. https:
//github.com/PaddlePaddle/PaddleOCR.
[Accessed Oct-2020].

SSD, a multi-label

image classiﬁcation tool

from
https://tfhub.dev/

GitHub/TensorﬂowHub.
google/openimages_v4/ssd/mobilenet_
v2/1. [Accessed Oct-2020].

spaCy, a named entity recognition tool from GitHub.
https://github.com/explosion/spaCy.
[Accessed Oct-2020].

Tencent Text Recognition API. https://intl.cloud.
[Accessed Oct-

tencent.com/product/ocr.
2020].

ZHNER dataset. https://github.com/zjy-ucas/
ChineseNER/tree/master/data. [Accessed Oct-
2020].

Akbar, M. M., Rahman, M. S., Kaykobad, M., Manning,
E. G., and Shoja, G. C. Solving the multidimensional
multiple-choice knapsack problem by constructing con-
vex hulls. Comput. Oper. Res., 33:1259–1273, 2006.

Andersen, E. D. and Andersen, K. D. The mosek interior
point optimizer for linear programming: an implementa-
tion of the homogeneous algorithm. In High performance
optimization, pp. 197–232. Springer, 2000.

Chen, G., Ye, D., Xing, Z., Chen, J., and Cambria, E. En-
semble application of convolutional and recurrent neural
networks for multi-label text categorization. In IJCNN,
2017.

Chen, L., Koutris, P., and Kumar, A. Towards model-based
pricing for machine learning in a data marketplace. In
Proceedings of the 2019 International Conference on
Management of Data, pp. 1535–1552, 2019.

Chen, L., Zaharia, M., and Zou, J. FrugalML: How to
use ML prediction apis more accurately and cheaply. In
NeurIPS, 2020.

Chen, L., Zaharia, M., and Zou, J. How did the model
change? efﬁciently assessing machine learning api shifts.
In International Conference on Learning Representations,
2021.

Chen, X., Liew, J., Xiong, W., Chui, C., and Ong, S. H.
Focus, segment and erase: An efﬁcient network for multi-
label brain tumor segmentation. In ECCV, pp. 674–689,
2018.

de Sá, A. G. C., Pappa, G. L., and Freitas, A. A. Towards a
method for automatically selecting and conﬁguring multi-
label classiﬁcation algorithms. In GECCO, pp. 1125—-
1132, 2017.

Devanur, N. R. and Hayes, T. P. The adwords problem:
Online keyword matching with budgeted bidders under
random permutations. In EC, pp. 71–78, 2009.

Devanur, N. R. and Hayes, T. P. Near optimal online al-
gorithms and fast approximation algorithms for resource
allocation problems. J. ACM, 66(1):1–41, 2019.

Du, Y., Li, C., Guo, R., Yin, X., Liu, W., Zhou, J., Bai, Y.,
Yu, Z., Yang, Y., Dang, Q., and Wang, H. PP-OCR: A
practical ultra lightweight OCR system. CoRR, 2020.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Everingham, M., Eslami, S. M. A., Gool, L. V., Williams, C.
K. I., Winn, J. M., and Zisserman, A. The pascal visual
object classes challenge: A retrospective. Int. J. Comput.
Vis., 111(1):98–136, 2015.

Feurer, M., Klein, A., Eggensperger, K., Springenberg, J.,
Blum, M., and Hutter, F. Efﬁcient and robust automated
machine learning. In NIPS. 2015.

Forrest, J. and Lougee-Heimer, R. Cbc user guide.

In
Emerging theory, methods, and applications, pp. 257–
277. INFORMS, 2005.

He, M., Liu, Y., Yang, Z., Zhang, S., Luo, C., Gao, F.,
Zheng, Q., Wang, Y., Zhang, X., and Jin, L. Contest on
robust reading for multi-type web images. In ICPR, pp.
7–12, 2018.

Hosseini, H., Xiao, B., and Poovendran, R. Google’s cloud

vision API is not robust to noise. In ICMLA, 2017.

Hosseini, H., Xiao, B., and Poovendran, R. Studying the
live cross-platform circulation of images with computer
vision API: An experiment based on a sports media event.
IJC, 13:1825–1845, 2019.

Huiskes, M. J. and Lew, M. S. The MIR ﬂickr retrieval

evaluation. In MIR, pp. 39–43, 2008.

Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M.,
Mengesha, Z., Toups, C., Rickford, J. R., Jurafsky, D.,
and Goel, S. Racial disparities in automated speech recog-
nition. PNAS, 117(14):7684–7689, 2020.

Kumar, P., Grewal, M., and Srivastava, M. M. Boosted
cascaded convnets for multilabel classiﬁcation of thoracic
diseases in chest radiographs. pp. 546–552, 2018.

Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J. R. R.,
Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci,
M., Kolesnikov, A., Duerig, T., and Ferrari, V. The open
images dataset V4. IJCV, 128(7):1956–1981, 2020.

Lee, C., Lehoczky, J. P., Rajkumar, R., and Siewiorek, D. P.
On quality of service optimization with discrete qos op-
tions. In RTAS, pp. 276, 1999.

Li, X., Sun, C., and Ye, Y. Simple and fast algorithm for

binary integer and online lp. In NeurIPS, 2020.

Mendoza, H., Klein, A., Feurer, M., Springenberg, J. T.,
Urban, M., Burkart, M., Dippel, M., Lindauer, M., and
Hutter, F. Towards automatically-tuned deep neural net-
works. In Automated Machine Learning - Methods, Sys-
tems, Challenges, pp. 135–149. 2019.

Moyano, J. M., Galindo, E. L. G., Cios, K. J., and Ventura, S.
Review of ensembles of multi-label classiﬁers: Models,
experimental study and prospects. Inf. Fusion, 44:33–45,
2018.

Pakrashi, A. and Namee, B. M. CascadeML: An automatic
neural network architecture evolution and training algo-
rithm for multi-label classiﬁcation. In AI, volume 11927,
pp. 3–17, 2019.

Pamela H. Vance, S. M. and Toth), P. Knapsack problems:
Algorithms and computer implementations. SIAM Rev.,
35(4):684–685, 1993.

Read, J., Pfahringer, B., and Holmes, G. Multi-label clas-
siﬁcation using ensembles of pruned sets. In ICDM, pp.
995–1000, 2008.

Read, J., Pfahringer, B., Holmes, G., and Frank, E. Classiﬁer
chains for multi-label classiﬁcation. Mach. Learn., 85(3):
333–359, 2011.

Reis, A., Paulino, D., Filipe, V., and Barroso, J. Using
online artiﬁcial vision services to assist the blind - an
assessment of microsoft cognitive services and google
cloud vision. In WorldCIST, pp. 174–184, 2018.

Sang, E. F. T. K. and Meulder, F. D. Introduction to the
conll-2003 shared task: Language-independent named
entity recognition. In CoNLL, pp. 142–147, 2003.

Sinha, P. and Zoltners, A. A. The multiple-choice knapsack

problem. Oper. Res., 27(3):503–515, 1979.

Sun, Y., Wang, X., and Tang, X. Deep convolutional network
cascade for facial point detection. In CVPR, pp. 3476–
3483, 2013.

Sun, Y., Karatzas, D., Chan, C. S., Jin, L., Ni, Z., Chng,
C. K., Liu, Y., Luo, C., Ng, C. C., Han, J., Ding, E., and
Liu, J. ICDAR 2019 competition on large-scale street
view text with partial labeling. In ICDAR, pp. 1557–1562,
2019.

Lin, T., Maire, M., Belongie, S. J., Hays, J., Perona, P.,
Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft
COCO: common objects in context. In ECCV, volume
8693, pp. 740–755, 2014.

Thornton, C., Hutter, F., Hoos, H. H., and Leyton-Brown, K.
Auto-WEKA: Combined selection and hyperparameter
optimization of classiﬁcation algorithms. In KDD, pp.
847—-855, 2013.

Liu, H., Burnap, P., Alorainy, W., and Williams, M. L. Fuzzy
multi-task learning for hate speech type identiﬁcation. In
WWW, pp. 3006–3012, 2019.

Tsoumakas, G. and Vlahavas, I. P. Random k-labelsets: An
ensemble method for multilabel classiﬁcation. In ECML,
volume 4701, pp. 406–417, 2007.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Viola, P. and Jones, M. Robust real-time object detection.
In International Journal of Computer Vision, 2001a.

Viola, P. A. and Jones, M. J. Fast and robust classiﬁcation
using asymmetric adaboost and a detector cascade. In
NIPS, pp. 1311–1318, 2001b.

Wang, L., Lin, J. J., and Metzler, D. A cascade ranking
model for efﬁcient ranked retrieval. In SIGIR, 2011.

Weischedel, R., Hovy, E., Marcus, M., and Palmer, M.
Ontonotes : A large training corpus for enhanced pro-
cessing. 2017.

Wever, M., Tornede, A., Mohr, F., and Hüllermeier, E. Au-
toml for multi-label classiﬁcation: Overview and empiri-
cal evaluation. IEEE TPAMI, 43(9):3037–3054, 2021.

Xu, X., Jiang, Y., Peng, L., Xue, X., and Zhou, Z. Ensemble
approach based on conditional random ﬁeld for multi-
label image and video annotation. In MM, pp. 1377–1380,
2011.

Xu, Z. E., Kusner, M. J., Weinberger, K. Q., Chen, M., and
Chapelle, O. Classiﬁer cascades and trees for minimizing
feature evaluation cost. J. Mach. Learn. Res., 15(1):2113–
2144, 2014.

Yao, Y., Xiao, Z., Wang, B., Viswanath, B., Zheng, H.,
and Zhao, B. Y. Complexity vs. performance: empirical
analysis of machine learning as a service. In IMC, pp.
384–397, 2017.

Zhang, M. and Zhou, Z. A review on multi-label learning

algorithms. IEEE TKDE, 26(8):1819–1837, 2014.

Zhang, R., Yang, M., Bai, X., Shi, B., Karatzas, D., Lu,
S., Jawahar, C. V., Zhou, Y., Jiang, Q., Song, Q., Li, N.,
Zhou, K., Wang, L., Wang, D., and Liao, M. ICDAR
2019 robust reading challenge on reading chinese text on
signboard. In ICDAR, pp. 1577–1581, 2019.

Zhou, Y. and Naroditskiy, V. Algorithm for stochastic
multiple-choice knapsack problem and application to key-
words bidding. In WWW, pp. 1175–1176, 2008.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Outline. The appendix is organized as follows. We present missing technical details in Section A. The proofs are provided
in Section B. Finally, Section C gives detailed experiment setups and additional empirical results.

A. Technical Details

Additional technical details are presented here.

Label combiner parameter search. Recall that the label combiner requires two parameters: the combining weight
w ∈ [0, 1] and the quality score threshold θ ∈ [0, 1]. We adopt a simple grid search approach to select w and θ. More
precisely, we ﬁrst create a parameter candidate set P CS (cid:44) {w0, w1, w2, · · · , wM }×{θ0, θ1, θ2, · · · , θM }, where wm = m
M
and θi = m
M . Next, for each (w, θ) ∈ P CS, we evaluate the performance of combining the base service and the kth
service using (w, θ), and select the parameter that gives the highest accuracy. Note that this involves M 2 number of label
combinations for each k ∈ [K]. In practice, we have found that M = 10 is sufﬁcient to obtain a good combiner.

δ selection in Algorithm 1. A naive approach is to set a small constant value, say, δ = 0.01. To obtain a more accurate
strategy, we can adopt a search algorithm to select the best δ value based on the evaluation the performance on a validation
dataset. More precisely, we ﬁrst create a constant set CS. Then for each α ∈ CS, let δ = α log N
N , and then solve Problem 4.2
to obtain the parameter ˆp, evaluate the performance on a validation dataset. Finally, we select the α ∈ CS that achieves
the highest accuracy on the validation dataset. In practice, we have found that CS = {−10, −9, −8, · · · , 0, 1, 2, · · · , 10} is
sufﬁcient to obtain a highly accurate solution.

B. Proofs
For ease of notations, let us introduce ˆb (cid:44) b − cccbase and ˆccck (cid:44) ccck · 1k(cid:54)=base ﬁrst. Then we can rewrite the API selection
problem (Problem 4.1) as

max
ZZZ∈RN ×K :

s.t.

1
N

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ n,kˆaˆaˆak(xn)

ZZZ n,kˆccck ≤ ˆb

K
(cid:88)

k=1

ZZZ n,k = 1, ∀n; ZZZ n,k ∈ {0, 1}, ∀n, k

Its corresponding linear programming simply becomes

max
ZZZ∈RN ×K :

s.t.

1
N

1
N

N
(cid:88)

n=1

ZZZ n,kˆaˆaˆak(xn)

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ n,kˆccck ≤ ˆb

K
(cid:88)

k=1

ZZZ n,k = 1, ∀n; ZZZ n,k ∈ [0, 1], ∀n, k

(B.1)

(B.2)

We will analyze some useful properties for those two problems ﬁrst, and then prove the desired results for the original API
selection problem on top of those properties.

B.1. Helpful Lemmas

Before proving the desired results, let us also provide a few generic lemmas.
Lemma B.1. Let AAA ∈ RN1×N2 be a ﬁxed matrix and βββ ∈ RN1 be a random vector. If βββ is supported on [0, 1]N1 with a
continuous density function, then with probability 1,

min
xxx

(cid:107)AAAxxx − βββ(cid:107)0 ≥ N1 − N2.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Proof. If N1 ≤ N2 then the above inequality obviously holds. Suppose N1 > N2. We prove this by contradiction. Assume
the inequality does not hold. Then there exists some xxx(cid:48), such that with probability larger than 0,

(cid:107)AAAxxx(cid:48) − βββ(cid:107)0 < N1 − N2.
That is to say, at least N1 − (N1 − N2) + 1 = N2 + 1 many equations in AAAxxx(cid:48) = βββ can be forced to 0. Let U be the set of
those N2 + 1 indexes. Then formally we have

AAAUxxx(cid:48) = βββU .

That is to say, with probability larger than 0, βββU is in the subspace formed by the columns of AAAU .

On the other hand, we can show that for any set of indexes V with |V | = N2 + 1, βββV lies in the subspace formed by the
columns of AAAV with probability 0, which gives a contradiction. To see this, let us start by considering a ﬁxed set of indexes
V . Let ΩΩΩV denote the subspace formed by AAAV and pβββV (·) be the density function of βββV . The density function of βββ is
continuous and thus pβββV (·) is also continuous. The support of βββ is in [0, 1]N1 , and thus the support of βββV is in [0, 1]N2+1
(since |V | = N2 + 1 by deﬁnition). That is to say, pβββV (·) is a continuous function on a compact set. Therefore, pβββV (·) must
be bounded, i.e., there exists a constant psup such that pβββV (·) ≤ psup. Hence we have
(cid:90)

(cid:90)

(cid:90)

Pr[βββV ∈ ΩΩΩV ] =

pβββV (xxx)dxxx ≤

psupdxxx = psup

1dxxx

xxx∈ΩΩΩV

xxxV ∈ΩΩΩV

xxxV ∈ΩΩΩV

where the ﬁrst equation is by deﬁnition of the random variable βββV , the inequality is by increasing the density function
to its upper bound psup, and the last equation simply moves the constant out of the integral. In addition, ΩΩΩV is a N2 + 1
dimensional space spanned by N2 vectors, which implies that its measure in RN2+1 is 0, i.e, (cid:82)
1dxxx = 0. Thus, we
have just shown that

xxxV ∈ΩΩΩV

Pr[βββV ∈ ΩΩΩV ] ≤ psup

1dxxx = 0

(cid:90)

Probability is non-negative, and thus Pr[βββV ∈ ΩΩΩV ] = 0 (for a ﬁxed V ). Note that the size of V is N2 + 1 and there are in
total N1 possible indexes. Thus, there are (cid:0) N1
(cid:1) many possible choices of V . Applying union bound, we have for any V ,
N2+1
Pr[βββV ∈ ΩΩΩV ] = 0. A contradiction. The assumption is incorrect, and thus we must have

xxxV ∈ΩΩΩV

min
xxx

(cid:107)AAAxxx − βββ(cid:107)0 ≥ N1 − N2.

Lemma B.2. Let f be a function deﬁned on Ωzzz. Assume there exists a set Ωzzz,1 ⊆ Ωzzz, such that for any zzz ∈ Ωzzz, there exists
zzz(cid:48) ∈ Ωzzz,1, such that (cid:107)f (zzz) − f (zzz(cid:48))(cid:107) ≤ ∆. Then we have

where zzz∗ = arg maxzzz∈Ωzzz f (zzz), zzz∗

1 = arg maxzzz∈Ωzzz,1 f (zzz).

Proof. By assumption, there exists a zzz(cid:48) ∈ Ωzzz,1, such that

(cid:107)f (zzz∗) − f (zzz∗

1)(cid:107) ≤ ∆,

which implies

(cid:107)f (zzz∗) − f (zzz(cid:48))(cid:107) ≤ ∆

f (zzz(cid:48)) ≥ f (zzz∗) − ∆

Noting that zzz∗

1 is the optimal solution on Ωzzz,1 and zzz(cid:48) is a feasible solution, we have

f (zzz∗

1) ≥ f (zzz(cid:48))

Combining the above two inequalities, we have

On the other hand, since Ωzzz,1 ⊆ Ωzzz, zzz∗

f (zzz∗

1) ≥ f (zzz∗) − ∆
1 is a feasible solution on Ωzzz, and thus we have
1) ≤ f (zzz∗) ≤ f (zzz∗) + ∆

f (zzz∗

Combing those two inequalities we have

which completes the proof.

(cid:107)f (zzz∗

1) − f (zzz∗)(cid:107) ≤ ∆

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Lemma B.3. Let X1, X2, · · · , XN1 and X (cid:48)
1, X (cid:48)
[xinf , xsup]. Then we have with probability 1 − (cid:15),

2, · · · XN2 be two i.i.d. samples from the same distribution which lies in

(cid:107)

1
N2

N2(cid:88)

n=1

X (cid:48)

n −

1
N1

N1(cid:88)

n=1

Xn(cid:107) ≤ (xsup − xinf )

(cid:34)(cid:114) log 4 − log (cid:15)

2N2

(cid:114) log 4 − log (cid:15)
2N1

+

(cid:35)

.

Proof. We can apply the Hoeffding’s inequality for both sequences separately, and we can obtain with probability 1 − (cid:15),

and with probability 1 − (cid:15)

(cid:107)

1
N1

N1(cid:88)

n=1

(cid:107)

1
N2

N2(cid:88)

n=1

Xn − E[X1](cid:107) ≤ (xsup − xinf )

X (cid:48)

n − E[X1](cid:107) ≤ (xsup − xinf )

Now applying union bound, we have with probability 1 − (cid:15),

and 1 − (cid:15)

(cid:107)

1
N1

N1(cid:88)

n=1

(cid:107)

1
N2

N2(cid:88)

n=1

Xn − E[X1](cid:107) ≤ (xsup − xinf )

X (cid:48)

n − E[X1](cid:107) ≤ (xsup − xinf )

(cid:114) log 2 − log (cid:15)
2N1

(cid:114) log 2 − log (cid:15)
2N2

(cid:114) log 4 − log (cid:15)
2N1

(cid:114) log 4 − log (cid:15)
2N2

Now applying the triangle inequality, we have

(cid:107)

1
N2

N2(cid:88)

n=1

X (cid:48)

n −

1
N1

N1(cid:88)

n=1

Xn(cid:107) ≤ (xsup − xinf )

(cid:34)(cid:114) log 4 − log (cid:15)

2N2

(cid:114) log 4 − log (cid:15)
2N1

+

(cid:35)

which completes the proof.

Lemma B.4. Let f1, f2, g1, g2 be functions deﬁned on Ωzzz, such that maxzzz∈Ωzzz |(f1zzz)−f2(zzz)| ≤ ∆1 and maxzzz∈Ωzzz (cid:107)g2(zzz)−
g1(zzz)(cid:107) ≤ ∆2. Suppose

and

then we must have

zzz∗
1 = arg max
zzz∈Ωzzz

f1(zzz)

s.t.g1(zzz) ≤ 0

zzz∗
2 = arg max
zzz∈Ωzzz

f2(zzz)

s.t.g2(zzz) ≤ ∆2,

f1(zzz∗
g1(zzz∗

2) ≥ f1(z∗
2) ≤ 2∆2.

1) − 2∆1

Proof. Note that maxzzz∈Ωzzz |(f1(zzz) − f2(zzz)| ≤ ∆1 implies f1(zzz) ≥ f2(zzz) − ∆1 for any zzz ∈ Ωzzz. Speciﬁcally,

f1(zzz∗

2) ≥ f2(zzz∗

2) − ∆1

Noting maxzzz∈Ωzzz (cid:107)g2(zzz) − g1(zzz)(cid:107) ≤ ∆2, we have g2(zzz∗
g1(zzz∗
no smaller than the value at zzz∗

1) ≤ 0 by deﬁnition. Since, zzz∗

1. That is to say,

1) − ∆2 ≤ −∆2, where the last inequality is due to
1 is a feasible solution to the second optimization problem, and the optimal value must be

1) ≤ g1(zzz∗

f2(zzz∗

2) ≥ f2(zzz∗
1)

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Hence we have

f1(zzz∗

2) ≥ f2(zzz∗

2) − ∆1 ≥ f2(zzz∗

1) − ∆1

In addition, maxzzz∈Ωz |(f1(zzz) − f2(zzz)| ≤ ∆1 implies f2(zzz) ≥ f1(zzz) − ∆1 for any zzz ∈ Ωzzz. Thus, we have f2(zzz∗
f1(zzz)∗

1 − ∆1 and thus

1) ≥

f1(zzz∗

2) ≥ f2(zzz∗

1) − ∆1 ≥ f1(zzz∗

1) − 2∆1

By maxzzz∈Ωzzz |g1(zzz) − g2(zzz)| ≤ ∆2, we must have g1(zzz∗
of z(cid:48), which completes the proof.

2) ≤ g2(zzz∗

2) + ∆2 ≤ 2∆2, where the last inequality is by deﬁnition

B.2. Proof of Theorem 4.2

Proof. We give a constructive proof via explicitly giving the value of p∗. In fact, let p∗ and qqq∗ be the optimal solution to

ˆbp +

min
p,qqq:

N
(cid:88)

n=1

qqqn

s.t.

ˆccckp + qqqn ≥

1
N
p, qqq ≥ 0

1
N

ˆaaak(xn)

(B.3)

Then our goal is to show that for this constructed p∗, sp∗
N with
probability 1 (Since probabilistic statement is only introduced in Lemma B.1 whose result holds with probability 1, and we
only apply it ﬁnite times, we will omit the probabilistic statement for the rest of the proof for simplicity). To achieve this, let
us construct a N × K matrix

is a feasible solution to Problem 4.1 and r(sp∗

) ≥ r(s∗) − 1

˜ZZZ p∗
n,k

(cid:44) 1

sp∗ (xn)=k

It is not hard to see that sp∗
feasibility of ˜ZZZ p∗

(xn) = arg maxk

˜ZZZ p∗

n,k. By construction of sp∗

, feasibility of sp∗

to Problem 4.1 is equivalent to

to Problem B.1. By construction of s∗ and sp∗

, r(sp∗

) ≥ r(s∗) − 1

N is equivalent to

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

˜ZZZ p∗

n,kˆaˆaˆak(xn) ≥

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗

n,kˆaˆaˆak(xn) −

1
N

.

k=1

n,k ∈ {0, 1} and (cid:80)K

and the above inequality. By construction of ˜ZZZ p∗

˜ZZZ p∗
n,k = 1, ∀n) are obviously satisﬁed. Thus, we only need to show ˜ZZZ p∗

Therefore, our goal becomes showing the feasibility of ˜ZZZ p∗
, the natural
constraints ( ˜ZZZ p∗
satisﬁes
the budget constraint and the above inequality. To show those two results, let us introduce another variable ZZZ ∗,LP , which
represents a sparse optimal solution to the relaxed version of Problem B.1 (i.e., Problem B.2). The proof idea is then
(roughly) to show (i) that ˜ZZZ p∗
is actually close to ZZZ ∗,LP , (ii) that ZZZ ∗,LP satisﬁes the budget constraint and gives an estimated
accuracy as high as that of the optimal solution ZZZ ∗, and (iii) that the difference between ˜ZZZ p∗
and ZZZ ∗,LP does not break the
budget constraints and only decreases the estimated accuracy by 1/N . Combining the three points ﬁnishes the proof. Now
we formalize this idea.
Step 1: We ﬁrst show that ˜ZZZ p∗
Lemma B.5. Let ZZZ ∗,LP be an optimal solution to Problem B.2. Then there exists some constant n(cid:48), such that ˜ZZZ p∗
ZZZ ∗,LP
n,·

and ZZZ ∗,LP are close to each other.

, ∀n (cid:54)= n(cid:48).

n,· =

Proof. Note that Problem B.3 is the dual problem to Problem B.2. We can write the complementary slackness constraints as
follows

ZZZ ∗LP
n,k (

1
N

ˆccckp∗ + qqq∗

n −

1
N

ˆaaak(xn)) = 0, ∀n, k

Now let us construct the matrix

AAA =








1
N ˆccc,
1
N ˆccc,
...,
1
N ˆccc,

111,
000,
...,
,
000,

000,
111,

· · · ,
000,








· · · , 000
· · · , 000
...
. . . ,
· · · , 111

∈ RN K×(N +1)

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

and the vector





ˆaaa(x1)
ˆaaa(x2)
...
ˆaaa(xN )
Then by Lemma B.1, minxxx (cid:107)AAAxxx − βββ(cid:107)0 ≥ N K − N − 1. Speciﬁcally, if xxx = [p∗, qqq∗T ]T , then we should have (cid:107)AAAxxx − βββ(cid:107)0 ≥
N K − N − 1. Note that each row of AAAxxx − βββ corresponds to 1

N ˆaaak(xn), and thus we effectively have

∈ RN K.

n − 1

1
N











βββ =

1
N

ˆccckp∗ + qqq∗

n −

N ˆccckp∗ + qqq∗
1
N

ˆaaak(xn) (cid:54)= 0

for at least N K − N − 1 choices of n, k. In other words, among all possible choices of n, k, at most N + 1 many of them
satisﬁes

Furthermore, note that the constraint (cid:80)K
ZZZ ∗,LP
n,k
(denoted by n(cid:48)), exactly one equation in { 1

N ccck(cid:48)p∗ + qqq∗

(cid:54)= 0 and thus 1

n − 1

ˆccckp∗ + qqq∗

n −

ˆaaak(xn) = 0

1
N

1
N
k=1 zzz∗LP
k

(xn) = 1 ensures that for any n, there must exist at least one k(cid:48) such that
N ˆaaak(cid:48)(xn) = 0. By the pigeonhole principle, we can conclude that for all n except one

Now let us ﬁx any n (cid:54)= n(cid:48). Then there exists some k(cid:48), such that 1
1
N ccckp∗ + qqq∗

N ˆaaak(xn) > 0 (due to the natural constraint in Problem B.3). That is to say, for any k (cid:54)= k(cid:48),

N ˆaaak(cid:48)(xn) = 0, and for any k (cid:54)= k(cid:48),

n − 1

N ccckp∗ + qqq∗

n − 1

N ˆaaak(xn) = 0}k can be satisﬁed.
N ccck(cid:48)p∗ + qqq∗

n − 1

1
N

ccckp∗ + qqq∗

n −

1
N

ˆaaak(xn) > 0 =

1
N

ccck(cid:48)p∗ + qqq∗

n −

1
N

ˆaaak(cid:48)(xn)

Multiplying N and rearranging the terms gives

ˆaaak(cid:48)(xn) − ccck(cid:48)p∗ > ˆaaak(xn) − ccckp∗

That is to say, k(cid:48) is the unique solution to maxk ˆaaak(xn)−ccckp∗. By deﬁnition of ˜ZZZ p∗
n − 1
k(cid:48). Meanwhile, for any k (cid:54)= k(cid:48), by the slackness constraint, since , 1
The natural constraint in Problem B.2 requires (cid:80)K
1.
That is to say, for any n (cid:54)= n(cid:48), we always have ˜ZZZ p∗

n,k = 1. Thus, we have ZZZ ∗,LP

N ccckp∗ + qqq∗

k=1 ZZZ ∗,LP

n,· = ZZZ ∗,LP

n,·

, which completes the proof.

n,k(cid:48) = 1 and ˜ZZZ p∗

, we have ˜ZZZ p∗
N ˆaaak(xn) > 0, we must have ZZZ ∗,LP
n,k(cid:48) = (cid:80)K

n,k = 0, ∀k (cid:54)=
n,k = 0.
n,k =

k(cid:54)=k(cid:48) ZZZ ∗,LP

n,k − (cid:80)

k=1 ZZZ ∗,LP

Step 2: Now we can show 1
N
˜ZZZ p∗

n,· = ZZZ ∗,LP

n,·

, ∀n (cid:54)= n(cid:48), we must have

(cid:80)N

n=1

(cid:80)K

k=1

˜ZZZ p∗

n,kˆaˆaˆak(xn) ≥ 1
N

(cid:80)N

n=1

(cid:80)K

k=1 ZZZ ∗

n,kˆaˆaˆak(xn) − 1

N . To see this, by Lemma B.5,

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗,LP

n,k ˆaˆaˆak(xn) −

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

˜ZZZ p∗

n,kˆaˆaˆak(xn) =

1
N

K
(cid:88)

k=1

As ˆak(x(cid:48)

n) is bounded in [0, 1], we have

ZZZ ∗,LP

n(cid:48),k ˆaˆaˆak(x(cid:48)

n) −

1
N

K
(cid:88)

k=1

˜ZZZ p∗

n(cid:48),kˆaˆaˆak(x(cid:48)
n)

1
N

K
(cid:88)

ZZZ ∗,LP

n(cid:48),k ˆaˆaˆak(x(cid:48)

n) −

K
(cid:88)

˜ZZZ p∗

n(cid:48),kˆaˆaˆak(x(cid:48)

n) ≤

1
N

1
N

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k · 1 −

1
N

K
(cid:88)

k=1

˜ZZZ p∗

n(cid:48),k · 0 =

1
N

K
(cid:88)

k=1

ZZZ ∗,LP
n(cid:48),k

k=1

k=1
By natural constraint in Problem B.2, (cid:80)K

k=1 ZZZ ∗,LP

n(cid:48),k = 1. Thus, we have

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗,LP

n,k ˆaˆaˆak(xn) −

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

˜ZZZ p∗

n,kˆaˆaˆak(xn) ≤

1
N

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k =

1
N

On the other hand, ZZZ ∗,LP is the optimal solution to Problem B.2 and ZZZ ∗ is a feasible solution. Thus we have

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗

n,kˆaˆaˆak(xn) ≤

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗,LP

n,k ˆaˆaˆak(xn)

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Combining the two inequalities leads to

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

˜ZZZ p∗

n,kˆaˆaˆak(xn) ≥

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗

n,kˆaˆaˆak(xn) −

1
N

Step 3: Finally, we are ready to show the budget constraint is satisﬁed. By Lemma B.5, ˜ZZZ p∗

n,· = ZZZ ∗,LP

n,·

, ∀n (cid:54)= n(cid:48), we have

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

˜ZZZ p∗

n,kˆccck −

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗,LP

n,k ˆccck =

1
N

K
(cid:88)

k=1

˜ZZZ p∗

n(cid:48),kˆccck −

1
N

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k ˆccck

Denote sp∗

(xn(cid:48)) by k1. By construction, we have

K
(cid:88)

k=1

˜ZZZ p∗

n(cid:48),kˆccck −

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k ˆccck = ˆccck1 −

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k ˆccck

Let S be the set of any k such that ZZZ ∗,LP

n(cid:48),k (cid:54)= 0. Then we can further write

K
(cid:88)

k=1

˜ZZZ p∗

n(cid:48),kˆccck −

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k ˆccck = ˆccck1 −

ZZZ ∗,LP

n(cid:48),k ˆccck

(cid:88)

k∈S

N and then adding qqq∗
n. By complementary slackness of Problem B.2, ZZZ ∗,LP

Note that k ∈ S implies k ∈ arg max ˆaaak(xn(cid:48)) −ˆcˆcˆckp∗ (Suppose not. Then there exists some k(cid:48), such that ˆaaak(cid:48)(xn(cid:48)) −ˆcˆcˆck(cid:48)p∗ >
ˆaaak(xn(cid:48)) − ˆcˆcˆckp∗. Multiplying both sides by − 1
N ˆaaak(xn(cid:48)) +
1
N ˆcˆcˆckp∗ + qqq∗
n) = 0. k ∈ S implies
ZZZ ∗,LP
n < 0, which contradicts with the
n,k
feasibility constraint in the dual problem.). Recall that k1 is determined by arg maxk ˆaaak(xn(cid:48)) − ˆcˆcˆckp∗ and we break ties by
picking k with smallest cost. Thus, for any k ∈ S, ˆccck ≥ ˆccck1. Therefore,

n,k (− 1
N ˆaaak(cid:48)(xn(cid:48)) + 1

n = 0. Thus, − 1

N ˆaaak(cid:48)(xn(cid:48)) + 1

N ˆaaak(xn(cid:48)) + 1

N ˆaaak(xn(cid:48)) + 1

(cid:54)= 0 and thus − 1

N ˆcˆcˆck(cid:48)p∗ + qqq∗

N ˆcˆcˆck(cid:48)p∗ + qqq∗

N ˆcˆcˆckp∗ + qqq∗

N ˆcˆcˆckp∗ + qqq∗

n gives − 1

n < − 1

K
(cid:88)

k=1

˜ZZZ p∗

n(cid:48),kˆccck −

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k ˆccck ≤ ˆccck1 −

ZZZ ∗,LP

n(cid:48),k ˆccck1 = (1 −

(cid:88)

k∈S

ZZZ ∗,LP

n(cid:48),k )ˆccck1

(cid:88)

k∈S

By feasibility constraint in Problem B.2, (cid:80)
(cid:80)K
n(cid:48),k ˆccck ≤ 0. Thus,

n(cid:48),kˆccck − (cid:80)K

k=1 ZZZ ∗,LP

˜ZZZ p∗

k=1

k∈S ZZZ ∗,LP

n(cid:48),k = (cid:80)K

k=1 ZZZ ∗,LP

n(cid:48),k = 1. Thus, the above inequality becomes

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

˜ZZZ p∗

n,kˆccck −

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗,LP

n,k ˆccck =

1
N

K
(cid:88)

k=1

˜ZZZ p∗

n(cid:48),kˆccck −

1
N

K
(cid:88)

k=1

ZZZ ∗,LP

n(cid:48),k ˆccck ≤ 0

(cid:80)K

k=1 ZZZ ∗,LP

n,k ˆccck ≤ b.

ZZZ ∗,LP is a feasible solution to Problem B.2, so it must satisfy the budget constraint and thus 1
N
Hence, we must have

(cid:80)N

n=1

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

˜ZZZ p∗

n,kˆccck ≤

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

ZZZ ∗,LP

n,k ˆccck ≤ b ≤ b

i.e., ˜ZZZ p∗

satisﬁes the budget constraint in Problem B.1.

Finally, combining step 2 and step 3 ﬁnishes the proof.

B.3. Proof of Theorem 4.3

Proof. Let us ﬁrst establish a few lemmas consisting of the main components of the proof.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Lemma B.6. Suppose δ ≥ (cid:107)ccc(cid:107)∞

b

solution to Problem 4.1.

(cid:20)(cid:113) log 4−log (cid:15)

N

(cid:113) log 4−log (cid:15)
N T r

+

(cid:21)
. Then with probability at least 1 − (cid:15), s ˆp is a feasible

Proof. We ﬁrst note that Problem 4.2 is a linear programming, and its dual problem is

max
ZZZ∈RN ×K :

1
N T r

s.t.

1
N T r

N T r
(cid:88)

n=1
N T r
(cid:88)

ZZZ n,kˆaˆaˆak(xT r
n )

K
(cid:88)

ZZZ n,kˆccck ≤ (1 − δ)ˆb

n=1

k=1

K
(cid:88)

k=1

ZZZ n,k = 1, ZZZ n,k ∈ [0, 1], ∀n, k

(B.4)

Note that this is in the same form of Problem B.2 except that the data become {xT r
similar argument in the proof for Theorem 4.2, s ˆp(xT r

n ) is a feasible solution to

n }N T r

n=1 instead of {xn}N

n=1. Using a

and thus we have

max

1
N T r

s.t.

1
N T r

N T r
(cid:88)

n=1
N T r
(cid:88)

n=1

rsp

(xT r
n )

η[sp](xT r

n , ccc) ≤ (1 − δ)b,

1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc) ≤ (1 − δ)b

Note that training data xT r
inequality, with probability 1 − (cid:15), we have

n are i.i.d samples from the true distribution and 0 ≤ η[s](xT r

n , ccc) ≤ (cid:107)ccc(cid:107)∞. Thus, by Hoeffding’s

(cid:107)

1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc) − E

(cid:104)

(cid:105)
η[sp](x, ccc)

(cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:114)

log 2 − log (cid:15)
2N T r

The data stream xn is also from the same distribution, and thus we also have with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

η[sp](xn, ccc) − E

(cid:104)

η[sp](x, ccc)

(cid:105)

(cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:114)

log 2 − log (cid:15)
2N

Applying union bound, we have with probability 1 − (cid:15),

and

(cid:107)

1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc) − E

(cid:104)

(cid:105)
η[sp](x, ccc)

(cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:114)

log 4 − log (cid:15)
2N T r

(cid:107)

1
N

N
(cid:88)

n=1

η[sp](xn, ccc) − E

(cid:104)

η[sp](x, ccc)

(cid:105)

(cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:114)

log 4 − log (cid:15)
2N

Using triangle inequality, we have with probability 1 − (cid:15),

(cid:107)

1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc) −

1
N

N
(cid:88)

n=1

η[sp](xn, ccc)(cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:114)

log 4 − log (cid:15)
2N

+ (cid:107)ccc(cid:107)∞

(cid:114)

log 4 − log (cid:15)
2N T r

.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Thus we have

1
N

N
(cid:88)

n=1

η[sp](xn, ccc) ≤

1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc) + (cid:107)ccc(cid:107)∞

(cid:114)

log 4 − log (cid:15)
2N

+ (cid:107)ccc(cid:107)∞

(cid:114)

log 4 − log (cid:15)
2N T r

≤(1 − δ)b + (cid:107)ccc(cid:107)∞

(cid:114)

log 4 − log (cid:15)
2N

(cid:114)

+ (cid:107)ccc(cid:107)∞

log 4 − log (cid:15)
2N T r

≤ b

where the last inequality is due to the assumption on δ. That is to say, with probability 1 − (cid:15), s ˆp is a feasible solution to
Problem 4.1, which completes the proof.

Lemma B.7. Construct the set ΩM (cid:44) {0,

1
(M −1) minccck (cid:54)=0 ccck

,

2
(M −1) minccck (cid:54)=0 ccck

, · · · ,

1
minccck (cid:54)=0 ccck

} and

ˆp(ΩM ) (cid:44) arg max
p∈ΩM

s.t.

N T r
(cid:88)

rsp

(xT r
n )

1
N T r
1
N T r η[sp](xn, ccc) ≤ (1 − δ)b.

n=1

Then with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

rs ˆp

(xn) −

1
N

N
(cid:88)

n=1

rs ˆp(ΩM )

(xn)(cid:107) ≤ O(

(cid:114)

log N + log 8 − log (cid:15)
2N

+

(cid:114)

log N T r + log 8 − log (cid:15)
2N T r

).

Proof. Note that ΩM ⊆ R. Consider an element p ∈ R.

(i) p ≥

1
minccck (cid:54)=0 ccck

: This effectively means the API with the smallest cost is always selected. In other words, we always have

To see this, simply note that for any other k1, we have

bs = arg max ˆaaak(x) − pˆccck

ˆaaabs(x) − pˆcccbs − (ˆaaak1(x) − pˆccck1 ) =ˆaaabs(x) − ˆaaak1(x) + p(ˆccck1 − ˆcccbs) = ˆaaabs(x) − ˆaaak1 (x) + pˆccck1

≥0 − 1 + pˆccck1 ≥ −1 + ˆccck1 ·

1
minccck(cid:54)=0 ccck

≥ 0

Thus, for such p, the objective value is the same as that for

1
minccck (cid:54)=0 ccck

∈ ΩM .

(ii): 0 ≤ p ≤
Let pj (cid:44)

1
minccck (cid:54)=0 ccck
j
(M −1) minccck (cid:54)=0 ccck

: By construction of ΩM , there exists some m, such that

m
(M −1) minccck (cid:54)=0 ccck

≤ p ≤

m+1
(M −1) minccck (cid:54)=0 ccck

.

for ease of notations. Clearly, we have pm ∈ ΩM .

Now let us partition the space of ˆaaa(x) into M regions, denoted by A1, A2, · · · , AM . Abusing the notation a little bit, let
φ(p, x) (cid:44) arg max ˆaaak(x) − pˆccck A1 is the set of all ˆaaa(x) such that φ(p, x) is a constant. A2 is the set of all ˆaaa(x) such that
φ(p, x) is a constant for p larger than p1. Generally, Aj is the set of all ˆaaa(x) such that φ(p, x) is a constant for p larger than
pj−1 subtracting Aj−1. Formally,

Aj =

(cid:40)

{ˆaaa(x) : φ(p, x)is a constant},
{ˆaaa(x) : φ(p, x)is a constant if p ≥ pj−1} − Aj,

j = 1
j > 1

One can easily verify that {Aj} form a partition of the space of the estimated accuracy, and further more, (cid:107)Aj(cid:107) ≤
. By the assumption of the distribution, there exists some constant u, such that P r(A) ≤ u(cid:107)A(cid:107), for any A in

(cid:107)ccc(cid:107)1
M minccck (cid:54)=0 ccck
the probability space. Thus, we must have

Pr[ˆaaa(x) ∈ Aj] ≤ (cid:107)Aj(cid:107)u =

u(cid:107)ccc(cid:107)1
M minccck(cid:54)=0 ccc2
k

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Now note that, when pm =
More precisely, we have

m
(M −1) minccck (cid:54)=0 ccck

≤ p ≤

m+1
(M −1) minccck (cid:54)=0 ccck

= pm+1, only elements in Am may affect the reward.

1
N

N
(cid:88)

n=1

rsp

(xn) −

1
N

N
(cid:88)

n=1

rspm (xn) =

1
N

(cid:88)

rsp

(xn) −

xn∈Am

1
N

(cid:88)

rspm (xn)

xn∈Am

Note that each estimated accuracy is an i.i.d sample from the true distribution, and its value is from [0, 1], by Hoeffding’s
inequality, with probability 1 − (cid:15), we have

(cid:107)

1
N

N
(cid:88)

n=1

1xn∈Aj − Pr[xn ∈ Aj](cid:107) ≤

(cid:114)

log 2 − log (cid:15)
2N

Applying the union bound, we have for any j, with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

1xn∈Aj − Pr[xn ∈ Aj](cid:107) ≤

(cid:114)

log M + log 2 − log (cid:15)
2N

Therefore, we have with probability 1 − (cid:15),

1
N

N
(cid:88)

n=1

rsp

(xn) −

1
N

N
(cid:88)

n=1

rspm (xn) =

1
N

(cid:88)

rsp

(xn) −

xn∈Am

1
N

(cid:88)

rspm (xn)

xn∈Am

1
N

N
(cid:88)

n=1

1xn∈Am

log M + log 2 − log (cid:15)
2N

(cid:88)

≥

0 −

xn∈Am

1
N

(cid:88)

1 =

xn∈Am
(cid:114)

≥ Pr[xn ∈ Am] −

(cid:114)

≥ −

log M + log 2 − log (cid:15)
2N

and similarly

That is to say,

1
N

N
(cid:88)

n=1

rsp

(xn) −

1
N

N
(cid:88)

n=1

rspm (xn) =

1
N

(cid:88)

rsp

(xn) −

xn∈Am

1
N

(cid:88)

rspm (xn)

xn∈Am

(cid:88)

≤

1 −

xn∈Am

1
N

(cid:88)

0 =

xn∈Am
(cid:114)

1
N

N
(cid:88)

n=1

1xn∈Am

≤ Pr[xn ∈ Am] +

≤

u(cid:107)ccc(cid:107)1
M minccck(cid:54)=0 ccck

+

(cid:114)

log M + log 2 − log (cid:15)
2N

log M + log 2 − log (cid:15)
2N

(cid:107)

1
N

N
(cid:88)

n=1

rsp

(xn) −

1
N

N
(cid:88)

n=1

rspm (xn)(cid:107) ≤

(cid:114)

u(cid:107)ccc(cid:107)1
M minccck(cid:54)=0 ccck

+

log M + log 2 − log (cid:15)
2N

(B.5)

Similarly, for the training dataset, we can also get, with probability 1 − (cid:15),

(cid:107)

1
N T r

N T r
(cid:88)

n=1

rsp

(xT r

n ) −

1
N T r

N
(cid:88)

n=1

rspm (xT r

n )(cid:107) ≤

(cid:114)

u(cid:107)ccc(cid:107)1
M minccck(cid:54)=0 ccck

+

log M + log 2 − log (cid:15)
2N

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Combining case (i) and case (ii), we have just shown that for any p ∈ R, there exists another p(cid:48) ∈ ΩM , such that

(cid:107)

1
N T r

N T r
(cid:88)

n=1

rsp

(xT r

n ) −

1
N T r

N
(cid:88)

n=1

rsp(cid:48)

(xT r

n )(cid:107) ≤

u(cid:107)ccc(cid:107)1
M minccck(cid:54)=0 ccck

+

(cid:114)

log M + log 2 − log (cid:15)
2N

(B.6)

Thus, applying Lemma B.2, we have with probability 1 − (cid:15),

(cid:107)

1
N T r

N T r
(cid:88)

n=1

rs ˆp

(xT r

n ) −

1
N T r

N
(cid:88)

n=1

rs ˆp(ΩM )

(xT r

n )(cid:107) ≤

(cid:114)

u(cid:107)ccc(cid:107)1
M minccck(cid:54)=0 ccck

+

log M + log 2 − log (cid:15)
2N

Now by Lemma B.3, for each ﬁxed j, we have with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

rspj (xn) −

1
N T r

N T r
(cid:88)

n=1

rspj (xT r

n )(cid:107) ≤

(cid:34)(cid:114)

log 4 − log (cid:15)
2N

+

(cid:114)

(cid:35)

log 4 − log (cid:15)
2N T r

Applying union bound, with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

rspj (xn) −

1
N T r

N T r
(cid:88)

n=1

rspj (xT r

n )(cid:107) ≤

(cid:34)(cid:114)

log M + log 4 − log (cid:15)
2N

+

(cid:114)

log M + log 4 − log (cid:15)
2N T r

(cid:35)

for all j. Speciﬁcally, we have

(cid:107)

1
N

N
(cid:88)

n=1

rs ˆp(ΩM )

(xn) −

1
N T r

N T r
(cid:88)

n=1

rs ˆp(ΩM )

(xT r

n )(cid:107) ≤

(cid:34)(cid:114)

log M + log 4 − log (cid:15)
2N

+

(cid:114)

log M + log 4 − log (cid:15)
2N T r

(cid:35)

(B.7)

and

(cid:107)

1
N

N
(cid:88)

n=1

rsp(cid:48)

(xn) −

1
N T r

N T r
(cid:88)

n=1

rsp(cid:48)

(xT r

n )(cid:107) ≤

(cid:34)(cid:114)

log M + log 4 − log (cid:15)
2N

+

(cid:114)

log M + log 4 − log (cid:15)
2N T r

(cid:35)

(B.8)

Now combining equations B.5, B.6, B.7, and B.8 with triangle inequality, we have with probability 1 − (cid:15),

N
(cid:88)

rs ˆp

(xn) −

(cid:107)

1
N

n=1
u(cid:107)ccc(cid:107)1
4M minccck(cid:54)=0 ccck

≤

+ 4

1
N
(cid:114)

N
(cid:88)

n=1

rs ˆp(ΩM )

(xn)(cid:107)

log M + log 8 − log (cid:15)
2N

+ 2

(cid:114)

log M + log 8 − log (cid:15)
2N T r

Setting M = min{NT r, N }, we have

(cid:107)

1
N

N
(cid:88)

n=1

rs ˆp

(xn) −

1
N

N
(cid:88)

n=1

rs ˆp(ΩM )

(cid:114)

(xn)(cid:107) ≤ O(

log N + log 8 − log (cid:15)
2N

+

(cid:114)

log N T r + log 8 − log (cid:15)
2N T r

)

which completes the proof.

Lemma B.8. Let

p(ΩM ) (cid:44) arg max
p∈ΩM

s.t.

1
N

1
N

N
(cid:88)

n=1

rsp

(xn)

η[sp](xn, ccc) ≤ (1 − δ)b − (cid:107)ccc(cid:107)∞

(cid:34)(cid:114)

log 8 − log (cid:15)
2N

+

(cid:114)

(cid:35)

log 8 − log (cid:15)
2N T r

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Then with probability 1 − (cid:15),

1
N

N
(cid:88)

n=1

rs ˆp(ΩM )

(xn) ≥

1
N

N
(cid:88)

n=1

rsp(ΩM )

−

(cid:114)

log 8 − log (cid:15)
2N

−

(cid:114)

log 8 − log (cid:15)
2N T r

and

1
N

N
(cid:88)

n=1

η[s ˆp(ΩM )](xn, ccc) ≤ (1 − δ)b + 2(cid:107)ccc(cid:107)∞

(cid:34)(cid:114)

log 8 − log (cid:15)
2N

+

(cid:114)

log 8 − log (cid:15)
2N T r

(cid:35)

.

Proof. By Lemma B.3, with probability 1 − (cid:15), we have

(cid:107)

1
N

N
(cid:88)

n=1

rsp

(xn) −

1
N T r

N T r
(cid:88)

n=1

rsp

(xT r

n )(cid:107) ≤

(cid:114)

log 4 − log (cid:15)
2N

+

(cid:114)

log 4 − log (cid:15)
2N T r

.

and similarly, with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

η[sp](xn, ccc) −

1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc)(cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:34)(cid:114)

log 4 − log (cid:15)
2N

+

(cid:114)

log 4 − log (cid:15)
2N T r

(cid:35)

.

which is the same as

(cid:107)

1
N

N
(cid:88)

n=1

η[sp](xn, ccc) − (1 − δ)b −





1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc) − (1 − δ)b


 (cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:34)(cid:114)

log 4 − log (cid:15)
2N

+

(cid:114)

log 4 − log (cid:15)
2N T r

(cid:35)

.

Now applying union bound, we have with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

rsp

(xn) −

1
N T r

N T r
(cid:88)

n=1

rsp

(xT r

n )(cid:107) ≤

(cid:114)

log 8 − log (cid:15)
2N

+

(cid:114)

log 8 − log (cid:15)
2N T r

.

and

(cid:107)

1
N

N
(cid:88)

n=1

η[sp](xn, ccc) − (1 − δ)b −





1
N T r

N T r
(cid:88)

n=1

η[sp](xT r

n , ccc) − (1 − δ)b


 (cid:107) ≤ (cid:107)ccc(cid:107)∞

(cid:34)(cid:114)

log 8 − log (cid:15)
2N

+

(cid:114)

log 8 − log (cid:15)
2N T r

(cid:35)

.

both hold. By Lemma B.4, we can conclude that

1
N

N
(cid:88)

n=1

rs ˆp(ΩM )

(xn) ≥

1
N

N
(cid:88)

n=1

rsp(ΩM )

−

(cid:114)

log 8 − log (cid:15)
2N

−

(cid:114)

log 8 − log (cid:15)
2N T r

.

and

1
N

N
(cid:88)

n=1

η[s ˆp(ΩM )](xn, ccc) − (1 − δ)b ≤ 2(cid:107)ccc(cid:107)∞

(cid:34)(cid:114)

log 8 − log (cid:15)
2N

+

(cid:114)

log 8 − log (cid:15)
2N T r

(cid:35)

.

with probability 1 − (cid:15), which completes the proof.

Lemma B.9. For δ = Ω(

(cid:113) log 4−log (cid:15)
N

(cid:113) log 4−log (cid:15)
N T r

+

), we have with probability 1 − (cid:15),

1
N

N
(cid:88)

n=1

rsp(ΩM )

(xn) −

1
N

N
(cid:88)

n=1

rsp∗

(cid:114)

(xn) ≥ −O(

logN − log (cid:15)
2N

+

(cid:114)

log N − log (cid:15)
2N T r

).

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Proof. Let ˜p(∆) be the optimal solution to the following problem

max
p∈R

1
N

N
(cid:88)

n=1

rsp

(xn)s.t.

1
N

η[sp](xn, ccc) ≤ b − ∆

On one hand, p∗ apparently is a feasible solution to the above problem with ∆ = 0, so we must have

1
N

N
(cid:88)

n=1

rsp∗

(xn) ≤

1
N

N
(cid:88)

n=1

rs ˜p(0)

(xn)

(B.9)

Let ∆(cid:48) = δ + (cid:107)ccc(cid:107)∞

(cid:20)(cid:113) log 8−log (cid:15)

2N

(cid:113) log 8−log (cid:15)
2N T r

+

(cid:21)
. Then ˜p(∆(cid:48)) corresponds to the following problem

max
p∈R

s.t.

1
N

1
N

N
(cid:88)

n=1

rsp

(xn)

η[sp](xn, ccc) ≤ (1 − δ)b − (cid:107)ccc(cid:107)∞

(cid:34)(cid:114)

log 8 − log (cid:15)
2N

+

(cid:114)

(cid:35)

log 8 − log (cid:15)
2N T r

Then using the same argument in the proof of Lemma B.7, we have with probability 1 − (cid:15),

(cid:107)

1
N

N
(cid:88)

n=1

rs ˜p(∆(cid:48) )

(xn) −

1
N

N
(cid:88)

n=1

rsp(ΩM )(xn)(cid:107) ≤

(cid:114)

logN + log 8 + log (cid:15)
2N

+

(cid:114)

log N + log 8 − log (cid:15)
2N T r

.

(B.10)

Furthermore, it is clear that ˜p(∆) is decreasingly-monotone with respect to ∆. In fact, removing the budget by ∆1, at most
accuracy decrease. That is to say, we

data’s APIs need be changed, and thus incurs at most

∆1
minccck >cj ck−cj

∆1
minccck >cj ck−cj
must have

(cid:107)

1
N

N
(cid:88)

n=1

rs ˜p(∆(cid:48) )

(xn) −

1
N

N
(cid:88)

n=1

rs ˜p(0)

(xn)(cid:107) ≤

∆1
minccck>cccj ccck − cccj

(B.11)

Now combining equations B.9, B.10, B.11, we can obtain

N
(cid:88)

n=1

N
(cid:88)

n=1

N
(cid:88)

n=1
(cid:114)

1
N

1
N

1
N

=

+

≥ −

rsp(ΩM )

(xn) −

1
N

N
(cid:88)

n=1

rsp∗

(xn)

rsp(ΩM )(xn) −

rs ˜p(∆(cid:48) )

(xn) −

1
N

1
N

N
(cid:88)

n=1

N
(cid:88)

n=1

rs ˜p(∆(cid:48) )

(xn)

rs ˜p(0)

(xn) +

1
N

N
(cid:88)

n=1

rs ˜p(0)

(xn) −

1
N

N
(cid:88)

n=1

rsp∗

(xn)

logN + log 8 + log (cid:15)
2N

(cid:114)

−

log N T r + log 8 − log (cid:15)
2N T r

−

∆1
minccck>cccj ccck − cccj

− 0

When δ = Ω(

(cid:20)(cid:113) log 4−log (cid:15)

N

(cid:113) log 4−log (cid:15)
N T r

+

(cid:21)

), we have

1
N

N
(cid:88)

n=1

rsp(ΩM )

(xn) −

1
N

(cid:114)

≥ − O(

logN − log (cid:15)
2N

+

N
(cid:88)

rsp∗

(xn)

n=1
(cid:114)

log N T r − log (cid:15)
2N T r

)

which completes the proof.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Now we are ready to prove the main theorem. We start by showing the bound on the reward. Suppose δ =

. By union bound, with probability 1 − (cid:15), Lemma B.7, Lemma B.8, and Lemma

(cid:18)(cid:113) log N −log (cid:15)

Θ

N
B.9 all hold, and we have

(cid:113) log N T r−log (cid:15)
N T r

+

(cid:19)

N
(cid:88)

n=1

N
(cid:88)

n=1

N
(cid:88)

1
N

1
N

1
N

=

+

n=1

(cid:32)(cid:114)

≥ − O

rs ˆp

(xn) −

rs ˆp

(xn) −

1
N

1
N

N
(cid:88)

n=1

N
(cid:88)

n=1

rsp(ΩM )

(xn) −

rsp∗

(xn)

rs ˆp(ΩM )

(xn) +

N
(cid:88)

rsp∗

(xn)

1
N

N
(cid:88)

n=1

rs ˆp(ΩM )

(xn)

1
N

n=1
(cid:114)

logN − log (cid:15)
N

+

log N T r − log (cid:15)
N T r

(cid:33)

Now note that by Theorem 4.2, we have with probability 1,

1
N

N
(cid:88)

n=1

rsp∗

(xn) ≥

1
N

N
(cid:88)

n=1

rs∗

(xn) −

1
N

Combing the above two inequalities, we have

1
N

N
(cid:88)

n=1

rs ˆp

(xn) −

1
N

N
(cid:88)

n=1

rs∗

(xn) ≥ −O

(cid:32)(cid:114)

logN − log (cid:15)
N

+

(cid:114)

log N T r − log (cid:15)
N T r

(cid:33)

Next we consider the feasibility requirement. By Lemma B.6, with probability 1 − (cid:15), s ˆp is a feasible solution to Problem
4.1. That is to say, s ˆp with probability 1 − (cid:15) is a feasible solution. Applying union bound completes the proof.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

C. Experimental Details

We provide missing experimental details in this part.

Experimental setup All experiments were run on a machine with 8 Intel Xeon Platinum 2.5 GHz cores, 32 GB RAM,
and 500GB disk with Ubuntu 16.04 LTS as the OS. Our code is implemented in Python 3.7. Each experiments, except the
case study, were run for ﬁve times to mitigate the randomness introduced by training-testing splitting.

ML tasks and services Recall that We focus on three multi-label classiﬁcation tasks, multi-label image classiﬁcation
(MIC), scene text recognition (STR), and named entity recognition (NER).

MIC is a computer vision task, where the goal is to assign a set of labels to a given image. For MIC, we use 3 different
commercial ML cloud services, Google Vision (Goo), Microsoft Vision (Mic), and Everypixel(Eve). We also use a single
shot detector model (SSD) pretrained on OpenImageV4 (Kuznetsova et al., 2020), which is freely available from GitHub
(SSD). All of those APIs produce labels from a large (and unknown) set, but the datasets we consider have bounded number
of labels. For example, there are only 80 distinct labels in COCO dataset. Thus, we remove the predicted labels which are
not in the full label set. For example, if Google API gives label {person, car, man} for an image in COCO, but man is not in
the full label set of COCO, then we will use {person, car} as the label set produced from Google.

STR is a computer vision task, where the goal is to predict all texts in a natural scene image. In the context of multi-label
classiﬁcation, we view each predicted word as a label, and all possible words as the label set. For STR, the ML services used
in the experiments are Google Vision (Goo), iFLYTEK API (Iﬂ), and Tencent API (Ten). We also use PP-OCR (Pad), an
open source model from GitHub.

NER is a natural language processing task where the goal is to extract all possible entities from a given text. For example, for
the sentence ICML was held in Long Beach in 2019, ICML should be extracted as an organization, and Long Beach should
be identiﬁed as a location. In this paper, we consider three common types of entities, person, location, and organization. For
any given text, each possible entity is viewed as a label, and the label set is the number of unique entities in the entire dataset.
For NER, we use three common APIs: Amazon Comprehend (Ama), Google NLP (GoN), and IBM natural language API
(IBM). a multi-task convolutional neural network model(Spa) from GitHub is also used.

Datasets. The experiments were conducted on 9 datasets. For MIC, we use three popular datasets including PASCAL (Ev-
eringham et al., 2015), MIR (Huiskes & Lew, 2008) and COCO (Lin et al., 2014). PASCAL is a standard object recognition
dataset with 20 distinct labels, and COCO is another one with 80 unique labels. PASCAL’s label set contains 20 common
objects: person, bird, cat, cow, dog, horse, sheep, aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining
table, potted plant, sofa, tv/monitor. The 80 objects in COCO include: person, bicycle, car, motorcycle, airplane, bus, train,
truck, boat, trafﬁc light, ﬁre hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear,
zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball
glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich,
orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse,
remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier,
toothbrush. For those two datasets, we use their original associated labels as the label set. MIR is a dataset designed for
image retrieval. There are originally 25 labels: animals, baby, bird, car, clouds, dog, female, ﬂower, food, indoor, lake, male,
night, people, plant_life, portrait, river, sea, sky, structures, sunset, transport, tree, water. We remove the label night since it
is not in the label set of any of the APIs or the GitHub model. On average, there are 1.44 labels per image for PASCAL, 3.71
labels per image for MIR, and 2.91 labels per image for COCO. The dataset statistic is summarized in Table 2. Most of the
datasets are open and under Creative Commons license (e.g., the dataset COCO (Lin et al., 2014)). The details can be found
in their corresponding paper and repository. As those datasets are actually open, they do not require an in-person consent
from the authors/developers. The datasets themselves may contain personal information (e.g., there are personal images in
COCO). Though, they have been render anonymous. For the purpose of deciding which API to call, we also do not use
personally identiﬁable information.

For STR, we use three large scale Chinese text recognition datasets, MTWI (He et al., 2018), ReCTS (Zhang et al., 2019)
and LSVT (Sun et al., 2019). The label set contains all possible Chinese characters as well as digits (0-9). MTWI contains
images from the internet mainly targeting at advertisements. Thus, most of its images have dense texts. ReCTS includes
photos taken on sign boards and thus has relatively fewer words. The images from LSVT are typically street view images

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Figure 6. Label Distribution on COCO.

and hence have medium number of words. All images in MTWI and ReCTS are fully annotated and used in our experiments.
LSVT contains both fully and partially annotated images, and we only use the subset with full annotations.

The other datasets, CONLL (Sang & Meulder, 2003), ZHNER (ZHN) and GMB (Bos, 2013), are used for NER task.
CONLL contains English sentences from newspapers, and texts from GMB are also English and from a wider range of
sources. On the other hand, ZHNER is a Chinese text dataset. We consider four common types of entities: organization,
person, and location. In this paper, we focus on three common types of entities that all datasets contain: persons,locations,
and organizations. Each sentence from those datasets is extracted as a data point, and the associated label set is simply all
entities in this sentence. An entity is considered correctly extracted if and only if it is labeled as an entity and its entity type
is correct.

GitHub model cost We evaluate the inference time of all GitHub models on an Amazon EC2 p2.x instance, which is
$0.90 per hour. For multi-label image classiﬁcation, the GitHub model (SSD) takes 6s to classify each image, resulting in an
equivalent cost of $0.0015 per image. For the named entity task, the GitHub model (Spa) can extract the entities from a
sentence in 0.015s, leading to $ 0.00000375 per sentence. The GitHub model (Pad) with the mobile version 3.0 text detector
and recognizer requires 1.5 on average to extract text from an image, causing a cost of $ 0.000375 per image. Compared to
the commercial APIs, this cost is much cheaper.

Case study on COCO Now we provide more details about the case study on the multi-label image classiﬁcation dataset,
COCO. There are in total 123,287 images containing labels from 80 different categories in COCO. Figure 6 gives the label
distribution. First note that the label distribution is quite skewed. overall, the label person is the most frequent: more than
50% of the images contain the person label. Among others, car, chair, and dining tables are also quite common labels in
this dataset with more than 10% occurrence. On the other hand, there are also quite some rare labels. For example, half
driver and toaster appear in less than 1% of the images. Such imbalance between different labels imposes a high data and
computational complexity to directly apply previous approach that learns a decision rule per label, and thus veriﬁes the
necessity of the proposed framework, FrugalMCT.

To further understand when and why FrugalMCT gives a better performance than single API, we present the precision and
recall per class for each API, majority vote, and FrugalMCT in Figure 7 and Figure 8. We ﬁrst note that there is no API
universally better than other APIs for each label. For example, GitHub and Microsoft APIs can hardly correctly predict
the label “toaster”, but Everypixel and Google APIs have a relatively high accuracy on label “toaster”. On the other hand,
Everypixel has a low accuracy on label “kite” and “knite”, while Microsoft, Google, GitHub APIs can usually predict those

airplaneapplebackpackbananabaseball batbaseball glovebearbedbenchbicyclebirdboatbookbottlebowlbroccolibuscakecarcarrotcatcell phonechairclockcouchcowcupdining tabledogdonutelephantfire hydrantforkfrisbeegiraffehair drierhandbaghorsehot dogkeyboardkiteknifelaptopmicrowavemotorcyclemouseorangeovenparking meterpersonpizzapotted plantrefrigeratorremotesandwichscissorssheepsinkskateboardskissnowboardspoonsports ballstop signsuitcasesurfboardteddy beartennis rackettietoastertoilettoothbrushtraffic lighttraintrucktvumbrellavasewine glasszebra0.00.10.20.30.40.5FrequencyEfﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Table 6. Comparison of ensemble methods as well as cost-aware approaches. For FrugalMCT and FrugalML, we pick their corresponding
strategies that minimize the cost while ensures that the accuracy reaches the highest possible.

best single API

FrugalML

FrugalMCT majority vote weighted maj vote

acc

74.8

41.2

47.5

67.9

61.3

53.8

52.6

61.3

50.1

PASCAL

MIR

COCO

MTWI

ReCTS

LSVT

CONLL

ZHNER

GMB

cost

acc

cost

acc

cost

acc

cost

10

10

10

210

210

210

3

30

30

76.9

43.8

49.3

68.1

63.4

56.2

55.7

67.4

52.6

11

8

8

213

213

213

32

31.2

30.1

78.5

49.2

54

71.1

64.7

57.2

56.8

71.8

53.1

8

14

12

208

208

208

36.8

36.8

20.5

77.8

41.4

50.1

75.4

70.2

62.8

58.5

66

51.3

31.01

31.01

31.01

275.01

275.01

275.01

43.01

43.01

43.01

acc

77.8

48.7

52.8

75.4

70.2

62.8

58.5

66

51.5

cost

31.01

31.01

31.01

275.01

275.01

275.01

43.01

43.01

43.01

labels with higher accuracy. This implies that combining different APIs may produce an accuracy better than any single one
of them. There are also some easy labels on which all APIs give a high accuracy. For example, on the label “zebra”, all APIs
give a 90% precision and recall. This actually suggests that it is not always necessary to use all API. For example, if GitHub
predicts an image has the label “zebra”, and we know there is no other labels in this image, then probably there is no need to
call any other APIs.

Another interesting observation is that FrugalMCT improves the precision and recall for almost every label compared to any
single API. This is primarily because FrugalMCT appropriately utilizes the predicted label information from GitHub model
to infer which API is better on certain input, and combine its performance with the base API aptly. Yet, the precision and
recall difference can be quite different for different APIs. For example, as shown in Figure 8(c), the recall for “airplane” is
much higher than its precision, but banana’s precision is much higher than its recall. For applications that have speciﬁc
precision and recall requirements, we may adopt different accuracy metrics in FrugalMCT. Another interesting observation
is that the precision and recall for some labels is extremely. For example, “hair drier” cannot be predicted by FrugalMCT,
which is due to that no API actually predicts this label correctly. How to extend FrugalMCT to recognize unseen labels
remains an open question.

Table 7. Accuracy predictor performance. RMSE and PCC stand for root mean square error and Pearson’s correlation coefﬁcient.

Data

PASCAL

MIR

COCO

MTWI

ReCTS

LSVT

CONLL

ZHNER

GMB

RMSE

PCC

FrugalMCT DAP

FrugalMCT

DAP

0.28

0.22

0.24

0.17

0.22

0.19

0.29

0.31

0.28

0.35

0.31

0.31

0.21

0.27

0.24

0.41

0.36

0.40

0.55

0.55

0.63

0.57

0.57

0.61

0.72

0.48

0.69

0.012

-0.013

0.001

0.004

0.001

-0.003

-0.003

-0.005

-0.006

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

Ensemble method comparison For comparison, we compare FrugalMCT against FrugalML as well as two ensemble
methods, majority vote and weighted majority vote. In majority vote, for each label, we accept it if at least half of the APIs
predict it. In weighted majority vote, we assign each API’s accuracy as its weight. Next, for each label, we compute a label
score, which is equal to the sum of each API’s conﬁdence score on this label weighted by its corresponding weight. If an
API does not predict a label, then its conﬁdence score is viewed as 0. Finally, we only accept the label if its label score is
larger than a threshold. We pick a threshold that maximizes the overall accuracy by grid search.

The results are summarized in Table 6. Overall, we observe that FrugalMCT and ensemble methods have similar performance
across different tasks and datasets, but with a much lower cost. In fact, for datasets including COCO and ZHNER,
FrugalMCT can achieve an accuracy enven higher than ensemble methods.

Accuracy predictor performance Note that FrugalMCT’s performance highly depends on its accuracy predictors’
performance. Tn obtain a quantitative sense of the accuracy predictors, we evaluate the accuracy predictors’ performance in
Table 7. RMSE measures the standard deviation of the difference between accuracy predictor’s output and the corresponding
true accuracy. PCC stands for Pearson correlation coefﬁcient, which roughly measures the linear correlation between the
true accuracy and the predicted value from the accuracy predictors. Overall, FrugalMCT’ random forest predictors enjoy a
much smaller RMSE and higher PCC than DAP (the dummy accuracy predictors), which matches the fact that FrugalMCT
gives a higher end to end performance than using the DAP.

Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

(a) GitHub

(b) Everypixel

(c) Microsoft

Figure 7. The per class precision and recall of different APIs .

airplaneapplebackpackbananabaseball batbaseball glovebearbedbenchbicyclebirdboatbookbottlebowlbroccolibuscakecarcarrotcatcell phonechairclockcouchcowcupdining tabledogdonutelephantfire hydrantforkfrisbeegiraffehair drierhandbaghorsehot dogkeyboardkiteknifelaptopmicrowavemotorcyclemouseorangeovenparking meterpersonpizzapotted plantrefrigeratorremotesandwichscissorssheepsinkskateboardskissnowboardspoonsports ballstop signsuitcasesurfboardteddy beartennis rackettietoastertoilettoothbrushtraffic lighttraintrucktvumbrellavasewine glasszebra0.00.20.40.60.81.0recallprecisionairplaneapplebackpackbananabaseball batbaseball glovebearbedbenchbicyclebirdboatbookbottlebowlbroccolibuscakecarcarrotcatcell phonechairclockcouchcowcupdining tabledogdonutelephantfire hydrantforkfrisbeegiraffehair drierhandbaghorsehot dogkeyboardkiteknifelaptopmicrowavemotorcyclemouseorangeovenparking meterpersonpizzapotted plantrefrigeratorremotesandwichscissorssheepsinkskateboardskissnowboardspoonsports ballstop signsuitcasesurfboardteddy beartennis rackettietoastertoilettoothbrushtraffic lighttraintrucktvumbrellavasewine glasszebra0.00.20.40.60.81.0airplaneapplebackpackbananabaseball batbaseball glovebearbedbenchbicyclebirdboatbookbottlebowlbroccolibuscakecarcarrotcatcell phonechairclockcouchcowcupdining tabledogdonutelephantfire hydrantforkfrisbeegiraffehair drierhandbaghorsehot dogkeyboardkiteknifelaptopmicrowavemotorcyclemouseorangeovenparking meterpersonpizzapotted plantrefrigeratorremotesandwichscissorssheepsinkskateboardskissnowboardspoonsports ballstop signsuitcasesurfboardteddy beartennis rackettietoastertoilettoothbrushtraffic lighttraintrucktvumbrellavasewine glasszebra0.00.20.40.60.81.0Efﬁcient Online ML API Selection for Multi-Label Classiﬁcation Tasks

(a) Google

(b) Majority Vote

(c) FrugalMCT

Figure 8. The per class precision and recall of different APIs .

airplaneapplebackpackbananabaseball batbaseball glovebearbedbenchbicyclebirdboatbookbottlebowlbroccolibuscakecarcarrotcatcell phonechairclockcouchcowcupdining tabledogdonutelephantfire hydrantforkfrisbeegiraffehair drierhandbaghorsehot dogkeyboardkiteknifelaptopmicrowavemotorcyclemouseorangeovenparking meterpersonpizzapotted plantrefrigeratorremotesandwichscissorssheepsinkskateboardskissnowboardspoonsports ballstop signsuitcasesurfboardteddy beartennis rackettietoastertoilettoothbrushtraffic lighttraintrucktvumbrellavasewine glasszebra0.00.20.40.60.81.0airplaneapplebackpackbananabaseball batbaseball glovebearbedbenchbicyclebirdboatbookbottlebowlbroccolibuscakecarcarrotcatcell phonechairclockcouchcowcupdining tabledogdonutelephantfire hydrantforkfrisbeegiraffehair drierhandbaghorsehot dogkeyboardkiteknifelaptopmicrowavemotorcyclemouseorangeovenparking meterpersonpizzapotted plantrefrigeratorremotesandwichscissorssheepsinkskateboardskissnowboardspoonsports ballstop signsuitcasesurfboardteddy beartennis rackettietoastertoilettoothbrushtraffic lighttraintrucktvumbrellavasewine glasszebra0.00.20.40.60.81.0airplaneapplebackpackbananabaseball batbaseball glovebearbedbenchbicyclebirdboatbookbottlebowlbroccolibuscakecarcarrotcatcell phonechairclockcouchcowcupdining tabledogdonutelephantfire hydrantforkfrisbeegiraffehair drierhandbaghorsehot dogkeyboardkiteknifelaptopmicrowavemotorcyclemouseorangeovenparking meterpersonpizzapotted plantrefrigeratorremotesandwichscissorssheepsinkskateboardskissnowboardspoonsports ballstop signsuitcasesurfboardteddy beartennis rackettietoastertoilettoothbrushtraffic lighttraintrucktvumbrellavasewine glasszebra0.00.20.40.60.81.0