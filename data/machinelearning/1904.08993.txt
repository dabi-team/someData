Playgol: learning programs through play

Andrew Cropper
University of Oxford
andrew.cropper@cs.ox.ac.uk

9
1
0
2

y
a
M
0
2

]

G
L
.
s
c
[

2
v
3
9
9
8
0
.
4
0
9
1
:
v
i
X
r
a

Abstract

Children learn though play. We introduce the anal-
ogous idea of learning programs through play. In
this approach, a program induction system (the
learner) is given a set of user-supplied build tasks
and initial background knowledge (BK). Before
solving the build tasks, the learner enters an unsu-
pervised playing stage where it creates its own play
tasks to solve, tries to solve them, and saves any
solutions (programs) to the BK. After the playing
stage is ﬁnished, the learner enters the supervised
building stage where it tries to solve the build tasks
and can reuse solutions learnt whilst playing. The
idea is that playing allows the learner to discover
reusable general programs on its own which can
then help solve the build tasks. We claim that play-
ing can improve learning performance. We show
that playing can reduce the textual complexity of
target concepts which in turn reduces the sample
complexity of a learner. We implement our idea
in Playgol, a new inductive logic programming sys-
tem. We experimentally test our claim on two do-
mains: robot planning and real-world string trans-
formations. Our experimental results suggest that
playing can substantially improve learning perfor-
mance. We think that the idea of playing is an im-
portant contribution to the problem of developing
program induction approaches that self-discover
BK.

play

learn

though

1 Introduction
[Schulz et al., 2007;
Children
Sim and Xu, 2017; Sim et al., 2017]. We introduce the
analogous idea of learning programs through play. In this
approach, a program induction system (the learner) is given
a set of user-supplied build tasks and initial background
knowledge (BK). Whereas a standard program induction
system would immediately try to solve the build tasks, in
our approach the learner ﬁrst enters an unsupervised playing
stage. In this stage the learner creates its own play tasks to
solve, tries to solve them, and saves any solutions (programs)
to the BK. After the playing stage is ﬁnished, the learner
enters the supervised building stage where it tries to solve

the user-supplied build tasks and can reuse solutions learned
whilst playing. The idea is that playing allows the learner
to discover reusable general programs on its own which can
then be reused in the building stage, and thus improve perfor-
mance. For instance, if trying to learn sorting algorithms, a
learner could discover the concepts of partition and merge
when playing which could then help learn quicksort and
mergesort respectively.

To further illustrate our play idea, imagine a child that had
never seen Lego before. Suppose you presented the child with
Lego bricks and immediately asked them to build a (minia-
ture) house with a pitched roof. The child would proba-
bly struggle to build the house without ﬁrst knowing how
to build a stable wall or how to build a pitched roof. Now
suppose that before you asked the child to build the house,
you ﬁrst left them alone to play with the Lego. Whilst play-
ing the child may build animals, gardens, ships, or many
other seemingly irrelevant things. However, the child is
likely to discover reusable and general concepts, such as the
concept of a stable wall. As we discuss in Section 2, the
cognitive science literature shows that children can better
learn complex rules after a period of play rather than solely
through observation [Schulz et al., 2007; Sim and Xu, 2017;
Sim et al., 2017].
In this paper, we explore whether a pro-
gram induction system can similarly better learn programs
after a period of play.

Our

forms

of
predeﬁned,

require
static BK as input

idea of using play to discover useful BK
program induction
contrasts with most
human-
often
usually
which
[Muggleton et al., 2015;
engineered,
Law et al., 2014;
Cropper and Muggleton, 2016a;
Cropper and Muggleton, 2018;
Sch¨uller and Benz, 2018;
Evans and Grefenstette, 2018;
Gulwani, 2011;
Ellis et al., 2018b]. Our idea is related to program in-
duction approaches that perform multitask or meta learning
[Lin et al., 2014; Dechter et al., 2013; Ellis et al., 2018a;
Ellis and Gulwani, 2017].
In these approaches, a learner
acquires BK in a supervised manner by solving sets of user-
provided tasks, each time saving solutions to the BK, which
can then be reused to solve other tasks. In contrast to these
supervised approaches, our play approach discovers useful
BK in an unsupervised manner whilst playing. Playing can
therefore be seen as an unsupervised technique for a learner
to discover the BK necessary to solve complex tasks, i.e. a

 
 
 
 
 
 
form of unsupervised bootstrapping for supervised program
induction.

We claim that playing can improve learning performance.

To support this claim, we make the following contributions:

• We introduce the idea of learning programs through play
and show that playing can reduce the textual complexity of
target concepts which in turn reduces the sample complex-
ity of a learner (Section 3).

• We implement our idea in Playgol, a new inductive
system based on meta-
[Muggleton et al., 2015;

logic programming (ILP)
interpretive
Cropper and Muggleton, 2016a] (Section 4).

learning

(MIL)

• We experimentally show on two domains (robot planning
and real-world string transformations) that playing can sig-
niﬁcantly improve learning performance (Section 5).

2 Related work

Program induction Program induction approaches learn
computer programs from data. Much recent work has focused
on task-speciﬁc approaches for real-world problems, such as
string transformations [Gulwani, 2011]. By contrast, we are
interested in general program induction approaches that work
on multiple domains. Speciﬁcally, we want to develop pro-
gram induction techniques that discover reusable general con-
cepts, which was the goal of many early AI systems, such as
Lenant’s AM system [Lenat, 1977].

the

require

form of

BK as
to
approaches

induction Program
a
restrict

Meta-program
induction
inductive
use
approaches
[Mitchell, 1997]
hypothesis
bias
[Muggleton et al., 2015;
space.
Most
Law et al., 2014;
Cropper and Muggleton, 2016a;
Cropper and Muggleton, 2018;
Sch¨uller and Benz, 2018;
Evans and Grefenstette, 2018;
Gulwani, 2011;
Ellis et al., 2018b]
often
as
limitation,
hand-engineered, BK. To
several approaches attempt
time
[Lin et al., 2014; Dechter et al., 2013; Ellis et al., 2018a;
Ellis and Gulwani, 2017], which
as
a
form of meta-learning [Thrun and Pratt, 2012].
In ILP,
also known as automatic bias-revision
meta-learning,
[Dietterich et al., 2008], involves saving learned programs to
the BK so that they can be reused to help learn programs for
unsolved tasks. Curriculum learning [Bengio et al., 2009]
is a similar idea but requires an ordering over the given
tasks. By contrast, our approach, and the aforementioned
approaches, do not require an ordering over the tasks.

input
overcome
to acquire BK over

a ﬁxed,

seen

this

can

be

Lin et al.

[Lin et al., 2014] use a technique called
dependent
learning to allow the MIL system Metagol
[Cropper and Muggleton, 2016b] to learn string transforma-
tions programs over time. Their approach uses predicate in-
vention to reform the bias of the learner where after a solution
is learned not only is the target predicate added to the BK but
also its constituent invented predicates. The authors show that
their dependent learning approach performs substantially bet-
ter than an independent (single-task) approach. Dechter et al.
[Dechter et al., 2013] studied a similar approach for learning
functional programs.

These existing approaches perform supervised meta-
they need a corpus of user-supplied training
learning, i.e.
tasks. By contrast, our playing approach is unsupervised,
where the tasks come not from the user but from the system it-
self. In other words, our approach allows a learner to discover
highly reusable concepts without a user-supplied corpus of
training tasks, which Ellis et al. [Ellis et al., 2018a] argue is
essential for program induction to become a standard part of
the AI toolkit.

Pre-training and latent structures Our playing idea can
be seen as an unsupervised pre-training step. DeepCoder
[Balog et al., 2017] also has a pre-training step. In this step,
Deepcoder enumerates every hypothesis in the hypothesis
space (up to a depth limit) and generates random input/out-
puts for each hypothesis. Deepcoder uses the examples and
the hypotheses to train a neural network to model the distri-
bution over the user-supplied functions in the BK. By con-
trast, Playgol randomly samples play tasks from the instance
space and tries to learn the solutions for them.
In addi-
tion, whereas DeepCoder learns a distribution over a ﬁxed-set
of user-supplied functions, Playgol discovers new programs
through play and predicate invention, which can be reused
during the building stage.

Our playing idea can also be seen as an unsupervised ap-
proach to discover latent features (i.e. predicates). Dumancic
and Blockeel [Dumancic and Blockeel, 2017] also consider
using unsupervised pre-training to improve the performance
of an ILP system (TILDE [Blockeel and Raedt, 1998]). Their
CUR2LED approach focuses on learning relational latent rep-
resentations in an unsupervised manner, and uses clustering
to obtain latent features. They show that their approach im-
proves the predicate accuracy of TILDE and reduces the com-
plexity of a learned model (compared to no pre-training). Al-
though Playgol differs from CUR2LED in many ways, both
share the goal of discovering new language constructs in an
unsupervised manner.

Playing Several studies have shown that children learn suc-
cessfully when they have the opportunity to choose what they
want to do. Schulz et al. [Schulz et al., 2007] found that chil-
dren were able to use self-generated evidence to learn about
a causal systems. Sim and Xu [Sim and Xu, 2017] found that
three-year-olds were capable of forming higher-order general-
isations about a causal system after a short play period. Sim
et al. [Sim et al., 2017] showed that children perform signif-
icantly better when learning complex clausal rules through
free play or by ﬁrst engaging in free play and then observing,
as opposed to solely through observation. As far as we are
aware, there is no research studying whether playing can im-
prove machine learning performance, especially in program
induction.

Meta-interpretive learning Our idea of learning programs
through play is sufﬁciently general to work with any form
of program induction, such as inducing functional programs.
However, to clearly explain our theoretical and empirical re-
sults, we formalise the problem in an ILP setting using MIL.
We use MIL for two key reasons. First, MIL supports learning
recursive programs, which is important in the string transfor-
mation experiments. Second, MIL uses predicate invention to

decompose problems into smaller problems which can then
be reused [Cropper and Muggleton, 2016a].

3 Problem setting
We now describe the learning programs through play prob-
lem, which, for conciseness, we refer to as the Playgol prob-
lem.

3.1 Problem deﬁnition
Given a set of tasks and BK, our problem is to induce a set
of programs to solve each task. We formalise the problem in
an ILP learning from entailment setting [Raedt, 2008]. We
deﬁne the input to the problem:

Deﬁnition 1 (Playgol input). A Playgol input is a tuple
(H, E, B, T ) where:
• H is the hypothesis space formed of datalog programs
• E is the instance space formed of ground atoms
• B is background knowledge represented as a datalog pro-

gram

• T is set of k build tasks {E1, E2, . . . , Ek} where each Ei
is a pair (E+
i ⊆ E repre-
sent positive and negative examples respectively of a target
predicate

i ⊆ E and E−

i ) where E+

i , E−

Note that in a Playgol input, a build task does not need to have
negative examples, i.e. E−

i may be an empty set.

The Playgol problem is to ﬁnd a consistent program for

each task:

Deﬁnition 2 (Playgol problem). Given a Playgol input
(H, E, B, T ), the goal is to return a set of hypotheses {Hi ∈
H|(E+

i ) ∈ T, (Hi ∪ B |= E+

i ) ∧ (Hi ∪ B 6|= E−

i , E−

i )}

3.2 Meta-interpretive learning
We solve the Playgol problem using MIL, a form of ILP based
on a Prolog meta-interpreter. For brevity, we omit a formal
description of MIL, and refer the reader to the literature for
more details [Cropper, 2017]. We instead provide an informal
overview. A MIL learner is given as input sets of atoms rep-
resenting positive and negative examples of a target concept,
background knowledge in the form of a logic program, and,
crucially, a set of second-order formulas called metarules. A
MIL learner works by trying to construct a proof of the posi-
tive examples. It uses the metarules to guide the proof search.
Metarules can therefore be seen as program templates. Fig-
ure 1 shows some commonly used metarules. Once a proof is
found a MIL learner extracts a logic program from the proof
and checks that it is inconsistent with the negative examples.
If not, it backtracks to consider alternative proofs.

3.3 Sample complexity
We claim that playing can improve learning performance. We
support this claim by showing that playing can reduce the size
of the MIL hypothesis space which in turn reduces sample
complexity [Mitchell, 1997] and expected error. In MIL the
size of the hypothesis space is a function of the metarules, the
number of background predicates, and the maximum program
size. We restrict metarules by their body size and literal arity:

Name Metarule
P (A, B) ← Q(A), R(A, B)
precon
postcon P (A, B) ← Q(A, B), R(B)
chain
tailrec

P (A, B) ← Q(A, C), R(C, B)
P (A, B) ← Q(A, C), P (C, B)

Figure 1: Example metarules. The letters P , Q, and R denote
second-order variables. The letters A, B, and C denote ﬁrst-order
variables.

Deﬁnition 3. A metarule is in Mi
in the body and each literal has arity at most i.

j if it has at most j literals

By restricting the form of metarules we can calculate the size
of a MIL hypothesis space:
Proposition 1 (Hypothesis space [Cropper, 2017]). Given
p predicate symbols and m metarules in Mi
j, the number of
programs expressible with n clauses is (mpj+1)n.
We use this result to show the MIL sample complexity:
complexity [Cropper, 2017]).
Proposition 2 (Sample
Given p predicate symbols, m metarules in Mi
j, and a clause
bound n, MIL has sample complexity s with error ǫ and
conﬁdence δ:

s ≥

1
ǫ

(n ln(m) + (j + 1)n ln(p) + ln

1
δ

)

Proposition 2 helps explain our idea of playing. When play-
ing, a learner creates its own play tasks and saves any solu-
tions to the BK, which increases the number of predicate sym-
bols p. The solutions learned whilst playing may in turn help
solve the user-supplied build tasks, i.e. could reduce the size
n of the target program. To reuse the example from the intro-
duction, if trying to learn sorting algorithms, a learner could
discover the concepts of partition and merge when playing
which could then help learn quicksort and mergesort respec-
tively. In other words, the key idea of playing is to increase
the number of predicate symbols p in order to reduce the size
n of the target program. We consider when playing can re-
duce sample complexity:
Theorem 1 (Playgol improvement). Given p predicate sym-
bols and m metarules in Mi
j, let n be the minimum num-
bers of clauses needed to express a target theory with standard
MIL. Let n − k be the minimum number of clauses needed
to express a target theory with Playgol using an additional c
predicate symbols. Let s and s′ be the bounds on the num-
ber of training examples required to achieve error less than ǫ
with probability at least 1 − δ with standard MIL and Playgol
respectively. Then s > s′ when:

n ln(p) > (n − k) ln(p + c)

Proof. Follows from Proposition 2 and rearranging of terms.

Theorem 1 shows when playing can reduce sample complex-
ity compared to not playing.
In such cases, if the number
of training examples is ﬁxed for both approaches, the corre-
sponding discrepancy in sample complexity is balanced by
an increase in predictive error [Blumer et al., 1989]. In other

words, Theorem 1 shows that adding extra (sometimes irrele-
vant) predicates to BK can improve learning performance so
long as some can be reused to learn new programs.

4 Playgol

Algorithm 1 shows the Playgol algorithm, which uses
Metagol [Cropper and Muggleton, 2016b], a MIL implemen-
tation, as the main learning algorithm. Playgol takes as input
an instance space (the set of all possible examples) E, initial
background knowledge BK, a set of user-supplied build tasks
Tb, a playtime value (playtime), and a maximum search depth
maxd.

Playgol ﬁrst enters the unsupervised playing stage. In this
stage, Playgol creates its own play tasks Tp by sampling
uniformly with replacement playtime elements from the in-
stance space. We consider this step to be unsupervised be-
cause (1) the tasks are not selected by the user, and (2) no
labels (i.e. positive or negative) are provided by the user. Af-
ter creating play tasks, Playgol then uses a dependent learn-
ing approach [Lin et al., 2014] to expand the BK. Starting at
depth d=1, Playgol tries to solve each play task using at most
d clauses. To solve an individual task, Playgol calls Metagol.
Each time a play task is solved, the solution (program) is
added the BK and can be reused to help solve other play
tasks. Once Playgol has tried to solve all play tasks at depth
d, it increases the depth and tries to solve the remaining play
tasks. Playgol repeats this process until it reaches the maxi-
mum depth (maxd), then it returns the initial BK augmented
with solutions to the play tasks.

Playgol then enters the supervised building stage. In this
stage Playgol tries to solve each user-supplied build task us-
ing the augmented BK using a standard independent learning
approach, eventually returning a set of induced programs.

Algorithm 1 Playgol

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

func playgol(E,BK,Tb,playtime,maxd)
BK = play(E,BK,playtime,maxd)
return build(Tb,BK,maxd)

func play(E,BK,playtime,maxd)
Tp = sample(E,playtime)
for d=1 to maxd

for E+ in Tp:

prog = metagol(BK,E+,{},maxd)
if prog != null

BK = BK ∪ {prog}
Tp = Tp \ E+

return BK

func build(Tb,BK,maxd)

P = {}
for (E+,E−) in Tb

prog = metagol(BK,E+,E−,maxd)
if prog != null

P = P ∪ {prog}

return P

5 Experiments

We claim that playing can improve learning performance. We
now experimentally test our claim. We test the null hypothe-
sis:

Null hypothesis 1 Playing cannot improve learning perfor-

mance

Theorem 1 shows that playing can reduce sample complexity
compared to not playing. Theorem 1 does do not, however,
state how many play tasks are needed to improve learning
performance. Playgol creates its own play tasks by sampling
from the instance space. Suppose we sampled uniformly
at random without replacement from a ﬁnite instance space.
Then if we sample enough times we will sample every in-
stance. One could therefore argue that Playgol is doing noth-
ing more than sampling play tasks that it will eventually have
to solve (i.e. Playgol is sampling build tasks whilst playing).
To refute this argument we test the null hypothesis:

Null hypothesis 2 Playing cannot improve learning perfor-

mance without many play tasks

To test null hypotheses 1 and 2 we compare Playgol’s perfor-
mance when varying the number of play tasks. When there
are no play tasks Playgol is equivalent to Metagol.

A key motivation for using MIL is that it supports predi-
cate invention. Although we provide no theoretical justiﬁca-
tion, we claim that predicate invention is useful when playing
because it allows for problems to be decomposed into smaller
reusable sub-problems. We test this claim with the null hy-
pothesis:

Null hypothesis 3 Saving invented predicates whilst playing

cannot improve learning performance

To test null hypothesis 3 we use a variant of Playgol called
PlaygolN H3.
The only difference between Playgol and
PlaygolN H3 is that Playgol uses all the top-level and in-
vented predicates discovered whilst playing when building.
By contrast, PlaygolN H3 uses only the top-level predicates
discovered whilst playing when building. For instance, sup-
pose that whilst playing both Playgol and PlaygolN H3 dis-
covered mergesort and did so by inventing predicates for the
sub-deﬁnitions split and merge. Then when building Playgol
would use mergesort, split, and merge, whereas PlaygolN H3
would only use mergesort.

5.1 Robot planning

Our ﬁrst experiment focuses on learning robot plans.
Materials There is a robot and a ball in an n2 space. The
robot can move around and can grab and drop the ball. The
goal is to learn a program to move from the initial state to
the ﬁnal state. The robot can perform six dyadic actions to
transform the state: up, down, right, left, grab, and drop.
Training examples are atoms of the form f (s1, s2), where f
is the target predicate and s1 and s2 are initial and ﬁnal states
respectively. We allow Playgol to learn programs using the
ident and chain metarules (Figure 1). We use 52 and 62 spaces
with instance spaces X5 and X6 respectively. The instance

spaces contain all possible f (s1, s2) atoms. The cardinalities
of X5 and X6 are approximately 58 and 68 respectively1.
Method Our experimental method is as follows. We sample
uniformly with replacement 1000 atoms from Xn to form the
build tasks Tb. Then for each p in {0, 200, 400, . . . , 2000},
we call playgol(Xn,BK,Tb,p,5) which returns a set of pro-
grams Pp. We measure the percentage of correct solutions
in Pp. We enforce a timeout of 60 seconds per play and build
task. We measure the standard error of the mean over 10 rep-
etitions.

Results Figure 2 shows that Playgol solves more build tasks
given more play tasks. For the 52 space, Playgol solves only
12% of the build tasks without playing. The baseline repre-
sents the performance of Metagol (i.e. learning without play).
By contrast, playing improves performance in all cases. Af-
ter 1000 play tasks, Playgol solves almost 100% of the build
tasks. For the 62 space, the results are similar, where the build
performance is only 7% without playing but over 60% after
1200 play tasks. These results suggest that we can reject null
hypothesis 1, i.e. we can conclude that playing can improve
learning performance.

As already mentioned, one may argue that Playgol is sim-
ply sampling build tasks as play tasks. Such duplication may
occur. In this experiment, for us to sample all of the build
tasks we would expect to sample Θ(|Xn| log(|Xn|)) play
tasks2, which corresponds to sampling approximately 5 mil-
lion and 24 million tasks for the 52 and 62 spaces respectively.
However, our experimental results show that to solve almost
all of the build tasks we only need to sample approximately
1000 and 2000 play tasks for the 52 and 62 spaces respectively.
These values are less than 1/1000 of the expected rate. There-
fore, our experimental results suggest that we can reject null
hypothesis 2, i.e. we can conclude that playing can improve
learning performance without needing to sample many play
tasks.

Finally, Figure 2 shows that Playgol solves more tasks than
PlaygolN H3, although in the 52 space both approaches con-
verge after 2000 play tasks. A McNemar’s test on the results
of Playgol and PlaygolN H3 conﬁrmed the signiﬁcance at the
p < 0.001 level for the 52 and 62 spaces. This result suggests
that we can reject null hypothesis 3, i.e. we can conclude that
predicate invention can improve learning performance when
playing.

5.2 String transformations
Our ﬁrst experiment tested the null hypotheses in a controlled
experimental setting. We now see whether playing can im-
prove learning performance on ‘real-world’ string transforma-
tions.

Materials We use 94 real-word string transformation tasks.
Our dataset is based on the dataset from [Lin et al., 2014],

1In each state there are n2 positions for the robot, n2 positions for
the ball, and the robot can or cannot be hold the ball, thus there are
approximately 2n4 states. The instance space contains all possible
start/end state pairs, thus approximately 2n8 atoms

2This problem is an instance of the coupon collectors problem

[Wikipedia contributors, 2018]

100

d
e
v
l
o
s

s
k
s
a
t
d
l
i
u
b
%

80

60

40

20

0

0

Playgol
PlaygolNH3
baseline

100

d
e
v
l
o
s

s
k
s
a
t
d
l
i
u
b
%

80

60

40

20

Playgol
PlaygolNH3
baseline

1,500

2,000

0

0

500

1,000
# play tasks

(a) 52 space

1,500

2,000

500

1,000
# play tasks

(b) 62 space

Figure 2: Robot experiment results. The baseline represents learning
without play (i.e. Metagol).

which in turn is based on [Gulwani, 2011]. We augmented
the dataset with more manually created tasks, taken from a
variety of sources (such as stackoverﬂow, excel forums, etc).
Each task has 10 examples. Each example is an atom of the
form f (x, y) where f is the task name and x and y are input
and output strings respectively. Figure 3 shows three exam-
ples for the build task build 95, where the goal is to learn a
program that extracts the ﬁrst three letters of the month name
and makes them uppercase.

Input
Output
JUL
22 July,1983 (35 years old)
30 October,1955 (63 years old)
OCT
2 November,1954 (64 years old) NOV

Figure 3: Examples for the build 95 string transformation problem.

In the build stage we use the real-word tasks. In the play stage,
Playgol samples play tasks from the instance space X formed
of random string transformations. The play tasks are formed
from an alphabet with 80 symbols, including the letters a-z, A-
Z, the numbers 0-9, and punctuation symbols (<,>,+,-, ,etc).
To generate a play task we use the following procedure. We
select a random integer l between 3 and 20 to represent the
input length. We generate a random string x of length l to rep-
resent the input string. We select a random integer p between
3 and 20 and enumerate all programs P of length p consistent
with x. We select a random program from P and apply it to
x to generate the output string y to form the example f (x, y)
where f is the play task name. This procedure only gener-
ates play tasks that are theoretically solvable, i.e. for which
there is a hypothesis in the hypothesis space. Figure 4 shows
example play tasks.

Input
.f\73\R)

Task
Output
play 9
F
play 52 @B4\X¿3MjKdyZzC B
play 136
9pfy”ktfbS1v
play 228
I6zihQk-

99PF
Q

Figure 4: Examples of randomly generated play tasks for the string
transformation experiment.

The play instance space X contains all possible string trans-

formations consistent with the aforementioned procedure.
The space contains approximately 8040 atoms3.

We provide Playgol with the metarules precon, postcon,
chain, and tailrec; the monadic predicates: empty, space,
letter, number, uppercase, lowercase; the negations of
the monadics not empty, not space, etc; and the dyadic
predicates copy, skip, mk uppercase, mk lowercase. These
predicates are based on those used in [Lin et al., 2014].
Method Our experimental method is as follows.
For
each real-word string transformation task ti, we sample
uniformly without replacement 5 atoms from ti
to form
the training examples ti,train and use the remaining 5
atoms as the testing examples ti,test. The set of build
tasks Tb is thus the set of individual tasks, e.g. Tb =
The set of testing examples Tt
{t1,train, t2,train, . . . }.
For each p
is likewise Tt = {t1,test, t2,test, . . . }.
in {0, 200, 400, . . . , 2000}, we call playgol(X,BK,Tb,p,5)
which returns a set of programs Pp. We measure the pre-
dictive accuracy of Pp against the testing examples Tt. We
enforce a learning timeout of 60 seconds per play and build
task. If Playgol learns no program then every test example
is deemed false. We measure the standard error of the mean
over 10 repetitions.

Results Figure 5 shows the mean predictive accuracies of
Playgol when varying the number of play tasks. Note that we
are not interested in the absolute predictive accuracies of Play-
gol, which are low because of the small timeout and the difﬁ-
culty of the problems. We are instead interested in how the ac-
curacies change given more play tasks, and the difference in
accuracies between Playgol and PlaygolN H3. Figure 5 shows
that Playgol’s predictive accuracy improves given more play
tasks. Playgol’s accuracy is 25% without playing. By con-
trast, playing improves accuracy in all cases. After 2000 play
tasks, the accuracy is almost 37%, an improvement of 12%.

Figure 6 shows an example of when playing improved
building performance, where the solution to the build task
b95 is composed of the solutions to many play tasks. The
solutions to the play tasks are themselves are often composed
of solutions to other play tasks, including reusing many in-
vented predicates. This example clearly demonstrates the use
of predicate invention to discover highly reusable concepts
that build on each other.

Overall the results from this experiment add further evi-

dence for rejecting all the null hypotheses.

6 Conclusions and future work
We have introduced the idea of learning programs through
play. In this approach, a program induction system creates its
own play tasks to solve, tries to solve them, and saves any so-
lutions to its BK, which can then be reused to solve the user-
supplied build tasks. We claimed that playing can improve
learning performance. Our theoretical results support this
claim and show that playing can reduce the sample complex-
ity of a learner (Theorem 1). We have implemented our idea
in Playgol, a new ILP system. Our experimental results on

3In the case that the input is length 20 there are 8020 possible

strings, thus 8040 pairs.

40

35

30

25

y
c
a
r
u
c
c
a

e
v
i
t
c
i
d
e
r
p
%

Playgol
PlaygolNH3
baseline

0

500

1,000
# play tasks

1,500

2,000

Figure 5: String experiment results.

build_95(A,B):-play_228(A,C),play_136_1(C,B).
play_228(A,B):-play_52(A,B),uppercase(B).
play_228(A,B):-skip(A,C),play_228(C,B).
play_136_1(A,B):-play_9(A,C),mk_uppercase(C,B).
play_9(A,B):-skip(A,C),mk_uppercase(C,B).
play_52(A,B):-skip(A,C),copy(C,B).

Figure 6: Program learned by Playgol for the build task build 95
(Figure 3). The solution for build 95 reuses the solution to the play
task play 228 and the sub-program play 136 1 from the play task
play 136, where play 136 1 is invented. The predicate play 228
is a recursive deﬁnition that corresponds to the concept of “skip
to the ﬁrst uppercase letter and then copy the letter to the output”.
The predicate play 228 reuses the solution for another play task
play 52. Figure 4 shows these play tasks.

two domains (robot planning and string transformations) fur-
ther support our claim and show that playing can substantially
improve learning performance without the need for many play
tasks. Our experimental results also show that predicate in-
vention can improve learning performance because it allows
a learner to discover highly reusable sub-programs.

Limitations and future work
Which domains? We have demonstrated the beneﬁts of
playing on robot planning and string transformations. How-
ever, the generality of the approach is unclear. Theorem 1
shows conditions for when playing can reduce sample com-
plexity and helps explain our empirical results. Theorem 1
does not, however, identify a priori on which domains playing
is useful. Our preliminary work suggests that playing is use-
ful in other domains, including to induce graphics programs
where playing allows a learner to discover general concepts
such as a vertical or horizontal line. Future work should de-
termine in which domains playing is useful.

How many play tasks? Our robot experiments show that
as the instance space grows Playgol needs to sample more
tasks to achieve high performance. In future work we want to
develop a theory that predicts how many play tasks Playgol
needs to substantially improve learning performance. In ad-
dition, we assume a suitably large instance space. We do not
know whether the approach would work when such a space is
unavailable.

Better sampling
In the string transformation experiment
playing did not continue to improve performance as it did in

the robot experiment. One explanation for this performance
plateau is the relevancy of sampled play tasks. In future work,
we want to explore methods to sample more useful play tasks.
For instance, rather than create play tasks in an unsupervised
manner, it may be beneﬁcial to create play tasks similar to
build tasks, i.e. in a semi-supervised manner.

Summary We have shown that playing can substantially
improve learning performance. We think that the idea of play-
ing (or more verbosely unsupervised bootstrapping for super-
vised program induction) opens an exciting research area fo-
cusing on how program induction systems can discover their
own BK without the need for user-supplied tasks.

References
[Balog et al., 2017] Matej Balog, Alexander L. Gaunt, Marc
Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.
Deepcoder: Learning to write programs. In ICLR. Open-
Review.net, 2017.

[Bengio et al., 2009] Yoshua Bengio, J´erˆome Louradour, Ro-
nan Collobert, and Jason Weston. Curriculum learning. In
ICML 2019, volume 382, pages 41–48. ACM, 2009.

[Blockeel and Raedt, 1998] Hendrik Blockeel and Luc De
Raedt. Top-down induction of ﬁrst-order logical decision
trees. Artif. Intell., 101(1-2):285–297, 1998.

[Blumer et al., 1989] Anselm Blumer, Andrzej Ehrenfeucht,
David Haussler, and Manfred K. Warmuth. Learnabil-
J. ACM,
ity and the vapnik-chervonenkis dimension.
36(4):929–965, 1989.

[Cropper and Muggleton, 2016a] Andrew Cropper

Stephen H. Muggleton.
programs through abstraction and invention.
2016, pages 1418–1424. IJCAI/AAAI Press, 2016.

and
Learning higher-order logic
In IJCAI

[Cropper and Muggleton, 2016b] Andrew

and Stephen H. Muggleton.
https://github.com/metagol/metagol, 2016.

Metagol

Cropper
system.

[Cropper and Muggleton, 2018] Andrew

and
Stephen H. Muggleton. Learning efﬁcient logic programs.
Machine Learning, Apr 2018.

Cropper

[Cropper, 2017] Andrew Cropper. Efﬁciently learning efﬁ-
cient programs. PhD thesis, Imperial College London, UK,
2017.

[Dechter et al., 2013] Eyal Dechter,

Jonathan Malmaud,
Ryan P. Adams, and Joshua B. Tenenbaum. Bootstrap
In IJCAI 2013,
learning via modular concept discovery.
pages 1302–1309. IJCAI/AAAI, 2013.

[Dietterich et al., 2008] Thomas G. Dietterich, Pedro M.
Domingos, Lise Getoor, Stephen Muggleton, and Prasad
Tadepalli. Structured machine learning: the next ten years.
Machine Learning, 73(1):3–23, 2008.

[Dumancic and Blockeel, 2017] Sebastijan Dumancic and
Hendrik Blockeel. Clustering-based relational unsuper-
vised representation learning with an explicit distributed
representation. In Carles Sierra, editor, IJCAI 2017, pages
1631–1637. ijcai.org, 2017.

[Ellis and Gulwani, 2017] Kevin Ellis and Sumit Gulwani.
Learning to learn programs from examples: Going beyond
In IJCAI 2017, pages 1638–1645. ij-
program structure.
cai.org, 2017.

[Ellis et al., 2018a] Kevin Ellis, Lucas Morales, Mathias
Sabl´e-Meyer, Armando Solar-Lezama, and Josh Tenen-
baum. Learning libraries of subroutines for neurally-
In NeurIPS 2018,
guided bayesian program induction.
pages 7816–7826, 2018.

[Ellis et al., 2018b] Kevin Ellis, Daniel Ritchie, Armando
Solar-Lezama, and Josh Tenenbaum. Learning to infer
graphics programs from hand-drawn images. In NeurIPS
2018, pages 6062–6071, 2018.

[Evans and Grefenstette, 2018] Richard Evans and Edward
Grefenstette. Learning explanatory rules from noisy data.
J. Artif. Intell. Res., 61:1–64, 2018.

[Gulwani, 2011] Sumit Gulwani. Automating string process-
ing in spreadsheets using input-output examples. In POPL
2011, pages 317–330, 2011.

[Law et al., 2014] Mark Law, Alessandra Russo, and Krysia
In

Inductive learning of answer set programs.

Broda.
JELIA 2014, pages 311–325, 2014.

[Lenat, 1977] Douglas B. Lenat. Automated theory forma-
tion in mathematics. In Raj Reddy, editor, Proceedings of
the 5th International Joint Conference on Artiﬁcial Intelli-
gence. Cambridge, MA, USA, August 22-25, 1977, pages
833–842. William Kaufmann, 1977.

[Lin et al., 2014] Dianhuan Lin, Eyal Dechter, Kevin Ellis,
Joshua B. Tenenbaum, and Stephen Muggleton. Bias refor-
mulation for one-shot function induction. In ECAI 2014,
pages 525–530, 2014.

[Mitchell, 1997] Tom M. Mitchell. Machine learning. Mc-
Graw Hill series in computer science. McGraw-Hill, 1997.
[Muggleton et al., 2015] Stephen H. Muggleton, Dianhuan
Lin, and Alireza Tamaddoni-Nezhad. Meta-interpretive
learning of higher-order dyadic datalog: predicate inven-
tion revisited. Machine Learning, 100(1):49–73, 2015.
[Raedt, 2008] Luc De Raedt. Logical and relational learn-

ing. Cognitive Technologies. Springer, 2008.

[Sch¨uller and Benz, 2018] Peter Sch¨uller and Mishal Benz.
Best-effort inductive logic programming via ﬁne-grained
cost-based hypothesis generation - the inspire system at
the inductive logic programming competition. Machine
Learning, 107(7):1141–1169, 2018.

[Schulz et al., 2007] Laura E Schulz, Alison Gopnik, and
Clark Glymour. Preschool children learn about causal
structure from conditional interventions. Developmental
science, 10(3):322–332, 2007.

[Sim and Xu, 2017] Zi L Sim and Fei Xu. Learning higher-
order generalizations through free play: Evidence from
2-and 3-year-old children. Developmental psychology,
53(4):642, 2017.

[Sim et al., 2017] Zi Lin Sim, Kuldeep K. Mahal, and Fei
Xu. Is play better than direct instruction? learning about
causal systems through play. In CogSci 2017, 2017.

[Thrun and Pratt, 2012] Sebastian Thrun and Lorien Pratt.
Learning to learn. Springer Science & Business Media,
2012.

[Wikipedia contributors, 2018] Wikipedia

contributors.
Coupon collector’s problem — Wikipedia,
the free
encyclopedia, 2018. [Online; accessed 20-October-2018].

