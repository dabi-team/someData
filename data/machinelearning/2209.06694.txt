2
2
0
2

p
e
S
4
1

]
E
S
.
s
c
[

1
v
4
9
6
6
0
.
9
0
2
2
:
v
i
X
r
a

Cornucopia: A Framework for Feedback Guided Generation of
Binaries
Extended Report

Vidush Singhal
Purdue University
United States
singhav@purdue.edu

Akul Abhilash Pillai
Purdue University
United States
pillai23@purdue.edu

Charitha Saumya
Purdue University
United States
cgusthin@purdue.edu

Milind Kulkarni
Purdue University
United States
milind@purdue.edu

Aravind Machiry
Purdue University
United States
amachiry@purdue.edu

ABSTRACT

Binary analysis is an important capability required for many se-
curity and software engineering applications. Consequently, there
are many binary analysis techniques and tools with varied capabil-
ities. However, testing these tools requires a large, varied binary
dataset with corresponding source-level information. In this paper,
we present Cornucopia, an architecture agnostic automated frame-
work that can generate a plethora of binaries from corresponding
program source by exploiting compiler optimizations and feedback-
guided learning. Our evaluation shows that Cornucopia was able
to generate 309K binaries across four architectures (x86, x64, ARM,
MIPS) with an average of 403 binaries for each program and out-
performs BinTuner [59], a similar technique. Our experiments
revealed issues with the LLVM optimization scheduler resulting
in compiler crashes (âˆ¼300). Our evaluation of four popular binary
analysis tools angr, Ghidra, ida, and radare, using Cornucopia
generated binaries, revealed various issues with these tools. Specif-
ically, we found 263 crashes in angr and one memory corruption
issue in ida. Our differential testing on the analysis results revealed
various semantic bugs in these tools. We also tested machine learn-
ing tools, Asm2Vec, SAFE, and Debin, that claim to capture binary
semantics and show that they perform poorly (e.g., Debin F1 score
dropped to 12.9% from reported 63.1%) on Cornucopia generated
binaries. In summary, our exhaustive evaluation shows that Cor-
nucopia is an effective mechanism to generate binaries for testing
binary analysis techniques effectively.

1 INTRODUCTION

Designing proper binary analysis tools is a challenging task. It re-
quires precisely modeling all the units of the underlying Instruction
Set Architecture (ISA). Even commonly-used, supposedly-robust
tools, such as qemu, have bugs in precisely modeling certain in-
structions [33]. One common approach to designing these tools,
especially static analysis tools, is to perform incremental develop-
ment [5]. Specifically, instead of painstakingly modeling all the
aspects of the underlying ISA, tool developers model only those
instructions and patterns commonly observed in binaries. These
common patterns are highly dependent on which binaries develop-
ers consider. Without evaluating a representative dataset of binaries,
some key patterns may be overlooked, and these tools will thus

be less robust. Similarly, Machine Learning (ML) techniques [82]
used to solve various binary analysis problems also rely on a varied
dataset of binaries for training. For certain security-critical appli-
cations such as malware detection [64], a misprediction (i.e., false
negative) by the corresponding ML model can be disastrous for the
security of the underlying system [40]. In order to mitigate such
issues and to build robust ML models, it is important to ensure that
the training dataset of binaries is sufficiently varied.

Most existing tools to produce binary datasets [1, 76] use binaries
generated using standard optimization flags (i.e., O0, O1, O2, O3, Os,
Ofast). Unfortunately, the binaries generated using standard opti-
mization flags frequently miss common idioms [14]. Consequently,
analysis tools developed based on these datasets fail to handle cer-
tain idioms, resulting in tool failures, as evident from a large number
of issues in angr [6, 63] and radare [18, 58]. These tools enable [55]
important security and software maintenance applications such as
Control Flow Integrity (CFI) [75], Automated Patching [37], and
Binary Rewriting [2, 78]. Failures in these tools impact their usabil-
ity and delay research progress. Unfortunately, irrespective of an
active open source community, academic researchers spend con-
siderable time fixing various robustness issues in these tools [19].
Similarly, ML tools trained using binaries generated from standard
optimization flags (-Ox) are shown to perform poorly on binaries
compiled with non-standard optimization flags [59].

We wish to automatically generate well-formed binaries so that
binary analysis and ML tools can use the generated datasets to
improve their robustness. The generated binaries should have as-
sociated high-level structures, specifically source code, to enable
the creation of ground-truth information (i.e., through debug sym-
bols) needed by machine learning tools. Existing binary-level tech-
niques [13, 28, 68, 74, 79] use semantics-preserving transforma-
tions (e.g., Register Swapping) to generate several semantically-
equivalent binaries from a single binary. These techniques are pri-
marily designed for program obfuscation and are based on fixed
patterns. Consequently, the number of variants generated for a
given binary is limited. Second, these techniques depend on the
ability to perform static binary rewriting and reassemblable dis-
assembly, which is known to be a hard problem [78]. Third, as
mentioned before, we need to have source code or ground truth
information corresponding to the generated binaries. However,

 
 
 
 
 
 
ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

generating source code for a given arbitrary binary (i.e., decom-
pilation) is known to be a hard problem [71]. Finally, generating
semantics preserving transformations requires a precise model of
the underlying ISA, which requires a considerable amount of ef-
fort [9, 23]. For instance, even a simple register swapping/renaming
transformation, such as renaming register RCX to RDX in a func-
tion, requires knowledge of the ABI. Specifically, we need to know
that the function does not use RCX or RDX for its arguments. To
determine this, we need to know the number and type (scalar or
not) of parameters [17] for the function and the calling convention
used by the function. Both are known to be challenging [31].

Another class of techniques performs semantics-preserving trans-
formations, but at the source level (e.g., tigress [21]) or IR level
(e.g., ob-llvm [38]). These techniques focus on ISA-agnostic control
flow and data flow related aspects of the program without consid-
ering the ISA-dependent instruction sequence or patterns used in
the resulting binary. Consequently, these techniques are shown to
have less or no impact on the generated binary [50]. Table 1 shows
a summary of the existing techniques along with their drawbacks.
In this paper, we focus on the problem of generating large num-
bers of binaries for a given program. We aim to develop a tool that
binary analysis framework developers can easily use to test their
framework effectively. Furthermore, We want to have ground truth
information (i.e., source code and debug information) for all the gen-
erated binaries. We observe that compilers have these precise mod-
els of ISA as part of their target code generation component [65].
Most compilers provide various options and target-(in)dependent
optimization flags that allow fine-grained control over choices in
code generation [60]. Our basic idea is to use these fine-grained op-
timization flags to generate different binaries. However, for a given
program, not all optimization flags affect the programâ€™s binary. For
instance, the flag â€“x86-use-base-pointer available in clang does
not affect programs with small local variables. Although individ-
ual flags may be ineffective for certain programs, combinations of
the flags could generate different binaries [14]. For a given pro-
gram, identifying which flag combinations affect the target binary
is a combinatorial problemâ€”intractable, especially when there are
a large and growing number of flags (âˆ¼ 892 usable flags for x86
in clang-12.0).

In fact, we tried the brute-force approach of enumerating all the
combinations of compiler to compile programs of different sizes. In
12 hours, on average, the brute-force approach was able to generate
197 unique binaries, whereas our approach was able to generate
6,512 (33Ã—) in just 6 hours (half the time).

We present Cornucopia, an automated, architecture-independent
framework for generating a plethora of binaries for a given program.
Given a source package (e.g., a2ps.tar.gz), compiler, and set of all
available optimization flags, Cornucopia iteratively learns to pro-
duce unique binaries for a given source package by feedback-guided
mutation of compiler flags, thus avoiding enumerating all combi-
nations of optimization flags. A recent work, BinTuner [59], also
explores the use of compiler flags to generate different variations
of binaries for a given program. Although it uses a search-based
iterative compilation, BinTunerâ€™s goal is not to generate diverse
binaries but to generate a binary very different from those gener-
ated by general Ox optimization levels. Furthermore, it requires
explicit specification of conflicting compilation options in the form

of first-order formulas, which must be specified for every ISA and
compiler combination. This requires an in-depth understanding
of various compiler options, which involves considerable effort
and conflicts with our requirement for an easy-to-use tool. Finally,
as shown in Section 5.3, BinTunerâ€™s fitness function is inferior
to Cornucopia for generating a plethora of diverse binaries. The
latter generated 8X more binaries than BinTuner in a given time.
Our evaluation shows that Cornucopia, in 6 hours, can generate,
on average, 403 binaries per program across all architectures. In
addition, standard tools for evaluating binary differences show that
these binary variants are highly varied (14) .

Generating a large number of binary variants is only useful if
those variants expose interesting behaviors in the software toolchain.
The binaries generated by Cornucopia revealed various issues
in current static analysis and ML tools, showing the inadequacy
of the current methods to make these tools robust. This shows
that Cornucopia generates binaries that can be used to improve
the robustness of binary analysis tools. Additionally, we observed
that Cornucopia can also be used to test the optimization scheduler
in compilers to find issues related to optimization dependencies [67].
We found issues with the LLVM optimization scheduler which re-
sulted in compiler crashes âˆ¼300. In summary, the following are our
contributions:

â€¢ We present Cornucopia, a feedback-guided mutation tech-
nique to efficiently find sets of compiler optimization flags
that produce different binaries for a given application and
show that it outperforms BinTuner (Section 5.3), a recent
approach that tries to find optimization flags resulting in a
large binary code difference.

â€¢ Our evaluation shows that Cornucopia generates a large
number of unique binaries for a given program, and these
binaries differ significantly from those generated using stan-
dard optimization levels (Section 5.3.2).

â€¢ Our evaluation of existing binary analysis tools and machine
learning tools with Cornucopia generated binaries revealed
various robustness issues (i.e., 263 crashes in angr and one
in ida), semantic issues (Section 5.4), and model performance
issues (Section 5.5), demonstrating the utility of Cornucopia
in testing existing tools.

â€¢ We show that Cornucopia is also effective at testing op-
timization schedulers in compilers by finding issues with
the LLVM optimization scheduler which resulted in âˆ¼300
compiler crashes .

â€¢ The source code and generated binaries of Cornucopia is
available at https://doi.org/10.5281/zenodo.7039858. Refer
to our website for more details: https://binarygeneration.
github.io/

2 OVERVIEW

This section presents an overview of Cornucopia, as shown in Fig-
ure 1. The core technique of Cornucopia is the identification of
the set of compiler flags that affect the binary generated from the
given source. Cornucopia starts with the program source package
ğ‘†, a compiler ğ¶, the list of all flags ğ‘‚ supported by the compiler,
and an initial |ğ‘‚ | (i.e., total number of flags) bytes of random data,
used as an initial input.

Cornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Technique

Binaries Have
corresponding
source code? 1

ISA Specification
Not Needed?

Reassemblable
Disassembly
Not Needed?

Affects
Generated
Binaries?

Generates
Large Number
of Binaries?

Compilation Through Standard
Optimization levels [1, 66, 76]
(e.g., O0, O1, O3, etc)
Source Level
Transformations [21, 73]
(e.g., CFG flattening [53])
IR level
Transformations [38, 69]
(e.g., Deadcode Insertion [12])
Binary Level
Transformations [13, 28, 68, 74, 79]
(e.g., Register renaming [26])
Cornucopia (Our System)

âœ“

âœ“

âœ—

âœ—

âœ“

âœ“

âœ“

âœ“

âœ—

âœ“

âœ“

âœ“

âœ“

âœ—

âœ“

âœ“

â

â

âœ“

âœ“

âœ—

â

â

âœ“

âœ“

Table 1: Comparison of Cornucopia to other binary generation techniques. For each of the feature, we indicate whether the
technique fully supports (âœ“), partially supports (â), or does not support (âœ—) it.

1 As mentioned in Section 1, this is needed to generate ground truth.

and sent as feedback to our collector. The collector checks if the
feedback value is greater than 0. If yes, it saves the corresponding
input (generated by the mutator) into a weighted list of interesting
inputs (inputs that yield differing binaries).

In the next iteration, the mutator again picks an input from the
list of interesting inputs, such that the probability of picking an
input is proportional to its feedback value. (This weighted sample
means that inputs corresponding to compiler flags that created
more varied binaries are preferred.) The selected input is mutated
and sent to the binary generator, and the process continues. All the
generated binaries will be saved into the database, and similar to
the random testing process, the user can stop Cornucopia when
she is satisfied with the generated binaries.

3 DESIGN

The design of Cornucopia is built around the way fuzzing frame-
works work, which expect to execute an â€œinputâ€ on a â€œprogramâ€,
generate an output, and from that output use a fitness function to
decide whether or how to perturb the input. Crucially, in our setting,
the â€œprogramâ€ we are fuzzing is the combination of a compiler plus
a program to be compiled (e.g., LLVM plus objdump). The input is
the compiler flags. Section 3.1 describes how an input is mapped
to compiler flags, and thence to generating an output. Section 3.2
describes how we design our fitness function.

3.1 Binary Generator

The binary generator maps input bytes to compilation flags and
uses these flags to compile a given source package to get a set of
binaries.

Mapping bytes to compiler flags: For most of the flags, we
map each input byte to a compiler flag. The corresponding byte
value indicates whether the option is selected or not. However,
directly using the byte value will result in unnecessary bias. For in-
stance, consider that we enable an option by just checking whether
the value of the byte is greater than 0. There is a 99% (or 255/256)
chance that the option is enabled, whereas there is only a 1%
(or 1/256) chance that the option will be disabled. To avoid this

Figure 1: Overview of Cornucopia.

Cornucopia uses feedback-guided mutation to select compiler
flags that have a high probability of changing the structure of the bi-
nary, as determined by a configurable fitness function. The mutator
takes one of the interesting inputs (initially random data), mutates
it, and sends it to the binary generator. The binary generator uses
the data to select a certain subset of compiler flags ğ‘œğ‘– âŠ† ğ‘‚. (In other
words, the input is used as a seed to select which compiler flags are
used.) These selected compiler flags are used to compile ğ‘† with ğ¶
to get a set of binaries ğ‘. Note that each source package can result
in multiple binaries. For instance, compiling binutils.tar.gz
package results in 19 binaries, such as objdump, nm, etc. The gener-
ated binaries ğµ are sent to a fitness checker, which checks if these
binaries are different from previously seen binaries and stores the
newly-seen binaries, ğµğ‘›ğ‘’ğ‘¤ âŠ† ğµ, into a database. Cornucopiaâ€™s
fitness checker measures how different the binaries in ğµğ‘›ğ‘’ğ‘¤ are
from all the previously seen binaries from the same source package.
The measure of difference is converted into a floating-point number

MutatorInterestingInputsCollectorBinaryGeneratorCompilerProgramsource FitnessCheckerCollectedBinariesOptimization  flagsCornucopiaFeedbackInitial RandomDataASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

bias, we use a modulus operation. Specifically, we compute byte_-
value mod 2 and enable the flag if the resulting value is 1. Sim-
ilarly, for flags that expect a value from a fixed list, we use mod-
ulus to select a value uniformly from that list. For instance, for â€“
frame-pointer=<value>, the <value> can be either all, non-leaf,
or none. We use byte_value mod 4 and enable the flag if the result-
ing value is greater than 0 and the <value> can be either all, non-
leaf, or none depending on whether the modulus result is 1 2 or 3
respectively.

For flags that take raw integers, we use 2 bytes, where the first
byte (mod 2) indicates whether the option is enabled, and if enabled,
the second byte is the value for the flag. For instance, we map 2 bytes
to the flag â€“stack-alignment=<uint>. The flag is selected when
the first_byte_value mod 2 is 1 and the second byte is passed
for <uint>, i.e., â€“stack-alignment=<second_byte_value>.

We will ignore additional bytes if the input has more bytes than
all the compiler flags. Similarly, we will not select the corresponding
flags if the input has fewer bytes.

Compiling using the selected flags: We use a dynamic ap-
proach by hooking into the build process and dynamically modify-
ing every compiler invocation to include only the selected flags. For
instance, consider that our target compiler is clang and selected
options are â€“addrsig and â€“tailcallopt. Our dynamic hook will
replace every compiler invocation, say gcc -O2 <source file(s)>,
with clang â€“addrsig â€“tailcallopt <source file(s)>. We also inclu-
de all the preprocessor directives (e.g., -D..) and linker flags that
were part of the original compiler invocation.

Handling conflicting flags: The compiler flags can have con-
straints, including adverse interactions and dependency relation-
ships. Few flags can negatively influence each other, and turning
them on together leads to a compilation error. Some other flags may
only work when another flag is specified. For example, -ftree-slp-
vectorize may not have an effect when loop unrolling is disabled
because SLP vectorizer may not have opportunities to vectorize the
loop body if the loop is not unrolled.

Automatically identifying conflicting compiler flags is a com-
binatorial problem i.e., requires enumerating all the possible flag
combinations, which is intractable when there are a large num-
ber of growing flags (âˆ¼ 892 for x86 in clang-12.0). On the other
hand, manually specifying conflicting flags for each compiler, as
in BinTuner, requires considerable effort. We use a feedback-driven
approach to handle this. Specifically, if the compilation fails with
selected flags, we compile using a default set of predefined flags
(e.g., -O0) and generate corresponding binaries. Since the selec-
tion of any conflicting flags results in the same binary (i.e., the
one built with default flags), the fitness function (Section 3.2) will
return a score of zero for these binaries. The zero score will cause
the corresponding input to be discarded by our collector, thereby
steering Cornucopia away from generating inputs that result in
conflicting compiler flags.

3.2 Fitness Checker

The goal of the fitness checker is to compute how different the
provided binaries are from all the previously generated binaries
from the same source package. We call this result Difference Score
(DScore).

The score computation mechanism should be efficient. Other-
wise, it will become a performance bottleneck, and then the overall
cost will increase drastically. Existing binary diffing techniques,
such as BinDiff [25], require disassembling the binary and per-
forming lightweight analysis, increasing their execution time. For
instance, BinDiff takes âˆ¼5 min for a medium-sized binary.

There are well-known techniques in malware signature research
that use heuristic methods to compute the similarity between two
binaries. We explore two such techniques and propose a custom
difference score based on the percentage of unique functions. In
all of these techniques, the computed DScore is a floating-point
number ranging from 0.0 to 1.0, where a larger value indicates a
bigger difference. As an optimization, before computing DScore,
we check if the binary is not unique i.e., if we have already seen
the exact binary, then we immediately return 0. If the provided
binaries are unique i.e., DScore is greater than 0, the fitness checker
also stores these binaries in a database. We explore the following
techniques to compute the DScore of a given binary.

Piecewise Hashing: Piecewise hashing or fuzzy hashing [41]
is a well-known technique to compare binaries. The comparison
of fuzzy hashes results in a value ranging from 0.0 to 1.0 (the
higher, the more different). We explore two approaches to com-
pute the DScore based on the piecewise hashing.

Piecewise average (ğ‘ƒğ‘): Here, we compute the difference in the
piecewise hash of the given binary with all the previously seen
binaries. The final DScore is the average of all the hash difference
scores. The intuition behind the average is to compute a score that
captures how different the current binary is when compared to all
the previously seen binaries.

Piecewise minimum (ğ‘ƒğ‘š): This technique is similar to the av-
erage one above. However, we select the minimum hash difference
value instead of the average as the final DScore. The intuition be-
hind the minimum is to prioritize the generation of binaries that
differ largely from all the previously seen binaries. If we consider
the binary generation as a graph traversal, the average strategy
can be considered as a Breadth-First traversal, whereas the min-
imum strategy is a Depth-First traversal. We do not consider the
maximum value because it unnecessarily prioritizes generating the
same kind of binaries. But, the goal of Cornucopia is to maximize
the generation of different binaries. For instance, consider a new
binary ğ‘ with piecewise hash similarity of 0.9, 0.1, and 0.4 against
binaries ğ‘¥, ğ‘¦ and, ğ‘§, respectively. Using the maximum value would
return 0.9 as the DScore, thus maximizing the generation of bina-
ries similar to ğ‘. However, the hash difference value 0.1 indicates
that binary ğ‘ is very similar to ğ‘¦. Hence, using the maximum value
may unnecessarily prioritize the generation of similar binaries and
decrease the overall variety of binaries.

Normalized Compression Distance: Normalized Compres-
sion Distance (NCD) is another well-known technique to compute
difference based on an information-theoretic measure [3]. Specifi-
cally, NCD infers the degree of similarity between arbitrary byte
sequences by the amount of space saved after compression. Pre-
vious works [59] which use NCD have shown to be effective at
capturing the difference between two arbitrary byte sequences.
NCD score ranges from 0.0 to 1.0 (the higher, the more different).
Similar to Piecewise hashing (Section 3.2), we define NCD average
(ğ‘ğ‘) and NCD minimum (ğ‘ğ‘š).

Cornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Percentage of Unique Functions (ğ¹â„): Here, we compute the
difference score as the percentage of unique functions in the pro-
vided binary. We determine unique functions as follows: For each
function, we compute function hash, which is the hash of the bi-
nary code of the function. We use this function hash to see if any
previously seen binaries have a function with the same hash. If not,
the function is considered unique. Finally, the DScore is computed
as the percentage of unique functions over the total number of
functions in the binary. The intuition here is to use function level
similarity rather than byte-sequences based similarity techniques
as used in the previous two approaches.

3.3 Collector and Mutator
The collector receives the feedback (i.e., DScore) for each input and
stores the input in a weighted list according to the value of the
score. The collector discards inputs with a feedback score of 0. The
weighted list is organized such that the probability of selecting an
element from the list is proportional to its feedback score.

The mutator selects one or more inputs from the weighted list
and performs various mutations on the bytes of the inputs. We use
mutation strategies, such as bit flips, byte flips, and splicing, that
are shown to be effective in fuzz testing [45].

4 IMPLEMENTATION

In this section, we present the implementation details of various
components of Cornucopia. Our implementation is agnostic to
the compiler and ISA of the target binary. Furthermore, our system
is modular and extensible.

AFL++ modifications: We modified AFL++ to implement col-
lector and mutator. This simplified our implementation, as AFL++
already has various mutation strategies and uses techniques, such
as fork server, to achieve very high execution rates. We also con-
figured the master/slave mode of AFL++ to implement the parallel
mode for Cornucopia, where multiple instances generate binaries
for a given program by sharing interesting inputs. We implemented
our collector by modifying AFL++â€™s coverage computation logic.
Specifically, instead of using the coverage bitmap [85], we mod-
ify AFL++ to use the DScore to determine whether the input is
interesting. We use a predefined region in the coverage shared
memory [85] to communicate the DScore. All interesting inputs
will be stored in a weighted list sorted according to their DScore.
Binary generator as custom mutator: The important compo-
nent of the binary generator is hooking into the build system, which
we implement by modifying Whole Program LLVM (WLLVM) [70],
which provides python-based compiler wrappers. These wrappers
provide programmable hooks to modify all compiler invocations.
We implemented our binary generator as a C++ program that maps
a given sequence of bytes to a set of compiler flags and selects the
appropriate flags according to the logic as explained in Section 3.1.
Next, we use these selected flags to invoke our modified WLLVM,
which will modify all compiler invocations to use the selected flags.
If any of the compiler invocations fail because of conflicting op-
tions (Section 3.1), we rerun the compilation using a default set of
predefined flags (i.e., -O0).

We use the custom mutator support [49] of AFL++ to integrate
our binary generator. Specifically, every input generated by AFL++

will be sent to our binary generator (i.e., custom mutator). Our
binary generator, as mentioned above, maps the input bytes to
appropriate compiler flags and generates a binary using these flags.
The generated binary will be returned to AFL++, which will forward
it to our fitness checker.

Optimization for LLVM: We have an optimized mode for
LLVM, where we use bitcode files generated using O0 (i.e., no
optimizations) instead of compiling from source code every time.
We run the optimizations using LLVM Bitcode Compiler (llc), a
bitcode compiler. We do an initial pass for a given source package of
getting bitcode files (instead of binaries) using O0. Subsequent calls
to the binary generator will pass the selected flags to run llc using
bitcode files. As we show in Section 5.3.2, this greatly improves the
overall yield as we avoid the unnecessarily step of going through the
compiler frontend every time for the same source package. For other
compilers that do not have an externally accessible intermediate
representation, such as gcc, we take the usual route of running the
whole compiler using WLLVM.

Fitness checker as a service: We split the fitness checker into
two components (i) Proxy program (ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘¦ğ‘ ) and (ii) Fitness checker
server (ğ¹ğ‘–ğ‘¡ğ‘†ğ‘’ğ‘Ÿ ). Splitting the fitness checker provides flexibility in
developing custom fitness checking functions. Users interested in
developing custom fitness checking functions need not worry about
the internals of Cornucopia and just need to expose an interface
that accepts a binary and returns a difference score.

The proxy program (ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘¦ğ‘ ) is written in C and is the main
program to which AFL++ passes the binary (provided by our binary
generator). The Fitness checker server (ğ¹ğ‘–ğ‘¡ğ‘†ğ‘’ğ‘Ÿ ) is a server program
that encapsulates the computation of the DScore. The ğ¹ğ‘–ğ‘¡ğ‘†ğ‘’ğ‘Ÿ ex-
poses a REST interface, which accepts a binary and returns the
difference score by using one of the configured techniques (Sec-
tion 3.2). It also stores the provided binary (if unique) into a database.
We only focus on the changes in the code (i.e., .text section) region of
the binary. In other words, we consider two binaries (i.e., ELF files)
as different if and only if they differ in their .text section.

For piecewise hashing, we use ssdeep python module. For NCD,
we use lzma [30] as the underlying compression algorithm, as it is
shown to be a good candidate [16].

We start our modified AFL++ by passing ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘¦ğ‘ as the target
program to be fuzz tested. Our process starts by AFL++ sending
the generated binary to ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘¦ğ‘ , which sends the binary to ğ¹ğ‘–ğ‘¡ğ‘†ğ‘’ğ‘Ÿ
and receives a DScore. The ğ‘ƒğ‘Ÿğ‘œğ‘¥ğ‘¦ğ‘ will store the DScore into the
preconfigured region of the shared memory and exits. As mentioned
above, AFL++ retrieves the DScore from the shared memory and
continues generating its next input.

Extensibility: Our implementation of Cornucopia is exten-
sible and can be easily configured to use a new compiler, source
package, and custom fitness checking functions. We provide the
exact details in Appendix D.

5 EVALUATION

We evaluate Cornucopia to demonstrate its effectiveness in gen-
erating binaries and their ability to test the robustness of various
binary analysis tools. We pose the following research questions to
guide our evaluation:

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

RQ1: Effectiveness: How effective is Cornucopia in generating
binaries, and how do different fitness metrics affect the quality and
quantity of the generated binaries?
RQ2: Cornucopia vs. BinTuner: How effective is Cornucopia
compared to BinTuner, a recent approach that also uses compiler
flags to generate binaries?
RQ3: Applicability to test static analysis tools: How effective
is the dataset generated by Cornucopia in testing binary static
analysis tools?
RQ4: Applicability to test ML tools: How effective is the dataset
generated by Cornucopia in tesing ML tools?

5.1 Setup
5.1.1 Dataset and Compiler. We choose clang (or LLVM) version
12 as our target compiler, which is the latest and most stable ver-
sion available during our experimentation. Our binary generator
for clang uses pre-generated LLVM Bitcode files as an optimization
to avoid rerunning frontend for the same sources.

We collected source packages by scrapping official Debian pack-
age repositories, compiled them, and randomly selected 191 bitcode
files for each of the four popular architectures,i.e., x86, x64, ARM,
and MIPS. We will refer to individual binaries or bitcode files as
programs. Table 2 shows the number of programs selected and
available optimization flags in clang for each architecture. Note
that the number of programs is limited by resource constraints;
specifically, the availability of machines at our disposal.

5.1.2 Machine Setup and Runtime. We used a server with Intel
Xeon 5215 CPU and ran Cornucopia on each program for 6 hours.
We ensured that each program ran on a processor core and avoided
overloading the server.

5.2 Effectiveness

As explained in Section 3.2, there are various lightweight approaches
to compute the difference score that can guide our mutations. There
is also another approach, as suggested by a recent work [59] where
they take the NCD score of the binary with the binary compiled with
-O0 as the difference score, which we denote as ğ‘ğ‘œ . First, we will
evaluate the relative effectiveness of our approaches ğ‘ƒğ‘, ğ‘ƒğ‘š, ğ‘ğ‘, ğ‘ğ‘š,
and, ğ¹â„ along with ğ‘ğ‘œ .
5.2.1 Effectiveness of different computation approaches. We choose
three programs of different sizes eot2ttf (5.5K, small), lscpu
(270K, medium), and, nab_r (1.1M, large) for this experiment. For
each of these programs, we ran Cornucopia with different ap-
proaches for six hours each. In summary, we had 18 (6 approaches *
3 programs) variations, with each running for six hours. To normal-
ize the effects of randomness, we repeated the whole experiment
eight times. We found that our function hash mechanism (ğ¹â„) re-
sulted in the largest number of unique binaries generated for all
three programs for most of the iterations. The second best technique
is Fuzzy Hashing (PIECEWISE) minimum (ğ‘ƒğ‘š).

To compare the quality of the generated binaries, we computed
the NCD score of each binary against the non-optimized i.e., -O0
compiled) binary. We found that, on average, the binaries generated
by ğ¹â„ have the highest difference score (i.e., more different variants)
of 0.79 compared to all the other fitness functions.

This shows that our ğ¹â„ technique to compute difference score is
both quantitatively (i.e., more unique binaries in a fixed interval of
time) and qualitatively (i.e., more different binaries) more effective at
generating unique binaries when used with Cornucopia.

There are two main reasons for the improved effectiveness of
ğ¹â„: (i) Most of the optimizations in compilers are intraprocedural
and work independently on each function. (ii) Functions within a
program share similar characteristics [52, 54]. For instance, most of
the functions in a string processing library work on strings i.e., char
* type variables. Hence optimization flags that affect a function in a
program most likely also affect other functions in the same program
as these functions share similar characteristics. Our ğ¹â„ approach
exploits this by assigning a higher score to the flag combinations
that affect more functions in the program.

We also ran the experiment by avoiding the precise difference
score but rather using a 1/0 binary feedback, i.e., whether the gen-
erated binary is different (1) or not (0). We observed that all ap-
proaches suffered and generated fewer binaries compared to the
precise difference score versions. This indicates that using a precise
difference score is important for generating large number of unique
binaries. The potential reason is that using a precise score helps
in guiding the search towards more productive flag combinations
while 1/0 will do a random search.
5.2.2 Binary Generation Effectiveness. We use the most effective
difference score approach,i.e., function hash (ğ¹â„), to evaluate the
overall effectiveness of Cornucopia. As mentioned in Section 5.1.2,
we ran Cornucopia for 6 hours for each program-architecture
combination. The summary of the results is shown in Table 2. In
total Cornucopia generated 308,269 unique variants across four
architectures for 191 programs, with an average of 403 and median
of 413 variants per program across all the architectures (The fine-
grained split is discussed in Figure 19 of Appendix ).
Variants across each architecture: Interestingly, as shown in Ta-
ble 2 the number of generated binaries differs across architec-
tures. Specifically, there are âˆ¼15% more binaries in ARM and MIPS,
which have a Reduced Instruction Set Computer (RISC) ISA, com-
pared to x86 and x64, having a Complex Instruction Set Com-
puter (CISC) ISA.

The main reason for this is the difference in the underlying ISA
and corresponding optimization opportunities. There are more
general-purpose registers in ARM and MIPS than x86 and x64,
which increases the compilerâ€™s choices for register allocation. An
example illustration is in one of our binaries as shown in Figure 2,
here compiler choose r12 and r3 in the left version v/s r3 and r4
in the right version, this further caused register spill (line 7 and
17) to occur in the right version. Furthermore, the fixed-length
instructions in ARM and MIPS results in relatively dense basic
blocks, i.e., the average number of instructions in a basic block are
more than in x86 and x64 [15]. This further increases optimization
opportunities.

We evaluated Cornucopia on other aspects and presented the
results in our extended report [7]. Our results show that Cornu-
copia is effective at generating a large number of different binaries
and can explore the variants that are not covered by the standard
optimization levels i.e., O0, O1, O2, and O3.
Size of programs v/s number of binaries generated: We ob-
served that the number of generated binaries follows a gaussian

Cornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Arch (Available Flags) Binaries Avg. Binaries Per Program

x86 (892)
x64 (892)
ARM (876)
MIPS (866)
Grand Total

63,197
74,169
83,701
87,192

308,269

330.87
388.32
438.23
456.50

N/A

Table 2: Performance of Cornucopia: The number of bina-
ries generated for each architecture for 191 programs. Each
program-architecture combination is run for 6 hours. The
number in the parenthesis show the total number of avail-
able optimization flags for that architecture.

Figure 2: Figure showing ARM assembly for 2 different vari-
ants generated from source program cat. Variant A uses reg-
isters (r12, r3) in place of (r3, r4) (Variant B) for the same
function.

distribution w.r.t the program size. This makes sense as larger pro-
grams have more optimization opportunities, hence generating
more binaries. However, compilation time also increases with pro-
gram size, which limits the number of iterations and consequently
results in fewer binaries. We present a detailed analysis along with
corresponding results in Appendix B. .
Quality of the generated binaries: In addition to the many (quan-
tity) binaries, we also want Cornucopia to generate different vari-
eties of binaries (quality). We use Inverse Bindiff (ğµğ·ğ¼ ) scores (i.e., 1
- similarity (bindiff) score) to measure the difference between two
binaries. We performed a detailed analysis of the binaries generated
by Cornucopia against those generated by standard optimization
levels (i.e., O1, O2, and, O3). A detailed analysis of the results is
in Appendix C. Our results show that Cornucopia is effective at
generating a large number of different binaries and can explore the
variants that are not covered by the standard optimization levels i.e.,
O0, O1, O2, and O3.
Scaling Cornucopia with parallel mode: As mentioned in Sec-
tion 4, we engineered Cornucopia to run in parallel mode, where
multiple instances try to generate binaries for a given program by
sharing interesting test cases across all the instances. To evaluate
this, we tested Cornucopia in parallel mode with six instances
on six programs of varying sizes (5.5KB - 209KB). We ran parallel
mode and single instance mode for six hours per program. Figure 25

in Appendix F shows the number of unique binaries generated by
both of these modes across different programs. For each program,
parallel mode generated 5X-41X (with an average of 21X) more
binaries than single instance mode, demonstrating the effective-
ness of parallel mode and scalability of Cornucopia with more
processor cores.

5.2.3 Compiler Crashes. Although unintended, Cornucopia could
be used to test optimization schedulers in compilers. As explained
in Section 3.1, our binary generator repeatedly invokes the com-
piler with different combinations of optimization flags on various
programs. Consequently, while generating binaries for different pro-
grams, Cornucopia is essentially testing optimization schedulers,
although in a blackbox manner. Nonetheless, in our experiments, we
found approx. 300 crashes (i.e., segfaults) in the optimization sched-
uler of clang. An example of one such crash is shown in Listing 20
(Appendix) . We analyzed one of these crashes and identified that
the â€“pre-RA-sched=vliw-td optimization flag is the root cause.
This is not a trivial issue to find because triggering the crash re-
quires specific program structure. We reported all our crashes and
have been acknowledged by the LLVM team as real bugs. They are
currently working on fixing these bugs.
5.2.4 Extending to Different Compilers. Cornucopia is extensible
in accommodating different compilers. We present the results of
extending Cornucopia with gcc in Appendix G.

5.3 Cornucopia vs. BinTuner

As mentioned in Section 1, BinTuner uses a search-based itera-
tive compilation (based on OpenTuner [8]) to find optimization
sequences that can maximize the amount of binary code differ-
ences. BinTuner requires an explicit specification of conflicting
compiler flags in the form of first-order logic formulas, which re-
quires an in-depth understanding of the flags. This process can be
tedious, especially when we need to do this for every architecture
supported by the compiler (i.e., x86, x64, ARM, MIPS, etc) and for
all desired compiler versions. This imposes considerable overhead
for binary analysis tool developers to use BinTuner. Furthermore, the
implementation of BinTuner does not support parallelism, and as
such, BinTuner cannot be used in a multi-processor/multi-threaded
manner to improve its throughput.

However, Cornucopia only requires specifying the compiler
and a corresponding list of supported optimization flags. It does not
require specifying conflicting flags. Our feedback-driven mechanism
(Section 3.1) enables Cornucopia to automatically steer away from
using conflicting flags. The modular design of Cornucopia enables
it to be trivially parallelizable by using multiple mutators, all shar-
ing the same interesting inputs source. As shown in Section 5.2,
running Cornucopia in parallel mode with six instances resulted
in an average of 21X more binaries.

To have an analytical comparison, we perform the following two
experiments on the programs on which BinTuner was evaluated.
Specifically, we use SPECint 2006, Coreutils, and OpenSSL.

5.3.1 Cornucopia with BinTunerâ€™s fitness function (ğ¶ğ‘ ). In this
first experiment, we evaluate the binary generation effectiveness
of BinTunerâ€™s fitness function when used in Cornucopia. Specifi-
cally, as in BinTuner, we use NCD score of the generated binary

1.     .type2. emit_ancillary_info,%function3.     .code 32                           4. emit_ancillary_info:                      5.     ...6.     @ %bb.0:                              7.     push    {r11, lr}                    8.     ...                        9.     bl    printf                           10.     ...               11.     cmp    r1, r12                          12.     moveq    r2, r3                       13.     ...       14.     bl    printf15.     ...                                         16.     mov    sp, r11                          17.     pop    {r11, lr}18.     mov    pc, lr1.      .type2. emit_ancillary_info,%function3.      .code    32                            4.  emit_ancillary_info:5.     ...6.     @ %bb.0:             7.     push    {r4, r10, r11, lr}8.     ...       9.     bl    printf10.     ...                 11.     cmp    r1, r312.     moveq    r2, r413.     ...           14.     bl    printf15.     ...           16.     sub    sp, r11, #8           17.     pop    {r4, r10, r11, lr}        18.     mov    pc, lrVariant AVariant BASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

with its -O0 version as the feedback (i.e., DScore) for the collector
in Cornucopia, denoted as ğ¶ğ‘ .

On average ğ¶ğ‘ generated 52 binaries vs 450 generated by Cor-
nucopia with the function hash score (ğ¹â„). The Figure 7 shows the
results across all the programs (Note that the y-axis is in logarithmic
(base 10) scale). Except for 447.dealII and 483.xalancbmk, Cor-
nucopia generated a large number of binaries, specifically, âˆ¼7X
more than ğ¶ğ‘ . The low yield in 447.dealII and 483.xalancbmk
is because of their large size and the randomness in mutation tech-
niques having less time to explore other effective optimization flag
combinations. The reason for the increased effectivenss of Cor-
nucopia is because BinTunerâ€™s fitness function (NCD with -O0)
maximizes the generation of a highly different binary rather than
generating a large number of diverse binaries. For instance, ğ¶ğ‘ likely
will not generate highly different binaries that have the same NCD
score with -O0.

5.3.2 Binary Generation Effectiveness. For this experiment, we
ran Cornucopia for 6 hours and BinTuner until it converges or 6
hours (whichever is the latest). On average BinTuner generated 48
binaries vs 450 generated by Cornucopia with the function hash
score (ğ¹â„), with Figure 7 showing the results across all the programs.
Except for five programs, Cornucopia was able to generate more
binaries (âˆ¼8X on average) than BinTuner. The low yield for a few
programs is because of their large size and Cornucopia getting less
number of iterations in identifying the optimization flags that are
effective for these binaries. However, BinTuner, based on Open-
Tuner [8], uses more systematic exploratory techniques and can
quickly identify the potent optimization flags. For instance, the
bitcode file for 483.xalancbmk is 13MB in size, and compilation
of it takes âˆ¼ 6 minutes. Consequently, Cornucopia gets less time
to explore different flag combinations and learn which flags are
effective. We confirmed this by running Cornucopia in parallel
mode with six cores and observed that we got considerably more
binaries than BinTuner.

5.3.3 Quality of the Generated Binaries. We used BinDiff scores to
evaluate the quality of binaries generated by different techniques
(BinTuner, ğ¶ğ‘ , and Cornucopia) and Figure 6 shows the cumula-
tive distributive function (CDF) of the scores across all binaries
generated for all programs by each of the corresponding tech-
niques. 1 The score ranges from 0 to 1, and it indicates the amount
of difference (i.e., larger the score higher the difference). First, as
expected, BinTuner was able to generate binaries with the largest
difference (âˆ¼0.95) against its -O0 and -O3 versions. However, its
steeper curve shows little variance, i.e., most of the BinTuner gen-
erated binaries are similar and have high diffence against its -O0
and -O3 versions. The less steep curves of Cornucopia and ğ¶ğ‘
show that they were able to generate more varied binaries, albeit
with a lower difference (âˆ¼0.45) against its -O0 and -O3 versions.

We also compared the best binary (i.e., with the highest Bin-
Diff score) generated by BinTuner with the binaries generated
by Cornucopia. The Figure 5 shows the CDF of the corresponding
score. The steeper curve towards the right indicates that most of
the Cornucopia generated binaries are quite different from those

1A point (x, y) on a line indicates y% of the binaries have their BinDiff score less than
or equal to x.

of BinTunerâ€™s. Specifically, 50% of the binaries have their BinDiff
scores between 0.75-0.95. This shows that Cornucopia is exploring
the binary generation space different from that of BinTuner. In
summary, BinTuner is effective at generating binaries highly differ-
ent from its -O0/ -O3 version, but the generated binaries have less
variance. However, Cornucopia is a complementary approach and
can efficiently generate a large number of binaries with relatively
high variance by exploring different binary generation spaces.

5.4 Applicability to Test Static Analysis Tools
We used four popular binary static analysis tools, i.e., Free and open
source: angr, Ghidra, and radare; Commercial: ida to evaluate
the effectiveness of Cornucopia generated binaries in testing these
tools. We choose analyses that are supported by all these tools.
Specifically, we choose the following:
Function Boundary Detection (FBD) [10]: This analysis gener-
ates a set of function boundaries, where each boundary is a pair of
addresses indicating the address of the first and last instruction of a
function. We got the ground truth information for FBD from debug
information [27] of binaries, specifically, the symbol table [84].
Calling Convention Recovery (CCR): This analysis aims to find
the signature [47] of all functions in the binary. For our experiment,
we only consider the number of parameters. Like FBD, we got the
CCR ground truth for each binary using the debug information
embedded in it.

To test these two analyses, we compare the ground truth of each
binary with the results produced by each tool. For each analysis, we
assigned a fixed time of 24 hours for each architecture, randomly
picked binaries, and tested them with each tool with a timeout of
10 minutes - most of the tools were able to complete within the
timeout except for angr, which timed out for a relatively few large
binaries.

Table 3 shows the result across the selected tools. Here, ğ¹ğ‘ 
indicate the number of binaries with single tool failures, i.e., only
the corresponding tool failed. ğ¹ğ‘š indicate multi-tool failures, i.e.,
two or more tools failed. Finally ğ‘†ğ‘ indicates binaries where all tools
succeeded, i.e., all tools correctly identified function boundaries for
these binaries.

For FBD (Top part of Table 3), on average, all tools correctly
identified boundaries for only 19.97% of the binaries across all ar-
chitectures. Unfortunately, none of the tools correctly identified
function boundaries for 42.15% of the binaries as indicated by the
last row of ğ¹ğ‘š column. For instance, for a binary of fallocate com-
piled for MIPS, with 172 functions, all the tools except Ghidra
failed to precisely detect all the functions. angr overestimated and
detected 182 functions, whereas ida and radare missed several
functions and detected 158 and 100 functions, respectively. radare
performs worst by failing on most binaries across all architectures.
angr performed relatively well on x86 and x64, confirming previ-
ous studies [55]. However, across all architectures, ida performs
better on average. For function boundary detection, angr performs
relatively well for all architectures except for MIPS, for which ida
performs exceptionally well.

For CCR and Control Flow Graph analysis (explained next), in
order to have a uniform comparison, we selected those functions
whose boundaries are correctly identified by all the tools. Unlike

Cornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

100

90

80

70

60

50

40

30

20

10

s
e
i
r
a
n
i
B
f
o
e
g
a
t
n
e
c
r
e
P

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

BinDiff score

BinTuner

Cornucopia

ğ¶ğ‘

BinDiff score

BinTuner

Cornucopia

ğ¶ğ‘

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

BinDiff score

Cornucopia

Figure 3: Against O0 Binary

Figure 4: Against O3 Binary

Figure 5: Against best BinTuner bi-
nary

Figure 6: CDF of Bindiff scores of generated binaries by each of the techniques against O0, O3 and best bintuner binary.

Figure 7: Number of binaries generated by each technique for OpenSSL, SPEC 2006, and Coreutils programs. The horizontal
lines represent average number of binaries per-program generated by each technique.

FBD, results are more uniform for CCR (Middle part of Table 3).
Here, all tools except radare have relatively the same number of
single tool failures (0.3% - 3%). These single tool failures reveal
interesting issues with these tools. Even the highly rated IDA Pro
Decompiler (HexRays) fails to identify the following signature of
the function "make_timespec" in a binary of the sleep program
in coreutils.

"make_timespec (time_t s, long int ns)"

Whereas all the other tools correctly detect two parameters. The
large amount of multi-tool failures (ğ¹ğ‘š: 49.70%) indicates that all
the tools fail to accurately detect the calling convention for a large
number of functions. Overall, Ghidra seems to perform relatively
well in accurately identifying calling convention (i.e., number of
function parameters).

All tools perform equally for calling convention analysis across
all architectures. However, Ghidra performs marginally well com-
pared to other tools.
Control Flow Graph (CFG) Recovery: This analysis aims to
find control flow graphs [81] of all the functions in a binary. These
graphs contain nodes, commonly called basic blocks, and the edges
represent possible control flows in the corresponding function.
Generating ground truth CFG is tricky. Either we need to modify

the compiler backend (not generalizable) to emit this information
or use one of the binary analysis tools to build it. However, as we
presented earlier, these tools might have bugs. To handle this, we
perform differential testing by normalizing the CFG of all the tools
to a common format using networkx [32] and comparing them
with each other. The bottom part of Table 3 shows the results. On
average, all tools produce the same or different CFG for 24.28%
and 45.01% of the functions across all architectures. Similar to the
results of the previous analyses radare again performs worse with
20.75% unique failures. Although CFG is such a common analysis,
it is interesting to see the difference in the results produced by
different tools. We manually inspected a few of these differences
and found that most of these are indeed failures. For instance, for a
binary of elfedit compiled for ARM, radare produced a different
result than the rest of the tools. On further inspection, as shown
in Figure 8, we find that radare fails to detect the blocks after the
address 0x14dc4 (left side). In comparison, angr CFG (right side)
accurately detects the blocks after this address.

We have dissected the results further in Table 4. We categorized
the divergence of each tool based on the underlying root causes, i.e.,
Mismatch in the number of basic blocks (N); Number of basic block
matches, but the starting addresses differ (A) or the ending address
or the size of one or more basic blocks differ (S), or the edges do

openssl482.sphinx3453.povray445.gobmk464.h264ref483.xalancbmk429.mcf433.milc447.dealII470.lbm450.soplex401.bzip2456.hmmer400.perlbench462.libquantum471.omnetpp444.namd473.astar458.sjengcoreutilsPrograms100101102103Number of Binaries (log scale)Cornucopia with Bintuner (Cb)BintunerCornucopiaASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

not match (E); Incomplete output (i.e., tool had an internal failure
and did not return any basic blocks) (P) and finally timeout (T).

Here we find that most tools diverge on the number of basic
blocks for a given function, except for radare, in which case most
divergences are due to incomplete output, i.e., tool failures. The
N divergences in ida for x64 are mostly due to failures in tail-call
detection, particularly the reason for almost 50% of these seem to
be stray ud2 instructions after a tail call. Although the number of
N divergences looks significant for angr, further analysis revealed
that approximately 99% of these failures are cases where angr
chooses to merge jumps to the immediate next address with no
other edges into the same basic block. Although this deviates from
the approach the other tools take, it can be considered a design
choice. Nonetheless, all tools also have internal failures while com-
puting CFG, as indicated by the P column - these cases represent
bugs in the underlying tools and can assist developers in fixing
the underlying issues. We are in the process of organizing these
results with appropriate reproducer scripts and reporting them to
the corresponding tool developers.
Summary: The analyzed tools have been previously tested with
binaries generated using standard optimization levels [76]. Our
results indicate that Cornucopia generates binaries that can effec-
tively reveal issues (missed by regular binaries) in static analysis
tools. Consequently, Cornucopia can be used to supplement the
existing binary datasets to test and further improve binary static
analysis tools.
Impact: Our results also raise interesting questions about eval-
uating advanced binary analysis techniques based on the above
tools. For instance, Consider OSPREY [87], a recent type inference
technique on binaries based on BDA [88] which uses radare for
disassembly and CFG. Our evaluation shows issues with radare
for function boundary detection and CFG recovery. However, OS-
PREY ignores functions that are missed by radare and perform
comparative evaluation on ida and Ghidra and show that OSPREY
performs better on those functions detected by all these. However,
the functions detected by all these tools, which, as we show in our
evaluation (Section 5.4) is considerably less. This raises questions
about the actual effectiveness of OSPREY as ida and Ghidra may
be better or worse on functions missed by radare. This problem
becomes severe when we compare similar techniques built using
different binary analysis tools. We strongly suggest that binary
analysis research should pay particular attention to comparative
evaluation, especially when using different binary analysis tools.
Tool Crashes. We also found that angr and ida crashed on certain
binaries. Specifically, angr crashed on 263 binaries and ida on one bi-
nary. For angr, the crashes are in their python framework, whereas
for ida, the crash is in the libdwarf library. All our issues have been
reported and acknowledged by corresponding developers. These
issues are being actively fixed.

5.5 Applicability to Test ML tools

In this section, we will explore the second application of testing the
robustness of ML tools on the binaries generated by Cornucopia.
We selected the following two recent tools, as these are open-source
and claim to have high accuracy.

Figure 8: CFGs that show the difference in basic blocks be-
tween radare and angr. For radare, there are no basic blocks
after the address 0x14dc4. Whereas for angr there are basic
blocks after 0x14dc4.

Binary diffing techniques (Asm2Vec [24] and SAFE [51]): These
are representation learning techniques based on neural networks.
They propose a representation of binaries into a vector space such
that binaries will be close. In other words, the distance between the
vector representations of two should be minimal, ideally 0. As in
these papers, we use cosine similarity to measure the difference be-
tween the generated vectors. Specifically, we compute Inverse cosine
similarity (i.e., 1 - cosine similarity) denoted as ğ¶ğ‘†ğ¼ ; a large value
of ğ¶ğ‘†ğ¼ indicates a higher difference. Ideally, we would want the ğ¶ğ‘†ğ¼
to be very low for all the binaries for the same program. However,
this is not the case. We got the pre-trained models for these two
tools and used the corresponding vectors to compute the ğ¶ğ‘†ğ¼ of
the generated binaries. Our results as shown that Cornucopia was
able to generate binaries with higher ğ¶ğ‘†ğ¼ scores than O3 for all the
programs. A detailed analysis of results is shown in Appendix E
. This shows that Cornucopia can generate binaries that cannot
be detected as similar by the existing techniques. We suggest that
these techniques should use Cornucopia to improve their dataset,
which could help in building more accurate models.
Debug information prediction (Debin [34]): This technique
combines two complementary probabilistic models to predict types
of variables in a stripped binary. Their evaluation shows that on
average, Debin has an F1 score of 67%. We used their pre-trained
model and tested its accuracy on each of the binaries generated
by Cornucopia. The Table 5 shows the results of our experiment.
Although Debin uses binaries of different optimization levels to
train their model, it still performs extremely poorly on the binaries
generated by Cornucopia, with F1 score dropping to 12.9% (x86),
18.2% (x64) and, 13.6% (ARM) from the reported 67%. We tried to
use StateFormer [56], a recent learning-based tool to predict types.
However, the pre-built model and the dataset are inaccessible and
did not receive help from the authors as well. Nonetheless, our
results on other ML techniques show that existing approaches to
generate binary datasets are inadequate and Cornucopia can help
to improve existing datasets, consequently helping in creating better
models.

           ...ldr r0, [s]     bl #strlenadds r0, r0, 1adc r1, r1, 0bl fcn.00015b100x14dc4            ...ldr r0, [s]     bl #strlenadds r0, r0, 1adc r1, r1, 0bl fcn.00015b100x14dc4 sub r0, fp, #0x8bl #sub_150e4ldr r0, [sp, #0xc]cmp r0, #0x0beq #0x14df40x14dcc RADAREANGRCornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Arch.

x86
x64
ARM
MIPS
Total

Arch.

x86
x64
ARM
MIPS
Total

Arch.

x86
x64
ARM
MIPS
Total

Randomly
Sampled
Binaries

4,600
3,516
5,382
4,818
18,316

Total
No. of
Functions*
49,546
80,174
228,107
132,461
490,288

Total
No. of
Functions*

121,108
109,791
105,674
126,179
462,752

Function Boundary Detection

ğ¹ğ‘ 

angr

271 (5.89%)
213 (6.06%)
2,388 (44.37%)
2,744 (56.95%)
5,616 (30.66%)

Ghidra
492 (10.7%)
302 (8.59%)
2,891 (53.72%)
2,149 (44.6%)
5,834 (31.85%)

ida
1,019 (22.15%)
255 (7.25%)
2,872 (53.36%)
679 (14.09%)
4,825 (26.34%)

radare
4,588 (99.74%)
2,349 (66.81%)
3,324 (61.76%)
3,750 (77.83%)
14,011 (76.50%)

ğ¹ğ‘š

ğ‘†ğ‘

1,160 (25.22%)
563 (16.01%)
3,020 (56.11%)
2,977 (61.79%)
7,720 (42.15%)

12 (0.26%)
1,092 (31.06%)
1,485 (27.59%)
1,068 (22.17%)
3,657 (19.97%)

Calling Convention Recovery

ğ¹ğ‘ 

angr
85 (0.17%)
1,732 (2.16%)
13,906 (6.10%)
390 (0.29%)
16,113 (3.29 %)

Ghidra
299 (0.60%)
470 (0.58%)
816 (0.36%)
129 (0.10%)
1,714 (0.35%)

ida

740 (1.49%)
4,521 (5.64%)
5,012 (2.20%)
356 (0.27%)
10,629 (2.17%)

radare

9,162 (18.49%)
16,018 (19.98%)
34,892 (15.30%)
57,729 (43.58%)
117,801 (24.03%)

ğ¹ğ‘š

ğ‘†ğ‘

18,295 (36.92%)
17,695 (22.07%)
141,680 (62.11%)
66,028 (49.85%)
243,698 (49.70%)

20,965 (42.31%)
39,738 (49.56%)
31,801 (13.94%)
7,829 (5.91%)
100,333 (20.46%)

Control Flow Graph Recovery

ğ¹ğ‘ 

angr
10,622 (8.77%)
5,025 (4.58%)
8,182 (7.74%)
18,244 (14.46%)

42,073 (9.09%)

Ghidra
75 (0.06%)
153 (0.14%)
91 (0.09%)
53 (0.04%)
372 (0.08%)

ida
375 (0.31%)
3,108 (2.83%)
79 (0.07%)
64 (0.05%)
3,626 (0.78%)

radare
26,209 (21.64%)
36,303 (33.07%)
16,414 (15.53%)
17,115 (13.56%)
96,041 (20.75%)

ğ¹ğ‘š

ğ‘†ğ‘

48,146 (39.75%)
27,629 (25.17%)
60,534 (57.28%)
71,988 (57.05%)
208,297 (45.01%)

35,681 (29.46%)
37,573 (34.22%)
20,374 (19.28%)
18,712 (14.83%)
112,340 (24.28%)

Table 3: Results of differential testing of various analysis. For function boundary detection and calling convention recovery, ğ¹ğ‘ 
and ğ¹ğ‘š indicate the number of binaries with single tool divergence (i.e., only one tool produces a different result) and multi-
tool divergence (i.e., Multiple tools produce different results), respectively. ğ‘†ğ‘ shows the number of times all the tools perfectly
agreed with each other. For control flow graph recovery, ğ¹ğ‘  , ğ¹ğ‘š indicate the divergence for number of functions ğ‘†ğ‘ indicates the
number of times all tools agree on functions. (*) Functions in the randomly sampled binaries whose boundaries are correctly
identified by all the tools.

Arch.

x86

x64

ARM

MIPS

angr

Ghidra

N

A

10,389

3,863

7,678

18,244

0

0

0

0

S

3

12

0

0

E

10

16

0

0

P

220

1,134

T

0

0

233

271

0

0

N

18

94

6

0

A

0

0

0

0

S

0

7

0

18

E

0

0

0

0

P

57

52

85

35

T

0

0

0

0

N

A

304

2,844

23

46

0

4

0

0

ida

S

10

87

11

7

E

28

173

23

11

P

0

0

22

0

T

33

0

0

0

N

5,091

6,246

8,920

1,989

radare

A

24

84

30

0

S

0

0

45

0

E

0

0

0

1

P

T

21,094

29,973

7,419

15,125

0

0

0

0

Total

0

26

15

1,587

40,174

0
Table 4: Detailed breakdown of the CFG results with single tool divergence. â€™Nâ€™ indicates a mismatch in the number of basic
blocks. â€™Aâ€™ shows cases where the number of basic blocks match, but the starting addresses differ. â€™Sâ€™ indicates cases where the
size(s) of the basic blocks do not match, â€™Eâ€™ indicates cases where the edges do not match, and â€™Pâ€™ indicates cases when the tools
gave incomplete output. â€™Tâ€™ indicates cases for which the tool timed out.

22,246

73,611

3,217

118

138

115

229

271

235

33

45

22

25

1

0

4

0

0

6 LIMITATIONS AND FUTURE WORK

Although, Cornucopia is effective at generating a plethora of bi-
naries. It has the following limitations.
Compiler bugs: We assume that the provided compiler preserves
the semantics of the program in the generated binary. However, this
may not be the case. The compiler may have bugs [67, 83] resulting
in binaries that may not be semantically equivalent, especially those
concerning undefined behavior.

Compiler frontend overhead: Although we mainly use the back-
end or code generation component of a compiler, in the general
case, we unnecessarily run all components of the compiler, includ-
ing its frontend. This adds a lot of overhead [46] as demonstrated
by the relatively low yield by gcc (Section 5.3.2).
Completeness of the Generated Dataset: Cornucopia uses ex-
isting programs to generate diverse binaries, and the completeness
(e.g., instructions covered in the underlying ISA) of the generated

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

from all previously seen variations. The use of a compiler to gen-
erate binaries have been explored before, especially in the area of
software diversity [35, 36, 43, 62]. These techniques only consider
limited, non-performance-impacting transformations. Cornucopia
has no such restrictions and explores all possible variations of the
binary using compiler flags.

Although the effects of non-standard compiler optimizations on
the generated binary have been explored before [14], the recent
work BinTuner [59] is the most closely related to Cornucopia.
However, as explained in Section 1, BinTuner requires consider-
able effort to use as it requires specifying conflicting compiler flags
manually as first-order constraints. Cornucopia is completely auto-
mated and uses a feedback-guided approach to identify conflicting
options automatically. Furthermore, as shown in Section 5.3, Cor-
nucopia is more effective than BinTuner in efficiently generating
diverse binaries. The use of fuzzing, especially AFL++, to gener-
ate a sequence of tokens has been explored before to fuzz inter-
preters [61]. Our approach allows the fuzzer to use its input genera-
tion ability fully, and enables Cornucopia to be easily configurable
to use other fuzzers.

8 CONCLUSIONS

We present Cornucopia, an architecture, compiler agnostic and
automated framework that generates a plethora of diverse binaries
from program source code by using feedback-guided fuzzing. Our
evaluation shows that Cornucopia is generally more effective at
generating diverse binaries for a given program than BinTuner, a
closely related work. It can be scaled on multiple threads for faster
binary generation and better resource utilization. We showed that
many binary analysis frameworks perform poorly on Cornucopia
generated binaries opening up opportunities for more research
in this area. We envision that Cornucopia becomes part of a bi-
nary analysis testing framework and helps in creating more robust
analysis tools.

9 ACKNOWLEDGMENTS

We would like to thank all the anonymous reviewers for their valu-
able feedback and suggestions which helped in making this paper
the best version of itself. This project was supported in part by
the Defense Advanced Research Projects Agency (DARPA) under
contract number N6600120C4031 and National Science Foundation
(NSF) awards CCF-1908504 and CCF-1919197. The U.S. Government
is authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. Any
opinions, findings, conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily reflect
the views of United States Government, National Science Founda-
tion or any agency thereof.

Arch

x86

x64

ARM

Prec

R

O

Rec

R

O

F1

R

O

62.6
63.7
63.1
63.5
74.1
68.8
61.6
66.8
64.2

7.4
11.1
9.3
3.2
24.7
14.2
7.0
14.6
10.7

62.5
63.7
63.1
63.1
73.4
68.3
61.3
68.0
64.7

15.6
33.9
24.0
5.2
47.8
26.7
12.5
24.8
20.3

62.5
63.7
63.1
63.3
73.8
68.6
61.5
67.4
64.5

10.0
15.6
12.9
3.9
31.9
18.2
8.7
17.9
13.6

Name
Type
Overall
Name
Type
Overall
Name
Type
Overall

Table 5: Figure showing Precision (Prec), Recall (Rec),
and, F1 scores for Debin across 3 different architectures: The
columns Reported (R) and Observed (O) show reported and
observed scores on Cornucopia generated binaries.

dataset depends on the features present in the corresponding pro-
grams. For instance, a program that does not use any floating point
variables is unlikely to produce binaries with floating point in-
structions e.g., fcmovb. This can be handled by using programs
from diverse sources, such as Debian Repositories [29], GitHub,
etc. We can also use systematic approaches such as Csmith [83] to
generate C programs with the desired features and then use them
in Cornucopia to generate a complete binary dataset.
Minimizing compiler crashes: Although, as discussed in Sec-
tion 5.2.3, Cornucopia could find compiler crashes, it does not try
to triage (e.g., minimizing options and the target binary) them. We
plan to integrate techniques like Delta Debugging [86] in our future
work to minimize the set of crash-causing compiler flags.

7 RELATED WORK

Program obfuscation [53] is a well-known technique to change a
programâ€™s structure without affecting the underlying functional-
ity. One possible approach to the problem of this paper is to use
various obfuscation techniques [42, 77] to generate semantically
equivalent but structurally different binaries. Many initial tech-
niques [20, 72] are aimed towards source or IR level obfuscation.
tigress [21] is a source-to-source transformer that has various con-
figurable transformations, such as control-flow flattening [44] and
opaque-predicates [22]. Similarly, ob-llvm [38] enables applying a
limited set of transformations at the LLVM IR level. Closure [48]
uses stochastic optimization to select a sequence of transformations
to produce the optimal obfuscation potency. Although these tech-
niques are effective at modifying the program at IR or source code
level, they have less impact on the generated binary [50].

A few binary-level techniques obfuscate control-flow using error
handling semantics such as signals [57] and exception handling [80].
Other virtual machineâ€“based techniques [28, 39] transform the
given binary into a custom virtual machine. These binary-level
techniques are known to introduce performance overhead [4]. The
binary-level techniques are based on a fixed set of carefully designed
patterns [53], which do not capture the entire range of behaviors
of the underlying ISA. Finally, the primary goal of obfuscation
techniques is to generate a hard-to-understand version of a given
program [11]. In contrast, Cornucopia does not care about the
understandability of the generated binary as long as it is different

Cornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

REFERENCES
[1] 2022. How Machine Learning Is Solving the Binary Function Similarity Problem.
In 31st USENIX Security Symposium (USENIX Security 22). USENIX Association,
Boston, MA. https://www.usenix.org/conference/usenixsecurity22/presentation/
marcelli

[2] Ioannis Agadakos, Di Jin, David Williams-King, Vasileios P Kemerlis, and Geor-
gios Portokalidis. 2019. Nibbler: debloating binary shared libraries. In Proceedings
of the 35th Annual Computer Security Applications Conference. 70â€“83.

[3] Nadia Alshahwan, Earl T Barr, David Clark, George Danezis, and HÃ©ctor D
MenÃ©ndez. 2020. Detecting malware with information complexity. Entropy 22, 5
(2020), 575.

[4] Bertrand Anckaert, Matias Madou, Bjorn De Sutter, Bruno De Bus, Koen De Boss-
chere, and Bart Preneel. 2007. Program obfuscation: a quantitative approach. In
Proceedings of the 2007 ACM workshop on Quality of protection. 15â€“20.

[5] Dennis Andriesse. 2018. Practical Binary Analysis: Build Your Own Linux Tools

for Binary Instrumentation, Analysis, and Disassembly. no starch press.

[6] Angr. 2021. Angr Issues. https://github.com/angr/angr/issues?q=is%3Aissue+

error.

[7] Anonymous. 2022. Cornucopia: A Framework for Feedback Guided Generation
of Binaries (extended report). https://drive.google.com/file/d/1bEiI1O4BP8VbD-
vWOiWnidolm9SuELLa/view?usp=sharing.

[8] Jason Ansel, Shoaib Kamil, Kalyan Veeramachaneni, Jonathan Ragan-Kelley,
Jeffrey Bosboom, Una-May Oâ€™Reilly, and Saman Amarasinghe. 2014. Opentuner:
An extensible framework for program autotuning. In Proceedings of the 23rd
international conference on Parallel architectures and compilation. 303â€“316.
[9] Alasdair Armstrong, Thomas Bauereiss, Brian Campbell, Shaked Flur, Kathryn E
Gray, Prashanth Mundkur, Robert M Norton, Christopher Pulte, Alastair Reid,
Peter Sewell, et al. 2018. Detailed models of instruction set architectures: From
pseudocode to formal semantics. In 25th Automated Reasoning Workshop. 23.
[10] Tiffany Bao, Jonathan Burket, Maverick Woo, Rafael Turner, and David Brumley.
2014. {BYTEWEIGHT}: Learning to recognize functions in binary code. In 23rd
{USENIX} Security Symposium ({USENIX} Security 14). 845â€“860.

[11] Boaz Barak, Oded Goldreich, Rusell Impagliazzo, Steven Rudich, Amit Sahai, Salil
Vadhan, and Ke Yang. 2001. On the (im) possibility of obfuscating programs. In
Annual international cryptology conference. Springer, 1â€“18.

[12] Cristian BarrÃ­a, David Cordero, Claudio Cubillos, and Robinson Osses. 2016.
Obfuscation procedure based in dead code insertion into crypter. In 2016 6th
International Conference on Computers Communications and Control (ICCCC).
IEEE, 23â€“29.

[13] Chandan Kumar Behera and D Lalitha Bhaskari. 2015. Different obfuscation

techniques for code protection. Procedia Computer Science 70 (2015), 757â€“763.

[14] Craig Blackmore, Oliver Ray, and Kerstin Eder. 2017. Automatically tuning the
gcc compiler to optimize the performance of applications running on embedded
systems. arXiv preprint arXiv:1703.08228 (2017).

[15] Emily Blem,

Jaikrishnan Menon, Thiruvengadam Vijayaraghavan, and
Karthikeyan Sankaralingam. 2015. ISA wars: Understanding the relevance of ISA
being RISC or CISC to performance, power, and energy on modern architectures.
ACM Transactions on Computer Systems (TOCS) 33, 1 (2015), 1â€“34.

[16] Rebecca Schuller Borbely. 2016. On normalized compression distance and large
malware. Journal of Computer Virology and Hacking Techniques 12, 4 (2016),
235â€“242.

[17] Juan Caballero and Zhiqiang Lin. 2016. Type inference on executables. ACM

Computing Surveys (CSUR) 48, 4 (2016), 1â€“35.

[18] Pan Cake. 2017. Libre and Portable Reverse Engineering Framework. https:

//rada.re/n/.

[19] Chris Casinghino, JT Paasch, Cody Roux, John Altidor, Michael Dixon, and Dustin
Jamner. 2019. Using binary analysis frameworks: The case for BAP and angr. In
NASA Formal Methods Symposium. Springer, 123â€“129.

[20] Mariano Ceccato, Massimiliano Di Penta, Jasvir Nagra, Paolo Falcarin, Filippo
Ricca, Marco Torchiano, and Paolo Tonella. 2009. The effectiveness of source
code obfuscation: An experimental assessment. In 2009 IEEE 17th International
Conference on Program Comprehension. IEEE, 178â€“187.

[21] Christian Collberg. 2021. The tigress c obfuscator. https://tigress.wtf/.
[22] Christian Collberg, Clark Thomborson, and Douglas Low. 1998. Manufacturing
cheap, resilient, and stealthy opaque constructs. In Proceedings of the 25th ACM
SIGPLAN-SIGACT symposium on Principles of programming languages. 184â€“196.
[23] Sandeep Dasgupta, Daejun Park, Theodoros Kasampalis, Vikram S Adve, and
Grigore RoÅŸu. 2019. A complete formal semantics of x86-64 user-level instruc-
tion set architecture. In Proceedings of the 40th ACM SIGPLAN Conference on
Programming Language Design and Implementation. 1133â€“1148.

[24] Steven HH Ding, Benjamin CM Fung, and Philippe Charland. 2019. Asm2vec:
Boosting static representation robustness for binary clone search against code
obfuscation and compiler optimization. In 2019 IEEE Symposium on Security and
Privacy (SP). IEEE, 472â€“489.

[25] Thomas Dullien and Rolf Rolles. 2005. Graph-based comparison of executable

objects (english version). Sstic 5, 1 (2005), 3.

[26] LukÃ¡Å¡ Äurfina and DuÅ¡an KolÃ¡Å™. 2011. Generic detection of register realignment.
In AIP Conference Proceedings, Vol. 1389. American Institute of Physics, 806â€“809.
[27] Michael Eager. 2012. The DWARF Debugging Standard. https://dwarfstd.org/.
[28] Hui Fang, Yongdong Wu, Shuhong Wang, and Yin Huang. 2011. Multi-stage
binary code obfuscation using improved virtual machine. In International Confer-
ence on Information Security. Springer, 168â€“181.

[29] JosÃ© Angel Galindo, David Benavides, and Sergio Segura. 2010. Debian Packages
Repositories as Software Product Line Models. Towards Automated Analysis.. In
ACoTA. Citeseer, 29â€“34.

[30] Edgar N. Gilbert and TT Kadota. 1992. The Lempel-Ziv algorithm and message
complexity. IEEE transactions on information theory 38, 6 (1992), 1839â€“1842.
[31] Frederic Grelot, Sebastien Larinier, and Marie Salmon. 2021. Automation of Binary
Analysis: From Open Source Collection to Threat Intelligence. Proceedings of the
28th C&ESAR (2021), 41.

[32] Aric Hagberg and Drew Conway. 2020. NetworkX: Network Analysis with

Python.

[33] Nadav Harâ€™El. 2017. x86 Floating point exceptions - incorrect support? https:

//bugs.launchpad.net/qemu/+bug/1668041.

[34] Jingxuan He, Pesho Ivanov, Petar Tsankov, Veselin Raychev, and Martin Vechev.
2018. Debin: Predicting debug information in stripped binaries. In Proceedings
of the 2018 ACM SIGSAC Conference on Computer and Communications Security.
1667â€“1680.

[35] Shohreh Hosseinzadeh, Sampsa Rauti, Samuel LaurÃ©n, Jari-Matti MÃ¤kelÃ¤, Jo-
hannes Holvitie, Sami Hyrynsalmi, and Ville LeppÃ¤nen. 2018. Diversifica-
tion and obfuscation techniques for software security: A systematic litera-
Information and Software Technology 104 (2018), 72â€“93. https:
ture review.
//doi.org/10.1016/j.infsof.2018.07.007

[36] Todd Jackson, Babak Salamat, Andrei Homescu, Karthikeyan Manivannan, Gregor
Wagner, Andreas Gal, Stefan Brunthaler, Christian Wimmer, and Michael Franz.
2011. Compiler-generated software diversity. In Moving Target Defense. Springer,
77â€“98.

[37] Haegeon Jeong, Jeanseong Baik, and Kyungtae Kang. 2017. Functional level
hot-patching platform for executable and linkable format binaries. In 2017 IEEE
International Conference on Systems, Man, and Cybernetics (SMC). IEEE, 489â€“494.
[38] Pascal Junod, Julien Rinaldini, Johan Wehrli, and Julie Michielin. 2015. Obfuscator-
LLVMâ€“software protection for the masses. In 2015 IEEE/ACM 1st International
Workshop on Software Protection. IEEE, 3â€“9.

[39] Patrick Kochberger, Sebastian Schrittwieser, Stefan Schweighofer, Peter Kiese-
berg, and Edgar Weippl. 2021. SoK: Automatic Deobfuscation of Virtualization-
protected Applications. In The 16th International Conference on Availability, Reli-
ability and Security. 1â€“15.

[40] Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio
Giacinto, Claudia Eckert, and Fabio Roli. 2018. Adversarial malware binaries:
Evading deep learning for malware detection in executables. In 2018 26th European
signal processing conference (EUSIPCO). IEEE, 533â€“537.

[41] Jesse Kornblum. 2006. Identifying almost identical files using context triggered

piecewise hashing. Digital investigation 3 (2006), 91â€“97.

[42] Pengwei Lan, Pei Wang, Shuai Wang, and Dinghao Wu. 2017. Lambda obfuscation.
In International Conference on Security and Privacy in Communication Systems.
Springer, 206â€“224.

[43] Per Larsen, Andrei Homescu, Stefan Brunthaler, and Michael Franz. 2014. SoK:
Automated software diversity. In 2014 IEEE Symposium on Security and Privacy.
IEEE, 276â€“291.

[44] TÄ±mea LÃ¡szlÃ³ and Ãkos Kiss. 2009. Obfuscating C++ programs via control flow
flattening. Annales Universitatis Scientarum Budapestinensis de Rolando EÃ¶tvÃ¶s
Nominatae, Sectio Computatorica 30, 1 (2009), 3â€“19.

[45] lcamtuf. 2014. Binary fuzzing strategies: what works, what doesnâ€™t. https:

//lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html.

[46] David Xinliang Li, Raksit Ashok, and Robert Hundt. 2010. Lightweight feedback-
directed cross-module optimization. In Proceedings of the 8th annual IEEE/ACM
international symposium on Code generation and optimization. 53â€“61.

[47] Yan Lin and Debin Gao. 2021. When Function Signature Recovery Meets Compiler
Optimization. In 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 36â€“52.
[48] Han Liu, Chengnian Sun, Zhendong Su, Yu Jiang, Ming Gu, and Jiaguang Sun.
2017. Stochastic optimization of program obfuscation. In 2017 IEEE/ACM 39th
International Conference on Software Engineering (ICSE). IEEE, 221â€“231.
[49] llzmb. 2020. Custom Mutators in AFL++. https://github.com/AFLplusplus/

AFLplusplus/blob/stable/docs/custom_mutators.md.

[50] Matias Madou, Bertrand Anckaert, Bruno De Bus, Koen De Bosschere, Jan Cap-
paert, and Bart Preneel. 2006. On the effectiveness of source code transformations
for binary obfuscation. In Proceedings of the International Conference on Software
Engineering Research and Practice (SERP06). CSREA Press, 527â€“533.

[51] Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Roberto Baldoni, and
Leonardo Querzoni. 2019. Safe: Self-attentive function embeddings for binary
similarity. In International Conference on Detection of Intrusions and Malware, and
Vulnerability Assessment. Springer, 309â€“329.

[52] Dongyu Meng, Michele Guerriero, Aravind Machiry, Hojjat Aghakhani, Priyanka
Bose, Andrea Continella, Christopher Kruegel, and Giovanni Vigna. 2021. Bran:

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

Reduce Vulnerability Search Space in Large Open Source Repositories by Learning
Bug Symptoms. In Proceedings of the 2021 ACM Asia Conference on Computer and
Communications Security. 731â€“743.

[53] Jasvir Nagra and Christian Collberg. 2009. Surreptitious Software: Obfuscation,
Watermarking, and Tamperproofing for Software Protection: Obfuscation, Water-
marking, and Tamperproofing for Software Protection. Pearson Education.
[54] Hoan Anh Nguyen, Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen,
and Hridesh Rajan. 2013. A study of repetitiveness of code changes in software
evolution. In 2013 28th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 180â€“190.

[55] Chengbin Pang, Ruotong Yu, Yaohui Chen, Eric Koskinen, Georgios Portokalidis,
Bing Mao, and Jun Xu. 2021. SoK: All You Ever Wanted to Know About x86/x64
Binary Disassembly But Were Afraid to Ask. 2021 IEEE Symposium on Security
and Privacy (SP) (2021), 833â€“851.

[56] Kexin Pei, Jonas Guan, Matthew Broughton, Zhongtian Chen, Songchen Yao,
David Williams-King, Vikas Ummadisetty, Junfeng Yang, Baishakhi Ray, and
Suman Jana. 2021. StateFormer: fine-grained type recovery from binaries using
generative state modeling. In Proceedings of the 29th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering. 690â€“702.

[57] Igor V Popov, Saumya K Debray, and Gregory R Andrews. 2007. Binary Obfusca-

tion Using Signals.. In USENIX Security Symposium. 275â€“290.

[58] randare. 2021. Radare2 Issues. https://github.com/radareorg/radare2/issues?q=

is%3Aissue+error.

[59] Xiaolei Ren, Michael Ho, Jiang Ming, Yu Lei, and Li Li. 2021. Unleashing the
hidden power of compiler optimization on binary code difference: an empirical
study. In Proceedings of the 42nd ACM SIGPLAN International Conference on
Programming Language Design and Implementation. 142â€“157.

[60] Rolf Rolles. 2014.

COMPILER OPTIMIZATIONS FOR REVERSE ENGI-
NEERS. https://www.msreverseengineering.com/blog/2014/6/23/compiler-
optimizations-for-reverse-engineers.

[61] Christopher Salls, Chani Jindal, Jake Corina, Christopher Kruegel, and Gio-
vanni Vigna. 2021. Token-Level Fuzzing. In 30th {USENIX} Security Symposium
({USENIX} Security 21). 2795â€“2809.

[62] Ina Schaefer, Rick Rabiser, Dave Clarke, Lorenzo Bettini, David Benavides, Goetz
Botterweck, Animesh Pathak, Salvador Trujillo, and Karina Villela. 2012. Software
diversity: state of the art and perspectives.

[63] Yan Shoshitaishvili, Ruoyu Wang, Christopher Salls, Nick Stephens, Mario Polino,
Audrey Dutcher, John Grosen, Siji Feng, Christophe Hauser, Christopher Kruegel,
and Giovanni Vigna. 2016. SoK: (State of) The Art of War: Offensive Techniques
in Binary Analysis. In IEEE Symposium on Security and Privacy.

[64] Jagsir Singh and Jaswinder Singh. 2021. A survey on machine learning-based
malware detection in executable files. Journal of Systems Architecture 112 (2021),
101861.

[65] YN Srikant and Priti Shankar. 2018. The compiler design handbook: optimizations

and machine code generation. CRC Press.

[66] JHU/APL Staff. 2019. Assembled Labeled Library for Static Analysis Research

(ALLSTAR) Dataset. http://allstar.jhuapl.edu/

[67] Chengnian Sun, Vu Le, Qirun Zhang, and Zhendong Su. 2016. Toward under-
standing compiler bugs in GCC and LLVM. In Proceedings of the 25th International
Symposium on Software Testing and Analysis. 294â€“305.

[68] Peter Szor and Peter Ferrie. 2001. Hunting for metamorphic. In Virus bulletin

conference. Prague.

[69] Teja Tamboli, Thomas H Austin, and Mark Stamp. 2014. Metamorphic code
generation from LLVM bytecode. Journal of Computer Virology and Hacking
Techniques 10, 3 (2014), 177â€“187.

[70] travitch. 2015. Whole Program LLVM. https://github.com/travitch/whole-

program-llvm.

[71] Freek Verbeek, Pierre Olivier, and Binoy Ravindran. 2020. Sound C Code Decom-
pilation for a subset of x86-64 Binaries. In International Conference on Software
Engineering and Formal Methods. Springer, 247â€“264.

[72] Alessio ViticchiÃ©, Leonardo Regano, Marco Torchiano, Cataldo Basile, Mariano
Ceccato, Paolo Tonella, and Roberto Tiella. 2016. Assessment of source code
obfuscation techniques. In 2016 IEEE 16th international working conference on
source code analysis and manipulation (SCAM). IEEE, 11â€“20.

[73] Alessio ViticchiÃ©, Leonardo Regano, Marco Torchiano, Cataldo Basile, Mariano
Ceccato, Paolo Tonella, and Roberto Tiella. 2016. Assessment of Source Code
Obfuscation Techniques. In 2016 IEEE 16th International Working Conference on
Source Code Analysis and Manipulation (SCAM). 11â€“20. https://doi.org/10.1109/
SCAM.2016.17

[74] Chang Wang, Zhaolong Zhang, Xiaoqi Jia, and Donghai Tian. 2018. Binary Obfus-
cation Based Reassemble. In 2018 13th International Conference on Malicious and
Unwanted Software (MALWARE). 153â€“160. https://doi.org/10.1109/MALWARE.
2018.8659363

[75] Minghua Wang, Heng Yin, Abhishek Vasisht Bhaskar, Purui Su, and Dengguo
Feng. 2015. Binary code continent: Finer-grained control flow integrity for
stripped binaries. In Proceedings of the 31st Annual Computer Security Applications
Conference. 331â€“340.

[76] Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi, Aravind Machiry, John
Grosen, Paul Grosen, Christopher Kruegel, and Giovanni Vigna. 2017. Ram-
blr: Making Reassembly Great Again.. In NDSS.

[77] Yan Wang, Shuai Wang, Pei Wang, and Dinghao Wu. 2017. Turing obfuscation.
In International Conference on Security and Privacy in Communication Systems.
Springer, 225â€“244.

[78] Matthias Wenzl, Georg Merzdovnik, Johanna Ullrich, and Edgar Weippl. 2019.
From hack to elaborate techniqueâ€”a survey on binary rewriting. ACM Computing
Surveys (CSUR) 52, 3 (2019), 1â€“37.

[79] Zhenyu Wu, Steven Gianvecchio, Mengjun Xie, and Haining Wang. 2010. Mim-
imorphism: A new approach to binary code obfuscation. In Proceedings of the
17th ACM conference on Computer and communications security. 536â€“546.
[80] Zhenyu Wu, Steven Gianvecchio, Mengjun Xie, and Haining Wang. 2010. Mim-
imorphism: A New Approach to Binary Code Obfuscation. In Proceedings of
the 17th ACM Conference on Computer and Communications Security (Chicago,
Illinois, USA) (CCS â€™10). Association for Computing Machinery, New York, NY,
USA, 536â€“546. https://doi.org/10.1145/1866307.1866368

[81] Liang Xu, Fangqi Sun, and Zhendong Su. 2009. Constructing precise control flow

graphs from binaries. University of California, Davis, Tech. Rep (2009).

[82] Hongfa Xue, Shaowen Sun, Guru Venkataramani, and Tian Lan. 2019. Machine
learning-based analysis of program binaries: A comprehensive study. IEEE Access
7 (2019), 65889â€“65912.

[83] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and under-
standing bugs in C compilers. In Proceedings of the 32nd ACM SIGPLAN conference
on Programming language design and implementation. 283â€“294.

[84] Eric Youngdale. 1995. Kernel korner: The ELF object file format: Introduction.

Linux Journal 1995, 12es (1995), 7â€“es.

[85] Michal Zalewski. 2020. Shared memory and coverage map. https://github.com/
AFLplusplus/AFLplusplus/blob/stable/docs/technical_details.md#1-coverage-
measurements.

[86] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-
inducing input. IEEE Transactions on Software Engineering 28, 2 (2002), 183â€“200.
[87] Zhuo Zhang, Yapeng Ye, Wei You, Guanhong Tao, Wen-chuan Lee, Yonghwi
Kwon, Yousra Aafer, and Xiangyu Zhang. 2021. OSPREY: Recovery of Variable
and Data Structure via Probabilistic Analysis for Stripped Binary. In 2021 IEEE
Symposium on Security and Privacy (SP). IEEE, 813â€“832.

[88] Zhuo Zhang, Wei You, Guanhong Tao, Guannan Wei, Yonghwi Kwon, and Xi-
angyu Zhang. 2019. BDA: practical dependence analysis for binary executables
by unbiased whole-program path sampling and per-path abstract interpretation.
Proceedings of the ACM on Programming Languages 3, OOPSLA (2019), 1â€“31.

A APPENDIX
B SIZE OF PROGRAMS V/S NUMBER OF

BINARIES GENERATED:

We also want to evaluate how the programâ€™s size affects the number
of binaries generated. We consider the size of O0 bitcode file as the
program size.

Intuitively, large programs should have more optimization op-
portunities, and hence Cornucopia should produce more binaries.
However, large programs have relatively higher compilation times.
As we fix the fuzzing time (6 hours), the large programs have fewer
compilations than smaller programs. Figure 9 shows the Probability
Density Function (PDF) of number of binaries (y-axis) generated
for programs of corresponding size (x-axis). For multiple programs
of the same size, we consider the average number of binaries across
these programs. As we can see, the number of binaries increases
with the program size showing the effect of increased optimization
opportunities for smaller sizes. However, the number of binaries
decreases as the size increases for larger sizes (> 18-21 KB) with
increased compilation time, which reduces the number of compila-
tions resulting in a smaller number of binaries.

C QUALITY OF THE GENERATED BINARIES

(DETAILED ANALYSIS)

In addition to many binaries, we also want Cornucopia to generate
different varieties of binaries. We use Inverse Bindiff (ğµğ·ğ¼ ) scores

Cornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

4,000

3,500

3,000

2,500

2,000

1,500

1,000

500

s
t
n
a
i
r
a
V

f
o
r
e
b
m
u
N

x86
mips

x64

arm

as expected, the median ğµğ·ğ¼ scores also follow the same trend as
average.

This shows that Cornucopia is effective at generating a large
number of different binaries and can explore the variants that are
not covered by the standard optimization levels i.e., O0, O1, O2, and,
O3.

D EXTENSIBILITY

Our implementation of Cornucopia is extensible in the following
three aspects.

7

4
1

1
2

8
2

5
3

2
4

9
4

5
8

5
3
1

5
7
1

5
2
2

Size (KB)

Figure 9: PDF showing percentage of binaries generated for
programs of different sizes across various architecture and
compiler combinations.

(i.e., 1 - similarity (bindiff) score) to measure the variance or quality
of the generated binaries. A large ğµğ·ğ¼ score indicates a higher dif-
ference. The Figure 14 shows the CDF the percentage of programs
against ğµğ·ğ¼ scores. Each line indicates the CDF of ğµğ·ğ¼ scores in
comparison with the binary generated by the corresponding op-
timization option. For instance, the Avg-O0 line in all subfigures
of Figure 14 shows the trend of average ğµğ·ğ¼ scores of all bina-
ries generated for a program against the binary compiled with O0.
Specifically, a point (x, y) on a Avg-O0 line shows the average ğµğ·ğ¼
score (of binaries generated by Cornucopia in comparison with
O0) for y% of the programs is less than or equal to x. Similarly, Med-
O0, Max-O0 show the trend for median and maximum ğµğ·ğ¼ scores.
Furthermore, the *-O3 shows the comparison with O3 (maximum
level) compiled binary. We also show the trend of ğµğ·ğ¼ scores of O0
against other optimization levels in O0 v/s Ox lines.
Maximum ğµğ·ğ¼ scores: The Max-* lines, which are consistently
at the right of O0 v/s O3, indicates that Cornucopia was able to
generate binaries with higher ğµğ·ğ¼ scores than O3 for all the programs.
For instance, for x64 (Figure 11), the maximum ğµğ·ğ¼ score of O0
v/s O3 is 0.4 as indicated by the peak of the line. However, there
are at least 43% (100-57) of programs for which the maximum ğµğ·ğ¼
score of Cornucopia generated binaries ranges from 0.45 - 0.7.
(Inferred from two points (0.45, 57) and (0.7, 100) on the Max - O0
line). Furthermore, as shown by the Max-O3 lines, the binaries
generated by Cornucopia have a consistently higher score than
O3 binaries hinting that we could explore the optimization options
that are not covered in O3.
Average ğµğ·ğ¼ scores: The average ğµğ·ğ¼ scores of Cornucopia gen-
erated binaries for x86 (Figure 10), and x64 (Figure 11) is slightly
lower than that of Avg-O0 v/s O0 v/s Ox variants. This is expected
and good as Cornucopia is able to explore the binary variations
that are not explored by existing optimization levels. It is interest-
ing to see that the average scores are more in the case of ARM
(Figure 12) and MIPS (Figure 13). This is also because of the un-
derlying RISC ISA and the greater optimization it provides. Finally,

â€¢ Adding a new compiler. Users can configure Cornucopia
to use a custom compiler by just changing the compiler en-
vironment variable of the binary generator to the file system
path of the compiler and providing the list of all possible
compiler flags in a file.

â€¢ Adding a custom fitness checking function. As men-
tioned before, users interested in developing custom fitness
checking functions can modify our existing python based
ğ¹ğ‘–ğ‘¡ğ‘†ğ‘’ğ‘Ÿ or need to expose an interface that accepts a binary
and returns a DScore (a floating-point number).

â€¢ Adding a new source package. We support all packages
that can be built using configure and make (or cmake).
Users just need to provide the path to the package (i.e.,
.tar.gz).

E TESTING ML BASED BINARY DIFFING

TECHNIQUES

Asm2Vec supports only x64, whereas as SAFE supports both x64
and ARM. These tools claim that their model can detect semantically
equivalence, such that binaries compiled with different optimiza-
tion levels (i.e., Ox) will be very similar or have a low ğ¶ğ‘†ğ¼ score.
We got the pre-trained models for these two tools and used the
corresponding vectors to compute the ğ¶ğ‘†ğ¼ of the generated binaries.
The Figure 24 shows the CDF of the average (Avg-*), median (Med-
*), and maximum (Max-*) ğ¶ğ‘†ğ¼ score for all the binaries generated for
each program against the binary generated with the corresponding
optimization (Similar to Section 5.3.2).

Both the tools claim that the comparison of O0 with O3 is tough-
est, i.e., should have the highest score. However, as shown in Fig-
ure 24 by the Max-* lines, which are consistently at the right of
O0 v/s O3, indicates that Cornucopia was able to generate binaries
with higher ğ¶ğ‘†ğ¼ scores than O3 for all the programs. The trend is
the same for average and median as well across all architectures
for both Asm2Vec and SAFE. This shows that Cornucopia can
generate binaries that cannot be detected as similar by the existing
techniques. We suggest that these techniques should use Cornu-
copia to improve their dataset, which could help in creating better
models.

F CORNUCOPIA IN PARALLEL MODE
G EXTENDING TO DIFFERENT COMPILERS

To demonstrate extensibility of Cornucopia in accommodating
different compilers, we tested Cornucopia with gcc version 9.3.0.
As mentioned before, for clang, we optimize our binary generator
by avoiding running the frontend and running our optimizations

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, and Aravind Machiry

100

90

80

70

60

50

40

30

20

10

s

m
a
r
g
o
r
P
f
o
e
g
a
t
n
e
c
r
e
P

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

ğµğ·ğ¼ score

Med-O0

Med-O3

O0 vs O2

Avg-O0

Avg-O3

O0 vs O1

Max-O0

Max-O3

O0 vs O3

Figure 10: x86

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

ğµğ·ğ¼ score

Med-O0

Med-O3

O0 vs O2

Avg-O0

Avg-O3

O0 vs O1

Max-O0

Max-O3

O0 vs O3

ğµğ·ğ¼ score

Med-O0

Med-O3

O0 vs O2

Avg-O0

Avg-O3

O0 vs O1

Max-O0

Max-O3

O0 vs O3

ğµğ·ğ¼ score

Med-O0

Med-O3

O0 vs O2

Avg-O0

Avg-O3

O0 vs O1

Max-O0

Max-O3

O0 vs O3

Figure 11: x64

Figure 12: ARM

Figure 13: MIPS

Figure 14: CDF of Inverse Bindiff (ğµğ·ğ¼ ) scores of generated binaries for all programs across different architectures.

s

m
a
r
g
o
r
P
f
o
e
g
a
t
n
e
c
r
e
P

30

20

10

0

0-100

100-200

200-300

300-400

400-500

500-600

600-700

700-3700

Number of Binary Variants

Figure 15: x86

0-100

100-200

200-300

300-400

400-500

500-600

600-700

700-2800

0-100

100-200

200-300

300-400

400-500

500-600

600-700

700-800

800-900

900-1000

1000-1100

1100-2100

Number of Binary Variants

Number of Binary Variants

0-100

100-200

200-300

300-400

400-500

500-600

600-700

700-800

800-900

900-1500

Number of Binary Variants

Figure 16: x64

Figure 17: ARM

Figure 18: MIPS

Figure 19: Number of binaries generated vs percentage of programs that lie in that particular range for the different architec-
tures.

relatively less time for a run. Since we used the same fuzzing time
(six hours) for both, clang performs more iterations (âˆ¼ 10x more)
and has a greater opportunity to produce different binaries. In
fact, when we increased the fuzzing time for gcc, we noticed an
increase in the number of generated binaries. These experiments
show that Cornucopia is extensible and can be used with other
compilers too.

*** Scheduling failed! ***
...
t54: ch,glue = CopyToReg t0,
Register:i32 $eflags, t46:1, rapper.c:481:31
has not been scheduled!
...
has not been scheduled!
SU(6): PHYS REG COPY
has successors left!

Figure 20: One of the crashes found in optimization sched-
uler of LLVM

directly on the bitcode files. But, in the case of gcc (with no middle-
end optimizer like clang) we run Cornucopia as is, i.e., the entire
source package will be re-compiled using the selected flags. Fur-
thermore, each source package can generate multiple binaries. To
handle this, we used the number of cores proportional to the num-
ber of binaries. For example, for a package with eight binaries, our
binary generator compiles it parallelly (i.e., -j n) on eight cores.
Similar to clang, we executed the gcc configured Cornucopia
on x64 for six hours each on several debian source packages con-
taining total of 307 programs. We got 1,554 binaries, a relatively
fewer number of binaries compared to clang. The primary rea-
son for this is less number of iterations. As mentioned before, gcc
works directly on the source package, and each fuzzing run takes a
relatively long time as we need to first run ./configure and then
build. However, clang works directly on the bitcode file and takes

Cornucopia: A Framework for Feedback Guided Generation of Binaries

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

100

90

80

70

60

50

40

30

20

10

s

m
a
r
g
o
r
P
f
o
e
g
a
t
n
e
c
r
e
P

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

1.05

1.1

1.15

1.2

0

0.1
âˆ’2
10
Â·
5

100

90

80

70

60

50

40

30

20

10

s

m
a
r
g
o
r
P
f
o
e
g
a
t
n
e
c
r
e
P

Avg-O0

Asm2Vec score

Med-O0

Med-O3

Avg-O3

O0 vs O3

Max-O0

Max-O3

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

Avg-O0

Avg-O3

Med-O0
SAFE score
Med-O3

Max-O0

Max-O3

O0 vs O3

100

90

80

70

60

50

40

30

20

10

s

m
a
r
g
o
r
P
f
o
e
g
a
t
n
e
c
r
e
P

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95 1

0

âˆ’2
10
Â·
5

Avg-O0

SAFE ARM score

Med-O0

Med-O3

Avg-O3

O0 vs O3

Max-O0

Max-O3

Figure 21: CDF of Asm2Vec score

Figure 22: CDF of SAFE score

Figure 23: CDF for ARM binaries (SAFE).

Figure 24: CDFs of scores generated by different tools on binaries generated by llvm for all programs for x64 and ARM.

Figure 25: Number of binaries generated for a particular source file when fuzzed in multi-core mode vs in single core mode,
each source was fuzzed for 6 hours in both modes

eot2ff-5.5Kpgpewrap-7.2Kchrt-108Kaserver-110Kt1asm-190Kxmlsec1-209KSource name and respective size02000400060008000100001200014000# of unique binaries31.9x5.8x9.2x7.3x41.1x14.6xParallel Mode (6 cores)Single Instance Mode