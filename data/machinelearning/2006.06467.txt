Learning Halfspaces with Tsybakov Noise

Ilias Diakonikolas∗
University of Wisconsin-Madison
ilias@cs.wisc.edu

Vasilis Kontonis
University of Wisconsin-Madison
kontonis@wisc.edu

Christos Tzamos
University of Wisconsin-Madison
tzamos@wisc.edu

Nikos Zariﬁs†
University of Wisconsin-Madison
zarifis@wisc.edu

June 12, 2020

Abstract

We study the eﬃcient PAC learnability of halfspaces in the presence of Tsybakov noise. In
the Tsybakov noise model, each label is independently ﬂipped with some probability which is
controlled by an adversary. This noise model signiﬁcantly generalizes the Massart noise model,
by allowing the ﬂipping probabilities to be arbitrarily close to 1/2 for a fraction of the samples.
Our main result is the ﬁrst non-trivial PAC learning algorithm for this problem under a
broad family of structured distributions — satisfying certain concentration and (anti-)anti-
concentration properties — including log-concave distributions. Speciﬁcally, we given an al-
gorithm that achieves misclassiﬁcation error ǫ with respect to the true halfspace, with quasi-
polynomial runtime dependence in 1/ǫ. The only previous upper bound for this problem — even
for the special case of log-concave distributions — was doubly exponential in 1/ǫ (and follows
via the naive reduction to agnostic learning).

Our approach relies on a novel computationally eﬃcient procedure to certify whether a
candidate solution is near-optimal, based on semi-deﬁnite programming. We use this certiﬁcate
procedure as a black-box and turn it into an eﬃcient learning algorithm by searching over the
space of halfspaces via online convex optimization.

0
2
0
2

n
u
J

1
1

]

G
L
.
s
c
[

1
v
7
6
4
6
0
.
6
0
0
2
:
v
i
X
r
a

∗Supported by NSF Award CCF-1652862 (CAREER), a Sloan Research Fellowship, and a DARPA Learning with

Less Labels (LwLL) grant.

†Supported in part by a DARPA Learning with Less Labels (LwLL) grant.

 
 
 
 
 
 
1

Introduction

1.1 Background and Motivation

Halfspaces (or Linear Threshold Functions) are one of the most fundamental concept classes in
machine learning and have been an object of intense investigation since the beginning of the
ﬁeld [Ros58, Nov62, MP68]. The study of their eﬃcient learnability in various models, starting
with the Perceptron algorithm in the 1950s [Ros58], has played a central role in the development
of machine learning, and has led to important tools such as SVMs [Vap98] and Adaboost [FS97].
of the form f (x) =

Formally, an (origin-centered) halfspace is any function f : Rd

∈
≥

w, x
), where the vector w
sign(
h
i
is deﬁned as sign(t) = 1 if t
1
}

Rd is called the weight vector of f . (The function sign : R
0 and sign(t) =

→
1 otherwise.) While the sample complexity
{±
of learning halfspaces is understood in a range of models, the computational complexity of the
problem depends critically on the choice of model. In the noise-free setting, halfspaces are known to
be eﬃciently learnable in the distribution-independent PAC model [Val84] via linear programming
(see, e.g., [MT94]). On the other hand, the picture is much less clear in the presence of noisy data.
Despite signiﬁcant theoretical progress over the past two decades, several fundamental algorithmic
questions in the noisy setting are still a mystery.

−

→ {±

1
}

In this work, we study the algorithmic problem of learning halfspaces under the Tsybakov noise
condition [Tsy04], a challenging noise model that has been extensively studied in the statistics and
machine learning communities. While the information-theoretic aspects of learning with Tsybakov
noise have been largely characterized, prior to this work, the computational aspects of this broad
problem had remained wide open.

We now proceed to deﬁne this noise model. The Tsybakov noise condition prescribes that the
label of each example is independently ﬂipped with some probability which is controlled by an
adversary. Importantly, this noise condition allows the ﬂipping probabilities to be arbitrarily close
to 1/2 for a fraction of the examples. More formally, we have the following deﬁnition:

Deﬁnition 1.1 (PAC Learning with Tsybakov Noise). Let
functions over X = Rd,
0

α < 1, A > 0 be parameters of the noise model.
Let f be an unknown target function in

≤

F

be a concept class of Boolean-valued
be a family of distributions on X, 0 < ǫ < 1 be the error parameter, and

C

. A Tsybakov example oracle, EXTsyb(f,
), works
) is invoked, it returns a labeled example (x, y), such that: (a)
η(x) and
, and (b) y = f (x) with probability 1
x, where
f (x) with probability η(x). Here η(x) is an unknown function that satisﬁes the Tsybakov
1/2, η(x) satisﬁes the condition

as follows: Each time EXTsyb(f,
x
y =
noise condition with parameters (α, A). That is, for any 0 < t
α
Prx
1−α .

x is a ﬁxed distribution in

∼ D
−

x[η(x)

1/2

A t

−

≤

D

F

F

F

t]

C

∼D
Let

≥

−

≤

D

denote the joint distribution on (x, y) generated by the above oracle. A learning algorithm
such

and its goal is to output a hypothesis function h : X

is given i.i.d. samples from
that with high probability h is ǫ-close to f , i.e., it holds Prx

D

→ {±

1
}

x[h(x)

= f (x)]

∼D

ǫ.

≤

The noise model of Deﬁnition 1.1 was ﬁrst proposed in [MT99] and subsequently reﬁned
in [Tsy04]. Since these initial works, a long line of research in statistics and learning theory
has focused on understanding a range of statistical aspects of the model in various settings (see,
e.g., [Tsy04, BBL05, BJM06, BBT07, Han11, HY15] and references therein).
Ignoring compu-
tational considerations, it is known that the class of halfspaces is learnable in this model with
poly(d, 1/ǫ1/α) samples, where d is the dimension and ǫ is the error to the target halfspace.

On the other hand, the algorithmic question has remained poorly understood. Roughly speak-
ing, the only known algorithms in this noise model (for any non-trivial concept class in high

1

6
dimension) are the ones that follow via the naive reduction to agnostic learning. We also note that
eﬃcient algorithms for learning halfspaces were previously known in more structured random noise
models, including random classiﬁcation noise and bounded (Massart) noise. (See Section 1.4 for a
detailed summary of prior work.)

1.2 Our Contributions

As explained in the above discussion (also see Section 1.4), obtaining computationally eﬃcient
learning algorithms in the presence of Tsybakov noise in any non-trivial setting — that is, for any
natural concept class and under any distributional assumptions — has been a long-standing open
problem in learning theory. In this work, we make the ﬁrst progress on this problem. Speciﬁcally,
we give a learning algorithm for halfspaces that succeeds under a class of well-behaved distributions
(including log-concave distributions) and runs in time quasi-polynomial in 1/ǫ.

We start by describing the distribution family for which our algorithm succeeds.

Deﬁnition 1.2 (Bounded Distributions). For any set of parameters L, R, B, β > 0, an isotropic
x on Rd is called (L, R, B, β)-bounded if
(i.e., zero mean and identity covariance) distribution
x on a 2-dimensional subspace V , the corresponding pdf γV on R2
for any projection (
satisﬁes the following properties:

x)V of

D

D

D

1. We have that γV (x)

≥

2. For any t > 0, we have that Prx

∼

∈
γV [
k

x
k2 ≥

t]

x
k2 ≤
k
B exp(

L, for all x

V such that

R (anti-anti-concentration).

βt) (concentration).

≤

−
V we have that γV (x)

U (anti-concentration),

≤

Moreover, if there exists U > 0 such that for all x
then the distribution

x is called (L, R, U, B, β)-bounded.

∈

D

Deﬁnition 1.2 speciﬁes the concentration and (anti-)anti-concentration properties on the un-
derlying data distribution that are needed to prove the correctness of our algorithm. We note that
the sample complexity and runtime of our algorithm depends on the values of these parameters.

For concreteness, we state a simpliﬁed version of our main result for the case that L, R, U, B, β
are positive universal constants. We call such distributions well-behaved. We note that the class
of well-behaved distributions is quite broad.
In particular, it is easy to show (Fact 4.3) that
every isotropic log-concave distribution is well-behaved. Moreover, the concentration and anti-
concentration conditions of Deﬁnition 1.2 do not require a speciﬁc nonparametric constraint for the
underlying density function, and are satisﬁed by many reasonable continuous distributions.

We show:

Theorem 1.3 (Learning Halfpaces with Tsybakov Noise). Let
be the class of origin-centered
be a family of well-behaved distributions on Rd. There is an algorithm with the
halfspaces and
F
following behavior: On input the error parameter ǫ > 0 and oracle access to a Tsybakov example
oracle EXTsyb(f,
is the target concept, the algorithm draws
N = dO((1/α2) log2(1/ǫ)) labeled examples, runs in poly(N, d) time, and computes a hypothesis h
that with high probability is ǫ-close to f .

) with parameters (α, A), where f

∈ C

∈ C

F

C

See Theorem 4.2 for a more detailed statement that takes into account the dependence on the

parameters L, R, U, B, β.

Some comments are in order. Theorem 1.3 provides the ﬁrst algorithm for learning halfspaces
(or any other concept class) in the presence of Tsybakov noise with running time beating that of
agnostically learning the class. For the special case of log-concave distributions, the best sample

2

complexity and running time bounds that can be obtained via agnostic learning are d2
.
(See Section 1.4 for a detailed summary.) That is, we provide a nearly doubly exponential improve-
ment on the ǫ-dependence, even for ﬁxed α > 0. Moreover, since our algorithm does not require
log-concavity, it applies to distribution families for which no sub-exponential in d upper bound was
previously known. Interestingly, recent work [DKZ20, GGK20] has given Statistical Query (SQ)
lower bounds of dpoly(1/ǫ) for agnostically learning halfspaces, even under Gaussian marginals. Since
our algorithm runs in dpolylog(1/ǫ) time, this implies a computational separation between agnostic
learning and Tsybakov learning for the class of halfspaces.

poly(1/ǫ1/α)

Finally, we note that the exponential dependence on 1/α is to some extent unavoidable, since

Ω(d/ǫ1/α) samples are information-theoretically necessary to solve our problem.

The main question left open by our work is whether the quasi-polynomial dependence on 1/ǫ
can be improved to polynomial, i.e., whether a poly(d, 1/ǫ1/α) time algorithm exists. We leave this
as an outstanding open problem.

1.3 Overview of Techniques

In this subsection, we give an intuitive description of our techniques that lead to Theorem 1.3 in
tandem with a brief comparison to prior techniques and why the fail in our context.

It is instructive to begin by explaining where algorithms for the related problem of learning
with Massart noise fall apart. The Massart noise model corresponds to the special case of Tsybakov
noise where the label of each example x is independently ﬂipped with probability η(x)
η, where
η < 1/2 is a parameter of the model. A line of work has developed eﬃcient algorithms for learning
halfspaces in this model, with the recent works [ZSA20, DKTZ20] being the state-of-the-art. (See
Section 1.4 for more details.)

≤

We start by brieﬂy describing the underlying idea behind several previous algorithms for learning
halfspaces with Massart noise [ZSA20, DKTZ20]. These algorithms are typically iterative: In each
iteration t, we have a current guess w for the normal vector w∗ to the true halfspace, and our
goal is to perform a local step to improve our guess (in expectation). To perform these updates,
the algorithms aim to boost the contribution of the disagreement region A between the halfspaces
corresponding to w and w∗. This is achieved by considering points only around a small band
around w, i.e., all x with
< T . This idea suﬃces to obtain eﬃcient algorithms for the
Massart noise model under well-behaved (e.g., log-concave) distributions as the total contribution
of those points is ampliﬁed.

w, x

i |

| h

≈

For the case of Tsybakov noise however, the situation is much more challenging. Even though
the probability mass of the points in region A increases by restricting to a band around the current
guess, it does not guarantee that the angle between w and w∗ improves. This is because in
the Tsybakov noise model, it is possible that all points in region A have ﬂipping probabilities
η(x)
1/2, which grow closer to 1/2 the more the band shrinks. Thus, even though the conditional
probability of region A increases with smaller band size T , the signal that these points provide to
improve the angle may not be strong enough to overcome the eﬀect that the remaining points have.
Our main idea to overcome this obstacle is to increase the contribution of points in region
A by appropriately reweighting them (see Figure 1). A key observation that drives our algo-
rithm (see Fact 3.1) is to ﬁnd a weighting scheme that certiﬁes whether a given guess w is
(near-)optimal. In more detail, if there exists a non-negative weighting function F (x) such that
)] < 0, then the weight vector w is not optimal. Conversely, if w is not
E(x,y)
optimal, a weighting function F that makes the above expectation negative always exists (take for
example the indicator of the disagreement region between w and w∗).

[F (x)y sign(
h

w, x
i

∼D

3

Our ﬁrst technical contribution is making the aforementioned certiﬁcate algorithmic. In more
detail, we show that in order to certify that a guess w is ǫ-far from optimal, it suﬃces to consider
weighting functions of a particular form, equal to the square of a multivariate polynomial restricted
on a band close to w. In particular, we show (Theorem 3.2) that it suﬃces to consider polynomials
of degree at most k = O(log2(1/ǫ)/α2). We provide an explicit construction of such a multivariate
polynomial with bounded coeﬃcients, making critical use of Chebyshev polynomials.

Given this structural result, we can eﬃciently check the validity of a particular guess by search-
ing all functions of the aforementioned form. Drawing suﬃciently many samples so that all functions
in the class converge uniformly, we can identify a good weighting (if one exists) by solving a semideﬁ-
nite program to check the required condition over all squares of polynomials of degree-k. The sample
complexity required to ﬁnd our certiﬁcate is dO(k) and can be achieved in sample-polynomial time
(Lemma 3.9).

We note that while our algorithm searches over multivariate polynomials that certify the error
of our estimate, our approach diﬀers signiﬁcantly from other approaches for learning halfspaces
by approximating them by polynomial threshold functions, like the L1-regression algorithm of
[KKMS08]. Our use of polynomials is done in order to certify whether a candidate halfspace is
suﬃciently accurate, instead of searching a larger class of hypotheses. Remaining within the class of
halfspaces allows us to use geometric properties of the underlying data distributions and the setting
we consider, like the relationship of the misclassiﬁcation error and the angle between the guess and
the optimal halfspace. Additionally, while the L1-regression can be written as a linear program,
our approach requires searching over squares of polynomials and inherently relies on solving SDPs
for obtaining a certiﬁcate.

Finally, turning the above algorithm for obtaining certiﬁcates into a learning algorithm is not
immediate. To achieve this, we rely on online convex optimization with a similar approach to the
one used in [ZSA20]. In contrast to an oﬄine method like stochastic gradient descent, online convex
optimization allows us to change the distribution of examples with which we penalize the guess,
and the distribution is allowed to depend on the current guess. For every guess w, we compute a
loss function according to the reweighted distribution of points given by our certiﬁcate. We set up
the objective so that any guess that is not close to optimal incurs a large loss, while the optimal
guess always incurs a very small loss. By the guarantees of online convex optimization, after few
iterations, the average loss of our guesses must be very close to the optimal loss. This means that
one of the guesses must be near-optimal (see Lemma 4.7). This property will cause the certiﬁcate
algorithm to accept this guess as close to optimal. A complication that arises in designing the loss
function is that guessing 0 must give a large loss compared to the optimal, which we ensure by
making the loss suﬃciently negative at the optimal linear classiﬁer.

1.4 Related Work

It is instructive to compare the Tsybakov noise model with two other classical noise models, namely
the agnostic model [Hau92, KSS94] and the bounded (or Massart) noise model [Slo88, MN06]. The
Tsybakov noise model lies in between these two models.

on labeled examples (x, y)

In the agnostic model [Hau92, KSS94], the learner is given access to iid labeled examples from
and the goal of the learner
1(h)

an arbitrary distribution
1
}
= y]
is to output a hypothesis h such that the misclassiﬁcation error errD0
∼D
−
OPT + ǫ, where OPT def=
1(h)
is as small as possible. In more detail, we want to achieve errD0
−
.
inf g
C
Agnostic noise is the most challenging noise model in the literature. Without assumptions on the

1(g) is the minimum possible misclassiﬁcation error by any function in the class

def
= Pr(x,y)

errD0
−

[h(x)

× {±

Rd

≤

D

∈C

∈

4

6
2T

w

θ

w∗

w⊥

Figure 1: The disagreement region A (“blue”) of the halfspaces w and w∗. Our reweighting boosts
points in region A: lower opacity means lower weight.

marginal distribution
computationally intractable [GR06, FGKP06, Dan16].

D

x on the (unlabaled) points, (even weak) agnostic learning is known to be

D

On the other hand, if

x is known to be well-behaved, in a precise sense, dimension-eﬃcient
agnostic algorithms are known. Speciﬁcally, the L1-regression algorithm of [KKMS08] agnostically
learns halfspaces under the standard Gaussian and, more generally, any isotropic log-concave distri-
bution, with sample complexity and runtime dm(1/ǫ), for an appropriate function m. In more detail,
x is the standard Gaussian N (0, I), then m(1/ǫ) = ˜Θ(1/ǫ2) (see, e.g., [DGJ+10, DKN10]) and
if
x is any isotropic log-concave distribution, then m(1/ǫ) = 2Θ(poly(1/ǫ)). These runtime bounds
if
are tight for the L1-regression approach, as they rely on the minimum degree of certain polyno-
mial approximations of the univariate sign function. Moreover, recent work [DKZ20, GGK20] has
shown Statistical Query lower bounds of dpoly(1/ǫ) for agnostically learning halfspaces, even under
Gaussian marginals.

D
D

Prior to this work, the only known algorithms for Tsybakov noise are the ones obtained via
the straightforward reduction to agnostic learning. Speciﬁcally, by applying the L1-regression
algorithm [KKMS08] for ǫ′ = Θ(ǫ1/α) in place of ǫ, where α
(0, 1] is the Tsybakov noise pa-
rameter of Deﬁnition 1.1, we have (see, e.g., Corollary 3.4) that the output hypothesis h satis-
= f (x)]
ﬁes Prx
ǫ. This straightforward reduction leads to algorithms with runtimes
dpoly(1/ǫ1/α) for Gaussian marginals, and d2

for log-concave marginals.

poly(1/ǫ1/α)

x[h(x)

∼D

≤

∈

We acknowledge a related line of work [KLS09, ABL17, Dan15, DKS18] that gave eﬃcient
algorithms for learning halfspaces with agnostic noise under similar distributional assumptions.
While these algorithms run in time poly(d/ǫ), they achieve a “semi-agnostic” error guarantee of
O(OPT) + ǫ — instead of 1
OPT + ǫ. This guarantee is signiﬁcantly weaker for our purposes and
cannot be used to obtain a hypothesis that is arbitrarily close to the target halfspace.

·

The bounded (Massart) noise model [Slo88, MN06] is the special case of Tsybakov noise, where
an adversary can ﬂip the label of each example x independently with probability η(x)
η, for some
parameter η < 1/2. This noise model has attracted signiﬁcant attention in recent years. A long line
of work, initiated by [ABHU15], has obtained computationally eﬃcient algorithms for PAC learning
halfspaces with Massart noise to arbitrary accuracy (under distributional assumptions) [ABHZ16,
ZLC17, YZ17, MV19, ZSA20, DKTZ20]. Recent works developed polynomial-time algorithms (in
all relevant parameters) under log-concave [ZSA20, DKTZ20], s-concave, and other structured

≤

5

6
distributions [DKTZ20]. These algorithms inherently fail for the more challenging Tsybakov noise
model, and new ideas are needed for this more general setting.

We note that the recent work [DGT19] developed the ﬁrst computationally eﬃcient weak learner
for halfspaces with Massart noise in the distribution-independent setting. The approach of [DGT19]
can be adapted to give a weak learner for halfspaces under Tsybakov noise as well, but cannot
directly lead to an arbitrarily close approximation to the true halfspace.

Finally, it should be noted that this work is part of the broader agenda of designing robust
estimators for a range of generative models with respect to various noise models. A recent line of
work [KLS09, ABL17, DKK+16, LRV16, DKK+17, DKK+18, DKS18, KKM18, DKS19, DKK+19]
has given eﬃcient robust estimators for a range of learning tasks (both supervised and unsupervised)
in the presence of a small constant fraction of adversarial corruptions.

2 Preliminaries

def
=

∈

For n

1, . . . , n
{

. We will use small boldface characters for vectors. For x
}

Z+, let [n]
[d], xi denotes the i-th coordinate of x, and
for the inner product of x, y

and i
∈
x. We will use
x, y
h
We will also denote 1A to be the characteristic function of the set A, i.e., 1A(x) = 1 if x
1A(x) = 0 if x /
∈

Rd
i )1/2 denotes the ℓ2-norm of
x
k2
k
Rd and θ(x, y) for the angle between x, y.
∈
A and

d
i=1 x2

def
= (

P

A.

∈

∈

i

Let ei be the i-th standard basis vector in Rd. For d
. Let ΠU (x) be the projection of x onto subspace U
1
}

∈
Rd, let U ⊥ be the orthogonal complement of U .

x
and
{
V
subspace U

N, let Sd
−

x
k2 ≤
k

Rd :

x
{

def=

∈

∈

Rd :

⊂

x
k2 = 1
k
}
Rd. For a

1 def=

D

Let E[X] denote the expectation of random variable X and Pr[
We consider the binary classiﬁcation setting where labeled examples (x, y) are drawn i.i.d. from
on x. The misclassiﬁcation error
= y]. The zero-

on Rd
a distribution
of a hypothesis h : Rd

D
(with respect to

] the probability of event

→ {±
one error between two functions f, h (with respect to

= h(x)].
For a square matrix M, we say that M is positive semi-deﬁnite if only if all the eigenvalues of
m the set of symmetric matrices of dimension m.

x) is errD
0
−

Z+, we denote

M are non-negative. For m
For an m-dimensional square matrix A, let tr(A) be its trace.

. We denote by
1
}
1
}

x the marginal of
) is errD0
−

1(h) def= Pr(x,y)
1(f, h)

∼D
def
= Prx

x[f (x)

[h(x)

×{±

∼D

D

D

D

∈

S

E

E

.

x

⊂

Let S = (s1, s2, . . . , sd) be a d-dimensional multi-index vector, where for all i

[d], si is non-
d
i=1 si and for a d-dimensional vector w = (w1, w2, . . . , wd),

∈

negative integer. We denote
we denote wS =

i=1 wsi
i .

d

=

S
|

|

P

For a degree-k multivariate polynomial p(x) =

k CSxS, let

S:

S

|

|≤

def
=

p
k

k2

k C 2

S and

S:

S

|

|≤

qP

.

P

def
=

p
k

k1

S:

S

|

|≤

Q
CS|

k |

P

3 Certifying Optimality

In this section, we describe an eﬃcient way to test whether a given candidate hypothesis w is close
to the optimal hypothesis w∗. Our approach is based on the following observation.

Fact 3.1. For any F : Rd
7→
noise condition, it holds that

R+ and any distribution

on Rd

× {±

1
}

D

that satisﬁes the Tsybakov

E
(x,y)

∼D

[F (x)

w∗, x
i

h

y]

≥

0 .

(1)

6

6
6
Proof. We have that

E
(x,y)

∼D

[F (x)

w∗, x
i

h

x

y] = E
∼D
= E
x
∼D

x

x

[F (x)

| h

[F (x)

| h

w∗, x

(1

i |

w∗, x

(1

i |

−

−

η(x))]

−
2η(x))]

x

[F (x)

| h

x

E
∼D
0 ,

w∗, x

η(x)]

i |

≥

where we used the fact that η(x)

1/2 and F (x)

0.

≤

Ex

≥
From Fact 3.1, we see that, given a hypothesis vector w that is not optimal, there exists a
non-negative function that will make the expression of Equation (1) negative. One such func-
tion is F (x) = 1
y] =
sign(
h
{
2η(x))] < 0. Since we cannot eﬃciently search over the space of all non-
w, x
(1
x[
−
i |
negative functions, we need to restrict our search space of certifying functions to some parametric
class, ideally with a small number of parameters. In Section 3.1, we show that considering squares
of low-degree polynomials suﬃces. In Section 3.2, we show that we can eﬃciently search in the space
of (squares of) low-degree polynomials and ﬁnd one that will make the expression of Equation (1)
negative.

, in which case we have E(x,y)
)
}

= sign(
h

w∗, x
i

w, x
i

w, x
i

[F (x)

∼D

∼D

| h

−

h

)

3.1 Existence of a Low-Degree Polynomial Certiﬁcate

We start by showing that given a candidate hypothesis w that is “far” from being optimal, that is
the angle θ(w, w∗) is bounded away from zero, we can construct a low complexity certiﬁcate F that
will satisfy E(x,y)
y] < 0. In particular, we construct a certiﬁcate that is the product
of a square of a low degree non-negative polynomial and an indicator function that depends on the
hypothesis w. This result is formally stated in the lemma bellow, which is the main result of this
subsection.

w, x
i

[F (x)

∼D

h

Theorem 3.2 (Low Complexity Certiﬁcate). Let
the Tsybakov noise condition with parameters (α, A) and the marginal
bounded. Fix any θ
w

that satisﬁes
x on Rd is (L, R, B, β)-
D
1 be the normal vector to the optimal halfspace and
R of degree

θ. There exists polynomial p : Rd

1 be such that θ(

be a distribution on Rd

× {±

1
}

Sd

Sd

D

∈

−

(0, π/2]. Let w∗ ∈
w, w∗)
≥

−

∈

7→

b

b

k = O

1
α2Rβ

log2

BA
LRθ

(cid:18)

(cid:18)

(cid:19)(cid:19)

satisfying

p

2
2 ≤
k

k

dO(k) such that

E
(x,y)

∼D

(cid:2)

p(x)2 1

0
{

≤ h

w, x

i ≤

θR/4
}

y

w, x
i
h

≤ −

θR
4

.

(cid:3)

2η(x) > 0. Notice that larger noise η(x) makes 1

We are going to use the following simple fact about Tsybakov noise that shows that large
probability regions will also have large integral even if we weight the integral with the noise function
2η(x) closer to 0, and therefore tends to
1
reduce the probability mass of the regions where η(x) is large. A similar lemma can be found
in [Tsy04]. Since the deﬁnition of η(x) is slightly diﬀerent than ours, we provide the proof for
completeness in Appendix A.1.

−

−

Lemma 3.3. Let
with parameters (α, A). Then for every measurable set S
C A

be a distribution on Rd

α , where C A

x[1S(x)])

1−α
α .

× {±

1
}

D

α = α

α (Ex

α

1

1

−
A

∼D

that satisﬁes the Tsybakov noise condition

Rd it holds Ex

∼

⊆

Dx[1S(x)(1

2η(x))]

−

≥

(cid:0)

(cid:1)

7

6
Using the lemma above, we can bound from below and above the errD0
−

between our current hypothesis h and the optimal f .

1(h) with the errD
0
−

x

1(h, f )

Corollary 3.4. Let
with parameters (α, A) and f (x) be the optimal halfspace. Then for any halfspace h(x), it holds

that satisﬁes the Tsybakov noise condition

be a distribution on Rd

× {±

1
}

D

Pr(x,y)

∼D

[h(x)

= y]

Pr(x,y)

∼D

[h(x)

= y]

≤

≥

Pr(x,y)

[f (x)

= y] + Prx

∼D

x[h(x)

= f (x)]

∼D
and

Pr(x,y)

∼D

[f (x)

= y] + C A

α Prx

∼D

x[h(x)

= f (x)]

1
α .

Proof. Let S =

Pr(x,y)

∼D

[h(x)

∈

x
{
= y] = E
(x,y)

Rd : f (x)

= h(x)
}

then

[1

= y

h(x)
{
h(x)
{

] = E
x
}
∼D
= f (x)
(1
}

−

∼D
[1

x

[1

x

h(x)
{

= f (x)
(1
}
[η(x)] .

−

2η(x))] + E
∼D

x

x

= E
x
∼D

η(x))] + E
∼D

x

x

[1

η(x)]
h(x) = f (x)
}
{

The ﬁrst inequality follows from the fact that 1

2η(x)

−

≤

1 and the second one from Lemma 3.3.

Central role in our construction play the Chebyshev polynomials. In the next fact, we collect
the properties of Chebyshev polynomials that we are going to use in our argument, and we prove
some of them in Appendix A.2.

Fact 3.5 (Chebyshev Polynomials [MH02]). We denote by Tk(t) the degree-k Chebyshev polynomial
of the ﬁrst kind. It holds

cos(k arccos t) ,

Tk(t) =




Moreover, it holds

Tkk
k


2
2 ≤

1
2

√t2

−

t
(cid:18)(cid:16)
26k+log k+4.

1

−

k

+

t + √t2

(cid:17)

(cid:16)

1

−

k

,

(cid:19)

(cid:17)

t
|
t
|

| ≤

| ≥

1

1 .

Given a univariate polynomial p(t), the following simple lemma bounds the blow-up of the
). We also give a simple bound on the

square norm of the multivariate polynomial q(x) = p(
w, x
i
h
coeﬃcient norm blow-up under shift of the argument of a univariate polynomial.

k
i=0 citi be a degree-k univariate polynomial. Given w

w, x
1, deﬁne the multivariate polynomial q(x) = p(
i
h
i . Moreover, let r(t) = p(at + b) =

Lemma 3.6. Let p(t) =
w
k
that
2
P
2 ≤
k
The proof of this lemma is given in Appendix A.2. We can now proceed to the proof of the

k2 ≤
P
k
i=0 c2
S:
|
(2 max(1, a) max(1, b))2k

Rd with
k CSxS. Then we have
R. Then

k
i=0 diti for some a, b
P

2
2 .
k

k C 2

S ≤

r
k

) =

d2k

P

P

∈

∈

|≤

|≤

S:

p

k

S

S

|

main technical theorem.

Proof of Theorem 3.2. Let V be the 2-dimensional subspace spanned by w∗ and w. To simplify
notation, let θ be the angle between w∗ and w. First, we assume that θ
π/2. Without loss of
ae1 + be2, where e1, e2 are the standard basis vectors of R2.
generality, assume w = e2 and w∗ =
For some parameter W > 0 to be speciﬁed later, we deﬁne the linear transformation

≤

−

g(t) = 1 + 2

t
R/4
W + R/4

−

.

8

6
6
6
6
6
6
6
6
6
6
6
1

R/4

W

−

Figure 2: Plot of the polynomial (Tk(g(t)))2 used in the proof of Theorem 3.2. Observe that this
polynomial boosts the contribution of points in the blue region of Figure 1: points in A2 have
signiﬁcantly boosted contribution because their density is lower bounded by some constant and the
polynomial takes very large values in A2, see Fact 3.7. In A0, even though the polynomial has large
value, the exponential tails of the distribution cancel the contribution of these points (given that
W is suﬃciently large).

Set p(x) = Tk(g(x1)), where Tk is the degree-k Chebyshev polynomial of Fact 3.5, and deﬁne the
following partition of Rd

A0 =

x : x1 ∈
{

,

[
−∞

,
W ]
}

−

A1 =

x : x1 ∈
{

[
−

,
W, R/4]
}

and A2 =

x : x1 ∈
{

[R/4, +

.

]
}

∞

We ﬁrst investigate the behavior of p(x) in each of these three regions.

Fact 3.7. For the polynomial p(x) deﬁned above, the following properties hold in each region:

1. For all x

A0, p(x)2

∈

≤

(2g(x1))2k.

2. For all x

A1, p(x)2

1.

∈
≤
3. For all x such that x1 ≥

R/2, it holds that p(x)2

1
2

≥

1 +

R
2W +R/2

2k

.

(cid:16)

q

(cid:17)

≤ −

Proof. By Fact 3.5, for the univariate Chebyshev polynomials of degree-k, we know that for all
t

1 it holds

Tk(t)
|
|

=

1
2

((t

−

Observe that for all x
≤ −
1.
have
≤
polynomial Tk (Fact 3.5), we have that for all t

(cid:12)
(cid:12)
A0, we have g(x1)
(cid:12)
(cid:12)
1, which leads to p(x)2

g(x1)

p

≤

−

≤

∈

1

0 it holds

t2

1)k + (t +

t2

−

−
p
1, thus p(x)2

≤

(2t)k .

1)k)
(cid:12)
(cid:12)
(2g(x1))2k. For all x
(cid:12)
(cid:12)

A1, we
Finally, from the deﬁnition of the Chebyshev

≤

∈

Tk(1 + t)

1
2

≥

(1 + t +

t2 + 2t)k

1
2

≥

(1 + √t)k.

≥

p

Moreover, all the roots of Tk(t) lie in the interval [
(Tk(1 + t))2 is increasing in t. Therefore, for any x with x1 ≥

−

≥
R/2 it holds that

1, 1] and hence, for t

0, the polynomial

p(x)2 = Tk(g(x1))

Tk(g(R/2))

≥

1
2  

≥

1 +

R
2W + R/2 !

s

2k

.

9

We bound the expectation E(x,y)

[p(x)2

w, x
i

w, x
y sign(
i
h

h

)1

0
{

≤ h

w, x

i ≤

θR
4 }

] in each of

∼D

the three regions separately. We start from A0, where we have

I0 = E
(x,y)

∼D

y 1

w, x

[p(x)2

w, x
i
h

{h
[Tk(g(x1))2 x2y 1
x2 ∈
{

i ∈

[0, θR/4]
}
1

1A0(x)]

∼D

[0, θR/4]
}

= E
(x,y)
θR
4
where to get the last inequality we used that x21[x2 ∈
θR/4 and Item 1 of Fact 3.7.
[0, θR/4]
≤
m] =
Using the fact that for any real random variable X it holds E[
t]dt
X
|
|
DV (see Deﬁnition 1.2), we obtain
and the exponential concentration of

[(2g(x1))2k1

x1 ≤ −
{

x1 ≤ −
{

1Pr[
X
|

∞0 mtm
−

E
(x1,x2)

] ,
}

∼DV

| ≥

]
}

W

W

≤

R

E
(x1,x2)

∼DV

[g(x1)2k1

x1 ≤ −
{

W

]
}

=

=

0
Z

1

∞

2kt2k

−

1

2kt2k

1e−

−

Pr
(x1,x2)

∼DV
βW dt +

g(x1)1
[
|

x1 ≤ −
{

W

}| ≤

t]dt

∞

2kt2k

1e−

−

β t+1

2 (W + R

4 )+β R

4 dt .

1
Z
We observe that for all t > 1, R > 0, W > 0 it holds

0
Z

t + 1
2

(cid:18)

W +

R
4

−

(cid:19)

R
4 ≥

tW
2

.

Therefore,

∞

2kt2k

1e−

−

1

Z

β t+1

2 (W + R

4 )+β R

4 dt

∞

2kt2k

1e−

tβW/2dt

−

≤

1

Z

∞

2kt2k

1e−

tβW/2dt

−

≤

0

Z

2k

−

(2k)! .

W β
2

(cid:19)

≤

(cid:18)

Combining the above inequalities we obtain

I0 ≤

=

θRB22k
4
θRB22k
4

(cid:16)

1

2kt2k

1e−

βW dt +

−

∞

2kt2k

1e−

−

β t+1

2 (W + R

4 )+β R

4 dt

0
(cid:18)Z
e−

βW + (W β/2)−

1
Z
2k(2k)!
(cid:17)

.

(cid:19)

We now set W = 8k/β and get

I0 ≤

θRB
4

(22ke−

8k + (2k)−

2k(2k)!)

θRB
4

≤

(e−

6k + e−

2k+1√2k)

θRB
4

,

≤

where we used Stirling’s approximation, i.e., (2k)!
e−

2k+1√2k
Bounding the contribution of region A1 is quite simple. Using from Fact 3.7, that p(x)2

2k(2k)2k, and the fact that e−

1, for all k

e√2ke−

≤

≤

≥

1.

for all x

∈

A1, we obtain

I1 = E
(x,y)

∼D

[p(x)2

w, x
i

h

y 1

w, x

{h

i ∈

[0, θR/4]
}

1A1(x)]

θR
4

.

≤

6k +

1

≤

10

We ﬁnally bound the contribution of region A2. We have

I2 = E
(x,y)

[p(x)2

h

[0, θR/4]
}

1A2(x)]

{h

w, x

i ∈
2η(x)) 1

w, x
i
w, x
i
w, x
i

h

h

y 1

(1

(1

−

−

[p(x)2

[p(x)2

w, x

i ∈

{h

[0, θR/4]
}

1A2(x)]

2η(x)) 1

w, x

[θR/8, θR/4]
}

1

i ∈

{h
2η(x)) 1

]
R/2
}

x1 ≥
{
1

Tk(g(R/2))2 E
∼D

x

[ (1

x

−

w, x

i ∈

{h

[θR/8, θR/4]
}

x1 ≥
{

] ,
R/2
}

x

x

x

∼D
E
∼D
E
x
∼D
θR
8

=

−

≤ −

≤ −

where we used Item 3 of Fact 3.7. Using Lemma 3.3, we obtain that

[ (1

x

E
∼D

x

−

2η(x)) 1

{h

w, x

[θR/8, θR/4]
}

1

x1 ≥
{

]
R/2
}

≥

C A

α (LθR/16)1/α .

i ∈

From Item 3 of Fact 3.7, we obtain

I2 ≤ −

α Tk(g(R/2))2 θR
C A
8

1/α

L

θR2
16

(cid:18)

(cid:19)

θR
4

≤ −

(B +2)

C A
α
2(B + 2)  

1 +

R
2W + R/2 !

s

2k

1
α

.

L

θR2
16

(cid:18)

(cid:19)

Using the inequality 1 + t

et/2 for all t

θR/4, it suﬃces to pick the degree k so that

≥

2, we obtain that in order to prove that I0 + I1 + I2 ≤

≤

−

C A
α
2(B + 2)

Rk2
2W +R/2

eq

1
α

θR2
16

L

(cid:18)

(cid:19)

1.

≥

By our choice of W = 8k/β, it follows that setting the degree of the polynomial to

k = O

1
α2Rβ

log2

BA
LRθ

(cid:18)

(cid:18)

(cid:19)(cid:19)

suﬃces. To complete the proof, we need to provide an upper bound on the magnitude of the
26k+2 log k+4. Using
Tk(x)
coeﬃcients of the polynomial p. From Fact 3.5, we have that
k
k
26k+2 log k+4 = 28k+2 log k+4. Moreover, from the
Lemma 3.6, we obtain that
·
Lemma 3.6, we can derive an upper bound on the square norm of the multivariate polynomial p,
which is

d2k28k+2 log k+4 = dO(k).

Tk(g(x))
k
k

2
2 ≤

2
2 ≤

22k

Moreover, for the case where π

θ > π/2, we can prove with the same argument that

≥

p
k

2
2 ≤
k

E
(x,y)

∼D

(cid:2)

p(x)2 1

0
{

≤ h

w, x

i ≤

πR/8
}

y

w, x
i

h

≤ −

πR
8

.

(cid:3)

This follows from the fact that the expectation over the partitions A0 and A1 are at most their
values for the case of θ = π/2, and the expectation over A2 is the same.

3.2 Eﬃciently Computing the Certiﬁcate

In this section, we show that we can eﬃciently compute our polynomial certiﬁcate given labeled
examples from the target distribution. For the rest of this section, let Q = dΘ(k) and let 1B(x)
. Denote by m(x) the vector
x : 0
be the indicator function of the region B =
θR/4
}
{
containing all monomials up to degree k, such that mS(x) def= xS, indexed by the multi-index S

w, x

i ≤

≤ h

11

satisfying
| ≤
deﬁne the following function

S
|

k. The dimension of m(x)

Rm is m =

d+k
k

∈

. For a real matrix A

Rm

×

m, we

∈

m(x)T A m(x)1B(x)

w(A) = E
(x,y)

L

∼D
m(x)m(x)T 1B(x)

(cid:2)

(cid:0)

(cid:1)

w, x
i

h

y

(cid:3)

= tr (AM) ,

(2)

h

(cid:2)

∼D

w, x
i

. Notice that

where M = E(x,y)
w is linear in its variable A.
y
From the discussion of the previous subsection, and in particular from Theorem 3.2, we know
θ, then there exists a polynomial p(x) and a vector b of coeﬃcients such that
that if θ(w, w∗)
≥
w(bbT )
p(x) =
b, m(x)
It follows that there exists a positive semi-deﬁnite
and
i
h
rank-1 matrix B = bbT such that
Q, which
θR/4. Moreover, we have that
2
translates to
Q. Therefore, we can formulate the following semi-deﬁnite program, which
F ≤
is feasible when θ(w, w∗)

≤ −
L

B
k
k

2
2 ≤

p2(x)

θR/4.

w(B)

≤ −

(cid:13)
(cid:13)

(cid:13)
(cid:13)

θ.

L

L

(cid:3)

≥

θR/4

≤ −
Q

tr(AM)
2
F ≤
k
A
(cid:23)

A
k

0

(3)

M = 1
N

We deﬁne
N samples from
D
deﬁne the following “empirical” SDP

f

N

i=1 m(x(i))m(x(i))T 1B(x(i))y(i)

w, x(i)

. We can now replace the matrix M in Equation (2) with the estimate
(cid:10)
P

(cid:11)

, the empirical estimate of M using
M and

3θR
16

f

(4)

tr(A

M)

≤ −
Q

A

k

2
f
F ≤
k
A
(cid:23)

0

In the following lemma, we bound the sample size required so that

M is suﬃciently close to M.

Lemma 3.8 (Estimation of M). Let Ω =
algorithm that draws

BQ2
ǫ2

N = O

(cid:18)

m : A

A
{
∈ S
(d + k)3k+2
(β/2)2k

log(1/δ)

(cid:19)

0,

A

f
kF ≤

k

Q

. There exists an
}

(cid:23)

samples from
such that

D

, runs in poly(N, d) time and with probability at least 1

Proof. Recall that

M is the empirical estimate of M, that is

tr(A

M)

tr(AM)

Pr

sup
A
Ω

∈

(cid:20)

(cid:12)
(cid:12)
(cid:12)

−

f

ǫ

≥

≤

(cid:21)

δ .

1

−

(cid:12)
(cid:12)
(cid:12)

δ outputs a matrix

M

−

f

M = E
(x,y)

∼D

f

[m(x)m(x)T 1B(x)y

w, x
i
h

] and

M =

Using the Cauchy-Schwarz inequality, we get

f

1
N

N

Xi=1

m(x(i))m(x(i))T 1B(x(i))y(i)

w, x(i)
D

.

E
(5)

tr

A(M

(cid:16)

−

M)

(cid:17)

f

A

kF

≤ k

M

−

.

F

M

(cid:13)
(cid:13)
(cid:13)

f

(cid:13)
(cid:13)
(cid:13)

12

Therefore, it suﬃces to bound the probability that
we have

M

M

ǫ/Q. From Markov’s inequality,

Pr

M

M

ǫ/Q

−

F ≥

.

(6)

i
Using multi-indices S1, S2 that correspond to the monomials xS1, xS2 (as indices of the matrix M),
we have

f

f

h(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
Q2
(cid:13)
ǫ2

≤

F ≥

−

E

(cid:13)
(cid:13)
(cid:13)
M

f

M

−

2

F
(cid:13)
(cid:13)
(cid:13)

(cid:21)

E

M

M

−

(cid:20)(cid:13)
(cid:13)
(cid:13)

2

F
(cid:13)
(cid:13)
(cid:13)

f

=

(cid:21)

,
S1
XS1,S2:
|
|
|

S2

|≤

k

(MS1,S2 −

MS1,S2)2 =

f

,
S1
XS1,S2:
|
|
|

S2

|≤

k

f

Var[

MS1,S2] .

Using the fact that the samples (x(i), y(i)) are independent, we can bound from above the variance
of each entry (S1, S2) of

M

x2(S1+S2) (1B(x)

w, x
i

h

y)2

i

f
Var[

MS1,S2]

f

1
N
1
N
1
N

≤

≤

≤

E
(x,y)

∼D h

x

x

E
∼D
E
∼D

x

x

h

h

x2(S1+S2)

2
2)|

(
k

x
k

S1+S2

|

2
2

x
k
k

i
+1

.

i

To bound the higher-order moments, we are going to use the (two-dimensional) exponential tails
of

x of Deﬁnition 1.2. For all t

t0, it holds

D

≥

Pr[
k

x
k2 ≥

t] = Pr[
k

2
x
2 ≥
k

t2]

≤

d

Xi=1

Pr

2

xi|
|

≥

(cid:20)

t2
d

≤

(cid:21)

Bde−

βt/√d ,

where β, B are the parameters of Deﬁnition 1.2. For every ℓ

1, we have

≥

2
2)ℓ

(
k

x
k

x

E
∼D

x

h

=

∞

2ℓt2ℓ

−

1Prx

t=0

Z

i

x[
k

x
k2 ≥

∼D

t]dt

≤

Bd ℓ+1β−

2ℓ(2ℓ)! .

Using the above bound for the variance and summing over all pairs S1, S2 with
obtain

,
S1|
|

S2| ≤
|

k, we

E

M

(cid:20)(cid:13)
(cid:13)
(cid:13)

−

2

F

M

(cid:13)
(cid:13)
(cid:13)

f

1
N
1
N

≤

(cid:21)

≤

Bd k+1β−

2k(2k)! m2 =

B(β/2)−

2k(d + k)3k+1 ,

1
N

Bd k+1β−

2k(2k)!

(cid:18)

d + k
k

2

(cid:19)

(7)

≥

BQ2(β/2)−

4n. Combining Equations (6) and (7) we obtain that
where we used the inequality (2n)!/(n!)2
2k(d + k)3k+1/(4ǫ2) samples we can estimate M within the target accuracy
with N
δ, we can simply use the above
with probability at least 3/4. To amplify the probability to 1
M(ℓ) and keep the coordinate-wise median
empirical estimate ℓ times to obtain estimates
as our ﬁnal estimate. It follows that ℓ = O(log(m/δ)) repetitions suﬃce to guarantee conﬁdence
probability at least 1

M(1), . . . ,

f

f

−

≤

δ.

−

The following is the main lemma of this subsection, where we bound the number of samples

and the runtime needed to construct the certiﬁcate given samples from the distribution

.

D

13

D

be a distribution on Rd

Lemma 3.9. Let
x on Rd is (L, R, B, β)-bounded. Let w∗ ∈
parameters (α, A) and the marginal
D
vector to the optimal halfspace and w
∈
∈
Let
BA
LRθ

that satisﬁes the Tsybakov noise condition with
1 be the normal
θ.

(0, π/2] and assume that θ(w∗, w)

1. Fix any θ

1
α2Rβ

k = O

× {±

log2

1
}

Sd

Sd

≥

−

−

,

(cid:18)

(cid:18)

(cid:19)(cid:19)

and Q = dΘ(k). There exists an algorithm that draws N = dO(k) log(1/δ) samples from
in time poly(N, d), and with probability 1
A
k

, runs
δ returns a positive semi-deﬁnite matrix A such that

Q and tr(AM)

2
F ≤

θR/16.

≤ −

−

D

k

Proof. From Lemma 3.8, we obtain that with N samples we can get a matrix
tr(AM)
bound for k and

θR/16 with probability at least 1

| ≤

−

−
δ. From Theorem 3.2, we know that with the given

f

f

tr(A
|

M

M such that

A
k

kF , there exists A∗ such that
tr(A∗M)

θR/4.

≤ −

Therefore, the SDP (3) is feasible. Moreover, from Lemma 3.8 we get that

tr(A∗

M)

≤ −

θR/4 + θR/16

3θR
16

.

≤ −

Thus, the following SDP is also feasible

f

tr(A

M)

3θR
16

≤ −
Q

k

A

2
f
F ≤
k
A
(cid:23)
Since the dimension of the matrix A is smaller than the number of samples, we have that the
runtime of the SDP is polynomial in the number of samples. Solving the SDP using tolerance
A, in the sense that tr(
θR/16, we obtain an almost feasible
θR/8.
Using again the guarantee of Lemma 3.8, we get that solving the SDP (8), we obtain a positive-semi
θR/8 + θR/16 =
deﬁnite matrix

3θR/16 + θR/16 =

A such that tr(

e
f
θR/16.

AM)

≤ −

M)

(8)

A

−

0

e
≤ −

−

e

e

4 Learning the Optimal Halfspace via Online Gradient Descent

w with small
In this section, we give a quasi-polynomial time algorithm that can learn a unit vector
angle from the normal vector of the optimal halfspace w∗. Our main result of this section is the
following theorem.

b

Theorem 4.1 (Parameter Estimation under (L, R, B, β)-bounded distributions). Let
be a dis-
tribution on Rd
that satisﬁes the Tsybakov noise condition with parameters (α, A) and
1 be the normal vector to
the marginal
the optimal halfspace. There exists an algorithm that draws N = dO(k) log (1/δ) examples from
w such that

× {±
x on Rd is (L, R, B, β)-bounded. Moreover, let w∗ ∈
D

1
}

Sd

D

−

1

where k = O
w, w∗)

α2Rβ log2
BA
ǫLR
ǫ, with probability 1

(cid:16)

D
θ(

(cid:0)

≤

, runs in poly(N, d) time, and computes a vector
δ.
(cid:1)(cid:17)
−

b

b

14

Note here that we do not need the U bounded assumption for Theorem 4.1. This corresponds
to an anti-concentration assumption.
If we have this additional property, we immediately get
Theorem 4.2, which is the main result of this paper. Speciﬁcally, with this additional structure on
the distribution, one can translate the small angle guarantee of Theorem 4.1 to the zero-one loss of
the hypothesis that our algorithm outputs.

1
}

× {±

Theorem 4.2 (PAC-Learning under (L, R, U, B, β)-bounded distributions). Let
on Rd
x on Rd is (L, R, U, B, β)-bounded. Moreover, let w∗ ∈
D
halfspace. There exists an algorithm that draws N = dO(k) log (1/δ) examples from
α2Rβ log2
B U A
O
ǫLRβ
ǫ, with probability 1

be a distribution
that satisﬁes the Tsybakov noise condition with parameters (α, A) and the marginal
1 be the normal vector to the optimal
where k =
x
w, f )
1(h
b

, runs in poly(N, d) time, and computes a vector
δ, where f is the target halfspace.

D
w such that errD
0
−

Sd

≤

D

−

1

(cid:16)

(cid:16)

(cid:17)(cid:17)
−

A corollary of the above theorem is that we can PAC learn halfspaces when the marginal
distribution
x is log-concave. The following known fact (see, e.g., Fact A.4 of [DKTZ20]) shows
that the family of log-concave distributions is indeed (L, R, U, B, β)-bounded for constant values of
the parameters.

D

b

Fact 4.3. An isotropic log-concave distribution on Rd is (2−
an absolute constant.

12, 1/9, e217, c, 1)-bounded, where c is

From Thereom 4.2 and Fact 4.3, we obtain the following corollary.

1
}

×{±

Corollary 4.4 (PAC-Learning under Isotropic Log-Concave Distributions). Let
be a distribution
on Rd
that satisﬁes the Tsybakov noise condition with parameters (α, A) and the marginal Dx
is an isotropic log-concave distribution. There exists an algorithm that draws N = dO(k) log (1/δ)
w
examples from
x
such that errD
0
−

α2 log2 (A/ǫ)
ǫ, with probability 1
(cid:1)

, runs in poly(N, d) time, and computes a vector

b
We now provide a high-level sketch of the proof of Theorem 4.1 for constant values of the
parameters L, R, B, and β. For every candidate halfspace w, that has angle greater than ǫ with
the optimal hypothesis vector w∗, our main structural result, Theorem 3.2, guarantees that there
exists a polynomial p of degree k = O((log(1/ǫ)/α)2) such that

where k = O
w, f )
b

δ, where f is the target halfspace.

D
1(h

−

≤

D

(cid:0)

1

E
(x,y)

∼D

[p2(x)1B(x)

w, x
i

h

y]

≤ −

Ω(ǫ) .

Moreover, from Lemma 3.8, we get that, given a candidate w, we can compute a witnessing
polynomial p in time dO(k). The next step is to use the certiﬁcate to improve the candidate w. We
are going to use Online Projected Gradient Decent (OPGD) to do this.

Lemma 4.5 (see, e.g., Theorem 3.1 of [Haz16]). Let
V ⊆
diameter K. Let ℓ1, . . . , ℓT be a sequence of T convex functions ℓt :
wℓtk2. Pick any w1 ∈ V
sets containing
Then, for all u

, and let G = maxt
[T ] k∇
∈
, we have that

Rn a non-empty closed convex set with
R diﬀerentiable in open
[T ].

V 7→
and set ηt = K
G√t

for t

∈

V
∈ V

T

(ℓt(wt)

Xt=1

ℓt(u))

−

3
2

≤

GK√T .

In particular, let pt be the re-weighting function returned by Lemma 3.8 for a candidate w(t). If
w(t) = 0, we set pt to be the zero function. The objective function that we give to the online gradient

15

descent algorithm, in the t-th step, is an estimator of ℓt(w(t)) =
y],
where λ is a non-negative parameter. Using ℓt, we perform a gradient update and project to get
a new candidate w(t+1). The OPGD guarantees that after roughly dΘ(k) steps, there exists a t,
where the value of function ℓt for our candidate is close to the value of the optimal one. From
Theorem 3.2, we know that this is possible only if the angle between the candidate and the optimal
is less than ǫ. For each iteration t, Step 15 of Algorithm 1 uses the OPGD algorithm, and the
remaining steps are used to calculate the function ℓt.

[(pt(w) + λ)

w, x
i

E(x,y)

∼D

−

h

3:

4:

5:
6:

7:

8:

9:
10:

11:
12:

13:

14:

15:

Algorithm 1 Learning Halfspaces with Tsybakov Noise
1: procedure ALG(ǫ, δ)
2:

w(0)
k

←

e1
α2Rβ log2

1

←
Θ
dΘ(k)
(cid:16)

T
(cid:0)
for t = 1, . . . , T do

←

BA
ǫLR

(cid:1)(cid:17)

ηt ←
If w(t

1
dΘ(k)√t
1) = 0 then

−

⊲ ǫ: accuracy, δ: conﬁdence

1)/

w(t

−

1)

2

⊲ Lemma 3.9

Else

0

pt ←
pt gets the output of SDP (4) with input w(t
−
= 0 then

−

1)

If SDP fails and w(t
1)
return w(t
−
Draw N = dΘ(k) log (T /δ) samples
Set ˆℓt(w) according to Lemma 4.6
w(t
1)
w(t)

w(t

w ˆℓt

−

1)

−

ηt∇

−

Π
V

←

(cid:16)

(cid:1)(cid:17)
, i.e., the unit ball with respect the

(cid:0)

(cid:13)
(cid:13)
(x(1), y(1)), . . . , (x(N ), y(N ))
}
{

(cid:13)
(cid:13)

from

D

⊲

=

x
{

∈

V

Rd :

x
k2 ≤

k

1
}

V

For the set

k·k2, the diameter K equals to 2. We are going
to show that in fact the optimal vector w∗ and our current candidate vector w(t) have indeed a
separation in the value of ℓt. Because we do not have access to ℓt to optimize, we need a function ˆℓt,
which is close to ℓt with high probability. The following lemma, which is proven in Appendix A.3,
gives us an eﬃcient way to compute an approximation ˆℓt of ℓt.
Lemma 4.6 (Estimating the function ℓt). Let pt(x) be the non-negative function, given from the
SDP (4). Then taking dO(k) log(1/δ) samples, where k = O
, we can eﬃciently
compute a function ˆℓt(w) such that with probability at least 1
(cid:16)
−
[(pt(x) + λ)y

α2Rβ log2
δ, the following conditions hold

ǫ, for any λ > 0 and w

ˆℓt(w)

BA
ǫLR

(cid:1)(cid:17)

E(x,y)

(cid:0)

,

1

w, x
i
h

]
| ≤

∈ V

• |

−

w ˆℓt

∼D
dO(k) .

∇

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

2 ≤

•
The last thing we need to proceed to our main proof is to show that when the Algorithm 1 in
Step 10 returns a function pt, then there exists a function ℓt for which our current candidate vector
w(t) and the optimal one w∗ are not close.
Lemma 4.7 (Error of ℓt). Let w(t) be a vector in

and w∗ be the optimal vector. Let gt(x) =
], where pt(x) is a non-negative function such that

V

(pt(x) + λ) and ℓt(w) = E(x,y)

gt(x)yx, w

i

θR
16 and λ a non-negative parameter. Then it holds

2

−
E(x,y)

∼D

[pt(x)y

w(t), x

]

(cid:10)

ℓt (w∗)

≤ −
R
2

λ

(cid:13)
C A
(cid:13)
α

(cid:11)
≤ −

[
h
∼D
w(t)

1/α

(cid:13)
R L
(cid:13)
2

(cid:19)

(cid:18)

and ℓt(w(t))

≥

16

Rθ
16 −

λ

.

(cid:19)

w(t)

(cid:13)
(cid:13)
(cid:13)

(cid:18)

2
(cid:13)
(cid:13)
(cid:13)

6
Proof. Without loss of generality, let w∗ = e1. From Fact 3.1 and the deﬁnition of η(x), for every
2η(x))]. To bound from above the expectation,
t
≤ −
−
we use the (L, R, B, β)-bound properties. We have

[T ], it holds ℓt(w∗)

w∗, x

λ Ex

(1

∼D

i |

| h

x[

∈

x

E
∼D

[
| h

x

w∗, x

(1

i |

2η(x))]

−

R
2

≥

R

R/2

Z

(1

2η(x1))γ(x1)dx1 ≥

−

R
2

C A
α

R L
2

(cid:18)

(cid:19)

1/α

,

where in the last inequality we used Lemma 3.3. Thus, ℓt (w∗)
Lemma 3.2, we have that

≤ −

λ R

2 C A
α

R L
2

1/α

. From

(cid:0)

(cid:1)

ℓt(w(t)) =

−

E
(x,y)

(pt (x) + λ)

w(t), x

y

w(t)

∼D h
Rθ
16 −

2

≥

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

λ

r

x

E
∼D

x

D

E
w(t), x

i
2

h(cid:10)

i

(cid:11)

x

Rθ
16 −
Rθ
16 −

(cid:18)

w(t)

2

(cid:13)
(cid:13)
w(t)
(cid:13)

≥

(cid:13)
(cid:13)
(cid:13)
≥

(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)

λ

,

(cid:19)

E
∼D

λ

w(t), x

y

x

h

D

E

i

where we used the Cauchy-Schwarz inequality and the fact that x is in isotropic position.

We are now ready to prove our main results.

Proof of Theorem 4.1. We start by setting all the parameters that we use in the proof. Let

k = Θ

1

α2Rβ log2

BA
ǫLR

and ǫ′ = ǫ R2

256 C A
α

R L
2

1
α . Assume, in order to reach a contradiction,

w(t), w∗
(cid:1)(cid:17)

(cid:1)

(cid:0)

(cid:0)

(cid:0)

(cid:16)

≥

that for all steps t, θ
rithm in Step 10. Then, from Lemma 3.9, we have that E(x,y)
(cid:1)
Let ˆℓt(w) be as in Lemma 4.6. Then ℓt (w) = E[ ˆℓt(w)] =
using Lemma 4.6, for N = dO(k)
ǫ′2

ǫ. Let pt(x) be the non-negative function output by the algo-
2 ǫ R
[pt(x)y
16 .
]. Now
(cid:13)
(cid:13)
δ
2T
≤
32 , in each step t we have

samples, we have Pr
log
2T . From Lemma 4.7, for λ = ǫ R

w(t), x
]
(pt(x) + λ) yx, w
(cid:11)
i
ℓt(w(t))
ǫ′

[
(cid:10)
h
∼D
ˆℓt(w(t))
|

(cid:0)
4ǫ′. From Lemma 4.5, for G = dO(k) and K = 2, we get

ℓt(w∗)
| ≥
≤
R
32 ǫ and ℓt (w∗)

(cid:13)
(cid:13)
| ≥

E(x,y)

w(t)

≤ −

∼D

−

−

ǫ′

T
δ

i

h

i

(cid:1)

δ

and Pr
ℓt(w(t))

ˆℓt(w∗)
|
w(t)

h
≥

−

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤ −
ˆℓt

T

T

w(t)
(cid:1)T

(cid:0)

−

ˆℓt (w∗)
T

3dO(k)
√T

.

≤

t=1
X
By the union bound, it follows that with probability at least 1

t=1
X

δ, we have that

−

T

ℓt

T

w(t)
(cid:1)T

(cid:0)

−

ℓt (w∗)
T

≤

3dO(k)
√T

+ 2ǫ′ .

−

Xt=1

T
t=1 ℓt

Xt=1
Thus, if the number of steps is T = dΘ(k)/ǫ′
1
w(t)
ℓt (w∗)
ℓt (w∗)
T
≤
4ǫ′. Using the
3ǫ′, which implies that ℓt
contrapositive of Theorem 3.2, it follows that Step 10 does not return a witnessing function and
also the w(t) is not zero because then ℓt(w(t)) = 0, which lead us to a contradiction. Therefore,
we have that for the last t it holds θ
ǫ. Moreover, the number of samples is O(T N ) =
(dk)O(k) log(1/δ), and since k is smaller than the dimension we use dO(k) log(1/δ) samples.
(cid:1)

2 then, with probability at least 1
[T ] such that ℓt
ǫ′ because from Lemma 4.7 it holds ℓt (w∗)

3ǫ′. This means that there exists t

δ we have that,
w(t)

w(t), w∗

≤
w(t)

(cid:0)
≤ −

P

−

−

≤

−

<

∈

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

To prove the Theorem 4.2, we need the following claim for the (L, R, U, B, β)-bounded distri-

butions.

17

Claim 4.8 (Claim 2.1 of [DKTZ20]). Let
x

Then, for any 0 < ǫ

1, we have that errD
0
−

≤

≤
Proof of Theorem 4.2. We run Algorithm 1 for ǫ′ = ǫβ2
2U
outputs a ˆw such that θ( ˆw, w∗)
This completes the proof.

ǫβ2
2U

≤

1

x be an (L, R, U, B, β)-bounded distribution on Rd.
θ(v, u) + ǫ .

U

D
1(hu, hv)

log2( B
ǫ )
β2

·

1

2 log(1/ǫ) . From Claim 4.8, we have that err0
−

log(2/ǫ) . From Theorem 4.1, Algorithm 1
ǫ.

1(h ˆw, f )

≤

References

[ABHU15] P. Awasthi, M. F. Balcan, N. Haghtalab, and R. Urner. Eﬃcient learning of linear
separators under bounded noise. In Proceedings of The 28th Conference on Learning
Theory, COLT 2015, pages 167–190, 2015.

[ABHZ16] P. Awasthi, M. F. Balcan, N. Haghtalab, and H. Zhang. Learning and 1-bit compressed
In Proceedings of the 29th Conference on Learning

sensing under asymmetric noise.
Theory, COLT 2016, pages 152–192, 2016.

[ABL17]

P. Awasthi, M. F. Balcan, and P. M. Long. The power of localization for eﬃciently
learning linear separators with noise. J. ACM, 63(6):50:1–50:27, 2017.

[BBL05]

S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classiﬁcation: a survey of some
recent advances. ESAIM: Probability and Statistics, (9):323–375, 2005.

[BBT07] M.-F. Balcan, A. Z. Broder, and T.Zhang. Margin based active learning. In Learning
Theory, 20th Annual Conference on Learning Theory, COLT 2007, volume 4539 of
Lecture Notes in Computer Science, pages 35–50. Springer, 2007.

[BJM06] P. L. Bartlett, M. I. Jordan, and J. D. Mcauliﬀe. Convexity, classiﬁcation, and risk

bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.

[Dan15]

[Dan16]

[DGJ+10]

[DGT19]

A. Daniely. A PTAS for agnostically learning halfspaces. In Proceedings of The 28th
Conference on Learning Theory, COLT 2015, pages 484–502, 2015.

A. Daniely. Complexity theoretic limitations on learning halfspaces.
In Proceedings
of the 48th Annual Symposium on Theory of Computing, STOC 2016, pages 105–117,
2016.

I. Diakonikolas, P. Gopalan, R. Jaiswal, R. Servedio, and E. Viola. Bounded indepen-
dence fools halfspaces. SIAM J. on Comput., 39(8):3441–3462, 2010.

I. Diakonikolas, T. Gouleakis, and C. Tzamos. Distribution-independent pac learning of
halfspaces with massart noise. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Sys-
tems 32, pages 4751–4762. Curran Associates, Inc., 2019.

[DKK+16] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proceedings
of FOCS’16, pages 655–664, 2016.

18

[DKK+17] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Being
robust (in high dimensions) can be practical. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, pages 999–1008, 2017.

[DKK+18] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Ro-
bustly learning a gaussian: Getting optimal error, eﬃciently.
In Proceedings of the
Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018,
pages 2683–2702, 2018.

[DKK+19] I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and Alistair Stewart. Sever:
A robust meta-algorithm for stochastic optimization. In Proceedings of the 36th Inter-
national Conference on Machine Learning, ICML 2019, pages 1596–1606, 2019.

[DKN10]

I. Diakonikolas, D. M. Kane, and J. Nelson. Bounded independence fools degree-2
threshold functions. In FOCS, pages 11–20, 2010.

[DKS18]

[DKS19]

I. Diakonikolas, D. M. Kane, and A. Stewart. Learning geometric concepts with nasty
noise.
In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2018, pages 1061–1073, 2018.

I. Diakonikolas, W. Kong, and A. Stewart. Eﬃcient algorithms and lower bounds for
robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2019, pages 2745–2754, 2019.

[DKTZ20] I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zariﬁs. Learning halfspaces with
massart noise under structured distributions. arXiv, February 2020. Available at
https://arxiv.org/abs/2002.05632. To appear in COLT’20.

[DKZ20]

I. Diakonikolas, D. M. Kane, and N. Zariﬁs. Near-optimal sq lower bounds for agnosti-
cally learning halfspaces and relus under gaussian marginals. Manuscript, 2020.

[FGKP06] V. Feldman, P. Gopalan, S. Khot, and A. Ponnuswami. New results for learning noisy

parities and halfspaces. In Proc. FOCS, pages 563–576, 2006.

[FS97]

Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and
an application to boosting. Journal of Computer and System Sciences, 55(1):119–139,
1997.

[GGK20]

S. Goel, A. Gollakota, and A. Klivans. Statistical-query lower bounds via functional
gradients. Manuscript, 2020.

[GR06]

[Han11]

[Hau92]

[Haz16]

V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proc.
47th IEEE Symposium on Foundations of Computer Science (FOCS), pages 543–552.
IEEE Computer Society, 2006.

S. Hanneke. Rates of convergence in active learning. Ann. Statist., 39(1):333–361, 02
2011.

D. Haussler. Decision theoretic generalizations of the PAC model for neural net and
other learning applications. Information and Computation, 100:78–150, 1992.

E. Hazan. Introduction to online convex optimization. Foundations and Trends R
(cid:13)
Optimization, 2(3-4):157–325, 2016.

in

19

[HY15]

S. Hanneke and L. Yang. Minimax analysis of active learning. J. Mach. Learn. Res.,
16:3487–3602, 2015.

[KKM18] A. R. Klivans, P. K. Kothari, and R. Meka. Eﬃcient algorithms for outlier-robust

regression. In Conference On Learning Theory, COLT 2018, pages 1420–1430, 2018.

[KKMS08] A. Kalai, A. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces.

SIAM Journal on Computing, 37(6):1777–1805, 2008.

[KLS09]

A. Klivans, P. Long, and R. Servedio. Learning halfspaces with malicious noise. To
appear in Proc. 17th Internat. Colloq. on Algorithms, Languages and Programming
(ICALP), 2009.

[KSS94] M. Kearns, R. Schapire, and L. Sellie. Toward Eﬃcient Agnostic Learning. Machine

Learning, 17(2/3):115–141, 1994.

[LRV16] K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In

Proceedings of FOCS’16, 2016.

[MH02]

J. C Mason and D. C Handscomb. Chebyshev polynomials. CRC press, 2002.

[MN06]

[MP68]

P. Massart and E. Nedelec. Risk bounds for statistical
34(5):2326–2366, 10 2006.

learning. Ann. Statist.,

M. Minsky and S. Papert. Perceptrons: an introduction to computational geometry.
MIT Press, Cambridge, MA, 1968.

[MT94] W. Maass and G. Turan. How fast can a threshold gate learn? In S. Hanson, G. Drastal,
and R. Rivest, editors, Computational Learning Theory and Natural Learning Systems,
pages 381–414. MIT Press, 1994.

[MT99]

[MV19]

E. Mammen and A. B. Tsybakov. Smooth discrimination analysis. Ann. Statist.,
27(6):1808–1829, 12 1999.

O. Mangoubi and N. K. Vishnoi. Nonconvex sampling with the metropolis-adjusted
langevin algorithm. In Conference on Learning Theory, COLT 2019, pages 2259–2293,
2019.

[Nov62]

A. Novikoﬀ. On convergence proofs on perceptrons. In Proceedings of the Symposium
on Mathematical Theory of Automata, volume XII, pages 615–622, 1962.

[Ros58]

[Slo88]

[Tsy04]

[Val84]

F. Rosenblatt. The Perceptron: a probabilistic model for information storage and
organization in the brain. Psychological Review, 65:386–407, 1958.

R. H. Sloan. Types of noise in data for concept learning. In Proceedings of the First
Annual Workshop on Computational Learning Theory, COLT ’88, pages 91–96, San
Francisco, CA, USA, 1988. Morgan Kaufmann Publishers Inc.

A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of
Statistics, 32(1):135–166, 2004.

L. G. Valiant. A theory of the learnable. In Proc. 16th Annual ACM Symposium on
Theory of Computing (STOC), pages 436–445. ACM Press, 1984.

20

[Vap98]

V. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.

[YZ17]

[ZLC17]

S. Yan and C. Zhang. Revisiting perceptron: Eﬃcient and label-optimal learning of half-
spaces. In Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, pages 1056–1066, 2017.

Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient
langevin dynamics. In Proceedings of the 30th Conference on Learning Theory, COLT
2017, pages 1980–2022, 2017.

[ZSA20]

C. Zhang, J. Shen, and P. Awasthi. Eﬃcient active learning of sparse halfspaces with
arbitrary bounded noise, 2020.

21

A Omitted Proofs

A.1 Proof of Lemma 3.3

Lemma 3.3. Let
with parameters (α, A). Then for every measurable set S
C A

be a distribution on Rd

α , where C A

x[1S(x)])

1−α
α .

× {±

1
}

α = α

α (Ex

D

α

1

1

−
A

∼D
Proof. We have

(cid:0)

(cid:1)

that satisﬁes the Tsybakov noise condition

Rd it holds Ex

∼

⊆

Dx[1S(x)(1

2η(x))]

−

≥

x

E
Dx
∼

[1S(x)(1

2η(x))]

−

≥

≥

≥

Dx

Dx

t E
x
∼
t E
x
∼
t E
x
∼

Dx

[1S(x)1

1
{

[1S(x)]

[1S(x)]

−

−

2η(x)

t

]
≥
}
[1S(x)1

−
t E
x
∼
A t

Dx
1
1−α .

1
{

2η(x)

t

]
}

≤

−

Let A = Ex

∼D

x[1S(x)] and set t =

(1

α)A
−
A

1−α

α . Then we have

(cid:16)
[1S(x)(1

x

E
Dx
∼

(cid:17)

2η(x))]

−

≥

A1/αα

1−α
α

.

α

1

−
A

(cid:18)

(cid:19)

A.2 Proof of Fact 3.5 and Lemma 3.6

Fact 3.5. We denote by Tk(t) the degree-k Chebyshev polynomial of the ﬁrst kind. It holds

Tk(t) =




cos(k arccos t) ,

1
2

−

√t2

t
(cid:18)(cid:16)
26k+2 log k+4.

−

k

1

+

t + √t2

(cid:17)

(cid:16)

1

−

k

,

(cid:19)

(cid:17)

t
|
t
|

| ≤

| ≥

1

1 .

Moreover, it holds

Proof. Using that

Tkk
k
Tkk


2
2 ≤
2
2 ≤ k

k

Tkk

2
1, we are going to show that

Tkk
k

2
1 ≤

26k+2 log k+4. We have that

Tk(t)
k

k1 =

k
2

k

i

−
i

1

k

(cid:19)

−

(cid:18)

xi

i

≤

F ib(k + 1)2k k

2 ≤

1 + √5

k+1

2kk ,

(cid:16)

(cid:17)

k

⌊

2 ⌋

2i

2k

−

Xi=1
k

k

i

−
i

2 ⌋i=1
⌊

where we used that

(cid:0)
Lemma 3.6. Let p(t) =
w
k

26k+2 log k+4.

= F ib(k + 1). Thus,

2
1 ≤
(cid:1)
k
i=0 citi be a degree-k univariate polynomial. Given w
k CSxS.
w, x
) =
S:
S
i
|
k
i=0 diti for some a, b

1, deﬁne the multivariate polynomial q(x) = p(
h
i . Moreover, let r(t) = p(at + b) =

k
i=0 c2

Tkk

d2k

P

P

|≤

k

P

∈

Rd with
It holds,
R. Then

∈

k2 ≤
S:
S
|
|≤
2
r
P
2 ≤
k
k
Proof. We write

S ≤

k C 2
(2 max(1, a) max(1, b))2k
P

p

2
2 .
k

k

q(x) =

k

Xi=0

ci h

i =
w, x
i

k

ci

Xi=0

=i
S
XS:
|
|

i!
S!

d

Yi=1

22

P

k

(xiwi)Si =

ci

Xi=0

=i
S
XS:
|
|

i!
S!

wSxS .

We have

k

c2
i

Xi=0 XS:
=i
S
|
|
where we used the fact that
have

(cid:18)
wi| ≤
|

k

i

i!
S!

2

w2S

(cid:19)

k

≤

Xi=0

c2
i 



2

i!
S! 

d2k

≤

c2
i ,

k

Xi=0

=i
S
XS:
|
|



1 for all i. To prove the second claim, we work similarly. We

r(x) =

ci

Xi=0

Xj=0 (cid:18)

i
j

(cid:19)

k

i

ajbi

−

jxj =

ci

Xi=0

Xj=0 (cid:18)

ajbi

−

jxj.

i
j

(cid:19)

We have

k

i

c2
i

Xi=0

Xj=0 (cid:18)(cid:18)

ajbi

−

j

i
j

(cid:19)

2

≤

(cid:19)

(2 max(1, a) max(1, b))2k

c2
i .

k

Xi=0

A.3 Proof of Lemma 4.6

Lemma 4.6. Let pt(x) be the non-negative function, given from the SDP (4). Then taking
dO(k) log(1/δ) samples, where k = O
, we can eﬃciently compute a function
ˆℓt(w) such that with probability at least 1
(cid:16)

δ, the following conditions hold

α2Rβ log2

BA
ǫLR

1

(cid:0)

(cid:1)(cid:17)

ˆℓt(w)

• |

E(x,y)

∼D

−

[(pt(x) + λ)y

w, x
i
h

ǫ, for any λ > 0 and w

,

∈ V

−
]
| ≤

w ˆℓt

dO(k) .

2 ≤

•

∇

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
N
i=1

Proof. For convenience, let gt(x) = pt(x) + λ. The proof is similar to Lemma 3.8. Let ˆℓt(w) =
1
]. Then from Cauchy-Schwarz we
N
have
P

and ℓt(w) = E(x,y)

gt(x)yx, w
[
h

gt(x(i))y(i)x(i), w

∼D

(cid:10)

N

i

(cid:11)
ℓt(w)

ˆℓt(w)
|

−

1
N

| ≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

gt(x(i))y(i)x(i)

−

E
(x,y)

∼D

Xi=1

w
k

k2 .

[gt(x)yx]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

We have that

w

k

k2 ≤

1, thus we need to prove that

N

gt(x(i))y(i)x(i)

Pr

1
N

"(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi=1
[m(x)m(x)T 1B(x)]xj and
Let Mj = E(x,y)
A be a matrix such that tr (AMj) = E(x,y)
polynomial and assume that
Lemma 3.8, we get

kF ≤

∼D

A

k

−

E
(x,y)

∼D

[gt(x)yx]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

> ǫ

# ≤

δ .

2

(9)

N

i=1 m(x(i))m(x(i))T 1B(x(i))x(i)
Mj = 1
j , and then
N
[pt(x)yxj ], i.e., the matrix of the coeﬃcients of the
Q, where Q = dO(k). Using the same proof ideas as in

g

P

∼D

tr

A(Mj −
(cid:16)

Mj)

A

≤ k

(cid:17)

Therefore, it suﬃces to bound the probability that
inequality, we have

g

Pr

Mj −
h(cid:13)
(cid:13)
(cid:13)

Mj

g

F ≥

(cid:13)
(cid:13)
(cid:13)

ǫ/(2dQ)

i

≤

23

ǫ/(2dQ). From Markov’s

kF

Mj −
(cid:13)
(cid:13)
Mj −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
4d2Q2
ǫ2

E

Mj

.

F

g
Mj

g

(cid:13)
(cid:13)
(cid:13)
F ≥
(cid:13)
(cid:13)
(cid:13)
Mj −

(cid:20)(cid:13)
(cid:13)
(cid:13)

Mj

g

2

F

.

(cid:21)

(cid:13)
(cid:13)
(cid:13)

Using Equation (7) (which holds in our case as well and is proved the same way by setting w = ej),
we get

Mj

Pr

Mj −
Bd3Q2(β/2)−

h(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

ǫ/(2dQ)

F ≥

i

Then, for N
accuracy with probability at least 1

≥

g

2k(d + k)3k+1/(4ǫ2) samples we can estimate Mj within the target

1/(8d). Now we are going to give a loose bound for the

−

4d2Q2
ǫ2

1
N

≤

B(β/2)−

2k(d + k)3k+1 .

Using the same argument as before, we have from Markov’s inequality, that

1
N

N

Xi=1

λy(i)x(i)

−

E
(x,y)

∼D

[λyx]

> ǫ

# ≤

δ .

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Pr

"(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N

Xi=1

y(i)x(i)

−

E
(x,y)

∼D

[yx]

ǫ/(2dλ)

# ≤

≥

2

4d2λ2
ǫ2

E

Using the linearity of expectation, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N

Xi=1





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

y(i)x(i)

−

E
(x,y)

∼D

.

2

2





[yx]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N

Xi=1

y(i)x(i)

−

E
(x,y)

∼D

[yx]

2

2



≤



d

Xj=1

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Then, using the fact that x is in isotropic position, we have

1
N

N

Xi=1

y(i)x(i)

j −

E
(x,y)

∼D

[yxj]

!

2

d



≤



Xj=1

Var

1
N

"

N

Xi=1

y(i)x(i)
j

.

#

Pr

"(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Var

1
N

"

N

Xi=1

y(i)x(i)
i

1
N

E
(x,y)

∼D

# ≤

[(x(i)

i y)2] = 1/N .

Thus, for N > 4d3λ2/ǫ2, with probability at least 1

1/8, we have that

−

N

1
N

λy(i)x(i)

E
(x,y)

−

[λyx]

ǫ/2 .

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Putting everything together and by the union bound, we have that for N > max(Bd3Q2(β/2)−
(cid:13)
k)3k+1/(4ǫ2), 4d3λ2/ǫ2), with probability 3/4, we have that

Xi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∼D

2

2k(d+

1
N

N

Xi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

gt(x(i))y(i)x(i)

−

E
(x,y)

∼D

[pt(x)yx]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

1
N

1
N

N

Xi=1
N

Xi=1

+

≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
−

pt(x(i))y(i)x(i)

−

E
(x,y)

∼D

λy(i)x(i)

−

E
(x,y)

∼D

[λyx]

ǫ/2 + ǫ/2 = ǫ .

[gt(x)yx]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

≤

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

To amplify the conﬁdence probability to 1
(1)

(ℓ)

δ, we can use the above empirical estimate ℓ times

to obtain estimates
∈
follows that ℓ = O(log(d/δ)) repetitions suﬃce to guarantee conﬁdence probability at least 1
To prove the second statement, from Equation (9), we have that with probability 1

[d] and keep the median as our ﬁnal estimate. It
δ.

for all j

, . . . ,

g

g

−

δ

Mj

Mj

(cid:13)
(cid:13)
where we used Theorem 3.2. This completes the proof.
(cid:13)

(cid:13)
(cid:13)
(cid:13)

w ˆℓt

∇

2 ≤ k∇

wℓtk2 + ǫ

≤

dO(k) + ǫ = dO(k) ,

24

 
