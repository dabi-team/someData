1
2
0
2

v
o
N
6
1

]

G
L
.
s
c
[

3
v
7
8
7
2
1
.
5
0
1
2
:
v
i
X
r
a

Self-Supervised Bug Detection and Repair

Miltiadis Allamanis, Henry Jackson-Flux∗, Marc Brockschmidt
Microsoft Research, Cambridge, UK
{miallama, mabrocks}@microsoft.com

Abstract

Machine learning-based program analyses have recently shown the promise of
integrating formal and probabilistic reasoning towards aiding software develop-
ment. However, in the absence of large annotated corpora, training these analyses
is challenging. Towards addressing this, we present BUGLAB, an approach for self-
supervised learning of bug detection and repair. BUGLAB co-trains two models:
(1) a detector model that learns to detect and repair bugs in code, (2) a selector
model that learns to create buggy code for the detector to use as training data. A
Python implementation of BUGLAB improves by up to 30% upon baseline meth-
ods on a test dataset of 2374 real-life bugs and ﬁnds 19 previously unknown bugs
in open-source software.

1

Introduction

Detecting and repairing bugs in source code requires strong reasoning skills over formal structures
(e.g. data and control ﬂow) and ambiguous information (e.g. identiﬁer names, coding idioms, and
comments). Traditional program analyses are able to detect critical bugs through formal reasoning
and combinatorial search, but need to be manually coded by experts. That is a lengthy and costly
process, which misses the opportunity to use ambiguous information pervasive within code.

Towards broadening the applicability of such methods, and utilizing ambiguous information, deep
learning-based bug detection methods are being investigated [22, 3, 13]. These methods have the
potential to further improve the engineering of software we rely on every day. However, many
challenges in the area remain open, such as creating robust bug detection and repair methods that
cover a wide range of common bugs in the absence of large supervised training corpora. Existing
work focuses on randomly inserted bugs [22, 13], Cloze test proxy tasks [3], corpora of small code
edits that may contain bugs [9] or build errors [28]. All these approaches rely on datasets of very
limited size or ones known not to be representative of the characteristics of bugs found in real code.

In this work, we propose BUGLAB, a self-supervised approach that trains robust bug detectors by
co-training a bug selector that learns to create hard-to-detect bugs (Sec. 2). For example, for a
given code snippet with two well-named variables, a variable misuse bug may be easy to detect
and repair, whereas an incorrect comparison operator might be signiﬁcantly harder to identify. We
propose a neural architecture for BUGLAB (Sec. 3) and implement it for Python (Sec. 4). Our
implementation considers four broad classes of seemingly simple, yet hard-to-detect bugs and shows
improved performance over training with randomly-inserted bugs on PYPIBUGS, a new, manually
curated test set of 2374 real-life bugs (Sec. 5). Furthermore, we tested our trained models on popular
open-source Python packages and identiﬁed 19 previously unreported bugs, though false positive
rates of ∼ 98% remain impractical. We hope that creating machine learning methods that can detect
these bugs early and assist developers will speed up software development and allow engineers
to deliver more robust software. We release PyPIBugs and our code at https://github.com/
microsoft/neurips21-self-supervised-bug-detection-and-repair.

∗Work done while at Microsoft Research.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
2 Self-Supervised Bug Detection

In this section, we ﬁrst introduce the concept of code rewriting, and then use it to deﬁne BUGLAB as
a framework for self-supervised learning of bug detection and repair.

Code Rewriting Rewriting is common within compilers and their optimizations, test-driven search-
based bug repair tools, mutation testing, and refactoring tools. Rewrites can be semantics-preserving
(e.g. renamings of local variables), or semantics-altering (e.g. replacing >= by !=).

Let S denote the set of all syntax trees (not necessarily rooted in the start symbol of the language
grammar). Syntax tree locations (cid:96) ∈ {(cid:15)} ∪ N∗ in a syntax tree s ∈ S are recursively deﬁned, where
s|(cid:15) = s and s|(cid:96) for (cid:96) = (cid:96)(cid:48)◦i is the i-th child of s|(cid:96)(cid:48) (i.e. s|(2,3) denotes the third child of the second child
of s). We deﬁne a rewrite rule ρ = (mρ, tρ) as a pair of a matching function mρ : S → {true, false}
and a transformation function tρ : S → S. The matching function mρ(s) yields true iff the rule
ρ is applicable at the root of a subtree s. The transformation function can be applied to obtain a
transformed syntax tree. For convenience, we deﬁne tρ(s) = s iff mρ(s) = false. We then write
ρ(s) to indicate the modiﬁcation of a syntax tree s using ρ when possible, and otherwise the identity
function. For reversible rewrite rules ρ, we denote the inverse rule as ρ−1 such that ρ−1(ρ(s)) = s
holds. We discuss concrete rewrite rules ρ in Sec. 4.
Given a set of rewrite rules R we deﬁne the set of “potential rewrites” in a syntax tree s as RR
s =
(cid:8)(cid:104)(cid:96), ρ(cid:105) | ρ ∈ R, (cid:96) location in s, mρ(s|(cid:96)) = true(cid:9). For each tuple (cid:104)(cid:96), ρ(cid:105) ∈ RR
s , we use s(cid:48) = s[ρ](cid:96) to
denote the new syntax tree obtained by applying ρ at location (cid:96) of s. In BUGLAB, we train models
that use rewrites from RR
s

to insert and repair bugs. We will discuss such neural models in Sec. 3.

BUGLAB In BUGLAB, we are interested in self-
supervised training of a robust bug detector model Dθ
with parameters θ on an unannotated codebase C. Let R
be a set of rewrite rules2 that allows to insert and repair
bugs. We train Dθ to be able to recognize the “hardest”
possible rewrites that could be applied on our codebase C.
For this, we consider the loss LDθ of Dθ on a rewritten
code snippet s[ρ](cid:96), for which the model needs to predict
the repairing rewrite (cid:10)(cid:96), ρ−1(cid:11). Formally, we want to min-
imize the objective

(cid:20)

Es∼C

max
(cid:104)(cid:96),ρ(cid:105)∈RR
s

LDθ

(cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1)

(cid:21)

.

However, for any useful detector the set of rewrites RR
s
is commonly very large or unbounded and computing the
maximum over all (cid:104)(cid:96), ρ(cid:105) ∈ RR
is practically intractable.
s
To address this, BUGLAB introduces a bug selector model
Sφ (with parameters φ), whose goal is to approximate the
intractable max(cid:104)(cid:96),ρ(cid:105)∈RR
LDθ (·). We can then sample rewrites from Sφ instead of computing the
maximum. We denote this as (cid:104)(cid:96), ρ(cid:105) ∼ Sφ(s) and the overall BUGLAB training objective can be
written as a min-max optimization problem:

Figure 1: BUGLAB overview: a se-
lector model Sφ decides which (bug-
introducing) rewrite to apply to an input
code snippet. Then a bug detector Dθ
tries to locate and repair the inserted bug
(if one was inserted).

s

max
φ

min
θ

Es∼C

(cid:2)E(cid:104)(cid:96),ρ(cid:105)∼Sφ(s)

(cid:2)LDθ

(cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1)(cid:3)(cid:3) .

(1)

The two models S and D in BUGLAB are “symmetric” in the sense that they both predict rewrites
on code snippets, and only differ in their objectives — one aiming to introduce bugs and one aiming
to repair them. In practice, we can and do use the same architecture to model both S and D, which
we will discuss in the next section. At test time, we discard S and only use the trained detector D to
locate and repair bugs.

2In this work we assume that R contains a special “identity” rewrite rule ρ∅ that does not change the code.

2

Bug  Selector 𝑆𝜙Bug  Detector 𝐷𝜃LearningOriginal CodeCode w/ introduced bugsDetected Bug3 Neural Models

In this section, we discuss how we represent code in BUGLAB and the neural models we use to learn
how to rewrite code in the selector and detector models.

Code Representation We consider source code as a set of entities vi ∈ V which relate to each
other with a set of typed relations ek ∈ E, where a relation ek = (vi, r, vj) denotes a relationship
between entities vi and vj with type r. The entities and relations can be thought as a heterogeneous
graph G = (V, E). The choice of code entities and their relationships is a form of high-level feature
extraction. We discuss concrete entities and relationships for Python in Sec. 4. We also deﬁne a
projection function Ptok that accepts V and E and returns a sequence Vtok of the token entities
in V with the nodes appearing in relations in E deterministically mapped to elements of Vtok, i.e.
Etok = {(p(vi), r, p(vj))}, where p maps the entities in V to Vtok. Ptok will be used for relational
transformer models.

To learn a neural representation of the code entities vi, ﬁrst we deﬁne an embedding function e(vi)
which maps the content of each entity to an initial D-dimensional representation. Throughout this
work — similar to Allamanis et al. [4] and other previous work — we deterministically split the
string representation of each node into subtokens (e.g., fooBar is split into foo and bar), embed
them through a learned embedding matrix, and use max pooling to get a single vector. We then
“contextualize” the entity representations within G using one of two models: a MLP-based GNN
model with max message aggregation and the GREAT relational transformer of Hellendoorn et al.
[13] over the token sequence and relations Vtok, Etok = Ptok(V, E). GREAT uses both positional
encodings and the projected relations in Etok. See Appx. A for detailed architecture descriptions.
Other models to compute entity representations can be used, but were not explored in this work.

We use r(cid:96) to denote the computed vector representation of the entity at location (cid:96), independent of
the model used to produce it. We use these representations to deﬁne our code rewriting models.

Probabilistic Code Rewriting Models Both bug selection and bug detection require to model the
probability of applying a speciﬁc rewrite at a location in a code snippet s, either to introduce or repair
a bug. For this, we factorize this task into localization and rewrite-given-location models, i.e.

p (cid:0)(cid:104)(cid:96), ρ(cid:105) | s, RR

(cid:1) = ploc
s } ∪ {NoBug},
We model ploc as a probability distribution over the relevant locations {(cid:96) | (cid:104)(cid:96), ρ(cid:105) ∈ RR
where NoBug is a special location used to indicate that the code is not buggy. In practice, we imple-
ment this similar to a pointer net [19] using the representations r(cid:96) (see Appx. A for details).

(cid:0)ρ | (cid:96), s, RR

(cid:0)(cid:96) | s, RR

(cid:1) prew

(cid:1) .

(2)

s

s

s

To select rewrites, we use rewrite type-speciﬁc learnable rule score functions wρ (r(cid:96), Mρ(s, (cid:96))).
This function maps a vector representation of an entity r(cid:96) and potential additional metadata onto
a scalar score. The rule-speciﬁc metadata Mρ(s, (cid:96)) is deﬁned for some rewrites, e.g. containing
representations of other entities that could be used in the location (cid:96). We will discuss three concrete
rule score functions in Sec. 4. The rewrite probability distribution prew is then modeled by a softmax
over the scores of all applicable rewrites at a target location (cid:96), i.e.
(cid:0)ρ | (cid:96), s, RR

(wρ(cid:48) (r(cid:96), Mρ(cid:48)(s, (cid:96)))) .

prew

(cid:1) = softmax
(cid:104)(cid:96),ρ(cid:48)(cid:105)∈RR
s

s

4 A Python Implementation

This section presents an implementation of BUGLAB for Python called PYBUGLAB. PYBUGLAB
currently tackles a large subset of “stupid simple bugs” [16]. Fixing these bugs requires small changes
to the code, but commonly has signiﬁcant impact on code correctness. Such bugs may be thought
as a form of a typographical mistake or a copy-paste error, and are often relatively hard to locate by
humans but obvious after the fact. They are also quite common, as observed in the empirical statistics
of Karampatsis and Sutton [16] and Just et al. [14]. Future work may focus on a broader set of rewrite
rules or even learnable rewrites, but as we will observe in Sec. 5 more work is needed towards this.
Almost all ideas in PYBUGLAB transfer straightforwardly to other programming languages other
than Python, but would require some engineering effort to implement.

PYBUGLAB Code Entities and Relations
In this work, we follow related literature (see Sec. 6
for more) and extract entities and relationships that are readily available by tokenizers, parsers,

3

def foo(a, b, c=0):
if a[1] in[2] b[3]:

c[4] +=[5] bar(b[7], c[8])[6]
c_is_neg =[9] c[10] <[11] 0[12]
if c is neg[13] or[14] a[15] is[16] int:

return True[17], c[18]

return c[19] >[20] 1[21], c[22]

(cid:15): NoBug
l1: b, c
l2: not in
l3: a, c
l4: a, b
l5: =,-= , *=, /=, //=, %=
l6: bar(c, b)
l7: a, c

l8: a, b
l9: += ,-= , *=, /=, //=, %=
l10: a, b
l11: <=, >, >=, ==, !=
l12:-2 ,-1 , 1, 2
l13: a, b, c, not c_is_neg
l14: and
l15: b, c, c_is_neg

l16: is not
l17: False
l18: a, b, c_is_neg
l19: a, b, c_is_neg
l20: >=, <, <=, ==, !=
l21:-2 ,-1 , 0, 2
l22: a, b, c_is_neg

Figure 2: Code snippet and rewrites available to PYBUGLAB.

existing simple program analyses, or other Python-speciﬁc program analysis tools. The complete
list of entities and relationships can be found in Appx. B and include syntactic entities and relations,
relations about the intraprocedural data and control ﬂow, types, and documentation. Some notable
entities include SyntaxNodes, Tokens, and Symbols (references to variables and functions). Fig. 4
in Appx. B shows a graph of the entities and relationships of the snippet in Fig. 2.

4.1 Bug-Inducing PYBUGLAB Rewrite Rules

PYBUGLAB focuses on four common kinds of bugs. Fig. 2 shows a code snippet and the rewrites
allowed for each location, which number 63 even for this small example.

Variable Misuse Originally deﬁned by Allamanis et al. [3] as a Cloze test for source code, Vasic et al.
[30] and Hellendoorn et al. [13] reformulated the task to localizing a variable misuse bug (if any)
within a snippet and repairing it. PYBUGLAB uses the latter representation. Variable misuse bugs are
common, with 12.8-14.8% found in the ManySStuBs4J corpus [16] and about 6% of them caught
during Java compilation in the Google build system [28]. To insert and repair variable misuse bugs,
PYBUGLAB supports variable-swapping rewrites, such as in locations l1, l3 and l4 (amongst others)
in Fig. 2. To score a variable-swapping rewrite, we use the representation of the rewrite location r(cid:96)
along with the representation rσ of a variable Symbol σ that could replace the current variable, i.e.
is in-scope and has been deﬁned before (cid:96). The rule score function wρ for replacing the variable at (cid:96)
with the symbol σ is then computed as the inner product r(cid:62)

(cid:96) rσ.

Argument Swapping (or Argument Selection) First coined by Rice et al. [26], it refers to swapping
the arguments of a function invocation, e.g. in l6 of Fig. 2. Rice et al. [26] and DeepBugs [22]
tackled this problem when all arguments are single identiﬁers. PYBUGLAB extends this to swapping
arbitrary argument expressions. The rule score function wρ for an argument swapping rewrite is a
two-layer MLP applied to the concatenation of the output representations of the representation of the
parameter and the to-be-swapped arguments arg1, and arg2: MLP (cid:0)[rparams, rarg1, rarg2](cid:1).
Wrong Operator Corrupting operators has a long history in mutation testing [14]. Detecting incor-
rect operators with deep learning was ﬁrst tackled by DeepBugs [22] by using learnable embeddings
of operators, operands and literals for arithmetic and comparison operators. DeepBugs focused only
on binary operators. In PYBUGLAB we tackle all binary operators, including Boolean, arithmetic
and comparison operators and two unary operators: logical and arithmetic negation. Locations l11,
l14, l16, and l20 in Fig. 2 are rewrites related to wrong operators. The rule score function wρ for an
operator rewrite again uses an inner product, r(cid:62)
(cid:96) rop, where rop is a learned embedding for operator
op. Note that we rewrite operators only to compatible operators (e.g. < to > but not +).

Wrong Literal Corrupting operands, and speciﬁcally, literals appearing in the source code, is also a
common strategy in mutation testing. As in mutation testing, PYBUGLAB handles a limited number
of commonly used literals, allowing rewrites to replace integer literals within the set of -2, -1, 0, 1, 2
and swapping the Boolean literal True with False and vice versa. The scoring function is identical
to the operator rewrite, using a learnable embedding rlit for each literal lit.

4.2 PYBUGLAB Rewrite Rules for Data Augmentation

We additionally consider more rewrite rules that are not meant to change the program semantics,
using them as a form of data augmentation. This is in spirit similar to ideas in computer vision where
images are transformed (e.g. rotated, cropped) but maintain their original content. Such rewrites

4

Algorithm 1 Sequential Training Procedure for Selector and Detector models

Require: Code dataset C, initial detector/selector model parameters θ(0), φ(0)
1: for meta-epoch i = 0 to I do
2:

// Create dataset of buggy programs:
D ← (cid:8)(cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1) | s ∈ C, k samples (cid:104)(cid:96), ρ(cid:105) ∼ Sφ(i) (s)(cid:9)
C (i)
θ(i+1) ← update θ(i) by training D on C (i)
D
// Create dataset of hard-to-detect bugs:
C (i)
s, arg max(cid:104)(cid:96),ρ(cid:105)∈RR
φ(i+1) ← update φ(i) by training S on C (i)
S

(cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1)(cid:17)(cid:17)

S ←

LD

θ(i+1)

(cid:110)(cid:16)

(cid:16)

s

3:

4:
5:

6:

7:

(cid:111)

| s ∈ C

have been shown to yield adversarially robust models of code [23]. Although our goal is not to
provide adversarial robustness, we believe that such rewrites can help generalization. PYBUGLAB
implements the following rewrites for this purpose:

• Variable Renaming renames a local variable to a random name not already in scope.

• Comment Deletion removes code comments, including docstrings and inline comments. Such
comments commonly contain natural language information that is useful for code comprehension,
but usually do not affect program semantics.

• Comparison Expression Mirroring swaps the two sides of a comparison operator and changes it
appropriately. For example, a<b is transformed to b>a. Note that in cases such as foo() < bar(),
this will change the order of execution of foo and bar, possibly altering program semantics.

• If-Else Branch Swapping negates the test condition of an if-else statement or a ternary expres-

sions using DeMorgan’s law and swaps the then body with the else body.

4.3

Implementation Details

To make the training computationally tractable we approximate Eq. 1. A simpliﬁed, sequential
version of our training procedure is shown in Alg. 1. Intuitively, we alternate between training the
two models, as the (discrete) sampling of rewrite rules in the selector models precludes direct end-
to-end training. We ﬁrst use the current state of the selector model to generate “hard” samples and
train the detector model on these samples (we always include the unmodiﬁed (i.e., NoBug case) as a
sample). Then, we use the loss of the detector model to identify those generated samples that were
hardest to detect and train the selector model to produce such samples.

In practice, we implemented the training procedure as a system of asynchronously communi-
cating processes, and all of the described steps happen in parallel. We do not use “generations”
D/S, C (1)
C (0)
D/S, . . . of datasets, but instead use two constantly updated “pools” of training data, one
for the detector and one for the selector. Each training step samples a minibatch from the current
state of the corresponding data pool. We remove samples from the data pool once they have been
sampled ν times for use in training, in spirit similar to replay buffers in reinforcement learning. In
our experiments, ν was set to 4. We regularly (in separate, concurrent processes) take snapshots of
the the current state of the D and S models to generate new elements that are updated to the data
pools, matching the procedure described in Alg. 1. We approximate the arg max in line 6 by only
considering the k samples chosen in line 3 for each input program. During training of S, we then
mask out the unobserved choices before computing the loss.

5 Evaluation

We now discuss our new dataset and evaluate PYBUGLAB. We

results.
highlight.....key........
..........

Datasets To train PYBUGLAB we retrieve the 4k most downloaded packages in the Python package
index (PyPI) and take 3.4k of them as training packages, using the rest for test purposes. During
training, PYBUGLAB installs each package along with all its dependencies. Installing all the depen-
dencies is important for extracting the entities and the relations beyond local syntactic ones (e.g. type

5

inference, method resolution). For each ﬁle, PYBUGLAB checks if it is a duplicate of a ﬁle that has
already been seen in the training following the method of Allamanis [1] and runs all the relevant
program analyses to extract the entities and relationships in each function. When we use additional
rewrites for data augmentation, these are applied at the input of the PYBUGLAB pipeline as a form
of pre-processing. Following Alg. 1, the bug selector S selects k = 5 bugs to introduce, rewrites
the source code text, and then the program analyses extract the new entities and relationships for the
rewritten code snippets. The initial and rewritten code snippets are then used to create the training
data for the detector and selector models.

We use two testsets to measure performance. First, we create RANDOMBUGS, a testset of 761 445
snippets derived from functions from the 600 PyPI test packages (not seen during training). For each
function we ﬁnd within these packages we add it to the dataset along with 9 rewritten functions with
a randomly inserted bug. On average graphs have 260 nodes, 601 edges, 25 rewrite locations, and
130 possible rewrites. We also collect a testset of real bugs. Although we conjecture that, in practice,
the vast majority of bugs like those discussed in Sec. 4.1 are ﬁxed when developers locally test their
software, a few of those slip and then are ﬁxed across different revisions checked into a version control
systems. We have crawled the accessible repositories of all 285k packages in the Python Package
Index (PyPI), collected and manually ﬁltered bugs captured by the rewrites from Sec. 4.1.
.....This......new
bugs. We describe the data collection process
real-world,.......
dataset, ... ........ ... .
.........
in detail in Appx. D. In addition, we consider PYPIBUGS-PostFix: the examples from PYPIBUGS
after a bug was ﬁxed - we believe these samples are very likely to not contain any bugs anymore.
We publish the dataset at https://www.microsoft.com/en-us/download/103554 and include
it in the supplementary material.

PYPIBUGS,... .......

2374 .............

contains... ....

small.......

5.1 Quantitative Evaluation

Our ﬁrst experiment aims to evaluate whether the BUGLAB training framework yields more precise
bug detectors. We consider two model architectures, using either GNNs or the GREAT transformer
to compute embeddings of code entities (architecture details and hyperparameter choices can be
found in Appx. A). We use four different training strategies: “supervised” is training only a bug
detector on a ﬁxed dataset of 1 million functions from the 3.4k training packages with randomly
inserted bugs. “Random Selector” refers to a variant of PYBUGLAB using a bug selector model that
uniformly at random picks a rewrite to insert bugs. Finally, PYBUGLAB and PYBUGLAB +Aug use
our framework from Sec. 2, with the latter also using additional rewrites to augment our code corpus.
For the fully supervised model, we train with early stopping over a validation set; the other models
are trained for a ﬁxed number of 300 epochs (with 200k training samples per epoch) for the bug
detector3 and the last detector model is used for evaluation.

Table 1: Accuracies (%) for different training strategies and
model architectures on RANDOMBUGS.

Effectiveness of BUGLAB Training
We ﬁrst consider the performance
of different models on the synthetic
RANDOMBUGS dataset. Tbl. 1 shows
the accuracy of predicting a full bug
repair correctly (“Joint”) and analo-
gous to Eq. 2 break this up into a local-
ization accuracy (“Loc”) of predicting
the correct location (or NoBug for cor-
rect examples) and a repair accuracy
(“Repair”) for selecting the correct rewrite given the buggy location.

62.4
Supervised
69.4
Random Selector
PYBUGLAB
69.6
PYBUGLAB +Aug 70.3

73.6
79.6
80.4
81.1

Joint

RANDOMBUGS

GNN
Loc Repair Joint

GREAT

Loc Repair

81.2 51.0
84.0 63.9
84.2 64.0
84.5 65.3

61.9
73.6
74.3
75.3

76.3
82.0
82.3
82.5

PYBUGLAB-training.......
... ........ ... ..... .....

We observe that
other
compared...to........
methods for both GNNs and GREAT. Random selector models — a form of data augmentation — im-
..........
prove performance over supervised methods but mostly on in-distribution RANDOMBUGS samples.
As expected,
generalization, but does not make a substantial
difference. Expanding the kinds of rewrites used to augment the data and learning to select them may
improve performance in the future.

leads....to .......more........

robust......bug ...........

helps.................

detectors.............

augmenting... ..
... ....... ...

dataset.......

code.........

the ... ...

3This amounts to about 1.5 weeks for the GNN models and about 1 week for the GREAT models on a single

P100 GPU.

6

Table 2: Results for different training strategies and model architectures on PYPIBUGS.

PYPIBUGS

Joint

GNN
Loc Repair Joint

GREAT

Loc Repair

20.0
Supervised
21.2
Random Selector
PYBUGLAB
24.2
PYBUGLAB +Aug 26.4

28.4
27.0
31.3
33.5

61.8 16.8
69.2 20.6
70.7 24.0
72.0 23.2

25.8
26.8
32.8
29.7

58.6
67.2
67.9
68.8

PYPIBUGS-PostFix

GNN
Joint AUC Loc

GREAT

Joint AUC

0.087 20.7
0.108 52.5
0.160 28.6
0.187 48.2

0.044
0.117
0.140
0.129

Loc

17.8
47.5
32.9
32.6

Table 3: Localization and Repair Accuracy (%) per bug kind for the PYBUGLAB +Aug model.

Bug Type

RANDOMBUGS

PYPIBUGS

GNN
Loc Repair

GREAT
Loc Repair

GNN
Loc Repair

GREAT
Loc Repair

Argument Swapping
Wrong Assign Op
Wrong Binary Op
Wrong Boolean Op
Wrong Comparison Op
Wrong Literal
Variable Misuse
NoBug

85.0
96.1
83.0
71.8
83.9
71.7
84.9

53.8

57.3
99.1
85.2
99.5
79.3
74.7
88.4

—

65.5
94.5
77.3
43.6
80.0
66.6
78.2

62.5

57.2
98.6
81.4
99.5
76.4
71.6
86.3

—

33.2
20.0
27.2
27.6
33.7
21.6
35.3

—

73.9
68.9
54.3
96.9
66.1
78.4
70.5

—

24.3
14.0
36.6
15.7
31.1
17.9
34.0

—

72.7
58.1
43.7
97.2
53.5
79.5
69.4

—

Furthermore,
location. This is somewhat
expected: there are many more candidate locations compared to potential repairs at a given location.
However, this suggests that research should focus on the localization problem rather than repair.

localization ...is .......much........

repair ...at ..a .......

... .bug... ....... .....

given..........

harder......

than........

We now turn to the results on PYPIBUGS, shown in Tbl. 2, which also includes the accuracy of
choosing the special NoBug location on the PYPIBUGS-PostFix dataset, as well as the area under the
precision recall curve for the results on both PYPIBUGS and PYPIBUGS-PostFix.

real-life.......

repairing .........

signiﬁcantly........

bugs ..is...............

detecting... ..and... ....... ..
... .......

We ﬁnd that
randomly
handling............
bugs. As PYBUGLAB models trained using a learned bug selector outperform those using
inserted... ...
.. .. .....
a “Random Selector”, we speculate that the learned selector avoids generating easy-to-detect bugs,
focusing the detector model on recognizing deeper semantic patterns. Despite this, improvements in
RANDOMBUGS often correlate with improvements in PYPIBUGS. This is encouraging: collecting
PYPIBUGS-like datasets is costly; corpora with random bugs can help measure relative improve-
ments to some extent. Finally, we ﬁnd that
hard, and in
particular, does not always proﬁt from training in PYBUGLAB.

recognizing .............
..............

non-buggy..........

samples...is ......

than...........

harder......

very ......

In our qualitative analysis (Sec. 5.2), we observed that the models raised some conﬁdent but incorrect
warnings at very “odd” locations. However, these warnings were different across models. We have
tested an ensembling strategy averaging the output probabilities of ﬁve separately trained GNN
models. This results in localization and repair accuracies of 83.0% and 85.4% on RANDOMBUGS
(vs. 81.1% and 84.5%) and 34.4% and 72.2% on PYPIBUGS (vs. 33.5% and 72.0%). As we discuss
in Sec. 5.2 ﬁnding the cause of the “spurious” warnings is important future work.

snippet... ..has ..a ... ..bug ...or ... .not... .....

Per-Bug Evaluation To better understand which bugs are hard to detect, we break down the results
the best-performing PYBUGLAB +Aug models on RANDOMBUGS by type of bug in Tbl. 3. We
observe that incorrect literals are some of the hardest bugs to detect. Incorrect assignment operators
(e.g. = and +=) are easy to detect in RANDOMBUGS, but signiﬁcantly harder in PYPIBUGS. This may
Detecting...if
be attributed to class imbalance, with simple assignment (=) being the majority class.
...........
.a.........
beyond......63%.
hardest......
We note that in our experiments, GNNs-based models seem to often outperform GREAT, somewhat
contradicting the results of Hellendoorn et al. [13]. We have performed substantial additional experi-
ments to investigate and verify these results, cf. Sec. A.2. This may have to do with the performance
of these models on long sequences or that the GNN has access to more ﬁne-grained information,
instead of relations over the projected token sequences. For example, this could be attributed to
the lack of syntax and symbol nodes in the representation used in GREAT. Nevertheless, GREAT

task:....no........model..........

seems ...to ...be....the.........

achieves...........

accuracy.........

7

Table 5: Bug distribution (%) in different datasets

Bug Kind

PYPIBUGS RANDOMBUGS

Selector Samples

Argument Swapping
Wrong Assignment
Wrong Binary Operator
Wrong Boolean Operator
Wrong Comparison Operator
Wrong Literal
Variable Misuse

11.9
1.9
3.4
8.1
17.1
3.7
53.8

8.4
8.5
2.4
2.2
8.2
11.6
58.6

23.8
5.3
2.3
6.4
7.4
12.4
42.5

is noticeably better (62.5% vs. 53.8%) at detecting NoBug and locating wrong binary operators in
PYPIBUGS.

Bug Selector Performance To understand how training
of the bug selector proceeds, we perform two experiments.
In our ﬁrst experiment, we take a snapshot of the selector
model during training of the PYBUGLAB +Aug (GNN)
model every 24 hours, after an initial burn-in phase of 12
hours. We then generate 10000 buggy samples using each
of these snapshots and then test a ﬁxed model on each of
these snapshots. The results of this are shown in Tbl. 4, us-
ing a fully trained PYBUGLAB +Aug (GNN) model from
another training run as a ﬁxed model. We conclude that
harder...to

PYBUGLAB succeeds... .in ... .......
........... ... ..... . ... . .

learning ...to ..........

generate........

Table 4: Development of Performance
on Bug Selector Samples
Training Time

Loc Repair

Joint

0.5 days
1.5 days
2.5 days
3.5 days
4.5 days
5.5 days
6.5 days

64.2
62.5
62.0
61.7
61.9
61.1
60.5

83.8
80.7
83.0
82.5
83.0
83.0
78.7

72.1
72.9
69.8
69.8
69.5
68.6
72.4

....ﬁnd... ...bugs, though we can observe the selector model trading off “harder-to-localize” and “harder-to-
ﬁx” properties. Tests on other models show similar trends, conﬁrming the robustness of this result.

In a second experiment, we compare the distribution of different bug kinds in PYPIBUGS and
RANDOMBUGS with the distribution of bugs sampled from the ﬁnal snapshot of our selector model
from above. The results are shown in Tbl. 5, where we can see that a number of bugs (argument
swapping, use of wrong literals and of assignment operators) are substantially over-represented,
whereas mistakes in comparison operators and variable misuse are under-represented. This indicates
that

hard...to ......ﬁnd, ....but....not..............

necessarily ..........

PYBUGLAB generates... ...
... ........ ... ..... . ... . ..

bugs.
realistic ......

Comparison to CuBERT Finally, we compare our models to CuBERT [15], which uses a masked
language modeling objective to pre-train a BERT-like model and then learns bug detectors speciﬁc
to a class of bugs (e.g., wrong binary operators) on top of this pre-trained model. Note that CuBERT
detects if a bug exists but does not localize it. For the comparison, we create two sub-datasets of PYP-
IBUGS: PYPIBUGS-WrongOp contains the 501 samples that involve the binary operators supported
by CuBERT, and PYPIBUGS-VarMisuse, which contains the 1278 bugs that involve variable misuses.
We complete both of these datasets with 501 (resp. 1278) random NoBug code samples from our
RANDOMBUGS, to match the 1:1 buggy/non-buggy distribution used in CuBERT’s training. Since
CuBERT classiﬁcation models focus on a single bug type, to compare to PYBUGLAB we mask out all
code locations that do not correspond to a bug that could be detected by the corresponding CuBERT
model. We then treat the prediction of the NoBug location as a “non-buggy” prediction and all other
locations as a “buggy” prediction. For example, for the snippet in Fig. 2, only the locations l2, l11,
l14, l16, and l20 and their corresponding rewrites are considered by PYBUGLAB for the comparison
on PYPIBUGS-WrongOp.

Tbl. 6 shows the results of
comparing the released Cu-
BERT snapshots with the
PYBUGLAB +Aug GNN
model. We observe that
PYBUGLAB models
the ... ....... ..... ... ... . ...
.. ..
substantially ... ....

Table 6: Comparison with CuBERT [15]

CuBERT

Prec Recall

F1

PYBUGLAB (GNN)
Prec Recall

F1

PYPIBUGS-WrongOp 0.764
PYPIBUGS-VarMisuse 0.632

0.251
0.403

0.378
0.493

0.730
0.740

0.764
0.840

0.746
0.787

.. ...have... ........ ... ..
models, even though they were trained to de-
CuBERT-based .........
tect more bug types. When calibrating the CuBERT models to have a recall equal to PYBUGLAB,
their precision drops substantially. In particular, on PYPIBUGS-WrongOp, it is reduced to 0.609, and

than...................

better... ....

recall... ...

8

1 def make_id(name):
2
3
4
5

r = get_rand_string(12)
if len(name) <= 22:
name = name[:22]
return name + "-" + r

title, **request_params):

1 def update(self, roomId,
2
3
4
5

check_type(roomId, basestring)
check_type(roomId, basestring)
[...]

(a) A wrong comparison operator bug (red box)
in PYPIBUGS detected and repaired by the GNN
PYBUGLAB +Aug models. Fixing commit is
found here.

(b) A variable misuse (red box) caught in an open-source
project. GNN PYBUGLAB +Aug suggests to rewrite
roomId to title. The ﬁxing pull request is found here.

Figure 3: Bugs found by PYBUGLAB. Snippets reformatted and abbreviated to ﬁt ﬁgure.

on PYPIBUGS-VarMisuse, it is reduced to 0.613; in both cases, PYBUGLAB outperforms CuBERT
substantially.

5.2 Qualitative Inspection of Raised Warnings

We now take a qualitative look at the raised warnings raised by PYBUGLAB. As example, Fig. 3a
shows a sample of PYPIBUGS where the developer used an incorrect comparison operator. Once
pointed to it, it is clear to a human that the truncation statement in line 4 has no effect (under the
reasonable assumption that name is a string), and that a different comparison operator (>) is necessary.

To gain an understanding of the performance of PYBUGLAB on realistic data, we performed an in-
depth analysis of the cases ﬂagged as bugs by our best-performing model on the code found within
the 4k top PyPI packages. We observed a mixture of false positives with few previously unseen
real-life bugs, matching the quantitative results in Tbl. 3. First, we ﬁnd that the majority of the false
positives are “incorrect literal” detections. This suggests that learning to detect such bugs is a hard
problem. Furthermore, many literals serve as default “conﬁgurations” (e.g. the number of retries
for a network request) and different values are not bugs. We posit that a large percentage of literal
replacements the selector learns to make fall in this category.

We also found that some repairs suggested by the model actually produce semantically equivalent
code. For example, the model lacks knowledge that two variables refer to the same object in memory
(aliasing), and so attempts to “repair” variable misuse bugs by switching between these. Other
examples includes checking the return values of standard functions such as Python’s str.find,
which returns -1 if the query string is not found. In such cases, PYBUGLAB often suggested to
rewrite an if x.find(y) <= -1 to if x.find(y) == -1, which makes no difference in practice.
These false negatives can be attributed to the fact that the bug selector model considers such changes
as introducing bugs, even though they are not actually changing behavior. This suggests that for better
results, the rewrite rules need to ensure that the rewrites are not semantics-preserving and represent
bugs.

Finally, some reported issues were sufﬁciently complex that it took us (the human authors) a couple of
minutes of thought to conclude that a warning is spurious. Simultaneously, there are some warnings
that are “obviously” incorrect to us, but the reasons why the neural models raise them is unclear. This
highlights the importance of research on explainability techniques along with better ways to calibrate
model conﬁdence. The fact that selectors may introduce spurious “bugs” may also be affecting how
the detector model learns. Ideas that have appeared in reinforcement learning, such as the one of
Dennis et al. [8], may allow models to improve their performance in spite of spurious bugs.

the ... ...

1000... ........

... ..only... .19...of... ..

reported ...........

found ...to ...be..........

warnings ......were........

Overall,
bugs. Of these 19, we
reported 11 on GitHub (6 already merged, 5 pending approval). See Appx. G for details. 3 other bugs
had already been ﬁxed between the version PYBUGLAB processed and the current version or the
project was deprecated, whereas another 5 bugs are minor and we decided not to report them. One
of the detected bugs is shown in Fig. 3b. Overall, most of the detected bugs appear within unit tests,
logging, or exception handling, possibly because bugs there do not impact the core functionality of a
project. However, given the number of such bugs we collected in PYPIBUGS, we believe that such
bugs arise equally often in other code, but that they are detected and ﬁxed more quickly.

real-life.......

9

Although our analysis only forms a lower bound on the precision of PYBUGLAB and related methods,
it suggests that there is still ample room for future improvements towards making machine learning-
based bug detection and repair practically useful.

6 Related Work

Detecting bugs in source code has been researched since the early days of computing. Traditionally,
bug detection is tackled as a formal task, where any code that cannot be proved to satisfy some
correctness property may contain a bug. This is essential for security- and safety-critical bugs, but
not for other — equally common — bugs. In the last decade, software engineering and programming
language research have increasingly realized ambiguous information within code (e.g. variable names,
comments) contains valuable information and using this information can yield valuable results [2].
The main premise is that patterns in source code, such as patterns in names, control, and data ﬂow
can be informative. This information can also be exploited to detect some bugs. For example, Ray
et al. [24] noted that even simple language models tend to assign lower probability to buggy code.

Multiple static analysis methods have been researched that combine some form of data-oriented
bug detection. This ranges from language model-based tools, such as the early work of Wang et al.
[32] to speciﬁcation-mining tools such as the work of Eberhardt et al. [10]. BUGLAB is related to
DeepBugs [22] which uses an MLP over a limited window of code tokens and train separate models
to detect wrong operators, operands, and argument swappings. BUGLAB opts for a more structured
representation of code and a single model. Allamanis et al. [3], Vasic et al. [30], Hellendoorn et al.
[13] tackle variable misuse bugs (one of the kinds of bugs included in PYBUGLAB) but either by
randomly introducing the bugs in code or using a Cloze-like test. Instead, BUGLAB opts for a self-
supervised approach and tackles a broader range of bugs. Concurrently to this work, Patra and Pradel
[21] showed an alternative method for learning to generate realistic bugs. Dinella et al. [9] learn a
supervised sequential model that performs graph transformations that replicate small edits in code
(refactoring, introducing functionality, bug ﬁxing, etc.). Their model — Hoppity — could serve as a
learnable rewrite operation in BUGLAB in future work. Dynamic analysis methods have also been
researched with promising results [31], but collecting representative dynamic traces over a diverse set
of programs at scale (e.g. from the top Python packages used in this work) is practically impossible.

BUGLAB is related to ideas around self-supervised learning recently explored in deep learning,
computer vision, and NLP. In our case, we aim to train a bug detection model without using training
data from real-life bugs. BUGLAB resembles ELECTRA [6], with the important difference that the
rewrites to the input code go beyond single token replacement that need to respect strict constraints
of programming languages (syntax, variable scopes) and the model is directly used for bug detection,
rather than for pre-training. The main BUGLAB objective Eq. 1 also resembles GANs [12] with
the exception that the objective is non-differentiable (introducing a bug alters the discrete data
representation), the selector is a structured probabilistic code rewriting model, and that we are mainly
interested in the bug detector (analogous to the discriminator) rather than the selector.

7 Discussion and Conclusions

Learned program analyses offer the promise to improve how we develop software. They also offer a
great opportunity to study machine learning models that combine formal and probabilistic reasoning.
Towards achieving these we presented BUGLAB, a self-supervised approach for learning program
analyses, that improves upon baseline methods and detects bugs in real-life code. We also empirically
show the limitations of existing bug-detecting machine learning methods, which suffer from imprac-
tical false-positive rates. Importantly, we show the large gap of performance of existing methods on
corpora of randomly inserted bugs — commonly used in prior work — and real-life bugs.

Acknowledgements

We want to thank Sebastian Nowozin and Marwin Segler for helpful discussions, Marwin Segler for
comments on a draft of this work, and the anonymous reviewers for useful questions and suggestions.
Finally, we would like to thank the contributors to the following open-source tools used: PyTorch [20],
PyDriller [27], MessagePack, LibCST, Jedi, Kubernetes, Helm.

10

References

[1] M. Allamanis. The adverse effects of code duplication in machine learning models of code.
In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New
Paradigms, and Reﬂections on Programming and Software, pages 143–153, 2019.

[2] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton. A survey of machine learning for big code

and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37, 2018.

[3] M. Allamanis, M. Brockschmidt, and M. Khademi. Learning to represent programs with graphs.

In International Conference on Learning Representations (ICLR), 2018.

[4] M. Allamanis, E. Barr, S. Ducousso, and Z. Gao. Typilus: Neural type hints. In Proceedings
of the ACM SIGPLAN Conference on Programming Language Design and Implementation
(PLDI), pages 91–105. Association for Computing Machinery (ACM), 2020.

[5] T. Bachlechner, B. P. Majumder, H. H. Mao, G. W. Cottrell, and J. McAuley. Rezero is all you

need: Fast convergence at large depth. arXiv preprint arXiv:2003.04887, 2020.

[6] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. ELECTRA: Pre-training text encoders

as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.

[7] M. Cvitkovic, B. Singh, and A. Anandkumar. Deep learning on code with an unbounded

vocabulary. In Machine Learning 4 Programming, 2018.

[8] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine. Emer-
gent complexity and zero-shot transfer via unsupervised environment design. arXiv preprint
arXiv:2012.02096, 2020.

[9] E. Dinella, H. Dai, Z. Li, M. Naik, L. Song, and K. Wang. Hoppity: Learning graph transfor-
mations to detect and ﬁx bugs in programs. In International Conference on Learning Represen-
tations, 2019.

[10] J. Eberhardt, S. Steffen, V. Raychev, and M. Vechev. Unsupervised learning of API aliasing
speciﬁcations. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Lan-
guage Design and Implementation, pages 745–759, 2019.

[11] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing
for quantum chemistry. In International Conference on Machine Learning, pages 1263–1272,
2017.

[12] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.

[13] V. J. Hellendoorn, C. Sutton, R. Singh, P. Maniatis, and D. Bieber. Global relational models of

source code. In International Conference on Learning Representations, 2020.

[14] R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst, R. Holmes, and G. Fraser. Are mutants a
valid substitute for real faults in software testing? In Proceedings of the 22nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering, pages 654–665, 2014.

[15] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi. Learning and evaluating contextual
embedding of source code. In International Conference on Machine Learning (ICML), 2020.

[16] R.-M. Karampatsis and C. Sutton.

How often do single-statement bugs occur? the
ManySStuBs4J dataset. In Proceedings of the 17th International Conference on Mining Soft-
ware Repositories, pages 573–577, 2020.

[17] J. Katz. Libraries.io Open Source Repository and Dependency Metadata, Jan. 2020. URL

https://doi.org/10.5281/zenodo.3626071.

[18] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In

International Conference on Learning Representations (ICLR), 2016.

11

[19] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In Interna-

tional Conference on Learning Representations (ICLR), 2017.

[20] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison,

L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. 2017.

[21] J. Patra and M. Pradel. Semantic bug seeding: a learning-based approach for creating realistic
In Proceedings of the 29th ACM Joint Meeting on European Software Engineering
bugs.
Conference and Symposium on the Foundations of Software Engineering, pages 906–918, 2021.

[22] M. Pradel and K. Sen. DeepBugs: A learning approach to name-based bug detection. Proceed-

ings of the ACM on Programming Languages, 2(OOPSLA):1–25, 2018.

[23] G. Ramakrishnan, J. Henkel, Z. Wang, A. Albarghouthi, S. Jha, and T. Reps. Semantic robust-

ness of models of source code. arXiv preprint arXiv:2002.03043, 2020.

[24] B. Ray, V. Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, and P. Devanbu. On the ”naturalness”
of buggy code. In 2016 IEEE/ACM 38th International Conference on Software Engineering
(ICSE), pages 428–439. IEEE, 2016.

[25] V. Raychev, M. Vechev, and A. Krause. Predicting program properties from Big Code. In

Principles of Programming Languages (POPL), 2015.

[26] A. Rice, E. Aftandilian, C. Jaspan, E. Johnston, M. Pradel, and Y. Arroyo-Paredes. Detecting
argument selection defects. Proceedings of the ACM on Programming Languages, 1(OOPSLA):
1–22, 2017.

[27] D. Spadini, M. Aniche, and A. Bacchelli. PyDriller: Python framework for mining soft-
In Proceedings of the 2018 26th ACM Joint Meeting on European Soft-
ware repositories.
ware Engineering Conference and Symposium on the Foundations of Software Engineering
- ESEC/FSE 2018, pages 908–911, New York, New York, USA, 2018. ACM Press.
ISBN
9781450355735. doi: 10.1145/3236024.3264598. URL http://dl.acm.org/citation.
cfm?doid=3236024.3264598.

[28] D. Tarlow, S. Moitra, A. Rice, Z. Chen, P.-A. Manzagol, C. Sutton, and E. Aftandilian. Learning
to ﬁx build errors with Graph2Diff neural networks. In Proceedings of the IEEE/ACM 42nd
International Conference on Software Engineering Workshops, pages 19–20, 2020.

[29] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J´egou. Going deeper with image

transformers. arXiv preprint arXiv:2103.17239, 2021.

[30] M. Vasic, A. Kanade, P. Maniatis, D. Bieber, and R. Singh. Neural program repair by jointly

learning to localize and repair. arXiv preprint arXiv:1904.01720, 2019.

[31] K. Wang and Z. Su. Blended, precise semantic program embeddings. In Proceedings of the
41st ACM SIGPLAN Conference on Programming Language Design and Implementation, pages
121–134, 2020.

[32] S. Wang, D. Chollak, D. Movshovitz-Attias, and L. Tan. Bugram: bug detection with n-gram lan-
guage models. In Proceedings of the 31st IEEE/ACM International Conference on Automated
Software Engineering, pages 708–719, 2016.

[33] J. Wei, M. Goyal, G. Durrett, and I. Dillig. LambdaNet: Probabilistic type inference using

graph neural networks. arXiv preprint arXiv:2005.02161, 2020.

12

A Model Architectures

We implemented our models in PyTorch, using a shared codebase for GNN and GREAT models. The
shared code covers both the input modules (embedding tokens into vectors) and the subnetworks
used for bug localization and rewrite scoring.

In particular, the embedding of tokens uses a subtokenization strategy in which subtokens are em-
bedded separately, and the token embedding is obtained by using max pooling over the subtoken
embeddings. We have also experimented with alternative strategies (token-level embeddings and
character-level embeddings followed by 1D-CNNs to obtain a single representation), but found
subtokens to work best. We use a subtoken vocabulary of size 15000 and consider at most the ﬁrst
6 subtokens of a token (dropping the remainder, if more exist), and an embedding dimension that
matches the hidden dimension d of the used GNN/GREAT architecture.

We discuss the details of the used GNN/GREAT models in the subsections below.

For localization, we use an architecture similar to pointer nets [19]. Let (cid:96) ∈ L be the potential
locations for rewrites, and r(cid:96) ∈ Rd their corresponding representations as computed by the GNN/-
GREAT, and rNoBug the representation of the special NoBug location. We ﬁrst compute a “location
query” as maximum over the projected representations of all considered locations, and then use a
simple 2-layer MLP to compute per-location scores s(cid:96):

q = max{Wqr(cid:96) | (cid:96) ∈ L}
s(cid:96) = Wmlp,2σ(Wmlp,1(r(cid:96)(cid:107)q)).

Here, Wq ∈ Rd×d is a learnable projection and Wmlp,2 ∈ R1×d, Wmlp,1 ∈ Rd×d are the learnable
weights of our MLP. We can then model our distribution ploc from Sec. 3 by a softmax over these
scores.

To model the distribution prew, we use the rewrite-scoring functions described in Sec. 4 followed by
a softmax.

For all models, we use dropout between message passing/GREAT layers with rate 0.2, and train using
the Adam optimizer with learning rate 1e-4 and a linear warm-up of 800 steps, additionally clipping
gradient norms at 0.5. Bug selectors sample the distribution S(·) with an epsilon-greedy policy, with
epsilon 0.02.

A.1 GNN Architecture

Our GNN architecture follows a standard message-passing graph neural network [11], i.e. each
message passing layer is deﬁned as

h(t+1)
vi

= f (t)


h(t)


vi

,

(cid:77)

(cid:16)

m(t) (cid:16)

h(t)
vi

, k, h(t)
vj

∀vj :vi

k→vj

(cid:17)(cid:17)




 .

, · · · , h(t)
v|V |

] be a |V | × D matrix containing the output node states for all nodes

Let H (t) = [h(t)
v0
vi ∈ V we can write the GNN computation as H (t+1) = GNN(H (t)).
Our speciﬁc GNN message passing layers uses the structure deﬁned as follows. Messages are com-
puted as

mt (cid:16)

h(t)
vi

, k, h(t)
vj

(cid:17)

= W (t)

k

(cid:104)
h(t)
vi

, h(t)
vj

(cid:105)

,

(3)

i.e. a linear layer of the concatenation of the source and target node representations at time t and
is a edge type-speciﬁc linear layer. We use element-wise max pooling operator as (cid:76). The node
W (t)
k
update function is deﬁned as

f (t)(·) = tanh

(cid:16)

W (t)
f

· LAYERNORM (GELU(m)) + bf

(cid:17)

,

where Wf and bf are learnable parameters and m is the output of the aggregation of the messages
in Eq. 3 Backwards edge types are added for each existing edge type k, as in Li et al. [18].

13

Table 7: Accuracies on VarMisuse data of Hellendoorn et al. [13].

Localization (on buggy data) Localization (on non-buggy data)

Repair

GREAT-6L [13]
GREAT-6L (ours)

GREAT-10L [13]
GREAT-10L (ours)

86.14%
86.10%

87.61%
89.04%

88.98% 85.85%
93.33% 89.69%

89.72% 87.41%
93.47% 91.84%

Table 8: Results of GREAT-based PYBUGLAB models in fully supervised setting.

Joint

Loc

Repair

48.63% 59.30% 75.66%
GREAT-5L
46.73% 57.68% 74.90%
GREAT-6L
43.33% 54.08% 73.20%
GREAT-7L
51.04% 61.87% 76.26%
GREAT-8L
GREAT-9L
47.91% 58.84% 75.10%
GREAT-10L 47.86% 58.67% 75.28%

We use 8 GNN layers like the one discussed above but with residual layers, i.e.

H (4) = GNN4

(cid:16)(cid:104)

H (8) = GNN8

(cid:16)(cid:104)

(cid:16)

(cid:16)

H (0), GNN3
(cid:16)

H (4), GNN7

GNN2
(cid:16)

GNN1
(cid:16)

GNN6

GNN5

(cid:16)

H (0)(cid:17)(cid:17)(cid:17)(cid:105)(cid:17)
H (4)(cid:17)(cid:17)(cid:17)(cid:105)(cid:17)

,

where the concatenations of the residual layers is over the node vectors. Finally, we set the represen-
tation of each entity vi as rvi = h(8)
vi .
We use a node hidden size of 256 and a minibatch size up to 300 graphs with no more than 10000
nodes in total.

A.2 GREAT Architecture

We re-implemented GREAT [13] in PyTorch, following the paper and consulting the TensorFlow
reference implementation where necessary. We veriﬁed that our implementation matches the ref-
erence implementation by using the model to train for the VarMisuse task deﬁned for the dataset
released by Hellendoorn et al. [13]. To this end, we considered two model conﬁgurations (6 and 10
layers, both with hidden representation size 512). The results are shown in Tbl. 7, indicating that our
implementation matches (and in some regards, even outperforms) the reference implementation.

However, in our main PYBUGLAB experiments, we found that our GREAT models were usually
outperformed by their GNN equivalents, contradicting earlier results by Hellendoorn et al. [13].
We tried to tune hyperparameters (such as number of layers) on the fully supervised bug detection
detection dataset (row “Supervised” in Tbl. 1), ﬁrst varying the number of layers from 5 to 10. This
yielded the results shown in Tbl. 8. From the lack of a trend in these results we concluded that model
capacity is not a limiting factor, and our reproduction results on the original GREAT data indicated
that no implementation bugs in the GREAT layers needed to be suspected. As everything but the
core code entity representation subnetwork is shared with the GNN models, which do not show such
behavior, we ruled out implementation issues overall. Finally, we experimented with methods to
stabilize training of deep Transformer networks such as ReZero [5] and LayerScale [29], and varying
where LayerNorm is applied (before/after each sublayer). All of these experiments did not show
signiﬁcant improvement.

Consequently, our main results in Sec. 5 are reported on the GREAT conﬁguration performing best
in the fully supervised setting; yielding 8 layers with a hidden representation size of 512, 8 heads,
and an intermediate size of 2048. During training, we have set the the maximum sequence length to
400 and used a minibatch size of 20.

14

Table 9: List of Entity (Node) Types in PYBUGLAB Representation.

Entity Type

Description

Token
SyntaxNode
Type
Documentation
Symbol
Subtoken

A token in the Python.
An AST node as deﬁned in libCST’s concrete syntax trees.
The fully-qualiﬁed name of a type inferred by Jedi.
The full docstring comment of an invoked method.
A symbol (variable, function, etc.) in Python’s symbol table.
A subtoken of an identiﬁer, deterministically split on camelCase and
pascal case.

FormalArgName The name of a formal argument of a method declaration.

Table 10: List of Relationship (Edge) Types in PYBUGLAB Representation.

Relation Type

Description

Links two consecutive Token nodes.
Links a parent SyntaxNode to its child SyntaxNode or Token.

NextToken
SyntaxChild
SyntaxNextSibling Links a SyntaxNode to its subsequent sibling.
Type
CallDoc

FormalArg
ControlFlowNext

AssignedFrom

ReturnsFrom

YieldsFrom

OccurenceOf

LastMayUse

LastMayWrite

MayFinalUseOf

Links a Symbol to its candidate type, if one has been inferred.
Links a method invocation SyntaxNode to its candidate Documenta-
tion.
Links an argument SyntaxNode to its FormalArgName.
Link an statement SyntaxNode to a potentially succeeding statement
SyntaxNode in terms of control ﬂow. When branching occurs, a state-
ment my have multiple links to other statements.
Links a target value SyntaxNode to the expression SyntaxNode syntax
node.
Links a function deﬁnition SyntaxNode to a return statement SyntaxN-
ode it contains.
Links a generator deﬁnition SyntaxNode to a yield statement SyntaxN-
ode it contains.
Links a variable Token or attribute SyntaxNode to the Symbol node it
refers to.
Links a usage of a variable Token or attribute SyntaxNode to all the
potential immediately previous usages.
Links a usage of a variable Token or attribute SyntaxNode to all the
last potential write operations.
Links any potential last usage of a variable Token or attribute SyntaxN-
ode to its Symbol node.

B Python Code Representation

To extract the entities and relationships we use libCST and Jedi that either directly provide the
necessary data or allow to compute them. Tbl. 9 brieﬂy describes the included entities and Tbl. 10
the relationships among the entities. Most of those entities and relationships are ﬁrst used or inspired
from Raychev et al. [25], Allamanis et al. [3, 4], Wei et al. [33], Cvitkovic et al. [7].

For the synthetic code snippet of Fig. 2, Fig. 4 shows the entities and relationship within that snippet.
The signature of bar is set to def bar(formal bar arg1, formal bar arg2) for illustration
purposes.

C PYBUGLAB Evaluation Metrics

Fig. 5 shows the deﬁnitions of the evaluation metrics used in this work.

15

Figure 4: The graph representation of the entities and relationships for the snippet of Fig. 2. Please
zoom this pdf.

16

TokensFunctionDefIndentedBlockChild.bodyParametersChild.paramsComparisonTargetControlFlowNextReturnReturnsFromReturnReturnsFromfooOccurrenceOf(ChildfooChild.name)Child:Child<INDENT>ChilddefChildSimpleStatementLineChild.bodyIfChild.bodySimpleStatementLineChild.bodyIfChild.bodySiblingParamChild.paramsParamChild.paramsParamChild.paramsSiblingbChild.name,Child.comma=Child.equal0Child.defaultcChild.nameSibling,Child.commaaChild.nameSiblingAssignChild.bodySiblingIndentedBlockChild.bodyComparisonChild.testifChild:Child<INDENT>ChildChild.bodySiblingBooleanOperationChild.testIndentedBlockChild.bodyifChild:Child<INDENT>ChildSimpleStatementLineChild.bodySiblingChild.comparisonsaChild.leftCallControlFlowNextComparisonTargetControlFlowNextinChild.operatorbChild.comparatorAugAssignChild.bodyChild.valueControlFlowNext+=Child.operatorcChild.targetControlFlowNextArgChild.argsArgChild.argsfigs.sampleSnippet.sampleSnippet.barCandidateCallbarChild.func(Child)ChildSiblingformal_bar_arg1MayFormalName,Child.commabChild.valueformal_bar_arg2MayFormalNamecChild.valueComparisonChild.valueAssignTargetChild.targetsComparisonTargetControlFlowNext=ChildChild.comparisonscChild.leftSiblingAssignedFromc_is_negChild.targetfoo.<locals>.c_is_negOccurrenceOf<DEDENT>Childc_is_negChildc_is_negLastMayWriteControlFlowNext<Child.operator0Child.comparatorSiblingComparisonChild.rightControlFlowNextComparisonTargetControlFlowNextChild.leftorChild.operatorSimpleStatementLineChild.bodyChild.comparisonsaChild.leftControlFlowNextintChild.comparatorisChild.operatorChild.bodyTupleChild.valuereturnChildElementChild.elementsElementChild.elementsSiblingTrueChild.value,Child.commacChild.valueTupleChild.value<DEDENT>ChildreturnChildElementChild.elementsElementChild.elementsSiblingComparisonChild.value,Child.commacChild.valueChild.comparisonscChild.leftSiblingControlFlowNext>Child.operator1Child.comparatorfoo.<locals>.afoo.<locals>.bfoo.<locals>.cbarbuiltins.intbuiltins.TrueOccurrenceOfNextTokenSiblingLastMayWriteLastMayWriteNextTokenNextTokenSiblingNextTokenSiblingNextTokenSiblingOccurrenceOfMayFinalUseOfNextTokenNextTokenSiblingNextTokenNextTokenNextTokenSiblingNextTokenOccurrenceOfNextTokenOccurrenceOfMayFinalUseOf<DEDENT>NextTokenNextTokenNextTokenNextTokenOccurrenceOfNextMayUseNextTokenNextTokenOccurrenceOfNextTokenSiblingNextTokenAssignedFromOccurrenceOfNextTokenSiblingLastMayWriteLastMayWriteLastMayWriteLastMayWriteLastMayWriteSiblingOccurrenceOfNextTokenNextMayUseNextMayUseNextTokenNextTokenNextTokenNextTokenNextTokenSiblingOccurrenceOfMayFinalUseOfNextMayUseNextTokenNextTokenOccurrenceOfMayFinalUseOfNextTokenSiblingSiblingOccurrenceOfMayFinalUseOfNextTokenNextTokenSiblingNextTokenNextTokenOccurrenceOfNextTokenSiblingOccurrenceOfMayFinalUseOfNextTokenSiblingSiblingNextTokenNextTokenNextTokenNextTokenOccurrenceOfNextTokenSiblingLastMayWriteLastMayWriteNextTokenNextTokenNextTokenSiblingNextTokenSiblingOccurrenceOfNextTokenNextMayUseNextTokenOccurrenceOfMayFinalUseOfNextTokenNextTokenSiblingOccurrenceOfNextMayUseNextTokenNextTokenNextTokenDetection False Warnings (DFW)
Detection True Warnings (DTW)
False Detection Rate (FDR)
Detection Precision (DPr)
Detection Recall (DRe)
Repair Accuracy Given Location (RAcc)
Detect and Repair True Warning (TW)
Detect and Repair False Warning (FW)

PredictedLoc (cid:54)= GroundLoc AND PredictedLoc(cid:54)=NoBug
PredictedLoc = GroundLoc AND GroundLoc(cid:54)=NoBug

=
=
= DFW / (DFW + DTW)
= 1 - FDR
= DTW / (# buggy samples)
=
= LTW AND PredictedRewrite = GroundRewrite
= DFW OR (PredictedLoc(cid:54)=NoBug AND PredictedRewrite

(# correct rewrite predictions) / (# buggy samples)

Detect and Repair Precision (Pr)
Detect and Repair Recall (Re)

(cid:54)= GroundRewrite)
= TW / (TW + FW)
= TW / (# buggy samples)

Figure 5: Evaluation Metrics for Bug Detection and Repair

D PYPIBUGS Description

We collected a dataset, called PYPIBUGS, by crawling all commits across all PyPi python packages
found in the libraries.io v1.6.0 dataset [17]. If any of the rewrites of Sec. 4.1 yields a change that is
identical (modulo formatting and comments) to the observed commit, we consider it a candidate bug
ﬁx.

This yielded around 9000 changes. Note that here, to increase the precision of the dataset we require
that a change is the sole code change within a ﬁle of code. This excludes cases where more than one
bug-ﬁxing change happened within a ﬁle, but also removes many changes that are not bug ﬁxes.

The collected commits, along with the commit message, were then shown to one of the authors
of this paper to ﬁlter out any non-bug ﬁxing snippets. The annotation guideline was to look at the
diff and the commit message and reject it if it is does not look like a bug-ﬁxing commit. Through
this process 2377 changes remain. The human annotation process removed various changes that
mainly concerned conﬁguration-like “constants”, mainly literals such as changing the verbosity level,
changing the default values of ﬂags and arguments, turning off or on different parts of the software, etc.
It also removed other deployment-related artifacts such as the version number of the software. Finally,
we checked out each software project and attempted to extract our different code representations,
which removed another 3 samples due to parsing issues. The result is PYPIBUGS, a highly sanitized
dataset of 2374 samples representing a large variety of real-world “stupid simple bugs”.

Note that PYPIBUGS differs from ManySStUBs [16] in many ways. First our dataset only includes
Python code exposing the issues that can be represented by the different types of rewrites discussed
in Sec. 4.1. It is also aims to achieve high precision, by looking at sole changes within a single ﬁle
and commit and using manual annotation to ﬁlter changes that do not ﬁx bugs. The breakdown of
the different kinds of bugs in the dataset follows:

Table 11: Kinds of Bugs in PYPIBUGS

Bug Kind

Num Pct (%)

Argument Swapping
Wrong Assignment
Wrong Binary Operator
Wrong Boolean Operator
Wrong Comparison Operator
Wrong Literal
Variable Misuse

Total

283
45
81
192
407
88
1278

2374

11.9
1.9
3.4
8.1
17.1
3.7
53.8

(100%)

Replication of dataset Due to licensing concerns, we cannot release the raw data. Instead, we
provide the GitHub git URLs of the projects along with the commit SHAs, ﬁlepaths, and bug types as
a jsonl ﬁle. Anyone wishing to replicate the dataset can do so by cloning the projects and looking
for the appropriate commits. We also provide a Python script in the supplementary material. The

17

script automates the whole cloning and checkout process, but requires the user to implement the
visit buggy code and visit fixed code with code extracting the appropriate representation of
the code for each of the bugs in PYPIBUGS.

E RANDOMBUGS Description

The following table contains the statistics per kind of bug for the the RANDOMBUGS dataset.

Table 12: Kinds of Bugs in RANDOMBUGS

Bug Kind

Num Pct (%)

Argument Swapping
Wrong Assignment
Wrong Binary Operator
Wrong Boolean Operator
Wrong Comparison Operator
Wrong Literal
Variable Misuse
NoBug

Total

58459
58821
16848
15070
57037
80025
405950
69235

761445

7.7
7.7
2.2
2.0
7.5
10.5
53.3
9.1

(100%)

F Additional Evaluation Results

Some additional evaluation results are included in this appendix.

F.1 Localization & Repair Assuming Code is Buggy

In this subsection, we mask out the NoBug option for both the GNN and GREAT models and ask
these models to localize and repair bugs only in buggy code. The results are shown in Tbl. 13.

G Detected Bugs in Open-Source Projects

The following real-life bugs were detected by PYBUGLAB and a pull request was submitted.

G.1 Bug in spulec/moto

@@ -45,7 +45,7 @@ def describe_identity_pool(self, identity_pool_id):

identity_pool = self.identity_pools.get(identity_pool_id, None)

Table 13: Localization (%) per bug kind for the PYBUGLAB +Aug models when masking out the
NoBug option. This is similar to Tbl. 3 but only buggy examples are input to the models and they are
disallowed to predict NoBug. Note that repair results in Tbl. 3 are intact.

Bug Type

Argument Swapping
Wrong Assign Op
Wrong Binary Op
Wrong Boolean Op
Wrong Comparison Op
Wrong Literal
Variable Misuse

All Bug Kinds

RANDOMBUGS

GNN

GREAT

PYPIBUGS

GNN

GREAT

72.1
95.5
81.7
49.3
84.0
72.3
80.3

79.6

41.0
20.0
33.3
32.8
41.3
29.5
37.9

37.6

39.3
18.6
45.1
24.7
46.5
24.3
40.1

39.0

86.5
92.6
84.7
74.3
85.4
73.8
85.5

84.9

18

if not identity_pool:

-
+

raise ResourceNotFoundError(identity_pool)
raise ResourceNotFoundError(identity_pool_id)

response = json.dumps(

{

Pull request: https://github.com/spulec/moto/pull/3582 (Merged)

G.2 Bug in apache/tinkerpop

@@ -64,7 +64,7 @@ def __init__(self, partition_key=None, write_partition=None, read_partitions=Non

self.configuration["partitionKey"] = partition_key

if write_partition is not None:

self.configuration["writePartition"] = write_partition

-
+

if write_partition is not None:
if read_partitions is not None:

self.configuration["readPartitions"] = read_partitions

if include_meta_properties is not None:

self.configuration["includeMetaProperties"] = include_meta_properties

Pull request: https://github.com/apache/tinkerpop/pull/1379 (Merged)

G.3 Bug in certbot/certbot

@@ -166,7 +166,7 @@ def probe_sni(name, host, port=443, timeout=300, # pylint: disable=too-many-argu

" from {0}:{1}".format(

source_address[0],
source_address[1]
) if socket_kwargs else ""
) if any(source_address) else ""

-
+
)
socket_tuple = (host, port)
sock = socket.create_connection(socket_tuple, **socket_kwargs)

# type: Tuple[str, int]

# type: ignore

Pull request: https://github.com/certbot/certbot/pull/8605 (Merged)

Note that here PYBUGLAB detected a variable misuse bug, however the repair was more nuanced
that replacing socket_kwargs with source_address.

G.4 Bug in Polyconseil/aioamqp

@@ -305,7 +305,7 @@ def _close_channels(self, reply_code=None, reply_text=None, exception=None):

if asyncio.iscoroutinefunction(self._on_error_callback):

asyncio.ensure_future(self._on_error_callback(exception), loop=self._loop)

else:

-
+

self._on_error_callback(exceptions.ChannelClosed(exception))
self._on_error_callback(exception)

for channel in self.channels.values():

channel.connection_closed(reply_code, reply_text, exception)

Pull request: https://github.com/Polyconseil/aioamqp/pull/224 (Open)

G.5 Bug in apache/beam

@@ -636,7 +636,7 @@ def test_track_pcoll_unbounded(self):
pcoll2 = pcoll1 | ’do1’ >> FlatMap(lambda x: [x + 1])
pcoll3 = pcoll2 | ’do2’ >> FlatMap(lambda x: [x + 1])
self.assertIs(pcoll1.is_bounded, False)
self.assertIs(pcoll1.is_bounded, False)
self.assertIs(pcoll2.is_bounded, False)

-
+

19

self.assertIs(pcoll3.is_bounded, False)

def test_track_pcoll_bounded(self):

Pull request: https://github.com/apache/beam/pull/13761 (Merged)

G.6 Bug in sarugaku/requirementslib

@@ -487,15 +487,13 @@ def get_dependencies_from_index(dep, sources=None, pip_options=None, wheel_cache

-

-

session, finder = get_finder(sources=sources, pip_options=pip_options)
dep.is_direct = True
requirements = None
setup_requires = {}
with temp_environ(), ExitStack() as stack:

if not wheel_cache:

wheel_cache = stack.enter_context(_get_wheel_cache())

os.environ["PIP_EXISTS_ACTION"] = "i"
if dep.editable and not dep.prepared and not dep.req:

setup_info = SetupInfo.from_ireq(dep)
results = setup_info.get_info()
setup_requires.update(results["setup_requires"])
requirements = set(results["requires"].values())

else:

results = pip_shims.shims.resolve(dep)

Pull request: https://github.com/sarugaku/requirementslib/pull/282 (Open)

Note that the bug detected by PYBUGLAB is caused by dead/unused code with this pull request
removes.

G.7 Bug in CiscoDevNet/webexteamssdk

@@ -233,7 +233,7 @@ def update(self, roomId, title, **request_parameters):

"""
check_type(roomId, basestring)
check_type(roomId, basestring)
check_type(title, basestring)

-
+

put_data = dict_from_items_with_values(

request_parameters,

Pull request: https://github.com/CiscoDevNet/webexteamssdk/pull/150 (Merged)

G.8 Bug in percolate/redset

@@ -250,7 +250,7 @@ def _pop_items(self, num_items):

try:

res.append(self._load_item(item_str))

except Exception:

-
+

log.exception("Could not deserialize ’%s’" % res)
log.exception("Could not deserialize ’%s’" % item_str)

return res

Pull request: https://github.com/percolate/redset/pull/12 (Open)

G.9 Bug in pytorch/pytorch

if extra_inputs:

-
+

extra_input_names, extra_input_sizes = zip( *extra_inputs)
extra_inputs = _RectifyNames(extra_input_names)
extra_input_names = _RectifyNames(extra_input_names)
extra_inputs = zip(extra_input_names, extra_input_sizes)

20

Issue: https://github.com/pytorch/pytorch/issues/51410 (Open, Triaged)

G.10 Two Bugs in saltstack/salt

@@ -494,7 +494,7 @@ def enable( **kwargs):

if "enabled" in beacons and beacons["enabled"]:

ret["result"] = True
ret["comment"] = "Enabled beacons on minion."

-
+

elif event_ret:
elif "enabled" in beacons and not beacons["enabled"]:

ret["result"] = False
ret["comment"] = "Failed to enable beacons on minion."

else:

@@ -546,7 +546,7 @@ def disable( **kwargs):

-
+

if "enabled" in beacons and not beacons["enabled"]:

ret["result"] = True
ret["comment"] = "Disabled beacons on minion."

elif event_ret:
elif "enabled" in beacons and beacons["enabled"]:

ret["result"] = False
ret["comment"] = "Failed to disable beacons on minion."

else:

Pull request: https://github.com/saltstack/salt/pull/59381 (Open)

G.11 Bug in mahmoud/botons

@@ -286,7 +286,7 @@ class DeferredValue(object):

"""
def __init__(self, func, cache_value=True):

-
+

self.func = func
self.cache_value = True
self.cache_value = cache_value
self._value = _UNSET

def get_value(self):

Pull request: https://github.com/mahmoud/boltons/pull/277 (Merged)

G.12 Bug in geopy/geopy

@@ -426,8 +426,6 @@ def testContentAttrib(selector, key):

-
-

for key, value in iter(el.items()):

if value is not None:

place[key] = value.text
if value.text is None:

place[key] = None

else:

place[key] = None

Pull request: https://github.com/geopy/geopy/pull/469 (Merged)

G.13 Bug in allure-framework/allure-python

@@ -51,7 +51,7 @@ def parse_tag(tag, issue_pattern=None, link_pattern=None):

if issue_pattern and kind == "issue" and not value.startswith("http"):

value = issue_pattern.format(value)

if link_pattern and kind == "link" and not value.startswith("http"):

-
+

value = issue_pattern.format(value)
value = link_pattern.format(value)

return Link(type=kind, name=name or value, url=value)

if __is(kind, LabelType):

21

Fixed
34a91fa1f32e9f5279f14a595cb5401469b75ad8

in-between:

https://github.com/allure-framework/allure-python/commit/

G.14 Bug in qiniu/python-sdk

@@ -38,7 +38,7 @@ def set_default(

if default_api_host:

_config[’default_api_host’] = default_api_host

if default_uc_host:

-
+

_config[’default_uc_host’] = default_api_host
_config[’default_uc_host’] = default_uc_host

if connection_retries:

_config[’connection_retries’] = connection_retries

if connection_pool:

Fixed
71cf09cc04060524b4835a9b5d45a8ae3a4483c6

in-between:

https://github.com/qiniu/python-sdk/commit/

22

