1
2
0
2

l
u
J

6

]

C
O
.
h
t
a
m

[

1
v
9
7
3
2
0
.
7
0
1
2
:
v
i
X
r
a

Chordal and factor-width decompositions for scalable semideﬁnite
and polynomial optimization

Yang Zhenga, Giovanni Fantuzzib, Antonis Papachristodoulouc

aDepartment of Electrical and Computer Engineering, University of California San Diego, CA 92093.
bDepartment of Aeronautics, Imperial College London, London, SW7 2AZ, UK.
cDepartment of Engineering Science, University of Oxford, Parks Road, Oxford OX1 3PJ, U.K.

Abstract

Chordal and factor-width decomposition methods for semideﬁnite programming and polynomial optimization have re-
cently enabled the analysis and control of large-scale linear systems and medium-scale nonlinear systems. Chordal
decomposition exploits the sparsity of semideﬁnite matrices in a semideﬁnite program (SDP), in order to formulate an
equivalent SDP with smaller semideﬁnite constraints that can be solved more eﬃciently. Factor-width decompositions,
instead, relax or strengthen SDPs with dense semideﬁnite matrices into more tractable problems, trading feasibility
or optimality for lower computational complexity. This article reviews recent advances in large-scale semideﬁnite and
polynomial optimization enabled by these two types of decomposition, highlighting connections and diﬀerences between
them. We also demonstrate that chordal and factor-width decompositions allow for signiﬁcant computational savings
on a range of classical problems from control theory, and on more recent problems from machine learning. Finally, we
outline possible directions for future research that have the potential to facilitate the eﬃcient optimization-based study
of increasingly complex large-scale dynamical systems.

Keywords: Chordal sparsity, semideﬁnite optimization, polynomial optimization, sum-of-squares, matrix
decomposition, factor-width decomposition, large-scale systems, scalability

Contents

1 Introduction
. . . . . . . . . . . . . . . . . . . .
1.1 Outline
1.2 Basic notation . . . . . . . . . . . . . . . .

2 Chordal graphs and matrix decomposition
2.1 Chordal graphs . . . . . . . . . . . . . . . .
2.2 Sparse matrix decomposition . . . . . . . .
2.2.1
. . . . .
Sparse symmetric matrices
2.2.2 Cone of sparse positive semideﬁnite
matrices . . . . . . . . . . . . . . . .
positive-semideﬁnite-
completable matrices . . . . . . . . .
2.3 Block-partitioned matrices . . . . . . . . . .
. . . . . . . .
sparse
. . . . . . . . . . . .

2.3.1
Sparse block matrices
2.3.2 Chordal decomposition of

block matrices

2.2.3 Cone

of

3 Sparse semideﬁnite optimization

3.1 Aggregate sparsity . . . . . . . . . . . . . .
3.2 Nonsymmetric formulation . . . . . . . . . .
3.3 First-order algorithms . . . . . . . . . . . .

Email addresses: zhengy@eng.ucsd.edu (Yang Zheng),

giovanni.fantuzzi10@imperial.ac.uk (Giovanni Fantuzzi),
antonis@eng.ox.ac.uk (Antonis Papachristodoulou)

2
3
4

4
4
4
4

5

6
8
8

9

10
10
11
12

12
13
15
15

16
18

18
19
19
19
20
20
21

23
24

3.4

3.3.1 Domain- and range-space conversion
3.3.2 ADMM for decomposed SDPs . . . .
Interior-point algorithms . . . . . . . . . . .
3.4.1 Conversion methods . . . . . . . . .
3.4.2 Nonsymmetric interior-point algo-
rithms . . . . . . . . . . . . . . . . .
3.5 Algorithm implementations . . . . . . . . .

4 Sparse polynomial optimization

4.1.1
4.1.2

4.1 Background . . . . . . . . . . . . . . . . . .
SOS polyonomials and SDPs
. . . .
SOS polynomial matrices and SDPs
4.2 Sparse SOS decompositions . . . . . . . . .
4.2.1 General approach . . . . . . . . . . .
4.2.2 Correlative sparsity . . . . . . . . .
4.2.3 TSSOS, chordal-TSSOS and related
hierarchies . . . . . . . . . . . . . . .
4.2.4 Correlatively term-sparse hierarchies
Sparse SOS decompositions on semi-
4.2.5
algebraic sets . . . . . . . . . . . . .

25
4.3 Decomposition of sparse polynomial matrices 27
4.3.1 Global decomposition . . . . . . . .
27
4.3.2 Decomposition on a semialgebraic set 28
29
29

4.4 Other approaches . . . . . . . . . . . . . . .
4.5 Open-source software implementations . . .

Preprint submitted to Annual Reviews in Control

July 7, 2021

 
 
 
 
 
 
5 Factor-width decomposition

5.1 Background . . . . . . . . . . . . . . . . . .
5.2 Factor-width-k decompositions
. . . . . . .
5.3 Block factor-width-two decomposition . . .
5.4 Applications to semideﬁnite programming .
5.5 Applications to SOS optimization . . . . . .

29
29
30
30
31
32

6 Applications

6.1 Stability analysis and decentralized control

33
33
Stability of linear networked systems
33
Stability of sparse polynomial systems 35

6.1.1
6.1.2
6.1.3 Decentralized control of linear net-
worked systems . . . . . . . . . . . .
6.2 Relaxation of nonconvex QCQPs . . . . . .
6.2.1 Max-Cut problem . . . . . . . . . .
Sensor network location . . . . . . .
6.2.2
6.3 Machine learning: Veriﬁcation of neural
networks . . . . . . . . . . . . . . . . . . . .

7 Conclusion and outlook

Appendix A
no ﬁll-in

Cholesky factorization with

Appendix B

A proof of Theorem 2.1

Appendix C

Some properties of maximal

cliques

1. Introduction

36
36
37
37

38

39

41

42

42

The design of innovative technology capable to address
the challenges of the 21st century relies on the ability
to analyze, predict, and control large-scale complex sys-
tems, which are typically nonlinear and may interact over
networks (Astrom & Kumar, 2014; Murray et al., 2003).
Convex optimization is one of the key tools for achieving
these goals, because many questions related to the stability
and operational safety of dynamical systems, the synthesis
of optimal control policies, and the certiﬁcation of robust
performance can be posed as (or relaxed into) convex op-
timization problems. Very often, these take the form of
semideﬁnite programs (SDPs)—linear optimization prob-
lems with positive semideﬁnite matrix variables.

For linear systems, well-known methods based on lin-
ear matrix inequalities (LMIs) enable one to tackle a wide
range of problems, including the study of stability, reach-
ability, input-to-state and input-to-output properties, and
the design of optimal and robust control strategies (Boyd
et al., 1994; Kailath, 1980; Zhou et al., 1996). Methods
based on LMIs have been successfully applied across a
broad spectrum of applications, including automotive ap-
plications (Rajamani, 2011), ﬂight control (Giulietti et al.,
2000), power grids (Riverso et al., 2014; Sadabadi et al.,
2016), and traﬃc systems (Li et al., 2017; Ploeg et al.,
2013; Zheng et al., 2020). For nonlinear systems with

2

polynomial dynamics, SDP relaxations based on sum-of-
squares polynomials (or, equivalently, moment sequences)
enable stability analysis (Anderson & Papachristodoulou,
2015; Henrion & Garulli, 2005; Parrilo, 2000; Peet & Pa-
pachristodoulou, 2012), the estimation of regions of attrac-
tions (Chesi, 2011; Henrion & Korda, 2014; Korda et al.,
2013; Topcu et al., 2009; Valmorbida & Anderson, 2017)
and reachable sets (Jones & Peet, 2019; Magron et al.,
2019), safety veriﬁcation (Miller et al., 2021; Prajna et al.,
2007), analysis of extreme or average behaviour (Fantuzzi
& Goluskin, 2020; Fantuzzi et al., 2016; Goluskin, 2020;
Korda et al., 2021; Kuntz et al., 2016), and optimal con-
trol (Han & Tedrake, 2018; Henrion & Lasserre, 2006;
Lasagna et al., 2016; Lasserre et al., 2008; Majumdar et al.,
2014; Prajna et al., 2004).

A widespread view since the 1990s is that, once a con-
trol problem is reformulated as an SDP or relaxed into
one, then the problem is eﬀectively solved (Boyd et al.,
1994; Parrilo & Lall, 2003). In today’s world of large-scale,
complex systems, however, this is no longer true, and the
formulation of SDPs that can be solved in practice requires
further thought. This is because, even though SDPs can
theoretically be solved using algorithms with polynomial-
time complexity (Nemirovski, 2006; Nesterov, 2003; Nes-
terov & Nemirovski, 1994; Vandenberghe & Boyd, 1996;
Ye, 2011), the very-large-scale SDPs encountered in real-
life applications require prohibitively large computational
resources in practice. One particular bottleneck is the
complexity of handling large semideﬁnite constraints; for
interior-point algo-
instance, each iteration of classical
rithms requires O(n3m+n2m2 +m3) time and O(n2 +m2)
memory (Nesterov, 2003, Section 4.3.3), where n is the size
of semideﬁnite constraint and m is the number of equality
constraints. The majority of established general-purpose
SDP solvers currently available, therefore, cannot handle
large problems (e.g., with n larger than a few hundreds and
m larger than a few thousands) on a regular computer.
Consequently, the application of SDP-based frameworks
for analysis and control is currently limited to medium-
scale linear systems and small-scale nonlinear ones.

Overcoming these scalability issues is a problem that has
received much attention in recent years (Ahmadi et al.,
2017b; De Klerk, 2010; Majumdar et al., 2020; Vanden-
berghe et al., 2015), and signiﬁcant progress has been made
through a number of diﬀerent approaches. Most of them
are related by a simple, yet powerful, underlying idea: de-
compose a large positive semideﬁnite matrix X as a sum of
structured ones, for which it is easier to impose positivity.
One type of structured decomposition considers sums of
low-rank matrices (Burer & Choi, 2006; Burer & Monteiro,
2003, 2005; Burer et al., 2002). Speciﬁcally, one writes
X = (cid:80)t
for some vectors v1, . . . , vt ∈ Rn, where
t ≤ n is a parameter to be chosen, and optimizes over the
choice of such vectors. Such a decomposition is guaranteed
to exist for a properly chosen t, and there are explicit lower
bounds on this parameter ensuring that the global mini-
mum of the decomposed problem coincides with that of

i=1 vivT
i

5

4

2

1

3

(a)











 =









 +















 ∈ Sn


+(E, ?) ⇔











 ,









X (cid:23) 0

Y1 (cid:23) 0

Y2 (cid:23) 0

X

X1 (cid:23) 0

X2 (cid:23) 0

(b)

(c)

Illustration of chordal decomposition, where (cid:23) denotes positive semideﬁniteness and X ∈ Sn

Figure 1.1:
+(E, ?) is a positive
semideﬁnite completion constraint (see Section 2.2 for a precise deﬁnition). (a) A chordal graph with ﬁve vertices, six edges, and
two maximal cliques (complete connected subgraphs), C1 = {1, 2, 3} and C2 = {3, 4, 5}; (b) Chordal decomposition of a semideﬁnite
constraint on a sparse matrix X into smaller positive semideﬁnite constraints on matrices Y1, Y2 with nonzero entries indexed
by the cliques C1 and C2; (c) Chordal decomposition of a positive semideﬁnite completion constraint on a sparse matrix X with
smaller positive semideﬁnite constraints on its principal submatrices X1 and X2 indexed by the cliques C1 and C2.

the original SDP (Pataki, 1998). However, while low-rank
decomposition can bring considerable performance gains
on large SDPs, it transforms a convex problem into a non-
convex one. Solution algorithms for the latter cannot be
guaranteed to converge to the global minimum unless the
original SDP is suﬃciently “smooth” and t is large enough
(Boumal et al., 2020; Waldspurger & Waters, 2020).

A second type of structured decomposition, which we
focus on in this paper, considers sums of sparse matrices.
In this case, one writes X = (cid:80)t
i=1 Yi for positive semideﬁ-
nite matrices Y1, . . . , Yt that are nonzero only on a certain
(and, ideally, small) principal submatrix. The choice of
these principal submatrices is crucial in determining the
particular type of matrix decomposition, as well as its
properties. Two common selection strategies distinguish
whether the original matrix X is dense or sparse.

If X is sparse, the principal submatrices are usually in-
dexed by the maximal cliques of the sparsity graph of X;
these notions will be deﬁned precisely in Section 2, but
are illustrated in Figure 1.1. When the sparsity graph is
chordal, meaning that all cycles of length larger than three
have an edge between nonconsecutive vertices, the exis-
tence of a clique-based decomposition is guaranteed (Agler
et al., 1988; Griewank & Toint, 1984; Kakimura, 2010).
One can therefore replace the optimization of the large
matrix X with the optimization of the matrices Y1, . . . , Yt
without any loss of generality. Together with a dual result
on the existence of positive semideﬁnite matrix comple-
tions (Grone et al., 1984), this chordal decomposition strat-
egy enables one to signiﬁcantly reduce the computational
complexity of SDPs involving sparse positive semideﬁnite
matrices (Fukuda et al., 2001; Kim et al., 2011; Nakata
et al., 2003; Vandenberghe et al., 2015).

k

When X is dense, instead, each matrix Yi in the de-
composition X = (cid:80)t
i=1 Yi is chosen to be nonzero only
(cid:1) possible k × k principal submatri-
on one of the t = (cid:0)n
ces of X, where the parameter k ≥ 2 is speciﬁed a priori.
This type of decomposition leads to factor-width-k inner
approximations of the positive semideﬁnite cone (Boman
et al., 2005), which are conservative but improve as k is
increased. When k (cid:28) n, optimizing over the matrices
Y1, . . . , Yt, rather than over the original dense matrix X,

leads to SDPs with small positive semideﬁnite cones, which
can often be handled eﬃciently. In the extreme case k = 2,
one obtains a second-order cone program, for which scal-
able algorithms exist (Alizadeh & Goldfarb, 2003).

This paper oﬀers a comprehensive review of chordal and
factor-width-k decomposition methods, as well as of their
application to large-scale semideﬁnite programming and
polynomial optimization. Our goal is to introduce prac-
titioners in control theory to the latest advances in these
ﬁelds, which over the last decade or so have increased the
scale of systems for which optimization-based frameworks
for analysis and control can be implemented at a reason-
able cost. Examples of problems that can now be handled
eﬃciently include the analysis and synthesis of large-scale
linear networked systems (Andersen et al., 2014b; Mason
& Papachristodoulou, 2014; Zheng et al., 2018c,d), the sta-
bility analysis and the approximation of regions of attrac-
tion for sparse nonlinear systems (Ahmadi & Majumdar,
2019; Schlosser & Korda, 2020; Tacchi et al., 2019a; Zheng
et al., 2019a), optimal power ﬂow in power grids (Andersen
et al., 2014a; Jabr, 2011; Molzahn et al., 2013), and nu-
merous problems in machine learning (Batten et al., 2021;
Chen et al., 2020b; Dahl et al., 2008; Kim et al., 2009;
Latorre et al., 2020; Newton & Papachristodoulou, 2021;
Zhang et al., 2018). We hope that knowledge of the ad-
vanced optimization techniques discussed here can assist
control theorists in developing eﬃcient modelling frame-
works that can be applied much more widely and, crucially,
to increasingly complex large-scale systems.

1.1. Outline

After introducing relevant graph-theoretic notions in Sec-
tion 2, we discuss chordal decomposition methods for gen-
eral SDPs in Section 3. Section 4 looks at decomposi-
tion methods for sparse polynomial optimization problems,
which arise when relaxing analysis and control problems
for nonlinear systems. Factor-width-k decompositions for
dense matrices are discussed in Section 5.
Section 6
presents examples of how matrix decomposition can be ap-
plied to some classical control problems and to some recent
problems in machine learning. Section 7 draws conclusions
and outlines possible directions for future research.

3

1.2. Basic notation
Mathematical symbols are deﬁned as necessary in each of
the following sections, but we summarize common nota-
tion here. The m-dimensional Euclidean space, the vector
space of n×n real symmetric matrices, and the cone of n×n
positive semideﬁnite symmetric matrices are denoted, re-
spectively, by Rm, Sn, and Sn
+. Angled brackets are used to
denote the inner product in any of these spaces; in particu-
lar, (cid:104)x, y(cid:105) = xTy when x, y ∈ Rm and (cid:104)X, Y (cid:105) = trace(XY )
when X, Y ∈ Sn. We often write X (cid:23) 0 instead of X ∈ Sn
+
when the matrix size is clear from the context or is unim-
portant, and write X (cid:31) 0 if X is strictly positive deﬁnite.

2. Chordal graphs and matrix decomposition

This section reviews chordal graphs and their applications
to sparse matrix decomposition. Matrix decomposition is
central to many sparsity-exploiting techniques for semidef-
inite and polynomial optimization. Detailed introductions
to chordal graphs can be found in the surveys by Blair &
Peyton (1993) and Rose (1970), and in the monographs by
Vandenberghe et al. (2015) and Golumbic (2004). We ﬁrst
introduce some graph-theoretic notions in Section 2.1, and
then given an overview of classical matrix decomposition
and completion results in Section 2.2. Extensions to sparse
block-partitioned matrices are discussed in Section 2.3.

2.1. Chordal graphs
A graph G(V, E) is deﬁned by a set of vertices V =
{1, 2, . . . , n} and a set of edges E ⊆ V × V. A graph G
is undirected if (vi, vj) ∈ E implies that (vj, vi) ∈ E. A
path in G(V, E) is a sequence of edges that connect a se-
quence of distinct vertices. A graph is connected if there is
a path between any two vertices, and complete if any two
vertices are connected by an edge, i.e., E = V × V. The
subgraph induced by a subset of vertices W ⊂ V is the
undirected graph with vertices W and edges E ∩ (W × W).
A subset of vertices C ⊆ V is called a clique if the sub-
graph induced by C is complete. If C is not contained in
any other clique, it is a maximal clique. The number of
vertices in C is denoted by |C|.

A cycle of length k ≥ 3 in a graph G is a set of pairwise
distinct vertices {v1, v2, . . . , vk} ⊂ V such that (vk, v1) ∈ E
and (vi, vi+1) ∈ E for i = 1, . . . , k − 1. A chord in a cycle
is an edge connecting two nonconsecutive vertices.

Deﬁnition 2.1. An undirected graph G(V, E) is chordal
if every cycle of length k ≥ 4 has at least one chord.

Examples of chordal graphs are given in Figure 2.1. Ob-
serve also that many common types of graphs are chordal,
including chains, acyclic undirected graphs (i.e., graphs
with no cycles, such as trees), undirected graphs with cy-
cles of length no greater than three, and complete graphs.
Chordal graphs have a number of properties that make
them easy to handle computationally. For example, a con-
nected chordal graph has at most n − 1 maximal cliques,

(a)

(b)

(c)

Figure 2.1: Examples of chordal graphs: (a) A generic chordal
graph. (b) A “banded” chordal graph; (c) A “block-arrow”
graph. The names “banded” and “block-arrow” are motivated
by the fact that, as explained in Section 2.2.1, these graphs
describe the sparsity patterns of the matrices in Figure 2.3.

and they can be identiﬁed in linear time with respect to the
number of vertices and edges (Vandenberghe et al., 2015)
using, for instance, Algorithm 2 in Appendix C. In addi-
tion, any induced subgraph of a chordal graph is chordal
because cycles in the subgraph are also cycles in the origi-
nal graph. This is a useful fact in several induction proofs
using chordality in Blair & Peyton (1993, Section 2). Fi-
nally, chordal graphs admit a so-called perfect elimination
ordering of the vertices, which is central to the zero ﬁll-
in property of Cholesky factorizations for sparse matrices.
These two properties are reviewed in Appendix A.

Given the rich structure implied by chordality, it is very
often convenient to extend a nonchordal graph G(V, E) into
a chordal graph ˆG(V, ˆE) with larger edge set ˆE ⊃ E, which
is called a chordal extension of G. Usually, a graph admits
many diﬀerent chordal extensions, including the trivial one
with edge set ˆE = V × V obtained by completion, and the
one obtained by completing only the graph’s connected
components. Finding a minimal chordal extension, mean-
ing that the smallest possible number of additional edges
has been added, is an NP-complete problem (Yannakakis,
1981). However, approximately minimal chordal exten-
sions can often be constructed in practice using heuristic
strategies such as the maximum cardinality search (Berry
et al., 2004) and the symbolic Cholesky factorization with
approximately minimum degree ordering (Fukuda et al.,
2001; Vandenberghe et al., 2015).

Figure 2.2 illustrates these concepts. The graph in Fig-
ure 2.2(a) is not chordal, but can be extended to the
chordal graph in Figure 2.2(b) by adding edge (2, 4), edge
(1, 3), or both. The ﬁrst two extensions are minimal, while
the latter is the trivial extension by completion. The min-
imal chordal extension obtained by adding edge (2, 4) has
two maximal cliques, C1 = {1, 2, 4} and C2 = {2, 3, 4}.

2.2. Sparse matrix decomposition
This subsection reviews two fundamental results on the de-
composition of sparse positive semideﬁnite matrices whose
sparsity can be described using chordal graphs.

2.2.1. Sparse symmetric matrices
Fix any positive integer n and set V = {1, . . . , n}. Given
an undirected graph G(V, E), we say that a symmetric ma-

4

1

4

2

3

1

4

2

3

1

4

2

3

1

4

2

3

(a)

(b)

(c)

(d)

Figure 2.2: (a) A nonchordal graph: the cycle (1-2-3-4) is
of length four but has no chords. (b) Minimal chordal exten-
sion obtained by adding edge (2, 4). The maximal cliques are
C1 = {1, 2, 4} and C2 = {2, 3, 4}. (c) Minimal chordal exten-
sion obtained by adding edge (1, 3). The maximal cliques are
C1 = {1, 2, 3} and C2 = {1, 3, 4}. (d) Trivial chordal extension
by completion.

















































(a)

(b)

(c)

Figure 2.3: Sparsity patterns of 8 × 8 matrices corresponding
to the chordal graphs in Figures 2.1(a) to 2.1(c), respectively
(throughout this paper,

denotes a real number).

trix X ∈ Sn has a sparsity graph G (alternatively, sparsity
pattern E) if Xij = Xji = 0 when (i, j) /∈ E. We denote
the space of sparse symmetric matrices by

Sn(E, 0) := {X ∈ Sn | Xij = Xji = 0, if (i, j) /∈ E}.

For example, the graph1 in Figure 2.2(b) describes the
sparsity pattern of the matrix

X =







X11 X12
0 X14
X21 X22 X23 X24
0 X32 X33 X34
X41 X42 X43 X44







∈ S4,

(2.1)

where each entry Xij may be nonzero or zero. Similarly,
the symbolic matrices in Figure 2.3 have sparsity patterns
described by the graphs in Figure 2.1.

Given X ∈ Sn(E, 0), the diagonal elements Xii and the
oﬀ-diagonal elements Xij with (i, j) ∈ E may be nonzero
or zero. Thus, if X ∈ Sn(E, 0) and ˆE ⊃ E is an extension
of the edge set, then we also have X ∈ Sn( ˆE, 0). In this
paper, we are especially interested in chordal extensions of
sparsity pattern. For simplicity, we will say that a matrix
X has a chordal sparsity pattern if its corresponding spar-
sity graph G(V, E) is chordal. Of course, this can always
be achieved via chordal extension.

In what follows, it will be convenient to refer to partic-
ular principal submatrices of a sparse matrix, indexed by
the maximal cliques of its sparsity graph. Given a clique

Ck of G(V, E), we deﬁne a matrix ECk ∈ R|Ck|×n with en-
tries

(ECk )ij =

(cid:40)

1,
0,

if Ck(i) = j,
otherwise,

(2.2)

where Ck(i) is the i-th vertex2. Given X ∈ Sn, the deﬁ-
∈ S|Ck|
nition of ECk implies that the operation ECk XET
Ck
extracts the principal submatrix of X indexed by the
clique Ck. Conversely, the operation ET
Y ECk “inﬂates” a
Ck
|Ck| × |Ck| matrix Y into a sparse n × n symmetric matrix
that has Y as its principal submatrix indexed by Ck, and
is zero otherwise. For example, the chordal graph in Fig-
ure 2.2(b) has a maximal clique C1 = {1, 2, 4}, and the
corresponding matrix EC1 is

EC1 =


1
0

0

0
1
0

0
0
0



 .

0
0
1

For the sparse matrix X ∈ S4 in (2.1) and any matrix
Y ∈ S3, we have

EC1 XET
C1

=

ET
C1

Y EC1 =













 ,

X11 X12 X14
X21 X22 X24
X41 X42 X44
Y11 Y12
Y21 Y22
0
0
Y31 Y32

0 Y13
0 Y23
0
0
0 Y33







.

2.2.2. Cone of sparse positive semideﬁnite matrices
Denote the set of positive semideﬁnite matrices with spar-
sity pattern E by

+(E, 0) := Sn(E, 0) ∩ Sn
Sn
+.

This set is a convex cone because it is the intersection of a
subspace and a convex cone. If G(V, E) is a chordal graph,
Sn
+(E, 0) can be represented using smaller but coupled con-
vex cones, as stated in the following result (Agler et al.
1988, Theorem 2.3; Griewank & Toint 1984, Theorem 4;
Kakimura 2010, Theorem 1).

Theorem 2.1. Let G(V, E) be a chordal graph with max-
imal cliques C1, C2, . . . , Ct. Then, Z ∈ Sn
+(E, 0) if and only
if there exist matrices Zk ∈ S|Ck|
+ for k = 1, . . . , t such that

Z =

t
(cid:88)

k=1

ET
Ck

ZkECk .

(2.3)

The “if” part of Theorem 2.1 is immediate, since a
sum of positive semideﬁnite matrices is positive semidef-
inite. The “only if” part, instead, can be proven using

1Throughout, we assume that each vertex has a self-loop, unless

otherwise noted. We omit the self-loops when plotting a graph.

2The elements of Ck can be sorted in any convenient order. We
implicitly use the natural ordering in this work, but using a diﬀerent
one simply amounts to a permutation of the columns of ECk

5

the zero ﬁll-in property of sparse Cholesky factorization
for Z ∈ Sn
+(E, 0) (Vandenberghe et al., 2015, Section 9.2);
see Appendix A and Appendix B for details. A simi-
lar elementary proof given by Kakimura (2010), based on
simple linear algebra and perfect elimination orderings for
chordal graphs, reveals that one can impose a rank con-
straint in the decomposition (2.3): there exist Zk with
rank(Z) = (cid:80)t

k=1 rank(Zk) such that (2.3) holds.

Remark 2.1. The chordality assumption in Theorem 2.1
is necessary. For every nonchordal pattern E, while partic-
ular matrices in Sn
+(E, 0) admit the decomposition (2.3),
there always exist matrices in Sn
+(E, 0) that do not; see
Vandenberghe et al. (2015, p. 342) for an explicit example.
In addition, the decomposition (2.3) generally requires all
maximal cliques C1, . . . , Ct, even when a subset of maximal
cliques has already covered the sparsity pattern E (that is
E = (cid:83)
k∈I Ck × Ck with I ⊂ {1, . . . , t}). An example of
(cid:4)
this is given in Appendix C.

Example 2.1. Consider the positive semideﬁnite matrix

(a)

(b)

(c)

Figure 2.4: Joint feasible set of the decomposed LMIs in (2.6):
(a) projection onto the (x1, x2) plane, (b) projection onto the
(x1, d) plane, (c) projection onto the (x2, d) plane. Panel (a)
also shows the boundary of the feasible set of the original 3 × 3
LMI (2.5).

such that





a
b
0



 = Z(x).

b
c + d
e

0
e
f

After eliminating the variables a, b, c, e and f using this
matching condition, we conclude that (2.5) holds if and
only if there exists d such that
(cid:20) 2x1
x1 + x2

x1 + x2
5 − x1 − x2 − d

(cid:23) 0,

(cid:21)

(2.6)

Z =


2
1

0



 ,

1 0
1 1
1 2

(2.4)

(cid:20) d
x1 x2 + 1

x1

(cid:21)

(cid:23) 0.

whose sparsity graph is a chordal chain graph with three
vertices, edge set E = {(1, 1), (2, 2), (1, 2), (2, 3)}, and
maximal cliques C1 = {1, 2} and C2 = {2, 3}. Theorem 2.1
guarantees that the decomposition (2.3) exists. Indeed, we
have

(cid:20)1 0
0 1

(cid:21)
0
0

, EC2 =

(cid:20)0 1
0 0

(cid:21)
0
1

,

EC1 =

and

Z = ET
C1

(cid:20)2
1

(cid:21)

1
0.5

(cid:124)

(cid:123)(cid:122)
Z1(cid:23)0

(cid:125)

EC1 + ET
C2

(cid:21)
(cid:20)0.5 1
2
1

(cid:124)

(cid:123)(cid:122)
Z2(cid:23)0

(cid:125)

EC2 .

This decomposition satisﬁes the rank constraint mentioned
above since rank(Z) = 2 and rank(Z1) = rank(Z2) = 1.
(cid:4)

Example 2.2. Given a variable x ∈ R2, consider the 3×3
linear matrix inequality (LMI)

Z(x) :=





2x1
x1 + x2
0

x1 + x2
5 − x1 − x2
x1

0
x1
x2 + 1



 (cid:23) 0.

(2.5)

This LMI has the same chordal sparsity pattern as the ma-
trix in (2.4). Consequently, Theorem 2.1 implies that (2.5)
holds if and only if there exist matrices

Z1 :=

(cid:21)

(cid:20)a b
c
b

(cid:23) 0

and Z2 :=

(cid:20)d
e

(cid:21)

e
f

(cid:23) 0

6

Figure 2.4 shows two-dimensional projections of the three-
dimensional feasible set of the two LMIs in (2.6). As ex-
pected, the projection on the (x1, x2) plane coincides with
the feasible set of LMI (2.5), which is contained inside the
thick black line in Figure 2.4(a). This conﬁrms that the
LMIs in (2.6) are equivalent to the LMI (2.5). Therefore,
we have decomposed a 3 × 3 LMI into two coupled LMIs
(cid:4)
of size 2 × 2.

2.2.3. Cone of positive-semideﬁnite-completable matrices
A concept related to the matrix decomposition above is
that of positive semideﬁnite matrix completion. Given a
matrix X ∈ Sn, let

PSn(E,0)(X) =

(cid:40)

Xij
0

if (i, j) ∈ E,
otherwise

(2.7)

be its projection onto the space of sparse matrices Sn(E, 0)
with respect to the Frobenius matrix norm. We deﬁne the
cone

+).

+(E, ?) := PSn(E,0)(Sn
Sn
Using (2.7), it is not hard to see that a sparse matrix X
is in Sn
+(E, ?) if and only if it has a positive semideﬁnite
completion, meaning that some (or all) of the zero en-
tries Xij with (i, j) /∈ E can be replaced with nonzeros
to obtain a positive semideﬁnite matrix X. We call X
the completion of X and refer to Sn
+(E, ?) as the cone of
positive-semideﬁnite-completable matrices.

Z ∈ Sn

+(E, 0)

Duality

X ∈ Sn

+(E, ?)

Theorem 2.1

Theorem 2.2

Z = (cid:80)t

k=1 ET
Ck

ZkECk , Zk ∈ S|Ck|

+

Duality

ECk XET
Ck

∈ S|Ck|

+ , k = 1, . . . , t

Figure 2.5: Summary of duality between Sn
graph G(V, E) with maximal cliques C1, . . . , Ct.

+(E, 0) and Sn

+(E, ?) and duality between Theorem 2.1 and Theorem 2.2 for a chordal

Remark 2.2 (Nonuniqueness of the positive semideﬁnite
completion). The positive semideﬁnite completion of a
matrix X ∈ Sn
+(E, ?) with sparsity pattern E is generally
not unique. For a chordal sparsity pattern E, two widely
used and eﬃcient strategies to compute a completion X
are the maximum determinant completion (Vandenberghe
et al., 2015, Chapter 10.2), which maximizes det X, and
the minimum rank completion (see Dancis 1992; Jiang
2017; Sun 2015, Chapter 3.3), which minimizes rank(X).
In particular, there exists a positive semideﬁnite comple-
tion X whose rank agrees with the maximum rank of the
principal submatrices ECiXET
(Dancis, 1992, Theorem
Ci
1.5), i.e.,

rank(X) = max

k=1,2,...,t

rank(ECk XET
Ck

).

(2.8)

(cid:4)

For any undirected graph G(V, E), the cones Sn

+(E, ?)
and Sn
+(E, 0) are dual to each other with respect to the
trace inner product (cid:104)X, Z(cid:105) = Trace(XZ) in the space
Sn(E, 0) (Vandenberghe et al., 2015, Chapter 10). To see
this, observe that

(Sn

+(E, ?))∗ = {Z ∈ Sn(E, 0) | (cid:104)X, Z(cid:105) ≥ 0, ∀X ∈ Sn

+(E, ?)}

= {Z ∈ Sn(E, 0) | (cid:10)PSn(E,0)(X), Z(cid:11) ≥ 0, ∀X (cid:23) 0}
= {Z ∈ Sn(E, 0) | (cid:104)X, Z(cid:105) ≥ 0, ∀X (cid:23) 0}
= {Z ∈ Sn(E, 0) | Z (cid:23) 0}
= Sn

+(E, 0).

For a chordal matrix sparsity pattern, Theorem 2.1 on
the decomposition of the cone Sn
+(E, 0) can be dualized
to obtain the following characterization of Sn
+(E, ?), ﬁrst
proved by Grone et al. (1984, Theorem 7).

Theorem 2.2. Let G(V, E) be a chordal graph with max-
imal cliques C1, . . . , Ct. Then, X ∈ Sn
+(E, ?) if and only
if

ECk XET
Ck

∈ S|Ck|
+

∀k = 1, . . . , t.

(2.9)

The “only if” part of Theorem 2.2 is immediate, since
any principal submatrix of a positive semideﬁnite matrix
is positive semideﬁnite. The “if” part, instead, relies on
the properties of chordal graphs and, as mentioned above,
can be proven by combining the duality between Sn
+(E, 0)
and Sn
+(E, ?) with Theorem 2.1 (Vandenberghe et al., 2015,

p. 357). Precisely,

X ∈ Sn

+(E, ?) ⇔ (cid:104)X, Z(cid:105) ≥ 0 ∀Z ∈ Sn
+(E, 0),
t
(cid:29)
(cid:88)

(cid:28)

⇔

X,

ET
Ck

ZkECk

≥ 0 ∀Zk ∈ S|Ck|
+ ,

k=1

t
(cid:88)

⇔

(cid:10)ECk XET

Ck

, Zk

(cid:11) ≥ 0 ∀Zk ∈ S|Ck|
+ ,

k=1
⇔ ECk XET
Ck

∈ S|Ck|
+

∀k = 1, . . . , t.

the duality between
The ﬁrst equivalence expresses
Sn
+(E, 0) and Sn
+(E, ?), the second one follows from Theo-
rem 2.1, and the third one follows from the cyclic property
of the trace operator: Trace(M N ) = Trace(N M ) for any
matrices M, N of compatible dimensions.

Figure 2.5 illustrates how the duality between Sn

+(E, 0)
+(E, ?) is mirrored in the duality between Theo-

and Sn
rem 2.1 and Theorem 2.2 for chordal graphs.

Example 2.3. Consider the symmetric matrix

X =


2
1

0

1
0.5
1


0
1
 ,
2

whose sparsity pattern is the (by now usual) 3-node
chordal chain graph with maximal cliques C1 = {1, 2} and
C2 = {2, 3}. It is easy to check that, while X is not posi-
tive semideﬁnite, the principal submatrices indexed by the
cliques C1 and C2 are. Then, Theorem 2.2 guarantees that
X ∈ Sn
+(E, ?), meaning that the zero entries may be re-
placed by nonzeros to obtain a positive semideﬁnite matrix
X. One possible positive semideﬁnite completion is

X =


2
1

2

1
0.5
1


2
1
 .
2

In fact, this is the minimum-rank completion whose rank,
rank(X) = 1, coincides with the maximum rank of indi-
(cid:4)
vidual principal submatrices of X (cf. Remark 2.2).

Example 2.4. Consider the problem of ﬁnding a variable
x ∈ R3 such that the matrix





1 − x1
x1 + x2
0

x1 + x2
x2
x2 + x3





0
x2 + x3
2x3 + 1

(2.10)

X(x) :=

7

2.3.1. Sparse block matrices
Given a positive integer n, any ﬁnite set of positive integers
α = {α1, α2, . . . , αp} is called a partition of n if (cid:80)p
i=1 αi =
n. The set of all possible partitions of n can be equipped
with the following (partial) order relation.

Deﬁnition 2.2. Let α = {α1, . . . , αp} and β =
{β1, . . . , βq} be two partitions of an integer n with p < q.
We say that β is ﬁner than α (and α is coarser than β), de-
noted by β (cid:64) α, if there exist integers {m1, m2, . . . , mp+1}
with m1 = 1, mp+1 = q +1 and mi < mi+1 for i = 1, . . . , p
such that αi = (cid:80)mi+1−1

βj for all i = 1, . . . , p.

j=mi

Essentially, a ﬁner partition β breaks some entries of
α into smaller ones (conversely, a coarser partition α is
obtained by merging some entries of β into a bigger one).
For example, the partitions α = {4, 2}, β = {2, 2, 2} and
γ = {1, 1, 1, 1, 1, 1} of n = 6 satisfy γ (cid:64) β (cid:64) α.

Given any integer n and any partition α = {α1, . . . , αp}
of n, a matrix M ∈ Rn×n can be written in the block form

Figure 2.6: Region of R3 where the matrix X(x) in (2.10)
admits a positive semideﬁnite completion (blue shading). This
region coincides with the intersection of the region of R3 where
the ﬁrst LMI in (2.12) is feasible (red shading; the region ex-
tends to inﬁnity in the x3 direction) and the cylindrical region
of R3 where the second LMI in (2.12) is feasible (green shading;
the region extends to inﬁnity in the x1 direction). Thick red
and green lines highlight the cross section of these two regions.

admits a positive semideﬁnite completion. This is equiv-
alent to ﬁnding x ∈ R3 as well as a corresponding scalar
y ∈ R such that





1 − x1
x1 + x2
y

x1 + x2
x2
x2 + x3

y
x2 + x3
2x3 + 1



 (cid:23) 0.

(2.11)

M =








M11 M12
M12 M22
...
...
Mp1 Mp2








. . . M1p
. . . M2p
...
. . .
. . . Mpp

Since the sparsity graph of X(x) is chordal, Theorem 2.2
implies that (2.10) is equivalent to the two LMIs

(cid:20) 1 − x1
x1 + x2

(cid:21)

x1 + x2
x2

(cid:23) 0,

(cid:20) x2
x2 + x3

(cid:21)

x2 + x3
2x3 + 1

(cid:23) 0. (2.12)

Feasible vectors x for the ﬁrst of these two LMIs can be
found by imposing 1 − x1 + x2 ≥ 0 and (1 − x1)x2 − (x1 +
x2)2 ≥ 0, while feasible x for the second LMI are found by
requiring x2 +2x3 +1 ≥ 0 and x2(2x3 +1)−(x2 +x3)2 ≥ 0.
The feasible sets obtained in each case are illustrated by
the red and green regions in Figure 2.6, respectively. The
blue region in the ﬁgure, instead, represents the three-
dimensional set of feasible x for (2.11). As expected from
Theorem 2.2, this is exactly the intersection of the feasible
regions for the two LMIs in (2.12). Similar to Example 2.2,
one can therefore replace the original 3 × 3 completion
constraint—which is equivalent to LMI (2.11)—with the
two 2 × 2 LMIs in (2.12) without any loss of generality. (cid:4)

2.3. Block-partitioned matrices

Theorems 2.1 and 2.2 can be extended to block-partitioned
matrices characterized by block-sparsity. Such matrices
arise, for example, when modeling network systems (cf.
Section 6.1), where each block in the partition corre-
sponds to an individual subsystem and sparsity in the
network connectivity translates into block-sparsity. Block-
partitioned matrices are also useful in extending factor-
width decomposition that will be discussed in Section 5.

8

with Mij ∈ Rαi×αj for all i, j = 1, . . . , p. For the ﬁnest
partition α = {1, . . . , 1} = 1n, the block Mij reduces to
the entry (i, j) of M . As shown below and in Section 5.3,
however, the freedom to consider a nontrivial partition
oﬀers considerable ﬂexibility when devising decomposition
strategies for a large matrix M . In particular, by reﬁning
or coarsening a partition one can in principle split a matrix
into blocks of optimal size for the computational resources
at one’s disposal.

The block sparsity pattern of an n × n matrix M whose
blocks are deﬁned by a partition α = {α1, . . . , αp} of n can
be described using a graph G(V, E) with V = {1, 2, . . . , p}
and edge set such that Mij = 0 if (i, j) /∈ E, where Mij
is the (i, j)-th block in M and 0 denotes a zero block of
appropriate size. We call α a chordal partition if the cor-
responding block sparsity graph G(V, E) is chordal. The
linear space of sparse symmetric block matrices with a pre-
scribed block sparsity pattern E is then given by

α(E, 0) := {M ∈ Sn|Mij = 0 if (i, j) /∈ E}.
Sn

The block-sparse positive semideﬁnite cone and the block-
sparse positive-semideﬁnite-completable cone are simply

Sn
α,+(E, 0) := Sn
α,+(E, ?) := PSn
Sn

α(E, 0) ∩ Sn
+,
α(E,0)(Sn
+).

(2.13a)

(2.13b)

Remark 2.3 (Chordal partitions and chordal extension).
If M is a sparse matrix with nonchordal sparsity pattern,
it is often possible to ﬁnd one or more chordal partitions

9

6

3

7

4

1

8

5

2

(a)

7, 8

9

7, 8, 9

4

5, 6

4, 5, 6

1, 2

3

(b)

1, 2, 3

(c)

Figure 2.7: (a) Nonchordal sparsity graph of the 9 × 9 matrix
M in Remark 2.3. Blue nodes and edges form a cycle of length
6 with no chord. (b) Chordal block sparsity graph of the same
matrix with partition α1 = {2, 1, 1, 2, 2, 1}. (c) Chordal block
sparsity graph of the same matrix with partition α2 = {3, 3, 3}.

α. An example is the 9 × 9 symbolic matrix








M =








=















=















where the partitions α1 = {2, 1, 1, 2, 2, 1} and α2 =
{3, 3, 3} are both chordal (the corresponding block sparsity
graphs are illustrated in Figure 2.7). For a given chordal
partition, in this example but also in general, completing
all blocks of M that are not identically zero results in a
chordal extension of M . For instance, the chordal exten-
sion of the 9×9 matrix above resulting from the partitions
α1 and α2 are, respectively,






















and








,

where entries colored in red have been added by the block-
completion process. Finding a chordal partition for a
matrix, therefore, gives a way of performing a particular
chordal extension of its sparsity pattern. The opposite,
however, is not true: not all chordal extensions are ob-
tained via a block-completion operation. One example for
the 9 × 9 matrix above is the chordal extension















,

which is obtained by a symbolic Cholesky factorization
(cid:4)
with approximately minimal degree ordering.

2.3.2. Chordal decomposition of sparse block matrices
As anticipated above, decomposition results similar to
Theorems 2.1 and 2.2 hold for Sn
α,+(E, 0)
when α is a chordal partition of n. Given a clique Ck

α,+(E, ?) and Sn

9

of the chordal block sparsity graph G(V, E) subordinate
to the chordal partition α, we deﬁne the block matrix
ECk,α ∈ Rs(α,k)×n, where s(α, k) = (cid:80)

αi, as

i∈Ck

(ECk,α)ij =

(cid:40)

Iαi,
0,

if Ck(i) = j,
otherwise.

(2.14)

is an identity matrix of dimension αi. When
Here, Iαi
α = {1, . . . , 1} is the trivial partition, ECk,α reduces to the
matrix ECk in (2.2). Similar to the case studied in Sec-
Ck,α ∈ Ss(α,k) extracts the
tion 2.2, the operation ECk,αXET
principal block-submatrix of X whose blocks are indexed
by Ck, while ET
Ck,αY ECk,α “inﬂates” an s(α, k) × s(α, k)
matrix into a sparse n × n block matrix.

We are now ready to extend Theorems 2.1 and 2.2 to

the case of sparse block matrices.

Theorem 2.3
(Chordal block-decomposition). Let
G({1, . . . , p}, E) be a chordal graph with maximal cliques
C1, C2, . . . , Ct, and let α = {α1, . . . , αp} be a partition of
n. Then, Z ∈ Sn
α,+(E, 0) if and only if there exist matrices
Zk ∈ Ss(α,k)
+

for k = 1, . . . , t such that

Z =

t
(cid:88)

k=1

ET

Ck,αZkECk,α.

(2.15)

(Chordal

block-completion). Let
Theorem 2.4
G({1, . . . , p}, E) be a chordal graph with maximal cliques
C1, C2, . . . , Ct, and let α = {α1, . . . , αp} be a partition of
n. Then, X ∈ Sn

α,+(E, ?) if and only if

ECk,αXET

Ck,α ∈ Ss(α,k)

+

∀k = 1, . . . , t.

(2.16)

The proofs of Theorems 2.3 and 2.4 rely on the fact
that the block sparsity graph of X ∈ Sn
α(E, 0) induces a
chordal extension of the standard sparsity graph of X (cf.
Remark 2.3) and, in fact, it is a hypergraph of the latter.
The normal chordal decomposition and completion from
Theorems 2.1 and 2.2 can then be applied to the chordal
extension of X, and the hypergraph structure implies the
two results above. Interested readers are referred to Zheng
(2019, Chapter 2.4) for details.

Example 2.5. Consider the 9 × 9 matrices

X =















2
1
0
1
1
0
0
0
0

1
2
1
0
1
1
0
0
0

0
1
2
0
0
1
0
0
0

1
0
0
2
1
0
1
1
0

1
1
0
1
2
1
0
1
1

0
1
1
0
1
2
0
0
1

0
0
0
1
0
0
2
1
0

0
0
0
1
1
0
1
2
1


0
0


0

0


1


1


0


1
2

and

Y =















1
1
0
1
1
0
0
0
0

1
1
1
0
1
1
0
0
0

0
1
1
0
0
1
0
0
0

1
0
0
1
1
0
1
1
0

1
1
0
1
1
1
0
1
1

0
1
1
0
1
1
0
0
1

0
0
0
1
0
0
1
1
0

0
0
0
1
1
0
1
1
1


0
0


0

0


1


1


0


1
1

,

which have the same nonchordal sparsity pattern as the
symbolic matrix considered in Remark 2.3. Readers can
easily check that X is positive semideﬁnite, while Y ad-
mits a positive semideﬁnite completion (e.g., replace all
zero entries with ones to obtain Y = 11T (cid:23) 0). The par-
titions α1 = {2, 1, 1, 2, 2, 1} and α2 = {3, 3, 3} are both
chordal, so while Theorem 2.1 cannot be directly applied
to decompose X, Theorem 2.3 guarantees the existence of
decompositions either in the symbolic form

X =








(cid:124)

(cid:123)(cid:122)
(cid:23)0








+








(cid:125)

(cid:124)

+








(cid:124)








(cid:125)








+








(cid:125)

(cid:124)

(cid:123)(cid:122)
(cid:23)0

(cid:123)(cid:122)
(cid:23)0








(cid:125)

(cid:123)(cid:122)
(cid:23)0

(corresponding to the partition α1) or in the symbolic form








X =








+















(corresponding to the partition α2). These coincide with
the classical chordal decompositions applied to the chordal
extensions of X from the chordal partitions α1 and α2.
Thus, one can choose whether to decompose X as a sum
of four matrices with 5 × 5 nonzero principal submatrices,
or as a sum of two matrices with 6 × 6 nonzero principal
submatrices. Similarly, one can apply Theorem 2.4 to ver-
ify that the matrix Y admits a positive semideﬁnite com-
pletion by checking the positive semideﬁniteness of either
(cid:4)
four 5 × 5 principal submatrices, or two 6 × 6 ones.

3. Sparse semideﬁnite optimization

The matrix decomposition and completion results in The-
orems 2.1 and 2.2 can be used to reduce the complex-
ity of algorithms for sparse semideﬁnite optimization. A

10

semideﬁnite program (SDP) in standard primal form takes
the form

min
X

(cid:104)C, X(cid:105)

subject to (cid:104)Ai, X(cid:105) = bi,

i = 1, . . . , m,

(3.1)

X ∈ Sn
+,

where C, A1, . . . , Am ∈ Sn, b ∈ Rm are the problem data.
The dual problem to (3.1) is also an SDP,

max
y,Z

bTy

subject to Z +

m
(cid:88)

yiAi = C,

(3.2)

i=1
Z ∈ Sn
+.

In this section, we describe decomposition techniques
for SDPs that exploit the joint sparsity pattern of the
coeﬃcient matrices C, A1, . . . , Am, called aggregate spar-
sity pattern. For simplicity, we assume that the matrices
A1, . . . , Am are linearly independent and that there exist
X (cid:31) 0, y ∈ R and Z (cid:31) 0 satisfying the equality constraints
in (3.1) and (3.2). This ensures that the primal and dual
optimal values are ﬁnite, equal, and attained. SDPs that
are infeasible or have unbounded objective can be tack-
led using homogeneous self-dual embeddings (O’Donoghue
et al., 2016; Ye, 2011; Ye et al., 1994) or by analyzing the
divergence of the iterates produced by solution algorithms
(Banjac et al., 2019; Liu et al., 2017). Sparsity can be
exploited within these frameworks, too, and we refer the
interested reader to Zheng et al. (2020, Section 5) and
Garstka et al. (2019) for details.

3.1. Aggregate sparsity

The pair of SDPs (3.1)-(3.2) is said to have aggregate spar-
sity graph G(V, E) if

C, A1, . . . , Am ∈ Sn(E, 0).

(3.3)

Of course, if E (cid:48) is an extension of E, then G(V, E (cid:48)) is also
a suitable aggregate sparsity graph. The minimal one,
therefore, is simply the union of the individual sparsity
graphs of C, A1, . . . , Am. Throughout this section, how-
ever, we consider a chordal extension of the minimal ag-
gregate sparsity graph. We therefore assume from now on
that the aggregate sparsity pattern E is chordal and has t
maximal cliques C1, . . . , Ct.

It must be noted that an SDP may have a fully con-
nected aggregate sparsity graph even if all coeﬃcient ma-
trices C and A1, . . . , Am are very sparse; see Zheng et al.
(2018b) for explicit examples. The decomposition methods
described below cannot be applied to such problems. How-
ever, there are broad classes of SDPs for which the sparsity
of the SDP data matrices can be expected to translate into
very sparse aggregate sparsity graphs.

One such family consists of SDPs arising from relax-
ations of graph optimization problems and control prob-
lems over networks, which typically inherit the structure
of the underlying network or graph. Notable examples
include SDP relaxations of combinatorial graph optimiza-
tion problems, such as Max-Cut (Goemans & Williamson,
1995) and graph equipartition (Karisch & Rendl, 1998),
eigenvalue optimization problems over graphs (Boyd et al.,
2004), analysis of linear networked systems (Deroo et al.,
2015; Mason & Papachristodoulou, 2014; Zheng et al.,
2018c,d), sensor network localization (Kim et al., 2009;
Nie, 2009; So & Ye, 2007), neural network veriﬁcation in
machine learning (Batten et al., 2021; Raghunathan et al.,
2018), and the optimal power ﬂow problem in electricity
networks (Andersen et al., 2014a; Bai et al., 2008; Jabr,
2011). We will brieﬂy discuss some of these applications
in Section 6.

Another source of SDPs with aggregate sparsity is the
reformulation of intractable constraints (either convex or
nonconvex) as tractable LMIs using auxiliary variables
(Ben-Tal & Nemirovski, 2001; Vandenberghe et al., 2015).
For example, consider the uncountable family of “uncer-
tain” convex quadratic constraints

xTATAx − 2bTx − c ≤ 0

on a variable x ∈ Rq, to be imposed for all matrices A ∈
Rp×q, vectors b ∈ Rq and scalars c ∈ R in the form

A = A0 +

r
(cid:88)

i=0

uiAi,

b = b0 +

r
(cid:88)

i=0

uibi,

c = c0 +

r
(cid:88)

i=1

uici

with uTu ≤ 1. Here, A0, b0 and c0 are nominal reference
values, and {Ai, bi, ci} are ﬁxed perturbations. Andersen
et al. (2010b) showed that this family of constraints is
equivalent to a sparse LMI in the form





f (x) − t
A0x
h(x)

(A0x)T
Ip
G(x)



 (cid:23) 0,

h(x)T
G(x)T
tIr

(3.4)

where t ∈ R while G : Rq → Rr×p, h : Rq → Rr and
f : Rq → R are known linear functions whose exact form
is not important here. When r (cid:29) p, this matrix has
a “block-arrow” aggregate sparsity pattern analogous to
that shown in Figure 2.3(c) (that ﬁgure is recovered ex-
actly when p = 1, r = 6, and q is arbitrary). This par-
ticular type of sparsity pattern is commonly encountered
in robust optimization (Andersen et al., 2010b; Ben-Tal &
Nemirovski, 1998; Goldfarb & Iyengar, 2003).

et al. (2001, Section 6) has sparse data matrices C and
A1, . . . , Am−1, but the m-th constraint (cid:104)11T, X(cid:105) = 0 de-
stroys the problem’s aggregate sparsity because the matrix
Am = 11T is dense. However, any matrix X ∈ Sn
+ satisfy-
ing (cid:104)11T, X(cid:105) = 0 can be expressed as X = V Y V T for some
matrix Y ∈ Sn−1

+ , where

V =










1
0 0 . . . 0
−1 1 0 . . . 0
0 −1 1 . . . 0
...
...
...
0
0

0
0
0
...
...
0 0 . . . −1 1
0 0 . . . 0 −1










∈ Rn×(n−1).

Thus, the original SDP can be reformulated as

(cid:104)C (cid:48), Y (cid:105)

min
Y
subject to (cid:104)A(cid:48)

i, Y (cid:105) = bi,
Y ∈ Sn−1
+ ,

i = 1, . . . , m − 1,

:= V TAiV . Since V is a
:= V TCV and A(cid:48)
where C (cid:48)
i
sparse basis matrix and the original data matrices are
sparse, this new SDP is characterized by aggregate spar-
sity (Fukuda et al., 2001, Section 6). Sparsity-promoting
modeling strategies that generalize this example are dis-
cussed by Vandenberghe et al. (2015, Chapter 14.1).

3.2. Nonsymmetric formulation

The aggregate sparsity of
the primal-dual pair of
SDPs (3.1)–(3.2) can be exploited by reformulating them
into a nonsymmetric pair of optimization problems, pro-
posed by Fukuda et al. (2001) and later discussed exten-
sively by Andersen et al. (2010a); Kim et al. (2011); Sun
et al. (2014); Zheng et al. (2020).

Consider ﬁrst the dual-standard-form SDP (3.2). Any
feasible matrix Z must be at least as sparse as the aggre-
gate sparsity pattern of the SDP. We can therefore restrict
Z to the subspace Sn(E, 0), where E is the edge set of the
aggregate sparsity graph, and rewrite (3.2) as

max
y,Z

(cid:104)b, y(cid:105)

subject to Z +

m
(cid:88)

Ai yi = C,

(3.5)

i=1
Z ∈ Sn
+(E, 0).

Remark 3.1 (Promoting aggregate sparsity). Sometimes,
it is possible to reformulate SDPs with no aggregate spar-
sity as equivalent SDPs with very sparse aggregate sparsity
graphs through a carefully chosen transformation of vari-
ables (Fukuda et al., 2001, Section 6; Vandenberghe et al.,
2015, Chapter 14.1). For instance, the SDP relaxation
of the graph equipartition problem studied by Fukuda

The primal-standard-form SDP (3.1), instead, typically
has a dense optimal matrix X. However, the value of
the cost function and the equality constraints depend only
on the entries Xij with (i, j) ∈ E, while the remaining
ones simply guarantee that X is positive semideﬁnite. We
can therefore pose (3.1) as an optimization problem over
the cone Sn
+(E, ?) of sparse matrix that admit a positive

11

Table 1: Comparison of ﬁrst-order algorithms for solving SDPs. “Chordal Sparsity”: whether the algorithm exploits chordal
sparsity; “SDP Type”: the types of SDP problems the algorithm considers; “Algorithm”: the underlying ﬁrst-order algorithm;
“infeas./unbounded”: whether the algorithm can detect infeasible or unbounded cases; “Solver”: whether the code is open-source.

Reference
Wen et al. (2010)
Zhao et al. (2010)
O’Donoghue et al. (2016)
Yurtsever et al. (2021)
Lu et al. (2007)
Lam et al. (2012)
Dall’Anese et al. (2013)
Sun et al. (2014)
Sun & Vandenberghe (2015)
Kalbat & Lavaei (2015)
Madani et al. (2017a)
Zheng et al. (2020)
Garstka et al. (2019)

Chordal Sparsity
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

SDP Type
(3.2)
(3.2)
(3.1)-(3.2)
(3.1)1
(3.1)
OPF2
OPF2
Special3
(3.1)-(3.2)
Special4
General5
(3.1)-(3.2)
Quad. SDP6

Algorithm
ADMM
Augm. Lagrang.
ADMM
SketchyCGAL
Mirror-Prox
Primal-dual
ADMM
Gradient proj.
Spingarn
ADMM
ADMM
ADMM
ADMM

Infeas./ Unbounded
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)

Solver
(cid:55)
SDPNAL
SCS
CGAL
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
CDCS
COSMO

Note: 1. It requires an explicit trace constraint on X; 2. Special SDPs from the optimal power ﬂow (OPF) problem; 3. Special SDPs from
the matrix nearness problem; 4. Special SDPs with decoupled aﬃne constraints; 5. General SDPs with inequality constraints; 6. A dual
SDP (3.2) with a quadratic objective function.

semideﬁnite completion,

min
X

(cid:104)C, X(cid:105)

subject to (cid:104)Ai, X(cid:105) = bi,
+(E, ?).

X ∈ Sn

i = 1, . . . , m,

(3.6)

Problems (3.6) and (3.5) are a primal-dual pair of
linear conic programs because the cones Sn
+(E, ?) and
Sn
+(E, 0) are dual to each other (see Section 2.2 and Fig-
ure 2.5). Even though the sparse matrix cones Sn
+(E, ?)
and Sn
+(E, 0) are not self-dual (Andersen, 2011; Andersen
et al., 2010a), so this sparse formulation is nonsymmetric,
one can solve (3.6), (3.5), or both problems simultaneously
using a variety of ﬁrst-order or interior-point algorithms.
The next two subsections discuss some of them.

Remark 3.2. A special type of aggregate sparsity arises
when the data matrices C, A1, . . . , Am are block-diagonal.
In this case, any feasible matrix X for (3.6) is automat-
ically positive semideﬁnite and, consequently, can be re-
stricted to Sn
+(E, 0). Therefore, the nonsymmetric formu-
lation described above becomes symmetric. In particular,
problems (3.5) and (3.6) are simply SDPs with a Cartesian
product Sn1
+ of semideﬁnite cones, where
ni is the size of the ith diagonal block and l is the number
(cid:4)
of blocks.

+ × . . . × Snl

+ × Sn2

3.3. First-order algorithms

First-order optimization algorithms rely only on gradient
information and have iterations with low computational
complexity, which can often be implemented in a dis-
tributed manner (Beck, 2017; Boyd et al., 2011). For these
reasons, the last decade has witnessed the development of
a range of ﬁrst-order methods to solve large-scale SDPs,

many of which are listed in Table 1. Some of these meth-
ods (O’Donoghue et al., 2016; Wen et al., 2010; Yurtsever
et al., 2021; Zhao et al., 2010) focus on generic SDPs and
do not exploit aggregate sparsity. Others, instead, tackle
the sparsity-exploiting nonsymmetric formulations (3.5)–
(3.6) using so-called domain space or range-space conver-
sion frameworks, which replace the matrix cones Sn
+(E, ?)
and Sn
+(E, 0) with smaller positive semideﬁnite cones using
the chordal decomposition and completion results in Theo-
rems 2.1 and 2.2 (see, e.g., Dall’Anese et al., 2013; Garstka
et al., 2019; Kalbat & Lavaei, 2015; Lam et al., 2012; Lu
et al., 2007; Madani et al., 2017a; Sun et al., 2014; Sun &
Vandenberghe, 2015; Zheng et al., 2020). Many of these
works combine this strategy with additional separability
assumptions for the equality constraints, which are sat-
isﬁed in optimal power ﬂow problems (Dall’Anese et al.,
2013; Kalbat & Lavaei, 2015; Lam et al., 2012) and the
matrix nearness problems (Sun et al., 2014) but not in
general. To the best of our knowledge, the only ﬁrst-order
methods that can currently handle general SDPs with ag-
gregate sparsity (including infeasible or unbounded ones)
are those developed by Zheng et al. (2020) and Garstka
et al. (2019).

3.3.1. Domain- and range-space conversion
Consider problem (3.6). When the aggregate sparsity
graph is chordal and has maximal cliques C1, . . . , Ct, The-
orem 2.2 allows one to replace the constraint X ∈ Sn
+(E, ?)
with

ECk XET
Ck

∈ S|Ck|
+

∀k = 1, . . . , t.

(3.7)

These constraints are coupled in general because the ma-
trices ECp XET
and ECq XET
depend on the same entries
Cq
Cp
of X if the cliques Cp and Cq overlap. The works referenced
above diﬀer primarily in how these couplings are handled

12

Primal SDP (3.1)

Theorem 2.2

Decomposed Primal SDP (3.9)

Duality

Duality

Dual SDP (3.2)

Theorem 2.1

Decomposed Dual SDP (3.12)

Figure 3.1: Duality between the original primal and dual SDPs, and the decomposed primal and dual SDPs.

and, as discussed in Remarks 3.3 and 3.5 below, the choice
of strategy can have a considerable impact on the overall
complexity of the iterations in a ﬁrst-order method.

A simple but powerful strategy was proposed recently
by Zheng et al. (2020), who used “slack” matrices
X1, . . . , Xt to rewrite (3.7) as

(cid:40)

Xk = ECk XET
Ck
Xk ∈ S|Ck|

+

∀k = 1, . . . , t,
∀k = 1, . . . , t.

The primal SDP (3.6) is then equivalent to

(cid:104)C, X(cid:105)

min
X,X1,...,Xt
subject to (cid:104)Ai, X(cid:105) = bi,

Xk = ECk XET
Ck
Xk ∈ S|Ck|
+ ,

,

i = 1, . . . , m,

k = 1, . . . , t,

k = 1, . . . , t.

Following Fukuda et al. (2001) and Zheng et al. (2020),
we refer to (3.9) as the domain-space decomposition of the
primal SDP (3.1).

A range-space decomposition of the dual SDP (3.2) can
be formulated in a very similar way. When the aggregate
sparsity pattern E is chordal, Theorem 2.1 implies that the
constraint Z ∈ Sn
+(E, 0) is equivalent to





Z =

t
(cid:88)

k=1

ET
Ck

ZkECk ,

Zk ∈ S|Ck|

+

∀k = 1, . . . , t.

(3.10)

Observe that, as before, the ﬁrst of these conditions cou-
ples the positive semideﬁnite matrices Zp and Zq if the
cliques Cp and Cq of the aggregate sparsity graph overlap.
To decouple them, Zheng et al. (2020) introduced slack
variables V1, . . . , Vt and reformulated (3.10) as

Using this to eliminate Z from (3.5) yields the range-space
decomposition

max
y,Z1,...,Zt,V1,...,Vt

(cid:104)b, y(cid:105)

subject to

m
(cid:88)

Ai yi +

t
(cid:88)

ET
Ck

VkECk = C,

i=1
k=1
Zk − Vk = 0, k = 1, . . . , t,
Zk ∈ S|Ck|
+ ,

k = 1, . . . , t.

(3.12)

(3.8)

(3.9)

While the domain- and range-space decompositions
(3.9) and (3.12) have been derived independently, it is not
diﬃcult to verify that they are a primal-dual pair of SDPs.
The duality between the original SDPs (3.1) and (3.2) is
thus inherited by the decomposed SDPs (3.9) and (3.12)
by virtue of the duality between Theorem 2.2 and Theo-
rem 2.1. This elegant picture is illustrated in Figure 3.1.

Remark 3.3. The introduction of variables Xk and Vk
leads to redundancies in the aﬃne constraints of (3.9)
and (3.12), but is essential to obtain a decomposition
framework that is suitable for the development of fast
ﬁrst-order SDP solvers. For example, as explained in Sec-
tion 3.3.2 below, applying the alternating direction method
of multipliers (ADMM) to (3.9) leads to an algorithm
whose iterations have closed-form update rules that can
be implemented eﬃciently. The same is usually not true if
the redundant constraints in (3.9) are used to eliminate
the matrix X:
the iterations of the ﬁrst-order method
proposed by Sun et al. (2014), for instance, require the
solution of a further SDP with quadratic objective func-
tion, which limits its scalability. However, the matrix
X may be eliminated from (3.9) without compromising
eﬃciency if the original primal SDP (3.1) has separable
aﬃne constraints. This observation was exploited to solve
sparse SDPs arising from optimal power ﬂow problems
(Dall’Anese et al., 2013; Kalbat & Lavaei, 2015) and ma-
trix nearness problems (Sun & Vandenberghe, 2015). Sim-
ilar observations apply to the seemingly redundant matri-
(cid:4)
ces Vk in the range-space decomposed SDP (3.12).






Z =

t
(cid:88)

k=1

ET
Ck

VkECk ,

Vk = Zk
Zk ∈ S|Ck|

+

∀k = 1, . . . , t,

∀k = 1, . . . , t.

3.3.2. ADMM for decomposed SDPs

The alternating direction method of multipliers (ADMM)
is a ﬁrst-order operator-splitting method developed in the
mid-1970s (Gabay & Mercier, 1976; Glowinski & Marroco,

(3.11)

13

1975) to solve general optimization problems in the form

f (X ) + g(Y)

min
X ∈X
Y∈Y

subject to A(X ) + B(Y) = C,

(3.13)

where f and g are proper convex (but not necessarily
smooth) functions on ﬁnite-dimensional normed vector
spaces X and Y, A and B are given linear operators from
X and Y into a ﬁnite-dimensional normed vector space Z,
and C ∈ Z is given. Given a penalty parameter ρ > 0 and
a dual variable Z ∈ Z that acts as a Lagrange multilier
for the equality constraint, ADMM ﬁnds a saddle point of
the (scaled) augmented Lagrangian

Lρ(X , Y, Z) := f (X ) + g(Y) +

(cid:107)A(X ) + B(Y) − C + Z(cid:107)2

ρ
2

by updating the primal variables X , Y and the dual vari-
able Z according to the following rules:

X (q+1) = arg min

X

Y (q+1) = arg min

Y

Lρ(X , Y (q), Z (q)),

Lρ(X (q+1), Y, Z (q)),

(3.14a)

(3.14b)

Z (q+1) = Z (n) + A(X (q+1)) + B(Y (q+1)) − C.

(3.14c)

The superscript (q) indicates that a variable is ﬁxed to
its value at the q-th iteration. Under mild technical con-
ditions (Boyd et al., 2011, Section 3.2), the method con-
verges to an (cid:15)-approximate solution of (3.13) using at most
O(1/(cid:15)) iterations.

Given its slow convergence rate, ADMM is suitable only
when (3.14a) and (3.14b) have closed-form expressions
and/or can be solved eﬃciently. Below, we show that this
is true when the method is applied to the decomposed
SDPs (3.9) and (3.12).

Domain-space decomposition. Consider the domain-space
decomposition (3.9). Let χK(x) denote the characteristic
function of a set K, i.e.,

χK(x) :=

(cid:40)

if x ∈ K,

0
+∞ otherwise.

For simplicity, we write χ0 when K ≡ {0}. Problem (3.9)
is equivalent to

min
X,X1,...,Xt

(cid:104)C, X(cid:105) +

(cid:20)
χ0 ((cid:104)Ai, X(cid:105) − bi) + χS|Ck |

+

(cid:21)
(Xk)

m
(cid:88)

i=1

subject to Xk = ECk XET
Ck

,

k = 1, . . . , t.

Upon letting X := {X} and Y := {X1, . . . , Xt}, this
problem may be written in the standard form (3.13) over
the spaces X = Sn and Y = Z = S|C1| × · · · × S|Ct|, and can
therefore be solved using ADMM. Introducing a penalty
parameter ρ > 0 and a dual variable Z := {Λ1, . . . , Λt},
where each Λk ∈ S|Ck| acts as a Lagrange multiplier for

the corresponding constraint Xk = ECk XET
, it is not
Ck
diﬃcult to check that the ADMM step (3.14a) reduces to
an equality-constrained quadratic program,

X (q+1) = argmin
(cid:104)Ai,X(cid:105)=bi
i=1,...,m

(cid:26) ρ
2

t
(cid:88)

k=1

(cid:13)
(cid:13)X (q)
(cid:13)

k − ECk XET
Ck

+ Λ(q)
k

(cid:13)
2
(cid:13)
(cid:13)
(cid:27)

F

+ (cid:104)C, X(cid:105)

.

Step (3.14b), instead, reduces to t independent positive
semideﬁnite projections of the form for k = 1, . . . , t

X (q+1)
k

= argmin
Xk∈S|Ck |

+

(cid:13)
(cid:13)Xk − ECk X (q+1)ET
(cid:13)

Ck

+ Λ(q)
k

(cid:13)
2
(cid:13)
(cid:13)

F

.

Finally, step (3.14c) updates the multipliers Λ1, . . . , Λt ac-
cording to

Λ(q+1)
k

= Λ(q)

k + X (q+1)

k

− ECk X (q+1)ET
Ck

.

1

, . . . , X (q+1)
t

+ required to compute X (q+1)

These three steps have eﬃcient closed-form solutions
and can be implemented eﬃciently (Zheng et al., 2020, Sec-
tion 4.1). In particular, the t independent projections onto
the cones S|Ck|
can
be computed through an eigenvalue decomposition with
complexity of O(|Ck|3) ﬂoating-point operations. This is
not expensive when all cliques C1, . . . , Ct of the aggregate
sparsity graph E are small, which is often true in many
In contrast, the ﬁrst-order algorithms for
applications.
generic SDPs developed in O’Donoghue et al. (2016); Wen
et al. (2010) require a projection onto the semideﬁnite cone
Sn
+ at each iteration, which becomes a bottleneck when
It is therefore clear that exploiting sparsity via
n (cid:29) 1.
chordal decomposition can bring signiﬁcant computational
savings in ADMM algorithms.

Range-space decomposition:. The range-domain decompo-
sition (3.12) of the dual-standard-form SDP (3.2) can be
solved using an ADMM algorithm very similar to that pre-
sented above. First, observe that (3.12) is equivalent to

min
y,Vk,Zk

− (cid:104)b, y(cid:105) + χ0

C −

(cid:18)

m
(cid:88)

i=1

Ai yi −

+

t
(cid:88)

k=1
t
(cid:88)

k=1

(cid:19)

ET
Ck

VkECk

χS|Ck |

+

(Zk)

subject to Zk − Vk = 0,

k = 1, . . . , t.

Grouping the variables as X := {y, V1, . . . , Vt} and
Y := {Z1, . . . , Zt}, this problem can be written in the gen-
eral form (3.13) over the spaces X = Rm × S|C1| × · · · × S|Ct|
and Y = Z = S|C1| × · · · × S|Ct| Given a penalty parame-
ter ρ > 0 and a dual variable Z := {Λ1, . . . , Λt}, where
each Λk acts as a Lagrange multiplier for the correspond-
ing constraint Zk − Vk = 0, one can easily verify that
the ADMM step (3.14a) reduces to solving the equality-

14

constrained quadratic program

min
y,V1,...,Vt

− (cid:104)b, y(cid:105) +

ρ
2

t
(cid:88)

k=0

(cid:13)
(cid:13)Z (q)
(cid:13)

k − Vk + Λ(q)

k

(cid:13)
2
(cid:13)
(cid:13)

F

subject to C −

m
(cid:88)

i=1

Ai yi −

t
(cid:88)

k=1

ET
Ck

VkECk = 0.

Step (3.14b), instead, reduces to t independent positive
semideﬁnite projections of the form

Z (q+1)
k

= argmin
Zk∈S|Ck |

+

(cid:13)
(cid:13)Zk − V (q+1)
(cid:13)

k

+ Λ(q)
k

(cid:13)
2
(cid:13)
(cid:13)

F

.

Finally, the dual variables Λ1, . . . , Λt are updated through
step (3.14c) as

Λ(q+1)
k

= Λ(q)

k + Z (q+1)

k

− V (q+1)
k

.

Again, these iterations admit inexpensive closed-loop
it is not diﬃcult to see that
expressions. Moreover,
the ADMM iterations for the range-space decomposi-
tion (3.12) and for the domain-space decomposition (3.9)
have similar leading-order complexity. In fact, Zheng et al.
(2020, Section 4.3) showed that the ADMM algorithms for
the primal and dual decomposed SDPs are scaled versions
of each other. This shows that the duality picture of Fig-
ures 2.5 and 3.1 is reﬂected also at the algorithmic level.

Remark 3.4. For all ﬁxed penalty ρ > 0, the primal and
dual ADMM algorithms outlined above converge to a so-
lution of (3.9) and (3.12), respectively, provided that strict
primal-dual feasibility conditions are satisﬁed (Boyd et al.,
2011, Section 3.2). An eﬃcient ADMM algorithm that can
handle primal or dual infeasible problems was developed
by Zheng et al. (2020, Section 5), who considered the ho-
mogeneous self-dual embedding (O’Donoghue et al., 2016;
Ye et al., 1994) of the domain-space decomposition (3.9)
(cid:4)
and the range-space decomposition (3.12).

Remark 3.5. As anticipated in Remark 3.3, consider-
ing the variables Xk and the constraints Xk = ECk XET
Ck
without eliminating any redundant variables is essential
to obtain eﬃcient ADMM iterations. This is because the
conic constraints separate completely from the aﬃne ones
in (3.9) when applying the splitting strategy of ADMM,
making it easy to update each Xk via simple projections
onto positive semideﬁnite cones. Similarly, the redundant
variables Vk and the constraints Zk = Vk in (3.12) are es-
sential to decouple the conic constraints from the aﬃne
ones, which enables one to handle positive semideﬁnite
(cid:4)
constraints via simple projections.

3.4. Interior-point algorithms

Interior-point algorithms for convex optimization prob-
lems with equality and inequality constraints employ New-
ton’s method to solve a sequence of modiﬁed equality-

constrained problems, obtained by replacing any inequal-
ity constraints with barrier functions in the objective (Nes-
terov, 2003; Ye, 2011). These barrier functions approxi-
mate the characteristic function of the set deﬁned by the
original inequality constraints and ensure that the optimal
solution of each modiﬁed problem is strictly feasible for the
original problem, meaning that it is an interior point of the
original feasible set.

Since Newton’s method relies on second-order (Hes-
sian) information, interior-point algorithms do not share
the slow convergence of ﬁrst-order methods.
Instead,
they converge to an (cid:15)-approximate solution using at most
O(log(1/(cid:15))) Newton iterations (Nesterov, 2003; Ye, 2011).
In practice, convergence often occurs within tens of it-
erations. Therefore, interior-point methods are typically
preferred when solving (3.1)-(3.2) to high accuracy. The
general-purpose SDP solvers SeDuMi (Sturm, 1999), SDPT3
(T¨ut¨unc¨u et al., 2003), SDPA (Yamashita et al., 2012), and
MOSEK (Mosek, 2015) are all based on primal-dual interior-
point methods, and they can very reliably solve small and
medium-sized SDPs (e.g., when n is less than a few hun-
dreds and m is less than a few thousands in (3.1)-(3.2))
on regular computers. However, they become impracti-
cal for large SDPs because the CPU time and memory
requirements for each interior-point iteration increase as
O(n3m + n2m2 + m3) and O(n2 + m2), respectively (Nes-
terov, 2003, Section 4.3.3).

Chordal graph techniques can be exploited to improve
the eﬃciency of interior-point methods when solving large-
scale SDPs with chordal aggregate sparsity (Andersen,
2011; De Klerk, 2010; Fukuda et al., 2001). This section
reviews two general approaches for doing so. The ﬁrst
one, similar to the conversion methods in Section 3.3.1,
reformulates problems (3.5) and (3.6) as SDPs with small
positive semideﬁnite cones, which are often easier to solve
with general-purpose interior-point solvers (Fukuda et al.,
2001; Kim et al., 2011; Nakata et al., 2003; Zhang &
Lavaei, 2020b). The second approach, instead, directly
solves (3.6)-(3.5) using an interior-point method for non-
symmetric conic optimization (Andersen et al., 2010a;
Coey et al., 2020; Nesterov, 2012; Skajaa & Ye, 2015). For
other ways to exploit chordal sparsity in the computation
of interior-point search directions, we refer the reader to
the works by (Benson et al., 2000), Pakazad et al. (2017b)
and Fukuda et al. (2001, Section 5).

3.4.1. Conversion methods
Starting from the domain-space decomposed SDP (3.9),
Fukuda et al. (2001) and Kim et al. (2011) suggested to
eliminate the global matrix X and rewrite the SDP (3.6)
only in terms of variables Xk ∈ S|Ck|
+ , k = 1, . . . t. To
rewrite the cost function and the ﬁrst set of equality con-
straints, one must choose matrices Ck and Aik that satisfy

t
(cid:88)

(cid:104)Ck, Xk(cid:105) = (cid:104)C, X(cid:105)

k=1

15

and

t
(cid:88)

k=1

(cid:104)Aik, Xk(cid:105) = (cid:104)Ai, X(cid:105),

i = 1, . . . , m.

These aﬃne relations do not usually determine Ck and
Aik uniquely, and some choices may be more convenient
than others from the point of view of computations (Sun
et al., 2014, Section 3.1, Zhang & Lavaei, 2020b, Section
6). The second set of constraints in (3.9), instead, can
be enforced via consistency constraints on the entries of
X1, . . . , Xt that correspond to the same elements of X.
Such consistency constraints can be formulated as

(cid:16)

ECj ∩Ck

ET
Ck

XkECk − ET
Cj

XjECj

(cid:17)

ET

Cj ∩Ck

= 0

(3.18)

∀j, k : Cj ∩ Ck (cid:54)= ∅.

The primal SDP (3.6) can therefore be rewritten as

min
X1,...,Xt

subject to

t
(cid:88)

k=1
t
(cid:88)

(cid:104)Ck, Xk(cid:105)

(cid:104)Aik, Xk(cid:105) = bi, i = 1, . . . , m,

(3.19)

k=1
(3.18), Xk ∈ S|Ck|

+ , k = 1, . . . t.

This conversion process, ﬁrst proposed in Fukuda et al.
(2001), is known as the domain-space decomposition (Kim
et al., 2011). The reformulated problem (3.19) has more
variables and constraints than the original SDP (3.1), but
the large matrix constraint X ∈ Sn
+ is replaced by t smaller
ones, Xk ∈ S|Ck|
+ for k = 1, . . . , t. In certain cases, the de-
composed problem (3.19) is easier to solve than the origi-
nal SDP (3.1) using general-purpose interior-point solvers;
see Nakata et al. (2003) and Fujisawa et al. (2009) for nu-
merical examples. Three other variants of this conversion
method, including range-space decompositions, have been
studied by Kim et al. (2011).

The main drawback of these conversion methods is that,
sometimes, the additional consistency constraints (3.18)
signiﬁcantly increase the size of the Schur complement
system that needs to be solved at each interior-point it-
eration. This can oﬀset the beneﬁts of the clique-based
matrix decomposition. As shown recently by Zhang &
Lavaei (2020b), this issue can be mitigated using a dual-
ization technique (L¨ofberg, 2009).

Remark 3.6 (Removing redundant constraints). Since
the maximal cliques in a chordal graph satisfy the running
intersection property (Blair & Peyton, 1993; Fukuda et al.,
2001) (see also Appendix C), it is in fact suﬃcient to en-
force the consistency between pairs Cj, Ck that correspond
to the parent-child pairs in a clique tree. Redundant con-
straints in (3.18) can therefore be removed using the run-
ning intersection property. Interested readers are referred
to Kim et al. (2011) and Vandenberghe et al. (2015) for
(cid:4)
details.

Remark 3.7 (Dropping or ﬁxing consistency constraints).
In some applications, the SDP (3.1) comes from a semidef-
inite relaxation of a nonconvex optimization problem.
Dropping some consistency constraints in (3.18) leads to a
valid weaker relaxation with a lower computational com-
plexity. This idea was successfully applied to semideﬁ-
nite relaxations for optimal power ﬂow problems (Ander-
sen et al., 2014a) and neural network veriﬁcation (Batten
et al., 2021). Other times, one can enforce some of the
consistency conditions a priori and look for feasible (but
suboptimal) points for an SDP at a low computational
cost. This idea was used in Zheng et al. (2018d) to de-
velop a scalable approach for solving distributed control
(cid:4)
problems.

3.4.2. Nonsymmetric interior-point algorithms

Chordal graph techniques can also be exploited to speed
up interior-point methods for the nonsymmetric pair of
sparse SDPs (3.6)-(3.5) without appealing to the ma-
trix decomposition and conversion frameworks described
above. Since the cones Sn
+(E, 0) are not
self-dual, such sparsity-exploiting methods cannot enjoy a
complete primal-dual symmetry (Andersen et al., 2010a).
Instead, one must resort to purely primal, purely dual,
or nonsymmetric primal-dual path-following methods (An-
dersen et al., 2010a; Burer, 2003; Coey et al., 2020; Nes-
terov, 2012; Skajaa & Ye, 2015).

+(E, ?) and Sn

To construct nonsymmetric interior-point methods,
Dahl et al. (2008) and Andersen et al. (2010a) introduced
barrier functions φ : Sn(E, 0) → R and φ∗ : Sn(E, 0) → R
for the cones Sn

+(E, ?), deﬁned as

+(E, 0) and Sn

(cid:40)

φ(Z) =

− log det Z Z ∈ int(Sn
otherwise,
+∞

+(E, 0)),

(3.20a)

and

φ∗(X) = sup

Z∈Sn(E,0)

(−(cid:104)X, Z(cid:105) − φ(Z)).

(3.20b)

Note that φ (resp. φ∗) is ﬁnite only on the interior of
Sn
+(E, 0) (resp. Sn
+(E, ?)) and tends to +∞ as Z (resp. X)
approaches the boundary of this cone. Observe also that
φ∗ is simply the Legendre transform of φ evaluated at −X.

Thanks to the properties of the barrier functions, a min-
imizing sequence {X µ}µ>0 for (3.6) can be computed by
solving the regularized primal problem

min
X

(cid:104)C, X(cid:105) + µφ∗(X)

subject to (cid:104)Ai, X(cid:105) = bi,

i = 1, . . . , m,

(3.21)

and letting µ → 0. Similarly, a minimizing sequence
{yµ, Z µ}µ>0 for (3.5) is found upon solving the regular-

16

ized dual problem

equations

max
y,Z

(cid:104)b, y(cid:105) − µφ(Z)

subject to Z +

m
(cid:88)

i=1

Ai yi = C

(cid:104)Ai, ∆X(cid:105) = ri, i = 1, . . . , m,

(3.22)

m
(cid:88)

i=1

∆yiAi −

1
µ

∇2φ(Z)−1[∆X] = R,

(3.25)

for µ → 0. Solutions of the regularized problems for ﬁxed
ﬁnite µ are usually found using Newton’s method, lead-
ing to so-called primal scaling and dual scaling interior-
point methods. Other methods can also be used; for in-
stance, Jiang & Vandenberghe (2021) recently suggested
solving (3.21) with a Bregman ﬁrst-order method, where
the complexity of evaluating the Bregman proximal oper-
ator can be reduced using a sparse Cholesky factorization.
When Newton’s method is applied to (3.21), the KKT

optimality conditions are

(cid:104)Ai, X µ(cid:105) = bi,

i = 1, . . . , m,

m
(cid:88)

i=1

yiAi + Z = C,

µ∇φ∗(X µ) + Z = 0,

(3.23a)

(3.23b)

(3.23c)

where y ∈ Rm is a Lagrange multiplier for the equality con-
straint in (3.21) and Z is an auxiliary variable arising from
the deﬁnition of φ∗ via the Legendre transform. Solutions
X µ ∈ Sn
+(E, ?) as µ is varied deﬁne the so-called central
path for (3.6). Similarly, the KKT optimality conditions
for (3.22) are

(cid:104)Ai, X(cid:105) = bi,

i = 1, . . . , m,

(3.24a)

m
(cid:88)

yµ
i Ai + Z µ = C,

i=1
µ∇φ(Z µ) + X = 0,

(3.24b)

(3.24c)

where X is a Lagrange multiplier for the equality con-
straint in (3.22). Solutions {yµ, Z µ} ∈ Rm × Sn
+(E, 0) as
µ is varied deﬁne the central path for (3.5). It is possible
to show that (3.23) and (3.24) are equivalent (Andersen,
2011, Chapter 3), so the set of points {X µ, yµ, Z µ}µ>0 in
+(E, ?)×Rm ×Sn
Sn
+(E, 0) deﬁne a primal-dual central path.
The rest of this section brieﬂy outlines how the chordal-
ity of the sparsity pattern E can be exploited in the context
of dual-scaling interior point methods. Similar ideas can
be used to formulate primal-scaling methods, and we refer
interested readers to the work by Andersen et al. (2010a,
Section 4.2) for details.

Dual-scaling interior-point methods. Search directions in
a dual-scaling interior-point method are obtained by lin-
earizing (3.24) around the current interior iterate X ∈
int(Sn
+(E, 0)). Replacing
X, y and Z with X + ∆X, y + ∆y, Z + ∆Z in (3.24),
linearizing (3.24c), and eliminating ∆Z yields the Newton

+(E, ?)), y ∈ Rm and Z ∈ int(Sn

where ∇2φ(Z)−1 is the inverse Hessian of φ at Z, ri =
bi −(cid:104)Ai, X(cid:105) and R = C −(cid:80)m
µ ∇2φ(Z)−1[X].
Further elimination of ∆X leads to the Schur complement
equation

i=1 yiAi −2Z + 1

H∆y = g,

(3.26)

where g ∈ Rm is a vector and H is an m × m positive def-
inite matrix, both depending only on the current (known)
iterates X, y and Z. Explicit expression for these quanti-
ties are given by Andersen et al. (2010a, Section 4.3).

Finding the dual-scaling search direction ∆X, ∆y, ∆Z
requires solving the Newton equation (3.25) or the Schur
complement equation (3.26). To do this using a direct
method, one must ﬁrst calculate the Hessian and inverse
Hessian of the barrier function φ(X) in (3.20a), and then
form and factorize the matrix H. This is the most compu-
tationally expensive part of any interior-point method. It
is in this computation that one can exploit the chordality
of the sparsity pattern E (Andersen et al., 2010a).

Fast calculations involving the barrier functions. The
value, gradient, Hessian, and inverse Hessian of the dual
barrier φ(Z) in (3.20a) can be computed eﬃciently if the
sparsity pattern E of Z is chordal. Similar fast algorithms
exist for the primal barrier φ∗(X) in (3.20b), but we do not
review them here and refer interested readers to Andersen
et al. (2010a, Section 3.2) for details.

The key ingredient of these eﬃcient algorithms is a
sparse Cholesky factorization with zero ﬁll-in (Blair & Pey-
ton, 1993; Rose, 1970; Vandenberghe et al., 2015): as re-
viewed in Appendix A, for any positive deﬁnite matrix Z
in int(Sn
+(E, 0)) with chordal sparsity there exists a per-
mutation matrix P and a lower triangular matrix L such
that

P ZP T = LLT,

P T(L + LT)P ∈ Sn(E, 0).

(3.27)

This factorization can be computed eﬃciently by following
a recursion on a clique tree (Vandenberghe et al., 2015,
Chapter 9.3).

Now, to evaluate φ(Z) it suﬃces to substitute Z =

P TLLTP into (3.20a) and observe that

φ(Z) = −2

n
(cid:88)

i=1

log Lii

because determinants distribute over products and permu-
tation matrices have unit determinant. Thus, φ(Z) can be
evaluated eﬃciently once the Cholesky factorization (3.27)
has been computed.

17

The gradient of φ(Z), instead, is given by the following

negative projected inverse

∇φ(Z) = −PSn(E,0)(Z −1).

Despite the fact that Z −1 is in general dense, the pro-
jection onto Sn(E, 0) can be computed from its sparse
Cholesky factorization (3.27) without computing any other
entries of Z −1 (Vandenberghe et al., 2015, Chapter 9.5).
The Hessian of φ at Z applied to a matrix Y ∈ Sn(E, 0)

is computed as

∇2φ(Z)[Y ] =

d
dt

∇φ(Z + tY ) |t=0= PSn(E,0)(Z −1Y Z −1).

Again, this quantity can be evaluated knowing only the
sparse Cholesky factorization of Z and its projected inverse
PSn(E,0)(Z −1), without explicitly computing the inverse
Z −1 or the matrix product Z −1Y Z −1 (Andersen et al.,
2010a, 2013).

Finally, thanks to the chordal structure, solving the lin-
ear equation ∇2φ(Z)[U ] = Y for U in order to evaluate the
inverse Hessian ∇2φ(Z)−1[Y ] has the same cost as the eval-
uating the Hessian ∇2φ(Z)[Y ]; see Andersen et al. (2010a,
Section 3.2) and Andersen et al. (2013).

3.5. Algorithm implementations

We conclude this section by providing a list of numerical
packages that implement some of the approaches reviewed
above. This list is not exhaustive, and the goal here is to
give the interested reader a starting point for numerical
experiments. First-order solvers based on augmented La-
grangian methods and ADMM for generic SDPs include
SDPNAL/SDPNAL+ (Sun et al., 2020; Zhao et al., 2010) and
SCS (O’Donoghue et al., 2019). CDCS (Zheng et al., 2016)
and COSMO (Garstka et al., 2019) are two open-source ﬁrst-
order solvers that exploit chordal sparsity in SDPs. The
MATLAB package CDCS implements the algorithms de-
scribed in Section 3.3.2 and has interfaces with the opti-
mization toolboxes YALMIP (L¨ofberg, 2004) and SOSTOOLS
(Prajna et al., 2002). The Julia package COSMO solves SDPs
with quadratic objective functions.

The conversion methods in Section 3.4.1 are im-
plemented in SparseCoLO (Fujisawa et al., 2009) and
CHOMPACK (Andersen & Vandenberghe, 2015). We note
implementation of
that CHOMPACK also provides useful
many other chordal matrix computations, including maxi-
mum determinant positive deﬁnite completion and min-
imum rank positive semideﬁnite completion. Another
MATLAB package Dual-CTC (Zhang & Lavaei, 2020a)
implements a dualized clique tree conversion (Zhang &
Lavaei, 2020b). The reformulated SDPs after conversion
can be solved using general-purpose interior-point solvers,
such as SeDuMi (Sturm, 1999), SDPT3 (T¨ut¨unc¨u et al.,
2003), SDPA (Yamashita et al., 2012), and MOSEK (Mosek,
2015). SMCP (Andersen & Vandenberghe, 2014) is a non-
symmetric interior-point solver that provides a Python

implementation of the algorithms in Section 3.4.2. Fi-
nally, SDPA-C (Fujisawa et al., 2004) is a primal-dual
interior-point solver that exploits chordal sparsity using
the maximum-determinant positive deﬁnite completion.

4. Sparse polynomial optimization

We have seen in Section 3 that the chordal decomposi-
tion of large semideﬁnite matrices allows for signiﬁcant
eﬃciency gains in the solution of sparse SDPs. The same
ideas can often be leveraged to replace SDP relaxations of
intractable optimization problems, which generally have
no inherent sparsity or other computationally advanta-
geous structure, with SDPs that do.

This section describes how sparsity (primarily chordal,
but also nonchordal) can be exploited in the context of
sum-of-squares (SOS) relaxation techniques for polyno-
mial optimization. As mentioned in the introduction, SOS
methods are at the heart of many recent tractable frame-
works for the analysis and optimal control of nonlinear
systems with polynomial dynamics; see Ahmadi & Gun-
luk (2018); Fantuzzi & Goluskin (2020); Fantuzzi et al.
(2016); Goluskin (2020); Han & Tedrake (2018); Henrion
& Korda (2014); Jones & Peet (2019); Korda et al. (2021);
Lasagna et al. (2016); Lasserre et al. (2008); Majumdar
et al. (2014); Miller et al. (2021); Papachristodoulou &
Prajna (2005); Prajna et al. (2004); Valmorbida & Ander-
son (2017) to name but a few contributions.

Our goal is not to oﬀer an exhaustive review of all
sparsity-exploiting methods that have been proposed in
this ﬁeld, but rather to introduce the key ideas underpin-
ning most of these methods from a general perspective, in
the hope that this can guide further developments. For
this reason, we concentrate mainly on two basic problems.
The ﬁrst, discussed in Section 4.2, is to prove that an n-
variate polynomial of even degree 2d is a sum of squares
and, therefore, globally nonnegative. In this case, we seek
to exploit the structure of polynomials that depend only a
small subset of all possible degree-2d monomials—a prop-
erty often referred to as term sparsity. The second prob-
lem, discussed in Section 4.3, is to check whether a sparse
and symmetric n-variate polynomial matrix P (x) is SOS,
and therefore positive semideﬁnite for all x ∈ Rn. In this
case, our goal is to leverage the structural sparsity of P ,
meaning that many of its entries are zero.

Although we focus only on global nonnegativity, all of
the sparsity-exploiting techniques discussed in this section
can be extended to prove polynomial (matrix) nonnega-
tivity locally on basic semialgebraic sets. Such extensions,
which have been studied extensively in order to build hi-
erarchies of sparse SDP relaxations for polynomial opti-
mization problems (Lasserre, 2006; Waki et al., 2006, 2008;
Wang et al., 2021a,b, 2020a; Zheng & Fantuzzi, 2020),
require some careful technical adjustments, but the un-
derlying strategy is the same as for the global nonnega-
tivity setting. We outline some of these adjustments in

18

Sections 4.2.5 and 4.3.2, and refer readers to the excellent
literature on this topic for full details.

ments from B, and construct a smaller exponent set for
which (4.5) is guaranteed to hold as long as f is SOS.

d

4.1. Background
Let R[x]n,d be the (cid:0)n+d
(cid:1)-dimensional space of polynomials
with independent variables x = (x1, . . . , xn) and degree no
larger than d. The n-variate monomial with exponent β =
(β1, . . . , βn) ∈ Nn and degree |β| = β1 +· · ·+βn is denoted
by xβ = xβ1
n . Given a ﬁnite set of exponents
B ⊂ Nn, we write xB = (xβ)β∈B for the (column) vector
of monomials with exponents in B. The cardinality of B is
denoted by |B|. We also deﬁne

2 · · · xβn

1 xβ2

B + B := {β + γ : β, γ ∈ B},
2B := {2β : β ∈ B}.

(4.1a)

(4.1b)

If Nn

d = {β ∈ Nn : |β| ≤ d} is the set of all n-variate
exponents of degree d or less, the vector xNn
d is a basis for
R[x]n,d and any polynomial f ∈ R[x]n,d can be written as
f (x) = (cid:80)
fβxβ for some coeﬃcients fβ ∈ R. The set
of exponents with nonzero coeﬃcient,

β∈Nn
d

supp(f ) = {β ∈ Nn

d : fβ (cid:54)= 0},

(4.2)

is called the support of f .
Newton polytope of f and is denoted by New(f ).

Its convex hull is called the

4.1.1. SOS polyonomials and SDPs
A polynomial f ∈ R[x]n,2d of even degree 2d is SOS if
there exist degree-d polynomials f1, . . . , fk ∈ R[x]n,d such
that

f = f 2

1 + · · · + f 2
k .

(4.3)

The set of n-variate degree-2d SOS polynomials, denoted
by Σn,2d, is a proper cone in R[x]n,2d (Blekherman et al.,
2012, Theorem 3.26). Given an exponent set A ⊆ Nn
2d, we
deﬁne the subcone of SOS polynomials supported on A as

Σ[A] := {f ∈ Σn,2d : supp(f ) ⊆ A}.

(4.4)

It is well known (see, e.g., Parrilo, 2003, 2013) that a
polynomial f ∈ R[x]n,2d is SOS if and only if there exist a
set of exponents B ⊆ Nn
d and a positive semideﬁnite matrix
Q ∈ S|B|
+ such that

f (x) = (xB)T Q xB.

(4.5)

In particular, if f is SOS, this so-called Gram matrix repre-
sentation (4.5) is guaranteed to exist with (Reznick, 1978)

B =

1
2

New(f ) ∩ Nn
d .

(4.6)

The exponent set obtained with this Newton polytope re-
duction can be simpliﬁed further using more general facial
reduction techniques (L¨ofberg, 2009; Permenter & Parrilo,
2014a,b; Waki & Muramatsu, 2010). These techniques an-
alyze the support of f in order to remove redundant ele-

It is clear that SOS polynomials are nonnegative glob-
ally. The converse is true only for univariate polynomials
(n = 1, d arbitrary), quadratic polynomials (d = 1, n ar-
bitrary), and bivariate quartics (n = 2, d = 2) (Hilbert,
1888).
In general, therefore, being SOS is only a suﬃ-
cient condition for global nonnegativity, and there are well-
known examples of nonnegative polynomials that are not
SOS, such the Motzkin polynomial (Motzkin, 1967). How-
ever, while verifying polynomial nonnegativity is an NP-
hard problem (Murty & Kabadi, 1987), checking whether
a polynomial f is SOS can be done in polynomial time by
solving an SDP. Speciﬁcally, for each exponent α ∈ B + B,
let Aα ∈ S|B| be the symmetric binary matrix satisfying
(cid:40)

1, β + γ = α,
0, otherwise,

(4.7)

[Aα]β,γ :=

and observe that

(xB)T Q xB = (cid:104)Q, xB(xB)T(cid:105) =

(cid:88)

(cid:104)Q, Aα(cid:105)xα.

(4.8)

α∈B+B

Then, condition (4.5) holds if and only if (cid:104)Q, Aα(cid:105) = fα for
all α ∈ B + B and we conclude that

f ∈ Σn,2d ⇐⇒

(cid:40)

∃Q ∈ S|B|
+ such that
(cid:104)Q, Aα(cid:105) = fα ∀α ∈ B + B.

(4.9)

The condition on the right-hand side deﬁnes an SDP, so a
positive semideﬁnite Gram matrix Q certifying that f is
SOS can (in principle) be constructed in polynomial time.

4.1.2. SOS polynomial matrices and SDPs
Let R[x]r×s
n,d be the space of r×s matrices whose entries are
n-variate polynomials of degree d. We say that a symmet-
ric polynomial matrix P ∈ R[x]r×r
n,2d is positive semideﬁnite
(resp. deﬁnite) globally if P (x) (cid:23) 0 (resp. P (x) (cid:31) 0) for
all x ∈ Rn. We also say that P is positive semideﬁnite
locally on a set K if the same conditions hold for x ∈ K,
but not necessarily otherwise.

A symmetric polynomial matrix P ∈ R[x]r×r

n,2d is called
SOS if there exists an integer s and a polynomial matrix
M ∈ R[x]s×r

n,d such that

P (x) = M (x)TM (x).

(4.10)

The set of r × r SOS polynomial matrices with entries in
R[x]n,2d will be denoted by Σr
n,2d. All SOS polynomial
matrices are clearly positive semideﬁnite globally, and the
converse is true in the univariate case (n = 1); see Aylward
et al. (2007) for a recent proof.

It is well known (see, e.g., Gatermann & Parrilo, 2004;
Kojima, 2003; Parrilo, 2013) that a symmetric polynomial
matrix P ∈ R[x]r×r
n,2d is SOS if and only if it admits a Gram

19

matrix representation in the form

P (x) = (Ir ⊗ xB)T Q (Ir ⊗ xB)

(4.11)

for some exponent set B ⊆ Nn
d and some positive semidef-
inite symmetric matrix Q ∈ Sr|B|
+ . One may always take
B = Nn
d , and smaller exponent sets can be constructed
with the same reduction techniques used for SOS poly-
nomials. As in the scalar case (r = 1), condition (4.11)
deﬁnes a set of aﬃne constraints on Q, so verifying that a
polynomial matrix is SOS amounts to solving an SDP.

4.2. Sparse SOS decompositions

A major obstacle to constructing SOS certiﬁcates of global
polynomial nonnegativity via semideﬁnite programming is
that the matrix Q is both dense and very large. If f ∈
R[x]n,2d has dense support supp(f ) = Nn
2d, then one must
(cid:1) dense matrix. Often,
d and Q is a (cid:0)n+d
take B = Nn
however, the support of f is small, i.e., |supp(f )| is much
smaller than (cid:0)n+2d
(cid:1). This property, called term sparsity
(Wang et al., 2019, 2021a,b, 2020a), can be exploited in
various ways to reduce the computational complexity of
the SDP in (4.9).

(cid:1)×(cid:0)n+d

2d

d

d

The facial reduction techniques mentioned above, which
replace the full exponent set Nn
d with a (sometimes sig-
niﬁcantly) smaller subset, are arguably the simplest way
to exploit term sparsity. However, as the next example
demonstrates, they are often not suﬃcient.

Example 4.1. Fix n = 50 and d = 2. The support of

ated from Nn
selection technique.

d using facial reduction or any other exponent

4.2.1. General approach
Let A be a small subset of Nn
2d and f be a term-sparse
polynomial supported on A. To reduce the cost of testing
if f is SOS, a natural idea is to check whether f belongs
to a subset of the sparse SOS cone Σ[A] that admits a
semideﬁnite representation with low computational com-
plexity. Such a subset can be constructed using a simple
strategy: prescribe a sparsity graph G(B, E) for the Gram
matrix Q and impose its positive semideﬁniteness through
matrix decomposition.

Precisely, let G(B, E) be a graph with maximal cliques

C1, . . . , Ct and with edge set E ⊆ B × B satisfying

A ⊆ {β + γ : (β, γ) ∈ E}.

(4.13)

Consider the cone of sparse SOS polynomial whose Gram
matrix Q has sparsity graph G and admits the clique-based
positive semideﬁnite decomposition

Q =

t
(cid:88)

k=1

ET
Ck

SkECk ,

Sk ∈ S|Ck|
+ .

(4.14)

We denote this cone by

Σ[A; E] := (cid:8)f ∈ Σ[A] : f (x) = (xB)T Q xB,

Q satisﬁes (4.14)(cid:9).

(4.15)

f (x) =

49
(cid:88)

(xi−1 + xi + xi+1)4

(4.12)

i=2

Conditions (4.13) and (4.14) imply that Σ[A; E] ⊆ Σ[A].
Moreover, inserting the clique-based decomposition (4.14)
of Q into (4.9) one ﬁnds that f ∈ Σ[A; E] if and only if

4

contains only 485 out of the (cid:0)50+4
(cid:1) = 316 251 possible
monomials, so f is term sparse. However, it is not hard
to check that the Newton polytope New(f ) consists of all
points ξ ∈ R50
+ with (cid:107)ξ(cid:107)1 = 4, so the Newton-reduced ex-
2 = N50
ponent set B = 1
1 contains all
homogeneous exponents of degree 2. Therefore, Newton
polytope reduction removes only (cid:0)50+1
(cid:1) = 51 of the possi-
(cid:1) = 1326 in the full set N50
ble (cid:0)50+2
2 , and the SDP in (4.9)
(cid:4)
still involves a 1275 × 1275 Gram matrix Q.

2 New(f ) ∩ N50

2 \ N50

2

1

Techniques to exploit term sparsity beyond what can be
achieved with facial reduction methods alone are clearly
desirable. Section 4.2.1 describes a general strategy to
search for sparse SOS decompositions, which is based on
the same matrix decomposition approach used to tackle
large-scale sparse SDPs in Section 3.
Sections 4.2.2
and 4.2.3 show that diﬀerent types of sparse SOS decom-
positions proposed in the literature are particular cases of
this general approach. Section 4.2.5 outlines how these
methods can be extended to prove polynomial nonneg-
ativity on basic semialgebraic sets, rather than globally.
Throughout, B will denote a ﬁxed set of candidate expo-
nents for the SOS decomposition of a polynomial f , gener-

20

∃S1 ∈ S|C1|

+ , . . . , St ∈ S|Ct|

+

such that

t
(cid:88)

(cid:104)Sk, ECk AαET
Ck

(cid:105) = fα ∀α ∈ B + B.

(4.16)

k=1

If the cliques of the prescribed sparsity graph are small,
the right-hand side is an SDP with small semideﬁnite cones
and can be solved more eﬃciently than (4.9).

Remark 4.1 (Chordality of the sparsity graph). The
Gram matrix decomposition (4.14) is motivated by the
chordal decomposition result in Theorem 2.1. However,
we do not assume here that the sparsity graph G(B, E)
is chordal, so (4.14) is generally not equivalent to requir-
ing Q ∈ S|B|
+ (E, 0). The lack of chordality makes search-
ing for the maximal cliques C1, . . . , Ct an NP-hard problem
(Tomita et al., 2006). Allowing for nonchordal graphs with
small cliques that can be determined analytically, however,
can be extremely useful when a chordal extension leads to
unacceptably large cliques even if it is approximately min-
imal. Examples of this situation can be found in works by
(cid:4)
Nie & Demmel (2009) and Koˇcvara (2020).

Remark 4.2 (Sparse SOS decompositions). Given a spar-
sity graph G(B, E), the cone Σ[A; E] ⊂ Σ[A] contains spe-
cial SOS polynomials that admit a sparse SOS decomposi-
tion, i.e., a decomposition into a sum of sparse SOS poly-
nomials. Indeed, substituting (4.14) into (4.5) yields

(1,0,0)
•

(0,0,1)
•

•
(1,1,0)

•
(0,1,0)

•
(0,1,1)











t
(cid:88)

f (x) =

(xB)TET
Ck

Sk ECk xB

k=1
t
(cid:88)

k=1

=

(ECk xB)T Sk (ECk xB)
(cid:123)(cid:122)
(cid:125)
(cid:124)
=:σk(x)

.

(4.17)

Each polynomial σk(x) is SOS because Sk is positive
semideﬁnite, and is sparse because the operation ECk xB
extracts a subset of the full monomial vector xB.
(cid:4)

It is important to observe that the reduction in compu-
tational complexity granted by the clique-based decompo-
sition (4.14) usually comes at the expense of conservatism.
This is because sparsity in the support set A does not
guarantee the existence of a sparse Gram matrix Q. For a
given support set A, special choices of the sparsity graph
G(B, E) may ensure that Σ[A; E] = Σ[A] (Zheng & Fan-
tuzzi, 2020, Corollaries 4.1 & 4.2; Mai et al., 2020, The-
orem 2.1; Wang et al., 2019, Theorem 4.1; Wang et al.,
2021b, Theorem 3.3).
In general, however, sparse SOS
polynomials need not admit a sparse SOS decomposition,
so the inclusion Σ[A; E] ⊂ Σ[A] is strict. The next example
illustrates this.

1x2

2 − 2x2
2x2

1 − 2x1x2 + 3x2
2x3 − 54x2x2

Example 4.2. (Klep et al., 2019, Lemma 5.2) Consider
the polynomial f (x) = x2
1x2 +
2x2
3 + 18x2
2 − 2x2x3 + 6x2
3 + 142x2
3, and
set A = supp(f ). Let B ⊂ N3
2 be the exponent set such
that xB = (x1, x1x2, x2, x3, x2x3), which is obtained via
Newton polytope reduction. Consider also the (chordal)
sparsity graph G(B, E) shown in Figure 4.1, which satis-
ﬁes (4.13). We claim that f belongs to Σ[A] but not to
Σ[A; E]. To see this, observe that any Gram matrix repre-
sentation of f must take the form

f (x) =

















x1
x1x2
x2
x3
x2x3

T 






(cid:124)

1 −1 −1
0
2
−1
−1
3
0
0 −α −1
0
α

0
−α
−1
6

9 −27
(cid:123)(cid:122)
Q









,









x1
x1x2
x2
x3
x2x3

α
0
9
−27
142








(cid:125)

where α ∈ R can be chosen arbitrarily. Setting α = 1
makes the Gram matrix Q positive semideﬁnite, so f ∈
Σ[A]. However, f cannot be in Σ[A; E] because this would
require α = 0, for which Q is not positive semideﬁnite. (cid:4)

4.2.2. Correlative sparsity
The general approach presented in Section 4.2.1 requires
specifying the sparsity graph for the Gram matrix Q

21

(a)

(b)

Figure 4.1: (a) Sparsity graph G(B, E) for Example 4.2. The
vertices B are such that xB = (x1, x1x2, x2, x3, x2x3). (b) Spar-
sity pattern of the Gram matrix of SOS polynomials in Σ[A; E].

in (4.5). A natural strategy to do this, pioneered by Waki
et al. (2006) and Lasserre (2006), is to consider the cou-
plings between any two independent variables xi and xj
in a polynomial f supported on A. Two variables xi and
xj are considered coupled if a monomial in the vector xA
depends on both simultaneously, i.e., if there exists α ∈ A
with αiαj > 0. These couplings can be described using
the correlative sparsity (csp) graph of the support set A
(or, alternatively, of the polynomial f ), which has vertices
{1, . . . , n} and edge set

Scsp(A) := {(i, j) : ∃α ∈ A with αiαj > 0}.

(4.18)

Correlatively sparse SOS decompositions are obtained
upon imposing that the entry Qγ,β of the Gram matrix
in (4.5) vanishes if the monomial xβ+γ introduces cou-
plings between variables that are not consistent with the
csp graph of f . This amounts to requiring that Q has
sparsity graph Gcsp(B, Ecsp) with edge set

Ecsp := {(β, γ) ∈ B × B :

(βi + γi)(βj + γj) > 0 ⇒ (i, j) ∈ Scsp(A)}.

(4.19)

One may consider G(B, Ecsp) a “hypergraph” with |B|
nodes, built from the csp graph of f (which has n
nodes) to ensure that polynomials (xB)T Q xB with Q ∈
S|B|(Ecsp, 0) inherit the correlative sparsity of the original
support set A. Unsurprisingly, therefore, the properties of
G(B, Ecsp) can be inferred from those of the (usually much
smaller) csp graph. In the following statement, which can
be proved using arguments similar to those given by Zheng
(2019, Section 2.4.3), nnz(β) denotes the indices of the
nonzero entries of an exponent β.

Proposition 4.1. Suppose that the csp graph of the
support set A has maximal cliques J1, . . . , Jt. Then,
G(B, Ecsp) has maximal cliques Ck = {β ∈ B : nnz(β) ⊆
Jk} for k = 1, . . . , t. Moreover, if the csp graph of A is
chordal, then so is G(B, Ecsp).

Proposition 4.1 considerably simpliﬁes the construction
of the “inﬂation” matrices ECk in (4.14), because it suﬃces
to ﬁnd the maximal cliques of the csp graph of A without
building the (much larger) graph G(B, Ecsp). In addition,
it is not diﬃcult to check that the operation ECk xB ex-
tracts monomials that depend only on variables indexed

1

4

2

3

(a)


















=




















































+

(b)



































+



































+

(cid:123)(cid:122)
Q

(cid:125)

(cid:124)

(cid:123)(cid:122)
S1

(cid:125)

(cid:124)

(cid:123)(cid:122)
S2

(c)

(cid:125)

(cid:124)

(cid:123)(cid:122)
S3

(cid:125)

(cid:124)

(cid:123)(cid:122)
S4


















(cid:125)


















(cid:124)

Figure 4.2: (a) Correlative sparsity graph of the polynomial in Example 4.4. (b) The corresponding sparsity graph G(B, Ecsp).
Graph vertices are labelled by monomials in xB instead of the corresponding exponents in B to ease the visualization. Filled
vertices have a self-loop (not shown), empty ones do not. Colors mark the maximal cliques C1 (
).
(c) Sparsity pattern of the Gram matrix Q induced by G(B, Ecsp) and its clique-based decomposition. The vertices of G(B, Ecsp)
are ordered anticlockwise starting from x1x2. The last two rows and columns of all matrices are empty.

) and C4 (

), C2 (

), C3 (

by Jk. Using (4.17), one concludes that exploiting cor-
relative sparsity amounts to searching for a sparse SOS
decomposition in the form

f (x) =

t
(cid:88)

k=1

σk(xJk ),

σk is SOS,

(4.20)

where xJk denotes the subset of variables x indexed by Jk
(cf. Zheng et al., 2019a, Theorem 2).

Remark 4.3. Example 4.2 shows that correlatively sparse
SOS polynomials need not admit the sparse SOS decom-
position (4.20), even if the csp graph is chordal. Thus, the
inclusion Σ[A; Ecsp] ⊂ Σ[A] is generally strict. For further
discussion on the existence of sparse SOS decompositions
for polynomials with chordal correlative sparsity, see Mai
(cid:4)
et al. (2020) and Zheng & Fantuzzi (2020).

Example 4.3. The quartic polynomial f in (4.12) is
correlatively sparse, and the csp graph of its support is
chordal with maximal cliques Ji = {i, i + 1, i + 2} for
i = 1, . . . , n − 2. It is clear that f admits a sparse SOS
decomposition (4.20) and this can be searched for by solv-
ing the SDP in (4.16). Since, for each clique Ji, only six
elements in B = Nn
1 can be multiplied together with-
out introducing spurious couplings to diﬀerent cliques, this
SDP has semideﬁnite matrix variables S1, . . . , Sn−2 ∈ S6
+.
Its computational complexity is clearly much lower than
the corresponding dense formulation in Example 4.1, and
a sparse SOS decomposition for f can be found in less than
(cid:4)
one second on a standard laptop.

2 \ Nn

Example 4.4. Consider the quartic polynomial

f (x) = 2 + x2

1x2

1x2
4(x2
+ (cid:80)4

i=2

4 − 1) − x2
(cid:0)x2
i−1(x2

i x2

1 + x4
1
i x2

i−1 − 1) − x2

i + x4
i

(cid:1) .

Its csp graph, shown in Figure 4.2(a), is nonchordal and
has maximal cliques J1 = {1, 2}, J2 = {2, 3}, J1 = {3, 4}
and J1 = {4, 1}. The corresponding graph G(B, Ecsp),
where the set of exponents obtained with Newton poly-
tope reduction is B = N4
2, is shown in Figure 4.2(b) and
has cliques C1, . . . , C4 containing 6 elements each, which are
determined using Proposition 4.1. The sparsity pattern of
the Gram matrix Q induced by G(B, Ecsp) and the clique-
based matrix decomposition in (4.14), also illustrated in
the ﬁgure, replaces a 15×15 positive semideﬁnite contraint
on Q with four semideﬁnite constraints on 6 × 6 matrices
S1, . . . , S4. According to (4.20), searching for these matri-
ces is equivalent to looking for a sparse SOS decomposition
f = σ1(x1, x2) + σ2(x2, x3) + σ3(x3, x4) + σ4(x4, x1). Such
a decomposition is not guaranteed to exist even if f were
SOS, but it does for this example with

σ1(x1, x2) = 1
2
σ2(x2, x3) = 1
2
σ3(x3, x4) = 1
2
σ4(x4, x1) = 1
2

(cid:0)x2
(cid:0)x2
(cid:0)x2
(cid:0)x2

1 − 1
2
2 − 1
2
3 − 1
2
4 − 1
2

(cid:1)2

(cid:1)2

(cid:1)2

(cid:1)2

2

+ (cid:0)x1x2 − 1
+ (cid:0)x2x3 − 1
+ (cid:0)x3x4 − 1
+ (cid:0)x4x1 − 1

2

2

2

This proves that f ∈ Σ[supp(f ); Ecsp].

22

(cid:1)2

(cid:1)2

(cid:1)2

(cid:1)2

+ 1
2
+ 1
2
+ 1
2
+ 1
2

(cid:0)x2
(cid:0)x2
(cid:0)x2
(cid:0)x2

2 − 1
2
3 − 1
2
4 − 1
2
1 − 1
2

(cid:1)2

(cid:1)2

(cid:1)2

(cid:1)2

.

(cid:4)

4.2.3. TSSOS, chordal-TSSOS and related hierarchies
Fix an exponent set A ⊂ Nn
2d and a polynomial f with
supp(f ) ⊆ A. Correlative sparsity exploits only the sparse
couplings between variables as encoded by the csp graph
of A, but does not take into account any further struc-
ture of A. This is not eﬃcient when |A| is much smaller
than (cid:0)n+2d
(cid:1), so f is term-sparse, but the csp graph is fully
connected or nearly so.

2d

For this reason, Wang et al. (2019, 2021a,b) introduced
the term-sparse-SOS (TSSOS) and the chordal-TSSOS de-
composition hierarchies, which exploit term sparsity irre-
spective of whether f is correlatively sparse. These are
two particular examples of a broader family of possible
sparsity-exploiting SOS decomposition hierarchies, each of
which is obtained upon imposing the clique-based Gram
matrix decomposition (4.14) for a sequence {G(B, Ek)}k≥1
of increasingly connected sparsity graphs (Ek ⊆ Ek+1).

Irrespective of the particular hierarchy being considered
(TSSOS, chordal-TSSOS, or another), the construction of
such sparsity graphs begins with the observation that, in
order to ensure (4.13), each edge set Ek should contain at
least all edges (β, γ) with β + γ ∈ A. This guarantees that
A ⊆ supp((xB)T Q xB) for any Gram matrix Q deﬁned via
the clique-based decomposition (4.14), which is necessary
for the feasibility of the SDP in (4.16). One should also not
force diagonal entries Qββ of the Gram matrix to vanish,
because this amounts to saying that the monomial xβ is
redundant and β could be removed from the exponent set
B. For these reasons, we deﬁne an initial exponent set B0
and an initial edge set E0 as

Diﬀerent extension operators produce diﬀerent types of

sparse SOS decomposition hierarchies. In particular:
• If E is a block-completion operator that completes all
connected components of the edge set {(β, γ) ∈ B × B :
β +γ ∈ Bk−1}, one recovers the TSSOS hierarchy (Wang
et al., 2019, 2021b). At each step of the hierarchy, Q has
chordal sparsity (speciﬁcally, a block-diagonal structure)
and (4.14) is equivalent to imposing Q ∈ S|B|

+ (Ek, 0).

• If E is an approximately minimal chordal extension oper-
ator that extends the edge sets {(β, γ) ∈ B × B : β + γ ∈
Bk−1} such that G(B, Ek) is chordal, one recovers the
chordal-TSSOS hierarchy (Wang et al., 2021a). At each
step of the hierarchy, Q has chordal sparsity and (4.14)
is equivalent to requiring Q ∈ S|B|

+ (Ek, 0).

In both cases, the edge extensions are performed on a
graph with |B| nodes and the maximal cliques of G(B, Ek)
must be found at each iteration. This is unlike the correla-
tive sparsity strategy in Section 4.2.2, where the maximal
cliques of G(B, Ecsp) are built from those in the csp graph
of A, which has only n nodes (cf. Proposition 4.1).

It is also clear that the choice of extension operator
determines the computational complexity of the result-
ing sparse SOS decomposition hierarchy, as well as the
gap between Σ[A, E ∗] and Σ[A]. For example, the chordal-
TSSOS hierarchy has a lower complexity than the TSSOS
one in general, as its sparsity graphs have fewer edges (see
Wang et al., 2021a,b for detailed complexity estimates).
However, the TSSOS hierarchy has a higher representa-
tion power because Σ[A, E ∗] = Σ[A], which is generally
not true for the chordal-TSSOS hierarchy.

B0 := 2B ∪ A,

E0 = {(β, γ) ∈ B × B : β + γ ∈ B0}.

(4.21a)

(4.21b)

Theorem 4.1 (Wang et al., 2021b). If E ∗
lized edge set of the TSSOS hierarchy, then Σ[A, E ∗
Σ[A], i.e., f is SOS if and only if f ∈ Σ[A, E ∗
tssos].

tssos is the stabi-
tssos] =

Next, consider an extension operator E : B × B → B × B,
which extends a given edge set E ⊂ B × B according to a
given rule. The edge sets E1 ⊆ E2 ⊆ · · · ⊆ Ek ⊆ · · · and
their corresponding support sets Bk are deﬁned using the
iterative rule

Ek := E ({(β, γ) ∈ B × B : β + γ ∈ Bk−1}) ,
Bk := {β + γ : (β, γ) ∈ Ek}.

(4.22a)

(4.22b)

Note that Ek ⊆ {(β, γ) ∈ B × B : β + γ ∈ Bk}, so the
extension operator guarantees that Ek ⊆ Ek+1. Moreover,
the sequence {Ek}k≥1 must converge to an edge set E ∗
in a ﬁnite number of iterations because Ek cannot be ex-
tended beyond the complete edge set B × B. The sequence
of sparsity graphs {G(B, Ek)}k≥1 obtained in this way is
therefore ﬁnite, and yields the (ﬁnite) hierarchy of nested
sparse SOS cones

Remark 4.4. Theorem 4.1 follows from a stronger re-
sult (Wang et al., 2021b, Theorem 6.5) which reveals that
the constraint Q ∈ S|B|
tssos, 0) imposes the well-known
block-diagonal structure implied by the sign symmetries
(cid:4)
of f (see, e.g., L¨ofberg, 2009).

+ (E ∗

Example 4.5. The trivariate quartic polynomial

f (x) = 1 + x4

1 + x4

2 + x4

3 + x2

1x2

2 + x2

1x2

3 + x2

2x2

3 + x2x3

is term sparse but not correlatively sparse, since its csp
graph is a complete graph with three nodes. The candidate
exponent set to search for an SOS decomposition of f is
B = N3
2, as Newton polytope reduction removes no terms.
For convenience, we order B such that

xB = (x2

3, x2

2, x2

1, x2x3, 1, x1, x1x3, x1x2, x3, x2)T.

Σ[A; E1] ⊆ Σ[A; E2] ⊆ · · · ⊆ Σ[A; E ∗] ⊆ Σ[A].

(4.23)

Here, Σ[A; E] is as deﬁned in (4.15) and all inclusions are
strict in general.

The TSSOS hierarchy yields the sparsity graphs shown
in Figure 4.3, which stabilize at the second iteration
(k = 2). The corresponding sparsity patterns of the Gram
matrix Q are also shown in that ﬁgure. Observe how

23





























































Figure 4.3: Sparsity graphs and corresponding matrix spar-
sity patterns for the TSSOS hierarchy in Example 4.5 at initial-
ization (top; edge set E0), at the ﬁrst iteration (middle; edge
set E1) and at the second iteration (bottom; edge set E2). Af-
ter that, the hierarchy stabilizes. Graph vertices are labelled
by monomials in xB instead of the corresponding exponents in
B to ease the visualization. Colors mark the maximal cliques.

3, x2

the connected components of the initial graph G(B, E0)
are completed at the ﬁrst iteration to obtain the graph
G(B, E1). As discusses in Remark 4.4, the stabilized block-
diagonal structure of Q coincides with the partition of
xB into the groups {x2
2, x2
1, 1, x2x3}, {x1}, {x3, x2}
and {x1x2, x1x3} implied by the sign symmetries of f ,
which is invariant under the transformations (x1, x2, x3) (cid:55)→
(−x1, −x2, −x3) and (x1, x2, x3) (cid:55)→ (x1, −x2, −x3) (the
four groups of monomials are invariant under both, the
ﬁrst, the second, and none of these transformations). In
this example, the SDP in (4.16) is feasible at all itera-
tions of the TSSOS hierarchy because f admits the posi-
tive semideﬁnite Gram matrix representation

f (x) =

(xB)T

1
8

















8
3
4
0
0
0
0
0
0
0

3
8
4
0
0
0
0
0
0
0

4
4
8
0
0
0
0
0
0
0

0
0
0
2
4
0
0
0
0
0

0
0
0
4
8
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0


0
0


0


0

0


0


0


0


0
0

xB

and the Gram matrix is consistent with the sparsity graphs
in Figure 4.3. Thus, all steps of the TSSOS hierarchy are

24





















Figure 4.4: Sparsity graph (left) and corresponding matrix
sparsity pattern (right) for the chordal-TSSOS hierarchy in Ex-
ample 4.5, which stabilizes at the ﬁrst iteration (k = 1). Graph
vertices are labelled by monomials in xB instead of the corre-
sponding exponents in B to ease the visualization. Colours
mark the maximal cliques; multicoloured vertices and matrix
entries belong to multiple cliques.

able to prove that f is SOS. Note that this can be guaran-
teed a priori only for the last step by virtue of Theorem 4.1.
For the same polynomial f , the chordal-TSSOS hier-
archy stabilizes at the ﬁrst iteration (k = 1) and yields
the sparsity graph shown in Figure 4.4. The correspond-
ing Gram matrix Q is sparser than those encountered in
the TSSOS hierarchy, leading to smaller semideﬁnite con-
straints in (4.16). Again, this SDP is feasible in light of the
Gram matrix decomposition given above, so the chordal-
TSSOS hierarchy is able to prove that f is SOS. This,
(cid:4)
however, cannot be guaranteed a priori.

Remark 4.5. The explicit Gram matrix decomposition
in Example 4.5 reveals that the smaller monomial basis
xB = (x2
3, x2
1, x2x3, 1) would suﬃce to construct an SOS
decomposition of f .
It remains to be seen whether this
reduced basis can be identiﬁed using strategies that are
more sophisticated than the Newton polytope reduction.

2, x2

4.2.4. Correlatively term-sparse hierarchies
The sparse SOS decomposition hierarchies described in
Section 4.2.3 can be combined with the correlative sparsity
techniques outlined in Section 4.2.2 in a natural way. Let
Ek be the edge set obtained using the iterations in (4.22)
for a given extension operator, and let Ecsp be the edge set
in (4.19) constructed using correlative sparsity. Then, the
sequence of sparsity graphs

G(B, Ek ∩ Ecsp),

k ≥ 1

(4.24)

yields a hierarchy of “correlatively term-sparse” SOS de-
compositions, which exploit simultaneously term and cor-
relative sparsity. Since Ek ⊆ Ek+1 by construction, and
since the sequence {Ek} stabilizes onto an edge set E ∗ in a
ﬁnite number of steps, the sparse SOS cones corresponding
to this hierarchy satisfy

Σ[A; E1 ∩ Ecsp] ⊆ Σ[A; E2 ∩ Ecsp] ⊆ · · ·

· · · ⊆ Σ[A; E ∗ ∩ Ecsp] ⊆

(cid:40)

Σ[A; E ∗]
Σ[A; Ecsp],

and all inclusions are generally strict. Note also that one
may remove from B all exponents that violate the correla-
tive sparsity before constructing the edge sets Ek, because
the intersection with Ecsp eliminates all edges between such
exponents (including self-loops).

When the extension operator used to build Ek is the
block-completion operation used in the TSSOS hierar-
chy, the sparsity graphs in (4.24) yield exactly the CS-
TSSOS hierarchy introduced by Wang et al. (2020a). In
this case, by Theorem 4.1, the stabilized sparsity graph
G(B, E ∗ ∩ Ecsp) simply encodes sign symmetries and correl-
ative sparsity. Since exploiting sign symmetries in SOS de-
compositions brings no conservatism (L¨ofberg, 2009), one
immediately obtains the following corollary.

Proposition 4.2. If E ∗
TSSOS hierarchy, then Σ[A; E ∗
any exponent set A ⊆ Nn

2d.

tssos is the stabilized edge set of the
tssos ∩ Ecsp] = Σ[A; Ecsp] for

Example 4.6. (Wang et al., 2020a, Example 3.4) Let

f (x) = 1 + x1x2x3 + x3x4x5 + x3x4x6

+ x3x5x6 + x4x5x6 +

6
(cid:88)

i=1

x4
i .

(4.25)

This polynomial is both term and correlatively sparse,
and its csp graph has two maximal cliques J1 = {1, 2, 3}
and J2 = {3, 4, 5, 6}.
It is also invariant under the
sign symmetry transformation (x1, x2, x3, x4, x5, x6) (cid:55)→
(−x1, −x2, x3, x4, x5, x6). To search for an SOS decom-
position of f using the CS-TSSOS hierarchy, we let B be
the exponent set deﬁning the monomial vector

xB = (x2, x1x3, x2x3, x1, x1x2, x3, x5x6, x4x6, x3x6,
5, x2

x5, x4x5, x3x5, x4, x3x4, x6, x2

3, 1, x2

2, x2

1, x2

6, x2

4).

This is obtained upon removing from the full basis xN6
2 all
monomials that violate the correlative sparsity of f (these
would be removed anyway by the CS-TSSOS hierarchy).
The CS-TSSOS sparsity graphs obtained with (4.24)
and the corresponding sparsity patterns for the Gram ma-
trix Q of f , illustrated in Figure 4.5, are chordal. The
hierarchy stabilizes after three steps. At the ﬁrst step,
the clique-based decomposition (4.14) replaces the 22 × 22
semideﬁnite constraint on the Gram matrix Q with six
semideﬁnite constraints of size 2, 2, 2, 10, 4 and 5. The ﬁrst
step of the TSSOS hierarchy, instead, leads to an SDP with
ﬁve semideﬁnite constraints of size 2, 2, 2, 10, 7 (Wang
et al., 2020a, Example 3.4). The second iteration of the
CS-TSSOS hierarchy produces signiﬁcant ﬁll-in, and the
size of the largest semideﬁnite constraint increases to 15.
The third iteration brings only minimal additional ﬁll-in.
At this ﬁnal stage, the connected components of the spar-
sity graph correspond to a partition of the monomials xB
according to the sign symmetry of f (the ﬁrst four mono-
mials in xB are not invariant under the symmetry trans-
formation, while the rest are), but the correlative sparsity



































































































































































Figure 4.5: Sparsity graphs and corresponding matrix spar-
sity patterns for the ﬁrst (top), second (middle) and third (bot-
tom) iterations of the CS-TSSOS hierarchy in Example 4.6. Af-
ter that, the hierarchy stabilizes. Graph vertices are labelled by
monomials in xB instead of the corresponding exponents in B to
ease the visualization. Colors mark the maximal cliques; mul-
ticolor vertices and matrix entries belong to multiple cliques.

prevents the completion of the largest connected compo-
nent, i.e., of the bottom-right connected matrix block in
Figure 4.5(c).

Numerical solution of the SDP (4.16) shows that all
steps of the CS-TSSOS hierarchy are feasible, so an SOS
decomposition of the polynomial f in (4.25) can be con-
structed at a lower computational cost than any other hi-
erarchy discussed in this work. Note that feasibility cannot
be guaranteed a priori at any step of the hierarchy, even the
last (stabilized) one, due to the conservative nature of cor-
relatively sparse SOS decomposition (see Remark 4.3). (cid:4)

4.2.5. Sparse SOS decompositions on semialgebraic sets
The sparsity-exploiting methods to construct SOS decom-
positions described so far prove global polynomial nonneg-
ativity, but can be extended to establish local nonnegativ-
ity on a basic semialgebraic set deﬁned by m polynomial

25

inequalities,

K := {x ∈ Rn : g1(x) ≥ 0, . . . , gm(x) ≥ 0}.

(4.26)

Set g0(x) ≡ 1 for convenience. To verify that f ∈ R[x]n,d
(not necessarily of even degree) is nonnegative on K, it
suﬃces to ﬁnd an integer ω (known as the relaxation order )
and exponent sets B0, . . . , Bm ⊆ Nn

ω such that

f (x) =

m
(cid:88)

i=0

gi(x)(xBi)T Qi xBi,

Qi ∈ S|Bi|
+ .

(4.27)

As before, these conditions deﬁne the feasible set of an
SDP. Generally, one chooses ω such that

2ω ≥ max{deg(f ), deg(g1), . . . , deg(gm)}

and then takes

Bi = Nn
ωi

,

ωi := ω − (cid:100) 1

2 deg(gi)(cid:101),

where (cid:100)a(cid:101) is the smallest integer greater than or equal to
a. This ensures that each term in the sum in (4.27) is a
polynomial of degree at most 2ω. One can allow for 2ω >
deg(f ) because cancellations may occur when summing all
terms.

Since each polynomial σi(x) := (xBi)T Qi xBi in (4.27) is
SOS, this condition gives a weighted SOS decomposition
of f , meaning a representation of f as a weighted sum
of SOS polynomials where the weights are g0 = 1 and
the polynomials g1, . . . , gm appearing in the semialgebraic
deﬁnition of K. Remarkably, this suﬃcient condition for
local nonnegativity is also necessary if f is strictly positive
on K and this is a compact set satisfying the so-called
Archimedean condition.

Assumption 1 (Archimedean condition). There exists an
integer ν ≥ 0, SOS polynomials σ0, . . . , σm ∈ Σn,2ν, and a
constant r ∈ R such that r2 − (cid:107)x(cid:107)2

i=0 gi(x)σi(x).

2 = (cid:80)m

Theorem 4.2 (Putinar, 1993). Suppose that f ∈ R[x]n,d
is strictly positive on a basic semialgebraic set K deﬁned as
in (4.26) that satisﬁes the Archimedean condition. Then,
there exists a relaxation order ω such that f admits the
weighted SOS decomposition (4.27).

If f and the polynomials g1, . . . , gm are term sparse, one
can proceed as in Section 4.2.1 and attempt to reduce the
computational complexity of the generic weighted SOS de-
composition (4.27) by requiring the matrices Q0, . . . , Qm
to be sparse and to admit a clique-based positive semidef-
inite matrix decomposition. The only new aspect is that
one must consider how the sparse polynomial (xBi)T Qi xBi
interacts with the corresponding gi in order to determine
the overall structure of the sum on the right-hand side
of (4.27). This requires some care, especially if one hopes
to recover sparse versions of Theorem 4.2.

To give an example of this general strategy, let us ex-
plain how to extend the correlative sparsity technique out-

26

lined in Section 4.2.2. In this case, one replaces the csp
graph of f constructed with the joint csp graph of the poly-
nomials f, g1, . . . , gm, which has vertices {1, . . . , n} and an
edge between vertices i and j if at least one of the following
conditions hold:

(a) The variables xi and xj are multiplied together in f ;
(b) At least one of g1, . . . , gm depends on both xi and xj,
even if these variables are not multiplied together.

The diﬀerent treatment of f and g1, . . . , gm reﬂects the
asymmetric role these polynomials play in (4.27). Then,
one imposes that each matrix Qi in (4.27) is the densest
possible matrix such that the support of gi(x)(xBi)T Qi xBi
is consistent with the joint csp graph. Precisely,
let
J1, . . . , Jt be the maximal cliques of the joint csp graph
and, for each i = 0, . . . , m, let var(gi) ⊂ {1, . . . , n} be
the set of indices of the variables on which gi depends,
with the convention that var(g0) = var(1) = ∅. By con-
dition (b) above, there is at least one clique Jk such that
var(gi) ⊆ Jk and we denote the set of clique indices k for
which this holds by

Ni := {k ∈ {1, . . . , t} : var(gi) ⊆ Jk} .

(4.28)

Observe in particular that N0 = {1, . . . , t} since var(g0) =
∅ ⊂ Jk for all k = 1, . . . , t. The sparsity graph Gi(Bi, Ei)
of Qi is deﬁned to have the edge set

(cid:91)

Ei :=

{(β, γ) ∈ Bi × Bi : nnz(β + γ) ⊆ Jk} .

(4.29)

k∈Ni

One can check that Gi(Bi, Ei) is chordal if so is the joint csp
graph of f, g1, . . . , gm. Moreover, it has maximal cliques
Ci,1, . . . , Ci,|Ni| with Ci,k := {β ∈ Bi : nnz(β) ⊆ Jk}. Con-
sequently, the clique-based positive semideﬁnite decompo-
sition of Qi reads

Qi =

|Ni|
(cid:88)

k=1

ET

Ci,k

SkECi,k ,

Sk ∈ S|Ci,k|

+

.

(4.30)

If the cliques Ci,k are small, imposing this clique-based
decomposition for each i = 0, . . . , m in (4.27) allows one to
search for a weighted SOS decomposition of f by solving
an SDP with low computational complexity. Moreover,
arguing as in Remark 4.2, one concludes that this process
yields the representation

f (x) =

m
(cid:88)

(cid:88)

i=0

k∈Ni

gi(x)σi,k(xJk ),

(4.31)

where each SOS polynomial σi,k depends only on variables
indexed by a single clique of the joint csp graph. Cru-
cially, the following sparse version of Theorem 4.2 guaran-
tees that such a sparse weighted SOS decomposition exists
if the joint csp graph is chordal, the semialgebraic deﬁni-
tion of the set K in (4.26) includes inequalities of the form
k − (cid:107)xJk (cid:107)2
r2

2 ≥ 0 for all k = 1, . . . , t, and f > 0 on K.

Theorem 4.3 (Grimm et al., 2007; Lasserre, 2006). Let
f be a polynomial that is strictly positive on a basic semi-
algebraic set K = {x ∈ Rn : g1(x) ≥ 0, . . . , gm(x) ≥ 0},
whose deﬁnition includes the inequalities r2
k − (cid:107)xJk (cid:107)2 ≥ 0
for some constants r1, . . . , rt and all k = 1, . . . , t. If the
joint csp graph of f, g1, . . . , gm is chordal, f has a sparse
weighted SOS decomposition in the form (4.31).

Remark 4.6. The assumption that the semialgebraic def-
inition of K includes the inequalities r2
2 ≥ 0 can
be weakened by requiring that the |Jk|-dimensional set

k − (cid:107)xJk (cid:107)2

2009). Note that all results presented in this section con-
sider the structural sparsity of polynomial matrices, not
their term sparsity. In principle, one could exploit both
structural and term sparsity by combining the results re-
viewed below with those of Section 4.2.

4.3.1. Global decomposition
Consider a symmetric n-variate polynomial matrix P ∈
R[x]r×r
n,2d of degree 2d whose (structural) sparsity pattern
is described by a chordal graph G({1, . . . , r}, E), i.e.,

Kk := {ˆx ∈ R|Jk| : gi(ˆx) ≥ 0 ∀i s.t. var(gi) ⊆ Jk}

(i, j) /∈ E

=⇒ Pij(x) ≡ 0 ∀x ∈ Rn.

(4.32)

satisﬁes the Archimedean condition for each k = 1, . . . , t.
Moreover, the assumption is mild when K is compact be-
cause, in principle, the inequalities r2
k − (cid:107)xJk (cid:107)2 ≥ 0 can
be added with values of rk large enough not to change the
set K. Proving that K remains unchanged for candidate
(cid:4)
rk, however, may not be easy in practice.

The TSSOS, chordal-TSSOS and CS-TSSOS hierarchies
can also be extended to produce weighted SOS decompo-
sition on basic semialgebraic sets (Wang et al., 2021a,b,
2020a). Interested readers are referred to these works for
the details. Here, we simply observe that, just like their
global counterparts described in Sections 4.2.3 and 4.2.4,
these extended hierarchies stabilize after a ﬁnite num-
ber of steps. Upon stabilization, moreover, the extended
TSSOS and CS-TSSOS hierarchies recover the block-
diagonal structure of the matrices Q0, . . . , Qm implied
by joint sign symmetries of the polynomials f, g1, . . . , gm
(see Wang et al., 2021b, Theorem 6.5 and Corollary 6.8;
Wang et al., 2020a, Proposition 3.10). This observation
can be combined with a symmetry-exploiting version of
Theorem 4.2 (Riener et al., 2013, Theorem 3.5) and with
Theorem 4.3 to conclude that the TSSOS and CS-TSSOS
hierarchies are guaranteed to work for term-sparse poly-
nomials that are strictly positive on compact sets whose
semialgebraic deﬁnition satisﬁes suitable versions of the
Archimedean condition.

4.3. Decomposition of sparse polynomial matrices

Having studied sparsity-exploiting techniques to reduce
the complexity of searching for SOS representations for
term-sparse polynomials, we now switch gear and review
how chordal sparsity can be exploited when looking for
SOS representations of sparse polynomial matrices. Sec-
tion 4.3.1 presents results by Zheng & Fantuzzi (2020)
that partially extend the classical chordal decomposition
theorem (Theorem 2.1) to SOS polynomial matrices with
chordal sparsity. Decomposition results giving SOS certiﬁ-
cates of matrix positivity on semialgebraic sets are brieﬂy
outlined in Section 4.3.2. All of these results are useful for
static output controller design (Henrion & Lasserre, 2006),
robust stability region analysis (Henrion & Lasserre, 2011),
and stability analysis of time-delay systems (Peet et al.,

Since checking whether P is positive semideﬁnite globally
via the SOS certiﬁcates described in Section 4.1.2 is expen-
sive when r is large, we seek to exploit the sparsity of P
and replace one large matrix SOS constraint with multiple
smaller ones.

Let C1, . . . , Ct be the maximal cliques of G. If P is pos-
itive semideﬁnite globally, then applying Theorem 2.1 for
each x ∈ Rn reveals that there exists x-dependent positive
semideﬁnite matrices Sk : Rn → S|Ck|

+ such that

P (x) =

t
(cid:88)

i=1

ET
Ck

Sk(x) ECk .

(4.33)

However, this decomposition is not immediately useful in
practice because the matrices Sk need not be polynomial,
so they cannot be searched for using SOS methods. As an
example, consider

P (x) =





2 + x2
x + x2
0

0

x + x2
1 + 2x2 x − x2
2 + x2
x − x2



 ,

whose sparsity graph is a simple three-node chain graph
with two maximal cliques, C1 = {1, 2} and C2 = {2, 3}.
Zheng & Fantuzzi (2020) proved that this matrix is pos-
itive deﬁnite globally, but does not admit a chordal de-
composition (4.33) with polynomial S1 and S2. Using this
example, and recalling that all positive semideﬁnite uni-
variate polynomial matrices are also SOS, one can prove
the following general statement.

Proposition 4.3 (Zheng & Fantuzzi, 2020). Let G be a
connected and not complete chordal graph with r ≥ 3 ver-
tices and maximal cliques C1, . . . , Ct. For any positive in-
tegers n and d, there exists a positive deﬁnite SOS matrix
P ∈ Σr
n,2d with sparsity graph G that does not admit a
decomposition (4.33) with polynomial matrices S1, . . . , St.

On the other hand, the direct proof of Theorem 2.1
given by Kakimura (2010) can be combined with a di-
agonalization procedure for polynomial matrices due to
Schm¨udgen (2009) to show that (4.33) holds with SOS
matrices S1, . . . , Sk for all positive semideﬁnite polynomial
matrices, up to multiplication by an SOS polynomial.

27

Theorem 4.4 (Zheng & Fantuzzi, 2020). Let P ∈ R[x]r×r
n,2d
be positive semideﬁnite and let C1, . . . , Ct be the maximal
cliques of its sparsity graph. There exist ν ∈ N, an SOS
polynomial σ ∈ Σn,2ν, and SOS polynomial matrices Sk ∈
Σ|Ck|

n,2d+2ν for k = 1, . . . , t, such that

σ(x)P (x) =

t
(cid:88)

k=1

ET
Ck

Sk(x) ECk .

(4.34)

When the maximal cliques of the sparsity graph of P are
small, this result enables one to construct an SOS certiﬁ-
cate of global positive semideﬁnitess using small matrix
SOS constraints, which have a much lower computational
complexity than simply requiring σP to be SOS.

Implementation of the chordal SOS decomposition in
Theorem 4.4 using SDPs requires the matrix P to be ﬁxed,
because the SOS weight σ must be determined alongside
the SOS matrices S1, . . . , St. Often, however, P depends
on a vector of parameters λ ∈ R(cid:96) that must be optimized
whilst ensuring that P is positive semideﬁnite. In these
cases, condition (4.34) is not jointly convex in λ and σ,
so the latter must be ﬁxed a priori. This is generally
restrictive because, when σ is ﬁxed arbitrarily, Proposi-
tion 4.3 implies that the decomposition (4.34) may not
exist. However, one can prove a sparse-matrix version of
Reznick’s Positivstellensatz (Reznick, 1995) to conclude
that the weight σ(x) = (cid:107)x(cid:107)2ν
is guaranteed to work at
2
least when P is a homogeneous positive deﬁnite matrix.

Theorem 4.5 (Zheng & Fantuzzi, 2020). Let P ∈ R[x]r×r
n,2d
be homogeneous of degree 2d and positive deﬁnite on
Rn \ {0}. Let C1, . . . , Ct be the maximal cliques of the spar-
sity graph of P . There exist ν ∈ N and SOS polynomial
matrices Sk ∈ Σ|Ck|

n,2d+2ν for k = 1, . . . , t, such that

(cid:107)x(cid:107)2ν

2 P (x) =

t
(cid:88)

k=1

ET
Ck

Sk(x) ECk .

Decomposition results such as this, where the SOS
weight σ is ﬁxed, are of considerable interest because
they enable the construction of convergent hierarchies of
sparsity-exploiting SOS relaxations for optimization prob-
lems with global polynomial matrix inequalities (see Hen-
rion & Lasserre, 2006, 2011 and Peet et al., 2009 for par-
ticular examples). To illustrate the idea, let us consider
the generic convex minimization problem

b∗ := min
λ∈R(cid:96)

b(λ)

s.t. P0(x) +

(cid:96)
(cid:88)

λiPi(x)

(cid:23) 0 ∀x ∈ Rn,

(4.35)

(cid:124)

i=1
(cid:123)(cid:122)
=:P (x,λ)

(cid:125)

where b : R(cid:96) → R is a convex cost function and
P0, . . . , P(cid:96) ∈ R[x]r×r
n,2d are symmetric polynomial matrices
whose sparsity graph is chordal and has maximal cliques

C1, . . . , Ct. Given any integer ν ≥ 0, a feasible vector λ
and an upper bound on the optimal cost b∗ may be found
by solving the SOS relaxation

b∗
ν := min
λ∈R(cid:96)

b(λ)

s.t. (cid:107)x(cid:107)2ν

2 P (x; λ) =

t
(cid:88)

k=1

ET
Ck

Sk(x)ECk ,

Sk(x) ∈ Σ|Ck|

n,2d+2ν for k = 1, . . . , t,

(4.36)

which can be reformulated as a standard-form SDP. If the
polynomial matrices P0, . . . , P(cid:96) are homogeneous of even
degree, and there exists λ0 ∈ R(cid:96) such that P (x, λ0) is pos-
itive deﬁnite, then one can use Theorem 4.5 to prove that
b∗
ν → b∗ from above as ν → ∞; see Zheng & Fantuzzi
(2020) for more details and numerical examples. Under
further technical assumptions (see Zheng & Fantuzzi, 2020
for details), asymptotic convergence when P0, . . . , P(cid:96) are
not homogeneous is preserved by replacing the SOS mul-
2)ν.
tiplier (cid:107)x(cid:107)2ν

2 with (1 + (cid:107)x(cid:107)2

4.3.2. Decomposition on a semialgebraic set
We now turn our attention to sparse polynomial matrix
inequalities on a semialgebraic set K deﬁned as in (4.26)
by m polynomial inequalities gi(x) ≥ 0, i = 1, . . . , m. A
suﬃcient condition for a symmetric polynomial matrix P ∈
R[x]r×r
n,d to be positive semideﬁnite on K is that there exist
an integer ν ∈ N and SOS matrices S0, . . . , Sm ∈ Σm
such that

n,2ν

P (x) = S0(x) +

m
(cid:88)

i=1

gi(x)Si(x).

(4.37)

A matrix version of Putinar’s Positivstellensatz proved by
Scherer & Hol (2006) states that this condition (4.37) is
also necessary when P is positive deﬁnite on K and this
set satisﬁes the Archimedean condition.

The weighted matrix SOS decomposition (4.37) can be
searched for with semideﬁnite programming, but this is
prohibitively expensive when P is large. If it has chordal
structural sparsity, however, one can show that the SOS
matrices Si admit a clique-based decomposition. This
yields the following sparse matrix version of Putinar’s Pos-
itivstellensatz.

Theorem 4.6 (Zheng & Fantuzzi, 2020). Let K be a
semialgebraic set deﬁned as in (4.26) that satisﬁes the
Archimedean condition. Suppose that the symmetric poly-
is positive deﬁnite on K
nomial matrix P ∈ R[x]r×r
n,d
and that its sparsity graph has maximal cliques C1, . . . , Ct.
There exist an integer ν ∈ N and SOS matrices Si,k ∈
Σ|Ck|

n,2ν for i = 0, . . . , m and k = 1, . . . , t such that

P (x) =

(cid:18)

ET
Ck

t
(cid:88)

k=1

S0,k(x) +

m
(cid:88)

i=1

(cid:19)

gi(x)Si,k(x)

ECk . (4.38)

28

This result can be used to construct sparsity-exploiting
SOS relaxations of optimization problems with polynomial
matrix inequalities on compact semialgebraic sets that sat-
isfy the Archimedean condition. For example, consider an
optimization problem analogous to (4.35), where the poly-
nomial matrix inequality is enforced on K rather than on
the full space Rn, and denote its optimal value by b∗. If
there exists λ0 ∈ R(cid:96) such that the inequality is strict on K
and this set satisﬁes the Archimedean condition, then the
optimal value of the SOS problem

min
λ∈R(cid:96)

b(λ)

s.t. P (x, λ) satisﬁes (4.38)

Si,k ∈ Σ|Ck|

n,2ν for i = 0, . . . , m and k = 1, . . . , t

converges to b∗ from above as ν → ∞. Interested readers
are referred to Zheng & Fantuzzi (2020) for more details
and computational examples.

4.4. Other approaches

The scalability of SOS approaches to polynomial inequal-
ities and polynomial optimization problems can be im-
proved using techniques beyond those described in this
section. One example is to replace semideﬁnite condi-
tions on a large Gram matrix with stronger conditions
based on factor-width-k decompositions, which are dis-
cussed in Section 5.2 below. For the particular case of
k = 2, one obtains scaled diagonally dominant SOS (SD-
SOS) certiﬁcates of nonnegativity (Ahmadi & Majumdar,
2019). Another approach is to use bounded-degree SOS
conditions (Lasserre et al., 2017), in which (loosely speak-
ing) one restrict the degree of the monomial basis xB used
in the Gram matrix representation and handles monomi-
als of higher degree using positivity certiﬁcates that can
be reformulated as linear programs. Term sparsity can be
exploited in these frameworks, too: the relation between
correlative sparsity and SDSOS conditions is discussed by
Zheng et al. (2019a), while Weisser et al. (2018) develop
sparsity-exploiting bounded-degree SOS hierarchies.

Finally, when working with polynomials that are in-
variant under groups of symmetry transformations, a
large Gram matrix can be replaced with one that has a
block-diagonal structure using symmetry reduction tech-
niques (Gatermann & Parrilo, 2004; L¨ofberg, 2009; Riener
et al., 2013). The block-diagonalization based on sign-
symmetries, recovered by the TSSOS and CS-TSSOS hi-
erarchies discussed in Sections 4.2.3 and 4.2.4, is only one
particular example; more sophisticated strategies require
using a “symmetry-adapted” basis for the space of poly-
nomials in lieu of the monomial basis xB.

4.5. Open-source software implementations

Many of the sparsity-exploiting techniques for polynomial
optimization described in this section are implemented in
open-source software. The Newton polytope reduction

technique is implemented in almost all parsers for SOS
optimization, including SOSTOOLS (Prajna et al., 2002),
YALMIP (L¨ofberg, 2004), GloptiPoly (Henrion et al., 2009)
and SumOfSquares.jl (Legat et al., 2017; Weisser et al.,
2019). Correlative sparsity techniques are implemented
in the MATLAB toolboxes SparsePOP (Waki et al., 2008)
and aeroimperial-yalmip (Fantuzzi, 2020). The recent
Julia package TSSOS (Magron & Wang, 2021) implements
the TSSOS, chordal-TSOS and CS-TSSOS hierarchies.
Term sparsity and symmetries in polynomial optimization
can also be exploited through SumOfSquares.jl (Legat
et al., 2017; Weisser et al., 2019).

5. Factor-width decomposition

We have seen that the matrix decomposition approach can
lead to signiﬁcant eﬃciency improvements in the solution
of sparse SDPs (cf. Section 3) and sparse polynomial op-
timization problems (cf. Section 4). We now turn our
attention to the problem of testing positive-semideﬁnitess
of matrices that are not necessarily sparse, for which simi-
lar matrix decomposition ideas can also be leveraged using
approximation methods. This class of methods is known as
factor-width decomposition (Boman et al., 2005). We will
highlight its connections and diﬀerences with the chordal
decomposition reviewed above.

After reviewing some background in Section 5.1, we dis-
cuss how a hierarchy of inner and outer approximations for
positive semideﬁnite matrices can be constructed based on
factor-width-k matrices in Section 5.2. We then discuss in
Section 5.3 how this can be extended further, leading to
the notion of block factor-width-two matrices (Zheng et al.,
2019b), which aims to strike a balance between numerical
computation and approximation quality. Applications in
semideﬁnite and SOS optimization are discussed in Sec-
tions 5.4 and 5.5.

5.1. Background
As emphasized in the previous sections, solving large-scale
semideﬁnite programs is at the centre of many problems
in control engineering and beyond, and the development
of fast and reliable solvers has attracted signiﬁcant atten-
tion recently, mainly focusing on sparsity exploiting and
low-rank solution exploiting methods (De Klerk, 2010; Ma-
jumdar et al., 2020). Some of these methods attempt to
solve the problem exactly using, e.g., chordal decomposi-
tion (cf. Sections 3 and 4) when sparsity is present, but
others are trying to provide approximate solutions when
these problems are large and dense. This section focuses
on the latter case, i.e., the case of dense and large SDPs,
and the general idea is still based on a certain matrix de-
composition, similar to Sections 3 and 4.

One basic approach is to approximate the positive
semideﬁnite cone Sn
+ with the cone of factor-width-k matri-
ces (Boman et al., 2005), which allows for a certain matrix
decomposition discussed in Section 5.2 below. We will de-
note the cone of factor-width-k matrices by FW n
k , where

29

n is the matrix dimension. The case k = 2 is of special in-
terest: this is also the case of symmetric scaled diagonally
dominant matrices, and enforcing FW n
2 is equivalent to
a number of second-order cone constraints, which implies
that linear functions can be optimized over FW n
2 by solv-
ing a second-order cone program (SOCP). Compared to
SDPs, SOCPs are much more scalable but this approxi-
mation is very conservative: the restricted problem may
even become infeasible. At the same time, attempting an
approximation over FW n
3 will result into an O(n3) number
of positive semideﬁnite constraints, which may not strike
a good balance between approximation and computational
eﬃciency. For this reason, most work has focused on the
case of factor-width-two matrices and on some closely re-
lated extensions (Ahmadi et al., 2017a; Ahmadi & Hall,
2017; Wang et al., 2021c).

This notion of factor-width-two matrices was recently
extended to the block case by Zheng et al. (2019b), who
showed that the approximation quality is signiﬁcantly im-
proved compared to FW n
2 and remains computationally
feasible unlike the approximation using FW n
3 . At the
same time, block factor-width-two matrices can form a
new hierarchy of approximations using a “coarsening” of
the decomposition results (cf. Deﬁnition 2.2). An alterna-
tive approach that results in an improved approximation is
based on the use of decomposed structured subsets (Miller
et al., 2019b).

5.2. Factor-width-k decompositions

We now introduce the concept of factor-width-k matrices,
originally deﬁned in Boman et al. (2005).

Deﬁnition 5.1. The factor width of a matrix X ∈ Sn
+
is the smallest integer k such that there exists a matrix
V where A = V V T and each column of V has at most k
nonzeros.

The factor width of X is also the smallest integer k for
which X is the sum of positive semideﬁnite matrices that
are non-zero at most on a k × k principal submatrix:

Z =

s
(cid:88)

i=1

ET
Ci

ZiECi

(5.1)

for some matrices Zi ∈ Sk
+, where Ci is a set of k distinct
integers from 1 to n and s = (cid:0)n
k to denote
the set of n × n matrices with factor-width at most k.
The dual of FW n
k with respect to the normal trace inner
product is

(cid:1). We use FW n

k

(FW n

k )∗ = (cid:8)X ∈ Sn | ECiXET

Ci

∈ Sk

+, ∀i = 1, . . . , s(cid:9) .

The following hierarchy of inner/outer approximations of
Sn
+ follows directly from these deﬁnitions:

The set FW n

2 is of particular interest because it is equiv-
alent to the set of symmetric scaled diagonally dominant
matrices (Boman et al., 2005). Furthermore, linear opti-
mization over FW n
2 can be converted into an SOCP, for
which eﬃcient algorithms exist. The better scalability of
SOCPs compared to SDPs makes inner approximations of
positive semideﬁnite cones based on FW n
2 very attractive,
and form the basis of the SDSOS framework for polynomial
optimization proposed by Ahmadi & Majumdar (2019).

Remark 5.1 (Factor-width decomposition vs chordal de-
composition). The decomposition (5.1) is formally the
same as the chordal decomposition in Theorem 2.1, and
the two diﬀer only in the choice of “cliques” C1, . . . , Cs.
For chordal decomposition, they are the maximal cliques
of (a chordal extension of) the sparsity graph of Z. For
factor-width-k decomposition, instead, they are all (cid:0)n
(cid:1) sets
of k distinct indices from {1, . . . , n}. These two diﬀerent
choices, however, have considerably diﬀerent implications:
while chordal decomposition is necessary and suﬃcient for
a sparse matrix to be positive semideﬁnite, factor-width-
k decomposition is only suﬃcient unless k = n. The
quality of the approximation of positive semideﬁnite cones
by (FW n
k )∗ was recently investigated by Song & Parrilo
(cid:4)
(2021) and Blekherman et al. (2020).

k

5.3. Block factor-width-two decomposition
The representation (5.1) reveals that checking whether a
matrix Z belongs to FW n
k for any values of n and k is
equivalent to an SDP. When k < n, this SDP has smaller
semideﬁnite cones than Sn
+, but may be more expensive
than checking whether Z ∈ Sn
+ directly because of the
(cid:1). Setting k = 2 does
combinatorial number of cones, (cid:0)n
lead to eﬃciency gains, but the gap between FW n
2 and Sn
+
might be unacceptably large in some applications. For this
reason, block factor-width-two matrices are of interest.

k

Recall from Section 2.3.1 the notion of a block-partition
of a matrix Z ∈ Sn subordinate to a partition α of n. Re-
call also the deﬁnition of the index matrix ECk,α in (2.14).
We here further deﬁne

Ei,α := (cid:2)0
. . .
Eij,α := (cid:2)(Ei,α)T

Iαi
. . .
(Ej,α)T(cid:3)T

0(cid:3) ∈ Rαi×n,
∈ R(αi+αj )×n.

(5.3a)

(5.3b)

The set of block factor-width-two matrices, denoted by
FW n

α,2, is deﬁned as follows (Zheng et al., 2019b).

Deﬁnition 5.2. For any partition α = {α1, . . . , αp} of n,
a symmetric matrix Z ∈ Sn belongs to the class FW n
α,2 of
block factor-width-two matrices if and only if

p−1
(cid:88)

p
(cid:88)

Z =

(Eij,α)TXijEij,α

(5.4)

i=1

j=i+1

for some Xij ∈ Sαi+αj

+

, where Eij,α is deﬁned in (5.3b).

FW n

1 ⊆ FW n
+ = (FW n
Sn

2 ⊆ . . . ⊆ FW n

n =

n)∗ ⊆ . . . ⊆ (FW n

2 )∗ ⊆ (FW n

1 )∗.

(5.2)

It is clear that (5.4) is a direct block extension of (5.1)
α,2 is a

when k = 2. Also, it is not hard to check that FW n

30

cone. Its dual (with respect to the trace inner product) is
characterized by the following proposition.

Proposition 5.1 (Zheng et al. (2019b)). For any partition
α = {α1, . . . , αp} of n, the dual of FW n

α,2 is

(FW n

α,2)∗ = {X ∈ Sn | Eij,αX(Eij,α)T (cid:23) 0,

1 ≤ i < j ≤ p}.

Furthermore, both FW n
i.e., they are convex, closed, solid, and pointed cones.

α,2 and (FW n

α,2)∗ are proper cones,

α,2 or to (FW n

It should be clear from Deﬁnition 5.2 and Proposi-
tion 5.1 that semideﬁnite programming can be used to ver-
ify whether a matrix belongs to FW n
α,2)∗.
While a gap between these cones and the positive semideﬁ-
nite cone Sn
+ remains, the next theorem states that the size
of the gap can be reduced by coarsening the partition α
(cf. Deﬁnition 2.2), generally at the expense of increasing
the computational complexity of the semideﬁnite represen-
tations of FW n
α,2)∗. This tradeoﬀ between
approximation gap and complexity is the main advantage
of using block factor-width-two cones.

α,2 and (FW n

Theorem 5.1 (Zheng et al. (2019b)). Let γ (cid:64) β (cid:64) α be
partitions of n with α = {α1, α2}, and let 1 = {1, . . . , 1}
denote the uniform unit partition. Then,

2 = FW n

FW n
≡ (FW n

1,2 ⊆ FW n
α,2)∗ ⊆ (FW n

γ,2 ⊆ FW n
β,2)∗ ⊆ (FW n

β,2 ⊆ FW n

α,2 ≡ Sn
+
1,2)∗.

γ,2)∗, ⊆ (FW n

This result does not quantify how well FW n

α,2 and
(FW n
α,2)∗ approximate the positive semideﬁnite cone.
Such information is clearly not only of theoretical inter-
est, but also of practical importance, especially for dense
positive semideﬁnite cone that cannot be studied using
chordal decomposition. Some progress in this direction
was recently made by Zheng et al. (2019b), who leveraged
results by Blekherman et al. (2020) to show that the nor-
malized distance between either FW n
α,2)∗ and
+ is at most p−2
Sn
p , where p is the number of blocks in the
partition α.

α,2 or (FW n

Compared to (5.2), one main advantage of the hierarchy
of inner/outer approximations using block factor-width-
two cones in Theorem 5.1 is that the number of basis ma-
trices in the representation (5.4) remains O(p2), instead
of a combinatorial number (cid:0)n
(cid:1). Moreover, the value of p
k
decreases when coarsening the partition. Therefore, the
cone FW n
α,2 is often computationally more tractable than
the cone FW n

k with k ≥ 3.

Example 5.1. Consider the 5 × 5 matrix








1 + 6x + 4y
3x + y
2x + y
x + 4y
3x + 3y

3x + y
1 + 6y
5x + 3y
y
2x + 2y

2x + y
5x + 3y
1 + 2x + 2y
x + 2y
5x + 6y

x + 4y
y
x + 2y
1 + 2x
3x + 3y

3x + 3y
2x + 2y
5x + 6y
3x + 3y
1 + 6x + 2y








31

FW 5

FW 5

1,2, (FW 5
β,2, (FW 5

1,2)∗
β,2)∗

FW 5

FW 5

α,2, (FW 5
γ,2, (FW 5

α,2)∗
γ,2)∗

Figure 5.1: Regions of the (x, y) plane for which the 5 × 5
matrix in Example 5.1 belongs to the block factor-width-two
cones FW 5
1,2 ⊂ FW 5
+ (top panel),
and the dual cones S5
α,2)∗
(bottom panel). The partitions are 1 = {1, 1, 1, 1, 1}, α =
{2, 1, 1, 1}, β = {2, 1, 2} and γ = {2, 3}. The inclusions of the
plotted regions reﬂect the inclusions of the cones and the order
relation 1 (cid:64) α (cid:64) β (cid:64) γ.

β,2 ⊂ FW 5
γ,2)∗ ⊂ (FW 5

α,2 ⊂ FW 5
+ ≡ (FW 5

β,2)∗ ⊂ (FW 5

γ,2 ≡ S5

1,2 ⊂ FW 5

β,2 ⊂ FW 5

α,2 ⊂ FW 5

and the progressively coarser partitions 1 = {1, 1, 1, 1, 1},
α = {2, 1, 1, 1}, β = {2, 1, 2} and γ = {2, 3}. The regions
of the (x, y) plane for which the matrix is in the cones
FW 5
+ are shown in
the top panel in Figure 5.1. The bottom panel of the same
ﬁgure, instead, shows the regions of the plane for which the
matrix is in the dual cones S5
β,2)∗ ⊂
(FW 5
α,2)∗. It is evident from these ﬁgures that all of the
inclusions are strict. However, the block factor-width-two
cones approximate well the positive semideﬁnite one along
(cid:4)
some directions.

γ,2)∗ ⊂ (FW 5

+ ≡ (FW 5

γ,2 ≡ S5

5.4. Applications to semideﬁnite programming

from Theorem 5.1 that the cones FW n

α,2 and
Recall
(FW n
α,2)∗ approximate the positive semideﬁnite cone Sn
+
from the inside and from the outside, respectively, and that
the approximation improves as the partition α is coars-
ened. This allows one to compute convergent sequences of
upper and lower bounds on the optimal value of an SDP
in the primal standard form (3.1), which we denote by J ∗
for simplicity, using optimization problems of increasing
computational complexity that are always simpler to solve
than (3.1) itself. Precisely, since FW n
+ for any par-
tition α of n, the optimal value of the block factor-width

α,2 ⊆ Sn

cone program

Uα := min
X

(cid:104)C, X(cid:105)

subject to (cid:104)Ai, X(cid:105) = bi, i = 1, . . . , m,

(5.5)

X ∈ FW n

α,2

bounds the optimal value of the SDP (3.1) from above. A
complementary lower bound is given by

Lα := min
X

(cid:104)C, X(cid:105)

subject to (cid:104)Ai, X(cid:105) = bi, i = 1, . . . , m,

(5.6)

X ∈ (FW n

α,2)∗

+ ⊆ (FW n

because Sn
α,2)∗. By Theorem 5.1, replacing α
with a coarser partition can only improve these upper and
lower bounds, and we have the following corollary.

Corollary 5.1. Let J ∗ denote the optimal value of the
SDP (3.1) and let α1 (cid:118) α2 (cid:118) . . . (cid:118) αk = {αk1, αk2} be
a sequence of partitions of n. Then, Lα1 ≤ · · · ≤ Lαk =
J ∗ = Uαk ≤ · · · ≤ Uα1 .

When α = 1 = {1, . . . , 1} is the ﬁnest possible partition,
problems (5.5) and (5.6) can be reformulated as SOCPs.
This case was studied extensively by Ahmadi & Majumdar
(2019), and numerical experiments show that the optimal
values L1 and U1 can often be very poor bounds for J ∗.
To obtain better results using coarser partitions, one can
leverage the deﬁnition of FW n
α,2 and rewrite the upper
bound problem (5.5) as

p−1
(cid:88)

p
(cid:88)

j=1

l=j+1

p−1
(cid:88)

p
(cid:88)

min
Xjl

s.t.

(cid:104)Cjl,α, Xjl(cid:105)

(cid:104)Aijl,α, Xjl(cid:105) = bi, i = 1, . . . , m,

(5.7)

j=1
l=j+1
Xjl ∈ Sαj +αl

+

, 1 ≤ j < l ≤ p,

where Cjl,α := Ejl,αC(Ejl,α)T, Aijl,α := Ejl,αAi(Ejl,α)T.
This is a standard-form SDP and can be solved with
general-purpose solvers. Observe that the number of
equality constraints in this SDP is the same as for the orig-
inal problem (3.1), but the dimension of semideﬁnite cones
has been reduced. Since general-purpose SDP solvers can
handle multiple small semideﬁnite cones much more eﬃ-
ciently than a single large one, problem (5.7) can often be
solved much faster than (3.1). For instance, the numeri-
cal experiments in Zheng et al. (2019b) show that useful
upper bounds Uα on the optimal value of SDP relaxations
of polynomial optimization problems can be found with a
reduction of up to 80% in CPU time.

p ∈ R[x]n,2d of even degree 2d is SOS if and only if there
exists an exponent set B ⊆ Nn
d and a positive semideﬁnite
matrix Q such that (Parrilo, 2000)

p(x) = (xB)T Q xB.

(5.8)

The fundamental computational challenge in optimization
over the cone Σn,2d of n-variate SOS polynomials of degree
at most 2d is that the parameterization (5.8) requires in
general an N × N positive semideﬁnite matrix with N =
(cid:1). This may be prohibitive even for moderate values
(cid:0)n+d
d

of n and d.

For polynomials characterized by term sparsity, the
computational complexity can be reduced dramatically us-
ing the approaches reviewed in Section 4, which are based
on chordal decomposition. To handle polynomials that are
not term sparse, Ahmadi & Majumdar (2019) introduced
the notion of scaled diagonally dominant sum-of-squares
(SDSOS). These are special SOS poynomials whose Gram
matrix Q in (5.8) belongs to the factor-width-two cone
FW |B|
2 . As in the case of semideﬁnite programming, deﬁn-
ing block-SDSOS polynomials by replacing FW |B|
2 with its
superset FW |B|
α,2 for any partition α of |B| oﬀers an im-
proved inner approximation of Σn,2d.

Deﬁnition 5.3. Given a partition α = {α1, . . . , αg} of
|B|, a polynomial p ∈ R[x]n,2d is said to be α-SDSOS if
and only if there exists coeﬃcient vectors fij,t ∈ Rαi+αj
and exponent sets Bij ⊆ Nn

d such that

p(x) =

(cid:88)

(cid:32)αi+αj
(cid:88)

(cid:0)f T

ij,txBij (cid:1)2

(cid:33)

.

(5.9)

1≤i<j≤g

t=1

The set of all α-SDSOS polynomials in n independent
variables and degree no larger than 2d will be denoted by
α-SDSOSn,2d. It is not diﬃcult to check that it is a cone.
Moreover, since deﬁnition (5.9) is considerably more struc-
tured that the deﬁnition (4.3) of general SOS polynomials,
the inclusion α-SDSOSn,2d ⊆ Σn,2d is immediate.

(cid:1),
For the uniform unit partition α = {1, . . . , 1} of (cid:0)n+d
the cone α-SDSOSn,2d reduces to the normal SDSOS
cone studied by Ahmadi & Majumdar (2019). At the
other hand of the spectrum, for any partition in the form
α = {α1, α2} one has α-SDSOSn,2d = Σn,2d. This second
statement is a direct consequence of the following result,
which reveals a connection between the polynomial cone
α-SDSOSn,2d and the block factor-width-two cone FW |B|
α,2.
Theorem 5.2 (Zheng et al., 2019b). A polynomial p ∈
R[x]n,2d belongs to the cone α-SDSOSn,2d if and only if it
admits a Gram matrix representation (5.8) with B ⊆ Nn
d
and Q ∈ FW |B|
α,2.

d

5.5. Applications to SOS optimization
Block factor-width-two decompositions can also be ap-
plied to reduce the computational cost of SOS optimiza-
tion. As discussed in Section 4, an n-variate polynomial

Similar to Theorem 5.1, we can build a hierarchy of inner

approximations for the SOS cone Σn,2d.

Corollary 5.2. Let 1 = {1, . . . , 1}, α = {α1, . . . , αg},
β = {β1, . . . , βh} and γ = {γ1, γ2} be partitions of |B|

32

such that α (cid:118) β. Then,

SDSOSn,2d = 1-SDSOSn,2d ⊆ α-SDSOSn,2d

⊆ β-SDSOSn,2d ⊆ γ-SDSOSn,2d = Σn,2d.

(5.10)

Consider now an optimization problem of the form

w∗ := min

u

wTu

s.t. p(x) := p0(x) +

t
(cid:88)

i=1

uipi(x) ≥ 0, ∀x ∈ Rn,

d

(5.11)
where p0, . . . , pt ∈ R[x]n,2d are given polynomials, w ∈ Rt
is a given cost vector, and u ∈ Rt is the decision vari-
able. Let α be any partition of (cid:0)n+d
(cid:1). To compute an
upper bound on the optimal cost w∗, one can strengthen
the nonnegavity constraint on p with the SOS constraints
p ∈ Σn,2d, the SDSOS constraint p ∈ SDSOSn,2d, or the
block-SDSOS constraint p ∈ α-SDSOSn,2d. The ﬁrst ap-
proach replaces (5.11) with an SDP, the second one leads
to an SOCP, and the third yields a block-factor-width
cone program that can be reformulated as a standard-
form SDP. According to Corollary 5.2, the SOS constraint
provides the best upper bound on w∗, but is the most
computationally expensive. At the other extreme is the
SDSOS constraint, which oﬀers the fastest computations
but may be too restrictive—in fact, the corresponding
SOCP may even be infeasible. The block-SDSOS con-
straint p ∈ α-SDSOSn,2d, instead, can balance the com-
putational speed and upper bound quality thanks to the
freedom one has in choosing the partition α. This expec-
tation is conﬁrmed by the numerical experiments of Zheng
et al. (2019b), but the problem of choosing an optimal par-
tition for given computational resources remains an open
problem.

6. Applications

The matrix decomposition techniques reviewed in the pre-
vious sections can be used to reduce the computational
complexity of a wide variety of analysis and control prob-
lems that can be formulated as SDPs or SOS programs. As
anticipated in Section 3.1, complex large-scale dynamical
systems at the heart of modern technology often possess
a natural graph-like structure, due for example to sparse
interactions between subsystems in a network (Andersen
et al., 2014a; Dall’Anese et al., 2013; Riverso et al., 2014;
Zheng et al., 2018d, 2020). The key to enabling eﬃcient
numerical treatment of control problems for such systems
is to devise SDP or SOS relaxations that preserve this
graph structure as much as possible. Precisely, one aims
to obtain SDPs with aggregate sparsity (cf. Section 3) or
polynomial optimization problems with term sparsity (cf.
Section 4). If this can be done, then the sparsity exploiting
techniques discussed in Sections 3 and 4 can bring consid-
erable computational gains and enable the study of very
large systems.

This section describes how chordal sparsity can be ex-
ploited for a small selection of problems in control and
machine learning. Section 6.1 focuses on stability analysis
for linear and nonlinear systems, and on decentralized con-
trol of networked linear systems. In Section 6.2, we review
sparsity-promoting relaxations of nonconvex quadratically
constrained quadratic programs (QCQPs) and apply them
to the well-known Max-Cut problem from graph theory,
as well as to a network sensor location problem. Finally,
Section 6.3 shows how chordal sparsity allows for eﬃcient
veriﬁcation of neural networks in machine learning. We
stress that these are only a few of the application do-
mains in which chordal decomposition has enabled con-
siderable progress in recent years; other ﬁelds include, for
instance, ﬂuid mechanics, model predictive control, and
optimal power ﬂow. Table 2 provides a (non-exhaustive)
list of references.

6.1. Stability analysis and decentralized control

Stability analysis and control synthesis problems for dy-
namical systems governed by ordinary diﬀerential equa-
tions can often be reformulated as SDPs or SOS programs
using Lyapunov functions (Boyd et al., 1994; Lasserre,
2010; Papachristodoulou & Prajna, 2005; Parrilo, 2000;
Zhou et al., 1996).
If the interactions between individ-
ual components of the system have a sparse graph struc-
ture, considering Lyapunov functions with a separable or
nearly-separable structure can lead to sparse SDPs and
SOS programs, which can be solved eﬃciently using the
techniques in Sections 3 and 4. Here, we give three simple
examples of this fact.

6.1.1. Stability of linear networked systems
Consider a continuous-time linear autonomous system

˙x(t) = Ax(t),

(6.1)

where x(t) ∈ Rn is the system state at time t and A ∈
Rn×n is the system matrix. It is well known (Boyd et al.,
1994; Zhou et al., 1996) that the equilibrium state x(t) = 0
is asymptotically stable if and only if all eigenvalues of A
have negative real part. Classical Lyapunov stability the-
ory guarantees that this is true if and only if there exists a
positive deﬁnite matrix P such that the (positive deﬁnite)
Lyapunov function V (x) = xTP x decays monotonically
along all system trajectories x(t). Equivalently, P must
satisfy the strict LMIs

P (cid:31) 0,

ATP + P A ≺ 0.

(6.2)

Now, suppose that (6.1) is a compact representation of
a network of l linear subsystems with states x1 ∈ Rn1, . . .,
xl ∈ Rnl , whose interactions can be represented by a static
undirected graph Gd({1, . . . , l}, Ed) with (i, j) ∈ Ed if and
only if systems i and j are directly coupled. In particular,

33

Table 2: Applications of exploiting chordal sparsity in control, machine learning, relaxation of QCQP (Quadratically-constrained
quadratic program), ﬂuid dynamics, and beyond.

Area
Control

Topic
Linear system analysis

Decentralized control

Nonlinear system analysis

Model predictive control

Machine learning Veriﬁcation of neural networks

References
Andersen et al. (2014b); Deroo et al. (2015); Mason & Pa-
pachristodoulou (2014); Pakazad et al. (2017b); Zheng et al. (2018c)
Deroo et al. (2014); Heinke et al. (2020); Zheng et al. (2020);
Zheng et al. (2018d)
Schlosser & Korda (2020); Tacchi et al. (2019a); Zheng et al.
(2019a); Mason (2015, Chapter 5)
Ahmadi et al. (2019); Hansson & Pakazad (2018)

Batten et al. (2021); Dvijotham et al. (2020); Newton & Pa-
pachristodoulou (2021); Zhang (2020)
Chen et al. (2020b); Latorre et al. (2020)

Lipschitz constant estimation
Training of support vector machine Andersen & Vandenberghe (2010)
Geometric perception & coarsening Chen et al. (2020a); Liu et al. (2019); Yang & Carlone (2020)
Covariance selection
Subspace clustering

Dahl et al. (2008); Zhang et al. (2018)
Miller et al. (2019a)

Relaxation of
QCQP and POPs

Sensor network locations
Max-Cut problem
Optimal power ﬂow (OPF)

Jing et al. (2019); Kim et al. (2009); Nie (2009)
Andersen et al. (2010a); Garstka et al. (2019); Zheng et al. (2020)
Andersen et al. (2014a); Dall’Anese et al. (2013); Jabr (2011);
Jiang (2017); Molzahn & Hiskens (2014); Molzahn et al. (2013)

Others

State estimation in power systems Weng et al. (2013); Zhang et al. (2017); Zhu & Giannakis (2014)

Fluid dynamics
Partial diﬀerential equations
Robust quadratic optimization
Binary signal recovery
Solving polynomial systems

Other problems

Arslan et al. (2021); Fantuzzi et al. (2018)
Mevissen (2010); Mevissen et al. (2008, 2011, 2009)
Andersen et al. (2010b)
Fosson & Abuabiah (2019)
Cifuentes & Parrilo (2016, 2017); Li et al. (2021); Mou et al.
(2021); Tacchi et al. (2019b)
Baltean-Lugojan et al. (2019); Jeyakumar et al. (2016); Madani
et al. (2017b); Pakazad et al. (2017a); Yang & Deng (2020)

the dynamics of each subsystem are given explicitly by

˙xi = Aiixi +

(cid:88)

j∈Ni

Aijxj,

i = 1, . . . , l,

(6.3)

where Ni := {j : (j, i) ∈ Ed} denotes the neighbors of
system i. Systems of this type are encountered, for exam-
ple, when modelling power grids (Riverso et al., 2014) and
traﬃc systems (Wang et al., 2020b; Zheng et al., 2020).

If the matrix P in (6.2) is assumed to be block-diagonal
with l blocks of size n1, . . . , nl, meaning that we consider a
quadratic Lyapunov function in the separable form (Boyd
& Yang, 1989; Geromel et al., 1994; Zheng et al., 2020;
Zheng et al., 2018d)

V (x) =

l
(cid:88)

i=1

xT
i Pixi,

(6.4)

then it is not hard to see that the block-sparsity graph of
the matrix ATP + P A in (6.2) is the same as the system
graph Gd. When this graph is chordal with small maximal
cliques, or admits a chordal extension with the same prop-
erty, a feasible block-diagonal matrix P satisfying (6.2)
can be constructed for signiﬁcantly larger networks than
that can be handled without sparsity exploitation. Equiv-

alently, for a given network size, CPU time requirements
can be reduced dramatically.

As an example, consider a network with a master node
and l−1 independent subsystems connected to it, sketched
in Figure 6.1(a). For simplicity, suppose that the subsys-
tems have size n1 = · · · = nl = 10. With a block-diagonal
P , the second LMI in (6.2) has the chordal “arrow-type”
block sparsity shown in Figure 6.1(b). Table 3 reports the
CPU time required to construct a feasible P with MOSEK as
a function of the number l of subsystems when the spar-
sity of this LMI is and is not exploited.3 It is evident that
exploiting chordal sparsity using the methods described
in Section 3 leads to a signiﬁcant reduction in CPU time.
Similar results are obtained for systems with more realistic
network graphs if its maximal cliques are small; see Ma-
son & Papachristodoulou (2014), Deroo et al. (2015) and
Zheng et al. (2018c,d).

Remark 6.1 (Separable Lyapunov functions). Searching
for a Lyapunov function V (x) with the separable struc-
ture (6.4) is convenient to ensure that the sparsity of the

3Computations were performed using the MATLAB toolboxes
YALMIP and SparseCoLO on a laptop with 16GB RAM and an In-
tel i7 processor. The nonzero system matrices Aij were generated
randomly whilst ensuring the existence of a feasible block-diagonal P .

34

2

3







1

l

4

•••

(a)

(cid:4) (cid:4) (cid:4) · · · (cid:4)
(cid:4) (cid:4)
(cid:4) (cid:4)
...
(cid:4)

. . .

(cid:4)







(b)

Figure 6.1: (a) Graph Gd for the network of 10-dimensional
linear systems used to generate the results reported in Table 3.
(b) Block sparsity pattern of the matrix P A + ATP when P
is block-diagonal; each block has size 10 × 10, and there are l
diagonal blocks.

Table 3: CPU time (in seconds) required by the SDP solver
MOSEK to construct a block-diagonal Lyapunov matrix P satisfy-
ing the LMIs in (6.2) for a network of l 10-dimensional systems
with connectivity graph Gd shown in Figure 6.1(a).

l
10
50
100
125
150
175
200

No sparsity exploitation
0.55
14.92
86.09
113.06
185.42
334.13
498.49

Sparsity exploitation
0.26
0.90
1.21
1.17
1.96
2.69
3.55

system matrix A is inherited by the LMI ATP + P A ≺ 0.
The existence of such a separable Lyapunov function can
be guaranteed for special classes of stable linear systems
(Carlson et al., 1992; Sootla et al., 2017, 2019), but not in
general. When a separable Lyapunov function V (x) fails
to exist, the structure of the network graph Gd may be still
be leveraged to promote sparsity in (6.2); for instance, the
case of banded graphs, cycles and trees was studied by
Mason & Papachristodoulou (2014). Determining a suit-
able structure for V (x) (equivalently, for the matrix P )
a priori for general graph structures, however, remains a
(cid:4)
challenging problem.

6.1.2. Stability of sparse polynomial systems
Structured Lyapunov functions can bring computational
advantages also when studying the asymptotic stability of
sparse nonlinear systems with polynomial dynamics. As
an example, consider a nonlinear system with the structure
(Zheng et al., 2019a, Section VI.D)

4

3

5

2

6

(a)

1

7

8

4

1

7

8

3

5

2

6

(b)

Figure 6.2: Correlative sparsity graphs for the polynomial
inequality in (6.6c) when the Lyapunov function V has (a) the
separable form (6.7), and (b) the partially separable form (6.8).

equilibrium is locally asymptotically stable if there exist a
region D ⊂ Rn1 × · · · Rnl containing the origin, a constant
(cid:15) > 0, and a Lyapunov function V : Rn1 × · · · × Rnl → R
such that

V (0) = 0,
V (x) ≥ (cid:107)x(cid:107)2
2
−f (x) · ∇V (x) ≥ (cid:15)(cid:107)x(cid:107)2
2

∀x ∈ D,

∀x ∈ D.

(6.6a)

(6.6b)

(6.6c)

i − (cid:107)xi(cid:107)2 ≥ 0 ∀i = 1, . . . , l}, which
Upon ﬁxing D = {x : r2
has a fully separable structure, and requiring V to be a
polynomial, the last two inequalities become polynomial
inequalities on a basic semialgebraic set. One can there-
fore search for V using SOS optimization. Moreover, the
structure of V can be chosen to ensure that these polyno-
mial inequalities are correlatively sparse (cf. Sections 4.2.2
and 4.2.5), enabling eﬃcient implementation.

For example, if one takes

V (x) =

l
(cid:88)

i=1

Vi(xi)

(6.7)

to have a fully separable structure as in the case of linear
systems considered previously, then the correlative spar-
sity graph of inequalities (6.6b) is a graph with no edges,
while that of (6.6c) is the same chain graph character-
izing the cascaded interactions between the state vectors
x1, . . . , xl, shown in Figure 6.2(a) for l = 8. If this choice
for V is insuﬃcient, one can try the structured choice

V (x) =

l−1
(cid:88)

i=2

Vi(xi−1, xi, xi+1).

(6.8)

˙x1 = f1(x1, x2),
˙xi = fi(xi−1, xi, xi+1),
˙xl = fl(xl−1, xl),

i = 2, . . . , l − 1,

(6.5)

where each vector ﬁeld fi depends polynomially on its ar-
guments and xi ∈ Rni. Let x = (x1, . . . , xl) be the collec-
tion of all system states and write f = (f1, . . . , fl). Sup-
pose the system has an equilibrium at the origin. This

In this case, the correlative sparsity graph of (6.6b) is the
chain graph mentioned above, while that of (6.6c) is a
chordal graph with maximal cliques {i, i+1, i+2, i+3} for
i = 1, . . . , l − 3, which is shown in Figure 6.2(b) for l = 8.
One can of course build an entire hierarchy of structured
Lyapunov functions with increasing degree of couplings
between subsystem variables, at the expense of increas-
ing the number of edges in the correlative sparsity graph

35

Table 4: CPU time, in seconds, required by MOSEK to con-
struct a structured quadratic Lyapunov function (6.8) for a lo-
cally asymptotically stable, degree-3 polynomial system of the
form (6.5). Entries marked oom indicate memory errors.

l

Standard SOS
Sparse SOS

10

1.4
0.6

15

21.3
0.7

40

30

20

50
262.1 oom oom oom
1.4

0.8

1.0

1.2

of the polynomial inequalities (6.6b) and (6.6c). Numeri-
cal experiments by Zheng et al. (2019a) for the structured
Lyapunov function in (6.8), which we report in Table 4,
show that this approach can signiﬁcantly reduce the com-
putation time and resources required to prove stability of
nonlinear systems compared to standard SOS techniques.
Similar ideas can be used to partition nonlinear systems
into subsystems (Anderson & Papachristodoulou, 2011)
and can be adapted to problems beyond stability analysis,
such as the estimation of region of attractions, positively
invariant sets, and global attractors (Schlosser & Korda,
2020; Tacchi et al., 2019a).

6.1.3. Decentralized control of linear networked systems
Consider a network of linear system with control inputs
and disturbances,

˙xi = Aiixi +

(cid:88)

j∈Ni

Aijxj + Biui + Midi,

i = 1, . . . , l,

where xi ∈ Rni, ui ∈ Rmi and di ∈ Rqi denote the lo-
cal state, input, and disturbance of subsystem i, respec-
tively, and Ni is the index set of all systems connected to
system i. Setting x = (x1, . . . , xl), u = (u1, . . . , ul) and
d = (d1, . . . , dl), the system can be written compactly as

˙x(t) = Ax(t) + Bu(t) + M d(t),

where A has block sparsity induced by the system graph
(cf. Section 6.1.1), while B = diag(B1, . . . , Bl) and M =
diag(M1, . . . , Ml) are block-diagonal.

The optimal decentralized control problem (Geromel

et al., 1994) seeks to design static state feedback laws,

ui(t) = −Kiixi(t),

∀i = 1, . . . , l,

(6.9)

that minimize the H2 norm of the transfer function from
disturbance d to the output

z =

(cid:21)

(cid:20)Q 1
2
0

x +

(cid:21)

(cid:20) 0
R 1

2

u,

where Q := diag(Q1, . . . , Ql) and R := diag(R1, . . . , Rl)
are given block-diagonal matrices. The decentralized con-
straint (6.9) makes the control problem challenging to
solve (Furieri et al., 2019; Geromel et al., 1994). One
simple strategy is to enforce that the closed-loop system
admits a separable Lyapunov function in the form (6.4).

36

This allows translating the decentralized constraint on the
controller to other auxiliary design variables (Furieri et al.,
2019, 2020). In particular, a suboptimal decentralized con-
troller can be computed using the formula Kii = ZiX −1
for each i = 1, . . . , l (Geromel et al., 1994; Zheng et al.,
2020, Section II.B), where the matrices Z1, . . . , Zl and
X1, . . . , Xl solve the SDP

i

min
Xi,Yi,Zi

l
(cid:88)

(cid:104)Qi, Xi(cid:105) + (cid:104)Ri, Yi(cid:105)

i=1

s.t. (AX −BZ) + (AX −BZ)T +M M T (cid:22) 0,

(6.10a)

(cid:21)

(cid:20) Yi Zi
Z T
i Xi

(cid:23) 0, Xi (cid:31) 0 ∀i = 1, . . . , l

(6.10b)

and X = diag(X1, . . . , Xl) and Z = diag(Z1, . . . , Zl) are
block-diagonal concatenations of the matrix variables.

The cost function of this SDP and the constraints
in (6.10b) are fully separable, as they depend only on vari-
ables corresponding to a single subsystem. The coupling
constraint (6.10a), instead, has a block sparsity pattern in-
duced by the system graph by virtue of the block-diagonal
structure of B, M , X and Z. As in Section 6.1.1, there-
fore, the chordal decomposition techniques of Section 3
allow for a fast numerical solution when the underlying
system graph is sparse, which enables control synthesis for
large-scale but sparse networks. In addition, customized
distributed design methods that combine chordal decom-
position with ADMM can solve (6.10) in a privacy-safe
way, without requiring subsystems to share information
about their local dynamics (Zheng et al., 2020).

6.2. Relaxation of nonconvex QCQPs
A (nonconvex) quadratically constrained quadratic pro-
gram (QCQP) is an optimization problem in the form

min
x

xTP0x + 2qT

0 x + r0

subject to xTPix + 2qT

i x + ri ≤ 0, i = 1, . . . , m,

(6.11)

where x ∈ Rn is the optimization variable, and Pi ∈
Sn, qi ∈ R, ri ∈ R, i = 0, 1, . . . , m are given problem data.
QCQPs have very powerful modeling capabilities; for in-
stance, many hard combinatorial and discrete optimization
problems can written in the form (6.11) (Nesterov et al.,
2000). This also means that QCQPs are hard to solve in
general, so many diﬀerent relaxation strategies have been
proposed to ﬁnd approximate bounds and feasible values
for the optimization variable x (Nesterov et al., 2000; Park
& Boyd, 2017). One approach that provides good bounds,
both empirically and theoretically (Nesterov et al., 2000),
is to introduce the positive semideﬁnite matrix X = xxT
and rewrite (6.11) as

min
x,X

(cid:104)P0, X(cid:105) + 2qT

0 x + r0

subject to (cid:104)Pi, X(cid:105) + 2qT

i x + ri ≤ 0, i = 1, . . . , m,

X = xxT.

Upon relaxing the intractable constraint X = xxT into
the inequality X (cid:23) xxT and applying Schur’s complement
to rewrite the latter as an LMI, we arrive at the semideﬁ-
nite relaxation

min
x,X

(cid:104)P0, X(cid:105) + 2qT

0 x + r0

subject to (cid:104)Pi, X(cid:105) + 2qT

i x + ri ≤ 0, i = 1, . . . , m,

(cid:21)

(cid:20)1 xT
x X

(cid:23) 0,

which is equivalent to the following primal-form SDP with
nonnegative variables

min
Z∈Sn+1,w

s.t.

(cid:69)

(cid:17)

(cid:17)

, Z

(cid:68)(cid:16) r0 qT
0
q0 P0
(cid:68)(cid:16) ri qT
i
qi Pi
Z11 = 1,
Z (cid:23) 0, w ≥ 0.

, Z

(cid:69)

+ wi = 0, i = 1, . . . , m,

(6.12)

(cid:17)

(cid:16) 1

It is not diﬃcult to see that the optimal value of prob-
lem (6.12) bounds that of the QCQP (6.11) from below and
that, if an optimal solution Z(cid:63) has rank one, then the relax-
xT
where x(cid:63) solves (6.11).
ation is exact and Z(cid:63) =
(cid:63)
x(cid:63) x(cid:63)xT
(cid:63)
If the data matrices P0, . . . , Pm are sparse, then the ag-
gregate sparsity pattern E of the SDP (6.12) is also sparse,
and the positive semideﬁnite constraint on Z can be re-
placed with the conic constraint Z ∈ Sn+1
+ (E, ?). The
chordal decomposition techniques described in Section 3
can therefore be applied to solve (6.12) eﬃciently. The
following subsections brieﬂy discuss two types of problem
for which sparsity can be exploited eﬀectively: Max-Cut
problems (Goemans & Williamson, 1995) and sensor net-
work location problems (Jing et al., 2019; Kim et al., 2009;
Nie, 2009; So & Ye, 2007).

6.2.1. Max-Cut problem
The maximum cut (Max-Cut) problem is a classic problem
in graph theory (Goemans & Williamson, 1995). Consider
an undirected graph G(V, E) with n vertices such that each
edge (i, j) ∈ E is assigned a nonzero weight Wij, and set
Wij = 0 if (i, j) /∈ E. The Max-Cut problem aims to
partition the graph’s vertices into two complementary sets
V1 and V2 such that the total weight of all edges linking
V1 and V2 is maximized. Given a binary variable x ∈
{−1, +1}n assigning nodes to one of the two partitions,
one seeks to maximize

1
2

(cid:88)

Wij =

i,j:xixj =−1

1
4

(cid:88)

i,j

Wij(1 − xixj).

This is equivalent to solving

xTW x

min
x
subject to x2

i = 1,

i = 1, . . . n,

(6.13)

37

where W is the given matrix of weights.

This problem is a particular QCQP, and can easily be
rewritten in the generic form (6.11) using data matrices
P0, P1, . . . , Pn whose aggregate sparsity graph coincides
with the original graph G. If G is sparse with small max-
imal cliques,therefore, SDP relaxations of (6.13) can be
solved eﬃciently using the sparsity-exploiting techniques
in Section 3. Indeed, numerical experiments by Andersen
et al. (2010a) and Zheng et al. (2020) demonstrated that
the sparsity-exploiting solvers SMCP and CDCS can solve
benchmark Max-Cut problems from the SDPLIB problem
library (Borchers, 1999) order of magnitude faster than
standard conic solvers.

6.2.2. Sensor network location
The sensor network location problem, also known as Graph
Realization (So & Ye, 2007), has important applications
such as inventory management and environment monitor-
ing. At a basic level, the problem is to ﬁnd unknown sensor
points x1, . . . , xn ∈ Rd (d = 2 or 3) satisfying some spec-
iﬁed distance constraints, as well as distance constraints
with respect to m known anchor points a1, . . . , am ∈ Rd.
Precisely, given pairing sets

Ex ⊆ {1, . . . , n} × {1, . . . , n},
Ea ⊆ {1, . . . , m} × {1, . . . , n},

we seek to ﬁnd sensor locations x1, . . . , xn ∈ Rd such that

(cid:107)xi − xj(cid:107)2 = d2
ij,
(cid:107)ai − xj(cid:107)2 = f 2
ij,

(i, j) ∈ Ex,

(i, j) ∈ Ea,

(6.14)

where the numbers dij and fij are speciﬁed distances.

One way to relax the sensor location problem into an
SDP is to consider (6.14) as a set of quadratic constraints
for x1, . . . , xn, and apply the generic SDP relaxation strat-
egy to the QCQP (Kim et al., 2009)

min
x1,...,xn∈Rd
subject to

0

(6.14).

(6.15)

It is clear that the data matrices and vectors of this QCQP
are very sparse, and that the aggregate sparsity pattern of
the corresponding SDP relaxation is determined only by
the edge sets Ea and Ex. Then, the techniques in Section 3
can be applied to solve the relaxed problem quickly; we
refer the interested reader to Kim et al. (2009) for more
detailed discussions and experiment results. Similar ideas
can be used to analyze sensor location problems where the
distance measurements dij and fij are aﬀected by noise
(Kim et al., 2009).

Remark 6.2. There are other ways to formulate an SDP
relaxation for (6.15). One (So & Ye, 2007) is to introduce
(cid:3) ∈
a matrix variable Y = XX T with X = (cid:2)x1, x2, . . . , xn
Rd×n, rewrite all the constraints in (6.15) as linear equal-
ities in X and Y , relax the nonconvex relation between

these variables into the inequality Y (cid:23) XX T and ap-
ply Schur’s complement to obtain an SDP. A sparsity-
exploiting version of this approach is described by Kim
et al. (2009, Section 3.3). Another option (Nie, 2009) is to
formulate the search for the sensor locations as an uncon-
strained polynomial optimization problem,

min
x1,...,xn

(cid:88)

(i,j)∈Ea

((cid:107)ai −xj(cid:107)2 −f 2

ij)2 +

(cid:88)

(i,j)∈Ex

((cid:107)xi −xj(cid:107)2 −d2

ij)2.

The polynomial objective is term-sparse when the coupling
set Ex contains only a small subset of all pairs (i, j) (in
fact, correlatively sparse; see Section 4.2 for deﬁnitions
of these concepts). Therefore, the sparse SOS techniques
outlined in Section 4.2 can be applied to solve the problem
eﬃciently. The interested reader is referred to Nie (2009)
(cid:4)
for experiment results.

6.3. Machine learning: Veriﬁcation of neural networks

Neural networks are one of the fundamental building
blocks of modern machine-learning methods. For safety-
critical applications, it is essential to ensure that they are
provably robust to input perturbations. Given a neural
network f (x0) : Rd → Rm, a nominal input ¯x ∈ Rd,
a linear function φ : Rm → R on the network’s output,
and a perturbation radius (cid:15) ∈ R, the network veriﬁcation
problem (Raghunathan et al., 2018; Salman et al., 2019;
Tjandraatmadja et al., 2020) asks to either verify that

φ(f (x0)) > 0 ∀x0 : (cid:107)x0 − ¯x(cid:107)∞ ≤ (cid:15),

(6.16)

or to identify at least one counterexample to this relation.
Consider an L-layer feedforward neural network where

f (x0) = WLxL + bL,
xi+1 = ReLU(Wixi + bi),

i = 0, . . . , L − 1,

where Wi ∈ Rni+1×ni and bi ∈ Rni+1 are the network
weights and biases, respectively, and the so-called Rec-
tiﬁed Linear Unit (ReLU) activation function ReLU :
Rk → Rk is the element-wise positive part of its argu-
ment, ReLU(z) = [max(zi, 0)]k
i=1. Condition (6.16) can be
decided by solving the optimization problem

γ(cid:63) := min

x0,...,xL

cTxL + c0

subject to xi+1 = ReLU(Wixi + bi), i ∈ [L], (6.17a)
(6.17b)

(cid:107)x0 − ¯x(cid:107)∞ ≤ (cid:15),

where [L] := {0, 1, . . . , L − 1} and c, c0 are problem data
related to the linear function φ(·). If γ(cid:63) > 0, then (6.16)
holds, otherwise counterexamples can be found.

Since the action of the ReLU function can be described

by quadratic constraints,

y = ReLU(z) ⇐⇒ y ≥ z, y ≥ 0, y(y − z) = 0,

(a)

i − 1

i

i + 1

i + 2

(b)

Figure 6.3: Abstraction of (a) a 4-layer neural network into
(b) a chordal chain graph with four vertices and maximal
cliques {i − 1, i}, {i, i + 1} and {i + 1, i + 2}.

Figure 6.4: CPU time (seconds) required to solve SDP re-
laxations of the neural network veriﬁcation problem (6.17)
for image veriﬁcation, with and without sparsity exploitation.
The SDP solver was MOSEK, and sparsity was exploited using
SparseCoLO. The neural network, with L = 2 layers and ni = 64
neurons per layer, was trained for image classiﬁcation on the
MNIST dataset.

problem (6.17) can be reformulated into a QCQP with
(cid:3)T
variable x = (cid:2)xT
(Raghunathan et al., 2018),
and subsequently relaxed into an SDP as described in Sec-
tion 6.2 above. If the optimal value of this SDP is positive,
the network is veriﬁed; otherwise, nothing can be said.

1 , . . . , xT
L

0 , xT

Since the constraints (6.17a) have a very natural cascad-
ing structure, the interaction among variables x0, . . . , xL
can be modeled by a line graph with maximal cliques
Ci = {i, i + 1} for i = 0, . . . , L − 1 (see Figure 6.3 for
illustration with L = 4). The SDP relaxation of (6.17) in-
herits this cascading structure, in addition to any sparsity
coming from the structure of the weight matrices Wi. The
chordal decomposition techniques described in Section 3
can therefore be applied to solve it eﬃciently. This idea
has been recently validated by Batten et al. (2021), who
considered robustness veriﬁcation in the context of image
classiﬁers. For instance, the results reproduced in Fig-
ure 6.4 for a neural network with L = 2 layers and ni = 64

38

neurons per layer show that exploiting sparsity reduced
by two orders of magnitude the CPU time required to ver-
ify the robustness of an image classiﬁer on the MNIST
dataset. Similar results were obtained by Newton & Pa-
pachristodoulou (2021), and interested readers are invited
to consult Table 2 for references to more machine learning
applications where sparsity exploitation can dramatically
reduce computational complexity.

7. Conclusion and outlook

In this paper, we reviewed theory and applications of de-
composition methods for large-scale semideﬁnite and poly-
nomial optimization. Speciﬁcally, we presented classical
chordal decomposition results for sparse positive semidef-
inite matrices (cf. Theorems 2.1 to 2.4) and we discussed
how they can be exploited to implement eﬃcient ﬁrst- and
second-order algorithms for SDPs (Section 3). We showed
also how matrix decomposition (primarily, but not nec-
essarily, chordal) can be leveraged to exploit term spar-
sity and structural sparsity in large-scale polynomial op-
timization (Section 4).
In particular, we demonstrated
that many sparsity-exploiting techniques for polynomial
inequalties—including the well-known correlatively sparse
SOS representations and the recent TSSOS, CS-TSSOS
and chordal-TSSOS hierarchies—are based on the general
matrix decomposition strategy outlined in Section 4.2.1.
We also discussed how the classical chordal decomposition
theorem (Theorem 2.1) can be generalized in diﬀerent ways
to obtain SOS chordal decomposition theorems for sparse
polynomial matrices (cf. Theorems 4.4, 4.5 and 4.6 and
further results by Zheng & Fantuzzi (2020)). In Section 5,
we reviewed factor-width decompositions for SDPs with
dense semideﬁnite constraints, to which chordal decompo-
sition cannot be applied. Finally, in Section 6 we demon-
strated how all of these techniques can be used to reduce
the computational complexity of SDPs and polynomial op-
timization problems encountered in some control and ma-
chine learning applications. References to these and other
applications are summarized in Table 2.

Despite the considerable progress made in recent years,
numerical methods for semideﬁnite and polynomial opti-
mization are still far from being mature. The most press-
ing open challenge, in our opinion, lies in bridging the
gap between the size of SDPs that can currently be solved
with tractable computational resources, and the size of
the SDPs that arise from complex control applications.
Indeed, the state-of-the-art decomposition techniques re-
viewed in this article are often still not enough to enable
the use of semideﬁnite programming to analyze and con-
trol large-scale nonlinear systems. The same is true for
control problems with systems of smaller size, but which
require real-time computations.

Achieving signiﬁcant progress is likely to require the-
oretical extensions of the decomposition approaches we
have discussed, as well as the development of eﬃcient soft-
ware that can eﬀectively exploit modern multi-core and

distributed-memory computer architectures. We conclude
this article by outlining some possible research directions
that may bear fruit in the near future.

Combining matrix decomposition with other structures

SDPs encountered in applications often have structural
properties beyond sparsity, which can also be leveraged
to reduce computational complexity; examples are sym-
metries, the existence of low-rank solutions, and low-rank
data matrices (De Klerk, 2010; Gatermann & Parrilo,
2004; Majumdar et al., 2020).
It is natural to try and
combine the exploitation of such additional structure with
matrix decomposition, but, to the best of our knowledge, a
uniﬁed and theoretically robust framework to do so is yet
to be developed. Particular questions to be answered in
this context include whether there exist symmetry reduc-
tion techniques that preserve (or even promote) sparsity
in SDPs, and whether low-rank positive semideﬁnite com-
pletions (Dancis, 1992, Theorem 1.5) can be exploited in
SDPs with aggregate sparsity and low-rank optimal solu-
tions (see Jiang, 2017 and Miller et al., 2019a for some
results in this direction).

In addition, although we have presented chordal and
factor-width decompositions separately, they can be com-
bined if either one, applied in isolation, does not reduce
the complexity of a large-scale SDP enough. A relatively
straightforward approach (Miller et al., 2019b) is to ﬁrst
apply the standard chordal decomposition, and then en-
force positive semideﬁnite constraints associated to large
maximal cliques using factor-width approximations. This
idea can be taken forward in various directions; for in-
stance, one could use block-chordal and block-factor-width
decompositions, or extend ideas by Garstka et al. (2020)
to formulate adaptive strategies wherein cliques are either
combined or factor-width decomposed, depending on their
relative sizes and on the available computational resources.
Both ideas remain largely unexplored, and further work is
required to determine if they can be brought to bear on
real-life control problems.

Tailored hierarchies for sparse polynomial optimization

Almost all existing methods for exploiting term sparsity
in polynomial optimization rely on the general matrix de-
composition approach presented in Section 4.2.1, where
the Gram matrix associated with SOS certiﬁcates of non-
negativity is decomposed according to the maximal cliques
of a sparsity graph to be prescribed a priori. While the
correlatively sparse, TSSOS, and related hierarchies de-
scribed in Section 4.2 give useful general strategies to se-
lect this sparsity graph, there is ample scope for tailoring
the graph structure in particular control applications. It is
not unreasonable to expect that problem-speciﬁc choices,
motivated for example by physical intuition on the dynam-
ical system one is trying to analyse or control, may bring
signiﬁcant further gains. However, it remains to be seen
whether this expectation can be met in practice. Better

39

integration between the development of optimization tools
and application-related modeling, discussed further below,
seems key to achieving progress in this direction.

Decomposition and completion of polynomial matrices

The exploitation of sparsity for polynomial matrix inequal-
ities can be improved in various directions, reducing com-
putational complexity beyond what can be achieved using
only the SOS chordal decomposition results summarized in
Section 4.3. For instance, those results can be combined in
a natural way with techniques to leverage term-sparsity in
scalar polynomial inequalities. Indeed, when a polynomial
matrix inequality P (x) (cid:23) 0 is “scalarized” into a nonnega-
tivity condition for the polynomial p(x, y) = yTP (x)y, the
structural sparsity of P translates into correlative sparsity
of p with respect to y. The matrix decomposition results
of Section 4.3 have equivalent statement at the scalar level
(Zheng & Fantuzzi, 2020, Section 4) that can be used to re-
ﬁne or extend term-sparse SOS decomposition hierarchies
for polynomials. The latter, in turn, can be used to eﬃ-
ciently handle (scalarized) polynomial matrix inequalities.
It would also be interesting to establish SOS completion
results for sparse polynomial matrices, in the spirit of The-
orem 2.2. Preliminary results in this direction exist (Zheng
et al., 2018a), but are far from complete. Extension of the
results in this reference will contribute to building a com-
prehensive theory for SOS chordal decomposition and com-
pletion of polynomial matrices, which can be used to build
tractable SDP approximations of large-scale optimization
problems with sparse polynomial matrix inequalities.

To chordality and beyond

Exploiting sparsity in semideﬁnite and polynomial opti-
mization without modifying the problem usually requires
chordality (cf. Theorems 2.1, 2.2, 2.3 and 2.4 for SDPs,
and Theorems 4.3, 4.5 and 4.6 for polynomial optimiza-
tion). Enforcing chordality with traditional chordal exten-
sion strategies, even if approximately minimal, may lead
to graphs with unacceptably large maximal cliques. The
largest maximal clique size plays a major role in determin-
ing the computational complexity of a decomposed SDP
(or SDP relaxation of a polynomial optimization problem).
Therefore, systematic techniques to produce chordal ex-
tensions that approximately minimize the largest maximal
cliques size would be very valuable.

If good chordal extensions prove hard to ﬁnd, a com-
pelling alternative is to sacriﬁce chordality and use non-
chordal graphs with small cliques that can be determined
analytically. This was done, for instance, by Nie & Dem-
mel (2009) and Koˇcvara (2020). While clique decomposi-
tions of matrix inequalities based on nonchordal graphs are
conservative in general, it may still be possible to identify
classes of matrices for which the equivalence between the
original and decomposed inequalities can be guaranteed.
For example, sparse (scaled)-diagonally dominant matri-
ces always admit a clique decomposition, even when their

sparsity graph is not chordal (Miller et al., 2019b, Propo-
sition 1). The same is true for certain positive semideﬁnite
matrices whose sparsity pattern can be extended to be of a
“block-arrow” type (Koˇcvara, 2020). Necessary and suﬃ-
cient cycle conditions for positive semideﬁnite completion
problem with nonchordal sparsity graphs were investigated
by Barrett et al. (1996). Extensions of these results, even
if limited to particular application domains, are likely to
enable considerable progress in the solution of large-scale
SDPs with nonchordal sparsity.

Eﬃcient software for modern computers

Reliable and user-friendly implementations of the cutting-
edge decomposition techniques for SDPs and polynomial
optimization problems reviewed in this paper are, in our
opinion, just as important as further theoretical advances.
Most of the available open-source packages mentioned in
Sections 3.5 and 4.5 have not yet reached the level of ma-
turity required to solve robustly a wide range of SDPs
or polynomial optimization problems arising from real-life
applications. Moreover, many of the commonly-used opti-
mization modeling environments on which these packages
rely are by now over a decade old, and often cannot handle
extremely large problems of industrial relevance eﬃciently.
The lack of very-high-performance software currently
limits the scale of problems that can be solved without ad-
hoc implementations. Since such implementations require
considerable expertise in large-scale optimization, the de-
ployment of SDP-based frameworks for system analysis
and control to real-world problems is currently hindered.
We expect that improvements in software reliability, eﬃ-
ciency, user-friendliness, and the ability to leverage mod-
ern multi-processor and/or distributed computing plat-
forms will considerably increase the practical impact of
decomposition methods for SDPs, bringing great beneﬁt
to the community of application-oriented researchers.

Blending application-driven modeling with optimization

The decomposition techniques reviewed in Sections 3, 4
and 5 apply to generic standard-form SDPs and polyno-
mial optimization problems, irrespective of the context in
which they arise. In control-related application, however,
SDPs and polynomial optimization problems often come
from modeling or relaxation frameworks for the study of
dynamical systems, the details of which strongly aﬀect the
structure of the eventual optimization problem. Bridg-
ing the existing gaps between application-driven modeling
and the development of large-scale optimization algorithm
promises to enable signiﬁcant progress in the study of lin-
ear and nonlinear systems. On the one hand, it may be
possible to implement tailored SDP solvers that target spe-
cial structures arising in particular applications. On the
other hand, given a particular control or analysis task,
one should attempt to formulate modelling approaches
that lead to optimization problems with a “computation-
ally friendly” structure. For example, when studying ﬂuid

40

ﬂows using semideﬁnite programming (see, e.g., Fantuzzi
et al., 2018 and Arslan et al., 2021), a smart discretization
of the ﬂow ﬁeld leads to SDPs with chordal aggregate spar-
sity that can be solved in minutes even though their linear
matrix inequalities have more than 10 000 rows/columns.
Similarly, using structured Lyapunov (or Lyapunov-like)
functions as explained in Section 6.1 can lead to struc-
tured SDPs, enabling the analysis of increasingly large
systems in ﬁelds such as robotics, smart energy grid, and
autonomous transportation.

Of course, the design of analysis and control frameworks
that combine system-level modeling with algorithmic con-
siderations will present a number of challenges. Resolv-
ing these challenges, however, promises to remove long-
standing barriers to the study of complex systems, espe-
cially nonlinear ones. Success seems likely to require a col-
laborative eﬀort between researchers working in diﬀerent
areas and an increasing awareness of outstanding problems
in particular application domains, as well as of state-of-
the-art tools for large-scale optimization. We hope that
the present review of decomposition methods for semideﬁ-
nite and polynomial optimization takes a step in the right
direction and can inspire new discoveries in the near fu-
ture.

Acknowledgements

Y.Z was supported in part by Clarendon Scholarship. G.F.
gratefully acknowledges funding from an Imperial College
Research Fellowship. A.P. was supported in part by the
Engineering and Physical Sciences Research Council (EP-
SRC) under project EP/M002454/1.

Appendix A. Cholesky factorization with no ﬁll-in

The no ﬁll-in property of the Cholesky factorization for
positive deﬁnite matrices with chordal sparsity is one of
the most important results for sparsity exploitation in ma-
trix calculations; for instance, it enables a simple proof
of Theorem 2.1 and eﬃcient computations involving bar-
rier functions for sparse matrix cones (cf. Section 3.4.2).
To formally introduce this no ﬁll-in property, we ﬁrst de-
ﬁne the notions of simplicial vertices and perfect elimina-
tion ordering for graphs.

Deﬁnition Appendix A.1. A vertex v in a graph
G(V, E) is called simplicial if all its neighbors are connected
to each other.

Algorithm 1 Maximal cardinality search

Input: A graph G(V, E)
Output: An elimination ordering α of G

for all vertices v in G do

w(v) = 0.

end for
for i = n to 1 do

pick an unnumbered vertex v with maximum weight

in w;

set α(v) = i;
for all unnumbered vertex u adjacent to v do

w(u) ← w(u) + 1;

end for

end for

(a)

(b)

C1 = {1, 2, 3}

C4 = {1, 3, 5}

C3 = {1, 5, 6}

C2 = {3, 4, 5}

(c)

(a) a chordal
Figure A.1: Chordal graph decomposition:
graph with six nodes; (b) maximal cliques; (c) a clique tree
that satisﬁes the clique intersection property.

and only if it has at least one perfect elimination order-
ing (Vandenberghe et al., 2015, Theorem 4.1). The max-
imal cardinality search (Algorithm 1) either returns one
of the perfect elimination orderings or certiﬁes that none
exists in O(|V| + |E|) time (Tarjan & Yannakakis, 1984).
+(E, 0) with
a chordal sparsity pattern E, we have a sparse Cholesky
factorization with zero ﬁll-in (Rose, 1970), (Vandenberghe
et al., 2015, Theorem 9.1)

Now, given a positive deﬁnite matrix Z ∈ Sn

PσZP T

σ = LLT,

σ (L + LT)Pσ ∈ Sn(E, 0),
P T

(A.1)

Deﬁnition Appendix A.2. An ordering σ =
{v1, . . . , vn} of the vertices in a graph G is a perfect
elimination ordering if each vi, i = 1, . . . , n, is a simplicial
vertex in the subgraph induced by nodes {vi, vi+1, . . . , vn}.

For example, vertices 2, 4, 6 are simplicial for the graph
in Figure A.1(a), and the ordering σ = {2, 4, 6, 1, 3, 5} is
a perfect elimination ordering. A graph G is chordal if

where Pσ is a permutation matrix corresponding to the
perfect elimination ordering σ and L is a lower-triangular
matrix. This can be proven using an elimination process
according to the perfect elimination ordering σ; see (Van-
denberghe et al., 2015, Chapter 9.1) and Kakimura (2010)
for details. Figure A.2 illustrates the process of sparse
Cholesky factorization for a 6 × 6 positive deﬁnite matrix
with chordal sparsity graph shown in Figure A.1(a).

41

412356135123435156























































Algorithm 2 Maximal clique search

Input: A chordal graph G(V, E), and a perfect elimination

ordering α = {v1, . . . , vn}

Output: All its maximal cliques C1, C2, . . . , Ct

(a)

(b)

(c)

(d)

Figure A.2: (a) A symbolic 6 × 6 sparse positive deﬁnite ma-
trix Z with chordal sparsity graph shown in Figure A.1(a). (b)
Sparsity pattern of PσZP T
σ for the perfect elimination order-
ing σ = {2, 4, 6, 1, 3, 5}. (c) Cholesky factor of Z; the entries
marked by
(d) Cholesky factor of
PσZP T
σ .

denote nonzero ﬁll-ins.

Initialize C0 = ∅;
for i = 1 to n do

Ci = {vi} ∪ {u adjacent to vi and behind vi in α};
if Ci is not a subset of C0 then
Ci is a maximal clique;
C0 = Ci;

end if

end for

Appendix B. A proof of Theorem 2.1

The sparse Cholesky factorization (A.1) with zero ﬁll-in al-
lows for a simple proof of Theorem 2.1. For simplicity, but
without loss of generality, assume that the matrix Z has al-
ready been permuted in such a way that σ = {1, 2, . . . , n}
is a perfect elimination ordering, so Pσ = I in (A.1). We
denote the columns of L by l1, l2, . . . , ln, and write

Z = LLT =

n
(cid:88)

i=1

lilT
i .

Since L+LT has the same sparsity pattern E, the non-zero
elements of each column vector li must be indexed by a
maximal clique Chi for some hi ∈ {1, . . . , t}. Thus, the
non-zero elements of li can be extracted through multipli-
cation by the matrix ECi, and we have

li = ET
Chi

EChi

li ⇒ lilT

i = ET
Chi

EChi

(cid:16)

(cid:124)

lilT
i ET
Chi
(cid:123)(cid:122)
Qi

(cid:17)

(cid:125)

EChi

.

Now, let Jk = {i : hi = k} be the set of column indices i
such that column i is indexed by clique Ck. These index
sets are disjoint and ∪kJk = {1, . . . , n}, so we obtain

Z = LLT =

=

=

n
(cid:88)

i=1
t
(cid:88)

ET
Chi

QiEChi

(cid:88)

ET
Ck

QiECk

k=1

i∈Jk

t
(cid:88)

k=1

(cid:18) (cid:88)

(cid:19)

Qi

ECk .

ET
Ck

i∈Jk

This is exactly (2.3) in Theorem 2.1 with matrices Zk =
(cid:80)

Qi that is in S|Ck|
+ .

i∈Jk

(Berry et al., 2004; Tarjan & Yannakakis, 1984). Algo-
rithm 2 is a simple strategy with a complexity O(|V| + |E|)
to ﬁnd all maximal cliques based on a perfect elimination
ordering. For example, the chordal graph in Figure A.1(a)
has the perfect elimination ordering σ = {2, 4, 6, 1, 3, 5},
and Algorithm 2 constructs the sets

C1 = {2, 1, 3}, C2 = {4, 3, 5}, C3 = {6, 5, 1},
C4 = {1, 3, 5}, C5 = {3, 5},

C6 = {5}.

The sets C1, . . . , C4 are maximal cliques, while C5, C6 are
not because they are subsets of C4.

The maximal cliques of a chordal graph can be arranged
in a so-called clique tree, that is, a graph T (Γ, Ξ) with the
maximal cliques Γ = {C1, . . . , Ct} as its vertices and an
edge set Ξ ⊆ Γ × Γ. In particular, the clique tree can be
chosen to satisfy the clique intersection property, meaning
that Ci ∩ Cj ⊆ Ck if clique Ck lies on the path between
cliques Ci and Cj in the tree and the intersection Ci ∩ Cj
is nonempty (Blair & Peyton, 1993). For example, the
clique tree in Figure A.1(c) satisﬁes the clique intersection
property.

The maximal cliques of a chordal graph play a central
role in the sparse matrix decomposition results stated in
Theorems 2.1, 2.2, 2.3 and 2.4. It is important to remem-
ber that these require one to use all maximal cliques in
the (chordal) sparsity graph of a matrix X, even when a
subset of cliques already covers all nonzero entries of X.
For example, consider the indeﬁnite matrix

X =


2
2


2


0


1

1

2
2
2
0
0
0

2
2
2
2
2
0

0
0
2
2
2
0

1
0
2
2
2
1











1
0
0
0
1
2

,

Appendix C. Some properties of maximal cliques

A connected chordal graph G(V, E) with n vertices has at
most n − 1 maximal cliques that can be identiﬁed in linear
time—more precisely, with a complexity of O(|V| + |E|)

whose chordal sparsity graph is shown in Figure A.1 and
has the four maximal cliques identiﬁed above. Even
though the maximal cliques C1, C2, and C3 already cover
all nonzero entries of the matrix, the maximal clique C4
is necessary when applying Theorem 2.2 to check whether
X admits a positive semideﬁnite completion. Indeed, ob-

42

serving that

EC1XET
C1

= EC2 XET
C2

=





2 2
2 2
2 2


 ∈ S3
+

2
2
2

and

EC3 XET
C3

=





2 1
1 2
1 1


 ∈ S3
+,

1
1
2

is not suﬃcient to conclude X ∈ S6
matrix

+(E, ?) because the sub-

EC4XET
C4

=


2
2

1





2 1
2 2
2 2

indexed by clique C4 has one negative eigenvalue. Simi-
larly, the matrix

Z =











4 2
2 4
2 2
0 0
1 0
1 0

2
2
3
2
2
0











0 1 1
0 0 0
2 2 0
3 2 0
2 3 1
0 1 3

is positive semideﬁnite and has the same sparsity graph as
above, but it does not admit a decomposition

Z =

3
(cid:88)

k=1

ET
Ck

ZkECk ,

Zk (cid:23) 0

that uses only cliques C1, C2, and C3; the last maximal
clique C4 is necessary for Theorem 2.1 to apply.
In-
deed, any decomposition using only the ﬁrst three maximal
cliques requires

Z1 =

Z2 =

Z3 =





2
α 2
2
4
2
2 β
2



 ,





2
3 − β 2
3
2
2 γ

2
2



 ,





4 − α
1
1

1
3 − γ
1


1
1
 ,
3

where α, β and γ must be selected to make these three
matrices positive semideﬁnite. For this, it is necessary
that the diagonal elements and all 2 × 2 principal minors
of Z1, Z2 and Z3 be nonnegative; in particular,

α, β, γ ≥ 0,

3 − β ≥ 0,

4 − α ≥ 0,

(C.1a)

4α − 4 ≥ 0, αβ − 4 ≥ 0,

(3 − β)γ − 4 ≥ 0,

(C.1b)

(4 − α)(3 − γ) − 1 ≥ 0.

(C.1c)

43

However, this set of inequalities is infeasible. Speciﬁcally,
inequality (C.1c) can be rearranged to show that

γ ≤

11 − 3α
4 − α

.

Moreover, we must have 3 − 4/α ≥ 3 − β ≥ 0, so α ≥ 4/3,
and therefore

4α
3α − 4

≤

4
3 − β

≤ γ ≤

11 − 3α
4 − α

.

But this cannot be true because 4/3 ≤ α ≤ 4, so 4α
11−3α
4−α strictly.

3α−4 >

References

Agler, J., Helton, W., McCullough, S., & Rodman, L. (1988). Pos-
itive semideﬁnite matrices with a given sparsity pattern. Linear
Algebra and Its Applications, 107 , 101–149.

Ahmadi, A. A., Dash, S., & Hall, G. (2017a). Optimization over
structured subsets of positive semideﬁnite matrices via column
generation. Discrete Optimization, 24 , 129–151.

Ahmadi, A. A., & Gunluk, O. (2018). Robust-to-Dynamics Opti-

mization. arXiv:1805.03682 [math.OC].

Ahmadi, A. A., & Hall, G. (2017). Sum of squares basis pursuit
with linear and second order cone programming. Contemporary
Mathematics, (pp. 25–54).

Ahmadi, A. A., Hall, G., Papachristodoulou, A., Saunderson, J., &
Zheng, Y. (2017b). Improving eﬃciency and scalability of sum of
squares optimization: Recent advances and limitations. In 2017
IEEE 56th Annual Conference on Decision and Control (CDC)
(pp. 453–462). IEEE.

Ahmadi, A. A., & Majumdar, A. (2019). DSOS and SDSOS op-
timization: more tractable alternatives to sum of squares and
semideﬁnite optimization. SIAM Journal on Applied Algebra and
Geometry, 3 , 193–230.

Ahmadi, S. P., Hansson, A., & Pakazad, S. K. (2019). Eﬃcient robust
model predictive control using chordality. In 2019 18th European
Control Conference (ECC) (pp. 4270–4275). IEEE.

Alizadeh, F., & Goldfarb, D. (2003). Second-order cone program-

ming. Mathematical programming, 95 , 3–51.

Andersen, M., & Vandenberghe, L. (2014). SMCP: Python extension
for sparse matrix cone programs, version 0.4. https://github.com/
cvxopt/smcp.

Andersen, M., & Vandenberghe, L. (2015). Chompack: a python

package for chordal matrix computations.

Andersen, M. S. (2011). Chordal sparsity in interior-point methods
for conic optimization. Ph.D. thesis University of California, Los
Angeles.

Andersen, M. S., Dahl, J., & Vandenberghe, L. (2010a).

Imple-
mentation of nonsymmetric interior-point methods for linear op-
timization over sparse matrix cones. Mathematical Programming
Comput., 2 , 167–201.

Andersen, M. S., Dahl, J., & Vandenberghe, L. (2013). Logarith-
mic barriers for sparse matrix cones. Optimization Methods and
Software, 28 , 396–423.

Andersen, M. S., Hansson, A., & Vandenberghe, L. (2014a).
Reduced-complexity semideﬁnite relaxations of optimal power
ﬂow problems. IEEE Transactions on Power Systems, 29 , 1855–
1863.

Andersen, M. S., Pakazad, S. K., Hansson, A., & Rantzer, A.
(2014b). Robust stability analysis of sparsely interconnected un-
certain systems. IEEE Transactions on Automatic Control, 59 ,
2151–2156.

Andersen, M. S., & Vandenberghe, L. (2010). Support vector ma-
chine training using matrix completion techniques. Technical Re-
port Technical report, University of California, Los Angeles, 2010.
6.2.

Andersen, M. S., Vandenberghe, L., & Dahl, J. (2010b). Linear
matrix inequalities with chordal sparsity patterns and applications
to robust quadratic optimization.
In 2010 IEEE International
Symposium on Computer-Aided Control System Design (pp. 7–
12). IEEE.

Anderson, J., & Papachristodoulou, A. (2011). A decomposition
technique for nonlinear dynamical system analysis. IEEE Trans-
actions on Automatic Control, 57 , 1516–1521.

Anderson, J., & Papachristodoulou, A. (2015). Advances in com-
putational Lyapunov analysis using sum-of-squares programming.
Discrete & Continuous Dynamical Systems-B , 20 , 2361.

Arslan, A., Fantuzzi, G., Craske, J., & Wynn, A. (2021). Bounds on
heat transport for convection driven by internal heating. Journal
of Fluid Mechanics, 919 , A15.

Astrom, K. J., & Kumar, P. (2014). Control: A perspective. Auto-

matica, 50 , 3–43.

Aylward, E. M., Itani, S. M., & Parrilo, P. A. (2007). Explicit
SOS decompositions of univariate polynomial matrices and the
In Proceedings of the 46th
Kalman-Yakubovich-Popov lemma.
IEEE Conference on Decision and Control (pp. 5660–5665).
IEEE.

Bai, X., Wei, H., Fujisawa, K., & Wang, Y. (2008). Semideﬁnite pro-
gramming for optimal power ﬂow problems. International Journal
of Electrical Power & Energy Systems, 30 , 383–392.

Baltean-Lugojan, R., Bonami, P., Misener, R., & Tramontani,
A. (2019).
Scoring positive semideﬁnite cutting planes for
quadratic optimization via trained neural networks. http://www.
optimization-online.org/DB HTML/2018/11/6943.html.

Banjac, G., Goulart, P., Stellato, B., & Boyd, S. (2019). Infeasibil-
ity detection in the alternating direction method of multipliers for
convex optimization. Journal of Optimization Theory and Appli-
cations, 183 , 490–519.

Barrett, W. W., Johnson, C. R., & Loewy, R. (1996). The Real Pos-
itive Deﬁnite Completion Problem: Cycle Completability volume
584. American Mathematical Soc.

Batten, B., Kouvaros, P., Lomuscio, A., & Zheng, Y. (2021). Eﬃcient
neural network veriﬁcation via layer-based semideﬁnite relaxations
and linear cuts. 30th International Joint Conference on Artiﬁcial
Intelligence (IJCAI-21), accepted.

Beck, A. (2017). First-order methods in optimization. SIAM.
Ben-Tal, A., & Nemirovski, A. (1998). Robust convex optimization.

Mathematics of operations research, 23 , 769–805.

Ben-Tal, A., & Nemirovski, A. (2001). Lectures on modern convex
optimization: analysis, algorithms, and engineering applications.
SIAM.

Benson, S. J., Ye, Y., & Zhang, X. (2000). Solving large-scale sparse
SIAM

semideﬁnite programs for combinatorial optimization.
Journal on Optimization, 10 , 443–461.

Berry, A., Blair, J. R., Heggernes, P., & Peyton, B. W. (2004). Max-
imum cardinality search for computing minimal triangulations of
graphs. Algorithmica, 39 , 287–298.

Blair, J. R., & Peyton, B. (1993). An introduction to chordal graphs
and clique trees. In Graph theory and sparse matrix computation
(pp. 1–29). Springer.

Blekherman, G., Dey, S. S., Molinaro, M., & Sun, S. (2020).
arXiv preprint

Sparse PSD approximation of the PSD cone.
arXiv:2002.02988.

Blekherman, G., Parrilo, P. A., & Thomas, R. R. (2012). Semideﬁ-

nite optimization and convex algebraic geometry. SIAM.

Boman, E. G., Chen, D., Parekh, O., & Toledo, S. (2005). On
factor width and symmetric h-matrices. Linear algebra and its
applications, 405 , 239–248.

Borchers, B. (1999). SDPLIB 1.2, a library of semideﬁnite program-
ming test problems. Optimization Methods and Software, 11 ,
683–690.

Boumal, N., Voroninski, V., & Bandeira, A. S. (2020). Deterministic
Guarantees for Burer–Monteiro Factorizations of Smooth Semidef-
inite Programs. Communications on Pure and Applied Mathemat-
ics, 73 , 581–608. URL: https://doi.org/10.1002/cpa.21830.

Boyd, S., Diaconis, P., & Xiao, L. (2004). Fastest mixing markov

chain on a graph. SIAM review , 46 , 667–689.

Boyd, S., El Ghaoui, L., Feron, E., & Balakrishnan, V. (1994). Linear
Matrix Inequalities in System and Control Theory. Society for
Industrial and Applied Mathematics.

Boyd, S., Parikh, N., Chu, E., Peleato, B., & Eckstein, J. (2011).
Distributed optimization and statistical learning via the alternat-
ing direction method of multipliers. Foundations and Trends®
in Machine Learning, 3 , 1–122.

Boyd, S., & Yang, Q. (1989). Structured and simultaneous Lyapunov
functions for system stability problems. International journal of
Control, 49 , 2215–2240.

Burer, S. (2003). Semideﬁnite programming in the space of partial
positive semideﬁnite matrices. SIAM Journal on Optimization,
14 , 139–172.

Burer, S., & Choi, C. (2006). Computational enhancements in low-
rank semideﬁnite programming. Optimization Methods and Soft-
ware, 21 , 493–512.

Burer, S., & Monteiro, R. D. (2003). A nonlinear programming algo-
rithm for solving semideﬁnite programs via low-rank factorization.
Mathematical Programming, 95 , 329–357.

Burer, S., & Monteiro, R. D. (2005). Local minima and convergence
in low-rank semideﬁnite programming. Mathematical Program-
ming, 103 , 427–444.

Burer, S., Monteiro, R. D. C., & Zhang, Y. (2002). Solving a class of
semideﬁnite programs via nonlinear programming. Mathematical
Programming, Series A, 93 , 97–122.

Carlson, D., Hershkowitz, D., & Shasha, D. (1992). Block diagonal
semistability factors and Lyapunov semistability of block triangu-
lar matrices. Linear Algebra and Its Applications, 172 , 1–25.
Chen, H., Liu, H.-T. D., Jacobson, A., & Levin, D. I. (2020a).
Chordal decomposition for spectral coarsening. arXiv preprint
arXiv:2009.02294.

Chen, T., Lasserre, J.-B., Magron, V., & Pauwels, E. (2020b). Semi-
algebraic optimization for Lipschitz constants of ReLU networks.
arXiv:2002.03657.

Chesi, G. (2011). Domain of attraction: analysis and control via
SOS programming volume 415. Springer Science & Business Me-
dia.

Cifuentes, D., & Parrilo, P. A. (2016). Exploiting chordal structure
in polynomial ideals: A grobner bases approach. SIAM Journal
on Discrete Mathematics, 30 , 1534–1570.

Cifuentes, D., & Parrilo, P. A. (2017). Chordal networks of polyno-
mial ideals. SIAM Journal on Applied Algebra and Geometry, 1 ,
73–110.

Coey, C., Kapelevich, L., & Vielma, J. P. (2020). Towards practical
generic conic optimization. arXiv preprint arXiv:2005.01136 , .
Dahl, J., Vandenberghe, L., & Roychowdhury, V. (2008). Covariance
selection for nonchordal graphs via chordal embedding. Optimiza-
tion Methods & Software, 23 , 501–520.

Dall’Anese, E., Zhu, H., & Giannakis, G. B. (2013). Distributed
optimal power ﬂow for smart microgrids. IEEE Transactions on
Smart Grid, 4 , 1464–1475.

Dancis, J. (1992). Positive semideﬁnite completions of partial hermi-
tian matrices. Linear algebra and its applications, 175 , 97–114.
De Klerk, E. (2010). Exploiting special structure in semideﬁnite
programming: A survey of theory and applications. European
Journal of Operational Research, 201 , 1–10.

Deroo, F., Meinel, M., Ulbrich, M., & Hirche, S. (2014). Distributed
control design with local model information and guaranteed sta-
bility. IFAC Proceedings Volumes, 47 , 4010–4017.

Deroo, F., Meinel, M., Ulbrich, M., & Hirche, S. (2015). Distributed
stability tests for large-scale systems with limited model infor-
mation. IEEE Transactions on Control of Network Systems, 2 ,
298–309.

Dvijotham, K. D., Stanforth, R., Gowal, S., Qin, C., De, S., & Kohli,
P. (2020). Eﬃcient neural network veriﬁcation with exactness
characterization.
In Uncertainty in Artiﬁcial Intelligence (pp.
497–507). PMLR.

Fantuzzi, G. (2020). Aeroimperial-YALMIP. https://github.com/

aeroimperial-optimization/aeroimperial-yalmip.

Fantuzzi, G., & Goluskin, D. (2020). Bounding extreme events in
nonlinear dynamics using convex optimization. SIAM Journal on
Applied Dynamical Systems, 19 , 1823–1864.

44

Fantuzzi, G., Goluskin, D., Huang, D., & Chernyshenko, S. I. (2016).
Bounds for deterministic and stochastic dynamical systems using
sum-of-squares optimization. SIAM Journal on Applied Dynam-
ical Systems, 15 , 1962–1988.

Fantuzzi, G., Pershin, A., & Wynn, A. (2018). Bounds on heat trans-
fer for B´enard–Marangoni convection at inﬁnite Prandtl number.
Journal of Fluid Mechanics, 837 , 562–596.

Fosson, S. M., & Abuabiah, M. (2019). Recovery of binary sparse
signals from compressed linear measurements via polynomial op-
timization. IEEE Signal Processing Letters, 26 , 1070–1074.

Fujisawa, K., Fukuda, M., Kojima, M., Nakata, K., & Yamashita,
M. (2004).
SDPA-C (semideﬁnite Programming Algorithm-
Completion Method). User’s Manual-Version 6-10 . Inst. of Tech-
nology.

Fujisawa, K., Kim, S., Kojima, M., Okamoto, Y., & Yamashita, M.
(2009). User’s manual for SparseCoLO: Conversion methods for
sparse conic-form linear optimization problems. Research Report
B-453, Dept. of Math. and Comp. Sci. Japan, Tech. Rep., (pp.
152–8552).

Fukuda, M., Kojima, M., Murota, K., & Nakata, K. (2001). Exploit-
ing sparsity in semideﬁnite programming via matrix completion I:
General framework. SIAM Journal on Optimization, 11 , 647–674.
Furieri, L., Zheng, Y., Papachristodoulou, A., & Kamgarpour, M.
(2019). On separable quadratic Lyapunov functions for convex
design of distributed controllers. In 2019 18th European Control
Conference (ECC) (pp. 42–49). IEEE.

Furieri, L., Zheng, Y., Papachristodoulou, A., & Kamgarpour, M.
(2020). Sparsity invariance for convex design of distributed con-
trollers. IEEE Transactions on Control of Network Systems, .
Gabay, D., & Mercier, B. (1976). A dual algorithm for the solution of
nonlinear variational problems via ﬁnite element approximation.
Computers & mathematics with applications, 2 , 17–40.

Garstka, M., Cannon, M., & Goulart, P. (2019). COSMO: A
conic operator splitting method for convex conic problems. arXiv
preprint arXiv:1901.10887.

Garstka, M., Cannon, M., & Goulart, P. (2020). A clique graph based
IFAC-PapersOnLine,

merging strategy for decomposable sdps.
53 , 7355–7361.

Gatermann, K., & Parrilo, P. A. (2004). Symmetry groups, semidef-
inite programs, and sums of squares. J. Pure Appl. Algebra, 192 ,
95–128.

Geromel, J. C., Bernussou, J., & Peres, P. L. D. (1994). Decentralized
control through parameter space optimization. Automatica, 30 ,
1565–1578.

via Occupation Measures. arXiv:1803.09022 [math.OC].

Hansson, A., & Pakazad, S. K. (2018). Exploiting chordality in
optimization algorithms for model predictive control. In Large-
Scale and Distributed Optimization (pp. 11–32). Springer.

Heinke, S., Schug, A.-K., & Werner, H. (2020). Distributed controller
design for systems interconnected over chordal graphs. In 2020
American Control Conference (ACC) (pp. 1569–1574). IEEE.
Henrion, D., & Garulli, A. (2005). Positive Polynomials in Control
volume 312 of Lecture Notes in Control and Information Sciences.
Springer-Verlag Berlin Heidelberg.

Henrion, D., & Korda, M. (2014). Convex computation of the region
of attraction of polynomial control systems. IEEE Transactions
on Automatic Control, 59 , 297–312.

Henrion, D., & Lasserre, J.-B. (2006). Convergent relaxations of
polynomial matrix inequalities and static output feedback. IEEE
Transactions on Automatic Control, 51 , 192–202.

Henrion, D., & Lasserre, J.-B. (2011).

Inner approximations for
polynomial matrix inequalities and robust stability regions. IEEE
Transactions on Automatic Control, 57 , 1456–1467.

Henrion, D., Lasserre, J.-B., & L¨ofberg, J. (2009). Gloptipoly 3: mo-
ments, optimization and semideﬁnite programming. Optimization
Methods & Software, 24 , 761–779.

Hilbert, D. (1888). Ueber die Darstellung deﬁniter Formen als
Summe von Formenquadraten. Mathematische Annalen, 32 , 342–
350.

Jabr, R. A. (2011). Exploiting sparsity in SDP relaxations of the
OPF problem. IEEE Transactions on Power Systems, 27 , 1138–
1139.

Jeyakumar, V., Kim, S., Lee, G., & Li, G. (2016). Semideﬁnite
programming relaxation methods for global optimization prob-
lems with sparse polynomials and unbounded semialgebraic feasi-
ble sets. Journal of Global Optimization, 65 , 175–190.

Jiang, X. (2017). Minimum rank positive semideﬁnite matrix com-

pletion with chordal sparsity pattern. Ph.D. thesis UCLA.

Jiang, X., & Vandenberghe, L. (2021). Bregman primal–dual ﬁrst-
order method and application to sparse semideﬁnite program-
ming. http://www.optimization-online.org/DB HTML/2020/03/
7702.html.

Jing, G., Wan, C., & Dai, R. (2019). Angle-based sensor network

localization. arXiv preprint arXiv:1912.01665 , .

Jones, M., & Peet, M. M. (2019). Using SOS and sublevel set vol-
ume minimization for estimation of forward reachable sets. IFAC-
PapersOnLine, 52 , 484–489.

Kailath, T. (1980). Linear systems volume 156. Prentice-Hall En-

Giulietti, F., Pollini, L., & Innocenti, M. (2000). Autonomous for-

glewood Cliﬀs, NJ.

mation ﬂight. IEEE Control Systems Magazine, 20 , 34–44.

Glowinski, R., & Marroco, A. (1975). Sur l’approximation, par
´el´ements ﬁnis d’ordre un, et la r´esolution, par p´enalisation-dualit´e
d’une classe de probl`emes de dirichlet non lin´eaires. ESAIM:
Mathematical Modelling and Numerical Analysis-Mod´elisation
Math´ematique et Analyse Num´erique, 9 , 41–76.

Goemans, M. X., & Williamson, D. P. (1995). Improved approxi-
mation algorithms for maximum cut and satisﬁability problems
using semideﬁnite programming. Journal of the ACM (JACM),
42 , 1115–1145.

Goldfarb, D., & Iyengar, G. (2003). Robust convex quadratically
constrained programs. Mathematical Programming, 97 , 495–515.
Golumbic, M. C. (2004). Algorithmic graph theory and perfect

graphs. Elsevier.

Goluskin, D. (2020). Bounding extrema over global attractors using

polynomial optimisation. Nonlinearity, 33 , 4878–4899.

Griewank, A., & Toint, P. L. (1984). On the existence of convex de-
compositions of partially separable functions. Mathematical Pro-
gramming, 28 , 25–49.

Grimm, D., Netzer, T., & Schweighofer, M. (2007). A note on the
representation of positive polynomials with structured sparsity.
Arch. Math. (Basel), 89 , 399–403.

Grone, R., Johnson, C. R., S´a, E. M., & Wolkowicz, H. (1984).
Positive deﬁnite completions of partial hermitian matrices. Linear
Algebra and Its Applications, 58 , 109–124.

Han, W., & Tedrake, R. (2018). Convex Optimization of Nonlinear
State Feedback Controllers for Discrete-time Polynomial Systems

45

Kakimura, N. (2010). A direct proof for the matrix decomposition of
chordal-structured positive semideﬁnite matrices. Linear Algebra
and Its Applications, 433 , 819–823.

Kalbat, A., & Lavaei, J. (2015). A fast distributed algorithm for
decomposable semideﬁnite programs. In 2015 54th IEEE Confer-
ence on Decision and Control (CDC) (pp. 1742–1749). IEEE.
Karisch, S. E., & Rendl, F. (1998). Semideﬁnite programming and
graph equipartition. Topics in Semideﬁnite and Interior-Point
Methods, 18 , 25.

Kim, S., Kojima, M., Mevissen, M., & Yamashita, M. (2011). Ex-
ploiting sparsity in linear and nonlinear matrix inequalities via
positive semideﬁnite matrix completion. Mathematical Program-
ming, 129 , 33–68.

Kim, S., Kojima, M., & Waki, H. (2009). Exploiting sparsity in
SDP relaxation for sensor network localization. SIAM Journal on
Optimization, 20 , 192–215.

Klep, I., Magron, V., & Povh, J. (2019). Sparse noncommutative

polynomial optimization. arXiv:1909.00569 [math.OC].

Koˇcvara, M. (2020). Decomposition of arrow type positive semidef-
inite matrices with application to topology optimization. Mathe-
matical Programming, (pp. 1–30).

Kojima, M. (2003).

Sums of squares relaxations of polynomial
semideﬁnite programs. Research Reports on Mathematical and
Computing Sciences Series B : Operations Research B-397 Tokyo
Institute of Technology.

Korda, M., Henrion, D., & Jones, C. N. (2013). Inner approxima-
tions of the region of attraction for polynomial dynamical systems.
IFAC Proceedings Volumes (IFAC-PapersOnline), 43 , 534–539.
Korda, M., Henrion, D., & Mezi´c, I. (2021). Convex computation of
extremal invariant measures of nonlinear dynamical systems and
Markov processes. J. Nonlinear Sci., 31 , 14(1–26).

Kuntz, J., Ottobre, M., Stan, G.-B., & Barahona, M. (2016). Bound-
ing stationary averages of polynomial diﬀusions via semideﬁ-
nite programming. SIAM Journal on Scientiﬁc Computing, 38 ,
A3891–A3920.

Lam, A. Y., Zhang, B., & David, N. T. (2012). Distributed algo-
rithms for optimal power ﬂow problem. In 2012 IEEE 51st IEEE
Conference on Decision and Control (CDC) (pp. 430–437). IEEE.
Lasagna, D., Huang, D., Tutty, O. R., & Chernyshenko, S. I. (2016).
Sum-of-Squares approach to feedback control of laminar wake
ﬂows. Journal of Fluid Mechanics, 809 , 628–663.

Lasserre, J.-B. (2006). Convergent SDP-relaxations in polynomial
optimization with sparsity. SIAM Journal on Optimization, 17 ,
822–843.

Lasserre, J.-B. (2010). Moments, Positive Polynomials and their

Applications. Imperial College Press.

Lasserre, J. B., Henrion, D., Prieur, C., & Tr´elat, E. (2008). Nonlin-
ear optimal control via occupation measures and LMI-relaxations.
SIAM Journal on Control and Optimization, 47 , 1643–1666.
Lasserre, J. B., Toh, K.-C., & Yang, S. (2017). A bounded degree
SOS hierarchy for polynomial optimization. EURO Journal on
Computational Optimization, 5 , 87–117.

Latorre, F., Rolland, P., & Cevher, V. (2020). Lipschitz constant
estimation of neural networks via sparse polynomial optimization.
arXiv preprint arXiv:2004.08688 , .

Legat, B., Coey, C., Deits, R., Huchette, J., & Perry, A. (2017). Sum-
of-squares optimization in Julia. In The First Annual JuMP-dev
Workshop.

Li, H., Xia, B., Zhang, H., & Zheng, T. (2021). Choosing the vari-
able ordering for cylindrical algebraic decomposition via exploiting
chordal structure. arXiv preprint arXiv:2102.00823 , .

Li, S. E., Zheng, Y., Li, K., Wu, Y., Hedrick, J. K., Gao, F., &
Zhang, H. (2017). Dynamical modeling and distributed control of
connected and automated vehicles: Challenges and opportunities.
IEEE Intelligent Transportation Systems Magazine, 9 , 46–58.
Liu, H.-T. D., Jacobson, A., & Ovsjanikov, M. (2019). Spectral
coarsening of geometric operators. ACM Transactions on Graph-
ics (TOG), 38 , 1–13.

Liu, Y., Ryu, E. K., & Yin, W. (2017). A new use of douglas-rachford
splitting and admm for identifying infeasible, unbounded, and
pathological conic programs. arXiv preprint arXiv:1706.02374.
L¨ofberg, J. (2004). YALMIP: A toolbox for modeling and optimiza-
tion in matlab. In Proceedings of the IEEE International Sympo-
sium on Computer-Aided Control System Design (pp. 284–289).
IEEE.

L¨ofberg, J. (2009). Dualize it: software for automatic primal and dual
conversions of conic programs. Optimization Methods & Software,
24 , 313–325.

L¨ofberg, J. (2009). Pre-and post-processing sum-of-squares programs
in practice. IEEE Transactions on Automatic Control, 54 , 1007–
1011.

Lu, Z., Nemirovski, A., & Monteiro, R. D. (2007). Large-scale
semideﬁnite programming via a saddle point mirror-prox algo-
rithm. Mathematical programming, 109 , 211–237.

Madani, R., Kalbat, A., & Lavaei, J. (2017a). A low-complexity par-
allelizable numerical algorithm for sparse semideﬁnite program-
ming.
IEEE Transactions on Control of Network Systems, 5 ,
1898–1909.

Madani, R., Sojoudi, S., Fazelnia, G., & Lavaei, J. (2017b). Finding
low-rank solutions of sparse linear matrix inequalities using convex
optimization. SIAM Journal on Optimization, 27 , 725–758.

Magron, V., Garoche, P.-L., Henrion, D., & Thirioux, X. (2019).
Semideﬁnite approximations of reachable sets for discrete-time
polynomial systems. SIAM Journal on Control and Optimiza-
tion, 57 , 2799–2820.

Magron, V., & Wang, J. (2021). TSSOS: A Julia library to exploit
sparsity for large-scale polynomial optimization. arXiv:2103.00915

46

[math.OC].

Mai, N. H. A., Magron, V., & Lasserre, J.-B. (2020). A sparse version
of Reznick’s Positivstellensatz. arXiv:2002.05101 [math.OC].
Majumdar, A., Hall, G., & Ahmadi, A. A. (2020). Recent scalability
improvements for semideﬁnite programming with applications in
machine learning, control, and robotics. Annual Review of Con-
trol, Robotics, and Autonomous Systems, 3 , 331–360.

Majumdar, A., Vasudevan, R., Tobenkin, M. M., & Tedrake, R.
(2014). Convex optimization of nonlinear feedback controllers via
occupation measures. The International Journal of Robotics Re-
search, 33 , 1209–1230.

Mason, R. (2015). A chordal sparsity approach to scalable linear and
nonlinear systems analysis. Ph.D. thesis University of Oxford.
Mason, R. P., & Papachristodoulou, A. (2014). Chordal sparsity,
decomposing SDPs and the Lyapunov equation. In 2014 American
Control Conference (pp. 531–537). IEEE.

Mevissen, M. (2010). Sparse semideﬁnite programming relaxations
for large scale polynomial optimization and their applications to
diﬀerential equations. Ph.D. thesis Tokyo Institute of Technology.
Mevissen, M., Kojima, M., Nie, J., & Takayama, N. (2008). Solving
partial diﬀerential equations via sparse SDP relaxations. Paciﬁc
Journal of Optimization, 4 , 213–241.

Mevissen, M., Lasserre, J. B., & Henrion, D. (2011). Moment and
SDP relaxation techniques for smooth approximations of prob-
lems involving nonlinear diﬀerential equations. IFAC Proceedings
Volumes, 44 , 10887–10892.

Mevissen, M., Yokoyama, K., & Takayama, N. (2009). Solutions of
Polynomial Systems Derived from the Steady Cavity Flow Prob-
lem. In Proceedings of the 2009 international symposium on sym-
bolic and algebraic computation (pp. 255–262). Seoul, Republic
of Korea: Association for Computing Machinery.

Miller, J., Henrion, D., & Sznaier, M. (2021). Peak estimation re-
IEEE Control Systems Letters, 5 ,

covery and safety analysis.
1982–1987.

Miller, J., Zheng, Y., Roig-Solvas, B., Sznaier, M., & Pa-
pachristodoulou, A. (2019a). Chordal decomposition in rank min-
imized semideﬁnite programs with applications to subspace clus-
tering. In 2019 IEEE 58th Conference on Decision and Control
(CDC) (pp. 4916–4921). IEEE.

Miller, J., Zheng, Y., Sznaier, M., & Papachristodoulou, A. (2019b).
Decomposed structured subsets for semideﬁnite and sum-of-
squares optimization. arXiv:1911.12859 [math.OC].

Molzahn, D. K., & Hiskens, I. A. (2014).

Sparsity-exploiting
moment-based relaxations of the optimal power ﬂow problem.
IEEE Transactions on Power Systems, 30 , 3168–3180.

Molzahn, D. K., Holzer, J. T., Lesieutre, B. C., & DeMarco, C. L.
(2013). Implementation of a large-scale optimal power ﬂow solver
based on semideﬁnite programming. IEEE Transactions on Power
Systems, 28 , 3987–3998.

Mosek, A. (2015). The mosek optimization toolbox for matlab man-

ual.

Motzkin, T. S. (1967). The arithmetic-geometric inequality.

In
Inequalities (Proc. Sympos. Wright-Patterson Air Force Base,
Ohio, 1965) (pp. 205–224).

Mou, C., Bai, Y., & Lai, J. (2021). Chordal graphs in triangular de-
composition in top-down style. Journal of Symbolic Computation,
102 , 108–131.

Murray, R. M., Astrom, K. J., Boyd, S. P., Brockett, R. W., &
Stein, G. (2003). Future directions in control in an information-
rich world. IEEE control systems magazine, 23 , 20–33.

Murty, K. G., & Kabadi, S. N. (1987). Some NP-complete problems
in quadratic and nonlinear programming. Mathematical Program-
ming, 39 , 117–129.

Nakata, K., Fujisawa, K., Fukuda, M., Kojima, M., & Murota, K.
(2003). Exploiting sparsity in semideﬁnite programming via ma-
trix completion II: implementation and numerical results. Math-
ematical Programming B , 95 , 303–327.

Nemirovski, A. (2006). Advances in convex optimization: Conic
programming. In International Congress of Mathematicians (pp.
413–444). volume 1.

Nesterov, Y. (2003). Introductory lectures on convex optimization:
A basic course volume 87. Springer Science & Business Media.
Nesterov, Y. (2012). Towards non-symmetric conic optimization.

Optimization methods and software, 27 , 893–917.

Nesterov, Y., & Nemirovski, A. (1994). Interior-Point Polynomial

Algorithms in Convex Programming. SIAM.

Nesterov, Y., Wolkowicz, H., & Ye, Y. (2000). Semideﬁnite pro-
gramming relaxations of nonconvex quadratic optimization.
In
Handbook of semideﬁnite programming (pp. 361–419). Springer.
Newton, M., & Papachristodoulou, A. (2021). Exploiting sparsity for
neural network veriﬁcation. 3rd Annual Learning for Dynamics
and Control Conference , accepted.

Nie, J. (2009). Sum of squares method for sensor network localiza-
tion. Computational Optimization and Applications, 43 , 151–179.
Nie, J., & Demmel, J. (2009). Sparse sos relaxations for minimiz-
ing functions that are summations of small polynomials. SIAM
Journal on Optimization, 19 , 1534–1558.

O’Donoghue, B., Chu, E., Parikh, N., & Boyd, S. (2016). Conic op-
timization via operator splitting and homogeneous self-dual em-
bedding. Journal of Optimization Theory and Applications, 169 ,
1042–1068.

O’Donoghue, B., Chu, E., Parikh, N., & Boyd, S. (2019). SCS:
Splitting conic solver, version 2.1.2. https://github.com/cvxgrp/
scs.

Pakazad, S. K., Hansson, A., Andersen, M. S., & Nielsen, I. (2017a).
Distributed primal–dual interior-point methods for solving tree-
structured coupled convex problems using message-passing. Op-
timization Methods and Software, 32 , 401–435.

Pakazad, S. K., Hansson, A., Andersen, M. S., & Rantzer, A.
(2017b). Distributed semideﬁnite programming with application
to large-scale system analysis. IEEE Transactions on Automatic
Control, 63 , 1045–1058.

Papachristodoulou, A., & Prajna, S. (2005). A tutorial on sum
of squares techniques for systems analysis.
In American Con-
trol Conference, 2005. Proceedings of the 2005 (pp. 2686–2700).
IEEE.

Park, J., & Boyd, S. (2017). General heuristics for nonconvex
quadratically constrained quadratic programming. arXiv preprint
arXiv:1703.07870 , .

Parrilo, P. A. (2000). Structured semideﬁnite programs and semial-
gebraic geometry methods in robustness and optimization. Ph.D.
thesis California Institute of Technology.

Parrilo, P. A. (2003). Semideﬁnite programming relaxations for semi-
algebraic problems. Mathematical Programming, 96 , 293–320.
Parrilo, P. A. (2013). Polynomial optimization , sums of squares and
applications. In G. Blekherman, P. A. Parrilo, & R. R. Thomas
(Eds.), Semideﬁnite optimization and convex algebraic geometry
chapter 3. (pp. 47–157). SIAM. (1st ed.).

Parrilo, P. A., & Lall, S. (2003). Semideﬁnite programming relax-
ations and algebraic optimization in control. European Journal of
Control, 9 , 307–321.

Pataki, G. (1998). On the rank of extreme matrices in semideﬁnite
programs and the multiplicity of optimal eigenvalues. Mathemat-
ics of Operations Research, 23 , 339–358.

Peet, M. M., & Papachristodoulou, A. (2012). A converse sum of
squares Lyapunov result with a degree bound. IEEE Transactions
on Automatic Control, 57 , 2281–2293.

Peet, M. M., Papachristodoulou, A., & Lall, S. (2009). Positive
forms and stability of linear time-delay systems. SIAM Journal
on Control and Optimization, 47 , 3237–3258.

Permenter, F., & Parrilo, P. A. (2014a). Basis selection for sos pro-
grams via facial reduction and polyhedral approximations. In Pro-
ceedings of the 53rd IEEE Conference on Decision and Control
(pp. 6615–6620). IEEE.

Permenter, F., & Parrilo, P. A. (2014b). Partial facial reduction:
simpliﬁed, equivalent SDPs via approximations of the PSD cone.
Mathematical Programming, (pp. 1–54).

Ploeg, J., Shukla, D. P., van de Wouw, N., & Nijmeijer, H. (2013).
Controller synthesis for string stability of vehicle platoons. IEEE
Transactions on Intelligent Transportation Systems, 15 , 854–865.
Prajna, S., Jadbabaie, A., & Pappas, G. J. (2007). A framework for
worst-case and stochastic safety veriﬁcation using barrier certiﬁ-

47

cates. IEEE Transactions on Automatic Control, 52 , 1415–1428.
Prajna, S., Papachristodoulou, A., & Parrilo, P. A. (2002). Introduc-
ing SOSTOOLS: A general purpose sum of squares programming
solver. In Proceedings of the 41st IEEE Conference on Decision
and Control, 2002. (pp. 741–746). IEEE volume 1.

Prajna, S., Papachristodoulou, A., & Wu, F. (2004). Nonlinear con-
trol synthesis by sum of squares optimization: A Lyapunov-based
approach. In 2004 5th Asian Control Conference (IEEE Cat. No.
04EX904) (pp. 157–165). IEEE volume 1.

Putinar, M. (1993). Positive polynomials on compact semi-algebraic
sets. Indiana University Mathematics Journal, 42 , 969–984.
Raghunathan, A., Steinhardt, J., & Liang, P. S. (2018). Semideﬁnite
relaxations for certifying robustness to adversarial examples. In
Advances in Neural Information Processing Systems (pp. 10877–
10887).

Rajamani, R. (2011). Vehicle dynamics and control. Springer Science

& Business Media.

Reznick, B. (1978). Extremal PSD forms with few terms. Duke

Math. J., 45 , 363–374.

Reznick, B. (1995). Uniform denominators in Hilbert’s seventeenth

problem. Math. Z., 220 , 75–97.

Riener, C., Theobald, T., Andr´en, L. J., & Lasserre, J.-B. (2013).
Exploiting symmetries in SDP-relaxations for polynomial opti-
mization. Mathematics of Operations Research, 38 , 122–141.
Riverso, S., Sarzo, F., & Ferrari-Trecate, G. (2014). Plug-and-play
voltage and frequency control of islanded microgrids with meshed
topology. IEEE Transactions on Smart Grid, 6 , 1176–1184.
Rose, D. J. (1970). Triangulated graphs and the elimination process.
Journal of Mathematical Analysis and Applications, 32 , 597–609.
Sadabadi, M. S., Shaﬁee, Q., & Karimi, A. (2016). Plug-and-play
voltage stabilization in inverter-interfaced microgrids via a robust
control strategy. IEEE Transactions on Control Systems Tech-
nology, 25 , 781–791.

Salman, H., Yang, G., Zhang, H., Hsieh, C.-J., & Zhang, P. (2019). A
convex relaxation barrier to tight robustness veriﬁcation of neural
networks. arXiv preprint arXiv:1902.08722 , .

Scherer, C., & Hol, C. (2006). Matrix sum-of-squares relaxations for
robust semi-deﬁnite programs. Mathematical Programming, 107 ,
189–211.

Schlosser, C., & Korda, M. (2020). Sparse moment-sum-of-squares
relaxations for nonlinear dynamical systems with guaranteed con-
vergence. arXiv:2012.05572 [math.OC].

Schm¨udgen, K. (2009). Noncommutative real algebraic geometry
some basic concepts and ﬁrst ideas. In Emerging Applications of
Algebraic Geometry (pp. 325–350). Springer.

Skajaa, A., & Ye, Y. (2015). A homogeneous interior-point algo-
rithm for nonsymmetric convex conic optimization. Mathematical
Programming, 150 , 391–422.

So, A. M.-C., & Ye, Y. (2007). Theory of semideﬁnite programming
for sensor network localization. Mathematical Programming, 109 ,
367–384.

Song, D., & Parrilo, P. A. (2021). On approximations of the psd
cone by a polynomial number of smaller-sized psd cones. arXiv
preprint arXiv:2105.02080 , .

Sootla, A., Zheng, Y., & Papachristodoulou, A. (2017). Block-
diagonal solutions to Lyapunov inequalities and generalisations
of diagonal dominance. In 2017 IEEE 56th Annual Conference
on Decision and Control (CDC) (pp. 6561–6566). IEEE.

Sootla, A., Zheng, Y., & Papachristodoulou, A. (2019). On the
existence of block-diagonal solutions to Lyapunov and H∞ Riccati
inequalities. IEEE Transactions on Automatic Control, 65 , 3170–
3175.

Sturm, J. F. (1999). Using SeDuMi 1.02, a MATLAB toolbox for
optimization over symmetric cones. Optim. Methods Softw., 11 ,
625–653.

Sun, D., Toh, K.-C., Yuan, Y., & Zhao, X.-Y. (2020). Sdpnal+: A
matlab software for semideﬁnite programming with bound con-
straints (version 1.0). Optimization Methods and Software, 35 ,
87–115.

Sun, Y. (2015). Decomposition methods for semideﬁnite optimiza-

tion. Ph.D. thesis UCLA.

Sun, Y., Andersen, M. S., & Vandenberghe, L. (2014). Decompo-
sition in conic optimization with partially separable structure.
SIAM Journal on Optimization, 24 , 873–897.

Sun, Y., & Vandenberghe, L. (2015). Decomposition methods for
sparse matrix nearness problems. SIAM Journal on Matrix Anal-
ysis and Applications, 36 , 1691–1717.

Tacchi, M., Cardozo, C., Henrion, D., & Lasserre, J.-B. (2019a).
Approximating regions of attraction of a sparse polynomial diﬀer-
ential system. arXiv:1911.09500 [math.OC].

Tacchi, M., Weisser, T., Lasserre, J.-B., & Henrion, D. (2019b).
Exploiting sparsity for semi-algebraic set volume computation.
arXiv:1902.02976 [math.OC].

Tarjan, R. E., & Yannakakis, M. (1984). Simple linear-time algo-
rithms to test chordality of graphs, test acyclicity of hypergraphs,
and selectively reduce acyclic hypergraphs. SIAM Journal on
computing, 13 , 566–579.

Tjandraatmadja, C., Anderson, R., Huchette, J., Ma, W., Patel, K.,
& Vielma, J. P. (2020). The convex relaxation barrier, revisited:
Tightened single-neuron relaxations for neural network veriﬁca-
tion. arXiv preprint arXiv:2006.14076 , .

Tomita, E., Tanaka, A., & Takahashi, H. (2006). The worst-case time
complexity for generating all maximal cliques and computational
experiments. Theoretical computer science, 363 , 28–42.

Topcu, U., Packard, A. K., Seiler, P., & Balas, G. J. (2009). Robust
region-of-attraction estimation. IEEE Transactions on Automatic
Control, 55 , 137–142.

T¨ut¨unc¨u, R. H., Toh, K.-C., & Todd, M. J. (2003).

Solving
semideﬁnite-quadratic-linear programs using sdpt3. Mathematical
programming, 95 , 189–217.

Valmorbida, G., & Anderson, J. (2017). Region of attraction es-
timation using invariant sets and rational Lyapunov functions.
Automatica, 75 , 37–45.

Vandenberghe, L., Andersen, M. S. et al. (2015). Chordal graphs and
semideﬁnite optimization. Found. Trends Optim., 1 , 241–433.
Vandenberghe, L., & Boyd, S. (1996). Semideﬁnite Programming.

SIAM Rev., 38 , 49–95.

Waki, H., Kim, S., Kojima, M., & Muramatsu, M. (2006). Sums
of squares and semideﬁnite program relaxations for polynomial
optimization problems with structured sparsity. SIAM Journal
on Optimization, 17 , 218–242.

Waki, H., Kim, S., Kojima, M., Muramatsu, M., & Sugimoto, H.
(2008). Algorithm 883: Sparsepop—a sparse semideﬁnite pro-
gramming relaxation of polynomial optimization problems. ACM
Transactions on Mathematical Software (TOMS), 35 , 1–13.

Waki, H., & Muramatsu, M. (2010). A facial reduction algorithm for
ﬁnding sparse SOS representations. Operations Research Letters,
38 , 361–365.

Waldspurger, I., & Waters, A. (2020). Rank optimality for the
Burer–Monteiro factorization. SIAM Journal on Optimization,
30 , 2577–2602. URL: https://doi.org/10.1137/19M1255318.

Wang, J., Li, H., & Xia, B. (2019). A new sparse SOS decomposition
algorithm based on term sparsity. In Proceedings of the ACM In-
ternational Symposium on Symbolic and Algebraic Computation
(pp. 347–354).

Wang, J., Magron, V., & Lasserre, J.-B. (2021a). Chordal-TSSOS:
a moment-SOS hierarchy that exploits term sparsity with chordal
extension. SIAM Journal on Optimization, 31 , 114–141.

Wang, J., Magron, V., & Lasserre, J.-B. (2021b). TSSOS: A moment-
SOS hierarchy that exploits term sparsity. SIAM Journal on Op-
timization, 31 , 30–58.

Wang, J., Magron, V., Lasserre, J.-B., & Mai, N. H. A. (2020a). CS-
TSSOS: Correlative and term sparsity for large-scale polynomial
optimization. arXiv:2005.02828 [math.OC].

Wang, J., Zheng, Y., Chen, C., Xu, Q., & Li, K. (2020b). Leading
cruise control in mixed traﬃc ﬂow: System modeling, controlla-
bility, and string stability. arXiv preprint arXiv:2012.04313 , .
Wang, Y., Tanaka, A., & Yoshise, A. (2021c). Polyhedral approxi-
mations of the semideﬁnite cone and their application. Computa-
tional Optimization and Applications, 78 , 893–913.

Weisser, T., Lasserre, J.-B., & Toh, K.-C. (2018). Sparse-BSOS:
a bounded degree SOS hierarchy for large scale polynomial opti-

mization with sparsity. Mathematical Programming Comput., 10 ,
1–32.

Weisser, T., Legat, B., Coey, C., Kapelevich, L., & Vielma,
J. P. (2019). Polynomial and moment optimization in julia and
jump. In JuliaCon. URL: https://pretalx.com/juliacon2019/talk/
QZBKAU/.

Wen, Z., Goldfarb, D., & Yin, W. (2010). Alternating direction aug-
mented lagrangian methods for semideﬁnite programming. Math-
ematical Programming Computation, 2 , 203–230.

Weng, Y., Li, Q., Negi, R., & Ili´c, M. (2013). Distributed algorithm
for SDP state estimation. In 2013 IEEE PES Innovative Smart
Grid Technologies Conference (ISGT) (pp. 1–6). IEEE.

Yamashita, M., Fujisawa, K., Fukuda, M., Kobayashi, K., Nakata,
K., & Nakata, M. (2012). Latest developments in the SDPA family
for solving large-scale sdps. In Handbook on semideﬁnite, conic
and polynomial optimization (pp. 687–713). Springer.

Yang, C.-H., & Deng, B. S. (2020). Exploiting sparsity in SDP
relaxation for harmonic balance method. IEEE Access, 8 , 115957–
115965.

Yang, H., & Carlone, L. (2020). One ring to rule them all: Certi-
ﬁably robust geometric perception with outliers. arXiv preprint
arXiv:2006.06769 , .

Yannakakis, M. (1981). Computing the minimum ﬁll-in is NP-
complete. SIAM Journal on Algebraic Discrete Methods, 2 , 77–
79.

Ye, Y. (2011). Interior point algorithms: theory and analysis vol-

ume 44. John Wiley & Sons.

√

Ye, Y., Todd, M. J., & Mizuno, S. (1994). An o(

nl)-iteration homo-
geneous and self-dual linear programming algorithm. Mathematics
of operations research, 19 , 53–67.

Yurtsever, A., Tropp, J. A., Fercoq, O., Udell, M., & Cevher, V.
(2021). Scalable semideﬁnite programming. SIAM Journal on
Mathematics of Data Science, 3 , 171–200.

Zhang, R., Fattahi, S., & Sojoudi, S. (2018). Large-scale sparse
inverse covariance estimation via thresholding and max-det matrix
completion.
In International Conference on Machine Learning
(pp. 5766–5775). PMLR.

Zhang, R. Y. (2020). On the tightness of semideﬁnite relaxations
for certifying robustness to adversarial examples. arXiv preprint
arXiv:2006.06759 , .

Zhang, R. Y., & Lavaei, J. (2020a). Dual-CTC. https://github.com/

ryz-codes/dual ctc.

Zhang, R. Y., & Lavaei, J. (2020b). Sparse semideﬁnite programs
with guaranteed near-linear time complexity via dualized clique
tree conversion. Mathematical programming, (pp. 1–43).

Zhang, Y., Madani, R., & Lavaei, J. (2017). Conic relaxations for
IEEE

power system state estimation with line measurements.
Transactions on Control of Network Systems, 5 , 1193–1205.
Zhao, X.-Y., Sun, D., & Toh, K.-C. (2010). A Newton-CG augmented
lagrangian method for semideﬁnite programming. SIAM Journal
on Optimization, 20 , 1737–1765.

Zheng, Y. (2019). Chordal sparsity in control and optimization of

large-scale systems. Ph.D. thesis University of Oxford.

Zheng, Y., & Fantuzzi, G. (2020).

Sum-of-squares chordal de-
composition of polynomial matrix inequalities. arXiv preprint
arXiv:2007.11410 , .

Zheng, Y., Fantuzzi, G., & Papachristodoulou, A. (2018a). Decom-
position and completion of sum-of-squares matrices. In Proceed-
ings of the 57th IEEE Conference on Decision and Control (pp.
4026–4031). IEEE.

Zheng, Y., Fantuzzi, G., & Papachristodoulou, A. (2018b). Fast
ADMM for sum-of-squares programs using partial orthogonality.
IEEE Transactions on Automatic Control, 64 , 3869–3876.

Zheng, Y., Fantuzzi, G., & Papachristodoulou, A.

(2019a).
Sparse sum-of-squares (SOS) optimization: A bridge between
DSOS/SDSOS and SOS optimization for sparse polynomials. In
Proceedings of the 2019 American Control Conference (pp. 5513–
5518). IEEE.

Zheng, Y., Fantuzzi, G., Papachristodoulou, A., Goulart, P., &
Wynn, A. (2016). CDCS: Cone decomposition conic solver, ver-
sion 1.1. https://github.com/oxfordcontrol/CDCS.

48

IEEE Transactions on Automatic Control, 63 , 752–767.

Zheng, Y., Sootla, A., & Papachristodoulou, A.

Block factor-width-two matrices and their applications
semideﬁnite and sum-of-squares optimization.
arXiv:1909.11076 , .

(2019b).
to
arXiv preprint

Zheng, Y., Wang, J., & Li, K. (2020). Smoothing traﬃc ﬂow via
control of autonomous vehicles. IEEE Internet of Things Journal,
7 , 3882–3896.

Zhou, K., Doyle, J. C., Glover, K. et al. (1996). Robust and optimal

control volume 40. Prentice hall New Jersey.

Zhu, H., & Giannakis, G. B. (2014). Power system nonlinear state es-
timation using distributed semideﬁnite programming. IEEE Jour-
nal of Selected Topics in Signal Processing, 8 , 1039–1050.

Zheng, Y., Fantuzzi, G., Papachristodoulou, A., Goulart, P., &
Wynn, A. (2020). Chordal decomposition in operator-splitting
methods for sparse semideﬁnite programs. Mathematical Pro-
gramming, 180 , 489–532.

Zheng, Y., Kamgarpour, M., Sootla, A., & Papachristodoulou, A.
(2018c). Scalable analysis of linear networked systems via chordal
decomposition. In 2018 European Control Conference (ECC) (pp.
2260–2265). IEEE.

Zheng, Y., Kamgarpour, M., Sootla, A., & Papachristodoulou, A.
(2020). Distributed design for decentralized control using chordal
IEEE Transactions on Control of
decomposition and ADMM.
Network Systems, 7 , 614–626.

Zheng, Y., Mason, R. P., & Papachristodoulou, A. (2018d). Scal-
able design of structured controllers using chordal decomposition.

49

