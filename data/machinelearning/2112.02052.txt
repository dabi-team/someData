TC-GNN: Accelerating Sparse Graph Neural Network
Computation Via Dense Tensor Core on GPUs

Yuke Wang, Boyuan Feng, Yufei Ding
{yuke_wang, boyuan, yufeiding}@cs.ucsb.edu
University of California, Santa Barbara

1
2
0
2

c
e
D
3

]

G
L
.
s
c
[

1
v
2
5
0
2
0
.
2
1
1
2
:
v
i
X
r
a

Abstract
Recently, graph neural networks (GNNs), as the backbone
of graph-based machine learning, demonstrate great success
in various domains (e.g., e-commerce). However, the perfor-
mance of GNNs is usually unsatisfactory due to the highly
sparse and irregular graph-based operations. To this end, we
propose, TC-GNN 1, the first GPU Tensor Core Unit (TCU)
based GNN acceleration framework. The core idea is to rec-
oncile the “Sparse” GNN computation with “Dense” TCU.
Specifically, we conduct an in-depth analysis of the sparse
operations in mainstream GNN computing frameworks. We
introduce a novel sparse graph translation technique to fa-
cilitate TCU processing of sparse GNN workload. We also
implement an effective CUDA core and TCU collaboration
design to fully utilize GPU resources. We fully integrate TC-
GNN with the Pytorch framework for ease of programming.
Rigorous experiments show an average of 1.70× speedup
over the state-of-the-art Deep Graph Library framework
across various GNN models and dataset settings.

1 Introduction
Over the recent years, with the increasing popularity of
graph-based learning, graph neural networks (GNNs) [19,
36, 42] become dominant in the computing of essential tasks
across various domains, including e-commerce, financial
services, etc. Compared with standard methods for graph
analytics, such as random walk [13, 16, 35] and graph lapla-
cians [6, 23, 24], GNNs highlight themselves with signifi-
cantly higher accuracy [19, 39, 42] and better generality [14].
From the computation perspective, GNNs feature an inter-
leaved execution phase of both graph operations (scatter-and-
gather [12]) at the Aggregation phase and Neural Network
(NN) operations (matrix multiplication) at the Update phase.
Our experimental studies further show that the aggregation
phase which involves highly sparse computation on irregular
input graphs generally takes more than 80% running time for
both GNN training and inference. Existing GNN frameworks,
e.g., Deep Graph Library [40] and Pytorch-Geometric [9],
are mostly built upon the popular NN frameworks that are
originally optimized for dense operations, such as general
matrix-matrix multiplication (GEMM). To support sparse
computations in GNNs, their common strategy is to incor-
porate sparse primitives (such as cuSPARSE [26]) for their
backend implementations. However, cuSPARSE leverages
the sparse linear algebra (LA) algorithm which involves lots

1Open-sourced at https://github.com/YukeWang96/TCGNN-Pytorch.git

1

of high-cost indirect memory accesses on non-zero elements
of a sparse matrix. Therefore, cuSPARSE cannot enjoy the
same level of optimizations (e.g., data reuse) as its dense
counterpart, such as cuBLAS [28]. Moreover, cuSPARSE is
designed to only utilize on CUDA core. Therefore, it can-
not benefit from the recent technical advancement on GPU
hardware features, such as Tensor Core Unit (TCU), which
can significantly boost the GPU performance of dense LA
algorithms (e.g., the linear transformation and convolution)
in most conventional deep-learning applications.

This work focuses on exploring the potentials of TCU for
accelerating such GNN-based graph learning. We remark
that making TCU effective for general GNN computing is a
non-trivial task. Our initial study shows that naively apply-
ing the TCU to sparse GNN computation would even result in
inferior performance compared with the existing sparse im-
plementations on CUDA core. There are several challenges.
First, directly resolving the sparse GNN computing problem
with the pure dense GEMM solution is impractical due to
the extremely large memory cost (O (𝑁 2), where 𝑁 is the
number of nodes). Besides, traversing the matrix tiles al-
ready known to be filled with all-zero elements would cause
excessive unnecessary computations and memory access.
Second, simply employing TCU to process non-zero matrix
tiles of the sparse graph adjacency matrix would still waste
most of the TCU computation and memory access efforts.
This is because TCU input matrix tiles are defined with fixed
dimension settings (e.g., Height (16) × Width(8)), whereas
the non-zero elements of a sparse graph adjacency matrix
are distributed irregularly. Thus, it requires intensive zero-
value padding to satisfy such a rigid input constraint. Third,
even though the recent CUDA release update enables TCU
to exploit the benefit of certain types of sparsity [27], it only
supports blocked SpMM, where non-zero elements must be
first fit into well-shaped blocks and the number of blocks
must be the same across different rows. Such a rigid input
restriction makes it hard to handle highly irregular sparse
graphs from real-world GNN applications efficiently.

To this end, we introduce, TC-GNN, the first TCU-based
GNN acceleration design on GPUs. Our key insight is to
let the input sparse graph fit the dense computation of TCU.
At the input level, instead of exhaustively traversing all
sparse matrix tiles and determine whether to process each
tile, we develop a new sparse graph translation (SGT) tech-
nique that can effectively identify those non-zero tiles and
condense non-zero elements from these tiles into a fewer

 
 
 
 
 
 
, ,

Figure 1. GNN General Computation Flow.

number of “dense” tiles. Our major observation is that neigh-
bor sharing is very common among nodes in real-world
graphs. Therefore, applying SGT can effectively merge the
unnecessary data loading of the shared neighbors among
different nodes to avoid high-cost memory access. Our SGT
is generally applicable towards any kind of sparse pattern of
input graphs and can always yield the correct results as the
original sparse algorithm. At the kernel level, for efficiently
processing GNN sparse workloads, TC-GNN exploits the
benefits of CUDA core and TCU collaboration. The major
design idea is that the CUDA core which is more excel at fine-
grained thread-level execution would be a good candidate
for managing memory-intensive data access. While TCU
which is more powerful in handling simple arithmetic oper-
ations (e.g., multiplication and addition) can be well-suited
for compute-intensive GEMM on dense tiles generated from
SGT. At the framework level, we integrate TC-GNN with
the popular Pytorch [34] framework. Thereby, users only
need to interact with their familiar Pytorch programming
environment by using TC-GNN APIs. This can significantly
reduce extra learning efforts meanwhile improving user pro-
ductivity and code portability across different platforms.
To sum, we summarize our contributions as follows:

• We conduct a detailed analysis (§3) of several existing
solutions (e.g., SpMM on CUDA core) and identify the
potentials of using TCU for accelerating the sparse
GNN workloads.

• We introduce a sparse graph translation technique (§4).
It can make the sparse and irregular GNN input graphs
easily fit the dense computing of TCU for acceleration.
• We build a TCU-tailored GPU kernel with effective
CUDA core and TCU collaboration (§5). It consists of
a novel two-level workload mapping strategy for com-
putation optimization and a TCU-optimized dataflow
design for memory access optimization.

• We deliver an end-to-end GNN framework design with
seamless integration with the popular Pytorch frame-
work for high programmability and configurability.
• Extensive experiments show the significant speedup
(on average 1.70×) over the state-of-the-art GNN com-
puting framework, Deep Graph Library, across various
mainstream GNN models and dataset settings.

2 Background and Related Work
2.1 Graph Neural Networks
Graph neural networks (GNNs) are an effective tool for
graph-based machine learning. The detailed computing flow

2

Figure 2. SpMM-like and SDDMM-like Operation in GNNs.
Note that “→” indicates loading data; “⊕” indicates neighbor
embedding accumulation.

𝑣

)

)

(1)

𝑢 |𝑢 ∈ N(𝑣) ∪ ℎ (𝑘)
𝑣

= 𝑨𝒈𝒈𝒓 𝒆𝒈𝒂𝒕𝒆 (𝑘+1) (ℎ (𝑘)
= 𝑼 𝒑𝒅𝒂𝒕𝒆 (𝑘+1) (𝑎 (𝑘+1)

of GNNs is illustrated in Figure 1. GNNs basically compute
the node feature vector (embedding) for node 𝑣 at layer 𝑘 + 1
based on the embedding information at layer 𝑘 (𝑘 ≥ 0), as
shown in Equation 1,
𝑎 (𝑘+1)
𝑣
ℎ (𝑘+1)
𝑣
where ℎ (𝑘)
is the embedding vector for node 𝑣 at layer 𝑘;
𝑣
𝑎 (𝑘+1)
is the aggregation results through collecting neigh-
𝑣
bors’ information (e.g., node embeddings); N(𝑣) is the neigh-
bor set of node 𝑣. The aggregation method and the order of
aggregation and update could vary across different GNNs.
Some methods [14, 19] just rely on the neighboring nodes
while others [39] also leverage the edge properties that are
computed by applying vector dot-product between source
and destination node embeddings. The update function is
generally composed of standard NN operations, such as a sin-
gle fully connected layer or a multi-layer perceptron (MLP)
in the form of 𝑤 · 𝑎 (𝑘+1)
+ 𝑏, where 𝑤 and 𝑏 are the weight
and bias parameter, respectively. The common choices for
node embedding dimensions are 16, 64, and 128, and the
embedding dimension may change across different layers.
After several iterations of aggregation and update (i.e., sev-
eral GNN layers), we will get the output feature embedding
of each node, which can usually be used for various down-
stream graph-based deep learning tasks, such as node classi-
fication [7, 11, 17] and link prediction [5, 20, 38].

𝑣

The sparse computing in the aggregation phase is gener-
ally formalized as the sparse-matrix dense-matrix multiplica-
tion (SpMM), as illustrated in Figure 2(a), and is handled by
many sparse libraries (e.g., cuSPARSE [26]) in many state-of-
the-art GNN frameworks [40, 41]. These designs only count
on GPU CUDA cores for computing, which waste the mod-
ern GPUs with diverse computing units, such as the Tensor
Core Unit (TCU). Specifically, we formalized the neighbor
aggregation as SpMM-like operations (Equation 2)

ˆX = (F𝑁 ×𝑁 ⊙ A𝑁 ×𝑁 ) · 𝑋𝑁 ×𝐷 )
where A is the graph adjacency matrix stored in CSR format.
X is a node feature embedding matrix stored in dense format.
𝑁 is the number of nodes in the graph, and 𝐷 is the size of
node feature embedding dimension; ⊙ is the elementwise

(2)

...............GNN-Layer-1ReLUSoftmax12NeighborAggregationNodeUpdate.........12NeighborAggregationNodeUpdateGNN-Layer-2ReLUGNN-Layer-N013240123423401Updated Node Embedding MatrixInitial Node Embedding Matrix+++++Updated Node Embedding Matrix (a) SpMM-likeNeighbor AggregationDimensionDimension (b) SDDMM-likeEdge Feature ComputationInitial Node Embedding Matrix0123423401DimensionDimensionTC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs

, ,

Figure 3. A Subcore of GPU SM with TCU.

multiplication and · is the standard matrix-matrix multipli-
cation; F is the edge feature matrix in CSR format and can
be computed by SDDMM-like operations (Equation 3), as
illustrated in Figure 2(b).

𝑇
𝑁 ×𝐷 ) ⊙ A𝑁 ×𝑁
F = (X𝑁 ×𝐷 · X

(3)

Note that the computation of 𝐹 is optional in GNNs, which
is generally adopted by Attention-based Graph Neural Net-
work in Pytorch [36] for identifying more complicated graph
structural information. Other GNNs, such as Graph Convolu-
tional Network [19], Graph Isomorphism Network [42], only
use the graph adjacency matrix for neighbor aggregation.

2.2 GPU Tensor Core
In the most recent GPU architectures (since Volta [31]),
NVIDIA announced a new type of computing unit, Tensor
Core Unit (TCU), for accelerating dense deep-learning opera-
tions (e.g., Dense GEMM). A GPU Streaming-Multiprocessor
(w/ TCU) is illustrated in Figure 3. Note that FP64, FP32,
INT, and SFU are for double-precision, single-precision, inte-
ger, and special function units, respectively. Different from
scalar computation on CUDA Cores, TCU provides tile-based
matrix-matrix computation primitives on register fragments,
which can deliver more than 10× throughput improvement.
In particular, TCU supports the compute primitive of D =
A × B + C, where A and B are required to be a certain type
of precision (e.g., half, TF-32), while C and D are stored in
FP32. Depending on the data precision and GPU architec-
ture version, the matrix size (MMA shape) of A(𝑀 × 𝐾),
B(𝐾 × 𝑁 ), and C(𝑀 × 𝑁 ) should follow some principles [29].
For example, TF-32 TCU computing requires 𝑀 = 𝑁 = 16
and 𝐾 = 8. In the recent CUDA release (>=11.0) on Ampere
(𝑠𝑚>=80), TF-32 serves as a good alternative of float/double
on TCU-based GPU computing for modern deep-learning
applications, according to NVIDIA’s in-depth studies [32].
TCU can be utilized in several ways. The simplest way is
to call cuBLAS [28] by using the cublasSgemmEX API. The
second way is to call the Warp Matrix Multiply-Accumulate
(WMMA) (nvcuda::wmma) API [33] in CUDA C++ to oper-
ate TCU directly. There are four major types of operations
(Listing 1). wmma::fragment defines the input matrix tile for
TCU computation. Each fragment consists of thread-local
registers from a warp of threads. wmma::load_matrix_sync
loads the input matrix tiles from global/shared memory to
register fragments. wmma::mma_sync executes the matrix
multiplication on loaded matrix tiles in register fragments.
Finally, wmma::store_matrix_sync moves the results from
registers to global/shared memory.

Listing 1. WMMA APIs for TCU in CUDA C.

1 wmma::fragment<matrix_a, M, N, K, tf32, row_major> a_frag;
2 wmma::load_matrix_sync(a_frag, A, M);
3 wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
4 wmma::store_matrix_sync(C, c_frag, N, mem_row_major);

Table 1. Profiling of GCN Sparse Operations.

Dataset Aggr. (%) Update (%) Cache(%) Occ.(%)

Cora
Citeseer
Pubmed

88.56
86.52
94.39

11.44
13.47
5.55

37.22
38.18
37.22

15.06
15.19
16.24

Since the appearance of the TCU, research efforts have
been devoted to accelerating high-performance computing
workloads with TCU. Ahmad et al. [2] process the batched
small-size GEMM on TCU for acceleration. Boyuan et al. [8]
introduce GEMM-based scientific computing on TCU with
extended precision and high performance. These prior efforts
mostly use the TCU in the dense applications that TCU is
initially designed for, while TC-GNN jumps out of the scope
defined by TCU designer by accelerating the sparse GNN
operations using TCU.

3 Motivation
In this section, we will discuss the major technical thrust for
us to leverage TCU for accelerating sparse GNN computation.
We use the optimization of SpMM as the major example in
this discussion, and the acceleration of SDDMM would also
benefit from similar optimization principles. We first char-
acterize the existing GNN computation solutions, including
SpMM on CUDA core, Dense GEMM on CUDA core/TCU, and
a hybrid Sparse-Dense solution. Then we give insights based
on pros/cons analysis and our motivation.

3.1 SpMM on CUDA core
As the major components of sparse linear algebra opera-
tion, SpMM has been incorporated in many off-the-shelf
libraries [1, 3, 4, 15, 26]. GE-SpMM [15] accelerates GNN
computations on GPU through a hand-optimized CUDA ker-
nel with coalesced row caching and coarse-grained warp
merging to improve the computation parallelism and mem-
ory performance. The close-sourced cuSPARSE [26] library
developed by NVIDIA is the most popular solution and it
can deliver state-of-the-art performance for most GPU-based
SpMM computation. cuSPARSE has also been widely adopted
by the many GNN computing framework, such as Deep
Graph Library (DGL) [40], as the backend for the sparse
neighbor aggregation operations. To understand its char-
acters, we profile DGL on one layer of a GCN [19] model
(neighbor aggregation + node update) on NVIDIA RTX3090.
We report two key kernel matrices for only neighbor aggre-
gation kernel, including L1/texture cache hit rate (𝐶𝑎𝑐ℎ𝑒)
and the achieved Streaming-Multiprocessor (SM) occupancy
(𝑂𝑐𝑐.). We select three representative GNN datasets: Cora
with 3,327 nodes, 9,464 edges, and 3,703 node embedding
dimensions; Citeseer with 2,708 nodes, 10,858 edges, and
1,433 dimensions; Pubmed with 19,717 nodes, 88,676 edges,

3

+FP64INTFP32TCURegister FilesWarp SchedulerL0 CacheLD/STSFUDispatch UnitACBMKKNDMN=SM Subcore, ,

Table 2. Comparison among Sparse GEMM, Dense GEMM, Hybrid Sparse-Dense, and TC-GNN.

Solution

Mem. Consumption Effective Mem. Access Computation Intensity Effective Computation

Sparse GEMM
Dense GEMM
Hyxbrid Sparse-Dense
TC-GNN

Low
High
High
Low

Low
High
Low
High

Low
High
Low
High

High
Low
High
High

Table 3. Medium-size Graphs in GNNs.

Dataset

# Nodes

# Edges Memory

Eff.Comp

OVCR-8H 1,890,931
1,714,644
Yeast
DD
334,925

3,946,402
3,636,546
1,686,092

14302.48 GB
11760.02 GB
448.70 GB

0.36%
0.32%
0.03%

and 500 dimensions. From Table 1, we have several obser-
vations: First, the aggregation phase usually dominates the
overall execution of the GNN execution. From these three
commonly used GNN datasets, we can see that the aggre-
gation phase usually takes more than 80% of the overall
execution time, which demonstrates the key performance
bottleneck of the GNNs is to improve the performance of
the sparse neighbor aggregation. Second, sparse operations
in GNNs show very low memory performance. The column
Cache of Table 1 shows GNN sparse operations could not
well benefit from the GPU cache system, thus, showing a
low cache-hit ratio (around 37%) and frequent global mem-
ory access. Third, sparse operations of GNNs show very
inefficient computation. As described in the column Occu-
pancy of Table 1, sparse operations of GNNs could hardly
keep the GPU busy because 1) its low computation intensity
(the number of non-zero elements in the sparse matrix is
generally small); 2) its highly irregular memory access for
fetching rows of the dense matrix during the computation,
resulting in memory-bound computation; 3) it currently can
only leverage CUDA core for computation, which naturally
has limited throughput performance. On the other side, this
study also points out several potential directions of improv-
ing the SpMM performance on GPUs, such as improving the
computation intensity (e.g., assigning more workload to each
thread/warp/block), boosting memory access efficiency (e.g.,
crafting specialized memory layout for coalesced memory
access), and breaking the computation performance ceiling
(e.g., using TCU).

3.2 Dense GEMM on CUDA Core/TCU
While the Dense GEMM is mainly utilized for dense NN com-
putations (e.g., linear transformation and convolution), it can
also be leveraged for GNN aggregation under some circum-
stances. For example, when an input graph has a very limited
number of nodes, we can directly use the dense adjacency
matrix of the graph and accelerate the intrinsically sparse
neighbor aggregation computation on CUDA core/TCU by
calling cuBLAS [28]. However, such an assumption may not
hold even for medium-size graphs in real-world GNN appli-
cations. As shown in Table 3, for these selected datasets, the
memory consumption of their dense graph adjacent matrix
(as a 2D float array) would easily exceed the memory con-
straint of today’s GPU (<100GB). Even if we assume the dense

4

adjacent matrix can fit into the GPU memory, the extremely
low effective computations (last column of Table 3) would
also be a major obstacle for us to achieve high performance.
We measure the effective computation as 𝑛𝑛𝑧
𝑁 ×𝑁 , where 𝑛𝑛𝑧
is the number of the non-zero elements (indicating edges)
in the graph adjacent matrix and 𝑁 is the number of nodes
in the graph. The number of 𝑛𝑛𝑧 is tiny in comparison with
the 𝑁 × 𝑁 . Therefore, computations and memory access on
zero elements are wasted.

3.3 Hybrid Sparse-Dense Solution
Another type of work [21, 27] takes the path of mixing the
sparse control (tile-based iteration) with Dense GEMM com-
putation. They first apply a convolution-like (2D sliding win-
dow) operation on the adjacent matrix and traverse all pos-
sible dense tiles that contain non-zero elements. Then, for
all identified non-zero tiles, they invoke GEMM on CUDA
Core/TCU for computation. However, this strategy has two
shortcomings. First, sparse control itself would cause high
overhead. Based on our empirical study, the non-zero ele-
ments are highly scattered on the adjacent matrix of a sparse
graph. Therefore, traversing all blocks in a super large adja-
cent matrix would be time-consuming. Second, the identified
sparse tiles would still waste lots of computations. The ir-
regular edge connections of the real-world graphs could
hardly fit into these fixed-shape tile frames. Therefore, most
of the dense tiles would still have very low occupation (few
non-zero elements in each tile).

Inspired by the above studies, we make several key design
choices in order to achieve high-performance sparse GNN
operations. 1) At the algorithm level, we choose the hybrid
sparse-dense solution as the starting point. This can give us
more flexibility for optimizations at the sparse control (e.g.,
traversing fewer tiles) and dense computation (e.g., increas-
ing the effective computation/memory access when process-
ing each tile), 2) At the GPU kernel level, we employ the
shared memory as the key space for GPU kernel-level data
management. It can help us to re-organize the irregular GNN
input data in a more “regularized” way such that both the
memory access efficiency and computing performance can
be well improved. 3) At the hardware level, we choose TCU
as our major computing unit since it can bring significantly
higher computing throughput performance in comparison
with CUDA Core. This also indicates the great potential of us-
ing TCU for harvesting more performance gains. Finally, we
crystallize our idea into TC-GNN that effectively coordinates
the execution of GNN sparse operations on dense TCU. We
show a brief qualitative comparison among TC-GNN and the
above three solutions in Table 2 and we justify these benefits

TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs

, ,

Figure 4. Sparse Graph Translation. Note that the grey-colored area indicates the TCU blocks that will be directly skipped.

Algorithm 1: TCU-aware Sparse Graph Translation.

Algorithm 2: TC-GNN Neighbor Aggregation.

input : Graph adjacent matrix A (nodePointer, edgeList).
output : Result of winPartition and edgeToCol.
/* Compute the total number of row windows.

*/

1 numRowWin = ceil(numNodes/winSize);
2 for 𝑤𝑖𝑛𝐼𝑑 in 𝑛𝑢𝑚𝑅𝑜𝑤𝑊 𝑖𝑛 do

*/

/* EdgeIndex range of the current rowWindow.
𝑤𝑖𝑛𝑆𝑡𝑎𝑟𝑡 = 𝑛𝑜𝑑𝑒𝑃𝑜𝑖𝑛𝑡𝑒𝑟 [𝑤𝑖𝑛𝐼𝑑 ∗ 𝑤𝑖𝑛𝑆𝑖𝑧𝑒 ];
𝑤𝑖𝑛𝐸𝑛𝑑 = 𝑛𝑜𝑑𝑒𝑃𝑜𝑖𝑛𝑡𝑒𝑟 [ (𝑤𝑖𝑛𝐼𝑑 + 1) ∗ 𝑤𝑖𝑛𝑆𝑖𝑧𝑒 ];
/* Sort the edges of the current rowWindow.
𝑒𝐴𝑟𝑟𝑎𝑦 = Sort(𝑤𝑖𝑛𝑆𝑡𝑎𝑟𝑡 , 𝑤𝑖𝑛𝐸𝑛𝑑, 𝑒𝑑𝑔𝑒𝐿𝑖𝑠𝑡 );
/* Deduplicate edges of the current rowWindow. */
𝑒𝐴𝑟𝑟𝐶𝑙𝑒𝑎𝑛 = Deduplication(𝑛𝐼𝑑𝐴𝑟𝑟𝑎𝑦);
/* #TC blocks in the current rowWindow.
𝑤𝑖𝑛𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 [𝑤𝑖𝑛𝐼𝑑 ] = ceil(𝑒𝐴𝑟𝑟𝐶𝑙𝑒𝑎𝑛.𝑠𝑖𝑧𝑒/𝑇𝐶_𝐵𝐿𝐾 _𝑤);
/* Edges-to-columnID mapping in TC Blocks.
for 𝑒𝐼𝑛𝑑𝑒𝑥 in [winStart, winEnd] do
𝑒𝑖𝑑 = 𝑒𝑑𝑔𝑒𝐿𝑖𝑠𝑡 [𝑒𝐼𝑛𝑑𝑒𝑥 ];
𝑒𝑑𝑔𝑒𝑇 𝑜𝐶𝑜𝑙 [𝑒𝐼𝑛𝑑𝑒𝑥 ] = 𝑒𝐴𝑟𝑟𝐶𝑙𝑒𝑎𝑛 [𝑒𝑖𝑑 ];

*/

*/

*/

3

4

5

6

7

8

9

10

end

11
12 end

through a detailed discussion of TC-GNN in the next two
sections. Note that Memory Consumption is the size of mem-
ory used by the sparse/dense graph adjacency matrix; The
Effective Memory Access is the ratio between the size of the ac-
cessed data that is actually involved in the later computation
and the total size of data being accessed; The Computation
Intensity is the ratio of the number of computing operations
versus the data being accessed; The Effective Computation
is the ratio between the operations for generating the final
result and the total operations.

4 TC-GNN
We will detail TC-GNN, including three algorithmic designs:
Sparse Graph Translation, Sparse (SpMM-like) Neighbor Ag-
gregation, and Sparse (SDDMM-like) Edge Feature Computing.

4.1 TCU-Aware Sparse Graph Translation
As the major component of TC-GNN, we propose a novel
Sparse Graph Translation (SGT) technique to facilitate the
TCU acceleration of GNNs. Our core idea is that the pattern
of the graph sparsity can be well-tuned for TCU computation
through effective graph structural manipulation meanwhile
guaranteeing the output correctness. Specifically, we condense
the highly-scattered neighbor ids without losing key infor-
mation (e.g., edge connections). As exemplified in Figure 4(a)

5

input : Condensed graph structural information (𝑛𝑜𝑑𝑒𝑃𝑜𝑖𝑛𝑡𝑒𝑟 , 𝑒𝑑𝑔𝑒𝐿𝑖𝑠𝑡 ,

𝑒𝑑𝑔𝑒𝑇 𝑜𝐶𝑜𝑙, 𝑤𝑖𝑛𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛) and node embedding matrix (X).

output : Updated node embedding matrix ( ˆX).
/* Traverse through all row windows.

1 for 𝑤𝑖𝑛𝐼𝑑 in 𝑛𝑢𝑚𝑅𝑜𝑤𝑊 𝑖𝑛𝑑𝑜𝑤𝑠 do

/* #TC blocks of the row window.
𝑛𝑢𝑚𝑇𝐶𝑏𝑙𝑜𝑐𝑘𝑠 = 𝑤𝑖𝑛𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 [𝑤𝑖𝑛𝐼𝑑 ] ;
/* Edge range of TC blocks of the row window.
𝑒𝑑𝑔𝑒𝑅𝑎𝑛 = GetEdgeRange(𝑛𝑜𝑑𝑒𝑃𝑜𝑖𝑛𝑡𝑒𝑟 , 𝑤𝑖𝑛𝐼𝑑);
for 𝑇𝐶𝑏𝑙𝑘𝐼𝑑 in 𝑛𝑢𝑚𝑇𝐶𝑏𝑙𝑜𝑐𝑘𝑠 do

*/

*/

*/

*/

/* The edgeList chunk in current TC block. */
𝑒𝑑𝑔𝑒𝐶ℎ𝑢𝑛𝑘 = GetChunk(𝑒𝑑𝑔𝑒𝐿𝑖𝑠𝑡 , 𝑒𝑑𝑔𝑒𝑅𝑎𝑛, 𝑇𝐶𝑏𝑙𝑘𝐼𝑑);
/* Neighbor node Ids in current TC block.
𝑐𝑜𝑙𝑇 𝑜𝑁 𝐼𝑑 = GetNeighbors(𝑒𝑑𝑔𝑒𝐶ℎ𝑢𝑛𝑘, 𝑒𝑑𝑔𝑒𝑇 𝑜𝐶𝑜𝑙 );
/* Initiate a dense tile (𝐴𝑇 𝑖𝑙𝑒).
𝐴𝑇 𝑖𝑙𝑒 = InitSparse(𝑒𝑑𝑔𝑒𝐶ℎ𝑢𝑛𝑘, 𝑤𝑖𝑛𝐼𝑑);
/* Initiate a dense tile (𝑋𝑇 𝑖𝑙𝑒).
𝑋𝑇 𝑖𝑙𝑒, 𝑐𝑜𝑙𝐼𝑑 = FetchDense(𝑐𝑜𝑙𝑇 𝑜𝑁 𝐼𝑑, 𝑋 );
/* Compute 𝑋𝑛𝑒𝑤𝑇 𝑖𝑙𝑒 via TCU GEMM.
𝑋𝑛𝑒𝑤𝑇 𝑖𝑙𝑒 = TCcompute(𝐴𝑇 𝑖𝑙𝑒, 𝑋𝑇 𝑖𝑙𝑒);
/* Store 𝑋𝑛𝑒𝑤𝑇 𝑖𝑙𝑒 of ˆ𝑋 .
ˆ𝑋 = StoreDense(𝑋 𝑁 𝑒𝑤𝑇 𝑖𝑙𝑒, 𝑤𝑖𝑛𝐼𝑑, 𝑐𝑜𝑙𝐼𝑑);

*/

*/

*/

*/

2

3

4

5

6

7

8

9

10

11
12 end

end

and (b), we take the regular graph in CSR format as the in-
put and condense the columns of each row window (in the
red-colored rectangular box) to build TCU blocks (𝑇𝐶_𝑏𝑙𝑜𝑐𝑘)
(a.k.a., the input operand shape of a single MMA instruc-
tion), in the orange-colored rectangular box. nodePointer is
the row pointer array edgeList is the edges of each node
stored continuously. In this paper, we demonstrate the use
of standard MMA shape for TF-32 of TCU on Ampere GPU
architecture, and other MMA shapes [29] can also be used
if different computation precision (e.g., half and int8) and
GPU architecture (e.g., Turing) are specified.

Our sparse graph translation scheme takes several steps
for processing each row window, as detailed in Algorithm 1
and visualized in Figure 4(c). Note that winPartition is an
array for maintaining the number of TC blocks in each row
window. edgeToCol is an array for maintaining the map-
ping between the edges and their corresponding position
in the graph after SGT. We choose the size of the row win-
dow (𝑤𝑖𝑛𝑆𝑖𝑧𝑒=TC_BLK_H ) and column width (TC_BLK_W )
according to TCU MMA specification (e.g., TC_BLK_H =16,
TC_BLK_W =8 in TF-32). After condensing the graph within
each row window, the time complexity of sliding the𝑇𝐶_𝑏𝑙𝑜𝑐𝑘

(c)(a)(b)Condensed  Sparse Matrix  TC-aware Sparse Graph TranslationTC_BLK_H2814170715271751017012345nodePointeredgeListSort Edges2814170715271751017sorted edgeListDeduplication2814170157510edgeToCol(a.k.a colToRow)146807325Column IDs in TC blockTC_BLK_WTC_BLK_WSparse Matrix Row WindowTC Block, ,

Algorithm 3: TC-GNN Edge Feature Computation.

input : Condensed graph structural information (𝑛𝑜𝑑𝑒𝑃𝑜𝑖𝑛𝑡𝑒𝑟 , 𝑒𝑑𝑔𝑒𝐿𝑖𝑠𝑡 ,

𝑒𝑑𝑔𝑒𝑇 𝑜𝐶𝑜𝑙, 𝑤𝑖𝑛𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛) and node embedding matrix (X).

output : Edge Feature List (𝑒𝑑𝑔𝑒𝑉 𝑎𝑙𝐿𝑖𝑠𝑡 ).
/* Traverse through all row windows.

1 for 𝑤𝑖𝑛𝐼𝑑 in 𝑛𝑢𝑚𝑅𝑜𝑤𝑊 𝑖𝑛 do

/* #TC blocks in the row window.
𝑛𝑢𝑚𝑇𝐶𝑏𝑙𝑜𝑐𝑘𝑠 = 𝑤𝑖𝑛𝑃𝑎𝑟𝑡𝑖𝑡𝑖𝑜𝑛 [𝑤𝑖𝑛𝐼𝑑 ] ;
/* Edge range of TC blocks of the row window.
𝑒𝑑𝑔𝑒𝑅𝑎𝑛 = GetEdgeRange(𝑛𝑜𝑑𝑒𝑃𝑜𝑖𝑛𝑡𝑒𝑟 , 𝑤𝑖𝑛𝐼𝑑);
for 𝑇𝐶𝑏𝑙𝑘𝐼𝑑 in 𝑛𝑢𝑚𝑇𝐶𝑏𝑙𝑜𝑐𝑘𝑠 do

/* EdgeList chunk in current TC block.
𝑒𝑑𝑔𝑒𝐶ℎ𝑢𝑛𝑘 = GetChunk(𝑒𝑑𝑔𝑒𝐿𝑖𝑠𝑡 , 𝑒𝑑𝑔𝑒𝑅𝑎𝑛, 𝑇𝐶𝑏𝑙𝑘𝐼𝑑);
/* Neighbor node Ids in current TC block.
𝑐𝑜𝑙𝑇 𝑜𝑁 𝐼𝑑 = GetNeighbors(𝑒𝑑𝑔𝑒𝐶ℎ𝑢𝑛𝑘, 𝑒𝑑𝑔𝑒𝑇 𝑜𝐶𝑜𝑙);
/* Fetch a dense tile (𝑋𝑇 𝑖𝑙𝑒𝐴).
𝑋𝑇 𝑖𝑙𝑒𝐴 = FetchDenseRow(𝑤𝑖𝑛𝐼𝑑, 𝑇𝐶𝑏𝑙𝑘𝐼𝑑, 𝑋 );
/* Fetch a dense tile (𝑋𝑇 𝑖𝑙𝑒𝐵 ).
𝑋𝑇 𝑖𝑙𝑒𝐵 = FetchDenseCol(𝑐𝑜𝑙𝑇 𝑜𝑁 𝐼𝑑, 𝑒𝑑𝑔𝑒𝑇 𝑜𝐶𝑜𝑙, 𝑋 );
/* Compute 𝑒𝑑𝑔𝑒𝑉 𝑎𝑙𝑇 𝑖𝑙𝑒 via TCU GEMM.
𝑒𝑑𝑔𝑒𝑉 𝑎𝑙𝑇 𝑖𝑙𝑒 = TCcompute(𝑋𝑇 𝑖𝑙𝑒𝐴, 𝑋𝑇 𝑖𝑙𝑒𝐵 );
/* Store 𝑒𝑑𝑔𝑒𝑉 𝑎𝑙𝑇 𝑖𝑙𝑒 to 𝑒𝑑𝑔𝑒𝑉 𝑎𝑙𝐿𝑖𝑠𝑡 .
StoreSparse(𝑒𝑑𝑔𝑒𝑉 𝑎𝑙𝐿𝑖𝑠𝑡 , 𝑒𝑑𝑔𝑒𝑉 𝑎𝑙𝑇 𝑖𝑙𝑒,

𝑒𝑑𝑔𝑒𝐿𝑖𝑠𝑡 , 𝑒𝑑𝑔𝑒𝑇 𝑜𝐶𝑜𝑙 );

2

3

4

5

6

7

8

9

10

11

end

12
13 end

*/

*/

*/

*/

*/

*/

*/

*/

*/

𝑁

𝑇𝐶_𝐵𝐿𝐾_𝑊 ) to only O (

𝑛𝑛𝑧𝑢𝑛𝑖𝑞𝑢𝑒
can be reduced from O (
𝑇𝐶_𝐵𝐿𝐾_𝑊 ),
where 𝑁 is the total number of nodes in the graph and
𝑛𝑛𝑧𝑢𝑛𝑖𝑞𝑢𝑒 is the size of the unique neighbor within the cur-
rent row window, which equals 𝑒𝐴𝑟𝑟𝐶𝑙𝑒𝑎𝑛.𝑠𝑖𝑧𝑒 in Algo-
rithm 1. Besides, the density (computation intensity) of each
identified TCU block can be largely improved. Considering
the case in Figure 4, after the sparse graph translation, we
can achieve 2× higher density on individual TCU blocks
(Figure 4(b)) compared with the original one (Figure 4(a)).
Note that SGT is applicable for both the SpMM and SDDMM
in GNN sparse operations, and it can be easily parallelized
because the processing of individual row windows is inde-
pendent of each other. Besides, the sparse graph translation
only needs to execute once and its result can be reused across
many epochs/rounds of GNN training/inference.

4.2 TCU-tailored GNN Computation

Neighbor Aggregation The major part of GNN sparse
computing is the neighbor aggregation, which can generally
be formalized as SpMM operations by many state-of-the-art
frameworks [40]. And they employ the cuSPARSE [26] on
CUDA core as a black-box technique for supporting sparse
GNN computation. In contrast, our TC-GNN design targets
at TCU for the major neighbor aggregation computation
which demands a specialized algorithmic design. TC-GNN fo-
cuses on maximizing the net performance gains by grace-
fully batching the originally highly irregular SpMM as dense
GEMM computation and solving it on TCU effectively. As
illustrated in Algorithm 2, the node aggregation processes
all TC blocks from each row window. nodePointer and edge-
List are directly from graph CSR, while edgeToCol and win-
Partition are generated from SGT discussed in the previous
section. Note that InitSparse is to initialize a sparse tile in
dense format according to the translated graph structure

6

of the current TC block. Meanwhile, FetchDense returns a
dense node embedding matrix tile 𝑋𝑇𝑖𝑙𝑒 for TCU computa-
tion, and the corresponding column range 𝑐𝑜𝑙𝐼𝑑 (embedding
dimension range) of matrix X. This is to handle the case that
the width of one 𝑋𝑇𝑖𝑙𝑒 could not cover the full-width (all
dimensions) of X. Therefore, the 𝑐𝑜𝑙𝐼𝑑 will be used to put
the current TCU computation output to the correct location
in the updated node embedding matrix ˆX.

Edge Feature Computing Previous research efforts [36,
39] have demonstrated the great importance of incorporat-
ing the edge feature for a better GNN model algorithmic
performance (e.g., accuracy, and F1-score). The underlying
building block to generate edge features is the Sparse Dense-
Dense Matrix Multiplication (SDDMM)-like operation. In
TC-GNN, we support SDDMM with the collaboration of the
above sparse graph translation and TCU-tailored algorithm
design, as described in Algorithm 3. The overall algorithm
structure and inputs are similar to the above neighbor ag-
gregation. The major difference is the output. In the case
of neighbor aggregation, our output is the updated dense
node embedding matrix ( ˆX), where edge feature computing
will generate a sparse output with the same shape as the
graph edge lists. Note that fetching the 𝑋𝑇𝑖𝑙𝑒𝐴 only needs to
consecutively access the node embedding matrix A by rows
while fetching the 𝑋𝑇𝑖𝑙𝑒𝐵 requires first computing the TCU
block column-id to node-id (𝑐𝑜𝑙𝑇𝑜𝑁 𝐼𝑑) to fetch the corre-
sponding neighbor node embeddings from the same node
embedding matrix X.

5 Implementation
We will detail TC-GNN by mapping the above algorithmic
design to low-level primitives (e.g., warp/block) and shared
memory layout. We discuss two key techniques: two-level
workload mapping and TCU-optimized dataflow design.

5.1 Two-level Workload Mapping
Different from previous work [9, 40] focusing on CUDA core
only, TC-GNN highlights itself with CUDA core and TCU
collaboration through effective two-level workload mapping.
The idea is based on the fact that CUDA Cores work in SIMT
fashion and are operated by individual threads, while TCU
designated for GEMM computation requires the collabora-
tion from a warp of threads (32 threads). Our key design
principle is to mix these two types of computing units as
a single GPU kernel, which can efficiently coordinate the
kernel execution at different levels of execution granularity.
In TC-GNN, we operate CUDA cores by thread blocks and
manage TCU by thread warps. Specifically, threads running
CUDA cores from the same thread block will load data (e.g.,
edges) from the global memory to shared memory. Note
that in our design we assign each row window (discussed in
Section 4.1) to one thread block. The number of threads in
each block should be divisible by the number of threads in
each warp (32) for better performance. Once threads running
on CUDA core (CUDA-core threads) finish the data loading,

TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs

, ,

Figure 5. TCU-Optimized Dataflow Design for (a) Neighbor Aggregation and (b) Edge Feature Computing in GNNs.

threads from each warp (TCU threads) will operate TCU for
GEMM computation (including loading the data from the
shared memory to thread-local registers (fragments), apply-
ing GEMM computation on data in registers, accumulating
results on registers, and storing the final results back to
global memory). Note that there would be a large overlap of
the CUDA-core threads and TCU threads, both of which are
threads from the same blocks but running at a different time
frame. In general, we use more CUDA-core threads than TCU
threads considering that global memory access demanding
more parallelization. There are two major benefits of such
a two-level workload mapping strategy. First, threads from
the same block can work together to improve the memory
access parallelization to better utilize memory bandwidth.
Second, warps from the same block can reuse the loaded
data, including the information (e.g., column index mapping)
of the translated graph and the tiles from the dense node
embedding matrix. Therefore, redundant high-cost global
memory operations can largely be avoided.

5.2 TCU-Optimized Dataflow Design
As the major technique to improve the GPU performance,
shared memory is customized for our TCU-based sparse
kernel design for re-organizing data layout for dense TCU
computation and reducing the redundant global memory
traffic. Our design takes the TCU specialty into careful con-
sideration from two aspects, 1) the input matrix tile size of
the TCU, which is M(16)×N(16)×K(8) in case of TF-32, and 2)
the tile fragment layout for fast computation. The common
practice of the loaded tile A and B are stored in row-major
and column-major for better performance. Next, we will
detail our TCU-optimized dataflow design for both neigh-
bor aggregation and edge feature computation. Neighbor
Aggregation As visualized in Figure 5(a) and detailed in
Listing 2, shared memory is mainly used for caching several
most frequently used information, including the tile of sparse
matrix A (sparse_A), the column-id of the sparse matrix A to
row-id of node embedding matrix X (sparse_AToX_index),
and the dense tile of X (dense_X). When handling each TCU
block, we assign all threads from the same block of threads
for loading the sparse tile while allowing several warps to
concurrently load the dense row tile from the matrix 𝑋 . The
reasons for enforcing such caching are two-folds. First, it
can bridge the gap between the sparse graph data and the

Listing 2. Implementation of Neighbor Aggregation.

1 __shared__ float sparse_A[BLK_H * BLK_W];
2 __shared__ unsigned sparse_AToX_index[BLK_H];
3 __shared__ float dense_X[warpPerblock * BLK_W * BLK_H];
4
5 for (i = 0; i < num_TC_blocks; i++){
tid = threadIdx.x; // thread id.
6
wid = tid % 32;
7
// Assigning dummy value for handling corner cases.
8
if (wid == 0 && laneid < BLK_W)
9
10
11
12

// Loading edges and initialize sparse_A.
for (eIdx=n_start+tid; eIdx<n_end; eIdx+=
threadPerBlock){

sparse_AToX_index[laneid] = numNodes + 1;

// warp id.

13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38 }

col = edgeToColumn[eIdx];
// Edges in the current TC_block column frame.

if (i*BLK_W<=col && col<(i+1)*BLK_W){

unsigned row = edgeToRow[eIdx] % BLK_H;
// set the edge of the sparse_A.
sparse_A[row*BLK_W + col%BLK_W] = 1;
// map columns of sparse_A to rows of dense_X.
sparse_AToX_index[col%BLK_W]=edgeList[eIdx];

}

}
// Initialize dense_X by column-major store,
// Threads of a warp for fetching a dense_X.
// each warp identify by wid.
for (j = tid; j < BLK_W*BLK_H; j += warpSize){

// TC_block_col to dense_tile_row.
dense_rowIdx = sparse_AToX_index[j%BLK_W];
// dimIndex of the dense tile.
dense_dimIdx = j / BLK_W;
target_idx = wid * BLK_W*BLK_H + j;
source_idx = dense_rowIdx*embedding_dim \

dense_X[target_idx] = in_mat[source_idx];

+ wid*dimPerWarp + dense_dimIdx;

}
// Call wmma load A_frag, X_frag from shared memory
// Compute and accumulate. Store to X_hat.

dense GEMM computing that requires continuous data lay-
out. For example, the adjacent matrix A is input as CSR
format that cannot be directed feed to TCU GEMM com-
putation, therefore, we use a shared memory sparse_A to
initialize its equivalent dense tile. Similarly, we cache rows
of X according to the columns of A to the row of X mapping
after our sparse graph translation, where originally scattered
columns of A (rows of X) are condensed. Second, it can en-
able the data reuse on sparse_AToX_index and sparse_A.
This is because in general, the BLK_H (16) cannot cover all
dimensions of a node embedding (e.g., 64), multiple warps
will be initiated of the same block to operate TCU in parallel
to working on non-overlapped dense tiles while using the
same sparse tile.

7

Node Embedding in Global MemoryNode Embedding in Global Memory TCUDense_XFragmentA_SparseFragment(a) SpMM-like Neighbor         Aggregation Dataflow.RegistersRegisters012Shared MemoryEdgeConnectionNodeEmbeddingTile_XTCUDense_XFragmentDense_YFragment(b) SDDMM-like Edge FeatureComputation Dataflow.RegistersRegistersShared MemoryNodeEmbeddingTile_Y34012RegistersEdgeFeaturesSharedMemory34012EdgeConnectionNodeEmbeddingTile_XUpdatedTile_XRegisters, ,

Listing 3. Implementation of Edge Feature Computation.

}

sparse_A[idx] = numEdges + 1;

sparse_AToX_index[laneid] = numNodes + 1;
for (idx=tid; idx<BLK_H*BLK_H; idx+=threadPerBlock)

// condensed column id in sparse_A.
col = edgeToColumn[eIdx];
// if the edge in the current TC_block of column.
if (i*BLK_H<=col && col<(i+1)*BLK_H){

// reverse indexing the row Id of the edge.
row = edgeToRow[eIdx] % BLK_H;
// set the edge of the sparse_A.
sparse_A[row*BLK_H + col%BLK_H] = eIdx;
// map sparse_A colId to dense_X rowId.
sparse_AToX_index[col%BLK_H] = edgeList[eIdx];

// Initialize sparse_A by BLK_H (16) threads from
// warp-0. We fetch all neighbors of the current
// nodes, then to see whether it can fit
// into current TC_block frame of column.
for(eIdx=tid+n_start;eIdx<n_end;eIdx+=threadPerBlock){

1 __shared__ float sparse_A[BLK_H*BLK_H];
2 __shared__ unsigned sparse_AToX_index[BLK_H];
3 __shared__ float dense_X[BLK_H*BLK_W];
4 __shared__ float dense_Y[BLK_W*BLK_H];
5
6 // Processing TC_blocks along the column of Sparse A.
7 // The block step here is 2, which is 16 = 8 + 8.
8 // In order to reuse the edgeToColumn in SpMM.
9 num_TC_blocks = (blockPartition[bid]*BLK_W+BLK_H-1)/BLK_H;
10 // dimension iteration for covering all dimension.
11 DimIterations = (embedding_dim+BLK_W-1)/BLK_W;
12
13 // traversing all TC blocks in the current row window.
14 for (i = 0; i < num_TC_blocks; i++){
if (wid == 0 && laneid < BLK_H)
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71 }

}
// Initialize dense_Y by column-major store,
// Threads of a warp for fetching a dense_Y.
for (j=tid; j<BLK_W*BLK_H; j+=threadPerBlock){
dense_rowIdx=sparse_AToX_index[j%BLK_H];
dense_dimIdx = j / BLK_W;
target_idx = j;
source_idx = dense_rowIdx*embedding_dim \

}
__syncthreads();
// Call wmma load dense_X and dense_Y from shared
// memory and do GEMM computation.

dense_rowIdx = j / BLK_W;
dense_dimIdx = j % BLK_W;
target_idx = j;
source_idx = dense_rowIdx*embedding_dim \

}
// traverse all dimension of the same sparse tile.
for (dim_iter=0; dim_iter<DimIterations; dim_iter++){

// Initialize dense_X by row-major store
// Threads of a block for fetching a dense_X.
for (j=tid; j<BLK_H*BLK_W; j+=threadPerBlock){

// boundary check for padding.
if (source_idx >= numNodes*embedding_dim)

// boundary check for padding.
if (source_idx >= numNodes*embedding_dim)

dense_X[target_idx] = in_mat[source_idx];

dense_Y[target_idx] = in_mat[source_idx];

+ dim_iter*BLK_W + dense_dimIdx;

+ dim_iter*BLK_W + dense_dimIdx;

// Store dense result to sparse EdgeValList.

dense_Y[target_idx] = 0;

dense_X[target_idx] = 0;

else

else

}

Edge Feature Computation Similar to the shared mem-
ory design in neighbor aggregation, for edge feature com-
puting, as visualized in Figure 5(b) and detailed in Listing 3
at the next page, the shared memory is utilized for sparse

8

Table 4. Datasets for Evaluation.

Type Dataset
Citeseer
Cora
Pubmed
PPI

I

II

III

PROTEINS_full
OVCAR-8H
Yeast
DD
YeastH

amazon0505
artist
com-amazon
soc-BlogCatalog
amazon0601

#Vertex
3,327
2,708
19,717
56,944

43,471
1,890,931
1,714,644
334,925
3,139,988

410,236
50,515
334,863
88,784
403,394

#Edge Dim.
3703
1433
500
50

9,464
10,858
88,676
818,716

#Class
6
7
3
121

162,088
3,946,402
3,636,546
1,686,092
6,487,230

4,878,875
1,638,396
1,851,744
2,093,195
3,387,388

29
66
74
89
75

96
100
96
128
96

2
2
2
2
2

22
12
22
39
22

tile A sparse_A, the column-id of sparse A to row-id of the
matrix X sparse_AToX_index, and the dense tile dense_X
from the matrix X. We assign all threads from the same
block of threads for loading the sparse tile while allowing
several warps to concurrently load the dense row tile from
the matrix X. Compared with dataflow design in neighbor
aggregation, edge feature computing demonstrates several
differences. First, the sizes of sparse_A are different. In the
neighbor aggregation computation, the sparse matrix A is
used as one operand in the SpMM-like computation, there-
fore, the minimal processing granularity is 16 × 8, while in
edge feature computing by following SDDMM-like opera-
tion, the sparse matrix A is served as the output matrix, thus,
maintaining the minimum processing granularity is 16 × 16.
To reuse the same translated sparse graph as SpMM, we need
to recalculate the total number of TC blocks (Line 9). Second,
iterations along the embedding dimension would be different.
Compared with neighbor aggregation, edge feature comput-
ing requires the result accumulation along the embedding
dimension. Therefore, the result will only be output until all
iterations have finished. In neighbor aggregation, the node
embedding vector is divided among several warps, each of
which will output their aggregation result to non-overlapped
embedding dimension range in parallel. Third, the output
format has changed. Compared with SpMM-like neighbor
aggregation which directly output computing result as an up-
dated dense matrix ˆX, SDDMM-like edge feature computing
requires a sparse format (the same shape as edgeList) out-
put for compatibility with neighbor aggregation and memory
space. Therefore, one more step of dense-to-sparse transla-
tion is employed after the result accumulation.

6 Evaluation

Benchmarks: We choose the two most representative
GNN models widely used by previous work [9, 25, 40] on
node classification tasks, which can cover different types of
aggregation. Specifically, 1) Graph Convolutional Network
(GCN) [19] is one of the most popular GNN model architec-
tures. It is also the key backbone network for many other

TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs

, ,

(b)
Figure 6. Speedup over (a) DGL and (b) PyG, on GCN and AGNN; (c) Speedup over cuSPARSE bSpMM on TCU.

(a)

(c)

GNNs, such as GraphSAGE [14], and differentiable pool-
ing (Diffpool) [43]. Therefore, improving the performance
of GCN will also benefit a broad range of GNNs. For GCN
evaluation, we use the setting: 2 layers with 16 hidden dimen-
sions, which is also the setting from the original paper [19].
2) Attention-based Graph Neural Network (AGNN) [36].
AGNN differs from GCN and GIN in its aggregation func-
tion, which compute edge feature (via embedding vector dot-
product between source and destination vertices) before the
node aggregation. AGNN is also the reference architecture
for many other recent GNNs for better model algorithmic
performance. For AGNN evaluation, we use the setting: 4
layers with 32 hidden dimensions.

Baselines: 1) Deep Graph Library (DGL) [40] is the state-
of-the-art GNN framework on GPUs, which is built with the
high-performance CUDA-core-based cuSPARSE [26] library
as the backend and uses Pytorch [34] as its front-end pro-
gramming interface. DGL significantly outperforms other
existing GNN frameworks [9] over various datasets on many
mainstream GNN model architectures. Therefore, we make
an in-depth comparison with DGL. 2) Pytorch-Geometric
(PyG) [9] is another GNN framework. PyG leverages torch-
scatter [10] library (highly-engineered CUDA-core kernel)
as the backend support, which highlights its performance
on batched small graph settings; 3) Blocked-SpMM [27]
(bSpMM) accelerates SpMM on TCU. It is included in the re-
cent update (March. 2021) on cuSPARSE library (CUDA 11.2).
bSpMM requires the sparse matrix with Blocked-Ellpack for-
mat for computation. Its computation on non-zero blocks
can be seen as the hybrid sparse-dense solution discussed
in Section 3.3. Note that the bSpMM has not been incorpo-
rated in any existing GNN frameworks. We also compare
TC-GNN with tSparse [44] and Triton [37] for non-vendor
developed and highly optimized kernels on TCU.

Datasets, Platforms, and Metrics: We cover three types
of datasets (Table 4), which have been used in previous
GNN-related work [9, 25, 40]. Specifically, Type I graphs
are the typical datasets used by previous GNN algorithm
papers [14, 19, 42]. They are usually small in the number of
nodes and edges, but rich in node embedding information
with high dimensionality. Type II graphs [18] are the popu-
lar benchmark datasets for graph kernels and are selected
as the built-in datasets for PyG [9]. Each dataset consists
of a set of small graphs, which only have intra-graph edge

connections without inter-graph edge connections. Type III
graphs [19, 22] are large in terms of the number of nodes
and edges. These graphs demonstrate high irregularity in
its structures, which are challenging for most of the exist-
ing GNN frameworks. TC-GNN backend is implemented
with C++ and CUDA C, and the front-end is implemented in
Python. Our major evaluation platform is a server with an
8-core 16-thread Intel Xeon Silver 4110 CPU and an NVIDIA
RTX3090 GPU. To measure the performance speedup, we
calculate the averaged latency of 200 end-to-end results.

6.1 Compared with DGL
As shown in Figure 6(a), TC-GNN achieves 1.70× speedup
on average compared to DGL over three types of datasets
across GCN and AGNN model on end-to-end training.

Type I Graphs: The performance improvements against
DGL are significantly higher for GCN (on average 2.23×)
compared to AGNN (on average 1.93×). The major reason is
their different GNN computation patterns. For GCN, it only
consists of a neighbor aggregation phase (SpMM-like opera-
tion) and a node update phase (GEMM operation). Whereas
in the AGNN, the aggregation phase would also require an
additional edge attention value (feature) computation based
on SDDMM-like operations. Compared with SpMM-like op-
erations, edge attention computation in SDDMM is more
sensitive to the irregular sparse graph structure because
of much more intensive computations and memory access.
Thus, the performance improvement is relatively lower.

Type II Graphs: TC-GNN achieves GCN (1.38×) and
AGNN (1.70×) on the Type II graphs. Speedup on Type
II graphs is relatively lower compared with Type I, since
Type II datasets consisting of a set of small graphs with very
dense intra-graph connections but no inter-graph edges. This
leads to a lower benefit from the sparse graph translation
that would show more effectiveness on highly irregular and
sparse graphs. Such a clustered graph structure would also
benefit cuSPARSE due to more efficient memory access, i.e.,
the fewer irregular data fetching from the sparse matrix. In
addition, for AGNN, TC-GNN can still demonstrate evident
performance benefits over the DGL (CUDA core only), which
can mainly contribute to our TCU-based SDDMM-like opera-
tion designs that can fully exploit the power of GPU through
an effective TCU and CUDA core collaboration.

9

0.00.51.01.52.02.53.0CiteseerCoraPubmedPPIPROTEINS_fullOVCAR-8HYeastDDYeastHamazon0505artistcom-amazonsoc-BlogCatalogamazon0601Type IType IIType IISpeedup-GCNSpeedup-AGNN0.00.51.01.52.02.53.03.54.04.5CiteseerCoraPubmedPPIPROTEINS_fullOVCAR-8HYeastDDYeastHamazon0505artistcom-amazonsoc-BlogCatalogamazon0601Type IType IIType IISpeedup-GCNSpeedup-AGNNPyG OOM0.00.51.01.52.02.53.0CiteseerCoraPubmedPPIPROTEINS_fullOVCAR-8HYeastDDYeastHamazon0505artistcom-amazonsoc-BlogCatalogamazon0601Type IType IIType IISpeedup Vs. cuSPARSE, ,

Table 5. Compare with TC-GNN SpMM with tSparse, Triton.

Dataset

tSparse (ms) Triton (ms) TC-GNN (ms)

amazon0505
artist
com-amazon
soc-BlogCatalog
amazon0601

18.60
9.15
13.84
9.74
11.93

31.64
12.86
15.50
14.38
21.78

4.09
3.06
3.26
3.59
3.41

Type III Graphs: The speedup is also evident (on average
1.59× for GCN and on average 1.51× for AGNN) on graphs
with a large number of nodes and edges and irregular graph
structures. The reason is the high overhead global memory
access can be well reduced through our spare graph transla-
tion. Besides, our dimension-split strategy further facilitates
efficient workload sharing among warps through improving
the data spatial/temporal locality. On the dataset artist and
soc-BlogCatalog, which have a higher average degree within
Type III datasets, we notice a better speedup performance
for both GCN and AGNN. This is because 1) more neighbors
per node can lead to the higher density of non-zero elements
within each tile/fragment. Thus, it can fully exploit the com-
putation benefits of each TCU GEMM operation; 2) it can
also facilitate more efficient memory access. For example, in
AGNN, fetching one dense row 𝑥 (node embedding of one
node) from the dense matrix 𝑋 can be reused more times by
applying dot-product between 𝑥 and many columns of (node
embedding of neighbors) the dense matrix 𝑋𝑇 .

6.2 Compared with Other Baselines

Comparison with PyG We further compare TC-GNN with
PyG [9], which is another popular GNN computing frame-
work built on the highly engineered torch-scatter [10] li-
brary running on CUDA core. As shown in Figure 6(b), TC-
GNN can outperform PyG with an average of 1.76× speedup
on GCN and an average of 2.82× speedup on AGNN. For
GCN, TC-GNN achieves significant speedup on datasets with
high-dimensional node embedding, such as Yeast, through ef-
fective TCU acceleration through a TCU-aware sparse graph
translation while reducing the synchronization overhead by
employing our highly parallelized TCU-tailored algorithm
design. PyG, however, achieves inferior performance because
its underlying GPU kernel can only leverage CUDA core,
thus, intrinsically bounded by the CUDA core computing
performance. its kernel implementation heavily relies on
the high-overhead atomic operations for thread-level syn-
chronization, thus, suffering from performance degradation.

Compared with cuSPARSE bSpMM We compare our
TC-GNN SpMM kernel with cuSPARSE bSpMM to demon-
strate the performance advantage of TC-GNN compared with
the state-of-the-art hybrid sparse-dense solution on TCU. Fig-
ure 6(c) shows that TC-GNN can outperform bSpMM with
on average 1.76× speedup on neighbor aggregation. Our SGT
technique can maximize the non-zero density of each non-
zero tile and significantly reduce the number of non-zero tiles
to be processed. However, bSpMM in cuSPARSE has to com-
ply with the strict input sparse pattern (indicated in official

(a)

(b)

Figure 7. (a) SGT effectiveness, and (b) SGT overheads.

API documentation [30]). For example, all rows in the arrays
must have the same number of non-zero blocks. Thus, more
redundant computations (on padding those non-structural
zero blocks) in bSpMM lead to inferior performance.

Compared with tSparse and Triton We compare TC-
GNN SpMM kernel with tSparse [44] and Triton [37] SpMM
on Type III datasets. From Table 5 (Column-2,4), TC-GNN can
outperform tSparse with on average 3.60× speedup on SpMM.
The major reason behind this is that TC-GNN can well reduce
the graph structural-level irregularity through our novel
sparse graph translation scheme to benefit the dense TCU-
based computation. In contrast, tSparse only considers parti-
tioning the input sparse matrix into dense/sparse tiles based
on their non-zeros elements but ignoring the potential of
compressing non-zero elements into fewer tiles to reduce the
workload. As shown in Table 5 (Column-3,4), TC-GNN can
outperform Triton with on average 5.42× speedup on SpMM.
Triton’s block-sparse GEMM for TCU acceleration is de-
signed for block-sparse traditional Neural Networks (focus-
ing on feature maps’ sparsity), which is quite different from
GNNs (focusing on the graph adjacency matrix’s sparsity)
with significant larger sparse matrix size and more irregular
pattern. The real-world graphs are highly irregular in edge
connections, which will be reflected as a highly scattered
distribution of non-zeros elements on adjacency matrices.

6.3 Additional Studies

SGT Effectiveness we conduct a quantitive analysis of
SGT in terms of the total number of TCU blocks between
graphs w/o SGT and the graphs w/ SGT applied. As shown
in Figure 7(a), across all types of datasets, our SGT technique
can significantly reduce the number of traversed TCU blocks
(on average 67.47%). The major reason is that SGT can largely
improve the density of non-zero elements within each TCU
Block. In contrast, the graphs w/o SGT would demonstrate
a large number of highly sparse TCU blocks. What is also
worth noticing is that on Type II graphs, such a reduction
benefit is lower. The reason is that Type II graphs consist of a
set of small subgraphs that only maintain the intra-subgraph
connections, which already maintain dense columns.

SGT Overhead We further evaluate the overhead of our
TC-aware sparse graph translation technique. Here we use
the training for illustration, and the inference in real GNN
application setting would also use the same graph structure
many times [14, 19] while only changing the node embed-
dings input. As shown in Figure 7(b), its overhead is con-
sistently tiny (on average 4.43%) compared with the overall

10

020406080100citeseercorapubmedppiPROTEINS_fullOVCAR-8HYeastDDYeastHamazon0505artistcom-amazonweb-BerkStansoc-BlogCatalogamazon0601Reduction (%)SpMM_16x8 (%)SDDMM_16x16 (%)0%20%40%60%80%100%amazon0505artistcom-amazonsoc-BlogCatalogamazon0601SGTTrainingTC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs

, ,

training time. We thus conclude that such one-time over-
head can be amortized during the GNN computation, which
demonstrates its applicability in real-world GNNs.

7 Conclusion
In this paper, we introduce the first GNN acceleration frame-
work on TCU of GPUs. We design a novel sparse graph
translation technique to gracefully fit the sparse GNN work-
load on dense TCU. Our TCU-tailored GPU kernel design
maximizes the TCU performance gains for GNN comput-
ing through effective CUDA core and TCU collaboration
and a set of memory/data flow optimizations. Our seamless
integration with the Pytorch framework further facilitates
end-to-end GNN computing with high programmability.

References
[1] [n.d.]. Intel Math Kernel Library. Reference Manual. Intel Corporation.

Santa Clara, USA.

[2] A. Abdelfattah, S. Tomov, and J. Dongarra. [n.d.]. Fast Batched Matrix
Multiplication for Small Sizes Using Half-Precision Arithmetic on
GPUs. In 2019 IEEE International Parallel and Distributed Processing
Symposium (IPDPS).

[3] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra,
J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D.
Sorensen. 1999. LAPACK Users’ Guide (third ed.). Society for Industrial
and Applied Mathematics, PA.

[4] Wieb Bosma, John Cannon, and Catherine Playoust. 1997. The Magma
algebra system. I. The user language. J. Symbolic Comput. (1997).
Computational algebra and number theory (London, 1993).

[5] Hsinchun Chen, Xin Li, and Zan Huang. 2005. Link prediction ap-
proach to collaborative filtering. In Proceedings of the 5th ACM/IEEE-CS
Joint Conference on Digital Libraries (JCDL). IEEE.

[6] De Cheng, Yihong Gong, Xiaojun Chang, Weiwei Shi, Alexander
Hauptmann, and Nanning Zheng. 2018. Deep feature learning via
structured graph Laplacian embedding for person re-identification.
Pattern Recognition (2018).

[7] Alberto Garcia Duran and Mathias Niepert. 2017. Learning graph
representations with embedding propagation. In Advances in neural
information processing systems (NeurIPS).

[8] Boyuan Feng, Yuke Wang, Guoyang Chen, Weifeng Zhang, Yuan Xie,
and Yufei Ding. 2021. EGEMM-TC: Accelerating Scientific Computing
Tensor Cores with Extended Precision. ACM SIGPLAN Symposium on
Principles & Practice of Parallel Programming (PPoPP) (2021).

[9] Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation
Learning with PyTorch Geometric. In ICLR Workshop on Representation
Learning on Graphs and Manifolds (ICLR).

[10] Matthias Fey and Jan E. Lenssen. 2019. PyTorch Extension Library of
Optimized Scatter Operations. https://github.com/rusty1s/pytorch_
scatter

[11] Jaume Gibert, Ernest Valveny, and Horst Bunke. 2012. Graph embed-
ding in vector spaces by node attribute statistics. Pattern Recognition
(2012).

[12] Joseph E Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, and
Carlos Guestrin. 2012. Powergraph: Distributed graph-parallel com-
putation on natural graphs. In Presented as part of the 10th USENIX
Symposium on Operating Systems Design and Implementation (OSDI).
[13] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature
learning for networks. In Proceedings of the 22nd ACM international
conference on Knowledge discovery and data mining (SIGKDD).
[14] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive repre-
sentation learning on large graphs. In Advances in neural information

processing systems (NeurIPS).

[15] Guyue Huang, Guohao Dai, Yu Wang, and Huazhong Yang. 2020.
GE-SpMM: General-Purpose Sparse Matrix-Matrix Multiplication on
GPUs for Graph Neural Networks. In SC20: International Conference
for High Performance Computing, Networking, Storage and Analysis.
1–12. https://doi.org/10.1109/SC41405.2020.00076

[16] Zexi Huang, Arlei Silva, and Ambuj Singh. 2021. A Broader Picture of
Random-walk Based Graph Embedding. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining. 685–695.
[17] Riesen Kaspar and Bunke Horst. 2010. Graph classification and cluster-

ing based on vector space embedding. World Scientific.

[18] Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel,
and Marion Neumann. 2016. Benchmark Data Sets for Graph Kernels.
http://graphkernels.cs.tu-dortmund.de

[19] Thomas N Kipf and Max Welling. 2017. Semi-supervised classifica-
tion with graph convolutional networks. International Conference on
Learning Representations (ICLR) (2017).

[20] Jérôme Kunegis and Andreas Lommatzsch. 2009. Learning spectral
graph transformations for link prediction. In Proceedings of the 26th
Annual International Conference on Machine Learning (ICML).
[21] Süreyya Emre Kurt, Aravind Sukumaran-Rajam, Fabrice Rastello, and
P. Sadayyapan. 2020. Efficient Tiled Sparse Matrix Multiplication
through Matrix Signatures. In Proceedings of the International Confer-
ence for High Performance Computing, Networking, Storage and Analysis
(Atlanta, Georgia) (SC ’20). IEEE Press, Article 87, 14 pages.

[22] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large

Network Dataset Collection. http://snap.stanford.edu/data.

[23] Dijun Luo, Chris Ding, Heng Huang, and Tao Li. 2009. Non-negative
laplacian embedding. In 2009 Ninth IEEE International Conference on
Data Mining (ICDM).

[24] Dijun Luo, Feiping Nie, Heng Huang, and Chris H Ding. 2011. Cauchy
graph embedding. In Proceedings of the 28th International Conference
on Machine Learning.

[25] Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong
Zhou, and Yafei Dai. 2019. Neugraph: parallel deep neural network
computation on large graphs. In 2019 USENIX Annual Technical Con-
ference (USENIX ATC).

[26] Nvidia. [n.d.]. CUDA Sparse Matrix library (cuSPARSE). developer.

nvidia.com/cusparse. developer.nvidia.com/cusparse

[27] Nvidia. [n.d.].

cuSPARSE Blocked SpMM.

https://developer.

nvidia.com/blog/accelerating-matrix-multiplication-with-
block-sparse-format-and-nvidia-tensor-cores/.
//developer.nvidia.com/blog/accelerating-matrix-multiplication-
with-block-sparse-format-and-nvidia-tensor-cores/

https:

[28] Nvidia. [n.d.]. Dense Linear Algebra on GPUs. developer.nvidia.com/

cublas. developer.nvidia.com/cublas

[29] NVIDIA. [n.d.].

Improved Tensor Core Operations.

https:

//docs.nvidia.com/cuda/ampere-tuning-guide/index.html#tensor-
operations.
guide/index.html#tensor-operations

https://docs.nvidia.com/cuda/ampere-tuning-

[30] Nvidia. [n.d.]. NVIDIA Blocked-Sparse API. https://docs.nvidia.
com/cuda/cusparse/index.html#cusparse-generic-function-spmm).
https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-
generic-function-spmm)

[31] Nvidia. [n.d.].

NVIDIA Volta.

Volta_(microarchitecture).
(microarchitecture)

https://en.wikipedia.org/wiki/
https://en.wikipedia.org/wiki/Volta_

[32] NVIDIA. [n.d.]. TensorFloat-32 in the A100 GPU Accelerates AI Train-
ing, HPC up to 20x. blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-
precision-format/.

[33] Nvidia. [n.d.]. Warp Matrix Multiply-Accumulate (WMMA).

docs.nvidia.com/cuda/cuda-c-programming-guide/index.
html#wmma.
guide/index.html#wmma

docs.nvidia.com/cuda/cuda-c-programming-

11

, ,

[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad-
bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein,
Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch:
An Imperative Style, High-Performance Deep Learning Library. In
Advances in Neural Information Processing Systems (NeurIPS).

[35] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk:
Online Learning of Social Representations. In Proceedings of the 20th
ACM International Conference on Knowledge Discovery and Data Mining
(SIGKDD).

[36] Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li.
2018. Attention-based graph neural network for semi-supervised
learning. (2018).

[37] Philippe Tillet, H. T. Kung, and David Cox. 2019. Triton: An Intermedi-
ate Language and Compiler for Tiled Neural Network Computations.
In Proceedings of the 3rd ACM SIGPLAN International Workshop on
Machine Learning and Programming Languages (Phoenix, AZ, USA)
(MAPL 2019). Association for Computing Machinery, New York, NY,
USA, 10–19. https://doi.org/10.1145/3315508.3329973

[38] Tomasz Tylenda, Ralitsa Angelova, and Srikanta Bedathur. 2009. To-
wards time-aware link prediction in evolving social networks. In Pro-
ceedings of the 3rd workshop on social network mining and analysis.

[39] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana
Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Net-
works. In International Conference on Learning Representations (ICLR).
[40] Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye,
Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo,
Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J Smola,
and Zheng Zhang. 2019. Deep Graph Library: Towards Efficient and
Scalable Deep Learning on Graphs. ICLR Workshop on Representation
Learning on Graphs and Manifolds (2019).

[41] Yuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng, Yuan
Xie, and Yufei Ding. 2021. GNNAdvisor: An Efficient Runtime System
for GNN Acceleration on GPUs. In USENIX Symposium on Operating
Systems Design and Implementation (OSDI’21).

[42] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How
Powerful are Graph Neural Networks?. In International Conference on
Learning Representations (ICLR).

[43] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L.
Hamilton, and Jure Leskovec. 2018. Hierarchical Graph Representation
Learning with Differentiable Pooling. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing Systems (NIPS).
[44] Orestis Zachariadis, Nitin Satpute, Juan Gómez-Luna, and Joaquín
Olivares. 2020. Accelerating sparse matrix–matrix multiplication with
GPU Tensor Cores. Computers & Electrical Engineering 88 (2020),
106848.

12

