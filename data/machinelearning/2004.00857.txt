Special Issue: Machine Learning and Combinatorial Optimization manuscript No.
(will be inserted by the editor)

Average Reward Adjusted Discounted Reinforcement Learning

Near-Blackwell-Optimal Policies for Real-World Applications

Manuel Schneckenreither

Received: date / Accepted: date

Abstract Although in recent years reinforcement learning has become very popular the number
of successful applications to diﬀerent kinds of operations research problems is rather scarce. Re-
inforcement learning is based on the well-studied dynamic programming technique and thus also
aims at ﬁnding the best stationary policy for a given Markov Decision Process, but in contrast
does not require any model knowledge. The policy is assessed solely on consecutive states (or state-
action pairs), which are observed while an agent explores the solution space. The contributions of
this paper are manifold. First we provide deep theoretical insights to the widely applied standard
discounted reinforcement learning framework, which give rise to the understanding of why these
algorithms are inappropriate when permanently provided with non-zero rewards, such as costs or
proﬁt. Second, we establish a novel near-Blackwell-optimal reinforcement learning algorithm. In
contrary to former method it assesses the average reward per step separately and thus prevents
the incautious combination of diﬀerent types of state values. Thereby, the Laurent Series expan-
sion of the discounted state values forms the foundation for this development and also provides
the connection between the two approaches. Finally, we prove the viability of our algorithm on a
challenging problem set, which includes a well-studied M/M/1 admission control queuing system.
In contrast to standard discounted reinforcement learning our algorithm infers the optimal policy
on all tested problems. The insights are that in the operations research domain machine learning
techniques have to be adapted and advanced to successfully apply these methods in our settings.

Keywords machine learning, reinforcement learning, average reward, operations research,
admission control.

M. Schneckenreither
Department of Information Systems, Production and Logistics Management
University of Innsbruck, Innsbruck 6020, Austria
E-mail: manuel.schneckenreither@uibk.ac.at

0
2
0
2

r
p
A
2

]

G
L
.
s
c
[

1
v
7
5
8
0
0
.
4
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
1 Introduction

Artiﬁcial Intelligence, or more precisely machine learning (ML), has been rising over the last decades
and became a very popular topic in science and industry. It is the ﬁeld that tries to mimic human
learning capabilities using computer algorithms. This is done by mathematically modeling the
learning process by ﬁnding a function that matches the desired outcome according to a given input.
Machine learning itself can be divided into the categories of supervised learning, unsupervised
learning and reinforcement learning [36, p.1ﬀ], where we focus on reinforcement learning in this
paper. Despite astonishing results in the application of reinforcement learning to game-like domains,
for instance chess [35], Go [34], and various Atari games [24], no such success was reported for non-
episodic operations research problems yet. We conjecture that this lack of reinforcement learning
applications is due to the reason that the widely applied standard discounted reinforcement learning
framework is inappropriate. To overcome this issue, we present a discounted reinforcement learning
variant which is able to deduce near-(Blackwell-)optimal policies, i.e. it performs better for real-
world economic problem structures, such as ones often found in the area of operations research.

Reinforcement Learning (RL), which is mathematically based on dynamic programming, is
similar to supervised learning, but diﬀers in the way that input-output pairs are actually never
presented to the algorithm, but an oracle function rewards actions taken by an agent [36, p.2]. In
contrast to dynamic programming, RL has the advantages that (i) the problem space is explored by
an agent and thus only expectantly interesting parts need to be assessed and (ii) a comprehensive
knowledge of the underlying model becomes unnecessary as the states are evaluated by consecutively
observed states solely. Like in operations research the goal is to optimise some measure, termed
reward in RL, by ﬁnding the best function for the underlying problem. The basic idea of RL is
quite simple. An agent explores the solution space by observing the current system state and taking
an action which makes the system traverse to the next state. Every time the agent chooses an action
a reward is generated by the (oracle) reward function. The agent tries to learn what actions to take
to maximise the reward over time, i.e. not only to maximise over the actions for the current state,
but it also respects possible stochastic transitions and reward values of future states. RL is most
often applied to control problems, games and other sequential decision making tasks [36], where
in almost all steps the reward function returns 0. However, in real-world (economic) applications,
where success is usually measured in terms of proﬁt or costs, the function intuitively returns a
non-zero reward in almost all steps as it continuously reports the actions results. Therefore, we
focus on problems that produce an average reward per step that cannot be approximated by 0.
Application areas of such a model could be an agent that periodically decides on buying and selling
instruments at the stock market, an agent performing daily replenishment decisions or many other
decision problems that arise in hierarchical supply chain and production planning systems, which
are strongly capacity-oriented [42] and with aggregated feedback (e.g., see [30,12]).

Only a very limited number of operations research optimisation papers that are using RL and
imposing this structure are available. Schneckenreither and Haeussler [31] presented an RL algorithm
which optimises order release decisions in production control environments. They use sophisticated
additions to allow the agent to link the actions to the actual rewards. The results were compared
to static order release measures only, which were outperformed. Gijsbrechts et al. [10] apply RL on
a dual sourcing problem for replenishment of a distribution center using rail or road, and compare
their results to optimal solutions if tractable or otherwise established heuristics. They found that
hyperparameter1 tuning is eﬀort-intensive and the resulting policies are often not optimal, especially

1 In machine learning parameters to be set before the start of the experiment are called hyperparameters.

2

for larger problems. Balaji et al. [1] use out-of-the-box RL algorithms with simple 2 layer neural
networks to tackle stochastic bin packing, newsvendor and vehicle routing problems (VRP). The
VRP is a generalised travelling salesman problem (TSP) where one or more vehicles have to visit
nodes in a graph. They report to sometimes beat the benchmarks and ﬁnd sensible solutions. The
capacitated VRP is also tackled with RL by Nazari et al. [26]. They minimise the total route
length and compare the results to optimal solutions for small instances. Although the optimum
is not reached, one instance, which keeps track of the most probable paths, performs better than
well established heuristics. Also for larger problem sizes this technique seems to outperforms the
other tested methods. Vera and Abad [39] extend this model to a multi-agent algorithm and thus
tackle the capacitated multi-vehicle routing problem with a ﬁxed ﬂeet size. They also report better
results compared to the heuristics, especially for large problem sizes, but in contrast to [26] are
outperformed by Google’s OR-Tools2. These applications, like other TSP applications [2,14, e.g.],
of RL to the VRP, except for [1], however, calculate the reward according to the length to the
ﬁnished route. Thus, the average reward for long routes can be approximated with 0 as the decision
problem is episodic and therefore perform well. Finally there are applications to the beer distribution
game [7,28], however the problem size and therefore its complexity imposed by the beer game
is rather small. All of the above cited papers use highly sophisticated methods to tackle rather
small problem sizes or result in far-from-optimal solutions. One reason for this is that they utilise
algorithms of the widely applied standard discounted RL framework, which are usually evaluated on
games. However, games are structured to have a terminal state describing victory or defeat, which
is reported as positive or negative reward to the agent, while any other preceding decision returns
reward 0. This does not comply with most of the above cited non-episodic optimisation problems.
Although some of these applications tackle intractable problems, in this paper we are concerned
with smaller problem sizes to be able to investigate the underlying mathematical issues. A similar
approach was taken by Mahadevan. In a series of papers the authors investigate average reward
RL. In average reward RL no discount factor is used but rather nested constraint problems are
approximated iteratively. In [17] they establish mainly foundations and examine R-Learning, an
average reward RL algorithm. R-learning was proposed by Schwartz in [32] and is similar to our
approach, but less sophisticated and therefore unable to produce near-optimal-policies. Then in [18]
RL optimality criteria are thoroughly discussed, while in the papers [16,19] they present model-
based and model-free bias-optimal algorithms. Similarly, Tadepalli and Ok [37] present an average
reward RL algorithm called H-Learning, which under certain assumptions ﬁnds bias-optimal values.
Furthermore, they apply it to simulated automatic guided vehicle scheduling. Later Mahadevan et
al. [20] lift the model-free average reward RL algorithm to continuous-time semi-Markov Decision
Processes (semi-MDPs), which handle non-periodic decision problems, and apply it to an inventory
problem consisting of a single machine. Then the algorithm of [20], called SMART, was applied on
the optimisation of transfer lines using a hierarchical approach [21] and preventive maintenance of
a production inventory system [8]. Although the results are promising, the adaption to continuous-
time problems eases the complexity for most applications. However, in practise usually decision have
to be made on a daily basis [9]. Therefore, we refrain from this adaption and concentrate on standard
MDPs only. Furthermore, continuous-time problems can be converted through uniformisation into
equivalent discrete time instances (see [29,4]).

The reason for the small number of RL applications to non-episodic operations research problems
and the fact that none of the available applications report tremendous success, as in game-like areas,
can be explained by the average reward per step that the agent receives. The average reward per

2 See https://developers.google.com/optimization.

3

step is usually by far the greatest part when the discounted state values3 are decomposed into
its sub-components. This results from the fact, that in contrast to the other parts, the average
reward contributes to the learned state-values in an exponentially up-scaled fashion. However, for
most applications the average reward is equal among all states and thus the agent has issues in
choosing the best action. Additionally, iteratively shifting all state values to the base imposed by the
exponentially up-scaled average reward easily causes the learning process to fail, ﬁnding itself in sub-
optimal maxima and with that complicating the hyperparameterisation process tremendously. On
the opposite average reward RL is cumbersome and computationally expensive, as nested constraints
have to be solved. Therefore, we contribute to the research streams of discounted and average
reward RL, by combining the best things of both worlds, that is, the stability and simplicity of
standard discounted RL and the idea of assessing the average reward separately and aiming for
broader optimality criteria of average reward RL with this work. Additionally, we contribute to the
operations research domain, which was the origin of the motivation for this work, by providing a
new machine learning algorithm and applying it to a well studied M/M/1 admission control queuing
system. In summary the contribution is as follows.

– First we analyse and illustrate why standard discounted reinforcement learning is inappropriate
for real-world applications, which usually impose an average reward that cannot be approximated
with 0,

– Second, we establish a novel near-Blackwell-optimal reinforcement learning algorithm and ana-

lytically prove its optimality, and

– Third, show the viability of the algorithms by experimentally applying them to three problem

speciﬁcations, one of which is a well-studied M/M/1 admission control queuing system.

The rest of the paper is structured as follows. The next section provides an overview of the
proposed method. Section 3 introduces discounted RL and average reward RL, and provides the
linkage between the two frameworks via the Laurent Series Expansion of discounted state values.
The insights gained of the expansion form the foundations for the developed average reward adjusted
discounted RL method established in Section 4. Section 5 proves the viability of the algorithm, while
Section 6 concludes the paper.

2 Overview of Average Reward Adjusted Discounted Reinforcement Learning

Before establishing the mathematical details of the method in the next sections, in this section we
give a high-level overview of the newly developed algorithm by providing the main concepts and
ideas using an example.

Although very convenient the evaluation of newly developed RL algorithms on game-like envi-
ronments, for instance chess or Atari games, brings major issues once these algorithms are applied
for optimisation in structurally diﬀerent real-world scenarios as found in operations research. The
most intuitive goal in such economic optimisations problems is to maximise for expected proﬁt
(minimize expected costs) at every decision, that is, always taking the actions that expectantly
result in the greatest proﬁt. I.e. when modelled as reinforcement learning process the reward func-
tion must return the accumulated proﬁt between consecutive decisions. However, reward functions

3 Note that in contrary to economics where discounting is often motivated by interest rates in discounted RL the
motivation of discounting future rewards origins from the fact that the “inﬁnite sum has a ﬁnite value as long as the
reward sequence [..] is bounded” [36, p.59].

4

0

2

A

0

1

2

B

0

2

Fig. 1: A MDP with the only action choice in state 1 producing policies πA (going left in 1) and
πB (going right) (adapted from [17])

of games are usually designed to return a non-zero reward only after the agent reached the state
describing victory or defeat. Therefore, currently newly designed RL algorithms are evaluated with
problems that impose an average reward per step of approximately 0. This leads to the fact, that
when discounted RL techniques are applied to real-world scenarios, they often perform poorly. As
we will see the average reward per step plays a crucial role in the performance of reinforcement
learning algorithms.

Therefore, our approach in this work is to separately learn the average reward value and rather
use average reward adjusted discounted state values to be able to selectively choose the best actions.
By using a suﬃciently large discount factor the agent reduces the set of best actions to the ones
that are (bias-)optimal. Additionally, we approximate the average reward adjusted discounted state
values with a smaller discount factor, which allows us to choose actions near-(Blackwell-)optimal.
To clarify consider the Markov Decision Process (MDP) in Figure 1 which consists of two
possible deﬁnite policies with the only action choice in state 1. Both policies πA (going left in 1)
and πB (going right) result in the same average reward ρπA = ρπB = 1. However, only the A-loop
is (bias-)optimal, as the actions with non-zero rewards are selected earlier. E.g. consider starting
in state 1. Under the policy πA which takes the A-loop the reward sequence is (2, 0, 2, 0, . . .), while
for the other policy πB it is (0, 2, 0, 2, . . .). Our algorithm infers i) the average reward based on the
state values of consecutively observed states and the returned reward, ii) approximates so called
bias values, and iii) uses a smaller discount factor to infer the greatest error term values which only
exist as the discount factor is strictly less than 1. Intuitively bias values are the additional reward
received when starting in a speciﬁc state, and the error term incorporates the number of steps until
the reward is collected.

With a discount factor of 0.999 our implementation automatically infers policy πA and thus
produces average reward adjusted discounted state values X πA
0.999((A, l)) = 0.493 for going left
(l) in state A and X πA
0.999((A, r)) = 0.492 for going right (r) in under 10k iterations. Here we
use state-action pairs as function parameters. The actual bias values, analytically inferred, are
V πA((A, l)) = V πA((A, r)) = 0.5, where the diﬀerences to the estimated values are due to the error
term. Note that for policy πB the state values are V πB ((A, l)) = V πB ((A, r)) = −0.5. Therefore, πA
is preferable. However, after the decision for πA is made, doing loop B once becomes attractive as
well, as the same amount of rewards are collected. This makes the bias values being equal under the
given policy. Therefore, our algorithm has a further decision layer using an approximation of average
adjusted discounted state values with a smaller discount factor. As the error term is increased when
the discount factor decreases, the agent chooses the action that maximises the error term. The
inferred values for a discount factor of 0.8 are X πA
0.8 ((A, r)) = 0.145, and
thus action l is preferred over action r.

0.8 ((A, l)) = 0.555 and X πA

In the sequel we establish the method, provide the algorithm and present an optimality analysis
thereof. I.e. we show that the presented algorithm is capable of inferring, so called, Blackwell-optimal
policies [5]. Blackwell-optimal policies are policies that maximise the average reward, the bias value

5

and the error term, in this speciﬁc order. Near-Blackwell-optimal algorithms infer bias-optimal
policies for any MDP and for given MDPs can be conﬁgured to infer Blackwell-optimal policies,
where inaccuracies due to the limitations of ﬂoating point representations of modern computer
systems are neglected.

3 The Laurent Series Expansion of Discounted State Values

This section brieﬂy introduces the needed formalism, then investigates the discounted RL framework
and provides the Laurent series expansion of its state values. The Laurent series expansion plays a
crucial role in the development of the newly established algorithm.

Like Miller and Veinott [22] we are considering problems that are observed in a sequence of points
in time labeled 1, 2, . . . and can be modelled using a ﬁnite set of states S, labelled 1, 2, . . . , |S|, where
the size |S| is the number of elements in S. At each point t in time the system is in a state st ∈ S.
Further, by choosing an action at of a ﬁnite set of possible actions As the system returns a reward
rt = r(st, at) and transitions to another state st+1 ∈ S at time t + 1 with conditional probability
p(st+1, rt | st, at). That is, we assume that reaching state st+1 from state st with reward rt depends
solely on the previous state st and chosen action at. In other words, we expect the system to
possess the Markov property [36, p.63]. RL processes that possess the Markov property are referred
to as Markov decision processes (MDPs) [36, p.66]. A MDP is called episodic if it includes terminal
states, or non-episodic otherwise [36, p.58]. Terminal states are absorbing states and are followed
by a reset of the system, which starts a new episode.
Thus, the action space is deﬁned as F = ×|S|

s=1As, where As is a ﬁnite set of possible actions. A
policy is a sequence π = (f1, f2, . . .) of elements ft ∈ F . Using the policy π means that if the system
is in state s at time t the action ft(s), i.e. the s-th component of ft, is chosen. A stationary policy
π = (f, f, . . .) does not depend on time. In the sequel we are concerned with stationary policies only.
An ergodic MDP consists of a single set of recurrent states under all stationary policies, that is, all
states are revisited with probability 1 [17]. A MDP is termed unichain if under all stationary policies
the transition matrix contains a single set of recurrent states and a possible empty set of transient
states. A MDP is multichain if there exists a policy with at least two recurrent classes [17]. Finally,
a state is termed periodic if the greatest common divisor of all path lengths to itself is greater than
1. Otherwise, it is called aperiodic. The goal in RL is to ﬁnd the optimal stationary policy π⋆ for
the underlying MDP problem deﬁned by the state and action spaces, as well as the reward function.

3.1 Discounted Reinforcement Learning

In the standard discounted framework the value of a state V πγ
counted sum of rewards under the stationary policy πγ when starting in state s. That is,

γ (s) is deﬁned as the expected dis-

V πγ
γ (s) = lim
N→∞

E[

N −1

X
t=0

γtRπγ

t (s)] ,

where 0 6 γ < 1 is the discount factor and Rπγ

t (s) = Eπγ [r(st, at) | st = s, at = a] the reward
received at time t upon starting in state s by following policy πγ [17, e.g.]. The aim is to ﬁnd an

6

3

4

2

2′

Printer

1

Mail

5

20

5

10′

3′

9′

. . .

Fig. 2: A MDP with two diﬀerent deﬁnite policies (adapted and corrected4 from Mahadevan [18])

optimal policy π⋆
any other policy πγ:

γ, which when followed, maximises the state value for all states s as compared to

π⋆
γ − V πγ
γ
V

γ > 0 .

γ (s) = V π

t (s) = Rπ

This criteria is usually referred to as discounted-optimality (or γ-optimality) as the discount
factor γ is ﬁxed [18]. Note that most works omit the index γ in the policies πγ, π⋆
γ and thus
γ = π⋆, where π⋆ is the optimal policy for the underlying problem. For
incorrectly indicate that π⋆
the rest of the paper we follow this convention and drop the index γ of the policy for the discounted
state value and the reward, thus V πγ

γ (s), and Rπγ
This also means that the actual value set for γ deﬁnes the policy which is optimised for. For
instance, Figure 2 depicts a MDP with a single action choice in state 1. Rewards of 5 or 10 re-
spectively, are received once upon traversing from state 5 or 10′ to state 1. In all other cases the
reward is 0. The only action choice is in state 1, in which the agent can choose between doing the
printer-loop or the mail-loop. Observe that the average reward received per step equals 1 for the
printer-loop and 2 for the mail-loop. Thus, the Blackwell-optimal policy is to choose the mail-loop,
as it maximises the returned reward. However, if γ < 3− 1
5 ≈ 0.8027 an agent using standard dis-
counted reinforcement learning selects the printer loop. Schartz [32] shows that for arbitrary large
γ < 1 it is possible that standard discounted RL fails in ﬁnding the optimal policy, while Zhang
and Dietterich [41] as well as Boyan [6] use RL on combinatorial optimization problems for which
the choice of discount factor is crucial.

t (s).

The idea behind the discount factor in standard discounted reinforcement learning is to prevent
inﬁnite state values [36, p.59]. However, Mahadevan [18,17] refers to the discounting approach as
unsafe, as it encourages the agent to aim for short-term gains over long-term beneﬁts. This results
from the fact that the impact of an action choice with long-term reward decreases exponentially
with time [27]. Besides bounding the state values another main idea behind the γ-parameter is to be
able to specify a balance between short-term (low γ-values) and long-term (high γ-values) reward
objectives [32]. But what seems to be an advantage rather becomes a disadvantage, as in almost all
decision problems the aim actually is to perform well over time, i.e. to collect as much reward as
possible over a presumably inﬁnite time horizon. In terms of reward this means to seek for average
reward maximising policies before more selectively choosing actions, cf. the example in Figure 2.
Therefore, in almost all RL studies the discount factor γ is set to a value very close to (but strictly

4 In [18] they claim that only for γ < 0.75 the policy is sub-optimal.

7

up

t
f
e
l

(0,1)

down
up

(0,0)
random

up

t
f
e
l

(1,0)

t
h
g
i
r

t
f
e
l

(1,1)

down

down

t
h
g
i
r

t
h
g
i
r

U(0, 8)−1

(0,0)
10

U(0, 8)

(1,0)

)
8
,
0
(
U

)
8
,
0
(
U

)
8
,
0
(
U

(0,1)

U(0, 8)
U(0, 8)

(1,1)

1
−
)
8
,
0
(
U

1
−
)
8
,
0
(
U

U(0, 8)−1

U(0, 8)−1

1
−
)
8
,
0
(
U

State/Action Space

Reward Function

Fig. 3: A gridworld MDP with 4 states and a set of 5 actions and the reward function

less than) 1, for instance to 0.99 [25,23,15, e.g.]. However, the issue is that the state values increase
exponentially as the average reward is multiplied by 1/(1 − γ), while the bias value and error term
are not. We will refer to the portion induced by the exponentially scaled average reward as base
level imposed by the average reward.

Example 1 To clarify, consider the gridworld MDP given in Figure 3 with states S = {(x, y) | x, y ∈
{0, 1}}, where (0, 0) is the goal state after which the agent is placed uniformly on the grid by using
the only available action random (see left side). In all other states the agent can choose among the
actions up, right, down, left, which move the agent in the expected direction, except when moving
out of the grid. In such cases the state is unchanged. The reward function is given on the right.
Action random in state (0, 0) returns reward 10, while taking any other action provides a uniformly
distributed reward of U(0, 8), where moving out of the grid adds a punishment of −1. It is obvious
that reaching state (0, 0) with as little steps as possible is optimal. As by deﬁnition the average
reward for unichain MDPs is equal among all states [17]. Thus the average reward for all states of
the optimal (and greedy) policy is 7, which already imposes a base level imposed by the average
reward of 700 in case of γ = 0.99.

For episodic MDPs standard discounted RL is unable to evaluate the average reward of terminal
states and its predecessors correctly, as there is no consecutive state at the end of an episode, i.e.
for the gridworld example state (0, 0) is evaluated with value 10, when a new episode starts upon
reaching the goal.

Furthermore, for both, episodic and non-episodic MDPs, there is another major issue. As each
state is assessed separately, so is the average reward of that state. Thus, states that are visited more
often in the iterative process of shifting all state values to the base level imposed by the average
reward are evaluated with a higher average reward, which however, increases the likeliness that the
agent will visit the state again. This behaviour forms clusters with cyclic paths of states (e.g. going
back and fourth between states (1, 0) and (1, 1)), which are visited more and more likely. Even setting
a very high exploration rate is usually no remedy by the same argument. Additionally, recall that
the average reward increases once the policy gets better through decreasing the exploration rate.
This information then needs to be traversed to all states, which however, requires high exploration.
This implies that ﬁnding the correct hyperparameter setting (e.g. exploration decay) needs a huge
amount of eﬀort and experience, which is exactly what was reported in [10]. Regardless of that, this
behaviour increases the number of required learning steps tremendously, as all states have to be
adapted every time a policy change imposes a change in the average reward (see also [18,17,32]).
Unfortunately this easily leads to aforementioned clusters.

8

It is obvious that optimal policies are very hard to obtain with all these diﬃculties, especially as
all these eﬀects complicate the parameterisation tremendously. However, recall that in operations
research and many other real-world applications, success is constantly measured. It is easy to see
that this corresponds directly to the problem structure of the gridworld example given in Figure 3.
Unfortunately this also means that all these issues directly exist in such applications, which also
explains the small number of works combining RL and operations research problems.

Therefore, in the sequel we present a more reﬁned RL approach for operations research, which
overcomes these issues by separately assessing the average reward. The relation of the computed
values of these two approaches, that is, the aformentioned discussed standard discounted RL tech-
nique and the in this paper developed average reward adjusted discounted RL (ARA-DRL) method,
are described by following established Laurent series expansion.

3.2 The Laurent Series Expansion of Discounted State Values

The Laurent series expansion of the discounted state values [22,29] provide important insights
by giving rise to basically three addends. For a given discount factor γ the ﬁrst addend is solely
determined by the average reward5, the second is the bias value and the third one, actually consisting
of inﬁnitely many sub-terms, is the error term. In the sequel we present the deﬁnitions of the average
reward and bias value, before providing the Laurent series expansion.

Deﬁnition 1 Due to Howard [13] for an aperiodic6 MDP the gain or average reward ρπ(s) of a
policy π and a starting state s is deﬁned as

ρπ(s) = lim
N→∞

E[P

t (s)]

N −1
t=0 Rπ
N

,

where Rπ
following policy π.

t (s) = Eπ[r(st, at) | st = s, at = a] is the reward received at time t, starting in state s and

Clearly ρπ(s) expresses the expected average reward received per action taken when starting in
state s and following policy π. In the common case of unichain MDPs, in which only a single set of
recurrent states exists, the average reward ρπ(s) is equal for all states s [17,29]. Thus, in the sequel
we may simply refer to it as ρπ.

Deﬁnition 2 For an aperiodic MDP problem the average adjusted sum of rewards or bias value is
deﬁned as

N −1

V π(s) = lim
N→∞

E[

X
t=0

(Rπ

t (s) − ρπ(s))] ,

where again Rπ

t (s) is the reward received at time t, starting in state s and following policy π.

5 Please note that we assume a positive average reward and the objective to maximise the returned reward through-

out this work.

6 In the periodic case the Cesaro limit of degree 1 is required to ensure stationary state transition probabilities and
thus stationary values [29]. Therefore to ease readability we concentrate on unichain and aperiodic MDPs. However,
the theory directly applies to period unichain MDPs by replacing the limits accordingly.

9

0

2

A

0

2

B

1

2

(0, r)

2

0

(1, l)

(1, r)

2

0

(2, l)

Fig. 4: The Blackwell-optimal policy πA of the MDP of Figure 1 as model-based (left) and model-
free (right) version

Note that the bias values are bounded due to the subtraction of the average reward. Thus the
bias value can be seen as the rewards that additionally sum up in case the process starts in state s.
γ (s) in standard discounted reinforcement learning can be decomposed in
its average reward, the bias value and an additional error term, which actually consists of inﬁnitely
many subterms.

Finally, a state value V π

Deﬁnition 3 Due to Miller and Veinott [22] the Laurent Series Expansion of the discounted state
value for a state s, a discount factor γ and a policy π is given by

V π
γ (s) =

ρπ(s)
1 − γ

+ V π(s) + eπ

γ (s) ,

(1)

where Puterman [29, p.341] shows that limγ→1 eπ

γ (s) = 0.

The error term eπ

γ (s) incorporates what amount of reward is collected in combination with
the number of steps until it is collected. The higher the discount factor the more long-sighted the
agent is. Note how the ﬁrst term depending on the average reward ρπ(s) converges to inﬁnity as
γ increases towards 1 and that the second addend does not depend on the discount factor. If the
average reward is non-zero and for large γ-values we can assume ρπ(s)/(1 − γ) ≫ V π(s) + eπ
γ (s)
which explains the behaviour of the standard discounted RL agent, cf. the example of Figure 3.
Regardless of the quality of the chosen actions all state-values need to iteratively increase from
the starting state (usually 0) to the base level imposed by the average reward ρπ(s)/(1 − γ) oﬀset
by V π(s) + eπ
γ (s). As usually there are more actions which are sub-optimal in comparison to the
number of optimal ones the agent will more likely choose such an action in usually applied tabula
rasa learning. Thus there is a high chance that cycles form.

Example 2 Reconsider the task depicted in Figure 1. The (Blackwell-)optimal MDP πA, that is, the
one that chooses the A-loop, is shown on the left side of Figure 4. The right side of Figure 4 shows
the same MDP for the model-free version. In model-free reinforcement learning state-action pairs
as opposed to state values are estimated. The dashed line indicates that these states are connected
and thus the agent has to choose among them. In this case the bias values are V πA((0, r)) = −0.5,
V πA((1, l)) = V πA((1, r)) = 0.5, and V πA((2, l)) = 1.5.

When using standard discounted reinforcement learning, that is, estimating V π

γ (s) the average
reward of 1 scales the state-action values. E.g. for a discount factor of γ = 0.99 the inferred values
for a converged system are V πA
0.99((1, r)) = 100.482,
and V πA
0.99((2, l)) = 101.497. For all states the discounted state-value consists of the scaled average
reward 1/0.01 = 100 and the bias value plus the error term. The duration of the process of learning
the state values is unintentionally increased as the scaled average reward has to be learned in an

0.99((1, l)) = 100.502, V πA

0.99((0, r)) = 99.497, V πA

10

iterative manner.Furthermore, the values itself are hard to interpret and the marginal diﬀerence
between optimal and non-optimal action as compared to their actual values increase the likelihood
of choosing sub-optimal actions, especially when function approximation is used to represent Vγ(s).
In this example the diﬀerence of the state values for choosing action l over action r in state 1 is
given by 0.02 as compared to their mean state value of 100.492.

4 Average Reward Adjusted Discounted Reinforcement Learning

This section establishes the average reward adjusted discounted reinforcement learning algorithm.
Furthermore, we provide the Bellman Equations and an optimality discussion of the presented
algorithm. For the rest of the paper we assume unichain MDPs, that is, we restrict our method to
MDPs that posses a scalar average reward value ρπ. In case of multichain MDPs we refer to the
companion paper Near-Blackwell-Optimal Average Reward Reinforcement Learning 7.

Starting from the Bellman Equations we derive the average reward adjusted reinforcement learn-
ing equation using the Laurent series expansion provided in Equation 1. However, let us ﬁrst intro-
duce a notion for the state values which are adjusted of the average reward.

Deﬁnition 4 We deﬁne the average reward adjusted discounted state value X π
policy π and with discount factor 0 6 γ 6 1 as

γ (s) of a state s under

t (s)] − ρπ
This can be reformulated to X π
deﬁnition is a reformulation of the average-adjusted reward values of Schwartz [32,33].

1−γ = limN→∞ E[P

N −1
t=0 γtRπ

γ (s) = V π

1−γ , thus our

X π

γ (s) := V π(s) + eπ
γ (s) − ρπ

γ (s) .

A major problem occurring at average reward RL is that the bias values are not uniquely deﬁned
without solving the ﬁrst set of constraints deﬁned by the error term addends (see [29,19, p.346]).
We could overcome this issue by simply requiring γ to be strictly less than 1. However, actually
our algorithm does not require the exact solution for V π(s), but a solution which is oﬀset suﬃces.
Clearly this observation reduces the required iteration steps tremendously as ﬁnding the exact
solution, especially for large discount factors, is tedious. Therefore, we allow to set γ = 1, which
induces X π
γ (s) = V π(s) + u, where u is for unichain MDPs a scalar value independent of s, i.e.
equivalent for all states of the MDP [29, p.346]. If we are interested in correct bias values, i.e. γ
is close but strictly less than 1, our approach is a tremendous advantage over average reward RL
as it reduces the number of iterative learning steps by requiring only a single constraint per state
plus one for the scalar average reward value. That is, for an MDP with N states only one more
constraint (N +1) has to be solved in ARA-DRL as compared to (at least) 2N +1 nested constraints
for average reward RL. Therefore, it is cheap to compute X π
γ (s), while it is rather expensive to ﬁnd
the correct values of V π(s) directly, especially in an iterative manner as RL is.

4.1 Bellman Equations

Using the above notion we are able to derive the average reward adjusted discount reinforcement
learning state balance equation. To do so we make use of an equivalence found in the derivation of

7 Working paper.

11

the Bellman equation for V π
γ (s) (see e.g. [36, p.70]) and transform it to our needs. As in [36, p.66]
s,s′ = E[rt | st = s, at = a, st+1 = s′] for the expected reward to receive when
we use the notation Ra
traversing from any current state s to state s′ using action a and π(a | s) the probability of taking
action a in state s as given by policy π.

γ (s) = E
V π
π

[rt + γV π

ρπ
1 − γ

+ V π(s) + eπ

γ (s) = E

π

[rt + γ(

X π

γ (s) = E

π

[rt + γX π

γ (st+1) | st = s]
ρπ
1 − γ
γ (st+1) − ρπ | st = s]

+ V π(st+1) + eπ

γ (st+1)) | st = s]

X π

γ (s) = X
a

π(a | s) X
s′

p(s′ | st = s, at = a)[Ra

s,s′ + γX π

γ (s′) − ρπ]

Thus, we can compute the average reward adjusted discounted state value X π
γ (s) of a state s by
the returned reward, the adjusted discounted state value X π
γ (st+1) of the next state st+1 and the
average reward ρπ. This is very similar to the Bellman equation of standard discounted RL, cf. the
ﬁrst line. Further, note that in line three we use the equivalence ρπ(s) = Eπ[ρπ(s)] described by the
ﬁrst addend of the Laurent series expansion (see [22]).

In the same manner we derive the Bellman optimality equation for average reward adjusted

discounted reinforcement learning, cf. [36, p.76].

ρπ⋆
1 − γ

+ V π⋆

V π⋆
γ (s) = max

a

(s) + eπ⋆

γ (s) = max

X π⋆

γ (s) = max

a

a

[rt + γV π⋆
γ (st+1) | st = s]
ρπ⋆
1 − γ
γ (s) − ρπ⋆

[rt + γX π⋆

+ V π⋆

[rt + γ(

E
π⋆

E
π⋆
E
π⋆

| st = s]

(s) + eπ⋆

γ (s)) | st = s]

X π⋆

γ (s) = max

a X
s′

p(s′ | st = s, at = a)[Ra

s,s′ + γX π⋆

γ (s′) − ρπ⋆

]

Again like in standard discounted RL, also in ARA-DRL the value for the optimal policy equals the
expected value of the best action from that state [36, p.76], where the average reward is subtracted
accordingly. This is very pleasing as we can use the same ideas in terms of exploration/exploitation
and state value acquisition as known from standard discounted RL.

4.2 Near-Blackwell-Optimal Algorithm

The reinforcement learning algorithm is depicted in Algorithm 1. In model-free methods, which is
what we aim for, state-action pairs are computed instead of state values only. Therefore, the algo-
rithm operates on state-action tuples, where for simplicity we write X π
γ ((s, a))
and the Bellman optimality equation is adopted as in [36, p.76], s.t. the agent is able to selectively
choose actions among multiple states, cf. Figure 4. After initialising all values the agent enters the
loop in which the ﬁrst task is to choose an action (step 3). In this action selection process we utilise
an ǫ-sensitive lexicographic order 4ǫ deﬁned as (a1, . . . , an) = a 4ǫ b = (b1, . . . , bn) if and only if
|aj − bj| 6 ǫ for all j < i and |ai − bi| > ǫ. Note that the resulting sets of actions may not be
disjoint. Although this is an unusual order in programming, ﬁnding the set of maximizing values as

γ (s, a) instead of X π

12

Algorithm 1 Model-free tabular near-Blackwell-optimal RL algorithm for unichain MDPs
1: Initialize state s0, ρπ = 0, X π

· (·, ·) = 0, set an exploration rate 0 6 pexp 6 1, exponential smoothing learning

rates 0 < α, γ < 1, and discount factors 0.5 6 γ0 < γ1 6 1, where γ1 = 1 is usually a good choice.

2: while the stopping criterion is not fulﬁlled do
3: With probability pexp choose a random action and probability 1 − pexp one that fulﬁlls

γ1 (st, a), X π

maxa 4ǫ (X π
Carry out action at, observe reward rt and resulting state st+1.
if a non-random action was chosen then

γ0 (st, a)).

4:
5:

ρπ ← (1 − α)ρπ + α[rt + max

a

X π
γ1

(st+1, a) − X π
γ1

(st, at)]

6:

Update the average reward adjusted discounted state-values.

X π
(st, at) ← (1 − γ)X π
γ0
γ0
X π
γ1 (st, at) ← (1 − γ)X π

(st, at) + γ[rt + γ0 max

a

γ1 (st, at) + γ[rt + γ1 max

a

X π
(st+1, a) − ρπ]
γ0
X π
γ1 (st+1, a) − ρπ]

7:

Set s ← s′, t ← t + 1 and decay parameters

in our algorithm is straightforward and thus cheap to compute. In case the resulting set of actions
contains more than one element a random action of this set of actions is chosen.

As usual in reinforcement learning the state-values are exponentially smoothed using parameters
α and γ. In step 4 the chosen action is carried out, while in step 5 the average reward estimate is
calculated. As we aim for an estimate of ρπ⋆
we only update the value in case the greedy action
was chosen. The formula used is a reformulation of the second addend of the Laurent series expan-
sion (see [22] or [29, p.346] for details): ρπ(s)+ V π(s)− E[V π(s)] = Rt(s). Like [37] we also observed
that the average reward has to be updated by this formula and not by exponentially smoothing the
actual observed rewards as done in the algorithms of Mahadevan [16,19], which likely leads to sub-
optimal policies. Furthermore, we adapted the above given algorithm by adding an exponentially
smoothed8 bound from below for the average reward value. The idea is that, once a policy was
established for some time, it does not make sense to aim for policies with smaller average reward.
Finally the state-values are updated according to the average reward adjusted Bellman opti-
mality equation derived above (step 6) and the environment is updated to the next state and time
period (step 7). In the next subsection we will introduce further optimality criteria of RL and prove
that for a given MDP the discount factors γ0 and γ1 and the comparison measure ǫ can be chosen s.t.
under the assumption of correctly approximated values the algorithm produces Blackwell-optimal
policies.

4.3 Optimality Criteria

In terms of optimality, we consider the notion of n-discount optimality as it is the broadest approach
of optimality criteria in reinforcement learning, and further for a suﬃciently large n it is known that
there always exists a policy which is optimal [5,38]. For a comprehensive discussion of optimality
criteria in reinforcement learning we refer to [18].

8 With rate 1

50 and update of 97.5% of the current reward in every period.

13

Deﬁnition 5 Due to Veinott [38] for MDPs a policy π⋆ is n-discount-optimal for n = −1, 0, 1, . . .
for all states s ∈ S with discount factor γ if and only if

(1 − γ)−n (V π⋆

γ (s) − V π

γ (s)) > 0 .

lim
γ→1

As a policy can only be m-discount optimal if it is n-discount-optimal for all n < m [29,38], this
leads to the component-wise comparison when greedily choosing actions, cf. the action selection
process of the algorithm.

Deﬁnition 6 If a policy is ∞-discount-optimal then it is said to be Blackwell-optimal [5].

That is, Blackwell-optimal policies are the in the sense of n-discount-optimality the best achiev-
able policies that ﬁrst optimise for the highest gain, as we have for n = −1 a measure for gain-
optimality [17], then for n = 0 for bias-optimality [17], and as we will see for n > 1 it maximises
for the greatest error term. For an agent that either expects to have inﬁnitely many time to collect
rewards, or one that is unaware when the system will halt, this is the most sensible approach.

There are two known possibilities to incorporate the expected reward to be collected in the
future. The ﬁrst one is to use a single discounted value, while the other approach in general in-
corporates solving of inﬁnitely many constraints. The following deﬁnition separates these kinds of
algorithms.

Deﬁnition 7 If an algorithm infers for any MDP bias-optimal policies and for a given MDP can
in theory be conﬁgured to infer ∞-discount-optimal policies, but in practise this ability is naturally
limited due to the ﬁnite accuracy of ﬂoating-point representation of modern computer systems, it
is said to be near-Blackwell-optimal under the given computer system. An according to a near-
Blackwell-optimal algorithm inferred Blackwell-optimal policy is called near-Blackwell-optimal.

This deﬁnition is of practical relevance, as it deﬁnes a group of algorithms that are by far less
computationally expensive in comparison to ones that solve inﬁnitely many constraints, but are
able to deduce suﬃciently optimal policies. To the best of our knowledge there is no Blackwell-
optimal algorithm that neither requires inﬁnitely many constraints to be solved nor is restricted by
the ﬂoating point precision.

4.4 Optimality Analysis

In this section we will analyse the procedure of Algorithm 1 by the above deﬁnitions of n-discount-
optimality, cf. Deﬁnition 5. Thus, we will start with n = −1 and then proceed from there.

4.4.1 (−1 )-Discount-Optimality

For the case of n = −1 this leads to gain-optimality [17], deﬁned as ρπ⋆
(s) − ρπ(s) > 0 for all
policies π and states s ∈ S. This means that a (−1)-discount-optimal agent puts its highest priority
to maximising for the greatest average reward. However, recall that in unichain MDPs the average
rewards is equal among all states. Thus, all bias values are assessed with the same average reward, i.e.
the agent automatically maximises for the greatest gain. Note that by deﬁnition, in case of a possibly
falsely predetermined and ﬁxed average reward value, the bias values are estimated according to the
given policy induced by the average reward. Furthermore, if the average reward is ﬁxed to a wrong
value the bias values are similarly shifted as in the standard discounted framework. Therefore, we
highly recommend to infer the average reward automatically as speciﬁed in the algorithm.

14

Example 3 Reconsider the MDP of Figure 2 and the discount factor γ = 0.99. If we ﬁx the average
reward to ρπ
ﬁx,γ(s) =
ρπ −ρπ
1−γ + X π
ﬁx

ﬁx = 1 as opposed to the correct value of ρπ = 2, the algorithm infers values X π
γ (s) = 100 + X π

γ (s) instead.

4.4.2 0 -Discount-Optimality

In the case of n = 0 the criteria of n-discount-optimality describes bias-optimality with V π⋆
(s) −
V π(s) > 0 for all policies π and states s ∈ S [17]. Thus for the algorithm the ﬁrst decision level
is to maximise for the policy yielding the highest bias values. As we have limγ→1 eπ
γ (s) = 0, we
know that according to the chosen ǫ for suﬃciently large γ1 the agent selects the set of actions that
maximise V π(s).

Theorem 1 For a suﬃciently large γ1 < 1, where suﬃciently large means that for all states s we
have |eπ
γ1(s)| 6 ǫ, i.e. it depends on the parameter ǫ, a 0-discount-optimal agent chooses an action
among the set of possible actions that maximise X π
γ1(s).

Proof Recall that limγ→1 eπ

γ (s) = 0 and ρπ⋆

= ρπ. We have

(1 − γ)0(V π⋆

γ (s) − V π

γ (s)) > 0

lim
γ→1

ρπ⋆

− ρπ

1 − γ

(
lim
γ→1

+ V π⋆

(s) − V π(s) + eπ⋆

γ (s) − eπ

γ (s)) > 0

V π⋆

(s) − V π(s) > 0

meaning that a 0-discount-optimal policy π has to maximise the bias values V π(s) for all states s.
By deﬁnition of the ǫ-sensitive lexicographic order (a 4ǫ b if and only if |aj − bj| 6 ǫ), we have for a
suﬃciently large γ1-value |eπ
γ1(s)| 6 ǫ for all states s. Thus the claim
⊓⊔
follows.

γ1(s)| 6 ǫ and thus |V π(s) − X π

4.4.3 ∞-Discount-Optimality

Finally, in case n > 1, the agent has to choose actions that satisfy eπ⋆
γ (s) > 0 once γ → 1.
That is, the agent must maximise the error term, which means for n > 1 we analyse the case where
γ < 1 is used to incorporate short term rewards into the discounted state value, i.e. how long-sighted
the agent shall be. Therefore, the number of actions to reach a desired goal or path is taken into
account, as well as when and how much rewards are collected. However, as the error term depends
on inﬁnitely many sub-terms simply estimating these and summing up does not work.

γ (s) − eπ

The RL algorithm depicted in Algorithm 1 is unable to generally deduce Blackwell-optimal
policies. The cause is illustrated in Figure 5, where we assume the error terms to be polynomials of
the same degree. Let the policies π1, π2 choose the actions a1 and a2 resp. of the set of 0-discount-
optimal stationary actions in state s and then follow policy π. Note the diﬀerent slopes of the two
approximated values of X ·
γ(s). Therefore, by interpolation we know that for very high values of γ we
have X π2
γ (s) > X π1
γ (s). However, the agent will choose action a1 as it maximises X π
γ0(s). Therefore,
this means that the parameter γ0 deﬁnes how long-sighted the agent is. The MDP of Figure 6
explains the idea. The only action choice is in state S, where the agent either decides to do the top-
or bottom-loop. For both loops the same amount of rewards are collected, thus the average reward

15

X π

γ (s)

)
s
(
1
π

0
γ
e

)
s
(
2
π

0
γ
e

X π1

γ (s) = V π1 (s) + eπ1

γ (s)

X π2

γ (s)

V π1 (s) = V π2 (s)

γ0

ǫ

γ

γ2

Fig. 5: Visualisation of the strictly monotonically decreasing error terms eπ0
approaches 1

γ (s) and eπ2

γ (s) as γ

up

S

down

1

T 1

T 2

4

T 3

T 4

T 5

T 6

1

Top-Loop

Bottom-Loop

B3

6

B4

E

B5

B6

B1

B2

Fig. 6: An example MDPs for which the discount factor γ0 can be used to balance short- and
long-sightedness for near-Blackwell-optimal algorithm

(0.75) and bias values (0.25 for action up, 0.375 for action down) are equal, regardless of the chosen
policy. But only going down is Blackwell-optimal, as the full amount of rewards is collected sooner.
This also manifests in a higher bias value. However, recall that the agent is unable to separate the
actions by the bias values in case the same amount of rewards are collected. If we set γ0 = 0.50 the
agent deduces state-values V πTop
(S) = −0.746 and thus like for any other
value γ0 < 0.84837 chooses the top-loop.

(S) = −0.480 and V πBottom

γ0

γ0

This shows how for our algorithm γ0 functions exactly as the discount factor in standard RL is
supposed to do, due to the fact that the state values are adjusted: It can be used to balance expected
short-term and long-term rewards, without changing the main optimisation objective, i.e. maximise
for the highest average reward and bias values, before taking path lengths into account. Especially
for highly volatile systems, e.g. stochastic production and control systems, being able to set the
long-sightedness can be an advantage over Blackwell-optimal agents. Nonetheless, setting very high
values for γ0 and using the ǫ ≈ 0 for the comparison of X π
γ0(s) values, could be an approach in
ﬁnding Blackwell-optimal policies for many MDPs. But recall that for any arbitrary large γ0 < 1 it
is possible to construct MDPs which lead to non-optimal policies [32].

16

Theorem 2 A n-discount-optimal agent for n > 1 has to maximise the error term eπ
γ → 1.

γ (s) once

Proof Recall that for unichain MDPs the average reward ρπ(s) for all states s is equal and stated as
ρπ. Furthermore, as the policy must be 0-discount-optimal to be eligible for n-discount-optimality
with n > 1 we have V π⋆

(s) = V π(s) for all states s. Therefore,

(1 − γ)−n(V π⋆

γ (s) − V π

γ (s)) > 0

lim
γ→1

(1 − γ)−n(

lim
γ→1

ρπ⋆
(s)
1 − γ

+ V π⋆

(s) + eπ⋆

γ (s) −

ρπ(s)
1 − γ

− V π(s) − eπ

γ (s)) > 0

(1 − γ)−n(eπ⋆

γ (s) − eπ

γ (s)) > 0

lim
γ→1

which completes the proof as the error term does not depend on n. However, note that as eπ⋆
γ (s)
approaches 0 as γ → 1, we are interested in the cases where γ < 1. That is, the error term
⊓⊔
incorporates the amount and number of steps until rewards are collected.

In other works, the error term is often split into its subterms by eπ

m=1( 1−γ
γ )m · yπ
γ (s) := P
m (for
the deﬁnition see [22]). Then for any n > 1 the terms evaluate to maximising yπ
n, as for all subterms
< n the values are equal due to (n − 1)-discount-optimality and for n > 1 the terms evaluate to
0. This leads to the approaches of average reward dynamic programming, where n-nested sets of
constraints are solved [29, e.g. p.511ﬀ]. Clearly, this straightforward approach is computationally
very expensive and inﬁnite. Therefore, and also due to the imposed inﬁnite polynomial structure
of the error term formula we refrain on adapting this strategy and rather let the user choose an
appropriate γ0 value for the provided situation.

∞

Thus, for a given MDP and under the assumption of correct approximations and wisely selected
discount-factors γ0 and γ1 in combination of the chosen ǫ-value our algorithm is able to infer
Blackwell-optimality policies. Nonetheless due to the accuracy of ﬂoating-point representation of
modern computer systems the previous statement is naturally bounded. Therefore, the in this paper
established reinforcement learning algorithm ARA-DRL is near-Blackwell-optimal.

5 Experimental Evaluation

In this section we prove the viability of the algorithm with three examples. We compare the algo-
rithm to standard discounted RL, where we choose the widely applied Q-Learning [36,40] technique
as appropriate model-free comparison method. The Q-Learning algorithm is given in Algorithm 2.
We have adapted the parameter names accordingly to match the ARA-DRL algorithm from above.

5.1 Printer-Mail

Reconsider Figure 2 discussed above. The agent chooses either the printer loop or the mail loop,
whereas the mail loop returns a reward of 20 every tenth step and the printer loop 5 every ﬁfth
step. As there is no stochastic in the reward function, nor the transition function, the problem can
be easily solved until convergence. To do so we set the learning rate γ = 0.01 and for ARA-DRL

17

Algorithm 2 Watkins Q-Learning algorithm [40]. Adapted from the version by Sutton [36, p.149].
γ1 (·, ·) = 0, set an exploration rate 0 6 pexp 6 1 and 0 < γ, γ1 < 1.
1: Initialize state s0, Qπ
2: while the stopping criterion is not fulﬁlled do
3: With probability pexp choose a random action and probability 1 − pexp one that fulﬁlls maxa Qπ
γ1

(st, a)

4:
5:

6:

at the current state st.
Carry out action at, observe reward rt and resulting state st+1.
Update the discounted state-values.

Qπ
γ1

(st, at) ← (1 − γ)Qπ
γ1

(st, at) + γ[rt + γ1 max

a′

Qπ
γ1

(st+1, a′

)]

Set s ← s′, t ← t + 1 and decay parameters

Q-Learning

Measure

ARA-DRL

γ1 = 0.99

γ1 = 0.80

γ1 = 0.50

State (1, left)
State (1, right)
Steps in 106

−13.349
−8.787
1.0

186.509
191.070
10.3

3.046
3.011
0.5

0.323
0.039
0.4

Table 1: The state-values for state 1 of the printer-mail MDP and the number of steps until
convergence, where ARA-DRL inferred an average reward of ρπ = 1.999

α = 0.01 exponentially decayed with rate 0.25 in 100k steps, where the minimum is set to 10−6, and
the discount factors γ0 = 0.8, γ1 = 0.99. For Q-Learning we repeated the experiment with discount
factors γ1 = 0.99, γ1 = 0.8 and γ1 = 0.50.

γ1 (·) for ARA-DRL, and Qπ

The results are depicted in Figure 1, where the state-action values for (1, left) and (1, right) are
shown. The table reports the values of X π
γ1(·) in case of Q-Learning. The
reported number of steps are measured until convergence, which we deﬁned as no state-value change
in 100k steps. As ARA-DRL, as well as Q-Learning with γ1 = 0.99, report a greater value for going
right than taking action left, both infer the optimal policy, while Q-Learning with γ1 = 0.80 and
γ1 = 0.50 are unable to ﬁnd it. This shows clearly that it is crucial in standard discounted RL to
use a discount factor close to 1. When setting γ1 = 0.8027 the discounted values are V π
γ1 (1, left) =
V π
γ1 (1, right) = 3.113. The average reward learned by ARA-DRL is 1.999 and thus very close to
the actual value of 2. Note that as all states are assessed with the same average reward it is not
necessarily to estimate it perfectly, and thus small deviations have no impact on the policy of ARA-
DRL. The reported steps to convergence for Q-Learning increase exponentially when increasing the
discount factor, which is due to the requirement of very high discount factors a rather unsatisfactory
behaviour. Especially as ARA-DRL converges in 106 steps, while Q-Learning γ1 = 0.99 requires a
more than ten times longer learning phase.

5.2 Gridworld

We use a scaled up version of the MDP given in Figure 3 by increasing the state space to a 5 × 5
grid to make the optimisation task a little bit more challenging. The reward function is adapted
accordingly, where moving out of the grid is punished and otherwise a stochastic reward of U(0, 8)
is returned. The goal state is indiﬀerent. Again, it is obvious that the optimal policy is to traverse
to (0, 0) as fast as possible. Further, the system is symmetric, where states (m, n) and (n, m) are

18

Sum Reward

Avg. Steps to Goal

Algorithm

Mean

StdDev

Mean

StdDev

ARA-DRL γ1 = 0.99
ARA-DRL γ1 = 0.999
ARA-DRL γ1 = 1.00
Q-Learning γ1 = 0.99
Q-Learning γ1 = 0.999
Q-Learning γ1 = 0.50

51894.094
51878.069
51856.529
34409.464
33931.917
30171.837

234.260
282.335
242.953
3391.818
3076.401
328.591

5.039
5.063
5.055
7661.833
7379.155
9999.000

0.047
0.054
0.039
3518.656
3745.914
0.000

Table 2: The results of the gridworld example, where all ARA-DRL instances inferred an average
reward of ρπ = 5.215

equal when also swapping the actions left with up and right with down respectively. For the optimal
policy the average number to reach the goal state is 4 steps, with a reward of on average 4 and
then there is the random action which produces a reward of 10. Thus, the average reward for the
optimal policy is 5.2 and the average number of steps to reach the goal state should be 5. However,
note that we use no episodes, i.e. there is no terminal state, as otherwise the Q-Learning algorithm
fails completely by ignoring the average reward in the terminal state, and thus producing policies
with the lowest estimate in the goal state.

We initialise the learning rates α = γ = 0.01, the discount factor γ0 = 0.80 and set ǫ = 0.25,
pexp = 1.00. The learning rates and exploration are exponentially decayed as follows. α with a rate
of 0.50 in 50k steps and a minimum of 10−5, γ with rate of 0.50 in 150k steps and 10−3, and ﬁnally
the exploration with rate 0.50 in 100k steps and a minimum of 0.01. We execute 500k learning
steps before doing an evaluation run of 10k steps for which exploration and learning is disabled.
The experiment, including the learning process, is repeated 40 times with the same random number
streams over the diﬀerent setups.

Table 2 presents the summary of the results for the gridworld experiment. We have evaluated
both methods with three diﬀerent discount factors, namely 0.99, 0.999 and 1.00 for ARA-DRL and
0.50, 0.99 and 0.999 for Q-Learning. We used the Friedman test with a signiﬁcance level of p = 0.05
for a statistical analysis of the mean sum of rewards and the mean average steps until the goal step.
As expected the omnibus null hypotheses (all samples are from the same distribution) are rejected
(with 3.223e−33 for the mean reward and 1.736e−34 for the mean avg. steps to goal). Therefore, we
conducted pairwise Conover post-hoc tests adjusted by the Benjaminyi-Hochberg FDR method [3]
to reduce liberality (see the Appendix for the detailed results). Measures highlighted with the same
shade of grey are not statistically distinguishable from each other. It can be seen that all ARA-DRL
instance perform very well, outperforming Q-Learning in terms of amount of collected rewards, as
well as ﬁnding the shortest path to the goal state. More precisely all ARA-DRL instances collect on
average over all 40 experiments a reward of 5.19 per step, which is only 0.01 less than the optimum
of 5.2, while the best Q-Learning variant receives 3.44 which means that Q-Learning not even learns
to avoid to steer of the grid. The standard deviations, especially of the average number of steps
to reach the goal state, undermine the great performance of ARA-DRL as it shows how stable the
algorithm works while the Q-Learning results are rather unstable. ARA-DRL γ1 = 0.99 performs
signiﬁcantly better in terms of mean average number of steps to the goal state than the other setups,
while for the mean sum of reward all ARA-DRL instances are signiﬁcantly indiﬀerent. This is due
to the very small deviations of the former measure. Further the average steps show Q-Learning is

19

23.113

162.243

354.839

398.391

(0,0)
101.260

0
6
1
.
7
6

(0,1)

3
8
7
.
1
6

2
1
8
.
9
4

(0,2)

5
1
5
.
9
4
3

1
6
7
.
6
0
3

(0,3)

0
6
5
.
9
9
3

6
7
3
.
9
9
3

(0,4)

4
4
4
.
8
9
3

37.331

153.653

351.907

399.539

. . .

. . .

. . .

. . .

. . .

Fig. 7: This Figure shows estimated values Qπ

0.99(s, a) of Q-Learning after 1 million steps

reject
λ/(λ + µ)

reject
λ/(λ + µ)

reject
λ/(λ + µ)

reject
λ/(λ + µ)

(0, T)

accept
λ/(λ + µ)

(1, T)

accept
λ/(λ + µ)

(2, T)

accept
λ/(λ + µ)

(3, T)

accept
λ/(λ + µ)

· · ·

t
c
e
j
e
r

)
µ
+
λ
(
/
µ

t
p
e
c
c
a

)
µ
+
λ
(
/
µ

e
u
n
i
t
n
o
c

)
µ
+
λ
(
/
λ

µ)

+

reject
µ /(λ

t
p
e
c
c
a

)
µ
+
λ
(
/
µ

e
u
n
i
t
n
o
c

)
µ
+
λ
(
/
λ

µ)

+

reject
µ /(λ

t
p
e
c
c
a

)
µ
+
λ
(
/
µ

e
u
n
i
t
n
o
c

)
µ
+
λ
(
/
λ

µ)

+

reject
µ /(λ

t
p
e
c
c
a

)
µ
+
λ
(
/
µ

e
u
n
i
t
n
o
c

)
µ
+
λ
(
/
λ

µ)

+

reject
µ /(λ

(0, F)

continue
µ/(λ + µ)

(1, F)

continue
µ/(λ + µ)

(2, F)

continue
µ/(λ + µ)

(3, F)

continue
µ/(λ + µ)

· · ·

continue
µ/(λ + µ)

Fig. 8: This diagram illustrates a simple M/M/1 admission control queuing system (adapted from
Mahadevan [16,19])

unable to ﬁnd even close-to near-optimal policies. The estimated average reward by the ARA-DRL
algorithm is ρπ = 5.215 for all setups, and thus almost perfectly matches the analytically inferred
5.20. An explanation, why Q-Learning performs that poorly can be seen in Figure 7. It provides
discounted state-values as estimated by the Q-Learning γ1 = 0.99 variant after 1 Million steps of
learning. Here we can see a cluster forming at the state-actions pairs of state (0, 4), which have with
almost 400 a far higher evaluation than the goal state (0, 0) of around 100. Clearly, the surrounding
states adapt accordingly worsening the situation.

5.3 Admission Control Queuing System

Finally, like Mahadevan [16,19] we evaluate the algorithm on a simple M/M/1 admission control
queuing system. That is, we assume one server that processes jobs which arrive by an exponential
(Markov) interarrvial time distribution. Furthermore, the processing duration is assumed to be ex-
ponentially distributed. The arrival and service rate are modeled by parameter λ and µ respectively.
On each new arrival the agent has to decide whether to accept the job and thus add it to the queue,
or reject the job. In case of acceptance an immediate reward is received, which however, also incurs
a holding cost depending on the current queue size. The goal is to maximise the reward by balancing
the admission allowance reward and the holding costs. The MDP is depicted in Figure 8 and was

20

observed through uniformisation from a continuous time problem to a discrete time speciﬁcation
(see [29,4] for a description of uniformisation). The set of states consists of elements (l, Arr) with
queue length l ∈ N and a Boolean variable Arr ∈ {T, F} where Arr symbolises an arrival (T), or
no arrival (F). The edges are labelled with the corresponding action, that is, accept and reject in
case of a new arrival, or continue for continuation when no new job arrived, and the corresponding
transition probability. We deﬁne the reward function r as in [16,19] by

r((0, F), continue) = r((0, T), reject) = 0
r((l, F), continue) = −f (l + 1)(λ + µ)
r((l, T), reject) = −f (l + 1)(λ + µ)
r((l, T), accept) = [R − f (l + 1)](λ + µ)

if s = 0 ,
if l > 1 ,
if l > 1 ,

where the factor λ + µ is an artifact of the uniformisation of the continuous time problem to the
discrete time MDP.

Haviv and Puterman [11] show that if the cost function has the shape f (l) = c · l, there are at
most two gain-optimal control limit policies. Namely to admit L or L + 1 jobs. However, only the
policy that admits L + 1 jobs is also bias-optimal as the extra reward received oﬀsets the additional
cost of the extra job. Furthermore, note that in such cases the reward function can be simpliﬁed by
removing the conditions and the ﬁrst line. We use exactly this cost function in our experiment.

Further, we choose the challenging problem setup with λ = 5, µ = 5, R = 12, c = 1, and
a maximum queue length of 20 as also selected by Mahadevan in [16,19]. To allow a comparison
to the optimal solution we implemented the constraints imposed by constraint formulation of the
addends of the Laurent series expansion [22,29, p.346] using mixed integer linear programming
(MILP). The MILP result shows that L = 2, i.e. both policies of admitting 2 or 3 jobs to the
queue are gain-optimal imposing and average reward of ρπ = 30. However, only admitting 3 jobs is
also bias-optimal and with that Blackwell-optimal, as it’s the only gain-optimal policy that is left.
This makes sense, as R is only collected when an order is accepted, which is for admitting 3 jobs
immediate in contrast to the policy of admitting 2. Note that the inferred average reward of about
27.5 in [19] is sub-optimal9. The correct queue lengths for admitting 2 jobs is 0.67, while for 3 it is
1.12.

For the algorithm setup we use the same values as in the gridworld experiment, except that we
changed the ǫ-Parameter to constantly be 5 as the returned reward is signiﬁcantly higher as in the
previous example. As the problem is more complex than the previous ones we decided to perform
106 learning steps before evaluating for 100k periods.

The results are depicted in Table 5. The Friedman test rejected the null hypotheses (with
3.399e−32 for the mean reward and 2.223e−35 for the mean queue length). We performed the pair-
wise Conover post-hoc tests adjusted by the Benjaminyi-Hochberg FDR method [3] and highlighted
measures that are not statistically distinguishable from each other with the a grey background (see
the Appendix for detailed results). The ARA-DRL variants with γ1 = 1.0 and γ1 = 0.999 infer
the Blackwell-optimal policy of admitting 3 jobs and thus accumulate a reward of about 29.88
and 29.77 per step over all evaluations of the 40 replications. This clearly shows the stability of
the ARA-DRL algorithm, especially when γ1 = 1.0. However, in this example ARA γ1 = 0.99
is unable to ﬁnd the optimal policy in 12 replications and therefore performs worse as compared

9 It is possible that the reported results in [19] do not coincide with the given setup, as the queue lengths do not
match our results either. Our algorithm infers the policy of admitting 2 and queue length 0.68 if we ﬁx the average
reward to 27.5 on the above speciﬁed setup.

21

Sum Reward

Queue Length

Algorithm

Mean

StdDev Mean

StdDev

ARA-DRL γ1 = 1.0
ARA-DRL γ1 = 0.999
ARA-DRL γ1 = 0.99
Q-Learning γ1 = 0.99
Q-Learning γ1 = 0.999
Q-Learning γ1 = 0.5

2988054.750
2976862.250
2683089.250
45360.750
32609.250
24917.500

23977.493
64688.399
1021845.424
13434.404
16216.020
1467.415

1.075
1.122
1.545
0.174
0.181
0.002

0.157
0.184
1.263
0.057
0.080
0.000

Table 3: The admission control queuing system results, where the mean inferred reward of ARA-
DRL with γ1 of 1.0, 0.999 and 0.99 are 29.965, 30.272 and 28.734 respectively

to the other ARA-DRL instances. Statistically the optimal queue length of 1.12 is matched by
ARA-DRL γ1 = 0.999 and ARA-DRL γ1 = 1.0. In contrast all Q-Learning setups are unable to
ﬁnd even close-to-optimal policies, resulting in rather small amount of collected rewards and very
short mean queue lengths. Furthermore, we investigated how to get Q-Learning γ1 = 0.99 to ﬁnd
better solutions. Using ǫ-sensitive comparison, instead of the max-operator, for the action selection
process we could infer the gain-optimal policy which admits 2 jobs. However, we were unable to
infer the Blackwell-optimal policy of admitting 3 jobs and thus collects rewards as soon as possible
with Q-Learning. Similarly by hyperparameter tuning of ARA-DRL γ1 = 1.0, namely increasing
the decay rates of α and γ to 0.8 and omitting the minimum values, we were able to ﬁnd more
stable state-action values such that setting ǫ 6 1 is possible while still ﬁnding the optimal policy
and the average reward stabilizes even more. We found that the stability of the average reward is
of major importance for the stability our ARA-DRL algorithm.

6 Conclusion

This paper introduces deep theoretical insights to reinforcement learning and explains why standard
discounted reinforcement learning is inappropriate for tasks that are presented with rewards in
non-terminal steps also as they produce an average reward per step that cannot be approximated
with 0. This kind of problem structure is easily obtained in real-world problem speciﬁcations, for
instance in the ﬁeld of operations research, where companies constantly aim for proﬁt-optimal
decisions. Furthermore, we established a novel average reward adjusted discounted reinforcement
learning algorithm ARA-DRL, which is computationally cheap and deduces near-Blackwell-optimal
policies. Additionally, we implemented the algorithm and prove its viability by testing it on three
diﬀerent decision problems. The results experimentally expose the superiority of ARA-DRL over
standard discounted reinforcement learning. In the future we plan to use neural networks for function
approximation to be able to apply ARA-DRL to bigger sized problems. Adding to this we are
planning to develop an actor-critic version of ARA-DRL.

Therefore, in conclusion although machine learning has tremendously advanced and solved many
problems in many diﬀerent areas the methods may not be directly applicable to operations research.
Thus, as a researcher it is important to have broad background knowledge of the selected method
instead of handling it as a black box, as we showed that sometimes the problem structures require
an adaption of the machine learning technique for successful applications. Nonetheless, machine

22

learning and in particular reinforcement learning have been producing astonishing results over the
past decades, which when handled wisely can be adapted to our ﬁeld of research as we demonstrated
by establishing ARA-DRL. Therefore, machine learning methods will likely play an important role
in advancing the ﬁeld of operations research and its techniques over the next years, especially when
used in combination with theoretical insights of our ﬁeld.

References

1. Balaji, B., Bell-Masterson, J., Bilgin, E., Damianou, A., Garcia, P.M., Jain, A., Luo, R., Maggiar, A.,
Narayanaswamy, B., Ye, C.: Orl: Reinforcement learning benchmarks for online stochastic optimization problems.
arXiv preprint arXiv:1911.10641 (2019)

2. Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S.: Neural combinatorial optimization with reinforcement

learning. arXiv preprint arXiv:1611.09940 (2016)

3. Benjamini, Y., Hochberg, Y.: Controlling the false discovery rate: a practical and powerful approach to multiple

testing. Journal of the Royal statistical society: series B (Methodological) 57(1), 289–300 (1995)

4. Bertsekas, D.P., Bertsekas, D.P., Bertsekas, D.P., Bertsekas, D.P.: Dynamic programming and optimal control,

vol. 1. Athena scientiﬁc Belmont, MA (1995)

5. Blackwell, D.: Discrete dynamic programming. The Annals of Mathematical Statistics 344, 719–726 (1962).

DOI 016/j.cam.2018.05.030

6. Boyan, J.A., Moore, A.W.: Generalization in reinforcement learning: Safely approximating the value function.

In: Advances in neural information processing systems, pp. 369–376 (1995)

7. Chaharsooghi, S.K., Heydari, J., Zegordi, S.H.: A reinforcement learning model for supply chain ordering man-

agement: An application to the beer game. Decision Support Systems 45(4), 949–959 (2008)

8. Das, T.K., Gosavi, A., Mahadevan, S., Marchalleck, N.: Solving semi-markov decision problems using average

reward reinforcement learning. Management Science 45(4), 560–574 (1999)

9. Enns, S.T., Suwanruji, P.: Work load responsive adjustment of planned lead times. Journal of Manufacturing

Technology Management 15(1), 90–100 (2004)

10. Gijsbrechts, J., Boute, R.N., Van Mieghem, J.A., Zhang, D.: Can deep reinforcement learning improve inventory
management? performance and implementation of dual sourcing-mode problems. Performance and Implementa-
tion of Dual Sourcing-Mode Problems (December 17, 2018) (2018)

11. Haviv, M., Puterman, M.L.: Bias optimality in controlled queueing systems. Journal of Applied Probability

35(1), 136–150 (1998)

12. Hax, A.C., Meal, H.C.: Hierarchical integration of production planning and scheduling. Report, DTIC Document

(1973)

13. Howard, R.A.: Dynamic programming and markov processes. John Wiley (1960)
14. Kool, W., van Hoof, H., Welling, M.: Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475

(2018)

15. Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D.: Continuous control

with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015)

16. Mahadevan, S.: An average-reward reinforcement learning algorithm for computing bias-optimal policies. In:

AAAI/IAAI, Vol. 1, pp. 875–880 (1996)

17. Mahadevan, S.: Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine

Learning 22, 159–195 (1996)

18. Mahadevan, S.: Optimality criteria in reinforcement learning. In: Proceedings of the AAAI Fall Symposium on

Learning Complex Behaviors in Adaptive Intelligent Systems (1996)

19. Mahadevan, S.: Sensitive discount optimality: Unifying discounted and average reward reinforcement learning.

In: ICML, pp. 328–336 (1996)

20. Mahadevan, S., Marchalleck, N., Das, T.K., Gosavi, A.: Self-improving factory simulation using continuous-time
In: Machine Learning-International Workshop Then Conference-, pp.

average-reward reinforcement learning.
202–210. Morgan Kaufmann Publishers, Inc. (1997)

21. Mahadevan, S., Theocharous, G.: Optimizing production manufacturing using reinforcement learning.

In:

FLAIRS Conference, pp. 372–377 (1998)

22. Miller, B.L., Veinott, A.F.: Discrete dynamic programming with a small interest rate. The Annals of Mathemat-

ical Statistics 40(2), 366–370 (1969). URL http://www.jstor.org/stable/2239451

23. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu, K.: Asynchronous
methods for deep reinforcement learning. In: International conference on machine learning, pp. 1928–1937 (2016)

23

24. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fid-
jeland, A.K., Ostrovski, G., et al.: Human-level control through deep reinforcement learning. Nature 518(7540),
529 (2015)

25. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fid-
jeland, A.K., Ostrovski, G., et al.: Human-level control through deep reinforcement learning. Nature 518(7540),
529 (2015)

26. Nazari, M., Oroojlooy, A., Snyder, L., Tak´ac, M.: Reinforcement learning for solving the vehicle routing problem.

In: Advances in Neural Information Processing Systems, pp. 9839–9849 (2018)
27. Ok, D., Tadepalli, P.: Auto-exploratory average reward reinforcement learning.

In: AAAI/IAAI, Vol. 1, pp.

881–887 (1996)

28. Oroojlooyjadid, A., Nazari, M., Snyder, L., Tak´aˇc, M.: A deep q-network for the beer game: A reinforcement

learning algorithm to solve inventory optimization problems. arXiv preprint arXiv:1708.05924 (2017)

29. Puterman, M.L.: Markov decision processes. j. Wiley and Sons (1994)
30. Rohde, J.: Hierarchical supply chain planning using artiﬁcial neural networks to anticipate base-level outcomes.

OR Spectrum 26(4), 471–492 (2004)

31. Schneckenreither, M., Haeussler, S.: Reinforcement learning methods for operations research applications: The
order release problem. In: International Conference on Machine Learning, Optimization, and Data Science, pp.
545–559. Springer (2018)

32. Schwartz, A.: A reinforcement learning method for maximizing undiscounted rewards. In: Proceedings of the

tenth international conference on machine learning, vol. 298, pp. 298–305 (1993)

33. Schwartz, A.: Thinking locally to act globally: A novel approach to reinforcement learning. In: Proceedings of the
ﬁfteenth annual conference of the cognitive science society, pp. 906–911. Lawrence Erlbaum Associates Hillsdale,
NJ (1993)

34. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou,
I., Panneershelvam, V., Lanctot, M., et al.: Mastering the game of go with deep neural networks and tree search.
nature 529(7587), 484 (2016)

35. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D.,
Graepel, T., et al.: Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv
preprint arXiv:1712.01815 (2017)

36. Sutton, R.S., Barto, A.G., et al.: Introduction to reinforcement learning, vol. 2. MIT press Cambridge (1998)
37. Tadepalli, P., Ok, D.: Model-based average reward reinforcement learning. Artiﬁcial intelligence 100(1-2), 177–

224 (1998)

38. Veinott, A.F.: Discrete dynamic programming with sensitive discount optimality criteria. The Annals of Math-

ematical Statistics 40(5), 1635–1660 (1969). DOI 10.1214/aoms/1177697379

39. Vera, J.M., Abad, A.G.: Deep reinforcement learning for routing a heterogeneous ﬂeet of vehicles. arXiv preprint

arXiv:1912.03341 (2019)

40. Watkins, C.J.C.H.: Learning from delayed rewards. Ph.D. thesis, King’s College (1989)
41. Zhang, W., Dietterich, T.G.: A reinforcement learning approach to job-shop scheduling. In: IJCAI, vol. 95, pp.

1114–1120. Citeseer (1995)

42. Zijm, W.H.: Towards intelligent manufacturing planning and control systems. OR-Spektrum 22(3), 313–345

(2000)

24

A Statistical Results

This section provides the detailed results of the pairwise Benjaminyi-Hochberg FDR adjusted Conover statistical
analysis for the the gridworld and the admission control queuing system examples.

A.1 Statistical Signiﬁcance for the Gridworld Example

ARA-DRL

Q-Learning

γ1 = 0.99

γ1 = 0.999

γ1 = 1.0

γ1 = 0.99

γ1 = 0.999

ARA-DRL γ1 = 0.999
ARA-DRL γ1 = 1.0
Q-Learning γ1 = 0.99
Q-Learning γ1 = 0.999
Q-Learning γ1 = 0.50

ARA-DRL γ1 = 0.999
ARA-DRL γ1 = 1.0
Q-Learning γ1 = 0.99
Q-Learning γ1 = 0.999
Q-Learning γ1 = 0.50

6.723e-01
7.834e-01
2.048e-71
1.094e-75
3.893e-96

2.465e-06
5.249e-04
3.083e-89
7.900e-88
2.242e-99

7.834e-01
3.365e-70
1.361e-74
1.392e-95

1.957e-01
3.696e-79
1.531e-77
1.352e-90

8.198e-71
3.764e-75
6.353e-96

6.936e-02
2.333e-25

5.691e-82
2.041e-80
4.932e-93

4.583e-01
7.775e-08

Mean
Sum Rew.

1.148e-19

Mean
Avg. S.
to Goal

1.787e-09

Table 4: The Benjaminyi-Hochberg FDR adjusted Conover p-values for the mean sum of rewards
(top) and the mean average number of steps to the goal state (bottom)

A.2 Statistical Signiﬁcance for the Admission Control Queue System Example

25

ARA-DRL

Q-Learning

γ1 = 1.0

γ1 = 0.999

γ1 = 0.99

γ1 = 0.99

γ1 = 0.999

ARA-DRL γ1 = 0.999
ARA-DRL γ1 = 0.99
Q-Learning γ1 = 0.99
Q-Learning γ1 = 0.999
Q-Learning γ1 = 0.5

ARA-DRL γ1 = 0.999
ARA-DRL γ1 = 0.99
Q-Learning γ1 = 0.99
Q-Learning γ1 = 0.999
Q-Learning γ1 = 0.5

5.947e-01
7.002e-11
6.035e-64
1.509e-83
5.459e-94

6.556e-01
1.554e-05
4.527e-74
7.425e-69
1.606e-105

3.469e-12
3.056e-65
1.498e-84
1.061e-94

2.370e-06
4.329e-73
8.293e-68
6.136e-105

5.232e-45
7.776e-68
3.485e-80

5.639e-15
1.154e-30

7.764e-84
4.425e-79
1.763e-112

2.858e-02
1.870e-37

Mean
Sum Rew.

2.906e-07

Mean
Queue Len.

4.645e-44

Table 5: The Benjaminyi-Hochberg FDR adjusted Conover p-values for the mean sum reward
(top) and mean queue length (bottom)

26

