1
2
0
2

t
c
O
1
3

]

C
D
.
s
c
[

2
v
7
4
6
6
0
.
3
0
1
2
:
v
i
X
r
a

Compiler-Guided Throughput Scheduling for Many-core Machines

Girish Mururu
Google

Sharjeel Khan
Georgia Institute of Technology

Bodhisatwa Chatterjee
Georgia Institute of Technology

Chao Chen
Amazon

Chris Porter
Georgia Institute of Technology

Ada Gavrilovska
Georgia Institute of Technology

Santosh Pande
Georgia Institute of Technology

Abstract
Typical schedulers in multi-tenancy environments make
use of reactive, feedback-oriented mechanisms based on per-
formance counters to avoid resource contention but suffer
from detection lag and loss of performance. In this paper,
we address these limitations by exploring the utility of pre-
dictive analysis through dynamic forecasting of applications’
resource-heavy regions during its execution. Our compiler
framework classiﬁes loops in programs and leverages tra-
ditional compiler analysis along with learning mechanisms
to quantify their behaviour. Based on the predictability of
their execution time, it then inserts different types of beacons
at their entry/exit points. The information produced by bea-
cons in multiple processes is aggregated and analyzed by the
proactive scheduler to respond to the anticipated workload
requirements. For throughput environments, we develop a
framework that demonstrates high-quality predictions and im-
provements in throughput over CFS by 76.78% on an average
and up to 3.2x on Amazon Graviton2 Machine on consoli-
dated workloads across 45 benchmarks.

1 Introduction

Modern systems offer a tremendous amount of computing re-
sources by assembling a very large number of cores. However,
when they are deployed in data-centers, it is very challeng-
ing to fully exploit such computing capability due to their
shared memory resources. The challenge is that on one hand,
there is a need to co-locate a large number of workloads
to maximize the system utilization (& thus minimizing the
overall cost), and on the other hand, this workload co-location
causes processes to interfere with each other by competing for
shared cache resources. This problem is further exacerbated
by the fact that modern machine learning & graph analyt-
ics workloads exhibit diverse resource demands throughout
their execution. In particular, the resource requirements of
processes vary not only across applications, but also within
a single application across its various regions/phases as it
executes [11, 17].

Prior works on resource management & utilization [5, 6, 8,
18, 29, 31] do not account for the dynamic phase behaviour
in modern workloads and use feedback-driven reactive ap-
proaches instead. These include approaches that rely on re-
source usage history or current resource contention (moni-
tored using hardware performance counters) and then reacting
to contention by invoking suitable scheduling mechanisms.
However, majority of modern workloads are input data de-
pendent and do not exhibit highly regular or repetitive be-
havior across different input sets. Thus, neither history-based
methods nor feedback directed methods provide sufﬁciently
accurate predictions for such workloads. Furthermore, these
reactive approaches detect a resource-intense execution phase
only after it already occurred, and this leads to two more re-
lated drawbacks. First, by the time the phase is detected, it
may be too late to act, especially if the detection lag is signiﬁ-
cant or phase duration is short (a phase might be end by the
time it is detected). Second, as a result of this detection lag,
the application state might have already bloated in terms of
its resource consumption; the damage to other co-executing
processes’ cache state might be already done, and it might
be prohibitively expensive to mitigate the offending process
because of the large state and its cache afﬁnity can be lost by
migrating or pausing. In addition, these approaches cannot
predict the duration of the phase, nor the sensitivity of its
forthcoming resource usage, thus signiﬁcantly limiting the
chances of further optimizations to co-location of processes.

To efﬁciently co-locate processes and maximize system
throughput, we propose Beacons Framework to predict the
resource requirement of applications based on a combina-
tion of compiler analysis and machine learning techniques
that enables pro-active scheduling decisions. This frame-
work consists of Beacons Compilation Component, that
leverages the compiler to analyze programs and predict the
resource requirements, & Beacons Runtime Component,
which includes the pro-active Beacons Scheduler, that uses
these analysis to undertake superior scheduling decisions.
The beacons scheduler relies on the compiler to insert “bea-
cons”(specialized markers) in the application at strategic pro-

 
 
 
 
 
 
gram points to periodically produce and/or update details of
anticipated (forthcoming) resource-heavy program region(s).
The beacons framework classiﬁes loops in programs based
on cache usage and predictability of their execution time and
inserts different types of beacons at their entry/exit points.
Moreover, the beacons scheduler augments imprecise infor-
mation through the use of performance counters to throttle
higher concurrency when needed. The novelty of this scheme
is that through the use of loop timing, memory footprint, and
reuse information provided via compiler analysis combined
with machine learning mechanisms, the scheduler is able to
precisely yet aggressively maximize co-location or concur-
rency without incurring cache degradation.

We evaulated Beacons Framework on a Amazon Gravi-
ton2 [1] machine on consolidated benchmark suites from
Machine Learning Workloads [7, 14–16], PolyBench [33], a
numerical computational benchmark, and Rodinia [4], a het-
erogeneous compute-based kernel based on diverse domains.
Our experiments demonstrate that the end result of such a high
throughput oriented framework is that it effectively schedules
a large number of jobs and improves throughput over CFS
(Completely Fair Scheduler, a production scheduler bundled
with Linux distributions) by 76.78% on average (geometric
mean) and up to 3.29x (§5). Overall, the main contributions
of our work can be summarized as follows:

• We present a compiler-directed predictive analysis frame-
work that determines resource requirements of dynamic
modern workloads in order to aid dynamic resource man-
agement.

• We show that an application’s resource requirement can
be determined by its loop-timings (§3.1) which in turn
depend upon the prediction of expected loop iterations
(§3.1.2), memory footprints (§3.2) and data-reuse be-
haviour (§3.2.2). We show how these attributes can be
estimated by combining compiler analysis with machine
learning techniques in Beacons Compilation Component
(§3).

• To predict precise loop timing for proactive schedul-
ing decisions, we develop a detailed loop classiﬁcation
scheme that covers all possible types of loops in modern
workloads. Based on this scheme, we develop a frame-
work that analyzes such loops by capturing both the
data-ﬂow and control-ﬂow aspects (§3.1.2).

• We design and implement a lightweight Beacons Run-
time Component (§4), which includes the proactive
throughput-oriented Beacons Scheduler (§4.1), that ag-
gregates the information generated by multiple processes
and allocate resources to the processes and enables com-
munication between the application and the scheduler.

2 Motivation & Overall Framework

Maximizing system throughput by efﬁcient process schedul-
ing in a multi-tenant execution setting (data centers, cloud,
etc) for modern workloads is challenging. The reason for this
is that these execution scenarios consists of thousand of jobs,
typically in batches. In such cases, completion time of each
individual job itself is of no value, but the completion time of
either whole batch of jobs, or the number of jobs completed
per unit time (if the jobs are an incoming stream) is of utmost
priority [2].

2.1 Motivating Example: Reactive Vs Proac-

tive

Consider an execution scenario in a multi-tenant enviroment,
where we have the popular convolutional neural network
(CNN) Alexnet, being trained on different input sets with var-
ied parameters (dropout rate, hidden units, etc) concurrently,
as a batch. During the training, several other jobs (matrix mul-
tiplication computations on different matrices) co-execute,
having access to the same shared resources (LLC, etc), but
on different cores. Table 1 shows the comparison of execu-
tion time of this scenario with (1) widely used production
Linux scheduler CFS [19], (2) a performance-counter based
reactive scheduler Merlin [29] and (3) proposed Beacons
Framework with Beacons Scheduler.

Constituent Processes
executing as a Batch
Alexnet Training
Matrix Multiplication (2mm)

Total Processes

Execution Time (sec)
CFS

Merlin Beacons

20
133768

249.43

358.23

100.58

Table 1: Alexnet Training and 2mm (Polybench) executing
as a batch. The 2mm process can be considered as a small
process compared to Alexnet, and hence can act as hogging
system resources with sheer numbers

As we can see from Table 1, the schedulers like CFS that
are agnostic to the diverse requirements and overlap in re-
source requirements can suffer from slowdowns as high as
1.5x. On the other hand, scheduler that use feedback-driven
approach ﬁrst uses performance counters to measure IPC/-
Cache miss and then undertakes scheduling decision perform
even worse, having performance degradation of more than
3.5x. The reason for this that Merlin [29] reacts to the re-
source requirements and suffers due to detection lag and other
reasons mentioned earlier. Therefore, in order to maximize
throughput, we need to act pro-actively, i.e anticipate the re-
source demand before it happens and act on its onset, which
forms the primary objective of Beacons Framework.

2.2 Overall Framework

Beacons framework consists of a Compilation Component
(Fig. 1), which in turn comprises of a sequence of compi-

2

lation, training, and re-compilation steps to instrument the
application; all of these steps are applied statically. Several
challenges are overcome by to produce the above informa-
tion at loop entrances. Beacons consist of statically inserted
code at the entrances of the loops that evaluate closed-form
formulae & static learning models for predicting various ar-
tifacts related to loop’s resource requirement. Other aspects,
e.g. the amount of time a region executes, can be predicted &
enhanced by employing machine learning mechanisms that
involves training. During runtime this information is evalu-
ated by beacons using actual dynamic values and is conveyed
to the beacons scheduler through library calls, which form
the Beacons Runtime Component (Fig. 1). The scheduler
aggregates and analyzes this information across all the pro-
cesses, constructing a global picture about contentions, the
sensitivity to them, and moreover the duration of contentions.
For the purposes of this paper, the resource under contention
is the last level cache and memory bandwidth shared by all
the processes in the machine. Since the resource demand and
duration (loop timings) information is known to the scheduler
before the loop executes, the scheduler is able to take pro-
active scheduling decisions overcoming the afore-mentioned
limitations of reactive approaches.

3 Beacons Compilation Component

The Beacons Compilation Component (Fig. 1) is responsi-
ble for instrumenting beacons in an application in order to
guide a scheduler through upcoming regions. The regions in
code are analyzed at the granularity of loop nests. During
the compilation phase, the applications are ﬁrst proﬁled to
generate sample loop timings through a compiler pass called
the beacons loop analysis pass. This pass runs the program
on a subset of application inputs, which form the training
input set. These timings are then used for training a linear
regression equation which establishes the loop timing as a
function of the total iterations of the loop (trip-count). Such
an approach directly handles simple loops whose trip-counts
can be predicted statically from its loop bounds. However, to
enable analysis of loops whose trip-counts are unknown at
compile time, we develop a loop categorization framework
that uniﬁes both data and control ﬂow of the loop bounds and
its predicates variables. We then predict the trip counts for
such loops by using decision tree classiﬁer and rule-based
mechanisms, which is futher integrated with the loop timings
to enhance its accuracy. The timing regression equation is
then embedded before the loop, along with two other pieces:
the memory footprint equation and the classiﬁcation of the
loop as either “reuse” or “streaming”, based on its memory
reference patterns. The memory footprints are estimated by
polyhedral compiler analysis [13], while the reuse classiﬁ-
cation is done with the help of static reuse distance (SRD)
analysis.

The beacons are inserted before the outermost loops and

then hoisted inter-procedurally. To rectify potential schedul-
ing errors arising from timing and memory footprint inaccu-
racies, the beacon-based scheduler is augmented with perfor-
mance counters which are used in cases of loop bounds which
can not be estimated even by machine learning; in addition a
completion beacon is inserted at loop exit points to signal the
end of the loop region. We now describe each of these above
aspects in detail in the following subsections.

3.1 Loop Timing Analysis & Categorization

In order to predict the timing of a loop (nest) present in the
application, we establish it as a function of its total iterations
(trip-count). Our goal is to obtain a linear relationship of form
y = ax1 + bx2 + .. + kxn, where x1, x2, ..., xk are trip-counts of
individual loops in a loop nest and y is the execution time of
the loop nest. The core-idea here is that the constants a, b, .., k
can then be learnt using linear-regression and this closed-form
equation can be directly utilized to generate the loop-timing
dynamically during runtime.

3.1.1 General Loop Modelling

For a given loop with various instructions, the loop time
depends on the number of loop iterations, i.e. loop time
(T ) is directly proportional to loop iterations (N). There-
fore, T ∝ N ⇒ T = α ∗ N, where proportionality constant
α which represents the average execution time of the loop
body. In a nested loop, the execution duration depends on
the number of iterations encountered at each nesting level.
Consider a loop nest consisting of n individual loops with
total iterations N1, N2, ..., Nn respectively. We observe that
the loop nest duration (T ) is a function of these loop iter-
ations, i.e T = f (N1, N2, ..., Nn). As far as the loop body is
concerned, each instruction in the loop nest contributes to
the loop time by the factor of the number of times the in-
struction is executed. Thus, a loop can be analyzed by de-
termining the nesting level of each of the basic blocks in its
body and then multiplying the timing of that basic block with
its execution frequency determined by the trip counts of sur-
rounding loops. Thus, timing equation can be represented as:
T ∝ g1(N1) + g2(N1 ∗ N2) + ... + gn(N1 ∗ N2 ∗ ... ∗ Nn), where
g1(N1) is the function of time contributed by outermost loop
or the basic block belonging to the outermost loop with N1
iterations, g2(N1 ∗ N2) is the function of time contributed by
the second loop at the nesting level 2 and so on. Removing
the proportionality sign, this equation can be rewritten as:

T = c0 + c1 ∗ N1 + c2 ∗ (N1 ∗ N2) + ... + cn ∗ (N1 ∗ N2...Nn)
(1)
where c0 is the constant term. Eq. 1 is a linear equation in
terms of each loop iteration Nk. Therefore, we use linear re-
gression to learn the coefﬁcients for each of the loop iterations
in the loop nest to predict the loop timings.

3

Figure 1: Beacons Compilation Component Workﬂow. The source code is translated into IR with LLVM front-end and is subjected to various
‘beacon’ compiler passes to generate the loop and memory proﬁle information. The IR is then instrumented with beacons information to
broadcast information at the runtime to the Beacons Scheduler with the help of Beacons library.

Figure 3: Examples of different instances of loop for predict-
ing trip counts. Each of these instances can be generalized to
nested loops as well.

Loop Classiﬁcation Scheme In order to obtain trip counts
for various loop-nests, we ﬁrst categorize the loops by devel-
oping a classiﬁcation scheme that takes into account both the
data-ﬂow & control-ﬂow characteristics of a loop. The data-
ﬂow characteristics of a loop can be determined by the loop
bounds, while the control ﬂow aspect can be captured by the
nature of loop termination. Thus, each individual loops can
either be Normal-bounded/Irregularly-bounded loops (data
ﬂow aspect) or Normal-Exit/Multi-Exit (control ﬂow aspect)
loops. Unifying the data-ﬂow & control-ﬂow characteriza-
tion results in a comprehensive framework that covers all
possible loops types in real workloads (Fig. 4) The resul-
tant four classes of loops are Normally Bounded Normal Exit
(NBNE) loops, Normally Bounded Multi Exit (NBME) loops,
Irregularly Bounded Normal Exit (IBNE) loops & Irregularly
Bounded Multi Exit (IBME) loops.

Based on this classiﬁcation scheme, we develop a Loop
Categorization Algorithm (Algo. 1) that detects loops for
each categories and invokes specialized trip count estimation
mechanisms for each case. This algorithm ﬁrst checks the
bounds of the given loop. If the bound is entirely composed
of numerical entities (integer, ﬂoat, double, etc.), which al-
lows a static estimation of symbolic upper bound the loop is
classiﬁed as Normal-Bounded. Alternatively, loops for which
bounds contain non-numerical entities (pointer references,
function calls, etc.) are considered as Irregularly-Bounded
Loops. To handle such cases, we propose Upwards Exposed
Control Backslicing (UECB) Algorithm (Algo. 2), which is
a general framework that predicts trip counts based on the
‘upward-exposed’ values of variables that decide the control

Figure 2: Beacons Runtime Component

3.1.2 Estimating Precise Loop Iterations

: The loop timing model described above depends on the
compile time knowledge of the exact loop iterations, i.e the
trip-count associated with the loop. For certain kinds of regu-
lar loops, it’s simple to statically determine the total number
of iterations for loops with regular bounds (Fig. 3a). Such
loops can be normalized using the loop-simplify pass in the
LLVM [21] compiler. Loop normalization converts a loop to
the form with lower bound set to zero and step increment by
one to reach an upper bound. The upper bound of the loop is
now equal to the number of loop iterations and can be easily
extracted from the loop and plugged into Eq. 1 for timing
prediction. In practice, however, loops need not adhere to this
template. Real-world workloads have loops with irregular
bounds (Fig. 3b) and non-uniform trip count that are often
input-dependent, which makes it harder to analyze their trip
count statically. This also extends to loop nests (either with
afﬁne or non-afﬁne bounds), having control ﬂow statements
that break out iterations transferring the control out of the loop
body (Fig. 3c). Therefore, to estimate precise loop iterations
we need to develop a framework that captures both input data
ﬂow characteristics and control ﬂow behaviour of the entire
loop nest.

4

Algorithm 1: Loop Categorization Algorithm

Input: Function F
Result: Determine the precise trip count (in terms of expression or

linear model) for all the loops

for each loop L ∈ Function F do
regular, normal ← true
exit_points, critical_bounds, critical_predicates ← φ
bounds ← L.getBounds()
for each argument arg ∈ bounds do
if type(arg) != num then
regular ← f alse
critical_bounds.push_back(arg)

end

end
for each basicblock bb ∈ L do

if (bb.successor /∈ L) and (bb != L.header) then

normal ← f alse
exit_points.push_back(bb)
for each point P ∈ exit_points do

for each variable v ∈ P do

critical_predicates.push_back(v)

end

end

end

end
if regular and normal then

tripCount ← (bound.upper() − bound.lower())

end
if regular and not normal then

PredictionModel ← UECB(critical_predicates)

end
if not regular and normal then

PredictionModel ← UECB(critical_bounds)

end
if not regular and not normal then

PredictionModel ←

UECB(critical_bounds ∪ critical_predicates)

end

end

set for the entire program is divided into two parts for training
& testing respectively. This model is then embedded in the
beacon associated with the outer-loop and is used to predict
the precise loop trip count when it’s invoked.

Loops not suitable for machine learning: UECB Algo-
rithm generates training data by logging the values of critical
variables upon various loop invocations. However, if the loop
is invoked only small number of times (< 10), then it’s im-
practical to train decision trees for predicting trip-counts of
such loops. This is because machine learning models require
the training and testing dataset to be sufﬁciently large in or-
der to provide meaningful predictions. Thus, for predicting
trip-counts of loops that are not invoked enough times to train
a classiﬁer model, we use Rule-Based Trip Count Predic-
tion. The core idea here is that expected trip-count is within
the one standard deviation of the mean trip-count of all the
rules. The rule-based mechanism is preferred over classiﬁer
model, when the number of data-points are lesser than hyper-
parameter threshold value (∼ 5).

After the expected trip count for various classes of loop are

5

Figure 4: Loop Classiﬁcation Scheme with 4 classes of loop:
NBNE, NBME, IBNE & IBME

ﬂow in the loop, using learning mechanisms. It returns a learn-
ing model that estimates the loop trip counts based on a loop’s
critical variables.

After the detection of expected trip counts based on loop
bounds, the loop bodies are analyzed for control-ﬂow state-
ments. If there are no statements in the loop body that exit
out of the loop for Normal-Bounded case, the loop is catego-
rized as Normal-Exit and the expected trip count is equal to
symbolic trip count. Next, if the loop is Irregularly-Bounded,
its expected bound is learnt using UECB algorithm described
below. The cases of multiple exits in the body are classiﬁed
as Multi-Exit and the critical variables present in the predi-
cates are extracted and passed on to the UECB algorithm to
generate the respective model.

UECB Algorithm: The ﬁrst step in UECB is to identify
critical variables that dictate a loop’s trip count, i.e variable
that decides whether the control ﬂow will go to the loop
body or loop exit. These variables can be either irregular loop
bounds, or part of predicates that break out of loop. The core
idea here is to use a learning model that can be trained on
these variables to estimate the loop’s trip count. The trained
model can then be embedded in the beacon associated with
the loop nest. However, since the inserted beacon exists at
the entrance of loop nests, the trip count prediction must be
done only with the variables & their deﬁnitions that are live
at that point. Such variables are called Out-of-loop variables,
i.e these variables are live at outermost loop header and their
deﬁnitions that come from outside the loop body. Therefore,
it is essential to back-propagate the critical variables in order
to express them in terms of Out-of-loop variables. UECB
achieves this back-propagation by following a stack-based
approach that involves analyzing the critical variables and
the upwards-exposed deﬁnitions on which they depend in
terms of their backward slice. When the program is run on
representative inputs, these out-of-loop variables are logged
to generate the training data for the learning model.

The learning model used to predict loop trip counts based
on critical variables is the decision tree classiﬁer. The input

Algorithm 2: Upwards Exposed Control Backslicing
(UECB) Algorithm

Input: Set of Critical Variables C
Result: Classiﬁer Model M that estimates precise trip count based

on unseen values of critical variables

model_parameters, worklist = {}
for each variable vc ∈ C do

de f _set ← getallDe f initions(vc)
for each deﬁnition d ∈ de f _set do

worklist.push_back(d)

end

end
while worklist is not empty do

d ← remove a deﬁnition from the worklist
for each operand op ∈ d do

if op is out-of-loop variable then

model_parameters.push_back(op)

else

de f _set ← getallDe f initions(op)
for each deﬁnition d ∈ de f _set do
worklist.push_back(op)

end

end

end

end
Generate Training & Testing data for model_parameters
if Total Data Points > Threshold then

Train Decision-Tree Classiﬁer M(model_parameters)

else

end

Obtain Rule-based Prediction Model M(model_parameters)

obtained, either by simple analysis (NBNE) or by classiﬁer
models (NBME/IBNE/IBME) from the Loop Categorization
algorithm (Algo. 1), they are integrated with timing equation
(Eq. 1) to enhance the loop-timing predictions through normal
regression models.

3.2 Memory Footprint Analysis

The footprint indicates the amount of cache that will be oc-
cupied by a loop. Memory footprint analysis consists of two
parts - (a) calculating the memory footprint of the loop, and
(b) classifying a loop as a reuse-oriented loop or a streaming
(which exhibits little or no reuse) loop.

3.2.1 Calculating Memory Footprint

For a given loop, its memory footprint is estimated based on
polyhedral analysis, which is a static program analysis per-
formed on LLVM intermediate representation (IR). For each
memory access statement in the loop, a polyhedral access rela-
tion is constructed to describe the accessed data points of the
statement across loop iterations. An access relation describes
a map from the loop iteration to the data point accessed in
that iteration. It contains three pieces of information: 1) pa-
rameters, which are compile-time unknown constants, 2) a
map from the iteration to array index(es); and 3) a Presburger
formula [30] describing the conditions when memory access

is performed. Generally, parameters contain all loop-invariant
variables that are involved in either array index(es) or the
Presburger formula, and the Presburger formula contains loop
conditions. We currently ignore if-conditions enclosing mem-
ory access statements; we thus get an upper bound in terms
of estimation of the memory footprints. For illustration, list 1
shows a loop with three memory accesses, with two of them
accessing the same array but different elements. A polyhedral
access relation is built for each of them. The polyhedral access
relation for d[2 ∗ i] is: [N] → {[i] → [2 ∗ i] : 0 <= i <= N},
where [N] speciﬁes the upper-bound of the normalized loop.
It is a compile-time unknown loop invariant since its value
is not updated across loop iterations. [i] → [2 ∗ i] is a map
from the loop iteration to the accessed data point (simply
array indexes). 0 <= i <= N is the Presburger formula with
constraints about when the access relation is valid.

Listing 1: Memory Footprint Estimation Example

i <= N; ++ i

) {

i n t

(
i = 0 ;
. . . = a [ i + 3 ] ;
d [ 2 * i ] = . . . ;
d [ 3 * i ] = . . . ;

f o r

}

Based on the polyhedral access relations constructed for
every memory access in the loop, the whole memory foot-
print for the loop can be computed leveraging polyhedral
arithmetic. It simply counts the number of data elements in
each polyhedral access relation set, and then adds them to-
gether. Instead of a constant number, the result of polyhedral
arithmetic is an expression of parameters. For d[2 ∗ i], its
counting expression generated using polyhedral arithmetic is:
[N] → {(1 + N) : N >= 0}. Therefore, as long as the value
of N is available, the memory footprints of the loop can be
estimated by evaluating the expressions. In our framework,
the value of N is given by the expected trip count (through one
of the ﬁve cases: NBNE or by classiﬁer models (NBME/IB-
NE/IBME) or rule-based system as described in the last sec-
tion). For statements that access the same arrays, e.g. d[2 ∗ i]
and d[3 ∗ i], a union operation will ﬁrst be performed to cal-
culate the actual number of accessed elements as a function
of compile-time unknown loop iterations and instrumented
in the program. It is evaluated at runtime to get the expected
memory footprint.

3.2.2 Classifying Reuse and Streaming Loops

A loop that reuses memory locations over a large number of
iterations (large reuse distance) needs enough cache space
to hold its working memory and might be sensitive to the
misses caused, and a loop that streams data which is reused
in next few iterations require almost no cache space at all and
might be insensitive due to a small ﬁxed reuse distance. For
efﬁcient utilization of cache, the scheduler must know this

6

classiﬁcation. We classify the loops using Static Reuse Dis-
tance (SRD), deﬁned as the number of possible instructions
between two accesses of a memory location. For example,
in Fig. 5 the SRD between statements (S1, S2) is in the order
of m ∗ 3 because an access in instruction S1 has to wait for
m instructions within the inner loop to cover the distance of
three outer iterations between successive access of the same
memory location. The SRD between statements (S5, S6) is in
the order of two, because the same memory is accessed after
two iterations.

Figure 5: SRD Estimation for two loop nests. Loop Nest 1 is classi-
ﬁed as reuse, while Loop Nest 2 is streaming.

Any loops with a constant SRD, that is the distance be-
tween the accesses is covered within a few iterations of the
same loop (e.g. the one between statements (S5, S6), (S5, S7)
and (S6, S7) in Fig. 5b), can be classiﬁed as streaming, be-
cause the memory locations must be in the cache for only a
few (constant) iterations of the loop during which it is highly
unlikely to be thrashed. More speciﬁcally, an SRD that in-
volves an inner loop (e.g. the one between statements (S1,
S2) in Fig. 5a) or outer loop (e.g. between statements (S3, S4)
in Fig. 5a) where a cache entry must wait in the cache over
the duration of the entire loop that it is dependent on – such
loops are classiﬁed as reuse. For example, array B must be
in cache for the duration of the entire outer loop. Thus, we
classify such loops in which the SRD is dependent on either
an outer or inner loop as reuse loops (reuse distance here is a
function of normalized loop bound N, for example), and we
classify the remaining loops (with small and constant reuse
distance) as streaming loops. Indirect and irregular references
such as a[b[i]] do not typically have a large reuse distance as-
sociated with them (compared to the sizes of modern caches)
and it is impossible to analyze them at compile time; in our
current approach, they are classiﬁed therefore as non-reuse
references.

3.3 Beacons Hoisting & Insertion

The beacon insertion compiler pass ensures that the bea-
cons are hoisted at the entrances of outermost loops intra-
procedurally. However, inter-procedural loops (function calls
in loop body) can overload the scheduler with too many bea-
con calls. Hence, if the beacons are inside inter-procedural
loops, then they are hoisted outside and also above the other
call sites that are not inside loops along all paths. For hoist-
ing the beacon call (& embedding the inner-loop attributes),

7

the inner loop bounds may not be available (or live) at the
outermost points inter-procedurally. Thus, use expected loop
bounds of the inner loops to calculate the beacon properties,
timing information and memory footprint. Each interproce-
dural inner loop’s memory footprint is added to the outer-
most inter-procedural loop’s memory footprint. On the other
hand, the loop timing is based only on the outermost inter-
procedural loop. The beacon is classiﬁed as reuse if there is a
single interprocedural nested loop. Unfortunately, such a con-
version transforms many known/inferred beacons to unknown
beacons. Also, hoisting is a repetitive process that stops once
no beacons are inside inter-procedural loops.

The decision trees are inserted as if-then-else blocks with
the trip count value being passed to both the loop timing
model and the memory footprint formula. The equations with
the coefﬁcients and loop bounds are instrumented at the pre-
header of the corresponding loop nests, followed by the mem-
ory footprint calculations. The variables that hold the timing
and memory footprint values along with loop type (reuse or
streaming) and beacon type are passed as arguments to the
beacon library calls. Facilitated by the beacon library, the
instrumented call ﬁres a beacon with loop properties to the
scheduler. We use shared memory for the beacon communica-
tions between the library and the scheduler. For every beacon,
a loop completion beacon is inserted at either the exit points
of the loop nest or after the call site for beacons hoisted above
call site. The completion beacon sends no extra information
other than signaling the completion of the loop phase and so
that any sub-optimal scheduling decision can be rectiﬁed.

4 Beacons Runtime Component

After the beacons and its attributes (loop timing regression
models, trip count classiﬁers, memory footprint calculations
and reuse classiﬁcation) are instrumented in the application,
it’s the job of the Runtime Component to evaluate & com-
municate the attributes during the execution. These evaluated
attributes are communicated to the scheduler through a li-
brary interface that communicates with the scheduler. We
refer to these function calls to the library as “beacon calls”.
A beacon call ﬁred from an application writes the beacons
attributes to a shared memory which is continuously polled
by the scheduler. Beacon Scheduler analyzes this beacon in-
formation and acts proactively to the resource requirements
among the co-executing processes, which sets it apart from
state-of-art schedulers like CFS. This establishes the com-
munication between applications and scheduler (no special
privileges required), and processes that can write to this shared
memory are agreed upon during initialization with a key. The
scheduler arbitrates the co-executing processes to maximize
concurrency while simultaneously addressing the demand on
the shared resources such as caches and memory bandwidth.
Then goal of the scheduler is to facilitate efﬁcient cache and
memory bandwidth usage to improve system job throughput.

Types of Beacon Calls: Based on the system and appli-
cation’s execution state, there can be three distinct beacon
library calls: Beacon_Init (to initialize the shared memory
for writing the attributes), Beacon (this writes the beacon at-
tributes the shared memory) & Loop_Complete (this signals
the end of a loop for a process).

Beacons Call Classiﬁcation: Based on the precision of
the attribute information, the beacon calls can either be clas-
siﬁed into Known/Inferred Beacon Calls, where the loop
trip-counts, timing and memory footprints are calculated via
closed-form expressions with high accuracy, and Unknown
Beacon Calls, where the attribute information is non-exact,
expected values mainly calculated by rule-based trip-count
estimation. This distinction helps us to identify potential im-
preciseness in application’s resource requirement and allows
us rectify certain sub-optimal scheduling decisions.

4.1 Beacons Scheduler (BES)

The beacon information sent by the applications is collected
by the scheduler to devise a schedule with efﬁcient resource
usage. The beacon scheduler dynamically operates in two
modes based on the loop data-reuse behaviour - reuse or
streaming mode. The two modes corresponding to two types
of reuse behaviour is to mitigate the adverse effects (resource
demand overlaps) caused by multiple loops that co-execute
together. Initially, the scheduler starts without a mode and
schedules as many processes as required to ﬁll up all the
processors (cores) in the machine. One primary objective of
the scheduler is to maximize system resource utilization and
never keep any processors idle.

The scheduler enters one of the two modes based on the
ﬁrst beacon it collects. Until the beacon call is ﬁred, a process
is treated as having no memory requirement and is referred to
as a non-cache-pressure type process. During the non-cache-
pressure phase, the processes typically have memory footprint
lower than the size of L1 private cache and do not affect
the shared cache, unlike streaming or reuse type with cache
requirements exceeding the L1 cache. Based on the timing
information received from an incoming beacon, the scheduler
estimates the time by which a certain loop should complete its
execution. An important point to note here is although loop
time values are obtained by compiling the process in isolation,
the loop timing will still be similar even with multi-tenancy
when scheduled by the beacon scheduler. This is because
the scheduler ensures the avoidance of contention among the
processes as detailed below.

When any process ﬁres a beacon, three possible timing
scenarios (Fig 6) can occur. In the ﬁrst case, the completing
(currently executing) beacon and the incoming beacon do
not overlap (Fig 6 (right)). The completing beacon will relin-
quish its resources, which will be accordingly updated by the
scheduler. The incoming beacon is then scheduled based on
the resource availability.In the second case, the completing

beacon and incoming beacon overlap for greater than 5-10%
(conﬁgurable) of the execution time (Fig 6 (middle)). Here, if
the resources required by the incoming beacon is more than
the available resources, then the incoming beacon process is
descheduled and replaced with another process. Finally, in the
third case where the overlap is less than 5-10%, if the incom-
ing beacons’ resource requirement is satisﬁed on completion
of the earliest beacon process, then the process is allowed to
continue but with performance monitoring turned on. How-
ever, if the IPC of the beacon processes degrade, then the
incoming beacon process is descheduled. Also, if the informa-
tion is known to be imprecise (unknown beacons), then the
scheduler turns on performance counters to rectify its actions.
In each case the resource is either last level cache in reuse
mode or memory bandwidth in stream mode.

Figure 6: Different timing Scenarios of Incoming Beacon

Reuse Mode: The goal of the scheduler in reuse mode is
to effectively utilize the cache by minimizing the execution
overlap between the processes that are reuse bound. At any
given time, the cores in the machine may be executing a mix
of reuse loops (RJ) that ﬁt the cache and non-cache-pressure
(FJ) applications only as shown in the scheduler mealy ma-
chine Fig 7. If any of these FJ processes ﬁres a reuse beacon
(RB), the scheduler ﬁrst checks the memory information to
ensure if the beacon ﬁts in the available cache space or not.
The scheduler only allows the process to continue if it ﬁts
in the available cache space. Once the reuse loop completes
(loop_completion beacon call), the process is re-classiﬁed as
FJ again. If a FJ process ﬁres a streaming beacon (SB), then
the process is suspended and replaced by a suspended reuse
process that ﬁts in the cache. If no such suspended reuse pro-
cess exists, then a non-cache-pressure process is scheduled.
When all reuse loops are completed (RC) or the number of
suspended stream loops hits a threshold (ST) (typically 90%
of the number of cores in the machine), then the remaining

Figure 7: A simpliﬁed Mealy state machine of the bea-
con scheduler. Key: Beacon, Reuse, Streaming, Filler, Job,
Complete, Threshold, dequeue, enqueue, $(cache)

8

RJ processes are suspended, if any, and all the SJ processes
are resumed and the scheduler switches to stream mode.

Stream Mode: A stream loop does not reuse its memory
in the cache and hence is not disturbed by other co-executing
processes as long as the system memory bandwidth is suf-
ﬁcient. The expected (mean) memory bandwidth (µbw) of
a stream loop can be calculated by using the memory foot-
print and the timing information as: µbw = MemoryFoot print
. In
stream mode the scheduler schedules the SJ processes by re-
placing all other processes (RJ and FJ) as long as the Total
Mean Memory Bandwidth (Tµbw) is less than the memory
bandwidth of the machine.

LoopTime

Any remaining core can only be occupied by a FJ process
because a RJ process will get thrashed by the streaming ap-
plications. If a streaming loop completes, then it is replaced
by a suspended streaming process when memory bandwidth
is available. Otherwise, the process is allowed to continue as
long as it does not ﬁre a reuse beacon (RB). In other words,
any non-streaming, non-cache-pressure application ﬁring a
reuse beacon is suspended and replaced by either a suspended
streaming process or a non-cache-pressure application. When
the number of such suspended reuse processes hits a thresh-
old (RT), which is typically 10% of the number of cores in
the machine and based on whether the reuse processes can
ﬁll the cache, the scheduler switches from stream mode to
reuse mode. An execution scenario is possible in which all
streaming processes get suspended, all reuse processes are
run, then after suspending more streaming jobs, all streaming
processes are scheduled again in a batch, and so on.

5 Evaluation

Our goal is to evaluate Beacons Framework in environments,
where throughput in terms of number of jobs completed per
unit time or the time for completion of the batch is important
and latency of each individual process itself is of not much
value. A common example of such a scenario is a server con-
ducting biological anomaly detection on thousands of patient
data records (each patient as a new process) and with the
goal to complete as many as possible to discover a particular
pattern [12].

Experimental Platform: The experiments were con-
ducted on a Amazon Graviton2 (m6g.metal) machine, running
Linux Ubuntu 20.4. The system has one socket with 64 proces-
sors and 32 MB L3 cache. We carried out our experiments on
60 processors, leaving the rest for other system applications
to run smoothly and not interfere with our results. Beacons
Compilation Component was implemented as uniﬁed set of
compiler and proﬁlling passes in LLVM 10.0. Machine learn-
ing library scikit-learn was used to implement the classiﬁer
models.

Benchmarks: We evaluated our system on four sets of di-
verse workload suites, consisting of 53 individual benchmarks.
We perform our experiments on: PolyBench [33], a numeri-

cal computational suite, consisting of linear algebra, matrix
multiplication kernels, Rodinia [4], which consists of graph
traversal, dynamic programming applications from diverse
domains like medical imaging, etc and on various popular ma-
chine learning benchmarks like AlexNet [15], DenseNet201
[16], Resnet101 [14], Resnet-18, Resnet-152, and VGG16 [28]
by training these networks on a subset of CIFAR-10 Dataset
[20]. In addition to that, we perform experiments on well-
known pre-trained networks like TinyNet, Darknet and RNN
(with 3 recurrent modules) for predicting data samples from
CIFAR-10, Imagenet [7] and NLTK [3] corpus respectively.
Methodology: We run experiments and report all the
benchmarks in Polybench. However, since the L1 data cache
size is 32KB, the beacon calls are ﬁred only if the loop mem-
ory footprint is above 32KB and if the loop predicted time
is above 10ms. This is neccessary because the average pro-
cessing time of loop complete, reuse, and stream beacons
are 116us, 427us, and 292us respectively, and thus the loop
timings below 10ms would be unneccessary for scheduling.
Leukocyte in Rodinia is one such benchmark with all beacons
statically removed because the expected memory footprint
is lower than 32KB and hence we do not report the values
here. Our ML workloads were created using Darknet [25] and
they were divided into training benchmarks and prediction
benchmarks. The training benchmarks are well-known convo-
lutional neural networks (CNNs) and we ran them by training
on a subset of Cifar-10 images. The prediction benchmarks
are pre-trained models that were used to classify images from
Imagenet dataset. For all the benchmarks, input sets were
divided into training & test sets to obtain the loop-timing
(regression) & trip-count (classiﬁcation) models. Table 2 sum-
marizes all the benchmarks used in our experiments. All the
benchmarks were compiled with their defaults optimization
ﬂags (-O2/O3, -pthread, etc). Designing Scheduling Jobs:

Benchmark Suite

Benchmarks

Polybench

Rodinia

Convolutional Neural Network
(Training)
Convolutional Neural Network
(Pre-Trained)

2mm, 3mm, atax, bicg, mvt, gemm,gesummv,
symm, syr2k, syrk, trmm, cholesky, lu,
ludcmp, trisolv, correlation,covariance,
ﬂoyd-warshall, nussinov, deriche, adi, fdtd-2d,
heat-3d, jacobi-1d, seidel-2d

backprop, bfs, cfd, heartwall, hotspot,
hotspot3D, kmeans-serial, lavaMD, nn,
particleﬁlter, srad_v2
Alexnet, Resnet-18, Resnet-101, Resnet-151,
VGG-16, Densenet-201

TinyNet, Darknet3

Dataset
SMALL
STANDARD
EXTRALARGE
(Training)

LARGE (Testing)

Custom Inputs (Training
& Testing)

CIFAR-10 (Training
& Testing)
Imagenet (Training
& Testing)

Table 2: Summary of Benchmarks used in our experiments

Our mixes consists of large and small processes. We set a rea-
sonable number of large processes (20-200) so the mix does
not run for unusually large duration (hours) for experimen-
tation although our scheduler is robust works long running
processes. Once the large processes are executed, we inject
4-5 small processes per large process to simulate a real-life
scenario of other processes getting added to mixes and trying
to hog the cache. Thus, the scheduler can end up executing
more than 10000 processes during the entire mix. In addition,

9

our mixes are homogeneous so all processes are the same,
but with different inputs only. We also created heterogeneous
mixes of different applications but a homogeneous mix tends
to be the worst case because all processes have the same
phases and cache requirements and execute in somewhat of a
phase synchrony causing each process to demand the same
resources as other processes at the same time during its exe-
cution.

Baselines: To evaluate the effectiveness of Beacons
Scheduling mechanism, we compare it against - (a) Linux’s
CFS [19] baseline, which is the widely-used standard sched-
uler in most computing environments, and (b) Merlin [29],
which is a performance-counter based reactive scheduler
(RES). The purpose of this comparison to investigate the
necessity of compiler-guided scheduling vs traditional perf-
based scheduling. Merlin ﬁrst uses the cache misses per thou-
sand instructions (MPKI) in LLC to determine memory inten-
sity; then it estimates cache reuse by calculating the memory
factor (MF), which is the ratio LLC/(LLC-1) MPKI. We use
the same MF threshold as Merlin (0.6) to classify reuse and
stream phases.

Figure 8: Loop Distribution across Rodinia & ML Workloads
and their trip-count prediction accuracy.

5.1 Beacons Prediction Accuracies

Loop Trip Count Prediction Accuracy: Fig. 8 shows the
distribution of different loop-classes for Rodinia and Machine
Learning workloads. Polybench is not shown in the pie-chart
since it contains 100% NBNE-type loops. Rodinia’s loops are
rule-based so there are no classiﬁers needed. The right part
of Fig.8 shows the accuracy of classiﬁer-based models in pre-
dicting the trip counts. The average accuracy is 85.3%. The
reason UECB achieves consistently high accuracy is because
of the nature of these benchmarks. Machine Learning work-
loads can be broadly dissected into several constituent layers
- convolutional layer, fully-connected layer, softmax-layer,
dropout-layer, etc. Each of these layers perform an unique
set of operations, that can be captured very well by classiﬁer
models employed by UECB. These models ﬁnd the correla-
tion between speciﬁc loop properties such as the trip counts,
critical variable values and hence are able to perform well.
Overall, trip counts for 60% of the loops in the workloads are
predicted by classiﬁer models, while the rest are obtained by
rule-based models.

10

Figure 9: Three different cases of Loop Timing Accuracy: positive
deviation (CFD), negative (2mm) and perfect ﬁt (Backprop)

Loop Timing Accuracy: The overall loop timing accu-
racy for both unknown and known beacons collectively was
83% across all the workloads (Fig.10). This accuracy can
vary depending on different classes of loops and their re-
spective training inputs for closed-form regression equation
(Eq.1). For known/inferred beacons (NBNE and other loops
covered by UECB algorithm), the accuracy of timing info
mainly depends on training. The regression ﬁt in Eq. 1 works
particularly well for continuous input sets, as the inputs can
be monotonously increased or decreased to obtain a proper
domain for the regression function. For example, in Fig.9,
backprop takes in continuous integer inputs and the predicted
curve matches closely with different testing inputs (mean-
squared error µ = 0.057). However, for applications with dis-
crete inputs, the regression training becomes dependent on
how representative the provided training inputs are of the ac-
tual application behaviour. Some benchmarks like hotspot in
Rodinia have ﬁve inputs (four used for training) that capture
the behaviour of the loop. The precise loop curve overlaps
almost exactly with the actual time curve, similar to Fig.9. In
contrast, a few cases had training inputs that are not sufﬁcient
enough to learn precise regression coefﬁcients, thus decreas-
ing the accuracy. For example, in 2mm, shown in Figure 9,
the predicted curve deviates for the fourth input (which is the
test input).

For some NBME, IBME and IBNE loops, the unknown
beacons predicts a loop time that is loop bound obtained
during regression runs. However, the actual loop time can
deviate from the predicted time, resulting in either positive or
negative error rate. These two cases are illustrated in Fig 9.
In CFD, the timing information is generated by hoisting the
expected values of inner loop bounds to a point above their
inter-procedural outer loop in the beacon. As a result, we
end up with a mean-squared error µ = 11.023, which shows
that “unknown” nature of these loops can sometimes cause
unreliable prediction. However, to ensure that such cases do
not impact the scheduling decisions, an end-of-loop beacon
typically ﬁres at the loop exit, which helps the scheduler
correct its course. Ultimately, these few cases of expected
predictions (with low accuracy) are still manageable mainly
due to loop completion beacons.

Figure 10: Loop Timing Accuracy across all three workloads suites

5.2 Throughput Improvement & Analysis

The throughput of the system is calculated as the total time
required by the scheduler to ﬁnish a ﬁxed number of incoming
jobs which is same as the average number of jobs completed
in unit time when normalized with a baseline (CFS). The
throughput of both the beacon scheduler and Merlin-based re-
active scheduler normalized against CFS is shown in Fig. 11.
Based on geometric mean average, we achieved speedup of
76.78% on Gravitron2 compared to the 33% slowdown by the
Merlin-based reactive scheduler. Among 45 evaluated bench-
marks from Polybench, Rodinia, and modern ML workloads,
we had signiﬁcant throughput improvement for 28 of them
with the modern ML workloads showing the most through-
put improvement (2.47x) on geometric mean. Polybench and
Rodinia have a modest improvement of 69.01% and 51.46%
respectively. These improvements are for the worst-case mix
of homogeneous processes that have the same cache phases.
The performance beneﬁt is mainly attributed to the smart
processes scheduling leveraging the knowledge of process’
loop type at a given time. The throughput improvement can
be divided into several categories based on its throughput
improvement and are analyzed below:

• Insigniﬁcant Improvement: In four workloads, bea-
cons performed worse or got similar timings compared
against CFS baseline. These workloads were typically
dominated by streaming loops (> 95%), and thus ma-
jority of execution time was spend on streaming loops.
In this scenario, beacons’ scheduling is similar to CFS,
where the resource demand are low and uniform. Adding
the small scheduling and beacons call overheads, we
end up with timing similar to CFS’ or small slowdowns
(≤ 9%).

• Small Improvement (10% - 50%): The majority of ap-
plications within this category tend to have small amount
of reuse-based loops ( 5% − 10%). Most of the reuse
loops typically have smaller footprints or short duration.
Deriche is a special case where reuse loops occur alterna-
tively between streaming loops. Therefore, the minimal
performance improvement is obtained from the data-
reuse in alternate loops, but the frequent streaming loops

prevents the inter-loop data reuse. While CFS can save
on memory accesses across loops (because CFS does not
preempt the application), BES gains performance during
the reuse loops, leading to smaller overall improvement.

• Medium Improvement (50% - 2x): Applications within
this category spend a considerable amount of time within
reuse loops. Bicg, gemm and atax have a majority of
streaming loops but the reuse loop executes for the
longest duration. On the other hand, trisolv has more
reuse loops (but lesser duration) which allows beacons
scheduler to be in reuse mode for a longer period of time
collectively, and beneﬁts from the proactive scheduling.

• High Improvement (2x - 3x): Applications run for
longer durations and spend a majority of their time within
reuse loops (although they have a mix of streaming and
reuse loops in some cases). Evidently, it it’s no surprise
that all the ML workloads fall in this category. These
workloads are mostly training or predicting so they have
to continuously reuse their neuron weights during the
execution. Aside from the neuron weights, they typi-
cally use matrix computations, which are further reused
to do back-propagation. Both the matrices and weights
account for a considerable amount of data, as the large
networks like Alexnet has eight layers (especially ﬁve
convolutional layers). Thus, beacons schedules the pro-
cesses to enable fast matrix multiplications due to less
trashing of the cache.

We ﬁnally present three interesting job completion time-
lines of Cholesky (which showed substantial beneﬁts with
beacons), juxtaposed with those of Correlation (which had
no noticeable beneﬁt)(Fig. 12) with three scheduler - CFS,
Beacons’ scheduler (BES) and Merlin-based reactive sched-
uler (RES). In Cholesky, BES starts with the same jobs as
CFS but soon replaces some of the reuse jobs with other non-
cache-pressure types to avoid cache overﬂow, unlike CFS
which takes longer to retire its ﬁrst jobs. BES later on intelli-
gently places reuse and non-cache-pressure jobs to maintain
high throughput, whereas CFS keeps scheduling both the non-
cache-pressure types and cache-pressure types throughout

11

Figure 11: Throughput Improvement across benchmarks in Polybench, Rodinia & Machine Learning Workloads. Both scheduler’s throughputs
are normalized against the baseline Linux’s CFS Scheduler

approach to reduce power consumption by inserting state-
ments to shut down functional units through a proﬁle-driven
approach in areas of a program where no access to the units
happens is proposed in [26]; it uses a proﬁle-driven approach
that predicts the future execution paths and the latency.

Proﬁle-based scheduling: Prediction of applications’ up-
coming phases by using a combination of ofﬂine and on-
line proﬁling is proposed in [24, 27]. Similarly, another ap-
proach [9] uses a reuse distance model for simple caches
calculated by proﬁling on a simulator for predicting L2 cache
usage. A cache-aware scheduling algorithm is proposed in
[10]. It monitors cache usage of threads at runtime and at-
tempts to schedule groups of threads that stay within the
cache threshold. It is not compiler-driven, nor sensitive to
phase changes within the threads. Several efforts have fo-
cused on the development of scheduling infrastructure for the
shared server platforms [22, 23, 29, 31, 34]. A key feature of
these efforts is their use of observation-based methods (i.e.
reactive approaches) to establish resource contention (e.g. for
caches, memory bandwidth, or other platform resources) and
to further determine the interference at runtime, and/or to as-
sess the workloads’ sensitivity to the contended resource(s)
by proﬁling.

7 Conclusion

In this work, we propose a compiler-directed approach for
proactively carrying out scheduling. The key insight is that
the compiler produces predictions in terms of loop timings
and underlying memory footprints along with the type of loop:
reuse oriented vs streaming which are used to make schedul-
ing decisions. A new framework based on the combination
of static analysis coupled with ML models was developed
to deal with irregular loops with statically non-determinable
trip counts and with multiple loop exits. It was shown that
this framework is able to successfully predict such loops with
85% accuracy. A prototype implementation of the framework
demonstrates signiﬁcant improvements in throughtput over
CFS by an average of 76.78% with up to 3.25x on Gravi-
ton2 for consolidated workloads. The value of prediction was

Figure 12: Histograms for the job completion times of
CFS and the beacon-enabled scheduler (BES) for cholesky
(left) and correlation (right). The X axis represents discrete
timesteps, and the Y axis is a count of the number of jobs that
completed within a given timestep

the execution. Compared to these two, RES keeps on mov-
ing both cache-pressure types and non-cache-pressure types
rather than prioritizing a particular type of them at the be-
ginning. In the case of correlation, the jobs are within the
cache size limit and this can be seen by the two graphs (BES
and RES) being interleaved. Thus, BES does not do anything
differently from RES, and both complete their workloads at
roughly the same time. BES and RES try to prioritize one
group and this leads to no completions then a spike of pro-
cesses completing especially in the tail end of the mix that
saw 1200 processes completing every second. On the other
hand, just like in cholesky, CFS has no knowledge of the
types and does not do it in a batch so CFS keep on schedul-
ing and preempting processes to allow a constant amount
of 80 processes to complete at each second. Thus, its line
is mostly horizontal. In summary, in Cholesky, by limiting
cache pressure through careful scheduling, BES wins over
others whereas in Correlation, it performs no worse.

6 Related Works

Conservative scheduling [32] presents a learning-based
technique for load prediction on a processing node. The load
information at a processing node over time is extrapolated to
predict load at a future time. Task scheduling is done based on
predicted future load over a time window. A compiler-driven

12

also demonstrated over a reactive framework which under-
performed CFS by 9%. Thus, to conclude, predictions help
pro-activeness in terms of scheduling decisions which lead
to signiﬁcant improvements in throughput for a variety of
workloads.

References

[1] Amazon graviton2. https://aws.amazon.com/ec2/

graviton/, 2019. Accessed: 2021 April 16.

[2] George Amvrosiadis, Jun Woo Park, Gregory R Ganger,
Garth A Gibson, Elisabeth Baseman, and Nathan De-
Bardeleben. On the diversity of cluster workloads and
In 2018 {USENIX}
its impact on research results.
Annual Technical Conference ({USENIX}{ATC} 18),
pages 533–546, 2018.

[3] Steven Bird, Ewan Klein, and Edward Loper. Natural
language processing with Python: analyzing text with
the natural language toolkit. " O’Reilly Media, Inc.",
2009.

[4] Shuai Che, Michael Boyer, Jiayuan Meng, David Tarjan,
Jeremy W Sheaffer, Sang-Ha Lee, and Kevin Skadron.
Rodinia: A benchmark suite for heterogeneous comput-
ing. In 2009 IEEE international symposium on workload
characterization (IISWC), pages 44–54. Ieee, 2009.

[5] Shuang Chen, Christina Delimitrou, and José F Martínez.
Parties: Qos-aware resource partitioning for multiple in-
teractive services. In Proceedings of the Twenty-Fourth
International Conference on Architectural Support for
Programming Languages and Operating Systems, pages
107–120, 2019.

[6] Christina Delimitrou, Nick Bambos, and Christos
Kozyrakis. Qos-aware admission control in heteroge-
neous datacenters. In Proceedings of the 10th Interna-
tional Conference on Autonomic Computing (ICAC 13),
pages 291–296, San Jose, CA, 2013. USENIX.

[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer
vision and pattern recognition, pages 248–255. Ieee,
2009.

[8] Eiman Ebrahimi, Chang Joo Lee, Onur Mutlu, and
Yale N Patt. Fairness via source throttling: a conﬁg-
urable and high-performance fairness substrate for multi-
core memory systems. ACM Sigplan Notices, 45(3):335–
346, 2010.

[9] Alexandra Fedorova and Margo Seltzer. Throughput-
oriented scheduling on chip multithreading systems. 04
2019.

[10] Alexandra Fedorova, Margo I. Seltzer, Christopher
Small, and Daniel Nussbaum. Performance of mul-
tithreaded chip multiprocessors and implications for
operating system design. In Proceedings of the 2005
USENIX Annual Technical Conference, April 10-15,
2005, Anaheim, CA, USA, pages 395–398. USENIX,
2005.

[11] Joshua Fried, Zhenyuan Ruan, Amy Ousterhout, and
Adam Belay. Caladan: Mitigating interference at
In 14th {USENIX} Sympo-
microsecond timescales.
sium on Operating Systems Design and Implementation
({OSDI} 20), pages 281–297, 2020.

[12] Julie Greensmith, Uwe Aickelin, and Gianni Tedesco.
Information fusion for anomaly detection with the den-
dritic cell algorithm. Information Fusion, 11(1):21–34,
2010.

[13] Tobias Grosser, Hongbin Zheng, Raghesh Aloor, An-
dreas Simbürger, Armin Größlinger, and Louis-Noël
Pouchet. Polly-polyhedral optimization in llvm. In Pro-
ceedings of the First International Workshop on Polyhe-
dral Compilation Techniques (IMPACT), volume 2011,
page 1, 2011.

[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.

[15] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. Improving
neural networks by preventing co-adaptation of feature
detectors. arXiv preprint arXiv:1207.0580, 2012.

[16] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and
Kilian Q Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 4700–
4708, 2017.

[17] C˘alin Iorgulescu, Reza Azimi, Youngjin Kwon, Sameh
Elnikety, Manoj Syamala, Vivek Narasayya, Herodotos
Herodotou, Paulo Tomita, Alex Chen, Jack Zhang, et al.
Perﬁso: Performance isolation for commercial latency-
sensitive services. In 2018 {USENIX} Annual Techni-
cal Conference ({USENIX}{ATC} 18), pages 519–532,
2018.

[18] Seyyed Ahmad Javadi, Amoghavarsha Suresh, Muham-
mad Wajahat, and Anshul Gandhi. Scavenger: A black-
box batch workload resource manager for improving
utilization in cloud environments. In Proceedings of the
ACM Symposium on Cloud Computing, pages 272–285,
2019.

13

[19] Jacek Kobus and Rafal Szklarski. Completely fair sched-

[30] Sven Verdoolaege. Presburger formulas and polyhedral

uler and its tuning. draft on Internet, 2009.

compilation. 2016.

[31] Hailong Yang, Alex Breslow, Jason Mars, and Lingjia
Tang. Bubble-ﬂux: Precise online qos management for
increased utilization in warehouse scale computers. In
Proceedings of the 40th Annual International Sympo-
sium on Computer Architecture (ISCA), ISCA ’13, pages
607–618, New York, NY, USA, 2013. ACM. Acceptance
Rate: 19%.

[32] Lingyun Yang, Jennifer M Schopf, and Ian Foster. Con-
servative scheduling: Using predicted variance to im-
prove scheduling decisions in dynamic environments.
In Proceedings of the 2003 ACM/IEEE conference on
Supercomputing, page 31. ACM, 2003.

[33] Tomofumi Yuki and Louis-Noël Pouchet. Polybench

4.0, 2015.

[34] Sergey Zhuravlev, Juan Carlos Saez, Sergey Blago-
durov, Alexandra Fedorova, and Manuel Prieto. Sur-
vey of Scheduling Techniques for Addressing Shared
Resources in Multicore Processors. ACM Computing
Surveys, 45(1), 2012.

[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning mul-

tiple layers of features from tiny images. 2009.

[21] Chris Lattner and Vikram Adve. Llvm: A compilation
framework for lifelong program analysis & transforma-
tion. In Proceedings of the International Symposium on
Code Generation and Optimization: Feedback-directed
and Runtime Optimization, CGO ’04, pages 75–, Wash-
ington, DC, USA, 2004. IEEE Computer Society.

[22] Jason Mars, Lingjia Tang, Robert Hundt, Kevin Skadron,
and Mary Lou Soffa. Bubble-up: Increasing utiliza-
tion in modern warehouse scale computers via sensi-
In Proceedings of the 44th Annual
ble co-locations.
IEEE/ACM International Symposium on Microarchitec-
ture (MICRO), MICRO-44, pages 248–259, New York,
NY, USA, 2011. ACM. Acceptance Rate: 21% - Se-
lected for IEEE MICRO TOP PICKS.

[23] Dejan Novakovi´c, Nedeljko Vasi´c, Stanko Novakovi´c,
Dejan Kosti´c, and Ricardo Bianchini. Deepdive: Trans-
parently identifying and managing performance interfer-
ence in virtualized environments. In 2013 {USENIX}
Annual Technical Conference ({USENIX}{ATC} 13),
pages 219–230, 2013.

[24] Shruti Padmanabha, Andrew Lukefahr, Reetuparna Das,
and Scott Mahlke. Trace based phase prediction for
tightly-coupled heterogeneous cores. In Proceedings of
the 46th Annual IEEE/ACM International Symposium
on Microarchitecture, pages 445–456. ACM, 2013.

[25] Joseph Redmon. Darknet: Open source neural networks
in c. http://pjreddie.com/darknet/, 2013–2016.

[26] Siddharth Rele, Santosh Pande, Soner Onder, and Rajiv
Gupta. Optimizing static power dissipation by func-
tional units in superscalar processors. In Compiler Con-
struction, pages 261–275. Springer, 2002.

[27] Xipeng Shen, Yutao Zhong, and Chen Ding. Locality
phase prediction. In ACM SIGOPS Operating Systems
Review, volume 38, pages 165–176. ACM, 2004.

[28] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.

[29] Priyanka Tembey, Ada Gavrilovska, and Karsten
Schwan. Merlin: Application- and Platform-aware Re-
source Allocation in Consolidated Server Systems. In
ACM Symposium on Cloud Computing (SOCC), Seattle,
WA, November 2014.

14

