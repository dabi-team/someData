Draft version September 5, 2022
Typeset using LATEX default style in AASTeX63

2
2
0
2

p
e
S
2

]

M

I
.
h
p
-
o
r
t
s
a
[

2
v
8
0
9
9
0
.
3
0
2
2
:
v
i
X
r
a

Identifying Transient candidates in the Dark Energy Survey using Convolutional Neural Networks

Venkitesh Ayyar,1, 2

Robert Knop Jr.,1

Autumn Awbrey,1, 3

Alexis Andersen,1, 3 and

Peter Nugent1, 3

1Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, Berkeley, CA, 94720, USA
2Hariri Institute for Computing and Computational Science and Engineering, Boston University, Boston, MA, 02215, USA
3Department of Astronomy, University of California, Berkeley, Berkeley, CA, 94720, USA

Abstract

The ability to discover new transient candidates via image diﬀerencing without direct human in-
tervention is an important task in observational astronomy. For these kind of image classiﬁcation
problems, machine Learning techniques such as Convolutional Neural Networks (CNNs) have shown
remarkable success. In this work, we present the results of an automated transient candidate identiﬁ-
cation on images with CNNs for an extant dataset from the Dark Energy Survey Supernova program
(DES-SN), whose main focus was on using Type Ia supernovae for cosmology. By performing an ar-
chitecture search of CNNs, we identify networks that eﬃciently select non-artifacts (e.g. supernovae,
variable stars, AGN, etc.) from artifacts (image defects, mis-subtractions, etc.), achieving the eﬃciency
of previous work performed with random Forests, without the need to expend any eﬀort in feature
identiﬁcation. The CNNs also help us identify a subset of mislabeled images. Performing a relabeling
of the images in this subset, the resulting classiﬁcation with CNNs is signiﬁcantly better than previous
results, lowering the false positive rate by 27% at a ﬁxed missed detection rate of 0.05.

Keywords: –Dark Energy Survey –Type Ia supernovae –Convolutional Neural Networks –Random

Forest

1. INTRODUCTION

A major aspect of observational astronomy is the ”survey” which involves the wholesale mapping of various regions
of the sky to create catalogs which are subsequently mined for scientiﬁcally important astronomical objects. We refer
to a transient candidate as the detection on a single image of a new or varying source with respect to a previously
taken reference image, regardless of its astrophysical nature since at this stage its classiﬁcation is unknown and will
remain so until further data is taken (spectroscopy and/or additional photometry). Some examples of such transient
candidates are solar system objects, supernovae, active galactic nuclei, variable stars, and neutron star mergers, etc.
Since some of these events are quite rare and will fade rapidly, it is often important to trigger follow-up observations
immediately to glean their underlying nature and discover new physics. Hence, identifying transient candidates in
images quickly and eﬃciently is very important so as not to waste precious, and expensive, follow-up resources. For
many years this process was conducted by manual inspection of images by humans. However, given the magnitude
of image data generated by modern telescopes, it became imperative to automate this process via machine learning
techniques. This was ﬁrst done by the SNFactory (Bailey et al. 2007) where boosted decision trees were employed to
greatly reduce the number of candidates. Subsequently, Bloom et al. (2012) advanced this work using random forests
to explore the Palomar Transient Factory data for new transients. Surveys such as the Dark Energy Survey (DES)
(Flaugher 2005) map the sky both on a large scale and deeply, producing up to 170GB of raw imaging data every
night.

In this work, we describe our eﬀorts to perform transient candidate identiﬁcation in DES. This work builds on previous
work with random forest (Goldstein et al. 2015; Mahabal et al. 2019; Wright et al. 2015) to classify Type Ia supernova
from other artifacts of processing and instrumentation for the DES-SN data. Machine learning techniques such as

Corresponding author: Venkitesh Ayyar
vayyar@bu.edu

 
 
 
 
 
 
2

Convolutional Neural networks (CNNs) (Lecun et al. 1998) have shown remarkable success in image classiﬁcation
problems. Here we apply these to identify transient supernova images.

CNNs have been used for transient candidate identiﬁcation by multiple groups. The work of Cabrera-Vives et al.
(2016) and Cabrera-Vives et al. (2017) were focused on u-band imaging from DECam. In Gieseke et al. (2017), they
used data from the SkyMapper Survey and their true-positives were solely drawn from discovered supernovae. Several
groups have undertaken the challenge of transient candidate discovery in very wide-ﬁeld, under-sampled imaging for
surveys like TESS, GOTO and GWAC (Jayaraman et al. 2021; Killestein et al. 2021; Turpin et al. 2020).

In G´omez et al. (2020) they use both the spatio and temporal data (from a higher cadence survey) to train their
CNN’s. This extra data is invaluable for classifying many transient candidates, but lacks the ﬂexibility to work with
a single image. We are also aware of another unpublished work with CNNs with a DES dataset 1. In Acero-Cuellar
et al. (2022) they train CNNs for transient detection while focusing on avoiding the use of a diﬀerence image in the
training process. In Duev et al. (2019), the authors include galactic transients while training their CNNs.

Here, we use the dataset found in Goldstein et al. (2015) where stamps were placed on galaxies, drawn from an
appropriate redshift range for the survey, and within the galaxy at a location proportional to its surface brightness.
This is diﬀerent than the approach in Cabrera-Vives et al. (2016) and Cabrera-Vives et al. (2017), where the true-
positives in their training and validation data were generated by selecting stamps of real PSF-like sources and placing
them at a diﬀerent location at the same epoch and in the same CCD they were observed. This approach works well
for u-band transients, as potential backgrounds are quite faint (i.e. a host galaxy). However, for a number of transient
candidates in an optical search (AGN, supernovae, etc.) this is troublesome as their brightness is comparable to or
fainter than their associated galaxies.

After giving a brief description of CNNs in Section 2, we describe our dataset in Section 3. In Section 4, we discuss
our procedure and present our results. Finally, we summarize our ﬁndings in Section 5. Our code is available at the
github repository 2.

2. CONVOLUTIONAL NEURAL NETWORKS

Here we give a brief introduction to convolutional neural networks. Neural networks are machine learning computing
systems that are very eﬃcient at learning patterns in input data. These are generic functions consisting of weight
parameters organized in layers. Acting on the input data, after periodic application of non-linear activation functions,
they produce outputs which can be either numbers (for regression) or class labels (for classiﬁcation). By minimizing
the deviation between computed output and the expected output, one arrives at the optimal weight parameters. The
procedure to compute the weights of a network is called training the network. A properly trained network learns the
generic function and can correctly predict the outputs for an unseen dataset. Essentially, they are universal function
approximators.

Convolutional neural networks are a class of neural networks that specialize in recognizing patterns in image data.
Using blocks of kernels that scan through the images, they extract features at diﬀerent scales. Figure 2 gives the
general layout of a CNN. A typical CNN is made up of the following basic layers:

• Convolution layers: These perform convolutional operations on the images to extract feature maps.

• Subsampling layers: Operating on feature maps, the subsampling layers compress the dimensionality to reduce

the number of parameters.

• Fully connected layers: These layers combine diﬀerent features of a single layer together.

CNNs typically have very large number of parameters and hence are prone to overﬁtting. One way to mitigate this
is by using dropout layers that help suppress the unimportant weights by setting weight parameters to zero during
training.

CNNs have been used extensively for image recognition, classiﬁcation for images obtained both in the real world and
in scientiﬁc experiments (Lecun et al. 1998; Ciregan et al. 2012). Large multi-layered Neural networks, despite having
large number of free parameters have shown remarkable success in image classiﬁcation (Szegedy et al. 2015; Krizhevsky
et al. 2012; Bhimji et al. 2018). Nevertheless, many studies have used specialized CNNs that have connections between

1 Previous work using CNN’s on DES data can be found here
2 We provide the best saved models and notebooks for plotting and visualization here

DES-CNN

3

non-adjacent layers such as Resnet (He et al. 2015) and Unet (Ronneberger et al. 2015). In a previous work (Ayyar,
Venkitesh et al. 2020), it was found that instead of using CNNs with a few but long layers, CNNs with a resonably
high number of layers with fewer parameters were remarkably successful in classifying signal from background for
datasets in high energy physics experiments. This prompted us to explore the potential of such layered deep CNNs for
classiﬁcation problems in astronomy.

3. DATASET

3.1. Dataset

We used the same dataset used in Goldstein et al. (2015). The data collected is from the DES science operations
from August 2013 to February 2014 and consists of 898,963 independent samples. Each sample in turn consists of 3
types of 2D images of dimension 51 × 51. The 3 types are labelled: Template, Search, and Diﬀerence. To incorporate
the information from all 3 images, we use them as channels. In other words, each input sample is a 3 channel image of
dimensions 51 × 51, having an expected label: Artifact (1) or Non-Artifact (0). Due to the original timing of the data
collection, it lacked non-artifact sources. Hence, the original authors used the method of artiﬁcial source construction,
and thus injected these non-artifacts into the images. This method has been used extensively before (Bailey et al.
2007; Bloom et al. 2012). For this dataset, all non-artifacts were artiﬁcially generated.

Figure 2 depicts the three channels for 5 independent samples for both artifacts and non-artifacts. Distinguishing
them visually requires some level of expertise. A more detailed explanation of the dataset can be found in Goldstein
et al. (2015).

3.2. Classiﬁcation and ROC curves

In classiﬁcation problems, the model provides a class prediction for each sample. The aim is to develop a model
that categorizes most samples correctly. For a binary classiﬁcation problem like this one, the performance can be
summarized by a 2 × 2 confusion matrix shown in Figure 3. Some common classiﬁer performance metrics are the True
Positive Rate (TPR), False Positive Rate (FPR) and Missed Detection Rate (MDR). They are deﬁned as:

M DR =

F P R =

T P R =

Fn
Tp + Fn
Fp
Fp + Tn
Tp
Tp + Fn

(1)

where the quantities Tp , Tn, Fp and Fn are deﬁned in Figure 3. Since the classiﬁer prediction is a ﬂoating point
number between 0 and 1, one uses a threshold parameter to determine a predicted class. The behavior of the classiﬁer
as the threshold is varied, can be seen through the Receiving Operator Characteristic (ROC) curve. The ROC curve
is the most commonly used method to compare a set of classiﬁer models. A useful quantity used for comparison of
models is the Area Under the Curve (AUC), which is expected to approach 1.

4. ANALYSIS AND RESULTS

4.1. CNN architecture search

Figure 1. The general structure of a CNN with convolution, subsampling and fully connected layers.

4

Figure 2. The ﬁgure shows the three channels: template, search and diﬀerence for 5 diﬀerent artifact and non-artifact samples.

Figure 3. The Confusion matrix for a binary classiﬁer.

The goal of this work was to develop optimized CNNs by exploring various CNN architectures. Starting with 4-5
diﬀerent CNN architectures, we obtained new architectures by varying kernel sizes, dropout layer locations, dropout
ratios, types of pooling, learning rates, etc., and compared the classiﬁcation performance of these models. Picking the
best performing models among these, we performed further variations and compared their performance. After a few
such iterations and studying about 100 diﬀerent CNN models in total, we shortlisted 4 models with fairly diﬀerent
architectures that achieved good performance. The structures of these models are listed in Table 4.

Some of the factors guiding the initial architectures were :

True labelsPredicted LabelsNon-artifact (=0)Artifact (=1)Non-artifact (=0)Tp
True positiveFp
False positiveArtifact   (=1)Fn
False negativeTn
True negativeDES-CNN

5

• Presence of Pooling layers (models 1-3 in Tables 4) as opposed to the use of kernel striding (model 4) to reduce

image size during convolution.

• Addition of multiple convolutional and batch normalization layer blocks before implementing pooling layers

(models 2,3).

• Presence of dropout layers after batch normalization layers.

We explored kernel sizes from 2 to 8 (since the image size is 51 x 51) and convolutional layer sizes from 10 to
400. As we approached models with good classiﬁcation performance, we lowered the learning rate to obtain better
classiﬁcation. The entire code was written in python using the keras package (Chollet et al. 2015). We used the
numpy library (Harris et al. 2020) for computations and jupyter notebooks (Kluyver et al. 2016) for analysis and
visualizations. While we do not claim to have explored every architecture, our search is reasonably thorough and we
do ﬁnd multiple CNN models that are eﬃcient for the given classiﬁcation problem.

4.2. Results with original labels

For the ﬁrst part of the work, we split the data into Training (50%), validation (5%) and test samples (5%). The
validation data was used to assess the classiﬁcation performance of trained models on unseen data. The test data was
used to compare the ROC curves of the diﬀerent models. At this stage, we kept about 40% of the data in reserve for
further analysis. We also trained a random forest using the hyperparameters described in the Goldstein et al. (2015).
Table 1 compares the four best CNN models. All models have an AUC score close to 1.0. Figure 4 shows the
FPR-MDR ROC curve for the best CNN models. The black squares represent the ROC curve of the random forest
from Figure 7 of Goldstein et al. (2015). Here we see that our CNN models and the random forest are comparable in
performance to the previous work.

Model name Number of parameters Area under curve (AUC) Training time per epoch on GPU

1
2
3
4

266k
415k
853k
954k

0.994
0.994
0.993
0.994

60s
127s
190s
47s

Table 1. Table describing the best performing CNN models.

Figure 5 shows the prediction histogram for Model 1. Noting the log-scale on the y-axis, it is clear that most of the
samples are classiﬁed correctly as either artifacts or non-artifacts. Also, very few samples have prediction scores in the
intermediate 0.2-0.8 range, which is the hallmark of a good classiﬁer. The prediction histograms for the other three
models also look very similar.

4.3. Mislabeled images

Since the CNNs use the entire information from the images, one would expect well trained CNNs to achieve optimal
classiﬁcation performance. While the CNNs in Figure 4 do perform very well, the fact that they do not improve upon
the performance of the random forest, prompted us to the explore their classiﬁcation in more detail.

In the bottom left part of Figure 5, it can be seen that a signiﬁcant number of artifacts with input label 1 are accorded
a prediction value very close to 0. Similarly, a large number of non-artifacts with input label 0 have prediction values
close to 1, as seen in the bottom right. We see a similar pattern for the other CNN models. This seems to imply that
the CNN models are strongly mis-classifying a few samples, thus aﬀecting the quality of their ROC curves.

To better understand this issue of strong mis-classiﬁcation, we divided the samples into 6 categories depending on the
original label and the predicted value for the model. For example, category 1 corresponds to samples that are labeled
as artifacts (label=1), but have prediction values between 0 and 0.1. We then compare the number of samples in the
diﬀerent categories for the diﬀerent models. The results and description of the categories are summarized in Table 2.
It can be see than the CNN models have more points in categories 1 and 6, corresponding to strongly mis-classiﬁed
samples.

To better understand this, we look at how the samples classiﬁed by Model 1 into category 1 are categorized by other
models. In Table 3, we show the prediction values for the other 3 models and random forest, for that were classiﬁed

6

Figure 4. The ROC curve of FPR vs MDR for the CNN models. The black squares show the points obtained in Goldstein
et al. (2015) with a random forest. The ROC curves of all the models are adjacent to each other implying similar classiﬁcation
performance.

Category Original Label Prediction range Description
1
2
3
4
5
6

0-0.1
0.1-0.5
0.5-1.0
0 - 0.5
0.5 - 0.9
0.9 - 1.0

Strongly mis-classiﬁed Artifact
Weakly mis-classiﬁed Artifact
Correctly classiﬁed Artifact
Correctly classiﬁed Non-artifact
Weakly mis-classiﬁed Non-artifact
Strongly mis-classiﬁed Non-artifact

1
1
1
0
0
0

Model 1 Model 3 Random Forest
0.35 %
0.9 %
48.2 %
48.4 %
1.2 %
0.8 %

0.15 %
1.0 %
48.4 %
48.4 %
1.6 %
0.4 %

0.76 %
1.2 %
47.7 %
49 %
0.8 %
0.7 %

Table 2. Dividing the samples into categories, depending on model predictions. Categories 1 and 6 correspond to the strongly
mis-classiﬁed samples. The CNN models have more strongly mis-classiﬁed samples.

by Model 1 to be in categories 1 and 6. It can be seen that about 66% of the samples are also placed in the same
category by the other 3 CNN models. However, the random forest model places a much smaller proportion of these
samples in category 1. This is further conﬁrmation that the four CNN models seem to strongly mis-classify the same
set of images.

4.4. Relabeling

The fact that diﬀerent optimally trained CNN models strongly mis-categorize the same set of samples, points to
the possibility of a case of mislabeling. To conﬁrm this, we went back to the set of images in categories 1 and 6
for Model 1. Upon performing a visual inspection for a small subset of these samples, we found that about 90% of
these samples were indeed given the wrong label during the process of data preparation. As the artifact sample was
randomly drawn from all detections, while the non-artifact came from injected transient candidates this is not too

0.000.020.040.060.080.10MDR0.000.010.020.030.040.05FPRMDR roc curve for old labelsmodel 1model 2model 3model 4Random forestDES-CNN

7

Figure 5. The prediction histograms for Model 1. Ideally, the classiﬁer should have a prediction value 0 for all artifacts and 1
for all non-artifacts. Note the log scale on the y-axis. Most of the labels are predicted correctly. Also, there are relatively very
few predictions in the intermediate region.

Points in category 1 for model 1

Points in category 6 for model 1

Category
CNN 2
CNN 3
CNN 4
Random forest

1

3

2
65.8 % 20.1 % 14.1 %
72.5 % 18.8 % 8.7 %
72.5 % 17.4 % 10.1 %
26.8 % 45.0 % 28.2 %

Category
2
3
4
Random forest

4

6

5
7.9 % 18.4 % 73.7 %
12.4 % 11.9 % 75.7 %
9.9 % 29.3 % 60.7 %
15.8 % 44.6 % 39.5 %

Table 3. The table on the left shows how the diﬀerent models categorize the 149 samples of a test dataset that are in category
1 (artifact classiﬁed as non-artifact) for Model 1. The CNNs place over 66% of these samples in category 1. The random forest
places only about 27% of these, instead placing many of the rest in category 2. Similarly, the table on the right shows how the
diﬀerent models categorize the 354 samples of a test dataset that are in category 6 for Model 1. From these, it can be inferred
that, the diﬀerent CNN models all strongly misclassify the same set of points.

surprising. In fact, what we saw in our visual inspection is that some of the injected non-artifacts fell on bad parts of
the CCD detector or were located near saturated stars while several of the artifacts were in fact heretofore unknown
astrophysical transients. This can be seen in Figure 6, where we show falsely classiﬁed non-artifacts in the left ﬁgure
and falsely classiﬁed artifacts in the ﬁgure to the right.

For the purposes of relabeling, we developed a GUI tool in python using the Tkinter (Van Rossum 2020) library
that enabled us to view blocks of images with their labels and allow an expert to quickly mark the images that require
relabeling.

As a ﬁrst step, we performed a relabeling by inspecting roughly 750 samples that were classiﬁed into categories 1
and 6 by Model 1. Using the same trained models and their predictions, and just using the new labels, the resulting

0.00.20.40.60.81.0Model prediction for m_1102103104Number of bin samples Artifact (1)  Non-artifact (0)8

Figure 6. The left ﬁgure show four samples that were incorrectly labeled as non-artifacts, while the right ﬁgure shows four
samples that were incorrectly labeled as artifacts. In almost all cases of a True-Positive being mislabeled, it is due to the image
stamp being placed on a saturated star or galaxy or on a bad part of the CCD. The mislabeling of the false negatives is well
understood, and expected, as this sample in Goldstein et al. (2015) was taken from all candidates and some true astrophysical
transients crept into the data.

ROC curves are shown in Figure 7. It is clear that the models are doing signiﬁcantly better with the newer labels. In
addition, the CNN models are now performing better than the random forest. Thus despite all models being trained
with a dataset containing some mislabeled points, the CNNs are doing a better job at classiﬁcation.

Having convinced ourselves of the eﬃcacy of the relabeling process, we then obtained the predictions of Model 1 on
the entire dataset. Collecting the samples in categories 1 and 6, we then inspected this subset of 8093 samples. We
found that 7402 (91%) of these had to be relabelled. In all, 0.8% of the samples were relabelled (7402 out of 898,963
samples).

4.5. Results with new labels

We trained all four CNN models and the random forest on the relabeled dataset, after splitting the data into
training (70%), validation (10%), and test samples (20%). The resulting ROC curves are shown in Figure 8. It is clear
that the random forest is performing better with the new, relabeled dataset, and the four CNN are comparable in
performance, but better than the random forest. Figure 4.5 shows the ROC comparing the true positive rate with the
false positive rate as deﬁned in Eqn 1. Based on the two ROC curves, we choose Model 1 as our best model, although
the classiﬁcation of the four CNN models are fairly similar. To quantify the improvement in performance, we compare
the FPR values for a ﬁxed MDR value of 0.05 as shown in the Figure 8. The corresponding FPR values for the CNN
model 1 and random forest at 0.008 and 0.011 respectively. Thus the CNN model 1 lowers the FPR value by 27%.

We present the learning curve of Model 1 in Figure 10 and its detailed structure in Table 4. The confusion matrix

is presented in Figure 11.

TemplateSearchDifferenceLabel change : non-artifact  artifact (0  1)TemplateSearchDifferenceLabel change  artifact  non-artifact  (1  0)DES-CNN

9

Figure 7. The ROC curve of FPR vs MDR for the previously trained models using new labels for the test dataset. The black
squares show the points obtained in Goldstein et al. (2015) with random forest. The CNN models and random forest show
signiﬁcant improvement.

We would like to reiterate that the relabeling procedure has been conducted by visual inspection, with the machine
learning method being used to only shortlist the suspected mislabeled images. It might be argued that our relabeling
process might be biasing the performance of Model 1, since we chose the points to relabel, based on the predictions of
Model 1. However, in Section 4.2, we trained with only 50% of the samples, keeping the rest of the data in reserve.
For this ﬁnal analysis, we built the dataset with a diﬀerent random seed and used 70% of the data for training, thus
mitigating the bias. Table 5 shows the number of points in various categories for Model 1 for the three cases: train-test
with old labels, train with old label, test with new labels, train and test with new labels. It is clear that values in
column 2 are lowest, due to the bias. But the values in column 3 are intermediate, indicating that the bias has been
mitigated. Hence, we are conﬁdent that our ﬁnal CNN models are indeed better at classiﬁcation than the random
forest.

4.6. Results in an ongoing survey

We are currently using the CNN to provide a real/bogus score for an ongoing survey, the DECam Deep Drilling
Fieds (DDF) program (Graham et al., 2022, in preparation). This is being run at the Blanco 4m telescope at Cerro
Tololo-Inter-American Observatory as part of the DECam Alliance for Transients (DECAT), a consortium of time-
domain DECam programs. We have been using real/bogus scores produced by the CNN to decide which detections
are sent out in alerts. Throughout 2021, we used the original CNN trained for this paper. Starting in 2022, and
ultimately for the analysis of the entire data from the survey, we will be using a retrained CNN as described below for
extragalactic (|b| > 20◦) events. Training of a CNN for galactic events is in progress.

Because this is real, incoming data, rather than a simulation, we don’t have the absolute truth as to what’s a genuine
astronomical detection on the diﬀerence image, and what’s a subtraction artifact or other ”bogus” event. In order
to produce an evaluation data set, several observers manually tagged events from this survey as ”real” or ”bogus”.

0.000.020.040.060.080.10MDR0.000.010.020.030.040.05FPRMDR roc curve for relabeled test datasetmodel 1model 2model 3model 4Random forest10

Figure 8. The ROC curves of the best chosen CNN model: model 1 and random forest that were trained on the relabeled
dataset. The black squares show the points obtained in Goldstein et al. (2015) with the random forest. It is clear that Model 1
outperforms the random forest even on the relabeled dataset. The dotted line represents an MDR value of 0.05. For this MDR
value, the correspond FPR values of model1 and random forest are 0.008 and 0.011 respectively. Thus the FPR value of the
CNN model is only 73% of the FPR value of the random forest.

Participating observers were all trained on examples of good and bad events. The observers included two with decades
of experience in vetting supernova candidates in searches like this, and three undergraduate student assistants. Each
observer was given events randomly chosen from the few million events that had been found by the data pipeline.
Once several observers had rated enough events, they were given events randomly chosen from those that had already
been rated by others. In this way, we were able to build up a set of (cid:39) 25, 000 events that had been tagged by three or
more observers.

It is important to emphasize that what we are trying to do here is diﬀerent from what is described in the rest of this
paper. The development of the CNN described in most of this paper aimed to correctly identify simulated transient
candidates. Here, we are using the same CNN architecture in an attempt to reproduce in bulk, the messier process
of human scanning of transient candidates from real data. The training and validation sets cannot be as clean in this
case. Of the 25, 000 events tagged by three or more observers, we selected those where the number of observers in the
majority was at least two greater than the number in the minority. (Eﬀectively, this means that for those only rated
by three observers, the tags would have been unanimous.) Of this subset, about 1, 700 were tagged by the majority
as good and 19, 000 as bad. There was a unanimous agreement on 75% of the events that the majority deemed to be
good; there was a consensus on 95% of the events deemed to be bad by the majority. The choice to use the majority
tags rather than the consensus tags represents a greater emphasis on reducing missed detections as opposed to reducing
false positives.

The CNN trained on the simulated transient candidates (whose results are in Section 4.5) did not perform particularly
well in reproducing human scanning of the live data set. In particular, there was a high missed detection rate (for any
reasonable r/b cutoﬀ) of ∼ 0.5 for candidates that were unanimously agreed to be good by three or more observers;

0.000.020.040.060.080.10MDR0.000.010.020.030.040.05FPRMDR roc curve for relabeled datasetmodel 1Random forestDES-CNN

11

Figure 9. The ﬁgure shows the ROC curves for Model 1 and random forest, plotting the true positive rate (TPR) with the
false positive rate (FPR).

this high MDR was present even when limiting to events with a high S/N ratio. To allow the CNN to better model the
visual scanning of this survey, we re-trained the model using the majority-tagged events described above as a training
and validation set. This retrained model performed much better than the original model on this new dataset, yielding
a MDR of ∼ 0.055 and a FPR of ∼ 0.04 (similar to the performance of the originally model). The results for the
retrained CNN are shown in Figures 12 and 13. We cannot expect the ROC curve (left plot of Figure 12 here to be as
good as the ones in Figure 8 because, as mentioned above, the training and validation set is not nearly as clean.

There is some evidence that the non-consensus events were at least sometimes more marginal cases, based on the
r/b scores produced by the CNN retrained on the majority rankings. The training of the CNN was just given a single
real or bogus ﬂag based on the majority of the human rankings; it had no knowledge of what was a consensus-good
or consensus-bad event. Despite this, the retrained CNN showed diﬀerent statistics for consensus vs. non-consensus
events. For the majority-good events, the r/b scores of the consensus-good events were on average higher than the
events on which there was disagreement (0.87 vs. 0.75). For the majority-bad events, the r/b scores of the consensus-
bad events were on average lower (0.05 vs 0.25). Note that the fraction of events that had a consensus did not
appreciably change when limiting to only high S/N events; those events that were marginal cases were not simply
low-S/N cases, but represented cases where visual appearance of the residual might have had some suggestion of being
an artifact, but was not clearly an artifact.

In conclusion, the model obtained using a CNN architecture optimized for the original dataset works fairly well with
another, similar dataset, after a re-training of the weights. This points to our model architecture being fairly generic
and hence more broadly applicable for datasets of this type. To improve upon the performance on this new dataset,
we would have to perform another architecture search with a subset of this new data. We aim to address this in a
future publication.

5. CONCLUSIONS AND DISCUSSION

105104103102FPR0.00.20.40.60.81.0TPRmodel 1Random forest12

Figure 10. The ﬁgure shows the learning curves for CNN Model 1. The top ﬁgure provides the training and validation accuracy
values at the end of each epoch, while the bottom ﬁgure provides the corresponding loss values. The validation loss and accuracy
stabilize after about 10 epochs.

5.1. Inference

We have discussed automating the identiﬁcation of transient detections obtained in astronomical imaging data using
machine learning. Here we developed CNN models trained directly on the raw image data. The best CNN models
match the performance of the previously used random forest method. In addition, using the CNNs predictions, we
were able to identify that some of the images were mislabeled in the original data. After performing a relabelling of
0.9% of the dataset, we re-trained the best CNN models. The resultant models outperform the original random forest
method. We also ﬁnd that the CNNs are more robust to mislabeled samples in the training data.

Since we have only relabeled a small subset of the data, there is still the possibility that many other points are mis-
labelled. However, the signiﬁcant increase in classiﬁcation performance suggests that we have identiﬁed and relabelled
most of the mislabeled images.

5.2. Discussion

There are two main beneﬁts of using CNNs over random forest for image classiﬁcation: classiﬁciation eﬃciency and

ease of use.

Currently in astronomy, random forest methods are the most common method to auto-identify transients in image
subtractions. In Wright et al. (2015), the authors compared the performance of random forests with neural networks
and demonstrated that their random forest was most eﬃcient. However, in our work, we have demonstrated that our
CNN, obtained by performing a detailed architecture search, outperforms our random forest. For example, comparing
the FPR value for a ﬁxed MDR, we ﬁnd that the CNN model lowers the FPR by 27% compared to the random forest.
Since the datasets are diﬀerent, one cannot perform a direct comparison of ROC curves. However, the fact that our
CNN outperforms the our random forest on the same dataset clearly demonstrates the beneﬁt of using CNNs.

Model 1

Model 3

Layer

Output Shape No. of Parameters

Layer

Output Shape No. of Parameters

DES-CNN

13

51 × 51 × 3
51 × 51 × 80
51 × 51 × 80
25 × 25 × 80
25 × 25 × 80
51 × 25 × 25
12 × 12 × 80
12 × 12 × 80
12 × 12 × 80
6 × 6 × 80
2880
2880
51
51
1

Input
Conv2D
BatchNorm
MaxPooling
Conv2D
BatchNorm
MaxPooling
Conv2D
BatchNorm
MaxPooling
Flatten
Dropout
Dense
BatchNorm
Dense
Total trainable
parameters

Model 2

0
2240
320
0
57680
320
0
57680
320
0
0
0
146931
204
52

265,165

Layer
Input
Conv2D
BatchNorm
Conv2D
BatchNorm
MaxPooling
Conv2D
BatchNorm
Conv2D
BatchNorm
MaxPooling
Flatten
Dropout
Dense
BatchNorm
Dense
Total trainable
parameters

Output Shape No. of Parameters
0
51 × 51 × 3
3920
51 × 51 × 80
320
51 × 51 × 80
102480
51 × 51 × 80
320
51 × 51 × 80
0
17 × 17 × 80
102480
17 × 17 × 80
320
17 × 17 × 80
102480
17 × 17 × 80
320
17 × 17 × 80
0
5 × 5 × 80
0
2000
0
2000
102051
51
204
51
52
1

414,205

51 × 51 × 3
51 × 51 × 120
51 × 51 × 120
51 × 51 × 120
51 × 51 × 120
17 × 17 × 120
17 × 17 × 120
17 × 17 × 120
17 × 17 × 120
17 × 17 × 120
5 × 5 × 120
3000
3000
51
51
1

Input
Conv2D
BatchNorm
Conv2D
BatchNorm
MaxPooling
Conv2D
BatchNorm
Conv2D
BatchNorm
MaxPooling
Flatten
Dropout
Dense
BatchNorm
Dense
Total trainable
parameters

0
5880
480
230520
480
0
230520
480
230520
480
0
0
0
153051
204
52

851,605

Model 4

Layer

Output Shape No. of Parameters

51 × 51 × 3
26 × 26 × 40
26 × 26 × 40
26 × 26 × 40
13 × 13 × 60
13 × 13 × 60
13 × 13 × 60
13 × 13 × 80
13 × 13 × 80
13 × 13 × 80
13520
13520
51
51
1

Input
Conv2D
BatchNorm
Dropout
Conv2D
BatchNorm
Dropout
Conv2D
BatchNorm
Dropout
Flatten
Dropout
Dense
BatchNorm
Dense
Total trainable
parameters

0
4360
160
0
86460
240
0
172880
320
0
0
0
689571
204
52

953,785

Table 4. Structures of the 4 best CNN models. As mentioned in 3.1, each CNN reads a batch of input images of dimensions
51 × 51 × 3, with the 3 corresponding to the three types of images Template, Search and Diﬀerence. As the diﬀerent layers of
the CNN are applied to each image, the dimensions of the image array change. The above table lists these details, with the
ﬁrst column describing the layer and the second column denoting the dimensions of the intermediate image array. The third
column gives the number of parameters in each layer. The terms Layers Conv2D, Batch Norm, MaxPooling, Flatten, Dropout
and Dense represent the standard CNN operations convolution, batch normalization, maxpooling, ﬂattening, dropout and dense
respectively. More information about these can be found in the keras layers API documentation.

14

Figure 11. The confusion matrix, with both normalized percentages and total number of triplets for used in training (in
parens), for Model 1 based on a threshold cut of 0.5.

Category
1
2
3
4
5
6

1: Train and test with old labels 2: train with old labels,test with new labels 3: train-test with new labels

0.35 %
0.75 %
48.4 %
48.4 %
1.2 %
0.84 %

0.2 %
0.82 %
49 %
48.8 %
1.02 %
0.23 %

0.36 %
0.83 %
49.1 %
48.5 %
0.77 %
0.33 %

Table 5. Comparing the points in various categories for the three cases:
1: Train and test with old labels, 2: train with old labels, but test with new labels, 3: train and test with new labels. Comparing
columns 1,2 and 3, it can be seen that column 2 has few points in categories 1 and 6, since the test dataset was biased by
Model 1. Column 3 has slightly higher values in these categories compared to column 2, since the models were re-trained on a
bigger dataset. Overall, column 3 has fewer points than column 1 in category 6, implying that the relabeling procedure had the
intended eﬀect.

Recently Acero-Cuellar et al. (2022) used CNN’s for transient discovery that is the most comparable to our work as
both used the datasets from Goldstein et al. (2015) to train the CNN’s. While the focus of their paper is on performing
transient discovery without image subtraction, they do present a confusion matrix for a similar design as ours. Both
their false-positives and true-negatives are a factor of ∼2 larger than ours.

Duev et al. (2019) has created CNN’s for the classiﬁcation of transients in the Zwicky Transient Facility (ZTF). It
should be noted that ZTF is slightly diﬀerent than our survey in that ∼30% of the images they take are undersampled
(Bellm et al. 2018), thus a direct comparison to our work is not exactly correct. That said, their confusion matrix
is very similar to ours in quality (their true-positive’s are slightly more pure while their false-negatives are slightly
worse). As the number of validation triplets was low in their study, the uncertainties on these numbers are on the
order of ∼2%. Both these studies conﬁrm the utility and beneﬁt of using CNNs for transient candidate detection and
that they are superior to random forest methods.

Another beneﬁt of CNNs is their ease of implementation. One of the drawbacks of the original random forest method
is the need to identify a set of important features to use. This process is fairly painstaking, and also involves performing
many, often computationally expensive, operations on the raw images. The CNN method, on the other hand works

True labelsPredicted LabelsNon-artifact (=0)Artifact (=1)Non-artifact (=0)97.8% (81916)2.4% (1855)Artifact   (=1)2.2% (2002)97.6% (82864)DES-CNN

15

Figure 12. Results for the CNN model trained against manual vetting of the ongoing DECAT/DDF survey. Left: ROC curve.
Middle: false positive rate (FPR) and missed detection rate (MDR) as a function of real/bogus score cutoﬀ. Right: stream
cleanliness vs. MDR. Stream cleanliness is the fraction of passed objects that are real objects. This is diﬀerent from 1-FPR
because in the real dataset, there are a factor of 7 more bogus events than real events. In the left and right plots, the red dot
indicates the chosen real/bogus threshold of 0.3 that will be used in determining if an alert should be sent out for the detection.
The dashed green vertical line in the middle plot is the same cutoﬀ.

Figure 13. Histogram of real/bogus produced by the retrained CNN for the ongoing DECAT/DDF survey. The blue histogram
are events labelled as ”real” by at least two out of three visual inspections, and the red histogram are events labelled as ”bogus”.
The vertical dashed green line is the real/bogus threshold of 0.3 that will be used for generating alerts. Compared to Figure 5;
as discussed in the text, we cannot expect the histogram to be as cleanly separated with this dataset as we can for the dataset
used for the bulk of the paper.

directly on the raw image data. Although it requires an architecture search, diﬀerent models with reasonably high
complexity perform well in classiﬁcation. Their computational cost is also quite reasonable as they run eﬃciently on
GPUs.

Hence, we believe the CNN method is more suitable for implementing automation of image subtraction classiﬁca-
tion in astronomy. Such methods could, and should be explored in upcoming transient surveys such as the Rubin
Observatory (Street et al. 2020) and the La Silla Schmidt Southern Survey (Nugent et al., in prep.) among others.

16

ACKNOWLEDGEMENTS

This research used resources of the National Energy Research Scientiﬁc Computing Center (NERSC), a U.S. De-
partment of Energy Oﬃce of Science User Facility operated under Contract No. DE-AC02-05CH11231. V.A.’s work
was supported by the Computational Center for Excellence, a Computational HEP program in the Department of
Energy’s Science Oﬃce of High Energy Physics (Grant #KA2401022). P.E.N. and R.A.K. acknowledge support from
the DOE under grant DE-AC02-05CH11231, Analytical Modeling for Extreme-Scale Computing Environments.

REFERENCES

Acero-Cuellar, T., Bianco, F., Dobler, G., Sako, M., & Qu,
H. 2022, There’s no diﬀerence: Convolutional Neural
Networks for transient detection without template
subtraction, arXiv, doi: 10.48550/ARXIV.2203.07390

G´omez, C., Neira, M., Hern´andez Hoyos, M., Arbel´aez, P.,
& Forero-Romero, J. E. 2020, Monthly Notices of the
Royal Astronomical Society, 499, 3130,
doi: 10.1093/mnras/staa2973

Ayyar, Venkitesh, Bhimji, Wahid, Gerhardt, Lisa,

Harris, C. R., Millman, K. J., van der Walt, S. J., et al.

Robertson, Sally, & Ronaghi, Zahra. 2020, EPJ Web
Conf., 245, 06003, doi: 10.1051/epjconf/202024506003
Bailey, S., Aragon, C., Romano, R., et al. 2007, Astrophys.

2020, Nature, 585, 357, doi: 10.1038/s41586-020-2649-2

He, K., Zhang, X., Ren, S., & Sun, J. 2015, CoRR,

abs/1512.03385. https://arxiv.org/abs/1512.03385

J., 665, 1246, doi: 10.1086/519832

Bellm, E. C., Kulkarni, S. R., Graham, M. J., et al. 2018,
Publications of the Astronomical Society of the Paciﬁc,
131, 018002, doi: 10.1088/1538-3873/aaecbe

Bhimji, W., Farrell, S. A., Kurth, T., et al. 2018, J. Phys.

Conf. Ser., 1085, 042034,
doi: 10.1088/1742-6596/1085/4/042034

Bloom, J. S., Richards, J. W., Nugent, P. E., et al. 2012,
Publications of the Astronomical Society of the Paciﬁc,
124, 1175, doi: 10.1086/668468

Cabrera-Vives, G., Reyes, I., F¨orster, F., Est´evez, P. A., &
Maureira, J.-C. 2017, The Astrophysical Journal, 836, 97,
doi: 10.3847/1538-4357/836/1/97

Cabrera-Vives, G., Reyes, I., F¨orster, F., Est´evez, P. A., &

Maureira, J.-C. 2016, in 2016 International Joint
Conference on Neural Networks (IJCNN), 251–258,
doi: 10.1109/IJCNN.2016.7727206

Chollet, F., et al. 2015, Keras, https://keras.io
Ciregan, D., Meier, U., & Schmidhuber, J. 2012, in 2012
IEEE Conference on Computer Vision and Pattern
Recognition, 3642–3649,
doi: 10.1109/CVPR.2012.6248110

Jayaraman, R., Fausnaugh, M., & Ricker, G. 2021, Bulletin
of the AAS, 53. https://baas.aas.org/pub/2021n1i211p07

Killestein, T. L., Lyman, J., Steeghs, D., et al. 2021,

Monthly Notices of the Royal Astronomical Society, 503,
4838, doi: 10.1093/mnras/stab633

Kluyver, T., Ragan-Kelley, B., P´erez, F., et al. 2016, in

Positioning and Power in Academic Publishing: Players,
Agents and Agendas, ed. F. Loizides & B. Scmidt
(Netherlands: IOS Press), 87–90.
https://eprints.soton.ac.uk/403913/

Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012, in
Proceedings of the 25th International Conference on
Neural Information Processing Systems - Volume 1,
NIPS’12 (Red Hook, NY, USA: Curran Associates Inc.),
1097–1105

Lecun, Y., Bottou, L., Bengio, Y., & Haﬀner, P. 1998,

Proceedings of the IEEE, 86, 2278, doi: 10.1109/5.726791

Mahabal, A., Rebbapragada, U., Walters, R., et al. 2019,
Publications of the Astronomical Society of the Paciﬁc,
131, 038002, doi: 10.1088/1538-3873/aaf3fa

Ronneberger, O., Fischer, P., & Brox, T. 2015, CoRR,
abs/1505.04597. https://arxiv.org/abs/1505.04597

Duev, D. A., Mahabal, A., Masci, F. J., et al. 2019,

Street, R. A., Bianco, F. B., Bonito, R., et al. 2020,

Monthly Notices of the Royal Astronomical Society, 489,
3582, doi: 10.1093/mnras/stz2357

Research Notes of the American Astronomical Society, 4,
41, doi: 10.3847/2515-5172/ab812a

Flaugher, B. 2005, International Journal of Modern Physics

Szegedy, C., Liu, W., Jia, Y., et al. 2015, in 2015 IEEE

A, 20, 3121, doi: 10.1142/S0217751X05025917

Gieseke, F., Bloemen, S., van den Bogaard, C., et al. 2017,
Monthly Notices of the Royal Astronomical Society, 472,
3101, doi: 10.1093/mnras/stx2161

Goldstein, D. A., D’Andrea, C. B., Fischer, J. A., et al.

2015, The Astronomical Journal, 150, 82,
doi: 10.1088/0004-6256/150/3/82

Conference on Computer Vision and Pattern Recognition
(CVPR), 1–9, doi: 10.1109/CVPR.2015.7298594
Turpin, D., Ganet, M., Antier, S., et al. 2020, Monthly
Notices of the Royal Astronomical Society, 497, 2641,
doi: 10.1093/mnras/staa2046

Van Rossum, G. 2020, The Python Library Reference,

release 3.8.2 (Python Software Foundation)

Wright, D. E., Smartt, S. J., Smith, K. W., et al. 2015,

Monthly Notices of the Royal Astronomical Society, 449,
451, doi: 10.1093/mnras/stv292

DES-CNN

17

