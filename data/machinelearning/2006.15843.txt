Noname manuscript No.
(will be inserted by the editor)

Non-Convex Exact Community Recovery in
Stochastic Block Model

Peng Wang · Zirui Zhou ·
Anthony Man-Cho So

Received: date / Accepted: date

Abstract Community detection in graphs that are generated according to
stochastic block models (SBMs) has received much attention lately. In this
paper, we focus on the binary symmetric SBM—in which a graph of n vertices
is randomly generated by ﬁrst partitioning the vertices into two equal-sized
communities and then connecting each pair of vertices with probability that
depends on their community memberships—and study the associated exact
community recovery problem. Although the maximum-likelihood formulation
of the problem is non-convex and discrete, we propose to tackle it using a
popular iterative method called projected power iterations. To ensure fast
convergence of the method, we initialize it using a point that is generated
by another iterative method called orthogonal iterations, which is a classic
method for computing invariant subspaces of a symmetric matrix. We show
that in the logarithmic sparsity regime of the problem, with high probability

A preliminary version of this work has appeared in the Proceedings of the 37th International
Conference on Machine Learning (ICML 2020), 2020 [37]. The ﬁrst and third authors are
supported in part by the Hong Kong Research Grants Council (RGC) General Research
Fund (GRF) project CUHK 14208117 and in part by the CUHK Research Sustainability of
Major RGC Funding Schemes project 3133236. The second author is supported in part by
the National Natural Science Foundation of China (NSFC) project 11901490 and in part
by a HKBU Start-up Grant. Most of the work of the second author was done when he was
aﬃliated with the Department of Mathematics of the Hong Kong Baptist University.

Peng Wang
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong, Shatin, NT, Hong Kong
E-mail: wangpeng@se.cuhk.edu.hk

Zirui Zhou
Huawei Technologies Canada Co., Ltd., Burnaby, Canada
E-mail: zirui.zhou@huawei.com

Anthony Man-Cho So
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong, Shatin, NT, Hong Kong
E-mail: manchoso@se.cuhk.edu.hk

1
2
0
2

p
e
S
7
2

]

C
O
.
h
t
a
m

[

4
v
3
4
8
5
1
.
6
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

P. Wang, Z. Zhou, A. M.-C. So

the proposed two-stage method can exactly recover the two communities down
(n log2 n/ log log n) time, which is com-
to the information-theoretic limit in
petitive with a host of existing state-of-the-art methods that have the same
recovery performance. We also conduct numerical experiments on both syn-
thetic and real data sets to demonstrate the eﬃcacy of our proposed method
and complement our theoretical development.

O

Keywords community detection
projected power iteration

·

·

exact recovery

orthogonal iteration

ﬁnite termination

nearly-linear time

·

·

·

1 Introduction

Community detection is a fundamental task in network analysis and has found
many applications in diverse ﬁelds such as physics [16,30], biology [13], and
social science [19], to name a few. In research on community detection, the
stochastic block model (SBM), which provides a way to generate graphs with
community structure, is widely used as a platform for validating theoretical
ideas and comparing numerical algorithms. In particular, substantial advances
have been made in the past decade on understanding the fundamental limits
of community detection in graphs that are generated by SBMs, and on de-
veloping computationally tractable methods that can meet diﬀerent recovery
requirements up to their corresponding fundamental limits; see, e.g., [1] and
the references therein.

One problem that has been extensively studied in the literature is the ex-
act recovery of communities in the binary symmetric SBM (also known as the
planted bisection model). Speciﬁcally, given an n-vertex graph with two equal-
sized hidden communities, and each pair of vertices in the graph is connected
by an edge with probability p if they both belong to the same community
and with probability q otherwise, the goal is to achieve exact recovery (i.e.,
recover the underlying communities exactly with high probability) using only
the adjacency matrix of the graph. It is well known that whether exact recov-
ery is achievable depends on the scalings of p, q, and p
q. When p = a/n
and q = b/n for some a > b > 0 (the constant sparsity regime), it is impos-
sible to recover the communities because the graph is disconnected with high
probability [15]. On the other hand, when p = α log n/n and q = β log n/n
for some α > β > 0 (the logarithmic sparsity regime), Abbe et al. [2] and
Mossel et al. [29] independently showed that exact recovery is impossible if
√β > √2, thereby establishing a sharp
√α
threshold for exact recovery. The proof of Abbe et al. [2] takes an information-
theoretic approach and obtains the said threshold by analyzing the following
maximum-likelihood (ML) formulation of the problem:

√β < √2 but is possible if √α

−

−

−

xT Ax : 1T

max

n x = 0, xi ∈ {±
Here, A is the adjacency matrix of the graph, 1n is the all-one vector of
encodes the community membership of vertex
dimension n, and xi ∈ {±

, i = 1, . . . , n

(MLE)

(cid:9)

(cid:8)

}

1

}

1

.

Non-Convex Exact Community Recovery

3

√β < √2, the ML
i for i = 1, . . . , n. It is shown in [2] that when √α
estimator (i.e., an optimal solution of (MLE)) fails to recover the communities
with probability bounded away from zero for suﬃciently large n, but when
√β > √2, the ML estimator can exactly recover the communities with
√α
high probability.

−

−

From a computational point of view, solving Problem (MLE) amounts
to ﬁnding a minimum bisection of a graph, which is NP-hard in the worst
case [18]. Over the past few decades, many algorithms have been proposed
to tackle the problem of exact community recovery in the binary symmetric
SBM; see, e.g., [1] for a summary of some of the earlier works. In view of the
information-theoretic limit established in [2,29], a natural task is to design ef-
ﬁcient algorithms that can exactly recover the communities down to the limit.
This has also been undertaken in [2,29]. The former presents a two-stage al-
gorithm that combines the partial recovery algorithm of Massouli´e [28] with
a local improvement procedure, while the latter gives a three-stage algorithm
that uses spectral clustering for initialization and then combines a partial re-
covery step with a local reﬁnement procedure. The former also presents an
algorithm based on a semideﬁnite relaxation (SDR) of Problem (MLE) and
poses the conjecture that the algorithm can exactly recover the communities
down to the information-theoretic limit. The conjecture was later resolved in
the aﬃrmative independently by Hajek et al. [21] and Bandeira [7].

Subsequent to the above development, a variety of eﬃcient algorithms with
the same recovery performance in the binary symmetric SBM have appeared in
the literature. For instance, Abbe and Sandon [4] developed a two-stage algo-
rithm that is similar in spirit to the one in [2]. Yun and Proutiere [38] presented
a spectral partition algorithm, which proceeds by applying spectral decompo-
sition to a trimmed adjacency matrix, followed by some local improvements.
Later, Gao et al. [17] proposed a two-stage algorithm that employs spectral
clustering for initialization and penalized local maximum likelihood estimation
for local reﬁnement. It is worth noting that the aforementioned algorithms ap-
ply not only to the binary symmetric SBM but also to more general SBMs.
More recently, Abbe et al. [3] showed that the vanilla spectral method, which
computes the eigenvector associated with the second largest eigenvalue of the
adjacency matrix and uses the signs of the entries to identify the communities,
already has the desired recovery performance.

Among the existing algorithms that can achieve exact recovery down to the
information-theoretic limit in the binary symmetric SBM, the best complexity
bound is nearly linear. This is attained by, e.g., the three-stage algorithm of
Mossel et al. [29] and the spectral partition algorithm of Yun and Proutiere
(n log2 n) runtime, and the two-stage algorithm of
[38], both of which have an
Abbe and Sandon [4], which has a runtime of o(n1+ǫ) for any ǫ > 0. It should be
pointed out that even though the vanilla spectral method is conceptually much
simpler than these algorithms, it needs to perform an eigenvector computation,
and standard complexity analyses of the commonly used methods for this
purpose (such as orthogonal iteration) only yield a quadratic bound at best
(see, e.g., [34, Part V]).

O

4

P. Wang, Z. Zhou, A. M.-C. So

1.1 Our Contributions

In this work, we propose a two-stage iterative algorithm that aims to achieve
exact recovery in the binary symmetric SBM by directly tackling the non-
convex ML estimation problem (MLE). In the ﬁrst stage, the algorithm ap-
plies the classic method of orthogonal iteration to compute an approximation
of the eigenvector u2 associated with the second largest eigenvalue of A. Such
an approximation is then used as an initialization in the second stage of the
algorithm, which applies the method of projected power iteration (cf. [25,26])
to solve Problem (MLE). The ﬁrst stage is akin to that of a host of exist-
ing algorithms, such as those in [3,29,38]. However, we are able to show that
a coarse approximation of u2 is suﬃcient for the second stage of our pro-
posed algorithm to ﬁnd an optimal solution of Problem (MLE), which is key
to the eﬃciency of our algorithm. Speciﬁcally, we show that in the logarith-
mic sparsity regime of the binary symmetric SBM, our proposed algorithm
achieves exact recovery all the way down to the information-theoretic limit
within
(log n/ log log n) pro-
jected power iterations, where each orthogonal iteration and projected power
iteration can be implemented in
(n log n) time. This yields an overall com-
(n log2 n/ log log n) for our algorithm, which is competitive
plexity bound of
with some of the most eﬃcient algorithms in the literature that have the same
recovery performance. By combining our techniques with the results in [3],
we can further show that the vanilla spectral method can be implemented in
(n log2 n/ log log n) time. To the best of our knowledge, this is currently the
O
best complexity bound for the method in the context of exact community re-
covery in the binary symmetric SBM. We also conduct numerical experiments
on synthetic and real data sets to evaluate the performance of our proposed
algorithm. The results demonstrate the eﬃcacy of the algorithm and comple-
ment our theoretical development.

(log n/ log log n) orthogonal iterations and

O

O

O

O

In recent years, there has been a growing body of literature exploring the
design and analysis of fast methods for tackling non-convex formulations that
arise in applications. These include deep neural networks [32,33], low-rank
matrix recovery [12,23], phase retrieval [27,35], source localization [24,31],
and synchronization [25,39]. As these works show, the non-convex formula-
tions in question often possess structures that can be exploited by simple and
scalable methods, thereby allowing optimal solutions of those formulations to
be found eﬃciently. Our work contributes to this emerging area by showing
that in the logarithmic sparsity regime of the binary symmetric SBM, the ML
estimation problem (MLE), albeit non-convex and discrete, can be solved to
optimality via a carefully designed, yet simple, iterative procedure. Prior to
our work, Bandeira et al. [8] considered another non-convex formulation of
the community recovery problem, which is obtained by applying the Burer-
Monteiro decomposition [11] to the semideﬁnite relaxation of Problem (MLE).
They showed that all second-order stationary points of the non-convex for-
mulation, which can be computed eﬃciently by the Riemannian trust-region
method [10], correspond to the underlying communities with high probability

Non-Convex Exact Community Recovery

5

−

q)/√p + q

cn−1/6 for some constant c > 0. Despite its low
as long as (p
computational complexity, the approach requires a much stronger condition
on p and q to ensure exact recovery. In particular, it cannot guarantee exact
recovery in the logarithmic sparsity regime of the binary symmetric SBM.

≥

Lastly, let us highlight the improvements made in this paper over its pre-
liminary version [37]. First, the method in [37] is designed for a regularized
version of Problem (MLE). When applying the method to real data sets, which
in general are not generated by the SBM, it is diﬃcult to tune the regulariza-
tion parameter. In the current work, we circumvent this diﬃculty by handling
Problem (MLE) directly, which makes our proposed method simpler and more
practical. Second, compared to its regularized version in [37], Problem (MLE)
is more challenging as it contains an additional linear constraint. Neverthe-
less, we show that a suitably initialized projected power method can solve it
eﬃciently and enjoys the same recovery guarantee as that in [37]. Third, al-
though both the method in [37] and the one proposed in this paper have the
property that an iterate will converge in one step to an optimal solution of
Problem (MLE) once the former is in a suitable neighborhood of the latter,
we show in this paper that the size of the neighborhood can be as large as

(√log n), which improves upon the

O

(1) bound established in [37].

O

1.2 Organization

The rest of this paper is organized as follows. In Section 2, we introduce the
proposed two-stage algorithm for exact community recovery and present the
main result of this paper. In Section 3, we prove the main result and discuss its
consequences. We then report some numerical results in Section 4 and conclude
in Section 5.

∈

Rm×n, we use σmax(A) or

Notation. Let Rn be the n-dimensional Euclidean space. We write matrices
in bold capital letters like A, vectors in bold lower-case letters like a, and
scalars in plain letters. Given a matrix A
A
k
to denote its largest singular value (i.e., spectral norm), σmin(A) its smallest
singular value, and aij its (i, j)-th element. If A is symmetric, then we use
λmin(A) to denote its smallest eigenvalue. Given a vector x
k2
to denote its Euclidean norm, xi its i-th element, and diag(x) the diagonal
matrix with x on its diagonal. We use 1n and En to denote the n-dimensional
n all-one matrix, respectively, and simply write 1 and
all-one vector and n
E when their dimensions can be inferred from the context. Given a positive
integer n, we denote by [n] the set
. Given a discrete set T , we denote
1, . . . , n
}
the number of elements in T . We use sgn to denote the element-wise
by
sign function; i.e., for any x

Rn, we use

{
Rn,

×

∈

x

T

k

k

|

|

∈

[sgn(x)]i =

if xi > 0,
if xi = 0,
if xi < 0,

[n].

i

∈

1,
0,
1,

−






6

P. Wang, Z. Zhou, A. M.-C. So

We use Bern(p) to denote the Bernoulli random variable with mean p. Given
two random variables X and Y , we write X d= Y if X and Y are equal in
distribution.

2 Preliminaries and Main Results

In this section, we formally state the considered problem, present the proposed
algorithm, and give a summary of our main theoretical results.

To begin, let us introduce a central object in our study—the binary sym-

metric SBM.

∈

[0, 1] be parameters with p > q. Furthermore, let x∗

Deﬁnition 1 (Binary Symmetric SBM) Let n
2 be an even integer and
n be
p, q
a label vector representing a partition of [n] into two equal-sized subsets (in
particular, 1T x∗ = 0). We say that a random graph G is generated according
to the binary symmetric SBM with parameters (n, p, q) and label x∗ if G has
aij}1≤i≤j≤n of its adjacency matrix A
vertex set V = [n] and the elements
are generated independently by

∈ {−

1, +1

≥

}

{

aij ∼ (

Bern(p),

Bern(q),

if x∗
if x∗

i x∗
i x∗

j = 1,
j =

−

1.

(1)

Intuitively, the label vector x∗ induces two equal-sized communities in the
graph G. Note that we allow self-loops in G, though our analysis also applies
to the case where no self-loop is allowed (i.e., aii = 0 for all i
[n]); see
Section 3.4.1.

∈

−

Now, given a realization of G that is generated according to the binary
symmetric SBM, the problem of interest is to recover the two communities.
x∗ represents the same community structure as x∗, this is equivalent
Since
x∗ from the adjacency matrix A. As in [2], we say that
to identifying x∗ or
x∗ with probability
an estimator achieves exact recovery 1 if it yields x∗ or
tending to one as n
, where the probability is taken with respect to the
distribution in (1).

→ ∞

−

−

In this paper, we focus on the logarithmic sparsity regime of the binary

symmetric SBM—i.e.,

p =

α log n
n

and q =

β log n
n

(2)

for some constants α > β > 0—and propose to solve the community recovery
problem by directly handling the non-convex ML estimation problem (MLE),
even though it is NP-hard in the worst case. The ﬁrst ingredient in our ap-
proach is a simple iterative procedure called the method of projected power

1 This is also termed strong consistency in the literature; see [29].

Non-Convex Exact Community Recovery

7

iteration, which is essentially the projected gradient method applied to Prob-
lem (MLE). Speciﬁcally, let

:=

x

∈

F

Rn : 1T x = 0, xi =

1, i

±

∈

[n]

(cid:9)
denote the feasible set of Problem (MLE). Furthermore, let
the projection operator onto

; i.e., for any c

Rn,

(cid:8)

Then, the projected power iterations take the form

F

(c) := Argmin

u∈Rn

P

u

c

k

−

k
(cid:8)

∈
2
2 : u

∈ F

.2

(cid:9)

x(k)

∈ P

(Ax(k−1)),

k = 1, 2, . . . .

(3)

: Rn ⇒ Rn be

P

(4)

(5)

As the following proposition shows, for every k
1, the problem of computing
x(k) in (5) boils down to that of ﬁnding the indices that correspond to the n/2
largest entries of Ax(k−1), which can be done eﬃciently.

≥

Proposition 1 For any c

Rn, it holds that v

∈

(c) if and only if

∈ P

1,
1,

ℓ
ℓ

,
[n]

vℓ =

(cid:26)

∈ I
−
∈
\ I
cj for all i
= n/2 and ci ≥

,

(6)

and j

[n]

.

\ I

∈

∈ I

where

I ⊂

[n] satisﬁes

|I|

Proof By (4), we have

,

(cid:9)

∈ F

. The

(c) = Argmin

u∈Rn

P

u

k

−

c

k

2
2 : u

∈ F

= Argmax

u∈Rn

cT u : u

∈ F

(cid:8)
where the second equality is due to the fact that
desired formula (6) follows immediately.

(cid:9)

(cid:8)
2
2 = n for all u

u

k

k

⊔⊓
Despite its simplicity, the method of projected power iteration may not
be eﬀective for solving Problem (MLE) unless a proper initial point x(0) is
available. Thus, we need an additional iterative procedure, which is usually
referred to as the method of orthogonal iteration (see, e.g., [20]) and constitutes
the second ingredient in our approach, to obtain a good initial point for the
projected power iterations. The method of orthogonal iteration starts with a
Rn×2 with orthonormal columns. In iteration k
matrix Q(0)
1, it computes
the QR decomposition of AQ(k−1); i.e.,

≥

∈

AQ(k−1) = Q(k)R(k),

Rn×2 has orthonormal columns and R(k)

R2×2 is upper tri-
where Q(k)
angular. It is known that the distance between the subspace spanned by the
columns of Q(k) and the invariant subspace of A that corresponds to its ﬁrst

∈

∈

2 By convention, we use the symbol “Argmin” to denote the solution set of the associated
minimization problem. When the solution set is known to be a singleton, we use the symbol
“argmin” to denote the unique solution in the set.

8

P. Wang, Z. Zhou, A. M.-C. So

two dominant eigenvalues converges linearly to 0 as k
; see, e.g., [20, The-
orem 8.2.2]. For our purpose, we only perform N orthogonal iterations, where
N is an input parameter of the algorithm. Then, we apply Ritz acceleration
(see, e.g., [20, Chapter 8.3.7]) to the last iterate, which amounts to computing
the eigenvalue decomposition of Q(N )T AQ(N ); i.e.,

→ ∞

Q(N )T

AQ(N ) = H (N )D(N )H (N )T

,

d(N )
1

R2×2 is orthogonal and D(N ) = diag(d(N )
d(N )
2

where H (N )
) is diagonal
. Finally, we extract the column of ¯Q(N ) = Q(N )H (N )
with
that corresponds to the smaller eigenvalue of Q(N )T AQ(N ) to construct a
suitable initial point for the projected power iterations.

∈
| ≥ |

, d(N )
2

1

|

|

∈

We now summarize our proposed method for solving Problem (MLE) in
Rn×2, whose entries are generated
Algorithm 1. It starts with a matrix Y
independently and identically from the standard normal distribution. In the
ﬁrst stage (lines 2–9 of Algorithm 1), the algorithm performs N orthogonal
iterations with the initial iterate Q(0), which is obtained by orthonormaliz-
ing the columns of Y via Q(0) = Y (Y T Y )−1/2, and then applies the Ritz
acceleration. In the second stage (lines 12–19 of Algorithm 1), the algorithm
employs projected power iterations to reﬁne the initial iterate x(0), which is
constructed from a suitable column of ¯Q(N ). The algorithm terminates when
x(k) = x(k−1) for some k in the second stage, at which point it outputs x(k).
We next present the main result of this paper, which shows that Algo-
rithm 1 achieves exact recovery at the information-theoretic limit and also
provides explicit iteration complexity bounds for Algorithm 1.

Theorem 1 Let A be the adjacency matrix of a random graph generated ac-
cording to the binary symmetric SBM with parameters (n, p, q) and label x∗,
where p, q satisfy (2) for some constants α > β > 0. Set N = Θ(log n/ log log n).
√β > √2, then for all suﬃciently large n, the following statement holds
If √α
n−Ω(1): Algorithm 1 takes
with probability at least 1
(log n/ log log n) or-
−
thogonal iterations and
(log n/ log log n) projected power iterations to output
x∗. Here, the probability is taken with respect to the random choices
x∗ or
in A and in Algorithm 1.

O

O

−

−

We remark that the value of N in Theorem 1 can be explicitly given;
see (69). Equipped with Theorem 1, it is not hard to derive the total com-
R2×2,
putational cost of Algorithm 1. Indeed, since Z (k)
∈
(n) time, while the eigen-
the QR decomposition in line 5 can be found in
O
decomposition in line 8 can be found in
(1) time [34]. Moreover, by Propo-
(z(k)) in line 15 can be found by ﬁrst identifying the
sition 1, the projection
(n/2)-th largest element ¯z(k) of z(k), which can be done in
(n) time [9], and
then comparing each element of z(k) with ¯z(k), which can be trivially done in
(n) time. Now, the remaining dominant computational cost is that of com-
Rn×n is generated
Rn is arbitrary. Using a simple

O
puting matrix-vector products of the form Av, where A
according to the setting of Theorem 1 and v

Rn×2 and S(N )

O

O

P

∈

∈

∈

Non-Convex Exact Community Recovery

9

Algorithm 1: A Two-Stage Algorithm for Solving Problem (MLE)

Input: adjacency matrix A, positive integer N
Output: label vector ˆx

1 choose a matrix Y

Rn×2, whose entries are generated independently and

identically from the standard normal distribution

∈

/* stage 1: method of orthogonal iteration with Ritz acceleration

*/

Y (Y T Y )−1/2

2 set Q(0)
←
3 for k = 1, 2, . . . , N do
4

set Z(k)
compute the QR decomposition Z(k) = Q(k)R(k)

AQ(k−1)

←

5

6 end
7 set S(N)
8 compute the eigen-decomposition S(N) = H(N)D(N)H(N)T

Q(N)T

AQ(N)

←

, where

D(N) = diag(d(N)

, d(N)
1
2
Q(N)H(N)

) such that

d(N)
1

|

d(N)
2

|

| ≥ |

9 set ¯Q(N)
10 set ˜y to be the i∗-th column of ¯Q(N), where i∗ = argmini∈{1,2} d(N)
11 set y

(1T ˜y/n)1

←

˜y

i

←

−

/* stage 2: method of projected power iteration

*/

←

2
k

√ny/

12 set x(0)
y
k
13 for k = 1, 2, . . . do
set z(k)
14
←
set x(k)
← P
if x(k) = x(k−1) then

Ax(k−1)
(z(k))

16

15

terminate and output ˆx = x(k)

17

18

end

19 end

O

concentration argument, one can show that the number of non-zero entries in
A is
(n log n) with high probability; see Section 3. Hence, with high probabil-
(n log n) for any v. Putting the above time
ity, the cost of computing Av is
bounds together and using the iteration bounds established in Theorem 1, we
obtain the following corollary.

O

Corollary 1 Consider the setting of Theorem 1. If √α
all suﬃciently large n, the probability that Algorithm 1 outputs x∗ or

√β > √2, then for
x∗ in

−

(n log2 n/ log log n) time is at least 1

n−Ω(1).

O

−
To put the above results in perspective, let us make the following remarks:

−

(a) While Problem (MLE) is known to be NP-hard in the worst case, the as-
sumption that the adjacency matrix A arises from the binary symmetric
SBM in Deﬁnition 1 allows us to conduct an average-case analysis of Al-
√β > √2,
gorithm 1. In particular, if the constants α, β > 0 satisfy √α
which is known to be the information-theoretic limit for exact recovery [2,
29], then with high probability, x∗ and
x∗ are the only optimal solutions
of Problem (MLE) [2]. Moreover, Corollary 1 shows that with high prob-
ability, Algorithm 1 computes an optimal solution of Problem (MLE) in
nearly-linear time. As such, Algorithm 1 is more eﬃcient than SDP-based

−

−

10

P. Wang, Z. Zhou, A. M.-C. So

methods (see, e.g., [7,21]). The time bound for Algorithm 1 is also com-
petitive with those for some of the most eﬃcient methods in the literature
(see, e.g., [4,17,29,38]) under the setting of Theorem 1.

b

→ ∞

(b) In the recent work [3], Abbe et al. showed that the vanilla spectral method,
which ﬁrst computes an exact eigenvector u2 associated with the second-
largest eigenvalue of A and then returns
x = sgn(u2) as the label vector,
achieves exact recovery under the setting of Theorem 1. Conceptually, the
method can be implemented in the framework of Algorithm 1 as follows.
First, by performing N
orthogonal iterations, we obtain a limit point
¯Q(∞), which can be used to construct u2; see [20, Theorem 8.2.2] and
x =
compare with lines 10–11 of Algorithm 1. Then, we return the label
x coincides (up to sign) with
sgn(u2). Incidentally, observe that if the label
the ground-truth label x∗, then u2 has exactly n/2 positive entries and n/2
x can also
negative entries. This, together with Proposition 1, implies that
be computed by projecting u2 onto
F
In actual implementation, however, we need to know when to terminate
the orthogonal iterations, so that the vanilla spectral method can proceed
to the sign-taking step. Unfortunately, the results in [3] do not provide
the required termination criterion. By contrast, Theorem 1 shows that
the underlying communities can be exactly recovered by ﬁrst performing
(log n/ log log n) orthogonal iterations to obtain a coarse approximation
O
y of u2 and then applying
(log n/ log log n) projected power iterations to
a suitably scaled y. As it turns out, by combining our results in Sections 3.1
and 3.2 with the arguments in [3], we can show that the vanilla spectral
(log n/ log log n) orthogonal iterations before the sign-
method only needs
taking step in order to exactly recover the underlying communities with
high probability; see Section 3.4.2. It is worth noting that this gives the
best complexity bound known to date for the vanilla spectral method in
the context of exact community recovery in the binary symmetric SBM.
This further demonstrates the power of our approach.

O

O

b

b

b

.

3 Proof of the Main Result

In this section, we prove our main result (i.e., Theorem 1) concerning the
recovery performance and iteration complexity of Algorithm 1. This involves
establishing some key properties of the orthogonal iterations and projected
power iterations, which will be accomplished in Sections 3.1 and 3.2, respec-
tively.

3.1 Analysis of the Method of Orthogonal Iteration

Our main goal in this sub-section is to provide a probabilistic analysis of the
convergence behavior of the orthogonal iterations deployed in the ﬁrst stage
of Algorithm 1. Such an analysis is not only useful for proving Theorem 1 but

Non-Convex Exact Community Recovery

11

may also be of independent interest. To proceed, let us introduce some further
notation that will be used in the sequel. Let A be as in Theorem 1 and consider
its eigenvalue decomposition A = U ΛU T , where Λ = diag(λ1, . . . , λn) with
λ1 ≥ · · · ≥

λn and U = [u1, . . . , un]. We write

Λα := diag(λ1, λ2), Λβ := diag(λ3, . . . , λn),
Uα := [u1, u2],

Uβ := [u3, . . . , un].

Moreover, we deﬁne, for every k

0,

≥

P (k) := U T Q(k), V (k) := U T

α Q(k), W (k) := U T

β Q(k),

(7)

(8)

0, P (k)

Q(k)
{
Rn×2, V (k)

}k≥1 is generated by Algorithm 1. Note that for all
where the sequence
R(n−2)×2, and P (k) has orthonor-
k
∈
mal columns. Besides, by the CS decomposition (see, e.g., [20, Theorem 2.5.2]),
we have

R2×2, W (k)

≥

∈

∈

min(V (k)) + σ2
σ2

max(W (k)) = 1,

k = 0, 1, . . . .

(9)

It is known that the quantity

σ2
min(V (0)) measures the distance be-
−
tween the subspaces spanned by the columns of Uα and Q(0); see, e.g., [20,
p
Theorem 2.5.1]. To bound this distance, we prove the following result.

1

Lemma 1 For all n

≥

6, it holds with probability at least 1

σmin(V (0))

1
√n2 + 1

.

≥

4

log n/n that

−

p

(10)

Rn×2 is
Proof Recall from Algorithm 1 that Q(0) = Y (Y T Y )−1/2, where Y
a random matrix whose entries are i.i.d. standard normal random variables. By
the deﬁnition of Uα, we have Uα = U E, where E := (e1, e2). This, together
with V (0) = U T
α Q(0), Q(0) = Y (Y T Y )−1/2, U U T = I, and the orthogonal
invariance of the normal distribution, yields

∈

V (0) = ET (U T Y )

(U T Y )T (U T Y )

−1/2 d= ET Y (Y T Y )−1/2 = ET Q(0).

It then follows that

(cid:0)

(cid:1)

a

= P

σmin

ET Q(0)

P

σmin

V (0)

(cid:16)

(cid:16)
= P

(cid:17)
λmin

≤
(cid:17)
Q(0)T

(cid:16)
(cid:16)
EET Q(0)

(cid:16)

(cid:16)

(cid:17)

a2

≤

(cid:17)

≤

a

(cid:17)
a2

(cid:17)
=

0
Z
Q(0)T

(cid:16)

p(x)dx,

(11)

EET Q(0)

. By [5,

(cid:17)

) is the probability density function of λmin

where p(
·
eq. (6)], p(
·

) takes the form

p(x) =

(n

−

2)(n
4

−

3)

(1

·

−

x)n−3
x

1

y

1 +

0
Z

(cid:18)

x

y

1

−
x

− 1
2

(cid:19)

n

y)

(1

−

5

−

2 dy (12)

12

P. Wang, Z. Zhou, A. M.-C. So

for any x
(11). Indeed, using the Cauchy-Schwarz inequality, we get

(0, 1]. This allows us to derive an upper bound on the integral in

∈

1

y

1 +

0
Z

(cid:18)

x

y

1

−
x

− 1
2

(cid:19)

−
x

1

1

1 +

x

y

≤  Z

0 (cid:18)
x

=

1

(cid:18)

−

x ·

log

1
x

(cid:18)

n

5

−
2 dy

y)

(1

−

−1

1
2

dy

!

·

0
(cid:18)Z

(cid:19)
1
2

(cid:19)(cid:19)

(n

·

(cid:18)

−

2)(n

Together with (12), this implies that for all n

≥

1

y2(1

−

1
2

y)n−5dy

(cid:19)

1
2

.

2

−
6,

3)(n

4)

(cid:19)

−

(n

2)(n
4

−
√n

3)

−

a2

·

0 s

Z

1
x

log

1
x

(cid:18)

(1

(cid:19)

−

x)n− 7

2 dx

a2

0
Z

p(x)dx

≤

≤

√2
4 · p
√2n
4

·

−
1
x

a2

0 s

Z

log

1
x

(cid:18)

dx,

(cid:19)

where the second inequality follows from the fact that for all n
(n
1 for every 0
2)(n
≤
letting x = e−2z2

n(n
−
, we obtain

x)n−7/2

4) and 0

3)

(1

−

−

−

≤

≤

(13)

6, we have
1. By
x

≤

≥
≤

a2

0 s

Z

1
x

log

1
x

(cid:18)

dx = 4√2

(cid:19)

∞

a )
qlog( 1

Z

z2e−z2

dz.

(14)

Moreover, using integration by parts, we compute

∞

a )
qlog( 1

Z

z2e−z2

dz =

a
2 s

log

1
a

(cid:18)

(cid:19)

+

1
2

∞

a )
qlog( 1

Z

e−z2

dz.

(15)

Notice that for any a

≤
∞

1/e, we have

log(1/a)

1, which leads to

≥

e−z2

dz

p

∞

≤

a )
qlog( 1

Z

z2e−z2

dz.

(16)

a )
qlog( 1

Z

Combining (15) and (16), we have that for any a

∞

a )
qlog( 1

Z

z2e−z2

dz

a

≤

log

s

This, together with (11), (13), and (14), yields

1/e,

≤

1
a

.
(cid:19)

(cid:18)

P

σmin

V (0)

(cid:16)

(cid:16)

(cid:17)

a

≤

≤

(cid:17)

2√na

log

s

1
a

(cid:18)

(cid:19)

Non-Convex Exact Community Recovery

for all n
for any n

6 and a
6) in the above inequality, we obtain

≤

1/e. By letting a = 1/√n2 + 1 (which satisﬁes a

≥
≥

13

1/e

≤

P

σmin

V (0)

(cid:18)

(cid:17)
which implies Lemma 1 as desired.

(cid:16)

1
√n2 + 1 (cid:19)

≤

≤

2√n
√n2 + 1 r

log

n2 + 1

(cid:16)p

log n
n

,

4

≤

r

(cid:17)

⊔⊓
Next, we present a spectral bound on the deviation of A from its mean. It

is a direct consequence of [22, Theorem 5.2] and thus we omit its proof.

Lemma 2 There exist constants c1 ≥
on α and β, such that
E[A]

A

holds with probability at least 1

−

1 and c2 > 0, whose values depend only

k

−

k ≤
c2n−3.

c1

log n

p

(17)

Based on Lemma 2, we can establish the following corollary, which provides
estimates on the eigenvalues and eigenvectors of A.

Corollary 2 With probability at least 1

c2n−3, the following statements hold:

(18)

(19)

(20)

(21)

(22)

≤

log n

−
λ1 ≤
λ2 ≤
≤
i = 3, . . . , n,

log n

α + β
2

α

β

−
2

log n + c1

log n,

log n + c1

p

log n,

p

α + β
2

log n

c1

−

log n

p

p

c1

−
log n,

p
θu1 −

1
√n
x∗
√n

|

α

β

c1

−
2
λi| ≤
min
θ∈{±1} (cid:13)
(cid:13)
(cid:13)
min
(cid:13)
θ∈{±1} (cid:13)
(cid:13)
(cid:13)
(cid:13)

,

c3
√log n
c3
√log n

2 ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2 ≤
(cid:13)
(cid:13)
(cid:13)
where c1, c2 are the constants in Lemma 2 and c3 := 2√2c1/ min
(cid:13)
{

θu2 −

,

}
Proof Suppose that the statement in Lemma 2 holds, which happens with
νn be the eigenvalues of
probability at least 1
E[A]. It follows from Weyl’s inequality (see, e.g., [36, Theorem 4.5.3]) that

c2n−3. Let ν1 ≥

ν2 ≥ · · · ≥

−

−

β, (α

β)/2

.

λi −

|

νi| ≤ k

A

−

E[A]

,

k

i = 1, 2, . . . , n.

According to the binary symmetric SBM in Deﬁnition 1, we have

E[A] =

p + q
2

11T +

p

q

−
2

x∗x∗T .

(23)

(24)

2 = n, we see that E[A] is a rank-2 matrix
Since 1T x∗ = 0 and
with 1 and x∗ being the eigenvectors associated with the largest and second-
largest eigenvalues, respectively. Using (2), we can compute

2
2 =

x∗

1

k

k

k

k

2

ν1 =

α + β
2

log n,

ν2 =

α

β

−
2

log n,

νi = 0,

i = 3, . . . , n.

(25)

14

P. Wang, Z. Zhou, A. M.-C. So

By (23), (25), and Lemma 2, the desired results (18)–(20) are immediate.
Moreover, it follows from (25) that

δ1 := min
i6=1 |

ν1 −

νi|

= β log n,

δ2 := min
i6=2 |

ν2 −

νi|

= min

β,

(cid:26)

α

β

−
2

(cid:27)

log n.

This, together with the Davis-Kahan theorem (see, e.g., [36, Theorem 4.5.5])
and Lemma 2, yields

θu1 −
min
θ∈{±1} (cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√n

2 ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)

and

E[A]

2√2

k

A

−
δ1

2√2c1
β√log n ≤

c3
√log n

k

≤

θu2 −
min
θ∈{±1} (cid:13)
(cid:13)
(cid:13)
(cid:13)

x∗
√n

2 ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)

The proof is then completed.

2√2

k

A

−
δ2

E[A]

k

≤

min

=

c3
√log n

.

√log n

2√2c1
β, α−β
2

n

o

⊔⊓
Now, we are ready to analyze the convergence of the orthogonal iterations.

From Algorithm 1, one can verify by induction that for every k

1,

≥

AkQ(0) = Q(k) ˜R(k),

(26)

where ˜R(k) := R(k)R(k−1)
· · ·
(7)–(8) and A = U ΛU T , yields

R(1) with ˜R(1) = R(1). This, together with

Λk

αV (0) = V (k) ˜R(k), Λk

βW (0) = W (k) ˜R(k).

(27)

Suppose that (10) and λ1 ≥
implies that the square matrix V (k) is invertible and ˜R(k) = (V (k))−1Λk
Together with (27), this leads to

αV (0) is non-singular, which
αV (0).

λ2 > 0 hold. Then, Λk

K (k) = Λk

βK (0)Λ−k
α ,

(28)

and k(k)
where we deﬁne K (k) := W (k)(V (k))−1 for all k
be the ﬁrst and second column of K (k), respectively. The following result
characterizes the convergence rates of k(k)

0. Let k(k)

≥

1

2

and k(k)
2 .

1

Proposition 2 Suppose that n
and λ2 > 0 hold. Then, for every k

≥

exp(16c2

1/(α

0, it holds that

−

β)2) and that (10), (18)–(20),

≥

k(k)
1 k2 ≤

k

n

(cid:18)

4c1
(α + β)√log n

k

(cid:19)

and

k(k)
2 k2 ≤

k

n

4c1
β)√log n

k

.

(cid:19)

(29)

(α

(cid:18)

−

Non-Convex Exact Community Recovery

15

Proof It follows from (28) and λ1 ≥

λ2 > 0 that

k(k)
1 k2 ≤

k

k

¯λ
λ1 (cid:19)

(cid:18)

K (0)

,

k

k

k(k)
2 k2 ≤

k

(cid:18)

where ¯λ = max

, . . . ,

λ3|

{|

λn|}

|

. By (20), we have ¯λ

≤

k

,

K (0)

¯λ
λ2 (cid:19)
c1√log n. Moreover,

k

k

(30)

K (0)

k

=

k

k

W (0)(V (0))−1

σmax(W (0))
σmin(V (0))

=

k ≤

1

σ2
min(V (0)) −

s

n,

1

≤

where the second equality follows from (9) and the last inequality is due to
(10). These, together with (18) and (19), yield

k(k)
1 k2 ≤

k

n

k(k)
2 k2 ≤

k

n

2c1√log n

(α + β) log n

(cid:18)

−

2c1√log n

2c1√log n

(α

(cid:18)

−

β) log n

−

2c1√log n

k

(cid:19)
k

(cid:19)

,

.

≥

1/(α

exp(16c2

β) log n, or equivalently, n

In particular, the desired result (29) holds for all n satisfying 4c1√log n
(α

β)2).
−
Next, we study the eﬀect of Ritz acceleration. Let

≤
⊔⊓
N −1
k≥1 be an auxil-
iary sequence constructed via ¯Q(k) = Q(k)H (k), where H (k) is obtained from
the eigen-decomposition of Q(k)T AQ(k)—i.e., Q(k)T AQ(k) = H (k)D(k)H (k)T
with D(k) = diag(d(k)
. We are interested in relating
the Ritz eigenvalues d(k)
to the eigenvalues λ1, λ2 of A. Towards that end,
we deﬁne

1 , d(k)
1 , d(k)

d(k)
1 | ≥ |

2 ) and

d(k)
2 |

¯Q(k)

−

}

{

2

|

(k)

d

:= max

{

d(k)
1 , d(k)
2 }

,

d(k) := min

{

d(k)
1 , d(k)
2 }

.

Then, we have the following estimates.

Proposition 3 For every k

0, it holds that

(k)

λ1 ≥

d

≥
λ1 −

2

A

k(k)
1 k

2
2,

d(k).

λ2 ≥

(31)

≥

k · k

k
1 , d(k)
Proof For ease of exposition, we shall omit the superscript (k) in d(k)
2 ,
(k)
1 , and K (k) throughout the proof. Since
d
λ2 are the largest two eigenvalues
d
≥
of A, we have λ1 ≥
d from the Courant-Fischer minimax theorem
(see, e.g., [20, Theorem 8.1.2]). Moreover, using (7)–(8) and the deﬁnition of
K and letting ˆe1 = (1, 0), we have

, d(k), Q(k), P (k), V (k), W (k), k(k)
d are the eigenvalues of QT AQ and λ1 ≥
d and λ2 ≥

(V −1 ˆe1)T QT AQ(V −1 ˆe1) = ˆeT

1 (Λα + K T ΛβK)ˆe1 = λ1 + kT

1 Λβk1

and

V −1 ˆe1k

k

2 = (V −1 ˆe1)T P T P V −1 ˆe1 = ˆeT
2

1 (I + K T K)ˆe1 = 1 +

k1k

k

2
2,

(32)

16

P. Wang, Z. Zhou, A. M.-C. So

where the ﬁrst equality in (32) is due to the fact that P has orthonormal
columns. Now, since d is the largest eigenvalue of QT AQ, we have

d

≥

which leads to

(V −1 ˆe1)T QT AQ(V −1 ˆe1)
V −1 ˆe1k

2
2

k

=

λ1 + kT
1 +

1 Λβk1
2
k1k
2

k

,

d

−

−

λ1k

λ1 ≥

2 + kT
2
1 Λβk1
2
k1k
2
The proof is then completed.
(k)

k1k
1 +

k

2

≥ −

A
k
1 +

2
2

k1k
k · k
2
k1k
2 ≥ −
k

2

k

A

k · k

2
2.

k1k

(k)

Now, let h
, d(k), respectively. Furthermore, we deﬁne q(k) := Q(k)h

⊔⊓
, h(k) denote the eigenvectors associated with the eigenvalues
and q(k) :=
d
Q(k)h(k). Equipped with Propositions 2 and 3, we can establish the conver-
gence rate of the orthogonal iterations.
Theorem 2 Let c1 ≥
that

1 and c2 > 0 be the constants in Lemma 2. Suppose

(k)

(cid:18)
Then, it holds with probability at least 1

(cid:26)

n

max

exp

≥

, 6

.

(33)

16c2
1
β2
(cid:19)
c2n−3

−

(cid:27)
4

−

log n/n that

min
θ∈{±1} k

q(k)

θu1k2 ≤

n

−

min
θ∈{±1} k

q(k)

θu2k2 ≤

n

−

8α
β

+ 10

p

4c1
(α + β)√log n

k

(cid:19)

(cid:19) (cid:18)

16α
β

+ 18

4c1
β)√log n

(cid:19)

(cid:19) (cid:18)

(α

−

,

k

(cid:18)

(cid:18)

k

∀

≥

0,

(34)

,

k

∀

≥

¯K, (35)

where

¯K =

2 log n + log

8(α+β)
β



(cid:16)
log log n + 2 log

(cid:17)

α+β
4c1

.



(36)

Proof Again, we shall omit the superscript (k) throughout the proof for ease
of exposition. By deﬁnition, we have





(cid:16)

(cid:17)





QT AQh = d h, QT AQh = d h.

(37)

Observe that for any v

R2 with

∈
T
v)v
T
2(h

h

k

−

sgn(h

2

−

≤

v

k

−

2
2 = 2

k

k2 = 1, we have
2sgn(h

v)h

T

T

v = 2

2

|

−

T
h

v

|

v)2 = 2(hT v)2,

where the ﬁrst equality follows from
sgn(a)
and the last equality is due to

a for any a

∈

v

·

h
a
=
k2 = 1, the second equality uses
|
|
k
T
R, the inequality follows from
v
h
k2 = 1,
|
k2 = 1 and H being orthogonal. This gives
(38)

k
sgn(h

h
k2k

| ≤ k

hT v

v)v

√2

h

v

T

k

−

k2 ≤

|

|

Non-Convex Exact Community Recovery

17

for any v

|

∈
(λ1 −
≤ k
=

k

R2 with
v
d)hT V −1 ˆe1|
λ1V −1 ˆe1 −
I
ˆe1 −
λ1
K
(cid:21)
k1k2,

(cid:20)
A

(cid:13)
(cid:13)
2
(cid:13)
k
(cid:13)

k2 = 1. Moreover, we obtain from (37) that

|

λ1hT V −1 ˆe1 −
=
P T ΛP V −1 ˆe1k2 ≤ k

Λα 0
0 Λβ(cid:21) (cid:20)

I
K

(cid:20)

(cid:21)

ˆe1

hT QT AQV −1 ˆe1|
λ1P V −1 ˆe1 −
λ1k1 −
=

k

Λβk1k2

ΛP V −1 ˆe1k2

≤

k · k
where the ﬁrst inequality is due to
second inequality follows from P T P = I and
equality uses (8) and K = W V −1. Then, by letting ¯θ = sgn(h
have

(39)
k2 = 1 and QT AQ = P T ΛP , the
h
v
k2, and the second
V −1 ˆe1), we

k2 ≤ k

P T v

k

k

T

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

hT V −1 ˆe1|
√2
|
V −1 ˆe1k2 ≤
k

2√2

A
k1k2
k · k
k
d
λ1 −
|
|

,

(40)

where the ﬁrst inequality follows from (38) and the second one is due to (39)
1 (implied by (32)). Also, by (8) and U T u1 = e1, we obtain
and

V −1 ˆe1
V −1 ˆe1k2 (cid:13)
2 ≤
(cid:13)
(cid:13)
(cid:13)

¯θ

h

−

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
V −1 ˆe1k2 ≥
k
QV −1 ˆe1 −
It then follows that

k

u1k2 =

P V −1 ˆe1 −

k

e1k2 =

I
K

(cid:20)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Qh

=

k1k2.

k

(41)

ˆe1 −

e1

(cid:21)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

min
θ∈{±1} k

q

−

≤

(cid:13)
(cid:13)
(cid:13)
+
(cid:13)

≤

≤

−

q

θu1k2 ≤ k
QV −1 ˆe1
V −1 ˆe1k2 (cid:13)
2
(cid:13)
(cid:13)
¯θu1
(cid:13)

Qh

¯θ

−

k

¯θ
(cid:13)
(cid:13)
(cid:13)
h
(cid:13)

k
u1
V −1 ˆe1k2 −
V −1 ˆe1
¯θ
V −1 ˆe1k2 (cid:13)
−
(cid:13)
k
(cid:13)
(cid:13)
2√2
A
k1k2
(cid:13)
(cid:13)
+
(cid:13)
(cid:13)
k · k
k
d
λ1 −
|
|
2√2
A
k
k
d
λ1 −
|

! k

+ 2

|

p
k1k2,

2
(cid:13)
(cid:13)
(cid:13)
+ k
(cid:13)

2

≤  

+

k

−

k
QV −1 ˆe1
V −1 ˆe1k2 −

¯θu1k2 =
¯θ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
QV −1 ˆe1 −
V −1 ˆe1k2
+

u1k2

1 +

k
k1k2
k
1 +
k

2
2

k1k

p

¯θu1k2
¯θ

u1
V −1 ˆe1k2 (cid:13)
2
(cid:13)
(cid:13)
(cid:13)

k

+

1
V −1 ˆe1k2 −
(cid:12)
k
(cid:12)
2
k1k
1
(cid:12)
2 −
(cid:12)
2
k1k
2
k

k
1 +

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

p

(42)

where the third inequality uses the fact that Q has orthonormal columns,
¯θ
u1k2 = 1; the fourth one follows from (32), (40), and (41); the
= 1, and
|
|
2
k1k
k1k
1 +
last one uses
2 −
By a similar argument, one can show that

k1k2.

2
2 ≥

1 and

≤ k

1 +

k

k

k

1

p

p

min
θ∈{±1} k

q

−

θu2k2 ≤  

2√2
A
k
k
d
λ2 −
|

|

+ 2

k2k2.

! k

(43)

Now, suppose that (10), (18)–(20), and λ2 > 0 hold, which happens with
probability at least 1
log n/n for all n satisfying (33) due to

c2n−3

4

−

−

p

18

P. Wang, Z. Zhou, A. M.-C. So

Lemma 1, Corollary 2, and the union bound. Then, by (18), (19), Proposition
3, and n

exp(16c2

1/β2), we obtain

≥

| ≥
Moreover, it follows from (18)–(20) and n

|

−

λ1 −

λ2 ≥

β log n

λ1 −

d

2c1

log n

β
2

≥

log n.

(44)

p
exp(4c2

1/(α + β)2) that

≥

A
k

k

= λ1 ≤

α + β
2

log n + c1

log n

≤

(α + β) log n.

(45)

These, together with the ﬁrst inequality in (29), imply (34) as desired. Besides,
¯K,
using n > exp(16c2

1/(α + β)2) and (29), one can verify that for every k

p

k(k)
1 k2 ≤

k

n

4c1
(α + β)√log n

¯K

β

(cid:18)

(cid:19)
where the last inequality follows from (44) and (45). It then follows from
¯K.
k1k
Proposition 3 that λ1 −
Together with (44), this implies that for all k

λ2)/2 for all k

k · k

A

≤

≥

(k)

d

2

k

≤ s

8(α + β) ≤ s

≥

λ2

λ1 −
A
4
k
k

,

(k)

(k)

d

λ1 + λ1 −
The desired result (35) then follows from (29), (43), (45), and (46).

λ2 ≥

λ2 = d

log n.

−

−

≥

(46)

⊔⊓

2
(λ1 −
2 ≤
¯K,
≥
λ1 −
2

λ2

β
4

3.2 Analysis of the Method of Projected Power Iteration

Now, let us turn to study the convergence behavior of the projected power
iterations employed in the second stage of Algorithm 1. Our goal is to show that
if x(0) is properly chosen, then the projected power iterations will terminate in
a ﬁnite number of iterations and output the ground-truth label vector x∗ with
high probability. To begin, let A be as in Theorem 1. In particular, Lemma 2
and Corollary 2 can be applied here.

Recall that the projected power iterations take the form

x(k)

∈ P

(Ax(k−1)),

k = 1, 2, . . . ,

P

is the projection operator onto

; see (3)–(5). The following result
where
shows that
is
a discrete set. Such a property plays an important role in the analysis of the
projected power iterations; cf. [25,26].

F
possesses a Lipschitz-like property, despite the fact that

F

P

Rn is arbitrary and ε > 0 is constant such that

Lemma 3 Suppose that c

∈

ci

ε,

≥
≤ −

ε,

(

i
i

,
[n]

∈ I
∈

\ I

for some
v′

[n] with
(c′), it holds that

I ⊂

∈ P

= n/2. Then, for any v

|I|

v′

v

k

−

k2 ≤

c

2

k

−
ε

c′

k2

.

(47)

(c), c′

∈

∈ P

Rn, and

(48)

19

(c)

∈ P

(49)

(50)

[n]

.

\J

Non-Convex Exact Community Recovery

Proof By (47) and Proposition 1, we see that
satisﬁes

P

(c) is a singleton and v

vi =

(cid:26)
Rn be arbitrary and v′

1,
1,

−

∈ P

Let c′
that

∈

i
i

,
[n]

∈ I
∈

.

\ I

(c′). It then follows from Proposition 1

for some
For ease of exposition, we write

[n] with

J ⊂

|J |

1,
1,

v′
i =

,
∈ J
[n]
−
∈
= n/2 such that c′

i
i

(cid:26)

c := [n]

I

\ I

\ J
c′
j for all i
and

c := [n]

i ≥

J

and j
∈
. Since

∈ J

\ J

|I ∩ J |

+

c

|I ∩ J

|

=

=

n
2

,

+

c

|I

∩ J |

=

|J |

=

n
2

,

|I ∩ J |

|I|

c

we deduce that
|I ∩ J
(49) and (50), we have

c

=

|

|I

∩ J |

= s for some 0

s

≤

≤

n/2. In addition, by

v′
i =

vi −

0,
2,
2,




−

i
i
i

)
(
I ∩ J
∈
c,
∈ I ∩ J
c
.
∈ I
∩ J

c

(
I

∪

∩ J

c),

Since

c

|I ∩ J

|

=

c

|I

∩ J |


= s, this yields

v

k

−

v′

k2 = 2√2s.

(51)

On the other hand, we have

c

k

−

c′

k

2
2 =

n

i=1
X

(ci −

c′
i)2

≥

Xi∈I∩J c

(ci −

c′
i)2 +

(cj −

c′
j)2.

(52)

Xj∈I c∩J
and j

It follows from (47) that ci −
from (50) that c′
c′
i for any i
j ≥
, it holds that
and j

c

cj ≥

∈ J

2ε for any i
c and j

∈ J

∈ I

∈ I
. Thus, for every i

c. Besides, recall
c

∈ I ∩ J

∈ I

∩ J

(ci −

i)2 + (cj −
c′

j)2
c′

≥

1
2

(ci −

cj

≥2ε

+ c′

j −
≥0

c′
i

)2

≥

2ε2,

where the ﬁrst inequality is due to a2 + b2
(52) and

| {z }
(a
≥
= s, we obtain

=

c

c

|I ∩ J

|

|I

∩ J |

| {z }
b)2/2 for any a, b

−

R. Using

∈

c

k

−

c′

2
2 ≥

k

2sε2.

(53)

The desired result (48) then follows from (51) and (53).

⊔⊓
Next, we recall the following result, which is established in [2] and pertains

to the diﬀerence of two binomial random variables; see also [3, Lemma 8].

20

P. Wang, Z. Zhou, A. M.-C. So

Lemma 4 Let α > β > 0 be given constants. Suppose that
i.i.d. Bern(α log n/n) and
Zi}
Wi}
being independent of

Wi}
n/2
i=1 are i.i.d. Bern(β log n/n), with
{

{
n/2
i=1. Then, for any γ

R, it holds that

∈

{

{

n/2
i=1 are
n/2
Zi}
i=1

n/2

n/2

P



Wi −

i=1
X

i=1
X

Zi ≤

γ log n



≤

n− (√α

√β)2
−
2

+ γ(log α
−
2

log β)

.

(54)




Equipped with Lemmas 3 and 4, we can show that the set-valued map
x Z
(Ax) possesses a contraction property in a certain neighborhood of x∗.
This would then imply the linear convergence of the projected power iterations.

⇒ P

√β > √2.
Proposition 4 Suppose that the constants α > β > 0 satisfy √α
−
Then, there exists a constant γ > 0, whose value depends only on α and β,
n−Ω(1): For
such that the following statement holds with probability at least 1
all x

Rn such that 1T x = 0,

−

x

∈

k

k2 = √n, and
n
log n

k2 ≤

5c3

,

r

x∗

x

k

−

one has

x∗

v
k

−

k2 ≤

c4
γ√log n k

x

x∗

k2

−

for any v
in Lemma 2 and Corollary 2, respectively.

(Ax), where c4 := 3c3(α

∈ P

−

β) + 2c1 and c1, c3 are the constants

Proof Since √α
on α and β, such that

−

√β > √2, there exists a γ > 0, whose value depends only

(√α

c5 :=

−
2

√β)2

γ(log α

−

log β)

−
2

1 > 0.

−

(57)

According to the binary symmetric SBM in Deﬁnition 1, we have

i (Ax∗)i =
x∗

n

aijx∗

i x∗
j

d=

n/2

n/2

Wi −

Zi

[n], where

for every i
∈
Bern(β log n/n), and
{
from Lemma 4, (57), and the union bound that

Wi}
{
Zi}

i=1
X

j=1
X
n/2
n/2
i=1 are i.i.d. Bern(α log n/n),
i=1 are i.i.d.
n/2
i=1. It then follows

n/2
i=1 are independent of

Zi}

i=1
X

{

Wi}

{

γ log n

(58)

min
i∈[n]

i (Ax∗)i ≥
x∗
n−c5.

−

holds with probability at least 1

In the rest of the proof, we suppose that both (17) and (58) hold, which
n−c5 due to Lemma 2 and the
c2n−3
. Since x∗
c = [n]
, we

happens with probability at least 1
[n] : x∗
union bound. Let

=

i

−
i = 1

−
and
I

}

\ I

∈ F

I

{

∈

(55)

(56)

Non-Convex Exact Community Recovery

21

−

≥

have
|I|
implies that

=

|I

= n/2 and x∗

i =

c

|

1 for all i

∈ I

c. This, together with (58),

(Ax∗)i

γ log n,

(

≤ −

γ log n,

i

i

,
c.

∈ I

∈ I

(59)

It then follows from Proposition 1 that
such that 1T x = 0,
and Lemma 3, we obtain

P
k2 = √n, and (55) holds. By (59),

(Ax∗) =

x∗

x

k

}

{

P

. Now, let x

∈
(Ax∗) =

Rn be
x∗
,

{

}

x∗

v

k

−

k2 ≤

2

k

Ax

Ax∗

−
γ log n

k2

(60)

for any v

∈ P

(Ax). In addition, using

x

k

−

x∗

2
2 =

k

2
2 +

x

k

k

x∗

2
2 −

k

k

k
2xT x∗ = 2

x∗

k

k2 =
x∗

2
2 −

k

x

k2, we compute
2xT x∗ = 2(x∗T x∗

xT x∗).

−

k

This, together with (17), (24), 1T x = 1T x∗ = 0, and

x∗

k2 = √n, yields

E[A](x
p + q
2

x∗) + (A
q
p

−
11T +

−
2

E[A])(x

−
x∗x∗T

−

(x

−

(cid:19)

≤

=

≤

(cid:18)

(cid:13)
(cid:13)
+
(cid:13)
(cid:13)

A
k
p

−
q

−
2
q

(cid:13)
(cid:13)
p
(cid:13)
(cid:13)

−
4

E[A]

k · k

xT x∗

x

x∗

−
x∗T x∗

k2

x∗

+

(cid:16)
√n

x

k

−

−

x∗

(cid:17)
2
2 + c1

k

2
(cid:13)
(cid:13)
(cid:13)
log n
(cid:13)

k
x∗)

k2
x∗)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
E[A]

A

k

−

x

−

k · k

x∗

k2

x

k

−

x∗

k2.

(61)

Ax

k

−

Ax∗

k2 =

k

Then, by (2), (55), (60), and (61), we obtain for any v

p

(Ax) that

∈ P

x∗

v
k

−

k2 ≤

2

k

Ax

Ax∗

−
γ log n
(α

k2

2
γ log n
3c3(α

−

β) log n
4√n
β) + 2c1

(cid:18)

−

γ√log n

c4
γ√log n k

x

−

x∗

x

k
k2.

≤

≤

=

x

k

−

x∗

k2 + c1

log n

p

x∗

k2

x

k

−

x∗

k2

(cid:19)

−

This completes the proof.

Since the feasible set

⊔⊓
of Problem (MLE) is discrete, the contraction prop-
erty (56) suggests that if an iterate is suﬃciently close to x∗, then the projected
power iterations will exhibit one-step convergence to x∗; i.e., all subsequent
iterates will stay at x∗. This would then imply the ﬁnite termination of stage
2 of Algorithm 1. Let us now formalize the above observation.

F

22

P. Wang, Z. Zhou, A. M.-C. So

√β > √2
Proposition 5 Suppose that the constants α > β > 0 satisfy √α
and let γ > 0 be such that (57) holds. Then, the following statement holds with
√2γ log n,
probability at least 1
one has

n−Ω(1): For all x

such that

k2 ≤

∈ F

x∗

−

−

−

x

k

(Ax) =

P

x∗

.

}

{

(62)

Proof Suppose that (58) holds, which happens with probability at least 1
n−c5, where c5 > 0 is given in (57). Let
Since x∗
∈ F
be such that
and an S′

i
∈
I
= n/2 and x∗
i =
=
|I
√2γ log n. Since x, x∗
S′

c = [n]
I
c. Let x
∈ I
, there exist an S

i = 1
}
1 for all i

n/2 such that

−
∈ F

[n] : x∗

−
.
\ I
∈ F
⊂ I

and

=

{

, we have
x∗
x
k
−
c with
⊂ I

|I|
k2 ≤
=
S
|
|

|

|

c

| ≤
x = x∗

2eS + 2eS′,

−
where eS (resp. eS′ ) is an n-dimensional vector with (eS)i = 1 if i
(resp. S′) and 0 otherwise. Observe that

S

∈

2γ log n

≥ k

x

p

This, together with
, we get
i

=

S

|

|

|

∈ I

−
S′

x∗

k2 = 2
, implies that

eS′ −
S

k

|

|

|

eSk2 = 2
S′
=

|

| ≤

|
p

+

S

|

S′

.

|

|

γ log n/4. Now, for all

(Ax)i = (Ax∗)i −
2
−
≥

γ log n

2

γ log n
1
2

−
γ log n,

≥

≥

2(AeS)i + 2(AeS′)i

aij + 2

aij

Xj∈S′

Xj∈S
S
|
|

where the ﬁrst inequality follows from (58), the second inequality uses 0
aij ≤
can show that for all i

≤
γ log n/4. By the same argument, we

1, and the last one is due to

| ≤

c,

S

|

∈ I

(Ax)i ≤ −

1
2

γ log n.

These, together with Proposition 1, imply (62) as desired.

⊔⊓
The next result establishes the iteration complexity of the projected power
iterations for ﬁnding the ground-truth label x∗. Recall that c1, c3, and c4 are
the constants deﬁned in Lemma 2, Corollary 2, and Proposition 4, respectively,
whose values depend only on α and β.

Theorem 3 Suppose that the constants α > β > 0 satisfy √α
and let γ > 0 be such that (57) holds. Suppose in addition that

−

√β > √2

n > max

exp

(cid:26)

(cid:18)

c2
4
γ2

, exp

(cid:19)

(cid:18)

5c3
√2γ

.

(cid:19)(cid:27)

(63)

Non-Convex Exact Community Recovery

Let

x(k)

{

}k≥0 be the sequence generated by Algorithm 1. If x(0) satisﬁes
1T x(0) = 0,

x(0)

x(0)

x∗

,

5c3

k2 = √n,

k

k

−

k2 ≤

n
log n

r

then the following statements hold with probability at least 1

n−Ω(1):

−

(i) For all k

≥

and

1, it holds that

x(k)

k

−

x∗

k2 ≤

c4
γ√log n k

x(k−1)

x∗

k2

−

(ii) There exists some k

≤

x(k)

x∗

5c3

k2 ≤

−

k
r
˜K such that x(k) = x(k−1) = x∗, where

n
log n

.

˜K =



log n

log log n + 2 log

+ 2.



γ
c4

23

(64)

(65)

(66)

(67)





(cid:17)
Proof Suppose that the statements in Propositions 4 and 5 hold simultane-
n−Ω(1) by the union bound.
ously, which happens with probability at least 1
We ﬁrst prove (i). It follows from (64) that (66) holds for k = 0. Moreover, by
(64), Proposition 4, and x(1)
(Ax(0)), one can observe that (65) holds for
k = 1. These, together with (63), yield

∈ P

−

(cid:16)





x(1)

k

−

x∗

k2 ≤

c4
γ√log n k

x(0)

x∗

−

k2 ≤

5c3

n
log n

r

and thus (66) holds for k = 1. Then, (i) can be established by a simple induc-
tive argument. Next, we prove (ii). By (63), we have c4/(γ√log n) < 1 and
5c3/√log n < √2γ log n. This, together with (64), (65), and (67), yields

x( ˜K−2)

k

x∗

−

k2 ≤ k

x(0)

−

x∗

k2

5c3

≤

r

n
log n

(cid:18)

c4
γ√log n

(cid:19)

˜K−2

c4
γ√log n

(cid:18)

(cid:19)
log n
log log n+2 log(γ/c4)

=

5c3

√log n ≤

2γ log n.

p

By Proposition 5 and the projected power iterations (5), we have x( ˜K−1) = x∗.
By applying Proposition 5 to x( ˜K−1), we further have x( ˜K) = x( ˜K−1) = x∗.
This, together with the stopping criterion in Algorithm 1, completes the proof
of (ii).

⊔⊓
Let us make two remarks before we leave this sub-section. First, due to the
symmetry of Problem (MLE), Theorem 3 also holds if we replace all the x∗ in
x∗. Second, Theorem 3(ii) implies that the second stage of
its statement by
Algorithm 1 terminates in a ﬁnite number of iterations at an optimal solution
of Problem (MLE), provided that x(0) is properly chosen. In particular, for
any x(0) satisfying (64), it terminates in roughly
(log n/ log log n) iterations
for all suﬃciently large n.

O

−

24

P. Wang, Z. Zhou, A. M.-C. So

3.3 Proofs of Theorem 1 and Corollary 1

With the preparations in Sections 3.1 and 3.2, we are ready to establish the
main results stated in Section 2. We ﬁrst provide a formal version of Theorem
1 and its proof. Recall that the constants c1, c3, c4 are given in Lemma 2,
Corollary 2, and Proposition 4, respectively, and their values depend only on
α and β.

Theorem 4 Consider the setting of Theorem 1. Suppose that √α
and let γ > 0 be such that (57) holds. In addition, suppose that

−

√β > √2

n > max

exp

(cid:26)

min
{

(cid:18)

16c2
1
β)2, β2

(α

−

, exp

} (cid:19)

(cid:18)

5c3
√2γ

c2
4
γ2

, exp

(cid:19)

(cid:18)

, 6

.

(68)

(cid:19)

(cid:27)

Then, with probability at least 1
−
thogonal iterations and N2 projected power iterations to ﬁnd x∗ or

n−Ω(1), Algorithm 1 takes at most N1 or-
x∗, where

−

N1 =



2 log n + log log n + 2 log

16α+18β
β·min{1,c3}




N2 =

log log n + 2 log

(cid:16)
α−β
4c1

(cid:16)

(cid:17)

log n



log log n + 2 log



γ
c4

+ 2.

,

(cid:17)







(69)

(70)





(cid:16)

(cid:17)





Proof Suppose that the statements in Lemma 2, Theorem 2, and Theorem 3
n−Ω(1) due to the union
hold, which happens with probability at least 1
bound. Recall from line 9 of Algorithm 1 that ¯Q(N1)=Q(N1)H(N1 )
. Moreover,
the columns of ¯Q(N1) are denoted by q(N1)=Q(N1)h(N1 )
and q(N1)=Q(N1)h(N1)
,
where h
eigenvalues d

, h(N1) are the eigenvectors of Q(N1)T AQ(N1)
(N1)
d(N1).

, d(N1), respectively, and d
By α > β > 0, (36), and (69), one can verify that N1 ≥

¯K. It then follows

associated with the

(N1)

(N1)

≥

−

1

satisfying

from Theorem 2 and (69) that there exist θ1, θ2 ∈ {±
8α
β

4c1
(α + β)√log n

θ1u1k2 ≤

q(N1)

+ 10

−

n

k

}
N1

(cid:18)

(cid:19) (cid:18)

c3
√log n

,

(71)

(cid:19)

≤

N1

q(N1)

k

θ2u2k2 ≤

n

−

16α
β

+ 18

4c1
β)√log n

(α

c3
√log n

≤

.

(72)

(cid:18)

(cid:19) (cid:18)
After obtaining ¯Q(N1), Algorithm 1 generates an initial point x(0) for the
projected power iterations by setting x(0) = √ny(0)/
k2, where y(0) =
y(0)
k
q(N1)
q(N1)
k2 = 1, we see that
y(0)

k2 = √n and

(1T q(N1)/n)1. Since

1 and

−

(cid:19)

1

k

k

−
k2 ≤

k

y(0)T

q(N1) = 1

(1T q(N1))2
n

−

0.

≥

Non-Convex Exact Community Recovery

25

These, together with

x(0)

k

k2 = √n, yield

x(0)

k

−

√nq(N1)

2
2 = 2n

k

= 2n

−

−

which implies that

2n

(y(0))T q(N1)
y(0)

k

≤

k2
(1T q(N1))2
n

2n

1

−

2n

2n(y(0))T q(N1)

−

= 2(1T q(N1))2,

!

x(0)

√nq(N1)

k2 ≤

1T q(N1)

√2
|

.

|

−

k
Besides, by (21), there exist ˜θ1, ˜θ2 ∈ {±
1
√n

c3
√log n

,

2 ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)
This, together with (71), (73), (q(N1))T q(N1) = 0, and

2 ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜θ1u1 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

such that

1

}

x∗
√n

˜θ2u2 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(73)

(74)

c3
√log n

.

x(0)

k

−

√nq(N1)

√2n

k2 ≤

˜θ1u1 + ˜θ1u1 −

q(N1)

k

k2 = 1, yields

˜θ1
θ1

T

q(N1)

q(N1)

!

1
√n

+

θ1u1 −
(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

q(N1)

(cid:19)

2
(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(75)

1
√n −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜θ1u1 −
(cid:12)
(cid:18)(cid:13)
(cid:13)
n
(cid:13)
(cid:13)
log n
r

.

√2n

2√2c3

≤

≤

Then, by (72), (74), (75), and θ2, ˜θ2 ∈ {±

1

}

, we obtain

min
θ∈{±1} k

x(0)

−

θx∗

k2 ≤

x(0)

≤ k

x(0)

=

k

−

−

√nq(N1)

√nq(N1)

(2 + 2√2)c3

≤

−

x(0)
(cid:13)
(cid:13)
(cid:13)
k2 + √n
(cid:13)
k

x∗

θ2
˜θ2

q(N1)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
−

θ2u2k2 + √n

q(N1)

θ2u2k2 + √n

−

k2 + √n
k
n

5c3

log n ≤

r

r

n
log n

.

θ2u2 −
(cid:13)
(cid:13)
(cid:13)
˜θ2u2 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x∗
√n

θ2
˜θ2
x∗
√n

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

x(0)

This, together with
if N2 = ˜K, where ˜K is deﬁned in (67), then Algorithm 1 can ﬁnd x∗ or
with probability at least 1

k2 = √n, 1T x(0) = 0, and Theorem 3, implies that
x∗

⊔⊓
Armed with the results in Theorem 4, we can now provide a proof of

n−Ω(1).

−

−

k

Corollary 1.

 
 
26

P. Wang, Z. Zhou, A. M.-C. So

Proof (of Corollary 1) From the discussion following Theorem 1, it remains
to bound the number of non-zero entries in A. Towards that end, let us ﬁrst
Rn of
estimate the number of non-zero entries in an arbitrary column a
A, which we denote by
k0. According to the binary symmetric SBM in
Deﬁnition 1, we have

∈

a

k

n/2

n/2

d=

Wi +

Zi,

a

k0

k

i=1
X

where
{
independent of

Wi}

i=1
X
n/2
i=1 are i.i.d. Bern(p),
Zi}
n/2
i=1. It follows that

{

Wi}
{
E[
a

k

k0] =

n
2

(p + q),

Var(
k

a

k0)

≤

n
2

(p + q).

n/2
i=1 are i.i.d. Bern(q), and

n/2
i=1 are

Zi}

{

Upon applying Bernstein’s inequality for bounded distributions (see, e.g., [36,
Theorem 2.8.4]), we get

P

a

k0 −

k
(cid:18)(cid:12)
(cid:12)
(cid:12)
= 2 exp

(p + q)

≥

(cid:12)
(cid:12)
(cid:12)
n(p + q)

n
2

9
8

−

(cid:18)

3
2

n(p + q)

(cid:19)

2 exp

≤

−

(cid:18)

= 2 exp

−

(cid:18)

(cid:19)

9
8

9

8 n2(p + q)2

1

2 n(p + q) + 1
= 2n− 9

2 n(p + q)

(cid:19)

8 (α+β).

(α + β) log n

(cid:19)

This gives

P (
k

a

k0 < 2n(p + q))

1

2n− 9

8 (α+β)

1

2n− 9
4 ,

≥

≥

−

√β > √2.

−
where the second inequality is due to α + β > 2, which follows from α > β > 0
and √α

−
Now, upon applying the union bound, we conclude that with probability at
2n−5/4, the number of non-zero entries in A is less than 2n2(p + q) =
least 1
(n log n).
2(α + β)n log n. Thus, the per-iteration cost of Algorithm 1 is
Together with Theorem 1, the desired bound on the total computational cost
of Algorithm 1 follows.

O

−

⊔⊓

3.4 Discussion

Before we leave this section, let us discuss two further consequences of our
technical development.

3.4.1 Binary Symmetric SBM without Self-Loops

Consider a variant of the binary symmetric SBM in which no self-loop is
allowed in the graph. Speciﬁcally, the entries of the adjacency matrix A are
still generated independently according to (1), except that aii = 0 for all
i
[n]. Our results are still valid under this setting. Indeed, it is easy to verify
∈
that

E[A] =

11T +

p + q
2

p

q

−
2

x∗x∗T

pI.

−

Non-Convex Exact Community Recovery

27

In particular, the eigenvalues of E[A] change by p =
(log n/n), which is
dominated by other quantities that appear Lemma 2 and Corollary 2. Thus,
the results in Section 3.1 still hold. Moreover, observe that

O

n/2−1

n/2

x∗
i (Ax∗)i =

aij x∗

i x∗
j

d=

Xj6=i

for every i
[n], where, as before,
n/2
i=1 are i.i.d. Bern(β log n/n), and
Zi}
{
Since

∈

Wi}
{
Zi}
{

Wi −

Zi

i=1
X

i=1
X
n/2−1
i=1
n/2
i=1 are independent of

are i.i.d. Bern(α log n/n),
n/2−1
.
i=1

Wi}

{

n/2−1

n/2

n/2−1

n/2−1

P



Wi −

Zi ≤

γ log n



≤



P

Wi −

Zi ≤

γ log n + 1



i=1
X

i=1
i=1
X
X

R, we can apply Lemma 4 and obtain the results in Section 3.2.
for any γ
Hence, we can conclude that Theorem 4 holds. Lastly, the number of non-
zero entries in A can only decrease if there is no self-loop, which implies that
Corollary 1 also holds.

i=1
X







∈

3.4.2 Implementation of the Vanilla Spectral Method and Its Complexity
Analysis

Recently, Abbe et al. [3] have shown that the vanilla spectral method, which
is presented in Algorithm 2, can also achieve exact recovery down to the
information-theoretic limit in the binary symmetric SBM. A popular and prac-
tically eﬃcient way of implementing this method is to employ orthogonal it-
erations to ﬁnd the eigenvector u2 of the adjacency matrix A. However, the
results in [3] do not establish the number of orthogonal iterations needed to
obtain a suﬃciently accurate approximation of u2 that can provably recover
the communities in the graph.

Algorithm 2: Vanilla Spectral Method

Input: adjacency matrix A
Output: label vector ˆx

1 compute u2, the eigenvector of A associated with the second largest eigenvalue of

A

2 output ˆx = sgn(u2)

By combining the results in [3] with those in Sections 3.1 and 3.2, we now
(log n/ log log n)
show that if the vanilla spectral method is implemented as
orthogonal iterations followed by a single projection onto
, then it can exactly
recover the underlying communities with high probability. In particular, such
(n log2 n/ log log n) time, which, to the best of
an implementation runs in
our knowledge, is currently the best complexity bound for the vanilla spectral

O

O

F

28

P. Wang, Z. Zhou, A. M.-C. So

method in the context of exact community recovery in the binary symmetric
SBM. To begin, we recall [3, Corollary 3.1], which states that with probability
1

(n−3), one has

− O

θ

Ax∗
ν2√n

C
√n log log n

,

(76)

min
θ∈{±1}

u2 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞ ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)

where C > 0 is a constant whose value depends only on α and β, and ν2 is
the second-largest eigenvalue of E[A] (see (25)). By Theorem 2, after

N =



3 log n + 2 log log log n + 2 log

16α+18β
Cβ

log log n + 2 log

(cid:16)

α−β
4c1

¯K

(cid:17)



≥

orthogonal iterations, where ¯K is given in (36), we obtain a vector q(N ) satis-
fying

(cid:16)

(cid:17)









min
θ∈{±1} k
From (76) and (77), we deduce the existence of ˜θ

θu2k2 ≤

−

q(N )

C
√n log log n

∈ {±

1

}

such that

.

(77)

q(N )
(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜θ

Ax∗
ν2√n

−

2C
√n log log n

.

∞ ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Now, by (58), it holds with probability at least 1

n−c5 that

−

It follows that

(Ax∗)i
ν2√n

min
i∈[n]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2γ
√n(α

−

.

β)

x∗ = sgn(x∗) = sgn(Ax∗) = sgn

Ax∗
ν2√n

(cid:18)

= ˜θ

·

(cid:19)

sgn(q(N )),

(78)

(79)

where the second equality is due to (58) and the fourth equality uses (78)
and (79). The above implies that q(N ) has n/2 positive entries and n/2 negative
entries. Thus, by Proposition 1, we have

as desired.

(q(N ))

x∗

P

∈ {±

}

4 Numerical Results

In this section, we report the recovery performance and numerical eﬃciency of
our proposed two-stage method (which we denote by PPM in this section for
ease of reference) for community recovery on both synthetic and real data sets.
We also compare our approach with four existing approaches, which are the
SDP-based approach in [6], the manifold optimization (MFO)-based approach
in [8], the spectral clustering (SC) approach in [3], and the two-stage approach
based on the generalized power method (GPM) in [37]. In the implementation,
we use alternating direction method of multipliers (ADMM) to solve the SDP
as suggested in [6], manifold gradient descent (MGD) method to solve the

Non-Convex Exact Community Recovery

29

MFO, and the MATLAB function eigs for computing the eigenvector that is
needed in the SC approach. Our codes are implemented in MATLAB R2020a
and can be downloaded at https://github.com/peng8wang/MP-Exact-Recovery-in-SBM.
All the experiments are conducted on a PC with 16GB memory and Intel(R)
Core(TM) i5-8600 3.10GHz CPU.

4.1 Phase Transition and Computation Time

We ﬁrst examine the phase transition property and runtime of the aforemen-
tioned methods for recovering communities in graphs that are randomly gener-
ated according to the binary symmetric SBM, both with and without self-loops
(see Deﬁnition 1 and Section 3.4.1). We choose n = 300 in the experiments
and let the parameters α and β in (2) vary from 0 to 30 and 0 to 10 with
increments of 0.5 and 0.4, respectively. For every pair of α and β, we generate
40 instances and calculate, for all the methods, the ratio of exact recovery.
The simulation results are presented in Figure 1, Figure 2, and Table 1. It
can be seen that all methods exhibit a phase transition phenomenon and the
recovery performance of PPM is slightly better than the other three methods.
Moreover, Figures 1(a) and 2(a) suggest that PPM can achieve the optimal
recovery threshold on graphs with and without self-loops, respectively. This
supports the results in Theorem 1 and Section 3.4.1. In Table 1, we record the
total CPU time consumed by each approach for completing the phase transi-
tion experiment. It can be observed from the table that PPM is comparable to
GPM, slightly better than SC, and substantially faster than SDP and MGD.

Table 1 Phase transition: Total CPU time (in seconds) of the diﬀerent approaches.

Methods
Self-loops
No self-loop

PPM GPM SDP MGD SC
104
8195
103
8356

922
932

14
14

18
19

4.2 Convergence Performance

Next, we study the convergence performance of PPM and MGD, both of which
have similar per-iteration cost, and report the number of iterations needed to
exactly identify the two communities in graphs generated according to the
binary symmetric SBM. We do not report the performance of GPM, SDP, and
SC, as GPM and PPM have very similar performance, SDP cannot be solved to
high accuracy using ADMM, and SC can be directly solved using the MATLAB
function eigs. We conduct 3 sets of numerical tests each on graphs with and
without self-loops, which correspond to β
. In each set, we generate
5 graphs of dimension n = 2000, which correspond to α = (√β + √2)2 + i
. Such a setting ensures that the information-theoretic
for i

1, 2, 3, 4, 5

4, 8, 16

∈ {

}

∈ {

}

30

30

25

20

15

10

5

0

P. Wang, Z. Zhou, A. M.-C. So

PPM

GPM

1

0.8

0.6

0.4

0.2

0

30

25

20

15

10

5

0

SDP

1

0.8

0.6

0.4

0.2

0

30

25

20

15

10

5

0

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8

10

0

2

4

6

8

10

0

2

4

6

8

10

(a) PPM

(b) GPM

(c) SDP

MGD

30

25

20

15

10

5

0

SC

1

0.8

0.6

0.4

0.2

0

30

25

20

15

10

5

0

0

2

4

6

8

10

0

2

4

6

8

10

(d) MGD

(e) SC

1

0.8

0.6

0.4

0.2

0

Fig. 1 Phase transition in graphs generated by the SBM with self-loops: The x-axis is β,
which ranges from 0 to 10 with an increment of 2; the y-axis is α, which ranges from 0 to 30
with an increment of 5. Darker pixels represent lower empirical probability of success. The
red curve is the information-theoretic threshold √α

√β = √2.

−

GPM

PPM

30

25

20

15

10

5

0

1

0.8

0.6

0.4

0.2

0

30

25

20

15

10

5

0

1

0.8

0.6

0.4

0.2

0

30

25

20

15

10

5

0

SDP

0

2

4

6

8

10

0

2

4

6

8

10

0

2

4

6

8

10

1

0.8

0.6

0.4

0.2

0

(a) PPM

(b) GPM

(c) SDP

MGD

30

25

20

15

10

5

0

SC

1

0.8

0.6

0.4

0.2

0

30

25

20

15

10

5

0

0

2

4

6

8

10

0

2

4

6

8

10

(d) MGD

(e) SC

1

0.8

0.6

0.4

0.2

0

Fig. 2 Phase transition in graphs generated by the SBM without self-loops: The x-axis is
β, which ranges from 0 to 10 with an increment of 2; the y-axis is α, which ranges from 0
to 30 with an increment of 5. Darker pixels represent lower empirical probability of success.
The red curve is the information-theoretic threshold √α

√β = √2.

−

Non-Convex Exact Community Recovery

31

xkxkT

threshold for exact recovery is met. Let xk and Qk denote k-th iterate of PPM
and MGD, respectively. In Figures 3 and 4, we plot the distances of the iterates
kF against the
to the ground truth
iteration number for PPM and MGD, respectively. It can be observed that
PPM exhibits a ﬁnite termination phenomenon and converges to the ground
truth much faster than MGD in graphs both with and without self-loops.
This also corroborates the iteration complexity established in Theorem 1 and
Section 3.4.1.

kF and

QkQkT

x∗x∗T

x∗x∗T

−

−

k

k

104

102

100

10-2

10-4

10-6

h
t
u
r
t

d
n
u
o
r
g

o
t

e
c
n
a
t
s
D

i

PPM on data 1
MGD on data 1
PPM on data 2
MGD on data 2
PPM on data 3
MGD on data 3
PPM on data 4
MGD on data 4
PPM on data 5
MGD on data 5

104

102

100

10-2

10-4

10-6

h
t
u
r
t

d
n
u
o
r
g

o
t

e
c
n
a
t
s
D

i

PPM on data 1
MGD on data 1
PPM on data 2
MGD on data 2
PPM on data 3
MGD on data 3
PPM on data 4
MGD on data 4
PPM on data 5
MGD on data 5

10-8

0

10

20

30
Iterations
(a) β = 4

40

50

60

10-8

0

10

40

50

20

30
Iterations
(b) β = 8

104

102

100

10-2

10-4

10-6

t

h
u
r
t

d
n
u
o
r
g
o

t

e
c
n
a

t
s
D

i

PPM on data 1
MGD on data 1
PPM on data 2
MGD on data 2
PPM on data 3
MGD on data 3
PPM on data 4
MGD on data 4
PPM on data 5
MGD on data 5

10-8

0

10

20

50

60

40

30
Iterations
(c) β = 16

Fig. 3 Convergence performance on graphs generated by the SBM with self-loops: The
x-axis is number of iterations, the y-axis is distance to ground truth, which is given by
xkxkT
kF for MGD. Here, xk and Qk are the
k
k-th iterates generated by the PPM and the MGD, respectively.

kF for PPM and

QkQkT
k

x∗x∗T

x∗x∗T

−

−

4.3 Computational Eﬃciency

In this sub-section, we compare the computational eﬃciency of our proposed
method with GPM, MGD, SDP, and SC on both synthetic and real data sets.
For the synthetic data sets, we ﬁx β = 16, α = (√β + √2)2 + 1, and gener-
ate three graphs of dimension n = 2000, 10000, and 20000, respectively. For
the real ones, we use the data sets polbooks and polblogs downloaded from

 
 
 
 
 
 
 
 
 
32

P. Wang, Z. Zhou, A. M.-C. So

104

102

100

10-2

10-4

10-6

h
t
u
r
t

d
n
u
o
r
g

o
t

e
c
n
a
t
s
D

i

PPM on data 1
MGD on data 1
PPM on data 2
MGD on data 2
PPM on data 3
MGD on data 3
PPM on data 4
MGD on data 4
PPM on data 5
MGD on data 5

104

102

100

10-2

10-4

10-6

h
t
u
r
t

d
n
u
o
r
g

o
t

e
c
n
a
t
s
D

i

PPM on data 1
MGD on data 1
PPM on data 2
MGD on data 2
PPM on data 3
MGD on data 3
PPM on data 4
MGD on data 4
PPM on data 5
MGD on data 5

10-8

0

5

10

15

20
25
Iterations
(a) β = 4

30

35

40

45

10-8

0

10

20

50

60

70

30
40
Iterations
(b) β = 8

104

102

100

10-2

10-4

10-6

h
t
u
r
t

d
n
u
o
r
g

o
t

e
c
n
a
t
s
D

i

PPM on data 1
MGD on data 1
PPM on data 2
MGD on data 2
PPM on data 3
MGD on data 3
PPM on data 4
MGD on data 4
PPM on data 5
MGD on data 5

10-8

0

10

20

50

60

70

30

40
Iterations
(c) β = 16

Fig. 4 Convergence performance on graphs generated by the SBM without self-loops: The
x-axis is number of iterations, the y-axis is distance to ground truth, which is given by
xkxkT
kF for MGD. Here, xk and Qk are the
k
k-th iterates generated by the PPM and the MGD, respectively.

kF for PPM and

QkQkT
k

x∗x∗T

x∗x∗T

−

−

xk

sgn

∈ P
∈

xk + Axk

k
(xk+Axk); for GPM, we terminate it when
n A1n/n2
1T

UF Sparse Matrix Collection [14].3 Since these real-world networks have un-
balanced or multiple communities, we extract 2 balanced communities from
them. The sizes of each community extracted from polbooks and polblogs are
43 and 732, respectively. The stopping criteria for the tested algorithms are
k2 < 10−3 for some
yk
set as follows. For PPM, we terminate it when
yk
k2 < 10−3 for some
yk
yk
n xk1n
1T
; for MGD, we terminate it when
the norm of the manifold gradient is less then 10−3; for ADMM, we terminate
it when the norm of the diﬀerence of two consecutive iterates is less than 10−3.
No stopping criterion is needed for SC as it simply employs the MATLAB eigs
function with some post-processing. We run each algorithm 10 times from ran-
domly generated initial points and select the best solution (in terms of function
value) as its recovery solution. Moreover, we set the maximum iteration num-
ber as 2000 for every algorithm. To compare the computational eﬃciency of
the tested algorithms, we record their CPU time, averaged over 10 runs, and
present the results in Table 2. It can be observed that our proposed method is

−
xk

−

−

k

(cid:1)

(cid:0)

·

3 https://sparse.tamu.edu/

 
 
 
 
 
 
 
 
 
Non-Convex Exact Community Recovery

33

Ground truth

PPM

GPM

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

(a) Ground Truth

(b) PPM

(c) GPM

SDP

MGD

SC

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

80

(d) SDP

(e) MGD

(f) SC

Fig. 5 Recovery performance on the polbooks network: The x-axis and y-axis give the labels
of the vertices (from vertex 1 to vertex 86) in the network polbooks.

nearly as fast as GPM, slightly better than MGD and SC, and substantially
faster than SDP.

All the tested methods can achieve exact recovery on synthetic data sets.
Their recovery performance on the two real data sets polbooks and polblogs
are presented in Figures 5 and 6, respectively. According to the ground truth
of polbooks, the number of misclassiﬁed vertices by PPM, GPM, SDP, MGD,
and SC are 0, 4, 1, 4, and 3, respectively. As for polblogs, the number of
misclassiﬁed vertices by PPM, GPM, SDP, MGD, and SC are 64, 698, 289, 294,
and 194, respectively. These results demonstrate that our proposed method is
comparable to SDP, MGD, SC and is better than GPM in terms of recovery
performance on the two real data sets.

Table 2 CPU times (in seconds) of the algorithms on synthetic and real data sets.

PPM GPM SDP MGD
0.313
0.005
2.562
0.031
0.074
4.687
0.059
0.003
0.019
6.118

n = 2000
n = 10000
n = 20000
polbooks
polblogs

0.004
0.028
0.074
0.002
0.102
* “–” denotes out of memory.

79.47
–*
–
0.172
492.1

SC
0.014
0.113
0.268
0.098
0.021

34

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

Ground truth

PPM

GPM

P. Wang, Z. Zhou, A. M.-C. So

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

(a) Ground truth

(b) PPM

(c) GPM

SDP

MGD

SC

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

(d) SDP

(e) MGD

(f) SC

Fig. 6 Recovery performance on the polblogs network: The x-axis and y-axis give the labels
of the vertices (from vertex 1 to vertex 1464) in the network polblogs.

5 Conclusions

In this work, we proposed a two-stage iterative algorithm that provably achieves
exact recovery down to the information-theoretic limit in the binary symmet-
(n log2 n/ log log n). The complexity bound
ric SBM and has a runtime of
is among the best for algorithms that have the same recovery performance.
In the process of establishing our main results, we developed new analyses of
the orthogonal iterations and projected power iterations used in our algorithm,
which could be of independent interest. Our numerical results on synthetic and
real data sets demonstrate the strong recovery performance and high compu-
tational eﬃciency of the proposed algorithm. A natural future direction is to
extend the proposed approach to tackle recovery tasks in more general SBMs.

O

References

1. Abbe, E.: Community detection and stochastic block models. Foundations and Trends®

in Communications and Information Theory 14(1–2), 1–162 (2018)

2. Abbe, E., Bandeira, A.S., Hall, G.: Exact recovery in the stochastic block model. IEEE

Transactions on Information Theory 62(1), 471–487 (2016)

3. Abbe, E., Fan, J., Wang, K., Zhong, Y.: Entrywise eigenvector analysis of random
matrices with low expected rank. The Annals of Statistics 48(3), 1452–1474 (2020)
4. Abbe, E., Sandon, C.: Community detection in general stochastic block models: Fun-
damental limits and eﬃcient algorithms for recovery. In: Proceedings of the 2015 IEEE
56th Annual Symposium on Foundations of Computer Science (FOCS), pp. 670–688
(2015)

5. Absil, P.A., Edelman, A., Koev, P.: On the largest principal angle between random

subspaces. Linear Algebra and its Applications 414(1), 288–294 (2006)

6. Amini, A.A., Levina, E., et al.: On semideﬁnite relaxations for the block model. The

Annals of Statistics 46(1), 149–179 (2018)

Non-Convex Exact Community Recovery

35

7. Bandeira, A.S.: Random Laplacian matrices and convex relaxations. Foundations of

Computational Mathematics 18(2), 345–379 (2018)

8. Bandeira, A.S., Boumal, N., Voroninski, V.: On the low-rank approach for semideﬁnite
programs arising in synchronization and community detection. In: Proceedings of the
29th Annual Conference on Learning Theory (COLT 2016), pp. 361–382 (2016)

9. Blum, M., Floyd, R.W., Pratt, V., Rivest, R.L., Tarjan, R.E.: Time bounds for selection.

Journal of Computer and System Sciences 7(4), 448–461 (1973)

10. Boumal, N., Absil, P.A., Cartis, C.: Global rates of convergence for nonconvex opti-

mization on manifolds. IMA Journal of Numerical Analysis 39(1), 1–33 (2018)

11. Burer, S., Monteiro, R.D.: A nonlinear programming algorithm for solving semideﬁnite
programs via low-rank factorization. Mathematical Programming 95(2), 329–357 (2003)
12. Chi, Y., Lu, Y.M., Chen, Y.: Nonconvex optimization meets low-rank matrix factoriza-
tion: An overview. IEEE Transactions on Signal Processing 67(20), 5239–5269 (2019)
13. Cline, M.S., Smoot, M., Cerami, E., Kuchinsky, A., Landys, N., Workman, C., Christ-
mas, R., Avila-Campilo, I., Creech, M., Gross, B., et al.: Integration of biological net-
works and gene expression data using cytoscape. Nature Protocols 2(10), 2366 (2007)
14. Davis, T.A., Hu, Y.: The University of Florida sparse matrix collection. ACM Trans-

actions on Mathematical Software 38(1) (2011)

15. Decelle, A., Krzakala, F., Moore, C., Zdeborov´a, L.: Asymptotic analysis of the stochas-
tic block model for modular networks and its algorithmic applications. Physical Review
E 84(6), 066106 (2011)

16. Fortunato, S.: Community detection in graphs. Physics Reports 486(3-5), 75–174 (2010)
17. Gao, C., Ma, Z., Zhang, A.Y., Zhou, H.H.: Achieving optimal misclassiﬁcation pro-
portion in stochastic block models. The Journal of Machine Learning Research 18(1),
1980–2024 (2017)

18. Garey, M.R., Johnson, D.S., Stockmeyer, L.: Some simpliﬁed NP-complete problems.
In: Proceedings of the 6th Annual ACM Symposium on Theory of Computing, pp. 47–63
(1974)

19. Girvan, M., Newman, M.E.: Community structure in social and biological networks.

Proceedings of the National Academy of Sciences 99(12), 7821–7826 (2002)

20. Golub, G.H., Van Loan, C.F.: Matrix Computations, 4 edn. The Johns Hopkins Uni-

versity Press (2013)

21. Hajek, B., Wu, Y., Xu, J.: Achieving exact cluster recovery threshold via semideﬁnite
programming. IEEE Transactions on Information Theory 62(5), 2788–2797 (2016)
22. Lei, J., Rinaldo, A.: Consistency of spectral clustering in stochastic block models. The

Annals of Statistics 43(1), 215–237 (2015)

23. Li, X., Zhu, Z., So, A.M.C., Vidal, R.: Nonconvex robust low-rank matrix recovery.

SIAM Journal on Optimization 30(1), 660–686 (2020)

24. Liu, H., Pun, Y.M., So, A.M.C.: Local strong convexity of maximum–likelihood tdoa–
based source localization and its algorithmic implications. In: Proceedings of the IEEE
7th International Workshop on Computational Advances in Multi–Sensor Adaptive Pro-
cessing (CAMSAP 2017), pp. 1–5 (2017)

25. Liu, H., Yue, M.C., So, A.M.C.: On the estimation performance and convergence rate of
the generalized power method for phase synchronization. SIAM Journal on Optimization
27(4), 2426–2446 (2017)

26. Liu, H., Yue, M.C., So, A.M.C., Ma, W.K.: A discrete ﬁrst-order method for large-
scale MIMO detection with provable guarantees.
In: Proceedings of the 18th IEEE
International Workshop on Signal Processing Advances in Wireless Communications
(SPAWC 2017), pp. 669–673 (2017)

27. Ma, C., Wang, K., Chi, Y., Chen, Y.: Implicit regularization in nonconvex statistical es-
timation: Gradient descent converges linearly for phase retrieval, matrix completion, and
blind deconvolution. Foundations of Computational Mathematics 20, 451–632 (2020)

28. Massouli´e, L.: Community detection thresholds and the weak Ramanujan property. In:
Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pp. 694–703
(2014)

29. Mossel, E., Neeman, J., Sly, A.: Consistency thresholds for the planted bisection model.

Electronic Journal of Probability 21(21), 1–24 (2016)

30. Newman, M.E., Girvan, M.: Finding and evaluating community structure in networks.

Physical Review E 69(2), 026113 (2004)

36

P. Wang, Z. Zhou, A. M.-C. So

31. Pun, Y.M., So, A.M.C.: Dynamic regret bound for moving target tracking based on
online time-of-arrival measurements. In: Proceedings of the 59th IEEE Conference on
Decision and Control (CDC 2020), p. to appear (2020)

32. Sun, R., Li, D., Liang, S., Ding, T., Srikant, R.: The global landscape of neural networks:
An overview (2020). Accepted for publication in IEEE Signal Processing Magazine
33. Sun, R.Y.: Optimization for deep learning: An overview. Journal of the Operations

Research Society of China 8(2), 249–294 (2020)

34. Trefethen, L.N., Bau, III, D.: Numerical Linear Algebra. SIAM (1997)
35. Vaswani, N.: Non-convex structured phase retrieval (2020). Accepted for publication in

IEEE Signal Processing Magazine

36. Vershynin, R.: High-Dimensional Probability: An Introduction with Applications in
Data Science, Cambridge Series in Statistical and Probabilistic Mathematics, vol. 47.
Cambridge University Press (2018)

37. Wang, P., Zhou, Z., So, A.M.C.: A nearly-linear time algorithm for exact community
recovery in stochastic block model. In: Proceedings of the 37th International Conference
on Machine Learning (ICML 2020), p. to appear (2020)

38. Yun, S.Y., Proutiere, A.: Optimal cluster recovery in the labeled stochastic block model.
In: D.D. Lee, M. Sugiyama, U.V. Luxburg, I. Guyon, R. Garnett (eds.) Advances in
Neural Information Processing Systems 29: Proceedings of the 2016 Conference, pp.
965–973 (2016)

39. Zhong, Y., Boumal, N.: Near-optimal bounds for phase synchronization. SIAM Journal

on Optimization 28(2), 989–1016 (2018)

