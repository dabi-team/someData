Forgetting to Learn Logic Programs

Andrew Cropper
University of Oxford
andrew.cropper@cs.ox.ac.uk

9
1
0
2

v
o
N
5
1

]

G
L
.
s
c
[

1
v
3
4
6
6
0
.
1
1
9
1
:
v
i
X
r
a

Abstract

Most program induction approaches require predeﬁned, of-
ten hand-engineered, background knowledge (BK). To over-
come this limitation, we explore methods to automatically
acquire BK through multi-task learning. In this approach, a
learner adds learned programs to its BK so that they can be
reused to help learn other programs. To improve learning per-
formance, we explore the idea of forgetting, where a learner
can additionally remove programs from its BK. We consider
forgetting in an inductive logic programming (ILP) setting.
We show that forgetting can signiﬁcantly reduce both the
size of the hypothesis space and the sample complexity of an
ILP learner. We introduce Forgetgol, a multi-task ILP learner
which supports forgetting. We experimentally compare For-
getgol against approaches that either remember or forget ev-
erything. Our experimental results show that Forgetgol out-
performs the alternative approaches when learning from over
10,000 tasks.

1 Introduction

A major challenge in machine learning is inductive bias:
how to choose a learner’s hypothesis space so that it is
large enough to contain the target hypothesis, yet small
enough to be searched efﬁciently (Baxter 2000). A major
challenge in program induction is how to chose a learner’s
background knowledge (BK), which in turn deﬁnes the
hypothesis space. Most approaches need predeﬁned, often
(Law, Russo, and Broda 2014;
hand-engineered,
Evans and Grefenstette 2018;
Balog et al. 2017;
Ellis et al. 2018b). By contrast, we want a learner
to
automatically learn BK.

BK

Several

approaches

(Dechter et al. 2013;
Ellis et al. 2018a; Cropper 2019)
learn BK through
meta-learning. The idea is to use knowledge gained from
solving one problem to help solve a different problem.
For instance, Lin et al. (2014) use this approach on 17
string transformation problems. They automatically identify
easier problems, learn programs for them, and then reuse
the learned programs to help learn programs for more
difﬁcult problems. The authors experimentally show that
their multi-task approach performs substantially better
than a single-task approach because learned programs are

frequently reused. They also show that this approach leads
to a hierarchical library of reusable programs.

to

in AI

develop

(>10,000)

tasks. Moreover, most

If we want
support
that

concepts.
systems
challenge

think the key limitation with

However, no approach has been shown to work
ap-
(Quinlan 1990;

on many
struggle even on few tasks
proaches
Ferri, Hern´andez-Orallo, and Ram´ırez-Quintana 2001).
By contrast, humans easily learn thousands of di-
verse
human-
lifelong learning – a
like AI
grand
(Silver, Yang, and Li 2013;
Lake et al. 2017) – then we need to overcome this lim-
itation.
We
existing ap-
learned programs
save
proaches
to the BK, which is a problem because too much
irrelevant BK is
perfor-
detrimental
(Srinivasan, Muggleton, and King 1995;
mance
bound
Srinivasan, King, and Bain 2003). The Blumer
(Blumer et al. 1987)1 helps explain this problem. This
bound states that given two hypothesis spaces, searching
the smaller space will result in fewer errors compared to the
larger space, assuming that the target hypothesis is in both
spaces. Here lies the problem: how to choose a learner’s
hypothesis space so that
is large enough to contain
the target hypothesis yet small enough to be efﬁciently
searched.

learning

they

that

all

to

is

it

To address this problem, we explore the idea of forgetting.
In this approach, a learner continually grows and shrinks its
hypothesis space by adding and removing programs to and
from its BK. We claim that forgetting can improve learning
performance, especially when learning from many tasks. To
support this claim, we make the following contributions:

• We deﬁne forgetting in an inductive logic programming
(ILP) setting (Section 3). We show that forgetting can re-
duce both the size of the hypothesis space (Theorem 2)
and the sample complexity of an ILP learner (Theorem 3).
We show that optimal forgetting is NP-Hard (Theorem 1).

• We introduce Forgetgol, a multi-task ILP learner that con-
tinually learns, saves, and forgets programs (Section 4).

1The bound is a reformulation of Lemma 2.1.

 
 
 
 
 
 
We introduce two forgetting methods: syntactical and sta-
tistical forgetting, the latter based on Theorem 2.

• We experimentally show on two datasets (robots and
Lego) that forgetting can substantially improve learning
performance when learning from over 10,000 tasks (Sec-
tion 5). This experiment is the ﬁrst to consider the perfor-
mance of a learner on many tasks.

2 Related Work

Several
approaches
(Dumancic and Blockeel 2017;
learn BK though
Dumancic et al. 2019; Cropper 2019)
unsupervised learning. These approaches do not require
user-supplied tasks as input. By contrast, we learn BK
through supervised learning and need a corpus of tasks as
input. Our forgetting idea is, however, equally applicable to
unsupervised approaches.

or

approaches

perform either multi-task
Supervised
lifelong (Silver, Yang, and Li 2013)
(Caruana 1997)
learning. A multi-task learner is given a set of tasks and
can solve them in any order. A lifelong learner is given
a sequence of tasks and can use knowledge gained from
the past n-1 tasks to help solve the nth task. The idea
behind both approaches is to reuse knowledge gained from
solving one problem to help to solve a different problem,
i.e. perform transfer learning (Torrey and Shavlik 2010).
Although we focus on multi-task learning, our forgetting
idea is suitable for both approaches.

Lin et al. (2014) state that an important problem with
multi-task learning is how to handle the complexity of grow-
ing BK. To address this problem, we propose forgetting,
where a learner continually revises its BK by adding and
removing programs, i.e. performs automatic bias-revision
(Dietterich et al. 2008). Forgetting has long been recog-
nised as an important feature in AI (Lin and Reiter 1994).
Most forgetting work focuses on eliminating logical redun-
dancy (Plotkin 1971), e.g. to improve efﬁciency in SAT
(Heule et al. 2015). Our approach is slightly unusual be-
cause we are willing to forget logically irredundant knowl-
edge.

Our forgetting idea comes from the observation that
existing approaches remember everything (Quinlan 1990;
Ferri, Hern´andez-Orallo, and Ram´ırez-Quintana 2001;
Lin et al. 2014;
is
which
a
evant
BK is
mance
Srinivasan, King, and Bain 2003).

Cropper 2019),
irrel-
perfor-
(Srinivasan, Muggleton, and King 1995;

problem because
to
detrimental

too much
learning

Dechter et al. 2013;

A notable case of forgetting in machine learning is
catastrophic forgetting (McCloskey and Cohen 1989), the
tendency of neural networks to forget previously learned
knowledge upon learning new knowledge. In multi-task
program induction we have the inverse problem: catas-
trophic remembering, the inability for a learner to for-
get knowledge. We therefore need intentional forgetting
(Beierle and Timm 2019).

Forgetting

in
forgetting

on
Widmer and Kubat 1996; Maloof 1997). By

examples

focuses
(Sablon and Raedt 1995;
contrast

program induction mostly

we forget BK. Mart´ınez-Plumed et al. (2015) explore ways
to compress BK without losing knowledge. Our work differs
in many ways, mainly because we work in a multi-task
setting with many tasks. Forgetting is essentially identifying
irrelevant BK. Deepcoder (Balog et al. 2017) and Dream-
coder (Ellis et al. 2018a) both train neural networks to score
how relevant programs are in the BK. Whereas Deepcoder’s
BK is ﬁxed and predetermined by the user, we grow and
revise our BK through multi-task learning. Dreamcoder
also revises its BK through multi-task learning. Our work
differs from Dreamcoder because we (1) formally show that
forgetting can improve learning performance, (2) describe
forgetting methods that do not require neural networks, and
(3) learn from 20,000 tasks, whereas Dreamcoder considers
at most 236 tasks.

3 Problem Setting
We now deﬁne the forgetting problem, show that optimal
forgetting is NP-hard, and that forgetting can reduce both
the size of the hypothesis space and the sample complexity
of a learner.

3.1 ILP Problem
We base our problem on the ILP learning from entailment
setting (Raedt 2008). We assume standard logic program-
ming deﬁnitions throughout (Lloyd 2012). We deﬁne the in-
put:
Deﬁnition 1 (ILP input). An ILP input
is a tuple
(B, E+, E−) where B is logic program background knowl-
edge, and E+ and E− are sets of ground atoms which repre-
sent positive and negative examples respectively.
By logic program we mean a set of deﬁnite clauses, and
thus any subsequent reference to a clause refers to a deﬁ-
nite clause. We assume that B contains the necessary lan-
guage bias, such as mode declarations (Muggleton 1995) or
metarules (Cropper and Tourret 2019), to deﬁne the hypoth-
esis space:
Deﬁnition 2 (Hypothesis space). The hypothesis space HB
is the set of all hypotheses (logic programs) deﬁned by back-
ground knowledge B.
The version space contains all complete (covers all the pos-
itive examples) and consistent (covers none of the negative
examples) hypotheses:
Deﬁnition 3 (Version space). Let (B, E+, E−) be an ILP
input. Then the version space is VB,E+,E− = {H ∈
HB|(H ∪ B |= E+) ∧ (∀e− ∈ E−, H ∪ B 6|= e−)}.
We deﬁne an optimal hypothesis:
Deﬁnition 4 (Optimal hypothesis). Let I = (B, E+, E−)
be an ILP input and cost : H → R be an arbitrary cost func-
tion. Then H ∈ VB,E+,E− is an optimal hypothesis for I if
and only if cost(H) ≤ cost(H ′) for all H ′ ∈ VB,E+,E−.
The cost of a hypothesis could be many things, such as the
number of literals in it (Law, Russo, and Broda 2014) or its
computational complexity (Cropper and Muggleton 2019).
In this paper, the cost of a hypothesis is the number of
clauses in it.

The ILP problem is to ﬁnd a complete and consistent hy-

pothesis:

Deﬁnition 5 (ILP problem). Given an ILP input
(B, E+, E−), the ILP problem is to return a hypothesis
H ∈ VB,E+,E−.
A learner should ideally return the optimal hypothesis, but it
is not an absolute requirement, and it is common to sacriﬁce
optimality for efﬁciency.

3.2 Forgetting
The forgetting problem is to ﬁnd a subset of the given BK
from which one can still learn the optimal hypothesis:
Deﬁnition 6 (Forgetting problem). Let I = (B, E+, E−)
be an ILP input and H be an optimal hypothesis for I.
Then the forgetting problem is to return B′ ⊂ B such that
H ∈ VB′,E+,E−. We say that B′ is reduced background
knowledge for E+ and E−.
We would ideally perform optimal forgetting:
Deﬁnition 7 (Optimal forgetting). Let I = (B, E+, E−)
be an ILP input and H be an optimal hypothesis for I. Then
the optimal forgetting problem is to return B′ ⊂ B such
that H ∈ VB′,E+,E− and there is no B′′ ⊂ B′ such that
H ∈ VB′′,E+,E−.
Even if we assume that the forgetting problem is decidable,
optimal forgetting is NP-hard:

Name Metarule
ident
P (A, B) ← Q(A, B)
precon
P (A, B) ← Q(A), R(A, B)
postcon P (A, B) ← Q(A, B), R(B)
chain

P (A, B) ← Q(A, C), R(C, B)

Table 1: Example metarules. The letters P , Q, and R denote
second-order variables. The letters A, B, and C denote ﬁrst-
order variables.

in B, each metarule in B has at most j body literals, and n
is a target hypothesis size. Cropper et al. (2019) show that
the size of the MIL hypothesis space is:

Proposition 1 (Hypothesis space). The size of the MIL hy-
pothesis space is at most (mpj+1)n.

The authors use this result with the Blumer bound to show
the MIL sample complexity in a PAC learning setting
(Valiant 1984):

Proposition 2 (Sample complexity). With error ǫ and con-
ﬁdence δ MIL has sample complexity s ≥ 1
ǫ (n ln(m) + (j +
1)n ln(p) + ln 1

δ ).

We use these results in the next section to show that forget-
ting can reduce sample complexity in MIL and in Section
4.3 to deﬁne a statistical forgetting method.

Theorem 1 (Complexity). Optimal forgetting is NP-Hard.

3.4 Forgetting Sample Complexity

Proof. We can reduce the knapsack problem, which is NP-
Hard, to optimal forgetting.

3.3 Meta-Interpretive Learning
We now show the beneﬁts of forgetting in meta-interpretive
(Muggleton, Lin, and Tamaddoni-Nezhad 2015;
(MIL)
Cropper, Morel, and Muggleton 2019), a powerful
form
of ILP that supports predicate invention (Stahl 1995) and
learning recursive programs. For brevity, we omit a formal
description of MIL and instead provide a brief overview.
A MIL learner is given as input two sets of ground atoms
that represent positive and negative examples of a target
concept, BK described as a logic program, and a set of
higher-order Horn clauses called metarules (Table 1). A
MIL learner uses the metarules to construct a proof of
the positive examples and none of the negative examples,
and forms a hypothesis using the substitutions for the
variables in the metarules. For instance, given positive and
negative examples of the grandparent relation, background
knowledge with the parent relation, and the metarules in
Table 1, a MIL learner could use the chain metarule with the
substitutions {P/grandparent, Q/parent, R/parent} to induce
the theory: grandparent(A,B) ← parent(A,C), parent(C,B).
The size of the MIL hypothesis space is a function of the
metarules, the number of predicate symbols in the BK, and
a target hypothesis size. We deﬁne sig(P ) to be the set of
predicate symbols in the logic program P . In the next two
propositions, assume an ILP input with background knowl-
edge B, where p = |sig(B)|, m is the number of metarules

The purpose of forgetting is to reduce the size of the hy-
pothesis space without excluding the target hypothesis. In
MIL we want to reduce the number of predicate symbols
without excluding the target hypothesis from the hypothesis
space. We now show the potential beneﬁts of doing so. In
the next two theorems, assume an ILP input (B, E+, E−)
and let B′ ⊂ B be reduced background knowledge for E+
and E−, where B and B′ both contain m metarules with at
most j body literals, p = |sig(B)|, p′ = |sig(B′)|, n be a
target hypothesis size, and r = p′/p, i.e. r is the forgetting
reduction ratio.

Theorem 2 (Hypothesis space reduction). Forgetting re-
duces the size of the MIL hypothesis space by a factor of
r(j+1)n.

Proof. Replacing p with rp in Proposition 1 and rearranging
terms leads to r(j+1)n(mp(j+1))n.

Theorem 3 (Sample complexity reduction). Forgetting re-
duces sample complexity in MIL by (j + 1)n ln(r).

Proof. Replacing p with rp in Proposition 2 and rearrang-
ing terms leads to s ≥ 1
ǫ (n ln(m) + (j + 1)n ln(r) + (j +
1)n ln(p) + ln 1

δ ).

These results show that forgetting can substantially reduce
the size of the hypothesis space and sample complexity of a
learner.

4 Forgetgol

Algorithm 1 Forgetgol

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

We now introduce Forgetgol, a multi-task ILP system
which supports forgetting. Forgetgol relies on Metagol
(Cropper and Muggleton 2016), a MIL system. We ﬁrst
brieﬂy describe Metagol.

4.1 Metagol

Metagol is based on a Prolog meta-interpreter. Metagol takes
as input (1) a logic program which represents BK, (2) a set
of predicate symbols S which denotes symbols that may
appear in a hypothesis, similar to determinations in Aleph
(Srinivasan 2001), (3) two sets of ground atoms which rep-
resent positive and negative examples, and (4) a maximum
program size. The set S is necessary because the BK may
contain relations which should not appear in a hypothesis.
For instance, the BK could contain a deﬁnition for quicksort
which uses partition and append but we may want Metagol
to only use quicksort in a hypothesis. The set S is usually
deﬁned as part of the BK but we separate it for clarity.

Given these inputs, Metagol tries learn a logic program by
proving each atom in the set of positive examples. Metagol
ﬁrst tries to prove an atom using the BK by delegating the
proof to Prolog. If this step fails, Metagol tries to unify the
atom with the head of a metarule and tries to bind the vari-
ables in a metarule to symbols in S. Metagol saves the sub-
stitutions and tries to prove the body of the metarule recur-
sively through meta-interpretation. After proving all atoms,
Metagol induces a logic program by projecting the substitu-
tions onto the corresponding metarules. Metagol checks the
consistency of the induced program with the negative exam-
ples. If the program is inconsistent, Metagol backtracks to
consider alternative programs. Metagol uses iterative deep-
ening to ensure that the learned program has the minimal
number of clauses. Metagol supports predicate invention by
adding d − 1 new predicate symbols to S at each depth d.

4.2 Forgetgol

Forgetgol (Algorithm 1) takes as input (1) a logic program
which represents BK, (2) a set of predicate symbols S which
denotes those that may appear in a hypothesis, (3) a set of
tasks T (a set of pairs of sets of positive and negative ex-
amples), (4) a maximum program size, and (5) a forgetting
function. Forgetgol uses dependent learning (Lin et al. 2014)
to learn programs and to expand B and S. Starting at depth
d = 1, Forgetgol tries to learn a program for each task us-
ing at most d clauses. To learn a program, Forgetgol calls
Metagol (line 17). Once Forgetgol has tried to learn a pro-
gram for each task at depth d, it updates B (line 6) with
the learned programs, increases the depth, and tries to learn
programs for the remaining tasks (line 8). Forgetgol also
updates S with learned predicate symbols (line 7), includ-
ing invented symbols. Forgetgol repeats this process until
it reaches a maximum depth, at which point it returns the
learned programs. In contrast to the approach used by Lin et
al, Forgetgol can also remove (forget) elements from S (line
4).

func forgetgol(B,S,T,maxd,forget):

P = {}
for depth=1 to maxd:
S′ = forget (S,B)
Sd, Td, Pd, = learn(B,S′,T,depth)
B = B ∪ Pd
S = S ∪ Sd
T = T \ Td
P = P ∪ Pd

return P

func learn (B,S,T,depth ):

Sd = {}
Td = {}
Pd = {}
for (E+,E−) in T:

prog = metagol(B,S,E+,E−,depth)
if prog != null :

Sd = Sd ∪ {p | p is the head symbol of a clause in prog}
Td = Td ∪ {(E+,E−)}
Pd = Pd ∪ prog

return Sd, Td, Pd

4.3 Forgetting

The purpose of forgetting is to reduce the size of the hypoth-
esis space without excluding the target hypothesis. Forget-
gol reduces the size of the hypothesis space by reducing the
number of predicate symbols allowed in a hypothesis (the set
S in Algorithm 1). To be clear: Forgetgol does not remove
clauses from the BK. Forgetgol removes predicate symbols
from S because, as stated, the number of predicate symbols
determines the size of the hypothesis space (Proposition 1).
We consider two forgetting methods: syntactical and statisti-
cal.

Syntactical Forgetting Algorithm 2 shows our syntacti-
cal forgetting method, which removes syntactically dupli-
cate programs. We use the Tamaki and Sato unfold operation
(Tamaki and Sato 1984) on each clause in the BK to replace
invented predicates with their deﬁnitions (but not when the
predicate refers to the head of the clause, i.e. when it is used
recursively). For each unfolded clause, we check whether
(1) the head symbol of the clause is in S (i.e. is allowed in
a hypothesis), and (2) we have already seen a clause with
the same head arguments and the same body (line 8). If so,
we forget the head predicate symbol; otherwise, we keep the
symbol and add the unfolded clause to the seen set.

Statistical Forgetting Statistical forgetting is based on the
hypothesis space results in Section 3. Deciding whether to
keep or forget a predicate symbol (an element of S) in MIL
depends on (1) the cost of relearning it, and (2) how likely
it is to be reused. As Proposition 1 states, given p predicate
symbols and m metarules with at most j body literals, the
number of programs expressible with n clauses is at most
(mpj+1)n. Table 2 uses this result to show the costs of sav-
ing or forgetting a predicate symbol when it is relevant or

Algorithm 2 Syntactical forgetting

Algorithm 3 Statistical forgetting

1
2
3
4
5
6
7
8
9
10
11

func forget (S,B):

S′ = {}
B′ = {}
for clause in B:

s = head sym(clause)
if s in S:

clause ′ = unfold ( clause ,B)
if clause ′ is new with respect

S′ = S′
B′ = B′

∪ {s}
∪ {clause′

}

return S′

1
2
3
4
5
6
7

to B′:

func forget (S,B):

S′ = {}
for clause in B:

s = head sym(clause)
if s in S and cost forget (s ,B) > cost keep (s ,B):

S′ = S′
return S′

∪ {s}

irrelevant. However, we do not know beforehand whether a
predicate symbol is relevant to future tasks. We can, how-
ever, estimate relevancy in our multi-task setting using the
relative reuse of a symbol, similar to Dechter et al. (2013).
Speciﬁcally, we deﬁne the probability P r(s, B) that a pred-
icate symbol s is relevant given background knowledge B
as:

P r(s, B) =

|{clause ∈ B| s in body of clause }| + 1
|B| + 1

Keep
Forget

Relevant
(m(p + 1)j+1)n−k
(mpj+1)n

Irrelevant
(m(p + 1)j+1)n
(mpj+1)n

Table 2: The costs of keeping or forgetting a predicate sym-
bol that is the head symbol of a deﬁnition composed of k
clauses. These costs are based on Proposition 1, where p
is the number of predicate symbols, m is the number of
metarules with at most j body literals, and n is the number
of clauses in the target program. The n − k exponent when
keeping a relevant symbol is because reusing a learned sym-
bol reduces the size of the target program by k clauses.

The +1 values denote additive smoothing. With this proba-
bility, we deﬁne the expected cost cost keep(s,B) of keeping
a symbol s that is the head of k clauses when searching for
a program with n clauses:

cost keep(s,B) = (pr(s, B)(m(p + 1)j+1)n−k)

+ ((1 − pr(s, B))(m(p + 1)j+1)n)

We can likewise deﬁne the expected cost cost forget(s,B) =
(mpj+1)n of forgetting s. These costs allow us to deﬁne the
forget function shown in Algorithm 3.

5 Experiments
To test our claim that forgetting can improve learning perfor-
mance, our experiments aim to answer the question:
Q1 Can forgetting improve learning performance?
To answer Q1 we compare three variations of Forgetgol:

• Forgetgolsyn: Forgetgol with syntactical forgetting

• Forgetgolstat: Forgetgol with statistical forgetting
• Metabias: Forgetgol set to remember everything (we call
Algorithm 1 with a forget function that returns all the BK),
which is equivalent to approach used in (Lin et al. 2014)

We introduced forgetting because we claim that learners
which remember everything will become overwhelmed by
too much BK when learning from many tasks. To test this
claim, our experiments aim to answer the question:

Q2 Do remember everything learners become overwhelmed

by too much BK when learning from many tasks?

To answer Q2, we compare the performance of the learners
on progressively more tasks. We expect that Metabias will
improve given more tasks but will eventually become over-
whelmed by too much BK, at which point its performance
will start to degrade. By contrast, we expect that Forget-
gol will improve given more tasks and should outperform
Metabias when given many tasks because it can forget BK.

We also compare Forgetgol and Metabias against Metagol.
However, this comparison cannot help us answer questions
Q1 and Q2, which is also why we do not compare Forget-
gol against other program induction systems. The purpose of
this comparison is to add more experimental evidence to the
results of Lin et al. (2014), where they compare multi-task
learning with single-task learning. We expect that Metagol
will not improve given more tasks because it cannot reuse
learned programs.

All

the

experimental

data

are

available

at

https://github.com/andrewcropper/aaai20-forgetgol.

5.1 Experiment 1 - Robot Planning
Our ﬁrst experiment is on learning robot plans.

Materials A robot and a ball are in a 6 × 6 space. A state
describes the position of the robot, the ball, and whether
the robot is holding the ball. A training example is an atom
f (s1, s2), where f is the target predicate and s1 and s2 are
initial and ﬁnal states respectively. The task is to learn a pro-
gram to move from the initial to the ﬁnal state. We gener-
ate training examples by generating random states, with the
constraint that the robot can only hold the ball if it is in the
same position as the ball. The robot can perform the actions
up, down, right, left, grab, and drop. The learners use the
metarules in Table 1.

Method Our dataset contains n tasks for each n in
{2000, 4000, . . . , 20000}. Each task is a one-shot learning
task and is given a unique predicate symbol. We enforce a

timeout of 60 seconds per task per search depth. We set the
maximum program size to 6 clauses. We measure the per-
centage of tasks solved (tasks where the learner learned a
program) and learning times. We plot the standard error of
the means over 5 repetitions.

Results Figure 1 shows the results on the small dataset. As
expected, Metagol’s performance does not vary when given
more tasks. By contrast, Forgetgol and Metabias continue
to improve given more tasks, both in terms of percentage
of tasks solved and learning time. Figure 2 shows the results
on the big dataset. We did not run Metagol on the big dataset
because the results on the small dataset are sufﬁciently con-
clusive. On the big dataset, the performances of Metabias
and Forgetgolstat start to degrade when given more than 8000
tasks. However, this performance decrease is tiny (<1%). By
contrast, Forgetgolsyn always solves 100% of the tasks and
by 20,000 tasks learns programs twice as quick as Metabias
and Forgetgolstat.

These results are somewhat unexpected. We thought that
Metabias would eventually become overwhelmed by too
much BK, at which point its performance would start to
degrade. Metabias does not strongly exhibit this behaviour
(the performance decrease is tiny) and performs well de-
spite large amounts of BK. After analysing the experimental
results, Metabias rarely has to learn a program with more
than two clauses because it could almost always reuse a
learned program. The performance of Forgetgolstat is simi-
lar to Metabias because Forgetgolstat rarely forgets anything,
which suggests limitations with the proposed statistical for-
getting method.

Overall, these results suggest that the answer to Q1 is yes

and the answer to Q2 is no.

100

d
e
v
l
o
s

s
k
s
a
t

%

80

60

40

20

Forgetgolsyn
Forgetgolstat
Metabias
Metagol

10

)
s
d
n
o
c
e
s
(

e
m

i
t
g
n
i
n
r
a
e
l
n
a
e
m

8

6

4

0
200

400

600

# tasks

800

1,000

2
200

Forgetgolsyn
Forgetgolstat
Metabias
Metagol

400

600

# tasks

800

1,000

(a) Percentage of tasks solved

(b) Mean learning time

Figure 1: Robot experiment small dataset results.

5.2 Experiment 2 - Lego
Our second experiment is on building Lego structures.

Materials We consider a Lego world with dimensionality
6×1. For simplicity we only consider 1×1 blocks of a single
colour. A training example is an atom f (s1, s2), where f is
the target predicate and s1 and s2 are initial and ﬁnal states
respectively. A state describes the Lego board as a list of in-
tegers. The value k at index i denotes that there are k blocks
stacked at position i. The goal is to learn a program to build
the Lego structure from a blank Lego board (a list of zeros).
We generate training examples by generating random ﬁnal

d
e
v
l
o
s

s
k
s
a
t

%

100

99.5

99

98.5

98

Forgetgolsyn
Forgetgolstat
Metabias

)
s
d
n
o
c
e
s
(

e
m

i
t
g
n
i
n
r
a
e
l
n
a
e
m

0.4

0.3

0.2

Forgetgolsyn
Forgetgolstat
Metabias

5,000

10,000

15,000

20,000

5,000

10,000

15,000

20,000

# tasks

# tasks

(a) Percentage of tasks solved

(b) Mean learning time

Figure 2: Robot experiment big dataset results.

states. The learner can move along the board using the ac-
tions left and right; can place a Lego block using the action
place block; and can use the ﬂuents at left and at right and
their negations to determine whether it is at the leftmost or
rightmost board position. The learners use the metarules in
Table 1.

Method The method is the same as in experiment 1, ex-
cept we only use a big dataset.

Results The results (Figure 3) show that the performance
of Metabias substantially worsens after 12,000 tasks. By
20,000 tasks Metabias can only solve around 80% of the
tasks. By contrast, Forgetgolsyn always solves 100% of the
tasks and by 20,000 tasks can learn programs three times
quicker than Metabias. The performance of Forgetgolstat is
again similar to Metabias. These results strongly suggest that
the answers to Q1 and Q2 are both yes.

100

95

90

85

d
e
v
l
o
s

s
k
s
a
t

%

Forgetgolsyn
Forgetgolstat
Metabias

Forgetgolsyn
Forgetgolstat
Metabias

)
s
d
n
o
c
e
s
(

e
m

i
t
g
n
i
n
r
a
e
l
n
a
e
m

6

4

2

5,000

10,000

15,000

20,000

5,000

10,000

15,000

20,000

# tasks

# tasks

(a) Percentage of tasks solved

(b) Mean learning time

Figure 3: Lego experiment results.

6 Conclusions and Limitations
We have explored the idea of forgetting. In this approach,
a program induction system learns programs over time and
is able to revise its BK (and thus its hypothesis space) by
adding and removing (forgetting) learned programs. Our the-
oretical results show that forgetting can substantially reduce
the size of the hypothesis space (Theorem 2) and the sam-
ple complexity (Theorem 3) of a learner . We implemented
our idea in Forgetgol, a new ILP learner. We described two
forgetting techniques: syntactical and statistical. Our experi-
mental results on two domains show that (1) forgetting can
substantially improve learning performance (answers yes to
Q1), and (2) remember everything approaches become over-
whelmed by too much BK (answers yes to Q2).

6.1 Limitations and Future Work

There are many limitations to this work, including (1) only
considering two domains, (2) only using Forgetgol with
Metagol, and (3) not evaluating the impact of forgetting
on predictive accuracy. However, future work can easily ad-
dress these minor limitations.

The main limitation of this work, and thus the main
topic for future research, is our forgetting methods. Syntac-
tical forgetting achieves the best performance, but if every
learned program is syntactically unique, then this method
will forget nothing. Future work can address this limita-
tion by exploring semantic forgetting methods, such as
those based on subsumption (Plotkin 1971) or derivability
(Cropper and Tourret 2019). Statistical forgetting, based on
expected search cost, does not perform better than Metabias
(which remembers everything). Statistical forgetting rarely
forgets programs because the expected search cost is domi-
nated by the target program size. Future work could improve
this method by adding a frugal dampening factor to force
Forgetgol to forget more. We suspect that another limitation
with our forgetting methods is that they will be sensitive to
concept drift (Widmer and Kubat 1996). For instance, if we
ﬁrst trained Forgetgol on the robot domain and then on the
Lego domain, we would want it to revise its BK accordingly.
Future work could address this limitation by adding an expo-
nential decay factor to forget programs that have not recently
been used.

To conclude, we think that forgetting is an important con-
tribution to lifelong machine learning and that our work will
stimulate future research into better forgetting methods.

References

[Balog et al. 2017] Balog, M.; Gaunt, A. L.; Brockschmidt, M.;
Nowozin, S.; and Tarlow, D. 2017. Deepcoder: Learning to write
programs. In ICLR. OpenReview.net.

[Baxter 2000] Baxter, J. 2000. A model of inductive bias learning.

J. Artif. Intell. Res. 12:149–198.

[Beierle and Timm 2019] Beierle, C., and Timm, I. J. 2019. Inten-
tional forgetting: An emerging ﬁeld in AI and beyond. KI 33(1):5–
8.

[Blumer et al. 1987] Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and
Inf. Process. Lett.
1987. Occam’s razor.

Warmuth, M. K.
24(6):377–380.

[Caruana 1997] Caruana, R. 1997. Multitask learning. Machine

Learning 28(1):41–75.

[Cropper and Muggleton 2016] Cropper, A., and Muggleton, S. H.

2016. Metagol system. https://github.com/metagol/metagol.

[Cropper and Muggleton 2019] Cropper, A., and Muggleton, S. H.
2019. Learning efﬁcient logic programs. Machine Learning
108(7):1063–1083.

[Cropper and Tourret 2019] Cropper, A., and Tourret, S. 2019. Log-

ical reduction of metarules. Machine Learning. To appear.

[Cropper, Morel, and Muggleton 2019] Cropper, A.; Morel, R.; and
Muggleton, S. H. 2019. Learning higher-order logic programs.
Machine Learning. To appear.

[Cropper 2019] Cropper, A. 2019. Playgol: Learning programs
through play. In Kraus, S., ed., IJCAI 2019, 6074–6080. ijcai.org.

[Dechter et al. 2013] Dechter, E.; Malmaud, J.; Adams, R. P.; and
Tenenbaum, J. B. 2013. Bootstrap learning via modular concept
discovery. In IJCAI 2013, 1302–1309. IJCAI/AAAI.

[Dietterich et al. 2008] Dietterich, T. G.; Domingos, P. M.; Getoor,
L.; Muggleton, S.; and Tadepalli, P. 2008. Structured machine
learning: the next ten years. Machine Learning 73(1):3–23.

[Dumancic and Blockeel 2017] Dumancic, S., and Blockeel, H.
2017. Clustering-based relational unsupervised representation
learning with an explicit distributed representation. In Sierra, C.,
ed., IJCAI 2017, 1631–1637. ijcai.org.

[Dumancic et al. 2019] Dumancic, S.; Guns, T.; Meert, W.; and
Blockeel, H. 2019. Learning relational representations with auto-
In Kraus, S., ed., IJCAI 2019, 6081–
encoding logic programs.
6087. ijcai.org.

[Ellis et al. 2018a] Ellis, K.; Morales, L.; Sabl´e-Meyer, M.; Solar-
Lezama, A.; and Tenenbaum, J. 2018a. Learning libraries of
In
subroutines for neurally-guided bayesian program induction.
NeurIPS 2018, 7816–7826.

[Ellis et al. 2018b] Ellis, K.; Ritchie, D.; Solar-Lezama, A.; and
Tenenbaum, J. 2018b. Learning to infer graphics programs from
hand-drawn images. In NeurIPS 2018, 6062–6071.

[Evans and Grefenstette 2018] Evans, R., and Grefenstette, E. 2018.
Learning explanatory rules from noisy data. J. Artif. Intell. Res.
61:1–64.

[Ferri, Hern´andez-Orallo, and Ram´ırez-Quintana 2001] Ferri, C.;
Hern´andez-Orallo, J.; and Ram´ırez-Quintana, M. J. 2001. Incre-
mental learning of functional logic programs. In Kuchen, H., and
Ueda, K., eds., FLOPS 2001, volume 2024 of Lecture Notes in
Computer Science, 233–247. Springer.

[Heule et al. 2015] Heule, M.; J¨arvisalo, M.; Lonsing, F.; Seidl, M.;
and Biere, A. 2015. Clause elimination for SAT and QSAT. J. Artif.
Intell. Res. 53:127–168.

[Lake et al. 2017] Lake, B. M.; Ullman, T. D.; Tenenbaum, J. B.;
and Gershman, S. J. 2017. Building machines that learn and think
like people. Behavioral and brain sciences 40.

[Law, Russo, and Broda 2014] Law, M.; Russo, A.; and Broda, K.
2014. Inductive learning of answer set programs. In JELIA 2014,
311–325.

[Lin and Reiter 1994] Lin, F., and Reiter, R. 1994. Forget it.
Working Notes of AAAI Fall Symposium on Relevance, 154–159.
[Lin et al. 2014] Lin, D.; Dechter, E.; Ellis, K.; Tenenbaum, J. B.;
and Muggleton, S. 2014. Bias reformulation for one-shot function
induction. In ECAI 2014, 525–530.

In

[Lloyd 2012] Lloyd, J. W. 2012. Foundations of logic programming.

Springer Science & Business Media.

[Maloof 1997] Maloof, M. A. 1997. Progressive Partial Memory
Learning. Ph.D. Dissertation, Fairfax, VA, USA. UMI Order No.
GAX97-10122.

[Mart´ınez-Plumed et al. 2015] Mart´ınez-Plumed, F.; Ferri, C.;
Hern´andez-Orallo, J.; and Ram´ırez, M. J.
2015. Knowledge
acquisition with forgetting: an incremental and developmental
setting. Adaptive Behaviour 23(5):283–299.

[McCloskey and Cohen 1989] McCloskey, M., and Cohen, N. J.
1989. Catastrophic interference in connectionist networks: The se-
quential learning problem. In Psychology of learning and motiva-
tion, volume 24. Elsevier. 109–165.

[Muggleton, Lin, and Tamaddoni-Nezhad 2015] Muggleton, S. H.;
Lin, D.; and Tamaddoni-Nezhad, A. 2015. Meta-interpretive learn-
ing of higher-order dyadic datalog: predicate invention revisited.
Machine Learning 100(1):49–73.

[Muggleton 1995] Muggleton, S. 1995. Inverse entailment and pro-

gol. New Generation Comput. 13(3&4):245–286.

[Plotkin 1971] Plotkin, G. 1971. Automatic Methods of Inductive

Inference. Ph.D. Dissertation, Edinburgh University.

[Quinlan 1990] Quinlan, J. R. 1990. Learning logical deﬁnitions

from relations. Machine Learning 5:239–266.

[Raedt 2008] Raedt, L. D. 2008. Logical and relational learning.

Cognitive Technologies. Springer.

[Sablon and Raedt 1995] Sablon, G., and Raedt, L. D. 1995. For-
getting and compacting data in concept learning. In Proceedings
of the Fourteenth International Joint Conference on Artiﬁcial Intel-
ligence, IJCAI 95, Montr´eal Qu´ebec, Canada, August 20-25 1995,
2 Volumes, 432–438. Morgan Kaufmann.

[Silver, Yang, and Li 2013] Silver, D. L.; Yang, Q.; and Li, L. 2013.
Lifelong machine learning systems: Beyond learning algorithms.
In Lifelong Machine Learning, Papers from the 2013 AAAI Spring
Symposium, Palo Alto, California, USA, March 25-27, 2013, vol-
ume SS-13-05 of AAAI Technical Report. AAAI.

[Srinivasan, King, and Bain 2003] Srinivasan, A.; King, R. D.; and
Bain, M. 2003. An empirical study of the use of relevance in-
formation in inductive logic programming. J. Mach. Learn. Res.
4:369–383.

[Srinivasan, Muggleton, and King 1995] Srinivasan, A.; Muggle-
ton, S. H.; and King, R. D. 1995. Comparing the use of background
knowledge by inductive logic programming systems. In Proceed-
ings of the 5th International Workshop on Inductive Logic Program-
ming, 199–230. Department of Computer Science, Katholieke Uni-
versiteit Leuven.

[Srinivasan 2001] Srinivasan, A. 2001. The ALEPH manual. Ma-
chine Learning at the Computing Laboratory, Oxford University.
[Stahl 1995] Stahl, I. 1995. The appropriateness of predicate inven-
tion as bias shift operation in ILP. Machine Learning 20(1-2):95–
117.
[Tamaki and Sato 1984] Tamaki, H., and Sato, T. 1984. Unfold/fold
transformation of logic programs. In T¨arnlund, S., ed., Proceedings
of the Second International Logic Programming Conference, Upp-
sala University, Uppsala, Sweden, July 2-6, 1984, 127–138. Upp-
sala University.

[Torrey and Shavlik 2010] Torrey, L., and Shavlik, J. 2010. Trans-
fer learning. In Handbook of research on machine learning applica-
tions and trends: algorithms, methods, and techniques. IGI Global.
242–264.

[Valiant 1984] Valiant, L. G. 1984. A theory of the learnable. Com-

mun. ACM 27(11):1134–1142.

[Widmer and Kubat 1996] Widmer, G., and Kubat, M. 1996. Learn-
ing in the presence of concept drift and hidden contexts. Machine
Learning 23(1):69–101.

