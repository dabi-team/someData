2
2
0
2

g
u
A
5

]

C
O
.
h
t
a
m

[

2
v
7
8
6
3
1
.
5
0
2
2
:
v
i
X
r
a

Asymptotic Convergence Rate and Statistical Inference for
Stochastic Sequential Quadratic Programming

Sen Na, Michael W. Mahoney

International Computer Science Institute
Department of Statistics, University of California, Berkeley

Abstract

We apply a stochastic sequential quadratic programming (StoSQP) algorithm to solve constrained
nonlinear optimization problems, where the objective is stochastic and the constraints are in equality
and deterministic. We study a fully stochastic setup, where only a single sample is available in
each iteration for estimating the gradient and Hessian of the objective. We allow StoSQP to select
a random stepsize ¯αt adaptively, such that βt ≤ ¯αt ≤ βt + χt, where βt, χt = o(βt) are prespeciﬁed
deterministic sequences. We also allow StoSQP to solve Newton system inexactly via randomized
iterative solvers, e.g., with the sketch-and-project method; and we do not require the approximation
error of inexact Newton direction to vanish (thus, the per-iteration computational cost does not blow
up). For this general StoSQP framework, we establish the asymptotic convergence rate for its last
iterate, with the worst-case iteration complexity as a byproduct; and we perform statistical inference.
In particular, under mild assumptions and with proper decaying sequences βt, χt, we show that: (i)
the StoSQP scheme can take at most O(1/(cid:15)4) iterations to achieve (cid:15)-stationarity; (ii) asymptotically
and almost surely, (cid:107)(xt −x(cid:63), λt −λ(cid:63))(cid:107) = O((cid:112)βt log(1/βt))+O(χt/βt), where (xt, λt) is the primal-
βt · (xt − x(cid:63), λt − λ(cid:63)) converges to a mean zero Gaussian
dual StoSQP iterate; (iii) the sequence 1/
distribution with a nontrivial covariance matrix. Furthermore, we establish the Berry-Esseen bound
for (xt, λt) to measure quantitatively the convergence of its distribution function. We also provide
a practical estimator for the covariance matrix, from which the conﬁdence intervals (or regions) of
(x(cid:63), λ(cid:63)) can be constructed using the iterates {(xt, λt)}t. All our theorems are validated using
nonlinear problems in CUTEst test set.

√

1

Introduction

We consider solving constrained stochastic nonlinear optimization problems of the form:

f (x) = E[f (x; ξ)],

min
x∈Rd

s.t. c(x) = 0,

(1)

where f : Rd → R is a stochastic objective function that involves a random variable ξ ∼ P, following
the distribution P, and c : Rd → Rm provides deterministic equality constraints. Problems of this
form appear widely in a variety of applications, including optimal control (Birge, 1997), multi-
stage optimization (Pﬂug and Pichler, 2014), PDE-constrained optimization (Rees et al., 2010),
constrained maximum likelihood estimation (Onuk et al., 2015), and constrained deep neural networks

1

 
 
 
 
 
 
(Chen et al., 2018). In practice, the variable ξ corresponds to a data point; f (x; ξ) is the loss at
data point ξ when using parameter x to ﬁt the model; and f (x) is the expected loss. Deterministic
constraints are also common in many real examples. They can characterize the physics of systems,
encode prior model knowledge, or reduce searching complexity.

Numerous methods have been proposed for solving constrained optimization problems (Bertsekas,
1982; Nocedal and Wright, 2006). Compared to most of the existing literature on solving constrained
optimization by either penalty methods, augmented Lagrangian methods, or sequential quadratic
programming (SQP), we consider here a stochastic objective in Problem (1), whose exact function
evaluation, gradient, and Hessian are inaccessible due to the expensive calculation of the expectation.
However, their stochastic estimates are accessible by drawing samples from P. Recently, several
algorithms built on stochastic SQP (StoSQP) have been proposed for solving (1). We point the reader
to Na et al. (2021a,b); Berahas et al. (2021b,a, 2022); Curtis et al. (2021b), and will review these
related literature in Section 1.3. Although these works all showed global convergence for a variety of
StoSQP schemes (with or without line search, with or without inequality constraints etc.), a “ﬁner”
understanding of StoSQP is missing.

By “ﬁner” understanding of StoSQP, we mean the (local) convergence rate, the (worst-case) iter-
ation or sample complexity, and the stationary distribution of the iteration sequence. These aspects
are important since, compared to global convergence, they characterize the behavior of the iterates
more precisely and demonstrate the eﬃciency of the algorithm. Furthermore, the primal-dual
solution (x(cid:63), λ(cid:63)), especially the primal solution x(cid:63), is the optimal model parameter that minimizes
the expected loss. Performing statistical inference on the model parameter, such as constructing
a conﬁdence region for x(cid:63) or conducting a hypothesis testing H0 : wT x(cid:63) = 0 v.s. H1 : wT x(cid:63) (cid:54)= 0
for a direction w ∈ Rd, is necessary for drawing any statistically signiﬁcant conclusions. Such
a statistical inference task can be performed once we understand the stationary distribution of
stochastic iterates.

In this paper, we make progress towards understanding the aforementioned aspects for StoSQP
schemes. We complement the existing literature by establishing the asymptotic convergence rate as
well as the asymptotic normality for the last iterate of an Adaptive Inexact StoSQP framework,
shortened as AI-StoSQP. As a result, we can perform statistical inference on (x(cid:63), λ(cid:63)) based on the
iterates {(xt, λt)}t generated by AI-StoSQP. As a byproduct, we also show the worst-case iteration
complexity. By the nature of the algorithm, the iteration complexity of AI-StoSQP is consistent
with the sample complexity. In the remaining of this section, we ﬁrst brieﬂy introduce AI-StoSQP
and connect it with the existing schemes in Section 1.1. Then, we summarize main results and
contributions in Section 1.2, followed by reviewing related literature on both constrained optimization
and unconstrained optimization in Section 1.3.

1.1 Algorithm sketch

This paper studies a StoSQP framework called AI-StoSQP. As suggested by the name, we allow the
scheme to adaptively select random stepsize ¯αt, although we require ¯αt to be controlled by βt ≤ ¯αt ≤
βt + χt =: ηt with βt, χt being prespeciﬁed deterministic sequences. We also allow the scheme to solve
Newton systems inexactly via randomized iterative solvers, e.g., with the sketch-and-project method.
In particular, given the current primal-dual iterate (xt, λt), AI-StoSQP performs the following

three steps (detailed descriptions are presented in Section 2):
• Step 1: We generate a single sample ξt ∼ P to estimate the gradient ¯∇xLt and the Hessian ¯∇2
xLt
(with respect to x) of the Lagrangian L(x, λ) = f (x) + λT c(x). Then, we construct a modiﬁed

2

(cid:80)t−1
i=0

¯∇2

xLi, and use Bt to form the full Hessian Kt of

Hessian Bt based on the averaged Hessian 1
t
the Lagrangian. See (2) and (4) for the expressions of Bt and Kt.
• Step 2: We solve Newton system Kt (cid:101)zt = − ¯∇Lt inexactly, by performing a ﬁxed, say τ , number of
iterations of the sketch-and-project method. Then, we obtain the inexact Newton direction zt,τ =
( ¯∆xt, ¯∆λt). See (6) for the updating rule of the sketch-and-project method.
• Step 3: We adaptively select a random stepsize ¯αt within the interval [βt, ηt], where βt, ηt = βt + χt
are deterministic prespeciﬁed sequences. Finally, we update the iterate by (xt+1, λt+1) = (xt, λt) +
¯αt( ¯∆xt, ¯∆λt) and repeat from Step 1.
Connections to the existing schemes. The above StoSQP framework is related to the existing
schemes (Na et al., 2021a,b; Berahas et al., 2021b,a, 2022; Curtis et al., 2021b,a), but it has several
important enhancements.

First, diﬀerent from Na et al. (2021a,b) but following from Berahas et al. (2021b,a); Curtis et al.
(2021b,a), we study a fully stochastic setup where only a single sample is generated in each iteration
for estimating the objective gradient. However, those fully stochastic schemes did not estimate the
objective Hessian, and simply let the modiﬁed Hessian Bt be identity matrix in the experiments.
Diﬀerently, we estimate the objective Hessian (with the same sample). This extra computation
is necessary for local analysis even for deterministic problems (Nocedal and Wright, 2006). We
emphasize that our modiﬁed Hessian Bt is constructed based on the averaged Hessian 1
xLi
t
excluding the t-th iteration, i.e., not based on a single noisy Hessian estimate. This choice of Hessian
is more eﬀective in practice (e.g., a recent study on stochastic Newton method has demonstrated
the eﬀectiveness of such Hessian averaging (Na et al., 2022)), and it is critical for our analysis. In
xL(cid:63) if (xt, λt) converges
particular, Bt is deterministic conditional on (xt, λt), and converges to ∇2
to (x(cid:63), λ(cid:63)).

(cid:80)t−1
i=0

¯∇2

Second, we solve Newton systems inexactly and randomly via the sketch-and-project method.
This method was initially proposed by Gower and Richt´arik (2015) for solving general linear systems.
We refer to that work for other equivalent interpretations of the method and for speciﬁc matrix-free
examples (e.g., randomized Kaczmarz method). Berahas et al. (2021b,a); Curtis et al. (2021a) solved
Newton systems exactly. Curtis et al. (2021b) solved Newton systems inexactly but deterministically
(via conjugate gradient (CG) and minimum residual (MINRES) methods). In addition, we perform
a ﬁxed number of iterations of the solver, so that the per-iteration computational cost does not
blow-up. In contrast, Curtis et al. (2021b) gradually vanished the approximation error. On the other
hand, we should mention that Curtis et al. (2021b) is more adaptive than AI-StoSQP; the residual of
their solver is controlled by either KKT residual or feasibility residual, both of which are computed
from the iterates (although converge to zero), while our iteration budget τ is given and ﬁxed.

Third, we allow for using any random stepsize ¯αt, as long as it ensures to lie in the interval
[βt, ηt]. For example, the designed stepsize selection schemes in Berahas et al. (2021b,a); Curtis et al.
(2021b,a) all ﬁt in our framework.

In this paper, we call AI-StoSQP a framework for two reasons. First, AI-StoSQP is trimmed
from a complete StoSQP scheme. It preserves all the features that are essential for local asymptotic
analysis (i.e. t → ∞). However, for a complete StoSQP scheme, one may insert another step between
Step 2 and Step 3 to select suitable penalty parameter for a certain merit function. The selected
parameter may kick in βt, ηt as a multiplier. See Berahas et al. (2021b) for the usage of the (cid:96)1 merit
function, f (x) + µ(cid:107)c(x)(cid:107)1. The step of penalty parameter selection is important for global analysis,
while is negligible for local analysis. This is because the penalty parameter is both upper and lower

3

bounded by deterministic thresholds, and always stabilizes for large t (under suitable conditions).
See, for example, (Berahas et al., 2021b, Section 3.2.2) and (Na et al., 2021a, Lemma 4.4). Thus,
the local behavior of (xt, λt) is fully characterized by the direction ( ¯∆xt, ¯∆λt) and the controlled
sequences {βt, ηt}, which are preserved by AI-StoSQP. (Note that the deterministic SQP has the
same trim, where a unit stepsize is employed and the local (quadratic/superlinear) behavior is
established by purely investigating the nature of (modiﬁed) Newton system). Second, AI-StoSQP
does not suggest particular designs in Step 2 and Step 3, thus providing much ﬂexibility. One can
adopt diﬀerent sketching matrices in Step 2, driven by the structure of the objective; and more
importantly, can adopt diﬀerent procedures to select ¯αt in Step 3. For example, Berahas et al.
(2021b,a); Curtis et al. (2021b,a) utilized either (cid:96)1 or (cid:96)2 merit functions, and projected a random
quantity into the interval [βt, ηt] to obtain ¯αt. One can design a similar procedure for augmented
Lagrangian merit function, as used in Na et al. (2021a,b). However, such a design would also ﬁt in
the presented framework. Thus, our analysis on AI-StoSQP is broadly applicable.

1.2 Main results and contributions

We study AI-StoSQP with decaying βt and χt. We establish three main results informally summarized
as follows. The rigorous statements are stated in Sections 3 and 4.

(a) Iteration complexity: AI-StoSQP can take at most O((cid:15)−4) iterations to achieve (cid:15)-stationarity

for the expected KKT residual.

(b) Asymptotic convergence rate: we have (cid:107)(xt−x(cid:63), λt−λ(cid:63))(cid:107) = O((cid:112)βt log(1/βt))+O(χt/βt)
) and βt decays polynomially in t (as commonly employed

almost surely. Thus, if χt = O(β3/2
in practice), then the error of the last iterate vanishes sublinearly locally.

t

(c) Asymptotic normality: we have 1/

d−→ N (0, Ξ(cid:63)), where the covariance
Ξ(cid:63) depends on the sketching distribution employed in the iterative solver. Furthermore, we
establish the Berry-Esseen bound to measure quantitatively how quickly the distribution
function of (xt, λt) converges, and we provide a practical estimator for Ξ(cid:63).

βt·(xt−x(cid:63), λt−λ(cid:63))

√

Our results contribute to the literature on StoSQP mentioned in Section 1.1. Na et al. (2021a,b);
Berahas et al. (2021b,a, 2022); Curtis et al. (2021b) showed global convergence for various StoSQP
schemes; Curtis et al. (2021a) showed iteration complexity for an exact StoSQP with constant βt;
none of them provided a local view of StoSQP. To be speciﬁc, our main results (a)-(c) lead to the
following four-fold contributions.

(2) Our result (a) relies on a non-asymptotic convergence rate of 1
t

(1) We show that the KKT residual (cid:107)∇Lt(cid:107) converges to zero almost surely, which diﬀers from the
convergence in expectation showed in prior works. Thus, our local rate also holds almost surely.
E[(cid:107)∇Li(cid:107)] for decaying βt,
while Curtis et al. (2021a) showed a similar non-asymptotic result for constant βt. By non-
asymptotic we mean the result holds for any t ≥ 0, which distinguishes from the asymptotic
results in (b) and (c) that hold for suﬃciently large t (i.e., (b) and (c) are local results).

(cid:80)t−1
i=0

(Curtis et al. (2021a) also established O((cid:15)−4) iteration complexity, however, we would like
to mention that two works are not fully comparable. That work involves the step of penalty
parameter selection for the (cid:96)1 merit function, which complicates the analysis; requires extra
conditions; and is inessential for our analysis in (b) and (c). In this sense, (a) is only our
by-product result, and is not as complete as the one in Curtis et al. (2021a). However, Curtis

4

et al. (2021a) can only perform ﬁnite SQP iterations since βt in their scheme is set as the
reciprocal of the iteration length.)

(3) Our results (b) and (c) show the local behavior of the last StoSQP iterate, and we provide
a statistical view of the scheme. To our knowledge, such a statistical view is missing in all
existing literature on constrained optimization. However, it is important in real parameter
estimation problems. When we apply StoSQP for estimating the true model parameter x(cid:63),
a natural task is to infer x(cid:63) given stochastic iterates generated by StoSQP. The results (b)
and (c) precisely characterize the uncertainty of the scheme, which in each iteration includes
the randomness of sample, the randomness of stepsize, and the randomness of solver. Such
characterization enables the inference of x(cid:63) (and λ(cid:63)) based on StoSQP iterates, and is novel
to the literature.

(4) For unconstrained stochastic optimization (we review in Section 1.3), the asymptotic conver-
gence analysis involving a random stepsize is open even for ﬁrst-order methods, and involving
an inexact randomized Newton direction is open for second-order methods. This paper directly
achieves both for constrained stochastic optimization.

1.3 Literature review

This paper relates to prior work both on constrained optimization and on unconstrained optimization.

Constrained optimization. As mentioned, there is a growing body of literature on designing
various StoSQP schemes for solving Problem (1). Compared to penalty methods and augmented
Lagrangian methods, SQP methods preserve the problem structure, are more robust to initialization,
and do not suﬀer ill-conditioning issues. Berahas et al. (2021b) designed a very ﬁrst StoSQP scheme.
At each iteration, the scheme adaptively selects a penalty parameter of the (cid:96)1 merit function to
ensure the Newton direction generates a suﬃcient decrease on the merit function; and then selects a
random stepsize ¯αt based on sequences βt and χt = O(β2
t ), such that βt ≤ ¯αt ≤ ηt = βt + χt. An
alternative StoSQP scheme is designed in Na et al. (2021a), where the authors embedded stochastic
line search into StoSQP to get rid of the prespeciﬁed sequences βt, χt. That algorithm is more
adaptive than the algorithm of Berahas et al. (2021b), while requiring a more stringent setup for
line search—one has to generate batch samples with an increasing batch size in each iteration.
Subsequently, Berahas et al. (2021a) designed a StoSQP to remove constraint qualiﬁcation condition;
Curtis et al. (2021b) designed an inexact StoSQP to solve Newton systems approximately; Na
et al. (2021b) designed an active-set StoSQP to enable inequality constraints; Berahas et al. (2022)
designed an accelerated StoSQP by applying variance reduction technique on ﬁnite-sum objective.
All these works showed global convergence of diﬀerent StoSQP schemes. In addition to these works,
Oztoprak et al. (2021); Sun and Nocedal (2022) considered optimization with noisy functions. Those
analyses require known, deterministic, and bounded noise, and thus they are not suited for the
considered problems in (1).

Unconstrained optimization. The asymptotic rate of convergence and asymptotic normality
have been established for the averaged stochastic gradient descent (ASGD) (Polyak and Juditsky,
1992). Subsequently, results on the asymptotic normality of other ﬁrst-order methods and on the
construction of the covariance estimator have been reported (Chen et al., 2020, 2021; Zhu et al.,
2021). The vast majority of the existing works on the asymptotic analysis of SGD focused on the
averaged iterate, and excluded the stepsize 1/t due to some technical challenges. Recently, the

5

asymptotic analysis of stochastic Newton methods has been proposed. Bercu et al. (2020) designed a
Newton scheme for logistic objective, and Boyer and Godichon-Baggioni (2020) extended to general
strongly convex objectives. Compared to the literature on ﬁrst-order methods, both works showed
the normality of the last iterate with 1/t stepsize. However, those analyses are not applicable for
Problem (1) due to the following reasons.

First, Bercu et al. (2020) and Boyer and Godichon-Baggioni (2020) studied regression problems,
where the Hessian is sum of rank one matrices so that the schemes directly update the Hessian inverse
via Sherman–Morrison formula. This step is not suitable for our general objective. Second, those two
papers computed the Hessian inverse to have exact Newton direction. Although the Hessian inverse
requires less computation for regression task, this is not the case for Problem (1). Diﬀerently, we allow
the scheme to solve Newton systems inexactly and randomly. Third, those two papers only studied
1/t deterministic stepsize, while we enable an adaptive random stepsize, and the controlled sequence
βt can have a general decay rate. Our analysis demonstrates that such extension is nontrivial since
the covariance Ξ(cid:63) depends on the decay rate. Fourth, the Berry-Esseen bound is missing in these
two papers, which provides a quantitative understanding on the convergence of the distribution
function of the iterate. Fifth, our covariance Ξ(cid:63) is more complex than those in the two papers
due to the additional sources of randomness (e.g., the randomness in the stepsize and the solver).
Therefore, we have to provide a computable estimator of Ξ(cid:63) to make our theory practical.

We would also like to mention a diﬀerent, but important, line of literature on solving stochastic
objectives (with or without constraints) via sample-average approximation (SAA) methods. See, for
example, Shapiro (1993); Kleywegt et al. (2002); Ahmed and Shapiro (2008) for applying SAA on
diﬀerent problems and Ruszczy´nski and Shapiro (2003); Shapiro et al. (2014) for surveys. The SAA
methods approximate a stochastic objective with some sampling schemes (such as Monte Carlo),
and apply deterministic solvers for solving the approximated objective. In contrast, we consider
stochastic approximation setup, where we apply a stochastic solver (i.e. StoSQP) for solving the
original (stochastic) objective. The solver utilizes stochastic gradient and Hessian of the objective
that are estimated by sampling.

Notation. We use boldface letters to denote column vectors, except that 0 may also denote the zero
matrix. I denotes the identity matrix, whose dimension (and the dimension of 0) is clear from the
context. We use (cid:107) · (cid:107) to denote the (cid:96)2 norm for vectors and the spectral norm for matrices. For
scalars a, b, a ∨ b = max(a, b) and a ∧ b = min(a, b). We use O(·) to denote big O notation in
the usual sense. In particular, f (x) = O(g(x)) means |f (x)| or (cid:107)f (x)(cid:107) ≤ Cg(x) for a constant C
that is independent of x. When x is the iteration index t, ft = O(gt) if |ft| or (cid:107)ft(cid:107) ≤ Cgt for
suﬃciently large t. In this case, we use ft = o(gt) if |ft|/gt → 0 as t → ∞. For a sequence of
compatible matrices {Ai}i, we let (cid:81)j
k=i Ak = AjAj−1 · · · Ai if j ≥ i and I if j < i (similar for scalar
sequence). We use ¯(·) to denote a random quantity that depends on a generated sample (except for
the iterate). We reserve the notation G(x) to denote the Jacobian matrix of constraints, that is
G(x) = ∇T c(x) = (∇c1(x), . . . , ∇cm(x))T ∈ Rm×d.
Structure of the paper. We introduce AI-StoSQP in Section 2; and we establish global almost
sure convergence in Section 3. We establish the asymptotic convergence rate and asymptotic
normality for AI-StoSQP iterates in Section 4. Experiments and conclusions are presented in
Sections 5 and 6, respectively. We defer all proofs to the appendix.

6

2 An Adaptive Inexact StoSQP Framework

In this section, we present AI-StoSQP framework. Let L(x, λ) = f (x) + λT c(x) be the Lagrangian
function of Problem (1) where λ ∈ Rm is the dual vector. Under certain constraint qualiﬁcations, a
necessary condition for (x(cid:63), λ(cid:63)) being a local solution is the KKT conditions: ∇xL(cid:63) = ∇f (x(cid:63)) +
GT (x(cid:63))λ(cid:63) = 0 and ∇λL(cid:63) = c(x(cid:63)) = 0.

We let B0 = I ∈ Rd×d. Given the current iterate (xt, λt), we let ct = c(xt) (similarly, Gt = G(xt),

∇Lt = ∇L(xt, λt), etc.) for shorthand. AI-StoSQP performs the following three steps.

Step 1: Estimate the gradient and Hessian. We generate a single sample ξt ∼ P and compute

Based on this, we let ¯∇2
We also deﬁne the modiﬁed Hessian (used in the Newton system (3)) as

i=1(λt)i∇2ci(xt) be the estimated Hessian of the Lagrangian.

¯gt = ∇f (xt; ξt)
xLt = ¯Ht + (cid:80)m

and

¯Ht = ∇2f (xt; ξt).

Bt =

1
t

t−1
(cid:88)

i=0

¯∇2

xLi + ∆t,

(2)

where ∆t = ∆(xt, λt) is a regularization term to let Bt be positive deﬁnite in the space {x : Gtx = 0},
which can simply be Levenberg-Marquardt type modiﬁcation. Intuitively, we hope ∆t vanishes when
(xt, λt) converges to a local solution. We emphasize that we do not use ¯Bt notation since Bt (and
∆t) is deterministic, conditioned on (xt, λt). Bt does not utilize the sample ξt via ¯Ht. In other words,
the calculation of ¯Ht is only for preparation of the next iteration.
Step 2: Solve the Newton system. We let ¯∇xLt = ¯gt + GT

t λt and solve the Newton system

(cid:18)Bt GT
t
0
Gt

(cid:19) (cid:32)

(cid:33)

(cid:101)∆xt
(cid:101)∆λt

= −

(cid:18) ¯∇xLt
ct

(cid:19)

.

(3)

Instead of solving (3) exactly, we apply randomized iterative solvers introduced in Gower and
Richt´arik (2015), which are competitive or better than deterministic solvers in many cases (Strohmer
and Vershynin, 2008). The core idea is the sketch-and-project step. In particular, we let

Kt =

(cid:18)Bt GT
t
0
Gt

(cid:19)

,

¯∇Lt =

(cid:18) ¯∇xLt
ct

(cid:19)

,

(cid:32)

(cid:33)

,

(cid:101)∆xt
(cid:101)∆λt

(cid:101)zt =

(4)

and consider solving Ktzt = − ¯∇Lt with solution (cid:101)zt. The j-th recursion has the form (zt,0 = 0)

zt,j+1 = arg min

z

(cid:107)z − zt,j(cid:107)2

subject to ST

t,jKtz = −ST
t,j

¯∇Lt,

(5)

where St,j ∈ R(d+m)×q is a random sketching matrix. Its dimension q can also be random. Let us
denote its randomness by variable ζt,j. We assume {ζt,j}j are independent and identically distributed,
and they are also independent of ξt. That is, how we generate the sketching matrix for solving the
Newton system (3) is independent of how we generate samples for estimating the derivatives of f .
iid∼ S and S denotes sketching distribution.
(This is reasonable in practice.) By this setup, we have St,j

By an equivalent formula of (5) in (Gower and Richt´arik, 2015, (2.7)), we derive a recursion

zt,j+1 = zt,j − KtSt,j(ST

t,jK2

t St,j)†ST

t,j(Ktzt,j + ¯∇Lt),

(6)

7

where (·)† denotes the Moore–Penrose pseudoinverse. Although (6) involves a pseudoinverse, when
q = 1 the quantity ST
t St,j reduces to a scalar and we then have a matrix-free update. See the
randomized Kaczmarz method in Strohmer and Vershynin (2008), for example. We perform τ ≥ 1
iterations of (6) and let ( ¯∆xt, ¯∆λt) := zt,τ be the inexact Newton direction.

t,jK2

With (6) and deﬁning Ct,j = I − KtSt,j(ST

t,jK2

t St,j)†ST

t,jKt, we have

zt,τ − (cid:101)zt = Ct,τ −1(zt,τ −1 − (cid:101)zt) = · · · =





τ −1
(cid:89)

j=0


 (zt,0 − (cid:101)zt) = −





Ct,j


 (cid:101)zt := (cid:101)Ct (cid:101)zt.

Ct,j

τ −1
(cid:89)

j=0

(7)

Theoretically, we can show that, if E[I − Ct,j] is invertible, then (cid:107)E[Ct,j](cid:107) < 1. This implies that
(cid:107)E[ (cid:101)Ct | xt, λt](cid:107) vanishes to zero exponentially in τ , and further implies that (cid:107)E[zt,τ − (cid:101)zt | xt, λt, ξt](cid:107)
vanishes to zero exponentially in τ due to (7). Moreover, if E[I − Ct,j] is positive deﬁnite, a similar
guarantee holds for E[(cid:107)zt,τ − (cid:101)zt(cid:107) | xt, λt, ξt]. We refer to (Gower and Richt´arik, 2015, Section 4) for
more details, and we state necessary results later (cf. Lemma 3.4) to make our paper self-contained.
We also mention that we need τ to be large, but it is independent of t. In other words, we do not
require a more and more precise Newton direction approximation as the iteration proceeds. This is
in contrast to the inexact scheme in Curtis et al. (2021b), which controls the approximation error by
the KKT residual and a stepsize related sequence βt (cf. (9)). In other words, Curtis et al. (2021b)
controls the inexactness more adaptively, while the approximation error also has to diminish to zero.
Step 3: Update the iterate with a random stepsize. With the inexact direction ( ¯∆xt, ¯∆λt),
we update the iterate (xt, λt) with a random stepsize ¯αt

(cid:19)

(cid:18)xt+1
λt+1

=

(cid:19)

(cid:18)xt
λt

+ ¯αt

(cid:18) ¯∆xt
¯∆λt

(cid:19)

.

(8)

Certainly, ¯αt depends on the randomness of ξt and {ζt,j}j. We allow for any adaptive scheme for
selecting ¯αt, but we require ¯αt to satisfy a sandwich condition:

0 < βt ≤ ¯αt ≤ ηt with

ηt = βt + χt.

(9)

Here, {βt, ηt} are deterministic prespeciﬁed upper and lower bound sequences, and χt is the gap. We
will impose conditions on these sequences later. The particular adaptive schemes designed in Berahas
et al. (2021b,a); Curtis et al. (2021b,a) all satisfy the above sandwich condition (e.g., (Berahas et al.,
2021b, Lemma 3.6)); and our conditions imposed later on {βt, ηt, χt} are satisﬁed by those studies
as well.

It seems that upper and lower bounds in (9) reduce the diﬃculty for studying a random stepsize.
However, as revealed by the series of works in Berahas et al. (2021b,a); Curtis et al. (2021b,a), the
analysis involving a random stepsize is intrinsically diﬀerent from the one for non-adaptive schemes;
additional terms arise due to the adaptivity; and adaptive schemes have promising practical beneﬁts.
These diﬀerences inspire us to enable a random stepsize in the framework (under the restriction (9)).
When χt = 0, we arrive at a non-adaptive scheme.

We combine the above three steps and summarize the framework in Algorithm 1. We provide a

remark to further discuss the condition (9).

Remark 2.1. We should point out that the condition (9) excludes the promising stochastic line
search method, as studied in Na et al. (2021a,b). This is because there is no clear decaying trend

8

Algorithm 1 An Adaptive Inexact StoSQP (AI-StoSQP) Framework

1: Input: initial iterate (x0, λ0), positive sequences {βt, ηt = βt + χt}, integer τ > 0, B0 = I;
2: for t = 0, 1, 2, . . . do
3:

Generate ξt and compute ¯gt = ∇f (xt; ξt), ¯Ht = ∇2f (xt; ξt), ¯∇2
Compute the modiﬁed Hessian Bt = 1
t

xLt = ¯Ht+(cid:80)m
i=1(λt)i∇2ci(xt);
xLi + ∆t to make it positive deﬁnite in the

(cid:80)t−1
i=0

¯∇2

space {x : Gtx = 0};
Generate {ζt,j}τ −1
j=0 from certain distribution to formulate {St,j}τ −1
Select any random stepsize ¯αt with βt ≤ ¯αt ≤ ηt, and update the iterate as (8);

j=0 , and apply (6) for τ times;

4:

5:

6:
7: end for

for random stepsize that is selected by line search. On the other hand, line search step requires to
generate more and more samples to have a precise estimation for the gradient and Hessian, which is
inapplicable under our fully stochastic setup (i.e., we only generate a single sample per iteration).

To end this section, we introduce additional notation for Algorithm 1 that we will use later. As
mentioned in Step 2, ζt = {ζt,j}τ −1
j=0 denote random variables for generating sketching matrices
{St,j}τ −1
j=0 at the t-th iteration. We also allow the stepsize ¯αt to depend on another random variable
ψt in addition to ξt and ζt (cf. Line 6 in Algorithm 1). For the generated sequence {(ξt, ζt, ψt)}t,
{Ft}t is its adapted ﬁltration; that is Ft = σ({ξi, ζi, ψi}t
i=0), ∀t ≥ 0, is the σ-algebra generated by
the randomness of {ξi, ζi, ψi}t

i=0. We also let

Ft−2/3 = σ({ξi, ζi, ψi}t−1

i=0 ∪ ξt),

Ft−1/3 = σ({ξi, ζi, ψi}t−1

i=0 ∪ ξt ∪ ζt),

and have Ft−1 ⊆ Ft−2/3 ⊆ Ft−1/3 ⊆ Ft. For consistency, F−1 is the trivial σ-algebra. Algorithm 1
has a generating process as follows: given (xt, λt), we ﬁrst generate ξt to estimate the gradient ¯gt
and Hessian ¯Ht and derive Ft−2/3; then we generate ζt to obtain inexact Newton direction and
derive Ft−1/3; then we may generate ψt to select the stepsize ¯αt and derive Ft. For some stepsize
selection procedures like Berahas et al. (2021b), ¯αt is fully determined by ξt, ζt so that no random
variable ψt has to be generated. In this case, we have Ft−1/3 = Ft. By our setup, it is easy to see
that the random quantities in Algorithm 1 have the following recursion

σ(xt, λt) ∪ σ(Bt) ⊆ Ft−1,

σ( ¯∆xt, ¯∆λt) ⊆ Ft−1/3,

σ(¯gt) ∪ σ( ¯Ht) ∪ σ( (cid:101)∆xt, (cid:101)∆λt) ⊆ Ft−2/3,
σ(¯αt) ∪ σ(xt+1, λt+1) ∪ σ(Bt+1) ⊆ Ft.

We also let (∆xt, ∆λt) be the solution of (3) but replace ¯∇xLt with ∇xLt.

3 Global Almost Sure Convergence

In this section, we establish an almost sure convergence for the KKT residual ∇Lt of Algorithm 1,
under standard assumptions. This type of convergence guarantee diﬀers from the convergence in
expectation, that is lim inf t→∞ E[(cid:107)∇Lt(cid:107)] = 0, established in Berahas et al. (2021b,a, 2022); Curtis
et al. (2021b). On the other hand, those convergence in expectation results may be reformed to
almost sure convergence by applying our following analysis on the (cid:96)1 (or (cid:96)2) merit function.

We utilize an exact augmented Lagrangian merit function to show the convergence. This function

has the form

Lµ,ν(x, λ) = L(x, λ) +

µ
2

(cid:107)c(x)(cid:107)2 +

ν
2

9

(cid:107)∇xL(x, λ)(cid:107)2

for µ, ν > 0.

(10)

The augmented Lagrangian (10) was initially proposed by Pillo and Grippo (1979), and it has been
adopted in SQP schemes for diﬀerent problems (Na et al., 2021c; Na, 2021). The advantage of this
augmented Lagrangian is that it is diﬀerentiable, and the inner product between Newton direction
and the gradient ∇Lµ,ν with properly chosen µ, ν is suﬃciently negative to endure inexactness. By
a simple calculation, we have

(cid:19)

(cid:18)∇xLµ,ν
∇λLµ,ν

(cid:18)I + ν∇2
νG

xL µGT
I

=

(cid:19)
(cid:19) (cid:18)∇xL

c

(11)

(the evaluation point has been suppressed). We will show that (xt, λt) of Algorithm 1 decreases Lµ,ν
in expectation in each step, and ﬁnally converges to a KKT point.

3.1 Assumptions and preliminary results

We begin by stating assumptions for showing global convergence.

Assumption 3.1. We assume f, c are twice continuously diﬀerentiable, and there exists a convex
compact set X × Λ that contains the iterates {(xt, λt)}t generated by Algorithm 1. We also assume
∇2L and ∇f are ΥL-Lipschitz continuous in X × Λ. That is, for any (x, λ), (x(cid:48), λ(cid:48)) ∈ X × Λ,

(cid:107)∇2L(x, λ) − ∇2L(x(cid:48), λ(cid:48))(cid:107) ≤ ΥL(cid:107)(x − x(cid:48), λ − λ(cid:48))(cid:107),

(cid:107)∇f (x) − ∇f (x(cid:48))(cid:107) ≤ ΥL(cid:107)x − x(cid:48)(cid:107).

(12)

Furthermore, we assume that Gt has full row rank, and that ∆t is chosen such that Bt satisﬁes (cid:107)Bt(cid:107) ≤
ΥB, and xT Btx ≥ γRH (cid:107)x(cid:107)2, for any x ∈ {x : Gtx = 0}, for some constants 0 < γRH ≤ 1 ≤ ΥB.

Assumption 3.1 is a standard assumption in SQP analysis (Bertsekas, 1982; Nocedal and Wright,
2006). The compactness of X × Λ ensures that Lµ,ν is lower bounded, and there exists a constant
Υu ≥ 1 such that

(cid:107)∇2L(x, λ)(cid:107) ∨ (cid:107)∇L(x, λ)(cid:107) ∨ (cid:107)∇f (x)(cid:107) ≤ Υu,

for any (x, λ) ∈ X × Λ.

(13)

The convexity of X × Λ ensures the Taylor expansion of Lµ,ν at any iterate. The Lipschitz continuity
of ∇2L relaxes the thrice continuous diﬀerentiability of f, c, as assumed in (Bertsekas, 1982, Chapter
4.3). Note that imposing conditions on ∇2L is equivalent to imposing the same conditions on
its components ∇2
xL and G(x), although the Lipschitz constant may be diﬀerent. The Lipschitz
continuity of ∇f is implied by the compactness of the set X , while we express it out explicitly with
constant ΥL. With the above setup and a simple calculation, we know from (11) that ∇Lµ,ν is also
Lipschitz continuous in X × Λ. That is, for any (x, λ), (x(cid:48), λ(cid:48)) ∈ X × Λ,

(cid:107)∇Lµ,ν(x, λ) − ∇Lµ,ν(x(cid:48), λ(cid:48))(cid:107)

≤ {(1 + (2ν + µ)Υu) Υu + (2ν + µ)ΥuΥL} (cid:107)(x − x(cid:48), λ − λ(cid:48))(cid:107) =: Υµ,ν(cid:107)(x − x(cid:48), λ − λ(cid:48))(cid:107).

(14)

Assumption 3.1 also assumes that Gt has full row rank, which is a common constraint qualiﬁcation
to ensure the uniqueness of the dual solution. By the compactness of X , we have GtGT
t (cid:23) γGI for
some constant 0 < γG ≤ 1. Together with the conditions on Bt, we know that the Newton system
(3) has a unique solution (cf. (Nocedal and Wright, 2006, Lemma 16.1)), and that the KKT matrix
inverse K−1

is uniformly bounded. We denote by (cid:107)K−1

t (cid:107) ≤ ΥK for ΥK ≥ 1.

We also impose the bounded moment condition on stochastic gradient ¯gt and Hessian ¯Ht.

t

10

Assumption 3.2. We assume E[¯gt | xt] = ∇ft, E[ ¯Ht | xt] = ∇2ft, and assume following moment
conditions when needed : for a constant Υm ≥ 1,

and

gradient

(bounded 2nd moment) :

(bounded 3rd moment) :

(bounded 4th moment) :

E[(cid:107)¯gt − ∇ft(cid:107)2 | xt] ≤ Υm,
E[(cid:107)¯gt − ∇ft(cid:107)3 | xt] ≤ Υm,
E[(cid:107)¯gt − ∇ft(cid:107)4 | xt] ≤ Υm,

Hessian (bounded 2nd moment) :

(bounded 2nd moment) :

E[(cid:107) ¯Ht − ∇2ft(cid:107)2 | xt] ≤ Υm,
(cid:107)∇2f (x; ξ)(cid:107)2] ≤ Υm.
E[sup
x∈X

(15a)

(15b)

(15c)

(15d)

(15e)

We write E[· | xt] to express the conditional variable clearly. It can also be written as E[· | Ft−1],
which means that the expectation is taken over randomness of ξt ∼ P. For conditions (15), we do
not require all of them at once, but we impose them step by step. In fact, (15c) implies (15b), which
implies (15a). By the compactness of X (which implies the boundedness of (cid:107)∇2ft(cid:107)), (15e) implies
(15d) although for a diﬀerent constant.

We mention that (15e) is imposed even for asymptotic analysis of ASGD (Chen et al., 2020). It
ensures the Lipschitz continuity of the mapping x → E[∇f (x; ξ)∇T f (x; ξ)] (as proved in (C.9)). See
(Chen et al., 2020, Assumption 3.2(2) and Lemma 3.1) for the discussion. We note that (Boyer and
Godichon-Baggioni, 2020, Assumption (A1c)) directly assumed the mapping is continuous. We prefer
to adopt (15e) due to two reasons. First, (15e) has a clear connection to (15d). It is satisﬁed by a
variety of objective functions, such as least squares regression f (x; ξ) = (ξy − xT ξx)2/2 and logistic
regression f (x; ξ) = log (cid:0)1 + exp (cid:0)xT ξx
(cid:1)(cid:1) − ξy · xT ξx, where ξ = (ξy, ξx) is the response-feature pair
with ξy ∈ R, ξx ∈ Rd for least squares regression and ξy ∈ {0, 1}, ξx ∈ Rd for logistic regression,
as long as ξx has bounded 4-th moment. This condition is more intuitive and checkable than the
continuity of the mapping. Second, although our asymptotic normality and Berry-Esseen bound also
hold under the continuity condition on the mapping, (15e) further enables us to design a practical
covariance estimator with an explicit convergence rate (cf. Lemma 4.12). The covariance matrix
estimation is not considered in Boyer and Godichon-Baggioni (2020).

In this section, we only require (15a) for studying the convergence of ∇Lt and require (15d) for
studying the convergence of Kt. In the next section, we require higher order moment condition for
conducting asymptotic analysis.

In terms of the distribution of sketching matrices of randomized iterative solvers, we need the

following assumption.

Assumption 3.3. For any t ≥ 0, we assume that the sketching matrices St,j
S satisﬁes

E[KtS(ST K2

t S)†ST Kt | xt, λt] (cid:23) γSI

for γS > 0.

iid∼ S with distribution

Assumption 3.3 is required by (Gower and Richt´arik, 2015, Theorem 4.6) to ensure that the
iterates generated by randomized solvers converge in expectation. It can be easily veriﬁed for some
sketching matrices. For example, in randomized Kaczmarz method, S = ei with equal probability
where ei is the i-th canonical basis of Rd+m. Then, we have

E[KtS(ST K2

t S)†ST Kt | xt, λt] (cid:23)

E[KtSST Kt | xt, λt]
maxj[K2

t ]j,j

=

K2
t
(d + m) · maxj[K2

t ]j,j

(cid:23)

I
(d + m)κ(K2
t )

,

11

t ]j,j denotes the (j, j)-entry of K2

t and κ(K2
where [K2
t (it is
independent of t due to the compactness of X × Λ). In this case, Assumption 3.3 is implied by
Assumption 3.1. Assumption 3.3 directly leads to the following result.

t ) denotes the condition number of K2

Lemma 3.4 (Guarantees of randomized solvers). Under Assumption 3.3, the following statements
hold for all t ≥ 0.
(a): 0 < γS ≤ 1.
(b): E[zt,τ − (cid:101)zt | xt, λt, ξt] = −(I − E[KtS(ST K2
ρ = 1 − γS < 1.
(c): E[(cid:107)zt,τ − (cid:101)zt(cid:107)2 | xt, λt, ξt] ≤ ρτ (cid:107)(cid:101)zt(cid:107)2.
Proof. We know that

(cid:101)zt =: Ct (cid:101)zt, and (cid:107)Ct(cid:107) ≤ ρτ with

t S)†ST Kt | xt, λt])τ

γS ≤ (cid:107)E[KtS(ST K2

t S)†ST Kt | xt, λt](cid:107) ≤ E[(cid:107)KtS(ST K2

t S)†ST Kt(cid:107) | xt, λt] ≤ 1,

t S)†ST Kt
where the second inequality is by Jensen’s inequality; the third inequality is since KtS(ST K2
is a projection matrix. This shows (a). (b) follows from (7) and the independence among {ζt,j}j.
(cid:4)
(c) is proved by (Gower and Richt´arik, 2015, Theorem 4.6).

3.2 Almost sure convergence

We now set the stage to establish the global almost sure convergence. The ﬁrst result shows that
(∆xt, ∆λt) is a descent direction of Lt
µ,ν = Lµ,ν(xt, λt) if µ is suﬃciently large and ν is suﬃciently
small. A similar result has been shown in (Na et al., 2021c, (5.15)), while we provide an alternative
proof that is more straightforward than there.

Lemma 3.5. Under Assumption 3.1, we have

(cid:18)∇xLt
∇λLt

µ,ν

µ,ν

(cid:19)

(cid:19)T (cid:18)∆xt
∆λt

≤ −

νγG
8

(cid:40)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)

2(cid:41)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

provided that

Proof. See Appendix B.1.

ν ≤

γRH
5(ΥB + Υu)2

and

µν ≥

9
γG

.

(16)

(cid:4)

Combining Lemmas 3.4 and 3.5, we are able to show the following recursion.

Lemma 3.6. Under Assumptions 3.1, 3.2(15a), 3.3, and suppose that (µ, ν) satisﬁes (16) and ρτ ≤
νγG/(16µΥu), we have

E[Lt+1

µ,ν | Ft−1] ≤ Lt

µ,ν −

νγGβt
16

(cid:107)∇Lt(cid:107)2 + (cid:101)Υµ,ν(χt + η2
t )

where (cid:101)Υµ,ν = 4µΥKΥ2
u(

√

Υm ∨ Υu) ∨ 8Υµ,νΥ2

K(Υ2

u ∨ Υm).

Proof. See Appendix B.2.

(cid:4)

A similar recursion to Lemma 3.6 for the (cid:96)1 merit function was established in Berahas et al.

(2021b). By Lemma 3.6, we are able to show an almost sure convergence of (cid:107)∇Lt(cid:107).

12

Theorem 3.7 (Convergence of the KKT residual). Consider Algorithm 1 under Assumptions 3.1,
3.2(15a), 3.3. Suppose τ satisﬁes

τ ≥

12 log 2 + 5 log(ΥB + Υu) + 2 log(1/(γRH γG))
log(1/(1 − γS))

,

and {βt, ηt = βt + χt} satisﬁes
∞
(cid:88)

βt = ∞,

t=0

∞
(cid:88)

t=0

η2
t < ∞,

∞
(cid:88)

t=0

χt < ∞,

then we have (cid:107)(xt+1 − xt, λt+1 − λt)(cid:107) → 0 and (cid:107)∇Lt(cid:107) → 0 as t → ∞ almost surely.

Proof. See Appendix B.3.

(17)

(18)

(cid:4)

Based on Theorem 3.7, we provide a corollary about the worst-case iteration complexity of Al-
gorithm 1, which recovers the result (a) introduced in Section 1.2. We do not need the condition
(18) since we provide the convergence rate of averaged expected KKT residual, (cid:80)t−1
E[(cid:107)∇Lj(cid:107)2]/t,
j=0
instead of providing the convergence rate of (cid:107)∇Lt(cid:107).

Corollary 3.8. Consider Algorithm 1 under Assumptions 3.1, 3.2(15a), 3.3. Suppose τ satisﬁes (17),
βt = (t + 1)−a, χt = (t + 1)−b where a ∈ (0, 1) and a < b, we deﬁne T(cid:15) = inf t {t ≥ 1 : E[(cid:107)∇Lt(cid:107)] ≤ (cid:15)}.
Then, we have

T(cid:15) = O

(cid:16)

2
a∧(1−a)∧(b−a)

(cid:15)−

(cid:17)

.

In particular, if b = 2a (as used in Berahas et al. (2021b)), then T(cid:15) = O((cid:15)−2/{a∧(1−a)}). In this case,
the best complexity is achieved by a = 1/2 and we have T(cid:15) = O((cid:15)−4).

Proof. See Appendix B.4.

(cid:4)

Before studying the convergence of the Hessian matrix Kt, we provide two remarks for Theorem

3.7 and Corollary 3.8.

Remark 3.9. Theorem 3.7 shows that all limiting points of the iteration sequence are stationary,
while it does not suggest that (xt, λt) always converges to a stationary point or a local solution. This is
similar to deterministic SQP guarantees (Nocedal and Wright, 2006, Theorem 18.3). The convergence
of (xt, λt) is equivalent to the convergence of ∇Lt for problems where the objective is strongly
convex and constraints are aﬃne. For nonlinear problems, if X × Λ has a single stationary point
inside, then (xt, λt) also converges to that point. Furthermore, if that point is a strict local solution
(by chance), then (xt, λt) converges to that strict local solution. To facilitate our analysis on the
convergence rate of StoSQP, we assume in Assumption 3.11 that (xt, λt) → (x(cid:63), λ(cid:63)) for a strict
local solution (x(cid:63), λ(cid:63)), whose Hessian K(cid:63) = ∇2L(cid:63) is invertible (as implied by (i) ∇2
xL(cid:63) is positive
deﬁnite in {x : G(cid:63)x = 0}; (ii) G(cid:63) has full row rank). Note that (xt, λt) → (x(cid:63), λ(cid:63)) is equivalent to
assuming that (xt, λt) jumps into a small neighborhood of (x(cid:63), λ(cid:63)) for a suﬃciently large t. This is
because (cid:107)(xt+1 − xt, λt+1 − λt)(cid:107) ∨ (cid:107)∇Lt(cid:107) → 0 implies that there exists an attraction neighborhood
around (x(cid:63), λ(cid:63))—once (xt, λt) lies in the neighborhood, then all the following iterates stay in the
neighborhood. We emphasize that targeting on an iteration sequence that converges to a strict
local solution is a routine setup for convergence rate analysis of deterministic SQP (e.g., Bertsekas
(1982); Lucidi (1990); Sun and Yuan (2006); Liu and Yuan (2011)); and it is reasonable for StoSQP
as well, given the convergence guarantee in Theorem 3.7 matches the one of deterministic SQP, up
to a a zero-probability event.

13

Remark 3.10. Corollary 3.8 complements the existing iteration complexity result in Curtis et al.
(2021a), in the sense that it shows the complexity with decaying sequences {βt, ηt} while (Curtis et al.,
2021a, Theorem 2) showed the complexity with constant sequences βt = β and ηt = β + β2. Our
analysis allows one to set χt to be diﬀerent higher orders of βt, instead of always setting χt = O(β2
t )
as did in that work. Our result indicates that, to achieve (cid:15) stationarity, O((cid:15)−4) iterations are required
t + 1 and χt = 1/(t + 1). The complexity of O((cid:15)−4) matches that work as
at most with βt = 1/
well. We should mention that Curtis et al. (2021a) studied a more practical algorithm that involves
the step of merit parameter selection. We remove such a step since we focus on (local) asymptotic
convergence rate. In addition, both Corollary 3.8 and Curtis et al. (2021a) diﬀer from our main
analyses in Section 4. Here, we provide a non-asymptotic convergence rate for the averaged expected
KKT residual (i.e., the result holds for any t); while in Section 4 we will provide an asymptotic
almost sure convergence rate for the last iterate (xt, λt) (i.e., the result holds for suﬃciently large t).

√

Finally, we study the convergence of Kt deﬁned in (4). We need the following (local) assumption.

Assumption 3.11. We assume (xt, λt) → (x(cid:63), λ(cid:63)) almost surely to a strict local solution (x(cid:63), λ(cid:63))
xL(cid:63) is positive deﬁnite in the space {x : G(cid:63)x = 0}.
that satisﬁes (i) G(cid:63) has full row rank, (ii) ∇2
Furthermore, we assume (cid:107)∆t(cid:107) ≤ ωt with a deterministic sequence wt → 0.

Assumption 3.11 assumes that the iteration sequence converges to a local solution. By Remark 3.9,
it enables us to study the local rate of the iterates. Assumption 3.11 also assumes that the Hessian
modiﬁcation ∆t vanishes, which is standard in deterministic SQP analysis. In fact, a naive way
to choose ∆t is based on checking the sign of index of inertia of the KKT matrix Kt. When the
inertia has correct sign, i.e., d positive eigenvalues and m negative eigenvalues, we do not need any
modiﬁcation. By this way, we have ωt = 0 for all suﬃciently large t.

We now show that Kt converges to K(cid:63) = ∇2L(cid:63) in the next theorem.

Theorem 3.12 (Convergence of the Hessian matrix). Suppose Assumptions 3.2(15d) and 3.11 hold,
and ∇2L is ΥL-Lipschitz continuous (cf. (12)). We have Kt → K(cid:63) where Kt is deﬁned in (4).

Proof. See Appendix B.5.

(cid:4)

4 Asymptotic Convergence Rate and Normality

In this section, we provide an explicit convergence rate for the iterate (xt, λt) when (xt, λt) converges
to a strict local solution (x(cid:63), λ(cid:63)). Our main results are presented in Theorems 4.8 and 4.11, which
certify the results (b) and (c) introduced in Section 1.2. These results extend the results of Section
3, where we showed that the KKT residual (cid:107)∇Lt(cid:107) → 0 and the Hessian Kt → K(cid:63). We summarize
all analyses in Sections 3 and 4, and present a complete result of AI-StoSQP in Theorem 4.14.

To study the convergence rate, we ﬁrst establish an iteration recursion in Section 4.1. The recur-
sion consists of three terms, which we analyze in Section 4.2. Then, we establish the asymptotic
convergence rate and asymptotic normality in Section 4.3. To preform statistical inference of (x(cid:63), λ(cid:63))
in practice, we propose a covariance estimator in Section 4.4. All global and local results are then
summarized in Section 4.5 with a discussion on the conditions on the sequences {βt, χt}t.

14

4.1 Iteration recursion

From a high-level view, we can show that Algorithm 1 generates a complicated stochastic process
(cid:18)xt − x(cid:63)
λt − λ(cid:63)

(cid:18)xt+1 − x(cid:63)
λt+1 − λ(cid:63)

= (1 − ¯αt)

+ ¯αt

+ ¯αt

(19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

,

(cid:18)θt
x
θt
λ

(cid:18)δt
x
δt
λ

x, θt

x, θt

λ) is a martingale diﬀerence with E[(θt

where (θt
λ) | Ft−1] = 0, brought by generating a sample
ξt ∼ P and solving Newton systems via randomized solvers; and (δt
λ) is the remaining error term.
Compared to the studies on unconstrained ASGD and stochastic Newton (Polyak and Juditsky, 1992;
Chen et al., 2020; Bercu et al., 2020; Boyer and Godichon-Baggioni, 2020), the two main components
of AI-StoSQP—adaptivity and inexactness—kick in all terms in the recursion (19), and they lead
to a much more challenging analysis. In short, we have to deal with a random stepsize ¯αt, and
θt = (θt

λ) also contain the approximation error from randomized solvers.

λ) and δt = (δt

x, θt

x, δt

x, δt

We formalize the recursion (19) in the following lemma. To ease notation, we let ϕt = (βt + ηt)/2.

Lemma 4.1. Algorithm 1 generates a sequence of iterates (xt, λt) that can be expressed as

(cid:19)

(cid:18)xt+1 − x(cid:63)
λt+1 − λ(cid:63)

= I1,t + I2,t + I3,t

where

I1,t =

I2,t =

t
(cid:88)

t
(cid:89)

i=0

j=i+1

t
(cid:88)

t
(cid:89)

i=0

j=i+1

{I − ϕj(I + C(cid:63))} ϕi

(cid:19)

,

(cid:18)θi
x
θi
λ

{I − ϕj(I + C(cid:63))} (¯αi − ϕi)

(cid:18) ¯∆xi
¯∆λi

(cid:19)

,

I3,t =

t
(cid:89)

i=0

{I − ϕi(I + C(cid:63))}

(cid:19)

(cid:18)x0 − x(cid:63)
λ0 − λ(cid:63)

+

t
(cid:88)

t
(cid:89)

i=0

j=i+1

{I − ϕj(I + C(cid:63))} ϕi

(cid:19)

,

(cid:18)δi
x
δi
λ

and (see Lemma 3.4(b) for the deﬁnition of Ci)

C(cid:63) = −(I − E[K(cid:63)S(ST (K(cid:63))2S)†ST K(cid:63)])τ ,
(cid:40)(cid:18) ¯∆xi
(cid:19)
¯∆λi

= −(I + Ci)K−1

+

(cid:19)

i

(cid:19)

− (I + Ci)

(cid:18)θi
x
θi
λ
(cid:18)δi
x
δi
λ
(cid:18)ψi
x
ψi
λ

(cid:18)¯gi − ∇fi
0
(cid:18)ψi
x
ψi
λ
(cid:18)xi − x(cid:63)
λi − λ(cid:63)

(cid:19)

(cid:19)

.

(cid:19)

(cid:26)

= −(I + Ci)

(K(cid:63))−1

(cid:19)

=

(cid:19)

(cid:18)∇xLi
ci

− K(cid:63)

+ (cid:8)K−1

i − (K(cid:63))−1(cid:9)

(cid:32)

(cid:33)(cid:41)

,

(cid:101)∆xi
(cid:101)∆λi
(cid:19)(cid:27)

(cid:18)∇xLi
ci

− (Ci − C(cid:63))

(cid:19)

(cid:18)xi − x(cid:63)
λi − λ(cid:63)

, (21c)

(21d)

Further, under Assumptions 3.2, 3.3, θi = (θi

Proof. See Appendix C.1.

x, θi

λ) is a martingale diﬀerence with E[θi | Fi−1] = 0.
(cid:4)

From Lemma 4.1, we see that the iteration recursion consists of three terms. I1,t is a martingale,
which further consists of the randomness of generating sample ξt to estimate ∇ft and the randomness

15

(20a)

(20b)

(20c)

(21a)

(21b)

of solving Newton system (3). I2,t characterizes the randomness of the stepsize ¯αt. I3,t contains all
the remaining terms. We will show that I1,t term provides the asymptotic convergence rate and
asymptotic normality, ensured by the central limit theorem for martingales; and that I2,t and I3,t
terms only contribute higher order errors as long as we set χt = ηt − βt properly.

To facilitate our analysis, we state here a preliminary continuity property for the projection matrix
t S)†ST Kt, appearing in quantities Ct,j, Ct etc. (cf. (7) and Lemma 3.4(b)). It is fairly
KtS(ST K2
easy to see that if ST K2
t S is invertible (i.e., S has full column rank), then the projection matrix is
continuous in Kt (because (·)−1 is a continuous function). However, to enable more general cases,
we do not require S to have full column rank. The continuity property of the projection matrix still
holds, ensured by Wedin’s sin(Θ) theorem (Wedin, 1972). 1

Lemma 4.2. Suppose Kt, K(cid:63) ∈ R(d+m)×(d+m) have full rank, for any S ∈ R(d+m)×q, we have

(cid:107)KtS(ST K2

t S)†ST Kt − K(cid:63)S(ST (K(cid:63))2S)†ST K(cid:63)(cid:107) ≤

2(cid:107)Kt − K(cid:63)(cid:107)
σmin(K(cid:63))

·

(cid:107)S(cid:107)
σ+
min(S)

,

where σmin(·) denotes the least singular value and σ+

Proof. See Appendix C.2.

min(·) denotes the least nonzero singular value.
(cid:4)

Lemma 4.2 not only suggests that the projection matrix KtS(ST K2

t S)†ST Kt is continuous in
Kt for any S (given the full rankness of Kt, cf. Assumption 3.1), but also provides an explicit
bound on characterizing the diﬀerence of two projection matrices. In particular, the diﬀerence of
two projection matrices is proportional to the diﬀerence of two Hessians. We introduce the following
condition on the sketching distribution S and present a corollary.

Assumption 4.3. The sketching distribution S satisﬁes E[(cid:107)S(cid:107)/σ+

min(S)] ≤ ΥS for some ΥS ≥ 1.

Assumption 4.3 is equivalent to assuming that the condition number of S, that is (cid:107)S(cid:107)(cid:107)S†(cid:107), has
bounded expectation. This assumption is mild, and is satisﬁed with ΥS = 1 for any sketching
distribution if S is a vector, such as S = ei ∈ Rd+m for i = 1, . . . , d + m with equal probability as
used in randomized Kaczmarz method.

Corollary 4.4. Suppose Kt, K(cid:63) have full rank and Kt → K(cid:63) almost surely, then Ct → C(cid:63) almost
surely. Furthermore, if Assumption 4.3 holds, then we quantitatively have

(cid:107)Ct − C(cid:63)(cid:107) ≤

2τ ΥS
σmin(K(cid:63))

(cid:107)Kt − K(cid:63)(cid:107).

Proof. See Appendix C.3.

(cid:4)

By Corollary 4.4, we know that δt in (21c) satisﬁes δt = o((cid:107)(xt − x(cid:63), λt − λ(cid:63))(cid:107)). This is because

ψt = O((cid:107)(xt − x(cid:63), λt − λ(cid:63))(cid:107)2), Kt → K(cid:63), and Ct → C(cid:63). See Lemma 4.7 for details.

1In fact, the pseudo-inverse is continuous when rank is not perturbed (which is the case in our paper). In particular,

(cid:107)A† − B†(cid:107) is small if rank(A) = rank(B) and (cid:107)A − B(cid:107) is small. See (Wedin, 1973, Theorem 4.1) for details.

16

4.2 Bounding I1,t, I2,t, I3,t terms

We bound I1,t, I2,t, I3,t in (20a), (20b), (20c), respectively. To simplify the presentation, we introduce
additional notation. We let I + C(cid:63) have an eigenvalue decomposition expressed as

I + C(cid:63) = U ΣU T

with Σ = diag(σ1, . . . , σd+m).

(22)

Under Assumption 3.3, full rankness of Kt and K(cid:63), and Kt → K(cid:63), Lemma 3.4(b) and Corollary 4.4
imply that (cid:107)C(cid:63)(cid:107) ≤ ρτ . Also, we note from (21a) that C(cid:63) (cid:22) 0. Thus, for any i = 1, . . . , d + m,

0 < 1 − ρτ ≤ σi ≤ 1

with ρ = 1 − γS.

We generate sketch matrices S1, . . . , Sτ

iid∼ S and deﬁne a random matrix

τ
(cid:89)

(cid:101)C(cid:63) = −

(I − K(cid:63)Sj(ST

j (K(cid:63))2Sj)ST

j K(cid:63)).

(23)

(24)

Obviously, we have E[ (cid:101)C(cid:63)] = C(cid:63). We also deﬁne

j=1

Ω(cid:63) = (K(cid:63))−1

(cid:18)E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)] − ∇f (x(cid:63))∇T f (x(cid:63)) 0
0
0

(cid:19)

(K(cid:63))−1.

(25)

With the above deﬁnitions, the next lemma establishes the asymptotic convergence rate, asymp-

totic normality, and Berry-Esseen bound for the martingale I1,t. Recall that ϕt = (βt + ηt)/2.

Lemma 4.5. Under Assumptions 3.1, 3.2(15a, 15e), 3.3, 3.11 and suppose

lim
t→∞

t

(cid:18)

1 −

(cid:19)

ϕt−1
ϕt

= ϕ < 0,

lim
t→∞

tϕt = (cid:101)ϕ ∈ (0, ∞],

1 − ρτ +

ϕ
(cid:101)ϕ

> 0.

(26)

Then, for any υ > 0, we have

I1,t = o

(cid:18)(cid:113)

ϕt {log(1/ϕt)}1+υ

(cid:19)

almost surely.

(27)

Furthermore, if (15b) holds, then we have
(a) (asymptotic rate) I1,t = O((cid:112)ϕt log(1/ϕt)) almost surely.
(b) (asymptotic normality) (cid:112)1/ϕt · I1,t
d−→ N (0, Ξ(cid:63)) where

Ξ(cid:63) = U

(cid:16)

Θ ◦ U T E

(cid:104)

(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T U

(cid:105)(cid:17)

U T with [Θ]k,l =

1
σk + σl + ϕ
(cid:101)ϕ

.

(28)

Here, ◦ denotes the matrix Hadamard product.
(c) (Berry-Esseen bound) For any vector w = (wx, wλ) ∈ Rd+m such that wT Ξ(cid:63)w (cid:54)= 0, we have

(cid:12)
(cid:12)
(cid:12)
P
(cid:12)
(cid:12)

sup
z∈R

(cid:32) (cid:112)1/ϕt · wT I1,t

√

wT Ξ(cid:63)w

≤ z

(cid:33)

(cid:12)
(cid:12)
(cid:12)
− P (N (0, 1) ≤ z)
(cid:12)
(cid:12)

= O

(cid:16)(cid:112)ϕt log(1/ϕt)

(cid:17)

.

Proof. See Appendix C.4.

(cid:4)

17

As we mentioned after Assumption 3.2 in Section 3, the condition (15e) in Lemma 4.5 can be
weakened to (15d) if we additionally assume the mapping, x → E[∇f (x; ξ)∇T f (x; ξ)], is continuous.
(This is because (C.10) holds immediately by the continuity of the mapping, and all the remaining
proofs still apply.)

Next, we characterize I2,t deﬁned in (20b). Recall that χt = ηt − βt is the gap of upper and

lower bounds for the stepsize ¯αt.

Lemma 4.6. Under Assumptions 3.1, 3.2(15a), 3.3, 3.11 and suppose (26) holds. Furthermore, we
suppose χt satisﬁes for a constant χ, and a positive constant υ(cid:48) > 0 that

lim
t→∞

t

(cid:18)

1 −

(cid:19)

χt−1
χt

= χ,

2(1 − ρτ ) +

2χ − ϕ
(cid:101)ϕ

> 0,

{log(1/χt)}1+υ(cid:48)

= O(1/ϕt).

(29)

Then, we have

I2,t = O

(cid:19)

(cid:18) χt
ϕt

almost surely.

Proof. See Appendix C.5.

Next, we characterize I3,t deﬁned in (20c).

(cid:4)

Lemma 4.7. Under Assumptions 3.1, 3.2(15a, 15e), 3.3, 3.11 and suppose (26) and (29) hold. Then,
for any υ > 0,

(cid:18)(cid:113)

I3,t = o

ϕt {log(1/ϕt)}1+υ

(cid:19)

+ o(χt/ϕt) = o(I1,t + I2,t)

almost surely.

Furthermore, if (15b) holds, the above result holds with υ = 0.

Proof. See Appendix C.6.

(cid:4)

4.3 Asymptotic rate and normality

We combine Lemmas 4.1, 4.5, 4.6, 4.7, and derive the following asymptotic convergence rate for the
iterates. The proof is omitted.

Theorem 4.8 (Asymptotic convergence rate). Under Assumptions 3.1, 3.2(15a, 15e), 3.3, 3.11 and
suppose (26) and (29) hold. Then, for any υ > 0,

(cid:107)(xt − x(cid:63), λt − λ(cid:63))(cid:107) = o

(cid:18)(cid:113)

ϕt {log(1/ϕt)}1+υ

(cid:19)

+ O(χt/ϕt)

almost surely.

Furthermore, if (15b) holds, we have

(cid:107)(xt − x(cid:63), λt − λ(cid:63))(cid:107) = O

(cid:17)
(cid:16)(cid:112)ϕt log(1/ϕt)

+ O(χt/ϕt)

almost surely.

From Theorem 4.8, we see that the asymptotic convergence rate consists of two terms. The ﬁrst
term comes from the strong law of large number for a martingale that characterizes the random
sampling in each iteration (cf. I1,t in (20a)). The second term comes from the adaptivity of random
stepsizes (cf. I2,t in (20b)). It disappears when χt = 0, i.e., setting ¯αt = βt deterministically. From

18

Lemma 4.7, we see that all the remaining terms contained in I3,t only contribute higher order
errors. We note that Berahas et al. (2021b,a); Curtis et al. (2021b) set χt = O(β2
t ). In this case,
χt = O(ϕ2
t ) and, hence, the ﬁrst term dominates the convergence rate. In fact, our theorem suggests
that χt = O(β3/2
) is suﬃcient to have the ﬁrst term dominate, which allows a wider interval for
selecting the stepsize ¯αt.

t

We will transfer conditions (26) and (29) to some conditions on the sequence {βt, χt}t in Theorem
4.14, and justify them in Lemma 4.15. Before that, we establish the asymptotic normality and
Berry-Esseen bound for the iterate (xt, λt). To show these results, we need an explicit convergence
rate for Kt, which further allows us to provide a better rate for the term I3,t.

Lemma 4.9. Under Assumptions 3.1, 3.2(15a, 15e), 3.3, 3.11 and suppose (26) and (29) hold. Then,
for any υ > 0, we have

(cid:107)Kt − K(cid:63)(cid:107) = o

(cid:18)(cid:113)

ϕt {log(1/ϕt)}1+υ

(cid:19)

+ O

(cid:19)

(cid:18) χt
ϕt

+ ωt

almost surely.

Furthermore, if (15b) holds, we have

(cid:107)Kt − K(cid:63)(cid:107) = O

(cid:16)(cid:112)ϕt log(1/ϕt)
(cid:17)

+ O

(cid:19)

(cid:18) χt
ϕt

+ o

(cid:32)(cid:114)

(cid:33)

(log t)1+υ
t

+ ωt

almost surely.

Proof. See Appendix C.7.

(cid:4)

We provide a better rate for I3,t. The following lemma diﬀers from Lemma 4.7 in the bound
of δt. Using Lemma 4.9, we can have a more precise bound on δt than (C.25) in the proof of
Lemma 4.7. We need to impose Assumption 4.3 and make use of Corollary 4.4. We only present the
result under (15b) and χt = o(ϕ3/2
). This is because the asymptotic normality and Berry-Esseen
bound, where we will apply the result, require both of them. In particular, (15b) is required for
the asymptotic normality of I1,t in Lemma 4.5. χt = o(ϕ3/2
) is required to ensure that, compared
to (cid:112)1/ϕt · I1,t, (cid:112)1/ϕt · I2,t = o(1) contributes higher order errors (cf. Lemma 4.6) and does not
aﬀect the normality.

t

t

Lemma 4.10. Under Assumptions 3.1, 3.2(15b, 15e), 3.3, 3.11, 4.3, and suppose (26) and (29)
hold. Suppose also that

lim
t→∞

t

(cid:18)

1 −

(cid:19)

ωt−1
ωt

= ω < 0,

1 − ρτ +

ϕ + (−1 ∧ 2ω)
2 (cid:101)ϕ

> 0, χ <

3ϕ
2

.

(30)

Then, for any υ > 0,

I3,t = O (ϕt log(1/ϕt)) + o

(cid:32)
(cid:112)ϕt log(1/ϕt) ·

(cid:114)

(cid:33)

(log t)1+υ
t

+ O

(cid:16)(cid:112)ϕt log(1/ϕt) · ωt

(cid:17)

almost surely.

The above result also holds if ωt = 0 for all suﬃciently large t.

Proof. See Appendix C.8.

(cid:4)

Combining Lemmas 4.1, 4.5, 4.6, and 4.10, we derive the following theorem.

19

Theorem 4.11 (Asymptotic normality and Berry-Esseen bound). Under Assumptions 3.1, 3.2(15b,
15e), 3.3, 3.11, 4.3, and suppose (26), (29), and (30) hold. Then,

(cid:112)1/ϕt ·

(cid:19)

(cid:18)xt − x(cid:63)
λt − λ(cid:63)

d−→ N (0, Ξ(cid:63)),

where Ξ(cid:63) is in (28). Further, for any vector w = (wx, wλ) ∈ Rd+m such that wT Ξ(cid:63)w (cid:54)= 0 and any
υ > 0, we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32) (cid:112)1/ϕt · wT (xt − x(cid:63), λt − λ(cid:63))
wT Ξ(cid:63)w

− P (N (0, 1) ≤ z)

sup
z∈R

≤ z

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

(cid:33)

P

√

= O (

ϕt log (1/ϕt)) + O

(cid:33)

+ o

(cid:32)

χt
ϕ3/2
t

(cid:32) (cid:112)log(1/ϕt)(log t)1+υ
√

t

(cid:33)

+ O

(cid:16)(cid:112)log(1/ϕt) · ωt

(cid:17)

.

The above results also hold if ωt = 0 for all suﬃciently large t.
Proof. See Appendix C.9.

(cid:4)

4.4 An estimator of the covariance matrix

Theorem 4.11 shows the asymptotic normality of (xt, λt). However, the covariance matrix Ξ(cid:63) (28)
has a complex form. It depends on Ω(cid:63) in (25) and an expectation of a random matrix (cid:101)C(cid:63) in (24),
both of which are inaccessible in practice. To perform statistical inference for (x(cid:63), λ(cid:63)), we propose
a simple estimator for Ξ(cid:63). Our estimator is independent of the sketching distribution employed in
randomized solvers.

Lemma 4.12. We let

(cid:32) 1
t

Ωt = K−1

t

(cid:80)t−1

i=0 ¯gi¯gT

i −

(cid:16) 1
t

(cid:80)t−1

i=0 ¯gi
0

(cid:17) (cid:16) 1
t

(cid:80)t−1

i=0 ¯gi

(cid:17)T

(cid:33)

0

0

K−1
t

and Ξt =

Ωt
2 + ϕ/ (cid:101)ϕ

.

(31)

Under Assumptions 3.1, 3.2(15c, 15e), 3.3, 3.11 and suppose (26) and (29) hold. Then, for any υ > 0,
we have

(cid:107)Ξ(cid:63) − Ξt(cid:107) = O(ρτ ) + O

(cid:17)
(cid:16)(cid:112)ϕt log(1/ϕt)

+ O

(cid:19)

(cid:18) χt
ϕt

+ o

(cid:32)(cid:114)

(cid:33)

(log t)1+υ
t

+ ωt.

Proof. See Appendix C.10.

(cid:4)

Lemma 4.12 leads to the following corollary, which shows Berry-Esseen bound with true covariance

being replaced by Ξt.
Corollary 4.13. Under Assumptions 3.1, 3.2(15c, 15e), 3.3, 3.11, 4.3, and suppose (26), (29), and
(30) hold. Then, for any vector w = (wx, wλ) ∈ Rd+m such that wT Ξtw (cid:54)= 0, for any υ > 0 and all
suﬃciently large τ , we have

sup
z∈R

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18)√

(cid:32) (cid:112)1/ϕt · wT (xt − x(cid:63), λt − λ(cid:63))

(cid:33)

≤ z

− P (N (0, 1) ≤ z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:112)

wT Ξtw
(cid:32)

(cid:19)(cid:19)

= O

χt
ϕ3/2
t
The above result also holds if ωt = 0 for all suﬃciently large t.

(cid:32) (cid:112)log(1/ϕt)(log t)1+υ
√

(cid:18) 1
ϕt

ϕt log

+ O

+ o

(cid:33)

t

(cid:33)

+ O

(cid:16)(cid:112)log(1/ϕt) · ωt

(cid:17)

+ O(ρτ ).

20

Proof. See Appendix C.11.

(cid:4)

Combining Corollary 4.13 with Theorem 4.11, we see that Berry-Esseen bound of using the co-

variance estimator Ξt only leads to an additional O(ρτ ) term. This is negligible for a large τ .

4.5 A complete understanding of AI-StoSQP

We summarize all results in Sections 3 and 4, and rephrase all conditions (26), (29), and (30) using
the input sequences {βt, χt = ηt − βt}.

Theorem 4.14 (Global and local convergence of AI-StoSQP). The following results of AI-StoSQP
in Algorithm 1 hold. The assumptions are gradually strengthened.
(a) Global convergence. Suppose Assumptions 3.1, 3.2(15a), 3.3 hold; τ satisﬁes (17); and the
input sequence {βt, χt}t satisﬁes

lim
t→∞

t

(cid:18)

1 −

(cid:19)

βt−1
βt

= β ∈ [−1, −

1
2

),

lim
t→∞

tβt = (cid:101)β ∈ (0, ∞],

lim
t→∞

t

(cid:18)

1 −

(cid:19)

χt−1
χt

= χ < −1,

(32)

then (cid:107)∇Lt(cid:107) → 0 as t → ∞ almost surely. Furthermore, if (15d) and Assumption 3.11 hold, then
Kt → K(cid:63) as t → ∞ almost surely.
(b) Local asymptotic convergence rate. If (15d) is strengthen to (15e), and for a constant
υ(cid:48) > 0,

> 0,

2(1 − ρτ ) +

> 0,

{log(1/χt)}1+υ(cid:48)

= O(1/βt),

(33)

2χ − β
(cid:101)β

1 − ρτ +

then, for any υ > 0,

β
(cid:101)β

(cid:13)
(cid:18)xt − x(cid:63)
(cid:13)
(cid:13)
λt − λ(cid:63)
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

= o

(cid:18)(cid:113)

βt {log(1/βt)}1+υ

(cid:107)Kt − K(cid:63)(cid:107) = o

(cid:18)(cid:113)

βt {log(1/βt)}1+υ

(cid:19)

(cid:19)

+ O(χt/βt)

+ O(χt/βt) + ωt

almost surely.

Furthermore, if (15a) is also strengthen to (15b), then

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:18)xt − x(cid:63)
(cid:13)
(cid:13)
λt − λ(cid:63)
(cid:13)
(cid:107)Kt − K(cid:63)(cid:107) = O

= O

(cid:17)
(cid:16)(cid:112)βt log(1/βt)
(cid:17)
(cid:16)(cid:112)βt log(1/βt)

+ O(χt/βt)

+ O(χt/βt) + o

(cid:16)(cid:112)

(log t)1+υ/t

(cid:17)

almost surely.

+ ωt

(c) Asymptotic normality and Berry-Esseen bound. If Assumption 4.3 holds and

lim
t→∞

t

(cid:18)

1 −

(cid:19)

ωt−1
ωt

= ω < 0,

1 − ρτ +

β + 2ω
2 (cid:101)β

> 0,

χ <

3β
2

(34)

(all following results also hold if ωt = 0 for large t; in this case, conditions that involve ωt can be
removed), then

(cid:112)1/βt ·

(cid:19)

(cid:18)xt − x(cid:63)
λt − λ(cid:63)

d−→ N (0, Ξ(cid:63))

21

where Ξ(cid:63) is deﬁned in (28), and for any vector w such that wT Ξ(cid:63)w (cid:54)= 0,

(cid:12)
(cid:12)
(cid:12)
P
(cid:12)
(cid:12)

sup
z∈R

(cid:32) (cid:112)1/βt · wT (xt − x(cid:63), λt − λ(cid:63))

√

wT Ξ(cid:63)w
(cid:19)(cid:19)

(cid:32)

+ O

(cid:33)

+ o

χt
β3/2
t

≤ z

(cid:33)

− P (N (0, 1) ≤ z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:32) (cid:112)log(1/βt)(log t)1+υ
√

t

(cid:18)
(cid:112)βt log

= O

(cid:18) 1
βt

(cid:33)

+ O

(cid:16)(cid:112)log(1/βt) · ωt

(cid:17)

.

(35)

(d) Practical covariance estimator. If (15b) is further strengthen to (15c), then we can employ
the estimator Ξt in (31), and a similar Berry-Esseen bound holds if we replace wT Ξ(cid:63)w by wT Ξtw—
only an additional O(ρτ ) term appears on the right hand side of (35).

Proof. See Appendix C.12.

(cid:4)

We emphasize that the asymptotic convergence rate requires Assumptions 3.1, 3.2(15a, 15e), 3.3,
3.11, while the asymptotic normality and Berry-Esseen bound require us to impose Assumption 4.3
and strengthen (15a) to (15b), although the asymptotic rate is also improved under stronger condi-
tions. To adopt the covariance estimator Ξt, (15b) need to be further strengthen to (15c). However,
even for the most stringent conditions (15c) and (15e), they are quite common in the literature (Chen
et al., 2020), and they enable wide applications including constrained least squares and constrained
logistic regression.

We let βt, χt be polynomial in t (which is convenient for implementation), and discuss the

conditions (32), (33), (34) on the input sequence {βt, χt}t.
Lemma 4.15. Suppose βt = c1/tc2 and χt = βc3
(a) (32) is satisﬁed if c1 > 0, c2 ∈ (0.5, 1], c3 > 1/c2.
(b) (32), (33) are satisﬁed if

t . Then,

c2 = 1, c3 > 1, c1 >

1 ∨ (c3 − 0.5)
1 − ρτ

OR

c2 ∈ (0.5, 1), c1 > 0, c3 > 1/c2.

(c) (32), (33), (34) are satisﬁed if

c2 = 1, c3 > 1.5, c1 >

(0.5 − ω) ∨ (c3 − 0.5)
1 − ρτ

OR

c2 ∈ (0.5, 1), c1 > 0, c3 > 1.5 ∨ 1/c2.

Proof. The result holds immediately by noting that β = −c2, (cid:101)β = c1 if c2 = 1 and ∞ if c2 < 1, and
(cid:4)
χ = −c2c3.

With the setup in Lemma 4.15, the result in Theorem 4.14 can be further simpliﬁed. For instance,
we let c2 ∈ (0.5, 1), c1 > 0, c3 = 2. Then, (32), (33), (34) are satisﬁed. Thus, under Assumptions
3.1, 3.2(15b, 15e), 3.3, 3.11, 4.3, we have

(cid:13)
(cid:18)xt − x(cid:63)
(cid:13)
(cid:13)
λt − λ(cid:63)
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O

(cid:32)(cid:114)

(cid:33)

log t
tc2

and

(cid:32) √

(cid:12)
(cid:12)
(cid:12)
P
(cid:12)
(cid:12)

sup
z∈R

tc2 · wT (xt − x(cid:63), λt − λ(cid:63))
c1wT Ξ(cid:63)w

(cid:112)

≤ z

(cid:33)

(cid:12)
(cid:12)
(cid:12)
− P (N (0, 1) ≤ z)
(cid:12)
(cid:12)

= O

(cid:19)

(cid:18) log t
√
tc2

+ O

(cid:16)(cid:112)log t · ωt

(cid:17)

.

22

The 95% conﬁdence interval of wT (x(cid:63), λ(cid:63)) is given by

wT (xt, λt) ±

1.96
tc2/2

·

(cid:112)

c1wT Ξtw.

(36)

We emphasize that our asymptotic results also hold (and the convergence rate does not change) if
we solve Newton systems exactly, and/or adopt deterministic stepsizes. The covariance matrix for
solving Newton systems exactly is Ξ(cid:63) = Ω(cid:63)/(2 + β/ (cid:101)β). However, it is not clear to us whether the
same results hold when we approximately solve Newton systems via deterministic solvers, such as
with CG and MINRES methods as employed in Curtis et al. (2021b). In that case, Lemma 3.4 does
not hold and signiﬁcant adjustments of the analysis are required.

5 Numerical Experiment

In this section, we provide experimental results of Algorithm 1. We develop a Julia implementation
(Siqueira and Orban, 2020); and we consider solving problems in CUTEst test set (Gould et al., 2014),
in particular using problems BYRDSPHR, HS7, HS48 as examples to validate our main theoretical
results. For each problem, we perform 105 SQP iterations and, in each iteration, we perform 50
randomized Kaczmarz iterations for solving the Newton system. That is, for each t, we let
iid∼ Uniform{e1, . . . , em+d} for j = 1, . . . , 50, and perform the iteration (6) with zt,0 = 0. The
St,j
true solution of each problem is solved by applying IPOPT solver (W¨achter and Biegler, 2006). See
a Julia implementation of the solver at https://github.com/jump-dev/Ipopt.jl.

Given an iterate xt, we generate ¯gt ∼ N (∇ft +σ2(I +11T )) where 1 ∈ Rd is an all-one vector. We
also generate the (i, j) and (j, i) entries of ¯Ht from N ((∇2ft)i,j, σ2). Here, ∇ft and ∇2ft are provided
by the package of CUTEst. Among the considered problems, we vary σ2 ∈ {10−8, 10−4, 10−2, 10−1, 1},
and vary βt ∈ {2/t0.5, 2/t0.6}. We also let χt = β2
t and ηt = βt + χt, and randomly pick the stepsize
¯αt ∼ Uniform([βt, ηt]). Here, we let βt decay slower to favor larger stepsizes. We note that, although
βt = 2/t0.5 is not allowed in our asymptotic analysis (see Lemma 4.15), we still implement it to
investigate the performance of this setup. In addition, for the Hessian modiﬁcation, we let

∆t = (−λmin(ZT
t

t−1
(cid:88)

i=0

¯∇2

xLiZt)/t + 0.1) · I whenever λmin(ZT
t

t−1
(cid:88)

i=0

¯∇2

xLiZt) < 0.

Here, Zt ∈ Rd×(d−m) has orthogonal columns that span the space {x ∈ Rd : Gtx = 0}, which is ob-
tained by QR decomposition in our implementation. Our code for implementing other problems in
CUTEst test set is publicly available at https://github.com/senna1128/Inf-StoSQP.

Convergence behavior. We ﬁrst study the convergence behavior of Algorithm 1. Figures 1 and 2
show the convergence plots of the KKT residual (cid:107)∇Lt(cid:107), the iteration error (cid:107)(xt − x(cid:63), λt − λ(cid:63))(cid:107), and
the Hessian error (cid:107)Kt − K(cid:63)(cid:107). We note that, by the Lipschitz continuity of the Hessian, Theorem
4.14 implies that the KKT residual (cid:107)∇Lt(cid:107) has (at least) the same asymptotic convergence rate as
the iterate, that is O((cid:112)βt log(1/βt)) in our experiments.

From Figure 1, we observe that the setting βt = 2/t0.5 works well on problems HS7, HS48, while
it does not work well on BYRDSPHR in terms of the iteration and the Hessian errors (see Figures 1(b)
and 1(c)). However, the KKT residual still converges to a level below the theoretical prediction (see
Figure 1(a)). Thus, under this setup, Algorithm 1 converges to a stationary point that is diﬀerent
from the one returned by IPOPT. The sequence βt = 2/t0.6 enhances the performance of Algorithm 1

23

on BYRDSPHR for both the iteration and the Hessian errors. From all ﬁgures in Figures 1 and 2 (we
should focus on the KKT residual plots in Figures 1(a) and 2(a) for BYRDSPHR), we see that the
theoretical convergence rate precisely catches the asymptotic behavior of Algorithm 1.

Asymptotic normality. We next study the asymptotic normality of the iterate (xt, λt). To visu-
alize the results, we only plot the histograms for [xt]1 − x(cid:63)
1; that is we only focus on
the ﬁrst coordinate of the primal and dual solutions. We regard the ﬁrst half of the iterations as
burn-in period; thus we only plot the errors for the second half of the iterations. For each histogram,
we ﬁt a normal density by matching the mean and variance. Figures 3, 4, 5 show the histograms of
three problems BYRDSPHR, HS7, HS48, respectively.

1 and [λt]1 − λ(cid:63)

From Figure 5 (i.e. histograms of problem HS48), we see that all iteration errors with both setups
of βt = 2/t0.5 and βt = 2/t0.6 follow a normal distribution with mean around zero. This is consistent
with our theorem. From Figure 4, we see that the histograms of [xt]1 − x(cid:63)
1 under
the setup of βt = 2/t0.6 and σ2 = 10−1 do not ﬁt a normal density (see Figures 4(n) and 4(s)). In
fact, from Figures 2(d) and 2(e), we know that (xt, λt) (cid:57) (x(cid:63), λ(cid:63)) but (cid:107)∇Lt(cid:107) → 0 under this setup.
Thus, the scheme converges to a stationary point that diﬀers from the one given by IPOPT. Similar
situations happen for few cases in Figure 3. In particular, Figure 3(r) suggests that [λt]1 (cid:57) λ(cid:63)
1 with
βt = 2/t0.6 and σ2 = 10−2, which is consistent with Figure 2(b). However, Figure 2(a) suggests that
(cid:107)∇Lt(cid:107) → 0 in this case. Overall, the histograms in Figures 3, 4, 5 suggest that the iterate (xt, λt)
follows a normal distribution with mean (x(cid:63), λ(cid:63)) when it converges to a local solution.

1 and [λt]1 − λ(cid:63)

Conﬁdence interval. We ﬁnally test the eﬀectiveness of the proposed covariance estimator. We
construct 95% conﬁdence interval for the solution (x(cid:63), λ(cid:63)). We take x(cid:63)
1 as an example. The
formula of 95% conﬁdence interval is given by (36), where Ξt is estimated by Lemma 4.12, and
w = (1, 0, . . . , 0, 1, 0 . . . , 0), i.e., w1 = wd+1 = 1 and all other entries are zero.

1 + λ(cid:63)

Figure 6 shows the conﬁdence intervals generated by the last 100 iterations. We observe from
Figures 6(u)-6(ad) that the conﬁdence intervals cover the true value of problem HS48 for all σ2
varying from 10−8 to 1, under both setups of βt = 2/t0.5 and 2/t0.6. For problem HS7, we observe
from Figures 6(k)-6(t) that the conﬁdence intervals also cover the true value, except for one setup
where βt = 2/t0.6 and σ2 = 10−1. As explained, the failure of this setup is because (xt, λt) does
not converge to the solution returned by IPOPT. For problem BYRDSPHR, we observe from Figures
6(a)-6(j) that the conﬁdence intervals are improved signiﬁcantly from βt = 2/t0.5 to βt = 2/t0.6.
The conﬁdence intervals with the former sequence only cover the true value when σ2 = 10−2, while
the conﬁdence intervals with the latter sequence cover the true value except when σ2 = 10−2.

We also observe from Figure 6 that the power of our conﬁdence interval construction is signiﬁcant.
In particular, the length of intervals varies from 10−3 to 10−1 when σ2 varies from 10−8 to 1; and
such a small length implies a high statistical power, which enables a precise inference for the solution
in practice (the solution of the optimization problem is often the true model parameter, and the
inference of the model parameter is an important statistical problem).

6 Conclusion

We investigated a stochastic sequential quadratic programming framework called AI-StoSQP. In
particular, we allow the scheme to employ any method to select random stepsizes within an interval
[βt, ηt]; and we allow the scheme to solve Newton systems via randomized iterative solvers. For such
a framework, we established the asymptotic convergence rate and asymptotic normality for the

24

iterates. Under suitable conditions and with proper decaying βt and ηt, we showed that the KKT
residual (cid:107)∇Lt(cid:107) converges to zero almost surely. Moreover, if the sequence converges to a strict
local minimum satisfying constraint qualiﬁcations, then the last iterate (xt, λt) has a local rate
O((cid:112)βt log(1/βt)) + O(χt/βt); and (cid:112)1/βt · (xt − x(cid:63), λt − λ(cid:63)) converges to a Gaussian distribution
with a nontrivial covariance matrix. The covariance matrix depends on the decay rate of βt and the
sketching distribution in the randomized solvers. An explicit form of the covariance matrix was
provided. For practical purpose, we also provided a practical estimator for the covariance matrix.
Our analysis answered an important open problem—what is the (local) asymptotic behavior of
StoSQP iterates under the fully stochastic setup. Our analysis precisely characterized the uncertainty
of the Markov process generated by StoSQP schemes, which consists of the randomness of sample,
randomness of solver, and randomness of stepsize. Since our analysis is independent of the choice of
merit functions, we know that designing a StoSQP scheme with diﬀerentiable merit functions under
the fully stochastic setup does not bring advantages over non-diﬀerentiable merit functions (such
as the (cid:96)1 merit function), in terms of the local rate. However, our analysis does not allow one to
select the stepsize by doing stochastic line search. The question still remains open whether adopting
diﬀerentiable merit functions in stochastic line search can overcome the Maratos eﬀect, lead to
a faster local rate, and bring advantages over non-diﬀerentiable merit functions (as they can in
deterministic line search). Therefore, the local analysis of StoSQP under the stochastic line search
setup deserves studying in the future work.

Acknowledgements

MWM would like to acknowledge the DOE, NSF, and ONR as well as a J. P. Morgan Chase Faculty
Research Award for providing partial support of this work.

References

S. Ahmed and A. Shapiro. Solving chance-constrained stochastic programs via sampling and integer
programming. In State-of-the-Art Decision-Making Tools in the Information-Intensive Age, pages
261–269. INFORMS, 2008.

A. S. Berahas, F. E. Curtis, M. J. O’Neill, and D. P. Robinson. A stochastic sequential quadratic op-
timization algorithm for nonlinear equality constrained optimization with rank-deﬁcient jacobians.
arXiv preprint arXiv:2106.13015, 2021a.

A. S. Berahas, F. E. Curtis, D. Robinson, and B. Zhou. Sequential quadratic optimization for
nonlinear equality constrained stochastic optimization. SIAM Journal on Optimization, 31(2):
1352–1379, 2021b.

A. S. Berahas, J. Shi, Z. Yi, and B. Zhou. Accelerating stochastic sequential quadratic program-
ming for equality constrained optimization using predictive variance reduction. arXiv preprint
arXiv:2204.04161, 2022.

B. Bercu, A. Godichon, and B. Portier. An eﬃcient stochastic Newton algorithm for parameter
estimation in logistic regressions. SIAM Journal on Control and Optimization, 58(1):348–367,
2020.

25

(a) KKT residual

(b) Iteration error

(c) Hessian error

(row 1) Problem BYRDSPHR

(d) KKT residual

(e) Iteration error

(f) Hessian error

(row 2) Problem HS7

(g) KKT residual

(h) Iteration error

(i) Hessian error

(row 3) Problem HS48

Figure 1: Convergence plots with βt = 2/t0.5. Each row corresponds to a problem, and has three
ﬁgures in a log scale; from the left to the right, they correspond to (cid:107)∇Lt(cid:107) versus t, (cid:107)(xt−x(cid:63), λt−λ(cid:63))(cid:107)
versus t, and (cid:107)Kt − K(cid:63)(cid:107) versus t. Each ﬁgure has 6 lines—5 lines correspond to 5 setups of σ2, and
the red line shows (cid:112)βt log(t) versus t, which is the theoretical asymptotic rate (cf. Theorem 4.14).

26

(a) KKT residual

(b) Iteration error

(c) Hessian error

(row 1) Problem BYRDSPHR

(d) KKT residual

(e) Iteration error

(f) Hessian error

(row 2) Problem HS7

(g) KKT residual

(h) Iteration error

(i) Hessian error

(row 3) Problem HS48

Figure 2: Convergence plots with βt = 2/t0.6. See Figure 1 for the interpretation.

27

(a) σ2 = 10−8

(b) σ2 = 10−4

(c) σ2 = 10−2

(d) σ2 = 10−1

(e) σ2 = 1

(row 1) [xt]1 − x(cid:63)

1 and βt = 2/t0.5

(f) σ2 = 10−8

(g) σ2 = 10−4

(h) σ2 = 10−2

(i) σ2 = 10−1

(j) σ2 = 1

(row 2) [λt]1 − λ(cid:63)

1 and βt = 2/t0.5

(k) σ2 = 10−8

(l) σ2 = 10−4

(m) σ2 = 10−2

(n) σ2 = 10−1

(o) σ2 = 1

(row 3) [xt]1 − x(cid:63)

1 and βt = 2/t0.6

(p) σ2 = 10−8

(q) σ2 = 10−4

(r) σ2 = 10−2

(s) σ2 = 10−1

(t) σ2 = 1

(row 4) [λt]1 − λ(cid:63)

1 and βt = 2/t0.6

Figure 3: Histograms of Problem BYRDSPHR (scaled to the probability density). Each row corresponds
to the histogram of the combination of {[xt]1 − x(cid:63)
1} and βt ∈ {2/t0.5, 2/t0.6}. Each row
has ﬁve ﬁgures, from the left to the right, corresponding to σ2 ∈ {10−8, 10−4, 10−2, 10−1, 1}. The
red line on each ﬁgure is the ﬁtted normal density.

1, [λt]1 − λ(cid:63)

28

(a) σ2 = 10−8

(b) σ2 = 10−4

(c) σ2 = 10−2

(d) σ2 = 10−1

(e) σ2 = 1

(row 1) [xt]1 − x(cid:63)

1 and βt = 2/t0.5

(f) σ2 = 10−8

(g) σ2 = 10−4

(h) σ2 = 10−2

(i) σ2 = 10−1

(j) σ2 = 1

(row 2) [λt]1 − λ(cid:63)

1 and βt = 2/t0.5

(k) σ2 = 10−8

(l) σ2 = 10−4

(m) σ2 = 10−2

(n) σ2 = 10−1

(o) σ2 = 1

(row 3) [xt]1 − x(cid:63)

1 and βt = 2/t0.6

(p) σ2 = 10−8

(q) σ2 = 10−4

(r) σ2 = 10−2

(s) σ2 = 10−1

(t) σ2 = 1

(row 4) [λt]1 − λ(cid:63)

1 and βt = 2/t0.6

Figure 4: Histograms of Problem HS7 (scaled to the probability density). See Figure 3 for the
interpretation.

29

(a) σ2 = 10−8

(b) σ2 = 10−4

(c) σ2 = 10−2

(d) σ2 = 10−1

(e) σ2 = 1

(row 1) [xt]1 − x(cid:63)

1 and βt = 2/t0.5

(f) σ2 = 10−8

(g) σ2 = 10−4

(h) σ2 = 10−2

(i) σ2 = 10−1

(j) σ2 = 1

(row 2) [λt]1 − λ(cid:63)

1 and βt = 2/t0.5

(k) σ2 = 10−8

(l) σ2 = 10−4

(m) σ2 = 10−2

(n) σ2 = 10−1

(o) σ2 = 1

(row 3) [xt]1 − x(cid:63)

1 and βt = 2/t0.6

(p) σ2 = 10−8

(q) σ2 = 10−4

(r) σ2 = 10−2

(s) σ2 = 10−1

(t) σ2 = 1

(row 4) [λt]1 − λ(cid:63)

1 and βt = 2/t0.6

Figure 5: Histograms of Problem HS48 (scaled to the probability density). See Figure 3 for the
interpretation.

30

(a) σ2 = 10−8

(b) σ2 = 10−4
(c) σ2 = 10−2
(d) σ2 = 10−1
(row 1) Problem BYRDSPHR with βt = 2/t0.5

(e) σ2 = 1

(f) σ2 = 10−8

(g) σ2 = 10−4
(h) σ2 = 10−2
(i) σ2 = 10−1
(row 2) Problem BYRDSPHR with βt = 2/t0.6

(j) σ2 = 1

(k) σ2 = 10−8

(p) σ2 = 10−8

(u) σ2 = 10−8

(z) σ2 = 10−8

(l) σ2 = 10−4

(m) σ2 = 10−2
(row 3) Problem HS7 with βt = 2/t0.5

(n) σ2 = 10−1

(q) σ2 = 10−4

(r) σ2 = 10−2
(row 4) Problem HS7 with βt = 2/t0.6

(s) σ2 = 10−1

(v) σ2 = 10−4

(w) σ2 = 10−2
(row 5) Problem HS48 with βt = 2/t0.5

(x) σ2 = 10−1

(aa) σ2 = 10−4

(ab) σ2 = 10−2
(row 6) Problem HS48 with βt = 2/t0.6

(ac) σ2 = 10−1

(o) σ2 = 1

(t) σ2 = 1

(y) σ2 = 1

(ad) σ2 = 1

Figure 6: 95% conﬁdence intervals of x(cid:63)
1. The ﬁrst two rows correspond to BYRDSPHR; the
middle two rows correspond to HS7; the last two rows correspond to HS48. Each row has ﬁve ﬁgures,
from the left to the right, corresponding to σ2 ∈ {10−8, 10−4, 10−2, 10−1, 1}. Each ﬁgure has three
lines. The blue and green lines correspond to the lower and upper interval boundaries, and the red
line corresponds to the true value x(cid:63)

1 + λ(cid:63)

1 + λ(cid:63)
1.

31

D. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Elsevier, Belmont, Mass,

1982.

J. R. Birge. State-of-the-art-survey—stochastic programming: Computation and applications.

INFORMS Journal on Computing, 9(2):111–133, 1997.

C. Boyer and A. Godichon-Baggioni. On the asymptotic rate of convergence of stochastic Newton

algorithms and their weighted averaged versions. arXiv preprint arXiv:2011.09706, 2020.

C. Chen, F. Tung, N. Vedula, and G. Mori. Constraint-aware deep neural network compression. In

Computer Vision – ECCV 2018, pages 409–424. Springer International Publishing, 2018.

X. Chen, J. D. Lee, X. T. Tong, and Y. Zhang. Statistical inference for model parameters in

stochastic gradient descent. The Annals of Statistics, 48(1):251–273, 2020.

X. Chen, W. Liu, and Y. Zhang. First-order Newton-type estimator for distributed estimation and

inference. Journal of the American Statistical Association, pages 1–17, 2021.

F. E. Curtis, M. J. O’Neill, and D. P. Robinson. Worst-case complexity of an sqp method for
nonlinear equality constrained stochastic optimization. arXiv preprint arXiv:2112.14799, 2021a.

F. E. Curtis, D. P. Robinson, and B. Zhou. Inexact sequential quadratic optimization for minimizing
a stochastic objective function subject to deterministic nonlinear equality constraints. arXiv
preprint arXiv:2107.03512, 2021b.

M. Duﬂo. Random iterative models, volume 34. Springer, Berlin New York, 1997.

R. Durrett. Probability, volume 49. Cambridge University Press, 2019.

X. Fan. Exact rates of convergence in some martingale central limit theorems. Journal of Mathe-

matical Analysis and Applications, 469(2):1028–1044, 2019.

N. I. M. Gould, D. Orban, and P. L. Toint. CUTEst: a constrained and unconstrained testing
environment with safe threads for mathematical optimization. Computational Optimization and
Applications, 60(3):545–557, 2014.

R. M. Gower and P. Richt´arik. Randomized iterative methods for linear systems. SIAM Journal on

Matrix Analysis and Applications, 36(4):1660–1690, 2015.

A. J. Kleywegt, A. Shapiro, and T. H. de Mello. The sample average approximation method for

stochastic discrete optimization. SIAM Journal on Optimization, 12(2):479–502, 2002.

X. Liu and Y. Yuan. A sequential quadratic programming method without a penalty function or
a ﬁlter for nonlinear equality constrained optimization. SIAM Journal on Optimization, 21(2):
545–571, 2011.

S. Lucidi. Recursive quadratic programming algorithm that uses an exact augmented lagrangian

function. Journal of Optimization Theory and Applications, 67(2):227–245, 1990.

S. Na. Global convergence of online optimization for nonlinear model predictive control. Advances

in Neural Information Processing Systems, 34, 2021.

32

S. Na, M. Anitescu, and M. Kolar. An adaptive stochastic sequential quadratic programming with

diﬀerentiable exact augmented lagrangians. arXiv preprint arXiv:2102.05320, 2021a.

S. Na, M. Anitescu, and M. Kolar. Inequality constrained stochastic nonlinear optimization via

active-set sequential quadratic programming. arXiv preprint arXiv:2109.11502, 2021b.

S. Na, M. Anitescu, and M. Kolar. A fast temporal decomposition procedure for long-horizon

nonlinear dynamic programming. arXiv preprint arXiv:2107.11560, 2021c.

S. Na, M. Derezi´nski, and M. W. Mahoney. Hessian averaging in stochastic Newton methods

achieves superlinear convergence. arXiv preprint arXiv:2204.09266, 2022.

J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research and

Financial Engineering. Springer New York, 2nd edition, 2006.

A. E. Onuk, M. Akcakaya, J. P. Bardhan, D. Erdogmus, D. H. Brooks, and L. Makowski. Constrained
maximum likelihood estimation of relative abundances of protein conformation in a heterogeneous
mixture from small angle x-ray scattering intensity measurements. IEEE Transactions on Signal
Processing, 63(20):5383–5394, 2015.

F. Oztoprak, R. Byrd, and J. Nocedal. Constrained optimization in the presence of noise. arXiv

preprint arXiv:2110.04355, 2021.

G. C. Pﬂug and A. Pichler. Multistage Stochastic Optimization, volume 1104. Springer International

Publishing, 2014.

G. D. Pillo and L. Grippo. A new class of augmented lagrangians in nonlinear programming. SIAM

Journal on Control and Optimization, 17(5):618–628, 1979.

B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM

Journal on Control and Optimization, 30(4):838–855, 1992.

T. Rees, H. S. Dollar, and A. J. Wathen. Optimal solvers for PDE-constrained optimization. SIAM

Journal on Scientiﬁc Computing, 32(1):271–298, 2010.

H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales

and some applications. In Optimizing Methods in Statistics, pages 233–257. Elsevier, 1971.

A. Ruszczy´nski and A. Shapiro. Stochastic programming models. Handbooks in operations research

and management science, 10:1–64, 2003.

A. Shapiro. Asymptotic behavior of optimal solutions in stochastic programming. Mathematics of

Operations Research, 18(4):829–845, 1993.

A. Shapiro, D. Dentcheva, and A. Ruszczy´nski. Lectures on Stochastic Programming: Modeling and

Theory, Second Edition. Society for Industrial and Applied Mathematics, 2014.

A. S. Siqueira and D. Orban. Cutest.jl. https://github.com/JuliaSmoothOptimizers/CUTEst.jl,

2020.

T. Strohmer and R. Vershynin. A randomized kaczmarz algorithm with exponential convergence.

Journal of Fourier Analysis and Applications, 15(2):262–278, 2008.

33

S. Sun and J. Nocedal. A trust region method for the optimization of noisy functions. arXiv preprint

arXiv:2201.00973, 2022.

W. Sun and Y.-X. Yuan. Optimization Theory and Methods, volume 1. Kluwer Academic Publishers,

2006.

A. W¨achter and L. T. Biegler. On the implementation of an interior-point ﬁlter line-search algorithm

for large-scale nonlinear programming. Math. Program., 106(1, Ser. A):25–57, 2006.

J.-G. Wang. The asymptotic behavior of locally square integrable martingales. The Annals of

Probability, 23(2):552–585, 1995.

P.-˚A. Wedin. Perturbation bounds in connection with singular value decomposition. BIT, 12(1):

99–111, 1972.

P.-˚A. Wedin. Perturbation theory for pseudo-inverses. BIT, 13(2):217–232, 1973.

W. Zhu, X. Chen, and W. B. Wu. Online covariance matrix estimation in stochastic gradient descent.

Journal of the American Statistical Association, pages 1–30, 2021.

A Auxiliary Lemmas

Lemma A.1. Suppose {ϕi}i is a positive sequence that satisﬁes lim
i→∞
for any p ≥ 0, we have

i(1 − ϕi−1/ϕi) = ϕ. Then,

Proof. By the condition, we know

lim
i→∞

i

1 −

(cid:18)

(cid:19)

ϕp
i−1
ϕp
i

= p · ϕ.

ϕi−1
ϕi

= 1 −

Thus, we have

(cid:18)

i

1 −

ϕp
i−1
ϕp
i

(cid:19)

(cid:18)

(cid:26)

= i

1 −

1 −

This completes the proof.

ϕ
i

ϕ
i

+ o

(cid:19)

.

(cid:18) 1
i

(cid:19)(cid:27)p(cid:19)

+ o

(cid:18) 1
i

= pϕ + o(1).

(cid:4)

Lemma A.2. Let {ϕi}i be a positive sequence. If lim
i→∞
i → ∞.

i(1 − ϕi−1/ϕi) = ϕ < 0, then ϕi → 0 as

Proof. By Lemma A.1, we know for any positive constant p,

lim
i→∞

i

(cid:18)

1 −

(cid:19)

ϕp
i−1
ϕp
i

= p · ϕ.

Choosing p large enough such that pϕ < −1, the Raabe’s test indicates that (cid:80)∞
implies ϕi → 0 and we complete the proof.

i=0 ϕp

i < ∞. This
(cid:4)

34

Lemma A.3. Let {φi}i, {ϕi}i, {σi}i be three positive sequences. Suppose we have

lim
i→∞

i

(cid:18)

1 −

(cid:19)

φi−1
φi

= φ,

lim
i→∞

ϕi = 0,

lim
i→∞

iϕi = (cid:101)ϕ

(A.1)

for a constant φ and a (possibly inﬁnite) constant (cid:101)ϕ ∈ (0, ∞]. For any l ≥ 1, if we further have

then the following results hold as t → ∞

l
(cid:88)

k=1

σk +

> 0,

φ
(cid:101)ϕ

t
(cid:88)

t
(cid:89)

l
(cid:89)

(1 − ϕjσk)ϕiφi −→

1
φt

1
φt

i=0





j=i+1

k=1

t
(cid:88)

t
(cid:89)

l
(cid:89)

(cid:80)l

,

1
k=1 σk + φ/ (cid:101)ϕ
l
(cid:89)

t
(cid:89)

(1 − ϕjσk)ϕiφiai + b ·

(1 − ϕjσk)

i=0

j=i+1

k=1

j=0

k=1

(A.2)






−→ 0,

where the second result holds for any constant b and any sequence {at}t such that at → 0.

Proof. For any scalar A, we have

1
φt

t
(cid:88)

t
(cid:89)

l
(cid:89)

(1 − ϕjσk)ϕiφi − A

i=0

j=i+1

k=1




t
(cid:88)

i
(cid:89)

l
(cid:89)



i=0

j=0

k=1

(1 − ϕjσk)−1ϕiφi − Aφt

t
(cid:89)

l
(cid:89)

(1 − ϕjσk)−1

j=0

k=1






.

=

1
φt

t
(cid:89)

l
(cid:89)

(1 − ϕjσk)

j=0

k=1

For the last term, we have

t
(cid:89)

l
(cid:89)

Aφt

(1 − ϕjσk)−1

j=0

k=1


Aφi

=

=

t
(cid:88)

i=1

t
(cid:88)

i=1

i
(cid:89)

l
(cid:89)

(1 − ϕjσk)−1 − Aφi−1

i−1
(cid:89)

l
(cid:89)



(1 − ϕjσk)−1

 + Aφ0

j=0

k=1

j=0

k=1

l
(cid:89)

(1 − ϕ0σk)−1

k=1

i
(cid:89)

l
(cid:89)

Aφi

(cid:40)

(1 − ϕjσk)−1

1 −

j=0

k=1

φi−1
φi

l
(cid:89)

(cid:41)

(1 − ϕiσk)

+ Aφ0

l
(cid:89)

(1 − ϕ0σk)−1.

k=1

k=1

Combining the above two displays, we obtain

1
φt

t
(cid:88)

t
(cid:89)

l
(cid:89)

(1 − ϕjσk)ϕiφi − A

i=0

j=i+1

k=1

=

1
φt

t
(cid:89)

l
(cid:89)

(cid:26) t

(cid:88)

(1 − ϕjσk)

i
(cid:89)

l
(cid:89)

(cid:40)

(cid:32)

(1 − ϕjσk)−1φi

ϕi − A

1 −

j=0

k=1

i=1

j=0

k=1

l
(cid:89)

+ φ0

(1 − ϕ0σk)−1 (ϕ0 − A)

(cid:27)

.

k=1

35

φi−1
φi

l
(cid:89)

(cid:33)(cid:41)

(1 − ϕiσk)

k=1

(A.3)

We aim to select A such that the middle term in (A.3) is small. By (A.1), we know

φi−1
φi

= 1 −

φ
i

+ o

(cid:19)

(cid:18) 1
i

= 1 −

φ
(cid:101)ϕ

· ϕi + o(ϕi),

where the second equality is due to 1/(iϕi) = 1/ (cid:101)ϕ + o(1) (which is true even if (cid:101)ϕ = ∞). Furthermore,

l
(cid:89)

(1 − ϕiσk) = 1 − ϕi

k=1

l
(cid:88)

k=1

σk + o(ϕi).

Combining the above two displays, we have

l
(cid:89)

(cid:33)

(cid:40)

(cid:18)

(1 − ϕiσk)

= ϕi − A

1 −

1 −

(cid:32)

ϕi − A

1 −

φi−1
φi

= ϕi − A

(cid:32)

+

φ
(cid:101)ϕ

k=1
l
(cid:88)

k=1

(cid:33)

ϕi + o(ϕi).

σk

(cid:19) (cid:32)

· ϕi + o(ϕi)

1 − ϕi

φ
(cid:101)ϕ

(cid:33)(cid:41)

σk + o(ϕi)

l
(cid:88)

k=1

(A.4)

Thus, we let A = 1/((cid:80)l

k=1 σk + φ/ (cid:101)ϕ) and (A.3) leads to

1
φt

t
(cid:88)

t
(cid:89)

l
(cid:89)

(1 − ϕjσk)ϕiφi −

i=0

j=i+1

k=1

(cid:80)l

1
k=1 σk + φ/ (cid:101)ϕ
(cid:26) t

(cid:88)

i
(cid:89)

l
(cid:89)

=

1
φt

t
(cid:89)

l
(cid:89)

j=0

k=1

l
(cid:89)

+ φ0

k=1

(1 − ϕjσk)

(1 − ϕjσk)−1φi · o(ϕi)

j=0

k=1

i=1
(cid:32)

(1 − ϕ0σk)−1

ϕ0 −

1
k=1 σk + φ/ (cid:101)ϕ

(cid:80)l

(cid:33) (cid:27)
.

Comparing the above display with (A.2), we notice that the ﬁrst result in (A.2) is implied by the
second result. Thus, it suﬃces to prove the second result. We deﬁne




t
(cid:88)

t
(cid:89)

l
(cid:89)

Ψt =

1
φt



(1 − ϕjσk)ϕiφiai + b ·

t
(cid:89)

l
(cid:89)

(1 − ϕjσk)

,

(A.5)

i=0

j=i+1

k=1

j=0

k=1






then

Ψt =

1
φt






ϕtφtat +

l
(cid:89)

(1 − ϕtσk)





t−1
(cid:88)

t−1
(cid:89)

l
(cid:89)

(1 − ϕjσk)ϕiφiai + b ·

t−1
(cid:89)

l
(cid:89)



(1 − ϕjσk)



k=1

i=0

j=i+1

k=1

j=0

k=1






(A.5)
=

φt−1
φt

l
(cid:89)

(1 − ϕtσk)Ψt−1 + ϕtat.

k=1

By (A.4), we know that

φt−1
φt

l
(cid:89)

(1 − ϕtσk) = 1 −

k=1

(cid:32)

+

φ
(cid:101)ϕ

l
(cid:88)

k=1

(cid:33)

σk

· ϕt + o(ϕt).

36

Since (cid:80)l

k=1 σk + φ/ (cid:101)ϕ > 0, we immediately know that for a constant c > 0 and for all large enough t,

|Ψt| ≤ (1 − cϕt)|Ψt−1| + ϕt|at|.

Let t1 be a ﬁxed integer. We apply the above inequality recursively and have for any t ≥ t1 + 1,

t
(cid:89)

|Ψt| ≤

(1 − cϕi)|Ψt1| +

t
(cid:88)

t
(cid:89)

(1 − cϕj)ϕi|ai|.

i=t1+1

i=t1+1

j=i+1

For any (cid:15) > 0, since ai → 0, we select t1 such that |ai| ≤ (cid:15), for all i ≥ t1. Then, the above inequality
leads to

t
(cid:89)

|Ψt| ≤

(1 − cϕi)|Ψt1| + (cid:15)

t
(cid:88)

t
(cid:89)

(1 − cϕj)ϕi =

t
(cid:89)

(1 − cϕi)|Ψt1| +

i=t1+1

(cid:32)

≤ |Ψt1| exp

−c

i=t1+1

j=i+1

i=t1+1

t
(cid:88)

(cid:33)

ϕi

+

i=t1+1

(cid:15)
c

.

(cid:8)1 −

(cid:15)
c

t
(cid:89)

(1 − cϕj)(cid:9)

j=t1+1

Since nϕi → (cid:101)ϕ ∈ (0, ∞], (cid:80)
|Ψt1| exp (cid:0)−c (cid:80)t
complete the proof.

i=t1+1 ϕi

t ϕt → ∞. Thus, for the above (cid:15) > 0, there exists t2 > t1 such that
(cid:1) ≤ (cid:15)/c, ∀t ≥ t2, which implies |Ψt| ≤ 2(cid:15)/c. This means |Ψt| → 0 and we
(cid:4)

Lemma A.4. For any scalars a, b, we have P (a < N (0, 1) ≤ b) ≤ b − a. Furthermore, if 0 < a ≤ b,
then P (a < N (0, 1) ≤ b) ≤ b/a − 1.

Proof. The ﬁrst part of statement holds naturally due to the fact that the density function of the
standard Gaussian exp(−t2/2)/

2π ≤ 1 for any t ∈ R. Moreover, for 0 < a ≤ b, we have

√

P (a < N (0, 1) ≤ b) =

(cid:90) b

a

1
√
2π

exp(−t2/2) dt ≤

b − a
√
2π

exp(−a2/2)

=

(cid:18) b
a

(cid:19) a
√

− 1

2π

exp(−a2/2) ≤

b
a

− 1,

where the last inequality is due to the fact that a exp(−a2/2) ≤ 1 for all a; the ﬁrst inequality uses
(cid:4)
0 < a ≤ b. This completes the proof.

Lemma A.5. Let At, Bt, Ct be three variables depending on the index t; let Φ(z) = P (N (0, 1) ≤ z)
be the cumulative distribution function of standard Gaussian variable. Suppose for the index t,

|P (At ≤ z) − Φ(z)| ≤ at,

|Bt| ≤ bt,

|Ct| ≤ ct

almost surely

(A.6)

sup
z∈R

where at, bt ≥ 0 and 0 ≤ ct < 1. Then, we have

(cid:12)
(cid:12)
P
(cid:12)
(cid:12)

(cid:18) At + Bt
1 + Ct

√

sup
z∈R

(cid:19)

≤ z

− Φ(z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ at + bt +

ct√

1 − ct

.

37

Proof. We only prove the result for any z > 0. The result of z ≤ 0 can be shown in the same way.
We know from (A.6) that

At − bt
√
1 + ct

≤

At + Bt
√
1 + Ct

≤

At + bt
√
1 − ct

almost surely.

Therefore, we have

At ≤ z(1 − ct)1/2 − bt

(cid:17) (A.6)

≥ Φ(z(1 − ct)1/2 − bt) − at

(cid:16)

− P

z(1 − ct)1/2 < N (0, 1) ≤ z

(cid:17)

− at

(z ≥ 0)

P

(cid:18) At + Bt
1 + Ct

√

= Φ(z) − P

(cid:19)

(cid:19)

(cid:16)

(cid:18) At + bt
1 − ct

√

(cid:16)

= P

≥ P

≤ z
≤ z
z(1 − ct)1/2 − bt < N (0, 1) ≤ z(1 − ct)1/2(cid:17)
(cid:18)

(cid:19)

− 1

− at

(by Lemma A.4)

1
1 − ct

≥ Φ(z) − bt −

≥ Φ(z) − bt −

√
ct√

− at.

1 − ct

On the other hand, we have

P

(cid:18) At + Bt
1 + Ct

√

= Φ(z) + P
(cid:16)

≤ Φ(z) +

(cid:19)

(cid:19)

(cid:18) At − bt
1 + ct

≤ z

≤ P

√
≤ z
z < N (0, 1) ≤ z(1 + ct)1/2(cid:17)
(cid:17)

(cid:16)

(1 + ct)1/2 − 1

+ bt + at

(by Lemma A.4)

= P

(cid:16)

At ≤ z(1 + ct)1/2 + bt

(cid:17) (A.6)

≤ Φ(z(1 + ct)1/2 + bt) + at

(cid:16)

+ P

z(1 + ct)1/2 < N (0, 1) ≤ z(1 + ct)1/2 + bt

(cid:17)

+ at

≤ Φ(z) + ct + bt + at.

Combining the above two displays completes the proof.

(cid:4)

B Proofs of Section 3

B.1 Proof of Lemma 3.5

By (11) and the deﬁnition of (∆xt, ∆λt), we have

(cid:19)

(cid:19)T (cid:18)∆xt
∆λt
(cid:19)T (cid:18)I + ν∇2
νGt

xLt µGT
t
I

(cid:19) (cid:18)∇xLt

(cid:19)

(cid:19)T (cid:18)I + ν∇2
νGt

xLt µGT
t
I

(cid:19)T (cid:18)Bt + ν∇2

xLtBt + µGT

Gt + νGtBt

ct
(cid:19) (cid:18)Bt GT
t
Gt
0
t Gt GT

(11)
=

µ,ν

µ,ν

(cid:18)∇xLt
∇λLt
(cid:18)∆xt
∆λt
(cid:18)∆xt
∆λt
(cid:18)∆xt
∆λt

(3)
= −

= −

(13)
≤ −∆xT

(cid:19)

(cid:19) (cid:18)∆xt
∆λt
t + ν∇2
xLtGT
t
νGtGT
t

(cid:19)

(cid:19) (cid:18)∆xt
∆λt

38

t Bt∆xt + νΥBΥu(cid:107)∆xt(cid:107)2 − µ(cid:107)Gt∆xt(cid:107)2 − 2∆λT
t ∆λt(cid:107) − ν(cid:107)GT

+ ν(Υu + ΥB)(cid:107)∆xt(cid:107)(cid:107)GT

t ∆λt(cid:107)2

t Gt∆xt

(also use Assumption 3.1)

(3)
≤ −∆xT
ν
2
≤ −∆xT

−

t Bt∆xt + νΥBΥu(cid:107)∆xt(cid:107)2 − µ(cid:107)ct(cid:107)2 + 2cT

t ∆λt +

(cid:107)GT

t ∆λt(cid:107)2

(also use Young’s inequality)

ν(Υu + ΥB)2
2

(cid:107)∆xt(cid:107)2

t Bt∆xt + ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2 − µ(cid:107)ct(cid:107)2 +

8
νγG

(cid:107)ct(cid:107)2 +

νγG
8

(cid:107)∆λt(cid:107)2

(Young’s inequality and Assumption 3.1)

−

νγG
4

(cid:107)∆λt(cid:107)2 −

ν
4

(cid:107)GT

t ∆λt(cid:107)2

(3)
= −∆xT

t Bt∆xt + ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2 −

≤ −∆xT

t Bt∆xt + ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2 −

(cid:18)

µ −

(cid:18)

µ −

(cid:107)ct(cid:107)2 −

(cid:107)ct(cid:107)2 −

(cid:19)

(cid:19)

8
νγG
8
νγG
8
νγG

(cid:19)

νγG
8
νγG
8
νγG
8

ν
4
ν
8
ν
8

(cid:107)∆λt(cid:107)2 −

(cid:107)Bt∆xt + ∇xLt(cid:107)2

(cid:107)∆λt(cid:107)2 −

(cid:107)∇xLt(cid:107)2 +

νΥ2
B
4

(cid:107)∆xt(cid:107)2

≤ −∆xT

t Bt∆xt + 2ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2 −

(cid:18)

µ −

(cid:107)ct(cid:107)2 −

(cid:107)∆λt(cid:107)2 −

(cid:107)∇xLt(cid:107)2,

(B.1)

where the second last inequality uses (cid:107)Bt∆xt + ∇xLt(cid:107)2 ≥ (cid:107)∇xLt(cid:107)2/2 − (cid:107)Bt∆xt(cid:107)2 ≥ (cid:107)∇xLt(cid:107)2/2 −
Υ2

B(cid:107)∆xt(cid:107)2. To further simplify (B.1), we decompose the step ∆xt as

∆xt = ∆ut + ∆vt,

where ∆ut ∈ span(GT

t ) and Gt∆vt = 0.

Then, we have
− ∆xT
= −∆uT
≤ ΥB(cid:107)∆ut(cid:107)2 + 2ΥB(cid:107)∆ut(cid:107)(cid:107)∆vt(cid:107) − γRH (cid:107)∆vt(cid:107)2 + 2ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2

t Bt∆xt + 2ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2
t Bt∆vt − ∆vT

t Bt∆vt + 2ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2

t Bt∆ut − 2∆uT

(Assumption 3.1)

(cid:19)

(cid:107)∆ut(cid:107)2 −

γRH
2

(cid:107)∆vt(cid:107)2 + 2ν(ΥB + Υu)2(cid:107)∆xt(cid:107)2

(Young’s inequality)

2Υ2
B
γRH
2Υ2
B
γRH
2Υ2
B
γRH

(cid:18)

≤

ΥB +

(cid:18)

=

ΥB +

(cid:18)

≤

ΥB +

≤

4Υ2
B
γRH γG

(cid:107)ct(cid:107)2 −

+

+

(cid:19)

(cid:107)∆ut(cid:107)2 −

(cid:16) γRH
2
(cid:19) 1
(cid:16) γRH
γG
2
− 2ν(ΥB + Υu)2(cid:17)

(cid:107)ct(cid:107)2 −

γRH
2
γRH
2
(cid:16) γRH
2

− 2ν(ΥB + Υu)2(cid:17)

(cid:107)∆xt(cid:107)2

− 2ν(ΥB + Υu)2(cid:17)

(cid:107)∆xt(cid:107)2

(cid:107)∆xt(cid:107)2

(use γRH ≤ 1 ≤ ΥB),

= (cid:107)Gt∆xt(cid:107)2 = (cid:107)Gt∆ut(cid:107)2 = (cid:107)GtGT

where the second last inequality uses the fact that ∆ut = GT
(cid:107)ct(cid:107)2 (3)
due to Assumption 3.1. Combining the above display with (B.1), we have
(cid:18)∇xLt
∇λLt

t ∆ ¯ut(cid:107)2 ≥ γG(cid:107)GT

(cid:16) γRH
2

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

−

(cid:19)

µ,ν

µ,ν

≤ −

(cid:19)T (cid:18)∆xt
∆λt

(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)
8
νγG
Thus, the statements hold provided

νγG
8
(cid:18)

µ −

−

−

ν
8

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
4Υ2
B
γRH γG

−

(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)
(cid:19)

−

ν
8

(cid:107)ct(cid:107)2.

t ∆ ¯ut for some vector ∆ ¯ut ∈ Rm, and
t ¯ut(cid:107)2 = γG(cid:107)∆ut(cid:107)2, and the inequality is

− 2ν(ΥB + Υu)2 −

(cid:17)

νγG
8

(cid:107)∆xt(cid:107)2

νγG
8
which is implied by (16) using (γG ∨ γRH ) ≤ 1 ≤ (ΥB ∧ Υu). This completes the proof.

≥ 2ν(ΥB + Υu)2 +

8
νγG

γRH
2

4Υ2
B
γRH γG

µ ≥

and

ν
8

+

+

,

39

B.2 Proof of Lemma 3.6

By (8) and (14), we have

µ,ν ≤ Lt
Lt+1

µ,ν + ¯αt

= Lt

µ,ν + ¯αt

+

Υµ,ν ¯α2
t
2

+

Υµ,ν ¯α2
t
2
(cid:19)
(cid:18)∆xt
∆λt

(cid:19)(cid:13)
(cid:13)
(cid:18) ¯∆xt
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
¯∆λt
(cid:13)
(cid:13)
(cid:18)∇xLt
∇λLt

µ,ν

µ,ν

+ ¯αt

µ,ν

µ,ν

µ,ν

(cid:18)∇xLt
∇λLt
(cid:18)∇xLt
∇λLt
(cid:13)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)

µ,ν
(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:19)

(cid:19)T (cid:18) ¯∆xt
¯∆λt

(cid:19)T

(I + Ct)

,

(cid:19)

(cid:19)T (cid:26)(cid:18) ¯∆xt
¯∆λt

− (I + Ct)

(cid:19)(cid:27)

(cid:18)∆xt
∆λt

(B.2)

where Ct is deﬁned in Lemma 3.4(b). Furthermore, by Lemma 3.5 and Lemma 3.4, we have

(cid:18)∇xLt
∇λLt

µ,ν

µ,ν

(cid:19)T

(I + Ct)

≤ −

(11),(13)
≤ −

≤ −

νγG
8

νγG
8

νγG
8

(cid:40)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)
(cid:40)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)
(cid:40)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

≤ −

(cid:16) νγG
8

− ρτ µΥu

(cid:17)

(cid:19)

+

(cid:18)∆xt
∆λt
(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)
(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)
(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)
(cid:40)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

+

+

2(cid:41)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:41)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:41)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ (cid:107)Ct(cid:107)

(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
∇λLt
(cid:13)

µ,ν

µ,ν

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ ρτ (1 + (2ν + µ)Υu)

+ 2ρτ µΥu

(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)
2(cid:41)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:19)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∆λt
(cid:13)
(cid:13)
(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)
(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

(Lemma 3.5)

(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(Lemma 3.4)

(1 ≤ Υu and 1 + 2ν ≤ µ)

Thus, if ρτ ≤ νγG/(16µΥu), we have

(cid:18)∇xLt
∇λLt

µ,ν

µ,ν

(cid:19)T

(I + Ct)

(cid:19)

(cid:18)∆xt
∆λt

≤ −

νγG
16

(cid:40)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)

2(cid:41)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

(B.3)

Moreover, we deal with the last two terms on the right hand side of (B.2). We note that

E

(cid:19)

(cid:20)(cid:18) ¯∆xt
¯∆λt

| Ft−1

(cid:21)

= E

(cid:20)

E

(cid:19)

(cid:20)(cid:18) ¯∆xt
¯∆λt

(cid:21)

(cid:21)

| Ft−2/3

| Ft−1

(cid:34)

= E

(I + Ct)

(cid:32)

(cid:101)∆xt
(cid:101)∆λt

(cid:33)

(cid:35)

| Ft−1

(Lemma 3.4(b))

(3)
= −(I + Ct)K−1

t

E

(cid:19)

(cid:20)(cid:18) ¯∇xLt
ct

(cid:21)

| Ft−1

= −(I + Ct)K−1

t

(cid:19)

(cid:18)∇xLt
ct

(Assumption 3.2)

(3)
= (I + Ct)

(cid:18)∆xt
∆λt

(cid:19)

,

(B.4)

40

(cid:18)∆xt
∆λt

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:35)

| Ft−1

− (I + Ct)

E

(cid:19)

and
(cid:34)(cid:13)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)

(cid:13)
(cid:32) ¯∆xt − (cid:101)∆xt
(cid:13)
(cid:13)
(cid:13)
¯∆λt − (cid:101)∆λt
(cid:13)


≤ 3E




 + 3E

| Ft−1

(cid:33)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)

(cid:101)∆xt − ∆xt
(cid:101)∆λt − ∆λt

(cid:33)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)


 + 3(cid:107)Ct(cid:107)2

| Ft−1

(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

≤ 3ρτ E



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)

(cid:101)∆xt
(cid:101)∆λt

(cid:33)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)


 + 3E

| Ft−1






 + 3ρ2τ

| Ft−1

(cid:33)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(Lemma 3.4(b,c))

= 3(ρτ + ρ2τ )

(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+ 3(1 + ρτ )E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



(cid:101)∆xt − ∆xt
(cid:101)∆λt − ∆λt



| Ft−1

 (bias-variance decomposition)

(cid:33)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:101)∆xt − ∆xt
(cid:101)∆λt − ∆λt


(cid:32)

(3),(13)

≤ 3(ρτ + ρ2τ )Υ2
≤ 3(1 + ρτ )Υ2

KΥ2
K(ρτ Υ2

u + 3(1 + ρτ )Υ2
K
u + Υm)

Thus, using (B.4) and (B.5), we know

E[(cid:107)¯gt − ∇ft(cid:107)2 | Ft−1]

(also use (cid:107)K−1

t (cid:107) ≤ ΥK)

(Assumption 3.2(15a)).

(cid:19)(cid:27)

(cid:35)

| Ft−1

(cid:34)

E

¯αt

(B.4)
= E

(cid:18)∇xLt
∇λLt
(cid:34)(cid:18)

¯αt −

(cid:19)

µ,ν

µ,ν

µ,ν

− (I + Ct)

(cid:18)∆xt
∆λt
(cid:19)T (cid:26)(cid:18) ¯∆xt
¯∆λt

(cid:19)T (cid:26)(cid:18) ¯∆xt
¯∆λt
(cid:19) (cid:18)∇xLt
∇λLt
(cid:13)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)
(cid:20)(cid:13)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)

βt + ηt
2
(cid:20)(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
∇λLt
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(1 + (2ν + µ)Υu)ΥuE

µ,ν

µ,ν

µ,ν

(cid:19)

(9)
≤

ηt − βt
2

E

(13)
≤

ηt − βt
2

(cid:19)

− (I + Ct)

(cid:19)(cid:27)

(cid:18)∆xt
∆λt

(cid:35)

| Ft−1

− (I + Ct)

| Ft−1

(cid:18)∆xt
∆λt

(cid:19)

− (I + Ct)

(cid:21)

(cid:21)

| Ft−1

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)∆xt
∆λt
(cid:35)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ (ηt − βt)µΥ2
u

(cid:118)
(cid:117)
(cid:117)
(cid:116)E

(cid:19)

(cid:34)(cid:13)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)

− (I + Ct)

(cid:18)∆xt
∆λt

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

| Ft−1

(1 ≤ Υu and 1 + 2ν ≤ µ)

(B.5)
≤ 2µΥKΥ2

u(1 + ρτ )(

Υm ∨ Υu)(ηt − βt) ≤ 4µΥKΥ2
u(

Υm ∨ Υu)(ηt − βt),

(cid:112)

(cid:112)

(B.5)

(B.6)

and

E

(cid:34)(cid:13)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:35)

| Ft−1

(B.4)
=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(I + Ct)

(cid:18)∆xt
∆λt

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(3),(13)

≤ (1 + ρτ )2Υ2

KΥ2

u + E

+ E

(cid:34)(cid:13)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)
(cid:34)(cid:13)
(cid:19)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)

(cid:19)

− (I + Ct)

(cid:18)∆xt
∆λt

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:35)

| Ft−1

− (I + Ct)

(cid:18)∆xt
∆λt

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:35)

| Ft−1

(also use Lemma 3.4(b))

(B.5)
≤ (1 + ρτ )2Υ2

KΥ2

u + 3(1 + ρτ )Υ2

K(ρτ Υ2

u + Υm) ≤ 16Υ2

K(Υ2

u ∨ Υm).

(B.7)

41

Combining (B.7) with (B.6) and (B.3), plugging into (B.2), and using (9), we obtain

E[Lt+1

µ,ν | Ft−1] ≤ Lt

µ,ν −

νγGβt
16

(cid:40)(cid:13)
(cid:18)∆xt
(cid:13)
(cid:13)
∆λt
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:18)∇xLt
(cid:13)
(cid:13)
ct
(cid:13)

2(cid:41)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 4µΥKΥ2
u(

Υm ∨ Υu)(ηt − βt) + 8Υµ,νΥ2

K(Υ2

u ∨ Υm)η2
t .

(cid:112)

This completes the proof.

B.3 Proof of Theorem 3.7

Note that the condition of τ in (17) implies that we can select (µ, ν) to satisfy (16) and have
ρτ ≤ νγG/(16µΥu) with ρ = 1 − γS. Thus, by Lemma 3.6, we have

E[Lt+1

µ,ν − min
X ×Λ

Lµ,ν | Ft−1] ≤ Lt

µ,ν − min
X ×Λ

Lµ,ν −

νγGβt
16

(cid:107)∇Lt(cid:107)2 + (cid:101)Υµ,ν(χt + η2

t ).

By Robbins-Siegmund theorem (see Robbins and Siegmund (1971) or (Duﬂo, 1997, Theorem 1.3.12)),
we immediately obtain (cid:80)
t βt = ∞, we have lim inf t→∞ (cid:107)∇Lt(cid:107) = 0.
Furthermore, we observe that

t βt(cid:107)∇Lt(cid:107)2 < ∞. Since (cid:80)

E

(cid:34)(cid:13)
(cid:18)xt+1 − xt
(cid:13)
(cid:13)
λt+1 − λt
(cid:13)

2(cid:35)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:34)

= E

E

(cid:35)(cid:35)

| Ft−1

(cid:34)(cid:13)
(cid:19)(cid:13)
(cid:18)xt+1 − xt
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
λt+1 − λt
(cid:13)
(cid:13)
(cid:34)(cid:13)
(cid:34)
(cid:18) ¯∆xt
(cid:13)
(cid:13)
¯∆λt
(cid:13)

(8)
≤ η2
t

E

E

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

| Ft−1

(cid:35)(cid:35) (B.7)

≤ 16Υ2

K(Υ2

u ∨ Υm) · η2
t .

(B.8)

Summing over t = 1 to ∞, exchanging the expectation and sum by applying Fubini’s theorem
(Durrett, 2019, Theorem 1.7.2), and noting that (cid:80)

t < ∞, we obtain

t η2

(cid:34) ∞
(cid:88)

E

t=1

(cid:13)
(cid:18)xt+1 − xt
(cid:13)
(cid:13)
λt+1 − λt
(cid:13)

2(cid:35)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

< ∞.

This implies (cid:80)∞
t=1 (cid:107)(xt+1 −xt, λt+1 −λt)(cid:107) < ∞ almost surely and, thus, (cid:107)(xt+1 −xt, λt+1 −λt)(cid:107) → 0
as t → ∞ almost surely. Suppose limt→∞ (cid:107)∇Lt(cid:107) (cid:54)= 0, then lim supt→∞ (cid:107)∇Lt(cid:107) = (cid:15) > 0. Then, there
exist two index sequences {t1,i}i, {t2,i}i with t1,i+1 > t2,i > t1,i, and for all i = 1, 2, . . ., we have

(cid:107)∇Lt1,i(cid:107) ≥ (cid:15)/2,

(cid:107)∇Lj(cid:107) ≥ (cid:15)/3 for j = t1,i + 1, . . . , t2,i − 1,

(cid:107)∇Lti,2(cid:107) < (cid:15)/3.

(B.9)

Since (cid:80)

t βt(cid:107)∇Lt(cid:107)2 < ∞, we know

∞ >

∞
(cid:88)

ti,2−1
(cid:88)

i=1

j=ti,1

βj(cid:107)∇Lj(cid:107)2

(B.9)
≥

(cid:15)2
9

∞
(cid:88)

ti,2−1
(cid:88)

i=1

j=ti,1

βj.

(B.10)

Furthermore, by (B.8), we have

E

(cid:20)(cid:13)
(cid:18)xt2,i − xt1,i
(cid:13)
(cid:13)
λt2,i − λt1,i
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21) (B.8)

≤ 4ΥK(Υu ∨

(9)
= 4ΥK(Υu ∨

ηj

(cid:112)

Υm)




ti,2−1
(cid:88)



j=ti,1

βj +

ti,2−1
(cid:88)

j=ti,1

χj






.

(cid:112)

Υm)

ti,2−1
(cid:88)

j=ti,1

42

Summing over i = 1 to ∞, and noting that (cid:80)∞
χj ≤
i=1
(cid:80)∞
j=1 χj < ∞, we exchange the expectation and sum by applying Fubini’s theorem again. We know
that the sequence {(xt2,i − xt1,i, λt2,i − λt1,i)}i converges to zero as i → ∞ with probability one.
This contradicts with (cid:107)∇Lt1,i(cid:107) ≥ (cid:15)/2 and (cid:107)∇Lti,2(cid:107) < (cid:15)/3 in (B.9). We complete the proof.

βj < ∞ by (B.10) and (cid:80)∞
i=1

(cid:80)ti,2−1
j=ti,1

(cid:80)ti,2−1
j=ti,1

B.4 Proof of Corollary 3.8

Applying Lemma 3.6 and taking full expectation, we know for some constants h1, h2 > 0 that

E[Lt+1

µ,ν ] ≤ E[Lt

µ,ν] − h1βtE[(cid:107)∇Lt(cid:107)2] + h2(χt + η2

t ),

∀t ≥ 0.

Rearranging the inequality and summing over t = 0 to T(cid:15) − 1, we obtain

T(cid:15)−1
(cid:88)

h1

E[(cid:107)∇Lt(cid:107)2] ≤

T(cid:15)−1
(cid:88)

t=0

1
βt

(cid:18)

(E[Lt

µ,ν] − min
X ×Λ

Lµ,ν) − (E[Lt+1

µ,ν ] − min
X ×Λ

(cid:19)

Lµ,ν)

+ h2

T(cid:15)−1
(cid:88)

t=0
E[L0

≤

µ,ν] − minX ×Λ Lµ,ν

β0

+

T(cid:15)−1
(cid:88)

t=1

(cid:18) 1
βt

(cid:19)

−

1
βt−1

(E[Lt

µ,ν] − min
X ×Λ

Lµ,ν) + h2

χt + η2
t
βt

.

t=0
χt + η2
t
βt

T(cid:15)−1
(cid:88)

t=0

Using the fact that minX ×Λ Lµ,ν ≤ E[Lt
minX ×Λ Lµ,ν, we further have

µ,ν] ≤ maxX ×Λ Lµ,ν and denoting ∆Lµ,ν = maxX ×Λ Lµ,ν −

h1

T(cid:15)−1
(cid:88)

t=0

E[(cid:107)∇Lt(cid:107)2] ≤ (∆Lµ,ν ∨ h2)

(cid:40)

(cid:40)

= (∆Lµ,ν ∨ h2)

1
β0

+

T(cid:15)−1
(cid:88)

t=1

(cid:18) 1
βt

−

1
βT(cid:15)−1

+

T(cid:15)−1
(cid:88)

t=0

χt + η2
t
βt

(cid:19)

1
βt−1
(cid:41)

+

T(cid:15)−1
(cid:88)

t=0

χt + η2
t
βt

(cid:41)

(cid:40)

= (∆Lµ,ν ∨ h2)

T a
(cid:15) +

T(cid:15)−1
(cid:88)

t=0

χt + η2
t
βt

(cid:41)

.

For the last term on the right hand side, we have

T(cid:15)−1
(cid:88)

(t + 1)a−b + (t + 1)a (cid:16)
(cid:110)

(t + 1)−2a + 2(t + 1)−(a+b) + (t + 1)−2b(cid:17)(cid:111)

T(cid:15)−1
(cid:88)

t=0

χt + η2
t
βt

=

≤

t=0
T(cid:15)−1
(cid:88)

(t + 1)a−b + 4

t=0

≤ 5 +

(cid:90) T(cid:15)−1

T(cid:15)−1
(cid:88)

(t + 1)−a = 5 +

t=0

T(cid:15)−1
(cid:88)

t=1

(t + 1)a−b + 4(t + 1)−a(cid:111)
(cid:110)

(t + 1)a−b + 4(t + 1)−adt

(by the convexity of xp with p < 0)

(cid:15)

0

5 + T 1+a−b
1+a−b + 4T 1−a

5 + log(T(cid:15)) + 4T 1−a

b−a−1 + 4T 1−a
5 + 1

(cid:15)
1−a

(cid:15)
1−a

(cid:15)
1−a

≤

if 1 + a > b,

if 1 + a = b,

if 1 + a < b.

43

Combining the above two displays, dividing T(cid:15) on both sides, and using “(cid:46)” to neglect constant
factors (i.e., not depending on T(cid:15)), we obtain

(cid:15)2 ≤

(cid:32)

1
T(cid:15)

T(cid:15)−1
(cid:88)

t=0

(cid:33)2

E[(cid:107)∇Lt(cid:107)]

≤

1
T(cid:15)

T(cid:15)−1
(cid:88)

(E[(cid:107)∇Lt(cid:107)])2 ≤

1
T(cid:15)

T(cid:15)−1
(cid:88)

t=0

E[(cid:107)∇Lt(cid:107)2]

+ 1

T b−a
(cid:15)

+ 1
T a
(cid:15)

t=0
if 1 + a > b,










1
T 1−a
(cid:15)
1
T 1−a
(cid:15)
1
T 1−a
(cid:15)
1
T b−a
(cid:15)
1
T 1−a
(cid:15)

(cid:46)

(cid:46)

+ 1
T a
(cid:15)
+ 1
T a
(cid:15)
+ 1
T a
(cid:15)
+ 1
T a
(cid:15)

if 1 + a = b,

if 1 + a < b,

(use 1/T(cid:15) ≤ 1/T a

(cid:15) and log(T(cid:15))/T(cid:15) (cid:46) 1/T a

(cid:15) ),

if 1 > b,

if 1 ≤ b,

=

1
T (1∧b)−a
(cid:15)

+

1
T a
(cid:15)

(cid:46)

1
T a∧(1−a)∧(b−a)
(cid:15)

.

This completes the proof.

B.5 Proof of Theorem 3.12

Using (cid:107)∆t(cid:107) ≤ ωt in Assumption 3.11, we have

(Gt)T − (G(cid:63))T
0

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(2)
≤

(cid:18) 1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:80)t−1
i=0

xLi − ∇2

¯∇2
Gt − G(cid:63)

xL(cid:63)

(Gt)T − (G(cid:63))T
0

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ wt

(cid:107)Kt − K(cid:63)(cid:107)

(4)
=

≤

≤

1
t

¯∇2

(cid:13)
(cid:18)Bt − ∇2
xL(cid:63)
(cid:13)
(cid:13)
Gt − G(cid:63)
(cid:13)
(cid:13)
t−1
(cid:13)
(cid:88)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i=0
t−1
(cid:88)

i=0
t−1
(cid:88)

1
t

1
t

i=0

xLi − ∇2

xL(cid:63)

¯Hi − ∇2fi

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(12)
≤

¯Hi − ∇2fi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

+ (cid:107)Gt − G(cid:63)(cid:107) + ωt

t−1
(cid:88)

i=0

(cid:13)
(cid:13)∇2

xLi − ∇2

xL(cid:63)(cid:13)

(cid:13) + (cid:107)Gt − G(cid:63)(cid:107) + ωt

+

ΥL
t

t−1
(cid:88)

i=0

(cid:13)
(cid:18)xi − x(cid:63)
(cid:13)
(cid:13)
λi − λ(cid:63)
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ ΥL(cid:107)xt − x(cid:63)(cid:107) + ωt.

(B.11)

Since (xt − x(cid:63), λt − λ(cid:63)) → 0 as t → ∞, and by the fact that at → a implies 1
i=0 ai → a for any
t
sequence at (known as Stolz–Ces`aro theorem), it suﬃces to show that ((cid:80)t−1
¯Hi − ∇2fi)/t converges.
i=0
In fact, noting from Assumption 3.2(15d) that E[ ¯Hi | Fi−1] = ∇2fi and E[(cid:107) ¯Hi−∇2fi(cid:107)2 | Fi−1] ≤ Υm,
we know ((cid:80)t−1
¯Hi − ∇2fi)/t is a square integrable martingale. Thus, (Duﬂo, 1997, Theorem 1.3.15)
i=0
suggests that for any υ > 0,

(cid:80)t−1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

t−1
(cid:88)

i=0

¯Hi − ∇2fi

(cid:32)(cid:114)

(log t)1+υ
t

(cid:33)

.

= o

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(B.12)

Combining (B.11) and (B.12), we complete the proof.

44

C Proofs of Section 4

C.1 Proof of Lemma 4.1

Using the scheme in Algorithm 1, we have

(cid:19) (8)
=

(cid:19)

(cid:18)xt − x(cid:63)
λt − λ(cid:63)
(cid:19)
(cid:18) ¯∆xt
¯∆λt

+ ϕt

+ ¯αt

(cid:19)

(cid:18) ¯∆xt
¯∆λt

+ (¯αt − ϕt)

+ ϕt(I + Ct)

(cid:33)

(cid:32)

(cid:101)∆xt
(cid:101)∆λt

+ ϕt

− ϕt(I + Ct)K−1

t

− ϕt(I + Ct)K−1

t

(cid:18) ¯∇xLt
ct
(cid:18)∇xLt
ct
(cid:33)(cid:41)

(cid:19)

(cid:19)

(cid:18) ¯∆xt
¯∆λt
(cid:40)(cid:18) ¯∆xt
¯∆λt
(cid:40)(cid:18) ¯∆xt
¯∆λt

+ ϕt

(cid:19)

(cid:19)

− (I + Ct)

(cid:32)

(cid:101)∆xt
(cid:101)∆λt

+ (¯αt − ϕt)

(cid:19)

(cid:18) ¯∆xt
¯∆λt

− ϕt(I + Ct)K−1

t ∇Lt + ϕtθt + (¯αt − ϕt)

(cid:19)

(cid:18) ¯∆xt
¯∆λt

=

=

(cid:18)xt+1 − x(cid:63)
λt+1 − λ(cid:63)
(cid:18)xt − x(cid:63)
λt − λ(cid:63)
(cid:18)xt − x(cid:63)
λt − λ(cid:63)
(cid:18)xt − x(cid:63)
λt − λ(cid:63)
(cid:18)xt − x(cid:63)
λt − λ(cid:63)

(3)
=

=

(cid:19)

(cid:19)

(cid:19)

(cid:19)

+ ϕt

(cid:40)(cid:18) ¯∆xt
¯∆λt
(cid:19)

(21b)
=

=

(cid:18)xt − x(cid:63)
λt − λ(cid:63)
(cid:19)
(cid:18)xt − x(cid:63)
λt − λ(cid:63)

− (I + Ct)

(cid:33)(cid:41)

(cid:32)

(cid:101)∆xt
(cid:101)∆λt

+ (¯αt − ϕt)

(cid:19)

(cid:18) ¯∆xt
¯∆λt

(cid:19)

− (I + Ct)

(cid:32)

(cid:33)(cid:41)

+ (¯αt − ϕt)

(cid:19)

(cid:18) ¯∆xt
¯∆λt

(cid:101)∆xt
(cid:101)∆λt
(cid:19)

(cid:19)

− ϕt(I + Ct)K−1

t

(cid:18)¯gt − ∇ft
0

− ϕt(I + Ct)(K(cid:63))−1∇Lt − ϕt(I + Ct) (cid:8)K−1

t − (K(cid:63))−1(cid:9) ∇Lt + ϕtθt + (¯αt − ϕt)

(cid:19)

(cid:18) ¯∆xt
¯∆λt

(21d)
= {I − ϕt(I + Ct)}

− ϕt(I + Ct)(K(cid:63))−1ψt − ϕt(I + Ct) (cid:8)K−1

t − (K(cid:63))−1(cid:9) ∇Lt

+ ϕtθt + (¯αt − ϕt)

(21c)
= {I − ϕt(I + C(cid:63))}

(cid:19)

(cid:18)xt − x(cid:63)
λt − λ(cid:63)
(cid:19)
(cid:18) ¯∆xt
¯∆λt
(cid:18)xt − x(cid:63)
λt − λ(cid:63)

(cid:19)

+ ϕt(θt + δt) + (¯αt − ϕt)

(cid:18) ¯∆xt
¯∆λt

(cid:19)

.

We then apply the above result recursively and have

(cid:19)

(cid:18)xt+1 − x(cid:63)
λt+1 − λ(cid:63)

= {I − ϕt(I + C(cid:63))}

(cid:19)

(cid:18)xt − x(cid:63)
λt − λ(cid:63)

+ ϕt(θt + δt) + (¯αt − ϕt)

(cid:19)

(cid:18) ¯∆xt
¯∆λt

= {I − ϕt(I + C(cid:63))}

(cid:26)

{I − ϕt−1(I + C(cid:63))}

(cid:19)

(cid:18)xt−1 − x(cid:63)
λt−1 − λ(cid:63)

+ ϕt−1(θt−1 + δt−1)

+ (¯αt−1 − ϕt−1)

(cid:19) (cid:27)

(cid:18) ¯∆xt−1
¯∆λt−1

= · · · = I1,t + I2,t + I3,t.

+ ϕt(θt + δt) + (¯αt − ϕt)

(cid:19)

(cid:18) ¯∆xt
¯∆λt

45

This shows the ﬁrst part of the result. Moreover, if Assumptions 3.2, 3.3 hold, we know E[¯gi − ∇fi |
Fi−1] = 0 by Assumption 3.2, and

E

(cid:19)

(cid:34)(cid:18) ¯∆xi
¯∆λi

− (I + Ci)

(cid:32)

(cid:101)∆xi
(cid:101)∆λi

(cid:33)

(cid:35)

(cid:34)

| Fi−1

= E

E

(cid:19)

(cid:34)(cid:18) ¯∆xi
¯∆λi

− (I + Ci)

(cid:33)

(cid:32)

(cid:101)∆xi
(cid:101)∆λi

(cid:35)

(cid:35)

| Fi−2/3

| Fi−1

= 0

(C.1)

by Lemma 3.4(b). Thus, E[θi | Fi−1] = 0 and θi is a martingale diﬀerence.

C.2 Proof of Lemma 4.2

Let us denote rank(S) = r in the proof. Since Kt, K(cid:63) have full rank, we know rank(KtS) =
rank(K(cid:63)S) = r. Let K(cid:63)S = EDF T be truncated singular value decomposition of K(cid:63)S. We have

E ∈ R(d+m)×r, F ∈ Rq×r, ET E = F T F = I, D = diag(D1, . . . , Dr) with D1 ≥ . . . ≥ Dr > 0.

Similarly, we let KtS = E(cid:48)D(cid:48)(F (cid:48))T . By direct calculation, we have

(cid:107)KtS(ST K2

t S)†ST Kt − K(cid:63)S(ST (K(cid:63))2S)†ST K(cid:63)(cid:107) = (cid:107)EET − E(cid:48)(E(cid:48))T (cid:107).

(C.2)

Deﬁne the principle angles θp between span(E) and span(E(cid:48)) to be θp = (θp,1, . . . , θp,r), so that ET E(cid:48)
has singular value decomposition ET E(cid:48) = P cos(θp)QT , where P, Q ∈ Rr×r are orthonormal matrices
and cos(θp) = diag(cos(θp,1), . . . , cos(θp,r)) (similar for sin(θp)). We further let E⊥ ∈ R(d+m)×(d+m−r)
be the complement of E, and express E(cid:48) as

E(cid:48) = EA + E⊥B.

(C.3)

Then, ET E(cid:48) = A = P cos(θp)QT and I = (E(cid:48))T E(cid:48) = AT A + BT B. By the above formulation,

(cid:107)EET − E(cid:48)(E(cid:48))T (cid:107)

(C.3)
=

≤

(cid:19) (cid:18) ET

(E⊥)T

(cid:13)
(cid:13)
(E, E⊥)
(cid:13)
(cid:13)
(cid:13)
(cid:18)I − AAT
(cid:13)
(cid:13)
0
(cid:13)

(cid:18)I − AAT −ABT
−BAT −BBT
(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

0
−BBT
≤ max{(cid:107)I − AAT (cid:107), (cid:107)BBT (cid:107)} + (cid:107)ABT (cid:107)
= max{(cid:107)I − AAT (cid:107), (cid:107)I − AT A(cid:107)} + (cid:107)ABT (cid:107)

(cid:13)
(cid:18) 0
(cid:13)
(cid:13)
BAT
(cid:13)

+

ABT
0

=

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:18)I − AAT −ABT
(cid:13)
(cid:13)
−BAT −BBT
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:113)

= (cid:107) sin(θp)(cid:107)2 +
(cid:107)P cos(θp) sin2(θp) cos(θp)P T (cid:107)
= (cid:107) sin(θp)(cid:107)2 + (cid:107) sin(θp) cos(θp)(cid:107) ≤ 2(cid:107) sin(θp)(cid:107).

On the other hand, by Wedin’s sin(Θ) theorem (Wedin, 1972, (3.1)), we know that

(cid:107) sin(θp)(cid:107) ≤

(cid:107)(K(cid:63) − Kt)S(cid:107)
Dr

.

Moreover, we let Fr be the r-th column of F and note that

(C.4)

(C.5)

D2

r = F T

r ST (K(cid:63))2SFr ≥ (σmin(K(cid:63)))2F T

r ST SFr.

46

Since kernel(K(cid:63)S) = kernel(S) and Fr ∈ kernel⊥(K(cid:63)S), we know Fr ∈ kernel⊥(S) = span(ST ). Thus,
r ST SFr ≥ λ+
min(S))2 is the least positive eigenvalue of ST S.
F T
Therefore, we have

min(ST S), where λ+

min(ST S) = (σ+

Combining all above derivations, we have

Dr ≥ σmin(K(cid:63))σ+

min(S).

(C.6)

(cid:107)KtS(ST K2

t S)†ST Kt − K(cid:63)S(ST (K(cid:63))2S)†ST K(cid:63)(cid:107)

(C.2)
= (cid:107)EET − E(cid:48)(E(cid:48))T (cid:107)

(C.4)
≤ 2(cid:107) sin(θp)(cid:107)

(C.5)
≤

2(cid:107)Kt − K(cid:63)(cid:107) · (cid:107)S(cid:107)
Dr

(C.6)
≤

2(cid:107)Kt − K(cid:63)(cid:107)
σmin(K(cid:63))

·

(cid:107)S(cid:107)
σ+
min(S)

.

This completes the proof.

C.3 Proof of Corollary 4.4

Noting that (cid:107)KtS(ST K2
Theorem 1.6.7) and Lemma 4.2, and almost surely have

t S)†ST Kt(cid:107) ≤ 1, we apply dominated convergence theorem (Durrett, 2019,

lim
t→∞

E[KtS(ST K2

t S)†ST Kt | xt, λt] = E[K(cid:63)S(ST (K(cid:63))2S)†ST K(cid:63)],

(cid:107)Ct − C(cid:63)(cid:107) = (cid:107)Aτ

where the expectation is taken over randomness of S only. This shows Ct → C(cid:63). Further, denoting
At = I − E[KtS(ST K2

(At − A(cid:63))(cid:13)

t S)†ST Kt | xt, λt] and A(cid:63) = I − E[K(cid:63)S(ST (K(cid:63))2S)†ST K(cid:63)], we have
t − (A(cid:63))τ (cid:107) ≤ (cid:13)
(cid:13)Aτ −1
t
t − (A(cid:63))τ −1(cid:13)
(cid:13)Aτ −1
(cid:13)
t S)†ST Kt − K(cid:63)S(ST (K(cid:63))2S)†ST K(cid:63)(cid:13)
(cid:104)(cid:13)
(cid:13)KtS(ST K2
(cid:13)
(cid:13)
(cid:13) | xt, λt
(cid:21)
(cid:20) (cid:107)S(cid:107)
2τ ΥS
σ+
σmin(K(cid:63))
min(S)

≤ (cid:107)At − A(cid:63)(cid:107) + (cid:13)
≤ τ (cid:107)At − A(cid:63)(cid:107) ≤ τ E
2τ (cid:107)Kt − K(cid:63)(cid:107)
σmin(K(cid:63))

(cid:13) + (cid:13)
((cid:107)At(cid:107) ∨ (cid:107)A(cid:63)(cid:107) ≤ 1)

t − (A(cid:63))τ −1)A(cid:63)(cid:13)
(cid:13)

(by Assumption 4.3).

(cid:107)Kt − K(cid:63)(cid:107)

(cid:13)(Aτ −1

≤

≤

E

(cid:105)

This completes the proof.

C.4 Proof of Lemma 4.5

We note that

I1,t

(20a)
=

t
(cid:88)

t
(cid:89)

i=0

j=i+1

{I − ϕj(I + C(cid:63))} ϕiθi (22)

= U

t
(cid:88)

t
(cid:89)

i=0

j=i+1

{I − ϕjΣ} ϕiU T θi.

Since E[θi | Fi−1] = 0, I1,t is a martingale. We hence aim to apply the strong law of large number
(Duﬂo, 1997, Theorem 1.3.15), the central limit theorem (Duﬂo, 1997, Corollary 2.1.10), and the
Berry-Esseen bound (Fan, 2019, Theorem 2.1) for martingales to show each result. We ﬁrst charac-
terize the conditional covariance of I1,t deﬁned as (cf. (Duﬂo, 1997, Proposition 1.3.7))

(cid:104)I1(cid:105)t := U

t
(cid:88)

t
(cid:89)

i=0

j=i+1

{I − ϕjΣ} ϕ2

i U T E[θi(θi)T | Fi−1]U (cid:0)

t
(cid:89)

j=i+1

{I − ϕjΣ} (cid:1)T U T .

(C.7)

47

For the term E[θi(θi)T | Fi−1], we have

(cid:20) (cid:40)

(21b)
= E

(I + Ci)K−1

i

(cid:19)

(cid:18)¯gi − ∇fi
0

−

(cid:19)

(cid:40)(cid:18) ¯∆xi
¯∆λi

− (I + Ci)

(cid:33)(cid:41)(cid:41)

(cid:32)

(cid:101)∆xi
(cid:101)∆λi

E[θi(θi)T | Fi−1]

(cid:40)

(I + Ci)K−1

i

(C.1)
= (I + Ci)K−1

i

E

(cid:18)¯gi − ∇fi
0
(cid:34)(cid:18)¯gi − ∇fi
0


(cid:40)(cid:18) ¯∆xi

¯∆λi

(cid:19)

+ E

− (I + Ci)

(cid:32)

(cid:101)∆xi
(cid:101)∆λi

(cid:19)

−

(cid:40)(cid:18) ¯∆xi
¯∆λi
(cid:19) (cid:18)¯gi − ∇fi

(cid:19)T

0
(cid:33)(cid:41) (cid:40)(cid:18) ¯∆xi
¯∆λi

(cid:19)

− (I + Ci)

(cid:32)

(cid:101)∆xi
(cid:101)∆λi

(cid:33)(cid:41)(cid:41)T

(cid:21)

| Fi−1

(cid:35)

| Fi−1

K−1
i

(I + Ci)

(cid:19)

− (I + Ci)

(cid:33)(cid:41)T

(cid:32)

(cid:101)∆xi
(cid:101)∆λi


 =: J1,i + J2,i.(C.8)

| Fi−1

For the term J1,i, we apply Assumption 3.2 and have

E[(¯gi − ∇fi)(¯gi − ∇fi)T | Fi−1] = E[¯gi¯gT
i

| Fi−1] − ∇fi∇T fi.

We also have

i − ∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ) | Fi−1](cid:13)
(cid:13)

(cid:13)
(cid:13)E[¯gi¯gT
≤ 2E [(cid:107)¯gi − ∇f (x(cid:63); ξ)(cid:107) · (cid:107)¯gi(cid:107) | Fi−1] + E (cid:2)(cid:107)¯gi − ∇f (x(cid:63); ξ)(cid:107)2 | Fi−1
≤ 2

(cid:112)E [(cid:107)¯gi − ∇f (x(cid:63); ξ)(cid:107)2 | Fi−1]

(cid:112)E [(cid:107)¯gi(cid:107)2 | Fi−1] + E (cid:2)(cid:107)¯gi − ∇f (x(cid:63); ξ)(cid:107)2 | Fi−1

(cid:3)

(cid:3) ,

and

and

E (cid:2)(cid:107)¯gi − ∇f (x(cid:63); ξ)(cid:107)2 | Fi−1

(cid:3) ≤ E[sup
x∈X

(cid:107)∇2f (x; ξ)(cid:107)2] · (cid:107)xi − x(cid:63)(cid:107)2

(15e)
≤ Υm(cid:107)xi − x(cid:63)(cid:107)2,

E[(cid:107)¯gi(cid:107)2 | Fi−1] = (cid:107)∇fi(cid:107)2 + E[(cid:107)¯gi − ∇fi(cid:107)2 | Fi−1]

(13),(15a)
≤

Υ2

u + Υm ≤ 2(Υ2

u ∨ Υm).

Thus, we combine the above three displays, and have
i − ∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ) | Fi−1](cid:13)

(cid:13)
(cid:13)E[¯gi¯gT

(cid:13) ≤ 2

2(Υ2

√

u ∨ Υm)((cid:107)xi − x(cid:63)(cid:107) + (cid:107)xi − x(cid:63)(cid:107)2) → 0.

(C.9)

Thus, we obtain

E[(¯gi − ∇fi)(¯gi − ∇fi)T | Fi−1] = E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)] − ∇f (x(cid:63))∇T f (x(cid:63)).

(C.10)

lim
i→∞

Further, by Theorem 3.12, we know Ki → K(cid:63) as i → ∞; and Corollary 4.4 shows Ci → C(cid:63). Thus,
we use the deﬁnition (25) and have

J1,i = (I + C(cid:63))Ω(cid:63)(I + C(cid:63)) + O(K1,i)

(C.11)

with K1,i → 0 as i → ∞ almost surely.

48

For the term J2,i, we have (recall from Section 2 that zi,τ = ( ¯∆xi, ¯∆λi) and (cid:101)zi = ( (cid:101)∆xi, (cid:101)∆λi))
J2,i = E (cid:2)(zi,τ − (I + Ci)(cid:101)zi)(zi,τ − (I + Ci)(cid:101)zi)T | Fi−1
(cid:18) ¯∇xLi
ci
(cid:18)¯gi − ∇fi
0

= E[( (cid:101)Ci − Ci)(cid:101)zi (cid:101)zT
(cid:35)

(cid:34)
( (cid:101)Ci − Ci)K−1

(cid:34)
( (cid:101)Ci − Ci)K−1

(cid:19) (cid:18)¯gi − ∇fi

(cid:19) (cid:18) ¯∇xLi

i ) | Fi−1]

i ) | Fi−1

i ) | Fi−1

i − CT

i − CT

i − CT

i ( (cid:101)CT

K−1
i

K−1
i

(3)
= E

( (cid:101)CT

( (cid:101)CT

= E

(cid:3) (7)

(cid:19)T

(cid:19)T

ci

(cid:35)

0

i

i

+ E

+ E

(cid:104)
( (cid:101)Ci − Ci)K−1
(cid:34)

( (cid:101)Ci − Ci)K−1

i

i ∇Li∇T LiK−1
(cid:18)¯gi − ∇fi
0

( (cid:101)CT
(cid:19) (cid:18)∇xLi

i − CT
(cid:19)T

i

ci

(cid:105)

i ) | Fi−1

K−1
i

( (cid:101)CT

i − CT

i ) | Fi−1

(cid:34)

+ E

( (cid:101)Ci − Ci)K−1

i

(cid:18)∇xLi
ci

(cid:19) (cid:18)¯gi − ∇fi

(cid:19)T

0

K−1
i

( (cid:101)CT

i − CT

i ) | Fi−1

(cid:35)

(cid:35)

.

(C.12)

For the last two terms, we apply the tower property of conditional expectation by ﬁrst conditioning on
the randomness of ζi to take expectation over the randomness of ξi, and then taking expectation over
the randomness of ζi. This is allowed even ζi is generated after ξi in the algorithm, because ξi and
ζi are independent. In particular, we have (similar for the second last term in (C.12))

(cid:34)

E

( (cid:101)Ci − Ci)K−1

i

(cid:34)

(cid:34)

= E

= E

( (cid:101)Ci − Ci)K−1

( (cid:101)Ci − Ci)K−1

(cid:18)∇xLi
ci
(cid:18)∇xLi
ci
(cid:18)∇xLi
ci

i

i

(cid:19) (cid:18)¯gi − ∇fi

(cid:19)T

K−1
i

( (cid:101)CT

i − CT

i ) | Fi−1

0

(cid:19) (cid:18)E[¯gi − ∇fi | Fi−1 ∪ ζi]

(cid:19)T

0

K−1
i

(cid:35)

(cid:35)

( (cid:101)CT

i − CT

i ) | Fi−1

(cid:19) (cid:18)E[¯gi − ∇fi | Fi−1]

(cid:19)T

0

K−1
i

( (cid:101)CT

i − CT

i ) | Fi−1

(cid:35)

= 0.

For the second term in (C.12), it converges to zero almost surely as i goes to inﬁnity since
(cid:107) (cid:101)Ci(cid:107) ∨ (cid:107)Ci(cid:107) ≤ 1, (cid:107)K−1

i (cid:107) ≤ ΥK, and ∇Li → 0. For the ﬁrst term in (C.12), we have

(cid:34)

E

( (cid:101)Ci − Ci)K−1

i

(cid:18)¯gi − ∇fi
0

(cid:19) (cid:18)¯gi − ∇fi

(cid:19)T

0

K−1
i

(cid:35)

( (cid:101)CT

i − CT

i ) | Fi−1

= E

(cid:20)
( (cid:101)Ci − Ci)K−1

i

(cid:18)E (cid:2)(¯gi − ∇fi)(¯gi − ∇fi)T | Fi−1 ∪ ζi
0

(cid:19)

(cid:3) 0
0

K−1
i

( (cid:101)CT

i − CT

i ) | Fi−1

(cid:21)

= E

(cid:20)
( (cid:101)Ci − Ci)K−1

i

(cid:18)E (cid:2)(¯gi − ∇fi)(¯gi − ∇fi)T | Fi−1
0

(cid:19)

(cid:3) 0
0

K−1
i

(24)
−→ E[( (cid:101)C(cid:63) − C(cid:63))Ω(cid:63)(( (cid:101)C(cid:63))T − C(cid:63))] = E[ (cid:101)C(cid:63)Ω(cid:63)( (cid:101)C(cid:63))T ] − C(cid:63)Ω(cid:63)C(cid:63).

( (cid:101)CT

i − CT

i ) | Fi−1

(cid:21)

Again, the convergence here is due to the dominated convergence theorem, (C.10), and Ki → K(cid:63);
and the expectation is taken over the randomness of τ sketch matrices S1, . . . , Sτ only. Thus,
combining the above two displays with (C.12), we have

J2,i = E[ (cid:101)C(cid:63)Ω(cid:63)( (cid:101)C(cid:63))T ] − C(cid:63)Ω(cid:63)C(cid:63) + O(K2,i)

(C.13)

49

with K2,i → 0 as i → ∞ almost surely.

Combining (C.13), (C.11), and (C.8), we obtain

E[θi(θi) | Fi−1] = E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ] + O(K1,i + K2,i).

By the deﬁnition of (cid:104)I1(cid:105)t in (C.7), let us denote

Γ = U T E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ]U,

and for any k, l ∈ {1, . . . , d + m}, the (k, l) entry of the matrix U T (cid:104)I1(cid:105)tU can be written as

[U T (cid:104)I1(cid:105)tU ]k,l =

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)(1 − ϕjσl)ϕ2

i (Γkl + ri,kl),

i=0

j=i+1

where ri,kl → 0 as i → ∞ almost surely. Noting from (23) that σk + σl ≥ 2(1 − ρτ ), and using the
condition (26), Lemmas A.2, A.3 lead to [U T (cid:104)I1(cid:105)tU ]k,l/ϕt → Γkl/(σk + σl + ϕ/ (cid:101)ϕ) as t → ∞ almost
surely. Thus, we have

· (cid:104)I1(cid:105)t

a.s.−→ U (Θ ◦ Γ)U T (28)

= Ξ(cid:63).

(C.14)

1
ϕt

Thus, (Duﬂo, 1997, Theorem 1.3.15) shows that (27) holds. This shows the ﬁrst part of the results.
For the second part of the results, we assume a higher order moment condition (15b). We have

E[(cid:107)θi(cid:107)3 | Fi−1]

(21b)
≤ 4

(cid:32)

E

(cid:34)(cid:13)
(cid:13)
(I + Ci)K−1
(cid:13)
(cid:13)

i

(cid:18)¯gi − ∇fi
0

(cid:19)(cid:13)
3
(cid:13)
(cid:13)
(cid:13)

(cid:35)

(cid:33)

| Fi−1

+ E[(cid:107)zi,τ − (I + Ci)(cid:101)zi(cid:107)3 | Fi−1]

(cid:16)

(7)
≤ 4

8Υ3
K

(15b)
≤ 4 (cid:0)8Υ3
(3)
≤ 4 (cid:0)8Υ3
(3)
≤ 4 (cid:0)8Υ3
(15b)
≤ 4 (cid:0)8Υ3

E[(cid:107)¯gi − ∇fi(cid:107)3 | Fi−1] + E[(cid:107)( (cid:101)Ci − Ci)(cid:101)zi(cid:107)3 | Fi−1]
KΥm + 8E[(cid:107)(cid:101)zi(cid:107)3 | Fi−1](cid:1)
KΥm + 8Υ3
K

E[(cid:107) ¯∇Li(cid:107)3 | Fi−1](cid:1)

(also use (cid:107)K−1

(also use (cid:107) (cid:101)Ci(cid:107) ∨ (cid:107)Ci(cid:107) ≤ 1)

i (cid:107) ≤ ΥK)

(cid:17)

(also use (cid:107)Ci(cid:107) ≤ 1, (cid:107)K−1

i (cid:107) ≤ ΥK)

KΥm + 8Υ3
K

KΥm + 8Υ3
K

(cid:8)4(cid:107)∇Li(cid:107)3 + 4E[(cid:107)¯gi − ∇fi(cid:107)3 | Fi−1](cid:9)(cid:1)

(cid:8)4Υ3

u + 4Υm

(cid:9)(cid:1)

(also use (13)).

(C.15)

Thus, θi has bounded third moment; and (Wang, 1995, pp. 554) together with (C.14) give the
result (a). For (b), we verify Lindeberg’s condition. For any (cid:15) > 0, we have

1
ϕt

≤

(22)
=

t
(cid:88)

E(cid:2)(cid:13)
(cid:13)

t
(cid:89)

i=0

j=i+1

{I − ϕj(I + C(cid:63))} ϕiθi(cid:13)
2 · 1(cid:107)(cid:81)t
(cid:13)

j=i+1{I−ϕj (I+C(cid:63))}ϕiθi(cid:107)≥(cid:15)

√

ϕt

| Fi−1

(cid:3)

1
(cid:15)ϕ3/2
t

1
(cid:15)ϕ3/2
t

t
(cid:88)

i=0

t
(cid:88)

t
(cid:89)

E(cid:2)(cid:13)
(cid:13)

j=i+1

t
(cid:89)

E(cid:2)(cid:13)
(cid:13)

i=0

j=i+1

{I − ϕj(I + C(cid:63))} ϕiθi(cid:13)
3 | Fi−1
(cid:13)

(cid:3)

{I − ϕjΣ} ϕiU T θi(cid:13)
3 | Fi−1
(cid:13)

(cid:3).

50

To show the right hand side converges to zero, it suﬃces to show that each entry of the vector on
the right hand side converges to zero. In particular, it suﬃces to show that for any 1 ≤ k ≤ d + m,

1
(cid:15)ϕ3/2
t

t
(cid:88)

t
(cid:89)

i=0

j=i+1

|1 − ϕjσk|3 ϕ3
i

E[|[U T θi]k|3 | Fi−1] −→ 0

as

t → ∞.

By (C.15) and the fact E[|[U T θi]k|3 | Fi−1] ≤ E[(cid:107)θi(cid:107)3 | Fi−1], we only need to show (cid:80)t
ϕjσk|3ϕ3

). Without loss of generality, we suppose 1 − ϕjσk ≥ 0 for all j ≥ 1, and show

(cid:81)t

j=i+1 |1−

i = o(ϕ3/2

i=0

t

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)3ϕ3

i = o(ϕ3/2

t

).

(C.16)

i=0

j=i+1

Otherwise, since ϕ < 0 from (26), Lemma A.2 shows that ϕi → 0. Thus, there exists (cid:101)t such that
1 − ϕjσk ≥ 0, ∀j ≥ (cid:101)t. Therefore,

t
(cid:88)

t
(cid:89)

i=0

j=i+1

|1 − ϕjσk|3 ϕ3

i =

(cid:101)t−2
(cid:88)

t
(cid:89)

i=0

j=i+1

|1 − ϕjσk|3 ϕ3

i +

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)3ϕ3
i

i=(cid:101)t−1

j=i+1

t
(cid:89)

(1 − ϕjσk)3

j=(cid:101)t
t
(cid:88)

t
(cid:89)

=

=

(cid:101)t−2
(cid:88)

(cid:101)t−1
(cid:89)

i=0

j=i+1

|1 − ϕjσk|3 ϕ3

i +

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)3ϕ3
i

i=(cid:101)t−1

j=i+1

(1 − ϕjσk)3(ϕ(cid:48)

i)3,

(C.17)

i=(cid:101)t−1

j=i+1

where

ϕ(cid:48)
(cid:101)t−1 =





(cid:101)t−2
(cid:88)

(cid:101)t−1
(cid:89)

i=0

j=i+1

|1 − ϕjσk|3 ϕ3

i + ϕ3

(cid:101)t−1



1/3



,

and

ϕ(cid:48)

i = ϕi,

∀i ≥ (cid:101)t.

Note that (C.17) has the same form as (C.16), and ϕ(cid:48)
and (C.16) have the same limit. For (C.16), we apply Lemma A.1, and note that

i diﬀers from ϕi only at i = (cid:101)t − 1. Thus, (C.17)

lim
i→∞

i

(cid:18)

1 −

ϕ2
i−1
ϕ2
i

(cid:19) (26)

= 2ϕ

and

3σk + 2ϕ/ (cid:101)ϕ

(26)
> 0.

Thus, Lemma A.3 suggests that

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)3ϕ3

i = O(ϕ2

t ).

i=0

j=i+1

This veriﬁes (C.16) and further veriﬁes Lindeberg’s condition. Thus, the central limit theorem of
martingale in (Duﬂo, 1997, Corollary 2.1.10) leads to (b). For (c), we apply (Fan, 2019, Theorem
2.1) with (cid:15) =
ϕt, δ = 0, ρ = 1 (in their notation), as proved for verifying Lindeberg’s condition
above, and obtain the normalized result immediately. This completes the proof.

√

51

C.5 Proof of Lemma 4.6

We note that

I2,t

(20b)
=

t
(cid:88)

t
(cid:89)

i=0

j=i+1

{I − ϕj(I + C(cid:63))} (¯αi − ϕi)zi,τ

t
(cid:88)

t
(cid:89)

(22)
= U

i=0

j=i+1

{I − ϕjΣ} (¯αi − ϕi)U T zi,τ .

Thus, for any 1 ≤ k ≤ d + m, we have

[U T I2,t]k =

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)(¯αi − ϕi)[U T zi,τ ]k.

i=0

j=i+1

For the same reason as (C.16) and (C.17), we suppose for any j ≥ 0 that 1 − ϕjσk ≥ 0. Then, we
know

(cid:12)
(cid:12)[U T I2,t]k

(cid:12)
(cid:12) ≤

1
2

t
(cid:88)

t
(cid:89)

i=0

j=i+1

|1 − ϕjσk|χi

(cid:12)
(cid:12)[U T zi,τ ]k

(cid:12)
(cid:12) =

1
2

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)χi

(cid:12)
(cid:12)[U T zi,τ ]k

(cid:12)
(cid:12)

i=0

j=i+1

=

1
2

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)χiE (cid:2)(cid:12)

(cid:12)[U T zi,τ ]k

(cid:12)
(cid:12) | Fi−1

(cid:3)

i=0

j=i+1

+

1
2

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)χi

(cid:8)(cid:12)
(cid:12)[U T zi,τ ]k

(cid:12)
(cid:12) − E (cid:2)(cid:12)

(cid:12)[U T zi,τ ]k

(cid:12)
(cid:12) | Fi−1

(cid:3)(cid:9) =: J3,t,k + J4,t,k.

(C.18)

i=0

j=i+1

Intuitively, J3,t,k dominates J4,t,k since the latter measures the error to the mean. We precisely
show such result in the following. We ﬁrst show that (cid:12)

(cid:12)
(cid:12) has bounded variance. We have

(cid:12)[U T zi,τ ]k

E

(cid:104)(cid:8)(cid:12)

(cid:12)[U T zi,τ ]k

(cid:12) − E (cid:2)(cid:12)
(cid:12)

(cid:12)[U T zi,τ ]k

(cid:12)
(cid:12) | Fi−1

(cid:3)(cid:9)2

| Fi−1

(cid:105)

≤ E

(cid:104)(cid:12)
(cid:12)[U T zi,τ ]k

(cid:12)
2
(cid:12)

(cid:105)

| Fi−1

≤ E (cid:2)(cid:107)zi,τ (cid:107)2 | Fi−1

(cid:3) (B.7)

≤ 16Υ2

K(Υ2

u ∨ Υm).

(C.19)

Thus, J4,t,k is a square integrable martingale. Its variance is bounded by

(cid:104)J4,k(cid:105)t :=

1
4

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)2χ2
i

E

(cid:104)(cid:8)(cid:12)

(cid:12)[U T zi,τ ]k

(cid:12) − E (cid:2)(cid:12)
(cid:12)

(cid:12)[U T zi,τ ]k

(cid:12)
(cid:12) | Fi−1

(cid:3)(cid:9)2

| Fi−1

(cid:105)

i=0

j=i+1

(C.19)

≤ 4Υ2

K(Υ2

u ∨ Υm)

t
(cid:88)

t
(cid:89)

(1 − ϕjσk)2χ2
i .

i=0

j=i+1

Using (26) and (29), we know

lim
i→∞

i

(cid:18)

1 −

χ2

i−1/ϕi−1
χ2
i /ϕi

(cid:19)

(cid:18)

1 −

= lim
i→∞

i

= lim
i→∞

i

(cid:19) (cid:18)

(cid:26)(cid:18)

1 −

χi−1
χi

(cid:19)(cid:19)

χ2
i−1
χ2
i

1 +

+

χ2
i−1
χ2
i
(cid:19)

χi−1
χi

(cid:18)

1 −

−

χ2
i−1
χ2
i

ϕi
ϕi−1
ϕi
ϕi−1

(cid:19)(cid:27)

(cid:18)

1 −

ϕi−1
ϕi

= 2χ − ϕ.

(C.20)

52

Further, (29) and (23) imply 2σk + (2χ − ϕ)/ (cid:101)ϕ > 0. Thus, Lemma A.3 leads to (cid:104)J4,k(cid:105)t = O(χ2
and the strong law of large number (Duﬂo, 1997, Theorem 1.3.15) suggests that

t /ϕt);

J4,t,k = o

(cid:18)(cid:113)

t /ϕt · {log(ϕt/χ2
χ2

t )}1+υ(cid:48)

(cid:19)

= o

(cid:18)(cid:113)

t /ϕt · {log(1/χt)}1+υ(cid:48)
χ2

(cid:19) (29)

= o(χt/ϕt).

(C.21)

For the term J3,t,k, we have

J3,t,k ≤

1
2

t
(cid:88)

t
(cid:89)

(1−ϕjσk)χi

i=0

j=i+1

(cid:113)

E[|[U T zi,τ ]k|2 | Fi−1]

≤ 2ΥK(Υu ∨

(C.19)

t
(cid:88)

t
(cid:89)

(cid:112)

Υm)

(1−ϕjσk)χi.

i=0

j=i+1

Using (26) and (29), and the fact that

lim
i→∞

i

(cid:18)

1 −

χi−1/ϕi−1
χi/ϕi

(cid:19) (C.20)

= χ − ϕ,

and σk + (χ − ϕ)/ (cid:101)ϕ > 0 (as implied by (29)), we apply Lemma A.3 and obtain

J3,t,k = O(χt/ϕt).

(C.22)

Plugging (C.22), (C.21) into (C.18), we complete the proof.

C.6 Proof of Lemma 4.7

Based on the deﬁnition of I3,t in (20c), we have the recursion

I3,t+1 = {I − ϕt+1(I + C(cid:63))} I3,t + ϕt+1δt+1.

(C.23)

By Assumption 3.1, we have

(cid:13)δt(cid:13)
(cid:13)
(cid:13)

(21c)
≤ 2 (cid:0)(cid:107)(K(cid:63))−1(cid:107)(cid:107)ψt(cid:107) + (cid:107)K−1

≤ 2ΥKΥL

(cid:13)
(cid:18)xt − x(cid:63)
(cid:13)
(cid:13)
λt − λ(cid:63)
(cid:13)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+ 2Υ2

t − (K(cid:63))−1(cid:107) · (cid:107)∇Lt(cid:107)(cid:1) + (cid:107)Ct − C(cid:63)(cid:107) ·
(cid:13)
(cid:18)xt − x(cid:63)
(cid:13)
(cid:13)
λt − λ(cid:63)
(cid:13)

KΥu(cid:107)Kt − K(cid:63)(cid:107) ·

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:18)xt − x(cid:63)
(cid:13)
(cid:13)
λt − λ(cid:63)
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)xt − x(cid:63)
(cid:13)
(cid:13)
λt − λ(cid:63)
(cid:13)

((cid:107)Ct(cid:107) ≤ 1)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

.(C.24)

+ (cid:107)Ct − C(cid:63)(cid:107) ·

Using the fact that Kt → K(cid:63) (cf. Theorem 3.12) and Ct → C(cid:63) (cf. Corollary 4.4), we know

δt = o((cid:107)(xt − x(cid:63), λt − λ(cid:63))(cid:107)).

(C.25)

Furthermore, we use (cid:107)C(cid:63)(cid:107) ≤ ρτ and know that for any a ∈ (0, 1), there exists a threshold t1 such
that for any t ≥ t1,

(cid:107)I3,t+1(cid:107) ≤ {1 − ϕt+1(1 − ρτ )} (cid:107)I3,t(cid:107) + ϕt+1 · o

(cid:18)(cid:13)
(cid:18)xt+1 − x(cid:63)
(cid:13)
(cid:13)
λt+1 − λ(cid:63)
(cid:13)

(cid:19)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ {1 − ϕt+1(1 − ρτ ) + o(ϕt+1)} (cid:107)I3,t(cid:107) + ϕt+1 · o((cid:107)I1,t(cid:107) + (cid:107)I2,t(cid:107))
≤ {1 − a(1 − ρτ )ϕt+1} (cid:107)I3,t(cid:107) + ϕt+1 · o((cid:107)I1,t(cid:107) + (cid:107)I2,t(cid:107)).

(Lemma 4.1)

53

We apply the above inequality recursively, and obtain

(cid:107)I3,t+1(cid:107) ≤

t+1
(cid:89)

j=t1+1

{1 − a(1 − ρτ )ϕj} (cid:107)I3,t1(cid:107)

t+1
(cid:88)

t+1
(cid:89)

+

i=t1+1

j=i+1

{1 − a(1 − ρτ )ϕj} ϕio((cid:107)I1,i−1(cid:107) + (cid:107)I2,i−1(cid:107)).

(C.26)

We apply Lemmas 4.5 and 4.6 for bounding (cid:107)I1,i−1(cid:107) and (cid:107)I2,i−1(cid:107). In particular, we note that for
any υ ≥ 0,



lim
i→∞

i

1 −

(cid:113)

(26)
= lim
i→∞

i

(cid:18)

1 −

(cid:113)

ϕi−2 {log(1/ϕi−2)}1+υ

ϕi−1 {log(1/ϕi−1)}1+υ
√
√

1 −

(cid:32)

(cid:19)

i

+ lim
i→∞

ϕi−2
ϕi−1

(26)
=

ϕ
2

(cid:32)

+ lim
i→∞

i

1 −

{log(1/ϕi−2)}

{log(1/ϕi−1)}

1+υ
2

1+υ
2

Furthermore, we have



(cid:32)

 = lim
i→∞

i

1 −

√
√

ϕi−2
ϕi−1

(cid:32)

√
√

ϕi−2
ϕi−1

+

1 −

(cid:33)(cid:33)

{log(1/ϕi−2)}

{log(1/ϕi−1)}

1+υ
2

1+υ
2

{log(1/ϕi−2)}

{log(1/ϕi−1)}
(cid:33)

(cid:33)

1+υ
2

1+υ
2

.

(Lemma A.1)

lim
i→∞

i

(cid:18)

1 −

(cid:19)

log(1/ϕi−2)
log(1/ϕi−1)

= lim
i→∞

i log(ϕi−2/ϕi−1)
log(1/ϕi−1)
(cid:110) ϕi−2−ϕi−1
ϕi−1

i

i log (1 + (ϕi−2 − ϕi−1)/ϕi−1)
log(1/ϕi−1)
(cid:17)(cid:111)

= lim
i→∞
(cid:16) (ϕi−2−ϕi−1)2
ϕ2

+ O

i−1

= lim
i→∞

log(1/ϕi−1)

= lim
i→∞

−ϕ
log(1/ϕi−1)

= 0,

where the last equality uses the fact that ϕi → 0, as implied by Lemma A.2. Combining the above
two displays with Lemma A.1, we have

Moreover, we have



lim
i→∞

i

1 −

(cid:113)

ϕi−2 {log(1/ϕi−2)}1+υ

(cid:113)

ϕi−1 {log(1/ϕi−1)}1+υ



 =

ϕ
2

.

lim
i→∞

i

(cid:18)

1 −

χi−2/ϕi−2
χi−1/ϕi−1

(cid:19) (C.20)

= χ − ϕ.

(C.27)

(C.28)

Letting a be any scalar such that

0 <

−ϕ/ (cid:101)ϕ
2(1 − ρτ )

∨

−(χ − ϕ)/ (cid:101)ϕ
1 − ρτ

< a < 1,

which is guaranteed to exist due to (26) and (29), we can see that

a(1 − ρτ ) +

ϕ
2 (cid:101)ϕ

> 0

and

a(1 − ρτ ) +

χ − ϕ
(cid:101)ϕ

> 0.

Thus, combining (C.26), (C.27), and (C.28) with Lemma A.3, we obtain the results for both (15a)
and (15b). This completes the proof.

54

C.7 Proof of Lemma 4.9

Combining (B.11) and (B.12), for any υ > 0, we have

(cid:107)Kt − K(cid:63)(cid:107) ≤

ΥL
t

=

ΥL
t

(cid:13)
(cid:18)x0 − x(cid:63)
(cid:13)
(cid:13)
λ0 − λ(cid:63)
(cid:13)

t−1
(cid:88)

i=0
(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:18)xi − x(cid:63)
(cid:13)
(cid:13)
λi − λ(cid:63)
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ ΥL(cid:107)xt − x(cid:63)(cid:107) + o

(cid:32)(cid:114)

(log t)1+υ
t

(cid:33)

+ ωt

t−1
(cid:88)

t
(cid:89)

(cid:18)

+ ΥL

1 −

(cid:19) 1
i

1
j

(cid:13)
(cid:18)xi − x(cid:63)
(cid:13)
(cid:13)
λi − λ(cid:63)
(cid:13)

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ ΥL(cid:107)xt − x(cid:63)(cid:107) + o

(cid:32)(cid:114)

(log t)1+υ
t

(cid:33)

+ ωt

= ΥL

t−1
(cid:88)

t
(cid:89)

i=1

j=i+1

(cid:18)

1 −

(cid:19) 1
i

1
j

j=i+1

i=1
(cid:26)

ai + O

(cid:19)(cid:27)

(cid:18)

+ ΥL

at + O

(cid:18) χi
ϕi

(cid:19)(cid:19)

(cid:18) χt
ϕt

+ o

(cid:32)(cid:114)

(cid:33)

(log t)1+υ
t

+ ωt,(C.29)

ϕi {log(1/ϕi)}(1+υ)/2) under (15a) and ai = O((cid:112)ϕi log(1/ϕi)) un-
where, by Theorem 4.8, ai = o(
der (15b). We claim that ϕ > −2. Otherwise, ϕ + 1.5 ≤ −0.5 < 0. We apply Lemma A.1 and have

√

lim
t→∞

t

(cid:18)

1 −

ϕt−1(t − 1)1.5
ϕtt1.5

(cid:19)

= lim
t→∞

t

(cid:18)

1 −

ϕt−1
ϕt

+

ϕt−1
ϕt

(cid:18)

1 −

(t − 1)1.5
t1.5

(cid:19)(cid:19) (26)

= ϕ + 1.5 < 0.

Then, Lemma A.2 suggests that ϕtt1.5 → 0, which cannot hold under (26). Thus, ϕ > −2. Using
(C.27) and Lemma A.3, and noting that 1 + ϕ/2 > 0, we obtain

t−1
(cid:88)

t
(cid:89)

i=1

j=i+1

(cid:18)

1 −

(cid:19) ai
i

1
j

= O(at).

(C.30)

Furthermore, we deal with the term that involves χi/ϕi in (C.29). Without loss of generality, we
suppose χ ≥ 3ϕ/2. Otherwise, we know

(cid:32)

lim
t→∞

t

1 −

(cid:33)

χt−1/ϕ3/2
t−1
χt/ϕ3/2
t

(C.20)

= χ −

3ϕ
2

< 0.

This implies that χt/ϕt = o(
the argument of the lemma holds immediately. Supposing χ ≥ 3ϕ/2 and noting that

ϕt) = o(at) and, thus, all terms O(χi/ϕi) in (C.29) are negligible and

√

(cid:18)

1 −

lim
t→∞

t

χt−1/ϕt−1
χt/ϕt

(cid:19) (C.20)

= χ − ϕ

and

1 + χ − ϕ ≥ 1 +

ϕ
2

> 0,

we obtain from Lemma A.3 that

t−1
(cid:88)

t
(cid:89)

i=1

j=i+1

(cid:18)

1 −

(cid:19) 1
i

1
j

· O

(cid:19)

(cid:18) χi
ϕi

= O

(cid:19)

.

(cid:18) χt
ϕt

(C.31)

Combining (C.29), (C.30), (C.31) together, and noting that o((cid:112)(log t)1+υ/t) = o(at) under (15a)
(as implied by the fact that tϕt → (cid:101)ϕ ∈ (0, ∞]), we complete the proof.

55

C.8 Proof of Lemma 4.10

Combining (C.24), Theorem 4.8, Lemma 4.9, and Corollary 4.4, we have for any υ > 0,

(cid:107)δt(cid:107) = O (ϕt log(1/ϕt)) + O

(cid:19)

+ o

(cid:18) χ2
t
ϕ2
t
(cid:17)
(cid:16)(cid:112)ϕt log(1/ϕt) · ωt
(cid:32)
(cid:112)ϕt log(1/ϕt) ·

(cid:18) χt
ϕt

+ O

· ωt

(cid:19)

(30)
= O (ϕt log(1/ϕt)) + o

+ O

(cid:32)
(cid:112)ϕt log(1/ϕt) ·

(cid:114)

(log t)1+υ
t

(cid:33)

(cid:32)

+ o

(cid:33)

(cid:114)

χt
ϕt

(log t)1+υ
t

(cid:33)

(cid:114)

(log t)1+υ
t

+ O

(cid:16)(cid:112)ϕt log(1/ϕt) · ωt

(cid:17)

.

We plug the above bound into the recursion (C.23) and apply Lemma A.3. In particular, we note
that

lim
t→∞

t

(cid:18)

1 −

(cid:32)

lim
t→∞

t

1 −

(cid:32)

lim
t→∞

t

1 −

ϕt−1 log(1/ϕt−1)
ϕt log(1/ϕt)

(cid:19) (C.27)

= ϕ,

(cid:112)ϕt−1 log(1/ϕt−1)(cid:112)(log(t − 1))1+υ/(t − 1)
(cid:112)ϕt log(1/ϕt)(cid:112)(log t)1+υ/t

(cid:112)ϕt−1 log(1/ϕt−1)ωt−1
(cid:112)ϕt log(1/ϕt)ωt

(cid:33)

=

ϕ
2

+ ω.

(cid:33)

=

ϕ
2

−

1
2

,

Thus, Lemma A.3 suggests that I3,t has the same order as δt provided

1 − ρτ +

ϕ − 1
2 (cid:101)ϕ
The above conditions are implied by (26) and (30). If ωt = 0 for all suﬃciently large t, the results
hold trivially. This completes the proof.

ϕ/2 + ω
(cid:101)ϕ

1 − ρτ +

1 − ρτ +

ϕ
(cid:101)ϕ

> 0,

> 0.

> 0,

C.9 Proof of Theorem 4.11

By Lemmas 4.6 and 4.10, we know that
(cid:112)1/ϕt · I2,t = O(χt/ϕ3/2

),

t

(cid:112)1/ϕt · I3,t = O(

√

ϕt log(1/ϕt)) + o

Using Lemma A.1 and the fact that

(cid:32) (cid:112)log(1/ϕt)(log t)1+υ
√

t

(cid:33)

+ O

(cid:16)(cid:112)log(1/ϕt) · ωt

(cid:17)

.

lim
t→∞

t

lim
t→∞

t

1 −

(cid:32)

(cid:32)

lim
t→∞

t

1 −

√

(cid:18)

1 −

ϕt−1 log(1/ϕt−1)
√
ϕt log(1/ϕt)

(cid:19)

=

ϕ
2

< 0,

(cid:112)log(1/ϕt−1)(log(t − 1))1+υ/
√

(cid:112)log(1/ϕt)(log t)1+υ/

t

(cid:112)log(1/ϕt−1) · ωt−1
(cid:112)log(1/ϕt) · ωt

(cid:33)

= ω < 0,

56

√

(cid:33)

t − 1

= −

1
2

< 0,

we know (cid:112)1/ϕt · I2,t = o(1) and (cid:112)1/ϕt · I3,t = o(1) almost surely. Thus, the Slutsky’s theorem
together with Lemma 4.5 lead to the asymptotic normality. Furthermore, Lemma A.5 with Ct = 0,
Bt = I2,t + I3,t leads to the Berry-Esseen bound. This completes the proof.

C.10 Proof of Lemma 4.12

We note that

(cid:107)Ξ(cid:63) − Ξt(cid:107) ≤

(cid:13)
(cid:13)
Ξ(cid:63) −
(cid:13)
(cid:13)

(cid:13)
(cid:13)
E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ]
(cid:13)
(cid:13)

1
2 + ϕ/ (cid:101)ϕ
1
+
2 + ϕ/ (cid:101)ϕ

E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ] − Ω(cid:63)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) +
(cid:13)

1
2 + ϕ/ (cid:101)ϕ

(cid:107)Ω(cid:63) − Ωt(cid:107) .

(C.32)

For the ﬁrst term in (C.32), we have

(cid:13)
(cid:13)
Ξ(cid:63) −
(cid:13)
(cid:13)

(cid:18)

(28)
=

1
2 + ϕ/ (cid:101)ϕ
1
Θ −
2 + ϕ/ (cid:101)ϕ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

Θ −

1
2 + ϕ/ (cid:101)ϕ
1
2 + ϕ/ (cid:101)ϕ
and for any 1 ≤ k, l ≤ d + m,

Θ −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 4

11T

(cid:13)
(cid:13)
(cid:13)
(cid:13)

11T

(cid:13)
(cid:13)
· E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ]
(cid:13)
(cid:13)

(cid:19)

11T

◦ U T E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ]U

(cid:13)
(cid:13)
(cid:13)
(cid:13)

· (cid:107)E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ](cid:107)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

((cid:107) (cid:101)C(cid:63)(cid:107) ≤ 1)

(cid:107)Ω(cid:63)(cid:107),

((cid:107)A ◦ B(cid:107) ≤ (cid:107)A(cid:107) · (cid:107)B(cid:107))

(cid:12)
(cid:12)
Θk,l −
(cid:12)
(cid:12)

1
2 + ϕ/ (cid:101)ϕ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
σk + σl + ϕ/ (cid:101)ϕ

−

1
2 + ϕ/ (cid:101)ϕ

|2 − σk − σl|
(σk + σl + ϕ/ (cid:101)ϕ)(2 + ϕ/ (cid:101)ϕ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(23)
≤

(26)
≤

2ρτ
(2 − 2(1 + ϕ/ (cid:101)ϕ) + ϕ/ (cid:101)ϕ)(2 + ϕ/ (cid:101)ϕ)

2ρτ
(2 − 2ρτ + ϕ/ (cid:101)ϕ)(2 + ϕ/ (cid:101)ϕ)
2ρτ
−ϕ/ (cid:101)ϕ(2 + ϕ/ (cid:101)ϕ)

=

.

Therefore, the above two displays lead to

(cid:13)
(cid:13)
Ξ(cid:63) −
(cid:13)
(cid:13)

1
2 + ϕ/ (cid:101)ϕ

· E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ]

(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O(ρτ ).

(C.33)

For the second term in (C.32), we have
E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ] − Ω(cid:63)(cid:13)
(cid:13)
(cid:13) ≤ (cid:107)C(cid:63)Ω(cid:63)(cid:107) + (cid:107)Ω(cid:63)C(cid:63)(cid:107) + (cid:107)E[ (cid:101)C(cid:63)Ω(cid:63)( (cid:101)C(cid:63))T ](cid:107)
(cid:13)
(cid:13)
(cid:13)

(since E[ (cid:101)C(cid:63)] = C(cid:63))

≤ O(ρτ ) + (cid:107)E[ (cid:101)C(cid:63)Ω(cid:63)( (cid:101)C(cid:63))T ](cid:107).

((cid:107)C(cid:63)(cid:107) ≤ ρτ )

57

Furthermore, since Ω(cid:63) (cid:22) Υ2

0 (cid:22) E[ (cid:101)C(cid:63)Ω(cid:63)( (cid:101)C(cid:63))T ] (cid:22) Υ2

KΥm · I, we obtain
KΥmE[ (cid:101)C(cid:63)( (cid:101)C(cid:63))T ]

T 









T 

















τ
(cid:89)



j=1









τ
(cid:89)



j=2







τ
(cid:89)





j=1







τ
(cid:89)





j=2

= Υ2

KΥmE

(I − K(cid:63)Sj(ST

j (K(cid:63))S)†ST

j K(cid:63))

(I − K(cid:63)Sj(ST

j (K(cid:63))S)†ST

j K(cid:63))

(cid:20)
KΥmE

= Υ2




τ
(cid:89)



j=2

(I − K(cid:63)Sj(ST

j (K(cid:63))Sj)†ST

j K(cid:63))






E[(I − K(cid:63)S1(ST

1 (K(cid:63))S1)†ST

j K(cid:63)) | S2:τ ]




τ
(cid:89)



j=2

(I − K(cid:63)Sj(ST

j (K(cid:63))Sj)†ST

j K(cid:63))


T


(cid:21)



(cid:22) Υ2

KΥmρE

(I − K(cid:63)Sj(ST

j (K(cid:63))S)†ST

j K(cid:63))

(I − K(cid:63)Sj(ST

j (K(cid:63))S)†ST

j K(cid:63))

(cid:22) Υ2

KΥmρτ · I,

where the second inequality from the end is from Assumption 3.3 and Corollary 4.4; and the last
inequality applies the same derivation for sketch matrices S2:τ . Combining the above two displays,

E[(I + (cid:101)C(cid:63))Ω(cid:63)(I + (cid:101)C(cid:63))T ] − Ω(cid:63)(cid:13)
(cid:13)
(cid:13) ≤ O(ρτ ).
(cid:13)
(cid:13)
(cid:13)

For the third term in (C.32), we have

(cid:107)Ωt − Ω(cid:63)(cid:107)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t



+ O

t−1
(cid:88)

i=0

(25)
= O((cid:107)Kt − K(cid:63)(cid:107))
(cid:32)

¯gi¯gT

i −

1
t

t−1
(cid:88)

i=0

(cid:33) (cid:32)

(cid:33)T

1
t

t−1
(cid:88)

i=0

¯gi

− E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)] − ∇f (x(cid:63))∇T f (x(cid:63))

¯gi

Furthermore, we have

(C.34)



(cid:13)
(cid:13)
(cid:13)
 .
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

≤

i=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

t−1
(cid:88)

i=0

t−1
(cid:88)

¯gi¯gT

i −

(cid:33) (cid:32)

(cid:32)

1
t

t−1
(cid:88)

i=0

¯gi

(cid:33)T

1
t

t−1
(cid:88)

i=0

¯gi

− E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)] − ∇f (x(cid:63))∇T f (x(cid:63))

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

¯gi¯gT

(cid:13)
(cid:13)
i − E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)]
(cid:13)
(cid:13)
(cid:13)

+

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

t−1
(cid:88)

i=0

¯gi

(cid:33) (cid:32)

(cid:33)T

1
t

t−1
(cid:88)

i=0

¯gi

(cid:13)
(cid:13)
(cid:13)
− ∇f (x(cid:63))∇T f (x(cid:63))
(cid:13)
(cid:13)
(cid:13)

.

We take the ﬁrst term as an example, while the second term has the same guarantee following the
same derivations. We note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

t−1
(cid:88)

i=0

¯gi¯gT

(cid:13)
(cid:13)
i − E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)]
(cid:13)
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

i=0

(¯gi¯gT

i ) − E[¯gi¯gT
i

| Fi−1]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

t−1
(cid:88)

i=0

E[¯gi¯gT
i

(cid:13)
(cid:13)
| Fi−1] − E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)]
(cid:13)
(cid:13)
(cid:13)

.

1
t
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

58

By (15c), we know the ﬁrst term on the right hand side is a square integrable martingale. The
strong law of large number (Duﬂo, 1997, Theorem 1.3.15) suggests that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

t−1
(cid:88)

(¯gi¯gT

i ) − E[¯gi¯gT
i

i=0

| Fi−1]

(cid:32)(cid:114)

(log t)1+υ
t

(cid:33)

.

= o

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

By (C.9), (C.29), (C.30), (C.31), the second term on the right hand side can be bounded by

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

t−1
(cid:88)

i=0

E[¯gi¯gT
i

(cid:13)
(cid:13)
| Fi−1] − E[∇f (x(cid:63); ξ)∇T f (x(cid:63); ξ)]
(cid:13)
(cid:13)
(cid:13)

= O

(cid:17)
(cid:16)(cid:112)ϕt log(1/ϕt)

+ O

(cid:19)

.

(cid:18) χt
ϕt

Combining the above ﬁve displays with Lemma 4.9, we have

(cid:107)Ωt − Ω(cid:63)(cid:107) = O

(cid:16)(cid:112)ϕt log(1/ϕt)

(cid:17)

+ O

(cid:19)

(cid:18) χt
ϕt

+ o

(cid:32)(cid:114)

(cid:33)

(log t)1+υ
t

+ ωt.

(C.35)

Combining (C.32), (C.33), (C.34), and (C.35), we complete the proof.

C.11 Proof of Corollary 4.13

We let τ large enough such that wT Ξ(cid:63)w (cid:54)= 0. We note that

wT (xt − x(cid:63), λt − λ(cid:63))
wT Ξtw

(cid:112)

=

√

wT (xt − x(cid:63), λt − λ(cid:63))

(cid:113)

wT Ξ(cid:63)w ·

1 + wT Ξtw−wT Ξ(cid:63)w

wT Ξ(cid:63)w

.

Thus, combining Lemmas 4.12, A.5 with Theorem 4.11, we complete the proof.

C.12 Proof of Theorem 4.14

By the Raabe’s test, we know that (32) implies that (18) holds. Thus, the convergence of (cid:107)∇Lt(cid:107)
comes from Theorem 3.7. The convergence of Kt comes from Theorem 3.12. Furthermore, we
note that (33) implies (26) and (29). To show this, we ﬁrst note from (32) that χ − β < 0. Thus,
χt = o(βt) (cf. Lemma A.2). This implies βt ≤ ϕt ≤ βt + o(βt), and further lim
tβt = (cid:101)β.
t→∞
Moreover, we have

tϕt = lim
t→∞

(cid:18)

1 −

lim
t→∞

t

(cid:19)

ϕt−1
ϕt
(cid:18)

= β + lim
t→∞

t

1 −

= β +

1
2

lim
t→∞

χt
βt

· t

(cid:18)

1 −

= lim
t→∞

t

2 + χt−1/βt−1
2 + χt/βt
χt−1
χt

1 −

(cid:18)

·

βt
βt−1

(cid:19)

2βt−1 + χt−1
2βt + χt
1
2

= β +

(cid:19)

lim
t→∞

= lim
t→∞
(cid:18) χt
βt

t

−

(cid:19)

= β +

χ − β
2

lim
t→∞

χt−1
βt−1
χt
βt

= β.

(cid:18)

t

1 −

(cid:26)

1 −

+

βt−1
βt

2 + χt−1/βt−1
2 + χt/βt

(cid:27)(cid:19)

βt−1
βt

(cid:19)

(since χt = o(βt))

Thus, (33) implies (26) and (29); and the asymptotic convergence rates of (xt, λt) and Kt come
from Theorem 4.8 and Lemma 4.9. Finally, it is easy to see (32), (33), and (34) imply (30). In fact,
it suﬃces to show 1 − ρτ + (β − 1)/(2 (cid:101)β) > 0. When (cid:101)β = ∞, it holds naturally. When (cid:101)β ∈ (0, ∞),
then we know β = −1 (otherwise, β ∈ (−1, −0.5) implies tβt → ∞). Thus, 1 − ρτ + (β − 1)/(2 (cid:101)β) =
1 − ρτ + β/ (cid:101)β > 0, as implied by (33). Thus, the asymptotic normality and Berry-Esseen bound
come from Theorem 4.11 and Corollary 4.13.

59

