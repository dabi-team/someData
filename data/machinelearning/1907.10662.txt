Formal Methods in Computer-Aided Design 2020

ART: Abstraction Reﬁnement-Guided Training for
Provably Correct Neural Networks

Xuankang Lin∗, He Zhu†, Roopsha Samanta∗ and Suresh Jagannathan∗
∗Purdue University, West Lafayette, IN 47907
†Rutgers University, Piscataway, NJ 08854

0
2
0
2

t
c
O
1

]

G
L
.
s
c
[

3
v
2
6
6
0
1
.
7
0
9
1
:
v
i
X
r
a

Abstract—Artiﬁcial Neural Networks (ANNs) have demon-
strated remarkable utility in various challenging machine learn-
ing applications. While formally veriﬁed properties of their
behaviors are highly desired,
they have proven notoriously
difﬁcult to derive and enforce. Existing approaches typically
formulate this problem as a post facto analysis process. In this
paper, we present a novel learning framework that ensures such
formal guarantees are enforced by construction. Our technique
enables training provably correct networks with respect to a
broad class of safety properties, a capability that goes well-beyond
existing approaches, without compromising much accuracy. Our
key insight
is that we can integrate an optimization-based
abstraction reﬁnement loop into the learning process and operate
over dynamically constructed partitions of the input space that
considers accuracy and safety objectives synergistically. The
reﬁnement procedure iteratively splits the input space from which
training data is drawn, guided by the efﬁcacy with which such
partitions enable safety veriﬁcation. We have implemented our
approach in a tool (ART) and applied it to enforce general safety
properties on unmanned aviator collision avoidance system ACAS
Xu dataset and the Collision Detection dataset. Importantly, we
empirically demonstrate that realizing safety does not come at
the price of much accuracy. Our methodology demonstrates that
an abstraction reﬁnement methodology provides a meaningful
pathway for building both accurate and correct machine learning
networks.

I. INTRODUCTION

Artiﬁcial neural networks (ANNs) have emerged in recent
years as the primary computational structure for implementing
many challenging machine learning applications. Their success
has been due in large measure to their sophisticated architec-
ture, typically comprised of multiple layers of connected neu-
rons (or activation functions), in which each neuron represents
a possibly non-linear function over the inputs generated in a
previous layer. In a supervised setting, the goal of learning is to
identify the proper coefﬁcients (i.e., weights) of these functions
that minimize differences between the outputs generated by the
network and ground truth, established via training samples.
The ability of ANNs to identify ﬁne-grained distinctions
among their inputs through the execution of this process makes
them particularly useful in a variety of diverse domains such as
classiﬁcation, image recognition, natural language translation,
or autonomous driving.

However, the most accurate ANNs may still be incorrect.
the ACAS Xu (Airborne Collision
Consider, for instance,
Avoidance System) application that targets avoidance of midair
collisions between commercial aircraft [1], whose system
is controlled by a series of ANNs to produce horizontal

maneuver advisories. One example safety property states that
if a potential intruder is far away and is signiﬁcantly slower
than one’s own vehicle, then regardless of the intruder’s and
subject’s direction, the ANN controller should output a Clear-
of-Conﬂict advisory (as it is unlikely that the intruder can
collide with the subject). Unfortunately, even a sophisticated
ANN handler used in the ACAS Xu system, although well-
trained, has been shown to violate this property [2]. Thus,
ensuring the reliability of ANNs, especially those adopted
in safety-critical applications,
is increasingly viewed as a
necessity.

The programming languages and formal methods commu-
nity has responded to this familiar, albeit challenging, problem
with increasingly sophisticated and scalable veriﬁcation ap-
proaches [2]–[5] — given a trained ANN and a property, these
approaches either certify that the ANN satisﬁes the property
or identify a potential violation of the property. Unfortunately,
when veriﬁcation fails, these approaches provide no insight
on how to effectively leverage veriﬁcation counterexamples to
repair complex, uninterpretable networks and ensure safety.
Further, many veriﬁcation approaches focus on a popular, but
ultimately, narrow class of properties — local robustness —
expressed over some, but not all of a network’s input space.
In this paper, we address the limitations of existing veri-
ﬁcation approaches by proposing a novel training approach
for generation of ANNs that are correct-by-construction with
respect to a broad class of correctness properties expressed
over the network’s inputs. Our training approach integrates
correctness properties into the training objective through a
correctness loss function that quantiﬁes the violation of the
correctness properties. Further, to enable certiﬁcation of cor-
rectness of a possibly inﬁnite set of network behaviors, our
training approach employs abstract interpretation methods [4],
[6] to generate sound abstractions of both the input space and
the network itself. Finally, to ensure the trained network is both
correct and accurate with respect to training data, our approach
iteratively reﬁnes the precision of the input abstraction, guided
by the value of the correctness loss function. Our approach is
sound — if the correctness loss reduces to 0, the generated
ANN is guaranteed to satisfy the associated correctness prop-
erties.

The workﬂow of this overall approach — Abstraction
Reﬁnement-guided Training (ART) — is shown in Fig. 1.
ART takes as input a correctness property (Φin, Φout) that pre-
scribes desired network output behavior using logic constraints

This article is
licensed under a Creative
Commons Attribution 4.0 International License

 
 
 
 
 
 
F :

Speed

Input
layer

v = 4
v

Hidden
layer

p1 = 4.5
p1

1

1

0.5

Direction

θ

−1

θ = 1

p2

p2 = 3

relu

relu

Hidden
layer

q1 = 4.5
q1

0.5

q2

q2 = 3

1

1

Output
layer

y1 = 1.5
y1

−1

Report

y2

Ignore

y2 = 5.25

Fig. 2: A monitoring system using 2-layer ReLU network.

such techniques.

The remainder of the paper is organized as follows. In
the next section, we provide a detailed motivating example
that illustrates our approach. Section III provides background
and Section IV formalizes our approach. Details about ART’s
implementation and evaluation are provided in Section V.
Related work and conclusions are presented in Section VI
and VII, resp.

II. ILLUSTRATIVE EXAMPLE

We illustrate and motivate the key components of our
approach by starting with a realistic, albeit simple, end-to-end
example. We consider the construction of a learning-enabled
system for autonomous driving. The learning objective is to
identify potentially dangerous objects within a prescribed
range of the vehicle’s current position.

Problem Setup. For the purpose of this example, we simplify
our scenario by assuming that we track only a single object and
that the information given by the vehicle’s radar is a feature
vector of size two, containing (a) the object’s normalized
relative speed v ∈ [−5, 5] where the positive values mean
that the objects are getting closer; and (b) the object’s relative
angular position θ ∈ [−π, π] in a polar coordinate system
with our vehicle located in the center. Either action Report or
action Ignore is advised by the system for this object given
the information.

Consider an implementation of an ANN for this problem
that uses a 2-layer ReLU neural network F with initialized
weights as depicted in Fig. 2. The network takes an input
vector x = (v, θ) and outputs a prediction score vector
y = (y1, y2) for actions Report and Ignore, respectively. The
action with higher prediction score is picked by the advisory
system. For simplicity, both layers in F are linear layers with
2 neurons and without bias terms. An element-wise ReLU
activation function relu(x) = max(x, 0) is applied after the
ﬁrst layer.

Correctness Property. To serve as a useful advisory system,
we can ascribe some correctness properties that we would like
the network to always satisfy. While our approach generalizes
to an arbitrary number of the correctness properties that one
may wish to enforce, we focus on one such correctness
property Φ in this example: Objects in front of the vehicle
that are stationary or moving closer should not be ignored.
The meaning of “stationary or moving closer” and “in front
of ” can be interpreted in terms of predicates Φin and Φout over

Fig. 1: The ART framework.

Φout when the inputs to the network are within a domain
described by Φin. ART is parameterized by an abstract domain
D that yields an abstraction over inputs in Φin. Additionally,
ART takes a set of labeled training data. The correctness loss
function quantiﬁes the distance of the abstract network output
from the correctness constraint Φout. In each training iteration,
ART both updates the network weights and reﬁnes the input
abstraction. The network weights are updated using classical
gradient descent optimization to mitigate the correctness loss
(upper loop of Fig. 1) and the standard accuracy loss (lower
loop of Fig. 1). The abstraction reﬁnement utilizes information
provided by the correctness loss to improve the precision of
the abstract network output (the top arrow of Fig. 1). As we
show in Section V, the key novelty of our approach - exploiting
the synergy between reﬁnement and approximation - (a) often
leads to, at worst, mild impact on accuracy compared to a safe
oracle baseline; and (b) provides signiﬁcantly higher assurance
on network correctness than existing veriﬁcation or training [7]
methods which do not exploit abstraction reﬁnement.

This paper makes the following contributions. (1) We
present an abstract interpretation-guided training strategy for
building correct-by-construction neural networks, deﬁned with
respect to a rich class of safety properties, including functional
correctness properties that relate input and output structure.
(2) We deﬁne an input space abstraction reﬁnement loop that
reduces training on input data to training on input space
partitions, where the precision of the abstraction is, in turn,
guided by a notion of correctness loss as determined by the
correctness property. (3) We formalize soundness claims that
capture correctness guarantees provided by our methodology;
these results characterize the ability of our approach to ensure
correctness with respect to domain-speciﬁc correctness prop-
erties. (4) We have implemented our ideas in a tool (ART) and
applied it to challenging benchmarks including the ACAS Xu
collision avoidance dataset [1], [2] and the Collision Detection
dataset [8]. We provide a detailed evaluation study quantifying
the effectiveness of our approach and assess its utility to
ensure correctness guarantees without compromising accuracy.
We additionally provide a comparison of our approach with
post
facto counterexample-guided veriﬁcation strategies to
demonstrate the beneﬁts of ART’s methodology compared to

feature vector components such as v ≥ 0 and θ ∈ [0.5, 2.5]1,
respectively. Using such representations and recalling that
v ∈ [−5, 5], Φ = (Φin, Φout) can be precisely formulated
as:

∀v, θ. v ∈ [0, 5] ∧ θ ∈ [0.5, 2.5]
(cid:125)

(cid:124)

(cid:123)(cid:122)
Φin

∧ y = F (v, θ) ⇒ y1 > y2
(cid:124) (cid:123)(cid:122) (cid:125)
Φout

.

Observe that this property is violated with the network and
the example input shown in Fig. 2.

Concrete Correctness Loss Function. To quantify how correct
F is on inputs satisfying predicate Φin, we deﬁne a correctness
loss function, denoted dist g, over the output y of the neural
network and the output predicate Φout:

dist g(y, Φout) = min
q|=Φout

g(y, q),

parameterized on a distance function g over
the input
space such as the Manhattan distance (L1-norm), Euclidean
distance (Euclid-norm), etc. The correctness distance function
is intentionally deﬁned to be semantically meaningful—when
distg(y, Φout) = 0,
it follows that y satisﬁes the output
predicate Φout. This function can then be used as a loss
function, among other
to train the
this
neural network towards
example, we can compute the correctness distance of the
network output y = (y1, y2) from Φout = y1 > y2 to be
(cid:16)
distEuclid(y, Φout) = max
which is
calculated based on the Euclidean distance between point
(y1, y2) and line y2 − y1 = 0.

training objectives
satisfying (Φin, Φout). For

(y2 − y1)/

2, 0

√

(cid:17)

Abstract Domain. A general correctness property like Φ is
often deﬁned over an inﬁnite set of data points; however,
since training necessarily is performed using only a ﬁnite set
of samples, we cannot generalize observations made on just
these samples to assert the validity of Φ on the trained network.
Our approach, therefore, leverages abstract interpretation tech-
niques to generate sound abstractions of both the network input
space and the network itself. By training on these abstractions,
our method obtains a ﬁnite approximation of the inﬁnite set of
possible network behaviors, enabling correct-by-construction
training.

i.e., abstractions in which an abstract output

We parameterize our approach on any abstract domain that
serves as a sound over-approximation of a neural network’s
is
behavior,
guaranteed to subsume all possible outputs for the set of
inputs. In the example, we consider the interval
abstract
abstract domain I that is simple enough to motivate the core
ideas of our approach. We note that ART is not bound to
speciﬁc abstract domains, the interval domain is used only for
illustrative purposes here, our experiments in Section V are
conducted using more precise abstractions.

An interval abstraction of our 2-layer ReLU network,
denoted FI, is shown in Fig. 3. The concrete neural network

1We pick [0.5, 2.5] because it is slightly wider than the front view angle
4 , 3π
4 ].

of [ π

FI:

Input
layer

v ∈ [0, 5]

Speed

Direction

v

θ

Hidden
layer

Hidden
layer

Output
layer

p1 ∈ [0.25, 6.25]
p1

1

relu

q1 ∈ [0.25, 6.25]
q1

1

0.5

−1

p2

relu

0.5

q2

y1 ∈ [−4.25, 6.25]
y1

Report

−1

y2

Ignore

1

1

θ ∈ [0.5, 2.5]

p2 ∈ [−2.5, 4.5]

q2 ∈ [0, 4.5]

y2 ∈ [0.125, 7.625]

Fig. 3: The 2-layer ReLU network over interval domain.

computation F is abstracted by maintaining the lower and
upper bounds [u, u] of each neuron u. For neuron p2 in this
example, following interval arithmetic [9], the lower bound
of neuron is computed by p2 = 1 · v + (−1) · θ = −2.5 and
the upper bound p2 = 1 · v + (−1) · θ = 4.5. For ReLU
activation function, FI resets negative lower bounds to 0 and
preserves everything else. Consider neurons p2 → q2, lower
bound q2 is reset to 0 while its upper bound q2 remains
unchanged. In this way, FI soundly over-approximates all
possible outputs generated by the network given any inputs
satisfying Φin. Applying FI, the neural network’s abstract
output is y1 ∈ [−4.25, 6.25] and y2 ∈ [0.125, 7.625], which
fails to show that y1 > y2 always holds. As a counterexample
depicted in Fig. 2, the input v = 4 ∧ θ = 1 leads to violation.

Abstract Correctness Loss Function. Given Φin, to quantify
how correct F is based on the abstract output y#, we can also
deﬁne an abstract correctness loss function, denoted Lg, over
y# and the output predicate Φout:

Lg(y#, Φout) = max
y∈γ(y#)

distg(y, Φout),

where γ(y#) maps y# to the set of values it represents in
the concrete domain and g is a distance function over the
input space as before. In our example, LEuclid(y#, Φout) =
max
= 11.875/

(y2 − y1)/

2, 0

√

√

2.

(cid:17)

(cid:16)

Measuring the worst-case distance of possible outputs
to Φout, Lg
is also semantically meaningful — when
Lg(y#, Φout) = 0, it follows that all possible values rep-
resented by y# satisfy the output predicate Φout. In other
words, the trained neural network F is certiﬁed safe w.r.t.
the correctness property Φ.

Lg can be leveraged as the objective function during
optimization. The min and max units
in Lg can be
implemented using MaxPooling and MinPooling units,
and hence is differentiable. Then we can use off-the-shelf
automatic differentiation libraries [10] in the usual fashion
to derive and backpropagate the gradients and readjust F ’s
weights towards minimizing Lg.

Input Space Abstraction Reﬁnement. The abstract correctness
loss function Lg provides a direction for neural network
weight optimization. However, Lg could be overly imprecise
since the amount of spurious cases introduced by the neural
network abstraction is correlated with the size of the abstract
input region. This kind of imprecision leads to sub-optimal

optimization, ultimately hurting the feasibility of correct-by-
construction as well as the model accuracy.

Such imprecision arises easily when using less precise
abstract domains like the interval domain. For our running
example, by bisecting the input space along each dimen-
sion, the resulting abstract correctness loss values of each
2. If the original
region range from 3.125/
abstract correctness loss 11.875/
2 pertains to a real input,
it should be reﬂected in some sub-region as well. Now that
2, the original abstract correctness loss
9.125/
must be spurious and thus suboptimal for optimization.

2 to 9.125/
√

2 < 11.875/

√

√

√

√

To use more accurate gradients

for network weight
optimization, our approach leverages the above observation
region Φin during
to also iteratively partition the input
training.
In other words, we seek for an input space
reduces imprecise
abstraction reﬁnement mechanism that
abstract correctness loss introduced by abstract interpretation.
Notably,
incorporating input space abstraction reﬁnement
with the gradient descent optimizer does not compromise the
soundness of our approach. As long as all sub-regions of Φin
are provably correct, the network’s correctness with respect
to Φin trivially holds.

Iterative Training. Our training algorithm interweaves input
space abstraction reﬁnement and gradient descent training on
a network abstraction in each training iteration by leveraging
the correctness loss function produced by the network abstract
interpreter (as depicted in Fig. 1), until a provably correct
ANN is trained. The reﬁned input abstractions computed in
an iteration are used for training over the abstract domain in
the next iteration.

For our illustrative example, we set the learning rate of
the optimizer to be 0.01. In our experiment, the maximum
correctness loss among all reﬁned input space abstractions
drops to 0 after 11 iterations. Convergence was achieved by
heuristically partitioning the input space Φin into 76 regions.
The trained ANN is guaranteed to satisfy the correctness
property (Φin, Φout).

III. BACKGROUND

Deﬁnition III.1 (Neural network). Neural networks are func-
tions F : Rd → Re composed of Q layers and Q−1 activation
functions. Each layer is a function fk(·) ∈ Rmk−1 → Rmk
for k = 1, . . . , Q where m0 = d and mQ = e. Each
activation function is of the form σk(·) ∈ Rmk → Rmk for
k = 1, . . . , Q − 1. Then, F = fQ ◦ σQ−1 ◦ fQ−1 ◦ . . . ◦ σ1 ◦ f1.

Deﬁnition III.2 (Abstraction). An abstraction D is deﬁned as
a tuple: (cid:104)Dc, Da, α, γ, T (cid:105) where

• Dc : {x | x ∈ Rd} and where d ∈ Z+ is the concrete

domain;

• Da is the abstract domain of interest;
• α(·) is an abstraction function that maps a set of concrete

elements to an abstract element;

• γ(·) is a concretization function that maps an abstract

element to a set of concrete elements;

• T = (cid:8)(Tc, Ta) | Tc(·) : Dc → Dc, Ta(·) : Da → Da

(cid:9) is

a set of transformer pairs over Dc and Da.

An abstraction is sound if for all S ⊆ Dc, S ⊆ γ(α(S)) holds
and given (Tc, Ta) ∈ T ,

∀c ∈ Dc, a ∈ Da, c ∈ γ(a) =⇒ Tc(c) ∈ γ(Ta(a)).

Deﬁnition III.3 (D-compatible). Given a sound abstraction
D = (cid:104)Dc, Da, α, γ, T (cid:105), a neural network F is D-compatible
iff for every layer or activation function ι(·) in F , there exists
(cid:1) ∈ T , and Ta is
an abstract transformer Ta such that (cid:0)ι(·), Ta
differentiable at least almost everywhere.

For a D-compatible neural network F , we denote by FD :
Da → Da the over-approximation of F where every layer
fk(·) and activation function σk(·) in F are replaced in FD
by their corresponding abstract transformers in D.

Although our approach is parametric over abstract domains,
we do require every abstract transformer Ta associated with
these domains to be differentiable, so as to enable training
using the worst cases over-approximated over D via gradient-
descent style optimization algorithms.

To reason about a neural network over an abstraction D, we
need to ﬁrst characterize what it means for an ANN to operate
over D.

Deﬁnition III.4 (Evaluation over Abstract Domain). Given
a D-compatible neural network F , the evaluation of F over
D and a range of inputs X ∈ Da is FD(X) where FD(X)
over-approximates all possible outputs in the concrete domain
corresponding to any input covered by X.

Theorem III.1 (Over-approximation Soundness). For sound
abstraction D, given a D-compatible neural network F , a
range of inputs X ∈ Da,

∀x. x ∈ γ(X) ⇒ F (x) ∈ γ (cid:0)FD (X)(cid:1) .

Proofs of all theorems are provided in the Appendix.

IV. CORRECT-BY-CONSTRUCTION TRAINING

Our approach aims to train an ANN F with respect to a
correctness property Φ, which is formally deﬁned in Sec-
tion IV-A. The abstraction of F w.r.t. Φ based on abstract
domain D essentially can be seen as a function parameterized
over the weights of F , which can nonetheless be trained to
ﬁt Φ using standard optimization algorithms. Section IV-B
formally deﬁnes the abstract correctness loss function LD to
guide the optimization of F ’s weights over D. Such an abstrac-
tion inevitably introduces spurious data samples into training
due to over-approximation. Section IV-C introduces the idea
of input space abstraction and reﬁnement as a mechanism
that can reduce such spuriousness during optimization over
D. The detailed pseudocode of ART algorithm, including the
reﬁnement procedure, is presented in Section IV-D.

A. Correctness Property

The correctness properties we consider are expressed as
logical propositions over the network’s inputs and outputs. We
assume that an ANN correctness property expresses constraints
on the outputs, given assumptions on the inputs.

Deﬁnition IV.1 (Correctness Property). Given a neural net-
work F : Rd → Re, a correctness property Φ = (Φin, Φout)
is a tuple in which Φin deﬁnes a bounded input domain over
Rd in the form of an interval [x, x] where x, x ∈ Rd, are
lower, upper bounds, resp., on the network input; and Φout
is a quantiﬁer-free Boolean combination of linear inequalities
over the network output vector y ∈ Re:

(cid:104)Φout(cid:105) ::= (cid:104)P(cid:105) | ¬(cid:104)P(cid:105) | (cid:104)P(cid:105) ∧ (cid:104)P(cid:105) | (cid:104)P(cid:105) ∨ (cid:104)P(cid:105);

(cid:104)P(cid:105) ::= A · y ≤ b where A ∈ Re, b ∈ R;

An input vector x ∈ Rd is said to satisfy Φin = [x, x],
denoted x |= Φin, iff x ≤ x ≤ x. An output vector y ∈ Re
satisﬁes Φout, denoted y |= Φout, iff Φout(y) is true. A neural
network F : Rd → Re satisﬁes Φ, denoted F |= Φ, iff ∀x. x |=
Φin ⇒ F (x) |= Φout.

Deﬁnition IV.2 (Concrete Correctness Loss Function). For
an atomic output predicate P , the concrete correctness loss
function, distg(y, P ), quantiﬁes the distance from an output
vector y ∈ Re to P :

distg(y, P ) = min
q|=P

g(y, q)

where g : Rd × Rd (cid:55)→ Z≥0 is a differentiable distance function
over the inputs. Similarly, distg(y, Φout), the “distance” from
an output vector y ∈ Re to general output predicate Φout, can
be computed efﬁciently by induction as long as g(·, ·) can be
computed efﬁciently:

• distg(y, P ) and distg(y, ¬P ) can be computed using

basic arithmetic;

• distg(y, P1 ∧ P2) = max(distg(y, P1), distg(y, P2));
• distg(y, P1 ∨ P2) = min(distg(y, P1), distg(y, P2)).

Note that distg(y, Φout) may not represent the minimum
distance for arbitrary Φout, but it is efﬁcient to compute while
still retaining the following soundness theorem.

Theorem IV.1 (Zero Concrete Correctness Loss Soundness).
Given output predicate Φout over Re and output vector y ∈ Re,

distg(y, Φout) = 0 =⇒ y |= Φout.

neural network F , and a correctness property Φ = (Φin, Φout),
the abstract correctness loss function is deﬁned as:

LD,g(F, Φ) = max

p∈γ(YD)

distg(p, Φout)

where YD = FD(α(Φin)).

Here g : Rd × Rd (cid:55)→ Z≥0 is a differentiable distance function
over concrete inputs as before.

The abstract correctness loss function measures the worst-
case distance to Φout of any neural network outputs subsumed
by the abstract network output. It is designed to extend the
notion of concrete correctness loss to the abstract domain with
a similar soundness guarantee, as formulated in the following
theorem.

Theorem IV.2 (Zero Abstract Correctness Loss Soundness).
Given a sound abstraction D, a D-compatible neural network
F , and a correctness property Φ,

LD,g(F, Φ) = 0 =⇒ F |= Φ.

In what follows, we ﬁx the distance function g over concrete
inputs and denote the abstract correctness loss function simply
as LD.

C. Abstraction Reﬁnement

Recall that in Section II we illustrated how imprecision in
the correctness loss for a coarse abstraction can be mitigated
using an input space abstraction reﬁnement mechanism. Our
notion of reﬁnement is formally deﬁned below.

Deﬁnition IV.4 (Input Space Abstraction). An input space
abstraction S reﬁnes a correctness property Φ = (Φin, Φout)
into a set of correctness properties S =
such
that Φin = (cid:83)
in. Given a neural network F and an input
space abstraction S, F |= S ⇐⇒ (cid:86)

in, Φout

(cid:110)(cid:0)Φi

i Φi

(cid:1)(cid:111)

Φ∈S F |= Φ.

Deﬁnition IV.5 (Input Space Abstraction Reﬁnement). A well-
founded abstraction reﬁnement (cid:118) is a binary relation over a
set of input abstractions S = {S1, S2, . . .} such that:

• (reﬂexivity): ∀Si ∈ S, Si (cid:118) Si;
• (reﬁnement): ∀Si ∈ S and correctness property

(Ψin, Ψout),







Ψin =

(cid:91)

Φj
in


 ∧




(Φj

in, )∈Si
=⇒ Si (cid:118) {(Ψin, Ψout)};

(cid:94)

Φj

out ⇔ Ψout

( ,Φj

out)∈Si






B. Over-approximation

To reason about correctness properties deﬁned over an
inﬁnite set of data points, our approach generates sound
abstractions of both the network input space and the network
itself, obtaining a ﬁnite approximation of the inﬁnite set
of possible network behaviors. We start by quantifying the
abstract correctness loss of over-approximated outputs.

Deﬁnition IV.3 (Abstract Correctness Loss Function). Given
a sound abstraction D = (cid:104)Dc, Da, α, γ, T (cid:105), a D-compatible

• (transitivity): ∀S1, S2, S3 ∈ S, S1 (cid:118) S2 ∧ S2 (cid:118) S3 =⇒

S1 (cid:118) S3;

• (composition): ∀S1, S2, S3, S4 ∈ S, S1 (cid:118) S3 ∧ S2 (cid:118)

S4 =⇒ S1 ∪ S2 (cid:118) S3 ∪ S4.

The reﬂexivity, transitivity, and compositional requirements
for a well-founded reﬁnement are natural. The reﬁnement
rule states that an input space abstraction S reﬁnes some
correctness property (Ψin, Ψout) if the union of all
input
domains in S is equivalent to Ψin and all output predicates in

S are logically equivalent to Ψout. This rule enables Ψin to
be safely decomposed into a set of sub-domains. As a result,
the problem of enforcing coarse-grained correctness properties
on neural networks can be converted into one that enforces
multiple ﬁne-grained properties, an easier problem to tackle
because much of the imprecision introduced by the coarse-
grained abstraction can now be eliminated.

Theorem IV.3 (Sufﬁcient Condition via Reﬁnement).

∀F, S1, S2, S1 (cid:118) S2 ∧ F |= S1 =⇒ F |= S2.

To do this, we naturally extend the notion of abstract correct-
ness loss over one property to an input space abstraction.

Deﬁnition IV.6 (Abstract Correctness Loss Function for In-
put Space Abstraction). Given a sound abstraction D, D-
compatible neural network F , and input space abstraction S,
the abstract correctness loss of F with respect to S is denoted
by2

LD(F, S) =

LD(F, Φ).

(cid:88)

Φ∈S

Theorem IV.4 (Zero Abstract Correctness Loss for Input
Space Abstraction). Given a sound abstraction D, a D-
compatible neural network F , and an input space abstraction
S,

LD(F, S) = 0 =⇒ F |= S.

D. The ART Algorithm

The goal of our ANN training algorithm, given in Fig. 4, is
to optimize the network to have LD(F, S) reduce to 0, thereby
ensuring a correct-by-construction network. The algorithm
takes as input both an initial input space abstraction S and a
set of labeled training data (cid:8)(xtrain, ylabel)(cid:9) in order to achieve
correctness while maintaining high accuracy on the trained
model. The abstract correctness loss, denoted (cid:96)D, is computed
at Line 4 according to Def. IV.3 and checked correctness by
comparing against 0. If (cid:96)D = 0, as long as the accuracy loss,
denoted (cid:96)A, is also satisfactory, ART returns a correct and
accurate network following Thm. IV.4.

The joint loss of (cid:96)D and (cid:96)A is used to guide the optimization
of neural network parameters using standard gradient-descent
algorithms. The requirement of abstract transformers being
differentiable at least almost anywhere in Def. III.3 enables
computation of gradients (cid:96)D using off-the-shelf automatic
differentiation libraries [10].

Starting from Line 10, abstractions in S that have the largest
(cid:96)D values represent the potentially most imprecise cases and
thus are chosen for reﬁnement. During reﬁnement, ART ﬁrst
picks a dimension to reﬁne using heuristic scores similar to [3].
The heuristic coarsely approximates the cumulative gradient
over one dimension, with a larger score suggesting greater
potential of decreasing correctness loss. The input abstraction
is then bisected along the picked dimension as reﬁnement.

2We can reﬁne the deﬁnition to have positive weighted importance of each
correctness property in S; ascribing different weights to different correctness
properties does not affect soundness.

Fig. 4: ART correct-by-construction training algorithm.

input space abstraction S,

Require: Abstract domain D, D-compatible neural network
learning rate η ∈ R+,
F ,
training data set {(xtrain, ylabel)}, accuracy loss function
LA, accuracy loss bound (cid:15)A ∈ R+, hyper-parameter k.
Ensure: Return the optimized F whose correctness properties

are enforced and accuracy loss bounded by (cid:15)A.

(cid:126)W ← all weights in F to optimize
while true do

1: procedure ART
2:
3:
4:
5:
6:

return F

(cid:96)D, (cid:96)A ← LD(F, S), LA(F, {(xtrain, ylabel)})
if (cid:96)D = 0 ∧ (cid:96)A ≤ (cid:15)A then

end if

∇F ← ∂((cid:96)D+(cid:96)A)
(cid:126)W ← (cid:126)W − η · ∇F

∂ (cid:126)W

(cid:46) optimization

(cid:46) reﬁnement

T ← Subset of S with k largest (cid:96)D values
S(cid:48) ← S \ T
for all (Φi

for all Ψj

in, Φi

out) ∈ T do
in ∈ REFINE(Φi
in, Φi

S(cid:48) ← S(cid:48) ∪ {(Ψj

in, (cid:96)D) do
out)}

end for

7:

8:

9:

10:
11:

12:
13:
14:
15:
16:

end for
S ← S(cid:48)
end while

17:
18:
19: end procedure

20: procedure REFINE(Ψin, (cid:96)D)
21:

for all dimension i of Ψin do

22:

scorei = ∂(cid:96)D

∂{Ψin}i

× |{Ψin}i|

end for
23:
dim ← arg max scorei
24:
in, Ψ2
Ψ1
25:
return (cid:8)Ψ1
26:
27: end procedure

in, Ψ2
in

(cid:9)

in ← Ψin bisected along dimension dim

(cid:46) pick dimension

Corollary 1 (ART Soundness). Given a sound abstraction D,
a D-compatible neural network F , and an initial input space
abstraction S of correctness properties, if the ART algorithm
in Fig. 4 generates a neural network F (cid:48), LD(F (cid:48), S) = 0 and
F (cid:48) |= S.

V. EVALUATION

We have performed an evaluation of our approach to vali-
date the feasibility of building neural networks that are correct-
by-construction over a range of correctness properties.3 All
experiments reported in this section were performed on a
Ubuntu 16.04 system with 3.2GHz CPU and NVidia GTX
1080 Ti GPU with 11GB memory. All experiments uses the

3The code is available at https://github.com/XuankangLin/ART.

DeepPoly abstract domain [11] implemented on Python 3.7
and PyTorch 1.4 [10].

A. ACAS Xu Dataset

Our ﬁrst evaluation study centers around the network archi-
tecture and correctness properties described in the Airborne
Collision Avoidance System for Unmanned Aircraft (ACAS
Xu) dataset [1], [2]. A family of 45 neural networks are used
in the avoidance system; each of these networks consists of
6 hidden layers with 50 neurons in each hidden layer. ReLU
activation functions are applied to all hidden layer neurons.
All 45 networks take a feature vector of size 5 as input
that encodes various aspects of an airborne environment. The
outputs of the networks are prediction scores over 5 advisory
actions to select the advisory action.

In the evaluation, we reason about sophisticated correctness
conditions of the ACAS Xu system in terms of its aggregated
ability to preserve up to 10 correctness properties [2] among
all 45 networks. Each network is supposed to satisfy some
subset of these 10 properties. All correctness properties Φ
can be formulated in terms of input (Φin) and output (Φout)
predicates as in Section IV-A.

Setup. Among the 45 provided networks, 36 are reported with
safety property violations and 9 are reported safe [2]. We
evaluate ART on those 36 unsafe networks to demonstrate the
effectiveness of generating correct-by-construction networks.
The test sets from unsafe networks may contain unsafe points
and are thus unauthentic, so we apply ART on those 9 already
safe networks to demonstrate the accuracy overhead when
enforcing the safety properties. Unfortunately,
the training
and test sets to build these ACAS Xu networks are not
publicly available online. In spite of that,
the ACAS Xu
dataset provides the state space of input states that is used for
training and over which the correctness properties are deﬁned.
We, therefore, uniformly sample a total of 10k training set
and 5k test set data points from the state space. The labels are
collected by evaluating each of the provided 45 networks on
these sampled inputs, with those ACAS Xu networks serving
as oracles. Each network is then trained by ART using its
safety speciﬁcation and the prepared training set, starting
with the provided weights when available or otherwise
randomly initialized weights. We record whether the trained
network is correct-by-construction, as well as their accuracy
evaluated on the prepared test set and the overall training time.

Applying ART. During each training epoch (i.e., each iteration
of the outermost while loop in Fig. 4), our implementation
reﬁnes up to k = 200 abstractions at a time that expose
the largest correctness losses. Larger k leads to ﬁner-grained
abstractions but incurs more training cost. The Adam optimizer
[12] is used in both training tasks and runs up to 100 epochs
with learning rate 0.001 and a learning rate decay policy if
the loss has been stable for some time. Cross entropy loss is
used as the loss function for accuracy . For all experiments
with reﬁnement enabled, reﬁnement operations are applied to

TABLE I: Applying ART to ACAS Xu Dataset.

Reﬁnement Safe% Min Accu. Mean Accu. Max Accu.

36 unsafe nets

9 safe nets

Yes
No

Yes
No

100%
94.44%

100%
88.89%

90.38%
87.88%

93.82%
86.32%

96.10%
94.45%

96.25%
94.29%

98.70%
98.22%

99.92%
99.92%

Correctness rate (%)

Accuracy change (%)

Sampled points
Counterexamples

Sampled points
Counterexamples

10

0

−10

−20

100

80

60

40

20

0

N1 N2 N3 N4 N5 N6 N7 N8

N1 N2 N3 N4 N5 N6 N7 N8

Fig. 5: Correctness rate and accuracy change of post facto
training using sampled points or counterexamples. Results are
normalized based on the baseline networks.

derive up to 5k reﬁned input space abstractions before weight
update starts. The detailed results are shown in Table I.

reﬁnement

To demonstrate the importance of abstraction reﬁnement
mechanism, we also compare between the results with
and without
(as done in existing work [6]).
For completeness, we record the correct-by-construction
enforced rate (Safe%) and the evaluated accuracy statistics
for both tasks among multiple runs. Observe that ART
successfully generates correct-by-construction networks for
loss in accuracy. On the
all scenarios with only minimal
other hand,
it fails to generate
correct-by-construction networks for all cases, and displays
lower accuracy than the reﬁnement-enabled instantiations.
The average training time for each network is 69.39s if with
reﬁnement and 57.85s if without.

if reﬁnement

is disabled,

Comparison with post facto training loop. We also consider
a comparison of our abstraction reﬁnement-guided training for
correct-by-construction networks against a post facto training
loop that feed concrete correctness related data points to
training loops. Such concrete points may be sampled from the
provided speciﬁcation or the collected counterexamples from
an external solver. We show the results on 8 representative
networks comparing to the same baseline in Figure 5. These 8
networks belong to a representative set of networks that cover
all 10 provided safety properties.

For the experiment using sampled data points, 5k points
sampled from correctness properties are used during training.
For the experiment using counterexamples, all counterexam-
ples from correctness queries to external veriﬁer ReluVal [3]
are collected and used during training. In both experiments, the
points from original training set are used for jointly training
to preserve accuracy and the correctness distance functions
following that in Section IV-B are used as loss functions.

TABLE II: Applying ART to Collision Detection Dataset.

Reﬁnement Enforced Accuracy Time

Original [8]

ART

N/A

Yes
No

328/500

99.87% N/A

481/500
420/500

96.83%
86.3%

583s
419s

We concluded the experiments using counterexamples after
20 epochs since no improvement was seen after this point.
Both experiments fail to enforce correctness properties in most
cases and they may impose great impact to model accuracy
compared to the baseline network. We believe this result
demonstrates the difﬁculty of applying a counterexample-
guided training loop strategy for generating safe networks
compared an abstraction-guided methodology.

B. Collision Detection Dataset

Our second evaluation task focuses on the Collision Detec-
tion Dataset [8] where a neural network controller is used to
predict whether two vehicles running curve paths at different
speeds would collide. The network takes as input a feature
vector of size 6, containing the information of distances,
speeds, and directions of the two vehicle. The network output
prediction score are used to classify the scenario as a colliding
or non-colliding case.

A total of 500 correctness properties are proposed in the
Collision Detection dataset that identify the safety margins
around particular data points. The network presented in the
dataset respects 328 such properties. In our evaluation, we
use a 3-layer fully-connected neural network controller with
50, 128, 50 neurons in different hidden layers. Using the same
training conﬁgurations as in Section V-A and evaluating on the
same training and test sets provided in the dataset, the results
are shown in Table II. After 100 epochs, ART converged to
a local minimum and managed to certify 481 out of all 500
safety properties. Although it did not achieve zero correctness
loss, ART can produce a solution that satisﬁes signiﬁcantly
more correctness properties than the oracle neural network, at
the cost of only a small accuracy drop.

VI. RELATED WORK

Neural Network Veriﬁcation. Inspired by the success of
applying program analysis to large software code bases, ab-
stract interpretation-based techniques have been adapted to
reason about ANNs by developing efﬁcient abstract trans-
formers that relax nonlinearity of activation functions into
linear inequality constraints [4], [6]–[8], [11], [13], [14].
Similar approaches [15]–[18] encode nonlinearity via linear
outer bounds of activation functions and may delegate the
veriﬁcation problem to SMT solvers [2], [19] or Mixed Integer
Programming solvers [20]–[22]. Most of those veriﬁers focus
on robustness properties only and do not support veriﬁable
training of network-wide correctness properties. For example,
[11] encodes concrete ANN operations into ELINA [23], a
numeric abstract transformer, and therefore disables opportu-
nities for training or optimization thereafter.

Correctness properties may also be retroﬁtted onto a
trained neural network for safety concerns [24]–[27]. These
approaches usually synthesize a reactive system that monitors
the potentially controller network and corrects any potentially
unsafe actions. Comparing to correct-by-construction methods,
runtime overheads are inevitable for such post facto shielding
techniques.

Correctness Properties in Neural Networks. There have been
a large number of recent efforts that have explored verifying
the robustness of networks against adversarial attacks [28]–
[30]. Recent work has shown how symbolic reasoning
approaches [3], [4] can be used to help validate network
robustness; other efforts combine optimization techniques
with symbolic reasoning to guide symbolic analysis [5]. Our
approach looks at the problem of veriﬁcation and certiﬁcation
from the perspective of general safety speciﬁcations that are
typically richer than notions of robustness governing these
other
techniques and provide the correct-by-construction
guarantee upon training termination. Encoding logical
constraints other than robustness properties into loss functions
has been explored in [31]–[34]. However,
they operate
only on concrete sample instances and do not provide any
correct-by-construction guarantees.

Training over Abstract Domains. The closest approach to our
setting is the work in [6], [35]. They introduced geometric
abstractions that bound activations as they propagate through
the network via abstract
interpretation. Importantly, since
these convex abstractions are differentiable, neural networks
can optimize towards much tighter bounds to improve the
veriﬁed accuracy. A simple bounding technique based on
interval bound propagation was also exploited in [7] (similar
to the interval domain from [6]) to train veriﬁably robust
neural networks that even beat the state-of-the-art networks
in image classiﬁcation tasks, demonstrating that a correct-
by-construction approach can indeed save the need of more
expensive veriﬁcation procedures in challenging domains.
They did not, however, consider veriﬁcation in the context
of global safety properties as discussed here, in which the
over-approximation error becomes non-negligible; nor did they
formulate their approach to be parametric in the speciﬁc form
of the abstractions chosen. Similar ideas have been exploited
in provable defenses works [35]–[38], however, they apply
best-effort adversarial defenses only and provide no guarantee
upon training termination.

VII. CONCLUSIONS

This paper presents a correct-by-construction toolchain that
can train neural networks with provable guarantees. The key
idea is to optimize a neural network over the abstraction of
both the input space and the network itself using abstraction
reﬁnement mechanisms. Experimental results show that our
technique realizes trustworthy neural network systems for a
variety of properties and benchmarks with only mild impact
on model accuracy.

ACKNOWLEDGMENT

This work was supported by C-BRIC, one of six centers in
JUMP, a Semiconductor Research Corporation (SRC) program
sponsored by DARPA; NSF under award CCF-1846327; and
NSF under Grant No. CCF-SHF 2007799.

REFERENCES

[1] K. D. Julian, J. Lopez, J. S. Brush, M. P. Owen, and M. J. Kochenderfer,
“Policy compression for aircraft collision avoidance systems,” in 2016
IEEE/AIAA 35th Digital Avionics Systems Conference (DASC), Sep.
2016, pp. 1–10.

[2] G. Katz, C. W. Barrett, D. L. Dill, K.

J.
Kochenderfer, “Reluplex: An efﬁcient SMT solver
for verifying
deep neural networks,” in Computer Aided Veriﬁcation - 29th
International Conference, CAV 2017, Heidelberg, Germany, July 24-28,
2017, Proceedings, Part I, 2017, pp. 97–117. [Online]. Available:
https://doi.org/10.1007/978-3-319-63387-9 5

Julian, and M.

[3] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Formal security
analysis of neural networks using symbolic intervals,” in 27th USENIX
Security Symposium, USENIX Security 2018, Baltimore, MD, USA,
August 15-17, 2018., 2018, pp. 1599–1614. [Online]. Available: https:
//www.usenix.org/conference/usenixsecurity18/presentation/wang-shiqi
[4] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri,
and M. T. Vechev, “AI2: safety and robustness certiﬁcation of neural
networks with abstract interpretation,” in 2018 IEEE Symposium on
Security and Privacy, SP 2018, Proceedings, 21-23 May 2018, San
Francisco, California, USA, 2018, pp. 3–18.
[Online]. Available:
https://doi.org/10.1109/SP.2018.00058

[5] G. Anderson, S. Pailoor, I. Dillig, and S. Chaudhuri, “Optimization
and abstraction: a synergistic approach for analyzing neural network
robustness,” in Proceedings of the 40th ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2019,
Phoenix, AZ, USA, June 22-26, 2019., 2019, pp. 731–744. [Online].
Available: https://doi.org/10.1145/3314221.3314614

[6] M. Mirman, T. Gehr, and M. T. Vechev, “Differentiable abstract
interpretation for provably robust neural networks,” in Proceedings
of
the 35th International Conference on Machine Learning, ICML
2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, 2018,
pp. 3575–3583. [Online]. Available: http://proceedings.mlr.press/v80/
mirman18b.html

[7] S. Gowal, K. D. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato,
R. Arandjelovic, T. Mann, and P. Kohli, “Scalable veriﬁed training
for provably robust image classiﬁcation,” in The IEEE International
Conference on Computer Vision (ICCV), October 2019.

[8] R. Ehlers, “Formal veriﬁcation of piece-wise linear feed-forward neural
networks,” in Automated Technology for Veriﬁcation and Analysis
- 15th International Symposium, ATVA 2017, Pune, India, October
3-6, 2017, Proceedings, 2017, pp. 269–286.
[Online]. Available:
https://doi.org/10.1007/978-3-319-68167-2 19

[9] R. E. Moore, R. B. Kearfott, and M.

to Interval Analysis.
//doi.org/10.1137/1.9780898717716

SIAM, 2009.

J. Cloud,

Introduction
[Online]. Available: https:

[10] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,
A. K¨opf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
“Pytorch: An
B. Steiner, L. Fang,
imperative style, high-performance deep learning library,” in Advances
in Neural Information Processing Systems 32: Annual Conference
on Neural
Information Processing Systems 2019, NeurIPS 2019,
8-14 December 2019, Vancouver, BC, Canada, 2019, pp. 8024–
8035. [Online]. Available: http://papers.nips.cc/paper/9015-pytorch-an-
imperative-style-high-performance-deep-learning-library

and S. Chintala,

J. Bai,

[11] G. Singh, T. Gehr, M. P¨uschel, and M. T. Vechev, “An Abstract Domain
for Certifying Neural Networks,” PACMPL, vol. 3, no. POPL, pp. 41:1–
[Online]. Available: https://dl.acm.org/citation.cfm?id=
41:30, 2019.
3290354

[12] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, 2015. [Online]. Available: http://arxiv.org/abs/1412.6980

[13] G. Singh, T. Gehr, M. Mirman, M. P¨uschel, and M. T. Vechev, “Fast and
effective robustness certiﬁcation,” in Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, 3-8 December 2018,
Montr´eal, Canada., 2018, pp. 10 825–10 836. [Online]. Available: http:
//papers.nips.cc/paper/8278-fast-and-effective-robustness-certiﬁcation

[14] G. Singh, T. Gehr, M. Puschel, and M. Vechev, “Robustness
certiﬁcation with reﬁnement,” in International Conference on Learning
Representations, 2019.
[Online]. Available: https://openreview.net/
forum?id=HJgeEh09KQ

[15] H. Zhang, T. Weng, P. Chen, C. Hsieh, and L. Daniel, “Efﬁcient neural
network robustness certiﬁcation with general activation functions,”
in Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS
2018, 3-8 December 2018, Montr´eal, Canada., 2018, pp. 4944–4953.
[Online]. Available: http://papers.nips.cc/paper/7742-efﬁcient-neural-
network-robustness-certiﬁcation-with-general-activation-functions
[16] T. Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, L. Daniel, D. S.
Boning, and I. S. Dhillon, “Towards fast computation of certiﬁed
robustness for relu networks,” in Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsm¨assan,
Stockholm, Sweden, July 10-15, 2018, 2018, pp. 5273–5282. [Online].
Available: http://proceedings.mlr.press/v80/weng18a.html

[17] S. Wang, Y. Chen, A. Abdou, and S. Jana, “Mixtrain: Scalable training
of formally robust neural networks,” CoRR, vol. abs/1811.02625, 2018.
[Online]. Available: http://arxiv.org/abs/1811.02625

[18] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Efﬁcient
in
safety analysis of neural networks,”
formal
Information Processing Systems 31: Annual Conference
Neural
on Neural
Information Processing Systems 2018, NeurIPS 2018,
3-8 December 2018, Montr´eal, Canada., 2018, pp. 6369–6379.
[Online]. Available: http://papers.nips.cc/paper/7873-efﬁcient-formal-
safety-analysis-of-neural-networks

in Advances

[19] G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim,
P. Shah, S. Thakoor, H. Wu, A. Zeljic, D. L. Dill, M. J. Kochenderfer,
and C. W. Barrett, “The marabou framework for veriﬁcation and
analysis of deep neural networks,” in Computer Aided Veriﬁcation -
31st International Conference, CAV 2019, New York City, NY, USA,
July 15-18, 2019, Proceedings, Part I, 2019, pp. 443–452. [Online].
Available: https://doi.org/10.1007/978-3-030-25540-4 26

[20] C. Cheng, G. N¨uhrenberg, and H. Ruess, “Maximum resilience of
artiﬁcial neural networks,” in Automated Technology for Veriﬁcation
and Analysis - 15th International Symposium, ATVA 2017, Pune,
India, October 3-6, 2017, Proceedings, 2017, pp. 251–268. [Online].
Available: https://doi.org/10.1007/978-3-319-68167-2 18

[21] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari, “Output range
analysis for deep feedforward neural networks,” in NASA Formal
Methods - 10th International Symposium, NFM 2018, Newport News,
VA, USA, April 17-19, 2018, Proceedings, 2018, pp. 121–138. [Online].
Available: https://doi.org/10.1007/978-3-319-77935-5 9

[22] V. Tjeng, K. Y. Xiao, and R. Tedrake, “Evaluating robustness of
neural networks with mixed integer programming,” in International
Conference on Learning Representations, 2019. [Online]. Available:
https://openreview.net/forum?id=HyGIdiRqtm

[23] G. Singh, M. P¨uschel, and M. T. Vechev, “Fast polyhedra abstract
domain,” in Proceedings of
the 44th ACM SIGPLAN Symposium
on Principles of Programming Languages, POPL 2017, Paris,
France, January 18-20, 2017, 2017, pp. 46–59. [Online]. Available:
http://dl.acm.org/citation.cfm?id=3009885

[24] H. Zhu, Z. Xiong, S. Magill, and S. Jagannathan, “An Inductive
Synthesis Framework for Veriﬁable Reinforcement Learning,” in
Proceedings of the 40th ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI 2019, Phoenix, AZ,
USA, June 22-26, 2019, 2019, pp. 686–701.
[Online]. Available:
https://doi.org/10.1145/3314221.3314638

[25] M. Alshiekh, R. Bloem, R. Ehlers, B. K¨onighofer, S. Niekum, and

U. Topcu, “Safe Reinforcement Learning via Shielding,” AAAI, 2018.

[26] R. Bloem, B. K¨onighofer, R. K¨onighofer, and C. Wang, “Shield synthe-
sis: - runtime enforcement for reactive systems,” in Tools and Algorithms
for the Construction and Analysis of Systems - 21st International
Conference, TACAS 2015, 2015, pp. 533–548.

[27] C. Fan, U. Mathur, S. Mitra, and M. Viswanathan, “Controller synthesis
made real: Reach-avoid speciﬁcations and linear dynamics,” pp. 347–
366, 2018.

[28] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards Deep Learning Models Resistant
to Adversarial Attacks,”
in 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings, 2018. [Online]. Available: https://openreview.net/
forum?id=rJzIBfZAb

[29] K. Pei, Y. Cao, J. Yang, and S. Jana, “Deepxplore: Automated
whitebox testing of deep learning systems,” in Proceedings of
the 26th Symposium on Operating Systems Principles, Shanghai,
China, October 28-31, 2017, 2017, pp. 1–18. [Online]. Available:
https://doi.org/10.1145/3132747.3132785

[30] I.

J. Goodfellow,

J. Shlens, and C. Szegedy, “Explaining and
harnessing adversarial examples,” in 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings, 2015. [Online]. Available:
http://arxiv.org/abs/1412.6572

[31] M. Fischer, M. Balunovic, D. Drachsler-Cohen, T. Gehr, C. Zhang,
and M. Vechev, “DL2: Training and querying neural networks with
logic,” in Proceedings of
the 36th International Conference on
Machine Learning, ser. Proceedings of Machine Learning Research,
K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97, Long Beach,
California, USA, 09–15 Jun 2019, pp. 1931–1941. [Online]. Available:
http://proceedings.mlr.press/v97/ﬁscher19a.html

[32] J. Xu, Z. Zhang, T. Friedman, Y. Liang,

and G. V. den
Broeck, “A semantic loss function for deep learning with symbolic
the 35th International Conference
knowledge,” in Proceedings of
on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,
Sweden, July 10-15, 2018, 2018, pp. 5498–5507. [Online]. Available:
http://proceedings.mlr.press/v80/xu18h.html

[33] P. Minervini and S. Riedel, “Adversarially regularising neural NLI
models to integrate logical background knowledge,” in Proceedings of
the 22nd Conference on Computational Natural Language Learning,
CoNLL 2018, Brussels, Belgium, October 31 - November 1, 2018,
2018, pp. 65–74. [Online]. Available: https://aclanthology.info/papers/
K18-1007/k18-1007

[34] Z. Hu, X. Ma, Z. Liu, E. H. Hovy, and E. P. Xing, “Harnessing deep
neural networks with logic rules,” in Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers, 2016.
[Online]. Available: http://aclweb.org/anthology/P/P16/P16-1228.pdf

[35] M. Balunovic and M. Vechev, “Adversarial

training and provable
defenses: Bridging the gap,” in International Conference on Learning
Representations, 2020.
[Online]. Available: https://openreview.net/
forum?id=SJxSDxrKDr

[36] E. Wong and J. Z. Kolter, “Provable defenses against adversarial
examples via the convex outer adversarial polytope,” in Proceedings
of
the 35th International Conference on Machine Learning, ICML
2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, 2018,
pp. 5283–5292. [Online]. Available: http://proceedings.mlr.press/v80/
wong18a.html

[37] E. Wong, F. R. Schmidt,

J. H. Metzen,

and J. Z. Kolter,
“Scaling provable adversarial defenses,” in Advances
in Neural
Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, 3-8 December
2018, Montr´eal, Canada., 2018, pp. 8410–8419. [Online]. Available:
http://papers.nips.cc/paper/8060-scaling-provable-adversarial-defenses

[38] A. Raghunathan, J. Steinhardt, and P. Liang, “Certiﬁed defenses against
adversarial examples,” in 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings, 2018. [Online]. Available:
https://openreview.net/forum?id=Bys4ob-Rb

APPENDIX

A. Proofs

Theorem III.1 (Over-approximation Soundness). For sound
abstraction D, given a D-compatible neural network F , a
range of inputs X ∈ Da,

∀x. x ∈ γ(X) ⇒ F (x) ∈ γ (cid:0)FD (X)(cid:1) .

Proof. Straightforward after unfolding Deﬁnition III.2 and
applying the properties of concrete and abstract transform-
ers.

Theorem IV.1 (Zero Concrete Correctness Loss Soundness).
Given output predicate Φout over Re and output vector y ∈ Re,

distg(y, Φout) = 0 =⇒ y |= Φout.

Proof. Straightforward after unfolding Deﬁnition IV.2 and
applying the fact that distance function g(·, ·) ≥ 0 always
holds.

Theorem IV.2 (Zero Abstract Correctness Loss Soundness).
Given a sound abstraction D, a D-compatible neural network
F , and a correctness property Φ,

LD,g(F, Φ) = 0 =⇒ F |= Φ.

Proof. When LD(F, Φin, Φout) = 0, by Deﬁnition IV.3,

max
p∈γ(FD(α(Φin)))

dist(p, Φout) = 0.

Since dist(·) is a non-negative function, we have:

∀p ∈ γ(FD(α(Φin))), dist(p, Φout) = 0.

By Deﬁnition III.2, we have Φin ⊆ γ(α(Φin)). By Theorem
III.1, we have

∀x, x |= Φin =⇒ F (x) ∈ γ(FD(α(Φin))).

Hence,

∀x, x |= Φin =⇒ dist(F (x), Φout) = 0.

By Theorem IV.1, it means

∀x, x |= Φin =⇒ F (x) |= Φout.

Thereby proved LD(F, Φin, Φout) = 0
(Φin, Φout).

=⇒ F |=

Theorem IV.3 (Sufﬁcient Condition via Reﬁnement).

∀F, S1, S2, S1 (cid:118) S2 ∧ F |= S1 =⇒ F |= S2.

Proof. By induction on Deﬁnition IV.5,
• When S1 = S2, obviously F |= S2;
• When S2 = (cid:8)Φ = (Φin, Φout)(cid:9):

By Deﬁnition IV.4, from F |= S1 we have

From

we have

Now that

(cid:94)

F |= (Ψin, Ψout)

(Ψin,Ψout)∈S1

(cid:94)

Ψout = Φout

( ,Ψout)∈S1

(cid:94)

F |= (Ψin, Φout)

(Ψin,Ψout)∈S1

Φin =

(cid:91)

Ψin,

(Ψin, )∈S1

by Deﬁnition IV.4, we have F |= (Φin, Φout). Thus F |=
S2.

• For transitivity rule, by induction hypothesis.
• For composition rule, by induction hypothesis.

All cases proved.

Theorem IV.4 (Zero Abstract Correctness Loss for Input
Space Abstraction). Given a sound abstraction D, a D-
compatible neural network F , and an input space abstraction
S,

LD(F, S) = 0 =⇒ F |= S.

Proof. Unfold the deﬁnitions, the proof is straightforward after
applying Theorem IV.2 and the fact that all abstract correctness
losses are non-negative.

Lemma A.1 (Valid Reﬁnement). For any input space abstrac-
tion S, the code snippet of Fig. 4 starting from Line 10 to
Line 17 yields an input space abstraction S(cid:48) such that S(cid:48) (cid:118) S.

Proof. In the code snippet, original input space abstraction
is divided into two parts, T and S(cid:48). S(cid:48) remains the same
throughout execution, so S(cid:48) (cid:118) S(cid:48).

For each correctness property (Φin, Φout) in T , the Reﬁne
procedure in Fig. 4 is called to generate two new input pred-
in. It is easy to show that Φ1
in and Φ2
icates Φ1
in = Φin.
So every new pair
(cid:26)(cid:16)

in ∪ Φ2

(cid:17)(cid:27)

(cid:17)

(cid:16)

Φ1

in, Φout

,

Φ2

in, Φout

(cid:118) (cid:8)(Φin, Φout)(cid:9) .

By composition rule of Deﬁnition IV.5, the union of S(cid:48) and
every such new pair reﬁnes original S.

Corollary 1 (ART Soundness). Given a sound abstraction D,
a D-compatible neural network F , and an initial input space
abstraction S of correctness properties, if the ART algorithm
in Fig. 4 generates a neural network F (cid:48), LD(F (cid:48), S) = 0 and
F (cid:48) |= S.

Proof. From Lemma A.1, we know for any input space ab-
straction S(cid:48) generated during the execution of Fig. 4, S(cid:48) (cid:118) S.
Then by Theorem IV.4 and Theorem IV.3, we have

LD(F, S(cid:48)) = 0 =⇒ F |= S(cid:48) =⇒ F |= S.

