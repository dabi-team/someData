1

Covariance-engaged Classiﬁcation of Sets via Linear Programming

Zhao Ren1, Sungkyu Jung2 and Xingye Qiao3

1University of Pittsburgh, 2Seoul National University, 3Binghamton University

Abstract: Set classiﬁcation aims to classify a set of observations as a whole, as opposed to classifying

individual observations separately. To formally understand the unfamiliar concept of binary set clas-

siﬁcation, we ﬁrst investigate the optimal decision rule under the normal distribution, which utilizes

the empirical covariance of the set to be classiﬁed. We show that the number of observations in the

set plays a critical role in bounding the Bayes risk. Under this framework, we further propose new

methods of set classiﬁcation. For the case where only a few parameters of the model drive the diﬀerence

between two classes, we propose a computationally-eﬃcient approach to parameter estimation using

linear programming, leading to the Covariance-engaged LInear Programming Set (CLIPS) classiﬁer.

Its theoretical properties are investigated for both independent case and various (short-range and long-

range dependent) time series structures among observations within each set. The convergence rates of

estimation errors and risk of the CLIPS classiﬁer are established to show that having multiple obser-

vations in a set leads to faster convergence rates, compared to the standard classiﬁcation situation in

which there is only one observation in the set. The applicable domains in which the CLIPS performs

better than competitors are highlighted in a comprehensive simulation study. Finally, we illustrate the

usefulness of the proposed methods in classiﬁcation of real image data in histopathology.

Key words and phrases: Bayes risk, (cid:96)1-minimization, Quadratic discriminant analysis, Set classiﬁcation,

Sparsity.

0
2
0
2

n
u
J

6
2

]
L
M

.
t
a
t
s
[

1
v
1
3
8
4
1
.
6
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
1.

Introduction

2

Classiﬁcation is a useful tool in statistical learning with applications in many important

ﬁelds. A classiﬁcation method aims to train a classiﬁcation rule based on the training

data to classify future observations. Some popular methods for classiﬁcation include linear

discriminant analyses, quadratic discriminant analyses, logistic regressions, support vector

machines, neural nets and classiﬁcation trees. Traditionally, the task at hand is to classify

an observation into a class label.

Advances in technology have eased the production of a large amount of data in various

areas such as healthcare and manufacturing industries. Oftentimes, multiple samples col-

lected from the same object are available. For example, it has become cheaper to obtain

multiple tissue samples from a single patient in cancer prognosis (Miedema et al., 2012).

To be explicit, Miedema et al. (2012) collected 348 independent cells, each contains obser-

vations of varying numbers (tens to hundreds) of nuclei. Here, each cell, rather than each

nucleus, is labelled as either normal or cancerous. Each observation of nuclei contains 51

measurements of shape and texture features. A statistical task herein is to classify the whole

set of observations from a single set (or all nuclei in a single cell) to normal or cancerous

group. Such a problem was coined as set classiﬁcation by Ning and Karypis (2009), studied

in Wang et al. (2012) and Jung and Qiao (2014), and was seen in the image-based pathology

literature (Samsudin and Bradley, 2010; Wang et al., 2010; Cheplygina et al., 2015; Shifat-E-

Rabbi et al., 2020) and in face recognition based on pictures obtained from multiple cameras,

sometime called image set classiﬁcation (Arandjelovic and Cipolla, 2006; Wang et al., 2012).

The set classiﬁcation is not identical to the multiple-instance learning (MIL) (Maron and

3

Lozano-P´erez, 1998; Chen et al., 2006; Ali and Shah, 2010; Carbonneau et al., 2018) as seen

by Kuncheva (2010). A key diﬀerence is that in set classiﬁcation a label is given to sets

whereas observations in a set have diﬀerent labels in the MIL setting.

While conventional classiﬁcation methods predict a class label for each observation, care

is needed in generalizing those for set classiﬁcation. In principle, more observations should

ease the task at hand. Moreover, higher-order statistics such as variances and covariances

can now be exploited to help classiﬁcation. Our approach to set classiﬁcation is to use the

extra information, available to us only when there are multiple observations. To elucidate

this idea, we illustrate samples from three classes in Fig. 1. All three classes have the same

mean, and Classes 1 and 2 have the same marginal variances. Classifying a single observation

near the mean to any of these distributions seems diﬃcult. On the other hand, classifying

several independent observations from the same class should be much easier. In particular,

a set classiﬁcation method needs to incorporate the diﬀerence in covariances to diﬀerentiate

these classes.

In this work, we study a binary set classiﬁcation framework, where a set of observations

“ tX1, . . . , XM u is classiﬁed to either

“ 1 or

Y

Y

X

“ 2. In particular, we propose set

classiﬁers that extend quadratic discriminant analysis to the set classiﬁcation setting, and

are designed to work well in set-classiﬁcation of high-dimensional data whose distributions

are similar to those in Fig. 1.

To provide a fundamental understanding of the set classiﬁcation problem, we establish

the Bayesian optimal decision rule under normality and homogeneity (i.i.d) assumptions.

This Bayes rule utilizes the covariance structure of the testing set of future observations.

4

Figure 1: A 2-dimensional toy example showing classes with no diﬀerence in the mean or

the marginal variance.

We show in Section 2 that it becomes much easier to make accurate classiﬁcation for a set

when the set size, m0, increases. In particular, we demonstrate that the Bayes risk can be

reduced exponentially in the set size m0. To the best of our knowledge, this is the ﬁrst formal

theoretical framework for set classiﬁcation problems in the literature.

Built upon the Bayesian optimal decision rule, we propose new methods of set classiﬁ-

cation in Section 3. For the situation where the dimension p of the feature vectors is much

smaller than the total number of training samples, we demonstrate that a simple plug-in

classiﬁer leads to satisfactory risk bounds similar to the Bayes risk. Again, a large set size

plays a key role in signiﬁcantly reducing the risk. In high-dimensional situations where the

number of parameters to be estimated (« p2) is large, we make an assumption that only a few

parameters drive the diﬀerence of two classes. With this sparsity assumption, we propose to

estimate the parameters in the classiﬁer via linear programming, and the resulting classiﬁers

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−202−202x1x2classl1235

are called Covariance-engaged LInear Programming Set (CLIPS) classiﬁers. Speciﬁcally, the

quadratic and linear parameters in the Bayes rule can be eﬃciently estimated under the

sparse structure, thanks to the extra observations in the training set due to having sets of

observations. Our estimation approaches are closely related to and built upon the successful

estimation strategies in Cai et al. (2011) and Cai and Liu (2011). In estimation of the con-

stant parameter, we perform a logistic regression with only one unknown, given the estimates

of quadratic and linear parameters. This allows us to implement CLIPS classiﬁer with high

computation eﬃciency.

We provide a thorough study of theoretical properties of CLIPS classiﬁers and establish

an oracle inequality in terms of the excess risk, in Section 4. In particular, the estimates

from CLIPS are shown to be consistent, and the strong signals are always selected with high

probability in high dimensions. Moreover, the excess risk can be reduced by having more

observations in a set, one of the new phenomena for set classiﬁcation, which are diﬀerent

from that obtained by naively having pooled observations.

In the conventional classiﬁcation problem where m0 “ 1, a special case of the proposed

CLIPS classiﬁer becomes a new sparse quadratic discriminant analysis (QDA) method (cf.

Fan et al., 2015, 2013; Li and Shao, 2015; Jiang et al., 2018; Qin, 2018; Zou, 2019; Gaynanova

and Wang, 2019; Pan and Mai, 2020). As a byproduct of our theoretical study, we show

that the new QDA method enjoys better theoretical properties compared to state-of-the-art

sparse QDA methods such as Fan et al. (2015).

The advantages of our set classiﬁers are further demonstrated in comprehensive simu-

lation studies. Moreover, we provide an application to histopathology in classifying sets of

6

nucleus images to normal and cancerous tissues in Section 5. Proofs of main results and

technical lemmas can be found in the supplementary material. Also present in the supple-

mentary material is a study on the case where observations in a set demonstrate certain

spatial and temporal dependent structures. There, we utilize various (both short- and long-

range) dependent time series structures within each set by considering a very general vector

linear process model.

2. Set Classiﬁcation

We consider a binary set-classiﬁcation problem. The training sample tp

Xi,

YiquN

i“1 contains

N sets of observations. Each set,

Xi “ tXi1, Xi2, . . . , XiMiu Ă Rp, corresponds to one object,
and is assumed to be from one of the two classes. The corresponding class label is denoted

by

Yi P t1, 2u. The number of observations within the ith set is denoted by Mi and can
:q, the goal of set

be diﬀerent among diﬀerent sets. Given a new set of observations p

:,

Y
: using a classiﬁcation rule φp¨q P t1, 2u

X

classiﬁcation is to predict

: accurately based on

Y

X

trained on the training sample.

To formally introduce set classiﬁcation problem and study its fundamental properties,

we start with a setting in which the sets in each class are homogeneous in the sense that all

the observations in a class, regardless of the set membership, follow the same distribution

independently. Speciﬁcally, we assume both the N sets tp

Xi,

YiquN

i“1 and the new set p

:,

:q

Y

X

are generated in the same way as p

X

,

Y

q independently. To describe the generating process

of p

X

,

Y

q, we denote the marginal class probabilities by π1 “ prp

Y

“ 1q and π2 “ prp

Y

“ 2q,

and the marginal distribution of the set size M by pM . We assume that the random variables

2.1 Covariance-engaged Set Classiﬁers7

M and

Y

are independent. In other words, the class membership

can not be predicted just

Y

based on the set size M . Conditioned on M “ m and

“ y, observations X1, X2, . . . , XM

Y

in the set

X

are independent and each distributed as fy.

2.1 Covariance-engaged Set Classiﬁers

Suppose that there are M : “ m observations in the set

: “ tX :

1, . . . , X :

mu that is to be

X
:. The Bayes optimal decision rule

classiﬁed (called testing set), and its true class label is

Y
: “ tx1, . . . , xmu to Class 1 if the conditional class probability of Class 1

classiﬁes the set

X

is greater than that of Class 2, that is, prp

: “ 1 | M : “ m, X :

j “ xj, j “ 1, . . . , mq ą 1{2.

This is equivalent to π1pM pmq

ś

m
j“1 f1pxjq ą π2pM pmq

ś

m
j“1 f2pxjq, due to Bayes theorem

Y

and the independence assumption among

Y

: and M :. Let us now assume that the conditional

distributions are both normal, that is, f1 „ N pµ1, Σ1q and f2 „ N pµ2, Σ2q. Then the Bayes

optimal decision rule depends on the quantity

gpx1, . . . , xmq “

“

1
m

1
m

#

log

π1pM pmq
π2pM pmq

+

ś
ś

m
j“1 f1pxjq
m
j“1 f2pxjq

logpπ1{π2q ´

1
2

logp|Σ1|{|Σ2|q ´

1
2

1 Σ´1
µT

1 µ1 `

` pΣ´1

1 µ1 ´ Σ´1

2 µ2qT ¯x `

1
2

¯xT pΣ´1

2 ´ Σ´1

1 q¯x `

1
2
1
2

2 Σ´1
µT

2 µ2

trtpΣ´1

2 ´ Σ´1

1 qSu. (2.1)

Here |Σk| denotes the determinant of the matrix Σk for k “ 1, 2, ¯x “

ř

m
j“1 xj{m and

ř

S “

m
j“1pxj ´ ¯xqpxj ´ ¯xqT {m are the sample mean and sample covariance of the testing set.

Note that the realization

X

: “ tx1, x2, . . . , xmu implies both the number of observations m

and the i.i.d. observations xj for j “ 1, . . . , m. The Bayes rule can be expressed as

2.1 Covariance-engaged Set Classiﬁers8

:q “ 2 ´ 1tgpx1, . . . , xmq ą 0u, where

φBp

X

(2.2)

gpx1, . . . , xmq “

1
m

logpπ1{π2q ` β0 ` βT ¯x ` ¯xT

¯x{2 ` trp

∇

∇

Sq{2,

in which the constant coeﬃcient β0 “ t´ logp|Σ1|{|Σ2|q ´ µT

1 Σ´1

1 µ1 ` µT

2 Σ´1

2 µ2u{2 P R,

the linear coeﬃcient vector β “ Σ´1

1 µ1 ´ Σ´1

2 µ2 P Rp and the quadratic coeﬃcient matrix

“ Σ´1

2 ´ Σ´1

1 P Rpˆp. The Bayes rule φB under the normal assumption in (2.2) uses the

∇

summary statistics m, ¯x and S of

:.

X

We refer to (2.2) and any estimated version of it as a covariance-engaged set classiﬁer. In

Section 3, several estimation approaches for β0, β and

will be proposed. In this section,

∇

we further discuss a rationale for considering (2.2).

The covariance-engaged set classiﬁer (2.2) resembles the conventional QDA classiﬁer. As

a natural alternative to (2.2), one may consider the sample mean ¯x as a representative of

the testing set and apply QDA to ¯x directly to make a prediction. In other words, one is

about to classify this single observation ¯x to one of the two normal distributions, that is,

1 „ N pµ1, Σ1{mq and f 1
f 1

2 „ N pµ2, Σ2{mq. This simple idea leads to

φB,¯xp

X

:q “ 2 ´ 1tgQDAp¯xq ą 0u, where

gQDAp¯xq “

1
m

logpπ1{π2q ` β1

0 ` βT ¯x ` ¯xT

¯x{2,

∇

(2.3)

in which β1

0 “ t´ 1

m logp|Σ1|{|Σ2|q ´ µT

1 Σ´1

1 µ1 ` µT

2 Σ´1

2 µ2u{2. One major diﬀerence between

(2.2) and (2.3) is that the term trp

∇

Sq{2 is absent from (2.3). Indeed, the advantage of

(2.2) over (2.3) comes from the extra information in the sample covariance S of

:. In the

X

regular classiﬁcation setting, (2.2) coincides with (2.3) since trp

Sq{2 vanishes when

∇

: is

X

2.2 Bayes Risk9

a singleton.

Given multiple observations in the testing set, another natural approach is a majority

vote applied to the QDA decisions of individual observations:

#

φM V p

X

:q “ 2 ´ 1

1
m

mÿ

j“1

+

signrgQDApxjqs ą 0

,

(2.4)

where signptq “ 1, 0, ´1 for t ą 0, t “ 0 and t ă 0 respectively.

In contrast, since

ř

:q “ 1
m

gp

X

m
j“1 gQDApxjq, our classiﬁer (2.2) predicts the class label by a weighted vote

of individual QDA decisions. In this sense, the majority voting scheme (2.4) can be viewed

as a discretized version of (2.2). In Section 5, we demonstrate that our set classiﬁer (2.2)

performs signiﬁcantly better than (2.4).

Remark 1. We have assumed that M and

Y

are independent in the setting. In fact, this

assumption is not essential and can be relaxed.

In a more general setting, there can be

two diﬀerent distributions of M , pM 1pmq and pM 2pmq conditional on

“ 1 and

“ 2

Y

Y

respectively. Our analysis throughout the paper remains the same except that they would

replace two identical factors pM pmq in the ﬁrst equality of (2.1). If pM 1pmq and pM 2pmq are

dramatically diﬀerent, then the classiﬁcation is easier as one can make decision based on the

observed value of m. In this paper, we only consider the more diﬃcult setting where

and

Y

M are independent.

2.2 Bayes Risk

We show below an advantage of having a set of observations for prediction, compared to

having a single observation. For this, we suppose for now that the parameters µk and Σk,

k “ 1, 2, are known and make the following assumptions. Denote λmaxpAq and λminpAq as

the greatest and smallest eigenvalues of a symmetric matrix A.

2.2 Bayes Risk10

Condition 1. The spectrum of Σk is bounded below and above: there exists some universal

constant Ce ą 0 such that C ´1

e ď λminpΣkq ď λmaxpΣkq ď Ce for k “ 1, 2.

Condition 2. The support of pM is bounded between cmm0 and Cmm0, where cm and

Cm are universal constants and m0 “ EpM q. In other words, pM paq “ 0 for any integer

a ă cmm0 or ą Cmm0. The set size m0 can be large or growing when a sequence of models

are considered.

Condition 3. The prior class probability is bounded away from 0 and 1: there exists a

universal constant 0 ă Cπ ă 1{2 such that Cπ ď π1, π2 ď 1 ´ Cπ.

We denote RBk “ prpφBp

:q ‰ k |

: “ kq as the risk of the Bayes classiﬁer (2.2) given

Y
: “ k. Let δ “ µ2 ´ µ1. For a matrix B P Rpˆp, we denote }B}F “ p

X

Y
as its Frobenius norm, where Bij is its ijth element. For a vector a P Rp, we denote

ř
p
i“1

ř
p
j“1 B2

ijq1{2

}a} “ p

ř
p
i“1 a2

i q1{2 as its (cid:96)2 norm. The quantity Dp “ p}

F ` }δ}2q1{2 plays an important
}2

∇

role in deriving a convergence rate of the Bayes risk RB “ π1RB1 ` π2RB2. Although the

Bayes risk does not have a closed form, we show that under mild assumptions, it converges

to zero at a rate on the exponent.

Theorem 1. Suppose that Conditions 1-3 hold. If D2

pm0 is suﬃciently large, then RB ď

`

4 exp

´c1m0D2
p

˘

for some small constant c1 ą 0 depending on Ce, cm and Cπ only.

In

particular, as D2

pm0 Ñ 8, we have RB Ñ 0.

2.2 Bayes Risk11

The signiﬁcance of having a set of observations is illustrated by this fundamental theorem.

When pM p1q “ 1, which implies M : ” 1 and m0 “ 1, Theorem 1 provides a Bayes risk bound

RB ď 4 exp

˘

`

´c1D2
p

for the theoretical QDA classiﬁer in the regular classiﬁcation setting.

To guarantee a small Bayes risk for QDA, it is clear that D2

p must be suﬃciently large. In

comparison, for the set classiﬁcation to be successful, we may allow D2

p to be very close to

zero, as long as m0D2

p is suﬃciently large. The Bayes risk of φB can be reduced exponentially

in m0 because of the extra information from the set.

We have discussed an alternative classiﬁer via using the sample mean ¯x as a representative

of the testing set, leading to φB,¯x (2.3). The following proposition quantiﬁes its risk, which

has a slower rate than that of Bayes classiﬁer RB.

Proposition 1. Suppose that Conditions 1-3 hold. Denote the risk of classiﬁer φB,¯x in (2.3)

as R¯x. Assume }

F ` m0}δ}2 is suﬃciently large. Then R¯x ď 4 exp p´c1p}
}2

∇

F ` m0}δ}2qq
}2

∇

for some small constant c1 ą 0 depending on Ce, cm and Cπ only. In addition, the rate on

the exponent cannot be improved in general, i.e., R¯x ě exp p´c2p}

∇

F ` m0}δ}2qq for some
}2

small constant c2 ą 0.

Remark 2. Compared to the result in Theorem 1, the above proposition implies that clas-

siﬁer φB,¯x needs a stronger assumption but has a slower rate of convergence when the mean

diﬀerence m0}δ}2 is dominated by the covariance diﬀerence }

}2
F . After all, this natural ¯x-

∇

based classiﬁcation rule only relies on the ﬁrst moment of the data set

: while the suﬃcient

X

statistics, the ﬁrst two moments, are fully used by the covariance-engaged classiﬁer in (2.2).

3. Methodologies

12

We now consider estimation procedures for φB based on N training sets tp

Xi,

YiquN

i“1. In

Section 3.1, we ﬁrst consider a moderate-dimensional setting where p ď c0m0N with a

suﬃciently small constant c0 ą 0.

In this case we apply a naive plug-in approach using

natural estimators of the parameters πk, µk and Σk. A direct estimation approach using linear

programming, suitable for high-dimensional data, is introduced in Section 3.2. Hereafter,

p “ ppN q and m0 “ m0pN q are considered as functions of N as N grows.

3.1 Naive Estimation Approaches

The prior class probabilities π1 and π2 can be consistently estimated by the class proportions

N

in the training data, ˆπ1 “ N1{N and ˆπ2 “ N2{N , where Nk “
ř

Yi “ ku. Let nk “
Yi “ ku denote the total sample size for Class k “ 1, 2. The set membership is
ignored at the training stage, due to the homogeneity assumption. Note nk, n1 ` n2 and

i“1 Mi1t

ř

N
i“1

1t

Nk are random while N is deterministic. One can obtain consistent estimators of µk and

Σk based on the training data and plug them in (2.2). It is natural to use the maximum

likelihood estimators given nk,

ÿ

ˆµk “

pi,jq:

Yi“k

Xij{nk and ˆΣk “

ÿ

pi,jq:

Yi“k

tpXij ´ ˆµkqpXij ´ ˆµkqT u{nk.

(3.5)

For classiﬁcation of

X

is estimated by

: “ tX :

1, . . . , X :

M :u with M : “ m, X :

i “ xi, the set classiﬁer (2.2)

"

:q “ 2 ´ 1

ˆφp

X

1
m

logpˆπ1{ˆπ2q ` ˆβ0 ` ˆβT ¯x ` ¯xT ˆ
∇

¯x{2 ` trp ˆ
∇

*

Sq{2 ą 0

,

(3.6)

where ˆβ0 “ ´ 1
2

!
logp| ˆΣ1|{| ˆΣ2|q ´ ˆµT
1

ˆΣ´1

1 ˆµ1 ` ˆµT
2

ˆΣ´1

2 ˆµ2

3.1 Naive Estimation Approaches13
)
, ˆβ “ ˆΣ´1

1 ˆµ1 ´ ˆΣ´1

“ ˆΣ´1

2 ´

2 ˆµ2 and ˆ
∇

ˆΣ´1

1 . In (3.6) we have assumed p ă nk so that ˆΣk is invertible.

The generalization error of set classiﬁer (3.6) is ˆR “ π1 ˆR1` π2 ˆR2 where ˆRk “ prp ˆφp

:q ‰

X

k |

Y

: “ kq. The classiﬁer itself depends on the training data tp

Xi,

YiquN

i“1 and hence is

random. In the equation above, pr is understood as the conditional probability given the

training data. Theorem 2 reveals a theoretical property of ˆR in a moderate-dimensional

setting which allows p, N, m0 to grow jointly. This includes the traditional setting in which

p is ﬁxed.

Theorem 2. Suppose that Conditions 1-3 hold. For any ﬁxed L ą 0, if D2

pm0 ě C0 for

some suﬃciently large C0 ą 0 and p ď c0N m0, p2{pN m0D2

pq ď c0, log p ď c0N for some

suﬃciently small constant c0 ą 0, then with probability at least 1 ´ Opp´Lq we have ˆR ď

`

4 exp

´c1m0D2
p

˘

for some small constant c1 ą 0 depending on Cπ, cm, L and Ce.

In Theorem 2, large values of m0 not only relax the assumption on Dp but also reduce

the Bayes risk exponentially in m0 with high probability. A similar result for QDA, where

Mi “ M : ” 1 and m0 “ 1, was obtained in Li and Shao (2015) under a stronger assumption

p2{pN D2

pq Ñ 0.

For the high-dimensional data where p “ ppN q " N m0 and hence p ą nk with probability

1 for k “ 1, 2 by Condition 2, it is problematic to plug in the estimators (3.5) since ˆΣk is rank

deﬁcient with probability 1. A simple remedy is to use a diagonalized or enriched version

of ˆΣk, deﬁned by ˆΣkpdq “ diagtpˆσk,iiqi“1,...,pu or ˆΣkpeq “ ˆΣk ` δIp, where δ ą 0 and Ip is a

p ˆ p identity matrix. Both ˆΣkpdq and ˆΣkpeq are invertible. However, to our best knowledge,

no theoretical guarantee has been obtained without some structural assumptions.

3.2 A Direct Approach via Linear Programming

3.2 A Direct Approach via Linear Programming14

To have reasonable classiﬁcation performance in high-dimensional data analysis, one usually

has to take advantage of certain extra information of the data or model. There are often

cases where only a few elements in

“ Σ´1

2 ´ Σ´1
1

∇

and β “ Σ´1

1 µ1 ´ Σ´1

2 µ2 truly drive

the diﬀerence between the two classes. A naive plug-in method proposed in Section 3.1 has

ignored such potential structure of the data. We assume that both

and β are known to

∇

be sparse such that only a few elements of those are nonzero. In light of this, the Bayes

decision rule (2.2) implies the dimension of the problem can be signiﬁcantly reduced, which

makes consistency possible even in the high-dimensional setting.

We propose to directly estimate the quadratic term

, the linear term β and the constant

∇

β0 coeﬃcients respectively, taking advantage of the assumed sparsity. As the estimates are

eﬃciently calculated by linear programming, the resulting classiﬁers are called Covariance-

engaged Linear Programming Set (CLIPS) classiﬁers.

We ﬁrst deal with the estimation of the quadratic term

∇

“ Σ´1

2 ´ Σ´1

1 , which is the

diﬀerence between the two precision matrices. We use some key techniques developed in the

literature of precision matrix estimation (cf. Meinshausen and B¨uhlmann, 2006; Bickel and

Levina, 2008; Friedman et al., 2008; Yuan, 2010; Cai et al., 2011; Ren et al., 2015). These

methods estimate a single precision matrix with a common assumption that the underlying

true precision matrix is sparse in some sense. For the estimation of the diﬀerence, we propose

to use a two-step thresholded estimator.

As the ﬁrst step, we adopt the CLIME estimator (Cai et al., 2011) to obtain initial

estimators ˜Ω1 and ˜Ω2 of the precision matrices Σ´1
1

and Σ´1

2 . Let }B}1 “

ř

i,j |Bij| and

}B}8 “ maxi,j |Bij| be the vector (cid:96)1 norm and vector supnorm of a pˆp matrix B respectively.

3.2 A Direct Approach via Linear Programming15

The CLIME estimators are deﬁned as

˜Ωk “ argmin
ΩPRpˆp

}Ω}1 subject to } ˆΣkΩ ´ I}8 ă λ1,N , k “ 1, 2,

(3.7)

for some λ1,N ą 0.

Having obtained ˜Ω1 and ˜Ω2, in the second step, we take a thresholding procedure on

their diﬀerence, followed by a symmetrization to obtain our ﬁnal estimator ˜
∇

“ p ˜

∇ijq where

∇ij “ mint ˘
˜

∇ij, ˘

∇jiu, ˘

!ˇ
ˇ
ˇ ˜Ω2,ij ´ ˜Ω1,ij
∇ij “ p ˜Ω2,ij ´ ˜Ω1,ijq1

ˇ
ˇ
ˇ ą λ1

1,N

)
,

(3.8)

for some thresholding level λ1

1,N ą 0.

Although this thresholded CLIME diﬀerence estimator is obtained by ﬁrst individually

estimating Σ´1

k , we emphasize that the estimation accuracy only depends on the sparsity

of their diﬀerence

∇

rather than the sparsity of either Σ´1

1 or Σ´1

2 under a relatively mild

bounded matrix (cid:96)1 norm condition. We will show in Theorem 3 in Section 4 that if the true

precision matrix diﬀerence

is negligible, ˜
∇
method described in (3.12) becomes a linear classiﬁer adaptively. The computation of ˜
∇

“ 0 with high probability. When ˜
∇

“ 0, our

∇

(3.8) is fast, since the ﬁrst step (CLIME) can be recast as a linear program and the second

step is a simple thresholding procedure.

Remark 3. As an alternative, one can also consider a direct estimation of

that does

∇

not rely on individual estimates of Σ´1

k . For example, by allowing some deviations from the

Σ2 ´ Σ1 ` Σ2 “ 0, Zhao et al. (2014) proposed to minimize the vector (cid:96)1 norm of

ZCL P argminB }B}1 subject to } ˆΣ1B ˆΣ2 ´ ˆΣ1 ` ˆΣ2}8 ď λ2

1,n,

identity Σ1∇
. Speciﬁcally, they proposed ˜
∇

∇
where λ2

1,n is some thresholding level. This method, however, is computationally expensive

3.2 A Direct Approach via Linear Programming16

(as it has Opp2q number of linear constraints when casted to linear programming) and can

only handle relatively small size of p. See also Jiang et al. (2018). We chose to use (3.8)

mainly because of fast computation.

Next we consider the estimation of the linear coeﬃcient vector β “ β1 ´ β2, where

βk “ Σ´1

k µk, k “ 1, 2. In the literature of sparse QDA and sparse LDA, typical sparsity

assumptions are placed on µ1 ´ µ2 and Σ1 ´ Σ2 (see Li and Shao, 2015) or placed on both

β1 and β2 (see, for instance Cai and Liu, 2011; Fan et al., 2015). In the latter case, β is also

sparse as it is the diﬀerence of two sparse vectors. For the estimation of β, we propose a new

method which directly imposes sparsity on β, without specifying the sparsity for µk, Σk or

βk except for some relatively mild conditions (see Theorem 4 for details.)

The true parameter βk satisﬁes Σkβk ´µk “ 0. However, due to the rank-deﬁciency of ˆΣk,

there are either none or inﬁnitely many θk’s that satisfy an empirical equation ˆΣkθk ´ ˆµk “ 0.

Here, ˆµk and ˆΣk are deﬁned in (3.5). We relax this constraint and seek a possibly non-

sparse pair pθ1, θ2q with the smallest (cid:96)1 norm diﬀerence. We estimate the coeﬃcients β by

˜β “ ˜β1 ´ ˜β2, where

p ˜β1, ˜β2q “ argmin

}θ1 ´ θ2}1 subject to } ˆΣkθk ´ ˆµk}8 ă λ2,N , k “ 1, 2,

(3.9)

pθ1,θ2q:}θk}1ďL1

where L1 is some suﬃciently large constant introduced only to ease theoretical evaluations. In

practice, the constraint }θk}1 ď L1 can be removed without aﬀecting the solution. Note that

Jiang et al. (2018) proposed to estimate pΣ´1

1 `Σ´1

2 qpµ1 ´µ2q rather than β “ Σ´1

1 µ1 ´Σ´1

2 µ2.

The direct estimation approach for β above shares some similarities with that of Cai and

Liu (2011), especially in the relaxed (cid:96)8 constraint. However Cai and Liu (2011) focused on

a direct estimation of Σ´1pµ2 ´ µ1q for linear discriminant analysis in which Σ “ Σ1 “ Σ2,

3.2 A Direct Approach via Linear Programming17

while we target on Σ´1

2 µ2 ´ Σ´1

1 µ1 instead. Our procedure (3.9) can be recast as a linear

programming problem (see, for example, Candes and Tao, 2007; Cai and Liu, 2011) and is

computationally eﬃcient.

Finally, we consider the estimation of the constant coeﬃcient β0. The conditional class

probability ηpx1, . . . , xmq “ prp

Y

“ 1 | M “ m, Xi “ xi, i “ 1, . . . , mq that a set belongs

to Class 1 given

“ tx1, . . . , xmu can be evaluated by the following logit function,

X

"

log

ηpx1, . . . , xmq
1 ´ ηpx1, . . . , xmq

*

“ log

π1
π2

` log

"ś
ś

*

m
i“1 f1pxiq
m
i“1 f2pxiq

“ logpπ1{π2q ` mpβ0 ` ¯xT β `

1
2

¯xT

∇

¯x `

1
2

trp

Sqq,

∇

where ¯x and S are the sample mean and covariance of the set tx1, . . . , xmu respectively.

Having obtained our estimators ˜
∇

and ˜β from (3.8) and (3.9), and estimated ˆπ1 and ˆπ2 by

N1{N and N2{N from the training data, we have only a scalar β0 undecided. We may ﬁnd an

estimate ˜β0 by conducting a simple logistic regression with dummy independent variable Mi

´
¯X T
i

¯

and oﬀset logpˆπ1{ˆπ2q ` Mi

˜β ` ¯X T
i

˜
∇

¯Xi{2 ` trp ˜
∇

Siq{2

for the ith set of observations in

the training data, where Mi, ¯Xi, and Si are sample size, sample mean, and sample covariance

of the ith set. In particular, we solve

˜β0 “ argmin

θ0PR

(cid:96)pθ0 | tp

Xi,

YiquN

i“1, ˜β, ˜
∇

q, where the negative log-likelihood is

i“1, ˜β, ˜
∇
ˆ

q

YiquN

(cid:96)pθ0 | tp
Xi,
´
Nÿ
Yi ´ 2qMi
p

1
N

“

i“1
„

"

θ0 `
ˆ

` log

1 ` exp

Mi

θ0 `

logpˆπ1{ˆπ2q
Mi

logpˆπ1{ˆπ2q
Mi

` ¯X T
i

˜β ` ¯X T
i

˜
∇

¯Xi{2 ` trp ˜
∇

Siq{2

` ¯X T
i

˜β ` ¯X T
i

˜
∇

¯Xi{2 ` trp ˜
∇

Siq{2

(3.10)

(3.11)

˙

˙* ¯

18

Since there is only one independent variable in the logistic regression above, the optimization

can be easily and eﬃciently solved.

For the purpose of evaluating theoretical properties, we apply the sample splitting tech-

nique (Wasserman and Roeder, 2009; Meinshausen and B¨uhlmann, 2010). Speciﬁcally, we

randomly choose the ﬁrst batch of N1{2 and N2{2 sets from two classes in the training data

to obtain estimators ˜
∇

and ˜β using (3.8) and (3.9). Then ˜β0 is estimated based on the

second batch along with ˜
∇

and ˜β using (3.10). We plug all the estimators in (3.8), (3.9) and

(3.10) into the Bayes decision rule (2.2) and obtain the CLIPS classiﬁer,

"

:q “ 2 ´ 1

˜φp

X

logpˆπ1{ˆπ2q
m

` ˜β0 ` ˜βT ¯x ` ¯xT ˜
∇

¯x{2 ` trp ˜
∇

*

Sq{2 ą 0

,

(3.12)

where ¯x and S are sample mean and covariance of

: and M : “ m is its size.

X

4. Theoretical Properties of CLIPS

In this section, we derive the theoretical properties of the estimators from (3.8)–(3.10) as well

as generalization errors for the CLIPS classiﬁer (3.12). In particular, we demonstrate the

advantages of having sets of independent observations in contrast to classical QDA setting

with individual observations under the homogeneity assumption of Section 2. Parallel results

under various time series structures can be found in the supplementary material.

To establish the statistical properties of the thresholded CLIME diﬀerence estimator ˜
∇

deﬁned in (3.8), we assume that the true quadratic parameter

∇

than sq nonzero entries,

“ Σ´1

2 ´ Σ´1

1 has no more

P

FM0psqq “ tA “ paijq P Rpˆp, symmetric :

∇

pÿ

i,j“1

1taij ‰ 0u ď squ.

(4.13)

19

Denote supppAq as the support of the matrix A. We summarize the estimation error and a

subset selection result in the following theorem.

Theorem 3. Suppose Conditions 1-3 hold. Moreover, assume

P

∇

FM0psqq, }Σ´1

k }(cid:96)1 ď C(cid:96)1

with some constant C(cid:96)1 ą 0 for k “ 1, 2 and log p ď c0N with some suﬃciently small constant

c0 ą 0. Then for any ﬁxed L ą 0, with probability at least 1 ´ Opp´Lq, we have that

} ˜
∇
} ˜
∇
} ˜
∇

´

}8 ď 2λ1

1,N ,

∇

}F ď 2

?

sqλ1

1,N ,

´

∇

}1 ď 2sqλ1

1,N ,

´

∇

and λ1

1,N ě 8C(cid:96)1λ1,N in (3.8), where C depends on L, Ce, Cπ

as long as λ1,N ě CC(cid:96)1

b

log p
N m0

and cm only. Moreover, we have prpsuppp ˜
∇

q Ă suppp

∇

qq “ 1 ´ Opp´Lq.

Remark 4. The parameter space

FM0psqq can be easily extended into an entry-wise (cid:96)q
ball or weak (cid:96)q ball with 0 ă q ă 1 (Abramovich et al., 2006) and the estimation results in

Theorem 3 remain valid with appropriate sparsity parameters. The subset selection result

also remains true and the support of ˜
∇

a

contains those important signals of

above the

∇

noise level

plog pq{N m0. To simplify the analysis, we only consider (cid:96)0 balls in this work.

Remark 5. Theorem 3 implies that both the error bounds of estimating

under vector (cid:96)1

∇

norm and Frobenius norm rely on the sparsity sq imposed on

rather than those imposed

∇

on Σ´1

2 or Σ´1

1 . Therefore, even if both Σ´1

2 and Σ´1

1 are relatively dense, we still have an

accurate estimate of

as long as

∇

∇

is very sparse and C(cid:96)1 is not large.

The proof of Theorem 3, provided in the supplementary material, partially follows from

Cai et al. (2011).

20

Next we assume β “ β1 ´ β2 is sparse in the sense that it belongs to the sl-sparse ball,

β P

F0pslq “ tα “ pajq P Rp :

pÿ

j“1

1tαj ‰ 0u ď slu.

(4.14)

Theorem 4 gives the rates of convergence of the linear coeﬃcient estimator ˜β in (3.9) under

the (cid:96)1 and (cid:96)2 norms. Both depend on the sparsity of β only rather than that of β1 or β2.

Theorem 4. Suppose Conditions 1-3 hold. Moreover, assume that β P

F0pslq, log p ď c0N ,
}βk}1 ď Cβ and }µk} ď Cµ with some constants Cβ, Cµ ą 0 for k “ 1, 2 and some suﬃciently

small constant c0 ą 0. Then for any ﬁxed L ą 0, with probability at least 1 ´ Opp´Lq, we

have that

as long as λ2,N ě C 1

b

log p
N m0

} ˜β ´ β}1 ď C 2C(cid:96)1slλ2,N ,

} ˜β ´ β} ď C 2C(cid:96)1

?

slλ2,N ,

in (3.9), where maxt}Σ´1

1 }(cid:96)1, }Σ´1

2 }(cid:96)1u ď C(cid:96)1 and C 2, C 1 depend

on L, Ce, cm, Cπ, Cβ and Cµ only.

Remark 6. The parameter space

F0psq can be easily extended into an (cid:96)q ball or weak (cid:96)q
ball with 0 ă q ă 1 as well and the results in Theorem 4 remain valid with appropriate

sparsity parameters. We only focus on

F0psq in this paper to ease the analysis.

Lastly, we derive the rate of convergence for estimating the constant coeﬃcient β0. Since

˜β0 is obtained by maximizing the log-likelihood function after plugging ˜β and ˜
∇

in (3.10),

the behavior of our estimator ˜β0 critically depends on the accuracy for estimating β and

.
∇
Theorem 5 provides the result for ˜β0 based on certain general initial estimators ˜β and ˜
∇

with the following mild condition.

Condition 4. The expectation of the conditional variance of class label

21

is

given

X

Y

bounded below, that is, E pVarp

Y

|

X

qq ą Clog ą 0, where Clog is some universal constant.

Theorem 5. Suppose Conditions 1-4 hold, log p ď c0N with some suﬃciently small constant

c0 ą 0 and }µk} ď Cµ with some constant Cµ ą 0 for k “ 1, 2. Besides, we have some initial
a

estimators ˜β, ˜
∇

, ˆπ1 and ˆπ2 such that m0p1 `

plog pq{m0q} ˜β ´ β} ` m0p1 ` plog pq{m0q} ˜
∇

´

}1 ` maxk“1,2 |πk ´ ˆπk| ď Cp for some suﬃciently small constant Cp ą 0 with probability at

∇
least 1 ´ Opp´Lq. Then, with probability at least 1 ´ Opp´Lq, we have

ˇ
ˇ
ˇ ˜β0 ´ β0

ˇ
ˇ
ˇ ď Cδ

˜

c

p1 `

log p
m0

q} ˜β ´ β} ` p1 `

log p
m0

q} ˜
∇

´

}1 ` max
k“1,2

∇

|πk ´ ˆπk|
m0

`

log p
N m2
0

,

d

¸

where constant Cδ depends on L, Ce, Cπ, Clog, Cµ, Cm and cm.

Remark 7. Condition 4 is determined by our data generating process stated in Section

2.1. It is satisﬁed when the classiﬁcation problem is non-trivial. For example, it is valid if

prtC 1 ă prp

Y

“ 1 |

X

q ă 1 ´ C 1u ą C with some constants C and C 1 P p0, 1q. As a matter

of fact, Condition 4 is weaker than the typical assumption: Clog ă prp

Y

“ 1 |

X

q ă 1 ´ Clog

with probability 1 for

X

, which is often seen in the literature of logistic regression. See, for

example, Fan and Lv (2013) and Fan et al. (2015).

Theorems 3, 4 and 5 demonstrate the estimation accuracy for the quadratic, linear and

constant coeﬃcients in our CLIPS classiﬁer (3.12) respectively. We conclude this section by

establishing an oracle inequality for its generalization error via providing a rate of conver-

gence of the excess risk. To this end, we deﬁne the generalization error of CLIPS classiﬁer

as ˜R “ π1 ˜R1 ` π2 ˜R2, where ˜Rk “ prp ˜φp

X

:q ‰ k |

Y

: “ kq is the probability that a new

set observation from Class k is misclassiﬁed by the CLIPS classiﬁer ˜φp

conditional probability given the training data tp

Xi,

YiquN

i“1 which ˜φp
X

22

:q. Again pr is the

X
:q depends on.

We introduce some notation related to the Bayes decision rule in (2.2). Recall that

given M : “ m, the Bayes decision rule φBp

X

:q solely depends on the sign of the function

:q “ 1

m logpπ1{π2q ` β0 ` βT ¯x ` ¯xT

gp

X

¯x{2 ` trp

∇

∇

Sq{2. We deﬁne by Fk,m the conditional

cumulative distribution function of the oracle statistic gp

X

:q given that M : “ m and

: “ k.

Y

The upper bound of the ﬁrst derivatives of F1,m and F2,m for all possible m near 0 is denoted

by dN ,

#

dN “

max
mPrcmm0,Cmm0s, k“1,2

sup
tPr´δ0,δ0s

ˇ
ˇF 1

ˇ
ˇ
k,mptq

+

,

where δ0 is any suﬃciently small constant. The value of dN is determined by the generating

process and is usually small whenever the Bayes rule performs reasonably well. According

to Theorems 3, 4 and 5, with probability at least 1 ´ Opp´Lq, our estimators satisfy that

ΞN :“ p1 `

c

log p
m0

q} ˜β ´ β} ` p1 `

log p
m0

´

q} ˜
∇
a

}1 ` max
k“1,2

∇

|ˆπk ´ πk|
m0

ˇ
ˇ
ˇ ˜β0 ´ β0

ˇ
ˇ
ˇ “ OpκN q,

`

?

slλ2,N `

a

plog pq{pN m2

0q. It

where κN :“ p1 ` plog pq{m0qsqλ1

1,N ` p1 `

plog pq{m0qC(cid:96)1

turns out the quantity κN dN is the key to obtain the oracle inequality. Condition 5 below

guarantees that the assumptions of Theorem 5 are satisﬁed with high probability in our

settings.

Condition 5. Suppose κN m0 ď c0 and κN dN ď c0 with some suﬃciently small constant

c0 ą 0.

Theorem 6 below reveals the oracle property of CLIPS classiﬁer and provides a rate of

convergence of the excess risk, that is, the generalization error of CLIPS classiﬁer less the

Bayes risk RB deﬁned in Section 2.2.

23

Theorem 6. Suppose that the assumptions of Theorems 3 and 4 hold and that Conditions

4–5 also hold. Then with probability at least 1 ´ Opp´Lq, we have the oracle inequality

˜R ď RB ` CgpκN dN ` p´Lq,

where constant Cg depends on L, Ce, Cπ, Clog, Cβ, Cm, cm and Cµ only. In particular, we have

˜R converges to the Bayes risk RB in probability as N goes to inﬁnity.

Theorem 6 implies that with high probability, the generalization error of CLIPS classiﬁer

is close to the Bayes risk with rate of convergence no slower than κN dN . In particular, when-

ever the the quantities dN and C(cid:96)1 are bounded by some universal constant, the thresholding

levels λ1

1,N “ Op

log p{pm0N qq and λ2,N “ Op

log p{pm0N qq yield the rate of convergence

a

a

κN dN in the order of

a

a

p1 `

plog pq{m0q

log p{pm0N q

sl ` p1 ` plog pq{m0q

log p{pm0N qsq.

(4.15)

?

a

The advantage of having large m0 can be understood by investigating (4.15) as a function

of m0. Indeed, the leading term of (4.15) is

c

?

c

log p
m3{2
c
0
log p
m0
log p
N

p

?

c

1
m0

log p
N

sq,

if m0 ď log p ¨ mint1,

s2
q
sl

u;

log p
N

?

sl,

if log p ¨

s2
q
sl

ď m0 ď log p;

sl ` sqq,

if log p ď m0.

To illustrate the decay rate, we assume sl ě s2

q. Then as m0 increases, the error decreases

at the order of m3{2

0 up to certain point log p ¨ s2

q
sl

, and then decreases at the order of m0 up

24

to another point log p. When m is large enough so that m0 ě log p, then the error decreases

at the order of

?

m0.

To further emphasize the advantage of having sets of observations, we compare a general

case m0 “ m˚ where log p ď m˚ with the special case that m0 “ 1, i.e., the regular

QDA situation. Then the quantity κN with m˚ has a faster decay rate with a factor of

order between

?

m˚ log p and

?

m˚ log p (depending on the relationship between sl and sq)

compared to the m0 “ 1 case, thanks to the extra observations within each set.

Remark 8. The above discussion reveals that in high-dimensional setting the beneﬁt of the

set-classiﬁcation cannot be simply explained by having N ˚ “ N m0 independent observations

instead of having only N individual observations as in the classical QDA setting. Indeed, if

we have N ˚ individual observations in the classical QDA setting, then the implied rate of

convergence would be either log p

b

log p
N m0

sq (if log p ¨ s2

q ě sl) or

?

log p

?

sl (otherwise),

log p
N m0

b

which is slower than the one provided in equation (4.15).

Remark 9. It is worthwhile to point out that even in the special QDA situation where

m0 “ 1, due to the sharper analysis, our result is still new and the established rate of
convergence plog pq{N 1{2?

sl ` plog pq3{2{N 1{2sq in Theorem 6 is at least as good as the one

plog pq3{2{N 1{2psq ` slq derived in the oracle inequality of Fan et al. (2015) under similar

assumptions. Whenever sl ą sq, our rate is even faster with a factor of order

?

sl log p than

that in Fan et al. (2015).

Remark 10. Results in this section, including Theorem 6, demonstrate the full advantages

of the set classiﬁcation setting in contrast to the classical QDA setting. When multiple obser-

vations within each set have short-range dependence, the rates of convergence for estimating

key parameters as well as the oracle inequality resemble the results under independent as-

sumption. However, the results signiﬁcantly change when there is a long-range dependence

25

structure among multiple observations.

5. Numerical Studies

In this section we compare various versions of covariance-engaged set classiﬁers with other set

classiﬁers adapted from traditional methods. In addition to the CLIPS classiﬁer, we use the

diagonalized and enriched versions of ˆΣk respectively (labeled as Plugin(d) and Plugin(e))

introduced at the end of Section 3.1, and plug them in the Bayes rule (2.2), as done in (3.6).

For comparisons, we also supply the estimated β0, β and

from the CLIPS procedure to a

∇

QDA classiﬁer which is applied to all the observations in a testing set, followed by a majority

voting scheme (labeled as QDA-MV). Lastly, we calculate the sample mean and variance of

each variable in an observation set to form a new feature vector as done in Miedema et al.

(2012); then support vector machine (SVM; Cortes and Vapnik, 1995) and distance weighted

discrimination (DWD; Marron et al., 2007; Wang and Zou, 2018) are applied to the features

to make predictions (labeled as SVM and DWD respectively). We use R library clime

to calculate the CLIME estimates, R library e1071 to calculate the SVM classiﬁer, and R

library sdwd (Wang and Zou, 2016) to calculate the DWD classiﬁer.

5.1 Simulations

Three scenarios are considered for simulations. In each scenario, we consider a binary setting

with N “ 7 sets in a class, and M “ 10 observations from normal distribution in each set.

Scenario 1 We set the precision matrix for Class 1 to be Σ´1

1 “ p1 `

5.1 Simulations26

?

pqIp. For Class 2, we

set Σ´1

2 “ Σ´1

1 ` ˜
∇

, where ˜
∇

is a p ˆ p symmetric matrix with 10 elements randomly

selected from the upper-triangular part whose values are ζ and other elements being

zeros. For the mean vectors, we set µ1 “ Σ1pu, u, 0, . . . , 0qT and µ2 “ p0, . . . , 0qT . Note

that this makes the true value of β “ Σ´1

1 µ1 ´ Σ´1

2 µ2 “ pu, u, 0, . . . , 0qT , that is, only

the ﬁrst two covariates have linear impacts on the discriminant function if u ‰ 0. In

this scenario, the true diﬀerence in the precision matrices has some sparse and large

non-zero entries, whose magnitude is controlled by ζ. Note that while the diagonals

of the precision matrices are the same, the diagonals of the covariance matrices are

diﬀerent between the two classes.

Scenario 2 We set the covariance matrices for both classes to be the identity matrix, except

that for Class 1 the leading 5 by 5 submatrix of Σ1 has its oﬀ-diagonal elements set

to ρ. The rest of the setting is the same as in Scenario 1. In this scenario, both the

diﬀerence in the covariance and the diﬀerence in the precision matrix are conﬁned in

the leading 5 by 5 submatrix, so that the majority of matrix entries are the same

between the two classes. The level of diﬀerence is controlled by ρ: when ρ “ 0, the

two classes have the same covariance matrix.

Scenario 3 We set the precision matrix Σ1 for Class 1 to be a Toeplitz matrix whose ﬁrst row

is p1 ´ ρ2q´1pρ0, ρ1, ρ2, . . . , ρp´1q. The covariance for Class 2, Σ2, is a diagonal matrix

with the same diagonals as those of Σ1. It can be shown that the precision matrix for

Class 1 is a band matrix with degree 1, that is, a matrix whose nonzero entries are

5.1 Simulations27

conﬁned to the main diagonal and one more diagonal on both sides. Since the precision

matrix for Class 2 is a diagonal matrix, the diﬀerence between the precision matrix

has up to p ` 2pp ´ 1q nonzero entries. The magnitude of the diﬀerence is controlled

by the parameter ρ. The rest of the setting is the same as in Scenario 1.

We consider diﬀerent comparisons where we vary the magnitude of the diﬀerence in the

precision matrices (ζ or ρ), the magnitude of the diﬀerence in mean vectors (u), or the

dimensionality (p), when the other parameters are ﬁxed.

Comparison 1 (varying ζ or ρ) We vary ζ or ρ but ﬁx p “ 100 and u “ 0, which means

that the mean vectors have no discriminant power since the true value of β is a zero

vector. It shows the performance with diﬀerent potentials in the covariance structure.

Comparison 2 (varying u) We vary u while ﬁxing p “ 100 and ζ “ 0.55 in Scenario 1 or

ρ “ 0.5 and 0.3 in Scenarios 2 and 3. This case illustrates the potentials of the mean

diﬀerence when there is some useful discriminative power in the covariance matrices.

Comparison 3 (varying p) We let p “ 80, 100, 120, 140, 160 while ﬁxing ζ or ρ in the same

way as in Comparison 2 and ﬁxing u “ 0.05, 0.025 and 0.025 in Scenarios 1, 2 and 3

respectively.

Figure 2 shows the performance for Scenario 1. In the left panel, as ζ increases, the diﬀer-

ence between the true precision matrices increases. The proposed CLIPS classiﬁer performs

the best among all methods under consideration. It may be surprising that the Plugin(d)

method, which does not consider the oﬀ-diagonal elements in the sample covariance, can

5.1 Simulations28

Figure 2: Set classiﬁcation for Scenario 1. The three panels are corresponding to varying ζ, varying u and

varying p respectively. The CLIPS classiﬁer performs very well when the eﬀect of covariance dominates that

of the mean diﬀerence.

work reasonably well in this setting where the major mode of variation is in the oﬀ-diagonal

of the precision matrices. However, since large values in the oﬀ-diagonal of the precision

matrix can lead to large values of some diagonal entries of the covariance matrix, the good

performance of Plugin(d) has some partial justiﬁcation.

In the middle panel of Figure 2, the mean diﬀerence starts to increase. While every

method more or less gets some improvement, the DWD method has gained the most (it is

even the best performing classiﬁer when the mean diﬀerence u is as large as 1.) This may

be due to the fact that the mean diﬀerence on which DWD relies, instead of the diﬀerence

in the precision matrix, is suﬃciently large to secure a good performance in separating sets

between two classes.

Figure 3 shows the results for Scenario 2. In contrast to Scenario 1, there is no diﬀerence

in the diagonals of the covariances between the two classes (the precision matrices are still

lllll0.10.20.30.40.50.450.50.550.60.65(1) Precision difference zTest Errorlllll0.10.20.300.250.50.751(2) Mean Difference uTest Errorlllll0.150.200.250.300.3580100120140160(3) Dimension pTest ErrormethodlCLIPSPlugin(d)Plugin(e)QDA−MVDWDSVM5.1 Simulations29

Figure 3: Set classiﬁcation for Scenario 2. The three panels are corresponding to varying ρ, varying u and

varying p respectively. The classiﬁers that do not engage covariance perform poorly when there is no mean

diﬀerence signal.

diﬀerent). When there is no mean diﬀerence (see the left panel), it is clear that DWD, SVM

and the Plugin(d) method fail for obvious reasons (note that the Plugin(d) method does

Figure 4: Set classiﬁcation for Scenario 3. The three panels are corresponding to varying ρ, varying u and

varying p respectively. As in Scenario 2, the classiﬁers that do not engage covariance perform poorly when

there is no mean diﬀerence signal.

lllll0.20.40.60.10.30.50.70.9(1) Precision difference rTest Errorlllll0.10.20.30.40.500.250.50.751(2) Mean Difference uTest Errorlllll0.350.400.450.500.5580100120140160(3) Dimension pTest ErrormethodlCLIPSPlugin(d)Plugin(e)QDA−MVDWDSVMlllll0.00.20.40.150.30.450.60.75(1) Precision difference rTest Errorlllll0.10.20.30.40.500.250.50.751(2) Mean Difference uTest Errorlllll0.10.20.30.480100120140160(3) Dimension pTest ErrormethodlCLIPSPlugin(d)Plugin(e)QDA−MVDWDSVM5.2 Data Example30

not read the oﬀ-diagonal of the sample covariances and hence both classes have the same

precision matrices from its viewpoint.) As a matter of fact, all these methods perform as

badly as random-guess. The CLIPS classiﬁer always performs the best in this scenario in

the left panel. Similar to the case in Scenario 1, as the mean diﬀerence increases (see the

middle panel), the DWD method starts to get some improvement.

The results for Scenario 3 (Figure 4) are similar to Scenario 2, except that, this time the

advantage of two covariance-engaged set classiﬁcation methods, CLIPS and Plugin(e), seems

to be more obvious when the mean diﬀerence is 0 (see left panel). Moreover, the QDA-MV

method also enjoys some good performance, although not as good as the CLIPS classiﬁer.

In all three scenarios, it seems that the test classiﬁcation error is linearly increasing in

the dimension p, except for Scenario 3 in which the signal level depends on p too (greater

dimensions lead to greater signals.)

5.2 Data Example

One of the common procedures used to diagnose hepatoblastoma (a rare malignant liver

cancer) is biopsy. A sample tissue of a tumor is removed and examined under a microscope.

A tissue sample contains a number of nuclei, a subset of which is then processed to obtain

segmented images of nuclei. The data we analyzed contain 5 sets of nuclei from normal liver

tissues and 5 sets of nuclei from cancerous tissues. Each set contains 50 images. The data set

is publicly available (http://www.andrew.cmu.edu/user/gustavor/software.html) and was in-

troduced in Wang et al. (2011, 2010).

We tested the performance of the proposed method on the liver cell nuclei image data

5.2 Data Example31

set. First, the dimension was reduced from 36,864 to 30 using principal component analysis.

Then, among the 50 images of each set, 16 images are retained as training set, 16 are tuning

set and another 16 are test set. In other words, for each of the training, tuning, and testing

data sets, there are 10 sets of images, ﬁve from each class, with 16 images in each set.

Table 1 summarizes the comparison between the methods under consideration. All three

covariance-engaged set classiﬁers (CLIPS, Plugin(d) and Plugin(e)), along with the QDA-

MV method, perform better than methods which do not take the covariance matrices much

into account, such as DWD and SVM (note that they do look into the diagonal of the

covariance matrix.)

To get some insights to the reason that covariance-engaged set classiﬁers work and tra-

ditional methods fail, we visualize the data set in Figure 5. Subﬁgure (1) shows the scatter

plot of the ﬁrst two principal components of all the elementary observations (ignoring the

set memberships) in the data sets, in which diﬀerent colors (blue versus violet) depict the

Method

number of misclassiﬁed sets

standard error

CLIPS

Plugin(d)

Plugin(e)

QDA-MV

DWD

SVM

0.01/10

0.74/10

0.97/10

0.08/10

3.24/10

3.13/10

0.0104

0.0450

0.0178

0.0284

0.1164

0.1130

Table 1: Classiﬁcation performance for the liver cell nucleus image data.

two diﬀerent classes. Observations in the same set are shown in the same symbol. The

ﬁrst strong impression is that there is no mean diﬀerence between the two classes on the

observation level. In contrast, it seems that it is the second moment such as the variance

5.2 Data Example32

that distinguishes the two classes.

Figure 5: PCA scatter plots for the liver cell nucleus image data. Both classes are shown in diﬀerent colors.

(1): the elementary observations in the raw space; diﬀerent sets are shown in diﬀerent symbols. (2) and (3):

the augmented space seen by the DWD and SVM methods. (4) is a zoomed-in version of (3). It is shown

that traditional multivariate methods have a fundamental diﬃculty for this data set.

−400−2000200400−400−2000200400(1) Raw spacellllllllllllllllllllllllllllllllll43000043500044000044500010002000300040005000(2) Training data in augmented spacePC2lllll43000043500044000044500010002000300040005000(3) Test data in augmented spacellll44709044711044713044715057005720574057605780(4) Zoomed−in version of (3)PC2Bias in principal component scores

One may argue that DWD and SVM should theoretically work here because they work

on the augmented space where the mean and variance of each variable are calculated for each

observation set, leading to a 2p-dimensional feature vector for each set. However, Subﬁgures

(2)–(4) invalidate this argument. We plot the augmented training data in the space formed

by the ﬁrst two principal components (Subﬁgure (2)). The augmented test data are shown

in the same space in Subﬁgure (3) with a zoomed-in version in Subﬁgure (4). Note that the

scales for Subﬁgures (2) and (3) are the same. These ﬁgures show that there are more than

just the marginal mean and variance that are useful here, and our covariance-engaged set

classiﬁcation methods have used the information in the right way.

Supplementary Materials

The online supplementary materials contain additional theoretical arguments and proofs

of all results.

Acknowledgments

This work was supported by the National Research Foundation of Korea (No. 2019R1A2C2002256)

and a collaboration grant from Simons Foundation (award number 246649).

References

Abramovich, F., Benjamini, Y., Donoho, D. L., and Johnstone, I. M. (2006). Special invited lecture: adapting to

unknown sparsity by controlling the false discovery rate. The Annals of Statistics, 34(2):584–653.

Ali, S. and Shah, M. (2010). Human action recognition in videos using kinematic features and multiple instance

learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(2):288–303.

Arandjelovic, O. and Cipolla, R. (2006). Face set classiﬁcation using maximally probable mutual modes. In Pattern

Recognition, 2006. ICPR 2006. 18th International Conference on, volume 1, pages 511–514. IEEE.

Bickel, P. J. and Levina, E. (2008). Regularized estimation of large covariance matrices. The Annals of Statistics,

REFERENCES

36(1):199–227.

Cai, T. and Liu, W. (2011). A direct estimation approach to sparse linear discriminant analysis. Journal of the

American Statistical Association, 106(496):1566–1577.

Cai, T., Liu, W., and Luo, X. (2011). A constrained (cid:96)1 minimization approach to sparse precision matrix estimation.

Journal of the American Statistical Association, 106(494):594–607.

Candes, E. and Tao, T. (2007). The Dantzig selector: statistical estimation when p is much larger than n. The

Annals of Statistics, 35(6):2313–2351.

Carbonneau, M.-A., Cheplygina, V., Granger, E., and Gagnon, G. (2018). Multiple instance learning: A survey of

problem characteristics and applications. Pattern Recognition, 77:329–353.

Chen, Y., Bi, J., and Wang, J. Z. (2006). MILES: Multiple-instance learning via embedded instance selection. IEEE

Transactions on Pattern Analysis and Machine Intelligence, 28(12):1931–1947.

Cheplygina, V., Tax, D. M., and Loog, M. (2015). On classiﬁcation with bags, groups and sets. Pattern Recognition

Letters, 59:11–17.

Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3):273–297.

Fan, Y., Jin, J., and Yao, Z. (2013). Optimal classiﬁcation in sparse Gaussian graphic model. The Annals of Statistics,

41(5):2537–2571.

Fan, Y., Kong, Y., Li, D., Zheng, Z., et al. (2015). Innovated interaction screening for high-dimensional nonlinear

classiﬁcation. The Annals of Statistics, 43(3):1243–1272.

Fan, Y. and Lv, J. (2013). Asymptotic equivalence of regularization methods in thresholded parameter space. Journal

of the American Statistical Association, 108(503):1044–1061.

Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso.

REFERENCES

Biostatistics, 9(3):432–441.

Gaynanova, I. and Wang, T. (2019). Sparse quadratic classiﬁcation rules via linear dimension reduction. Journal of

multivariate analysis, 169:278–299.

Jiang, B., Wang, X., and Leng, C. (2018). A direct approach for sparse quadratic discriminant analysis. The Journal

of Machine Learning Research, 19(1):1098–1134.

Jung, S. and Qiao, X. (2014). A statistical approach to set classiﬁcation by feature selection with applications to

classiﬁcation of histopathology images. Biometrics, 70:536–545.

Kuncheva, L. I. (2010). Full-class set classiﬁcation using the hungarian algorithm. International Journal of Machine

Learning and Cybernetics, 1(1-4):53–61.

Li, Q. and Shao, J. (2015). Sparse quadratic discriminant analysis for high dimensional data. Statistica Sinica,

25:457–473.

Maron, O. and Lozano-P´erez, T. (1998). A framework for multiple-instance learning. Advances in neural information

processing systems, pages 570–576.

Marron, J., Todd, M. J., and Ahn, J. (2007). Distance-weighted discrimination. Journal of the American Statistical

Association, 102(480):1267–1271.

Meinshausen, N. and B¨uhlmann, P. (2006). High-dimensional graphs and variable selection with the lasso. The

Annals of Statistics, 34(3):1436–1462.

Meinshausen, N. and B¨uhlmann, P. (2010). Stability selection. Journal of the Royal Statistical Society: Series B

REFERENCES

(Statistical Methodology), 72(4):417–473.

Miedema, J., Marron, J. S., Niethammer, M., Borland, D., Woosley, J., Coposky, J., Wei, S., Reisner, H., and

Thomas, N. E. (2012). Image and statistical analysis of melanocytic histology. Histopathology, 61(3):436–444.

Ning, X. and Karypis, G. (2009). The set classiﬁcation problem and solution methods. In Proceedings of the 2009

SIAM International Conference on Data Mining, pages 847–858. SIAM.

Pan, Y. and Mai, Q. (2020). Eﬃcient computation for diﬀerential network analysis with applications to quadratic

discriminant analysis. Computational Statistics & Data Analysis, 144:106884.

Qin, Y. (2018). A review of quadratic discriminant analysis for high-dimensional data. Wiley Interdisciplinary

Reviews: Computational Statistics, 10(4):e1434.

Ren, Z., Sun, T., Zhang, C.-H., Zhou, H. H., et al. (2015). Asymptotic normality and optimalities in estimation of

large gaussian graphical models. The Annals of Statistics, 43(3):991–1026.

Samsudin, N. A. and Bradley, A. P. (2010). Nearest neighbour group-based classiﬁcation. Pattern Recognition,

43(10):3458–3467.

Shifat-E-Rabbi, M., Yin, X., Fitzgerald, C. E., and Rohde, G. K. (2020). Cell image classiﬁcation: a comparative

overview. Cytometry Part A, 97(4):347–362.

Wang, B. and Zou, H. (2016). Sparse distance weighted discrimination. Journal of Computational and Graphical

Statistics, 25(3):826–838.

Wang, B. and Zou, H. (2018). Another look at distance-weighted discrimination. Journal of the Royal Statistical

Society: Series B (Statistical Methodology), 80(1):177–198.

Wang, R., Guo, H., Davis, L. S., and Dai, Q. (2012). Covariance discriminative learning: A natural and eﬃcient ap-

proach to image set classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference

REFERENCES

on, pages 2496–2503. IEEE.

Wang, W., Ozolek, J. A., and Rohde, G. K. (2010). Detection and classiﬁcation of thyroid follicular lesions based on

nuclear structure from histopathology images. Cytometry Part A, 77(5):485–494.

Wang, W., Ozolek, J. A., Slepˇcev, D., Lee, A. B., Chen, C., and Rohde, G. K. (2011). An optimal transportation

approach for nuclear structure-based pathology. IEEE Transactions on Medical Imaging, 30(3):621–631.

Wasserman, L. and Roeder, K. (2009). High dimensional variable selection. The Annals of Statistics, 37(5A):2178–

2201.

Yuan, M. (2010). High dimensional inverse covariance matrix estimation via linear programming. The Journal of

Machine Learning Research, 11:2261–2286.

Zhao, S. D., Cai, T. T., and Li, H. (2014). Direct estimation of diﬀerential networks. Biometrika, 101(2):253–268.

Zou, H. (2019). Classiﬁcation with high dimensional features. Wiley Interdisciplinary Reviews: Computational

Statistics, 11(1):e1453.

Zhao Ren

Department of Statistics, University of Pittsburgh, Pittsburgh, PA 15260, USA

E-mail:zren@pitt.edu

Sungkyu Jung

Department of Statistics, Seoul National University, Gwanak-gu, Seoul 08826, Korea

E-mail: sungkyu@snu.ac.kr

Xingye Qiao

Department of Mathematical Sciences, Binghamton University, State University of New York, Binghamton, NY,

13902 USA

E-mail: qiao@math.binghamton.edu

REFERENCES

Covariance-engagedClassiﬁcationofSetsviaLinearProgramminZhaoRen,SungkyuJungandXingyeQiaoUniversityofPittsburgh,SeoulNationalUniversity,BinghamtonUniversitySupplementaryMaterialS1.TheoreticalPropertiesunderTimeSeriesStructuresWeconsidertheperformanceofCLIPSclassiﬁerwhenobservationswithineachsetareallowedtofollowvarioustimeseriesstructures,andextendtheresultsobtainedinTheorems3,4,5and6inthesedependentsettings.WefollowtheassumptioninSection2thatboththeNsets{(Xi,Yi)}Ni=1andthenewset(X†,Y†)aregeneratedinthesamewayas(X,Y)independently.Inthissection,thegenerat-ingprocessof(X,Y)isgeneralizedtoallowbothshort-rangeandlong-rangedependenttimeseries.Speciﬁcally,whilewestillassumeYandMareindependentwithclassprobabilitiesπk(k=1,2)anddistributionpMrespectively,hereweassumethatconditionedonM=mandY=y,observationsX1,X2,...,XminthesetXfollowavectorlinearprocess,Xi=µy+∞Xt=0Aytξi−t,(S1.1)whereAytarep×pdimensionalcoeﬃcientmatricesinclassY=yandξt=(ξt1,...,ξtp)Twith(ξtj)t∈Z,j=1,...,pbeingi.i.d.standardnormalvariables.NotethatthecovariancematricesofindividualobservationfromtwoclassesareΣy:=Σy0=P∞t=0AytATytfory=1,2.Ingeneral,theauto-covariancematricesatlagkofallobservationswithineachset,thatis1S1.THEORETICALPROPERTIESUNDERTIMESERIESSTRUCTURESCov(Xi,Xi+k):=Σyk=P∞t=0AytATy(t+k)fory=1,2.TheabovevectorlinearprocessisﬂexiblesincethecoeﬃcientmatricesAytcancapturebothspatialandtemporaldependences.Oneimportantexampleisthevectorauto-regression(VAR)model.Ithasbeenwidelyusedinmanyﬁelds,includingfunctionalMagneticResonanceImagine(fMRI)andmicroarraydata(Dinovetal.,2005;Posekanyetal.,2011).Tocharacterizethedependencerelationshipofthetimeseries,weimposeconditionsonthecoeﬃcientmatrices.SetAyt=(ayt,ij)1≤i,j≤p.ThenweassumetheGaussianlinearprocesssatisﬁesthefollowingdecayconditiononAytforbothclassesy=1,2,andallt≥0,max1≤i≤p(pXj=1a2yt,ij)1/2≤CTS(1+t)−ν,(S1.2)whereCTS>0issomeconstantandν>1/2reﬂectsthedecayrate.Therequirementν>1/2isneededtoguaranteethatthecovariancematrixΣy=P∞t=0AtATtisﬁnite.Inparticular,inthetimeseriesliterature,whenν>1,thecorrespondinglinearprocessissaidtohaveashort-rangedependence(SRD)becauserowsofthethecorrespondingauto-covariancematricesΣykareabsolutelysummable,whichyieldsrelativelyweakdependenceamongallobservationswithineachset.When1/2<ν<1,thecorrespondingauto-covariancematricesmaynotbeabsolutelysummableandthusthelinearprocessissaidhavealong-rangedependence(LRD).See,forexampleBeran(2017);Wuetal.(2010)formoredetails.WeinvestigategeneralizationerrorsfortheCLIPSclassiﬁer˜φ(X†)in(3.12)underthevectorlinearprocessmodelforbothshort-rangandlong-rangedependence.ItisworthwhilepointingoutthatφBin(2.2)isnolongertheBayesdecisionruleduetothetimeseriesstructure.Incontrast,thefullBayesdecisionruleformodel(S1.1)requirestheknowledge2S1.THEORETICALPROPERTIESUNDERTIMESERIESSTRUCTURESofallcoeﬃcientmatricesAytfort∈Z,y=1,2.However,inhigh-dimensionalsituations,itisdiﬃculttoestimateallcoeﬃcientmatricesAytaccuratelyifnotimpossibleatall.Withthedecaycondition(S1.2),itisstillreasonabletoapplysomesimpliﬁedquadraticclassiersuchasφB(X†)in(2.2)topredictY†asifallobservationsinthetestsetX†areindependent.Indeed,undertheindependencecaseinwhichAyt=0forallt≥1,φB(X†)istheoracleofourCLIPSclassiﬁer˜φ(X†).Withthegeneraltimeseriesstructure(S1.1),weneedtodeﬁnetheoracleofourCLIPSﬁrst.˜φ(X†)=2−1(cid:26)log(ˆπ1/ˆπ2)m+˜β0+˜βT¯x+¯xT˜∇¯x/2+tr(˜∇S)/2>0(cid:27).RecallthatthekeyestimationinourCLIPSclassiﬁerdisplayedaboveincludequadraticterm˜∇,linearcoeﬃcient˜βandaninterceptcoeﬃcient˜β0.Whiletheestimations˜∇in(3.8)and˜βin(3.9)areproposedtoestimatetheircounterpartsinourCLIPSclassiﬁer∇=Σ−12−Σ−11andβ=β1−β2whereβy=Σ−1yµyrespectively,theconstantcoeﬃcientestimator˜β0in(3.10)isobtainedviaalogisticregressionmodel.Therefore,theoracleβ0,TSof˜β0inthecurrentsettingisdeﬁnedastheminimizerofthefollowingpopulationlossfunction,thatis,β0,TS=argminθ0∈RE‘(θ0|{(Xi,Yi)}Ni=1,β,∇),(S1.3)where‘(θ0|{(Xi,Yi)}Ni=1,β,∇)isdeﬁnedin(3.11).Wepointouttheinterpretationof‘(·)isnolongerthenegativelog-likelihoodfunctionandthusβ0,TSisnotalwaysequaltothequantityβ0={−log(|Σ1|/|Σ2|)−µT1Σ−11µ1+µT2Σ−12µ2}/2deﬁnedin(2.2).However,theoracleclassiﬁerφB,TSofCLIPSdeﬁnedbelowisalwaysnoworse(i.e.,hasthesameorsmallergeneralizationerror)thanφBin(2.2)duetoitsdeﬁnition(S1.3).Again,fortheindependencecase,wehaveφB,TS=φB.φB,TS(X†)=2−1(cid:26)log(π1/π2)m+β0,TS+βT¯x+¯xT∇¯x/2+tr(∇S)/2>0(cid:27).(S1.4)3S1.THEORETICALPROPERTIESUNDERTIMESERIESSTRUCTURESFromnowon,wedenotebyRB,TStheoracleriskalthoughthesubscriptBnolongerimpliestheBayesdecisionrule.WeﬁrstextendTheorem3andestablishthestatisticalpropertiesofthethresholdedCLIMEdiﬀerenceestimator˜∇deﬁnedin(3.8).Again,weassumethatthetruequadraticparameter∇=Σ−12−Σ−11∈FM0(sq)hassparsitynomorethansqdeﬁnedin(4.13).Theorem1.Considerthevectorlinearprocessdeﬁnedin(S1.1)thatsatisﬁesthedecaycondition(S1.2).SupposeConditions1-3hold.Moreover,assume∇∈FM0(sq),kΣ−1kk‘1≤C‘1withsomeconstantC‘1>0fork=1,2andlogp≤c0Nwithsomesuﬃcientlysmallconstantc0>0.ThenforanyﬁxedL>0,withprobabilityatleast1−O(p−L),wehavethatk˜∇−∇k∞≤2λ01,N,k˜∇−∇kF≤2√sqλ01,N,k˜∇−∇k1≤2sqλ01,N,aslongasλ01,N≥8C‘1λ1,Nin(3.8)andλ1,N≥CC‘1qlogpNm0ifν>3/4CC‘1qlogpNm4ν−20if1/2<ν<3/4,whereCdependsonL,Ce,Cπ,CTSandcm.Moreover,wehavepr(supp(˜∇)⊂supp(∇))=1−O(p−L).Remark1.Thechoiceoftuningparameterλ1,Nandtheratesofconvergenceonthebound-arycaseν=3/4canalsobedealt.Inparticular,werequireλ1,N≥CC‘1qlogplogm0Nm0ifν=3/4.SeetheproofofTheorem1forfurtherdetails.TheresultsinTheorem1criticallydependontheestimationaccuracyofthesampleco-variancematrixunderthesupnorminvarioustimeseriesdependencestructureswithineach4S1.THEORETICALPROPERTIESUNDERTIMESERIESSTRUCTURESset.SuchtechnicalresultsaredetailedinLemma6inAppendix,wherethecorrespondinganalysisrequiresanapplicationofHanson-Wrightinequality.Inparticular,ifν>3/4,thentheratesofconvergenceforestimating∇arethesameasthoseundertheindependenceassumption.Ifn<3/4,thatis,thevectorlinearprocesshasalong-rangedependence,thentheratescanbeaﬀectedandreducedcorrespondingly.Weturntothestatisticalpropertiesofthelinearcoeﬃcientestimator˜βdeﬁnedin(3.9)undertimeseriesstructure.ThefollowingtheoremisanextensionofTheorem4,inwhichweassumethatβ=β1−β2belongstothesl-sparseballdeﬁnedin(4.14).Theorem2.Considerthevectorlinearprocessdeﬁnedin(S1.1)thatsatisﬁesthedecaycondition(S1.2).SupposeConditions1-3hold.Moreover,assumethatβ∈F0(sl),logp≤c0N,kβkk1≤Cβandkµkk≤CµwithsomeconstantsCβ,Cµ>0fork=1,2andsomesuﬃcientlysmallconstantc0>0.ThenforanyﬁxedL>0,withprobabilityatleast1−O(p−L),wehavethatk˜β−βk1≤C00C‘1slλ2,N,k˜β−βk≤C00C‘1√slλ2,N,aslongasthetuningparameterλ2,Nin(3.9)satisﬁesλ2,N≥C0qlogpNm0ifν>1C0qlogpNm2ν−10if1/2<ν<1,wheremax{kΣ−11k‘1,kΣ−12k‘1}≤C‘1andC00,C0dependonL,Ce,cm,Cπ,Cβ,CTSandCµ.Remark2.Thechoiceoftuningparameterλ2,Nandtheratesofconvergenceonthebound-arycaseν=1canalsobedealt.Inparticular,werequireλ2,N≥C0qlogplog2m0Nm0ifν=1.SeetheproofofTheorem2forfurtherdetails.5S1.THEORETICALPROPERTIESUNDERTIMESERIESSTRUCTURESAtahightlevel,theestimationaccuracyoflinearcoeﬃcientsaredeterminedbybothestimationaccuracyofthesamplemeanandthatofthesamplecovariancematrixunderthesupnorm.Whileundertheshort-rangedependencestructurebothratesofconvergenceareequaltop(logp/(Nm0)),therateofconvergenceofsamplemeandominatesthatofsamplecovariancematrixwhenthereisalong-rangedependenceamongmultipleobservationswithineachset.Next,wederivetherateofconvergenceforestimatingtheoracleconstantcoeﬃcientβ0,TSdeﬁnedin(S1.3)underthegeneraltimeseriesstructure.Theaccuracyofourestimator˜β0criticallydependsontheaccuracyforestimatingβand∇.Theorem3extendsTheorem5fromtheindependentcasetothegeneraltimeseriesstructure.Weneedonemildcondition,thepopulationstrongconvexityofthelossfunction‘(β0,TS|{(Xi,Yi)}Ni=1,β,∇)attheoraclepointβ0,TS.Condition1.Set¯XandSasthesamplemeanandvarianceofthesetofobservations(X,Y)withsetsizeM.DeﬁneZi=log(π1/π2)/M+¯XTβ+¯XT∇¯X/2+tr(∇S)/2.Theexpectationofthevariableexp(M(β0+Z))(1+exp(M(β0+Z)))2isboundedbelowbyClog>0,whereClogissomeuniversalconstant.Remark3.StrongconvexityCondition1coincideswithCondition4fortheindependentcase.Indeed,fortheindependentcasewehaveVar(Y|X)=exp(M(β0+Z))(1+exp(M(β0+Z)))2.Theorem3.Considerthevectorlinearprocessdeﬁnedin(S1.1)thatsatisﬁesthedecaycondition(S1.2).SupposeConditions1-4and1hold,logp≤c0Nwithsomesuﬃcientlysmallconstantc0>0andkµkk≤CµwithsomeconstantCµ>0fork=1,2.Besides,wehavesomeinitialestimators˜β,˜∇,ˆπ1andˆπ2suchthatm0(k˜β−βk1)(1+Uβ)+m0(k˜∇−6S1.THEORETICALPROPERTIESUNDERTIMESERIESSTRUCTURES∇k1)(1+U∇)+maxk=1,2|πk−ˆπk|≤CpforsomesuﬃcientlysmallconstantCp>0withprobabilityatleast1−O(p−L).Then,withprobabilityatleast1−O(p−L),wehave(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12)≤Cδ (k˜β−βk1)(1+Uβ)+(k˜∇−∇k1)(1+U∇)+maxk=1,2|πk−ˆπk|/m0+slogpNm20!,whereUβsatisﬁesUβ=qlogpm0ifν>1qlogpm2ν−10if1/2<ν<1,U∇satisﬁesU∇=logpm0ifν>1logpm2ν−10if1/2<ν<1,andconstantCδdependsonL,Ce,Cπ,Clog,Cµ,CTS,Cm,cm.Remark4.Theratesofconvergenceontheboundarycaseν=1canalsobedealt.Inparticular,werequireUβ=qlogplog2m0m0andU∇=logplog2m0m0ifν=1.SeetheproofofTheorem3forfurtherdetails.Wepointoutthattherateofconvergenceforestimatingβ0,TSdependsontheestimationaccuracyofthelinearcoeﬃcientthroughatermk˜β−βk1inTheorem3whileitreliesonapotentiallysmallertermk˜β−βk2inTheorem5underindependentassumptioninSection4.Thisisduetoatechnicalreasonandtheresultcannotbeimproved(i.e.,replacingk˜β−βk1byk˜β−βk2)ifweonlyassumethedecaycondition(S1.2).Theorems1,2and3extendTheorems3,4and5respectively,anddemonstratetheestimationaccuracyforthequadratic,linearandconstantcoeﬃcientsinourCLIPSclassiﬁer(3.12)underthegeneraltimeseriesstructure.Finally,weestablishanoracleinequalityforitsgeneralizationerrorviaprovidingarateofconvergenceoftheexcessrisk.Recallthe7S1.THEORETICALPROPERTIESUNDERTIMESERIESSTRUCTURESgeneralizationerrorofCLIPSclassiﬁeris˜R=π1˜R1+π2˜R2,where˜Rk=pr(˜φ(X†)6=k|Y†=k).Againpristheconditionalprobabilitygiventhetrainingdata{(Xi,Yi)}Ni=1which˜φ(X†)dependson.Inaddition,wedeﬁnethegeneralizationerroroftheoracleclassiﬁerφB,TSasRB,TS=π1R1,TS+π2R2,TS,whereRk,TS=pr(φB,TS(X†)6=k|Y†=k).WeneedtointroducesomenotationdN,TSrelatedtotheoracleclassiﬁerin(S1.4),whichissimilartodNdeﬁnedinSection4forindependencecase.RecalltheoracleclassiﬁerφB,TS(X†)solelydependsonthesignofthefunctiongTS(X†)=1mlog(π1/π2)+β0,TS+βT¯x+¯xT∇¯x/2+tr(∇S)/2.WedeﬁnebyFk,m,TStheconditionalcumulativedistributionfunctionoftheoraclestatisticgTS(X†)giventhatM†=mandY†=k,anddeﬁnebydN,TStheupperboundoftheirﬁrstderivativesforallpossiblemnear0,dN,TS=maxm∈[cmm0,Cmm0],k=1,2(supt∈[−δ0,δ0](cid:12)(cid:12)F0k,m,TS(t)(cid:12)(cid:12)),whereδ0isanysuﬃcientlysmallconstant.ThevalueofdN,TSisdeterminedbythevectorlinearprocess(S1.1)andperformanceoftheoracleclassiﬁer.WedeﬁnethecounterpartsofΞNanditsstatisticalorderκNdeﬁnedinSection4underthegeneraltimeseriesstructurebelow,whichcriticallydeterminetheexcessrisk.Indeed,onecanshowthatTheorems1,2and3implythatwithprobabilityatleast1−O(p−L),ΞN,TS:=(1+Uβ)k˜β−βk1+(1+U∇)k˜∇−∇k1+maxk=1,2|ˆπk−πk|m0+(cid:12)(cid:12)(cid:12)˜β0−β0,TS(cid:12)(cid:12)(cid:12)=O(κN,TS),whereκN,TS:=(1+U∇)sqλ01,N+(1+Uβ)C‘1slλ2,N+p(logp)/(Nm20),andthekeyquantitiesUβ,U∇,λ01,Nandλ2,NarespeciﬁedinthestatementofTheorems1,2and3forvariousvalueofµ.ThequantityκN,TSdN,TSistheleadingrateofconvergenceintheoracleinequality.WeneedonemoreconditiontoguaranteetheassumptionsofTheorem3aresatisﬁedwithhighprobability,whichissimilartoCondition5forindependencecase.8S2.PROOFSOFMAINRESULTSCondition2.Supposem0κN,TS≤c0andκN,TSdN,TS≤c0withsomesuﬃcientlysmallconstantc0>0.Theorem4belowrevealstheoraclepropertyofCLIPSclassiﬁerunderthegeneraltimeseriesstructure.Theorem4.SupposethattheassumptionsofTheorems1and2holdandthatConditions1–2alsohold.Thenwithprobabilityatleast1−O(p−L),wehavetheoracleinequality˜R≤RB,TS+Cg(κN,TSdN,TS+p−L),whereconstantCgdependsonL,Ce,Cπ,Clog,Cβ,Cm,cm,CTSandCµonly.S2.ProofsofMainResultsProofofTheorem1Proof.WeonlyprovethatRB1→0andtheproofofRB2→0issimilar.InadditionnotethatRBk=pr(φB(X†)6=k|Y†=k)=Cmm0Xm=cmm0pr(φB(X†)6=k|Y†=k,M†=m)·pM(m):=Cmm0Xm=cmm0RBk,m·pM(m),wherethelastequalityisduetoindependenceofY†andM†,andCondition2.Henceitissuﬃcientforustofocusonanyﬁxedm∈[cmm0,Cmm0].GiventhatthesetisfromClass1,wehaveX†i∼N(µ1,Σ1),i=1,...,m.TheBayesdecisionruleclassiﬁesthesettoClass2,i.e.,φB(X†)=2in(2.2)ifg(X†1,...,X†m)<0,9S2.PROOFSOFMAINRESULTSwhichisequivalenttomXi=1(cid:16)X†i−µ1(cid:17)T∇(cid:16)X†i−µ1(cid:17)−2mδTΣ−12(¯X−µ1)+mδTΣ−12δ−mlog(cid:18)|Σ1||Σ2|(cid:19)+2log(cid:18)π1π2(cid:19)<0,(S2.5)where¯X=Pmi=1X†i/misthesamplemean.DeﬁneV=Σ1/21Σ−12Σ1/21−IwhereIistheidentitymatrix.WesetZi=Σ−1/21(X†i−µ1)∼N(0,I),Am,p=Pmi=1ZTiVZi−2mδTΣ−12Σ1/21¯Zwith¯Z=Pmi=1Zi/m.ThentheBayesriskRB1,mcanbewrittenas,followingfrom(S2.5),RB1,m=pr(Am,p−EAm,p<−α),whereα=mtr(V)+mδTΣ−12δ−mlog{|Σ1|/|Σ2|}+2log(π1/π2)sinceEAm,p=mtr(V).ThestrategytoboundRB1,mistoshowthat|Am,p−EAm,p|concentrateson√mDpbutα>0divergesatafasterrateofmD2p.WeﬁrstgiveanupperboundofthemagnitudeofAm,p−EAm,p.Writetheeigen-decompositionofVasUΛUTandthediagonalmatrixΛ=diag(λj)withλ1≥λ2≥...≥λp.Moreover,set˜Zi=UTZi∼N(0,I)with˜Zi,jitsjthentry.NotethatAm,p−EAm,p=mXi=1pXj=1λj(˜Z2i,j−1)−2mδTΣ−12Σ1/21¯Z.Thetailprobabilityofnormaldistributionimpliespr(|2mδTΣ−12Σ1/21¯Z|>t)≤2exp−12 t2√mkδTΣ−12Σ1/21k!2≤2exp(cid:18)−C−3et28mkδk2(cid:19),(S2.6)wherethelastinequalityisduetoCondition1.Since˜Z2i,j−1issub-exponential,Bernstein’sinequality(e.g.Vershynin,2012,Proposition5.16)impliesthatthereexistssomeuniversal10S2.PROOFSOFMAINRESULTSconstantc1>0suchthatpr(|mXi=1pXj=1λj(˜Z2i,j−1)|>t)≤2exp(cid:18)−c1min(t2mkΛk2F,tmax{|λ1|,|λp|})(cid:19).(S2.7)Nowwefocusonthelowerboundofα.Firstofall,noticethatmδTΣ−12δ≥mC−1ekδk2byCondition1.Moreover,thereexistssomeconstantc2>0dependingonCeonlysuchthatmtr(V)−mlog{|Σ1|/|Σ2|}=m(tr(V)−log|I+V|)=mpXj=1(λj−log(1+λj))≥c2mkΛk2F(S2.8)wherethelastinequalityfollowsfromthatλj+1∈[C−2e,C2e]accordingtoCondition1.NotethatkΛkF=kVkF=kΣ1/21∇Σ1/21kFandC−1e≤kVkF/k∇kF≤CeaccordingtoCondition1.Thereforebycombiningtheabovetworesultsweconcludeα≥c3mD2p+2log(π1/π2)withc3=min(c2C−2e,C−1e)>0.NotethatbyConditions1and3,λ1inequation(S2.7)and2log(π1/π2)intheexpressionofαarebounded.WhenmD2pislargeenough,wecanpickt=cmD2pforsmallenoughc>0inequations(S2.6)and(S2.7)suchthatAm,p−EAm,p>−αwithprobabilityatleast1−4exp(cid:0)−c0mD2p(cid:1).Thereforewecompleteourproofbyseeingthatforeachﬁxedm,RB1,m≤4exp(cid:0)−c0mD2p(cid:1)forsomesmallconstantc0>0,togetherwiththefactm∈[cmm0,Cmm0]fromCondition2.ProofofProposition1Proof.Notethatinsteadofmobservationswithi.i.d.N(µk,Σk)fromeitherclassk=1,2,inthecurrentcase,weonlyhaveonerepresentative¯x∼N(µk,Σk/m)withk=1or2.Therefore,theproofofupperbound,i.e.,R¯x≤4exp(−c0(k∇k2F+m0kδk2))forsomesmallconstantc0>0,simplyfollowsfromtheproofofTheorem1byreplacingm0andΣkby111S2.PROOFSOFMAINRESULTSandΣk/m0respectively.Toshowtherateontheexponentcannotbefurtherimprovedingeneral,weneedalittlemoreeﬀorts.FollowingtheproofproceduresforTheorem1,itissuﬃcienttoshowthesamelowerboundoneachR¯xk,m:=pr(φB,¯x(X†)6=k|Y†=k,M†=m)wherem∈[cmm0,Cmm0].GiventhatthesetisfromClass1,wehave¯X†∼N(µ1,Σ1/m).φB,¯xclassiﬁesthesettoClass2ifgQDA(¯X†)<0,whichisequivalenttom(cid:0)¯X†−µ1(cid:1)T∇(cid:0)¯X†−µ1(cid:1)−2mδTΣ−12(¯X†−µ1)+mδTΣ−12δ−log(cid:18)|Σ1||Σ2|(cid:19)+2log(cid:18)π1π2(cid:19)<0.DeﬁneV=Σ1/21Σ−12Σ1/21−IwhereIistheidentitymatrix.Writetheeigen-decompositionofVasUΛUTandthediagonalmatrixΛ=diag(λj)withλ1≥λ2≥...≥λp.Moreover,setZ=√mUTΣ−1/21(¯X†−µ1)∼N(0,I)withZjitsjthentry,andAm,p=Ppj=1λjZ2j−2√mδTΣ−12Σ1/21UZ.ThentheriskR¯x1,mcanbewrittenasR¯x1,m=pr(Am,p−EAm,p<−α),whereα=Ppj=1λj+mδTΣ−12δ−log{|Σ1|/|Σ2|}+2log(π1/π2)sinceEAm,p=Ppj=1λj.Weﬁrstupperboundthevalueofα.NoticethatmC−1ekδk2≤mδTΣ−12δ≤mCekδk2byCondition1.Moreover,asimilarargumentto(S2.8)alsoprovidesanupperbound,i.e.,fortwosmallconstantsc2<c02<1,wehavec2kΛk2F≤Ppj=1λj−log{|Σ1|/|Σ2|}≤c02kΛk2F.ByCondition3,2log(π1/π2)intheexpressionofαisbounded.Therefore,underourassumptiononsuﬃcientlylargek∇k2F+m0kδk2,wehavethat0<α<c(k∇k2F+m0kδk2)withsomesmallc>0.Weshowtherateonexponentcannotbefurtherimprovedbyshowingalowerboundforsomespecialcasesofµ1,µ2,Σ1,Σ2.Assumethesupportofvector(λ1,...,λp)TandthesupportofvectorδTΣ−12Σ1/21Uaredisjoint(e.g.,bothΣkarediagonalmatriceswithdiﬀerenceontheﬁrstp/2diagonalentries,andonlythelastp/2coordinatesonmeandiﬀerenceδ12S2.PROOFSOFMAINRESULTSarenonzero).Forthisscenario,theﬁrsttermT1:=Ppj=1λjZ2jandsecondtermT2:=−2√mδTΣ−12Σ1/21UZinAm,pareindependent.ToshowthatthetermT1isnon-positivewithprobabilityawayfromzero,weapplyProposition2.4inJohnstone(2001)toobtainthatpr(T1<0)>γ>0withsomeabsoluteconstantγ>0bynotingthattheﬁrsttermisaweightedChi-squarevariable.Bytailprobabilityofnormaldistributionandtheupperboundofα,wefurtherobtainthatpr(T2<−α)>exp(c(k∇k2F+m0kδk2))withsomesmallc>0.Intheend,byindependence,weobtainthatR¯x1,m=pr(Am,p−EAm,p<−α)>exp(c(k∇k2F+m0kδk2))γ>exp(c00(k∇k2F+m0kδk2))withsomesmallc00>0,whichcompletesourproof.ProofofTheorem2Proof.WeonlyprovethatˆR1→0withhighprobabilityandˆR2→0canbeshownbysymmetry.ThestrategyoftheproofissimilartothatforTheorem1.Wefurtherfocusoneachﬁxedm∈[cmm0,Cmm0]sinceˆRk=Cmm0Xm=cmm0pr(ˆφ(X†)6=k|Y†=k,M†=m)·pM(m):=Cmm0Xm=cmm0ˆRk,m·pM(m).(S2.9)Thequadraticsetclassiﬁerclassiﬁesthesetto2,thatis,ˆφ(X†)=2in(3.6)ifmXi=1(cid:16)X†i−ˆµ1(cid:17)Tˆ∇(cid:16)X†i−ˆµ1(cid:17)−2mˆδTˆΣ−12(¯X−ˆµ1)+mˆδTˆΣ−12ˆδ−mlog(cid:12)(cid:12)(cid:12)ˆΣ1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ˆΣ2(cid:12)(cid:12)(cid:12)+2log(cid:18)ˆπ1ˆπ2(cid:19)<0,whereˆδ=ˆµ2−ˆµ1and¯X=Pmi=1X†i/m.DeﬁneˆAm,p=mXi=1(cid:16)X†i−ˆµ1(cid:17)Tˆ∇(cid:16)X†i−ˆµ1(cid:17)−2mˆδTˆΣ−12(¯X−ˆµ1):=ˆA1,m,p+ˆA2,m,p.13S2.PROOFSOFMAINRESULTSThenthegeneralizationerrorˆR1,m,whichisarandomvariableasafunctionof{(Xi,Yi)}Ni=1,canbewrittenasˆR1=ˆR1((X,Y))=pr(cid:16)ˆAm,p−EˆAm,p<−ˆα(cid:17),(S2.10)whereprandEareunderstoodastheconditionalexpectationgiven{(Xi,Yi)}Ni=1andˆα=E(ˆA1,m,p+ˆA2,m,p)+mˆδTˆΣ−12ˆδ−mlog(cid:16)(cid:12)(cid:12)(cid:12)ˆΣ1(cid:12)(cid:12)(cid:12)/(cid:12)(cid:12)(cid:12)ˆΣ2(cid:12)(cid:12)(cid:12)(cid:17)+2log(cid:18)ˆπ1ˆπ2(cid:19).Thefollowinglemmafacilitatesouranalysis.Lemma1.ForanyﬁxedL>0,undertheassumptionsp≤c0Nm0andlogp≤c0Nwithsuﬃcientlysmallc0>0,wehavethat(i)C0−1≤λmin(ˆΣk)≤λmax(ˆΣk)≤C0;(ii)kµk−ˆµkk≤C0qpNm0;(iii)kΣk−ˆΣkkF≤C0qp2Nm0and(iv)|πk−ˆπk|≤C0qlogpN,k=1,2withprobabilityatleast1−O(p−L),wherepositiveconstantC0dependonCe,cm,LandCπonly.Fromnowon,weconditionontheeventEinwhichresults(i)-(iv)ofLemma1holdfortrainingdata{(Xi,Yi)}Ni=1.AllpositiveconstantsusedhereafteronlydependonCeandc0.Clearly,sincep2/(Nm0D2p)issuﬃcientlysmall,Lemma1(ii)and(iii)implythatˆDp=(cid:16)kˆ∇k2F+kˆδk2(cid:17)1/2(cid:16)Dp.(S2.11)WeshowtheconcentrationradiusofˆAm,p−EˆAm,pismuchsmallerthanˆαunderouras-sumptions.Firstofall,weanalyzetheleftsideˆAm,p−EˆAm,p=Σ2k=1(ˆAk,m,p−EˆAk,m,p).NotethatˆA2,m,p−EˆA2,m,p=−2Pmi=1ˆδTˆΣ−12Σ1/21Zi,whereZi=Σ−1/21(X†i−µ1)i.i.d∼N(0,I).NoteLemma1impliesthespectralnorm(cid:13)(cid:13)(cid:13)ˆΣ−12Σ1/21(cid:13)(cid:13)(cid:13)‘2≤C0C1/2e.Thetailprobabilityofnormaldistributionimplies(similarlyasinequation(S2.6))thereexistssomeconstantC1>0such14S2.PROOFSOFMAINRESULTSthat,pr(|ˆA2,m,p−EˆA2,m,p|>t)≤2exp −C1t2mkˆδk2!.(S2.12)Besides,ˆA1,m,p−EˆA1,m,p=W1+W2,whereW1:=tr[ˆ∇(mXi=1(cid:16)X†i−µ1(cid:17)(cid:16)X†i−µ1(cid:17)T)]−tr[ˆ∇mΣ1],W2:=2(µ1−ˆµ1)Tˆ∇Σ1/21mXi=1Zi.SetˆV=Σ1/21ˆ∇Σ1/21anditseigen-values{ˆλj}pj=1.ByasimilarargumentusingBernstein’sinequalitylike(S2.7),wehavethatthereexistssomeconstantc1>0suchthatpr(|W1|>t)≤2exp−c1min(t2mkˆVk2F,tmax{(cid:12)(cid:12)(cid:12)ˆλ1(cid:12)(cid:12)(cid:12),(cid:12)(cid:12)(cid:12)ˆλp(cid:12)(cid:12)(cid:12)}).(S2.13)TocontrolW2,weapplyagainthetailprobabilityofnormaldistributiontoobtainthatforsomeconstantsC2,C3>0,pr(|W2|>t)≤2exp −C2t2mkˆ∇k2‘2·kµ1−ˆµ1k2!≤2exp −C3t2mkˆ∇k2F!,(S2.14)sincekµ1−ˆµ1k≤C0qpNm0≤C0c1/20byLemma1.Thereforeequations(S2.12)-(S2.14),togetherwith(S2.11),implythatforsomeC4>0,pr(|ˆAm,p−EˆAm,p|>t)≤6exp(cid:18)−C4t2mD2p(cid:19).(S2.15)Nowwelowerboundtherightsideˆα.Thistermcanbedecomposedintosixterms.ˆα=mˆδTˆΣ−12ˆδ+hmtr(ˆ∇ˆΣ1)−mlog(cid:16)(cid:12)(cid:12)(cid:12)ˆΣ1(cid:12)(cid:12)(cid:12)/(cid:12)(cid:12)(cid:12)ˆΣ2(cid:12)(cid:12)(cid:12)(cid:17)i+2log(cid:18)ˆπ1ˆπ2(cid:19)+mtr(ˆ∇(Σ1−ˆΣ1))−2mˆδTˆΣ−12(µ1−ˆµ1)+m(µ1−ˆµ1)ˆ∇(µ1−ˆµ1)T.ThesetermshavethefollowingboundsrespectivelywithsomeconstantC5,C6,C7,C8,C9>15S2.PROOFSOFMAINRESULTS0,mˆδTˆΣ−12ˆδ≥C5mkˆδk2,(S2.16)mtr(ˆ∇ˆΣ1)−mlog(cid:16)(cid:12)(cid:12)(cid:12)ˆΣ1(cid:12)(cid:12)(cid:12)/(cid:12)(cid:12)(cid:12)ˆΣ2(cid:12)(cid:12)(cid:12)(cid:17)≥C5mkˆ∇k2F,(S2.17)(cid:12)(cid:12)(cid:12)mtr(ˆ∇(Σ1−ˆΣ1))(cid:12)(cid:12)(cid:12)≤C6mkˆ∇kFkΣ1−ˆΣ1kF≤C7mkˆ∇kF(p2/Nm0)1/2,(S2.18)(cid:12)(cid:12)(cid:12)2mˆδTˆΣ−12(µ1−ˆµ1)(cid:12)(cid:12)(cid:12)≤C6mkˆδkkµ1−ˆµ1k≤C7mkˆδk(p/Nm0)1/2,(S2.19)|2log(ˆπ1/ˆπ2)|≤C6,(S2.20)(cid:12)(cid:12)(cid:12)m(µ1−ˆµ1)ˆ∇(µ1−ˆµ1)T(cid:12)(cid:12)(cid:12)≤C8mkˆ∇k‘2kµ1−ˆµ1k2≤C9m(p/Nm0).(S2.21)Equations(S2.16)and(S2.17)aredueto(i)ofLemma1.Inparticular,(S2.17)followsfromasimilarargumentas(S2.8).Equations(S2.18)and(S2.19)followfrom(iii)and(ii)ofLemma1respectivelywhileequation(S2.20)isdueto(iv)ofLemma1andCondition3.Equation(S2.21)followsfrom(i)and(ii)ofLemma1.Furthermore,noticethatp2/(Nm0D2p)issuﬃcientlysmallandm0D2pissuﬃcientlylarge,equations(S2.16)-(S2.21)aswellas(S2.11)yieldthatˆα≥C10mD2pforsomesmallconstantC10>0.Finally,thelowerboundofˆαandconcentrationofˆAm,p−EˆAm,pin(S2.15)witht=c00mD2pforsmallenoughc00>0,togetherwiththeassumptionD2pmissuﬃcientlylarge,implythatthegeneralizationerrorofthequadraticsetclassiﬁcationruleˆR1,m≤2exp(cid:0)−c0mD2p(cid:1)foreachm∈[cmm0,Cmm0]ontheeventE.HencewecompleteourproofbyapplyingLemma1andequation(S2.9),thatis,ˆR≤4exp(cid:0)−c0m0D2p(cid:1)withprobabilityatleast1−O(p−L).ProofofTheorem3Proof.FirstweshowthatΣ−1kisfeasiblefortheoptimizationproblem(3.7),thatiskˆΣkΣ−1k−Ik∞<λ1,N.ItsuﬃcestoshowthatkˆΣk−Σkk∞<C−1‘1λ1,NbecausekˆΣkΣ−1k−Ik∞≤16S2.PROOFSOFMAINRESULTSkˆΣk−Σkk∞kΣ−1kk‘1≤kˆΣk−Σkk∞C‘1.Thefollowinglemmaestablishesthisresult,givenourchoiceofλ1,N≥CC‘1p(logp)/(Nm0)andtheassumptionlogp≤c0Nwithsomesuﬃcientlysmallc0>0.Lemma2.RecallthenumberofsetfromclasskisdenotedasNk=PNi=11{Yi=k}.ThengivenanypositiveintegerN1andN2,wehavethatwithprobabilityatleast1−O(p−L)(i)kˆµk−µkk∞≤C0p(logp)/(Nkm0)and(ii)kˆΣk−Σkk∞≤C0(p(logp)/(Nkm0)+(logp)/(Nkm0)),k=1,2,wherepositiveconstantC0dependsonCe,cmandLonly.Undertheassumptionlogp≤c0Nwithsomesuﬃcientlysmallc0>0,wefurtherhavethat(i)kˆµk−µkk∞≤Cp(logp)/(Nm0)and(ii)kˆΣk−Σkk∞≤Cp(logp)/(Nm0),k=1,2withprobabilityatleast1−O(p−L),wheretheconstantCalsodependsCπbesidesCe,cm,L.Fromnowon,weconditionontheeventinwhichbothresultsofthesecondpartinLemma2hold.Wenextcontrolthesupnormbound(cid:13)(cid:13)(cid:13)Σ−1k−˜Ωk(cid:13)(cid:13)(cid:13)∞.SincebothΣ−1kand˜Ωkarefeasiblefor(3.7),wehavekˆΣk(Σ−1k−˜Ωk)k∞=kˆΣkΣ−1k−I−(ˆΣk˜Ωk−I)k∞≤2λ1,N.Moreover,kΣk(Σ−1k−˜Ωk)k∞≤k(ˆΣk−Σk)(Σ−1k−˜Ωk)k∞+kˆΣk(Σ−1k−˜Ωk)k∞≤kΣ−1k−˜Ωkk‘1kˆΣk−Σkk∞+2λ1,N≤(cid:16)(cid:13)(cid:13)Σ−1k(cid:13)(cid:13)‘1+k˜Ωkk‘1(cid:17)C−1‘1λ1,N+2λ1,N≤2C‘1C−1‘1λ1,N+2λ1,N=4λ1,N,wherewehaveusedthefact˜ΩkisthesolutionofCLIMEwhichimpliesforeachj=1,...,p,k(˜Ωk)jk1≤(cid:13)(cid:13)(Σ−1k)j(cid:13)(cid:13)1andhencek˜Ωkk‘1≤(cid:13)(cid:13)Σ−1k(cid:13)(cid:13)‘1,where(˜Ωk)jand(Σ−1k)jdenotethejthcolumnof˜ΩkandΣ−1krespectively.WeconcludewithkΣ−1k−˜Ωkk∞≤kΣ−1kk‘1kΣk(Σ−1k−˜Ωk)k∞≤4M0λ1,N.17S2.PROOFSOFMAINRESULTSBasedonthesupnormboundobtainedabove,wehavek(˜Ω2−˜Ω1)−∇k∞≤kΣ−11−˜Ω1k∞+kΣ−12−˜Ω2k∞≤8C‘1λ1,N.(S2.22)Recallthatsupp(∇)isthesupportofthematrix∇.Thethresholdingstep(3.8),togetherwith(S2.22),guaranteesthat˜∇ij=0forany(i,j)/∈supp(∇),notingthatλ01,N≥8C‘1λ1,N.Thereforewehaveshownthesubsetselectionresult,thatis,pr(supp(˜∇)⊂supp(∇))=1−O(p−L).Moreover,wehavethatk˜∇−∇k∞≤8C‘1λ1,N+λ01,N≤2λ01,N.Intheend,wecompletetheproofbynotingthattheFrobeniusnormboundandvector‘1normboundaretheconsequencesofsupnormboundandsubsetselectionresult,thatis,pr(k˜∇−∇kF≤2λ01,N√sq)=1−O(p−L)andpr(k˜∇−∇k1≤2λ01,Nsq)=1−O(p−L).ProofofTheorem4Proof.Weﬁrstshowthat(β1,β2)=(Σ−11µ1,Σ−12µ2)isfeasiblein(3.9)withtheconstantL1setasCβ.Notesincekβkk1≤Cβ,Thepair(β1,β2)satisﬁesthe‘1normconstraint.Thisfact,togetherwiththefollowinglemma,impliesthat(β1,β2)isfeasiblewithprobabilityatleast1−O(p−L)andhencekˆβk1≤kβk1.Lemma3.Undertheassumptionlogp≤c0Nwithsomesuﬃcientlysmallconstantc0>0,wehavethatpr(kˆΣkβk−ˆµkk∞≥CqlogpNm0)≤C0p−L,k=1,2,whereC0>0issomeuniversalconstantandconstantC>0dependsonCe,cm,Cπ,Cβ,CµandLonly.Nextweshowthatk˜β−βk∞≤6C‘1λ2,N.Noticethatfork=1,2,thereexistssome18S2.PROOFSOFMAINRESULTSconstantC>0suchthatwithprobabilityatleast1−O(p−L),kΣk(cid:16)˜βk−βk(cid:17)k∞≤kˆΣk(cid:16)˜βk−βk(cid:17)k∞+k(cid:16)Σk−ˆΣk(cid:17)(cid:16)˜βk−βk(cid:17)k∞≤kˆΣkβk−ˆµkk∞+kˆΣk˜βk−ˆµkk∞+kΣk−ˆΣkk∞(cid:16)kβkk1+k˜βkk1(cid:17)≤2λ2,N+2CβCrlogpNm0≤3λ2,N,wherewehaveusedassumptiononkβkk1,constraintsonestimators,thechoiceofourλ2,Nandtheresult(ii)ofthesecondpartinLemma2.Thereforewefurtherhave,k˜β−βk∞≤2Xk=1k˜βk−βkk∞≤2Xk=1kΣ−1kk‘1kΣk(cid:16)˜βk−βk(cid:17)k∞≤6C‘1λ2,N.(S2.23)Intheend,weconditionontheeventinwhichboth(S2.23)andthefactthat(β1,β2)isfeasiblehold.Theargumentsaboveimplythiseventholdswithprobabilityatleast1−O(p−L).Wearereadytoprovetheratesofconvergenceof˜βunder‘1and‘2normlosses.DenotethesupportofβbyT.Sett=6C‘1λ2,Nandthethresholdedversionof˜βas˜βthr=(˜βthrj),where˜βthrj=˜βj1n|˜βj|≥2to.Sinceβ=β1−β2isfeasible,wehavethatkβk1≥k˜βk1=k˜βthrk1+k˜β−˜βthrk1≥k˜β−˜βthrk1+kβk1−k˜βthr−βk1.Thereforeweobtainthatk˜β−˜βthrk1≤k˜βthr−βk1,whichfurtherimpliesthatk˜β−βk1≤2k˜βthr−βk1.Toshowtheboundofk˜β−βk1,itsuﬃcestoboundk˜βthr−βk1.Indeed,weboundits‘2normasanintermediatestep,k˜βthr−βk2=k(cid:16)˜βthr−β(cid:17)Tk2=Xj∈T(cid:16)˜βthrj−βj(cid:17)21n˜βthrj=0o+Xj∈T(cid:16)˜βj−βj(cid:17)21n˜βthrj6=0o≤Xj∈Tβ2j1{βj≤3t}+slt2≤10slt2,(S2.24)wherewehaveusedsupnormbound(S2.23)intheﬁrstandthirdequationsandthefact19S2.PROOFSOFMAINRESULTS|T|≤slduetoβ∈F0(sl)inthethirdandfourthequations.Consequently,k˜βthr−βk1=k(cid:16)˜βthr−β(cid:17)Tk1≤√slk˜βthr−βk=√10slt,whichcompletesourﬁrstdesiredresultk˜β−βk1≤2√10slt=12√10C‘1slλ2,N.Toshowtheboundofk˜β−βk≤k˜βthr−βk+k˜β−˜βthrk,itsuﬃcestoboundk˜β−˜βthrkgiven(S2.24).Tothisend,wenotekβk1≥k˜βk1impliesthatk˜βTck1≤k˜β−βk1≤2√10slt.Moreover,k˜β−˜βthrk2=k(cid:16)˜βthr−˜β(cid:17)Tk2+k(cid:16)˜βthr−˜β(cid:17)Tck2≤4t2sl+Xj∈Tc˜β2j1n|˜βj|<2to≤4t2sl+k˜βTck1maxj∈Tc{|˜βj|1n|˜βj|<2to}≤(4+4√10)t2sl,(S2.25)wheretheﬁrstinequalityfollowsfrom|˜βthrj−˜βj|<2tand|T|≤sl,andthesecondoneisduetoH¨older’sinequality.Thereforecombining(S2.24)and(S2.25),weobtainedtheseconddesiredresultk˜β−βk≤√slt(√10+(4+4√10)1/2).ProofofTheorem5Proof.Sinceweusesamplesplittingtechnique,estimators˜βand˜∇areindependentwiththesecondbatchofthetrainingdatausedin(3.10).Weassumeﬁxed˜βand˜∇,whichsatisfyourassumptionsthroughouttheanalysis.Withaslightabuseofnotation,westilluseNtodenotethenumberofsamplesets,althoughonlyhalfofthesamplesetsareappliedtocountnkandˆπk,k=1,2.Recallthat¯XiandSiarethesamplemeanandvarianceoftheithsetofobservations.Deﬁne˜Zi=log(ˆπ1/ˆπ2)/Mi+¯XTi˜β+¯XTi˜∇¯Xi/2+tr(˜∇Si)/2,whichisusedtoapproximateZi=log(π1/π2)/Mi+¯XTiβ+¯XTi∇¯Xi/2+tr(∇Si)/2.Tofacilitateanalysis,wedenote20S2.PROOFSOFMAINRESULTS‘(θ0|{(Xi,Yi)}Ni=1,˜β,˜∇)as‘(θ0)forshort.Rewriteourestimatorinthefollowingway,˜β0=argminθ0∈R‘(θ0),where‘(θ0)=1NΣNi=1[log(cid:16)1+exp(Mi(θ0+˜Zi))(cid:17)−(2−Yi)Mi(cid:16)θ0+˜Zi(cid:17)].Westartouranalysisbyconditioningon{Xi}Ni=1.Deﬁne‘0(θ0,˜Z)=E(‘(θ0)|{Xi}Ni=1)wheretheexpectationisunderstoodastheconditionalexpectationgiven{Xi}Ni=1.Notethatthefunction‘0(θ0,˜Z)dependsonθ0,{Mi}Ni=1and{˜Zi}Ni=1only.Thenthediﬀerence‘(θ0)−‘0(θ0,˜Z)=1NΣNi=1(Yi−E(Yi|Xi))Mi(θ0+˜Zi):=Eθ0.Recallβ0isthetrueconstantcoeﬃcient.Since˜β0istheminimizer,wehave‘(˜β0)≤‘(β0),i.e.,‘0(˜β0,˜Z)≤‘0(β0,˜Z)+Eβ0−E˜β0≤‘0(β0,˜Z)+m0R1(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12).(S2.26)Intheend,weneedtoboundthetermR1=|1Nm0ΣNi=1(Yi−E(Yi|Xi))Mi|.ByapplyingHo-eﬀding’sinequality(e.g.Vershynin,2012,Proposition5.10),weobtainR1≤Crp(logp)/Nwithprobabilityatleast1−O(p−L),whereconstantCrdependsonLandCmonly,notingthatMi≤Cmm0byCondition2.ThisprobabilisticstatementonboundingR1isvalidconditioningonanyrealizationof{Xi}Ni=1andthusisalsovalidunconditionally.NextweapplytheTaylorexpansiontothefunction‘0(θ0,˜Z)toanalyzeourestimator.Hereduetomisspeciﬁedvalues˜Zi,weneedareﬁnedversionofTaylorexpansion(Bachetal.,2010,Proposition1).Lemma4(Bachetal.(2010)).Letg(t):R→Rbeaconvexthreetimesdiﬀerentiablefunctionsuchthatitsatisﬁesforallt∈R,|g000(t)|≤Lg00(t)forsomeL>0.Thenwehave21S2.PROOFSOFMAINRESULTSforanytandv∈R,g(t+v)≥g(t)+vg0(t)+g00(t)L2(e−L|v|+L|v|−1).Itisnothardtoseethatthethirdderivativeof‘0(θ0,˜Z)w.r.t.θ0isboundedbyitssecondderivativeuptoamultiplicativefactormaxiMi,i.e.,maxθ0(cid:12)(cid:12)(cid:12)‘0000(θ0,˜Z)/‘000(θ0,˜Z)(cid:12)(cid:12)(cid:12)≤maxiMi,wherehereafter‘00(·,·),‘000(·,·)and‘0000(·,·)aredeﬁnedastheﬁrst,secondandthirdderivativeof‘0(·,·)w.r.t.theﬁrstargumentrespectively.ApplyingLemma4to‘0(θ0,˜Z)atpointβ0andbyCondition2,weobtainthat‘0(˜β0,˜Z)−‘0(β0,˜Z)≥‘00(β0,˜Z)(˜β0−β0)+‘000(β0,˜Z)C2mm20(e−Cmm0|˜β0−β0|+Cmm0(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12)−1).(S2.27)Notethatwithmisspeciﬁedvalues˜Zi,ingeneral‘00(β0,˜Z)6=0.Toﬁnishourproof,weneedanupperboundfor‘00(β0,˜Z)andalowerboundfor‘000(β0,˜Z)withmisspeciﬁedvalues˜Zi.Thustheterm|˜Zi−Zi|criticallydeterminestheestimationaccuracy.Thefollowingboundof|˜Zi−Zi|ishelpfulforourlateranalysis.Lemma5.UndertheassumptionsofTheorem5,thereexistssomeconstantCz>0depend-ingoncm,Cm,Cπ,CµandCesuchthatwithprobabilityatleast1−O(p−L)wehaveuniformlyforalli=1,...,N(cid:12)(cid:12)(cid:12)˜Zi−Zi(cid:12)(cid:12)(cid:12)≤1Mi(cid:12)(cid:12)(cid:12)(cid:12)log(cid:18)ˆπ1π2ˆπ2π1(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)+(cid:12)(cid:12)(cid:12)¯XTi(cid:16)˜β−β(cid:17)(cid:12)(cid:12)(cid:12)+1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1XTij(cid:16)˜∇−∇(cid:17)Xij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤Cz (1+rlogpm0)k˜β−βk+(1+logpm0)k˜∇−∇k1+maxk=1,2|πk−ˆπk|m0!.(S2.28)Indeed,theconclusion(S2.28)isvalidwiththesameprobability1−O(p−L)conditioningonanyrealizationof{Yi}Ni=1and{Mi}Ni=1.22S2.PROOFSOFMAINRESULTSLemma5andourassumptionimplythatwithprobabilityatleast1−O(p−L)wehavem0maxi|˜Zi−Zi|:=R2issuﬃcientlysmall.Notethattheexpectationofthescorefunction‘00(β0,Z)=0where‘00(β0,Z)isobtainedbyreplacing˜ZibyZiin‘00(β0,˜Z),i=1,...,N.Wearereadytoboundthemagnitudeof‘00(β0,˜Z),(cid:12)(cid:12)(cid:12)‘00(β0,˜Z)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1NΣNi=1 Miexp(Mi(β0+˜Zi))1+exp(Mi(β0+˜Zi))−Miexp(Mi(β0+Zi))1+exp(Mi(β0+Zi))!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤1NΣNi=1M2i(cid:12)(cid:12)(cid:12)˜Zi−Zi(cid:12)(cid:12)(cid:12)≤C2mm0R2,(S2.29)wheretheﬁrstinequalityfollowsfromthatthederivativeofexp(Mi(β0+˜Zi))1+exp(Mi(β0+˜Zi))w.r.t.˜ZiisalwaysboundedbyMiandthesecondinequalityisduetoCondition2,Mi≤Cmm0anddeﬁnitionofR2.Moreover,byCondition4,wehavethattheexpectationofthei.i.d.boundedrandomvariableVar(Yi|Xi)=exp(Mi(β0+Zi))(1+exp(Mi(β0+Zi)))2,i=1,...,N,isboundedawayfromClog.WeapplyHoeﬀding’sinequalityandthefactlogp≤c0Ntoobtainthatwithprobabilityatleast1−O(p−L),wehave1NΣNi=1M2i(cid:18)exp(Mi(β0+Zi))1+exp(Mi(β0+Zi))(cid:19)(cid:18)11+exp(Mi(β0+Zi))(cid:19)≥C0lowm20,wherethepositiveconstantC0low>0dependsonClogandL.Sincem0maxi|˜Zi−Zi|:=R2issuﬃcientlysmallwithprobabilityatleast1−O(p−L),theunionboundargumentfurtherimpliesthat‘000(β0,˜Z)=1NΣNi=1M2iexp(cid:16)Mi(β0+˜Zi)(cid:17)1+exp(cid:16)Mi(β0+˜Zi)(cid:17)11+exp(cid:16)Mi(β0+˜Zi)(cid:17)≥Clowm20,(S2.30)23S2.PROOFSOFMAINRESULTSwithprobabilityatleast1−O(p−L)forsomepositiveconstantClow>0.Intheend,plugging(S2.26),(S2.29)and(S2.30)into(S2.27)andapplyingtheunionboundargument,weobtainthatwithprobability1−O(p−L),ClowC−2m(e−Cmm0|˜β0−β0|+Cmm0(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12)−1)≤m0(cid:0)C2mR2+R1(cid:1)(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12).(S2.31)Weapplythefollowingfacte−2γ/(1−γ)+(1−γ)2γ1−γ−1≥0forγ∈(0,1),to(S2.31)andobtainthatCmm0(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12)≤2Cm(C2mR2+R1)/Clog1−Cm(C2mR2+R1)/Clog.SinceC2mR2+R1aresuﬃcientlysmall,wehavethatCm(C2mR2+R1)/Clog<1/2whichimpliesCmm0|˜β0−β0|<2.Thisfactitselffurtherimpliesthat(e−Cmm0|˜β0−β0|+Cmm0|˜β0−β0|−1)≥(Cmm0|˜β0−β0|)2/2.Consequently,(S2.31)impliesthat(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12)≤2C−1lowm−10(cid:0)C2mR2+R1(cid:1),whichfurthercompletesourproof,togetherwithLemma5(boundofR2)andtheboundofR1,(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12)≤Cδ (1+rlogpm0)k˜β−βk+(1+logpm0)k˜∇−∇k1+maxk=1,2|πk−ˆπk|m0+slogpNm20!,wheretheconstantCδ=2C−1low(C2mCz+Cr).24S2.PROOFSOFMAINRESULTSProofofTheorem6Proof.Recallthatforeachk=1,2,thecorrespondingBayesriskandgeneralizationerrorofCLIPSclassiﬁercanbedecomposedasRBk=Cmm0Xm=cmm0pr(φB(X†)6=k|Y†=k,M†=m)pM(m):=XmRBk,mpM(m),˜Rk=Cmm0Xm=cmm0pr(˜φ(X†)6=k|Y†=k,M†=m)pM(m):=Xm˜Rk,mpM(m).Therefore,itissuﬃcienttoboundthediﬀerence˜Rk,m−RBk,mforeachﬁxedk=1,2andﬁxedm∈[cmm0,Cmm0].RecallthatΞN=(1+qlogpm0)k˜β−βk+(1+logpm0)k˜∇−∇k1+maxk=1,2|ˆπk−πk|m0+|˜β0−β0|.DeﬁnetheeventE0={ΞN≤CΞκN},whereκN=(1+logpm0)sqλ01,N+(1+qlogpm0)C‘1√slλ2,N+qlogpNm20,theconstantCΞ=2(2+C00)(Cδ+1)andotherconstantsC00,CδcanbetrackedbackfromTheorems3-5.Weﬁrstshowthatourestimatorssatisfythatpr(E0)=1−O(p−L)byTheorems3-5.Indeed,Theorems3and4providesboundsofk˜β−βkandk˜∇−∇k1respectively.Theestimationerrorofmaxk=1,2|ˆπk−πk|/m0followsfromLemma1.Assumingtheseboundshold,theﬁrstpartofCondition5impliesthattheassumptioninTheorem5issatisﬁedwiththeinitialestimatorsbeingourquadraticandlinearestimators.ThusTheorem5furtherimpliestheupperboundfor|˜β0−β0|.Hereafter,weassumeeventE0holds.WefollowthenotationintroducedintheproofofTheorem5onthesetofobservations(X†,Y†)anddeﬁne˜Z=log(ˆπ1/ˆπ2)/M†+¯xT˜β+¯xT˜∇¯x/2+tr(˜∇S)/2,whichisusedtoapproximateZ=log(π1/π2)/M†+¯xTβ+¯xT∇¯x/2+tr(∇S)/2,where¯xandSarethesamplemeanandcovarianceofthesetX†.ThenwedeﬁnetheeventEz={|˜Z−Z|≤CzΞN}.Lemma5appliedto(X†,Y†)andthesecondpartofCondition5implythatoneventE0uniformlyforallk=1,2andm∈[cmm0,Cmm0],wehavepr(Ez|Y†=k,M†=m)≥1−C0gp−L.25S2.PROOFSOFMAINRESULTSWithoutlossofgenerality,wefocusonthecasek=1.Recall˜Rk,mreliesontheestimators˜β0,˜β,˜∇,ˆπ1andˆπ2andhenceisrandom.OntheeventE0,wehavethat˜R1,m=pr(cid:16)˜Z+˜β0≤0|Y†=1,M†=m(cid:17)=pr(cid:16)Z+β0≤Z−˜Z+β0−˜β0|Y†=1,M†=m(cid:17)=pr(cid:16)Z+β0≤Z−˜Z+β0−˜β0,Ez|Y†=1,M†=m(cid:17)+pr(Ecz|Y†=1,M†=m)≤C0gp−L+pr(cid:0)Z+β0≤(Cz+1)ΞN,Ez|Y†=1,M†=m(cid:1)≤C0gp−L+pr(cid:0)Z+β0≤(Cz+1)CΞκN|Y†=1,M†=m(cid:1)=C0gp−L+F1,m((Cz+1)CΞκN),(S2.32)wheretheﬁrstinequalityfollowsfromtheconditionalprobabilitypr(Ez|Y†=k,M†=m)≥1−C0gp−LandthedeﬁnitionoftheeventEz,thesecondinequalityisduetotheeventE0,andthelastequalityfollowsfromthedeﬁnitionofthecumulativedistributionfunctionF1,m(t).Inaddition,bythedeﬁnitionofthedeterministicvalueRBk,m,wehaveRB1,m=pr(cid:0)Z+β0≤0|Y†=1,M†=m(cid:1)=F1,m(0).(S2.33)Byourassumption,thequantity(Cz+1)CΞκNissuﬃcientlysmallandhencelessthanδ0.Itfollowsfrom(S2.32)-(S2.33)anddeﬁnitionofdNthatontheeventE0,wehavethat˜R1,m−RB1,m≤C0gp−L+supt∈[−δ0,δ0](cid:12)(cid:12)F01,m(t)(cid:12)(cid:12)(Cz+1)CΞκN≤C0gp−L+(Cz+1)CΞκNdN.Similarlywecanshowthatsameupperboundappliesto˜R2,m−RB2,muniformlyforallm∈[cmm0,Cmm0].ThereforeontheeventE0,weobtainthat˜R≤RB+C0gp−L+(Cz+1)CΞκNdN,whichcompletesourproof.26S2.PROOFSOFMAINRESULTSProofofTheorem1Proof.ByinspectingtheproofofTheorem3,onerealizesthattheproofofTheorem1isalmostidenticaltothatofTheorem3exceptthattheroleofLemma2isreplacedbythefollowingimportantlemmaundertimeseriesstructuresfordiﬀerentvaluesofν.Morespeciﬁcally,oneonlyneedtoshowthatkˆΣk−Σkk∞<C−1‘1λ1,Nwithprobabilityatleast1−O(p−L)andthechoiceofλ1,Nundertimeseriesstructuresisdeterminedby(result(ii)ofthesecondpartin)Lemma6.Therefore,weomittheproofdetails.Lemma6.Considerthevectorlinearprocessdeﬁnedin(S1.1)thatsatisﬁesthedecaycon-dition(S1.2).SupposeConditions1-3hold.RecallthenumberofsetfromclasskisdenotedasNk=PNi=11{Yi=k}.ThengivenanypositiveintegerN1andN2,wehavethat(i)kˆµk−µkk∞≤C0qlogpNkm0ifν>1C0qlogplog2m0Nkm0ifν=1C0qlogpNkm2ν−10if1/2<ν<1,and(ii)kˆΣk−Σkk∞≤C0(cid:16)qlogpNkm0+logpNkm0(cid:17)ifν>1C0(cid:16)qlogpNkm0+logplog2m0Nkm0(cid:17)ifν=1C0(cid:16)qlogpNkm0+logpNkm2ν−10(cid:17)if3/4<ν<1C0(cid:18)qlogplogm0Nkm0+logpNkm1/20(cid:19)ifν=3/4C0(cid:16)qlogpNkm4ν−20+logpNkm2ν−10(cid:17)if1/2<ν<3/4,fork=1,2withprobabilityatleast1−O(p−L),wherepositiveconstantC0dependsonCe,cm,CTSandLonly.Inaddition,undertheassumptionlogp≤c0Nwithsomesuﬃcientlysmallc0>0,we27S2.PROOFSOFMAINRESULTShavethat(i)kˆµk−µkk∞≤CqlogpNm0ifν>1Cqlogplog2m0Nm0ifν=1CqlogpNm2ν−10if1/2<ν<1,and(ii)kˆΣk−Σkk∞≤CqlogpNm0ifν>3/4Cqlogplogm0Nm0ifν=3/4CqlogpNm4ν−20if1/2<ν<3/4,fork=1,2withprobabilityatleast1−O(p−L),wherepositiveconstantCalsodependsonCπbesidesCe,cm,CTSandL.ProofofTheorem2Proof.ByinspectingtheproofofTheorem4,onerealizesthattheproofofTheorem2isalmostidenticaltothatofTheorem4exceptthattheroleofLemma3isreplacedbythefollowingimportantlemma(Lemma7)undertimeseriesstructuresfordiﬀerentvaluesofν.Morespeciﬁcally,theresultsfollowfromsomealgebra(deterministically)ontheeventthatbothk˜β−βk∞≤6C‘1λ2,Nandthat(β1,β2)isfeasiblehold.Tothisend,Lemma7impliesthat(β1,β2)isfeasiblewithprobabilityatleast1−O(p−L).Inaddition,weshowthatthechoiceofλ2,NandLemma7togetherimplythatk˜β−βk∞≤6C‘1λ2,Nwithprobability28S2.PROOFSOFMAINRESULTSatleast1−O(p−L).Indeed,ontheeventthat(β1,β2)isfeasible,wehavekΣk(cid:16)˜βk−βk(cid:17)k∞≤kˆΣk(cid:16)˜βk−βk(cid:17)k∞+k(cid:16)Σk−ˆΣk(cid:17)(cid:16)˜βk−βk(cid:17)k∞≤kˆΣkβk−ˆµkk∞+kˆΣk˜βk−ˆµkk∞+kΣk−ˆΣkk∞(cid:16)kβkk1+k˜βkk1(cid:17)≤2λ2,N+2CβCκΣ≤3λ2,N,whereκΣ=qlogpNm0(qlogplogm0Nm0,qlogpNm4ν−20)whenν>3/4(ν=3/4,1/2<ν<3/4)respectively.Intheabovederivation,wehaveusedassumptiononkβkk1,constraintsonestimators,thechoiceofourλ2,N(i.e.,2CβCκΣ≤λ2,N)andtheresult(ii)ofthesecondpartinLemma6.Thereforewefurtherhave,k˜β−βk∞≤2Xk=1k˜βk−βkk∞≤2Xk=1kΣ−1kk‘1kΣk(cid:16)˜βk−βk(cid:17)k∞≤6C‘1λ2,N.Therefore,wecompletetheproof.Lemma7.Considerthevectorlinearprocessdeﬁnedin(S1.1)thatsatisﬁesthedecaycon-dition(S1.2).SupposeConditions1-3hold.Undertheassumptionskβkk1≤Cβ,k=1,2withsomeconstantsCβ>0andlogp≤c0Nwithsomesuﬃcientlysmallconstantc0>0,wehavethatkˆΣkβk−ˆµkk∞≤CqlogpNm0ifν>1Cqlogplog2m0Nm0ifν=1CqlogpNm2ν−10if1/2<ν<1,fork=1,2withprobabilityatleast1−O(p−L),whereconstantC>0dependsonCe,cm,Cπ,Cβ,Cµ,CTSandLonly.29S2.PROOFSOFMAINRESULTSProofofTheorem3Proof.ByinspectingtheproofofTheorem5,onerealizesthattheproofofTheorem3isverysimilartothatofTheorem4.Themajordiﬀerencesarethatβ0isreplacedbyβ0,TSandthattheroleofLemma5isreplacedbyLemma8undertimeseriesstructuresfordiﬀerentvaluesofν,whichisprovidedattheendofthisproof.WeonlyhighlightthediﬀerencesfromtheproofofTheorem3belowbrieﬂy.Westilldeﬁne˜Zi=log(ˆπ1/ˆπ2)/Mi+¯XTi˜β+¯XTi˜∇¯Xi/2+tr(˜∇Si)/2,whichisusedtoapproximateZi=log(π1/π2)/Mi+¯XTiβ+¯XTi∇¯Xi/2+tr(∇Si)/2.Notethatunderthegeneraltimeseriesstructures,β0,TSisthepopulationminimizerofthelossfunction.Thus,wecanstillobtaintheinequalitysimilarto(S2.26),i.e.,‘0(˜β0,˜Z)≤‘0(β0,TS,˜Z)+Eβ0,TS−E˜β0≤‘0(β0,TS,˜Z)+m0R1(cid:12)(cid:12)(cid:12)˜β0−β0,TS(cid:12)(cid:12)(cid:12),(S2.34)whereR1=|1Nm0ΣNi=1(Yi−E(Yi|Xi))Mi|≤Crp(logp)/Nwithprobabilityatleast1−O(p−L)byHoeﬀding’sinequality.Again,thisstatementisvalidconditioningonanyrealizationof{Xi}Ni=1andthusisalsovalidunconditionally.Inaddition,applyingLemma4to‘0(θ0,˜Z)atpointβ0,TSandbyCondition2,westillhave(S2.27)withβ0beingreplacedbyβ0,TS,i.e.,‘0(˜β0,˜Z)−‘0(β0,TS,˜Z)(S2.35)≥‘00(β0,TS,˜Z)(˜β0−β0,TS)+‘000(β0,TS,˜Z)C2mm20(e−Cmm0|˜β0−β0,TS|+Cmm0(cid:12)(cid:12)(cid:12)˜β0−β0,TS(cid:12)(cid:12)(cid:12)−1).Wenextbound‘00(β0,TS,˜Z)fromaboveandbound‘000(β0,TS,˜Z)frombelow.ByapplyingLemma8andourassumption,wehavethatwithprobabilityatleast1−O(p−L),m0maxi|˜Zi−Zi|:=R2issuﬃcientlysmall.Therefore,withthefactthat‘00(β0,TS,Z)=30S2.PROOFSOFMAINRESULTS0where‘00(β0,TS,Z)isobtainedbyreplacing˜ZibyZiin‘00(β0,TS,˜Z),i=1,...,N,wecanstillobtainanupperbound|‘00(β0,TS,˜Z)|similarto(S2.29),i.e.,(cid:12)(cid:12)(cid:12)‘00(β0,TS,˜Z)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1NΣNi=1 Miexp(Mi(β0,TS+˜Zi))1+exp(Mi(β0,TS+˜Zi))−Miexp(Mi(β0,TS+Zi))1+exp(Mi(β0,TS+Zi))!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤C2mm0R2,(S2.36)Moreover,byCondition1,wehavethattheexpectationofthei.i.d.boundedrandomvariableexp(Mi(β0+Zi))(1+exp(Mi(β0+Zi)))2,i=1,...,N,isboundedawayfromClog.Followingasimilarargument,weareabletoobtainasimilarresultto(S2.30),i.e.,withprobabilityatleast1−O(p−L),‘000(β0,TS,˜Z)=1NΣNi=1M2iexp(cid:16)Mi(β0,TS+˜Zi)(cid:17)1+exp(cid:16)Mi(β0,TS+˜Zi)(cid:17)11+exp(cid:16)Mi(β0,TS+˜Zi)(cid:17)≥Clowm20.(S2.37)Intheend,plugging(S2.34),(S2.36)and(S2.37)into(S2.35)andapplyingtheunionboundargument,weobtainthatwithprobability1−O(p−L),ClowC−2m(e−Cmm0|˜β0−β0,TS|+Cmm0(cid:12)(cid:12)(cid:12)˜β0−β0,TS(cid:12)(cid:12)(cid:12)−1)≤m0,TS(cid:0)C2mR2+R1(cid:1)(cid:12)(cid:12)(cid:12)˜β0−β0(cid:12)(cid:12)(cid:12).(S2.38)Thenfollowingasimilardeterministicargument,weobtainthatwithprobability1−O(p−L),(cid:12)(cid:12)(cid:12)˜β0−β0,TS(cid:12)(cid:12)(cid:12)≤2C−1lowm−10(cid:0)C2mR2+R1(cid:1),whichfurthercompletesourproof,togetherwithLemma8andtheboundofR1,(cid:12)(cid:12)(cid:12)˜β0−β0,TS(cid:12)(cid:12)(cid:12)≤Cδ (1+Uβ)k˜β−βk1+(1+U∇)k˜∇−∇k1+maxk=1,2|πk−ˆπk|m0+slogpNm20!.Lemma8.UndertheassumptionsofTheorem3,thereexistssomeconstantCz>0de-pendingoncm,Cm,Cπ,Cµ,CTSandCesuchthatwithprobabilityatleast1−O(p−L)wehave31S2.PROOFSOFMAINRESULTSuniformlyforalli=1,...,N(cid:12)(cid:12)(cid:12)˜Zi−Zi(cid:12)(cid:12)(cid:12)≤1Mi(cid:12)(cid:12)(cid:12)(cid:12)log(cid:18)ˆπ1π2ˆπ2π1(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)+(cid:12)(cid:12)(cid:12)¯XTi(cid:16)˜β−β(cid:17)(cid:12)(cid:12)(cid:12)+1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1XTij(cid:16)˜∇−∇(cid:17)Xij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤Cz(cid:18)(1+Uβ)k˜β−βk1+(1+U∇)k˜∇−∇k1+maxk=1,2|πk−ˆπk|m0(cid:19),(S2.39)whereUβsatisﬁesUβ=qlogpm0ifν>1qlogplog2m0m0ifν=1qlogpm2ν−10if1/2<ν<1,andU∇satisﬁesU∇=logpm0ifν>1logplog2m0m0ifν=1logpm2ν−10if1/2<ν<1.Indeed,theconclusion(S2.39)isvalidwiththesameprobability1−O(p−L)conditioningonanyrealizationof{Yi}Ni=1and{Mi}Ni=1.ProofofTheorem4Proof.ByinspectingtheproofofTheorem6,onerealizesthattheproofofTheorem4isalmostidenticaltothatofTheorem6withφB,β0,ΞN,κN,Fk,mandRBbeingreplacedbytheircounterpartsφB,TS,β0,TS,ΞN,TS,κN,TS,Fk,m,TSandRB,TSunderthetimeseriesstructurerespectively.Therefore,weomittheproofdetails.32S3.PROOFSOFSUPPORTINGLEMMASS3.ProofsofSupportingLemmasProofofLemma1Proof.Recalln1=PNi=1Mi1{Yi=1}with1{Yi=1}i.i.d.Bernoulliwithprobabilityπ1∈[Cπ,1−Cπ]andMi∈[cmm0,Cmm0]withprobability1.Hoeﬀding’sinequality(e.g.Vershynin,2012,Proposition5.10)impliesthatthereexistssomeconstantC0dependingonCπandLonlysuchthat(iv)holds,i.e.|π1−ˆπ1|≤C0qlogpNwithprobabilityatleast1−p−L.Consequently,n1≥cNm0forsomeconstantcdependingoncm,CπandLwithprobabilityatleast1−p−Lgivenlogp≤c0NandCondition3.Similarresultsapplytoˆπ2andn2.Fromnowon,weconditionontheaboveeventandonlyneedtoshow(i)-(iii)holdwithprobabilityatleast1−p−L.SinceΣ−1/2k(ˆµk−µk)∼N(0,1nkIp),thetailprobabilityofChi-squareddistribution(Lau-rentandMassart,2000,e.g.)impliesthatforany0<t<1,pr(|k√nkΣ−1/2k(ˆµk−µk)k2/p−1|≥t)≤2exp(pt2/8).Hence,bypickingasmallt(e.g.t=0.1)aswellasCondition1andnk>cNm0,weobtaintheresult(ii)holdswithprobabilityatleast1−O(p−L).Inaddition,itfollowsfromtheDavidson-Szarekbound(e.g.DavidsonandSzarek,2001,TheoremII.7)thatforeachk,thereexistssomeconstantC>0dependingonCe,LsuchthatkΣk−ˆΣkk‘2<Cpp/(Nm0)withprobabilityatleast1−2p−L,givenCondition1andthefactp<c0Nm0withasuﬃcientlysmallc0.Herek·k‘2denotesthematrixspectralnorm.Consequently,theassumptionp<c0Nm0andCondition1,togetherwithaunionboundargument,impliestheresult(i).Result(iv)alsofollows,notingthatk·kF≤√pk·k‘2.33S3.PROOFSOFSUPPORTINGLEMMASProofofLemma2Proof.Recallthatnk=PNi=1Mi1{Yi=k}denotethetotalsamplesizeforClassk=1,2.Fromnowon,weconditiononn1andn2.WriteXij=EXij+Uij,whereUij∼N(0,ΣYi).ThenwehaveˆΣk=(1nkP(i,j):Yi=kUijUTij)−(µk−ˆµk)(µk−ˆµk)T.Sinceˆµk−µk∼N(0,1nkΣk),tailprobabilityofnormaldistributionwithunionboundimpliesthatforanyL>0,thereexistssomeconstantC1>0dependingonLonlysuchthatfork=1,2,pr(kˆµk−µkk∞≥C1s(maxjσk,jj)logpnk)≤p−L.(S3.40)Moreover,sinceE1nkP(i,j):Yi=kUijUTij=ΣkandeachentryofUijUTijissub-exponentiallydistributed,Bernstein’sinequality(e.g.Vershynin,2012,Proposition5.16)withunionboundimpliesthatthereexistssomeconstantC2>0dependingonLsuchthatpr(k1nkX(i,j):Yi=kUijUTij−Σkk∞≥C2maxjσk,jj(rlogpnk+logpnk))≤p−L.(S3.41)Combining(S3.40)and(S3.41)andthefactthatMi∈[cmm0,Cmm0],wehaveobtainedbothresults(i)and(ii)oftheﬁrstpartofLemma2withprobabilityatleast1−4p−L,wheretheconstantC0>0dependsoncm,CeandLonly.WemovetothesecondpartofLemma2.NotethedistributionofeachXijisindependentofNkandnk.Wefollowthesameargumentonboundingn1andn2asthatatthebeginningoftheproofofLemma1.Inparticular,givenlogp≤c0N,wehavepr(nk≥cNm0)=1−p−Lfork=1,2andsomeconstantc>0.Thenbothresults(i)and(ii)ofthesecondpartofLemma2immediatelyfollowfromtheﬁrstpartofLemma2andaunionboundargument.34S3.PROOFSOFSUPPORTINGLEMMASProofofLemma3Proof.Wefollowthesameargumentonboundingn1andn2asthatatthebeginningoftheproofofLemma1.Inparticular,givenlogp≤c0N,wehavepr(nk≥cNm0)=1−p−Lfork=1,2andsomeconstantc>0.WriteXij=EXij+Uij,whereUij∼N(0,ΣYi).WehaveˆΣk=(1nkP(i,j):Yi=kUijUTij)−(µk−ˆµk)(µk−ˆµk)T.Result(i)ofLemma2impliesthatthereexistssomeconstantC1>0suchthatpr(kˆµk−µkk∞≥C1rlogpNm0)=O(p−L).(S3.42)Accordingtoourassumptions,wehavekΣ−1kµkk≤λ−1min(Σk)kµkk≤CeCµ.Weconditiononn1andn2.Thenthenormalityofˆµk−µk∼N(0,Σk/nk)yieldsthatfork=1,2andsomeconstantC00dependingonLonly,wehave(cid:12)(cid:12)(cid:12)(µk−ˆµk)TΣ−1kµk(cid:12)(cid:12)(cid:12)≥C00λmax(Σk)CeCµqlogpnkwithprobabilityatmostp−L.Takingunionboundwiththeeventnk≥cNm0,weobtainthatthereexistssomeconstantC02>0suchthatpr((cid:12)(cid:12)(cid:12)(µk−ˆµk)TΣ−1kµk(cid:12)(cid:12)(cid:12)≥C02rlogpNm0)≤2p−L.(S3.43)Therefore,equations(S3.42)-(S3.43)implythathereexistssomeconstantC2>0suchthatwithprobability1−O(p−L),k(µk−ˆµk)(µk−ˆµk)Tβkk∞<C2logpNm0.(S3.44)Byourchoiceofλ2,N,wehavethatλ2,N/2>(C1+C2+C02)p(logp)/(Nm0).Consequently,givenequations(S3.42)-(S3.44),decompositionofΣkandlogp=o(N),toconclude(β1,β2)isfeasible,i.e.(cid:13)(cid:13)(cid:13)ˆΣkβk−ˆµk(cid:13)(cid:13)(cid:13)∞<λ2,N,k=1,2,weonlyneedtoshowwithprobability35S3.PROOFSOFSUPPORTINGLEMMAS1−O(p−L)thatk(1nkX(i,j):Yi=kUijUTij)Σ−1kµk−µkk∞<12λ2,N.(S3.45)Notethattherthcoordinateis1nkP(i,j):Yi=k(cid:0)Uij,rUTijΣ−1kµk−µk,r(cid:1),thesumofi.i.d.cen-teredsub-exponentialvariablesinceeachsummandistheproductoftwonormalvariablesUi,jandUTiΣ−1kµk.Moreover,thesub-exponentialvariablehasconstantparametersinceUTijΣ−1kµkandUij,rhaveboundedvariance.ThusBernstein’sinequality(e.g.Vershynin,2012,Proposition5.16)withunionboundoverallcoordinatesandtheeventnk≥cNm0impliesthatthereexistssomeconstantC3>0suchthat(wealsousedthatlogp≤c0NwhenapplyingtheBernstein’sinequality)pr(k(1nkX(i,j):Yi=kUijUTij)Σ−1kµk−µkk∞>C3rlogpNm0)≤2p−L.(S3.46)BypickingalargeconstantC0inourchoiceofλ2,N,weobtainλ2,N/2>C3p(logp)/(Nm0),whichcompletestheproofof(S3.45).ProofofLemma5Proof.Itissuﬃcienttoshowthatforanyrealizationof{Yi}Ni=1and{Mi}Ni=1,equation(S2.28)isvalidforeachiwithprobabilityatleast1−O(p−L−1).Indeed,thisfact,togetherwiththeunionboundargumentandp≥Nimpliesthedesiredresult.Theﬁrstinequalityof(S2.28)followsfromthedeﬁnitionsof˜ZiandZidirectly.Weshowthesecondinequalityholdsintheremainingofproofwithprobabilityatleast1−O(p−L−1)fortheﬁxedi.Withoutlossofgenerality,weassumeYi=1andMi=m0cm.Recallthattheinitialestimatorssatisfymaxk=1,2|πk−ˆπk|≤CpwithasuﬃcientlysmallconstantCp.Consequently,wehavethatˆπ1,ˆπ2∈[Cπ/2,1−Cπ/2]byCondition3,which36S3.PROOFSOFSUPPORTINGLEMMASfurtheryields1m0cm(cid:12)(cid:12)(cid:12)log(cid:16)ˆπ1π2ˆπ2π1(cid:17)(cid:12)(cid:12)(cid:12)≤Cz1maxk=1,2|πk−ˆπk|/m0withsomeuniversalconstantCz1dependingoncmandCπonlybytheboundednessofˆπ1/ˆπ2.Todealwiththeterm|¯XTi(˜β−β)|,wenotethat¯Xi∼N(µ1,Σ1/(m0cm)),whichimpliesthat|¯XTi(˜β−β)|≤k˜β−βk·kµ1k+k˜β−βk(Ce/(m0cm))1/2|D|,whereD∼N(0,1).Accordingtothetailprobabilityofstandardnormaldistribution,weobtainthatwithprobabilityatleast1−O(p−L−1),that|D|≤C0z√logpwhereC0zonlydependsonL.Thisfact,togetherwiththeassumptionkµ1k≤Cµfurtherimpliesthat|¯XTi(˜β−β)|≤Cz2k˜β−βk(1+p(logp)/m0)withprobability1−O(p−L−1),whereCz2=((Ce/cm)1/2C0z+Cµ).Finally,weprovideanupperboundfor1Mi(cid:12)(cid:12)(cid:12)PMij=1XTij(˜∇−∇)Xij/2(cid:12)(cid:12)(cid:12).SinceXi1,...,XiMiarei.i.d.copiesofN(µ1,Σ1),wenaturallydecomposeitintothreetermsasfollowswithUij:=Xij−µ1∼N(0,Σ1)1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1XTij(cid:16)˜∇−∇(cid:17)Xij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1UTij(cid:16)˜∇−∇(cid:17)Uij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+(cid:12)(cid:12)(cid:12)µT1(cid:16)˜∇−∇(cid:17)µ1/2(cid:12)(cid:12)(cid:12)+1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1µT1(cid:16)˜∇−∇(cid:17)Uij(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).(S3.47)Wedealwiththesethreetermsindividually.Firstofall,|µT1(˜∇−∇)µ1/2|≤C2µk˜∇−∇k1/2bytheassumptionkµ1k≤Cµ.Second,theterm(PMij=1µT1(˜∇−∇)Uij)/MifollowsadistributionofN(0,µT1(˜∇−∇)Σ1(˜∇−∇)µ1/(m0cm)),whichyieldsthatwithprobabilityatleast1−O(p−L−1)that1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1µT1(cid:16)˜∇−∇(cid:17)Uij(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤(cid:16)µT1(cid:16)˜∇−∇(cid:17)Σ1(cid:16)˜∇−∇(cid:17)µ1/(m0cm)(cid:17)1/2C00zplogp≤CµC0z(Ce/cm)1/2k˜∇−∇k1rlogpm0,wherewehaveusedtailprobabilityofstandardnormaldistributionandthelastinequality37S3.PROOFSOFSUPPORTINGLEMMASfollowsfromCondition1.Third,byH¨older’sinequality,wehave1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1UTij(cid:16)˜∇−∇(cid:17)Uij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)tr((cid:16)˜∇−∇(cid:17)MiXj=1UTijUij/Mi)/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤12(cid:13)(cid:13)(cid:13)˜∇−∇(cid:13)(cid:13)(cid:13)1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)MiXj=1UTijUij/Mi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∞.SinceeachentryofPMij=1UTijUij/Mi−Σ1isthesumofcenteredsub-exponentialvariablewithboundedparameter.TheBernstein’sinequality(e.g.Vershynin,2012,Proposition5.16)withunionboundoverallp2entriesimpliesthatthereexistssomeconstantC00z>0dependingonLandCeonlysuchthat(cid:13)(cid:13)(cid:13)PMij=1UTijUij/Mi−Σ1(cid:13)(cid:13)(cid:13)∞≤C00z(qlogpcmm0+logpcmm0)withprobabilityatleast1−O(p−L−1).Therefore,weobtainthatwithprobability1−O(p−L−1),1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1UTij(cid:16)˜∇−∇(cid:17)Uij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤(C00z(rlogpcmm0+logpcmm0)+Ce)k˜∇−∇k1/2,wherewehaveusedkΣ1k∞≤CebyCondition1.Combiningtheupperboundsofthreetermsabove,weﬁnallyobtainthatwithprobability1−O(p−L−1),1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1XTij(cid:16)˜∇−∇(cid:17)Xij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤C0z3(rlogpm0+logpm0+1)k˜∇−∇k1≤Cz3(logpm0+1)k˜∇−∇k1,whereconstantC0z3=C2µ/2+CµC0z(Ce/cm)1/2+(Ce+C00z/√cm+C00z/cm)/2andCz3=2C0z3.Tocompleteourproof,wecombineallboundsfor1m0cm(cid:12)(cid:12)(cid:12)log(cid:16)ˆπ1π2ˆπ2π1(cid:17)(cid:12)(cid:12)(cid:12),|¯XTi(˜β−β)|and1Mi|PMij=1XTij(˜∇−∇)Xij/2|withCz=Cz1+Cz2+Cz3.ProofofLemma6Proof.WeshowtheﬁrstpartofLemma6inthisproof.ThesecondpartofLemma6immediatelyfollowsfromtheﬁrstpart,thatlogp≤c0N,andthefactthatpr(nk≥cNm0)=38S3.PROOFSOFSUPPORTINGLEMMAS1−p−Lfork=1,2andsomeconstantc>0,whichisobtainedfromtheargumentatthebeginningoftheproofofLemma1.Inthisproof,weneedthefollowingtechnicalresult,whichisadirectconsequenceofLemmaVI.1inChenetal.(2016).Lemma9(Chenetal.(2016)).Letν>1/2and(at)t∈Zbearealsequencesuchthatat≤CTS(1+t)−νfort≥0andat=0ift<0.Letγl=P∞t=0|atat+l|.Then(i)γl=O(l−ν)(O(l−1logl)andO(l1−2ν))andPlk=0γk=O(1)(O(log2l)andO(l2−2ν))holdforν>1(ν=1and1/2<ν<1respectively);(ii)Plk=0γ2k=O(1)(O(logl)andO(l3−4ν))holdforν>3/4(ν=3/4and1/2<ν<3/4respectively).Withoutlossofgenerality,weassumethattheﬁrstN1setsarefromClass1(i.e.,Yi=1fori=1,...,N1)andonlyproveresults(i)-(ii)forClass1.Weﬁrstshowresult(i),i.e.,boundthetermkµ1−ˆµ1k∞.Inthefollowing,weboundeachentryofµ1−ˆµ1andthentakeaunionboundargumenttoﬁnishtheproof.Toboundthelthentry(l=1,...,p),i.e.,|µ1l−ˆµ1l|,wecollectthelthentryXij,lofeachobservationXij,i=1,...,N1,j=1,...,Miandobservethatitscenteredversioncanbedenotedaccordingtothevectorlinearprocess(S1.1)as(X1M1,l,...,X11,l;X2M2,l,...,X21,l;...;XN1MN1,l,...,XN11,l)T−(µ1l,...,µ1l)T=A(l)ξ,(S3.48)whereξ=(ξ1M1,ξ1(M1−1)...;ξ2M2,ξ2(M2−1)...;...;ξN1MN1,ξN1(MN1−1)...)Twithi.i.d.N(0,1)en-39S3.PROOFSOFSUPPORTINGLEMMAStries,andA(l)isablockdiagonalmatrix,A(l)=A(l),10000A(l),200.........000A(l),N1,inwhichtheitheblock(i=1,...,N1)A(l),ihasthefollowingformA(l),i=A10,l·A11,l·A12,l·...A1(Mi−1),l·A1Mi,l·...0A10,l·A11,l·...A1(Mi−2),l·A1(Mi−1),l·...00A10,l·...A1(Mi−3),l·A1(Mi−2),l·........................000...A10,l·A11,l·....Intheaboverepresentation,A1t,l·denotesthelthrowofthecoeﬃcientmatrixA1tdeﬁnedinourvectorlinearprocess(S1.1).Given(S3.48),oneimmediatelyobtainsthatµ1−ˆµ1∼N(cid:16)0,1TA(l)(cid:0)A(l)(cid:1)T1/n21(cid:17),(S3.49)wheren1=PN1i=1MidenotethetotalsamplesizeforClass1,and1denotesthen1-dimensionalvectorwitheachentrybeing1.Itremainstoboundthevariancein(S3.49)fordiﬀerentvalueofν>1/2.Tothisend,wenotethat1TA(l)(cid:0)A(l)(cid:1)T1=N1Xi=11TA(l),i(cid:0)A(l),i(cid:1)T1:=N1Xi=11TΓ(l),i1,wherewesetΓ(l),i=A(l),i(cid:0)A(l),i(cid:1)Tand1intheithsummanddenotestheMi-dimensionalvectorwitheachentrybeing1respectively.Duetothetimeseriesstructure,thematrix40S3.PROOFSOFSUPPORTINGLEMMASΓ(l),iisaMi-dimensionalToeplitzmatrixwithelements(γlj)Mi−1j=0,where|γlj|=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∞Xt=0A1t,l·(cid:0)A1(t+j),l·(cid:1)T(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤∞Xt=0 pXk=1a21t,lk!1/2 pXk=1a21(t+j),lk!1/2≤Cζj,(S3.50)whereC>0issomeconstant,ζj=j−ν(j−1logjandj1−2ν)forν>1(ν=1and1/2<ν<1respectively).TheﬁrstinequalityabovefollowsfromCauchy-Schwarzinequalityandthesecondinequalityisduetothedecayconditionofthecoeﬃcientmatrixin(S1.2)andLemma9(i).Consequently,wecanboundthevarianceasfollows,notingthatn1>cmN1m0byCondition3,1TA(l)(cid:0)A(l)(cid:1)T1n21≤PN1i=1MiPMi−1j=0ζj(cmN1m0)2≤CmN1m0PCmm0−1j=0ζj(cmN1m0)2≤C1N1m0ifν>1Clog2m0N1m0ifν=1C1N1m2ν−10if1/2<ν<1,wherethelastinequalityfollowsfromLemma9(i).Intheend,theresult(i)oftheﬁrstpartimmediatelyfollowsfromtheabovevarianceboundandthethetailprobabilityofnormaldistributionwithaunionboundargument.Nowweturntotheresult(ii).Inthefollowing,weboundeachentryofΣ1−ˆΣ1andthentakeaunionboundargumenttoﬁnishtheproof.Toboundthelkthentry(l,k=1,...,p),i.e.,|σ1,lk−ˆσ1,lk|,wenotethatσ1,lk−ˆσ1,lk=1n1(cid:16)ξT(cid:0)A(l)(cid:1)TA(k)ξ−EξT(cid:0)A(l)(cid:1)TA(k)ξ(cid:17)−(µ1l−ˆµ1l)(µ1k−ˆµ1k):=T1+T2,(S3.51)41S3.PROOFSOFSUPPORTINGLEMMASwherethesecondtermcanbeboundedwithprobabilityatleast1−O(p−(L+2))usingtheresult(i)shownabove,thatis,|T2|≤ClogpN1m0ifν>1Clogplog2m0N1m0ifν=1ClogpN1m2ν−10if1/2<ν<1.Itremainstoboundtheﬁrstterm|T1|.Tothisend,weapplytheHanson-Wrightinequality(e.g.RudelsonandVershynin,2013,Theorem1.1)sinceξcontainsi.i.dN(0,1)entries.NotethatbyCondition3,wehavecmm0≤Mi≤Cmm0.Therefore,pr(|T1|≥x)(S3.52)≤2exp(cid:16)−Cminnk(cid:0)A(l)(cid:1)TA(k)k)−2Fx2N21m20,λ−1max(cid:16)(cid:0)A(l)(cid:1)TA(k)(cid:17)xN1m0o(cid:17),whereλmax(·)denotesthelargestsingularvalue.Inwhatfollows,weboundk(cid:0)A(l)(cid:1)TA(k)k2Fandλmax(cid:16)(cid:0)A(l)(cid:1)TA(k)(cid:17)separately.Toboundtheﬁrstterm,wenotethatbyCauchy-Schwarzinequality,k(cid:0)A(l)(cid:1)TA(k)k2F=trace(cid:16)A(l)(cid:0)A(l)(cid:1)TA(k)(cid:0)A(k)(cid:1)T(cid:17)≤kΓ(l)kFkΓ(k)kF,(S3.53)wherewesetΓ(l)=A(l)(cid:0)A(l)(cid:1)T.Inaddition,wehavekΓ(l)k2F=N1Xi=1kΓ(l),ik2F=N1Xi=1(Mi(γl0)2+2(Mi−1)(γl1)2+...+2(γlMi−1)2)≤CN1m0Cmm0−1Xj=0(γlj)2≤CN1m0ifν>3/4CN1m0logm0ifν=3/4CN1m4−4ν0if1/2<ν<3/4,(S3.54)42S3.PROOFSOFSUPPORTINGLEMMASwherethelastinequalityfollowsfromLemma9(ii).Toboundthesecondterm,wenotethatλmax(cid:16)(cid:0)A(l)(cid:1)TA(k)(cid:17)≤λmax(cid:0)Γ(l)(cid:1)1/2λmax(cid:0)Γ(k)(cid:1)1/2.(S3.55)Inaddition,duetotheblockstructureofΓ(l),wehaveλmax(cid:0)Γ(l)(cid:1)=maxi=1,...,N1(cid:8)λmax(cid:0)Γ(l),i(cid:1)(cid:9)≤2Cmm0Xj=0γ(l)j≤Cifν>1Clogm0ifν=1Cm2−2ν0if1/2<ν<1,(S3.56)wherethelastinequalityisduetoLemma9(i).Pluggingequations(S3.53)-(S3.56)intoequation(S3.52),weobtainthatwithprobabilityatleast1−O(p−(L+2)),|T1|≤C(cid:16)qlogpNkm0+logpNkm0(cid:17)ifν>1C(cid:16)qlogpNkm0+logplog2m0Nkm0(cid:17)ifν=1C(cid:16)qlogpNkm0+logpNkm2ν−10(cid:17)if3/4<ν<1C(cid:18)qlogplogm0Nkm0+logpNkm1/20(cid:19)ifν=3/4C(cid:16)qlogpNkm4ν−20+logpNkm2ν−10(cid:17)if1/2<ν<3/4.Intheend,theresult(ii)oftheﬁrstpartimmediatelyfollowsfromaunionboundargu-mentbypluggingtheboundsofT1andT2aboveintoequation(S3.51).WepointoutthattheupperboundofT1dominatesthatofT2.Therefore,wecompletetheproof.43S3.PROOFSOFSUPPORTINGLEMMASProofofLemma7Proof.TheproofofthislemmaisessentiallysimilartothatofLemma3.Recallthatkβkk1≤Cβfork=1,2.Therefore,byH¨older’sinequalitywehave(cid:13)(cid:13)(cid:13)ˆΣkβk−ˆµk(cid:13)(cid:13)(cid:13)∞≤(cid:13)(cid:13)(cid:13)ˆΣk−Σk(cid:13)(cid:13)(cid:13)∞kβkk1+kµk−ˆµkk∞≤(cid:13)(cid:13)(cid:13)ˆΣk−Σk(cid:13)(cid:13)(cid:13)∞Cβ+kµk−ˆµkk∞.Consequently,thefactthat(β1,β2)isfeasiblewithprobabilityatleast1−O(p−L)immedi-atelyfollowsfromourchoiceofλ2,N,thefactthatlogp≤c0Nandresults(i)-(ii)inthesecondpartofLemma6.Itisworthwhiletopointoutthataccordingtothefactlogp≤c0NandtheboundsprovidedinLemma6,(cid:13)(cid:13)(cid:13)ˆΣkβk−ˆµk(cid:13)(cid:13)(cid:13)∞isdominatedbythetermkµk−ˆµkk∞.ProofofLemma8Proof.TheproofofthislemmaissimilartothatofLemma5.Weonlyhighlightthemaindiﬀerencesbrieﬂybelow.Theﬁrstinequalityfollowsfromthedeﬁnitionsof˜ZiandZidirectly.Weshowthesecondinequalityholdsbelowwithprobabilityatleast1−O(p−(L+1))fortheﬁxedi.Withoutlossofgenerality,weassumeYi=1andMi=m0cm.FollowingthelinesintheproofofLemma5,westillcanshowthat1m0cm(cid:12)(cid:12)(cid:12)log(cid:16)ˆπ1π2ˆπ2π1(cid:17)(cid:12)(cid:12)(cid:12)≤Cz1maxk=1,2|πk−ˆπk|/m0withsomeconstantCz1.Todealwiththeterm|¯XTi(˜β−β)|,wenotethatwithprobabilityatleast1−O(p−L−1)|¯XTi(˜β−β)|≤k¯Xik∞k˜β−βk1≤(kµ1k∞+Uβ)k˜β−βk1≤C(1+Uβ)k˜β−βk1,(S3.57)44S3.PROOFSOFSUPPORTINGLEMMASwheretheﬁrstinequityisduetoCauchy-Schwarzinequality,thesecondonefollowsfromresult(i)intheﬁrstpartofLemma6withN1=1,andthelastoneisduetothefactthatkµ1k≤Cµ.Finally,weprovideanupperboundfor1Mi(cid:12)(cid:12)(cid:12)PMij=1XTij(˜∇−∇)Xij/2(cid:12)(cid:12)(cid:12).SetUij:=Xij−µ1.WestilldecomposeitaswedidintheproofofLemma5,1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1XTij(cid:16)˜∇−∇(cid:17)Xij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1UTij(cid:16)˜∇−∇(cid:17)Uij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)+(cid:12)(cid:12)(cid:12)µT1(cid:16)˜∇−∇(cid:17)µ1/2(cid:12)(cid:12)(cid:12)+1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1µT1(cid:16)˜∇−∇(cid:17)Uij(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).Thesecondtermstillcanbeboundedas|µT1(˜∇−∇)µ1/2|≤C2µk˜∇−∇k1/2bytheassumptionkµ1k≤Cµ.Toboundtheﬁrstterm(PMij=1µT1(˜∇−∇)Uij)/Mi,wenotethatwithprobabilityatleast1−O(p−L−1),1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1µT1(cid:16)˜∇−∇(cid:17)Uij(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤k1MiMiXj=1Uijk∞kµT1(cid:16)˜∇−∇(cid:17)k1≤CUβCµk˜∇−∇k1,whereweusedresult(i)intheﬁrstpartofLemma6withN1=1duringthelastinequalityabove.Toboundthethirdterm,byH¨older’sinequality,wehavewithprobabilityatleast45BIBLIOGRAPHY1−O(p−L−1),1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1UTij(cid:16)˜∇−∇(cid:17)Uij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)tr((cid:16)˜∇−∇(cid:17)MiXj=1UTijUij/Mi)/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤12(cid:13)(cid:13)(cid:13)˜∇−∇(cid:13)(cid:13)(cid:13)1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)MiXj=1UTijUij/Mi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∞≤C(cid:13)(cid:13)(cid:13)˜∇−∇(cid:13)(cid:13)(cid:13)1(cid:16)qlogpm0+logpm0(cid:17)ifν>1C(cid:13)(cid:13)(cid:13)˜∇−∇(cid:13)(cid:13)(cid:13)1(cid:16)qlogpm0+logplog2m0m0(cid:17)ifν=1C(cid:13)(cid:13)(cid:13)˜∇−∇(cid:13)(cid:13)(cid:13)1(cid:16)qlogpm0+logpm2ν−10(cid:17)if3/4<ν<1C(cid:13)(cid:13)(cid:13)˜∇−∇(cid:13)(cid:13)(cid:13)1(cid:18)qlogplogm0m0+logpm1/20(cid:19)ifν=3/4C(cid:13)(cid:13)(cid:13)˜∇−∇(cid:13)(cid:13)(cid:13)1(cid:16)qlogpm4ν−20+logpm2ν−10(cid:17)if1/2<ν<3/4,wherewehaveappliedtheboundof|T1|intheproofofLemma6withN1=1inthelastinequalityabove.Combiningtheupperboundsofthreetermsabove,weﬁnallyobtainthatwithprobability1−O(p−L−1),1Mi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)MiXj=1XTij(cid:16)˜∇−∇(cid:17)Xij/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)≤C(1+U∇)k˜∇−∇k1.Tocompleteourproof,wecombineallboundsfor1m0cm(cid:12)(cid:12)(cid:12)log(cid:16)ˆπ1π2ˆπ2π1(cid:17)(cid:12)(cid:12)(cid:12),|¯XTi(˜β−β)|and1Mi|PMij=1XTij(˜∇−∇)Xij/2|.BibliographyBach,F.etal.(2010).Self-concordantanalysisforlogisticregression.ElectronicJournalofStatistics,4:384–414.Beran,J.(2017).Statisticsforlong-memoryprocesses.Routledge.46BIBLIOGRAPHYChen,X.,Xu,M.,andWu,W.B.(2016).Regularizedestimationoflinearfunctionalsofprecisionmatricesforhigh-dimensionaltimeseries.IEEETrans.SignalProcessing,64(24):6459–6470.Davidson,K.R.andSzarek,S.J.(2001).Localoperatortheory,randommatricesandBanachspaces.HandbookoftheGeometryofBanachSpaces,1:317–366.Dinov,I.D.,Boscardin,J.W.,Mega,M.S.,Sowell,E.L.,andToga,A.W.(2005).Awavelet-basedstatisticalanalysisoffMRIdata.Neuroinformatics,3(4):319–342.Johnstone,I.(2001).Thresholdingforweightedχ2.StatisticaSinica,11:691–704.Laurent,B.andMassart,P.(2000).Adaptiveestimationofaquadraticfunctionalbymodelselection.TheAnnalsofStatistics,28(5):1302–1338.Posekany,A.,Felsenstein,K.,andSykacek,P.(2011).Biologicalassessmentofrobustnoisemodelsinmicroarraydataanalysis.Bioinformatics,27(6):807–814.Rudelson,M.andVershynin,R.(2013).Hanson-Wrightinequalityandsub-gaussiancon-centration.ElectronicCommunicationsinProbability,18(82):1–9.Vershynin,R.(2012).Introductiontothenon-asymptoticanalysisofrandommatrices.InCompressedSensing,TheoryandApplications,pages210–268.Wu,W.B.,Huang,Y.,andZheng,W.(2010).Covariancesestimationforlong-memoryprocesses.AdvancesinAppliedProbability,42(1):137–157.ZhaoRenDepartmentofStatistics,UniversityofPittsburgh,Pittsburgh,PA15260,USA47BIBLIOGRAPHYE-mail:zren@pitt.eduSungkyuJungDepartmentofStatistics,SeoulNationalUniversity,Gwanak-gu,Seoul08826,KoreaE-mail:sungkyu@snu.ac.krXingyeQiaoDepartmentofMathematicalSciences,BinghamtonUniversity,StateUniversityofNewYork,Binghamton,NY,13902USAE-mail:qiao@math.binghamton.edu48