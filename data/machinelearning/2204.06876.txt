Distributed Over-the-air Computing for Fast

Distributed Optimization: Beamforming Design and

1

Convergence Analysis

Zhenyi Lin, Yi Gong, and Kaibin Huang

Abstract

Distributed optimization concerns the optimization of a common function in a distributed network,

which ﬁnds a wide range of applications ranging from machine learning to vehicle platooning. Its

key operation is to aggregate all local state information (LSI) at devices to update their states. The

required extensive message exchange and many iterations cause a communication bottleneck when the

LSI is high dimensional or at high mobility. To overcome the bottleneck, we propose in this work the

framework of distributed over-the-air computing (AirComp) to realize a one-step aggregation for dis-

tributed optimization by exploiting simultaneous multicast beamforming of all devices and the property

of analog waveform superposition of a multi-access channel. Equivalently, the technique superimposes

multiple instances of conventional AirComp processes, giving rise to the challenge of jointly designing

multicast beamforming at devices to rein in errors due to interference and channel distortion. We consider

two design criteria. The ﬁrst one is to minimize the sum AirComp error (i.e., sum mean-squared error

(MSE)) with respect to the desired average-functional values. An efﬁcient solution approach is proposed

by transforming the non-convex beamforming problem into an equivalent concave-convex fractional

program and solving it by nesting convex programming into a bisection search. The second criterion,

called zero-forcing (ZF) multicast beamforming, is to force the received over-the-air aggregated signals

at devices to be equal to the desired functional values. In this case, the optimal beamforming admits

closed form. Both the MMSE and ZF beamforming exhibit a centroid structure resulting from averaging

columns of conventional MMSE/ZF precoding. Last, the convergence of a classic distributed optimization

algorithm is analyzed. The distributed AirComp is found to accelerate convergence by dramatically

reducing communication latency. Another key ﬁnding is that the ZF beamforming outperforms the

MMSE design as the latter is shown to cause bias in subgradient estimation.

Z. Lin and K. Huang are with the Dept. of Electrical and Electronic Engr. at The University of Hong Kong, Hong Kong.
Z. Lin is also with the Dept. of EEE at Southern University of Science and Technology, China. Y. Gong is with the same
department. Contact: K. Huang (Email: huangkb@eee.hku.hk), Y. Gong (Email: gongy@sustech.edu.cn).

2
2
0
2

r
p
A
4
1

]
T
I
.
s
c
[

1
v
6
7
8
6
0
.
4
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

I. INTRODUCTION

Distributed optimization concerns the optimization of a common function in a distributed

network comprising a cluster of edge devices connected by device-to-device (D2D) links [1].

The broad ﬁeld covers two popular areas: distributed machine learning aiming at leveraging

local data and computer resources at edge devices to train a global artiﬁcial-intelligence (AI)

model [2], and distributed consensus ﬁnding a wide range of applications in, for example,

swarms of drones or robots and vehicle platooning [3]. A fundamental operation in distributed

optimization, which is usually implemented using an iterative gradient-descent algorithm, is to

aggregate all local state information (LSI) and use the result to update the states of all devices in

each round. The required extensive message exchange and many rounds cause a communication

bottleneck when the LSI is high dimensional (e.g., distributed learning) or at high mobility (e.g.,

drones). To overcome the bottleneck, we propose in this work the framework of distributed

over-the-air computing (AirComp) to realize a one-step aggregation for distributed optimization

by exploiting simultaneous multicast beamforming by all devices and the property of analog

waveform superposition of a multi-access channel. To develop the framework, beamforming

designs to rein in channel distortion are presented and the convergence of distributed optimization

adopting distributed AirComp is studied.

Recent years have witnessed fast-growing interests in distributed machine learning as driven

by the distillation of enormous mobile data into artiﬁcial intelligence (AI) to power next-

generation applications ranging from industrial automation to smart cities and extended reality.

Relevant research mainly focuses on the efﬁcient deployment of federated learning, arguably

the most popular distributed learning framework, at the network edge, giving rise to the fast

growing area of federated edge learning (FEEL) [4]. FEEL gains popularity for its capabilities

of leveraging local data while helping to preserve their privacy and distributed computation

resources. Typically implemented in networks with a star topology, the FEEL algorithm iterates

between 1) aggregation at an edge server over local models, which are updated by edge de-

vices using local data and uploaded over wireless channels, to update a global model, and 2)

broadcast of the model to all devices for updating, until the global model converges. The main

challenge confronting efﬁcient FEEL is the communication bottleneck caused by uploading high-

dimensional local modes (or stochastic-gradients) from potentially many devices over a multi-

access channel. The required large number of rounds/iterations (e.g., tens to hundreds of rounds)

3

exacerbates the issue. Attempts to overcome the bottleneck have led to the proposal of different

techniques including radio resource management [5], [6], model quantization [7], [8], and device

scheduling [9], [10]. One particular class of techniques of our interest, known as over-the-air

FEEL, features the application of AirComp to realize over-the-air model aggregation in FEEL

[11]–[14]. The principle underpinning AirComp (as well as over-the-air FEEL) is to exploit

the waveform superposition property such that the signal received at the server approximates

a desired aggregation function of linear analog modulated data (e.g., local models/gradients)

simultaneously transmitted by devices [12], [15].

Most recently, researchers have studied distributed FEEL targeting a cluster of devices without

coordination by a sever and connected by D2D links [16]–[18]. The original techniques for

server-assisted FEEL can be adopted by arranging the devices to take turn or use orthogonal

channels to play the role of edge server [17], [19]. As the resultant sequential aggregation is

time consuming, attempts on realizing parallel operations have been made by selecting multiple

weakly coupled clusters of devices to perform simultaneous intra-cluster aggregation [16], [18].

Such approaches are ineffective for a single cluster of devices with tightly coupled links such as

a drone swarm or a vehicle platoon, and still require multiple time slots to complete aggregation

over all devices. Fundamentally, the drawback of the existing approaches is rooted in attempting

to orthogonalize multiple aggregation processes in distributed optimization. On the contrary, we

advocate fusing them into a single multi-aggregation process to enable a one-step distributed

aggregation over all devices. Thereby, the resultant design, termed distributed AirComp, supports

fast distributed FEEL and distributed optimization at large.

Consider the aggregation operation of distributed optimization among K devices. The goal

of designing distributed AirComp is to realize one-step updating of the states of all devices via

over-the-air aggregation over their LSI. To this end, all devices simultaneously multicast LSI to

their peers and at the same time receive aggregated LSI via full-duplex communication [20].

We propose provisioning devices with transmit antenna arrays to enable multicast beamforming

for reining in the sum AirComp error. On the other hand, the use of receive arrays can support

aggregation beamforming as studied in [15]. To simplify our design, we assume single receive

antenna at devices. The extension of the current design to include aggregation beamforming is

straightforward but make the design tedious without new insight. Usually targeting a single-

cell system, traditional multicast beamforming at the base station aims to efﬁciently deliver

information to multiple receivers under their quality-of-service requirements. Mathematically,

4

the beamforming design can be formulated as an optimization problem with the objective of

minimizing the required transmission resources at the base station (e.g., power and array size)

under the constraints of receive signal-to-noise ratios (SNRs) [21], [22]. Another popular “max-

min” formulation targets maximizing the lowest rate or receive SNR among the receivers under

a transmission power constraint [23], [24]. Consider the context of distributed AirComp. Deﬁne

the AirComp error as the deviation of an aggregated signal from the desired average-functional

value due to channel fading and noise. Then the multicast beamforming from the perspective

of an individual device aims to minimize the sum AirComp error over other devices under a

transmission power constraint. Solving such a problem is no more difﬁcult than the mentioned

traditional ones. However, the new challenge arises from K simultaneous over-the-air aggregation

processes coupling multicast beamforming at K devices. This necessitates the joint beamforming

design that should account for all D2D-channel states in the system. To be precise, the multicast

radiation patterns of K devices should be jointly optimized under individual power constraints

such that their over-the-air superposition leads to the minimization of sum AirComp error.

AirComp requires the aggregated signal arriving at a device to be scaled by a factor, called

alignment level, to balance approaching a desired functional value and noise ampliﬁcation [25].

The said problem is further complicated by the need of optimization over the alignment level.

In contrast, transmit beamforming for single-aggregation AirComp is simple as its main purpose

is to overcome the fading of an associated single-user channel, and hence is irrelevant to the

current problem [15], [26].

In this work, besides proposing the principle of distributed AirComp as described earlier, we

design distributed multicast beamforming to materialize the principle and further apply the design

to distributed optimization. The key contributions and ﬁndings are summarized as follows.

• MMSE beamforming for distributed AirComp: Consider the design criterion of minimiz-

ing the sum AirComp error deﬁned as the sum mean-squared error (MSE) over all de-

vices with respect to the desired average-functional values. We propose an efﬁcient ap-

proach for optimally solving the mentioned problem of distributed multicast beamform-

ing. Without compromising the optimality, the optimal alignment factor conditioned on

beamforming is derived in a closed form and substituted into the original problem. Even

though the resultant problem is still non-convex, it can be transformed into an equivalent

concave-convex fractional program. The transformation admits an efﬁcient solution method

that nests solving a convex problem into a bisection search. The results reveal that the

5

optimal multicast beamforming at a device is steered along the centroid of the column

vectors of a traditional MMSE precoder for the associated D2D multiuser channel to the

peers, thereby balancing their AirComp errors; at least one device transmits with full power

while some devices transmit with partial power.

• Zero-forcing (ZF) beamforming for distributed AirComp: Consider the design criterion of

forcing all receive signal power to approach a uniform level for the purpose of aggregation

without attempting to avoid potential noise ampliﬁcation as in the MMSE case. To mini-

mize the resultant sum AirComp error, the ZF multicast beamformers conditioned on the

alignment factor can be reduced into a single-device design with the solution derived in a

closed form. It is observed to have a similar centroid form as the MMSE counterpart but is

based on a traditional ZF precoder. Given the beamformers, the alignment factor is obtained

as the minimum of a derived expression over devices, which represents an attempt to cope

with the weakest set of D2D links limiting the performance of distributed AirComp.

• Convergence of distributed optimization: The preceding distributed AirComp framework is

applied to implement distributed optimization over D2D links, which is based on the widely-

used distributed dual averaging algorithm. The convergence analysis shows that AirComp

errors induce bias terms in the gap between the minimized loss function and its ground

truth. The key ﬁnding is that the MMSE and ZF designs for distributed multicast beam-

forming lead to biased and unbiased estimations of ground-truth stochastic subgradients at

devices, respectively. Consequently, at a low-to-medium receive SNR, the former results in a

converged test accuracy substantially lower than that in the ideal (noise-free) case while the

loss of the ZF counterpart is negligible. This is opposite to the fact that ZF is sub-optimal

in terms of minimizing the sum MMSE AirComp error. For both designs, the said loss is

shown to diminish as the transmit SNR grows by being inversely proportional to its square

root.

The remainder of this paper is organized as follows. An overview of distributed AirComp is

given in Section II. The MMSE and ZF designs of distributed multicast beamforming are pres-

ened in Sections III and IV, respectively. The application of distributed AirComp to distributed

optimization is studied in Section V. Experimental results are presented in Section VI, followed

by concluding remarks in Section VII.

6

Fig. 1. Distributed AirComp system featuring distributed multicast beamforming and full-duplex communication.

II. OVERVIEW OF DISTRIBUTED AIRCOMP

A. System Model

As illustrated in Fig. 1, the considered distributed system comprises K edge devices without

coordination by a server. Each device is equipped with Nt transmit antennas and one single

receive antenna to support multicast beamforming and full-duplex communication. The number
of transmit antennas is assumed sufﬁciently large, i.e., (Nt (cid:62) K −1), to provide enough degrees-

of-freedom (DoF) for each device to support simultaneous aggregations at (K − 1) peers. For

full-duplex communication, it is assumed that a transmitter is perfectly decoupled from a receiver

at the same device via passive and/or active self-inference cancellation [27] as illustrated in Fig. 1.

As the process of distributed optimization spans multiple rounds, time is divided into rounds

with a ﬁxed duration denoted as Tcmm; each round is further divided into D symbol slots.

Channels are assumed to be static in each round and varying over rounds. For simplicity,

TxRxTxRxTxRxTxRxTxRxpkskrk−hkk+TxRxscalingzkq(⋅)q−1(⋅)×Full-duplex Transceiver at Device kRadiation Pattern State variableUpdate stateState (zk,xk)7

each link is assumed frequency non-selective with the bandwidth represented by B, giving the
symbol duration Ts = 1

B . Global channel state information (CSI) is available at each device
via estimation exploiting channel reciprocity or efﬁcient feedback [15]. The required training

overhead is assumed negligible compared with high-dimensional LSI exchange. In each round,

each device, say device k, transmits a symbol vector, denoted as sk, that comprises D symbols

and spans the round duration of Tcmm = DTs.

B. Distributed AirComp Design

An iterative algorithm for distributed optimization (see Appendix A) comprises multiple

communication rounds. In each round, all devices broadcast their LSI simultaneously using

linear analog modulation and multicast beamforming, and at the same time receive the over-

the-air aggregated signals from other devices. All devices are assumed to be synchronized via
a common clock. Consider an arbitrary round, for each device, say device k, let pk ∈ CNt×1
denote the multicast beamformer under the power constraint, (cid:107)pk(cid:107)2 (cid:54) P0, with P0 representing

the maximum power. Then the over-the-air aggregated signal vector received by the device,

denoted as yk, is given as

yk =

K
(cid:88)

(cid:96)=1,(cid:96)(cid:54)=k

hH

(cid:96)kp(cid:96)s(cid:96) + ˜wk,

(1)

where s(cid:96) denotes the symbol row vector transmitted by device (cid:96), h(cid:96)k ∈ CNt×1 is the channel
vector for the link from device (cid:96) to k, and ˜wk ∼ CN (0, σ2I) is channel noise. Upon receiving

the aggregated signal, it is scaled as shown below to yield the desired aggregation form:

rk =

yk

(K − 1)

√

,

η

(2)

where η is the alignment factor that helps to reduce the AirComp error.

Consider round n and device k. The transmitted symbol vector is a linear analog modulated

local state variable (D × 1 vector), zk(n) (see Appendix A). Normalization of the variable is

necessary for meeting the transmission power constraint and avoiding a nonzero DC level, which

unnecessarily increases the power. For distributed optimization with a general objective function,

typically the statistics of the elements of zk(n) are different and also vary over rounds (see e.g.,

uniform for all devices, as M (n) = E

[28]). Deﬁne the round-dependent mean and variance of local state variables, which are assumed
d z(d)
parameters can be estimated ofﬂine and stored at devices [28]. Let q(·) denote the normalization

(cid:105)
k (n)
. These

and V 2(n) = Var

(cid:105)
k (n)

d z(d)

(cid:104) 1
D

(cid:104) 1
D

(cid:80)

(cid:80)

function such as sk(n) = q(zk(n)) = zk(n)−M (n)
k (n)] = 0 with s(d)
and E[ 1
D

d s(d)

(cid:80)

V (n)

k (n) being the dth element of sk(n). Note that E[sk(n)] (cid:54)= 0
due to the non-uniform distributions of the elements in sk(n). Then the received signal is de-

for all k. Consequently, Var

(cid:104) 1
D

(cid:80)

(cid:105)
k (n)

d s(d)

8

= 1

normalized as

rk = q−1

(cid:18)

yk

(K − 1)

(cid:19)

=

η

√

V

√

K
(cid:88)

η

(cid:96)=1,(cid:96)(cid:54)=k

hH

(cid:96)kp(cid:96)s(cid:96) + M + wk,

(3)

(K − 1)

where the channel noise wk ∼ CN

(cid:16)

0,

(cid:17)
V 2σ2
(K−1)2η I

.

C. Distributed Optimization and Performance Metric

In distributed optimization, there exist local objective functions associated with individual de-
vices. The function for device k is denoted as fk : RD → R. Following the common assumptions,

each function is assumed to be convex and hence sub-differentiable, but not necessarily smooth.

The global objective, denoted as f , is related to local objectives by f (x) = 1
K

k=1 fk(x) for
a common state of devices. Then the goal of distributed optimization is to ﬁnd the optimal

(cid:80)K

common state x(cid:63), which represents a consensus among devices, that minimizes the objective.

Mathematically,

x(cid:63) = arg min
x∈X

f (x) = arg min
x∈X

1
K

K
(cid:88)

k=1

fk(x),

(4)

where the state space X is a closed, D-dimensional convex set.

A classic algorithm, distributed dual averaging, for solving (4) is discussed in Appendix A.

In this algorithm, the local state of each device, say device (cid:96), contains a primal variable and a
dual variable, which are denoted as x(cid:96)(n) ∈ RD and z(cid:96)(n) ∈ RD for round n, respectively. Each

round, say round n, of the algorithm comprises a three-step procedure simultaneously executed

at each device, say device (cid:96): (1) calculate a stochastic subgradient ˜g(cid:96)(n) of its local loss function

at x(cid:96)(n); (2) aggregate the dual variables of all other devices and apply the result together with

˜g(cid:96)(n) to update the local dual variable as z(cid:96)(n + 1); (3) project z(cid:96)(n + 1) onto the feasible

set X to obtain the updated primal variable x(cid:96)(n + 1) [29]. Only Step (2) requires information

exchange among devices. We focus on its one-step implementation using distributed AirComp

in the preceding subsection. To this end, (27) for Step (2) at device (cid:96) is modiﬁed as

z(cid:96)(n + 1) = (1 − β)z(cid:96)(n) + βr(cid:96)(n) + ˜g(cid:96)(n)

(5)

where the received signal r(cid:96)(n) is given in (3) and β is a constant weight (see Appendix A).

Note that the ideal received signal, namely the ground truth, is

1
K−1

zk(n). Then the sum

AirComp error is suitably deﬁned as the sum mean-square-error (MSE) between the received

K
(cid:80)
k(cid:54)=(cid:96)

9

signal and its ground-truth:

MSE = E





K
(cid:88)

(cid:32)

D
(cid:88)

r(d)
(cid:96) −

(cid:96)=1

d=1

=

V 2D
(K −1)2 E

1
K − 1

K
(cid:88)

k(cid:54)=(cid:96)

z(d)
k

(cid:33)H (cid:32)

r(d)
(cid:96) −

1
K − 1

K
(cid:88)

k(cid:54)=(cid:96)

(cid:33)


z(d)
k





K
(cid:88)

(cid:32) K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

hH
k(cid:96)pk√
η

s(d)
k −

K
(cid:88)

k(cid:54)=(cid:96)

k +w(d)
s(d)

k

(cid:33)H (cid:32) K
(cid:88)

k(cid:54)=(cid:96)

hH
k(cid:96)pk√
η

s(d)
k −

K
(cid:88)

k(cid:54)=(cid:96)

(cid:33)


k +w(d)
s(d)

k

=

=

V 2D
(K − 1)2

V 2D
(K − 1)2

K
(cid:88)

(cid:34) K
(cid:88)

(cid:96)=1

K
(cid:88)

k=1

K
(cid:88)

k(cid:54)=(cid:96)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:96)(cid:54)=k

(cid:13)
(cid:13)
(cid:13)
(cid:13)

hH
k(cid:96)pk√
η

− 1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(cid:35)

+

σ2
η

hH
k(cid:96)pk√
η

− 1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+

KDV 2σ2
(K − 1)2

1
η

,

(6)

where the expectation is taken over the distribution of the transmitted symbol and the received
noise w(d)
k

and the round index (n) is omitted here for brevity.

III. MMSE DESIGN OF DISTRIBUTED MULTICAST BEAMFORMING

In this section, distributed multicast beamforming for distributed AirComp as well as the

alignment factor are jointly optimized under the criterion of minimum sum AirComp error. The

result is called minimum MSE (MMSE) multicast beamforming. An efﬁcient solution approach

is developed based on fractional programming.

A. Optimal Distributed Multicast Beamforming

Consider an arbitrary round with its index omitted for ease of notation. With the sum AirComp

error in (6), the mentioned optimization problem is formulated as follows

min
{pk},η

MSE(p1, · · · , pk, η)

s.t.

(cid:107)pk(cid:107)2 (cid:54) P0, ∀k,

η ≥ 0.

(P1)

Due to the coupling of {pk} and η, Problem (P1) is non-convex and hence directly solving it is

difﬁcult. To overcome the difﬁculty, an alternative approach is presented as follows.

First, given {pk}, Problem (P1) is reduced to the following conditional problem:

min
η≥0

V 2D
(K − 1)2

K
(cid:88)

K
(cid:88)

k=1

(cid:96)(cid:54)=k

(cid:13)
(cid:13)
(cid:13)
(cid:13)

hH
k(cid:96)pk√
η

− 1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

+

KDV 2σ2
(K − 1)2

1
η

.

The objective in (7) is found to be convex and then the optimal η(cid:63)

mmse is obtained as

η(cid:63)
mmse =

(cid:32)2Kσ2 + 2 (cid:80)K
(cid:80)K
(cid:80)K

(cid:80)K

(cid:96)=1
k(cid:54)=(cid:96)(hH

k(cid:54)=(cid:96) pH
k(cid:96)pk + pH

k hk(cid:96)hH
k hk(cid:96))

(cid:96)=1

k(cid:96)pk

(cid:33)2

.

Next, we proceed to optimize {pk}. Substituting η(cid:63)

mmse in (8) into Problem (P1) gives

min
{pk}

K
K − 1

−

1
(K − 1)2 ·

s.t.

(cid:107)pk(cid:107)2 (cid:54) P0,

∀k.

(cid:16)(cid:80)K

(cid:96)=1

(cid:80)K

4(Kσ2 + (cid:80)K

k(cid:96)pk + pH
k(cid:54)=(cid:96)(hH
(cid:80)K
k(cid:54)=(cid:96) hH

k hk(cid:96))
k(cid:96)pkpH

(cid:96)=1

k hk(cid:96))

(cid:17)2

10

(7)

(8)

(P1.1)

Problem (P1.1) remains non-convex. Nevertheless, the result in the following lemma can trans-

form this problem into a tractable equivalent form.

Lemma 1. Given (cid:80)K

(cid:96)=1

(cid:80)K

k(cid:54)=(cid:96)(hH

k(cid:96)pk+pH

k hk(cid:96)) (cid:62) 0, Problem (P1.1) is equivalent to the following

concave-convex fractional program:

(cid:16)(cid:80)K

(cid:96)=1

(cid:80)K

k(cid:54)=(cid:96)(hH

(cid:113)
2

(Kσ2 + (cid:80)K

(cid:96)=1

k(cid:96)pk + pH
(cid:80)K

k(cid:54)=(cid:96) hH

(cid:17)
k hk(cid:96))

k(cid:96)pkpH

k hk(cid:96))

max
{pk}

s.t.

(cid:107)pk(cid:107)2 (cid:54) P0 ∀k,

(P1.2)

K
(cid:88)

K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

Proof. See Appendix B.

(hH

k(cid:96)pk + pH

k hk(cid:96)) (cid:62) 0.

It can be straightforwardly proved by a simple variable substitution. When (cid:80)K
k hk(cid:96)) < 0, the optimal solution of (P1.1) can be written as ˜p(cid:63)
pH
k and p(cid:63)
solution of (P1.2). Since ˜p(cid:63)
(P1.1), considering the case in Lemma 1 where (cid:80)K

k(cid:54)=(cid:96)(hH
k(cid:96)pk+
k = −p(cid:63)
k is the optimal
k correspond to the same optimal value of the objective of
k hk(cid:96)) (cid:62) 0 is sufﬁcient for
k(cid:96)pk + pH
the purpose of solving Problem (P1.1). This allows a property of factional program to be applied.
To this end, deﬁne the upper contour set of a function f : Rn → R at c = f (x0) for some
x0 ∈ Rn as the set {x ∈ X : f (x) (cid:62) c}. One useful property of the concave-convex fractional

(cid:96)=1
k, where p(cid:63)

k(cid:54)=(cid:96)(hH

(cid:80)K

(cid:80)K

(cid:96)=1

program in Lemma 1 is its strict quasi-concavity [30]. Speciﬁcally, the upper contour sets of the

objective of Problem (P1.2), are convex. By introducing an auxiliary variable α(α > 0), which

corresponds to the aligned fraction, Problem (P1.2) can be written as the convex problem of

11

maximization over α as follows:

max
α,{pk},

α

s.t.

(cid:107)pk(cid:107)2 (cid:54) P0,
(cid:16)(cid:80)K

∀k,

(cid:80)K

k(cid:54)=(cid:96)(hH

(cid:96)=1

α (cid:54)

(cid:113)
2

(Kσ2 + (cid:80)K

(cid:96)=1

(cid:17)
k hk(cid:96))

k(cid:96)pk + pH
(cid:80)K

k(cid:54)=(cid:96) hH

k(cid:96)pkpH

k hk(cid:96))

(P1.3)

.

To solve the above problem requires consideration of a related problem of transmission power

minimization described as follows. Deﬁne the maximum power as pmax = maxk (cid:107)pk(cid:107)2

2. Then

given α, the mentioned problem is given as

min
{pk},pmax

pmax

(cid:118)
(cid:117)
(cid:117)
(cid:116)(Kσ2 +

s.t.

2α

K
(cid:88)

K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

(cid:107)pk(cid:107)2
2

(cid:54) pmax,

∀k.

k(cid:96)pkpH
hH

k hk(cid:96)) (cid:54)

(cid:32) K
(cid:88)

K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

(cid:33)

(hH

k(cid:96)pk + pH

k hk(cid:96))

,

(P1.4)

The solution of Problem (P1.4) is denoted as a function of α, p(cid:63)(α). Note that given the desired

aligned fraction α, Problem (P1.4) is feasible if and only if the solution for Problem (P1.4)
satisﬁes p(cid:63)(α) (cid:54) P0. Two useful lemmas for relating Problems (P1.3) and (P1.4) are given as

follows, which are proved in Appendices C and D, respectively.

Lemma 2. The minimum transmission power p(cid:63)(α) over devices is a monotonously increasing

function of α.

It follows from the result in Lemma 2 that the solution for Problem (P1.3) is the maximum

alignment level, denoted as α(cid:63), such that the corresponding minimum transmission power p(cid:63)(α(cid:63))

is no larger than P0. This suggests a solution method of Problem (P1.3) by a search for α(cid:63) over
the range of 0 < p(cid:63)(α) (cid:54) P0. This requires solving Problem (P1.4) so as to compute the function
p(cid:63)(α). To this end, the following result is useful.

Lemma 3. Given α, Problem (P1.4) is convex.

12

Algorithm 1 Optimal Algorithm for MMSE Distributed Multicast Beamforming

1: Inputs: K, σ, {hk(cid:96)}.

2: Initialization:

Select αu so that α = αu makes p(cid:63)(αu) deﬁned in (P1.4) larger than P0.
Select αl so that α = αl makes p(cid:63)(αl) < P0.

3: while |αu − αl| (cid:62) (cid:15) do

4:

5:

6:

7:

8:

9:

Let αm = (αu + αl)/2 and substitute α = αm into (P1.4).
Solve (P1.4) by CVX toolbox to obtain p(cid:63)(αm) and {p(cid:63)
if p(cid:63)(αm) > P0 then

k}.

αu = αm.

else

αl = αm.

10:

end if

11: end while

12: return {p(cid:63)

k}.

The convexity of Problem (P1.4) allows it to be solved efﬁciently using rich existing techniques

for convex programming, for example, the primal-dual method.

In summary, Problem (P1.3) for optimal transmit beamforming can be solved efﬁcient by

nesting a one-dimensional search over α and the solution of the convex Problem (P1.4). The

detailed algorithm is presented in Algorithm 1.

B. Centroid Beamforming

To shed light on the structure of the optimal MMSE design of distributed multicast beam-
former, we consider a special case when the number of antennas Nt = K−1. Let Hk ∈ CNt×(K−1)
be the matrix with its (cid:96)-th column being hH

k(cid:96) with (cid:96) (cid:54)= k. In the current case, Hk is an invertible
full-rank matrix. The convexity of Problem (P1.4) allows its KKT conditions to be a sufﬁcient

and necessary condition for its optimal solution. Based on the KKT conditions, Lemmas 4 and

5 are obtained as follows with proofs in Appendix E.

Lemma 4. At least one device performs full-power transmission, i.e. ∃k, (cid:107)pk(cid:107)2 = P0.

Lemma 5 (Centroid Beamforming). Deﬁne [H](cid:96) as the (cid:96)-th column of matrix H. When Nt =

K − 1, the beamforming directions of devices with partial-power transmission are given as

13

p(cid:63)
k
(cid:107)p(cid:63)
k(cid:107)

=

K
(cid:80)
(cid:96)(cid:54)=k
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:80)
(cid:96)(cid:54)=k

(cid:104)(cid:0)HH

k

(cid:104)

(HH

(cid:1)−1(cid:105)
(cid:96)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k )−1(cid:105)

(cid:96)

,

(9)

and those with full-power transmission are given as

p(cid:63)
k
(cid:107)p(cid:63)
k(cid:107)

=

K
(cid:80)
(cid:96)(cid:54)=k
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:80)
(cid:96)(cid:54)=k

(cid:104)(cid:0)HkHH

k + µkI(cid:1)−1 Hk

(cid:105)

(cid:104)

(HkHH

k + µkI)−1 Hk

(cid:105)

(cid:96)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:96)

,

(10)

where µk is the regularization term that diminishes as the SNR increases.

The result in (9) shows that the direction of the optimal beamforming of a device with

partial-power transmission points to the centroid of the column vectors of a ZF or a regularized

channel-inversion precoder. Such a design overcomes fading of multiuser channels to facilitate

simultaneous signal alignments at other devices while balancing their AirComp errors.

IV. ZERO-FORCING DESIGN OF DISTRIBUTED MULTICAST BEAMFORMING

In this section, we consider the ZF design criterion of forcing the received signals to approach

the desired ground-truth values without consideration of channel noise. Under this criterion, a

low-complexity design of distributed multicast beamforming is presented as follows. Conditioned

on the alignment factor η, the ZF criterion allows the joint beamforming design to be decoupled

into the following individual beamforming problems formulated for device k as

p(cid:63)

k = arg min
pk

(cid:13)
(cid:13)HH

k pk −

√

η1(K−1)

(cid:13)
2
(cid:13)

, ∀k,

(P2)

where the objective function is termed as misalignment error. Note that in this case, since
Nt (cid:62) K − 1, there is sufﬁcient DoF to align the channels, i.e., HH
√
satisﬁed. Therefore (cid:13)

η1(K−1), ∀k is always
k pk =
(cid:13)
2 = 0 always has non-trivial solutions. Then following
(cid:13)

η1(K−1)

(cid:13)HH

k pk −

√

lemma gives a solution with the smallest norm.

Proposition 1. When Nt (cid:62) K − 1, given the alignment factor η, the misalignment error of

device k is minimized by following ZF multicast beamforming:

pk =

√

ηHk

(cid:0)HH

k Hk

(cid:1)−1

1(K−1),

∀k,

(11)

14

where η is chosen to meet the power constraint.

Proof. See Appendix F.

The beamformer in (11) can be written in a centroid form similarly as its MMSE counterpart,

i.e., pk =

√

η

K
(cid:80)
(cid:96)(cid:54)=k

(cid:104)

Hk

(cid:0)HH

k Hk

(cid:1)−1(cid:105)
(cid:96)

. Speciﬁcally, this multicast beamforming is given by the

centroid of the column vectors of a traditional ZF precoder for the associated D2D multiuser

channel to the peers. However, it should be noted that compared with MMSE beamforming, this

ZF scheme can potentially amplify the noise and is therefore vulnerable to deep fading.

By substituting (11) in Proposition 1, the problem of alignment-error minimization over the

alignment factor η is

min
{η}

s.t.

KDV 2σ2
(K − 1)2η

,

η1H

(K−1)

(cid:0)HH

k Hk

(cid:1)−1

1(K−1) (cid:54) P0,

∀k.

(P2.1)

Since the objective of Problem (P2.1) is a monotonically decreasing function of η, the optimal

η(cid:63)
ZF is directly given as

η(cid:63)
ZF = min

k

P0
k Hk)−1 1(K−1)

(K−1) (HH
1H

.

By constructing a Rayleigh quotient as following, we have

η(cid:63)
ZF = min

k

(K−1)1(K−1)

P01H
(K−1) (HH

k Hk)−1 1(K−1)

(K − 1)1H

(cid:62) min
k

P0
(cid:16)

(K − 1)λmax

(HH

k Hk)−1(cid:17)

(12)

(13)

(a)
= min

k

P0
(K − 1)λ−1
min (HH

k Hk)

,

where (a) is due to that HH

k Hk is a hermitian matrix and λmin(.), λmax(.) correspond to the
minimum and maximum eigenvalue of given matrix separately. Using this upper bound on η(cid:63)
ZF

leads to an upper bound on the sum AirComp error in the current case:

MSEZF (cid:54)

KDV 2σ2

(K − 1)P0 mink λmin (HH

k Hk)

.

(14)

15

Note that the minimum eigenvalue λmin

(cid:1) is a monotonically increasing function of Nt
[26]. This demonstrates the diversity gain of increasing the transmit array size for reducing the

(cid:0)HH

k Hk

sum AirComp error. Next, comparing the sum AirComp errors of the MMSE and ZF designs,
it is worth mentioning that MSEmmse (cid:54) MSEZF.

V. APPLICATION TO DISTRIBUTED OPTIMIZATION

In this section, we consider the application of distributed AirComp designed in the preceding

sections to provide an efﬁcient air interface for implementing the distributed-optimization algo-

rithm in Appendix A in a D2D network. The effects of AirComp error on its convergence are

charaterized mathematically. For tractable analysis, several assumptions commonly made in the

literature (see e.g., [31]) are also adopted in this work.

Assumption 1 (Continuity). Each local objective function is L-Lipschitz continuous:

|fk(x) − fk(y)| ≤ L(cid:107)x − y(cid:107)

for x, y ∈ X

(15)

where L is a constant.

Assumption 2 (Local Gradient Estimation). The stochastic subgradient ˜gk(n) at the k-th device

is an unbiased estimate of the ground-truth local subgradient, gk(n) ∈ ∂fk (xk(n)), with bounded

second moment:

E[˜gk(n)] = gk(n)

and

E (cid:2)||˜gk(n)||2(cid:3) ≤ Ω2,

(16)

where Ω is a given constant.

A. Effects of AirComp Error

In this subsection, we investigate the effects of AirComp error on the subgradient estimation

and state updating, which are the key operations of distributed optimization. The variables in

the subsequent analysis follow those deﬁned in Appendix A.

First, consider round n, the receive signal vector at device k in (3) can be rewritten as

rk(n) =

1
K − 1

K
(cid:88)

(cid:96)=1,(cid:96)(cid:54)=k

z(cid:96)(n) + ∆k(n),

(17)

16

where the random variable ∆k(n) deﬁned below represents the distortion caused by wireless

propagation:

∆k(n) =

V (n)
K − 1

(cid:32)

K
(cid:88)

(cid:96)=1,(cid:96)(cid:54)=k

hH

k(cid:96)(n)pk(n)
(cid:112)η(n)

(cid:33)

− 1

s(cid:96)(n) + wk(n).

(18)

Note that the summation of (cid:107)∆k(n)(cid:107)2 over devices gives the sum AirComp error in (6). Then

based on (5) and (30), the state variable at device i is updated as

zk(n + 1) =

K
(cid:88)

(cid:96)=1

Wk(cid:96)z(cid:96)(n) + ˆgk(n),

(19)

where ˆgk(n) represents the channel distorted version of the stochastic subgradient, ˆgk(n), namely

ˆgk(n) = ˜gk(n) + β∆k(n).

Next, the effects of AirComp error are reﬂected by the deviation of the noisy subgradient,

ˆgk(n), from its ground truth. Consider ZF beamforming designed in Section IV. From its

deﬁnition, the channel distortion in (18), ∆k(n), reduces to channel noise: ∆k(n) = wk(n).

It follows from this fact and Assumption 1 that the noisy subgradient ˆgk(n) yields an unbiased

estimate of the ground truth:

E [ˆgk(n)] = E [˜gk(n) + β∆k(n)] = gk(n).

(20)

Consider MMSE beamforming designed in Section III. It follows from (18) and its deﬁnition,

the corresponding noisy subgradient, however, is a biased estimate of the ground truth:

E [ˆgk(n)] = E [˜gk(n) + β∆k(n)] = gk(n) +

βV (n)
K − 1

(cid:32)

K
(cid:88)

(cid:96)=1,(cid:96)(cid:54)=k

hH

k(cid:96)(n)pk(n)
(cid:112)η(n)

(cid:33)

− 1

E [s(cid:96)(n)] ,

(21)

where E [s(cid:96)(n)] (cid:54)= 0 as discussed in Section II. Even though the MMSE beamforming achieves

lower AirComp error as deﬁned in (6) than the ZF counterpart, the former’s biased subgradient

estimation leads to worse convergence performance as elaborated in the next subsection.

Last, a result useful for convergence analysis is obtained by bounding the deviation of the state

variable (i.e., dual variable), zk(n), from its average z(n) = 1
K

k zk(n), termed dual-variable
deviation. Note that it also serves as a measure of the deviation of individual noisy subgradients.

(cid:80)

Mathematically, the deviation is deﬁned as E (cid:107)z(n) − zk(n)(cid:107)∗ with (cid:107) · (cid:107)∗ = sup(cid:107)u=1(cid:107)(cid:104)., u(cid:105)
denoting the dual norm. To bound the dual-variable deviation, based on Assumption 2, the

second moment of the noisy sub-gradient, ˆgk(n), can be bounded as:

E (cid:2)(cid:107)ˆgk(n)(cid:107)2(cid:3) = E (cid:2)(cid:107)˜gk(n)(cid:107)2 + β2(cid:107)∆k(cid:107)2(cid:3) (cid:54) Ω2 +

β2MSE(n)
K

,

17

(22)

where the AirComp error MSE(n) deﬁned in (6) applies to both the cases of ZF and MMSE

beamforming. Using the bound, the desired result is obtained as follows.

Lemma 6. For distributed optimization using distributed AirComp (with either MMSE or ZF

beamforming), the expected dual-variable deviation can be bounded as:

E (cid:107)z(n) − zk(n)(cid:107)∗

(cid:54)

2ξ
β(1 − λ2)

√

K) + 3ξ,

log(N

(23)

(cid:113)

Ω2 + β2 maxn MSE(n)

K

where ξ =

matrix P.

Proof. See Appendix G.

and λ2 is the second largest eigenvalue of the edge-weight

√

One can observe from the above result that dual-variable deviation grows with the numbers

of rounds and users following O(log(N

K)). Nevertheless, it is shown in the next section

that with a properly chosen step size, the noisy primal variables, {ˆxk(N )}, can asymptotically

approach the optimal point as the number of rounds, N , increases.

B. Convergence Analysis

The convergence of the distributed-optimization algorithm (see Appendix A) as implemented

using distributed AirComp is analyzed as follows. To begin with, given N rounds, the conver-

gence is evaluated using the metric of expected suboptimality gap, E [f (ˆxk(N )) − f (x(cid:63))], where
ˆxk(N ) and x(cid:63) represent the noisy state at device k and the optimal point, respectively.

Theorem 1 (Convergence with Distributed AirComp). Given the optimal point x(cid:63), assume that

the proximal function satisfying (26) in Appendix A is bounded by some constant R as ψ (x(cid:63)) ≤
R2. Let the step size α(n) be chosen as α(n) = R

n . Then the expected suboptimality gap

1−λ2
√

√

4ξ
can be bounded for the ZF multicast beamforming as

E [f (ˆxk(N )) − f (x(cid:63))] (cid:54) 20R log(N
√

√

√

K)
1 − λ2

β

N

(cid:18)

Ω2 +

β2 maxn MSE(n)
K

(cid:19) 1

2

,

(24)

and for the MMSE design as

E [f (ˆxk(N ))−f (x(cid:63))] (cid:54) 20R log(N
√

√

β

N

1−λ2

√

(cid:18)

K)

18

Ω2+

β2 maxn MSE(n)
K

(cid:19) 1

2

+

(cid:107)x(cid:63)(cid:107)
N

(cid:114)

N
(cid:88)

n=1

MSE(n)
K

,

(25)

where λ2 is the second largest eigenvalue of the edge-weight matrix P.

Proof. See Appendix H.

In the above results, those terms comprising the AirComp error, MSE(n), reﬂect the ef-

fects of distributed AirComp. For a sanity check, substituting MSE(n) = 0 into the results

in Theorem 1 yields the existing result for the ideal case of noiseless channels [29]. Next,

comparing with (24), the last term at the right-hand size of (25) reveals slower convergence

due to the MMSE beamforming as opposed to the ZF counterpart due to the former’s bias in

subgradient estimation as shown in the preceding subsection. More critically, the said term

does not vanish as the number of rounds, N , grows, but instead converges to a constant,
(cid:107)x(cid:63)(cid:107) · E[(cid:112)MSE(n)/K]. As a result, in terms of expected suboptimality gap, ZF beamforming

can signiﬁcantly outperform the MMSE counterpart in the regime of low-to-meidum SNRs. Last,
the AirComp-error term, introduced by β2 maxn MSE(n)

, scales with the number of rounds, N , and

√

√

K

K)/

N ). However, it decays with the increasing transmit SNR

devices, K, following O(log(N
√

following O(1/

SNR). This leads to a narrowing performance gap between the ZF and MMSE

beamforming as the SNR grows.

VI. SIMULATION RESULTS

A distributed system with a varying number of devices, K, is simulated. In the system, all

channel gains are modelled as i.i.d. Rician fading with the power ratio between the direct and

scatter paths being 0.6 and unit total power. Transmit SNR is deﬁned as SNR = P0/σ2 and is set

equal for all devices. The bandwidth is B = 1 MHz. Other case-dependent simulation settings

are speciﬁed in the sequel. For distributed optimization, we consider the speciﬁc scenario of

distributed FEEL that trains a classiﬁer model for handwritten-digit recognition using the well-

known MNIST dataset. This dataset contains 60, 000 labeled training data samples in total. To

distribute these samples in a non i.i.d. manner, they are ﬁrst sorted by their digit label, then

divided into 20 shards of size 3, 000, with 2 shards allocated to each of the 10 devices. The

classiﬁer model is implemented using a 6-layer convolutional neural network (CNN) that consists

19

Fig. 2. Per-round communication latency with transmit SNR = 20 dB.

of two 5 × 5 convolution layers with ReLU activation, each followed by a 2 × 2 max pooling

layer, a fully-connected layer with 512 units and ReLU activation, and a ﬁnal softmax output

layer. The total number of parameters is 21, 840. The test accuracy is deﬁned as the lowest

test accuracy among all devices. In this scenario, the LSI of each device is a locally computed

stochastic gradient for model updating.

Two benchmarking schemes are described below

• Digital communication: Each coefﬁcient of a transmitted state variable is quantized into

Q = 16 bits, which are transmitted reliably at a capacity-achieving rate. In each round,

devices take turn to broadcast their LSI to peers based on time-division multiple access

(TDMA). ZF precoding is applied.

• Single-aggregation AirComp: In each round, the K AirComp processes in the system are

orthogonalized or equivalently completed sequentially using TDMA, giving the name of the

scheme. Consequently, the per-round latency is K-time higher than that of the proposed

distributed AirComp although the AirComp error is smaller as demonstrated in the sequel.

The traditional beamforming design for a multiple-input-single-output channel is applied

together with effective channel inversion for receive-signal alignment, which is a special

case of the AirComp design in [15].

5101520253035404550Number of devices10-210-1100Per-round communication latencyDigital communicationSingle-aggregation AirCompDistributed AirComp20

A. Performance of Distributed AirComp

In Fig. 2, the per-round communication latency for LSI exchange between K devices is

compared between the proposed distributed AirComp and benchmarking schemes for a varying

number of devices, K. The transmit SNR is 20 dB. To support the number of devices as many

as 50, each device is provisioned with a large-scale array with Nt = 100. One can observe

that the feature of simultaneous multicasting enables the proposed distributed AirComp to keep

the latency constant instead of increasing with the number of devices as for the benchmarking

schemes. Distributed AirComp is observed to achieve much lower latency than the latter with

the gap increasing rapidly as K grows. When there are many devices (e.g., 50), the latency

reduction of distributed AirComp is more than two-order of magnitude with respect to (w.r.t.)

digital communication and about 50-time w.r.t. single-aggregation AriComp.

Next, the curves of AirComp error versus transmit SNR, number of transmit antennas Nt, and

number of devices K are plotted in Fig. 3(a), 3(b), and 3(c), respectively. By default, the number

of devices K = 5. All simulation results demonstrate decreasing AirComp error as any of the

three parameters increases. Relatively small array sizes, Nt = 4 and Nt = 18, are considered in

Fig. 3(a) and and 3(c), respectively, to investigate the limitations of distributed AirComp. One

can observe from Fig. 3(a) that distributed AirComp, which strives to support K AirComp

processes simultaneously despite a small array size, incurs about 10-time larger AirComp error

than single-aggregation AirComp. Furthermore, as shown in Fig. 3(c), the former sees that

the AirComp error saturates as K increases, indicating the cancellation of the opposite effects

of aggregation gain in error suppression and severer receive-signal misalignment. The above

disadvantage of distributed AirComp as a price for dramatic latency reduction is alleviated when

the array size increases as shown in Fig. 3(b). For instance, for Nt = 40, the error ratio between

distributed AirComp and single-aggregation counterpart reduces to below 4 times. In addition, it

can be observed that for distributed AirComp, MMSE multicast beamforming outperforms the

ZF design in the regimes of low-to-medium SNRs or array size or as the number of devices

grows.

B. Performance of Distributed Optimization

Consider FEEL, a typical scenario of distributed optimization, where the number of devices

K = 10 and each is equipped with Nt = 18 antennas. The distributed FEEL algorithm is

implemented using either distributed AirComp or one of the benchmarking schemes. Deﬁne the

21

(a)

(b)

Fig. 3. Comparison of AirComp error between distributed AirComp with MMSE and ZF beamforming and single-aggregation
AirComp for (a) varying transmit SNR, (b) a varying number of transmit antennas Nt, and (c) a varying number of devices K.
The default value of K = 5 is used.

(c)

communication latency given a number of rounds as the accumulated latency from the start of

the task to the current round. The test accuracy of the considered schemes are compared in Fig. 4

as a function of communication latency. First, by comparing sub-ﬁgures in the same row, the

dramatic convergence-speed acceleration achieved by distributed AirComp is aligned with the

latency comparison in Fig. 2. Second, in terms of converged test accuracy, distributed AirComp

with ZF multicast beamforming performs similarly as the benchmarking schemes. Third, in the

context of distributed AirComp, MMSE beamforming design is observed from the left-most

subﬁgure of Fig. 4(a) to suffer signiﬁcant accuracy loss (i.e., 15%) w.r.t. the ZF counterpart

due to the bias in subgradient estimation as discussed in Section V. The loss is signiﬁcant in

20253035404550Transmit SNR per device (dB)10-710-610-510-410-310-210-1AirComp errorDistributed AirComp (MMSE)Distributed AirComp (ZF)Single-aggregation AirComp510152025303540Number of antennas10-310-210-1AirComp errorDistributed AirComp (MMSE)Distributed AirComp (ZF)Single-aggregation AirComp345678910Number of devices10-410-3AirComp errorDistributed AirComp (MMSE)Distributed AirComp (ZF)Single-aggregation AirComp22

(a) SNR = 10 dB

(b) SNR = 20 dB

Fig. 4. Test-accuracy comparison between (from left to right) distributed AirComp, single-aggregation AirComp, and digital
communication for varying communication latency for (a) a medium SNR (SNR = 10 dB) or (b) a high SNR (SNR = 20 dB).

the regime of low-to-medium SNRs but varnishes at high SNRs as observed from the left-most

subﬁgure of Fig. 4(b).

VII. CONCLUDING REMARKS

To overcome the communication bottleneck in the deployment of distributed optimization in

a distributed network, we have proposed the framework of distributed AirComp that realizes a

one-step distributed aggregation of the local states of all devices. The framework features the

seamless integration of simultaneous multicast beamforming of all devices and their full-duplex

communication, which makes it possible to support multiple concurrent over-the-air aggregation

processes at devices. This has led to dramatic communication-latency reduction when there are

many devices and thus provides a promising solution to data intensive applications such as

distributed machine learning and high-mobility applications, such as drone swarm or vehicle

platooning.

This work points to a number of directions warranting follow-up investigations. It is interesting

to extend the current single-stream transmission to distributed AirComp with spatial multiplex-

012345678910Communication latency0.20.30.40.50.60.70.80.9Test accuracyDistributed AirComp (MMSE, SNR=10dB)Distributed AirComp (ZF, SNR=10dB)Accuracy loss01000200030004000500060007000Communication latency0.20.30.40.50.60.70.80.9Test accuracyDigital communication (SNR=10dB)0102030405060708090100Communication latency0.20.30.40.50.60.70.80.9Test accuracySingle-aggregation AirComp (SNR=10dB)012345678910Communication latency0.20.30.40.50.60.70.80.9Test accuracyDistributed AirComp (MMSE, SNR=20dB)Distributed AirComp (ZF, SNR=20dB)050100150200250300350Communication latency0.20.30.40.50.60.70.80.9Test accuracyDigital communication (SNR=20dB)0102030405060708090100Communication latency0.20.30.40.50.60.70.80.9Test accuracySingle-aggregation AirComp (SNR=20dB)23

ing, which can further shorten communication latency. On the other hand, the performance of

distributed AirComp can be enhanced by distributed resource allocation such as adaptive power

control and broadband transmission. Last, particularization of the current design to the area of

distributed machine learning provides abundance of cross-disciplinary research opportunities, for

example, distributed reinforcement learning with distributed AirComp.

A. Distributed Optimization Algorithm

APPENDIX

Consider distributed optimization in a distributed network described by an undirected graph
denoted as G = (V, E), where V represents the set of K nodes (e.g., edge devices) and E

represents the set of edges. Each edge, say the one connecting the kth and (cid:96)th node, is assigned

a non-negative weight denoted as Pk(cid:96). Then the graph structure can be speciﬁed by the K × K

weight matrix P with (k, (cid:96))th element being Pk(cid:96). In particular, two nodes k (cid:54)= (cid:96) are connected
(i.e., (k, (cid:96)) ∈ E) if and only if Pk(cid:96) > 0 or otherwise Pk(cid:96) = 0. The matrix P is constrained to be
doubly stochastic, namely that (cid:80)K

(cid:96)=1 Pk(cid:96) = (cid:80)K

k=1 Pk(cid:96) = 1.

Given the distributed network, the problem of distributed optimization in (4) can be solved

using the classic iterative algorithm of distributed dual averaging described as follows [29], [31].

To begin with, let each device, say device k, maintain a two-variable state, (zk(n), xk(n)), each

being a D×1 row vector with real elements and xk(n) ∈ X , which is the support of the objective
function. Moreover, a key component of the algorithm is a proximal function ψ : RD → R that

is assumed to be 1-strongly convex with respect to some norm (cid:107) · (cid:107):

ψ(y) ≥ ψ(x) + (cid:104)∇ψ(x), y − x(cid:105) +

1
2

(cid:107)x − y(cid:107)2 ∀ x, y ∈ X .

(26)

One example of such a function is ψ(x) = 1

2(cid:107)x(cid:107)2. For device k, the state variable xk(n) ∈ X
represents the local estimate of the optimal solution for (4) at iteration n. Recall that ˜gk(n)

represents the stochastic subgradient of the local loss function as computed at device k using local

data. Then given non-increasing step size {α(n)}, all devices perform simultaneous updating of

their states [31]: for all k and the n iteration,

zk(n + 1) = (1 − β)zk(n) + β

(cid:88)

(cid:96)∈N (k)

Pk(cid:96)z(cid:96)(n) + ˜gk(n),

xk(n + 1) = Πψ

X (zk(n + 1), α(n)) ,

(27)

(28)

where the constant β ∈ (0, 1), N (k) represents the neighbourhood of node k on the graph G,
and Πψ

X the projection function deﬁned as

24

Πψ

X (zk(n + 1), α(n)) := argmin

x∈X

(cid:26)

(cid:104)zk(n + 1), x(cid:105) +

(cid:27)

ψ(x)

1
α(n)

(29)

with ψ(.) being the proximal function that satisﬁes (26). For ease of notation, deﬁne the weight

matrix W = (1 − β)I + βP with the (k, (cid:96))th element denoted as Wk(cid:96). Then (27) is rewritten as

zk(n + 1) =

K
(cid:88)

(cid:96)=1

Wk(cid:96)z(cid:96)(n) + ˜gk(n), .

(30)

Note that the updating in (30) is for node k to compute a consensus based on the states of the

peers. On the other hand, the projection function in (29) yields xk(n + 1) that minimizes an
averaged ﬁrst-order approximation of the objective function. It is assumed that ψ(x) (cid:62) 0 ∀ x ∈ X

and ψ(0) = 0 without loss of generality.

The above updating procedure is iterated until a consensus on the minimum of the objective

is reached, as the changes of the states of all devices fall below a given threshold. Speciﬁcally,

given a threshold ε, the convergence criterion can be speciﬁed using the expected suboptimality

gap [29]:

max
k

E [f (ˆxk(N ))−f (x(cid:63))] (cid:54) ε,

(31)

where ε is a given constant and the expectation in (31) taken over all sources of randomness.

B. Proof of Lemma 1

(cid:80)K

First, given (cid:80)K

k(cid:54)=(cid:96)(hH

k hk(cid:96)) (cid:62) 0, it is direct to see that the objective function of
k(cid:96)pk + pH
(cid:96)=1
P1.2 is positive. Denote xk(cid:96) = Re(hH
k(cid:96)pk), yk(cid:96) = Im(hH
k(cid:96)pk) as the real and imaginary parts of
hH
k(cid:96)pk, respectively. Then the numerator and denominator of objective function of Problem (P1.2)
can be written as A(x) = 2 (cid:80)K
k(cid:96))
respectively. Notice that the eigenvalues of the Hessian matrix of function B(x, y) : e1 =
k(cid:96)))1/2 are all positive numbers,
(Kσ2+(cid:80)K
hence the B(x, y) is a convex function. Then, since A(x) is a linear function, one can obtain

k(cid:96)))3/2 , e2 = e3 = · · · = e2K(K−1) =

k(cid:54)=(cid:96) xk(cid:96) and B(x, y) = 2

Kσ2 + (cid:80)K

2Kσ2
k(cid:54)=(cid:96)(x2

k(cid:96) + y2

2
k(cid:54)=(cid:96)(x2

k(cid:54)=(cid:96)(x2

(Kσ2+(cid:80)K

k(cid:96)+y2

k(cid:96)+y2

(cid:80)K

(cid:80)K

(cid:113)

(cid:96)=1

(cid:96)=1

that the the Problem (P1.2) is a concave-convex fractional program, with its objective function

proved to be strictly quasi-concave and pseudo-concave [30]. Therefore, the objective function

of Problem (P1.2) is a positive unimodal function and Problem (P1.2) has an unique optimal

25

solution, and the proof is completed.

C. Proof of Lemma2

It is ﬁrst to prove that for any α1 (cid:54) α2, p(cid:63)

k(α2) by contradiction. Assume there
exists α1 (cid:54) α2 such that p(cid:63)
k(α2). Since Problem P1.2 has been proved to be an strictly
quasi-concave function, then for α1 (cid:54) α2, the size of feasible region P1 for α = α1 is larger

k(α1) (cid:54) p(cid:63)

k(α1) > p(cid:63)

than or equal to the size of P2 for α = α2, which means we can at least ﬁnd a set of {pk}
in P1 that makes p(cid:63)
k(α1) (cid:54) p(cid:63)
α1 (cid:54) α2, p(cid:63)

k(α2). This contradicts the previous assumption and thus for any

k(α1) = p(cid:63)
k(α2).

Moreover, for α = 0, we have pk = 0, ∀k, and thus p(cid:63)

must ∃ pk such that (cid:107)pk(cid:107)2 > 0, which means p(cid:63)

k(α) = 0. Then, for any α > 0, there
k(α) cannot be a constant

k(α) > 0. To this end, p(cid:63)

function for all α. Then the proof is ﬁnished.

D. Proof of Lemma 3

First, the objective of Problem (P1.4) is linear and the second constraint is a convex set. Then

given any α, left-hand-side of the ﬁrst constraint can be regarded as a 2-norm function and hence

convex. Then the right-hand-side of the ﬁrst constraint is a linear function. Therefore, the proof

is completed.

E. Proof of Lemma 4 and Lemma 5

We ﬁrst give the corresponding Lagrange function of Problem (P1.4) as below

L({pk} , pmax, λ, {vk}) = pmax+



λ

2α

(cid:118)
(cid:117)
(cid:117)
(cid:116)(Kσ2 +

K
(cid:88)

K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

k(cid:96)pkpH
hH

k hk(cid:96)) −

(cid:32) K
(cid:88)

K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

(cid:33)

(hH

k(cid:96)pk + pH

k hk(cid:96))

 +

K
(cid:88)

k=1

vk(pH

k pk − pmax)

(32)






where λ and {vk} are the Lagrangian multipliers. The KKT conditions are given by

26

∂L({pk} , pmax, λ, {vk})
∂pmax

= 1 −

K
(cid:88)

k=1

vk = 0,

∂L({pk} , pmax, λ, {vk})
∂pk

= λ









(cid:32) K
(cid:80)
(cid:96)(cid:54)=k

α

(cid:33)

hk(cid:96)hH
k(cid:96)

pk

(cid:113)

Kσ2 + (cid:80)K

(cid:96)=1

(cid:80)K

k(cid:54)=(cid:96) (cid:107)hH

k(cid:96)pk(cid:107)2









−

K
(cid:88)

(cid:96)(cid:54)=k

hk(cid:96)

+ 2vkpk = 0, ∀k,



α

(cid:118)
(cid:117)
(cid:117)
(cid:116)Kσ2 +

K
(cid:88)

K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

λ

(cid:32) K
(cid:88)

(cid:107)hH

k(cid:96)pk(cid:107)2 −

K
(cid:88)

(cid:96)=1

k(cid:54)=(cid:96)

(cid:33)

(hH

k(cid:96)pk + pH

k hk(cid:96))

vk(pH

k pk − pmax) = 0, ∀k.

 = 0,

(33)

In (33), the ﬁrst condition indicates ∃k, vk (cid:54)= 0. This reveals that for any α, at least one device

transmits with power p(cid:63)(α). Then by Lemma 2, one can conclude that for the maximum aligned

fraction α, at least one device transmits with the maximum power P0.

Next, together with the second and the forth conditions, one can have λ (cid:54)= 0. By the second

condition, let a constant c0 =

(cid:113)

Kσ2+(cid:80)K

(cid:96)=1

(cid:2)c0λ (cid:0)HkHH

k

α
(cid:80)K

k(cid:54)=(cid:96)(cid:107)hH

k(cid:96)pk(cid:107)2 , we have
(cid:1) + 2vkI(cid:3) pk = Hk1(K−1).

(34)

By denoting µk = 2vk

c0λ , then the desired results are obtained.

F. Proof of Proposition 1

The problem of ﬁnding the smallest norm solution for (cid:13)

(cid:13)HH

k pk −

√

η1(K−1)

(cid:13)
2 = 0 is equivalent
(cid:13)

to the following optimization problem:

min
pk

(cid:107)pk(cid:107)2

s.t. HH

k pk =

√

η1(K−1).

This problem is convex since both the objective and feasible region are convex. By reusing

Lagrange multiplier λ, the Lagrange function can be written as

L(pk, λ) = (cid:107)pk(cid:107)2 + λ (cid:0)HH

k pk −

√

η1(K−1)

(cid:1) .

(35)

The KKT conditions are given by




∂L(pk, λ)
∂pk



(cid:0)HH

k pk −

= 2pk + HkλH = 0,

√

η1(K−1)

(cid:1) = 0.

27

(36)

Take the ﬁrst condition into the second one, one can get the expression of λ. Then take λ back

to the ﬁrst condition, the desired result is obtained.

G. Proof of Lemma 6

From (19), the average state in round n is given as

z(n + 1) =

1
K

K
(cid:88)

k=1

zk(n + 1) =

1
K

K
(cid:88)

(cid:32) K
(cid:88)

k=1

(cid:96)=1

Wk(cid:96)(n)z(cid:96)(n) + ˆgk(n)

,

(cid:33)

= z(n) +

1
K

K
(cid:88)

k=1

ˆgk(n).

(37)

Deﬁne the matrix Φ(n, s) = Wn−s+1. Then, for the state update zk(n + 1) at device k, it is

expanded as follows

zk(n + 1) =

K
(cid:88)

(cid:96)=1

[Φ(n, s)]k(cid:96)z(cid:96)(s) +

n
(cid:88)

(cid:32) K
(cid:88)

r=s+1

(cid:96)=1

[Φ(n, r)]k(cid:96) ˆg(cid:96)(r − 1)

+ ˆgk(n),

(38)

(cid:33)

where [Φ(n, s)]k(cid:96) is the (cid:96)-th entry of the k-th column of Φ(n, s). Since the initial state zk(0) = 0,

using (37) and (38) yields

¯z(n)−zk(n) =

n−1
(cid:88)

K
(cid:88)

s=1

(cid:96)=1

(cid:18) 1
K

−[Φ(n−1, s)]k(cid:96)

(cid:19)

ˆg(cid:96)(s−1) +

(cid:32)

1
K

K
(cid:88)

(cid:96)=1

(ˆg(cid:96)(n−1)− ˆgk(n−1))

. (39)

(cid:33)

Based on (22), ξ2 = Ω2 + β2 maxn MSE(n)
Then by Jensen’s inequality, one has (E [(cid:107)ˆgk(n)(cid:107)])2 (cid:54) E (cid:2)(cid:107)ˆgk(n)(cid:107)2(cid:3) (cid:54) ξ2 for all k. Hence

denotes an upper bound on the second moment of ˆgk(n).

K

E (cid:107)z(n) − zk(n)(cid:107)∗

(cid:54)

n−1
(cid:88)

s=1

(cid:13)
(cid:13)
[Φ(n − 1, s)]k −
(cid:13)
(cid:13)

ξ

1
K

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

+ 2ξ.

(40)

Following steps similar to [31], we separate the sum in (40) into two terms by a cutoff point ˆn =
n − log N

β log λ−1 , where λ2 = max {λ2(P), −λK(P)} is the second-largest magnitude of eigenvalues

K

√

of P,

E (cid:107)z(n) − zk(n)(cid:107)∗

(cid:54)

ˆn
(cid:88)

s=1

(cid:13)
(cid:13)
[Φ(n − 1, s)]k −
(cid:13)
(cid:13)

ξ

1
K

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

+

n−1
(cid:88)

s=ˆn+1

(cid:13)
(cid:13)
[Φ(n − 1, s)]k −
(cid:13)
(cid:13)

ξ

1
K

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

+ 2ξ. (41)

28

Then for s (cid:54) ˆn,we have (cid:107)[Φ(n − 1, s)]k − 1/K(cid:107)1
a more loose bound (cid:107)[Φ(n − 1, s)]k − 1/K(cid:107)1
with the fact log λ−1
2

(cid:62) 1 − λ2, one can obtain the desired result.

(cid:54) 1/K and for larger s, we relax this term via
(cid:54) 2. Taking these two bounds into (41) together

H. Proof of Theorem 1

The proof is close to the convergence proof for distributed dual averaging [31] with some

modiﬁcations. First, an useful lemma is introduced as follows.

Lemma 7.

[31, lemma 8] Let {xk(n)} and {zk(n)} be the primal and dual variables, then the

expected suboptimality gap for both ZF and MMSE beamforming cases is bounded as

E [fk (ˆxk(N )) − f (x(cid:63))] (cid:54)

1
N α(N )

ψ (x(cid:63)) +

ξ2
2N

N
(cid:88)

n=1

α(n − 1)

+

L + ξ
N K

N
(cid:88)

K
(cid:88)

n=1

k=1

α(n)E [(cid:107)¯z(n) − zk(n)(cid:107)] +

α(n)E [(cid:107)¯z(n) − zk(n)(cid:107)]

(42)

L
N

N
(cid:88)

n=1
(cid:35)

+ E

(cid:34)

1
N K

N
(cid:88)

K
(cid:88)

n=1

k=1

(cid:104)gk(n) − ˆgk(n), xk(n) − x(cid:63)(cid:105) ,

(cid:113)

where ξ =

Ω2 + β2 maxn MSE(n)

K

, and (cid:107) · (cid:107) represents the (cid:96)2-norm that is its own dual.

For the ZF beamforming case, by (20), one can ﬁnd that the last term of (42) is equal

to zero. By taking the upper bound in Lemma 6 into (42) and with ψ (x(cid:63)) ≤ R2, α(n) =

√

R

1−λ2
√

4ξ

n , the convergence for ZF beamforming in Theorem 1 can be obtained. However, for the
MMSE beamforming case, by (21), the last term of (42) is nonzero due to the biased gradient.

Nevertheless, this term can be bounded as

(cid:34)

E

1
N K

N
(cid:88)

K
(cid:88)

n=1

k=1

(cid:35)
(cid:104)gk(n) − ˆgk(n), xk(n) − x(cid:63)(cid:105)

(cid:54) E

(cid:34)

1
N K

N
(cid:88)

K
(cid:88)

n=1

k=1

29

(cid:35)

(cid:107)gk(n) − ˆgk(n)(cid:107) (cid:107)xk(n) − x(cid:63)(cid:107)

(cid:54) 1
N K

N
(cid:88)

K
(cid:88)

n=1

k=1

E [(cid:107)∆k(n)(cid:107)] E [(cid:107)xk(n) − x(cid:63)(cid:107)]

(43)
The last inequality follows from E [(cid:107)xk(n) − x(cid:63)(cid:107)] (cid:54) E [(cid:107)xk(0) − x(cid:63)(cid:107)] and xk(0) = 0, ∀k. Taking

N
(cid:88)

(cid:112)

MSE(n)/K.

(cid:54) (cid:107)x(cid:63)(cid:107)
N

n=1

this into (42), then Theorem 1 is proved.

REFERENCES

[1] M. G. Rabbat and R. D. Nowak, “Quantized incremental algorithms for distributed optimization,” IEEE J. Sel. Areas

Commun., vol. 23, no. 4, pp. 798–808, 2005.

[2] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge intelligence: Paving the last mile of artiﬁcial intelligence

with edge computing,” Proc. IEEE, vol. 107, no. 8, pp. 1738–1762, 2019.

[3] K. B. Letaief, Y. Shi, J. Lu, and J. Lu, “Edge artiﬁcial intelligence for 6g: Vision, enabling technologies, and applications,”

IEEE J. Sel. Areas Commun., vol. 40, no. 1, pp. 5–36, 2021.

[4] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang, D. Niyato, and C. Miao, “Federated learning in

mobile edge networks: A comprehensive survey,” IEEE Commun. Surveys Tuts., vol. 22, no. 3, pp. 2031–2063, 2020.

[5] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A joint learning and communications framework for federated

learning over wireless networks,” IEEE Trans. Wireless Commun., vol. 20, no. 1, pp. 269–283, 2020.

[6] Q. Zeng, Y. Du, K. Huang, and K. K. Leung, “Energy-efﬁcient radio resource allocation for federated edge learning,” in

Proc. IEEE Int. Conf. Commun. Workshops (ICC WKSHPS), Dublin, Ireland, Jun. 7-11, 2020.

[7] G. Zhu, Y. Du, D. G¨und¨uz, and K. Huang, “One-bit over-the-air aggregation for communication-efﬁcient federated edge

learning: Design and convergence analysis,” IEEE Trans. Wireless Commun., vol. 20, no. 3, pp. 2120–2135, 2020.

[8] Y. Du, S. Yang, and K. Huang, “High-dimensional stochastic gradient quantization for communication-efﬁcient edge

learning,” IEEE Trans. Signal Process., vol. 68, pp. 2128–2142, 2020.

[9] H. H. Yang, Z. Liu, T. Q. Quek, and H. V. Poor, “Scheduling policies for federated learning in wireless networks,” IEEE

Trans. Commun., vol. 68, no. 1, pp. 317–333, 2019.

[10] J. Ren, Y. He, D. Wen, G. Yu, K. Huang, and D. Guo, “Scheduling for cellular federated edge learning with importance

and channel awareness,” IEEE Trans. Wireless Commun., vol. 19, no. 11, pp. 7690–7703, 2020.

[11] M. Chen, D. G¨und¨uz, K. Huang, W. Saad, M. Bennis, A. V. Feljan, and H. V. Poor, “Distributed learning in wireless

networks: Recent progress and future challenges,” IEEE J. Sel. Areas Commun., 2021.

[12] G. Zhu, J. Xu, K. Huang, and S. Cui, “Over-the-air computing for wireless data aggregation in massive iot,” IEEE Wireless

Commun., vol. 28, no. 4, pp. 57–65, 2021.

[13] M. M. Amiri and D. G¨und¨uz, “Machine learning at the wireless edge: Distributed stochastic gradient descent over-the-air,”

IEEE Trans. Signal Process., vol. 68, pp. 2155–2169, 2020.

30

[14] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-the-air computation,” IEEE Trans. Wireless Commun.,

vol. 19, no. 3, pp. 2022–2035, 2020.

[15] G. Zhu and K. Huang, “MIMO Over-the-Air Computation for High-Mobility Multimodal Sensing,” IEEE Internet Things

J., vol. 6, no. 4, pp. 6089–6103, 2019.

[16] H. Xing, O. Simeone, and S. Bi, “Federated learning over wireless device-to-device networks: Algorithms and convergence

analysis,” IEEE J. Sel. Areas Commun., vol. 39, no. 12, pp. 3723–3741, 2021.

[17] Y. Shi, Y. Zhou, and Y. Shi, “Over-the-air decentralized federated learning,” in Proc. IEEE Int. Symp. Inf. Theory (ISIT),

Melbourne, Australia, Jul. 12-20, 2021.

[18] E. Ozfatura, S. Rini, and D. G¨und¨uz, “Decentralized sgd with over-the-air computation,” in 2020 IEEE Global

Communications Conference (GLOBECOM), Taipei, Taiwan, Dec. 7-11, 2020.

[19] S. Savazzi, S. Kianoush, V. Rampa, and M. Bennis, “A joint decentralized federated learning and communications

framework for industrial networks,” in IEEE Int. Workshop Comput. Aided Model. Des. Commun. Links Netw. (CAMAD),

Pisa, Italy, Sept. 14-16, 2020.

[20] A. C. Cirik, Y. Rong, and Y. Hua, “Achievable rates of full-duplex mimo radios in fast fading channels with imperfect

channel estimation,” IEEE Trans. Signal Process., vol. 62, no. 15, pp. 3874–3886, 2014.

[21] N. D. Sidiropoulos, T. N. Davidson, and Z.-Q. Luo, “Transmit beamforming for physical-layer multicasting,” IEEE Trans.

Signal Process., vol. 54, no. 6, pp. 2239–2251, 2006.

[22] O. Mehanna, N. D. Sidiropoulos, and G. B. Giannakis, “Joint multicast beamforming and antenna selection,” IEEE Trans.

Signal Process., vol. 61, no. 10, pp. 2660–2674, 2013.

[23] Y. Sun and K. R. Liu, “Transmit diversity techniques for multicasting over wireless networks,” in IEEE Wirel. Commun.

Netw. Conf. (WCNC), Atlanta, GA, USA, March 21-25, 2004.

[24] W. Lee, H. Park, H.-B. Kong, J. S. Kwak, and I. Lee, “A new beamforming design for multicast systems,” IEEE Trans.

Veh. Technol., vol. 62, no. 8, pp. 4093–4097, 2012.

[25] X. Cao, G. Zhu, J. Xu, and K. Huang, “Optimized power control for over-the-air computation in fading channels,” IEEE

Trans. Wireless Commun., vol. 19, no. 11, pp. 7498–7513, 2020.

[26] D. Wen, G. Zhu, and K. Huang, “Reduced-dimension design of mimo over-the-air computing for data aggregation in

clustered iot networks,” IEEE Trans. Wireless Commun., vol. 18, no. 11, pp. 5255–5268, 2019.

[27] Z. Zhang, X. Chai, K. Long, A. V. Vasilakos, and L. Hanzo, “Full duplex techniques for 5g networks: self-interference

cancellation, protocol design, and relay selection,” IEEE Commun. Mag., vol. 53, no. 5, pp. 128–137, 2015.

[28] N. Zhang and M. Tao, “Gradient statistics aware power control for over-the-air federated learning,” IEEE Trans. Wireless

Commun., vol. 20, no. 8, pp. 5115–5128, 2021.

[29] J. Duchi, A. Agarwal, and M. Wainwright, “Dual Averaging for Distributed Optimization: Convergence Analysis and

Network Scaling,” IEEE Trans. Automat. Contr., vol. 57, no. 3, pp. 592–606, Mar. 2012.

[30] S. Schaible, “Fractional programming. I, duality,” Manag. Sci., vol. 22, no. 8, pp. 858–867, 1976.

[31] R. Saha, S. Rini, M. Rao, and A. Goldsmith, “Decentralized optimization over noisy, rate-constrained networks: Achieving

consensus by communicating differences,” IEEE J. Sel. Areas Commun., vol. 40, no. 2, pp. 449-467, 2022.

