0
2
0
2

r
p
A
3
2

]
E
N
.
s
c
[

1
v
8
7
9
0
1
.
4
0
0
2
:
v
i
X
r
a

Constructing Complexity-eﬃcient Features in
XCS with Tree-based Rule Conditions

Trung B. Nguyen1[0000−0002−1990−8647], Will N. Browne1[0000−0001−8979−2224],
and Mengjie Zhang1[0000−0003−4463−9538]

Victoria University of Wellington, Wellington 6140, NZ
{trung.nguyen,will.browne,mengjie.zhang}@ecs.vuw.ac.nz

Abstract. A major goal of machine learning is to create techniques
that abstract away irrelevant information. The generalisation property
of standard Learning Classiﬁer System (LCS) removes such information
at the feature level but not at the feature interaction level. Code Frag-
ments (CFs), a form of tree-based programs, introduced feature manip-
ulation to discover important interactions, but they often contain irrele-
vant information, which causes structural ineﬃciency. XOF is a recently
introduced LCS that uses CFs to encode building blocks of knowledge
about feature interaction. This paper aims to optimise the structural eﬃ-
ciency of CFs in XOF. We propose two measures to improve constructing
CFs to achieve this goal. Firstly, a new CF-ﬁtness update estimates the
applicability of CFs that also considers the structural complexity. The
second measure we can use is a niche-based method of generating CFs.
These approaches were tested on Even-parity and Hierarchical problems,
which require highly complex combinations of input features to capture
the data patterns. The results show that the proposed methods signif-
icantly increase the structural eﬃciency of CFs, which is estimated by
the rule “generality rate”. This results in faster learning performance
in the Hierarchical Majority-on problem. Furthermore, a user-set depth
limit for CF generation is not needed as the learning agent will not adopt
higher-level CFs once optimal CFs are constructed.

Keywords: LCS · XCS · Code Fragments · XOF

1

Introduction

A major goal of machine learning techniques is to abstract away irrelevant in-
formation. This improves the explainability of complex learned knowledge. A
popular representation for encoding knowledge that encourages the explainabil-
ity of learned knowledge is tree-based programs, such as Genetic Programming
trees [8]. However, the problem of bloat, i.e. meaningless or even harmful sub-
trees, in learned trees inhibits their explainability and hurts the performance of
the system [9]. It can be ineﬃcient computationally and disrupts rule discovery
by including poor building blocks of knowledge in recombination operators. In
case of continual learning [14] or layered learning [12], the accumulated knowl-
edge can suﬀer from exponentially increasing ineﬃciency in complex trees as the
learning system continues to deal with more and more complex problems.

 
 
 
 
 
 
2

T. B. Nguyen et al.

Learning Classiﬁer Systems (LCSs) are a set of evolutionary techniques that
enables layered learning [2] and explainability due to their transparent and niche-
based nature1 [3,15]. LCSs were originally a concept of cognitive systems that
was adapted to become a rule-based system for machine learning and robotics,
e.g. classiﬁcation, regression, and multi-step navigation problems. XCS is a pow-
erful Michigan-style LCS which complements the “divide and conquer” ability
inherent in LCSs with an accuracy-based ﬁtness measure [16,4]. This enables
XCS to divide a complex problem into subproblems with its niches to solve
eﬃciently. XCS is also a framework where any complex representation can be
integrated into its rules. Tree-based programs are the representation of interest
as they enable higher-level feature construction that could encode “abstract”
data patterns. Trees also encourage the explainability of evolved rules as their
structures can provide insights of the learned knowledge. A popularly used tree-
based programs in XCS is Code Fragments (CFs), which have been introduced
in XCS to improve its scalability [7].

XOF is one such system that grows high-level CFs based on a set of the most
applicable CFs that are included in an Observed List (OL) [11]. XOF can learn
hierarchical and large-scale problems by capturing the data patterns in CFs.
However, its constructed trees also contain bloat due to the panmictic crossover
of CFs in the OL. The general learning process of an XOF, when addressing a
hierarchical problem, is to generalise from small niches, i.e. some speciﬁc cases,
to larger niches by combining the building blocks from the small niches. Figure 1
illustrates the general relationship of the lower-level CFs in less generalised rules
and the higher-level CFs in more generalised rules that can replace all the more
speciﬁc rules. The higher-level CFs here is shown to combine the lower-level CFs
to create CFs that can describe a superset niche. This heuristic suggests that a
niching method for CFs can be beneﬁcial for the generalising process of XOF.

Niching is a unique advantage of XCS. Previous implementations of XOF
have not included any niching property for constructing CFs. This means that
all CFs in the OL and in the CF population are grouped together without any
discrimination among niches. If the information that a CF in the OL performs
the best in the current niche or another niche is available, the learning system
can avoid combing CFs from unrelated niches, which was the likely cause of the
in non-optimal trees with bloat.

This paper proposes two novel methods to combat bloat in CFs. Accordingly,

the objectives of this paper are as follows:

1. To develop a new CF-ﬁtness measure, which is to estimate the applicability
of CFs in creating high-ﬁtness classiﬁers, to emphasise the eﬃciency of CF
structures. We also apply criteria based on the structural eﬃciency of CFs
to select the most applicable CFs accordingly.

2. To introduce a niching method for the CFs in the OL.
3. To investigate the inﬂuence of the two implemented approaches on the struc-
tural eﬃciency of constructed CFs and the learning performances of XOF.

1 Niche here means the local subsets of the data space

Constructing Complexity-eﬃcient Tree-based Features in XCS

3

Fig. 1. An example of generalising from a smallest niche to the largest one in a
hierarchical problem (11-bit Even-parity problem) by XOF. × is the abbreviation
for XOR. ‘,’s separate CFs in a condition:action binary classiﬁer eﬀecting 1 or 0.
!
is N OT . The generalising process beneﬁts from growing from lowest-level CFs
(D0, D1, ..., D10) to more complex ones gradually, i.e. (D0 × D1), (!(D0 × D1) × D2),
..., (!(...!(!(!(D0 × D1) × D2) × D3) × ... × D10)).

The system will be tested on complex problems that require building hier-
archical features to capture the patterns of data. Benchmark problems include
Even-parity, Hierarchical Multiplexer, and Hierarchical Majority-on problems
[3] because these Boolean problems require accurate hierarchical combinations
of input attributes. These combinations must match with the data patterns of
these problems to carry the maximal discriminative information of the prob-
lems. As a result, constructing such combinations can reduce the search space of
rules in XOF. However, ﬁnding accurate complex combinations in CFs is chal-
lenging due to the large search space. Even-parity problems stress generalisation
ability, which is not applicable to standard XCS using the ternary alphabet in
rule conditions. In addition to posing the generalisation challenge, the Hierar-
chical Multiplexer domain is also epistatic and heterogeneous, while Hierarchical
Majority-on domain has overlapping niches.

2 Background

2.1 Learning Classiﬁer Systems

LCSs are a family of rule-based algorithms based on a concept of cognitive sys-
tem [5]. An LCS generally interacts with an environment representing a target
problem to evolve a population of rules using evolutionary techniques [15]. XCS
is a simpliﬁed reinforcement learning implementation of LCS that can be easily
adapted to machine learning problems, robotic tasks, etc. [4,16]. The rule condi-
tions of XCS enable it to “divide” a hard problem into subproblems, i.e. niches,
and “conquer” each niche more easily.

!(...!((!(!(D0xD1)xD2))xD3)x....xD10) : 1/0niche: matched all odd (not even) parity instances (half instance space)!(!(D0xD1)xD2), ..., D10 : 1/0niche: 1111...1 |  0011...1 | 0101...1 | 1001...1!(D0xD1), D2, ..., D10 : 1/0niche: 1111...1 |  0011...1D0, D1, D2, ..., D10 : 1/0niche: 111...1Niches4

T. B. Nguyen et al.

2.2 CF-based XCSs

Code Fragments (CFs) was originally introduced as binary tree-based programs
used in Boolean problems with a depth limit of 2 [7]. A CF tree is connected
graph with internal nodes corresponding to functions from a function set and
terminal/leaf nodes representing input attributes from the environment state or
reused learned CFs. The function set for Boolean domains usually has general
binary operators, such as {AN D, OR, N OT, N AN D, XOR}.

CFs can describe complex patterns of data which enable XCS to generalise its
rules and solve hierarchical problems using more compacted rule-sets. CFs can be
used to represent rule conditions or actions in XCS [6,7]. However, the power of
tree-based programs also comes with challenges, especially the large search space
in search of trees with high discriminative information for a problem. Existing
methods used transfer learning and layered learning to resolve this issue. Iqbal
et al. introduced XCSCFC that transfers learned CFs to the leaf nodes of new
CFs in rule conditions to scale up to 135-bit Multiplexer problem [7]. XCSCF2
extended the transferring capability of CFs to function nodes. This method used
the rule populations of solved problems as rule-set functions for function nodes
[1]. Based on reusing rule-set functions, XCSCF* solved the general Multiplexer
problem by decomposing the problem domain into subproblems and combining
the solutions of the subproblems [2]. These approaches require human guidance
for either the transferring process or the learning order.

2.3 XOF

Similar to XCSCFC, an XCS with CFs in rule conditions, XOF grows from
the initial data features to learn deeper tree-based features (CFs) containing
discriminative patterns for classiﬁcation problems [10,11]. XOF extended XCS
with the Online Feature-generation (OF) module (see Figure 2) and introduced
CF-ﬁtness as a parameter to estimate the applicability of CFs. The applicability
of a CF is deﬁned as its capability of producing high-ﬁtness classiﬁers using the
CF in the condition parts.

The OF constructs tree-based features by combining the most useful CFs
in the Observed List (OL) to grow higher-level useful CFs for rule conditions.
The ability of CFs to create more accurate and generalised classiﬁers deﬁnes
CF-ﬁtness (the applicability of CFs):

cf.f =

cl.f
number of CFs in cl

,

(1)

where cf.f is the CF-ﬁtness of CF cf , and cl.f is the ﬁtness of classiﬁer cl.
This applicability measurement evaluates a CF according to the accuracy and
generality per CF of the highest-ﬁtness classiﬁer containing the CF. Thus, this
CF-ﬁtness creates pressure in combining CFs in the conditions of high-ﬁtness
classiﬁers without caring about the rule complexity. The combination pushed by
this CF-ﬁtness may result in new classiﬁers without increased generality.

The evolutions of rules and CFs can mutually support each other. The rule
ﬁtness guides the evolution of CFs, while the evolution of CFs provides building
blocks for rule conditions that contain more discriminative information of the

Constructing Complexity-eﬃcient Tree-based Features in XCS

5

Fig. 2. The OF module in XOF. It returns a CF when being requested by general
processes of XCS. The returned CF can be an existing one selected by Roulette Wheel
selection or a newly generated CF. In both cases, the OF relies on the OL.

patterns of data. In the ﬁrst versions of XOFs, the learning system periodically
updates the OL every 500 iterations using the tournament selection based on CF-
ﬁtness. XOF-BF, the baseline algorithm in this paper, updates the CF-ﬁtness of
a CF according to its best classiﬁer. The best classiﬁer here refers to the highest-
ﬁtness classiﬁer containing the CF. Because XOF enables learning of high-level
trees, it also contains irrelevant information in the form of bloat.

3 Method

3.1 Generalising CF-ﬁtness using Rule-Fitness Rate

To improve the structural eﬃciency of CFs, we ﬁrst introduce the term “com-
plexity” of CFs, which estimates the structural complexity of CFs. Because CFs
here are binary trees (without considering negation, i.e. function N OT , as a
separated node and adding complexity [11]), the number of function nodes (in-
ternal nodes) is always 1 less than the number of leaf nodes. Thus, we deﬁne the
complexity of a CF as the number of leaf nodes, which are the amount of input
information involved in evaluating the CF. Accordingly, the complexity of a rule
is the accumulated complexity of all CFs in its condition.

According to [10], the CF-based XCS targets to generate highly applicable
tree-based features. Therefore, the structural eﬃciency of a CF is its capability to
construct high-ﬁtness rules with the least rule complexity. We use “ﬁtness rate”
of a classiﬁer, which is equivalent to the ﬁtness per unit of classiﬁer complexity:

cl.f rate =

cl.f
cl.complexity

,

(2)

Constructing rule conditionsOnline Feature-generationSet of in-use CFs and base CFsUpdate CF-fitnessObserved ListUpdateGenerate a CF- Terminal nodes: roulette wheel- Function: randomrandom() < p_newCF?terminal nodesRoulette wheel selectionCovering/Genetic OperationsReturned CFsRequest CFsCheck with existing CFsNew CFs?YesUpdate classifiersexistingCFsYesNogenerated CFNoall CFsselected CFrepeatedexisting CFnew   CFConstructing rule conditionsOnline Feature-generationSet of in-use CFs and base CFsUpdate CF-fitnessObserved ListUpdateGenerate a CF- Terminal nodes: roulette wheel- Function: randomrandom() < p_newCF?terminal nodesRoulette wheel selectionCovering/Genetic OperationsReturned CFsRequest CFsCheck with existing CFsNew CFs?YesUpdate classifiersexistingCFsYesNogenerated CFNoall CFsselected CFrepeatedexisting CFnew   CF6

T. B. Nguyen et al.

to estimate the CF-ﬁtness of all CFs within the classiﬁer. The CF-ﬁtness of a CF
based on the rule-ﬁtness rate of the classiﬁer (containing the CF) rewards higher
CF-ﬁtness on the CF that can construct accurate and more generalised rules
using the least input information. Thus, we call this CF-ﬁtness as Generalising
CF-ﬁtness (GCFF). At this point, this CF-ﬁtness has similar goals with the rule
ﬁtness of traditional XCS. The additional beneﬁt is that CF-based conditions
enable more complex patterns with the same input attributes compared with
XCS, which can result in larger niches.

Accordingly, the OL gathers CFs from the conditions of the classiﬁers with
the highest ﬁtness per complexity unit in the action set of XCS (see Section 3.3).
The updates of CF-ﬁtness also follow the Widrow-Hoﬀ learning rule [13] based
on the classiﬁer with the highest ﬁtness per complexity unit containing the CF:

cf.f += βcf ∗ (

max
cl|cf ∈cl.condition

cl.f
cl.complexity

− cf.f ),

(3)

where βcf is the learning rate of CF-ﬁtness [11]. This CF-ﬁtness represents the
highest ﬁtness per complexity unit of a rule among rules having the CF, there-
fore called rule-ﬁtness rate. In short, the OF module evaluates generated CFs
based on their eﬃciency of CFs in using binary functions to combine the input
attributes to produce accurate and generalised classiﬁers.

3.2 Niching for CFs

The part of the OF module that associates with the learning processes of XCS,
i.e. covering and genetic operations, is the CF generation by either selecting an
existing CF or constructing a new one. As this process relies on CF-ﬁtness, a
niching method for CFs needs to be implemented for at least the CF-ﬁtness. We
develop a niching method that calibrates the CF-ﬁtness of a CF based on the
performance of the CF on the current niche. This method is designed to cre-
ate boundaries between niches to prevent continual undesirable sharing of CFs.
While sharing knowledge among niches is generally beneﬁcial in many prob-
lems, undesirable transfers of CFs between niches can hold back the discovery
of optimal building blocks for each niche.

The niching method calibrates the CF-ﬁtness in three cases to estimate a
local CF-ﬁtness for the CF. First, if a CF has its best classiﬁer matched in the
current action set, this CF is known to perform the best in this niche. In this
case, the OF uses its CF-ﬁtness directly. The second case is when a CF never
appears in any classiﬁer in the current action set. The system obviously has
no data on its actual performance in this niche. This niching method estimates
the local CF-ﬁtness of this CF naively with a constant rate of 0.1 of its global
CF-ﬁtness. This value should be further investigated. The third case is in the
middle of the ﬁrst two cases when a CF does appear in at least one classiﬁer in
this local niche, but its best classiﬁer is not the best overall. The estimated local
CF-ﬁtness of this CF is as follows:

cf.flocal = cf.f ∗

cf.local best classif ier.f
cf.best classif ier.f

,

(4)

Constructing Complexity-eﬃcient Tree-based Features in XCS

7

where the cf.local best classif ier is the “best” classiﬁer containing the CF in the
current action set [A], and the cf.best classif ier is its global “best” classiﬁer in
the whole rule population [P ]. It is noted that the deﬁnition of classiﬁer being the
“best” for a CF varies according to the CF-ﬁtness. In XOF-BF, it is the highest-
ﬁtness classiﬁer containing the CF. Because we will test this niching method
with the implementation that stacks this method with generalising CF-ﬁtness,
the quality of classiﬁer is based on this new CF-ﬁtness. Speciﬁcally, classiﬁers
selected for Equation 4 are the ones with the highest rule-ﬁtness rate:

cf.local best classif ier =

argmax
cl∈[A]|cf ∈cl.condition

cl.f /cl.complexity,

cf.best classif ier =

argmax
cl∈[P ]|cf ∈cl.condition

cl.f /cl.complexity,

(5)

(6)

3.3 The Simpliﬁed OL Update

In this work, we simplify XOF’s processes and thereby eliminate a number of
hyper-parameters. Also, the remaining hyper-parameters can still control the
pace of the evolution of CFs, such as the learning rate for CF-ﬁtness βcf . Instead
of periodical updates, the system updates the OL in every exploiting iteration
with two processes. The ﬁrst process is to collect the CFs in the conditions of
the classiﬁers that best represent the action sets. For example, when using the
CF-ﬁtness in Section 3.1, the classiﬁers satisfying the following criteria will be
used to collect the CFs for the OL:

cl.f /cl.complexity (cid:62) 0.9 ∗ max
cl∈[A]

cl.f /cl.complexity,

(7)

where 0.9 represents the selectivity of the OL. This value is empirically chosen
among high values to compress the OL size. The second process is to remove CFs
in outdated classiﬁers in the current niche that do not satisfy Eq. 7. This step
could remove necessary building blocks of other niches in case of the problems
with overlapping niches. However, as we place the OL update before genetic
operations, the removed necessary CFs in other niches are always added back.
This method is to collect all necessary building blocks in all niches.

4 Experiments

4.1 Generality Rate to Estimate the Structural Eﬃciency of CFs

To compare the ability to generate complexity-eﬃcient CFs, we tracked and eval-
uated the structural eﬃciency of the CFs in the highest-ﬁtness classiﬁers. Also,
the evaluation should be the least niche-biased, which focuses more on a sub-
set of niches. Thus, the classiﬁers for collecting CFs for tracking the structural
eﬃciency are gathered from at most one classiﬁer per action set. These classi-
ﬁers also need to be accurate and experienced to avoid irrelevant estimation of
performance, such as the high structural eﬃciency of an inaccurate general rule
cl.experience ≥ θGA and cl.error ≤ (cid:15)0 [4].

This estimation of structural eﬃciency is still somewhat niche-biased because
any niche with no experienced and accurate classiﬁers has no contribution to

8

T. B. Nguyen et al.

the estimated structural eﬃciency. This case is common when the accuracy is
not 100%, but does not occur otherwise. Even after achieving 100% accuracy,
the estimation of the CF-structural eﬃciency can still be niche-biased if the
estimation is not weighted by niche size. However, precise measurement requires
that niche sizes that are prerequisite knowledge for a given problem. As we
try to be naive about the tested problems, the evaluation will approximate the
evolution of structural eﬃciency of the highest-ﬁtness classiﬁers by averaging
them among niches where experienced and accurate classiﬁers are available.

Having the representative classiﬁers to collect the most applicable CFs of
the tested problem, we need a method to estimate the structural eﬃciency of
these CFs. Since these CFs are from experienced and accurate classiﬁers, the
other aspect of eﬃciency is only the generality [11]. Therefore, the structural
eﬃciency should involve the generality and complexity. We track the “generality
rate” of these classiﬁers to evaluate the structural eﬃciency of a classiﬁer:

cl.generality =

cl.matches
cl.matches + cl.no matches

, thus

cl.generality rate =

cl.generality
cl.complexity

,

where cl.matches and cl.no matches respectively track the numbers of times
classiﬁer cl matches and does not match all instances since it was created,
and therefore the part cl.matches/(cl.matches + cl.no matches) provides the
generality of classiﬁer cl as it tracks the probability that classiﬁer cl matches
any instance. The generality rate of classiﬁers estimates their eﬃciency in using
cl.complexity complexity units (CF structures) to produce accurate classiﬁers
with the highest generality.

4.2 Experimental Design

In this section, we compare XOF with the two newly implemented features with
itself in the lack of one or both new features. There are two criteria for compar-
isons: the learning performance as well as the discovery of complexity-eﬃcient
CFs. All experiments were evaluated based on the average of 30 runs using 30
independent random seeds. In this paper, we abbreviate the existing CF-ﬁtness
focusing on shortening rule conditions as Shortening CF-ﬁtness (SCFF) and the
niching method for CFs as Niching CFs (NCF). Thus, besides the existing ver-
sion named XOF-BF [10], we will experiment with three new other approaches
abbreviated as XOF-SCFF, XOF-GCFF, and XOF-GCFF-NCF within this pa-
per. All of these new systems use the simpliﬁed OL update in Section 3.3.

We conﬁgured all parameters of all tested versions of XOF equally except for
the rule population size and stopping iteration. A general conﬁguration of XOF
[10] is used in these experiments: the learning rate for rule parameters β = 0.2
and the learning rate for CFs βcf = 0.001; the crossover rate is χ = 0.2; the
mutation rate is µ = 0.9; the experience thresholds for deletion is θdel = 20; the
initial ﬁtness of covered classiﬁers are Finit = 0.01; the probability of speciﬁcness
pspec = 0.25 with maximum rule-condition length set at twice the number of

Constructing Complexity-eﬃcient Tree-based Features in XCS

9

original input attributes; and the experience thresholds for subsumption θsub =
50. All three newly implemented versions of XOF have no limit on the OL size
and a redundantly high limit on the CF depth, i.e. the maximum depth is 20.

4.3 Results on 11-bit Even-parity Problem

The population sizes for all systems on this problem were equal at 8000 clas-
siﬁers. The learning graphs of tested approaches in this experiment were not
substantially diﬀerent except for the convergence phase, see Figure 3a. XOF-
SCFF and XOF-BF had a slight advantage in the early phase. In 30 runs of the
three new systems in this paper, there was always one or two runs that were
stuck at 50% accuracy. The reason for being stuck was that the evolution of
CFs creates an extra force to push the evolution of rules further to the local
optima. Also, this Even-parity problem already poses a high probability of local
optima for XCS as the probability of ﬁnding correct rules in XCS is very low.
All inaccurate rules have the same accuracy of 50%, including the simplest rules
and the rules with genotypes near the accurate ones. These stuck runs always
ended up with the domination of a few very short rules (with only one CF in
its conditions). The high rule numerosity and simple rule conditions caused the
CFs in these rules to achieve higher CF-ﬁtness and thereafter pushed these rules
to earn more numerosity through genetic operations.

(a) Accuracy

(b) Generality Rate

Fig. 3. Results on 11-bit Even-parity problem.

All three systems discovered CFs with signiﬁcantly more eﬃcient structures
than ones by XOF-BF. The system with the niching method for CFs evolved the
most optimal CFs (Figure 3b), although there were two stuck runs in most of
the tested experience (1,000,000 instances), which had very low generality rate.
The evolved CFs of this system reached near the optimal generality rate for this
problem. The optimal generality rate is equivalent to the generality rate of the
most generalised classiﬁer in Figure 1. This classiﬁer has the generality of 0.5
as it matches half of all instances. Also, its complexity is 11 for 11 leaf nodes.

02004006008001000instances (x1000)0.50.60.70.80.91.0accuracyXOF-BFXOF-SCFFXOF-GCFFXOF-GCFF-NCF02004006008001000instances (x1000)0.000.010.020.030.040.05generality rateXOF-BFXOF-SCFFXOF-GCFFXOF-GCFF-NCF10

T. B. Nguyen et al.

Hence, its generality rate is 0.5/11 = 0.04545, which is the optimal generality
rate here.

4.4 Results on Hierarchical Problems

We use 18-bit Hierarchical Multiplexer and 18-bit Hierarchical Majority-on prob-
lems to evaluate the generality rate of the tested approaches. These two problems
pose relatively large search spaces as their hierarchy adds complexity to the data
patterns. To capture these complex patterns, constructed CFs need to cover all
6 non-overlapped three-successive-bit chunks with 3-bit Even-parity problems.
An optimal CF that can cover a chunk, say (D3, D4, D5), has to use XOR in all
function nodes except for any arbitrary negation, such as (!((!D3) × D4)) × D5).
Such CFs can match half of all possibilities for the three bits and the other half
by its negated version, which is not counted as a distinct construction in XOF
because each CF has one corresponding negated version.

All systems in these two experiments had the same population size of 20,000.
The learning performances of all approaches are no substantially diﬀerent from
one another in the 18-bit Hierarchical Multiplexer problem, see Figure 4a. In the
18-bit Hierarchical Majority-on problem, two XOFs with the SCFF, XOF-SCFF
and XOF-BF, did not converge to 100% accuracy, while the two XOFs using
the GCFF, XOF-GCFF and XOF-GCFF-NCF, did (see Figure 5a). The niching
CFs also improves the learning performances of XOF in both experiments.

(a) Accuracy

(b) Generality Rate

Fig. 4. Results on 18-bit Hierarchical Multiplexer problem.

In these two experiments, XOF with niching CFs (XOF-GCFF-NCF) yielded
the most optimal CFs among the tested versions of XOF (Figures 4b and 5b).
The evolution of CFs in XOF-BF again was stuck, plus it contained the most
bloated CFs. Table 1 illustrates a few samples of CFs in the OLs of three tested
systems. We show CFs related to chunks without optimal CFs discovered to
show the diﬀerence of the inﬂuence by the two CF-ﬁtness. SCFF, the CF-ﬁtness
focusing on shortening rule conditions, rated the two non-optimal CFs (!((!D14)×

050010001500instances (x1000)0.50.60.70.80.91.0accuracyXOF-BFXOF-SCFFXOF-GCFXOF-GCF-NCF050010001500instances (x1000)0369generality rate (x103)XOF-BFXOF-SCFFXOF-GCFXOF-GCF-NCFConstructing Complexity-eﬃcient Tree-based Features in XCS

11

(a) Accuracy

(b) Generality Rate

Fig. 5. Results on 18-bit Hierarchical Majority-on problem. The beginning parts of
generality rates are omitted because over-general (inaccurate) rules that are temporar-
ily accurate and experienced can create unreliable estimation.

D13)) ∧ D12 and (!((!D14) × D13)) ∨ D12 much higher CF-ﬁtness than the lower-
level CFs !((!D14) × D13) and D12. The two latter CFs were the ones that can
be combined to construct optimal CFs for the 3-bit Even-parity problem (on
three bits (D12, D13, D14)). Although the patterns of the two former CFs did not
generalise more than the lower-level ones on the 3-bit Even-parity problem, these
higher-level CFs still had higher CF-ﬁtness because they can produce rules with
shorter conditions and the same patterns. This process even hindered discovering
the optimal CFs for this 3-bit Even-parity chunk because the higher-level CFs
have more probabilities to be selected in constructing CFs. Meanwhile, GCFF,
the generalising CF-ﬁtness, did not face this problem because such non-optimal
combinations do not achieve higher CF-ﬁtness than the lower-level CFs.

Table 1. A few samples of CFs with their CF-ﬁtness in the OLs of XOF-SCFF, XOF-
GCFF, and XOF-GCFF-NCF after 150,000 instances of learning the 18-bit Hierarchical
Multiplexer problem. These samples were CFs related to a chunk that optimal CFs have
not been constructed. To be as fair as possible, these samples were chosen from runs
with generality rates in the small range from 0.0075 to 0.0080.

XOF-SCFF

XOF-GCFF
(!((!D14) × D13)) ∧ D12 (0.157) (!D9) ∧ (!D11) (0.058)
(!((!D14) × D13)) ∨ D12 (0.157) (D9 ∧ D11) × (!D10) (0.056) (!D8) × D7 (0.061)
!((!D14) × D13) (0.121)
D12 (0.121)
n/a

D6 (0.071)
n/a
n/a

D10 (0.057)
D11 (0.054)
D9 (0.054)

XOF-GCFF-NCF
(!D8) × (!D7) (0.071)

5 Further Discussions

SCFF rewards higher CF-ﬁtness on complex patterns without adding any more
generalisation. This mechanism is likely to push the evolution of tree features to

0500100015002000instances (x1000)0.50.60.70.80.91.0accuracyXOF-BFXOF-SCFFXOF-GCFFXOF-GCFF-NCF0500100015002000instances (x1000)01234generality rate (x103)XOF-BFXOF-SCFFXOF-GCFFXOF-GCFF-NCF12

T. B. Nguyen et al.

early local optima. The new CF-ﬁtness, GCFF, only rewards higher CF-ﬁtness
on more complex patterns when these patterns can construct more generalised
rules. Thus, the depth growth in the system using generalising CF-ﬁtness will
be slower and more reliable. As a result, the system can avoid being trapped in
local optima with non-optimal CFs.

On the 11-bit Even-parity problem, the pure pressure on combining CFs
and shortening rule conditions of SCFF has slightly better learning performance
than the generalising pressure of GCFF. This can be explained as this CF-ﬁtness
estimation awards higher CF-ﬁtness on combined CFs, which pushes the gener-
alisation process faster. However, GCFF has a better performance on the Hier-
archical Majority-on problem because it does not push the system towards rules
with shorter conditions, which can easily become over-general rules in problems
with overlapping niches.

The niching method for CFs improves the structural eﬃciency of CFs in XOF
signiﬁcantly, as shown by its superior average generality rate. Niching CFs guides
combining optimal CFs to generalise existing patterns. Therefore, the evolution
of CFs with niching CFs is accelerated without adding the likelihood of being
trapped in local optima. This also results in slightly faster learning performances
on the tested Hierarchical problems.

6 Conclusions

We have developed a new CF-ﬁtness, called generalising CF-ﬁtness, that focuses
on more generalised patterns and avoid naively combining existing CFs. Ac-
cordingly, other processes of XOF have been adjusted to select CFs following
the new criteria. The new CF-ﬁtness slows down the growth of CF depth but
adds more reliability to the CF construction. Although the structural eﬃciency
of generated CFs has not been improved, it enables integrating a newly devel-
oped niching method for CFs, which results in accelerating the evolution of CFs
without being trapped in local optima.

The niching method for CFs introduces the niching property to CF con-
struction in XCS with the OF module, i.e. XOF. The niching property enables
appropriate combinations of CFs to grow optimally complex CFs for hierarchical
problems. This property accelerates the generalisation of XOF rules. With this
new feature, XOF, as an extension of XCS, has the niching property in both the
evolutions of rules and CFs.

Future research will consider extending XOF with the new features to a
multi-agent system to target multitask learning. The availability of the OL as
the representative patterns can facilitate the automation of transferring CFs
among systems.

References

1. Alvarez, I.M., Browne, W.N., Zhang, M.: Reusing learned functionality in xcs: code
fragments with constructed functionality and constructed features. In: Proceedings
of the Companion Publication of the 2014 Annual Conference on Genetic and
Evolutionary Computation. pp. 969–976 (2014)

Constructing Complexity-eﬃcient Tree-based Features in XCS

13

2. Alvarez, I.M., Browne, W.N., Zhang, M.: Human-inspired scaling in learning clas-
siﬁer systems: Case study on the n-bit multiplexer problem set. In: Proceedings of
the Genetic and Evolutionary Computation Conference 2016. pp. 429–436 (2016)

3. Butz, M.V.: Rule-based evolutionary online learning systems. Springer (2006)
4. Butz, M.V., Wilson, S.W.: An Algorithmic Description of XCS. In: Advances in
Learning Classiﬁer Systems. pp. 253–272. Springer, Berlin, Heidelberg (Sep 2000)
5. Holland, J.H.: Adaptation in natural and artiﬁcial systems: An introductory anal-
ysis with applications to biology, control, and artiﬁcial intelligence. Adaptation in
natural and artiﬁcial systems: An introductory analysis with applications to bi-
ology, control, and artiﬁcial intelligence, The University of Michigan Press, Ann
Arbor, Oxford, England (1975)

6. Iqbal, M., Browne, W.N., Zhang, M.: Evolving optimum populations with XCS

classiﬁer systems. Soft Computing 17(3), 503–518 (Mar 2013)

7. Iqbal, M., Browne, W.N., Zhang, M.: Reusing building blocks of extracted knowl-
edge to solve complex, large-scale boolean problems. IEEE Transactions on Evo-
lutionary Computation 18(4), 465–480 (2013)

8. Koza, J.R.: Genetic Programming: On the Programming of Computers by Means

of Natural Selection. MIT Press, Cambridge, MA, USA (1992)

9. Luke, S., Panait, L.: A comparison of bloat control methods for genetic program-

ming. Evolutionary Computation 14(3), 309–344 (2006)

10. Nguyen, T.B., Browne, W.N., Zhang, M.: Improvement of code fragment ﬁtness to
guide feature construction in xcs. In: Proceedings of the Genetic and Evolutionary
Computation Conference. pp. 428–436. ACM (2019)

11. Nguyen, T.B., Browne, W.N., Zhang, M.: Online feature-generation of code frag-
ments for xcs to guide feature construction. In: 2019 IEEE Congress on Evolution-
ary Computation (CEC). pp. 3308–3315. IEEE (2019)

12. Stone, P., Veloso, M.: Layered Learning. In: Lpez de Mntaras, R., Plaza, E. (eds.)
Machine Learning: ECML 2000. pp. 369–381. Springer Berlin Heidelberg, Berlin,
Heidelberg (2000)

13. Sutton, R.S.: Learning to predict by the methods of temporal diﬀerences. Machine

learning 3(1), 9–44 (1988)

14. Thrun, S., Pratt, L.: Learning to learn. Springer Science & Business Media (2012)
15. Urbanowicz, R.J., Browne, W.N.: Introduction to Learning Classiﬁer Systems.
SpringerBriefs in Intelligent Systems, Springer-Verlag, Berlin Heidelberg (2017)
16. Wilson, S.W.: Classiﬁer Fitness Based on Accuracy. Evolutionary Computation

3(2), 149–175 (Jun 1995)

