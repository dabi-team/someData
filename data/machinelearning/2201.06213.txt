An Improved Reinforcement Learning Algorithm for Learning to Branch

Qingyu Qu1,2 Xijun Li3,2∗ Yunfan Zhou2 Jia Zeng2 Mingxuan Yuan2 Jie Wang3 Jinhu Lv1 Kexin
Liu1 and Kun Mao4
1Beihang University
2Huawei Noah’s Ark Lab
3MIRA Lab, USTC
4Huawei Cloud Co.
{quqingyu, skxliu}@buaa.edu.cn, {xijun.li, zhouyunfan, zeng.jia, Yuan.Mingxuan,
maokun}@huawei.com, jhlu@iss.ac.cn, jiewangx@ustc.edu.cn

2
2
0
2

n
a
J

7
1

]

G
L
.
s
c
[

1
v
3
1
2
6
0
.
1
0
2
2
:
v
i
X
r
a

Abstract

Most combinatorial optimization problems can be
formulated as mixed integer linear programming
(MILP), in which branch-and-bound (B&B) is a
general and widely used method. Recently, learn-
ing to branch has become a hot research topic in
the intersection of machine learning and combi-
natorial optimization.
In this paper, we propose
a novel reinforcement learning-based B&B algo-
rithm. Similar to ofﬂine reinforcement learning, we
initially train on the demonstration data to acceler-
ate learning massively. With the improvement of
the training effect, the agent starts to interact with
the environment with its learned policy gradually.
It is critical to improve the performance of the al-
gorithm by determining the mixing ratio between
demonstration and self-generated data. Thus, we
propose a prioritized storage mechanism to control
this ratio automatically. In order to improve the ro-
bustness of the training process, a superior network
is additionally introduced based on Double DQN,
which always serves as a Q-network with compet-
itive performance. We evaluate the performance of
the proposed algorithm over three public research
benchmarks and compare it against strong base-
lines, including three classical heuristics and one
state-of-the-art imitation learning-based branching
algorithm. The results show that the proposed algo-
rithm achieves the best performance among com-
pared algorithms and possesses the potential to im-
prove B&B algorithm performance continuously.

1 Introduction
The mixed integer linear programming (MILP) is one of the
most widely-used mathematical formulations in practical sce-
narios of optimization, such as planning and scheduling, bin
packing, resource allocation, etc. Generally, the vast majority
of MILP problems are NP-hard, which makes it inevitable to
compromise in terms of solving speed, precision, generaliza-
tion, etc. when using traditional methods to solve them. The

∗Xijun Li is the corresponding author.

NP-hardness is mainly due to the scale, such as the number
of integer variable. Therefore, some scholars pay attention to
improving the solutions of MILP by adopting some machine
learning algorithms such as the imitation learning, reinforce-
ment learning, etc. [Huang et al., 2021]. The branch-and-
bound (B&B) algorithm, together with the various skills for
increasing its efﬁciency, constitute one of the cores of MILP.
B&B algorithm enumerates the candidate solutions systemat-
ically by means of state space search, in which the set of can-
didates solutions is considered to form a search tree with the
full set at the root. The efﬁciency of B&B algorithm mainly
depends on branching variable selection and node selection.
In this paper, we concentrate on the former. Usually, choos-
ing good variables to branch on can lead to a dramatic re-
duction in terms of the number of nodes needed to solve an
instance [Alvarez et al., 2014a].

At present, there is still no commonly accepted method
for the strategy of branching variable selection. Traditional
methods are mostly simple heuristic rules, such as the Most
Infeasible branching, Pseudo-Cost branching (PC), Strong
Branching (SB), Hybrid Strong/Pseudocost Branching, Pseu-
docost branching with strong branching initialization, Relia-
bility Branching (RB), etc. [Achterberg et al., 2005], in which
PC and SB approaches are the two most common heuristic
rules. PC is a sophisticated rule in the sense that it keeps a
history of the success of the variables on which already has
been branched [Benichou et al., 1971]. Despite tiny computa-
tion, PC relies on human intuition and extensive engineering,
requiring signiﬁcant manual tuning. SB is to evaluate which
of the fractional candidates gives the largest progress before
actually branching on any of them [Applegate et al., 1995].
This approach can lead to the smallest search tree currently
known. However, it increases the computation signiﬁcantly.
[Alvarez et al., 2014a] adopted machine learning algo-
rithms early to learn the strategies of branching variable se-
lection in the B&B algorithm. Such kind of learning-based
strategies are also known as learning to branch. Learning
to branch is different from the traditional optimization meth-
ods. It introduces the concept of learning in the optimization
process to help search the optimal solution more effectively.
[Balcan et al., 2018] has shown empirically and theoretically
that it is possible to learn high-performing branching strate-
gies for a given application domain. Learning branching poli-

 
 
 
 
 
 
cies for MILP has become an active research area. Most rele-
vant researches use supervised or imitation learning to imitate
SB method and specialize it to distinct classes of problems.
However, only the data collected by expert policies can be
used to train during the imitation learning process in their
studies, which leads to mismatch between training data and
real-world data. This factor prevents learning a good enough
policy by the means of imitation learning. Some other schol-
ars attempt to use reinforcement learning and model the vari-
able selection process as a Markov Decision Process (MDP),
to obtain more effective and non-myopic policies. However,
for each instance, the MDP contains a large number of steps
and actions, which can lead to a large variance in gradient
estimation. Besides, too large action set makes it hard to ex-
plore in MILP. These challenges restrict the applications of
reinforcement learning in solving MILP [Sun et al., 2020].

In this work, we attempt to address the above challenges
by proposing a novel reinforcement learning-based branching
algorithm. Although there have been some ideas to solve the
branching problem by means of reinforcement learning [Sun
et al., 2020], we propose to address the learning problem in a
novel way, of which the contributions are summarized as the
following points:

• Demonstration data is leveraged to accelerate the learn-
ing massively at the early stage. With the improvement
of the training effect, the agent starts to interact with the
environment with its learned policy gradually.

• The RL agent updates its network with a mixture of
demonstration and self-generated data. We introduce a
prioritized storage mechanism to control the mixing ra-
tio automatically.

• In order to improve the robustness of the training pro-
cess, a superior Q-network is additionally introduced
based on Double DQN, which always serves as a Q-
network with competitive performance.

• We evaluate the performance of the proposed algo-
rithm on three benchmarks with comparative experi-
ments against three heuristic approaches and one state-
of-the-art (SOTA) imitation learning-based branching
algorithm [Gasse et al., 2019]. With the dual integral as
a metric, our algorithm outperforms the SOTA imitation
learning-based branching algorithm by 35.88% at most.
Besides, we also make an ablation study to validate the
rationality of this algorithm.

2 Related work
Supervised machine learning and imitation learning are cur-
rently the mainstream approaches for learning to branch. [Al-
varez et al., 2014b] proposed a new approach that uses su-
pervised learning to improve the performances of optimiza-
tion algorithms in the context of MILP. [Khalil et al., 2016]
proposed a machine learning framework for variable branch-
ing in MILP. Based on the data collected by SB method,
they learned an easy-to-evaluate surrogate function that mim-
ics the SB method, by means of solving a learning-to-rank
problem. And it is competitive with a state-of-the-art com-
[Gasse et al., 2019] proposed a new graph
mercial solver.

convolutional neural network (GCNN) model for learning
to branch, which leverages the natural variable-constraint
bipartite graph representation of MILP. They trained the
GCNN model via imitation learning from the SB method, and
demonstrated that this model produced policies that improved
upon state-of-the-art machine learning methods for branch-
ing. [Gupta et al., 2020] proposed a new hybrid architecture
for efﬁcient branching on CPU machines, which combined
the expressive power of GCNNs with computationally inex-
pensive multi-layer perceptrons (MLPs) for branching.

In order to obtain more efﬁcient and non-myopic policies,
some scholars attempt to use reinforcement learning to solve
this problem. [Etheve et al., 2020] proposed Fitting for Min-
imising the SubTree Size, a novel approach based on rein-
forcement learning, whose strength lies in the consistency
between a local value function and a global metric of inter-
[Sun et al., 2020] introduced a novel set representa-
est.
tion and optimal transport distance for the branching process
associated with a policy, to train the reinforcement learning
agent. The results showed substantial improvements in em-
pirical evaluation. More related researches can refer to the
survey provided by [Huang et al., 2021].

3 Background
3.1 Mixed integer linear programs
A mixed integer linear program is an optimization problem of
the form

(cid:8)cT x | Ax ≤ b, l ≤ x ≤ u, x ∈ Zp × Rn-p(cid:9)

arg min
x

where c ∈ Rn is the objective coefﬁcient vector, A ∈ Rm×n
is the constraint coefﬁcient matrix, b ∈ Rm is the constraint
right-hand-side vector, l, u ∈ Rn represent the lower and up-
per variable bound vectors respectively, and p is the number
of integer variables. The linear programming (LP) relaxation
of a MILP is shown below

(cid:8)cT x | Ax ≤ b, l ≤ x ≤ u, x ∈ Rn(cid:9)

arg min
x

The LP solution provides a lower bound to the original MILP.
Speciﬁcally, if the LP solution is subject to the integer con-
straint, then it is also a optimal feasible solution of the MILP.
Otherwise, the LP relaxation is required to be decomposed
into two sub-problems. This is done by branching on a vari-
able that does not obey the integrality constraint in the cur-
rent LP solution. The solving process terminates when the
feasible regions cannot be decomposed anymore, and subse-
quently a certiﬁcate of optimality or infeasibility can be pro-
vided respectively.

3.2 Branch-and-bound
A key factor inﬂuencing the efﬁciency of B&B algorithm
is how to select a fractional variable to branch on. There
are commonly two basic heuristic branching rules, includ-
ing Strong Branching (SB) and Pseudocost Branching (PC)
strategies. Most other heuristic rules are derived from them.
The details of SB and PC are described as follows.

SB is to evaluate which of the fractional candidates gives
the best progress before actually branching on any of them.

For each candidate variable, this evaluation process is real-
ized by solving the LP relaxations of the two sub-problems.
Thus, a huge amount of computation is required when adopt-
ing SB method.

• State: A node bipartite graph representation of B&B
states used in Gasse et al. [Gasse et al., 2019], using
the ecole.observation.NodeBipartite observation func-
tion [Prouvost et al., 2020].

j (Ψ−

PC is a sophisticated rule in the sense that it keeps a his-
tory of the success of the variable on which already has been
branched. Ψ+
j ) denotes the average unit objective gain
taken over upwards (downwards) branching on xj in previous
nodes. Pseudocost branching at node N with LP relaxation
solution ˇx consists in computing values:
P Cj = score((ˇxj − (cid:98)ˇxj(cid:99))Ψ−

j , ((cid:100)ˇxj(cid:101) − ˇxj)Ψ+
j )

and choosing the candidate variable with highest such value.
Compared with the SB method, PC method is simpler but
faster. However, PC method is overly dependent on human
intuition and extensive engineering, requiring considerable
manual tuning.

3.3 Reinforcement learning for branching
According to the reference [Gasse et al., 2019], the B&B pro-
cess can be modeled as a Markov decision process (MDP),
in which the solver is considered as the environment. Prou-
vost et al.
presented Ecole, a new library to simplify
machine learning research for combinatorial optimization,
which lowers the bar of entry and accelerates innovation in
this ﬁeld [Prouvost et al., 2020].

Based on Ecole, the B&B is episodic. Each episode corre-
sponds to a MILP instance. The state and reward of the MDP
can be deﬁned by an observation function and a reward func-
tion in Ecole respectively. Or even users can deﬁne new en-
vironments and simply reuse existing observation and reward
functions to fulﬁll speciﬁc requirements. The probability of a
trajectory τ = (s0, ..., sT ) ∈ T depends on both the branch-
ing policy π and the remaining components of the solver,

pπ(τ ) = p(s0)

T −1
(cid:89)

(cid:88)

t=0

a∈A(st)

π(a | st)p(st+1 | st, a)

Compared with the imitation learning, the reinforcement
learning can balance the exploration and exploitation in an
unknown environment. Theoretically speaking, it possesses
higher performance.

4 Proposed algorithm
In this paper, a novel algorithm based on reinforcement learn-
ing is proposed for dealing with the B&B variable selection
problem in MILP. In order to illustrate our algorithm, we or-
ganize this section in two parts. Firstly, we give our formula-
tion of the variable selection process as a reinforcement learn-
ing problem. Then, we introduce the speciﬁc implementation
of this algorithm in detail.

4.1 Formulation
Let the solver be the environment. The sequential deci-
sion making of variable selection can be formulated as a
Markov Decision Process (MDP). Speciﬁcally, the state s, ac-
tion space A, action a, transition P, and reward r are described
as follows.

• Action space: The set of candidate variables.

• Action: The selected candidate variable to branch on.
• Transition: Given state st and action at, the next state

st+1 is determined by the node selection policy.

• Reward: The reward is deﬁned as the dual integral since
the previous state, where the integral is computed with
respect to the solving time. Details are explained in the
Metric part of the Experimental evaluation section.

• Next state: The node bipartite graph representation of

the next node.

• Next action space: The set of candidate variables corre-

sponding to the next node.

• Done: The termination ﬂag.

4.2 Algorithm
In this section, based on Double DQN [Van Hasselt et al.,
2016], we propose a novel deep reinforcement learning al-
gorithm for branching problem. It leverages demonstration
data to massively accelerate the learning process by means of
ofﬂine reinforcement learning. In order to balance the explo-
ration and exploitation, this algorithm can automatically con-
trol the mixing ratio of demonstration data during the learning
process due to a prioritized storage mechanism. Besides, in
order to avoid the multi-fold challenges caused by the large
state space and action set, we additionally introduce a supe-
rior network, which always serves as a Q-network with com-
petitive performance. Figure 1 shows an overview of the pro-
posed algorithm.

In order to accelerate the learning process at the early stage,
the replay buffer is given a set of demonstration data, which
will remain permanently. In this phase, the agent trains solely
on the demonstration data by means of ofﬂine reinforcement
learning, without any interaction with the environment. And
it can obtain relatively good performance in a short period of
time. However, the agent may make incorrect estimates for
unfamiliar (s, a) pairs due to the lack of interaction with the
environment. Therefore, we introduce a prioritized storage
mechanism, in which the agent collects self-generated data
and adds it to the replay buffer conditionally. In detail, only
the data collected by the policy that leads to high performance
is added to the replay buffer, which can prevent some poor-
quality data from misleading the training. Data is added to
the buffer until it is full, and then agent starts overwriting old
data in the buffer. However, the agent never overwrites the
demonstration data.
In this way, the mixing ratio between
demonstration and self-generated data can be controlled au-
tomatically.

It is commonly believed that too large action set increases
the difﬁculty of RL training [Sun et al., 2020]. The RL agent
may learn a local optimal policy, or even worse, a random
policy. In order to solve this issue, a superior Q-network is
additionally introduced based on the framework of Double

Figure 1: An overview of the proposed algorithm: Environment corresponds to a solver aiming at variable selection problem in B&B; Buffer
is ﬁlled with both demonstration and self-generated data, and only high-quality self-generated data can be added in it; The main part of the
algorithm is composed of three networks, where online net is used for action selection, target net is used for action evaluation to decompose
the max operation, superior net always serves as a Q-network with competitive performance to help to stabilize the training process.

DQN. And then, there are three Q-networks in this frame,
i.e. online network, target network, and superior network.
The online network is used for action selection. The tar-
get network is used for action evaluation to decompose the
max operation. The superior network always serves as a Q-
network with competitive performance to help to stabilize
the training process. Similar to the works of Gasse et al.,
a graph convolutional neural network (GCNN) is adopted to
parametrize the Q-network [Gasse et al., 2019]. In detail, the
input of the GCNN model is the bipartite state representa-
tion st = (G, C, V, E). The graph convolution can be broken
down into two successive passes, one from variables to con-
straints and the other one from constraints to variables, which
are shown as

(i,j)∈ε
(cid:88)

ci ← fc(ci,

gc(ci, vj, ei,j))

vi ← fv(vj,

j

(i,j)∈ε
(cid:88)

j

gv(ci, vj, ei,j))

where fc, fv, gc and gv are two-layer perceptrons with ReLU
activation functions. In addition to TD loss used in Double
DQN, we add another term in the overall loss used to update
the online network, which is shown as

L(θ) =E(r + γ max

a(cid:48)

Qt(s(cid:48), a(cid:48), θt) − Q(s, a, θ))

+E(Qs(s, a, θs) − Q(s, a, θ))

where the superscript t and s represent target and superior
respectively.

Pseudo-code is sketched in Algorithm 1. The behavior pol-

icy πεQθ is ε-greedy with respect to Qθ.

5 Experimental evaluation

In this part, we achieve a comparative experiment against
three heuristic approaches and one machine learning ap-
proach to evaluate the effectiveness of our algorithm. Mean-
while, we also make an ablation study to validate the ratio-
nality of this algorithm.

5.1 Setup

The proposed method is trained on a computing server
which is equipped with Intel(R) Xeon(R) Platinum 8180M
CPU@2.50GHz, a V100 GPU card with 32GB graphic mem-
ory and 1TB main memory.

Benchmark: We evaluate our algorithm on three bench-
marks from diverse application areas, including Balanced
Item Placement (BIP), Workload Apportionment (WA), and
an Anonymous Problem (AP). All the three benchmarks are
from the Machine Learning for Combinatorial Optimization
(ML4CO) NeurIPS 2021 competition [NeurIPS 2021 Com-
petition, 2021]. The ﬁrst two benchmarks are inspired by
real-life applications of large-scale systems at Google, and
the third benchmark is an anonymous problem also inspired
by a real-world, large-scale industrial application.

Metric: The dual integral is used as the evaluation met-
ric, which measures the area over the curve of the solver’s
dual bound (a.k.a. global lower bound).
It usually corre-
sponds to a solution of a valid relaxation of the MILP. Theo-
retically, a small dual integral will lead to both good and fast
decisions. By branching, the LP relaxations corresponding to
the branch-and-bound tree leaves get tightened, and the dual
bound increases over time. With a time limit T, the dual inte-
gral expresses as:

T cT x∗ −

(cid:90) T

t=0

z∗
t dt

Algorithm 1 Double Deep Q-learning with Superior Network
Require: Dreplay: initialized with demonstration data set;
θ: weights for online network (random initialization);
θt: weights for target network (random initialization);
θs: weights for superior network (random initialization);
τ t: frequency at which to update target network;
τ s: frequency at which to evaluate the trained policy;
G0: a threshold to measure the performance of behavior
policy;
Gbest: the cumulative reward of the best trained policy
in the training process (zero initialization).

Sample action from behavior policy a ∼ πεQθ
Play action a and observe (s(cid:48), r)
Store (s, a, r, s(cid:48)) into a temporary buffer Dtemporary
Sample a mini-batch of n transitions from Dreplay
Calculate loss L using target and superior network
Perform a gradient descent step to update θ
if t mod τ t = 0 then

1: for steps t ∈ 1, 2, ... do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

end if
if t mod τ s = 0 then

Compute the cumulative reward G to evaluate the
trained policy
if G > G0 then

θt ← θ

Store (s, a, r, s(cid:48)) in Dtemporary into Dreplay,
overwriting oldest self-generated transition if
over capacity, emptying Dtemporary

13:
14:

end if
if G > Gbest then

θs ← θ

15:
16:
17:
18:
19:
20:
21: end for

end if
s ← s(cid:48)

end if

where z∗
t is the best dual bound at time t, and T cT x∗ is an
instance-speciﬁc constant that depends on the optimal solu-
tion value cT x∗. The dual integral is to be minimized, and
takes an optimal value of 0. The optimization process is
shown in Figure 2. In the experiments of this paper, the time
limit is set as 15 minutes.

Baselines: We compete against three heuristic approaches,
including strong branching, pseudocost branching, active
constraint method (a method suitable for solving large-scale
problems) [Patel and Chinneck, 2007], and one machine
imitation learning from the strong
learning approach, i.e.
branching expert rule [Gasse et al., 2019].

5.2 Comparative experiment
In this experiment, the demonstration data set size is 50K for
BIP and AP datasets, and 10K for the WA dataset.

• Batch size: 32 for BIP and AP, 24 for WA

• Learning rate: 0.001

• ε-greedy with ε = 0.01

• Dimension of input embedding in GCNN: 64

Figure 2: The dual integral during the optimization process

• Replay buffer size: 100K for BIP and AP, 20K for WA

• Discount factor: 0.99
• Period for target model’s hard update: τ t = 500
• Period for evaluating the trained policy: τ s = 1000
• Epochs: 50K

Algorithms
SB
PC
AC
IL
RL

Scores Wins
0/100
3767.95
1/100
4268.95
3/100
5800.05
33/100
5335.94
63/100
7250.29

Table 1: Evaluation results on Balanced Item Placement

Algorithms
SB
PC
AC
IL
RL

Scores
623469.3
624727.1
624256.8
624326.4
624932.8

Wins
13/100
18/100
14/100
20/100
35/100

Table 2: Evaluation results on Workload Apportionment

Algorithms
SB
PC
AC
IL
RL

Scores
30387450
31600684
30995284
32446178
32547931

Wins
27/100
1/100
1/100
29/100
42/100

Table 3: Evaluation results on Anonymous Problem

We evaluate different branching algorithms on 100 in-
stances for each benchmarks. Evaluation results on three
benchmarks are shown in Tables 1-3 respectively.

In Tables 1-3, SB, PC, AC, IL, RL represent the strong
branching, pseudocost branching, active constraint method,

Dual IntegralDual Bound z*tTime tt=Tt=0cTx*0(a) Balanced Item Placement

(b) Workload Apportionment

(c) Anonymous Problem

Figure 3: The dual bound curve of different algorithms during the solving process

(a) Balanced Item Placement

(b) Workload Apportionment

(c) Anonymous Problem

Figure 4: The performance curve of ablation study in the training process

imitation learning from the strong branching expert rule, and
our algorithm respectively.

As can be seen, AC is indeed more suitable for solving
large-scale MILP problems among the three heuristic algo-
rithms. However, its performance is still inferior to our al-
gorithm. Furthermore, it is worth mentioning that both the
imitation learning algorithm proposed by Gasse et al. and our
reinforcement learning algorithm learn from the demonstra-
tion data collected by SB and PC approaches. It can be ob-
served that both machine learning algorithms have a greater
performance improvement than expert rules. Meanwhile, un-
der the same conditions including the data set size, learning
rate, etc, the performance of our algorithm is signiﬁcantly
better than that of the imitation learning algorithm, which is
demonstrated by the facts that our algorithm obtains the high-
est scores and wins on the largest number of instances for all
three benchmarks. Compared with IL, this proposed algo-
rithm possesses a performance improvement of up to 35.88%,
exactly on the ﬁrst benchmark.

In order to further illustrate the effect of different algo-
rithms, we perform branch and bound on one instance for
each benchmark, and give the dual bound curve in this pro-
cess, which are shown in Figure 3. It can be seen that our al-
gorithm can indeed result in the smallest dual integral, which
suggests that it can lead to both the best and fastest decisions.
5.3 Ablation study
We present an ablation study of our proposed algorithm
by comparing with two reinforcement learning algorithms,
including the deep reinforcement learning with double Q-
learning and the double deep Q-learning only with demon-
stration data. We plot the performance curves of different
algorithms on test dataset during training process for all three
benchmarks. The results are shown in Figure 4. DQN, DQfD
and DQfDwS represent the Double DQN, Double DQN with

the guidance of expert rules and our algorithm respectively.
It can be found that the performance of deep reinforcement
learning with double Q-learning is extremely poor to solve
the MILP problems, in which the trained policy are almost
always random. After adding the guidance of expert rules, it
can be seen that the training process is greatly accelerated in
the early stage of training, but it will soon fall into the local
optimal strategy, which is caused by the too large state space
and action set. However, after adding the superior Q-network,
our algorithm can not only help to accelerate the training pro-
cess in the early stage but also help to stabilize the training
process in the late stage. Thus our algorithm has the potential
to advance the performance of B&B algorithm continuously.
6 Conclusion
In this paper, we propose a novel reinforcement learning
method to improve the B&B algorithm performance in MILP.
Firstly, in view of the shortcomings of reinforcement learning
algorithms that are difﬁcult to explore in the early stage of this
problem, demonstration data collected by strong branch rule
is leveraged to accelerate the learning process signiﬁcantly.
Subsequently, as the training effect improves, the agent grad-
ually begins to interact with the solver with its learned policy.
To avoid misleading training by low-quality self-generated
data, a prioritized storage mechanism is introduced to guar-
antee the high quality of data in the replay buffer and control
the mixing ratio of demonstration and self-generated data au-
tomatically. Besides, too large state space and action set lead
to a large variance in gradient estimation, which makes it hard
to explore in MILP. In order to address this challenge, we ad-
ditionally introduced a superior Q-network based on Double
DQN, which always serves as a Q-network with competitive
performance. A group of comparative experiments and ab-
lation studies demonstrate that this algorithm is signiﬁcantly
effective in performance improvement of B&B algorithm.

03691215Time(min)4.106.158.2010.2512.30Dual BoundSBPCACILRL03691215Time(min)658.575659.198659.820660.443661.065Dual BoundSBPCACILRL03691215Time(min)22533380450656336759Dual BoundSBPCACILRL01020304050Episode19982996399549945993ScoreDQNDQfDDQfDwS01020304050Episode592906593343593781594218594655595092ScoreDQNDQfDDQfDwS01020304050Episode29670513956068494508559341036923120ScoreDQNDQfDDQfDwS[Prouvost et al., 2020] Antoine

Justin Du-
mouchelle, Lara Scavuzzo, Maxime Gasse, Didier
Ch´etelat, and Andrea Lodi. Ecole: A gym-like library for
machine learning in combinatorial optimization solvers.
arXiv preprint arXiv:2011.06069, 2020.

Prouvost,

[Sun et al., 2020] Haoran Sun, Wenbo Chen, Hui Li, and
Le Song. Improving learning to branch via reinforcement
learning. 2020.

[Van Hasselt et al., 2016] Hado Van Hasselt, Arthur Guez,
and David Silver. Deep reinforcement learning with dou-
ble q-learning. In Proceedings of the AAAI conference on
artiﬁcial intelligence, volume 30, 2016.

References
[Achterberg et al., 2005] Tobias Achterberg, Thorsten Koch,
and Alexander Martin. Branching rules revisited. Opera-
tions Research Letters, 33(1):42–54, Jan 2005.

[Alvarez et al., 2014a] Alejandro Marcos Alvarez, Quentin
Louveaux, and Louis Wehenkel. A supervised machine
learning approach to variable branching in branch-and-
bound. In IN ECML, 2014.

[Alvarez et al., 2014b] Marcos Alvarez, Alejandro, Quentin
Louveaux, and Louis Wehenkel. A supervised machine
learning approach to variable branching in branch-and-
bound. 2014.

[Applegate et al., 1995] D Applegate, R Bixby, V Chv´atal,
and W Cook. Finding cuts in the tsp. DIMACS Technical
Report 95-05, Mar 1995.

[Balcan et al., 2018] MF Balcan, T Dick, T Sandholm, and
E Vitercik. Learning to branch. In Proceedings of the 35th
International Conference on Machine Learning, Stock-
holm, Sweden, July 2018. ACM.
[Benichou et al., 1971] M. Benichou,

J. M. Gauthier,
P. Girodet, G. Hentges, G. Ribiere, and O. Vincent.
Experiments
linear programming.
in mixed-integer
Mathematical Programming, 1:76–94, Dec 1971.

[Etheve et al., 2020] Marc Etheve, Zacharie Al`es, Cˆome Bis-
suel, Olivier Juan, and Saﬁa Kedad-Sidhoum. Reinforce-
ment learning for variable selection in a branch and bound
algorithm. In International Conference on Integration of
Constraint Programming, Artiﬁcial Intelligence, and Op-
erations Research, pages 176–185. Springer, 2020.

[Gasse et al., 2019] Maxime Gasse, Didier Ch´etelat, Nicola
Ferroni, Laurent Charlin, and Andrea Lodi. Exact combi-
natorial optimization with graph convolutional neural net-
works. arXiv preprint arXiv:1906.01629, 2019.

[Gupta et al., 2020] Prateek Gupta, Maxime Gasse, Elias B
Khalil, M Pawan Kumar, Andrea Lodi, and Yoshua Ben-
gio. Hybrid models for learning to branch. arXiv preprint
arXiv:2006.15212, 2020.

[Huang et al., 2021] Lingying Huang, Xiaomeng Chen, Wei
Huo, Jiazheng Wang, Fan Zhang, Bo Bai, and Ling Shi.
Branch and bound in mixed integer linear programming
arXiv
problems: A survey of techniques and trends.
preprint arXiv:2111.06257, 2021.

[Khalil et al., 2016] Elias Khalil, Pierre Le Bodic, Le Song,
Learning to
George Nemhauser, and Bistra Dilkina.
branch in mixed integer programming. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, volume 30,
2016.

[NeurIPS 2021 Competition, 2021] NeurIPS 2021 Competi-
tion. Machine learning for combinatorial optimization,
https://www.ecole.ai/2021/ml4co-competition/,, 2021.
[Patel and Chinneck, 2007] Jagat Patel and John W Chin-
neck. Active-constraint variable ordering for faster fea-
sibility of mixed integer linear programs. Mathematical
Programming, 110(3):445–474, 2007.

