International Journal of Computer Vision manuscript No.
(will be inserted by the editor)

MAP Inference via ℓ2-Sphere Linear Program Reformulation

Baoyuan Wu · Li Shen · Tong Zhang · Bernard Ghanem

0
2
0
2

n
a
J

0
2

]

G
L
.
s
c
[

3
v
3
3
4
3
0
.
5
0
9
1
:
v
i
X
r
a

the date of receipt and acceptance should be inserted later

Abstract Maximum a posteriori (MAP) inference is an im-
portant task for graphical models. Due to complex depen-
dencies among variables in realistic models, ﬁnding an exact
solution for MAP inference is often intractable. Thus, many
approximation methods have been developed, among which
the linear programming (LP) relaxation based methods show
promising performance. However, one major drawback of
LP relaxation is that it is possible to give fractional solu-
tions. Instead of presenting a tighter relaxation, in this work
we propose a continuous but equivalent reformulation of the
original MAP inference problem, called LS-LP. We add the
ℓ2-sphere constraint onto the original LP relaxation, leading
to an intersected space with the local marginal polytope that
is equivalent to the space of all valid integer label conﬁgura-
tions. Thus, LS-LP is equivalent to the original MAP infer-
ence problem. We propose a perturbed alternating direction
method of multipliers (ADMM) algorithm to optimize the

Baoyuan Wu was partially supported by Tencent AI Lab and King Ab-
dullah University of Science and Technology (KAUST). Li Shen was
supported by Tencent AI Lab. Bernard Ghanem was supported by the
King Abdullah University of Science and Technology (KAUST) Of-
ﬁce of Sponsored Research (OSR). Tong Zhang was supported by the
Hong Kong University of Science and Technology (HKUST). Li Shen
is the corresponding author.

Baoyuan Wu
Tencent AI Lab, Shenzhen 518000, China
E-mail: wubaoyuan1987@gmail.com

Li Shen
Tencent AI Lab, Shenzhen 518000, China
E-mail: mathshenli@gmail.com

Tong Zhang
Hong Kong University of Science and Technology, Hong Kong, China
E-mail: tongzhang@tongzhang-ml.org

Bernard Ghanem
King Abdullah University of Science and Technology, Thuwal 23955,
Saudi Arabia
E-mail: bernard.ghanem@kaust.edu.sa

LS-LP problem, by adding a sufﬁciently small perturbation
ǫ onto the objective function and constraints. We prove that
the perturbed ADMM algorithm globally converges to the
ǫ-Karush–Kuhn–Tucker (ǫ-KKT) point of the LS-LP prob-
lem. The convergence rate will also be analyzed. Experi-
ments on several benchmark datasets from Probabilistic In-
ference Challenge (PIC 2011) and OpenGM 2 show com-
petitive performance of our proposed method against state-
of-the-art MAP inference methods.

1 Introduction

Given the probability distribution of a graphical model, max-
imum a posteriori (MAP) inference aims to infer the most
probable label conﬁguration. MAP inference can be formu-
lated as an integer linear program (ILP) [39]. However, due
to the integer constraint, the exact optimization of ILP is
intractable in many realistic problems. To tackle it, a popu-
lar approach is relaxing ILP to a continuous linear program
over a local marginal polytope, i.e., LG (deﬁned in Section
3), called linear programming (LP) relaxation. The optimal
solution to the LP relaxation will be obtained at the vertices
of LG. It has been known [39] that all valid integer label
conﬁgurations are at the vertices of LG, but not all vertices
of LG are integer, while some are fractional. Since LP re-
laxation is likely to give fractional solutions, the rounding
method must be adopted to generate integer solutions. To
alleviate this issue, intense efforts have been made to design
tighter relaxations (e.g., high-order relaxation [36]) based on
LP relaxation, such that the proportion of fractional vertices
of LG can be reduced. However, the possibility of fractional
solutions still exists. And, these tighter relaxations are often
much more computationally expensive than the original LP
relaxation. Moreover, there are also exact inference meth-
ods, such as branch-and-bound [23] and cutting-plane [17],

 
 
 
 
 
 
2

Baoyuan Wu et al.

which utilize LP relaxation as sub-routines, leading to much
higher computational cost than approximate methods.

2 k2

2= n

Instead of proposing a new approximation with a tighter
relaxation, we propose an exact reformulation of the orig-
inal MAP inference problem. Speciﬁcally, we add a new
constraint, called ℓ2-sphere [41], onto the original LP relax-
ation problem. It enforces that the solution x ∈ Rn should
be on a ℓ2-sphere, i.e., k x − 1
4 . We can prove that
the intersection between the ℓ2-sphere constraint and the lo-
cal polytope LG is equivalent to the set of all possible la-
bel conﬁgurations of the original MAP inference problem,
i.e., the constraint space of the ILP problem. Thus, the pro-
posed formulation, dubbed LS-LP, is an equivalent but con-
tinuous reformulation of the ILP formulation for MAP in-
ference. Furthermore, inspired by [28] and [41], we adopt
the ADMM algorithm [6], to not only separate the different
constraints, but also decompose variables to allow parallel
inference by exploiting the factor graph structure. Although
the ℓ2-sphere constraint is non-convex, we prove that the
ADMM algorithm for the LS-LP problem with a sufﬁciently
small perturbation ǫ will globally converges to the ǫ-KKT
[16,22] point of the original LS-LP problem. The obvious
advantages of the proposed LS-LP formulation and the cor-
responding ADMM algorithm include: 1) compared to other
LP relaxation based methods, our method directly gives the
valid integer label conﬁguration, without any rounding tech-
niques as post-processing; 2) compared to the exact meth-
ods like branch-and-bound [23] and cutting-plane [17], our
method optimizes one single continuous problem once, rather
than multiple times. Experiments on benchmarks from Prob-
abilistic Inference Challenge (PIC 2011) [7] and OpenGM 2
[14] verify the competitive performance of LS-LP against
state-of-the-art MAP inference methods.

The main contributions of this work are three-fold. 1)
We propose a continuous but equivalent reformulation of the
MAP inference problem. 2) We present the ADMM algo-
rithm for optimizing the perturbed LS-LP problem, which is
proved to be globally convergent to the ǫ-KKT point of the
original LS-LP problem. The analysis of convergence rate is
also presented. 3) Experiments on benchmark datasets ver-
ify the competitive performance of our method compared to
state-of-the-art MAP inference methods.

2 Related Work

As our method is closely related to LP relaxation based MAP
inference methods, here we mainly review MAP inference
methods of this category. For other categories of methods,
such as message passing and move making, we refer the
readers to [39] and [14] for more details. Although some
off-the-shelf LP solvers can be used to optimize the LP re-
laxation problem, in many real-world applications the prob-
lem scale is too large to adopt these solvers. Hence, most

methods focus on developing efﬁcient algorithms to opti-
mize the dual LP problem. Block coordinate descent meth-
ods [9,19] are fast, but they may converge to sub-optimal
solutions. Sub-gradient based methods [20,15] can converge
to global solutions, but their convergence is slow. Their com-
mon drawback is the non-smoothness of the dual objective
function. To handle this difﬁculty, some smoothing meth-
ods have been developed. The Lagrangian relaxation [12]
method uses the smooth log-sum-exp function to approx-
imate the non-smooth max function in the dual objective.
A proximal regularization [13] or an ℓ2 regularization term
[30] is added to the dual objective. Moreover, the steepest
ǫ-descent method proposed in [34] and [35] can accelerate
the convergence of the standard sub-gradient based methods.
Parallel MAP inference methods based on ADMM have also
been developed to handle large-scale inference problems.
For example, AD3 [28,27] and Bethe-ADMM [8] optimize
the primal LP problem, while ADMM-dual [29] optimizes
the dual LP problem. The common drawback of these meth-
ods is that they are likely to produce fractional solutions,
since the underlying problem is merely a relaxation to the
MAP inference problem.

Another direction is pursuing tighter relaxations, such
as high-order consistency [36] and SDP relaxation [24]. But
they are often more computationally expensive than LP re-
laxations. In contrast, the formulation of the proposed LS-
LP is an exact reformulation of the original MAP inference
problem, and the adopted ADMM algorithm can explicitly
produce valid integer label conﬁgurations, without any round-
ing operation. In comparison with other expensive exact MAP
inference methods (e.g., Branch-and-Bound [23] and cutting
plane [17]), LS-LP is very efﬁcient owing to the resulting
parallel inference, similar to other ADMM based methods.

Another related work is ℓp-Box ADMM [41], which is
a framework to optimize the general integer program. The
proposed LS-LP is inspired by this framework, where the
integer constraints are replaced by the intersection of two
continuous constraints. However, 1) LS-LP is speciﬁcally
designed for MAP inference, as it replaces the valid inte-
ger conﬁguration space (e.g., {(0, 1), (1, 0)} for the variable
with binary states), rather than the whole binary space (e.g.,
{(0, 0), (0, 1), (1, 0), (1, 1)}) as did in ℓp-Box ADMM. 2)
LS-LP is tightly combined with LP relaxation, and the ADMM
algorithm decomposes the problem into multiple simple sub-
problems by utilizing the structure of the factor graph, which
allows parallel inference for any type of inference problems
(e.g., multiple variable states and high-order factors). In con-
trast, ℓp-Box ADMM does not assume any special property
for the objective function, and it optimizes all variable nodes
in one sub-problem. Especially for large-scale models, the
sub-problem involved in ℓp-Box ADMM will be very cost.
3) As LP relaxation is parameterized according to the factor
graph, any type of graphical models (e.g., directed models,

MAP Inference via ℓ2-Sphere Linear Program Reformulation

3

high-order potentials, asymmetric potentials) can be natu-
rally handled by LS-LP. In contrast, ℓp-Box ADMM needs
to transform the inference objective based on MRF mod-
els to some simple forms (e.g., binary quadratic program
(BQP)). However, the transformation is non-trivial in some
cases. For example, if there are high-order potentials, the
graphical model is difﬁcult to input into a BQP problem.

3 Background

3.1 Factor Graph

Denote G = {g1, g2, . . . , gN } as a set of N random vari-
ables in a discrete space X = X1 × . . . × XN , where Xi =
{0, . . . , ri − 1} with ri = |Xi| being the number of possible
states of gi. The joint probability of G is formulated based
on a factor graph G [18],

P (G) ∝ exp

θi(gi) +

(cid:0) Xi∈V

,

θα(gα)
(cid:1)

Xα∈F

(1)

where G = (V, F, E) with V = {1, . . . , N } being the node
set of variables, F being the node set of factors, as well
as the edge set E ⊆ V × F linking the variable and fac-
tor nodes. A simple MRF model and its factor graph are
shown in Fig. 1(a,b). We refer the readers to [18] for the
detailed deﬁnition of the factor graph. gα indicates the la-
bel conﬁguration of the factor α, and its state will be deter-
mined according to the states of connected variable nodes,
i.e., {gi|i ∈ Nα}, with Nα being the set of neighborhood
variable nodes of the factor α. θi(·) denotes the unary log
potential (logPot) function, while θα(·) indicates the factor
logPot function.

MRF    Factor Graph

   Augmented Factor Graph

  extra 
 variable

   L2 sphere

  variable

       factor

(a)

(b)

(c)

Fig. 1: An example of (a) MRF, (b) factor graph correspond-
ing to LP and (c) augmented factor graph corresponding to
LS-LP.

1, while all other entries are 0. Similarly, µα ∈ {0, 1}|Xα|
indicates the label vector corresponding to gα. The local
marginal polytope is deﬁned as follows,

LG =

µ

(cid:8)

µα ∈ ∆|µα|, ∀α ∈ F ;
(cid:12)
µi = Miαµα, ∀(i, α) ∈ E
(cid:12)

.
(cid:9)

with ∆|a| = {a|1⊤a = 1, a ≥ 0} being the probability
simplex, and the second constraint ensures the local consis-
tency between µi and µα. Miα ∈ {0, 1}|Xi|×|Xα| of the
local consistency constraint included in LG is deﬁned as:
the entry of Miα is 1 if gα ∼ gi, where gα ∼ gi indicates
the state of gi and the state of the corresponding element
in gα are the same; otherwise, the entry is 0. For example,
we consider a binary-state variable node µi ∈ {0, 1}2 and
a pairwise factor node µα ∈ {0, 1}4 connected to two vari-
able nodes (the variable node i is the ﬁrst). The ﬁrst entry
of µi indicates the score of choosing state 0, while the sec-
ond entry corresponds to that of choosing state 1. The four
entries of µα indicate the scores of four label conﬁgurations
of two connected variables, i.e., (0, 0), (0, 1), (1, 0), (1, 1).
In this case, Miα = [1, 1, 0, 0; 0, 0, 1, 1].

3.2 MAP Inference as Linear Program

Moreover, Eq. (2) can also be rewritten as

Given P (G), an important task is to ﬁnd the most probable
label conﬁguration of G, referred to as MAP inference,

MAP(θ) = max
µ∈MG

hθ, µi,

MAP(θ) = max

G∈X Xi∈V

θi(gi) +

θα(gα).

(2)

Xα∈F

Eq. (2) can be reformulated as the integer linear program

(ILP) [39],

ILP(θ) = max

µ

θ⊤
i µi +

Xi∈V

Xα∈F

θ⊤
α µα = max

µ

hθ, µi,

(3)

s.t. µ ∈ LG ∩ {0, 1}|µ|,

where the marginal polytope is deﬁned as follows,

MG = {µ | ∃P (X), such that µi, µα ∈ LG}.

Solving MAP(θ) is difﬁcult (NP-hard in general), especially
for large scale problems. Instead, the approximation over
LG is widely adopted, as follows:

LP(θ) = max
µ∈LG

hθ, µi ≥ ILP(θ) = MAP(θ),

(7)

where θ = (. . . ; θi; . . . ; θα; . . .), i ∈ V, α ∈ F denotes
the log potential (logPot) vector, derived from θi(gi) and
θα(gα). µ = [µV ; µF ], where µV = [µ1; . . . ; µ|V |] and
µF = [µ1; . . . ; µ|F |]. µi ∈ {0, 1}|Xi| indicates the label
vector corresponding to gi: if the state of gi is t, then µi(t) =

which is called LP relaxation. Note that here µi and µα
are continuous variables, and they are considered as local
marginals of gi and gα, respectively.

According to [39], the characteristics of LP(θ), MAP(θ)
and their relationships are brieﬂy summarized in Lemma 1.

(4)

(5)

(6)

4

Baoyuan Wu et al.

Lemma 1 [39] The relationship between MG and LG, and
that between MAP(θ) and LP(θ) are as follows.

– MG ⊆ LG;
– MAP(θ) ≤ LP(θ);
– All vertices of MG are integer, while LG includes both
integer and fractional vertices. And the set of integer ver-
tices of LG is same with the set of the vertices of MG.
All non-vertices in MG and LG are fractional points.
– Since both MG and LG are convex polytopes, the global
solutions of MAP(θ) and LP(θ) will be on the vertices
of MG and LG, respectively.

– The global solution µ∗ of LP(θ) can be fractional or
integer. If it is integer, then it is also the global solution
of MAP(θ).

3.3 Kurdyka-Lojasiewicz Inequality

The Kurdyka-Lojasiewicz inequality was ﬁrstly proposed in
[26], and it has been widely used in many recent works [2,
40,25] for the convergence analysis of non-convex prob-
lems. Since it will also be used in the later convergence anal-
ysis of our algorithm, it is ﬁrstly produced here, as shown in
Deﬁnition 1.

Deﬁnition 1 [2] A function f : Rn → R ∪ +∞ is said
to have the Kurdyka-Lojasiewicz (KL) property at x∗ ∈
dom(∂f ) (dom(·) denotes the domain of function, ∂ indi-
cates the sub-gradient operator), if the following two condi-
tions hold

– there exist a constant η ∈ (0, +∞], a neighborhood Vx∗
of x∗, as well as a continuous concave function ϕ :
[0, η) → R+, with ϕ(0) = 0 and ϕ is differentiable
on (0, η) with positive derivatives.

– ∀x ∈ Vx∗ satisfying f (x∗) < f (x) < f (x∗) + η, the

Kurdyka-Lojasiewicz inequality holds

ϕ′(f (x) − f (x∗))dist(0, ∂f (x)) ≥ 1.

(8)

Remark. According to [2,4,5], if f is semi-algebraic, then
it satisﬁes the KL property with ϕ(s) = cs1−p, where p ∈
[0, 1) and c > 0 are constants. This point will be used in
later analysis of convergence.

4 MAP Inference via ℓ2-sphere Linear Program
Reformulation

4.1 Equivalent Reformulation

Note that S is deﬁned with respect to the vector x, rather
than individual scalars xi, i = 1, . . . , n. We propose to add
the ℓ2-sphere constraint onto the variable nodes µV . Com-
bining this with LP relaxation (see Eq. (7) ), we propose a
new formulation for MAP inference,

LS-LP(θ) = max

hθ, µi, s.t. µ ∈ LG, µV ∈ S.

µ

(10)

Due to the non-convex constraint S, it is no longer a lin-
ear program. However, to emphasize its relationship to LP
relaxation, we still denote it as a ℓ2-sphere constrained lin-
ear program (LS-LP) reformulation. More importantly, as
shown in Proposition 1, LS-LP is equivalent to the origi-
nal MAP inference problem, rather than a relaxation as in
LP. Inspired by the constraint separation in ℓp-Box ADMM
[41], we introduce the extra variable υ to reformulate (10)
as

LS-LP(θ) = max
µ,υ

hθ, µi = min
µ,υ

h−θ, µi,

(11)

s.t. µ ∈ LG, υ ∈ S, µi = υi, ∀i ∈ V,

where υ = [υ1; . . . ; υi; . . . ; υ|V |], i ∈ V is the concate-
nated vector of all extra variable nodes. The combination
of the original factor graph and these extra variable nodes
is referred to as augmented factor graph (AFG). An ex-
ample of AFG corresponding to Problem (11) is shown in
Figure 1(c). The gray circles correspond to extra variables
υ, and connections to the purple box indicate that υ ∈ S.
Note that AFG does not satisfy the deﬁnition of the stan-
dard factor graph, where connections only exist between
variables nodes and factor nodes. However, AFG provides
a clear picture of the structure of LS-LP and the node rela-
tionships. The proposed LS-LP problem is equivalent to the
original MAP inference problem, as shown in Proposition 1.
It means that the global solutions of this two problems are
equivalent.

Lemma 2 The following constraint spaces are equivalent,

C1 = {µ | µ ∈ LG ∩ {0, 1}|µ|}
≡ C2 = {µ | µ ∈ LG and µV ∈ S}
≡ C3 = {µ | µ ∈ MG ∩ {0, 1}|µ|}.

Proof We start from C2, where we have

µV ∈ S ⇐⇒

k µi −

Xi∈V

1
2

k2
2=

i∈V |Xi|
4

.

P

Besides, the following relations hold

(12)

(13)

Firstly, we introduce the ℓ2-sphere constraint [41],

µ ∈ LG ⇐⇒ µα ∈ ∆|Xα| and µi = Miαµα

(14)

S =

x ∈ Rn | k x −

(cid:8)

1
2

1 k2

2=

n
4

.

(cid:9)

(9)

⇒µi ∈ [0, 1]|Xi| ⇒k µi −

1
2

k2
2≤

|Xi|
4

,

MAP Inference via ℓ2-Sphere Linear Program Reformulation

5

∀i ∈ V, ∀(i, α) ∈ E. The equation in the last relation holds
if and only if µi ∈ {0, 1}|Xi|. Combining with (13), we
conclude that µi ∈ {0, 1} holds ∀i ∈ V . Consequently,
utilizing the local consistency constraint µi = Miαµα, we
obtain that µα ∈ {0, 1} also holds ∀α ∈ F . Thus, we have
µ ∈ {0, 1}|µ|. Then, the relation C1 ≡ C2 is proved.

Besides, as shown in Lemma 1, the set of integer vertices
of LG is same with the one of MG, and all non-vertices in
MG and LG are fractional points. Thus, it is easy to know
C1 ≡ C3. Hence the proof is ﬁnished.

Theorem 1 Utilizing Lemma 2, the aforementioned MAP
inference problems have the following relationships,

LS-LP(θ) = ILP(θ) = MAP(θ) ≤ LP(θ).

(15)

Proof According to Lemma 1.3 and 1.4, as well as C2 ≡ C3
in Lemma 2 (see Eq. (12)), we have

hθ, µi = max
µ∈C3

max
µ∈MG
⇐⇒ MAP(θ) = LS-LP(θ).

hθ, µi = max
µ∈C2

hθ, µi

(16)

(17)

Combining with MAP(θ) = ILP(θ) ≤ LP(θ) (see Eq. (7)),
the proof is ﬁnished.

4.2 A General Form and KKT Conditions

For clarity, we ﬁrstly simplify the notations and formula-
tions in Eq. (11) to the general shape,

LS-LP(θ) = min
x,y

f (x) + h(y), s.t. Ax = By.

(18)

Our illustration for (18) is separated into three parts, as fol-
lows:
1. Variables. x = [µ1; . . . ; µ|V |] ∈ RPV

i |Xi|, and it con-
catenates all variable nodes µV . y = [y1; . . . ; y|V |] with
α |Xα|. y con-
yi = [υi; µαi,1 ; . . . ; µαi,|Ni |
catenates all factor nodes µV and the extra variable nodes
υ; yi concatenates the factor nodes and the extra vari-
able node connected to the i-th variable node µi. Ni in-
dicates the set of neighborhood factor nodes connected
to the i-th variable node; the subscript αi,j indicates the
j-th factor connected to the i-th variable, with i ∈ V and
j ∈ Ni.

] ∈ R|Xi|+PNi

2. Objective functions. f (x) = w⊤

x x with wx = −[θ1; . . . ;

1

|Nαi,1 | θαi,1 ; . . . ;

θ|V |]. h(y) = g(y) + w⊤
with wi = −[0;

y y, with wy = [w1; . . . ; w|V |]
1
| θαi,|Ni| ], and
|Nαi,|Ni |
Nα = {i | (i, α) ∈ E} being the set of neighbor-
hood variable nodes connected to the α-th factor. g(y) =
I(µα ∈ ∆|Xα|), with I(a) being
I(υ ∈ S) +
the indicator function: I(a) = 0 if a is true, otherwise
I(a) = ∞.

α∈F

P

Algorithm 1 The perturbed ADMM algorithm

Input: The initialization y0, ˆx0, λ0, the perturbation ǫ, the hyper-

parameter ρ

1: for k = 0 to K do:
2:

Update yk+1 as follows (see Section 5.1 for details)

yk+1 = argmin

y

L

ρ,ǫ(y, ˆxk, λk)

3:

Update ˆxk+1 as follows (Section 5.2 for details)

ˆxk+1 = argmin

ˆx

L

ρ,ǫ(yk+1, ˆx, λk)

4:

Update λk+1 (see Section 5.3 for details)

λk+1 = λk + ρ( ˆAˆxk+1

Byk+1)

−

Check stopping criterion, as shown in Section 5.4

5:
6: end for
7: return y∗, x∗

(21)

(22)

(23)

3. Constraint matrices. The constraint matrix A = diag(
A1, . . . , Ai, . . . , A|V |) with Ai = [I|Xi|; . . . ; I|Xi|] ∈
{0, 1}(|Ni|+1)|Xi|×|Xi|. B = diag(B1, . . . , Bi, . . . , B|V |),
with Bi = diag(I|Xi|, Mi,αi,1 , . . . , Mi,αi,|Ni |). A sum-
marizes all constraints on µV , while B collects all con-
straints on µF and υ.

Note that Problem (18) has a clear structure with two groups
of variables, corresponding the augmented factor graph (see
Fig. 1(c)).

Deﬁnition 2 The solution (x∗, y∗) of the LS-LP problem
(18) is said to be the KKT point if the following conditions
are satisﬁed:

B⊤λ∗ ∈ ∂h(y∗), ∇f (x∗) = −A⊤λ∗, Ax∗ = By∗, (19)

where λ∗ denotes the Lagrangian multiplier; ∂h indicates
the sub-gradient of h, while ∇f represents the gradient of
f . Moreover, (x∗, y∗) is considered as the ǫ-KKT point if
the following conditions hold:

dist(B⊤λ∗, ∂h(y∗)) ≤ O(ǫ), k∇f (x∗) + A⊤λ∗k ≤ O(ǫ),
kAx∗ − By∗k ≤ O(ǫ).
(20)

5 Perturbed ADMM Algorithm for LS-LP

We propose a perturbed ADMM algorithm to optimize the
following perturbed augmented Lagrangian function,

Lρ,ǫ(y, ˆx, λ) = ˆf (ˆx) + h(y) + λ⊤( ˆAˆx − By)

+

ρ
2

k ˆAˆx − Byk2
2,

(24)

where ˆA = [A, ǫI] with a sufﬁciently small constant ǫ > 0,
then ˆA is full row rank. ˆx = [x; ¯x], with ¯x = [¯x1; . . . ; ¯x|V |] ∈

6

Baoyuan Wu et al.

RPV
i (|Ni|+1)|Xi| and ¯xi = [µi; . . . ; µi] ∈ R(|Ni|+1)|Xi|.
ˆf (ˆx) = f (x) + 1
2 ǫˆx⊤ ˆx. Note that both ˆA and B are full
row rank, and the second-order gradient ∇2 ˆf (ˆx) = ǫI is
bounded. These properties will play key roles in our later
analysis of convergence.

Following the conventional ADMM algorithm, the so-
lution to the LS-LP problem (18) can be obtained through
optimizing the following sub-problems based on (24) itera-
tively. The general structure of the algorithm is summarized
in Algorithm 1.

5.1 Sub-Problem w.r.t. y in LS-LP Problem

Given ˆxk and λk, yk+1 can be updated by solving the sub-
problem (21) (see Algorithm 1). According to the deﬁni-
tions of ˆA, ˆx, B, y, this problem can be further separated
to the following two independent sub-problems, which can
be solved in parallel.

Update υk+1:

min
υ∈S Xi∈V (cid:2)

− (λk

i )⊤υi +

ρi
2

k (1 + ǫ)µk

i − υi k2
2

(25)

.
(cid:3)

5.2 Sub-Problem w.r.t. ˆx in LS-LP Problem

Given yk+1 and λk, ˆxk+1 can be updated by solving the
sub-problem (22) (see Algorithm 1). According to the deﬁ-
nition of ˆx, this problem can be separated to |V | independent
sub-problems w.r.t. {µi}i∈V , as follows:

(λk

i − θi)⊤µi +

min
µi

ǫ(|Ni| + 2)
2

µ⊤

i µi +

(28)

(cid:20)

Xα∈Ni

(1 + ǫ)(λk

iα)⊤µi +

ρiα
2

k (1 + ǫ)µi − Miαµk+1

α

k2
2 (cid:21)

+

ρi
2

k (1 + ǫ)µi − υi k2
2

= (1 + ǫ)

λk
iα − ρiαMiαµk+1

α

(cid:20) Xα∈Ni (cid:0)

⊤

+ λk
i (cid:21)

µi + µ⊤

i Qµi + const,

− ρiυk+1

i − θi

(cid:1)

(29)

2 [ǫ(|Ni| + 2) + ρi(1 + ǫ)2 +

where Q = 1
ρiα(1 +
ǫ)2] · I. The above sub-problem can be further simpliﬁed to
minµi a·µ⊤
i µi +b⊤µi, where a and b can be easily derived
from the above equation. Its close-form solution is obtained
by setting its gradient to 0, i.e., µk+1

α∈Ni

P

i = b
2a .

It has a closed form solution as follows

υk+1 = PS(υk+1),

5.3 Update λ in LS-LP Problem

(26)

Given yk+1 and ˆxk+1, λk+1 is updated using (23) (see Al-
gorithm 1). Similarly, it can be separately to |V | + |E| inde-
pendent sub-problems, as follows

1

; . . . ; υk+1

|V | ] with υk+1

where υk+1 = [υk+1
i = (1 + ǫ)µk
i +
i . PS(·) is the projection onto S: PS(a) = n1/2
λk
1
2 ×
ρi
a
+ 1
2 1n and n being the dimen-
kak2
sion of a. As demonstrated in [41], this projected solution is
the optimal solution to (25).

2 1n, with a = a − 1

λk+1
i = λk
λk+1
iα = λk

i + ρi[(1 + ǫ)µk+1
iα + ρiα[(1 + ǫ)µk+1

i − υk+1

i

],

i − Miαµk+1
α ],

(30)

(31)

where i ∈ V, (i, α) ∈ E.

Update µk+1
in parallel ∀α ∈ F ,

α : The sub-problems w.r.t. {µα}α∈F can be run

min
µα∈∆|Xα |

− θ⊤

α µα +

Xi∈Nα (cid:2)
iα)⊤Miαµα

− (λk

ρiα
2

.

(cid:3)

k (1 + ǫ)µk

i − Miαµα k2
2

(27)

It is easy to know that Problem (27) is convex, as M⊤
iαMiα
is positive semi-deﬁnite and ∆|Xα| is a convex set. Any off-
the-shelf QP solver can be adopted to solve (27). In exper-
iments, we adopt the active-set algorithm implemented by
a publicly-available toolbox called Quadratic Programming
in C (QPC)1, which is written in C and can be called from
MATLAB.

1 http://sigpromu.org/quadprog/download.php?sid=3wtwk5tb

5.4 Complexity and Implementation Details

Complexity.
In terms of computational complexity, as all
other update steps have simple closed-form solutions, the
main computational cost lies in updating µα, which is con-
vex quadratic programming with the probability simplex con-
straint. Its computational complexity is O(|Xα|3). As the
iαMiα ∈ R|Xα|×|Xα| in
matrix with the largest size is M⊤
α∈F (|Xα|)2). Both the
LS-LP, the space complexity is O(
computational and space complexity of AD3 are similar with
LS-LP. More detailed analysis about the computational com-
plexity will be presented in Section 7.5.

P

Implementation details. In each iteration, we use the same
value of ρ for all ρi and ρiα. After each iteration, we update
ρ using an incremental rate η, i.e., ρ ← ρ × η. A upper limit
ρupper of ρ is also set: if ρ is larger than ρupper, it is not up-
dated anymore. The perturbation ǫ is set to 10−5, and ρupper

MAP Inference via ℓ2-Sphere Linear Program Reformulation

7

can be set as any constant than 1
ǫ , such as 2 × 105. We utilize
two stopping criterion jointly, including: 1) the violation of
ρiα
the local consistency constraint, i.e., (
2 k (1 +
1
P
ǫ)µi−Miαµα k2
2 ; 2) the violation of the equivalence con-
2)
1
straint (1+ǫ)µi = υi, i.e., (
2 .
We set the same threshold 10−5 for both criterion. If this two
P
violations are lower than 10−5 simultaneously, then the al-
gorithm stops.

ρi
2 k (1+ǫ)µi −υi k2
2)

(i,α)∈E

i∈V

6 Convergence Analysis

The convergence property of the above ADMM algorithm
is demonstrated in Theorem 2. Due to the space limit, the
detailed proof will be presented in Appendix A.

Theorem 2 We suppose that ρ is set to be larger than a con-
stant, then the variable sequence {yk, ˆxk, λk} generated
by the perturbed ADMM algorithm globally converges to
(y∗, ˆx∗, λ∗), where (y∗, x∗) is the ǫ-KKT point to the LS-
LP problem (18), as deﬁned in Deﬁnition 2.

Furthermore, according to Deﬁnition 1, we assume that
Lρ,ǫ has the KL property at (y∗, x∗, λ∗) with the concave
function ϕ(s) = cs1−p, where p ∈ [0, 1), c > 0. Conse-
quently, we can obtain the following inequalities:

(i) If p = 0, then the perturbed ADMM algorithm will con-

verge in ﬁnite steps.

(ii) If p ∈ (0, 1

2 ], then we will obtain the ǫ-KKT solution to
steps, with
the LS-LP problem in at least O
τ ∈ (0, 1) being a small constant, which will be later
deﬁned in Appendix A.5.

( 1
ǫ )2

log 1
τ

(cid:1)

(cid:0)

(iii) If p ∈ ( 1

2 , 1), then we will obtain the ǫ-KKT solution to

the LS-LP problem in at least O

( 1
ǫ )
(cid:0)

4p−2
1−p

steps.

(cid:1)

Proof The general structure of the proof consists of the fol-
lowing steps, as follows:

1. The perturbed augmented Lagrangian function Lρ,ǫ (see
(24)) is monotonically decreasing along the optimiza-
tion.

2. The variable sequence {yk, ˆxk, λk} is bounded.
3. The sequence of variable residuals is converged, i.e., {k
yk+1 −ykk, kˆxk+1 − ˆxkk, kλk+1 − λkk} → 0, as k →
∞.

4. The variable sequence {yk, ˆxk, λk} globally converges

to the cluster point (y∗, ˆx∗, λ∗).

5. (y∗, x∗) is the ǫ-KKT point of the LS-LP problem (18).
6. We ﬁnally analyze the convergence rate that how many

steps are required to achieve the ǫ-KKT point.

Table 1: Benchmark datasets used in the Probabilistic Infer-
ence Challenge (PIC 2011) [7] and OpenGM 2 [14]. C1 to
C7 represent: number of models, average variables, average
factors, average edges, average factor sizes (i.e., the average
number of adjacent variables for each factor), average vari-
able states, average factor states.

dataset

Seg-2
Seg-21
Scene
Grids
Protein

C1
50
50
715
21
7

C2
229.14
229.14
182.56
3142.86
14324.7

C3
622.28
622.28
488.99
6236.19
21854.7

C4
1244.56
1244.56
977.98
12472.4
57680.4

C6 C7
C5
2
2
21
2
8
2
2
2
2.64 2

4
441
64
4
6.56

7 Experiments

7.1 Experimental Settings

7.1.1 Datasets

We evaluate on four benchmark datasets from the Proba-
bilistic Inference Challenge (PIC 2011) [7] and OpenGM 2
[14], including Segmentation [7] , Scene [10], Grids [7], and
Protein [11], as shown in Table 1. Segmentation consists of
Seg-2 and Seg-21, with different variable states. Protein in-
cludes higher-order potentials, while others include pairwise
potentials.

7.1.2 Compared Methods

We compare with different categories of MAP inference meth-
ods, including: 1) moving making methods, i.e., ICM [3];
2) message-passing methods, including belief propagation
(BP) [21] and TRBP [38]; 3) polyhedral methods (includ-
ing LP relaxation based methods), including dual decompo-
sition using sub-gradient (DD-SG) [15], TRWS [19], AD-
Sal [33], PSDD [20] and AD3 [27][28]. 4) We also com-
pare with LP-LP, which calls the the active-set method (im-
plemented by linprog in MATLAB) to optimize LP(θ). It
serves as a baseline to measure the performance of above
methods. 5) The most related work ℓ2-Box ADMM (i.e..,
the special case of ℓ2-Box ADMM with p = 2) algorithm
[41] is also compared. However, the presented algorithm
in [41] can only handle MRF models with pairwise poten-
tials, which is formulated as a binary quadratic program-
ming (BQP) problem. Thus, ℓ2-Box ADMM (hereafter we
call it ℓ2-Box for clarity) is not compared on Protein, of
which models include high-order potentials. 6) We also com-
pare with two hybrid methods, including method DAOOPT
(adopting branch-and-bound method [23] as a sub-routine)
[31][32] and MPLP-C [37] (adopting MPLP [9] as a sub-
routine). The ‘hybrid’ indicates that the method is a combi-
nation of an off-the-shelf single method and some heuristic

8

Baoyuan Wu et al.

steps. And we call above 5 types as non-hybrid methods.
Both the proposed LS-LP and ℓ2-Box are implemented by
MATLAB. The following methods are implemented by the
author provided C++ package, including: PSDD and AD32,
MPLP-C3, and DAOOPT4. All other methods are imple-
mented through the OpenGM 2 software [14], and we add a
preﬁx “ogm" before the method name, such as ogm-TRWS.
In experiments, we set some upper limits: the maximal
iteration as 2000 for PSDD and AD3, 500 for ℓ2-Box and
LS-LP, and 1000 for other methods; for DAOOPT, the mem-
ory limit of mini buckets is set as 4000 MB and the up-
per time limit as 2 hours. The parameter tuning of all com-
pared methods (except ℓ2-Box) is self-included in their im-
plementations. Both LS-LP and ℓ2-Box are ADMM algo-
rithms, and their hyper-parameters are tuned as follows: the
hyper-parameters ρ0, η and ρupper (see implementation de-
tails of Section 5) are adjusted in the ranges {0.05, 0.1, 1, 5,
10, 102, 103, 104}, {1.01, 1.03, 1.05, 1.1, 1.2} and {106, 108},
respectively, and those leading to the higher logPot value are
used.

7.1.3 Evaluation Metrics

We evaluate the performance of all compared methods us-
ing three types of metrics, including the log potential (log-
Pot) values, the solution type, as well as the computational
complexity and runtime.

Evaluation using logPot values. The logPot value indi-
cates the objective value hθ, µi of MAP(θ) (see Eq. (5)).
Given that the constraint MG of MAP(θ) is satisﬁed, the
larger logPot value indicates the better inference performance.
Since LP-LP gives the optimal solution to LP(θ) (see Eq.
(7)) constrained to LG, and we know that MG ⊆ LG, then
the logPot value of any valid label conﬁgurations cannot be
larger than that of LP-LP. Note that in the implementation
of OpenGM 2 [14], a rounding method is adopted as the
post-processing step to produce the integer solution for the
continuous MAP inference methods. However, the perfor-
mance of different MAP inference methods may be signif-
icantly changed by rounding. Thus, for other methods not
implemented by OpenGM 2, we report the logPot values of
original continuous solutions, without any rounding.

Evaluation using solution types. Since LP-LP, PSDD and
AD3 are possible to give continuous solutions, the larger
logPot value doesn’t always mean the better MAP inference
result. Thus, we also deﬁne four qualitative measures, in-
cluding valid, uniform, fractional and approximate, to intu-
itively measure the inference quality. Valid (V) means that
the solution is integer and satisﬁes the constraints in LG;
Uniform (U) denotes that the solution belongs to LG, but

2 http://www.cs.cmu.edu/∼ark/AD3/
3 https://github.com/opengm/MPLP
4 https://github.com/lotten/daoopt

Table 2: A brief illustration of four types of measures for in-
ference quality, on a toy graphical model with two connected
variable nodes.

Variable 1 Variable 2

Possible states

Inferred
Probability

0, 1

}

{
(1, 0)
(0.5, 0.5)
(0.2, 0.8)
(0.2, 0.8)

0, 1

}

{
(0, 1)
(0.5, 0.5)
(0.4, 0.6)
(0.4, 0.6)

Factor
00, 01, 10, 11

Measure

}

{
(0, 1, 0, 0)
(0.25, 0.25, 0.25, 0.25) Uniform
(0.08, 0.12, 0.32, 0.48)
(0.16, 0.3, 0.4, 0.14)

Valid

Fractional
Approximate

the value is uniform, such as (0.5, 0.5) for the variable node
with binary states. Fractional (F) indicates that the solution
belongs to LG, but its value is fractional, while not uniform;
Approximate (A) means that some constraints in LG are vi-
olated, and its solutions is integer or fractional. Note that it
makes sense to compare the logPot values for different solu-
tions, only when the solutions are valid. In contrast, it makes
no sense to compare the logPot values, if the solutions be-
long to the other three types of measures. To better illustrate
above measures, we present a brief example on a toy graph-
ical model, as shown in Table 2.

Evaluation using the computational complexity and prac-
tical runtime. The computational complexity and the prac-
tical runtime are also important performance measures for
MAP inference methods, as shown in Section 7.5.

7.2 Results on Segmentation and Scene

The average results on Seg-2, Seg-21 and Scene are shown
in Table 3. LP-LP gives valid solutions on all models, i.e.,
the best solutions. Except for PSDD, all other methods give
valid solutions, and their logPot values can not be higher
than those of LP-LP. The logPot values of ICM are the low-
est, and those of ogm-BP, ogm-TRBP are slightly lower than
the best logPot values, while other methods achieve the best
logPot values on most models. Only PSDD gives approxi-
mate solutions (i.e., the constraints in LG are not fully sat-
isﬁed) on some models, speciﬁcally, 5 models in Seg-2, 8
models in Seg-21 and 166 models in Scene. ogm-DD-SG
fails to give solutions on some models of these datasets, thus
we ignore it. Evaluations on these easy models only show
that the performance ranking is ogm-ICM < ogm-BP, ogm-
TRBP, ℓ2-Box < others.

7.3 Results on Grids

The results on Grids are shown in Table 4. For clarity, we use
the model indexes M1 to M21 to indicate the model name to
save space in this section. The corresponding model names
from M1 to M21 are grid20x20.f10.uai, grid20x20.f10.wrap.
uai, grid20x20.f15.uai, grid20x20.f15.wrap.uai, grid20x20.
f5.wrap.uai, grid40x40.f10.uai, grid40x40.f10.wrap.uai, grid

MAP Inference via ℓ2-Sphere Linear Program Reformulation

9

Table 3: LogPot values of MAP inference solutions on Seg-2, Seg-21 and Scene. Except of PSDD, all other methods give
valid solutions. The best result among valid solutions in each row is highlighted in bold. Please refer to Section 7.2 for
details.

Method type
Dataset

Seg-2

Seg-21

Scene

→
↓
mean
std
mean
std
mean
std

Baseline
LP-LP

Hybrid methods
DAOOPT MPLP-C

ogm-ICM ogm-BP

ogm-TRBP

Non-hybrid methods
ogm-TRWS

ogm-ADSal PSDD

-75.5
19.63
-324.89
58.12
866.66
109.34

-75.5
19.63
-325.34
58.14
866.66
109.34

-75.5
19.63
-324.89
58.12
866.66
109.36

-137.1
70.1
-393.37
74.47
864.27
109.64

-79
20.24
-330.37
58.54
866.49
109.22

-76.8
19.36
-328.92
58.57
866.51
109.2

-75.5
19.24
-324.89
56.97
866.66
109.19

-75.5
19.24
-324.89
56.97
866.66
109.19

-75.4
19.77
-325.1
58.16
866.65
109.34

AD3

-75.5
19.63
-324.89
58.12
866.66
109.34

ℓ2-Box

-76.5
20.3
-344.51
59.24
864.11
108.66

Proposed
LS-LP

-75.6
19.69
-324.89
58.12
866.66
109.34

Table 4: MAP inference results on Grids dataset. LP-LP, PSDD and AD3 produce uniform solutions on all models in Grids,
while all other methods give valid solutions. Here we only show the logPot of LP-LP as the upper bound of other meth-
ods. The best logPot among integer solutions in each row is highlighted in bold. The number with in circle indicates the
performance ranking of each method. Please refer to Section 7.3 for details.

Method type
Model

↓

M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
M11
M12
M13
M14
M15
M16
M17
M18
M19
M20
M21

→

Baseline
LP-LP

Hybrid methods

Non-hybrid methods

DAOOPT

MPLP-C

ogm-ICM

ogm-BP

ogm-TRBP

ogm-DD-SG

ogm-TRWS

ogm-ADSal

3736.7
3830.3
5605.1
5745.5
1915.2
15601.2
16291.5
23401.8
24437.3
3121.2
3231.6
7800.6
8078.5
62943
63993.1
94414.5
96243.6
12721.3
12875.6
31809.7
31996.9

3015.7 1
3051 1
4517.3 1
4563.2 1
1542.7 1
12662.9 2
13050.7 2
18952.45 1
19538 1
2689 1
2714.67 1
6401.15 1
6472.9 1
–
–
–
–
–
–
–
–

3015.7 1
3033.6 2
4517.3 1
4563.2 1
1542.7 1
12665.7 1
13054.8 1
18896.8 1
19427.5 2
2688.8 2
2714.52 2
6396 2
6469.7 2
45813.6 2
47444.4 2
69408.6 2
71730.8 2
10445.8 2
10674.1 2
22292.5 3
24032.4 2

2708.9 5
2567.9 7
4067.3 5
3837.12 7
1318.41 7
10753.7 6
10903.8 5
16154.2 6
16334.2 6
2255.38 6
2258.54 7
5356.28 6
5425.16 7
43538.9 4
42855 5
65081.2 4
63768.1 6
9062.03 5
9214.57 5
21527.9 6
21529.6 5

121.3 9
276.4 9
332.1 9
924.5 9
481.5 9
2793.5 9
1217.1 9
4314.9 10
3560.8 9
1665.3 8
1399.6 8
2033.5 9
1711.5 9
5690.9 9
4287.1 8
4374.2 9
13662.7 8
5198.7 7
5944.6 7
5410.9 8
4242.3 8

-235.2 10
19.2 10
14.02 10
-36.7 10
-47.8 10
2214.3 10
132.4 10
5371.1 9
-1111 10
1582.9 9
42.8 10
1953.1 10
381 10
6426.7 8
956.4 9
4656.5 8
-529.3 9
4975.4 8
1213.1 9
4762 9
47.6 9

1286.3 8
1484.7 8
1889.7 8
2023.4 8
807.6 8
5051.9 8
4634.8 8
7160 8
7187.3 8
1330.7 10
1285.9 9
2832.5 8
2814.3 8
18700.4 7
18811.9 7
27320.6 7
27287.7 7
4785.5 9
5328.5 8
9837.3 7
10423.8 7

2524.9 7
2674.4 5
3829.3 7
3894.6 6
1325.5 5
10500.8 7
10665 7
16014 7
16004.3 7
2215.7 7
2271.5 6
5282.5 7
5452.8 6
42274.2 6
42535 6
63148.1 6
63885.1 5
8793.5 6
8952.4 6
21546.8 5
21195.9 6

2605.2 6
2670.2 6
3884 6
4015 5
1323.9 6
11029 5
10870.4 6
16276.9 5
16508.1 5
2369.1 5
2370.1 5
5558.8 5
5646.1 5
43292.5 5
42918.7 4
64401.1 5
64487.9 4
9408.4 4
9385.1 4
22109.5 4
21730.3 4

ℓ2-Box

2794.8 4
2812.4 4
4301.1 4
4202.4 4
1427.4 4
11486.2 4
11867.6 4
17367.5 4
17990 4
2552.6 4
2556.8 4
5903 4
5923.5 4
44397.5 3
44759.5 3
66784.2 3
67589.4 3
10015.1 3
10163.6 3
22913.3 2
22668.7 3

Proposed
LS-LP

2931.8 3
2936.7 3
4408.9 3
4446.6 3
1503.2 3
12336.1 3
12537.2 3
18358.7 3
18785.8 3
2659.8 3
2654.9 3
6201.2 3
6275.4 3
48766.1 1
48657.3 1
72993.8 1
73486 1
10580.8 1
10698.4 1
24834.5 1
24532.8 1

40x40.f15.uai, grid40x40.f15.wrap.uai, grid40x40.f2.uai, grid
40x40.f2.wrap.uai, grid40x40.f5.uai, grid40x40.f5.wrap.uai,
grid80 x80.f10.uai, grid80x80.f10.wrap.uai, grid80x80.f15.uai,
grid 80x80.f15.wrap.uai, grid80x80.f2.uai, grid80x80.f2.wrap.
uai, grid80x80.f5.uai, grid80x80.f5.wrap.uai, respectively.

The models in Grids are much challenging for LP relax-
ation based methods, as all models have symmetric pairwise
log potentials and very dense cycles in the graph. In this
case, many vertices of LG are uniform solutions (0.5, 0.5).
Consequently, the LP relaxation based methods are likely to
produce uniform solutions. This is veriﬁed by that LP-LP,
AD3, PSDD give uniform solutions on all models in Grids,
i.e., most solutions are 0.5. Thus, we only show the logPot
values of LP-LP in Table 4, to provide the theoretical upper-
bound of logPot of valid solutions from other methods. In
contrast, the additional ℓ2-sphere constraint in LS-LP ex-
cludes the uniform solutions. On small scale models M1

to M13, DAOOPT and MPLP-C show the highest logPot
values, while LS-LP gives slightly lower values. On large
scale models M14 to M21, DAOOPT fails to give any result
within 2 hours. LS-LP gives the best results, while MPLP-
C shows slightly lower results. ℓ2-Box performs worse than
LS-LP, MPLP-C and DAOOPT on most models, while bet-
ter than all other methods, among which ogm-BP, ogm-TRBP
and ogm-DD-SG perform worst. These results demonstrate
that 1) LS-LP is comparable to hybrid methods DAOOPT
and MPLP-C, but with much lower computational cost (shown
in Section 7.5); 2) LS-LP performs much better than other
non-hybrid methods.

10

Baoyuan Wu et al.

Table 5: LogPot values of MAP inference solutions on Protein dataset. Except for PSDD and AD3, all other methods give
integer solutions. Both PSDD and AD3 produce mixed types of solutions on all models. The best result among valid solutions
in each row is highlighted in bold. The number with in circle indicates the performance ranking of each method. Please refer
to Section 7.4 for details.

Method type
Model

↓

M1
M2
M4
M5
M6
M7
M8

→

Hybrid methods
MPLP-C

ogm-ICM

ogm-BP

Non-hybrid methods
ogm-DD-SG

ogm-TRBP

-30181.3 2
-29305.4 2
-28952.1 2
-269567 3
-30070.6 2
-30288.3 2
-29336.5 2

-32409.9 5
-32561.3 5
-32570 5
-256489 1
-31699.1 5
-32562.2 5
-32617.2 5

-32019.1 4
-30966.1 3
-31031.4 3
-382766 5
-30765.2 3
-31659.6 3
-31064.7 3

-31671.6 3
-31253.3 4
-31176.6 4
-357330 4
-30772.2 4
-31791.1 4
-31219.9 4

-33381.2 6
-33583.6 6
-33747.7 6
-553376 6
-32952.9 6
-33620.4 6
-34549.9 6

PSDD

-30128.8
-29307.3
-28952.5
-66132.3
-30063.6
-30248.5
-29331

AD3

-30143.6
-29302.6
-28952
-115562
-30062.2
-30239.8
-29336.1

Proposed
LS-LP

-30165.5 1
-29295.4 1
-28952 1
-267814 2
-30063.4 1
-30266 1
-29334.7 1

7.4 Results on Protein

Table 6: Computational complexities of all compared meth-
ods. Excluding E, the deﬁnitions of all other notations can
be found in Section 3. E denotes the edge set of the origi-
nal MRF graph, while E indicates the edge set of the corre-
sponding factor graph. T represents the number of iterations.

The results on Protein are shown in Table 5. Different with
above three datasets, Protein includes 8 large scale models,
and with high-order factors. Similarly, we use the model in-
dexes M1 to M8 to indicate the model name to save space in
this section. The corresponding model names of M1 to M8
are didNotconverge1.uai, didNotconverge2.uai, didNotcon-
verge4.uai, didNotconverge5.uai, didNotconverge6.uai, did-
Notconverge7.uai, didNotconverge8.uai, respectively. As M1
and M3 are the same model, we remove M3 in experiments.
DAOOPT fails to give solutions within 2 hours on all mod-
els, and LP-LP cannot produce solutions due to the memory
limit. ogm-TRWS and ℓ2-Box are not evaluated as it cannot
handle high-order factors. LS-LP produces valid integer so-
lutions on all models, and gives the highest logPot values on
all models except of M5. MPLP-C gives slightly lower log-
Pot values than LS-LP. AD3 only produces a fractional solu-
tion on M4, while produces approximate and fractional so-
lutions on other models, while PSDD gives approximate and
fractional solutions on all models. Speciﬁcally, the solution
types of AD3 on M1 to M8 are: A+ 2.01%U + 21.52F ; A+
0.55%U + 0.66F ; 0.17%U ; A + 9.51%U + 32.44%F ; A +
0.74%U +13.7%F ; A+3.41%U +17.16%F ; A+ 0.38%U +
0.13%F . Those of PSDD are: A+6.96%U +11.75%F ; A+
1.47%U +3.19%F ; A+0.73%U +2.06%F ; A+13.72%U +
19.67%F ; A+4.35%U +7.9%F ; A+6.28%U +10.4%F ; A+
0.68%U + 1.89%F . Other methods also show much worse
performance than LS-LP and MPLP-C. One exception is
that ogm-ICM gives the best results on M5, and we ﬁnd that
M5 is the most challenging model for approximated meth-
ods.

i
|X

+

)(cid:1)

j
|X

|

|

(i,j)(
(cid:3)(cid:1)

|E|

α |X

α

|

+ PF
α (

α

|N

| −

Methods

MPLP
MPLP-C
ogm-ICM
ogm-BP
ogm-TRBP

ogm-TRWS
ogm-ADSal
PSDD
AD3

ℓ2-Box
LS-LP

+ 2 PE

|

2

i
· |X

i
i |N

Complexities
O(cid:0) PV
|
O(cid:0)Touter(cid:2)TinnerO(MPLP) +
O(cid:0)T [PV
O(cid:0)T (cid:2) PV
Nα
1) P
i
O(cid:0)T (cid:2)

i
i |X
|
i (
i
|N
(cid:3)(cid:1)
i
|
|X
maxi∈V

| −

(cid:3)(cid:1)

](cid:1)

1) PNi

i
|X
|
]+PF

|E| ·
O(cid:0)T (cid:2) PV
i [
(cid:3)(cid:1)
α
|X
|
O(cid:0)T [PV
O(cid:0)T (cid:2) PV

i
|N

i
|·|X

|

α

α |X

3 +PE

i
(i,α) |X

|

|·

]3(cid:1)
+PF

i
i |X
|
i
i |X

|

α

α |X

3+PE

i
(i,α) |X

|

α

|·|X

(cid:3)(cid:1)

|

7.5 Comparisons on Computational Complexities and
Practical Runtime

Computational complexities of all compared methods (ex-
cept of LP-LP and DAOOPT) are summarized in Table 6.
As there is no clear conclusion of the complexity of the
active-set algorithm for linear programming, the complexity
of LP-LP is not presented. DAOOPT [32] is combination of
6 sequential sub-algorithms and heuristic steps, thus its com-
putational complexity cannot be computed. As these com-
plexities depend on the graph structure (i.e., V, E, E, F, Ni,
Nα, Xi, Xα), it is impossible to give a ﬁxed ranking of them.
However, it is notable that the complexity of LS-LP is lin-
ear w.r.t. the number of variables, factors and edges (i.e.,
|V |, |F |, |E|). Moreover, as the sub-problems w.r.t. variables/
factors/edges in LS-LP are independent, they can be solved
in parallel. However, the complexity of LS-LP is super-linear
w.r.t. the state space |Xα| and |Xi|. Thus, the proposed LS-
LP method is suitable for graphical models with very large-

MAP Inference via ℓ2-Sphere Linear Program Reformulation

11

Table 7: Iterations and practical runtime on Seg-2, Seg-21 and Scene.

Datasets

iters

runtime

iters

runtime

iters

runtime

mean
std
mean
std

mean
std
mean
std

mean
std
mean
std

Seg-2

Seg-21

Scene

LP-LP

DAOOPT

MPLP-C

ogm-ICM ogm-BP

ogm-TRBP

ogm-TRWS

ogm-ADSal

9.4
1.03
0.096
0.01

16.13
1.55
45.71
9.34

12.56
1.04
1.907
0.266

–
–
0.54
0.504

–
–
1311.8
1593.2

–
–
82.89
18.46

1.3
0.463
0.033
0.054

1.46
0.646
0.745
1.1

1.32
0.48
0.072
0.111

2.12
0.32
0.007
0.001

76.9
82.2
0.185
0.062

319.5
58.89
0.081
0.011

1000
0
36.51
0.61

1000
0
1612
25.05

1000
0
229.67
15.46

1000
0
40.73
0.67

1000
0
1891.6
25.7

1000
0
265.45
17.6

18.74
15.67
0.191
0.167

23.4
20.53
16.33
12.89

15.03
12.21
1.7
1.3

16.96
8.55
0.7828
0.6373

17.58
11.95
57.01
44.51

10.94
11.15
5.03
5.88

PSDD

632.5
618.2
0.032
0.027

783
672
1.14
1.1

791
766
0.066
0.071

AD3

70.12
76.79
0.001
0.006

73.26
45.09
0.228
0.114

71.59
82.83
0.029
0.024

ℓ2-Box

LS-LP

78.84
100.6
0.849
1.044

39.4
68.1
8.643
14.71

43.03
37.47
0.777
0.681

84.14
49.58
12.53
7.39

78.63
34.65
342
147

75.36
52.47
29.23
20.36

Table 8: Iterations and practical runtime on Grids.

Models

DAOOPT MPLP-C

ogm-ICM ogm-BP

ogm-TRBP

ogm-DD-SG

ogm-TRWS ogm-ADSal ℓ2-Box

LS-LP

M1

M2

M3

M4

M5

M6

M7

M8

M9

M10

M11

M12

M13

M14

M15

M16

M17

M18

M19

M20

M21

iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime

–
8
–
10
–
8
–
10
–
11
–
7200
–
7200
–
7200
–
7200
–
7200
–
7200
–
7200
–
7200
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–

420
303.9
1000
613.3
686
331.5
1000
523.5
1000
468.7
1000
840.3
1000
1040.5
1000
917.1
1000
1096.8
521
273.9
1000
843.1
1000
824.8
1000
999.7
1000
1956.4
1000
2028.9
1000
2110.4
1000
2238.8
1000
1956.3
1000
1953.5
1000
1974.2
1000
2167.3

195
0.012
192
0.013
195
0.022
193
0.013
193
0.013
741
0.052
750
0.053
730
0.072
739
0.053
738
0.072
765
0.053
727
0.073
755
0.052
3029
0.218
2991
0.218
3038
0.217
3048
0.221
3173
0.221
3095
0.221
3038
0.206
2985
0.218

1000
40.1
1000
42.71
1000
40.2
1000
42.72
1000
42.77
1000
166.6
1000
171.6
1000
166.6
1000
171.7
1000
166.6
1000
171.7
1000
166.7
1000
171.8
1000
676.5
1000
687.2
1000
676.8
1000
686.8
1000
677
1000
687.4
1000
676.4
1000
687.2

1000
40.89
1000
42.13
1000
39.28
1000
45.53
1000
48.07
1000
182.3
1000
187.2
1000
182.1
1000
189.5
1000
182.1
1000
187.7
1000
187.6
1000
188.4
1000
756
1000
770.2
1000
762.2
1000
767.6
1000
756.1
1000
773
1000
756
1000
767.5

1000
86.9
1000
91.1
1000
86.4
1000
91.5
1000
91.2
1000
362.3
1000
371.7
1000
362.4
1000
372.3
1000
362.4
1000
372.2
1000
361.5
1000
372.3
1000
1474
1000
1495
1000
1471
1000
1492.2
1000
1475.2
1000
1493.8
1000
1473.2
1000
1494.4

14
0.13
18
0.18
13
0.12
13
0.17
21
0.02
19
0.78
18
0.7
17
0.68
16
0.85
45
2.14
36
1.57
26
1.05
34
1.44
21
3.52
19
3.37
15
2.58
20
3.37
54
8.87
109
19.13
30
4.99
22
3.75

39
2.79
37
2.72
36
2.52
42
3.16
37
2.73
37
11.1
37
11.4
37
10.9
36
10.6
38
10.6
44
12.6
38
11
41
11.9
37
43.5
36
42
35
41.5
37
43
53
63.7
45
50.7
38
42.5
37
40.6

22
0.317
35
0.478
36
0.412
27
0.318
26
0.291
18
2.19
16
1.96
29
3.47
31
3.80
45
5.68
39
5.13
19
2.25
17
2.14
12
21.72
12
21.65
20
36.53
15
26.93
35
60.28
37
63.93
13
23.06
12
21.54

234
41.4
227
39.4
155
27
434
75.3
265
45.7
271
46.5
204
35.3
407
66.1
327
56.3
500
88.3
390
68.4
314
52.5
493
85.9
290
50.5
202
33.7
435
76
354
64
250
43.5
250
39.9
316
55.4
280
49.6

12

Baoyuan Wu et al.

Table 9: Iterations and practical runtime on Protein.

Models

MPLP-C

ogm-ICM ogm-BP

ogm-TRBP

ogm-DD-SG PSDD

AD3

M1

M2

M4

M5

M6

M7

M8

iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime
iters
runtime

499
2320
478
2399
24
80
500
2248
433
2427
475
2399
16
80

797
0.39
634
0.44
859
0.51
5275
0.8
597
0.36
836
0.43
778
0.51

1000
32019
1000
30966
1000
31031
1000
382766
1000
30765
1000
31660
1000
31065

1000
31671
1000
31253
1000
31177
1000
357330
1000
30772
1000
31791
1000
31220

1000
33381
1000
33583
1000
33748
1000
553376
1000
32953
1000
33620
1000
34550

2000
14.14
2000
7.74
2000
5.73
2000
22.62
2000
12.96
2000
13.55
2000
5.13

2000
20.28
2000
3.87
1524
3.32
2000
28.36
2000
14.12
2000
18.63
2000
3.31

LS-LP

161
1746
98
1076
140
1535
1000
10787
482
5314
145
1989.7
186
1566

scale variables and dense connections, but with modest state
spaces for variables and factors.

Practical runtime. Due to the dependency of the computa-
tional complexity on the graph structure, the practical run-
time of these methods will vary signiﬁcantly on different
graphs. In the following, we present the practical runtime of
all compared methods on above evaluated datasets. To test
the runtime fairly, we run all methods at the same machine,
and only run one experiment at the same time. The itera-
tions and practical runtime on different datasets are shown
respectively in Table 7 for Segmentation and Scene, Table 8
for Grids and Table 9 for Protein.

As shown in Table 7, on the small and easy models, the
runtime of LP-LP, MPLP-C, ogm-ICM, PSDD and AD3 are
very small, and the runtime of ogm-BP, ogm-TRBP, ogm-
TRWS, ogm-ADSal, LS-LP and ℓ2-Box are larger, while the
runtime of DAOOPT are the largest. Besides, the iterations
of AD3 and LS-LP are much smaller than the one of PSDD,
given the fact that their similar computational complexities
per iteration. The iterations of LP-LP and MPLP-C are also
provided, but they are incomparable with PSDD, AD3 and
LS-LP. The complexity of each iteration in LP-LP depends
on the problem size |µ| (see Eq. (7)). In MPLP-C, each outer
iteration includes 100 iterations of MPLP and adding vio-
lated constraints. The complexity of each MPLP iteration
is stable, but the complexity of adding violated constraints
varies signiﬁcantly in different outer iterations.

In terms of the comparison on Grids (see Table 8), the
iterations and runtime of LP-LP are smaller than those of
other methods, but it gives uniform solutions on all mod-
els. As demonstrated Section 7.2, LP-LP, PSDD and AD3
produce uniform solutions on all models, thus we also don’t
present their iterations and runtime. The runtime of DAOOPT
on small models (i.e., M1 to M5) are small, and it achieves
7200 seconds (the upper limit) on M6 to M13, while it can-
not give any solution in 7200 seconds on M14 to M21. For

MPLP-C, both iterations and runtime are large on all mod-
els. Note that the runtime per iteration of MPLP-C becomes
larger along with the model scale. For ICM, the iteration
is large on multiple models, but with very small runtime,
as its complexity per iteration is low. Both message passing
methods, including ogm-BP and ogm-TRBP, achieve the up-
per limit of iterations. It demonstrate that their convergence
is very slow. The convergence of ogm-DD-SG is also slow,
and its runtime per iteration is even higher than above two
message passing methods. Two LP relaxation based meth-
ods, including ogm-TRWS and ogm-ADSal, converge in a
few iterations, and are of small runtime. In contrast, the it-
erations of LS-LP are only larger than those of ogm-TRWS,
ogm-ADSal and ℓ2-Box, while smaller than other methods.
And, the runtime per iteration of LS-LP is similar with that
of ogm-BP, while smaller than those of other methods ex-
cept of ogm-ICM and ogm-TRWS. Considering that C++ is
much more efﬁcient than MATLAB, if LS-LP is also im-
plemented by C++, its runtime should be much lower than
those of most compared methods. In other words, the com-
putational complexity of LS-LP is much smaller than most
compared methods. Note that the runtime per iteration of
ℓ2-Box increases along with the model size. For example,
its runtime per iteration on M1 is 0.014 seconds, while that
on M21 is 1.795 seconds. In contrast, the runtime per iter-
ation of LS-LP are rather stable. As shown in Table 6, the
. Ob-
complexity per iteration of ℓ2-Box is O
viously, ℓ2-Box is difﬁcult to apply to large-scale models.
(cid:1)
In contrast, LS-LP is conducted based on the decomposi-
tion of the factor graph to independent factors and variables,
due to which the parallel computation is allowed. Thus, the
scalability of LS-LP is much better than ℓ2-Box for MAP
inference.

V
i
P

|Xi|]3

(cid:0)

[

In terms of the comparison on Protein (see Table 9), the
ascending ranking of runtime is ogm-ICM, PSDD, AD3,
MPLP-C, LS-LP, ogm-BP, ogm-TRBP, ogm-DD-SG. Although

MAP Inference via ℓ2-Sphere Linear Program Reformulation

13

ing from the standard linear programming (LP) relaxation,
we added the ℓ2-sphere constraint onto variable nodes. The
intersection between the ℓ2-sphere constraint and the local
marginal polytope LG in LP relaxation is proved to be the
exact set of all valid integer label conﬁgurations. Thus, the
proposed LS-LP problem is equivalent to the original MAP
inference problem. By adding a sufﬁciently small perturba-
tion ǫ onto the objective function and constraints, we pro-
posed a perturbed ADMM algorithm for optimizing the LS-
LP problem. Although the ℓ2-sphere constraint is non-convex,
we proved that the ADMM algorithm will globally converge
to the ǫ-KKT point of the LS-LP problem. The analysis of
convergence rate is also presented. Experiments on three
benchmark datasets show the competitive performance of
LS-LP compared to state-of-the-art MAP inference meth-
ods.

the runtime of PSDD and AD3 are very small, but they only
give approximate solutions in 2000 iterations on all models,
except for AD3 on M4. For MPLP-C, the iterations and run-
time are small on M4 and M8, but very large on all other 5
models. In contrast, although the runtime of LS-LP is much
larger than those of PSDD and AD3, its iterations are much
smaller on all models. If LS-LP is also implemented by C++,
its practical runtime will be much smaller than those of PSDD
and AD3.

In summary, the above comparisons on iterations and

practical runtime demonstrate:

1. LS-LP converges much faster than most compared meth-
ods, except of ogm-TRWS, ogm-ADSal and ℓ2-Box. On
large-scale models (see Table 5), the complexities per it-
eration of LS-LP, PSDD and AD3 are similar, and are
much smaller than those of other methods (except of
ICM).

2. The complexity of MPLP-C is larger than PSDD, AD3
and LS-LP, while smaller than DAOOPT. But its practi-
cal iterations and runtime vary signiﬁcantly on different
models. Both the complexity and practical runtime of
DAOOPT are much larger than other methods.

3. Considering the performance of the MAP inference re-
sults presented in Section 7.2, 7.3 and 7.4, we conclude
that LS-LP shows very competitive performance com-
pared to state-of-the-art MAP inference methods.

7.6 Discussions

We obtain three conclusions from above experiments eval-
uated on different types of models. 1) Compared with the
hybrid methods including DAOOPT and MPLP-C, the per-
formance of LS-LP is comparable. However, the computa-
tional cost of LS-LP is much lower, as the hybrid methods
adopt LP relaxation based methods as sub-routines. 2) Com-
pared with LP relaxation based methods, especially PSDD
and AD3, LS-LP always give valid solutions, without round-
ing; and, the logPot values of LS-LP are much higher, with
similar computational cost. 3) Compared to ℓ2-Box, which
can be only applied to models with pairwise potentials, LS-
LP is applied to any type of models. Besides, the decom-
position of the factor graph allows for the parallel computa-
tions with respect to each factor and each variable, while ℓ2-
Box solves a QP problem over the whole MRF model. Thus,
LS-LP is a much better choice than ℓ2-Box for MAP infer-
ence. 4) Compared to other approximated methods, LS-LP
always shows much better performance in difﬁcult models
(e.g., Grids and Protein).

8 Conclusions

In this work, we proposed an novel formulation of MAP
inference, called ℓ2-sphere linear program (LS-LP). Start-

14

A Convergence Analysis

Baoyuan Wu et al.

To facilitate the convergence analysis, here we rewrite some equations and notations ﬁrstly deﬁned in Sections 5. Problem (11) can be simpliﬁed to the
following general shape, as follows

LS-LP(θ) = min
x,y

f (x) + h(y), s.t. Ax = By.

Our illustration for (18) is separated into three parts, as follows:

(32)

1. Variables. x = [µ1; . . . ; µ|V |]

∈
Ni
α |Xα|. y concatenates all factor nodes µV and the extra variable nodes υ; yi concatenates the factor nodes and the extra variable node
i indicates the set of neighborhood factor nodes connected to the i-th variable node; the subscript αi,j

R|Xi|+P
connected to the i-th variable node µi.
indicates the j-th factor connected to the i-th variable, with i

i |Xi|, and it concatenates all variable nodes µV . y = [y1; . . . ; y|V |] with yi = [υi; µαi,1

; . . . ; µαi,|Ni|

N

∈

]

RPV

V and j
[θ1; . . . ; θ|V |]. h(y) = g(y)+w⊤

∈ N

∈

i.

being the set of neighborhood variable nodes connected to the α-th factor. g(y) = I(υ

y y, with wy = [w1; . . . ; w|V |] with wi =

[0;

1

|Nαi,1 | θαi,1 ;

−

2. Objective functions. f (x) = w⊤

x x with wx =

−
(i, α)

α =

N

i
{

|

E

}

∈

∈

∆|Xα|), with I(a) being the indicator function: I(a) = 0 if a is true, otherwise I(a) =

.

∞

P

3. Constraint matrices. The constraint matrix A = diag( A1, . . . , Ai, . . . , A|V |) with Ai = [I|Xi|; . . . ; I|Xi|]

(|Ni|+1)|Xi|×|Xi|.
}
B = diag(B1, . . . , Bi, . . . , B|V |), with Bi = diag(I|Xi|, Mi,αi,1 , . . . , Mi,αi,|Ni| ). A summarizes all constraints on µV , while B collects
all constraints on µF and υ.

∈ {

0, 1

. . . ;

) +

S

1
|Nαi,|Ni|
α∈F

| θαi,|Ni| ], and
I(µα ∈

Note that Problem (18) has a clear structure with two groups of variables, corresponding the augmented factor graph (see Fig. 1(c)).

According to the analysis presented in [40], a sufﬁcient condition to ensure the global convergence of the ADMM algorithm for the problem LS-LP(θ)
is that Im(B)
Im(A), with Im(A) being the image of A, i.e., the column space of A. However, A in (32) is full column rank, rather than full row rank,
while B is full row rank. To satisfy this sufﬁcient condition, we introduce a sufﬁciently small perturbation to both the objective function and the constraint
in (32), as follows

⊆

LS-LP(θ; ǫ) = min
ˆx,y

ˆf (ˆx) + h(y), s.t. ˆAˆx = By,

(33)

where ˆA = [A, ǫI] with a sufﬁciently small constant ǫ > 0, then ˆA is full row rank. ˆx = [x; ¯x], with ¯x = [¯x1; . . . ; ¯x|V |]
¯xi = [µi; . . . ; µi]
the sufﬁcient condition Im(B)

R(|Ni|+1)|Xi|. ˆf (ˆx) = f (x) + 1
Im( ˆA) holds.

2 ǫˆx⊤ ˆx. Consequently, Im( ˆA)

i (|Ni|+1)|Xi| and
∈
Rrank of ˆA, as both ˆA and B are full row rank. Then,

Im(B)

⊆

≡

∈

RPV

The augmented Lagrangian function of (33) is formulated as

⊆

ρ,ǫ(ˆx, y, λ) = ˆf (ˆx) + h(y) + λ⊤( ˆAˆx

L

By) +

−

ρ
2 k

ˆAˆx

By

2
2
k

−

The updates of the ADMM algorithm to optimize (33) are as follows

yk+1 = argminy L
ˆxk+1 = argminˆx L
λk+1 = λk + ρ( ˆAˆxk+1

ρ,ǫ(ˆxk, y, λk),
ρ,ǫ(ˆx, yk+1, λk),
Byk+1).

−





The optimality conditions of the variable sequence (yk+1, ˆxk+1, λk+1) generated above are

B⊤λk + ρB⊤( ˆAˆxk

Byk+1) = B⊤λk+1

ˆf (ˆxk+1) + ˆA⊤λk + ρ ˆA⊤( ˆAˆxk+1

−

−
Byk+1) =

ρB⊤ ˆA(ˆxk+1

∂h(yk+1),
ˆxk)
ˆf (ˆxk+1) + ˆA⊤λk+1 = 0,

−

∈

−

∇

∇
1
ρ

(λk+1

λk) = ˆAˆxk+1

Byk+1.

−

−

(34)

(35)

(36)

(37)

(38)

The convergence of this perturbed ADMM algorithm for the LS-LP problem is summarized in Theorem 2. The detailed proof is presented in the
2 represents
1
A
2 means the Hessian operator, and ∂ is the

indicates the ℓ2 norm for a vector, or the Frobenius norm for a matrix;

following sub-sections sequentially. Note that hereafter
that
2 being square matrices;
2 is positive semi-deﬁnite, with
sub-gradient operator; I represents the identity matrix with compatible shape.

denotes the gradient operator,

(cid:23) A

− A

k · k

1
A

∇

∇

1,

A

A

A.1 Properties

In this section, we present some important properties of the objective function and constraints in (33), which will be used in the followed convergence
analysis.

Properties on objective functions (P1)

ρ,ǫ are semi-algebraic, lower semi-continuous functions and satisfy Kurdyka-Lojasiewicz (KL) property, and h is closed and proper

– (P1.1) f, h and
L
– (P1.2) There exist
Q
– (P1.3) lim inf kˆxk→∞ k∇
Properties on constraint matrices (P2)

2 such that
ˆf (ˆx)
=
k

Q

1,

2 ˆf (ˆx)

2,

ˆx

∀

(cid:23) Q

(cid:23) ∇

1

Q
∞

– (P2.1) There exists σ > 0 such that ˆA ˆA⊤

σI

(cid:23)

MAP Inference via ℓ2-Sphere Linear Program Reformulation

15

2 + ρ ˆA⊤ ˆA

– (P2.2)
Q
ˆx, and δI
– (P2.3) There exists
≻
(cid:23)
– (P2.4) Both ˆA and B are full row rank, and Im( ˆA)

δI for some ρ, δ > 0, and ρ > 1
ǫ

2 ˆf (ˆx)]2,

2
σρ Q
Im(B)

(cid:23)
3

∇

Q

∀

3

[

Rrank of ˆA

≡

⊆

Hx

1,
k

Remark. (1) Although the deﬁnition of KL property (see Deﬁnition 1) is somewhat complex, but it holds for many widely used functions, according to
[42]. Typical functions satisfying KL property includes: a) real analytic functions, and any polynomial function such as
belongs to this type; b)
locally strongly convex functions, such as the logistic loss function log(1 + exp(
−
b
). It is easy to verify that P1.1 holds in our problem. (2) Here we provide an instantiation of
ǫ2I,
above hyper-parameters satisfying above properties. Firstly, it is easy to obtain that
as well as ρ ˆA⊤ ˆA
ǫI, when ǫ is small enough and ρ > 1
3 = ǫ2I, δ = 2ǫ, σ = ǫ2 satisfy P1.2,
P2.1, P2.2 and P2.3. Without loss of generality, we will adopt these speciﬁc values for these hyper-parameters to simplify the following analysis, while only
keeping ρ and ǫ.

k
2 ˆf (ˆx) = ǫI, and ˆA ˆA⊤ = [A, ǫI][A, ǫI]⊤ = AA⊤ + ǫ2I

Hx
x)); c) semi-algebraic functions, such as

∞ and the indicator function I(
·
k

∇
ǫ ). Then, the values

ǫ (e.g., ρ = 2

2 = ǫI,

∞,
k

k
1,
k

−
x
k

2,
k

2,
k

1 =

Hx

Hx

Q

Q

Q

−

(cid:23)

−

≻

−

b

b

b

x

x

k

k

k

k

k

A.2 Decreasing of Lρ,ǫ(yk, ˆxk, λk)

In this section, we ﬁrstly prove the decreasing property of the augmented Lagrangian function, i.e.,

ρ,ǫ(yk, ˆxk, λk) >

L

L

ρ,ǫ(yk+1, ˆxk+1, λk+1),

k.

∀

Firstly, utilizing P2.1, P2.3 and (37), we obtain that

ǫ2

λk+1
k

λk

2
2 ≤ k
k

ˆA(λk+1

λk)
2
2 =
k

−

k∇

ˆf (ˆxk+1)

ˆf (ˆxk)
2 = ǫ2
2
k

ˆxk+1
k

ˆxk

2
2.
k

−

− ∇

−
Then, we have

ρ,ǫ(yk+1, ˆxk+1, λk+1)

L
1
ρ k

=

λk+1

λk

2
2 ≤
k

−

− L
ˆxk+1

1
ρ k

ˆxk

2
2
k

−

ρ,ǫ(yk+1, ˆxk+1, λk) = (λk+1

λk)⊤( ˆAˆxk+1

Byk+1)

−

−

According to P1.2 and P2.2,

L

ρ,ǫ(yk+1, ˆx, λk) is strongly convex with respect to ˆx, with the parameter of at least 2ǫ. Then, we have

ρ,ǫ(yk+1, ˆxk+1, λk)

ρ,ǫ(yk+1, ˆxk, λk)

ǫ

ˆxk+1
k

ˆxk

2
2.
k

−

≤ −

− L

As yk+1 is the minimal solution of

ρ,ǫ(y, ˆxk, λk), it is easy to know

L
ρ,ǫ(yk, ˆxk, λk)

0.

ρ,ǫ(yk+1, ˆxk, λk)

− L
Combining (41), (42) and (43), we have

≤

ρ,ǫ(yk+1, ˆxk+1, λk+1)

ρ,ǫ(yk, ˆxk, λk)

− L

(

1
ρ −

≤

ǫ)

ˆxk+1
k

ˆxk

2
2 < 0,
k

−

L

L

L

where the last inequality utilizes P2.3 and ρ > 1
ǫ .

A.3 Boundedness of {yk, ˆxk, λk}

Next, we prove the boundedness of

yk, ˆxk, λk
{

ˆf (ˆx)

inf
ˆx

1

−

2ǫ2γ k∇

ˆf (ˆx)
2
2
k

= f ∗ >

.

−∞

(cid:0)

According to (44), for any k

(cid:1)
1, we have

≥

. We suppose that ρ is large enough such that there is 0 < γ < ρ with

}

ρ,ǫ(yk, ˆxk, λk) = ˆf (ˆxk) + h(yk) +

L

ρ
2 k

ˆAˆxk

−

Byk +

λk
2
2 −
ρ k

1
2ρ k

λk

2
2 ≤ L
k

ρ,ǫ(y1, ˆx1, λ1) <

.

∞

Besides, according to P2.1, we have

ǫ2

λk
k

ˆA⊤λk

2
2 ≤ k
k

ˆf (ˆxk)
2
2.
k
Plugging (47) into (46), we obtain that

2
2 =
k

k∇

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

> ˆf (ˆxk) + h(yk) +

∞

ρ
2 k

ˆAˆxk

−

Byk +

λk
2
2 −
ρ k

1

2ǫ2ρ k∇

ˆf (ˆxk)
2
2 ≥
k

f ∗ +

1
1
ρ
γ −
2ǫ2 k∇

ˆf (ˆxk)
2 + h(yk) +
2
k

ρ
2 k

ˆAˆxk

−

Byk +

λk
2
2.
ρ k

(48)

According to the coerciveness of
of
P2.4), the boundedness of

∇
. Besides, according to P2.4,

λk
{

ˆAˆxk
{
is proved.

}

}

yk
{

}

ˆf (ˆxk) (i.e., P1.3), we obtain that ˆxk <

,

k, i.e., the boundedness of

is also bounded. From (38), we obtain the boundedness of

∞

∀

. From (47), we know the boundedness
}
. Considering the full row rank of B (i.e.,

ˆxk
{
Byk
{

}

16

A.4 Convergence of Residual

According to the boundedness of
lower semi-continuity of

yk, ˆxk, λk
{

}

ρ,ǫ (i.e., P1.1), we have

, there is a sub-sequence

L

Baoyuan Wu et al.

yki , ˆxki , λki
{

}

that converges to a cluster point

y∗, ˆx∗, λ∗
{

. Considering the

}

lim inf
i→∞ L

ρ,ǫ(yki , ˆxki , λki )

≥ L

ρ,ǫ(y∗, ˆx∗, λ∗) >

.

−∞

Summing (44) from k = M, . . . , N

1 with M

−

≥

1, we have

ρ,ǫ(yN , ˆxN , λN )

L

− L

ρ,ǫ(yM , ˆxM , λM )

(

1
ρ −

ǫ)

≤

N−1

Xk=M

ˆxk+1
k

ˆxk

2
2 < 0.
k

−

Then, by setting N = ki and M = 1, we have

ρ,ǫ(yki , ˆxki , λki )

ρ,ǫ(y1, ˆx1, λ1)

L

− L

Xk=1
Taking limit on both sides of the above inequality, we obtain

(

1
ρ −

≤

ki−1

ǫ)

ˆxk+1
k

ˆxk

2
2.
k

−

− ∞

<

L

ρ,ǫ(y∗, ˆx∗, λ∗)

− L

ρ,ǫ(y1, ˆx1, λ1)

(

1
ρ −

ǫ)

≤

∞

Xk=1

ˆxk+1
k

ˆxk

2
2 < 0.
k

−

It implies that

lim
k→∞ k

ˆxk+1

ˆxk

k

−

= 0.

Besides, according to (40), it is easy to obtain that

lim
k→∞ k

λk+1

λk

k

−

= 0.

Moreover, utilizing Byk+1 = ˆAˆxk+1

1

ρ (λk+1

−

−

λk) from (38), we have

B(yk+1
k

−

yk)

k ≤ k

ˆA(ˆxk+1

ˆxk)
k

−

+

1
ρ k

(λk+1

λk)
k

+

−

1
ρ k

(λk

λk−1)
.
k

−

Besides, as shown in Lemma 1 in [40], the full row rank of B (i.e., P1.4) implies that

yk

yk+1
k
where ¯M > 0 is a constant. Taking limit on both sides of (55) and utilizing (56), we obtain

B(yk+1
k

yk)
,
k

k ≤

¯M

−

−

lim
k→∞ k

yk+1

yk

k

−

= 0.

Combining (53), (54) and (57), we obtain that

yk+1

lim
k→∞ k

yk

2
2 +
k

ˆxk+1
k

−

ˆxk

2
2 +
k

λk+1
k

λk

2
2 = 0.
k

−

−

(49)

(50)

(51)

(52)

(53)

(54)

(55)

(56)

(57)

(58)

By setting k + 1 = ki, plugging (53) into (36) and (54) into (37), and taking limit ki
(y∗, ˆx∗) is the KKT point of LS-LP(θ; ǫ) (i.e., (33)).

→ ∞

, we obtain the KKT conditions. It tells that the cluster point

A.5 Global Convergence

Inspired by the analysis presented in [25], in this section we will prove the following conclusions:

ˆxk+1

∞
k=1 k
yk, ˆxk, λk
P
{

–
–
– (y∗, ˆx∗) is the KKT point of (33).

;
−
∞
converges to (y∗, ˆx∗, λ∗);

ˆxk

<

k

}

Firstly, utilizing the optimality conditions (36, 37, 38), we have that

∂y

L
∇ˆxL

∇λL

ρ,ǫ(ˆxk+1, yk+1, λk+1) = ∂h(yk+1)
ρ,ǫ(ˆxk+1, yk+1, λk+1) =

ρ,ǫ(ˆxk+1, yk+1, λk+1) = ˆAˆxk+1

−

B⊤λk+1
ρB⊤( ˆAˆxk+1
−
∇ˆx ˆf (ˆxk+1) + ˆA⊤λk+1 + ρ ˆA⊤( ˆAˆxk+1
1
ρ

Byk+1 =

(λk+1

λk).

−

−

−

−

Byk+1)

B⊤(λk+1
∋ −
Byk+1) = ˆA⊤(λk+1

Further, combining with (40), there exists a constant C > 0 such that

dist

0, ∂(y,ˆx,λ)L
(cid:0)

ρ,ǫ(yk+1, ˆxk+1, λk+1)

C

ˆxk+1
k

ˆxk

,
k

−

≤

(cid:1)

ρB⊤ ˆA(ˆxk+1

ˆxk),

−

λk)
λk),

−

−

−

(59)

(60)

(61)

(62)

MAP Inference via ℓ2-Sphere Linear Program Reformulation

17

where dist(
·
for clarity. Besides, the relation (44) implies that there is a constant D

) denotes the distance between a vector and a set of vectors. Hereafter we denote ∂(y,ˆx,λ)L

(0, ǫ

,

·

1
ρ ) such that

∈

−

ρ,ǫ(yk+1, ˆxk+1, λk+1) as ∂

ρ,ǫ(yk+1, ˆxk+1, λk+1)

L

ρ,ǫ(yk, ˆxk, λk)

L

− L

ρ,ǫ(yk+1, ˆxk+1, λk+1)

D

ˆxk+1
k

ˆxk

2
2.
k

−

Moreover, the relation (49) implies that
its decreasing property, the limit of

is lower bounded along the convergent sub-sequence

exists. Thus, we will show that

≥
ρ,ǫ(yk, ˆxk, λk)
}

{L
ρ,ǫ(yk, ˆxk, λk)
}

{L
ρ,ǫ(y∗, ˆx∗, λ∗).

lim
k→∞L

ρ,ǫ(yk, ˆxk, λk) = l∗ :=

L

To prove it, we utilize the fact that yk+1 is the minimizer of

ρ,ǫ(y, ˆxk, λk), such that

L

ρ,ǫ(yk+1, ˆxk, λk)

L

≤ L

ρ,ǫ(y∗, ˆxk, λk).

(63)

(yki , ˆxki , λki )
}
{

. Combining with the

(64)

(65)

Combining the above relation, (58) and the continuity of
converges to (y∗, ˆx∗, λ∗),

L

ρ,ǫ w.r.t. ˆx and λ, the following relation holds along the sub-sequence

(yki , ˆxki , λki )
}
{

that

lim sup
i→∞ L

ρ,ǫ(yki+1, ˆxki+1, λki+1)

ρ,ǫ(y∗, ˆx∗, λ∗).

≤ L

According to (58), the sub-sequence
have

(yki+1, ˆxki+1, λki+1)
}
{

lim inf
i→∞ L

ρ,ǫ(yki+1, ˆxki+1, λki+1)

ρ,ǫ(y∗, ˆx∗, λ∗).

≥ L

also converges to (y∗, ˆx∗, λ∗). Then, utilizing the lower semi-continuity of

(66)

ρ,ǫ, we

L

(67)

Combining (66) with (67), we know the existence of the limit of the sequence

, which proves the relation (64).

ρ,ǫ(yk, ˆxk, λk)
}

{L

R+, as well as a neighbourhood

V

→

of (y∗, ˆx∗, λ∗). ϕ is differentiable

As

ρ,ǫ is KL function, according to Deﬁnition 1, it has the following properties:

L

– There exist a constant η

(0,

], a continuous concave function ϕ : [0, η)

∞
on (0, η) with positive derivatives.

∈

– For all (y, ˆx, λ)

∈ V

satisfying l∗ <

ρ,ǫ(y, ˆx, λ) < l∗ + η, we have

L

ϕ′(

ρ,ǫ(y, ˆx, λ)

l∗)dist(0, ∂

L

−
Then, we deﬁne the following neighborhood sets:

L

ρ,ǫ(y, ˆx, λ))

1.

≥

ˆx∗

ˆx

(y, ˆx, λ)

Vζ :=
(cid:26)
Vζ,ˆx :=
k
(cid:9)
where ζ > 0 is a small constant.

k
ˆx∗

(cid:12)
(cid:12)
(cid:12)
−
(cid:12)

< ζ

−

ˆx

ˆx

(cid:8)

k

k

,

(cid:12)
(cid:12)

< ζ,

y

k

−

y∗

ˆA
< ¯M (
k

k

k

+ 1)ζ,

λ

k

−

λ∗

k

< ζ

⊆ V

(cid:27)

Utilizing the relations (37) and (38), as well as P2.1, we obtain that for any k

1, the following relation holds:

ǫ2

λk
k

−

λ∗

2
2 ≤ k
k

ˆA⊤(λk

2
λ∗)
2 =
k

▽ ˆf (ˆxk)
k

▽ ˆf (ˆx∗)
2 = ǫ2
2
k

ˆxk
k

−

−

−

Also, the relations (37) and (38) imply that for any k

1, we have

≥

≥
2
2.
k

ˆx∗

B(yk
k

y∗)
k

−

=

ˆA(ˆxk
k

−

ˆx∗)

−

(λk

1
ρ

−

λk−1)

ˆA

(ˆxk

k ≤ k

kk

ˆx∗)
k

+

−

1
ρ k

λk

−

λk−1

.
k

Moreover, the relation (58) implies that

N0

∃

≥

1 such that

k

∀

≥

N0, we have

λk
k

−

λk−1

k ≤

ρζ.

Similar to (56), the full row rank of B implies

yk
k

−

y∗

k ≤

¯M

B(yk
k

−

y∗)
k

. Then, plugging (73) into (72), we obtain that

+ 1)ζ,

y∗

yk
k
−
for any ˆxk

ˆA
¯M (
k ≤
k
k
∈ Vζ,ˆx and k
following relations hold:

≥

Moreover, (44) and (64) implies that

N0. Combining (71) and (74), we know that if ˆxk

ρ,ǫ(yk, ˆxk, λk)

L

l∗,

k

∀

≥

≥

∈ Vζ,ˆx and k

≥

N0, then (yk, ˆxk, λk)

∈ Vζ ⊆ V

.

1. Besides, as (y∗, ˆx∗, λ∗) is a cluster point, we will obtain that

ˆxN
l∗ <
ˆxN
k

∈ Vζ,ˆx
L

−

k

ρ,ǫ(yN , ˆxN , λN ) < l∗ + η
ˆx∗

ρ,ǫ(yN , ˆxN , λN )

+ 2

(
L

q
Next, We will show that if ˆxN






−
∈ Vζ,ˆx and l∗ <

l∗)/D + C

D (

ρ,ǫ(yN , ˆxN , λN )

l∗) < ζ

−

L

ρ,ǫ(yN , ˆxN , λN ) < l∗ + η hold for some ﬁxed k

L

N0, then the following relation holds

≥

ˆxk+1
k

−

ˆxk

+

k

ˆxk+1
k
(cid:0)

ˆxk

−

ˆxk

k − k

ˆxk−1

−

C
D

ϕ

(cid:20)

L

(cid:0)

≤

k
(cid:1)

ρ,ǫ(yk, ˆxk, λk)

l∗

−

ϕ

−

L

(cid:1)

(cid:0)

ρ,ǫ(yk+1, ˆxk+1, λk+1)

−

l∗

.

(cid:21)
(cid:1)

(68)

(69)

(70)

(71)

(72)

(73)

(74)

N

∃

≥

N0, the

(75)

(76)

18

Baoyuan Wu et al.

To prove (76), we utilize the fact that ˆxk
we obtain that

∈ Vζ,ˆx, k

≥

N0 implies that (yk, ˆxk, λk)

∈ Vζ,ˆx ⊆ V

. And, combining with l∗ <

ρ,ǫ(yk, ˆxk, λk) < l∗+η,

L

ϕ′(

L

ρ,ǫ(yk, ˆxk, λk)

l∗)dist(0, ∂

L

−

ρ,ǫ(yk, ˆxk, λk))

1.

≥

Combining the relations (62), (63) and (77), as well as the concavity of ϕ, we obtain that

C

ˆxk
k

−
dist(0, ∂

≥

≥

≥

dist(0, ∂
L
ˆxk+1
k

D

L

k ·

L
(cid:0)

ˆxk−1
ϕ
ρ,ǫ(yk, ˆxk, λk))
(cid:2)
ρ,ǫ(yk, ˆxk, λk))
2,
k

ˆxk

−

ρ,ǫ(yk, ˆxk, λk)

−

l∗
ϕ
−
ρ,ǫ(yk, ˆxk, λk)
L
ρ,ǫ(yk, ˆxk, λk)
(cid:0)
L
(cid:0)

(cid:1)

L
(cid:0)
−

−

ϕ
ϕ′
(cid:2)

·

·

ρ,ǫ(yk+1, ˆxk+1, λk+1)
l∗
l∗

ϕ
L
ρ,ǫ(yk, ˆxk, λk)
(cid:0)

−

(cid:1)

·

L

l∗

−

− L

ρ,ǫ(yk+1, ˆxk+1, λk+1)

l∗
(cid:1)(cid:3)
ρ,ǫ(yk+1, ˆxk+1, λk+1)
(cid:1)(cid:3)

−

(cid:1)

(cid:2)

(cid:3)

(77)

(78)

for all such k. Taking square root on both sides of (78), and utilizing the fact that a + b

2√ab, then (76) is proved.

We then prove

N, ˆxk

k

∀

≥

(75). For k = N + 1, we have

∈ Vζ,ˆx holds. This claim can be proved through induction. Obviously it is true for k = N by construction, as shown in

≥

ˆxN+1
k

−

ˆx∗

ˆxN+1

k ≤ k

ˆxN

+

ˆxN
k

k

−

ˆx∗

k

−

ρ,ǫ(yN , ˆxN , λN )

− L

ρ,ǫ(yN+1, ˆxN+1, λN+1)

/D +

ˆxN
k

ˆx∗

k

−

ρ,ǫ(yN , ˆxN , λN )

l∗

/D +

−

ˆxN
k

ˆx∗

k

−

(cid:1)

< ζ,

L

L

≤

≤

q(cid:0)

q(cid:0)

where the ﬁrst inequality utilizes (63), and the last inequality follows the last relation in (75). Thus,

N, ˆxk
∈ Vζ,ˆx holds.
k
≥
∈ Vζ,ˆx for some t > 1, and we need to prove that ˆxN+t
∈ Vζ,ˆx also holds, i.e.,

∀

Next, we suppose that ˆxN , . . . , ˆxN+t−1

(cid:1)

ˆxN+t
k

−

ˆx∗

ˆxN

k ≤ k

ˆx∗

+

ˆxN+1
k

k

−

−

ˆxN

+

k

t−1

i=1
X

ˆxN+i+1
k

−

ˆxN+i

k

t−1

i=1 (cid:20)
X

ˆxN+i+1
k

−

ˆxN+i

+

k

ˆxN+i+1
k
(cid:0)

−

ˆxN+i

ˆxN+i

k − k

ˆxN+i−1

−

k
(cid:21)
(cid:1)

=

ˆxN
k

ˆx∗

k

−

+ 2

ˆxN+1
k

−

ˆxN

ˆxN+t

k − k

ˆxN+t−1

+

k

−

ˆxN

≤k

ˆx∗

k

−

+ 2

ˆxN+1
k

−

ˆxN

+

k

ˆxN

≤k

ˆx∗

k

−

+ 2

ˆxN+1
k

−

ˆxN

+

k

C
D

C
D

t−1

i=1
X
t−1

i=1
X

ϕN+i

−

ϕN+i+1

(cid:2)
ϕN+1

(cid:3)

ˆxN

≤k

ˆx∗

k

−

+ 2

s

N+1
ρ,ǫ

L

N
ρ,ǫ − L
D

+

C
D

ˆxN

≤k

ˆx∗

k

−

+ 2

s

l∗

L

N
ρ,ǫ −
D

+

C
D

t−1

i=1
X

ϕN+1

t−1

i=1
X

ϕN+1 < ζ

ρ,ǫ(yN+i, ˆxN+i, λN+i)

where ϕN+i = ϕ(
l∗) and
follows from (63). The ﬁfth inequality utilizes the fact that
holds. We have proved that
k
Then, according to
k

≥
N, ˆxk

N, ˆxk

−

L

∈ Vζ,ˆx holds by induction.

∀
≥

∀

∞

ˆxk+1
k

−

ˆxk

Xk=N
which implies that

C
D

k ≤

ϕN +

ˆxN
k

−

ˆxN−1

<

,

∞

k

ρ,ǫ(yN , ˆxN , λN ). The second inequality follows from (76). The fourth inequality

N
ρ,ǫ =
L
N+1
ρ,ǫ > l∗, and the last inequality follows from the last relation in (75). Thus, ˆxN+k
L

L

∈ Vζ,ˆx, we can sum both sides of (76) from k = N to

, to obtain that

∞

∞
k=1 k

ˆxk+1

ˆxk

<

holds. Thus

∞
in (38) and (58), as well as the surjectivity of B (i.e., full row rank). The convergence of
surjectivity of ˆA (i.e., full row rank). Consequently,
yk, ˆxk, λk
{
point of Problem (33) has been proved in Section A.4.

P

−

k

}

}

ˆxk
{

converges. The convergence of
λk
{

λk)
ˆA⊤λk+1 in (37) and the
∇
converges to the cluster point (y∗, ˆx∗, λ∗). The conclusion that (y∗, ˆx∗) is the KKT

follows from Byk+1 = ˆAˆxk+1 + 1

yk
{
follows from

ˆf (ˆxk+1) =

ρ (λk+1

−

−

}

}

A.6 ǫ-KKT Point of the Original LS-LP Problem

Proposition 1 The globally converged solution (y∗, x∗, λ∗) produced by the ADMM algorithm for the perturbed LS-LP problem (33) is the ǫ-KKT
solution to the original LS-LP problem (32).

Proof The globally converged solution (y∗, ˆx∗, λ∗) to the perturbed LS-LP problem (33) satisﬁes the following relations:

B⊤λ∗

∈

∂h(y∗),

∇

ˆf (ˆx∗) =

−

ˆA⊤λ∗, ˆAˆx∗ = By∗.

(82)

(79)

(80)

∈ Vζ,ˆx

(81)

MAP Inference via ℓ2-Sphere Linear Program Reformulation

Recalling the deﬁnitions ˆA = [A, ǫI], ˆx = [x; ¯x] and ˆf (ˆx) = f (x) + ǫ

2 ˆx⊤ ˆx, the above relations imply that

ˆf (ˆx∗) + ˆA⊤λ∗ =

f (x∗) + A⊤λ∗ + ǫx∗ = 0

∇
ˆAˆx∗ + By∗ = Ax∗ + ǫ¯x∗ + By∗ = 0

∇

⇒ k∇
Ax∗ + By∗

f (x∗) + A⊤λ∗
ǫ¯x∗
k

k
= O(ǫ),

=

k

= ǫ

x∗
k

k

= O(ǫ),

⇒ k
. Thus, according to Deﬁnition 2, the globally converged point (y∗, x∗) is the ǫ-KKT solution to the original
}

k

where we utilize the boundedness of
LS-LP problem (32).

ˆx∗
{

A.7 Convergence Rate

Lemma 3 Firstly, without loss of generality, we can assume that l∗ =
We further assume that
can obtain the following inequalities:

L

L
ρ,ǫ has the KL property at (y∗, ˆx∗, λ∗) with the concave function ϕ(s) = cs1−p, where p

ρ,ǫ(y∗, ˆx∗, λ∗) = 0 (e.g., one can replace lk =

∈

ρ,ǫ(yk, ˆxk, λk) by lk −
l∗).
L
[0, 1), c > 0. Consequently, we

(i) if p = 0, then
(ii) If p

(0, 1

(yk, ˆxk, λk)
{

2 ], then there exist c > 0 and τ
∈
∈
( 1
2 , 1), then there exist c > 0 such that

}k=1,...,∞ can converge in ﬁnite steps;
ˆxk+1
−
k
ck− 1−p
2p−1 .

(0, 1) such that

ˆxk

(iii) p

∈

k ≤

ˆxk

cτ k;

k ≤

ˆxk+1
k

−
N : ˆxk 6

= ˆxk+1}

. If k

Proof (i) If p = 0, we deﬁne a subset H =

k

{

∈

ˆxk

ˆxk+1
2
k
k
Combining with (63), we have

C3 > 0.

≥

−

H is sufﬁciently large, then these exists C3 > 0 such that

∈

(85)

(86)

ˆxk

2
k

ˆxk+1
k

−

D

C3D > 0.

lk+1 ≥

lk −
≥
If the subset H is inﬁnite, then it will contradict to the fact that lk −
that
yk, λk}k∈N converges in ﬁnite steps.
{
xk + 1
△k =

By deﬁning

∞
k k

ˆxk
{

xk

−

k

, the inequality (81) can be rewritten as follows

. Thus, H is a ﬁnite subset, leading to the conclusion
}k∈N will converge in ﬁnite steps. Recalling the relationships between ˆxk and yk, λk (see the descriptions under (81)), we also obtain that

lk+1 →

0 as k

→ ∞

△k ≤

C
D

P

ϕ(lk) + (

△k−1 − △k) <

.

∞

Besides, the KL property and l∗ = 0 give that

ϕ′(lk)dist(0, ∂(lk)) = c(1

p)l1−p
k

−

dist(0, ∂(lk))

1

≥

⇒

lp
k ≤

c(1

−

p)dist(0, ∂(lk)).

Combining with (62), we obtain

c(1

p)C(

lp
k ≤
Then, inserting (89) into (87), we obtain

△k−1 − △k)

⇒

−

ϕ(lk) = cl1−p

k ≤

1−p

1−p

c(c(1

p)C)

−

p (

△k−1 − △k)

p = C1(

△k−1 − △k)

1−p

p .

1−p

p + (

△k ≤
(ii) If p

C2(

△k−1 − △k)
2 ], then 1−p
(0, 1

∈

that (

△k−1 − △k)

1−p
p

△k−1 − △k) <

.

1. Besides, since (

∞
△k−1 − △k)
p ≥
△k−1 − △k). Inserting it into (90), we obtain that
(
C3

0 when k

→

≤

→ ∞

, there exists an integer K0 such that (

△k−1 − △k) < 1, leading to

△k ≤

(C2 + 1)(

△k−1 − △k)

⇒ △k ≤

C3(

△k−1 − △k)

⇒ △k ≤

1 + C3 △k−1 = τ

△k−1, with τ

∈

(0, 1),

k > K0.

∀

(91)

It is easy to deduce that
index. Combining with

ˆxk+1
k

−

ˆxk

ˆxk

k ≤ k

△k ≤
ˆxk
−
k

ˆx∗

k

−

K0 τ −K0 )τ k = c
(
△
ˆx∗
k ≤ △k, it is easy to obtain that
ˆxk+1
(τ k+1 + τ k)
k

k ≤

ˆx∗

−

c
2

+

2 τ k, with c being a positive constant. Note that k in τ k indicates k power of τ , rather than the iteration

ˆxk
k

−

ˆx∗

k ≤

c

2 τ k with τ

(0, 1) and c being a positive constant. Then, we have

∈

(iii) If p

∈

2 , 1), then 1−p
( 1

p < 1. Then, it is easy to obtain that (

△k−1 − △k)

p > (

△k−1 − △k). Inserting it into (90), we obtain that

△k ≤

(C2 + 1)(

△k−1 − △k)

1−p
p

⇒ △k ≤

C3(

△k−1 − △k)

1−p
p

⇒ △

C4(

△k−1 − △k),

k > K0.

∀

It has been studied in Theorem 2 of [1] that the above inequality can deduce

we have that

ˆxk
k

−

ˆx∗

k ≤

c

2 k− 1−p

2p−1 . Then, we have

△k ≤

ˆxk+1
k

−

ˆxk

ˆxk

k ≤ k

ˆx∗

+

ˆxk+1
k

k

−

−

ˆx∗

k ≤

c
2

k− 1−p

2p−1 + (k + 1)− 1−p

2p−1

(cid:0)

ck− 1−p
2p−1 .

≤

(cid:1)

2p−1 , with c being a positive constant. Since

ˆxk
k

−

ˆx∗

k ≤ △k,

(94)

cτ k.

≤

1−p

p
1−p
k

≤
2 k− 1−p

c

19

(83)

(84)

(87)

(88)

(89)

(90)

(92)

(93)

20

Baoyuan Wu et al.

Proposition 2 We adopt the same assumptions in Lemma 3. Then,

(i) If p = 0, then we will obtain the ǫ-KKT solution to the LS-LP problem in ﬁnite steps.
(ii) If p

2 ], then we will obtain the ǫ-KKT solution to the LS-LP problem in at least O

(0, 1

∈

(iii) If p

∈

(cid:0)
( 1
2 , 1), then we will obtain the ǫ-KKT solution to the LS-LP problem in at least O

( 1
ǫ )2
log 1
τ
4p−2
1−p

( 1
ǫ )

steps.

(cid:1)
steps.

Proof The conclusion (i) directly holds from Lemma 3(i).

According to the optimality condition (36), we have

(cid:0)

(cid:1)

dist

B⊤λk+1, ∂h(yk+1)

(cid:0)
dist2

(cid:1)
B⊤λk+1, ∂h(yk+1)

=

ρB⊤ ˆA(ˆxk+1
k
=

ˆxk

ˆxk+1
k

−

ˆxk)
2
k

−
2
ρ2 ˆA⊤ BB⊤ ˆA ≤
k

dist

(cid:0)
B⊤λk+1, ∂h(yk+1)

(cid:1)
≤

O(

1
ǫ

)

ˆxk+1

· k

ˆxk

2
k

−

⇒

⇒

ˆxk+1
ξmax(ρ2 ˆA⊤BB⊤ ˆA)
k

ˆxk

2
2 = O(
k

−

1
ǫ2

ˆxk+1
)
k

ˆxk

2
2
k

−

(cid:0)

(cid:1)

According to the optimality condition (38) and the relation (40), we obtain that

ˆAˆxk+1
k

−

Byk+1

2 =
k

1
ρ k

λk+1

λk

−

2
k

≤

1
ρ k

ˆxk+1

ˆxk

ǫ

2
k

≤

−

ˆxk+1

· k

ˆxk

−

2
k

≤

O(

1
ǫ

)

ˆxk+1

· k

ˆxk

2.
k

−

According to Lemma 3, we have
2 ], then

(0, 1

(ii) If p

∈

O(

1
ǫ

)

ˆxk+1

· k

ˆxk

−

2
k

≤

O(

)τ k

1
ǫ

≤

O(ǫ)

O

k

≥

log 1
τ

(

⇒

)2

,

1
ǫ

(95)

(96)

which means that when k
original LS-LP problem.
( 1
2 , 1), then

∈

(iii) If p

O

log 1
τ

( 1
ǫ )2

≥

(cid:0)

(cid:1)

, we will obtain the ǫ-KKT solution to the perturbed LS-LP problem, i.e., the ǫ-KKT solution to the

(cid:0)

(cid:1)

O(

1
ǫ

)

ˆxk+1

· k

ˆxk

−

2
k

≤

O(

)k− 1−p

2p−1

1
ǫ

O(ǫ)

k

≥

⇒

O

(

≤

4p−2
1−p

1
ǫ

)

,

(cid:1)

(cid:0)

which means that when k
LS-LP problem.

≥

4p−2
1−p

O

( 1
ǫ )

(cid:0)

(cid:1)

, we will obtain the ǫ-KKT solution to the perturbed LS-LP problem, i.e., the ǫ-KKT solution to the original

MAP Inference via ℓ2-Sphere Linear Program Reformulation

21

References

1. Attouch, H., Bolte, J.: On the convergence of the proximal algo-
rithm for nonsmooth functions involving analytic features. Math-
ematical Programming 116(1-2), 5–16 (2009)

2. Attouch, H., Bolte, J., Redont, P., Soubeyran, A.: Proximal alter-
nating minimization and projection methods for nonconvex prob-
lems: An approach based on the kurdyka-lojasiewicz inequality.
Mathematics of Operations Research 35(2), 438–457 (2010)
3. Besag, J.: On the statistical analysis of dirty pictures. Journal of
the Royal Statistical Society. Series B (Methodological) pp. 259–
302 (1986)

4. Bolte, J., Daniilidis, A., Lewis, A.: The łojasiewicz inequality for
nonsmooth subanalytic functions with applications to subgradient
dynamical systems. SIAM Journal on Optimization 17(4), 1205–
1223 (2007)

5. Bolte, J., Daniilidis, A., Lewis, A., Shiota, M.: Clarke subgradients
of stratiﬁable functions. SIAM Journal on Optimization 18(2),
556–572 (2007)

6. Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J.: Distributed
optimization and statistical learning via the alternating direction
method of multipliers. Foundations and Trends R
in Machine
(cid:13)
Learning 3(1), 1–122 (2011)
G.,
2011

7. Elidan,
Pascal
probabilistic
http://www.cs.huji.ac.il/project/PASCAL/index.php (2012)

U.:
challenge.

Heinemann,

Globerson,

inference

A.,

8. Fu, Q., Banerjee, H.W.A.: Bethe-admm for tree decomposition
based parallel map inference. In: Uncertainty in Artiﬁcial Intel-
ligence, p. 222. Citeseer (2013)

9. Globerson, A., Jaakkola, T.S.: Fixing max-product: Convergent
message passing algorithms for map lp-relaxations. In: NIPS, pp.
553–560 (2008)

10. Gould, S., Fulton, R., Koller, D.: Decomposing a scene into ge-
ometric and semantically consistent regions. In: ICCV, pp. 1–8.
IEEE (2009)

11. Jaimovich, A., Elidan, G., Margalit, H., Friedman, N.: Towards
an integrated protein–protein interaction network: A relational
markov network approach.
Journal of Computational Biology
13(2), 145–164 (2006)

12. Johnson, J.K., Malioutov, D.M., Willsky, A.S.: Lagrangian re-
laxation for map estimation in graphical models. arXiv preprint
arXiv:0710.0013 (2007)

13. Jojic, V., Gould, S., Koller, D.: Accelerated dual decomposition

for map inference. In: ICML, pp. 503–510 (2010)

14. Kappes, J.H., Andres, B., Hamprecht, F.A., Schnörr, C., Nowozin,
S., Batra, D., Kim, S., Kausler, B.X., Kröger, T., Lellmann, J.,
et al.: A comparative study of modern inference techniques for
structured discrete energy minimization problems.
International
Journal of Computer Vision 115, 155–184 (2015)

15. Kappes, J.H., Savchynskyy, B., Schnörr, C.: A bundle approach to
efﬁcient map-inference by lagrangian relaxation. In: CVPR, pp.
1688–1695. IEEE (2012)

16. Karush, W.: Minima of functions of several variables with inequal-
ities as side constraints. M. Sc. Dissertation. Dept. of Mathemat-
ics, Univ. of Chicago (1939)

17. Kelley, J.: The cutting-plane method for solving convex programs.
Journal of the Society for Industrial and Applied Mathematics pp.
703–712 (1960)

18. Koller, D., Nir, F. (eds.): Probabilistic Graphical Models: Princi-

ples and Techniques. MIT Press, Cambridge, MA (2009)

19. Kolmogorov, V.: Convergent tree-reweighted message passing for
energy minimization. IEEE transactions on pattern analysis and
machine intelligence 28(10), 1568–1583 (2006)

20. Komodakis, N., Paragios, N., Tziritas, G.: Mrf optimization via
dual decomposition: Message-passing revisited. In: ICCV, pp. 1–
8. IEEE (2007)

21. Kschischang, F.R., Frey, B.J., Loeliger, H.A.: Factor graphs and
IEEE Transactions on information

the sum-product algorithm.
theory 47(2), 498–519 (2001)

22. Kuhn, H.W., Tucker, A.W.: Nonlinear programming.

In: Traces
and emergence of nonlinear programming, pp. 247–258. Springer
(2014)

23. Land, A.H., Doig, A.G.: An automatic method of solving discrete
programming problems. Econometrica: Journal of the Economet-
ric Society pp. 497–520 (1960)

24. Laurent, M., Rendl, F.: Semideﬁnite programming and integer
programming. Centrum voor Wiskunde en Informatica (2002)
25. Li, G., Pong, T.K.: Global convergence of splitting methods for
nonconvex composite optimization. SIAM Journal on Optimiza-
tion 25(4), 2434–2460 (2015)

26. Lojasiewicz, S.: Une propriété topologique des sous-ensembles
analytiques réels. Les équations aux dérivées partielles 117, 87–89
(1963)

27. Martins, A.F., Figeuiredo, M.A., Aguiar, P.M., Smith, N.A., Xing,
E.P.: An augmented lagrangian approach to constrained map in-
ference. In: ICML (2011)

28. Martins, A.F., Figueiredo, M.A., Aguiar, P.M., Smith, N.A., Xing,
E.P.: Ad3: Alternating directions dual decomposition for map in-
ference in graphical models. Journal of Machine Learning Re-
search 16, 495–545 (2015)

29. Meshi, O., Globerson, A.: An alternating direction method for
dual map lp relaxation.
In: Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 470–
483. Springer (2011)

30. Meshi, O., Mahdavi, M., Schwing, A.: Smooth and strong: Map
inference with linear convergence. In: NIPS, pp. 298–306 (2015)
31. Otten, L., Dechter, R.: Anytime and/or depth-ﬁrst search for com-
binatorial optimization. AI Communications 25(3), 211–227
(2012)

32. Otten, L., Ihler, A., Kask, K., Dechter, R.: Winning the pascal 2011
In: IN

map challenge with enhanced and/or branch-and-bound.
NIPS WORKSHOP DISCML. Citeseer (2012)

33. Savchynskyy, B., Schmidt, S., Kappes, J., Schnörr, C.: Efﬁcient
mrf energy minimization via adaptive diminishing smoothing.
arXiv preprint arXiv:1210.4906 (2012)

34. Schwing, A.G., Hazan, T., Pollefeys, M., Urtasun, R.: Globally
convergent dual map lp relaxation solvers using fenchel-young
margins. In: NIPS, pp. 2384–2392 (2012)

35. Schwing, A.G., Hazan, T., Pollefeys, M., Urtasun, R.: Globally
convergent parallel map lp relaxation solver using the frank-wolfe
algorithm. In: ICML, pp. 487–495 (2014)

36. Sontag, D.A.: Approximate inference in graphical models using lp
relaxations. Ph.D. thesis, Massachusetts Institute of Technology
(2010)

37. Sontag, D.A., Li, Y., et al.: Efﬁciently searching for frustrated cy-

cles in map inference. In: UAI (2012)

38. Wainwright, M.J., Jaakkola, T.S., Willsky, A.S.: Map estimation
via agreement on trees: message-passing and linear program-
IEEE transactions on information theory 51(11), 3697–
ming.
3717 (2005)

39. Wainwright, M.J., Jordan, M.I.: Graphical models, exponential
families, and variational inference. Foundations and Trends in
Machine Learning 1(1-2), 1–305 (2008)

40. Wang, Y., Yin, W., Zeng, J.: Global convergence of admm in non-
convex nonsmooth optimization. Journal of scientiﬁc program-
ming (2017)

41. Wu, B., Ghanem, B.: ℓp-box admm: A versatile framework for
integer programming. IEEE transactions on pattern analysis and
machine intelligence 41(7), 1695–1708 (2019)

42. Xu, Y., Yin, W.: A block coordinate descent method for regular-
ized multiconvex optimization with applications to nonnegative
tensor factorization and completion. SIAM Journal on imaging
sciences 6(3), 1758–1789 (2013)

