1
2
0
2

n
a
J

2
1

]

G
L
.
s
c
[

5
v
0
7
9
2
1
.
2
1
9
1
:
v
i
X
r
a

Pontryagin Differentiable Programming:
An End-to-End Learning and Control Framework

Wanxin Jin
Purdue University

Zhaoran Wang
Northwestern University

{wanxinjin,zhaoranwang}@gmail.com

Zhuoran Yang
Princeton University
zy6@princeton.edu

Shaoshuai Mou
Purdue University
mous@purdue.edu

Abstract

This paper develops a Pontryagin Differentiable Programming (PDP) methodology,
which establishes a uniﬁed framework to solve a broad class of learning and control
tasks. The PDP distinguishes from existing methods by two novel techniques: ﬁrst,
we differentiate through Pontryagin’s Maximum Principle, and this allows to obtain
the analytical derivative of a trajectory with respect to tunable parameters within an
optimal control system, enabling end-to-end learning of dynamics, policies, or/and
control objective functions; and second, we propose an auxiliary control system in
the backward pass of the PDP framework, and the output of this auxiliary control
system is the analytical derivative of the original system’s trajectory with respect
to the parameters, which can be iteratively solved using standard control tools. We
investigate three learning modes of the PDP: inverse reinforcement learning, system
identiﬁcation, and control/planning. We demonstrate the capability of the PDP in
each learning mode on different high-dimensional systems, including multi-link
robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing.

1

Introduction

Many learning tasks can ﬁnd their counterpart problems in control ﬁelds. These tasks both seek to
obtain unknown aspects of a decision-making system with different terminologies compared below.

Table 1: Topic connections between control and learning (details presented in Section 2)

UNKNOWNS IN A SYSTEM

LEARNING METHODS

CONTROL METHODS

Dynamics xt+1=f θ(xt, ut)
Policy ut = πθ(t, xt)
Control objective J= (cid:80)

t cθ(xt, ut)

System identiﬁcation
Markov decision processes
Reinforcement learning (RL) Optimal control (OC)
Inverse RL

Inverse OC

With the above connections, learning and control ﬁelds
have begun to explore the complementary beneﬁts of
each other: control theory may provide abundant models
and structures that allow for efﬁcient or certiﬁcated algo-
rithms for high-dimensional tasks, while learning enables
to obtain these models from data, which are otherwise
not readily attainable via classic control tools. Examples
that enjoy both beneﬁts include model-based RL [1, 2],
where dynamics models are used for sample efﬁciency;
Figure 1: left: PDP learns rocket landing
and Koopman-operator control [3, 4], where via learning,
control, right: PDP learns quadrotor dy-
nonlinear systems are lifted to a linear observable space
namics and control objective for imitation.
to facilitate control design. Inspired by those, this paper
aims to exploit the advantage of integrating learning and control and develop a uniﬁed framework that
enables to solve a wide range of learning and control tasks, e.g., the challenging problems in Fig. 1.

East (m)84048North (m)63036Upward (m)036912Learning to control  rocket powered landingX (m)84048Y (m)63036Z (m)036912Imitation learning for  quadrotor maneuveringlearnerexpert 
 
 
 
 
 
2 Background and Related Work

Learning dynamics. This is usually referred as to as system identiﬁcation in control ﬁelds, which
typically consider linear systems represented by transfer functions [5]. For nonlinear systems, the
Koopman theory [6] provides a way to lift states to a (inﬁnite-dimensional) linear observable space
[3, 7]. In learning, dynamics is characterized by Markov decision proceses and implemented using
linear regression [8], observation-transition modeling [9], latent-space modeling [10], (deep) neural
networks [11], Gaussian process [12], transition graphs [13], etc. Although off-the-shelf, most of
these methods have to trade off between data efﬁciency and long-term prediction accuracy. To achieve
both, physically-informed learning [14–17] injects physics laws into learning models, but they are
limited to mechanical systems. Recently, a trend of work starts to use dynamical systems to explain
(deep) neural networks, and some new algorithms [18–25] have been established.

This paper focuses on learning general dynamical models, encompassing either physical dynamics
with unknown parameters or neural difference equations. The proposed learning framework is injected
with inductive knowledge of optimal control theory to achieve efﬁciency and explainability.

Learning optimal polices. In learning ﬁelds, it relates to reinforcement learning (RL). Model-free RL
provides a general-purpose framework to learn policies directly from interacting with environments
[26–28], but usually suffers from signiﬁcant data complexity. Model-based RL addresses this by ﬁrst
learning a dynamics model from experience and then integrating it to policy improvement [1, 12, 29–
31]. The use of a model can assist to augment experience data [32, 33], perform back-propagation
through time [12], or test policies before deployment. Model-based RL also faces some challenges
that are not well-addressed. For example, how to efﬁciently leverage imperfect models [34], and
how to maximize the joint beneﬁt by combining policy learning and motion planning (trajectory
optimization) [31, 35], where a policy has the advantage of execution coherence and fast deployment
while the trajectory planning has the competence of adaption to unseen or future situations.

The counterpart topic in control is optimal control (OC), which is more concerned with characterizing
optimal trajectories in presence of dynamics models. As in RL, the main strategy for OC is based on
dynamic programming, and many valued-based methods are available, such as HJB [36], differential
dynamical programming (DDP) [37] (by quadratizing dynamics and value function), and iterative
linear quadratic regulator (iLQR) [38] (by linearizing dynamics and quadratizing value function). The
second strategy to solve OC is based on the Pontryagin’s Maximum/Minimal Principle (PMP) [39].
Derived from calculus of variations, PMP can be thought of as optimizing directly over trajectories,
thus avoiding solving for value functions. Popular methods in this vein include shooting methods [40]
and collocation methods [41]. However, the OC methods based on PMP are essentially open loop
control and thus susceptible to model errors or disturbances in deployment. To address these, model
predictive control (MPC) [42] generates controls given the system current state by repeatedly solving
an OC problem over a ﬁnite prediction horizon (only the ﬁrst optimal input is executed), leading to a
closed-loop control form. Although MPC has dominated across many industrial applications [43],
developing fast MPC implementations is still an active research direction [44].

The proposed learning framework in this work has a special mode for model-based control tasks. The
method can be viewed as a complement to classic open-loop OC methods, because, although derived
from PMP (trajectory optimization), the method here is to learn a feedback/closed-loop control policy.
Depending on the speciﬁc policy parameterization, the method here can also be used for motion
planning. All these features will provide new perspectives for model-based RL or MPC control.

Learning control objective functions. In learning, this relates to inverse reinforcement learning
(IRL), whose goal is to ﬁnd a control objective function to explain the given optimal demonstrations.
The unknown objective function is typically parameterized as a weighted sum of features [45–47].
Strategies to learn the unknown weights include feature matching [45] (matching the feature values
between demonstrations and reproduced trajectories), maximum entropy [46] (ﬁnding a trajectory
distribution of maximum entropy subject to empirical feature values), and maximum margin [47]
(maximizing the margin of objective values between demonstrations and reproduced trajectories). The
learning update in the above IRL methods is preformed on a selected feature space by taking advantage
of linearity of feature weights, and thus cannot be directly applied to learning objective functions
that are nonlinear in parameters. The counterpart topic in the control ﬁeld is inverse optimal control
(IOC) [48–51]. With knowledge of dynamics, IOC focuses on more efﬁcient learning paradigms. For
example, by directly minimizing the violation of optimality conditions by the observed demonstration
data, [48, 50–52] directly compute feature weights without repetitively solving the OC problems.

2

Despite the efﬁciency, minimizing optimality violation does not directly assure the closeness between
the ﬁnal reproduced trajectory and demonstrations or the closeness of their objective values.

Fundamentally different from existing IRL/IOC methods, this paper will develop a new framework
that enables to learn complex control objective functions, e.g., neural objective functions, by directly
minimizing the loss (e.g., the distance) between the reproduced trajectory and demonstrations.

A uniﬁed perspective on learning dynamics/policy/control objective functions. Consider a gen-
eral decision-making system, which typically consists of aspects of dynamics, control policy, and
control objective function. In a uniﬁed perspective, learning dynamics, policies, or control objective
functions can be viewed as instantiations of the same learning problem but with (i) unknown parame-
ters appearing in the system’s different aspects and (ii) the different losses. For example, in learning
dynamics, a differential/difference equation is parameterized and the loss function can be deﬁned as
the prediction error between the equation’s output and target data; in learning policies, the unknown
parameters are in a feedback policy and the loss function is just the control objective function; and
in learning control objective functions, the control objective function is parameterized and the loss
function can be the discrepancy between the reproduced trajectory and the observed demonstrations.

Claim of contributions. Motivated by the above, this paper develops a uniﬁed learning framework,
named as PDP, that is ﬂexible enough to be customized for different learning and control tasks and
capable enough to efﬁciently solve high-dimensional and continuous-space problems. The proposed
PDP framework borrows the idea of ‘end-to-end’ learning [53] and chooses to optimize a loss function
directly with respect to the tunable parameters in the aspect(s) of a decision-making system, such
as the dynamics, policy, or/and control objective function. The key contribution of the PDP is that
we inject the optimal control theory as an inductive bias into the learning process to expedite the
learning efﬁciency and explainability. Speciﬁcally, the PDP framework centers around the system’s
trajectory and differentiates through PMP, and this allows us to obtain the analytical derivative of the
trajectory with respect to the tunable parameters, a key quantity for end-to-end learning of (neural)
dynamics, (neural) policies, and (neural) control objective functions. Furthermore, we introduce an
auxiliary control system in the back pass of the PDP framework, and its output trajectory is exactly
the derivative of the trajectory with respect to the parameters, which can be iteratively solved using
standard control tools. In control ﬁelds, to our best knowledge, this is the ﬁrst work to propose the
technique of the differential PMP, and more importantly, we show that the differential PMP can be
easily obtained using the introduced auxiliary control system.

3 Problem Formulation

We begin with formulating a base problem and then discuss how to accommodate the base problem
to speciﬁc applications. Consider a class of optimal control systems Σ(θ), which is parameterized by
a tunable θ ∈ Rr in both dynamics and control (cost) objective function:

Σ(θ) :

dynamics:

xt+1 = f (xt, ut, θ) with given x0,

control objective:

J(θ) =

(cid:88)T −1
t=0

ct(xt, ut, θ) + h(xT , θ).

(1)

Here, xt ∈ Rn is the system state; ut ∈ Rm is the control input; f : Rn × Rm × Rr (cid:55)→ Rn is the
dynamics model, which is assumed to be twice-differentiable; t = 0, 1, · · · , T is the time step with
T being the time horizon; and J(θ) is the control objective function with ct : Rn × Rm × Rr (cid:55)→ R
and h : Rn × Rr (cid:55)→ R denoting the stage/running and ﬁnal costs, respectively, both of which are
twice-differentiable. For a choice of θ, Σ(θ) will produce a trajectory of state-inputs:

ξθ={xθ

0:T , uθ

0:T −1} ∈ arg min{x0:T ,u0:T -1} J(θ)

subject to

xt+1=f (xt, ut, θ) for all t given x0

,

(2)

that is, ξθ optimizes J(θ) subject to the dynamics constraint f (θ). For many applications (we will
show next), one evaluates the above ξθ using a scalar-valued differentiable loss L(ξθ, θ). Then, the
problem of interest is to tune the parameter θ, such that ξθ has the minimal loss:

min
θ

L(ξθ, θ)

subject to ξθ is in (2).

(3)

Under the above base formulation, for a speciﬁc learning or control task, one only needs to accordingly
change precise details of Σ(θ) and deﬁne a speciﬁc loss function L(ξθ, θ), as we discuss below.

3

Suppose that we are given optimal demonstrations ξd = {xd
0:T −1} of an
IRL/IOC Mode.
expert optimal control system. We seek to learn the expert’s dynamics and control objective function
from ξd. To this end, we use Σ(θ) in (1) to represent the expert, and deﬁne the loss in (3) as

0:T , ud

L(ξθ, θ) = l(ξθ, ξd),
(4)
where l is a scalar function that penalizes the inconsistency of ξθ with ξd, e.g., l(ξθ, ξd) = (cid:107)ξθ −ξd(cid:107)2.
By solving (3) with (4), we can obtain a Σ(θ∗) whose trajectory is consistent with the observed
demonstrations. It should be noted that even if the demonstrations ξd signiﬁcantly deviate from the
optimal ones, the above formulation still ﬁnds the ‘best’ control objective function (and dynamics)
within the parameterized set Σ(θ) such that its reproduced ξθ in (2) has the minimal distance to ξd.
0:T , u0:T −1} collected from, say, a physical
SysID Mode.
system (here, unlike ξd, ξo is not necessarily optimal), and we wish to identify the system’s dynamics.
Here, u0:T −1 are usually externally supplied to ensure the physical system is of persistent excitation
[54]. In order for Σ(θ) in (1) to only represent dynamics (as we do not care about its internal control
law), we set J(θ) = 0. Then, ξθ in (2) accepts any uθ
0:T −1 = u0:T −1 as it always optimizes J(θ)=0.
In other words, by setting J(θ) = 0, Σ(θ) in (1) now only represent a class of dynamics models:

Suppose that we are given data ξo = {xo

with x0 and uθ

0:T −1 = u0:T −1.

(5)

Σ(θ) :
Now, Σ(θ) produces ξθ = {xθ

dynamics: xt+1 = f (xt, ut, θ)
0:T , uθ

0:T −1} subject to (5). To use (3) for identifying θ, we deﬁne

where l is to quantify the prediction error between ξo and ξθ under the same inputs u0:T −1.

L(ξθ, θ) = l(ξθ, ξo),

(6)

Control/Planning Mode. Consider a system with its dynamics learned in the above SysID. We
want to obtain a feedback controller or trajectory such that the system achieves a performance of
minimizing a given cost function. To that end, we specialize Σ(θ) in (1) as follows: ﬁrst, set f as the
learned dynamics and J(θ) = 0; and second, through a close-loop link, we connect the input ut and
state xt via a parameterized policy block ut = u(t, xt, θ) (reminder: unlike SysID Mode with ut
supplied externally, the inputs here are from a policy via a feedback loop). Σ(θ) now becomes

Σ(θ) :

Now, Σ(θ) produces a trajectory ξθ = {xθ

dynamics: xt+1 = f (xt, ut) with

x0,

control policy: ut = u(t, xt, θ).
0:T , uθ
(cid:88)T −1
t=0

L(ξθ, θ) =

t , uθ

0:T −1} subject to (7). We set the loss in (3) as
l(xθ

t ) + lf (xθ

T ),

(7)

(8)

where l and lf are the stage and ﬁnal costs, respectively. Then, (3) is an optimal control or planning
problem: if ut=u(t, xt, θ) (i.e., feedback policy explicitly depends on xt), (3) is a close-loop optimal
control problem; otherwise if ut=u(t, θ) (e.g., polynomial parameterization), (3) is an open-loop
motion planning problem. This mode can also be used as a component to solve (1) in IRL/IOC Mode.

4 An End-to-End Learning Framework

To solve the generic problem in (3), the idea of end-to-end learning [53] seeks to optimize the loss
L(ξθ, θ) directly with respect to the tunable parameter θ, by applying the gradient descent

θk+1 = θk − ηk

dL
dθ

(cid:12)
(cid:12)
(cid:12)θk

with

dL
dθ

(cid:12)
(cid:12)
(cid:12)θk

=

∂L
∂ξ

(cid:12)
(cid:12)
(cid:12)ξθk

∂ξθ
∂θ

(cid:12)
(cid:12)
(cid:12)θk

+

∂L
∂θ

(cid:12)
(cid:12)
(cid:12)θk

.

(9)

Here, k = 0, 1, · · · is the iteration index; dL
is the gradient of the loss with respect to θ evaluated
dθ
at θk; and ηk is the learning rate. From (9), we can draw a learning architecture in Fig. 2. Each update
of θ consists of a forward pass, where at θk, the corresponding trajectory ξθk is solved from Σ(θk)
and the loss is computed, and a backward pass, where ∂L
∂ξ

are computed.

, and ∂L
∂θ

, ∂ξθ
∂θ

(cid:12)
(cid:12)θk

(cid:12)
(cid:12)θk

(cid:12)
(cid:12)ξθk

(cid:12)
(cid:12)θk

In the forward pass, ξθ is obtained by solving an optimal control problem in Σ(θ) using any available
OC methods, such as iLQR or Control/Planning Mode, (note that in SysID or Control/Planning
∂ξ and ∂L
modes, it is reduced to integrating difference equations (5) or (7)). In backward pass, ∂L
∂θ are
easily obtained from the loss function L(ξθ, θ). The main challenge, however, is to solve ∂ξθ
∂θ , i.e.,
the derivative of a trajectory with respect to the parameters in the system. Next, we will analytically
solve ∂ξθ

∂θ by proposing two techniques: differential PMP and auxiliary control system.

4

Figure 2: PDP end-to-end learning framework.

5 Key Contributions: Differential PMP & Auxiliary Control System

We ﬁrst recall the discrete-time Pontryagin’s Maximum/Minimum Principle (PMP) [39] (a derivation
of discrete-time PMP is given in Appendix C). For the optimal control system Σ(θ) in (1) with a
0:T , uθ
ﬁxed θ, PMP describes a set of optimality conditions which the trajectory ξθ = {xθ
0:T −1} in
(2) must satisfy. To introduce these conditions, we ﬁrst deﬁne the following Hamiltonian,

Ht = ct(xt, ut; θ) + f (xt, ut; θ)(cid:48)λt+1,
(10)
where λt ∈ Rn (t = 1, 2, · · · , T ) is called the costate variable, which can be also thought of as the
Lagrange multipliers for the dynamics constraints. According to PMP, there exists a sequence of
costates λθ

1:T , which together with the optimal trajectory ξθ = {xθ

0:T −1} satisfy

0:T , uθ

dynamics equation:

xθ

t+1 =

costate equation:

input equation:

boundary conditions:

λθ

t =

0 =

λθ

T =

t+1

∂Ht
∂λθ
∂Ht
∂xθ
t
∂Ht
∂uθ
t
∂h
∂xθ
T

,

=

=

= f (xθ

t , uθ

t ; θ),

∂f (cid:48)
∂xθ
t
∂f (cid:48)
∂uθ
t

λθ

t+1,

λθ

t+1,

+

+

∂ct
∂xθ
t
∂ct
∂uθ
t
xθ

0 = x0.

(11a)

(11b)

(11c)

(11d)

For notation simplicity, ∂g
∂xt

means the derivative of function g(x) with respect to x evaluated at xt.

5.1 Differential PMP

To begin, recall that our goal (in Section 4) is to obtain ∂ξθ

∂θ , that is,
(cid:27)

∂ξθ
∂θ

=

(cid:26) ∂xθ
∂θ

0:T

,

∂uθ

0:T −1
∂θ

.

(12)

To this end, we are motivated to differentiate the PMP conditions in (11) on both sides with respect to
θ. This leads to the following differential PMP:

differential dynamics equation:

differential costate equation:

differential input equation:

differential boundary conditions:

t+1

∂xθ
∂θ
∂λθ
t
∂θ

= Ft

∂xθ
t
∂θ

+ Gt

∂uθ
t
∂θ

+ Et,

= H xx

t

0 = H ux

t

∂λθ
T
∂θ

= H xx
T

∂xθ
t
∂θ
∂xθ
t
∂θ
∂xθ
T
∂θ

t+1

+ H xu
t

+ H uu
t

∂uθ
t
∂θ
∂uθ
t
∂θ

+ F (cid:48)
t

+ G(cid:48)
t

∂λθ
∂θ
∂λθ
∂θ

t+1

+ H xe

t

,

+ H ue

t

,

+ H xe
T ,

∂xθ
0
∂θ

=

∂x0
∂θ

= 0.

(13a)

(13b)

(13c)

(13d)

Here, to simplify notations and distinguish knowns and unknowns, the coefﬁcient matrices in the
above differential PMP (13) are deﬁned as follows:

Ft=

Et=

∂f
∂xθ
t
∂f
∂θ

,

Gt=

, H uu

t =

,

∂f
∂uθ
t
∂2Ht
t ∂uθ
t

∂uθ

H xx

t =

, H ue

t =

∂2Ht
t ∂xθ
t

∂xθ
∂2Ht
∂uθ
t ∂θ

, H xe

t =

, H xx

T =

,

∂2Ht
∂xθ
t ∂θ
∂2h
T ∂xθ
T

∂xθ

H xu

t =

, H xe

T =

∂xθ

∂2Ht
t ∂uθ
t
∂2h
T ∂θ

,

∂xθ

=(H ux

t )(cid:48),

(14a)

(14b)

where we use
Since the trajectory ξθ = {xθ

to denote the second-order derivative of a function g(x, u) evaluated at (xt, ut).
0:T −1} is obtained in the forward pass (recall Fig. 2), all matrices

0:T , uθ

∂2g
∂xt∂ut

5

LossAuxiliary control system Chain ruleUpdateParameterizedcontrolsystemSystemtrajectoryin (14) are thus known (note that the computation of these matrices also requires λθ
1:T , which can be
obtained by iteratively solving (11b) and (11d) given ξθ). From the differential PMP in (13), we note
that to obtain ∂ξθ
in (13).
Next we will show that how these unknowns are elegantly solved by introducing a new system.

∂θ in (12), it is sufﬁcient to compute the unknowns

, ∂λθ

0:T −1
∂θ

∂θ ,

(cid:110) ∂xθ

∂xθ

(cid:111)

∂θ

0:T

1:T

5.2 Auxiliary Control System

One important observation to the differential PMP in (13) is that it shares a similar structure to the
original PMP in (11); so it can be viewed as a new set of PMP equations corresponding to an ‘oracle
control optimal system’ whose the ‘optimal trajectory’ is exactly (12). This motivates us to ‘unearth’
this oracle optimal control system, because by doing so, (12) can be obtained from this oracle system
by an OC solver. To this end, we ﬁrst deﬁne the new ‘state’ and ’control’ (matrix) variables:

Xt =

∂xt
∂θ

∈ Rn×r,

Ut =

∂ut
∂θ

∈ Rm×r,

(15)

respectively. Then, we ‘artiﬁcially’ deﬁne the following auxiliary control system Σ(ξθ):

dynamics: Xt+1 = FtXt + GtUt + Et with X0 = 0,

Σ(ξθ) :

control objective:

¯J = Tr

(cid:32)

T −1
(cid:88)

t=0

1
2

(cid:20)Xt
Ut

(cid:21)(cid:48) (cid:20)H xx
t
H ux
t

H xu
t
H uu
t

(cid:21) (cid:33)

+

(cid:20)H xe
t
H ue
t

(cid:21)(cid:48) (cid:20)Xt
Ut

(16)

(cid:21)

(cid:21) (cid:20)Xt
Ut
(cid:19)

+ Tr

(cid:18) 1
2

X (cid:48)

T H xx

T UT + (H xe

T )(cid:48) XT

.

∂θ = 0 because x0 in (1) is given; ¯J is the deﬁned control objective function which
Here, X0 = ∂x0
needs to be optimized in the auxiliary control system; and Tr denotes matrix trace. Before presenting
the key results, we make some comments on the above auxiliary control system Σ(ξθ). First, its
state and control variables are both matrix variables deﬁned in (15). Second, its dynamics is linear
and control objective function ¯J is quadratic, for which the coefﬁcient matrices are given in (14).
Third, its dynamics and objective function are determined by the trajectory ξθ of the system Σ(θ) in
forward pass, and this is why we denote it as Σ(ξθ). Finally, we have the following important result.
0:T , U θ
Lemma 5.1. Let {X θ
0:T −1} be a stationary solution to the auxiliary control system Σ(ξθ) in
0:T , U θ
(16). Then, {X θ
0:T −1} satisﬁes Pontryagin’s Maximum Principle of Σ(ξθ), which is (13), and

{X θ

0:T , U θ

0:T −1} =

(cid:26) ∂xθ
∂θ

0:T

,

∂uθ

0:T −1
∂θ

(cid:27)

=

∂ξθ
∂θ

.

(17)

A proof of Lemma 5.1 is in Appendix A. Lemma 5.1 states two assertions. First, the PMP condition
for the auxiliary control system Σ(ξθ) is exactly the differential PMP in (13) for the original system
Σ(θ); and second, importantly, the trajectory {X θ
0:T −1} produced by the auxiliary control
system Σ(ξθ) is exactly the derivative of trajectory of the original system Σ(θ) with respect to the
parameter θ. Based on Lemma 5.1, we can obtain ∂ξθ
∂θ from Σ(ξθ) efﬁciently by the lemma below.
Lemma 5.2. If H uu
in (16) is invertible for all t = 0, 1 · · · , T − 1, deﬁne the following recursions

0:T , U θ

t

(18a)

(18b)

Pt = Qt + A(cid:48)
Wt = A(cid:48)

t(I + Pt+1Rt)−1Pt+1At,

T and WT = H xe

with PT = H xx
t )-1G(cid:48)
Gt(H uu
are all known given (14). Then, the stationary solution {X θ
iteratively solving the following equations from t = 0 to T − 1 with X θ

t )-1H ux
0:T , U θ

t, Mt=Et−Gt(H uu

t )-1H ue
t

, Qt=H xx

t −H xu

t(I + Pt+1Rt)−1(Wt+1+P t+1Mt) + Nt,
T . Here, I is identity matrix, At=Ft − Gt(H uu
t (H uu

t )-1H ux

, Rt =
t )-1H ue
t −H xu
t
0:T −1} in (17) can be obtained by

t
t (H uu

, Nt=H xe

t

0 = X0 = 0:

U θ

t = −(H uu

t

)-1 (cid:16)

H ux

t X θ

t + H ue

t + Gt

X θ

t+1 = FtX θ

t + GtU θ

t + Et.

(cid:48)(I + Pt+1Rt)−1(cid:16)

Pt+1AtX θ

t + Pt+1Mt + Wt+1

(cid:17)(cid:17)

,

(19a)

(19b)

A proof of Lemma 5.2 is in Appendix B. Lemma 5.2 states that the trajectory of the above auxiliary
control system Σ(ξθ) can be obtained by two steps: ﬁrst, iteratively solve (18) backward in time to

6

obtain matrices Pt and Wt (all other coefﬁcient matrices are known given Σ(ξθ)); second, calculate
{X θ
0:T −1} by iteratively integrating a feedback-control system (19) forward in time. In fact,
these two steps constitute the standard procedure to solve general ﬁnite-time LQR problems [55].

0:T , U θ

As a conclusion to the techniques developed in Section 5, in Algorithm 1 we summarize the procedure
of computing ∂ξθ
∂θ via the introduced auxiliary control system. Algorithm 1 serves as a key component
in the backward pass of the PDP learning framework, as shown in Fig. 2.
Algorithm 1: Solving ∂ξθ
Input: The trajectory ξθ in (2) produced by the system Σ(θ) in (1) in the forward pass.

∂θ using Auxiliary Control System (See details in Appendix D)

Compute the coefﬁcient matrices (14) to obtain the auxiliary control system Σ(ξθ) in (16);
Solve the auxiliary control system Σ(ξθ) to obtain {X θ

0:T −1} using Lemma 5.2;

0:T , U θ

Return: ∂ξθ

∂θ = {X θ

0:T , U θ

0:T −1}

6 Applications to Different Learning Modes and Experiments
We investigate three learning modes of PDP, as described in Section 3. For each mode, we demonstrate
its capability in four environments listed in Table 2, and a baseline and a state-of-the-art method are
compared. Both PDP and environment codes are available at https://github.com/wanxinjin.
Table 2: Experimental environments (results for 6-DoF rocket landing is in Appendix I)

Systems

Dynamics parameter θdyn

Control objective parameter θobj

Cartpole
Two-link robot arm
6-DoF quadrotor maneuvering mass, wing length, inertia matrix
6-DoF rocket powered landing mass, rocket length, inertia matrix

cart mass, pole mass and length
length and mass for each link

c(x, u)=(cid:107)θ(cid:48)

h(x, u) = (cid:107)θ(cid:48)

obj(x − xg)(cid:107)2+(cid:107)u(cid:107)2
obj(x − xg)(cid:107)2

We ﬁx the unit weight to (cid:107)u(cid:107)2, because estimating all weights will incur ambiguity [48]; xg is the goal state.
IRL/IOC Mode. The parameterized Σ(θ) is in (1) and the loss in (4). In the forward pass of PDP,
ξθ is solved from Σ(θ) by any OC solver. In the backward pass, ∂ξθ
∂θ is computed from the auxiliary
control system Σ(ξθ) in (16) using Algorithm 1. The full algorithm is in Appendix D.
Experiment: imitation learning. We use IRL/IOC Mode to solve imitation learning in environments
in Table 2. The true dynamics is parameterized, and control objective is parameterized as a weighted
distance to the goal, θ = {θdyn, θobj}. Set imitation loss L(ξθ, θ)=(cid:107)ξd − ξθ(cid:107)2. Two other methods are
compared: (i) neural policy cloning, and (ii) inverse KKT [52]. We set learning rate η = 10−4 and run
ﬁve trials given random initial θ0. The results in Fig. 3a-3c show that PDP signiﬁcantly outperforms
the policy cloning and inverse-KKT for a much lower training loss and faster convergence. In Fig.
3d, we apply the PDP to learn a neural control objective function for the robot arm using the same
demonstration data in Fig. 3b, and we also compare with the GAIL [56]. Results in Fig. 3d show that
the PDP successfully learns a neural objective function and the imitation loss of PDP is much lower
than that of GAIL. It should note that because the demonstrations are not strictly realizable (optimal)
under the parameterized neural objective function, the ﬁnal loss for the PDP is small but not zero.
This indicates that given sub-optimal demonstrations, PDP can still ﬁnd the ‘best’ control objective
function within the function set J(θ) such that its reproduced ξθ has the minimal distance to the
demonstrations. Please refer to Appendix E.2 for more experiment details and additional validations.

(a) Cart-pole

(b) Robot arm

(c) Quadrotor

(d) Comparison

Figure 3: (a-c) imitation loss v.s. iteration, (d) PDP learns a neural objective function and comparison.

SysID Mode. In this mode, Σ(θ) is (5) and loss is (6). PDP is greatly simpliﬁed: in forward pass,
ξθ is solved by integrating the difference equation (5). In the backward pass, Σ(ξθ) is reduced to
Σ(ξθ) :
(20)

t + Et with X0 = 0.

dynamics: X θ

t+1 = FtX θ

7

0200040006000800010000Iteration104103102101100101102103104Imitation LossPDPInverse KKTPolicy cloning0200040006000800010000Iteration105103101101103Imitation LossPDPInverse KKTPolicy cloning0200040006000800010000Iteration104103102101100101102103104Imitation LossPDPInverse KKTPolicy cloning0100020003000Iteration100101102103Imitation LossPDPGAILt = 0 in (13a). The algorithm is in Appendix D.

This is because Σ(θ) in (5) results from letting J(θ) = 0, (13b-13d) and ¯J in (16) are then trivialized,
and due to u0:T −1 given, U θ
Experiment: system identiﬁcation. We use the SysID Mode to identify the dynamics parameter
θdyn for the systems in Table 2. Set the SysID loss L(ξθ, θ) = (cid:107)ξo − ξθ(cid:107)2. Two other methods
are compared: (i) learning a neural network (NN) dynamics model, and (ii) DMDc [57]. For all
methods, we set learning rate η = 10−4, and run ﬁve trials with random θ0. The results are in Fig. 4.
Fig. 4a-4c show an obvious advantage of PDP over the NN baseline and DMDc in terms of lower
training loss and faster convergence speed. In Fig. 4d, we compare PDP and Adam [58] (here both
with η = 10−5) for training the same neural dynamics model for the robot arm. The results again
show that PDP outperforms Adam for faster learning speed and lower training loss. Such advantages
are due to that PDP has injected an inductive bias of optimal control into learning, making it more
efﬁcient for handling dynamical systems. More experiments and validations are in Appendix E.3.

(a) Cart-pole

(b) Robot arm

(c) Quadrotor

(d) Learn neural dynamics

Figure 4: (a-c) SysID loss v.s. iteration, (d) PDP learns a neural dynamics model.

Control/Planning Mode. The parameterized system Σ(θ) is (7) and loss is (8). PDP for this mode
is also simpliﬁed. In forward pass, ξθ is solved by integrating a (controlled) difference equation (7).
In backward pass, ¯J in the auxiliary control system (16) is trivialized because we have considered
J(θ) = 0 in (7). Since the control is now given by ut = u(t, xt, θ), U θ
t is obtained by differentiating
t = ∂ut
t with U x
the policy on both side with respect to θ, that is, U θ
∂θ . Thus,
∂xt
t+1 = FtX θ
t X θ
t = U x
U θ
0:T , U θ

t X θ
t = U x
t + GtU θ
t + U e
t .
0:T −1} = ∂ξθ

Integrating (21) from t = 0 to T leads to {X θ

∂θ . The algorithm is in Appendix D.

dynamics: X θ

t with X0 = 0,

control policy:

Σ(ξθ) :

t = ∂ut

and U e

t +U e

(21)

Experiment: control and planning. Based on identiﬁed dynamics, we learn policies of each system
to optimize a control objective with given θobj. We set loss (8) as the control objective (below called
control loss). To parameterize policy (7), we use a Lagrange polynomial of degree N (for planning) or
neural network (for feedback control). iLQR [38] and guided policy search (GPS) [59] are compared.
We set learning rate η=10−4 or 10−6 and run ﬁve trials for each system. Fig. 5a-5b are learning
neural network feedback policies for the cart-pole and robot arm, respectively. The results show that
PDP outperforms GPS for having lower control loss. Fig. 5c is motion planning for quadrotor using a
polynomial policy. It shows that PDP achieves a competitive performance with iLQR. Compared to
iLQR, PDP minimizes over polynomial policies instead of input sequences, and thus has a higher ﬁnal
loss which depends on the expressiveness of the polynomial: e.g., the polynomial of degree N =35
has a lower loss than that of N =5. Since iLQR can be viewed as ‘1.5-order’ method (discussed in
Section 2), it has faster converging speed than PDP which is only ﬁrst-order, as shown in Fig. 5c. But
iLQR is computationally extensive, PDP, instead, has a huge advantage of running time, as illustrated
in Fig. 5d. Due to space constraint, we put detailed analysis between GPS and PDP in Appendix E.4.

(a) Cart-pole control

(b) Robot arm control

(c) Quadrotor planning

(d) Timing results

Figure 5: (a-c) control loss v.s. iteration, (d) comparison for running time per iteration.

8

0200040006000800010000Iteration1025102110171013109105101103SysID LossPDPDMDcNN dynamics0200040006000800010000Iteration1019101610131010107104101102105SysID LossPDPDMDcNN dynamics0200040006000800010000Iteration1010108106104102100102104106SysID LossPDPDMDcNN dynamics0200040006000800010000Iteration2004006008001000SysID LossPDPPytorch Adam0100200300400Iteration130140150160170180190200Control lossPDP with neural policyGPS with neural policy0200400600800Iteration01020304050Control lossPDP with neural policyGPS with neural policy050100150Iteration1234Control loss×104iLQRPDP with poly policy (N=5)PDP with poly policy (N=35)solved by OC solverPDPiLQRGPS103102101100Time [s] per iterationForward passBackward passOverall7 Discussion

The related end-to-end learning frameworks. Two lines of recent work are related to PDP. One
is the recent work [60–64] that seeks to replace a layer within a deep neural network by an argmin
layer, in order to capture the information ﬂow characterized by a solution of an optimization. Similar
to PDP, these methods differentiate the argmin layer through KKT conditions. They mainly focus on
static optimization problems, which can not directly be applied to dynamical systems. The second
line is the recent RL development [65–68] that embeds an implicit planner within a policy. The idea
is analogous to MPC, because using a predictive OC system (i.e., embedded planner) to generate
controls leads to better adaption to unseen situations. The key problem in these methods is to learn
a planner (i.e., OC system), which is similar to our formulation. [65, 66] learn a path-integral OC
system [69], which is a special class of OC systems. [68] learns an OC system in a latent space.
However, all these methods adopt the ‘unrolling’ strategy to facilitate differentiation. Speciﬁcally,
they treat the forward pass of solving an OC problem as an ‘unrolled’ computational graph of multiple
steps of applying gradient descent, because by this computational graph, automatic differentiation
tool [70] can be immediately applied. The drawbacks of this ‘unrolling’ strategy are apparent: (i) they
need to store all intermediate results over the entire computational graph, thus are memory-expensive;
and (ii) the accuracy of gradient depends on the length of the ‘unrolled’ graph, thus facing trade-off
between complexity and accuracy. To address these, [67] develops a differentiable MPC framework,
where in forward pass, a LQR approximation of the OC system is obtained, and in backward pass, the
gradient is solved by differentiating such LQR approximation. Although promising, this framework
has one main weakness: differentiating LQR requires to solve a large linear equation, which involves
the inverse of a matrix of size (2n+m)T × (2n+m)T , thus can incur huge cost when handling
systems of longer horizons T . Detailed descriptions for all these methods is in Appendix F.

Compared to [35, 65–68], the efﬁciency of PDP stems
from the following novel aspects. First, in forward pass,
without needing an unrolled computational graph, PDP
only computes and stores the resulting trajectory of the
OC system, ξθ, (does not care about how ξθ is solved).
Second, without obtaining intermediate (LQR) approx-
imations, PDP differentiates through PMP of the OC
system to directly obtain the exact analytical gradient.
Figure 6: Runtime (per iteration) compari-
Third, in the backward pass, unlike differentiable MPC
which costs at least a complexity of O (cid:0)(m+2n)2T 2(cid:1)
son between PDP and differentiable MPC
for varying horizons of a pendulum system.
to differentiate a LQR approximation, PDP explicitly
solves ∂ξθ
∂θ by an auxiliary control system, where thanks to the recursion structure, the memory and
comptuation complexity of PDP is only O ((m+2n)T ). In Fig. 6, we have compared the running
time of PDP with that of differentiable MPC. The results show PDP is 1000x faster than differentiable
MPC. Due to space constraint, we put the detailed complexity analysis of PDP in Appendix G.

Convergence and limitation of PDP. Since all gradient quantities in PDP are analytical and exact,
and the development of PDP does not involves any second-order derivative of functions or models,
PDP essentially is a ﬁrst-order gradient-descent framework to solve non-convex bi-level optimization.
Therefore, in general, PDP can only achieve local minima. As explored by [71], if we pose further
assumptions such as convexity and smoothness on all functions (dynamics, policy, loss, and control
objective function), the global convergence of the bi-level programming could be established. But we
do think these conditions are too restrictive for dynamical control systems. As a direction of future
work, we will investigate the mild conditions for good convergence by taking advantage of control
theory, e.g., Lyapunov theory. Due to space constraint, limitation of PDP is detailed in Appendix H.

8 Conclusions

This paper proposes a Pontryagin differentiable programming (PDP) methodology to establish an
end-to-end learning framework for solving a range of learning and control tasks. The key contribution
in PDP is that we incorporate the knowledge of optimal control theory as an inductive bias into the
learning framework. Such combination enables PDP to achieve higher efﬁciency and capability than
existing learning and control methods in solving many tasks including inverse reinforcement learning,
system identiﬁcation, and control/planning. We envision the proposed PDP could beneﬁt to both
learning and control ﬁelds for solving many high-dimensional continuous-space problems.

9

Broader Impact

This work is expected to have the impacts on both learning and control ﬁelds.

• To the learning ﬁeld, this work connects some fundamental topics in machine learning to
their counterparts in the control ﬁeld, and uniﬁes some concepts from reinforcement learning,
backpropagation/deep learning, and control theory in one generic learning framework. The
contribution of this framework is a deep integration of optimal control theory into end-to-end
learning process, leading to an optimal-control-informed end-to-end learning framework
that is ﬂexible enough to solve a broad range of learning and control tasks and efﬁcient
enough to handle high-dimensional and continuous-space problems. In a broad perspective,
we hope that this paper could motivate more future work that integrates the beneﬁts of both
control and learning to promote efﬁciency and explainability of artiﬁcial intelligence.

• To the control ﬁeld, this work proposes a generic paradigm, which shows how a challenging
control task can be converted into a learning formulation and solved using readily-available
learning techniques, such as (deep) neural networks and backpropagation. For example, the
proposed framework, equipped with (deep) neural networks, shows signiﬁcant advantage for
handling non-linear system identiﬁcation and optimal control over state-of-the-art control
methods. Since classic control theory typically requires knowledge of models, we expect
that this work could pave a new way to extend classic control with data-driven techniques.

Since the formulation of this paper does not consider the boundness or constraints of a decision-
making system, the real-world use of this work on physical systems might possibly raise safety issues
during the training process; e.g., the state or input of the physical system at some time instance might
exceeds the safety bounds that are physically required. One option to address this is to include these
safety boundness as soft constraints added to the control objective or loss that is optimized. In future
work, we will formally discuss PDP within a safety framework.

Acknowledgments and Disclosure of Funding

We acknowledge support for this research from Northrop Grumman Mission Systems’ University
Research Program.

References

[1] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
In International Conference on Machine Learning, pages

with model-based acceleration.
2829–2838, 2016.

[2] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa.
Learning continuous control policies by stochastic value gradients. In Advances in Neural
Information Processing Systems, pages 2944–2952, 2015.

[3] Joshua L Proctor, Steven L Brunton, and J Nathan Kutz. Generalizing koopman theory to allow
for inputs and control. SIAM Journal on Applied Dynamical Systems, 17(1):909–930, 2018.

[4] Ian Abraham and Todd D Murphey. Active learning of dynamics for data-driven control using

koopman operators. IEEE Transactions on Robotics, 35(5):1071–1083, 2019.

[5] Rolf Johansson. System modeling and identiﬁcation. Prentice Hall, 1993.

[6] Bernard O Koopman. Hamiltonian systems and transformation in hilbert space. Proceedings of

the National Academy of Sciences of the United States of America, 17(5):315, 1931.

[7] Matthew O Williams, Ioannis G Kevrekidis, and Clarence W Rowley. A data–driven approxima-
tion of the koopman operator: Extending dynamic mode decomposition. Journal of Nonlinear
Science, 25(6):1307–1346, 2015.

[8] Masahiko Haruno, Daniel M Wolpert, and Mitsuo Kawato. Mosaic model for sensorimotor

learning and control. Neural computation, 13(10):2201–2220, 2001.

10

[9] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interac-
tion through video prediction. In Advances in Neural Information Processing Systems, pages
64–72, 2016.

[10] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to
control: A locally linear latent dynamics model for control from raw images. In Advances in
Neural Information Processing Systems, pages 2746–2754, 2015.

[11] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. Recurrent network
models for human dynamics. In IEEE International Conference on Computer Vision, pages
4346–4354, 2015.

[12] Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efﬁcient
approach to policy search. In International Conference on Machine Learning, pages 465–472,
2011.

[13] Amy Zhang, Sainbayar Sukhbaatar, Adam Lerer, Arthur Szlam, and Rob Fergus. Composable
planning with attributes. In International Conference on Machine Learning, pages 5842–5851,
2018.

[14] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686–707, 2019.

[15] Steindor Saemundsson, Alexander Terenin, Katja Hofmann, and Marc Deisenroth. Variational
integrator networks for physically structured embeddings. In International Conference on
Artiﬁcial Intelligence and Statistics, pages 3078–3087, 2020.

[16] Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as

model prior for deep learning. arXiv preprint arXiv:1907.04490, 2019.

[17] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning

hamiltonian dynamics with control. arXiv preprint arXiv:1909.12077, 2019.

[18] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in Neural Information Processing Systems, pages 6571–6583,
2018.

[19] Jiequn Han and Weinan E. Deep learning approximation for stochastic control problems. Deep
Reinforcement Learning Workshop, Advances in Neural Information Processing Systems, 2016.

[20] Qianxiao Li, Long Chen, Cheng Tai, and E Weinan. Maximum principle based algorithms for

deep learning. Journal of Machine Learning Research, 18(1):5998–6026, 2017.

[21] Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to

discrete-weight neural networks. arXiv preprint arXiv:1803.01299, 2018.

[22] Weinan E, Jiequn Han, and Qianxiao Li. A mean-ﬁeld optimal control formulation of deep

learning. Research in the Mathematical Sciences, 6(1), 2019.

[23] Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Painless adversarial training using maximal principle. arXiv preprint arXiv:1905.00877,
2019.

[24] Martin Benning, Elena Celledoni, Matthias J Ehrhardt, Brynjulf Owren, and Carola-Bibiane
Schönlieb. Deep learning as optimal control problems: models and numerical methods. arXiv
preprint arXiv:1904.05657, 2019.

[25] Hailiang Liu and Peter Markowich. Selection dynamics for deep neural networks. arXiv preprint

arXiv:1905.09076, 2019.

[26] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory,

active perception, and action in minecraft. arXiv preprint arXiv:1605.09128, 2016.

11

[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.

[29] Jeff G Schneider. Exploiting model uncertainty estimates for safe dynamic control learning. In

Advances in Neural Information Processing Systems, pages 1047–1053, 1997.

[30] Pieter Abbeel, Morgan Quigley, and Andrew Y Ng. Using inaccurate models in reinforcement

learning. In International Conference on Machine Learning, pages 1–8, 2006.

[31] Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search
under unknown dynamics. In Advances in Neural Information Processing Systems, pages
1071–1079, 2014.

[32] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM

Sigart Bulletin, 2(4):160–163, 1991.

[33] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel, Sergey Levine, Kate
Saenko, and Trevor Darrell. Adapting deep visuomotor representations with weak pairwise
constraints. In Algorithmic Foundations of Robotics XII, pages 688–703. Springer, 2020.

[34] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:
Model-based policy optimization. In Advances in Neural Information Processing Systems,
pages 12519–12530, 2019.

[35] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks.

In Advances in Neural Information Processing Systems, pages 2154–2162, 2016.

[36] Jiongmin Yong and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations,

volume 43. Springer Science & Business Media, 1999.

[37] David H Jacobson and David Q Mayne. Differential dynamic programming. 1970.

[38] Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear bio-
logical movement systems. In International Conference on Informatics in Control, Automation
and Robotics, pages 222–229, 2004.

[39] Lev Semenovich Pontryagin, V. G. Boltyanskiy, R. V. Gamkrelidze, and E. F. Mishchenko. The

Mathematical Theory of Optimal Processes. John Wiley & Sons, Inc., 1962.

[40] Hans Georg Bock and Karl-Josef Plitt. A multiple shooting algorithm for direct solution of

optimal control problems. IFAC Proceedings Volumes, 17(2):1603–1608, 1984.

[41] Michael A Patterson and Anil V Rao. Gpops-ii: A matlab software for solving multiple-phase
optimal control problems using hp-adaptive gaussian quadrature collocation methods and sparse
nonlinear programming. ACM Transactions on Mathematical Software, 41(1):1, 2014.

[42] Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer Science &

Business Media, 2013.

[43] S Joe Qin and Thomas A Badgwell. An overview of nonlinear model predictive control

applications. In Nonlinear model predictive control, pages 369–392. Springer, 2000.

[44] Yang Wang and Stephen Boyd. Fast model predictive control using online optimization. IEEE

Transactions on control systems technology, 18(2):267–278, 2009.

[45] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning.

In International Conference on Machine Learning, pages 1–8, 2004.

12

[46] Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy
inverse reinforcement learning. In AAAI Conference on Artiﬁcial Intelligence, pages 1433–1438,
2008.

[47] Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In

International Conference on Machine Learning, pages 729–736, 2006.

[48] Arezou Keshavarz, Yang Wang, and Stephen Boyd. Imputing a convex objective function. In

IEEE International Symposium on Intelligent Control, pages 613–619, 2011.

[49] Katja Mombaur, Anh Truong, and Jean-Paul Laumond. From human to humanoid locomo-
tion—an inverse optimal control approach. Autonomous Robots, 28(3):369–383, 2010.

[50] Wanxin Jin, Dana Kuli´c, Shaoshuai Mou, and Sandra Hirche. Inverse optimal control from

incomplete trajectory observations. arXiv preprint arXiv:1803.07696, 2018.

[51] Wanxin Jin, Dana Kuli´c, Jonathan Feng-Shun Lin, Shaoshuai Mou, and Sandra Hirche. Inverse
optimal control for multiphase cost functions. IEEE Transactions on Robotics, 35(6):1387–1398,
2019.

[52] Peter Englert, Ngo Anh Vien, and Marc Toussaint. Inverse kkt: Learning cost functions of
manipulation tasks from demonstrations. The International Journal of Robotics Research,
36(13-14):1474–1488, 2017.

[53] Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, and Yann L Cun. Off-road obstacle avoidance
through end-to-end learning. In Advances in Neural Information Processing Systems, pages
739–746, 2006.

[54] Michael Green and John B Moore. Persistence of excitation in linear systems. Systems &

control letters, 7(5):351–360, 1986.

[55] Huibert Kwakernaak and Raphael Sivan. Linear optimal control systems, volume 1. New York:

Wiley-Interscience, 1972.

[56] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. arXiv preprint

arXiv:1606.03476, 2016.

[57] Joshua L Proctor, Steven L Brunton, and J Nathan Kutz. Dynamic mode decomposition with

control. SIAM Journal on Applied Dynamical Systems, 15(1):142–161, 2016.

[58] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International

Conference on Learning Representations, 2015.

[59] Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on

Machine Learning, pages 1–9, 2013.

[60] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural

networks. International Conference on Machine Learning, 2017.

[61] Po-Wei Wang, Priya L Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning
and logical reasoning using a differentiable satisﬁability solver. International Conference on
Machine Learning, 2019.

[62] Bryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decision-
focused learning for combinatorial optimization. In AAAI Conference on Artiﬁcial Intelligence,
volume 33, pages 1658–1665, 2019.

[63] Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter.
End-to-end differentiable physics for learning and control. In Advances in Neural Information
Processing Systems, pages 7178–7189, 2018.

[64] Priya Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning in
stochastic optimization. In Advances in Neural Information Processing Systems, pages 5484–
5494, 2017.

13

[65] Masashi Okada, Luca Rigazio, and Takenobu Aoshima. Path integral networks: End-to-end

differentiable optimal control. arXiv preprint arXiv:1706.09597, 2017.

[66] Marcus Pereira, David D Fan, Gabriel Nakajima An, and Evangelos Theodorou. Mpc-inspired
neural network policies for sequential decision making. arXiv preprint arXiv:1802.05803, 2018.

[67] Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and J Zico Kolter. Differentiable mpc
for end-to-end planning and control. In Advances in Neural Information Processing Systems,
pages 8289–8300, 2018.

[68] Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal

planning networks. arXiv preprint arXiv:1804.00645, 2018.

[69] Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of

Statistical Mechanics: Theory and Experiment, 2005.

[70] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale
machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467,
2016.

[71] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv

preprint arXiv:1802.02246, 2018.

[72] Michael Athans. The matrix minimum principle. Information and Control, 11(5-6):592–606,

1967.

[73] Mordecai Avriel. Nonlinear programming: analysis and methods. Courier Corporation, 2003.

[74] Daniel Liberzon. Calculus of variations and optimal control theory: a concise introduction.

Princeton University Press, 2011.

[75] Jack B Kuipers. Quaternions and rotation sequences, volume 66. Princeton University Press,

1999.

[76] Taeyoung Lee, Melvin Leok, and N Harris McClamroch. Geometric tracking control of a
quadrotor uav on se(3). In IEEE Conference on Decision and Control, pages 5420–5425, 2010.

[77] Mark W Spong and Mathukumalli Vidyasagar. Robot dynamics and control. John Wiley &

Sons, 2008.

[78] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.

[79] Joel A E Andersson, Joris Gillis, Greg Horn, James B Rawlings, and Moritz Diehl. CasADi – A
software framework for nonlinear optimization and optimal control. Mathematical Programming
Computation, 11(1):1–36, 2019.

[80] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas,

graphs, and mathematical tables, volume 55. U.S. Government Printing Ofﬁce, 1948.

[81] Gamal Elnagar, Mohammad A Kazemi, and Mohsen Razzaghi. The pseudospectral legendre
method for discretizing optimal control problems. IEEE Transactions on Automatic Control,
40(10):1793–1796, 1995.

[82] Aviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, and Pieter Abbeel. Learning from
the hindsight plan—episodic mpc improvement. In IEEE International Conference on Robotics
and Automation, pages 336–343, 2017.

[83] Peng Xu, Fred Roosta, and Michael W Mahoney. Second-order optimization for non-convex
machine learning: An empirical study. In SIAM International Conference on Data Mining,
pages 199–207, 2020.

[84] Michael Szmuk and Behcet Acikmese. Successive convexiﬁcation for 6-dof mars rocket
powered landing with free-ﬁnal-time. In AIAA Guidance, Navigation, and Control Conference,
page 0617, 2018.

14

Supplementary materials for the Pontryagin Differentiable Programming paper

A Proof of Lemma 5.1

To prove Lemma 5.1, we just need to show that the Pontryagin’s Maximum Principle for the auxiliary
control system Σ(ξθ) in (16) is exactly the differential PMP in (13). To this end, we deﬁne the
following Hamiltonian for the auxiliary control system Σ(ξθ):

(cid:32)

¯Ht = Tr

(cid:34)Xt
Ut

(cid:35)(cid:48) (cid:34)H xx
t
H ux
t

1
2

(cid:35)

H xu
t
H uu
t

(cid:35) (cid:34)Xt
Ut

+

(cid:34)H xe
t
H ue
t

(cid:35)(cid:48) (cid:34)Xt
Ut

(cid:35) (cid:33)

+ Tr (cid:0)Λ(cid:48)

t+1(FtXt + GtUt + Et)(cid:1),

(S.1)

with t = 0, 1, · · · , T − 1. Here Λt+1 ∈ Rn×r denotes the costate (matrix) variables for the auxiliary
control system. Based on Section 3 in [72], there exists a sequence of costates Λθ
1:T , which together
the stationary solution {X θ
0:T −1} to the auxiliary control system must satisfy the following the
matrix version of PMP (we here follow the notation style used in (11)).

0:T , U θ

The dynamics equation:

∂ ¯Ht
∂Λθ

t+1

=

∂ Tr (cid:0)Λ(cid:48)

t+1(FtXt + GtUt + Et)(cid:1)

∂Λt+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t+1

Λt+1=Λθ
Xt=X θ
t
Ut=U θ
t

(S.2a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t+1

Λt+1=Λθ
Xt=X θ
t
Ut=U θ
t
(S.2b)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t+1

Λt+1=Λθ
Xt=X θ
t
Ut=U θ
t

(S.2c)

(S.2d)

= FtX θ

t + GtU θ

t + Et = 0.

The costate equation:
∂ ¯Ht
∂X θ
t

∂ Tr ( 1

2 X (cid:48)

=

= H xx

t X θ

t + H xu

t U θ

t + H xe

t + F (cid:48)

t Λθ

∂Xt
t+1 = Λθ
t .

tH xx

t Xt) + ∂ Tr (U (cid:48)

tH ux

t Xt) + ∂ Tr (H ex

t Xt) + ∂ Tr (Λ(cid:48)

t+1FtXt)

Input equation:

∂ ¯Ht
∂U θ
t

=

∂ Tr ( 1

2 U (cid:48)

tH uu

t Ut) + ∂ Tr (U (cid:48)

tH ux

t Xt) + ∂ Tr (H eu

t Ut) + ∂ Tr (Λ(cid:48)

t+1GtUt)

∂Ut

t X θ

t + H ue

t + G(cid:48)

tΛθ

t+1 = 0.

t U θ

= H uu

t + H ux
And boundary conditions:
2 X (cid:48)

∂ Tr( 1

Λθ

T =

T H xx

T XT ) + ∂ Tr((H xe

T )(cid:48)XT )

∂XT

(cid:12)
(cid:12)
(cid:12)
(cid:12)XT =X θ

T

= H xx

T X θ

T + H xe
T ,

and X θ

0 = 0. Note that in the above derivations, we used the following matrix calculus [72]:

= B(cid:48),

∂ Tr(AB)
∂A

∂f (A)
∂A(cid:48) =
and the following matrix trace properties:
Tr(A) = Tr(A(cid:48)), Tr(ABC) = Tr(BCA) = Tr(CAB), Tr(A+B) = Tr(A)+Tr(B). (S.4)

∂ Tr(X (cid:48)HX)
∂X

= HX + H (cid:48)X,

(S.3)

(cid:20) ∂f (A)
∂A

(cid:21)(cid:48)

,

Since the above obtained PMP equations (S.2) are the same with the differential PMP in (13), we thus
can conclude that the Pontryagin’s Maximum Principle of the auxiliary control system Σ(ξθ) in (16)
is exactly the differential PMP equations (13), and thus (17) holds. This completes the proof.

B Proof of Lemma 5.2

Based on Lemma 5.1 and its proof, we known that the PMP of the auxiliary control system, (S.2),
is exactly the differential PMP equations (13). Thus below, we only look at the differential PMP
equations in (S.2). From (S.2c), we solve for U θ

invertible):

t (if H uu

t

U θ

t = −(H uu

t )−1(cid:16)

H ux

t X θ

t + G(cid:48)

tΛθ

t+1 + H ue
t

(cid:17)

.

(S.5)

1

By substituting (S.5) into (S.2a) and (S.2b), respectively, and considering the deﬁnitions of matrices
At, Rt, Mt, Qt and Nt in (18), we have

t+1 + Mt,
t+1 + Nt,
for t = 0, 1, . . . , T − 1, and also the boundary condition in (S.2d)

t+1 = AtX θ
t = QtX θ
Λθ

t − RtΛθ
tΛθ
t + A(cid:48)

X θ

Λθ

T = H xx

T X θ

T + H xe
T ,

for t = T . Next, we prove that there exist matrices Pt and Wt such that

Λθ

t = PtX θ

t + Wt.

(S.6)

(S.7)

(S.8)

Proof by induction: (S.2d) shows that (S.8) holds for t = T if PT = H xx
(S.8) holds for t + 1, then by manipulating (S.6) and (S.7), we have

T and WT = H xe

T . Assume

t = (cid:0)Qt + A(cid:48)
Λθ
(cid:124)

t(I + Pt+1Rt)−1Pt+1At

(cid:123)(cid:122)
Pt

(cid:1)

(cid:125)

X θ

t + A(cid:48)
(cid:124)

t(I + Pt+1Rt)−1(Wt+1+P t+1Mt) + Nt
, (S.9)
(cid:125)
(cid:123)(cid:122)
Wt

which indicates (S.8) holds for t, if Pt and Wt satisfy (18a) and (18b), respectively. Substituting (S.8)
to (S.7) and also considering (S.5) will lead to (19a). (19b) directly results from (S.2a). We complete
the proof.

C Proof of the Discrete-Time Pontryagin’s Maximum Principle

We here provide an easy-approach derivation of the discrete-time PMP based on Karush-Kuhn-Tucker
(KKT) conditions in non-linear optimization [73]. The original derivation for continuous optimal
control systems uses the calculus of variation theory, which can be found in [39] and [74].

We view the optimal control system (1) with a ﬁxed θ as a constrained optimization problem, where
the objective function is given by J(θ) and the constraints given by dynamics f (θ). Deﬁne the
following Lagrangian for this constrained optimization problem:

L = J(θ) +

=

=

(cid:88)T −1
t=0

(cid:88)T −1
t=0

λ(cid:48)

t+1

(cid:0)f (xt, ut, θ) − xt+1

(cid:1)

(cid:88)T −1
t=0

(cid:18)

ct(xt, ut, θ) + λ(cid:48)

t+1

(cid:0)f (xt, ut, θ) − xt+1

(cid:1)

(cid:19)

+ h(xT , θ)

(S.10)

(cid:18)

Ht − λ(cid:48)

t+1xt+1

(cid:19)

+ h(xT , θ),

where λt is the Lagrange multiplier for the dynamics constraint for t = 1, 2, · · · , T , and the third
line in (S.10) is due to the deﬁnition of Hamiltonian in (10). According to the KKT conditions, for
the optimal solution ξθ = {xθ
1:T (in optimal control
they are called costates) such that the following ﬁrst-order conditions are satisﬁed:

0:T −1}, there must exist the multiplers λθ

0:T , uθ

∂L
∂λθ

1:T

= 0,

∂L
∂xθ

0:T

= 0,

∂L
∂uθ

0:T -1

= 0.

(S.11)

By extending the above three conditions in (S.11) at each λt, xt and ut, respectively, and particularly
taking care of xT , we will obtain

0 =

0 = f (xθ
∂Ht
∂xθ
t
∂Ht
∂uθ
t
∂h
∂xθ
T

0 =

0 =

t , uθ

− λθ

t+1,

t =

t ; θ) − xθ
∂ct
∂xθ
t
∂f (cid:48)
∂uθ
t

∂ct
∂uθ
t

+

+

=

− λθ
T ,

∂f (cid:48)
∂xθ
t

t+1 − λθ
λθ
t ,

λθ

t+1,

(S.12a)

(S.12b)

(S.12c)

(S.12d)

respectively, which are exactly the PMP equations in (11). This completes the proof.

2

D Algorithms Details for Different Learning Modes

Algorithm 2: Solving ∂ξθ
Input: The trajectory ξθ generated by the system Σ(θ)

∂θ using Auxiliary Control System

Compute the coefﬁcient matrices (14) to obtain the auxiliary control system Σ(ξθ) in
(16);
def Auxiliary_Control_System_Solver ( Σ(ξθ) ): (cid:46) implementation of Lemma 5.2

Set PT = H xx
for t ← T to 0 by −1 do

T and WT = H xe
T ;

Update Pt and Wt using equations (18);

(cid:46) backward in time

end
Set X θ
for t ← 0 to T by 1 do
t and U θ

Update X θ

0 = 0;

end
Return: {X θ
∂θ = {X θ

0:T , U θ

0:T , U θ
0:T −1}

0:T −1}

Return: ∂ξθ

t using equations (19);

(cid:46) forward in time

Algorithm 3: PDP Algorithm for IRL/IOC Mode
Data :Expert demonstrations {ξd}
Parameterization: The parameterized optimal control system Σ(θ) in (1)
Loss: L(ξθ, θ) in (4)
Initialization :θ0, learning rate {ηk}k=0,1,···
for k = 0, 1, 2, · · · do

Solve ξθk from the current optiaml control system Σ(θk) ;
(cid:12)
Obtain ∂ξθ
using Algorithm 2 given ξθk ;
(cid:12)θk
∂θ
(cid:12)
Obtain ∂L
from the given loss function L(ξθ, θ) ;
(cid:12)ξθk
∂ξ
Apply the chain rule (9) to obtain dL
dθ

;

(cid:12)
(cid:12)θk

Update θk+1 ← θk − ηk

end

dL
dθ

(cid:12)
(cid:12)θk

;

(cid:46) using any OC solver

(cid:46) using Algorithm 2

Algorithm 4: PDP Algorithm for SysID Mode
Data: Input-state data {ξo}
Parameterization: The parameterized dynamics model Σ(θ) in (5)
Loss: L(ξθ, θ) in (6)
Initialization: θ0, learning rate {ηk}k=0,1,···
for k = 0, 1, 2, · · · do

Obtain ξθk by iteratively integrating Σ(θk) in (5) for t = 0, ..., T − 1;
Compute the coefﬁcient matrices (14) to obtain the auxiliary control system Σ(ξθ) in (20);
(cid:12)
Obtain ∂ξθ
(cid:12)θk
∂θ
(cid:12)
Obtain ∂L
(cid:12)ξθk
∂ξ
Apply the chain rule (9) to obtain dL
dθ

by iteratively integrating Σ(ξθk
from the given loss function in (6);

) in (20) for t = 0, ..., T − 1;

;

(cid:12)
(cid:12)θk

Update θk+1 ← θk − ηk

end

dL
dθ

(cid:12)
(cid:12)θk

;

3

Algorithm 5: PDP Algorithm for Control/Planning Mode
Parameterization: The parameterized-policy system Σ(θ) in (7)
Loss: L(ξθ, θ) in (8)
Initialization: θ0, learning rate {ηk}k=0,1,···
for k = 0, 1, 2, · · · do

Obtain ξθk by iteratively integrating Σ(θk) in (7) for t = 0, ..., T − 1;
Compute the coefﬁcient matrices (14) to obtain the auxiliary control system Σ(ξθ) in (21);
(cid:12)
Obtain ∂ξθ
(cid:12)θk
∂θ
(cid:12)
Obtain ∂L
(cid:12)ξθk
∂ξ
Apply the chain rule (9) to obtain dL
dθ

by iteratively integrating Σ(ξθk
from the given loss function L(ξθ, θ) in (8);
(cid:12)
(cid:12)θk

) in (21) for t = 0, ..., T − 1;

;

Update θk+1 ← θk − ηk

dL
dθ

end

(cid:12)
(cid:12)θk

;

Additional comments: combining different learning modes.
In addition to using different learn-
ing modes to solve different types of problems, one can combine different modes in a single learning
task. For example, when solving model-based reinforcement learning, one can call SysID Mode to
ﬁrst learn a dynamics model, then use the learned dynamics in Control/Planning Mode to obtain an
optimal policy. In problems such as imitation learning, one can ﬁrst learn a dynamics model using
SysID Mode, then use the learned dynamics as the initial guess in IRL/IOC Mode. In forward pass
of IOC/IRL Mode, one can call Control/Planning Mode to solve the OC system. For control and
planning problems, the loss required in Control/Planning Mode can be learned using IOC/IRL Mode.
In MPC-based learning and control [67], one can use the general formulation in (3) to learn a MPC
controller, and then execute the MPC controller by calling Control/Planning Mode.

E Experiment Details

We have released the PDP source codes and different simulation environments/systems in this paper
as two standalone packages, both of which are available at https://github.com/wanxinjin/
Pontryagin-Differentiable-Programming. The video demos for some of the experiments are
available at https://wanxinjin.github.io/posts/pdp.

E.1 System/Environment Setup

Quadrotor maneuvering control on SE(3). We consider a quadrotor system maneuvering on
SE(3) space (i.e. full position and full attitude space). The equation of motion of a quadrotor is given
by:

˙pI = ˙vI ,

m ˙vI = mgI + f I ,
1
2

˙qB/I =

Ω(ωB)qB/I ,

(S.13)

JB ˙ωB = M B − ω × JBωB.
Here, the subscriptions B and I denote that a quantity is expressed in the quadrotor’s body frame
and inertial (world) frame, respectively; m is the mass of the quadrotor, respectively; p ∈ R3 and
v ∈ R3 are the position and velocity vector of the quadrotor; JB ∈ R3×3 is the moment of inertia
of the quadrotor with respect to its body frame; ωB ∈ R3 is the angular velocity of the quadrotor;
qB/I ∈ R4 is the unit quaternion [75] describing the attitude of quadrotor with respect to the inertial
frame; Ω(ωB) is

Ω(ωB) =








0

0 −ωx −ωy −ωz
ωz −ωy
ωx
ωx
0
ωy −ωz
0
ωz

ωy −ωx




(S.14)

and used for quaternion multiplication; M B ∈ R3 is the torque applied to the quadrotor; and
f I ∈ R3 is the force vector applied to the quadrotor’s center of mass (COM). The total force

4

magnitude (cid:107)f I (cid:107) ∈ R (along the z-axis of the body frame) and torque M B = [Mx, My, Mz] are
generated by thrusts [T1, T2, T3, T4] of the four rotating propellers of the quadrotor, which can be
written as






 =









(cid:107)f I (cid:107)
Mx
My
Mz

1
0
−lw/2
c

1
−lw/2
0
−c

1
0
lw/2
c

1
lw/2
0
−c







T1
T2
T3
T4








 ,

(S.15)

with lw being the wing length of the quadrotor and c a ﬁxed constant.

We deﬁne the state and input vectors of the quadrotor system as

x = [p(cid:48) v(cid:48) q(cid:48) ω(cid:48)](cid:48) ∈ R13

(S.16)
respectively. In design of the quadrotor’s control objective function, to achieve SE(3) maneuvering
control performance, we need to carefully design the attitude error. As used in [76], we deﬁne the
attitude error between the quadrotor’s current attitude q and the goal attitude qg as

and u = [T1 T2 T3 T4](cid:48) ∈ R4.

e(q, qg) =

1
2

Tr(I − R(cid:48)(qg)R(q)),

(S.17)

where R(q) ∈ R3×3 are the direction cosine matrix directly corresponding to q (see [75] for more
details). Other error term in the control objective is the distance to the respective goal:
e(ω, ωg) = (cid:107)ω − ωg(cid:107)2.

e(v, vg) = (cid:107)v − vg(cid:107)2,

e(p, pg) = (cid:107)p − pg(cid:107)2,

(S.18)

Two-link robot arm.
The dynamics of a two-link robot arm can be found in [77, p. 171], where
the state vector is x = [q, ˙q](cid:48) with q ∈ R2 the vector of joint angles and ˙q ∈ R2 the vector of joint
angular velocities, and the control input u ∈ R2 is the vector of torques applied to each joint.

Dynamics discretization.
The continuous-time dynamics of all experimental systems in Table 2
are discretized using the Euler method: xt+1 = xt + ∆ · f (xt, ut) with the discretization interval
∆ = 0.05s or ∆ = 0.1s.

Simulation environment source codes. We have made different simulation environments/systems in
Table 2 as a standalone Python package, which is available at https://github.com/wanxinjin/
Pontryagin-Differentiable-Programming. This environment package is easy to use and has
user-friendly interfaces for customization.

E.2 Experiment of Imitation Learning

Data acquisition.
optimal control system with the expert’s dynamics and control objective parameter θ∗ = {θ∗
given. We generate a number of ﬁve trajectories, where different trajectories ξd = {xd
have different initial conditions x0 and time horizons T (T ranges from 40 to 50).

The dataset of expert demonstrations {ξd} is generated by solving an expert
dyn}
0:T −1}

0:T , ud

dyn, θ∗

Inverse KKT method. We choose the inverse KKT method [52] for comparison because it is
suitable for learning objective functions for high-dimensional continuous-space systems. We adapt
the inverse KKT method, and deﬁne the KKT loss as the norm-2 violation of the KKT condition
(S.11) by the demonstration data ξd, that is,

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂x0:T

min
θ,λ1:T

(xd

0:T , ud

0:T −1)

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂L
∂u0:T -1

(xd

0:T , ud

0:T −1)

2(cid:33)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(S.19)

∂L
∂x0:T

∂L
∂u0:T −1

(·) and

(·) are deﬁned in (S.11) and θ={θdyn, θdyn}. We minimize the above

where
KKT-loss with respect to the unknown θ and the costate variables λ1:T .
Note that to illustrate the inverse-KKT learning results in Fig. 3, we plot the imitation loss L(ξθ, θ) =
(cid:107)ξd − ξθ(cid:107)2 instead of the KKT loss (S.19), because we want to guarantee that the comparison
criterion is the same across different methods. Thus for each iteration k in minimizing the KKT loss
(S.19), we use the parameter θk to compute the optimal trajectory ξθk and obtain the imitation loss.
Neural policy cloning.
For the neural policy cloning (similar to [78]), we directly learn a neural-
network policy u = πθ(x) from the dataset using supervised learning, that is

(cid:88)T −1
t=0

min
θ

(cid:107)ud

t − πθ(xd

t)(cid:107)2.

(S.20)

5

Learning neural control objective function.
function of the robot arm. The neural objective function is constructed as

In Fig. 3d, we apply PDP to learn a neural objective

J(θ) = Vθ(x) + 0.0001(cid:107)u(cid:107)2,
(S.21)
with Vθ(x) a fully-connected feed-forward network with n-n-1 layers and tanh activation functions,
i.e., an input layer with n neurons equal to the dimension of state, n, one hidden layer with n neurons
and one output layer with 1 neuron. θ is the neural network parameter. We separate the input cost
from the neural network because otherwise it will cause instability when solving OC problems in
the forward pass. Also, in learning the above neural objective function, we ﬁx the dynamics because
otherwise it will also lead to instability of solving OC.

In the comparing GAIL method [56], we use the following hyper-parameters: the policy network is a
fully-connected feed-forward network with n-400-300-m layers and relu activation functions; the
discriminator network is a (n+m)-400-300-1 fully-connected feed-forward network with tanh and
sigmoid activation functions; and the policy regularizer λ is set to zero.

Results and validation.
In Fig. S1, we show more detailed results of imitation loss versus iteration
for three systems (cart-pole, robot arm, and quadrotor). On each system, we run ﬁve trials for all
methods with random initial guess, and the learning rate for all methods is set as η = 10−4. In
Fig. S4, we validate the learned models (i.e., learned dynamics and learned control objective) by
performing motion planning of each system in unseen settings. Speciﬁcally, we set each system with
new initial state x0 and horizon T and plan the control trajectory using the learned models, and we
also show the corresponding true trajectory of the expert.

E.3 Experiment of System Identiﬁcation

Data acquisition.
In the system identiﬁcation experiment, we collect a total number of ﬁve
trajectories from systems (in Table 2) with dynamics known, wherein different trajectories ξo =
{xo
0:T , u0:T −1} have different initial conditions x0 and horizons T (T ranges from 10 to 20), with
random inputs u0:T −1 drawn from uniform distribution.

DMDc method.
The DMDc method [57], which can be viewed as a variant of Koopman theory [6],
estimates a linear dynamics model xt+1 = Axt + But, using the following least square regression

min
A,B

(cid:88)T −1
t=0

(cid:107)xo

t+1 − Axo

t − But(cid:107)2.

(S.22)

For the neural network baseline, we use a neural network f θ(x, u) to
Neural network baseline.
represent the system dynamics, where the input of the network is state and control vectors, and output
is the state of next step. We train the neural network by minimizing the following residual

(cid:88)T −1
t=0

min
θ

(cid:107)xo

t+1 − f θ(xo

t, ut)(cid:107)2.

(S.23)

Learning neural dynamics model.
In Fig. 4d, we compare the performance of PDP with Adam
[58] for learning the same neural dynamics model for the robot arm system. Here, the neural dynamics
model is a fully-connected feed-forward neural network with (m+n)-(2m+2n)-n layers and tanh
activation functions, that is, an input layer with (m+n) neurons equal to the dimension of state, n,
plus the dimension of control m, one hidden layer with (2m+2n) neurons and one output layer with
(n) neurons. The learning rate for the PDP and the PyTorch Adam is both set as η = 10−5.

Results and validation.
In Fig. S2, we show more detailed results of SysID loss versus iteration
for the three systems (cart-pole, robot arm, and quadrotor). On each system, we run ﬁve trials with
random initial guess, and we set the learning rate as η = 10−4 for all methods. In Fig. S5, we use the
learned dynamics model to perform motion prediction of each system in unactuated conditions (i.e.,
ut = 0), in order to validate the effectiveness/correctness of the learned dynamics models.

E.4 Experiment of Control/Planning

We use the dynamics identiﬁed in the system ID part, and the speciﬁed control objective function
is set as weighted distance to the goal, as given in Table 2 (θobj is given). Throughout the optimal
control/planning experiments, we use the time horizons T ranging from 20 to 40.

6

Learning neural network policies. On the cart-pole and robot-arm systems (in Fig. 5a and Fig.
5b), we learn a feedback policy by minimizing given control objective functions. For both systems, we
parameterize the policy using a neural network. Speciﬁcally, we use a fully-connected feed-forward
neural network which has a layer structure of n-n-m with tanh activation functions, i.e., there is an
input layer with n neurons equal to the dimension of state, one hidden layer with n neurons and one
output layer with m neurons. The policy parameter θ is the neural network parameter. We apply the
PDP Control/Planning mode in Algorithm 5 and set the learning rate η = 10−4. For comparison, we
apply the guided policy search (GPS) method [59] (its deterministic version) to learn the same neural
policy with the learning rate η = 10−6 (η in GPS is used to update the Lagrange multipliers for the
policy constraint and we choose η = 10−6 because it achieves the most stable results).

Motion planning with Lagrange polynomial policies. On the 6-DoF quadrotor, we use PDP to
perform motion planning, that is, to ﬁnd a control sequence to minimize the given control cost (loss)
function. Here, we parameterize the policy ut = u(t, θ) as N -degree Lagrange polynomial [80] with
N +1 pivot points evenly populated over the time horizon, that is, {(t0, u0), (t1, u1), · · · , (tN , uN )}
with ti = iT /N , i = 0, · · · , N . The analytical form of the parameterized policy is

u(t, θ) =

N
(cid:88)

i=0

uibi(t)

with

bi(t) =

(cid:89)

0≤j≤N,j(cid:54)=i

t − tj
ti − tj

.

(S.24)

Here, bi(t) is called Lagrange basis, and the policy parameter θ is deﬁned as

θ = [u0, · · · , uN ](cid:48) ∈ Rm(N +1).

(S.25)

The above Lagrange polynomial parameterization has been normally used in some trajectory opti-
mization method such as [41, 81]. In this planning experiment, we have used different degrees of
Lagrange polynomials, i.e., N = 5 and N = 35, respectively, to show how policy expressiveness can
inﬂuence the ﬁnal control loss (cost). The learning rate in PDP is set as η = 10−4. For comparison,
we also apply iLQR [38] to solve for the optimal control sequence.

Results
In Fig. S3, we show the detailed results of control loss (i.e. the value of control objective
function) versus iteration for three systems (cart-pole, robot arm, and quadrotor). For each system,
we run ﬁve trials with random initial parameter θ0. In Fig. S6, we apply the learned neural network
policies (for cart-pole and robot arm systems) and the Lagrange polynomial policy (for quadrotor
system) to simulate the corresponding system. For reference, we also plot the optimal trajectory
solved by an OC solver [79] (which corresponds to the minimal control cost).

Comments on the result comparison between GPS [59] and PDP.
In learning feedback policies,
comparing the results obtained by the guided policy search (GPS) [59] and PDP in Fig. S3 and in Fig.
S6, we have the following remarks.

(1) PDP outperforms GPS in terms of having lower control loss (cost). This can be seen in Fig. S3 and
Fig. S6 (in Fig. S6, PDP results in a simulated trajectory which is closer to the optimal one than that
of GPS). This can be understood from the fact that GPS considers the policy as constraint and updates
it in a supervised learning step during the learning process. Although GPS aims to simultaneously
minimize the control cost and the degree to which the policy is violated, it does not necessarily mean
that before the learning researches convergence, when strictly following a pre-convergence control
policy, the system will have a cost as minimal as it can possibly achieve.

(2) Instead, PDP adopts a different way to synchronize the fulﬁllment of policy constraints and
the minimization of the control cost. In fact, throughout the entire learning process, PDP always
guarantees that the policy constraint is perfectly respected (as the forward pass strictly follows the
policy). Therefore, the core difference between PDP and GPS is that PDP does not simultaneously
minimize two aspects—the policy violation and control cost, instead, it enforces that one aspect—
policy—is always respected and only focuses on minimizing the other—control cost. The beneﬁt of
doing so is that at each learning step, the control cost for PDP is always as minimal as it can possibly
achieve. This explains why PDP outperforms GPS in terms of having lower control cost (loss).

7

Figure S1: Experiments for PDP IRL/IOC Mode: imitation loss versus iteration. For each system, we
run ﬁve trials starting with random initial guess θ0, and the learning rate is η = 10−4 for all methods.
The results show a signiﬁcant advantage of the PDP over the neural policy cloning and inverse-KKT
[52] in terms of lower training loss and faster convergence speed. Please see Appendix Fig. S4 for
validation. Please ﬁnd the video demo at https://youtu.be/awVNiCIJCfs.

8

010000Iteration102101100101102103Cartpole imitation lossTrial#1PDPInv. KKTClo. Policy010000Iteration102101100101102103Trial#2PDPInv. KKTClo. Policy010000Iteration102101100101102103Trial#3PDPInv. KKTClo. Policy010000Iteration102101100101102103Trial#4PDPInv. KKTClo. Policy010000Iteration102101100101102103Trial#5PDPInv. KKTClo. Policy010000Iteration102101100101102103Robot arm imitation lossTrial#1PDPInv. KKTClo. policy010000Iteration103102101100101102103Trial#2PDPInv. KKTClo. policy010000Iteration103102101100101102103Trial#3PDPInv. KKTClo. policy010000Iteration104103102101100101102103Trial#4PDPInv. KKTClo. policy010000Iteration104103102101100101102103Trial#5PDPInv. KKTClo. policy010000Iteration103102101100101102103Quadrotor imitation lossTrial#1PDPInv. KKTClo. policy010000Iteration103102101100101102103Trial#2PDPInv. KKTClo. policy010000Iteration103102101100101102103Trial#3PDPInv. KKTClo. policy010000Iteration103102101100101102103Trial#4PDPInv. KKTClo. policy010000Iteration104103102101100101102103Trial#5PDPInv. KKTClo. policyFigure S2: Experiments for PDP SysID Mode: SysID loss versus iteration. For each system, we
run ﬁve trials with random initial guess θ0, and set the learning rate η = 10−4 for all methods. The
results show a signiﬁcant advantage of the PDP over neural-network dynamics and DMDc in terms
of lower training loss and faster convergence speed. Please see Fig. S5 for validation. Please ﬁnd the
video demo at https://youtu.be/PAyBZjDD6OY.

9

010000Iteration1024102010161012108104100104Cartpole SysID lossTrial#1PDPDMDcNN dyn010000Iteration1024102010161012108104100104Trial#2PDPDMDcNN dyn010000Iteration1024102010161012108104100104Trial#3PDPDMDcNN dyn010000Iteration1024102010161012108104100104Trial#4PDPDMDcNN dyn010000Iteration1025102110171013109105101103Trial#5PDPDMDcNN dyn010000Iteration102010161012108104100104Robot arm SysID lossTrial#1PDPDMDcNN dyn010000Iteration10131010107104101102Trial#2PDPDMDcNN dyn010000Iteration10151012109106103100103Trial#3PDPDMDcNN dyn010000Iteration102010161012108104100104Trial#4PDPDMDcNN dyn010000Iteration1019101610131010107104101102Trial#5PDPDMDcNN dyn010000Iteration107105103101101103105Quadrotor SysID lossTrial#1PDPDMDcNN dyn010000Iteration107105103101101103105Trial#2PDPDMDcNN dyn010000Iteration1010108106104102100102104Trial#3PDPDMDcNN dyn010000Iteration107105103101101103105Trial#4PDPDMDcNN dyn010000Iteration108106104102100102104Trial#5PDPDMDcNN dynFigure S3: Experiments for PDP Control/Planning Mode: control loss (i.e., objective function
value) versus iteration. For the cart-pole (top panel) and robot arm (middle panel) systems, we
learn neural feedback policies, and compare with the GPS method [59]. For the quadrotor system,
we perform motion planning with a Lagrange polynomial policy (we use different degree N ),
and compare with iLQR and an OC solver [79]. The results show that for learning feedback
control policies, PDP outperforms GPS in terms of having lower control loss (cost); and for motion
planning, iLQR has faster convergence speed than PDP. Please ﬁnd the video demo at https:
//youtu.be/KTw6TAigfPY.

10

0250Iteration130140150160170180190200Cartpole control lossTrial#1PDPGPS0250Iteration130140150160170180190200Trial#2PDPGPS0250Iteration130140150160170180190200Trial#3PDPGPS0250Iteration130140150160170180190200Trial#4PDPGPS0250Iteration130140150160170180190200Trial#5PDPGPS0250Iteration01020304050Robot arm control lossTrial#1PDPGPS0250Iteration01020304050Trial#2PDPGPS0250Iteration01020304050Trial#3PDPGPS0250Iteration01020304050Trial#4PDPGPS0250Iteration01020304050Trial#5PDPGPS0100Iteration0.51.01.52.02.53.03.54.04.5Quadrotor control loss×104      Trial#1PDP, N=5PDP, N=35iLQRby OC solver0100Iteration0.51.01.52.02.53.03.54.04.5×104      Trial#2PDP, N=5PDP, N=35iLQRby OC solver0100Iteration0.51.01.52.02.53.03.54.04.5×104      Trial#3PDP, N=5PDP, N=35iLQRby OC solver0100Iteration0.51.01.52.02.53.03.54.04.5×104      Trial#4PDP, N=5PDP, N=35iLQRby OC solver0100Iteration0.51.01.52.02.53.03.54.04.5×104      Trial#5PDP, N=5PDP, N=35iLQRby OC solver(a) Cart-pole

(b) Robot arm

(c) Quadrotor

Figure S4: Validation for the imitation learning experiment in Fig. S1. We preform motion planing
for each system in unseen conditions (new initial condition and new time horizon) using the learned
models. Results show that compared to the neural policy cloning and inverse KKT [52], PDP result
can accurately plan the expert’s trajectory in unseen settings. This indicates PDP can accurately learn
the dynamics and control objective, and has the better generality than the other two. Although policy
imitation has lower imitation loss than inverse KKT, it has the poorer performance in planing. This is
because with limited data, the cloned policy can be over-ﬁtting, while the inverse KKT learns a cost
function, a high-level representation of policies, thus has better generality to unseen conditions.

(a) Cart-pole

(b) Robot arm

(c) Quadrotor

Figure S5: Validation for the system identiﬁcation experiment in Fig. S2. We perform motion
prediction in unactuated conditions (u = 0) using the learned dynamics. Results show that compared
to neural-network dynamics training and DMDc, PDP can accurately predict the motion trajectory of
each systems. This indicates the effectiveness of the PDP in identifying dynamics models.

(a) Cart-pole

(b) Robot arm

(c) Quadrotor

Figure S6: Simulation of the learned policies in the control and planning experiment in Fig. S3. Fig.
S6a-S6b are the simulations of the learned neural feedback policies on the cart-pole and robot arm
systems, respectively, where we also plot the optimal trajectory solved by an OC solver [79] for
reference. From Fig. S6a-S6b, we observe that PDP results in a trajectory that is much closer to the
optimal one than that of GPS; this implies that PDP has lower control loss (please check our analysis
on this in Appendix E.4) than GPS. Fig. S6c is the planning results for the quadrotor system using
PDP, iLQR, and an OC solver [79], where we have used different degrees of Lagrange polynomial
policies in PDP. The results show that PDP can successfully plan a trajectory very close to the ground
truth optimal trajectory. We also observe that the accuracy of the resulting trajectory depends on
choice of the policy parameterization (i.e., expressive power): for example, the use of polynomial
policy of a higher degree N results in a trajectory closer to the optimal one (the one using the OC
solver) than the use of a lower degree. iLQR is generally able to achieve high-accuracy solutions
because it directly optimizes the loss function with respect to individual control inputs (instead of a
parameterized policy), but this comes at the cost of high computation expense, as shown in Fig. 5d.
11

0.02.55.07.510.012.515.017.5Time20151050510Cart forceGround truthPDPInverse KKTPolicy cloneCartpole planning0510152025303540010Torque 1Ground truthPDPInverse KKTPolicy clone0510152025303540Time5.02.50.02.5Torque 2Robot arm planning0204010010Thrust 1020400510Torque 2Ground truthPDPInverse KKTPolicy clone02040Time0510Torque 302040Time10010Torque 4Quadrotor planning0510152025303540101Cart positionGround truthPDPDMDcNN dynamics0510152025303540Time202Pole angleCartpole prediction05101520253035403210Joint1 angele0510152025303540Time1510505Joint 2 angleGround truthPDPDMDcNN dynamicsRobot arm prediction0122010Position x012105Position y012Time100Position zGround truthPDPDMDcNN dynamicsQuadrotor prediction0510152025Time1.00.50.00.51.01.5Cart forceNeural policy learned by PDPNeural policy learned by GPSTrajectory solved by OC solverCartpole optimal control0510152024Joint torque 1Neural policy learned by PDPNeural policy learned by GPSTrajectory solved by OC solver051015Time20Joint torque 2Robot arm optimal control0102030024Thrust 201020302010010Thrust 1iLQRPDP, N=5PDP, N=35by OC solver0102030Time024Thrust 30102030Time2010010Thrust 4Quadrotor optimal controlF Related End-to-End Learning Frameworks

As discussed in Section 7, two categories are related to this work. Here, we only detail the difference
of PDP from the second category, i.e., the methods that learn an implicit planner within a RL policy.

Differentiable MPC. [67] develops an end-to-end differentiable MPC framework to jointly learn
the system dynamics model and control objective function of an optimal control system. In the
forward pass, it ﬁrst uses iLQR [38] to solve the optimal control system and ﬁnd a ﬁxed point,
and then approximate the optimal control system by a LQR at the ﬁxed point. In the backward
pass, the gradient is obtain by differentiating the LQR approximation. This process, however, may
have two drawbacks: ﬁrst, since the differentiation in the backward pass is conducted on the LQR
approximation instead of on the original system, the obtained gradient thus may not be accurate due
to discrepancy of approximation; and second, computing the gradient of the LQR approximation
requires the inverse of a coefﬁcient matrix, whose size is (2n + m)T × (2n + m)T with n and m
state and action dimensions, respectively, T the time horizon of the OC system, thus this will cause
huge computational cost when handling the system of longer time horizon T .

Compared to differentiable MPC, the ﬁrst advantage of the PDP framework is that the differentiation
in the backward pass is directly performed on the parameterized optimal control system (by differen-
tiating through PMP). Second, we develop the auxiliary control system in the backward pass of PDP,
whose trajectory is exactly the gradient of the system trajectory in the forward pass. The gradient
then is iteratively solved using the auxiliary control system by Lemma 5.2 (Algorithm 2). Those
proposed techniques enables the PDP to have signiﬁcant advantage in computational efﬁciency over
differentiable MPC. To illustrate this, we have compare the algorithm complexity for both PDP and
differentiable MPC in Table S1 and provide an experiment in Fig. S7.

Figure S7: Runtime (per iteration) comparison between the PDP and differentiable MPC [67] for
different time horizons of a pendulum system. Note that y-axis is log-scale, and the runtime is
averaged over 100 iterations. Both methods are implemented in Python and run on the same machine
using CPUs. The results show that the PDP runs 1000x faster than differentiable MPC.

Path Integral Network. [65] and [66] develop a differentiable end-to-end framework to learn
path-integral optimal control systems. Path-integral optimal control systems [69] however are a
limited category of optimal control systems, where the dynamics is afﬁne in control input and
the control objective function is quadratic in control input. More differently, this path integral
network is essentially an ‘unrolling’ method, which means that the forward pass of solving optimal
control is extended as a graph of multiple steps of applying gradient descent, and the solution of
the optimal control system is considered as the output of the ﬁnal step of the gradient descent
operations. Although the advantage of this unrolling (gradient descent) computational graph is that
it can immediately take advantage of automatic differentiation techniques such as TensorFlow [70]
to obtain the gradient in backpropagation, its drawback is however obvious: the framework is both
memory- and computationally- expensive because it needs to store and traverse all intermediate results
of the gradient descent process along the graph; furthermore, there is a conﬂict between computational
complexity and accuracy in the forward pass. We have provided its complexity analysis in Table S1.

Universal Planning Network. In [68], the authors develop an end-to-end imitation learning frame-
work consisting of two layers: the inner layer is a planner, which is formulated as an optimal control
system in a latent space and is solved by gradient descent, and an outer layer to minimize the imitation

12

loss between the output of inner layer and expert demonstrations. However, this framework is also
based on the ‘unrolling’ strategy. Speciﬁcally, the inner planning layer using gradient descent is
considered as a large computation graph, which chains together the sub-graphs of each step of
gradient descent. In the backward pass, the gradient derived from the outer layer back-propagates
through the entire computation graph. Again, this unrolled learning strategy will incur huge memory
and computation costs in implementation. Please ﬁnd its complexity analysis in Table S1.

Different from the above ‘unrolling’ learning methods [65, 66, 68, 82], the proposed PDP method
handles the learning of optimal control systems in a ‘direct and compact’ manner. Speciﬁcally, in
forward pass, PDP only obtains and stores the ﬁnal solution of the optimal control system and does
not care about the (intermediate) process of how such solution is obtained. Thus, the forward pass
of the PDP accepts any external optimal control solver such as CasADi [79]. Using the solution in
the forward pass, the PDP then automatically builds the auxiliary control system, based on which,
the exact analytical gradient is solved efﬁciently in backward pass. Such features guarantee that the
complexity of the PDP framework is only linearly scaled up to the time horizon of the system, which
is signiﬁcantly efﬁcient than the above ‘unrolling’ learning methods (please ﬁnd the comparison in
Table S1). In Appendix G, we will present the detailed complexity analysis.

Table S1: Complexity comparison for different end-to-end learning frameworks

Learning
frameworks

PI-Net [65]

UPN [68]

Forward pass

Backward pass

Method and accuracy

N -step unrolled graph
using gradient descent;
accuracy depends on N
N -step unrolled graph
using gradient descent;
accuracy depends on N

Complexity
(linear to)

Method

Complexity
(linear to)

computation: N T
memory: N T

Back-propagation over
the unrolled graph

computation: N T
memory: N T

computation: N T
memory: N T

Back-propagation over
the unrolled graph

computation: N T
memory: N T

Diff-MPC [67]

iLQR ﬁnds ﬁxed points;
can achieve any accuracy

computation: —
memory: T

PDP

Accept any OC solver;
can achieve any accuracy

computation: —,
memory: T

*Here T denotes the time horizon of the system;

Differentiate the LQR
approximation and
solve linear equations

Auxiliary control system

computation: T 2
memory: T 2

computation: T ,
memory: T

G Complexity of PDP

We consider the algorithm complexity of different learning modes of PDP (see Appendix D), and
suppose that the time horizon of the parameterized system Σ(θ) is T .

IRL/IOC Mode (Algorithm 3): in forward pass, PDP needs to obtain and store the optimal trajectory
ξθ of the optimal control system Σ(θ) in (1), and this optimal trajectory can be solved by any
(external) optimal control solver. In backward pass, PDP ﬁrst uses ξθ to build the auxiliary control
system Σ(ξθ) in (16) and then computes ∂ξθ
SysID Mode (Algorithm 4): in forward pass, PDP needs to obtain and store the trajectory ξθ of the
original dynamics system Σ(θ) in (5). Such trajectory is simply a result of iterative integration of (5),
which takes T steps. In backward pass, PDP ﬁrst uses ξθ to build the auxiliary control system Σ(ξθ)
in (20) and then computes ∂ξθ

∂θ by Lemma 5.2, which takes 2T steps.

∂θ by iterative integration of (20), which takes T steps.

Control/Planning Mode (Algorithm 5): in forward pass, PDP needs to obtain and store the trajectory
ξθ of the controlled system Σ(θ) in (7). Such trajectory is simply a result of iterative integration of
(7), which takes T steps. In backward pass, PDP ﬁrst uses ξθ to build an auxiliary control system
Σ(ξθ) in (21) and then computes ∂ξθ
Therefore, we can summarize that the memory- and computational- complexity for the PDP frame-
work is only linear to the time horizon T of the parameterized system Σ(θ). This is signiﬁcantly
advantageous over existing end-to-end learning frameworks, as summarized in Table S1.

∂θ by integration of (21), which takes T steps.

13

H Limitation of PDP

PDP is a ﬁrst-order algorithm. We observe that (i) all gradient quantities in PDP are analytical and
exact; (ii) the development of PDP does not involve any second-order derivative/approximation of
functions or models (note that PMP is a ﬁrst-order optimality condition for optimal control); and
(iii) PDP minimizes a loss function directly with respect to unknown parameters in a system using
gradient descent. Thus, we conclude that PDP is a ﬁrst-order gradient-descent based optimization
framework. Speciﬁcally for the SysID and Control/Planning modes of PDP, they are also ﬁrst-order
algorithms. When using these modes to solve optimal control problems, this ﬁrst-order nature may
bring disadvantages of PDP compared to high-order methods, such as iLQR which can be considered
as 1.5-order because it uses second-order derivative of a value function and ﬁrst-order derivative of
dynamics, or DDP which is a second-order method as it uses the second-order derivatives of both
value function and dynamics. The disadvantages of PDP have already been empirically shown in Fig.
5c and Fig. S3, where the converging speed of PDP in its planning mode is slower than that of iLQR.
For empirical comparisons between ﬁrst- and second-order techniques, we refer the reader to [83].

Convergence to local minima. Since PDP is a ﬁrst-order gradient-descent based algorithm, PDP
can only achieve local minima for general non-convex optimization problems in (3). Furthermore, we
observe that the general problem in (3) belongs to a bi-level optimization framework. As explored in
[71], under certain assumptions such as convexity and smoothness on models (e.g., dynamics model,
policy, loss function and control objective function), global convergence of the bi-level optimization
can be established. But we think such conditions are too restrictive in the context of dynamical
control systems. As a future direction, we will investigate mild conditions for good convergence by
resorting to dynamical system and control theory, such as Lyapunov theory.

Parameterization matters for global convergence. Although PDP only achieves local convergence,
these still exists a question of how likely PDP can obtain the global convergence. In our empirical
experiments, we ﬁnd that how models are parameterized matters for good convergence performance.
For example, in IOC/IRL mode, we observe that using a neural network control objective function
(in Fig. 3d) is more likely to get trapped in local minima than using the parameterization of weighted
distance objective functions (in Fig. 3a-3c). In control/planning mode, using a deeper neural network
policy (in Fig. 5a-5b) is more like to result in local minima than using a simpler one. Also in the
motion planning experiment, we use the Lagrange polynomial to parameterize a policy instead of
using standard polynomials, because the latter can lead to poor conditioning and sensitivity issues (a
small change of polynomial parameter results in large change in performance) and thus more easily
get stuck in local minima. One high-level explanation is that more complex parameterization will
bring extreme non-convexity to the optimization problem, making the algorithm more easily trapped
in local minima. Again, how to theoretically justify those empirical experience and ﬁnd the mild
conditions for global convergence guarantee still needs to be investigated in future research.

I PDP to Solve 6-DoF Rocket Powered Landing Problems

As a ﬁnal part in this supplementary, we will demonstrate the capability of PDP to solve the more
challenging 6-DoF rocket powered landing problems.

We here omit the description of mechanics modeling for the 6-DoF powered rocket system, and refer
the reader to Page 5 in [84] for the rigid body dynamics model of a rocket system (the notations and
coordinates used below follows the ones in [84]). The state vector of the rocket system is deﬁned as

x = (cid:2)m r(cid:48)

I v(cid:48)
I

B/I ω(cid:48)
q(cid:48)
B

(cid:3)(cid:48)

∈ R14,

(S.26)

where m ∈ R is the mass of the rocket; rI ∈ R3 and vI ∈ R3 are the position and velocity of the
rocket (center of mass) in the inertially-ﬁxed Up-East-North coordinate frame; qB/I ∈ R4 is the unit
quaternion denoting the attitude of rocket body frame with respect to the inertial frame (also see the
description in the quadrotor dynamics in Appendix E.1); and ωB ∈ R3 is the angular velocity of the
rocket expressed in the rocket body frame. In our simulation, we only focus on the ﬁnal descending
phase before landing, and thus assume the mass depletion during such a short phase is very slow and
thus ˙m ≈ 0. We deﬁne the control input vector of the rocket, which is the thrust force vector

u = T B = [Tx, Ty, Tz](cid:48) ∈ R3,

(S.27)

14

acting on the gimbal point of the engine (situated at the tail of the rocket) and is expressed in the
body frame. Note that the relationship between the total torque M B applied to the rocket and the
thrust force vector T B is M B = rI,B × T B, with rI,B ∈ R3 being constant position vector from the
center-of-mass of the rocket to the gimbal point of the engine. The continuous dynamics is discretized
using the Euler method: xt+1 = xt + ∆ · f (xt, ut) with the discretization interval ∆ = 0.1s.
For the rocket system, the unknown dynamics parameter, θdyn, includes the rocket’s initial mass m0,
and the moment of inertia J B ∈ R3×3, and the rocket length (cid:96), thus, θdyn = {m0, J B, (cid:96)} ∈ R8.
For the control objective (cost) function, we consider a weighted combination of the following
aspects:

• distance of the rocket position from the target position, associated with weight w1;
• distance of the rocket velocity from the target velocity, associated with weight w2;
• penalty of the excessive title angle of the rocket, associated with weight w3;
• penalty of the side effects of the thrust vector, associated with weight w4;
• penalty of the total fuel cost, associated with weighted w5.

So the parameter of the control objective function, θobj = [w1, w2, w3, w4, w5](cid:48) ∈ R5. In sum,
the overall parameter for the 6-DoF rocket powered landing control system is

θ = {θdyn, θobj} ∈ R13.

(S.28)

Imitation learning. We apply the IRL/IOC mode of PDP to perform imitation learning of the
6-DoF rocket powered landing. The experiment process is similar to the experiments in Appendix
E.2, where we collect ﬁve trajectories from an expert system with dynamics and control objective
function both known (different trajectories have different time horizons T ranging from 40 to 50 and
different initial state conditions). Here we minimize imitation loss L(ξθ, θ)=(cid:107)ξd − ξθ(cid:107)2 over the
parameter of dynamics and control objective, θ in (S.28). The learning rate is set to η = 10−4, and
we run ﬁve trials with random initial parameter guess θ0. The imitation loss L(ξθ, θ) versus iteration
is plotted in Fig. S8a. To validate the learned models (the learned dynamics and the learned objective
function), we use the learned models to perform motion planing of rocket powered landing in unseen
settings (here we use new initial condition and new time horizon). The planing results are plotted in
Fig. S8b, where we also plot the ground truth for comparison.

System identiﬁcation. We apply the SysID mode of PDP to identify the dynamics parameter θdyn
of the rocket. The experiment process is similar to the experiments in Appendix E.3, where we collect
ﬁve trajectories with different initial state conditions, time horizons (T ranges from 10 to 20), and
random control inputs. We minimize the SysID loss L(ξθ, θ) = (cid:107)ξo − ξθ(cid:107)2 over θdyn in (S.28). The
learning rate is set to η = 10−4, and we run ﬁve trials with random initial parameter guess for θdyn.
The SysID loss L(ξθ, θ) versus iteration is plotted in Fig. S9a. To validate the learned dynamics, we
use it to predict the motion of rocket given a new sequence of control inputs. The prediction results
are in Fig. S9b, where we also plot the ground truth for reference.

Optimal powered landing control. We apply the Control/Planning mode of PDP to ﬁnd an optimal
control sequence for the rocket to perform a successful powered landing. The experiment process
is similar to the experiments performed for the quadrotor system in Appendix E.4. We set the time
horizon as T = 50, and randomly choose an initial state condition x0 for the rocket. We minimize
the control loss function, which is now a given control objective function with θobj known. The
control policy we use here is parameterized as the Lagrangian polynomial, as described in (S.24) in
Appendix E.4, here with degree N = 25. The control loss is set as the control objective function
learned in the previous imitation learning experiment. The learning rate is set to η = 10−4, and we
run ﬁve trials with random initial guess of the policy parameter. The the control loss L(ξθ, θ) versus
iteration is plotted in Fig. S10a. To validate the learned optimal control policy, we use it to simulate
the motion (control trajectory) of the rocket landing, and compare with the ground truth optimal
trajectory obtained by an OC solver. The validation results are in Fig. S10b.

15

(a) Training

(b) Validation
(Tx is deﬁned along the rocket direction)

Figure S8: (a) Training process for imitation learning of 6-DoF rocket powered landing: the imitation
loss versus iteration; here we have performed ﬁve trials (labeled by different colors) with random
initial parameter guess. (b) Validation: we use the learned models (dynamics and control objective
function) to perform motion planning of the rocket powered landing in unseen settings (i.e. given new
initial state condition and new time horizon requirement); here we also plot the ground-truth motion
planning of the expert for reference. The results in (a) and (b) show that the PDP can accurately learn
the dynamics and control objective function from demonstrations, and have good generalizability to
novel situations. Please ﬁnd the video demo at https://youtu.be/4RxDLxUcMp4.

(a) Training

(b) Validation

Figure S9: (a) Training process for identiﬁcation of rocket dynamics: SysID loss versus iteration;
here we have performed ﬁve trials (labeled by different colors) with random initial parameter guess.
(b) Validation: we use the learned dynamics model to perform motion prediction of the rocket given a
new control sequence; here we also plot the ground-truth motion (where we know the exact dynamics).
The results in (a) and (b) show that the PDP can accurately identify the dynamics model of the rocket.

(a) Training

(b) Validation
(Tx is deﬁned along the rocket)

Figure S10: (a) Training process of learning the optimal control policy for rocket powered landing:
the control loss versus iteration; here we have performed ﬁve trials (labeled by different colors) with
random initial guess of the policy parameter. (b) Validation: we use the learned policy to simulate
the rocket control trajectory; here we also plot the ground-truth optimal control solved by an OC
solver. The results in (a) and (b) show that the PDP can successfully ﬁnd the optimal control policy
(or optimal control sequence) to successfully perform the rocket powered landing. Please ﬁnd the
video demo at https://youtu.be/5Jsu772Sqcg.

16

02004006008001000Iteration103101101103Imitation LossImitation of powered landing025Time4030201001020Tx [N]025Time864202Ty [N]025Time20246Tz [N]truthPDPPlanning for powered landing0500100015002000Iteration102210161010104102SysID LossDynamics identification010100vx0100.000.250.50vy0100.00.20.4vztruthPDP010Time0.10.00.1x010Time0.00.2y010Time0.20.0zPrediction using learned dynamics0200040006000800010000Iteration1000015000200002500030000Control lossControl for powered landing050Time51015Tx [N]050Time051015Ty [N]050Time1012Tz [N]OC solverPDPLearned control for powered landing