2
2
0
2

y
a
M
3
1

]
I

A
.
s
c
[

2
v
5
4
5
4
0
.
5
0
2
2
:
v
i
X
r
a

A Probabilistic Generative Model of Free Categories

Eli Sennesh
Khoury College of Computer Sciences

Northeastern University
Boston, Massachusetts, USA
sennesh.e@northeastern.edu

Tom Xu
School of Computing
The Australian National University
Canberra, Australia
tom.xu@anu.edu.au

Yoshihiro Maruyama
School of Computing
The Australian National University
Canberra, Australia
yoshihiro.maruyama@anu.edu.au

Applied category theory has recently developed libraries for computing with morphisms in inter-
esting categories, while machine learning has developed ways of learning programs in interesting
languages. Taking the analogy between categories and languages seriously, this paper deﬁnes a prob-
abilistic generative model of morphisms in free monoidal categories over domain-speciﬁc generating
objects and morphisms. The paper shows how acyclic directed wiring diagrams can model spec-
iﬁcations for morphisms, which the model can use to generate morphisms. Amortized variational
inference in the generative model then enables learning of parameters (by maximum likelihood) and
inference of latent variables (by Bayesian inversion). A concrete experiment shows that the free
category prior achieves competitive reconstruction performance on the Omniglot dataset.

1 Introduction

Applied category theory has recently developed software libraries for representing and computing with
morphisms in various categories, with a particular focus on (symmetric) monoidal categories and dia-
grammatic reasoning (e.g. [16, 28, 53, 52]). Categories have also emerged as a common representation
for computer programs in different languages [64, 13]. In parallel, the ﬁeld has worked to provide cat-
egorical semantics and interpretations for a variety of machine learning (ML) and artiﬁcial intelligence
(AI) building blocks, including neural networks [17, 18, 10], and probabilistic inference [24, 11, 6, 21].
The intersection of learning and compositional reasoning has challenged the cognitive sciences (AI,
ML, cognitive science, etc.) from the beginning. Theoretical arguments suggest that a generally intel-
ligent agent ought to reason compositionally [54, 42], to employ something like computable programs
as models of the world [63], and to prefer simpler programs to more complex ones [68]. Evidence from
a variety of experiments [38, 34, 26, 37, 73] suggests that abductive learning of program-like causal
models provides a domain-general foundation for human intelligence.

Most process models of cognition spring from a limited selection of metaphors for how the mind
works. For instance, cognitive scientists continue to debate whether the mind builds concepts by means
of a “language of thought” [55, 50, 57] or convex geometric spaces [22]. Some cognitive neuroscientists
ﬁnd tentative support for both “map-like” and “sentence-like” processes in the brain [20] while others
argue for a more connectionist “direct ﬁt” [29]. Machine learning scientists tackling the learning and
synthesis of task-speciﬁc policies or programs tend to employ either Bayesian inference over a context-
free grammar of programs, reinforcement learning of neural network policies, or both [51, 67, 49].

Submitted to:
5th Annual International Conference on Applied Category Theory (ACT 2022)

© E. Sennesh, T. Xu, & Y. Maruyama

 
 
 
 
 
 
2

A Probabilistic Generative Model of Free Categories

Category theory readers will notice that program learning tasks, via their context-free grammars,
work with the compositional structure of operads [31, 19]. The connection suggests that categories, too,
could provide a setting for learning and inference. This paper thus suggests an additional alternative to
the options of neural networks and grammatical programs: a probabilistic generative model (PGM) of
morphisms in a free monoidal category. This free category prior is parameterized by generating objects
and morphisms (to construct the free category) and wiring diagrams (to sample speciﬁc morphisms).
An experiment shows that the model’s software implementation can learn to compose deep generative
models to achieve competitive performance in reconstruction evaluation data.

Outline Section 2 summarizes the categorical background for the rest of the paper: our chosen setting
for categorical probability in Section 2.1, and the operad of wiring diagrams for use as speciﬁcations
in Section 2.2. Section 3 then deﬁnes the necessary categorical machinery for generative modeling,
presents the free category prior, and demonstrates its basic properties. Section 4 describes its software
implementation, describes amortized variational inference (Section 4.2), and presents a basic experiment
(Section 4.3) showing that ﬁtting the free category model to data yields competitive performance in a
generative modeling task. Section 5 discusses extensions and further applications.

2 Categorical background

2.1 Markov categories

In this paper we build upon Markov categories, a formalism presented by Fritz [21] following a series
of works by Golubtsov [25] and Cho and Jacobs [6]. It allows us to represent fundamental concepts
from probability theory (such as conditioning, causality, almost surety, etc.) in purely categorical terms.
In particular we work in QBS (i.e., the category of quasi-Borel spaces) by Heunen et al. [32]. In the
following we use the notation
Deﬁnition 1 (Markov Category). A Markov category C is a symmetric monoidal category in which
C is equipped with a commutative comonoid structure given by a comultiplication
every object X
X and a counit delX : X
X
copyX : X

throughout for sequential and monoidal composition.

and

(cid:12)

I.

(cid:35)

→

→
Modeling probability theory in a Markov category involves the following steps:
• Identify a base Markov category C (usually a category of certain type of measurable spaces).
• Identify a monad (usually Giry/probability monad) represented by an endofunctor P : C
C .
• Take the Kleisli category Kl(P ) of the monad P .

→

∈
(cid:12)

Corollary 3.2 in [21] says that if P is a symmetric monoidal afﬁne monad then the Kleisli category Kl(P )
is again a Markov category. In particular, the Giry monad is symmetric monoidal and afﬁne.

⊆

→

[R

Typically Markov categories consist of certain measurable spaces as objects and Markov kernels as
morphisms. In QBS, by contrast, each object consists of a pair of a sample space X and a collection
X] (which must be closed under certain conditions) and the
of X-valued random variables MX
morphisms are random functions between sample spaces that extend the random variables in the domain
to random variables in the codomain. Random variables are thus fundamental, rather than being derived
from a σ -algebra of measurable subsets. They then derive their randomness from probability measures
in R, as we will discuss below.
Deﬁnition 2 (Category of quasi-Borel spaces). A quasi-Borel space is a set X, which we call the sample
space, together with a set of functions MX
• Contains all constant functions: α

[R
→
⊆
MX if α : R

X is constant;

X] that

∈

→

E. Sennesh, T. Xu, & Y. Maruyama

3

• Is closed under pre-composition: α

f

MX if α

MX and f : R

R is measurable; and

◦

∈

∈

→

• Is closed under countable mixtures: with respect to a partition of R into a disjoint union of count-
MX then the mixed random variable β is

ably many Borel sets i.e. R = (cid:85)
in MX where β (r) = αi(r) for r

N Si, and α1, α2, . . .
i
∈
Si.
∈

∈

◦

∈

Morphisms (X, MX )
MY for all α
f

(Y, MY ) between quasi-Borel spaces consist of functions f : X
MX .

α
Together these form the category of Quasi-Borel Spaces and measurable functions between them
(QBS), with function composition as composition of morphisms and identity functions as identity mor-
phisms.

Y such that

→
∈

→

To study probability theory via quasi-Borel spaces, there is a notion of probability measure on them.

Deﬁnition 3 (Probability measure on quasi-Borel space.). A probability measure on a quasi-Borel space
(X, MX ) is a pair (α, µ) where α
MX and µ is a probability measure on R. Here µ is a probability
measure from the standard measure theory.

∈

The above deﬁnition reﬂects the usual notion of a probability measure on a measurable space which is
induced via pushing-forward through a random variable (a measurable function) from the sample space
with a probability measure. Another way of phrasing the notion of probability measure is that with a
MX in
source of randomness from R and a distribution µ on R, we observe the effect of the process α
X. From this notion of probability measure, there is generalized version of Giry monad in QBS which is
referred as the probability monad.

∈

Lemma 1 (Probability monad in QBS, due to Heunen et al. [32]). The set of all probability measures
on a quasi-Borel space naturally forms a quasi-Borel space (see Heunen et al. [32]). The endofunctor
P sending a quasi-Borel X to the space P X of probability measures on X (quotienting out a suitable
notion of equivalent measures) gives rise to a commutative monad in QBS with Kleisli composition
P from the
P : QBS(A, P B)
(cid:35)
identity functor I.

QBS(A, P C) and unit natural transformation η : I

QBS(B, P C)

→

→

×

2.2 Wiring diagram operads and algebras over them

Rupel and Spivak [59] introduced wiring diagrams as a formal graphical language to represent the struc-
ture of composite processes. Despite their geometric presentation, wiring diagrams should be understood
as combinatorial entities. Every wiring diagram is a combinatorial scheme of composing blank boxes
and directed wires, and Patterson et al. [53] showed that wiring diagrams provide a combinatorial normal
form for morphisms in SMCs. This makes wiring diagrams suitable to be represented in a data structure.
This paper employs acyclic wiring diagrams as a syntax for specifying morphisms in categories.

Mathematically, wiring diagrams are expressed via the notion of a symmetric colored operad. Loosely
speaking, a colored operad is a category except that the hom-sets are allowed to go from a one ﬁnite set
of objects to another ﬁnite set of objects. Operads capture algebraic structures in SMCs. We utilize the
speciﬁc deﬁnition of an operad found in Spivak [65]. In short, an operad O is encoded by two pieces of
data: structure and laws. The structure of O consists of a) objects; b) morphisms; c) speciﬁed identity
morphisms on objects; and d) a composition formula for morphisms. The structure is required to satisfy
an identity law and an associativity law. We give a quick overview of the τ-typed operad Wτ of acyclic
wiring diagrams for the convenience of readers. The full deﬁnition can be found in Patterson [53].

Deﬁnition 4 (Operad of acyclic wiring diagrams). Let τ be a set. A τ-typed set is an object in the slice
category Set/τ. The operad Wτ has the following structure:

4

A Probabilistic Generative Model of Free Categories

• Objects. An object (box) t of Wτ is a pair of τ-typed ﬁnite sets. Pictorially t = (t

box with a ﬁnite number of τ-typed input and output wires 1.

, t+) is a blank

−

(cid:40)

m

...

t

...

(cid:41)

n

• Morphisms. A morphism (wiring diagram) Φ from a set of indexed boxes t1, . . . , tn to a box v is a
span in category Bijτ which is the category of τ-typed ﬁnite sets and bijective functions between
them. The morphism is denoted as Φ : t1, . . . , tn
v. Pictorially Φ is a wiring diagram with
t1, . . . , tn as inner boxes and v as outer box. More importantly, Φ encodes how the wires are con-
nected. Additionally, the indexed boxes t1, . . . , tn must satisfy a progress condition which ensures
that there is no cycle in a wiring diagram. For example:

→

t1

v

t2

• Identities. The identity morphism Idt on box t is the identity span associated with t. Pictorially,
it is a wiring diagram with no internal box, i.e., there is only one wire going right through the
diagram.

• Composition formula. Given wiring diagram Ψ : s1, . . . , sm

v, their i-th
iΦ. We follow the notation from Patterson, Spivak and Vagner
(cid:35)
gives composition of morphisms in operad and the order of composition is left to right. The

ti and Φ : t1, . . . , tn

partial composition is denoted as Ψ
that
composition is a new wiring diagram:

→

→

(cid:35)

Ψ

iΦ : t1, . . . , ti
(cid:35)

1, s1, . . . , sm, ti+1, . . . , tn
−

v

→

The formula says that the composition slots the whole wiring diagram Ψ into the inner box ti in
wiring diagram Φ.

i
(cid:35)

=

It can be shown that composition satisﬁes the progress condition which ensures that the wiring
diagram is acyclic.

The necessary identity law and associativity law are left to readers to check.

Operad algebras take composition structures in operads and map them to a meaning in some SMC.
OV , where the latter is the operad underlying V .
A WOb(C )-algebra is an operad functor H : WOb(C ) →
Proposition 1 (Enriched SMCs have enriched operad algebras). Each V -enriched strict SMC C has an
OV . This operad provides
operad of wiring diagrams WOb(C ) and a V -enriched algebra H : WOb(C ) →
a “normal form” for C , in the sense that the V -enriched SMC arising from the algebra and C itself.

1The wires are directed from left to right.

E. Sennesh, T. Xu, & Y. Maruyama

5

Proof. Deﬁnition 6.63 in Fong and Spivak [17] shows how to construct the operad OV underlying V .
OV , substituting OV
Theorem 5.3 in Patterson [53] then shows how to obtain the algebra H : WOb(C ) →
for the operad Set and hom-objects for hom-sets.

We therefore understand the V -enriched algebra H as

, t+)

• Sending a box (t
• Sending a wiring diagram Φ : t1

, t+)
v to the morphism H(Φ) : C (t1
, t1
→
−
, v+) from the boxes’ hom-objects to the whole diagram’s hom-object.

WOb(C ) to the hom-object C (t
tn

V whose shapes it speciﬁes.
C (tn
−

+)

. . .

. . .

×

⊗

×

⊗

∈

∈

−

−

, tn

+)

→

C (v
−

2.3 Free monoidal categories over quivers

Learning structures of composition will require specifying, a priori, some set of generating objects and
morphisms between them. However, a speciﬁc structure of possible compositions may have semantics in
several concrete categories, depending on the needs of the application. The free category prior therefore
defers semantics to speciﬁc applications, considering only structure in the generative model itself.

The free category prior requires, as a hyperparameter, a directed multigraph G = (V, E) as a nerve.
N Σn (the Kleene closure of a ﬁnite symbol-set
Each vertex v
Σ), and each label in Σ1 must appear on a vertex. A unique label I
Σ is reserved to denote in the graph
the vertex underlying the monoidal unit. Each edge has a “domain” dom(e) and a “codomain” cod(e).
We write E(A, B) to denote the edge-set between A, B

V must have a label σ (v) drawn from (cid:83)
n
∈

V , and write (assuming /0 = E0)

∈

∈

∈

P(A, B) :=

(cid:91)

n
∈

N

p

{

∈

En : dom(p1) = A, cod(pn) = B

}

for the path-set from A to B. We impose the following condition on the nerve-graph G.

Condition 1 (All non-unit generating objects have points). Each vertex v
and a nonzero out-degree must have a path into it from the unit

∈

V with

σ (v)
|
|

= 1, σ (v)

= I,

v
∀

σ (v)
V.(
|
|

∈

= 1)

∧

(deg+(v) > 0)

P(I, v)
(
|
|

→

> 0).

Deﬁnition 5 (Directed multigraph with monoidal points). Given a directed multigraph G meeting Con-
dition 1, the quiver Q = (VQ, EQ) where

VQ = V

EQ = E

I

}

∪ {

(cid:91)

∪

(σ (v1)σ (v2))
{

∈

V :(I,v1),(I,v2)

E

∈

(I, σ (v1)σ (v2))
}
{
}

has paths from the unit vertex I to its vertices with labels longer than one symbol. Adding the extra edges
to the quiver ensures that any two generating vertices with paths from the unit give rise to a compound
vertex with a path from the unit. Note that the original multigraph G may already include a vertex for I;
this construction simply ensures one is added if it doesn’t.

Deﬁnition 6 (Free monoidal category on a directed multigraph). Assume that Q = (VQ, EQ) is a quiver
constructed via Deﬁnition 5. The free monoidal category Free(Q) (notation due to Gavranovic [23]) on
that quiver has as objects ﬁnite Cartesian products of vertex labels

Ob(Free(Q)) :=

(cid:91)

n
∈

σ (v) : v
{
N

VQ

n.
}

∈

(cid:54)
6

A Probabilistic Generative Model of Free Categories

We deﬁne the hom-sets for the free monoidal category inductively. For each A, B
1,

= 1,

σ (B)
|
|

VQ such that

σ (A)
|
|

=

∈

Free(Q)(σ (A), σ (B)) := P(A, B).

For A, B,C, D

∈

Free(Q), none of which equals I,

Free(Q)(σ (A)σ (C), σ (B)σ (D)) :=

( f , g) : f

{

∈

Free(Q)(σ (A), σ (B)), g

Free(Q)(σ (C), σ (D))
}

.

∈

Finally, for each A

∈

Free(Q) we deﬁne the identity morphism to be

idA = /0

∈

Free(Q)(A, A).

The above construction gives the free (Cartesian) monoidal product on paths in a graph, with the
monoidal action on objects being concatenation of vertex labels. This category has as morphisms ﬁ-
nite tuples of paths between the respect elements of ﬁnite tuples of vertices.

3 Probabilistic categorical generative modeling

3.1 Assigning probabilities to morphisms by enrichment in a Markov category

Deﬁnition 6 deﬁned the free category over a given quiver, giving a purely syntactic description of compo-
sitional structure. Constructing a probabilistic generative model over such structure will require deﬁning
a version of the free category Free(Q) enriched in a Markov category, in our case QBS. This requires
ﬁrst showing that hom-sets in Free(Q) admit the structure of objects in the enriching category.
Lemma 2 (Hom-sets in free (monoidal) categories admit the structure of quasi-Borel spaces). Let G
be a directed multigraph and Free(Q) its free monoidal category as deﬁned above. Each hom-set
Free(Q)(A, B) can admit the structure of a quasi-Borel space.

Proof. An object in QBS is a pair (X, MX ) with X a set and MX
X] a set of primitive random
→
V we already have a hom-set Free(Q)(A, B), and just need to
variables over that space. For each A, B
construct a corresponding set of random variables. We take advantage of the countable-discrete nature
of the underlying path-set Free(Q)(A, B) to construct the standard discrete σ -algebra over the set, which
Free(Q)(A, B). We therefore
we call ΣQ(A,B). We then call MBA the set of measurable functions R
have (Free(Q)(A, B), MBA))

QBS as required.

[R

→

⊆

∈

∈

Deﬁning quasi-Borel spaces with path-sets as their sample spaces enables deﬁning a probabilistically
enriched free category. Recall from Lemma 1, P is the probability monad in QBS. Furthermore, we
deﬁne the enrichment here with a twist: since we require a probabilistic generative model, composition
builds up joint distributions, without marginalizing away intermediate spaces.
Deﬁnition 7 (Probabilistic generative free category). The probabilistic generative category P Q is a
version of Free(Q) enriched in QBS. We construct it by specifying

• Its objects Ob(P Q) = Ob(Free(Q));

• For each (A, B)

Ob(P Q)

Ob(P Q) the hom-object P Q(A, B) = P (Free(Q)(A, B), MBA)

• For each A

∈

∈

×
Ob(P Q) the identity element

QBS;

∈

idA : QBS(I, P (Free(Q)(A, A), MAA))
idA = I

ηFree(Q)(A,A)(const /0 : (Free(Q)(A, A), MAA))

(cid:55)→

gives a Dirac delta distribution over the empty path in Free(Q)(A, A);

E. Sennesh, T. Xu, & Y. Maruyama

7

• For each A, B,C

∈

Ob(P Q) a way of composing morphisms, in this case

A,B,C : P Q(A, B)

f

(cid:35)
A,B,C g = ( f
(cid:35)

(cid:12)

(cid:12)
QBS g)

P Q(B,C)
y
P (x
(cid:35)

(cid:12)

(cid:55)→

→

P Q(A,C)
ηFree(Q)(A,C)(x

Free(Q) y)),
(cid:35)

pushes-forward the joint distribution deﬁned by f and g into a distribution over Free(Q)(A,C).

• P Q inherits its monoidal structure from QBS

A,B,CD : P Q(A, B)

P Q(C, D)

(cid:12)
A,B,C,D g = ( f

×
QBS g).

(cid:12)

f

(cid:12)

P Q(A

C, B

D)

(cid:12)

(cid:12)

→

The usual probabilistic composition uses a random outcome from one distribution to parameterize
another. Here we take their monoidal product, representing their joint distribution.

Above we deﬁned operads WOb(C ) of wiring diagrams, and algebras over them. Now, having an
enriched category P Q, we can demonstrate the existence of an enriched operad algebra mapping wiring
diagrams Φ into P Q.

Lemma 3 (Existence of algebras of wiring diagrams over the probabilistic generative free category).
We consider the concrete setting where C = P Q, the probabilistic generative free category, with the
enriching category being V = QBS. An operad algebra in this setting has the form H : WOb(C ) →
Op(QBS), which sends a box t = (t
QBS and
the collection of hom-distributions in a wiring diagram Φ to their joint distribution within the space of
distributions over paths from the input and output types of the diagram as a whole.

WOb(C ) to the quasi-Borel space P Q(t

, t+)

, t+)

∈

∈

−

−

Proof. Deﬁnition 7 (of P Q) gives the machinery for forming τ-typed hom-distributions, and joint hom-
distributions, in QBS. Since QBS is an SMC and P Q inherits its monoidal structure, P Q supports an
operad algebra as an instantiation of Proposition 1 above. This construction requires that the hom-
distribution assigned to each box in a wiring diagram be conditionally independent of all others, given
the wiring diagram itself.

3.2 The free category prior over morphisms

Deﬁnition 7 demonstrates that free categories allow enrichment with joint distributions, and similarly,
Lemma 3 demonstrates the existence of operad algebras taking wiring diagrams into joint distributions
over morphisms in the free category. These existence proofs alone do not constitute a generative model:
that requires a distribution with constructive procedures for sampling and density evaluation. This section
will deﬁne such procedures and the joint distribution they induce on morphisms.

A probabilistic generative model over some space must generally impose some notion of simplicity,
assigning higher prior probability to simpler elements of the space. Free monoidal categories have only
two kinds of structure that give rise to complexity: sequential and parallel composition. We hypothesize
that path-length and product-width in the underlying quiver may serve as a reasonable inductive bias.
Shorter, “narrower” paths should have higher probability, but no path should have zero probability.

We thus begin with our quiver Q representing the free category, and a wiring diagram Φ. Each box
dom(Φ) consists of a source and target ti
−

VQ. To generate morphisms, we need a policy

, ti

ti

∈

+ ∈
v, t+) : EQ(v,

π(e

|

(0, 1]

)
·

→

8

A Probabilistic Generative Model of Free Categories

that randomly selects edges out of v, yielding short paths to the destination t+. This is a stochastic
short-paths problem, generalizing the all-destinations shortest-path problem (which has itself inspired a
learning problem [36]) to a setting of probabilistic transitions.

We construct our stochastic short-paths problem by transforming the original nerve quiver Q into a

simple directed bipartite graph G(cid:48) = (V (cid:48), E(cid:48)) for

V (cid:48) = VQ

EQ

∪

E(cid:48) =

(cid:91)

e

EQ

∈

(dom(e), e), (e, cod(e))
}

.

{

∈

VQ and another of the
This directed graph has two sets of vertices: one of the generating objects v
generating morphisms e
EQ. In this graph, generating object vertices only receive edges from generat-
ing morphism vertices, and vice-versa, while each generating morphism vertex has only a single exiting
edge. No two vertices have multiple edges between them.
Lemma 4 (Equivalence of free categories under conversion from quiver to directed bipartite graph).
The original free category Free(Q) is equivalent to the one generated by taking the directed bipartite
graph G(cid:48), converting it back into a quiver Q(cid:48) according to the above construction, and generating a free
category Free(Q(cid:48)) from that.

∈

Proof. Representing both graphs via their adjacency matrices will demonstrate equivalence of the graphs,
with proofs due to Brualdi [5] and others as old results. Equivalence of the underlying graphs then
produces an equivalence of their free categories.

We represent Q as G(cid:48) because the latter supports representing our policy as a transition kernel π

V (cid:48)|×|

R|

∈
V (cid:48)|, Estrada and Hatano [14] deﬁned the

QBS(V (cid:48),V (cid:48)). Given the directed adjacency matrix A(cid:48) ∈
communicability between ti
−

and ti

+ as

Cti

−

,ti
+

=

(A(cid:48)

∞
∑
n=1

l)ti
−
n!

,ti
+

= (eA(cid:48))ti

−

.

,ti
+

(1)

This communicability measure, also known as the matrix exponential, measures the number of paths in
the underlying graph, weighted by their length. Its negative logarithm

d(ti
−

, ti

+) =

log(C)ti

,ti
+

−
provides a quasimetric2 of “intuitive distance” on the graph [1]. Intuitive distance in a graph penalizes
actual path length, while rewarding paths connecting many sources and targets.

−

We therefore parameterize the policy in log-space log π(e

VQ, with global random variables

v; t+, Q), for global hyperparameters Q

|

N ((cid:126)0,(cid:126)1)

R|

V (cid:48)|: a vector of single-step “preferences” over objects and morphisms sampled

and t+ ∈
• w
∼

∈
from a standard Gaussian prior;

(2)

• β

γ(1, 1)

R+: a positive inverse-temperature specifying the “conﬁdence” of the policy, sam-

∼

∈
pled from a Gamma prior;

and the local random variable v
The policy is then written in terms of its surprisal as

∈

VQ. v is a “present location” in the graph G(cid:48), obtained autoregressively.

log π(e

|

−

v, w, β ; t+, Q) ∝

1
β

−

log (cid:0)C + (A(cid:48))diag(w)(cid:1)

cod(e),t+

,

(3)

and we have the following theorem relating the policy to the global structure of the quiver Q.

2Lacking symmetry due to directedness of edges.

E. Sennesh, T. Xu, & Y. Maruyama

9

Theorem 1 (The intuitive distance lower-bounds expected policy surprisal). Under the prior distribu-
tions on the global random variables, the expected unnormalized surprisal of the policy (Equation 3) is
at least the intuitive distance (Equation 2)

d(cod(e), t+)

≤

Ew

N ,β

∼

γ(1,1) [
∼

−

log π(e

|

v, w, β ; t+, Q)] .

Proof. We begin by expanding the deﬁnition of the expectation

γ(1,1) [
∼

−

log π(e

v, w, β ; t+, Q)] = Ew
∼

N

|

Eβ

γ(1,1)

∼

−

(cid:20)

(cid:20)

1
β

log (cid:0)C + (A(cid:48))diag(w)(cid:1)

cod(e),t+

(cid:21)(cid:21)

Ew

N ,β

∼

and recognizing that our Gamma prior has a mean of 1. Substituting the inner expectation away and
cancelling, we have

Ew

N ,β

∼

γ(1,1) [log π(e
∼

|

v, w, β ; t+, Q)] = Ew
∼

N

(cid:104)
log (cid:0)C + (A(cid:48))diag(w)(cid:1)

cod(e),t+

(cid:105)

.

Jensen’s inequality for log-expectations says an expected-log lower bounds a log-expectation

Ew

N ,β

∼

γ(1,1) [log π(e
∼

|

v, w, β ; t+, Q)]

≤

≤

N

log (cid:0)
Ew
log (cid:0)C + Ew

∼

(cid:2)C + (A(cid:48))diag(w)(cid:3)(cid:1)
(cid:2)(A(cid:48))diag(w)(cid:3)(cid:1)

N

∼

cod(e),t+

cod(e),t+

,

and we can now substitute in the standard normal’s mean
(cid:17)
(cid:16)
C + (A(cid:48))diag((cid:126)0)

v, w, β ; t+, Q)]

log

N ,β

Ew

γ(1,1) [log π(e
∼

|

∼

≤

cod(e),t+

−

≤
log (C)cod(e),t+ ≤
d(cod(e), t+)
≤

log (C)cod(e),t+
γ(1,1) [
Ew
N ,β
∼
γ(1,1) [
∼

N ,β

Ew

∼

∼

−

−

log π(e

log π(e

|

|

v, w, β ; t+, Q)]
v, w, β ; t+, Q)] .

The policy’s parametric form thus normalizes to

π(e

|

v, w, β ; t+, Q) = softmin

(cid:18)

−

1
β

log(C + (A(cid:48))diag(w))
,t+
·

(cid:19)

.

cod(e)

(4)

Theorem 1 demonstrates that Equation 4 assigns “energy” to ﬁnite edge-sets proportionately to a stochas-
tic upper bound on their intuitive distance, scales the energies according to its “conﬁdence” β , and
employs soft minimization to assign probabilities to edges. When arrival to v = t+ samples a path
f = (e1, . . . , eL) : t

t+ of length L, it has probability

− →

P( f

|

w, β ; t

−

, t+, Q) = π(e1

, w, β ; t+, Q)

t

−

|

L
∏
l=2

π(el

cod(el

1), w, β ; t+, Q).
−

|

(5)

, t+ ∈

Equation 5 relies on the explicit quiver representation Q, and so can only “connect” generator objects
, t+). However, the second term deﬁning EQ in Deﬁni-
VQ to deﬁne a hom-distribution P Q(t
t
−
−
t2
tion 5 deﬁnes a series of edges in Q mapping I
+). These edges represent “points” in the
product object, and their semantic content is to recursively invoke the free category prior to generate
f : I

t2
+, and then ﬁnally combine them into f

→
We employ “macro expansion” rather than split and simplify boxes in wiring diagrams because some
applications may supply their own generating morphisms into product objects. Our approach here simply
supplies points in product objects when the component objects support points.

t1
+, g : I

t1
+ (cid:12)

P Q(t1

+ (cid:12)

t2
+.

g : I

→

→

→

(cid:12)

10

A Probabilistic Generative Model of Free Categories

3.3 The complete generative model over wiring diagrams

In summary, given a quiver Q and a wiring diagram Φ : t1
generating a morphism f

P Q ﬁtting Φ proceeds as

∼

. . .

tn

×

×

→

v, the sampling process for

i
∀
This procedure induces a joint distribution over all latent variables

[1..n]. fi

w, β ; ti
−

, ti

∼

∈

|

+, Q).

β
w

∼

∼

γ(1, 1)
N (0, 1)
P(fi

p( f , w, β ; Φ, Q) = p(β )p(w) ∏
t
Φ
∈

P( ft

|

w, β ; t, Q).

(6)

→

C into an arbitrary SMC C and a likelihood p(x

Finally, in order to learn morphisms, an application must supply both a concrete semantic functor F :
Free(Q)
Theorem 2 (Bayesian learning with free category priors). Assuming that the semantic category C sup-
ports enrichment in QBS via joint distributions, the free category prior indeed acts as a prior. Sampling
p( f ; Φ, Q) from the prior, assigning semantics F( f ) to that morphism, and evaluating
a morphism f
∼
F( f )) relating those semantics to data induces a joint density and Bayesian inverse.
a likelihood p(x
|

F( f )) relating morphisms to data.

|

Proof. We can “lift” the semantics F : Free(Q)
the action on morphisms

→

C into a functor P F : P Q

QBS

−

→

C by deﬁning

) : P Q(A, B)
P F(
·
P F(p) = p

P ( f

(cid:35)

(cid:55)→

QBS

C (F(A), F(B))

→

−
ηF(B)(F( f ))),

to push-forward P Q(A, B) via F( f ). The semantics and likelihood then deﬁne a joint density

with a Bayesian inversion

p(x, f , w, β ; Φ, Q) = p(x

F( f ))p( f , w, β ; Φ, Q)

|

p( f , w, β

|

x; Φ, Q) =

p(x, f , w, β ; Φ, Q)
p(x; Φ, Q)

.

(7)

(8)

Section 4 will discuss the software implementation of the free category prior, and an experiment in

which a concrete quiver Q is assigned speciﬁc semantics and likelihood to perform learning.

4 Machine learning with the free category prior

For machine learning applications, we provide a software implementation of the free category prior. We
call it Discopyro; it is available as a Python library on top of the Discopy [16] library for computing with
morphisms in monoidal categories and the Pyro [2] probabilistic programming language. The Discopyro
implementation takes the directed multigraph G as a hyperparameter; constructs its corresponding Q,
Free(Q), and P Q; and uses those to conditionally sample morphisms for application-speciﬁc wiring
diagrams Φ. The Discopyro software supports implicitly associating each edge (generating morphism)
EQ with an instance of discopy.monoidal.Diagram, representing an arbitrary SMC C via Dis-
e
C to interpret
copy. The user can also apply a Discopy Functor deﬁning the semantics F : Free(Q)
morphisms into the chosen category.

→

∈

E. Sennesh, T. Xu, & Y. Maruyama

11

4.1

Implementing the sampling process

To specify wiring diagrams in Python, we have added wiring diagrams to a fork of Discopy. Our imple-
mentation is fairly naive and just consists of Box, Id wires, and Sequential and Parallel composites,
all subclasses of a wiring.Diagram base class. These come equipped with a collapse() method
which implements the canonical catamorphism on the algebraic data type of wiring diagrams. We then
implement wiring-diagram algebras for sampling from the P Q representation via F-algebras on wiring
diagrams. Python pseudocode describing Discopyro’s implementation will denote sequential (categori-
cal) and parallel (monoidal product) compositions by their Discopy operators >> and @.

We represent the free category prior deﬁned in Section 3.2 as a program that samples from the model
in the Pyro [2] probabilistic programming language. We give the name path_through to the sample
procedure deﬁned above, and give pseudocode for its implementation in Listing 1.

def path_through(self, wbox, energies, temperature):

loc = wbox.dom
f = idA
while loc != wbox.cod:

logits = [energies[a, wbox.cod] for a in self.out_arrows(loc)]
idx = pyro.sample(Categorical(logits=logits / temperature))
arrow = self.out_arrows(loc)[idx]
if not isinstance(arrow, Box):

wiring = [wiring.Box(I, ob)] for ob in arrow.dest.objects]
wiring = reduce(λ x, y: x @ y, wiring, wiring.Id(I))
arrow = self.sample_morphism(wiring, energies)

f = f >> arrow # Composition of morphisms
loc = arrow.cod

return f

Listing 1: Generative model for short paths from A to B in the nerve Q.

Listing 2 then extends the short-paths procedure from individual boxes to whole wiring diagrams,
implementing the operad algebra of Lemma 3. Since probability distributions are once again represented
by random samplers and wiring diagrams by combinatorial data structures, the algebra over wiring dia-
grams is written as an F-algebra over the wiring diagrams themselves.

4.2 Amortized inference with the free category prior

Learning morphisms from data via a likelihood requires approximately estimating the marginal likeli-
hood p(x; Φ, Q) and thereby approximating the Bayesian inversion (Equation 8). Discopyro provides
amortized variational inference over its own random variables via neural proposals, whose parameters
we label φ , for the “conﬁdence” β

x) and the edge “preferences” w

x).

qφ (w

qφ (β

The samples from these proposal distributions then parameterize a proposal, identical to the gen-
w, β ; Φ, Q). This gives us a complete proposal for the latent variables

∼

|

∼

|

erative model, over fθ ∼
induced by the free category prior itself,

P( fθ |

qφ ( fθ , w, β

x; Φ, Q) = P( fθ |

|

w, β ; Φ, Q)qφ (β

x)qφ (w

x).

|

|

Finally, if an application programmer wants to make use of Discopyro’s amortized inference func-
fθ , x) which approximates the posterior distribution

tionality, they must supply a proposal qφ (F( fθ )

|

12

A Probabilistic Generative Model of Free Categories

def __sampler_falg__(self, f, energies, temp):

if isinstance(f, Id):

return ar_factory.id(f.dom)

if isinstance(f, Box):

return self.path_through(f, energies, temp)

if isinstance(f, Sequential):

return reduce(λ f, g: f >> g, arrows)

if isinstance(f, Parallel):

return reduce(λ x, y: x @ y, factors, ar_factory.id(Ty()))

def sample_morphism(self, wdiagram, energies, temperature):

falg = λ f: self.__sampler_falg__(f, energies, temperature)
return wdiagram.collapse(falg)

Listing 2: The operad functor mapping wiring diagrams to morphisms in Free(Q)

Model
Sequential Attention
Variational Homoencoder (PixelCNN)
Graph VAE
Generative Neurosymbolic
Free Category DGM (ours)

Image Size Learns Structure

28x28
28x28
28x28
105x105
28x28

(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)

log- ˆZ
-95.5
-61.2
-104.6
-383.2
-11.6

log- ˆZ/dim
-0.1218
-0.0780
-0.1334
-0.0348
-0.0148

Table 1: Average log model evidence on the Omniglot evaluation set across deep generative models

over the morphism’s semantics F( fθ ). This ﬁnal piece induces a joint proposal density

qφ (F( fθ ), fθ , w, β

|

x; Φ, Q) = qφ (F( fθ )

fθ , x)qφ ( fθ , w, β

x; Φ, Q),

|

|

(9)

on the parameters w and β , the string diagram structure fθ (with internal parameters θ ) compatible with
the wiring diagram Φ, and the semantics F( f ). Maximizing the ELBO (Equation 12)

L (θ , φ ) = Eqφ

(cid:20)

log

pθ (x, F( fθ ), fθ , w, β ; Φ, G)

(cid:21)

qφ (F( fθ ), fθ , w, β

x)

|

by stochastic gradient ascent adjusts the proposal to approximate the true posterior distribution (a process
called variational Bayesian inference [3]). Appendices A and B derive and justify this objective function.

4.3 Experiment: representing Omniglot with deep generative models

As a demonstrative experiment, we constructed a graphical nerve G whose edges (generating morphisms)
implemented probabilistic programs in Pyro by inheriting from discopy.cartesian.Box. We trained
the resulting free category model on the Omniglot challenge dataset for few-shot learning [39].

Internally, the edges implemented functions consisting of neural building blocks (ie: neural networks
elements with probabilistic sampling) for deep generate models. Each generating morphism consisted
not only of a domain, codomain, and function but also of a trace-type for its random sampling. Lew et

E. Sennesh, T. Xu, & Y. Maruyama

13

(a) Characters from the Omniglot evaluation set
(above) and our model’s reconstructions (below)

(b) An example string diagram drawn from the ap-
proximate posterior of the free category model.

Figure 1: Reconstructions (left) generated by inference in the diagrammatic generative model (right) on
handwritten characters in the Omniglot evaluation set. The string diagram shows a model that generates
a glimpse, decodes it into an image canvas via a variational ladder decoder, and then performs a simpler
process to generate another glimpse and insert it into the canvas.

al. [41] gave semantics in QBS to traced probabilistic programs with a monoidal structure over traces3

P τ : QBS(A, P (B
(cid:35)

×

τ))

×

QBS(B, P (C

τ))

×

→

QBS(A, P (C

τ)).

×

This additional monoidal structure induces an appropriately “traced”4 probability monad and Kleisli
category, which we call P QBSτ as a subcategory of QBS. The implied semantics functor and likelihood
function apply the monadic multiplier of P in QBS to yield only a single-level probability endofunctor.
Our experiment assumes data x

28, and that morphisms therefore induce the joint likelihood
R28
×

∈

pθ (x
p(x

|

z, f ) = N (µ θ (z, f ), τI)
z, f )pθ (z

|
F( f )) = pθ (x

|

f )

|

and semantics proposal over the random variable x

qφ (F( f )

|

f , x) = qφ (z

f †
φ , x).

|

The random variable z denotes unobserved random variables occurring in the randomness trace when

running fθ on data x. f †

P QBSτ with action on objects

φ is the result of an endofunctor Fq : P QBSτ →
(cid:40)

Fq(A) =

D

if A = R
A otherwise,

D

R
A

(cid:12)

and action Fq( fθ ) : P QBSτ (A, B)
P QBSτ (Fq(B), Fq(A)) on morphisms fθ : P QBSτ (A, B) is Fq( fθ ) =
φ . Each f †
f †
φ is represented by a neural network with parameters φ . The inference functor “doubles”
objects in order to produce, for each piece of data and each latent variable, two parameters µ, σ for a
Gaussian proposal with the appropriate trace-type.

→

3Note that τ here refers to trace types, not to wire types in a wiring diagram.
4In the sense of program execution, not traced categories.

R8R196R2R784R49R196R784R784p(Z4,Z8|R4)p(Z196|Z8)p(Z2)p(Z784|(R196×R2))p(Z24,Z49|R24)p(Z196|Z49)p(Z3|Z784×Z196)p(X784|R784)14

A Probabilistic Generative Model of Free Categories

Table 1 compares the free category prior’s performance to other structured generative models (de-
scribed in Appendix D). We report the estimated log model evidence (estimated as described in Ap-
pendix A). Our free category prior over deep generatives models achieves the best log-evidence per data
dimension. Figure 1 shows samples from the trained model’s posterior distribution, including recon-
struction of evaluation data (Figure 1a) and the morphism sampled for that data (Figure 1b).

5 Discussion

This paper described a probabilistic generative model over free categories. Section 3 gave a novel cat-
egorical description of distributions over morphisms, by enriching in QBS with joint distributions and
push-forwards. It then showed how to sample morphisms from QBS-enriched free monoidal categories
via a stochastic short-paths policy. Section 4 described the software implementation of the free category
prior as Discopyro, showing that enriching in a Markov category can also be understood as randomly
sampling the computational representations of morphisms in an SMC. It then showed that amortized
variational inference in Discopyro achieved competitive log-likelihood in generative modeling of the
Omniglot dataset. This section discusses potential future work.

Future improvements to the free category prior The underlying quiver Q could be expanded stochas-
tically by randomly choosing and applying commuting diagrams (such as functors, universal construc-
tions, etc.). In the inﬁnite limit, such expansions would give a Q encoding the “true” category given by
the original graph and the commuting diagrams: a (nonparametric) probability model P(Q) over the cat-
egory. This would provide a nonparametric category model as an alternative to nonparametric grammar
(i.e. operad) models [35]. Stochastic memoization [58] would enable sampling from that model, with
Russian Roulette methods [71] providing gradient estimates of Equation 12.

We deﬁned the free category prior in terms of Estrada and Hatano’s [14] communicability quasimet-
ric. Boyd et al. [4] recently published a proper metric on quivers and their Markov chains, which might
provide a more interpretable foundation for our stochastic short-paths algorithm. Stochastically general-
izing shortest-path algorithms may improve the tractability of credit-assignment in policy learning. We
plan to explore a subgoal decomposition (e.g. such as Jurgenson’s[36]) of path-sampling.

Future improvements to the Discopyro implementation Discopyro employs amortized variational
inference to model the posterior distribution over morphisms. The inference functor mentioned in Sec-
tion 4.2 was deﬁned ad-hoc, meeting only the requirements for constructing faithful inverses [70] for
string diagrams. We used QBS as a semantic category for our learned probabilistic programs, and it is
not yet known whether QBS and QBS have all the conditionals necessary to encode Bayesian inversion
as a dagger functor. Conjecture 1 hypothesizes explicitly that an inference construction similar to ours
could approximate Bayesian inversions (in the sense of Cho and Jacobs [6]), rather than just a proposal.
Conjecture 1 (Approximate Bayesian inversions in P QBS). Consider an inference functor F † whose
action on objects is identity, with action on morphisms similar to Fq above. Such a functor is a dagger
endofunctor F † : P QBS
P QBS sending objects to themselves and morphisms f : QBS(A, P B) to
→
(approximate) Bayesian inversions f † : QBS(B, P A).

Conclusion In our experiment the free category prior learned compositional structures from real data.
Future applications could include learning program-like representations, a long-term goal for artiﬁcial
intelligence [7, 39, 48], or “cognitive maps” for spatial navigation [43] in cognitive science [46].

E. Sennesh, T. Xu, & Y. Maruyama

15

References

[1] Alon B Baram, Timothy H Muller, James CR Whittington & Timothy EJ Behrens (2018): Intuitive planning:

global navigation through cognitive maps based on grid-like codes. bioRxiv, p. 421461.

[2] Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos,
Rohit Singh, Paul Szerlip, Paul Horsfall & Noah D Goodman (2019): Pyro: Deep universal probabilistic
programming. The Journal of Machine Learning Research 20(1), pp. 973–978.

[3] David M. Blei, Alp Kucukelbir & Jon D. McAuliffe (2017): Variational Inference: A Review for Statis-
ticians, doi:10.1080/01621459.2017.1285773. Available at http://arxiv.org/abs/1601.00670http:
//dx.doi.org/10.1080/01621459.2017.1285773.

[4] Zachary M. Boyd, Nicolas Fraiman, Jeremy Marzuola, Peter J. Mucha, Braxton Osting & Jonathan Weare
(2021): A Metric on Directed Graphs and Markov Chains Based on Hitting Probabilities. SIAM Journal on
Mathematics of Data Science 3(2), pp. 467–493, doi:10.1137/20m1348315.

[5] Richard A. Brualdi, Frank Harary & Zevi Miller (1980): Bigraphs versus digraphs via matrices.

Jour-
nal of Graph Theory 4(1), pp. 51–73, doi:https://doi.org/10.1002/jgt.3190040107. Available at https:
//onlinelibrary.wiley.com/doi/abs/10.1002/jgt.3190040107.

[6] Kenta Cho & Bart Jacobs (2019): Disintegration and Bayesian inversion via string diagrams. Mathematical

Structures in Computer Science 29(7), pp. 938–971, doi:10.1017/S0960129518000488.

[7] Franc¸ois Chollet (2019): On the Measure of Intelligence, pp. 1–64. Available at http://arxiv.org/abs/

1911.01547.

[8] Nicolas Chopin & Omiros Papaspiliopoulos (2020): An Introduction to Sequential Monte Carlo. Springer,

doi:10.1007/978-3-030-47845-2.

[9] Stephen Clark, Bob Coecke & Mehrnoosh Sadrzadeh (2008): A compositional distributional model of mean-
ing. In: Proceedings of the Second Quantum Interaction Symposium (QI-2008), Schuetze 1998, pp. 133–140.

[10] G. S. H. Cruttwell, Bruno Gavranovi´c, Neil Ghani, Paul Wilson & Fabio Zanasi (2021): Categorical Foun-
dations of Gradient-Based Learning. In: Applied Category Theory Conference (ACT 2021). Available at
http://arxiv.org/abs/2103.01931.

[11] Jared Culbertson & Kirk Sturtz (2014): A categorical foundation for bayesian probability. Applied Categor-

ical Structures 22(4), pp. 647–662, doi:10.1007/s10485-013-9324-9.

[12] Fredrik Dahlqvist, Alexandra Silva, Vincent Danos & Ilias Garnier (2018): Borel Kernels and their
Approximation, Categorically.
Electronic Notes in Theoretical Computer Science 341, pp. 91–
119, doi:https://doi.org/10.1016/j.entcs.2018.11.006. Available at https://www.sciencedirect.com/
science/article/pii/S1571066118300860. Proceedings of the Thirty-Fourth Conference on the Math-
ematical Foundations of Programming Semantics (MFPS XXXIV).

[13] Conal Elliott (2017): Compiling to categories. Proceedings of the ACM on Programming Languages

1(ICFP), pp. 1–27, doi:10.1145/3110271.

[14] Ernesto Estrada & Naomichi Hatano (2008): Communicability in complex networks. Physical Review E -

Statistical, Nonlinear, and Soft Matter Physics 77(3), pp. 1–12, doi:10.1103/PhysRevE.77.036111.

[15] Reuben Feinman & Brenden M. Lake (2021): Learning Task-General Representations with Generative
International Conference on Learning Representations. Available at

In:

Neuro-Symbolic Modeling.
http://arxiv.org/abs/2006.14448.

[16] Giovanni de Felice, Alexis Toumi & Bob Coecke (2020): DisCoPy: Monoidal Categories in Python. In:
Applied Category Theory Conference, pp. 1–20. Available at http://arxiv.org/abs/2005.02975.

[17] Brendan Fong & Michael Johnson (2019): Lenses and learners. CEUR Workshop Proceedings 2355(Bx),

pp. 16–29.

16

A Probabilistic Generative Model of Free Categories

[18] Brendan Fong, David Spivak & Remy Tuyeras (2019): Backprop as Functor: A compositional perspective
on supervised learning. Proceedings - Symposium on Logic in Computer Science 2019-June, pp. 1–13,
doi:10.1109/LICS.2019.8785665.

[19] Brendan Fong & David I Spivak (2019): Seven Sketches in Compositionality: An Invitation to Applied Cate-

gory Theory. Cambridge University Press. Available at http://arxiv.org/abs/1803.05316.

[20] Steven M. Frankland & Joshua D. Greene (2020): Concepts and Compositionality: In Search of the Brain’s
Language of Thought. Annual Review of Psychology 71(1), pp. 273–303, doi:10.1146/annurev-psych-
122216-011829.

[21] Tobias Fritz (2020): A synthetic approach to Markov kernels, conditional independence and theorems on
sufﬁcient statistics. Advances in Mathematics 370, p. 107239, doi:10.1016/j.aim.2020.107239. Available at
https://doi.org/10.1016/j.aim.2020.107239.

[22] Peter Gardenfors (2004): Conceptual spaces: The geometry of thought. MIT press.

[23] Bruno Gavranovic (2019): Learning functors using gradient descent. In: Applied Category Theory Con-
ference (ACT 2019), 323, Electronic Proceedings in Theoretical Computer Science, EPTCS, pp. 230–245,
doi:10.4204/EPTCS.323.15.

[24] Michele Giry (1982): A categorical approach to probability theory. In: Categorical aspects of topology and

analysis, Springer, pp. 68–85.

[25] Petr Viktorovich Golubtsov (1999): Axiomatic description of categories of information transformers. Prob-

lemy Peredachi Informatsii 35(3), pp. 80–98.

[26] Erin Grant,
for concept
tive Science Society,
paper/Learning-deep-taxonomic-priors-for-concept-learning-Grant-Peterson/
8eef236bca7ed58f8fa96786925c0e1da4a124eb.

Joshua C. Peterson & Tom Grifﬁths
learning from few positive examples.
pp. 1865–1870.

Learning deep taxonomic priors
(2019):
In:
the Cogni-
at https://www.semanticscholar.org/

The Annual Meeting of

Available

[27] Edward Grefenstette & Mehrnoosh Sadrzadeh (2011): Experimental support for a categorical compositional
distributional model of meaning. In: EMNLP 2011 - Conference on Empirical Methods in Natural Language
Processing, Proceedings of the Conference, pp. 1394–1404.

[28] Micah Halter, Evan Patterson, Andrew Baas & James Fairbanks (2020): Compositional Scientiﬁc Computing

with Catlab and SemanticModels, pp. 1–3. Available at http://arxiv.org/abs/2005.04831.

[29] Uri Hasson, Samuel A. Nastase & Ariel Goldstein (2020): Direct Fit

tionary Perspective on Biological and Artiﬁcial Neural Networks.
doi:10.1016/j.neuron.2019.12.002. Available at https://doi.org/10.1016/j.neuron.2019.12.002.

to Nature: An Evolu-
Neuron 105(3), pp. 416–434,

[30] Jiawei He, Yu Gong, Greg Mori, Joseph Marino & Andreas M. Lehrmann (2019): Variational autoencoders
with jointly optimized latent dependency structure. In: 7th International Conference on Learning Represen-
tations, ICLR 2019, pp. 1–16.

[31] C. Hermida, M. Makkai & J. Power (1998): Higher dimensional multigraphs.

In: Proceedings. Thir-
teenth Annual IEEE Symposium on Logic in Computer Science (Cat. No.98CB36226), pp. 199–206,
doi:10.1109/LICS.1998.705656.

[32] Chris Heunen, Ohad Kammar, Sam Staton & Hongseok Yang (2017): A convenient category for
In: Proceedings - Symposium on Logic in Computer Science,

higher-order probability theory.
doi:10.1109/LICS.2017.8005137.

[33] Luke B. Hewitt, Maxwell I. Nye, Andreea Gane, Tommi Jaakkola & Joshua B. Tenenbaum (2018): The
Variational Homoencoder: Learning to learn high capacity generative models from few examples. 34th
Conference on Uncertainty in Artiﬁcial Intelligence 2018, UAI 2018 2, pp. 988–997.

[34] Mark K Ho & Tom Grifﬁths (2018): Human Priors in Hierarchical Program Induction.

In: Cognitive

Computational Neuroscience, 1. Available at http://lightbot.com.

E. Sennesh, T. Xu, & Y. Maruyama

17

[35] Mark Johnson, Thomas Grifﬁths & Sharon Goldwater (2006): Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models. Advances in neural information processing systems
19.

[36] Tom Jurgenson, Or Avner, Edward Groshev & Aviv Tamar (2020): Sub-goal trees – A framework for goal-
In: 37th International Conference on Machine Learning, ICML 2020, pp.

based reinforcement learning.
5020–5030.

[37] Brenden M. Lake & Steven T. Piantadosi (2019): People infer recursive visual concepts from just a few

examples. Computational Brain & Behavior. Available at http://arxiv.org/abs/1904.08034.

[38] Brenden M. Lake, Ruslan Salakhutdinov & Joshua B. Tenenbaum (2015): Human-level con-
cept
Science 350(6266), pp. 1332–1338,
doi:10.1126/science.aab3050. Available at http://science.sciencemag.org/content/350/6266/
1332https://www.sciencemag.org/content/350/6266/1332.full.pdf.

learning through probabilistic program induction.

[39] Brenden M. Lake, Ruslan Salakhutdinov & Joshua B. Tenenbaum (2019): The Omniglot challenge: a 3-year
progress report. Current Opinion in Behavioral Sciences 29, pp. 97–104, doi:10.1016/j.cobeha.2019.04.007.
Available at https://doi.org/10.1016/j.cobeha.2019.04.007.

[40] Guy Latouche & Vaidyanathan Ramaswami (1999): Introduction to Matrix Analytic Methods in Stochastic

Modeling. Society for Industrial and Applied Mathematics, Philadelphia, PA.

[41] Alexander K. Lew, Marco F. Cusumano-Towner, Benjamin Sherman, Michael Carbin & Vikash K. Mans-
inghka (2020): Trace Types and Denotational Semantics for Sound Programmable Inference in Probabilistic
Languages. In: ACM Principles of Programming Languages, 4, pp. 1–31, doi:10.1145/3371087.

[42] Gary F Marcus (2018): The algebraic mind: Integrating connectionism and cognitive science. MIT press.

[43] Jade Master (2021): The Open Algebraic Path Problem. In Fabio Gadducci & Alexandra Silva, editors: 9th
Conference on Algebra and Coalgebra in Computer Science (CALCO 2021), Leibniz International Proceed-
ings in Informatics (LIPIcs) 211, Schloss Dagstuhl – Leibniz-Zentrum f¨ur Informatik, Dagstuhl, Germany,
pp. 20:1–20:20, doi:10.4230/LIPIcs.CALCO.2021.20. Available at https://drops.dagstuhl.de/opus/
volltexte/2021/15375.

[44] Jan-Willem van de Meent, Brooks Paige, Hongseok Yang & Frank Wood (2018): An introduction to proba-

bilistic programming. arXiv preprint arXiv:1809.10756.

[45] Andriy Mnih & Karol Gregor (2014): Neural variational inference and learning in belief networks. In: 31st

International Conference on Machine Learning, ICML 2014, 5, pp. 3800–3809.

[46] Ida Momennejad (2020): Learning Structures: Predictive Representations, Replay, and Generalization.
Current Opinion in Behavioral Sciences 32, pp. 155–166, doi:10.1016/j.cobeha.2020.02.017. Available at
https://doi.org/10.1016/j.cobeha.2020.02.017.

[47] P K Murphy (2012): Machine Learning: A Probabilistic Perspective. doi:10.1007/SpringerReference 35834.

[48] Weili Nie, Zhiding Yu, Lei Mao, Ankit B. Patel, Yuke Zhu & Animashree Anandkumar (2020): Bongard-
In: Advances in Neural

LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning.
Information Processing Systems, NeurIPS. Available at http://arxiv.org/abs/2010.00763.

[49] Maxwell I. Nye, Armando Solar-Lezama, Joshua B. Tenenbaum & Brenden M. Lake (2020): Learning
compositional rules via neural program synthesis. Advances in Neural Information Processing Systems
2020-December(NeurIPS), pp. 1–11.

[50] Matthew C. Overlan, Robert A. Jacobs & Steven T. Piantadosi (2017): Learning abstract visual con-
cepts via probabilistic program induction in a Language of Thought. Cognition 168, pp. 320–334,
doi:10.1016/j.cognition.2017.07.005. Available at http://dx.doi.org/10.1016/j.cognition.2017.
07.005.

[51] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou & Pushmeet Kohli
(2017): Neuro-Symbolic Program Synthesis. In: International Conference on Learning Representations, pp.
1–14. Available at http://arxiv.org/abs/1611.01855.

18

A Probabilistic Generative Model of Free Categories

[52] Evan Patterson, Owen Lynch & James Fairbanks (2021): Categorical Data Structures for Technical Comput-

ing, pp. 1–28. Available at https://arxiv.org/abs/2106.04703.

[53] Evan Patterson, David I. Spivak & Dmitry Vagner (2021): Wiring diagrams as normal forms for computing
in symmetric monoidal categories. Electronic Proceedings in Theoretical Computer Science, EPTCS 333,
pp. 49–64, doi:10.4204/EPTCS.333.4.

[54] Steven Phillips & William H. Wilson (2010): Categorial Compositionality: A Category Theory Ex-
PLOS Computational Biology 6(7), pp. 1–14,

planation for the Systematicity of Human Cognition.
doi:10.1371/journal.pcbi.1000858. Available at https://doi.org/10.1371/journal.pcbi.1000858.

[55] Steven T. Piantadosi, Joshua B. Tenenbaum & Noah D. Goodman (2016): The logical primitives of thought:
Empirical foundations for compositional cognitive models. Psychological Review 123(4), pp. 392–424,
doi:10.1037/a0039980.

[56] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor & Daan Wierstra (2016): One-Shot

Generalization in Deep Generative Models. In: International Conference on Machine Learning, 48.

[57] Sergio Romano, Alejo Salles, Marie Amalric, Stanislas Dehaene, Mariano Sigman & Santiago Figueira
(2018): Bayesian validation of grammar productions for the language of thought. PLoS ONE 13(7), pp.
1–20, doi:10.1371/journal.pone.0200420.

[58] Daniel M Roy, VK Mansinghka, ND Goodman & JB Tenenbaum (2008): A stochastic programming per-
spective on nonparametric Bayes. In: Nonparametric Bayesian Workshop, Int. Conf. on Machine Learning,
22, p. 26.

[59] Dylan Rupel & David I. Spivak (2013): The operad of temporal wiring diagrams: formalizing a graphical
language for discrete-time processes, pp. 1–37. Available at http://arxiv.org/abs/1307.6894.

[60] John Schulman, Nicolas Heess, Theophane Weber & Pieter Abbeel (2015): Gradient estimation using
stochastic computation graphs. In: Advances in Neural Information Processing Systems, 2015-Janua, pp.
3528–3536.

[61] Adam ´Scibior, Ohad Kammar, Matthijs V´ak´ar, Sam Staton, Hongseok Yang, Yufei Cai, Klaus Oster-
mann, Sean K. Moss, Chris Heunen & Zoubin Ghahramani (2018): Denotational validation of higher-order
Bayesian inference. In: Principles of Programming Languages, 2, doi:10.1145/3158148.

[62] Dan Shiebler, Bruno Gavranovi´c & Paul Wilson (2021): Category Theory in Machine Learning. In: Applied

Category Theory Conference. Available at http://arxiv.org/abs/2106.07032.

[63] R J Solomonoff (1964): A formal theory of inductive inference. Part I. Information and Control 7(1), pp. 1–
22, doi:https://doi.org/10.1016/S0019-9958(64)90223-2. Available at http://www.sciencedirect.com/
science/article/pii/S0019995864902232.

[64] Morten Heine Sørensen & Pawel Urzyczyn (2006): Lectures on the Curry-Howard isomorphism. Elsevier.

[65] David I Spivak (2013): The operad of wiring diagrams: formalizing a graphical language for databases,

recursion, and plug-and-play circuits. arXiv preprint arXiv:1305.0297.

[66] Andreas Stuhlm¨uller, Jacob Taylor & Noah Goodman (2013): Learning Stochastic Inverses.

In C. J. C.
Burges, L. Bottou, M. Welling, Z. Ghahramani & K. Q. Weinberger, editors: Advances in Neural Informa-
tion Processing Systems, 26, Curran Associates, Inc. Available at https://proceedings.neurips.cc/
paper/2013/file/7f53f8c6c730af6aeb52e66eb74d8507-Paper.pdf.

[67] Lazar Valkov, Dipak Chaudhari, Charles Sutton, Akash Srivastava & Swarat Chaudhuri (2018): Houdini:
In: Advances in Neural Information Processing Systems, 2018-

Lifelong learning as program synthesis.
Decem, pp. 8687–8698.

[68] Paul M.B. Vit´anyi & Ming Li (2000): Minimum description length induction, Bayesianism, and Kolmogorov

complexity. IEEE Transactions on Information Theory 46(2), pp. 446–464, doi:10.1109/18.825807.

[69] R. F.C. Walters (1989): The free category with products on a multigraph. Journal of Pure and Applied

Algebra 62(2), pp. 205–210, doi:10.1016/0022-4049(89)90152-7.

E. Sennesh, T. Xu, & Y. Maruyama

19

[70] Stefan Webb, Adam Goli´nski, Robert Zinkov, N. Siddharth, Tom Rainforth, Yee Whye Teh & Frank Wood
(2018): Faithful Inversion of Generative Models for Effective Amortized Inference. In: Proceedings of the
32nd International Conference on Neural Information Processing Systems, NIPS’18, Curran Associates Inc.,
Red Hook, NY, USA, p. 3074–3084.

[71] Kai Xu, Akash Srivastava & Charles Sutton (2019): Variational Russian Roulette for Deep Bayesian Non-
parametrics. In Kamalika Chaudhuri & Ruslan Salakhutdinov, editors: Proceedings of the 36th International
Conference on Machine Learning, Proceedings of Machine Learning Research 97, PMLR, pp. 6963–6972.
Available at https://proceedings.mlr.press/v97/xu19e.html.

[72] Shengjia Zhao, Jiaming Song & Stefano Ermon (2017): Learning Hierarchical Features from Generative
Models. In: International Conference on Machine Learning. Available at http://arxiv.org/abs/1702.
08396.

[73] Yanli Zhou & Brenden M. Lake (2021): Flexible Compositional Learning of Structured Visual Concepts. In:
Proceedings of the 43rd Annual Conference of the Cognitive Science Society. Available at http://arxiv.
org/abs/2105.09848.

20

A Probabilistic Generative Model of Free Categories

A Estimation of log-likelihood by importance weighting

In approximate inference techniques based on importance weighting, we sample latent variables from a
proposal density qφ and then score them with an importance weight

wθ ,φ =

pθ (x, z, f , β ,W ; Φ, G)

qφ (z, f , β ,W

x)

|

(10)

equal to the ratio of the proposal joint density over the latent variables and the generative joint density
over all variables. This weighting adjusts for the bias of the proposal density, relative to the normalized
generative joint density (that is, the Bayesian inverse or posterior distribution).

Lemma 5 (Importance weighting provides an estimator of the marginal density). Given the generative
and proposal joint distributions above, the expectation of the importance weights equals the marginal
density of the observation

Eqφ (z, f ,β ,W

x;Φ)
|

(cid:2)wθ ,φ

(cid:3) = pθ (x; Φ, G).

(11)

Finite samples approximating this expectation therefore provide Monte Carlo estimators of the analyti-
cally intractable marginal density:

pθ (x; Φ, G)

1
K

K
∑
k=1

≈

θ ,φ for wk
wk

θ ,φ ∼

qφ (z, f , β ,W

x; Φ).

|

Proof. We begin with the deﬁnition of the expected importance weight

Eqφ (z, f ,β ,W

x;Φ)
|

(cid:90)

(cid:2)wθ ,φ

(cid:3) =

dz, f , β ,W qφ (z, f , β ,W

x; Φ) wθ ,φ ,

|

and expand it to include the density ratio

(cid:90)

(cid:90)

=

=

(cid:90)

dz, f , β ,W qφ (z, f , β ,W

dz, f , β ,W (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)

qφ (z, f , β ,W

|

x; Φ)

(cid:20) pθ (x, z, f , β ,W ; Φ, G)
qφ (z, f , β ,W

x)
x; Φ)pθ (x, z, f , β ,W ; Φ, G)
(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
|
x)
qφ (z, f , β ,W

|

,

(cid:21)

,

|

dz, f , β ,W pθ (x, z, f , β ,W ; Φ, G) ,

=
(cid:3) = pθ (x; Φ, G).

Eqφ (z, f ,β ,W

x;Φ)
|

(cid:2)wθ ,φ

This quantity can be estimated by Monte Carlo techniques, obtaining the estimator

Zθ ,φ ≈

1
K

K
∑
k=1

wθ ,φ

ˆZθ ,φ =

1
K

K
∑
k=1

wθ ,φ .

Resampling proportionally to these weights can then provide us with an unbiased sample from the true
posterior distribution [8]. Storing the actual weights in log-space typically provides better numerical
stability, when coupled to a specialized mean-exp implementation. In our case, PyTorch provides such
an implementation.

E. Sennesh, T. Xu, & Y. Maruyama

21

Taking the logarithm of both sides of Equation 11 yields an expression known as the log-evidence

for the model

log Eqφ (z, f ,β ,W

x;Φ)
|

(cid:2)wθ ,φ

(cid:3) = log pθ (x; Φ, G),

and applying Jensen’s Inequality yields a variational lower bound to the log-evidence

Eqφ (z, f ,β ,W

x;Φ)
|

(cid:2)log wθ ,φ

(cid:3)

≤

log pθ (x; Φ, G).

We call the left-hand side of this inequality the Evidence Lower Bound (ELBO)

L (θ , φ ) = Eqφ (z, f ,β ,W

(cid:2)log wθ ,φ

(cid:3)

≤

x;Φ)
|

log pθ (x; Φ, G).

(12)

Appendix B will provide an alternate derivation for the ELBO, showing that maximizing the ELBO
minimizes the (exclusive) Kullback Lieblier divergence from the proposal to the posterior distribution.

B Derivation of the ELBO objective

Any proposal like Equation 9 can help sample from the posterior distribution by importance weighting,
but we can obtain progressively better approximations by minimizing the Kullback Liebler divergence
(see [47] for details) from the proposal to the true posterior

KL(qφ (z, f , β ,W

x)

||

|

pθ (z, f , β ,W

|

x; Φ, G)) = Eqφ (z, f ,β ,W

(cid:20)

log

x)
|

Equation 8 shows the true posterior to be

qφ (z, f , β ,W

pθ (z, f , β ,W

|

x)
x; Φ, G)

|

(cid:21)

.

pθ (z, f , β ,W

x; Φ, G) =

|

pθ (x, z, f , β ,W ; Φ, G)
pθ (x; Φ, G)

,

with the numerator being the joint distribution deﬁned in Equation 7 above. Substituting the right-hand
side of this equation into the deﬁnition of the divergence above

x)pθ (x; Φ, G)

|
pθ (x, z, f , β ,W ; Φ, G)

(cid:21)

,

KL(qφ

||

pθ ) = Eqφ (z, f ,β ,W

= Eqφ (z, f ,β ,W
= Eqφ (z, f ,β ,W

= Eqφ (z, f ,β ,W

x)
|

x)
|

x)
|

x)
|

(cid:20)

qφ (z, f , β ,W

log
(cid:2)log qφ (z, f , β ,W
(cid:2)log qφ (z, f , β ,W
(cid:20)

log

|

x) + log pθ (x; Φ, G)
x)
qφ (z, f , β ,W

log pθ (x, z, f , β ,W ; Φ, G)(cid:3) ,
log pθ (x, z, f , β ,W ; Φ, G)(cid:3) + log pθ (x; Φ, G),
−
x)

−

(cid:21)

|

|

+ log pθ (x; Φ, G),

pθ (x, z, f , β ,W ; Φ, G)

= log pθ (x; Φ, G)

Eqφ (z, f ,β ,W

x)
|

−

log

(cid:20)

pθ (x, z, f , β ,W ; Φ, G)

qφ (z, f , β ,W

x)

|

(cid:21)

shows that the divergence decomposes into two components: the log model evidence and the negative
expected log importance weight. The sum of the expected log weight and the divergence is the log model
evidence

KL(qφ

||

pθ ) + Eqφ (z, f ,β ,W

(cid:20)

log

x)
|

pθ (x, z, f , β ,W ; Φ, G)

(cid:21)

qφ (z, f , β ,W

x)

|

= log pθ (x; Φ, G),

22

A Probabilistic Generative Model of Free Categories

so that the expected log weight itself

Eqφ (z, f ,β ,W

x)
|

(cid:20)

log

pθ (x, z, f , β ,W ; Φ, G)

(cid:21)

qφ (z, f , β ,W

x)

|

= log pθ (x; Φ, G)

KL(qφ

pθ ),

||

−

equals the log evidence minus the divergence. Since the divergence itself is always non-negative

Eqφ (z, f ,β ,W

x)
|

log

log pθ (x; Φ, G)
(cid:20)

−
pθ (x, z, f , β ,W ; Φ, G)

||

KL(qφ

pθ )
(cid:21)

qφ (z, f , β ,W

x)

|

log pθ (x; Φ, G)

log pθ (x; Φ, G),

≤

≤

the expected log-weight therefore provides a lower bound to the log-evidence. For this reason, we typi-
cally call it the Evidence Lower Bound (ELBO)

L (θ , φ ) = Eqφ

(cid:20)

log

pθ (x, z, f , β ,W ; Φ, G)

qφ (z, f , β ,W

x)

|

(cid:21)

,

and take it as an objective function to perform variational Bayesian inference [3]. The derivation above
implies that maximizing L will minimize the divergence between the tractable proposal density and the
intractable true posterior, with L equalling the true model-evidence only if the divergence reaches zero.

C Training and evaluation details

We parameterized the free category Free(Q) by generating input and output pairs from dimensionali-
ties ranging from 4 to 196, picking powers of 2, and constructing the following build blocks for deep
generative models:

1. Fully-connected VAE decoder networks. Decoders mapping down to R196 were considered to de-
code image glimpses, and therefore used transposed convolutional layers to produce their outputs;

2. Variational “Ladder” decoder and prior networks from Zhao [72]’s work on learning hierarchical

features, with a noise dimension of 2; and

3. Spatial attention mechanisms from Rezende [56] which map R14
×

14

transformation code z

∈

R3 from a learned Gaussian prior.

R28

28, sampling a spatial
×

→

Attaching the likelihood to the generator morphism required sampling the free category prior via the
two-part wiring diagram

fθ : QBS(I, P (R
P (cid:96)σ : QBS(I, P (R
(cid:35)

28

28

×

28

28

×

fθ

τ))

28

(x : R

28))).
×

×

×

(cid:96)σ : QBS(R

28

28, P (R
×

28

28

×

(x : R

28

28)))
×

×

L and its gradients were approximated by Monte Carlo sampling; Pyro [2]’s TraceGraphELBO class
provided gradient estimators [45, 60]. We trained this model for 1200 epochs, with a learning rate
3. At test time we substitute a Bernoulli likelihood for the Gaussian, to “compare like to like”
η = 10−
with other models in the literature.

E. Sennesh, T. Xu, & Y. Maruyama

23

D Dataset history and selection of competing models

Lake [38, 39] and colleagues proposed the Omniglot dataset to challenge the machine learning com-
munity to achieve human-like concept learning by learning a single generative model from very few
examples; the Omniglot challenge requires that a model be usable for classiﬁcation, latent feature recog-
nition, concept generation from a type, and exemplar generation of a concept. The deep generative
models research community has focused on producing models capable of few-shot reconstruction of un-
seen characters. Rezende [56] and Hewitt [33] ﬁxed as constant the model architecture, attempting to
account for the compositional structure in the data with static dimensionality. In contrast, He [30] and
Feinman [15] jointly learned the model structure alongside inferring the posterior distribution over the
latent variables and reconstructing the data.

