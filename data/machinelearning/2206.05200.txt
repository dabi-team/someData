2
2
0
2

n
u
J

0
1

]
L
M

.
t
a
t
s
[

1
v
0
0
2
5
0
.
6
0
2
2
:
v
i
X
r
a

Dynamic mean ﬁeld programming

George Stamatescu
School of Computer Science
University of Adelaide
Adelaide, SA 5001
george.stamatescu@adelaide.edu.au

Abstract

A dynamic mean ﬁeld theory is developed for model based Bayesian reinforcement
learning in the large state space limit. In an analogy with the statistical physics of
disordered systems, the transition probabilities are interpreted as couplings, and
value functions as deterministic spins, and thus the sampled transition probabilities
are considered to be quenched random variables. The results reveal that, under
standard assumptions, the posterior over Q-values is asymptotically independent
and Gaussian across state-action pairs, for inﬁnite horizon problems. The ﬁnite
horizon case exhibits the same behaviour for all state-actions pairs at each time
but has an additional correlation across time, for each state-action pair. The results
also hold for policy evaluation. The Gaussian statistics can be computed from a set
of coupled mean ﬁeld equations derived from the Bellman equation, which we call
dynamic mean ﬁeld programming (DMFP). For Q-value iteration, approximate
equations are obtained by appealing to extreme value theory, and closed form
expressions are found in the independent and identically distributed case. The
Lyapunov stability of these closed form equations is studied.

1

Introduction

An agent interacting with a Markov decision process (MDP) with unknown parameters faces the
problem of both estimating these parameters and determining a control strategy. The question is
how to use samples efﬁciently to balance the two goals. Here, we assume both the rewards and
transition probabilities are unknown to the agent. Two popular approaches to sample efﬁcient learning
in unknown MDP environments are based on Bayesian and frequentist approaches to estimation in
so-called bandit problems: Thompson or posterior sampling and “optimism in the face of uncertainty"
Osband and Van Roy [2017], Lattimore and Szepesvári [2020]. Both approaches continually update
the estimates or beliefs for the MDP parameters, typically at the start of episodes of interaction.

In this paper we restrict our attention to the Bayesian case and consider in particular the discounted
inﬁnite horizon problem, though we argue that our results hold for all horizons. Under posterior
sampling for reinforcement learning (PSRL) Osband and Van Roy [2017], an agent updates the
posterior over parameters at the end of each interaction or episode, and a sample is drawn from this
distribution. From such samples, the contraction property of Bellman’s optimality equation allows
the estimation of a ﬁxed point for the Q-value via iteration of this equation. A policy can then be
derived by acting greedily with respect to the Q-values, and thus the agent acts as if the sampled
MDP is reality. Remarkably, this approach balances exploration and exploitation, attaining sub-linear
Bayesian regret Osband and Van Roy [2017].

From the Bayesian perspective the optimal Q-values are random variables, by virtue of the rewards
and transition probabilities being random variables. It is often written that since the optimal Q-values
depend on these parameters and each other in a complex non-linear manner via the Bellman equation,
this then makes exact calculation of the posterior over Q-values impossible. The results in this paper

Preprint. Under review.

 
 
 
 
 
 
reveal that the situation is not so dire, provided we study systems with large state spaces. Under this
setting we obtain, via methods from statistical physics, results akin to the central limit theorem.

1.1 Posterior sampling for RL as a disordered dynamical system

In the language of statistical physics, we interpret value iteration for a sampled MDP as a deterministic
dynamical system subject to quenched disorder Crisanti and Sompolinsky [2018]. The disorder
in this context arises from the sampled transition probabilities and rewards, ˜P and ˜r, which in
physicists’ language are termed quenched random variables. This interpretation can be understood
from an analogy with the spin glass literature of statistical physics Castellani and Cavagna [2005],
or equivalently random recurrent neural networks Advani et al. [2013]. In the spin glass language,
the transition probabilities represent couplings and the Q-value functions represent the local ﬁelds
for deterministic spins. In neural networks these are weights and activation inputs for neurons,
respectively.

Inspired by this analogy we study Q-value iteration under Bayesian model uncertainty, in the large
state space limit, using a dynamic mean ﬁeld theory. The fundamental theoretical result of the paper
is that asymptotically the posterior distribution over optimal Q-values is factorises over state-action
pairs and, under some standard assumptions, is Gaussian. More speciﬁcally, in the inﬁnite horizon
setting we study the posterior over the iterates of Q-value iteration and derive dynamic mean ﬁeld
equations which describe a Gaussian process over iterates where the Q-values are independent across
state-action pairs but each have a correlation across iterates. The ﬁxed point of the equations produce
the statistics for the posterior over the optimal Q-values. The ﬁnite horizon case is identical to the
theory for Q-value iterates over a ﬁnite number of iterations. We refer to the computation of these
statistics as dynamic mean ﬁeld programming (DMFP).

We are able to systematically derive this result by developing a dynamic mean ﬁeld theory in the spirit
of Sompolinsky et al. [1988], originally derived for random recurrent neural networks, despite several
differences in the reinforcement learning case. The basis of the theory is the generating functional
or path integral formalism for disordered systems. A key assumption is that a moment generating
function deﬁned for Q-value iterates is a “self-averaging" quantity, meaning that it concentrates in
measure on its average. Under this assumption, from the disorder averaged generating function one
can calculate statistics for a typical realisation of the system. Leveraging a central limit theorem
for linear combinations of Dirichlet random vectors Matsunawa [1985], we are able to average a
generating functional for the Q-value iterates. We then derive a saddle point approximation and thus
establish the asymptotic independence and Gaussian behaviour of the Q-values.

In order to compute the DMFP solution we derive the approximate equations using extreme value the-
ory, whose accuracy improves with increasing action space size. In the i.i.d case, which corresponds
to having identical priors at the start of learning, we ﬁnd closed form expressions for the mean ﬁeld
equations. We study the stability of the closed form approximations in the inﬁnite horizon setting. We
ﬁnd the maximum Lyapunov exponent is the logarithm of the discount factor, implying the system is
stable, which one might expect from the contraction property of the underlying Bellman equations.
In the general case, during learning, we resort to numerical approximations based on extreme value
theory. We provide simulation evidence in support of the results, conﬁrming the Gaussian process
behaviour and the accuracy of the approximate mean ﬁeld equations. We note that our results also
hold for policy evaluation in the Bayesian setting.

1.2 Signiﬁcance and related work

We anticipate that the approach and theoretical results presented here will stimulate new research
into reinforcement learning with large state spaces. To the best of our knowledge, this asymptotic
limit and its implications for the posterior over Q-value functions has not been studied. Arguably, it
is problems with large state spaces which pose the greatest challenge.

A recent line of work from O’Donoghue has applied an epistemic risk-seeking approach to develop
Bayesian reinforcement learning with regret guarantees in ﬁnite horizon problems O’Donoghue
[2018] O’Donoghue et al. [2020]. This approach is motivated by the intractability for both computing
posteriors for value functions and in sampling posteriors over MDP model parameters. The work
essentially bounds an object related to the cumulant generating function of the posterior over Q-values.
The bounds are restricted to the ﬁnite horizon case, or the case where the MDP is a directed acyclic

2

graph. In contrast, the mean ﬁeld result is exact in the large system limit and applies to general
inﬁnite horizon problems, where cycles may be present. Indeed, it is cycles which produce statistical
dependencies between Q-values which we show vanish in the large system limit. Our theory may
also be relevant to risk sensitive approaches since it reveals that the cumulant generating function for
Q-values is that of a Gaussian, and we also provide methods for calculating its moments. Earlier work
has attempted to approximate the posterior variance for Q-values using bounds O’Donoghue et al.
[2018]. Similarly, Osband and Van Roy [2017] use a bound on the posterior variance to heuristically
propose a Gaussian approximation for posterior sampling. In all cases the bounds may be loose.

More generally, we hope the current paper stimulates new interactions between statistical physics and
reinforcement learning. To date, there has been little contact. This hope is founded on successful
interaction between statistical physics and other research ﬁelds. Previously, dynamic mean ﬁeld
theory Sompolinsky et al. [1988] has been applied to theoretical neuroscience, and more recently was
used to develop initialisation theory for deep neural networks Poole et al. [2016], Schoenholz et al..

1.3 Outline

The paper proceeds as follows. We introduce our notation for Bayesian reinforcement learning in
section 2. In section 3 we present the basic analogy between random neural networks and Bayesian
reinforcement learning. We also provide a brief overview of mean ﬁeld theory as encountered in
machine learning, placing dynamic mean ﬁeld theory in the broader gamut. In section 4 we present
the dynamic mean ﬁeld programming (DMFP) equations. We describe a naive but intuitive derivation
for the dynamic mean ﬁeld equations, and then derive approximate computations based on extreme
value theory. We present chieﬂy the discounted inﬁnite horizon case, since the ﬁnite horizon and other
settings follow the same form. In the i.i.d case we establish the Lyapunov stability of the approximate
DMFP equations. Finally, in section 5 we develop the dynamic mean ﬁeld theory for RL, which
provides a systematic derivation of the DMFP equations. We discuss the results in section 7.

2 Bayesian reinforcement learning

We introduce our notation here for Markov decision processes (MDPs) and Bayesian reinforcement
learning in the discounted inﬁnite horizon setting. The ﬁnite horizon theory will be detailed in the
appendix. We consider an inﬁnite horizon, discounted MDP M = (S, A, P, r, β), to be speciﬁed
by a state space S, an action space A, and we denote the size of |S| = N ; a transition function
P : S × A → ∆(S), where ∆(S) the probability simplex and we say Ps(cid:48)|sa is the probability of
transitioning into state s(cid:48) upon taking action a in state s; a reward function r : S × A → R, denoted
rsa and assumed to be bounded and random with mean ρsa and ﬁnite variance, and a discount factor
β ∈ [0, 1), which deﬁnes an effective horizon for the problem.

A policy speciﬁes a decision-making strategy. We restrict attention to the set of stationary policies
π : S → ∆(A). For a ﬁxed policy and ﬁxed starting state s0 = s, we deﬁne the Q-value function
Qπ : S × A → R as the discounted sum of future rewards

Qπ(s, a) = E[

∞
(cid:88)

t=1

βtrstat|π, s0 = s, a0 = a]

(2.1)

where expectation is with respect to the randomness of the trajectory, that is, the randomness in
state transitions and π. Since rsa is bounded, so are the Q-values. The solutions to the Bellman
optimality equation are known to give the optimal Q-value function. In the case the parameters are
known, we may use dynamic or linear programming methods to compute solutions. In the case
that the MDP parameters are unknown an agent must balance exploration and exploitation. In the
Bayesian approach it is common to model the transition probabilities as Dirichlet random vectors
which are independent across state-action pairs, often referred to as the ﬂat Dirichlet prior assumption,
Psa ∼ Dirichlet(αsa). with αsa a vector of strictly positive real numbers, which here encapsulates
the posterior statistics for the transition probabilities. This assumption, as we will discuss, is key
for the mean ﬁeld theory hold. We consider here that the mean rewards ρsa are either known or,
if unknown, have Gaussian posteriors with mean and variance denoted µρsa and σ2
ρsa respectively,
as in Osband and Van Roy [2017]. This corresponds, for example, to having Gaussian “noise" on
otherwise deterministic rewards. A common objective in Bayesian reinforcement learning is to design
algorithms with low (ie. sub-linear) Bayesian regret.

3

3 An analogy between reinforcement learning and random neural networks

The analogy we work from is most clear when presenting side by side the equations for Q-value
iteration for the discounted inﬁnite horizon problem and random recurrent neural networks. Denote
xi(t) as neurons at time t, θi as thresholds, weights (or couplings) Jij are here assumed to be
Gaussian and i.i.d and we denote φ(·) as a non-linearity, for example tanh(·). The equations are,

Q-Value Iteration:

Qn+1

sa = ρsa + β

Random Recurrent Neural Network:

xi(t + 1) = θi +

Ps(cid:48)|s,a max

a(cid:48)

Qn

s(cid:48)a(cid:48)

Jijφ(xj(t))

(cid:88)

s(cid:48)
(cid:88)

j

(3.1)

(3.2)

By inspection, state-action pairs (sa) correspond to the site indices i in the neural network case.
Thus each transition probability “couples" a Q-value to a set of other Q-values. The non-linearity
φ(·) is typically a smooth function or at least piece-wise linear, where as the maximum function is
neither. Couplings for neural networks are often taken as i.i.d and Gaussian, whereas the transition
probabilities are drawn from Dirichlet distribution, and exhibit negative dependence. In order to obtain
interesting limits, the mean and variance of the Jij are set to scale as ¯J/N and σ2
J /N respectively. In
the MDP case, the couplings scale naturally as they sum to one. Remarkably, despite these differences,
we are able to develop a dynamic mean ﬁeld theory for Bayesian reinforcement learning.

3.1 Static and dynamic mean ﬁeld theory

Mean ﬁeld theory and statistical physics have been commonly encountered in machine learning
within the context of variational Bayesian inference Wainwright et al. [2008]. The idea is that one
attempts to approximate an intractable posterior distribution using a simpler one whose parameters
may be computed using optimisation algorithms. This approach is borrowed from statistical physics,
which can be recognised when interpreting the posterior of interest as a Boltzmann-Gibbs distribution
for a system with a given Hamiltonian, an energy function which deﬁnes the true interactions between
the variables of interest. The Boltzmann-Gibbs distribution describes a physical system in thermal
equilibrium. This is often referred to as a “static" theory, which is not to say that such systems have
no dynamics, but that equilibrium is the long-time limit to which appropriately deﬁned dynamics
relax towards. Dynamic theories study systems out of equilibrium, which in general may never relax
towards a steady state.

Large deviations theory underlies any mean ﬁeld theory Touchette [2009]. The principles essentially
state that whilst the variables of interest may be dependent on each other, they are weakly dependent
enough that as the system gets large we can ignore most interactions. In the static case, we say that
there exists an effective Hamiltonian, with a simpliﬁed interaction “ﬁeld" between the variables, such
as each variable only depending on the mean of other variables. Disordered systems, such as spin
glasses, are complex varieties of systems where the interactions between the spins are via random or
quenched couplings.

A new point of contact between machine learning and a dynamic mean ﬁeld theory of disordered
systems recently occurred in a series of papers which has helped establish initialisation theory for
deep learning Poole et al. [2016] Schoenholz et al.. Originally developed in a series of papers from
Sompolinsky and collaboratorsSompolinsky and Zippelius [1982] Sompolinsky et al. [1988], this
theory applied to systems where the couplings between ﬂuctuating or deterministic spins are random
and asymmetric (or generally independent). The asymmetry implies that equilibrium is not guaranteed
since Markov dynamics for the system do not satisfy detailed balance. Like static mean ﬁeld theory,
the idea in the dynamic case is to replace the local ﬁeld for variables in the dynamical system with an
effective ﬁeld with simpliﬁed interactions. The results show that neurons are effectively subject to a
Gaussian ﬁeld, whose statistics are given by a self-consistent, time-dependent mean ﬁeld equations.
We propose the same approach in the Bayesian reinforcement learning setting.

4 Dynamic mean ﬁeld programming

In this section we present the dynamic mean ﬁeld equations for Q-value iteration, which we call
dynamic mean ﬁeld programming (DMFP). In order to provide some intuition about the results, we

4

present the naive derivation based on independence assumptions that are derived via a generating
functional approach in section 5. We develop approximations using extreme value theory to compute
the solutions. Finally, we establish the Lyapunov stability of the equations in the i.i.d case, where
extreme value theory provides closed form approximations.

4.1 A naive derivation

The naive approach to deriving dynamic mean ﬁeld equations is based ﬁrst on an assumption of
independence across all variables within Q-value iteration, originally developed in the neural network
context Amari [1972]. More speciﬁcally, with ⊥ denoting statistical independence, the assumptions
are,

Qn

sa ⊥ ρs(cid:48)a(cid:48), Qn

sa ⊥ Ps(cid:48),a(cid:48), Qn

sa ⊥ Qn

s(cid:48)a(cid:48)

∀ (s, a), (s(cid:48), a(cid:48))

We can combine this assumption with a central limit theorem for linear combinations of Dirichlet
random vectors Matsunawa [1985] to deduce the asymptotic Gaussian result for the iterates of
Q-value iteration. From this result we can write down equations which propagate the moments at
each iteration. For the mean, this is straightforward

sa = Er,Ps,a,Qn(s,a)Qn+1(s, a) = µρsa + β
µn+1

(cid:88)

s(cid:48)

EPs,a Ps(cid:48)|s,aEQn(s(cid:48),a(cid:48)) max
a(cid:48)

Qn(s(cid:48), a(cid:48))

(4.1)

We denote the variance of Qn+1(s, a) as νn+1

sa

and obtain,

sa = σ2
νn+1

ρsa

+ β2 (cid:88)

s(cid:48),s(cid:48)(cid:48)

Cs(cid:48),s(cid:48)(cid:48)|saE(cid:2) max

a(cid:48)

Qn(s(cid:48), a(cid:48)) max
a(cid:48)(cid:48)

Qn(s(cid:48)(cid:48), a(cid:48)(cid:48))(cid:3)

(4.2)

where we used that the variance of dependent terms is the sum of the covariances, and denoted the
(cid:3). Note we use the CLT Matsunawa [1985]
Dirichlet covariance as Cs(cid:48),s(cid:48)(cid:48)|sa = Cov(cid:2)Ps(cid:48)|s,a, Ps(cid:48)(cid:48)|s,a
differently in section 5, as part of a more systematic derivation of the result.

Note that, if the rewards are stochastic and unknown, but do not have Gaussian posteriors, then the
Q-values will not be Gaussian but a mixture. However, the DMFP theory still holds. In particular,
the term (cid:80)
s(cid:48)a(cid:48) will still have Gaussian distribution, and hence only the ﬁrst two
moments will be propagated across iterations in the ﬁnite horizon setting, or time in the ﬁnite.

s(cid:48) Ps(cid:48)|s,a maxa(cid:48) Qn

4.2 Extreme value theory for approximate statistics

In order to compute the solutions to DMFP one needs to calculate the ﬁrst and second moments of
the maxima over |A| non-identical Gaussian Qn
sa. Here we appeal to extreme value theory, which
describes the distribution of the maximum over asymptotically many variables (appropriately re-
scaled). This theory was originally developed for i.i.d variables, but there exist generalisations to
non-independent cases Falk et al. [2010], Hüsler [1994]. This work establishes that the distribution
approaches a Poisson limit, from which we may calculate moments. See the appendix for details.

In the case that all the parameters of the system are identically distributed for all state-action pairs,
the system is described by one set of coupled scalar equations for the mean and variance of the value
function. In this case we are able to obtain closed form approximate mean ﬁeld equations Fisher and
Tippett [1928]. We assume here that the Dirichlet parameters are uniform and set to αsa(s(cid:48)) = 1
N .
This produces simpliﬁcations in the DMFP equations, as we detail in appendix A. Denoting the mean
µ(k) = E[Qsa(k)] and variance ν(k) = Var[Qsa(k)] for all (s, a) since they are identical, if we use
a Type-I extreme value or Gumbel distribution as an approximation, we have
µn+1 = µρ + β(cid:0)µn +

νn(b|A| + a|A|γEM)(cid:1)

(4.3)

√

νn+1 = σ2

ρ + β2 π2
12

νna|A|

(4.4)

where γEM is the Euler-Mascheroni constant, and a|A| and b|A| are constants dependent on the action
space size |A|, given in Fisher and Tippett [1928]. See the appendix for the complete derivation. The
solution can be found in closed form by solving ﬁrst eq. (4.4) then eq. (4.3). The approximation
can be improved by using a Type-III or Weibull distribution Fisher and Tippett [1928] or other
approximations Cohen [1982].

5

4.3 Stability and ﬁxed points of DMFP in the i.i.d case

The Bellman optimality equation is a contraction mapping for any instance or realisation of the MDPs
we consider. This is in contrast to the spin glass and random neural network cases, where we observe
chaos or limit cycles, outside of a stable regime. The contractive properties of the Bellman equation
do not appear to extend in a straightforward manner to the dynamic mean ﬁeld equations, which can
be viewed as an operator propagating moments. Furthermore, due to the maximum function, the
Lyapunov stability results for the mean ﬁeld equations developed in Sompolinsky et al. [1988] also
do not directly translate here. However, in the i.i.d case we ﬁnd from the closed form approximate
equations that the Jacobian explicitly.We ﬁnd the maximum eigenvalue of the Jacobian of the system
is the discount factor β. This means that the system is Lyapunov stable about its unique ﬁxed point,
since β ∈ (0, 1). We expect these results to hold in the ergodic and stochastic shortest path problems,
where the Bellman equation still has a modulus of contraction.

5 Dynamic mean ﬁeld theory for Bayesian RL

In this section we work through the generating functional approach, systematically deriving the
dynamic mean ﬁeld theory. We emphasise the key steps, and leaving a detailed account to the
appendix. We follow Helias and Dahmen [2019] and Crisanti and Sompolinsky [2018]. In order to
write down a moment generating function (MGF), we deﬁne a probability distribution over Q-value
iterates by artiﬁcially injecting i.i.d Gaussian noise at each state-action site and iteration. This noise
can be later removed. Second, we represent the empirical distribution over iterates with a Dirac
delta function, from which we then write down the moment generating function for a realisation of
the disorder. By ﬁrst switching to a Fourier representation of the Dirac delta we are then able to
take an average over the disorder in the system by appealing to the central limit theorem for linear
combinations of Dirichlet random vectors. Finally, we seek to apply a saddle-point approximation.
For this to be applied, we reveal a dependence on N (the state space size) in the exponent of the
generating function by carefully choosing auxiliary ﬁelds to decouple various interaction terms. The
saddle-point equations may be solved, which yield the standard Gaussian theory.

5.1 Probability distribution over Q-value iterates

Dynamic mean ﬁeld theory considers as a starting point the moment generating function (MGF) of
the distribution over trajectories of a system. Given that the system we study is deterministic, albeit
subject to “disorder" due to uncertain parameters, we consider a noise-perturbed system. We consider
the perturbation to be independent Gaussian noise W n
sa at each state action pair (s, a) and at each
iteration n. The noise term W n
sa is technically needed for a large deviations analysis (even with
arbitrarily small noise) Moynot and Samuelides [2002], but can be safely removed in the generating
functional approach later, and thus only serves to ﬁrst deﬁne a moment generating function Crisanti
and Sompolinsky [2018]. Thus we study the following system

Qn+1

sa = ρsa +

(cid:88)

s(cid:48)

Ps(cid:48)|saφ(Qn

s(cid:48):) + W n
sa

(5.1)

For shorthand we write the maximum as φ(Qn
from iteration n = 1 to n = M can be written as

s(cid:48):) = maxa(cid:48) Qn

s(cid:48)a(cid:48). The distribution over a trajectory

p(Q1:M ) =

(cid:90) M −1
(cid:89)

n=1

dW nρN (W n)δ(Qn+1

sa − ρsa −

(cid:88)

s(cid:48)

Ps(cid:48)|saφ(Qn

sa) − W n
sa)

(5.2)

where we have denoted the density over Gaussian noise terms W n
sa).
One could go on to write the probability distribution as a product over Gaussian densities, but in
order to average over the disorder, we introduce a so-called response ﬁeld ˜x. This ﬁeld enters by
(cid:82) i∞
eq. (5.2) by representing the Dirac delta by its Fourier integral δ(x) = 1
−i∞ d˜xe˜xx. We obtain
2πi
the discrete time analogue of the Martin-Siggia-Rose-De Dominicis-Janssen (MSRDJ) path integral

sa ρN (W n

sa as ρN (W n) = (cid:81)

6

formalism Castellani and Cavagna [2005], by replacing the Dirac delta with its Fourier form,

p(Q1:M ) =

(cid:90) M −1
(cid:89)

n=1

dW nρN (W n)

(cid:90) i∞

−i∞

d ˜Qn
2πi

exp (cid:2) ˜Qn

sa(Qn+1

sa − ρsa −

(cid:88)

s(cid:48)

Ps(cid:48)|saφ(Qn

sa) − W n

sa)(cid:3)

M −1
(cid:89)

(cid:89)

(cid:90) i∞

=

n=1

sa

−i∞

d ˜Qn
2πi

exp (cid:2) ˜Qn

sa(Qn+1

sa − ρsa −

(cid:88)

s(cid:48)

Ps(cid:48)|saφ(Qn

sa) − W n

sa)(cid:3)ZW (− ˜Qn
sa)

(5.3)

where ZW (− ˜Qn) is the moment generating function for the noise.

5.2 Moment generating function and the disorder average

We now consider the MGF for Q-value iterates by taking the expectation of terms exp((cid:80)M

n=1 jnQn),

Z[j] =

M
(cid:89)

(cid:26) (cid:90) ∞

n=1

−∞

dQn exp(jnQn)

(cid:27)

p(Q1:M )

(5.4)

where we write j = (j1, . . . , jM ). From the MGF we can of course study moments of the process as
δjn Z[j]|j=0 = (cid:104)Qn(cid:105). Higher moments can be obtained from higher derivatives.
functional derivatives
By including a small perturbation localised at an iteration −˜jn
sa, we can probe dependencies across
iterations, if we wish, by calculating the so called response function, see Helias and Dahmen [2019]
for details. So, using the notation (jn)T Qn = (cid:80)

δ

sa, we write

Z[j,˜j] =

M
(cid:89)

(cid:26) (cid:90) ∞

(cid:90) i∞

dQn

−∞

−i∞

d ˜Qn
2πi

ZW (− ˜Qm)

saQn
sa jn
(cid:27)

n=1
(cid:20) M −1
(cid:88)

× exp

n=1

sa

(cid:88)

˜Qn

sa(Qn

sa − ρsa −

Ps(cid:48)|saφ(Qn

(cid:21)
s(cid:48):) + (jn+1)T Qn+1 + (˜jn)T ˜Qn)

(5.5)

(cid:88)

s(cid:48)

Assuming the MGF is self-averaging Helias and Dahmen [2019], Crisanti and Sompolinsky [2018],
then from its average we can calculate the statistics of typical realisations of the system. So,
we now look to take the average over the disorder. The term involving ρsa in eq. (5.5) presents
no difﬁculty compared to the neural network case. However, the term involving the uncertain
transition probabilities demands consideration. Extracting the term in the moment generating function
depending on Ps(cid:48)|sa, we see that the calculation of interest is

(cid:90) (cid:89)

dP·|saDir(P·|sa; αsa) exp

−

(cid:18)

(cid:88)

s(cid:48)

Ps(cid:48)|sa

M −1
(cid:88)

n=1

(cid:19)

˜Qn

saφ(Qn

s(cid:48):)

(sa),(s(cid:48))
(cid:90)

(cid:89)

dSsaN ( ¯Ssa, σ2

Ssa

=

(sa)

) exp(−Ssa) =

exp( ¯Ssa + σ2

Ssa

)

(cid:89)

(sa)

(5.6)

where we have deﬁned Ssa = (cid:80)
˜Qn
s(cid:48):). Crucially, we have used that P (: |sa)
is independent across pairs (sa), and applied a CLT for linear combinations of Dirichlet vectors
Matsunawa [1985]. This is possible since Q and ˜Q terms are not random in eq. (5.5). The mean and
variance of Ssa are given by,

(cid:80)M −1
n=1

saφ(Qn

s(cid:48) Ps(cid:48)|sa

¯Ssa =

M −1
(cid:88)

(cid:88)

n=1

s(cid:48)

¯Ps(cid:48)|sa

˜Qn

saφ(Qn

s(cid:48):),

σ2
Ssa

=

(cid:88)

s(cid:48),s(cid:48)(cid:48)

Cs(cid:48),s(cid:48)(cid:48)|sa

M −1
(cid:88)

(cid:0)

n=1

˜Qn

saφ(Qn

s(cid:48):)(cid:1)(cid:0)

M −1
(cid:88)

m=1

˜Qm

saφ(Qm

s(cid:48)(cid:48):)(cid:1)

From here we can write the disorder averaged generating function for Q-value Iteration as

¯Z[j,˜j] =

M
(cid:89)

(cid:26) (cid:90) ∞

dQn

(cid:27) M −1
(cid:89)

(cid:26) (cid:90) i∞

n=1

−∞

m=1

−i∞

d ˜Qm
2πi

(cid:27)

ZW (−˜xm)

× exp (cid:2)

M −1
(cid:88)

(cid:88)

n=1

sa

˜Qn

saQn

sa − µρsa

˜Qn
sa

+

σ2
ρsa
2

( ˜Qn

sa)2 − ( ¯Ssa + σ2

Ssa

) + jT

n+1Qn+1 + ˜jT

n

˜Qn)(cid:3)

(5.7)

7

Finally we note that, for the next section, we make analytic progress if we have, in the i.i.d case,
uniform scaling αs(cid:48)|sa = α for each Ps(cid:48)|sa, and in the case α ∼ O( 1

N ), we have

1
N

˜Qn

saφ(Qn

s(cid:48):)

M −1
(cid:88)

(cid:88)

¯Ssa =

σ2
Ssa

=

n=1

s(cid:48)

1
2N

(cid:88)

s(cid:48)

M −1
(cid:88)

(cid:0)

n=1

˜Qn

saφ(Qn

s(cid:48):)(cid:1)2

(cid:88)

−

s(cid:48)(cid:54)=s(cid:48)(cid:48)

1
2N 2

M −1
(cid:88)

(cid:0)

n=1

˜Qn

saφ(Qn

s(cid:48):)(cid:1)(cid:0)

(5.8)

M −1
(cid:88)

m=1

˜Qm

saφ(Qm

s(cid:48)(cid:48):)(cid:1) (5.9)

The non-identical case can be written as a perturbation about the identical one, and provided these
are of smaller order in N , can be ignored asymptotically. We detail this argument in the appendix.

5.3 Saddle point approximation

We now seek a saddle-point approximation for the disorder averaged generating function, analogously
to the neural network case. Such an approximation is appropriate when the exponential term in
eq. (5.7) can be written in the form (cid:82) dQ exp(N f (Q)). Similar to the neural network case, it is
the variance term for the couplings which requires “decoupling" to reveal the large N dependence.
However, in contrast to the Gaussian weight case, we require three real auxiliary ﬁelds rather than
one to decouple the interacting Q ﬁelds Sompolinsky et al. [1988], Crisanti and Sompolinsky [2018].
The ﬁrst Θ1(n, m) is very similar to the neural network case, however we require two more real
ﬁelds in order to decouple the interactions,

Θ1(n, m) =

1
2N

(cid:88)

φ(Qn

s(cid:48):)φ(Qm

s(cid:48):) −

1
2N 2

(cid:88)

φ(Qn

s(cid:48):)

φ(Qm

s(cid:48)(cid:48):)

(cid:88)

s(cid:48)(cid:48)

Θ3(n) =

1
N

s(cid:48)
(cid:88)

s(cid:48)

φ(Qn

s(cid:48)(cid:48):),

Θ5(m) =

φ(Qm

s(cid:48)(cid:48):)

s(cid:48)
1
N

(cid:88)

s(cid:48)(cid:48)

(5.10)

(5.11)

The ﬁelds are inserted by use of a Dirac delta in each case, which produce corresponding complex
ﬁelds. Please see the appendix for the full derivation, where the saddle-point equations are solved,
which correspond to stationary point of the action (the term within the exponential). The solution
takes the same Gaussian form as in Sompolinsky et al. [1988], Crisanti and Sompolinsky [2018].

6 Simulations

We can test the predictions of the Gaussian theory and accuracy of the DMFP approximations
empirically as follows. We draw from given Dirichlet and mean reward distributions, as in Thomspon
sampling, and run Q-value iteration until convergence. The simulations presented in Figure 6 are
for a small system with only |S| = 50 and |A| = 30, with a uniform αsa = 1
N and rewards with
identical uniform priors. The ﬁgure on the left shows the empirical mean and variance of a particular
Q-value, and overlaid are the theoretical mean and variance, as given by eq. (4.3) and eq. (4.4). The
ﬁgure on the right is a Q-Q plot for the Q-value. As shown in the supplementary material, the state
space increases, we see the convergence to Gaussianity. We detail experiments in the non-i.i.d cases
as well. Note that there are two approximations in effect, the ﬁrst due to ﬁnite size effects in the state
space, the second due to the extreme value theory approximations of the DMFP equations.

7 Discussion

The work presented raises several interesting questions. Perhaps most pressing would be to ask what
the rate of convergence is to the Gaussian result. We expect this of course to be dependent on several
factors in the model. First, the state space size. Second, the discount factor, which controls the
“memory" between iterations. Third, the overall uncertainty within the system. If for instance the
Dirichlet vectors are known with partial certainty (ie. some Dirichlet parameters are large) then there
may be an effective state space size which is smaller.

Relatedly, an interesting property of the system in the mean ﬁeld limit is the behaviour of the variance.
The linear combination of Dirichlet random vectors has vanishing variance in the large state space
limit as long as the Dirichlet parameters scale at least as great as O( 1√
). Investigating this behaviour
N

8

Figure 6.1: DMFP simulations. Left: Mean and variance for a Q-value Q(s, a) of an MDP of size
|S| = 50, |A| = 20 with identical uniform Dirichlet priors for the transitions for each state action
pair, Gaussian priors for the rewards. Empirical mean and variance given by dots, theory by dashed
lines. Discount factor β = 0.9. Right: Q-Q plots for a Q-value, against normal quantiles.

during the learning process, where the Dirichlet parameters increase in integer quantities, and where
parts of the system are relatively unexplored is of great interest.

There are many other theoretical and algorithmic developments that should be explored. The
asymptotic independence and Gaussian result may be surprising to many in the ﬁeld. A rigorous
proof, based on large deviations analysis such as Ben Arous and Guionnet [1995] and Samuelides and
Cessac [2007] is of course needed. This would allow us to consider the case where the ﬂat Dirichlet
assumption does not hold, and the subsequent breakdown in self-averaging and thus the mean ﬁeld
theory.

In terms of the direct application of DMFP, the extreme value theory based approximations must be
improved, for instance to not require numerical computation based on the Poisson approximation, or
the restrictive i.i.d assumption. Furthermore, approximations for the case where rewards are unknown
but non-Gaussian are needed, for which the mean-ﬁeld posterior over Q-values is a mixture of a
Gaussian and the reward posterior.

Acknowledgements

I would like to acknowledge the support, insight and assistance offered by Federica Gerace and Carlo
Lucibello, Giel van Bergen, Kelli Francis-Staite, Blake O’Donnelly, Sean McGowan and Ian Fuss, as
well as the ongoing material and technical guidance of Hung Nguyen and Langford White.

References

M. Advani, S. Lahiri, and S. Ganguli. Statistical mechanics of complex neural systems and high
dimensional data. Journal of Statistical Mechanics: Theory and Experiment, 2013(03):P03014,
2013.

S.-I. Amari. Characteristics of random nets of analog neuron-like elements. IEEE Transactions on

systems, man, and cybernetics, (5):643–657, 1972.

G. Ben Arous and A. Guionnet. Large deviations for langevin spin glass dynamics. Probability

Theory and Related Fields, 102(4):455–509, 1995.

T. Castellani and A. Cavagna. Spin-glass theory for pedestrians. Journal of Statistical Mechanics:
Theory and Experiment, 2005(05):P05012, may 2005. doi: 10.1088/1742-5468/2005/05/p05012.
URL https://doi.org/10.1088/1742-5468/2005/05/p05012.

J. P. Cohen. The penultimate form of approximation to normal extremes. Advances in Applied
Probability, 14(2):324–339, 1982. ISSN 00018678. URL http://www.jstor.org/stable/
1426524.

9

05101520253002468101214Empirical meansTheoretical meansEmpirical varianceTheoretical variance-3-2-10123Standard Normal Quantiles-3-2-10123Quantiles of Input SampleA. Crisanti and H. Sompolinsky. Path integral approach to random neural networks. Physical Review

E, 98(6):062120, 2018.

M. Falk, J. Hüsler, and R.-D. Reiss. Laws of small numbers: extremes and rare events. Springer

Science & Business Media, 2010.

R. A. Fisher and L. H. C. Tippett. Limiting forms of the frequency distribution of the largest or
smallest member of a sample. Mathematical Proceedings of the Cambridge Philosophical Society,
24(2):180–190, 1928. doi: 10.1017/S0305004100015681.

M. Helias and D. Dahmen. Statistical ﬁeld theory for neural networks, 2019.

J. Hüsler. Extremes: Limit results for univariate and multivariate nonstationary sequences. In Extreme

Value Theory and Applications, pages 283–304. Springer, 1994.

T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2020.

T. Matsunawa. The exact and approximate distributions of linear combinations of selected order
statistics from a uniform distribution. Annals of the Institute of Statistical Mathematics, 37(1):
1–16, 1985.

O. Moynot and M. Samuelides. Large deviations and mean-ﬁeld theory for asymmetric random

recurrent neural networks. Probability Theory and Related Fields, 123(1):41–75, 2002.

B. O’Donoghue. Variational bayesian reinforcement learning with regret bounds. CoRR,

abs/1807.09647, 2018. URL http://arxiv.org/abs/1807.09647.

B. O’Donoghue, I. Osband, R. Munos, and V. Mnih. The uncertainty Bellman equation and ex-
ploration. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3839–3848.
PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/odonoghue18a.html.

B. O’Donoghue, I. Osband, and C. Ionescu. Making sense of reinforcement learning and probabilistic
inference. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=S1xitgHtvS.

I. Osband and B. Van Roy. Why is posterior sampling better than optimism for reinforcement
learning? In Proceedings of the 34th International Conference on Machine Learning - Volume 70,
ICML’17, page 2701–2710. JMLR.org, 2017.

B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep
neural networks through transient chaos. Advances in neural information processing systems, 29:
3360–3368, 2016.

M. Samuelides and B. Cessac. Random recurrent neural networks dynamics. The European Physical

Journal Special Topics, 142(1):89–122, 2007.

S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 2017.

URL https://openreview. net/pdf.

H. Sompolinsky and A. Zippelius. Relaxational dynamics of the edwards-anderson model and the

mean-ﬁeld theory of spin-glasses. Physical Review B, 25(11):6860, 1982.

H. Sompolinsky, A. Crisanti, and H.-J. Sommers. Chaos in random neural networks. Physical review

letters, 61(3):259, 1988.

H. Touchette. The large deviation approach to statistical mechanics. Physics Reports, 478(1-3):1–69,
Jul 2009. ISSN 0370-1573. doi: 10.1016/j.physrep.2009.05.002. URL http://dx.doi.org/10.
1016/j.physrep.2009.05.002.

M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational

inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.

10

A Derivations of DMFP equations and extreme value theory approximations

A.1 DMFP derivation in the identical case

To review, here we derive the mean ﬁeld equations based on assumptions on the scale of parameters
within the transition function, a local chaos hypothesis, and a central limit argument for our transition
function. The result of the assumptions is that a central limit type argument can be applied to certain
terms in the Q-value iteration equations. This allows analytical expressions for the mean and variance
of the Q-values to be found for the large system limit.

Recall the transition probabilities given by P in the MDP are considered to be Dirichlet random
variables which are negatively dependent. The Dirichlet distribution is a multivariate generalisation
of the beta distribution and is characterised by parameters α1, ..., α|S| > 0. The formulae for the
mean, variance and covariance for Dirichlet random variables are,

E[pi] =

αi
t αt

(cid:80)

Var[pi] =

(cid:80)

(1 − αi

αi
t αt
1 + (cid:80)

(cid:80)
t αt

t αt

)

Cov[pi, pj] =

− αi
(cid:80)
t αt
1 + (cid:80)

αj
(cid:80)
t αt
t αt

.

(A.1)

Assumption 1: Identical Dirichlet distribution parameters
Let us now consider how the parameters αi scale with the size of the state space N . It is convenient
to assume then that the αi are all identical and in this case the distribution is known as the symmetric
Dirichlet distribution. Thus let αi = α, ∀i. Then,

E[pi] =

1
N

Var[pi] =

N − 1
N 2(N α + 1)

Cov[pi, pj] = −

1
N 2(N α + 1)

.

(A.2)

Therefore, assuming that the transition functions are independent across pairs (s, a), and assuming
(cid:1) we can derive closed form
we have uniform scaling αs(cid:48)|s,a = α for each Ps(cid:48)|s,a and that α ∼ O (cid:0) 1
approximate mean ﬁeld equations.

N

Assumption 2: Local chaos hypothesis
To derive our mean ﬁeld equations we will also assume the independences,

Qk(s, a) ⊥ r(s(cid:48), a(cid:48)) ∀(s, a), (s(cid:48), a(cid:48))
r(s, a) ⊥ Ps(cid:48),a(cid:48) ∀(s, a), (s(cid:48), a(cid:48))
Qk(s, a) ⊥ Ps(cid:48),a(cid:48) ∀(s, a), (s(cid:48), a(cid:48))
Qk(s, a) ⊥ Qk(s(cid:48), a(cid:48)) ∀(s, a), (s(cid:48), a(cid:48)).

(A.3)

(A.4)

(A.5)

(A.6)

This assumption states that, when our state space is large, the system behaves as if the variables Q
were independent of each other, the reward values, and transition values.
Expressions for the mean and variance of Qk+1 can then be found.
We denote the mean of Qk+1(s, a) as

EP (r),PPs,a ,P (Qk(s,a))Qk+1(s, a)

= µρ + β

(cid:88)

s(cid:48)

(cid:20)
Ps(cid:48)|s,a

(cid:21)
EQk(s(cid:48),a(cid:48))

(cid:20)

EPs,a

(cid:21)

Qk(s(cid:48), a(cid:48))

max
a(cid:48)

(A.7)

where Ps,a is a Dirichlet distribution with parameter αs,a. We denote the variance of Qk+1(s, a) as

VarP (r),PPs,a ,P (Qk(s,a))

= σ2

ρ + β2VarDir(Ps,a),Qk(s,a)

Ps(cid:48)|s,a max

a(cid:48)

(cid:21)
Qk(s(cid:48), a(cid:48))
.

(A.8)

(cid:2)Qk+1(s, a)(cid:3)
(cid:20) (cid:88)

s(cid:48)

11

Considering the second term, we have
(cid:20) (cid:88)

VarDir(Ps,a),Qk(s,a)

s(cid:48)

Ps(cid:48)|s,a max

a(cid:48)

Qk(s(cid:48), a(cid:48))

(cid:21)

N
(cid:88)

N
(cid:88)

(cid:20)

Cov

Ps(cid:48)|s,a max

a(cid:48)

Qk(s(cid:48), a(cid:48)), Ps(cid:48)(cid:48)|s,a max
a(cid:48)(cid:48)

(cid:21)
Qk(s(cid:48)(cid:48), a(cid:48)(cid:48))

max
a(cid:48)

Qk(s(cid:48), a(cid:48)) max
a(cid:48)(cid:48)

(cid:21)
Qk(s(cid:48)(cid:48), a(cid:48)(cid:48))

Cov

(cid:20)
Ps(cid:48)|s,a, Ps(cid:48)(cid:48)|s,a

=

=

s(cid:48)=1

s(cid:48)(cid:48)=1

N
(cid:88)

N
(cid:88)

(cid:20)

E

s(cid:48)=1

s(cid:48)(cid:48)=1

(A.9)

(A.10)

(cid:21)
.

Assumption 3: Central limit type argument of Dirichlet variables
To derive closed form expressions from the moments of Qk+1, we must assume a central limit
type argument for the transition probabilities given by P . We assume that as the size of the state
space |S| increases, we apply a central limit theorem (CLT) for linear combinations of Dirichlet
random variables Matsunawa [1985], which establishes the linear combination approaches a normal
distribution. This extends in a straightforward manner to the case where the weights in the linear
combination are themselves random variables. These weights correspond to the value functions in
the Bellman optimality equation.

Due to the discontinuous and nonlinear function max(·) in the value iteration schemes, the distri-
butions of maxima of Q-values must be studied through extreme value statistics Fisher and Tippett
[1928], Cohen [1982]. This allows analytical expressions for the mean and variance of the Q-values
to be derived. Using a type I extreme value distribution for Gaussian random variables allows the
moments of the maxima to be calculated Fisher and Tippett [1928], Cohen [1982].
Deﬁning the mean as µ(k) = E (cid:2)Qk(s, a)(cid:3) and variance as ν(k) = Var(Qk(s, a)), the mean ﬁeld
iteration functions can be found through substituting in the Dirichlet moments and extreme value
expressions into equations A.7 and A.8 and taking the limit as the state space size goes to inﬁnity.
The mean ﬁeld iteration functions are

µ(k + 1) = ¯r + β

(cid:16)

µ(k) + (cid:0)b|A| + a|A|γEM

(cid:1) (cid:112)ν(k)

(cid:17)

ν(k + 1) = σ2

r + β2 π2
12

a|A|ν(k)

,

(A.11)

(A.12)

where γEM is the Euler-Mascheroni constant, and a|A| and b|A| are positive constants dependent on
the action space size |A| given by solving the expressions

√

b|A|

2π exp(0.5(b|A|)2) = |A|

a|A| =

b|A|
(b|A|)2 + 1

.

(A.13)

(A.14)

These constants increase as the size of the action space increases. The accuracy of the extreme value
distribution can be improved through using a type III Weibull distribution Fisher and Tippett [1928].

A.2 Extreme value theory for non-identical case

In the case that we consider non-identically distributed Dirichlet transition probabilities Psa and
non-identical mean rewards ρsa, we thus have that the Q-values will be independent but non-identical
Gaussian variables. This poses a challenge for calculating the statistics of the maximum maxa Qsa.

An asymptotic approximation would appeal to extreme value theory, however this theory was derived
in the case where one takes n samples of from a population Fisher and Tippett [1928]. While closed
form approximations exist in the case of non-identical means and identical variances, we would of
course like to consider both non-identical means and variances. For this, we must resort to numerical
approximations. Consider a sequence of Gaussian random variables

{Xi}i≥1

12

Figure A.1: The CDF of the maximum of n non-identical independent Gaussians P (Mn ≤ u) can
be approximated for large n by the function exp(−τ ), where τ depends on u and all the means and
variances of the Gaussian variables.

with non-constant mean values E(Xi) = µi and variances Var(Xi) = σ2
i . Now suppose that we
have another sequence of Gaussian random variables: {Yi}i≥1, which is stationary. We can then
deﬁne from this a non-stationary Gaussian sequence as:

{Xi}i≥1 = {µi + σiYi}i≥1,

from which the maximum, Mn = sup Xi | i ≤ n gives us the relationship

(cid:26)

{Mn ≤ u} =

Yi ≤

(cid:27)

| i ≤ n

u − µi
σi

where u is a bound for a stationary Gaussian sequence. The CDF is given by:

P (Mn ≤ u) =

n
(cid:89)

i=1

P (Xi ≤ u) =

n
(cid:89)

i=1

P (Yi ≤ ui) =

n
(cid:89)

i=1

Φ(ui)

due to the independence. From the lemma below we know that there exists a real value τ , depending
on {(µi, σ2
i )}i such that this product tends to the function e−τ . that there exists a real value τ , such
that this product tends to the function e−τ .

Lemma A.1. Falk et al. [2010] Let {uni, 1 ≤ i ≤ n, n = 1, 2, ...} be constants such that λn =
min1≤i≤n uni → ∞. Then for 0 ≤ τ ≤ ∞,

n
(cid:89)

i=1

Φ(uni) → e−τ (u,µi,σi),

as n → ∞

(A.15)

if and only if

n
(cid:88)

i=1

(1 − Φ(ui)) → τ (u, {µ}i, {σ}i)

as n → ∞.

(A.16)

We have the CDF of the maximum of non-identical Gaussians, which are independent (or possibly
weakly dependent). From the CDF, we can calculate the mean and variance of the maximum. This
can be done by standard methods (ie. Tonelli’s theorem).

13

B Dynamic mean ﬁeld theory derivations

B.1 Derivation in the i.i.d case

Consider Q-value iteration. Recall the analogy with spin glasses and neural networks is as follows.
Time corresponds to iterations, which we denote n. The local ﬁelds xi(t) are the Q-values Qsa(n),
so each site corresponds to a state and action pair. The couplings are transition probabilities Ps(cid:48)|sa ∼
Dir(αsa), and we have a reward term rsa ∼ N (µr, σ2
The random Bellman optimality equation, perturbed by Gaussian noise Wsa(n), is

r ).

Qsa(n + 1) = rsa +

(cid:88)

s(cid:48)

Ps(cid:48)|saφ(Qs(cid:48):(n)) + Wsa(n)

(B.1)

The noise term Wsa(n) is technically needed for a large deviations analysis, but can be removed in
the generating functional approach later.

The non-linearity φ(·) is

φ(Qs(cid:48):(n)) = max

a(cid:48)

Qs(cid:48)a(cid:48)(n)

(B.2)

So we see each Qsa “couples" to a set of {Qs(cid:48)a(cid:48)}a(cid:48)∈A(s(cid:48)), where A(s(cid:48)) is the action set corresponding
to s(cid:48).

Following the steps in the previous section, the random Q-value Iteration Generating Function is

Z[j,˜j] =

M
(cid:89)

(cid:26) (cid:90) ∞

(cid:27) M −1
(cid:89)

dQ(n)

(cid:26) (cid:90) i∞

−∞

m=1

−i∞

d ˜Q(m)
2πi

(cid:27)

ZW (−˜x(m))

n=1
(cid:20) M −1
(cid:88)

× exp

n=1

sa

(cid:88)

˜Qsa(n)(Qsa(n + 1) − rsa −

Ps(cid:48)|saφ(Qs(cid:48):(n)) + jT

n+1Q(n + 1) + ˜jT

n

(cid:88)

s(cid:48)

(cid:21)
˜Q(n))

(B.3)

We now turn to performing the average over the disorder. The term involving rsa presents no
difﬁculty compared to the neural network case. However, the term involving the uncertain transition
probabilities demands special consideration.

Extracting the term in the moment generating function equation B.3 depending on Ps(cid:48)|sa, the
calculation of interest is

(cid:90) (cid:89)

Dir(P (: |sa); αsa) exp

−

(cid:18)

(cid:88)

s(cid:48)

Ps(cid:48)|sa

M −1
(cid:88)

n=1

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)

(sa),(s(cid:48))

(cid:89)

(cid:90) (cid:89)

(sa)

(s(cid:48))

=

Dir(P (: |sa); αsa) exp

−

(cid:18)

(cid:88)

s(cid:48)

Ps(cid:48)|sa

M −1
(cid:88)

n=1

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)

(B.4)

(B.5)

Here we have used that we assume each random vector P (: |sa) is independent, across pairs (sa).

We propose to use a central limit theorem for weighted Dirichlet random vectors, as proven in
Matsunawa [1985]. This is possible because we have shifted the expectation over the disorder within
the integrals in equation B.3, and thus the Q and ˜Q terms are not random. Denote the sum as
S = (cid:80)

˜Qsa(n)φ(Qs(cid:48):(n)), thus

(cid:80)M −1
n=1

s(cid:48) Ps(cid:48)|sa

(cid:90) (cid:89)

Dir(P (: |sa); αsa) exp

−

(cid:18)

(cid:88)

Ps(cid:48)|sa

(s(cid:48))
(cid:90)

N ( ¯Ssa, σ2

Ssa

s(cid:48)

(cid:19)

(cid:18)

)dS exp

− S

=

= exp( ¯Ssa + σ2

Ssa

)

14

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)

M −1
(cid:88)

n=1

(B.6)

(B.7)

where

¯Ssa =

M −1
(cid:88)

(cid:88)

n=1

s(cid:48)

¯Ps(cid:48)|sa

˜Qsa(n)φ(Qs(cid:48):(n))(cid:1)

σ2
Ssa

=

(cid:88)

s(cid:48),s(cid:48)(cid:48)

Cs(cid:48),s(cid:48)(cid:48)|sa

(cid:18) M −1
(cid:88)

n=1

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)(cid:18) M −1
(cid:88)

m=1

˜Qsa(m)φ(Qs(cid:48)(cid:48):(m))

(cid:19)

(B.8)

(B.9)

In the case we have uniform scaling αs(cid:48)|sa = α for each Ps(cid:48)|sa, and in the case α ∼ O( 1

N ), we have

¯Ssa =

M −1
(cid:88)

(cid:88)

n=1

s(cid:48)

1
N

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:18) M −1
(cid:88)

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)2

1
2N

σ2
Ssa

=

−

(cid:88)

s(cid:48)

(cid:88)

s(cid:48)(cid:54)=s(cid:48)(cid:48)

n=1
(cid:18) M −1
(cid:88)

n=1

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)(cid:18) M −1
(cid:88)

m=1

˜Qsa(m)φ(Qs(cid:48)(cid:48):(m))

(cid:19)

1
2N 2

(B.10)

(B.11)

From here we can write the disorder averaged generating function for Q-value Iteration is
(cid:27) M −1
(cid:89)

(cid:26) (cid:90) i∞

(cid:26) (cid:90) ∞

M
(cid:89)

(cid:27)

¯Z[j,˜j] =

dQ(n)

ZW (−˜x(m))

d ˜Q(m)
2πi

n=1

−∞

m=1

−i∞

× exp

(cid:20) M −1
(cid:88)

(cid:88)

n=1

sa

˜Qsa(n)Qsa(n + 1) − µrsa

˜Qsa +

1
2

σ2
rsa

˜Q2
sa

− ( ¯Ssa + σ2

Ssa

) + jT

n+1Q(n + 1) + ˜jT

n

(cid:21)

˜Q(n))

(B.12)

In order to reveal the dependence on N of the “action" (the terms within the exponential), we will
introduce auxilliary ﬁelds for the variance term, which we write explicitly as follows

σ2
Ssa

=

1
2N

(cid:88)

sa

(cid:88)

(cid:88)

(cid:18) M −1
(cid:88)

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)2

−

1
2N 2

s(cid:48)

sa

(cid:88)

(cid:88)

n=1
(cid:18) M −1
(cid:88)

sa

s(cid:48),s(cid:48)(cid:48)

n=1

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)(cid:18) M −1
(cid:88)

˜Qsa(m)φ(Qs(cid:48)(cid:48):(m))

(cid:19)

m=1

=

1
2N

M −1
(cid:88)

M −1
(cid:88)

(cid:88)

(cid:88)

n=1

m=1

s(cid:48)

sa

˜Qsa(n)φ(Qs(cid:48):(n)) ˜Qsa(m)φ(Qs(cid:48):(m))

−

1
2N 2

M −1
(cid:88)

M −1
(cid:88)

(cid:88)

(cid:88)

n=1

m=1

sa

s(cid:48),s(cid:48)(cid:48)

˜Qsa(n)φ(Qs(cid:48):(n)) ˜Qsa(m)φ(Qs(cid:48)(cid:48):(m))

M −1
(cid:88)

M −1
(cid:88)

(cid:18) (cid:88)

˜Qsa(n) ˜Qsa(m)

(cid:19)(cid:18) 1
2N

(cid:88)

s(cid:48)

φ(Qs(cid:48):(n))φ(Qs(cid:48):(m))

sa

φ(Qs(cid:48):(n))φ(Qs(cid:48)(cid:48):(m))

(cid:19)

(B.13)

=

−

n=1
1
2N 2

m=1
(cid:88)

s(cid:48)(cid:54)=s(cid:48)(cid:48)

Six Auxilliary Fields:

We will progressively introduce six auxiliary ﬁelds. We use a convention that odd powers correspond
to real ﬁelds and even their associated imaginary ﬁelds, arising from the Dirac delta representations.

Θ1(n, m) =

1
2N

(cid:88)

s(cid:48)

φ(Qs(cid:48):(n))φ(Qs(cid:48):(m)) −

1
2N 2

(cid:88)

s(cid:48)

φ(Qs(cid:48):(n))

(cid:88)

s(cid:48)(cid:48)

φ(Qs(cid:48)(cid:48):(m))

(B.14)

15

thus with a Fourier representation

(cid:20)

δ

− 2N Θ1(n, m) + (cid:0) (cid:88)

φ(Qs(cid:48):(n))φ(Qs(cid:48):(m)) −

(cid:90)

=

d ˜Θ2(n, m) exp

s(cid:48)

(cid:20)
˜Θ2(n, m)

(cid:18)

− 2N Θ1(n, m)

1
N

(cid:88)

s(cid:48)

φ(Qs(cid:48):(n))

φ(Qs(cid:48)(cid:48):(m))(cid:1)

(cid:21)

(cid:88)

s(cid:48)(cid:48)

+ (cid:0) (cid:88)

s(cid:48)

φ(Qs(cid:48):(n))φ(Qs(cid:48):(m)) −

φ(Qs(cid:48):(n))φ(Qs(cid:48)(cid:48):(m))(cid:1)

(cid:19)(cid:21)

1
N

(cid:88)

s(cid:48)(cid:54)=s(cid:48)(cid:48)

We consider a particular (n, m) term from equation B.13

(cid:90) (cid:90)

dΘ1(n, m)d ˜Θ2(n, m) exp

(cid:20)
(cid:0) (cid:88)

(cid:21)
˜Qsa(n) ˜Qsa(m)(cid:1)Θ1(n, m)

(cid:20)
˜Θ2(n, m)

(cid:18)

× exp

− N Θ1(n, m) + (cid:0) (cid:88)

φ(Qs(cid:48):(n))φ(Qs(cid:48):(m))

sa

−

1
N

(cid:88)

s(cid:48)(cid:54)=s(cid:48)(cid:48)

φ(Qs(cid:48):(n))φ(Qs(cid:48)(cid:48):(m))(cid:1)

s(cid:48)

(cid:19)(cid:21)

(B.15)

We now introduce two more real ﬁelds

Θ3(n) =

1
N

(cid:88)

s(cid:48)

φ(Qs(cid:48)(cid:48):(n)),

Θ5(m) =

1
N

(cid:88)

s(cid:48)(cid:48)

φ(Qs(cid:48)(cid:48):(m))

(B.16)

The Fourier representation of the Diracs of each ﬁeld (for different times m, n) are,

(cid:20)

δ

− N ˜Θ3(m) +

(cid:88)

s(cid:48)(cid:48)

(cid:21)

φ(Qs(cid:48):(n))

=

(cid:90)

d ˜Θ4(n) exp

(cid:18)

(cid:20)
˜Θ4(n)

− N Θ3(n) +

(cid:19)(cid:21)

φ(Qs(cid:48):(n))

(cid:88)

s(cid:48)

Returning to the (n, m)
1
N

s(cid:48) φ(Qs(cid:48):(n))φ(Qs(cid:48):(m)),

(cid:80)

term from equation B.15,

and ignoring the diagonal

term

(cid:90)

dΘ1(n, m)d ˜Θ2(n, m)dΘ3(m)d ˜Θ4(m) exp

(cid:20)
(cid:0) (cid:88)

(cid:21)
˜Qsa(n) ˜Qsa(m)(cid:1)Θ1(n, m)

(cid:20)
˜Θ2(n, m)

(cid:18)

× exp

− 2N Θ1(n, m) + (cid:0) (cid:88)

φ(Qs(cid:48):(n))φ(Qs(cid:48):(m)) − N Θ3(n)Θ5(m)(cid:1)

(cid:19)(cid:21)

sa

(cid:18)

(cid:20)
˜Θ4(n)

× exp

− N Θ3(n) +

(cid:88)

s(cid:48)

s(cid:48)

(cid:19)(cid:21)

φ(Qs(cid:48)(cid:48):(n))

exp

(cid:20)
˜Θ6(m)

(cid:18)

− N Θ5(m) +

(cid:19)(cid:21)

φ(Qs(cid:48)(cid:48):(m))

(cid:88)

s(cid:48)(cid:48)

We see we have succesfully decoupled the interaction terms, and thus revealed the dependence of the
action in terms of N .

¯Z[k1, ˜k2, k3, ˜k4, k5, ˜k6] =

(cid:90)

dΘ1(n, m)d ˜Θ2(n, m)dΘ3(m)d ˜Θ4(m)

× exp

(cid:20) M −1
(cid:88)

M −1
(cid:88)

n=1

m=1

−2N Θ1(n, m) ˜Θ2(n, m) − N Θ2(n, m)Θ3(n)Θ5(m) − N Θ3(n) ˜Θ4(n)

− N Θ5(m) ˜Θ6(m) + N ln Z[Θ1, Θ2, Θ3, Θ4] + k1

T Θ1 + ˜k2

T ˜Θ2 + k3

T Θ3 + ˜kT
4

˜Θ4

(cid:21)

(B.17)

16

where we deﬁne

Z N [Θ1, Θ2, Θ3, Θ4, Θ5, Θ6] =

M
(cid:89)

(cid:26) (cid:90) ∞

n=1

−∞

(cid:27) M −1
(cid:89)

dQ(n)

(cid:26) (cid:90) i∞

m=1

−i∞

d ˜Q(m)
2πi

(cid:27)

ZW (−˜x(m))

(cid:20) M −1
(cid:88)

N
(cid:88)

× exp

˜Qsa(n)Qsa(n + 1) − µrsa

˜Qsa +

σ2
rsa

˜Q2

sa − ¯Ssa

(cid:21)

1
2

n=1
(cid:20) M −1
(cid:88)

sa

M −1
(cid:88)

N
(cid:88)

× exp

˜Qsa(n)Θ1(m, n) ˜Qsa(m) + ˜Θ2(n, m)

n=1
(cid:88)

+ ˜Θ4(n)

s(cid:48)

m=1

sa

φ(Qs(cid:48):(n)) + ˜Θ6(m)

(cid:21)

φ(Qs(cid:48)(cid:48):(m))

(cid:88)

s(cid:48)(cid:48)

(cid:88)

s(cid:48)

φ(Qs(cid:48):(n))φ(Qs(cid:48):(m))

(B.18)

The form of equation B.17 suggests a saddle point approximation, as in the recurrent network
literature,

δ
δΘ{1,2,3,4}

(cid:20)

(cid:21)
− N Θ1(n, m) ˜Θ2(n, m) − N Θ3(m) ˜Θ4(m) + N ln Z[Θ1, Θ2, Θ3, Θ4]

= 0

(B.19)

δS
δΘ1

= 0 =⇒ ˜Θ2(n, m) = (cid:104) ˜Qsa(n) ˜Qsa(m)(cid:105)Θ∗ = 0

(B.20)

δS
δΘ2

= 0 =⇒ Θ1(n, m) = (cid:104)φ(Qs(cid:48):(n))φ(Qs(cid:48):(m))(cid:105)Θ∗ − (cid:104)Θ3(n)Θ5(m)(cid:105)Θ∗

(B.21)

δS
δΘ3

= 0 =⇒ −N

(cid:88)

m

Θ2(n, m)Θ5(m) − N Θ4(n) = 0 =⇒ Θ4(n) = 0 since Θ2 is zero.

(B.22)

δS
δΘ4

= 0 =⇒ Θ3(n, m) = (cid:104)φ(Qs(cid:48):(n))(cid:105)Θ∗

(B.23)

with identical results for Θ5 and Θ6

17

B.2 The non-i.i.d case

We argue here that in the ﬁrst instance, the non-i.i.d case can be considered via perturbations about
the identical case. These perturbations essentially do not contribute to the “action", the term within
the exponential of the moment generating function, and thus they can be ignored. The more general
case of a mixture of terms of different orders in the action, arising from Dirichlet parameters of
different orders, is slightly more tedious to deal with. Stepping through the algebra essentially reveals
that above the order 1/N case the variance will ultimately vanish. However, the rate at which this
will vanish in practice will depend on the mixture. We aim to explore this in future work.

B.2.1 Perturbations about the mean for the disorder averaged MGF

Recall that the key term in the disorder averaged generating function equation B.12 is exp( ¯Ssa+σ2
where

Ssa

),

¯Ssa =

M −1
(cid:88)

(cid:88)

n=1

s(cid:48)

¯Ps(cid:48)|sa

˜Qsa(n)φ(Qs(cid:48):(n))(cid:1)

σ2
Ssa

=

(cid:88)

s(cid:48),s(cid:48)(cid:48)

Cs(cid:48),s(cid:48)(cid:48)|sa

(cid:18) M −1
(cid:88)

n=1

˜Qsa(n)φ(Qs(cid:48):(n))

(cid:19)(cid:18) M −1
(cid:88)

˜Qsa(m)φ(Qs(cid:48)(cid:48):(m))

(cid:19)

m=1

(B.24)

(B.25)

where for the Dirichlet random variables recall that

¯Ps(cid:48)|sa =

αs(cid:48)|sa
t αt|sa

(cid:80)

Cs(cid:48),s(cid:48)(cid:48)|sa =

δs(cid:48)s(cid:48)(cid:48) −

αs(cid:48) |sa
t αt|sa

αs(cid:48)(cid:48)|sa
(cid:80)
t αt|sa

(cid:80)
1 + (cid:80)

t αt|sa

Recall also that in the case that αs(cid:48)|sa = α, we always have

and that if αs(cid:48)|sa = α = 1/N then

¯Ps(cid:48)|sa =

1
N

Cs(cid:48),s(cid:48)(cid:48)|sa =

N δs(cid:48)s(cid:48)(cid:48) − 1
N 2(N α + 1)

.

(B.26)

(B.27)

(B.28)

√

Now if we simply consider the non-identical case as a small perturbation about the identical, such
as αs(cid:48)|sa = α + ∆s(cid:48)|sa with ∆s(cid:48)|sa a lower order in N , then this term will not contribute to the
exponent exp( ¯Ssa + σ2
) and the analysis follows the same steps as above. For example if α is of
order O(1/N ) and ∆ is of order O(1/N 2).

Ssa

Finally, consider that if α is of order greater than O(1/
N ) the covariance vanishes, meaning the
auxiliary ﬁelds and saddle point approximation are not required. This implies the Gaussian process
that the mean ﬁeld theory predicts will have vanishing variance, and thus a typical realisation will be
well described by the mean equation of the DMFP equations.

Intermediate cases are of course possible, and will arise in practice, since the Dirichlet parameters
will increase in integer quantities. We anticipate the Gaussian theory to hold, however what we may
observe in practice are ﬁnite size effects, not only due to the ﬁnite nature of the system but also the
mixture of highly uncertain random variables and those more certain. We aim to pursue this question
in future work.

18

C Simulations

C.1 Q-Q plots of Q-values.

Figure C.1: |S| = 5

Figure C.2: |S| = 50

Figure C.4: Q-Q plots of standardised optimal Q-values at initialisation, against quantiles of standard
normal distribution for varying state space size. discount factor β = 0.95

Figure C.3: |S| = 500

19

-3-2-10123Standard Normal Quantiles-3-2-101234Quantiles of Input Sample-3-2-10123Standard Normal Quantiles-3-2-10123Quantiles of Input Sample-3-2-10123Standard Normal Quantiles-3-2-101234Quantiles of Input SampleC.2 Dynamic mean ﬁeld programming: empirical data versus theoretical predictions.

Figure C.5: β = 0.5, N = 5

Figure C.6: β = 0.8, N = 50

Figure C.7: β = 0.9, N = 500

Figure C.8: Comparison between empirical and DMFP theoretical mean and variances against
iterations k for varying discount factor β and varying values of N .

20

05101520253002468101214Empirical meansTheoretical meansEmpirical varianceTheoretical variance05101520253002468101214Empirical meansTheoretical meansEmpirical varianceTheoretical variance05101520253002468101214Empirical meansTheoretical meansEmpirical varianceTheoretical variance