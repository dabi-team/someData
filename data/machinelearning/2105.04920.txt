More Powerful Conditional Selective Inference for

Generalized Lasso by Parametric Programming

Vo Nguyen Le Duy

Nagoya Institute of Technology and RIKEN

duy.mllab.nit@gmail.com

Ichiro Takeuchi

Nagoya Institute of Technology and RIKEN

takeuchi.ichiro@nitech.ac.jp

May 12, 2021

Abstract

Conditional selective inference (SI) has been studied intensively as a new statistical inference frame-

work for data-driven hypotheses. The basic concept of conditional SI is to make the inference conditional

on the selection event, which enables an exact and valid statistical inference to be conducted even when

the hypothesis is selected based on the data. Conditional SI has mainly been studied in the context of

model selection, such as vanilla lasso or generalized lasso. The main limitation of existing approaches

is the low statistical power owing to over-conditioning, which is required for computational tractability.

In this study, we propose a more powerful and general conditional SI method for a class of problems

that can be converted into quadratic parametric programming, which includes generalized lasso. The

key concept is to compute the continuum path of the optimal solution in the direction of the selected

test statistic and to identify the subset of the data space that corresponds to the model selection event

by following the solution path. The proposed parametric programming-based method not only avoids

the aforementioned major drawback of over-conditioning, but also improves the performance and practi-

cality of SI in various respects. We conducted several experiments to demonstrate the eﬀectiveness and

eﬃciency of our proposed method.

1
2
0
2

y
a
M
1
1

]
L
M

.
t
a
t
s
[

1
v
0
2
9
4
0
.
5
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1

Introduction

As machine learning (ML) is applied to solve numerous practical problems, the quantiﬁcation of the reliability

of data-driven knowledge obtained by ML algorithms is becoming increasingly important. Among the various

potential approaches for reliable ML, conditional selective inference (SI) has been recognized as a new

promising method for assessing the statistical reliability of data-driven hypotheses that are selected by

ML algorithms. The main concept of conditional SI is to make the inference for a data-driven hypothesis

conditional on the selection event that the hypothesis is selected, which enables an exact and a valid inference

to be conducted on the selected hypothesis. In the conditional SI framework, the statistical signiﬁcance and

reliability of data-driven selected hypotheses are quantiﬁed by the so-called selective p-values and selective

conﬁdence intervals, which have proper false positive rates and coverage guarantees, respectively.

Lee et al. [2016] ﬁrst introduced conditional SI as a statistical inference tool for the features selected by

lasso [Tibshirani, 1996]. Subsequently, Hyun et al. [2018a] studied SI for inference on the selected model

using generalized lasso [Tibshirani and Taylor, 2011]. Their basic concept was to characterize the selection

event using a polytope (a set of linear inequalities) in the sample space. In general, we refer to such methods

as polytope-based SI approaches. The practical computational methods developed by these authors can be

used when the selection event can be characterized by a single polytope.

However, the application scope of such polytope-based SI is limited because it can only be used when

the characterization of all relevant selection events is represented by a polytope. Therefore, in most existing

polytope-based SI studies, additional conditioning is required for the selection event to be characterized as a

polytope. For example, in the case of lasso [Lee et al., 2016], the set of selected features as well as their signs

require conditioning. Similarly, in the case of generalized lasso [Hyun et al., 2018a], additional conditioning

on the signs as well as on the history (sequential order) whereby the selected elements enter the selected

model is required. Such additional conditioning results in low statistical power, which is widely recognized

as a major drawback of polytope-based SI studies [Fithian et al., 2014].

Contributions. The contributions of this study are as follows:

• We go beyond the scope of polytope-based SI and propose a new SI approach based on parametric

programming (PP). We name the proposed method PP-based SI. The basic concept of PP-based SI is

to compute the continuum path of the optimal solutions in the direction of the selected test statistic

using PP, which is subsequently used to identify the exact sampling distribution of the test statistic

with the minimum amount of conditioning. Therefore, PP-based SI can fundamentally resolve the

over-conditioning problem, which is a major concern in polytope-based SI, thereby achieving high

statistical power.

• We derive the proposed PP-based SI for a generic class of conditional SI problems that can be repre-

sented as parametric quadratic programs (QPs). We demonstrate that the conditional SI formulations

2

for many practical problems, including generalized lasso, elastic net, non-negative least squares, and

Huber regression with the (cid:96)1 penalty, belong to this class, which means that the proposed PP-based

SI can be used extensively.

• Furthermore, we discuss how the advantages of PP can be exploited for the eﬀective performance

of conditional SI in various data analysis tasks. As an example, using PP, we demonstrate that

conditional SI can be conducted with minimal conditioning for regularization parameter selection by

cross-validation (CV), the selection event of which is too complicated to be characterized as a single

polytope, but can be fully characterized using PP1.

• We conducted intensive experiments on both synthetic and real-world datasets, by means of which we

presented evidence that our proposed method can successfully control the false positive rate, has higher

statistical power than polytope-based SI, and provides superior results in practical applications.

A preliminary short version of this work was presented at the AI & Statistics (AISTATS2021) conference

[Le Duy and Takeuchi, 2021].

In the conference version, we only studied a speciﬁc case of vanilla lasso.

In this study, we extended the basic concept of PP-based SI to a more general class of problems that can

be formulated as parametric QPs, which includes vanilla lasso as a special case. Moreover, we extended

the proposed method to various aspects and conducted intensive additional experiments to demonstrate the

applicability of the generalized PP-based SI to a wider class of problems and settings.

Related works.

In traditional statistical inference, it is assumed that the hypothesis is ﬁxed in advance.

That is, the hypothesis on which we wish to conduct inference is determined prior to observing the dataset.

Therefore, if traditional statistical inference methods are applied to data-driven hypotheses, the inferential

results will no longer be valid. This problem has been discussed extensively in the context of testing the

signiﬁcance of the features selected by a feature selection method, such as lasso or stepwise feature selection.

Several approaches have been proposed to address this problem [Benjamini and Yekutieli, 2005, Leeb and

P¨otscher, 2005, 2006, Benjamini et al., 2009, P¨otscher and Schneider, 2010, Berk et al., 2013, Lockhart et al.,

2014, Taylor et al., 2014].

In recent years, Lee et al. [2016] proposed a practical SI framework to perform exact (non-asymptotic)

inference for a set of features selected by lasso. In their work, the authors revealed that the selection event

can be characterized as a polytope by conditioning on a set of selected features and their signs. Furthermore,

Hyun et al. [2018a] demonstrated that polytope-based SI is applicable to generalized lasso by performing

additional conditioning on the signs as well as the history (sequential order) whereby the selected elements

entered the selected model. Following the seminal work of [Lee et al., 2016], conditional inference-based SI

has been actively studied and applied to various problems [Fithian et al., 2015, Choi et al., 2017, Tian and

1Loftus [2015] considered conditional SI for CV-based regularization parameter selection, but it was highly over-conditioned

with additional events. We demonstrate that our PP-based SI is more powerful than this approach.

3

Taylor, 2018, Chen and Bien, 2019, Hyun et al., 2018b, Loftus and Taylor, 2014, Loftus, 2015, Panigrahi

et al., 2016, Tibshirani et al., 2016, Yang et al., 2016, Suzumura et al., 2017, Tanizaki et al., 2020, Duy et al.,

2020b,a, Sugiyama et al., 2020, Tsukurimichi et al., 2021].

It is desirable to conduct more powerful inference by conditioning on as little information as possible in

conditional SI [Fithian et al., 2014]. However, in polytope-based SI, an excessive amount of over-conditioning

is required to represent the selection event using a single polytope. The authors of Lee et al. [2016] already

mentioned the problem of over-conditioning and discussed the solution for removing the additional condi-

tioning by enumerating all possible combinations of signs and taking the union over the resulting polyhedra.

Unfortunately, such an enumeration for an exponentially increasing number of sign combinations is only

feasible when the number of selected features is small. Loftus and Taylor [2014] extended polytope-based

SI to cases in which the selection event is characterized by quadratic inequalities. Although we do not dis-

cuss quadratic inequality-based SI further, as it suﬀers from a similar over-conditioning issue, our proposed

PP-based SI can also be applied to resolve the issue for this class of conditional SIs.

Our work was motivated by Liu et al. [2018], in which the authors proposed solutions to overcome the

over-conditioning issue of polytope-based SI for vanilla lasso in certain special settings. In one of the settings,

inference on the full model parameters in which conditional SI can be performed with minimal conditioning

was studied, because a full model parameter is not dependent on other parameters. Moreover, our work was

motivated by a discussion in the paper where the authors noted that multiple lasso ﬁtting at a sequence

of grid points may aid in alleviating over-conditioning. However, the authors did not suggest any practical

computational methods to realize this concept. Furthermore, the conditional sampling distribution that is

evaluated at a ﬁnite number of grid points only provides an approximation of the exact distribution, which

means that the theoretical validity of the conditional SI is no longer guaranteed. Our proposed PP-based SI

can be interpreted as a means of solving lasso at inﬁnitely many grid points, which completely resolves these

challenging problems. As another direction to resolve over-conditioning, Tian and Taylor [2018] and Terada

and Shimodaira [2019] proposed methods using randomization. A drawback of these randomization-based

approaches (including the simple data-splitting approach) is that further randomness is added in both the

feature selection and inference stages.

PP has long been studied in the optimization ﬁeld to solve a family of optimization problems that

are parameterized by a scalar parameter [Ritter, 1984, Allgower and George, 1993, Gal, 1995, Best, 1996].

Moreover, PP has been used in the context of the regularization path in ML [Osborne et al., 2000, Efron

and Tibshirani, 2004, Hastie et al., 2004, Rosset and Zhu, 2007, Bach et al., 2006, Rosset and Zhu, 2007,

Tsuda, 2007, Lee and Scott, 2007, Takeuchi et al., 2009, Takeuchi and Sugiyama, 2011, Karasuyama and

Takeuchi, 2010, Hocking et al., 2011, Karasuyama et al., 2012, Ogawa et al., 2013, Takeuchi et al., 2013].

The regularization path is a method of tracking the manner in which the optimal solution changes when the

regularization parameter changes, which is useful for eﬃcient model selection. Our main idea was to employ

PP to track how the optimal solution and selected features change when the training dataset changes in

4

the direction of the selected test statistic, which enables the exact sampling distribution of the test statistic

that is conditional on the selection event to be identiﬁed. The power of the conditional SI introduced in the

pioneering work of Lee et al. [2016] can be optimized using the proposed approach, which is applicable to

conditional SI for a wide class of problems including generalized lasso.

2 Problem Statement

To formulate the problem, we consider a random response vector

Y = (Y1, ..., Yn)(cid:62)

N(µ, Σ),

∼

(1)

where n is the number of instances, µ is an unknown vector, and Σ

Rn×n is a covariance matrix that is
known or estimable from independent data. The goal is statistical quantiﬁcation of the signiﬁcance of the

∈

data-driven hypotheses that are obtained by applying the generalized lasso estimator to the response vector.

Generalized lasso and its selection event. We consider a linear regression model with p features
Rn×p, in which the features are considered as non-

Rn and denote the feature matrix as X

x1, . . . , xp

∈

∈

random. We do not make any assumption about the relationship between the µ and p features x1, . . . , xp
∈
Rn, but consider the case in which a linear model with generalized lasso regularization is employed to model
Rn
the relationship between X and a random response vector Y . Given an observed response vector yobs

∈

that is sampled from model (1), the generalized lasso optimization problem is expressed as

ˆβ = arg min

β∈Rp

1
2 (cid:107)

yobs

Xβ

2
2 + λ
(cid:107)
(cid:107)

Dβ

1,

(cid:107)

−

(2)

Rm×p is a penalty matrix and λ

where D
0 is a regularization parameter. The matrix D and its number
of rows are predetermined by the user to produce the desired structures in the solution ˆβ in (2). Examples

≥

∈

of matrix D are presented in Examples 1, 2, and 3.

As the optimization in (2) produces the sparsity of D ˆβ, we deﬁne a set of non-zero components (the

active set) as follows:

obs =

M

(yobs) =

A

j : (D ˆβ)j
{

,
= 0
}

[m],

j

∈

where

: Y

A

(cid:55)→ M

indicates the algorithm that maps a response vector Y to a set of non-zero components

. Thereafter, we deﬁne the selection event in which the active set for a random response vector Y is the

M
same as the observed response vector yobs:

Statistical inference. Let η(cid:62)

j Y be a linear contrast that indicates the test statistic that we wish to
consider, where ηj is deﬁned depending on the problem and the jth selected component in the observed

(Y ) =

A

(yobs)

A

.

(cid:9)

(cid:8)

(3)

active set.

5

(cid:54)
Example 1. In the case of testing the features selected by vanilla lasso [Tibshirani, 1996], D = Ip
which is the identity matrix. The test statistic η(cid:62)

Rp×p,
j Y = ˆβj represents the coeﬃcient of the jth selected feature

∈

[Lee et al., 2016], where ηj is deﬁned as

ηj = XMobs

X (cid:62)

MobsXMobs

−1

ej,

(4)

in which ej

R|Mobs| is a basis vector with 1 at the jth position. This form of test statistic can also be
applied to test the features that are selected by other regression methods, such as elastic net [Zou and Hastie,

∈

(cid:0)

(cid:1)

2005], Huber regression [Huber, 1992], or non-negative least squares.

Example 2. In the context of changepoint (CP) detection using fused lasso, the matrix D

R(p−1)×p is

∈

expressed as

D =

1

−
0

1

1

−

0

0











0

1

0

0

0

· · ·

· · ·

· · ·

1
· · · −

.

0

0




1




The test statistic η(cid:62)

j Y represents the diﬀerence in the sample mean between the left and right segments of

the CP at the jth position, which was also used in Hyun et al. [2018a]. In this case, ηj is deﬁned as

ηj =

j

−

1
jprev

1n
jprev+1:j −

1
jnext

1n
j+1:jnext ,

j

−

(5)

where jprev
j, respectively, and 1n

∈ M

obs and jnext

obs are the CP positions before and after the selected CP at position
Rn is a vector in which the elements from positions s to e are set to 1, and 0

∈ M

s:e ∈

otherwise.

Example 3. In trend ﬁltering, the aim is to test whether a change occurs in the trend at position j

obs.

∈ M

We deﬁne ηj as follows:

ηj = ej−1

−

2ej + ej+1,

Rn. The test statistic η(cid:62)

j Y indicates that we wish to test whether the points at positions j,

where ej

∈

1, and j + 1 lie on the same line statistically. In this case, the matrix D

j

−

R(p−2)×p is expressed as

∈

D =

2

1
−

1

−
2

0

0

· · ·

· · ·

· · ·

0

0

0

1
· · · −

1

−
0











0

0

2

.

0

0







1




−

For the inference, we consider the following null hypothesis and alternative hypothesis:

H0,j : η(cid:62)

j µ = 0 vs. H1,j : η(cid:62)

j µ

= 0.

(6)

6

(cid:54)
Conditional SI. Suppose that the hypotheses in (6) are ﬁxed; that is, non-random. Thus, the naive

(two-sided) p-value in the classical z-test is obtained by

P naive
j

= PH0,j

η(cid:62)
j Y
|

j yobs
η(cid:62)

| ≥ |

.

|

(7)

However, as the hypotheses in (6) are not ﬁxed in advance, the naive p-value is not valid in the sense that,

(cid:0)

(cid:1)

if we reject H0,j with a signiﬁcance level α (e.g., α = 0.05), the false positive rate (type-I error) cannot be

controlled at the level α. This is because the hypotheses in (6) are selected by the data and selection bias

exists.

It is necessary to remove the information that has been used for the initial hypothesis generation process

to correct the selection bias. This can be achieved by considering the sampling distribution of the test

statistic η(cid:62)

j Y that is conditional on the selection event; that is,

η(cid:62)
j Y

|

(Y ) =

A

(yobs), q(Y ) = q(yobs)

A

,

(8)

j Σηj)−1 is the nuisance component that is independent of
where q(Y ) = (In
−
j Y . The second condition q(Y ) = q(yobs) indicates that the nuisance component for a
the test statistic η(cid:62)

j )Y with c = Σηj(η(cid:62)

cη(cid:62)

(cid:8)

(cid:9)

random vector Y is the same as that for yobs 2.

Once the selection event has been identiﬁed, the pivotal quantity can be computed:

F Z
j µ,η(cid:62)
η(cid:62)

j Σηj

(η(cid:62)

j Y ).

(Y ) =

A

|

(yobs), q(Y ) = q(yobs)

A

,

(9)

which is the c.d.f. of the truncated normal distribution with a mean η(cid:62)

j µ, variance η(cid:62)

j Σηj, and truncation

(cid:8)

(cid:9)

region

Z

, which is calculated based on the selection event. Based on the pivotal quantity, the selective type-I

error or selective p-value [Fithian et al., 2014] can be considered in the following form:

P selective

j

πj, 1
= 2 min
{

−

πj

,
}

(10)

where πj = 1

−

F Z

0,η(cid:62)

j Σηj

(η(cid:62)

j Y ), which is valid in the sense that

ProbH0,j

P selective

j

< α

= α,

α

∀

∈

[0, 1].

Furthermore, to obtain a conﬁdence level of 1

(cid:0)

(cid:1)

α for any α

−

∈

[0, 1], by inverting the pivotal quantity in

Equation (9), we can determine the smallest and largest values of η(cid:62)

j µ such that the value of the pivotal

quantity remains in the interval

[Lee et al., 2016].

α
2 , 1

α
2

−

(cid:2)

(cid:3)

Challenge in conditional data space characterization. The main diﬃculty in the above conditional

SI is that the characterization of the minimal conditional data space

(Y ) =

A

(yobs), q(Y ) = q(yobs)

A

2q(Y ) corresponds to the component z in the seminal paper (see Lee et al. [2016], Section 5, Eq. 5.2 and Theorem 5.2).

(cid:8)

(cid:9)

7

Figure 1: Schematic of proposed method. We obtain the observed active set

obs by applying generalized
lasso to the observed data yobs. The statistical inference for each selected element in the observed active

M

set is conducted conditional on the subspace

Y

, the data of which have the same active set as yobs. We

introduce a PP method for characterizing the conditional data space

by searching the parameterized line.

Y

in Equation (8) is intractable. To overcome this issue, Hyun et al. [2018a] considered the inference to be

conditional not only on the active set, but also on the signs and history (order) whereby the elements of
D ˆβ entered the active set. Unfortunately, such additional conditioning on the signs and history leads to low
statistical power owing to over-conditioning 3.

In the following section, we introduce a method for identifying the minimum amount of conditioning

(Y ) =

A

(yobs), q(Y ) = q(yobs)

A

that results in higher statistical power. The main concept is to compute

the path of the generalized lasso solutions in the direction of interest ηj. By focusing on the line along ηj, the
(cid:8)

(cid:9)

majority of irrelevant regions that do not aﬀect the truncated normal sampling distribution can be skipped

because they do not intersect with this line. Thus, we can skip the majority of combinations of signs and

history that never appear when applying generalized lasso to the data on the line.

3 Proposed Method

We present the technical details of the proposed method in this section. A schematic of the method is
provided in Figure 1. We ﬁrst introduce a QP reformulation for the generalized lasso problem in §3.1.
Thereafter, the characterization of the conditional data space is presented in §3.2. Subsequently, we propose
a PP approach for identifying the conditional data space in §3.3. Finally, the detailed algorithm is presented
in §3.4.

3This over-conditioning corresponds to the additional conditioning on the signs of the selected features in the seminal

conditional SI study of vanilla lasso [Lee et al., 2016].

8

Data Space ℝnyobsAlgorithm 𝒜(yobs)Proposed MethodObserved Active Setℳobs={1,5,7}Parametrized
Liney(z)=a+bzConditional Data Space 
𝒴={y∈ℝn∣𝒜(y)=𝒜(yobs),q(y)=q(yobs)}{1,5,7}{1,5,7}{1,5}{5,7}{1,7}MorePowerfulSelectiveInferenceforGeneralizedLassoWeobtain(12)bydecomposing⇠=⇠+ ⇠ with⇠+,⇠  0.Here,thekeypointisthatthecomponent||⇠||1in(13)canbewrittenas||⇠||1=Pj2[m](⇠+j+⇠ j)becauseatleastoneof⇠+jand⇠ j,j2[m],mustbesetto0attheoptimalsolution.Inotherwords,ˆ⇠+j>0)ˆ⇠ j=0,andviceversa.Theideaofdecomposing⇠=⇠+ ⇠ areoftenemployedinreformulating`1-normof⇠.3.2ConditionalDataSpaceCharacterizationLetusdeﬁnethesetofy2RnwhichsatisﬁestheconditionsinEquation(8)asY={y2Rn|A(y)=A(yobs),q(y)=q(yobs)}.(14)Accordingtothesecondcondition,thedatainYisrestrictedtoaline(seeSec6inLiuetal.(2018),andFithianetal.(2014)).Therefore,thesetYcanbere-written,usingascalarparameterz2R,asY={y(z)=a+bz|z2Z},(15)wherea=q(yobs),b=⌃⌘j(⌘>j⌃⌘j) 1,andZ=nz2R|A(y(z))=A(yobs)o.(16)Now,letusconsiderarandomvariableZ2Randitsobservationzobs2R,whichsatisfyY=a+bZandyobs=a+bzobs.Theconditionalinferencein(8)isre-writtenastheproblemofcharacterizingthesamplingdistributionofZ|{Z2Z}.(17)SinceZ⇠N(0,⌘>j⌃⌘j)underthenullhypothesis,Z|Z2ZfollowsatruncatedNormaldistribution.OncethetruncationregionZisidentiﬁed,thepivotalquantityinEquation(9)isequaltoFZ0,⌘>j⌃⌘j(Z),andcanbeeasilyobtained.Thus,theremainingtaskistocharacterizeZ.CharacterizationoftruncationregionZ.Letusintroducetheoptimizationproblem(12)withparametrizedresponsevectory(z)=a+bz(a,baredeﬁnedin(15))forz2Rasˆr(z)=argminr12r>Pr+(q0+q1z)>rs.t.Grh0+h1z,(18)wherer=( ,⇠+,⇠ )>2Rp+2m,q0=  X>a, 1m, 1m >2Rp+2m,q1=  X>b,0,0 2Rp+2m,P2R(p+2m)⇥(p+2m)andG2R4m⇥(p+2m),h0=h1=04m,P=0@X>X000000001A,G=0@ DD00Im Im Im0 ImIm0 Im1A>.9MorePowerfulSelectiveInferenceforGeneralizedLassoWeobtain(12)bydecomposing⇠=⇠+ ⇠ with⇠+,⇠  0.Here,thekeypointisthatthecomponent||⇠||1in(13)canbewrittenas||⇠||1=Pj2[m](⇠+j+⇠ j)becauseatleastoneof⇠+jand⇠ j,j2[m],mustbesetto0attheoptimalsolution.Inotherwords,ˆ⇠+j>0)ˆ⇠ j=0,andviceversa.Theideaofdecomposing⇠=⇠+ ⇠ areoftenemployedinreformulating`1-normof⇠.3.2ConditionalDataSpaceCharacterizationLetusdeﬁnethesetofy2RnwhichsatisﬁestheconditionsinEquation(8)asY={y2Rn|A(y)=A(yobs),q(y)=q(yobs)}.(14)Accordingtothesecondcondition,thedatainYisrestrictedtoaline(seeSec6inLiuetal.(2018),andFithianetal.(2014)).Therefore,thesetYcanbere-written,usingascalarparameterz2R,asY={y(z)=a+bz|z2Z},(15)wherea=q(yobs),b=⌃⌘j(⌘>j⌃⌘j) 1,andZ=nz2R|A(y(z))=A(yobs)o.(16)Now,letusconsiderarandomvariableZ2Randitsobservationzobs2R,whichsatisfyY=a+bZandyobs=a+bzobs.Theconditionalinferencein(8)isre-writtenastheproblemofcharacterizingthesamplingdistributionofZ|{Z2Z}.(17)SinceZ⇠N(0,⌘>j⌃⌘j)underthenullhypothesis,Z|Z2ZfollowsatruncatedNormaldistribution.OncethetruncationregionZisidentiﬁed,thepivotalquantityinEquation(9)isequaltoFZ0,⌘>j⌃⌘j(Z),andcanbeeasilyobtained.Thus,theremainingtaskistocharacterizeZ.CharacterizationoftruncationregionZ.Letusintroducetheoptimizationproblem(12)withparametrizedresponsevectory(z)=a+bz(a,baredeﬁnedin(15))forz2Rasˆr(z)=argminr12r>Pr+(q0+q1z)>rs.t.Grh0+h1z,(18)wherer=( ,⇠+,⇠ )>2Rp+2m,q0=  X>a, 1m, 1m >2Rp+2m,q1=  X>b,0,0 2Rp+2m,P2R(p+2m)⇥(p+2m)andG2R4m⇥(p+2m),h0=h1=04m,P=0@X>X000000001A,G=0@ DD00Im Im Im0 ImIm0 Im1A>.9MorePowerfulSelectiveInferenceforGeneralizedLassoconsideredtobenon-random.Inthisstudy,wedonotposeanyassumptionabouttherelationshipbetweenµandpfeaturesx1,...,xp2Rn,butconsiderthecasewherealinearmodelwithgeneralizedlassoregularizationisemployedformodelingtherelationshipbetweenXandarandomresponsevectorY.Givenanobservedresponsevectoryobs2Rnsampledfromthemodel(1),thegeneralizedlassooptimizationproblemisgivenbyˆ =argmin 2Rp12kyobs X k22+ kD k1,(2)whereD2Rm⇥pisapenaltymatrix,and  0isaregularizationparameter.ThematrixDanditsnumberofrowsarepre-determinedbytheuserforthepurposeofproducingsomedesiredstructuresinthesolutionˆ in(2).SomeexamplesofthematrixDarepresentedinExamples1,2and3.Sincetheoptimizationin(2)producesthesparsityofDˆ ,wedeﬁneasetofnon-zerocomponents(activeset)asMobs=A(yobs)={j:(Dˆ )j6=0},j2[m],whereA:Y7!MindicatesthealgorithmwhichmapsaresponsevectorYintoasetofnon-zerocomponentsM.Then,wedeﬁnetheselectioneventthattheactivesetforarandomresponsevectorYisthesameastheobservedresponsevectoryobsasnA(Y)=A(yobs)o.(3)Statisticalinference.Let⌘>jYbealinearcontrastthatindicatesthetest-statisticwewanttoconsider,where⌘jisdeﬁneddependingontheproblemandthejthselectedcom-ponentintheobservedactiveset.Example1Inthecaseoftestingthefeatureselectedbyvanillalasso(Tibshirani,1996),D=Ip2Rp⇥p,whichistheidentitymatrix.Thetest-statistic⌘>jY=ˆ jrepresentsthecoe cientofthejthselectedfeature(Leeetal.,2016)where⌘jisdeﬁnedas⌘j=XMobs⇣X>MobsXMobs⌘ 1ej,(4)withej2R|Mobs|beingabasisvectorwitha1atthejthposition.Thisformoftest-statisticcanalsobeappliedtotestthefeaturesselectedbyotherregressionmethodssuchaselasticnet(ZouandHastie,2005),huberregression(Huber,1992),ornon-negativeleastsquares.Example2Inthecontextofchangepoint(CP)detectionusingfusedlasso,thematrixD2R(p 1)⇥pisgivenbyD=0BB@ 110···000 11···00···000··· 111CCA.5MorePowerfulSelectiveInferenceforGeneralizedLassoconsideredtobenon-random.Inthisstudy,wedonotposeanyassumptionabouttherelationshipbetweenµandpfeaturesx1,...,xp2Rn,butconsiderthecasewherealinearmodelwithgeneralizedlassoregularizationisemployedformodelingtherelationshipbetweenXandarandomresponsevectorY.Givenanobservedresponsevectoryobs2Rnsampledfromthemodel(1),thegeneralizedlassooptimizationproblemisgivenbyˆ =argmin 2Rp12kyobs X k22+ kD k1,(2)whereD2Rm⇥pisapenaltymatrix,and  0isaregularizationparameter.ThematrixDanditsnumberofrowsarepre-determinedbytheuserforthepurposeofproducingsomedesiredstructuresinthesolutionˆ in(2).SomeexamplesofthematrixDarepresentedinExamples1,2and3.Sincetheoptimizationin(2)producesthesparsityofDˆ ,wedeﬁneasetofnon-zerocomponents(activeset)asMobs=A(yobs)={j:(Dˆ )j6=0},j2[m],whereA:Y7!MindicatesthealgorithmwhichmapsaresponsevectorYintoasetofnon-zerocomponentsM.Then,wedeﬁnetheselectioneventthattheactivesetforarandomresponsevectorYisthesameastheobservedresponsevectoryobsasnA(Y)=A(yobs)o.(3)Statisticalinference.Let⌘>jYbealinearcontrastthatindicatesthetest-statisticwewanttoconsider,where⌘jisdeﬁneddependingontheproblemandthejthselectedcom-ponentintheobservedactiveset.Example1Inthecaseoftestingthefeatureselectedbyvanillalasso(Tibshirani,1996),D=Ip2Rp⇥p,whichistheidentitymatrix.Thetest-statistic⌘>jY=ˆ jrepresentsthecoe cientofthejthselectedfeature(Leeetal.,2016)where⌘jisdeﬁnedas⌘j=XMobs⇣X>MobsXMobs⌘ 1ej,(4)withej2R|Mobs|beingabasisvectorwitha1atthejthposition.Thisformoftest-statisticcanalsobeappliedtotestthefeaturesselectedbyotherregressionmethodssuchaselasticnet(ZouandHastie,2005),huberregression(Huber,1992),ornon-negativeleastsquares.Example2Inthecontextofchangepoint(CP)detectionusingfusedlasso,thematrixD2R(p 1)⇥pisgivenbyD=0BB@ 110···000 11···00···000··· 111CCA.5MorePowerfulSelectiveInferenceforGeneralizedLassoWeobtain(12)bydecomposing⇠=⇠+ ⇠ with⇠+,⇠  0.Here,thekeypointisthatthecomponent||⇠||1in(13)canbewrittenas||⇠||1=Pj2[m](⇠+j+⇠ j)becauseatleastoneof⇠+jand⇠ j,j2[m],mustbesetto0attheoptimalsolution.Inotherwords,ˆ⇠+j>0)ˆ⇠ j=0,andviceversa.Theideaofdecomposing⇠=⇠+ ⇠ areoftenemployedinreformulating`1-normof⇠.3.2ConditionalDataSpaceCharacterizationLetusdeﬁnethesetofy2RnwhichsatisﬁestheconditionsinEquation(8)asY={y2Rn|A(y)=A(yobs),q(y)=q(yobs)}.(14)Accordingtothesecondcondition,thedatainYisrestrictedtoaline(seeSec6inLiuetal.(2018),andFithianetal.(2014)).Therefore,thesetYcanbere-written,usingascalarparameterz2R,asY={y(z)=a+bz|z2Z},(15)wherea=q(yobs),b=⌃⌘j(⌘>j⌃⌘j) 1,andZ=nz2R|A(y(z))=A(yobs)o.(16)Now,letusconsiderarandomvariableZ2Randitsobservationzobs2R,whichsatisfyY=a+bZandyobs=a+bzobs.Theconditionalinferencein(8)isre-writtenastheproblemofcharacterizingthesamplingdistributionofZ|{Z2Z}.(17)SinceZ⇠N(0,⌘>j⌃⌘j)underthenullhypothesis,Z|Z2ZfollowsatruncatedNormaldistribution.OncethetruncationregionZisidentiﬁed,thepivotalquantityinEquation(9)isequaltoFZ0,⌘>j⌃⌘j(Z),andcanbeeasilyobtained.Thus,theremainingtaskistocharacterizeZ.CharacterizationoftruncationregionZ.Letusintroducetheoptimizationproblem(12)withparametrizedresponsevectory(z)=a+bz(a,baredeﬁnedin(15))forz2Rasˆr(z)=argminr12r>Pr+(q0+q1z)>rs.t.Grh0+h1z,(18)wherer=( ,⇠+,⇠ )>2Rp+2m,q0=  X>a, 1m, 1m >2Rp+2m,q1=  X>b,0,0 2Rp+2m,P2R(p+2m)⇥(p+2m)andG2R4m⇥(p+2m),h0=h1=04m,P=0@X>X000000001A,G=0@ DD00Im Im Im0 ImIm0 Im1A>.93.1 QP for Generalized Lasso

We demonstrate that the generalized lasso problem can be reformulated as a QP problem.

Lemma 1. We denote ξ = Dβ, and the generalized lasso in (2) can be rewritten as

1
2 ||

y

Xβ

2
2 + λ
||

ξ

||

1
||

−

subject to ξ = Dβ.

(11)

(cid:16)
By decomposing ξ = ξ+

= arg min

ˆβ, ˆξ
(cid:17)
ξ−, ξ+, ξ−

β∈Rp,ξ∈Rm

−

0, the generalized lasso problem can be formulated as the following

≥

QP problem:

ˆβ, ˆξ+, ˆξ−
(cid:16)

(cid:17)

= arg min
β,ξ+,ξ−

1
2

(cid:62)

β

X (cid:62)X 0



ξ+



ξ−


















0

0

0

0

β

0


0

β



ξ+



ξ−

0
















+ 





0p

X (cid:62)y

(cid:62)

β

1m

1m

λ 




β

0m

0m

−































ξ+



ξ−











(12)

s.t

0

Im

(cid:16)

Im

−

(cid:17)



ξ+



=

D 0

0



ξ+



, ξ+

0, ξ−

0,

≥

≥

ξ−











(cid:16)

(cid:17)

ξ−











Rm are vectors in which all elements are set to 1 and 0, respectively, and

where 1m

Rm and 0m
Rm×m is an identity matrix.

∈

∈

Im

∈

Proof. The optimization problem in (11) can be rewritten as follows:

= arg min

β(cid:62)X (cid:62)Xβ

1
2

β,ξ

ˆβ, ˆξ
(cid:17)

(X (cid:62)y)(cid:62)β + λ
||

ξ

1
||

−

s.t ξ = Dβ.

(13)

(cid:16)
We obtain (12) by decomposing ξ = ξ+

0.
≥
j + ξ−
j∈[m](ξ+
[m], must be set to zero in the optimal solution. That is, ˆξ+
j > 0

−
1 in (13) can be written as

ξ− with ξ+, ξ−

1 =

(cid:80)

||

||

||

||

ξ

ξ

component

j

∈

of decomposing ξ = ξ+

ξ− is often employed to reformulate the (cid:96)1-norm of ξ.

−

In this case, the key point is that the
j and ξ−
j ) because at least one of ξ+
j ,
ˆξ−
j = 0, and vice versa. The concept
(cid:4)

⇒

3.2 Conditional Data Space Characterization

We deﬁne the set of y

∈

Rn that satisﬁes the conditions in Equation (8):

=

y
{

∈

Y

n

R

(y) =

| A

(yobs), q(y) = q(yobs)
.
}

A

(14)

According to the second condition, the data in

Y

are restricted to a line (see Section 6 in Liu et al. [2018]

and Fithian et al. [2014]). Therefore, the set

can be rewritten using the scalar parameter z

R, as follows:

∈

Y

where a = q(yobs), b = Σηj(η(cid:62)

j Σηj)−1, and

=

{

Y

y(z) = a + bz

z

|

,

∈ Z}

=

z

R

∈

Z

(y(z)) =

| A

(yobs)

A

.

(cid:8)

9

(cid:9)

(15)

(16)

Next, we consider a random variable Z

R, which satisﬁes Y = a + bZ and
yobs = a + bzobs. The conditional inference in (8) is rewritten as the problem of characterizing the sampling

R and its observation zobs

∈

∈

distribution of

Z

Z

| {

.

∈ Z}

(17)

As Z

∼

N(0, η(cid:62)

j Σηj) under the null hypothesis, Z

Z

|

∈ Z

follows a truncated normal distribution. Once

the truncation region

Z

has been identiﬁed, the pivotal quantity in Equation (9) is equal to F Z

0,η(cid:62)

j Σηj

(Z),

and it can be obtained easily. Thus, the remaining task is the characterization of

.

Z

Characterization of truncation region

Z

. We introduce the optimization problem (12) with the pa-

rameterized response vectors y(z) = a + bz (a, b that are deﬁned in (15)) for z

R as follows:

∈

ˆr(z) = arg min

r

1
2

r(cid:62)P r + (q0 + q1z)(cid:62)r

s.t. Gr

≤

h0 + h1z,

(18)

where r = (β, ξ+, ξ−)

(cid:62)

∈
R(p+2m)×(p+2m) and G

P

∈

∈

Rp+2m, q0 =

X (cid:62)a, λ1m, λ1m

−
R4m×(p+2m), h0 = h1 = 04m,
(cid:0)

(cid:62)

(cid:1)

∈

Rp+2m, q1 =

X (cid:62)b, 0, 0

Rp+2m,

∈

(cid:1)

−

(cid:0)

X (cid:62)X 0

0

0

0

0

0

0

0

P = 












, G = 





D

−
Im

Im

−

D

0

Im

−
Im

Im

−
0

(cid:62)



.

0

0

Im

−






Let ˆu(z) be the vector of optimal Lagrange multipliers and row(G) be the number of rows in matrix G.

The KKT conditions of (18) are written as

P ˆr(z) + q0 + q1z + G(cid:62) ˆu(z) = 0,

Gˆr(z)

h0

h1z

0,

≤

−
h1z)i = 0,

−
h0

−

−

ˆui(z)(Gˆr(z)

ˆui(z)

0,

≥

[row(G)],

[row(G)].

i
∀
i
∀

∈

∈

(19)

To construct the truncation region

ˆξ−(z) in (18), and 2) identify the set of intervals of z on which
ˆξ+(z)
it is diﬃcult to compute ˆξ+(z) and ˆξ−(z) for inﬁnitely many values of z
R. We demonstrate that the
paths of ˆξ+(z) and ˆξ−(z) can be computed within ﬁnite operations by introducing parametric quadratic

(y(z)) =

in Equation (16), we must 1) compute the entire path of ˆξ(z) =
(yobs). However,

A

A

−

Z

∈

programming.

3.3 Piecewise Linear Homotopy

In this section, we demonstrate that ˆr(z) in (18) is a piecewise linear function of z, which also indicates that
ˆξ+(z) and ˆξ−(z) are piecewise linear functions of z.

10

Lemma 2. We denote

z =

[row(G)] : ˆui(z) > 0

I

i

{

∈

c
z = [row(G)]

,
}

I

z, and GIz as the rows of matrix

\ I

G in a set

I

z. Consider two real values z and z(cid:48) (z < z(cid:48)). If

z =

I

I

z(cid:48), we obtain

(20)

(21)

(22)

(23)

(24)

(25)

ˆr(z(cid:48))

ˆuIz (z(cid:48))

−

ˆr(z) = ψ(z)

−
ˆuIz (z) = γ(z)

(z(cid:48)

(z(cid:48)

z),

z),

−

−

×

×

ψ(z)
γ(z)






=

P



GIz



−1

G(cid:62)
Iz
0 


q1

−
h1
Iz





.




where ψ(z)

Rrow(P ), γ(z)

R|Iz|,

∈

∈

Proof. From the KKT conditions in (19), we obtain

P ˆr(z) + q0 + q1z + G(cid:62) ˆu(z) = 0,

(Gˆr(z)

(Gˆr(z)

h0

h0

−

−

−

−

h1z)i = 0,

h1z)i

0,

≤

i
∀
i
∀

z,

∈ I

c
z.

∈ I

According to (22), we obtain the following linear system:

P



GIz



G(cid:62)
Iz
0 


=

=

ˆr(z)
ˆuIz (z)



G(cid:62)
Iz
0 


GIz

P





q0

−
h0
Iz



+



q1

−
h1
Iz



z




−1


q0

−
h0
Iz







+


P



GIz







−1

G(cid:62)
Iz
0 


q1

−
h1
Iz





z.





ˆr(z)
ˆuIz (z)


Similarly, for z(cid:48), we obtain

⇔ 

ˆr(z(cid:48))
ˆuIz(cid:48) (z(cid:48))




=

P



GIz(cid:48)

−1

−1

P

+

q0

−
h0

G(cid:62)
Iz(cid:48)
0 


G(cid:62)
Iz(cid:48)
0 


z(cid:48), we can express the following:

GIz(cid:48)

Iz(cid:48)











q1

−
h1

Iz(cid:48)





z(cid:48).






By subtracting (23) from (24) and

z =

I

I

ˆr(z(cid:48))
ˆuIz (z(cid:48))



G(cid:62)
Iz
0 


P

− 

ˆr(z)
ˆuIz (z)


−1

q1

−
h1
Iz









We denote

ψ(z)
γ(z)
achieve the results in Lemma 2.


GIz

=









=

P



GIz



−1

G(cid:62)
Iz
0 


q1

−
h1
Iz



(z(cid:48)

z).

−



×





with ψ(z)

∈

Rrow(P ) and γ(z)

∈

R|Iz|, and we subsequently

For simplicity, we assume that the generalized lasso solution is unique for any y(z), z

in the generalized lasso problem has been studied in Ali and Tibshirani [2019].

In this case, it can be

guaranteed that the matrix inverse in Equation (25) always exists.

If this is not the case, we can use

parametric quadratic programming for the degenerate cases in Best [1996].

11

(cid:4)

R. The uniqueness

∈

Computation of breakpoint. According to Lemma 2, the solution ˆr(z) is a linear function of z until z

reaches a breakpoint, at which one component of ˆu(z) enters or leaves the set

z. At this point, we discuss

I

the identiﬁcation of the breakpoint.

Lemma 3. Consider a real value z. Subsequently,

z =

I

I

z(cid:48) for any real value z(cid:48) in the interval [z, z + tz),

where z + tz is the value of the breakpoint:

z, t2
t1
,
tz = min
z}
{

(GIc

z ˆr(z)
(GIc

h0
Ic
−
z −
h1
z ψ(z)
Ic
z

h1
Ic
z
)j

−

z)j

(cid:33)++

and

t2
z = min

j∈Iz (cid:18)

ˆuj(z)
γj(z)

−

.

(cid:19)++

t1
z = min
j∈Ic

z (cid:32)−

(26)

(27)

In this case, for any a

∈

R, (a)++ = a if a > 0, and (a)++ =

otherwise.

∞

Proof. We ﬁrst illustrate how to derive t1

z. According to (20), we obtain

ˆr(z(cid:48)) = ˆr(z) + ψ(z)

(z(cid:48)

z).

−

×

Thereafter, we need to guarantee

GIc

z (ˆr(z) + ψ(z)

⇔

(z(cid:48)

×

z))

−

−

⇔

GIc

z ˆr(z(cid:48))
h1
Ic
z ×

−
(z(cid:48)

h0
Ic
z −
z)

−

h0
Ic
z −
GIc

z ψ(z)

(cid:16)

h1
Ic
z

−

×

(cid:17)

h1
Ic
z

z(cid:48)

−
(z(cid:48)

h1
Ic
z

z

z)

−

0

0

≤

≤

(GIc

z ˆr(z)

≤ −

h0
Ic
z −

−

h1
Ic
z

z).

(28)

The right-hand side of (28) is positive because GIc

z ˆr(z)

h0
Ic
z −

−

h1
Ic
z

z

≤

0. Therefore, to satisfy Equation

(28),

z(cid:48)

z

−

≤

min
j∈Ic

z (cid:32)−

(GIc

z ˆr(z)
(GIc

h0
Ic
z −
−
h1
z ψ(z)
Ic
z

h1
Ic
z
)j

−

z)j

= t1
z.

(cid:33)++

Next, we explain how to derive t2

z. According to (21), we obtain

ˆuIz (z(cid:48)) = ˆuIz (z) + γ(z)

(z(cid:48)

z).

−

×

We need to guarantee

ˆuIz (z(cid:48)) > 0

⇔

ˆuIz (z) + γ(z)

(z(cid:48)

×

−

z) > 0.

(29)

Therefore, to satisfy Equation (29),

z(cid:48)

−

z < min

j∈Iz (cid:18)

ˆuj(z)
γj(z)

−

(cid:19)++

= t2
z.

z, t2
t1
Finally, using tz = min
z}
{

, we obtain the interval in which

z(cid:48) =

I

z for any z(cid:48)
I

∈

[z, z + tz).

(cid:4)

12

Algorithm 1 parametric SI
Input: X, yobs, λ, D, [zmin, zmax]

1: Obtain observed active set

Mobs =

(yobs) for data (X, yobs)

A

2: for each selected j

∈ Mobs do

3:

4:

5:

6:

Compute ηj , and subsequently calculate a and b based on ηj and yobs

Equation (15)

←

(y(z))

A

←

compute solution path (X, λ, D, a, b, [zmin, zmax])

Truncation region

Z ← {

z :

A

(y(z)) =

Mobs}

P selective

j

←

Equation (10) (and/or selective conﬁdence interval)

7: end for

Output:

P selective
{

j

}j

∈Mobs (and/or selective conﬁdence intervals)

Algorithm 2 compute solution path
Input: X, λ, D, a, b, [zmin, zmax]

1: Initialization: k = 1, zk = zmin,

= zk

T

2: while zk < zmax do

3:

4:

5:

tzk ,

M

zk ←
zk+1 = zk + tzk ,

k = k + 1

compute step size(X, zk, a, b, λ, D)

=

T ∪ {

zk+1}

T

(zk+1 is the value of the next breakpoint)

6: end while

Output:

{M

zk }k

∈

[

|T |−

1]

3.4 Algorithm

In this section, we present the detailed algorithm of the proposed PP-based SI method. In Algorithm

1, to obtain the active set, we simply apply generalized lasso to the data (X, yobs), and we obtain

obs.

M

Thereafter, we conduct SI for each observed active set. For every j

obs, we ﬁrst obtain the direction of

∈ M

interest ηj. The main task is to compute the solution path of ˆr(z) in Equation (18) for the parameterized

response vector y(z), where the parameterized solution ˆr(z) varies for diﬀerent j

obs because the

∈ M

direction of interest ηj is dependent on j. This task can be achieved by Algorithm 2. Finally, after obtaining

the path, we can easily determine the truncation region

, which is used to compute the selective p-value

Z

or selective conﬁdence interval.

In Algorithm 2, a sequence of breakpoints is computed individually. The algorithm is initialized at

zk = zmin, k = 1. At each zk, the task is to determine the next breakpoint zk+1. This task can be performed

by computing the step size in Algorithm 3. This step is repeated until zk > zmax. By identifying all of the

13

Algorithm 3 compute step size
Input: X, z, a, b, λ, D

1: y(z) = a + bz
2: Compute ˆr(z) for data (X, y(z)) and calculate ˆξ(z) = ˆξ+(z)

ˆξ−(z) based on ˆr(z)

−

3: Obtain

z =

M

(y(z)) =

A

j : ˆξj (z)
{

= 0

}

4: Compute tz

Lemma 3

←

Output: tz,

z

M

breakpoints

zt

}t∈[|T |], the entire path of

{

z for z

M

∈

R is expressed as

z =

M

(y(z)) =

A

(y(z1))

A

(y(z2))
...
(y(z|T |−1))

A

A

if z

if z

if z

∈

∈

∈

[z1, z2],

[z2, z3],

[z|T |−1, z|T |].






Selection of [zmin, zmax]. Under normality, very positive and negative values of z do not aﬀect the in-

ference. Therefore, it is reasonable to consider a range of values, such as [

20σ, 20σ] [Liu et al., 2018] or

−

j yobs
η(cid:62)

20σ,

j yobs
η(cid:62)

+ 20σ

−|
distribution of the test statistic.
(cid:2)

| −

|

|

[Sugiyama et al., 2020], where σ is the standard deviation of the sampling

(cid:3)

In Line 2 of Algorithm 3, ˆr(z) can be computed based on the KKT conditions. However, it is well known

that numerical issues will arise. Therefore, in our experiments, we modify the algorithm slightly to overcome

these numerical problems. In particular, we ﬁrst replace Line 1 in Algorithm 3 with y(z) = a + b(z + ∆z),

where ∆z is a small value such that zk + ∆z < zk+1 for all k

[
|T | −

∈

1]. Subsequently, at Line 2 of Algorithm

3, we simply obtain ˆr(z) by applying the QP solver to y(z). We can conﬁrm whether ∆z is suﬃciently small

by verifying whether exactly one component in the vector of the optimal Lagrange multipliers ˆu(z) has

already entered or left the set

I

∆z = 0.0001.

z. This condition is satisﬁed in all of the experiments by simply setting

The complexity of Algorithm 1 is dependent on the number of breakpoints.

In the literature on PP,

the worst-case complexity increases exponentially with the problem size [Ritter, 1984, Allgower and George,

1993, Gal, 1995, Best, 1996, Mairal and Yu, 2012]. However, in practice, it has been reported that the

number of breakpoints is approximately linear with the problem size and it does not actually increase as

much as in the theoretical worst case. This has also been noted in regularization path studies [Osborne

et al., 2000, Efron and Tibshirani, 2004, Hastie et al., 2004, Mairal and Yu, 2012]. In fact, in all of the
experiments in §6, the number of breakpoints is within a reasonable size and the computational cost is not

a major problem of the proposed method.

14

(cid:54)
4 Generality of Proposed Method

Although we only focused on generalized lasso in §3, the parametric QP formulation in (18) is more general
and the forms of matrices P and G as well as vectors q0, q1, h0, and h1 can be changed depending on the
problem. That is, the method proposed method in §3 is ﬂexible and can be applied to any problem that can

be converted into a parametric QP in the form of (18). In this section, we demonstrate the extensions of the

proposed method for testing the statistical signiﬁcance of the features that are selected by various feature

selection algorithms.

When applying a feature selection algorithm

A

set can be deﬁned as follows:

to the observed response vector yobs, the observed active

obs =

M

(yobs) =

A

j : ˆβj
{

.
= 0
}

In this setting, D = Ip. To test the selected features in

obs, the conditional inference is the same as that

M

deﬁned in (8) and the characterization of the truncation region
remaining task is to compute the solution path ˆβ(z) for z
R and to identify the intervals of z in which
we obtain the same active set as yobs. In the following sections, we present the parametric QP formulations

is the same as (16). To identify

, the

Z

Z

∈

for vanilla lasso, elastic net, non-negative least squares, and Huber regression. As all of these regression
problems can be converted into the form of (18), the path of ˆβ(z) can be computed within ﬁnite operations
by using PP, as demonstrated in §3.3.

Parametric QP for vanilla lasso. Vanilla lasso with a parameterized response vector y(z) for z

deﬁned as

ˆβlasso(z) = arg min

β∈Rp

1
2 ||

y(z)

Xβ

2
2 + λ
||

β

||

1.

||

−

R is

∈

(30)

Lemma 4. By decomposing β = β+

β−, β+, β−

−

≥

following parametric QP:

0p, the lasso problem in (30) can be solved by the

lasso(z), ˆβ−
ˆβ+

lasso(z)

(cid:16)

(cid:17)

= arg min
β+,β−∈Rp

1
2 



β+
β−


(cid:62)





X (cid:62)X

X (cid:62)X
−
X (cid:62)X X (cid:62)X 


(cid:62)

−



β+
β−


X (cid:62)y(z)
X (cid:62)y(z)


−






β+
β−






λ12p

+



− 

0p, β−

0p.


≥

s.t. β+


≥

Xβ

2
2 + λ
||

β

||

1
||

−

Proof. According to (30), we obtain

ˆβlasso(z) = arg min

β∈Rp

= arg min

β∈Rp

y(z)

1
2 ||
1
2

(cid:0)

y(z)(cid:62)y(z)

−

β(cid:62)X (cid:62)y(z)

−

y(z)(cid:62)Xβ + β(cid:62)X (cid:62)Xβ

+ λ

β

||

1.

||

(cid:1)

(31)

15

(cid:54)
Similar to the proof of Lemma 1, as the component 1

2 y(z)(cid:62)y(z) does not aﬀect the optimal solution ˆβlasso(z),

we can rewrite (31) as

ˆβlasso(z) = arg min

β∈Rp

1
2

β(cid:62)X (cid:62)Xβ

(X (cid:62)y(z))(cid:62)β + λ
||

β

1.

||

−

(32)

Finally, by decomposing β = β+

β−, β+, β−

−

0p and

β

||

1 =

||

≥

Lemma 4.

(cid:80)

j∈[p](β+

j + β−

j ), we obtain the result in
(cid:4)

In our preliminary conference paper [Le Duy and Takeuchi, 2021], we initially presented the idea of

introducing PP for vanilla lasso in a slightly diﬀerent (but essentially the same) manner.

Parametric QP for elastic net. The elastic net with a parameterized response vector y(z) for z

deﬁned as

ˆβelastic(z) = arg min

β∈Rp

1
2n ||

y(z)

Xβ

2
2 + λ
||
||

β

1 +

||

−

1
2

ζ

β

||

2
2.
||

R is

∈

(33)

Lemma 5. By decomposing β = β+

β−, β+, β−

−

≥

the following parametric QP:

0p, the elastic net problem in (33) can be solved using

elastic(z), ˆβ−
ˆβ+

elastic(z)

(cid:16)

(cid:17)

= arg min
β+,β−∈Rp

(cid:62)





−
(cid:62)

β+
β−

β+
β−

1
n 

1
2 



−

λ12p

1
2n 



+ ζ

·

+



X (cid:62)X

X (cid:62)X
−
X (cid:62)X X (cid:62)X 




β+
β−


Ip

Ip
−
Ip 


(cid:62)





Ip

−

X (cid:62)y(z)
X (cid:62)y(z)







β+
β−

β+
β−






s.t. β+


≥

0p, β−

≥



−
0p.

Proof. The proof of Lemma 5 is similar to the proof of Lemma 4.

(cid:4)

Parametric QP for non-negative least squares. The non-negative least squares problem with a

parametrized response vector y(z) for z

R is deﬁned as

∈

ˆβnon−negative(z) = arg min

β∈Rp

1
2 ||

y(z)

Xβ

2
2
||

−

s.t β

0.

≥

The above problem can be formulated as the following parametric QP:

ˆβnon−negative(z) = arg min

β∈Rp

β(cid:62)X (cid:62)Xβ

1
2

X (cid:62)y(z)

(cid:62)

β s.t β

0.

≥

(cid:1)

−

(cid:0)

16

Parametric QP for Huber regression with (cid:96)1 penalty. The Huber regression with the (cid:96)1 penalty for

a parameterized response vector y(z) is formulated as

ˆβhuber(z) = arg min

β∈Rp

n

i=1
(cid:88)

Lδ(yi(z)

−

with

x(cid:62)

i β) + λ
||

β

1,

||

(34)

Lδ(e) =

1

2 e2
e
δ(
|




1
2 δ)

| −

δ,

if

e

|

| ≤
otherwise,

where δ > 0 is also a predetermined tuning parameter. Let e = y(z)



vectors in which the ith element φi and νi are respectively deﬁned as

Xβ, φ

−

∈

Rn, and ν

Rn be the

∈

φi = min

{|

ei

, δ

,

}

|

νi = max

{|

ei

,

δ, 0
}

| −

where ei is the ith element of e. Obviously,
β+, β−

|
0p, the problem in (34) can be solved by the following parametric QP:

= φi + νi. Subsequently, by decomposing β = β+

ei

|

≥

β−,

−

ˆφ(z), ˆν(z), ˆβ+

huber(z), ˆβ−

huber(z)

(cid:16)

(cid:17)

= arg min
φ,ν,β+,β−

1
2

φ(cid:62)φ + δ

1(cid:62)
n ν +

·

j + β−
β+

j

j
(cid:88)
X

(cid:0)
β+

(cid:0)
0n,

≥

β−

−

(cid:1)

≤

(cid:1)

φ + ν,

s.t

−
0n

φ

ν

y(z)

−
u

≤
δ

−
1n, v

≤

β+

≥

≤
·
0p, β−

0p.

≥

The lasso-like formulation of the Huber regression has been discussed in She and Owen [2011]. Condi-

tional SI for outliers was studied in Chen and Bien [2019], and we recently investigated its PP version in

Tsukurimichi et al. [2021].

5 Characterization of CV-Based Tuning Parameter Selection Event

Various ML tasks involve careful tuning of a regularization parameter λ that controls the balance between

an empirical loss term and a regularization term; for example, this is commonly achieved by CV. However,

the majority of the current SI studies have assumed a pre-speciﬁed λ and have ignored the fact that λ

is selected based on the data, because it is diﬃcult to characterize the CV selection event. Loftus [2015]

and Markovic et al. [2017] proposed solutions to incorporate CV events. However, the former work required

additional conditioning on all intermediate models, which led to a loss of power, whereas the latter considered

a randomization version of CV instead of vanilla CV.

In this section, we introduce a new means of characterizing the minimal selection event in which λ is

selected based on the data; for example, via CV 4. For notational simplicity, we consider the case in which

4We note that the following discussion is only applicable when the number of features p is independent of n.

17

the data are divided into training and validation sets, and the latter is used for selecting λ. The following

discussion can easily be extended to CV scenarios. We rewrite the observed data as follows:

X, yobs
{

}

=

(Xtrain Xval)(cid:62)

R

∈

n×p, (yobs

train yobs

val )(cid:62)

n

R

.

∈

Given a set of regularization parameter candidates Λ, the process of selecting λ is as follows:

(cid:8)

(cid:9)

1. For each λ

∈

Λ, we ﬁrst obtain ˆβλ using the training data

ˆβλ

arg min
β

∈

1
2 (cid:107)

yobs
train −

Xtrainβ

2
2 + λ
(cid:107)

Dβ
(cid:107)

1.
(cid:107)

Thereafter, the validation error is deﬁned as

Eλ =

1
2 (cid:107)

yobs
val −

Xval ˆβλ

2
2.

(cid:107)

2. We select λobs = λ

∈

Λ, which has the corresponding smallest validation error Eλ.

The selection event of the above validation process is deﬁned as

(Y ) =

{V

(yobs)

,
}

V

(35)

Λ is the event that λobs is selected when validation is performed on yobs.

where

(yobs) = λobs

V

∈

After selecting λobs, we can obtain the observed active set

obs by applying generalized lasso on yobs
with the selected λobs, which can be deﬁned as in (3). However, to conduct a statistical test for each element

M

in

M

obs, the conditional inference will be diﬀerent from (8), because we must incorporate the selection event

of the validation process. Therefore, for each j

obs, we consider the following conditional inference:

∈ M

η(cid:62)
j Y

(Y ) =

| {A

(yobs),

A

(Y ) =

V

V

(yobs), q(Y ) = q(yobs)

.
}

(36)

According to the third condition, the data are restricted on the line, as discussed in §3.2. Therefore, the

conditional data space in (36) can be rewritten as:

where

CV =

Y

y(z) = a + bz
{

|

z

CV

∈ Z

,
}

CV =

z
{

∈

R

Z

(y(z)) =

| A

(yobs),

A

V

(y(z)) =

(yobs)
}

.

V

The remaining task to conduct the inference is to identify

CV. We can decompose

Z

CV into two separate

Z

sets

CV =

2,

1

Z

∩ Z

Z

where

1 =

Z

z

{

∈

R

| A

(y(z)) =

(yobs)
}

A

and

2 =

Z

z

{

∈

R

| V

(y(z)) =

(yobs)
}

V

. The set

Z

1 can easily be

constructed using the method proposed in the previous sections. The remaining challenge is to identify

2.

Z

18

Algorithm 4 SI with K fold cross validation
Input: X, yobs, Λ, D, K, [zmin, zmax]

1: Conduct K-fold CV to select λobs

2: Obtain

(yobs) for data (X, yobs) with the selected λobs

3: for each j

Mobs =

A
∈ Mobs do

4:

5:

6:

7:

8:

9:

Compute ηj , and subsequently calculate a and b based on ηj and yobs

Equation (15)

←

Obtain

A

(y(z)) using Algorithm 2

←

compute solution path (X, λobs, D, a, b, [zmin, zmax])

Mobs}
2(X, a, b, Λ, D, K)

1

Z

← {

z :

A

(y(z)) =

2

compute

Z

Z
←
ZCV =
Z
Compute P selective

∩ Z

1

2

j

(characterize model selection event)

(characterize CV selection event)

in Equation (10) with truncation region

ZCV

10: end for

Output:

P selective
{

j

}j

∈Mobs

To construct

Z

2, it is necessary to identify the intervals of z on which λobs has the smallest validation

error. That is, we can redeﬁne

2 =

z
{

R

|

∈

Z

Eλobs(z)

≤

Eλ(z) for any λ

Λ

,
}

∈

where

Eλ(z) =

ˆβλ(z)

arg min
β∈Rp

∈

1
2 (cid:107)
1
2 (cid:107)

yval(z)

ytrain(z)

−

2
2, ,

Xval ˆβλ(z)
(cid:107)
2
2 + λ
(cid:107)

Xtrainβ

−

Dβ
(cid:107)

1.
(cid:107)

For each λ

∈

Λ, although it appears to be intractable to compute Eλ(z) for inﬁnitely many values of z

the task can be completed within ﬁnite operations using PP.

(37)

(38)

R,

∈

As demonstrated in §3.3, the optimal solution ˆβλ(z) in (38) is a piecewise linear function of z. That is,

λ(z

λ(z

ˆβλ(z1) + s1
ˆβλ(z2) + s2
...

z1)

z2)

−

−

if z

if z

ˆβλ(z|Tλ|−1) + s|Tλ|−1

λ

(z

z|Tλ|−1)

if z

−

[z1, z2],

[z2, z3],

[z|Tλ|−1, z|Tλ|],

∈

∈

∈

(39)

ˆβλ(z) =






is the slope vector, the elements of which are dependent on ψ(zk∈|Tλ|) in (20). The subscript

where sk∈|Tλ|
λ
λ in ˆβλ(z), sk
λ,
y(z) = (ytrain(z), yval(z))(cid:62), the vectors a and b can be rewritten as follows:

T

λ indicates that these components are dependent on λ. Moreover, because we decompose

a = (atrain aval)(cid:62) ,

b = (btrain bval)(cid:62) .

Therefore, we can write

yval(z) = aval + bvalz.

(40)

19

Algorithm 5 compute
Input: X, a, b, Λ, D, K

2

Z

1: for λ

∈
for k

Λ do

1 to K do

←

2:

3:

4:

5:

6:

Compute ˆβk

λ(z) as in (38) for fold k

Compute validation error Ek

λ(z) as in (37) for fold k

end for

Compute mean error ¯Eλ(z) = 1
K

(cid:80)K

k=1 Ek

λ(z) among K folds for current λ candidate

7: end for

8:

2 =

Z
Output:

z
{

R

|

∈

¯Eλobs (z)

≤

¯Eλ(z) for any λ

Λ

}

∈

2

Z

According to the piecewise linearity of ˆβλ(z) in (39) and the linearity of yval(z) in (40), the validation error

Eλ(z) in (37) is a piecewise quadratic function of z, which can be expressed as follows:

(1/2)

(aval

||

(1/2)

(aval

||

−

−

(1/2)

(aval

||

−

Xval ˆβλ(z1) + Xvals1
Xval ˆβλ(z2) + Xvals2
...

λz1) + (bval

λz2) + (bval

Xvals1
Xvals2

λ)z

λ)z

2
2
||
2
2
||

if z

if z

∈

∈

[z1, z2],

[z2, z3],

−

−

Xval ˆβλ(z|Tλ|−1) + Xvals|Tλ|−1

λ

z|Tλ|−1).

+ (bval

−

Xvals|Tλ|−1
λ

)z

2
2
||

if z

∈

[z|Tλ|−1, z|Tλ|].

Eλ(z) =






At this point, for each λ

Λ, we have a corresponding validation error Eλ(z), which is a piecewise quadratic

∈
function of z. Finally,
2 can be identiﬁed by determining the intervals of z in which the validation error
Eλobs(z) corresponding to λobs is the minimum among a set of piecewise quadratic functions. The procedure

Z

for the K-fold CV case is presented in Algorithm 4.

6 Experiments

In this section, we discuss the performance evaluation of the proposed method. First, the experimental
setup is presented in §6.1. Thereafter, the results on synthetic and real data are outlined in §6.2 and §6.3,

respectively.

6.1 Experimental Setup

We conducted experiments on fused lasso, which is one of the most commonly studied cases of generalized

lasso, as well as on feature selection methods including vanilla lasso, elastic net, non-negative least squares,

and Huber regression + (cid:96)1 penalty. We executed the code on an Intel(R) Xeon(R) CPU E5-2687W v4 @

3.00 GHz.

20

Methods for comparison. We present the false positive rate (FPR), true positive rate (TPR), and

conﬁdence interval (CI) for the following conditional inferences:

• Proposed method: Conditional inference without extra conditioning, as deﬁned in (8), which was the

main focus of this study.

• Over-conditioning (OC): Conditional inference with extra conditioning. Regarding fused lasso, OC is

the method that was proposed in Hyun et al. [2018a]. Regarding vanilla lasso, elastic net, and Huber

regression + (cid:96)1 penalty, OC is the polytope-based SI that was proposed in Lee et al. [2016].

• Data splitting (DS) [Cox, 1975]: DS is the commonly used procedure for selection bias correction. In

this approach, the data are randomly divided into two halves: one half is used for model selection and

the other half is used for inference.

Synthetic data generation for fused lasso. We set X = In, and the matrix D

R(n−1)×n was deﬁned

∈

as

1

−
0

1

1

−

0

0











0

1

0

0

0

· · ·

· · ·

· · ·

1
· · · −

.

0

0




1




Regarding the FPR experiments, we generated 100 null data y = (y1, ..., yn)(cid:62), where yi∈[n] ∼
. To test the TPR, we generated y = (y1, ..., yn)(cid:62) with n = 60, in which
each n
}

70, 80, 90, 100

∈ {

N(0, 1) for

yi

∼

N(µi, 1), µi = 


0,

if 1

i

≤

0 + ∆µ,

if 21

≤

≤
i

≤

40.

20 or 41

i

≤

≤

60,

The signiﬁcance level α was set to 0.05. We used Bonferroni correction to account for the multiplicity in all



of the experiments. If v selected features (hypotheses) are tested simultaneously, Bonferroni correction tests

each individual hypothesis at α∗ = α/v. We ran 100 trials for each ∆µ

1, 2, 3, 4
}

∈ {

and we repeated the

experiment 10 times.

Synthetic data generation for feature selection methods. We generated n outcomes as yi = x(cid:62)

i β+εi,

i = 1, ..., n, where xi

∼

N(0, Ip), in which p = 5, and εi

∼

N(0, 1). We set the regularization parameter λ = 1

and signiﬁcance level α = 0.05. Bonferroni correction was also applied. For the FPR experiments, all of the

elements of β were set to 0. For the TPR experiments, the ﬁrst two elements of β were set to 0.25. We ran

100 trials for each n

50, 100, 150, 200

}
we set n = 100, p = 5 and ran 100 trials.

∈ {

and we repeated this experiment 10 times. For the CI experiments,

21

(a) FPR

(b) TPR

Figure 2: Results of FPR and TPR for fused lasso.

(a) FPR

(b) TPR

(c) CI

Figure 3: Results of FPR, TPR, and CI for vanilla lasso.

(a) FPR

(b) TPR

(c) CI

Figure 4: Results of FPR, TPR, and CI for elastic net.

Deﬁnition of TPR.

In SI, statistical testing is only conducted when at least one hypothesis is discovered

by the algorithm. Therefore, the deﬁnition of the TPR, which is also known as the conditional power, is as

follows:

TPR =

# correctly detected & rejected
# correctly detected

.

22

(a) FPR

(b) TPR

(c) CI

Figure 5: Results of FPR, TPR, and CI for non-negative least squares.

(a) FPR

(b) TPR

(c) CI

Figure 6: FPR, TPR, and CI results for Huber regression with (cid:96)1 penalty.

In the case of fused lasso, a detection is considered as correct if it is within L =

2 of the true CP locations.

±

This is because it is often diﬃcult to identify the exact CPs accurately in the presence of noise. Many

existing studies considered a detection to be correct if it was within L positions of the true CP locations

[Truong et al., 2020]. In the case of feature selection, # correctly detected indicates the number of truly

positive features that are selected by the algorithm (e.g., lasso), whereas # rejected indicates the number of

truly positive features for which the null hypothesis is rejected by the SI.

6.2 Numerical Results

FPR, TPR, and CI results. The fused lasso results are presented in Figure 2. The results for the vanilla

lasso, elastic net, non-negative least squares, and Huber regression + (cid:96)1 norm are depicted in Figures 3, 4,

5, and 6, respectively. No over-conditioning occurred in the case of the non-negative least squares as we had

already restricted the coeﬃcients to be positive. In summary, although all of the methods could properly

control the FPR at a signiﬁcance level of α, the proposed method had the highest power among the methods.

The CI results were also consistent with the TPR results. That is, the shortest CI for the proposed method

indicated that it exhibited the highest power.

23

Figure 7: Robustness of proposed method for fused lasso.

Figure 8: Robustness of proposed method for vanilla lasso.

Figure 9: Robustness of proposed method for elastic net.

Figure 10: Robustness of proposed method for non-negative least squares.

Figure 11: Robustness of proposed method for Huber regression with (cid:96)1 penalty.

24

(a) Λ1 = {2−

1, 20, 21}

(b) Λ2 = {2−

10, ..., 210}

Figure 12: TPR comparison with existing method [Loftus, 2015] when accounting for CV selection event.

Robustness of proposed method in terms of FPR control. We demonstrated the robustness of our

method in terms of FPR control by considering the following cases:

Non-normal noise: We considered the noise following the Laplace distribution, skew normal distribution

•

(skewness coeﬃcient: 10), and t20 distribution.

Unknown σ2: We considered the case in which the variance was estimated from the data.

•
We generated n outcomes: yi = x(cid:62)

i β +εi, i = 1, ..., n, where p = 5, xi

N(0, Ip), and εi follows a Laplace

∼

distribution, skew normal distribution, or t20 distribution with a zero mean and the standard deviation set
to 1. In the case of the estimated σ2, εi

N(0, 1). We set all elements of β to 0 and set λ = 0.5. For every

∼
50, 100, 150, 200

∈ {

. We conﬁrmed that our method maintained high
}

case, we ran 1,200 trials for each n

performance on FPR control. The results are presented in Figures 7, 8, 9, 10, and 11.

Results when accounting for CV selection event. We conducted a comparison of the TPRs between

the proposed method and the OC version that was proposed in Loftus [2015] when λ was selected from

Λ1 =

{

2−1, 20, 21

or Λ2 =

}

2−10, 2−9, ..., 29, 210
{

}

. The results are presented in Figure 12. The existing

method had lower power because additional conditioning on all intermediate models was required, which

was also discussed in Markovic et al. [2017]. Our method exhibited higher power as we could characterize

the minimum conditioning amount.

Eﬃciency of proposed method. Our main purpose was to demonstrate that the proposed method has

not only high statistical power, but also reasonable computational costs. We conducted experiments on

feature selection by lasso. The computational time of the proposed method was almost linear with respect

to the number of active features, as illustrated in Figure 13a. Figure 13b presents a boxplot of the actual

number of intervals of z that were encountered on the line when constructing the truncation region

. Figure

Z

13c depicts the eﬃciency of our method compared to that of Lee et al. [2016], in which the authors mentioned

the naive method for removing sign conditioning by enumerating all possible combinations of signs 2|Mobs|.

25

(a) Computational time of proposed

(b) Number of encountered intervals

(c) Comparison with Lee et al.

method.

on line.

[2016].

Figure 13: Eﬃciency of proposed method.

Figure 14: Comparison between proposed method and methods in Liu et al. [2018], in which an exponentially

increasing number of all possible sign combinations is still required.

Comparison with Liu et al. [2018]. Furthermore, we demonstrated the eﬃciency of our method com-
pared to the two methods proposed in §3 (inference for partial regression targets) of Liu et al. [2018]. In

this work, only stable features were allowed to inﬂuence the formation of the test statistic. Stable features

are those with very strong signals and that cannot missed. In the ﬁrst method, which we refer to as TN-(cid:96)1,

the stable features were selected by setting a higher value of λ. In the second method, which we refer to as

TN-Custom, the stable features were selected by setting a cutoﬀ value. The details of these two methods

are presented in Appendix B. In general, to perform SI with these two methods, all possible combinations of

signs, which increase exponentially, still need to be naively enumerated. The proposed method can be used

to solve this computational bottleneck. A comparison of the computational costs is illustrated in Figure 14.

6.3 Results on Real-World Datasets

Array CGH data. Array CGH analyses enable the detection of changes in copy numbers across the

genome. We applied the proposed method and OC to the dataset with the ground truth provided in Snijders

et al. [2001]. The results of the detected CPs and tables of p-values are presented in Figures 15 and 16.

26

A

B

C

D

Proposed

0.9

0.6

0.3

1.4

OC

0.9

0.3

0.3

6.0

15

10−

5
10−

×

×

E

0.0

1.2

×

11

10−

Proposed

OC

A

0.0

0.0

(a) Chromosomes 17, 18, and 19.

(b) Chromosome 14.

Figure 15: Experimental results for cell lines GM00143 and GM01750.

Proposed

2.7

OC

5.3

A

320

10−

98

10−

×

×

B

0.0

0.0

A

B

Proposed

OC

1.1

1.1

×

×

41

10−

41

10−

9.1

9.1

×

×

14

10−

14

10−

(a) Chromosomes 1, 2, and 3.

(b) Chromosomes 20, 21, and 22.

Figure 16: Experimental results for cell line GM03576.

The solid red line denotes the signiﬁcant CPs, which had a p-value that was smaller than the signiﬁcance

level following Bonferroni correction. All of the results were consistent with those of Snijders et al. [2001].

Moreover, we compared the p-values of the proposed method and OC. The p-values of the proposed method

were smaller than or equal to those of OC for all true CPs, which indicates that the proposed method had

27

Figure 17: Boxplots of p-values. The left plot in each ﬁgure depicts the distributions of the p-values, whereas

the right plot displays the distributions of the p-values for the cases in which the two p-values of the proposed

method and OC diﬀered. In Dataset 1, the percentage of the p-value of the proposed method was 55.81%

smaller than that of OC. In Dataset 2, the percentage of the p-value of the proposed method was 54.24%

smaller than that of OC. In general, the p-value of the proposed method tended to be smaller than that of

OC, which indicates that the proposed method had higher statistical power than OC.

higher power than OC.

The boxplots of the distribution of the p-values for the proposed method and OC on the real-world

dataset are illustrated in Figure 17. We used the jointseg package [Pierre-Jean et al., 2014] to generate

realistic DNA copy number proﬁles of cancer samples with “known” truths. Two datasets consisting of 1,000

proﬁles, each with a length of n = 60, were created, as follows:

• Dataset 1: Resampled from GSE11976 with tumor fraction = 1.

• Dataset 2: Resampled from GSE29172 with tumor fraction = 1.

Nile data. These data contain the annual ﬂow volume of the Nile River at Aswan from 1871 to 1970 (100

years). In this case, the interest lies in unexpected events such as natural disasters. According to Figure 18,

the proposed algorithm identiﬁed a CP at the 28th position, corresponding to the year 1899. This result was

consistent with that of Jung et al. [2017].

Prostate data. We applied our proposed method for lasso to the prostate dataset from Hastie et al. [2009].

As p < n for this dataset, we could estimate σ2 using the residual sum of squares from the full regression

model with all p predictors. We set λ = 5. Figure 19 depicts the 95% CIs for the features that were selected

by both lasso and DS.

28

Figure 18: Experimental result for Nile data. A CP was detected at the 28th position, which indicates that

a change in the volume level occurred in 1899.

Figure 19: Experimental results for prostate data. The indices of the features were 1: lcavol, 2: lweight, 3:

age, 4: lbph, 6: lcp, 7: gleason, and 8: pgg45.

Acknowledgements

This work was partially supported by MEXT KAKENHI (20H00601, 16H06538), JST Moonshot R&D

(JPMJMS2033-05), NEDO (JPNP18002, JPNP20006), RIKEN Center for Advanced Intelligence Project,

and RIKEN Junior Research Associate Program.

29

References

A. Ali and R. J. Tibshirani. The generalized lasso problem and uniqueness. Electronic Journal of Statistics,

13(2):2307–2347, 2019.

E. L. Allgower and K. George. Continuation and path following. Acta Numerica, 2:1–63, 1993.

F. R. Bach, D. Heckerman, and E. Horvits. Considering cost asymmetry in learning classiﬁers. Journal of

Machine Learning Research, 7:1713–41, 2006.

Y. Benjamini and D. Yekutieli. False discovery rate–adjusted multiple conﬁdence intervals for selected

parameters. Journal of the American Statistical Association, 100(469):71–81, 2005.

Y. Benjamini, R. Heller, and D. Yekutieli. Selective inference in complex research. Philosophical Transactions

of the Royal Society A: Mathematical, Physical and Engineering Sciences, 367(1906):4255–4271, 2009.

R. Berk, L. Brown, A. Buja, K. Zhang, and L. Zhao. Valid post-selection inference. The Annals of Statistics,

41(2):802–837, 2013.

M. J. Best. An algorithm for the solution of the parametric quadratic programming problem. Applied

Mathemetics and Parallel Computing, pages 57–76, 1996.

S. Chen and J. Bien. Valid inference corrected for outlier removal. Journal of Computational and Graphical

Statistics, pages 1–12, 2019.

Y. Choi, J. Taylor, and R. Tibshirani. Selecting the number of principal components: Estimation of the true

rank of a noisy matrix. The Annals of Statistics, 45(6):2590–2617, 2017.

D. R. Cox. A note on data-splitting for the evaluation of signiﬁcance levels. Biometrika, 62(2):441–444,

1975.

V. N. L. Duy, S. Iwazaki, and I. Takeuchi. Quantifying statistical signiﬁcance of neural network

representation-driven hypotheses by selective inference. arXiv preprint arXiv:2010.01823, 2020a.

V. N. L. Duy, H. Toda, R. Sugiyama, and I. Takeuchi. Computing valid p-value for optimal changepoint by

selective inference using dynamic programming. In Advances in Neural Information Processing Systems,

2020b.

B. Efron and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004.

W. Fithian, D. Sun, and J. Taylor. Optimal inference after model selection. arXiv preprint arXiv:1410.2597,

2014.

W. Fithian, J. Taylor, R. Tibshirani, and R. Tibshirani. Selective sequential model selection. arXiv preprint

arXiv:1512.02565, 2015.

30

T. Gal. Postoptimal Analysis, Parametric Programming, and Related Topics. Walter de Gruyter, 1995.

T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector

machine. Journal of Machine Learning Research, 5:1391–415, 2004.

T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: data mining, inference, and

prediction. Springer Science & Business Media, 2009.

T. Hocking, j. P. Vert, F. Bach, and A. Joulin. Clusterpath: an algorithm for clustering using convex fusion

penalties. In Proceedings of the 28th International Conference on Machine Learning, pages 745–752, 2011.

P. J. Huber. Robust estimation of a location parameter.

In Breakthroughs in statistics, pages 492–518.

Springer, 1992.

S. Hyun, M. G’sell, and R. J. Tibshirani. Exact post-selection inference for the generalized lasso path.

Electronic Journal of Statistics, 12(1):1053–1097, 2018a.

S. Hyun, K. Lin, M. G’Sell, and R. J. Tibshirani. Post-selection inference for changepoint detection algorithms

with application to copy number variation data. arXiv preprint arXiv:1812.03644, 2018b.

M. Jung, S. Song, and Y. Chung. Bayesian change-point problem using bayes factor with hierarchical prior

distribution. Communications in Statistics-Theory and Methods, 46(3):1352–1366, 2017.

M. Karasuyama and I. Takeuchi. Nonlinear regularization path for quadratic loss support vector machines.

IEEE Transactions on Neural Networks, 22(10):1613–1625, 2010.

M. Karasuyama, N. Harada, M. Sugiyama, and I. Takeuchi. Multi-parametric solution-path algorithm for

instance-weighted support vector machines. Machine Learning, 88(3):297–330, 2012.

V. N. Le Duy and I. Takeuchi. Parametric programming approach for more powerful and general lasso

selective inference. In International Conference on Artiﬁcial Intelligence and Statistics, pages 901–909.

PMLR, 2021.

G. Lee and C. Scott. The one class support vector machine solution path. In Proc. of ICASSP 2007, pages

II521–II524, 2007.

J. D. Lee, D. L. Sun, Y. Sun, and J. E. Taylor. Exact post-selection inference, with application to the lasso.

The Annals of Statistics, 44(3):907–927, 2016.

H. Leeb and B. M. P¨otscher. Model selection and inference: Facts and ﬁction. Econometric Theory, pages

21–59, 2005.

H. Leeb and B. M. P¨otscher. Can one estimate the conditional distribution of post-model-selection estima-

tors? The Annals of Statistics, 34(5):2554–2591, 2006.

31

K. Liu, J. Markovic, and R. Tibshirani. More powerful post-selection inference, with application to the lasso.

arXiv preprint arXiv:1801.09037, 2018.

R. Lockhart, J. Taylor, R. J. Tibshirani, and R. Tibshirani. A signiﬁcance test for the lasso. Annals of

statistics, 42(2):413, 2014.

J. R. Loftus. Selective inference after cross-validation. arXiv preprint arXiv:1511.08866, 2015.

J. R. Loftus and J. E. Taylor. A signiﬁcance test for forward stepwise model selection. arXiv preprint

arXiv:1405.3920, 2014.

J. Mairal and B. Yu. Complexity analysis of the lasso regularization path. arXiv preprint arXiv:1205.0079,

2012.

J. Markovic, L. Xia, and J. Taylor. Unifying approach to selective inference with applications to cross-

validation. arXiv preprint arXiv:1703.06559, 2017.

K. Ogawa, M. Imamura, I. Takeuchi, and M. Sugiyama. Inﬁnitesimal annealing for training semi-supervised

support vector machines. In International Conference on Machine Learning, pages 897–905, 2013.

M. R. Osborne, B. Presnell, and B. A. Turlach. A new approach to variable selection in least squares

problems. IMA journal of numerical analysis, 20(3):389–403, 2000.

S. Panigrahi, J. Taylor, and A. Weinstein. Bayesian post-selection inference in the linear model. arXiv

preprint arXiv:1605.08824, 28, 2016.

M. Pierre-Jean, G. Rigaill, and P. Neuvial. Performance evaluation of dna copy number segmentation

methods. Brieﬁngs in bioinformatics, 16(4):600–615, 2014.

B. M. P¨otscher and U. Schneider. Conﬁdence sets based on penalized maximum likelihood estimators in

gaussian regression. Electronic Journal of Statistics, 4:334–360, 2010.

K. Ritter. On parametric linear and quadratic programming problems. mathematical Programming: Pro-

ceedings of the International Congress on Mathematical Programming, pages 307–335, 1984.

S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics, 35:1012–1030, 2007.

Y. She and A. B. Owen. Outlier detection using nonconvex penalized regression. Journal of the American

Statistical Association, 106(494):626–639, 2011.

A. M. Snijders, N. Nowak, R. Segraves, S. Blackwood, N. Brown, J. Conroy, G. Hamilton, A. K. Hindle,

B. Huey, and K. Kimura. Assembly of microarrays for genome-wide measurement of dna copy number.

Nature genetics, 29(3):263, 2001.

32

K. Sugiyama, V. N. L. Duy, and I. Takeuchi. More powerful and general selective inference for stepwise

feature selection using the homotopy continuation approach. arXiv preprint arXiv:2012.13545, 2020.

S. Suzumura, K. Nakagawa, Y. Umezu, K. Tsuda, and I. Takeuchi. Selective inference for sparse high-order

interaction models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,

pages 3338–3347. JMLR. org, 2017.

I. Takeuchi and M. Sugiyama. Target neighbor consistent feature weighting for nearest neighbor classiﬁcation.

In Advances in neural information processing systems, pages 576–584, 2011.

I. Takeuchi, K. Nomura, and T. Kanamori. Nonparametric conditional density estimation using piecewise-

linear solution path of kernel quantile regression. Neural Computation, 21(2):539–559, 2009.

I. Takeuchi, T. Hongo, M. Sugiyama, and S. Nakajima. Parametric task learning. Advances in Neural

Information Processing Systems, 26:1358–1366, 2013.

K. Tanizaki, N. Hashimoto, Y. Inatsu, H. Hontani, and I. Takeuchi. Computing valid p-values for image

segmentation by selective inference. In Proceedings of the IEEE/CVF Conference on Computer Vision

and Pattern Recognition, pages 9553–9562, 2020.

J. Taylor, R. Lockhart, R. J. Tibshirani, and R. Tibshirani. Post-selection adaptive inference for least angle

regression and the lasso. arXiv preprint arXiv:1401.3889, 354, 2014.

Y. Terada and H. Shimodaira. Selective inference after variable selection via multiscale bootstrap. arXiv

preprint arXiv:1905.10573, 2019.

X. Tian and J. Taylor. Selective inference with a randomized response. The Annals of Statistics, 46(2):

679–710, 2018.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:

Series B (Methodological), 58(1):267–288, 1996.

R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. The annals of statistics, 39(3):

1335–1371, 2011.

R. J. Tibshirani, J. Taylor, R. Lockhart, and R. Tibshirani. Exact post-selection inference for sequential

regression procedures. Journal of the American Statistical Association, 111(514):600–620, 2016.

C. Truong, L. Oudre, and N. Vayatis. Selective review of oﬄine change point detection methods. Signal

Processing, 167:107299, 2020.

K. Tsuda. Entire regularization paths for graph data. In In Proc. of ICML 2007, pages 919–925, 2007.

33

T. Tsukurimichi, Y. Inatsu, V. N. L. Duy, and I. Takeuchi. Conditional selective inference for robust regres-

sion and outlier detection using piecewise-linear homotopy continuation. arXiv preprint arXiv:2104.10840,

2021.

F. Yang, R. F. Barber, P. Jain, and J. Laﬀerty. Selective inference for group-sparse linear models.

In

Advances in Neural Information Processing Systems, pages 2469–2477, 2016.

H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the royal statistical

society: series B (statistical methodology), 67(2):301–320, 2005.

A Full Target Case for Lasso in Liu et al. [2018]

In the full target case, as discussed in Liu et al. [2018], the data are used to select the interesting features,

but they are not used to summarize the relation between the response and the selected features. Therefore,

all features can be used to deﬁne the direction of interest.

ηj = X(X (cid:62)X)−1ej,

where ej

∈

Rp is a zero vector with 1 at the jth coordinate. The conditional inference is deﬁned as

η(cid:62)
j Y

j

|

(Y ), q(Y ) = q(yobs)

∈ A

.

(41)

In Liu et al. [2018], the authors proposed a solution for conducting conditional inference for a speciﬁc case

(cid:8)

(cid:9)

when p < n, and there was no solution for the case when p > n. This problem can be solved with the

proposed PP method. First, we rewrite the conditional inference in (41) as the problem of characterizing

the sampling distribution of

Z

Z

| {

∈ Z}

where

=

z

{

R

j

|

∈

Z

(y(z))
}

∈ A

.

(42)

, only the path of the lasso solution ˆβ(z) needs
y(z) in (42) is deﬁned as in (15). Thereafter, to identify
to be obtained, as proposed in §3, and the intervals in which j is an element of the active set corresponding
to ˆβ(z) simply need to be veriﬁed along the path. Finally, after obtaining

, we can easily compute the

Z

Z

selective p-value or selective CI.

B Stable Partial Target Case for Lasso in Liu et al. [2018]

In the stable partial target case, as discussed in Liu et al. [2018], only stable features are allowed to inﬂuence

the formation of the test statistic. Stable features are those with very strong signals that we do not wish to

omit. We select a set

H

obs of stable features. Subsequently, for any j

obs, j

∈ H

obs,

∈ M

ηj = XHobs (X (cid:62)

HobsXHobs)−1ej.

34

For any j

obs, j

(cid:54)∈ H

obs,

∈ M

ηj = XHobs∪{j}(X (cid:62)

Hobs∪{j}XHobs∪{j})−1ej.

Next, we demonstrate how to construct

obs according to Liu et al. [2018].

H

Stable target formation by setting higher value of λ (TN-(cid:96)1).

In this case,

set, but with a higher value of λ than that used to select

obs. We denote

M

obs =

H

H

the conditional inference is deﬁned as

H

obs is the lasso active
(yobs), and subsequently,

η(cid:62)
j Y

j

|

(Y ),

∈ A

(Y ) =

H

H

(yobs), q(Y ) = q(yobs)

.

(43)

The main drawback of the method in Liu et al. [2018] is that all 2|Hobs| sign vectors must be considered,

(cid:8)

(cid:9)

which requires substantial computation time when

obs

is large. This limitation can easily be overcome

|H
using our piecewise linear homotopy computation. First, we rewrite the conditional inference in (43) as the

|

problem of characterizing the sampling distribution of

Z

Z

| {

∈ Z}

where

=

z
{

R

j

|

∈

Z

(y(z)),

∈ A

(y(z)) =

H

(yobs)
}

.

H

Thereafter, we can easily identify

(y(z)) =

H

(yobs)

H

1 =
∈
, which we can simply obtain using the method proposed in §3 of the main paper.
}

2, where

(y(z))

∈ A

2 =

∩ Z

z
{

and

=

Z

Z

Z

Z

R

∈

{

}

z

j

1

|

R

|

Stable target formation by setting cutoﬀ value c (TN-Custom).

In this case, we determine

by setting a cutoﬀ value c to select βj, such that

βj
|

| ≥

c 5. The set

H

obs is deﬁned as

obs

H

obs =

H

j

{

obs,

∈ M

βj
|

| ≥

,

c
}

where βj = e(cid:62)

MobsXMobs)−1X (cid:62)
conditional inference is formulated as

j (X (cid:62)

Mobsyobs. We denote

obs =

H

(

H

obs)

M

⊂ M

obs, and subsequently, the

η(cid:62)
j Y

(Y )) =

(
A

(

H

obs),

M

| {H

(Y ) =

A

obs

M

.

}

(44)

The main drawback of the method in Liu et al. [2018] is that it still requires conditioning on

(Y ) =

obs

,

}
is large, because the enumeration of 2|Mobs| sign vectors

{A

M

which is computationally intractable when

|M

obs

|

is required. This limitation can easily be overcome using our proposed method.

5Our formulation is slightly diﬀerent from but more general than that in Liu et al. [2018].

35

