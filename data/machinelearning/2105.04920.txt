More Powerful Conditional Selective Inference for

Generalized Lasso by Parametric Programming

Vo Nguyen Le Duy

Nagoya Institute of Technology and RIKEN

duy.mllab.nit@gmail.com

Ichiro Takeuchi

Nagoya Institute of Technology and RIKEN

takeuchi.ichiro@nitech.ac.jp

May 12, 2021

Abstract

Conditional selective inference (SI) has been studied intensively as a new statistical inference frame-

work for data-driven hypotheses. The basic concept of conditional SI is to make the inference conditional

on the selection event, which enables an exact and valid statistical inference to be conducted even when

the hypothesis is selected based on the data. Conditional SI has mainly been studied in the context of

model selection, such as vanilla lasso or generalized lasso. The main limitation of existing approaches

is the low statistical power owing to over-conditioning, which is required for computational tractability.

In this study, we propose a more powerful and general conditional SI method for a class of problems

that can be converted into quadratic parametric programming, which includes generalized lasso. The

key concept is to compute the continuum path of the optimal solution in the direction of the selected

test statistic and to identify the subset of the data space that corresponds to the model selection event

by following the solution path. The proposed parametric programming-based method not only avoids

the aforementioned major drawback of over-conditioning, but also improves the performance and practi-

cality of SI in various respects. We conducted several experiments to demonstrate the eÔ¨Äectiveness and

eÔ¨Éciency of our proposed method.

1
2
0
2

y
a
M
1
1

]
L
M

.
t
a
t
s
[

1
v
0
2
9
4
0
.
5
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1

Introduction

As machine learning (ML) is applied to solve numerous practical problems, the quantiÔ¨Åcation of the reliability

of data-driven knowledge obtained by ML algorithms is becoming increasingly important. Among the various

potential approaches for reliable ML, conditional selective inference (SI) has been recognized as a new

promising method for assessing the statistical reliability of data-driven hypotheses that are selected by

ML algorithms. The main concept of conditional SI is to make the inference for a data-driven hypothesis

conditional on the selection event that the hypothesis is selected, which enables an exact and a valid inference

to be conducted on the selected hypothesis. In the conditional SI framework, the statistical signiÔ¨Åcance and

reliability of data-driven selected hypotheses are quantiÔ¨Åed by the so-called selective p-values and selective

conÔ¨Ådence intervals, which have proper false positive rates and coverage guarantees, respectively.

Lee et al. [2016] Ô¨Årst introduced conditional SI as a statistical inference tool for the features selected by

lasso [Tibshirani, 1996]. Subsequently, Hyun et al. [2018a] studied SI for inference on the selected model

using generalized lasso [Tibshirani and Taylor, 2011]. Their basic concept was to characterize the selection

event using a polytope (a set of linear inequalities) in the sample space. In general, we refer to such methods

as polytope-based SI approaches. The practical computational methods developed by these authors can be

used when the selection event can be characterized by a single polytope.

However, the application scope of such polytope-based SI is limited because it can only be used when

the characterization of all relevant selection events is represented by a polytope. Therefore, in most existing

polytope-based SI studies, additional conditioning is required for the selection event to be characterized as a

polytope. For example, in the case of lasso [Lee et al., 2016], the set of selected features as well as their signs

require conditioning. Similarly, in the case of generalized lasso [Hyun et al., 2018a], additional conditioning

on the signs as well as on the history (sequential order) whereby the selected elements enter the selected

model is required. Such additional conditioning results in low statistical power, which is widely recognized

as a major drawback of polytope-based SI studies [Fithian et al., 2014].

Contributions. The contributions of this study are as follows:

‚Ä¢ We go beyond the scope of polytope-based SI and propose a new SI approach based on parametric

programming (PP). We name the proposed method PP-based SI. The basic concept of PP-based SI is

to compute the continuum path of the optimal solutions in the direction of the selected test statistic

using PP, which is subsequently used to identify the exact sampling distribution of the test statistic

with the minimum amount of conditioning. Therefore, PP-based SI can fundamentally resolve the

over-conditioning problem, which is a major concern in polytope-based SI, thereby achieving high

statistical power.

‚Ä¢ We derive the proposed PP-based SI for a generic class of conditional SI problems that can be repre-

sented as parametric quadratic programs (QPs). We demonstrate that the conditional SI formulations

2

for many practical problems, including generalized lasso, elastic net, non-negative least squares, and

Huber regression with the (cid:96)1 penalty, belong to this class, which means that the proposed PP-based

SI can be used extensively.

‚Ä¢ Furthermore, we discuss how the advantages of PP can be exploited for the eÔ¨Äective performance

of conditional SI in various data analysis tasks. As an example, using PP, we demonstrate that

conditional SI can be conducted with minimal conditioning for regularization parameter selection by

cross-validation (CV), the selection event of which is too complicated to be characterized as a single

polytope, but can be fully characterized using PP1.

‚Ä¢ We conducted intensive experiments on both synthetic and real-world datasets, by means of which we

presented evidence that our proposed method can successfully control the false positive rate, has higher

statistical power than polytope-based SI, and provides superior results in practical applications.

A preliminary short version of this work was presented at the AI & Statistics (AISTATS2021) conference

[Le Duy and Takeuchi, 2021].

In the conference version, we only studied a speciÔ¨Åc case of vanilla lasso.

In this study, we extended the basic concept of PP-based SI to a more general class of problems that can

be formulated as parametric QPs, which includes vanilla lasso as a special case. Moreover, we extended

the proposed method to various aspects and conducted intensive additional experiments to demonstrate the

applicability of the generalized PP-based SI to a wider class of problems and settings.

Related works.

In traditional statistical inference, it is assumed that the hypothesis is Ô¨Åxed in advance.

That is, the hypothesis on which we wish to conduct inference is determined prior to observing the dataset.

Therefore, if traditional statistical inference methods are applied to data-driven hypotheses, the inferential

results will no longer be valid. This problem has been discussed extensively in the context of testing the

signiÔ¨Åcance of the features selected by a feature selection method, such as lasso or stepwise feature selection.

Several approaches have been proposed to address this problem [Benjamini and Yekutieli, 2005, Leeb and

P¬®otscher, 2005, 2006, Benjamini et al., 2009, P¬®otscher and Schneider, 2010, Berk et al., 2013, Lockhart et al.,

2014, Taylor et al., 2014].

In recent years, Lee et al. [2016] proposed a practical SI framework to perform exact (non-asymptotic)

inference for a set of features selected by lasso. In their work, the authors revealed that the selection event

can be characterized as a polytope by conditioning on a set of selected features and their signs. Furthermore,

Hyun et al. [2018a] demonstrated that polytope-based SI is applicable to generalized lasso by performing

additional conditioning on the signs as well as the history (sequential order) whereby the selected elements

entered the selected model. Following the seminal work of [Lee et al., 2016], conditional inference-based SI

has been actively studied and applied to various problems [Fithian et al., 2015, Choi et al., 2017, Tian and

1Loftus [2015] considered conditional SI for CV-based regularization parameter selection, but it was highly over-conditioned

with additional events. We demonstrate that our PP-based SI is more powerful than this approach.

3

Taylor, 2018, Chen and Bien, 2019, Hyun et al., 2018b, Loftus and Taylor, 2014, Loftus, 2015, Panigrahi

et al., 2016, Tibshirani et al., 2016, Yang et al., 2016, Suzumura et al., 2017, Tanizaki et al., 2020, Duy et al.,

2020b,a, Sugiyama et al., 2020, Tsukurimichi et al., 2021].

It is desirable to conduct more powerful inference by conditioning on as little information as possible in

conditional SI [Fithian et al., 2014]. However, in polytope-based SI, an excessive amount of over-conditioning

is required to represent the selection event using a single polytope. The authors of Lee et al. [2016] already

mentioned the problem of over-conditioning and discussed the solution for removing the additional condi-

tioning by enumerating all possible combinations of signs and taking the union over the resulting polyhedra.

Unfortunately, such an enumeration for an exponentially increasing number of sign combinations is only

feasible when the number of selected features is small. Loftus and Taylor [2014] extended polytope-based

SI to cases in which the selection event is characterized by quadratic inequalities. Although we do not dis-

cuss quadratic inequality-based SI further, as it suÔ¨Äers from a similar over-conditioning issue, our proposed

PP-based SI can also be applied to resolve the issue for this class of conditional SIs.

Our work was motivated by Liu et al. [2018], in which the authors proposed solutions to overcome the

over-conditioning issue of polytope-based SI for vanilla lasso in certain special settings. In one of the settings,

inference on the full model parameters in which conditional SI can be performed with minimal conditioning

was studied, because a full model parameter is not dependent on other parameters. Moreover, our work was

motivated by a discussion in the paper where the authors noted that multiple lasso Ô¨Åtting at a sequence

of grid points may aid in alleviating over-conditioning. However, the authors did not suggest any practical

computational methods to realize this concept. Furthermore, the conditional sampling distribution that is

evaluated at a Ô¨Ånite number of grid points only provides an approximation of the exact distribution, which

means that the theoretical validity of the conditional SI is no longer guaranteed. Our proposed PP-based SI

can be interpreted as a means of solving lasso at inÔ¨Ånitely many grid points, which completely resolves these

challenging problems. As another direction to resolve over-conditioning, Tian and Taylor [2018] and Terada

and Shimodaira [2019] proposed methods using randomization. A drawback of these randomization-based

approaches (including the simple data-splitting approach) is that further randomness is added in both the

feature selection and inference stages.

PP has long been studied in the optimization Ô¨Åeld to solve a family of optimization problems that

are parameterized by a scalar parameter [Ritter, 1984, Allgower and George, 1993, Gal, 1995, Best, 1996].

Moreover, PP has been used in the context of the regularization path in ML [Osborne et al., 2000, Efron

and Tibshirani, 2004, Hastie et al., 2004, Rosset and Zhu, 2007, Bach et al., 2006, Rosset and Zhu, 2007,

Tsuda, 2007, Lee and Scott, 2007, Takeuchi et al., 2009, Takeuchi and Sugiyama, 2011, Karasuyama and

Takeuchi, 2010, Hocking et al., 2011, Karasuyama et al., 2012, Ogawa et al., 2013, Takeuchi et al., 2013].

The regularization path is a method of tracking the manner in which the optimal solution changes when the

regularization parameter changes, which is useful for eÔ¨Écient model selection. Our main idea was to employ

PP to track how the optimal solution and selected features change when the training dataset changes in

4

the direction of the selected test statistic, which enables the exact sampling distribution of the test statistic

that is conditional on the selection event to be identiÔ¨Åed. The power of the conditional SI introduced in the

pioneering work of Lee et al. [2016] can be optimized using the proposed approach, which is applicable to

conditional SI for a wide class of problems including generalized lasso.

2 Problem Statement

To formulate the problem, we consider a random response vector

Y = (Y1, ..., Yn)(cid:62)

N(¬µ, Œ£),

‚àº

(1)

where n is the number of instances, ¬µ is an unknown vector, and Œ£

Rn√ón is a covariance matrix that is
known or estimable from independent data. The goal is statistical quantiÔ¨Åcation of the signiÔ¨Åcance of the

‚àà

data-driven hypotheses that are obtained by applying the generalized lasso estimator to the response vector.

Generalized lasso and its selection event. We consider a linear regression model with p features
Rn√óp, in which the features are considered as non-

Rn and denote the feature matrix as X

x1, . . . , xp

‚àà

‚àà

random. We do not make any assumption about the relationship between the ¬µ and p features x1, . . . , xp
‚àà
Rn, but consider the case in which a linear model with generalized lasso regularization is employed to model
Rn
the relationship between X and a random response vector Y . Given an observed response vector yobs

‚àà

that is sampled from model (1), the generalized lasso optimization problem is expressed as

ÀÜŒ≤ = arg min

Œ≤‚ààRp

1
2 (cid:107)

yobs

XŒ≤

2
2 + Œª
(cid:107)
(cid:107)

DŒ≤

1,

(cid:107)

‚àí

(2)

Rm√óp is a penalty matrix and Œª

where D
0 is a regularization parameter. The matrix D and its number
of rows are predetermined by the user to produce the desired structures in the solution ÀÜŒ≤ in (2). Examples

‚â•

‚àà

of matrix D are presented in Examples 1, 2, and 3.

As the optimization in (2) produces the sparsity of D ÀÜŒ≤, we deÔ¨Åne a set of non-zero components (the

active set) as follows:

obs =

M

(yobs) =

A

j : (D ÀÜŒ≤)j
{

,
= 0
}

[m],

j

‚àà

where

: Y

A

(cid:55)‚Üí M

indicates the algorithm that maps a response vector Y to a set of non-zero components

. Thereafter, we deÔ¨Åne the selection event in which the active set for a random response vector Y is the

M
same as the observed response vector yobs:

Statistical inference. Let Œ∑(cid:62)

j Y be a linear contrast that indicates the test statistic that we wish to
consider, where Œ∑j is deÔ¨Åned depending on the problem and the jth selected component in the observed

(Y ) =

A

(yobs)

A

.

(cid:9)

(cid:8)

(3)

active set.

5

(cid:54)
Example 1. In the case of testing the features selected by vanilla lasso [Tibshirani, 1996], D = Ip
which is the identity matrix. The test statistic Œ∑(cid:62)

Rp√óp,
j Y = ÀÜŒ≤j represents the coeÔ¨Écient of the jth selected feature

‚àà

[Lee et al., 2016], where Œ∑j is deÔ¨Åned as

Œ∑j = XMobs

X (cid:62)

MobsXMobs

‚àí1

ej,

(4)

in which ej

R|Mobs| is a basis vector with 1 at the jth position. This form of test statistic can also be
applied to test the features that are selected by other regression methods, such as elastic net [Zou and Hastie,

‚àà

(cid:0)

(cid:1)

2005], Huber regression [Huber, 1992], or non-negative least squares.

Example 2. In the context of changepoint (CP) detection using fused lasso, the matrix D

R(p‚àí1)√óp is

‚àà

expressed as

D =

1

‚àí
0

1

1

‚àí

0

0

Ô£´

Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠

0

1

0

0

0

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

1
¬∑ ¬∑ ¬∑ ‚àí

.

0
Ô£∂
0
Ô£∑
Ô£∑
Ô£∑
Ô£∑
1
Ô£∑
Ô£∑
Ô£∏

The test statistic Œ∑(cid:62)

j Y represents the diÔ¨Äerence in the sample mean between the left and right segments of

the CP at the jth position, which was also used in Hyun et al. [2018a]. In this case, Œ∑j is deÔ¨Åned as

Œ∑j =

j

‚àí

1
jprev

1n
jprev+1:j ‚àí

1
jnext

1n
j+1:jnext ,

j

‚àí

(5)

where jprev
j, respectively, and 1n

‚àà M

obs and jnext

obs are the CP positions before and after the selected CP at position
Rn is a vector in which the elements from positions s to e are set to 1, and 0

‚àà M

s:e ‚àà

otherwise.

Example 3. In trend Ô¨Åltering, the aim is to test whether a change occurs in the trend at position j

obs.

‚àà M

We deÔ¨Åne Œ∑j as follows:

Œ∑j = ej‚àí1

‚àí

2ej + ej+1,

Rn. The test statistic Œ∑(cid:62)

j Y indicates that we wish to test whether the points at positions j,

where ej

‚àà

1, and j + 1 lie on the same line statistically. In this case, the matrix D

j

‚àí

R(p‚àí2)√óp is expressed as

‚àà

D =

2

1
‚àí

1

‚àí
2

0

0

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

0

0

0

1
¬∑ ¬∑ ¬∑ ‚àí

1

‚àí
0

Ô£´

Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠

0

0

2

.

0

0

Ô£∂

Ô£∑
Ô£∑
Ô£∑
Ô£∑
1
Ô£∑
Ô£∑
Ô£∏

‚àí

For the inference, we consider the following null hypothesis and alternative hypothesis:

H0,j : Œ∑(cid:62)

j ¬µ = 0 vs. H1,j : Œ∑(cid:62)

j ¬µ

= 0.

(6)

6

(cid:54)
Conditional SI. Suppose that the hypotheses in (6) are Ô¨Åxed; that is, non-random. Thus, the naive

(two-sided) p-value in the classical z-test is obtained by

P naive
j

= PH0,j

Œ∑(cid:62)
j Y
|

j yobs
Œ∑(cid:62)

| ‚â• |

.

|

(7)

However, as the hypotheses in (6) are not Ô¨Åxed in advance, the naive p-value is not valid in the sense that,

(cid:0)

(cid:1)

if we reject H0,j with a signiÔ¨Åcance level Œ± (e.g., Œ± = 0.05), the false positive rate (type-I error) cannot be

controlled at the level Œ±. This is because the hypotheses in (6) are selected by the data and selection bias

exists.

It is necessary to remove the information that has been used for the initial hypothesis generation process

to correct the selection bias. This can be achieved by considering the sampling distribution of the test

statistic Œ∑(cid:62)

j Y that is conditional on the selection event; that is,

Œ∑(cid:62)
j Y

|

(Y ) =

A

(yobs), q(Y ) = q(yobs)

A

,

(8)

j Œ£Œ∑j)‚àí1 is the nuisance component that is independent of
where q(Y ) = (In
‚àí
j Y . The second condition q(Y ) = q(yobs) indicates that the nuisance component for a
the test statistic Œ∑(cid:62)

j )Y with c = Œ£Œ∑j(Œ∑(cid:62)

cŒ∑(cid:62)

(cid:8)

(cid:9)

random vector Y is the same as that for yobs 2.

Once the selection event has been identiÔ¨Åed, the pivotal quantity can be computed:

F Z
j ¬µ,Œ∑(cid:62)
Œ∑(cid:62)

j Œ£Œ∑j

(Œ∑(cid:62)

j Y ).

(Y ) =

A

|

(yobs), q(Y ) = q(yobs)

A

,

(9)

which is the c.d.f. of the truncated normal distribution with a mean Œ∑(cid:62)

j ¬µ, variance Œ∑(cid:62)

j Œ£Œ∑j, and truncation

(cid:8)

(cid:9)

region

Z

, which is calculated based on the selection event. Based on the pivotal quantity, the selective type-I

error or selective p-value [Fithian et al., 2014] can be considered in the following form:

P selective

j

œÄj, 1
= 2 min
{

‚àí

œÄj

,
}

(10)

where œÄj = 1

‚àí

F Z

0,Œ∑(cid:62)

j Œ£Œ∑j

(Œ∑(cid:62)

j Y ), which is valid in the sense that

ProbH0,j

P selective

j

< Œ±

= Œ±,

Œ±

‚àÄ

‚àà

[0, 1].

Furthermore, to obtain a conÔ¨Ådence level of 1

(cid:0)

(cid:1)

Œ± for any Œ±

‚àí

‚àà

[0, 1], by inverting the pivotal quantity in

Equation (9), we can determine the smallest and largest values of Œ∑(cid:62)

j ¬µ such that the value of the pivotal

quantity remains in the interval

[Lee et al., 2016].

Œ±
2 , 1

Œ±
2

‚àí

(cid:2)

(cid:3)

Challenge in conditional data space characterization. The main diÔ¨Éculty in the above conditional

SI is that the characterization of the minimal conditional data space

(Y ) =

A

(yobs), q(Y ) = q(yobs)

A

2q(Y ) corresponds to the component z in the seminal paper (see Lee et al. [2016], Section 5, Eq. 5.2 and Theorem 5.2).

(cid:8)

(cid:9)

7

Figure 1: Schematic of proposed method. We obtain the observed active set

obs by applying generalized
lasso to the observed data yobs. The statistical inference for each selected element in the observed active

M

set is conducted conditional on the subspace

Y

, the data of which have the same active set as yobs. We

introduce a PP method for characterizing the conditional data space

by searching the parameterized line.

Y

in Equation (8) is intractable. To overcome this issue, Hyun et al. [2018a] considered the inference to be

conditional not only on the active set, but also on the signs and history (order) whereby the elements of
D ÀÜŒ≤ entered the active set. Unfortunately, such additional conditioning on the signs and history leads to low
statistical power owing to over-conditioning 3.

In the following section, we introduce a method for identifying the minimum amount of conditioning

(Y ) =

A

(yobs), q(Y ) = q(yobs)

A

that results in higher statistical power. The main concept is to compute

the path of the generalized lasso solutions in the direction of interest Œ∑j. By focusing on the line along Œ∑j, the
(cid:8)

(cid:9)

majority of irrelevant regions that do not aÔ¨Äect the truncated normal sampling distribution can be skipped

because they do not intersect with this line. Thus, we can skip the majority of combinations of signs and

history that never appear when applying generalized lasso to the data on the line.

3 Proposed Method

We present the technical details of the proposed method in this section. A schematic of the method is
provided in Figure 1. We Ô¨Årst introduce a QP reformulation for the generalized lasso problem in ¬ß3.1.
Thereafter, the characterization of the conditional data space is presented in ¬ß3.2. Subsequently, we propose
a PP approach for identifying the conditional data space in ¬ß3.3. Finally, the detailed algorithm is presented
in ¬ß3.4.

3This over-conditioning corresponds to the additional conditioning on the signs of the selected features in the seminal

conditional SI study of vanilla lasso [Lee et al., 2016].

8

Data Space ‚ÑùnyobsAlgorithm ùíú(yobs)Proposed MethodObserved Active Set‚Ñ≥obs={1,5,7}Parametrized
Liney(z)=a+bzConditional Data Space 
ùí¥={y‚àà‚Ñùn‚à£ùíú(y)=ùíú(yobs),q(y)=q(yobs)}{1,5,7}{1,5,7}{1,5}{5,7}{1,7}MorePowerfulSelectiveInferenceforGeneralizedLassoWeobtain(12)bydecomposing‚á†=‚á†+ ‚á† with‚á†+,‚á†  0.Here,thekeypointisthatthecomponent||‚á†||1in(13)canbewrittenas||‚á†||1=Pj2[m](‚á†+j+‚á† j)becauseatleastoneof‚á†+jand‚á† j,j2[m],mustbesetto0attheoptimalsolution.Inotherwords,ÀÜ‚á†+j>0)ÀÜ‚á† j=0,andviceversa.Theideaofdecomposing‚á†=‚á†+ ‚á† areoftenemployedinreformulating`1-normof‚á†.3.2ConditionalDataSpaceCharacterizationLetusdeÔ¨Ånethesetofy2RnwhichsatisÔ¨ÅestheconditionsinEquation(8)asY={y2Rn|A(y)=A(yobs),q(y)=q(yobs)}.(14)Accordingtothesecondcondition,thedatainYisrestrictedtoaline(seeSec6inLiuetal.(2018),andFithianetal.(2014)).Therefore,thesetYcanbere-written,usingascalarparameterz2R,asY={y(z)=a+bz|z2Z},(15)wherea=q(yobs),b=‚åÉ‚åòj(‚åò>j‚åÉ‚åòj) 1,andZ=nz2R|A(y(z))=A(yobs)o.(16)Now,letusconsiderarandomvariableZ2Randitsobservationzobs2R,whichsatisfyY=a+bZandyobs=a+bzobs.Theconditionalinferencein(8)isre-writtenastheproblemofcharacterizingthesamplingdistributionofZ|{Z2Z}.(17)SinceZ‚á†N(0,‚åò>j‚åÉ‚åòj)underthenullhypothesis,Z|Z2ZfollowsatruncatedNormaldistribution.OncethetruncationregionZisidentiÔ¨Åed,thepivotalquantityinEquation(9)isequaltoFZ0,‚åò>j‚åÉ‚åòj(Z),andcanbeeasilyobtained.Thus,theremainingtaskistocharacterizeZ.CharacterizationoftruncationregionZ.Letusintroducetheoptimizationproblem(12)withparametrizedresponsevectory(z)=a+bz(a,baredeÔ¨Ånedin(15))forz2RasÀÜr(z)=argminr12r>Pr+(q0+q1z)>rs.t.GrÔ£øh0+h1z,(18)wherer=( ,‚á†+,‚á† )>2Rp+2m,q0=  X>a, 1m, 1m >2Rp+2m,q1=  X>b,0,0 2Rp+2m,P2R(p+2m)‚á•(p+2m)andG2R4m‚á•(p+2m),h0=h1=04m,P=0@X>X000000001A,G=0@ DD00Im Im Im0 ImIm0 Im1A>.9MorePowerfulSelectiveInferenceforGeneralizedLassoWeobtain(12)bydecomposing‚á†=‚á†+ ‚á† with‚á†+,‚á†  0.Here,thekeypointisthatthecomponent||‚á†||1in(13)canbewrittenas||‚á†||1=Pj2[m](‚á†+j+‚á† j)becauseatleastoneof‚á†+jand‚á† j,j2[m],mustbesetto0attheoptimalsolution.Inotherwords,ÀÜ‚á†+j>0)ÀÜ‚á† j=0,andviceversa.Theideaofdecomposing‚á†=‚á†+ ‚á† areoftenemployedinreformulating`1-normof‚á†.3.2ConditionalDataSpaceCharacterizationLetusdeÔ¨Ånethesetofy2RnwhichsatisÔ¨ÅestheconditionsinEquation(8)asY={y2Rn|A(y)=A(yobs),q(y)=q(yobs)}.(14)Accordingtothesecondcondition,thedatainYisrestrictedtoaline(seeSec6inLiuetal.(2018),andFithianetal.(2014)).Therefore,thesetYcanbere-written,usingascalarparameterz2R,asY={y(z)=a+bz|z2Z},(15)wherea=q(yobs),b=‚åÉ‚åòj(‚åò>j‚åÉ‚åòj) 1,andZ=nz2R|A(y(z))=A(yobs)o.(16)Now,letusconsiderarandomvariableZ2Randitsobservationzobs2R,whichsatisfyY=a+bZandyobs=a+bzobs.Theconditionalinferencein(8)isre-writtenastheproblemofcharacterizingthesamplingdistributionofZ|{Z2Z}.(17)SinceZ‚á†N(0,‚åò>j‚åÉ‚åòj)underthenullhypothesis,Z|Z2ZfollowsatruncatedNormaldistribution.OncethetruncationregionZisidentiÔ¨Åed,thepivotalquantityinEquation(9)isequaltoFZ0,‚åò>j‚åÉ‚åòj(Z),andcanbeeasilyobtained.Thus,theremainingtaskistocharacterizeZ.CharacterizationoftruncationregionZ.Letusintroducetheoptimizationproblem(12)withparametrizedresponsevectory(z)=a+bz(a,baredeÔ¨Ånedin(15))forz2RasÀÜr(z)=argminr12r>Pr+(q0+q1z)>rs.t.GrÔ£øh0+h1z,(18)wherer=( ,‚á†+,‚á† )>2Rp+2m,q0=  X>a, 1m, 1m >2Rp+2m,q1=  X>b,0,0 2Rp+2m,P2R(p+2m)‚á•(p+2m)andG2R4m‚á•(p+2m),h0=h1=04m,P=0@X>X000000001A,G=0@ DD00Im Im Im0 ImIm0 Im1A>.9MorePowerfulSelectiveInferenceforGeneralizedLassoconsideredtobenon-random.Inthisstudy,wedonotposeanyassumptionabouttherelationshipbetween¬µandpfeaturesx1,...,xp2Rn,butconsiderthecasewherealinearmodelwithgeneralizedlassoregularizationisemployedformodelingtherelationshipbetweenXandarandomresponsevectorY.Givenanobservedresponsevectoryobs2Rnsampledfromthemodel(1),thegeneralizedlassooptimizationproblemisgivenbyÀÜ =argmin 2Rp12kyobs X k22+ kD k1,(2)whereD2Rm‚á•pisapenaltymatrix,and  0isaregularizationparameter.ThematrixDanditsnumberofrowsarepre-determinedbytheuserforthepurposeofproducingsomedesiredstructuresinthesolutionÀÜ in(2).SomeexamplesofthematrixDarepresentedinExamples1,2and3.Sincetheoptimizationin(2)producesthesparsityofDÀÜ ,wedeÔ¨Åneasetofnon-zerocomponents(activeset)asMobs=A(yobs)={j:(DÀÜ )j6=0},j2[m],whereA:Y7!MindicatesthealgorithmwhichmapsaresponsevectorYintoasetofnon-zerocomponentsM.Then,wedeÔ¨ÅnetheselectioneventthattheactivesetforarandomresponsevectorYisthesameastheobservedresponsevectoryobsasnA(Y)=A(yobs)o.(3)Statisticalinference.Let‚åò>jYbealinearcontrastthatindicatesthetest-statisticwewanttoconsider,where‚åòjisdeÔ¨Åneddependingontheproblemandthejthselectedcom-ponentintheobservedactiveset.Example1Inthecaseoftestingthefeatureselectedbyvanillalasso(Tibshirani,1996),D=Ip2Rp‚á•p,whichistheidentitymatrix.Thetest-statistic‚åò>jY=ÀÜ jrepresentsthecoe cientofthejthselectedfeature(Leeetal.,2016)where‚åòjisdeÔ¨Ånedas‚åòj=XMobs‚á£X>MobsXMobs‚åò 1ej,(4)withej2R|Mobs|beingabasisvectorwitha1atthejthposition.Thisformoftest-statisticcanalsobeappliedtotestthefeaturesselectedbyotherregressionmethodssuchaselasticnet(ZouandHastie,2005),huberregression(Huber,1992),ornon-negativeleastsquares.Example2Inthecontextofchangepoint(CP)detectionusingfusedlasso,thematrixD2R(p 1)‚á•pisgivenbyD=0BB@ 110¬∑¬∑¬∑000 11¬∑¬∑¬∑00¬∑¬∑¬∑000¬∑¬∑¬∑ 111CCA.5MorePowerfulSelectiveInferenceforGeneralizedLassoconsideredtobenon-random.Inthisstudy,wedonotposeanyassumptionabouttherelationshipbetween¬µandpfeaturesx1,...,xp2Rn,butconsiderthecasewherealinearmodelwithgeneralizedlassoregularizationisemployedformodelingtherelationshipbetweenXandarandomresponsevectorY.Givenanobservedresponsevectoryobs2Rnsampledfromthemodel(1),thegeneralizedlassooptimizationproblemisgivenbyÀÜ =argmin 2Rp12kyobs X k22+ kD k1,(2)whereD2Rm‚á•pisapenaltymatrix,and  0isaregularizationparameter.ThematrixDanditsnumberofrowsarepre-determinedbytheuserforthepurposeofproducingsomedesiredstructuresinthesolutionÀÜ in(2).SomeexamplesofthematrixDarepresentedinExamples1,2and3.Sincetheoptimizationin(2)producesthesparsityofDÀÜ ,wedeÔ¨Åneasetofnon-zerocomponents(activeset)asMobs=A(yobs)={j:(DÀÜ )j6=0},j2[m],whereA:Y7!MindicatesthealgorithmwhichmapsaresponsevectorYintoasetofnon-zerocomponentsM.Then,wedeÔ¨ÅnetheselectioneventthattheactivesetforarandomresponsevectorYisthesameastheobservedresponsevectoryobsasnA(Y)=A(yobs)o.(3)Statisticalinference.Let‚åò>jYbealinearcontrastthatindicatesthetest-statisticwewanttoconsider,where‚åòjisdeÔ¨Åneddependingontheproblemandthejthselectedcom-ponentintheobservedactiveset.Example1Inthecaseoftestingthefeatureselectedbyvanillalasso(Tibshirani,1996),D=Ip2Rp‚á•p,whichistheidentitymatrix.Thetest-statistic‚åò>jY=ÀÜ jrepresentsthecoe cientofthejthselectedfeature(Leeetal.,2016)where‚åòjisdeÔ¨Ånedas‚åòj=XMobs‚á£X>MobsXMobs‚åò 1ej,(4)withej2R|Mobs|beingabasisvectorwitha1atthejthposition.Thisformoftest-statisticcanalsobeappliedtotestthefeaturesselectedbyotherregressionmethodssuchaselasticnet(ZouandHastie,2005),huberregression(Huber,1992),ornon-negativeleastsquares.Example2Inthecontextofchangepoint(CP)detectionusingfusedlasso,thematrixD2R(p 1)‚á•pisgivenbyD=0BB@ 110¬∑¬∑¬∑000 11¬∑¬∑¬∑00¬∑¬∑¬∑000¬∑¬∑¬∑ 111CCA.5MorePowerfulSelectiveInferenceforGeneralizedLassoWeobtain(12)bydecomposing‚á†=‚á†+ ‚á† with‚á†+,‚á†  0.Here,thekeypointisthatthecomponent||‚á†||1in(13)canbewrittenas||‚á†||1=Pj2[m](‚á†+j+‚á† j)becauseatleastoneof‚á†+jand‚á† j,j2[m],mustbesetto0attheoptimalsolution.Inotherwords,ÀÜ‚á†+j>0)ÀÜ‚á† j=0,andviceversa.Theideaofdecomposing‚á†=‚á†+ ‚á† areoftenemployedinreformulating`1-normof‚á†.3.2ConditionalDataSpaceCharacterizationLetusdeÔ¨Ånethesetofy2RnwhichsatisÔ¨ÅestheconditionsinEquation(8)asY={y2Rn|A(y)=A(yobs),q(y)=q(yobs)}.(14)Accordingtothesecondcondition,thedatainYisrestrictedtoaline(seeSec6inLiuetal.(2018),andFithianetal.(2014)).Therefore,thesetYcanbere-written,usingascalarparameterz2R,asY={y(z)=a+bz|z2Z},(15)wherea=q(yobs),b=‚åÉ‚åòj(‚åò>j‚åÉ‚åòj) 1,andZ=nz2R|A(y(z))=A(yobs)o.(16)Now,letusconsiderarandomvariableZ2Randitsobservationzobs2R,whichsatisfyY=a+bZandyobs=a+bzobs.Theconditionalinferencein(8)isre-writtenastheproblemofcharacterizingthesamplingdistributionofZ|{Z2Z}.(17)SinceZ‚á†N(0,‚åò>j‚åÉ‚åòj)underthenullhypothesis,Z|Z2ZfollowsatruncatedNormaldistribution.OncethetruncationregionZisidentiÔ¨Åed,thepivotalquantityinEquation(9)isequaltoFZ0,‚åò>j‚åÉ‚åòj(Z),andcanbeeasilyobtained.Thus,theremainingtaskistocharacterizeZ.CharacterizationoftruncationregionZ.Letusintroducetheoptimizationproblem(12)withparametrizedresponsevectory(z)=a+bz(a,baredeÔ¨Ånedin(15))forz2RasÀÜr(z)=argminr12r>Pr+(q0+q1z)>rs.t.GrÔ£øh0+h1z,(18)wherer=( ,‚á†+,‚á† )>2Rp+2m,q0=  X>a, 1m, 1m >2Rp+2m,q1=  X>b,0,0 2Rp+2m,P2R(p+2m)‚á•(p+2m)andG2R4m‚á•(p+2m),h0=h1=04m,P=0@X>X000000001A,G=0@ DD00Im Im Im0 ImIm0 Im1A>.93.1 QP for Generalized Lasso

We demonstrate that the generalized lasso problem can be reformulated as a QP problem.

Lemma 1. We denote Œæ = DŒ≤, and the generalized lasso in (2) can be rewritten as

1
2 ||

y

XŒ≤

2
2 + Œª
||

Œæ

||

1
||

‚àí

subject to Œæ = DŒ≤.

(11)

(cid:16)
By decomposing Œæ = Œæ+

= arg min

ÀÜŒ≤, ÀÜŒæ
(cid:17)
Œæ‚àí, Œæ+, Œæ‚àí

Œ≤‚ààRp,Œæ‚ààRm

‚àí

0, the generalized lasso problem can be formulated as the following

‚â•

QP problem:

ÀÜŒ≤, ÀÜŒæ+, ÀÜŒæ‚àí
(cid:16)

(cid:17)

= arg min
Œ≤,Œæ+,Œæ‚àí

1
2

(cid:62)

Œ≤

X (cid:62)X 0

Ô£´

Œæ+

Ô£∂

Œæ‚àí

Ô£¨
Ô£¨
Ô£¨
Ô£≠

Ô£∑
Ô£∑
Ô£∑
Ô£∏

Ô£´

Ô£¨
Ô£¨
Ô£¨
Ô£≠

0

0

0

0

Œ≤

0
Ô£∂

0

Œ≤

Ô£´

Œæ+

Ô£∂

Œæ‚àí

0

Ô£∑
Ô£∑
Ô£∑
Ô£∏

Ô£¨
Ô£¨
Ô£¨
Ô£≠

Ô£∑
Ô£∑
Ô£∑
Ô£∏

+ Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠

0p

X (cid:62)y

(cid:62)

Œ≤

1m

1m

Œª Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ≤

0m

0m

‚àí

Ô£∂

Ô£∑
Ô£∑
Ô£∑
Ô£∏

Ô£´

Ô£¨
Ô£¨
Ô£¨
Ô£≠

Ô£∂

Ô£∂

Ô£∑
Ô£∑
Ô£∑
Ô£∏

Ô£∑
Ô£∑
Ô£∑
Ô£∏

Ô£´

Œæ+

Ô£∂

Œæ‚àí

Ô£¨
Ô£¨
Ô£¨
Ô£≠

Ô£∑
Ô£∑
Ô£∑
Ô£∏

(12)

s.t

0

Im

(cid:16)

Im

‚àí

(cid:17)

Ô£´

Œæ+

Ô£∂

=

D 0

0

Ô£´

Œæ+

Ô£∂

, Œæ+

0, Œæ‚àí

0,

‚â•

‚â•

Œæ‚àí

Ô£¨
Ô£¨
Ô£¨
Ô£≠

Ô£∑
Ô£∑
Ô£∑
Ô£∏

(cid:16)

(cid:17)

Œæ‚àí

Ô£¨
Ô£¨
Ô£¨
Ô£≠

Ô£∑
Ô£∑
Ô£∑
Ô£∏

Rm are vectors in which all elements are set to 1 and 0, respectively, and

where 1m

Rm and 0m
Rm√óm is an identity matrix.

‚àà

‚àà

Im

‚àà

Proof. The optimization problem in (11) can be rewritten as follows:

= arg min

Œ≤(cid:62)X (cid:62)XŒ≤

1
2

Œ≤,Œæ

ÀÜŒ≤, ÀÜŒæ
(cid:17)

(X (cid:62)y)(cid:62)Œ≤ + Œª
||

Œæ

1
||

‚àí

s.t Œæ = DŒ≤.

(13)

(cid:16)
We obtain (12) by decomposing Œæ = Œæ+

0.
‚â•
j + Œæ‚àí
j‚àà[m](Œæ+
[m], must be set to zero in the optimal solution. That is, ÀÜŒæ+
j > 0

‚àí
1 in (13) can be written as

Œæ‚àí with Œæ+, Œæ‚àí

1 =

(cid:80)

||

||

||

||

Œæ

Œæ

component

j

‚àà

of decomposing Œæ = Œæ+

Œæ‚àí is often employed to reformulate the (cid:96)1-norm of Œæ.

‚àí

In this case, the key point is that the
j and Œæ‚àí
j ) because at least one of Œæ+
j ,
ÀÜŒæ‚àí
j = 0, and vice versa. The concept
(cid:4)

‚áí

3.2 Conditional Data Space Characterization

We deÔ¨Åne the set of y

‚àà

Rn that satisÔ¨Åes the conditions in Equation (8):

=

y
{

‚àà

Y

n

R

(y) =

| A

(yobs), q(y) = q(yobs)
.
}

A

(14)

According to the second condition, the data in

Y

are restricted to a line (see Section 6 in Liu et al. [2018]

and Fithian et al. [2014]). Therefore, the set

can be rewritten using the scalar parameter z

R, as follows:

‚àà

Y

where a = q(yobs), b = Œ£Œ∑j(Œ∑(cid:62)

j Œ£Œ∑j)‚àí1, and

=

{

Y

y(z) = a + bz

z

|

,

‚àà Z}

=

z

R

‚àà

Z

(y(z)) =

| A

(yobs)

A

.

(cid:8)

9

(cid:9)

(15)

(16)

Next, we consider a random variable Z

R, which satisÔ¨Åes Y = a + bZ and
yobs = a + bzobs. The conditional inference in (8) is rewritten as the problem of characterizing the sampling

R and its observation zobs

‚àà

‚àà

distribution of

Z

Z

| {

.

‚àà Z}

(17)

As Z

‚àº

N(0, Œ∑(cid:62)

j Œ£Œ∑j) under the null hypothesis, Z

Z

|

‚àà Z

follows a truncated normal distribution. Once

the truncation region

Z

has been identiÔ¨Åed, the pivotal quantity in Equation (9) is equal to F Z

0,Œ∑(cid:62)

j Œ£Œ∑j

(Z),

and it can be obtained easily. Thus, the remaining task is the characterization of

.

Z

Characterization of truncation region

Z

. We introduce the optimization problem (12) with the pa-

rameterized response vectors y(z) = a + bz (a, b that are deÔ¨Åned in (15)) for z

R as follows:

‚àà

ÀÜr(z) = arg min

r

1
2

r(cid:62)P r + (q0 + q1z)(cid:62)r

s.t. Gr

‚â§

h0 + h1z,

(18)

where r = (Œ≤, Œæ+, Œæ‚àí)

(cid:62)

‚àà
R(p+2m)√ó(p+2m) and G

P

‚àà

‚àà

Rp+2m, q0 =

X (cid:62)a, Œª1m, Œª1m

‚àí
R4m√ó(p+2m), h0 = h1 = 04m,
(cid:0)

(cid:62)

(cid:1)

‚àà

Rp+2m, q1 =

X (cid:62)b, 0, 0

Rp+2m,

‚àà

(cid:1)

‚àí

(cid:0)

X (cid:62)X 0

0

0

0

0

0

0

0

P = Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠

Ô£∂

Ô£∑
Ô£∑
Ô£∑
Ô£∏

, G = Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠

D

‚àí
Im

Im

‚àí

D

0

Im

‚àí
Im

Im

‚àí
0

(cid:62)

Ô£∂

.

0

0

Im

‚àí

Ô£∑
Ô£∑
Ô£∑
Ô£∏

Let ÀÜu(z) be the vector of optimal Lagrange multipliers and row(G) be the number of rows in matrix G.

The KKT conditions of (18) are written as

P ÀÜr(z) + q0 + q1z + G(cid:62) ÀÜu(z) = 0,

GÀÜr(z)

h0

h1z

0,

‚â§

‚àí
h1z)i = 0,

‚àí
h0

‚àí

‚àí

ÀÜui(z)(GÀÜr(z)

ÀÜui(z)

0,

‚â•

[row(G)],

[row(G)].

i
‚àÄ
i
‚àÄ

‚àà

‚àà

(19)

To construct the truncation region

ÀÜŒæ‚àí(z) in (18), and 2) identify the set of intervals of z on which
ÀÜŒæ+(z)
it is diÔ¨Écult to compute ÀÜŒæ+(z) and ÀÜŒæ‚àí(z) for inÔ¨Ånitely many values of z
R. We demonstrate that the
paths of ÀÜŒæ+(z) and ÀÜŒæ‚àí(z) can be computed within Ô¨Ånite operations by introducing parametric quadratic

(y(z)) =

in Equation (16), we must 1) compute the entire path of ÀÜŒæ(z) =
(yobs). However,

A

A

‚àí

Z

‚àà

programming.

3.3 Piecewise Linear Homotopy

In this section, we demonstrate that ÀÜr(z) in (18) is a piecewise linear function of z, which also indicates that
ÀÜŒæ+(z) and ÀÜŒæ‚àí(z) are piecewise linear functions of z.

10

Lemma 2. We denote

z =

[row(G)] : ÀÜui(z) > 0

I

i

{

‚àà

c
z = [row(G)]

,
}

I

z, and GIz as the rows of matrix

\ I

G in a set

I

z. Consider two real values z and z(cid:48) (z < z(cid:48)). If

z =

I

I

z(cid:48), we obtain

(20)

(21)

(22)

(23)

(24)

(25)

ÀÜr(z(cid:48))

ÀÜuIz (z(cid:48))

‚àí

ÀÜr(z) = œà(z)

‚àí
ÀÜuIz (z) = Œ≥(z)

(z(cid:48)

(z(cid:48)

z),

z),

‚àí

‚àí

√ó

√ó

œà(z)
Œ≥(z)Ô£π
Ô£ª

Ô£Æ

Ô£∞

=

P

Ô£Æ

GIz

Ô£∞

‚àí1

G(cid:62)
Iz
0 Ô£π
Ô£ª

q1

‚àí
h1
Iz

Ô£Æ

Ô£∞

.
Ô£π

Ô£ª

where œà(z)

Rrow(P ), Œ≥(z)

R|Iz|,

‚àà

‚àà

Proof. From the KKT conditions in (19), we obtain

P ÀÜr(z) + q0 + q1z + G(cid:62) ÀÜu(z) = 0,

(GÀÜr(z)

(GÀÜr(z)

h0

h0

‚àí

‚àí

‚àí

‚àí

h1z)i = 0,

h1z)i

0,

‚â§

i
‚àÄ
i
‚àÄ

z,

‚àà I

c
z.

‚àà I

According to (22), we obtain the following linear system:

P

Ô£Æ

GIz

Ô£∞

G(cid:62)
Iz
0 Ô£π
Ô£ª

=

=

ÀÜr(z)
ÀÜuIz (z)Ô£π
Ô£Æ
Ô£ª
Ô£∞
G(cid:62)
Iz
0 Ô£π
Ô£ª

GIz

P

Ô£Æ

Ô£∞

q0

‚àí
h0
Iz

Ô£Æ

+

Ô£π

q1

‚àí
h1
Iz

Ô£Æ

z

Ô£π

Ô£∞
‚àí1

Ô£ª
q0

‚àí
h0
Iz

Ô£Æ

Ô£∞

Ô£∞

+

Ô£ª
P

Ô£Æ

GIz

Ô£∞

Ô£π

Ô£ª

‚àí1

G(cid:62)
Iz
0 Ô£π
Ô£ª

q1

‚àí
h1
Iz

Ô£Æ

Ô£∞

z.

Ô£π

Ô£ª

ÀÜr(z)
ÀÜuIz (z)Ô£π
Ô£∞
Ô£ª
Similarly, for z(cid:48), we obtain

‚áî Ô£Æ

ÀÜr(z(cid:48))
ÀÜuIz(cid:48) (z(cid:48))Ô£π
Ô£Æ
Ô£ª
Ô£∞

=

P

Ô£Æ

GIz(cid:48)

‚àí1

‚àí1

P

+

q0

‚àí
h0

G(cid:62)
Iz(cid:48)
0 Ô£π
Ô£ª

G(cid:62)
Iz(cid:48)
0 Ô£π
Ô£ª
Ô£ª
z(cid:48), we can express the following:

GIz(cid:48)

Iz(cid:48)

Ô£Æ

Ô£π

Ô£∞

Ô£∞

Ô£Æ

q1

‚àí
h1

Iz(cid:48)

Ô£Æ

Ô£∞

z(cid:48).

Ô£π

Ô£ª

Ô£∞
By subtracting (23) from (24) and

z =

I

I

ÀÜr(z(cid:48))
ÀÜuIz (z(cid:48))Ô£π
Ô£Æ
Ô£ª
Ô£∞
G(cid:62)
Iz
0 Ô£π
Ô£ª

P

‚àí Ô£Æ

ÀÜr(z)
ÀÜuIz (z)Ô£π
Ô£ª
Ô£∞
‚àí1

q1

‚àí
h1
Iz

Ô£Æ

Ô£∞

Ô£π

Ô£ª

We denote

œà(z)
Œ≥(z)Ô£π
achieve the results in Lemma 2.
Ô£ª

GIz

=

Ô£Æ

Ô£∞

Ô£Æ

Ô£∞

=

P

Ô£Æ

GIz

Ô£∞

‚àí1

G(cid:62)
Iz
0 Ô£π
Ô£ª

q1

‚àí
h1
Iz

Ô£Æ

(z(cid:48)

z).

‚àí

Ô£π

√ó

Ô£∞

Ô£ª

with œà(z)

‚àà

Rrow(P ) and Œ≥(z)

‚àà

R|Iz|, and we subsequently

For simplicity, we assume that the generalized lasso solution is unique for any y(z), z

in the generalized lasso problem has been studied in Ali and Tibshirani [2019].

In this case, it can be

guaranteed that the matrix inverse in Equation (25) always exists.

If this is not the case, we can use

parametric quadratic programming for the degenerate cases in Best [1996].

11

(cid:4)

R. The uniqueness

‚àà

Computation of breakpoint. According to Lemma 2, the solution ÀÜr(z) is a linear function of z until z

reaches a breakpoint, at which one component of ÀÜu(z) enters or leaves the set

z. At this point, we discuss

I

the identiÔ¨Åcation of the breakpoint.

Lemma 3. Consider a real value z. Subsequently,

z =

I

I

z(cid:48) for any real value z(cid:48) in the interval [z, z + tz),

where z + tz is the value of the breakpoint:

z, t2
t1
,
tz = min
z}
{

(GIc

z ÀÜr(z)
(GIc

h0
Ic
‚àí
z ‚àí
h1
z œà(z)
Ic
z

h1
Ic
z
)j

‚àí

z)j

(cid:33)++

and

t2
z = min

j‚ààIz (cid:18)

ÀÜuj(z)
Œ≥j(z)

‚àí

.

(cid:19)++

t1
z = min
j‚ààIc

z (cid:32)‚àí

(26)

(27)

In this case, for any a

‚àà

R, (a)++ = a if a > 0, and (a)++ =

otherwise.

‚àû

Proof. We Ô¨Årst illustrate how to derive t1

z. According to (20), we obtain

ÀÜr(z(cid:48)) = ÀÜr(z) + œà(z)

(z(cid:48)

z).

‚àí

√ó

Thereafter, we need to guarantee

GIc

z (ÀÜr(z) + œà(z)

‚áî

(z(cid:48)

√ó

z))

‚àí

‚àí

‚áî

GIc

z ÀÜr(z(cid:48))
h1
Ic
z √ó

‚àí
(z(cid:48)

h0
Ic
z ‚àí
z)

‚àí

h0
Ic
z ‚àí
GIc

z œà(z)

(cid:16)

h1
Ic
z

‚àí

√ó

(cid:17)

h1
Ic
z

z(cid:48)

‚àí
(z(cid:48)

h1
Ic
z

z

z)

‚àí

0

0

‚â§

‚â§

(GIc

z ÀÜr(z)

‚â§ ‚àí

h0
Ic
z ‚àí

‚àí

h1
Ic
z

z).

(28)

The right-hand side of (28) is positive because GIc

z ÀÜr(z)

h0
Ic
z ‚àí

‚àí

h1
Ic
z

z

‚â§

0. Therefore, to satisfy Equation

(28),

z(cid:48)

z

‚àí

‚â§

min
j‚ààIc

z (cid:32)‚àí

(GIc

z ÀÜr(z)
(GIc

h0
Ic
z ‚àí
‚àí
h1
z œà(z)
Ic
z

h1
Ic
z
)j

‚àí

z)j

= t1
z.

(cid:33)++

Next, we explain how to derive t2

z. According to (21), we obtain

ÀÜuIz (z(cid:48)) = ÀÜuIz (z) + Œ≥(z)

(z(cid:48)

z).

‚àí

√ó

We need to guarantee

ÀÜuIz (z(cid:48)) > 0

‚áî

ÀÜuIz (z) + Œ≥(z)

(z(cid:48)

√ó

‚àí

z) > 0.

(29)

Therefore, to satisfy Equation (29),

z(cid:48)

‚àí

z < min

j‚ààIz (cid:18)

ÀÜuj(z)
Œ≥j(z)

‚àí

(cid:19)++

= t2
z.

z, t2
t1
Finally, using tz = min
z}
{

, we obtain the interval in which

z(cid:48) =

I

z for any z(cid:48)
I

‚àà

[z, z + tz).

(cid:4)

12

Algorithm 1 parametric SI
Input: X, yobs, Œª, D, [zmin, zmax]

1: Obtain observed active set

Mobs =

(yobs) for data (X, yobs)

A

2: for each selected j

‚àà Mobs do

3:

4:

5:

6:

Compute Œ∑j , and subsequently calculate a and b based on Œ∑j and yobs

Equation (15)

‚Üê

(y(z))

A

‚Üê

compute solution path (X, Œª, D, a, b, [zmin, zmax])

Truncation region

Z ‚Üê {

z :

A

(y(z)) =

Mobs}

P selective

j

‚Üê

Equation (10) (and/or selective conÔ¨Ådence interval)

7: end for

Output:

P selective
{

j

}j

‚ààMobs (and/or selective conÔ¨Ådence intervals)

Algorithm 2 compute solution path
Input: X, Œª, D, a, b, [zmin, zmax]

1: Initialization: k = 1, zk = zmin,

= zk

T

2: while zk < zmax do

3:

4:

5:

tzk ,

M

zk ‚Üê
zk+1 = zk + tzk ,

k = k + 1

compute step size(X, zk, a, b, Œª, D)

=

T ‚à™ {

zk+1}

T

(zk+1 is the value of the next breakpoint)

6: end while

Output:

{M

zk }k

‚àà

[

|T |‚àí

1]

3.4 Algorithm

In this section, we present the detailed algorithm of the proposed PP-based SI method. In Algorithm

1, to obtain the active set, we simply apply generalized lasso to the data (X, yobs), and we obtain

obs.

M

Thereafter, we conduct SI for each observed active set. For every j

obs, we Ô¨Årst obtain the direction of

‚àà M

interest Œ∑j. The main task is to compute the solution path of ÀÜr(z) in Equation (18) for the parameterized

response vector y(z), where the parameterized solution ÀÜr(z) varies for diÔ¨Äerent j

obs because the

‚àà M

direction of interest Œ∑j is dependent on j. This task can be achieved by Algorithm 2. Finally, after obtaining

the path, we can easily determine the truncation region

, which is used to compute the selective p-value

Z

or selective conÔ¨Ådence interval.

In Algorithm 2, a sequence of breakpoints is computed individually. The algorithm is initialized at

zk = zmin, k = 1. At each zk, the task is to determine the next breakpoint zk+1. This task can be performed

by computing the step size in Algorithm 3. This step is repeated until zk > zmax. By identifying all of the

13

Algorithm 3 compute step size
Input: X, z, a, b, Œª, D

1: y(z) = a + bz
2: Compute ÀÜr(z) for data (X, y(z)) and calculate ÀÜŒæ(z) = ÀÜŒæ+(z)

ÀÜŒæ‚àí(z) based on ÀÜr(z)

‚àí

3: Obtain

z =

M

(y(z)) =

A

j : ÀÜŒæj (z)
{

= 0

}

4: Compute tz

Lemma 3

‚Üê

Output: tz,

z

M

breakpoints

zt

}t‚àà[|T |], the entire path of

{

z for z

M

‚àà

R is expressed as

z =

M

(y(z)) =

A

(y(z1))

A

(y(z2))
...
(y(z|T |‚àí1))

A

A

if z

if z

if z

‚àà

‚àà

‚àà

[z1, z2],

[z2, z3],

[z|T |‚àí1, z|T |].

Ô£±

Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

Selection of [zmin, zmax]. Under normality, very positive and negative values of z do not aÔ¨Äect the in-

ference. Therefore, it is reasonable to consider a range of values, such as [

20œÉ, 20œÉ] [Liu et al., 2018] or

‚àí

j yobs
Œ∑(cid:62)

20œÉ,

j yobs
Œ∑(cid:62)

+ 20œÉ

‚àí|
distribution of the test statistic.
(cid:2)

| ‚àí

|

|

[Sugiyama et al., 2020], where œÉ is the standard deviation of the sampling

(cid:3)

In Line 2 of Algorithm 3, ÀÜr(z) can be computed based on the KKT conditions. However, it is well known

that numerical issues will arise. Therefore, in our experiments, we modify the algorithm slightly to overcome

these numerical problems. In particular, we Ô¨Årst replace Line 1 in Algorithm 3 with y(z) = a + b(z + ‚àÜz),

where ‚àÜz is a small value such that zk + ‚àÜz < zk+1 for all k

[
|T | ‚àí

‚àà

1]. Subsequently, at Line 2 of Algorithm

3, we simply obtain ÀÜr(z) by applying the QP solver to y(z). We can conÔ¨Årm whether ‚àÜz is suÔ¨Éciently small

by verifying whether exactly one component in the vector of the optimal Lagrange multipliers ÀÜu(z) has

already entered or left the set

I

‚àÜz = 0.0001.

z. This condition is satisÔ¨Åed in all of the experiments by simply setting

The complexity of Algorithm 1 is dependent on the number of breakpoints.

In the literature on PP,

the worst-case complexity increases exponentially with the problem size [Ritter, 1984, Allgower and George,

1993, Gal, 1995, Best, 1996, Mairal and Yu, 2012]. However, in practice, it has been reported that the

number of breakpoints is approximately linear with the problem size and it does not actually increase as

much as in the theoretical worst case. This has also been noted in regularization path studies [Osborne

et al., 2000, Efron and Tibshirani, 2004, Hastie et al., 2004, Mairal and Yu, 2012]. In fact, in all of the
experiments in ¬ß6, the number of breakpoints is within a reasonable size and the computational cost is not

a major problem of the proposed method.

14

(cid:54)
4 Generality of Proposed Method

Although we only focused on generalized lasso in ¬ß3, the parametric QP formulation in (18) is more general
and the forms of matrices P and G as well as vectors q0, q1, h0, and h1 can be changed depending on the
problem. That is, the method proposed method in ¬ß3 is Ô¨Çexible and can be applied to any problem that can

be converted into a parametric QP in the form of (18). In this section, we demonstrate the extensions of the

proposed method for testing the statistical signiÔ¨Åcance of the features that are selected by various feature

selection algorithms.

When applying a feature selection algorithm

A

set can be deÔ¨Åned as follows:

to the observed response vector yobs, the observed active

obs =

M

(yobs) =

A

j : ÀÜŒ≤j
{

.
= 0
}

In this setting, D = Ip. To test the selected features in

obs, the conditional inference is the same as that

M

deÔ¨Åned in (8) and the characterization of the truncation region
remaining task is to compute the solution path ÀÜŒ≤(z) for z
R and to identify the intervals of z in which
we obtain the same active set as yobs. In the following sections, we present the parametric QP formulations

is the same as (16). To identify

, the

Z

Z

‚àà

for vanilla lasso, elastic net, non-negative least squares, and Huber regression. As all of these regression
problems can be converted into the form of (18), the path of ÀÜŒ≤(z) can be computed within Ô¨Ånite operations
by using PP, as demonstrated in ¬ß3.3.

Parametric QP for vanilla lasso. Vanilla lasso with a parameterized response vector y(z) for z

deÔ¨Åned as

ÀÜŒ≤lasso(z) = arg min

Œ≤‚ààRp

1
2 ||

y(z)

XŒ≤

2
2 + Œª
||

Œ≤

||

1.

||

‚àí

R is

‚àà

(30)

Lemma 4. By decomposing Œ≤ = Œ≤+

Œ≤‚àí, Œ≤+, Œ≤‚àí

‚àí

‚â•

following parametric QP:

0p, the lasso problem in (30) can be solved by the

lasso(z), ÀÜŒ≤‚àí
ÀÜŒ≤+

lasso(z)

(cid:16)

(cid:17)

= arg min
Œ≤+,Œ≤‚àí‚ààRp

1
2 Ô£´

Ô£≠

Œ≤+
Œ≤‚àíÔ£∂
Ô£∏

(cid:62)

Ô£´

Ô£≠

X (cid:62)X

X (cid:62)X
‚àí
X (cid:62)X X (cid:62)X Ô£∂
Ô£∏

(cid:62)

‚àí

Ô£´

Œ≤+
Œ≤‚àíÔ£∂
Ô£∏

X (cid:62)y(z)
X (cid:62)y(z)Ô£∂
Ô£∏

‚àí

Ô£∂

Ô£∏

Ô£≠
Œ≤+
Œ≤‚àíÔ£∂
Ô£∏

Ô£´

Ô£≠

Œª12p

+

Ô£´

‚àí Ô£´

0p, Œ≤‚àí

0p.

Ô£≠
‚â•

s.t. Œ≤+

Ô£≠
‚â•

XŒ≤

2
2 + Œª
||

Œ≤

||

1
||

‚àí

Proof. According to (30), we obtain

ÀÜŒ≤lasso(z) = arg min

Œ≤‚ààRp

= arg min

Œ≤‚ààRp

y(z)

1
2 ||
1
2

(cid:0)

y(z)(cid:62)y(z)

‚àí

Œ≤(cid:62)X (cid:62)y(z)

‚àí

y(z)(cid:62)XŒ≤ + Œ≤(cid:62)X (cid:62)XŒ≤

+ Œª

Œ≤

||

1.

||

(cid:1)

(31)

15

(cid:54)
Similar to the proof of Lemma 1, as the component 1

2 y(z)(cid:62)y(z) does not aÔ¨Äect the optimal solution ÀÜŒ≤lasso(z),

we can rewrite (31) as

ÀÜŒ≤lasso(z) = arg min

Œ≤‚ààRp

1
2

Œ≤(cid:62)X (cid:62)XŒ≤

(X (cid:62)y(z))(cid:62)Œ≤ + Œª
||

Œ≤

1.

||

‚àí

(32)

Finally, by decomposing Œ≤ = Œ≤+

Œ≤‚àí, Œ≤+, Œ≤‚àí

‚àí

0p and

Œ≤

||

1 =

||

‚â•

Lemma 4.

(cid:80)

j‚àà[p](Œ≤+

j + Œ≤‚àí

j ), we obtain the result in
(cid:4)

In our preliminary conference paper [Le Duy and Takeuchi, 2021], we initially presented the idea of

introducing PP for vanilla lasso in a slightly diÔ¨Äerent (but essentially the same) manner.

Parametric QP for elastic net. The elastic net with a parameterized response vector y(z) for z

deÔ¨Åned as

ÀÜŒ≤elastic(z) = arg min

Œ≤‚ààRp

1
2n ||

y(z)

XŒ≤

2
2 + Œª
||
||

Œ≤

1 +

||

‚àí

1
2

Œ∂

Œ≤

||

2
2.
||

R is

‚àà

(33)

Lemma 5. By decomposing Œ≤ = Œ≤+

Œ≤‚àí, Œ≤+, Œ≤‚àí

‚àí

‚â•

the following parametric QP:

0p, the elastic net problem in (33) can be solved using

elastic(z), ÀÜŒ≤‚àí
ÀÜŒ≤+

elastic(z)

(cid:16)

(cid:17)

= arg min
Œ≤+,Œ≤‚àí‚ààRp

(cid:62)

Ô£≠

Ô£´

‚àí
(cid:62)

Œ≤+
Œ≤‚àíÔ£∂
Ô£∏
Œ≤+
Œ≤‚àíÔ£∂
Ô£∏
1
n Ô£´

1
2 Ô£´

Ô£≠

‚àí

Œª12p

1
2n Ô£´

Ô£≠

+ Œ∂

¬∑

+

Ô£´

X (cid:62)X

X (cid:62)X
‚àí
X (cid:62)X X (cid:62)X Ô£∂
Ô£∏

Ô£´

Œ≤+
Œ≤‚àíÔ£∂
Ô£∏

Ip

Ip
‚àí
Ip Ô£∂
Ô£≠
Ô£∏
(cid:62)

Ô£´

Ô£´

Ip

‚àí
Ô£≠
X (cid:62)y(z)
X (cid:62)y(z)Ô£∂
Ô£∏

Ô£∂

Ô£∏

Ô£≠
Œ≤+
Œ≤‚àíÔ£∂
Ô£∏
Œ≤+
Œ≤‚àíÔ£∂
Ô£∏

Ô£´

Ô£≠

s.t. Œ≤+

Ô£≠
‚â•

0p, Œ≤‚àí

‚â•

Ô£≠

‚àí
0p.

Proof. The proof of Lemma 5 is similar to the proof of Lemma 4.

(cid:4)

Parametric QP for non-negative least squares. The non-negative least squares problem with a

parametrized response vector y(z) for z

R is deÔ¨Åned as

‚àà

ÀÜŒ≤non‚àínegative(z) = arg min

Œ≤‚ààRp

1
2 ||

y(z)

XŒ≤

2
2
||

‚àí

s.t Œ≤

0.

‚â•

The above problem can be formulated as the following parametric QP:

ÀÜŒ≤non‚àínegative(z) = arg min

Œ≤‚ààRp

Œ≤(cid:62)X (cid:62)XŒ≤

1
2

X (cid:62)y(z)

(cid:62)

Œ≤ s.t Œ≤

0.

‚â•

(cid:1)

‚àí

(cid:0)

16

Parametric QP for Huber regression with (cid:96)1 penalty. The Huber regression with the (cid:96)1 penalty for

a parameterized response vector y(z) is formulated as

ÀÜŒ≤huber(z) = arg min

Œ≤‚ààRp

n

i=1
(cid:88)

LŒ¥(yi(z)

‚àí

with

x(cid:62)

i Œ≤) + Œª
||

Œ≤

1,

||

(34)

LŒ¥(e) =

1

2 e2
e
Œ¥(
|

Ô£±
Ô£≤

1
2 Œ¥)

| ‚àí

Œ¥,

if

e

|

| ‚â§
otherwise,

where Œ¥ > 0 is also a predetermined tuning parameter. Let e = y(z)

Ô£≥

vectors in which the ith element œÜi and ŒΩi are respectively deÔ¨Åned as

XŒ≤, œÜ

‚àí

‚àà

Rn, and ŒΩ

Rn be the

‚àà

œÜi = min

{|

ei

, Œ¥

,

}

|

ŒΩi = max

{|

ei

,

Œ¥, 0
}

| ‚àí

where ei is the ith element of e. Obviously,
Œ≤+, Œ≤‚àí

|
0p, the problem in (34) can be solved by the following parametric QP:

= œÜi + ŒΩi. Subsequently, by decomposing Œ≤ = Œ≤+

ei

|

‚â•

Œ≤‚àí,

‚àí

ÀÜœÜ(z), ÀÜŒΩ(z), ÀÜŒ≤+

huber(z), ÀÜŒ≤‚àí

huber(z)

(cid:16)

(cid:17)

= arg min
œÜ,ŒΩ,Œ≤+,Œ≤‚àí

1
2

œÜ(cid:62)œÜ + Œ¥

1(cid:62)
n ŒΩ +

¬∑

j + Œ≤‚àí
Œ≤+

j

j
(cid:88)
X

(cid:0)
Œ≤+

(cid:0)
0n,

‚â•

Œ≤‚àí

‚àí

(cid:1)

‚â§

(cid:1)

œÜ + ŒΩ,

s.t

‚àí
0n

œÜ

ŒΩ

y(z)

‚àí
u

‚â§
Œ¥

‚àí
1n, v

‚â§

Œ≤+

‚â•

‚â§
¬∑
0p, Œ≤‚àí

0p.

‚â•

The lasso-like formulation of the Huber regression has been discussed in She and Owen [2011]. Condi-

tional SI for outliers was studied in Chen and Bien [2019], and we recently investigated its PP version in

Tsukurimichi et al. [2021].

5 Characterization of CV-Based Tuning Parameter Selection Event

Various ML tasks involve careful tuning of a regularization parameter Œª that controls the balance between

an empirical loss term and a regularization term; for example, this is commonly achieved by CV. However,

the majority of the current SI studies have assumed a pre-speciÔ¨Åed Œª and have ignored the fact that Œª

is selected based on the data, because it is diÔ¨Écult to characterize the CV selection event. Loftus [2015]

and Markovic et al. [2017] proposed solutions to incorporate CV events. However, the former work required

additional conditioning on all intermediate models, which led to a loss of power, whereas the latter considered

a randomization version of CV instead of vanilla CV.

In this section, we introduce a new means of characterizing the minimal selection event in which Œª is

selected based on the data; for example, via CV 4. For notational simplicity, we consider the case in which

4We note that the following discussion is only applicable when the number of features p is independent of n.

17

the data are divided into training and validation sets, and the latter is used for selecting Œª. The following

discussion can easily be extended to CV scenarios. We rewrite the observed data as follows:

X, yobs
{

}

=

(Xtrain Xval)(cid:62)

R

‚àà

n√óp, (yobs

train yobs

val )(cid:62)

n

R

.

‚àà

Given a set of regularization parameter candidates Œõ, the process of selecting Œª is as follows:

(cid:8)

(cid:9)

1. For each Œª

‚àà

Œõ, we Ô¨Årst obtain ÀÜŒ≤Œª using the training data

ÀÜŒ≤Œª

arg min
Œ≤

‚àà

1
2 (cid:107)

yobs
train ‚àí

XtrainŒ≤

2
2 + Œª
(cid:107)

DŒ≤
(cid:107)

1.
(cid:107)

Thereafter, the validation error is deÔ¨Åned as

EŒª =

1
2 (cid:107)

yobs
val ‚àí

Xval ÀÜŒ≤Œª

2
2.

(cid:107)

2. We select Œªobs = Œª

‚àà

Œõ, which has the corresponding smallest validation error EŒª.

The selection event of the above validation process is deÔ¨Åned as

(Y ) =

{V

(yobs)

,
}

V

(35)

Œõ is the event that Œªobs is selected when validation is performed on yobs.

where

(yobs) = Œªobs

V

‚àà

After selecting Œªobs, we can obtain the observed active set

obs by applying generalized lasso on yobs
with the selected Œªobs, which can be deÔ¨Åned as in (3). However, to conduct a statistical test for each element

M

in

M

obs, the conditional inference will be diÔ¨Äerent from (8), because we must incorporate the selection event

of the validation process. Therefore, for each j

obs, we consider the following conditional inference:

‚àà M

Œ∑(cid:62)
j Y

(Y ) =

| {A

(yobs),

A

(Y ) =

V

V

(yobs), q(Y ) = q(yobs)

.
}

(36)

According to the third condition, the data are restricted on the line, as discussed in ¬ß3.2. Therefore, the

conditional data space in (36) can be rewritten as:

where

CV =

Y

y(z) = a + bz
{

|

z

CV

‚àà Z

,
}

CV =

z
{

‚àà

R

Z

(y(z)) =

| A

(yobs),

A

V

(y(z)) =

(yobs)
}

.

V

The remaining task to conduct the inference is to identify

CV. We can decompose

Z

CV into two separate

Z

sets

CV =

2,

1

Z

‚à© Z

Z

where

1 =

Z

z

{

‚àà

R

| A

(y(z)) =

(yobs)
}

A

and

2 =

Z

z

{

‚àà

R

| V

(y(z)) =

(yobs)
}

V

. The set

Z

1 can easily be

constructed using the method proposed in the previous sections. The remaining challenge is to identify

2.

Z

18

Algorithm 4 SI with K fold cross validation
Input: X, yobs, Œõ, D, K, [zmin, zmax]

1: Conduct K-fold CV to select Œªobs

2: Obtain

(yobs) for data (X, yobs) with the selected Œªobs

3: for each j

Mobs =

A
‚àà Mobs do

4:

5:

6:

7:

8:

9:

Compute Œ∑j , and subsequently calculate a and b based on Œ∑j and yobs

Equation (15)

‚Üê

Obtain

A

(y(z)) using Algorithm 2

‚Üê

compute solution path (X, Œªobs, D, a, b, [zmin, zmax])

Mobs}
2(X, a, b, Œõ, D, K)

1

Z

‚Üê {

z :

A

(y(z)) =

2

compute

Z

Z
‚Üê
ZCV =
Z
Compute P selective

‚à© Z

1

2

j

(characterize model selection event)

(characterize CV selection event)

in Equation (10) with truncation region

ZCV

10: end for

Output:

P selective
{

j

}j

‚ààMobs

To construct

Z

2, it is necessary to identify the intervals of z on which Œªobs has the smallest validation

error. That is, we can redeÔ¨Åne

2 =

z
{

R

|

‚àà

Z

EŒªobs(z)

‚â§

EŒª(z) for any Œª

Œõ

,
}

‚àà

where

EŒª(z) =

ÀÜŒ≤Œª(z)

arg min
Œ≤‚ààRp

‚àà

1
2 (cid:107)
1
2 (cid:107)

yval(z)

ytrain(z)

‚àí

2
2, ,

Xval ÀÜŒ≤Œª(z)
(cid:107)
2
2 + Œª
(cid:107)

XtrainŒ≤

‚àí

DŒ≤
(cid:107)

1.
(cid:107)

For each Œª

‚àà

Œõ, although it appears to be intractable to compute EŒª(z) for inÔ¨Ånitely many values of z

the task can be completed within Ô¨Ånite operations using PP.

(37)

(38)

R,

‚àà

As demonstrated in ¬ß3.3, the optimal solution ÀÜŒ≤Œª(z) in (38) is a piecewise linear function of z. That is,

Œª(z

Œª(z

ÀÜŒ≤Œª(z1) + s1
ÀÜŒ≤Œª(z2) + s2
...

z1)

z2)

‚àí

‚àí

if z

if z

ÀÜŒ≤Œª(z|TŒª|‚àí1) + s|TŒª|‚àí1

Œª

(z

z|TŒª|‚àí1)

if z

‚àí

[z1, z2],

[z2, z3],

[z|TŒª|‚àí1, z|TŒª|],

‚àà

‚àà

‚àà

(39)

ÀÜŒ≤Œª(z) =

Ô£±

Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

is the slope vector, the elements of which are dependent on œà(zk‚àà|TŒª|) in (20). The subscript

where sk‚àà|TŒª|
Œª
Œª in ÀÜŒ≤Œª(z), sk
Œª,
y(z) = (ytrain(z), yval(z))(cid:62), the vectors a and b can be rewritten as follows:

T

Œª indicates that these components are dependent on Œª. Moreover, because we decompose

a = (atrain aval)(cid:62) ,

b = (btrain bval)(cid:62) .

Therefore, we can write

yval(z) = aval + bvalz.

(40)

19

Algorithm 5 compute
Input: X, a, b, Œõ, D, K

2

Z

1: for Œª

‚àà
for k

Œõ do

1 to K do

‚Üê

2:

3:

4:

5:

6:

Compute ÀÜŒ≤k

Œª(z) as in (38) for fold k

Compute validation error Ek

Œª(z) as in (37) for fold k

end for

Compute mean error ¬ØEŒª(z) = 1
K

(cid:80)K

k=1 Ek

Œª(z) among K folds for current Œª candidate

7: end for

8:

2 =

Z
Output:

z
{

R

|

‚àà

¬ØEŒªobs (z)

‚â§

¬ØEŒª(z) for any Œª

Œõ

}

‚àà

2

Z

According to the piecewise linearity of ÀÜŒ≤Œª(z) in (39) and the linearity of yval(z) in (40), the validation error

EŒª(z) in (37) is a piecewise quadratic function of z, which can be expressed as follows:

(1/2)

(aval

||

(1/2)

(aval

||

‚àí

‚àí

(1/2)

(aval

||

‚àí

Xval ÀÜŒ≤Œª(z1) + Xvals1
Xval ÀÜŒ≤Œª(z2) + Xvals2
...

Œªz1) + (bval

Œªz2) + (bval

Xvals1
Xvals2

Œª)z

Œª)z

2
2
||
2
2
||

if z

if z

‚àà

‚àà

[z1, z2],

[z2, z3],

‚àí

‚àí

Xval ÀÜŒ≤Œª(z|TŒª|‚àí1) + Xvals|TŒª|‚àí1

Œª

z|TŒª|‚àí1).

+ (bval

‚àí

Xvals|TŒª|‚àí1
Œª

)z

2
2
||

if z

‚àà

[z|TŒª|‚àí1, z|TŒª|].

EŒª(z) =

Ô£±

Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

At this point, for each Œª

Œõ, we have a corresponding validation error EŒª(z), which is a piecewise quadratic

‚àà
function of z. Finally,
2 can be identiÔ¨Åed by determining the intervals of z in which the validation error
EŒªobs(z) corresponding to Œªobs is the minimum among a set of piecewise quadratic functions. The procedure

Z

for the K-fold CV case is presented in Algorithm 4.

6 Experiments

In this section, we discuss the performance evaluation of the proposed method. First, the experimental
setup is presented in ¬ß6.1. Thereafter, the results on synthetic and real data are outlined in ¬ß6.2 and ¬ß6.3,

respectively.

6.1 Experimental Setup

We conducted experiments on fused lasso, which is one of the most commonly studied cases of generalized

lasso, as well as on feature selection methods including vanilla lasso, elastic net, non-negative least squares,

and Huber regression + (cid:96)1 penalty. We executed the code on an Intel(R) Xeon(R) CPU E5-2687W v4 @

3.00 GHz.

20

Methods for comparison. We present the false positive rate (FPR), true positive rate (TPR), and

conÔ¨Ådence interval (CI) for the following conditional inferences:

‚Ä¢ Proposed method: Conditional inference without extra conditioning, as deÔ¨Åned in (8), which was the

main focus of this study.

‚Ä¢ Over-conditioning (OC): Conditional inference with extra conditioning. Regarding fused lasso, OC is

the method that was proposed in Hyun et al. [2018a]. Regarding vanilla lasso, elastic net, and Huber

regression + (cid:96)1 penalty, OC is the polytope-based SI that was proposed in Lee et al. [2016].

‚Ä¢ Data splitting (DS) [Cox, 1975]: DS is the commonly used procedure for selection bias correction. In

this approach, the data are randomly divided into two halves: one half is used for model selection and

the other half is used for inference.

Synthetic data generation for fused lasso. We set X = In, and the matrix D

R(n‚àí1)√ón was deÔ¨Åned

‚àà

as

1

‚àí
0

1

1

‚àí

0

0

Ô£´

Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠

0

1

0

0

0

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

1
¬∑ ¬∑ ¬∑ ‚àí

.

0
Ô£∂
0
Ô£∑
Ô£∑
Ô£∑
Ô£∑
1
Ô£∑
Ô£∑
Ô£∏

Regarding the FPR experiments, we generated 100 null data y = (y1, ..., yn)(cid:62), where yi‚àà[n] ‚àº
. To test the TPR, we generated y = (y1, ..., yn)(cid:62) with n = 60, in which
each n
}

70, 80, 90, 100

‚àà {

N(0, 1) for

yi

‚àº

N(¬µi, 1), ¬µi = Ô£±
Ô£¥Ô£≤

0,

if 1

i

‚â§

0 + ‚àÜ¬µ,

if 21

‚â§

‚â§
i

‚â§

40.

20 or 41

i

‚â§

‚â§

60,

The signiÔ¨Åcance level Œ± was set to 0.05. We used Bonferroni correction to account for the multiplicity in all

Ô£¥Ô£≥

of the experiments. If v selected features (hypotheses) are tested simultaneously, Bonferroni correction tests

each individual hypothesis at Œ±‚àó = Œ±/v. We ran 100 trials for each ‚àÜ¬µ

1, 2, 3, 4
}

‚àà {

and we repeated the

experiment 10 times.

Synthetic data generation for feature selection methods. We generated n outcomes as yi = x(cid:62)

i Œ≤+Œµi,

i = 1, ..., n, where xi

‚àº

N(0, Ip), in which p = 5, and Œµi

‚àº

N(0, 1). We set the regularization parameter Œª = 1

and signiÔ¨Åcance level Œ± = 0.05. Bonferroni correction was also applied. For the FPR experiments, all of the

elements of Œ≤ were set to 0. For the TPR experiments, the Ô¨Årst two elements of Œ≤ were set to 0.25. We ran

100 trials for each n

50, 100, 150, 200

}
we set n = 100, p = 5 and ran 100 trials.

‚àà {

and we repeated this experiment 10 times. For the CI experiments,

21

(a) FPR

(b) TPR

Figure 2: Results of FPR and TPR for fused lasso.

(a) FPR

(b) TPR

(c) CI

Figure 3: Results of FPR, TPR, and CI for vanilla lasso.

(a) FPR

(b) TPR

(c) CI

Figure 4: Results of FPR, TPR, and CI for elastic net.

DeÔ¨Ånition of TPR.

In SI, statistical testing is only conducted when at least one hypothesis is discovered

by the algorithm. Therefore, the deÔ¨Ånition of the TPR, which is also known as the conditional power, is as

follows:

TPR =

# correctly detected & rejected
# correctly detected

.

22

(a) FPR

(b) TPR

(c) CI

Figure 5: Results of FPR, TPR, and CI for non-negative least squares.

(a) FPR

(b) TPR

(c) CI

Figure 6: FPR, TPR, and CI results for Huber regression with (cid:96)1 penalty.

In the case of fused lasso, a detection is considered as correct if it is within L =

2 of the true CP locations.

¬±

This is because it is often diÔ¨Écult to identify the exact CPs accurately in the presence of noise. Many

existing studies considered a detection to be correct if it was within L positions of the true CP locations

[Truong et al., 2020]. In the case of feature selection, # correctly detected indicates the number of truly

positive features that are selected by the algorithm (e.g., lasso), whereas # rejected indicates the number of

truly positive features for which the null hypothesis is rejected by the SI.

6.2 Numerical Results

FPR, TPR, and CI results. The fused lasso results are presented in Figure 2. The results for the vanilla

lasso, elastic net, non-negative least squares, and Huber regression + (cid:96)1 norm are depicted in Figures 3, 4,

5, and 6, respectively. No over-conditioning occurred in the case of the non-negative least squares as we had

already restricted the coeÔ¨Écients to be positive. In summary, although all of the methods could properly

control the FPR at a signiÔ¨Åcance level of Œ±, the proposed method had the highest power among the methods.

The CI results were also consistent with the TPR results. That is, the shortest CI for the proposed method

indicated that it exhibited the highest power.

23

Figure 7: Robustness of proposed method for fused lasso.

Figure 8: Robustness of proposed method for vanilla lasso.

Figure 9: Robustness of proposed method for elastic net.

Figure 10: Robustness of proposed method for non-negative least squares.

Figure 11: Robustness of proposed method for Huber regression with (cid:96)1 penalty.

24

(a) Œõ1 = {2‚àí

1, 20, 21}

(b) Œõ2 = {2‚àí

10, ..., 210}

Figure 12: TPR comparison with existing method [Loftus, 2015] when accounting for CV selection event.

Robustness of proposed method in terms of FPR control. We demonstrated the robustness of our

method in terms of FPR control by considering the following cases:

Non-normal noise: We considered the noise following the Laplace distribution, skew normal distribution

‚Ä¢

(skewness coeÔ¨Écient: 10), and t20 distribution.

Unknown œÉ2: We considered the case in which the variance was estimated from the data.

‚Ä¢
We generated n outcomes: yi = x(cid:62)

i Œ≤ +Œµi, i = 1, ..., n, where p = 5, xi

N(0, Ip), and Œµi follows a Laplace

‚àº

distribution, skew normal distribution, or t20 distribution with a zero mean and the standard deviation set
to 1. In the case of the estimated œÉ2, Œµi

N(0, 1). We set all elements of Œ≤ to 0 and set Œª = 0.5. For every

‚àº
50, 100, 150, 200

‚àà {

. We conÔ¨Årmed that our method maintained high
}

case, we ran 1,200 trials for each n

performance on FPR control. The results are presented in Figures 7, 8, 9, 10, and 11.

Results when accounting for CV selection event. We conducted a comparison of the TPRs between

the proposed method and the OC version that was proposed in Loftus [2015] when Œª was selected from

Œõ1 =

{

2‚àí1, 20, 21

or Œõ2 =

}

2‚àí10, 2‚àí9, ..., 29, 210
{

}

. The results are presented in Figure 12. The existing

method had lower power because additional conditioning on all intermediate models was required, which

was also discussed in Markovic et al. [2017]. Our method exhibited higher power as we could characterize

the minimum conditioning amount.

EÔ¨Éciency of proposed method. Our main purpose was to demonstrate that the proposed method has

not only high statistical power, but also reasonable computational costs. We conducted experiments on

feature selection by lasso. The computational time of the proposed method was almost linear with respect

to the number of active features, as illustrated in Figure 13a. Figure 13b presents a boxplot of the actual

number of intervals of z that were encountered on the line when constructing the truncation region

. Figure

Z

13c depicts the eÔ¨Éciency of our method compared to that of Lee et al. [2016], in which the authors mentioned

the naive method for removing sign conditioning by enumerating all possible combinations of signs 2|Mobs|.

25

(a) Computational time of proposed

(b) Number of encountered intervals

(c) Comparison with Lee et al.

method.

on line.

[2016].

Figure 13: EÔ¨Éciency of proposed method.

Figure 14: Comparison between proposed method and methods in Liu et al. [2018], in which an exponentially

increasing number of all possible sign combinations is still required.

Comparison with Liu et al. [2018]. Furthermore, we demonstrated the eÔ¨Éciency of our method com-
pared to the two methods proposed in ¬ß3 (inference for partial regression targets) of Liu et al. [2018]. In

this work, only stable features were allowed to inÔ¨Çuence the formation of the test statistic. Stable features

are those with very strong signals and that cannot missed. In the Ô¨Årst method, which we refer to as TN-(cid:96)1,

the stable features were selected by setting a higher value of Œª. In the second method, which we refer to as

TN-Custom, the stable features were selected by setting a cutoÔ¨Ä value. The details of these two methods

are presented in Appendix B. In general, to perform SI with these two methods, all possible combinations of

signs, which increase exponentially, still need to be naively enumerated. The proposed method can be used

to solve this computational bottleneck. A comparison of the computational costs is illustrated in Figure 14.

6.3 Results on Real-World Datasets

Array CGH data. Array CGH analyses enable the detection of changes in copy numbers across the

genome. We applied the proposed method and OC to the dataset with the ground truth provided in Snijders

et al. [2001]. The results of the detected CPs and tables of p-values are presented in Figures 15 and 16.

26

A

B

C

D

Proposed

0.9

0.6

0.3

1.4

OC

0.9

0.3

0.3

6.0

15

10‚àí

5
10‚àí

√ó

√ó

E

0.0

1.2

√ó

11

10‚àí

Proposed

OC

A

0.0

0.0

(a) Chromosomes 17, 18, and 19.

(b) Chromosome 14.

Figure 15: Experimental results for cell lines GM00143 and GM01750.

Proposed

2.7

OC

5.3

A

320

10‚àí

98

10‚àí

√ó

√ó

B

0.0

0.0

A

B

Proposed

OC

1.1

1.1

√ó

√ó

41

10‚àí

41

10‚àí

9.1

9.1

√ó

√ó

14

10‚àí

14

10‚àí

(a) Chromosomes 1, 2, and 3.

(b) Chromosomes 20, 21, and 22.

Figure 16: Experimental results for cell line GM03576.

The solid red line denotes the signiÔ¨Åcant CPs, which had a p-value that was smaller than the signiÔ¨Åcance

level following Bonferroni correction. All of the results were consistent with those of Snijders et al. [2001].

Moreover, we compared the p-values of the proposed method and OC. The p-values of the proposed method

were smaller than or equal to those of OC for all true CPs, which indicates that the proposed method had

27

Figure 17: Boxplots of p-values. The left plot in each Ô¨Ågure depicts the distributions of the p-values, whereas

the right plot displays the distributions of the p-values for the cases in which the two p-values of the proposed

method and OC diÔ¨Äered. In Dataset 1, the percentage of the p-value of the proposed method was 55.81%

smaller than that of OC. In Dataset 2, the percentage of the p-value of the proposed method was 54.24%

smaller than that of OC. In general, the p-value of the proposed method tended to be smaller than that of

OC, which indicates that the proposed method had higher statistical power than OC.

higher power than OC.

The boxplots of the distribution of the p-values for the proposed method and OC on the real-world

dataset are illustrated in Figure 17. We used the jointseg package [Pierre-Jean et al., 2014] to generate

realistic DNA copy number proÔ¨Åles of cancer samples with ‚Äúknown‚Äù truths. Two datasets consisting of 1,000

proÔ¨Åles, each with a length of n = 60, were created, as follows:

‚Ä¢ Dataset 1: Resampled from GSE11976 with tumor fraction = 1.

‚Ä¢ Dataset 2: Resampled from GSE29172 with tumor fraction = 1.

Nile data. These data contain the annual Ô¨Çow volume of the Nile River at Aswan from 1871 to 1970 (100

years). In this case, the interest lies in unexpected events such as natural disasters. According to Figure 18,

the proposed algorithm identiÔ¨Åed a CP at the 28th position, corresponding to the year 1899. This result was

consistent with that of Jung et al. [2017].

Prostate data. We applied our proposed method for lasso to the prostate dataset from Hastie et al. [2009].

As p < n for this dataset, we could estimate œÉ2 using the residual sum of squares from the full regression

model with all p predictors. We set Œª = 5. Figure 19 depicts the 95% CIs for the features that were selected

by both lasso and DS.

28

Figure 18: Experimental result for Nile data. A CP was detected at the 28th position, which indicates that

a change in the volume level occurred in 1899.

Figure 19: Experimental results for prostate data. The indices of the features were 1: lcavol, 2: lweight, 3:

age, 4: lbph, 6: lcp, 7: gleason, and 8: pgg45.

Acknowledgements

This work was partially supported by MEXT KAKENHI (20H00601, 16H06538), JST Moonshot R&D

(JPMJMS2033-05), NEDO (JPNP18002, JPNP20006), RIKEN Center for Advanced Intelligence Project,

and RIKEN Junior Research Associate Program.

29

References

A. Ali and R. J. Tibshirani. The generalized lasso problem and uniqueness. Electronic Journal of Statistics,

13(2):2307‚Äì2347, 2019.

E. L. Allgower and K. George. Continuation and path following. Acta Numerica, 2:1‚Äì63, 1993.

F. R. Bach, D. Heckerman, and E. Horvits. Considering cost asymmetry in learning classiÔ¨Åers. Journal of

Machine Learning Research, 7:1713‚Äì41, 2006.

Y. Benjamini and D. Yekutieli. False discovery rate‚Äìadjusted multiple conÔ¨Ådence intervals for selected

parameters. Journal of the American Statistical Association, 100(469):71‚Äì81, 2005.

Y. Benjamini, R. Heller, and D. Yekutieli. Selective inference in complex research. Philosophical Transactions

of the Royal Society A: Mathematical, Physical and Engineering Sciences, 367(1906):4255‚Äì4271, 2009.

R. Berk, L. Brown, A. Buja, K. Zhang, and L. Zhao. Valid post-selection inference. The Annals of Statistics,

41(2):802‚Äì837, 2013.

M. J. Best. An algorithm for the solution of the parametric quadratic programming problem. Applied

Mathemetics and Parallel Computing, pages 57‚Äì76, 1996.

S. Chen and J. Bien. Valid inference corrected for outlier removal. Journal of Computational and Graphical

Statistics, pages 1‚Äì12, 2019.

Y. Choi, J. Taylor, and R. Tibshirani. Selecting the number of principal components: Estimation of the true

rank of a noisy matrix. The Annals of Statistics, 45(6):2590‚Äì2617, 2017.

D. R. Cox. A note on data-splitting for the evaluation of signiÔ¨Åcance levels. Biometrika, 62(2):441‚Äì444,

1975.

V. N. L. Duy, S. Iwazaki, and I. Takeuchi. Quantifying statistical signiÔ¨Åcance of neural network

representation-driven hypotheses by selective inference. arXiv preprint arXiv:2010.01823, 2020a.

V. N. L. Duy, H. Toda, R. Sugiyama, and I. Takeuchi. Computing valid p-value for optimal changepoint by

selective inference using dynamic programming. In Advances in Neural Information Processing Systems,

2020b.

B. Efron and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407‚Äì499, 2004.

W. Fithian, D. Sun, and J. Taylor. Optimal inference after model selection. arXiv preprint arXiv:1410.2597,

2014.

W. Fithian, J. Taylor, R. Tibshirani, and R. Tibshirani. Selective sequential model selection. arXiv preprint

arXiv:1512.02565, 2015.

30

T. Gal. Postoptimal Analysis, Parametric Programming, and Related Topics. Walter de Gruyter, 1995.

T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector

machine. Journal of Machine Learning Research, 5:1391‚Äì415, 2004.

T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: data mining, inference, and

prediction. Springer Science & Business Media, 2009.

T. Hocking, j. P. Vert, F. Bach, and A. Joulin. Clusterpath: an algorithm for clustering using convex fusion

penalties. In Proceedings of the 28th International Conference on Machine Learning, pages 745‚Äì752, 2011.

P. J. Huber. Robust estimation of a location parameter.

In Breakthroughs in statistics, pages 492‚Äì518.

Springer, 1992.

S. Hyun, M. G‚Äôsell, and R. J. Tibshirani. Exact post-selection inference for the generalized lasso path.

Electronic Journal of Statistics, 12(1):1053‚Äì1097, 2018a.

S. Hyun, K. Lin, M. G‚ÄôSell, and R. J. Tibshirani. Post-selection inference for changepoint detection algorithms

with application to copy number variation data. arXiv preprint arXiv:1812.03644, 2018b.

M. Jung, S. Song, and Y. Chung. Bayesian change-point problem using bayes factor with hierarchical prior

distribution. Communications in Statistics-Theory and Methods, 46(3):1352‚Äì1366, 2017.

M. Karasuyama and I. Takeuchi. Nonlinear regularization path for quadratic loss support vector machines.

IEEE Transactions on Neural Networks, 22(10):1613‚Äì1625, 2010.

M. Karasuyama, N. Harada, M. Sugiyama, and I. Takeuchi. Multi-parametric solution-path algorithm for

instance-weighted support vector machines. Machine Learning, 88(3):297‚Äì330, 2012.

V. N. Le Duy and I. Takeuchi. Parametric programming approach for more powerful and general lasso

selective inference. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages 901‚Äì909.

PMLR, 2021.

G. Lee and C. Scott. The one class support vector machine solution path. In Proc. of ICASSP 2007, pages

II521‚ÄìII524, 2007.

J. D. Lee, D. L. Sun, Y. Sun, and J. E. Taylor. Exact post-selection inference, with application to the lasso.

The Annals of Statistics, 44(3):907‚Äì927, 2016.

H. Leeb and B. M. P¬®otscher. Model selection and inference: Facts and Ô¨Åction. Econometric Theory, pages

21‚Äì59, 2005.

H. Leeb and B. M. P¬®otscher. Can one estimate the conditional distribution of post-model-selection estima-

tors? The Annals of Statistics, 34(5):2554‚Äì2591, 2006.

31

K. Liu, J. Markovic, and R. Tibshirani. More powerful post-selection inference, with application to the lasso.

arXiv preprint arXiv:1801.09037, 2018.

R. Lockhart, J. Taylor, R. J. Tibshirani, and R. Tibshirani. A signiÔ¨Åcance test for the lasso. Annals of

statistics, 42(2):413, 2014.

J. R. Loftus. Selective inference after cross-validation. arXiv preprint arXiv:1511.08866, 2015.

J. R. Loftus and J. E. Taylor. A signiÔ¨Åcance test for forward stepwise model selection. arXiv preprint

arXiv:1405.3920, 2014.

J. Mairal and B. Yu. Complexity analysis of the lasso regularization path. arXiv preprint arXiv:1205.0079,

2012.

J. Markovic, L. Xia, and J. Taylor. Unifying approach to selective inference with applications to cross-

validation. arXiv preprint arXiv:1703.06559, 2017.

K. Ogawa, M. Imamura, I. Takeuchi, and M. Sugiyama. InÔ¨Ånitesimal annealing for training semi-supervised

support vector machines. In International Conference on Machine Learning, pages 897‚Äì905, 2013.

M. R. Osborne, B. Presnell, and B. A. Turlach. A new approach to variable selection in least squares

problems. IMA journal of numerical analysis, 20(3):389‚Äì403, 2000.

S. Panigrahi, J. Taylor, and A. Weinstein. Bayesian post-selection inference in the linear model. arXiv

preprint arXiv:1605.08824, 28, 2016.

M. Pierre-Jean, G. Rigaill, and P. Neuvial. Performance evaluation of dna copy number segmentation

methods. BrieÔ¨Ångs in bioinformatics, 16(4):600‚Äì615, 2014.

B. M. P¬®otscher and U. Schneider. ConÔ¨Ådence sets based on penalized maximum likelihood estimators in

gaussian regression. Electronic Journal of Statistics, 4:334‚Äì360, 2010.

K. Ritter. On parametric linear and quadratic programming problems. mathematical Programming: Pro-

ceedings of the International Congress on Mathematical Programming, pages 307‚Äì335, 1984.

S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics, 35:1012‚Äì1030, 2007.

Y. She and A. B. Owen. Outlier detection using nonconvex penalized regression. Journal of the American

Statistical Association, 106(494):626‚Äì639, 2011.

A. M. Snijders, N. Nowak, R. Segraves, S. Blackwood, N. Brown, J. Conroy, G. Hamilton, A. K. Hindle,

B. Huey, and K. Kimura. Assembly of microarrays for genome-wide measurement of dna copy number.

Nature genetics, 29(3):263, 2001.

32

K. Sugiyama, V. N. L. Duy, and I. Takeuchi. More powerful and general selective inference for stepwise

feature selection using the homotopy continuation approach. arXiv preprint arXiv:2012.13545, 2020.

S. Suzumura, K. Nakagawa, Y. Umezu, K. Tsuda, and I. Takeuchi. Selective inference for sparse high-order

interaction models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,

pages 3338‚Äì3347. JMLR. org, 2017.

I. Takeuchi and M. Sugiyama. Target neighbor consistent feature weighting for nearest neighbor classiÔ¨Åcation.

In Advances in neural information processing systems, pages 576‚Äì584, 2011.

I. Takeuchi, K. Nomura, and T. Kanamori. Nonparametric conditional density estimation using piecewise-

linear solution path of kernel quantile regression. Neural Computation, 21(2):539‚Äì559, 2009.

I. Takeuchi, T. Hongo, M. Sugiyama, and S. Nakajima. Parametric task learning. Advances in Neural

Information Processing Systems, 26:1358‚Äì1366, 2013.

K. Tanizaki, N. Hashimoto, Y. Inatsu, H. Hontani, and I. Takeuchi. Computing valid p-values for image

segmentation by selective inference. In Proceedings of the IEEE/CVF Conference on Computer Vision

and Pattern Recognition, pages 9553‚Äì9562, 2020.

J. Taylor, R. Lockhart, R. J. Tibshirani, and R. Tibshirani. Post-selection adaptive inference for least angle

regression and the lasso. arXiv preprint arXiv:1401.3889, 354, 2014.

Y. Terada and H. Shimodaira. Selective inference after variable selection via multiscale bootstrap. arXiv

preprint arXiv:1905.10573, 2019.

X. Tian and J. Taylor. Selective inference with a randomized response. The Annals of Statistics, 46(2):

679‚Äì710, 2018.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:

Series B (Methodological), 58(1):267‚Äì288, 1996.

R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. The annals of statistics, 39(3):

1335‚Äì1371, 2011.

R. J. Tibshirani, J. Taylor, R. Lockhart, and R. Tibshirani. Exact post-selection inference for sequential

regression procedures. Journal of the American Statistical Association, 111(514):600‚Äì620, 2016.

C. Truong, L. Oudre, and N. Vayatis. Selective review of oÔ¨Ñine change point detection methods. Signal

Processing, 167:107299, 2020.

K. Tsuda. Entire regularization paths for graph data. In In Proc. of ICML 2007, pages 919‚Äì925, 2007.

33

T. Tsukurimichi, Y. Inatsu, V. N. L. Duy, and I. Takeuchi. Conditional selective inference for robust regres-

sion and outlier detection using piecewise-linear homotopy continuation. arXiv preprint arXiv:2104.10840,

2021.

F. Yang, R. F. Barber, P. Jain, and J. LaÔ¨Äerty. Selective inference for group-sparse linear models.

In

Advances in Neural Information Processing Systems, pages 2469‚Äì2477, 2016.

H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the royal statistical

society: series B (statistical methodology), 67(2):301‚Äì320, 2005.

A Full Target Case for Lasso in Liu et al. [2018]

In the full target case, as discussed in Liu et al. [2018], the data are used to select the interesting features,

but they are not used to summarize the relation between the response and the selected features. Therefore,

all features can be used to deÔ¨Åne the direction of interest.

Œ∑j = X(X (cid:62)X)‚àí1ej,

where ej

‚àà

Rp is a zero vector with 1 at the jth coordinate. The conditional inference is deÔ¨Åned as

Œ∑(cid:62)
j Y

j

|

(Y ), q(Y ) = q(yobs)

‚àà A

.

(41)

In Liu et al. [2018], the authors proposed a solution for conducting conditional inference for a speciÔ¨Åc case

(cid:8)

(cid:9)

when p < n, and there was no solution for the case when p > n. This problem can be solved with the

proposed PP method. First, we rewrite the conditional inference in (41) as the problem of characterizing

the sampling distribution of

Z

Z

| {

‚àà Z}

where

=

z

{

R

j

|

‚àà

Z

(y(z))
}

‚àà A

.

(42)

, only the path of the lasso solution ÀÜŒ≤(z) needs
y(z) in (42) is deÔ¨Åned as in (15). Thereafter, to identify
to be obtained, as proposed in ¬ß3, and the intervals in which j is an element of the active set corresponding
to ÀÜŒ≤(z) simply need to be veriÔ¨Åed along the path. Finally, after obtaining

, we can easily compute the

Z

Z

selective p-value or selective CI.

B Stable Partial Target Case for Lasso in Liu et al. [2018]

In the stable partial target case, as discussed in Liu et al. [2018], only stable features are allowed to inÔ¨Çuence

the formation of the test statistic. Stable features are those with very strong signals that we do not wish to

omit. We select a set

H

obs of stable features. Subsequently, for any j

obs, j

‚àà H

obs,

‚àà M

Œ∑j = XHobs (X (cid:62)

HobsXHobs)‚àí1ej.

34

For any j

obs, j

(cid:54)‚àà H

obs,

‚àà M

Œ∑j = XHobs‚à™{j}(X (cid:62)

Hobs‚à™{j}XHobs‚à™{j})‚àí1ej.

Next, we demonstrate how to construct

obs according to Liu et al. [2018].

H

Stable target formation by setting higher value of Œª (TN-(cid:96)1).

In this case,

set, but with a higher value of Œª than that used to select

obs. We denote

M

obs =

H

H

the conditional inference is deÔ¨Åned as

H

obs is the lasso active
(yobs), and subsequently,

Œ∑(cid:62)
j Y

j

|

(Y ),

‚àà A

(Y ) =

H

H

(yobs), q(Y ) = q(yobs)

.

(43)

The main drawback of the method in Liu et al. [2018] is that all 2|Hobs| sign vectors must be considered,

(cid:8)

(cid:9)

which requires substantial computation time when

obs

is large. This limitation can easily be overcome

|H
using our piecewise linear homotopy computation. First, we rewrite the conditional inference in (43) as the

|

problem of characterizing the sampling distribution of

Z

Z

| {

‚àà Z}

where

=

z
{

R

j

|

‚àà

Z

(y(z)),

‚àà A

(y(z)) =

H

(yobs)
}

.

H

Thereafter, we can easily identify

(y(z)) =

H

(yobs)

H

1 =
‚àà
, which we can simply obtain using the method proposed in ¬ß3 of the main paper.
}

2, where

(y(z))

‚àà A

2 =

‚à© Z

z
{

and

=

Z

Z

Z

Z

R

‚àà

{

}

z

j

1

|

R

|

Stable target formation by setting cutoÔ¨Ä value c (TN-Custom).

In this case, we determine

by setting a cutoÔ¨Ä value c to select Œ≤j, such that

Œ≤j
|

| ‚â•

c 5. The set

H

obs is deÔ¨Åned as

obs

H

obs =

H

j

{

obs,

‚àà M

Œ≤j
|

| ‚â•

,

c
}

where Œ≤j = e(cid:62)

MobsXMobs)‚àí1X (cid:62)
conditional inference is formulated as

j (X (cid:62)

Mobsyobs. We denote

obs =

H

(

H

obs)

M

‚äÇ M

obs, and subsequently, the

Œ∑(cid:62)
j Y

(Y )) =

(
A

(

H

obs),

M

| {H

(Y ) =

A

obs

M

.

}

(44)

The main drawback of the method in Liu et al. [2018] is that it still requires conditioning on

(Y ) =

obs

,

}
is large, because the enumeration of 2|Mobs| sign vectors

{A

M

which is computationally intractable when

|M

obs

|

is required. This limitation can easily be overcome using our proposed method.

5Our formulation is slightly diÔ¨Äerent from but more general than that in Liu et al. [2018].

35

